Under review as a conference paper at ICLR 2020
Improved Generalization Bound of
Permutation Invariant Deep Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
We theoretically prove that learning with permutation invariant deep neural net-
works largely improves their generalization performance. Learning problems with
data that are invariant to permutations are frequently observed in various applica-
tions, for example, point cloud data and graph data. Numerous methodologies
have been developed and they achieve great performances, however, understand-
ing a mechanism of the performance is still a developing problem. In this paper,
we derive a theoretical generalization bound for invariant deep neural networks
with a ReLU activation to clarify their mechanism. Consequently, our bound
shows that the main term of their generalization gap is improved by √n! where
n is a number of permuting coordinates of data. Moreover, we prove that an ap-
proximation power of invariant deep neural networks can achieve an optimal rate,
though the networks are restricted to be invariant. To achieve the results, we de-
velop several new proof techniques such as correspondence with a fundamental
domain and a scale-sensitive metric entropy.
1 Introduction
A learning task with permutation invariant data frequently appears in various situations in data anal-
ysis. A typical example is learning on sets such as a point cloud, namely, the data are given as a set
of points and permuting the points in the data does not change a result of its prediction. Another
example is learning with graphs which contain a huge number of edges and nodes. Such the tasks
are very common in various scientific fields (Ntampaka et al., 2016; Ravanbakhsh et al., 2016; Faber
et al., 2016), hence, numerous deep neural networks have been developed to handle such the data
with invariance (Zaheer et al., 2017; Li et al., 2018a; Su et al., 2018; Li et al., 2018b; Yang et al.,
2018; Xu et al., 2018). The succeeding methods show that their networks for invariance can greatly
improve the accuracy with a limited size of networks and data.
An important question with invariant data is to understand the reason for the empirical high accu-
racy from theoretical aspects. Since invariant data are high-dimensional in general, learning theory
claims that the high-dimensionality reduces its generalization performance. However, the methods
for invariant data achieve better accuracy, thus it contradicts the theoretical principle. Though several
theoretical studies (Maron et al. (2019) and Sannai et al. (2019)) prove a universal approximation
property of neural networks for invariant data and guarantee that invariant deep neural networks
have sufficient expressive power, the generalization power of the invariant deep neural networks is
left as an open question.
In this paper, we prove a theoretical bound for generalization of invariant deep neural networks.
To show an overview of our result, we provide a simplified version as follows. We consider a
supervised-learning problem with m pairs of observations (Xi , Yi) where Xi are regarded as p-
dimensional vectors, and Xi can divided ton coordinates and each of them have D = p/n dimen-
sion. Also, let f Sn denote a function by a deep neural network which satisfies an invariant property,
f (x) = f (σ ∙ x) holds for any X ∈ Rn× D where σ is an arbitrary permutation of D-dimensional
coordinates in x. Also, we define Rm (f) = m-1 Pim=1 L(Yi, f(Xi)) and R(f) = E[L(Y, f(X))]
as an empirical and expected loss value L(Y, f(X)). Then, we show that following:
Theorem 1 (Informal version of Theorem 2). Let fSn be a function by a deep neural network which
takes p-dimensional inputs and invariant to any permutations of n coordinates. Then, for sufficiently
1
Under review as a conference paper at ICLR 2020
small ε > 0, we obtain
RfSn)≤ Rm(fSn)+n nmεp+o(log(1∕ε)),
with probability at least 1 - O(ε). Here, C > 0 is a constant independent of m and n.
As a consequence of Theorem 1, the generalization bound is improved by √n! by the invariant
property. Since the number of coordinates n is huge in practice, e.g. there are n ≥ 1,000 points
in the point cloud data in Zaheer et al. (2017) and hence √n! ≥ 101,000 holds, We show that the
derived generalization bound is largely improved by invariance. Further, we also derive a rate of
approximation of neural networks with invariance (Theorem 4) and its optimality, thus we show that
an invariance property for deep neural networks does not reduce an expressive power.
From a technical aspect, we develop mainly three proof technique to obtain the improved bound
in Theorem 1. Firstly, we introduce a notion of a fundamental domain to handle invariance of
functions and evaluate the complexity of the domain (Lemma 1). Secondly, we show a one-to-one
correspondence between a function by invariant deep neural networks and a function on the funda-
mental domain (Proposition 2). Thirdly, we develop a scale-sensitive covering number to control
a volume of invariant functions with neural networks (Proposition 5). Based on the techniques, we
can connect a generalization analysis to the invariance of deep neural networks.
We summarize the contributions of this paper as follow:
•	We investigate the generalization bound of deep neural networks which are invariant to
permutation of n coordinates, then we show that the bound is improved by √n!.
•	We derive a rate of approximation of invariant deep neural networks. The result shows that
the approximation rate is optimal.
•	We develop several proof techniques to achieve the bound such as a complexity analysis
for a fundamental domain and a scale-sensitive metric entropy.
1.1 Notation
For a vector b ∈ RD, its d-th element is denoted by bd. Also, b-d := (b1, ..., bd-1, bd+1, ..., bD) ∈
RD-1 is a vector without bd. kbkq := (PjD=d bqd)1/q is the q-norm for q ∈ [0, ∞]. For a tensor
A ∈ RD1 ×D2, a (d1,d2)-th element of A is written as Ad1,d2. For a function f : Ω → R with a
set Ω, kf ∣∣Lq := (Rω |f (x)∣qdx)1/q denotes the Lq-norm for q ∈ [0, ∞]. For a subset Λ ⊂ Ω,九八
denotes the restriction of f to Λ. For an integer z, z ! = Qjn=1 j denotes a factorial of z . For a set
Ω with a norm ∣∣ ∙ ∣∣, N(ε, Ω, k ∙ ∣∣) := inf{N : ∃{ωj}N=ι s.t. ∪N=1 {ω : ∣∣ω - ωjk ≤ ε} ⊃ Ω} is
a covering number of Ω with ε > 0. For a set Ω, idn or id denotes the identity map on Ω, namely
idn(x) = X for any X ∈ Ω. For a subset ∆ ⊂ Rn, int(∆) denotes the set of the inner points of ∆.
2 Problem Setting
2.1	Invariant Deep Neural Network
We define a set of permutation Sn in this paper. Consider X ∈ Rn×D where n be a number of
coordinates in X and D be a dimension of each coordinate. Then, an action σ ∈ Sn on X is defined
as
(σ ∙ x)i,d = xσ-1(i),d, i = 1,…，n,d = 1,…，D,
here, σ is a permutation of indexes i. Also, we define an invariant property for general functions.
Definition 1 (Sn -Invariant/Equivariant Function). For a set X ⊂ Rn×D, we say that a map f : X →
RM is
•	Sn-invariant (or simply invariant) if f (σ ∙ x) = f (x) for any σ ∈ Sn and any X ∈ X,
•	Sn-equivariant (or simply equivariant) if there is an Sn-action on RM and f (σ ∙ x)=
σ ∙ f (x) for any σ ∈ Sn and any X ∈ X.
2
Under review as a conference paper at ICLR 2020
In this paper, we mainly treat fully connected deep neural networks with a ReLU activation function.
The ReLU activation function is defined by ReLU(x) = max(0, x). Deep neural networks are built
by stacking blocks which consist of a linear map and a ReLU activation. More formally, it is a
function Zi : Rdi → Rdi+1 defined by Zi(x) = ReLU(Wix + bi), where Wi ∈ Rdi+1 ×di and
bi ∈ Rdi+1 for i = 1, ..., H. Here, H is a depth of the deep neural network and di is a width of the
i-th layer. An output of deep neural networks is formulated as
f(x) := ZH ◦ ZH-1...Z2 ◦ Zi(x).	(1)
Let FDNN be a set of functions by deep neural networks.
We also consider an invariant deep neural network defined as follows:
Definition 2 (Invariant Deep Neural Network). f ∈ FDNN is a function by a Sn -invariant deep
neural network, if f is a Sn-invariant function. Let FDSnN N ⊂ FDNN be a set of functions by
Sn-invariant deep neural networks.
The definition is a general notion and it contains several explicit invariant deep neural networks. We
provide several representative examples as follow.
Example 1 (Deep Sets). Zaheer et al. (2017) develops an architecture for invariant deep neural
networks by utilizing layer-wise equivariance. Their architecture consists of equivariant layers
`1 , ..., `j, an invariant linear layer h, and a fully-connected layer f0. For each `i .i = 1, .., j, its
parameter matrix is defined as
Wi=λI+γ(11>),λ,γ∈R,1= [1,..., 1]>,
which makes 'i as a layer-wise equivariant function. They show that f = f 0 ◦ h ◦ 'j∙ ◦•…'1 is an
invariant function. Its illustration is provided in Figure 1.
Example 2 (Invariant Feature Extraction). Let e is a mapping for invariant feature extraction which
will be explicitly constructed by deep neural networks in Proposition 2. Then, a function f = g ◦ e
where g is a function by deep neural networks with a restricted domain. Figure 2 provides its image.
2.2	Learning Problem with Invariant Network
Problem formulation: Let I = [0, 1]n×D be an input space with dimension p = dD. Let Y be an
output space. Also, let L : Y × Y → R be a loss function which satisfies supy,y0∈Y |L(y, y0)| ≤ 1
and I-LiPSChitz continuous. Let P * (x, y) be the true unknown distribution on IXY, and for f : I →
Y, R(f) = E(X,Y)〜P * [L(f * (X), Y)] be the expected risk of f. Also, suppose we observe a training
dataset Dm := {(X1,Y1), ..., (Xm,Ym)} of size m. LetRm(f) := m-1 Pim=1 L(f(Xi),Yi) be the
empirical risk of f. A goal of this study to investigate the expected loss R(f) with a function f from
a set of functions as a hypothesis set.
Learning with Invariant Network: We consider learning with a hypothesis set by invariant deep
networks. Namely, we fix an architecture of deep neural networks preserves fSn ∈ FDSnN N to be an
invariant function. Then, we evaluate the expected loss R(f Sn ).
layer, h is a linear invariant layer, and f0 is a function
by networks.
nected layer g and a feature ex-
traction layer e.
3
Under review as a conference paper at ICLR 2020
3 Main Result
3.1	Complexity-control bound
We show that the learning procedure with invariance can largely improve the generalization perfor-
mance of a deep neural network by proving the improved bound for the generalization error of f
with invariance. We firstly derive a Complexity-dependent bound which holds with an arbitrary true
distribution. The bound depends on a Complexity control of FDSnNN and the Rademacher complex-
ity.
Theorem 2 (Main Result 1). Let FDSnN N be a set of functions by Sn-invariant deep neural networks
which are C∆-Lipschitz continuous and bounded by B > 0. Then, for any fSn ∈ FDSnN N and for
any ε > 0, the following inequality holds with probability at least 1 - 2C∆ε:
R(fSn) ≤ Rm(fSn)+
2log(2c2B∕ε) +2log(1∕2ε)
m
=:I1
=:I2
where c1 , c2 > 0 are constants which are independent of n and m.
Significantly, the main term Ii in Theorem 2 is improved by √n! in the denominator. Note that
we regard I1 as the main term since I2 is a logarithmic order in ε. As n is huge in practice, e.g. a
number of points in point cloud data, the term n! largely improves the tightness of the bound.
We mention that εp appears in the denominator of I1 and it looks a harmful term when p = nD
is large. However, the term is negligible with large n, because n! increases much faster than εnD .
Namely, We can obtain n!εnD = Ω(nc) holds for any c > 0 and ε > 0 as n → ∞.
Proof of Theorem 2 utilizes a complexity control for FDSnNN . As a preparation, we apply the well-
knoWn bound (e.g. a slightly modified version of Theorem 10.1 in Anthony & Bartlett (2009)) and
obtain
2bg2N(ε,FDNN, ∣H∣l∞) + 2log(1∕2ε)
m
(2)
R(fSn) ≤ Rm(fSn) +
which describes generalization of fSn by the covering number log 2N(ε, FDNN, k ∙ I∣l∞). Then,
We bound the covering number by the folloWing Theorem Which plays a key role to achieve the main
result in Theorem 2. Proof of Theorem 3 depends on several newly developed results presented in
Section 4.
Theorem 3 (Complexity Bound). LetFDSnNN be defined in section 2. Then, with an existing constant
c > 0, we obtain
log N (2C∆ δ,FDNN ,∣H∣l∞(i) ) ≤
c
n! δP
+ log
2cB
δ~ J
Remark 1 (Bound without invariance). The bound is a general version of an ordinary learning
f ∈ FDNN which does not have invariance. Rigorously, suppose FDNN is a set of functions which
are C∆ -Lipschitz continuous and bounded by B > 0. Then, for any f ∈ FDNN and ε > 0, the
inequality in Theorem 2 holds with n = 1.
Remark 2 (Bound for covering numbers). We mention that there is another way to bound the
covering number of FDSnNN by a number of parameters (e.g. Theorem 14.5 in Anthony & Bartlett
(2009)). Such a bound has a fast order since its order is a logarithm of ε. However, the bound
has a linear order in a number of parameters, hence it easily increases with large-scale deep neural
networks which possess a huge number of nodes and edges. Moreover, such a bound is independent
of the volume of the domain, hence we cannot obtain the scale-sensitive covering number. To avoid
the problem, we employ another strategy in Theorem 3.
3.2	Approximation-control bound
We investigate the approximation power of invariant deep neural networks to clarify how they can
achieve a small empirical loss. Although we restrict the expressive power of deep neural networks
4
Under review as a conference paper at ICLR 2020
in the learning procedure, we prove that our networks have sufficient power of approximation. To
the aim, We define the Holder space which is a class of smooth functions, then investigate the
approximation power of invariant deep neural networks for the space.
Definition 3 (Holder space). Let α > 0 be a degree of smoothness. For f : I → R, the Holder
norm is defined as
kf ∣∣Hα := max sup ∣∂βf (x)| + max sup
β“βl<bαc x∈I	β=bαc x,x0∈I,x=x0
∣∂βf(x)- ∂βf (x0)∣
∣χ - χ0∣α-bαc
Then, the Holder space on I is defined as
Hα= nf∈Cbαc∣f∣Hα <∞o.
Also, HBα = {f ∈ Hα | ∣f ∣Hα ≤ B} denotes the B-radius closed ball in Hα.
Intuitively, Hα is a set of bounded functions which are α-times differentiable. The notion of the
Holder space is often utilized in characterizing the optimal functions f * (e.g. see Schmidt-Hieber
(2017)). We achieve the more detailed bound for the generalization error with assuming f * ∈ HB.
Theorem 4 (Main Theorem 2). For any ε > 0, suppose FDNN has at most O(log(1∕ε))layers and
O(ε~p/ɑ log(1∕ε)) non-zero parameters. Then, for any invariant f * ∈ HB , there is f Sn ∈ FDNN
such that
∣f Sn - f* ∣L∞ (I) ≤ ε.
The result in Theorem 4 clarifies the approximation power of deep networks, and also show that a
sufficient number of parameters (nodes) makes the generalization error converge to zero. Also, the
theorem shows that the approximation error decreases as the number of parameters increase with
the rate -p∕α up to log factors. The rate is the optimal rate by Yarotsky (2017) without invariance.
Hence, we prove that the deep networks with invariance can achieve the optimal approximation rate
even with the invariance restriction.
4	Proof and its Strategy
4.1	Fundamental Domain and its Correspondence
To handle the invariance property in our proof, we provide a key notion to show the main result.
Definition 4 (Fundamental Domain). Let G be a group acting on a set J. ∆ ⊂ J is said to be a
fundamental domain of J with respect to the action of G if ∆ satisfy the following properties;
•	J 二 ∪σ∈G {σ ∙ X | X ∈ ∆}.
•	σ ∙ int(∆) ∩ T ∙ int(∆) = 0 for any σ = T ∈ G.
In our case, we can take a fundamental domain explicitly.
Proposition 1. PutI = [0, 1]n×D. Then
△ := {x ∈ I | X1,1 ≥ X2,1 ≥ …≥ Xn,l}
is a fundamental domain ofI with respect to the permutation action ofSn defined in Section 2.1.
Figure 3 provides △ with n = 3 and D = 1. Intuitively, △ is an extracted feature space for an
invariant function. Any element of I corresponds to an element of △ with an existing action in Sn ,
namely, we can obtain
I = ∪σ∈Sn {σ ∙ X | X ∈ △}.
Proof of Proposition 1. We confirm the first property of the fundamental domain, namely I =
∪σ∈G {σ ∙ x | x ∈ △}. Take X ∈ I. There is a σ-1 ∈ Sn such that Xσ(i),ι ≥ Xσ(2),1 ≥ … ≥
Xσ(n),ι∙ Then by the definition of △, σ-1 ∙ X ∈ △. Hence X ∈ σ ∙ △ = {σ ∙ X | X ∈ △} . This
5
Under review as a conference paper at ICLR 2020
Figure 4: The Sort layer (red) which converts g ∈ FD∆N N
to f = Λ-1 (g) ∈ FDSnNN with n = D = 3. The Sort
layer exchanges the first elements of each xi ∈ RD .
Figure 3: A fundamental do-
main ∆ (the green cone) in I
(the blue cube) with n = 3
and D = 1.
implies the first property.
We confirm the second property. We have int(∆) = {x ∈ I | x1,1 > x2,1 > ∙ ∙ ∙ > xn,1} By the
definition of our action, σ ∙ int(∆)= x ∈ I | xσ-1(1),1 > xσ-1(2),1 > ∙∙∙ > xσ-1(n),1 . Hence
σ ∙ int(∆) ∩ τ ∙ int(∆) = 0 for any σ = τ ∈ G.	口
We provide two important properties of ∆. Firstly, we start with showing that there is a one-to-one
correspondence between deep neural networks on ∆ and invariant deep neural networks on I . To
the aim, we consider a set of functions on ∆ by (not necessarily invariant) deep neural networks;
FD∆N N = {g : ∆ → R | g has the form (1)}.
Then, we obtain the following result:
Proposition 2. There exists a bijection map Λ : FDSnN N → FD∆NN. Further, for any f ∈ FDSnN N,
Λ(f) is obtained by the restriction of f, namely Λ(f) = f∆, and for any g ∈ FD∆NN, Λ-1(g) can
be obtained by adding sorting layers appeared in the proof.
Figure 4 provides an image for Λ-1(g) for g ∈ FD∆NN. For preparation for proof of Proposition
2, we define an explicit invariant deep neural network. For a vector z ∈ RN for some N , let
max(j) (z1, . . . , zN) (resp. min(j) (z1, . . . , zN)) be a function which returns the j-th largest (resp.
smallest) element of {z1, . . . , zN}. We can easily see that these functions are a Sn-invariant func-
tion. More strongly, we have the following proposition.
Proposition 3. max(j) (z1, . . . , zN) and min(j) (z1, . . . , zN) are represented by an existing deep
neural networks with an ReLU activation for any j = 1, ..., N.
Proof of Proposition 3. Firstly, since
max(z1, z2) = max(z1 - z2, 0) + z2, and min(z1, z2) = - max(z1 - z2, 0) + z1
hold, we see the case ofj = 1, N = 2. By repeating max(z1, z2), we construct max(1)(z1, . . . , zN)
and min(1) (z1, . . . , zN). Namely, we prove the claim in the case of j = 1 and arbitrary N. At
first, we assume N is even without loss of generality, then we divide the set {z1, ...zN} into sets
of pairs {(z1, z2), ...(zN-1, zN)}. Then, by taking a max operation for each of the pairs, we have
{y1 = max(z1, z2), ..., yN/2 = max(zN -1, zN)} . We repeat this process to terminate. Then
we have max(1) (z1, . . . , zN) it is represented by an existing deep neural network. Similarly, we
have min(1) (z1, . . . , zN). Finally, we prove the claim on j = 2, ..., N by induction. Assume that
for any N and ' < j, max(') (z1,..., ZN) is represented by a deep neural network. We construct
max(j)(z1, . . . , zN) as follows: since
max(j-1) (z-`) =
max(j-1) (z1, . . . , zN)
max(j) (z1, . . . , zN)
(if z` ≤ max(j)(z1, . . . ,zN))
(otherwise)
holds, We have max(j) (z1,..., ZN) = min({max(j-1) (z`) | ' = 1,..., N}). By inductive hypoth-
esis, the right hand side is represented by a deep neural network.	口
6
Under review as a conference paper at ICLR 2020
Proof of Proposition 2. We first define sorting	layers which	is an Sn -invariant
network mapping from I to ∆.When D =	1,	put Sort1 (x1,1, . . . , xn,1)	=
(max(1) (x1,1, . . . , xn,1), . . . , max(n) (x1,1, . . . , xn,1)).	Then	by Proposition	3,
Sort1 (x1,1, . . . , xn,1) = Sort1 (x1,1, . . . , xn,1) is also	a	function by an	Sn-invariant deep	neu-
ral network and Sort(x1,1 , . . . , xn,1) is the function from I to ∆. When D > 1, we first consider
Sort1(x1,1, . . . , xn,1). Since Sort1(x1,1, . . . , xn,1) gives a permutation on (x1,1, . . . , xn,1), for each
(x1,1, . . . , xn,1), we can find σ ∈ Sn such that
Sort1 (x1,1 , . . . , xn,1 ) = (Sort1 (x1,1 , . . . , xn,1 )1 , ..., Sort1 (x1,1 , . . . , xn,1 )n ) = (xσ(1),1 , . . . , xσ(n),1 ).
Then we define
	/ SortI(Xι,ι,. .	.,Xn,1) 1	∙ .	•	Xb(1),d	∙	•	Xb(1),D、 .
Sort(x)=	. . Sort1 (X1,1, .	. , Xn,1 )i	. . Xσ(i),d	. . Xσ(i),D
∖ SortI(X1,1, . .. , Xnj)n ∙ ∙ ∙	xσ(n),d ∙ ∙ ∙	xσ(n),D )
By the construction and the definition of ∆, Sort(x) is the function to ∆. We confirm Sort(x)
is Sn-invariant. Take arbitrary T ∈ Sn and fix X and σ ∈ Sn as above. Put T ∙ X = y. We
show Sort(y) = Sort(X). Since Sort1 is an Sn-invariant function, we see Sort1(y1,1, . . . , yn,1) =
Sort1 (T (X1,1 , . . . , Xn,1 )) = (Xσ(1),1 , . . . , Xσ(n),1 ) = (yσ(τ -1 (1)),1 , . . . , yσ(τ -1 (n)),1 ). Then we have
	(	Sort1(y1,1, . .	. . , yn,1)1	• • .	•	yσ(τ -1 (1)),d	• • •	yσ(τ-1(1)),D .	)
Sort(y)=		. . Sort1(y1,1,. .	. . . , yn,1)i	. yσ(τ -1 (i)),d	. . yσ(τ -1 (i)),D ..	
	∖	. . Sort1(y1,1, .	. . , yn,1 )n	• •	• yσ(τ -1 (n)),d	.. • • •	yσ(τ -1 (n)),D	
		Sort1(X1,1,. .	. . , Xn,1 )1	• .	• •	Xσ(1),d	• • •	Xσ(1),D、 .	
=		. . Sort1(X1,1, . .	. . , Xn,1 )i	. . Xσ(i),d .	. . Xb(i),D .	
	∖	. . Sort1(X1,1,.	. . , Xn,1 )n	•	. . • •	Xσ(n),d	• • •	. . Xσ(n),D	
= Sort(X),
where the second equality follows from T-1 ∙ y = x.
By using this function, we define the inverse of Λ. For any function f by a deep neural network on
∆, we define Φ(f) = f ◦ Sort. We confirm Λ ◦ Φ = idF∆ and Φ ◦ Λ = idFSn . Since we have
Λ ◦ Φ(f) = Λ ◦ f ◦ Sort = (f ◦ Sort)∆ = f,
Λ ◦ Φ is equal to idF∆. Similarly,
Φ ◦ Λ(f) = Φ ◦ f∆ = f∆ ◦ Sort = f,
where the last equality follows from the Sn-invariance of f. Hence, We have the desired result. □
The second key property of ∆ is that we can measure its size. Since ∆ is included in I, we can
naturally measure its volume by the Euclidean metric. By utilizing the property, we evaluate its
volume by a covering number of ∆ by the following lemma:
Lemma 1 (Covering bound for ∆). There is a constant C such that for enough small ε > 0, we
obtain
C
N(ε, δ, k∙k∞) ≤ L.
Proof of Lemma 1. Let C(I) be a set of ε-cubes which is a standard subdivision ofI. We can easily
see that C(I) attains the minimum value ε-nD of the number of ε-cubes which is the covering of I.
7
Under review as a conference paper at ICLR 2020
We show that We can find a subset of C(I) whose cardinality is 胃——+ O(ε-n(D-1)). The strategy
of the proof is as follows. At first, we calculate the number A of cubes in C(I) which intersect
with the boundary of σ ∙ ∆. Then since the permutation on the cubes which do not intersect with the
boundary of σ ∙ ∆ is free, if A is O(ε-n(DT)), we can find the covering whose cardinality is ⅛T +
O(ε-n(DT)). Since σ ∙ ∆ is {x ∈ I | Xσ-i(i),ι ≥ Xσ-i(2),1 ≥ …≥ Xσ-i(n),ι}, any boundary of
σ ∙ ∆ is of the form {x ∈ I | Xσ-I(1),1 ≥ …Xσ-I(i),1 = Xσ-i(i+1),1 ≥ …≥ Xσ-I(n),l}.
Fix σ and i. Consider the projection π : Rn×D → Rn-1×D which sends xσ-1 (i),1 to zero. π
induces the map πe : C (I) → C (π(I)), where C (π(I)) is a set of -cubes which is the subdivision
of π(I) induced by C(I) . Let C(I)diag denote the set of cubes in C(I) which intersect with the set
B = x ∈ I | xσ-1(i),1 = xσ-1 (i+1),1 . Then we can see that πe is injective on C(I)diag as follows.
Let us denote a = (as,r) ∈ RnD the center ofan ε-cube in C(I). Assume that there are two cubes in
C (I)diag whose images by πe are equal. Let us denote the centers of two cubes by a and a0 .Then we
have πe(a) = πe(a0) and hence as,r = a0s,r holds for (s, r) 6= (σ-1(i), 1). Here, by our construction
of ε-cubes, a cube (in C(I)) intersect with B if and only if its center is on B . Therefore, since
two cubes are in C(I)diag, we have aσ-1(i),1 = aσ-1 (i+1),1 and a0σ-1(i),1 = a0σ-1(i+1),1. Hence
as,r = a0s,r holds for any (s, r) and two cubes are equal and πe is injective on C(I).
Next, let C0(I) be the set of ε-cubes in C(I) which intersect a boundary of σ ∙ ∆. We see that the
cardinality of C0(I) is bounded by Eε-n(D-1) for some E. Since the number of components of
the boundaries is finite, we prove the claim for a component of the boundary. Since peC(I)diag is
injective, we see the number of cubes which intersect the component is bounded by the number of
ε-cubes in C(p(I)), hence ε-n(D-1). Put C(I)inn = C(I) - C0(I). Then each cubes in C(I)inn
does not intersect the boundaries of σ ∙ ∆. Hence, there is a σ such that the number of cubes C(I)inn
which are contained in σ ∙ ∆ is lower than IC(I)IinnI. By adding the cubes which cover the boundaries
of σ ∙ ∆, we have the covering of σ ∙ ∆. Furthermore, by pulling back by σ , we have the covering
of ∆. Hence, we have
N(ε, ∆,k∙k∞) ≤ "I)|- E nD 1 + E0ε-n(DT).
Since |C(I)| = ε-nD, we have the desired result.
□
4.2	Proof for the Complexity-Control Bound (Theorem 2)
We utilize the results of ∆ and prove Theorem 2. The proof mainly contains the following two-step:
i) show that the covering number of FDSnNN is equal to that of FD∆NN, and ii) bound the covering
number of FD∆NN . The first step is provided by the following proposition.
Proposition 4. For any ε > 0, we obtain
logN(ε, FDnNN，k ∙ l∣L∞(I)) = logN(ε, FDNN, k ∙ l∣L∞(I)).
The result shows that the functional set by deep neural networks on I with invariance is well de-
scribed by a set of functions on ∆ without invariance. The key point of this result is that we can
describe the effect of invariance restriction on FDSnNN by the size of FD∆NN.
Proof of Proposition 4. For any f, f0 ∈ FDSnN N, there exists f∆, f0 ∈ F∆ by Proposition 2. Then,
we can obtain
lf - f0lL∞(I) = lf∆ ◦ g - f0∆ ◦ glL∞(I) ≤ lf∆ - f0∆ lL∞(∆).
Based on the result, we can bound logN(ε, FDNN, k ∙ Il∞(i)) by logN(ε, F∆, k ∙ ∣∣l∞(δ)).
Suppose logN(ε,F∆, k ∙ ∣∣l∞(δ)) =: K is finite. Then, there exist f]∆1,...,f[∆K, and for
any f∆ ∈ F∆, there exists j ∈ {1, ..., K} such as kf∆ - f∆jkL∞(∆) ≤ ε. Here, for
any f ∈ FDSnN N, there exists fj := f∆j ◦ g ∈ FDSnNN with corresponding j and it satisfies
kf - fj∣∣L∞(i) ≤ kf∖∆ 一 f∖∆j Il∞(∆) ≤ ε. Then, we obtain the statement.	口
8
Under review as a conference paper at ICLR 2020
The second step of this section is shown by the following proposition:
Proposition 5. With an existing constant c > 0 and C in Lemma 1, for any δ > 0, we obtain
log N (2C∆ δ, FDNN, k ∙ l∣L∞ (I) ) ≤
C
n! δP
+ log
2cB
Importantly, the result shows that the main term of the covering number is improved by n!, and it is
a key factor to improve the overall generalization error.
Proof of Poposition 5. We bound a covering number of a set of C∆-Lipschitz continuous functions
on ∆. Let {x1, ..., xK} ⊂ ∆ by a set of centers of δ-covering set for ∆. By Lemma 1, we set
K = C/(n! δp) with δ with a parameter δ > 0, where C > 0 is a constant.
We will define a set of vectors to bound the covering number. We define a discretization operator
A : F∆ → RK as
Af = (f (xι)∕δ,...,f (XK )∕δ)>.
Let Bδ (x) be a ball with radius δ in terms of the ∣∣ ∙ ∣∣∞-norm. For two functions f, f ∈ Fδ SUch as
Af = Af0, we obtain
∣f - f0∣L∞(I) =	max	sup	|f (x) -	f0(x)|
k=1,...,K x∈Bδ (xk)
≤	max	sup	|f (x) -	f(xk)|	+ |f0(xk) -	f(xk)|	≤ 2C∆δ,
k=1,...,K x∈Bδ (xk)
where the second inequality follows f(xk) = f0(xk) for all k = 1, ..., K and the last inequality
follows the C∆-Lipschitz continuity of f and f0. By the relation, we can claim that F∆ is covered
by 2C∆δ balls whose center is characterized by a vector b ∈ RK such as b = Af for f ∈ F∆ .
Namely, N (2C∆ δ, F∆, ∣∣∙ ∣∣l∞(i)) is bounded by a number of possible b.
Then, we construct an explicit set of b to cover F∆ . Without loss of generality, assume that
x1, ..., xK are ordered satisfies such as ∣xk - xk+1 ∣∞ ≤ 2δ for k = 1, ..., K - 1. By the def-
inition, f ∈	F∆	satisfies ∣f∣L∞(∆)	≤	B.	b1 =	f(x1)	can take values in	[-B∕δ, B∕δ]. For
b2 = f(x2), since ∣x1 - x2∣∞ ≤ 2δ and hence |f(x1) - f(x2)| ≤ 2C∆δ, a possible value for b2
is included in [(b1 - 2δ)∕δ, (b1 + 2δ)∕δ]. Hence, b2 can take a value from an interval with length 4
given b1. Recursively, given bk for k = 1, ..., K - 1, bk+1 can take a value in an interval with length
4.
Then, we consider a combination of the possible b. Simply, we obtain the number of vectors is
(2cB∕δ) ∙ (4c)K-1 with a universal constant C ≥ 1. Then, we obtain that
logN(2C∆δ,F∆,∣∙ ∣l∞) ≤ (K - 1)log4c + log(2cB∕δ).
Then, we specify K which describe a size of ∆ through the set of covering centers.	□
Proof of Theorem 2 and 3. For Theorem 3, we combine the result in Proposition 4 and 5. For The-
orem 2, we substitute the result in Theorem 3 into the well-known result (2), then obtain the state-
ment.	□
4.3	Proof for Approximation-Control Bound (Theorem 4)
Proof of the approximation power also depends on the correspondence mapping Λ in Proposition 2.
Although Proposition 2 claims that the correspondence holds fora function by deep neural networks,
the similar discussion in the proof shows that it holds for a general invariant function.
Proofof Theorem 4. Let f * be an invariant function on I. Then by Proposition 2, we have a function
f on ∆ such that f * = f ◦ Sort holds. By Theorem 5 in Schmidt-Hieber (2017), for enough big
N, there exists a constant c and a neural network g with at most O(log(N)) layers and at most
O(Nlog(N)) nonzero weights such that ∣f 一 g∣L∞(i) ≤ CN-a/p. Then, we have
Ilf * - g ◦ SortIlL∞(I) = kf ◦ SOrt- g ◦ SOrtlIL∞(I) = kf - g∣L∞(∆) ≤ kf - g∣L∞(I) ≤ CN-α∕p,
9
Under review as a conference paper at ICLR 2020
where g ◦ Sort is a neural network with at most O(log(N)) + Ki layers and at most O(N log(N)) +
K2 nonzero weights, where K1 and K2 are the number of layers and the number of nonzero weights
of the neural network expressing Sort respectively. By replacing N-1 with ε, we have the desired
inequality.	□
5	Discussion and Comparison
We discuss the technical non-triviality of our result. One can consider that our result seems straight-
forward, because the improvement by √n! looks a kind of folklore. However, there are several
technical difficulties to prove it. Rigorously speaking, to obtain the improved bound, we have to find
n! subsets of functions without overlapping with each other. To find them, we introduce the notion
of the fundamental domain (Definition 4) and prove that a volume of overlapping of the subsets has
measure zero (Lemma 1). To the best of knowledge, this is the first study to show the result.
We mention that Sokolic et al. (2016) investigates a generalization bound for classification with
invariant algorithms. Scope of the study is not limited to deep neural networks, but a wide class
of learning problems. The generalization bound by the study is improved by √T, where T is a
number of possible transformations for invariance. To discriminate this paper from the study by
Sokolic et al. (2016), we provide several differences between the study and this paper. Firstly, we
construct an explicit framework for invariant deep neural networks, which guarantees the practical
usage of several methods. Especially, the framework contains the famous network as the DeepSets
by Zaheer et al. (2017). Hence, our paper can provide useful knowledge for practical use, while the
study by Sokolic et al. (2016) investigates an abstract problem. Secondly, our analysis is not limited
to classifications, but can be applied to general learning methods including regression. Thirdly,
we focus on the more specific permutation invariance, and obtain the explicit improvement of the
generalization bound. The bound by Sokolic et al. (2016) considers a more abstract problem with a
wide class of invariance, hence it is not clear to obtain the same generalization bound of this paper.
6	Conclusion
In this paper, we develop a generalization theory to clarify the higher precision of the invariant deep
neural network. Our generalization bound shows that it gets much tight by the invariant property,
rigorously, the bound is improved by √n! where n is a number of permutation-invariant coordinates.
Intuitively, this is caused by fact that the input space is divided into n! copies of subspace which can
be moved by permutation to each other. We further prove that the invariant deep neural network with
a ReLU activation can achieve the optimal approximation rate for smooth functions. By the results,
our theory shows a great advantage of deep neural networks.
As an improvement of our result, it is an open question to connect the invariant property and the
norm-controlled entropy control for deep neural networks (e.g. the work by Bartlett et al. (2017)).
To describe the practical high accuracy of deep learning, numerous studies investigigate the the
norm-controlled entropy. We guess that our theory is valid with the entropy and more suitable to
analyze the performance of invariant deep neural networks.
References
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge
university press, 2009.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Felix A Faber, Alexander Lindmaa, O Anatole Von Lilienfeld, and Rickard Armiento. Machine
learning energies of 2 million elpasolite (a b c 2 d 6) crystals. Physical review letters, 117(13):
135502, 2016.
Jiaxin Li, Ben M Chen, and Gim Hee Lee. So-net: Self-organizing network for point cloud analysis.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9397-
9406, 2018a.
10
Under review as a conference paper at ICLR 2020
Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Con-
volution on x-transformed points. In Advances in Neural Information Processing Systems, pp.
820-830, 2018b.
Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant
networks. arXiv preprint arXiv:1901.09342, 2019.
Michelle Ntampaka, Hy Trac, Dougal J Sutherland, Sebastian Fromenteau, Barnabas Poczos, and
Jeff Schneider. Dynamical mass measurements of contaminated galaxy clusters using machine
learning. The Astrophysical Journal, 831(2):135, 2016.
Siamak Ravanbakhsh, Junier B Oliva, Sebastian Fromenteau, Layne Price, Shirley Ho, Jeff G
Schneider, and Barnabas Poczos. Estimating cosmological parameters from the dark matter dis-
tribution. In ICML, pp. 2407-2416, 2016.
Akiyoshi Sannai, Yuuki Takai, and Matthieu Cordonnier. Universal approximations of permutation
invariant/equivariant functions by deep neural networks. arXiv preprint arXiv:1903.01939, 2019.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation
function. arXiv preprint arXiv:1708.06633, 2017.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Generalization error of
invariant classifiers. arXiv preprint arXiv:1610.04574, 2016.
Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji, Evangelos Kalogerakis, Ming-Hsuan Yang,
and Jan Kautz. Splatnet: Sparse lattice networks for point cloud processing. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2530-2539, 2018.
Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. Spidercnn: Deep learning on point
sets with parameterized convolutional filters. In Proceedings of the European Conference on
Computer Vision (ECCV), pp. 87-102, 2018.
Yaoqing Yang, Chen Feng, Yiru Shen, and Dong Tian. Foldingnet: Point cloud auto-encoder via
deep grid deformation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 206-215, 2018.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:
103-114, 2017.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov,
and Alexander J Smola. Deep sets. In Advances in neural information processing systems, pp.
3391-3401, 2017.
11