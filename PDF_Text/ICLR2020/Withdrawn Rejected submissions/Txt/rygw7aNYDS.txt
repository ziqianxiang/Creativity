Under review as a conference paper at ICLR 2020
Efficient Inference and Exploration for Rein-
forcement Learning
Anonymous authors
Paper under double-blind review
Abstract
Despite an ever growing literature on reinforcement learning algorithms and ap-
plications, much less is known about their statistical inference. In this paper, we
investigate the large-sample behaviors of the Q-value estimates with closed-form
characterizations of the asymptotic variances. This allows us to efficiently construct
confidence regions for Q-value and optimal value functions, and to develop policies
to minimize their estimation errors. This also leads to a policy exploration strategy
that relies on estimating the relative discrepancies among the Q estimates. Numeri-
cal experiments show superior performances of our exploration strategy than other
benchmark approaches.
1	Introduction
We consider the classical reinforcement learning (RL) problem where the agent interacts with a
random environment and aims to maximize the accumulated discounted reward over time. The
environment is formulated as a Markov decision process (MDP) and the agent is uncertain about the
true dynamics to start with. As the agent interacts with the environment, data about the system
dynamics are collected and the agent becomes increasingly confident about her decision. With finite
data, however, the potential reward from each decision is estimated with errors and the agent may
be led to a suboptimal decision. Our focus in this paper is on statistically efficient methodologies to
quantify these errors and uncertainties, and to demonstrate their use in obtaining better policies.
More precisely, we investigate the large-sample behaviors of estimated Q-value, optimal value
function, and their associated policies. Our results are in the form of asymptotic convergence to an
explicitly identified and computable Gaussian (or other) distribution, as the collected data sizes
increase. The motivation of our investigation is three-fold. First, these precise asymptotic statements
allow us to construct accurate confidence regions for quantities related to the optimal policy, and,
like classical statistical inference, they can assess the reliability of the current estimates with respect
to the data noises. Second, our results complement some finite-sample error bounds developed in
the literature (Kearns & Singh, 1998; Kakade, 2003; Munos & Szepesvari, 2008), by supplementing
a closed-form asymptotic variance that often shows up in the first-order terms in these bounds.
Our third and most important motivation is to design good exploration policies by directly using
our tight error estimates. Motivated by recent autonomous-driving and other applications (e.g.,
Kalashnikov et al. (2018)), we consider the pure exploration setting where an agent is first assigned
an initial period to collect as much experience as possible, and then, with the optimal policy
trained offline, starts deployment to gain reward. We propose an efficient strategy to explore
by optimizing the worst-case estimated relative discrepancy among the Q-values (ratio of mean
squared difference to variance), which provides a proxy for the probability of selecting the best
policy. Similar criteria have appeared in the so-called optimal computing budget allocation (OCBA)
1
Under review as a conference paper at ICLR 2020
procedure in simulation-based optimization (Chen & Lee, 2011) (a problem closely related to
best-arm identification (Audibert & Bubeck, 2010) in online learning). In this approach, one
divides computation (or observation) budget into stages in which one sequentially updates mean and
variance estimates, and optimizes next-stage budget allocations according to the worst-case relative
discrepancy criterion. Our proposed procedure, which we term Q-OCBA, follows this idea with a
crucial use of our Q-value estimates and randomized policies to achieve the optimal allocation. We
demonstrate how this idea consistently outperforms other benchmark exploration policies, both in
terms of the probability in selecting the best policy and generating the tightest confidence bounds
for value estimates at the end of the exploration period.
Regarding the problem of constructing tight error estimates in RL, the closest work to ours is Mannor
et al. (2004; 2007), which studies the bias and variance in value function estimates with a fixed policy.
Our technique resolves a main technical challenge in Mannor et al. (2004; 2007), which allows us to
substantially generalize their variance results to Q-values, optimal value functions and asymptotic
distributional statements. The derivation in Mannor et al. (2004; 2007) hinges on an expansion of
the value function in terms of the perturbation of the transition matrix, which (as pointed out by
the authors) is not easily extendable from a fixed-policy to the optimal value function. In contrast,
our results utilize an implicit function theorem applied to the Bellman equation that can be verified
to be sufficiently smooth. This idea turns out to allow us to obtain gradients for Q-values, translate
to the optimal value function, and furthermore generalize to similar results for constrained MDP and
approximate value iterations. We also relate our work to the line of studies on dynamic treatment
regimes (DTR) (Laber et al., 2014) applied commonly in medical decision-making, which focuses on
the statistical properties of polices on finite horizon (such as two-period). Our infinite-horizon results
on the optimal value and Q-value distinguishes our developments from the DTR literature. Moreover,
our result on the non-unique policy case can be demonstrated to correspond to the “non-regularity”
concept in DTR, where the true parameters are very close to the decision “boundaries" that switch
the optimal policy (motivated by situations of small treatment effects), thus making the obtained
policy highly sensitive to estimation noises.
In the rest of this paper, we first describe our MDP setup and notations (Section 2). Then we
present our results on large-sample behaviors (Section 3), demonstrate their use in exploration
strategies (Section 4), and finally substantiate our findings with experimental results (Section 5). In
the Appendix, we first present generalizations of our theoretical results to constrained MDP (A.1)
and problems using approximate value iteration (A.2). Then we include more numerical experiments
(B), followed by all the proofs (C).
2	Problem Setup
Consider an infinite horizon discounted reward MDP, M = (S, A, R, P, γ, ρ), where S is the state
space, A is the action space, R(s, a) denotes the random reward when the agent is in state s ∈ S
and selects action a ∈ A, P (s0 |s, a) is the probability of transitioning to state s0 in the next
epoch given current state s and taken action a, γ is the discount factor, and ρ is the initial state
distribution. The distribution of the reward R and the transition probability P are unknown
to the agent. We assume both S and A are finite sets. Without loss of generality, we denote
S = {1, 2, . . . , ms} and A = {1, 2, . . . , ma}. Finally, we make the following stochasticity assumption:
Assumption 1. R(s, a) has finite mean μR(s, a) and finite variance σR(s, a) ∀ S ∈ S, a ∈ A. For
any given S ∈ S and a ∈ A, R(s, a) and S0 〜 P(∙∣s, a) are all independent random variables.
A policy π is a mapping from each state s ∈ S to a probability measure over actions a ∈ A. Specifically,
We write π(a∣s) as the probability of taking action a when the agent is in state S and π(∙∣s) as the m°-
dimensional vector of action probabilities at state S. For convenience, we sometimes write π(S) as the
2
Under review as a conference paper at ICLR 2020
realized action given the current state is s. The value function associated with a policy π is defined
as Vπ(s) = E[>∞=o YtR(st,∏(st))∣so = s] With st+ι 〜P(.∣st,∏(st)). The expected value function,
under the initial distribution ρ, is denoted by χπ = Es ρ(s)Vπ(s). A policy π* is said to be optimal
if Vπ* (S) = max∏ V π(s) for all s ∈ S. For convenience, we denote V * = Vπ* and χ* = Ps ρ(s)V *(s).
The Q-value, denoted by Q(s,a), is defined as Q(s,a) = μκ(s,a)+ γE[V*(S0)∣s, a]. Correspondingly,
V* (s) = maxa Q(s, a) and the Bellman equation for Q takes the form
Q(s,a) = μκ(s,a) + YE ImaxQ(s0,a0)∣s,a],
(1)
for any (s,a) ∈ S ×A. Denoting the Bellman operator as TμR,p(∙), Q is a fixed point associated
with Tnr,p, i.e. Q = Tμκp>(Q).
For the most part of this paper we make the following assumption about Q:
Assumption 2. For any state s ∈ S, arg maxa∈A Q(s, a) is unique.
Under Assumption 2, the optimal policy π* is unique and deterministic. Let a* (s)
argmaxa∈A Q(s,a). Then π*(a∣s) = 1 (a = a*(s)), where 1(∙) denotes the indicator function.
We next introduce some statistical quantities arising from data. Suppose we have n ob-
servations (whose collection mechanism will be made precise later), which we denote as
{(st, at, rt(st, at), s0t(st, at)) : 1 ≤ t ≤ n}, where rt(st, at) is the realized reward at time t and
St(st, at) = st+ι. We define the sample mean μR,n and the sample variance σR 元 of the reward as
μR,n(3 * s = i,a = j)
&R,n(S = i,a = j)
El≤t≤n rt(St,a∕I(St = i,at = j)
Pl≤t≤n I(St = i,at = j)	,
Pl≤t≤n rt(st, at)21(st = i, at = j)
Pl≤t≤n I(St = i,at = j)
-μR,n(i,j)2.
Similarly, we define the empirical transition matrix Pn as
ʌ ,, 、
Pn (s = k|s = i,a = j)
1≤t≤n 1(St = i,at = j, S0t(St, at) = k)
1≤t≤n 1(st = i,at =j)
(2)
(3)
(4)
and its ms × ms sampling covariance matrix ΣPs,a (with one sample point of 1(st = s, at = a)) as
ΣPs,a(k1,k2)
P(k1 |s, a)(1 - P(k1 |s, a))	k1 = k2
-P(k1 |s, a)P(k2 |s, a) k1 6= k2.
, for 1 ≤ k1 ≤ ms , 1 ≤ k2 ≤ ms .
With the data, we construct our estimate of Q, called Qn which is the empirical fixed point of
∕T-	∙ z^ι	∕T-	∕A∖Z^1	1 ∙	1	1	∙,τV*∕∖	z^ι /	∖	1
TlRnPn,	i.e.	Qn	=	TIRnPn (Qn).	Correspondingly, we also write	Vn*(s)	= maxa∈A Qn(s,a)	and
,,
Xn = Ps∈S P(S)V*(s).
We shall focus on the empirical errors due to noises of the collected data, and assume the MDP
or Q-value evaluation can be done off-line so that the fixed point equation for Qn can be solved
exactly. .
3 Quantifying Asymptotic Estimation Errors
YXT	l	r	ι l	ι ∙	l ι	1 1 ∙ ι ι	「A	ITAr* m
We present an array of results regarding the asymptotic behaviors of Qn and Vn*. To prepare, we
first make an assumption on our exploration policy π to gather data. Define the extended transition
probability P as Pπ(s0,α0∣s,a) = P(s0∣s, a)π(a0∣s0). We make the assumption:
3
Under review as a conference paper at ICLR 2020
π
Assumption 3. The Markov chain with transition probability Pπ is positive recurrent.
Under Assumption 3, Pπ has a unique stationary distribution, denoted w, equal to the long run
frequency in visiting each state-action pair, i.e. w(s, a) = limn→∞ 1 J2ι≤t≤n 1(st = i, at = j), where
all w(s, a)’s are positive. Note that Assumption 3 is satisfied if for any two states s, s0, there exists
a sequence of actions such that s0 is attainable from s under P , and, moreover, if π is sufficiently
mixed, e.g., π satisfies π(a0∣s0) > 0 for all s0,a0.
Our results in the sequel uses the following further notations. We denote “⇒” as “convergence in
distribution"，and N(μ, Σ) as a multivariate Gaussian distribution with mean vector μ and covariance
matrix Σ. We write I as the identity matrix, and ei as the i-th unit vector. The dimension of
N(μ, Σ), I and e% should be clear from the context. When not specified, all the vectors are column
vectors. Let N = m§ma. In our algebraic derivations, we need to re-arrange μR, Q and W as
N -dimensional vectors. We thus define the following indexing rule: (s = i, a = j) is re-indexed as
(i 一 1)ma + j, e.g. μκ(i,j) = μκ((i — 1)m0 + j). We also need to re-arrange PK as an N X N matrix
„ __ . - . . - . ~ ................................. ~ ,,. .、 .,.. .、
following the same indexing rule, i.e. Pn(i0,j0∣i,j) = P ((i — 1)m0 + j, (i — 1)m0 + j0).
3.1 Limit Theorems under Sufficient Exploration
We first establish the asymptotic normality of Qn under exploration policy π:
Theorem 1. Under Assumptions 1 and 2, if the data is collected according to π satisfying Assumption
3, then Qn is a strongly consistent estimator of Q, i.e. Qn → Q almost surely as n → ∞. Moreover,
√n(Qn — Q) ⇒ N(0, Σ) as n → ∞,
where
Σ = (I — γPπ* )-1W T(DR + DQ)((I — γPπ* )-1)t ,	(5)
W, DR and DQ are N × N diagonal matrices with
W((i — 1)ma +j, (i — 1)ma +j) = w(i, j), DR((i — 1)ma +j, (i — 1)ma +j) = σR2 (i, j)
and Dq((i — 1)m0 + j, (i — 1)m0 + j) = (V*)T∑Pi,j∙V* respectively .
In addition to the asymptotic Gaussian behavior, a key element of Theorem 1 is the explicit form of
the asymptotic variance Σ. This is derived from the delta method (Serfling, 2009) and, intuitively, is
the product of the sensitivities (i.e., gradient) of Q with respect to its parameters and the variances
of the parameter estimates. Here the parameters are μR and P, with corresponding gradients
(I — YPKI-I and (I — YPπ* )-1V*. The variances of these parameter estimates (i.e., (2) and (4))
involve σR2 (i, j) and ΣPi,j , and the sample size allocated to estimate each parameter, which is
proportional to w(i, j).
Using the relations that Vn* (s) = maxa∈A Q(s, a) and Vn* (s) = maxa∈A Qn (s, a), we can leverage
**
Theorem 1 to further establish the asymptotic normality of Vn and χ^n:
Corollary 1. Under Assumptions 1, 2 and 3,
√n(V* — V*) ⇒ N(0, ∑v) and √n(^^n — χ*) ⇒ N(0, σX) as n → ∞
where
∑V = (I — γPπ* )-1(Wπ* )-1[DR* + DV* ]((I — γPπ* )-1)t ,
σχ2 = ρTΣVρ, PK* is an ms × ms transition matrix with PK* (i, j) = P(j|s = i, a = a*(s)), WK* ,
DRK* and DVK* are ms × ms diagonal matrices with WK* (i, i) = w(i, a* (i)), DRK* (i, i) = σR2 (i, a* (i))
and DVK* (i, i) = (V*)TΣPi,a*(i) V* respectively.
4
Under review as a conference paper at ICLR 2020
In the Appendix we also prove, using the same technique as above, a result on the large-sample
behavior of the value function for a fixed policy (Corollary 2), which essentially recovers Corollary
4.1 in Mannor et al. (2007). Different from Mannor et al. (2007), we derive our results by using
an implicit function theorem on the corresponding Bellman equation to obtain the gradient of Q,
viewing the latter as the solution to the equation and as a function of μR, P. This approach is able
to generalize the results for fixed policies in Mannor et al. (2007) to the optimal value functions,
and also provide distributional statements as Theorem 1 and Corollary 1 above. We also note
that another potential route to obtain our results is to conduct perturbation analysis on the linear
program (LP) representation of the MDP, which would also give gradient information of V * (and
hence also Q), but using the implicit function theorem here seems sufficient.
Theorem 1 and Corollary 1 can be used immediately for statistical inference. In particular, we can
construct confidence regions for subsets of the Q-value jointly, or for linear combinations of the
Q-values. A quantity of interest that we will later utilize in designing good exploration policies is
Q(s, a1) - Q(s, a2), i.e. the difference between action a1 and a2 when the agent is in state s. Define
σ∆2 Q as
σ∆Q(s, a1, a2) = (e(s-1)ma+a1 - e(s-1)ma +a2) Σ(e(s-1)ma+a1 - e(s-1)ma+a2)	(6)
and its estimator σ∆Q,n by replacing Q, V*, σR,n, w, P with Qn,V* σR,n, Wn, Pn in Σ, where
Wn is the empirical frequency of visiting each state-action pair, i.e. Wn(i,j) = n ∑2ι≤t≤n 1(st =
i, at = j). Then the 100(1 - α)% confidence interval (CI) for Q(s, a1) - Q(s, a2) takes the form
(Qn(s, aι) — Qn(s, &2)) ± Za?>Q n(s, 01, 02), where Za is the (1 — a∕2)-quantile of N(0,1).
3.2 Non-Unique Optimal Policy
Suppose the optimal policy for the MDP M is not unique, i.e., Assumption 2 does not hold. In this
situation, the estimated Qn and Vn* may “jump” around different optimal actions, leading to a more
complicated large-sample behavior as described below:
Theorem 2. Suppose Assumptions 1 and 3 hold but there is no unique optimal policy. Then there
exists K ≥ 1 distinct ms × (Nms +N) matrices {Gk}1≤k≤K and a deterministic partition of U = {u ∈
RmsN+ms : ||u|| = 1} = ∪ι≤k≤κUk such that √n(Vn* - V*) ⇒ PK=I Gk 1 (Z∕∣∣Z∣∣ ∈ Uk) Z, where
Z = N(0, ΣR,P), ΣR,P = Diag(W -1DR, DP) and DP = Diag(ΣP1,1 /W(0ma + 1), . . . , ΣPi,j /W((i -
1)ma +j), . . ., ΣPms,ma ∕W((ms - 1)ma +ma)).
In the case that K > 1 in Theorem 2, the limit distribution becomes non-Gaussian. This arises
because the sensitivity to P or μR can be very different depending on the perturbation direction,
which is a consequence of solution non-uniqueness that can be formalized as a non-degeneracy in the
LP representation of the MDP. We note that this phenomenon is analogous to the “non-regularity”
concept in DTR that arises because the “true” parameters in these problems are very close to the
decision “boundaries”, which makes the obtained policy highly sensitive to estimation noises and
incurs a 1∕√n-order bias behavior. Our case of non-unique optimal policy here captures precisely
this same behavior, where we see in Theorem 2 that when K > 1 the asymptotic limit no longer has
mean zero and consequently a 1∕√n-order bias arises.
We also develop two other generalizations of large-sample results, for constrained MDP and approxi-
mate value iteration respectively (see Appendices A.1 and A.2).
5
Under review as a conference paper at ICLR 2020
4 Efficient Exploration Policy
We utilize our results in Section 3 to design exploration policies. We focus on the setting where an
agent is assigned a period to collect data by running the state transition with an exploration policy.
The goal is to obtain the best policy at the end of the period in a probabilistic sense, i.e., minimize
the probability of selecting a suboptimal policy for the accumulated reward.
We propose a strategy that maximizes the worst-case relative discrepancy among all Q-value
estimates. More precisely, We define, for i ∈ S, j ∈ A and j = a*(i), the relative discrepancy as
hij = (Q(i,a*(i)) - Q(i,j))2 /σ∆Q(i,a*(i),j),
where σ∆Q(i,a*(i),j) is defined in (6). Our procedure attempts to maximize the minimum of hj's,
max min min hij ,
w∈Wη i∈S j∈A,j=a*(i) J
(7)
where w denotes the proportions of visits on the state-action pairs, within some allocation set Wη
(which we will explain). Intuitively, hij captures the relative “difficulty” in obtaining the optimal
policy given the estimation errors of Q’s. If the Q-values are far apart, or if the estimation variance is
small, then hij is large which signifies an “easy” problem, and vice versa. Criterion (7) thus aims to
make the problem the “easiest”. Alternatively, one can also interpret (7) from a large deviations view
(Glynn & Juneja, 2004; Dong & Zhu, 2016). Suppose the Q-values for state i between two different
actions a*(i) and j are very close. Then, one can show that the probability of suboptimal selection
between the two has roughly an exponential decay rate controlled by hij . Obviously, there can be
many more comparisons to consider, but the exponential form dictates that the smallest decay rate
dominates the calculation, thus leading to the inner min’s in (7). Criterion like (7) is motivated
from the OCBA procedure in simulation optimization (which historically has considered simple
mean-value alternatives (Chen & Lee, 2011)). Here, we consider the Q-values. For convenience, we
call our procedure Q-OCBA.
Implementing criterion (7) requires two additional considerations. First, solving (7) needs the model
primitives Q, P and σR2 that appear in the expression of hij . These quantities are unknown a priori,
but as we collect data they can be sequentially estimated. This leads to a multi-stage optimization
plus parameter update scheme. Second, since data are collected through running a Markov chain on
the exploration actions, not all allocation w is admissible, i.e., realizable as the stationary distribution
of the MDP. To resolve this latter issue, we will derive a convenient characterization for admissibility.
Call ∏(∙∣s) admissible if the Markov Chain with transition probability Pπ, defined for Assump-
tion 3, is positive recurrent, and denote wπ as its stationary distribution. Define the set
W= nw > 0 : P1≤j≤ma w((i - 1)ma +j) = P1≤k≤ms P1≤l≤ma w((k - 1)ma + l)P (i|k, l)
∀1 ≤ i ≤ ms, P1≤i≤m P1≤j≤m w((i - 1)ma + j) = 1 . The following provides a characteriza-
tion of the set of admissible π:
Lemma 1. For any admission policy π, wπ ∈ W. For any w ∈ W, πw with πw(a = j|s = i) =
w((i - 1)ma + j)/ ( km=a1 w((i - 1)ma + k)) is an admissible policy.
In other words, optimizing over the set of admissible policies is equivalent to optimizing over the set
of stationary distributions. The latter is much more tractable thanks to the linear structure of W .
In practice, we will use Wη = W ∩ {w ≥ η} for some small η > 0 to ensure closedness of the set
(our experiments use η = 10-6).
Algorithm 1 describes Q-OCBA. In our experiments shown next, we simply use two stages, i.e.,
K = 2. Finally, we also note that criterion like (7) can be modified according to the decision goal.
6
Under review as a conference paper at ICLR 2020
For example, if one is interested in obtaining the best estimate of χ*, then it would be more beneficial
to consider minw∈Wη σχ2 . We showcase this with additional experiments in the Appendix.
Input: Number of iterations K, length of each batch {Bk}1≤k≤K, initial exploration policy π0;
Initialization: k = 0;
while k ≤ K do
Run πk for Bk steps and set k = k + 1;
2
Calculate PBk, μRBk ,σR Bk and WBk based on the Bk data points collected ;
2
Apply value-iteration using PBk and μR Bk to obtain QBk ;
2
Plug the estimates PBk, σR Bk and QBk into ⑺ to solve for the optimal Wk ;
Set πk(a = j|s = i) = wk((i - 1)ma + j)/ Plm=a1 wk((i - 1)ma + l);
end
Algorithm 1: Q-OCBA sequential updating rule for exploration
Note that (7) is equivalent to minw maxi∈s maxj∈A,j=a*(i) Es a Cij (s, a)/ws,a subject to W ∈ Wn,
where cij (s, a)’s are non-negative coefficients. Based on the closed-form characterization of Σ in
Theorem 1, cij (s, a)’s can be estimated with plug-in estimators using data collected in earlier stages.
5 Numerical Experiments
We conduct several numerical experiments to support our large-sample results in Sections 3 and
demonstrate the performance of Q-OCBA against some benchmark methods. We use the RiverSwim
problem in (Osband et al., 2013) with ms states and two actions at each state: swim left (0) or
swim right (1) (see Figure 1). The triplet above each arc represents i) the action, 0 or 1, ii) the
transition probability to the next state given the current state and action, iii) the reward under the
current state and action. Note that, in this problem, rewards are given only at the left and right
boundary states (where the value of rL will be varied). We consider the infinite horizon setting with
γ = 0.95 and ρ = [1/ms, . . . , 1/ms]T.
(1,0.7,0)	(1,0.6,0)	(1,0.6,0)	(1,0.6,0)	(1,0.6,0)	(1,0.3,10)
Figure 1: RiverSwim Problem
We first demonstrate the validity of our large-sample results. We use a policy that swims right
with probability 0.8 at each state, i.e. π(1∣s) = 0.8. Tables 1 and 2 show the coverage rates of
the constructed 95% CIs, for a small ms = 6 (using Theorem 1 and Corollary 1) and a large
ms = 31 (using Theorem 4 in the Appendix) respectively. The latter case uses a linear interpolation
with S0 = {1, 4, . . . , 28, 31}. All coverage rates are estimated using 103 independent experimental
repetitions (the bracketed numbers in the tables show the half-widths of 95% CI for the coverage
estimates). For the Q-values, we report the average coverage rate over all (s, a) pairs. When the
number of observations n is large enough (≥ 3 × 104 for exact update and ≥ 105 for interpolation),
we see highly accurate CI coverages, i.e., close to 95%.
7
Under review as a conference paper at ICLR 2020
Table 2: Approximate value iteration
Table 1: Exact tabular update
n	104	3 × 104	5 × 104	n	104	10	106
Q	0.77(0.03)	0.93(0.02)	0.96(0.01)	~Q~	0.53(0.02)	0.95(0.01)	0.95(0.01)
Z7^ χπ	0.77(0.03)	0.93(0.02)	0.96(0.01)	Z7^ χπ	0.80(0.03)	0.94(0.02)	0.95(0.01)
Next we investigate the efficiency of our exploration policy. We compare Q-OCBA with K = 2 to
four benchmark policies: i) -greedy with different values of , ii) random exploration (RE) with
different values of π(1∣s), iii) UCRL2 (a variant of UCRL) with δ = 0.05 (Jaksch et al., 2010), iv)
PSRL with different posterior updating frequencies (Osband et al., 2013), i.e., PSRL(x) means
PSRL is implemented with x episodes. We use ms = 6 and vary rL from 1 to 3. To ensure fairness,
we use a two-stage implementation for all policies, with 30% of iterations first dedicated to RE (with
π(1∣s) = 0.6) as a warm start, i.e., the data are used to estimate the parameters needed for the
second stage. To give enough benefit of the doubt, we notice the probabilities of correct selection for
both UCRL2 and PSRL are much worse without the warm start.
Tables 3 and 4 compare the probabilities of obtaining the optimal policy (based on the estimated
Qn's). For e-greedy, RE, and PSRL, we report the results with the parameters that give the best
performances in our numerical experiments. The probability of correct selection is estimated using
103 replications of the procedure. We observe that Q-OCBA substantially outperforms the other
methods, both with a small data size (n = 103 in Table 3) and a larger one (n = 104 in Table
4). Generally, these benchmark policies perform worse for larger values of rL . This is because for
small rL, the (s, a) pairs that need to be explored more also tend to have larger Q-values. However,
as rL increase, there is a misalignment between the Q-values and the (s, a) pairs that need more
exploration.
The superiority of our Q-OCBA in these experiments come as no surprise to us. The benchmark
methods like UCRL2 and PSRL are designed to minimize regret which involves balancing the
exploration-exploitation trade-off. On the other hand, Q-OCBA focuses on efficient exploration
only, i.e., our goal is to minimize the probability of incorrect policy selection, and this is achieved by
carefully utilizing the variance information gathered from the first stage that is made possible by
our derived asymptotic formulas. We provide additional numerical results in Appendix B.
Table 3: Probability of correct selection for different exploration policies, n = 103
rL	0.2-greedy	RE(0.6)	UCRL2	PSRL(100)	Q-OCBA
1	0.95(0.01)’	0.70(0.03)	0.44(0.03)	0.53(0.03)=	0.87(0.02)’
2	0.15(0.02)	0.29(0.03)	0.11(0.02)	0.33(0.03)	0.55(0.03)
3	0.00(0.00)	0.45(0.03)	0.21(0.02)	0.41(0.03)-	0.84(0.02).
Table 4: Probability of correct selection for different exploration policies, n = 104
rL	0.2-greedy	RE(0.6)	UCRL2	PSRL(100)	Q-OCBA
1	1.00(0.00)’	0.95(0.01)	0.82(0.02)	1.00(0.00)=	1.00(0.00)’
2	0.55(0.03)	0.80(0.03)	0.52(0.03)	0.94(0.02)	1.00(0.00)
3	0.21(0.03)	0.94(0.01)	0.75(0.03)	0.76(0.03)-	1.00(0.00).
8
Under review as a conference paper at ICLR 2020
References
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 22-31.
JMLR. org, 2017.
Eitan Altman. Constrained Markov Decision Processes, volume 7. CRC Press, 1999.
Jean-Yves Audibert and Sebastien Bubeck. Best arm identification in multi-armed bandits. In
COLT-23th Conference on learning theory-2010, pp. 13-p, 2010.
Craig Boutilier and Tyler Lu. Budget allocation using weakly coupled, constrained markov decision
processes. In Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intel ligence,
pp. 52-61. AUAI Press, 2016.
Chun-hung Chen and Loo Hay Lee. Stochastic Simulation Optimization: An Optimal Computing
Budget Al location, volume 1. World scientific, 2011.
Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained
reinforcement learning with percentile risk criteria. The Journal of Machine Learning Research,
18(1):6070-6120, 2017.
Jing Dong and Yi Zhu. Three asymptotic regimes for ranking and selection with general sample
distributions. In Proceedings of the 2016 Winter Simulation Conference, pp. 277-288. IEEE Press,
2016.
Eugene A Feinberg and Uriel G Rothblum. Splitting randomized stationary policies in total-reward
markov decision processes. Mathematics of Operations Research, 37(1):129-153, 2012.
Peter Glynn and Sandeep Juneja. A large deviations perspective on ordinal optimization. In Pro-
ceedings of the 36th conference on Winter Simulation Conference, pp. 577-585. Winter Simulation
Conference, 2004.
Geoffrey J Gordon. Stable function approximation in dynamic programming. In Machine Learning
Proceedings 1995, pp. 261-268. Elsevier, 1995.
T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning. Journal
of Machine Learning Research, 11(Apr):1563-1600, 2010.
S. M. Kakade. On the sample complexity of reinforcement learning. PhD Thesis, University College
London, 2003.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Qt-opt: Scalable deep
reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293,
2018.
M. Kearns and S. Singh. Finite-sample convergence rates for Q-learning and indirect algorithms.
In Proceedings of the conference on Advances in neural information processing systems II, pp.
996-1002, 1998.
Eric B Laber, Daniel J Lizotte, Min Qian, William E Pelham, and Susan A Murphy. Dynamic
treatment regimes: Technical challenges and applications. Electronic journal of statistics, 8(1):
1225, 2014.
9
Under review as a conference paper at ICLR 2020
Shie Mannor, Duncan Simester, Peng Sun, and John N Tsitsiklis. Bias and variance in value function
estimation. In Proceedings of the twenty-first international conference on Machine learning, pp.
72. ACM, 2004.
Shie Mannor, Duncan Simester, Peng Sun, and John N Tsitsiklis. Bias and variance approximation
in value function estimates. Management Science, 53(2):308-322, 2007.
Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. Journal of Machine
Learning Research, 9(May):815-857, 2008.
Ian Osband, Daniel Russo, and Benjamin Van Roy. (More) efficient reinforcement learning via
posterior sampling. In Advances in Neural Information Processing Systems, pp. 3003-3011, 2013.
Robert J Serfling. Approximation Theorems of Mathematical Statistics, volume 162. John Wiley &
Sons, 2009.
A Additional Theoretical Results
In this section, we present additional results on large-sample behaviors for constrained MDPs and
also estimations based on approximation value iteration.
A.1 Constrained Problems
We consider the constrained MDP setting for budgeted decision-making (Boutilier & Lu, 2016) and
more recently safety-critical applications (Achiam et al., 2017; Chow et al., 2017). Suppose now we aim
to maximize the long-run accumulated discounted reward, Vπ(S) = E[£∞=0 YtR(St,π(st))∣s0 = s],
while at the same time want to ensure that a long-run accumulated discounted cost, denoted as
Lπ (S) = E[ t∞=0 γtC(St, π (St))|S0 = S] which we call the loss function, is constrained by some given
value η, i.e.,
max X ρ(S)Vπ (S) sub ject to X ρ(S)Lπ (S) ≤ η	(8)
ss
We assume data coming in like before and, in addition, that we have observations on the incurred
cost at each sample of (s, a). Call the empirical estimate of the cost μc,n. We follow our paradigm
to solve the empirical counterpart of the problem, namely to find a policy ∏n that solves (8) by using
V∏(s) and Ln(S) instead of Vπ(s) and Ln(s), where V∏(SyS and Ln(SyS are the value functions and
loss functions evaluated using the empirical estimates ∕^R,n, μc,n,P^n∙ We focus on the estimation
error of the optimal value (instead of the feasibility, which could also be important but not pursued
here).
To understand the error, we first utilize an optimality characterization of constrained MDPs. In
general, an optimal policy for (8) is a “split” policy (Feinberg & Rothblum, 2012), namely, a policy
that is deterministic except that at one particular state a randomization between two different
actions is allowed. This characterization can be deduced from the associated LP using occupancy
measures (Altman, 1999). We call the randomization probability the mixing parameter ɑ*, i.e.,
whenever this particular state, say Sr, is visited, action a1(Sr) is chosen with probability a* and
action a2(Sr) is chosen with probability 1 一 α*. We then have the following result:
Theorem 3.	Suppose Assumptions 1 and 3 hold and there is a unique optimal policy. Moreover,
assume that there is no deterministic policy π that satisfies s ρ(S)Lπ (S) = η. Then we have
!/W*	T7*∖
√n(Vnc 一 V*) ⇒ N(0, Σ) as n → ∞, where one of the following cases hold:
10
Under review as a conference paper at ICLR 2020
1.	The optimal policy is deterministic. We then have Σ = ΣV where ΣV is defined in Theorem 1.
2.	The optimal policy is deterministic, except at one state where a randomization between two
actions occurs, with the mixing parameter α*. Denote the state where the randomization occurs by
Sr and two possible actions for Sr by a； (Sr) and a； (Sr), We have
∑=	(I-YPn)-1 [Gπ*,0,HV*]-
(I - YPπ* 尸 qvpT(I - YPπ* )-1[0,Gπ*,HL*]
P(I - YPπ*)-1qL
ΣR,C,P (I-YPπ*)-1[Gπ*,0,HVπ*] -
(I - YPπ*)-1qvPT(I - YPπ*)-1[0,Gπ*,HL*]
P(I - YPπ*)-1qL
where ΣR,C,P = Diag(W -1DR, W-1DC, DP), qV and qL are ms-dimensional vectors with qV (S) =
3r(s, a1(S))-μκ(s, a；(S))+Pm=S1 YVπ* Cj)(PCjls, a；(S))-P(j∣s, a；(S))) and qL(S) = (μc(s, a；(s))-
μc (S,a2(S))+ pm=s1 YLπ* (j)(P (j∣S, a；(S))- P (j∣S, a；(S))) when S = Sr, and qv (s) = 9l(s) = 0 when
S 6= Sr ,
仔(∙∣i)T
rπ* =
∖
where qVπ* (i)T = Y[π ；(1li)(V π* )T, . . . ,π ； (j li)(Vπ* )T, . . . ,π ；(mali)(V π* )T], which is an N-
dimensional row vector, and HL and q∏*(i) are defined similarly by substituting V with L.
Case 1 in Theorem 3 corresponds to the case where the constraint in (8) is non-binding. This
effectively reduces to the unconstrained scenario in Theorem 1 since a small perturbation of μR,μc, P
does not affect feasibility. Case 2 is when the constraint is binding. In this case, a； must be chosen
such that the split policy ensures equality in the constraint, and when μR,μc, P is perturbed the
estimated On would adjust accordingly. Thus, in this case the estimation of V； incurs two sources
of noises, one from the uncertainty of R, P that appears also in unconstrained problems, and one
from the uncertainty in calibrating c^n that is in turn affected by C, P, thus leading to the extra
terms in the variance expression.
A.2 Approximate Value Iteration
When the state space S is large, updating an m§ X m° look-up table via TμR,p(.) can be computation-
ally infeasible. Approximate value iteration operates by applying a mapping M over TμR,p. In many
cases, M = Mg ◦ MIS0 where MIS0 is a dimension-reducing “inherit” mapping Rmsma → Rms0 ma ,
and Mg is the “generalization” mapping Rms0 ma → Rms ma that lifts back to the full dimension. By
selecting a “representative” subset S0 ⊂ S with cardinality lS0 l = ms0 ms , MIS0 is defined as
MIS0 (x) = [x(i, j)]i∈S0,1≤j≤ma where [xi]i∈I denotes the set of entries of x whose index i ∈ I. In this
setup, we define QM as a fixed point of the operator M ◦ Tμκ,p(∙), and VM(s) = max。QM(s, a).
We also define QM = MS0 oTμR,p(QM) as the dimension-reduced Q-value.
We derive large-sample error estimates in this case. For this, we first assume there is a well-
defined metric on S. To guarantee the existence of QM , we make the following assumption on the
generalization map Mg :
Assumption 4. Mg is a max-norm non-expansion mapping in S, i.e., llMg (x) - Mg (y)ll∞ <
llx - yll∞ ∀x, y ∈ S.
We also need the following analogs of Assumptions 2 and 3 to QM and S0 :
11
Under review as a conference paper at ICLR 2020
Assumption 5. For any state s ∈ S, arg maxa∈A QM (s, a) is unique.
π
Assumption 6. For the Markov Chain with transition probability Pπ, the set of states {(s, a) : s ∈
S0 , a ∈ A} is in the same communication class and this class is positive recurrent.
Let No = msom° and Is。= {(i - 1)m° + j : i ∈ So,a ∈ A}. With Assumption 6, We denote PM
as a sub-matrix of the matrix P π that only contains roWs With indexes in IS0 . We also denote
So(i) as the i-th element (state) in So. We define QM as the empirical estimator of Qm built on n
observations. Then We have:
M	M	i	MHf ■ I ■	M ■ M M M ι M IM M
Theorem 4.	Under Assumption 4, 5 and 6, if Mg is continuously differentiable, then √n(QM —
QM ) ⇒ N (0, ΣSM ) as n → ∞, where
∑M = (I-YVMg (QMo )PM TRMg (QMo )(W S0 )-1[dR0 + DQ ]
VMg (QM )t ((I-Y VMg (QMo )Pm )-1)t ,
VMg is the Jacobian of mapping Mg, WSo , DRSo , DQSo are No × No diagonal matrices with WSo ((i -
1)ma +j, (i - 1)ma +j) = w(So(i), j), DRSo ((i - 1)ma +j, (i - 1)ma +j) = σR2 (So(i),j), DQSo ((i -
1)ma+j,(i-1)ma+j)=(VM)TΣPSo(i),jVM.
Assumption 4 is generally satisfied by “local” approximation methods such as linear interpolation,
k-nearest neighbors and local Weighted average (Gordon, 1995). In all these cases, VMg in Theorem
4 is actually a constant matrix.
B Additional Numerical Results
This section reports additional numerical experiments. Sections B.1 present further results on the
estimation quality of Q-values, V*and χ*. Section B.2 provides additional results to demonstrate
the efficiency of our proposed exploration strategy.
B.1 Statistical Quality of Interval Estimators
In this section, We provide additional numerical results about Tables 1 and 2 in the main paper.
For Q-values in Table 1 of main paper, We only report the average coverage rate over all (s, a)
pairs. Table 5 presents the coverage rates for each individual Q-values. It also provides the coverage
rates for the value functions and χ*, where χ* is the uniformly initialized value function, i.e.
P = [1/ms,..., 1/m§]t. We use RE with π(1∣s) = 0.5 as our exploration policy. We see that the
behaviors of these individual estimates are largely consistent. The coverage rates all converge to the
nominal 95% as the number of observations n increases. Moreover, the coverages for the individual
Q's, V*,s, and the averages of these quantities are similar at any given n. Specifically, when n = 104,
the coverages are all around 77% - 78%, when n = 3 × 104 they are all around 93%, and when
n = 5 × 104 they are all very close to 95%. These suggest a sample size of 5 × 104 (or lower) is
enough to elicit our asymptotic results in Theorem 1 and Corollary 1 in this problem.
12
Under review as a conference paper at ICLR 2020
Table 5: Coverage for Q(s,a), V * and χ* values using exact tabular update
n	104	-3 × 104~	-5 × 104
Q(1, 0)	0.773(0.026)	0.930(0.016)	0.956(0.013)
Q(1, 1)	0.773(0.026)	0.930(0.016)	0.956(0.013)
Q(2, 0)	0.773(0.026)	0.930(0.016)	0.956(0.013)
Q(2, 1)	0.771(0.026)	0.930(0.016)	0.955(0.013)
Q(3, 0)	0.771(0.026)	0.930(0.016)	0.955(0.013)
Q(3, 1)	0.771(0.026)	0.929(0.016)	0.957(0.013)
Q(4, 0)	0.771(0.026)	0.929(0.016)	0.957(0.012)
Q(4, 1)	0.775(0.026)	0.934(0.015)	0.958(0.012)
Q(5, 0)	0.775(0.026)	0.934(0.015)	0.958(0.012)
Q(5, 1)	0.775(0.026)	0.935(0.015)	0.954(0.013)
Q(6, 0)	0.775(0.026)	0.935(0.015)	0.954(0.013)
Q(6, 1)	0.750(0.026)	0.920(0.017)	0.950(0.014)
Average of Q	0.771(0.026)	0.931(0.016)	0.956(0.013)
V*(1)	0.773(0.026)	0.930(0.016)	0.956(0.013)
V*(2)	0.772(0.026)	0.930(0.016)	0.955(0.013)
V*(3)	0.774(0.026)	0.929(0.016)	0.957(0.013)
V* (4)	0.776(0.026)	0.934(0.015)	0.958(0.012)
V*(5)	0.772(0.026)	0.935(0.015)	0.954(0.013)
V*(6)	0.752(0.027)	0.920(0.017)	0.950(0.014)
Average of V *	0.769(0.026)	0.929(0.016)	0.955(0.012)
* χ*	0.772(0.026厂	0.933(0.015)-	0.958(0.012)~
Tables 6, 7 and 8 compare the CI coverage rates when the state space is large, i.e. ms = 31, using
RE with different values of π(1∣s), i.e., π(1∣s) = 0.8,0.85, and 0.9. Compared to exact update, the
coverage convergence for approximate update appears generally slower. Specifically, comparing
Tables 5 and 6 that use the same RE with π(1∣s) = 0.8, We see that the coverages on the averages
of Q’s and V*’s for approximate update are only around 23% - 25% when n = 104, whereas they
are 77% - 78% for exact update. Also, while the nominal coverage, i.e., 95%, is obtained when
n = 5 × 104 in the exact update for all studied quantities, this sample size is not enough for
approximate update, where it appears we need n to be of order 107 to obtain the nominal coverage.
Furthermore, Tables 6, 7 and 8 show that the rates of convergence to the nominal coverage are
quite different for different values of n(1|s)'s. The convergence rate when π(1∣s) = 0.85 seems to
be the fastest, with the coverage close to 95% already when n = 105. On the other hand, when
π(1∣s) = 0.8, the coverage is close to 95% only when n is as large as 107, and when π(1∣s) = 0.9,
even n = 107 is not enough for convergence to kick in. We also see that, when the coverage is very
far from the nominal rate, discrepancies can show up among the estimates of Q, V* and χ*. For
example, when π(1∣s) = 0.8 and n = 104, the coverages of Q and V * are around 23% — 25% but the
coverage of χ* is as low as 1%, and when π(1∣s) = 0.9 and n = 107, the coverages of Q and V * are
around 75% — 77% but that of χ* is only 29%. However, in settings where the coverage is close
to 95%, all these quantities appear to attain this accuracy simultaneously in all considered cases.
These caution that coverage accuracy can be quite sensitive to the specifications of the exploration
policy. Nonetheless, the convergence behaviors predicted by Theorem 1, Corollary 1 and Theorem 4
are all observed to hold.
13
Under review as a conference paper at ICLR 2020
Table 6: Linear interpolation in approximate value iteration with π(1∣s) = 0.8
n	104	10	10	10
Average of Q	0.23(0.01)	0.55(0.03)	0.92(0.02)	0.95(0.01)
Average of V *	0.25(0.0i)	0.56(0.02)	0.93(0.0i)	0.95(0.01)
χ* coverage	0.0i(0.0i)	0.35(0.03)	0.94(0.02)	0.94(0.02).
Table 7: Linear interpolation in approximate value iteration with π(1∣s) = 0.85
n	104	10	10	107
Average of Q	0.53(0.02)	0.95(0.01)	0.95(0.01)	0.95(0.01)
Average of V *	0.59(0.02)	0.95(0.01)	0.95(0.01)	0.95(0.01)
χ* coverage	0.80(0.03)	0.94(0.02)	0.95(0.01)	0.94(0.01).
Table 8: Linear interpolation in approximate value iteration with π(1∣s) = 0.9
n	104	10	10	10
Average of Q	0.31(0.01)	0.49(0.01)	0.67(0.01)	0.75(0.02)
Average of V *	0.37(0.01)	0.54(0.01)	0.71(0.01)	0.77(0.02)
χ* coverage	0.25(0.03)	0.30(0.03)	0.28(0.02)	0.29(0.03).
B.2 Efficiency of Exploration Policies
In Q-OCBA, our second-stage exploration policy is derived by maximizing the worst-case relative
discrepancy among all Q-value estimates. If one is interested in obtaining the best estimate of χ*
(i.e., the optimal value function initialized at a distribution ρ), then it would be more beneficial to
consider solving
2
wm∈Winη σχ2
(9)
to derive the optimal second-stage exploration policy πw (recall Lemma 1). The motivation is that
by doing so We would obtain a CI for χ* as short as possible.
Table 9 compares the 95% CI lengths and coverages for this exploration policy with other benchmark
strategies, for rL ranging from 1 to 3. For each rL , we show the averages of the coverages and CI
lengths of Q estimates among all (s, a) pairs, and also the coverage and CI length of χ* estimates.
Note that our strategy intends to shorten the CI lengths of χ* estimates. Like our experiment in
Section 5 in the main paper, we use a total observation budget n = 104, and devote 30% to the
initial stage where RE with π(1∣s) = 0.8 is used to estimate the parameters used to plug in the
criterion to be optimized in the second stage. For convenience and the consistency of terminology,
we continue to call our procedure to attain criterion (9) Q-OCBA. We compare this with pure RE
and -greedy, with ranging from 0.01 to 0.2.
Table 9 shows that our budget is enough to achieve the nominal 95% coverages for both the Q-values
and χ* using all strategies, which is consistent with the conclusion from Theorem 1 and Corollary 1.
However, Q-OCBA leads to desirably much shorter CI’s generally, with the shortest CI lengths in
all settings and sometimes by a big margin. For example, when rL = 2, the CI length derived by
Q-OCBA is at least 80% less than those derived by all the other methods. We also observe that
Q-OCBA performs much more stably than RE and -greedy, the latter varying quite significantly
14
Under review as a conference paper at ICLR 2020
for different values of rL. When rL = 1, -greedy with = 0.01 can perform almost as well as
Q-OCBA, with both CI lengths for Q being 2.45 — 2.46 and for χ* being 2.41 — 2.42. But when
rL = 2, -greedy with the same = 0.01 cannot even explore all (s, a) pairs. The situation worsens
when rL = 3, where none of the considered values of can explore all (s, a) pairs. This observation
on -greedy is consistent with Table 3 in the main paper where we consider the criterion using the
probability of correct selection. Regardless of using that or the current criteria, the performances of
-greedy depend fundamentally on whether the (s, a) pairs that need to be explored more also tend
to have larger Q-values. Note that when changing rL , the corresponding changes in the Q-values
would change the exploration “preference” for -greedy. However, as the underlying stochasticity of
the system does not change with rL , the states that need more exploration remain unchanged. This
misalignment leads us to observe quite different performances for -greedy when rL varies. Lastly,
again consistent with the results on the probability of correct selection shown in Table 3 of the main
paper, we observe that Q-OCBA outperforms pure RE in all cases in Table 5, with at least 40%
shorter CI lengths for the χ* estimates. This is attributed to the efficient use of variance information
in the second stage of Q-OCBA.
Table 9: Length of CI comparison for different exploration policies
	E = 0.2	E = 0.1	E = 0.01	π(1∣s) = 0.8	Q-OCBA
		rL 二	1		
Q Coverage	0.94(0.05)	0.97(0.03)	0.97(0.03)	0.97(0.01)	0.96(0.04)
Q CI length	3.86(0.16)	2.73(0.09)	2.46(0.03)	3.85(0.05)	2.45(0.03)
χ* Coverage	0.94(0.05)	0.97(0.03)	0.98(0.03)	0.97(0.01)	0.95(0.04)
χ* CI length	4.11(0.04)	2.86(0.0l)	2.42(0.01)	4.10(0.01广	2.41(0.01)
		rL 二	=2		
Q Coverage	0.98(0.01)	0.96(0.01)	NA a	0.97(0.01)	0.97(0.01)
Q CI length	2.25(0.i4)	2.72(0.17)	NA	1.84(0.11)	0.32(0.02)
χ* coverage	0.96(0.0i)	0.94(0.02)	NA	0.96(0.01)	0.96(0.01)
χ* CI length	2.69(0.02)	3.23(0.04)	NA	2.20(0.01广	0.37(0.01)
		IrL 二	=3		
Q coverage	NA	NA	NA	0.97(0.01)	0.97(0.01)
Q CI length	NA	NA	NA	0.74(0.06)	0.40(0.03)
χ* coverage	NA	NA	NA	0.95(0.01)	0.96(0.01)
χ* CI length	NA	NA	NA	0.91(0.01广	0.49(0.01)
aNA means that some (s, a) pair has never been visited.
C Proofs of Main Results
In this section, we present the proofs of the main results. In the proofs, we shall treat P as an
N ms -dimensional vector following the index rule: P ((i — 1)N + (j — 1)ms + k) = P (k|i, j)
Proof of Theorem 1. Define F (Q0, r0, P0) as a mapping from RN × RN × RNms to RN . Specifically,
F (Q0, r0, P 0)((i — 1)ma +j) = Q0((i — 1)ma +j) — r0((i — 1)ma +j)
—Y X P«i -1')N + (j — 1)ms + k) gk(Q0)
1≤k≤ms
for 1 ≤ i ≤ ms and 1 ≤ j ≤ ma, where gk (Q0) = maxl Q0((k — 1)ma + l), for 1 ≤ k ≤ ms.
15
Under review as a conference paper at ICLR 2020
By Assumption 2, there exists an open neighborhood of Q, which we denote as Ω, such that ∀Q0 ∈ Ω,
arg maxj Q0((i — 1)ma + j) is still unique for each 1 ≤ i ≤ ms. Then, for each 1 ≤ k ≤ ms, gk(Q0)
has all its partial derivatives exist and continuous. This implies that F(Q0, r0, P0) is continuously
differentiable in Ω × RN × RNms.
Denote the partial derivatives of F as
∂F	_	'FF	∂F	∂F'
∂(Q0,r0,P0)	∂Q	∂r0	犷
Note that ∂∂F is an N × N matrix. Denote its element at the ((i — 1)m0 + j)-th row, ((k — 1)m0 + l)-th
column by dF0i_1)ma+j . Then we have
∂Q(k-1)ma+l
dF(i-1)ma+j
dQ(k-1)ma+l
1 (k = i,j = l) - γP0((i - 1)N + (j - 1)ms + k)
×1 Q0((k — 1)ma + l)
Putting all the elements together, we have
∂F
∂Q0 = 一叫
maxQ0((k — 1)ma +
u
where P0 is an N X N matrix with
P ((i - 1)ma + j,(k — 1)ma + l)
P0((i — 1)N+ (j — 1)ms +k)
×1(Q0((k — 1)ma + l) = maxQ0((k — 1)ma + u)),
u
for 1 ≤ i ≤ ms , 1 ≤ j ≤ ma,1 ≤ k ≤ ms,1 ≤ l ≤ ma .
Since all rows of P0 sum up to one, P0 can be interpreted as the transition matrix of a Markov Chain
with state space {(i,j) : 1 ≤ i ≤ m$, 1 ≤ j ≤ m°}. Note that ∂F is invertible for any Q0 ∈ Ω.
We can then apply the implicit function theorem to the equation F(Q,μR,P) = 0. In particular,
there exists an open set U around μR × P ∈ RN × RNms, and a unique continuously differentiable
function φ: U → RN , such that for any r0 × P0 ∈ U
φ(μR,p) = Q
F(φ(r0,P0),r0,P0) =0.
In addition, the partial derivatives of φ satisfy
vφ(μR,p) := ∂(⅜)
It is also easy to verify that
∂F
∂r0
r0=μR,P0 = P
Q0=Q,r0=μR,P 0 = P
Q0 = Q,r0 =μR,P 0 = P
We also note that
dF(i-1)ma+j
∂P0	:	^^^:
(k-1)N+(l-1)ms+v
Q0 = Q,r0=μR,P0 = P
γ max Q((v - 1)ma + u)1 (k = i,j = l)
u
YV*(V)I (k = i,j = I),
16
Under review as a conference paper at ICLR 2020
for 1 V i V m.s, 1 V j V m0,1 V k V m.s,1 V l V m°, and 1 V V V ms. Then
/(V *)t
C∏* :=%	=	. . . (V *)T
UP	Q0=Q,r0=μR,P 0 = P	
∖
(V寸)
which is an N × NmS matrix.
Next, for
μR,n((i — 1)ma + j)
P1≤t≤n rt(st, at )1(St = i,αt = j )
P1≤t≤n I(St = i,at = j)
Ei≤t≤n rt(St, at)1(St = i,αt = j)	w((i - 1)ma + j)n
w((i - 1)ma + j)n	P1≤t≤n I(St = i,at = j)
and
Pn((i - 1)N + (j - 1)ms + k)
P1≤t≤n I(St = Kat = j, St(St, at) = k)
Σ1≤t≤n I(St = i,at = j)
Pι≤t≤n I(St = i, at = j, S'tlSt, at) = k)	w((i — 1)m0ι + j)n
w((i - 1)ma + j)n	Pι≤t≤n I(St = i,at = j)
by Assumption 1, 3 and Slutsky's theorem, we have
[^R,nj
and
1-(∖^ A 1
n ([{[^R,n↑ Pn]
W W rWTDR 0 ʌ j
where Σr,p = h——0--------D^j , and
/ ς%i
w(0mα⅛1)
.
.
■
DP =
∖
which is an NmS × NmS matrix. By tl
,,. ʌ .
φ(μR,n, Pn)
~t 1 Γ	7~»1	C
n] - [μR, P] --→ 0 a∙s
-[^r,p]) ⇒ N(O, ςr,p)
ʌ
MPij
w((i-1)ma+j)
.
.
.
____EPms ,mɑ____
w((ms-1)ma+ma)) /
continuous mapping theorem, we have
φ(μR, P) → 0 a.s. as n → ∞,
(10)
∖
which implies Qn T Q a.s.. In addition, using the delta method, we have
√n((^n - Q)	=	√n(φ(μR,n, Pn)- Φ"r, P))
⇒ N(0, %φ(μR, P)∑r,p%Φ(μR, P)T) as n → ∞.
17
Under review as a conference paper at ICLR 2020
We also have
Vφ(μR ,P )∑R,P Vφ(μR,P )T
(I-YPn)-1[I,Cπ*] (W-ODR S [I,Cπ*]T((I-YPn)-1)T
(I - YPπ*)-1(WTDR + Cπ*Dp(Cπ*)t)((I - YPπ*)-1)T
(I-YPπ*)-1WTDR + Dq]((I - γPπ*)-1)t.
□
Proof of Corollary 1. Define gV (Q): RN → Rms as
gV(Q)=(g1(Q),...,gms(Q))=(Vπ*(1),...,Vπ*(ms)),
which is continuously differentiable in an open neighborhood of Q. Then we can apply the delta
method to get
√n(V⅛ - Vπ*) = √n(gv(Qn)- gv(Q)) ⇒N(0, VgV(Q)Σ(Vgv(Q))T) as n →∞.
Note that VgV(Q) is a m§ X N matrix with VgV(Q)(i, (j - 1)m0 + k) = 1 (i = j,k = a*(i)). By
0
0
*
rearranging the index such that Pπ
(where “*" denotes a placeholder of some quan-
tities), We have VgV(Q) = [1,0], DR = Diag(DR*, *), W = Diag(Wπ*, *) and DQ = Diag(DV, *).
We also note that
VgV (Q)(I-YPπ* )-1
∞
[1,0] X Yi(Pπ* )i
i=0
[1,0]X Yi F
∞
XYih(Pπ*)i,0i = h(I-YPπ*)-1,0i .
i=0
Thus,
VgV (Q)Σ(VgV (Q))T = h(I-YPπ*)-1, 0iW-1[DR+DQ] h(I-YPπ*)-1, 0iT
= (I-YPπ*)-1(Wπ*)-1[DRπ*+DVπ*]((I-YPπ*)-1)T.
Lastly, the asymptotic normality of Χ∏* follows from the continuous mapping theorem.	□
We next establish the asymptotic normality for the estimated value function under a given policy π.
In this case, the value function Vπ satisfies a Bellman equation
Vπ(s) = X μR(s,a)∏(a∣s) + Y X∏(a∣s) X P(s0∣s,a)Vπ(s0).
a	a	s0
i ∖ i i i i η	i ∙ i	c τ λ-7γ 1	τ^r-τr τ	∣ ∙ ι	tλλ-tγ ∙ ∣ ι r∙ η ∙ ∣ r ∣ η	η ∙
Denote the the estimator of Vπ by Vnπ . In particular, Vnπ is the fixed point of the corresponding
empirical Bellman equation that replaces (μR,P) by (”R,n,Pn). We have:
18
Under review as a conference paper at ICLR 2020
Corollary 2. Under Assumptions 1 and 3,
√n(VΠ - Vπ) ⇒N(o, ∑V)
where
∑V = X ZW ZX zt ,
XZ = (I — YPπ)-1, Pπ is an m§ × m§ transition matrix with Pπ(i, j) = Pa P(j∣s = i, a)斤(a∣s = i),
WZ is an ms × ms diagonal matrix with
WZ(i,i) = X j2 [(yv π )τ ∑Pi,j (yv π)+ σR (i,j)]
j W(Gj)
Proof of Corollary 2. Similar to the proof of Theorem 1, define Fπ as a mapping from RmS × RN ×
RNmS τ RmS :
Fπ (V ,,r',P)(S) = V 0(s)- X r'(s,α)∏(α∣s)-γ X ∏(α∣s) X P(SiS, a)V'(s').
a
a
s0
Note that Fπ (Vπ,μn,P) = 0, Fπ is continuously differentiable and I - YPπ is invertible. We
can thus apply the implicit function theorem. In particular, there exists an open set Uπ around
μR × P ∈ RN × RNmS, and a unique continuously differentiable function φπ: Uπ T RN, such that
φπ (MR ,p ) = v π
F π (φπ (r0,P z),rz,P z)=0
for any r0 × P' ∈ Uπ. For the partial derivatives of φπ, we
1
have
where
∂φπ I
d(r 0, PZ) I r0=μR,P0 = P
—
一 ∂Fπ
∂r0，
∂Fπ 一
aP7
Gπ :
∂F π
∂r0
∂F π
^∂V7
V0=Vπ ,r0=μR,P0 = P
∕∏(∙∣i)τ
=I-YPπ,
V0 = Vπ ,r0=μR,P0 = P
π(∙∣i)τ
V0 = Vπ ,r0=μR,P0 = P
∖
∖
π(∙ms)T7
and
/(q∏)T
∖
HV ：
∂F π
aP7
V0 = Vπ ,r0=μR,P0 = P
(q∏ )T
∖
where (q∏ )t = Y [Π(1∣i)(Vπ )t ,...Π(j∣i)(Vπ )t ,... Π(m0∣i)(Vπ )t ], which is an N -dimensional row
vector.
19
Under review as a conference paper at ICLR 2020
Applying the delta method, we have
√n(V∏ - Vπ ) = √n(φπ(μR,n,Pn)- φπ (μR,P))
⇒ N(0, Vφπ(μR, P)∑R,PVφπ(μR, P)T) as n → ∞,
where
Vφπ (μR,P )∑R,P Vφπ (μR,P )T
=(I - γPπ)-1[Gπ,HV] (W-ODR -D0-^ [Gπ,HV]T((I - γPπ)-1)t
=(I - γPπ)-1(GπWTDR(Gn)t) + HVDP(HV)T)((I - γPπ)-1)T
and the conclusion follows.
□
Proof of Theorem 2. Write the MDP problem in its LP representation
max	Ps ρ(s)V (s)
sub ject to V(s) ≥ r(s, a) + γ s0∈S P(s0|s, a)V(s0), ∀s, a
with the dual problem
max	∑s,a μκ(s,a)xs,a
subject to Pa xs,a - γPs0,a P(s|s0, a)xs0,a = ρ(s), ∀s
xs,a ≥ 0, ∀s, a
The decision variables in the dual problem, xs,a ’s, in particular represent the occupancy measures
of the MDP. If the MDP has non-unique optimal policies, the dual problem also has non-unique
optimal solutions, which implies a degeneration of the primal problem. Degeneration here means
that some constraints are redundant at the primal optimal solution (i.e., the corner-point solution
is at the intersection of more than ms hyperplanes). Since the rows of the primal LP are linearly
independent, we know that in this case, there are multiple (K0 > 1) choices for the set of basic
variables (vkB)1≤k≤K0 at the optimal solution. When the coefficients in the intersecting hyperlanes
perturb slightly along a given direction, the objective value will change by a perturbation of the
objective coefficients along a chosen set of basic variables vkB . In other words, we can partition the
set of directions U into subsets {Uk}1≤k≤κ0 such that, if the direction of perturbation of (P, μκ),
say u, lies in Uk , then the LP optimal value perturbs by fixing the basic variables as vkB. Denote
Gρk as the gradient vector corresponding to this direction. If some of the Gρk ’s are equal, we merge
the corresponding Uk 's into one partition set. Thus, We have KP ≥ 1 distinct GP's and a partition
of U = ∪ι≤k≤κρUP, where UP = {u : Du(P*r) = Gk}, and Du(P*r) denotes the gradient of
Es P(S)V(s) with respect to (P,μκ) along the direction u.
Note that the argument so far focuses on the LP with objective value s ρ(s)V (s). How-
ever, we can repeat the same argument for each V(s) by setting ρ(s) = es . For any u ∈ U
and s ∈ S, denote the directional gradient of V(S) with respect to P,μR by DuV(P,μR)(s),
thus we can define the directional Jacobian of V with respect to P,μR as DuV(P,μR):=
[DuV (P,μR)(1),...,DuV (P,μR)(s),.. .,DuV (P,μR )(ms)]T, which leads to K ≥ 1 (potentially
larger than KP) distinct Gk’s, where Gk ∈ RmcWms+N) and U = ∪ι≤k≤κUk is partitioned with
Uk = {u : DuV (P,μR) = Gk }∙ Define Un = (Pn - P, μR,n - NR) {\\Pn - P ∣∣2 + ∣∣∕⅛n - μR∣∣2 ∙ We
have
K
Vn - V	= ɪ2 GkI	(un	∈	Uk I(Pn	- P, RR,n - MR)+	oP (Il(Pn	- P,	RR,n - NR )∣∣)
k=1
20
Under review as a conference paper at ICLR 2020
Multiply √n on both sides and notice that
Un = (√n(Pen - P), √n(μR,n - μR))/↑/∖∖√n(ftn - P)||2 + ∣∣√n(μR,n - μR)∣∣2
is a continuous mapping of (√n(Pn 一 P), √n(μR,n 一 μκ)). By taking n → ∞, we get the result by
the continuous mapping theorem.
□
Proof of Theorem 3. We use the LP representation of the constrained MDP. Define xs,a as the
occupancy measure
∞
xs,a = XγtPρ(St = s)
t=0
where Pρ denotes the distribution of St ’s with initial distribution ρ. Then, xs,a satisfies the LP
max	∑s,a μκ(s,a)xs,a
subject to Es,a μc(s, a)xs,a ≤ η
Pa xs,a - γ Ps0,a P(s∣s0, a)xs0,a = ρ(s), ∀s
xs,a ≥ 0, ∀s, a
(11)
(This is the dual formulation in the proof of Theorem 2 with an extra constraint.) The objective and
the first constraint correspond to the ob jective and constraint in the constrained MDP formulation.
The second constraint can be deduced by a one-step analysis on the definition of occupancy measure.
Once (11) is solved to obtain an optimal solution (x；a)s,a
policy
it can be translated into an optimal
π*(a∣s)
*
s,a
a xs*,a
x
Note that the number of structural constraints in (11) is ms + 1, and a corner-point optimal solution
has ms + 1 basic variables. Moreover, by our assumptions, the optimal solution is unique and
the LP is non-degenerate, so that perturbing the parameters μκ(s, a), μc(s, a), P(s∣s0, a) does not
immediately imply an overshoot to negativity for the reduced costs of the non-basic variables. Now
consider two cases depending on whether the first constraint is non-binding or binding. The first
case corresponds to a deterministic optimal policy, i.e., for any s, xs,a > 0 for only one a. In this
case a small perturbation of the parameters still retains the same basic and non-basic variables, and
the derived perturbed policy still retains the non-binding first constraint. In this case, the analysis
reduces back to Corollary 2.
In the second case, xs,a > 0 for only one a, for all s except one state sr , where we can have
xsr,aKsr) > 0 and xsr,a*(sr) > 0 for two distinct actions a1(sr), a*(sr). Again, perturbing the
parameters retains these basic and non-basic variables. In particular, the first constraint remains
binding in the perturbation, so that the perturbed optimal policy π* is still split at the same state
and satisfies s ρ(s)Lπ (s) = η. Now denote the mixing parameter by α* := π*(a1*(sr)∣sr), and so
π* (a2* (sr)∣sr) = 1 - α*. By applying the implicit function theorem to the Bellman equation, there
exists a continuously differentiable function Φl such that Ln (s) = Φl(mc,P,α*). By applying
the implicit function theorem again to the equation Es P(S)φL(μc,P,α*) = η, we know a* is a
continuously differentiable function of μc and P. Thus V * can be viewed as a function of μR, P and
a*, the latter in turn depending on μc and P. It can also be viewed as a function of μR, μc and P
directly. We use Vμβ,μc ,p V *(μR, μc, P) to denote the Jacobian of V * with respect to μR, μc, P
when viewing V * as a function of these variables. We also use Vμβ,μc ,p,αV * (pr, P, α*) to denote
the Jacobian of V * with respect to μR, μc, P, α*, this time viewing V * as a function of μR, P, α*.
21
Under review as a conference paper at ICLR 2020
We use similar notations throughout, and their meanings should be clear from the context. To
facilitate derivations, We also distinguish the notation Vχf, which denotes the multi-dimensional
Jacobian matrix, from ∂xf, which is used to denote the Jacobian when x is a scalar (1-dimensional).
VMr,MC ,P V *(μR, μC, P ) = P μR,μc,Ρ,α* V 2R, P, α*)[I, V μR ,μc,P α*(μC, P )T ]T
=VμR,μc,p V *(μR,P,α*)+ ∂°* V *(μR,P,α*)VμR,μc,p α*(μ°,P)	(12)
Differentiating
PTLπ* (μR,μc,P) = η,
we have
PT vmr,Mc ,p Lπ (μc ,P,α") + PT dα* Lπ (μc ,p,α")^ μR,μc ,P α*(μC,P) = 0.
By rearranging the equation, we have
V"R,μC,Pα*(μC,p) = - pT∂α* l∏* (μC,p,α^)PT V"R，"C，PLn (μC,p,α*)
Substituting this into (12), we get
vμR,μc,P V *(μR,μC,p )= vμR,μc,P V (μR,P,α*)
PTVμR,μc,PLπ* (μc, P, α*)∂α* V*(μκ, P, α*)
PT∂α* Lπ* (μc,P,α*)
Next, define an ms-dimensional vector r° by r0(s) = Em=I μc(s,j)∏*(j∣s)∙ Then
∂α* Lπ* (μc,P,α*) = VrC ,p ∏* Lπ* (r°,P π*)[(∂α* r° (α*))T, (∂α* Pπ* (α*))T]T
Note that (I - γPπ* )Lπ* = rC. By applying the implicit function theorem, we have
/Lπ*
∂α* Lπ* (μc,P,α*) = (I - γPπ* )-1
I,
∖
(I-γPπ*)-1qL
where q； is a vector with q；(Sr) = μR(Sr, a；(sr)) 一 μκ(sr, ag(sr)), qL(m§ + (s『一1)m0 + j)
P(j∣Sr, a1(sr)) — P(j∣Sr,。2国))for 1 ≤ j ≤ ma and qLL(i) = 0 for any other index i.
Similarly, we can define rR by rR(s) = E催 1 μκ(s,j)π*(j∣s), and we have
∂α* V *(〃R ,P,α*) = (I — γPπ* )-1qv
The derivation of VμR,μC,pV*(μκ,P,α*) and VμR,μC,pLπ* (μc,P,α*) follows exactly the same line
of analysis as how we derive Gn and HV in the proof of Corollary 2.	□
Proof of Theorem 4. Denote
[μR,n, pn] So = [μR,n((i - 1)ma + j ),Pn ((i - I)N + (j - 1)ms + k)]i∈So, 1≤j ≤ma ,1≤k≤ms ,
22
Under review as a conference paper at ICLR 2020
and
[μR, P]So = [μR((i - 1)ma + j), P((i -I)N + Cj-I)ms + k)]i∈S0,1<j≤ma,1≤k≤ms ∙
By Assumption 6, we have
I-/ Γ ʌ A 1	Γ E ∖ . A ∕∙∕c ∖ '	∖	1_  ∖ '
Λ∕n([μR,n, Pn↑So — [μR ,P ]S0 ) ⇒ N (0, ∑R,Ρ,So ) where ∑R,P,So
0
DS0
DQ
Notice Mg ◦ MS0 ^T^r P only involves random variables [μR,n, Pn]s° . Changing the distribution of
[μR,n, Pn]s∖So will not change the distribution of QM. We can thus assign auxiliary random variables
to μR,n and Pn for all i ∈ So, 1 ≤ j ≤ ma, 1 ≤ k ≤ m§. In particular, we define independent random
variables for each i ∈/ S0 by letting
μR,n((i - 1)ma + j) = √nN(μκ((i - 1)ma + j),1)
Pn((i - 1)N + (j - 1)ms + k) = ɪ N (P ((i - 1)N + (j - 1)ms + k), 1).
n
T-1 1 ∙	l 111	1 ∙	∙ 1 1	∙ 11 Γ ʌ	A 1 l	1 ∙	∙ 1
By doing so, we extend the ms° ma-dιmensιonal random variable [^rnj, Pn]s° to an msma-dimensional
random variable [〃R,n,Pn]s and
√n([μR,n,Pn]s - [μR,P]s) ⇒ N(0, ∑r,p,s) Where ∑R,P,S = ("Rp,S0
0I
Similar to the proof of Theorem 1, define
Fm(Q0, r0, P) = Q0 - Mg ◦ MS ◦ Trp (Q0).
By Assumption 4, Mg is max-norm non-expansion. Then, Mg ◦ MIS0 is also max-norm non-expansion,
implying that V(Mg ◦ MS0) has all its eigenvalues less than or equal to 1. Thus,
dFM = VM(Tr0,P0(Q0))(I - YP0)
∂Q
is invertible. By Assumption 4, we have FM(Qm,μR,P) = 0. By Assumption 5, there exists a
neighborhood Ωm around (Qm,μR,P), such that FM is continuously differentiable on Ωm. Then,
applying the implicit function theorem, we have that there exists an open set EM ⊂ Ωm and a
continuously differentiable mapping Φm on EM, such that Φm(μR,P) = Qm and
VφM (μR,P)
Γ ∂Fm ] 1 Γ ∂Fm ∂Fm ] I
∂Q0	∖~0hj~,Hργ I
l j L	JlQO=QM ,r0=μR,p 0 = p
Using the delta method, we have
√n(Q M - QM) ⇒ N (0, VφM (μR, P )∑R,P,S (VφM (μR, P ))T )= N (0, ∑M )∙
□
Proof of lemma 1. For any given policy π, by the balance equation for Markov Chains, its induced
stationary distribution wπ satisfies
wπ((k - 1)ma + l)P (i|s = k,a = l)π(a = j|s = i) = wπ ((i - 1)ma +j)
k,l
23
Under review as a conference paper at ICLR 2020
for any i ∈ S, j ∈ A. Summing up across j’s for each i, we have
wπ((i - 1)ma + j)
j
=	wπ ((k - 1)ma + l)P (i|s = k, a = l)π(a = j|s = i)
=	wπ ((k - 1)ma + l)P (i|s = k, a = l)
k,l
On the other hand, for any w in W , πw satisfies
w((k - 1)ma + l)P (i|s = k, a = l)πw (a = j|s = i)
k,l
=	w((k - 1)ma + l)P (i|s = k,a = l)w((i - 1)ma +j)/	w((i - 1)ma +u)
k,l	u
=	w((i - 1)ma + u)w((i - 1)ma +j)/	w((i - 1)ma +u) = w((i - 1)ma +j)
uu
for all i ∈ S. Thus, W is the stationary distribution of transition matrix Pnw.	□
24