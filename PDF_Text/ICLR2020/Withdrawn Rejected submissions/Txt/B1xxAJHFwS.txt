Under review as a conference paper at ICLR 2020
A Finite-Time Analysis of Q-Learning with
Neural Network Function Approximation
Anonymous authors
Paper under double-blind review
Ab stract
Q-learning with neural network function approximation (neural Q-learning for
short) is among the most prevalent deep reinforcement learning algorithms. De-
spite its empirical success, the non-asymptotic convergence rate of neural Q-
learning remains virtually unknown. In this paper, we present a finite-time analy-
sis of a neural Q-learning algorithm, where the data are generated from a Markov
decision process and the action-value function is approximated by a deep ReLU
neural network. We prove that neural Q-learning finds the optimal policy with
O(1∕√T) convergence rate if the neural function approximator is sufficiently
overparameterized, where T is the number of iterations. To our best knowledge,
our result is the first finite-time analysis of neural Q-learning under non-i.i.d. data
assumption.
1	Introduction
Q-learning has been shown to be one of the most important and effective learning strategies in
Reinforcement Learning (RL) over the past decades (Watkins & Dayan, 1992; Schmidhuber, 2015;
Sutton & Barto, 2018), where the agent takes an action based on the action-value function (a.k.a., Q-
value function) at the current state. Recent advance in deep learning has also enabled the application
of Q-learning algorithms to large-scale decision problems such as mastering Go (Silver et al., 2016;
2017), robotic motion control (Levine et al., 2015; Kalashnikov et al., 2018) and autonomous driving
(Shalev-Shwartz et al., 2016; Schwarting et al., 2018). In particular, the seminal work by Mnih
et al. (2015) introduced the Deep Q-Network (DQN) to approximate the action-value function and
achieved a superior performance versus a human expert in playing Atari games, which triggers a
line of research on deep reinforcement learning such as Double Deep Q-Learning (Van Hasselt
et al., 2016) and Dueling DQN (Wang et al., 2016).
Apart from its widespread empirical success in numerous applications, the convergence of Q-
learning and temporal difference (TD) learning algorithms has also been extensively studied in the
literature (Jaakkola et al., 1994; Baird, 1995; Tsitsiklis & Van Roy, 1997; Perkins & Pendrith, 2002;
Melo et al., 2008; Mehta & Meyn, 2009; Liu et al., 2015; Bhandari et al., 2018; Lakshminarayanan &
Szepesvari, 2018; Zou et al., 2019b). However, the convergence guarantee of deep Q-learning algo-
rithms remains a largely open problem. The only exceptions are Yang et al. (2019) which studied the
fitted Q-iteration (FQI) algorithm (Riedmiller, 2005; Munos & Szepesvari, 2008) with action-value
function approximation based on a sparse ReLU network, and Cai et al. (2019a) which studied the
global convergence of Q-learning algorithm with an i.i.d. observation model and action-value func-
tion approximation based on a two-layer neural network. The main limitation of the aforementioned
work is the unrealistic assumption that all the data used in the Q-learning algorithm are sampled i.i.d.
from a fixed stationary distribution, which fails to capture the practical setting of neural Q-learning.
In this paper, in order to bridge the gap between the empirical success of neural Q-learning and
the theory of conventional Q-learning (i.e., tabular Q-learning, and Q-learning with linear function
approximation), we study the non-asymptotic convergence of a neural Q-learning algorithm under
non-i.i.d. observations. In particular, we use a deep neural network with the ReLU activation func-
tion to approximate the action-value function. In each iteration of the neural Q-learning algorithm, it
updates the network weight parameters using the temporal difference (TD) error and the gradient of
the neural network function. Our work extends existing finite-time analyses for TD learning (Bhan-
dari et al., 2018) and Q-learning (Zou et al., 2019b), from linear function approximation to deep
1
Under review as a conference paper at ICLR 2020
Table 1: Comparison with existing finite-time analyses of Q-learning.
	Non-i.i.d.	Neural Approximation	Multiple Layers	Rate
Bhandari et al. (2018)	✓	X	X	O(1/T)
Zou et al. (2019b)	✓	X	X	O(1∕T)
Cai et al. (2019a)	X	✓	X	O(1∕√T)
This paper	✓	✓	✓	O(1∕√T)
neural network based function approximation. Compared with the very recent theoretical work for
neural Q-learning (Yang et al., 2019; Cai et al., 2019a), our analysis relaxes the non-realistic i.i.d.
data assumption and applies to neural network approximation with arbitrary number of layers. Our
main contributions are summarized as follows
•	We establish the first finite-time analysis of Q-learning with deep neural network function approx-
imation when the data are generated from a Markov decision process (MDP). We show that, when
the network is sufficiently wide, neural Q-learning converges to the optimal action-value function
up to the approximation error of the neural network function class.
•	We establish an O(1/√T) convergence rate of neural Q-learning to the optimal Q-ValUe function
up to the approximation error, where T is the number of iterations. This convergence rate matches
the one for TD-learning with linear function approximation and constant stepsize (Bhandari et al.,
2018). Although We study a more challenging setting where the data are non-i.i.d. and the neural
network approximator has multiple layers, our convergence rate also matches the O(1/√T) rate
proved in Cai et al. (2019a) with i.i.d. data and a two-layer neural network approximator.
To sum up, we present a comprehensive comparison between our work and the most relevant work
in terms of their respective settings and convergence rates in Table 1.
Notation We denote [n] = {1, . . . , n} for n ∈ N+. kxk2 is the Euclidean norm ofa vector x ∈ Rd.
For a matrix W ∈ Rm×n, we denote by kWk2 and kWkF its operator norm and Frobenius norm
respectively. We denote by vec(W) the vectorization of W, which converts W into a column
vector. For a semi-definite matrix Σ ∈ Rd×d and a vector X ∈ Rd, kx∣∣∑ = √x>Σx denotes the
Mahalanobis norm. We reserve the notations {Ci}i=0,1,... to represent universal positive constants
that are independent of problem parameters. The specific value of {Ci}i=1,2,... can be different
line by line. We write an = O(bn) if an ≤ Cbn for some constant C > 0 and an = O(bn) if
an = O(bn) up to some logarithmic terms of bn.
2	Related Work
Due to the huge volume of work in the literature for TD learning and Q-learning algorithms, we
only review the most relevant work here.
Asymptotic analysis The asymptotic convergence of TD learning and Q-learning algorithms has
been well established in the literature (Jaakkola et al., 1994; Tsitsiklis & Van Roy, 1997; Konda &
Tsitsiklis, 2000; Borkar & Meyn, 2000; Ormoneit & Sen, 2002; Melo et al., 2008; Devraj & Meyn,
2017). In particular, Tsitsiklis & Van Roy (1997) specified the precise conditions for TD learning
with linear function approximation to converge and gave counterexamples that diverge. Melo et al.
(2008) proved the asymptotic convergence of Q-learning with linear function approximation from
standard ODE analysis, and identified a critic condition on the relationship between the learning
policy and the greedy policy that ensures the almost sure convergence.
Finite-time analysis The finite-time analysis of the convergence rate for Q-learning algorithms has
been largely unexplored until recently. In specific, Dalal et al. (2018); Lakshminarayanan & Szepes-
vari (2018) studied the convergence of TD(0) algorithm with linear function approximation under
i.i.d. data assumptions and constant step sizes. Concurrently, a seminal work by Bhandari et al.
(2018) provided a unified framework of analysis for TD learning under both i.i.d. and Markovian
noise assumptions with an extra projection step. The analysis has been extended by Zou et al.
(2019b) to SARSA and Q-learning algorithms with linear function approximation. More recently,
2
Under review as a conference paper at ICLR 2020
Srikant & Ying (2019) established the finite-time convergence for TD learning algorithms with lin-
ear function approximation and a constant step-size without the extra projection step under non-i.i.d.
data assumptions through carefully choosing the Lyapunov function for the associated ordinary dif-
ferential equation of TD update. A similar analysis was also extended to Q-learning with linear
function approximation (Chen et al., 2019). Hu & Syed (2019) further provided a unified analysis
for a class of TD learning algorithms using Markov jump linear system.
Neural function approximation Despite the empirical success of DQN, the theoretical convergence
of Q-learning with deep neural network approximation is still missing in the literature. Following
the recent advances in the theory of deep learning for overparameterized networks (Jacot et al.,
2018; Chizat & Bach, 2018; Du et al., 2019b;a; Allen-Zhu et al., 2019b;a; Zou et al., 2019a; Arora
et al., 2019; Cao & Gu, 2019a; Zou & Gu, 2019; Cai et al., 2019b), two recent work by Yang et al.
(2019) and Cai et al. (2019a) proved the convergence rates of fitted Q-iteration and Q-learning with
a sparse multi-layer ReLU network and two-layer neural network approximation respectively, under
i.i.d. observations.
3	Preliminaries
A discrete-time Markov Decision Process (MDP) is denoted by a tuple M = (S, A, P, r, γ). S and
A are the sets of all states and actions respectively. P : S × A → P (S) is the transition kernel
such that P(s0|s, a) gives the probability of transiting to state s0 after taking action a at state s.
r : S × A → [-1, 1] is a deterministic reward function. γ ∈ (0, 1) is the discounted factor. A policy
∏ : S → P(A) is a function mapping a state S ∈ S to a probability distribution ∏(∙∣s) over the
action space. Let st and at denote the state and action at time step t. Then the transition kernel P
and the policy π determine a Markov chain {st}t=0,1,... For any fixed policy π, its associated value
function V π : S → R is defined as the expected total discounted reward:
Vπ(s) = E[Pt∞=0 γtr(st, at)|s0 = s], ∀s ∈ S.
The corresponding action-value function Qπ : S × A → R is defined as
Qπ(s,a)= E[P∞=o Ytr(st,at)∣s0 = s,ao = a] = r(s,a) + Y RS Vπ(s0)P(s0∣s,a)ds0,
for all s ∈ S,a ∈ A. The optimal action-value function Q* is defined as Q*(s, a) = sup∏ Qπ(s, a)
for all (s, a) ∈ S ×A. Based on Q*, the optimal policy ∏* can be derived by following the greedy
algorithm such that π*(a|s) = 1 if Q(s,a) = maxb∈A Q*(s,b) and π*(a∣s) = 0 otherwise. We
define the optimal Bellman operator T as follows
T Q(s, a) = r(s, a) + Y ∙ E[maXb∈AQ(s0, b)∣s0 〜P (∙∣s, a)].	(3.1)
It is worth noting that the optimal Bellman operator T is Y-contractive in the sup-norm and Q* is
the unique fixed point of T (Bertsekas et al., 1995).
4 The Neural Q-Learning Algorithm
In this section, we start with a brief review of Q-learning with linear function approximation. Then
we will present the neural Q-learning algorithm.
4.1	Q-Learning with Linear Function Approximation
In many reinforcement learning algorithms, the goal is to estimate the action-value function Q(∙, ∙),
which can be formulated as minimizing the mean-squared Bellman error (MSBE) (Sutton & Barto,
2018):
min Eμ,∏,P [(TQ(s, a) - Q(s, a))2],	(4.1)
Q(∙,∙)
where state S is generated from the initial state distribution μ and action a is chosen based on a
fixed learning policy π. To optimize (4.1), Q-learning iteratively updates the action-value function
using the Bellman operator in (3.1), i.e., Qt+1(S, a) = TQt(S, a) for all (S, a) ∈ S × A. However,
due to the large state and action spaces, whose cardinalities, i.e., |S| and |A|, can be infinite for
continuous problems in many applications, the aforementioned update is impractical. To address this
3
Under review as a conference paper at ICLR 2020
issue, a linear function approximator is often used (Szepesvari, 2010; Sutton & Barto, 2018), where
the action-value function is assumed to be parameterized by a linear function, i.e., Q(s, a; θ) =
φ(s, a)>θ for any (s, a) ∈ S × A, where φ : S × A → Rd maps the state-action pair to a d-
dimensional vector, and θ ∈ Θ ⊆ Rd is an unknown weight vector. The minimization problem in
(4.1) then turns to minimizing the MSBE over the parameter space Θ.
4.2	Neural Q-Learning
Analogous to Q-learning with linear function approximation, the action-value function can also be
approximated by a deep neural network to increase the representation power of the approximator.
Specifically, we define a L-hidden-layer neural network as follows
f(θ; x) = √mWLσL(WL-i ∙∙∙ σ(Wιx) ∙∙∙),	(4.2)
where x ∈ Rd is the input data, W1 ∈ Rm×d, WL ∈ R1×m and Wl ∈ Rm×m for l = 2, . . . , L -
1, θ = (vec(W1)>, . . . , vec(WL)>)> is the concatenation of the vectorization of all parameter
matrices, and σ(x) = max{0, x} is the ReLU activation function. Then, we can parameterize
Q(s, a) using a deep neural network as Q(s, a; θ) = f(θ; φ(s, a)), where θ ∈ Θ and φ : S × A →
Rd is a feature mapping. Without loss of generality, we assume that kφ(s, a)k2 ≤ 1 in this paper.
Let π be an arbitrarily stationary policy. The MSBE minimization problem in (4.1) can be rewritten
in the following form
minEμ,∏,p [(Q(s, a; θ) - TQ(s, a; θ))2].
θ∈Θ
(4.3)
Recall that the optimal action-value function Q* is the fixed point of Bellman optimality operator T
which is Y-contractive. Therefore Q* is the unique global minimizer of (4.3).
The nonlinear parameterization of Q(∙, ∙) turns the MSBE in (4.3) to be highly nonconvex, which
imposes difficulty in finding the global optimum θ*. To mitigate this issue, we will approximate the
solution of (4.3) by project the Q-value function into some function class parameterized by θ, which
leads to minimizing the mean square projected Bellman error (MSPBE):
minEμ,∏,P [(Q(s, a； θ) — ∏FTQ(s, a； θ))2],
θ∈Θ
(4.4)
where F = {Q(∙, ∙; θ) : θ ∈ Θ} is some function class parameterized by θ ∈ Θ, and ∏f is a
projection operator. Then the neural Q-learning algorithm updates the weight parameter θ using the
following projected descent step: θt+1 = ΠΘ(θt - ηtgt(θt)), where the gradient term gt(θt) is
defined as
gt(θt) = Vθ f(θt; φ(st ,at ))(f(θt; φ(st,at)) - rt - Y maxb∈A f(θt; φ(st+ι,b))
dpf .	_ , _ . , _	.,	一
= ∆t(st, at, st+1; θt)Vθf(θt; φ(st, at)),	(4.5)
and ∆t is the temporal difference (TD) error. It should be noted that gt is not the gradient of
the MSPBE nor an unbiased estimator for it. The details of the neural Q-learning algorithm are
displayed in Algorithm 1, where θ0 is randomly initialized, and the constraint set is chosen to be
Θ = B(θ0, ω), which is defined as follows
B(θ0, ω) d=ef {θ = (vec(W1)>,...,vec(WL)>)> : kWl -Wl(0)kF ≤ω,l= 1,...,L} (4.6)
for some tunable parameter ω. It is easy to verify that kθ - θ0 k22 = PlL=1 kWl - Wl0 k2F .
5 Convergence Analysis of Neural Q-Learning
In this section, we provide a finite-sample analysis of neural Q-learning. Note that the optimization
problem in (4.4) is nonconvex. We focus on finding a surrogate action-value function in the neural
network function class that well approximates Q*.
5.1	Approximate S tationary Point in the Constrained S pace
To ease the presentation, we abbreviate f(θ; φ(s, a)) as f(θ) when no confusion arises. We define
the function class FΘ,m as a collection of all local linearization of f(θ) at the initial point θ0
FΘ,m = {f(θ0) + hVθf(θ0), θ - θ0i : θ∈ Θ}.	(5.1)
4
Under review as a conference paper at ICLR 2020
Algorithm 1 Neural Q-Learning with Gaussian Initialization
1:	Input: learning policy π, learning rate {ηt}t=0,1,..., discount factor γ, constraint set Θ, Ran-
domly generate the entries of Wl(0) from N(0, 1/m), l = 1, . . . , m
2:	Initialization: θ0 = (W0(1)> , . . . , W0(L)>)>
3:	for t = 0, . . . , T - 1 do
4:	Sample data (st, at, rt, st+1) from policy π
5:	∆t = f(θt; φ(st, at)) - (rt + γ maxb∈A f (θt; φ(st+1, b)))
6:	gt(θt) = Vθf(θt; φ(st,at))∆t
7:	θt+1 = ΠΘ (θt - ηtgt(θt))
8:	end for
Following to the local linearization analysis in Cai et al. (2019a), we define the approximate station-
ary point of Algorithm 1 as follows.
Definition 5.1 (Cai et al. (2019a)). A point θ* ∈ Θ is said to be the approximate stationary point
of Algorithm 1 if for all θ ∈ Θ it holds that
Eμ,∏,P [∆(s, a, s0; θ*)(Vθf(θ*; φ(s, a)), θ - θ*>] ≥ 0,	(5.2)
where f(θ; φ(s, a)) := f(θ) ∈ FΘ,m and the temporal difference error ∆ is
∆b (s, a, s0; θ) = fb(θ; φ(s, a)) - r(s, a) + γ maxb∈A fb(θ; φ(s0,b)) .	(5.3)
^ ^, . , . . .. , .,
For any f ∈ Fθ,m, it holds that (Vθf(θ*), θ - θ*> = (Vθf(θo), θ - θ*i = f(θ) - f(θ*).
Definition 5.1 immediately implies
Eμ,∏,P [(f(θ*) - Tf(θ*))(f(θ) - f(θ*))] ≥ 0,	∀θ ∈ Θ.	(5.4)
According to Proposition 4.2 in Cai et al. (2019a), this further indicates f (θ*) = ∏Fθ,mTf (θ*)∙
In other words, f (θ*) is the unique fixed point of the MSPBE in (4.4). Therefore, we can show the
convergence of neural Q-learning to the optimal action-value function Q* by first connecting it to
the minimizer f(θ*) and then adding the approximation error ofFΘ,m.
5.2	The Main Theory
Before we present the convergence of Algorithm 1, let us lay down the assumptions used throughout
our paper. The first assumption controls the bias caused by the Markovian noise in the observations
through assuming the uniform ergodicity of the Markov chain generated by the learning policy π .
Assumption 5.2. The learning policy π and the transition kernel P induce a Markov chain
{st}t=0,1,... such that there exist constants λ > 0 and ρ ∈ (0, 1) satisfying
suPs∈sdτv(P(St ∈ ∙∣so = s), ∏) ≤ λρt,	for all t = 0,1,...
Assumption 5.2 also appears in Bhandari et al. (2018); Zou et al. (2019b), which is essential for
the analysis of the Markov decision process. The uniform ergodicity can be established via the mi-
norization condition for irreducible Markov chains (Meyn & Tweedie, 2012; Levin & Peres, 2017).
For the purpose of exploration, we also need to assume that the learning policy π satisfies some
regularity condition. Denote bmaχ(θ) = argmaxb∈/ |(Vef (θ0; s, b), θ)∣ for any θ ∈ Θ. Similar to
Melo et al. (2008); Zou et al. (2019b); Chen et al. (2019), we define
Σ∏ = 1∕mEμ,∏ [Vθf (θo; s, a)Vθf (θ°; s, a)>],	(5.5)
Σ∏(θ) = 1∕mEμ,∏ [Vθf (θo; S, bmaχ(θ))Vθf (θo; S, bmaχ(θ))>].	(5.6)
Note that Σπ is independent of θ and only depends on the policy π and the initial point θ0 in the
definition off. In contrast, Σ*π (θ) is defined based on the greedy action under the policy associated
with θ. The scaling parameter 1∕m is used to ensure that the operator norm of Σπ to be in the order
of O(1). It is worth noting that Σπ is different from the neural tangent kernel (NTK) or the Gram
matrix in Jacot et al. (2018); Du et al. (2019a); Arora et al. (2019), which are n × n matrices defined
based on a finite set of data points {(si, ai)}i=1,...,n. When f is linear, Σπ reduces to the covariance
matrix of the feature vector.
5
Under review as a conference paper at ICLR 2020
Assumption 5.3. There exists a constant α > 1 such that Σ∏ 一 αγ2Σ∏(θ) * 0 for all θ and θo.
Assumption 5.3 is also made for Q-learning with linear function approximation in Melo et al. (2008);
Zou et al. (2019b); Chen et al. (2019). Moreover, Chen et al. (2019) presented numerical simulations
to verify the validity of Assumption 5.3. Cai et al. (2019a) imposed a slightly different assumption
but with the same idea that the learning policy π should be not too far away from the greedy policy.
The regularity assumption on the learning policy is directly imposed on the action value function in
Cai et al. (2019a), which can be implied by Assumption 5.3 and thus is slightly weaker. We note
that Assumption 5.3 can be relaxed to the one made in Cai et al. (2019a) without changing any
of our analysis. Nevertheless, we choose to present the current version which is more consistent
with existing work on Q-learning with linear function approximation (Melo et al., 2008; Chen et al.,
2019).
Theorem 5.4. Suppose Assumptions 5.2 and 5.3 hold. The constraint set Θ is defined as in (4.6).
We set the radius as ω = C0m-1/2L-9/4, the step size in Algorithm 1 as η = 1/(2(1 -α-1/2)mT),
and the width of the neural network as m ≥ Ci max{dL2 log(m∕δ),ω-"3L-8/3 log(m∕(ωδ))},
where δ ∈ (0,1). Then with probability at least 1 一 2δ 一 L2 exp(-C?m2/3L) over the randomness
of the Gaussian initialization θ0 , it holds that
T-1
T X E[(f(θt)- f(θ*))2∣θo] ≤
T t=0
CT log(T∕δ)log T +
β2 √
C3 log m log(T∕δ)
βm'∕6
where β = 1 一 α-1/2 ∈ (0,1) is a constant, T* = min{t = 0,1, 2,... ∣λρt ≤ ητ} is the mixing
time of the Markov chain {st, at}t=0,1,..., and {Ci}i=0,...,5 are universal constants independent of
problem parameters.
Remark 5.5. Theorem 5.4 characterizes the distance between the output of Algorithm 1 to the
approximate stationary point defined in function class FΘ,m. From (5.4), we know that f (θ*) is
the minimizer of the MSPBE (4.4). Note that T* is in the order of O(log(mT ∕ log T)). Theorem
5.4 suggests that neural Q-learning converges to the minimizer of MSPBE with a rate in the order
of O((log(mT))3∕√T + log m log T∕m1/6), which reduces to O(1∕√T) when the width m of the
neural network is sufficiently large.
In the following theorem, we show that neural Q-learning converges to the optimal action-value
function within finite time if the neural network is overparameterized.
Theorem 5.6. Under the same conditions as in Theorem 5.4, with probability at least 1 一 3δ 一
L2 exp(-Com2/3L) over the randomness of θo, it holds that
TXIE[(Q(s, a； θt) - Q*(s, a))2] ≤	：—：「*"『]+ √
+ Cιτ* log(T∕δ)log T + C2 log(T∕δ)log m
β2√T	βm1/6	,
where all the expectations are taken conditional on θ0, Q* is the optimal action-value function,
δ ∈ (0, 1) and {Ci}i=0,...,2 are universal constants.
The optimal policy π* can be obtained by the greedy algorithm derived based on Q*.
Remark 5.7. The convergence rate in Theorem 5.6 can be simplifies as follows
T X E[(Q(s, a； θt) 一 q*(S,a))2∣θ0] = o(E[(πFθ,mQ*(S,a) — Q*(S,a))2] + mi/6 + √T)
The first term is the projection error of the optimal Q-value function on to the function class FΘ,m,
which decreases to zero as the representation power of FΘ,m increases. In fact, when the width m
of the DNN is sufficiently large, recent studies (Cao & Gu, 2019a;b) show that f (θ) is almost linear
around the initialization and the approximate stationary point f (θ*) becomes the fixed solution of
the MSBE (Cai et al., 2019a). Moreover, this term diminishes when the Q function is approximated
by linear functions when the underlying parameter has a bounded norm (Bhandari et al., 2018; Zou
et al., 2019b). As m goes to infinity, we obtain the convergence of neural Q-learning to the optimal
Q-value function with an O(1∕√T) rate.
6
Under review as a conference paper at ICLR 2020
6 Proof of Main Results
In this section, we provide the detailed proof of the convergence of Algorithm 1. To simplify the
presentation, we write f(θ; φ(s, a)) as f(θ; s, a) throughout the proof when no confusion arises.
We first define some notations that will simplify the presentation of the proof. Recall the definition
of gt(∙) in (4.5). For any θ ∈ Θ,we define the following vector-value map g that is independent of
the data point.
g(θ) = Eμ,∏,P[Vθf(θ; s,a)(f(θ; s,a) -r(s,a) - Ymaxb∈A f(θ; s0,b))],	(6.1)
where S follows the initial state distribution μ, a is chosen based on the policy π(∙∣s) and s0 follows
the transition probability P(∙∣s, a). Similarly, for all θ ∈ Θ, we define the following gradient terms
based on the linearized function f ∈ FΘ,m
mt(θ) = ∆(st, at, st+ι; θ)Vθf(θ),	m(θ) = Eμ,∏,p [∆(s, a, s0; θ)Vθf(θ)],	(6.2)
where ∆b is defined in (5.3), and a population version based on the linearized function.
Now we present the technical lemmas that are useful in our proof of Theorem 5.4. For the gradients
gt(∙) defined in (4.5) and mt(∙) defined in (6.2), we have the following lemma that characterizes the
difference between the gradient of the neural network function f and the gradient of the linearized
function f .
Lemma 6.1. The gradient of neural network function is close to the linearized gradient. Specifically,
if θt ∈ B(Θ, ω) and m and ω satisfy
m ≥ Co max{dL2 log(m∕δ), ω-4/3L-8/3 log(m∕(ωδ))},
and C1d3/2L-1m-3/4 ≤ ω ≤ C2L-6(logm)-3,
(6.3)
then it holds that
∣hgt(θt) - mt(θt), θt- θ* i∣ ≤ C3(2 + γ)ω1∕3L3,m log m log(T∕δ) ||仇一θ*∣∣2
+ (C4ω4∕3L11∕3mplog m + C5ω2L4 m)∣∣θt — θ*∣∣2,
with probability at least 1—2δ—3L2 exp(-Cemω2∕3L) over the randomness of the initial point, and
∣∣gt(θt)∣∣2 ≤ (2 + γ)C7pm log(T∕δ) holds with probability at least 1 — δ — L2 exp(-C6mω2∕3L).
where {Ci > 0}i=0,...,7 are universal constants.
The next lemma upper bounds the bias of the non-i.i.d. data for the linearized gradient map.
Lemma 6.2. Suppose the step size sequence {η0, η1, . . . , ηT} is nonincreasing. Then it holds that
E[hmt(θt) — m(θt), θt — θ*i∣θo] ≤ Co(mlog(T∕δ) + m2ω2)τ*ηmaχ{o,t-τ*},
for any fixed t ≤ T, where C > 0 is an universal constant and T* = min{t = 0,1, 2,... ∣λρt ≤
ηT } is the mixing time of the Markov chain {st, at}t=o,1,.
Since f is a linear function approximator of the neural network function f, we can show that the
gradient of f satisfies the following nice property in the constrained set Θ.
Lemma 6.3. Under Assumption 5.3, m(∙) defined in (6.2) satisfies
hm(θ) — m(θ*), θ — θ*i ≥ (1 — α-1∕2)E[(f(θ) — f(θ*))[θo],	∀θ ∈ Θ.
Now we can integrate the above results and obtain proof of Theorem 5.4.
Proof of Theorem 5.4. By Algorithm 1 and the non-expansiveness of projection ΠΘ, we have
kθt+ι — θ*k2 = kπθ(θt — ηtgt) — θ*k2
≤ kθt — ηtgt — θ* k22
= kθt — θ*k22 + ηt2kgtk22 — 2ηthgt, θt — θ*i.	(6.4)
7
Under review as a conference paper at ICLR 2020
We need to find an upper bound for the gradient norm and a lower bound for the inner prod-
uct. According to	Definition 5.1, the	approximate stationary point θ* of Algorithm	1	satisfies
hm(θ*), θ - θ*i ≥	0 for all θ ∈ Θ. The inner product in	(6.4) can be decomposed into
hgt, θt- θ*i =	hgt- mt(θt), θt-	θ*i + hmt(θt) -	m(θt), θt- θ*i + hm(θt), θt-	θ*i
≥	hgt - mt(θt), θt-	θ*i + hmt(θt) -	m(θt), θt- θ*i
+ hm(θt) - m(θ*), θt- θ)	(6.5)
Combining results from (6.4)and (6.5), we have
kθt+ι - θ*k2 ≤ llθt - θ*k2 + η2kgtk2 - 2ηt hgt - mt(θt),θt - θ*i
`-----------------------------------------------------{z--------}
I1
-2ηt hmt(θt) - m(θt), θt -。*)—2n hm(θt) - m(θ*), θt - θ*i .	(6.6)
`-----------{z-------------}	`------------------------}
I2	I3
Recall constraint set defined in (4.6). We have Θ = B(θ0, ω) = {θ : kWl - Wl(0) kF ≤ ω, ∀l =
1, . . . , L} and that m and ω satisfy the condition in (6.3).
Term I1 is the error of the local linearization off(θ) at θ0. By Lemma 6.1, with probability at least
1 - 2δ - 3L2 exp(-Cimω2^3L) over the randomness of the initial point θo, We have
Ihgt - mt(θt), θt - θ*i∣≤ C2(2 + Y )m-1/6 Vzlog m log(T∕δ)	(6.7)
holds uniformly for all θt, θ* ∈ Θ, where we used the fact that ω = C0m-1/2L-9/4.
Term I2 is the bias of caused by the non-i.i.d. data (st, at, st+1) used in the update of Algorithm 1.
Conditional on the initialization, by Lemma 6.2, we have
E[hmt(θt) - m(θt), θt - θ*i∣θo] ≤ C3(mlog(T∕δ) + m2ω2)τ*ηmaχ{0,t-τ*},	(6.8)
where T * = min{t = 0,1, 2,... ∣λρt ≤ ητ} is the mixing time of the Markov chain {st, at}t=0,1,….
Term I3 is the estimation error for the linear function approximation. By Lemma 6.3, we have
hm(θt) -m(θ*),θt - θ*i ≥ βE[(f(θt)- f(θ*))2∣θo],	(6.9)
where β = (1 - α-1/2) ∈ (0,1) is a constant. Substituting (6.7), (6.8) and (6.9) into (6.6), we have
it holds that
kθt+i - θ*k2 ≤ kθt - θ*k2+ η2c2(2 + Yymlog(T∕δ)
-2ntC2(2 + Y)m-1/6Plogmlog(T∕δ) - 2ηtβE[(f(θt) - f(θ*))[θο]
- 2ηtC3(m log(T ∕δ) + m2ω2)τ *ηmax{0,t-τ*},	(6.10)
with probability at least 1 - 2δ - 3L2 exp(-Cιmω2/3L) over the randomness of the initial point
θo, where we used the fact that kgtkF ≤ C4(2 + Y)ʌ/mlog(T∕δ) from Lemma 6.1. Rearranging
the above inequality yields
E[(f(θt)- f(θ*))2∣θo] ≤
kθt- θ*k2-kθt+i- θ*k2	C2(2 + Y)mT/6logmlog(T∕δ)
访t	+	β
C4(2 + Y)2m log(T∕δ)ηt	C3m(log(T7δ) + mω2)τ*ηmaχ{0,t-τ*}
+	β	+	β
with probability at least 1 - 2δ - 3L2 exp(-Cιmω2/3L) over the randomness of the initial point
θo. Recall the choices of the step sizes no = ... = ητ = 1∕(2βm√T) and the radius ω =
C0m-1/2L-9/4. Dividing the above inequality by T and telescoping it from t = 0 to T yields
T-1
T X E[(f(θt)- f(θ*))2∣θo] ≤
T t=0
m∣∣θo — θ*k2	C2(2 + γ)m-1∕6 log mlog(T∕δ)
一√T — +	β
+ C4(2 + γ)2 log(T∕δ)logT + C3(log(T∕δ) + 1)τ* logT
β2√T	e √T
8
Under review as a conference paper at ICLR 2020
For θo, θ* ∈ Θ, again by ω = CmT/2L-9/4, We have ∣∣θo - θ*∣∣2 ≤ 1/m. Since f(∙) ∈ Fθ,m,
by Lemma 6.1, it holds with probability at least 1 - 2δ - 3L2 exp(-C0m2/3L) over the randomness
of the initial point θ0 that
T XE E[(f(θt) - f(θ*))lθ0] ≤ √ + C1YlogT + C2「0,
where We used the fact that γ < 1. This completes the proof.	□
7	Conclusions
In this paper, we provide the first finite-time analysis of Q-learning with neural network function
approximation (i.e., neural Q-learning), where the data are generated from a Markov decision pro-
cess and the action-value function is approximated by a deep ReLU neural network. We prove that
neural Q-learning converge to the optimal action-value function UP to the approximation error with
O(1/√T) rate, where T is the number of iterations. Our proof technique is of independent interest
and can be extended to analyze other deep reinforcement learning algorithms. One interesting fu-
ture direction would be to remove the projection step in our algorithm by applying the ODE based
analysis in Srikant & Ying (2019); Chen et al. (2019).
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In Advances in neural information processing
systems, 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252, 2019b.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332, 2019.
Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In
Machine Learning Proceedings 1995, pp. 30-37. Elsevier, 1995.
Dimitri P Bertsekas, Dimitri P Bertsekas, Dimitri P Bertsekas, and Dimitri P Bertsekas. Dynamic
programming and optimal control, volume 1. Athena scientific Belmont, MA, 1995.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference
learning with linear function approximation. In Conference On Learning Theory, pp. 1691-1692,
2018.
Vivek S Borkar and Sean P Meyn. The ode method for convergence of stochastic approximation
and reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447-469, 2000.
Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference learning con-
verges to global optima. In Advances in Neural Information Processing Systems, 2019a.
Tianle Cai, Ruiqi Gao, Jikai Hou, Siyu Chen, Dong Wang, Di He, Zhihua Zhang, and Liwei Wang.
A gram-gauss-newton method learning overparameterized deep neural networks for regression
problems. arXiv preprint arXiv:1905.11675, 2019b.
Yuan Cao and Quanquan Gu. A generalization theory of gradient descent for learning over-
parameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019a.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. In Advances in Neural Information Processing Systems, 2019b.
Zaiwei Chen, Sheng Zhang, Thinh T. Doan, Siva Theja Maguluri, and John-Paul Clarke. Perfor-
mance of q-learning with linear function approximation: Stability and finite-time analysis. arXiv
preprint arXiv:1905.11425, 2019.
9
Under review as a conference paper at ICLR 2020
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in neural information processing
systems,pp. 3036-3046, 2018.
Gal Dalal, Balazs Szorenyi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td (0) with
function approximation. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Adithya M Devraj and Sean Meyn. Zap q-learning. In Advances in Neural Information Processing
Systems, pp. 2235-2244, 2017.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019a.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019b. URL https://openreview.net/forum?id=S1eK3i09YQ.
Bin Hu and Usman Ahmed Syed. Characterizing the exact behaviors of temporal difference learning
algorithms using markov jump linear system theory. arXiv preprint arXiv:1906.06781, 2019.
Tommi Jaakkola, Michael I Jordan, and Satinder P Singh. Convergence of stochastic iterative dy-
namic programming algorithms. In Advances in Neural Information Processing Systems, pp.
703-710, 1994.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Scalable deep reinforce-
ment learning for vision-based robotic manipulation. In Conference on Robot Learning, pp. 651-
673, 2018.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information
Processing Systems, pp. 1008-1014, 2000.
Chandrashekar Lakshminarayanan and Csaba Szepesvari. Linear stochastic approximation: How
far does constant step-size and iterate averaging go? In International Conference on Artificial
Intelligence and Statistics, pp. 1347-1355, 2018.
David A Levin and Yuval Peres. Markov chains and mixing times, volume 107. American Mathe-
matical Soc., 2017.
Sergey Levine, Nolan Wagener, and Pieter Abbeel. Learning contact-rich manipulation skills with
guided policy search. In 2015 IEEE International Conference on Robotics and Automation
(ICRA), pp. 156-163. IEEE, 2015.
Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample
analysis of proximal gradient td algorithms. In Proceedings of the Thirty-First Conference on
Uncertainty in Artificial Intelligence, pp. 504-513. AUAI Press, 2015.
Prashant Mehta and Sean Meyn. Q-learning and pontryagin’s minimum principle. In Proceedings
of the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese
Control Conference, pp. 3598-3605. IEEE, 2009.
Francisco S Melo, Sean P Meyn, and M Isabel Ribeiro. An analysis of reinforcement learning
with function approximation. In Proceedings of the 25th International Conference on Machine
Learning, pp. 664-671. ACM, 2008.
Sean P Meyn and Richard L Tweedie. Markov chains and stochastic stability. Springer Science &
Business Media, 2012.
10
Under review as a conference paper at ICLR 2020
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. Journal ofMachine
Learning Research, 9(May):815-857, 2008.
Dirk Ormoneit and Saunak Sen. Kernel-based reinforcement learning. Machine learning, 49(2-3):
161-178, 2002.
Theodore J Perkins and Mark D Pendrith. On the existence of fixed points for q-learning and sarsa
in partially observable domains. In Proceedings of the Nineteenth International Conference on
Machine Learning, pp. 490-497. Morgan Kaufmann Publishers Inc., 2002.
Martin Riedmiller. Neural fitted q iteration-first experiences with a data efficient neural reinforce-
ment learning method. In European Conference on Machine Learning, pp. 317-328. Springer,
2005.
Jurgen Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85-117,
2015.
Wilko Schwarting, Javier Alonso-Mora, and Daniela Rus. Planning and decision-making for au-
tonomous vehicles. Annual Review of Control, Robotics, and Autonomous Systems, 2018.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. CoRR, abs/1610.03295, 2016. URL http://arxiv.org/
abs/1610.03295.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game
of go with deep neural networks and tree search. Nature, 529:484-489, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 550(7676):354, 2017.
R Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation and td learning.
arXiv preprint arXiv:1902.00923, 2019.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Csaba Szepesvari. Algorithms for reinforcement learning. Synthesis lectures on artificial intelli-
gence and machine learning, 4(1):1-103, 2010.
John N Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function
approximation. In Advances in Neural Information Processing Systems, pp. 1075-1081, 1997.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Thirtieth AAAI conference on artificial intelligence, 2016.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling
network architectures for deep reinforcement learning. In International Conference on Machine
Learning, pp. 1995-2003, 2016.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine Learning, 8(3-4):279-292, 1992.
Zhuoran Yang, Yuchen Xie, and Zhaoran Wang. A theoretical analysis of deep q-learning. arXiv
preprint arXiv:1901.00137, 2019.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. In Advances in Neural Information Processing Systems, 2019.
11
Under review as a conference paper at ICLR 2020
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. Machine Learning, 2019a.
Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for sarsa and q-learning with
linear function approximation. In Advances in Neural Information Processing Systems, 2019b.
12
Under review as a conference paper at ICLR 2020
A Proof of Theorem 5.6
Before we prove the global convergence of Algorithm 1, we present the following lemma that shows
that near the initialization point θ0, the neural network function f(θ; x) is almost linear in θ for all
unit input vectors.
Lemma A.1 (Theorems 5.3 and 5.4 in Cao & Gu (2019a)). Let θ0 = (W(01)>, . . . , W(0L)>)> be
the initial point and θ = (W(1)>, . . . , W(L)>)> ∈ B(θ0, ω) be a point in the neighborhood of θ0.
If
m ≥ Ci max{dL2 log(m∕δ),ω-"3L-8/3 log(m/(ωδ))},	and ω ≤ C2L-5(log m)-3/2,
then for all x ∈ Sd-1, with probability at least 1 - δ it holds that
L	L
if (θ; X)- f(θ; χ)∣ ≤ 31/3L8/3 Pmiogm χ ∣∣w(l) - w0l)∣∣2+c3L3√m X ∣∣w(l) - w0l)∣∣2.
l=1	l=1
Under the same conditions on m and ω, if θt ∈ B(θ0, ω) for all t = 1, . . . , T, then with probability
at least 1 一 δ,we have ∣f (θt; φ(st, at))∣ ≤ C4pl0g(T∕δ) for all t ∈ [T].
Proof of Theorem 5.6. By triangle inequality, it holds that
Q(s, a; θτ)	- Q (s, a) ≤ f (θτ ； s, a)	- f(θτ ； s, a)	+ f (θτ ； s,	a)	- f (θ	； s, a)
^, _ , 、 _ ,,	、
+ f(θ*; s,a) - Q*(s,a).	(A.1)
Recall that f (θ*; ∙, ∙) is the fixed point of ∏fT and Q* (∙, ∙) is the fixed point of T. Then We have
lf(θ* ； s,a) - Q*(s, a)| = lf(θ*; s,a) - πFθ,m Q*(s, a) +πFθ,m Q*(s,a) - Q*(s,a)l
= ΠFΘ,m T fb(θ*; s, a) - ΠFΘ,m T Q*(s, a) + ΠFΘ,m Q*(s, a) - Q*(s, a)
≤ ∣∏Fθ,mTf(θ*; s,a)- ∏Fθ,mTQ*(s,a)∣ + ∣∏Fθ,mQ*(s,a) - Q*(s,a)∣
≤ Yif(θ*; s,a) - Q*(s,a)| + ∣πFθ,mQ*(s,a) - Q*(s,a)∣,
where the first inequality follows the triangle inequality and in the second inequality we used the
fact that ΠFΘ,m T is γ-contractive. This further leads to
, .., _ , , , . _____________________________ _ , , 、 _ ,, ..
(1 - Y)if (θ*; s, a) - Q*(s, a)i ≤ l∏Fθ,mQ飞,a) - Q*(s, a)|.
To simplify the notation, we abbreviate E[∙∣θ0] as E[∙] in the rest of this proof. Therefore, we have
E(Q(s, a; θT) - Q*(s, a))2
≤ 3E[(f (θτ ； s, a) - f(θτ ； s, a))[ + 3E[(f(θτ ； s, a) - f(θ*; s, a))2]
+ 3E[(f(θ*; s,a) - Q*(s,a))2]
≤ 3E[(f (θτ ； s, a) - f(θτ; s, a))2] + 3E[(f(θτ; s, a) - f(θ*; s, a))2]
+ 3(1-Y)-2E[(∏Fθ,mQ*(s,a) - Q*(s,a))2].
By Lemma A.1 and the parameter choice that ω = Cι∕(√mL9/4), we have
E[(f(θτ; s,a) - f(θT; s, a))2] ≤ C2(ω4∕3L4Pmlog m)2 ≤ C4/3C2m-1/3 log m
with probability at least 1 - δ. Combining the above result with Theorem 5.4, we have
E(Q(s, a; θT) - Q*(s, a))2] ≤
3E[(∏Fθ,mQ*(s,a) - Q*(s,a))2]
o-τP
+ C2τ* log(T∕δ)log T + C3 log(T∕δ)log m
β2√T	βm1∕6
with probability at least 1 - 3δ - L2 exp(-C6m2/3L), which completes the proof.	□
13
Under review as a conference paper at ICLR 2020
B Proof of Supporting Lemmas
B.1 Proof of Lemma 6.1
Before we prove the error bound for the local linearization, we first present some useful lemmas
from recent studies of overparameterized deep neural networks. Note that in the following lemmas,
{Ci}i=1,... are universal constants that are independent of problem parameters such as d, θ, m, L
and their values can be different in different contexts. The first lemma states the uniform upper
bound for the gradient of the deep neural network. Note that by definition, our parameter θ is a long
vector containing the concatenation of the vectorization of all the weight matrices. Correspondingly,
the gradient Vθf (θ; x) is also a long vector.
Lemma B.1 (Lemma B.3 in Cao & Gu (2019b)). Let θ ∈ B(θ0, ω) with the radius satisfying
Cid3/2LTm-3/2 ≤ ω ≤ C2L-6(logm)-3/2. Then for all unit vectors in Rd, i.e., X ∈ Sd-1,
the gradient of the neural network f defined in (4.2) is bounded as ∣∣Vθf (θ; x)k2 ≤ C3√m with
probability at least 1 一 L2 exp(-C4ms2/3L).
The second lemma provides the perturbation bound for the gradient of the neural network function.
Note that the original theorem holds for any fixed d dimensional unit vector x. However, due to the
choice ofω and its dependency on m and d, it is easy to modify the results to hold for all x ∈ Sd-1.
Lemma B.2 (Theorem 5 in Allen-Zhu et al. (2019b)). Let θ ∈ B(θ0 , ω) with the radius satisfying
C1d3/2L-3/2m-3/2(log m)-3/2 ≤ ω ≤ C2L-9/2(log m)-3.
Then for all X ∈ Sd-1, with probability at least 1 一 exp(-C3mG2/3L) over the randomness of θo,
it holds that
∣Vθf(θ; x) - Vθf(θo;x)k2 ≤ C4ωV3L3pi⅛m∣Vθf(θo; x)∣2.
Now we are ready to bound the linearization error.
Proof of Lemma 6.1. Recall the definition of gt(θt) and mt(θt) in (4.5) and (6.2) respectively. We
have
∣gt(θt)	一	mt(θt)∣2 =	Vθf(θt; st,	at)∆(st, at, st+1; θt)	一	Vθfb(θt; st,at)∆b(st,at,	st+1;	θt)2
≤ ∣∣(Vθ f (θt; st, at) - Vθ f(θt; st, at))∆(st, at, st+1； θt 儿
+ Ilvθf (θt; st,a∕0(st, at, st+ι; θt) - ∆(st,at, st+ι; θt)) ∣∣2. (B∙D
Since fb(θ) ∈ FΘ,m, we have fb(θ) = f(θ0) + hVθf(θ0), θ 一 θ0i and Vθfb(θ) = Vθf(θ0). Then
with probability at least 1 一 2L2 exp(-Cιmω2^3L), We have
Il (Vθ f(θt; St, at) - Vθ f(θt; St, at))∆(st, at, st+1； θt) ∣∣2
=心(st,at, st+1; θt)1 ∙ Il (Vθf (θt; st, at) - vθf (θ0; st, at)) ∣∣2
≤ CQI/Lp pm log m ∣∆(st, at, st+1； θt)∣,
where the inequality comes from Lemmas B.1 and B.2. By Lemma A.1, with probability at least
1 一 δ, it holds that
∣∆(st,at,st+ι; θt)∣ = If (θt; st,at) — rt 一 Y maχf(θt; st+ι,b)∖ ≤ (2 + γ)C3√l0g(T∕δ),
b∈A
which further implies that with probability at least 1 — δ — 2L2 exp(—Cims2/3L), We have
∣∣(vθ f (θt; st, at)- vθ f(θt; / ,a/Det,at, st+i； θt) 112
≤ (2 + γ)C2C3ω1∕3L3 √m log m log(T/δ).
For the second term in (B.1), we have
∣∣ Vθf(θt; st, at)(∆(st, at, st+1； θt) — ∆(st, at, st+1； θt))∣∣2
14
Under review as a conference paper at ICLR 2020
≤ ∣∣Vθ f(θt; st, at)f (θt; st, at) — f(θt; st, at)) ∣∣2
—^, _ , / . , _ 一、
+ ∣∣Vθ f (θt; st, at) (max f (θt; st+1 ,b) - max f (θt; st+ι, b
≤ ∣∣Vθ f@； st，at)∣∣2 ∙ f (仇；st，at) — f(θt; st,电)|
+ Vθfb(θt; st, at)k2max f(θt; st+1,b) - fb(θ; st+1, b).
)2
(B.2)

By Lemma A.1, with probability at least 1 — δ we have
|f (θt; st,at) — f(θt; st,at)∣ ≤ g4/3L11/3Pmlog m + C4ω2L4√m,
for all (st, at) ∈ S × A such that kφ(st, at)k2 = 1. Substituting the above result into (B.2)
and applying the gradient bound in Lemma B.1, we obtain with probability at least 1 — δ —
L exp(-Cιmω2∕3L) that
∣∣ vθf (θt; st, at) (δ(St, at, st+i; θt) — δ(St,at, st+i; θt)) ∣∣2
≤ C5S4/3L11/3mPlog m + C6ω2L4m.
Note that the above results require that the choice of ω should satisfy all the constraints in Lemmas
B.1, A.1 and B.2, of which the intersection is
C7d3/2L-1 m-3/4 ≤ ω ≤ C8L-6(log m)-3.
Therefore, the error of the local linearization of gt(θt) can be upper bounded by
∣hg(θt) — m(θt), θt — θ*i∣ ≤ (2 + Y)C203ω1∕3L3PmogmogT∕δ)||仇 — θ*∣∣2
+ (C5ω4∕3L11∕3mPlog m + C6ω2L4m)∣∣θt — θ*∣∣2,
which holds with probability at least 1 一 2δ 一 3L2 exp(—Cims2/3L) over the randomness of the
initial point. For the upper bound of the norm ofgt, by Lemmas B.1 and A.1, we have
kgtk2 = ∣∣Vθf(θt; st,at)f(θt; st,at) — rt — γmaxf(θt; st+1, b)∣∣∣
≤ (2 + γ)C9 Pm log(T∕δ)
holds with probability at least 1 一 δ 一 L2 exp(—Cims2/3L).	□
B.2 Proof of Lemma 6.2
Let us define Zt(θ) = hmt(θ) — m(θ), θ 一 θ*), which characterizes the bias of the data. Different
from the similar quantity ζt in Bhandari et al. (2018), our definition is based on the local linearization
of f, which is essential to the analysis in our proof. It is easy to verify that E[mt(θ)] = m(θ) for
any fixed and deterministic θ. However, it should be noted that E[mt(θt) ∣θt = θ] = m(θ) because
θt depends on all historical states and actions {st, at, st-ι, at-ι,...} and mt(∙) depends on the
current observation {st, at, st+1} and thus also depends on {st-1, at-1, st-2, at-2, . . .}. Therefore,
we need a careful analysis of Markov chains to decouple the dependency between θt and mt(∙).
The following lemma uses data processing inequality to provide an information theoretic control of
coupling.
Lemma B.3 (Control of coupling, (Bhandari et al., 2018)). Consider two random variables X and
Y that form the following Markov chain:
X → st → st+τ → Y,
where t ∈ {0, 1, 2, . . .} and τ > 0. Suppose Assumption 5.2 holds. Let X0 and Y0 be independent
copies drawn from the marginal distributions of X and Y respectively, i.e., P(X0 =
P(X = ∙) 0 P(Y = ∙). Then for any bounded function h : S ×S → R, it holds that
∣E[h(X,Y)] 一 E[h(X0,Y0)]∣ ≤ 2sup ∣h(s,s0)∣λρτ.
Y0 = ∙)
15
Under review as a conference paper at ICLR 2020
Proof of Lemma 6.2. The proof of this lemma is adapted from Bhandari et al. (2018), where the
result was originally proved for linear function approximation of temporal difference learning. We
first show that ζt(θ) is Lipschitz. For any θ, θ0 ∈ B(θ0, ω), we have
Zt(θ) - Zt(θ0) = hmt(θ) - m(θ), θ - θ*i-hmt(θ0) - m(θ0), θ - θ*>
=hmt(θ) - m(θ) - (mt(θ0) - m(θ0)), θ - θ*i
+ hmt(θ0) - m(θ0), θ - θ0i,
which directly implies
∣Zt(θ) - Zt(θ0)∣ ≤ kmt(θ) - mt(θ0)k2 ∙kθ - θ*k2 + km(θ) - m(θ0)k2 ∙kθ - θ*k2
+ kmt(θ0)- m(θ0)k2 ∙∣∣θ - θ0∣∣2.
By the definition of mt, we have
kmt(θ) - mt(θ0)k2
=∣∣Vθf(θo)((f(θ; s,a) - f(θ0; s, a)) - γ(maxf(θ; s0, b) - maxf(θ0; s', b)))(
≤ (1 + γ)C32mkθ - θ0k2,
which holds with probability at least 1 - L2 exp(-C4mω2/3L), where We used the fact that the
neural network function is LiPschitz with parameter C3√m by Lemma B.1. Similar bound can also
be established for kmt(θ) - mt(θ0)k in the same way. Note that for θ ∈ B(θ0, ω) with ω and m
satisfying the conditions in Lemma 6.1, we have by the definition in (6.2) that
I∣mt(θ)k2 ≤ (lf(θ; s,a)l + r(s,a) + YlmaXf(θ; s0, b)∣) ∣∣Vθf(θ)k2
≤ 2(2 + Y)(lf(θo)l + kVθf(Θ0)k2 ∙∣θ - θ0k2)kVθf(Θ0)k2
≤ 2(2 + γ)C3(C8√mplog(T∕δ) + C3mω).
The same bound can be established for ∣m t∣ in a similar way. Therefore, we have ∣Zt(θ)-Zt(θ0 )| ≤
'm,L∣θ - θ0∣2, where 'm,L is defined as
'm,L = 2(1 + Y )C2mω + 2(2 + γ) C3 (C8 √m plog(T∕δ) + C3mω).
Applying the above inequality recursively, for all τ = 0, . . . , t, we have
t-1
ζt(θt) ≤ Ct(θt-τ) + 'm,L X kθi+1 - θik2
i=t-τ
t-1
≤ Zt(θt-τ) + 2(2 + Y)C3(C8√mpiog(τ∕δ) + C3mω)'m,L X m.	(B.3)
i=t-τ
Next, we need to bound ζt (θt-τ). Define the observed tuple Ot = (st, at, st+1) as the collection of
the current state and action and the next state. Note that θt-τ → st-τ → st → Ot forms a Markov
chain induced by the target policy ∏. Recall that mt(∙) depends on the observation Ot. Let's rewrite
m(θ, Ot) = mt(θ). Similarly, we can rewrite ζt(θ) as ζ(θ, Ot). Let θt0 and Ot0 be independently
drawn from the marginal distributions of θt and Ot respectively. Applying Lemma B.3 yields
E[Z(θt-τ,Ot)] - E[Z(θ'-τ, O0)] ≤ 2sup IZ(θ, O)∣λρτ,
θ,O
where we used the uniform mixing result in Assumption 5.2. By definition θt0-τ and Ot0 are inde-
pendent, which implies E[m(θ0, O0) ∣Θ0] = m(θ0) and
E[ZWt-T, O0)] = E[E[hm(θ0, O0) - m(θ0), θ0 - θ*i]∣θ0] = 0.
Therefore, for any τ = 0, . . . , t, we have
t-1
E[Zt(θt)] ≤ EZt(θt-τ) + 2(2 + Y)C3(C8√mpiog(T∕δ) + C3mω)'m,L X η
i=t-τ
16
Under review as a conference paper at ICLR 2020
≤ 2supλρτ + 2(2 + γ)C3(C8√mVzlog(T/δ) + C3mω)'m,Lτηt-τ.	(B.4)
Define T* as the mixing time of the Markov chain that satisfies
T* = min{t = 0,1, 2,... ∣λρt ≤ ητ}.
When t ≤ τ* , we choose τ = t in (B.4) and obtain
E[Zt(θt)] ≤ E[Zt(θo)]+ 2(2 + Y)C3(C8√mplog(T∕δ) + C3mω)'m,LT*ηo
=2(2 + γ)C3 (C8√m√log(T∕δ) + C3mω)'m,L τ *ηο,
where we used the fact that the initial point θ0 is independent of {st, at, st-1, at-1, . . . , s0, a0} and
thus independent of Zt(∙). When t > T*, we can choose T = T* in (B.4) and obtain
E[Zt(θt)] ≤ 2ητ + 2(2 + γ)C3(C8√m√log(T∕δ) + C3mω)'m,LT*r∣t-τ*
≤ Ce(m log(T∕δ) + m2ω2)t*r∣t-τ*,
where C > 0 is a universal constant, which completes the proof.	□
B	.3 Proof of Lemma 6.3
ProofofLemma 6.3. To simplify the notation, We use En to denote Eμ,∏,p, namely, the expectation
over S ∈ μ, a 〜 π(∙∣s) and s0 〜 P(∙∣s, a), in the rest of the proof. By the definition of m in (6.2),
we have
hm(θ) - m(θ*), θ - θ*i
=En [(∆(s, a, s0; θ) - ∆(s, a, s0; θ*)) (Vef (θ°; s, a), θ - θ*i]
=En [(f(θ; s, a) - f(θ*; s, a)) (Vef (θ°; s,a), θ - θ*i]
- γEn h max fb(θ; s0,b) - maxfb(θ*; s0, b)hVθ f (θ0; s, a),θ - θ*ii,
b∈A	b∈A
1	.1 r'	J	1 .1	X-Z R/八、	X-Z ，/八 ∖ ∕'	11 八一 X-X.	一 T-
where in the first equation we used the fact that Vθf(θ) = Vθf(θ0) for all θ ∈ Θ and f ∈ FΘ,m.
Further by the property of the local linearization of f at θ0, we have
^, _ 、 ^, _ , 、 ____________ , _ , _ ..
f (θ; s, a) - f (θ*; s, a) = hVθf (θo; s, a), θ - θ*i,	(B.5)
which further implies
E[f(θ; s, a) - f(θ*; s, a)) (Vef (θ°; s, a), θ - θ*i∣θo]
=(θ - θ*)>E[Vθf (θo; s, a)Vθf (θo; s, a)>∣θo](θ - θ*)
=mkθ-θ*k2Σπ.
where Σn is defined in Assumption 5.3. For the other term, we define bmax(θ) =
argmaxb∈A f(θ; s0, b) and bmax (θ*) = argmaxb∈A f(θ*; s0, b). Then we have
En h max fb(θ; s0, b) - maxfb(θ*; s0, b)hVθf(θ0; s, a), θ - θ*ii
=En [(f(θ; s0, bmax) - f(θ*; s0, b；Lx)) Bef ®； S a), θ - θ*i].	(B.6)
For all (s, a, s0), when hVθf(θ0; s, a), θ - θ*i ≥ 0, (B.6) can be upper bounded by
f(θ; s0, bmax) - f(θ* ； s0, bmax)) (Vθ f @； s, a), θ - θ*>
=f(θ; s0, bmax) - f(θ*; s0, bmax) + F®； s0, bmax) - f(θ*; s0, bmnaχ)) (Vθ f ®； s, a), θ - C
≤ f(θ; s0, bmax) - f(θ*; s0, bmax)) Nef 但0； s, a), θ - θ*
=(θ - θ*)>Vθf (θo; s0, bmax)Vθf (θo; s, a)>(θ - θ*)
≤ ∣(θ - θ*)>Vef (θo; s0, bmax)∣∙∣Vθf (θo; s, a)>(θ - θ*)∣,
17
Under review as a conference paper at ICLR 2020
where the inequality comes from the optimality of bjnax and the last equality follows the fact that
f (θ; ∙, ∙) is linear. When (V^ f (仇；s, a), θ — θ ∖ < 0, using the same argument, we can upper
bound (B.6) as follows
f(θ; s0, bmax) — f(θ*; s0, %ax)) "θ f @； S, a), θ — θ*>
=(f (θ; S, bmax) — f (θ; S, %ax) + f ⑹ S, b↑asχ) — f (θ*; S, b^x)) "θ f (θ° ； S, a), θ — θ*)
≤ (f(θ; S, bmax) — f(θ*; S, b；„ax)) hVθf(θo; S, a), θ — θ*i
≤ l(θ — θ*)>Vθf (θ0; S, bjnax)∣ ∙ ∣vθf (θ0; S, a)τ(θ — θ*)∣.
Combining the above result, we have for all tuples (s, a, S0) it holds that
(f(θ; S, bmax) — f(θ*; S, bnnax)) "θ f @； S,a), θ — θ*>
≤ l(θ — θ*)τVθf (θ0; S, bmax)∣ ∙ ∣Vθf (θ0; S, a)τ(θ — θ*)∣1 +
+ ∣(θ — θ*)τVθf (θ0; S, bmax)∣ ∙ ∣Vθf (θ0; S, a)τ(θ — θ*)∣1一,
where we denote 1+ = 1{(Vef (θ0; s, a), θ—θ*) ≥ 0} and 1- = 1{"ef (θ0; s, a), θ — θ*〉< 0}.
Taking expectation over the above inequality and applying Cauchy-Schwarz inequality, we have
E”,∏,p [(f(θ; S, bmax) — f(θ*; S, bmax)) Ce f ®； S, a), θ — θ*>]
≤ JEn [(m^x ∣(θ — θ*)τVθf (θ0; S, b)∣)2] ,E∏ [(Vθf (θ°; S,a)τ(θ — θ*))2]
=mkθ — θ*ll∑∏(θ-e*)llθ — θ*ll∑∏,


where we used the fact that Σ∏(θ — θ*) = 1∕mEμ[Vef(θ°; s,emax)Vef(θ°; s,emax)τ] and
bmax = argmaxb∈/ ∣(Vef (θ0; s, b), θ — θ*>∣ according to (5.6). Substituting the above results
into (B.6), we obtain
E∏ [(maχ f(θ; S, b) — maχ/(θ*; S, b)) "ef (θ°; s, a), θ — θ*>]
b∈A-	b∈A-
≤ mkθ — θ*ll∑∏(θ-e*)llθ — θ*ll∑∏,
which immediately implies
hm(θ) — m(θ*), θ 一 θ*i ≥ m∣∣θ - θ*∣∣∑∏ ∙ (∣∣θ - θ*∣∣∑∏ — kθ 一 θ*∣∣∑∏(e-e*))
∣∣θ — θ*∣∣∑ — γ2∣∣θ — θ*∣∣∑*9 *
=mkθ — θ*∣∣∑∏ ∙	J	凡(θ-θ)
l∣θ 一 θ ll∑π + Yl∣θ 一 θ k∑∏(e-e*)
≥ m(1 — α-1∕2)kθ — θ*良
=(1 — α-1∕2)E[(f(θ) — f(θ* ))2∣θ0],
where the second inequality is due to Assumption 5.3 and the last equation is due to (B.5) and the
definition of Σ∏ in (5.5).	□
18