Under review as a conference paper at ICLR 2020
An efficient homotopy training algorithm for
NEURAL NETWORKS
Anonymous authors
Paper under double-blind review
Ab stract
We present a Homotopy Training Algorithm (HTA) to solve optimization prob-
lems arising from neural networks. The HTA starts with several decoupled sys-
tems with low dimensional structure and tracks the solution to the high dimen-
sional coupled system. The decoupled systems are easy to solve due to the low
dimensionality but can be connected to the original system via a continuous ho-
motopy path guided by the HTA. We have proved the convergence of HTA for
the non-convex case and existence of the homotopy solution path for the convex
case. The HTA has provided a better accuracy on several examples including VGG
models on CIFAR-10. Moreover, the HTA would be combined with the dropout
technique to provide an alternative way to train the neural networks.
1	Introduction
Deep learning methods, as the rising star among all machine learning methods in recent years, have
already had great success in many applications. Many advancements Cang et al. (2018); Cang
& Wei (2018); Yin et al. (2018; 2016) on deep learning have been made in the last few years.
However, as the size of new state-of-the-art models keep growing larger and larger, they rely more
and more heavily on efficient algorithms for training and making inferences from such models. This
clearly puts strong limitations on the application scenarios of nerual netowrks on roboticsKonda
et al. (2012), auto-pilot automobiles Hammerla et al. (2016), and aerial systems Maire et al. (2014).
At present, there are two big challenges in fundamentally understanding deep neural networks:
•	How to efficiently solve the highly nonlinear and non-convex optimization problems that
arise during the training of a deep learning model?
•	How to design a deep neural network structure for specific problems?
In order to solve these challenges, in this paper, we will present a new training algorithm based
on homotopy continuation method Bates et al. (2011; 2008); Morgan & Sommese (1987) which
has been successfully used to study nonlinear problems such as nonlinear differential equations
and optimizations Hao et al. (2012); Hao & Harlim. In order to tackle the nonlinear optimization
problem in neural networks, the homotopy training algorithm is designed and shows efficiency and
feasibility for both convolutional and fully connected neural networks with complex structures. The
idea of the HTA is to start with a simplified optimization problem and construct a continuous path
to the original problem. Specifically, let’s consider the following general optimization problem
min f (θ) where θ ∈ Rn.	(1)
We then divide θ in two groups, namely, θ = θ1 ∪ θ2, where θ1 ∈ Rn1 and θ2 ∈ Rn2 (n1 +n2 = n).
(Although that it can be easily extended to any m decompositions, we use m = 2 as an illustration
of the HTA idea.) Then we first solve the following decoupled simplified optimization problems
.,_ ~ . . , ~ _ .
minf(θ1,θ2) and minf(θ1,θ2)	(2)
where θ = θ1 ∪ θ2 is a random chosen point in Rn for instance we choose θ = 0 in the dropout
Srivastava et al. (2014). Second we define the following homotopy function
H(Θ,L) = f(θ) + Lkθι - θ1k2 + f(η) + Lkη2 - θ2k2 + 1 kθ - ηk2,	(3)
L
1
Under review as a conference paper at ICLR 2020
where Θ = (θ, η) and L ∈ (0, ∞) is the homotopy parameter. When L → ∞, we have two
decoupled optimization problems which we have already solved in the first step; when L → 0,
we recover the original optimization by forcing θ = η. By gradually increasing parameter L, we
are able to construct a continuous path Θ(L)between two decoupled low-dimensional optimization
problems and the original optimization problem.
Remark The HTA can be reviewed in point view of nonlinear equations by considering the neces-
sary condition of the optimization problem (1):
/ Vθ2 f(θ)∖
, .. ~
H…("θ) 1 + L f
η2 - θ2
When L is large, θ and ηι can be solved by ▽6?/(θ) = 0 and Vm f (η) = 0 respectively. When
L is small, We have the solution of V f (θ) = 0 by the diagonal homotopy θ 一 η = 0. This
technique has been widely used in solving systems of nonlinear equations especially for polynomial
systems Bates et al. (2008); Wampler & Sommese (2005). Since the θ is randomly chosen, the
universal approximation theory Park & Sandberg (1991) can guarantee that the random point θ in
Rn is alWays connected to the solution of Vθf(θ) = 0. In another Word, the solution path Θ(L)
alWays exists or the probability that Θ(L) is not connected is zero Wampler & Sommese (2005).
2	Applications of HTA on Neural Network
2.1	Convolutional neural networks
In this subsection, We Will specify the variable θ and the objective function f(θ) of (3) in the convo-
lutional neural netWork (CNN) setup.
Definition 2.1. Suppose we have two tensors, Al×m×n = (ai,j,k)1≤i≤l,1≤j≤m,1≤k≤n,
Bc×l×m0 ×n0 = (bh,i,j,k)1≤h≤c,1≤i≤l,1≤j≤m0,1≤k≤n0, where c, m, n, m0,n0, l ∈ N and m0 ≤
m, n0 ≤ n. The convolution of two tensors is defined by the following:
00
lmn
A * Bh,j,k =	ai0,j0+j-1,k0+k-1 • bh,i0,j0,k0,	(4)
where B is called the convolutional kernel.
Then With the input x and the output y , the CNN is Written as
y(x; θ) = σ(σ(…σ(σ(x * Wi + bi) * W2 + b2)--+ bn-i) * Wn + bn)	(5)
Where Wi ∈ Rci ,li ,mi ,ni and σ is the activation function. Then We have θ = {Wi , bi }in=1 Which can
be easily divided into two subgroups θj = {Wj, bi}n=ι where Wij ∈ R号,li,mi,ni. The objective
function f(θ) in (3) is defined by the cross entropy loss function GoodfelloW et al. (2016) and
defined as L(χ,y) = -x(y) + log(Pj ex[j]). Due to the random algorithms in machine learning
Bottou (2010); Kingma & Ba (2014), the homotopy function in (3) can be rewritten as H(Θ; ξ, L)
where ξ is a random variable due to random algorithms.
2.2	Fully connected neural networks
Similar to the CNN, a fully connected neural network with n hidden layers is written as
y(x, θ) = σ(Wnσ(Wn-iσ(W2σ(Wix + bi) + b2) • • • + bn-i) + bn),	(6)
where Wi ∈ Rn×di, βi ∈ Rdi, n is the dimension of the input x and di is the width of the i-th layer.
Then we have θ = {Wi, bi}in=i which can be easily divided into two subgroups θi = {Wi, bi}in=/2i
and θ2 = {Wi, bi}in=n/2+i. Or we can use the dropout technique to randomly split θ as θi and
θ2 . Similarly, the homotopy function in (3) can be also rewritten as H(Θ; ξ, L) due to the random
algorithms Bottou (2010); Kingma & Ba (2014).
2
Under review as a conference paper at ICLR 2020
3	Theoretical Analysis
In this section, we will provide a theoretical analysis of the HTA by using stochastic gradient scheme
to solve (3) for each L. We redefine our stochastic gradient scheme for the homotopy process as
θk+1 = θk - γkG(θk ; ξk , Lk),	(7)
where G(θ; ξ, L) := VθF(θ; ξ, L), Yk is the learning rate, Lo = M, Lk % 0 and M is a given
large number. Then we consider the expectation of the homotopy function as Eξ(H(θ; ξ, L)), for
simplicity, and denote Eξ [H(θ; ξ, L)] as g(θ; L).
3.1	CONVERGENCE OF THE HTA FOR ANY GIVEN L IN THE NON-CONVEX CASE
Theorem 3.1. (Nonconvex Convergence) If {θk} is contained in a bounded open set, supposing
that (7) is run with a step size sequence satisfying
∞∞
γk = ∞ and	γk2 < ∞,	(8)
k=1	k=1
then we have
1K	K
E[ A~ X Yk llvg(θk; L) k 2 ] → 0 as K → ∞ with Ak := X Yk .	(9)
k k=1	k=1
Proof. First, we prove the Lipschitz-continuous objective gradients condition Bottou et al. (2018),
which means that g(θ; L) is C1 and Vg(θ; L) is Lipschitz continuous:
• g(θ; L) is C 1. Since y(xξ; θ) ∈ C1 and L(∙, y) is C1, we have that H(θ; ξ,L) ∈ C1 or that
VθH(θ; ξ, L) is continuous. Considering
Vθg(θ; L) = VθEξ(H(θ; ξ,L))= Eξ(VθH(θ; ξ, L)),	(10)
we have that Vθg(θ, L) is continuous or that g(θ; L) ∈ C1.
• Vg(θ; L) is Lipschitz continuous. Since Vθg(θ; L) = Eξ(VθH(θ; ξ, L)), we only need
to prove Eξ(VθL(y(xξ; θ), yξ)) is Lipschitz continuous. In fact
vθL(y(xξ; θ),yξ) = vχL(y(xξ; θ),yξ) ∙ vθy(xξ; θ)	(II)
we will show that VxL(y(xξ; θ), yξ) and Vθy(xξ; θ) are bounded and Lipschitz continu-
ous. Since that both σ(x)=[+；-X and σ0(x) = σ(x)(1-σ(x)) are bounded and Lipschitz
continuous, and that {θk} is in an open set (assumption of Theorem 3.1), Vθy(xξ; θ) is
bounded and Lipschitz continuous. (xξ is bounded because the size of our dataset is finite.)
By differentiating L(x, y), we have
ex1	exn
VxL(X, y) = (-δy + q~~X- , ∙∙∙ , -δy + q~~X- ),	(12)
y	j exj	y	j exj
where δ is the Kronecker delta. since
exi Pj exj -(exi)2
---Qxj、2------ k = i
-exi e篷	k = i	(13)
(Pj exj )2	k= i,
which implies that ∣ 嬴 Pexexj ∣ ≤ 2, We have VxL(∙,y) is Lipschitz continuous and
bounded. Therefore VxL(y(xξ; θ), yξ) is Lipschitz continuous and bounded. Therefore
we have that Vg(θ; L) is Lipschitz continuous.
Second, we prove the first and second moment limits condition Bottou et al. (2018):
∂	exi
∂xk Pj exj
3
Under review as a conference paper at ICLR 2020
a.	According to our theorem assumption, {θk } is contained in an open set which is bounded.
Since that g is continuous, g is bounded;
b.	Since that G(θk; ξk, L) = VθF®; ξk,L) is continuous, We have
Eξk [G(θk； ξk, L)] = vθEξk [H(θk； ξk, L)] = vθg(θk； L).	(14)
Therefore
Vg(θk; L)TEξk [G(θk; ξk,L)] = Vg(θk; L)T∙Vg(θk; L) = ∣∣Vg(θk; L)k2 ≥ u∣∣Vg(θk; L)k2
(15)
for 0 < u ≤ 1. On the other hand, We have
kEξk[G(θk； ξk, L)]k2 = kVg(θk; L)k2 ≤ UGkVg(θk; L)k2	(16)
for uG ≥ 1.
c.	Since that VH(θk; ξk, L) is bounded for a given L, We have Eξk [kVH (θk; ξk, L)k22] is also
bounded. Thus
Vξk [G(θk; ξk, L)] := Eg% [∣∣G(θk; ξk, L)k2]-kEξk [G(θk; ξk, L)]k2 ≤ Eξ% [∣∣G(θk; ξk, L)k2],
(17)
Which implies that Vξk [G(θk; ξk, L)] is bounded.
We have checked assumptions 4.1 and 4.3 in Bottou et al. (2018). By theorem 4.10 in Bottou et al.
(2018), With the diminishing stepsize, namely,
∞∞
γk = ∞ and	γk2 < ∞,	(18)
the folloWing convergence is obtained
1K
E[ AfY kVg(θk; L)k2] → 0 as K →∞.	(19)
Ak k=1
□
3.2 EXISTENCE OF SOLUTION PATH θ(L) FOR THE CONVEX CASE
Second We explore the existence of solution path θ(L) When t varies from M to 0 for the convex case
theoretically. This is quite difficult for the non-convex case although We may find it numerically.
HoWever, theoretical analysis of the convex case Would also give us some guidance on hoW the HTA
obtains an optimal solution for L = 0.
Theorem 3.2. Assume that g(∙, ∙) is convex and differentiable and that ∣∣G(θ; ξ,L)k ≤ Mo. Then
for stochastic gradient scheme (7), we have
lim E[g(θn,Ln)]= g(θ1,0),	(20)
n→∞
where θn = ?三0；；，k and Ln = PPknO：kLk.
Proof.
E[kθk+1 — θLk+1 k2] =	E[kθk- YkG(θk； ξk,Lk) — θL k2]
-2E[hθk — γkG®; ξk, Lk)- θL,θLk+1 — θLi]
+E[kθLk+1 — θL k2].	(21)
By defining
Ak = -2E[{0k — γk G®; ξk,Lk) — θLk ,θLk+1 — θLk i]+ E[kθLk+1 — θLk ∣∣2],	(22)
4
Under review as a conference paper at ICLR 2020
and assuming that Pkm=0 Ak ≤ A < ∞, we obtain
E[kθk+1 — θLk+1k2] = E[kθk - YkG(θk； ξk,Lk) - θLk k2]+ Ak
=E[kθk - θLk k2] - 2γkE[(G(θk; ξk,Lk),θk - θLki] + γ2E[∣∣G(θk； ξk, Lk)k2] + Ak
≤ E[kθk - θLkk2] - 2γkE[hG(θk；ξk,Lk),θk - θLki] + YkM2 + Ak
Since that
E[hG(θk; ξk, Lk),θk - θLki] = Eg。,…"i [Eg% [hG(θk; ξk, Lk),θk - θLki∣ξo,…，ξk-ι]]
=Eg。,…,ξk-JhVg(θk; Lk),θk-θLki∣ξo,…，ξk-ι]
=Elhvg(θk; Lk), θk - θLki],	(23)
we have
E[kθk+1 - θLk+1 k2] ≤ E[kθk - θLk k2] - 2γkE[hVg(θk; Lk),θk - θLki] + YkM2 + Ak.	(24)
Due to the convexity of g(∙,Lk), namely,
hvg(θk , Lk ), θk - θLk i ≥ g(θk ; Lk ) - g(θLk ; Lk ),	(25)
we conclude that
E[kθk+1 - θLk+1 k2] ≤ E[kθk - θLk k2] - 2γkE[g(θk； Lk) - g(θLk,Lk)]+ YkM2 + Ak,	(26)
or
2YkE[g(θk; Lk)- g(θLk ; Lk)] ≤ -E[kθk + 1 - θf k+1 k2 - kθk - θLk k2] + YIkMM + Ak∙
By summing up k from 0 to n,
n
2 X YkE[g(θk; Lk ) - g(θLk ; Lk )]	≤
k=0
≤
nn
-E[kθn+1 - θLn+1 k2 - kθ0 - θ0k2] + M2 X Yk + X Ak
k=0	k=0
nn
D2 + M2 X Yk2 + X Ak,	(27)
k=0	k=0
where D = ∣∣θo - θ01∣. Dividing 2 p；=。Yk on both sides, We have
1
vɔn
k=0 Yk
n
X YkE[g(θk； Lk) - g(θLk; Lk)] ≤
k=0
d2+M2 pn=o Y2+pn=o Ak
2 pn=o Yk
D2 + M2 Pn=0 Yk + A
2 pn=o Yk
According to the convexity of g(∙; ∙) and Jensen's inequality Jensen (1906),
1
vɔn
k=0 Yk
n
X YkE[g(θk； Lk)] ≤ E[g(θn; Ln)],
k=0
(28)
where θn = PPknO:心 and L； = PPnO YYLk.
Then we have
旧历®； ； Ln)] - Pn=Png(θLk; Lk)
k=0 Yk
≤ D2 + M2 Pn=0 y2+ A
-	2 Pn=o Yk
(29)
We choose Yk such that P；=。Yk = ∞ and P；=。Yk < ∞, for example, Yk = k. Taking n to
infinite, we have
0 ≤ nl→m∞ E[g(θn,tn)]- ；^	k ≤ 0∙
Because Lk % 0, g(∙, ∙) is continuous and P；=。Yk = ∞, then
nl→∞ Pn=Pn=0θYkk,Lk)=g(θ0, 0),
which implies that
lim E[g(^n,Ln)] = g(θ0,0).
n→∞
(30)
(31)
(32)
□
5
Under review as a conference paper at ICLR 2020
Figure 1: Two-dimensional highly non-convex function
4 Numerical experiments
4.1	A Non-Convex example
We first use the following multi-dimensional highly non-convex example Chow et al. (2013) to
illustrate the feasibility and efficiency of the HTA.
minf(x) = (π∕n) {ksin2(πyι) + X(yi - A)2[1 + ksin2(πyi+1)] + (yn - A)2} ,	(33)
where
X = (xι, ∙∙∙,Xn) ∈ Rn, yi = 1 + (xi — 1)/4, i = 1,…，n and k = 10, A = 1.	(34)
The function has roughly 5n local minima in the region {|xi| < 10} and a unique global minimum
located at Xi = 1, i = 1, ∙∙∙ ,n. (See Figure 4.1 for an illustration of n = 2.)
The idea of the homotopy setup of this problem is splitting the n dimensional optimization problem
into two n/2 dimensional problems which can be optimized separately and are tracked to an optimal
of solution of f(X) with respect to the homotopy parameter L. Then the objective function of the
homotopy method is written as follows:
F(χ,y; L) = f(χ) + Lk(Xι,…,χ2) - (χ0,…,χ⅜)k2 + g(y) + Lk(yn+1,…,yn) - (y0+1,…,yn)k2
+ L kχ - yk2,
L	(35)
where
x = (χι,…,χn) ∈ Rn, y = (yi,…,yn) ∈ Rn,	(36)
(χi, ∙∙∙ ,xfn) ∈ R2, (yn+i, .一，yn) ∈ Rn are fixed and chosen based on the initial guess. When
L is large, F is equivalent to optimize
X2m,in,xng(X0,…，吗,x2 + 1,…，xn) + yιminn g(y1,…，y2,yn+1,…
(37)
which can be solved separately. When L becomes small, the last term of (35) forces X and y to be
equal. Then F(X, y; L) is equivalent to optimizing f(X) which is the original n-dimensional opti-
mization problem. Therefore F(X, y; L) connects two optimization problems by gradually decreas-
ing penalty factor L and builds a continuous path between the decoupled systems and the original
optimization function f (X).
6
Under review as a conference paper at ICLR 2020
Dimensions (n)	Initial Values	QUaSi-newton with the ho- motopy setup	Quasi-newton
2	[0,1,…，5]2 (62 Points)	366	24
4	[θ, 1, ∙∙∙ , 5]4 (64 points)	^I296	864
6	—	[3, 4, 5, 6]6 (46 points)	4096	—	991	—
Table 1: Finding global minimums by two algorithms
Base	Model Name	Original Error Rate	Error Rate with HTA	Rate of Imrpove- ment(ROIs)
VGG11	7.83%	7.02%	10.34%
VGG13	5.82%	5.14%	11.68%
VGG16	6.14%	5.71%	7.00%
VGG19	6.35%	5.88%	7.40%
Table 2: The results of HTA and traditional method of VGG models on CIFAR-10
We compared the traditional optimization method (the quasi-Newton method) and the quasi-Newton
with the homotopy setup. For n = 4, we chose the same initial value [5, 4, 5, 5] and set L from 0.9 to
0.1 for the quasi-newton with the homotopy setup. It takes 8 steps and 14 steps for the quasi-newton
method and the quasi-newton with the homotopy setup to converge, respectively. However, the
quasi-newton method does not find the global minimum while the HTA does. We also compared two
algorithms with different initial values and count the times when converging to global minimums.
Table 1 shows the results of n = 2, n = 4, n = 6 and n = 8.
4.2 Applications on VGG
Secondly, we applied the HTA to both CNN and fully connected neural networks on the CIFAR-
10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html) with the training and
validation datasets from the CIFAR-10 website. In terms of the pre-processing step: for training
dataset, we used random horizontal flip, random crop and normalization; for the validation dataset,
we just used the same normalization. We implemented the HTA with stochastic gradient descent
(SGD) method on VGG11, VGG13, VGG16 and VGG19 as our base models with batch normaliza-
tion Simonyan & Zisserman (2014). In order to compare the SGD with the HTA method, we set all
the regular parameters (e.g. learning rate, batch size, momentum, epochs, weight decay and etc) to
be exactly the same, varied the homotopy parameter L from L = 0.01 to L = 0.005 with a stepsize
∆L = 0.001. For each L, we run 4 epochs with the SGD method. The code for this method can be
found at https://github.com/Bill-research/homotopy which has been run on the
DGX-1 (p100 version) with 8 p100 GPUs and Tesla structures. Figure 2 shows the comparison of
validation loss between the HTA with fully connected neural networks and traditional method on
the VGG13 model. The HTA has a less error rate (5.14%) than the traditional method (6.35%) by
improving 11.68%. All the results for different models are shown in Table 2. It is clearly seen that
accuracies of HTA for different models are better than that of the traditional method. For example,
vgg11 with HTA gets 7.02% error rate while traditional method only gets 7.83%; The vgg16 with
HTA has an error rate of 5.88% while traditional method only has 6.35%. We run the numerical
experiments for 5 times for each model and the results are shown in Table 3. We also applied HTA
to the only CNN part of two VGG models: vgg11 and vgg13. Figure 3 shows the validation accu-
racies of two models which has been improved by the HTA (91.61% (vgg11) and 93.09% (vgg13))
comparing with the traditional method (91.01% (vgg11) and 92.29% (vgg13)).
Base	Model Name	mean	stddev	max
VGG11	91.53%	0.09%	91.66%
VGG13	92.76%	0.22%	93.09%
Table 3: The validation accuracies of HTA method of VGG models on CIFAR-10
7
Under review as a conference paper at ICLR 2020
10
(％)①看α」。匕山
Figure 2: Comparison of error rate of VGG13 between HTA and the traditional method.
Figure 3: The performance of the HTA on the CNN part of vgg11 (left) and vgg13 (right) and
comparisons with the traditional method
5	Conclusion
In this paper, we developed a homotopy training algorithm for solving the optimization problem
arising from neural networks. This algorithm starts from decoupled low dimensional systems and
gradually transforms to the coupled original system with complex structure. Then the complex
neural networks can be trained by the HTA with a better accuracy. The convergence of the HTA
for any given L is proved for the non-convex optimization. Then existence of solution path θ(L) is
demonstrated theoretically for the convex case although that it exists numerically in the non-convex
case. Several numerical examples have used to demonstrate the efficiency and feasibility of HTA.
The application of HTA to VGG models on CIFAR-10 provides a better accuracy than the traditional
method. Moreover, the HTA method would provide a new way to couple with the dropout techniques
to improve the training accuracy which we will explore in the future.
References
D. Bates, J. Hauenstein, A. Sommese, and C. Wampler. Adaptive multiprecision path tracking.
SIAM Journal on Numerical Analysis, 46(2):722-746, 2008.
8
Under review as a conference paper at ICLR 2020
D. Bates, J. Hauenstein, and A. Sommese. Efficient path tracking methods. Numerical Algorithms,
58(4):451-459, 2011.
L. Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMP-
STAT’2010, pp. 177-186. Springer, 2010.
L. Bottou, F. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. Siam
Review, 60(2):223-311, 2018.
Z. Cang and G. Wei. Integration of element specific persistent homology and machine learning
for protein-ligand binding affinity prediction. International journal for numerical methods in
biomedical engineering, 34(2):e2914, 2018.
Z. Cang, L. Mu, and G. Wei. Representability of algebraic topology for biomolecules in machine
learning based scoring and virtual screening. PLoS computational biology, 14(1):e1005929, 2018.
S.	Chow, T. Yang, and H. Zhou. Global optimizations by intermittent diffusion. In Chaos, CNN,
Memristors and Beyond: A Festschrift for Leon Chua With DVD-ROM, composed by Eleonora
Bilotta, pp. 466-479. World Scientific, 2013.
I.	Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016.
http://www.deeplearningbook.org.
N. Hammerla, S. Halloran, and T. Ploetz. Deep, convolutional, and recurrent models for human
activity recognition using wearables. IJCAI 2016, 2016.
W. Hao and J. Harlim. An equation-by-equation method for solving the multidimensional moment
constrained maximum entropy problem. Communications in Applied Mathematics and Compu-
tational Science, 13.
W. Hao, J. Hauenstein, B. Hu, Y. Liu, A. Sommese, and Y.-T. Zhang. Bifurcation for a free boundary
problem modeling the growth of a tumor with a necrotic core. Nonlinear Analysis: Real World
Applications, 13(2):694-709, 2012.
J.	Jensen. Sur les fonctions convexes et les ine´galite´s entre les valeurs moyennes. Acta mathematica,
30(1):175-193, 1906.
D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
K.	Konda, A. Konigs, H. Schulz, and D. Schulz. Real time interaction with mobile robots using
hand gestures. In Proceedings of the seventh annual ACM/IEEE international conference on
Human-Robot Interaction, pp. 177-178. ACM, 2012.
F. Maire, L. Mejias, and A. Hodgson. A convolutional neural network for automatic analysis of aerial
imagery. In Digital lmage Computing: Techniques and Applications (DlCTA), 2014 International
Conference on, pp. 1-8. IEEE, 2014.
A. Morgan and A. Sommese. Computing all solutions to polynomial systems using homotopy con-
tinuation. Applied Mathematics and Computation, 24(2):115-138, 1987.
J.	Park and I. Sandberg. Universal approximation using radial-basis-function networks. Neural
computation, 3(2):246-257, 1991.
K.	Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recogni-
tion. Computer Science, 2014.
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way
to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):
1929-1958, 2014.
C. Wampler and A. Sommese. The Numerical solution of systems of polynomials arising in engi-
neering and science. World Scientific, 2005.
9
Under review as a conference paper at ICLR 2020
P. Yin, S. Zhang, Y. Qi, and J. Xin. Quantization and training of low bit-width convolutional neural
networks for object detection. arXiv preprint arXiv:1612.06052, 2016.
P. Yin, S. Zhang, J. Lyu, S. Osher, Y. Qi, and J. Xin. Binaryrelax: A relaxation approach for training
deep neural networks with quantized weights. arXiv preprint arXiv:1801.06313, 2018.
10