Under review as a conference paper at ICLR 2020
Distributed Online Optimization
with Long-Term Constraints
Anonymous authors
Paper under double-blind review
Ab stract
We consider distributed online convex optimization problems, where the distribut-
ed system consists of various computing units connected through a time-varying
communication graph. In each time step, each computing unit selects a con-
strained vector, experiences a loss equal to an arbitrary convex function evaluated
at this vector, and may communicate to its neighbors in the graph. The objective is
to minimize the system-wide loss accumulated over time. We propose a decentral-
ized algorithm with regret and cumulative constraint violation in O(T max{c,1-c})
and O(T 1-c/2), respectively, for any c ∈ (0, 1), where T is the time horizon.
When the loss functions are strongly convex, We establish improved regret and
constraint violation upper bounds in O(log(T)) and O(，Tlog(T)). These re-
gret scalings match those obtained by state-of-the-art algorithms and fundamental
limits in the corresponding centralized online optimization problem (for both con-
vex and strongly convex loss functions). In the case of bandit feedback, the pro-
posed algorithms achieve a regret and constraint violation in O(T max{c,1-c/3})
and O(T1-c/2) for any c ∈ (0, 1). We numerically illustrate the performance
of our algorithms for the particular case of distributed online regularized linear
regression problems.
1	Introduction
The Online Convex Optimization (OCO) paradigm Hazan (2016) has recently become prominent
in various areas of machine learning Where the environment sequentially generating data is too
complex tobe efficiently modeled. OCO portrays optimization as a process, and applies a robust and
sequential optimization approach Where one learns from experiences as time evolves. Specifically,
under the OCO frameWork, at each time-step the learner commits to a decision and suffers from
a loss, a convex function of the decision. The successive loss functions are unknoWn beforehand
and may vary arbitrarily over time. At the end of each step, the loss function may be revealed (a
scenario referred to as full information). Alternatively, the experienced loss only might be available
(bandit feedback). The objective of the decision maker is to minimize the loss accumulated over
time. The performance of an algorithm in OCO is assessed through the notion of regret, comparing
the accumulated loss under the algorithm and that achieved by an Oracle alWays selecting the best
fixed decision. In case of full information feedback, it is known that the best possible regret scales
in O( √T) (resp. O (log T)) for convex (resp. strongly convex) loss functions Zinkevich (2003);
Hazan et al. (2007); Abernethy et al. (2009).
This paper extends the OCO framework to a distributed setting where (different) data is collect-
ed and processed at N computing units in a network. More precisely, we consider scenarios
where in each time-step, each unit i commits to a decision xi(t) and then experiences a local
loss equal to 'i,t(Xi(t)). Units update their decision based on previously observed local loss-
es and messages received from neighboring units with the objective of identifying the decision
x? = argminχ PT=I PN=I 'i,t(x) minimizing the accumulated system-wide loss. Many tradition-
al applications of the centralized OCO framework Hazan (2016) naturally extend to this distributed
setting. As a motivating example, consider the following distributed online spam filtering task (refer
to Hazan (2016) for a description of the spam filter design problem in a centralized setting). In each
time-step, each unit i (here an email server) receives an email characterized by a vector ai,t ∈ Rd
(according to the “bag-of-words” representation). Unit i applies for this email a filter represented
1
Under review as a conference paper at ICLR 2020
by a vector xi(t) ∈ X where X is convex compact subset of Rd, returns a label f(ai>,txi(t)), and
experiences a loss equal to `i,t (xi (t)) = (f (ai>,txi(t)) - yi,t)2 where yi,t is the true email label (-1
for spam or 1 for valid). Note that the sequences of loss functions are inherently different at various
units because the latter receive different emails. Nevertheless, each unit would ideally wish to iden-
tify and apply as fast as possible the filter minimizing the system-wide loss, i.e., a filter that exploits
the knowledge extracted from all emails, including those received at other units. By leveraging this
knowledge, each unit would adapt faster to an adversary also modifying in an online manner spam
emails. More generally the distributed OCO framework can be applied to networks of learning a-
gents, where each agent wishes to take advantage of what other agents have learnt to speed up and
robustify its own learning process.
1.1	The DOCO (Distributed Online Convex Optimization) framework
We describe here our distributed optimization problem in more detail. We consider a network of
N computing units described by a sequence of directed graphs Gt = {V, Et } with node set V =
{1, . . . , N} and edge set Et at time t. Gt represents the communication constraints at the end of
time-step t: each unit is allowed to send its decision at time t to its neighbors in Gt . Each unit i ∈ V
is associated with a sequence of convex loss functions {'i,t}T=ι, where 'i,t : Rd → R.
Optimization process. In each time-step t, each unit i ∈ V selects xi(t) ∈ Rd. Then, in case of full
information feedback, the loss function `i,t is revealed to unit i, whereas in case of bandit feedback,
the loss 'i,t(xi(t)) is revealed only. Unit i finally receives vectors, functions of decisions selected
by its neighbors in Gt, i.e., xj(t) for j such that (j, i) ∈ Et, and updates its decision for the next
time-step.
Decision constraints. The decisions should be selected in X a convex subset of Rd character-
ized by a family of inequalities: X = {x ∈ Rd | cs(x) ≤ 0, s = 1, . . . , p}. Imposing such
constrained decisions implies that each unit should be able in each time-step to perform a projec-
tion onto X , which can be extremely computationally expensive. To circumvent this difficulty, we
adopt the notion of long-term constraints introduced in Mahdavi et al. (2012). Specifically, we
only impose that the constraints are satisfied in a long run rather than in each time-step, i.e., that
PtT=1 PiN=1 Psp=1 cs(xi(t)) ≤ 0. This relaxation allows units to violate the constraints by pro-
jecting onto a simpler set that contains X. Our results can be modified to account for the actual
constraints (but using projection steps).
Regrets and cumulative absolute constraint violation. The objective is to design distributed se-
quential decision selection algorithms so that each unit identifies the decision minimizing the accu-
mulated system-wide loss. The performance of such an algorithm is hence captured by the regrets
at the various units. The regret at unit i is:
TN	TN
Reg(i,T):= XX'j,t(xi(t)) - XX'j,t(x*),	(1)
where x? = argminχ∈χ PT=I PN=ι 'j,t(x). The system-level regret is defined as the worst pos-
sible regret at all units: SReg(T) , maxi=1,...,N Reg(i, T). Now since we allow units to select
decisions outside X, the performance of an algorithm is further characterized by the so-called cu-
mulative absolute constraint violation defined by: (here [a]+ = max{0, a})
TNp
CACV(T) :=XXX[cs(xi(t))]+.	(2)
t=1 i=1 s=1
1.2	Main results
We propose simple distributed algorithms where in each time-step, each unit combines information
received from its neighbors to update its decision and its local dual variable. Our algorithms enjoy
the following performance guarantees:
Full Information feedback. In the case of full information feedback, the proposed algorithm-
s achieve a system-level regret and a cumulative constraint violation in O(T max{1-c,c}) and
2
Under review as a conference paper at ICLR 2020
O(T 1-c/2), respectively and for any c ∈ (0, 1) (c expresses the trade-off between regret and cumula-
tive constraint violation). Theses bounds match those of centralized online optimization algorithms
in Mahdavi et al. (2012); Jenatton et al. (2016); Yuan & Lamperski (2018). When C = 1/2, Wegeta
regret scaling in O(√T), which corresponds to the fundamental regret limits for centralized online
problems Abernethy et al. (2009), Which is rather surprising in vieW of the dynamically changing
environment, the decentralized structure of the algorithm, and the presence of the constraints. When
the loss functions are strongly convex, we establish improved upper bounds on the regret and cu-
mulative connstraint violation in O(lοg(T)) and O(y/Tlοg(T)). These bounds generalize to our
distributed setting those derived in Yuan & Lamperski (2018) for centralized problems.
Bandit feedback. In the case of bandit feedback, the proposed algorithms achieve a system-
level regret and a cumulative constraint violation in O(d2 T max{1-c/3,c}) and O(dT1-c/2), re-
spectively, for any C ∈ (0,1). For example, when C = 3, the proposed algorithm attains a re-
gret bound in O(d2T 3/4). The performance guarantees can be improved to O(d2T 2/3 log(T)) and
O(dy∕T lοg(T)) in the case of strongly convex losses.
1.3	Related Work
Early work on online convex optimization in a centralized setting include Zinkevich (2003); Flax-
man et al. (2005). Today we know that a regret in O(√T) is achievable in both full information and
bandit feedback, see e.g. Bubeck et al. (2017). Projection-free algorithms have been also developed
Mahdavi et al. (2012); Jenatton et al. (2016); Yuan & Lamperski (2018) with regret and cumulative
constraint violation in O(Tmax{c,1-c}) and O(T 1-c/2) (C ∈ (0, 1)) in case of full information feed-
back (Yuan & Lamperski (2018) uses the cumulative squared constraint violation). Our algorithms
achieve the same guarantees in a distributed setting.
It is worth zooming into the rich literature on centralized online convex optimization with bandit
feedback. In the seminal work Flaxman et al. (2005), the authors designed an algorithm with one-
point bandit feedback and regret in O(d2T 3/4). The work Agarwal et al. (2010) extended this
algorithm to multi-point bandit feedback setting, where multiple points around the decision can
be queried for the loss function; they established O(d2√T) and O(d2 lοg(T)) regret bounds for
general convex and strongly convex loss functions, respectively. The work Mahdavi et al. (2012)
studied the online bandit optimization with long-term constraints under two-point bandit feedback
for domain. They established O(√T) and O(d2T3/4) bounds on the regret and the cumulative
constraint violations, respectively. In this paper, we design distributed algorithms with one-point
bandit feedback only, and with the same regret guarantees as the centralized algorithm in Flaxman
et al. (2005).
Over the last few years, there have been a rising interest for the Distributed OCO framework. Par-
ticularly, Shahrampour & Jadbabaie (2017); Lee et al. (2017) propose distributed algorithms with
O(√T) regret, but require an exact projection onto the decision set in each time-step. Zhang et al.
(2017) presents a distributed online conditional gradient algorithm, replacing the projection steps
by a much simpler linear optimization steps, but at the expense of worse and sub-optimal regret
guarantees, scaling in O(T3/4). The other approach to avoid projections is to allow the algorithm
to violate the constraints, and has been studied in Yuan et al. (2018). The problem studied in Yuan
et al. (2018) is a special case of our problem (where only one inequality constraint is considered),
and the regret and cumulative constraint violation guarantees obtained there are much worse than
ours. The authors of Li et al. (2018); Yi et al. (2019) also use the long-term constraints approach
to avoid projections, but analyze a very different optimization problem where units have different
decision variables, and no consensus among units is required. Finally it is worth mentioning that all
the aforementioned papers are restricted to full information feedback.
Notation and Terminology. Let kxk and [x]i to denote the Euclidean norm and the ith component
of a vector x ∈ Rd, respectively. Let ΠX [x] be the Euclidean projection of a vector x onto the set
X . Let Rp+ be the nonnegative orthant in Rp: Rp+ = {x ∈ Rp | [x]i ≥ 0, i = 1, . . . , p}. Denote the
(i, j)-th element of a matrix A by [A]ij. For a convex function f, a subgradient (resp. gradient when
f is differentiable) at a point X is denoted by ∂f (x) (resp. Vf (x)). Given two positive sequences
{at}t∞=1 and {bt}t∞=1, we write at = O(bt) if lim supt→∞ at/bt < ∞.
3
Under review as a conference paper at ICLR 2020
2	Full-Information Feedback
In this section, we focus on the case of full-information feedback, where at the end of each time-step,
the entire loss function `i,t is revealed to unit i. More precisely, unit i has access to the gradient of
the loss function `i,t at any query point. We make the following assumptions, which are standard in
the literature e.g., Mahdavi et al. (2012); Jenatton et al. (2016); Yuan & Lamperski (2018); Nedic &
Ozdaglar (2009); Yan et al. (2013); Duchi et al. (2012); Hosseini et al. (2013); Nedic et al. (2010).
Assumption 1 X ⊆ B := {x ∈ Rd∣kxk ≤ RX } with RX > 0.
Assumption 2 The functions `i,t and cs are convex with bounded gradients:
max max max ∣∣V'i t(x)k ≤ g`,	max max ∣∣Vcs(x)k ≤ Gc.
i=1,...,N t=1,...,T x∈B	,	s=1,...,p x∈B
We let G = max{G', GJ.
Assumption 3 There exists an integer B ≥ 1 such that the union graph (V, EkB+ι ∪ ∙ ∙ ∙∪E(k+i)B)
is strongly connected for all k ≥ 0.
Assumption 4 Associated with Gt there is the weight matrix A(t) which satisfies for all t ≥ 1:
(i) A(t) is doubly stochastic for all t ≥ 1, i.e., PjN=1 [A(t)]ij = 1 and PiN=1 [A(t)]ij = 1, ∀i, j ∈ V;
(ii) There exists a scalar ζ > 0 such that [A(t)]ii ≥ ζ for all i and t ≥ 1, and [A(t)]ij ≥ ζ if
(j, i) ∈ Et and [A(t)]ij = 0 for allj otherwise.
Assumption 4 is quite standard in the literature on distributed online or offline optimization, and
easy to achieve in a distributed manner in real-world networks. For example, when bidirectional
communication between nodes is allowed, we can enforce symmetry on the node interaction matrix,
which immediately makes it doubly stochastic. There are also other methods to construct doubly
stochastic matrices for a network, see, e.g., Garin & Schenato (2010); Gharesifard & Cortes (2010).
Algorithm 1 DOCO-LTC with full-information feedback
Input: Step sizes {βt}tT=1, regularization parameters {ηt}tT=1
Initialize: xi(1) = 0 ∈ Rd, λi(1) = 0 ∈ Rp, ∀i = 1, . . . , N
1:	for t = 1 to T do
2:	Unit i commits to a decision xi(t), and then after receiving `i,t, compute
p
yi(t) = xi(t) - βt V'i,t(xi(t)) +，入—^(t))] +
s=1
3:	Unit i communicates yi(t) to its neighbors and updates its decision as
N
xi(t + 1) = ΠB (pi(t)) , where pi(t) = X[A(t)]ij yj (t)
j=1
4:	Unit i updates its dual variable: λi(t + 1) = arg maxλ∈Rd Li,t((xi(t + 1), λ)
5:	end for
The pseudo-code of our algorithm, DOCO-LTC (LTC stands for Long-Term Constraints), is pre-
sented in Algorithm 1. It generalizes the algorithm in Yuan & Lamperski (2018) to our distributed
setting. In contrast to the literature on (online) distributed optimization with inequality constraints
Yuan et al. (2018); Li et al. (2018); Khuzani & Li (2016), the algorithm does not need to maintain an
iterative dual update process for every unit, which can be computed locally and explicitly. Moreover,
no consensus updates on the dual variables are necessary, reducing the communication complexity.
The design and convergence analysis of DOCO-LTC rely on the following online augmented La-
grangian function associated with unit i ∈ V: for t ≥ 1,
p
Li,t(x,λ) , 'i,t(x) + X[λ]s[cs(x)]+ - η2tkλk2,	⑶
4
Under review as a conference paper at ICLR 2020
where λ = [[λ]1, . . . , [λ]p]T ∈ Rp+ is the vector of Lagrangian multipliers with [λ]s being associated
with the sth inequality constraint cs (x) ≤ 0 and ηt is the regularization parameter. We note that:
p
VχLi,t(xi(t), λi(t)) = V'i,t(Xi(t)) + E[λi(t)]s∂[csX(t))]+,
s=1
where ∂[cs(xi(t))]+ can be calculated as follows for s = 1, . . . ,p:
∂[ ( (t))]	Vcs(xi(t)), if cs(xi(t)) > 0
∂[cs(xi(t))]+ =	0,	otherwise.
Moreover, the dual update λi(t + 1) in DOCO-LTC can be calculated explicitly as follows:
+	= [cs(χi(t +1))]+	S = ι,...,p.	(4)
s	ηt
Theorem 1 (Convex loss functions and full-information feedback) Under Assumptions 1-4, the
regret and CumuIative Constraint violation of DOCO-LTC with parameters η = Tc and βt =
apG?TC for some C ∈ (0,1), a > 1, and all t ≥ 1, satisfy: for all T ≥ 1,
SReg(T) ≤ CTmax{1-c,c} and CACV(T) ≤ CT1-c/2,
where C = 2aPNG2RX + apN(I + C) + 4aΝ-1)p with C = 2N (ψ2+1/BNφι∕B)+ 4) and
ψ = (1 — 4nn2 )	, and C = 小αNι(1 + 2apGRx + 2 a2p2G2RX).
Theorem 1 shows that DOCO-LTC has the same guarantees as those of the centralized algorithms in
Mahdavi et al. (2012); Jenatton et al. (2016); Yuan & Lamperski (2018). The user-defined parameter
c tunes the trade-off between SReg and CACV (for c = 1/2, we get a regret and constraint violation
in O(√T) and O(T3/4)).
Communication cost vs. regret. The communication cost, i.e., the number of vectors transmitted
per round in the network, is simply equal to the number of edges in the network. Taking the case of
B = 1 (i.e., the graph is fixed and connected) as an example, we can establish that the regret bound
in Theorem 1 scales as O Q-Νa))2Tmax{c,1-c}), where σ2(A) is the second largest singular
value of the weight matrix A. If we choose the weight matrix as the maximum-degree weights (see,
e.g., Yuan et al. (2019)), we have the following conclusions: i) Random geometriC graph: the regret
bound scales as 色；N)Tmax{c,1-c} and at most 2log1+e(N)N vectors are transmitted per round;
ii) k-regular expander graph: σ2 (A) is constant, the regret bounds scales as N 4 T max{c,1-c} and
2kN vectors are transmitted per round; and iii) Complete graph: σ2(A) = 0 and N(N - 1) vectors
are transmitted per round.
Next we improve DOCO-LTC performance guarantees when the loss functions are strongly convex.
Assumption 5 The loss funCtion `i,t is σ-strongly Convex over B, that is, for any x, y ∈ B,
'i,t(X) ≥ 'i,t(y) + v'i,t(y)T(X - y) + σ∣∣χ - yk2.
Theorem 2 (Strongly convex loss functions and full-information feedback) Under Assumptions
1-5, the regret and Cumulative ConStraint violation of DOCO-LTC with parameters η = 2G and
βt = σ for all t ≥ 1, satisfy: for all T ≥ 3,
SReg(T) ≤ Csc log(T), and CACV(T) ≤ CscpT log(T),
where Csc = NG(4 + 4C + C2) (C is ShOwn in Theorem 1) and Csc =旬^^/ (√RX + JG∣).
In the case of strongly convex loss functions, the regret and constraint violation guarantees of
DOCO-LTC also match those obtained by the centralized algorithm in Yuan & Lamperski (2018).
Note that one cannot actually get a better regret scaling, even in the centralized setting Abernethy
et al. (2009).
5
Under review as a conference paper at ICLR 2020
3	One-Point Bandit Feedback
This section is devoted to the case of bandit feedback, where at the end of each time-step, unit i can
observe the value of the loss function `i,t at only one point around xi (t). The pseudo-code of our
algorithm adapted to this feedback is presented in Algorithm 2.
Algorithm 2 DOCO-LTC with one-point bandit feedback
Input: Step sizes {βt}tT=1, regularization parameters {ηt}tT=1, exploration parameters {εt}tT=1, and
shrinkage parameter π
Initialize: xi(1) = 0 ∈ Rd, λi(1) = 0 ∈ Rp, ∀i = 1, . . . , N
1:	for t = 1 to T do
2:	Unit i commits to a decision Xi(t), and then observes the loss 'i,t(xi(t) + εtui(t)) where
ui(t) is randomly chosen on the unit sphere (kui(t)k = 1)
3:	Unit i builds the following one-point gradient estimator:
d
▽ 'i,t(xi(t)) = 一'i,t(xi(t)+ εtUi(t))ui(t)
εt
and computes
p
yi(t) = xi(t) - βt ▽ 'i,t(xi(t)) + ^[λi(t)]s∂[cs(xi(t))] +
s=1
4:	Unit i updates its decision using yj(t) received from its neighbors as
N
xi(t + 1) = ΠB (pi(t)) , where pi(t) = X[A(t)]ij yj (t)
j=1
5:	Node i updates its dual variable λi(t + 1) = arg maxλ∈Rd Li,t((xi(t + 1), λ)
6:	end for
EI ∙>♦	1	1	∙ Γ∙	IJll	1	. 1	. »	1	∙	Γ / X ∖
The design and convergence analysis of our algorithm here rely on the smoothed version Li,t(x, λ)
of the online augmented Lagrangian function (3), i.e., Li,t(x, λ): for t ≥ 1,
p
L,t (x,λ)，'i,t(x; ε)+f[λ]s[cs(x)]+ - η2t kλk2,	(5)
where 'i,t(x; ε) = Ev ['i,t(x + εv)] is the smoothed loss function, and V is a vector uniformly
distributed over the unit sphere. As in the case of full information feedback, the dual update λi(t+1)
can be calculated explicitly according to (4).
In the case of bandit feedback, we need to introduce the shrinkage parameter π to ensure that the
random query point xi(t) + εtui(t) belongs to the set B. Indeed, we have:
kxi(t) +εtui(t)k ≤ kxi(t)k +εtkui(t)k ≤ (1 - π)RX +εt ≤ RX
where the second inequality follows from the fact that xi(t) ∈ (1 - π)B and kui(t)k = 1 and the
last inequality holds when εt ≤ πRX .
To establish upper bounds on the regret and cumulative constraint violation of our algorithm, we
make the following standard assumption on the loss functions `i,t (x) (commonly adopted even in
centralized online bandit optimization Flaxman et al. (2005)).
Assumption 6 The loss functions 'i,t(x) are uniformly bounded over B:
sup max max ∣'it(x)∣≤ C.
x∈B i=1,...,N t=1,...,T ,
Since algorithms for bandit feedback are inherently randomized, we investigate averaged versions
of the regret and the cumulative constraint violation: E-SReg(T) := maxi=1,...,N E[Reg(i, T)] and
E-CACV(T) := PtT=1 PiN=1 Psp=1 E[[cs(xi(t))]+].
6
Under review as a conference paper at ICLR 2020
Theorem 3 (Convex functions with bandit feedback) Under Assumptions 1-4 and 6, the regret
and cumulative constraint violation of DOCO-LTC with parameters
_ 1	1	0 _ 1	_	1
ηt = Tc,	βt = apG2 Tc, εt = Tb, π = RX Tb
for some c ∈ (0, 1), b = c/3 and all t ≥ 1, satisfy: for all T ≥ 1,
E-SReg(T) ≤ C§Tmax{1-c/3，c} and E-CACV(T) ≤ C§T1-c/2,
where C§ = 3NG + NaCC + NapCGd + 2 apNG2RX + 4aNC1)p (C is Shown in Theorem 1) and
C§ = qN (Cd2 + 2apGRχ + 1 a2p2 G2 RX).
Note that DOCO-LTC achieves a regret scaling as T3/4 When C = 3, which is identical to that of
centralized online bandit optimization Flaxman et al. (2005). This is rather remarkable considering
the decentralized nature of the algorithm. Again, we can improve our bounds in the case of strongly
convex loss functions.
Theorem 4 (Strongly convex functions with bandit feedback) Under Assumptions 1-6, the re-
gret and cumulative constraint violation of DOCO-LTC with parameters
2pG2	1	1	1
ηt = ^ττ, βt = σt, εt = Tb, π = RXTb
for b = 3, and all t ≥ 1, satisfy: for all T ≥ 3,
E-SReg(T) ≤ CgCT2/3 log(T) and E-CACV(T) ≤ CgCPlog(T),
where CgC = 3NG + N(4CCGd + 4C2d2 + C2 G2) (C is shown in Theorem 1) and CgC =
4√g (√GRX + √厂
4 Numerical Experiment
We illustrate the performance of the proposed algorithms using a simple experiment. Specifically,
we consider distributed online regularized linear regression problem over a network, formulated as
follows:
minimize	PT=I PN=I 1 (ai ⑴TX - bi ⑴『+ PkXk 2
subject to	cm(x) = L - [x]m ≤ 0, m = 1, . . . , d	(6)
cd+m(X) = [X]m - U ≤ 0, m = 1, . . . , d
where P ≥ 0 denotes the regularization parameter. The data (ai(t), bi(t)) ∈ Rd × R is revealed only
to unit i at time t.
Results on Synthetic Data. Every entry of ai (t) is generated uniformly at random within the
interval [-1, 1] and bi(t) is generated according to
bi (t) = ai (t)τ x + Ci (t)
where [X]i = 1, for all 1 ≤ i ≤ [d/2」 and 0 otherwise, and the noise Ci(t)〜N(0,1). Throughout
the experiments, we implement our algorithms over a time-varying directed network depicted in
Fig. 1: the network is not connected in every time-step, but the union graph of any two consecutive
time instances is strongly connected, that is, we have B = 2 in Assumption 3. The weight matrices
associated with the networks in Fig. 1 are generated according to the maximum-degree weights (see,
e.g., Yuan et al. (2019)). We set the parameters as follows: N = 6, d = 4, L = -0.15, U = 0.15,
and RX = Udrd. The performance of DOCO-LTC is averaged over 10 runs.
(a)
(b)
(c)
(d)
Figure 1: The network switches sequentially in a round robin manner between (a), (b), (c), and (d).
7
Under review as a conference paper at ICLR 2020
To get (not strongly) convex loss functions, we set ρ = 0. We run Algorithm 1 and Algorithm 2
with c = 1/2 and c = 3/4 and plot the maximum regret maxi∈V Reg(i, T ), and CACV(T ) as a
function of the time horizon T in Fig. 2(a) and Fig. 2(b), respectively. It can be seen from Fig.
2(a) that in the case of full-information feedback, the regret is smaller for c = 1/2, while in bandit
feedback setting, the regret is smaller for c = 3/4. This is because c = 1/2 and c = 3/4 correspond
to a balanced regret in the full-information setting and bandit feedback setting. By balanced, we
mean that 1 - c = c in T max{1-c,c} in Theorem 1 and 1 - c/3 = c in T max{1-c/3,c} in Theorem 3,
respectively. From Fig. 2(b) we also observe that for both feedback models, CACV is smaller for a
larger value of c, i.e., c = 3/4. This is in compliance with the results established in Theorems 1 and
3. Finally, the performance is really degraded when going from full information to bandit feedback.
This was also expected.
O max regret: full (c = 1/2)
-^Θ - max regret: bandit (c = 1/2)
max regret: full (c = 3/4)
-A- max regret: bandit (c = 3/4)
Φ-I6al LUnlU-XEW
-^CACV
--σ-CACV
^CACV
7-CACV
full(c = 1/2)
bandit (c = 1/2)
full (c = 3/4)
bandit (c = 3/4)

Figure 2: SReg and CACV vs. time for convex costs.
In the case of strongly convex losses, we run Algorithm 1 and Algorithm 2 with ρ > 0, namely
ρ = 1 and ρ = 2. We plot the performance of the algorithms as a function the time horizon in
Fig. 3(a) and Fig. 3(b), respectively. From Fig. 3, we confirm that the cost of bandit feedback
is rather high. We also observe the regret and the violation constraint are smaller and flatter than
those achieved of non-strongly convex loss functions (ρ = 0), for both feedback models. All these
observations comply with the results established in Theorems 2 and 4.
O	200	400	600	800	1000
Time horizon T
(a)
Figure 3: SReg and CACV vs. time for strongly
CACV: full (p= 1)
-O-CACV: bandit (p = 1)
Y^CACV: full (p = 2)
-A-CACV: bandit (p = 2)
convex costs.
I 0 I ~~~4---------
300	400	500	600	700	800
Time horizon T
(b)
Results on Real Datasets. We demonstrate the efficiency of our proposed algorithms on two real
datasets selected from the LIBSVM1 repository. The details of the datasets are summarized in Table
1.
We use the same network and parameters as those used in the synthetic data and let c = 1/2. For
each dataset, we run Algorithm 1 and Algorithm 2 with ρ = 0 and ρ = 1, respectively. We plot
1https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/
8
Under review as a conference paper at ICLR 2020
Table 1: Summary of datasets
dataset	# of features	# of instances
mg	6	1385
bodyfat	14	252
the performance of the algorithms as a function the time horizon in Fig. 4 (mg dataset) and Fig.
5 (bodyfat dataset), respectively. These numerical experiments on real-world datasets show the
convergence of the proposed algorithms and are consistent with the results established in Theorems
1-4. Finally, the performance is really degraded when going from strongly convex loss functions to
convex loss functions.
Figure 4: SReg and CACV vs. time for mg dataset.
Figure 5: SReg and CACV vs. time for bodyfat dataset.
(a)
(b)
Figure 6: Comparison of the proposed algorithms with D-OCG on mg and bodyfat datasets.
9
Under review as a conference paper at ICLR 2020
We finally make comparisons with a standard distributed online projection-free algorithm (D-OCG
in Zhang et al. (2017)) using mg and bodyfat real datasets. The detailed results are provided in Fig.
6. From these plots one can confirm that: (i) Our DOCO algorithm achieves better performance than
D-OCG under the same information feedback (of course, D-OCG exhibits no constraint violations);
and (ii) For both algorithms, the performance is degraded from full information to bandit feedback.
5 Conclusions
In this paper, we consider the distributed online convex optimization problem with long-term con-
straints under full-information and bandit feedback. By introducing and exploiting the notion of
online augmented Lagrangian function, we develop distributed algorithms that are based on con-
sensus algorithms. For the case of full-information feedback, we establish sub-linear regret and
cumulative absolute constraint violations that match those of centralized online optimization in the
literature. Moreover, we also establish sub-linear regret and constraint violation in the case of bandit
feedback, where the loss function can be locally evaluated at one point in each time-step.
10
Under review as a conference paper at ICLR 2020
References
Jacob D. Abernethy, Alekh Agarwal, Peter L. Bartlett, and Alexander Rakhlin. A stochastic view of
optimal regret through minimax duality. In COLT, 2009. URL http://dblp.uni-trier.
de/db/conf/colt/colt2009.html#AbernethyABR09.
Alekh Agarwal, Ofer Dekel, and Lin Xiao. Optimal algorithms for online convex optimization with
multi-point bandit feedback. In COLT,, pp. 28-40. Citeseer, 2010.
Sebastien Bubeck, Yin Tat Lee, and Ronen Eldan. Kernel-based methods for bandit convex
optimization. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of
Computing, STOC 2017, Montreal, QC, Canada, June 19-23, 2017, pp. 72-85, 2017. doi:
10.1145/3055399.3055403. URL https://doi.org/10.1145/3055399.3055403.
John C Duchi, Alekh Agarwal, and Martin J Wainwright. Dual averaging for distributed optimiza-
tion: Convergence analysis and network scaling. IEEE Transactions on Automatic control, 57(3):
592-606, 2012.
Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization
in the bandit setting: gradient descent without a gradient. In Proceedings of the sixteenth annual
ACM-SIAM symposium on Discrete algorithms, pp. 385-394. Society for Industrial and Applied
Mathematics, 2005.
Federica Garin and Luca Schenato. A survey on distributed estimation and control applications
using linear consensus algorithms. In Networked control systems, pp. 75-107. Springer, 2010.
Bahman Gharesifard and Jorge Cortes. When does a digraph admit a doubly stochastic adjacency
matrix? In Proceedings of the 2010 American Control Conference, pp. 2440-2445. IEEE, 2010.
Elad Hazan. Introduction to online convex optimization. Found. Trends Optim., 2(3-4):157-325,
August 2016. ISSN 2167-3888. doi: 10.1561/2400000013. URL https://doi.org/10.
1561/2400000013.
Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex
optimization. Machine Learning, 69(2-3):169-192, 2007.
Saghar Hosseini, Airlie Chapman, and Mehran Mesbahi. Online distributed optimization via dual
averaging. In 52nd IEEE Conference on Decision and Control, pp. 1484-1489. IEEE, 2013.
Rodolphe Jenatton, Jim C Huang, and Cedric Archambeau. Adaptive algorithms for online convex
optimization with long-term constraints. In Proceedings of the 33rd International Conference on
International Conference on Machine Learning-Volume 48, pp. 402-411. JMLR. org, 2016.
Masoud Badiei Khuzani and Na Li. Distributed regularized primal-dual method: Convergence anal-
ysis and trade-offs. arXiv preprint arXiv:1609.08262, 2016.
Soomin Lee, Angelia Nedic, and Maxim Raginsky. Stochastic dual averaging for decentralized
online optimization on time-varying communication graphs. IEEE Transactions on Automatic
Control, 62(12):6407-6414, 2017.
Xiuxian Li, Xinlei Yi, and Lihua Xie. Distributed online optimization for multi-agent networks with
coupled inequality constraints. arXiv preprint arXiv:1805.05573, 2018.
Mehrdad Mahdavi, Rong Jin, and Tianbao Yang. Trading regret for efficiency: online convex opti-
mization with long term constraints. Journal of Machine Learning Research, 13(Sep):2503-2528,
2012.
Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimiza-
tion. IEEE Transactions on Automatic Control, 54(1):48, 2009.
Angelia Nedic, Alex Olshevsky, Asuman Ozdaglar, and John N Tsitsiklis. Distributed subgradient
methods and quantization effects. In 2008 47th IEEE Conference on Decision and Control, pp.
4177-4184. IEEE, 2008.
11
Under review as a conference paper at ICLR 2020
Angelia Nedic, Asuman Ozdaglar, and Pablo A Parrilo. Constrained consensus and optimization in
multi-agent networks. IEEE Transactions on Automatic Control, 55(4):922-938, 2010.
Shahin Shahrampour and Ali Jadbabaie. Distributed online optimization in dynamic environments
using mirror descent. IEEE Transactions on Automatic Control, 63(3):714-725, 2017.
Feng Yan, Shreyas Sundaram, SVN Vishwanathan, and Yuan Qi. Distributed autonomous online
learning: Regrets and intrinsic privacy-preserving properties. IEEE Transactions on Knowledge
and Data Engineering, 25(11):2483-2493, 2013.
Xinlei Yi, Xiuxian Li, Lihua Xie, and Karl H Johansson. Distributed online convex optimization
with time-varying coupled inequality constraints. arXiv preprint arXiv:1903.04277, 2019.
Deming Yuan, Daniel WC Ho, and Guo-Ping Jiang. An adaptive primal-dual subgradient algorithm
for online distributed constrained optimization. IEEE Transactions on Cybernetics, 48(11):3045-
3055, 2018.
Deming Yuan, Alexandre Proutiere, and Guodong Shi. Distributed online linear regression. arXiv
preprint arXiv:1902.04774, 2019.
Jianjun Yuan and Andrew Lamperski. Online convex optimization for cumulative constraints. In
Advances in Neural Information Processing Systems, pp. 6140-6149, 2018.
Wenpeng Zhang, Peilin Zhao, Wenwu Zhu, Steven CH Hoi, and Tong Zhang. Projection-free dis-
tributed online learning in networks. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 4054-4062. JMLR. org, 2017.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the Twentieth International Conference on International Conference on Machine
Learning, ICML’03, pp. 928-935. AAAI Press, 2003. ISBN 1-57735-189-4. URL http://
dl.acm.org/citation.cfm?id=3041838.3041955.
12
Under review as a conference paper at ICLR 2020
Appendix
A Proof of Theorem 1
A.1 Key lemmas
The following two lemmas are crucial to the convergence analysis of Algorithm 1. The first lemma
establishes the basic convergence results of Algorithm 1.
Lemma 1 (Basic Convergence) Let Assumptions 1 and 2 hold. For every node i ∈ V and T ≥ 1,
we have
Reg「T) ≤ X P= kXi(t)-x*k2- PILI kXi(t + 1)-x*k2
2βt
T
+ N G2 X	βt
t=1
TN
+ G XX kXi∙(t)-
t=1 i=1
Proof. To simplify the presentation, we write
Vi(t) , VχLi,t(xi(t), λi(t)).
We study the general evolution of kxi(t + 1) - x?k2,
kXi(t + 1) - x?k2 = k∏B (Pi(t)) - x?k2 ≤ kPi(t) - x?k2
where the inequality is based on the non-expansiveness of the Euclidean projection and x? ∈ X ⊆
B. Expanding the right-hand side, we further obtain
NN
Xkxi(t+1)-x?k2 ≤X
i=1	i=1
N
X[A(t)]ij[xj(t)-βtVj(t)]-x?
j=1
2
NN
≤ XX[A(t)]ijkxj(t)-x?- βtVj(t)k2
i=1 j=1
N
≤ X kxi(t) - x? - βtVi(t)k2
i=1
N	NN
Xkxi(t) - x?k2 + βt2 X kVi(t)k2 -2βtXVi(t)T(xi(t) -x?)
i=1	i=1	i=1
NN
≤ Xkxi(t) - x?k2 + βt2 X kVi(t)k2
i=1	i=1
N
-2βt	[Li,t(xi(t),λi(t)) - Li,t(x?, λi(t))]
i=1
(7)
where the second and third inequalities follow from the doubly stochasticity of A(t) and the last
inequality from the convexity of Li,t (x, λ) with respect to x. Combining the preceding inequality
with the definition of online augmented Lagrangian function in (3), yields
N
X
i=1
≤
p
'i,t(xi(t)) + X[λi(t)]s[cs(xi(t))]+ - η2t kλi(t)k2
s=1
-卜,t(x?)+ X[λi(t)]s[cs(x?)]+ - η2tk%(t)k；
PNJlXi(t)-x*k2- Pi=IkXi(t +1)-x*k2
2βt
βN
+ β X kVi(t)k2.
i=1
(8)
13
Under review as a conference paper at ICLR 2020
On the other hand, it follows from Assumption 2 that
p 2
kVi(t)k2 = V'i,t(xi(t)) + ]T[λi(t)]s∂[cs(xi (t))] +
s=1
p
≤ 2kV'i,t(Xi(t))k2 + 2pE[λi(t)]2∣∣∂[cs(Xi(t))] + k2
s=1
p
≤ 2G2 + 2pG2 X[λi(t)]s2
s=1
=2GG +2pG2 XX [cs(X2(t))]+	(9)
s=1	ηt-1
where the last equality follows from the dual update (4). Combining the inequalities (8) and (9), and
using the fact that [λi(t)]s = [cs(Xi?+ (cf. (4)), We obtain
N
X ['i,t(xi(t))- 'i,t(x?)] ≤
i=1
+ NG2βt
PN=I kχi⑴一x?k2 — PiNN=I kxi(t + I) — x?k2
2βt
(10)
The left-hand side can be further loWer bounded by
'i,t(Xi(t)) = 'i,t(x*(t)) + 'i,t(xi(t)) — 'i,t(x*(t)) ≥ 'i,t(x*(t)) — Gkxi(t) - Xi∙ (t)k	(11)
summing the inequalities in (10) over t = 1, . . . , T, and using the bound (11) and definition of regret
(1), We arrive at the desired conclusion.
Remark 1 The first two terms in Lemma 1 are optimization errors that are common in the analysis
of online optimization algorithms, the third term is the cost of aligning the decisions of nodes, and
the last term is the penalty incurred by the violation of constraints.
The second lemma establishes a bound on the disagreement among all the nodes, Which is measured
by the difference betWeen the norms of decisions of nodes.
Lemma 2 (Disagreement) LetAssumptions 1-4 hold. For every node i* ∈ V and T ≥ 1, we have
TN	T	TN p
XX kXi∙(t) — Xi(t)k≤ NGGXβt + CGXXX[cs(Xi(t))] + 国
ηt-1
t=1 i=1	t=1	t=1 i=1 s=1
where C is given in Theorem 1.
Proof. By deriving the general expressions for the average decision X(t + 1) = -N PN=I Xi(t + 1)
and xi(t + 1) it follows that
N	Nt	N
X kx(t + 1) — Xi(t +1)k≤ XX βm X∣[A(t, m)]i,j - NTlkVj(m)k
i=1	i=1 m=1 j=1
N t-1 N
+ X X X |[A(t,m + 1)]i,j - N-1∣ ∙ kπB(Pj(m)) - Pj(m)k
i=1 m=1 j=1
N
+2XkΠB(Pi(t))-Pi(t)k
i=1
(12)
14
Under review as a conference paper at ICLR 2020
where A(t, m) = A(t)…A(m), ∀t ≥ m ≥ 1 and A(t, t) = A(t). On the other hand, by resorting
to Corollary 1 in Nedic et al. (2008), we have that, for all t ≥ m ≥ 1,
Combining the inequalities (12) and (13), we obtain
T N	3N	T-1	N
XX kx(t) - Xi(t)k ≤ (ψ2+1∕B 3N ψ1∕B) +4) X βt X kVi(t)k.
Then, combining (9) and (14) with the following inequality,
NN
X kχi∙⑴一Xi ⑴ k = X kχi•⑴一X⑴十 X⑴一Xi⑴k
i=1	i=1
(13)
(14)
NN
=X kxi∙⑴一X⑴k + X kX⑴一Xi⑴k
i=1	i=1
N
≤ (N +1) X kX(t)-Xi(t)k
i=1
we arrive at the conclusion. The proof is complete.
A.2 Proof of the theorem
We consider an arbitrary unit i ∈ V and establish a regret bound for that unit. Combining the
results in Lemmas 1 and 2, we have
Reg(i∙,T) ≤
T
X
t=1
P= kXi(t)-X*k2- PL kXi(t + 1)-X*k2
2βt
T
+ (1 + C)ng2 X βt
t=1
TNp
+ C G2 XXX[cs(Xi(t))]+ 户
t=1 i=1 s=1	ηt-1
(15)
Substituting η = Tc and βt = °?/2 TC into the preceding inequality, yields
Reg(i∙,T) ≤ 2apG2 (X k~(1)- X*k j TC + a^N(1 + C)T 1-c
TNp	TNp
+ ɪ C XXX[cs(Xi(t))]+ - 1 - 1 T cXXX[cs(Xi(t))] + .
ap t=1 i=1 s=1	a	t=1 i=1 s=1
We turn our attention to bound the last two terms. To this end, write
TNp
ρ, XXX[cs(Xi(t))]+
t=1 i=1 s=1
which implies that
TNp	TNp	2
XXX[cs(Xi(t))]+ ≥ pNT XXX[cs(Xi(t))]+	= pNTP
t=1 i=1 s=1	t=1 i=1 s=1
because of the inequality that (a1 + …+ an)2 ≤ n Pn=I a2 Hence, (16) now becomes
Reg(i∙,T) ≤ 5apNG2RXTC +	n(1 + C)T 1-c + CP - (1-) NTI-C p .
2	ap	ap	a	pNT 1-c
X{z}
,f(ρ)
(16)
(17)
(18)
(19)
15
Under review as a conference paper at ICLR 2020
where we have used Assumption 1, i.e., PiN=1 kxi(1) - x?k2 = PiN=1 kx?k2 ≤ NRX2 . We can
replace the term f (ρ) by the following,
2
max f ⑺=_㈤__________ = NC T 1-c
max f (ρ) = 4 0- W) pNτk-c =4a(α- 1)pT .
Hence, combining the inequalities (19) and (20), we finally have
Reg(i∙,T) ≤ 1 apNG2RXTC + ɪN(1 + C)T1-c + max f(P)
2	X ap	ρ≥0
≤ 1 apNG2RXTC + ( ɪN(1 + C)+	NC2、] T 1-c.
≤ 2 p X + lap ( + )+4a(a-1)pJ
Therefore, we complete the first statement of Theorem 1.
We are left to bound the CACV. From (10) it follows that
TN	T
XX ['i,t(Xi(t)) - 'i,t(x?)] ≤ X
(20)
(21)
(22)
Pi=IkXi(t)-x*k2- Pi=IkXi(t +1)-x*k2
2βt
T
+ NG2 X βt
t=1
the right-hand side can be bounded by following similar lines as that of the regret analysis, that is,
r.h.s. of (22) ≤ 1 apNG2RXTC	+ ɪNT1-c -(1 -	1) TC	X X X[cs(xi(t))]+	(23)
2	ap	a	t=1 i=1 s=1
the left-hand side on (22) can be bounded as follows, according to Assumptions 1 and 2:
TN
l	.h.s. of (22) ≥ -GXX kxi(t) -x?k ≥ -2NGRXT.	(24)
t=1 i=1
Combining (23) and (24) and regrouping terms, we have
T Np
XXX[cs(xi(t))]2+≤
t=1 i=1 s=1
2⅛ NG2RX+ (0⅛T 1-2c + 为 NGRX T1-c
≤ aN1 (p+ 2aGRX+ 2 a2pG2 RX) T1-c
the desired bound follows by combining the preceding inequality and (18). The proof is complete.
B Proof of Theorem 2
We first derive the bound on CACV. Note that Li,t (x, λ) is σ-strongly convex, according to As-
sumption 5. This fact, combined with (7), leads to
N	NN
X llχi(t + 1)-χ*k2 ≤ X kχi(t) -χ*k2 + β2X ∣∣Vi(t)k2
i=1	i=1	i=1
N
-2βt X [Li,t(χi(t), %(t))-Li,t(χ*, λi(t))+2kχi(t) — χ*k2] (25)
i=1
16
Under review as a conference paper at ICLR 2020
regrouping the terms, we further have
TN
XX
[Li,t(xi(t),%(t))- Li,t(x?, %(t))]
t=1 i=1
T
≤X
t=1
P= kXi(t)-x*k2- PL kXi(t +1)-x*k2
2βt
t=1	t=1	i=1
2X G- £ -σ) X kxi⑴ - x*k2+2 G - 0)X kxi ⑴ - x*k2
1 N	TβN
- 2Γ X kxi(τ + 1) - x*k2 + X β X kVi(t)k2.
2βT i=1	t=1 2 i=1
Substituting βt = 煮 into the preceding inequality and dropping the negative term, yields
TN	T N
XX
[Li,t(xi(t), λi(t)) - Li,t(x*, λi(t))] ≤
X β X kVi(t)k2.	(26)
Noting that λi(t) is the maximizer of Li,t((xi(t), λ) over λ ∈ Rp+, i.e., Li,t(xi(t), λi(t)) ≥
Li,t(xi(t), λ) for all λ ∈ Rp+, we have the following estimate for all λ ∈ Rp+, according to (26),
TN	T N
XX [Li,t(xi(t), λ) - Li,t(x*, %(t))] ≤ X 4 X kVi(t)k2.	(27)
Expanding the left-hand side by using the definition of online augmented Lagrangian function in
(3), we further have
TN	p
XX
'i,t(Xi(t)) + X[λ]s[cs(xi(t))]+ - η2tkλk2
'i,t(x?) + X[λi(t)]s[cs(x?)]+ - η2tk%(t)k2)
T N	T	TN
≤ X β X kVi(t)k2 ≤ NG2 Xβt + PG XXβtkλi(t)k2	(28)
where we recalled (9). Applying ηt = 2pG2βt to (28), we find that the last terms on both sides will
cancel each other out. This leads to
TN	p TN	p	T
XX ['i,t(xi(t)) -'i,t(x?)] + X XX[c(xi(t))]+	[λ]s - X 1NXηt [λ]2
t=1 i=1	s=1 t=1 i=1	s=1	t=1
S------------------{z---------------------}
,g(λ)
T
≤ NG2 X βt	(29)
note that (29) holds for all λ ∈ Rd+, and hence we can replace g(λ) by the following,
TN	T	T
λm∈aRxp g(λ) ≤ XX Qt(Xit(xi(t))] + N G2 X βt ≤2NGRXT+NG2Xβt	(30)
λ∈Rp+	t=1 i=1	t=1	t=1
where the last inequality follows from the same reasoning as that of (24). For the left-hand side on
(30), we have
p	PtT=1PiN=1[cs(xi(t))]+2	PtT=1PiN=1Psp=1[cs(xi(t))]+2
max g(λ) = E —…T------------------ ≥ ^Z=^=-------------------- (31)
λ∈Rd+	s=1	2N PtT=1 ηt	2PN PtT=1 ηt
17
Under review as a conference paper at ICLR 2020
Combining the inequalities (30) and (31) with the following estimate,
T1
X t = 1 +
t=1
T 1	T1
t ≤ 1 + J Udu = 1 + log(T)
(32)
we further obtain for all T ≥ 3,
, ,、S	∕16p2N 2G3Rχn~Z∑~16p2N 2G4, 2°、
∑∑∑[cs(χi(t))]+ ≤ V --------XTlog(T) +	— iog2(τ)
t=1 i=1 s=1	σ	σ
≤
4PNGi + 4PNG2 )pTwy
σσ
(33)
Next, we turn our attention to the first statement, i.e., the regret bound. Again We consider an
arbitrary unit i∙ ∈ V. Combining the inequalities (30) and (31), and using the notation (17), We
obtain
T	TN
NGXβt-τ-τττ1^-P2 ≥XX['i,t(Xi(U)- 'i,t(X?)]
t=1	2pN t=1 ηt	t=1 i=1
TN
≥ Reg(i∙,T) - GXX kx*(t) - Xi(t)k.	(34)
t=1 i=1
Applying Lemma 2 to the preceding inequality gives
T1
Reg(i∙,T) ≤ NG2(1 + C)Eβt— ——t— P2
t=1	2pN	t=1 ηt
TNp
+cg2 XXX …)]+ηβt7
t=1 i=1 s=1
substituting the expressions for βt and ηt into (35), We have
T1
Reg(i∙,T) ≤ NG2(1 + C)Eβt + 2pCρ
t=1	、
(35)
{Z-
,h(ρ)
Then, folloWing an argument similar to that of (20) and (21), We get
_______1______ 2
2pN P=I nt：.
(36)
—
Reg(i∙,T) ≤ NG2(1 + C) Xβt + 4 (2pC) — ≤ NG2
t=1	4 2pΝ PT=1 ηt
T
T
(1+C)E βt + 8>c 2Σ
t=1	t=1
ηt
The proof is complete.
2NG2(1 + C)+ NC2 G2
2σ
log(T).
(37)
≤
σ
C Proof of Theorem 3
C.1 Key lemma
The proof relies on the properties of the one-point gradient estimator V'i,t(xi(t)), which can be
vieWed as a distributed version of the one in Flaxman et al. (2005). In particular, We have the
following lemma that characterizes the properties of the one-point gradient estimator and the relation
between 'i,t(x) and its smoothed version 'i,t(x; ε).
Lemma 3	(i) Let G be the uniform Lipschitz constant of the loss functions `i,t (X) over B, then
the smoothed loss functions 'i,t (x) are LiPschitz continuous With the same constant G and
we have that, for all x ∈ B,
∣2,t(x; ε) - 'i,t(x)∣ ≤ Gε.
18
Under review as a conference paper at ICLR 2020
(ii)	The one-point gradient estimator satisfies
E [十'i,t(xi(t))] = V%i,t(xi(t);εt).
(iii)	Let Assumption 6 hold, then the one-point gradient estimator satisfies
Il v 'i,t(χi(t))∣∣ ≤ W.
C.2 Proof of the theorem
Denote
p
V i(t)，V 'i,t(xi(t)) + f[λi(t)]s∂ [cs(xi(t))]+	(38)
s=1
then, it follows from Lemma 3(ii) that
p
E [Vi(t)j = VK,t(xi(t);εt) + X[λi(t)]s∂[cs(xi(t))]+ = VxL,t(xi(t), λi(t)).	(39)
s=1
Following similar lines as that of Lemma 1, we immediately have
N	N	N	2N
X kxi(t + 1)	- x∏k2	= X kxi(t)	- x∏k2 +	β2 X IlVi(t)∣∣	- 2βt XVi(t)T(xi(t)	- x∏)
i=1	i=1	i=1	i=1
(40)
where x∏ = (1 一 ∏)x? ∈ (1 一 ∏)X ⊆ (1 一 ∏)B. Taking expectation on both sides of (40) and using
(39), yields
N
XE [L,t(xi(t), λi(t)) - Li,t(x?, λi(t))]
i=1
P= E[kxi(t) - x∏k2] - PL E[kxi(t +1) -Xnk2]
2βt
i=1
(41)
Using the definition (5), the left-hand side on (41) becomes
i=1
p
Li,t(xi(t);εt) + E[λi(t)]s[csX(t))]+ - η2tkλi(t)k2
p
K,t(x∏; εt) + X[λi(t)]s[cs(x∏)]+ -
s=1
η2t at)，]
XE [Li,tX(t); εt) - K,t(x∏; εt)] + X X Eb(Xi⑻田
i=1	i=1 s=1 ηt-1
(42)
where the equality follows from [cs(x∏)]+ = 0, because x∏ ∈ (1 - ∏)X ⊂ X. On the other hand,
it follows from (38) and Lemma 3(iii) that
E ∣∣∣VL i (t)∣∣∣2
p
V 'i,t(Xi(t)) + X[λi(t)]s∂[Cs(Xi(t))] +
s=1
p
≤ 2E [kV'i,t(xi(t))k2] + 2pG2 XE [[λi(t)]2]
s=1
1p
≤ 2C2d2 3 + 2pG2 X
εt	s=1
E[[csX(t))]+]
η2-ι
≤
N
E
—
E
(43)
19
Under review as a conference paper at ICLR 2020
this, combined with equations (41) and (42), gives
N
XE pi,t(xi(t); εt) - 'i,t(x∏； εt)]
i=1
≤ NC2d2 β2 +
εt2
PN=IE [kxi(t) - x∏k2] - PN=ι E [kx# +1) -Xnk2]
2βt
Np
- XX E [[cs(xi(t))]2+
i=1 s=1
(44)
the left-hand side on (44) can be further lower bounded by utilizing the relation between the losses
'i,t and their smoothed variants 'i,t (cf. Lemma 3(i)), given as follows:
~ , . 、 ~ 、 . ,一 ,. 一
'i,t(xi(t); εt) - 'i,t(Xn; εt) ≥ 'i,t(xi(t)) - 'i,t(Xn) - 2Gεt
≥ 'i,t(xi(t)) - 'i,t(x?) - GRXn - 2Gεt	(45)
where the last inequality follows from the fact that `i,t is G-Lipschitz and kx? k ≤ RX . Summing
the inequalities in (44) over t = 1, . . . , T and using (45), we find that
TN	T	T
XX
E ['i,t(xi(t)) - 'i,t(x?)] ≤ NGRXnT + 2NG X εt, + NC2d2 X Ie
T
+X
t=1
PN=IE [∣X(t) - x∏k2] - PN=IE [∣X(t +1) -Xnk2]
2βt
TNp
- XXX E [[cs(xi(t))]2+]
t=1 i=1 s=1
(46)
On the other hand, we have the following estimate of the disagreement among nodes by resorting to
Lemma 2:
T N	T-1 N
XXE[∣∣xi∙(t)-xi(t)k] ≤ C X βt XE [∣∣^7i(t)∣∣]
T N Cd p
≤ C X βt X C + G X
t=1 i=1	s=1
E [[cs(xi(t))]+]
ηt-1
T	TNp
≤NCCd X βt+C G X X XE…))心
(47)
where the second inequality is based on (43). Combining inequalities (46) and (47), and following
an argument similar to that of Theorem 1, we obtain
T	TT
E[Reg(i∙,T)] ≤ NGRXnT + 2NGXεt + NCCGdX βt + NC2d2 X βt
T
+X
t=1
PLE [kxi(t) - x∏k2] - PN=IE [kxi(t + 1) -Xnk2]
2βt
TNp
+ CG2 XXXE [[cs(Xi(t))]+]旦
t=1 i=1 s=1	ηt-1
TN p
- XXXE[[cs(xi(t))]2+]
t=1 i=1 s=1
(48)
20
Under review as a conference paper at ICLR 2020
Substituting ηt
T1c, Bt
apG2τC, ɛt = T1b and ∏ = R∖b into the preceding inequality, and
following similar lines as that of Theorem 1, yields
ʌ
NCCd
E [Reg(i∙,T)] ≤ 3NGT1-b + 卯G T +
NC2d2
apG2
1	TV∕^2
+ 1 apNG2R2x T C + ------T 1-c.
2 1 X	4a(a- 1)p
(49)
It follows from some simple algebra that the choice of b = C yields the optimal regret bound
O(T max{1-c/3,c}).
The bound on CACV can be derived by lower bounding the left-hand side on (44), that is,
N
N
X E IXt(Xi(t); εt) - 2i,t(Xn； εt)] ≥ -GX E[∣∣Xi(t) - x∏∣∣] ≥ -2NGRX
i=1
i=1
where the first inequality follows from Lemma 3(i) and the last one from Assumption 1. This,
combined With (44) and the expressions of η =击,βt
to
apG2Tc , εt = T1b and π = RXITb , leads
TNp
XXXE[cs(Xi(t))]2+≤
t=1 i=1 s=1
-ɪ (2aGRχT1-c + 1 a2pG2RX + C2⅛T 1+2b-2c
a - 1	2	pG2
a-1
where in the last inequality we used b =
inequality with the following,
TNp
2aGRχ + ɪa2 pG2Rχ +
T1-c
(50)
C. The desired result follows by combining the preceding
TNp
1/2
t=1 i=1 s=1
E[[cs(xi(t))]+] ≤E pNT	[cs(xi(t))]2+
t=1 i=1 s=1
≤
N
T N p	1/2
pNTXXXE [[cs(xi(t))]2+]
t=1 i=1 s=1
because of Jensen’s inequality. The proof is complete.
(51)
D Proof of Theorem 4
We first claim that the strongly convexity of the loss functions `i,t implies the strongly convexity of
their smoothed variants Qi,t with the same constant σ. This fact leads us to the following bound that
is analogous to (26):
T
N
TN
XXE LLi,t(xi(t),λi(t)) -
t=1 i=1
TN
L,t(x*, λi(t))i ≤ X β XE 怛i(t)
t=1	i=1
(52)
Following an argument similar to that of Theorem 2, we can replace the left-hand side on (52) by
p
the following, due to the fact that λi(t) is the maximizer of Li,t((Xi(t), λ) over λ ∈ Rp+:
TN
XXE [Li,t(xi(t), λ) - Li,t(x∏, λi(t))]
t=1 i=1
TN
XXE
Li,t(xi(t))+X[λ]s [cs(xi(t))]+ - ηt ∣λ∣2
t=1 i=1
s=1
p
Qi,t(Xn)+E[λi(t)]s[cs(x∏)]+ - η2tkλi(t)k2
(53)
—
p
2
TN	TN
xxE Rt(Xi(t)) -Li,t(x∏)] + E[g(λ)] + 2 xxηtE [kλi(t)k2]
21
Under review as a conference paper at ICLR 2020
where g(λ) is defined in (29). On the other hand, using (43) we have
≤ 2C2d2g + 2pG2E [kλi(t)k2]
εt2
(54)
Combining the equations (52), (53) and (54), and using ηt = 2pG2 βt, we find that for all λ ∈ Rp+
and T ≥ 3,
TN	T
E[g(λ)] ≤ XXE Fi,t(x∏) — &,t(xi(t))] + NC2d2 X β
t=1 i=1	t=1 t
≤ 2NGRXT +
N C 2 d2 τ 2bX 1
t=1 ;
≤ 2NGRχT + 2NC2d2 T2b log(T)
σ
(55)
where we recalled (32). Substituting λ = λ?, the maximizer of g(λ) over λ ∈ Rp+, into the (55),
the left-hand side on (55) becomes
E [g(λ?)] =
s=1
E	PtT=1 PiN=1[cs(xi(t))]+2
2N P一 η
p
≥X
s=1
Pt=1P= E[[cs(Xi(t))]+])2
2N P一 η
p
≥
(P3 P=I Pp=IE [[cs(Xi(t))]+])2
2PN P=I ηt
(56)
where the first inequality follows from Jensen’s inequality. Combining the inequalities in (32), (55),
and (56), yields
XXXE[[csM(t))]+] ≤ 44NG√E√T10g(T) + 4pNGCdT1/3log(T)
t=1 i=1 s=1	F	σ
where We used b = 3.
We now turn our attention to the regret bound. It follows from (55) and (56) that
TN	T
XXE
Pi,t (Xi(t)) - 'i,t(Xn)] ≤ NC2d2 X β
(E[ρ])2
2PN PT=I n
—
this, combined with (45), further leads to
T N	T	T β	(E [ρ])2
^NEWi,t(xi(t)) -4i,t(x?)] ≤ NGRXnT+2NGZεt + NC2d2 ɪj '-	P) 〃 .
t=1 i=1	t=1	t=1	t=1 ηt(57)
Then, following the similar lines as that of Theorem 2 and using the disagreement estimate (47), we
find that
T
E [Reg(i∙ ,T)] ≤ NGRXπT + 2NG X εt + NCCGd
t1
(E[ρ])2
2PN PT=I ηt
/
X βt + NC2d2 X β
t=1 εt	t=1 ε2
+ C G2E [ρ] -β--
ηt-1
|
'∙^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^
=h(E[ρ])
T	Tβ	Tβ 1	T
≤ NGRXπT+2NG X εt+NCC^Gd X βt + NC2d2 X βt + 止 NC2 X ηt
t=1	t=1 εt	t=1 εt 8p t=1
(58)
22
Under review as a conference paper at ICLR 2020
where the last inequality follows from the same reasoning as that of (35)-(37). This, combined with
ηt = 2pt , Bt = σ1t, εt = τ1b and π = RXITb, leads to
ʌ.
σ
E [Reg(i∙,T)] ≤ 3NGT1-b + 2NCCGdTb iog(τ) +
3T2blog(T) + - log(T)
σ	2σ
(59)
hence, the optimal regret bound follows by setting b = ɪ. The proof is complete.
23