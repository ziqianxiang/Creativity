Under review as a conference paper at ICLR 2020
PopSGD: Decentralized Stochastic Gradient
Descent in the Population Model
Ab stract
The population model is a standard way to represent large-scale decentralized
distributed systems, in which agents with limited computational power interact in
randomly chosen pairs, in order to collectively solve global computational tasks. In
contrast with synchronous gossip models, nodes are anonymous, lack a common
notion of time, and have no control over their scheduling. In this paper, we examine
whether large-scale distributed optimization can be performed in this extremely
restrictive setting.
We introduce and analyze a natural decentralized variant of stochastic gradient
descent (SGD), called PopSGD, in which every node maintains a local parameter,
and is able to compute stochastic gradients with respect to this parameter. Every
pair-wise node interaction performs a stochastic gradient step at each agent, fol-
lowed by averaging of the two models. We prove that, under standard assumptions,
SGD can converge even in this extremely loose, decentralized setting, for both
convex and non-convex objectives. Moreover, surprisingly, in the former case,
the algorithm can achieve linear speedup in the number of nodes n. Our analysis
leverages a new technical connection between decentralized SGD and randomized
load-balancing, which enables us to tightly bound the concentration of node pa-
rameters. We validate our analysis through experiments, showing that PopSGD
can achieve convergence and speedup for large-scale distributed learning tasks in a
supercomputing environment.
1	Introduction
Distributed machine learning has become commonplace, and it is not unusual to encounter systems
which distribute model training among tens or even hundreds of nodes. In this paper, we take this
trend to the extreme, and ask: would it be possible to distribute basic optimization procedures such as
stochastic gradient descent (SGD) to thousands of agents? How could the dynamics be implemented
in such a large-scale setting, and what would be with the resulting convergence and speedup behavior?
To get some intuition, let us consider the classical data-parallel distribution strategy for SGD Bottou
(2010). We are in the classical empirical risk minimization setting, where we have a set of samples S
from a distribution, and wish to minimize the function f : Rd → R, which is the average of losses
over samples from S by finding x? = argmin x Ps∈S fs(x)/|S|. Assume that we have P compute
nodes which can process samples in parallel. Data-parallel SGD consists of parallel iterations, in
which each node computes the gradient for one sample, followed by a gradient exchange. Globally,
this leads to the iteration:
P
xt+1 = xt - ηt	geti (xt),
i=1
where ηt is the learning rate, xt is the value of the global parameter, initially 0d, and geti (xt) is the
stochastic gradient with respect to the parameter obtained by node i at time t.
When extending this strategy to high node counts, two major bottlenecks are communication and
synchronization. In particular, to maintain a consistent view of the parameter xt , the nodes would
need to broadcast and receive all gradients, and would need to synchronize with all other nodes, at the
end of every iteration. Recently, a tremendous amount of work has been dedicated to address these
two barriers. In particular, there has been significant progress on communication-reduced variants
of SGD (e.g. Seide et al. (2014); Strom (2015); Alistarh et al. (2017b); Wen et al. (2017); Aji and
Heafield (2017); Dryden et al. (2016); Grubic et al. (2018)), asynchronous variants (e.g. Recht et al.
(2011); Sa et al. (2015); Duchi et al. (2015); Alistarh et al. (2018b)), as well as large-batch or periodic
model averaging methods, which aim to reduce the frequency of communication(e.g. Goyal et al.
(2017); You et al. (2017) and Chen and Huo (2016); Stich (2018)), or even decentralized synchronous
1
Under review as a conference paper at ICLR 2020
variants(e.g. Lian et al. (2017a); Tang et al. (2018); Koloskova et al. (2019)). Using such techniques,
it is possible to scale SGD to hundreds of nodes, even for complex objectives such as the training of
deep neural networks. However, in systems with node counts in the thousands or larger, some of the
communication and synchronization requirements of these algorithms become infeasible.
In this paper, we consider the classic population model of distributed computing Angluin et al.
(2006), defined as follows. We are given a population of n compute agents, each with its own input,
which cooperate to perform some globally meaningful computation with respect to their inputs.
Interactions occur pairwise, where the two interaction partners are randomly chosen in every step.
Thus, algorithms are specified in terms of the agents’ state transitions upon an interaction. The
basic unit of time is a single pairwise interaction between two nodes, whereas global (parallel)
time is measured as the total number of interactions divided by n, the number of nodes. Parallel
time corresponds intuitively to the average number of interactions per node to reach convergence.
Population protocols have a rich history in distributed computing (e.g. Angluin et al. (2006; 2007;
2008a;b;c); Alistarh et al. (2017a; 2018a)), and are standard in modelling distributed systems with
millions or billions of nodes, such as Chemical Reaction Networks (CRNs) Bower and Bolouri
(2004); Chen et al. (2017) and synthetic DNA strand displacement cascades Chen et al. (2013). The
key difference between population protocols and the synchronous gossip models (e.g. Xiao and Boyd
(2004); Lian et al. (2017a); Koloskova et al. (2019)) previously used to analyze decentralized SGD is
that nodes are not synchronized: since interactions occur randomly at arbitrary times, there are no
global rounds, and nodes lack a common notion of time.
While the population model is a theoretical construct, we show that it can be efficiently mapped to
large-scale super-computing scenarios, with large numbers of compute nodes connected by a fast
point-to-point interconnect, where we can avoid the high costs of global synchronization.
An immediate instantiation of SGD in the population model would be to initially assign one sample
si from the distribution to each node i, and have each node maintain its own parameter estimate xi .
Whenever two nodes interact, they exchange samples, and each performs a gradient update with
respect to the other’s sample. If we assume interaction pairs are uniform random (with replacement),
each node would obtain a stochastic gradient upon each interaction, and therefore each model would
converge locally. However, this instance would not have any parallel speedup, since the SGD instances
at each node are essentially independent.
In this context, a natural change to the above procedure is to have nodes i and j first perform a
gradient step, and then also average their resulting models upon every interaction. Effectively, if
node i interacts with node j, node i’s updated model becomes
Xi — Xi + Xj	n
igei(Xi) +gej(Xj)
(1.1)
2
where j is the interaction partner, and the stochastic gradients gei and gej are taken with respect to each
other’s samples. The update for node j is symmetric. In this paper, we analyze a variant of the above
protocol, which we call PopSGD, in the population protocol model.
We show that, perhaps surprisingly, this simple decentralized SGD averaging dynamic provides
strong convergence guarantees for both convex and non-convex objectives. First, we prove that,
under standard convexity and smoothness assumptions, PopSGD has convergence speedup that linear
in the number of nodes n. Second, we show that, if the objective is non-convex but satisfies the
Polyak-Eojasiewicz (PL) assumption, PoPSGD can still ensure linear convergence in the number of
nodes. Third, We show a Θ(√n) speedup in the non-convex case in the absence of this assumption,
matching or slightly improving results from previous work which considered similar models (Lian
et al., 2017b).
On the practical side, we provide convergence and speedup results using an efficient implementation
of PopSGD using Pytorch/MPI applied to regression tasks, but also to the standard CIFAR/ImageNet
classification tasks for deployments on a multi-GPU nodes, and on the Piz Daint supercomputer (Piz).
Experiments confirm the scalability of PopSGD. We also observe an improvement in convergence
versus number of SGD iterations per model at higher node counts, in both convex and non-convex
settings. In particular, using PopSGD, we are able to train the ResNet18 and ResNet50 He et al.
(2016) models to full accuracy using only 1/8 the number of SGD updates per model, compared to
the sequential baseline, resulting in fast convergence with nearly linear scalability.
2
Under review as a conference paper at ICLR 2020
This suggests that, even though interactions occur only pairwise, uniformly at random, and in an
uncoordinated manner, as long as the convergence time is large enough to amortize the information
propagation, the protocol enjoys the full parallel speedup of mini-batch SGD with a batch size
proportional to the number of nodes. While similar speedup behaviour has been observed in various
synchronous models for the convex case- e.g. Stich (2018); Koloskova et al. (2019), or for complex
accelerated algorithms Hendrikx et al. (2018)-we are the first to show that SGD does not require the
existence of globally synchronized rounds or global communication.
Central to our analytic approach is a new technical connection between averaging decentralized
SGD and the line of research studying load-balancing processes in computer science (e.g. Azar et al.
(1999); Mitzenmacher (2000); Talwar and Wieder (2007); Peres et al. (2015a); Boyd et al. (2006)).
Intuitively, we show PopSGD can be viewed as a composition between a set of instances of SGD-each
corresponding to one of the local parameters xi-which are loosely coupled via pairwise averaging,
whose role is to “balance” the models by keeping them well concentrated around their mean, despite
the random nature of the pairwise interactions. Our analysis characterizes this concentration, showing
that, in essence, the averaging process propagates enough information to globally “simulate” SGD
with a batch of size Θ(n), even though communication is only performed pairwise. We emphasize
that the convexity of the objective function in isolation would not be sufficient to prove this fact:
simply averaging n independent SGD models at the end of training would not lead to speedup in
the objective (please see e.g. Stich (2018) for a detailed discussion). Along the way, we overcome
non-trivial technical difficulties, such as the lack of a common notion of time among nodes, or the fact
that, due to the structure of SGD, this novel load-balancing process exhibits non-trivial correlations
within the same round.
Related Work. The study of decentralized optimization algorithms dates back to Tsitsiklis (1984),
and is related to the study of gossip algorithms for information dissemination Kempe et al. (2003);
Xiao and Boyd (2004); Boyd et al. (2006). Gossip is usually studied in one of two models Boyd et al.
(2006): synchronous, structured in global rounds, where each node interacts with a randomly chosen
neighbor, and asynchronous, where each node wakes up at times given by a local Poisson clock, and
picks a random neighbor to interact with. The population model is functionally equivalent to the
asynchronous gossip model, since the interaction times in the latter model can be “discretized” to
lead to pairwise uniform interactions. The key difference between our work and averaging in the
gossip model, e.g. Boyd et al. (2006), is that their input model is static (node inputs are fixed, and
node estimates must converge to the true mean), whereas we study the a dynamic setting, where the
models are updated in each round by SGD, and should remain concentrated around the parameter
mean as it converges towards the optimum. Several optimization algorithms have been analyzed in
this setting Nedic and Ozdaglar (2009); Johansson et al. (2009); Shamir and Srebro (2014). Tang
et al. (2018); Koloskova et al. (2019) analyze quantization in the synchronous gossip model.
Lian et al. (2017a;b); Assran et al. (2018) consider SGD-type algorithms in the gossip model.
Specifically, they analyze the same SGD averaging dynamic in the non-convex setting. Table 2
in the appendix summarizes their assumptions, results, and rates. Their results are phrased in the
synchronous gossip model, in which nodes interact in a sequence of perfect matchings, for which
they provide O(1/√Tn) convergence rates (under analytical assumptions). Further, they extend their
results to a variant of the gossip model where updates can be performed based on stale information.
Upon careful examination, we find that their results can be extended to the population proto-
col/asynchronous gossip model, although at the cost of slower convergence relative to the synchronous
case. For Lian et al. (2017b), the convergence rate bound requires that the total number of iterations
is Ω(n6), while in our case only O(n4) iterations are needed. The difference comes from the fact that
our bound on the potential Γ is tighter. For Assran et al. (2018), speedup with respect to the number
of nodes depends on the parameter C . Which in turn, depends on the dimension of the objective
function, number iterations for the graph given by edge sets of all matrices used in averaging to
be connected and the diameter of aforementioned graph. Unfortunately, in the population model
parameter C will not be a constant and this will eliminate the speedup. Further, these references
present scalability results for training neural networks using both synchronous and asynchronous
variants of their algorithms, thereby slightly relaxing their analytic assumptions.
Relative to this prior work, our contributions are as follows. First, we consider both convex and
non-convex objectives. We are the first to show linear speedup in the objective for convex objectives
in this decentralized asynchronous model. Our bounds in the non-convex case match or slightly
improve those presented above, under similar assumptions. Furthermore, under the PL condition,
3
Under review as a conference paper at ICLR 2020
we are the first to show linear convergence speedup in the non-convex case. Our analysis technique
relies on a fine-grained analysis of individual interactions, which is different than that of previous
work. From the implementation perspective, the performance of our algorithm is competitive with
that of previous methods, notably DA-PSGD Lian et al. (2017b) and SGP Assran et al. (2018).
Other instances in the literature which consider dynamic interaction models are Nedic et al. Nedic
et al. (2017), who present a gradient tracking algorithm in a different dynamic graph model, and
Hendrickx et al. Hendrikx et al. (2018), who achieve exponential convergence rates in a gossip model
where transmissions are synchronized across edges. The algorithm they consider is a more complex
instance of accelerated coordinate descent, and is therefore quite different from the simple dynamics
we consider. Neither reference considers large-scale deployments for non-convex objectives.
2	Preliminaries
The Population Protocol Model. We consider a variant of the population protocol model which
consists of a set of n ≥ 2 anonymous agents, or nodes, each executing a local state machine. (Our
analysis will make use of node identifiers only for exposition purposes.) Since our application
is continuous optimization, we will assume that the agents’ states may store real numbers. The
execution proceeds in discrete steps, where in each step a new pair of agents is selected uniformly
at random to interact from the set of all possible pairs. (To preserve symmetry of the protocols, we
will assume that a process may interact with a copy of itself, with low probability.) Each of the two
chosen agents updates its state according to a state update function, specified by the algorithm. The
basic unit of time is a single pairwise interaction between two nodes. Notice however that in a real
system Θ(n) of these interactions could occur in parallel. Thus, a standard global measure is parallel
time, defined as the total number of interactions divided by n, the number of nodes. Parallel time
intuitively corresponds to the average number of interactions per node to convergence.
Stochastic Optimization. We assume that the agents wish to minimize a d-dimensional, differen-
tiable and strongly convex function f : Rd → R with parameter ` > 0, that is:
(X - y)T (Vf(X)- Vf (y)) ≥ ' ∣∣x - yk2,∀x,y ∈ Rd.	(2.1)
Specifically, we will assume the empirical risk minimization setting, in which agents are given access
to a set of data samples S = {s1, . . . , sm} coming from some underlying distribution D, to a function
fi : Rd → R which encodes the loss of the argument at the sample si. The goal of the agents is to
converge on a model x* which minimizes the empirical loss, that is
m
X* = argminx f (X) = argminx(1/m)	fi(X).	(2.2)
i=1
In this paper, we assume that the agents employ these samples to run a decentralized variant of SGD,
described in detail in the next section. For this, we will assume that agents have access to stochastic
gradients geof the function f, which are functions such that E[ge(X)] = Vf (X). Stochastic gradients
can be computed by each agent by sampling i.i.d. the distribution D, and computing the gradient of
f at θ with respect to that sample. In the population model, we could implement this by procedure
either by allowing agents to sample in each step, or by assigning a sample si to each agent i, and
having agents compute gradients of their local models with respect to each others’ samples. We will
assume the following about the gradients:
•	Smooth Gradients: The gradient Vf (X) is L-Lipschitz continuous for some L > 0, i.e. for all
X, y ∈ Rd :
∣Vf(X)-Vf(y)∣ ≤L∣X-y∣.	(2.3)
•	Bounded Variance: The variance of the stochastic gradients is bounded by some σ2 > 0, i.e. for
all X ∈ Rd:
Ege(X) - E[ge(X)]2 ≤σ2.	(2.4)
•	Bounded Second Moment: The second moment of the stochastic gradients is bounded by some
M 2 > 0, i.e. for all X ∈ Rd :
E∣ge (X) ∣2 ≤M2.	(2.5)
4
Under review as a conference paper at ICLR 2020
3 The Population SGD Algorithm
Algorithm Description. We now describe a decentralized variant of SGD, designed to be executed
by a population of n nodes, interacting in uniform random pairs as per the population protocol model.
We assume that each node i has access to local stochastic gradients gei, and maintains a model estimate
Xi , as well as a local learning rate ηi . For simplicity, we will assume that this initial estimate is
0d at each agent, although its value may be arbitrary. We detail the way in which the learning rates
are updated below. Specifically, upon every interaction, the interacting agents i and j perform the
following steps:
1	% i and j are chosen uniformly at random, with replacement
2	upon each interaction between agents i and j
3	% each agent performs a local SGD step
4	Xi	— Xi - ηiei(xi)
5	Xj	— Xj - ηjej(xj)
6	% agents average their estimates coordinate-wise
7	avg — (Xi + Xj )/2
8	Xi	J avg
9	Xj	J avg
Algorithm 1: Population SGD pseudocode for each interaction between arbitrary nodes i and j.
We are interested in the convergence of local models : X1, X2, ..., Xn after T interactions occur in
total. For the theoretical reasons, in the case when f is convex, we derive convergence for yT which
is weighted average of average values of local models per step(See Theorem 4.1). In the beginning of
section 5 we show that by performing single global averaging step at time step 0 ≤ t < T , which
is carefully chosen from specified distribution, we can make sure that in expectation local models
converge with the same rate as yT .
Estimating Time and the Learning Rate. In parallel with the above algorithm, each agent maintains
a local time value V i , which is estimated using a local “phase clock” protocol. These local times are
defined and updated as follows. The initial value at each agent is V i = 0. Upon each interaction,
the interacting agents i and j exchange their time values. The agent with a lower time value, say
V i < Vj, will increment its value by 1(ties are broken arbitrarily). The other agent keeps its local
value unchanged. (We break ties arbitrarily.) Although intuitively simple, the above procedure
provides strong probabilistic guarantees on how far individual values may stray from the mean: with
high probability,1 all the estimates V i are in the interval [t/n - clogT, t/n + clogT], where c is a
constant.
Given the current value of Vi at the agent, the value of the learning rate at i is simply ηi = b/(nV i+a),
where a and b are constant parameters which we will fix later. This will ensure that the gap between
two agents’ learning rates will be in the interval [0.5, 2], w.h.p. (See Lemma 4.2.)
4 The Convergence of PopS GD in the Convex Case
This section is dedicated to proving that the following result holds with high probability:
Theorem 4.1. Let f be an L-smooth, '-Strongly convex function satisfying conditions (2.3)一
(2.5), whose minimum x? we are trying to find via the PopSGD procedure given in Algorithm 1.
Let the learning rate for process i at local time ti = nVti be ηti = b/(ti + a), where a =
max(2cn log T, 18n, 256L/') and b = 4n∕' are fixed(for some Constant c). Let the SeqUence
of weights Wt be given by Wt = (a + t)2. Define μt = Pn=ι Xi, ST = PT=O Wt ≥ ɪ T3 and
yτ == PT-01 wtμt. Then,forany time T, we have with probability 1 一 O(1/ Poly T) that
E[f (yτ) 一 f (x*)] ≤ 襄kμ0 - x*k2 + 64T(T +2a)σ2 + 9216TnM2L.	(4.1)
2Sτ	'Sτ	'2 Sτ
Discussion. We first emphasize that, in the above bound, the time T refers to the number of
interactions (as opposed to parallel time). With this in mind, we focus on the bound in the case
where T n, and the parameters M, L, and ` are assumed to be well-behaved. In this case, since
Sτ ≥ T3/3, the first and third terms are vanishing as T grows, and we get that convergence is
1An event holds with high probability (w.h.p.) if it occurs with probability ≥ 1 — 1 /Tγ, for constant γ > 0
and the total number of interactions - T .
5
Under review as a conference paper at ICLR 2020
dominated by the second term, which can be bounded as O(σ2∕T). It is tempting to think that this is
roughly the same rate as sequential SGD; however, our notion of time is different, as we are counting
the total number of SGD steps executed in total at all the models. (In fact, the total number of SGD
steps up to T is 2T, since each interaction performs two SGD steps.)
It is interesting to interpret this from the perspective of an arbitrary local model. For this, notice
that the parallel time corresponding to the number of total interactions T, which is by definition
Tp = T∕n, corresponds (up to constants) to the average number of interactions and SGD steps
performed by each node up to time T. Thus, for any single model, convergence with respect to its
number of performed SGD steps Tp would be O(σ2∕(nTp)), which would correspond to running
SGD with a batch size of n. Notice that this reduction in convergence time is solely thanks to the
averaging step: in the absence of averaging, each local model would converge independently at a
rate of O(σ2∕Tp). We note that our discussion assumes a batch size of 1, but it would generalize to
arbitrary batch size b, replacing σ2 with σ2 ∕b. We note that, due to the concentration properties of
the averaging process, the claim above can be extended to show convergence behavior for arbitrary
individual models (instead of the average of models μτ).
Proof Overview. The argument, given in full in the Additional Material, can be split into two steps.
The first step aims to bound the variance of the local models Xti at each time t and node i with
respect to the mean μt = Pi Xi∕n. It views this quantity as a potential Γt, which We show has
supermartingale-like behavior, which enables us to bound its expected value as O(ηt2n). This shows
that the variance of the parameters is always bounded with respect to the number of nodes, but also,
importantly, that it can be controlled via the learning rate. The key technical step here is Lemma 4.3,
which provides a careful bound for the evolution of the potential at a step, by modelling SGD as
a dynamic load balancing process: each interaction corresponds to a weight generation step (in
which gradients are generated) and a load balancing step, in which the “loads” of the two nodes
(corresponding to their model values) are balanced through averaging.
In the second step of the proof, We first bound the rate at which the mean μt converges towards x*,
where we crucially (and carefully) leverage the variance bound obtained above. This is our second
key technical lemma. Next, with this in hand, we can apply a standard argument to characterize the
rate at which the quantity E[f (yT) - f(x*)] converges towards 0.
Notation and Preliminaries. In this section, we overview the analysis of the PopSGD protocol. We
begin with some notation. Recall that n is the number of nodes. We will analyze a sequence of time
steps t = 1, 2, . . . , T, each corresponding to an individual interaction between two nodes, which are
usually denoted by i and j . Recall the definition of parallel time Tp = T∕n, where T counts the
number of pairwise interactions. For any time t, define by ηt = b∕(a + t) the “true” learning rate at
time t, where a and b are constants to be fixed later, such that a ≥ 2cn log n for some constant c. We
denote by x* the optimum of the function f.
Learning Rate Estimates. Our first technical result characterizes the gap between the “global”
learning rate ηt = b∕(a + t) (in terms of the true time t), and the individual learning rates at an
arbitrary agent i at the same time, denoted by ηti .
Lemma 4.2. Let ηti = b∕(a + nVti), be the learning rate estimate of agent i at time step t, in terms of
its time estimate Vti. Then, there exists a constant γ > 1 such that, with probability at least 1 - 1∕Tγ
(Here, T is a total number of steps our algorithms takes), the following holds for every T ≥ t ≥ 0
and agent i:
1	≤ Tnt ≤ 2.	(4.2)
2	_ ηi 一
Step 1: Parameter Concentration. Next, let Xt be a vector of model estimates at time step t, that
n
is Xt = (X1,X2,…，Xn). Also, let μt = 1 P Xi, be an average estimate at time step t. The
i=1
following potential function measures the concentration of the models around the average:
n
Γt = X kXi - μtk2.
i=1
With this in place, one of our key technical results is to provide a supermartingale-type bound on the
evolution of the potential Γt, in terms of M, ηt, and the number of nodes n.
6
Under review as a conference paper at ICLR 2020
Lemma 4.3. For any time step t and fixed learning rate ηt used at t, we have the bound
E[Γt+ι∣Γt] ≤(1 - n) Γt + 4ηtM (Γ)1/2 + 8η2M2.
Next, we unroll this recurrence to upper bound Γt in expectation for any time step t, by choosing an
appropriate series of non-constant learning rates.
Lemma 4.4. If a ≥ 18n , then the potential is bounded as follows
E[Γt] ≤ 36nb2/(t + a)2M2 = 36nηt2M2.
Step 2: Convergence of the Mean and Risk Bound. The above result allows us to characterize how
well the individual parameters are concentrated around their mean, in terms of the second moment
of the gradients, the number of nodes, and the learning rate. In turn, this will allow us to provide
a recurrence for how fast the parameter average is moving towards the optimum, in terms of the
variance and second-moment bounds of the gradients:
Lemma 4.5. For η ≤ 6^^, we have that
E∣"1 - x*『≤ (1 -号)Ekμt - x*k2 - 2nE[f (μt) - f (x*)] + 审 + 288⅛ml.
Finally, we wish to phrase this bound as a recurrence which will allow us to bound the expected risk
of the weighted sum average. We aim to use the following standard result (see e.g. Stich (2018)):
Lemma 4.6. Let {at}t≥0, at ≥ 0, {et}t≥0, et ≥ 0 be sequences satisfying
at+1 ≤(1 一 'α,at — αtetA + α2B + α3C,
for at = '(t+a), A > 0, B, C ≥ 0, ' > 0 then
A T-1	`a3	2T(T +2a)	16T
st ⅛ Wtet ≤ 4st a0+ —`st—B+E c,
for Wt = (a +1)2 and ST = PTo1 WT ≥ 1 T3∙
(4.3)
To use the above lemma, We set η = nat = '(ɑ),and the parameter b = 4n∕'. We also
use A = 1/2, B = 16σ2, and C = 288M2Ln2. Let yτ = n∣^ PZi PT=o1 wtX；. Also, let
et = E[f (μt) - f(x*)] and at = E∣∣μt - x*∣∣ .
Using convexity and Lemma 4.6 above We obtain the folloWing final bound:
E[f (yτ) - f (x*)] ≤ 等 kμ0 - x*k2 + 64T≡^ σ2 + W M 2L.	(4.4)
2St	'Sτ	'2 St
To complete the proof of the Theorem, We only need to find the appropriate value of the parameter
a. For that, we list all the constraints on a: a ≥ 2cn log T, a ≥ 18n and a器)≤ 蠢.These
inequalities can be satisfied by setting a = max (2cn log T, 18n, 256 LL). This concludes our proof.
5 Extensions
Convergence of local models and alternative to computing yT. Notice that Theorem 4.1 measures
convergence of f (yτ), where yτ = PT-o1 W EW= Xt = PTo1 Wμt, is a weighted average of
μt-s per step. Notice that actually computing yτ can be expensive, since we need values of local
models over T steps and it does not necessarily guarantee convergence of each individual model. In
order to circumvent this issue, we can look at the following inequality, which in combination with the
Jensen’s inequality gives us the proof of Theorem 4.1 (Please see Appendix for details) :
1 T-1
S- E wtE[f (Mt)- f (x*)] ≤
ST t=0
业kμo - x*k2 + 64t(T + 2a)σ2 + 92⅛Tn2M2L.
2St kμ0	k +	'St	+ '2St
(5.1)
What we can do is, instead of computing yT, we just sample time step 0 ≤ t ≤ T - 1 with probability
W and compute f (μt) = f (Pn=I Xti/n), by using single global averaging procedure. Observe that
Et[Eμt [f (μt)]] is exactly the left hand side of the above inequality.
7
Under review as a conference paper at ICLR 2020
Hence, we get the convergence identical to the one in Theorem 4.1 and additionally, since we are
using global averaging, we also guarantee the same convergence for each local model. Finally, we
would like to emphasize that in practice there is no need to compute yT or to use global averaging,
since local models are already converged after T interactions.
General Interaction Graphs. Our analysis can be extended to more general interaction graphs by
tying the evolution of the potential in this case. In the following, we present the results for a cycle,
leaving the exact derivations for more general classes of expander graphs for the full version. In
particular, we assume that each agent is a node on a cycle, and that it is allowed to interact only with
its neighbouring nodes. Again, the scheduler chooses interaction edges uniformly at random. In this
setting, we can show the following result, which is similar to Theorem 4.1:
Theorem 5.1. Let f be an L-smooth, '-Strongly ConvexfUnction satisfying conditions (2.3)一(2.5),
whose minimum x? we are trying to find via the PopSGD procedure on a cycle. Let the learning rate
forprocess i at local time ti = nV； be ηii = b/(ti+a), where a = max(2cn log T, 18n, 256L/') and
b = 4n∕' arefixed(for some constant C). Let the sequence ofweights Wt be given by Wt = (a +1)2.
Define μt = Pn==i Xt, ST = PT-o1 Wt ≥ 1T3 and yτ = ± PT-o1 wtμt. Then, for any time T,
we have with probability 1 - O(1/ poly T) that
E[f(yT) - f W] ≤ 却 “° -叫2 + 64T≡^ σ + 256≡n6 M
2ST	'ST	'3Sτ
Notice that for T n3, the second term dominates convergence and we can repeat the same argument
as for Theorem 4.1 to show O(σ2∕T) convergence (where T is the total number of interactions).
Next we provide the sketch of a proof for the PopSGD on a cycle case. See section ?? in the appendix
for the proof sketch.
The Non-Convex Case. Next, we show convergence for non-convex, but smooth functions. The
Following theorem deals with the bounded gradient case:
Theorem 5.2. Let f be an non-convex, L-smooth, function satisfying assumption 2.5, whose
minimum x? we are trying to find via the PopSGD procedure given in Algorithm 1. Let the learning
rate we use be η = n∕√Γ. Then, for any T ≥ n4:
T X EkVf(μt)k2 ≤ fμfl + 36祟 + 岑.
T t=0	T	T	T
Next, we show the similar result for the case when gradient is not bounded, but it’s variance is. This
can be achieved by carefully following steps given in the Lian et al. (2017b).
Theorem 5.3. Let f be an non-convex, L-smooth, function satisfying condition 2.4, whose minimum
x? we are trying to find via the PopSGD procedure given in Algorithm 1. Let the learning rate we use
be η = n∕√T. Then,forany T ≥ 4624max{1∕L2,1}n4, we have
X 1 fii f( 、|I2 V	E[f(μ°)] - E[f(x*)] *	4σ2	*	32L2σ2	768L2σ2
t⅛ TEkf(μt)k ≤	√T	+	√T	+	~√F	+ ~√~∙
Observe that, since T is the total number of interactions and is equal to nTp , where Tp is a parallel
time, in both theorems, We get convergence O(1∕√T)= OQNTpn). Which gives us 1∕√n
speedup over O(1∕√T) convergence of the sequential version. (Note that in the sequential case
parallel time and the total number of interactions are the same.)
Finally, we derive convergence for the case where gradient is bounded and function we are trying to
optimize satisfies the Polyak-Eojasiewicz (PL) assumption with constant a. More formally, function
f satisfies the PL assumption with constant α if :
2EkVf(μt)k2 ≥ α(f(μt) - f(x*)).	(5.2)
We use a similar approach to Haddadpour et al. (2019), but, our tighter analysis of Γ potential allows
us to remove global synchronization.
Theorem 5.4. Let f be an non-convex, L-smooth, function satisfying assumption 2.5 and PL-
assumption with constant α(5.2), whose minimum x? we are trying to find via the PopSGD procedure
8
Under review as a conference paper at ICLR 2020
given in Algorithm 1. Let the learning rate at time step t be η = ɑ(4\)(Local learning rates at time
Ti are ηi =加正黑))∙ Thenfor a ≥ 2cn log T and time T, with probability 1 一 O(1/ Poly T) we
have
E[f(μτ)] 一 f(x*) ≤
a3	4608L2M 2n2T
KTF(E[f (μ0)] - f (x))+ α3(T + a)3	+
64LM2
α2(T + a).
Observe that as in Theorem 4.1, this gives us a linear sPeeduP over the sequential version.
6	Experimental Results
In this section, we validate our results numerically by imPlementing PoPSGD in Pytorch, using
MPI for inter-node communication MPI. We are interested in the convergence behavior of the
algorithm, and in the scalability with resPect to the number of nodes. Our study is sPlit into simulated
experiments for convex Objectives-to examine the validity of our analysis as n increases—and large-
scale real-world exPeriments for non-convex objectives (training neural networks), aimed to examine
whether PopSGD can provide scalability and convergence for such objectives.
Convex Objectives. To validate our analysis in the convex case, we evaluated the performance
of PopSGD on three datasets: (1) a real-world linear regression problem (the Year Prediction
dataset Chang and Lin (2011)) with a 463, 715/51, 630 test/train split, and d = 90; (2) a real-
world classification problem (gisette Chang and Lin (2011)) with 6, 000/1, 000 test/train split, and
d = 5000; (3) a synthetic least-squares problem of the form (2.2) with f (x) = 1 ∣∣Aχ 一 b∣∣2, where
A ∈ Rm×d and b ∈ Rm, with m = 104 and variable d. As a baseline, we employ vanilla SGD with
manual learning rate tuning. The learning rate is adjusted in terms of the number of local steps each
node has taken, similar to our analysis.
Figure 1: PopSGD convergence (test loss at the step versus parallel time) for various node counts n on a real
linear regression (left) and logistic regression (right) datasets. The baseline is sequential SGD, which
is identical to PopSGD with node count 1.
Our first set of experiments examines train and test loss for PopSGD on the real-world tasks specified
above. We examine the test loss behavior with respect to the number of nodes n, and execute
for powers of 10 between 1 and 10000. Each node obtains a stochastic gradient by sampling 128
elements from the training set in a batch. We tuned the learning rate parameter for each instance
independently, through line search, and obtained learning rates in the interval [0.0005, 0.015] for
Gisette, and [0.05, 0.2] for Year Prediction.
Please see Figure 1(b) for the results.(The number of epochs is cropped to maintain visibility, but
the trends are maintained in general.) The results confirm our analysis; notice in particular the clear
separation between instances for different n, which follows exactly the increase in the number of
nodes, although the X axis values correspond to the same number of gradient steps for the local
model. In Appendix B, we present additional experiments which precisely examine the reduction in
variance versus the number of nodes on the synthetic regression task, confirming our analysis.
Training Neural Networks. Our second set of experiments tests PopSGD on the CSCS Piz Daint
supercomputer, which is composed of Cray XC50 nodes, each with a Xeon E5-2690v3 CPU and
an NVIDIA Tesla P100 GPU, using a state-of-the-art Aries interconnect. For this, we implemented
9
Under review as a conference paper at ICLR 2020
PopSGD in Pytorch using MPI one-sided primitives MPI, which allow nodes to read eachothers’
models for averaging without explicit synchronization. We used PopSGD to train ResNets on the
classic CIFAR-10 and ImageNet datasets.
Training proceeds in epochs, each of which is structured as follows. At the beginning of each epoch,
we shuffle the dataset and partition it among processes. Notice that, in data-parallel SGD, an epoch
ends after each process iterates exactly once over its partition, i.e. each sample is seen once. However,
Theorem 4.1 suggests that, for PopSGD, processes should iterate several times over their partitions,
for the corresponding gradient information to be propagated. To match this, we introduce a multiplier
constant mult, which counts the number of times each process will iterate through its partition before
an epoch is complete. At the same time, we scale down the total number of epochs executed by n,
the number of nodes. In practical terms, if sequential SGD trains ResNet50 in 90 epochs, decreasing
the learning rate at 30 and 60 epochs, then PopSGD with 32 nodes and multiplier 4 would use
90 * 4/32 ` 12 epochs per node, decreasing the learning rate at 4 and 8 epochs.
Since PopSGD scales almost linearly in terms of time per epoch (see Figure 2, middle), this should
ensure end-to-end speedup for PopSGD. In particular, for ResNet50, we obtain a 2x end-to-end
time-to-convergence speedup versus data-parallel SGD. Figure 2 shows the test and train accuracies
for the ResNet18 model trained on the ImageNet dataset, with 32 Piz Daint nodes and mult = 4, as
well as scalability versus number of nodes. The hyperparameters used for model training are identical
to the standard sequential recipe (batch size 128 per node), with the number of epochs scaled down to
12 per node.
Figure 2: PopSGD test accuracy using 32 nodes on Piz Daint, measured at a fixed arbitrary node. The X axis
measures SGD steps per model, whereas the Y axis measures Top-1 accuracy. The dotted red line
is the accuracy of the Torchvision baseline. PopSGD surpasses the test accuracy of the baseline by
0.34%, although it processes each sample 4× less times, and each model sees 8× less gradient updates.
The third graph shows the average runtime per batch for PopSGD (center) versus DA-PSGD (Lian
et al., 2017b) and SGP (Assran et al., 2018) on the same setup.
The results suggest that PopSGD can indeed preserve convergence, while being scalable and
competitive with state-of-the-art algorithms. Appendix B presents additional experiments for
ResNet50/Imagenet and ResNet20/CIFAR-10, which further substantiate this claim.
7	Discussion and Future Work
We have analyzed the convergence of decentralized SGD in the population model of distributed
computing. We have shown that SGD is able to still converge in this restrictive setting, and moreover,
under parameter and objective assumptions, can even achieve linear speedup in the number of agents
n in terms of parallel time. The empirical results confirmed our analytical findings. The main
surprising result is that PopSGD presents speedup behavior roughly similar to mini-batch SGD, even
though a node only sees one gradient update and a single model at a time. Our work opens several
avenues for extensions. One natural direction is to study PopSGD with quantized communication,
or allowing the interactions to present inconsistent (stale) model views to the two agents. Another
avenue is to tighten the bounds in terms of their dependence on the problem conditioning, and on the
objective assumptions.
References
Mpich: high performance and widely portable implementation of the message passing interface (mpi)
standard. http://www.mpich.org/. Accessed: 2018-1-25.
10
Under review as a conference paper at ICLR 2020
The CSCS Piz Daint supercomputer. http://www.cscs.ch/computers/piz_daint. Ac-
cessed: 2018-1-25.
Alham Fikri Aji and Kenneth Heafield. Sparse communication for distributed gradient descent. arXiv
preprint arXiv:1704.05021, 2017.
Dan Alistarh, James Aspnes, David Eisenstat, Rati Gelashvili, and Ronald L Rivest. Time-space
trade-offs in population protocols. In Proceedings of the 28th ACM-SIAM Symposium on Discrete
Algorithms, (SODA), pages 2560-2579, 2017a.
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Randomized
quantization for communication-efficient stochastic gradient descent. In Proceedings of NIPS
2017, 2017b.
Dan Alistarh, James Aspnes, and Rati Gelashvili. Space-optimal majority in population protocols. In
Proceedings of the 29th ACM-SIAM Symposium on Discrete Algorithms, (SODA), pages 2221-
2239, 2018a.
Dan Alistarh, Christopher De Sa, and Nikola Konstantinov. The convergence of stochastic gradient
descent in asynchronous shared memory. In PODC, pages 169-178, 2018b.
Dana Angluin, James Aspnes, Zoe Diamadi, Michael J Fischer, and Rene Peralta. Computation in
networks of passively mobile finite-state sensors. Distributed computing, 18(4):235-253, 2006.
Dana Angluin, James Aspnes, David Eisenstat, and Eric Ruppert. The computational power of
population protocols. Distributed Computing, 20(4):279-304, 2007.
Dana Angluin, James Aspnes, and David Eisenstat. A simple population protocol for fast robust
approximate majority. Distributed Computing, 21(2):87-102, 2008a.
Dana Angluin, James Aspnes, and David Eisenstat. Fast computation by population protocols with a
leader. Distributed Computing, 21(3):183-199, 2008b.
Dana Angluin, James Aspnes, Michael J Fischer, and Hong Jiang. Self-stabilizing population
protocols. ACM Transactions on Autonomous and Adaptive Systems, 3(4):13:1-13:28, 2008c.
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Michael Rabbat. Stochastic gradient push for
distributed deep learning. arXiv preprint arXiv:1811.10792, 2018.
Yossi Azar, Andrei Z Broder, Anna R Karlin, and Eli Upfal. Balanced allocations. SIAM Journal on
Computing, 29(1):180-200, 1999.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT’2010, pages 177-186. Springer, 2010.
James M Bower and Hamid Bolouri. Computational modeling of genetic and biochemical networks.
MIT press, 2004.
Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip algorithms.
IEEE/ACM Trans. Netw., 14(SI):2508-2530, June 2006. ISSN 1063-6692. doi: 10.1109/TIT.2006.
874516. URL https://doi.org/10.1109/TIT.2006.874516.
Chih-Chung Chang and Chih-Jen Lin. Libsvm: A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):27:1-27:27, May 2011. ISSN 2157-6904. doi: 10.1145/1961189.
1961199. URL http://doi.acm.org/10.1145/1961189.1961199.
Ho-Lin Chen, Rachel Cummings, David Doty, and David Soloveichik. Speed faults in computation
by chemical reaction networks. Distributed Computing, 30(5):373-390, 2017.
Kai Chen and Qiang Huo. Scalable training of deep learning machines by incremental block
training with intra-block parallel optimization and blockwise model-update filtering. In 2016 ieee
international conference on acoustics, speech and signal processing (icassp), pages 5880-5884.
IEEE, 2016.
11
Under review as a conference paper at ICLR 2020
Yuan-Jyue Chen, Neil Dalchau, Niranjan Srnivas, Andrew Phillips, Luca Cardelli, David Soloveichik,
and Georg Seelig. Programmable chemical controllers made from dna. Nature Nanotechnology, 8
(10):755-762, 2013.
Nikoli Dryden, Sam Ade Jacobs, Tim Moon, and Brian Van Essen. Communication quantization
for data-parallel training of deep neural networks. In Proceedings of the Workshop on Machine
Learning in High Performance Computing Environments, pages 1-8. IEEE Press, 2016.
John C Duchi, Sorathan Chaturapruek, and Christopher R6. Asynchronous stochastic convex
optimization. arXiv preprint arXiv:1508.00882, 2015.
Priya Goyal, Piotr Dolldr, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Demjan Grubic, Leo Tam, Dan Alistarh, and Ce Zhang. Synchronous multi-gpu deep learning with
low-precision communication: An experimental study. In EDBT, 2018.
Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck R. Cadambe. Local
SGD with Periodic Averaging: Tighter Analysis and Adaptive Synchronization. arXiv e-prints, art.
arXiv:1910.13598, Oct 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770-778, 2016.
Hadrien Hendrikx, Laurent MassouliC, and Francis Bach. Accelerated decentralized optimization
with local updates for smooth and strongly convex objectives. arXiv preprint arXiv:1810.02660,
2018.
Bjorn Johansson, Maben Rabi, and Mikael Johansson. A randomized incremental subgradient
method for distributed optimization in networked systems. SIAM Journal on Optimization, 20(3):
1157-1170, 2009.
David Kempe, Alin Dobra, and Johannes Gehrke. Gossip-based computation of aggregate information.
In 44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings., pages
482-491. IEEE, 2003.
Anastasia Koloskova, Sebastian U Stich, and Martin Jaggi. Decentralized stochastic optimization
and gossip algorithms with compressed communication. arXiv preprint arXiv:1902.00340, 2019.
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jio Hsieh, Wei Zhang, and Ji Liu. Can decentralized
algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic
gradient descent. arXiv preprint arXiv:1705.09056, 2017a.
Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous decentralized parallel stochastic
gradient descent. arXiv preprint arXiv:1710.06952, 2017b.
SCbastien Marcel and Yann Rodriguez. Torchvision the machine-vision package of torch. In
Proceedings of the 18th ACM international conference on Multimedia, pages 1485-1488. ACM,
2010.
Michael Mitzenmacher. How useful is old information? IEEE Transactions on Parallel and
Distributed Systems, 11(1):6-20, 2000.
Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization.
IEEE Transactions on Automatic Control, 54(1):48, 2009.
Angelia Nedic, Alex Olshevsky, and Wei Shi. Achieving geometric convergence for distributed
optimization over time-varying graphs. SIAM Journal on Optimization, 27(4):2597-2633, 2017.
Yuval Peres, Kunal Talwar, and Udi Wieder. Graphical balanced allocations and the one plus beta-
choice process. Random Struct. Algorithms, 47(4):760-775, 2015a. doi: 10.1002/rsa.20558. URL
https://doi.org/10.1002/rsa.20558.
12
Under review as a conference paper at ICLR 2020
Yuval Peres, Kunal Talwar, and Udi Wieder. Graphical balanced allocations and the 1 + beta-choice
process. Random Struct. Algorithms,47(4):760-775, December 2015b. ISSN 1042-9832. doi:
10.1002/rsa.20558. URL http://dx.doi.org/10.1002/rsa.20558.
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to
parallelizing stochastic gradient descent. In NIPS, pages 693-701, 2011.
C. M. De Sa, C. Zhang, K. Olukotun, and C. Re. Taming the wild: A unified analysis of hogwild-style
algorihms. In Advances in Neural Information Processing Systems, 2015.
F. Seide, H. Fu, L. G. Jasha, and D. Yu. 1-bit stochastic gradient descent and application to data-
parallel distributed training of speech dnns. Interspeech, 2014.
Ohad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In 2014 52nd
Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 850-
857. IEEE, 2014.
Sebastian U Stich. Local sgd converges fast and communicates little. arXiv preprint arXiv:1805.09767,
2018.
Nikko Strom. Scalable distributed dnn training using commodity gpu cloud computing. In Sixteenth
Annual Conference of the International Speech Communication Association, 2015.
Kunal Talwar and Udi Wieder. Balanced allocations: the weighted case. In David S. Johnson and
Uriel Feige, editors, Proceedings of the 39th Annual ACM Symposium on Theory of Computing, San
Diego, California, USA, June 11-13, 2007, pages 256-265. ACM, 2007. ISBN 978-1-59593-631-8.
doi: 10.1145/1250790.1250829. URL https://doi.org/10.1145/1250790.1250829.
Hanlin Tang, Ce Zhang, Shaoduo Gan, Tong Zhang, and Ji Liu. Decentralization meets quantization.
CoRR, abs/1803.06443, 2018.
John Nikolas Tsitsiklis. Problems in decentralized decision making and computation. Technical
report, Massachusetts Inst of Tech Cambridge Lab for Information and Decision Systems, 1984.
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad:
Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural
Information Processing Systems, pages 1508-1518, 2017.
Lin Xiao and Stephen Boyd. Fast linear iterations for distributed averaging. Systems & Control
Letters, 53(1):65-78, 2004.
Yang You, Igor Gitman, and Boris Ginsburg. Scaling sgd batch size to 32k for imagenet training.
arXiv preprint arXiv:1708.03888, 2017.
13
Under review as a conference paper at ICLR 2020
A Summary and comparison of results
In this section we compare convergence rates of existing algorithms, while specifying the bounds
they require for convergence. In the tables T -corresponds to the parallel time and n is a number of
processes. We use the following notations for needed bounds:
1.	σ2 - bound on the variance of gradient.
2.	M2 - bound on the second moment of gradient.
3.	PL - Polyak-Eojasiewicz assumption.
4.	d - bounded dimension.
5.	ρ - bounded spectral gap of the averaging matrix.
6.	τ - bounded message delay.
	Global synchronization Assumptions	Convergance Rate
PopSGD Local SGD	NO	σ2,M2	O(1∕Tn) YES	σ2,M2	O(1/T n) Table 1: convex case
	Global synchronization	Assumptions	Convergance Rate
PopSGD	NO	σ2,M 2	o(i∕√τn)
PopSGD	NO	σ2,M2,PL	O(1∕Tn)
PopSGD	NO	σ2	O(1∕√Tn)
LUPA-SGD	YES	σ2,M2,PL	O(1∕Tn)
AD-SGD	NO	σ2, ρ, τ	O(1∕√Tn)
SGP	NO	σ2 , d, τ	O(1∕√Tn)
Table 2: non-convex case
B Additional Experiments
Convex Losses. In these experiments, we examine the convergence of PopSGD versus parallel time
for different node counts, and compared it with the sequential baseline. More precisely, for PopSGD,
we execute the protocol by simulating the entire sequence of interactions sequentially, and track the
evolution of train and test loss at an arbitrary fixed model xi with respect to the number of SGD steps
it performs. Notice that this is practically equivalent to tracking with respect to parallel time. In this
case, the theory suggests that loss convergence and variance should both improve when increasing
the number of nodes. Figure 3(a) presents the results for the synthetic linear regression example with
d = 32, for various values of n, for constant learning rate η = 0.001 across all models, and batch
size 1 for each local gradient. Figure 3(b) compares PopSGD convergence (with local batch size 1)
against sequential mini-batch SGD with batch size equal to the number of nodes n.
Examining Figure 3(a), we observe that both the convergence and loss variance improve as we
increase the number of nodes n, even though the target model executes exactly the same number of
gradient steps at the same point on the x axis. Of note, variance decreases proportionally with the
number of nodes, with n = 128 having the smallest variance. Compared to mini-batch SGD with
batch size = n (Figure 3(b)), PopSGD with n = 128 has similar, but notably higher variance, which
follows the analytical bound in Theorem 4.1.
CIFAR-10 Experiments. We illustrate convergence and scaling results for non-convex objectives
by using PopSGD to train a standard ResNet20 DNN model on CIFAR-10 in Pytorch, using 8 GPU
nodes, comparing against vanilla and local SGD performing global averaging every 100 batches (we
found this value necessary for the model to converge). We measure the error/loss at an arbitrary
process for PopSGD. We run the parallel versions at 4 and 8 nodes.
14
Under review as a conference paper at ICLR 2020
(a) PopSGD convergence vs. n.
(b) PopSGD convergence vs. mini-batch SGD.
Figure 3: PopSGD convergence (training loss at the step versus parallel time) on the synthetic regression task
versus the number of nodes n (left), and versus sequential SGD with different batch sizes (right).
Sequential SGD is identical to PopSGD with node count 1. The cutouts represent zoomed views.
(a) Train accuracy for ResNet20/CIFAR10.
(b) Test error for ResNet20/CIFAR10.
(c) Time to 90% train accuracy.
The results in Figure 4(c) show that (a,b) PopSGD does indeed converge faster as we increase
population size, tracking the trend from the convex case; and (c) PopSGD can provide non-trivial
scalability, comparable or better than data-parallel and local SGD.
Training ResNet50 on ImageNet. Figure 4 shows the test and train accuracies for the ResNet50
model trained on the ImageNet dataset, with 32 Piz Daint nodes and mult = 4. PopSGD achieves
test accuracy within < 0.5% relative to the Torchvision baseline, despite the vastly inferior number
of iterations, in a total of 29 hours. By way of comparison, end-to-end training using standard
data-parallel SGD takes approximately 48h on the same setup.
C Complete Correctness Argument
Lemma 4.2. Let ηti = b/(a+nVti), be the learning rate estimate of agent i at time step t, in terms of
its time estimate Vi. Then, there exists a constant γ > 1 such that, with probability at least 1 - 1/TY
15
Under review as a conference paper at ICLR 2020
Train accuracy (Resnet50)
Test accuracy (Resnet50)
Figure 4: PopSGD train and test accuracy using 32 nodes on Piz Daint, measured at a fixed arbitrary node, for
training ResNet50 on ImageNet. The round multipler value is mult = 4. The X axis measures SGD
steps per model, whereas the Y axis measures Top-1 accuracy. The dotted red line is the accuracy of
the Torchvision baseline (Marcel and Rodriguez, 2010). PopSGD is below the test accuracy of the
baseline by < 0.5%.
(Here, T is a total number of steps our algorithms takes), the following holds for every T ≥ t ≥ 0
and agent i:
1	≤ ηt ≤ 2.	(C.1)
2	— ηi
Proof. Let Gt = P exp (Z (Vt - -n)) + P exp ( - Z (Vt - -n)), for some fixed constant Z .The
following lemma is proved as Theorem 2.10 in Peres et al. (2015b):
Lemma C.1. For any t ≥ 0, and some fixed constants e and θ, E [Gt] ≤ 4∣ n.
Subsequently, we can show that for any t ≥ 0 and agent i:
t	q	M arkov 4θ n
PrK-Viι≥qlogTi≤Pr[G≥Tq]	≤	z∙t.	(Cz
Hence, for large enough constant q, using union bound over T steps, we can show that there exists a
constant γ > 0 such that for every T ≥ t ≥ 0 and agent i, |	- Vi l≤ Z log T, With probability at
least 1 - 1/Tγ.
Let C be q, thus a ≥ 2cn log T = 2qn log T. This allows US to finish the proof of the lemma:
1	a + t ——Z n log T	j a +1 + q n log T
2	≤ a + t ≤ ηi ≤ a +1	≤ .
(C.3)
This allows us to bound the per step change of potential Γ, in terms of global learning rate ηt .
Lemma 4.3. For any time step t and fixed learning rate ηt used at t, we have the bound
E[rt+ι∣rt] ≤ (1 - n)rt + 4ηtM(nt-)	+ 8ηtM2.
Proof. First we bound change in potential ∆t = Γt+1 - Γt for some time step t > 0. Let ∆it,j be
a change in potential when we choose different agents i and j at random and let ∆it be a change in
potential when we select the same node i. We get that
Ehδ'Xti= XX n2 Eh^'jlXt] + X n2 EhAlχt].	(C.4)
i i6=j	i=1
16
Under review as a conference paper at ICLR 2020
We proceed by bounding a change in potential for fixed i 6= j . Observe, that in this case
μt+ι = μt - (ηiei(xi) + ηj而(Xj))/n and xi+ι = xj+ι = (Xi + Xj)/2 - (ηt7i(xi) +
ηj ej (Xj))/2.
Hence,
n2
Xi+1 - μt+ι = Xj+1 - μt+ι = (Xi + Xj)/2 - -2n-(ηiei(Xi) + ηjej(Xj)) - μt.
For k ∈/ {i, j}, since Xtk+1 = Xtk we get that
Xk+ι- μt+ι = Xk + -(ηiei(Xi) + ηje (Xj))- μt∙
This gives us that
Eh∆t,j∣Xti	=	EIkXi	+ Xj)/2	- Y(ηiei(Xi)	+	ηjej(Xj))-μtk2	-kXi	-μtk2
+ E∣∣(Xi + Xj)/2 - ---2(ηiei(Xi) + ηje(Xj))- μtk2 -kXj - μtk2
+ X (e∣M + 1(ηiei(Xi)+ηjej(Xj))-μt∣∣2-∣∣Xk-μtk2)
k∈{i,j}
=2k(Xi - 〃t)/2 + (Xj - μt"2k2 -kXi - μtk2 -kXj - ^2
-	2
-	丁Ehniei(Xi) + ηjej(Xj), (Xi- μt) + (Xj- μt)i
+	2( -2-^) 2Ekniei(Xi) + nj ej (Xj) k2
+	X (-Ehniei(Xi) + njej(Xj),Xk - μti + -2Ekniei(Xi) + njej(Xj)k2)
k∕{i,j} n	n
Observe that
Fact (2.5)	Lemma 4.2
Ekntigei(Xti)+ntgej(Xtj)k2 ≤2(nti)2Ekgei(Xti)k2+2(ntj)2Ekgej(Xtj)k2	≤	2M2 (nti)2+(ntj)2	≤	16nt2M2.
and
n
XEhniei(Xi) + njej(Xj), Xk - μti = 0.
k=1
Thus, we have that
E[∆t,j∣Xti ≤	2k(Xi-〃t)/2 + (Xj-〃t)/2k2-kXi-〃tk2-kXj-〃tk2
-Ehniei(Xi) + njej(Xj), (Xi - μt) + (Xj- μt)i
+ 32n2(-2-a)2M2 + X -62n2M2
k∕{i,j}
≤	- kXi - μtk2∕2 -kXj- μtk2∕2 + hXi - μt, Xj - μti
-Ehniei(Xi) + njej(Xj), (Xi - μt) + (Xj - μt)i
+ 8nt2M2 .	(C.6)
similarly we can prove that
Eh∆i∣Xt] ≤-Ehniei(Xi)	+	niei(Xi),	(Xi	-	μt)	+	(Xi	- 〃/ +	8n2M2.	(C.7)
By using inequalities C.6 and C.7 in inequality C.4 we get that
Ehd闷=XX -2Eh夕[Xti + X -2EhNXti
i i6=j	i=1
≤ - XX -2 (kXi - μtk2/2 + kXj - μtk2/2)+ XX -2 hXi - μt,Xj - μti
i i6=j	i i6=j
17
Under review as a conference paper at ICLR 2020
-XX n12Ehniei(Xi) + ηjg(Xj), (Xi - μt) + (Xj- μt)i + 8η2M2.
Observe that
1	n1	1	1
XX n hXt-μt,Xt- μt = X n hXt- μt, X Xt -μti = n X τ∣χt-μtk = - n rt.
i i6=j
and
i=1
j6=i
i
XX n2 (kXi - μtk2∕2 + kXj - μtk2∕2) = n-1 X kXi - μt k2 = n-1 rt.
i i6=j	i
Hence, we get that
Ehd闷 ≤-rt-XX*Ehniei(Xi)+njej(Xj), (X；-〃t) + (Xj-〃t)i+8*m2. (C.9)
Further, we have that
XX
n2Ehniei(Xi) + nj岳(Xj), (Xi - μt) + (Xj- μt)i
1	1	n 2ni
=XX n Ehniei(Xi), (Xj - 3)+ XX n Ehnjej (Xj ),(&-〃/ + X T Ehei(Xi),x; - 〃»
ij	ij
n 2 i	Cauchy-Schwarz n 2 i
E 子 Ehei(Xi),Xi -μti ≤	E 子 E Uei(Xi)IHx -川
i=1 n	i=1 n
i=1
XX 乎 Ehkei (χi)k] kχi -调 Je≤en XX 2ηi (Ekei(Xi) k2)1 kχi
i=1	i=1
Lemma 4.2 4ηtM	n i	Cauchy-Schwarz 4ηtM
≤	kX kXt- μt k	≤
nn
i=1
-μtk≤ X 2nMkXi-μtk
i=1 n
(n X kXi-μtk2)1/2 =4ntM( Γ )1/2
i=1	n
By plugging above inequality in inequality C.9, we get that
E[∆t∣Xt] ≤ -Γ +4ntM(Γ)1/2 +8n2M2.
Hence, considering the definition of ∆t and the fact that the above inequality implies
E[∆t∣Γt] ≤ - 1Γt + 4ntM(rt)1/2 + 8n2M2,
nn
we get the proof of the Lemma.
Lemma 4.4. If a ≥ 18 , then the potential is bounded as follows
E[Γt] ≤ 36nb2/(t+a)2M2 = 36nnt2M2.
Proof. We prove the lemma using induction. Base case t = 0 trivially holds, since Γt = 0. For
induction step, we assume that at time step t, E[Γt] ≤ 36nb2M2/(t + a)2. Our goal is to prove that
E[Γt+1] ≤ 36nb2M2/(t+a+ 1)2.
E[Γt+1] = E[E[Γt+1∣Γt]] Lem≤a4.3 (1 - 1 )E[Γt]+4ntMe[(r)1/2] +8^M2
Je≤en(i - £) EQ] + 4ntM (EhΓti) 1/2 + 8n2M2
(	1 ) 36nb2 M2	24b2M2	8b2M2
一(	n) (t + a)2 + (t + a)2 + (t + a)2
/ 36nb2 M2 ι
≤ (t + a +1)2 +
36nb2 M 2	36nb2 M 2
4b2M2
—
—
(t + a)2	(t + a + 1)2	(t + a)2
18
Under review as a conference paper at ICLR 2020
36b2M2	36nb2M2 (2(t + a) + 1)	4b2M2
(t + a + 1)2 +	(t + a)2(t + a + 1)2	(t + a)2
36nb2 M2	72nb2M 2(t + a +1)	4b2M2
一(t + a + 1)2 + (t + a)2(t + a + 1)2	(t + a)2 .
Using the fact that t + a + 1 ≥ a ≥ 18n in the above inequality allows us to get
/ 36nb2M2	4b2M2	4b2M2 / 36nb2M2
E[r t+1] ≤ (t + a + 1)2 + (t + a)2 - (t + a)2 ≤ (t + a + 1)2 .
Lemma 4.5. For η ≤ 6^^, we have that
E|"i - x*『≤ (1 - 为EM - x*『- 2tE[f (μt) - f (x*)] + 中 + ML∙
i
Proof. Let Ft be the amount by which μt decreases at step t. So, Ft is a sum of ηtgi(Xt) and
j
ηte(Xj) for agents i and j, which interact at step t Also, let Ft be the amount by which μt would
decrease if interacting agents used true gradients. That is, for agents i and j which interact at step t,
jj
Ft is sum of ηt Vf(Xi) and ηtNf(Xt).
E∣∣μt+ι	- χ* U = E∣∣μt	-	Ft	- χ* U = E∣∣μt	-	Ft -	χ* - Ft + FtF
=E∣∣μt	-	x*	- Ft∣ + E∣∣F∕	-	Ft『+ 2E(μt - x*	- Ft, Ft	- FtE	©.⑼
Observe that E[Ft] = Ft0, hence the last term in the equation above is 0. This means that in order to
upper bound E∣∣μt+ι - x* 11 ,we need to upper bound E∣∣μt - x* - Fj0∣∣ and E∣∣F0 - Ft∣∣ .
For the latter, we get that
2	nn i	j	2
EUFt' - FtU = n xx EU n (gi(Xt) - Vf(Xi))+η (et (Xj) - Vf(Xj 川
i=1 j=1
≤ n XX (C)2EiIei(Xi)- Vf(Xi)k2 + (N)2Ekej(Xj)- Vf(Xj)∣∣2
n i=1 j =1	n	n
n i 2
-X (⅛9	EIlei(Xi)-Vf(Xi)II2
i=1
Fact (2.4) - n	ηti 2 2 Lemma 4.2
- n 二 ∖ n / σ 一
i=1
22
16 σ>.
n2
For the former, we have that
E∣∣μt - x* - Ft,∣∣2 = E∣μt - x*I2 + E阐『- 2E(〃t - x*,Ft)
nn i	j
=E∣μt - x*k2 + F XXEk生Vf(Xi) + 曳Vf(Xj)∣∣2
n2	n	n
i=1 j=1
nn	i	j
- n XX 2E( μt - x*, n Vf(Xi) + ηt Vf X)
n i=1 j =1	n	n
≤ Ellμt - x*||2 +
nn
F X(ηi)2E∣∣Vf(Xi)Il2 - F XE(〃t - x*,ηiVf(Xi))
n i=1	n i=1
nn
E∣μt - x*k2 + n3 X(ηi)2E∣Vf(Xi)- Vf(x*)∣∣2 - n XE5- Xi + Xi - x*,ηiVf (Xi))
n i=1	n i=1
19
Under review as a conference paper at ICLR 2020
nn
Ekμt - χ*k2 + 滔 X(ηi)2EkVf(Xi)- Vf(χ*)k2 - n X混"-Xi, Vf(X；))
n	i=1	n i=1
4n
-覆 X ηiE(Xi-x*, Vf(XA
(C.11)
i=1
In ordertobound ∣∣Vf (Xi)-Vf (χ*)∣∣2 We can use the L-Smoothness property for convex functions,
in the following form
f(y) ≥ f(x) + hVf(χ),y - Xi + 1-∣∣VfIy)- Vf(χ)∣2.
2L
(C.12)
By setting y = Xti and x = x* We get
∣Vf(Xti) -Vf(x*)∣2 ≤ 2L(f(Xti) - f(x*)).	(C.13)
Additionally, by `-strong convexity, We have that
-	DXi - x*, Vf(Xi)E ≤ -(f (Xi) - f(x*)) - '∣Xi - x*k2.	(C.14)
By Cauchy-SchWarz inequality, We get that
-	2(〃t - Xi, Vf(Xi)E ≤ 2L∣Xi - μtk2 + ∣Vf (Xi)k2∕(2L)
=2L∣Xi - μt∣2 + ∣Vf(Xi)- Vf(x*)k2∕(2L)
Using L-smoothness property (C.13) in the above inequality gives us that
-	2"- Xi, Vf(Xi)E ≤ 2L∣Xi - μt∣2 + (f (Xi) - f (χ*))∙	(c.15)
By plugging inequalities (C.13), (C.14) and (C.15) in inequality (C.11), We get
n
用μt -x* - FtI ≤ E∣μt - x*k2 + 42XηiE∣Xi - μtk2
n i=1
8L n	2 n
+ 肃 ∑(ηi)2E[f(Xi) - f (x*)]-滔 ∑ηiE[f (Xi) - f(x*)]
2' 二,	C
-覆 X ηiEkXi - X*k2.
i=1
Observe that EkXi — x*k2, EIlXi — μt∣∣2 and E[f (Xi) 一 f (x*)] are non-negative terms, Thus, by
using Lemma 4.2 in the above inequality We have that:
E∣∣μt - x* - Ft,∣∣2 ≤ E∣μt -x*k2 + 学XEkXi - μtk2
n i=1
32Lη2 n	η	n
+ -ɪ ∑E[f (Xi) - f (x*)] - n ∑E[f (Xi) - f (x*)]
n i=1	n i=1
n
-⅞ X EkXi -x*k2
i=1
=E∣μt - x*∣2 +—2ntXEkXi- μt∣2
n2
i=1
_Lnt X 3 (32Lnt _
n2	(( n
By using n ≤ e in the above inequality we get that
20
E[f (Xi) - f (x*)] - 'E∣Xi- x*k2
Under review as a conference paper at ICLR 2020
E∣∣μt - x* — Ft]∣
≤ Ekμt—x*k2 + nη X EkXi- μt ∣∣2
n i=1
ηn 1
+ n^∑	-2Ef(Xi)-f(x*)]-'EkXi-x*k2
i=1
By Jensen’s inequality and convexity of f and square of norm we have that
E∣∣μt - x* - η F/1∣ ≤ Ekμt - x*k2 + n； X EkXi- μtk2
n	n i=1
+ ηt ( - 2E[f(μt) - f (x*)] - 'E∣μt - x*k2)
=(I- ηt')Ek〃t - x*k2 - ηn E[f (μt)- f (x*)]+8nL E[rt]
Lemma4.4 0 - nt`)Ek〃t -x*k2 - 2ηnEEf(μt) - f(x*)]
288η3M 2L
n
Finally, by using the above inequality in inequality (C.10) we get
κ∣∣	*∣∣2v/ 1 ηt'∖vw	*∣∣2 ηt叫"、* *>ι , 16σ2η2 , 288η3M2L
E∣∣μt+ι-χ∣∣ ≤ (ι- ―)Ekμt-χ k - 2nE[f(μt)-f(x)] + ~n^ + ---
Theorem 4.1. Let f be an L-smooth, '-strongly convex function satisfying conditions (2.3)一
(2.5), whose minimum x? we are trying to find via the PopSGD procedure given in Algorithm 1.
Let the learning rate for process i at local time ti = nVti be ηti = b/(ti + a), where a =
max(2cn log T, 18n, 256L/') and b = 4n/' are fixed(for some constant c). Let the sequence
of weights Wt be given by Wt = (a + t)2. Define μt = Pn=ι Xi, ST = PT-O Wt ≥ 3 T3 and
yτ = S^ PT-01 wtμt. Then,forany time T, we have with probability 1 - O(1/ Poly T) that
T E[f (yτ) - f (x*)] ≤ 襄 kμo - x*k； + 64T≡^ σ + T M 2L.
2ST	'Sτ	'2Sτ
Proof. We use Lemma 4.6 to solve the recurrence given by Lemma 4.5. For this we set ηt = αt =
'(4+na). That is, We set parameter b = 4n/'. We also use A = 1/2, B = 16σ2, and C = 288M2Ln2.
This way we can rewrite Lemma 4.5 as :
E∣∣μt+ι - x*∣∣ ≤ (1 - αt')E∣∣μt - x*k2 - AatE[f (μt) - f (x*)] + Bα2 + Cα3.
Further, let yτ = nSτ Pn=I PT=OL WtXi. Also, let et be E[f(μt) - f (x*)] and at = E∣∣μt - x*∣∣ .
By convexity of f We have that
1 T-L
E[f (yτ) - f (x*)] ≤ b E wtE[f (μt) - f (x*)]	(C.16)
ST t=0
Using this fact and the Lemma 4.6 We obtain the folloWing.
E[f(yτ) - f (x*)] ≤ T kμ0 - x*k2 + 64T ∖s +2a) σ2 + 92⅛26Tn2 M 2L.	(C.17)
2ST	'Sτ	'2Sτ
21
Under review as a conference paper at ICLR 2020
what is left is to find the appropriate a. For that we remember all the constraints on a:
a ≥ 2cn log T, a ≥ 18n and a工)≤ 奇. These inequalities can be satisfied by setting
a = max(2cn log T, 18n, 256^).
D NON CONVEX ANALYSIS WITH P L-ASSUMPTION
In this section we deal with the case when the function we are trying to optimize is non con-
vex but it satisfies P L-assumption(5.2) with constant α. More formally, function f satisfies PL-
assumption(5.2) with constant α.
Theorem 5.4. Let f be an non-convex, L-smooth, function satisfying assumption 2.5 and PL-
assumption(5.2) with constant α, whose minimum x? we are trying to find via the PopSGD procedure
given in Algorithm 1. Let the learning rate at time step t be η =ɑ滑。).Then, for any time T, we
have
E[f(μτ)] — f(x*) ≤
a3 gf∕ ʌ] ,/ *ʌʌ l 4608L2M2n2T
(a + T)3 ( [f (μ0)] — f (x )) + α3(T + a)3	+
64LM 2
a2(T + a).
Proof.
E[f (μt+ι)]	≤	E[f (Mt)] + EhVf(Mt),μt+ι — μti + 2 Ekμt+1 — μtk2	(D.1)
nn	i	j =E[f(μt)] + XX FEhVf(μt), — ηtei(Xi) — 曳Gj(Xjy) n2	n	n i=1 j=1 n	n	n	(D.2)
nn	i	j + XX 2n2 Ek n Gi(χi) + ηnt Gj (Xj)k2 i=1 j=1	(D.3)
nn	i	j ≤ E[f (μt)] + XX F EhVf (μt), — ηt Gi(Xi) — 曳 Gj (Xjy) n2	n	n i=1 j=1	(D.4)
nn	i	j + XX n Ehk * Gi(χi)k2 + k kη ej (Xj)k2i i=1 j=1	(D.5)
n 2	ηi	n 2L ηi =E[f (μt)] + X - EhVf (μt), — ηnt Gi (Xi)i + X ~n Ek ηnt Gi(Xi)k2.	(D.6)
i=1	i=1
Using E[ei(χ)] = Vf (x) and property (2.5) We can rewrite the above inequality as
E[f (μt+ι)] ≤ E[f (μt)] + X -EhVf (μt), — ηiVf (Xi)i + X 2L(η^M2 n	n	n3 i=1	i=1	(D.7)
n 2ηi =E[f (μt)] + X TEhVf (μt), Vf (μt) — Vf (Xf)) i=1 n	(D.8)
- X 萼EkVf (μt)k2 + X 2L(η^M2 i=1 n	i=1 n	(D.9)
ni ≤ E[f (μt)] + X η E [kVf (μt )k2 + kVf (μt) - Vf(Xi )k2] i=1 n	(D.10)
-X 2ηi EkVf (μt)k2+X 2Lnη^ M2 i=1	i=1	(D.11)
ni	ni	n =E[f (μt)] + X nEkVf (μt) - Vf(Xi)k2 - X nEkVf (μt)k2 + X	3 M 2 n3
i=1	i=1	i=1	(D.12)
22
Under review as a conference paper at ICLR 2020
Lemma 4.2
≤
n	n	n	L2
E[f(μt)] + X 晋EkVf(μt) - Vf(Xi)k2 - X *E∣Nf(μt)k2 + X -nFM2
i=1	i=1	i=1
(D.13)
L-smoothness
≤
E[f(μt)] + X 2L2ηtEkμt- Xik2 -
i=1 n
≡EkVfwk2 + 警M2. (D.14)
Recall that by Lemma 4.4 we have that E[Γt] = PZi Ekμt 一 Xik2 ≤ 36nη2M2, hence the above
inequality becomes:
E[f(μt+1)] - E[f(μt)] ≤ 72L2η3M2 - ηEkVf(μt)k2 + 驾M2.
n	2n	n2
Next we use P L-assumption(5.2), which says that
2EkVf(μt)k2 ≥ α(f(μt) - f(x*)).
Hence, we get that
E[f(μt+ι)] - E[f (μt)] ≤ 72L2n3M2 -吧(fg- f (x*)) + -Lη2M2.
n	n	n2
This can be rewritten as:
E[f (μt+ι)] - f (x*) ≤ (1 - ηtα)(E[f(μt)] - f(x*)) + 72L2n3M2 + 邛M2.	(D.15)
n	n	n2
Next as in the proof for convex case, we define Wt = (a +1)2 and we set ηt = α(t+a). We get that
上 ηtα (a + t)3α(i - _L)= (a + t)2α(/ +。- 4)
ηt	n	4n	t + a	4n
α	wt 1
≤ (a+1 -1)3 厂=q.
4n	ηt-1
By using this in inequality D.15 and unrolling recursion we get that
WT (E[f (μτ)] - f S ≤ (1 -书 W0 (E[f (μo)]-…+X1 Wt—+X1 Wt 学”2
4 3	T-1
(I - a)不^ (E[f(μo)]- f(x*)) + X
a 4n	t=0
1152L2 M 2n	TX
—α 一 + ⅛
32(t + a)LM2
αn
a3α
≤ In(E[f(μo)] -f(χ*)) +
1152L2M 2nT	16(T + a)2 LM2
2	1
α2	αn
Next we divide the above inequality by WT =(。咬几.We get that
E[f (μτ)] - f(x*) ≤
a3	mr∕ M ,，*、、, 4608L2M2n2T ,	64LM2
(a + T)3 ( [f (μ0)] - f (x )) + α3(T + a)3	+ α2(T + a).
E	Non-Convex Analysis with constant learning rate
In this section we address the case when function we want to optimize is non-convex by using
constant learning rate over all iterations and process. Let our learning rate be eta. Observe that since
ηti = ηt = η ≤ 2ηt, Lemma 4.3 holds. So, we get that
E[Γt+ι∣Γt] ≤(1 - n)Γt +4ηM(In)1/2 +8η2M2.	(E.1)
Using induction as in the proof of Lemma 4.4 we can prove that the similar results holds for the
constant learning rate as well:
Lemma E.1. For any constant learning rate η and t > 0, we have
E[Γt] ≤ 36nη2M2 .	(E.2)
Theorem 5.2. Let f be an non-convex, L-smooth, function satisfying assumption 2.5, whose
minimum x? we are trying to find via the PopSGD procedure given in Algorithm 1. Let the learning
23
Under review as a conference paper at ICLR 2020
rate we use be η = n∕√T. Then, for any T ≥ n4:
1 X1v^f( a∣∣2 < (f(μ0) - f(x*))q 36LM2 q 2LM2
Tt=0EkVfm)k ≤ —√T- + -√Γ + -√F
Proof.
L-smoothness	L E[f (μt+ι)]	≤	E[f (Mt)]+ EhVf (μt), μt+ι - μti + 2Ekμt+1 - μtk	(E.3)
nn =E[f (μt)] + XX FEhVf (μt), -ηei(Xi) - ηej(Xj) n2	n	n i=1 j=1	(E.4)
nn + XX F2 Ek ηei(xi) + Zej (Xj)k2 2n2	n	n i=1 j=1	(E.5)
nn ≤ E[f (μt)] + XX FEhVf (μt), -ηei(xi) - ηej(Xj)) n2	n	n i=1 j=1 n	n	n	(E.6)
nn + XX n Ehk ηei(χi)k2 + k ngj3 (Xj)k2i i=1 j=1 n	n	n	(E.7)
nn =E[f (μt)] + X 2 EhVf (μt), - ηei(Xi)) + X 2L Ek ηei(Xi)k2. n	n	nn i=1	i=1	(E.8)
Using E[gei(x)] = Vf (x) and property (2.5) we can rewrite the above inequality as	
n 2	η	n 2Lη2 E[f (μt+ι)] ≤ E[f (μt)] + E -EhVf (μt), -ηVf(Xt)) + E -LrM2 n	n	n3 i=1	i=1	(E.9)
n 2η =E[f(μt)] + E nEhVf (μt), Vf (μt) - Vf(Xt)) i=1 n	(E.10)
-X n? EkVf (μt)k2 + X 誓 M 2 t=1	t=1	(E.11)
n ≤ E[f(μt)] + X -⅞E[kVf(μt)k2 + kVf (μt) - Vf(Xi)k2] t=1 n	(E.12)
n	n2 -X -2EkVf (μt)k2 + X 驾M2 n2	n3 t=1	t=1	(E.13)
n η	n η	n 2Lη2 =E[f(μt)] + X -EkVf (μt) - Vf(Xt)∣∣2 - X -E∣∣Vf (μt)∣∣2 + X t=1	t=1	t=1	M2
	(E.14)
L-smoothness	n L2η	η	2Lη2 ≤	E[f (μt)] +	Ekμt - Xt∣∣2 -EkVf (μt)∣∣2 + TM2. n	nn	(E.15)
recall that by Lemma E.1 We have that E[Γt] = PZi E∣∣μt - Xtk2 ≤ 36-η2M2, hence the above inequality becomes:	
E[f(μt+ι)] - E[f(μt)] ≤ 36L2η3M2 - ηEkVf (μt)k2 + 汇M2. n	n	n2	(E.16)
by summing the above inequality for t = 0 to t = T - 1, We get that	
E[f(μτ)] - f(〃0) ≤ X1 (36L2η3M2 - ηEkVf (μt)k2 + 汇M2). n	n	n2 t=0	(E.17)
24
Under review as a conference paper at ICLR 2020
From this we get that :
T-1
X ηEkVf(μt)k2 ≤
t=0 n
"、和"T-1 36L2η3M2
f(μo) - E[f (μτ)] + >,-------
n
t=0
T-1 2Lη2	2
+ > T-M2.
n2
t=0
(E.18)
Note that E[f (μτ)] ≥ f (x*), hence after multiplying the above inequality by 和 We get that
1 T-1
T EEkVf(μt)k2
T t=0
≤
n(f 51"+36LM 2η2 + 2LM⅛
Tη	n
Observe that η = n/√T ≤ 1/n, since T ≥ n4. This allows Us to finish the proof:
1 T-1
T EEkVf(μt)k2 ≤
T t=0
n(f(μο) — f(x*)) ι 36LM2η ∣ 2LM2η
F 一	+ 一 十 一
Tη	n	n
(f(μo) — f(x*)) , 36LM2 + 2LM2
√T	√T	√T
Next we replace assumption 2.5 with assumption 2.4. We start by proving the following lemma:
Lemma E.2. For any time step t , we have:
Γ	4η2 n
E[∆t∣χt] ≤- 2n + 4η- X Ekei (χi)k2.
n n i=1
Proof. First we bound change in potential ∆t = Γt+1 — Γt for some time step t > 0. Let ∆it,j be
a change in potential when we choose different agents i and j at random and let ∆it be a change in
potential when we select the same node i. We get that
Ehd间=XX W Eh△河Xti + X J Eh占间.	(E.19)
i i6=j	i=1
We proceed by bounding a change in potential for fixed i 6= j . Observe, that in this case
μt+ι = μt — (ηei(Xi)+ ηgj (Xj))/n and Xi+1 = Xj+1 = (Xi + Xj )/2 — (ηgi(Xl)+ηgj (Xj))/2.
Hence,
n2
Xt+1 -	μt+1 =	Xt+1	- μt+1	=	(Xt	+ Xt )/2-2n~ (ηei(Xt )	+ ηgj (Xt)) -	μt.
For k ∈/ {i, j}, since Xtk+1 = Xtk we get that
Xk+1 - μt+1 = Xk + n(ηei(Xi) + ηgj (Xj))- μt.
This gives us that
EhAjIXt] = ElkXi + Xj)/2 — n-2(ηei(Xi) + ηgj(Xj))-曲|2 — ||相—曲|2
+ E∣∣(Xi + Xj)/2 — n—n2(ηei(Xi) + ηe(Xj))- μtk2 — kXj — μtk2
+ X (E∣∣Xk + 1(ηei(Xi) + ηgj (Xj))-〃t||2 — kXk—〃tk2)
k∈{i,j}	n
=2k(Xi	— μt)/2	+ (Xj	—	〃t)/2『	— kXi	— ^2 —	||Xj	—	〃t『
n2
—丁Ehηei(Xi) + ηej(Xj), (Xi — μt) + (Xj — μt)i
+ 2( M )2Ekη7i(Xi) + ηej (Xj)∣∣2
+ X (nEhηei(Xi) + ηej(Xj),Xk — μti + WEkηei(Xi) + ηej(Xj)k2)
k∈{i,j} n	n
25
Under review as a conference paper at ICLR 2020
Observe that
n
XEhnei(Xi) + ηgj(Xj),Xk - μti = 0.
k=1
Thus, we have that
E[∆i,jXti ≤ 2k(Xi -μt)∕2 + (χj-μt)∕2k2-kχi-μtk2-kχj-μtk2
-Ehnei(Xi) + ngj(Xj), (Xi - μt) + (Xj- μt)i
+4n2(1)2(Ekei(Xi)k2+Ekej(Xj)k2)+ χ 驾(Ekei(Xi)k2 + Ekej(Xj)k2)
2n	n
k∈{i,j}
≤	- kXi - μtk2/2 - kXj - μtk2/2 + hXi - μt, Xj- μti
-Ehnei(Xi) + ngj (Xj), (Xi- μt) + (Xj- μt)i
+n2(Ekgei(Xti)k2+Ekgej(Xtj)k2).	(E.21)
similarly we can prove that
Eh∆t∣Xt]	≤ -Ehnei(Xi)	+	ngi(Xi), (Xi -	μt)	+	(Xi	- 〃/	+	2〃2阳血(羽)『).(E.22)
By using inequalities E.21 and E.22 in inequality E.19 we get that
Ehd闷=XX n12Eh夕，闷 + X n12EhAXti
i i6=j	i=1
≤	- XX J (kXi - μtk2∕2 + kXj - μtk2∕2) + XX J hXi - μt, Xj - μti
i i6=j	i i6=j
1	2n2	n
-XX nEhnei(Xi) + nej(Xj), (Xi- μt) + (Xj - μt)i + -n- XEkei(Xi)k2.
ij
i=1
Observe that
1	n1	1	1
XX n hXt-μt,Xt- μti = X n hXt- μt, X Xt -μti = n X-kXt- μtk = - n □.
i i6=j	i=1	j 6=i	i
and
XX n12 (kXi - μtk2∕2 + kXj - μtk2∕2) = nn-1 X kXi - μt k2 = nn-1，
i i6=j	i
Hence, we get that
Eh∆t∣Xt] ≤-Γ-XX*Ehnei(Xi)+nej(Xj),(Xi-μt)+(Xj-μt)i+2n2XEkei(Xi)k2.
ij
Further, we have that
XX
*Ehnei(Xi) + ne∙(Xj), (Xi - μt) + (Xj - 〃/
i=1
(E.24)
XX n12 Ehnei(Xi), (Xj-
i j	i=1
n	n 2	2n
=X	Ehei(Xi), Xi - μt i ≤ X (Ekei(Xi)k2 + 2n kXi - 〃tk2)=X Ekei(Xi)k2 + Γ.
i=1	i=1	i=1
By plugging above inequality in inequality E.24, we get that
Γ	4n2 n
E[∆t∣Xt] ≤-Γ + -ɪ EEkei(Xi)k2.
n	n i=1
Hence, considering the definition of ∆t we get the proof of the Lemma.
26
Under review as a conference paper at ICLR 2020
Lemma E.3.
nn
XEkei(Xi)k2 ≤ 2nσ2 + 12LEn] + 6nEk X Vf (Xi)∕n∣∣2
i=1	i=1
(E.25)
Proof.
nn
XEkgei(Xti)k2=XEkgei(Xti)-Vf(Xti)+Vf(Xti)k2
i=1	i=1
nn
≤ 2XEkgei(Xti)-Vf(Xti)k2+2XEkVf(Xti)k2
i=1	i=1
n
=2XEkgei(Xti)-Vf(Xti)k2
i=1
n	nn
+ 2 XEkVf(Xi)- Vf (μt) + Vf (μt) - X Vf (Xj)∕n + X Vf(Xj)∕n∣∣2
i=1	j=1	j=1
n	n	nn
≤ 2XEkei(Xi) - Vf(Xi)k2 + 6XEkVf(Xi)- Vf(Wk2 + 6XEk X(Vf(μt) - Vf(Xj))∕n∣∣2
i=1	i=1	i=1	j=1
nn
+ 6 XEk X Vf(Xtj)∕nk2
i=1	j=1
n	nn	n
≤ 2nσ2 + 6LX Ekμt - Xik2 + -n XX Ekμt - Xj k2 + 6nEk X Vf (Xi)∕nk2
i=1	i=1 j=1	i=1
n
= 2nσ2 + 12LE[Γt] + 6nEk XVf(Xti)∕nk2.
i=1
Notice that for η ≤ ɪ the above two lemmas give us
E[Γt+1] ≤ (1 -
1
4n
n
)E[Γt] + 8η2σ2 + 24η2Ek X Vf (Xti)∕nk2.
i=1
(E.26)
Observe that since Pi∞=0 (1 - 1∕4n)i = 4n, the above equation results in
Lemma E.4.
T	T-1	n
XE[Gt] ≤ 32Tnη2σ2 + X 96nη2Ek X Vf(Xti)∕nk2
(E.27)
Now, we are ready to prove the following theorem
Theorem 5.3. Let f be an non-convex, L-smooth, function satisfying condition 2.4, whose minimum
x? we are trying to find via the PopSGD procedure given in Algorithm 1. Let the learning rate we use
be η = n∕√T. Then,forany T ≥ 4624max{1∕L2,1}n4, we have
T-1 1
ETEkf(μt)k2 ≤
t=0 T
E[f(μo)] — E[f(x*)]	4σ2	32L2σ2	768L2σ2
√T	+ √T + ~TfΓ +	√T
Proof.
L-smoothness	L
E[f (μt+1)]	≤	E[f (μt)] + EhVf (μt), μt+1 - μti + 2Ekμt+1 - μtk
27
Under review as a conference paper at ICLR 2020
nn
=E[f (μt)] + XX -2EhVf (μt), -ηei(Xi) - ηej(Xj)
n2	n	n
i=1 j=1
nn
+ XX 2EkEk ~9i (Xi) + "ej (Xj) k2
2n2 n	n
i=1 j=1
nn
≤ E[f (μt)] + XX -2 EhVf (μt), -ηei(Xi) - ηej (Xj))
n2	n	n
i=1 j=1 n	n	n
nn
+XX n Ehk neiX)k2+k nej (Xj)k2i
i=1 j=1 n	n	n
nn
=E[f (μt)] + X 2 EhVf (μt), -ηei(Xi)) + X 2L Ek ηei(Xi)k2
n	n	nn
i=1	i=1
nn
=E[f (μt)] + X 2EhVf (μt), -ηVf(Xi)) + X 2LEkηei(Xi)k2
n	n	nn
i=1	i=1
n
=E[f (μt)] + ηEk X(Vf (μt) - Vf(Xi))/nk2 - ηEkVf (μt)k2
nn
i=1
nn
-ηE| X Vf(Xi)/nk2 + X — Ekηei(Xi)k2
n	nn
i=1	i=1
≤ E[f(μt)] + ηL2E[Γt] - ηEkVf(μt)k2 - ηE| X Vf(Xi)/nk2
n2	n	n
i=1
+ 4σ2η2 + 24L3r2E[Γt] + 12Lη2Ek X Vf(Xi)/nk2.
n2 n3	n2
i=1
Next, we sum the above inequality from t = 0 to t = T - 1 and divide by T .
We get:
X1 ɪEkf(μt)k2 ≤ E[f(〃0)]-E[f(μT)] +4σ22η2 + X1 咤E[Γt] + X1 *E[Γt]
Tn	T	n2	n2T	n3T
t=0	t=0	t=0
T -1	2 n	T -1	n
+ X 穹e∣ X Vf(Xi)/nk2 - X MEl X Vf(Xi)/nk2
LemmaE.4 E[f(μo)] -E[f(μτ)]	4σ2η2	32η3L2σ2	TX196η3L2wuX, i	ll2
≤	--------T----------+ 丁 +	+ 工盛厂Ek 工 Vf(Xt)/nk
t=0	i=1
+ 768η4L2σ2 + X12304η4L2 Ek X Vf(Xi)/班
n2	n2T
t=0	i=1
-X1 %EI X Vf (Xi)/nk2.
t=0	i=1
Next we assume that η < 68L and η < 1. This allows us to eliminate terms with
E| Pin=1 Vf(Xti)/nk2 multiplicative factor from the above inequality, hence we get:
XIɪEu”讨2 V E[f(μo)] - E[f(μτ)] , 4σ2η2	32η3L2σ2	768η4L2σ2
ɪ, E Ekf(μt 川—	+	+	2	+	+	2	.
Tn	T	n2	n	n2
t=0
next, We divide the above inequality by η:
X1 ɪEkf (μt)k2 — n(E[f(〃0)]- E[f(μT)]) +4σ2η + 32η2L2σ2 + 768道尤.
T	Tη	n	n
t=0
28
Under review as a conference paper at ICLR 2020
Further, assuming that η ≤ 1/n, we get:
T-1 1
TEkf Ekf(μt)k2 ≤
t=0T
n(E[f (μo)] - E[f (μτ)])	4σ2η	32ηL2σ2	768ηL2σ2
+	+	+	+
T η	n	n	n
n(E[f (μo)] — Ef(X*)])	4σ2η	32ηL2σ2	768ηL2σ2
+	+	+
T η	n	n	n
E[f (μo)] — E[f(x*)]	4σ2	32L2σ2	768L2σ2
√t	+ √f + ~√F +	√t	.
Observe that all the assumptions on η are satisfied forT	≥ 4624 max{1/L2, 1}n4.
F	General Interaction Graphs
The crucial difference between interactions on arbitrary graph and interactions on clique graph comes
from the fact that we get different bound on Gamma potential.
In the case of arbitrary graph we can show that:
E[Γt+ι∣Γt] ≤(1 - Θ(λ2∕m))Γt +4ηtM(Γ)1/2 +8ηtM2.
Where λ2 is a second smallest eigenvalue of the Laplacian of the interaction graph and m is the
number of it’s edges. This allows us to bound Γ and then we can follow analysis of clique case to get
the convergence rate. For example, in a case of cycle
E[Γt+ι∣Γt] ≤ (1 - Θ(1∕n3))Γt + 4ηtM(r)1/2 + 8宿〃2.
and hence
E[Γt] ≤ O(n5ηt2M2 ).
and this allows us to prove Theorem 5.1.
29