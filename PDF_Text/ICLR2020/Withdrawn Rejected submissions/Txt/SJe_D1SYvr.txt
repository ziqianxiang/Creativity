Under review as a conference paper at ICLR 2020
Partial Simulation For Imitation Learning
Anonymous authors
Paper under double-blind review
Ab stract
Model-based imitation learning methods require full knowledge of the transi-
tion kernel for policy evaluation. In this work, we introduce the Expert Induced
Markov Decision Process (eMDP) model as a formulation of solving imitation
problems using Reinforcement Learning (RL), when only partial knowledge about
the transition kernel is available. The idea of eMDP is to replace the unknown
transition kernel with a synthetic kernel that: a) simulate the transition of state
components for which the transition kernel is known (sr), and b) extract from
demonstrations the state components for which the kernel is unknown (su). The
next state is then stitched from the two components: s = {sr, su}. We describe
in detail the recipe for building an eMDP and analyze the errors caused by its
synthetic kernel. Our experiments include imitation tasks in multiplayer games,
where the agent has to imitate one expert in the presence of other experts for
whom we cannot provide a transition model. We show that combining a policy
gradient algorithm with our model achieves superior performance compared to the
simulation-free alternative.
1	Introduction
Recent work on Imitation Learning (IL) offers a new approach to the problem. Torabi et al. (2018)
and Baram & Mannor (2018) suggest defining success if the agent and the expert influence the
environment in the same way and not if they necessarily take the same actions. They remove the
need to label expert actions and allow to settle for examples that include nothing but state transitions.
The objective function they define seeks to minimize the distance between the state-transition
density functions induced by the agent and the expert. However, since the state-transition dynamics
is usually complex and unknown, it can not be calculated explicitly and is instead estimated through
sampling. For reasons such as cost, time and safety, the sampling occurs in simulation and not
in the real plant. However, despite considerable advances in simulation technologies, it is still a
limited tool when it comes to complex real-world problems. Therefore, the adoption of advanced
IL methods is hindered.
Consider the self-driving car example. Given their intent, physical simulation of pedestrians
and vehicles can be done with high accuracy. However, simulating the intent itself, i.e., the
decision-making process of those entities, is challenging. In this example, simulating the transition
of other road users is hard (requires intent and physical transformation), however, simulating the ego
car that is controlled externally is feasible (requires physical transformation only). The unwelcome
solution, in this case, is to resort to behavior cloning (BC) that do not require simulation at all
(Pomerleau, 1991). However, as previously mentioned, BC follows a different success criterion
that requires access to expert actions and is less in line with the true definition of success. But
most importantly, BC discards two key ingredients of the problem: a) states order (i.e., it breaks
trajectories), and b) partial knowledge about the transition kernel (ego car in the example above). In
this paper we ask the following question:
Can we enjoy the benefits of contemporary imitation methods ifwe keep every
trajectory intact, and maintain partial information on the transition kernel?
We answer this question affirmatively and introduce a new model called expert-induced Markov
Decision Process (eMDP) that fulfill this wish. eMDP transforms a given set of demonstrations into
1
Under review as a conference paper at ICLR 2020
(a)	(b)	(c)	(d)
Figure 1: Illustration of the transition kernel in an eMDP. (a) two players are playing the game
of Pong. We wish to imitate the left (blue) one at the presence of the right (red) expert. The state is
split into two components: (b) Responsive components that we can simulate, and (c) Unresponsive
components that we do not know how to simulate. (d) the eMDP model produces the next state by
updating each component separately and then stitching them together. The unresponsive elements
(gray) are extracted as is from the consecutive state in the demonstration, while the responsive
element (blue player) is updated according to an external command. Notice that the original expert
is faintly visible next to the blue agent for visualisation purposes.
a Markov Decision Process (MDP), and we show that solving it amounts to solving an imitation
problem that seeks to match the state densities at each step. The idea of eMDP is to replace the
unknown transition kernel with a synthetic kernel that: 1) simulate the transition of state components
for which the transition kernel is known, and 2) extract from demonstrations the state components
for which the kernel is unknown (see illustration in Figure 1). To understand the conditions when
the use of eMDP is just, we derive a PAC result that bounds the error in the state-value function
between the eMDP model and the genuine model that uses the original transition kernel. Finally, we
show empirical results that stress the benefits of using eMDP when the transition kernel is partially
available and model-based approaches are not applicable.
2	Preliminaries
The following describes the mathematical formulation of MDPs and the assumptions we require to
build an eMDP model. Following after, we outline the optimization problem solved by eMDP.
2.1	Mathematical Formulation
We assume an MDP defined as M = {S, A, P, r, γ}, where S is a continuous state space endowed
with a metric dS , A is the action space, P : S × A × S → R+ is the transition probability induced
by a transition kernel F : S × A → S, r : S × A → R is the reward function and γ ∈ [0, 1] is the
discount factor. We also wish to define ρ0 : S → R+ as the initial state distribution.
Our interest is in special MDPs where S can be factored as S = Sr × Su , where Sr refers to state
components that are governed by a Responsive kernel Fr : S × A → Sr. Put in words, Sr represents
element in the state space that we can simulate (ego car in the example above). On the other hand, Su
represents the complement part of S , governed by an unknown kernel Fu : S × A → Su (other road
users in the example above). Therefore, it will be replaced by a synthetic kernel Fu : T → S that
will be explained in Section 4. We also assume the existence of an expert policy πe : S × A → [0, 1]
that is used to generate a set of state-only trajectories D = sie,0, ..., sie,T in=1.
To measure the distance between probability distributions we will use the Integral Probability Mea-
sure (IPM) formulation:
KSq)=Sup {1 g(s)p(s)ds TS g(s)q(s)ds∣: g ∈G o
(1)
The IPM formulation is attractive since it represents multiple distance measures that can be recov-
ered under different choices of G. For example, choosing G = {g : ||g||L ≤ 1}, where L is the
Lipschitz constant of g (see Definition 2.1), reduces Eq. (1) to the popular Wasserstein metric. Set-
ting G = {g : ||g||H ≤ 1}, where H represents a Reproducing Kernel Hilbert Space (RKHS)1, and
1Informally, a Hilbert space is a RKHS if the evaluation functional Ls : g → g(s) is continuous at any g.
2
Under review as a conference paper at ICLR 2020
Eq. (1) becomes the famous kernel-distance. Although the analysis in Section 5 applies to a set of
IPM measures, hereinafter we will set G = {g : ||g||L ≤ 1} to invoke the Wasserstein metric, which
was previously shown to be weak (Arjovsky et al., 2017).
Definition 2.1 (Lipschitz continuity). Define two metric spaces (M1, d1), (M2, d2). Let g : M1 →
M2 be a function from M1 to M2. The Lipschitz constant of g is given by:
d2(g(s1),g(s2))
∣∣g∣∣L =sup{ —d](s] S?) — : Sl,S2 ∈ Ml }∙
2.2	Problem Formulation
Let dtπ , dtE denote the state distribution at time t, induced by the agent and the expert respectively.
We wish to solve the following optimization problem:
∞
arg min X γtK(dtπ, dtE).	(2)
π t=1
However, we cannot calculate either of the distributions. While dtE can at least be estimated based
on D, d∏ cannot even be estimated because Fr ( F .To solve this problem, We turn to use d∏,
the distribution that is induced by F = {Fu , Fr } and Which can be estimated in the simulation. To
justify the replacement of d∏ by d∏ we propose the following proposition:
Proposition 2.1 (eMDP Optimal Solution). Let K(p, q) be an IPM measure over probability distri-
butions p, q. Let dtπ , dtπ denote the t-step state distributions induced by F and F respectively. Then,
if P∞=ι YtK(d∏ ,dE) = 0 then d∏ = d∏ forall t ≥ 0.
Proof. See Appendix A.	□
Proposition 2.1 ensures that the optimal solution of eMDP coincides with the optimal solution of
problem (2). However, it does not provide any guarantees when the optimum is not reached. In
Section 5 we bound the errors between the eMDP and the original MDP for any π. Motivated to
replace d∏ with d∏ in equation 2 we end up with a tractable optimization problem:
∞
arg min X YtK(d∏,dE)
π t=1
∞
arg min γt sup E |g(st) - g(ste)|.
∏	τr1	g∈GSt 〜d∏
t=1	Se 〜dE
(3)
Denoting rt = -|g(st) - g(ste)|, it is easier to see that equation 3 takes the form of adversarial imita-
tion learning (Ho & Ermon, 2016), with g playing the role of the discriminator. It is also interesting
to see that if dtπ, dtE were to describe transition densities, then (3) would coincide with GAILfO’s
objective function. In the same way, if we had chosen G = {g : ||g||L ≤ 1}, then (3) would have
taken the form of Wasserstein GAN (Arjovsky et al., 2017).
However, solving (3) requires to address two optimization problems concurrently, which compli-
cates the problem and leads to many instability issues associated with training GANs (Salimans
et al., 2016). Fortunately, we note a key feature in our setup that allows simplifying problem (3).
At each step, eMDP exposes two states: the synthetic state {srr,sU}〜d∏ and the reference state
{srr,e, sU,e}〜dE that share the same unresponsive component: Su = stu,e. Therefore, we know
that to achieve the optimal solution of (3) we must have that str = str,e. This condition can be en-
forced explicitly without invoking a neural-network based classification network. For example, by
restricting G to be a class of norm function. If this is the case, then using the help of Lemma 2.1 we
can upper bound (3) in the following way:
arg min γt sup E |g(St) - g(Ste)|	≤
π	t= 1	g∈G St〜d∏	gisa norm
t=1	Se 〜&%
∞
arg min γt sup E |g(St - Ste)|.	(4)
π 匕	g∈GSt 〜d∏	()
t=1	Se 〜dE
Lemma 2.1 (Norm Function Inequality). Let g be a norm function. Then ∀x, y the following holds:
|g(x) - g(y)| ≤ |g(x -y)|
3
Under review as a conference paper at ICLR 2020
Proof. See Appendix B.	□
To conclude, denoting r(str, str,e) = -g(str - str,e), where g(x) = ||x||dS, we arrive at the final
optimization problem of the eMDP model:
∞
arg	γt E r(str, str,e).
∏	.1	St 〜d∏
t=1	Se 〜&%
(5)
3	Related Work
Solving problem (5) is challenging. From one hand, the constraint that Fr ( F prohibits using
model-based (i.e., simulation-based) methods. On the other hand, the constraint on inaccessible
expert actions prohibits the use of BC or other simulation-free methods. This setup differs from
previous imitation learning configurations that were introduced in the literature. As no method
exactly fits problem (5), we turn to review the ones that can operate under slight modifications.
Methods requiring access to F, A, S : A fairly permissive simulation-based setup that violates our
setup twice. First, for assuming access to expert actions, and second, for assuming full access to
F. Algorithms of this type execute a simulation step Z 〜∏ followed by a policy improvement
step. DAGGer (Ross et al., 2011), an online no-regret algorithm, queries the expert at the states of
ζ to generate ground-truth actions. Policy improvement is carried out by minimizing a loss function
between expert and agent actions. GAIL (Ho & Ermon, 2016) and mGAIL (Baram et al., 2016) use
ζ to train a discriminator to classify between fake and authentic trajectories. The discriminator’s
classification probability is used as a reward signal that the agent seeks to maximize.
Methods requiring access to A, S : Included in this group are simulation-free methods that assume
access to expert actions. The most prominent approach of this class is BC that administers supervised
learning tools to the problem. I.e., it tries to directly learn a state-to-action mapping. The strength of
BC lies in its simplicity and is favorable when enough state-action pairs are available. When this is
not true, BC will tend to suffer compounding errors. Extensions ofBC include Bojarski et al. (2016)
that learn a mapping from images to vehicle steering commands, andTai et al. (2016); Giusti et al.
(2016) that learn a mapping between depth images to robot commands.
Methods requiring access to F, S : These are imitation algorithms that can train policies from
state-only trajectories when full access to F is available. Torabi et al. (2018) follows GAIL’s struc-
ture but uses a discrimination rule that is based on the state-transition distribution and not on the
state-action joint distribution. Baram & Mannor (2018) describes a similar setup, however, there,
the reward is modified to enhance stability using ideas of Preference-Based RL.
Methods requiring access to S : The most stringent approach in terms of prior knowledge is also
the only approach that can operate under the configuration proposed here. One notable example is
Time Contrastive Networks (TCN) (Sermanet et al., 2018) that proved able to train a robot controller
with zero-knowledge of F. However, TCN also requires multiple views (cameras) of the agent to
achieve good performance.
Methods requiring access to a recovered model M: Although not referred to explicitly, it is al-
ways possible, yet presumptuous, to use the demonstrations to learn a model M of the environment,
or at least parts of it. Depending on what MDP components are recovered, different approaches can
be invoked as listed above. For example, a complete recovery of the underlying MDP, including the
reward function r, also known as Inverse Reinforcement Learning (IRL) (Ng et al., 2000; Ziebart
et al., 2008), allows administering any RL algorithm with no constraints what so ever.
4	THE eMDP MODEL
The motivation of eMDP is borrowed from techniques for learning time-series models from demon-
strations (Venkatraman et al., 2015). Cascading prediction errors from forward-simulation with a
learned model can result in predicting infeasible states. To solve this, the demonstrations are used
to generate synthetic examples for the learner to ensure that prediction returns to typical states. In
4
Under review as a conference paper at ICLR 2020
the same spirit, we propose to construct an MDP that allows recovering from “off-route” states,
even with partial knowledge of the state dynamics. eMDP provides guidance that compares the re-
sponsive elements in the agent’s trajectory (Sr) to the responsive elements in the original trajectory
(Sr,e). In the following, we outline the elements of the eMDP model.
Action Space A: The action space of an eMDP can be defined arbitrarily and there is no require-
ment that A = AE since eMDP ignores expert actions.
State Space S: We assume a decomposable state space S = {Sr , Su }. Sr, where we demand that
Su ≡ Su,e. There is also a similar demand on the responsive part Sr, however this can be relaxed
provided that a suitable reward function is available (see below).
Reward Function r: The reward is derived directly from (5), and is defined over the metric of S :
rt = -||str - str,e ||dS. In Appendix C we demonstrate how additional shaping of rt is possible with
the help of prior knowledge about the correct relations between sr and su,e.
Discount Factor γ: The discount factor lies in the same range of [0, 1). We denote γ = 0 as the
BC-regime of eMDP (see Section 6.1), and γ close to one as the RL-regime of the model. At the
BC-regime, eMDP encourages π to be greedy w.r.t the short-horizon-expert, while at the RL-regime
it encourages π to follow the “long-horizon-expert”. Short-term experts can guide π to fix immediate
drifts that occur after short action sequences (easier). Long-term experts will help it to fix drifts that
compound after long action sequences (harder). Therefore eMDP naturally supports γ-annealing.
Transition Kernel F: Upon receiving an action, the new state is calculated in the following way:
•	Simulating the transition of the responsive component: Sr+1 = Fr (st, at).
•	Reflecting the transition of the unresponsive component: sU+1 ≡ Fu(t) = sU+1.
•	Stitching Sr, SU together to get the final state: st+1 = {s：+1, sU+1}.
That is, the responsive component is updated according to Fr , while the unresponsive component is
extracted as is from the next state in the demonstration.
Initial State Distribution ρ0 : The initial state distribution is equivalent to the empirical state dis-
tribution of the demonstration set. I.e., initial states are sampled uniformly across time-steps and
episodes in D: S0 = {Sr0, S0U} = {S(ri,e,t0), S(Ui,,et0) }, where i 〜 U[1,...n] is the episode index and
t0 〜U(1,…T) is the time index. For simplicity, hereinafter We omit the trajectory index i.
Absorbing Set B: The termination set is defined by the set of states that the agent reaches after
T - t0 steps. I.e., the last step in the current demonstration: B = {St|t = T - t0}. In Appendix D
we demonstrate how an augmentation ofB is possible using prior knowledge.
5	THEORETICAL JUSTIFICATION OF THE eMDP MODEL
Lemma 2.1 ensures that eMDP perfectly follows the original MDP at the optimum solution. How-
ever, it tells us nothing otherwise. As long as the optimum is not met, eMDP induces an improper
state-distribution which leads to an error. Denote the model governed by F as M, we are interested
to compare the performance of the agent in M to the one in the true model M. The error we wish to
calculate is the difference in the value function between the models, and in the following, we show
how to derive it from samples of {Str, Str,e}. Due to space constraints, we deffer parts of the proof to
the appendix and present here the highlights of the analysis.
Analysis Overview We rely on the IPM formulation introduced in Section 2 in the Wasserstein
form. We desire to use the Wasserstein distance since it can metrize the weak topology of the state
space. In this way, we can relate the error in a metric state space to a continuous distance in the
distribution space. We will show how to estimate the distance from samples, without requiring
access to the true, unavailable model M. Since expert samples are scarce, it is also important to
discuss the rate to which the empirical estimator converges to the real quantity, which we do so
in Appendix H. With the empirical estimator at hand, and assuming that the transition kernel is a
Lipschitz function (see Definition 5 in Asadi et al. (2018)), we are able to bound the divergence
between the state distributions after a single step with high probability. Following right after, we
5
Under review as a conference paper at ICLR 2020
relate the 1-step divergence to the one that compounds after n steps. All that is left is to use the
n-steps divergence to bound the error in the value function.
Empirical Estimation of the Single Step Divergence Given two i.i.d sample sets:
{Sr,i}rm=0, {Sr,e,i}rm=0 drawn randomly from PM(∙∣s, a) and PM(∙∣s, a) respectively, the empiri-
cal estimator of K is given by:
(6)
where PG(∙∣s, a)= * Pm=I 方后⑸ represents the next-state empirical distribution under model G,
(i)	(i)
ci	= +1 when	Si	=	Sr	for i =	1, ...m and	ci	= -1 when Sm+i	=	Sr,e. This estimator is strongly
consistent and converges to the population value as m → ∞ as the following proposition suggests:
Proposition 5.1. Let (S, dS be a totally bounded metric space. Then as m → ∞:
∣K(PM(∙∣s,a),PM (∙∣s,α)) -K (Pm(∙∣s, a), PM^ (∙∣s,α))∣ -4 0	(7)
Proof. See Proposition 3.2 in Sriperumbudur et al. (2012).
□
The points we sample from the eMDP model are a function of a specific expert realization. Dif-
ferent realizations will lead to different estimations. In Appendix H we draw a PAC result on the
convergence rate of the empirical estimator.
From 1-step Divergence to n-step Divergence In the following, we use the Lipschitz assumption
on the transition function to bound the n-step error using the error that compounds after a single
step2 * *.
Lemma 5.1. Let M, M be two MDPs with Lipschitz transition functions with constants L, L re-
SPectively. Let L = min{L, L}, and denote KPM q = maxK (Pm(∙∣s),Pm (∙∣s)) + m,δ,c,ν,C0,C1,
then for all n ≥ 1 the following holds:
n-1
P(K(PM(∙∣μ),PM^(∙∣μ)) ≥ KPm,m XLi) ≤2e-c,	⑻
i=0
where PGn(∙∣μ) is the distribution induced by model G after n steps taken from distribution μ.
Proof. See Appendix K.
□
Approximation Error in the State-Value Function We are now ready to present the main result
that relates the n-step divergence to the error in the value function. The following lemma is true
for all s:
Lemma 5.2. Let M, M be two MDPs with Lipschitz transition functions with constants L, L re-
spectively such that L = min{L, L}, and Lipschitz reward functions with a constant LR. Then, ∀s,
L ∈ [0,1)] and n ≥ 1, with probability at least 1 一 2e-c the following holds:
|V ( ) _V」)l≤ YLRKPm,m
I M⑶	M(s" -(1 一 γ)(1 -YL)
(9)
Proof. See Appendix I.
□
Not surprisingly, Lemma (5.2) show us that the correctness of the eMDP model depends on the
Lipschitz constant of the reward function (which we control) and on the maximal error between the
eMDP and the demonstration set: KP …
M,M
2Lemma 5.1 applies to several IPM measures besides the Wasserstein metric. The composition lemmas
we prove in Appendix J can be used in the Kantarovich-Rubinstein duality theorem to generalize the result to
compositions over transition function with bounded TV, Dudley or Wasserstein norms.
6
Under review as a conference paper at ICLR 2020
6 Empirical Evaluation
The evaluation was made between the BC baseline and a Policy Gradient (PG) algorithm (Schulman
et al., 2017) equipped with our eMDP model. Both agents used the same CNN-based policy (See
Appendix L for details), and the same set of 10 demonstrations, each of a maximal length of 500
steps. In the following, we briefly describe the construction of the eMDP model for each experiment.
Unless otherwise mentioned, both agents share the same action space.
Pong-v0 a two-player tennis-like game (Brockman et al., 2016). We record 2 human players. Our
goal was to imitate player1 at the presence of human player2. We do not have full knowledge about
the dynamics of the problem because human player2 can not be simulated. Therefore, we resort to
use our eMDP model in the following way: Fr = {player1}, Fu = {player2, ball}, and A consists
of 5 vertical move commands in oppose to 3 movements used by the human experts.
Surround a two-player game where each player controls a growing “snake” comprised of the
player’s trajectory (Gam, 2019). The player who first touches either snakes loses. We let two human
players to compete. The goal of the agent we trained was to imitate human player 1 at the presence of
human player 2. The eMDP model was constructed in the following way: Fr = {player1’s snake},
Fu = {Player2，s snake} and the action space consists of two turning commands.
Boxing a two-player game that simulates a boxing match from a top view camera (Gam, 2019).
We let two humans to compete. The goal of the agent we trained was to imitate human player
1 at the presence of human player 2. The eMDP was constructed in the following configuration:
Fr = {boxer1}, Fu = {boxer2}, and A consists of 5 movement commands and two hit commands.
Tennis a two-player game simulating a tennis match (Brockman et al., 2016). We let two human
players to compete. The goal of the agent we trained was to imitate human player 1 at the presence
of human player 2. The following eMDP model was created: Fr = {PlayerI}, Fu = {player2}
and the action space consists of 4 move commands and a single hit command.
Assault-v0, Carnival-v0 a single-player shooter game (Brockman et al., 2016) where the player’s
character faces multiple enemy shooters. We trained an agent to imitate a human player while using
a modified action space. Since we introduced modifications to the action space, we could no longer
use the emulator of the game, and model-based imitation methods were impractical to use. We
construct an eMDP model in the following way: Fr = {player's shooter} Fu = {enemy shooters}
and the action space consists of seven movement commands and a single shoot command. The BC
agent used the unmodified action space of the original game.
Breakout-v0 a single-player game (Brockman et al., 2016) that do not require the construction of
an eMDP model but is included here for its popularity. The eMDP was constructed in the following
way: Fr = {racket}, Fu = {ball, bricks} and the action space consists of seven horizontal move
command. The BC agent used the unmodified action space of the human player.
The performance of the PG agent and BC are brought in Figure 2. The graphs present the cumulative
return of the eMDP model as a function of the number of processed game frames. We note that this
is an unfair comparison since the BC agent was trained to solve a different optimization problem,
Therefore, we also provide video results for a visual comparison (see supplementary material).
6.1 Qualitative Evaluation
As shown in Figure 2, the PG agent clearly outperforms the BC baseline. In the following, we try
to formalize the claim introduced in Section 1 that the solution of BC (to induce the same actions)
deviates from the native objective of the problem (to induce the same effects on the environment).
To do so, we compare the optimal solution of BC and eMDP. For a fair and clear comparison, we
conduct the comparison at the BC Regime of eMDP (γ = 0) to encourage greedy policies that take
into account a single-step horizon, much like the BC solution that completely overlooks transitions.
Lemma 6.1 shows that the optimal policy assigns action probabilities proportionally to r:
Lemma 6.1. The solution of oPtimization Problem 5 with entroPy regularization at the BC-regime
(Y = 0) is given by： ∏RL(a∣s) = Pre：(?,!；/, ∙
Proof∙ See Appendix E.
□
7
Under review as a conference paper at ICLR 2020
Figure 2: Performance Comparison: results on 7 games. The x-axis represent the number of game
frames and the y-axis Numbers represent the cumulative return from the eMDP model.
On the other hand, the BC solution will seek to find the mode of the posterior distribution defined
by the training set and the prior function. For simplicity, we assume a coherent training set (i.e., that
∀si = sj : ai = aj ), although a similar result can be drawn in the general case. We also assume a
uniform prior function on the hypothesis set of τ -regularized policies: Πτ = {π | ∀(a, s) ∈ A × S :
∏(a∣s) ≥ T}. Lemma 6.2 shows that the optimal solution of BC assigns minimal probability to each
sub-optimal action and a probability of 1 - (|A| - 1)τ to the ground-truth one.
Lemma 6.2. Assume a set of policies Πτ with a uniform prior function and a coherent training set.
The solution of BC is given by:
∏bc (a|s) = [1-(1A|T)T，if y(S) = a	(10)
τ,	else
Proof. See Appendix F.	口
We can see that the optimal solution of BC distributes the probability evenly across all non-optimal
actions, regardless of what next state they induce. On the other hand, the optimal solution of eMDP
assigns probabilities proportionally to their immediate reward which is directly related to the error
in the state space which accounts for the deviation between the agent and the expert. Thus, while
BC is oblivious to action outcomes, eMDP considers the dynamics of the problem.
7 Conclusions
We introduced the eMDP model as a formulation of solving imitation problems using RL, without
requiring full knowledge about the state dynamics. eMDP can augment a training set by simulating
the components of the state space for which the transition model is known. However, the degree to
which the synthetic augmentation is reliable highly depends on the extensiveness of the responsive
kernel. For example, when the responsive kernel is minimal (Sr → 0, Su → S), so is the level
of synthetic augmentation. However, in this case the reward will be highly indicative of the task at
hand because we fix a large portion of S . Overall, we identify a tension between the reliability of
the model (measured in the reward), and its level of augmentation (See Appendix N for a further
discussion about kernel extensiveness). An additional factor that affects the eMDP model but was
not accounted for in our analysis is the level of interaction between Fr and Fu . As future research,
it would be interesting to suggest an improved model that can quantify the level of interaction and
use it to reward policies that follow the expert while leading to low Fr/Fu interaction.
8
Under review as a conference paper at ICLR 2020
References
Top 10 lists: The top 10 multiplayer games on the atari 2600. https://gamefaqs.gamespot.
com/top10/626-the-top-10-multiplayer-games-on-the-atari-2600,
2019. Accessed: 2019-09-10.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Kavosh Asadi, Dipendra Misra, and Michael L Littman. Lipschitz continuity in model-based rein-
forcement learning. arXiv preprint arXiv:1804.07193, 2018.
Nir Baram and Shie Mannor. Inspiration learning through preferences. arXiv preprint
arXiv:1809.05872, 2018.
Nir Baram, Oron Anschel, and Shie Mannor. Model-based adversarial imitation learning. arXiv
preprint arXiv:1612.02179, 2016.
Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon
Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning
for self-driving cars. arXiv preprint arXiv:1604.07316, 2016.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Alessandro Giusti, Jerome Guzzi, Dan C Cireyan, Fang-Lin He, JUan P Rodriguez, Flavio Fontana,
Matthias Faessler, Christian Forster, Jurgen Schmidhuber, Gianni Di Caro, et al. A machine
learning approach to visual perception of forest trails for mobile robots. IEEE Robotics and
Automation Letters,1(2):661-667, 2016.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. arXiv preprint
arXiv:1606.03476, 2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between
value and policy based reinforcement learning. In Advances in Neural Information Processing
Systems, pp. 2775-2785, 2017.
Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml, pp.
663-670, 2000.
Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neu-
ral Computation, 3(1):88-97, 1991.
Stephane Ross, Geoffrey J Gordon, and Drew Bagnell. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In AISTATS, volume 1, pp. 6, 2011.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in neural information processing systems,
pp. 2234-2242, 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey
Levine, and Google Brain. Time-contrastive networks: Self-supervised learning from video. In
2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 1134-1141. IEEE,
2018.
Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Scholkopf, Gert RG Lanck-
riet, et al. On the empirical estimation of integral probability metrics. Electronic Journal of
Statistics, 6:1550-1599, 2012.
9
Under review as a conference paper at ICLR 2020
Lei Tai, Shaohua Li, and Ming Liu. A deep-network solution towards model-less obstacle avoidance.
In 2016 IEEE/RSJInternational Conference on IntelligentRobots and Systems (IROS),pp. 2759-
2764. IEEE, 2016.
Faraz Torabi, Garrett Warnell, and Peter Stone. Generative adversarial imitation from observation.
arXiv preprint arXiv:1807.06158, 2018.
Arun Venkatraman, Martial Hebert, and J Andrew Bagnell. Improving multi-step prediction of
learned time series models. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. In AAAI, pp. 1433-1438, 2008.
10
Under review as a conference paper at ICLR 2020
Appendix
A OPTIMAL S OLUTION OF THE eMDP MODEL
The following Lemma ensures that the optimal solution of the eMDP model coincides with the
optimal solution of Eq 2.
Lemma A.1 (eMDP Optimal Solution). Let K(p, q) be an IPM measure over probability distri-
butions p, q. Let d∏, d∏ denote the t-step state distributions induced by F, {Fr, Fu} respectively.
Then, if P∞=ι γtK(d∏ ,dE) = 0 then d∏ = d∏ for all t ≥ 0
Proof. Let Fr, Fu be the responsive and unresponsive kernels in the original MDP. I.e., s(rt+1) =
Fr (st, at), sUt+1) = Fu(st, at). Let Fr, Fu be the responsive and unresponsive kernels inthe eMDP
model. I.e., s(rt+1) = Fr(st, at), s(ut+1) = tildeFu(st, at). The responsive kernels are equivalent
by definition: F ≡ F. Regarding the unresponsive kernel, we write:
FF(st,t) = Sut+ 1) = Fu(St,πE (St) = Fu(St, at) I	.
at=πE
Therefore, we get an equivalence in the unresponsive transition kernels at the point where actions
are taken according to the expert.
Using the kernel equivalence along with the fact that the initial state distribution is the same, d∏ =
d0π , it can be easily shown in induction that the state distribution remains the same for every step
t.	□
B	Norm Function Inequality
Lemma B.1 (Norm Function Inequality). Let f be a norm function. Then ∀x, y the following holds:
|f (x) -f(y)| ≤ |f(x-y)|
Proof. Using Minkowski’s inequality we have that:
f(x) +f(y-x) ≥ f(x+y-x) = f(y)
Which leads to:
f(y-x) ≥ f(y) - f(x)
And in the same way, since f is symmetric we have that:
f(y)+f(x-y) ≥ f(y+x-y) = f(x)
Which leads to:
f(x-y) = f(y-x) ≥ f(x) -f(y)
Combining the above we get that ∀x, y:
|f (y) -f(x)| ≤ f(y-x) = |f(y - x))|
□
C Responsive Reward
Consider a self-driving imitation task where a policy car Sr follows an expert car Sr,e . Sr is pe-
nalized via the Metric-reward for deviating from Sr,e: rmetric = ||Sr - Sr,e ||dS. However, prior
knowledge tells us that Sr should also maintain distance from other cars in the scene: Su,e. There-
fore, extra cost terms between Sr and Su,a can be added.
11
Under review as a conference paper at ICLR 2020
D Responsive Termination
Re-consider the self-driving example. We know that an accident or even a ”bump” with other cars
should terminate the current episode. Therefore, we can design an auxiliary responsive termination
signal by applying prior knowledge on the relations between Sr and Su,e.
E	The Optimal Solution of Entropy Regularized RL at The BC
Regime
Lemma E.1. The solution of optimization problem 5 at the bc-regime (γ = 0) is given by:
πRL(a∣s)
er(S,a"τ
PG er(S,G”τ
(11)
Proof. It was previously shown by Nachum et al. (2017) that the causal entropy term can be written
in the following recursive way:
H(s,π) = £n(a|s)[ -logπ(a∣s) + YH(s0,∏)],
a
which allows Us to draw a similar recursive connection between the optimal value function V*(s)
and the optimal policy:
e(r(s,a)+Yv*(S0))∕τ
πRL(a∣s) = -------U 、/---------
v I e	eV*(s)∕τ
We use the fact that γ = 0 at the BC regime to get that:
e(r(S,a)+YV*(S0))/T eV *(s)∕τ	二	e(r(s,a)+γv*(SO))/t “log P-e(r(s，a)+YV*(s0))∕τ e / -	、	(12) e (r(s,a)+Yv*(s0))/τ	er(s,a)∕τ .P e(r(s,a)+γV*(s0))∕τ = Pa er(s,a”τ
□
F The Optimal Solution of Entropy Regularized B ehavior
Cloning
Lemma F.1. Assume a set of policies Πτ with a uniform prior function and a coherent training set.
The solution of behavior cloning is given by:
πbc (a|s)
1-(∣A-1∣)τ,
τ,
if y(s) = a.
else
(13)
Proof. Entropy-regularized behavior cloning aims to solve the following maximum a posteriori op-
timization problem:
πbc = arg max P(π∣D).	(14)
π
Using Bayes rule and the naive assumption we have that:
» I八、	P(D∣∏)P(∏)
arg max P(π∣D) = arg max---	----
π	π	P (D)
=arg max P(π)Π(s,a)∈DP(s,a∣π) = arg max log P(∏)+	log ∏(a∣s)
(15)
(S,a)∈D
π
π
12
Under review as a conference paper at ICLR 2020
Define Πτ to be the set of policies with a τ lower bound on the allowed conditional probability:
∏τ = {π | ∀(a, S) ∈ A×S : π(a∣s) ≥ T}.
The prior function assigns zero-probability to functions outside of Πτ , and an even probability
measure of ∣∏j to each of its members. Therefore, We can limit the search to policies that reside in
Πτ:	τ
π
bc
arg max log P (π) +
π∈Πτ
E log π(a∣s)
(s,a)∈D
. Since the prior assigns an even distribution to all members in the set, We can remove the first term
and get that:
πbc = arg max ɪ2 log ∏(a∣s)
π∈Πτ (s,a)∈D
Removing the log function We end up With:
πbc = arg max ɪ2 ∏(a∣s)
π∈Πτ (s,a)∈D
From here it easy to see that the policy that maximizes the expression above is the one that assigns
the maximum alloWed probability to the ground-truth action While evenly distributing the remaining
Weight among the other ones:
∏bc(a∣s) = F-(A- 11)T，ify(S) = a.	(16)
τ, else
□
G Tractable Computation of The Empirical Probability Distance
Estimator
Calculating the estimator in ( 6) requires to solve an optimization problem. Therefore, it is not
straightforWard to calculate. In the folloWing, We present an easier form of the estimator as a solution
to a linear program.
Lemma G.1. For all α ∈ [0, 1] the following function attains the supremum in 6:
/∙∕∖	♦	∕*,7∕K∖∖
fα(s) ： = α min (a + ds(s, Si))
i=1,...2m
∕*
+ (1 — α) max (a* — ds(s,Si)),
i=1,...2m
(17)
where {ai*}i2=m1solve the following linear program:
max
a1 ,...a2m
2m
-X
m
i=1
ciai
(18)
such that:
_	, -T.	-T. .	, -T.	-T..
-ds (Si, Sj) ≤ ai - aj ≤ ds (Si, Sj), ∀i,j.
Proof. See Theoram 2.1 in Sriperumbudur et al. (2012)
□
13
Under review as a conference paper at ICLR 2020
H Convergence Rate of the Empirical Estimator
The points we sample from the eMDP model are a function of the specific realization of expert
demonstrations D. Different demonstration sets will lead to different estimations. In the following,
we draw a PAC result on the convergence rate of the empirical estimator.
Lemma H.1. Let F be the space of measurable functions such that ||f ∣∣∞ ≤ V, , Var {p,Q}(f) ≤
σ2pQ} ∀f ∈ F. Let C0,C1 < ∞, α > 0, δ ∈ (0,1), D = {Si}；=^1 〜P X P. Then ∀s the
following holds:
P(K(PM(∙∣s), Pm(∙∣s)) >
K(Pm(∙∣s), Pm(∙∣s)) + e(m,δ,c,ν,C0,C1)) ≤ 2e-c
where:
emac/QoG =2(⅛a2 (C。/2+ C1 d+∕m
+m √4c maχ{σ2p,P}}
+ 4CV (2∣1∣	1 + α )
m ∖3 α	δ(1 — δ) J
(19)
Proof. See Theorem 3.3 in Sriperumbudur et al. (2012)
□
I eMDP VALUE FUNCTION ERROR
In the following, we present the main result that relates the n—step divergence to the error in the
value function. The following lemma is true for all s:
Lemma I.1. Let M, M be two MDPs with Lipschitz transition functions with constants L, L re-
spectively such that L = min{L, L}, and Lipschitz reward functions with a constant LR. Then, ∀s,
L ∈ [0,1)] and n ≥ 1, with probability at least 1 — 2e-c the following holds:
1VM (s) — VM (s)| ≤
YLRKPm,m_
(1 — Y)(1 — γL)
(20)
Proof. (Following Theorem 2 in Asadi et al. (2018)) Let δs be a Dirac delta function in s, then, for
each state s, the difference between the value function under M and M is given by:
VM (s) —
∞
VM (S) = X Yn	T(SNM (s0∣δs) —PM (s0 ∣δs))ds
n=。
0
Define the function f (s) = rr(s) where LR is the LiPschitz constant of r(s). It can be easily shown
LR
that ||f ||L = 1. With the definition of f(S) we can re-write the state value gaP as:
VM(S) —
∞
VM (s)= Lr X Yn f f (s0)(PM (s0∣δs) —PM (s0∣δs))
n=。
ds0
Since f(s) ∈ F where F = {f : ||f ||L ≤ 1} we can bound the state value gaP from above by
taking the suPremum over F:
∞
VM(S) — VM (s) =Lr X Yn f f (s0)(PM (s0∣δs) —PM (s0∣δs))ds0
n=。
≤ LR
∞
XYn
n=。 sup
f (s0)(PM (s0∣δs) — PMM (s0∣δs )) ds0
f∈F
14
Under review as a conference paper at ICLR 2020
Following the definition of IPM in Eq equation 1, we see that the right hand side is exactly the
Wasserstein distance between the state distribution induced after taking n steps in M and M:
∞
VM(S)- VM(S)= Lr X YnK(PM(∙∣δs),PM (∙∣δs))
n=0
Relying on Lemma 5.1, with probability of at least 1 -2e-c we can upper bound the n-step deviation
as follows:
∞	∞	n-1
LRXYnK(PM(∙∣δs),pM(∙∣δs)) ≤ LRXYnkPm,m XLi.
n=0	n=0	i=0
Re-writing the right hand side we get that:
∞	n-1	∞	n
LR X YnkPm,m X Li = LRKPM,M X Yn二
n=0	i=0	n=0
And after re-arranging we can write that:
—	∞	1 _ Ln
LRKPM,M X Y I - L
n=0
K
T pm,M	Zn
LR E n=o Y
∞
-X(YL)n]
n=0
Using on the fact that L ∈ [0,1)] We know that the right hand side is finite:
T KPM,M hX 〜n	X(〜八 ni	T Kpm,M h 1	1 i
LRT-T[TY -T(YL) ]= LRi-ɪ[厂-ι-YLJ
n=0 n=0
Which can be written as:
L KPM,M h_J_____i == L KPM,M h Y(1 - L)	i = YLRKPM,M
R 1 - L [1 - Y 1 - YLl R 1 - L [(1 — y)(1 — YL)」(1 — y)(1 — YL)
Since K equipped with F = {f : ||f ||L ≤ 1} leads to the Wasserstein distance which is a metric, it
is therefore symmetric. We can repeat the process, this time with VM(S) — VM (s), which concludes
the proof.	□
J Composition Lemmas
We wish to make our result general to several IPMs besides the Wasserstein metric. In the following
we present two helpful lemmas.
J.1 Lipschitz Composition
The following lemma (Following Lemma 2 in Asadi et al. (2018)) shows that the composition of
two Lipschitz functions is also Lipschitz constant:
Lemma J.1. Define 3 metric spaces (M1, d1), (M2, d2), (M3, d3). Define Lipschitz functions g :
M1 → M2 , f : M2 → M3, then h : f ◦ g : M1 → M3 is also Lipschitz with a constant bounded
above by ||g||L||f ||L
Proof.
||h||L =	sup
s1,s2∈M1
d3(f(g(s1)),f (g(s2)))
d1(s1,s2)
sup
s1,s2∈M1
d2(g(si),g(s2)) d3(f(g(si)),f(g(s2)))
dl(si,S2)	d2(g(si),g(s2))
(21)
≤ sup
s1,s2∈M1
d2(g(s1),g(s2))	SU	ʤ(f(g(sl)),f (g(s2)))
d1(s1, s2)	s1,s2∈M2	d2(g(s1),g(s2))
||g||L||f||L
□
15
Under review as a conference paper at ICLR 2020
J.2 Dudley Composition
Recall that the Dudley metric can be extracted from the IPM formulation (Eq. 1) by setting
F = {f : Ilfl∣BL ≤ 1},
where Ilf ||bl ：= Ilf I∣∞ + Ilf ||l, and Ilf ∣∣∞ ：= sup{∣f (x)| : X eS}. Thefollowing lemma states
that the composition of two function with a bounded BL norm has a bounded BL norm itself.
Lemma J.2. Define 3 metric spaces (M1, d1), (M2, d2), (M3, d3). Define functions g : M1 →
M2 , f : M2 → M3. Then h : f ◦g : M1 → M3 has a bounded BL norm: IIhIIBL ≤ IIgIILIIf IIL +
IIfII∞
Proof.
d3(f(g(s1)), f(g(s2)))
IIhIIBL = sup -----------TI------ʌ------+ sup If(g(s3))I
s1,s2∈M1	d1(s1, s2)	s3∈M1
≤ IIgIILIIfIIL+ sup If(s0)I	(22)
s0∈M1
= IIgIILIIfIIL+IIfII∞
where the inequality is given by Lemma J.1.	□
J.3 Total Variation Composition
Recall that the TV metric can be extracted from the IPM formulation (Eq. 1) by setting
F = {f ： IIfII∞ ≤ 1},
where IIfII∞ := sup{If (x)I : x ∈ S}. The following lemma states that the composition of two
functions with a bounded TV norm has a bounded TV norm itself:
Lemma J.3. Define 3 metric spaces (M1, d1), (M2, d2), (M3, d3). Define functions g : M1 →
M2 , f : M2 → M3. Then h : f ◦ g : M1 → M3 has a bounded TV norm: IIhII∞ ≤ IIfII∞
Proof.
IIhII∞ = sup If(g(s))I ≤ sup If(s0)I = IIf II∞
s∈M1	s0∈M2
(23)
□
K FROM 1-STEP DIVERGENCE TO n-STEP DIVERGENCE
We prove this lemma following the proof Theorem 1 in Asadi et al. (2018). We generalize the result
to several IPM measures using the composition lemmas we proved in Appendix J.
Lemma K.1. Let M, M be two MDPs with Lipschitz transition functions with constants L, L re-
SPectively. Let L = min{L, L}, and denote KPM q = maxK (Pm(∙Is),Pm (∙IS)) + m,δ,c,ν,C0,C1,
then for all n ≥ 1 the following holds:
n-1
P(K(PM(∙Iμ),PM(∙Iμ)) ≥ KPm,m XLi) ≤2e-c,	(24)
i=0
where PGn(∙∖μ) is the state distribution induced by model G after n steps takenfrom an initial distri-
bution μ.
Proof. The proof is done by induction.
Induction base: Using the definition of an IPM we can write that:
K(Pm(W),Pm(∙Iμ)) = ；up/ / (PM(S0Is)-PM(s0Is))f(s0)μ(s)dsds0
16
Under review as a conference paper at ICLR 2020
/ (PM(Sls)- pM(Sls))f(SO)dS0M(S)dS = / K(PM(Sls),pm(Sls))μ(S)dS.
≤ sup
f∈F
We can use the fact that JS μ(s)ds = 1 to get:
=K(PM(S∖s),pM(Sls)) /μ(S)ds = K(PM(S∖s),pM(Sls)).
With probability at least 1 - 2e-c on the choice of D, the above can be bounded above by KPAt ^
which concludes the proof of the base of the induction.
Induction step: Assume that the claim holds for n - 1, i.e.:
n-2
K(PM-(∙∣μ),PM-I(W) ≤ KPm,m X Li
i=0
Using the triangle inequality we write that:
K(PM(∙∣μ),PM(∙∣μ)) ≤κ(PM(∙∣μ),PM^(∙∣PM-1(∙∣μ))) + K(PM(∙∣PM-1(∙∣μ)),PM(∙∣μ))
=K(PM (∙∣PM-1(∙∣μ)),PM^ (∙∣PM-1(∙∣μ))) + K(PM (∙∣PM-1(∙∣μ)),PM(∙∣PM-1(∙∣μ))).
≤K(PM (∙∣PM-PM (∙∣PM-1(∙∣μ))) + KPm,m .
To upper bound the first term we rely on the proof of the Kantarovich-Rubinstein duality theorem
(See Asadi et al. (2018), page 3 13). It is easy to see that using our composition lemmas (See Ap-
pendix J), the result proved there can be generalized in several ways: 1) A Dudley-metric composed
on a transition model with a bounded BL norm. 2) a TV-metric composed on a transition model
with a bounded T V norm. Therefore:
K(PM(∙∣PM-1(∙∣μ)),PM(∙∣PM-1(∙∣μ))) ≤ I∣p||K(PM-1(∙∣μ)),PM-1(∙∣μ))
where ∣∣PjM|| is the transition-model norm. E.g., L when reducing IPM to Wasserstein-metric to-
gether with a Lipschitz transition model as we shell assume for the rest of the proof:
K(PM(∙∣PM-1(∙∣μ)),PM^(∙∣PM-1(∙∣μ))) ≤ LK(PM-1(∙∣μ)),PM-1(∙∣μ))
Finally:
n-1
K(PM(∙∣μ),pM(∙∣μ)) ≤ LK(PM-1(∙∣μ)),PM-1(∙∣μ)) + K^,心 ≤ KPm,m XLi
i=0
□
L	Policy Network Architecture & Technical Details
The CNN architecture used by all agents is described in Table 1. The state is a concatenation of last
four frames, where each frame is 80 × 80 gray-level pixels. Although our method is agnostic to the
action space, all environments we tested have discrete action spaces (not intentional). We used the
Adam optimizer Kingma & Ba (2014) across all experiments. We used a linearly decaying learning
rate with an initial coefficient of 7e - 4. All agents were trained on a single workstation with a single
GPU/CPU.
17
Under review as a conference paper at ICLR 2020
Layer Type	Size	Details	Activation
2D Convolution	32 filters	kernel size = 8, stride=4	Relu
2D Convolution	64 filters	kernel size = 4, stride=2	Relu
2D Convolution	64 filters	kernel size = 2, stride=1	Relu
Fully Connected	512		Relu
Fully Connected	|A|		
Table 1: Policy Network Architecture
M Further Experimental Results
Videos results can be found in the following link: https://drive.google.com/drive/
folders/1FSveO1JLNn4bNoP9FvfKHLRYnSYifxCn. Note that the expert is overlaid in
red for visualization purposes only.
N On the Effects of Kernel Extensivenes s
In the following, we discuss the effects of using an extensive kernel in the eMDP model, in the case
of extensive knowledge about the transition kernel, vs the case of using a minimal kernel in the case
of minimal prior knowledge.
BC Limit: Sr → 0, Su → S With insufficient prior-knowledge about the world, the responsive
kernel is minimal. In this case, partial simulation barely enriches the demonstration set D with new
simulated states. However, in this case, the reward signal provided by a PS-Env is highly indicative
of the task at hand. This is true because we fix a large portion of the state space (Su → S). Therefore,
the hypothetical behavior of the expert in the observed state S should align to a high degree with the
recorded behavior of the expert Sr,e upon which we define rmetric.
Model-Based Limit: Sr → S, Su → 0 On the contrary, with extensive knowledge about the world,
a comprehensive responsive kernel can be designed. Partial simulation can enrich the training set
with numerous new simulated states. Yet, the reward signal provided at this limit is dubious: we fix
a small subset of the state (Su → 0), and as a result we have fewer guarantees on the hypothetical
behavior of the expert in the observed state S with respect to the recorded behavior Sr,e upon which
we reward the agent. To exemplify this problem, consider the extreme case where Sr ≡ S . At the
presence of any randomness, the simulated trajectory will completely diverge from the reference
trajectory, thus ruling out the utility of rmetric . The only case in which rmetric maintains relevancy
is in a completely deterministic environment.
18