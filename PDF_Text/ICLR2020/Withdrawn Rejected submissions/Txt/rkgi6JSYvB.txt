Under review as a conference paper at ICLR 2020
GraphFlow: Exploiting Conversation Flow
with Graph Neural Networks for Conversa-
tional Machine Comprehension
Anonymous authors
Paper under double-blind review
Ab stract
Conversational machine comprehension (MC) has proven significantly more chal-
lenging compared to traditional MC since it requires better utilization of conver-
sation history. However, most existing approaches do not effectively capture con-
versation history and thus have trouble handling questions involving coreference
or ellipsis. We propose a novel graph neural network (GNN) based model, namely
GraphFlow, which captures conversational flow in the dialog. Specifically, we
first propose a new approach to dynamically construct a question-aware context
graph from passage text at each turn. We then present a novel flow mechanism to
model the temporal dependencies in the sequence of context graphs. The proposed
GraphFlow model shows superior performance compared to existing state-of-
the-art methods. For instance, GraphFlow outperforms two recently proposed
models on the CoQA benchmark dataset: FlowQA by 2.3% and SDNet by 0.7%
on F1 score, respectively. In addition, visualization experiments show that our
proposed model can better mimic the human reasoning process for conversational
MC compared to existing models.
1	Introduction
Recent years have observed a surge of interest in conversational machine comprehension (MC).
Unlike the setting of MC that requires answering a single question given a passage (aka context),
the conversational MC task is to answer the current question in a conversation given a passage and
the previous questions and answers. The goal of this task is to mimic real-world situations where
humans seek information in a conversational manner.
Despite the success existing works have achieved on MC (e.g., SQuAD (Rajpurkar et al., 2016)),
conversational MC has proven significantly more challenging. We highlight two major challenges
here. First, in conversational MC the focus usually shifts as the conversation progresses (Reddy
et al., 2018; Choi et al., 2018). Second, many questions refer back to the conversation history via
either coreference or ellipsis. Therefore, without fully utilizing the conversation history, one can not
understand the current question correctly. In this work, we model the concept of conversation flow
as a sequence of latent states associated with these shifts of focus in a conversation.
To cope with the above challenges, many methods have been proposed to effectively utilize con-
versation history, including previous questions and/or previous answers. Most existing approaches,
however, simply prepend the conversation history to the current question (Reddy et al., 2018; Zhu
et al., 2018) or add previous answer locations to the passage (Choi et al., 2018; Yatskar, 2018), and
treat the task as a single-turn MC while ignoring the important information from the conversation
flow. (Huang et al., 2018) assumed that the hidden representations generated during the previous
reasoning processes potentially capture important information for answering the previous questions,
and thus provide additional clues for answering the current question. They proposed an Integration-
Flow (IF) mechanism to first process sequentially in passage, in parallel of question turns and then
process sequentially in question turns, in parallel of passage words. Their FlowQA model achieves
strong empirical results on two benchmarks (i.e., CoQA and QuAC) (Reddy et al., 2018; Choi et al.,
2018).
1
Under review as a conference paper at ICLR 2020
However, the IF mechanism is not quite natural since it does not mimic how humans perform reason-
ing. This is because when humans execute such task, they typically do not first perform reasoning
in parallel for each question, and then refine the reasoning results across different turns. This may
partially explain why this strategy is inefficient because the results of previous reasoning processes
are not incorporated into the current reasoning process. As a result, the reasoning performance at
each turn is only slightly improved by the hidden states of the previous reasoning process, even
though they use stacked IF layers to try to address this problem.
To address the aforementioned issues, we propose GraphFlow, a Graph Neural Network (GNN)
based model for conversational MC. As shown in Fig. 1, GraphFlow consists of three components,
Encoding Layer, Reasoning Layer, and Prediction Layer. The Encoding Layer encodes conversa-
tion history and the context text that aligns question embeddings. The Reasoning Layer dynamically
constructs a question-aware context graph at each turn, and then applies GNNs to process the se-
quence of context graphs. In particular, the graph node embedding outputs of the reasoning process
at the previous turn are used as a starting state when reasoning at the current turn, which is closer to
how humans perform reasoning in a conversational setting, compared to existing approaches. The
Prediction Layer predicts the answers based on the matching score of the question embedding and
the learned graph node embeddings for context at each turn.
We highlight our contributions as follows:
•	We propose a novel graph neural network based model, namely GRAPHFLOW, for conver-
sational MC which captures conversational flow in the dialog.
•	We dynamically construct a context graph that consists of each passage word as a node,
which encodes not only the passage itself, but also the question as well as the conversation
history. Besides, to our best knowledge, we are the first to treat context as a graph of words
in the literature of MC.
•	We propose a novel flow mechanism to process a sequence of context graphs, which models
the temporal dependencies between consecutive context graphs.
•	On two public benchmarks (i.e., CoQA and QuAC), the proposed model shows superior
performance compared to existing state-of-the-art methods. For instance, GraphFlow
outperforms two recently proposed models FlowQA by 2.3% F1 and SDNet by 0.7% F1
on benchmark dataset CoQA, respectively.
•	Our interpretability analysis shows that our model can better mimic human reasoning pro-
cess on this task.
2	The GraphFlow Approach
Figure 1: Overall architecture of the proposed model. Best viewed in color.
2
Under review as a conference paper at ICLR 2020
2.1	Encoding Layer
Let us denote C as the context which consists of a sequence of words tc1, c2, ..., cmu and Qi as
the question at the i-th turn which consists of a sequence of words tq1piq , q2piq , ..., qnpiq u. We ap-
ply an effective encoding layer to encode the context and the question, which additionally exploits
the interactions (i.e., soft alignment) between them as well as conversation history (i.e., previous
question-answer pairs). The details of the encoder are given next.
Linguistic features For each context word, we encode linguistic features to a vector flingpcpjiqq
concatenating POS (part-of-speech), NER (named entity recognition) and exact matching (which
indicates whether the context word appears in Qi) embeddings.
Pretrained word embeddings We use 300-dim GloVe (Pennington et al., 2014) embeddings as
well as 1024-dim BERT (Devlin et al., 2018) embeddings to embed each word in the context and
the question. Compared to GloVe embeddings, BERT embeddings better utilize the contextual in-
formation and can learn different embeddings for the same word type within different context.
Aligned question embeddings Exact matching only matches words on the surface form, we further
apply an attention mechanism to learn the soft alignment between context words and question words.
Following (Lee et al., 2016) and recent work, for each context word cj at the i-th turn, we incorporate
an aligned question embedding faiignPcji) “ Xk ajikgQi where gQi is the GloVe embedding of
piq	piq	piq
question word qk and ajk is an attention score between context word cj and question word qk .
piq
Here we define the attention score aj,k as,
apji,kq 9 exppReLUpWgjC qT ReLUpWgkQi qq	(1)
where W P Rdχ300 is a trainable model parameter, d is the hidden state size, and g is the GloVe
embedding of context word cj. To simplify notation, we denote the above attention mechanism as
AlignpA, B, Cq, meaning that an attention matrix is computed between two sets of vectors A and
B, which is later used to get a linear combination of vector set C. Hence we can reformulate the
above alignment as falignpCpiqq “ AlignpgC , gQi , gQi q.
Conversation history Conversation history is crucial for understanding the current question in this
task. (Choi et al., 2018) found it useful to concatenate a feature vector fanspcpjiqq encoding previous N
answer locations to the context word embeddings. We adopt this strategy, and additionally prepend
previous N question-answer pairs to the current question. When prepending conversation history,
a common choice is to separate the current question from conversation history using certain special
token, which does not work well in practice as observed by (Choi et al., 2018). We find a more
effective strategy to do the separation which is to concatenate a turn marker embedding fturnpqkpiqq to
each word vector in the augmented question. The turn marker is intended to indicate which turn the
word belongs to (e.g., i indicates the previous i-th turn).
In summary, at the i-th turn in a conversation, each context word cjis encoded by a vector wpcijq
which is a concatenation of linguistic vector flingpcpjiqq, GloVe vector gjC, BERT vector BERTjC,
aligned vector falignpcpjiqq and answer vector fanspcpjiqq. And each question word qkpiq is encoded
by a vector wkQi which is a concatenation of GloVe vector gkQi , BERT vector BERTkQi and turn
marker vector fturnpqkpiqq. We denote WCpiq and WQi as a sequence of context word vectors wcpijq and
question word vectors wkQi , respectively at the i-th turn.
2.2	Reasoning Layer
When performing reasoning over context, unlike most previous methods that solely regard context
as a sequence of words, we opt to treat context as a ”graph” of words, and hence it is straightfor-
ward to apply GNNs to process the context graph. Our Reasoning Layer consists of the Question
Understanding module, Context Graph Learning module and Context Graph Reasoning module.
3
Under review as a conference paper at ICLR 2020
2.2.1	Question Understanding
For each question Qi, we apply a BiLSTM (Hochreiter & Schmidhuber, 1997) to the raw question
embeddings WQi to obtain contextualized embeddings Qi P Rdxn.
Qi “ qp1iq,...,qpniq “ BiLSTMpWQi q	(2)
Each question is then represented as a weighted sum of word vectors in the question via a simple
self attention mechanism.
qpiq “ ∑ akiqki, Where akiq 9 eχppwT qkiqq	⑶
k
Where w is a d-dim trainable Weight.
Finally, to capture the dependency among questions at different turns, We encode question history
sequentially in question turns With a LSTM to generate history-aWare question vectors.
p1,..., pT “LSTMpqrp1q,..., qrpTqq	(4)
The output hidden states of the LSTM netWork p1, ..., pT Will be used for predicting ansWers.
2.2.2	Context Graph Learning
The intrinsic context graph structure is unfortunately unknoWn in the MC task. Moreover, intuitively
speaking, the context graph structure might vary across different turns by considering the change of
questions and conversation history. Most existing applications of GNNs take as input ground-truth
graphs or manually constructed graphs Which have some limitations. First, the ground-truth graphs
are not alWays available. Second, the error in the manual construction process can be propagated
to the subsequent GNN-based models and cannot be reduced during the learning process. In this
Work, We choose to automatically construct graphs from raW context, Which Will be combined With
the remaining of the system to make the Whole learning system end-to-end trainable. The difference
betWeen manual graph construction and automatic graph construction is analogous to the difference
betWeen the feature engineering + Machine Learning paradigm and the Deep Learning end-to-end
paradigm.
We dynamically build a Weighted graph to model semantic relationships among context Words at
each turn in a conversation. We make the process of building such a context graph depend on not
only the semantic meanings of context Words, but also on the question being asked, as Well as the
conversation history, so as to help better ansWer the question.
piq
Specifically, We first apply an attention mechanism to the context representations WC (Which addi-
tionally incorporate both question information and conversation history) at the i-th turn to compute
an attention matrix ApCiq, serving as a Weighted adjacency matrix for the context graph, defined as,
ApCiq “ ReLUpUWCpiq qTReLUpUWpCiq q	(5)
where U is a d X dc trainable weight and dc is the embedding size of wCjq.
Considering that a fully connected context graph is not only computationally expensive but also
makes little sense for reasoning, a simple KNN-style graph sparsification operation is conducted to
extract a sparse and weighted graph from the fully connected one. To be concrete, given a learned
attention matrix ApCiq, we only keep the K nearest neighbors (including itself) as well as the associ-
ated attention scores (i.e., the remaining attentions scores are masked off) for each context node, and
apply a softmax function to these selected adjacency matrix elements to get a sparse and normalized
adjacency matrix Ar pCiq .
Ar pCiq “ softmaxptopkpApCiqqq
(6)
4
Under review as a conference paper at ICLR 2020
Note that the supervision signal is still able to back-propagate through the KNN-style graph sparsif-
icalation module since the K nearest attention scores are kept and used to compute the weights of
the final normalized adjacency matrix.
2.2.3	Context Graph Reasoning
Figure 2: Architecture of the proposed Recurrent Graph Neural Network (RGNN) for processing a
sequence of context graphs. Best viewed in color.
Given the context graphs constructed at each turn, we propose a novel Recurrent Graph Neural
Network (RGNN) to sequentially process a sequence of graphs, as shown in Fig. 2. Conceptually,
one can think that it is analogous to an RNN-style structure where the main difference is that each
element in a sequence is not a data point, but instead a graph. As we advance in a sequence of
graphs, we process each graph using a shared GNN cell and the output of the GNN cell will be used
when processing the next graph. Our RGNN module combines the advantages of RNNs which are
good at sequential learning (i.e., modeling sequential data), and GNNs which are good at relational
reasoning (i.e., modeling graph-structured data).
The computational details of RGNNs are as follows. At the i-th turn, before we apply a GNN to the
context graph Gi, We update node embeddings by fusing both the original node information。；一1
and the updated node information at the previous turn C[_1 Via a fusion function.
Ci “ GNN(Ci´1, ACqq CIj “ FUSe(Cij1, CiTj)	⑺
Where l is the RGNN layer index. Note that We can stack multiple RGNN layers to enhance the
performance if necessary. The fusion function is designed as a gated sum of tWo information sources,
Fuse(a, b)= Z * a ' (1 — z)* b Z = σ(Wz [a; b; a * b; a — b] + bz)	(8)
Where σ is a sigmoid function and z is a gating vector.
As a result, the graph node embedding outputs of the reasoning process at the previous turn are used
as a starting state when reasoning at the current turn. Note that we set C0j1 “ C0-1 as we will
not incorporate any historical information at the first turn. Notably, even though there is no direct
link between cjat the (i — 1)-th turn and ck at the i-th turn, the information flow can be propagated
between them as the GNN progresses, which means information can be exchanged among all the
objects spatially and temporally.
We use Gated Graph Neural Networks (GGNN) (Li et al., 2015) as our GNN module, but the frame-
work is agnostic to the particular choice of GNN module. In GGNN we can do multi-hop message
passing through the graph to capture long-range dependency where the same set of network param-
eters are shared at every hop of computation. At each hop of GGNN computation, for every node
in the graph, we apply an aggregation function which takes as input a set of neighboring node em-
beddings and outputs an aggregation vector. In our framework, we compute a weighted average for
piq
aggregation where the weights come from the normalized adjacency matrices AC . Then, a Gated
Recurrent Unit (GRU) (Cho et al., 2014) is used to update the node embeddings by incorporating
the aggregation information. We use the updated node embeddings at the last hop as the final node
embeddings.
5
Under review as a conference paper at ICLR 2020
l	l1
To simplify notation, We denote the above RGNN module as Cl “ RGNN(Cl 1, AC) which takes
as input a sequence of graph node embeddings el´1 as well as a sequence of the normalized adja-
cency matrices Ar C, and outputs a sequence of updated graph node embeddings Cl.
While a GNN is responsible for modeling the global interactions among context words, modeling
local interactions among consecutive context words is also important for the task. Therefore, before
feeding the context word representations to a GNN, we first apply a BiLSTM to the context words,
that is, Ci0 “ BiLSTM(WCpiq), and we then use the output Ci0 as the initial context node embedding.
Inspired by recent work (Wang et al., 2018) on modeling the context with different levels of granu-
larity, we choose to apply one RGNN layer on low level representations of the context and another
RGNN layer on high level representations of the context, as formulated in the following.
C1 “ RGNN(C0, Ar C)
HiC “ rCi1; gC; BERTC s
HiQ “ rQi;gQi;BERTQis
fa2lign(Cpiq) “ Align(HiC,HiQ,Qi)s
Cri1 “ BiLSTM(rCi1;fa2lign(Cpiq)s)
C2 “ RGNN(Cr1,ArC)
2.3	Prediction Layer
We predict answer spans by computing the start and end probabilities PiS,j and PiE,j of the j-th context
word for the i-th question. To predict the start index, the probability is calculated by,
PiS,j 9 exp(ci2,j T WS pi )	(10)
where WS is a d X d trainable weight. Next, Pi is passed to a GRU by incorporating context
summary and converted to pri .
Pri “ GRU(Pi,	PiS,jci2,j)
(11)
Then, the end probability is calculated by,
PiE,j 9 exp(ci2,jTWEPri)	(12)
where WE is a d X d trainable weight.
We apply an answer type classifier to handle unanswerable questions and questions whose answers
are not text spans in the context. The probability of the answer type (e.g., “unknown”, “yes” and
“no”) is calculated as follows,
C2 “ rfmean(C2); fmax(C2)S PiC “ σ(f,(pi)(C2)T)	(13)
where f is a dense layer which maps a d-dim vector to a (num_class X 2d)-dim vector. Further, σ is
a sigmoid function for binary classification and a softmax function for multi-class classification. We
use Cri2 to represent the whole context at the i-th turn which is a concatenation of average pooling
and max pooling outputs of Ci2 .
2.4	Training and Inference
The training objective is defined as the cross entropy loss of both text span prediction (if the question
requires it) and answer type prediction.
L “ 一£ IS (iog(pSSi) + IOg(PEy) + log PCi
i
(14)
where IiS indicates whether this question requires text span prediction, si and ei are the ground-truth
start and end positions of the answer span, and ti indicates the ground-truth answer type.
During inference, we first use PiC to predict whether the question requires text span prediction. If
yes, we predict the span to be si , ei with maximum PiS,s PiE,e subject to the length constraint on the
span.
6
Under review as a conference paper at ICLR 2020
3	Experiments
In this section, we conduct an extensive evaluation of our proposed model against state-of-the-art
conversational MC models. We use two popular benchmarks, described below. The implementation
of the model will be publicly available soon. Detailed description of model settings is provided
in Appendix A.
Table 1: Model and human performance (% inF1 score) on the CoQA test set.
	Child.	Liter.	Mid-High.	News	Wiki	Reddit	Science	Overall
PGNet	49.0	43.3	47.5	47.5	45.1	38.6	38.1	44.1
DrQA	46.7	53.9	54.1	57.8	59.4	45.0	51.0	52.6
DrQA+PGNet	64.2	63.7	67.1	68.3	71.4	57.8	63.1	65.1
BiDAF++	66.5	65.7	70.2	71.6	72.6	60.8	67.1	67.8
FlowQA	73.7	71.6	76.8	79.0	80.2	67.8	76.1	75.0
SDNet	75.4	73.9	77.1	80.3	83.1	69.8	76.8	76.6
GraphFlow	^ΓTA^^	75.6	77J5S	79.1	82.5	70.8	78.4	^TT3
Human	90.2^^	88.4	^^898	88.6	89.9	86.7	88.1	88.8
Table 2: Model and human performance (in %) on the QUAC test set.
	F1	HEQ-Q	HEQ-D
BiDAF++	60.1	54.8	4.0
FlowQA	64.1	59.6	5.8
GraphFlow	64.9	60.3	5.1
Human	80.8	100	100
3.1	Data and Metrics
Although CoQA is released as an abstractive CMC dataset, Yatskar (2018) shows that the extractive
approach is also effective for CoQA. Thus, we also use our extractive approach on CoQA. To handle
answer types in CoQA, we predict the probability distribution of the answer type (SPAN, YES, NO,
and UNANSWERABLE) and replace the predicted span with yes, no, or unknown tokens ex- cept
for the SPAN answer type. In QuAC, the unanswerable questions are handled as an answer span
(P contains a special token), and the type prediction for yes/no questions is not evaluated on the
leaderboard. Therefore, we skip the answer type prediction step.
CoQA CoQA contains 127k questions with answers, obtained from 8k conversations. In CoQA,
answers are in free-form and hence are not necessarily text spans from the context (i.e., 33.2% of
the questions have abstractive answers). Although this calls for a generation approach, following
previous work (Yatskar, 2018; Huang et al., 2018; Zhu et al., 2018), as detailed in Section 2.3,
we adopt an extractive approach with additional answer type classifiers to handle non-extractive
questions. The average length of questions is only 5.5 words (i.e., 70% questions have coreference
or ellipsis), which means conversation history is important for better understanding those questions.
The average number of turns per dialog is 15.2. Notably, in the testing set, there are two out-of-
domain datasets which are reserved for testing only.
QuAC QuAC contains 98k questions with answers, obtained from 13k conversations. Unlike CoQA,
all the answers in QuAC are text spans from the context. The average length of questions is 6.5 and
there are on average 7.2 questions per dialog. The average length of QuAC context is 401 which is
longer than that of CoQA which is 271. The average length of QuAC answers is 14.6 which is also
longer than that of CoQA which is 2.7.
The main evaluation metric is F1 score which is the harmonic mean of precision and recall at word
level between the predicted answer and ground truth. In addition, for QuAC the Human Equivalence
Score (HEQ) is used to judge whether a system performs as well as an average human. HEQ-Q and
HEQ-D are model accuracies at question level and dialog level, respectively. Please refer to (Reddy
et al., 2018; Choi et al., 2018) for details of these metrics.
7
Under review as a conference paper at ICLR 2020
3.2	Model Comparison
Baseline methods We compare our approach with the following baseline methods: PGNet (See
et al., 2017), DrQA (Chen et al., 2017), DrQA+PGNet (Reddy et al., 2018), BiDAF++ (Yatskar,
2018), FLOWQA (Huang et al., 2018) and SDNet (Zhu et al., 2018). We will discuss the details of
these methods in Section 4.1.
Results As shown in Section 3 and Section 3, our model consistently outperforms these state-of-the-
art baselines in terms of F1 score. In particular, GraphFlow yields improvement over all existing
models on both datasets by at least +0.7% F1 on CoQA and +0.8% F1 on QuAC, respectively.
Compared with FlowQA which is also based on the flow idea, our model improves F1 by 2.3% on
CoQA and 0.8% on QuAC, which demonstrates the superiority of our RGNN mechanism over the
Integration-Flow mechanism. Compared with SDNet which relies on sophisticated inter-attention
and self-attention mechanisms, our model improves F1 by 0.7% on CoQA1.
3.3	Ablation Study
Table 3: Ablation study: model performance (in %) on the CoQA dev. set.
	F1		
GraphFlow (2-His)	78.3
-PreQues	78.2
-PreAns	77.7
- PreAnsLoc	76.6
- BERT	76.0
- RecurrentConn	69.9
- RGNN	68.8
GraphFlow (1-His)	78.2
GraphFlow (0-His)	76.7
Q1: Who went to the farm? -> Q2: Why?
^HWem to the farm to buy some beef for his brother ls birthday . When he arrived there he saw that all six of the
cows were sad and had brown ipots . The cows were all eating their breakfast in a big grassy meadow . He thought
that the spots looked very strange so he went closer to the cows to get a better look . When he got closer he also
saw that there were five white chickens sitting on the fence . The fence was painted blue and had some dirty black
spots on it. Billy wondered where the dirty spots had come . Soon he got close to the chickens and they got scared .
All five chickens flew away and went to eat some food . After BngOt a good look at the cows he went to the farmer
缸buy some beef. The farmer gave him four pounds of beef for ten dollars . Billy thought that it was a good deal so
he went home and cooked his brother dinner. His brother was very happy with the dinner. Billy's mom was also
very happy .
Q2: Why? -> Q3: For what?
Billy went to the farm	er's birthday . When he arrived there he saw that all six of the
cows were sad and had brown spots . The cows were all eating their breakfast in a big grassy meadow . He thought
that the spots looked very strange so he went closer to the cows to get a better look . When he got closer he also
saw that there were five white chickens sitting on the fence . The fence was painted blue and had some dirty black
spots on it. Billy wondered where the dirty spots had come . Soon he got close to the chickens and they got scared .
All five chickens flew away and went to eat some food . After Billy got a good look at the cows he went to the farmer
to buy some beef. The farmer gave him four pounds of beef	Billy thought that it was a good deal so
he went home and cooked his brother Mnner. His brother was very happy with the dinner. Billy's mom was also
very happy .
Figure 3: The highlighted part of the context indicates the QA model’s focus shifts between consec-
utive turns. 1
1They did not report the results on QuAC.
8
Under review as a conference paper at ICLR 2020
We conduct an extensive ablation study to further investigate the performance impact of different
components in our model. Here We briefly describe ablated systems: - RecurrentConn removes
temporal connections between consecutive context graphs, - RGNN removes the RGNN module,
- PreQues does not prepend previous questions to the current turn, - PreAns does not prepend
previous answers to the current turn, - PreAnsLoc does not mark previous answer locations in the
context, and - BERT removes pretrained BERT embeddings. We also show the model performance
with no conversation history GraphFlow (0-His) or one previous turn of the conversation history
GraphFlow (1-His).
Section 3.3 shows the contributions of the above components on the CoQA development set. We
find that the pretrained BERT embedding has the most impact on the performance, which again
demonstrates the power of large-scale pretrained language models. Our proposed RGNN module
also contributes significantly to the model performance (i.e., improves F1 score by 7.2%). In addi-
tion, within the RGNN module, both the GNN part (i.e., 1.1% F1) and the temporal connection part
(i.e., 6.1% F1) contribute to the results. We also notice that explicitly adding conversation history to
the current turn helps the model performance by comparing GraphFlow (2-His), GraphFlow
(1-His) and GraphFlow (0-His). We can see that the previous answer information is more crucial
than the previous question information. And among many ways to use the previous answer infor-
mation, directly marking previous answer locations seems to be the most effective. We conjecture
this is partially because the turn transitions in a conversation are usually smooth and marking the
previous answer locations helps the model better identify relevant context chunks for the current
question.
3.4 Interpretability Analysis
Here we visualize the memory bank (i.e., an m by d matrix) which stores the hidden representations
(and thus reasoning output) of the context throughout a conversation. While directly visualizing the
hidden representations is difficult, thanks to the flow-based mechanism introduced into our model,
we instead visualize the changes of hidden representations of context words between consecutive
turns. We expect that the most changing parts of the context should be those which are relevant to
the questions being asked and therefore should probably be able to indicate shifts of the focus in a
conversation.
Following (Huang et al., 2018), we visualize this by computing the cosine similarity of the hidden
representations of the same context words at consecutive turns, and then highlight the words that
have small cosine similarity scores (i.e., change more significantly)2. Fig. 3 highlights the most
changing context words between consecutive turns in a conversation from the CoQA dev. set. As we
can see, the hidden representations of context words which are relevant to the consecutive questions
are changing most and thus highlighted most. We suspect this is in part because when the focus
shifts, the model finds out the context chunks relevant to the previous turn become less important
but those relevant to the current turn become more important. Therefore, the memory updates in
these regions are the most active. Obviously, this makes the model easier to answer follow-up
questions. As we observe in our visualization experiments, in conversations extensively involving
coreference or ellipsis, our model can still perform reasonably well. We refer the interested readers
to the supplementary material for complete visualization examples.
4	Related Work
4.1	Conversational MC
Many methods have been proposed to utilize conversation history in the literature of conversational
MC. (Reddy et al., 2018; Zhu et al., 2018) concatenate previous questions and answers to the cur-
rent question. (Choi et al., 2018) concatenate a feature vector encoding the turn number to the
question word embedding and a feature vector encoding previous N answer locations to the context
embeddings. However, as claimed by (Huang et al., 2018), these methods ignore previous reason-
ing processes performed by the model when reasoning at the current turn. Instead, they propose
2For better visualization, we apply an attention threshold of 0.3 to highlight only the dramatically changing
context words.
9
Under review as a conference paper at ICLR 2020
the idea of Integration-Flow to allow rich information in the reasoning process to flow through a
conversation.
Another challenge of this task is how to handle abstractive answers. (Reddy et al., 2018) propose
a hybrid method DrQA+PGNet, which augments a traditional extractive RC model with a text gen-
erator. (Yatskar, 2018) propose to first make a Yes/No decision, and output an answer span only if
Yes/No was not selected.
4.2	Graph Neural Networks
Over the past few years, graph neural networks (GNNs) (Kipf & Welling, 2016; Gilmer et al., 2017;
Hamilton et al., 2017; Li et al., 2015; Xu et al., 2018a) have drawn increasing attention since they
extend traditional Deep Learning approaches to non-euclidean data such as graph-structured data.
Recently, GNNs have been applied to various question answering tasks including visual question
answering (VQA) (Teney et al., 2017; Norcliffe-Brown et al., 2018), knowledge base question an-
swering (KBQA) (Sun et al., 2018) and MC (De Cao et al., 2018; Song et al., 2018; Xu et al., 2018c),
and have shown advantages over traditional approaches. For tasks where the graph structure is un-
known, (De Cao et al., 2018; Song et al., 2018; Xu et al., 2018b) construct a static graph where entity
mentions in a passage are nodes of this graph and edge information is determined by coreferences
of entity mentions. (Norcliffe-Brown et al., 2018) dynamically construct a graph which contains all
the visual objects appearing in an image. In parallel to our work, (Liu et al., 2018) also dynamically
construct a graph of all words from free text.
5	Conclusion
We proposed a novel GNNs-based model, namely GraphFlow, for conversational MC which
carries over the reasoning output throughout a conversation. On two recently released conversa-
tional MC benchmarks, our proposed model achieves superior results over previous approaches.
Interpretability analysis shows that the model can better mimic the human reasoning process for
conversational MC compared to existing models.
In the future, we would like to investigate more effective ways of automatically learning graph
structures from free text and modeling temporal connections between sequential graphs.
References
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-
domain questions. arXiv preprint arXiv:1704.00051, 2017.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and YoshUa Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. In EMNLP, pp. 1724-1734, 2014.
Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke
Zettlemoyer. Quac: Question answering in context. arXiv preprint arXiv:1808.07036, 2018.
Nicola De Cao, Wilker Aziz, and Ivan Titov. Question answering by reasoning across documents
with graph convolutional networks. arXiv preprint arXiv:1808.09920, 2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 1263-1272. JMLR. org, 2017.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in Neural Information Processing Systems, pp. 1024-1034, 2017.
SePP Hochreiter and JUrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
10
Under review as a conference paper at ICLR 2020
Hsin-Yuan Huang, Eunsol Choi, and Wen-tau Yih. Flowqa: Grasping flow in history for conversa-
tional machine comprehension. arXiv preprint arXiv:1810.06683, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
Kenton Lee, Shimi Salant, Tom Kwiatkowski, Ankur Parikh, Dipanjan Das, and Jonathan Be-
rant. Learning recurrent span representations for extractive question answering. arXiv preprint
arXiv:1611.01436, 2016.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural
networks. arXiv preprint arXiv:1511.05493, 2015.
Pengfei Liu, Shuaichen Chang, Xuanjing Huang, Jian Tang, and Jackie Chi Kit Cheung. Contextu-
alized non-local neural networks for sequence learning. arXiv preprint arXiv:1811.08600, 2018.
Will Norcliffe-Brown, Stathis Vafeias, and Sarah Parisot. Learning conditioned graph structures for
interpretable visual question answering. In Advances in Neural Information Processing Systems,
pp. 8344-8353,2018.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532-1543, 2014.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.
Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question answering
challenge. arXiv preprint arXiv:1808.07042, 2018.
Abigail See, Peter J Liu, and Christopher D Manning. Get to the point: Summarization with pointer-
generator networks. arXiv preprint arXiv:1704.04368, 2017.
Linfeng Song, Zhiguo Wang, Mo Yu, Yue Zhang, Radu Florian, and Daniel Gildea. Exploring
graph-structured passage representation for multi-hop reading comprehension with graph neural
networks. arXiv preprint arXiv:1809.02040, 2018.
Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and
William W Cohen. Open domain question answering using early fusion of knowledge bases
and text. arXiv preprint arXiv:1809.00782, 2018.
Damien Teney, Lingqiao Liu, and Anton van den Hengel. Graph-structured representations for
visual question answering. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 1-9, 2017.
Wei Wang, Ming Yan, and Chen Wu. Multi-granularity hierarchical attention fusion networks for
reading comprehension and question answering. arXiv preprint arXiv:1811.11934, 2018.
Kun Xu, Lingfei Wu, Zhiguo Wang, and Vadim Sheinin. Graph2seq: Graph to sequence learning
with attention-based neural networks. arXiv preprint arXiv:1804.00823, 2018a.
Kun Xu, Lingfei Wu, Zhiguo Wang, Mo Yu, Liwei Chen, and Vadim Sheinin. Exploiting
rich syntactic information for semantic parsing with graph-to-sequence model. arXiv preprint
arXiv:1808.07624, 2018b.
Kun Xu, Lingfei Wu, Zhiguo Wang, Mo Yu, Liwei Chen, and Vadim Sheinin. Sql-to-text generation
with graph-to-sequence model. arXiv preprint arXiv:1809.05255, 2018c.
Mark Yatskar. A qualitative comparison of coqa, squad 2.0 and quac. arXiv preprint
arXiv:1809.10735, 2018.
Chenguang Zhu, Michael Zeng, and Xuedong Huang. Sdnet: Contextualized attention-based deep
network for conversational question answering. arXiv preprint arXiv:1812.03593, 2018.
11
Under review as a conference paper at ICLR 2020
A Model Settings
We construct the vocabulary of words from the training set but filter out those infrequent words (i.e.,
word count less than 5) to reduce the vocabulary size. The embedding sizes of POS, NER, exact
matching and turn marker embeddings are set to 12, 8, 3 and 3, respectively. We fix the pretrained
GloVe vectors since which we find gives better results than the fine-tuning strategy. Following (Zhu
et al., 2018), we pre-compute BERT embeddings for each word using a weighted sum of BERT
layer outputs. The size of all hidden layers is set to 300. When constructing context graphs, the
neighborhood size is set to 10. The number of GNN hops is set to 5 for CoQA and 3 for QuAC.
During training, we apply dropout after the embedding layers (0.3 for GloVe and 0.4 for BERT). A
dropout rate of 0.3 is also applied after the output of all RNN layers. We use Adamax (Kingma & Ba,
2014) as the optimizer and the learning rate is set to 0.001. We reduce the learning rate by a factor
of 0.5 if the validation F1 score has stopped improving every one epoch. We stop the training when
no improvement is seen for 10 consecutive epochs. We clip the gradient at length 10. We batch over
dialogs and the batch size is set to 1. When augmenting the current turn with conversation history,
we only consider the previous two turns. When doing text span prediction, the span is constrained
to have a maximum length of 12 for CoQA and 35 for QuAC. All these hyper-parameters are tuned
on the development set.
12