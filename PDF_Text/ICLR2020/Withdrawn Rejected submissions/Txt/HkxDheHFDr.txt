Under review as a conference paper at ICLR 2020
LAVAE: Disentangling Location
and Appearance
Anonymous authors
Paper under double-blind review
Ab stract
We propose a probabilistic generative model for unsupervised learning of struc-
tured, interpretable, object-based representations of visual scenes. We use amor-
tized variational inference to train the generative model end-to-end. The learned
representations of object location and appearance are fully disentangled, and ob-
jects are represented independently of each other in the latent space. Unlike previ-
ous approaches that disentangle location and appearance, ours generalizes seam-
lessly to scenes with many more objects than encountered in the training regime.
We evaluate the proposed model on multi-MNIST and multi-dSprites data sets.
1	Introduction
Many hallmarks of human intelligence rely on the capability to perceive the world as a layout
of distinct physical objects that endure through time—a skill that infants acquire in early child-
hood (Spelke, 1990; 2013; Spelke and Kinzler, 2007). Learning compositional, object-based repre-
sentations of visual scenes, however, is still regarded as an open challenge for artificial systems (Ben-
gio et al., 2013; Garnelo and Shanahan, 2019).
Recently, there has been a growing interest in unsupervised learning of disentangled representa-
tions (Locatello et al., 2018), which should separate the distinct, informative factors of variations in
the data, and contain all the information on the data in a compact, interpretable structure (Bengio
et al., 2013). This notion is highly relevant in the context of visual scene representation learning,
where distinct objects should arguably be represented in a disentangled fashion. However, despite
recent breakthroughs (Chen et al., 2016; Higgins et al., 2017; Kim and Mnih, 2018), multi-object
scenarios are rarely considered (Eslami et al., 2016; Van Steenkiste et al., 2018; Burgess et al., 2019).
We propose the Location-Appearance Variational AutoEncoder (LAVAE), a probabilistic generative
model that, without supervision, learns structured, compositional, object-based representations of
visual scenes. We explicitly model an object’s location and appearance with distinct latent vari-
ables, unlike in most previous works, thus providing a highly beneficial inductive bias. Following
the framework of variational autoencoders (VAEs) (Kingma and Welling, 2013; Rezende et al.,
2014), we parameterize the approximate variational posterior of the latent variables with inference
networks that are trained end-to-end with the generative model. Our model learns to correctly count
objects and compute a compositional, object-wise, interpretable representation of the scene. Objects
are represented independently of each other, and each object’s location and appearance are disen-
tangled. Unlike previous approaches that disentangle location and appearance, LAVAE generalizes
seamlessly to scenes with many more objects than in the training regime. We demonstrate these
capabilities on multi-MNIST and multi-dSprites data sets similar to those by Eslami et al. (2016)
and Greff et al. (2019).
2	Method
Generative model. We propose a latent variable model for images in which the latent space is
factored into location and appearance of a variable number of objects. For each image x with D
pixels, the number of objects is modeled by a latent variable n, their locations by n latent variables
{zl(oi)c}in=1, and their appearance by {z(aip)p}in=1. We assume the number of objects in every image to
1
Under review as a conference paper at ICLR 2020
be bounded by M. The joint distribution of the observed and latent variables for each data point is:
p(x, zloc, zapp, n) = p(x | zloc, zapp)p(zloc | n)p(zapp | n)p(n)
= P(X | zloc, zapp) (Y p(z(θC | {z(0)}j<i)p(Zai)P)PS),	⑴
where we use the shorthand zaPP = {z(aiP)P}in=1 and similarly for zloc .
The generative process can be described as follows. First, the number of objects n is sampled from
a categorical distribution
P(n) = Cat(M + 1, a),	n ∈ {0, . . . , M} ,	(2)
where a is a learned probability vector of size M + 1. The n location variables are sequentially
sampled without replacement from a categorical distribution with D classes:
p(z(0C l{z(0) }j<i) =Cat(D, b(i)),	b(i) = D-1i+1 (1D - Xj=1 Zj)),	⑶
where i = 1, . . . , n, and 1D is a vector of ones of length D. To each zl(oi)c, which is a one-hot repre-
Sentation of an object's location, corresponds a continuous appearance vector Zap)P 〜N(0l, Il) of
size L that describes the object.
The likelihood function P(x | zloc , zaPP) is parameterized in a compositional manner. For each im-
(i)
age, the visual representation, or sprite, of the ith object is generated from zaPP by a shared function
fsPrite . Each sprite is then convolved with a 2-dimensional Kronecker delta that is the one-hot rep-
resentation of the object’s location. Finally, the resulting n tensors are added together to give the
pixel-wise parameters λ = (λ1, . . . , λD) of the distribution:
n
λ = X fsprite (ZaiPP) * Zloc ,	(4)
i=1
where * denotes 2-dimensional discrete convolution.
Inference model. The approximate posterior for a data point x has the following form:
q(zaPP, zloc, n |x) = q(zaPP | zloc, n, x)q(zloc | n, x)q(n | x)
= Yq(z(aiP)P | zl(oi)c, x)qzl(oi)c {zl(ojc)}j<i,x	q(n|x) .	(5)
Since each data point is assumed independent, the lower bound (ELBO) to the marginal log likeli-
hood is the sum of the ELBO for each data point, which is:
logP(X) ≥ Eq [logP(X | Zloc, Zapp)] + Eq log P(Zloc, zapp,n)	,	(6)
q(zloc, zaPP, n | x)
where the first term is the likelihood and the second is the negative Kullback-Leibler (KL) divergence
between q(Zloc, Zapp, n | X) and P(Zloc, Zapp, n). The different terms of the KL divergence can be
derived as in Appendix A and estimated by Monte Carlo sampling.
Two inference networks compute appearance and location feature maps, happ and hloc, both having
size D like the input. The inference model for the number of objects n is a categorical distribution
parameterized by a function of the location features fcount (hloc). Object locations follow n cate-
(i)
gorical distributions without replacement parameterized by logits hloc . The vector at location Zloc
in the feature map happ represents the appearance parameters for object i, i.e. the mean and log
variance of q(Z(aip)p | Zl(oi)c, X). The overall inference process can be summarized as follows:
q(n |X) = Cat(n; M + 1, fcount(hloc))	(7)
qZl(oi)c {Zl(ojc)}j<i, X = Cat(Zl(oi)c; D, b(i))	i=1,...,n	(8)
2
Under review as a conference paper at ICLR 2020
〃app, σ2Pp)=happ[z(oc]	i=ι,...,n	⑼
q(Zai)P|z(oc,X)=N(ZaiPp； μ(aPp,mg^ap(P)))	i=1,...,n,	(IO)
where by v[ek] we denote the kth element of v (with ek a standard basis vector), and the probability
vector for location sampling at each step is computed iteratively:
b0i)= softmaχ(hloC) ∙ (ID - Xj=I Z(O)),	b(i) = bOi) ∙ llbθi)11-1 . (II)
The expectations in the variational bound are handled as follows: For n we use discrete categorical
sampling. This gives a biased gradient estimator, but in practice this has not affected inference on n.
We use the Gumbel-softmax relaxation (Jang et al., 2016; Maddison et al., 2016) for Zloc and the
Gaussian reparameterization trick for Zapp .
3 Results
In all experiments we use the same architecture for LAVAE. See Appendix B for all implementation
details. Briefly, the sprite decoder in the generative model p(X | Zloc, Zapp) consists of a fully con-
nected layer and a convolutional layer, followed by 3 residual convolutional blocks. The appearance
and location inference networks are fully convolutional, thus the feature maps happ and hloc have
the same spatial size as X.
As a baseline to compare the proposed model against we used a VAE implemented as a fully convolu-
tional network: the absence of fully connected layers makes it easier to preserve spatial information,
thereby allowing the model to achieve a higher likelihood and generalize more naturally. Moreover,
this choice of baseline is closer in spirit to our model than a VAE with fully connected layers.
We evaluate LAVAE on multi-MNIST and multi-dSprites data sets consisting of 200k images with 0
to 3 objects in each image. 190k images are used for training, whereas the remaining 10k are left for
evaluation. We generated an additional test set of 10k images with 7 objects each. Examples with
more than 3 objects are never used for training.
3.1	Multi-MNIST
Each image in the multi-MNIST data set consists of a number of statically binarized MNIST digits
scattered at random (avoiding overlaps) onto a black canvas of size 64 × 64. The digits are first
rescaled from their original size (28 × 28) to 15 × 15 by bilinear interpolation, and finally binarized
by rounding. When generating images, digits are picked from a pool of either 1k or 10k MNIST
digits. We call the two resulting data sets multi-MNIST-1k and multi-MNIST-10k, depending on the
number of MNIST digits used for generation. We independently model each pixel as a Bernoulli
random variable parameterized by the decoder:
DD
p(X | Zloc,	Zapp)	=	p(xd	|	Zloc,	Zapp)	= Bernoulli(xd；	λd)	,	(12)
d=1	d=1
where the parameter vector λ defined in Section 2 is the pixel-wise mean of the Bernoulli distribu-
tion.
Figure 1 qualitatively shows the performance of LAVAE on multi-MNIST-10k test images. The
inferred location of all objects in an image are summarized by a black canvas where white pixels
indicate object presence. For each of those locations, the model infers a corresponding appearance
latent variable. Each digit on the right is generated by fsprite from one of these appearance variables.
Samples from the prior are shown in Figure 2: note that the number of generated objects is consistent
with the training set, since the prior p(n) is learned. Quantitative evaluation results are shown in
Table 1 and compared with a VAE baseline. The inferred object count was correct on almost all
images of all test sets—even with a held-out number of objects—and across all tested random seeds.
A fundamental characteristic of disentangled representations is that a change in a single representa-
tional factor should correspond to a change in a single underlying factor of variation (Bengio et al.,
2013; Locatello et al., 2018). By demonstrating that the appearance and location of single objects
3
Under review as a conference paper at ICLR 2020
Figure 1: Inference and reconstruction on test images. For each image, from left to right: in-
put, reconstruction, summary of inferred locations, sprites generated from each appearance latent
variable.
from its prior p(z).
can be independently manipulated in the latent space, the qualitative disentanglement experiments
in Figure 3 prove that objects are disentangled from one another, and so are the appearance and
location of each object.
3.2	Multi-dSprites
The multi-dSprites data set is generated similarly to the multi-MNIST ones, by scattering a number
of sprites on a black canvas of size 64 × 64. Here, the sprites are simple shapes in different col-
ors, sizes, and orientations, as in the dSprites data set (Higgins et al., 2017; Matthey et al., 2017).
The shape of each sprite is randomly chosen among square, ellipse, and triangle. The maximum
sprite size is 19 × 19, and each sprite’s scale is randomly chosen among 6 linearly spaced values in
[0.6, 1.0]. The orientation angle of each sprite is uniformly chosen among 40 linearly spaced values
in [0, 2π). The color is uniformly chosen among the 7 colors such that the RGB values are saturated
(either 0 or 255), and at least one of them is not 0. This means that each color component of each
pixel is a binary random variable and can be modelled independently as in the multi-MNIST case
(leading to 3D terms in the likelihood, instead of D).
Figure 4 shows images generated by sampling from the prior in LAVAE and in the VAE baseline,
where we can appreciate how our model accurately captures the number and diversity of objects
in the data set. Figure 5 shows instead an example of inference on test images, as explained in
Section 3.1. Finally, as we did for multi-MNIST, we performed disentanglement experiments where,
starting from the latent representation of a test image, we manipulate the latent variables. In Figure 6
we change the order of the location variables to swap objects, whereas Figure 7 shows the effect of
separately altering the location and appearance latent variables of a single object.
4
Under review as a conference paper at ICLR 2020
Table 1: Quantitative results on multi-MNIST and multi-dSprites test sets. The log likelihood lower
bound is estimated with 100 importance samples. Note that the MNIST-10k dataset is a more com-
plex task than MNIST-1k because the model has to capture a larger variation in appearance. The
object count accuracy is measured as the percentage of images for which the inferred number of
objects matches the ground truth (which is not available during training).
	multi-MNIST-1k	multi-MNIST-10k	multi-dSprites
	- log p(x) ≤	n acc.	- log p(x) ≤	n acc.	- log p(x) ≤	n acc.
LAVAE	42.2 ± 1.1	≥ 99.9%	60.4 ± 2.8	≥ 99.7%	43.9 ± 1.0	≥ 99.9%
baseline	51.6 ± 0.6	—	64.6 ± 0.8	—	45.8 ± 0.7	—
Figure 3: Disentanglement experiments on test images. Objects are represented independently of
each other, and their location and appearance are disentangled by design. Left: Latent traversal
on one of the 7 location variables. Top right: Reordering the sequence {zl(oi)c }i or equivalently of
{z(aip)p}i leads to objects being swapped (top row: original reconstruction; bottom row: swapped
objects). Bottom right: In each row, latent traversal on one of the appearance variables along one
dimension.
3.3	Generalizing to more objects
As mentioned above, LAVAE correctly infers the number of objects in images from the 7-object
versions of our data sets, despite the fact that it was only trained on images with up to 3 objects.
Furthermore, by representing each object independently and disentangling location and appearance
of each object, it accurately decomposes 7-object scenes and allows intervention as easily as in
images with fewer objects. Figure 8 demonstrates this on the 7-object version of the multi-MNIST-
10k data set. Finally, in Figure 9 we show images generated by LAVAE after modifying the prior
p(n) to be uniform in {4, 5}.
4	Related work
Our work builds on recent advances in probabilistic generative modelling, in particular variational
autoencoders (VAEs) (Kingma and Welling, 2013; Rezende et al., 2014). One of the methods clos-
est to our work in spirit is Attend Infer Repeat (AIR) (Eslami et al., 2016), which performs explicit
object-wise inference through a recurrent network that iteratively attends to one object at a time.
A limitation of this approach however is that it has not been shown to generalize well to a larger
number of objects. Closely related to our work is also the multi-entity VAE (Nash et al., 2017),
5
Under review as a conference paper at ICLR 2020
Figure 5: Example of inference and reconstruction on multi-dSprites test images. From left to
right: input, reconstruction, summary of inferred locations, sprites generated from the inferred ap-
pearance latent variables.
in which multiple objects are independently modelled by different latent variables. The inference
process does not include an explicit attention mechanism, and uses instead a spatial KL map as a
proxy for object presence. Each object’s latent is decoded into a full image, and these are aggregated
by an element-wise operation, thus the representation of each object’s location and appearance are
entangled. In the same spirit, the recently proposed generative models MONet (Burgess et al., 2019)
Figure 6: Object swap. Changing the order of location (or equivalently appearance) latent variables
leads to objects being swapped. Top row: original reconstruction of test image; bottom row: objects
are swapped by manipulating the latent variables.
6
Under review as a conference paper at ICLR 2020
Figure 7: Left: Location traversal. Latent traversal on location variables in a test image with 3
(i)
objects. Right: Appearance traversal. Latent traversal on appearance variables: changing zapp
for some i along one latent dimension corresponds to changing appearance attributes of one specific
object. The appearance latent space is only partially disentangled. Here we show examples where
a change in one latent dimension leads to a change of a single factor of variation (rows 2, 4, 5, 6).
However, in row 1 there is a change both in color and shape, and in row 3 both in color and scale.
Figure 8: Disentanglement with more objects. Latent traversal on location (top) and appearance
(bottom) variables, on multi-MNIST-10k test images containing 7 objects. LAVAE can still correctly
infer the scene’s structure and reconstruct it, allowing intervention on location or appearance of
single objects.
and IODINE (Greff et al., 2019) learn without supervision to segment the scene into independent
and interpretable object-based parts. Although these are more flexible than AIR and can model more
complex scenes, the representations they learn of object location and appearance are not disentan-
gled. All methods cited here are likelihood based so they can and should be compared in terms of
test log likelihood. We leave this for future work.
Other unsupervised approaches to visual scene decomposition include Neural Expectation Maxi-
mization (Greff et al., 2017; Van Steenkiste et al., 2018), which amortizes the classic EM for a spa-
tial mixture model, and Generative Query Networks (Eslami et al., 2018), that learn representations
of rich 3D scenes but do not factor them into objects and need point-of-view information during
training. Methods following the vision-as-inverse-graphics paradigm (Poggio et al., 1985; Yuille
and Kersten, 2006) learn structured, object-centered representations by making strong assumptions
on the latent codes orby exploiting the true generative model (Kulkarni et al., 2015; Wu et al., 2017;
Tian et al., 2019). Non-probabilistic approaches to scene understanding include adversarially trained
7
Under review as a conference paper at ICLR 2020
Figure 9: Generation with fixed number of objects. Images generated by LAVAE from a modified
prior in which n takes value 5 or 6 with probability 1/2.
generative models (Pathak et al., 2016) and self-supervised methods (Doersch et al., 2015; Vondrick
et al., 2018). These, however, do not explicitly tackle representation learning, and often have to rely
on heuristics such as region masks. Finally, examples of supervised approaches are semantic and
instance segmentation (Ronneberger et al., 2015; He et al., 2017; JegoU et al., 2017; LiU et al., 2018),
where acquiring labels for training is typically expensive, and the focus is not on learning structured
representations.
5	Conclusion
We presented LAVAE, a probabilistic generative model for UnsUpervised learning of strUctUred,
compositional, object-based representations of visUal scenes. We follow the amortized stochastic
variational inference framework, and approximate the latent posteriors by inference networks that
are trained end-to-end with the generative model. On mUlti-MNIST and mUlti-dSprites data sets,
LAVAE learns withoUt sUpervision to correctly coUnt and locate all objects in a scene. Thanks
to the strUctUre of the generative model, objects are represented independently of each other, and
the location and appearance of each object are completely disentangled. We demonstrate this in
qUalitative experiments, where we manipUlate location or appearance of single objects independently
in the latent space. OUr model natUrally generalizes to visUal scenes with many more objects than
encoUntered dUring training.
These properties make LAVAE robUst to scene complexity, opening Up possibilities for leveraging
the learned representations for downstream tasks and reinforcement learning agents. However, in
order to smoothly transfer to scenes with semantically different components, the appearance latent
space shoUld be disentangled. Since in this work we focUsed on robUst model based disentanglement
of location and appearance, more work shoUld be done to fUlly assess and improve disentanglement
properties in the appearance model. Another natUral extension to this work is to apply it to complex
natUral images and 3d scenes.
References
Y. Bengio, A. CoUrville, and P. Vincent. Representation learning: A review and new perspectives.
IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828, 2013.
C. P. BUrgess, L. Matthey, N. Watters, R. Kabra, I. Higgins, M. Botvinick, and A. Lerchner. Monet:
UnsUpervised scene decomposition and representation. arXiv preprint arXiv:1901.11390, 2019.
8
Under review as a conference paper at ICLR 2020
X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. Infogan: Interpretable
representation learning by information maximizing generative adversarial nets. In Advances in
neural information processing Systems, pages 2172-2180, 2016.
C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual representation learning by context
prediction. In Proceedings of the IEEE International Conference on Computer Vision, pages
1422-1430, 2015.
S. A. Eslami, N. Heess, T. Weber, Y. Tassa, D. Szepesvari, G. E. Hinton, et al. Attend, infer, repeat:
Fast scene understanding with generative models. In Advances in Neural Information Processing
Systems, pages 3225-3233, 2016.
S. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Morcos, M. Garnelo, A. Ruderman, A. A.
Rusu, I. Danihelka, K. Gregor, et al. Neural scene representation and rendering. Science, 360
(6394):1204-1210, 2018.
M. Garnelo and M. Shanahan. Reconciling deep learning with symbolic artificial intelligence: rep-
resenting objects and relations. Current Opinion in Behavioral Sciences, 29:17-23, 2019.
K. Greff, S. van Steenkiste, and J. Schmidhuber. Neural expectation maximization. In Advances in
Neural Information Processing Systems, pages 6691-6701, 2017.
K. Greff, R. L. Kaufmann, R. Kabra, N. Watters, C. Burgess, D. Zoran, L. Matthey, M. Botvinick,
and A. Lerchner. Multi-object representation learning with iterative variational inference. arXiv
preprint arXiv:1903.00450, 2019.
K. He, G. Gkioxari, P. Dollar, and R. Girshick. Mask r-cnn. In Proceedings ofthe IEEE international
conference on computer vision, pages 2961-2969, 2017.
I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner.
beta-vae: Learning basic visual concepts with a constrained variational framework. ICLR, 2(5):6,
2017.
E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint
arXiv:1611.01144, 2016.
S.	Jegou, M. Drozdzal, D. Vazquez, A. Romero, and Y. Bengio. The one hundred layers tiramisu:
Fully convolutional densenets for semantic segmentation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition Workshops, pages 11-19, 2017.
H. Kim and A. Mnih. Disentangling by factorising. arXiv preprint arXiv:1802.05983, 2018.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,
2013.
T.	D. Kulkarni, W. F. Whitney, P. Kohli, and J. Tenenbaum. Deep convolutional inverse graphics
network. In Advances in neural information processing systems, pages 2539-2547, 2015.
S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia. Path aggregation network for instance segmentation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8759-
8768, 2018.
F. Locatello, S. Bauer, M. Lucic, S. Gelly, B. Scholkopf, and O. Bachem. Challenging com-
mon assumptions in the unsupervised learning of disentangled representations. arXiv preprint
arXiv:1811.12359, 2018.
C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete distribution: A continuous relaxation of
discrete random variables. arXiv preprint arXiv:1611.00712, 2016.
L. Matthey, I. Higgins, D. Hassabis, and A. Lerchner. dsprites: Disentanglement testing sprites
dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
C.	Nash, S. A. Eslami, C. Burgess, I. Higgins, D. Zoran, T. Weber, and P. Battaglia. The multi-entity
variational autoencoder. NIPS Workshops, 2017.
9
Under review as a conference paper at ICLR 2020
D.	Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros. Context encoders: Feature
learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 2536-2544, 2016.
T. Poggio, V. Torre, and C. Koch. Computational vision and regularization theory. Nature, 317
(6035):314-319, 1985.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate infer-
ence in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image
segmentation. In International Conference on Medical image computing and computer-assisted
intervention, pages 234-241. Springer, 2015.
E. S. Spelke. Principles of object perception. Cognitive science, 14(1):29-56, 1990.
E. S. Spelke. Where perceiving ends and thinking begins: The apprehension of objects in infancy.
In Perceptual development in infancy, pages 209-246. Psychology Press, 2013.
E.	S. Spelke and K. D. Kinzler. Core knowledge. Developmental science, 10(1):89-96, 2007.
Y. Tian, A. Luo, X. Sun, K. Ellis, W. T. Freeman, J. B. Tenenbaum, and J. Wu. Learning to infer
and execute 3d shape programs. arXiv preprint arXiv:1901.02875, 2019.
S. Van Steenkiste, M. Chang, K. Greff, and J. Schmidhuber. Relational neural expectation maximiza-
tion: Unsupervised discovery of objects and their interactions. arXiv preprint arXiv:1802.10353,
2018.
C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, and K. Murphy. Tracking emerges by col-
orizing videos. In Proceedings of the European Conference on Computer Vision (ECCV), pages
391-408, 2018.
J. Wu, J. B. Tenenbaum, and P. Kohli. Neural scene de-rendering. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 699-707, 2017.
A. Yuille and D. Kersten. Vision as bayesian inference: analysis by synthesis? Trends in Cognitive
Sciences, 10(7):301-308, 2006.
10
Under review as a conference paper at ICLR 2020
A KL divergence
The KL in Eq. equation 6 can be expanded as follows:
DKL (q(Zloc, zapp, n | X)Il P(Zloc, zapp, n)) = Eq
Eq(zloc,zapp,n
q(Zapp | Zloc, n, X)
1x) [log	P(ZaPP | n)
log
q(ZaPP | zloc, n, X)q(zloc | n, x)q(n | X)
p(Zapp | n)p(zloc | n)p(n)
+ Eq(n | x)
log
q(n | x)
P(n)
+ Eq(zloc,n | x)
log q(Zloc |n, X)
.	P(Zloc | n)
n
Eq(zloc | n,x)q(n | x)	X Dkl (q(Zapp	Z(oC,X)	∣	P(ZaiPP))	+	DKL	(q(n	|X)	I	P(n))
i=1
+ Eq(n | x)q(ziοc | n,x) X (log 9 (Z(oC । {Z(j)}j<i, X)- log P(Z(oC । {Zj) }j<i))
i=1
where all expectations can be estimated by Monte Carlo sampling.
B Implementation details
The input image X is fed into a residual network with 4 blocks having 2 convolutional layers each.
Every convolution is followed by a Leaky ReLU and batch normalization. A final convolutional
layer outputs the feature map happ with 2L channels. The output of an identical but independent
residual network is fed into a 3-layer convolutional network with one output channel that represents
the location logits hloc. The logits are multiplied by a constant factor γ = 4 ≈ 1/2 log D. See
below for more details about the rescaling of logits.
The number of objects is then inferred by a deterministic function fcount of the location logits. This
function filters out points in hloc that are not local maxima, then counts the number of points above
the threshold 4γ. If n is the inferred number of objects in the scene, f count outputs the corresponding
one-hot vector, and the distribution q(n | x) = δ(n - n) deterministically takes the value n. We fix
the maximum number of objects per image to M = 10.
The locations {Zl(oi)c}in=1 are iteratively sampled from a categorical distribution with logits hloc,
where all the logits of the previously sampled location are set to a low value to prevent them from
being sampled again. At the ith step, when the ith object’s location is sampled, the corresponding
feature vector in happ is interpreted as the means and log variances of the L components of the ith
appearance variable. When sampling from the categorical distribution, we use Gumbel-Softmax for
single-sample gradient estimation. The temperature parameter τ is exponentially annealed from 0.5
to 0.02 in 100k steps. The latent space for each appearance variable has L = 32 dimensions.
The sprite-generating function fsprite is a convolutional network that takes as input a sampled ap-
(i)
pearance vector Zapp . The first part of the network consists of a fully connected layer, a convolu-
(i)
tional layer, and a bilinear interpolation. The vector Zapp is then expanded and concatenated to the
resulting tensor along the channel dimension. The second part of the network consists of 3 residual
blocks with 2 convolutional layers each, followed by a final convolutional layer with a sigmoid ac-
tivation. Leaky ReLU and batch normalization are used after each layer, except in the last residual
block. The size ofa generated sprite for the multi-MNIST data set is 17 × 17, for multi-dSprites it is
21 × 21. The mean of the Bernoulli output is then computed from these sprites as explained above.
We optimized the model with stochastic gradient descent, using Adamax with batch size 64. The
initial learning rate was 1e-4 or 5e-4 for the location inference network (for the multi-MNIST and
multi-dSprites data sets, respectively) and 1e-3 for the rest of the model. In the location inference
net, the learning rate was exponentially decayed by a factor of 100 in 100k steps. The model param-
eters are about 500k, split almost evenly among location inference, appearance inference, and sprite
generation.
11
Under review as a conference paper at ICLR 2020
Training warm-up. In practice we found it beneficial to include a warm-up period at the be-
ginning of training, in which an auxiliary loss is added to the negative ELBO. We train a fully-
convolutional VAE in parallel with our model, and take the location-wise KL in the latent space as
a rough proxy for object location, as suggested by Nash et al. (2017). The additional loss is the
squared distance between hloc and the KL map. The network inferring object location is therefore
initially encouraged to mimic a function that is a (rather rough) approximation of the location, and
then it is fine-tuned. Intuitively, we are biasing sampling of zloc in favor of information-rich loca-
tions, which significantly speeds up and stabilizes training. To stabilize training, we also found it
beneficial to randomly force n = 1 during training with a probability that is 1 during warm-up (30k
steps) and then linearly decays to 0.1 in the following 30k steps.
The auxiliary VAE is implemented as a fully convolutional network, in which the encoder consists of
3 residual blocks with downsampling between blocks, and a final convolutional layer. The decoder
loosely mirrors the encoder. The latent variables are arranged as a 3D tensor with size 11 × 11 × 8.
The auxiliary loss is added to the original training loss for a warm-up phase of 30k steps. In the
subsequent 30k steps, the contribution of this term to the overall loss is linearly annealed to 0.
Rescaling location logits. Assume we have k objects and the logits of the inferred location
distribution are either 0 or h. The probability of one of the “hot” locations after softmax is
eh/(D - k + keh) = t where t is a constant that should not depend on D, and should be close
to 1/k. Solving for h we get h = logt + log(D - k) - log(1 - kt). If D is large enough and k
is small enough, we have log(D - k) ≈ log D, and the constant logt is relatively small in mag-
nitude. Because of our assumptions on the logits and on t, we can write log(1 - kt) ≈ 0, and
therefore the high logits h should be approximately proportional to log D. Thus, using the same
fully convolutional network architecture for multiple image sizes, the logits should be scaled by a
factor proportional to log D. The threshold for counting objects should likewise follow this rule of
thumb.
Baseline implementation. As baseline we use a fully convolutional β-VAE, where the latent vari-
ables are organized as a 3D tensor. Being closer in spirit to our method, this leads to a fairer compar-
ison. Furthermore, this inductive bias makes it easier to preserve spatial information. Indeed, this
empirically leads to better likelihood and generated samples than a VAE with fully connected layers
(and a 1D structure for latent variables), and allows to model more naturally a varying number of
objects in a scene. The encoder consists of a convolutional layer with stride 2, followed by 4 resid-
ual blocks with 2 convolutional layers each. Between the first two pairs of blocks, a convolutional
layer with stride 2 reduces dimensionality. The resulting tensor has size 64 × 8 × 8, and a final
convolutional layer brings the number of channels to 2dVAE/82 where dVAE is the number of latent
variables. The decoder architecture takes as input a sample z of size dVAE /82 × 8 × 8 and outputs
the pixel-wise Bernoulli means. Its architecture loosely mirrors the encoder’s, with convolutions
being replaced by transposed convolutions, except for the last upsampling operation which consists
of bilinear interpolation and ordinary convolution. All convolutions and transposed convolutions,
except for the ones between two residual blocks, are followed by Leaky ReLU and batch normal-
ization. The last convolutional layer is only followed by a sigmoid nonlinearity. In our experiments
we used dVAE = 1024 and we linearly annealed β from 0 to 1 in 100k steps. The number of model
parameters is about 1M.
C Results on multi-MNIST- 1 k
Here we present additional visual results on the multi-MNIST-1k data set, similar to those discussed
in the main text.
12
Under review as a conference paper at ICLR 2020
Figure 10: Prior samples. Left: images generated by sampling from LAVAE’s prior
p(zloc | n)p(zapp | n)p(n), where p(n) is learned from data. Right: images generated by sampling
from the baseline’s prior p(z).
Figure 11: Disentanglement experiments on test images with more objects than in the training
regime. Objects are represented independently of each other, and their location and appearance are
disentangled by design. Top left: Reordering the sequence {zl(oi)c}i or equivalently of {z(aip)p}i leads
to objects being swapped (top row: original reconstruction; bottom row: swapped objects). Top
right: Latent traversal on one of the 7 location variables. Bottom: Latent traversal on one of the 7
appearance variables (along 3 different dimensions).
13