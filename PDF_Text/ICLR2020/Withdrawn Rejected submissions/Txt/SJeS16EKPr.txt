Under review as a conference paper at ICLR 2020
Learning relevant features for statistical in-
FERENCE
Anonymous authors
Paper under double-blind review
Ab stract
Given two views of data, we consider the problem of finding the features of one
view which can be most faithfully inferred from the other. We find that these are
also the most correlated variables in the sense of deep canonical correlation anal-
ysis (DCCA). Moreover, we show that these variables can be used to construct
a non-parametric representation of the implied joint probability distribution. This
representation can be used to compute the expectations of functions over one view
of data conditioned on the other, such as Bayesian estimators and their standard
deviations. We test the approach using inference on occluded MNIST images, and
show that our representation contains multiple modes. Surprisingly, when applied
to supervised learning (one dataset consists of labels), this approach automatically
provides regularization and faster convergence compared to the cross-entropy ob-
jective. We also explore using this approach to discover salient independent vari-
ables of a single dataset.
1	Introduction
Given samples (x1, y1), . . . , (xn, yn) from an unknown joint probability distribution p(x, y), we
want to construct a useful representation of the conditional probabilities p(x|y) and p(y|x), so that
we that we can infer one view from the other on new data.
For instance, x and y could be past and future histories of dynamical data, visual and auditory inputs,
actions and their effects, etc.
Following the approach introduced in Beny & Osborne (2013; 2015b) in the context of quantum
information theory, we look at the problem as follows: the conditional distributions p(y|x) can
be thought of as representing a noisy communication channel (stochastic map). This channel is a
linear map between spaces of typically ludicrously large dimensions (the spaces of all probability
distributions over x or y). We want a pair of small subspaces which best represent the channel.
Specifically, we look for those vectors representing probability distributions over x which lose least
distinguishability under the channel, where the distinguishability is measured by the χ2 divergence.
We show in Section 3 that this is equivalent to performing a certain singular value decomposition
of the channel (seen as an operator in Hilbert space) and keep only the components with the largest
singular values. Moreover, the full singular value decomposition is equivalent to the decomposition
in terms of canonical variables introduced in Lancaster (1958), namely,
D
p(y|x) = p(y)	ηiui(x)vi(y),	(1)
i=1
where ui and uj are real non-linear functions such that E(uiuj ) = δij , E(vivj ) = δij , and 0 <
ηD ≤ ∙∙∙ ≤ ηι ≤ no = 1 are the singular values.
The practical advantage of this representation for inference (or prediction) is that it reduces the
evaluation of conditional expectations to that of empirical averages over the (unconditional) marginal
p(y).
As observed in Michaeli et al. (2016), the span of the first k canonical variables ui , vj is what
is learned by the deep canonical correlation analysis (DCCA) (Andrew et al., 2013). Indeed, these
1
Under review as a conference paper at ICLR 2020
variables are those which maximize the correlations E(uivi) subject to the same constraints as above.
(This reduces to CCA (Hotelling, 1936) when ui, vj are linear maps).
In this work, besides establishing this new information-theoretical interpretation of canonical vari-
ables and DCCA, we experiment with using this representation for performing inference on new
data. Moreover, we propose a strategy for extracting disentangled variables from the canonical
variables, inspired by analytical solutions.
2	Related work
This general problem (of building an effective representation of the conditional probability distribu-
tions implied by joint samples) covers many existing approaches in different contexts. For instance,
if the variables y has few possible states, then it reduces to a classification problem, usually solved
by minimizing the crossentropy between a predicted distribution and the one-hot encoding of the
classes.
When y has a large number of states, or is fundamentally continuous, existing approaches usually
do not model the whole conditional distribution, but either provide the average (regression), or
approximately sample from it.
The main class of methods which allows for sampling from the conditional distributions are varia-
tional: a deterministic neural networks produces the parameters of analytical classes of probabilities.
This includes variational autoencoders Kingma & Welling (2013) (e.g., Iten et al. (2018); Wang et al.
(2016)), and approaches based on the minimum description length principle such as Gregor et al.
(2013).
Alternatively, it may also be possible to use adversarial training Goodfellow et al. (2014), by using
a conditional Mirza & Osindero (2014) version of an energy-based GAN Zhao et al. (2016).
By contrast, our approach doesn’t require the training of a generative model. Instead, conditional
expectations are constructed as linear combinations of unconditional empirical averages over the
training data.
Previous approaches to equipping CCA or DCCA with an information-theoretic interpretation have
explored different directions. For instance, in Wang et al. (2016), the authors generalize a probabilis-
tic interpretation for CCA in terms of gaussian distributions, which leads to a variational approach.
In Painsky et al. (2018), additional constraints on the mutual information between the data and the
learned variables are added to the optimizations.
A previous attempt at designing a numerical solution for our singular value problem can be found in
Beny (2018b). In that work, the relevant variables were represented through a PCA kernel produced
by Monte Carlo sampling, but wasn’t practical.
3	Theory
We formalize the problem by assuming that our data was sampled from an unknown joint distribution
p(x, y) over two random variables X and Y .
Let VX and VY denote the linear spaces spanned by all probability distributions over X and Y re-
spectively. Here we assume that X andY take finitely many values for simplicity, but this formalism
can be straightforwardly extended to infinite-dimensional vector spaces.
We will need inner products on VX and VY , to make them into real Hilbert spaces. We use the Fisher
information metrics evaluated at the points p(x) and p(y) respectively (marginals of p(x, y)), that
is,
：=X μx)μ(x and hν"iγ:= X ν(y)ν0(y)	⑵
p(x)	p(y)
xy
for any vectors μ,μ0 ∈ VX and ν,ν0 ∈ Vγ.
Below we also call the marginals pX and pY respectively when omitting their arguments (pX (x) ≡
p(x) and pY (y) ≡ p(y)).
2
Under review as a conference paper at ICLR 2020
These inner products allow us to define the χ2 divergence:
χ2 (q, pX) = hq -pX,q -pXiX,	(3)
which a measures of statistical distinguishability between q and pX . Specifically, it quantifies how
easy it is to reject the null hypothesis that the state is pX when it is actually q, based on the empirical
distribution obtained from independent samples. It is also the lowest order approximation of the
Kullback-Leibler divergence.
The joint distribution p(x, y) yields conditional distributions p(y|x) and p(x|y). These can be un-
derstood as the components (or kernels) of stochastic maps N : VX → VY and N* : VX → VY
respectively. Explicitely, if μ ∈ VX and V ∈ Vγ, then the images N(μ) ∈ VY and N*(ν) ∈ VX are
defined by
N(μ)(y) = Xp(y∣χ)μ(χ) and N*(V)(χ) = Xp(χ∣y)ν(y).	(4)
These stochastic maps NandN* perform inference of one variable given some (possibly imperfect)
knowledge about the other, with priors given by the marginals p(x) or p(y) of p(x, y) depending on
the direction of the inference. Importantly, N* is the transpose of N in terms of the inner products
defined above:
hν,N(μ)iγ = hN *(ν ),μiX.	(5)
We now have the tools to address the problem mentioned in the introduction. The distinguishability
between q ∈ VX and pX after the action of the channel N is χ2 (N(q), pY) since pY = N(pX).
Hence We want to find the distributions P which maximize the relevance Beny & Osborne (2013).
f X	X2(N(q),PY)	hN(μ),N(μ)iγ	公
η(q) = -2---------= —/—\---------------,	⑹
χ2(q,PX)	hμ,μX
where μ = q - PX. The inner-product formulation makes it clear that this amounts to finding
the eigenvector with largest eigenvalue for the symmetric map N*N, which is also the singular
vector with largest singular value for N. On can then go on to find the eigenvector with next largest
eigenvalue and so on, which are automatically orthogonal.
In practice, the inner products are more tractable to compute if we express elements μ ∈ VX and
v ∈ VY in terms of variables f and g as μ = PX f and V = PY g, or
μ(x) = p(x)f(x) and v (y) = p(y)g(y)
for all x, y . Indeed, this yields simply
hμ, μ0iX = E(ff0) and hv, u∖γ = E(gg0).
We are now in measure to make the connection with DCCA Andrew et al. (2013). Indeed, the aims
of DCCA is to maximize the correlations corr(f, g) = E(fg) over function f(x) and g(y) such that
E(f2) = E(g2) = 1. But, using μ = PX f and V = PY g , we have
Efg) = hv, N (μ)iY,
which is maximized by the left- and right- singular vectors μ and V of N with largest singular value.
Given all the singular vectors μi = PXf and Vi = PYgi with singular values ∣, we obtain the
representation
N (μ) =	Vihμi, μi X,
i
which, using a more standard notation and the Kronecker delta δx, yields Eq. 1:
P(y|x) = NM ⑹=P⅛XηiVi(y)μi(x) = P(y) XηiVi(y)ui(x),
where μi = PX Ui and Vi = PY vi.
3
Under review as a conference paper at ICLR 2020
3.1	Non-diagonal form and relevant variables
For the purpose of the optimization and inference, we do not need to full diagonal decomposition,
but just functions f = μ%∕pχ and gj = Vj/pγ, i,j = 1,..., ko, which have the same span as the
canonical variables ui and vj respectively for i, j = 1, . . . , k0 (assuming that η1, . . . , ηk0 are the
largest singular vector). Below we refer to fi and gj as the k0 most relevant variables.
Because these functions may not be orthogonal, we need the covariance matrices
Kij = hμi,μj iχ = E(fifj), Lij = hνi,νj iχ = E(gigj), Aij = hνi, N (μj )iγ = E(gfj).
If Nij denote the components of N in the sense that N(pXfj) = Pi NijpY gi, then, using our
inner products to isolate Nij, We obtain N = L-1A. Similarly, the components of N* are Nij =
K-1A>. This implies that the sum of the square of the singular values ofN restricted to the spans
of the vectors pXfi and pY gj for all i, j, which is what we want to maximize, is just given by
k0
Xηi2 = Tr (N*N) = Tr (K-1A>L-1A).
i=1
This is the DCCA objective. Below we use the objective function C = k0 - Tr (N*N), for the
cosmetic reason that its optimal value is zero.
Moreover, the corresponding truncated representation of the conditional distribution is
k0
p(y|x) = N(δx)(y) 'p(y) X (L-1AK-1)ijgi(y)fj(x),
i,j=1
where we used the fact that the components of δx are δj = Pi Kj-i1fi(x).
Of course, This approach can produce a faithful representation of the correlations only if N is
actually close to being of rank k0 (see Appendix B for a more precise statement). Ifwe interpret the
relevant subspace as a space of probability over latent variable, this means that our latent variables
have at most k0 discrete states.
However, even if the rank k0 corner of N is a not a good approximation, this strategy allows us
to nevertheless do the correct inference on certain random variables, namely those which are in the
span of the canonical variables!
Indeed, the exact conditional expectation ofgk is (assuming D is the actual rank ofN),
D
X gk(y)p(y|x) = X (L-1AK-1)ijE(gkgi)fj(x)
y	i,j=1
D	k0
= X(AK-1)kjfj(x) = X(AK-1)kjfj(x),
j=1	j=1
where the last truncation is exact if k ≤ k0 due to the assumption that the basis fi and gj have the
same span as the k0 largest right and left singular vectors of N respectively.
For instance, if p(x, y) is Gaussian, the canonical variables can be computed analytically, as in
Lancaster (1958) or Beny (2018a) in the multivariate case. Solutions for other distributions were
also computed in Eagleson (1964).
Notably, for any two dimensional Gaussian, the space ofk most relevant variables is simply spanned
by the moments fn(x) = xn and gn(y) = yn for n = 0, . . . , k - 1. Hence, in this case the first k
moments can be inferred exactly using only the k + 1 most relevant variables (See Appendix C).
4	Algorithm
Let us explicit the algorithm resulting from the above analysis.
4
Under review as a conference paper at ICLR 2020
We assume that we are given independent samples (x1, y1), (x1 , y2), . . . from the otherwise un-
known joint distribution p(x, y).
We first perform DCCA (Andrew et al., 2013). That is, we need two independent deterministic feed-
forward neural networks. The first maps x to a set of k0 real-valued variables f1 (x), . . . , fk0 (x).
The second maps y to a different set of k0 variables g1 (y), . . . , gk0 (y).
The parameters of the neural networks are to be set to minimize the objective function
C = k0 - Tr (K-1A>L-1A),	(7)
where the matrices K, L, A can be approximated over a mini-batch (xn, yn), n = 1, . . . , N via
1N	1N	1N
Kij = N	fi(xn)fj (Xn)，	Lij = Ns^^gi (yn)gj (yn), Aij = N	gi(yn)fj (Xn).⑻
We found that, provided the batch size is sufficiently large compared to k0 (about 10 times in our
experience), this can be minimized using ADAM or direct gradient descent. However, to guarantee
stability when using large k0, we needed to explicit the gradient of the objective function in order
to force the use of the Moore-Penrose pseudo-inverses for K-1 and L-1 in both the forward and
backward passes, in addition to using 64 bits floats in these computations.
Once the relevant variables have been learned, we still need to use the training data in a second
step. Indeed, suppose that we wish to use our model to infer the value of some function Θ(X), i.e.,
to compute its approximate expectation value in terms of the conditional distribution X 7→ p(X|y).
Then we need to store, for each variable j = 1, . . . , k0, the quantities
1 Nfull
θj = ∙^~ X θ(Xnfj (Xn ),	⑼
Nfull n=1
where the average is to be taken on the full training batch (of size Nfull). The same can be done
exchanging X with y and fj with gj for the reverse inference.
For instance, if a data point X is composed of real components Xa—such as pixel color components
for an image—and we are interested in the estimator which minimize the expected l2 distance to
the predicted values of these components, then we need the expectation values of the components
Θ(X) = Xa for all a, and possibly higher moments to gain more knowledge about the shape of the
posterior distribution, such as the second moments Θ0(X) = X2, etc.
Inference can then be performed with new data using
k0
Θ = Xp(x∣y)Θ(x) ≈ X (K-1A>L-1)jiΘjgi(y).	(10)
x	i,j=1
Moreover, the accuracy of these predictions does not depend on the rank k0 if Θ is taken in the span
of the relevant variables, i.e., Θ(X) = Pik=1 cifi(X), for which Θj = Pik=0 1 ciKij.
The reverse inference formulas are obtained simply by the exchanges K 什 L, A 什 A>, and
gi 什 fi.
5	Experiments
In all our experiments, we used the ADAM optimizer with learning rate 0.001. We used the Flux
package (Innes, 2018) for Julia, as well as Tensorflow.
As usual the data is divided into a training set and a testing set. No aspect of the testing set is used
during training. The loss function refers to Eq. (7). In order to monitor overfitting, we compute a
“test loss” and a “training loss”. The test loss is computed from the trained variables using only the
test data, and accordingly, the training loss is computed purely using the training data.
Moreover, when performing inference on test data using Eq. (10), we use the covariances A, L, K
and expectations Θj (Equ. (9)) built from the training data only.
5
Under review as a conference paper at ICLR 2020
5.1	Inference on occluded MNIST
In this experiment, we use the left and right halves of the MNIST digit images as correlated variables
X and Y . The goal is to obtain the expected left halves given the right halves, or vice versa.
The training set was augmented by random small rotations and displacements to make the task more
ambiguous, as we want to explore the uncertainty in the prediction.
The relevant variables were represented by two convolutional neural networks of identical architec-
ture. They are composed of four convolutional layers and one fully connected layer, an architecture
that performs well for supervised learning on this dataset. For ease of implementation, these CNN
have the whole image as input, but with either half zeroed (same value as black pixels). Half-width
CNNs with proper padding at the cut perform similarly.
After training, we used the training dataset to also compute the expected pixel gray value as well as
their covariance for each relevant variable using Eq. (9).
These were used into Eq. (10) to compute the mean pixel gray values and their covariances over the
conditional probability of X given Y on test data. This mean is the Bayesian estimator for the l2
distance between half images, i.e., it should minimize the expected distance dl2 over the conditional
distribution, where dl22 (x, y) = Pi(xi - yi)2, where xi ∈ [0, 1] is the value of the ith pixel. (This
is equivalent to minimizing the mean square error).
The results on a randomly selected subset of test digits is shown in Fig. 1. For each example, we also
computed the images obtained by adding plus or minus one standard deviation along the direction
of greatest variance in the space of relevant variables. This reveals the main ambiguities (such as
between 8 and 3 or 7 and 9 which share a similar right half).
The graph of the singular values shows that the rank cutoff of 200 is too low to capture all of the
relevant variables (the sudden drop at the end is not robust to an increase in the cutoff), but the re-
sults are reasonable nevertheless. This shows that our representation of the conditional distributions
contains valuable information besides the simple mean.
5.2	Supervised learning
In the context of a supervised classification task, one of the dataset (the labels) is of sufficiently low
dimensionality that we can use a complete basis over its probability space as our relevant variables,
such as the standard one-hot encoding of labels. This serves as a good first sanity test for our
approach. Surprisingly, we find that it converges faster than standard approaches, and without the
need for regularization.
Let the variable Y stands for the labels, with values in {1, . . . , k}. The probability space consists of
vectors with k real components. The canonical basis corresponds to the one-hot encoding gi (j) =
δij (Kronecker delta). All we need is a neural network to encode k variables f1 , . . . , fk on X . After
learning the most relevant variables fi, we apply the reverse of Eq. (10) for function Θ(y) = y, and
use the maximum component of expected value y to infer the labels from the data.
Let us refer to this procedure as DCCI (Deep Canonical Correlations based Inference).
We tested this approach on the MNIST and CIFAR10 datasets, and compared the results to the
standard cross-entropy objective (Fig. 2).
We plotted the accuracy as function of the epoch rather than clock time which would depend on
many factors. But the time per epoch is roughly the same for each approaches in the above experi-
ments. Indeed, the training time is dominated by the forward and backward evaluations of the neural
networks which are identical. (However, the time it takes to evaluate our objective can become sig-
nificant for much larger number of labels k, since it involves the inversion of matrices of dimension
k. This is in addition to the fact that a greater dimension would require also larger batches.)
We found that, without regularization, simply changing the objective from cross-entropy to DCCI
provided a large improvement both of convergence speed and final accuracy for both models.
6
Under review as a conference paper at ICLR 2020
Figure 1: Left mosaic: the left halves of the MNIST digits in a random sample from the test set are
inferred from the right half, with cutoff k0 = 200. Three images are shown for each digit. Within
each triplet, the middle image represents the mean over pixel intensity of the inferred condition
distribution, while left and right images corresponds to a plus and minus one standard deviation
from the mean in the direction of largest covariance (in the space of half-images). A particularly
interesting example is highlighted. Top-right: loss per epoch for k0 = 200. Bottom-right: the
singular values for different values of the cutoff k0, after 150 epochs in each case.
epoch
0	50	100	150	200	250
singular values
epoch
Figure 2: Loss and inaccuracy (error rate) on test sets for two classification tasks. The models were
trained either using the cross-entropy (CE) or our approach (DCCI), with or without regularization
layers. On the MNIST dataset, we used an “all CNN” network, and for the CIFAR10 dataset we used
a short VGG variation with 10 convolutions and 3 fully connected layers. In the regularized form,
post-activation Batchnorm layers were placed after each convolutional layers on the VGG network.
What is shown is the mean over 10 independent runs for MNIST and 5 runs for CIFAR10. The
shaded area spans the standard deviation. ADAM with default parameters was used in all cases. No
data augmentation was used except for horizontal flips for CIFAR10 (resulting in epochs of 100,000
images).
Inaccuracy on CIFAR10 (%)
epoch
7
Under review as a conference paper at ICLR 2020
Figure 3: Top-left: First 20 relevant variables (on X) determined by DCCA for a system where X
consists of two coordinates uniformly sampled over a circle and a surrounding ring, and Y consists
of the same points but shifted by a small normally distributed vector. The variables are arranged
from left-to-right and top-to-bottom in order of decreasing relevance. Top-right: the same variables
multiplied by the marginal pX . Bottom row: introducing a gap in the ring allows for a monotonous
function of the angle to serve as second most relevant variable (instead of the sine/cosine couple).
Hence the angle is automatically “disentangled” from the other variables. (Mid-gray represents the
value 0).
On MNIST, DCCI alone also outperformed cross-entropy with dropout. (Dropout did not yield
any improvement in conjunction with DCCI). However, adding batch-normalization layers on the
CIFAR example, erased any distinction between DCCI and cross-entropy.
5.3	Structure of the relevant variables
We mentioned in Section 3 that if p(x, y) is a two-dimensional Gaussian distribution with zero
mean, then the n most relevant variables of X are the first n powers of X itself, independently of
the covariance matrix. This implies that the canonical variables are the Hermite polynomials in X
(which results from applying the Gram-Schmidt procedure to the basis {1, x, x2 , . . . }).
A similar property holds for multivariate Gaussians, namely, the less relevant singular values are
polynomials in the more relevant ones. If this is true more generally, it should be possible to further
compress and organize the latent space extracted with DCCA by finding a minimal set of generators,
which ought to also be in the span of the most relevant variables.
We applied DCCA to a synthetic dataset to explore this idea, shown in Fig. 3. In this case, we
actually performed a final SVD to obtain the unique uncorrelated canonical variables, and ordered
them by decreasing relevance (their respective singular values).
Here, X consists of two real numbers, distributed uniformly within a ring and a disk. The variable
Y is obtained by adding a random Gaussian shift to X with a small standard deviation. The more
relevant variables ought to be those which are more robust to such small random displacement. This
formalizes the idea that We are interested in extracting “large-scale” variables Beny (2018b).
We would expect the relevant independent variables to be: the binary variable indicating whether
the point is in the disk or the ring and the angle around the ring, folloWed by the radial component
in the ring, and finally the Cartesian coordinates inside the disk. This is precisely What We see in
Fig. 3.
Indeed—if We put aside for noW the fact that the angle itself is not directly represented—besides the
constant function, the tWo most relevant variables are the sine and cosine of the angle, folloWed by
the binary variable separating the disk from the ring.
But these variables ought to span the space of probabilities over the relevant variables, not just the
variables themselves. Hence the next six variables are sines and cosines of smaller Wavelength,
8
Under review as a conference paper at ICLR 2020
reconstruction error vs latent dimension
latent dimension
O08-3∕G∕G 9 7
gs6l753夕Cq
7sbo 7 夕，cn5G
sqb3 4 夕β∕<r3
Y5"Λ¥oovα /
3 0 33½-夕 755,
9 0/ 300。Ca 户S
f77oss3otr3
75 0γ*5 33y9a
2
Figure 4: Left: Best mean squared error for images reconstructed from the k most relevant variables,
as a function of k (the latent dimension). This logarithmic plot shows that improvements stop once
the dimension reaches 19 (where the two lines cross). Right: images produced by the generator from
latent variables sampled according to the best Gaussian fit in latent space, for feature subspaces of
dimensions 2, 8 and 19.
which can encode probability distributions which are increasingly more precisely localized, down to
a precision (wavelength) comparable with the diameter of the inner disk. Accordingly, the next two
most relevant variables are the Cartesian coordinates inside the disk. This is followed by additional
moments of the angle, down to a wavelength equal to the ring’s thickness, at which point we see the
radius in the ring appear.
As mentioned, we see that the angle itself is not represented, likely because it is discontinuous.
However, as shown also in Fig. 3, creating a gap in the ring allows for the angle to emerge as
most relevant variable. This suggests that this approach may be able to automatically learn intrinsic
coordinates of the latent variable manifold.
5.4	Independent variables and generative model
If we postulate that the independent (or disentangled) relevant latent variables can be found in the
linear span of the relevant variables, we can attempt to extract them by optimizing a neural network
composed of two parts. Firstly, a linear layer maps the relevant variables to a small number of
outputs (equal to the latent dimension). The purpose of this linear layer is to find the independent
variables. These latent variables are then processed by an arbitrarily complex generative network to
produce a possible value of the variable X. As objective function, we may us an appropriate measure
of similarity between the output and the data element from which the variables were obtained.
We tested this idea as follows. We took X to consist of the MNIST digits, and produced Y by
randomly permuting neighboring pixels in the image, until the mean displacement per pixel is of
order 1. In addition, we added independent Gaussian noise to the pixel values. (Hence the noise
map N simulates the Coarse-graining channel introduced in Beny & Osborne (2015a)).
As in the previous experiment, we do so to implement our intuition that the more relevant variables
ought to be the ones which are of larger scale, or more robust to local perturbations.
The relevant variables of the clean images were produced by the same convolutional neural network
as in Section 5.2, while the variables of the coarse-grained images were extracted by a network of
the same geometry, but with half the number of filters and neurons.
We extracted the 1000 most relevant out of 1200 learned variables in this way. (The least relevant
variables in this system happen to be highly dependent on the total number of variables and hence
cannot be trusted to be correct). As a second step, we trained a linear layer coupled to a network
composed of 5 fully-connected layers of 800 hidden neurons each. We refer to the number of output
neurons in the first linear layer as the latent dimension.
9
Under review as a conference paper at ICLR 2020
As input, this network received the variables extracted from MNIST images using the above convo-
lutional neural net (after it was fully trained using DCCA), and was trained to minimize the mean
square error between its output and the original MNIST digit.
The resulting best mean square errors are shown in Fig. 4, as function of the latent dimension. Here
we see a distinct change of polynomial scaling law at dimension 19. Increasing the dimension further
provides no improvement. This behaviour is compatible with our hypothesis that the extra variables
are just functions of those first twenty variables (functions which are effectively re-implemented by
the generative network).
Images generated by sampling from a Gaussian approximation of the latent distribution for different
latent dimensions are shown in Fig. 4. Below dimension 20, most generated image can be recognized
as a specific digit.
6	Outlook
We studied the classical (non-quantum) form of the theory introduced in Beny & Osborne (2013),
and found that the relevant observables of that theory are just the most correlated canonical variables
in the sense of DCCA Andrew et al. (2013), and can be learned effectively using standard machine
learning methods.
This point of views on DCCA provided us with several new insights. The first is that the learned
relevant variables provide a useful representation of a joint probability distribution. We showed that
performing inference using this representation can outperform crossentropy in predicting classes.
Our experiments on halves of MNIST also show that the conditional distribution we obtain can
effectively represent the uncertainty in the prediction of high-dimensional data.
A second insight relates to the interpretation of the canonical variables as spanning directions in
the space of probability distributions. As suggested by the gaussian solutions and our experiment
on synthetic data, we postulate that the canonical variables are functions of a small number of
independent generators contained in their span. This hypothesis is supported by our experiment on
MNIST, but further work is required to find a way to cleanly extract these variables.
We have yet to explore the potential applications of one of the salient aspect of this approach to
inference, the fact that the canonical variables learned using DCCA are also those which can be most
reliably predicted, irrespective of the value of the cutoff. To see why this is potentially significant,
we observe that a central feature of scientific exploration is that we are not so concerned with making
predictions about some given variables, as much as we are with discovering variables which can be
predicted.
Another important feature of this approach is the fact that the resulting model allows for the direct
evaluation of the expectation values in the posterior distribution without sampling. In particular
this allows for the evaluation of credible intervals. Hence it should be especially suited to scientific
applications where the ability to quantify uncertainty is essential.
Finally, the relationship that we established with theory of quantum origin points towards a potential
quantum generalization of DCCA that would apply to quantum data, or classical measurements of
quantum systems.
Acknowledgments
We would like to thank Joel Beny and Raban Iten for helpful suggestions. We are also in-
debted to an anonymous ICLR2020 referee for pointing out the connection between our approach
and DCCA. This work was supported by the National Research Foundation of Korea (NRF-
2018R1D1A1A02048436).
References
Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. Deep canonical correlation analysis.
In International conference on machine learning, pp. 1247-1255, 2013.
10
Under review as a conference paper at ICLR 2020
C. Beny and T. J. Osborne. Information geometric approach to the renormalisation group. Phys.
Rev. A, 92:022330, 2015a. doi: 10.1103/PhysRevA.92.022330. (arXiv:1206.7004).
Cedric Beny. Coarse-grained distinguishability of field interactions. Quantum, 2:67, 2018a.
(arXiv:1509.03249).
Cedric Beny. Inferring relevant features: from qft to pca. International Journal of Quantum Infor-
mation, 16:1840012, 2018b. (arXiv:1802.05756).
Cedric Beny and Tobias J Osborne. Renormalisation as an inference problem. (arXiv:1310.3188),
2013.
CedriC Beny and Tobias J Osborne. The renormalisation group via statistical inference. New J.
Phys., 17:083005, 2015b. doi: 10.1088/1367-2630/17/8/083005. (arXiv:1402.4949).
GK Eagleson. Polynomial expansions of bivariate distributions. The Annals of Mathematical Statis-
tics, 35(3):1208-1215,1964.
Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychome-
trika, 1(3):211-218, 1936.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregres-
sive networks. arXiv preprint arXiv:1310.8499, 2013.
Harold Hotelling. Relations between two sets of variates. Biometrika, 28(3/4):321-377, 1936.
Mike Innes. Flux: Elegant machine learning with julia. Journal of Open Source Software, 2018.
doi: 10.21105/joss.00602.
Raban Iten, Tony Metger, Henrik Wilming, Lldia Del Rio, and Renato Renner. Discovering physical
concepts with neural networks. (arXiv:1807.10300), 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. (arXiv:1312.6114), 2013.
HO Lancaster. The structure of bivariate distributions. The Annals of Mathematical Statistics, 29
(3):719-736, 1958.
Tomer Michaeli, Weiran Wang, and Karen Livescu. Nonparametric canonical correlation analysis.
In International Conference on Machine Learning, pp. 1967-1976, 2016.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
M. Ohya and D. Petz. Quantum entropy and its use. Springer Verlag, 2004.
Amichai Painsky, Meir Feder, and Naftali Tishby. An information-theoretic framework for non-
linear canonical correlation analysis. arXiv preprint arXiv:1810.13259, 2018.
Weiran Wang, Xinchen Yan, Honglak Lee, and Karen Livescu. Deep variational canonical correla-
tion analysis. arXiv preprint arXiv:1610.03454, 2016.
Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network.
arXiv preprint arXiv:1609.03126, 2016.
11
Under review as a conference paper at ICLR 2020
A Extra information about the algorithm
A. 1 Alternative interpretation of the objective
If we write Fij := fj (xi) and Gij := gj (yi) for the value of our variables on the dataset, then
K = NF>F, L = NnG>G and A =得G>F. The DCCA objective can then be written as
Tr (K-1A>L-1A) = Tr (PQ)
where P = F(F> F)-1F> and Q = G(G>G)-1G> are the projectors on the ranges of F and
G respectively. Hence, we are maximizing the overlap between those ranges (which represent pos-
sible linear combinations of datapoints, respectively determined from variables of one or the other
correlated views.)
A.2 Heuristic
Batch size—In our experiments, we observed that the batch size during training needs to be an
order of magnitude larger than the number of variables (rank cutoff). When the batch size was
too small, learning seemed to converge normally in terms of training and test loss, but resulted
in variables which yield dramatically different losses when evaluated on larger batches, and yield
spurious predictions.
Constant variables—The loss function C takes value between 0 and k0 - 1 because the constant
variable always has relevance 1. The constant variable could be enforced a priori rather than learned,
which, due to the objective, automatically forces the learned variables to have zero expectation
values (be orthogonal to the constant variable). This might have advantages in certain circumstances,
but in our experiments we found that this sometime hindered convergence.
Invertibility issues—The covariance matrices K and L can be ill-conditioned, potentially causing
the gradient to “explode” because of the inverses K-1et L-1 involved in the loss function. This can
be avoided either by using the Moore-Penrose pseudo-inverse, or by replacing K-1by (K + 1)-1
in the loss for some small positive number , and likewise for L-1.
Symmetries in the loss function—The loss C only depends on the span of the variables fi and gj ,
hence it has a very large group of symmetries. In particular, it is invariant under a change of the
norm of each variable independently from each other. Because of that, it is preferable not to have a
linear last layer. Using a hyperbolic tangent as last nonlinearity worked in our experiments.
Regularization—In all our tests, dropout had no beneficial effect. In fact, our objective seems to
already provide a form of regularization, as shown in Section 5.2.
B	Theory in more details
We consider two correlated random variables X and Y with a joint probability distribution p(x, y).
We assume that we are able to numerically evaluate expectations with respect to this distribu-
tion, for instance because we can sample from it. We want to use this ability in order to com-
pute expectations with respect to the conditional distributions pX|Y (x|y) = p(x, y)/pX(x) and
pY|X(y|x)	=	p(x, y)/pY (y),	where	pX (x)	= Py p(x,	y)	and pY (y)	= Px p(x,	y)	are the
marginals of p. Below we sometime remove the subscripts X, X|Y or Y |X if there is no ambi-
guity.
For instance, suppose we generated samples ofy given x, through explicit knowledge ofpY |X. Then
the evaluation of expectations with respect to pX|Y is the subject of Bayesian inference. However,
this is generally done in a context where the variable X has low dimensionality and parameterizes
a hand-crafted model. Our approach, however, is free of such a model and the variable X can be of
very high dimensionality.
12
Under review as a conference paper at ICLR 2020
B.1	Inner product on probability vectors
In order to define our strategy, we need to equip the spaces of probability distributions for X and Y
with an inner product structure. Let us focus on X , and assume that it takes discrete values to avoid
unnecessary technicalities. The set of probability vectors is a convex subset of the real linear space
VX = Rn . Let us equip this space with the product
0 o∖ L μ(X)μ0(X)
hμ,μ iχ ∙=T K(Xr
x
(11)
for any μ,μ0 ∈ Vχ. We also write ∣∣μ∣∣X = hμ,μix. Importantly, this depends explicitly on the
fixed probability vectorpX(X), which we took to be the marginal of p(X, y). IfpX has full support,
this makes VX into a real inner product space. The same can be done for the variable Y , yielding
the inner product hν, ν0iY for ν, ν0 ∈ VY .
Had we interpreted μ and μ0 as tangent vectors to VX, considered as a manifold, this would be the
Fisher information (Riemannian) metric, as in Beny & Osborne (2015b). But this quantity is also
meaningful for finite vectors: the induced norm distance between pX and any probability vector q is
the χ2-divergence:
χ2(q, pX) = hq -pX,q -pXiX.	(12)
The set of conditional probability distributions pY |X form a stochastic map, i.e., a linear map N :
Vχ → VY, μ → N(μ), where
N (μ)(y) = X PY ∣x (y∖χ)μ(χ)	(13)
x
for any μ ∈ VX.
It is straightforward to check that the stochastic map N * defined by
N *(ν)(x) = X PX|Y (x∖y)ν(x)	(14)
x
is the transpose N* of N with respect to the inner products we defined (Ohya & Petz, 2004), i.e.,
for all V ∈ VY and μ ∈ VX,
hν,Ν(μ)iγ = hN *(ν ),μiχ.	(15)
Also, we observe thatN(pX) = pY andN*(pY) = pX.
B.2	Eigen-relevance decomposition
We can use the inner products on VX and VY to define a singular value decomposition of the stochas-
tic map N. That is, there is an orthonormal family u1, . . . , uk of VX and an orthonormal family
v1 , . . . , vk of VY, such that
N(uj) = ηjvj,	(16)
for j = 1, . . . , k. For each j , ηj is a singular value of N, whose square we call the relevance of
the vector vj. Moreover ηj ∈ [0, 1] since the χ2 divergence is contractive under any stochastic map.
Given that N* is the transpose of N:
N* (vj) = ηjuj .	(17)
Equivalently, uj is an eigenvector of N* ◦ N and vj is an eigenvectors of N ◦ N* , both with
eigenvalue ηj2 .
Because N maps pX to pY, we always have the dual eigenvectors u0 = pX and v0 = pY with
eigenvalue 1.
B.3	Low-rank approximation
Typically, the dimension k of the space of probabilities is more than astronomically large. For
instance, if the values of X consists of small 256 gray level images of 28 × 28 pixels, then k =
13
Under review as a conference paper at ICLR 2020
256282 ' 101888. However, in many case, only very few of these dimensions may be relevant for
the purpose of inferring other variables.
The core of our approach is to approximate N and N* by restricting them to the span of the first ko
eigenvectors uj and vj with largest singular values ηj . That is, if we order the singular values ηj ,
j = 1, . . . , k in decreasing order, we propose to use the approximations
N0(μ) = E ηjhuj,μiχVj	(18)
j≤k0
N0*(ν) = X ηjhvj,νiYuj	(19)
j≤k0
(20)
to N and N* respectively, for some ko typically much smaller than k, and any μ ∈ VX, V ∈ Vγ.
We denote the components ofN0 and N0* by q(y|x) and q(x|y), e.g.,
NO (μ)(y) = X q(yIx)μ(X).	(21)
x
Since N0 and N0* are adjoint, we can define q(x, y) = q(x|y)pY (y) = q(y|x)pX (x). Although
the marginals of q(x, y) are the probability distributions pX and pY, the numbers q(x, y) are not
necessarily positive.
The quality of this approximation for a given k0 does not directly depend on the dimensionality of
X and Y , but only on the amount of correlations between the two variables. Our aim is to use a k0
small enough that the components of N0 and N0* can be computed explicitly.
Theorem 1. N0 is the map of rank k0 which minimizes the average distance
X p(χ)kNo(δx)-N (δx)kY=X(q(x；:/yx,y))2
(22)
Proof. The low rank approximation N0 minimizes the distance kN0 - NkF where
kMk2F = Tr (M*M)	(23)
is the Hilbert-Schmidt (or Frobenius) norm (Eckart & Young, 1936). This follows from the fact
that this is also the l2 -norm of the vector of singular values of M. Let us find the explicit form
of the trace. Each possible value x of the variable X is associated with a probability distribution
δx(y) = 1 when x = y and zero otherwise. These distributions form an orthogonal basis ofVX, and
have norms hδx, δxi = 1/pX (x). Therefore,
Tr(M*M) =XpX(x)hδx,M*M(δx)iY
x
= XpX(x)kM(δx)k2Y
x
□
B.4	Relevant variables
We express the elements μ ∈ Vχ and V ∈ Vγ in terms of the marginals PX and PY as simple
products:
μ(x) = PX (x)f (x) and V (y) = PY (y)g(y)	(24)
for all x, y , where f and g are real functions of x and y.
The inner products then simply become correlations among variables. Using also μ0 = PX f and
V0 = PY g0, we obtain
hμ, μ0iX = X px (X)f (X)f O(X) = Tf,	(25)
x
hν, ν0iγ = X Py (y)g(y)g0(y) = gg0.	(26)
y
14
Under review as a conference paper at ICLR 2020
These are simple expectation values with respect to p, which we assumed is the type of quantity we
can evaluate for arbitrary functions f, f0 , g, g0 .
Since N*N is self-adjoint in terms of this inner product, its eigenvectors Ui are orthogonal, and
hence the corresponding variables ai defined by ui (x) = pX (x)ai (x) are uncorrelated. Indeed,
aiaj = hui,ujiX = 0,	(27)
for all i, j . Moreover, accounting for the eigenvector u0 = pX (corresponding to the constant feature
a0 (x) = 1 for all x),
a = 0	(28)
for all i 6= 0. Hence we trivially have
aiaj = ɑiθj	(29)
for all i, j 6= 0.
Likewise for the eigenvectors of NN*. If vi(y) = PY(y)bi(y):
bibj = hvi,vj iγ = 0 = bibj.	(30)
for all i, j 6= 0.
Importantly, this does not mean that the variables u1, u2,... nor v1, v2,... are “disentangled”, i.e.,
they are not statistically independent. These variables represent components in the space of proba-
bility vectors, rather than the “sample” space. They should be understood as spanning a subspace
of the space of functions over the relevant independent variables. We discuss this in more detail in
Section 5.3.
B.5	CORNERS OFNAND LOSS FUNCTION
The final piece of puzzle We need, is the ability to express the components (corners) of N and N*
in the span of possible non-orthogonal families of variables.
Let us therefore consider two arbitrary families f1,... , fk0 and g1,... , gk0 of variables, which
respectively represent the vectors pX fj ∈ VX and pYgj ∈ VY .
Firstly, we need matrices representing the components of the inner products on VX and VY . Those
are the symmetric matrices
Kij = hPX fi,PX fjiX = fifj,	(31)
Lij = hpY gi,pY gj i Y = gi gj .	(32)
The components Nij of N are defined by
N(pXfj) =	NijpY gi .
i
Taking the inner product with pYgk, we obtain
hpY gk , N(pXfj )i =	Nij Lki .
i
The left-hand side can be computed using Equ. 13. It is the matrix
Akj = hpYgk, N(pXfj)i
Σ
x,y
pY (y)gk(y)pY |X(y|x)pX(x)fj(x)
pY (y)
=£p(X,y)gk (y)fj(X) = gk fj.
x,y
Therefore, in matrix notation, Equ. (34) is A = LN, or
N = L-1A.
(33)
(34)
(35)
(36)
15
Under review as a conference paper at ICLR 2020
The components Nj of N* are obtained by just swapping X and Y, yielding
N * = K-1A>.	(37)
Hence the singular values of the corner ofN defined by the variables fj and gj are just the square-
root of the eigenvalues of the matrix N*N = K-1A>L-1A. In order to find the variables fj and gj
with the same span as the first k0 eigenvectors uj , vj , we just need to maximize all the eigenvalues
of N*N. A simple way to do this is to use (minus) the trace of N*N as loss function, since it is the
sum of the square of the singular values. We call Tr (N* N) the relevance of the subspaces defines
by the variables fj ad gi for all i, j . This yields the loss/cost function:
C = k0 - Tr(N*N) = k0 - Tr(K-1A>L-1A).	(38)
Once optimal variables have been found, one can obtain the components of the eigenvectors in the
span of f1, . . . , fk0 through standard numerical diagonalization of N*N.
B.6	Inference
The variables minimizing C can be used to infer one variable from the other. For instance, given
y, the inferred probability distribution over x is given by pX|Y (x|y) = N * (δy)(x), where δy(y0) is
1 when y = y0 and zero otherwise. In order to compute this, we first need the components of the
distribution δy in terms of the family pYg1, . . . ,pYgk0, i.e., the real numbers (δy)j such that
k0
δy(y0) = pY (y0) X(δy)igi(y0) + r(y0),	(39)
i=1
where hr, pY δiiY = 0 for all i. Taking the inner product with pYgj, we obtain
k0
hpY gj , δyiY =	(δy)iLji ,	(40)
i=1
where the left hand side is also just
hpYgj, δyiY = gj(y).	(41)
Therefore the components of δy are explicitly
(δy)i = X(L-1)ijgj (y).	(42)
j
It follows that
pX|Y (x|y) = N*(δy)(x) ≈ N0*(δy)(x)
= X Nk*i(L-1)ijgj(y)fk(x).	(43)
ijk
Then, for instance, the expected inferred value of X is
X = X Nki(LT rjgj (y) X PX (X)Xfk (X) .	(44)
ij k	x
For the inference of Y from X, we have
pY |X(y|X) ≈	Nki(K-1)ijfj(X)gk(y).	(45)
ijk
16
Under review as a conference paper at ICLR 2020
C Analytical example
When p(x, y) is any multivariate Gaussian distribution, everything can be computed analytically. Let
Us consider here the one-dimensional case. We use p(χ) 8 exp (-χ2∕2τ2), and the conditional
p(y∣χ) H
This gives
exp -(y - x)2∕2σ2 . That is, y is equal to x but with some added Gaussian noise.
pX|Y (x|y) H exp
(X - Yy)2 ∖
2τ 2(1- Y J ,
where
T 2
Y σ2 + τ2 .
(46)
—
It was show in Lancaster (1958), that the most relevant subspace of dimension k0 on the variable X
is simply spanned by the variables
fn(x) = xn,	(47)
n = 0, . . . , k0 - 1. Similarly for Y ;
gn(y) = yn.	(48)
This independence of the relevant variables on the detailed parameters of p is a general property of
Gaussian joint distributions.
This means, for instance, that the most relevant feature (n = 1) for predicting the value of X given
Y = y is simply Y itself. The higher order variables have to do with inferring extra aspects of the
probability distribution over X .
A set of orthogonal variables can be obtain from the Gram-Schmidt procedure, which, if done from
small to large n much necessarily yield the eigenvectors un and vn . For illustration purpose, let us
work with the non-orthogonal vectors fn and gn , keeping only the first k0 = 3 vectors.
The three matrices (correlators) we need can be easily computed:
(1	0 τ2 ∖	(	1	0	τ2 + σ2 ∖
K =	0	τ2	0	L =	0	τ2 + σ2	0	(49)
τ2	0	3τ4	τ2+σ2	0	3(τ2+σ2)2
(	1	0	τ2	∖
A =	0 τ2	0	.	(50)
τ2+σ2	0 τ2(σ2+3τ2)
We obtain
(1 0 T 2(1-γ2)∖
M = K-1A>L-1A =	0 Y 0	.	(51)
0 0	Y2
The eigenvalues of M can be read on the diagonal, and the corresponding eigenvectors are (1, 0, 0),
(0, 1, 0) and (-τ2, 0, 1), which means that the eigenfunctions are in order u0(x) = 1, u1(x) = x
and u2 (x) = x2 - τ 2 .
Because we are working with continuous variables, the true rank of N is infinite, even for any finite
cutoff on the singular values. Nevertheless, it is instructive to see how the approximate inference
fares for rank k0 = 3. Given the value y for Y , the inferred distribution over X is
2
No (δy)(X) = PX(x)PY(y) X (KTATL-Dkjyjxk.	(52)
j,k=0
The approximately inferred first and second moments of X is given by integrating the above times
X (resp. X2) over X. We obtain
x = Yy and x2 = γ2y2 + (1 — Y)T2,	(53)
which are actually exact: they are equal to the first two moments ofX overpX|Y as given in Eq. (46).
In fact, it is easy to see that this would be true for the first ko - 1 moments had we kept the ko most
relevant variables.
17