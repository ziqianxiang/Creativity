Under review as a conference paper at ICLR 2020
The Dual Information B ottleneck
Anonymous authors
Paper under double-blind review
Ab stract
The Information Bottleneck (IB) framework is a general characterization of op-
timal representations in large scale learning that suggests an intriguing interpre-
tation of efficient representations in deep learning in particular. It is based on
the optimal trade off between the representation complexity and accuracy, both of
which are quantified by mutual information. The problem is solved by alternat-
ing projections between the encoder and decoder of the representation, which can
be performed locally at each representation level. The non-parametric IB frame-
work, however, has several drawbacks: (i) the difficult estimation of information
measures in high dimensions; (ii) the inability to extend the encoder beyond the
training data - the need to to know the full joint distribution of the inputs and
labels; (iii) the IB does not necessarily optimize the actual prediction of unseen
labels, which can be critical when training the encoder from finite samples. Here
we present a new framework, the Dual Information Bottleneck (dualIB), which
improves all the above drawbacks. By switching the order in the KL-divergence
between the representation decoder and data, formally the dual distortion, the op-
timal decoder becomes the geometric rather than the arithmetic mean of the in-
put points. This conversion preserves exponential forms of the original distribu-
tion and consequently conserves the original features of the data. We show how
to solve this new representation learning formulation and illustrate it’s favorable
properties over the original IB when trained from a small sample. We also analyze
the critical points of the dualIB and discuss their importance for the quality of this
approach.
1	Introduction
1.1	The Information Bottleneck method
The Information Bottleneck (IB) method (Tishby et al., 1999), is an information-theoretic frame-
work for describing efficient representations of a large scale “input” random variable X (input pat-
terns), for predicting an “output” variable Y (desired label). In this setting the joint distribution of
X and Y, p(x, y) defines the problem, or rule, and the training data are a finite sample from this
distribution. In general, we assume that p(y | x) is strictly stochastic, and hence bounded away from
{0,1}1. The representation variable X is in general a stochastic function of X which forms a Markov
chain Y → X → X, and only depends on Y through the input X. We call the map p(x | x) the en-
coder of the representation and denote by p(y | x) the Bayes optimal decoder for this representation;
i.e., the best possible prediction of the desired label Y from the representation X.
The IB has direct successful applications for representation learning in various domains, from vision
and speech processing (Ma et al., 2019), to neuroscience (Schneidman et al., 2001), and Natural
Language Processing (Li & Eisner, 2019). Due to the notorious difficulty in estimating mutual
information in high dimension, variational approximations to the IB have been suggested and applied
also to Deep Learning (Alemi et al., 2016; Parbhoo et al., 2018).
For large scale learning, when almost all input patterns can be considered typical in the information
theoretic sense (Cover & Thomas, 2006), the cardinality of the typical inputs is characterized by
the pattern’s entropy, H(X) = - Px p(x) log p(x). Under similar typicality assumption, the mutual
1This may seem like a limitation of the method, but it can be shown that in the deterministic limit the IB is
well defined. This stochastic assumption simplifies the analysis but does not pose any real limitation.
1
Under review as a conference paper at ICLR 2020
information (MI) of the encoder, I(X; X) = Pxx p(x, ^) log Pp(XIx) characterizes the cardinality of
the typical representations, X. On the other hand, the mutual information of the Bayes optimal
decoder, I (Y; X) = Pyx p(y, x) log pp(yy)), is equivalent to the cross-entropy generalization error.
The IB trade off between the encoder and decoder mutual information values is defined by the
minimization of the Lagrangian:
--	,.. 、 ,.. , _ , ʌ , _ _ , ʌ .
F[pβ(X I x);Pβ(x);Pβ(y I x)] = i(x;X)- βι(Y; x) ,	(1)
independently over the convex sets of the normalized distributions, {pβ(^ ∣ x)}, {pβ(x)} and
{pβ(y ∣ x)}, given a positive Lagrange multiplier β. As shown in (Tishby et al., 1999; Shamir et al.,
2010), this is a natural generalization of the classical concept of Minimal Sufficient Statistics (Cover
& Thomas, 2006), where the estimated parameter is replaced by the output variable Y and exact
statistical sufficiency is characterized by the mutual information equality: I(X; Y) = I(X; Y). The
minimality of the statistics is captured by the minimization of I(X; X), due to the Data Processing
Inequality (DPI). However, non-trivial minimal sufficient statistics only exist for very special para-
metric distributions known as exponential families (Brown, 1986). Thus in general, the IB relaxes
the minimal sufficiency problem to a continuous family of representations X which are characterized
by the trade off between compression, I(X; X) ≡ Iχ, and accuracy, I(Y; X) ≡ Iy, along a convex
line in the Information-Plane (IY vs. IX). When the rule p(x, y) is strictly stochastic, the convex op-
timal line is smooth and each point along the line is uniquely characterized by the value ofβ. We can
then consider the optimal representations X = x(β) as encoder-decoder pairs: (pβ(x∣x),pβ(y∣x))2 -
a point in the continuous manifold defined by the Cartesian product of these distribution simplexes.
We also consider a small variation of these representations, δ^, as an infinitesimal change in this
(encoder-decoder) continuous manifold (not necessarily on the optimal line(s)).
1.2	IB and Rate-Distortion Theory
The IB optimization trade off can be considered as a generalized rate-distortion problem (Cover
& Thomas, 2006) with the distortion function between a data point, X and a representation point ^
taken as the KL-divergence between their predictions of the desired label y:
diB(x,支)=D[p(y | X)IIpe(y向] = Xp(y | X)Iog p(y l∣? .	(2)
y	Pe (y |x)
The expected distortion Epβ(x,x) [diB(x, ^)] for the optimal decoder is simply the label-information
loss: I(X; Y) - I(X; Y), using the Markov chain condition. Thus minimizing the expected IB dis-
tortion is equivalent to maximizing I(X; Y), or minimizing equation 1. Minimizing this distortion is
equivalent to minimizing the log-loss or the cross-entropy loss, as done in most deep learning appli-
cations, and it upper-bounds other loss functions such as the L1-loss (due to the Pinsker inequality,
or (Painsky & Wornell, 2018)). The Pinsker inequality shows that both orders of the cross-entropy
upper bound to the Lι-loss, or total-variation distance, min{D[q∣∣p],D[p∣∣q]} ≥ 23Og2 Ilp - q∣∣2 .
1.3	The IB Equations
For discrete X and Y, a necessary condition for the IB (local) minimization is given by the three
self-consistent equations for the optimal encoder-decoder pairs, known as the IB equations:
((i)	pe(^ I x) = Zpex⅛e-eD[ρ(y∣x)kρβ(ylx)]
((ii)	pe(x) = Pxpe(x i X)P(X)	，	⑶
I (iii)	Pe(y i x) = PxP(y i X)Pe (x i X)
where Z(x; β) is the normalization function. Iterating these equations is a generalized, Blahut-
Arimoto, alternating projection algorithm (Tusnady & Csiszar, 1984; Cover & Thomas, 2006) and
it converges to a stationary point of the Lagrangian, equation 1 (Tishby et al., 1999). Notice that the
minimizing decoder, (equation 3-(iii)), is precisely the Bayes optimal decoder for the representation
^(β), given the Markov chain conditions.
2Here we use the inverse encoder, which is in the fixed dimension simplex of distributions over X.
2
Under review as a conference paper at ICLR 2020
1.4	Critical points and critical slowing down
One of the most interesting aspects of the IB equations is the existence of critical points along the
optimal line of solutions in the Information-Plane (i.e. the information curve). At these points the
representations change topology and cardinality (number of clusters) (Zaslavsky & Tishby, 2019;
Parker et al., 2003) and they form the skeleton of the information curve and representation space.
Under the strict stochastic assumption, the information-curve is a smooth function of the Lagrange
multiplier β, but its derivative may not be smooth. Critical points are bifurcations of the solutions,
which are values of β for which two different solutions (representations) co-exist. To identify such
points we perform a perturbation analysis of the IB equations, as in (Zaslavsky & Tishby, 2019).
Taking a small perturbation of the representation, denoted for brevity by δ^, the changes in the
log encoder and log decoder that satisfy equation 3 for a given β can be determined through the
nonlinear eigenvalues problems3:
[I - βcXBo (^, β)]d log PdxX0® =0,	[I - βcyB0 (^, β)] R匕区 = 0,	(4)
with the two square matrices C defined by:
CXBo氏⑶=Xp(y	|	x)pβ((x	||	X)p(y|	XO),	CyB(X,Ie)	= Xp(y	|	X)Pe(X	|	x)p(y0	| X).⑸
y	pβ(y | x)	x	pβ(y | x)
As shown in (Zaslavsky & Tishby, 2019), these two matrices have the same eigenvalues and have
non-trivial eigenvectors (i.e., different co-eXisting optimal representations) precisely at the critical
values of β, the bifurcation points of the IB solution. At these points the cardinality of the repre-
sentation X (the number of “IB-clusters")changes due to splits of clusters, resulting in topological
phase transitions in the encoder. These critical points form the “skeleton” of the topology of the
optimal representations. Between critical points the optimal representations change continuously
(with β).
The important computational consequence of critical points is known as critical slowing down
(Tredicce et al., 2004). For binary Y, near a critical point the convergence time, τβ, of the iter-
ations of equation 3 scales like: τβ 〜 1/(1 - βλ2), where λ2 is the second eigenvalue of either
CyB or CXB. At criticality, λ2(^) = β-1 and the number of iterations diverges. This phenomenon
dominates any local minimization of equation 3 which is based on alternate encoder-decoder opti-
mization.
To illustrate the importance of the critical points and the critical slowing-down we consider a low-
dimensional problem that is easy to visualize. Taking the joint distribution, P(y, X) with a binary Y
and only 5 possible inputs. For a given β, the Blahut-Arimoto iterations converge to a stationary
solutions of the IB equations, which amounts to discrete points in the interval [0, 1] - the simpleX of
the P(y) distributions - for each value of the Lagrange multiplier, β .
The evolution of the optimal decoder, pβ(y = 0 | X), ∀X ∈ X, as a function of β, is a bifurcation
diagram (Figure 1(a)). The critical points of the IB equations correspond to bifurcations in the
decoders’ graph. At these critical points the non-trivial eigenvalues of CyIyB0 (β) cross the β-1 value
(Figure 1(b)), and the number of iterations diverges (critical slowing down) (Figure 1(c)).
1.5	Drawbacks of the IB
The main theoretical and practical drawback of the IB is it’s apparent reliance on the eXplicit knowl-
edge of the joint distribution P(X, y). In the conteXt of representation learning this is often considered
a major flaw, as we are always given only a sample from the distribution. While the generalization
properties of the finite sample IB representations have been studied in the past (Shamir et al., 2010),
the main problem remains the eXtension of the encoder, p(X | x), when trained from a small sample,
to new unobserved patterns X ∈ X. The standard approach in representation learning is to assume
(at least implicitly) the eXistence of features of X with their topology, or a specific parametric form
for the encoder. The original IB is completely non-parametric and does not assume any topology on
the input space, beyond label similarity. A related drawback of the IB is it’s poor scalability with the
size of the pattern-space X, which makes the reliable estimation of the encoder mutual information
impractical for large learning problems.
3We ignore here the interactions between the representations, for simplicity.
3
Under review as a conference paper at ICLR 2020
Figure 1: Example of the evolution of the IB decoder with growing β. (a) The bifurcation diagram;
each color corresponds to one component of the representation X ∈ X and depicts the decoder
Pβ(y = 0 | ^), inthesimplex [0,1]. The black dots correspond to the input distributionp(y = 0 | x).
(b) The second eigenvalue of Cyy，(^; β), λ2 (^), along with βT as a function of β. (C) Convergence
time of the algorithm as a function of β .
1.6	Contributions of this work
Our main contribution is by providing a general formal framework resolving the encoder exten-
sion drawback of the IB. While the use of specific features and variational approximations to the
IB are not new, here we take a different approach assuming that p(y | x) can be approximated by
exponential families. This defines a new rate-distortion problem, the Dual Infromation Bottleneck
(dualIB).
We show that the new dualIB formulation preserves the sufficient statistics of the original distribution
for all values of the trade off parameter, β . A desired property upon predictions of a parametrized
distribution. Furthermore, it obtains a significant improvement of the finite sample label prediction.
We demonstrate numerically that the dualIB encoder-decoder pairs improve the prediction of unseen
labels when trained from small samples, compared to the original IB. We also discuss in detail the
critical points of the new formulation and their relation to those of the original IB. Interestingly,
as we elaborate in the following sections, all these desirable properties are achieved by merely
switching the order of the arguments in the KL-divergence of the IB distortion.
The remainder of the paper is organized as follows. We present and solve the dualIB problem
in §2. We discuss the dualIB’s critical points in §2.1 and prove a general relation between the
critical points of the dual and original IB in §2.2. In particular, we show that the dual critical points
give the best approximation to the original IB information curve. In §3 we discuss the dualIB for
exponential families, dualExpIB, and demonstrate the advantages of our framework in this case. We
also demonstrate numerically the improvement in the label prediction of the dualIB when trained
from a small sample. We conclude in §4 with further extensions and possible applications.
2	The dualIB: Maximizing the Prediction Information
Supervised learning is generally separated into two phases: the training phase, where the internal
representations are formed from the training data, and the prediction phase, where these representa-
tions are used to predict labels of new input patterns. We add to our Markov chain another variable,
Y, the predicted label which obtains the same values as Y, but distributed differently:
training
Z /、
ʌ ʌ
Y → X → X β → Y .
、------{------}
prediction
(6)
The left-hand part of this chain describes the representation training, and the right-hand part is the
Maximum Likelihood (ML) prediction using these representations (Slonim et al., 2006). So far
the prediction variable Y has not been part of the IB optimization problem. It has been implicitly
assumed that the Bayes optimal decoder, Pe(y | X), which minimizes the IB distortion at a given β,
4
Under review as a conference paper at ICLR 2020
♦	1	.1 1	. 1 ∙	Γ∙	1 ∙	f	i~ 't'τ Γ∙	. 1	.	√V .1	1 .1	∙ 1 .
is also the best choice for making predictions of Y from the representation Xβ through the right-
hand Markov chain. That is, denoting pβ(^ | X) ≡ pβ(y | ^) the ML decoder is the mixture over the
internal representations:
Pe(y | x) ≡ XPe(y | x)Pβ便 | x).	⑺
X
This, however, is not necessarily optimal when the encoder and decoder are trained from finite
samples (Shamir et al., 2010), or when the decoder is not Bayes optimal. We define the dualIB
distortion by merely switching the order of the arguments in the KL-divergence of the original IB
distortion, namely:
dduaiiB(x,X) = D[pe(y | X)l∣p(y | x)] = XPe(y | RiogPe(y∣l X) ,	(8)
y	P(y | x)
which in geometric terms is known as the dual distortion problem (Felice & Ay, 2019).
That is, given the “input” random variable X (input patterns), the ”output” random variable Y (de-
sired label) and the “representation” X, the ML optimal “predicted label” Y is given by the mixture
distribution, equation 7.4
The dualIB optimization can be written as the following rate-distortion problem:
_____ ,	.	.	,	.	.	,..、r	_	,	ʌ ,	__ --	,	—一
F [pe(X | x);Pe(x);Pe(y | x)] = I(X; X)+ βEp(x,X)[dduaiiB(x, x)] ,	(9)
with the average distortion given in terms of mutual information on Y, I(X; Y) and I(X; Y):
Ep(x,x)[dduaiiB(x,x)] = I(X; Y)-1(X; Y) + Ep(x)[D[Pe(^ | x)kP(y | x)]]
'-------------------------------{z-------} '-------------{-----------}
(a)	(b)
_ , ʌ ʌ , _ , ʌ ,
≥ I(X; Y) -1(X; Y).	(10)
This is similar to the known IB relation: Ep(x,x)[diB(x, ^)] = I(Y; X) - I(Y; X) with an extra
positive term (b).
Both terms, (a) and (b), vanish precisely when X is a sufficient statistic for X with respect to Y,
since We can then reverse the order of X and X in the Markov chain (equation 6). This replaces
the roles of Y and Y as the variable for which Xe are approximately minimally sufficient statistics.
In that sense the dualIB shifts the emphasis from the training phase to the prediction phase. This
implies that minimizing the dualIB functional maximizes the mutual information between Y and Y,
I(Y; Y), as well as the mutual information I(X; Y). This is illustrated in figure 4(b)-c.
The next theorem states the form of the solutions of the Dual Information Bottleneck:
Theorem 1. The minima of equation 9, can be obtained by generalized Blahut-Arimoto iterations
between the encoder and the decoder as in the original IB, with the following modifications: (i)
Replace the distortion by its dual in the encoder update; (ii) Update the decoder by the encoder’s
”geometric” mean of the data distributions P(y | x).
The proof is given in §A.2.
The alternating projections between the encoder and decoder, which converge to a solution of the
dualIB at a given value of the Lagrange multiplier β, are implemented by the following iterative
algorithm5:
4We use the notation p(y | x) ≡ p(y | x), when there is no β subscript.
5Unless stated otherwise, log = ln ≡ loge
5
Under review as a conference paper at ICLR 2020
Algorithm 1 dualIB iterative algorithm
1:	while Ipe+1(y | X)- Pte (y | 川 > e do
2:	ZX+1(x; β) = PxPe (X)e-βD[pβ (y|X)kp(y|x)]
3:	pt+1(^ | x) = -t+β(x)-e-eD[pβ (yIx)kp(y|X)]
Z	z^∣χ (x；e)
4：	pβ+1(X) = Pxpβ+1(X | χ)p(x)
5:	Zyt+χ1(X; β) = Py epχpβ+1 (XIX) logp(y|X)
6：	pte+1(y I X)= zy⅛)epXpβ+1(XIx)logp(y|X)
7: returnPe(X),pe(X I X),Pe(y I .
Here the Z’s are standard normalization factors (partition functions).
As in the IB, the encoder update (row 3) and decoder update (row 6) are the core of the algorithm.
Figure 2 illustrates the behavior of the dualIB algorithm for the same distribution as in Figure 1. The
overall behavior is very similar, but the critical points (representation splits) slightly change their
location. In the following sections we provide a detailed analysis and comparison between the two
frameworks.
Figure 2: Same as Figure 1 for the dualIB solutions with the Cydyu0alIB matriX. (a) The representations
bifurcate at the critical values of β. (b) Critical points appear when an eigenvalue (color) crosses the
β-1 (black) line. (c) Near these points there is critical slowing down and the numbers of iterations
of Algorithm 1 diverge.
2.1 The Critical Points of the dualIB
As discussed in §1.4 the skeleton of the IB optimal bound (the information curve) is constituted
by the critical points in which the topology (cardinality) of the representation changes. A similar
stability analysis of the dualIB equations reveals similar conditions for the critical points.
Theorem 2. The dualIB critical points are detected by non-trivial solutions of the nonlinear eigen-
value problem:
[I-βcd0alIB(X,β)]alogPexX0| 刈=0,	[I-βcdUalIB(X,β)]SlogPexy0 | 月=o,	(11)
with the matrices C dualIB given by:
CxxOalIB 区 β) = X Pe (y i X)Pe(x i x) log p(y I x)
y,y,x	P'y I )
CyyualIB (X; β) = X Pe (x I X)Pe (y I ^)log Pl^
x,x,y
• Pe(x01 X)Pe(y I ©log
• Pe(y01 X)pe(x I ©log
p(y i x0)
p(y i x0)
PyJX)	(12)
P(y0 I x).	( )
The proof to theorem 2 is given in §A.3.
6
Under review as a conference paper at ICLR 2020
Lemma 3. The matrices CdUalIB(X; β), Cyy0allB(x; β) have the same eigenvalues {λi}, with
λι(^) = 0. With binary Y, the critical points are obtained at λ2(^) = β-1.
The proof of lemma 3 given in section §A.3.1.
As in the IB, at the critical points, βcdualIB , the partial derivatives of the encoder and decoder with
respect to β, ∂ logpβ(X | ^)∕∂β, ∂ logpβ(y | x)∕∂β, have multiple (at least two) values. This re-
sults in discontinuities (cusps) in the encoder and decoder mutual information values as functions of
β along the optimal line, with an undefined second derivative.
2.2 The Information Plane of the dualIB
The Information Plane, IX = I(X; X) vs. IY = I(X; Y), is the standard visualization of the
compression-prediction trade off of the IB. It can be defined for any encoder once the decoder is
the Bayes optimal (equation 3-(iii)), for which the IY is the actual information of the representation
on the desired label (Tishby et al., 1999).
Comparing the dualIB information curve to the IB curve shows the quality of this approXimation.
Figure 3(a) depicts this comparison. While we know thatIYIB(β) is always higher, the two curves are
almost indistinguishable. To better understand the relationship between these two curves, we look
at the values of IX and IY as functions of the corresponding β (Figure 3(b),(c)). The important role
of the critical points is revealed as the corresponding cusps along these curves. As we argue below,
the IB information values are strictly below those of the dualIB, but the distance between them is
minimized precisely at the dual critical points.
8 6 4 2 0
. . ■ . .
Ooooo
(X - Xw(X- X)7
•	IB, diB
d	dualIB, diB
•	dualIB, ddualIB
(X)H/(X W~
P(X,X) %β(x,X)
1.0
0.8
0.0
dualIB
βCB
0dualIB
IB
8 6 4 2 0
■ . ■ . .
Ooooo
(X)HKX -A)I
(X)H/(X -X)I
4
.
O
0.00 0.25	0.50	0.75	1.00	0	2	4	6	0	2	4	6
(a) I(X; X)∕H(X)	(b) log2(β)	(C) log2(Ie)
Figure 3: The IB’s and dualIB’s Information Plane. (a) IY vs. IX for the two algorithms. The
black dots are the dualIB critical points, βcdualIB , and the grey triangles are the IB critical points,
βcIB. The corresponding distortion functions are shown in the inset. (b) The functions IXIB (β) and
IXdualIB(β). Both curves are monotonic and concave between the critical points. The inset indicates
the relative difference between the curves, where the alternating order of the critical points is clearly
observed. (c) Similarly, IYIB (β) and IYdualIB (β ) are monotonic and piece-wise concave. The relative
discrepancy between the information curves is clearly minimized at the dualIB critical points (inset).
The functions approach each other for large β .
Lemma 4. IX(β) and IY(β), along the optimal lines, are non-decreasing piece-wise concave, func-
tions of β. When their second derivative (with respect to β) is defined, it is strictly negative.
Lemma 5. For any sub-optimal information curve (IX,IY), IXIB(β) ≤ IX(β) and IYIB(β) ≤ IY(β),
for all values of β.
Proofs of the above are given in §A.4.
The information plane properties are summarized by the following theorem.
Theorem 6. (i) The critical points of the two algorithms alternate: for each critical point, βcdualIB ≤
βcIB. (ii) The distance between the two information curves is minimized precisely at the dualIB
critical points βcdualIB. (iii) The two curves approach each other as β → ∞.
Proof. The proof follows from lemmas 4 and 5, together with the critical points analysis above,
and is only sketched here. As the encoder and decoder at the critical points, βcIB and βcdualIB, have
7
Under review as a conference paper at ICLR 2020
different left and right derivatives, they form cusps in the curves of the mutual information (IX and
IY) as functions of β . These cusps can only be consistent with the optimality of the IB curves if
βcdualIB < βcIB (this is true for any sub-optimal distortion), otherwise the curves intersect.
Moreover, at the dualIB critical points, the distance between the curves is minimized due to the
strict concavity of the functions segments between the critical points. As the critical points imply
discontinuity in the derivative, this results in a ”jump” in the information values. Therefore, at
any βcdualIB the distance between the curves has a (local) minimum. This is depicted in Figure 3,
comparing IX(β) and IY(β) and their differences for the two algorithms.
The two curves approach each other for large β since the two distortion functions become close in
the low distortion limit (as long as p(y | x) is bounded away from 0).	□
3	The Exponential Family dualIB
As discussed in §1, one of the major drawbacks of the IB is the extension of the decoder inputs
from an empirical sample to new unseen patterns. To address this issue we can assume the exis-
tence of features, or statistics of the pattern, Ar(x), which are sufficient or approximately sufficient
for the prediction of the label Y. That is, we consider the rule’s distribution, p(y | x), to be an
exponential family with these features. Exponential families form the elegant theoretical core of
parametric statistics and often emerge as maximum entropy (Jaynes, 1957) or stochastic equilibrium
distributions, subject to observed expectation constraints. They also form the class of parametric
distributions for which exact, finite dimensional and additive, Minimal Sufficient Statistics exist
(Kullback, 1959). Such distributions should also best match the rational behind the IB as extracting
approximate sufficient statistics.
A key property of exponential families is their invariance under geometric averages. For this tech-
nical reason the dualIB encoder and decoder preserve the exponential form of the original data
distribution for all values of β . Furthermore, the new (assumed limited to a finite dimension d) set
of features/statistics are precisely the empirical expectations of these features. These d empirical
averages are the only numbers needed for predicting the label at the required accuracy, hence their
advantage in reducing the encoder’s computational complexity.
Assuming that the rule distribution is of the form:
d
p(y | X) = e-Pd=0 λr(y)Ar(x) = e-λ(y)∙ A(x)-log Zy∣χ(x) = Y e-λr(y)A,(x) ,	(13)
r=0
where Ar(x) are d linearly independent functions of the input x and λr(y) are functions of the label
y, or the parameters of this exponential family. The λ(y) can also be considered Lagrange multipli-
ers associated with the constraints conditional expectations Ep(x|y) [Ar(x)] in entropy maximization.
The normalization factors, Zy|x(x), are written, for brevity, as λ0x ≡ log(Py Qrd=1 e-λr(y)Ar(x))
with A0(x) ≡ 1. We do not constrain the marginal p(x).
The important fact about the exponential form is that all the mutual information, I(X; Y), is fully
captured by the d conditional expectations, Ep(x|y) [Ar(x)], since these are the (minimal) sufficient
statistics for the parameters. This means that all the relevant information (in the training sample)
is captured solely by d-dimensional empirical expectations. This can lead to a huge reduction in
computational complexity (from dim(X) to d).
We next show that for the dualIB, this dimension reduction is preserved or improved along the dual
information curve, for all values of β .
Theorem 7. (dualExpIB) For data from an exponential family, equation 13, the optimal decoder of
the dualIB, at a given β, is given by:
Pβ(y | X)= e-Pd=ι λr(y)Ar,β(X)-λβ(X) , λβ(X)= log(X e- Pd=ι λr(X)) ,	(14)
y
and the respective encoder:
8
Under review as a conference paper at ICLR 2020
Pe(X | X)
Pe (x)e-βλβ (X) e-β Pd=ι λ(X)[Ar (X)-Ar,β(X)]
Zx|x(x; β)
with the constraints and multipliers expectations,
Ar,e(X) ≡ EPe(x | X)Ar(X), λβ⑺ ≡ EPe(y | x)∙(y), 1 ≤ r ≤ d.
(15)
(16)
X
y
The complete derivations for this section are given in §A.5.
This defines a simplified iterative algorithm to solve the dualExpIB problem, since we can replace
the decoders’ update rule at each iteration in the dualIB algorithm (Algorithm 1, row 6) with the
simplified eXpression given in equation 14.
As a concrete illustration to the performance of the dualEXpIB we eXamine a problem in which
the rule distribution is given by an eXponential family, p(y | x) 〜 N(0,a + b∣sin(c∏X)∣), as in
Globerson & Tishby (2003). Setting the constants a, b, c we produce a sample S (of size N =
1000) on which We calculate the empirical distribution, p(y | x) and the empirical sufficient statistics
Ar (X). Given the assumption on the rule we assess the eXponential form empirical distribution,
Pexp(y | x; Ar (x)). We then evaluate the IB on both empirical distributions, P andPexp, and compare
it to the dualExpIB given Ar (x), λr (y)6.
The comparison between the IB and the dualEXpIB, is given in Figure 4(a) as function of the con-
tinuous ^ for a fixed X. It can be seen that the dualExpIB better captures the structure of the original
data and is much less affected by the sampling noise. NeXt, we consider the Information Plane
with respect to Y, where the eXpected maXimization by the dualEXpIB is observed, Figure 4(b). To
evaluate the prediction ability of the dualEXpIB we consider the L1 -expected loss with respect to
the original distribution. Looking at Figure 4(c) it is apparent that for all values of the trade off
parameter, β, the dualEXpIB minimizes the loss providing the superiority in prediction.
⑷ y	(b) I(X； X)∕H(X)
Figure 4: The IB,s and dualExpIB,s behavior over data sampled from P(y | x)	〜
N(0, a + b| sin(cπx)∣) andP(X)〜U(-1, 1). (a) The decoder Pe(y | ^) as a function of y for some
X ∈ ( — 1, 1) at log2(β) = 4. (b) The Information Plane with respect to Y, I(Y; Y) vs. I(X; X). (c)
The Li- expected loss, Ep(X) [∣∣P(y | x) - Pe (^ | x)k], as a function of the trade off parameter β.
4 Conclusion
We presented here the Dual Information Bottleneck (dualIB), a framework to preform the Informa-
tion Bottleneck (IB) compression in a parametric setting. We have shown that the dualIB has several
interesting properties: (i) it provides a good approximation to the original IB while using the low
relevant dimensions of the original data. This can significantly reduce the complexity of finding
good IB representations; (ii) it optimizes the information between the representation and the pre-
dicted label rather than the desired label as in the original IB. As we have shown, this can improve
6λr (y) are defined by the specific exponential family.
9
Under review as a conference paper at ICLR 2020
the generalization error when trained on small samples since the predicted label is the one used in
practice; (iii) It preserves the exponential form of data from exponential families, while reducing the
dimensionality of the compressed representations. This important property was known to be satis-
fied by the Gaussian IB (Chechik et al., 2005) but not known for other distributions. The Gaussian
case is self-dual in that sense. Generalizing this property to other exponential families was an open
problem for many years; (iv) The exponential form of the optimal encoder-decoder pairs allows for
the application to distributions with special symmetries, which can be naturally expressed in this
form.
We conclude that the exponential Dual Information Bottleneck (dualExpIB) framework allows us
now to approach problems previously considered intractable by the IB. Furthermore, it may allow
better predictions to IB applications using finite samples or data with low internal dimensionality or
special symmetries.
10
Under review as a conference paper at ICLR 2020
References
Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information
bottleneck. ArXiv, abs/1612.00410, 2016.
Lawrence D. Brown. Fundamentals of statistical exponential families with applications in statistical
decision theory. Lecture Notes-Monograph Series, 9:i-279,1986. ISSN 07492170. URL http:
//www.jstor.org/stable/4355554.
Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss. Information bottleneck for gaussian
variables. J. Mach. Learn. Res., 6:165-188, December 2005. ISSN 1532-4435. URL http:
//dl.acm.org/citation.cfm?id=1046920.1046926.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecom-
munications and Signal Processing). Wiley-Interscience, New York, NY, USA, 2006. ISBN
0471241954.
Domenico Felice and Nihat Ay. Divergence functions in information geometry. In Frank Nielsen
and Frederic Barbaresco (eds.), Geometric Science ofInformation - 4th International Conference,
GSI 2019, Toulouse, France, August 27-29, 2019, Proceedings, volume 11712 of Lecture Notes
in Computer Science, pp. 433⑷2. Springer, 2019. ISBN 978-3-030-26979-1. doi: 10.1007/
978-3-030-26980-7∖,45. URL https://doi.org/10.1007/978-3-030-26980-7_
45.
Ran Gilad-bachrach, Amir Navot, and Naftali Tishby. An information theoretic tradeoff between
complexity and accuracy. In In Proceedings ofthe COLT, pp. 595-609. Springer, 2003.
Amir Globerson and Naftali Tishby. Sufficient dimensionality reduction. Journal of Machine Learn-
ingResearch, 3(Mar):1307-1331, 2003.
E. T. Jaynes. Information theory and statistical mechanics. Phys. Rev., 106:620-630, May 1957. doi:
10.1103/PhysRev.106.620. URL https://link.aps.org/doi/10.1103/PhysRev.
106.620.
S. Kullback. Information Theory and Statistics. Wiley, New York, 1959.
Xiang Lisa Li and Jason Eisner. Specializing word embeddings (for parsing) by information
bottleneck. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-
guage Processing and 9th International Joint Conference on Natural Language Processing, pp.
2744-2754, Hong Kong, November 2019. URL http://cs.jhu.edu/- jason/papers/
#li-eisner-2019. Best Paper Award.
Shuang Ma, Daniel McDuff, and Yale Song. Unpaired image-to-speech synthesis with multimodal
information bottleneck. In Proceedings of the IEEE International Conference on Computer Vi-
sion,pp. 7598-7607, 2019.
Amichai Painsky and Gregory W. Wornell. Bregman Divergence Bounds and the Universality of the
Logarithmic Loss. arXiv e-prints, art. arXiv:1810.07014, Oct 2018.
Sonali Parbhoo, Mario Wieser, and Volker Roth. Causal deep information bottleneck. ArXiv,
abs/1807.02326, 2018.
Albert E. Parker, Tomas Gedeon, and Alexander G. Dimitrov. Annealing and the rate distortion
problem. In S. Becker, S. Thrun, and K. Obermayer (eds.), Advances in Neural Information
Processing Systems 15, pp. 993-976. MIT Press, 2003. URL http://papers.nips.cc/
paper/2264- annealing- and- the- rate- distortion- problem.pdf.
Elad Schneidman, Noam Slonim, Naftali Tishby, R deRuyter van Steveninck, and William Bialek.
Analyzing neural codes using the information bottleneck method. Advances in Neural Informa-
tion Processing Systems, NIPS, 2001.
Ohad Shamir, Sivan Sabato, and Naftali Tishby. Learning and generalization with the information
bottleneck. Theor ComPut Sci., 411:2696-2711, 2010.
11
Under review as a conference paper at ICLR 2020
Noam Slonim, Nir Friedman, and Naftali Tishby. Multivariate information bottleneck. Neural
Computation, 18(8):1739-1789, 2006. doi: 10.1162∕neco.2006.18.8.1739. URL https://
doi.org/10.1162/neco.2006.18.8.1739. PMID: 16771652.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. In
Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing, pp.
368-377, 1999.
J. R. Tredicce, G. L. Lippi, Paul Mandel, B. Charasse, A. Chevalier, and B. Picqu. Critical slowing
down at a bifurcation. American Journal of Physics, 72(6):799-809, 2004. doi: 10.1119/1.
1688783. URL https://doi.org/10.1119/1.1688783.
G. Tusnady and I. Csiszar. Information geometry and alternating minimization procedures. Statistics
& Decisions: Supplement Issues, 1:205-237, 1984.
Noga Zaslavsky and Naftali Tishby. Deterministic annealing and the evolution of optimal informa-
tion bottleneck representations. Preprint, 2019.
12
Under review as a conference paper at ICLR 2020
A Appendix
A. 1 dualIB Mathematical formulation
As presented in §1.6 the dualIB is solved with respect to the full Markov chain (equation 6) in which
we introduce the new variable, Y, the predicted label. Thus, in analogy to the IB we want to write
the optimization problem in term of Y.
Developing the expected distortion we find:
EPβ(x,X)[ddualIB(X,钏=XPβ (X, X) X Pβ Cy | 攵乂寒蔡；|黑
=X pβ ⑺pβ (^∣χ)log Tf -Xp(X)Pe (y∣χ)log K
+X PgPe(y | χ)log ρp((y∣lχ))
,ʌ ʌ . ʌ . -- .............................
=I(X; Y)-1(X; Y) + Ep(X)D[Pβ(^ I χ)kp(^ I X)]].
Allowing the dual optimization problem to be written as:
F"p(X | χ);p(X);p(y | X)]= I(X;X) - β{l(X; Y)- I(X; Y)- Ep(XM[pe(^ | χ)kp(y | χ)]]}.
A.2 The DualIB solutions
To prove theorem 1 we want to obtain the normalized distributions minimizing the dualIB rate-
distortion problem.
Proof. (i) Given that the problem is formulated as a rate-distortion problem the encoder’s update
rule must be the known minimizer of the distortion function (Cover & Thomas, 2006). Thus the
IB encoder with the dual distortion is plugged in. (ii) For the decoder, by considering a small
perturbation in the distortion dduaiIB(χ, ^), with α(X) the normalization Lagrange multiplier, We
obtain:
δddualIB (x, X)
δ (X Pe (y |χ) log Peyy|x)+。⑺(X Pe (y | © - 1))
δddualIB (x, X)
log
Pe (y | 攵)
P(y | x)
+ 1 + α(X).
δPe (y | 攵)
Hence, minimizing the eXpected distortion becomes:
0 = XPe(x | x) log P(yy∣ Ix)) + 1 +。⑺
X
=logPe(y I 攵)-XPe(χ I 攵)logP(y I χ) +1 +。(攵),
X
which yields Algorithm 1, row 6.
□
Considering the dualIB encoder-decoder, Algorithm 1, we find that Epe(χ,χ) [dduaiIB(x, ^)] reduces
to the eXpectation of the decoder’s log partition function:
Pe (y i 攵)
P(y i x)
Epe(χ,χ)[dduaiIB(χ, X)] = EPe(χ, X) EPe(y I X) log
χ,x	y
-Epe(χ) [logZyIx(χ;e)] + EPe⑺ EPe(χ0 i X)IogP(y i XO)- EPe(χ i X)IogP(y i x)
X,y	L x0	x
-Epe(X) [log Zy∣x区 β)].
13
Under review as a conference paper at ICLR 2020
A.3 Stability analysis
Here we provide the detailed stability analysis allowing the definition of the matrices
Cxdxu0alIB, Cydyu0alIB (equation 12) and which allows us to claim that they obey the same rules as the C
matrices (equation 5) in equation 4. Considering a variation in X We get:
a lθgp^(xlx) =β XPe(y | ^) (log 4⅛ - 1)d logp^(ylx)
∂x	∖	Pe (y |X) J ∂X
=β XPe(y I X) logp(y I x) - XPe(x I X) logp(y I x) d logp^(y | x)
y	L	X	」
+β X log Zy∣x(X; β)a⅛∕
y
=β XPe (y∣^)pe 因 x)log *•，	(°)
y,x
∂ logPe(y I x)	1	aZy∣χ(x;β)	ʌʌ / I W logPe(x I x)
一宗-=-ZyF	ax + ∑Pe(x i x) logP(y i x) —∂X—
=-XPe (y i χ) XPe(x i x) logP(y i x)a logPe(X i x)
ax
y	X
+ X Pe (XWlOg P(yIx)d log PX(XIX)
x
=XPe (xI^)Pe (yI^)log 3	.	(18)
X y	P(y i )
Substituting equation 18 into equation 17 and vice versa one obtains:
∂ logPe(X I ^)
∂X
β X Pe (y i x)Pe(x i x) log P(y-^Xy
x0,y,y,x
•	Pe (x0IX)Pe (yI^)log 奈 d log PFW
Ty=β X Pe (xI^)Pe(yIX)log PI⅛
x,y0,x,y
/ 01 - - i 7 i P(y01 x) a logPe (y01 攵)
•	Pe(y i X)Pe(x i X)IogPyrnEy —ax— ∙
We noW define the CdualIB matrices as folloWs:
CdUalIB (x；β) = XPe(y i x)Pe(x i χ) log P(y-j∣)- • Pe(x0 i x)Pe( i χ) log M I ：)
CyyUalIB (x；β) = XPe(x i x)Pe(y i χ) log P(； I X) • Pe(y0 i X)Pe(x i x) log P(y7j∣) ∙
x,x,y
Using the above definition We have an equivalence to equation 4 in the form of:
[i-βcxXualIB(x,β)]alogPeXX7i攵)=ο,	[i-βcyyualIB(x,β)]alogPeXy7iX) = 0.
Note that for the binary case, the matrices may be simplified to:
CdUalIB(x；β) = XPe(y i X)Pe(x i χ) logP(； I X) • Pe(x0 i x)(1 -Pe(y i X))Iog 1 -(P(]；；/)
CyyUalIB(x；β) = XPe(x i x)(1 -Pe(y i X))Iog 1 -11X)X) • Pe(y7 i X)Pe便 i ©logP(y7j1)∙
x,x
14
Under review as a conference paper at ICLR 2020
A.3.1 Proof of Lemma 3
We show that the CduallB matrices share the same eigenvalues with λι(x) = 0.
Proof. The matrices, CdUalIB (X; β), CdUalIB(X; β), are given by:
Cd0alIB(X;β) = Axy(^;β)Βyx0(^;β) , Cd0alIB(^;β) = Βyx(X;β)Axy0(X;β),
with:
Axy值; β) =Pe (y | X) X Pe 依 | ©bg p(y | X) , Byx(X β) =Pe(X | 攵)X Pe(y | ©bg p(y | X).
p(y | X)	p(y | X)
Xy
Given that the matrices are obtained by the multiplication of the same matrices, it follows that they
have the same eigenvalues {λi(X; β)}.
To prove that λι(X; β) = 0 We show that det(CdUalIB) = 0. We present the exact calculation for a
binary label Y ∈ {y0, y1} (the argument for general Y follows by encoding the label as a sequence
of bits and discussing the first bit only, as a binary case):
det(CdUaliβ(x; β)) = XPe(x | X)Pe(yi | X)IOg p(y0 | X) ∙Ped | X)Pe值 | X)Iog p(y0 | X)
P(y1 I x)	P(y0 I x)
X,X
• X Pe (x0 | X)Pe(yo	| x) log P(y1	|	X)	∙ Pe(yi	| ©Pe (x'	| x) log P(y1	|
i	P(yο	|	XO)	P(yι	|	X)
X ,X ,
-XPe (x | 攵)Pe(yο | x) log P(y1 | X) • Pe d | 攵)Pe值 | X)Iog P；；。| ：)
X,X
• XPe(x0 | X)Pe(yι | 攵)log P(y0 | X0) • Pe(yι | X)Pe(WI x) log ；(：1 | XO)
= X	Pe (x	i	X)Pe(x0	i	xx)P2e (y。i	X)Pe(；i	i	X)Pe伉 i x) log P；01 X)Pe 便1	x) log P(y1	]	X，)
x,x0,X,X0	11	/
.jog p(y0	| x) log p(y1	|	x')	- log p(y0	|	x)	log p(y1	| x') 一
[g p(yι	I x) g p(yo	I	x0) g p(yι	I	x) g p(y°	I x0)」
0.
Given that the determinant is 0 implies that λι(^) = 0.
□
For a binary problem we can describe the non-zero eigenvalue using λ2(X) = Tr(CdUalIB(^; β)).
That is:
为值)=XPe(x i X)Pe(；i i ^)logP(y01 X) • Pe(y。i 攵)Pe(x i ^)logP(y01 x)
X X	P(y 1 i )	P(y O i )
+ XPe(x i ©Pe (y。i ^) log P(y1 i x)
X,X
• Pe(yι i x)Pe (x i ^) log P(y1 i x)
=Pe(yι i X)Pe(yο i 攵)XPe (x i X)Pe伉 i x) log	[log 黑号 - log P⅛^
X,X
A.4 Information Plane analysis
We rely on known results for the rate-distortion problem and the information plane:
Lemma 8. I(X; X) is a non-increasing convex function of the distortion Epe (x,x)[d(x, ^)] With a
slope of -β.
We emphasis that this is a general result of rate-distortion thus holds for the dualIB as well.
15
Under review as a conference paper at ICLR 2020
Lemma 9. For a fixed encoder pβ (X | x) and the Bayes optimal decoder pβ (y | ^):
E[diB(x, ^)]= I(X; Y)- I(X; Y).
Thus, the information curve, IY vs. IX, is a non-decreasing concave function with a positive slope,
β-1. The concavity implies that β increases along the curve.
(Cover & Thomas, 2006; Gilad-bachrach et al., 2003).
A.4. 1 Proof of Lemma 4
In the following section we provide a proof to lemma 4, for the IB and dualIB problems.
Proof. We want to analyze the behaviorofIX(β), IY(β), that is the change in each term as a function
of the corresponding β. From lemma 9, the concavity of the information curve, we can deduce that
both are non-decreasing functions of β. As the two β derivatives are proportional it’s enough to
discuss the first one.
Next, we focus on their behavior between two critical points. That is, where the cardinality of X is
fixed (clusters are ”static”). For ”static” clusters, the β derivative of IX, along the optimal line is
given by:
∂I(x； X) _ ∂
-∂β- = - ∂β
Epe(x, x) (log Zχ∣x(x; β) + βd(x,刈
x,x
-β(d(x, ^)R产)
∂β
〉。β (x,x)
d∣g -Jdlogz^∣χ(x;β) , ,z r]∖
≈ β∖d(x, x) [-----∂β-----+ d(x，x)] p	ʌ
≈ β,d2(x，x)>pβ(x∣x) -hd(x，x)iPβ(x|x))；.
VarwX))	p(x)
This first of all reassures that the function is non-decreasing as Var(d(x)) ≥ 0.
The piece-wise concavity follows from the fact that when the number of clusters is fixed (between the
critical points) - increasing β decreases the clusters conditional entropy H(X | X), as the encoder
becomes more deterministic. The mutual information is bounded by H(X) and it’s β derivative
decreases. Further, between the critical points there are no sign changes in the second β derivative.
□
A.4.2 Proof of Lemma 5
Proof. The information curve has a positive slope, β-1, with β increasing along it, lemma 9. That
is, given a value of β, there exists a pair lYB(β),lXB(β) such that 91^^)/9^^6^) = β-1. Now,
consider a sub-optimal information curve, IY,&. There exist values β0, β00 such that:
IX(β0) = IXB (β), IY (β00) = IYB (β).
The optimality of the IB implies that sub-optimal curves lie below it; i.e, the IB slope is steeper:
β-1 > β0-1 , β-1 > β00-1.
Thus, given that β increases along the information curve, it holds that:
IX(β) > IX(β0) = IXB(β), IY(β) > IY(β00) = IYB(β).
□
16
Under review as a conference paper at ICLR 2020
A.5 Derivation of the dualExpIB
We provide elaborate derivations to theorem 7; that is, we obtain the dualIB optimal encoder-decoder
under the exponential assumption over the data. We use the notations defined in §3.
•	The decoder, equation 14.
Substituting the exponential assumption into the dualIB log-decoder yields:
logPe(y | 攵)=XPe(X | ©logp(y | x) - logZy∣χ(X;β)
x
d
=- XXPe(x | Qr(y)Ar(x) - logZy∣χ区β)
x r=0
d
=-X λr(y)Ar,β(x) - Epe(x|X) [λX] - log ZyIx(^；β).
r=1
Taking a closer look at the normalization term:
ZyIx (x; β) = X ePx pβ (x∣x) log p(y∣x) = e-Epβ (x∣x)[λX]X e-Pd=ι λr(y)Ar,β (x)
yy
log ZyIx (x； β) = -Epβ(x∣x)[λx] + log (X e-Pd=1 λr(y"(X))
From which it follows that λβ (x) is given by:
λβ (^) = log (X e-Pd=ι λr⑼Ar")
and we can conclude that the dualExpIB decoder takes the form:
d
logPe(y | 攵)=-Xλr(y)Ar,e(x)∙
r=0
•	The encoder, equation 15.
The core of the encoder is the dual distortion function which may now be written as:
dduaiiB(x,^)=X Pe (y | x) log Pe(y| ：)
y	P(y | x)
d
=XPe(y I x) (λx - λ(x)) + Xλr(y)(Ar(x) - Ar,e(x))
y	r=1
d
=λx - λ(^) + X λe(^)(Ar(x) - Ar,e(x)),
r=1
substituting this into the encoder’s definition we obtain:
Pe (支 | x)
Pe ㈤	e-e[λX-λβ (x)+pd=ι λβ (x)[Ar (X)-Ar,β (x)]]
Zx∣x(x;β)
Pe (X)eeλβ (X) e - e Pd=I λβ (X)[Ar (X)-Ar,β (X)]
Zx∣x(x; β)
17