Under review as a conference paper at ICLR 2020
Learning Representations in Reinforcement Learning: An
Information Bottleneck Approach
Anonymous authors
Paper under double-blind review
Abstract
The information bottleneck principle in (Tishby et al., 2000) is an elegant
and useful approach to representation learning. In this paper, we investi-
gate the problem of representation learning in the context of reinforcement
learning using the information bottleneck framework, aiming at improving
the sample efficiency of the learning algorithms. We analytically derive the
optimal conditional distribution of the representation, and provide a vari-
ational lower bound. Then, we maximize this lower bound with the Stein
variational (SV) gradient method (originally developed in (Liu & Wang,
2016; Liu et al., 2017)). We incorporate this framework in the advanta-
geous actor critic algorithm (A2C)(Mnih et al., 2016) and the proximal
policy optimization algorithm (PPO) (Schulman et al., 2017). Our experi-
mental results show that our framework can improve the sample efficiency
of vanilla A2C and PPO significantly. Finally, we study the information
bottleneck (IB) perspective in deep RL with the algorithm called mutual
information neural estimation(MINE) (Belghazi et al., 2018). We exper-
imentally verify that the information extraction-compression process also
exists in deep RL and our framework is capable of accelerating this process.
We also analyze the relationship between MINE and our method, through
this relationship, we theoretically derive an algorithm to optimize our IB
framework without constructing the lower bound.
1	Introduction
In training a reinforcement learning algorithm, an agent interacts with the environment, ex-
plores the (possibly unknown) state space, and learns a policy from the exploration sample
data. In many cases, such samples are quite expensive to obtain (e.g., requires interactions
with the physical environment). Hence, improving the sample efficiency of the learning al-
gorithm is a key problem in RL and has been studied extensively in the literature. Popular
techniques include experience reuse/replay, which leads to powerful off-policy algorithms
(e.g., (Mnih et al., 2013; Silver et al., 2014; Van Hasselt et al., 2015; Nachum et al., 2018a;
Espeholt et al., 2018)), and model-based algorithms (e.g., (Hafner et al., 2018; Kaiser et al.,
2019)). Moreover, it is known that effective representations can greatly reduce the sample
complexity in RL. This can be seen from the following motivating example: In the envi-
ronment of a classical Atari game: Seaquest, it may take dozens of millions samples to
converge to an optimal policy when the input states are raw images (more than 28,000 di-
mensions), while it takes less samples when the inputs are 128-dimension pre-defined RAM
data(Sygnowski & Michalewski, 2016). Clearly, the RAM data contain much less redundant
information irrelevant to the learning process than the raw images. Thus, we argue that an
efficient representation is extremely crucial to the sample efficiency.
In this paper, we try to improve the sample efficiency in RL from the perspective of
representation learning using the celebrated information bottleneck framework (Tishby
et al., 2000). In standard deep learning, the experiments in (Shwartz-Ziv & Tishby,
2017) show that during the training process, the neural network first ”remembers”
the inputs by increasing the mutual information between the inputs and the repre-
sentation variables, then compresses the inputs to efficient representation related to
1
Under review as a conference paper at ICLR 2020
the learning task by discarding redundant information from inputs (decreasing the mu-
tual information between inputs and representation variables). We call this phenomena
”information extraction-compression process”(information E-C process). Our experiments
shows that, similar to the results shown in (Shwartz-Ziv & Tishby, 2017), we first (to the
best of our knowledge) observe the information extraction-compression phenomena in the
context of deep RL (we need to use MINE(Belghazi et al., 2018) for estimating the mu-
tual information). This observation motivates us to adopt the information bottleneck (IB)
framework in reinforcement learning, in order to accelerate the extraction-compression pro-
cess. The IB framework is intended to explicitly enforce RL agents to learn an efficient
representation, hence improving the sample efficiency, by discarding irrelevant information
from raw input data. Our technical contributions can be summarized as follows:
1.	We observe that the ”information extraction-compression process” also exists in
the context of deep RL (using MINE(Belghazi et al., 2018) to estimate the mutual
information).
2.	We derive the optimization problem of our information bottleneck framework in
RL. In order to solve the optimization problem, we construct a lower bound and
use the Stein variational gradient method developed in (Liu et al., 2017) to optimize
the lower bound.
3.	We show that our framework can accelerate the information extraction-compression
process. Our experimental results also show that combining actor-critic algorithms
(such as A2C, PPO) with our framework is more sample-efficient than their original
versions.
4.	We analyze the relationship between our framework and MINE, through this rela-
tionship, we theoretically derive an algorithm to optimize our IB framework without
constructing the lower bound.
Finally, we note that our IB method is orthogonal to other methods for improving the
sample efficiency, and it is an interesting future work to incorporate it in other off-policy
and model-based algorithms.
2	Related Work
Information bottleneck framework was first introduced in (Tishby et al., 2000). They solve
the framework by iterative Blahut Arimoto algorithm, which is infeasible to apply to deep
neural networks. (Shwartz-Ziv & Tishby, 2017) tries to open the black box of deep learning
from the perspective of information bottleneck, though the method they use to compute
the mutual information is not precise. (Alemi et al., 2016) derives a variational information
bottleneck framework, yet apart from adding prior target distribution of the representation
distribution P (Z|X), they also assume that P (Z|X) itself must be a Gaussian distribution,
which limits the capabilities of the representation function. (Peng et al., 2018) extends
this framework to variational discriminator bottleneck to improve GANs(Goodfellow et al.,
2014), imitation learning and inverse RL.
As for improving sample-efficiency, (Mnih et al., 2013; Van Hasselt et al., 2015; Nachum
et al., 2018a) mainly utilize the experience-reuse. Besides experience-reuse, (Silver et al.,
2014; Fujimoto et al., 2018) tries to learn a deterministic policy, (Espeholt et al., 2018)
seeks to mitigate the delay of off-policy. (Hafner et al., 2018; Kaiser et al., 2019) learn
the environment model. Some other powerful techniques can be found in (Botvinick et al.,
2019).
State representation learning has been studied extensively, readers can find some classic
works in the overview (Lesort et al., 2018). Apart from this overview, (Nachum et al.,
2018b) shows a theoretical foundation of maintaining the optimality of representation space.
(Bellemare et al., 2019) proposes a new perspective on representation learning in RL based on
geometric properties of the space of value function. (Abel et al., 2019) learns representation
via information bottleneck(IB) in imitation/apprenticeship learning. To the best of our
knowledge, there is no work that intends to directly use IB in basic RL algorithms.
2
Under review as a conference paper at ICLR 2020
3	Preliminaries
A Markov decision process(MDP) is a tuple, (X, A, R, P,μ), where X is the set of states,
A is the set of actions, R : X × A × X → R is the reward function, P : X × A × X →[0, 1]
is the transition probability function(where P (X0 |X, a) is the probability of transitioning
to state X0 given that the previous state is X and the agent took action a in X), and
μ : X →[0, 1] is the starting state distribution. A policy π : X → P(A) is a map from states
to probability distributions over actions, with π(a∣X) denoting the probability of choosing
action a in state X .
In reinforcement learning, we aim to select a policy π which maximizes K(π) =
ETz∏ [£∞=o YtR((Xt, at, Xt+ι)], with a slight abuse of notation We denote R(Xt, at, Xt+ι)=
rt. Here γ ∈ [0, 1) is a discount factor, τ denotes a trajectory (X0, a0, X1, a1, ...). Define the
state value function as Vπ (X) = ET^∏ [£∞=o γtrt∣X0 = X], which is the expected return by
policy π in state X. And the state-action value function Qπ(X, a) = ET^∏ [£∞=o Ytrt∣X0 =
X, a0 = a] is the expected return by policy π after taking action a in state X .
Actor-critic algorithms take the advantage of both policy gradient methods and value-
function-based methods such as the well-known A2C(Mnih et al., 2016). Specifically, in
the case that policy π(a∣X; θ) is parameterized by θ, A2C uses the following equation to
approximate the real policy gradient 口K(π) = J J(θ):
∞∞
Nθ J(θ) ≈^θ[log∏(at∣Xt； θ)(Rt- b(Xt))+ α2H(∏(∙Xt))] =	θ J(Xt； θ)	⑴
t=0	t=0
where Rt =	i∞=0 Yirt+i is the accumulated return from time step t, H(p) is the entropy of
distribution p and b(Xt) is a baseline function, which is commonly replaced by Vπ (Xt).
A2C also includes the minimization of the mean square error between Rt and value function
Vπ (Xt). Thus in practice, the total objective function in A2C can be written as:
∞∞
J(θ) ≈ ElOg∏(atIXt； θ)(Rt- Vπ(Xt)) — α 1 kRt — Vπ(Xt)k2 + «2H(∏(∙Xt)) = E J(Xt； θ)
t=0	t=0
(2)
where α1 , α2 are two coefficients.
In the context of representation learning in RL, J(Xt; θ)(including Vπ (Xt) and Qπ (Xt, at))
can be replaced by J(Zt; θ) where Zt is a learnable low-dimensional representation of state
Xt. For example, given a representation function Z 〜Pφ(∙∣X) with parameter φ, define
Vπ(Zt； Xt,φ) Zt〜Pφ(.∣Xt)= Vπ(Xt). For simplicity, we write Vπ(Zt； Xt,φ) ∣z,〜Pφ(∙∣Xt) as
Vπ(Zt).
4 Framework
4.1	Information Bottleneck in Reinforcement Learning
The information bottleneck framework is an information theoretical framework for extract-
ing relevant information, or yielding a representation, that an input X ∈ X contains about
an output Y ∈ Y . An optimal representation of X would capture the relevant factors and
compress X by diminishing the irrelevant parts which do not contribute to the prediction
of Y . In a Markovian structure X → Z → Y where X is the input, Z is representation of
X and Y is the label of X, IB seeks an embedding distribution P?(Z|X) such that:
P?(Z|X) = arg max I(Y, Z) - βI(X, Z) = arg max H(Y) -H(Y|Z) - βI(X, Z)
P(Z|X)	P(Z|X)
= arg max -H(Y |Z) - βI (X, Z)
P(Z|X)
(3)
3
Under review as a conference paper at ICLR 2020
for every X ∈ X , which appears as the standard cross-entropy loss1 in supervised learning
with a MI-regularizer, β is a coefficient that controls the magnitude of the regularizer.
Next we derive an information bottleneck framework in reinforcement learning. Just like the
label Y in the context of supervised learning as showed in (3), we assume the supervising
signal Y in RL to be the accurate value Rt of a specific state Xt for a fixed policy π,
which can be approximated by an n-step bootstrapping function Yt = Rt = Pin=-02 γirt+i +
γn-1V π (Zt+n-1) in practice. Let P(Y |Z) be the following distribution:
P(yt∖Zt)) (x exp(-«(Rt- Vπ(Zt))2)
(4)
.This assumption is heuristic but reasonable: If we have an input Xt and its relative label
Yt = Rt , we now have Xt ’s representation Zt , naturally we want to train our decision
function Vπ (Zt) to approximate the true label Yt . If we set our target distribution to be
C ∙ exp(-α(Rt — Vπ(Zt))2), the probability decreases as Vπ(Zt) gets far from Yt while
increases as Vπ(Zt) gets close to Yt.
For simplicity, we just write P(R∖Z) instead of P(Yt∖Zt) in the following context.
With this assumption, equation (3) can be written as:
P ? (Z ∖X)
arg max
P(Z|X)
EX,R,Z~P(X,R,Z)[log P(r∖z)] - βI(x, Z)
arg max
P(Z|X)
EXzP(X),Z~P(ZX),R〜P(R∣Z)[-α(R - V"(Z))2] - βI(X, Z)
(5)
The first term looks familiar with classic mean squared error in supervisd learning. In a
ʌ
network with representation parameter φ and policy-value parameter θ, policy loss J(Z; θ)
in equation(1) and IB loss in (5) can be jointly written as:
L(θ, Φ) = EX〜P(X),ZzPφ(Z∣X)[J(Z； θ) + ER[-α(R - Vπ(Z”))2]] - βI(X, Z; φ)	(6)
|{z}
J (Z; θ)
where I(X, Z; φ) denotes the MI between X and Z 〜Pφ(∙∖X). Notice that J(Z; θ) itself is
a standard loss function in RL as showed in (2). Finally we get the ultimate formalization
of IB framework in reinforcement learning:
Pφ* (Z ∖X )= arg maχ、E XZP (X) Zf(z∖x ) [ J (Z; θ)] - βI (X, Z; φ)	⑺
Pφ(z∣x)	' ‘' 八''
The following theorem shows that if the mutual information I(X, Z) of our framework and
common RL framework are close, then our framework is near-optimality.
Theorem 1 (Near-optimality theorem). Policy πr = πθr, parameter φr, optimal policy π? =
πθ? and its relevant representation parameter φ? are defined as following:
θr,φr = argmin EPφ(X,Z)[log Pφ'ZX) - 1 J(Z； θ)]	⑻
θ,φ	Pφ(Z)	β
θ？,φ? = argminEPφ(X,Z)[- 1 J(Z; θ)]	⑼
θ,φ	β
Define Jπr as EPφr (X,Z) [J(Z; θr)] and Jπ? as EPφ? (X,Z) [J(Z; θ?)]. Assume that for any
e > 0, ∖I(X,Z; φ?) - I(X,Z; φr) ∖ < β, We have ∖Jπr - Jπ? ∖ < e.
4.2	Target Distribution Derivation and Variational Lower Bound Construction
In this section we first derive the target distribution in (7) and then seek to optimize it by
constructing a variational lower bound.
1Mutual information I(X, Y) is defined as R dXdZP(X, Z) log PPX(X)PZZ), conditional en-
tropy H(Y|Z) is defined as - R dYdZP(Y, Z) log P(Y|Z). In a binary-classification problem,
ʌ ʌ
-log P (YIZ) = - (1 - Y) Iog(ι - Y(Z))- Y Iog( Y(Z)).
4
Under review as a conference paper at ICLR 2020
We would like to solve the optimization problem in (7):
maχ、E X~P (X) z~Pφ (ZX)[ J (Z ； θ)-β log Pφ (ZlX)+ β log Pφ (Z)]
pΦ( zlX)	|----------{z----------} |----{-----}
L1(θ,φ)	L2 (θ,φ)
(10)
Combining the derivative of L1 and L2 and setting their summation to 0, we can get that
Pφ(Z∣X) X Pφ(Z)exp( 1J(Z; θ))
(11)
We provide a rigorous derivation of (11) in the appendix(A.2). We note that though our
derivation is over the representation space instead of the whole network parameter space,
the optimization problem (10) and the resulting distribution (11) are quite similar to the one
studied in (Liu et al., 2017) in the context of Bayesian inference. However, we stress that
our formulation follows from the information bottleneck framework, and is mathematically
different from that in (Liu et al., 2017). In particular, the difference lies in the term L2,
which depends on the the distribution Pφ(Z ∣ X) we want to optimize (while in (Liu et al.,
2017), the corresponding term is a fixed prior).
The following theorem shows that the distribution in (11) is an optimal target distribution
(with respect to the IB objective L). The proof can be found in the appendix(A.3).
Theorem 2. (Representation Improvement Theorem) Consider the objective function
L(θ,φ) = EX~p(X)z”6(z∣χ)[J(Z; θ)] — βI(X, Z; φ), given a fixed policy-value Parame-
ter θ, representation distribution Pφ(Z∣X) and state distribution P(X). Define a new
representation distribution: P^(Z∣X) H Pφ(Z)exp(β J(Z; θ)), We have L(θ, φ) ≥ L(θ, φ).
Though we have derived the optimal target distribution, it is still difficult to compute Pφ(Z).
In order to resolve this problem, we construct a variational lower bound with a distribution
U(Z) which is independent of φ. Notice that R dZPφ(Z) log Pφ(Z) ≥ R dZPφ(Z) log U(Z).
Now, we can derive a lower bound of L(θ, φ) in (6) as follows:
L(θ, φ) = EX,z[J(Z; θ) —
βlogPφ(Z∣X)] +β
dZPφ(Z)logPφ(Z)
≥ Ex,z [ J(Z； θ)—
βlogPφ(Z∣X)] +β
dZPφ(Z)logU(Z)
ʌ
Ex~p(X),z~Pφ(ZX) [J(Z； θ) — β log Pφ(Z∣X) + β log U(Z)] = L(θ Φ)
Naturally the target distribution of maximizing the lower bound is:
Pφ(Z∣X) (X U(Z)exp(1 J(Z; θ))
β
(12)
(13)
4.3	Optimization by Stein Variational Gradient Descent
Next we utilize the method in (Liu & Wang, 2016)(Liu et al., 2017)(Haarnoja et al., 2017)
to optimize the lower bound.
Stein variational gradient descent(SVGD) is a non-parametric variational inference algo-
rithm that leverages efficient deterministic dynamics to transport a set of particles {Zi}in=1
to approximate given target distributions Q(Z). We choose SVGD to optimize the lower
bound because of its ability to handle unnormalized target distributions such as (13).
Briefly, SVGD iteratively updates the “particles” {Zi}n=ι via a direction function Φ*(∙) in
the unit ball of a reproducing kernel Hilbert space (RKHS) H:
Zi — Zi + eφ* (Zi )	(14)
where Φ* (∙) is chosen as a direction to maximally decrease2 the KL divergence between the
^
particles, distribution P(Z) and the target distribution Q(Z) = QC) (Q is unnormalized
2In fact, Φ* is chosen to maximize the directional derivative of F(P) = — DKL(P∣∣Q), which
appears to be the ”gradient” of F
5
Under review as a conference paper at ICLR 2020
distribution, C is normalized coefficient) in the sense that
Φ* — arg 1φ∈H{-ddeDKL(P[eφ]UQ) s∙t∙ IlΦ∣∣H ≤ 1}	(15)
where P[Φ] is the distribution of Z + Φ(Z) and P is the distribution of Z. (Liu & Wang,
2016) showed a closed form of this direction:
__ ______________ ʌ ʌ _____________ ʌ
Φ(Z)= EZ尸P[K(Zj,Zi)NZ log Q(Z) IZ=Z∙ + NZK(Z,Zi) IZ=ZJ	(16)
where K is a kernel function(typically an RBF kernel function). Notice that C has been
omitted.
In our case, We Seek to minimize D KL (Pφ (∙lX ) 11 ",)expg 1J3 ; " ) E =R dZu (Z)eχp( 1 J (Z ； θ)),
ʌ
which is equivalent to maximize L(θ, φ), the greedy direction yields:
1
φ(Zi) = EZ,iφ(∙∣x)[K(Zj,Zi)RZ(βJ(Z”)+logU(Z)) Z=ZTRZK(ZZ) Z=ZJ (17)
ʌ ʌ
In practice We replace log U(Z) With ζ log U(Z) Where ζ is a coefficient that controls the
ʌ
magnitude of NZ log U(Z). Notice that Φ(Zi) is the greedy direction that Zi moves towards
ʌ ʌ
L(θ,φ)′s target distribution as showed in (13)(distribution that maximizes L(θ, φ)). This
means Φ(Zi) is the gradient of L(Z%, θ, φ): dLZθ④) H Φ(Zi).
Since our ultimate purpose is to update φ, by the chain rule, d(Zφθ(φ) H Φ(Zi) ∂φ. Then
for L(θ,Φ) = EPφ(x,z)[L(Z,θ,φ)]:
ʌ
∂L( θ,φ)	口	∏∂Z"	,re、
—∂φ— H EXZP(X),Zi~Pφ(∙∖x)[φ(Z)-φ-]	(18)
Φ(Zi) is given in equation(17). In practice we update the policy-value parameter θ by
common policy gradient algorithm since:
ʌ
∂L( θ,Φ) =E	[ ∂J (Z ”) ]
∂θ	E pφ(XZ) [	∂θ	(19)
and update representation parameter φ by (18).
4.4	Verify the information E-C process with MINE
This section we verify that the information E-C process exists in deep RL with MINE and
our framework accelerates this process.
Mutual information neural estimation(MINE) is an algorithm that can compute mutual
information(MI) between two high dimensional random variables more accurately and effi-
ciently. Specifically, for random variables X and Z, assume T to be a function of X and Z,
the calculation of I(X, Z) can be transformed to the following optimization problem:
I(X, Z) = maxEP(χZ)[T] — log(Ep(X)0p(Z)[expT])	(2。)
The optimal function T ?(X, Z) can be approximated by updating a neural network
T (X, Z ； η).
With the aid of this powerful tool, we would like to visualize the mutual information between
input state X and its relative representation Z : Every a few update steps, we sample a batch
of inputs and their relevant representations {Xi, Zi}in=1 and compute their MI with MINE,
every time we train MINE(update η) we just shuffle {Zi}in=1 and roughly assume the shuffled
representations {Zishuffled}in=1 to be independent with {Xi}in=1 :
nn
I (X,Z) ≈ max- X[ T (Xi, Zi; η)] — log(- X [exp T(Xi ZsHUffIedn)])	(21)
ηn	n
i=1	i=1
6
Under review as a conference paper at ICLR 2020
Figure(1) is the tensorboard graph of mutual information estimation between X and Z
in Atari game Pong, x-axis is update steps and y-axis is MI estimation. More details
and results can be found in appendix(A.6) and (A.7). As we can see, in both A2C with
our framework and common A2C, the MI first increases to encode more information from
inputs(”remember” the inputs), then decreases to drop irrelevant information from in-
puts(”forget” the useless information). And clearly, our framework extracts faster and
compresses faster than common A2C as showed in figure(1)(b).
(a) MI in A2C
Figure 1: Mutual information visualization in Pong
(b) MI in A2C with our framework
After completing the visualization of MI with MINE, we analyze the relationship between
our framework and MINE. According to (Belghazi et al., 2018), the optimal function T* in
(20) goes as follows:
eχpT* (Xz;η) = C Pφ( X, Z)	s.t. C = E P (X)心 Pφ( Z)[eχPT*]	(22)
P(X)Pφ(Z)
Combining the result with Theorem(2), we get:
expT*(x,;η) = CPZP 8 exp(β J(Z； θ))	(23)
Through this relationship, we theoretically derive an algorithm that can directly optimize
our framework without constructing the lower bound, we put this derivation in the ap-
pendix(A.5).
5	Experiments
In the experiments we show that our framework can improve the sample efficiency of basic RL
algorithms(typically A2C and PPO). Our anonymous code can be found in https://github.
com/AnonymousSubmittedCode/SVIB. Other results can be found in last two appendices.
In A2C with our framework, We sample Z by a network φ(X, e) where e 〜N(∙; 0,0.1) and the
number of samples from each state X is 32, readers are encouraged to take more samples
if the computation resources are sufficient. We set the IB coefficient as β = 0.001. We
choose two prior distributions U(Z) of our framework, the first one is uniform distribution,
apparently when U(Z) is the uniform distribution, NZ log U(Z) Z=Z can be omitted. The
second one is a Gaussian distribution, which is defined as follows: for a given state Xi ,
sample a batch of {Zj}rn=32, then: U(Z) = N(Z; μ = n P；= Zj,σ2 = 1 P；=《Zj - μ)2).
We also set ζ as 0.005||V_Zβ J(Z; θ)/NZ^ log U(Z) ∣∣ ∣；^=Z to control the magnitude of
ʌ
NZ log U(Z) ∣r^=z. Following (LiU et al., 2017), the kernel function in (17) we used is
the Gaussian RBF kernel K(Zi, Zj) = exp(-|Zi - Zj |2/h) where h = med2/2 log(n + 1),
med denotes the median of pairwise distances between the particles {Zji}in=1 . As for the
hyper-parameters in RL, we simply choose the default parameters in A2C of Openai-
baselines(https://github.com/openai/baselines/tree/master/baselines/a2c). In summary,
we implement the following four algorithms:
7
Under review as a conference paper at ICLR 2020
A2C with uniform SVIB: Use φ(X, ) as the embedding function, optimize by our frame-
work(algorithm(A.4)) with U(Z) being uniform distribution.
A2C with Gaussian SVIB: Use φ(X, ) as the embedding function, optimize by our frame-
work(algorithm(A.4)) with U(Z) being Gaussian distribution.
A2C:Regular A2C in Openai-baselines with φ(X) as the embedding function.
A2C with noise(For fairness):A2C with the same embedding function φ(X, ) as A2C with
our framework.
Figure(2)(a)-(e) show the performance of four A2C-based algorithms in 5 gym Atari games.
We can see that A2C with our framework is more sample-efficient than both A2C and A2C
with noise in nearly all 5 games.
Figure 2: (a)-(e) show the performance of four A2C-based algorithms, x-axis is time
steps(2000 update steps for each time step) and y-axis is the average reward over 10 episodes,
(f)-(h) show the performance of four PPO-based algorithms, x-axis is time steps(300 update
steps for each time step). We make exponential moving average of each game to smooth
the curve(In PPO-Pong, we add 21 to all four curves in order to make exponential moving
average). We can see that our framework improves sample efficiency of basic A2C and PPO.
Notice that in SpaceInvaders, A2C with Gaussian SVIB is worse. We suspect that this is
because the agent excessively drops information from inputs that it misses some information
related to the learning process. There is a more detailed experimental discussion about
this phenomena in appendix(A.7) . We also implement four PPO-based algorithms whose
experimental settings are same as A2C except that we set the number of samples as 26 for
the sake of computation efficiency. Results can be found in the in figure(2)(f)-(h).
6	Conclusion
We study the information bottleneck principle in RL: We propose an optimization problem
for learning the representation in RL based on the information-bottleneck framework and
derive the optimal form of the target distribution. We construct a lower bound and utilize
Stein Variational gradient method to optimize it. Finally, we verify that the information
extraction and compression process also exists in deep RL, and our framework can accelerate
this process. We also theoretically derive an algorithm based on MINE that can directly
optimize our framework and we plan to study it experimentally in the future work.
8
Under review as a conference paper at ICLR 2020
References
David Abel, Dilip Arumugam, Kavosh Asadi, Yuu Jinnai, Michael L Littman, and Law-
son LS Wong. State abstraction as compression in apprenticeship learning. In Proceedings
of the AAAI Conference on Artificial Intelligence. AAAI Press, pp. 3134-3142, 2019.
Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational
information bottleneck. In Proceedings of the International Conference on Learning Rep-
resentations, 2016.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio,
Aaron Courville, and R Devon Hjelm. Mine: Mutual information neural estimation. arXiv
preprint arXiv:1801.04062, 2018.
Marc G. Bellemare, Will Dabney, Robert Dadashi, Adrien Ali Taiga, Pablo Samuel Castro,
Nicolas Le Roux, Dale Schuurmans, Tor Lattimore, and Clare Lyle. A geometric perspec-
tive on optimal representations for reinforcement learning. International Conference on
Learning Representations, 2019.
Mathew Botvinick, Sam Ritter, Jane X Wang, Zeb Kurth-Nelson, Charles Blundell, and
Demis Hassabis. Reinforcement learning, fast and slow. Trends in cognitive sciences, 23
(5):408-422, 2019.
Tessler Chen, Daniel J. Mankowitz, and Shie Mannor. Reward constrained policy optimiza-
tion. arXiv preprint arXiv:1805.11074, 2018.
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom
Ward, Yotam Doron, Vlad Firoiu, Tim Harley, and Iain Dunning. Impala: Scalable
distributed deep-rl with importance weighted actor-learner architectures. arXiv preprint
arXiv:1802.01561, 2018.
Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation
error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Xu Bing, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In International
Conference on Neural Information Processing Systems, pp. 2672-2680, 2014.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning
with deep energy-based policies. Proceedings of the 34th International Conference on
Machine Learning, 70:1352-1361, 2017.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, and James Davidson. Learn-
ing latent dynamics for planning from pixels. International Conference on Machine Learn-
ing, pp. 2555-2565, 2018.
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell,
Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, and Sergey and
Levine. Model-based reinforcement learning for atari. arXiv preprint arXiv:1903.00374,
2019.
Timothee Lesort, Natalia Diaz-Rodriguez, Jean Frangois Goudou, and David Filliat.
State representation learning for control: An overview. Neural Networks, 108:
S0893608018302053-, 2018.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian
inference algorithm. Advances in Neural Information Processing Systems 29, pp. 2378-
2386, 2016.
Yang Liu, Prajit Ramachandran, Qiang Liu, and Jian Peng. Stein variational policy gradi-
ent. arXiv preprint arXiv:1704.02399, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. Com-
puter Science, 2013.
9
Under review as a conference paper at ICLR 2020
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, and Koray
Kavukcuoglu. Asynchronous methods for deep reinforcement learning. International
Conference on Machine Learning, pp. 1928—1937, 2016.
Ofir Nachum, Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical rein-
forCement learning. Neural Information Processing Systems, pp. 3307-3317, 2018a.
Ofir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Near-optimal representation
learning for hierarchical reinforcement learning. International Conference on Learning
Representations, 2018b.
Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, and Sergey Levine. Vari-
ational discriminator bottleneck: Improving imitation learning, inverse rl, and gans by
constraining information flow. International Conference on Learning Representations,
2018.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via
information. arXiv preprint arXiv:1703.00810, 2017.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Ried-
miller. Deterministic policy gradient algorithms. In International Conference on Interna-
tional Conference on Machine Learning, pp. 387-395, 2014.
Jakub Sygnowski and Henryk Michalewski. Learning from the memory of atari 2600. arXiv
preprint arXiv:1605.01335, 2016.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck
method. University of Illinois, 411(29-30):368-377, 2000.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double
q-learning. Computer Science, pp. 2094-2100, 2015.
A Appendix
A.1 Proof of Theorem 1
Theorem. (Theorem 1 restated)Policy πr = πθr, parameter φr, optimal policy π? = πθ? and
its relevant representation parameter φ? are defined as following:
θ ,Φ' =argmin EpΦ(χ,z)[log 彳’(ZZ 啖) - 1 J(Z; θ)]	(24)
θ,φ	Pφ(Z ) β
θ?,φ* = argminEPφ(X,Z)[-1J(Z； θ)]	(25)
θ,φ	β
Define Jπr as EPφr (X,Z) [J(Z; θr)] and Jπ? as EPφ? (X,Z) [J(Z; θ?)]. Assume that for any
e > 0, ∖I(X, Z; φ?) — I(X, Z; φr) | < β, We have ∖Jπr — Jπ? | < E. Specifically, in value-based
algorithm, this theorem also holds between expectation of two value functions.
Proof. From equation(24) We can get:
I(X, Z; φ?) — 1 Jπ? ≥ I(X, Z; φr) — 1Jπr	(26)
ββ
From equation(25) We can get:
—1 Jn ≥ — 1 Jπ?
β ~ β
These tWo equations give us the folloWing inequality:
β(I(X, Z; φ*) — I(X, Z; φτ)) ≥ Jπ? — Jn ≥ 0
(27)
(28)
10
Under review as a conference paper at ICLR 2020
According to the assumption, naturally we have:
IJπr - Jπ? I <e
(29)
Notice that if we use our IB framework in value-based algorithm, then the objective function
Jπ can be defined as:
Jπ = Vπ = (1 - γ)-1
(1 - γ)-1
dXdπ(X)Rπ(X)
X
X dXdπ(X)[ ZdZPφ(ZIX)Rπ(Z)]
(30)
where Rπ (Z) = JX∈{χ0:Φ(X0)=z} dXRπ (X) and dπ is the discounted future state distribu-
tion, readers can find detailed definition of dπ in the appendix of (Chen et al., 2018). We
can get:
∣ Vπr - Vπ? I < 6	(31)
A.2 Target Distribution Derivation
We show the rigorous derivation of the target distribution in (11).
Denote P as the distribution of X, PφZ (Z) = Pφ(Z) as the distribution of Z. We use Pφ
as the short hand notation for the conditional distribution Pφ(ZIX). Moreover, we write
L(θ,φ) = L(θ,Pφ) and hp, qiχ = R dXp(X)q(X). Notice that Pφ(Z) =(P(∙),Pφ(Z∣∙RX.
Take the functional derivative with respect to Pφ of the first term L1 :
δL11(θ,Pφ) Λ	f,°"δL 1(θ,Pφ(ZIX))	Jd r S P q
δp-^pφ-, φ/XZ = J dZdX	δPφ(ZIX)	φ(z*)= ∖JL11(θ,Pφ + 6叽=0
=d6 ] dXP(X)DPφ(^IX) + 6Φ(∙,X),J(∙; θ) - βlog(Pφ(-IX) + 6Φ(∙,X)))Z
=J dXP(X) Dφ(∙,X),J(∙; θ) - βlogPφ(^IX))+ DPφ(^IX), -βp5T(XEZ
=DP(∙)[J(∙; θ) - βlogPφ(∙I∙) - β], Φ(∙,∙)〉xz
=0
Hence, we can see that
δL I (θ,Pφ)
δPΦ(ZX)
P(X)[J(Z; θ) - βlogPφ(ZIX) - β].
Then we consider the second term. By the chain rule of functional derivative, we have that
δL 2( θ,Pφ)
δPΦ(ZX)
口2((，P) SP(∙) ∖ = β ∕1 + ioεPZ(∙)必(.) ∖
δ SPZ (∙) ,δP>4( (ZIX) ∕z β∖ + g φ () ,δPφ (ZIX) ∕z
β/ dZ(1 + logPZ(Z))δ(Z - Z)P(X) = βP(X)(1 + logPZ(Z))
(32)
Combining the derivative of L1 and L2 and setting their summation to 0, we can get that
Pφ(ZIX) 8 Pφ(Z)exp(1 J(Z; θ))
β
(33)
A.3 Proof of Theorem 2
Theorem. (Theorem 2 restated)For L(θ, φ) = EX〜P(X)Z〜p@(z∣x)[J(Z; θ)] — βI(X, Z; φ),
given a fixed policy-value parameter θ, representation distribution Pφ(ZIX) and state dis-
tribution P(X), define a new representation distribution:P^(ZX) 8 Pφ(Z) exp(β J(Z; θ)),
ʌ
we have L(θ, φ) ≥ L(θ, φ).
11
Under review as a conference paper at ICLR 2020
Proof. Define I(X) as:
I (X )= d dZPφ( Z ∣X )= d dZPφ (Z)exp(1 J (Z ； θ))
Z	Zβ
…P Pφ	r	E	r,	Pφ(Z)exp( 1J(Z；θ))
L(θ, φ) = EX{EZ~Pφ(Z∣X)[J(Z; θ)] 一 βEZ~Pφ(Z∣X)[log-T(χ∖p (7∖--
φ	φ	1 (X) Pφ( Z)
=EX{βEZ~P^(Z∣X) [log I(X)] 一 βEZ~P^(Z∣X)[log pφ) / ]}
Pφ( Z)
]}
=βEX [logI(X)] 一 βEX,Z~P^(X,Z)[log pφ(ZZ)]
=βEX [log I(X)] 一 βEZ~P^(Z)[log pφ) / ]
Pφ( Z)
=β E X~P( X )[log I (X)]+ β D KL (Pφ( Z) l∣Pφ (Z))
L(θ, φ) = EX{βEZ~Pφ(Z∣X) [logeχp(- J(Z; θ))] + βEZ~Pφ(Z∣X) log P φ>R∖
β	Pφ (Z |X )
}
Pφ(Z)exp( 1J(Z; θ))
E X {β E …ZX) [log	Pφ (ZXβ I( X)	]+ β log I(X)}
P^( Z ∣X)
βEX[logI(X)] + βEX~P(X),Z~Pφ(Z∣X) [log pφ(Z|X)]
βEX~P( X )[log I (X)] — β E X~P (X )[D KL (Pφ (Z |X) ∣∣Pφ( Z|X))]
L(θ, Φ) - L(θ, Φ) = βDKL(Pφ(Z)∣∣Pφ(Z)) + βEx~p(X)[DKL(Pφ(Z|X)∣∣Pφ(Z|X))]
(34)
(35)
(36)
(37)
According to the positivity of the KL-divergence, We have L(θ, φ) ≥ L(θ, φ).
A.4 Algorithm
Algorithm 1 Information-bottleneck-based state abstraction in RL
θ, φ — initialize network parameters
β, Z — initialize hyper-parameters in (17)
E — learning rate
M — number of samples from Pφ(∙∣X)
repeat
Draw a batch of data {Xt, at, Rt, Xt+1}tn=1 from environment
for each Xt ∈ {Xt}tn=1 do
Draw M samples {Z∩^=1 from Pφ(∙∖Xt)
end for
Get the batch of data D = {Xt, {Zit}iM=1, at, Rt, Xt+1}tn=1
Compute the representation gradients Vφ L(θ, φ) in D according to (18)
Compute the RL gradients N § L (θ, φ) in D according to (19)
Update φ: φ  φ + ENφL(θ, φ)
Update θ: θ J θ + NL(θ, φ)
until Convergence
A.5 Integrate MINE to our framework
MINE can also be applied to the problem of minimizing the MI between Z and X where Z
is generated by a neural network Pφ(∙∖X):
I?(X,Z) = mφnmaxEpΦ(x,z)[T(X、Z;η)] - log(EP(X)®p$(z)[expT(X,Zη)])	(38)
12
Under review as a conference paper at ICLR 2020
Apparently I? (X, Z) is 0 without any constraints, yet if we use MINE to optimize our IB
framework in (6), I? (X, Z) might not be 0. With the help of MINE, like what people did
in supervised learning, the objective function in (6) can be written as:
L(θ, φ, η) = min{EPφ(X,Z)[J(Z; θ)] -βEPφ(X,Z)[T (X, Z; η)]
η |----------------------{z-----}
RL loss term
+ β log(EP(X)心Pφ(Z) [expT(X,Z;η)])}	(39)
The key steps to optimize L(θ, φ, η) is to update θ, φ, η iteratively as follows:
ηt +1 J arg min — E Pφt( XZ)[ T (X,Z ； ηt)] + log(E P (X)心 Pφt( Z) [exp T(*,,;ηt)])	(40)
ηt |----------------------------{----------------------------}
mutual information term
θt+1, φt+1 J argmaxL(θt,φt,ηt+1)	(41)
θt,φt
Yet in our experiment, these updates of θ, φ, η did not work at all. We suspect it might be
caused by the unstability of T(X, Z; η). We have also found that the algorithm always tends
to push parameter φ to optimize the mutual information term to 0 regardless of the essential
RL loss J(Z; θ): In the early stage of training process, policy π is so bad that the agent is
unable to get high reward, which means that the RL loss is extremely hard to optimize. Yet
the mutual information term is relatively easier to optimize. Thus the consequence is that
these updates tend to push parameter φ to optimize the mutual information term to 0 in
the early training stage.
However, MINE’s connection with our framework makes it possible to optimize T(X, Z; η)
in a more deterministic way and put the RL loss into mutual information term.
According to equation(23), T(X, Z; η) = ɪ J(Z; θ) + C0, this implies that We could introduce
another function T in place of T(X, Z; η) for the sake of variance reduction:
1
T(X; η) J & J(Z; θ) + TX，Z;η)
(42)
in the sense that parameter η only needs to approximate the constant C0 (T), the optimiza-
tion steps turn out to be:
__	入
θt+1, φt+1, ηt+1 J arg max arg min{-βEPφ (X,Z)[T (X, Z; ηt)]+
θt,φt	ηt	φt
β log(E P (X) *( Z)[expT XZ; ηt)+1 J Z;叼)}
This algorithm has the folloWing three potential advantages in summary:
(43)
1.
2.
3.
We’re able to optimize T(X, Z; η) in a more deterministic way instead of the form
like equation(40), which is hard to converge in reinforcement learning.
We prevent excessive optimization in mutual information term by putting RL loss
J(Z; θ) into this term.
We’re able to directly optimize the IB framework without constructing a variational
lower bound.
Here We just analyze and derive this algorithm theoretically, implementation and experi-
ments Will be left as the future Work.
Notice that We say if We directly optimize our IB frameWork With MINE, one of the problems
is that the function T might be unstable in RL, yet in section(4.4) and experiments(A.6),
We directly use MINE to visualize the MI. This is because When We optimize our frameWork,
We need to start to update T every training step. While When We visualize the MI, We start
to update T every 2000 training steps. Considering the computation efficiency, every time
We start to update T When We use MINE to optimize our frameWork, We must update T in
one step or a feW steps. While When visualizing the MI, We update T in 256 steps. Besides,
We reset parameter η every time We begin to update T When We visualize the MI, clearly We
can’t do this When optimizing our frameWork since it’s a min-max optimization problem.
13
Under review as a conference paper at ICLR 2020
A.6 Study the information-bottleneck perspective in RL
Now we introduce the experimental settings of MI visualization. And we show that the
agent in RL usually tends to follow the information E-C process.
We compare the MI(I (X, Z)) between A2C and A2C with our framework. Every 2000
update steps(2560 frames each step), we re-initialize the parameter η, then sample a batch
of inputs and their relevant representations {Xi, Zi = φ(Xi, )}in=1, n = 64, and compute the
MI with MINE. The learning rate of updating η is same as openai-baselines’ A2C: 0.0007,
training steps is 256 and the network architecture can be found in our code file ”policy.py”.
Figure(3) is the MI visualization in game Qbert. Note that there is a certain degree of
fluctuations in the curve. This is because that unlike supervised learning, the distribution
of datasets and learning signals Rπ (X) keep changing in reinforcement learning: Rπ (X)
changes with policy π and when π gets better, the agent might find new states, in this case,
I(X, Z) might increase again because the agent needs to encode information from new states
in order to learn a better policy. Yet finally, the MI always tends to decrease. Thus we can
say that the agent in RL usually tends to follow the information E-C process.
(a) MI in A2C
Figure 3: Mutual information visualization in Qbert. As policy π gets better, the agent
might find new states, in this case, I(X, Z) might increase again because the agent needs
to encode information from new states in order to learn a better policy. Yet finally, the MI
always tends to decrease. Thus it follows the information E-C process.
(b) MI in A2C with our framework
We argue that it’s unnecessary to compute I(Z, Y ) like (Shwartz-Ziv & Tishby, 2017):
According to (3), if the training loss continually decreases in supervised learning(Reward
continually increases as showed in figure(2)(a) in reinforcement learning), I(Z, Y ) must
increase gradually.
We also add some additional experimental results of MI visualization in the appendix(A.7).
A.7 Additional experimental results of performance and MI visualization
This section we add some additional experimental results about our framework.
Notice that in game MsPacman, performance of A2C with our framework is worse than
regular A2C. According to the MI visualization of MsPacman in figure(5)(b), we suspect that
this is because A2C with our framework drops the information from inputs so excessively
that it misses some information relative to the learning process. To see it accurately, in
figure(5)(b), the orange curve, which denotes A2C with our framework, from step(x-axis)
80 to 100, suddenly drops plenty of information. Meanwhile, in figure(4)(b), from step(x-
axis) 80 to 100, the rewards of orange curve start to decrease.
As showed in figure(6), unlike Pong, Breakout, Qbert and some other shooting games, the
frame of MsPacman contains much more information related to the reward: The walls, the
ghosts and the tiny beans everywhere. Thus if the agent drops information too fast, it may
hurt the performance.
14
Under review as a conference paper at ICLR 2020
(a) A2C-Assault
(b) A2C-MsPacman
Figure 4: Additional results of performance: Average rewards over 10 episodes
(c) A2C-Breakout
(a) MI in Assault
(b) MI in MsPacman
Figure 5: Additional results of MI visualization
(a) MsPacman
Figure 6: The raw frame of MsPacman
15