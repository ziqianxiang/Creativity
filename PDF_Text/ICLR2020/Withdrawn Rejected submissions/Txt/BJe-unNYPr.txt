Under review as a conference paper at ICLR 2020
Accelerated Information Gradient flow
Anonymous authors
Paper under double-blind review
Ab stract
We present a systematic framework for the Nesterov’s accelerated gradient flows
in the spaces of probabilities embedded with information metrics. Here two met-
rics are considered, including both the Fisher-Rao metric and the Wasserstein-2
metric. For the Wasserstein-2 metric case, we prove the convergence properties
of the accelerated gradient flows, and introduce their formulations in Gaussian
families. Furthermore, we propose a practical discrete-time algorithm in parti-
cle implementations with an adaptive restart technique. We formulate a novel
bandwidth selection method, which learns the Wasserstein-2 gradient direction
from Brownian-motion samples. Experimental results including Bayesian infer-
ence show the strength of the current method compared with the state-of-the-art.
1	Introduction
Recently, optimization problems on the space of probability and probability models attract increas-
ing attentions from machine learning communities. These problems include variational inference
(Blei et al., 2017), Bayesian inference (Liu & Wang, 2016), Generative Adversary Networks (GAN,
Goodfellow et al. (2014)), and policy optimizations (Zhang et al., 2018), etc. For instance, vari-
ational inference methods approximate a target density by minimizing the Kullback-Leibler (KL)
divergence as the loss (objective) function.
Gradient descent methods with sampling efficient properties play essential roles to solve these opti-
mization problems. Here the gradient descent direction often relies on the information metric over
the probability space. This direction naturally reflects the change of the loss function with respect to
the metric. In literature, two important information metrics, such as the Fisher-Rao metric and the
Wasserstein-2 (in short, Wasserstein) metric, are of great interests (Amari, 1998; Otto, 2001; Laf-
ferty, 1988). For the Fisher-Rao gradient, classical results including Adam (Kingma & Ba, 2014)
and K-FAC (Martens & Grosse, 2015) demonstrate its effectiveness in probability models. For the
Wasserstein gradient, many classical methods such as Markov chain Monte Carlo (MCMC) meth-
ods (Geman & Geman, 1987; Neal et al., 2011; Welling & Teh, 2011) and particle-based variational
inference (ParVI) methods (Liu & Wang, 2016; Chen & Zhang, 2017; Chen et al., 2018) are based
on this framework in the probability space. The strength of using the Wasserstein gradient is also
shown in probability models such as GANs. (Arjovsky et al., 2017; Lin et al., 2018; Li et al., 2019).
The Nesterov’s accelerated method (Nesterov, 1983) is widely applied in accelerating the vanilla
gradient descent under the Euclidean metric. It corresponds to a damped Hamiltonian flow, known
as the accelerated gradient flow (Su et al., 2016). A natural question is whether there exists a
counterpart of the accelerated gradient flow in the probability space under information metrics. For
optimization problems on a Riemannian manifold, the accelerated gradient methods are studied by
Liu et al. (2017); Zhang & Sra (2018). The probability space embedded with information metric can
be viewed as a Riemannian manifold. Several previous works explore accelerated methods in this
manifold under the Wasserstein metric. Liu et al. (2018; 2019) propose an acceleration framework of
ParVI methods based on manifold optimization. Taghvaei & Mehta (2019) introduce the accelerated
flow from an optimal control perspective. On the other hand, Cheng et al. (2017); Ma et al. (2019)
explore and analyze the acceleration on MCMC, based on the underdamped Langevin dynamics.
In this paper, we present a unified framework of accelerated gradient flows in the probability space
embedded with information metrics, named Accelerated Information Gradient (AIG) flows. From an
information-differential-geometry perspective, we derive AIG flows by damping Hamiltonian flows,
concerning both the Fisher-Rao metric and the Wasserstein metric. Then we focus on the Wasser-
1
Under review as a conference paper at ICLR 2020
stein metric with the KL divergence loss function. In Gaussian families, we verify the existence
of the solution to AIG flows. Here we show that the AIG flow corresponds to a well-posed ODE
system in the space of symmetric positive definite matrices. We rigorously prove the convergence
rate of AIG flows based on the geodesic convexity of the loss function. Here we note that our proof
removes the unnecessary technical assumption in (Taghvaei & Mehta, 2019, Theorem 1).
Besides, we handle two difficulties in numerical implementations of AIG flows. On the one hand, as
pointed out by Taghvaei & Mehta (2019); Liu et al. (2019), the logarithm of density term (Wasser-
stein gradient of KL divergence) is hard to approximate in particle formulations. We propose a novel
kernel selection method, whose bandwidth is learned by sampling from Brownian motions. We call
it the BM method. On the other hand, we notice that the AIG flow can be a numerically stiff system,
especially in high-dimensional sample spaces. This is because the solution of AIG flows can be
close to the boundary of the probability space. To handle this issue, we propose an adaptive restart
technique, which accelerates and stabilizes the AIG algorithm. Numerical results in toy examples,
Gaussian measures and Bayesian Logistic regression indicate the validity of the BM method and the
acceleration effects of the proposed AIG flow.
This paper is organized as follows. Section 2 briefly reviews the information metrics and their corre-
sponding gradient flows and Hamiltonian flows in the probability space. In Section 3, we formulate
various forms of AIG flows and analyze W-AIG flows in Gaussian measures. We theoretically prove
the convergence rate of W-AIG flows in Section 4. Section 5 presents the discrete-time algorithm
for W-AIG flows, including the BM method and the adaptive restart technique. Section 6 provides
numerical experiments.
2	Metric and flows in the probability space
Suppose that Ω is a region in Rn. Let F(Ω) denote the set of smooth functions on Ω.〈•, •)and ∣∣ ∙ ∣∣
are the Euclidean inner product and norm in Rn. V, ▽• and ∆ represent the gradient, divergence
and Laplacian operators in Rn . Denote the set of probability density
P(Ω) = {ρ ∈ F(Ω):
ρdx
Jω
1,
ρ≥0 .
The tangent space at P ∈ P(Ω) follows Tρ(Ω) = {σ ∈ F(Ω) : / σdx = 0.}. The cotangent space
at ρ, TPP(Ω), can be treated as the quotient space F(Ω)∕R, which are functions in F(Ω) defined
up to addition of constants.
Definition 1 (Metric in the probability space) A metric tensor G(ρ) : TPP(Ω) → TPP(Ω) is an
invertible mapping from the tangent space at ρ to the cotangent space at ρ. This metric tensor defines
the metric (inner product) on the tangent space TPP(Ω). Namely, for σι, σ? ∈ TPP(Ω), We define
gρ(σ1,σ2)
σ1 G(ρ)σ2 dx
Φ1G(ρ)-1Φ2dx,
where Φi is the solution to σi = G(ρ)-1Φi, i = 1, 2.
We present two important examples of metrics on the probability space P (Ω): the Fisher-Rao metric
from information geometry and the Wasserstein metric from optimal transport.
Example 1	(Fisher-Rao metric) The inverse of the Fisher-Rao metric tensor is defined by
GF(ρ)-1Φ=ρ
Φ - Φρdx
Φ ∈ TPP(Ω).
The Fisher-Rao metric on the tangent space is given by
gPF(σ1,σ2)
Φ1Φ2ρdx
Φ1ρdx
Φ2ρdx
σ1,σ2 ∈ TρP(Ω),
where Φi is the solution to σi = ρ Φi - Φiρdx , i = 1, 2.
Example 2	(Wasserstein metric) The inverse of the Wasserstein metric tensor is defined by
GW(ρ)-1Φ = -V ∙ (ρVΦ),	Φ ∈ TPP(Ω).
2
Under review as a conference paper at ICLR 2020
The Wasserstein metric on the tangent space is given by
gW(σ1,σ2) = / P hVΦ1, VΦ2i dx,	σ1,σ2 ∈ TρP(Ω),
where Φi is the solution to σ% = -V ∙ (pVΦJ, i = 1,2.
2.1 Gradient flows
In learning, many problems can be formulated as the optimization problem in the probability space,
min E (P).
ρ∈P(Ω)	'
Here E(P) is a divergence or metric loss functional between P and a target density p* ∈ P(Ω). One
typical example of E(P) is the KL divergence from P to p*,
E(P) = DKL(PkP*) = /log (/) Pdx.
Another example is the maximum mean discrepancy (MMD, Gretton et al. (2012)),
E(P) = MMD(P,P*) = //(P(X)-P*(χ))K(x,y)(P(y) -P*(y))dχdy,
where K(x, y) is a given kernel function. The gradient flow for E(P) in (P(Ω),gρ) takes the form
∂tρt
-G(Pt)-I F
δPt
Here ^EPPt) is the L2 first variation w.r.t. ρt. We formulate the gradient flow under either the
Fisher-Rao metric or the Wasserstein metric.
Example 3	(Fisher-Rao gradient flow) The Fisher-Rao gradient flow is given by
∂tρt = -ρt
δE
δρt
Example 4	(Wasserstein gradient flow) The Wasserstein gradient flow writes
∂tρt = V ∙	Pt
2.2 Hamiltonian flow
In this subsection, we briefly review the Hamiltonian flow in the probability space. By using the
metric gρin the probability space, we can define a Lagrangian by
L(ρt, dtρt) = 2gρt(dtρt, dtρt) - E(Pt).
The Euler-Lagrange equation for the Lagrangian follows
δL	δL
dt i	—ʌ I = ʌ—+ C(t),
δ(∂tPt)	δPt
(1)
where C(t) is a spatially-constant function.
Proposition 1 Ifwe let Φt = δL∕δ(∂tρt) = G(ρt)∂tPt, the equation (1) can be formulated as a
system of (Pt, Φt), i.e.,
(∂tpt - G(Pt)TΦt = 0,
∂tΦt +
δE
+ δρ~ = 0,
(2)
where Φt is up to a spatially-constant function shrift. Here (2) is the Hamiltonian flow
∂t
ρt
Φt
0
-1
1] ∣"δ⅛H(Pt，φt)
0	δΦt H(Pt, Φt)
0,
(3)
—
with respect to the Hamiltonian H(ρt , Φt)
2 R φtG(pt)-1φtdχ + E(Pt).
3
Under review as a conference paper at ICLR 2020
Similarly, we can write the Hamiltonian flow under the Fisher-Rao metric or the Wasserstein metric.
Example 5	(Fisher-Rao Hamiltonian flow) The Fisher-Rao Hamiltonian flow follows
∂tρt - Φtρt + Φtρtdx = 0,
dtφt + 2φ2
δE
PtΦtdx Φt + L
δρt
0.
The corresponding Hamiltonian is HF(ρt, Φt) = 2 (R Φ1ρtdx - (R ρtΦtdx)2) + E(Pt).
Example 6	(Wasserstein Hamiltonian flow) The Wasserstein Hamiltonian flow writes
dtPt + ▽ ∙ (PtVφt) = 0,
1	δE
∂tΦt +2 kVΦtk2 + 而=0.
The corresponding Hamiltonian is HW(Pt, Φt) = 2 / ∣∣VΦtk2ρtdx + E(ρt). This is identical to
the Wasserstein Hamiltonian flow introduced by Chow et al. (2019).
3 Accelerated information gradient flow
Let αt ≥ 0 be a scalar function of t. We add a damping term αtΦt to the Hamiltonian flow (3).
∂ Pt +	0
∂t Φt + αtΦt
11 ∣^δpt H(Pt,φt)
0I [δ⅛H(Pt, φt)
(4)
—
0
-
This renders the Accelerated Information Gradient (AIG) flow
(∂tρt - G(ρt)-1Φt = 0,
∂tΦt + αtΦt +
δE
+ δρ^ = O,
(AIG)
with initial values P0 = P0 and Φ0 = 0. The choice of αt depends on the geodesic convexity of
E(P), which has an equivalent definition as follows.
Definition 2 For a functional E(P) defined on the probability space, we say that E(P) has a β-
positive Hessian (in short, Hess(β)) w.r.t. the metric gρ if there exists a constant β ≥ 0 such that for
any P ∈ P(Ω) and any σ ∈ TPP(Ω), We have
gρ(Hess E(P)σ, σ) ≥ βgρ(σ,σ).
Here Hess is the Hessian operator W.r.t. gρ.
If E(ρ) is Hess(β) for β > 0, then at = 2√β; if E(ρ) is Hess(O), then at = 3/t.
Remark 1 The Nesterov’s accelerated method (Nesterov, 1983) is a first-order method to optimize
f(x) in the Euclidean space. The corresponding accelerated gradient floW by Su et al. (2016) is
equivalent to a damped Hamiltonian system
x +
atp
0 I	VxHE (x, p)
-I 0	VpHE (x, p)
0,
HE (x,p) = 1 kpk2 + f(x).
0
—
with initial values x(0) = xo and p(0) = 0. The choice of at depends on the property of f (x).
If f (x) is β-strongly convex, then at = 2√β; if f (x) is convex, then at = 3/t. We apply this
Hamiltonian flow interpretation to construct (AIG) in the probability space with information metrics.
We give examples of AIG flows under either the Fisher-Rao metric or the Wasserstein metric.
Example 7	(Fisher-Rao AIG flow) The Fisher-Rao AIG flow writes
(F-AIG)
4
Under review as a conference paper at ICLR 2020
Example 8	(Wasserstein AIG flow) The Wasserstein AIG flow writes
∂tρt + V∙ (ρtVΦt)=0,
1	δE
∂tΦt + αtΦt + 2 ∣∣VΦtk2 + δ~ =0.
(W-AIG)
For the rest of this paper, we mainly focus on the Wasserstein metric. Here the AIG flow (Eulerian
formulation in fluid dynamics) has a counterpart in the particle level (Lagrangian formulation).
Proposition 2 Suppose that Xt 〜Pt and Vt = VΦt(Xt) are the position and the velocity of a
particle at time t. Then, the differential equation of the particle system corresponding to (W-AIG)
writes
dXt = Vtdt	dVt = -αtVt dt -V (	) (Xt)dt.	(W-AIG-P)
If E(P) evaluates the KL divergence and ρ* α exp(-f (x)), (W-AIG-P) is equivalent to
dXt = Vtdt,	dVt = -αtVtdt - Vf(Xt)dt - Vlogρt(Xt)dt.
(W-AIG-P-KL)
Remark 2 The V logPt(Xt)dt term cannot be simply replaced by a Brownian motion dBt because
Pt is the marginal distribution on Xt . Several previous works have studied the accelerated gradient
flow of KL divergence in the probability space under the Wasserstein metric. Taghvaei & Mehta
(2019) construct the accelerated gradient flow in the probability space based on Wibisono et al.
(2016)’s variational formulation on the Nesterov’s accelerated method. Their flows coincide with
(W-AIG-P-KL) with αt = 3/t after rescaling. The underdamped Langevin dynamics in (Cheng
et al., 2017; Ma et al., 2019) damps the Hamiltonian flow of the particles, which is different from
(W-AIG-P) as shown in (Taghvaei & Mehta, 2019). Liu et al. (2018; 2019) give the discrete-time
accelerated algorithm from the perspective of manifold optimization.
3.1 Wasserstein metric restricted to Gaussian
In this subsection, we demonstrate that (W-AIG) has an ODE formulation in Gaussian family. De-
note Nn to the multivariate Gaussian densities with zero means. Namely, if ρ0, ρ* ∈ Nn, then We
show that (W-AIG) has a solution (Pt, Φt) and Pt ∈ Nn0.
Let Pn and Sn represent symmetric positive definite matrix and symmetric matrix with size n × n
respectively. Each P ∈ Nn0 can be uniquely expressed by its covariance matrix ∑ ∈ Pn by P(x; ∑) =
√)”) exp (—1XT∑-1χ) . The Wasserstein metric on P(Rn) induces a metric on Nn0, which is a
totally-geodesic submanifold in P(Rn), see (Takatsu, 2008; Modin, 2016; Malago et al., 2018). So
there exists a Wasserstein metric on Pn , also known as the Bures metric. For ∑ ∈ Pn , the tangent
space and cotangent space follow T∑Pn ` T∑Pn ` Sn.
Definition 3 (Wasserstein metric in Gaussian) For ∑ ∈ Pn , the metric tensor G(∑) : Sn → Sn
is defined by	G(∑)-1S	=	2(∑S	+	S∑). The Wasserstein metric on	Sn	is	gΣ(A1, At)	=
tr(A1G(∑)At) = tr(S1∑St), where Si ∈ Sn is the solution to Ai = ∑Si +Si∑, i = 1,2.
Proposition 3 Given an energy function E(∑), the Wasserstein gradient flow in Gaussian writes
∑ t = -2(∑tV∑t E(∑t) + V∑t E(∑t)∑t).
Here VΣt is the standard matrix derivative. The Hamiltonian flow is a system of (∑t, St), i.e.,
(∑ t-(St∑t + ∑tSt ) = 0,
+ St +2V∑t E(∑t) = 0.
The corresponding Hamiltonian writes H(∑t, St) = tr(St∑tSt) + 2E(∑t).
(5)
Therefore, by adding the damping term αtSt, we obtain the Wasserstein AIG flow in Gaussian.
JΣ t-(St∑t + ∑tSt ) = 0,
[St + αtSt + St + 2V∑t E(∑t) = 0,
(W-AIG-G)
5
Under review as a conference paper at ICLR 2020
with initial values Σ0 = Σ0 and S0 = 0. For now, we consider E(Σ) to be the KL divergence.
E(Σ)，E(ρ( ∙ K)) = 2 [tr(∑(∑*)-1) — logdet(Σ(Σ*)-1) - n],	⑹
where Σ* is the covariance matrix of ρ*. The following theorem proves the Well-Posedness of
(W-AIG-G) and illustrates the connection between W-AIG flows in Pn and P(Rn).
Theorem 1 Suppose that ρ0, ρ* ∈ Nn and their covariance matrices are Σ0 and Σ*. E(Σ) defined
in (6) evaluates the KL divergence from P to ρ*. Let (∑t, St) be the solution to (W-AIG-G) with
initial values Σ0 = Σ0 and S0 = 0. Then, for any t ≥ 0, Σt is well-defined and stays positive
definite. Furthermore, we denote
Pt(X)=pne") exp(—1XT ς-1 x),②(X)=1XT StX+C (t),
where C(t) = —t + 2 RtlOgdet(∑s(Σ*)-1)ds. Then, (ρt, Φt) is the solution to (W-AIG) with
initial values P0 = P0 and Φ0 = 0.
4 Convergence rate analysis on W-AIG flows
In this section, we prove the convergence rate of (W-AIG).
Theorem 2 Suppose that E(P) satisfies Hess(β) for β > 0. The solution Pt to (W-AIG) with
at = 2 √β satisfies
E(Pt) ≤ C0e-√βt = O(e-√βt).
IfE(P) only satisfies Hess(0), then the solution Pt to (W-AIG) with αt = 3/t satisfies
E(Pt) ≤ C00t-2=O(t-2).
Here the constants C0, C00 only depend on P0.
Remark 3 Here Hess(β) is equivalent to the β-geodesic convexity in the probability space w.r.t.
gρ . For the Wasserstein metric, it is also known as β-displacement convexity; see (Villani, 2008,
Chap 16). Consider the case where E(P) is the KL divergence and the target density takes the form
P* 8 exp(-f (x)). A sufficient condition for Hess(β) is that f (x) is β-strongly convex, see (Otto
& Villani, 2000; Bakry & Emery, 1985). If E(P) satisfies Hess(β) for β > 0, then the classical
analysis indicates that the solution to the Wasserstein gradient flow has an O(e-2βt) convergence
rate. The W-AIG flow improves the convergence rate to O(e-Vzβt), especially when β is close to 0.
Here we provide a sketch in the proof of Theorem 2. Given Pt, we can find the optimal transport
plan Tt from Pt to P* . Let T #P denote the push-forward density from P by the mapping T . The
following proposition characterizes the inverse of the exponential map in probability space with the
Wasserstein metric.
Proposition 4 Denote the geodesic curve γ(s) that connects Pt and P* by γ(s) = (sTt + (1 -
S)Id)#Pt, S ∈ [0,1]. Here Id is the identity mapping from Rn to itself. Then, γ(0) corresponds to
a tangent vector —V ∙ 3(x)(Tt(x) — x)) ∈ TPtP(Ω).
We first consider the case where E(P) satisfies Hess(β) for β > 0. Motivated by the Lyapunov
function for Nesterov’s ODE in the Euclidean case, we construct the following Lyapunov function.
E(t) = e√βt Q / ∣∣-Pβ(Tt(X)- x) + vφt(χ)U Pt(χ)dχ + E(Pt)- E(P*)).	⑺
Proposition 5 Suppose that E(P) satisfies Hess(β) for β > 0. Pt is the solution to (W-AIG) with
αt = 2√β. Then, E(t) defined in (7) satisfies E(t) ≤ 0. As a result,
E(Pt) ≤ e-√βtE(t) ≤ e-√βtE(0) = O(e-√βt).
6
Under review as a conference paper at ICLR 2020
Note that E (0) only depends on ρ0. This proves the first part of Theorem 2. We now consider the
case where E(ρ) satisfies Hess(0). We construct the following Lyapunov function.
1	t	2	t2
E(t) = 2 J -(Tt(X)- X) + 2▽6(X) Pt(X)dx + W(E(Pt)- E(P )).	⑻
Proposition 6 Suppose that E(ρ) satisfies Hess(0). ρt is the solution to (W-AIG) with αt = 3/t.
Then, E(t) defined in (8) satisfies E(t) ≤ 0. As a result,
E(Pt) ≤ t2E(t) ≤ t2E(0) = O(t-2).
Because E(0) only depends on P0, we complete the proof.
Remark 4 For the Hess(0) case, we obtain the same result in (Taghvaei & Mehta, 2019,
Theorem 1). Their proof comes from the Lagrangian formulation (W-AIG-P) and our
proof is based on the Eulerian formulation (W-AIG). However, their technical assumption
E [(Xt + e-γt Yt — Tρ∞ (Xt)) ∙ dtTρ∞ (Xt)] = 0 is only valid in 1-dimensional case. In Appendix
C.4, we prove that this quantity is non-negative. This is due to the Hodge decomposition behind
the optimal transport, see Lemma 1 in Appendix C.3.
5 Discrete-time algorithm for W-AIG flows
In this section, we present the discrete-time implementation of (W-AIG-P-KL). This implementation
is simpler and more stable than the one in (Taghvaei & Mehta, 2019). Suppose that initial positions
of a particle system {X0}N=ι are given and 吗=0. The time parameter t is related to the step size
√τ via t = √Tk. The update rule follows
vki+ι = αkVk-√τ(Vf(Xk)+ξk(Xk)), Xk+ι = χk + √τvki+ι,	⑼
for i = 1,2 ...N. If E(P) is Hess(β), then ɑk = 1+√βT; if E(ρ) is Hess(0) or β is unknown, then
αk = k-1. Here ξk(x) is an approximation of V log Pt(x).
We review two common choices of ξk as follows. If Xki follows a Gaussian distribution, then
ξk(X) = -Σk-1(X - mk),	(10)
where mk and Σk are the mean and the covariance matrix of {Xki }iN=1. For the non-Gaussian
case, We use the kernel density estimation (KDE, Singh (1977)), Pk(x)=得 PN=I K(x, Xk) to
approximate Pt(X). Here K(X, y) is a kernel function. Then, ξk writes
ξk(x) = VlogPk(x)= PPN VxK(XYXk).	(11)
i=1 K(X, Xki )
A common choice of K(X, y) is a Gaussian kernel with the bandwidth h, KG (X, y; h) =
(2πh)-n/2 exp (∣∣x - y∣∣2∕(2h)). There are two difficulties in the discretization. For one thing, the
bandwidth h strongly affects the estimation of Vlog Pt, so we propose the BM method to adaptively
learn the bandwidth from samples. For another, the second equation in (W-AIG) is the Hamilton-
Jacobi equation, which usually has strong stiffness. We propose an adaptive restart technique to deal
with this problem.
Remark 5 Our numerical implementations of W-AIG flows can be viewed as a special case of
ParVI methods. Compared to traditional MCMC methods, ParVI methods are more sample-efficient
because make full use ofa finite number of particles by taking particle interaction into account.
5.1 Learn the bandwidth via Brownian motion
SVGD uses a median (MED) method to choose the bandwidth, i.e.,
hk = 2lθg(N+i)median ({kχk- Xkk2}N=I).	(12)
7
Under review as a conference paper at ICLR 2020
Liu et al. (2018) propose a Heat Equation (HE) method to adaptively adjust bandwidth. Motivated by
the HE method, we introduce the Brownian motion (BM) method to adaptively learn the bandwidth.
Given the bandwidth h, {Xi }iN=1 and a step size s, we can compute two particle systems
Yki(h)= Xk-sξk(x; h), Zk = Xk + √2SBi, i =1,...N,
where Bi is the standard BroWnian motion. We want to minimize MMD(PY, PZ), the MMD be-
tween the empirical distribution PY(x) = PN=I δ(x - Yki(h)) and PZ(x) = PN=I δ(x - Zk). Here,
the kernel K(x, y) for the MMD is the Gaussian kernel with a bandwidth of 1. So we optimize over
h to minimize MMD(PY, PZ), using the bandwidth hk-ι from the last iteration as the initialization.
For simplicity we denote BM(hk-1, {Xki }iN=1, s) as the output of the BM method.
Remark 6 Besides KDE, there are other methods that approximate the V log Pt(X) (compute ξk)
via a kernel function, such as the blob method (Carrillo et al., 2019) and the diffusion map (Taghvaei
& Mehta, 2019). The BM method can also select the kernel bandwidth for these methods.
5.2 Adaptive restart
To enhance the practical performance, we introduce an adaptive restart technique, which shares the
same idea of gradient restart in (Odonoghue & Candes, 2015; Wang et al., 2019) under the Euclidean
case. Consider
N
中k = - X〈Vi+1, Vf(Xk) + ξk (Xk )〉,	(13)
i=1
which can be viewed as discrete-time approximation of -gW(∂tPt, Gw(Pt)TδδE) = -∂tE(Pt).
If ψk < 0, then we restart the algorithm with initial values X0 = Xk and Vq = 0. This essentially
keeps ∂tE(Pt) negative along the trajectory. The numerical results show that the adaptive restart
accelerates and stabilizes the discrete-time algorithm. The overall algorithm is summarized below.
Algorithm 1 Particle implementation of Wasserstein AIG flow
Require: initial positions {Xqi}iN=1, step size τ, number of iteration L.
1:	Set k = 0, Vqi = 0, i = 1, . . . N. Set the bandwidth hq by MED (12).
2:	for l = 1, 2,...L do
3:	Compute hi based on BM: h = BM(hι-ι, {Xk}i=ι, √τ).
4:	Calculate ξk (Xki ) by (10) or by (11) with the bandwidth hl.
5:	Set ak based on whether E(P) is Hess(β) or Hess(0). For i = 1, 2,...N, update
Vi+1 = aVi - √τ(Vf(Xk) + ξk(Xk)),	Xk+1 = Xk + √TVi+ι.
6:	if RESTART then
7:	Compute ψk = - PL (Vi+1, Vf(Xk) + ξk(Xk)).
8:	If ψk < 0, set X0 = Xk and Vq = 0 and k = 0; otherwise set k = k + 1.
9:	else
10:	Set k = k + 1.
11:	end if
12:	end for
6	Numerical experiments
In this section, we present several numerical experiments to demonstrate the validity of the BM
method, the acceleration effect of the Wasserstein AIG flow, and the strength of the adaptive restart
technique. Implementation details can be found in Appendix D.
6.1	Toy example
We first investigate the validity of the BM method in selecting the bandwidth. The target density
P* is a toy bimodal distribution (Rezende & Mohamed, 2015). We compare two types of particle
implementations of the Wasserstein gradient flow over the KL divergence:
Xk+ι =	Xk	-	τVf(Xk)	+	√2TBk,	Xk+ι = Xk-T(Vf(Xk)+ξ(Xk)).
8
Under review as a conference paper at ICLR 2020
Here Bk 〜N(0,1) is the standard BroWnian motion and ξk takes the form (11). The first method
is known as the MCMC method and the second method is called the ParVI method. For the second
method, the bandWidth h is selected by MED/HE/BM respectively. Figure 1 shoWs the distribution
of 200 samples based on different methods. Samples from MCMC match the target distribution in a
stochastic Way; samples from MED collapse; samples from HE align tidily around the contour lines;
samples from BM arrange neatly and are closer to samples from MCMC. This indicates that the BM
method makes the particle system behave similar to MCMC, though in a deterministic Way.
Figure 1: The effect of the BM method. Samples are plotted as blue dots. Left to right: MCMC,
MED, HE and BM. All methods are run for 200 iterations With the same initialization.
6.2	Gaussian measures
Next, We explore the effectiveness of (W-AIG) floW compared to the Wasserstein gradient floW and
demonstrate the strength of the adaptive restart. The target density ρ* is a Gaussian distribution
with zero mean on R100, the covariance matrix of ρ* is Σ* and W * = (Σ* )-1. Let L and β be
the largest/smallest eigenvalue of W *. E (ρ) satisfies Hess(β) and the step size is T = 1∕(4L). The
condition number of W * is defined as K = L∕β. The large L indicates Σ* is close to be singular.
We first demonstrate the effectiveness of (W-AIG-G) in the ODE level. Detailed discretization is
left in Appendix D.2. The initial value is set to be Σ0 = I. For now, WGF denotes the discretization
of the Wasserstein gradient flow; AIG-(r)(s) denotes the discretization of the Wasserstein AIG flow.
For letters in the parentheses, ‘r’ denotes using the adaptive restart and ‘s’ denotes utilizing β . Figure
2 presents the convergence of the KL divergence on two target distributions with small/large L. We
observe that AIG converges faster compared to WGF, which verifies Theorem 2. The adaptive restart
also accelerates the algorithm.
Figure 2: The acceleration effect of W-AIG flow and the strength of adaptive restart (ODE level).
The target density is a Gaussian distribution with zero mean on R100. Left: L = 1, κ ≈ 3.8 × 103.
Right: β = 1, κ ≈ 4.0 × 103.
Then, we demonstrate the results in the particle level. The setting of ρ* is same as the previous
experiment. The initial distribution of samples follows N(0, I) and the number of samples is N =
600. For a particle system {X∖}N=1, we record the KL divergence E(Σk) (6) using the empirical
covariance matrix Σ k. The left part of Figure 3 (small L) is almost identical to Figure 2, which
verifies the acceleration effect of AIG flows. It also indicates that the adaptive restart helps to
accelerate the convergence. From the right part of Figure 3 (large L), AIG and AIG-s diverge
because of the ill target distribution, and the adaptive restart solves this problem.
9
Under review as a conference paper at ICLR 2020
Figure 3: The acceleration effect of W-AIG flow and the strength of adaptive restart (particle level).
The setting of target densities is identical to the ones in Figure 2.
6.3	Bayesian logistic regression
We perform the standard Bayesian logistic regression experiment on the Covertype dataset, follow-
ing the same settings as Liu & Wang (2016). We compare our methods with MCMC, SVGD (Liu
& Wang, 2016), WNAG (Liu et al., 2018) and WNes (Liu et al., 2019). We select the bandwidth
using either the MED method or the proposed BM method. Figure 4 indicates that the BM method
accelerates and stabilizes the performance of WGF and AIG. The performance of MCMC and WGF
are similar and they achieve the best log-likelihood. In test accuracy, AIG-r converges faster than
other methods and is more stable. The adaptive restart improves the overall performance of AIG.
Figure 4: Comparison of different methods on Bayesian logistic regression, averaged over 10 inde-
pendent trials. The shaded areas show the variance over 10 trials. Top: BM; Bottom: MED. Left:
Test accuracy; Right: Test log-likelihood.
7	Conclusion
In summary, we propose the framework of AIG flows by damping Hamiltonian flows with respect
to certain information metrics in the probability space. AIG flows have been carefully studied in
Gaussian families. Theoretically, we establish the convergence rate of W-AIG flows. Numerically,
we propose the discrete-time algorithm and the adaptive restart technique to overcome the numerical
stiffness of W-AIG flows. We introduce a novel kernel selection method by learning from Brownian-
motion samples. The numerical experiments verify the acceleration effect of AIG flows and the
strength of the adaptive restart. In future works, we intend to systematically explain the stiffness of
AIG flows and the effect of the adaptive restart. We shall apply our results to general information
metrics, especially for the Fisher-Rao metric. We expect to study the related sampling efficient
optimization methods and discrete-time algorithms on general probability models.
10
Under review as a conference paper at ICLR 2020
References
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251-
276, 1998.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214-223, 2017.
Dominique Bakry and Michel Emery. Diffusions hypercontractives. In Seminaire de Probabilites
XIX 1983/84, pp. 177-206. Springer, 1985.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti-
cians. Journal of the American Statistical Association, 112(518):859-877, 2017.
Jose Antonio Carrillo, Katy Craig, and Francesco S Patacchini. A blob method for diffusion. Cal-
culus of Variations and Partial Differential Equations, 58(2):53, 2019.
Changyou Chen and Ruiyi Zhang. Particle optimization in stochastic gradient mcmc. arXiv preprint
arXiv:1711.10927, 2017.
Changyou Chen, Ruiyi Zhang, Wenlin Wang, Bai Li, and Liqun Chen. A unified particle-
optimization framework for scalable bayesian sampling. arXiv preprint arXiv:1805.11659, 2018.
Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan. Underdamped langevin
mcmc: A non-asymptotic analysis. arXiv preprint arXiv:1707.03663, 2017.
Shui-Nee Chow, Wuchen Li, and Haomin Zhou. Wasserstein hamiltonian flows. arXiv preprint
arXiv:1903.01088, 2019.
Stuart Geman and Donald Geman. Stochastic relaxation, gibbs distributions, and the bayesian
restoration of images. In Readings in computer vision, pp. 564-584. Elsevier, 1987.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723-773, 2012.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
John D Lafferty. The density manifold and configuration space quantization. Transactions of the
American Mathematical Society, 305(2):699-741, 1988.
Wuchen Li, Alex Tong Lin, and Guido Montufar. Affine natural proximal learning. Geometric
science of information, 2019, 2019.
Alex Tong Lin, Wuchen Li, Stanley Osher, and Guido MOntUfar. Wasserstein proximal of gans.
CAM report 18-53, 2018.
Chang Liu, Jingwei Zhuo, Pengyu Cheng, Ruiyi Zhang, Jun Zhu, and Lawrence Carin. Ac-
celerated first-order methods on the wasserstein space for bayesian inference. arXiv preprint
arXiv:1807.01750, 2018.
Chang Liu, Jingwei Zhuo, Pengyu Cheng, Ruiyi Zhang, and Jun Zhu. Understanding and acceler-
ating particle-based variational inference. In International Conference on Machine Learning, pp.
4082-4092, 2019.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. In Advances in neural information processing systems, pp. 2378-2386, 2016.
Yuanyuan Liu, Fanhua Shang, James Cheng, Hong Cheng, and Licheng Jiao. Accelerated first-order
methods for geodesically convex optimization on riemannian manifolds. In Advances in Neural
Information Processing Systems, pp. 4868-4877, 2017.
11
Under review as a conference paper at ICLR 2020
Yi-An Ma, Niladri Chatterji, Xiang Cheng, Nicolas Flammarion, Peter Bartlett, and Michael I Jor-
dan. Is there an analog of nesterov acceleration for mcmc? arXiv preprint arXiv:1902.00996,
2019.
LUigi Malago, LUigi Montrucchio, and Giovanni Pistone. Wasserstein Iiemannian geometry of
positive definite matrices. arXiv preprint arXiv:1801.09269, 2018.
James Martens and Roger Grosse. Optimizing neUral networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
Klas Modin. Geometry of matrix decompositions seen throUgh optimal transport and information
geometry. arXiv preprint arXiv:1601.01875, 2016.
Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo,
2(11):2, 2011.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate
O(1/k2). Soviet Mathematics Doklady, 27(2):372-376, 1983.
Felix Otto. The geometry of dissipative evolution equations: the porous medium equation. Commu-
nications in Partial Differential Equations, 26(1-2):101-174, 2001.
Felix Otto and Cedric Villani. Generalization of an inequality by talagrand and links with the loga-
rithmic sobolev inequality. Journal of Functional Analysis, 173(2):361-400, 2000.
Brendan Odonoghue and Emmanuel Candes. Adaptive restart for accelerated gradient schemes.
Foundations of computational mathematics, 15(3):715-732, 2015.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. arXiv
preprint arXiv:1505.05770, 2015.
Radhey S Singh. Improvement on some known nonparametric uniformly consistent estimators of
derivatives of a density. The Annals of Statistics, pp. 394-399, 1977.
Weijie Su, Stephen Boyd., and Emmanuel J. Candes. A differential equation for modeling Nesterov's
accelerated gradient method: Theory and insights. Journal of Machine Learning Research, 2016.
Amirhossein Taghvaei and Prashant G Mehta. Accelerated flow for probability distributions. arXiv
preprint arXiv:1901.03317, 2019.
Asuka Takatsu. On wasserstein geometry of the space of gaussian measures. arXiv preprint
arXiv:0801.2250, 2008.
Cedric Villani. Topics in optimal transportation. American Mathematical Soc., 2003.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Yifei Wang, Zeyu Jia, and Zaiwen Wen. The search direction correction makes first-order methods
faster. arXiv preprint arXiv:1905.06507, 2019.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681-688,
2011.
Andre Wibisono, Ashia C. Wilson, and Michael I. Jordan. A variational perspective on accelerated
methods in optimization. proceedings of the National Academy of Sciences, 113(47):E7351-
E7358, 2016.
Hongyi Zhang and Suvrit Sra. Towards riemannian accelerated gradient methods. arXiv preprint
arXiv:1806.02812, 2018.
Ruiyi Zhang, Changyou Chen, Chunyuan Li, and Lawrence Carin. Policy optimization as wasser-
stein gradient flows. arXiv preprint arXiv:1808.03030, 2018.
12
Under review as a conference paper at ICLR 2020
A The proofs and derivations in Section 2
In this section, we provide the derivation of the Euler-Lagrange equation, proofs in Section 2 and
the Euler-Lagrange formulation of (AIG).
A. 1 Derivation of the Euler-Lagrange equation
We derive the Euler-Lagrange equation (1) in this subsection. For a fixed T > 0 and two given
densities ρ0, ρT , consider the variational problem
I(ρt)
inf
ρt
L(ρt,∂tρt)dt
ρ0 = ρ0, ρT = ρT	.
Let ht ∈ F(Ω) be the smooth perturbation function that satisfies J htdX = 0,t ∈ [0,T] and
h0 = hT ≡ 0. Denote ρt = ρt + ht . Note that
I3) = / L(Pt,dtPt)dt +1 /(δLht + δ∂Pt)dtht)
dxdt + o().
From dIF L0
0, it follows that
0T Z( δL ht+备 dtht) dxdt=0
Note that h0 = hT ≡ 0. Perform integration by parts w.r.t. t yields
/ Z (δL - dt δp⅛) JhdXdd = Q
Because	htdX = 0, (1) holds with a spatially constant function C(t).
A.2 The proof of Proposition 1 in Section 2
In this subsection, we derive the Hamiltonian flow in the probability space. First, we give a useful
identity. Given a metric tensor G(P) : TPP(Ω) → TPP(Ω), We have
/ σ1G(P)σ2dX = / G(P)σ1σ2dX =	Φ1G(P)-1Φ2dX =	G(P)-1Φ1Φ2dX.
(14)
Here Φ1 = G(P)-1σ1 and Φ2 = G(P)-1σ2. We then check that
∂tPtG(Pt)∂tPtdX
ΦtG(Pt)-1ΦtdX .
(15)
Let Pt = Pt + eh, where h ∈ TPtP(Ω). For all σ ∈ TρtP, it follows
G(Pt + h)-1G(Pt + h)σ = σ.
The first-order derivative w.r.t. e of the left hand side shall be 0, i.e.,
(一∙ h) G(Pt)σ + G(Pt)-I (竽∙ h) σ = 0.
∂Pt	∂Pt
Because ∂tPt = G(P)-1Φt, applying (14) yields
Z ∂tPt (W)
• h) ∂tPtdx = Z ΦtG(Pt)-1 (dGP)
• h ∂t Pt dX
-/ Φt ("Gd?) 1 ∙ h) G(Pt)∂tPtdx = - / Φt (dG∂Pt) 1 ∙ h) Φtdx.
(16)
13
Under review as a conference paper at ICLR 2020
Based on basic calculations, we can compute that
I dtptG(Pt)dtptdχ -
∂tρtG(ρt)∂tρtdx
∂tρt
∂G(ρt)
∂pt
• h ∂tρtdx + o(e),
(17)
-/ ΦtG(Pt)-1Φtdx + / ΦtG(ρt)Tφtdx = -e / Φt (dGpt)
• h Φtdx + o().
(18)

Combining (16), (17) and (18) yields (15). Hence, the Euler-Lagrange equation (1) is equivalent to
∂tΦt
∂tρtG(ρt)∂tρtdx
δE
--赢
ΦtG(ρt)-1Φtdx
δE
--访
This equation combining with ∂tρt = G(ρ)-1Φt recovers the Hamiltonian flow (2). In short, the
Euler-Lagrange equation (1) is from the primal coordinates (ρt, ∂tρt) and the Hamiltonian flow (2)
is from the dual coordinates (ρt, Φt). Similar interpretations can be found in (Chow et al., 2019).
A.3 The Euler-Lagrangian formulation of AIG flows
We can formulate (AIG) as a second-order equation of ρt ,
D2	δE
Dt2 Pt + αtdtpt + G(Pt) 而一 =0.
Here D2/Dt is the covariant derivative in metric G. We can also explicitly write 嘉Pt as follows.
D2	1
Dt2Pt = dttρt - (dtG(Pt)	WtPt + 2G(Pt)
∂tρtG(ρt)∂tρtdx
B The proofs in Section 3
In this section, we present proofs of propositions and theorems in Section 3.
B.1 The proof of Proposition 2 in Section 3
We start with an identity. For a twice differentiable Φ(x), we have
1 VkVΦk2 = V2ΦVΦ = (VΦ • V)VΦ.	(19)
From (W-AIG), it follows that
∂tPt + V • (PtVΦt) = 0.	(20)
This is the continuity equation of Pt . Hence, on the particle level, Xt shall follows
dXt = VΦt (Xt)dt.
Let Vt = VΦt(Xt). Then, by the conservation of momentum and (W-AIG), we have
dVt = (∂t+VΦt(Xt) • V)VΦt(Xt)dt
=(—αtVΦt(Xt) — 1 V∣∣VΦ∣∣2 — VIE) dt + (VΦ • V)VΦdt
δE	δE
=—at VΦt(Xt)dt — V δ— (Xt) dt = —at Vt dt — V ∣— (Xt) dt
B.2 The proof of Proposition 3 in Subsection 3.1
In this subsection, we derive the Hamiltonian flow in Gaussian. For A ∈ Sn, we define the linear
operator MA : Sn → Sn by
MAB = AB + BA, B ∈ Sn .
14
Under review as a conference paper at ICLR 2020
It is easy to verify that if A ∈ Pn, then MA-1 is well-defined. For a flow ∑t ∈ Pn , t ≥ 0, we define
.1 τ	♦	r∕□J∖	1	∕JJ∖ τ~!∕c∖ EI	1 ∙ τ-,ι τ	. ∙
the Lagrangian L(∑t, ∑t) = 2g∑t(∑t, ∑t) - E(∑t). The corresponding EUler-Lagrange equation
writes
d dL dL
- :-= .
dt d∑t d∑
Let St = M-1Σt, i.e., Σt = St∑t + ∑tSt. Then, it follows
11
g∑t (∑t, ∑t) = tr(St∑tSt) = -tr((St∑t + ∑tSt)St) = -tr(∑tSt)
(21)
1
∕r(Σ tM∑t1∑ t).
This leads to 普=M-1∑t = St. For simplicity, We denote g = g∑t (∑t, ∑t). First, We show that
dg 2
d∑t = - t
1
Because St = MΣ-1∑t. Given ∑t, St can be viewed as a continuous function of∑t. For any A ∈ Sn,
define lA = tr((∑tSt + St∑t)A).
dlA	∂St ∂LA	∂lA	∂St
0=W = ∂∑t W+W = ∂∑t (A∑t+∑tA)+(ASt+StA).
Here we view ∂St/∂∑t as a linear operator on Sn. Let B = A∑t + ∑tA, then A = M-1B.
∂StB + MstM-1B = 0 holds for all B ∈ Sn. Therefore, we have 翁=-MStM-1. Hence,
-^g-=^t yg+~^g~=-MSt M-j(St∑t+∑tSt)+S2=-MSt St+S2=-S2.
d∑t	∂ ∑t ∂St	∂ ∑t	t
As a result, the Euler-Lagrange equation (21) is equivalent to
S dL
一=____
2	d∑t
S2
-S -VE(∑t).
(22)
Combining (22) with ∑t = St ∑t + ∑tSt renders the Hamiltonian flow (5).
B.3 The proof of Theorem 1 in Subsection 3.1
We first show that ∑t stays in Pn. Suppose that ∑t ∈ Pn for 0 ≤ t ≤ T . Define Ht = H(∑t, St)
tr(St∑tSt)∕2 + E(∑t). We observe that (W-AIG-G) is equivalent to
∑t = 2dH,	St = -αtSt - 2dHt.
(23)
We show that Ht is decreasing with respect to t.
dH = tr (会St + M∑t) = tr (会(-αtSt - 2祟)+ 2黑祟
dt	∂St	∂∑t	∂St	∂∑t	∂∑t ∂St
-αt tr 卜t dH
—2 tr(St(∑tSt + St∑t)) = -αt tr(St∑tSt) ≤ 0.
For simplicity, we denote W * = (∑*)-1. Let λt be the smallest eigenvalue of ∑t. Then,
logdet(∑tW*) ≥ logdet(∑t) = nlogλt. Therefore,
n-
-2(logλt + 1) ≤ -2[logdet(∑tw ) + n] ≤ E(∑t) ≤ H(t) ≤ H(0),
which indicates that
λt ≥ exp (--H(0) — 1).
(24)
This means that as long as ∑t ∈ Pn , the smallest eigenvalue of ∑t has a positive lower bounded.
If there exists T > 0 such that ∑T ∈/ Pn . Because ∑t is continuous with respect to t, there exists
T1 < T, such that ∑t ∈ Pn, 0 ≤ t ≤ T1 and λT1 < exp (-2H (0)/n - ), which violates (24).
15
Under review as a conference paper at ICLR 2020
We then reveal the relationship between (W-AIG) in P (Rn) and Pn. We can compute that
∂∂
∂t detND = detNDtrNt ςt), ∂tς = -2 ς»t .
Combining with Σt = ΣtSt + St Σt, we obtain
.tr(∑-1∑ t) = tr(St + ∑-1St∑t) = 2tr(St),
tr(xΣ-1 Σ t∑-1x) = tr(xT Σ-1StX + xτ St∑-1x) = 2 tr(St∑-1xxT).
Therefore, it follows
∂tρt(x)
∂
∂t
1
pdet(∑t)
Pdet(∑t)ρt(x) + 2 tr(xT∑-1∑t∑-1x)pt(x)
—$ tr(∑t 1∑t)pt(x) + tr(St∑t 1 xxT)pt(x) = — tr(St(I - ∑t 1xxT))pt(x).
Note that VΦt(x) = Stx. Hence,
nn
一 V ∙ (ρtVΦt) = 一 X ∂i(ρt(x)Stx)i = 一 X [ρt(x)∂i(Stx)i + (Stx)i∂iρt(x)]
i=1	i=1
=—ρt(x) [tr(St) + (StX)T(-∑-1x)] = —ρt(x) tr(St(I — ∑-1xxT)) = ∂tρt(x).
The first equation of (W-AIG) holds. Because ∂tΦt(x) = xτStx/2 + C(t),
∂tΦt(x) + αtΦt(x) + 2 ∣∣VΦt(x)∣∣2 = 2 xτ StX + -2 xτ StX + 2 XT StIX + C(t)
=—XT V∑t E(∑t)x + C(t) = 1 XT (Σ-1 — W *)x + C(t).
Note that P is the GaUssian density With the covariance matrix Σ*. Because C(t)
1 logdet(∑tW*) — 1, we can compute
δE = log Pt(x) — log P*(x) + 1 = —1 XT (∑-1 — W *)x — Ilogdet(∑tW *) + 1
δρt	2	2
=一 2xT(匕 1 — W")x — C(t) = — (dtφt(x) + αtφt(X) + 2IVφt(X)Il2).
Therefore, the second equation of (W-AIG) holds. Because Σ0 = Σ0, S0 = 0 and C(0) = 0, we
have ρ0 = ρ0 and Φ0 = 0. This completes the proof.
C The proofs in Section 4
In this section, we briefly review some geometric properties of the probability space as a Riemannian
manifold and present proofs of propositions in Section 4.
C.1 A brief review on the geometric properties of the probability space
Suppose that we have a metric gρ in the probability space P(Ω). Given two probability densities
ρ0, ρ1 ∈ P(Ω), we define the distance as follows
d(ρ0,ρ1)=(则{/ gγ(s)(Y(S),γ(s))ds ： Y(O) = P0,γ(1) = P1})
The minimizer Y(S) of the above problem is defined as the geodesic curve connecting P0 and P1. An
exponential map at ρ0 ∈ P(Ω) is a mapping from the tangent space Τρ0P(Ω) to P(Ω). It requires
that σ ∈ Tρ0P(Ω) is mapped to a point ρ1 ∈ P(Ω) such that there exists a geodesic curve Y(S)
satisfying Y(0) = ρ0, γ(0) = σ and Y(1) = ρ1.
16
Under review as a conference paper at ICLR 2020
C.2 The proof of Proposition 4 in Section 4
In this subsection, we characterize the inverse of the exponential map in the probability space with
the Wasserstein metric. Let Tts = (sTt + (1 - s) Id)-1 , s ∈ [0, 1]. Then, based on the theory of
optimal transport (Villani, 2003), we can write the explicit formula of the geodesic curve γ(s) by
Y(s) = Ts#Pt = det(VTj)ρt ◦ Ts.
Through basic calculations, we can compute that
=--d (STt + (1 — s)Id)	=Id -Tt.
s=0	ds	s=0
de det(VTts)	= -ʒ- det(I + S(I — DTt) + o(s))	= tr(I — DTt).
ds	s=0 ds	s=0
Therefore,
t(S)
s=0
(x) = tr(I 一 VTt)Pt(X) + hVρt(x),x 一 夕t(x)i
=V ∙ (x ― Tt(X))Pt(X) + hVρt(x),x ― Tt(Xyi = -V ∙ (Pt(X)(Tt(X)- x)),
which completes the proof.
C.3 The proof of Proposition 5 and 6 in Section 4
The main goal of this subsection is to prove the Lyapunov function E(t) is non-increasing.
Preparations. We first give a better characterization of the optimal transport plan Tt. We can write
Tt = VΨt, where Ψt is a strictly convex function, see (Villani, 2003). This indicates that VTt is
symmetric. We then introduce the following proposition.
Proposition 7 Suppose that E(P) satisfies Hess(β) for β ≥ 0. Let Tt (X) be the optimal transport
Planfrom Pt to ρ*, then
E(P*) ≥ E(Pt) +
Z	Tt (X) -
X,
VδE)PdX + 2 Z kTt(X)
- Xk2PtdX.
This is a direct result of β-geodesic convexity of E(P) based on Proposition 4.
Next, let us denote ut = ∂t(Tt)-1 ◦ Tt. We show that ut satisfies
V ∙ (Pt(ut - VΦt)) = 0.	(25)
Because (Tt)-1#p* = Pt, let Ut = ∂t(Tt)-1 ◦ Tt and Xt = (Tt)TXo,where X。〜p*. This yields
dXt = Ut(Xt). The distribution of Xt follows Pt. By the Euler's equation, Pt shall follows
dtPt + V ∙ (Ptut) = 0.
Combining this with the continuity equation (20) yields (25).
Then, we formulate ∂tTt(X) with Ut. By the Taylor expansion,
Tt+s (X) = Tt(X) + S∂tTt(X) + o(S).
Let y = (Tt)-1X. it follows
(Tt+s)-1(X) = (Tt)-1(X) + SUt((Tt)-1(X)) + o(S) = y + SUt(y) + o(S).
Therefore,
0 = Tt+s((Tt+s)-1(X)) - X = Tt+s(y + SUt(y) + o(S)) - X
=Tt(y + SUt (y)) + S∂tTt(y + SUt(y)) - X + o(S)
=Tt(y) + SVTt(y)Ut(y) + S∂tTt(y) - X + o(S)
=S [VTt (y)Ut (y) + ∂tTt(y)] + o(S).
We shall have VTt(y)Ut (y) + ∂tTt (y) = 0. Replacing y by X gives
∂tTt(X) = -VTt(X)Ut (X).	(26)
The following lemma illustrates two important properties ofUt and ∂tTt.
17
Under review as a conference paper at ICLR 2020
Lemma 1 For ut satisfying (25), we have
/ hVΦt - Ut, VTtVΦti Ptdx ≥ 0,	/ hVΦt - Ut, VTt(x)(Tt(x) - x)i Pt = 0.
Proof We first notice that Ut - VΦt is divergence-free in term of Pt. From -VTtUt = ∂tTt =
V∂tΨt, We observe that -VTtUt is the gradient of ∂tΨt. Therefore,
/ hVΦt - Ut, VTtUti Pt = -/ h∂tΨt, V ∙ (Pt(VΦt - Ut))i = 0.
Based on our previous characterization on the optimal transport plan Tt, VTt = V2Ψt is symmetric
positive definite. This yields that
/
Z
hVΦt - ut, VTtVΦti Ptdx = hVΦt - ut, VTtVΦti Ptdx - hVΦt - ut, VTtuti Pt
hvφt- ut, VTt(Vφt- Uty) Ptdx ≥ 0.
The last inequality utilizes that VTt is positie definite and Pt is non-negative. Then, we prove the
equality in Lemma 1. Because VTt(x)(Tt(x) 一 x) = 1 V(∣∣Tt(x) — xk2 + Tt(x) 一 ∣∣xk2) is a
gradient. Similarly, it follows
hVΦt 一 ut, VTt(x)(Tt(x) 一 x)i Pt = 0.
Lemma 1 and the relationship (26) gives
一	h∂tTt, VΦti Ptdx =	hut, VTtVΦti Ptdx ≤	hVΦt, VTtVΦti Ptdx,
h∂tTt, Tt(x) 一 xi Ptdx = 一	hVΦt, VTt(x)(Tt(x) 一 x)i Ptdx.
Proof of Proposition 5. Based on the definition of the Wasserstein metric, we have
∂tE(ρt)
ZδE
δρ-V ∙ (ρtVΦt)dx.
(27)
(28)
Differentiating E(t) (7) w.r.t. t renders
E(t)e-√βt =β / h∂tTt,Tt(x) - x) Ptdx 一
一 Pβ / hdtTt, vφ/ Ptdx 一
2 / kTt(x)- xk2v∙(ρtVΦt)dx
pβ/ hTt(x) - x,∂tVΦt) Ptdx
+ √β/ Ut(X)- x, VΦti V ∙ (ρtVΦt)dx + / hVΦt,∂tVΦti Ptdx
一 1 / ∣VΦtk2V ∙ (PtVΦt) - / δEV ∙ (PtVΦt)dx
+ 邛 Z ∣VΦtk2Ptdx 一 β Z hTt (x) 一 x, VΦt (x)) Ptdx
+ v2^ Z kTt(x) 一 xk2Ptdx + pp∕β(E(Pt)- E(P*)).
For the part (29), Proposition 7 renders
(29)
^^2^ Z IITt(X)- xk2Ptdx + ∕βE(Pt) ≤ -∕βZ(Tt(X)- x,
ρtdx.
(30)
We first compute the terms with the coefficient β0 in E(t)e-vβt. We observe that
/ hVΦt,∂tΦt) Ptdx - 2 / ∣∣VΦtk2V ∙ (PtVΦt)dx -J δEV ∙ (PtVΦt)Ptdx
=/ (∂tVΦt + 1 V∣VΦtk2 + VδE:, VΦ>tdx = -2∕β/ ∣VΦtk2Ptdx,
(31)
18
Under review as a conference paper at ICLR 2020
where the last equality uses (W-AIG) with at = 2√β. Substituting (30) and (31) into the expression
of E(t)e-√βt yields
E(t)e-√βt ≤β/ h∂tTt,Tt(x) - Xi PtdX - β / kTt(x) - xk2V∙ (PtVΦt)dx
-β / hτt(x) - x, vφti Ptdx - √β / hdtτt, vφti Ptdx
-√β/ hTt(x) - X, ∂tVΦti PtdX - √β /(Tt(X)- X,
(32)
ρtdx
+ Pβ/ hTt(x)- x, VΦtiV∙(ρtVΦt)dx -殍 / kVΦtk2ρtdx.
Then, we deal with the terms with V ∙ (PtVΦt). We have the following two identities
/ hTt(x) - x, VΦti V ∙ (PtVΦt)dx = - / hV hTt(x) - x, VΦti, VΦti PtdX
=-	VΦt,V2Φt(X)(Tt(X) - X) + (VTt(X) - I)VΦt PtdX
=-1 / (Tt(X)- X, VkVΦtk2)Ptdx -J hVΦt, VTtVΦti Ptdx + / kVΦtk2Ptdx.
一 1 / l∣Tt(x) — x∣∣2V∙ (ρtVΦt)dx = / h(VTt(x) - I)(Tt(x) - x), VΦ∕ Ptdx
hTt(x) - x, VTtVΦti ρtdx -	hTt(x) - x, VΦti ρtdx.
Hence, We can proceed to compute the terms with the coefficient √β. (27) and (33) yields
—
—
—
—
√β/ h∂tTt, VΦti PtdX - pβ/(Tt(X)- x,∂tVΦt +
警 / kVΦtk2Ptdx + √β/ Ut(X)- x, VΦti V ∙ (ρtVΦt)dx
√β/ h∂tTt + VTtVΦt, VΦti PtdX -殍 / ∣VΦt∣2ρtdx
√β/ Tτxχ- - x,dtV^ + V-P + 2vIIv^k2,Ptdx
(33)
(34)
(35)
ρtdx
≤-
条 / kVΦtk2ρtdx + 2β/ hTt(x)- x, VΦti Ptdx.
Substituting (34) and (35) into (32) gives
E(t)e-√βt +号 / kVΦtk2Ptdx
≤β / h∂tTt,Tt(x) - Xi Ptdx - 2 / kTt(x) - xk2V ∙ (PtVΦt)dx
- β	hTt (x) - x, VΦti Ptdx + 2β	hTt(x) - x, VΦti Ptdx
=β	h∂tTt + VTtVΦt, Tt(x) - xi Ptdx = 0,
where the last equality uses (28). In summary, we have
E(t)e-√βt ≤ -亨 / kVΦtk2Ptdx ≤ 0.
19
Under review as a conference paper at ICLR 2020
Proof of Proposition 6. Differentiating E(t) (8) w.r.t. t, we compute that
E(t)= / h∂tTt,Tt(X)- Xi PtdX - 2 / ∣Tt(X) - Xk2V ∙ (PtVΦt)dX
dtTt, 2vφ, Ptdχ - /(Tt(X)- x,
+ / ∕τt(x) - x, 2VΦt∖ V ∙ (ρtVΦt)dx +
2VΦt + t∂tVΦt, PtdX
/ 2VVΦt,1 VΦt + t∂tVΦt∖ PtdX
—
2
-2/
tVΦt V∙(PtVΦt)dX -
t2	δE	t
4 J 赢 V∙ (PtVΦt)dX +-(E (Pt)- E(P*)).
t	(36)
Because E(ρ) is Hess(0), Proposition 7 yields
E(Pt) = E(Pt)- E(P*) ≤-	T Tt(X)- x,
ρtdx.
(37)
Utilizing the inequality (37) and substituting the expressions of terms involving ∂tT and V ∙(ρtVΦt)
in (36) with the expressions in (27) (28) and (33) (34), we obtain
E(t) ≤-/ hVΦt, VTt(X)(Tt(X)- X)i PtdX + / hTt(X) - X, VTtVΦti PtdX
-	J (Tt(X) - X, VΦti PtdX +t / hVΦt, VTtVΦti PtdX
-	2 / hTt(X) - X, VΦti PtdX - 2 / (∂tVΦt,Tt(X) - Xi PtdX
-	4 /〈Tt(X)- X, V∣∣VΦtk2> PtdX - I / hVΦt, VTtVΦti PtdX
(38)
+ 2.
t2
+ W
/ ∣VΦtk2ρtdx +4 / ∣VΦtk2ρtdx + t2 / hVΦt,∂tVΦti PtdX
/ (VΦt, V∣∣VΦtk2) PtdX + t2 / <VΦt,
ρt dx
-2 / (Tt(X) - x,
ρtdx.
The expression of (38) can be reformulated into
E(t) ≤ - 2 / hTt(X) - x, vφ∕ PtdX + 4- / kV^k2PtdX
-2 j(3(X)- X,∂tVΦt + 2V∣VΦtk2 +
+ t2 I (VΦt,∂tVΦt + 2V∣VΦtk2 +
ρt dx
ρtdx.
From (W-AIG) with αt = 3/t, we have the following equalities.
—
t2
Z
VΦt,∂tVΦt + 1 V∣VΦtk2 +
ρt dx
-ɪ / ∣∣vφtk2ρtdχ,
2 /(Tt(X)- X,∂tVΦt + 2V∣VΦtk2 +
ρt dx
3 / hTt(x) - x, VΦti Ptdx.
As a result, E (t) ≤ 0. This completes the proof.
20
Under review as a conference paper at ICLR 2020
C.4 Comparison with the proof in the accelerated flow
The accelerated flow in (Taghvaei & Mehta, 2019) is given by
dXt = eαt-γt Yt
dYt
dt
eat+βt+γt ▽
(Xt).
(39)
Here the target distribution Satisies ρ∞(x) = ρ*(x) 8 exp(-f (x)). Suppose that We take at =
logp - log t, βt = plog t + log C and γt = plog t. Here we specify p = 2 and C = 1/4. Then
the accelerated floW (39) is identical to (W-AIG-P-KL) if We replace Yt by 2t-3Vt . The Lyapunov
function in (Taghvaei & Mehta, 2019) folloWs
V(t) =1 E[kXt + e-γtYt - Tp； (Xt)k2]+ eβt (E(P)- E(ρ*))
1	t	* Ct2
=2E[kXt + G匕 - TPt (Xt)k2] + 4(E(Pt) - E(P*))
1	t	2	t2
=2 J -(Tt(X) - x) + 2vφt(χ)	Pt(X)dx + j(E(Pt) - E(P)).
The last equality is based on the fact that Vt = VΦt(Xt) and Tt = TPt is the optimal transport plan
from Pt to P* . This indicates that the Lyapunov function in (Taghvaei & Mehta, 2019) is identical to
ours (8). The technical assumption in (Taghvaei & Mehta, 2019) folloWs
0 =E ](Xt + e-γY - Tp； (Xt)) ∙ ddtTp* (Xt)
=E	(Xt	+ ∣Vt	-	Tt(Xt))	∙ dtTt(Xt)
=E	(Xt	+ ∣Vt	-	Tt(Xt))	∙ ((∂tTt)(Xt)	+ VTtVt)
=/ (x — Tt(X) + 2 VΦt(χ), ∂tTt + VTtVφ) Ptdx
Based on ∂tTt = -VTtut and Lemma 1, We have
hX - Tt (X), ∂tTt + VTtVΦti PtdX
hX - Tt(X), VTt(VΦt - ut)i PtdX = 0.
hVΦt, ∂tTt + VTtVΦti PtdX
hVΦt,VTt(VΦt
- ut)i PtdX
=	hVΦt - ut, VTt (VΦt - ut)i PtdX ≥ 0.
As a result, We have
E (Xt + e-γt Yt - Tp∞ (Xt)) ∙ dtTP∞ (Xt) = 2 / hVΦt- ut, VTt (VΦt - Ut) Ptdx ≥ 0.
In 1-dimensional case, because V ∙ (Pt(ut — VΦt)) = 0 indicates that Pt(Ut 一 VΦt) = 0. For
Pt (x) > 0, We have ut (x) - VΦt(x) = 0. So the technical assumption holds. In general cases,
although Ut = ∂t(Tt)-1 ◦ Tt satisfies V ∙ (Pt(Ut - VΦt)) = 0, but this does not necessary indicate
that Ut = VΦt. Hence, E [(Xt + e-γtYt - Tp∞ (Xt)) ∙第Tp∞ (Xt)] = 0 does not necessary hold
except for 1-dimensional case.
D Implementation details in the numerical experiments
In this section, We elaborate on the implementation details in the numerical experiments.
D. 1 Details in Subsection 6.1
The initial distribution of the particle system folloWs the standard Gaussian N (0, I). The objective
density function is
P*(x) H exp(-2(kxk — 3)2)(exp(-2(xι - 3)2) + exp(-2(xι + 3)2)),	x ∈ R2.
All methods run for 200 iterations using the same fixed step size τ = 0.1.
21
Under review as a conference paper at ICLR 2020
D.2 Details in Subsection 6.2
The discretization the Wasserstein gradient flow in Gaussian follows
Σk+1 = ∑k - 2τ(ΣkV∑kE(∑k) + V∑kE(∑k)∑k).	(40)
The discretization of (W-AIG-G) is given by
Sk+1 = αk Sk - √τ (S2 + 2v∑k ENk )), 力+1 = (I + √τsk+1 Rk (I + λ∕τSk + 1).	(41)
The choice of αk is same as the one for (9), based on whether β is known. To ensure the update
of Σk+1 won’t blow up, we check whether tr(τS2) > n. If this holds, we restart the algorithm. In
fact, the update on ∑k can be viewed as the exponential map from ∑k with the direction ∑kSk+ι +
Sk+ι∑k. Nevertheless, this is an exponential map if and only if I + √τSk+ι is positive definite.
The update rule of √τSk+ι can rewrite into
√TSk+ι = αk√TSk -(FSk)2 - 2τV∑kE(∑k).	(42)
Because of the existence of -(√τSk )2, as long as the spectral radius (the eigenvalue with the largest
absolute value) of √τSk is greater than 1, then I + √τSk+ι cannot maintain to be positive definite.
So at this time we need to restart the algorithm. Nevertheless, to compute the spectral radius is
computational costly. Instead, we use a weaker condition tr(τ Sk2) > n.
We also introduce the adaptive restart technique for (41). Consider
2k = - tr(Sk+ι∑k V∑kE(∑k)).
This restarting criterion corresponds to the particle version of (13). Similarly, if 夕k < 0, We restart
the algorithm using initial values Σ0 = Σk and S0 = 0.
For the particle implementation, the update rule of WGF writes
Xki = -τ(Vf(Xki))+ξk(Xki)),
where ξk is computed based on KDE (11).
D.3 Details in Subsection 6.3
We follow the same setting as (Liu & Wang, 2016), which is also adopted by Liu et al. (2018; 2019).
The data set is split into 80% for training and 20% for testing. The mini-batch size is taken as 50.
For MCMC, the number of particles is N = 1000; for other methods, the number of particles is
N = 100. The BM method is not applied to SVGD in selecting the bandwidth.
The initial step sizes for the compared methods are given in Table 1. The step size of SVGD is
adjusted by Adagrad, which is same as (Liu & Wang, 2016). For WNAG, AIG and WRes, the step
size is give by Tl = τo∕l0∙9 for l ≥ 1. The parameters for WNAG and Wnes are identical to (Liu
et al., 2018) and (Liu et al., 2019). For MCMC, WGF and AIG-r, the step size is multiplied by 0.9
every 100 iterations. We record the cpu-time for each method in Table 2. The computational cost
Method	MCMC	SVGD	WNAG	Wnes	WGF	AIG	AIG-r
Step size T	1e-5	0.05	1e-6	1e-5	1e-5	1e-5	1e-6
Table 1: Initial step sizes of algorithms in comparison.
of the BM method is much higher than the MED method because we need to evaluate the MMD of
two particle systems several times in optimizing the subproblem.
Method	MCMC	SVGD	WNAG	Wnes	WGF	AIG	AIG-r
-BM-	13.962	5.539	78.038	78.509	79.006	79.094	78.945
MED	13.909	5.581	5.623	5.625	5.395	5.890	5.689
Table 2: Averaged cpu time(s) cost for algorithms in comparison.
22