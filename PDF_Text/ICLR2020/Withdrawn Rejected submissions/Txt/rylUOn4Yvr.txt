Under review as a conference paper at ICLR 2020
Robust Discriminative Representation Learn-
ing via Gradient Rescaling: An Emphasis Reg-
ularisation Perspective
Anonymous authors
Paper under double-blind review
Ab stract
It is fundamental and challenging to train robust and accurate Deep Neural Net-
works (DNNs) when semantically abnormal examples exist. Although great
progress has been made, there is still one crucial research question which is not
thoroughly explored yet: What training examples should be focused on and how
much more should they be emphasised to achieve robust learning? In this work,
we study this question and propose gradient rescaling (GR) to solve it. GR modi-
fies the magnitude of logit vector’s gradient to emphasise on relatively easier train-
ing data points when noise becomes more severe, which functions as explicit em-
phasis regularisation to improve the generalisation performance of DNNs. Apart
from regularisation, we connect GR to examples weighting and designing robust
loss functions. We empirically demonstrate that GR is highly anomaly-robust and
outperforms the state-of-the-art by a large margin, e.g., increasing 7% on CIFAR-
100 with 40% noisy labels. It is also significantly superior to standard regularisers
in both clean and abnormal settings. Furthermore, we present comprehensive ab-
lation studies to explore the behaviours of GR under different cases, which is
informative for applying GR in real-world scenarios.
1	Introduction
DNNs have been successfully applied in diverse applications (Socher et al., 2011; Krizhevsky et al.,
2012; LeCun et al., 2015). However, their success is heavily reliant on the quality of training data,
especially accurate semantic labels for learning supervision. Unfortunately, on the one hand, main-
taining the quality of semantic labels as the scale of training data increases is expensive and almost
impossible when the scale becomes excessively large. On the other hand, it has been demonstrated
that DNNs are capable of memorising the whole training data even when all training labels are ran-
dom (Zhang et al., 2017). Therefore, DNNs struggle to discern meaningful data patterns and ignore
semantically abnormal examples1 simultaneously (Krueger et al., 2017; Arpit et al., 2017). Conse-
quently, it becomes an inevitable demand for DNNs to hold robustness when training data contains
anomalies (Larsen et al., 1998; Natarajan et al., 2013; Sukhbaatar & Fergus, 2014; Xiao et al., 2015;
Patrini et al., 2017; Vahdat, 2017; Veit et al., 2017; Li et al., 2017).
Recently, great progress has been made towards robustness against anomalies when training
DNNs (Krueger et al., 2017). There are three appealing perspectives in terms of their simplicity
and effectiveness: 1) Examples weighting. For example, knowledge distilling from auxiliary mod-
els is popular for heuristically designing weighting schemes. However, it is challenging to select
and train reliable auxiliary models in practice (Li et al., 2017; Malach & Shalev-Shwartz, 2017;
Jiang et al., 2018; Ren et al., 2018; Han et al., 2018b). 2) Robust loss functions (Van Rooyen et al.,
2015; Ghosh et al., 2017; Zhang & Sabuncu, 2018; Wang et al., 2019b); 3) Explicit regularisation
techniques (Arpit et al., 2017; Zhang et al., 2018a). Although designing robust losses or explicit
regularisation is easier and more flexible in practice, the performance is not the optimal yet.
1One training example is composed of an input and its corresponding label. A semantically abnormal
example means the input is semantically unrelated to its label, which may come from corrupted input or label.
For example, in Figure 3 in the supplementary material: 1) Out-of-distribution anomalies: An image may
contain only background or an object which does not belong to any training class; 2) In-distribution anomalies:
An image of class a may be annotated to class b or an image may contain more than one semantic object.
1
Under review as a conference paper at ICLR 2020
Regarding examples weighting, there is a core research question which is not well answered yet:
What training examples should be focused on and how large the emphasis spread should be?
In this work, we present a thorough study of this practical question under different settings. For
better analysis, we propose two basic and necessary concepts: emphasis focus and spread with
explicit definition in Sec. 3.2. They are conceptually introduced as follows:
Emphasis focus. It is a common practice to focus on harder instances when training DNNs (Shri-
vastava et al., 2016; Lin et al., 2017). When a dataset is clean, it achieves faster convergence and
better performance to emphasise on harder examples because they own larger gradient magnitude,
which means more information and a larger update step for model’s parameters. However, when
severe noise exists, as demonstrated in (Krueger et al., 2017; Arpit et al., 2017), DNNs learn simple
meaningful patterns first before memorising abnormal ones. In other words, anomalies are harder to
fit and own larger gradient magnitude in the later stage. Consequently, if we use the default sample
weighting in categorical cross entropy (CCE) where harder samples obtain higher weights, anoma-
lies tend to be fitted well especially when a network has large enough capacity. That is why we need
to move the emphasis focus towards relatively easier ones, which serves as emphasis regularisation.
Emphasis spread. We term the weighting variance of training examples emphasis spread. The
key concept is that we should not treat all examples equally, neither should we let only a few be
emphasised and contribute to the training. Therefore, when emphasis focus changes, the emphasis
spread should be adjusted accordingly.
We integrate emphasis focus and spread into a unified example weighting framework. Emphasis
focus defines what training examples own higher weights while emphasis spread indicates how
large variance over their weights. Specifically, we propose gradient rescaling (GR), which modifies
the magnitude of logit vector’s gradient. The logit vector is the output of the last fully connected
(FC) layer of a network. We remark that we do not design the weighting scheme heuristically from
scratch. Instead, it is naturally motivated by the gradient analysis of several loss functions.
Interestingly, GR can be naturally connected to examples weighting, robust losses, explicit regu-
larisation: 1) The gradient magnitude of logit vector can be regarded as weight assignment that is
built-in in loss functions (Gopal, 2016; Alain et al., 2016; Zhang et al., 2018b). Therefore, rescaling
the gradient magnitude equals to adjusting the weights of examples; 2) A specific loss function owns
a fixed gradient derivation. Adjusting the gradient can be treated as a more direct and flexible way
of modifying optimisation objectives; 3) Instead of focusing on harder examples2 by default, we
can adjust emphasis focus to relative easier ones when noise is severe. GR serves as emphasis reg-
ularisation and is different from standard regularisers, e.g., L2 weight decay constraints on weight
parameters and Dropout samples neural units randomly (Srivastava et al., 2014);
GR is simple yet effective. We demonstrate its effectiveness on diverse computer vision tasks using
different net architectures: 1) Image classification with clean training data; 2) Image classification
with synthetic symmetric label noise, which is more challenging than asymmetric noise evaluated by
(Vahdat, 2017; Ma et al., 2018); 3) Image classification with real-world unknown anomalies, which
may contain open-set noise (Wang et al., 2018), e.g., images with only background, or outliers, etc.;
4) Video person re-identification, a video retrieval task containing diverse anomalies. Beyond, we
show that GR is notably better than other standard regularisers, e.g., L2 weight decay and dropout.
Besides, to comprehensively understand GR’s behaviours, we present extensive ablation studies.
Main contribution. Intuitively and principally, we claim that two basic factors, emphasis focus and
spread, should be babysat simultaneously when it comes to examples weighting. To the best of our
knowledge, we are the first to thoroughly study and analyse them together in a unified framework.
2	Related Work
Aside from examples weighting, robust losses minimisation and explicit regularisation techniques,
there are another two main perspectives for training robust and accurate DNNs when anomalies exist:
2 An example’s difficulty can be indicated by its loss (Shrivastava et al., 2016; Loshchilov & Hutter, 2016;
Hinton, 2007), gradient magnitude (Gopal, 2016; Alain et al., 2016), or input-to-label relevance score (Lee
et al., 2018). The input-to-label relevance score means the probability of an input belonging to its labelled class
predicted by a current model. The difficulty of an example may change as the model learns. In summary, higher
difficulty, larger loss, larger gradient magnitude, and lower input-to-label relevance score are equal concepts.
2
Under review as a conference paper at ICLR 2020
1) Robust training strategies (Miyato et al., 2018; Guo et al., 2018; Li et al., 2019; Thulasidasan et al.,
2019); 2) Noise-aware modelling, and alternative label and parameter optimisation are popular when
only label noise exists. Some methods focus on noise-aware modelling for correcting noisy labels
or empirical losses (Larsen et al., 1998; Natarajan et al., 2013; Sukhbaatar & Fergus, 2014; Xiao
et al., 2015; Vahdat, 2017; Veit et al., 2017; Goldberger & Ben-Reuven, 2017; Han et al., 2018a).
However, it is non-trivial and time-consuming to learn a noise-aware model, which also requires
prior extra information or some specific assumptions. For example, Masking (Han et al., 2018a) is
assisted by human cognition to speculate the noise structure of noise-aware matrix while (Veit et al.,
2017; Li et al., 2017; Lee et al., 2018; Hendrycks et al., 2018) exploit an extra clean dataset, which
is a hyper-factor and hard to control in practice. Some other algorithms iteratively train the model
and infer latent true labels (Wang et al., 2018; Tanaka et al., 2018). Those methods have made great
progress on label noise. But they are not directly applicable to unknown diverse semantic anomalies,
which covers both out-of-distribution and in-distribution cases.
2.1 Remarks on Robustness Theorems Conditioned on Symmetric Losses
We note that (Ghosh et al., 2017) proposed some theorems showing that empirical risk minimization
is robust when the loss function is symmetric and the noise type is label noise. However, they are
not applicable for deep learning under arbitrary unknown noise: 1) We remark that we target at
the problem of diverse or arbitrary abnormal examples, where an input may be out-of-distribution,
i.e., not belonging to any training class. As a result, the symmetric losses custom-designed for
label noise are not applicable. 2) GR is independent of empirical loss expressions as presented
in Table 1. Therefore, one specific loss is merely an indicator of how far we are away from a
specific minimisation objective. It has no impact on the robustness of the learning process since it
has no direct influence on the gradient back-propagation. Similar to the prior work of rethinking
generalisation (Zhang et al., 2017), we need to rethink robust training under diverse anomalies,
where the robustness theorems conditioned on symmetric losses and label noise are not directly
applicable.
3	Emphasis Regularisation by Gradient Rescaling
Notation. We are given N training examples X = {(xi, yi)}iN=1, where (xi, yi) denotes i-th
sample with input xi ∈ RD and label yi ∈ {1, 2, ..., C}. C is the number of classes. Let’s consider
a deep neural network Z composed of an embedding network f (∙) : RD → RK and a linear classifier
g(∙) : RK → RC, i.e., Zi = Z(Xi) = g(f (Xi)) : RD → RC. Generally, the linear classifier is the
last FC layer which produces the final output of z, i.e., logit vector z ∈ RC. To obtain probabilities
of a sample belonging to different classes, logit vector is normalised by a softmax function:
p(j|Xi) = exp(zij)/XC	exp(zim).	(1)
m=1
p(j |Xi ) is the probability of Xi belonging to class j . A sample’s input-to-label relevance score
is defined by pi = p(yi |Xi). In what follows, we will uncover the sample weighting in popu-
lar losses: CCE, Mean Absolute Error (MAE) and Generalised Cross Entropy (GCE) (Zhang &
Sabuncu, 2018).
3.1 Analysing Intrinsic Sample Weighting in CCE, MAE and GCE
CCE. The CCE loss with respect to (Xi, yi), and its gradient with respect to zij are defined as:
LCCE(Xi,yi) = -logp(yi∣Xi) and dLCCE =［噜，-1, j = yi .	⑵
∂zij	p(j|Xi),	j 6= yi
Therefore, we have || dL∂ZcE ∣∣ι = 2(1 - p(y∕xi)) = 2(1 - pi). Herewe chooseL1 norm to measure
the magnitude of gradient because of its simpler statistics and computation.
Since we back-propagate dLCCE/zi to update the model’s parameters, an example’s gradient mag-
nitude determines how much impact it has, i.e., its weight WCCE = ||d⅛CEl∣ι = 2(1 - pi). In
CCE, more difficult examples with smaller pi get higher weight.
MAE. When it comes to MAE, the loss of (Xi, yi) and gradient with respect to zim are:
3
Under review as a conference paper at ICLR 2020
Table 1: Comparison between GR and other learning supervisions. 0-0.5 and 0-1 indicate the em-
phasis focus is adjustable and ranges from 0 to 0.5 and0 to 1, respectively. Note that GR manipulates
the gradients and is independent of specific losses, e.g., CCE, MAE and GCE.
Supervision	Empirical loss	Gradient rescaling	Emphasis focus	Adjustable emphasis spread
CCE	CCE	No	0	No
MAE	MAE	No	0.5	No
GCE	GCE	No	0 〜0.5	No
GR	CCE/MAE/GCE	Yes	0-1	Yes
——GR∕8-Λ0∙5 ……GR-∕J12-λl zʌ X ~^2'λo Z¾	C i： 	  		d∣m∣⅝⅛-0	80 r —GR-∕34-λO.5 /ʌ二薪擦	⅞∞ /	\ ……GR-∕312-λ2	Φ \—GRm6-Λ2	M /	..V	（：
4
Under review as a conference paper at ICLR 2020
According to Eq. (2), (3), (4), the gradients of CCE, MAE and GCE share the same direction. Our
proposal GR unifies them from the gradient perspective. Being independent of loss formulas, a
sample’s gradient is rescaled linearly so that its weight is wiGR :
GR	λ	∂L ∂LCCE wiGR	∂LMAE wiGR	∂ LGCE wiGR
Wi = g(βpi (1-Pi))=> 诙=^zr WpE = ^zr WMAE =	T，⑹
where λ, β are hyper-parameters for controlling the emphasis focus and spread, respectively. By
choosing a larger λ when more anomalies exist, GR regularises examples weighting by moving
emphasis focus toward relatively easier training data points, thus embracing noise-robustness.
For clarification, we explicitly define the emphasis focus and spread over training examples:
Definition 1 (Emphasis Focus ψ). The emphasis focus refers to those examples that own the largest
weight. Since an example’s weight is determined by its input-to-label relevance score pi, for sim-
plicity, we define the emphasis focus to be an input-to-label score to which the largest weight is
assigned, i.e., ψ = arg max WiGR ∈ [0, 1).
pi
Definition 2 (Emphasis Spread σ). The emphasis spread is the weight variance over all training
instances in a mini-batch, i.e., σ = E((WGR — E(WGR))2), where E(∙) denotes the expectation
value of a variable.
With these definitions, we differentiate GR with other methods in Table 1. We show the sample
weighting curves of GR with different settings in Figure 1. As shown in Figure 1c, the emphasis
spread declines as λ increases. Therefore, we choose larger β values when λ is larger in Sec. 4.2.1.
Principally, transformation g could be designed as any monotonically increasing function. Because
the non-linear exponential mapping can change the overall weights’ variance and relative weights
between any two examples, We choose g(∙) = exp(∙), which works well in our practice. By integral,
the exact loss format is an error function (non-elementary). We summarise several existing cases as
follows (the ellipsis refers to other potential options which can be explored in the future):
WCCE,	β =	2,λ = 0,g = identity
WMAE,	β =	4,λ = 1,g = identity
WiGR = WiGCE,	β =	1, 1 ≥ λ ≥ 0, g = identity
exp(βpλ(1 — pi)),	β ≥	0,λ ≥ 0,g = exp
(7)
3.3 Why Does GR Contribute to Robust Learning?
Let’s regard a deep network z as a black box, which produces C logits. C is the class number.
Then during gradient back-propagation, an example’s impact on the update of z is determined by
its gradient w.r.t. the logit vector. The impact can be decomposed into two factors, i.e., gradient
direction and magnitude. To reduce the impact of a noisy sample, we can either reduce its gradient
magnitude or amend its gradient direction. In this work, inspired by the analysis of CCE, MAE
and GCE, which only differ in the gradient magnitude while perform quite differently, leading to a
natural motivation that gradient magnitude matters. That is why we explore rescaling the gradient
magnitude as illustrated in Figure 1. Itis worth studying amending gradient directions in the future.
4	Experiments
4.1	Image Classification with Clean Training Data
Datasets. We test on CIFAR-10 and CIFAR-100 (Krizhevsky, 2009), which contain 10 and 100
classes, respectively. In CIFAR-10, the training data contains 5k images per class while the test set
includes 1k images per class. In CIFAR-100, there are 500 images per class for training and 100
images per class for testing.
Implementation details. On CIFAR-10, following (He et al., 2016), we adopt ResNet-20 and
ResNet-56 as backbones so that we can compare fairly with their reported results. On CIFAR-100,
we follow D2L (Ma et al., 2018) to choose ResNet-44 and compare with its reported results. We also
use an SGD optimiser with momentum 0.9 and weight decay 10-4. The learning rate is initialised
with 0.1, and multiplied with 0.1 every 5k iterations. We apply the standard data augmentation as
in (He et al., 2016; Ma et al., 2018): The original images are padded with 4 pixels on every side,
followed by a random crop of 32 × 32 and horizontal flip. The batch size is 128.
5
Under review as a conference paper at ICLR 2020
Table 2: Classification accuracies (%) of CCE, and GR on clean CIFAR-10 and CIFAR-100. λ = 0
means the emphasis focus is 0 where we fix β = 2. β = 0 means all examples are treated equally.
Dataset	Backbone	CCE	GR (λ = 0)	GR (β = 0)
CIFAR-10	ResNet-20	91.8	91.8	91.0
	ResNet-56	92.4	92.5	91.9
CIFAR-100	ResNet-44	68.1	68.4	66.4
Results. Our purpose is to show GR can achieve competitive performance with CCE under clean
data to demonstrate its general applicability. As reported in D2L, all noise-tolerant proposals (Patrini
et al., 2017; Reed et al., 2015; Ma et al., 2018) perform similarly with CCE when training labels are
clean. Therefore we do not present other related competitors here. Our reimplemented results are
shown in Table 2. For reference, the reported results in (He et al., 2016) on CIFAR-10 with CCE are
91.3% for ResNet-20 and 93.0% for ResNet-56. In D2L, the result on CIFAR-100 with ResNet-44
is 68.2%. Our reimplemented performance of CCE is only slightly different. For GR, we observe
the best performance when emphasis focus is 0, i.e., λ = 0. Furthermore, it is insensitive to a wide
range of emphasis spreads according to our observations in Figure 5 in the supplementary material.
Treating training examples equally. As shown in Table 2, we obtain competitive performance by
treating all training examples equally when β = 0. This is quite interesting and motivates us that
sample differentiation and reweighting work much better only when noise exists.
4.2	Image Classification with Synthetic Symmetric Label Noise
Symmetric noise generation. Given a probability r , the original label of an image is changed to
one of the other class labels uniformly following (Tanaka et al., 2018; Ma et al., 2018). r denotes
the noise rate. Symmetric label noise generally exists in large-scale real-world applications where
the dataset scale is so large that label quality is hard to guarantee. It is also demonstrated in (Vahdat,
2017) that it is more challenging than asymmetric noisy labels (Reed et al., 2015; Patrini et al., 2017),
which assume that label errors only exist within a predefined set of similar classes. All augmented
training examples share the same label as the original one.
4.2.1	Empirical analysis of GR on CIFAR- 1 0
To understand GR well empirically, we explore the behaviours of GR on CIFAR-10 with r =
20%, 40%, 60%, 80%, respectively. We use ResNet-56 which has larger capacity than ResNet-20.
Design choices. We mainly analyse the impact of different emphasis focuses for different noise
rates. We explore 5 emphasis focuses by setting β = 0 or different λ: 1) None: β = 0. There is
no emphasis focus since all examples are treated equally; 2) 0: λ = 0; 3) 0~0.5: λ = 0.5; 4) 0.5:
λ = 1; 5) 0.5~1: λ = 2. We remark that when λ is larger, the emphasis focus is higher, leading
to relatively easier training data points are emphasised. As shown in Figure 1, when emphasis focus
changes, emphasis spread changes accordingly. Therefore, to set a proper spread for each emphasis
focus, we try 4 emphasis spread and choose the best one3 to compare the impact of emphasis focus.
Results analysis. We show the results in Table 3. The intact training set serves as a validation set
and we observe that its accuracy is always consistent with the final test accuracy. This motivates us
that we can choose our model’s hyper-parameters β, λ via a validation set in practice. We display
the training dynamics in Figure 2. We summarise our observations as follows:
Fitting and generalisation. We observe that CCE always achieves the best accuracy on corrupted
training sets, which indicates that CCE has a strong data fitting ability even if there is severe noise
(Zhang et al., 2017). As a result, CCE has much worse final test accuracy than most models.
Emphasising on harder examples. When there exist abnormal training examples, we obtain the
worst final test accuracy if emphasis focus is 0, i.e., CCE and GR with λ = 0. This unveils that in
applications where we have to learn from noisy training data, it will hurt the model’s generalisation
dramatically if we use CCE or simply focus on harder training data points.
Emphasis focus. When noise rate is 0, 20%, 40%, 60%, and 80%, we obtain the best final test
accuracy when λ = 0, λ = 0.5, λ = 1, λ = 2, and λ = 2, respectively. This demonstrates that
3Since there is a large interval between different β in our four trials, we deduce that the chosen one is not
the optimal. The focus of this work is not to optimize the hyper-parameters.
6
Under review as a conference paper at ICLR 2020
Table 3: Results of CCE, GR on CIFAR-10 with noisy labels. For every model, we show its best test
accuracy during training and the final test accuracy when training terminates, which are indicated by
‘Best’ and ‘Final’, respectively. We also present the results on corrupted training sets and original
intact one. The overlap rate between corrupted and intact sets is (1 - r). Therefore, we can regard
the intact training set as a validation set. When λ is larger, β should be larger as shown in Figure 1c.
Noise Rate r	Emphasis Focus	Model	Testing Accuracy (%)		Accuracy on Training Sets (%)	
			Best	Final	Corrupted/Fitting	Intact/Validation
	0	CCE	86.5	76.8	95.7	80.6
	None	GR (β=0)	83.5	58.1	50.6	60.2
20%	0 (λ = 0)	GR (β = 2)	84.9	76.4	85.3	80.5
	0-0.5 (λ = 0.5)	GR (β = 12)	89.4	87.8	81.5	95.0
	0.5 (λ = 1)	GR (β = 16)	87.3	86.7	78.4	93.8
	0.5-1 (λ = 2)	GR (β = 24)	85.8	85.5	76.0	91.4
	0	CCE	82.8	60.9	83.0	64.4
	None	GR (β=0)	71.8	44.9	31.3	45.8
40%	0 (λ = 0)	GR (β = 1)	78.4	65.6	63.3	66.6
	0-0.5 (λ = 0.5)	GR (β = 12)	85.1	79.9	67.7	85.7
	0.5 (λ = 1)	GR (β = 16)	84.7	83.3	60.3	88.9
	0.5-1 (λ = 2)	GR (β = 20)	52.7	52.7	35.4	53.6
	0	CCE	69.5	37.2	84.1	40.5
	None	GR (β=0)	69.9	57.9	40.1	58.6
60%	0 (λ = 0)	GR (β = 0.5)	72.3	53.9	42.1	55.1
	0-0.5 (λ = 0.5)	GR (β = 12)	77.5	58.5	55.5	62.6
	0.5 (λ = 1)	GR (β = 12)	71.9	70.0	41.0	73.9
	0.5-1 (λ = 2)	GR (β = 12)	80.2	72.5	44.9	75.4
	0	CCE	36.1	16.1	54.3	18.4
	None	GR (β=0)	44.4	28.2	20.6	28.8
80%	0 (λ = 0)	GR (β = 0.5)	46.2	21.3	27.8	23.1
	0-0.5 (λ = 0.5)	GR (β = 8)	51.6	22.4	46.1	24.4
	0.5 (λ = 1)	GR (β = 8)	35.5	31.5	19.8	32.3
	0.5-1 (λ = 2)	GR (β = 12)	33.0	32.8	14.2	32.6
when noise rate is higher, we can improve a model’s robustness by moving emphasis focus towards
relatively less difficult examples with a larger λ, which is informative in practice.
Emphasis spread. As displayed in Table 3 and Figures 7-10 in the supplementary material, emphasis
spread also matters a lot when fixing emphasis focus, i.e., fixing λ. For example in Table 3 , when
λ = 0, although focusing on harder examples similarly with CCE, GR can outperform CCE by
modifying the emphasis spread. As shown in Figures 7-10, some models even collapse and cannot
converge if the emphasis spread is not rational.
4.2.2	Competing with the state-of-the-art on CIFAR- 1 0
Implementation details. We follow the same settings as MentorNet (Jiang et al., 2018) to compare
fairly with its reported results. Optimiser and data augmentation are described in Section 4.1.
Competitors. FullModel is the standard CCE trained using L2 weight decay and dropout (Srivastava
et al., 2014). Forgetting (Arpit et al., 2017) searches the dropout parameter in the range of (0.2-0.9).
Self-paced (Kumar et al., 2010), Focal Loss (Lin et al., 2017), and MentorNet (Jiang et al., 2018)
are representatives of example reweighting algorithms. Reed Soft (Reed et al., 2015) is a weakly-
supervised learning method. All methods use GoogLeNet V1 (Szegedy et al., 2015).
Results. We compare the results under different noise rates in Table 4. GR with fixed hyper-
parameters β = 8, λ = 0.5 outperforms the state-of-the-art GCE by a large margin, especially when
label noise becomes severe. Better results can be expected when optimising the hyper-parameters
for each case. We remark that FullModel (naive CCE) (Jiang et al., 2018) was trained with L2
weight decay and dropout. However, GR’s regularization effect is much better in both clean and
noisy cases.
7
Under review as a conference paper at ICLR 2020
(a) r = 20%.	(b) r = 40%.	(c) r = 60%.
Figure 2: The learning dynamics of ResNet-56 on CIFAR-10, i.e., training and testing accuracies
along with training iterations. The legend in the top left is shared by all subfigures. ‘xxx: yyy’ means
‘method: emphasis focus’. The results of r = 80% are shown in Figure 6 in the supplementary
material. We have two key observations: 1) When noise rate increases, better generalisation is
obtained with higher emphasis focus, i.e., focusing on relatively easier examples; 2) Both overfitting
and underfitting lead to bad generalisation. For example, ‘CCE: 0’ fits training data much better
than the others while ‘GR: None’ generally fits it unstably or a lot worse. Better viewed in colour.
4.2.3	Competing with the state-of-the-art on CIFAR-100
Implementation details. Most baselines have been reimplemented in (Ma et al., 2018) with the
same settings. Therefore, for direct comparison, we follow exactly their experimental configurations
and use ResNet-44 (He et al., 2016). Optimiser and data augmentation are described in Section 4.1.
We repeat training and evaluation 5 times where different random seeds are used for generating
noisy labels and model’s initialisation. The mean test accuracy and standard deviation are reported.
Competitors. We compare with D2L (Ma et al., 2018), GCE (Zhang & Sabuncu, 2018), and other
baselines reimplemented in D2L: 1) Standard CCE (Ma et al., 2018); 2) Forward (Patrini et al.,
2017) uses a noise-transition matrix to multiply the network’s predictions for label correction; 3)
Backward (Patrini et al., 2017) applies the noise-transition matrix to multiply the CCE losses for
loss correction; 4) Bootstrapping (Reed et al., 2015) trains models with new labels generated by
a convex combination of the original ones and their predictions. The convex combination can be
soft (Boot-soft) or hard (Boot-hard); 5) D2L (Ma et al., 2018) achieves noise-robustness from a
novel perspective of restricting the dimensionality expansion of learned subspaces during training
and is the state-of-the-art; 6) Since GCE outperforms MAE (Zhang & Sabuncu, 2018), we only
reimplement GCE for comparison; 7) SL (Wang et al., 2019c) boosts CCE symmetrically with a
noise-robust counterpart, i.e., reverse cross entropy.
Results. We compare the results of GR and other algorithms in Table 5. GR outperforms other
competitors by a large margin, especially when label noise is severe, e.g., r = 40% and 60%. More
importantly, we highlight that GR is much simpler without any extra information. Compared with
Forward and Backward, GR does not need any prior knowledge about the noise-transition matrix.
Bootstrapping targets at label correction and is time-consuming. D2L estimates the local intrinsic
dimensionality every b mini-batches and checks the turning point for dimensionality expansion every
e epochs. However, b and e are difficult to choose and iterative monitoring is time-consuming.
4.3	Image Classification with Real-world Unknown Noise
Dataset. Clothing 1M (Xiao et al., 2015) contains 1 million images. It is an industrial-level dataset
and its noise structure is agnostic. According to (Xiao et al., 2015), around 61.54% training labels
are reliable, i.e., the noise rate is about 38.46%. There are 14 classes from several online shopping
websites. In addition, there are 50k, 14k, and 10k images with clean labels for training, validation,
8
Under review as a conference paper at ICLR 2020
Table 4: The results of GR and other noise-robust approaches on CIFAR-10 using GoogLeNet V1.
Noise rate r	FullModel (naive CCE)	Forgetting	Self- paced	Focal Loss	Reed Soft	MentorNet PD	Mentor DD	GCE	(β=	GR 8,λ = 0.5)
0	0.81	-	-	-	-	-	-	0.83		0.85
20%	0.76	0.76	0.80	0.77	0.78	0.79	0.79	0.81		0.83
40%	0.73	0.71	0.74	0.74	0.73	0.74	0.76	0.78		0.79
80%	0.42	0.44	0.33	0.40	0.39	0.44	0.46	0.50		0.57
Table 5: The accuracies (%) of GR and recent approaches on CIFAR-100. The results of fixed
parameters (β = 8, λ = 0.5) are shown in the second last column. With a little effort for optimising
β and λ, the results and corresponding parameters are presented in the last column. The trend is
consistent with Table 3: When r raises, we can increase β, λ for better robustness. The increasing
scale is much smaller. This is because CIFAR-100 has 100 classes so that its distribution of pi
(input-to-label relevance score) is different from CIFAR-10 after softmax normalisation.
Nraoteisre CCE
GCE Forward Backward Bhoaordt-	Bsoooftt-	D2L
GR
SL	(β = 8,	GR (β,λ)
λ=0.5)
20% 52.9±0.2 53.4±0.3	60.3±0.2	58.7±0.3	58.5±0.4	57.3±0.3	62.2±0.4 60.0±0.2 62.6±0.3 64.1±0.2	(6, 0.3)
40% 42.9±0.2 47.0±0.2	51.3±0.3	45.4±0.2	44.4±0.1	41.9±0.1	52.0±0.3 53.7±0.1 59.3±0.2 60.0±0.1	(6, 0.4)
60% 30.1±0.2 41.0±0.2	41.2±0.3	34.5±0.2	36.7±0.3	32.3±0.1	42.3±0.2 41.5±0.0 49.9±0.3 49.9±0.3	(8, 0.5)
and testing, respectively. Here, we follow and compare with existing methods that only learn from
noisy training data since we would like to avoid exploiting auxiliary information.
Implementation details. We train ResNet-50 (He et al., 2016) and follow exactly the same settings
as (Patrini et al., 2017; Tanaka et al., 2018): 1) Initialisation: ResNet-50 is initialised by publicly
available model pretrained on ImageNet (Russakovsky et al., 2015); 2) Optimisation: A SGD opti-
miser with a momentum of 0.9 and a weight decay of 10-3 is applied. The learning rate starts at
10-3 and is divided by 10 after 5 epochs. Training terminates at 10 epochs; 3) Standard data aug-
mentation: We first resize a raw input image to 256 × 256, and then crop it randomly at 224 × 224
followed by random horizontal flipping. The batch size is 64 due to memory limitation. Since the
noise rate is around 38.46%, we simply set λ = 1, β = 16 following Table 3 when noise rate is 40%.
Competitors. We compare with other noise-robust algorithms that have been evaluated on Cloth-
ing 1M with similar settings: 1) Standard CCE (Patrini et al., 2017); 2) Since Forward outper-
forms Backward on Clothing 1M (Patrini et al., 2017), we only present the result of Forward; 3)
S-adaptation applies an additional softmax layer to estimate the noise-transition matrix (Goldberger
& Ben-Reuven, 2017); 4) Masking is a human-assisted approach that conveys human cognition
to speculate the structure of the noise-transition matrix (Han et al., 2018a). 5) Label optimisation
(Tanaka et al., 2018) learns latent true labels and model’s parameters iteratively. Two regularisation
terms are added for label optimisation and adjusted in practice.
Results. The results are compared in Table 6. Under real-world agnostic noise, GR also outperforms
the state-of-the-art. It is worth mentioning that the burden of noise-transition matrix estimation in
Forward and S-adaptation is heavy due to alternative optimisation steps, and such estimation is
non-trivial without big enough data. Masking exploits human cognition of a structure prior and
reduces the burden of estimation, nonetheless its performance is not competitive. Similarly, Label
Optimisation requires alternative optimisation steps and is time-consuming.
4.4	Video retrieval with diverse anomalies
Dataset and evaluation settings. MARS contains 20,715 videos of 1,261 persons (Zheng et al.,
2016). There are 1,067,516 frames in total. Because person videos are collected by tracking and de-
tection algorithms, abnormal examples exist as shown in Figure 3 in the supplementary material. We
remark that there are some anomalies containing only background or an out-of-distribution person.
Exact noise type and rate are unknown. Following standard settings, we use 8,298 videos of 625
persons for training and 12,180 videos of the other 636 persons for testing. We report the cumulated
matching characteristics (CMC) and mean average precision (mAP) results.
9
Under review as a conference paper at ICLR 2020
Table 6: The classification accuracy (%) on Clothing1M with ResNet-50. CCE and GCE were
reported in (Patrini et al., 2017) and (Wang et al., 2019c), respectively. CCE* and GCE* are our
reproduced results using the Caffe framework (Jia et al., 2014).
B-soooftt Forward	Bilevel Optimisation	S-adaptation Masking	Joint Optimisation	CCE CCE* GCE GCE*	SL	GR
69.1	69.8	69.9	704	71.1 ^^	72.2	68.9 71.7 69.8 72.5	71.0	73.2
Table 7: The video retrieval results on MARS. For fair comparison, all other methods use
GoogLeNet V2 except DRSA and CAE using more complex ResNet-50.
Metric	CCE	MAE	GCE	DRSA	CAE	OSM+CAA	GR
mAP (%)	58.1	12.0	31.6	65.8	67.5	72.4	72.8
CMC-1 (%)	73.8	26.0	51.5	82.3	82.4	84.7	84.3
Implementation details. Following (Liu et al., 2017; Wang et al., 2019a), we train GoogleNet V2
(Ioffe & Szegedy, 2015) and treat a video as an image set, which means we use only appearance
information without exploiting latent temporal information. A video’s representation is simply the
average fusion of its frames’ representations. The learning rate starts from 0.01 and is divided by 2
every 10k iterations. We stop training at 50k iterations. We apply an SGD optimiser with a weight
decay of 0.0005 and a momentum of 0.9. The batch size is 180. We use standard data augmentation:
a 227 × 227 crop is randomly sampled and flipped after resizing an original image to 256 × 256.
Training settings are the same for each method. We implement GCE with its reported best settings.
At testing, following (Wang et al., 2019a; Movshovitz-Attias et al., 2017; Law et al., 2017), we first
L2 normalise videos’ features and then calculate the cosine similarity between every two of them.
Results. The results are displayed in Table 7. Although DRSA (Li et al., 2018) and CAE (Chen et al.,
2018) exploit extra temporal information by incorporating attention mechanisms, GR is superior to
them in terms of both effectiveness and simplicity. OSM+CAA (Wang et al., 2019a) is the only
comparable method. However, OSM+CAA combines CCE and weighted contrastive loss to address
anomalies, thus being more complex than GR. In addition, we highlight that one query may have
multiple matching instances in the MARS benchmark. Consequently, mAP is a more reliable and
accurate performance assessment. GR is the best in terms of mAP.
4.5	B eating S tandard Regularisers Under Label Noise
In Table 8, we compare our proposed regulariser GR with other standard ones, i.e., L2 weight decay
and Dropout (Srivastava et al., 2014). We set the dropout rate to 0.2 and L2 weight decay rate to
10-4. For GR, as mentioned in Section 4.2.3, we fix β = 8, λ = 0.5. Interestingly, Dropout+L2
achieves 52.8% accuracy, which is even better than the state-of-the-art in Table 5, i.e., D2L with
52.0% accuracy. However, GR is better than those standard regularisers and their combinations
significantly. GR works best when it is together with L2 weight decay.
Table 8: Results of GR and other standard regularisers on CIFAR-100. We set r = 40%, i.e., the
label noise is severe but not belongs to the majority. We train ResNet-44. We report the average test
accuracy and standard deviation (%) over 5 trials. Baseline means CCE without regularisation.
Baseline	L2	Dropout	Dropout+L2	GR	GR+L2	GR+Dropout	GR+L2+Dropout
44.7±0.1	51.5±0.4	46.7±0.5	52.8±0.4	55.7±0.3	59.3±0.2	54.3±0.4	58.3±0.3
5	Conclusion
In this work, we present three main contributions: 1) We analyse and answer a core research ques-
tion: What training examples should be focused on and how large the emphasis spread should be?
2) We uncover and analyse that two basic factors, emphasis focus and spread, should be babysat
simultaneously when it comes to examples weighting. Consequently, we propose a simple yet ef-
fective gradient rescaling framework serving as emphasis regularisation. 3) Extensive experiments
on different tasks using different network architectures are reported for better understanding and
demonstration of GR’s effectiveness, which are also valuable for applying GR in practice.
10
Under review as a conference paper at ICLR 2020
References
Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron Courville, and Yoshua Bengio. Variance
reduction in sgd by distributed importance sampling. In ICLR Workshop, 2016.
Devansh Arpit, StanisIaW Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon
Lacoste-Julien. A closer look at memorization in deep netWorks. In ICML, 2017.
Dapeng Chen, Hongsheng Li, Tong Xiao, Shuai Yi, and Xiaogang Wang. Video person re-
identification With competitive snippet-similarity aggregation and co-attentive snippet embedding.
In CVPR, 2018.
Aritra Ghosh, Himanshu Kumar, and PS Sastry. Robust loss functions under label noise for deep
neural netWorks. In AAAI, 2017.
Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-netWorks using a noise adaptation
layer. In ICLR, 2017.
Siddharth Gopal. Adaptive sampling for sgd by exploiting side information. In ICML, 2016.
Sheng Guo, Weilin Huang, Haozhi Zhang, Chenfan Zhuang, Dengke Dong, MattheW R Scott, and
Dinglong Huang. Curriculumnet: Weakly supervised learning from large-scale Web images. In
ECCV, 2018.
Bo Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, Ivor Tsang, Ya Zhang, and Masashi Sugiyama.
Masking: A neW perspective of noisy supervision. In NeurIPS, 2018a.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural netWorks With extremely noisy labels. In
NeurIPS, 2018b.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.
Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train
deep netWorks on labels corrupted by severe noise. In NeurIPS, 2018.
Geoffrey E Hinton. To recognize shapes, first learn to generate images. Progress in brain research,
pp. 535-547, 2007.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep netWork training by
reducing internal covariate shift. In ICML, 2015.
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Ser-
gio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embed-
ding. In ACMMM, 2014.
Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-
driven curriculum for very deep neural netWorks on corrupted labels. In ICML, 2018.
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification With deep convo-
lutional neural netWorks. In NeurIPS, 2012.
David Krueger, Nicolas Ballas, StanislaW Jastrzebski, Devansh Arpit, Maxinder S KanWal, Tegan
Maharaj, Emmanuel Bengio, Asja Fischer, and Aaron Courville. Deep nets don’t learn via mem-
orization. In ICLR Workshop, 2017.
M PaWan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable
models. In NeurIPS, 2010.
Jan Larsen, L Nonboe, Mads Hintz-Madsen, and Lars Kai Hansen. Design of robust neural netWork
classifiers. In ICASSP, 1998.
11
Under review as a conference paper at ICLR 2020
Marc T Law, Raquel Urtasun, and Richard S Zemel. Deep spectral clustering learning. In ICML,
2017.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, pp. 436, 2015.
Kuang-Huei Lee, Xiaodong He, Lei Zhang, and Linjun Yang. Cleannet: Transfer learning for
scalable image classifier training with label noise. In CVPR, 2018.
Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S Kankanhalli. Learning to learn from noisy
labeled data. In CVPR, 2019.
Shuang Li, Slawomir Bak, Peter Carr, and Xiaogang Wang. Diversity regularized spatiotemporal
attention for video-based person re-identification. In CVPR, 2018.
Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia Li. Learning from
noisy labels with distillation. In ICCV, 2017.
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object
detection. In ICCV, 2017.
Yu Liu, Junjie Yan, and Wanli Ouyang. Quality aware network for set to set recognition. In CVPR,
2017.
Ilya Loshchilov and Frank Hutter. Online batch selection for faster training of neural networks. In
ICLR Workshop, 2016.
Xingjun Ma, Yisen Wang, Michael E Houle, Shuo Zhou, Sarah M Erfani, Shu-Tao Xia, Sudanthi
Wijewickrema, and James Bailey. Dimensionality-driven learning with noisy labels. In ICML,
2018.
Eran Malach and Shai Shalev-Shwartz. Decoupling "when to update" from "how to update". In
NeurIPS, 2017.
Takeru Miyato, Shin-ichi Maeda, Shin Ishii, and Masanori Koyama. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE Transactions on Pattern
Analysis and Machine Intelligence, pp. 1979 - l993, 2018.
Yair Movshovitz-Attias, Alexander Toshev, Thomas K Leung, Sergey Ioffe, and Saurabh Singh. No
fuss distance metric learning using proxies. In ICCV, 2017.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In NeurIPS, 2013.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Ra-
binovich. Training deep neural networks on noisy labels with bootstrapping. In ICLR Workshop,
2015.
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for
robust deep learning. In ICML, 2018.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International Journal of Computer Vision, pp. 211-252, 2015.
Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors
with online hard example mining. In CVPR, 2016.
Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. Parsing natural scenes and natural
language with recursive neural networks. In ICML, 2011.
12
Under review as a conference paper at ICLR 2020
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine
Learning Research ,pp.1929-1958, 2014.
Sainbayar Sukhbaatar and Rob Fergus. Learning from noisy labels with deep neural networks. arXiv
preprint arXiv:1406.2080, 2014.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
CVPR, 2015.
Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization frame-
work for learning with noisy labels. In CVPR, 2018.
Sunil Thulasidasan, Tanmoy Bhattacharya, Jeff Bilmes, Gopinath Chennupati, and Jamal Mohd-
Yusof. Combating label noise in deep learning using abstention. In ICML, 2019.
Arash Vahdat. Toward robustness against label noise in training deep discriminative neural networks.
In NeurIPS, 2017.
Brendan Van Rooyen, Aditya Menon, and Robert C Williamson. Learning with symmetric label
noise: The importance of being unhinged. In NeurIPS. 2015.
Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge Belongie. Learning
from noisy large-scale datasets with minimal supervision. In CVPR, 2017.
Xinshao Wang, Yang Hua, Elyor Kodirov, Guosheng Hu, and Neil M. Robertson. Deep metric
learning by online soft mining and class-aware attention. In AAAI, 2019a.
Xinshao Wang, Elyor Kodirov, Yang Hua, and Neil M Robertson. Improving MAE against CCE
under label noise. arXiv preprint arXiv:1903.12141, 2019b.
Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, and Shu-Tao Xia.
Iterative learning with open-set noisy labels. In CVPR, 2018.
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross
entropy for robust learning with noisy labels. In ICCV, 2019c.
Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy
labeled data for image classification. In CVPR, 2015.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In ICLR, 2017.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. ICLR, 2018a.
Xu Zhang, Felix Xinnan Yu, Svebor Karaman, Wei Zhang, and Shih-Fu Chang. Heated-up softmax
embedding. arXiv preprint arXiv:1809.04157, 2018b.
Zhilu Zhang and Mert R Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels. In NeurIPS, 2018.
Liang Zheng, Zhi Bie, Yifan Sun, Jingdong Wang, Chi Su, Shengjin Wang, and Qi Tian. Mars: A
video benchmark for large-scale person re-identification. In ECCV, 2016.
13
Under review as a conference paper at ICLR 2020
Supplementary Material for
Robust Discriminative Representations Learning via Gradient Rescaling: An
Emphasis Regularisation Perspective
A Display of Semantically Abnormal Training Examples
This video is labelled as the person wearing green shirt.
Figure 3: Diverse semantically abnormal training examples highlighted by red boxes. The 1st row
shows synthetic abnormal examples from corrupted CIFAR-10 (Krizhevsky, 2009). The 2nd and
3rd rows present realistic abnormal examples from video person re-identification benchmark MARS
(Zheng et al., 2016).
Out-of-distribution anomalies: 1) The first image in the 3rd row contains only background and no
semantic information at all. 2) The 2nd first image or the last one in the 3rd row may contain a
person that does not belong to any person in the training set.
In-distribution anomalies: 1) Some images of deer class are wrongly annotated to horse class. 2)
We cannot decide the object of interest without any prior when an image contains more than one
object, e.g., some images contain two persons in the 2nd row.
B Derivation Details of Softmax, CCE, MAE and GCE
B.1	Derivation of Softmax Normalisation
Based on Eq. (1), we have
p(yi|xi)-1 = 1 +	exp(zij - ziyi).
j6=yi
(8)
For left and right sides of Eq. (8), we calculate their derivatives w.r.t. zij simultaneously.
Ifj=yi,
T	∂p(yi∣Xi)
p(yi∣χi)2	Ziyi
-	exp(zij - ziyi)
j6=yi
=> dp(yilxi) = p(yi∣xi)(1 - p(yi∣xi)).
ziyi
(9)
If j 6= yi ,
-1	∂p(yi∣Xi)
p(yi∣χi)2	Zij
= exp(zij - ziyi)
=>
∂p(yi∣xi)
Zij
(10)
= -p(yi|xi)p(j|xi).
14
Under review as a conference paper at ICLR 2020
In summary, the derivation of softmax layer is:
dp(y，|xi) = (p(yi∣χi)(i -p(yi∣χi)),	j = yi
∂ Zij I-P(yi|Xi)PjIXi),	j = y
B.2	Derivation of CCE
According to Eq. (2), we have
LCCE(xi; fθ, W) = - logP(yi|xi).
Therefore, we obtain (the parameters are omitted for brevity),
dLccE = f-p(yi|xi)-1, j = yi
∂p(j∣χi)	[0,	j = yi .
(11)
(12)
(13)
B.3	Derivation of MAE
-2,	j = yi
0,	j 6= yi
According to Eq. (3), we have
LMAE(xi; fθ, W) = 2(1 - (P(yi|xi)).
Therefore, we obtain
dLMAE
∂p(j∣Xi)
B.4	Derivation of GCE
According to Eq. (4), we have
LGCE(Xi； fθ, W) = 1 - p(yi|Xi)q .
Therefore, we obtain
dLGCE = [-p(yi|Xi)q-1, j = yi
∂p(j∣Xi)	[0,	j = yi
B.5	DERIVATIVES W.R.T. LOGITS Zi
B.5.1	dLCCE/dZi
The calculation is based on Eq. (13) and Eq. (11).
If j = yi , we have:
C
∂LccE = 'X ∂LccE dp(yi∣Xi)
dziyi	j=1 dP(j|Xi)	Zij
= P(yi |Xi ) - 1.
If j 6= yi , it becomes:
C
dLccE =	dLccE dp(yi∣Xi)
dzij	j=1 dp(j ∣ Xi)	Zij
= P(j |Xi).
In summary, dLccE /dZi can be represented as:
dLccE = Jp(yi∣Xi) - 1, j = yi
dzij lp(j|Xi),	j = yi .
(14)
(15)
(16)
(17)
(18)
(19)
(20)
15
Under review as a conference paper at ICLR 2020
B.5.2 ∂Lmae/∂z
The calculation is analogous with that of ∂LCCE /∂zi .
According to Eq. (15) and Eq. (11), ifj = yi:
otherwise (j 6= yi):
C
dLMAE = X dLMAE dP(yi|Xi)
dziyi - j=1 dP(jlχi)	Zij	(21)
= -2p(yi|Xi)(1 -p(yi|Xi)).
C
dLMAE = X dLMAE dP(yi|Xi)
dzij	- j=1 dP(jlχi)	Zij	(22)
= 2p(yi|Xi)p(j|Xi).
In summary, dLMAE/dZi is:
dLMAE = 12p(yi∣Xi)(p(y∕xi) - 1), j = y
dzij t2p(yi∣Xi)p(j∣Xi),	j = y
(23)
B.5.3 dLGCE/dZi
The calculation is based on Eq. (17) and Eq. (11).
If j = yi , we have:
dLGCE
d Ziyi
C
X
j=1
dLGCE dp(yi∣Xi)
dp(j∣Xi)	Zij
(24)
= p(yi|Xi)q(p(yi|Xi) - 1).
If j 6= yi , it becomes:
C
dLGCE = X dLGCE dP(yjXi)
dzij-	j=1 dp(j∣Xi)	Zij
= p(yi|Xi)qp(j|Xi).
In summary, dLGCE/dZi can be represented as:
dLGCE = ∫p(yi∣Xi)q(p(yi∣Xi) - 1), j = y
dZij lp(yi∣Xi )q p(j∣Xi),	j = y
(25)
(26)
C Small-scale Fine-grained visual categorisation of vehicles
How does GR perform on small datasets, for example, the number of data points is no more than
5,000? We have tested GR on CIFAR-10 and CIFAR-100 in the main paper. However, both of them
contain a training set of 50,000 images.
For this question, we answer it from different perspectives as follows:
1.	The problem of label noise we study on CIFAR-10 and CIFAR-100 in Section 4.2 is of similar
scale. For example:
•	In Table 4, when noise rate is 80% on CIFAR-10, the number of clean training examples is
around 50, 000 × 20% = 5, 000 × 2. Therefore, this clean set is only two times as large as
5,000. Beyond, the learning process may be interrupted by other noisy data points.
•	In Table 5, when noise rate is 60% on CIFAR-100, the number of clean training data points
is about 50, 000 × 40% = 5, 000 × 4, i.e., four times as large as 5,000.
16
Under review as a conference paper at ICLR 2020
2.	We compare GR with other standard regularisers on a small-scale fine-grained visual categori-
sation problem in Table 9.
Vehicles-10 Dataset. In CIFAR-100 Krizhevsky (2009), there are 20 coarse classes, including ve-
hicles 1 and 2. Vehicles 1 contains 5 fine classes: bicycle, bus, motorcycle, pickup truck, and train.
Vehicles 2 includes another 5 fine classes: lawn-mower, rocket, streetcar, tank, and tractor. We build
a small-scale vehicles classification dataset composed of these 10 vehicles from CIFAR-100. Specif-
ically, the training set contains 500 images per vehicle class while the testing set has 100 images per
class. Therefore, the number of training data points is 5,000 in total.
Table 9: The test accuracy (%) of GR and other standard regularisers on Vehicles-10. We train
ResNet-44. Baseline means CCE without regularisation. We test two cases: with symmetric label
noise r = 40% and without symmetric label noise r = 0.
r	Baseline	L2	Dropout	Dropout+L2	GR	GR+L2	GR+Dropout	GR+L2+Dropout
0	75.4	76.4	77.9	78.7	83.8	84.4	84.5	84.7
40%	42.3	44.8	41.6	47.4	45.8	55.7	48.8	58.1
D	Training under asymmetric label Noise
We evaluate on CIFAR-100, whose 100 classes are grouped into 20 coarse classes. Every coarse
class has 5 fine classes. Within each coarse class, an image’s label is flipped to one of the other four
labels uniformly with a probability r. r represents the noise rate. We set r = 0.2. The results are
displayed in Table 10. When GR is used, the performance is better than its counterparts without GR.
Table 10: The test accuracy (%) of GR and other standard regularisers trained under asymmetric
label noise. We train ResNet-44. Baseline means CCE without regularisation. We simply fix β =
8, λ = 0.5 when GR is used. Better results can be expected if β, λ are optimised for each case.
Baseline	L2	Dropout	DroPout+L2	GR	GR+L2	GR+Dropout	GR+L2+Dropout
-551	59^	57.1	60^	605	637	59^	614
E The Effectivenes s of Label Correction
The results are shown in Table 11.
Table 11: How much fitting of the clean training subset and how much fitting of the noisy training
subset? Is it plausible to correct the labels of training data?
Our results demonstrate the effectiveness of label correction using DNNs trained by GR.
When retraining from scratch on the relabelled training data, we do not adjust the hyper-parameters
β and λ. Therefore, the reported results of retraining on relabelled datasets are not the optimal.
Noise Rate r	Emphasis Focus	Model		Testing Accuracy (%)		Accuracy on Training Sets (%)		Fitting degree of subsets (%)		Retrain after label correction
				Best	Final	Noisy	Intact	Clean	Noisy	
	0	CCE		86.5	76.8	^^5T7^^	80.6	99.0	85.9	—
20%	0-0.5 (λ = 0.5)	GR (β =	12)	89.4	87.8	81.5	95.0	98.8	11.7	89.3 (+1.5)
	0	CCE		82.8	60.9	83.0	64.4	97.0	81.1	—
40%	0.5 (λ = 1)	GR (β =	16)	84.7	83.3	60.3	88.9	94.8	7.5	85.3 (+2)
17
Under review as a conference paper at ICLR 2020
F More Empirical Results
F.1 Review
Question: What training examples should be focused on and how much more should they be em-
phasised when training DNNs under label noise?
Proposal: Gradient rescaling incorporates emphasis focus (centre/focal point) and emphasis spread,
and serves as explicit regularisation in terms of sample reweighting/emphasis.
Finding: When noise rate is higher, we can improve a model’s robustness by moving emphasis
focus towards relatively less difficult examples.
F.2 Detailed Results on CIFAR-100
The more detailed results on CIFAR-100 are shown in Table 12, which is the supplementary of
Table 5 in the main text.
Table 12: Exploration of GR with different emphasis focuses (centres) and spreads on CIFAR-100
when r = 20%, 40%, 60%, respectively. This table presents detailed information of optimising λ, β
mentioned in Table 5 in the paper. Specifically, for each λ, we try 5 β values from {2, 4, 6, 8, 10} and
select the best one as the final result of the λ. We report the mean test accuracy over 5 repetitions.
Our key finding is demonstrated again: When r raises, we can increase β, λ for better robustness.
The increasing scale is much smaller than CIFAR-10. This is because CIFAR-100 has 100 classes so
that its distribution of pi (input-to-label relevance score) is different from CIFAR-10 after softmax
normalisation.
Noise rate r	λ	β	Testing accuracy (%)
	0.1	-^4^^	61.3
	0.2	4	63.3
20%	0.3	6	64.1
	0.4	6	63.6
	0.5	8	62.6
	0.6	8	62.5
	0.1	-^4^^	55.5
	0.2	4	58.2
40%	0.3	6	59.1
	0.4	6	60.0
	0.5	8	59.3
	0.6	8	58.5
	0.1	-^4^^	44.9
	0.2	4	47.5
60%	0.3	6	49.7
	0.4	6	49.9
	0.5	8	49.9
	0.6	8	47.3
F.3 Detailed Training Dynamics
There are more detailed training dynamics displayed in the Figures 4-10.
18
Under review as a conference paper at ICLR 2020
CCE
—GR-∕3O.5-λO
..... GR-∕31-λO
^-GR-02-入0
..... GR-∕34-λO
-GR/O
Figure 4: The training and test accuracies on clean CIFAR-10 along with training iterations. The
training labels are clean. We fix λ = 0 to focus on harder examples while changing emphasis spread
controller β. The backbone is ResNet-20. The results of ResNet-56 are shown in Figure 5. Better
viewed in colour.
—GR-∕3O.5-ΛO
...GR-∕31-λO
GR-∕32-Λ0
...GR-∕34-λO
—GR-OO
19.8.76.5.432 2
666666666
S ①一 WyB U'SJ.L
1.9.8.76,54.321
Cicicici0.60.60.
S ①PBJrmfS ①一
Figure 5: The training and test accuracies on clean CIFAR-10 along with training iterations. The
training labels are clean. We fix λ = 0 to focus on more difficult examples while changing emphasis
spread controller β . The backbone is ResNet-56. The results of ResNet-20 are shown in Figure 4.
Better viewed in colour.
19
Under review as a conference paper at ICLR 2020
GR: 0-0.5
GR： 0.5~l
Figure 6: The learning dynamics on CIFAR-10 (r = 80%) With ResNet-56, i.e., training and testing
accuracies along With training iterations. The legend in the top left is shared by tWo subfigures. ‘xxx:
yyy’ means ‘method: emphasis focus’. The results of r = 20%, 40%, 60% are shoWn in Figure 2 in
the paper.
We have tWo key observations: 1) When noise rate increases, better generalisation is obtained With
higher emphasis focus, i.e., focusing on relatively easier examples; 2) Both overfitting and underfit-
ting lead to bad generalisation. For example, ‘CCE: 0’ fits training data much better than the others
While ‘GR: None’ generally fits it unstably or a lot Worse. Better viewed in colour.
一CCE
■■…GR/0
5
Iterations
-G R-^O.5-λO - G R-∕32-λO
...GfV^l-AO ......GR-∕34-λO
10
×104
XlO4
IO4
Figure 7: ResNet-56 on CIFAR-10 (r = 20%). From left to right, the results of four emphasis
focuses 0, 0〜0.5, 0.5, 0.57 With different emphasis spreads are displayed in each column respec-
tively. When λ is larger, β should be larger as displayed in Figure 1c in the paper. Specifically :
1) When	λ	= 0:	We	tried β	=	0.5, 1, 2, 4;
2)	When	λ	= 0.5: We tried	β	= 4, 8, 12, 16;
3)	When	λ	= 1:	We	tried β	=	8, 12, 16, 20;
4)	When	λ	= 2:	We	tried β	=	12, 16, 20, 24.
20
Under review as a conference paper at ICLR 2020
GR-^2O-Λ1
GR-04-Mλ5…・,GR-β8-Λ0.5
-GRm2-入。,5
GR-81600.5
一GR√2.5-ΛO —GR∕2-Λ0
■"GR-Z?1-AO
-GR-M-AO
—GR-^16-Λ1
-GR-08-λl
'GR-∕312-λl
-GR√H2C2--GRN2O-Λ2
"GR-∕316-λ2
GR-624-λ2
Figure 8: ResNet-56 on CIFAR-10 (r = 40%). From left to right, the results of four emphasis
focuses 0, 0~0.5, 0.5, 0.5~1 With different emphasis spreads are displayed in each column respec-
tively. When λ is larger, β should be larger as displayed in Figure 1c in the paper. Specifically :
1) When	λ	= 0:	We	tried β	=	0.5, 1, 2, 4;
2) When	λ	= 0.5: We tried	β	= 4, 8, 12, 16;
3) When	λ	= 1:	We	tried β	=	8, 12, 16, 20;
4) When	λ	= 2:	We	tried β	=	12, 16, 20, 24.
S①一ɔsroɔe U-Sl
Figure 9: ResNet-56 on CIFAR-10 (r = 60%). From left to right, the results of four emphasis
focuses 0, 0~0.5, 0.5, 0.5~1 with different emphasis spreads are displayed in each column respec-
tively. When λ is larger, β should be larger as displayed in Figure 1c in the paper. Specifically :
1) when	λ	= 0:	we	tried β	=	0.5, 1, 2, 4;
2) when	λ	= 0.5: we tried	β	= 4, 8, 12, 16;
3) when	λ	= 1:	we	tried β	=	8, 12, 16, 20;
4) when	λ	= 2:	we	tried β	=	12, 16, 20, 24.
21
Under review as a conference paper at ICLR 2020
Iterations
Figure 10: ResNet-56 on CIFAR-10 (r = 80%). From left to right, the results of four emphasis
focuses 0, 0〜0.5, 0.5, 0.57 With different emphasis spreads are displayed in each column respec-
tively. When λ is larger, β should be larger as displayed in Figure 1c in the paper. Specifically :
1) When λ	=	0:	We	tried β =	0.5, 1, 2, 4;
2) When λ	=	0.5: We tried β	= 4, 8, 12, 16;
3) When λ	=	1:	We	tried β =	8, 12, 16, 20;
4) When λ	=	2:	We	tried β =	12, 16, 20, 24.
22