Under review as a conference paper at ICLR 2020
Learning Function-Specific Word
Representations
Anonymous authors
Paper under double-blind review
Ab stract
We present a neural framework for learning associations between interrelated
groups of words such as the ones found in Subject-Verb-Object (SVO) structures.
Our model induces a joint function-specific word vector space, where vectors of
e.g. plausible SVO compositions lie close together. The model retains information
about word group membership even in the joint space, and can thereby effectively
be applied to a number of tasks reasoning over the SVO structure. We show the
robustness and versatility of the proposed framework by reporting state-of-the-art
results on the tasks of estimating selectional preference (i.e., thematic fit) and event
similarity. The results indicate that the combinations of representations learned
with our task-independent model outperform task-specific architectures from prior
work, while reducing the number of parameters by up to 95%. The proposed
framework is versatile and holds promise to support learning function-specific
representations beyond the SVO structures.
1	Introduction
Word representations are in ubiquitous usage across all areas of natural language processing (NLP)
(Collobert et al., 2011; Chen & Manning, 2014; Melamud et al., 2016). Standard approaches rely
on the distributional hypothesis (Harris, 1954; Schutze, 1993) and learn a single word vector space
based on word co-occurrences in large text corpora (Mikolov et al., 2013b; Pennington et al., 2014;
Bojanowski et al., 2017). This purely context-based training produces general word representations
that capture the broad notion of semantic relatedness and conflate a variety of possible semantic
relations into a single space (Hill et al., 2015; Schwartz et al., 2015). However, this mono-faceted
view of meaning is a well-known deficiency in NLP applications (Faruqui, 2016; Mrksic et al., 2017)
as it fails to distinguish between fine-grained word associations.
In this work we present a novel approach to word representation learning that moves beyond the
restrictive single-space assumption. We propose to learn a joint function-specific word vector space
grouped by the different roles/functions a word can take in text. The space is trained specifically for
one structure (such as SVO),1 and the space topology is governed by the associations between the
groups. In other words, vectors for plausible combinations from two or more groups will lie close, as
illustrated by Figure 1. For example, the verb vector study will be close to plausible subject vectors
researcher or scientist and object vectors subject or art. For words that can occur as either subject or
object such as chicken, this effectively means we may obtain several vectors, one per group, e.g., one
for chicken as subject and another one for chicken as object.
We achieve this through a novel multidirectional neural representation learning approach, which takes
a list of N groups of words (G1, . . . , GN), factorises it into all possible “group-to-group” sub-models,
and trains them jointly with an objective similar to skip-gram negative sampling used in word2vec
(Mikolov et al., 2013a;b, §3). In other words, we learn the joint function-specific word vector space
by relying on sub-networks which consume one group Gi on the input side and predict words from a
second group Gj on the output side, i,j = 1, . . . , N; i 6= j. At the same time, all sub-network losses
are tied into a single joint loss, and all groups G1, . . . , Gn are shared between all sub-networks.2
1We choose the SVO structure as there is a number of well defined tasks reasoning over it. Future work could
look at different phenomena and how to combine vectors from several function-specific spaces.
2This can be seen as a form of multi-task learning on shared parameters (Ruder, 2017).
1
Under review as a conference paper at ICLR 2020
Word	Nearest Neighbours
Subject
memory	dream, feeling, shadow, sense, moment, consciousness
country state, nation, britain, china, uk, europe, government
student pupil, participant, learner, candidate, trainee, child
Verb
see	saw, view, expect, watch, notice, witness
eat	drink, consume, smoke, lick, swallow, cook, ingest
avoid	eliminate, minimise, anticipate, overcome, escape
Object
virus	bacteria, infection, disease, worm, mutation, antibody
beer	ale, drink, pint, coffee, tea, wine, soup, champagne
Joint SVO
study (V) researcher (S), scientist (S), subject (O), art (O)
eat (V) food (O), cat (S), dog (S)
need (V) help (O), implementation (S), support (O)
scientist
researcher 冷 science
study
“art
Su
”•chicken
eat food
ChiCken∙ . Y
cat
need
help . implementation
* * ,
assistance	SuPPOrt
Figure 1: Left: Nearest neighbours in a function-specific space trained for the SVO structure. In the
Joint SVO space (bottom) we show nearest neighbors for verbs (V) from the two other subspaces (O
and S). Right: Illustration of three neighbourhoods in a function-specific space trained for the SVO
structure. The space is structured by group (i.e. S, V, and O) and optimised such that vectors for
plausible SVO compositions will be close. Note that one word can have several vectors, for example
chicken can occur as subject or object (e.g., it can eat something or someone/something can eat it).
To validate the effectiveness of our multidirectional model in language applications, we focus on
modeling a prominent linguistic phenomenon: a general model of who does what to whom (Gell-Mann
& Ruhlen, 2011). In language, this event understanding information is typically uttered by the SVO
structures and, according to the cognitive science literature, is well aligned with how humans process
sentences (McRae et al., 1997; 1998; Grefenstette & Sadrzadeh, 2011a; Kartsaklis & Sadrzadeh,
2014); it reflects the likely distinct storage and processing of objects (typically nouns) and actions
(typically verbs) in the brain (Caramazza & Hillis, 1991; Damasio & Tranel, 1993). When focusing on
the SVO structures, the model will produce one joint space for the three groups (S, V and O) by tying
6 sub-networks (S→V ; V →S; S→O, . . .) with shared parameters and a joint loss (i.e. there are no
duplicate parameters, the model has one set of parameters for each group). The vectors from the
induced function-specific space can then be composed by standard composition functions (Milajevs
et al., 2014) to yield the so-called event representations (Weber et al., 2018), that is, representations
for the full SVO structure.
The quantitative results are reported on two established test sets for the compositional event similarity
task (Grefenstette & Sadrzadeh, 2011a; Kartsaklis & Sadrzadeh, 2014) which requires reasoning
over SVO structures: it quantifies the plausibility of the SVO combinations by scoring them against
human judgments. We report consistent gains over standard single vector spaces as well as over two
recent tensor-based architectures (Tilk et al., 2016; Weber et al., 2018) which were tailored to solve
the event similarity task in particular.
Furthermore, we show that our method is general and not tied to the 3-group condition. We conduct
additional experiments in a 4-group setting where indirect objects are also modeled, along with a
selectional preference3 evaluation of 2-group SV and VO relationships (Chambers & Jurafsky, 2010;
Van de Cruys, 2014), yielding the highest scores on several established benchmarks.
2	Background and Motivation
Representation Learning. Standard word representation models such as skip-gram negative sam-
pling (SGNS) (Mikolov et al., 2013b;a), Glove (Pennington et al., 2014), or FastText (Bojanowski
et al., 2017) induce a single word embedding space capturing broad semantic relatedness (Hill et al.,
2015). For instance, SGNS makes use of two vector spaces for this purpose, which are referred to as
Aw and Ac. SGNS has been shown to approximately correspond to factorising a matrix M = AwAcT ,
where elements in M represent the co-occurrence strengths between words and their context words
(Levy & Goldberg, 2014a). Both matrices represent the same vocabulary: therefore, only one of them
is needed in practice to represent each word. Typically only Aw is used while Ac is discarded, or the
two vector spaces are averaged to produce the final space.
3Also referred to as thematic-fit (Sayeed et al., 2016)
2
Under review as a conference paper at ICLR 2020
Levy & Goldberg (2014b) used depdendency-based contexts, resulting in two separate vector spaces;
however, the relation types were embedded into the vocabulary and the model was trained only in
one direction. Rei et al. (2018) described a related task-dependent neural network for mapping word
embeddings into relation-specific spaces for scoring lexical entailment. In this work, we propose a
task-independent approach and extend it to work with a variable number of relations.
Neuroscience. Theories from cognitive linguistics and neuroscience reveal that single-space represen-
tation models fail to adequately reflect the organisation of semantic concepts in the human brain (i.e.,
semantic memory): there seems to be no single semantic system indifferent to modalities or categories
in the brain (Riddoch et al., 1988). Recent fMRI studies strongly support this proposition and suggest
that semantic memory is in fact a widely distributed neural network (Davies et al., 2009; Huth et al.,
2012; Pascual et al., 2015; Rice et al., 2015; de Heer et al., 2017), where sub-networks might activate
selectively or more strongly for a particular function such as modality-specific or category-specific
semantics (such as objects/actions, abstract/concrete, animate/inanimate, animals, fruits/vegetables,
colours, body parts, countries, flowers, etc.) (Warrington, 1975; Warrington & McCarthy, 1987;
McCarthy & Warrington, 1988). This indicates a function-specific division of lower-level semantic
processing. Single-space distributional word models have been found to partially correlate to these
distributed brain activity patterns (Mitchell et al., 2008; Huth et al., 2012; 2016; Anderson et al.,
2017), but fail to explain the full spectrum of fine-grained word associations humans are able to make.
Our work has been partly inspired by this literature.
Compositional Distributional Semantics. Partially motivated by similar observations, prior work
frequently employs tensor-based methods for composing separate tensor spaces (Coecke et al.,
2010): there, syntactic categories are often represented by tensors of different orders based on
assumptions on their relations. One fundamental difference is made between atomic types (e.g.,
nouns) versus compositional types (e.g., verbs). Atomic types are seen as standalone: their meaning
is independent from other types. On the other hand, verbs are compositional as they rely on their
subjects and objects for their exact meaning.4 The goal is then to compose constituents into a semantic
representation which is independent of the underlying grammatical structure. Therefore, a large body
of prior work is concerned with finding appropriate composition functions (Grefenstette & Sadrzadeh,
2011a;b; Kartsaklis et al., 2012; Milajevs et al., 2014) to be applied on top of word representations.
Since this approach represents different syntactic structures with tensors of varying dimensions,
comparing syntactic constructs is not straightforward. This compositional approach thus struggles
with transferring the learned knowledge to downstream tasks. State-of-the-art compositional models
(Tilk et al., 2016; Weber et al., 2018) combine similar tensor-based approaches with neural training,
leading to task-specific compositional solutions. While effective for a task at hand, the resulting
model relies on a large number of parameters and is not robust: we observe deteriorated performance
on other related compositional tasks, as shown in §5.
Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compo-
sitional event similarity using all three variables, and thematic fit modeling based on SV and VO
associations separately. Traditional solutions are typically based on clustering of word co-occurrence
counts from a large corpus (Baroni & Lenci, 2010; Greenberg et al., 2015a;b; Sayeed et al., 2016;
Emerson & Copestake, 2016). More recent solutions combine neural networks with tensor-based
methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both
two and three groups with a max-margin loss. Grefenstette & Sadrzadeh (2011a;b); Kartsaklis &
Sadrzadeh (2014); Milajevs et al. (2014); Edelstein & Reichart (2016) employ tensor compositions
on standard single-space word vectors. Hashimoto & Tsuruoka (2016) discern compositional and
non-compositional phrase embeddings starting from HPSG-parsed data.
Objectives. We propose to induce function-specific vector spaces which enable a better model of
associations between concepts and consequently improved event representations by encoding the
relevant information directly into the parameters for each word during training. Word vectors offer
several advantages over tensors: a large reduction in parameters and fixed dimensionality across
concepts. This facilitates their reuse and transfer across different tasks. For this reason, we find
our multidirectional training to deliver good performance: the same function-specific vector space
achieves state-of-the-art scores across multiple related tasks, previously held by task-specific models.
4Due to this added complexity, the compositional types are represented with more parameters than the atomic
types, e.g., with a matrix instead of a vector.
3
Under review as a conference paper at ICLR 2020
(a) Predicting n → 1
(b) Predicting 1 → n
(c) Our multidirectional approach
Figure 2: The directionality of prediction in neural models is important. Representations can be of
varying quality depending on whether they are induced at the input or output side of the model. Our
multidirectional approach resolves this problem by training on shared representations in all directions.
3	Methodology
We require a flexible model that has a high capacity for learning associations between all groups
of words and their representations. Formally, our goal is to model the mutual associations (co-
occurrences) between N groups of words, where the vocabularies of each group can partially overlap.
We induce an embedding matrix RlVil×d for each group i = 1,...,N, where M corresponds to the
vocabulary size of the i-th group. For consistency, the vector dimensionality d is kept equal across all
variables.
Multiple Groups. Without loss of generality we present a model which creates a function-specific
vector space for N = 3 groups, referring to those groups as A, B, and C. Note that the model is not
limited to this setup, as we show later in §5. A, B and C might be interrelated phenomena, and we
aim for a model which can reliably score the plausibility of combining three vectors (A,B,C) taken
from this space.5 In addition to the full joint prediction, we aim for any two vector combinations
(AB , BC , CA) to have plausible scores of their own. Observing relations between words inside
single-group subspaces (A, B, or C) is another desirable feature.
Directionality. To design a solution with the necessary properties, we first need to consider the
influence of prediction directionality in representation learning. A representation model such as
SGNS (Mikolov et al., 2013a;b) learns two vectors for each word in one large vocabulary, one vector
on the input side (word vector), another on the output side (context vector).6 Here, we require several
distinct vocabularies (i.e., three, one each for group A, B, and C). Instead of context vectors, we train
the model to predict words from another group, hence directionality is an important consideration.
We find that prediction directionality has a strong impact on the quality of the induced representations,
and illustrate this effect on an example that is skewed extremely to one side: an n:1 assignment case.
Let us assume data of two groups, where each word of group A1 is assigned to exactly one of three
clusters in group B3 . We expect a function-specific word vector space customised for this purpose
to show three clearly separated clusters. Figure 2 visualises obtained representations.7 Figure 2a
plots the vector spaces when we use words on the input side of the model and predict the cluster:
A1 → B3; this can be seen as n:1 assignment. In the opposite direction (B3 → A1, 1:n assignment)
we do not observe the same trends (Figure 2b).
Representations for other and more complex phenomena suffer from the same issue. For example,
the verb eat can take many arguments corresponding to various food items such as pizza, beans,
or kimchi. A more specific verb such as embark might take only a few arguments such as journey,
whereas journey might be fairly general and can co-occur with many other verbs themselves. We thus
5As mentioned in the introduction, the three groups for which embedding spaces are induced can be Subject
nouns, Verbs, and Object nouns forming the SVO structure.
6Typically, only the input word vectors are used (Levy & Goldberg, 2014a, §2)
7We train on 10K randomly selected German nouns (A1) and their corresponding noun gender (B3) from a
German-English dictionary obtained from dict.cc, and train a 25-dim model for 24 epochs. Points in the
figures show 1K words which were randomly selected from the 10K training vocabulary. The embedding spaces
have been mapped to 2D with tSNE (van der Maaten & Hinton, 2012).
4
Under review as a conference paper at ICLR 2020
effectively deal with an n:m assignment case, which might be inclined towards 1:n or n:1 entirely
depending on the words in question. Therefore, it is unclear whether one should rather construct a
model predicting verb → object or object → verb.
3.1	Synchronous Joint Model
We resolve this fundamental design question by training representations in a multidirectional way with
a joint loss function. For all directions the model operates on the same shared parameters. As shown
in Figure 2c, we learn sensibly clustered representations without explicitly injecting directionality
assumptions. By relying on shared parameters the model also becomes more memory-efficient. We
now describe details of the architecture.
Sub-Network Architecture. We factorise groups into sub-networks, representing all possible di-
rections of prediction.8 Similar to Mikolov et al. (2013a;b), we calculate the dot-product between
two word vectors to quantify their association. For instance, the sub-network A → B computes its
prediction Pa→b = σ(~ ∙ Be + bab), where ~ is a word vector from the input group A, Be is the
word embedding matrix for the target group B, bab is a bias vector, and σ is the sigmoid function.
The loss of each sub-network is computed using cross-entropy between this prediction and the correct
labels La→b = cross .entropy (Pa→b ,La→b ), where La→b are one-hot vectors corresponding
to the correct predictions. We leave experiments with more sophisticated sub-network designs for
future work.
Synchronous Joint Training. We integrate all sub-networks into one joint model via two mech-
anisms: (1) Shared Parameters. The three embedding matrices referring to groups A, B and C
are shared across all sub-networks. That is, we train one matrix per group, regardless of whether
it is being employed at the input or the output side of any sub-network. This leads to a substantial
reduction in the model size.9 (2) Joint Loss. We also train all sub-networks with a single joint
loss and a single backward pass. We refer to this manner of joining the losses as synchronous: it
synchronises the backward pass of all sub-networks. It could also be seen as a form of multi-task
learning, where each sub-network optimises the shared parameters for a different task. In practice,
for the synchronous loss we do a forward pass in each direction separately, then join all sub-network
cross-entropy losses and backpropagate this joint loss through all sub-networks. We rely on addition
to compute the joint loss: L = P* Lμ, where μ represents one of the six sub-networks, Lμ is the
corresponding loss, and L the overall joint loss.
4	Evaluation Setup
Preliminary Task: Pseudo-Disambiguation. In the first evaluation, we adopt a standard pseudo-
disambiguation task from the selectional preference literature (Rooth et al., 1999; Bergsma et al.,
2008; Erk et al., 2010; Chambers & Jurafsky, 2010; Van de Cruys, 2014). For the three-group (S-V-O)
case, the task is to score a true triplet (i.e., the (S-V-O) structure attested in the corpus) above all
corrupted triplets (S-V’-O), (S’-V-O), (S-V-O’), where S’, V’ and O’ denote subjects and objects
randomly drawn from their respective vocabularies. Similarly, for the two-group setting, the task is to
express a higher preference towards the attested pairs (V-O) or (S-V) over corrupted pairs (V-O’) or
(S’-V). We report accuracy scores, i.e., we count all items where score(true) > score(corrupted).
This simple pseudo-disambiguation task serves as a preliminary sanity check: it can be easily applied
to a variety of training conditions with different variables. However, as pointed out by Chambers &
Jurafsky (2010), the performance on this task is strongly influenced by a number of factors such as
vocabulary size and the procedure for constructing corrupted examples. Therefore, we additionally
evaluate our models on a number of other established datasets (Sayeed et al., 2016).
Event Similarity (3 Variables: SVO). A standard task to measure the plausibility of SVO structures
(i.e., events) is event similarity (Grefenstette & Sadrzadeh, 2011a; Weber et al., 2018): the goal is
8Two groups will lead to two sub-networks A → B and B → A. Three groups lead to six sub-networks.
9For example, with a vocabulary of 50, 000 words and 25-dimensional vectors (our experimental setup, see
§4), we work only with 1.35M parameters. Comparable models for the same tasks are trained with much larger
sets of parameters: 26M or even up to 179M when not factorised (Tilk et al., 2016). Our modeling approach thus
can achieve a large reduction in the number of parameters, > 95%.
5
Under review as a conference paper at ICLR 2020
Data set	Train	Test	Model	Accuracy
SVO+iO	187K	15K	4 Variables SVO+iO	0.950
SVO	22M	214K	3 Variables: SVO	
	Vocab size	Freq.	Van de Cruys (2009)	0.874
S	22K	people,one,company,student	Van de Cruys (2014)	0.889
V	5K	have,take,include,provide	Tilk et al. (2016) (our reimplementation)	0.937
O	15K	place,information,way,number	Ours	0.943
SV	69M	232K	2 Variables Rooth et al. (1999)	0.720
				
	Vocab size	Freq.	Erk et al. (2010)	0.887
S V	45K 19K	people,what,one,these be,have,say,take,go	Van de Cruys (2014) Ours: SV	0.880 0.960
VO	84M	240K	Ours: VO	0.972
	Vocab size	Freq.		
V	9K	have,take,use,make,provide		
O	32K	information,time,service		
Table 1: Training data statistics.			Table 2: Pseudo-disambiguation: accuracy scores.	
to score similarity between SVO triplet pairs and correlate the similarity scores to human-elicited
similarity judgements. Robust and flexible event representations are important to many core areas in
language understanding such as script learning, narrative generation, and discourse understanding
(Chambers & Jurafsky, 2009; Pichotta & Mooney, 2016; Modi, 2016; Weber et al., 2018). We
evaluate event similarity on two benchmarking data sets: GS199 (Grefenstette & Sadrzadeh, 2011a)
and KS108 (Kartsaklis & Sadrzadeh, 2014). GS199 contains 199 pairs of SV O triplets/events. In
the GS199 data set only the V is varied, while S and O are fixed in the pair: this evaluation prevents
the model from relying only on simple lexical overlap for similarity computation.10 KS108 contains
108 event pairs for the same task, but is specifically constructed without any lexical overlap between
the events in each pair.
For this task function-specific representations are composed into a single event representation/vector.
Following prior work, we compare cosine similarity of event vectors to averaged human scores and
report Spearman’s ρ correlation with human scores. We compose the function-specific vectors into
event vectors using simple addition and multiplication, as well as more sophisticated compositions
from prior work (Milajevs et al., 2014, inter alia). The summary is provided in Table 3.
Thematic-Fit Evaluation (2 Variables: SV and VO). Similarly to the 3-group setup, we also
evaluate the plausibility of SV and V O pairs separately in the 2-group setup. The thematic-fit
evaluation (Sayeed et al., 2016) quantifies the extent to which a noun fulfils the selectional preference
of a verb given a role (i.e., agent:S, or patient:O) (McRae et al., 1997). We evaluate our 2-group
function-specific spaces on two standard benchmarks: 1) MST1444 (McRae et al., 1998) contains
1,444 word pairs where humans provided thematic fit ratings on a scale from 1 to 7 for each noun
to score the plausibility of the noun taking the agent role, and also taking the patient role.11 2)
PADO414 (PadO, 2007)is similar to MSTl444, containing 414 pairs with human thematic fit ratings,
where role-filling nouns were selected to reflect a wide distribution of scores for each verb. We
compute plausibility by simply taking the cosine similarity between the verb vector (from the V
space) and the noun vector from the appropriate function-specific space (S space for agents; O space
for patients). We again report Spearman’s ρ correlation scores.
Training Data. We parse the ukWaC corpus (Baroni et al., 2009) and the British National Corpus
(BNC) (Leech, 1992) using the Stanford Parser with Universal Dependencies v1.4 (Chen & Manning,
2014; Nivre et al., 2016) and extract co-occurring subjects, verbs and objects. All words are
lowercased and lemmatised, and tuples containing non-alphanumeric characters are excluded. We
also remove tuples with (highly frequent) pronouns as subjects, and filter out training examples
containing words with frequency lower than 50. After preprocessing, the final training corpus
comprises 22M SVO triplets in total. Table 1 additionally shows training data statistics when training
in the 2-group setup (SV and VO) and in the 4-group setup (when adding indirect objects: SVO+iO).
10For instance, the phrases ’people run company’ and ’people operate company’ have a high similarity score
of 6.53, whereas ’river meet sea’ and ’river satisfy sea’ have been given a low score of 1.84.
11Using an example from Sayeed et al. (2016), the human participants were asked “how common is it for a
{snake, monster, baby, cat} to frighten someone/something” (agent role) as opposed to “how common is it for a
{snake, monster, baby, cat} to be frightened by someone/something” (patient role).
6
Under review as a conference paper at ICLR 2020
Composition	Reference	Formula
Verb only	Milajevs et al. (2014)	V~
Addition	Mitchell & Lapata (2008)	S~ + V~ + O~
Copy Object	Kartsaklis et al. (2012)	S~	(V~ × O~)
Concat	Edelstein & Reichart (2016)	[S~,V~,O~]
Concat Add	Edelstein & Reichart (2016)	[S~,V~]+[V~,O~]
Network	Ours	S~V~ T + V~ O~ T + S~O~ T
Table 3: Left: Composition functions used to obtain event vectors from function-specific vector
spaces. +: addition, Θ: element-wise multiplication, ×: dot product. [∙, ∙]: concatenation. Right:
Results on the event similarity task. Best baseline score is underlined, and the best overall result is
provided in bold.
Model	Reference	Spearman’s ρ	
		GS199	KS108
Copy Object W2V	Milajevs et al. (2014)	0.46	0.66
Add KS14	Milajevs et al. (2014)	028	0.73
	Tilk et al. (2016)	0.34	-
	Weber et al. (2018)	-	0.71
Ours: SVO d100			
Verb only	Ours	0.34	0.63
Add	Ours	0.27	0.76
Concat	Ours	0.26	0.75
Concat Add	Ours	0.32	0.77
Copy Object	Ours	0.40	0.52
Network	Ours	0.53	-
We report the number of examples in training and test sets, as well as vocabulary sizes and most
frequent words across different categories.
Hyperparameters. We train with batch size 128, and use Adam for optimisation (Kingma & Ba,
2015) with a learning rate 0.001. All gradients are clipped to a maximum norm of 5.0. All models
were trained with the same fixed random seed. We train 25-dimensional vectors for all setups (2/3/4
groups), and we additionally train 100-dimensional vectors for the 3-group (SVO) setup.
5	Results and Analysis
Pseudo-Disambiguation. Accuracy scores on the pseudo-disambiguation task in the 2/3/4-group
setups are summarised in Table 2.12 We find consistently high pseudo-disambiguation scores (>0.94)
across all setups. As mentioned in §4, this initial evaluation already suggests that our model is able to
capture associations between interrelated groups which are instrumental to modeling SVO structures
and composing event representations.
Event Similarity. We now test correlations of SVO-based event representations composed from a
function-specific vector space (see Table 3) to human scores in the event similarity task. A summary
of the main results is provided in Table 3. We also report best baseline scores from prior work.
The main finding is that our model based on function-specific word vectors outperforms previous
state-of-the-art scores on both datasets. It is crucial to note that different modeling approaches and
configurations from prior work held previous peak scores on the two evaluation sets.13 Interestingly,
by relying only on the representations from the V subspace (i.e., by completely discarding the
knowledge stored in S and O vectors), we can already obtain reasonable correlation scores. This is
an indicator that the verb vectors indeed stores some selectional preference information as designed,
i.e., the information is successfully encoded into the verb vectors themselves.
Thematic-Fit Evaluation. Correlation scores on two thematic-fit evaluation data sets are summarised
in Table 4. We also report results with representative baseline models for the task: 1) a TypeDM-based
model (Baroni & Lenci, 2010), further improved by Greenberg et al. (2015a;b) (G15), and 2) current
state-of-the-art tensor-based neural model by Tilk et al. (2016) (TK16). We find that vectors taken
from the model trained in the joint 3-group SVO setup perform on a par with state-of-the-art models
also in the 2-group evaluation on SV and VO subsets. Vectors trained explicitly in the 2-group setup
using three times more data lead to substantial improvements on PADO414. As a general finding, our
function-specific approach leads to peak performance on both data sets. The results are similar with
25-dim SVO vectors.
12We also provide baseline scores taken from prior work, but the reader should be aware that the scores may
not be directly comparable due to the dependence of this evaluation on factors such as vocabulary size and
sampling of corrupted examples (Chambers & Jurafsky, 2010; Sayeed et al., 2016).
13Note the two tasks are inherently different. KS108 requires similarity between plausible triplets. Using
the network score directly (which is a scalar, see Table 3) is not suitable for KS108 as all KS108 triplets are
plausible and scored highly. This is reflected in the results in Table 3.
7
Under review as a conference paper at ICLR 2020
Setup		Baselines		Ours		async			sync	
				SVO	SV-VO		sep	shared	sep	shared
						3 Variables				
Dataset	Eval	G15	TK16	(d=100)	(d=25)		0.56	0.48	0.58	0.60
						KS108 Verb only				
	SV	0.36	-	0.37	0.31	KS108 Addition	0.51	0.66	0.73	0.78
MST1444	VO	0.34	-	0.35	0.35	GS199 Verb only	0.24	0.26	0.26	0.34
	full	0.33	0.38	0.36	0.34	GS199 Network	0.10	0.40	0.28	0.52
	SV	0.54	-	0.38	0.55	2 Variables				
PADO414	VO	0.53	-	0.54	0.61	MST1444	0.17	0.10	0.30	0.39
	full	0.53	0.52	0.45	0.58	PADO414	0.41	0.21	0.44	0.44
Table 4: Results on the 2-variable thematic-fit eval-Table 5: Evaluation of different model variants, by
uation. Spearman’s ρ correlation.	training regime and parameter sharing.
Our model is also more light-weight than the baselines: we do not require a full (tensor-based) neural
model, but simply function-specific word vectors to reason over thematic fit. To further verify the
importance of joint multidirectional training, we have also compared our function-specific vectors
against standard single-space word vectors (Mikolov et al., 2013b). The results indicate the superiority
of function-specific spaces: respective correlation scores on MST1444 and PADO414 are 0.28 and
0.41 (vs 0.34 and 0.58 with our model). It is interesting to note that we obtain state-of-the-art scores
calculating cosine similarity of vectors taken from two groups found in the joint space. This finding
verifies that the model does indeed learn a joint space where co-occurring words from different groups
lie close to each other.
Qualitative Analysis. We retrieve nearest neighbours from the function-specific (S, V , O) space,
shown in Figure 1. We find that the nearest neighbours indeed reflect the relations required to
model the SVO structure. For instance, the closest subjects/agents to the verb eat are cat and dog.
The closest objects to need are three plausible nouns: help, support, and assistance. As the model
has information about group membership, we can also filter and compare nearest neighbours in
single-group subspaces. For example, we find subjects similar to the subject memory are dream and
feeling, and objects similar to beer are ale and pint.
Model Variants. We also conduct an ablation study that compares different model variants. The
variants are constructed by varying 1) the training regime: asynchronous (async) vs synchronous
(sync)14 and 2) the type of parameter sharing: training on separate parameters for each sub-network
(sep)15 or training on shared variables (shared). Table 5 shows the results with the model variants,
demonstrating that both aspects (i.e., shared parameters and synchronous training) are important
to reach improved overall performance. We reach the peak scores on all evaluation sets using the
sync+shared variant. We suspect that asynchronous training deteriorates performance because each
sub-network overwrites the updates of other sub-networks as their training is not tied through a joint
loss function. On the other hand, the synchronous training regime guides the model towards making
updates that can benefit all sub-networks.
6	Conclusion and Future Work
We presented a novel multidirectional neural framework for learning function-specific word represen-
tations, which can be easily composed into multi-word representations to reason over event similarity
and thematic fit. We induce a joint vector space in which several groups of words (e.g., S, V, and O
words forming the SVO structures) are represented while taking into account the mutual associations
between the groups. We found that resulting function-specific vectors yield state-of-the-art results
on established benchmarks for the tasks of estimating event similarity and evaluating thematic fit,
previously held by task-specific methods.
In future work we will investigate more sophisticated neural (sub-)networks within the proposed
framework. We will also apply the idea of function-specific training to other interrelated linguistic
phenomena and other languages, probe the usefulness of function-specific vectors in other language
tasks, and explore how to integrate the methodology with sequential models. The pre-trained word
vectors used in this work are available online at: [URL].
14In the asynchronous setup we update the shared parameters per sub-network directly based on their own
loss, instead of relying on the joint synchronous loss as in §3.
15With separate parameters we merge vectors from “duplicate” vector spaces by non-weighted averaging.
8
Under review as a conference paper at ICLR 2020
References
Andrew Anderson, Douwe Kiela, Stephen Clark, and Massimo Poesio. Visually grounded and
textual semantic models differentially decode brain activity associated with concrete and abstract
nouns. Transactions of the ACL, 5:17-30, 2017. URL https://transacl.org/ojs/
index.php/tacl/article/view/879.
Marco Baroni and Alessandro Lenci. Distributional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673-721, December 2010. ISSN 0891-2017. doi:
10.1162/coli_a_00016. URL http://dx.doi.org/10.1162/coli_a_0 0 016.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. The WaCky wide web:
a collection of very large linguistically processed web-crawled corpora. Language Resources
and Evaluation, 43(3):209-226, 2009. URL https://link.springer.com/article/
10.1007/s10579-009-9081-4.
Shane Bergsma, Dekang Lin, and Randy Goebel. Discriminative learning of selectional preference
from unlabeled text. In Proceedings of EMNLP, pp. 59-68, 2008. URL http://www.aclweb.
org/anthology/D08-1007.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with
subword information. Transactions of the ACL, 5:135-146, 2017. URL http://arxiv.org/
abs/1607.04606.
Alfonso Caramazza and Argye E. Hillis. Lexical organization of nouns and verbs in the brain.
Nature, 349(6312):788-790, feb 1991. ISSN 0028-0836. doi: 10.1038/349788a0. URL http:
//www.nature.com/doifinder/10.1038/349788a0.
Nathanael Chambers and Dan Jurafsky. Unsupervised learning of narrative schemas and their
participants. In Proceedings of ACL, pp. 602-610, 2009. URL http://www.aclweb.org/
anthology/P09-1068.
Nathanael Chambers and Dan Jurafsky. Improving the use of pseudo-words for evaluating selectional
preferences. In Proceedings of ACL, pp. 445-453, 2010. URL http://www.aclweb.org/
anthology/P10-1046.
Danqi Chen and Christopher D. Manning. A fast and accurate dependency parser using neural
networks. In Proceedings of EMNLP, pp. 740-750, 2014. URL http://www.aclweb.org/
anthology/D14-1082.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. Mathematical foundations for a compo-
sitional distributional model of meaning. Linguistic Analysis, 36(1-4):345-384, 2010. URL
https://arxiv.org/abs/1003.4394.
Ronan Collobert, Jason Weston, Lon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:
2493-2537, 2011. URL http://www.jmlr.org/papers/volume12/collobert11a/
collobert11a.pdf.
Antonio R. Damasio and Daniel Tranel. Nouns and verbs are retrieved with differently dis-
tributed neural systems. Proceedings of the National Academy of Sciences of the United
States of America, 90(11):4957-60, jun 1993. ISSN 0027-8424. doi: 10.1073/PNAS.
90.11.4957. URL http://www.ncbi.nlm.nih.gov/pubmed/8506341http://www.
pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC46632.
R. Rhys Davies, Glenda M. Halliday, John H. Xuereb, Jillian J. Kril, and John R. Hodges. The
neural basis of semantic memory: Evidence from semantic dementia. Neurobiology of Ag-
ing, 30(12):2043-2052, dec 2009. ISSN 0197-4580. doi: 10.1016/J.NEUROBIOLAGING.
2008.02.005. URL https://www.sciencedirect.com/science/article/pii/
S0197458008000547.
9
Under review as a conference paper at ICLR 2020
Wendy A. de Heer, Alexander G. Huth, Thomas L. Griffiths, Jack L. Gallant, and Frederic E.
Theunissen. The hierarchical cortical organization of human speech processing. Journal of
Neuroscience, 37(27):6539-6557,2017. ISSN0270-6474. doi:10.1523/JNEUROSCI.3267-16.
2017. URL http://www.jneurosci.org/content/37/27/6539.
Lilach Edelstein and Roi Reichart. A factorized model for transitive verbs in compositional distri-
butional semantics. CoRR, abs/1609.07756, 2016. URL http://arxiv.org/abs/1609.
07756.
Guy Emerson and Ann A. Copestake. Functional distributional semantics. In Proceedings of the 1st
Workshop on Representation Learning for NLP, pp. 40-52, 2016. URL https://doi.org/
10.18653/v1/W16-1605.
Katrin Erk, Sebastian Pado, and Ulrike Pado. A flexible, corpus-driven model of regular and
inverse selectional preferences. Computational Linguistics, 36(4):723-763, 2010. URL https:
//doi.org/10.1162/coli_a_00017.
Manaal Faruqui. Diverse Context for Learning Word Representations. PhD thesis, Carnegie Mellon
University, 2016. URL http://www.manaalfaruqui.com/papers/thesis.pdf.
Murray Gell-Mann and Merritt Ruhlen. The origin and evolution of word order. Proceedings of
the National Academy of Sciences, 108(42):17290-17295, 2011. URL https://www.pnas.
org/content/108/42/17290.
Clayton Greenberg, Vera Demberg, and Asad Sayeed. Verb polysemy and frequency effects in the-
matic fit modeling. In Proceedings of the 6th Workshop on Cognitive Modeling and Computational
Linguistics, pp. 48-57, 2015a. URL http://www.aclweb.org/anthology/W15-1106.
Clayton Greenberg, Asad Sayeed, and Vera Demberg. Improving unsupervised vector-space thematic
fit evaluation via role-filler prototype clustering. In Proceedings of NAACL-HLT, pp. 21-31, 2015b.
URL http://www.aclweb.org/anthology/N15-1003.
Edward Grefenstette and Mehrnoosh Sadrzadeh. Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of EMNLP, pp. 1394-1404, 2011a. URL
http://www.aclweb.org/anthology/D11-1129.
Edward Grefenstette and Mehrnoosh Sadrzadeh. Experimenting with transitive verbs in a DisCoCat.
In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language
Semantics, pp. 62-66, 2011b. URL http://www.aclweb.org/anthology/W11- 2507.
Zellig S. Harris. Distributional Structure. Word, 10(2-3):146-162, 1954. ISSN 0043-7956.
doi: 10.1007/978-94-009-8467-7{\_}1. URL http://psycnet.apa.org/psycinfo/
1956-02807-001.
Kazuma Hashimoto and Yoshimasa Tsuruoka. Adaptive joint learning of compositional and non-
compositional phrase embeddings. In Proceedings of ACL, pp. 205-215, 2016. URL https:
//www.aclweb.org/anthology/P/P16/P16-1020.pdf.
Felix Hill, Roi Reichart, and Anna Korhonen. SimLex-999: Evaluating semantic models with
(genuine) similarity estimation. Computational Linguistics, 41(4):665-695, 2015. URL https:
//arxiv.org/abs/1408.3456.
Alexander G. Huth, Shinji Nishimoto, An T. Vu, and Jack L. Gallant. A continuous semantic
space describes the representation of thousands of object and action categories across the hu-
man brain. Neuron, 76(6):1210-1224, dec 2012. ISSN 0896-6273. doi: 10.1016/J.NEURON.
2012.10.014. URL https://www.sciencedirect.com/science/article/pii/
S0896627312009348.
Alexander G. Huth, Wendy A. de Heer, Thomas L. Griffiths, Frederic E. Theunissen, and Jack L.
Gallant. Natural speech reveals the semantic maps that tile human cerebral cortex. Nature, 532
(7600):453-458, 2016. URL https://www.nature.com/articles/nature17637.
10
Under review as a conference paper at ICLR 2020
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. A study of entanglement in a categorical framework
of natural language. In Proceedings ofQPL, pp. 249-261, 2014. URL https://arxiv.org/
abs/1405.2874.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen Pulman. A unified sentence space for
categorical distributional-compositional semantics: Theory and experiments. In Proceedings of
COLING, pp. 549-558, 2012. URL http://www.aclweb.org/anthology/C12-2054.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of
ICLR (Conference Track), 2015. URL http://arxiv.org/abs/1412.6980.
Geoffrey Neil Leech. 100 million words of English: The British National Corpus (BNC). 1992. doi:
https://doi.org/10.1017/S0266078400006854.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In
Proceedings of NIPS, pp. 2177-2185, 2014a. URL https://papers.nips.cc/paper/
5477- neural- word- embedding- as- implicit- matrix- factorization.
Omer Levy and Yoav Goldberg. Dependency-based word embeddings. In Proceedings of ACL, pp.
302-308, 2014b. URL http://aclweb.org/anthology/P14-2050.
Rosaleen A. McCarthy and E. K. Warrington. Evidence for modality-specific meaning systems in the
brain. Nature, 334(6181):428-430, aug 1988. ISSN 0028-0836. doi: 10.1038/334428a0. URL
http://www.nature.com/doifinder/10.1038/334428a0.
Ken McRae, Todd Ferretti, and Liane Amyote. Thematic roles as verb-specific concepts. Lan-
guage and Cognitive Processes, 12(2):137-176, mar 1997. ISSN 0169-0965. doi: 10.
1080/016909697386835. URL http://www.tandfonline.com/doi/abs/10.1080/
016909697386835.
Ken McRae, Michael J. Spivey-Knowlton, and Michael K. Tanenhaus. Modeling the influence of
thematic fit (and other constraints) in on-line sentence comprehension. Journal of Memory and
Language, 38(3):283-312, 1998. URL https://www.sciencedirect.com/science/
article/pii/S0749596X97925432.
Oren Melamud, David McClosky, Siddharth Patwardhan, and Mohit Bansal. The role of context types
and dimensionality in learning word embeddings. In Proceedings of NAACL-HLT, pp. 1030-1040,
2016. URL http://www.aclweb.org/anthology/N16-1118.
Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Efficient estimation of word
representations in vector space. In Proceedings of ICLR (Workshop Papers), 2013a. URL
https://arxiv.org/abs/1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S Corrado, and Jeffrey Dean. Distributed
representations of words and phrases and their compositionality. In Proceedings of NIPS, pp.
3111-3119, 2013b.
Dmitrijs Milajevs, Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Matthew Purver. Evaluating
neural word representations in tensor-based compositional settings. In Proceedings of EMNLP, pp.
708-719, 2014. URL http://www.aclweb.org/anthology/D14-1079.
Jeff Mitchell and Mirella Lapata. Vector-based models of semantic composition. Proceedings of ACL,
pp. 236-244, 2008. URL http://www.aclweb.org/anthology/P/P08/P08-1028.
Tom M. Mitchell, Svetlana V. Shinkareva, Andrew Carlson, Kai-Min Chang, Vicente L. Malave,
Robert A. Mason, and Marcel Adam Just. Predicting human brain activity associated with the
meanings of nouns. Science, 320(5880):1191-1195, 2008. ISSN 0036-8075. doi: 10.1126/science.
1152876. URL http://science.sciencemag.org/content/320/5880/1191.
Ashutosh Modi. Event embeddings for semantic script modeling. In Proceedings of CoNLL, pp.
75-83, 2016. URL http://aclweb.org/anthology/K/K16/K16-1008.pdf.
11
Under review as a conference paper at ICLR 2020
Nikola Mrksic, Ivan VUliC, Diarmuid O Seaghdha, Ira Leviant, Roi Reichart, Milica Gasic, Anna
Korhonen, and Steve Young. Semantic specialisation of distributional word vector spaces using
monolingual and cross-lingual constraints. Transactions of the ACL, 5:309-324, 2θ17. URL
https://transacl.org/ojs/index.php/tacl/article/view/1171.
Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajic, Christo-
pher D Manning, Ryan T McDonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, et al.
Universal Dependencies v1: A multilingual treebank collection. In Proceedings of LREC,
pp. 1659-1666, 2016. URL http://www.lrec-conf.org/proceedings/lrec2016/
pdf/348_Paper.pdf.
Ulrike Pado. The integration of syntax and semantic plausibility in a Wide-Coverage model
of human sentence processing. 2007. URL https://nlpado.de/~ulrike/papers/
CunyPosterUP.pdf.
Belen Pascual, Joseph C. Masdeu, Mark Hollenbeck, Nikos Makris, Ricardo Insausti, Song-Lin
Ding, and Bradford C. Dickerson. Large-scale brain netWorks of the human left temporal
pole: A functional connectivity MRI study. Cerebral Cortex, 25(3):680-702, mar 2015. ISSN
1460-2199. doi: 10.1093/cercor/bht260. URL https://academic.oup.com/cercor/
article-lookup/doi/10.1093/cercor/bht260.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for Word
representation. In Procedings of EMNLP, pp. 1532-1543, 2014. URL http://www.aclweb.
org/anthology/D14-1162.
Karl Pichotta and Raymond J. Mooney. Learning statistical scripts With LSTM recurrent neural
netWorks. In Proceedings of AAAI, pp. 2800-2806, 2016. URL http://www.aaai.org/
ocs/index.php/AAAI/AAAI16/paper/view/12157.
Marek Rei, Daniela Gerz, and Ivan Vulic. Scoring lexical entailment with a supervised directional
similarity netWork. In Proceedings of ACL, pp. 638-643, 2018. URL http://aclweb.org/
anthology/P18-2101.
Grace E. Rice, Paul Hoffman, and Matthew A. Lambon Ralph. Graded specialization within and
between the anterior temporal lobes. Annals of the New York Academy of Sciences, 1359(1):84-97,
nov 2015. ISSN 00778923. doi: 10.1111/nyas.12951. URL http://doi.wiley.com/10.
1111/nyas.12951.
M. Jane Riddoch, Glyn W. Humphreys, Max Coltheart, and Elaine Funnell. Semantic sys-
tems or system? Neuropsychological evidence re-examined. Cognitive Neuropsychol-
ogy, 5(1):3-25, 1988. URL https://www.tandfonline.com/doi/abs/10.1080/
02643298808252925.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Carroll, and Franz Beil. Inducing a semantically
annotated lexicon via EM-based clustering. In Proceedings of ACL, pp. 104-111, 1999. URL
http://www.aclweb.org/anthology/P99-1014.
Sebastian Ruder. An overview of multi-task learning in deep neural networks. CoRR, abs/1706.05098,
2017. URL http://arxiv.org/abs/1706.05098.
Asad Sayeed, Clayton Greenberg, and Vera Demberg. Thematic fit evaluation: An aspect of selectional
preferences. In Proceedings of the 1st Workshop on Evaluating Vector Space Representations
for NLP, pp. 99-105, 2016. URL http://aclweb.org/anthology/W/W16/W16-2518.
pdf.
Hinrich Schutze. Word space. In Proceedings ofNIPS,pp. 895-902, 1993. URL https://pdfs.
semanticscholar.org/5d24/afe3a62331ebfad400c3fec77c836d2b99db.
pdf.
Roy Schwartz, Roi Reichart, and Ari Rappoport. Symmetric pattern based word embeddings for
improved word similarity prediction. In Proceedings of CoNLL, pp. 258-267, 2015. URL
http://www.aclweb.org/anthology/K15-1026.
12
Under review as a conference paper at ICLR 2020
Ottokar Tilk, Vera Demberg, Asad Sayeed, Dietrich Klakow, and Stefan Thater. Event participant
modelling with neural networks. In Proceedings ofEMNLP, pp. 171-182, 2016. URL https：
//aclweb.org/anthology/D16- 1017.
Tim Van de Cruys. A non-negative tensor factorization model for selectional preference induction. In
Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pp. 83-90,
2009. URL https://dl.acm.org/citation.cfm?id=1705426.
Tim Van de Cruys. A neural network approach to selectional preference acquisition. In Proceedings
of EMNLP, pp. 26-35, 2014. URL http://www.aclweb.org/anthology/D14-1004.
Laurens van der Maaten and Geoffrey E. Hinton. Visualizing non-metric similarities in multi-
ple maps. Machine Learning, 87(1):33-55, 2012. URL https://doi.org/10.1007/
s10994-011-5273-4.
Elizabeth K. Warrington. The Selective Impairment of Semantic Memory. Quarterly Jour-
nal of Experimental Psychology, 27(4):635-657, nov 1975. ISSN 0033-555X. doi: 10.
1080/14640747508400525. URL http://journals.sagepub.com/doi/10.1080/
14640747508400525.
Elizabeth K. Warrington and Rosaleen A. McCarthy. Categories of knowledge. Brain, 110(5):1273-
1296, oct 1987. ISSN 0006-8950. doi: 10.1093/brain/110.5.1273. URL https://academic.
oup.com/brain/article- lookup/doi/10.1093/brain/110.5.1273.
Noah Weber, Niranjan Balasubramanian, and Nathanael Chambers. Event representations with
tensor-based compositions. In Proceedings of AAAI, pp. 4946-4953, 2018. URL https://www.
aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17126/16027.
13