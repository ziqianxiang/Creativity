Under review as a conference paper at ICLR 2020
Generalized Zero-shot ICD Coding
Anonymous authors
Paper under double-blind review
Ab stract
The International Classification of Diseases (ICD) is a list of classification codes
for the diagnoses. Automatic ICD coding is a multi-label text classification task
with noisy clinical document inputs and extremely long-tailed label distribution,
making it difficult to perform fine-grained classification on both frequent and zero-
shot codes at the same time. In this paper, we propose a latent feature generation
framework for generalized zero-shot ICD coding, where we aim to improve the
prediction on codes that have no labeled data without compromising the perfor-
mance on seen codes. Our framework generates semantically meaningful features
for zero-shot codes by exploiting ICD code hierarchical structure and a novel cycle
architecture that reconstructs the relevant keywords. To the best of our knowledge,
this is the first adversarial generative model for the generalized zero-shot learning
on multi-label text classification. Extensive experiments demonstrate the effec-
tiveness of our approach. On the public MIMIC-III dataset, our methods improve
the F1 score from nearly 0 to 20.91% for the zero-shot codes, and increase the
AUC score by 3% (absolute improvement) from previous state of the art.
1	Introduction
In healthcare facilities, clinical records are classified into a set of International Classification of
Diseases (ICD) codes that categorize diagnoses. ICD codes are used for a wide range of purposes
including billing, reimbursement, and retrieving of diagnostic information. Automatic ICD cod-
ing (Stanfill et al., 2010) is in great demand as manual coding can be labor-intensive and error-prone.
ICD coding is a multi-label text classification task with a long-tailed class label distribution. Ma-
jority of ICD codes only have a few or no labeled data due to the rareness of the disease. In the
medical dataset MIMIC III (Johnson et al., 2016), among 17,000 unique ICD-9 codes, more than
50% of them never occur in the training data. It is extremely challenging to perform fine-grained
multi-label classification on both codes with labeled data (seen codes) and zero-shot (unseen) codes
at the same time. Automatic ICD coding for both seen and unseen codes fits into the generalized
zero-shot learning (GZSL) paradigm (Chao et al., 2016), where test examples are from both seen
and unseen classes and we classify them into the joint labeling space of both types of classes. Nev-
ertheless, current GZSL works focus on visual tasks (Xian et al., 2017; Felix et al., 2018; Liu et al.,
2019). The study of GZSL for multi-label text classification is largely under-explored.
Modern automatic ICD coding models (Mullenbach et al., 2018; Rios & Kavuluru, 2018) can accu-
rately assign frequent ICD codes while perform poorly on zero-shot codes. To resolve this discrep-
ancy, we aim to improve the predictive power of existing models on zero-shot codes by finetuning
the models with synthetic latent features. To generate semantically meaningful features, we exploit
the domain knowledge about ICD codes. The official ICD guidelines provide each code a short text
description and a hierarchical tree structure on all the ICD codes (ICD-9 Guidelines). We propose
AGMC-HTS, an Adversarial Generative Model Conditioned on code descriptions with Hierarchical
Tree Structure to generate synthetic feature. As illustrated in Figure 1, AGMC-HTS consists of
a generator to synthesize code-specific latent features based on the ICD code descriptions, and a
discriminator to decide how realistic the generated features are. To guarantee the semantic consis-
tency between the generated and real features, AGMC-HTS reconstructs the keywords in the input
documents that are relevant to the conditioned codes. To further facilitate the feature synthesis of
zero-shot codes, we take advantage of the hierarchical structure of the ICD codes and encourage the
zero-shot codes to generate similar features with their nearest sibling code. The ICD coding models
are finetuned on the generated features to achieve more accurate prediction for zero-shot codes.
1
Under review as a conference paper at ICLR 2020
ICDTree
Sibling
code lsιb
A χ labeled with I or lsιb
ICD code I
Label
Encoder
Feature
Extractor
Generator
G
right I	I
hepatic	—
artery∣	I
branch
Real / Fake
t
Discriminator
Keywords hepatic, artery,
Reconstruction -
Figure 1: Overview of AGMC-HTS. The generator synthesizes features for an ICD code and the discriminator
decides how realistic the input feature is. For a zero-shot ICD code, the discriminator distinguishes between the
generated features and the real features from the data of its nearest sibling in the ICD hierarchy. The generated
features are further used to reconstruct the keywords in the input documents to preserve semantics.
The contributions of this paper are summarized as follows: 1) To the best of our knowledge, we
propose the first adversarial generative model for the generalized zero-shot learning on multi-label
text classification. AGMC-HTS generates latent features conditioned on the code descriptions and
finetunes the zero-shot ICD code assignment classifiers. 2) AGMC-HTS exploits the hierarchical
structure of ICD codes to generate semantically meaningful features for zero-shot codes without
any labeled data. 3) AGMC-HTS has a novel pseudo cycle generation architecture to guarantee the
semantic consistency between the synthetic and real features by reconstructing the relevant keywords
in input documents. 4) Extensive experiments demonstrate the effectiveness of our approach. On
MIMIC-III dataset, our methods improve the F1 score from nearly 0 to 20.91% for the zero-shot
codes and AUC score by 3% (absolute improvement) from previous state of the art. We also show
that AGMC-HTS improves the performance on few-shot codes with a handful of labeled data.
2	Related Work
Automated ICD coding. Several approaches have explored automatic assigning ICD codes on clin-
ical text data (Stanfill et al., 2010). Mullenbach et al. (2018) proposed to extract per-code textual
features with attention mechanism for ICD code assignments. Shi et al. (2017) explored charac-
ter based short-term memory (LSTM) with attention and Xie & Xing (2018) applied tree LSTM
with ICD hierarchy information for ICD coding. Most existing work either focused on predicting
the most common ICD code or did not utilize the ICD hierarchy structure for prediction. Rios &
Kavuluru (2018) proposed a neural network models incorporating ICD hierarchy information that
improved the performance on the rare and zero-shot codes. The performance is evaluated in terms
of the relative ranks to other infrequent codes. The model hardly ever assign rare codes in its final
prediction as we show in Section 4.2, making it impractical to deploy in real applications.
Feature generation for GZSL. The idea of using generative models for GZSL is to generate latent
features for unseen classes and train a classifier on the generated features and real features for both
seen and unseen classes. Xian et al. (2018) proposed using conditional GANs to generate visual
features given the semantic feature for zero-shot classes. Felix et al. (2018) added a cycle-consistent
loss on generator to ensure the generated features captures the class semantics by using linear re-
gression to map visual features back to class semantic features. Ni et al. (2019) further improves the
semantics preserving using dual GANs formulation instead of a linear model. Previous works focus
on vision domain where the features are extracted from well-trained deep models on large-scale im-
age dataset. We introduce the first feature generation framework tailored for zero-shot ICD coding
by exploiting existing medical knowledge from limited available data.
Zero-shot text classification. Pushp & Srivastava (2017) has explored zero-shot text classification
by learning relationship between text and weakly labeled tags on large corpus. The idea is similar
to Rios & Kavuluru (2018) in learning the relationship between input and code descriptions. Zhang
et al. (2019a) introduced a two-phase framework for zero-shot text classification. An input is first
determined as from a seen or an unseen classes before the final classification. This approach does
not directly apply to ICD coding as the input is labeled with a set of codes which can include both
seen and unseen codes. It is not possible to determine if the data is from a seen or an unseen class.
2
Under review as a conference paper at ICLR 2020
Figure 2: ZAGRNN as the feature extractor model . ZAGRNN extracts label-wise features and construct
embedding of each ICD codes using GRNN. ZAGRNN makes a binary prediction for each code based on the
dot product between graph label embedding and the label specific feature.
3	Method
3.1	Overview
The task of automatic ICD coding is to assign ICD codes to patient’s clinical notes. We formulate the
problem as a multi-label text classification problem. Let L be the set of all ICD codes and L = |L|,
given an input text, the goal is to predict yl ∈ {0, 1} for all l ∈ L. Each ICD code l has a short text
description. For example, the description for ICD-9 code 403.11 is “Hypertensive chronic kidney
disease, benign, with chronic kidney disease stage V or end stage renal disease.” There is also a
known hierarchical tree structure on all the ICD codes: for a node representing an ICD code, the
children of this node represent the subtypes of this ICD code.
We focus on the generalized zero-shot ICD coding problem: accurately assigning code l given that
l is never assigned to any training text (i.e. yl = 0), without sacrificing the performance on codes
with training data. We assume a pretrained model as a feature extractor that performs ICD coding
by extracting label-wise feature fl and predicting yl by σ(g> ∙ fl), where σ is the sigmoid function
and gl is the binary classifier for code l. For the zero-shot codes, gl is never trained on fl with yl = 1
and thus at inference time, the pretrained feature extractor hardly ever assigns zero-shot codes.
We propose to use generative adversarial networks (GAN) (Goodfellow et al., 2014) to generate fl
with yl = 1 by conditioning on code l . Figure 1 shows an overview of the generation framework.
The generator G tries to generate the fake feature f given an ICD code description. The discrim-
inator D tries to distinguish between f and real latent feature f from the feature extractor model.
After the GAN is trained, We use G to synthesize fl and fine-tune the binary classifier gl with fl for
a given zero-shot code l . Since the binary code classifiers are independently fine-tuned for zero-shot
codes, the performance on the seen codes is not affected, achieving the goal of GZSL.
3.2	Feature extractor
The pretrained feature extractor model is zero-shot attentive graph recurrent neural networks (ZA-
GRNN) modified from zero-shot attentive graph convolution neural networks (ZAGCNN), which
is the only previous work that is tailored towards solving zero-shot ICD coding (Rios & Kavuluru,
2018). We improve the original implementation by replacing the GCNN with GRNN and adopting
the label-distribution-aware margin loss (Cao et al., 2019) for training. The detailed discussion of
the performance gain by our modification is described Appendix E. Figure 2 shows the architecture
of the ZAGRNN. At a high-level, given an input x, ZAGRNN extracts label-wise feature fl and
performs binary prediction on fl for each ICD code l.
Label-wise feature extraction. Given an input clinical document x containing n words, we repre-
sent it with a matrix X = [w1, w2, . . . , wn] where wi ∈ Rd is the word embedding vector for the
i-th word. Each ICD code l has a textual description. To represent l , we construct an embedding
vector vl by averaging the embeddings of words in the description.
The word embedding is shared between input and label descriptions for sharing learned knowl-
edge. Adjacent word embeddings are combined using a one-dimension convolutional neural net-
work (CNN) to get the n-gram text features H = conv(X) ∈ RN×dc. Then the label-wise attention
3
Under review as a conference paper at ICLR 2020
feature al ∈ Rd for label l is computed by:
Sl = Softmax(tanh(H ∙ W> + ba) ∙ vι), aι = s> ∙ H for l = 1, 2,...L
where sl is the attention scores for all rows in H and al is the attended output of H for label l.
Intuitively, al extracts the most relevant information in H about the code l by using attention. Each
input then has in total L attention feature vectors for each ICD code.
Multi-label classification. For each code l, the binary prediction yl is generated by:
fl = rectifier(Wo ∙ al + bo),	yl = σ(g> ∙ fl)
We use graph gated recurrent neural networks (GRNN) (Li et al., 2015) to encode the classifier gl.
Let V(l) denote the set of adjacent codes of l from the ICD tree hierarchy and t be the number of
times we propagate the graph, the classifier gl = glt is computed by:
g00 = Vl,	h =忐 ∑j∈V(l)gj-1,	gt = GRUCell(ht ,gt-1)	(1)
|V (l)|
where GRUCell is a gated recurrent units (Chung et al., 2014) and the construction is detailed
in Appendix A. The weights of the binary code classifier is tied with the graph encoded label
embedding gl so that the learned knowledge can also benefit zero-shot codes since label embedding
is computed from a shared word. The loss function for training is multi-label binary cross-entropy:
L
Lbce3, y) = - X[yl log(yl) + (1 - yl)log(i - yl)]	(2)
l=1
As mentioned above, the distribution of ICD codes is extremely long-tailed. To counter the label
imbalance issue, we adopt label-distribution-aware margin (LDAM) (Cao et al., 2019), where we
subtract the logit value before sigmoid function by a label-dependent margin ∆l :
ym = σ(g> ∙ fl- 1(yl = 1)∆l)	⑶
where function 1(∙) outputs 1 if yι = 1 and ∆l = -C and nl is the number of training data labeled
nl
with l and C is a constant. The LDAM loss is thus: LLDAM = LBCE (y, ym).
3.3	Zero-shot Latent Feature Generation with WGAN-GP
For a zero-shot code l, the code label yl for any training data example is yl = 0 and the binary
classifier gl for code assignment is never trained with data examples with yl = 1 due to the dearth
of such data. Previous works have successfully applied GANs for GZSL in the vision domain (Xian
et al., 2018; Felix et al., 2018). We propose to use GANs to improve zero-shot ICD coding by
generating pseudo data examples in the latent feature space for zero-shot codes and fine-tuning the
code-assignment binary classifiers using the generated latent features.
More specifically, we use the Wasserstein GAN (Arjovsky et al., 2017) with gradient penalty
(WGAN-GP) (Gulrajani et al., 2017) to generate code-specific latent features conditioned on the
textual description of each code. Detail of WGAN-GP is described in Appendix B. To condition on
the code description, we use a label encoder function C : L 7→ C that maps the code description
to a low-dimension vector c. We denote cl = C(l). The generator, G : Z × C 7→ F, takes in a
random Gaussian noise vector z ∈ Z and an encoding vector c ∈ C ofa code description to generate
a latent feature fl = G(z, C) for this code. The discriminator or critic, D : F X C → R, takes in
a latent feature vector f (either generated by WGAN-GP or extracted from real data examples) and
the encoded label vector c to produce a real-valued score D(f, c) representing how realistic f is.
The WGAN-GP loss is:
—	-一,一、、—	-	一 r
LWGAN=E(f,c)~Psf,c [D(f, c))] - E(M~Pf,c [D(f, C))] +
λ ∙ E(f,C)~p^,c[(∣∣VD(f,c))∣∣2 - 1)2]	(4)
where (∙,c) ~ PS∙,c is the joint distribution of latent features and encoded label vectors from the
set of seen code labels S, f = α ∙ f +(1 — α) ∙ f with α ~ U(0,1) and λ is the gradient penalty
coefficient. WGAN-GP can be learned by solving the minimax problem: minG maxD LWGAN .
4
Under review as a conference paper at ICLR 2020
Label encoder. The function C is an ICD-code encoder that maps a code description to an embed-
ding vector. For a code l, we first use a LSTM (Hochreiter & Schmidhuber, 1997) to encode the
sequence of M words in the description into a sequence of hidden states [e1, e2, . . . , eM]. We then
perform a dimension-wise max-pooling over the hidden state sequence to get a fixed-sized encoding
vector el. Finally, we obtain the eventual embedding cl = el||gl of code l by concatenating el with
gl which is the embedding of l produced by the graph encoding network. cl contains both the latent
semantics of the description (in el) as well as the ICD hierarchy information (in gl).
Keywords reconstruction loss. To ensure the generated feature vector f captures the semantic
meaning of Code l, We encourage 力 to be able to well reconstruct the keywords extracted from the
clinical notes associated with code l.
For each input text x labeled with code l, we extract the label-specific keyword set Kl =
{w1, w2, . . . , wk} as the set of most similar words in x to l, where the similarity is measured by
cosine similarity between word embedding in x and label embedding vl. Let Q be a projection ma-
trix, K be the set of all keywords from all inputs and ∏(∙, ∙) denote the cosine similarity function,
the loss for reconstructing keywords given the generated feature is as following:
LKEY = - log P (Kl |fi) ≈- E ∏(wk ,V1) ∙ log P (Wkfl)
wk ∈Kl
-E ∏(wk,vl) ∙ log
wk ∈Kl
exp(w> ∙ Qfl)
Pw∈K eχp(w> ∙ Qfl)
(5)
Discriminating zero-shot codes using ICD hierarchy. In the current WGAN-GP framework, the
discriminator cannot be trained on zero-shot codes due to the lack of real positive features. In order
to include zero-shot codes during training, we utilize the ICD hierarchy and use fsib, the latent
feature extracted from real data of the nearest sibling lsib of a zero-shot code l, for training the
discriminator. The nearest sibling code is the closest code to l that has the same immediate parent.
This formulation would encourage the generated feature f to be close to the real latent features of the
siblings of l and thus f can better preserving the ICD hierarchy. More formally, let Csib = C(M),
we propose the following modification to LWGAN for training zero-shot codes:
LWGAN-Z =EsPJc [π(c, Csib) ∙ D(fsib, c)] - E(M)〜PfU∏(c, CSsb) ∙ D(f,即十
λ ∙ E(M)〜Pf^,c[(∣∣VD(f,c)∣∣2 - 1)2]	(6)
where C 〜PU is the distribution of encoded label vectors for the set of zero-shot codes U and
(∙, c)〜PUc is defined similarly as in Equation 4. The loss term by the cosine similarity π(c, Csib)
to prevent generating exact nearest sibling feature for the zero-shot code l. After adding zero-shot
codes to training, our full learning objective becomes:
min max LWGAN + LWGAN-Z + ∣β ∙ LKEY	(7)
GD
where β is the balancing coefficient for keyword reconstruction loss.
Fine-tuning on generated features. After WGAN-GP is trained, we fine-tune the pretrained classi-
fier gl from baseline model with generated features for a given zero-shot code l. We use the generator
to synthesize a set of fl and label them with yl = 1 and collect the set of fl from training data with
yl = 0 using baseline model as feature extractor. We finally fine-tune gl on this set of labeled feature
vectors to get the final binary classifier for a given zero-shot code l.
4	Experiments
4.1	Setup
Dataset description. We use the publicly available medical dataset MIMIC-III (Johnson et al.,
2016) for evaluation, which contains approximately 58,000 hospital admissions of 47,000 patients
who stayed in the ICU of the Beth Israel Deaconess Medical Center between 2001 and 2012. Each
admission record has a discharge summary that includes medical history, diagnosis outcomes, sur-
gical procedures, discharge instructions, etc. Each admission record is assigned with a set of most
5
Under review as a conference paper at ICLR 2020
Table 1: Results on seen codes using ZAGRNN feature extractor described in Section 3.2
Method	Pre	Micro		AUC	Macro			
		Rec	F1		Pre	Rec	F1	AUC
ZAGRNN	58.06	44.94	50.66	96.67	30.91	25.57	27.99	94.03
ZAGRNN + LLDAM	56.06	47.14	51.22	96.70	31.72	28.06	29.78	94.08
Table 2: Zero-shot ICD coding results. Scores are averaged over 10 runs on different seeds.
Method	Micro				Macro			
	Pre	Rec	F1	AUC	Pre	Rec	F1	AUC
ZAGRNN	0.00	0.00	0.00	89.05	0.00	0.00	0.00	90.89
ZAGRNN + LLDAM	0.00	0.00	0.00	90.78	0.00	0.00	0.00	91.91
ZAGRNN + Meta (Liu et al., 2019)	46.70	0.89	1.74	90.08	3.88	0.95	1.52	91.88
LWGAN (Xian et al., 2018)	23.92	17.63	20.30	91.94	17.30	17.38	17.34	92.26
LWGAN + LCLS (Xian et al., 2018)	23.57	16.55	19.44	91.71	18.39	16.81	17.56	92.32
LWGAN + LCYC (Felix et al., 2018)	23.97	17.93	20.51	91.88	17.86	17.83	17.84	92.27
LWGAN-Z + LCLS	22.49	17.40	19.62	91.80	16.56	17.26	16.90	92.16
LWGAN-Z + LCYC	21.44	17.24	19.11	91.90	16.05	17.06	16.54	92.25
LWGAN + LKEY (Ours)	23.26	18.24	20.45	91.73	17.09	18.38	17.71	92.21
LWGAN-Z (Ours)	22.18	19.03	20.48	91.79	16.87	18.84	17.80	92.28
LWGAN-Z + LKEY (Ours)	22.54	19.51	20.91	92.18	17.70	19.15	18.39	92.34
relevant ICD-9 codes by medical coders. The dataset is preprocessed as in (Mullenbach et al., 2018).
Our goal is to accurately predict the ICD codes given the discharge summary.
We split the dataset for training, validation, and testing by patient ID. In total we have 46,157
discharge summaries for training, 3,280 for validation and 3,285 for testing. There are 6916 unique
ICD-9 diagnosis codes in MIMIC-III and 6090 of them exist in the training set. We use all the codes
for training while using codes that have more than 5 data examples for evaluation. There are 96 out
of 1,646 and 85 out of 1,630 unique codes are zero-shot codes in validation and test set, respectively.
Baseline methods. We compare our method with ZAGRNN modified from previous state of the
art approaches on zero-shot ICD coding (Rios & Kavuluru, 2018) as described in Section 3.2,
meta-embedding for long-tailed problem (Liu et al., 2019) and WGAN-GP with classification loss
LCLS (Xian et al., 2018) and with cycle-consistent loss LCYC (Felix et al., 2018) that were applied
to GZSL classification in computer vision domain. Detailed description and hyper-parameters of
baseline methods are in Appendix C.
Training details. For WGAN-GP based methods, the real latent features are extracted from the final
layer in the ZAGRNN model. Only features fl for which yl = 1 are collected for training. We use
a single-layer fully-connected network with hidden size 800 for both generator and discriminator.
For the code-description encoder LSTM, we set the hidden size to 200. We train the discriminator 5
iterations per each generator training iteration. We optimize the WGAN-GP with ADAM (Kingma
& Ba, 2015) with mini-batch size 128 and learning rate 0.0001. We train all variants of WGAN-GP
for 60 epochs. We set the weight of LCLS to 0.01 and LCYC, LKEY to 0.1. For LKEY, we predict the
top 30 most relevant keywords given the generated features.
After the generators are trained, we synthesize 256 features for each zero-shot code l and fine-tune
the classifier gl using ADAM and set the learning rate to 0.00001 and the batch size to 128. We
fine-tune on all zero-shot codes and select the best performing model on validation set and evaluate
the final result on the test set.
4.2	Results
Evaluation metrics. We report both the micro and macro precision, recall, F1 and AUC scores
on the zero-shot codes for all methods. Micro metrics aggregate the contributions of all codes to
compute the average score while macro metrics compute the metric independently for each code
and then take the average. All scores are averaged over 10 runs using different random seeds.
6
Under review as a conference paper at ICLR 2020
(a) LWGAN
(b) LWGAN-Z
Figure 3: T-SNE projection of generated features for zero-shot codes using (a) LWGAN and (b) LWGAN-Z . Lighter
color are projection of generated features and darker color are of real features from the nearest sibling codes.
Features generated for zero-shot codes using LWGAN-Z are closer to the real features from the nearest sibling
codes. This shows that using LWGAN-Z can generate features that better preserve the ICD hierarchy.
Results on seen codes. Table 1 shows the results of ZAGRNN models on all the seen codes. Note
that fine-tuning the zero-shot code classifiers with meta-embedding or WGAN-GP will not affect
the classification for seen codes since the code assignment classifiers are independently fine-tuned.
Results on zero-shot codes. Table 2 summarizes the results for zero-shot codes. For the base-
line ZAGRNN and meta-embedding models, the AUC on zero-shot codes is much better than ran-
dom guessing. LLDAM improves the AUC scores and meta-embedding can achieve slighter better F1
scores. However, since these methods never train the binary classifiers for zero-shot codes on pos-
itive examples, both micro and macro recall and F1 scores are close to zero. In other words, these
models almost never assign zero-shot codes at inference time. For WGAN-GP based methods, all
the metrics improve from ZAGRNN and meta-embedding except for micro precision. This is due
to the fact that the binary zero-shot classifiers are fine-tuned on positive generated features which
drastically increases the chance of the models assigning zero-shot codes.
Ablation studies on WGAN-GP methods. We next examine the detailed performance of WGAN-
GP methods using different losses as shown in Table 2. Adding LCLS hurts the micro metrics, which
might be counter-intuitive at first. However, since the LCLS is computed based on the pretrained
classifiers, which are not well-generalized on infrequent codes, adding the loss might provide bad
gradient signal for the generator. Adding LCYC , LKEY and LWGAN-Z improves LWGAN and achieves
comparable performances in terms of both micro and macro metrics. At a closer look, LWGAN-Z
improves the recall most, which matches the intuition that learning with the sibling codes enables
the model to generate more diverse latent features. The performance drops when combing LWGAN-Z
with LCLS and LCYC. We suspect this might be due to a conflict of optimization that the generator
tries to synthesize f close to the sibling code lsib and simultaneously maps f back to the exact code
semantic space of l. Using LKEY resolves the conflict as it reconstructs more generic semantics from
the words instead of from the exact code descriptions. Our final model that uses the combination of
LWGAN-Z and LKEY achieves the best performance on both micro and macro F1 and AUC score.
T-SNE visualization of generated features. We plot the T-SNE projection of the generated features
for zero-shot codes using WGAN-GP with LWGAN and LWGAN-Z in Figure 3. Dots with lighter color
represent the projections of generated features and those with darker color correspond to the real
features from the nearest sibling codes. Features generated for zero-shot codes using LWGAN-Z are
closer to the real features from the nearest sibling codes. This shows that using LWGAN-Z can generate
features that better preserve the ICD hierarchy.
Keywords reconstruction from generated features. We next qualitatively evaluate the generated
features by examining their reconstructed keywords. We first train a keyword predictor using LKEY
on the real latent features and their keywords extracted from training data. Then we feed the gener-
ated features from zero-shot codes into the keyword predictor to get the reconstructed keywords.
Table 3 shows some examples of the top predicted keywords for zero-shot codes. Even the keyword
predictor is never trained on zero-shot code features, the generated features are able to find relevant
words that are semantically close to the code descriptions. In addition, features generated with
LWGAN-Z can find more relevant keywords than LWGAN . For instance, for zero-shot code V10.62, the
7
Under review as a conference paper at ICLR 2020
Table 3: Keywords found by generated features using LWGAN and LWGAN-Z for zero-shot ICD-9 codes. Bold
words are the most related ones to the ICD-9 code description.
Code	Description	Keywords from LWGAN	Keywords from LWGAN-Z
V10.62	Personal history of myeloid leukemia	AICD, inferoposterior, car- diogenic, leukemia, silent	leukemia, Zinc, myeloge- nous, CML, metastases
E860.3	Accidental poisoning by iso- propyl alcohol	apneic, pulses, choking, sub- stance, fractures	intoxicated, alcoholic, AST, EEG, alcoholism
956.3	Injury to peroneal nerve	vault, injury, pedestrian, or- thopedics, TSICU	injuries, neurosurgery, in- jury, TSICU, coma
851.05	Cortex contus-deep coma	contusion, injury, trauma, neurosurgery, head	brain, head, contusion, neu- rosurgery, intracranial
772.2	Subarachnoid hemorrhage of fetus or newborn	subarachnoid, SAH, neuro- surgical, screening	SUbaraChnoid,hemorrhages, SAH, newborn, pregnancy
Table 4: Few-shot ICD coding results. Scores are averaged over 10 runs on different seeds.
Method	Micro				Macro			
	Pre	Rec	F1	AUC	Pre	Rec	F1	AUC
ZAGRNN	64.00	1.27	2.48	92.11	4.15	1.23	1.90	90.99
ZAGRNN + LLDAM	60.53	1.82	3.53	92.10	6.29	1.80	2.80	90.74
ZAGRNN + Meta (Liu et al., 2019)	48.88	6.75	11.84	92.15	16.65	6.77	9.62	90.92
LWGAN (Xian et al., 2018)	29.18	18.14	22.37	92.59	20.76	18.09	19.33	90.99
LWGAN + LCLS (Xian et al., 2018)	29.18	17.67	22.01	92.54	19.88	17.62	18.68	91.01
LWGAN + LCYC (Felix et al., 2018)	28.82	18.43	22.48	92.57	20.39	18.28	19.28	90.96
LWGAN-Z + LCLS	27.97	17.70	21.68	92.58	20.18	17.59	18.80	91.01
LWGAN-Z + LCYC	28.40	18.24	22.22	92.61	20.82	18.17	19.40	90.99
LWGAN + LKEY (Ours)	28.97	18.31	22.44	92.62	20.92	18.24	19.49	91.05
LWGAN-Z (Ours)	27.66	18.81	22.39	92.56	20.45	18.81	19.59	90.97
LWGAN-Z + LKEY (Ours)	27.95	18.96	22.60	92.63	21.55	18.92	20.15	91.00
top predicted keywords from LWGAN-Z include leukemia, myelogenous, CML (Chronic myelogenous
leukemia) which are all related to myeloid leukemia, a type of cancer of the blood and bone marrow.
Results on few-shot codes. As we have seen promising results on zero-shot codes, we also evaluate
our feature generation framework on few-shot ICD codes, where the number of training data for such
codes are less than or equal to 5. We apply the exact same setup as zero-shot codes for synthesizing
features and fine-tuning classifiers for few-shot codes. There are 220 and 223 unique few-shot codes
in validation and test set, respectively.
Table 4 summarizes the results. The performance of ZAGRNN models on few-shot codes is slightly
better than zero-shot codes yet the recall are still very low. Meta-embedding can boosts the recall and
F1 scores from baseline models. WGAN-GP methods can further boosts the performance on recall,
F1 and AUC scores and the performance using different combination of losses generally follows the
pattern in zero-shot code results. In particular, our final model trained with LWGAN-Z and LKEY can
perform slightly better than other WGAN-GP models in terms of F1 and AUC scores.
5	Conclusion
We introduced the first feature generation framework, AGMC-HTS, for generalized zero-shot multi-
label classification in clinical text domain. We incorporated the ICD tree hierarchy to design GAN
models that significantly improved zero-shot ICD coding without compromising the performance on
seen ICD codes. We also qualitatively demonstrated that the generated features using our framework
can preserve the class semantics as well as the ICD hierarchy compared to existing feature generation
methods. In addition to zero-shot codes, we showed that our method can improve the performance
on few-shot codes with limited amount of labeled data.
8
Under review as a conference paper at ICLR 2020
References
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced
datasets with label-distribution-aware margin loss. In NeurIPS, 2019.
Wei-Lun Chao, Soravit Changpinyo, Boqing Gong, and Fei Sha. An empirical study and analysis
of generalized zero-shot learning for object recognition in the wild. In ECCV, 2016.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
Rafael Felix, Vijay BG Kumar, Ian Reid, and Gustavo Carneiro. Multi-modal cycle-consistent
generalized zero-shot learning. In ECCV, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In NeurIPS, 2017.
SePP Hochreiter and JUrgen Schmidhuber. Long short-term memory. Neural computation, 1997.
ICD-9 Guidelines. International classification of diseases,ninth revision, clinical modification (ICD-
9-CM). https://www.cdc.gov/nchs/icd/icd9cm.htm.
Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-wei, Mengling Feng, Mohammad
Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii,
a freely accessible critical care database. Scientific data, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. In ICLR, 2015.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graPh sequence neural
networks. arXiv preprint arXiv:1511.05493, 2015.
Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Large-
scale long-tailed recognition in an oPen world. In CVPR, 2019.
James Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng Sun, and Jacob Eisenstein. ExPlainable
Prediction of medical codes from clinical text. In NAACL, 2018.
Jian Ni, Shanghang Zhang, and Haiyong Xie. Dual adversarial semantics-consistent network for
generalized zero-shot learning. In NeurIPS, 2019.
PushPankar Kumar PushP and Muktabh Mayank Srivastava. Train once, test anywhere: Zero-shot
learning for text classification. arXiv preprint arXiv:1712.05972, 2017.
Anthony Rios and Ramakanth Kavuluru. Few-shot and zero-shot multi-label learning for structured
label sPaces. In EMNLP, 2018.
Haoran Shi, Pengtao Xie, Zhiting Hu, Ming Zhang, and Eric P Xing. Towards automated icd coding
using deeP learning. arXiv preprint arXiv:1711.04075, 2017.
Mary H Stanfill, Margaret Williams, Susan H Fenton, Robert A Jenders, and William R Hersh.
A systematic literature review of automated clinical coding and classification systems. JAMIA,
2010.
Yongqin Xian, Bernt Schiele, and ZeyneP Akata. Zero-shot learning-the good, the bad and the ugly.
In CVPR, 2017.
Yongqin Xian, Tobias Lorenz, Bernt Schiele, and ZeyneP Akata. Feature generating networks for
zero-shot learning. In CVPR, 2018.
9
Under review as a conference paper at ICLR 2020
Pengtao Xie and Eric Xing. A neural architecture for automated icd coding. In ACL, 2018.
Jingqing Zhang, Piyawat Lertvittayakumjorn, and Yike Guo. Integrating semantic knowledge to
tackle zero-shot text classification. In ACL, 2019a.
Yijia Zhang, Qingyu Chen, Zhihao Yang, Hongfei Lin, and Zhiyong Lu. Biowordvec, improving
biomedical word embeddings with subword information and mesh. Scientific data, 2019b.
10
Under review as a conference paper at ICLR 2020
A Appendix: Gated Recurrent Units
Below is the detailed construction of GRUCell in equation 1 from Section 3.2:
Zt = σ(Wz ∙ ht + Uz ∙ gt-1 + bz)
r； = σ(Wr ∙ ht + Ur ∙ gt-1 + br)
gt = (I-Zt) Θ gt-1 + Ztl Θ tanh(Wh ∙ ht + UhYrt Θ gtT) + bh)
where is the dimension-wise multiplication.
B Appendix: Generative adversarial networks
GANs (Goodfellow et al., 2014) have been extensively studied for generate highly plausible data.
The idea of GAN is to train a generator and a discriminator through a minimax game. The generator
takes in a random noise and generate fake data to fool the discriminator while the discriminator
tries to distinguish between generated data and real data. The training procedure of GANs can be
unstable, thus Arjovsky et al. (2017) proposes Wasserstein-GAN (WGAN) to counter the instability
problem by optimizing the Wasserstein distance instead of the original Jenson-Shannon divergence.
Gulrajani et al. (2017) further improves WGAN by using gradient instead of weight clipping for the
required 1-Lipschitz constraint in WGAN discriminator.
C Appendix: More training details
ICD-9 code information. We extract the ninth version of the ICD code descriptions and hierarchy
from the CDC website1. In addition to the official description, we extend the descriptions with
medical knowledge, including synonyms and clinical information, crawled from online resources2.
ZAGRNN. For the ZAGRNN model, we use 100 convolution filters with a filter size of 5. We
use 200 dimensional word vectors pretrained on PubMed corpus3 (Zhang et al., 2019b). We use
dropout on the word embedding layer with rate 0.5. We use the ADAM (Kingma & Ba, 2015) for
optimization with a minibatch size of 8 and a learning rate of 0.001. The final feature size and
GRNN hidden layer size are both set to 400. We train the ZAGRNN model for 40 epochs.
Meta-embedding. Liu et al. (2019) proposed meta-embedding for solving large long-tail problem
by transferring knowledge from head classes to tail classes. The method naturally fits ICD coding
due to the long-tailed code distribution. To apply meta-embedding in ICD coding, we first construct
a set of centroids M as the mean of fl for each code l from the training data. Let Θ denote dimension-
wise multiplication, then the meta-embedding for f is calculated as:
fmeta = f + e(f) Θ (o(f)>∙ M)	(8)
where o(f) is the attention scores for selecting centroids M and e(f) is a dimension-wise coefficient
for selecting the attended features. Both o and r are parameterized as neural networks and are learned
during fine-tuning. The final classification is performed by y = σ(g> ∙ fmela).
For meta-embedding, we fine-tune the neural network modules e and o using ADAM and set learning
rate to 0.0001 and batch size to 32.
WGAN-GP with classification loss. Xian et al. (2018) proposed to add a cross-entropy loss during
training WGAN-GP to generate features being correctly classified as conditioned labels. In ICD
coding, this loss translates to enforcing f being classified as positive for code l:
LCLS = -log P (yι = 1∣f) = - log σ(g> ∙ fl)	(9)
WGAN-GP with cycle consistency loss. Similar to adding LCLS to prevent the generated features
being random, Felix et al. (2018) proposed to add a loss that constrains the synthetic representations
to generate back their original semantic features. Let R : F 7→ C be a linear regression estimate the
label embedding cι from the generated feature fι, the cycle consistency loss is defined as:
LCYC = l∣cι - R(fι)ll2	(10)
1https://www.cdc.gov/nchs/icd/icd9cm.htm
2http://www.icd9data.com/
3https://github.com/ncbi-nlp/BioWordVec
11
Under review as a conference paper at ICLR 2020
D Appendix: Precision-Recall Curves
(a) Zero-shot codes
Figure 4: Micro averaged precision-recall curves comparisons on zero-shot codes (a) and few-shot codes (b).
Red line is the baseline feature extractor described in Section 3.2 and blue line is from our final model. The
legend denotes the correspond area under the curve (AUC) value.
We plot the precision-recall curve with micro averaged scores for previous state of the art ICD
coding model and ours on few-shot codes and zero-shot codes. As demonstrated in Figure 4, our
model can achieve better precision and recall trade-off and also higher area under the curve scores
for both zero-shot codes and few-shot codes.
E Comparison of feature extrators on Seen ICD codes
Table 5: Results on all the seen codes using different feature extractors described in Section 3.2
Method	Pre	Micro		AUC	Macro			
		Rec	F1		Pre	Rec	F1	AUC
ZAGCNN (Rios & KaVUlUrU, 2018)	58.29	44.64	50.56	96.59	30.00	24.65	27.06	94.00
ZAGRNN (ours)	58.06	44.94	50.66	96.67	30.91	25.57	27.99	94.03
ZAGRNN + LLDAM (ours)	56.06	47.14	51.22	96.70	31.72	28.06	29.78	94.08
We compare the performance of our modified ZAGRNN with the original ZAGCNN proposed
by Rios & Kavuluru (2018) on seen ICD codes in Table 5. Discussion of the modification is de-
tailed in Section 3.2. With ZAGRNN, almost all metrics slightly increased from ZAGCNN except
for micro precision. With LLDAM loss, our final feature extractor can improve more significantly from
ZAGCNN especially for macro metrics and achieve better precision recall trade-off. These modifi-
cations are not enough to get reasonable performance zero-shot codes as shown in Table 2, mainly
due to the lack of positive example for zero-shot codes during training.
12