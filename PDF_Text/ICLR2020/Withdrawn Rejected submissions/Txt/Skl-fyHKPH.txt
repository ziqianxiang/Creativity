Under review as a conference paper at ICLR 2020
A Mean-Field Theory for Kernel Alignment
with Random Features in Generative Adverse-
rial Networks
Anonymous authors
Paper under double-blind review
Ab stract
We propose a novel supervised learning method to optimize the kernel in maxi-
mum mean discrepancy generative adversarial networks (MMD GANs). Specif-
ically, we characterize a distributionally robust optimization problem to compute
a good distribution for the random feature model of Rahimi and Recht to approx-
imate a good kernel function. Due to the fact that the distributional optimization
is infinite dimensional, we consider a Monte-Carlo sample average approxima-
tion (SAA) to obtain a more tractable finite dimensional optimization problem.
We subsequently leverage a particle stochastic gradient descent (SGD) method to
solve the derived finite dimensional optimization problem. Based on a mean-field
analysis, we then prove that the empirical distribution of the interactive particles
system at each iteration of the SGD follows the path of the gradient descent flow
on the Wasserstein manifold. We also establish the non-asymptotic consistency
of the finite sample estimator. Our empirical evaluation on synthetic data-set as
well as MNIST and CIFAR-10 benchmark data-sets indicates that our proposed
MMD GAN model with kernel learning indeed attains higher inception scores
well as Frechet inception distances and generates better images compared to the
generative moment matching network (GMMN) and MMD GAN with untrained
kernels.
1 Introduction
A fundamental and long-standing problem in unsupervised learning systems is to capture the under-
lying distribution of data. While deep generative models such as Boltzmann machines Salakhutdi-
nov & Hinton (2009) and auto-encoding variational Bayes Kingma & Welling (2013) accomplish
this task to some extent, they are inadequate for many intractable probabilistic computations that
arise in maximum likelihood estimation. Moreover, in many machine learning tasks such as caption
generation Xu et al. (2015), the main objective is to obtain new samples rather than to accurately
estimate the underlying data distribution. Generative adverserial network (GAN) Goodfellow et al.
(2014) provides a framework to directly draw new samples without estimating data distribution. It
consists ofa deep feedforward network to generate new samples from a base distribution (e.g. Gaus-
sian distribution), and a discriminator network to accept or reject the generated samples. However,
training GAN requires finding a Nash equilibrium of a non-convex minimax game with continuous,
high-dimensional parameters. Consequently, it is highly unstable and prone to miss modes Salimans
et al. (2016); Che et al. (2016). To obtain more stable models, the generative moment matching net-
works (GMMNs) Li et al. (2015) are proposed, wherein instead of training a discriminator network,
a non-parametric statistical hypothesis test is performed to accept or reject the generated samples via
the computation of the kernel maximum mean discrepancy Gretton et al. (2007). While leveraging a
statistical test simplifies the loss function for training GMMN, in practice, the diversity of generated
samples by GMMN is highly sensitive to the choice of the kernel. Thus, to improve the sampling
performance, the kernel function also needs to be jointly optimized with the generator. Rather than

Under review as a conference paper at ICLR 2020
optimizing the kernel directly, the MMD GAN model Li et al. (2017) is proposed in which an em-
bedding function is optimized in conjunction with a fixed user-defined kernel (e.g. RBF Gaussian
kernel). However, there are no theoretical guarantees that the user-defined kernel is the ‘right’ kernel
for embedded features.
Contributions. To address the kernel model selection problem in MMD GAN Li et al. (2017), in this
paper we put forth a novel framework to learn a good kernel function from training data. Our kernel
learning approach is based on a distributional optimization problem to learn a good distribution for
the random feature model of Rahimi and Recht Rahimi & Recht (2008; 2009) to approximate the
kernel. Since optimization with respect to the distribution of random features is infinite dimensional,
we consider a Monte Carlo approximation to obtain a more tractable finite dimensional optimization
problem with respect to the samples of the distribution. We then use a particle stochastic gradient
descent (SGD) to solve the approximated finite dimensional optimization problem. We provide a
theoretical guarantee for the consistency of the finite sample-average approximations. Based on a
mean-field analysis, we also show the consistency of the proposed particle SGD. In particular, we
show that when the number of particles tends to infinity, the empirical distribution of the particles in
SGD follows the path of the gradient descent flow of the distributional optimization problem on the
Wasserstein manifold.
2 Preliminaries of MMD GANs
Assume we are given data {vi}in=1 that are sampled from an unknown distribution PV with the
support V . In many unsupervised tasks, we wish to attain new samples from the distribution PV
without directly estimating it. Generative Adversarial Network (GAN) Goodfellow et al. (2014)
provides such a framework. In vanilla GAN, a deep network G(∙; W) parameterized by W ∈ W
is trained as a generator to transform the samples Z 〜PZ, Z ∈ Z from a user-defined distribution
PZ (e.g. Gaussian) into a new sample G(Z; W)〜PW, such that the distributions PW and PV
are close. In addition, a discriminator network D(∙; δ) parameterized by δ ∈ ∆ is also trained
to reject or accept the generated samples as a realization of the data distribution. The training of
the generator and discriminator networks is then accomplished via solving a minimax optimization
problem as below
min max IEPV [D(X; δ)] + IEPZ [log(1 - D(G(Z; W); δ))].
W∈W δ∈∆
(1)
In the high dimensional settings, the generator trained via the min-max program of equation 1 can
potentially collapse to a single mode of distribution where it always emits the same point Che et al.
(2016). To overcome this shortcoming, other adversarial generative models are proposed in the
literature, which propose to modify or replace the discriminator network by a statistical two-sample
test based on the notion of the maximum mean discrepancy which is defined below:
Definition 2.1. (MAXIMUM MEAN DISCREPANCY GRETTON ET AL. (2007)) Let (X , d) be a
metric space, F be a class of functions f : X → IR, and P, Q ∈ B(X) be two probability measures
from the set of all Borel probability measures B(X) on X. The maximum mean discrepancy (MMD)
between the distributions P and Q with respect to the function class F is defined below
DF [P, Q]
def
sup
f∈F X
f(x)(P - Q)(dx).
(2)
Different choices of the function class F in equation 2 yield different adversarial models such as
Wasserstein GANs (WGAN) Arjovsky et al. (2017), f-GANs Nowozin et al. (2016), and GMMN
and MMD GAN Li et al. (2017; 2015). In the latter two cases, the function class F d=ef {f : X →
IR : kf kHX ≤ 1}, where HX is a reproducing kernel Hilbert space (RKHS) of functions with a
kernel K : X × X → IR, denoted by (HX , K). Then, the squared MMD loss in equation 2 as a

Under review as a conference paper at ICLR 2020
measure of the distance between the distributions PV and PW has the following expression
DK [PV , PW]
def
sup
f:x→R"f kHχ ≤1
f(x)(PV
X
- PW )(dx)
IEV,v0〜PV [K(V; V0)] + IEW W〜PW [K(W, W0)] - 2IEv〜PV ,w〜PW [K(V; W)],
(3)
(4)
where X = V ∪ W . Instead of training the generator via solving the minimax optimization in
equation 1, the MMD GAN model of Li et al. (2015) proposes to optimize the discrepancy between
two distributions via optimization of an embedding function ι : IRD 7→ IRp , p ≤ D, i.e.,
min maxMMDko∣[PV, PW],
W∈W ι∈Q
(5)
where k : IRp × IRp → IR is a user-defined fixed kernel. In Li et al. (2015), the proposal for the
kernel k : IRp × IRp → IR is a mixture of the Gaussians,
k ◦ ∣(χ,y) = k(∣(χ),∣(y)) = X (kι(x) 2ι(y)k2
i=1	σi
(6)
where the bandwidth parameters σι, •…，σm, > 0 are manually selected. Nevertheless, in practice
there is no guarantee that the user-defined kernel k(ι(x), q(y)) can capture the structure of the
embedded features ι(x).
3 Proposed approach: kernel learning with random features for
MMD GANS
In this section, we first expound our kernel learning approach. Then, we describe a novel MMD
GAN model based on the proposed kernel learning approach.
3.1	Robust distributional optimization for kernel learning
To address the kernel model selection issue in MMD GAN Li et al. (2017), we consider a kernel
optimization scheme with random features Rahimi & Recht (2008; 2009). Let 夕： IRd × IRD →
[-1,1] denotes the explicit feature maps and μ ∈ M(IRD) denotes a probability measure from the
space of probability measures M(IRD) on IRD. The kernel function is characterized via the explicit
feature maps using the following integral equation
K (χ, y) = !Eμ3(χ; ξ)φ(y; ξ)]
o(χ; ξ)φ(y; ξ)μ(dξ).
(7)
Ξ
Let MMDμ[Pv,PW] =f MMDκμ [Pv, PW]. Then, the kernel optimization problem in can be
formulated as a distribution optimization for random features, i.e,
min SuPMMDμ[Pv, PW].	(8)
W∈W μ∈P
Here, P is the set of probability distributions corresponding to a kernel class K. In the sequel, we
consider P to be the distribution ball of radius R as below
P =f IBR(μo) =f {μ ∈ M(IRD) : Wp(μ, μo) ≤ R},	(9)
where μo is a user-defined base distribution, and To establish the proof, We consider the Wp(∙, ∙) is
the p-Wasserstein distance defined as below
Wp(μ1,μ2) =f	πinf “2)ZM RO kξι — ξ2kPdπ(ξ1, ξ)) p ,	(10)
iii
Under review as a conference paper at ICLR 2020
where the infimum is taken with respect to all couplings ∏ of the measures μ, μo ∈ M(IRD), and
Π(μ, μo) is the set of all such couplings with the marginals μ and μo.
The kernel MMD loss function in equation 8 is defined with respect to the unknown distributions of
the data-set PV and the model PW. Therefore, we construct an unbiased estimator for the MMD
loss function in equation 8 based on the training samples. To describe the estimator, sample the
labels from a uniform distribution yι,…，yn 〜i.i.d. Uniform{-1, +1}, where We assume that the
number of positive and negative labels are balanced. In particular, consider the set of positive labels
I = {i ∈ {1, 2, ∙∙∙ ,n} : yi = +1}, and negative labels J = {1, 2, ∙∙∙ , n}∕I, where their
cardinality is |I| = |J| = 2. We consider the following assignment of labels:
•	Positive class labels: If yi = +1, sample the corresponding feature map from data-distribution
Xi = Vi 〜PV.
•	Negative class labels: Ifyi = -1, sample from the corresponding feature map from the generated
distribution Xi = G(Zi, W)〜PW, Zi 〜PZ.
By this construction, the joint distribution of features and labels PY,X has the marginals PX|Y=+1 =
PV, and PX|Y=-1 = PW. Moreover, the following statistic, known as the kernel alignment in the
literature (see, e.g., Sinha & Duchi (2016); Cortes et al. (2012)), is an unbiased estimator of the
MMD loss in equation 8,
Tmin, sup M∖Dμ hPv,Pwi =f / 8 X yyKμ(χi, Xj).	(ii)
W∈w μ∈Ρ	L	」n(n - 1) ι≤⅛n
See Appendix C.1 for the related proof. The kernel alignment in equation 11 can also be viewed
through the lens of the risk minimization
W∈W μnPM∖DahPV ,PW i =f n(n⅛ JXKμ (xi, Xj ))2	(12a)
8
n(n - 1)a E	(αyM - 1Eμ[2(Xi； ξ2(Xj；ξ用.(12b)
1≤i<j≤n
Here, α > 0 is a scaling factor that determines the separation between feature vectors, and
K* def ayyT is the ideal kernel that provides the maximal separation between the feature vec-
tors over the training data-set, i.e., K* (Xi, Xj) = a when features have identical labels y% = yj, and
K*(Xi, Xj) = -α otherwise. Upon expansion of the risk function in equation 12, it can be easily
shown that it reduces to the kernel alignment in equation 11 when α → +∞. Intuitively, the risk
minimization in equation 12 gives a feature space in which pairwise distances are similar to those in
the output space Y = {-1, +1}.
3.2	SAA for distributional optimization
The distributional optimization problem in equation 8 is infinite dimensional, and thus cannot be
solved directly. To obtain a tractable optimization problem, instead of optimizing with respect to
the distribution μ of random features, we optimize the i.i.d. samples (particles) ξ1, ∙∙∙ , ξN 〜i上& μ
generated from the distribution. The empirical distribution of these particles is accordingly defined
as follows
N
bN (ξ) =f NN X δ(ξ -ξk),
k=1
(13)
where δ(∙) is the Dirac,s delta function concentrated at zero. In practice, the optimization problem in
equation 12 is solved via the Monte-Carlo sample average approximation of the objective function,
min min
w ∈w μN ∈Ρn
α
MMDbN
hPV , PW i
n(n - 1)α
1N	2
X	(ayiyj- N X 夕(Xi；ξk)中(Xj; ξk)) , (14)
1≤i<j ≤n
k=1
8
iv
Under review as a conference paper at ICLR 2020
where PN	def	IBN(bN) = {∕N	∈ M(IRD) :	Wp(bN,bN)	≤	r},	and	μN	is the empirical
measure associated with the initial samples ξ0, •…，ξN nd μ0. The empirical objective function
in equation 14 can be optimized with respect to the samples ξ1,…,ξN using the particle stochastic
gradient descent. For the optimization problem in equation 14, the (projected) stochastic gradient
descent (SGD) takes the following recursive form,1
ξm + 1 = ξm - N 卜 mδrm - α1N X 夕(Xm； ξm )中 @m； ξ3 )^ ▽{(夕(Xm； ξk.)夕 @m； ξm )),	(15a)
for k =1,2,…，N, where (ym, Xm), @m, Xm)〜i.i.d PX,y and η ∈ IR>0 denotes the learning rate
of the algorithm, and the initial particles are ξ1, ∙∙∙ , ξN 〜i.i.d. μo. At each iteration of the SGD
dynamic in equation 15, a feasible solution for the inner optimization of the empirical risk function
in equation 14 is generated via the empirical measure
1N
bm(ξ) = N ∑δ(ξ - ξm).	(16)
Indeed, we prove in Section 4 that for an appropriate choice of the learning rate η > 0, the empirical
measure in equation 16 remains inside the distribution ball bmm ∈ PN for all m ∈ [0, NT] ∩ IN,
and is thus a feasible solution for the empirical risk minimization equation 14 (see Corollary 4.2.1
in Section 4).
3.3	Proposed MMD GAN with kernel learning
In Algorithm 1, we describe the proposed method MMD GAN model with the kernel learning ap-
proach described earlier. Algorithm 1 has an inner loop for the kernel training and an outer loop
for training the generator, where we employ RMSprop Tieleman & Hinton (2012). Our proposed
MMD GAN model is distinguished from MMD GAN of Li et al. (2017) in that we learn a good
kernel function in equation 17 of the inner loop instead of optimizing the embedding function that is
implemented by an auto-encoder. However, we mention that our kernel learning approach is com-
patible with the auto-encoder implementation ofLi et al. (2017) for the dimensionality reduction of
features (and particles). In the case of including an auto-encoder, the inner loop in Algorithm 1 must
be modified to add an additional step for training the auto-encoder. However, to convey the main
ideas more clearly, the training step of the auto-encoder is omitted from Algorithm 1.
3.4	Computational Complexity Analysis
Sampling the labels y, y 〜i.i.d. Uniform{-1, +1} require O(1) complexity, while sampling
Xe|ye = +1, X|y = +1 〜 PV and e∣e = +1, x|y = —1 〜 PW has a complexity of
O(d). The computation of the stochastic gradient step in equation 17 requires computing the sum
N PlN=I 3(x； ξk	; ξk). This can be done as a separate preprocessing step prior to executing
the SGD in equation 17, and requires preparing the vectors φ =f [夕(x; ξ1),… “(x; ξN)] and
0 =f [夕@; ξ1),…，夕@; ξN)]. Using the random feature model of Rahimi and Recht Rahimi &
Recht (2008; 2009), where 夕(x; ξ) = √2cos(xτξ + b). Here b 〜Uniform[-π, +∏], the complex-
ity of computing the vectors φ and 0 is of the order O(Nd) on a single processor. However, this
construction is trivially parallelizable. Furthermore, computation can be sped up even further for cer-
tain distributions μo. For example, the Fastfood technique can approximate φ and 0 in O(Nlog(d))
1To avoid clutter in our subsequent analysis, the normalization factor 似：-1)of the gradient is omitted by
modifying the step-size η.
v
Under review as a conference paper at ICLR 2020
Algorithm 1 MMD GAN with a supervised kernel learning Method (Monte-Carlo Approach)
Inputs: The learning rates ij,η > 0 , the number of iterations of discriminator per generator update T ∈ N, the batch-size n, the number
of random features N ∈ N. Regularization parameter α > 0.
while W has not converged do
for t = 1, 2, ∙∙∙ ,T do
Sample the labels y, e 〜i.i.d Uniform{ — 1, 1}.
Sample the features x|y = +1 〜PV, and x|y = -1 〜PW. Similarly, x|e = +1 〜PV, and X∣e = -1 〜PW.
For all k = 1, 2, ∙∙∙ ,N, update the particles,
1N
N X W(X； ξ )奴x； ξ ) 1 Vξ (奴χ;针)φ(x; ξ )),
(17)
end for
Sample a balanced minibatch of labels {yi}n=ι 〜i.i.d. Uniform{ - 1, +1}.
Sample the minibatch {x}n=ι SUCh that Xi |yi = +1 〜 PV, and Xi∣yi = —1 〜 PW for all i = 1, 2, ∙∙∙ ,n.
Update the generator
gw ^ VwDaN [Pv , Pw], μN
N
(18a)
end while
W — W — ηRMSprop(gw, W).
(18b)
time for the Gaussian kernel Le et al. (2013). Updating each particle in equation 17, involves the
computation of the gradient Vξ(^(ξ; ξk)夕(ξ; ξk))) WhiCh IS O(d). Thus, the complexity of one
iteration of SGD for all the particles is O(N d).
Overall, one step of the kernel learning has a complexity of O(N d). On the other hand, to attain
ε-suboptimal solution maxk=1,2,…,N ∣∣ξk 一 ξk ∣∣ ≤ ε, the SGD requires has the sample complexity
O (log( ε)). Consequently, the computational complexity of the kernel learning is of the order of
O(Nd log( ε)). To compare this complexity with thatofMMD GAN is of the order O (B2' log( ɪ)),
Where B is the batch size for approximation of the population MMD, and ` is the number of kernel
mixtures.
4 Consistency and a mean-field analysis
In this section, we provide theoretical guarantees for the consistency of various approximations we
made to optimize the population MMD loss function in equation 8. We defer the proofs of the fol-
lowing theoretical results to AppendixC. The main assumptions ((A.1),(A.2), and (A.3)) underlying
our theoretical results are also stated in the same section.
Consistency of finite-sample estimate: In this part, we prove that the solution to finite sample
optimization problem in equation 14 approaches its population optimum in equation 8 as the number
of data points as well as the number of random feature samples tends to infinity.
Theorem 4.1. (NON-ASYMPTOTIC CONSISTENCY OF FINITE-SAMPLE ESTIMATOR) Suppose
conditions (A.1)-(A.3) of Appendix C are satisfied. Consider the distribution balls P and PN
that are defined with respect to the 2-Wasserstein distance (p = 2). Furthermore, consider the
optimal MMD values of the population optimization and its finite sample estimate
(W*,μ*) =ef arg min arg sup MMDμ[Pv,Pw].
1 Λ / f- I ∕Λ /	—ι∙`.
W ∈W μ∈P
(19a)
(WN，bN) d=f arg W∈W 产 Jnf.. ∖DαN [PV,PW ],
w ∈W	μN ∈Pn
(19b)
vi
Under review as a conference paper at ICLR 2020
respectively. Then, with the probability of (at least) 1 - 3% over the training data samples
{(xi, yi)}in=1 and the random feature samples {ξ0k}kN=1, the following non-asymptotic bound holds
MMDμ*[Pv,Pw*] - MMDμN [Pv,PwN]
(20)
/L2 *(d + 2) 1 ι
≤V -ɪln 2
28Ndiam2 (X)
--------+ 2 max
ln 2
4 ) ,C2RL4 ln
% n2
8L2
+-----,
α
%
where ci = 3 1 X 24, and c? = 9 X 211.
The proof of Theorem 4.1 is presented in Appendix C.1.
Notice that there are three key parameters involved in the upper bound of Theorem 4.1. Namely,
the number of training samples n, the number of random feature samples N, and the regularization
parameter α. The upper bound in equation 20 thus shows that when n, N, α → +∞, the solution
obtained from solving the empirical risk minimization in equation 12 yields a MMD population
value tending to the optimal value of the distributional optimization in equation 8.
Consistency of particle SGD for solving distributional optimization. The consistency result
of Theorem 4.1 is concerned with the MMD value of the optimal empirical measure bN (ξ) =
N PN=I δ(ξ - ξk) of the empirical risk minimization equation 14. In practice, the particle SGD
is executed for a few iterations and its values are used as an estimate for (ξ1,…,ξN). Conse-
quently, it is desirable to establish a similar consistency type result for the particle SGD estimates
(ξm, •…，ξN) at the m-th iteration. To reach this objective, We define the scaled empirical measure
as follows
1N
μNNtc = N∑δ(ξ-Em」)，0≤t≤τ.
k=1
(21)
At any time t, the scaled empirical measure μN is a random element, and thus (μN)o≤t≤τ is a
measured-valued stochastic process. Therefore, we characterize the evolution of its Lebesgue den-
sity PN (ξ) = μN(dξ)∕dξ in the following theorem:
Theorem 4.2. (MCKEAN-VLASOV MEAN-FIELD PDE) Suppose conditions (A.1)-(A.3) ofAp-
pendix C are satisfied. Further, suppose that the Radon-Nikodyme derivative qo(ξ) = μo(dξ)∕dξ
exists. Then, there exists a unique solution (Pt=(ξ))0≤t≤T to the following non-linear partial differ-
ential equation
ʃ dP∂tξ) = -η RRx×y (RRp w(x,ξ⅛(e,e)pt(e)de- αye) Vξ(pt(ξ)VξM(χ; ξ)w(e; ξ))dpχ2γ,
[po(ξ)	= qo (ξ).
(22)
Moreover, the measure-valued process {(μN)o≤t≤τ}n∈in defined in equation 21 converges
(weakly) to the unique solution μt (ξ) = PJ= (ξ)dξ as the number OfParticleS tend to infinity N → ∞.
2
Due to the mean-field analysis of Theorem 4.2, we can prove that the empirical measure μNι of the
particles in SGD dynamic equation 15 remains inside the feasible distribution ball PN:
Corollary 4.2.1. Consider the learning rate η = O (亍√而Rp金百») for the SGD in equation 15.
Then, the empirical measure bm of the particles remains inside the distributional ball bN ∈ PN =
{bN ∈ M(IRD) : Wp(μN, bN) ≤ R} for all m ∈ [0, NT ] ∩ IN, with the probability of (at least)
1 - δ.
2The notion of the weak convergence of a sequence of empirical measures is formally defined in Supple-
mentary.
vii
Under review as a conference paper at ICLR 2020
Let us make two remarks about the PDE in equation 22. First, the seminal works of Otto Otto
(2001), and Jordan, et al. Jordan et al. (1998) establishes a deep connection between the McKean-
Vlasov type PDEs specified in equation 22 and the gradient flow on the Wasserstein manifolds.
More specifically, the PDE equation in equation 22 can be thought of as the minimization of the
energy functional
μ∈M(fRD) Ea(pt(ξ)) = 1 LD Rα(ξ,pt(ξy)pt(ξ)dξ
(23a)
Rα(ξ,Pt(ξ)) def -α(IEPχ,γ [yψ(x; ξ)])2 + IEe〜Pth(IEPX [φ(x; ξ)φ(x; e)]) i,	(23b)
using the following gradient flow dynamics
dpt(ξ)
dt
-η ∙ gradptEa(Pt(ξ)),
p0 (ξ) = q0(ξ),
(24)
where gradptE(Pt(ξ)) = Vξ ∙ (pt(ξ)VξRa(Pt(ξ))) is the Riemannian gradient of Ra(μt(ξ)) With
respect to the metric of the Wasserstein manifold . This shows that when the number of particles
in particle SGD equation 15 tends to infinity (N → +∞), their empirical distribution follows a
gradient descent path for minimization of the population version (with respect to data samples) of
the distributional risk optimization in equation 12. In this sense, the particle SGD is the ‘consistent’
approximation algorithm for solving the distributional optimization.
4.1	Related works
The mean-field description of SGD dynamics has been studied in several prior works for different
information processing tasks. Wang et al. Wang et al. (2017) consider the problem of online learning
for the principal component analysis (PCA), and analyze the scaling limits of different online learn-
ing algorithms based on the notion of finite exchangeability. In their seminal papers, Montanari and
co-authors Mei et al. (2018); Javanmard et al. (2019); Mei et al. (2019) consider the scaling limits
of SGD for training a two-layer neural network, and characterize the related Mckean-Vlasov PDE
for the limiting distribution of the empirical measure associated with the weights of the input layer.
Similar mean-field type results for two-layer neural networks are also studied recently in Rotskoff
& Vanden-Eijnden (2018); Sirignano & Spiliopoulos (2018). Our work is also related to the unpub-
lished work of Wang, et al. Wang et al., which proposes a solvable model of GAN and analyzes the
scaling limits. However, our GAN model is different from Wang et al. and is based on the notion
of the kernel MMD. Our work is also closely related to the recent work of Li, et al Li et al. (2019)
which proposes an implicit kernel learning method based on the following kernel definition
Kh(ι(x), ι(y)) = IEξ〜“° [e(K)(I(X)-IQ)))] ,	(25)
where μo is a user defined base distribution, and h ∈ H is a function that transforms the base distri-
bution μo into a distribution μ that provides a better kernel. Therefore, the work of Li, et al Li et al.
(2019) implicitly optimizes the distribution of random features via transforming a random variable
with a function. In contrast, the proposed distributional optimization framework in this paper opti-
mizes the distribution of random feature explicitly, via optimizing their empirical measures. Perhaps
more importantly from a practical perspective is the fact that our kernel learning approach does not
require the user-defined function class H. Moreover, our particle SGD method in equation 15 ob-
viates tuning hyper-parameters related to the implicit kernel learning method such as the gradient
penalty factor and the variance constraint factor (denoted by λGP and λh, respectively, in Algorithm
1 ofLi et al. (2019)).
5 Empirical evaluation
5.1	Synthetic data-set
Due to the space limitation, the experiments on the synthetic data are deferred to Appendix A.
viii
Under review as a conference paper at ICLR 2020
5.2	Performance on benchmark datasets
We evaluate our kernel learning approach on large-scale benchmark data-sets. We train our MMD
GAN model on two distinct types of datasets, namely on MNIST LeCun et al. (1998) and CIFAR-10
LeCun et al. (1998), where the size of training instances are 60 × 103 and 50 × 103, respectively.
All the generated samples are from a fixed noise random vectors and are not singled out.
Implementation and hyper-parameters. We implement Algorithm 1 as well as MMD GAN Li
et al. (2017) in Pytorch using NVIDIA Titan V100 32GB graphics processing units (GPUs). The
source code of Algorithm 1 is built upon the code ofLi et al. (2017), and retains the auto-encoder im-
plementation. In particular, we use a sequential training of the auto-encoder and kernel as explained
in the Synthetic data in Section A of Supplementary. For a fair comparison, our hyper-parameters
are adjusted as in Li et al. (2017), i.e., the learning rate of 0.00005 is considered for RMSProp
Tieleman & Hinton (2012). Moreover, the batch-size for training the generator and auto-encoder is
n = 64. The learning rate of particle SGD is tuned to η = 10.
Random Feature Maps. To approximate the kernel, We use the the random feature model of
Rahimi and Recht Rahimi & Recht (2008; 2009), where 夕(x; ξ) = √2cos(xτξ + b). Here b 〜
Uniform{-π, +π} is a random bias term.
Practical considerations. When data-samples {Vi} ∈ IRd are high dimensional (as in CIFAR-
10), the particles ξ1, ∙∙∙ , ξN ∈ IRD, D = d in SGD equation 15 are also high-dimensional. To
reduce the dimensionality of the particles, we apply an auto-encoder architecture similar to Li et al.
(2017), and train our kernel on top of learned embedded features. More specifically, in our simula-
tions, we train an auto-encoder where the dimensionality of the latent space is h = 10 for MNIST,
and h = 128 (thus D = d = 128) for CIFAR-10. Therefore, the particles ξ1, ∙∙∙ , ξN in subsequent
kernel training phase have the dimension of D = 10, and D = 128, respectively.
Choice of the scaling parameter α. There is a trade-off in the choice ofα. While for large values
of α, the kernel is better able to separate data-samples from generated samples, in practice, it slows
down the convergence of particle SGD. This is due to the fact that the coupling between the particle
dynamics in equation 15 decrease as α increase. The scaling factor is set to be α = 1 in all the
following experiments.
Qualitative comparison. We now show that without the bandwidth tuning for Gaussian kernels
and using the particle SGD to learn the kernel, we can attain better visual results on benchmark
data-sets. In Figure 1, we show the generated samples on CIFAR-10 and MNIST data-sets, using
our Algorithm 1, MMD GAN Li et al. (2017) with a mixed and homogeneous Gaussian RBF kernels,
and GMMN Li et al. (2015).
Figure 1(a) shows the samples from Algorithm 1, Figure 1(b) shows the samples from MMD GAN
Li et al. (2017) with a mixture RBF Gaussian kernel κ(x, y) = P5k=1 κσk (x, y), where σk ∈
{1, 2, 4, 8, 16} are the bandwidths of the Gaussian kernels that are fine tuned and optimized. We
observe that our MMD GAN with automatic kernel learning visually attains similar results to MMD
GAN Li et al. (2017) which requires manual tuning of the hyper-parameters. In Figure 1(c), we show
the MMD GAN result with a single kernel RBF Gaussian kernel whose bandwidth is manually tuned
at σ = 16. Lastly, in Figure 1(d), we show the samples from GMMN Li et al. (2015) which does not
exploit an auto-encoder or kernel training. Clearly, GMMN yield a poor results compared to other
methods due to high dimensionality of features, as well as the lack of an efficient method to train
the kernel.
On MNIST data-set in Figure 1,(e)-(h), the difference between our method and MMD GAN Li
et al. (2017) is visually more pronounced. We observe that without a manual tuning of the kernel
bandwidth and by using the particle SGD equation 15 to optimize the kernel, we attain better gener-
ated images in Figure 1(e), compared to MMD GAN with mixed RBF Gaussian kernel and manual
bandwidth tuning in Figure 1(f). Moreover, using a single RBF Gaussian kernel yields a poor result
regardless of the choice of its bandwidth. The generated images from GMMN is also shown in
Figure 1(h).
ix
Under review as a conference paper at ICLR 2020
(c)
d)
3¾75t∖⅛o
8/夕&
夕7QG∕ygo
。,5-Mk->∙6 —
⅞5G□9s-τ,
⅜N6 □ / ɔ Al Z
&今/31 夕7，
∕∙*qrJ9(z/
Cy
匕Y 4
G 4?
民42
J Q 0
0 S 3
r 5 q
'Cd
Q 4 W
CKg
《 1 q
勺码
(g)	(h)
(e)	(f)
a



Figure 1: Sample generated images using CIFAR-10 (top row), and MNIST (bottom row) data-sets.Panels
(a)-(e): Proposed MMD GAN with an automatic kernel selection via the particle SGD (Algorithm 1), Panels
(b)-(f): MMD GAN Li et al. (2017) with an auto-encoder for dimensionality reduction in conjunction with a
mixed RBF Gaussian kernel whose bandwidths are manually tuned, Panels (c)-(g): MMD GAN in Li et al.
(2017) with a single RBF Gaussian kernel with an auto-encoder for dimensionality reduction in conjunction
with a single RBF Gaussian kernel whose bandwidth is manually tuned, Panel (d)-(g): GMMN without an
auto-encoder Li et al. (2015).
Quantitivative comparison. To quantitatively measure the quality and diversity of generated sam-
ples, We compute the inception score (IS) Salimans et al. (2016) as well as FreChet Inception Dis-
tance (FID) Heusel et al. (2017) on CIFAR-10 images. Intuitively, the inception score is used for
GANs to measure samples quality and diversity. Accordingly, for generative models that are col-
lapsed into a single mode of distribution, the inception score is relatively low. The FID improves on
IS by actually comparing the statistics of generated samples to real samples, instead of evaluating
generated samples independently.
In Table 1, we report the quantitative measures for different MMD GAN model using different scor-
ing metric. Note that in Table 1 lower FID scores and higher IS scores indicate a better performance.
We observe from Table 1 that our approach attain lower FID score, and higher IS score compared
to MMD GAN with single Gaussian kernel (bandwidth σ = 16), and a mixture Gaussian kernel
(bandwidths {1, 2, 4, 8, 16}).
]	Method	FIDTZ)	IS (↑)	Il
MMD GAN (GaUssian) Li et al. (2017)	67.244 ± 0.134	5.608±0.05T7
MMD GAN (MiXtUre GaUssian) Li et al. (2017)	67.129 ± 0.148	5.850±0.055-
SGD Alg. 1	65.059 ± 0.153	5.97 ± 0.046
Real Data	0	11.237±0.1k
Table 1: Comparison of the quantitative performance measures of MMD GANs with different kernel
learning approaches.
x
Under review as a conference paper at ICLR 2020
References
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein GAN. arXiv preprint
arXiv:1701.07875, 2017.
Patrick Billingsley. Convergence of probability measures. John Wiley & Sons, 2013.
Stephane Boucheron, Gabor Lugosi, and Pascal Massart. Concentration inequalities: A nonaSymp-
totic theory of independence. Oxford university press, 2013.
Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative
adversarial networks. arXiv preprint arXiv:1612.02136, 2016.
Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Algorithms for learning kernels based
on centered alignment. Journal ofMachine Learning Research, 13(Mar):795-828, 2012.
Joseph Leo Doob. Stochastic processes, volume 101. New York Wiley, 1953.
Rui Gao and Anton J Kleywegt. Distributionally robust stochastic optimization with wasserstein
distance. arXiv preprint arXiv:1604.02199, 2016.
Kay Giesecke, Konstantinos Spiliopoulos, Richard B Sowers, et al. Default clustering in large
portfolios: Typical events. The Annals OfApplied Probability, 23(1):348-385, 2013.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Scholkopf, and Alex J Smola. A kernel
method for the two-sample-problem. In Advances in neural information processing systems, pp.
513-520, 2007.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems, pp. 6626-6637, 2017.
Adam Jakubowski. On the skorokhod topology. In Annales de l'IHP Probabilites et statistiques,
volume 22, pp. 263-285, 1986.
Adel Javanmard, Marco Mondelli, and Andrea Montanari. Analysis of a two-layer neural network
via displacement convexity. arXiv preprint arXiv:1901.01375, 2019.
Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the Fokker-
Planck equation. SIAM journal on mathematical analysis, 29(1):1-17, 1998.
Masoud Badiei Khuzani and Na Li. Stochastic primal-dual method on riemannian manifolds with
bounded sectional curvature. arXiv preprint arXiv:1703.08167, 2017.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint
arXiv:1312.6114, 2013.
Quoc Le, Tamas Sarlos, and Alexander Smola. Fastfood-computing hilbert space expansions in
loglinear time. In International Conference on Machine Learning, pp. 244-252, 2013.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas PoCzos. Mmd GAN:
Towards deeper understanding of moment matching network. In Advances in Neural Information
Processing Systems, pp. 2203-2213, 2017.
xi
Under review as a conference paper at ICLR 2020
Chun-Liang Li, Wei-Cheng Chang, Youssef Mroueh, Yiming Yang, and Barnabas Poczos. Implicit
kernel learning. arXiv preprint arXiv:1902.10214, 2019.
Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In International
Conference on Machine Learning, pp. 1718-1727, 2015.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of the IEEE international conference on computer vision, pp. 3730-3738, 2015.
Shishi Luo and Jonathan C Mattingly. Scaling limits of a model for selection at two scales. Nonlin-
earity, 30(4):1682, 2017.
Colin McDiarmid. On the method of bounded differences. Surveys in combinatorics, 141(1):148-
188, 1989.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural
networks: dimension-free bounds and kernel limit. arXiv preprint arXiv:1902.06015, 2019.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f -GAN: Training generative neural sam-
plers using variational divergence minimization. In Advances in neural information processing
systems, pp. 271-279, 2016.
Felix Otto. The geometry of dissipative evolution equations: the porous medium equation. 2001.
Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in Optimization, 1
(3):127-239, 2014.
David Pollard. Empirical processes: theory and applications. In NSF-CBMS regional conference
series in probability and statistics, pp. i-86. JSTOR, 1990.
Yu V Prokhorov. Convergence of random processes and limit theorems in probability theory. Theory
of Probability & Its Applications, 1(2):157-214, 1956.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
neural information processing systems, pp. 1177-1184, 2008.
Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization
with randomization in learning. In Advances in neural information processing systems, pp. 1313-
1320, 2009.
Philippe Robert. Stochastic networks and queues, volume 52. Springer Science & Business Media,
2013.
Grant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems:
Asymptotic convexity of the loss landscape and universal scaling of the approximation error.
arXiv preprint arXiv:1805.00915, 2018.
W Rudin. Real and complex analysis mcgraw-hill book co. New York 3rd ed., xiv, 1987.
Ruslan Salakhutdinov and Geoffrey Hinton. Deep boltzmann machines. In Artificial intelligence
and statistics, pp. 448-455, 2009.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training GANs. In Advances in neural information processing systems,
pp. 2234-2242, 2016.
xii
Under review as a conference paper at ICLR 2020
Shashank Singh and Barnabas Poczos. Minimax distribution estimation in Wasserstein distance.
arXiv preprint arXiv:1802.08855, 2018.
Aman Sinha and John C Duchi. Learning kernels With random features. In Advances in Neural
Information Processing Systems, pp. 1298-1306, 2016.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural netWorks. arXiv
preprint arXiv:1805.01053, 2018.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-
31, 2012.
VS Varadarajan. On the theorem of Riesz concerning the form of linear functional. 1958.
Chuang Wang, Hong Hu, and Yue M Lu. A solvable high-dimensional model of GAN.
Chuang Wang, Jonathan Mattingly, and Yue Lu. Scaling limit: Exact and tractable analysis of
online learning algorithms With applications to regularized regression and PCA. arXiv preprint
arXiv:1712.04332, 2017.
Jeff Webb. Extensions of GronWall’s inequality With quadratic groWth terms and applications. Elec-
tronic Journal of Qualitative Theory of Differential Equations, 2018(61):1-12, 2018.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. ShoW, attend and tell: Neural image caption generation With visual
attention. In International conference on machine learning, pp. 2048-2057, 2015.
Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning With humans in the loop. arXiv
preprint arXiv:1506.03365, 2015.
Appendix
We provide additional material to support the content presented in the paper. This Appendix is
organized as folloWs:
Appendix A.	We provide the results of our experiments on the synthetic data-set described in Section
5.1 of the main text.
Appendix B.	We provide the results of our experiments on the LSUN and CelebA data-sets.
Appendix C.	We present the proofs of the main theoretical results of Section 4 in the main text.
Appendix D.	We present the proofs of auxiliary lemmas used to support the proof of main results.
Appendix E.	We prove additional theoretical results regarding the so-called chaoticity of the particle
SGD in equation 15.
A	Experimental Results on the S ynthetic Data-Set
The synthetic data-set We consider is as folloWs:
•	The distribution of training data is PV = N(0, (1 + λ)Id×d),
•	The distribution of generated data is PW = N(0, (1 - λ)Id×d).
xiii
Under review as a conference paper at ICLR 2020
(a)	(b)	(c)
Figure 2: Visualization of data-points from the synthetic data-set PV = N(0, (1 + λ)Id×d) and PW =
N(0, (1 - λ)Id×d) for d = 2. Panel (a): λ = 0.1, Panel (b): λ = 0.5, and Panel (c): λ = 0.9.
To reduce the dimensionality of data, we consider the embedding ι : IRd 7→ IRp, x 7→ ι(x) = Σx,
where Σ ∈ IRp×d andp < d. In this case, the distribution of the embedded features are PX|Y=+1 =
N(0, (1 + λ)ΣΣT), and PX|Y =-1 = N(0, (1 - λ)ΣΣT).
Note that λ ∈ [0, 1] is a parameter that determines the separation of distributions. In particular,
the Kullback-Leibler divergece of the two multi-variate Gaussian distributions is controlled by λ ∈
[0, 1],
DKL(PX|Y=-1, PX|Y=+1) = I
2
-p+p(1 - λ2)
(26)
In Figure 2, we show the distributions of i.i.d. samples from the distributions PV and PW for
different choices of variance parameter of λ = 0.1, λ = 0.5, and λ = 0.9. Notice that for larger λ
the divergence is reduced and thus performing the two-sample test is more difficult. From Figure 2,
we clearly observe that for large values of λ, the data-points from the two distributions PV and PW
have a large overlap and conducting a statistical test to distinguish between these two distributions
is more challenging.
A.0. 1 Kernel Learning Approach
Figure 4 depicts our two-phase kernel learning procedure which we also employed in our implemen-
tations of Algorithm 1 on benchmark data-sets of Section 5.2 in the main text. The kernel learning
approach consists of training the auto-encoder and the kernel optimization sequentially, i.e.,
α
sup SUpMMDk_N◦i[Pv,Pw].	(27)
bN∈PN ∣∈Q	μ
where the function class is defined Q =f {ι(z) = Σz, Σ ∈ IRDXd}, and (KbN ◦ ι)(xι, x2)=
KbN (ι(xι), 1(x2)). Now, we consider a two-phase optimization procedure:
•	Phase (I): we fix the kernel function, and optimize the auto-encoder to compute a co-
variance matrix Σ for dimensionality reduction
•	Phase (II): we optimize the kernel based on the learned embedded features.
This two-phase procedure significantly improves the computational complexity of SGD as it reduces
the dimensionality of random feature samples ξ ∈ IRD, D	d. When the kernel function K is
fixed, optimizing the auto-encoder is equivalent to the kernel learning step of Li et al. (2017).
xiv
Under review as a conference paper at ICLR 2020
(a)
(b)
(c)
(d)
Figure 3: The evolution of the empirical measure μNτ(ξ) = N PN=I δ(ξ - ξkl) of the SGD particles
ξm,… ,ξm ∈ IR2 at different iterations m. The empirical measure of random feature maps seemingly Con-
verges to a Gaussian stationary measure corresponding to a Gaussian RBF kernel. Panel (a): m = 0, Panel (b):
m = 300, Panel (c): m = 1000, and Panel (d): m = 2500.
A.0.2 Statistical Hypothesis Testing with the Kernel MMD
Let V1,…，Vm	〜i.i.d.	PV	=	N(0, (1 + λ)Id×d), and W1,…，Wn	〜PW	=	N(0, (1 -
λ)Id×d). Given these i.i.d. samples, the statistical test T({Vi}im=1, {Wi}jn=1) : Vm × Wn → {0, 1}
is used to distinguish between these hypotheses:
•	Null hypothesis H0 : PV = PW (thus λ = 0),
•	Alternative hypothesis H1 : PV 6= PW (thus λ > 0).
To perform hypothesis testing via the kernel MMD, we require that HX is a universal RKHS, defined
on a compact metric space X. Universality requires that the kernel K(∙, ∙) be continuous and, HX
be dense in C(X). Under these conditions, the following theorem establishes that the kernel MMD
is indeed a metric:
Theorem A.1. (METRIZABLITY OF THE RKHS) Let F denotes a unit ball in a universal RKHS
HX defined on a compact metric space X with the associated continuous kernel K(∙, ∙). Then, the
kernel MMD is a metric in the sense that MMDK[PV, PW] = 0 if and only if PV = PW.
To design a test, let μmm(ξ) = N PN=I δ(ξ - ξm) denotes the solution of SGD in equation 15
for solving the optimization problem. Consider the following MMD estimator consisting of two
xv
Under review as a conference paper at ICLR 2020
U -statistics and an empirical function
1N
MMDKbN ◦∣[{Vi}m=ι, {Wi}i=ι] = ^~π ∑∑^(∣(Vi), ξm W(∣(匕),ξm)
μm	m( m — 1) -i—* -i—*
k=1 i6=j
1N
+ nn-1 E ∑>(∣(Wi), ξmm∣(Wj), ξm
k=1 i6=j
Nm n
-nmmXXX以∣(Wi),ξm)ψ(∣(Vj),ξm).	(28)
k=1 i=1 j =1
Given the samples {Vi}im=1 and {Wi}in=1, we design a test statistic as below
T({Vi}im=1,{Wi}in=1)
d=ef	H0
= H1
if MMDκμN^∣[{Vi}m=ι, {Wi}n=ι] ≤ T
if MMDKbN◦ι[{Vi}i=1, {Wi}n=l] >T,
μm
(29)
where τ ∈ IR is a threshold. Notice that the unbiased MMD estimator of equation 28 can be
negative despite the fact that the population MMD is non-negative. Consequently, negative values
for the statistical threshold τ equation 29 are admissible. In the following simulations, we only
consider non-negative values for the threshold τ.
A Type I error is made when H0 is rejected based on the observed samples, despite the null hy-
pothesis having generated the data. Conversely, a Type II error occurs when H0 is accepted despite
the alternative hypothesis H1 being true. The significance level of a test is an upper bound on the
probability of a Type I error: this is a design parameter of the test which must be set in advance, and
is used to determine the threshold to which we compare the test statistic. The power of a test is the
probability of rejecting the null hypothesis H0 when it is indeed incorrect. In particular,
Power d=ef IP(reject H0|H1 is true).
(30)
In this sense, the statistical power controls the probability of making Type II errors.
A.0.3 Empirical Results
In Figure 3, We show the evolution of the empirical measure μNr (ξ) of SGD particles by plotting
the 2D histogram of the particles ξm,…，ξN ∈ IRD at different iterations of SGD (D = d).
Clearly, starting with a uniform distribution in 3(a), the empirical measure seemingly evolves into
a Gaussian measure in Figure 3(d). The evolution to a Gaussian distribution demonstrates that
the RBF Gaussian kernel corresponding to a Gaussian distribution for the random features indeed
provides a good kernel function for the underlying hypothesis test with Gaussian distributions.
In Figure 4, we evaluate the power of the test for 100 trials of hypothesis test using the test statistics
of equation 29. To obtain the result, we used an autoencoder to reduce the dimension from d = 100
top = 50. Clearly, for the trained kernel in Panel (a) of Figure 4, the threshold τ for which Power =
1 increases after learning the kernel via the two phase procedure described earlier. In comparison,
in Panel (b), we observe that training an auto-encoder only with a fixed standard Gaussian kernel
K(x, y) = e-kx-yk22 attains lower thresholds compared to our two-phase procedure. In Panel
(c), we demonstrate the case of a fixed Gaussian kernel without an auto-encoder. In this case, the
threshold is significantly lower due to the large dimensionality of the data.
From Figure 4, we also observe that interestingly, the phase transition in the statistical threshold τ
is not sensitive to the parameter λ. This phenomenon can be justified by the fact that the kernel
learning indeed improves the MMD value more significantly for smaller values of λ (i.e., more
difficult hypothesis testing problems) than larger values of λ. See Figure 5.
xvi
Under review as a conference paper at ICLR 2020
(a)
(b)	(c)
Figure 4: The statistical power versus the threshold τ for the binary hypothesis testing via the unbiased
estimator of the kernel MMD. The parameters for this simulations are λ ∈ {0.1, 0.5, 0.9}, d = 100, n + m =
100, p = 50. Panel (a): Trained kernel using the two-phase procedure with the particle SGD in equation 15 and
an auto-encoder, Panel (b): Trained kernel with an auto-encoder and a fixed Gaussian kernel with the bandwidth
σ = 1, Panel (c): Untrained kernel without an auto-encoder.
50000 IOOOOO 150000	200000	250000	O 50000 IOOOOO 150000	200000	250000	O 50000 IOOOOO 15∞00	200000	25∞00
lterati(Mis	Iterations	Iterations
(a)	(b)	(C)
Figure 5: The MMD value during the two phase procedure for the kernel training. In the first phase, an
auto-enCoder is trained (blue Curve). In the seCond phase, the kernel is trained using the embedded features (red
curve). Panel (a): λ = 0.9, Panel (b): λ = 0.5, Panel (c):X = 0.1 .
B	Experimental Results on LSUN Bedroom and CelebA Data-sets
In this section, we present additional simulations using CelebA Liu et al. (2015), and LSUN bedroom
Yu et al. (2015) data-sets. The LSUN dataset of bedroom pictures resized to 64 × 64, and the CelebA
dataset of celebrity face images resized and cropped to 160 × 160. The sample generated images
are shown in Figure 6.
The inception score for LSUN bedroom data set is 3.860 ± 0.0423 using a single Gaussian kernel
with the bandwidth of σ = 16, and 4.064 ± 0.061 using our proposed method in Algorithm 1.
C	Proofs of Main Theoretical Results
Before we delve into proofs, we state the main assumptions underlying our theoretical results:
Assumptions:
(A.1) The feature space X = V ∪ W ⊂ IRd is compact with a finite diameter diam(X ) < ∞,
where V = support(PV ) and W = support(PW) are the supports of the distributions
PV and PW respectively.
xvii
Under review as a conference paper at ICLR 2020
(b)
(a)
(c)
Figure 6: The sample generated images from CelebA (top row), and LSUN bedroom data-sets (bottom row).
Panels (a): Proposed MMD GAN with an automatic kernel selection via the particle SGD (Algorithm 1), Panels
(b): MMD GAN Li et al. (2017) with an auto-encoder for dimensionality reduction in conjunction with a mixed
RBF Gaussian kernel whose bandwidths are manually tuned, Panels (c): MMD GAN in Li et al. (2017) with
a single RBF Gaussian kernel with an auto-encoder for dimensionality reduction in conjunction with a single
RBF Gaussian kernel whose bandwidth is manually tuned.
(A.2) The feature maps are bounded and Lipchitz almost everywhere (a.e.) ξ ∈ IRD. In particu-
lar, suPχ∈x |夕(X ξ) < LO, suPχ∈x l∣Vξ夕(Xξ)k2 ≤ LI, and suPξ∈iRD ∣∣Vχ夕(X ξ)k <
def
L2. Let L = max{LO, L1, L2} < +∞.
(A.3) Let bN (ξ) def N PN=I δ(ξ — ξk) denotes the empirical measure for the initial particles
ξ0,…，ξN. We assume that bN (ξ) converges (weakly) to a deterministic measure μ0 ∈
M(IRD). Furthermore, We assume the limiting measure μ0 is absolutely continuous w.r.t.
Lebesgue measure and has a compact support support(μ0) = Ξ ⊂ IRD .
Notation: We denote vectors by lower case bold letters, e.g. X = (x1, ∙ ∙ ∙ , Xn) ∈ IRn, and matrices
by the upper case bold letters, e.g., M = [Mij] ∈ IRn×m . The Frobenius norm of a matrix is
denoted by lMlF = Pin=1 Pjm=1 |Mij|2. Let IBr(X) d=ef {y ∈ IRd : ly — Xl2 ≤ r} denote the
Euclidean ball of radius r centered at X. For a given metric space X, Let Cb(IRd) denote the space
of bounded and continuous functions on X equipped with the usual supremum norm
lfl∞ d=ef sup |f(X)|.	(31)
x∈X
Further, Cbk (X) the space of all functions in Cb(X) whose partial derivatives up to order k are
bounded and continuous, and Cck (X) the space of functions whose partial derivatives up to order k
are continuous with compact support.
xviii
Under review as a conference paper at ICLR 2020
We denote the class of the integrable functions f with f (t) ≥ 0 a.e., on 0 ≤ t ≤ T by L1+ [0, T].
Similarly, L+∞[0, T] will denote the essentially bounded functions with f(t) ≥ 0 almost everywhere.
For a given metric space X, we denote the Borel σ-algebra by B(X). For a Borel set B ∈ B(X),
the measure value of the set B with respect to the measure is given by μ(B). The space of finite
non-negative measures defined on X is denoted by M(X). The Dirac measure with the unit mass
at X ∈ X is denoted by δ(x). For any measure μ ∈ M(X) and any bounded function f ∈ Cb(X),
we define
hμ,f i=f I
X
f (x)μ(dx).
(32)
The space M(X) is equipped with the weak topology, i.e., a (random) sequence {μN}n∈n con-
verges weakly to a deterministic measure μ ∈ M(X) if and only if hμN, f)→ hμ, f〉for all
f ∈ Cb(X). We denote the weak convergence by μN w→ly μ. Notice that when X is Polish,
then M(X) equipped with the weak topology is also Polish.3 For a Polish space X, let DX ([0, T])
denotes the Skorokhod space of the CadIag functions that take values in X defined on [0,T]. We
assume that DX ([0, T]) is equipped with the Skorokhod’s J1-topology Billingsley (2013), which in
that case DX ([0, T]) is also a Polish space.
We use asymptotic notations throughout the paper. We use the standard asymptotic notation for
sequences. Ifan and bn are positive sequences, then an = O(bn) means that lim supn→∞ an/bn <
∞, whereas an = Ω(bn) means that hmmf n→∞ an/bn > 0. Furthermore, an = O (bn) implies
an = O(bnpoly log(bn)). Moreover an = o(bn) means that limn→∞ an/bn = 0 and an = ω(bn)
means that limn→∞ an/bn = ∞. Lastly, we have an = Θ(bn) if an = O (bn) and an = Ω(bn).
Finally, for positive a, b > 0, denote a . b if a/b is at most some universal constant.
Definition C.1. (ORLICZ NORM) The Young-Orlicz modulus is a convex non-decreasing function
ψ : IR+ → IR+ such that ψ(0) = 0 and ψ(x) → ∞ when x → ∞. Accordingly, the Orlicz norm of
an integrable random variable X with respect to the modulus ψ is defined as
kXkψ def inf{β > 0 : IE[Ψ(I∣X|- IE[∣X∣]∣∕β)] ≤ 1}.	(33)
def
In the sequel, we consider the Orlicz modulus ψν(x) = (XV) - 1 . Accordingly, the cases of k ∙ ∣∣ψ2
and k ∙ ∣∣ψι norms are called the sub-Gaussian and the sub-exponential norms and have the following
alternative definitions:
Definition C.2. (SUB-GAUSSIAN NORM) The sub-Gaussian norm of a random variable Z, denoted
by ∣Z∣ψ2, is defined as
∣Z∣ψ2 = sup q-1/2(IE|Z|q)1/q.	(34)
q≥1
For a random vector Z ∈ IRn , its sub-Gaussian norm is defined as follows
∣Z∣ψ2 = sup ∣hx,Zi∣ψ2.	(35)
x∈Sn-1
Definition C.3. (SUB-EXPONENTIAL NORM) The sub-exponential norm ofa random variable Z,
denoted by ∣Z ∣ψ1 , is defined as follows
∣Z∣ψ1 = supq-1(IE[|Z|q])1/q.	(36)
q≥1
For a random vector Z ∈ IRn , its sub-exponential norm is defined below
∣Z∣ψ1 = sup ∣hZ, xi∣ψ1.	(37)
x∈Sn-1
3A topological space is Polish if it is homeomorphic to a complete, separable metric space.
xix
Under review as a conference paper at ICLR 2020
C.1 Proof of Theorem 4.1
By the triangle inequality, we have that
∣MMDμ*[Pv,Pw*] - MMDbN [°v，0WNil ≤ Ai + A2 + A3 + A4,	(38)
where the terms Ai, i = 1, 2, 3, 4 are defined as follows
Ai =f ∣MMDμ* [Pv ,Pw* ] - min sup MMDμ[Pv ,Pw ] l
I	W∈w μ∈P	I
A2 d=ef ∣ min sup M\MD [PV, Pw] - min sup M\MDbN hPV, Pwi∣
2	l W ∈W μ∈P	*L V , W	W ∈W μN ∈PN	μ [ V , W l
A3 == ∣ min SUP MMDbNhPV, PWi — MMDbNhPV, PWi ∣
IW∈w μN ∈pN	L	J	* L	-I I
A4 =f ∣MMDbN hpV,PwNi - MMDbN [「v, PWNi ∣∙
In the sequel, we compute an upper bound for each term on the right hand side of equation 38:
Upper bound on Ai :
First, notice that the squared kernel MMD loss in equation 4 can be characterized in terms of class
labels and features defined in Section 3.1 as follows
MMDμ [Pv , PW] = 4IEp®2 [ybKμ (x, b)].	(39)
x,y
To see this equivalence, we first rewrite the right hand side of equation 39 as follows
IEP®2 [ybκμ(X, b)] = IP{y = +1}IP{b = +1}IEχ X〜P®2	Kμ(X, b)]
y,x	,	x|y=+1
+ IP{y = -1}IP{b = -1}IEx X〜P®2	[Kμ(X, b)]
,	x|y=-1
-IHy= -l}Ip{b = +1}IEx 〜Pχ∣y=-1,X 〜Px|y=+i[K“(X,b)]
-IP{y = +1}IP{b = -1}IEx 〜Pχ∣y=+ι,x 〜Pχ∣y=-1 [Kμ(x, b)].	(40)
Now, recall from Section 3.1 that PX|y=+i = Pv, and PX|y=-i = PW by construction of the labels
and random features. Moreover, y,b 〜i.i.d. Uniform{-1, +1}, and thus IP{y = -1} = IP{y =
+1} = 1. Therefore, from equation 40, We derive
IEP瞪[yybKμ(x, b)] = 4IEP02 [Kμ(x; b)] + IIEP®2 [K"(x; b)] - IIEPV ,Pw [Kμ(x; b)]
=∣MMDμ[PV ,Pw ].
For any given W ∈ W , We have that
∣ sup MMDμ[Pv,Pw] - sup MMDμ[Pv,Pw]∣
Iμ∈P	μ∈P	I
≤ SUPIMMDμ[PV ,pW] - MMDμ[PV ,PW ]∣
4 sup
μ∈P
n(n - J) ^X yiyjKμ(xi, Xj) - IEP®X [yyKμ(X, X)]
i6=j
4 μuP ∣IEμ [En(ξ)]∣
≤ 4 sup IEμ [En(ξ)]
μ∈P
xx
Under review as a conference paper at ICLR 2020
where the error term is defined using the random features
En(ξ) =f / 1 ι、Xyyj中(Xi ξ)φ(xj;ξ) - IEP®2[ybXx;ξ)dχ,ξ)].
n(n - 1)	x,y
i6=j
(41)
Now, we invoke the following strong duality theorem Gao & Kleywegt (2016):
Theorem C.4. (STRONG DUALITY FOR ROBUST OPTIMIZATION, (GAO & KLEYWEGT, 2016,
THEOREM 1)) Consider the general metric space (Ξ, d), and any normal distribution ν ∈ M(Ξ),
where M(Ξ) is the set of Borel probability measures on Ξ. Then,
μ∈sMpΞ) {IEμ[ψ⑻]: Wp(μ,V) ≤ Ro=min bRp" ξnΞ[",Z)- ψ(ξ)]νW)}, (42)
provided that Ψ is upper semi-continuous in ξ.
Under the strong duality of Theorem C.4, we obtain that
I sup M∖Dμ[Pv ,Pw] - sup MMDμ[Pv ,Pw]
Iμ∈P	μ∈P
≤4 ∣min bRp- ZRD ζ∈nfD [λkξ -ζkp — En(ζ)]μo(dξ)}∣.
(43)
In the sequel, letp = 2. The Moreau’s envelope Parikh & Boyd (2014) of a function f : X → IR is
defined as follows
Mf (y) =f inf [ 1∑llχ — yk2 + f(X)], ∀y ∈ X,	(44)
x∈X 2β
where β > 0 is the regularization parameter. When the function f is differentiable, the following
lemma can be established:
Lemma C.5. (MOREAU’ S ENVELOPE OF DIFFERENTIABLE FUNCTIONS) Suppose the function
f : X → IR is differentiable. Then, the Moreau’s envelope defined in equation 44 has the following
upper bound and lower bounds
f(y) — β Z sup l∣Vf(y + S(X — y))k2ds ≤ Me(y) ≤ f(y).	(45)
2	0 x∈X
In particular, when f is Lf -Lipschitz, we have
βL2
f(y) — 寸 ≤ Me(y) ≤ f(y).	(46)
The proof is presented in Appendix D.1.
Now, we return to Equation equation 43. We leverage the lower bound on Moreau’s envelope in
equation 45 of Lemma C.5 as follows
I sup ΜΜDμ[Pv ,Pw] — sup MMDμ[Pv ,Pw]
Iμ∈P	μ∈P
≤4
≤4
min bR2-LD M-En (ξ)μo(dξ)}∣
min (λR2 + IEμo [En(ξ)] + 4λIEμo "∕1 ζ∈RD kVEn((1 — s)ξ + sζ)k2ds
j)
≤ 4∣IEμo [Εn(ξ)]l +4RIEμo
Z sup
0 ζ ∈IRD
lVEn((1 — s)ξ + sζ)l22ds
(47)
xxi
Under review as a conference paper at ICLR 2020
Let Z* = Z*(ξ,s) = argsupζ∈RD ∣∣VEn(1 - s)ξ + sZ∣∣2. Then, applying the union bound in
conjunction with Inequality equation 47 yields
IP I I SUP MMDμ [Pv , PW] — SUP MMDμ [Pv , PW]
∖J μ∈P	μ∈P
≤ IP(I^RD En(ξ)μo(dξ) ≥ 8) + IP (ZJIkVEn((I - s)ξ + sZ*)k2dsμ°(dξ) ≥ 8R)
(48)
Now, we state the following lemma:
Lemma C.6. (TAIL BOUNDS FOR THE FINITE SAMPLE ESTIMATION ERROR) Consider the esti-
mation error En defined in equation 41. Then, the following statements hold:
•	Z = ∣VEn(ξ)∣22 is a sub-exponential random variable with the Orlicz norm of ∣Z ∣ψ1 ≤
9×2n×L4 for every ξ ∈ IRD Moreover,
∣VEn((1 - s)ξ + sZ*)∣2dsμo(dξ) ≥ δ
n2δ	+ L±
≤ 2e 9×29×l4 + 9
(49)
•	En(ξ) is zero-mean sub-Gaussian random variable with the Orlicz norm of∣En(ξ)∣ψ2 ≤
16^n3L4 for ^very ξ ∈ IRD. Moreover
IP
((OEn(ξ)μo(dξ) ≥ δ
n2δ2
≥ 2e i6√3l4 .
(50)
The proof of Lemma C.6 is presented in Appendix C.2.
Now, we leverage the probability bounds equation 49 and equation 50 of Lemma C.6 to upper bound
the terms on the right hand side of equation 48 as below
(I	I ʌ	-	n2 δ2	-	n2δ	+ Li
IP I SuP MMDμ[Pv ,Pw ] - SuP MMDμ [Pv ,Pw ]I ≥ δ ≤ 2e √3×211×L4 + 2e 9×212×RL4 + 9
yl μ∈P	μ∈P	1 J
--	n2δ2	-	n2δ	+ L£)
≤ 4 max e e √3×211 ×L4 , e 9×212 ×RL4 9
(51)
where the last inequality comes from the basic inequality a + b ≤ 2 max{a, b}. Therefore, with the
probability of at least 1 - %, we have that
I SuPMMDμ[Pv,Pw] - Sup MMDμ[Pv,Pw]I
Iμ∈P	μ∈P	I
/	3 34 × 2号 × L2 1 1 /4、
≤ max < ---------ln2	—
-I n	V%√
(52)
for all W ∈ W .
Lemma C.7. (DISTANCE BETWEEN MINIMA OF ADJACENT FUNCTIONS) Let Ψ(W ) : W → IR
and Φ(W) : W → IR. Further, suppose ∣Ψ(W) - Φ(W)∣∞ ≤ δfor some δ > 0. Then,
min Ψ(W) - min Φ(W)I ≤ δ.
W∈W	W∈W	I
(53)
xxii
Under review as a conference paper at ICLR 2020
See Appendix D.4 for the proof.
____ dpf	-_	_	.	___ dpf	_ __ _	-_	_
Let Ψ(W) =	supμ∈p MMDμ[Pv,Pw],	and	Φ(W) =	sup*∈p MMD*[Pv, Pw].	Then, from
Inequality equation 53, we have the following upper bound on A1
Ai = ∣MMDμ*[Pv,Pw*] - min SuPMMDμ[Pv,Pw]∣
I	w ∈W μ∈P	I
=∣ min SuPMMDμ[Pv, PW] — min SuPMMDμ[Pv, PW] ∣
I W∈W μ∈p μ	W∈W μ∈p μ	I
3 34 X 24 X L2 1 1 ∕4∖ 9 X 21i X RL4 1 44eL94 ʌ ]
≤ max < -----------ln2	- ,--------5----ln ------- > .	(54)
n	% ,	n2	%
with the probability of (at least) 1 - %.
Upper bound on A2 :
To establish the upper bound on A2, we recall that
MMDμN [Pv , PW ]
1	1
n(n — 1) N
N
XX
yy 6 Xi ξk')ψ(xj;ξk)
i6=j k=1
1
MMDμ[Pv,Pw]=n⑺ - i)EyiyjIEμ[g(xi; ξ)夕(Xj; ξ)].
(55a)
(55b)
Therefore,
ISuPMMDμ [PV, Pw] — SuP MMDbN [Pv ,Pw] ∣
1μ∈P	μN∈Pn	1
≤ I SuPIEμ[p(xi; ξ)夕(Xj; ξ)] — sup IEμN[P(Xi; ξ)夕(Xj; ξ)]∣. (56)
1	μ∈P	μN∈Pn	1
Here, the last inequality is due to Theorem C.4 and the following duality results hold
SupEXx；	ξ)φ(xb;	ξ)]	=	inf ιλR2	-	j inf {λkξ	- ζk2	-夕(Xi； Z)中(Xj； ζ)}μo(dξ))
μ∈P	λ≥0 I	JRD Z∈RD	J
SuP	IEbN [夕(Xi； ξ)φ(xj；	ξ)]	=	inf ]λR2	-	-1 X i∏f {λkξk	-	ζk2 -夕(Xi； Z)中(Xj； Z)}) .
μN∈Pn N	λ≥0 [	N k=1 Z∈rd	J
Now, in the sequel, we establish a uniform concentration result for the following function
T(X,b)：RN ×d→ R
[	1	1	1	1	1、N、	1	,	1 ɪ
(ξ1,…，ξN )→ τx,b)(ξ1,…，ξN) = N 々吟(x,∙”,∙)(ξk)-乙 D M¾x,",∙)(ξ)μ0(dξ).
Then, from equation 56 we have
∣ sup MMDμ[Pv,Pw] — SuP MMDμN [Pv,Pw]∣ ≤ sup sup ∣Tλχχ)(ξ0,…，ξN)∣∙ (57)
1 μ∈P	bN ∈Pn	1	λ≥0 x,X∈X	'
We now closely follow the argument of Rahimi & Recht (2008) to establish a uniform concentration
result with respect to the data points X, Xb ∈ X. In particular, consider an -net cover of X ⊂ IRd.
xxiii
Under review as a conference paper at ICLR 2020
Then, We require Ne = (4diam(X)) balls of the radius e > 0, e.g., see (Pollard, 1990, Lemma
4.1, Section 4). Let Z = {zι,…，ZNJ ⊂ X denotes the center of the covering net. Now, let
(ξ1,…，ξk,…,ξN) ∈ IRN×D and (ξ0,…，ξ0,…，ξNN) ∈ IRN×D be two sequences that differs
in the k-th coordinate for 1 ≤ k ≤ N. Then,
∣T(zi,zj)(£1, …，ξo,…，ξN) - T(Zi,zj)(S0, …，ξk,…，ξN)∣
="lM⅛(Zi,)Rzj,)(ξk) - M-Mzi,∙)Rzj,∙)(ξk)∣.	(58)
Without loss of generality suppose M-^(Z ..)Rw)(ξk) ≥ MU;.)Rs)(ξk). Then,
M-Ui.∙)Hs)(ξk)- MU,∙)Rzj,∙)(ξk)
=JnL {λkζ-ξk I∣2- ψ(zi; ζ)ψ(zj; ζ)}- JnL {λkζ- ξkk2 - ψ(zi;ζ )ψ(.zj ；ζ )0
ζ∈IRD	ζ∈IRD
(a)
≤ —dzi； ξkW(Zj; ξo)- inf0 {λkζ - ξkk2 —以4； ZW(Zj；z”
ζ∈IRD
(b)
≤ —中(Zi ξo W(Zj； ξk) + SUp {3(zi； ZW(Zj； ζ)}
ζ∈IRD
(c)
≤ 2L2,	(59)
where (a) follows by letting ζ = ξoo in the first optimization problem, (b) follows by using the
fact that -λIζ - ξeoo I2 is non-positive for any ζ ∈ IRD and can be dropped, and (c) follows from
Assumption (A.2).
Now, plugging the upper bound in equation 59 into equation 58 yields
lT(Zi,zj )(ξ1, ∙∙∙ , ξ0 ,…，ξN ) - T(Zi,zj )(ξ0,…，ξ0,…，ξN )l ≤ Nj--
From McDiarmid’s Martingale inequality McDiarmid (1989) and the union bound, we obtain that
IP (∪zi,zj∈zITzi,zj)阁，…，ξN)i ≥ °)≤(4diam(X))d ∙(—N),	(6o)
for all λ ≥ 0. Now, consider arbitrary points (x, xb) ∈ X × X. Let the center of the balls containing
those points be Zi, Zj ∈ Z, i.e., x ∈ IBε(Zi) and xb ∈ IBε(Zj) for some Zi, Zj ∈ Z. Then, by the
triangle inequality, we have that
|T(X,b)(ξ0,…，ξN) - T(Zi,zj)(ξo,…，ξN)|
≤ |T(X,b) (ξ0,…，ξN) - T(Zi,b) (ξo,…，ξN)|
+ Tzi,b)(ξ1,…，ξN)- Tzi ,Zj )(ξ0,…，ξN )1
≤ kVχT(t,b)(ξ°,…，ξN)∣∣2∣∣χ - Zik2 + INbT(t,b)(ξ°,…，ξN)k2kb- Zjk2
≤ 2LT ,	(61)
where LT = Lt(ξ1, ∙∙∙ ,ξN) =f supχ,χ∈χ ∣ΝχTX,χ)(ξ1，…，ξN)k2 is the Lipschitz constant
of the mapping T. Note that the Lipschitz constant LT is a random variable with respect to the
random feature samples ξo,…，ξN. Let (x*，b*) = argsupχ,χ∈χ kVχTX,x)(ξ0,…，ξN)∣2∙
xxiv
Under review as a conference paper at ICLR 2020
We compute an upper bound on the second moment of the random variable LT as follows
1Eμo [LT] = 1Eμo
[MTU 同)闹,…，ξN )k2i

1 ɪ	ɪ	,	1	1
N X vχM¾χ*%(b*;.)(Ek)- JJM々x
■*
r)Rb*;.)(E)μ00ξJ
≤
N X VxM-K(χ*r)Rb*r)(ξk)[
k=1
N IEμo	X Vx
N2
k=1
2
M⅛χ*,)Hb*,)(ξk)]
叫∙o
/
IRD
VxMix*；,）Rb*；.）（£〃0（d£）『

—
N
We further proceed using the triangle inequality as well as the basic inequality (aι + a2 + …+
aN)2 ≤ N(a2 + a2 + •…+ aN),
IEμo [LT] = N IEμo
X VxM 专(χ *r)Hx*I)(ξk )∣2j
2
≤ N IEμo
1
VxMs(X*;.”*;.)
2
2
1N
≤ N X
k=1
以0	||VxMi(x*;.)W(x* ;.)
(62)
To proceed from equation 62, we leverage the following lemma:
Lemma C.8. (MOREAU’ S ENVELOP OF PARAMETRIC FUNCTIONS) Consider the parametric
function f : X × Θ → IR and the associated Moreau’s envelope for a given θ ∈ Θ ⊂ IRd:
Mf(沏(X)= inf 2∖~^x kx — yk2 + f (y； θ)∖.
y∈X 2β
Furthermore, define the proximal operator as follows
PrOXeGθ)(X) = arg inf gkχ — yk2 + f(y;θ)].
y∈X 2β
Then, Moreau’s envelope has the following upper bound
∣∣vθ Mβ(∙jθ)(χ)∣∣2 ≤ ∣∣vθ f(proxe(.；e)(x)； θ)∣∣2.
(63)
(64)
(65)
The proof is presented in Appendix D.2.
Equipped with Inequality equation 65 of Lemma C.8, we now compute an upper bound on the right
hand side of equation 62 as follows
1N
IΕμo [LT ] ≤ REIE“0 Mb* ； ξ)∣2 ∙kV∕(x*; ξk)k2] ≤ L4,
k=1
(66)
where the last inequality is due to (A.2).
xxv
Under review as a conference paper at ICLR 2020
Invoking Markov’s inequality now yields
ip (∣T(‰)(ξ1,…,ξ0) - mg)阁,…，ξN)1 ≥ s)
(67)
Now, using the union bound, for every arbitrary pair of data points (x, xb) ∈ X × X the following
inequality holds
ip (∣T(‰)(ξ0,…，ξ0)1 ≥ δ) ≤ ip (IT杳/。)间,…，ξN)1 ≥ s/2)
+ ip (1篇㈤阁,…,ξ0) - A")阁，…,ξ0)I≥ s/2)
(68)
≤ ( f )2 L4 + (…『(-爷).
Following the proposal of Rahimi & Recht (2008), We select E = (κ1∕κ2)d+2, where κ1
,	2Nλδ2	def	C ,
(4diam(X))d ∙ e 2L2λ+L4 and κ2 = (2∕δ)2L4. Then,
IP(SupX* 阁,…，ξN )l≥δ) ≤28 (Ldiam≡ j
(-NF j
I L2(d +2)广
Thus, with the probability of at least 1 - %, the following inequality holds
λ≥pXsupXITλx,χ)(ξ0, …,ξN)l≤
(L2(d +2) W (28Ndiam2(X)))2
(69)
where W(∙) is the Lambert W-function.4 Since W(X) ≤ ln(x) for x > e, we can rewrite the upper
bound in terms of elementary functions
l ʌ “1 N'	/L2(d +2) 1 ι (28Ndiam2(X)、
SUPsUP lT(χ,bg)(ξ0,…，ξ0 )| ≤ √ —丙-ln 2 --------------------- ,	(7O)
λ≥0 x,xb∈X	,	N	%
provided that N is sufficiently large and/or % is sufficiently small so that2 Ndiam (X) ≥ e. Plugging
%
Inequality equation 70 in equation 57 now results in the following inequality
∣supM∖Dμ[Pv,Pw] - sup MMMDbN[Pv,Pw]∣ ≤ rL2(d +2) ln2 (28Ndiam2(x)) , (71)
1 μ∈P	μN ∈Pn	K	IVN	∖	%	)
for all W ∈ W. Employingequation 53 from Lemma C.7 now yields the following upper bound
A2 = min sup M\MD [PV , PW ] - min sup M\MDbN PV , PW
2	I W ∈W μ∈P	μ V , W	W ∈W μN ∈PN	μ V , W
/L2(d + 2)	ι (28Ndiam2(X)
N N n I %
(72)
4Recall that the lambert W -function is the inverse of the function f (W) = WeW.
xxvi
Under review as a conference paper at ICLR 2020
Upper bound on A3 :
Recall that the solution of the empirical risk function of equation 14 is denoted by
(WN, μN) =f arg wmiw arg ∕nfz MMDaN [Pv ,Pw ]
8
arg min arg SUp ---------
W∈w	μN∈Pn n(n — 1)
E yyIEbN∈Pn [以xi； ξ)dxj;ξ)]
1≤< ≤n
—
8
n(nɪiɑ	N "； ξ)-(xj；ξ)]).
We also define the solution of the empirical kernel alignment as follows
(WN ,μN) def arg min arg sup MMDbN [Pv ,Pw]
W ∈w	μN ∈Ρn	'
8
nτnɪn	E	ViVj IEbbN [P(Xi；ξ)夕(Xj；ξ)].
n(n - 1) 1≤i<j≤n
(73)
(74)
Due to the optimality of the empirical measure μN for the inner optimization in equation 73, the
following inequality holds
αα
MMDbN [Pv，PWN] ≤ MMDbN[Pv，PWN]
8
S n(n — 1)	ViVjIEbN [P(Xi； ξ)夕(Xj； ξ)]∙	(75)
Upon expansion of MMDbN [Pv ,PwN ], and after rearranging the terms in equation 75, We arrive
◊	*
at
MMDbbN [Pv,PWN] — MMDbN [Pv,PWN]
8
=n(n-D	Σ ViVj (IEμN [P(Xi； ξ)夕(Xj； ξ)] — IEμN [P(Xi； ξ)夕(Xj ； ξ)])
1≤i<j≤n
8
n(n — 1)α
≤
X	(IEμN [Ψ(χi； EW(Xj； ξ)])2
1≤i<j ≤n
8L4
≤----,
α
(76)
where the last inequality is due to the fact that k夕k∞ < L by (A.1). Now, due to optimality of WN
for the outer optimization problem in equation 74, We have
MMDbN [Pv,PWN] ≤ MMDbN [Pv,PWN].
Putting together Inequalities equation 76 and equation 77 yields
MMDbN [Pv，PWn ] — MMDbN [Pv, PWN ] ≤ 8L4.
(77)
(78)
Similarly, due to the optimality of the empirical measure μN for the optimization in equation 74 we
have that
MMDbN hPv, PWN i ≤ MMDbN hPv, PWN i
≤ MMDbN hPv ,PWN i∙	(79)
xxvii
Under review as a conference paper at ICLR 2020
Combining equation 78 and equation 79 then yields
A3 = ∣M∖DbN [Pv, PWN] - MMMDbN [Pv, PWN] ∣ ≤ 8L4.
(80)
Upper bound on A4:
The upper bound on A4 can be obtained exactly the same way as A1. Specifically, from equation 52
it follows directly that
A4 = ∣MMMDμN [Pv, PWN i - MMDμN〔Pv ,Pw J ∣
≤ ʌsup ∣ MMMDbN [Pv ,PWn i - MMDbNhPV ,PW J ∣	(81)
3 3 4 X 2 ɪ X L2 1 1
≤ max < ----------ln2
n
(82)
Now, plugging the derived upper bounds in A1 -A4 in equation 38 and employing the union bound
completes the proof.
C.2 Proof of Theorem 4.2
The proof has three main ingredients and follows the standard procedure in the literature, see, e.g.,
Wang et al. (2017); Luo & Mattingly (2017). In the first step, we identify the mean-field limit of the
particle SGD in equation 15. In the second step, we prove the convergence of the measured-valued
process {(μN)o≤t≤τ} to the mean-field solution by establishing the pre-compactness of Sokhorhod
space. Lastly, we prove the uniqueness of the mean-field solution of the particle SGD.
Step 1-Identification of the scaling limit: First, we identify the weak limit of converging sub-
sequences via the action of the empirical measure bN(ξ) = N PN=I δ(ξ 一 ξk1) on a test function
f ∈ Cb3(IRD). In particular, we use the standard techniques of computing the scaling limits from
Luo & Mattingly (2017).
Recall that the action of an empirical measure on a bounded function is defined as follows
N
hf,bNi =f N X f(ξN).
(83)
k=1
We analyze the evolution of the empirical measure μNι via its action on a test function f ∈ C3 (IRD).
Using Taylor’s expansion, we obtain
hf,bN+ιi-hf,bmi = hf,μN+1i-hf,VN+1
1N
=N Ef 磊+ι)- /跋)
k=1
1N
=N EVf (ξ3(ξm+ι-ξ3T+RN.
k=1
where RN is a remainder term defined as follows
m
N
RN =f N X(ξN+ι - ξN)T v2f (ξk )(ξN+ι - ξN),
k=1
(84)
xxviii
Under review as a conference paper at ICLR 2020
where ek =f (ek(1),…冷(p)), and e⑶ ∈ 琰⑶,ξm +ι(i)], for i = 1, 2,…，p.
Plugging the difference term (ξmk +1 - ξmk ) from the SGD equation in equation 15 results in
hf,bN+ιi-hf,bNi	(85)
X 夕(Xm； ξm )夕(em； ξm ) - αym,em,) ▽{(夕(Xm； ξkn )中(em； ξk, )^ + RN.
Now, we define the drift and Martingale terms as follows
k=1
Dm =f N- //	(〈夕(x,ξ)中(e,ξ),bNi- aye)	(86a)
α X×Y
× hyf(ξ)(4(X;ξ)Vξ^(χ; ξ) + 中(x；ξ)Vξ^(e;ξ)),bNidPX,2((χ,y), (e,e))
MN =f ?(〈夕(Xm,ξ)中(em,ξ),bNi - aymem)	(86b)
Nα
× hVf(ξ)(夕(em； ξ)Vξ 夕(Xm； ξ) + 中(em； ξ)Vξ 夕(Xm； ξ)),bNi — Dm .
respectively. Using the definitions of DmN and MmN in equation 86a-equation 86b, we recast Equation
equation 85 as follows
hf,bΝ+ιi-hf,bm i = DN + MN + Rm.	(87)
Summation over ' = 0,1, 2 …，m - 1 and using the telescopic sum yields
m-1	m-1	m-1
hf, bNi-hf, bN i = X DN + X MN + X RN.	(88)
'=0	'=0	'=0
We also define the following continuous embedding of the drift, martingale, and the remainder terms
as follows
bNtc
DNA= X DN	(89a)
2=0
bNtc
MN =f X MN	(89b)
2=0
bNtc
RN =f X RN,	t ∈ (0,T].	(89c)
2=0
The scaled empirical measure μN =f b^Nt」then can be written as follows
hf,μN i-hf,μN i = DN + MN + RN.	(90)
Since the drift process (DN)o≤t≤τ is a piecewise CadIag process, We have
R[μs]ds,
(91)
where the functional R[μs] is defined as follows
R[μs]
=f η jj
α X×Y
(Q(X,ξ)^(e, ξ),μsi - aye
(92)
X(▽/(ξ)(P(e；ξ)Vξ^(χ;ξ) + 夕(x；ξ)Vξ夕(e；ξ))T,μs>pχ,y((dχ,de), (dy,de)).
xxix
Under review as a conference paper at ICLR 2020
Therefore, the expression in equation 90 can be rewritten as follows
hf,μN i-hf,μN i
Zt
0
R[μs]ds + MN + RN.
(93)
In the following lemma, we prove that the remainder term sup0≤t≤T |RtN | vanishes in probabilistic
sense as the number of particles tends to infinity N → ∞:
Lemma C.9. (LARGE N -LIMIT OF THE REMAINDER PROCESS) Consider the remainder process
(RtN)0≤t≤T defined via scaling in equation 84-equation 89c. Then, there exists a constant C0 > 0
such that
sup |RtN |
0≤t≤T
(94)
and thus lim supN→∞ sup0≤t≤T |RtN | = 0.
Proof. The proof is relegated to Appendix D.5.
□
We can also prove a similar result for the process defined by the remainder term:
Lemma C.10. (LARGE N-LIMIT OF THE MARTINGALE PROCESS) Consider the Martingale
process (MtN)0≤t≤T defined via scaling in equation 86b-equation 89b. Then, for some constant
C1 > 0, the following inequality holds
IP S SUp MN| ≥ ε) ≤ -^4√2L2PbNTjηC1(L2 + α)2.	(95)
0≤t≤T	Nαε
In particular, with the probability of at least 1 - ρ, we have
SUp MN| ≤ -^4√2L2PbNTCηC1(L2 + α)2.	(96)
0≤t≤T	Nαρ
and thus lim SUpN→∞ SUp0≤t≤T |MtN | = 0 almost surely.
Proof. The proof is deferred to Appendix D.6.	□
Now, using the results of Lemmata C.10 and C.9 in conjunction with equation 93 yields the following
mean-field equation as N → ∞,
hμt,fi = hμo,fi + η Z (ZZ	(Q(X, ξ)中(x,ξ),μsi - αyy)	(97)
α 0	X×Y
× hVf (ξ)(φ(Xe; ξ)Vξ^(x; ξ) + g(x; ξ)Vξ‹Xe; ξ)), μsiPX,2((dx, dX), (dy, dy)∕jds.
Notice that he mean-field equation in equation 97 is in the weak form. When the Lebesgue density
Pt(ξ) = dμt∕dξ exists, the McKean-Vlasov PDE in equation 22 can be readily obtained from
equation 97.
Step 2: Pre-compactness of the Skorkhod space: To establish our results in this part of the proof,
we need a definition and a theorem:
xxx
Under review as a conference paper at ICLR 2020
Definition C.11. (TIGHTNESS) A set A of probability measures on a metric space S is tight if there
exists a compact subset S0 ⊂ S such that
ν(S0) ≥ 1 - ε, for all ν ∈ A,	(98)
for all ε > 0. A sequence {XN}N∈IN of random elements of the metric space S is tight if there
exists a compact subset S0 ⊂ S such that
ν(XN ∈ S0) > 1 - ε,	(99)
for all ε > 0, and all N ∈ IN.
Now, to show the tightness of the measured valued process (μN)o≤t≤τ, We must verify
Jakubowski’s criterion (Jakubowski, 1986, Thm. 1):
Theorem C.12. (JAKUBOWSKI’ S CRITERION (JAKUBOWSKI, 1986, THM. 1)) A sequence of
measured-valued process {(ζtN)0≤t≤T}N ∈IN is tight in DM(IRD)([0, T]) if and only if the following
two conditions are satisfied:
(J.1) For each T > 0 and γ > 0, there exists a compact set UT,γ such that
lim infIP (ZN ∈ Uτ,γ,∀t ∈ (0, T]) > 1 一 γ.	(100)
N→∞	,
This condition is referred to as the compact-containment condition.
(J.2) There exists a family H of real-valued functions H : M(IRD) 7→ IR that separates
points in M(IRD) and is closed under addition such that for every H ∈ H, the sequence
{(H (ξtN))0≤t≤T}N ∈IN is tight in DIR([0, T]).
To establish (J1), we closely follow the proof of (Giesecke et al., 2013, Lemma 6.1.). In particular,
for each L > 0, we define SL = [0, B]p. Then, SB ⊂ IRp is compact, and for each t ≥ 0, and
N ∈ IN, we have
iE[μN ("/SB)]
=	1N nn X 1P (kξ}tC k2 ≥ B)	(101)
(a) ≤	1 X IE[kξkNtc k2] N乙	B k=1	(102)
(b) ≤	co + ηαL2T + 2ηL4T TJ	,	(103)
where (a) follows from Markov’s inequality, and (b) follows from the upper bound on the norm of
the particles in equation 177 of Appendix D. We now define the following set
Ub = {μ ∈ MCRp) : μ(IRp∕S(B+j)2) < √B^ for all j ∈ In} .	(104)
xxxi
Under review as a conference paper at ICLR 2020
We let Uτ,γ = UB, where UB is the completion of the set UB. By definition, Uτ,7 is a compact
subset of M(IRD). Now, we have
∞
IP (μN ∈Uτ,γ) ≤ X IP μN (IRp∕S(B+j)2) >
j=1
√B+j
∞
≤X
j=1
IE[μN (IRp∕S(B+j)2)]
1∕√B+j
V I^X co + ηL2T + 2(η∕α)L4T
≤ j=	(B + j)2∕√B+j
'^X co + ηL2T + 2(η∕α)L4T
j=	(B + j)3/2
Now, since
lim X co+ ηL2T + 2Wa)L4T =0,
B→∞ 乙^	(B + j)3/2
this implies that for any γ > 0, there exists a B > 0, such that
lim inf IP (μN ∈ Ub, ∀t ∈ (0, T]) > 1 — γ.
N→∞	t
(105)
(106)
(107)
This completes the proof of (J.1). To verify (J.2), we consider the following class of functions
H =f {H : ∃f ∈ C3(IRD) such that H(μ) = hμ, f i, ∀μ ∈ M(IRD)}.	(108)
By definition, every function H ∈ H is continuous with respect to the weak topology of M(IRD)
and further the class of functions H separate points in M(IRD) and is closed under addition. Now,
we state the following sufficient conditions to establish (J.2). The statement of the theorem is due
to (Robert, 2013, Thm. C.9):
Theorem C.13. (TIGHTNESS IN DIR([0, T]), (ROBERT, 2013, THM.	C.9)) A sequence
{(ZtN)o≤t≤T }N∈IN is tight in DIR ([0, T]) iff for any δ > 0, we have
(T.1) There exists > 0, such that
IP(|ZoN| > ) ≤ δ,	(109)
for all N ∈ IN.
(T.2) For any ρ > 0, there exists σ > 0 such that
IP	sup	|ZtN1 —ZtN2| >σ ≤δ,	(110)
∖t1,t2≤T,lt1-t2l≤P	f
This completes the tightness proof of the of the laws of the measured-valued process
{(μN)o≤t≤τ}n∈in. Now, We verify the condition (J.2) by showing that the sufficient conditions
(Τ.1) and (T.2) hold for function values {(H(μN))o≤t≤τ}n∈n, where H ∈ H and H is defined in
Eq. equation 108. Now, condition (T.1) is readily verified since
H(μN) = hμN,fi = Z皿El f (ξ)μN(dξ)	(iii)
≤kfk∞∕RD μN(dξ)	(112)
≤ b,	(113)
xxxii
Under review as a conference paper at ICLR 2020
where in the last step, we used the fact that f ∈ C3(RD),andhence, kf k∞ ≤ b. Thus, IP(H(μN) ≥
b) = 0 for all N ∈ IN, and the condition (T.1) is satisfied. Now, consider the condition (T.2). From
Equation equation 93, and with 0 ≤ t1 < t2 ≤ T we have
|H (μN)- H (μN )1 = lhf,μN i-hf,μN il
t2
∣R[μs]∣ ds + |MN -MN| + RN -RN|.	(114)
t1
To bound the first term, recall the definition of R[μs] from equation 92. The following chain of
inequalities holds,
∣R[μs]∣ ≤ ηiEpχ,y [|〈夕(χ,ξ)夕(e,ξ),μsi- αye∣lh^f(ξ)(夕(X; ξ)Vξ^(χ; ξ) + 中(x； ξ)Vξ^(X; ξ))T,4$〉|]
≤ ηIEP名2[(l〈夕(χ,ξ)夕(X,ξ),μsil + α)lhvf(ξ)(夕(e;ξ)Vξ夕(χ; ξ) + 中(χ;ξ)Vξ夕(e;ξ))τ,μsi∣].
(115)
Let I : IRD → IR, I(ξ) = 1 denotes the identity function. Notice that(I, μS = JRD μs(ds) = 1.
From equation 115, we proceed as follows
∣R[μs]∣ ≤ ηIEP®2[(k夕k∞ ∙∣hi,μsil + α) ∙ kvf(ξ)(夕(e; ξ)Vξ夕(χ; ξ) + 中(χ; ξ)Vξ夕(e; ξ))Tk∞ ∙∣hi,μsi∣]
≤ ηIEP®2 [(k夕k∞ + α) ∙kVf (ξ)(夕(X; ξ)Vξ夕(x; ξ) + 中(x; ξ)Vξ夕(e; ξ))Tk∞]
≤ 2η (L2 + a)L2C1,	(116)
where the last inequality is due to (A.1). Therefore,
t2
/	∣R[μs]∣ds ≤ S0∣t2 — tι∣,
t1
(117)
where so =f — (L2 + ɑ)L2Cι.
α
Consider the middle term of equation 114. Using the definition of the martingale term in equa-
tion 89b, we obtain that
bNt1c	bNt2c
MN-MNI = X MN - X MN
'=0	'=0
bNt2c
≤ X	M'N .
'=bNt1c
(118)
In Equation of Section D, we have proved the following concentration bound
N2α2ε2
IP(IMNl≥ ε) ≤ 2 (-8mL4rfc2(L 2 +α)2 )	∀m ∈ [0，NT] ∩ N (119)
Now, recall the alternative definition of the sub-Gaussian random variables:
Definition C.14. (SUB-GAUSSIAN RANDOM VARIABLES BOUCHERON ET AL. (2013)) A ran-
dom variable X is σ2-sub-Gaussian if
λ2σ2
IE[(λ(X - IEX]))] ≤ (-2-).	(120)
xxxiii
Under review as a conference paper at ICLR 2020
We enumerate a few standard consequences of sub-Gaussianity Boucheron et al. (2013). If Xi
are independent and σi2 -sub-Gaussian, then Pin=1 Xi is Pin=1 σi2 -sub-Gaussian. Moreover, X is
σ2-sub-Gaussian if and only if
IP(|X - IE[X]| ≥ε) ≤
(121)
Now, it is clear from equation 119 andthat MmN is sub-Gaussian random variable with a zero mean,
2	4mL4η2C12 (L2 + α)2	bNt2c	N
and With the parameter σ* = --------2------------. Therefore, T'= [Nt1 C MN is SUb-GaUssian
with the parameter σ2(t1,t2) =f 2 η CJL2 + &) (bNtιC - bNt2C + 1)(bNtιC + bNt2C). Con-
N2 α2
seqUently, from IneqUality eqUation 118 and the concentration ineqUality in eqUation 121, we have
IP	sup	|MtN1 - MtN2 | ≥ ε
∖t1,t2≤τ,lt1-t2l≤P
≤ IP	sup
∖ t1 ,t2≤T, | t1 -12 l≤P
bNt2c
X	MN
'=[ΝtιC
≥ε
≤2
/I [Nt2C
IP ( I X MN
'l'=[N埒」
ε2
σ2(tι,t2)
≥ε
α2ε2
≤ 2 - 4L4η2C2(L2 + α)2(ρ +1)T),
(122)
(123)
(124)
(125)
—
where 因,超) =f argsuPt1,t2≤τ,∣t1-t2∣≤ρ1PbNtNtIc M'v∣.
We first compUte a boUnd for the last term of eqUation 114 Using the definition of the scaled term
RtN from eqUation 89c. We have
|RtN1 - RtN2 | =
bNt1c	bNt2c
X RN - X RN
'=0	'=0
bNt2 c	II
X R'
'=bNtιc
bNt2c
≤ X	∣R'I
'=[Ntιc
(a)	C
≤ lbNt2c - bNticlN(ηL + (LVa))
(b)
≤ si |t2 - ti |,
(126)
def
where (a) follows from the Upper boUnd in eqUation 178 of Section D, and in (b) we define si =
-N0 (ηL2 +(L4/a)).
xxxiv
Under review as a conference paper at ICLR 2020
Putting together equation 117, equation 122, and equation 126, we conclude from Inequality equa-
tion 114 that
ip( SuP	|H(μN)- H(μN)| ≥ σ) ≤ IP( SuP IMN -MN| + (so + sι)ρ ≥ σ)
'tl,t2≤T,∣tl-t2∣≤P	t	'tl,t2≤T,∣tl-t2∣≤P
≤ 2 (—	α29 - (SO + SI)P)2	ʌ
≤ V 4L4η2C2(L2 + α)2(ρ + 1)T) .
Therefore, condition (T.2) is also satisfied. Since the sufficient conditions (T.1) and (T.2) are sat-
isfied, the condition (J.2) is satisfied. This completes the tightness proof of the measured-valued
sequence {μN}n尔.
Now, we prove its convergence to a mean-field solution (μ*)o≤t≤τ.
Theorem C.15. (PROKHOROV’ S THEOREM PROKHOROV (1956)) A subset of probability mea-
sures on a complete separable metric space is tight if and only if it is pre-compact.
According to Theorem C.15, the tightness of the Skorkhod Space DM(IRD) ([0, T]) implies its pre-
compactness which in turn implies the existence of a converging sub-sequence {(μN)0≤t≤τ}Nk of
{μN}n∈n . Notice that {(μN)0≤t≤τ}弗 is a stochastic process defined on the Skorkhod space.
Therefore, let ∏Nk denotes the law of the converging sub-sequence {(μN)0≤t≤τ}Nk. By definition,
πNk is an element of the measure space M(D[0,T] (M(IRD))). In the sequel, we closely follow the
argument of (Wang et al., 2017, Proposition 4) to show that the limiting measure π∞ is a Dirac’s
delta function concentrated at a mean-field solution μJ= ∈ D[o,τ] (M (IRD)). We define the following
functional
Ft : D[0,T] (M(IRD)) → IR,
μt → Ft [μt]
hμt, f i - hμo, f i -
R R[μs]ds .
0
(127)
We compute the expectation of the functional Ft with respect to πNk. We then have
IEnNk [Ft (M)]=叫 Ft[μN]]
=IE	hμNk /-("N/- / t R[μNk ]ds|.j .
(128)
Now, from Equation equation 93, we have that
hμNk,fi-hμNk,fi- Zt R[μNk]ds = MNk + RNk
0
Plugging equation 129 in equation 128 gives
IEnNk [Ft(M)] = IE[Ft[μNk ]]
(129)
IE
≤ IE
hMtNk + RtNki
h sup |MtNk|i +IEh sup |RtNk|i
0≤t≤T	0≤t≤T
J--√2LLpbNNCηc ηC1(L2 + α)2 + CT (ηα^2rT + 2ηL4 T),
Nαρ	N
(130)
where the last equality is due to the bounds in equation 94 and equation 95 of Lemmata C.9 and
C.10, respectively. Taking the limit of N → ∞ from equation 130 yields
Iim IEnNk [∣Ft[μ]∣]=0.
Nk →∞
(131)
xxxv
Under review as a conference paper at ICLR 2020
It can be shown that the functional FtH is continuous and bounded. Therefore, due the weak Con-
vergence of the sequence {πNk }Nk∈IN to π∞, equation 131 implies that
IE∏∞[|Ft(M)|] = 0.	(132)
Since the identity equation 132 holds for all bounded test functions f ∈ Cb3(IRD) and for all t ∈
(0,T], it follows that ∏∞ is a Dirac,s delta function concentrated at a solution (μt)o≤t≤τ of the
mean-field equation.
Step 3: Uniqueness of a mean-field solution: Before we establish the uniqueness result we make
two remarks:
First, we make it clear that from the compact-containment condition (J.1) of Jakubowski’s criterion
in Theorem C.12, the support of the measured-valued process (μN)o≤t≤τ = (b(Ntj )o≤t≤τ is com-
pact for all 0 ≤ t ≤ T. Moreover, in Step 2 of the proof, we established that the measure valued
process (μN)o≤t≤τ converges weakly to a mean-field solution as the number of particles tends to
infinity (i.e., N → ∞). Thus, all the possible solutions of the mean-field equation also have com-
pact supports. Let Ξb ⊂ IRD denotes a compact set containing the supports of all such solutions at
0 ≤ t ≤ T. In the sequel, it suffices to establish the uniqueness of the mean-field solution for the
test functions with a compact domain, i.e., let f ∈ Cb3 (Ξb).
Second, for all bounded continuous test functions f ∈ C3(Ξ), the operator f → hμt, f〉is a linear
operator with μt(IRD) = 1. Hence, from Riesz-Markov-Kakutani representation theorem Rudin
(1987); Varadarajan (1958) by assuming μt ∈ M(IRD) , existence of unique operator implies
f → hf, μti implies the existence of the unique probability measure μt. Now, We equip the measure
space M(IRD) with the following norm
kμk
def sup lhf, μil
f∈c3(Ξ) kfk∞
kfk∞6=0
(133)
Given an initial measure μo, we next prove that there exists at most one mean-field model solution
by showing that there exists at most one real valued process hμt, f〉corresponding to the mean-field
model. Suppose (μj,1)o≤t≤τ, (μj,2)o≤t≤τ are two solutions satisfying the mean-field equations
equation 97 with the initial distributions μ1, μ2 ∈ M(IRD), respectively. For any test function
f ∈ Cb3(Ξb) we have that
h〃；J - μ产,fi = hμ0 - μ0,f i + η J： (ZZχ Ja(x, ξW(X, ξ),忠, - μ*s吟-ɑyy)
(134)
× hvf(ξ)(VξWe; ξ^(χ;ξ)))T, μs,1 - 〃；,2)p肾((dz,d力ds.
We bound the first term on the right side of Equation equation 134 as follows
hμ1 - μ2,fi ≤ kμ1 - μ0k ∙ kfk∞	(135)
≤ bkμθ - μ0k,	(136)
where used the definition of the norm ∣∣ ∙ k on the measure space M(IRD) from equation 133.
xxxvi
Under review as a conference paper at ICLR 2020
Furthermore, let
ZZ	αyehVf (ξ)(Vξ (φ(Xe; ξ)φ(x; ξ)))T, 〃；1 - 〃；,2iP肾(d(x, y), d(X,y))
X×Y
≤ ZZ	α∣ye∣∙∣hVf(ξ)(Vξ " ξ)φ(x; ξ)))T ,〃；」-〃；,2)厚2 (d(x,y), d(S,y))
X×Y
≤ ak〃;J-〃;,2k / kVf(ξ)(VξW(e;ξ3(χ;ξ)))TkPX>2(dx,de)
X
≤ ak〃；，1- 〃；,2k / kVf(ξ)k∞ ∙kVξ以e；ξW(χ;ξ)k∞pf2(dx,de)
X
≤ αL2Cιkμ↑,1 - μ^,2k,
(137)
where in the last inequality, we used the fact that kVf (ξ)k ≤ C1 since the test function is three-
times continuously differentiable f ∈ Cb3(Ξb) on a compact support.
Similarly, we have
Z h<Aχ, ξ)ψ(Xe,ξ),μS,1 - K,2ihVf(ξ)(Vξψ(χ, ξ)ψ(χ,ξ)),μs,1 - κ,2iPf2(dx,de)
X
≤ k〃；,1- 〃；,2k2 I k^(x, ξ)ψ(Xe, ξ)k∞ kVf(ξ)(Vξ 以 x, ξ3(x, ξ))Tk∞P詈(dx, de)
X
≤ L4Cιkμ:,1 - μ^,2k2.	(138)
Putting together the inequalities in equation 136,equation 137, and equation 138 yield
hμt,1 - μt,2, fi ≤ bkμ0 - μ0Il + L2CιηZ kμs,1 - μS,2kds + n a ι Z kμs,1 - μs,2k2ds. (139)
The above inequality holds for all bounded functions f ∈ Cb3(Ξb). Thus, by taking the supremum
with respect to f we obtain
l∣μt,1 -μt,2k =	SUp hμt,1 -μt,2,fi
f∈Cb3(Ξb)
≤ bkμ0 - μ0k + LCIn/ llμs,1 - μs,2kds+	o1~
(140)
Z tkK,1-K,2k2ds.
0
(141)
Now, we employ the following result which generalizes Gronewall’s inequality when higher order
terms are involved:
Lemma C.16. (EXTENDED GRONEWALL’ S INEQUALITY, (WEBB, 2018, THM 2.1.)) Letp ∈ IN
and suppose that for a.e. t ∈ [0, T], u ∈ L+∞ [0, T] satisfies
Ut ≤ co(t) + / (cι(s)us + C2(s)u2 +-----+ Cp+ι(s)up+1)ds,
0
where co ∈ L∞[0, T] is non-decreasing, and Cj ∈ L∣[0, T] for j ∈ {1,…,p +1}. Then, if
Z cj+1(S)Ujds ≤ Mj, j ∈{1,2,…，p}.
0
It follows that for a.e. t ∈ [0, T]
(142)
(143)
Ut
(M1 + …+ Mp).
(144)
xxxvii
Under review as a conference paper at ICLR 2020
We now apply the extended GroneWall's Inequality equation 142 with P = 1, c0(t) = b∣∣μ0 -
μ0 k, 0 ≤ t ≤ T, ci(t) = ηL2Cι, 0 ≤ t ≤ T, c2 ⑴=ηLcC1,0 ≤ t ≤ T, and us = 1也,1-μ"2 k. In
this case, it is easy to see that Mi = 2bnTL"1. Consequently, from equation 140 and equation 144,
we obtain that
∣∣μt,1—μt,2k ≤ bkμ0—μok ∙ QL2Cit +	ɪɑ	1),	0 ≤ t ≤ T.	(145)
Thus, starting from an initial measure μ1 = μ0 = μo, there exists at most one solution for the
mean-field model equations equation 97.
C.3 Proof of Corollary 4.2. 1
To establish the proof, we recall from equation 88 that
m-i	m-i	m-i
hf, bNi-hf, bN i = X DN + X MN + X RN,	(i46)
'=0	'=0	'=0
for all f ∈ Cb(IRD). Recall the definition of the total variation distance TV(∙, ∙) on a metric space
(X,d).
TV(μ, V) = 1 sup ∣μ(A) - ν(A)∣.	(147)
2 A⊂X
The total variation distance admits the following variational form
TV(μ,ν) =	sup	hf, μi - hf, Vi.	(148)
f ：kf k∞≤ 2
Now, using the variation form and by taking the supremum of equation 146 with respect to the
functions from the function class Fc d=ef {f ∈ Ci/2(IRD)}, we obtain the following upper bound on
the TV distance
τv(bN,bN) ≤
m-i	m-i	m-i
2 X DNI + 2 X IMNI + 2 X |RN|.
乙2=0	乙2=0	乙'=0
(149)
Based on the upper bound equation 178 on the remainder term, we have
IRN| ≤ N2 (ηL2+2αL4),	` ∈ [0,m - 1],	(15O)
for some constant C0 > 0. Moreover, from the concentration inequality equation 189, we also have
that with the probability of at least 1 - δ, the following inequality holds
IMNI ≤
8 √⅞L2C1(L2 + a)
Na
(151)
Lastly, recall the definition of the drift term in equation 86a. By carrying out a similar bounding
method leading to equation 116, it can be shown that
IDN i ≤ η(- (L2+α)L2Cι.	(152)
Nα
By plugging equation 150, equation 151, and equation 152 into equation 149, we derive that
MbN ,bN) ≤ 笔 © + 2 L4) + …N-+ a) log (f) + N (L2 + α)L2 G,
(153)
with the probability of 1 - δ. We now leverage the following lemma:
xxxviii
Under review as a conference paper at ICLR 2020
Lemma C.17. (BOUNDED EQUIVALENCE OF THE WASSERSTEIN AND TOTAL VARIATION DIS-
TANCES, SINGH & P6czos (2018)) Suppose (X, d) is a metric space, and suppose μ and V are
Borel probability measures on X with countable support; i.e., there exists a countable set X0 ⊆ X
such that μ(X0) = V(X0) = 1. Then, for any P ≥ 1, we have
SeP(X0)(2TV(μ,ν)) 1 ≤ Wp(μ,ν) ≤ Diam(X0)(2TV(μ,ν)) 1,
where Diam(X0) d=ef supx,y∈X 0 d(x, y), and Sep(X0) d=ef inf x6=y ∈X 0 d(x, y).
(154)
Consider the metric space (IRD, ∣ ∙ ∣2). Note that the empirical measures μNι, bN have a countable
support X0 = {ξmk }kN=1 ∪ {ξ0k }kN=1 ⊂ IRD. Therefore, using the upper bounds in equation 154 of
Lemma C.17 and 153, we conclude that when the step-size is of the order
η=O
Rp
(155)
then Wp(bm,bN) ≤ R for all m ∈ [0, NT] ∩ IN.
D Proofs of Auxiliary Results
D.1 Proof of Lemma C.5
The upper bound follows trivially by letting X = y in the optimization problem equation 44.
Now, consider the lower bound. Define the function g : [0, 1] → IR, t 7→ g(t) = f(y + t(X - y)).
Then, when f is differentiable, we have g0 (t) = hX - y, Vf(y + t(X - y))i. In addition, g(0) =
f(y), and g(1) = f (X). Based on the basic identity g(1) = g(0) + R01 g0(S)dS, we derive
f(X) = f(y) + Z hX - y, Vf(y + S(X - y))idS
0
≥ f(y) - ∣χ — y∣2 Z l∣Vf(y + S(X — y))∣2ds,
0
(156)
where the last step is due to the Cauchy-Schwarz inequality. Using Inequality equation 156 yields
the following lower bound on Moreau’s envelope
Mny) ≥	f(y)	+	inf	[ɪIlx — yk2	- kχ — y∣2 Z	l∣Vf(y +S(X	— y))k2dsl
x∈X 2β	0
≥ f(y) -
(a)
≥ f(y) -
f(y) + inf
x∈X
∣x - y ∣2
lVf (y + S(X - y))l2dS!
l∣Vf(y + S(X - y))k2ds
β SuP (Z l∣Vf(y + s(x — y))∣∣2ds]
2 x∈X	0
β SuP Z IlVf(V +S(X — y))k2ds
2 x∈X 0
≥fV)- 2/1 XuXkVf (y+S(X - y))k2ds,
where (a) is due to Jensen’s inequality.
(157)
xxxix
Under review as a conference paper at ICLR 2020
D.2 Proof of Lemma C.8
Let u ∈ IRd denotes an arbitrary unit vector kuk2 = 1. From the definition of the gradient of a
function, we have that
(VθMfGe)⑺,UE = Du[MfGθ)(x)],	(158)
where Du[M〃.；e)(x)] is the directional derivative
Du[Mf(∙a(x)] =f lim MfGθ+δu)(x) - MfGeNx .	(159)
δ→0	δ
We now have
Mfgθ+δu)(X) = inf ʃ ɪ kχ — yk2 + f (y; θ + δU)J
x∈X 2β
=inf ∖ 13 kχ — yk2 + f (y; θ) + δhvef (y; θ), ui] + O(δ2)
x∈X 2β
(a) 1
≤ 2βkχ — Proxf(•；e)(x)k2 + f(Proχf(∙a⑺;θ)
+ δhVef (ProXf(∙a(x); θ), Ui + O(δ2),
(160)
where the inequality in (a) follows by letting y = Proxf(.；e)(x) in the optimization problem. Now,
recall that
MAM(X) = if 昌 kx - yk2 + f(y； θ)],
y∈X 2β
PrOXfGθ)(X) = argmiX ∣2βkχ — yk2 + f(y;θ)}.
Therefore,
MfGθ)(X) = 2βkx — Proxf(•；e)(x)k2 + f (PrOXf(.⑻(X)； θ).	(161)
Substitution of equation 161 in equation 160 yields
MfGθ+δu)(X) ≤ Mf(U)(X) + δhVef (ProXf(∙a(X); θ), Ui + O(δ2).	(162)
Hence, Du[Mf(.；e)(X)] ≤(Vef (ProXf(.；e)(X); θ), u〉. From equation 158 and by using Cauchy-
Schwarz inequality, we compute the following bound on the inner product of the gradient with the
unit vectors U ∈ IRd , kUk2 = 1,
(VeMfGθ)(X), U ≤ ∣∣Vef(ProXf(.⑻(x); θ)k2 ∙ ∣∣u∣∣2 = ∣∣Vef (ProXfge)(X); θ)k2. (163)
Since the preceding upper bound holds for all the unit vectors U ∈ IRd, we let U
to get Inequality equation 65.
▽e Mβ(rθ)(X)
gθMβ(1θ)(x)k2
D.3 Proof of Lemma C.6
Let z ∈ Sd-1 denotes an arbitrary vector on the sphere. Define the random variable
Qz ((yi , Xi),…，(yn, Xn)) = hz, VEn(ξ)i
=n(n1- 1) X ViVj (夕(Xi； ξ)hz, Vξ夕(Xj； ξ)i + 中(Xj； ξ)hz,峰夕8；ξ)i)
i6=j
-IEpX,y [yb(中(Xi； ξ)hz,峰夕(Xj； ξ)i + 中(Xj； ξ)hz, Vξ^(Xi; ξ)i)].
,	(164)
xl
Under review as a conference paper at ICLR 2020
Clearly, IEPx,y [Qz] = 0. Now, let (ybm, xbm) ∈ Y × X, 1 ≤ m ≤ n. By repeated application of the
triangle inequality, we obtain that
Qz((y1, XI), ∙∙∙ , (ym, Xm), ∙∙∙ , (yn, Xn)) - Qz((y1, XI),…，，(ym, Xm),…，，(yn, Xn))
≤	( — ]) I X yi夕(Xi； ξ)hz,ymyξ夕(Xm； ξ) - UmVξW(Xm； ξ)R
n(n - 1) i6=m
1
n(n - 1)
+
Eyihz, Vξw(Xi; ξ)i(ymW(Xm; ξ) - XmW(Xm； ξ))
i6=m
≤	( - 1) X |w(Xi； ξ)l ∙ I∣zk2 ∙ ∣∣ymVξW(Xm; ξ) - XmVξW(Xm； ξ)∣∣2
n(n - 1) i6=m
+ n(n- 1) X l∣z∣2 ∙ kVξW(X3 ξ)∣2 ∙ |ymW(Xm； ξ) - XmW(Xm； ξ)l
4L2
≤---,
n
(165)
where the last inequality is due to assumption (A.2) and the fact that IzI2 = 1 for z ∈ Sd-1. In
particular, to derive Inequality equation 165, we employed the following upper bounds
lw(Xi；ξ)l ≤ L,
IymW(Xm； ξ) - XmW(Xm； ξ) | ≤ |W(Xm； ξ) | + IW(Xm； ξ) | ≤ 2L,
kvξW(Xi； ξ)k2 ≤ L,
Iym Vξ W(Xm ； ξ) - yXm Vξ W(XXm ； ξ)I2 ≤ IVξW(Xm ； ξ)I2 + IVξ W(XXm ； ξ)I2 ≤ 2L.
Using McDiarmid Martingale’s inequality McDiarmid (1989) then gives us
IP (∣Qz((Xl, X1),…，(Xn, Xn))I ≥ U) ≤ 2 (-InL4) ,	(166)
for x ≥ 0. Now, for every p ∈ IN, the 2p-th moment of the random variable Qz is given by
IE [Qzp((yι, Xi), 一
• , (yn, Xn))i = J 2pu P IP(Qz((X1, x1 ), ∙ ∙ ∙ , (yn, Xn)) ≥ u)du
(≤a) Z	4pu2p-1
IR+
= 2(16L4/n)2pp!,
—
i⅛4)du
(167)
where (a) is due to the concentration bound in equation 166. Now Therefore,
∞1
IE[(Qz ((yi, Xi),…,(Xn, Xn))∕γ2)] = X pγ2p IE [φzp((χi, Xi),…,(Xn, Xn))]
1+2 Xf 必)2p
nγ
p∈IN
2
----;---rτ-777 - 1.
1 — (16L4 ∕nγ)2
For Y = 16√3L4∕n, we obtain IE[(Qz((yι, Xi),…，(yn Xny)/τ2)] ≤ 2. Therefore, ∣Qz|星=
khz, VEn(ξ)ikψ2 ≤ 16√3L4∕n for all Z ∈ Sn-i and ξ ∈ IRD. Consequently, by the definition
xli
Under review as a conference paper at ICLR 2020
of the SUb-GaUssian random vector in equation 35 of Definition C.2, We have ∣∣VEn(ξ)kψ2 ≤
16√3L4∕n for every ξ ∈ IRD. We invoke the following lemma proved by the first author in
(KhUzani & Li, 2017, Lemma 16):
Lemma D.1. (THE ORLICZ NORM OF THE S QUARED VECTOR NORMS, (KHUZANI & LI, 2017,
LEMMA 16)) Consider the zero-mean random vector Z satisfying ∣Z ∣ψν ≤ β for every ν ≥ 0.
Then, k∣Z∣2kψV ≤ 2 ∙ 32 ∙ β2.	“
Using Lemma D.1, we now have that ∣∣VEn(ξ)∣22 ∣ψ1 ≤ 4608L4/n2 for every ξ ∈ IRD. Applying
the exponential Chebyshev’s inequality with β = 4608L4/n2 yields
IP
-n2δ
≤ e 4608L4 IEχ,y
∣VEn((1 - s)ξ + sζ*)k2 - IEχ,y[∣VEn((1 - s)ξ + sζ*)k2]∣μo(dξ) ≥ δ
e (4608L4 RIRD R01
∣∣VEn((1-s)ξ+sζ* )k2-IEχ,y [kVEn((1-s)ξ+sζ* )k2] ∣ dsμ0 (dξ))
(a)	— n2δ4 /	1 π7 Γ n2 4 (∣∣REn((1-s)ξ+sZ*)k2-Ex VEn((1-s)ξ + sZ*)k2])∣]χ	/只占、
≤ e 4608l4 / I IEx y e4608l4 vι 11 n`` 八 ∣ s*7ll2 x,yL∣∣ n — 八 ∣ 、*“2」"Jdsμo(dξ)
(b)	n2δ
≤ 2e 4608L4 ,
where (a) follows by Jensen’s inequality, and (b) follows from the fact that
IExyhe46n2L4 (lkVEn((1-s)ξ+sZ*)k2-IEχ,y[kVEn((1-s)ξ+sZ*)k2])|i ≤ 2
due to Definition C.1. Therefore,
IP IL AlVEn((I -s)ξ + sζ*)k2ds”(dξ) ≥ J
n2(δ - Ro1 JRD IEx,y[kVEn((I - S)ξ + SZJk2]ds〃0(dE))
≤2 -
4608L4
(168)
(169)
It now remains to compute an upper bound on the expectation IEx,y [∣VEn ((1 一 s)ξ + sZ*) k 2]. But
this readily follows from equation 167 by letting P = 1 and Z = ∣ 竟n(((1l-)ξ++Z*))∣ ? as follows
IEx,y [∣VEn((1 - s)ξ + sZ*)k2] = IEx,y [( u；"1 - S))ξ:	, VEn((I - s)ξ + sZ*)^
[∖ k VEn((I - s)ξ + sC*川2	/
=1Ex,y Q ▽En((1-s)ξ + sZ*)
L ∣∣VEn((1-s)ξ + sZ*)k2 -∣
L8
≤ 2-.	(170)
n2
Plugging the expectation upper bound of equation 170 into equation 169 completes the proof of the
first part of Lemma C.6.
The second part of Lemma C.6 follows by a similar approach and we thus omit its proof.
xlii
Under review as a conference paper at ICLR 2020
D.4 Proof of Lemma C.7
Let W* def argminw∈w Ψ(W) and W^ def argminw∈w Φ(W).
Φ( W) | ≤ δ for all W ∈ W, we have that
Then, since ∣Ψ(W)-
∣Ψ(W*) - Φ(W*)| = 朋％Ψ(W) - Φ(W*) ≤ δ.
Therefore,
min Φ(W) ≤ Φ(W*) ≤ min Ψ(W) +δ.
W∈W	*	W∈W
Similarly, it can be shown that
min Ψ(W) ≤ Ψ(W) ≤ min Φ(W) +δ.
W∈W	W∈W
Combining equation 172 and equation 173 yields the desired inequality.
(171)
(172)
(173)
D.5 Proof of Lemma C.10
We recall the expression of the remainder term {RNm}0≤m≤NT from equation 84. For each 0 ≤
m ≤ NT, N ∈ IN, we can bound the absolute value of the remainder term as follows
1N
IRmI = NN X(ξm+ι -ξm)v2f(ek+1 -gmʃ
k=1
1NI	I
≤ nn χ∣(ξm+ι - ξm )v2f (ξk )跋+1 -gmʃl
k=1
≤
1N
NN X kξm+ι-ξm k2 ∙Mf(ek )∣∣f.
k=1
(174)
Next, we characterize a bound on the difference term kξmk +1 - ξmk k2 . To attain this goal, we use the
iterations of the particle SGD in Equation equation 15. We have that
kξmk +1 - ξmk k2
≤ NN Il Iymym
1N
一 Na X ^(Xm； em)8(Xm； ξm) I Vξ (W(Xm； £3)以图皿；ξm)
α k=1
2
≤ NN 1ymem | (| 夕(Xm； ξK )| ∙ ∣∣Vξ 夕(em； ξkm)∣∣2 + | 夕(em； ξK )∣∙ ∣∣Vξ 夕(Xm； ξkm ) ∣∣ 2
N
+ NN (Nα Xh
k=1
(a) ηL2	2ηL4
≤ ɪ + Na ,
k
| 中(Xm； ξm )∣kVξ^(em; ξm )k2 + 3(Xm； ξk1 )|||峰夕(况恒；ξn )∣∣2
(175)
where in (a), We used the fact that ∣3∣∞ < L and ∣∣Vξ^(x, ξ)k2 < L due to (A.1), and ym, ym, ∈
{-1, 1}. Plugging the last inequality in equation 175 yields
|RmN|
N
+	X ∣V2f(ξk)∣F.
k=1
(176)
xliii
Under review as a conference paper at ICLR 2020
We next compute an upper bound on the FrobenioUs norm of the Hessian matrix V2f (ξk). To this
end, We first show that there exists a compact set C ⊂ IRD such that ξk1 ∈ C for all k = 1,2,…，N
and all m ∈ [0,NT ] ∩ IN. For each k = 1,2,∙∙∙ ,N, from Inequality equation 175 we obtain that
kξm k2 ≤kξm-1k2+ηL2+2N04
=kξk k2+¥+2Nf
≤kξk k2 + ηL2T + 2(η∕α)L4T.	(177)
Now, kξk ∣∣2 < c0 for some constant c0 > 0 since the initial samples ξ1,…，ξN are drawn from the
measure μo whose support SuPPort(μ0) = Ξ is assumed to be compact by (A.3). From upper bound
in equation 177, it thus follows that ∣ξmk ∣2 < C for some constant C > 0, for all m ∈ [0, NT] ∩IN.
Now, recall that ξk = (ξk ⑴，•二,ξk (p)), where ξk(i) ∈ 归m(i),ξm +ι (i)],i = 1, 2, ∙∙∙,m + 1,
for i = 1,2,…，p. Therefore, ξk ∈ C. Since all the test function f ∈ C3(IR3) are three-times
continuously differentiable, it follows that there exists a constant C0 d=ef C0(T) > 0 such that
11 X—79 C / J^∖ 11	, /-« I -	T	1 ∙ .	. ∙	I r /	∙ . CIl	. 1
suPξe∈C ∣V2f (ξ)∣F < C0. From Inequality equation 176, it follows that
∣Rm∣ ≤ N (ηL2 + 2ηα4) , m ∈ [0, NT] ∩ IN.
(178)
Now, recall the definition of the scaled term RtN from equation 89c. Using the Inequality equa-
tion 178 as well as the definition of RtN , we obtain
suP |RtN |
0≤t≤T
(179)
D.6 Proof of Lemma C.10
Let Fm-1 = σ((xk, yk)0≤k≤m-1, (xek, yek)0≤k≤m-1) denotes the σ-algebra generated by the sam-
ples up to time m 一 1. We define F-ι =f 0. Further, define the following random variable
△m def (〈夕(Xm, ξ)夕@m, ξ),bmi - αymVm) ×(Vf (ξ)Vξ(2@m； ξ)夕(Xm； £)),力普〉.(180)
Notice that 焉IE[∆N ∣Fm-ι] = Dm. We now rewrite the martingale term in equation 86b in term of
△N
m,
m
MN =f nτ X(∆m - IE[∆m ∣F'-ι]),
m Nα
2=0
(181)
with M0N = 0.
By construction of MmN in equation 181, it is a Martingale IE[MmN |Fm-1] = MmN-1. We now prove
that MmN has also bounded difference. To do so, we define the shorthand notations
am =f (ψ(Xm, ξ)ψ(Xm, ξ), bmmi - aymVm,	(182)
bm =f (Vf (ξ)(Vξ(φ(Xem; ξ)φ(xm; ξ)))T,μmi.	(183)
xliv
Under review as a conference paper at ICLR 2020
Then, we compute
∣mN - Mm-1∣ = Nηα∣∆N -叫∆m∣Fm-ι]∣
≤ ηr- @m|+^n- IEsNiiFm-ι]
Nα m Nα m
≤ Nnα IaNHbmI + Nnα IEljamHbN||Fm-ι].	(184)
For the difference terms, we derive that
IaNI = ∣h^(xm, ξ)中(Xm, ξ),bNi - aymym∣
1N
≤ NEIiXxm,ξmW(Xm,Em)I + a|ymym|
k=1
≤ L2 + α,	(185)
where the last step follows from the fact that kik∞ ≤ L due to (A.1). Similarly, we obtain that
IbmI = IhVf (ξ)(Vξ(i(em; ξ)i(xm; ξ)))T ,bmii
1N
≤ N∑Ii(em;ξm)I∙ ∣Vf(ξm)(Vξi(xm;ξm))T∣
1N
+ N ∑ Ii(xm;ξm)I∙ IVf(ξm)(Vξφ(xm;ξm))TI
(a)	L N	L N
≤ N X ∣Vf(ξm)(Vξi(xm;ξm))T∣ + NX IVf(ξm)(Vξψ(xm;ξm))TI
k=1	k=1
(b)	L N	L N
≤ N X kVf(ξm)k2 ∙kVξ i(xem; ξm)k2 + N X kVf (ξm)k2 ∙kVξ i(xem; ξm)k2
k=1	k=1
(c)	2L2 N
≤ IN EkVf (ξm)k2,	(186)
k=1
where (a) and (c) follows from (A.1), and (b) follows from the Cauchy-Schwarz inequality. From
Inequality equation 177 and the ensuing disucssion in Appendix D.5, we recall that kξmk k2 < C for
some constant and for all m ∈ [0, NT] ∩ IN, and k = 1,2, ∙∙∙ ,N. For the two times continuously
test function f ∈ Cb3(IRD), it then follows that IVf (ξmk )k2 ≤ C1 for some constant C1 > 0. The
following bound can now be computed from equation 186,
IbmI ≤ 2CNL2.	(187)
Plugging the upper bounds on IaNmI and IbNmI from equation 185-equation 187 into equation 184 we
obtain that
IMN—MN-II ≤ Na L2 (L2+α).	(188)
Thus, (MmN)m∈[0,N T]∩IN is a Martingale process with bounded difference. From the Azuma-
Hoeffding inequality it follows that
N2α2ε2
IP(IMN I ≥ ε) = IP(IMN	-	MN I≥ ε)	≤ 2 --8mL4n2C2(L 2	+	a)2 ),	∀m	∈	[0,	NT ]	∩ IN.
(189)
xlv
Under review as a conference paper at ICLR 2020
Therefore, since MtN = MbNNtc , we have
IP(MN| ≥ ε) ≤ 2 (-8L4bNτCη2c2(L2 + α)2 ) .
Then,
IE
hMN"
IP(|MTN | ≥ ε)dε
≤2
∞
0
—
N2α2ε2
8L4bNT Cη2C2(L2 + α)2
34√2L2 PNTTηCι (L2 + α)2.
Nα
where the inequality follows from equation 190.
By Doob’s Martingale inequality Doob (1953), the following inequality holds
IP ( Sup MNI ≥ ε) ≤ IEM
0≤t≤T	ε
≤ Nαε4√2L2PbNTCηC1(L2 + α)2.
In particular, with the probability of at least 1 - ρ, we have
Sup MN| ≤ -^4√2L2√bNTCηC1(L2 + α)2.
0≤t≤T	Nαρ
(190)
(191)
(192)
(193)
(194)
E Chaoticity and propagation of chaos in particle SGD
In this appendix, we establish the so called ‘propagation of chaos’ property of particle SGD. We
now establish the so called ‘propagation of chaos’ property of particle SGD. At a high level, the
propagation of chaos means that when the number of samples {ξk}kN=1 tends to infinity (N → +∞),
their dynamics are decoupled.
Definition E.1. (EXCHANGABLITY) Let ν be a probability measure on a Polish space S and. For
N ∈ IN, we Say that V包N is an exchangeable probability measure on the product space Sn if it is
invariant under the permutation π = (π(1), .…，π(N)) of indices. In particular,
户N(π ∙ B) = v0N(B),	(195)
for all Borel subsets B ∈ B(Sn).
An interpretation of the exchangablity condition equation 195 can be provided via De Finetti’s rep-
resentation theorem which states that the joint distribution of an infinitely exchangeable sequence
of random variables is as if a random parameter were drawn from some distribution and then the
random variables in question were independent and identically distributed, conditioned on that pa-
rameter.
Next, we review the mathematical definition of chaoticity, as well as the propagation of chaos in the
product measure spaces:
Definition E.2. (CHAOTICITY) Suppose ν0N is exchangeable. Then, the sequence {ν0N}N∈IN is
v-chaotic if, for any natural number ' ∈ IN and any test function f1,f2,…，fk ∈ C)(S), we have
Nim (∏ fk (Sk ),v 0N (ds1,…，dsN η = YYhfk ,V i	(196)
→∞ k=1	k=1
xlvi
Under review as a conference paper at ICLR 2020
According to equation 196 of Definition E.2, a sequence of probability measures on the product
spaces S is ν-chaotic if, for fixed k the joint probability measures for the first k coordinates tend
to the product measure V(ds1)ν(ds2) ∙∙∙ ν(dsk) = ν0k on Sk. If the measures V包N are thought
of as giving the joint distribution of N particles residing in the space S, then {ν0N} is V-Chaotic if
k particles out of N become more and more independent as N tends to infinity, and each particles
distribution tends to V. A sequence of symmetric probability measures on SN is chaotic if it is
V-chaotic for some probability measure V on S.
If a Markov process on SN begins in a random state with the distribution V0N, the distribution of
the state after t seconds of Markovian random motion can be expressed in terms of the transition
function KN for the Markov process. The distribution at time t > 0 is the probability measure
UtNV0N is defined by the kernel
UtNV0N(B) d=ef	KN(s,B,t)V0N(ds).
Definition E.3. (PROPOGATION OF CHAOS) A sequence functions
nKN(s,B,t)o
N∈IN
(197)
(198)
whose N-th term is a Markov transition function on SN that satisfies the permutation condition
KN(s, B, t)	=	KN(π ∙ s, π ∙	B, t),	(199)
propagates chaos if whenever {V0N}N∈IN	is	chaotic, so is	{UtN}	for any t ≥ 0,	where UtN is
defined in equation 197.
We note that for finite systems size N,the states of the particles are not independent of each other.
However, as we prove in the following result, in the limiting system N → +∞, the particles are
mutually independent. This phenomena is known as the propagation of chaos (a.k.a. asymptotic
independence):
Theorem E.4. (CHAOTICITY IN PARTICLE SGD) Consider Assumptions (A.1) - (A.3). Further-
more, suppose that {ξk }ι≤k≤N 〜i.i.d. μo is exchangable in the sense that the joint law is invariant
under the permutation of indices. Then, at each time instant t ∈ (0, T], the scaled empirical measure
μN ∈ M(IRD) defined via scaling
μN(dξ1,…，dξN) =f bfNtc(dξ1,…，dξN) = IP{ξiNtc ∈ dξ1,…，ξNNtc ∈ dξN},	(200)
is μ^-chaotic, where μJ= is mean-field solution of equation 97.
Proof. To establish the proof, it suffices to show that for every integer ` ∈ IN, and for all the test
functions fι,…，fk ∈ C3(RD), We have
lim sup IE
N→∞
`
Y fk(ξbkNtc )
k=1
`
-Y hμ= ,fki = 0.
k=1
(201)
Using the triangle inequality, We noW have that
``
IE Y fk(ξkNtc) - Yhμ=,fki
k=1	k=1
`
≤ IE YhbN,fki
k=1
`
-Yhμ=,fki + IE
k=1
`
YhbN ,fk i
k=1
`
- IE Y fk(ξbkNtc )
k=1
(202)
xlvii
Under review as a conference paper at ICLR 2020
For the first term on the right side of equation 202 we have
lim sup IE
N→∞
∏hbN,fki -∏h区,fki
k=1
k=1
(a)
≤ lim sup IE
N→∞
``
YhbN ,fk i-Yhμt,fk i
k=1	k=1
`
`
(b)
≤ IE
lim sup
N→∞
``
YhbN ,fk i-Yhμt,fk
k=1	k=1
(c)
≤ b'-1IE
`
Xlim sup IhbN,fki - hμ>t,fki∣
N→∞
---I
(=d) 0,	(203)
where (a) is by Jensen’s inequality, (b) is by Fatou’s lemma, (c) follows from the basic inequality
I QN=I a，i - QN=I bi∣ ≤ PN=IIai- bi| for |ai|, |bi| ≤ 1,i = 1,2,…，N, as well as the fact that
hμ↑, fk) ≤ b and hμN, fk〉≤ b for all k = 1, 2,∙∙∙,N due to the boundedness of the test functions
fι, ∙.. , f` ∈ Cb3(IRD), and (d) follows from the weak convergence μN w→kly μt to the mean-field
solution equation 97.
Now, consider the second term on the right hand side of equation 202. Due to the exchangeability
of the initial states (ξ0k)1≤k≤N, the law of the random variables (ξ0k)1≤k≤N is also exchangeable.
Therefore, we obtain that
"'	H	`!	Γ	'	一
IE Y fk(ξkNtc) = NTIE	X Y fk(ξ∏Ntc) ,	(204)
_k = 1	_l	π∈Π(',N) k = 1
where Π(', N) is the set of all permutations of ' numbers selected from {1, 2,…，N}. Notice that
the right hand side of equation 204 is the symmetrized version of the left hand side equation 205.
Further, by the definition of the empirical measure μbtN we obtain that
`
IE YhμbtN,fki
k=1
`
Y
k=1
fk(ξbmNtc)
X (Yfk(ξ∏Nktc))∣.
∕∈Π(',N) ∖k=1	)_|
(205)
(206)
Therefore, subtracting equation 204 and equation 205 yields
`
IE YhμbtN,fki
k=1
- IE
Y fk(ξfNtc)J≤ b'
(207)
Hence,
lim sup IIE
N→∞ I
'
YhμbtN,fki
k=1
-IE Y' fk(ξbkNtc) IIII =0.
(208)
Combining equation 203-equation 208 yields the desired result.
xlviii