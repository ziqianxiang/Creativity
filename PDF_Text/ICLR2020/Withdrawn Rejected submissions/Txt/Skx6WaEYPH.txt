Under review as a conference paper at ICLR 2020
Bandlimiting Neural Networks Against
Adversarial Attacks
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we study the adversarial attack and defence problem in deep learning
from the perspective of Fourier analysis. We first explicitly compute the Fourier
transform of deep ReLU neural networks and show that there exist decaying but
non-zero high frequency components in the Fourier spectrum of neural networks.
We then demonstrate that the vulnerability of neural networks towards adversarial
samples can be attributed to these insignificant but non-zero high frequency compo-
nents. Based on this analysis, we propose to use a simple post-averaging technique
to smooth out these high frequency components to improve the robustness of neural
networks against adversarial attacks. Experimental results on the ImageNet and the
CIFAR-10 datasets have shown that our proposed method is universally effective
to defend many existing adversarial attacking methods proposed in the literature,
including FGSM, PGD, DeepFool and C&W attacks. Our post-averaging method is
simple since it does not require any re-training, and meanwhile it can successfully
defend over 80-96% of the adversarial samples generated by these methods without
introducing significant performance degradation (less than 2%) on the original
clean images.
1	Introduction
Although deep neural networks (DNN) have shown to be powerful in many machine learning tasks,
Szegedy et al. (2013) found that they are vulnerable to adversarial samples. Adversarial samples are
subtly altered inputs that can fool the trained model to produce erroneous outputs. They are more
commonly seen in image classification task and typically the perturbations to the original images are
so small that they are imperceptible to human eye.
Research in adversarial attacks and defences is highly active in recent years. In the attack side, many
attacking methods have been proposed (Szegedy et al., 2013; Goodfellow et al., 2014; Papernot
et al., 2016a; Papernot et al., 2017; Moosavi-Dezfooli et al., 2016; Kurakin et al., 2016; Madry et al.,
2017; Carlini and Wagner, 2017a; Chen et al., 2017; Alzantot et al., 2018; Brendel et al., 2017),
with various ways to generate effective adversarial samples to circumvent new proposed defence
methods. However, since different attacks usually are effective to different defences or datasets,
there is no consensus on which attack is the strongest. Hence for the sake of simplicity, in this
work, we will evaluate our proposed defence approach against four popular attacks for empirical
analysis. In the defence side, various defence mechanisms have also been proposed, including
adversarial training (Rozsa et al., 2016; Kurakin et al., 2016; Tramer et al., 2017; Madry et al., 2017),
network distillation (Papernot et al., 2016b), gradient masking (Nguyen and Sinha, 2017), adversarial
detection (Feinman et al., 2017) and adding modifications to neural networks (Xie et al., 2017).
Nonetheless, many of them were quickly defeated by new types of attacks (Carlini and Wagner,
2016; 2017b;c;a; Athalye et al., 2018; Athalye and Carlini, 2018; Alzantot et al., 2018). Madry et al.
(2017) tried to provide a theoretical security guarantee for adversarial training by a min-max loss
formulation, but the difficulties in non-convex optimization and in finding the ultimate adversarial
samples for training may loosen this robustness guarantee. As a result, so far there is no defence that
is universally robust to all adversarial attacks.
Along the line of researches, there were also investigations into the properties and existence of
adversarial samples. Szegedy et al. (2013) first observed the transferability of adversarial samples
across models trained with different hyper-parameters and across different training sets. They also
1
Under review as a conference paper at ICLR 2020
attributed the adversarial samples to the low-probability blind spots in the manifold. In (Goodfellow
et al., 2014), the authors explained adversarial samples as ”a result of models being too linear, rather
than too nonlinear.” In (Papernot et al., 2016), the authors showed the transferability occurs across
models with different structures and even different machine learning techniques in addition to neural
networks. In summary, the general existence and transferability of adversarial samples are well
known but the reason of adversarial vulnerability still needs further investigation.
Generally speaking, when we view neural network as a multivariate function f (x) of input x,
if a small imperceptible perturbation ∆x leads to a huge fluctuation ∆f (x), the large quantity
∆f (x)/∆x essentially corresponds to high frequency components in the Fourier spectrum of f (x).
In this paper, we will start with the Fourier analysis of neural networks and elucidate why there
always exist some decaying but nonzero high frequency response components in neural networks.
Based on this analysis, we show that neural networks are inherently vulnerable to adversarial samples
due to the underlying model structure. Next, we propose a simple post-averaging method to tackle
this problem. Our proposed method is fairly simple since it works as a post-processing stage of any
given neural network models and it does not require re-training the networks at all. Furthermore, we
have evaluated the post-averaging method against four popular adversarial attacking methods and our
method is shown to be universally effective in defending all examined attacks. Experimental results
on the ImageNet and the CIFAR-10 datasets have shown that our simple post-averaging method can
successfully defend over 80-96% of the adversarial samples generated by these attacks with little
performance degradation (less than 2%) on the original clean images.
2	Fourier analysis of neural networks
In order to understand the behaviour of adversarial samples, it is essential to find the Fourier transform
of neural networks. Fortunately, for some widely used neural networks, namely fully-connected
neural networks using ReLU activation functions, we may explicitly derive their Fourier transform
under some minor conditions. As we will show, these theoretical results will shed light on how
adversarial samples happen in neural networks.
2.1	Fourier transform of fully-connected ReLU neural networks
As we know, any fully-connected ReLU neural networks (prior to the softmax layer) essentially form
piece-wise linear functions in input space. Due to space limit, we will only present the main results
in this section and the proofs and more details may be found in Appendix.
Definition 2.1. A piece-wise linear function is a continuous function f : Rn -→ R such that there
are some hyperplanes passing through origin and dividing Rn into M pairwise disjoint regions Rm,
(m = 1, 2, ..., M), on each of which f is linear:
{wι ∙ X X ∈ Ri
W2 ∙ X X ∈ R2
...
WM ∙ X X ∈ RM
Lemma 2.2. Composition of a piece-wise linear function with a ReLU activation function is also a
piece-wise linear function.
Theorem 2.3. The output of any hidden unit in an unbiased fully-connected ReLU neural network is
a piece-wise linear function.
This is straightforward because the input to any hidden node is a linear combination of piece-wise
linear functions and this input is composed with the ReLU activation function to yield the output,
which is also piece-wise linear. However, each region Rm is the intersection of a different number of
half-spaces, enclosed by various hyperplanes in Rn. In general, these regions Rm (m = 1, •…，M)
do not have simple shapes. For the purpose of mathematical analysis, we need to decompose each
region into a union of some well-defined shapes having a uniform form, which is called infinite
simplex.
2
Under review as a conference paper at ICLR 2020
Definition 2.4. Let V = {v1, v2, ..., vn} be a set of n linearly independent vectors in Rn. An
infinite simplex, R+V, is defined as the region linearly spanned by V using only positive weights:
ɑkVk ak > 0, k = 1, 2,…，n}
(1)
Theorem 2.5. Each piece-wise linear function f (x) can be formulated as a summation of some
simpler functions: f(x) = PlL=1 fl (x), each of which is linear and non-zero only in an infinite
simplex as follows:
fl (x) = { wl∙ X
x∈R+Vl
otherwise
(2)
where Vl is a set of n linearly independent vectors, and wl is a weight vector.
In practice, we can always assume that the input to neural networks, x, is bounded. As a result, for
computational convenience, we may normalize all inputs x into the unit hyper-cube, Un = [0, 1]n.
Obviously, this assumption can be easily incorporated into the above analysis by multiplying each
fl(x) in eq.(2) by Qrn=1 h(xr)h(1 - xr) where h(x) is the Heaviside step function. Alternatively, we
may simplify this term by adding n2 additional hyperplanes to further split the input space to ensure
all the elements of x do not change signs within each region R+V . In this case, within each region
R+V , the largest absolute value among all elements of x is always achieved by a specific element,
which is denoted as rq . In other words, the dimension xrq achieves the largest absolute value inside
R+V . Similarly, the normalized piece-wise linear function may be represented as a summation of
some functions: f (x) = PQ=ι gq(x), where each gq(x) (q = 1, 2,…，Q) has the following form:
“ (χ) = ʃ Wq ∙ X h(I- Xrq ) X ∈ RVq
gq (x) = 0	otherwise
For every Vq, there exists an n × n invertible matrix Aq to linearly transform all vectors of Vq into
standard basis vectors ei in Rn. As a result, each function gq(x) may be represented in terms of
standard bases V* = {eι,…，en} as follows:
G(X)= ʃ Wq ∙ Xq h(I- 1 ∙ Xq) xq ∈ RV*
gq(x) = 0	otherwise
where Xq = XAT, and Wq = Wq A-1.
Lemma 2.6. Fourier transform of the following function:
may be presented as:
S(X) = I h(1-1 ∙ x)	x ∈ RV*
0	otherwise
n	-iωr
S(ω) = (√2∏)"X Q (ωr0-ωr)
r=0 r0 6=r
(3)
where ωr is the r -th component of frequency vector ω (r = 1, ∙∙∙ ,n), and ωo = 0.
Finally we derive the Fourier transform of fully-connected ReLU neural networks as follows.
Theorem 2.7. The Fourier transform of the output of any hidden node in a fully-connected unbi-
ased1 ReLU neural network may be represented as PQ=ι Wq A-1VS(ωA-1), where V denote the
differential operator.
1For mathematical convenience, we assume neural networks have no biases here. However, regular neural
networks with biases may be reformulated as unbiased ones by adding another dimension of constants. Thus,
the main results here are equally applicable to both cases. Note that regular neural networks with biases are used
in our experiments in this paper.
3
Under review as a conference paper at ICLR 2020
Figure 1: Illustration of input space divided into sub-regions by a biased neural network. The black
lines are the hyperplanes for the first network layer, while the blue lines are for the second layer and
the red lines are for the third layer. A small perturbation from point A to point B may possibly cross
many hyperplanes.
Obviously, neural networks are the so-called approximated bandlimited models as defined in (Jiang,
2019), which have decaying high frequency components in Fourier spectrum. Theorem 2.7 further
suggests that the matrices Aq-1 may contribute to the high frequency components when the corre-
sponding region R+V are too small. This is clear because the determinant of Aq is proportional to the
volume of R+V in Rn . In summary, the high frequency components of neural networks are mostly
attributed to these tiny regions in the input space. As we will show later, these small regions may be
explicitly exploited to generate adversarial samples for neural networks.
2.2	Understanding adversarial samples
As shown in Theorem 2.3, neural network may be viewed as a sequential division of the input
space into many small regions, as illustrated in Figure 1. Each layer is a further division of the
existing regions from the previous layers, with each region being divided differently. Hence a neural
network with multiple layers would result in a tremendous amount of sub-regions in the input space.
For example, when cutting an n-dimensional space using N hyperplanes, the maximum number
of regions may be computed as (N) + (N) + …+ (：). For a hidden layer of N = 1000 nodes
and input dimension is n = 200, the maximum number of regions is roughly equal to 10200. In
other words, even a middle-sized neural network can partition input space into a huge number of
sub-regions, which can easily exceed the total number of atoms in the universe. When we learn a
neural network, we can not expect there is at least one training sample inside each region. For those
regions that do not have any training sample, the resultant linear functions in them may be arbitrary
since they do not contribute to the training objective function at all. Of course, most of these regions
are extremely small in size. When we measure the expected loss function over the entire space, their
contributions are negligible since the chance for a randomly sampled point to fall into these tiny
regions is extremely small. However, adversarial attack is imposing a new challenge since adversarial
samples are not naturally sampled. Given that the total number of regions is huge, those tiny regions
are almost everywhere in the input space. For any data point in the input space, we almost surely can
find such a tiny region in proximity where the linear function is arbitrary. If a point inside this tiny
region is selected, the output of the neural network may be unexpected. We believe that these tiny
unlearned regions may be a major reason why neural networks are vulnerable to adversarial samples.
In layered deep neural networks, the linear functions in all regions are not totally independent. If we
use v(l) to denote the weight matrix in layer l, the resultant linear weight wk in eq.(2) is actually the
sum of all concatenated v(l) along all active paths. When we make a small perturbation ∆x to any
input x, the fluctuation in the output of any hidden node can be approximated represented as:
∆f(x) Y N ∙ Y E h∣v(j) I]	⑷
l
where N denotes the total number of hyperplanes to be crossed when moving x to x + ∆x. In
any practical neural network, we normally have at least tens of thousands of hyperplanes crossing
the hypercube Un = [0, 1]n. In other words, for any input x in a high-dimensional space, a small
perturbation can always easily cross a large number of hyperplanes to enter a tiny unlearned region.
When N is fairly large, the above equation indicates that the output of a neural network can still
fluctuate dramatically even after all weight vectors are regularized by L1 or L2 norm. As a reference,
4
Under review as a conference paper at ICLR 2020
we have verified this on some ImageNet data using a VGG16 model. When PGD is used to generate
adversarial samples with average perturbation ∣∣∆x∣∣2 ≤ 0.35, which is extremely small perturbation
since x has over a hundred thousand dimensions on ImageNet, we have observed that in average
about N = 5278 hyperplanes are crossed per layer even after such a small perturbation is added.
At last, since the ubiquitous existence of unlearned tiny regions is an intrinsic property of neural
networks given its current model structure, we believe that adversarial training strategies will not
be sufficient to completely get rid of adversarial samples. In principle, neural networks must be
strictly bandlimited to filter out those decaying high frequency components in order to completely
eliminate all adversarial samples. We definitely need more research efforts to figure out how to do
this effectively and efficiently for neural networks.
3	The proposed defence approach: post-averaging
3.1	Post-averaging
In this paper, we propose a simple post-processing method to smooth out those high frequency
components as much as possible, which relies on a simple idea similar to moving-average in one-
dimensional sequential data. Instead of generating prediction merely from one data point, we use the
averaged value within a small neighborhood around the data point, which is called post-averaging
here. Mathematically, the post-averaging is computed as an integral over a small neighborhood
centered at the input:
fC (X) = V1C【…LCfX- x0) dx0
(5)
where X is the input and f (X) represents the output of the neural network, and C denotes a small
neighborhood centered at the origin and VC denotes its volume. When we choose C to be an n-sphere
in Rn of radius r, we may simply derive the Fourier transform of fC (X) as follows:
FC (ω) = F (a)；[…[	e-iχ0∙ω dx0 = F (ω)
VC	x0∈C
Γ(n + 1) Jn (r∣ω∣)
π n (r∣ω∣)n
(6)
where Jn (∙) is the first kind Bessel function of order n/2. Since the Bessel functions, JV (ω), decay
with rate 1∕√ω as ∣ω∣ → ∞ (Watson, 1995), we have FC(ω)〜一F(*ι as ∣ω∣ → ∞. Therefore,
CD -ɪ
if r is chosen properly, the post-averaging operation can significantly bandlimit neural networks by
smoothing out high frequency components. Note that the similar ideas have been used in (Jiang et al.,
1999; Jiang and Lee, 2003) to improve robustness in speech recognition.
3.2	Sampling methods
However, it is intractable to compute the above integral for any meaningful neural network used in
practical applications. In this work, we propose to use a simple numerical method to approximate it.
For any input x, we select K points in the neighborhood C centered at x, i.e. {xι, x2,…，XK} , to
approximately compute the integral as
1K
fC(X) ≈ K ：PJ(Xk).	⑺
Obviously, in order to defend against adversarial samples, it is important to have samples outside
the current unlearned tiny region. In the following, we use a simple sampling method based on
directional vectors. To generate a relatively even set of samples for eq.(7), we first determine some
directional vectors V, and then move the input X along these directions using several step sizes within
the sphere of radius r:
x0 = x + λ∙ V	(8)
where λ = [± 3, ± 2r, ±r], and V is a selected unit-length directional vector. For each selected
direction, we generate six samples within C along both the positive and the negative directions to
ensure efficiency and even sampling. We use this implementation for the convenience to extend with
different types of sampling strategies.
5
Under review as a conference paper at ICLR 2020
We tried several direction sampling strategies, including using the directions towards the closest
region boundaries, and found that the simple random direction sampling gives the best performance.
In this sampling method, we fill the directional vectors with random numbers generated from a
standard normal distribution, and then normalize them to have unit length.
4	Experiments
In this section, we evaluate the above post-averaging method on defending against several popular
adversarial attacking methods.
4.1	Experimental setup
•	Dataset: We evaluated our method on both the ImageNet (Russakovsky et al., 2015) and
CIFAR-10 (Krizhevsky et al., 2009) datasets. Since our proposed post-averaging method
does not need to re-train neural networks, we do not need to use any training data in our
experiments.
For evaluation purpose, we use the validation set of the ImageNet dataset. The validation set
consists of 50000 images labelled into 1000 categories. For computational efficiency, we
randomly choose 5000 images from the ImageNet validation set and evaluate our model on
these 5000 images.
For the CIFAR-10 dataset, we use the full test set, which consists of 10000 images labelled
into 10 categories.
•	Target model: For model on ImageNet, we use a pre-trained ResNet-152 (He et al., 2016)
network that is available from PyTorch, while for CIFAR-10, we use a pre-trained ResNet-
110 network from Yerlan Idelbayev 2. In our experiments, we directly use these pre-trained
models without any modification.
•	Source of adversarial attacking methods: We use Foolbox (Rauber et al., 2017), an
open source tool box to generate adversarial samples using different adversarial attacking
methods. In this work, we tested our method against four popular attacking methods in
the literature: Fast Gradient Sign method (FGSM) (Goodfellow et al., 2014), Projected
Gradient Descent (PGD) method (Kurakin et al., 2016; Madry et al., 2017), DeepFool (DF)
attack method (Moosavi-Dezfooli et al., 2016) and Carlini & Wagner (C&W) L2 attack
method (Carlini and Wagner, 2017a). We used these attack methods in their default settings.
•	Threat model: In our experiments, we use an l∞ norm to constrain the allowed perturba-
tion distance.
4.2	Evaluation criteria
For each experiment, we define:
•	Clean set: The dataset that consists of the original images from ImageNet or CIFAR-10.
•	Attacked set: For every correctly classified image in the Clean set, if an adversarial sample
is successfully generated under the attacking criteria, the original sample is replaced with
the adversarial sample; if no adversarial sample is found, the original sample is kept in the
dataset. Meanwhile, all the misclassified images are kept in the dataset without any change.
Therefore the Attacked set has the same number of images as the clean set.
In our experiments, we evaluate the original network and the network defended by post-averaging on
both the Clean and the Attacked sets. The performance is measured in terms of :
•	Accuracy: number of correctly classified images over the whole dataset.
•	Defence rate: number of successfully defended adversarial samples over the total number
of adversarial samples in the Attacked set. By ”successfully defended”, it refers to the case
where an adversarial sample is correctly classified after the original model is defended by
the post-averaging approach.
2https://github.com/akamaster/pytorch_resnet_cifar10/tree/master/
pretrained_models
6
Under review as a conference paper at ICLR 2020
Figure 2: Defence rate of post-averaging when using different sampling radius r on ImageNet.
Table 1: Performance of post-averaging defending against different attacking methods ( = 8/255,
and K = 15)._________________________________________________________________________________
	Original Model			Defended by Post-Averaging			
		Top-1 Accuracy		Top-1 Accuracy		Defence	
attack, defence	Dataset	Clean	Attacked	Clean	Attacked	Rate	#Adv
FGSM, random(r=30)			0.0750	0.7734	0.7488	0.9426	3500
PGD, random(r=30)	ImageNet	0.7750	0.0004	0.7732	0.7606	0.9639	3873
DF, random(r=30)			0.0350	0.7730	0.7614	0.9639	3710
C&W, random(r=30)			0.0090	0.7734	0.7548	0.9554	3830
FGSM, random(r=6)			0.1816	0.9247	0.8022	0.8080	7552
PGD, random(r=6)	CIFAR-10	0.9368	0.0000	0.9255	0.8841	0.9330	9368
DF, random(r=6)			0.1968	0.9254	0.8626	0.8872	7413
C&W, random(r=6)			0.0322	0.9257	0.8902	0.9367	9046
4.3	Experimental results
Table 1 shows the performance of our defence approach against different attacking methods. In this
table, the samples for post-averaging are selected within an n-sphere of radius r as in eq.(8), with
K = 15 different directions. Thus results in a total of 15 × 2 × 3 + 1 = 91 samples (including the
input) for each input image to be used in eq.(7). Moreover, all the adversarial samples generated
are restricted to be within the perturbation range = 8/255. We show the top-1 accuracy of the
original model and the defended model on both the Clean and the Attacked set respectively, as well
as the defence rate of the defended model. Besides, we also show the number of adversarial samples
successfully generated by each attacking method in the last column.
From Table 1, we can see that our proposed defence approach is universally robust to all of the
attacking methods we have examined. It has achieved above 80-96% defence rates in all the
experiments with only a minor performance degradation in the Clean set (less than 2%). Especially on
the ImageNet dataset, our method is able to defend about 95% of the adversarial samples. However,
an interesting observation from the experimental results is that the defence rate in the CIFAR-10
dataset is lower than the usually more challenging ImageNet dataset. We think this may be because
data points are sparser in the ImageNet space than in the CIFAR-10 space, as ImageNet has a much
larger dimensionality.
Generally, using a larger sampling radius r can increase the chance of moving out of the unlearned
regions as we desired, but it will also introduce more noise that can harm the prediction accuracy;
On the other hand, using a smaller sampling radius r can reduce the performance degradation but
it may not be sufficient to defend against adversarial samples. The optimal value for r varies with
different datasets due to their dimensionality and data sparsity. In experiments, we found that r = 30
for ImageNet and r = 6 for CIFAR-10 achieved relatively better performance. Figure 2 shows how
the model defence rate on ImageNet varies with different r. As shown in the figure, the optimal value
for r also varies in different attacking methods, but the performance variations are small. In general,
our model retains high defence rate throughout the r range [15, 30].
7
Under review as a conference paper at ICLR 2020
Figure 3: Defence rate of post-averaging on different allowed perturbation distances on CIFAR-10.
Table 2: Performance of post-averaging on ImageNet with different number of sampling directions
(e = 8∕255 and r = 30).
	Original Model		Defended by Post-Averaging				Averaged
	Top-1 Accuracy		Top-1 Accuracy		Defence		Inference
attack, defence	Clean	Attacked	Clean	Attacked	Rate	#Adv	Time
PGD, random(K=6)		0.0004	0.7736	0.7606	0.9636	3873	0.18s
PGD, random(K=15)	0.7750	0.0004	0.7730	0.7604	0.9636	3873	0.40s
PGD, random(K=30)		0.0006	0.7732	0.7614	0.9644	3874	0.86s
PGD, random(K=60)		0.0004	0.7734	0.7618	0.9649	3874	1.74s
We also tested the effect of K, the number of sampling directions used, on the model performance.
From Table 2, we can see that our model performance is not very sensitive to K . It is able to achieve a
good defence rate with only K = 6, that is, 37 samples used for each input image. In implementation,
these samples can be easily packed into a mini-batch for fast computation in GPUs. When running on
the same machine, we measured the averaged inference time for a single input image on the original
network as 0.04 seconds, while the inference time for our models with different K are shown in
Table 2. By comparison, we can know that the inference time after adding post-averaging is roughly
2 K of the original inference time.
At last, we evaluated our post-averaging defence approach against attacks with different allowed
perturbation ranges e. The results are shown in Figure 3. As we can see, our model retains very
good attack defence rate up to e = 32/255. Note that the defence rate against PGD and C&W doesn’t
change much along the variation of e, this is because PGD and C&W have already successfully
generated adversarial samples for most of the correctly classified inputs when e is small. Hence their
generated adversarial samples will not change much when using larger e. For FGSM, our method
yields lower defending performance. The possible reason is that FGSM tends to generate much
larger perturbations than other three stronger attacking methods under the same setting. A large
perturbation is more likely to move samples across class-specific decision boundaries to generate
much more confusing samples. In our opinion, this is a general phenomenon in pattern classification,
not particular to adversarial attacks.
5	Final remarks
In this paper, we have presented some theoretical results by Fourier analysis of ReLU neural networks.
These results are useful for us to understand why neural networks are vulnerable to adversarial
samples. Based on the results, we hypothesize that the inevitable and ubiquitous existence of tiny
unlearned regions in the model function mapping may be a major reason for adversarial vulnerability.
As a possible defence strategy, we have proposed a simple post-averaging method. Experimental
results on the ImageNet and the CIFAR-10 datasets have demonstrated that our simple defence
technique turns out to be very effective against many popular attack methods in the literature. Finally,
it will be interesting to see whether our post-averaging method will be still robust against any new
attack methods in the future.
8
Under review as a conference paper at ICLR 2020
References
Moustafa Alzantot, Yash Sharma, Supriyo Chakraborty, and Mani B. Srivastava. Genattack: Practical
black-box attacks with gradient-free optimization. CoRR, abs/1805.11090, 2018. URL http:
//arxiv.org/abs/1805.11090.
Anish Athalye and Nicholas Carlini. On the robustness of the cvpr 2018 white-box adversarial
example defenses. arXiv preprint arXiv:1804.03286, 2018.
Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. CoRR, abs/1802.00420, 2018. URL
http://arxiv.org/abs/1802.00420.
Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable
attacks against black-box machine learning models. arXiv preprint arXiv:1712.04248, 2017.
Nicholas Carlini and David Wagner. Defensive distillation is not robust to adversarial examples.
arXiv preprint arXiv:1607.04311, 2016.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pages 39-57. IEEE, 2017a.
Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and
Security, pages 3-14. ACM, 2017b.
Nicholas Carlini and David Wagner. Magnet and ”efficient defenses against adversarial attacks” are
not robust to adversarial examples. arXiv preprint arXiv:1711.08478, 2017c.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order
optimization based black-box attacks to deep neural networks without training substitute models. In
Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, AISec ’17, pages 15-
26, New York, NY, USA, 2017. ACM. ISBN 978-1-4503-5202-4. doi: 10.1145/3128572.3140448.
URL http://doi.acm.org/10.1145/3128572.3140448.
Reuben Feinman, Ryan R. Curtin, Saurabh Shintre, and Andrew B. Gardner. Detecting adversarial
samples from artifacts. CoRR, abs/1703.00410, 2017.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770-778, 2016.
Hui Jiang. A new perspective on machine learning: How to do perfect supervised learning. arXiv
preprint arXiv:1901.02046., 2019.
Hui Jiang and Chin-Hui Lee. A new approach to utterance verification based on neighborhood
information in model space. IEEE Transactions on Speech and Audio Processing, 11(5), 2003.
Hui Jiang, Keikichi Hirose, and Qiang Huo. A new approach to utterance verification based on
neighborhood information in model space. IEEE Transactions on Speech and Audio Processing, 7
(4), 1999.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
9
Under review as a conference paper at ICLR 2020
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 2574-2582, 2016.
Linh Nguyen and Arunesh Sinha. A learning approach and masking approach to secure learning.
CoRR, abs/1709.04447, 2017. URL http://arxiv.org/abs/1709.04447.
N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The limitations of
deep learning in adversarial settings. In 2016 IEEE European Symposium on Security and Privacy
(EuroS P), pages 372-387, March 2016a. doi: 10.1109/EuroSP.2016.36.
N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial
perturbations against deep neural networks. In 2016 IEEE Symposium on Security and Privacy
(SP), pages 582-597, May 2016b. doi: 10.1109/SP.2016.41.
Nicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. Transferability in machine learning:
from phenomena to black-box attacks using adversarial samples. CoRR, abs/1605.07277, 2016.
URL http://arxiv.org/abs/1605.07277.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM
on Asia Conference on Computer and Communications Security, ASIA CCS ’17, pages 506-519,
New York, NY, USA, 2017. ACM. ISBN 978-1-4503-4944-4. doi: 10.1145/3052973.3053009.
URL http://doi.acm.org/10.1145/3052973.3053009.
Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox: A python toolbox to benchmark
the robustness of machine learning models. arXiv preprint arXiv:1707.04131, 2017. URL
http://arxiv.org/abs/1707.04131.
Andras Rozsa, Ethan M. Rudd, and Terrance E. Boult. Adversarial diversity and hard positive
generation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
Workshops, June 2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115
(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
E.M. Stein and G. Weiss. Introduction to Fourier Analysis on Euclidean Spaces. Mathematical Series.
Princeton University Press, 1971. ISBN 9780691080789. URL https://books.google.
ca/books?id=YUCV678MNAIC.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204,
2017.
George Neville Watson. A treatise on the theory of Bessel functions. Cambridge university press,
1995.
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan L. Yuille. Mitigating adversarial
effects through randomization. CoRR, abs/1711.01991, 2017. URL http://arxiv.org/
abs/1711.01991.
10
Under review as a conference paper at ICLR 2020
Appendix: Mathematical proofs
Definition B.1. A piece-wise linear function is a continuous function f : Rn -→ R such that there
are some hyperplanes passing through origin and dividing Rn into M pairwise disjoint regions Rm,
(m = 1, 2, ..., M), on each of which f is linear:
f(x)
Wl ∙ X
W2 ∙ X
.
.
.
WM ∙ X
X ∈ R1
X∈R2
X ∈ RM
Lemma B.2. Composition of a piece-wise linear function with a ReLU activation function is also a
piece-wise linear function.
Proof. Let r(.) denote the ReLU activation function. If f(X) on region Rm takes both positive and
negative values, r f(X) will break it into two regions Rp+ and R0p. On the former r f(X) = f(X)
and on the latter r f(X) = 0, which both are linear functions. As f (X) on Rp is linear, common
boundary of R+ and Rp lies inside a hyperplane passing through origin - which is the kernel of the
linear function. Therefore, if f (X) is a piece-wise linear function defined by k hyperplanes resulting
in M regions, r f (x)) will be a piece-wise linear function defined by at most k + m hyperplanes. □
Theorem B.3. The output of any hidden unit in an unbiased fully-connected ReLU neural network is
a piece-wise linear function.
Proof. This proposition immediately follows lemma B.2.
□
Definition B.4. Let V = {v1, v2, ..., vn} be a set of n independent vectors in Rn. An infinite
simplex, R+V, is defined as the region linearly spanned by V using only positive weights:
n
R+V = {X αkvk | ∀k αk > 0}
k=1
(9)
Theorem B.5. Each piece-wise linear function f(X)can be formulated as a summation of some
functions: f(X)= PkK=1 fk (X), each of which is linear and non-zero only in an infinite simplex as
follows:
fk(X)
Wk ∙ x
0
X∈R+Vk
otherwise
where Vk is a set of n independent vectors, and Wk is a weight vector.
Proof. Each region Rp of a piece-wise linear function, f (X), which describes the behavior ofa ReLU
node if intersects with an affine hyper-plane results in a convex polytope. This convex polytope can
be triangulated into some simplices. Define Vk,(k = 1, 2, ..., K), sets of vertexes of these simplices.
The infinite simplexes created by these vector sets will have the desired property and f(X)can be
written as: f (x) = Pk=I fk(x).	□
As explained earlier in the original article by adding n2 hyper-planes to those defining the piece-wise
linear function, the output of a ReLU node may be represented as f(x)= PqQ=1 gq (x). These
hyper-planes are those perpendicular to standard basis vectors and subtraction of one of these vectors
from another one. That is, ei(i = 1, . . . , n) and ei - ej(1 ≤ i < j ≤ n). Given this representation,
the final step to achieve the Fourier transform is the following lemma:
Lemma B.6. Fourier transform of the following function:
S(X) = { h(1 — 1 ∙x)演RVe
11
Under review as a conference paper at ICLR 2020
may be presented as:
n	iω
S(ω) =(√2∏rX⅛ Q In)
r0 6=r
where ωr is the r th component of frequency vector ω (r = 1, ∙∙∙ ,n), and ωo = 0.
Proof. Alternatively, s(x) may be represented as:
n
S(X) = h(1 ∙ x)h(1 — 1 ∙ x) Y h(xj )h(1 — Xj)
j=1
Therefore, we need to compute Fourier transform of h(x)h(1 — x):
ɪ Z∞ e-ixωh(x)(1 — x)dx = ɪ [ 1 e-ixωdx
2π -∞	2π 0
_ —i 1 — e-iω
√2π	ω
By taking the inverse Fourier transform of the function:
,—	,∕*∞ —i 1 — e-iζ
(K	L √2∏ -Z- δ(ω — ζ 1)dζ
(10)
(11)
(12)
(13)
(14)
where δn is n-dimensional Dirac Delta function, it can be shown that it is the Fourier transform of
h(1 ∙ x)h(1 — 1 ∙ x):
(
∞	-	,—	J∞	-i 1 — e-iζ
…L∕ω∙x(√2∏)nτ/	√2∏-z^ δn(ω — Z 1)dZ dω
1	∞ i 1	e-iζ
=√2∏∕ …Lni ∕∞ √2∏ -Z- δn(ω - ζ 1)dζ dω
_	1	/ ∞	—i	1 —	e-iζ
√2∏ J-∞ √2∏	Z
_	1	/ ∞	—i	1 —	e-iζ
√2∏ J-∞ √2∏	Z
=h(1 ∙ x)h(1 — 1 ∙ x)
Now we can find the Fourier transform of s(x)
I …I eiω∙xδn(ω — Z1) dω dZ
Rn
eiζ1.x dζ
,二 -i 1 - e-iωr 1	,_	_ ∞ -i 1 - e-iζ
S(ω) = (∏ 萍 ~ω^)* (√2π)n-1 L √2π 丁 δn(ω -ZI) dZ
i	∞	n 1	-i(ωr -ζ)
=i(专产	e-iζ Y1 —e 广	dZ
v2∏	--OO	r=0 ωr - Z
where * is convolution operator. The final integrand may be represented as:
LI! 1—^ = LI!
r=0	r	r=0
n
=e-iζX
r=0
n
=e-iζX
r=0
1
ωr - Z
Ar
ωr - Z
n
Y(1 — e-i(ωr-ζ))
r=0
n
Y (1 — e-i(ωr-ζ) )
r=0
Ar	X ( —1)∣B∣e-i(σB-∣B∣Z)
ωr - Z B⊆Ω
X Ar	X ( —1)∣B%-i(σB-(∣B∣-1)Z)
r=0 ωr - Z b⊆ω
(15)
(16)
(17)
(18)
(19)
(20)
(21)
(22)
(23)
(24)
(25)
12
Under review as a conference paper at ICLR 2020
where Ω 二	二{ωo,...,ωn}, σB is the summation over elements of B and Ar = Q ⑴— ∙ Therefore: r0 6=r r	r ∞	n 1 - e-i(ωr-ζ) e-iζ Y	- dZ	(26) -∞	r=0	ωr - ζ =Z∞ X -Ar^ X (-1)lBle-i(σB-(IBIT)Z) dZ	(27) n∞ =X ArJ	;ɪ X (-1)lBle-iSB-(IBT)Z) dZ	(28) n∞ =X Ar	— X (-i)lBl+1e-i(σB-(|BIT)sr+(1B1T)Z) dZ	(29) r=0	-∞ Z B⊆S n = XAr X (-1)IBIiπ sign(|B|- 1)e-i(σB -(IBI-1)ωr)	(30) r=0	B⊆S
If B does not contain ωr and have at least 2 elements then the terms for B and B ∪ {ωr} will cancel
each other out. Also, sign(|B| - 1) will vanish if B has only one element. Therefore, there only
remains empty set and sets with two elements one of them being ωr . Given the fact that P Ar = 0,
the result of the integral will be:
	∞	n 1	-i(ωr-Z)	n e-iζ Y 1-e 广	dZ = iπXAr(-e-iωr + X e-iωr0)	(31) -∞	r=0	ωr -Z	r=0	r06=r n
= -2iπ	Are-iωr	(32)
r=0
Finally, substituting 32 into 21 yields to the desired result.	□
Theorem B.7. The Fourier transform of the output of any hidden node in a fully-connected ReLU
neural network may be represented as PQ=I Wq A-1VS(ωA-1), where V denote the differential
operator.
Proof. As discussed in the original paper, f(x) = PqQ=1 gq (x) where:
gq (x) = ( Wq ∙ Xq h(1- 1 ∙ Xq) Xq ∈ RV*	(33)
q	0	otherwise
or equivalently:
gq(X)= Wq ∙ XqS(Xq)	(34)
Therefore:
Q
F(ω) =	Gq(ω)	(35)
q=1
Q
=X Wq.VS(ωq)	(36)
q=1
where ω q = ωA-1.	□
Derivation of eq.(6)
As for the Fourier transform computed in section 3.1, it should be mentioned that the integral in
equation 6 is the Fourier transform of:
hr(X) = h(r - |X|)	(37)
13
Under review as a conference paper at ICLR 2020
which can be derived utilizing the property of the Fourier transforms for radially symmetric func-
tions (Stein and Weiss, 1971):
Hr (ω) =	=|3|-	∞ Tjo	Jn-2 (HP)Pɪ h(r - P)P dP	(38)
	=|3|-	-^n― / Jn-2 (∣ω∣ρ)ρ2 dρ	(39)
	=(— (∣ω∣	∣)2 J2 (r∣ω∣)	(40)
Given this transform:			
FC (ω) =		F(ω)-^ […[	e-ix0∙ω dx0 VC	x0∈C	(41)
	=	F」(2 + 1) J2"I) F (ω)	π 2	(r∣ω∣) 2	(42)
14