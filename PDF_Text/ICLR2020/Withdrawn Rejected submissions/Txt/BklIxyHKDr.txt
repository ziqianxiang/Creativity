Under review as a conference paper at ICLR 2020
Deep k-NN for Noisy Labels
Anonymous authors
Paper under double-blind review
Ab stract
Modern machine learning models are often trained on examples with noisy labels
that hurt performance and are hard to identify. In this paper, we provide an empir-
ical study showing that a simple k-nearest neighbor-based filtering approach on
the logit layer of a preliminary model can remove mislabeled training data and
produce more accurate models than some recently proposed methods. We also
provide new statistical guarantees into its efficacy.
1	Introduction
Machine learned models can only be as good as the data they were used to train on. With increas-
ingly large modern datasets and automated and indirect labels like clicks, it is becoming ever more
important to investigate and provide effective techniques to handle noisy labels.
We revisit the classical method of filtering out suspicious training examples using k-nearest neigh-
bors (k-NN) (Wilson, 1972). Like Papernot & McDaniel (2018), we apply k-NN on the learned
intermediate representation of a preliminary model, which adds robustness. In fact, the k-nearest
neighbor approach has recently been receiving attention for its robustness properties (Wang et al.,
2018; Reeve & Kaban, 2019) and as an auxiliary strategy for modern machine learning (Jiang et al.,
2018).
The main contributions of this paper are:
•	Experimentally showing that identifying mislabeled examples by k-NN executed on an in-
termediate layer of a preliminary deep model works well compared to state-of-art methods
for handling noisy labels across noise levels, and is robust to the choice of k.
•	Theoretically showing that k-NN’s predictions will only identify a training example as
clean if its label is the Bayes-optimal label. We also provide finite-sample analysis in terms
of the margin and how spread out the corrupted labels are (Theorem 1), rates of convergence
for the margin (Theorem 2) and rates under Tsybakov’s noise condition (Theorem 3) with
all rates matching minimax-optimal rates in the noiseless setting.
Our work shows that even though the preliminary neural network is trained with corrupted labels,
it still yields intermediate representations that are useful for k-nearest neighbor filtering. Given
labels which are in high disagreement, one can either automatically remove them and retrain on
the remaining, or send to a human operator for further review. This strategy is also be useful in
human-in-the-loop systems where one can warn the human annotator that a label is suspicious, and
automatically propose new labels based on its nearest neighbors’ labels.
In addition to strong empirical performance, deep k-NN filtering has a couple of advantages. Firstly,
many methods require a clean set of samples whose labels can be trusted. Here we show that the
k-NN based method is robust and does not require such a clean set of samples. Second, while k-NN
does introduce the hyperparameter k, we will show that deep k-NN filtering is stable to the choice
of k: such robustness to hyperparameters is highly desirable as optimal tuning for this problem is
often not available in practice (i.e. when no clean validation set is available).
2	Related Work
We review relevant prior work in training on noisy labels and related k-NN theory.
1
Under review as a conference paper at ICLR 2020
2.1	Training with noisy labels
Methods to handle label noise can be classified into two main strategies: (i) explicitly identify and
remove the noisy examples, and (ii) indirectly handle the noise with robust training methods.
Data Cleaning This proposal fits into the broad family of data cleaning methods, in that our pro-
posal detects and filters dirty data (see Chu et al. (2016) for a recent survey). The use of k-NN to
“edit” training data has been popular since Wilson (1972) used it to throw away training examples
that were not consistent with their k = 3 nearest neighbors. The idea of using a preliminary model
to help identify mislabeled examples dates to at least Guyon et al. (1994), who proposed using the
model to compute an information gain for each example and then suspecting ones with high gain.
Other early work used a cross-validation set-up to train a classifier on part of the data, then use it
to make a prediction on held-out training examples, and remove any examples if the prediction dis-
agrees with the label; ensembles of models can also be used for the predictions. (Brodley & Freidl,
1999).
Noise Corruption Estimation For multi-class problems, a popular approach is to account for noisy
labels by applying a confusion matrix after the model’s softmax layer (Sukhbaatar et al., 2014). Such
methods rely on a confusion matrix which is often unknown and must be estimated. Patrini et al.
(2017) suggest deriving it from the softmax distribution of the model trained on noisy data while
Goldberger & Ben-Reuven (2016); Jindal et al. (2016); Han et al. (2018) give alternatives. Accurate
estimates are generally hard to attain when only untrusted data is available. Hendrycks et al. (2018)
achieves more accurate estimates in the setting where some amount of known clean, trusted data
is available. Xiao et al. (2015); Khetan et al. (2017); Vahdat (2017) use EM-type algorithms to
estimate the clean label distribution.
Noise-Robust Training Natarajan et al. (2013) propose a method to make any surrogate loss func-
tion noise-robust given knowledge of the corruption rates. Ghosh et al. (2017) proves that losses
like mean absolute error (MAE) are inherently robust under symmetric or uniform label noise while
Zhang & Sabuncu (2018) shows that training with MAE results in poor convergence and accuracy.
They propose a new loss function based on the negative Box-Cox transformation that trades off the
noise-robustness of MAE with the training efficiency of cross-entropy. Lastly, the ramp, unhinged,
and savage losses have been proposed and theoretically justified to be noise-robust for support vector
machines (Brooks, 2011; Van Rooyen et al., 2015; Masnadi-Shirazi & Vasconcelos, 2009). Rolnick
et al. (2017) empirically shows that deep learning models are robust to noise when there are enough
correctly labeled examples and when the model capacity and training batch size are sufficiently
large.
Auxiliary Models Veit et al. (2017) propose learning a label cleaning network on trusted data by
predicting the differences between clean and noisy labels. Li et al. (2017) suggests training on a
weighted average between noisy labels and distilled predictions of an auxiliary model trained on
trusted data.
Example Weighting Here we make a hard decision about whether to keep a training example, but
one can also adapt the weights on training examples based on the confidence in their labels. Liu
& Tao (2015) provides an importance-weighting scheme for binary classification. Ren et al. (2018)
suggests upweighting examples whose loss gradient is aligned with those of trusted examples at
every step in training. Jiang et al. (2017) investigates a recurrent network that learns a sample
weighting scheme to give to the base model.
2.2	k-NEAREST NEIGHBOR THEORY
The theory of k-nearest neighbor classification has a long history, for example: Fix & Hodges Jr
(1951); Cover (1968); Stone (1977); Devroye et al. (1994); Chaudhuri & Dasgupta (2014). Much
of the prior work focuses on k-NN’s statistical consistency properties. However, with the growing
interest in adversarial examples and learning with noisy labels, there have recently been analyses of
k-nearest neighbor methods in these settings. Wang et al. (2018) analyze the robustness of k-NN
classification and provide a robust variant of 1-NN classification where their notion of robustness
is that predictions of nearby points should be similar. Gao et al. (2016) provides an analysis of
the k-NN classifier under noisy labels and like us, show that k-NN can attain similar rates in the
noisy setting as in the noiseless setting. Gao et al. (2016) assumes a noise model where labels are
2
Under review as a conference paper at ICLR 2020
corrupted uniformly at random, while we assume an arbitrary corruption pattern and provide results
based on a notion of how spread out the corrupted points are. Moreover, we provide finite-sample
bounds borrowing recent advances in k-NN convergence theory in the noiseless setting (Jiang, 2019)
while the guarantees of Gao et al. (2016) are asymptotic. Reeve & Kaban (2019) provide stronger
guarantees on a robust modification of k-NN proposed by Gao et al. (2016). To the best of our
knowledge, we provide the first finite-sample rates of consistency for the classical k-NN method in
the noisy setting with very little assumptions on the label noise.
3	Algorithm
We first define the k-nearest neighbor classifier:
Definition 1 (k-NN). Let the k-NN radius of x ∈ X be rk (x) := inf {r : |B(x, r) ∩ X| ≥ k} where
B(x, r) := {x0 ∈ X : |x - x0| ≤ r} and the k-NN set of x ∈ X be Nk (x) := B(x, rk (x)) ∩ X.
Then for all x ∈ X, the k-NN classifier function w.r.t. X has discriminant function
ηk (y; x) :
∣Nk(X)i i=11[yi
y, xi ∈ Nk(x)] ,
with prediction ηk(x) := argmaxy ηk (y; x).
Our method Algorithm 1 assumes a dataset Dnoisy with potentially noisy labels, along with a dataset
Dclean consisting of clean or trusted labels. Note that we allow Dclean to be empty (i.e. in instances
where no such trusted data is available). We have found that having Dclean becomes important when
Dnoisy has a high corruption rate; otherwise the representations learned by training on Dnoisy alone
are often reasonable enough. The procedure begins by training on either Dnoisy ∪ Dclean or Dclean. For
our experiments, we partition Dclean into a training set Dct and validation set Dcv and train models
on Dct and Dnoisy ∪ Dct and choose the one that performs better on Dcv .
We then filter examples that disagree with the k-NN classifier prediction, where the k-NN is com-
puted on the final logit layer of the trained model (i.e. the layer right before softmax).
Algorithm 1 Filtering datapoints via deep k-NN.
Inputs： Dnoisy, Dclean, k
Train model M on either Dnoisy ∪ Dclean or Dclean.
Let N be the activations of Dnoisy ∪ Dclean on the logit layer of M.
Dfiltered := {(x, y) ∈ N : ηk(x) = y}, where ηk is computed w.r.t. N .
Train final model on Dfiltered ∪ Dc .
4	Theoretical Analysis
For the theoretical analysis, we assume the binary classification problem with the features defined on
compact set X ⊆ RD . We assume that points are drawn according to distribution F as follows: the
features come from distribution PX on X and the labels are distributed according to the measurable
conditional probability function η : X → [0, 1]. That is, a sample (X, Y ) is drawn from F as
follows: X is drawn according to PX and Y is chosen according to P(Y = 1|X = x) = η(x).
The goal will be to show that given corrupted examples, the k-NN disagreement method is still able
to identify the examples whose labels do not match that of the Bayes-optimal label.
We will make a few regularity assumptions for our analysis to hold. The first regularity assumption
ensures that the support X does not become arbitrarily thin anywhere. This is a standard non-
parametric assumption (e.g. Singh et al. (2009); Jiang (2019)).
Assumption 1 (Support Regularity). There exists ω > 0 and r0 > 0 such that Vol(X ∩ B(x, r)) ≥
ω ∙ Vol(B(x, r)) for all X ∈ X and 0 < r < r0, where B(x, r) := {x0 ∈ X : |x — x0∣ ≤ r}.
Let pX be the density function corresponding to PX . The next assumption ensures that with a
sufficiently large sample, we will obtain a good covering of the input space.
3
Under review as a conference paper at ICLR 2020
Assumption 2 (pX bounded from below). pX,0 := infx∈X pX (x) > 0.
Finally, we make a smoothness assumption on η, as done in other analyses of k-NN classification
(e.g. Chaudhuri & Dasgupta (2014); Reeve & Kaban (2019))
Assumption 3 (η Holder continuous). There exists 0 < α ≤ 1 and Ca > 0 such that ∣n(x) 一
η(χ0)∣ ≤ Ca|x - x0∣a for all χ, χ0 ∈ X∙
We propose a notion of how spread out a set of points is based on the minimum pairwise distance
between the points. This will be a quantity in the finite-sample bounds we will present. Intuitively,
the more spread out a contaminated set of points is, the less clean samples we will be needed to
overcome the contamination of that set.
Definition 2 (Minimum pairwise distance).
S2(C) :
min	|x - x0 |.
x,x0 ∈C,x6=x0
Also define the ∆-interior region of X where there is at least ∆ margin in the probabilistic label:
Definition 3. Let ∆ ≥ 0. Define Xδ := {x ∈ X : 11 — η(x) ∣ ≥ ∆}.
We now state the result, which says that with high probability uniformly on X∆ when ∆ > 0 is
known, we have that the label disagrees with the k-NN classifier if and only if the label is not the
Bayes-optimal prediction. Due to space, all of the proofs have been deferred to the Appendix.
Theorem 1 (Fixed ∆). Let ∆, δ > 0 and suppose Assumptions 1, 2, and 3 hold. There exists
constants Kl, Ku > 0 depending only on F such that the following holds with probability at least
1 - δ. Let X[n] be n (uncorrupted) examples drawn from the F and C be a set of points with
corrupted labels and denote our sample X := X[n] ∪ C. Suppose k lies in the following range
Ki ∙ ± ∙ log2(1∕δ) ∙ logn ≤ k ≤ Ku ∙ min{S2(C)D, ∆"a} ∙ n,
∆2
then the following holds uniformly over x ∈ X∆: the k-NN prediction computed w.r.t. X agrees
with the label if and only if the label is the Bayes-optimal label η*(x) := 1[η(x) ≥ 1 ].
In the last result, we assumed that ∆ was fixed. We next show how we can make a similar guarantee
but show that we can take ∆ → 0 as we choose k, n → ∞ appropriately and provide rates of
convergence.
Theorem 2 (Rates of convergence for ∆). Let δ > 0 and suppose Assumptions 1, 2, and 3 hold.
There exist constants Kl , Ku, K > 0 depending only on F such that the following holds with
probability at least 1 - δ. Let X[n] be n (uncorrupted) examples drawn from F, and C be a set of
points with corrupted labels and denote our sample X := X[n] ∪ C. Suppose k lies in the following
range
Ki ∙log2(1∕δ) ∙ nɑ⅛ ≤ k ≤ Ku ∙ S2(C)d ∙ n,
then the following holds uniformly over x ∈ X∆: the k-NN prediction computed w.r.t. X agrees
with the label if and only if the label is the Bayes-optimal label η*(x) := 1[η(x) ≥ 2 ] where
δ=k ∙ (r log n+log，10+&
Remark 1. Choosing k = O(n2a/(2a+D)) in the above result gives us ∆ = O(n-a/(2a+D)). ThiS
rate for ∆ is the minimax-optimal rate for k-nearest neighbor classification on X∆ given a sample
of size n (Chaudhuri & Dasgupta, 2014) in the uncorrupted setting. Thus, our analysis is tight up
to logarithmic factors.
We next give results with an additional margin assumption, also known as Tsybakov’s noise condi-
tion (Mammen et al., 1999; Tsybakov et al., 2004):
Assumption 4 (Tsybakov Noise Condition). The following holds for some Cβ and β and all ∆ > 0:
Pχ(χ∈Xδ) ≤ Ce ∙ ∆β.
4
Under review as a conference paper at ICLR 2020
Figure 1: Left: training samples. We observe that test accuracy improves as S2 (C) increases
(middle) and that fewer clean training samples are needed to achieve an accuracy of 90% (right).
Theorem 3 (Rates under Tsybakov Noise Condition). Let δ > 0 and suppose Assumptions 1, 2, 3
and 4 hold. There exists constants Kl, Ku, K, K0 > 0 depending only on F such that the following
holds with probability at least 1 - δ. Let X[n] be n (uncorrupted) examples drawn from the F and
C be a set of points with corrupted labels and denote our sample X := X[n] ∪ C. Suppose k lies in
the following range
Kl ∙ log2(1∕δ) ∙ nα+D ≤ k ≤ Ku ∙ S2(C)D ∙ n,
and define ηk(x) := arg maxy ηk(y; x). Then,
P(ηk(χ) = η*(χ)) ≤ k ∙
RX - R* ≤ K J
Iog n + log(1∕δ) + ( k )a/D )β
log n + log(1∕δ) + ( k ) OaD! β+1
where RX := EF [gk(x) 6= y] and R* := EF [g*(x) 6= y] denote the risk of the k-NN method and
Bayes optimal classifier, respectively.
Remark 2. Choosing k = O(n2a/(2a+D)) in the above gives Us a rate of O (n-α(β+1)/(2a+D)) for
the excess risk. This matches the lower bounds of Audibert et al. (2007) up to logarithmic factors.
4.1	Impact of Minimum Pairwise Distance
The minimum pairwise distance across corrupted samples, S2 (C), is a key quantity in the theory
presented in the previous section. We now empirically study its significance in a simulated binary
classification task in 2 dimensions. Clean samples with label L are generated by sampling i.i.d from
N(μL, I2×2), where μ0 = (0, -2) and μ1 = (0, 2). The decision boundary is the line y = 0.
We take 100 samples uniformly spaced on a square grid centered about (0, 0) and corrupt them by
flipping their true label. With this construction, S2(C) is precisely the grid width, which we let vary.
The training set is a union of 100 clean samples and the 100 corrupted samples. Using 1000 clean
samples as a test set we study the classification performance of a majority vote k-NN classifier,
where k = 10. Results are shown in Figure 1. As expected, we see that as S2 (C) decreases, so does
test accuracy and we need more clean training samples to compensate.
5	Experiments
We evaluate the effectiveness of our algorithm as follows. We split each dataset’s training set into
two parts, Dclean and Dnoisy. We then corrupt the labels of some fraction of examples in Dnoisy by
applying a corruption matrix prescribed by one of the following methods.
•	Uniform: The label is flipped to any one of the labels (including itself) with equal proba-
bility.
•	Flip: The label is flipped to any other label with equal probability.
•	Hard Flip: With probability ɪ, we flip the label m to π(m) where π is some predefined
permutation of the labels.
5
Under review as a conference paper at ICLR 2020
Figure 2: UCI Results. Error plots against amount of noise applied to the labels of Dnoisy. Dclean
contains 5% of the data. Each column is a different corruption and each row is for a different dataset.
We see that the k-NN method consistently chooses the best datapoints to filter leading to lower error.
More results are in the Appendix.
We compare against the following baselines:
•	Gold Loss Correction (GLC) (Hendrycks et al., 2018) estimates the corruption matrix by
averaging the softmax outputs of the clean examples on a model trained on noisy data.
•	Distill (Li et al., 2017) assigns each example in the combined dataset a “soft” label that is
a convex combination of its label and its softmax output from a model trained solely on
clean data.
•	Forward (Patrini et al., 2017), similar in spirit to GLC, estimates the corruption matrix by
training a model on noisy data and using the softmax output for prototype examples for
each class. It does not require a clean dataset like other methods.
•	Clean. We define this as training on the clean data only.
•	Full. We define this as training on the full (clean and noisy) data.
•	k-NN Classify is like “Full” except we use k-NN majority voting on the logits layer for
classification at test time.
We report test errors and show the average across multiple runs with standard error bands shaded.
Errors are computed on 11 uniformly distributed noise rates between 0 and 1 inclusive. For the
results shown in the main text, we have that Dclean is randomly selected and is 5% of the data. In
the Appendix, we show results over different sizes of Dclean. We implement all methods using the
Tensorflow 2.0 Keras API and Scikit-Learn. We use the Adam optimizer with default learning rate
0.001 and a batch size of 128 across all experiments. For the UCI datasets, we set k = 50 and set
k = 500 for all other datasets. We chose k = 50 for the UCI datasets because some of the datasets
were of small size. However, we found that the k-NN method’s performance was quite stable to the
choice of k, which we show in Section 5.4. We describe the permutations used for hard flipping in
the Appendix.
5.1	UCI AND MNIST RESULTS
We show the results for one of the UCI datasets in Figure 2 and Fashion MNIST in Figure 3. Due to
space, results for MNIST and the remaining UCI datasets are in the Appendix. For UCI, we use a
fully-connected neural network with a single hidden layer of dimension 100 with ReLU activations
and train for 100 epochs. For both MNIST datasets, we use is a two hidden-layer fully-connected
neural network where each layer has 256 hidden units with ReLU activations. We train the model
for 20 epochs. We see that the k-NN approach attains models with a low error rate across noise rates
and either outperforms or is competitive with the next best method, GLC.
5.2	CIFAR Results
For CIFAR10/100 we use ResNet-20, which we train from scratch on single NVIDIA P100 GPUs.
We train CIFAR10 for 100 epochs and CIFAR100 for 150 epochs. We show results for CIFAR10
in Figure 4 and results for CIFAR100 in the Appendix, due to space. We see that the k-NN method
performs competitively. It generally outperforms on the uniform and flip noise types but performs
6
Under review as a conference paper at ICLR 2020
山∙4-JS①-L
Figure 3: Fashion MNIST. Each column is a different corruption method. We see that the k-NN
approach performs competitively. More results are in the Appendix.
」0」」山-MS①一
Figure 4: CIFAR10. Each column is a different corruption method. We see that our k-NN method
performs competitively or outperforms on the uniform and flip noise types but performs worse for
the hard flip noise type. More results are in the Appendix.
1^0
worse for the hard flip noise type. It is not too surprising that k-NN would be weaker in the presence
of hard flip noise (i.e. where labels are mapped based on a pre-determined mapping between labels)
as the noise is much more structured in that case making it more difficult to be filtered out by
majority vote among the neighbors. In other words, unlike the uniform and flip noise types, we are
no longer dealing with white label noise in the hard flip noise type.
5.3	SVHN RESULTS
We show the results in Figure 5. We train ResNet-20 from scratch on NVIDIA P100 GPUs for 100
epochs. As in the CIFAR experiments, we see that the k-NN method tends to be competitive in the
uniform and flip noise types but does slightly worse in the hard flip.
5.4	ROBUSTNESS TO k
In this section, we show that our procedure is stable in its hyperparameter k . The theoretical results
suggest that a wide range of k can give us statistical consistency guarantees and we show that in
practice a wide range of k gives us similar results for Algorithm 1 (Figure 6). Such robustness in
」0」」山.MS①F-
Figure 5: SVHN. We see that the k-NN method performs competitively on the uniform and flip
noise types but performs worse for the hard flip noise type. More results in the Appendix.
7
Under review as a conference paper at ICLR 2020
		Uniform							Flip							Hard Flip						
DataSet	% Clean	Forward	Clean	Distill	GLC	k-NN	k-NN Classify	Full	Forward	Clean	Distill	GLC	k-NN	k-NN Classify	Full	Forward	Clean	Distill	GLC	k-NN	k-NN Classify	Full
Letters	5 10 20	4.55- 4.28 3.77	3.16 2.51 2.06	2.48 2.05 1.76	2.33 1.91 1.57	2.05 1.78 1.56	^ΣT9 1.79 1.34	^26T 2.23 1.85	-483- 4.45 3.85	3.16 2.52 2.07	2.96 2.12 1.78	2.35 1.92 1.56	2.1 1.8 1.58	^Σ52 2.06 1.42	^Σ92^ 2.49 1.93	-377- 3.63 3.35	3.17 2.52 2.05	-21- 1.85 1.62	1.83 1.6 1.38	1.82 1.6 1.41	T82 1.64 1.42	^Σ52^ 2.36 2.04
Phonemes	5 10 20	7.89- 7.86 7.72	1.91 1.54 1.34	1.79 1.53 1.33	2.12 1.67 1.35	1.26 1.16 1.13	^238 2.28 1.76	1 2.85 2.24	^791 7.95 7.89	1.93 1.54 1.34	3.21 2.75 1.97	2.16 1.69 1.35	1.34 1.22 1.16	T97 3.64 2.87	^43T 3.96 3.28	~66 6.73 6.36	1.92 1.55 1.33	1.77 1.61 1.45	1.96 1.6 1.3	1.2 1.15 1.14	T7≡ 1.62 1.38	27Γ~ 2.58 2.28
Wilt	5 10 20	-51- 4.68 4.31	0.56 0.43 0.36	0.85 0.75 0.86	0.54 0.45 0.35	0.39 0.32 0.32	^093 0.77 0.57	jɪ 1.77 1.5	5.27- 5.63 5.18	0.56 0.44 0.34	3.89 3.14 2.67	0.53 0.43 0.35	0.52 0.41 0.34	3J5 4.86 4.23	^495- 4.84 4.32	-46 4.78 5.63	0.55 0.43 0.34	0.73 0.66 0.61	0.58 0.43 0.36	0.39 0.31 0.3	^09≡ 0.78 0.57	T9- 1.81 1.49
Seeds	5 10 20	-32- 3.43 2.99	4.22 3.04 2.69	3.71 2.84 2.27	4.2 2.99 2.74	3.08 2.14 1.72	^ΣS7 2.74 2.44	TTT 2.57 2.2	-5J3- 5.02 4.41	4.33 3.38 2.56	5.56 4.58 3.57	4.39 3.19 2.65	3.64 2.65 2.09	^5∏ 4.9 4.23	^488- 4.73 3.86	-294- 2.75 2.85	4.06 3.42 2.53	3.63 2.96 2.25	3.91 3.14 2.57	2.99 2.25 1.62	1.84 2.69 2.43	ʒɪ 2.86 2.49
Iris	5 10 20	-297- 2.89 2.46	3.23 2.48 1.85	3.48 2.59 1.97	3.97 2.25 1.52	2.46 1.32 0.6	^E72 1.26 0.96	^205^ 1.55 1.32	-525- 5.06 4.46	3.38 2.53 1.51	5.15 4.58 3.45	4.03 2.24 1.38	3.02 2.17 1.46	^432 3.98 3.36	"4.43- 4.08 3.53	-29- 2.6 2.51	3.29 2.34 1.84	3.13 2.02 1.48	4.1 1.86 1.34	2.32 1.15 0.58	1.14 0.91 0.65	T3T 1.16 1
Parkinsons	5 10 20	5 5.35 4.88	3.46 3.26 3.01	3.26 3.22 3.08	4.22 3.45 3.1	3.4 3.21 2.98	^336 3.22 2.97	ʒɪ 3.82 3.52	5.17- 5.43 5.19	3.55 3.38 3.02	4.49 4.25 3.94	4.1 3.44 3.05	3.36 3.32 2.95	334 5.27 4.99	33- 5.13 5.06	-498- 5.24 5.1	3.71 3.26 2.98	3.56 3.1 3.01	4.59 3.37 2.98	3.68 3.11 2.96	"3.28 3.12 2.91	T6T 3.82 3.47
MNIST	5 10 20	-288- 2.57 2.07	0.69 0.5 0.35	1.03 0.85 0.69	0.5 0.41 0.34	0.4 0.33 0.27	^2T72 2.42 1.97	^Σ75^ 2.45 2.03	-36- 3.22 2.48	0.69 0.5 0.35	1.91 1.5 0.86	0.5 0.42 0.34	0.44 0.35 0.27	■3.46 3.1 2.36	33F 3.14 2.41	-203- 1.86 1.54	0.69 0.5 0.35	0.78 0.67 0.53	0.22 0.21 0.2	0.29 0.26 0.22	^065 0.48 0.36	^214^ 1.98 1.67
FashionMNIST	5 10 20	-2:76- 2.47 2.07	1.88 1.71 1.56	1.73 1.6 1.48	T39- 1.52 1.45	1.56 1.52 1.44	^233 2.21 1.95	^234^ 2.3 2.05	-3.55- 3.14 2.38	1.87 1.71 1.56	2.55 2.13 1.54	1.59 1.53 1.46	1.6 1.54 1.46	T3 2.92 2.17	ʒɪ 2.99 2.27	~23 2.14 1.92	1.87 1.71 1.56	1.62 1.52 1.43	1.44 1.41 1.38	1.48 1.46 1.41	^231 2.19 2.04	^24^ 2.22 1.97
CIFAR10	5 10 20	-6:74- 6.58 6.4	-7- 6.58 5.52	6.86 6.32 5.66	5.43 5.39 5.11	5.03 5.27 4.57	^634 6.11 5.93	^674^ 6.55 6.36	7.14- 6.82 6.62	7.2 6.56 5.59	7.12 6.72 5.9	5.52 5.62 5.16	5.35 5.32 4.85	^671 6.48 6.1	^7yτ∑- 6.83 6.56	-50- 4.89 4.77	7.13 6.53 5.45	5.85 5.3 4.68	3.76 3.91 3.51	4.27 4.4 3.82	^433 4.21 4.03	^496^ 4.89 4.75
CIFAR100	5 10 20	-108- 10.79 10.78	10.22 9.94 9.38	9.98 9.7 9.15	-939- 9.42 9.06	9.57 9.63 9.23	^9T7 9.09 8.92	^929^ 9.25 9.07	10.79 10.81 10.8	10.24 9.89 9.44	10.03 9.68 9.13	9.64 9.46 8.97	9.66 9.63 9.17	^93 9.1 8.92	^92T 9.25 9.09	-10.64- 10.65 10.66	10.23 9.89 9.41	9.88 9.38 8.65	8.58 8.56 8.07	8.98 9.19 8.81	^7T48 7.44 7.33	^804 7.99 7.9
SVHN	5 10 20	-50- 4.98 4.32	3.52 2.2 1.83	3.56 3.2 2.67	T9- 2.27 2.14	1.62 1.34 1.2	^464 4.51 3.96		^495^ 4.82 4.41	-56- 5.5 4.85	3.65 2.28 1.83	4.09 3.72 3.09	2.17 2.29 1.9	2.05 1.48 1.19	313 4.96 4.34		3.52- 5.29 4.82	-304- 2.98 2.7	4.17 2.41 1.85	2.16 2.01 1.54	0.89 0.88 0.98	1.35 1.04 0.98	^232 2.16 1.83		ʒɪ 2.98 2.74
Table 1: Area under the test error vs noise rate curve. Each row corresponds to a dataset and
size of clean dataset Dclean pair, where the size is a percentage of the total training set (5%, 10%,
20%). Each column shows the area under the error curve across noise rates for a particular method
and noise type (Uniform, Flip, Hard Flip). We see that the k-NN method consistently outperforms
the other methods for Uniform and Flip and outperforms the other methods on Hard Flip on the
smaller datasets.
hyperparameter is highly desirable because optimal tuning is often not available, especially when no
sufficient clean validation set is available.
」0」」山U
Figure 6: Performance across different values of k. Here we show that on a UCI dataset, the
performance of Algorithm 1 is stable when varying its hyperparameter k . Note that the y-axis has
been zoomed in to better see the differences between the curves.
5.5	Area under error vs noise level curve across datasets
In the figures shown so far, it may be difficult to compare the curves in some cases so we report an
area under the curve metric in Table 1.
Conclusions and Open Questions We conclude from our experiments and theory that the
k-NN based method (Algorithm 1) is a relatively safe method to remove problematic training
examples before training. While k-NN methods can be sensitive to the choice of k when used with
small datasets (Garcia et al., 2009), we hypothesize that with today’s large datasets one can blithely
set k to a fixed practically medium-sized value (e.g. k = 500) as done here and expect reasonable
performance. Theoretically we provided some new results for how well k-NN can identify clean
versus corrupted labels. Open theoretical questions are whether there are alternate notions of how
to characterize the difficulty of a particular configuration of corrupted examples and whether we
can provide both upper and lower learning bounds under these noise conditions.
8
Under review as a conference paper at ICLR 2020
References
Jean-Yves Audibert, Alexandre B Tsybakov, et al. Fast learning rates for plug-in classifiers. The
AnnalsofStatistics, 35(2):608-633, 2007.
Carla E. Brodley and Mark A. Freidl. Identifying mislabeled training data. Journal Artificial Intel-
ligence Research, 1999.
J Paul Brooks. Support vector machines with the ramp loss and the hard margin loss. Operations
Research, 59(2):467-479, 2011.
Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor classification.
In Advances in Neural Information Processing Systems, pp. 3437-3445, 2014.
Xu Chu, Ilhab F. Ilyas, Sanjay Krishnan, and Jiannan Wan. Data cleaning: Overview and emerging
challenges. In SIGMOD, 2016.
Thomas M Cover. Rates of convergence for nearest neighbor procedures. In Proceedings of the
Hawaii International Conference on Systems Sciences, pp. 413-415, 1968.
LUc Devroye, Laszlo Gyorfi, Adam Krzyzak, Gabor Lugosi, et al. On the strong universal Consis-
tency of nearest neighbor regression function estimates. The Annals of Statistics, 22(3):1371-
1385, 1994.
Evelyn Fix and Joseph L Hodges Jr. Discriminatory analysis-nonparametric discrimination: consis-
tency properties. Technical report, California Univ Berkeley, 1951.
Wei Gao, Xin-Yi Niu, and Zhi-Hua Zhou. On the consistency of exact and approximate nearest
neighbor with noisy data. arXiv preprint arXiv:1607.07526, 2016.
E. K. Garcia, S. Feldman, M. R. Gupta, and S. Srivastava. Completely lazy learning. IEEE Trans.
on Knowledge and DataEngineering, 2009.
Aritra Ghosh, Himanshu Kumar, and PS Sastry. Robust loss functions under label noise for deep
neural networks. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.
Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adaptation
layer. 2016.
I. Guyon, N. Matic, and V. Vapnik. Discovering informative patterns and data cleaning. In AAAI
Workshop on Knowledge Discovery in Databases, 1994.
Bo Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, Ivor Tsang, Ya Zhang, and Masashi Sugiyama.
Masking: A new perspective of noisy supervision. In Advances in Neural Information Processing
Systems, pp. 5836-5846, 2018.
Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train
deep networks on labels corrupted by severe noise. In Advances in neural information processing
systems, pp. 10456-10465, 2018.
H. Jiang, B. Kim, M. Y. Guan, and M. R. Gupta. To trust or not to trust a classifier. In Advances in
Neural Information Processing Systems (NeurIPS), 2018.
Heinrich Jiang. Non-asymptotic uniform rates of consistency for k-nn regression. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 33, pp. 3999-4006, 2019.
Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Regularizing very
deep neural networks on corrupted labels. arXiv preprint arXiv:1712.05055, 4, 2017.
Ishan Jindal, Matthew Nokleby, and Xuewen Chen. Learning deep networks from noisy labels with
dropout regularization. In 2016 IEEE 16th International Conference on Data Mining (ICDM),
pp. 967-972. IEEE, 2016.
Ashish Khetan, Zachary C Lipton, and Anima Anandkumar. Learning from noisy singly-labeled
data. arXiv preprint arXiv:1712.04577, 2017.
9
Under review as a conference paper at ICLR 2020
Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia Li. Learning from
noisy labels with distillation. In Proceedings of the IEEE International Conference on Computer
Vision ,pp.1910-1918, 2017.
Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 38(3):447-461, 2015.
Enno Mammen, Alexandre B Tsybakov, et al. Smooth discrimination analysis. The Annals of
Statistics, 27(6):1808-1829, 1999.
Hamed Masnadi-Shirazi and Nuno Vasconcelos. On the design of loss functions for classification:
theory, robustness to outliers, and savageboost. In Advances in Neural Information Processing
Systems, pp. 1049-1056, 2009.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In Advances in Neural Information Processing Systems, pp. 1196-1204, 2013.
Nicolas Papernot and Patrick McDaniel. Deep k-nearest neighbors: Towards confident, interpretable
and robust deep learning. arXiv preprint arXiv:1803.04765, 2018.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1944-1952, 2017.
Henry WJ Reeve and Ata Kaban. Fast rates for a kNN classifier robust to unknown asymmetric label
noise. arXiv preprint arXiv:1906.04542, 2019.
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for
robust deep learning. arXiv preprint arXiv:1803.09050, 2018.
David Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit. Deep learning is robust to massive
label noise. arXiv preprint arXiv:1705.10694, 2017.
Aarti Singh, Clayton Scott, Robert Nowak, et al. Adaptive Hausdorff estimation of density level
sets. The Annals of Statistics, 37(5B):2760-2782, 2009.
Charles J Stone. Consistent nonparametric regression. The Annals of Statistics, pp. 595-620, 1977.
Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. Training
convolutional networks with noisy labels. arXiv preprint arXiv:1406.2080, 2014.
Alexander B Tsybakov et al. Optimal aggregation of classifiers in statistical learning. The Annals
of Statistics, 32(1):135-166, 2004.
Arash Vahdat. Toward robustness against label noise in training deep discriminative neural networks.
In Advances in Neural Information Processing Systems, pp. 5596-5605, 2017.
Brendan Van Rooyen, Aditya Menon, and Robert C Williamson. Learning with symmetric label
noise: The importance of being unhinged. In Advances in Neural Information Processing Systems,
pp. 10-18, 2015.
Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge Belongie. Learning
from noisy large-scale datasets with minimal supervision. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 839-847, 2017.
Yizhen Wang, Somesh Jha, and Kamalika Chaudhuri. Analyzing the robustness of nearest neighbors
to adversarial examples. In International Conference on Machine Learning, pp. 5120-5129, 2018.
D. Wilson. Asymptotic properties of nearest neighbor rules using edited data. IEEE Trans. on
Systems, Man and Cybernetics, 1972.
Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy
labeled data for image classification. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 2691-2699, 2015.
Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels. In Advances in Neural Information Processing Systems, pp. 8778-8788, 2018.
10
Under review as a conference paper at ICLR 2020
A Proofs
A. 1 Supporting theoretical results
The following bounds rk (x) uniformly in x ∈ X .
Lemma 1 (Lemma 2 of Jiang (2019)). Thefollowing holds with probability at least 1 一 δ∕2. If
28 ∙ D log2(4∕δ) ∙ log n ≤ k ≤
2 ∙ ω ∙ pχ,o ∙ VD ∙ rD ∙ n,
then supx∈X rk (x) ≤
2k
ω∙VD ∙n∙pχ,o
1/D
, where vD is the volume of the unit ball in RD.
The next result bounds the number of distinct k-NN sets over X.
Lemma 2 (Lemma 3 of Jiang (2019)). Let M be the number of distinct k-NN sets over X, that is,
M :二 |{Nk(x) : X ∈ X}|. Then M ≤ D ∙ nD.
A.2 MINIMUM k-NN SPREAD
We propose a more notion of how spread out a set of points is than S2 which will be used in the
theoretical analysis. This will allow us to more precisely characterize how difficult a configuration
of incorrectly labeled examples will be to work with in the k-NN context. For example, if such
examples are spread out far apart, then there will be many correctly labeled examples nearby for the
k-NN approach to identify the incorrectly labeled examples. On the other hand, if the corrupted ex-
amples are all close together, then it will be more difficult to identify them without many uncorrected
examples in that region. To this end, we define the minimum k-NN spread:
Definition 4 (minimum k-NN spread).
Sk(C) := minrk(x, C),
x∈C
where rk(x, C) denotes the distance from x to the k-th closest neighbor in C.
Note that this definition is consistent with the earlier definition of S2 .
A.3 Proof of Theorem 1
Proof of Theorem 1. Let τ, γ, > 0 be quantities that will be determined later. Suppose that for
some x ∈ Xδ, We have rk(x) ≤ T and S](ι-γ>kj(C) ≥ T. Then, at least 2 + Y fraction of
the points within x’s k-nearest neighbors are not in the corrupted set C. Let Ax := Nk(x)\C,
that is, the k-nearest neighbors of x that are not in C. Then it is clear that Ax is a k0-nearest
neighbor set of X relative to X\C for some ko ≥ d(2 + γ) ∙ k]. We have that Ax ⊆ Xδ ㊉ T
where A ㊉ r := {x ∈ X : infa∈A |x 一 a| ≤ r}. Let us consider without loss of generality that
η(x) ≥ 1 + ∆ (call this set Xδ,+). The case X△,- := {x ∈ Xδ : η(χ) ≤ 1 一 ∆} follows by
symmetry. Thus, we have η(χ0) ≥ 1 + ∆ 一 CaTa for all χ0 ∈ Ax. By Hoeffing,s inequality, we
have
P (^jA^ ^X y(x0) < 2 +△ 一 CaTa 一 E) ≤ eXp(-2c2 ∙ k0),
where y(x) is the label corresponding to sample x. By Lemma 2, we have that there are at most
D ∙ nD such ko-nearest neighbor sets across all ko in X\C. That is, this is also a bound on the
number of distinct Ax for x ∈ X. Therefore, ifwe set
E=
D log n + log(4D∕δ)
(1 + 2γ) ∙ k
then by union bound, we have that
P I inf ɪ	X	y(x0)	< 1+∆ — CaTa	一 e ) ≤ δ.
%∈XJ IAxI	x0∈A	2	α	≤ 4
x ∈Ax
11
Under review as a conference paper at ICLR 2020
and thus, with probability at least 1 - δ∕4, We have that ∣a^ Pχ∕∈A, y(χ0) ≥ 2 + ∆ -
CaTa - C uniformly over x ∈ X∆,+. Similarly, with probability at least 1 - δ∕4 we have that
Pχo∈Aκ y(x0) ≤ 1 - △ + Cata + c uniformly over X ∈ Xδ,-.
Hence, in order for k-nearest neighbor prediction to predict the Bayes-optimal label on X∆, it
suffices that
k0 (2 + δ - CaTT - ) ≥ 2 .
Since k0 ≥ (1 + Y) ∙ k, we have that the above holds if
1 - 2γ
δ ≥ CaTT + e +E⅛.
We now choose the values of T, γ, to upper bound each of the terms on the R.H.S. by ∆∕3 so that
the above holds.
We can bound the last term by ∆∕3 by setting:
1
2
γ
3 - 2∆
3 + 2∆.
Next, taking
3(3 + 2∆)
k ≥	2∆2
(D log n + log(4D∕δ)) ,
we have that ≤ ∆∕3. Now, by Lemma 1, we have that setting
2k
1/D
T ∖ω ∙ VD ∙ n ∙ pχ,o )
gives us that rk(x) ≤ T for all x ∈ X with probability at least 1 - δ∕2. It thus suffices to take
1	∆ D/a
k ≤ 2 3 3-C- 1	∙ω ∙ VD ∙ Px,0 ∙n
so that CaTα ≤ ∆∕3. Now in order for S](ι-γ>kj (C) ≥ T, it suffices to have S2(C) ≥ T. This
can be accomplished by having the following hold:
k ≤ 1 ∙ S2(C)D ∙ ω ∙ VD ∙ pχ,o ∙ n.
Thus, there exists positive constants Kl and Ku depending only on F such that if
Ku ∙ ∆12 ∙ (log2(1∕δ) ∙ logn) ≤ k ≤ Ku ∙ min{S2(C)D, ∆"α}∙ n,
then the desired conditions hold.
□
A.4 Proof of Theorem 2
Proof of Theorem 2. The proof begins in the same way as the proof of Theorem 1. As before, let
T, γ , > 0 be quantities that will be determined later. Like before, we are reduced to showing
1	- 2γ
∆ ≥ CaTT + C +-------------,
≥ a + + 2(1 + 2γ),
as long as the conditions for Lemma 1 and 2 hold and S2 (x) ≥ T and rk (x) ≤ T where we choose
C=
D log n + log(4D∕δ)
(1 + 2γ) ∙ k
1 3-2∆
Y = 2 ∙ 3 + 2∆, T
2k
ω ∙ VD ∙ n ∙ pχ,o
1/D
12
Under review as a conference paper at ICLR 2020
These conditions hold for some Ku and Kl depending on F. Then we are reduced to having
2	a、/ D log n + log(4D∕δ)
3δ ≥V —(1 + 2γ) ∙ k +Cα
2k	「
ω ∙ VD ∙ n ∙ Px,oJ
Since γ ≥ 0, it suffices to have
3δ ≥ J
D log n + log(4D∕δ)
+ Cα
2k	「
ω ∙ VD ∙ n ∙ pχ,o )
k
The desired form for ∆ clearly follows for some choice of K depending only onD, ω, pX,0 , Cα, all
of which depend only on F.
Finally, we must ensure that [(2 - γ) ∙ k] ≥ 2 so that S[( ι _7＞句(。)≥ S2(x). Given the expression
for γ, it is equivalent to have [(言宏)∙ k] ≥ 2. It suffices to show that k ≥ 3(3+20. Given the
form of ∆ in terms of n and k, we see that it suffices to have that
k ≥ 9 ∙ K ∙
-2
log n + log(1∕δ)
k
+
which holds when k ≥ Ko ∙ n2a/(2a+D) for some Ko depending only on F, as desired.	□
A.5 Proof of Theorem 3
Proof of Theorem 3. The first part follows from Theorem 2. For the second part, we have by Theo-
rem 2 that if x ∈ X ∆ , then the k-NN classifier and the Bayes-optimal classifier match with proba-
bility 1 - δ uniformly. Thus, we have
RX - R ≤ P(X ∈ Xδ) (EF[gk(χ) = y|x ∈ Xδ] - EF[g*(χ) = y|x ∈ Xδ])
≤ Cβ∙ ∆β ∙ (EF [gk (x) = y|x ∈ X δ] - EF [g*(x) = y|x ∈ X δ]) ≤。产∙ ∆β ∙ 2∆.
The result follows immediately from Theorem 2.	□
B Hard Flip Permutations
For Fashion MNIST We hard flip by swapping the following classes: TSHIRT J SHIRT, TROUSER
J DRESS, PULLOVER J COAT, SANDAL J BAG, SNEAKER J ANKLEBOOT. For CI-
FAR10 we swap the pairs: TRUCK J AUTOMOBILE, BIRD J AIRPLANE, DEER J HORSE,
CAT J DOG, FROG J SHIP. For CIFAR100, we hard flip circularly (i.e. π(i) = (i+1) mod K)
within each of the 20 superclasses. For all other datasets, we hard flip circularly.
C Additional Plots
We provide the plots that were ommitted from the main text due to space constraints.
13
Under review as a conference paper at ICLR 2020
Figure 7: Plots for UCI Phonemes dataset at 10, 20% clean data and all corruption types.
」0」」山.MS①-L
山.MS①-L
」0」」山-MS①一 」0」」山.MS①F-
Figure 8: Plots for UCI Letters dataset at 5, 10, 20% clean data and all corruption types.
14
Under review as a conference paper at ICLR 2020
山-MS①一 山.MS①I-
山-MS①一 山.MS①-L
UC Wilt - Uniform - 10%
UC1I Wilt - Uniform - 20%
UCI Wit-Flip-20%
Figure 10: Plots for UCI Parkinsons dataset at 5, 10, 20% clean data and all corruption types.
Noise Rate
Noise Rate
UCI Wilt - Flip - 10%
Wilt-Hard Flip - 10%
Figure 9: Plots for UCI Wilt dataset at 5, 10, 20% clean data and all corruption types.
Parkinsons -l
15
Under review as a conference paper at ICLR 2020
Figure 11: Plots for UCI Seeds dataset at 5, 10, 20% clean data and all corruption types.
Figure 12: Plots for UCI Iris dataset at 5, 10, 20% clean data and all corruption types.
16
Under review as a conference paper at ICLR 2020
Figure 13: Plots for MNIST at 5, 10, 20% clean data and all corruption types.
Noise Rate
Noise Rate
MNIST-FIip - 20%
MNlST-Hard F1Iip - 20%
Figure 14: Plots for Fashion MNIST at 10, 20% clean data and all corruption types.
17
Under review as a conference paper at ICLR 2020
Figure 15: Plots for CIFAR10 at 10, 20% clean data and all corruption types.
Figure 16: Plots for CIFAR100 at 5, 10, 20% clean data and all corruption types.
18
Under review as a conference paper at ICLR 2020
Figure 17: Plots for SVHN at 10, 20% clean data and all corruption types.
」0」」山4S①一
19