Under review as a conference paper at ICLR 2020
Entropy Penalty: Towards Generalization Be-
yond the IID Assumption
Anonymous authors
Paper under double-blind review
Ab stract
It has been shown that instead of learning actual object features, deep networks
tend to exploit non-robust (spurious) discriminative features that are shared be-
tween training and test sets. Therefore, while they achieve state of the art per-
formance on such test sets, they achieve poor generalization on out of distribu-
tion (OOD) samples where the IID (independent, identical distribution) assump-
tion breaks and the distribution of non-robust features shifts. Through theoretical
and empirical analysis, we show that this happens because maximum likelihood
training (without appropriate regularization) leads the model to depend on all the
correlations (including spurious ones) present between inputs and targets in the
dataset. We then show evidence that the information bottleneck (IB) principle can
address this problem. To do so, we propose a regularization approach based on IB
called Entropy Penalty, that reduces the model's dependence on spurious features-
features corresponding to such spurious correlations. This allows deep networks
trained with Entropy Penalty to generalize well even under distribution shift of
spurious features. As a controlled test-bed for evaluating our claim, we train deep
networks with Entropy Penalty on a colored MNIST (C-MNIST) dataset and show
that it is able to generalize well on vanilla MNIST, MNIST-M and SVHN datasets
in addition to an OOD version of C-MNIST itself. The baseline regularization
methods we compare against fail to generalize on this test-bed.
1 Introduction
It is now known that deep networks trained on clean training data (without proper regularization)
often learn spurious (non-robust) features which are features that can discriminate between classes
but do not align with human perception (Jo & Bengio, 2017; Geirhos et al., 2018a; Tsipras et al.,
2018; Ilyas et al., 2019). An example of non-robust feature is the presence of desert in camel images,
which may correlate well with this object class. More realistically, models can learn to exploit the
abundance of input-target correlations present in datasets, not all of which may be invariant under
different environments. Interestingly, such classifiers can achieve good performance on test sets
which share the same non-robust features. However, due to this exploitation, these classifiers per-
form poorly under distribution shift (Geirhos et al., 2018a; Hendrycks & Dietterich, 2019) because
it violates the IID assumption which is the foundation of existing generalization theory (Bartlett &
Mendelson, 2002; McAllester, 1999b;a).
The research community has approached this problem from different directions. In part of domain
adaptation literature (Eg. Ganin & Lempitsky (2014)), the goal is to adapt a model trained on a
source domain (often using unlabeled data) so that its performance improves on a target domain that
contains the same set of target classes but under a distribution shift. There has also been research
on causal discovery (Hoyer et al., 2009; Janzing et al., 2009; Lopez-Paz et al., 2017; Kilbertus et al.,
2018) where the problem is formulated as identifying the causal relation between random variables.
This framework may potentially then be used to train a model that only depends on the relevant
features. However, it is often hard to discover causal structure in realistic settings. Adversarial
training (Goodfellow et al., 2014; Madry et al., 2017) on the other hand aims to learn models whose
predictions are invariant under small perturbations that are humanly imperceptible. Thus adversarial
training can be seen as the worst-case distribution shift in the local proximity of the original training
distribution.
1
Under review as a conference paper at ICLR 2020
Our goal is different from the aforementioned approaches. We aim to directly learn a classification
model using labeled data which is capable of generalizing well under input distribution shift (not
constrained to being locally) without making any changes to the model during test time. Thus our
goal is more aligned with the recently proposed Invariant Risk Minimization (Arjovsky et al., 2019),
but imposes less constraints on the data collection process. For a detailed discussion on related
work, see section 4. Our contributions are as follows:
1.	Our theoretical and empirical analysis shows that models trained with maximum likelihood objec-
tive without appropriate regularization can, in general, learn to exploit/depend on all the correlations
present between inputs and targets in the training set, leading to non-robust representations. While
this representation may allow them to yield state-of-the-art performance on test sets whose distribu-
tion is identical to the training set, they would perform poorly when the distribution of non-robust
features shift. This effect is not mitigated by a larger training set containing more variations between
samples. Thus based on our analysis, it should not be surprising that deep networks trained on image
datasets show poor performance under input perturbations and (in general) input distribution shifts
as discussed in numerous recent papers (Hendrycks & Dietterich, 2019; Jo & Bengio, 2017; Geirhos
et al., 2018b;a).
2.	We provide evidence showing that the information bottleneck (IB) principle (Tishby et al., 2000;
Tishby & Zaslavsky, 2015) is capable of addressing the out of distribution generalization prob-
lem. Specifically, our proposal, that we call Entropy Penalty is based on the IB principle and aims
at learning a representation that throws away maximum possible information about the input dis-
tribution while achieving the goal of correctly predicting targets. Intuitively, doing so makes the
representation agnostic to non-robust features in the input, thus allowing the model’s predictions to
be invariant under a shift of the distribution of such features during test time.
3.	We show experimental results using Entropy Penalty in which a deep network trained on a colored
version of MNIST dataset (see appendix A for samples) is able to generalize well on vanilla MNIST,
MNIST-M, SVHN and a distribution-shifted version of the colored MNIST dataset itself. We note
that most of the baseline methods failed to the extent of achieving performance close to random
chance.
2 How to Generalize under Distribution Shift?
In order to approach a solution to our problem, we observe that the out of distribution nature of
samples during test time arises because certain aspects of the input change even though the input
still corresponds to the same pool of targets as seen during training. An instance of this change
would be seeing a camel in the city (during test time) which has dramatically different background
features compared to the desert (seen during training). Thus, if a model is trained to depend only
on camel features (which defines the class more universally) and ignore other aspects, a shift in the
distribution of such aspects will no longer affect the model’s decision during test time.
We note that the above intuition is encapsulated by the information bottleneck learning principle
(Tishby et al., 2000; Tishby & Zaslavsky, 2015) which minimizes the following objective,
LIB(θ) = -I(fθ(X),Y)+βI(fθ(X),X)	(1)
where X and Y represent the input and target (often class label) random variables, fθ(.) denotes a
deterministic model with learnable parameters θ, and β is a hyper-parameter. While the first term
effectively maximizes the training data likelihood, it is the second term that regularizes the model
representations to become invariant to non-robust features that are not dominantly present in all
samples by minimizing mutual information between input and representation random variables. We
now derive Entropy Penalty which is equivalent to the IB regularization for deterministic models.
We note thatI(fθ(X), Y) and I(fθ(X), X) can be written equivalently as follows,
I(fθ (X), Y) = H(Y)-H(Y fθ (X))	⑵
I(fθ(X), X) = H(fθ(X))- H(fθ(X)|X)	(3)
where H(.) denotes entropy and H(.|.) denotes conditional entropy. Note that H(Y) in Eq. 2 is
fixed and H(Y∣fθ(X)) is the same as the maximum likelihood loss. Secondly, since fθ(.) is a de-
terministic function, the conditional entropy H(fθ(X)|X) is fixed. Thus We only need to minimize
2
Under review as a conference paper at ICLR 2020
the entropy of the learned representation H(fθ(X)) in Eq. 3. Thus in the case of deterministic fθ(.),
the minimization in IB objective can be equivalently framed as,
ʌ ,. .. ..
LIB (θ)= H(Y∣fθ (X))+ βH(fθ (X))	(4)
Therefore, we call this general form of regularization Entropy Penalty. Note that while the first term
(data likelihood) is easy to optimize, the second term is often intractable for continuous high dimen-
sional distributions. The following proposition shows an equivalent form of the above objective.
ʌ .. ..
Proposition 1 LIB (θ) = (1 - β)H(Y∣fθ (X)) + βH(fθ (X)Y) + C
where C is a positive constant for discrete Y, independent of θ.
The benefit of the above form for LIB (θ) instead of Eq. 4 is that it can often be easier to
model the conditional Pr(fθ(X)|Y) compared to the marginal Pr(fθ(X)). For instance, if We
assume Pr(fθ(X)|Y) is Gaussian for all class labels Y, the entropy of the conditional distribution
Pr(fθ(X)|Y) has a closed-form solution given by 0.5log(2πeσY), where σγ denotes the variance
of the class conditional Gaussian distribution of fθ(X) for class Y.
On a practical note, when applying entropy penalty to deep networks, we found that it was not effec-
tive when applied to the last layer representations. However, applying this penalty to the first hidden
layer representation improved performance under distribution shift. While we do not have a com-
plete explanation for this behavior, we conjecture that this could be because of the data processing
inequality for deep networks (Tishby & Zaslavsky, 2015) which states,
I(h1, X) ≥I(h2,X) ≥ ... ≥I(hL,X)	(5)
where hi denotes the ith hidden layer representation and L is the depth of the network. Writing
mutual information in terms of entropy and conditional entropy, and taking advantage of the fact
that the conditional entropy term is fixed for a deterministic conditional, we have that,
H(h1) ≥ H(h2) ≥ ...≥H(hL)	(6)
Thus entropy is larger for lower layers. Further, minimizing entropy for higher layers does not
ensure entropy is minimized for lower layers due to the above inequalities. Thus, any excess infor-
mation about the input captured by the first layer gets propagated to the higher layers, the effect of
which may get amplified under distribution shift if entropy minimization at last layer is not done ap-
propriately. For all experiments conducted using entropy penalty (EP), we use the aforementioned
Gaussian assumption on the representation of the first hidden layer and compute its class condi-
tional variance in order to minimize the conditional entropy. Specifically, let h(x) represent the first
hidden layer representation of an input x (before non-linearity), then we implement EP as,
K
REP = E Ex~D(x∣ y=k) [(h(x) - μk)2]	⑺
k=1
where μk := Eχ~D(χ∣y=k)[(h(x)] and D denotes the data distribution. In practice, we re-
place expectation with average over mini-batch samples. For CNNs a mini-batch has dimensions
(B, C, H, W), where we denote B- batch size, C- channels, H- height, W- width. In this case, we
reshape this tensor to take the shape (B × H × W, C) and treat each row as a hidden vector h.
2.1	Theoretical Analysis
We now theoretically study the behavior of IB principle on two synthetic datasets designed to pro-
vide insights into the invariant representation that IB helps in learning, and simultaneously reveals
why it should not be surprising that models trained using maximum likelihood (without appropri-
ate regularization) perform poorly under input perturbations and distribution shift during test time.
Although these analyses are done for linear regression, in each case, we empirically verify these
predictions on deep ReLU networks. For our analysis, we use the following objective,
J (θ) = E[(fθ (x) — y)2]+ λkθk2 + ^eH(f。(X)Iy)	(8)
2πe
Here the IB regularization H(fθ(x)|y) is kept in the exponent for the ease of analytical simplicity.
Also, setting β = 0 yields our baseline case without the IB regularization.
3
Under review as a conference paper at ICLR 2020
2.1.1	Synthetic Dataset A
Minimizing the class conditional entropy forces the distribution of neural network representation
corresponding to each class to have the minimum amount of information about the input data.
Therefore combining this regularization with the traditional classification losses (Eg. cross-entropy)
should encourage the neural network to pick features that are dominantly present in class samples
and able to discriminate between samples from different classes. To formalize the above intuition,
we consider the following synthetic data generating process where the data samples x ∈ Rd and
labels y are sampled from the following distribution,
y 〜{-11}	Xi 〜∫N(y,σ2)	WithPrObability Pi
,	N(-y, σ2 ) with probability 1 - pi
where i ∈ {1,2, ∙∙∙ ,d}, y is drawn with uniform probability, and X = [xι, x2,…,Xd] is a data
samPle. Also, all xi |y are indePendent of each other. Thus dePending on the value of pi , a feature
Xi has a small or large amount of information about the label y. Specifically, values of pi close to
0.5 do not tell us anything about the value of y while values close to 0 and 1 can reliably predict
its value. Here we make the assumption that features with pi closer to 0.5 are non-robust features
whose distribution may shift during test time, while features with pi closer to 0 and 1 are robust ones.
Thus we would ideally want a trained model to be insensitive to non-robust features. The theorem
below shows how the model parameters depend on input dimensions for the optimal parameters
when training a linear regression model fθ(X) := θTX using the IB objective.
Theorem 1 Let θ* be the minimizer of J(θ) in Eq. 8 where we have used synthetic dataset A. Then
for a large enough d, θ* = M-1∣2p - 1|, where M := Σ + λI + β(σ2I + 4diag(P Θ (1 - P))),
such that Σ is a positive definite matrix if1 pi 6∈ {0, 0.5, 1} for all i.
As an implication of the above statement, since M-1 is a full rank matrix, aside from the effects
due to Σ (which is data dependent and beyond our control), θ* can in general be non-zero for all
input and output correlations. This is especially the case when β = 0 (no IB regularization). When
using a sufficiently large β, we find that θ* gets reduced for larger values of pi(1 - Pi), i.e., whenPi
is closer to 0.5. Thus the IB objective can help suppress dependence of the learned model on non-
robust (low correlation) features. Although this analysis is for linear regression, it provides evidence
that it should not be surprising that deep networks trained with maximum likelihood objective with-
out an appropriate regularization could exhibit a similar behavior. Note that this is not a problem
when training and test set are sampled IID from the same distribution, but only becomes one under
distribution shift. Also note that since the analysis depends on expectation over data distribution, a
larger training set cannot solve our problem of avoiding dependence on non-robust features.
To verify that the behavior studied above also holds for deep networks, we conduct experiments with
both linear and deep models on samples drawn randomly from synthetic dataset A. Details of the
experimental setup can be found in appendix B.
In figure 1 (left), we plot the parameters θ* vs. Pi for the linear regression model. Since the same
analysis cannot be done for deep networks, we use the perspective that the output-input sensitivity
s*, where Si := Ex dfθχ(X) , is equal to θ* for linear regression. So for deep networks, we plot
Si vs. pi instead as shown in figure 1 (right). In both models, we normalize the sensitivity values so
that the maximum value is 1 for the ease of comparison across different β values. Both for linear and
deep models, we find that the sensitivity profile goes to 0 away from Pi = 0 and 1 when applying
the IB regularization with larger values of coefficients β ; this effect being more dramatic for deep
networks. Thus the IB regularization helps suppress the dependence of model on non-robust (low
correlation) features whose distribution may shift during test time, thus allowing its predictions to
be invariant to such shifts.
Here we additionally note that for a linear regression model, while sensitivity is same as the model
parameter θ, it is not merely the first-order sensitivity that gets suppressed for certain input dimen-
sions, the output becomes invariant to such dimensions altogether. Although this argument does
not necessarily apply to deep networks, note that the IB regularization itself enforces a more gen-
1This assumption is needed due to technicality.
4
Under review as a conference paper at ICLR 2020
Figure 1: Sensitivity s* of output fθ* (x) With respect to input dimensions Xi vs. the probability Pi
(controlling correlation between input dimension i and target) for synthetic dataset A (Eq. 9). Left
plot shows θi (same as sensitivity) computed for a trained linear model. Right plot shows sensitivity
computed for a trained MLP. IB regularization acts as a filter, suppressing the sensitivity of both
these models to weak correlation features (pi close to 0.5).
eral condition of finding low entropy representation rather than merely suppressing input sensitivity.
Hence the implications of IB could be more general than what our sensitivity analysis shows.
2.1.2 Synthetic Dataset B
Using the same intuition that small class conditional entropy induces learned representations to have
less uncertainty, given two features that can equally differentiate between classes in expectation, the
IB objective should pick the one with smaller variance. To formalize this intuition, we consider the
following binary classification problem where the data samples x ∈ Rd and labels y are sampled
from the following distribution,
N (y, σ2)	with probability pi
y ,	i N(y, kσ2) with probability 1 - pi
where i ∈ {1,2,…，d}, y is drawn with uniform probability, and X = [xι, x2,…，Xd] is a data
sample. Once again, all xi |y are independent of each other. Thus depending on the value of pi and
k, a feature xi has a small or large variance. We would ideally like the model to avoid dependence
on dimensions with high variance because they are non-robust and a minor shift in their distribution
during test time can affect the model’s decision by a large amount. The theorem below shows how
the model parameters depend on input dimensions for the optimal parameters when training a linear
regression model fθ (X) := θTX using the IB objective.
Theorem 2 Let θ* be the minimizer of J(θ) in Eq. 8 where we have used synthetic dataset B. Then
for a large enough d, θ* = MTL where, M := Σ + λI + βσ2diag(P + k(1 一 P)), such that Σ is
a positive definite matrix.
Once again, we find that θi is non-zero for all dimensions of the input. Assume without loss of
generality that k > 1. Then using a sufficiently large β would make the value of θi approach 0 if Pi
is close to 0. In other words, IB regularization forces the model to be less sensitive to features with
high variance. Thus, such a model’s prediction will not be affected significantly under a shift of the
distribution of high variance features during test time.
To study the extent of similarity of this behavior between linear regression and deep networks,
we once again conduct experiments with both these models on a finite number of samples drawn
randomly from synthetic dataset B with k = 10 and σ2 = 0.001. The rest of the details regarding
dataset generation and models and optimization are identical to what was used in section 2.1.1.
The sensitivity s* vs. Pi plots are shown in figure 2 (left) for linear regression and figure 2 (right) for
MLP. In the case of linear regression s* = θ*. For both linear regression and MLP, the model,s sen-
sitivity to all features are high irrespective of Pi when trained without the IB regularization (β = 0)
and this is especially more so for the MLP. On the other hand, when training with IB regularization,
we find that a larger β forces the models to be less sensitive to input feature dimensions with higher
variance (which correspond to Pi = 0). The discussion around the generality of the IB regularization
beyond sensitivity analysis is same as that discussed in section 2.1.1.
5
Under review as a conference paper at ICLR 2020
Pi
Pi
Figure 2: Sensitivity Si of output fθ* (x) with respect to input dimensions Xi vs. the probability
pi (deciding the choice between feature with variance σ2 vs. 10σ2) for synthetic dataset B (Eq.
10). Left plot shows θii (same as sensitivity) computed for a trained linear model. Right plot shows
sensitivity computed for a trained MLP. IB regularization suppresses the sensitivity of both these
models to large variance features (pi close to 0).
3	Experiments with Data Distribution S hift
The experiments below are aimed at investigating: 1. the ability of relevant existing methods to
generalize under distribution shift; 2. how well can the proposed method generalize under this shift.
Details not mentioned in the main text can be found in appendix B.
Datasets: We use a colored version of the MNIST dataset (see appendix A for dataset samples and
details) for experiment 1, and MNIST-M (Ganin et al., 2016), SVHN (Netzer et al., 2011), MNIST
(LeCun & Cortes, 2010) in addition to C-MNIST for experiment 2. All image pixels lie in 0-1
range and are not normalized. The reason for this is that since we are interested in out of distribution
(OOD) classification, the normalization constants of training distribution and OOD may be different,
in which case data normalized with different statistics cannot be handled by the same network easily.
Other Details: We use ResNet-56 (He et al., 2016b) in all our experiments. We use Adam optimizer
(Kingma & Ba, 2014) with batch size 128 and weight decay 0.0001 for all experiments unless
specified otherwise. We do not use batch normalization in any experiment except for the adaptive
batch normalization baseline method. Discussion and experiments around batch normalization can
be found in appendix D. We do not use any bias parameter in the network because we found it
led to less overfitting overall. For all configurations specified for proposed method and baseline
methods below, the hyper-parameter learning rate was chosen from {0.0001, 0.001} unless specified
otherwise. For entropy penalty, the regularization coefficient is chosen from {0.1, 1, 10}.
Baseline methods:
1.	Vanilla maximum likelihood (MLE) training: Since there are no regularization coefficients in this
case, we search over batch sizes from {32, 64, 128} for each learning rate value.
2.	Variational bottleneck method (VIB, Alemi et al. (2016)) is an existing approximation to the IB
objective that uses a non-deterministic network. We therefore investigate its behavior under distribu-
tion shift at test time. The regularization coefficient for VIB is chosen from the set {0.01, 0.1, 1, 5}.
3.	Clean logit pairing (CLP): Proposed in Kannan et al. (2018), this method minimizes the `2 norm
of the difference between the logits of different samples. As shown in proposition 3 (in appendix),
minimizing this `2 norm is equivalent to minimizing the entropy of the distribution in logit space
under the assumption that this distribution is Gaussian. In contrast entropy penalty minimizes the
entropy of the class conditional distribution of the first hidden layer. Due to this similarity, we
consider CLP a baseline. The regularization coefficient for CLP is chosen from {0.1, 0.5, 1, 10}.
4.	Projected gradient descent (PGD) based adversarial training (Madry et al., 2017) has been shown
to yield human interpretable features. This makes it a good candidate for investigation. For PGD,
`inf perturbation is used with a maximum perturbation from the set {8, 12, 16, 20} and step size of
2, where all these numbers are divided by 255 since the input is normalized to lie in [0, 1] . The num-
ber of PGD steps is chosen from the set {20, 50}. We randomly choose 12 different configurations
out of these combinations. 5
5. Adversarial logit pairing (ALP, Kannan et al. (2018)) is another approach for adversarial ro-
bustness and an alternative to PGD. Since it has the most number of hyper-parameters, we tried a
larger number of configurations for this baseline. Specifically, we use `inf norm with a maximum
6
Under review as a conference paper at ICLR 2020
>U23UU<tiωJ.
100
80
60
40
20
0
Hyper-parameter Configurations
---Entropy Penalty
PGD
-VIB
---Input Noise
一 CLP
AdaBN
MLE
ALP
Dataset Accuracy
C-MNIST	96.88
MNIST	93.75
MNIST-M	85.94
SVHN	60.94
Table 1: Out of distribution
performance on test sets using
a model trained with Entropy
Penalty on C-MNIST dataset.
Figure 3: Performance on the distribution shifted test set of C-
MNIST for various methods trained on C-MNIST training set.
See figure 5 in appendix for samples from C-MNIST dataset.
Epochs
>U2DUU<tttυl
----Entropy Penalty ------- VIB	---- CLP ---------- MLE
PGD	- Input Noise - AdaBN - ALP
Figure 4: Baseline methods severely overfit color features in the C-MNIST training set leading to
near 100% accuracy on C-MNIST validation set but close to chance performance on the distribution
shifted C-MNIST test set.
perturbation from the set {8, 16, 20} and step size of 2, where all these numbers are divided by
255 since the input is normalized to lie in [0, 1] . The number of PGD steps is chosen from the
set {20, 50}. The regularization coefficient is chosen from {0.1, 1, 10}. We randomly choose 15
different configurations out of these combinations.
6.	Gaussian Input Noise has been shown to have a similar effect as that from adversarial training
(Ford et al., 2019) with even better performance in certain cases. We choose Gaussian input noise
with standard deviation from the set {0.05, 0.1, 0.2, 0.3}.
7.	Adaptive batch normalization (AdaBN, Li et al. (2016)) has been proposed as a simple way to
achieve domain adaptation in which the running statistics of batch normalization are updated with
the statistics of the target domain data. Although this does not fall within our goal of learning a
model that does not need any adaption during test time, we investigate this method in experiment 1
due to its simplicity. Since there are no regularization coefficients in this case, we search over batch
sizes from {32, 64, 128} for each learning rate value.
Experiment 1: In this experiment, we train ResNet-56 on the colored MNIST dataset using the
baseline methods and entropy regularization, and test the performance of the trained models on the
distribution shifted test set of colored MNIST dataset in each case. For each method, we record the
best test performance for each hyper-parameter configuration used, and after sorting these numbers
across all configurations, we plot them in figure 3. We find that all the baseline methods tend
to severely overfit the non-robust color features in C-MNIST leading to poor performance on the
distribution shifted test set of C-MNIST. Figure 4 further confirms this by plotting the validation and
test accuracy vs. epoch for all methods for one of the hyper-parameter configurations (see appendix
C for details). Clearly, baseline methods achieve near 100% accuracy on C-MNIST validation set
but close to chance performance on the distribution shifted C-MNIST test set, showing that these
methods have overfitted the color features. Entropy penalty is able to avoid this dependence.
Surprisingly even VIB suffers from this issue. This could be because of improper minimization of
the information bottleneck (IB) regularization, which could in turn be due to 1. the same reason due
to which entropy penalty does not work when applied to higher layers; 2. VIB minimizes an upper
bound of the original IB objective. Entropy penalty is able to overcome these difficulties.
7
Under review as a conference paper at ICLR 2020
Experiments 2: In this experiment, we hand-pick the model trained with entropy penalty on C-
MNIST in experiment 1 above, such that it simultaneously performs well on SVHN, MNIST-M
and MNIST datasets (see section 5 for discussion on this). We used the C-MNIST test set for early
stopping. These performances are shown in table 1. We note that it is non-trivial fora single model to
perform well on all datasets with such distribution shifts without any domain adaptation, especially
given it is trained on a dataset on which all baseline methods severely overfit to non-robust features.
4	Related Work
Invariant Risk Minimization: The goal of IRM (Arjovsky et al., 2019) is to achieve out of dis-
tribution generalization by learning representations such that there exists a predictor (Eg. a linear
classifier) that is simultaneously optimal across all environments. IRM achieves this goal by learning
(stable) features whose correlation with target is invariant across different environments. In other
words, if there are multiple features that correlate with label, then IRM aims to learn the feature
which has the same degree of correlation with label irrespective of the environment, while ignoring
other features. If the representation induced by such stable features, among others, simultaneously
also contain the minimum amount of information about the input, then such representations can
alternatively be learned using the information bottleneck principle. Thus it boils down to which
strategy forms a better inductive bias for handling distribution shift. On a practical note, the main
difference between IRM and our proposal is that IRM requires the explicit knowledge of the envi-
ronment from which each training data is sampled from. Our approach does not have this restriction.
Due to this, we cannot evaluate IRM in our experimental setting.
Adversarial Training: There is an abundance of literature around robust optimization (Wald, 1945;
Ben-Tal et al., 2009) and adversarial training (Goodfellow et al., 2014; Madry et al., 2017) which
study robustness of models to small perturbations around input samples and are often studied us-
ing first order methods. Such perturbations can be seen as the worst case distribution shift in the
local proximity of the original training distribution. Further, Tsipras et al. (2018) discusses that the
representations learned by adversarially trained deep network are more human interpretable. These
factors make it a good candidate for investigating its behavior under distribution shift.
Our theoretical analysis has similarities to this line of work, but our goal and conclusions are broader.
Specifically, for linear regression, we derive the optimal parameter value analytically under the infor-
mation bottleneck objective. Since, the value of parameters is same as the output-input Sensitivity-
the derivative of this model’s output (not loss) with respect to its input, we plot sensitivity in the
case of deep networks because parameters do not correspond to input dimensions for deep networks.
Nonetheless, this is a limitation of our analysis and not of the information bottleneck principle be-
cause its objective of minimizing representation entropy is more general than reducing first order
sensitivity of the model.
Domain Adaptation: Domain adaptation (Wang & Deng, 2018; Patel et al., 2014) addresses the
problem of distribution shift between source and target domain, and has attracted considerable at-
tention in computer vision, NLP and speech communities (Kulis et al., 2011; Blitzer et al., 2007;
Hosseini-Asl et al., 2018). Some of these methods address this issue by aligning the two distribu-
tions (Jiang & Zhai, 2007; Bruzzone & Marconcini, 2009), while others by making use of adversarial
training (Ganin & Lempitsky, 2014; Ganin et al., 2016) and auxilliary losses (Ghifary et al., 2015;
Heet al., 2016a). A common characteristic of all these methods is that they require labeled/unlabeled
target domain data during the training process. This is not necessary in the information bottleneck
approach, which makes it more flexible.
5	Discussion and Conclusion
Based on our analysis, it appears that deep networks are good at achieving state-of-the-art general-
ization in common settings because a. they are able to exploit all the correlations present between
inputs and targets; and b. the IID assumption holds between training and test sets. However, these
attributes also make them perform poorly on distribution shifted test sets. Our analysis provides ev-
idence that the information bottleneck (IB) principle can be a potential remedy to this problem. We
reached this conclusion by introducing entropy penalty- an equivalent form of the IB regularization
for deterministic networks, and showing it generalizes well on out of distribution test sets.
8
Under review as a conference paper at ICLR 2020
However, note that while entropy penalty itself is a general form of regularization, our proposed
implementation of entropy penalty has certain limitations and it lacks of complete theoretical under-
standing. Specifically,
1.	The Gaussian distribution assumption of hidden representation is a limitation and may not ap-
ply to more general datasets other than MNIST, where class samples have multi-modal features.
This requires alternate ways of minimizing the entropy of distributions which is currently a hard
open problem. Additionally, since entropy penalty works best when there is a significant gap be-
tween correlation/variance of robust and non-robust features (see section 2.1), it may not be easy to
get good OOD performance when training set does not have this property. As evidence, we found
this to be the case when training with entropy penalty on SVHN and testing on other datasets (not
shown). Two possible solutions to this problem could be: a. design more generic algorithms for min-
imizing entropy of hidden representation; b. data augmentation techniques that selectively amplify
the difference in levels of correlation of robust feature with the target vs. the non-robust ones.
2.	Despite our attempt to explain why entropy penalty improves performance under distribution
shift when applied to the first hidden layer of deep networks, but not higher layers, a more detailed
understanding of this phenomenon remains elusive and is left as future work.
Disjoint from above discussion, the traditional practice of using a validation set for early stopping
and selecting hyper-parameters is based on the assumption that training/validation/test sets are sam-
pled IID from the same distribution (Arlot et al., 2010). However, it is not clear how to early stop
and select hyper-parameter values when the goal is to evaluate on out of distribution test sets. This
is because a set of non-robust features can be shared between a training and validation set, and
thus a high performance on such a validation set does not necessarily imply the learned model can
generalize to out of distribution test sets. This topic needs further attention.
References
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016.
Martin Arjovsky, Leon BottoU,Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893, 2019.
Sylvain Arlot, Alain Celisse, et al. A survey of cross-validation procedures for model selection.
Statistics surveys, 4:40-79, 2010.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
A. Ben-Tal, L. El Ghaoui, and A.S. Nemirovski. Robust Optimization. Princeton Series in Applied
Mathematics. Princeton University Press, October 2009.
John Blitzer, Mark Dredze, and Fernando Pereira. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classification. In Proceedings of the 45th annual
meeting of the association of computational linguistics, pp. 440-447, 2007.
Lorenzo Bruzzone and Mattia Marconcini. Domain adaptation problems: A dasvm classification
technique and a circular validation strategy. IEEE transactions on pattern analysis and machine
intelligence, 32(5):770-787, 2009.
Nic Ford, Justin Gilmer, Nicolas Carlini, and Dogus Cubuk. Adversarial examples are a natural
consequence of test error in noise. arXiv preprint arXiv:1901.10513, 2019.
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. arXiv
preprint arXiv:1409.7495, 2014.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. The Journal of Machine Learning Research, 17(1):2096-2030, 2016.
9
Under review as a conference paper at ICLR 2020
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and
Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias im-
proves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018a.
Robert Geirhos, Carlos RM Temme, Jonas Rauber, Heiko H Schutt, Matthias Bethge, and Felix A
Wichmann. Generalisation in humans and deep neural networks. In Advances in Neural Informa-
tion Processing Systems, pp. 7538-7550, 2018b.
Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generaliza-
tion for object recognition with multi-task autoencoders. In Proceedings of the IEEE international
conference on computer vision, pp. 2551-2559, 2015.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, and Wei-Ying Ma. Dual
learning for machine translation. In Advances in Neural Information Processing Systems, pp.
820-828, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016b.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.
Ehsan Hosseini-Asl, Yingbo Zhou, Caiming Xiong, and Richard Socher. A multi-
discriminator cyclegan for unsupervised non-parallel speech domain adaptation. arXiv preprint
arXiv:1804.00522, 2018.
Patrik O Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Scholkopf. Nonlin-
ear causal discovery with additive noise models. In Advances in neural information processing
systems, pp. 689-696, 2009.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. arXiv preprint arXiv:1905.02175,
2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Dominik Janzing, Patrik O Hoyer, and Bernhard Scholkopf. Telling cause from effect based on
high-dimensional observations. arXiv preprint arXiv:0909.4386, 2009.
Jing Jiang and ChengXiang Zhai. Instance weighting for domain adaptation in nlp. In Proceedings
of the 45th annual meeting of the association of computational linguistics, pp. 264-271, 2007.
Jason Jo and Yoshua Bengio. Measuring the tendency of cnns to learn surface statistical regularities.
arXiv preprint arXiv:1711.11561, 2017.
Harini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arXiv preprint
arXiv:1803.06373, 2018.
Niki Kilbertus, Giambattista Parascandolo, and Bernhard Scholkopf. Generalization in anti-causal
learning. arXiv preprint arXiv:1812.00524, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Brian Kulis, Kate Saenko, and Trevor Darrell. What you saw is not what you get: Domain adaptation
using asymmetric kernel transforms. In CVPR 2011, pp. 1785-1792. IEEE, 2011.
10
Under review as a conference paper at ICLR 2020
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/.
Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normaliza-
tion for practical domain adaptation. arXiv preprint arXiv:1603.04779, 2016.
DavidLoPez-Paz, Robert Nishihara, SoUmith Chintala, Bernhard Scholkopf, and Leon Bottou. Dis-
covering causal signals in images. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 6979-6987, 2017.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
David A McAllester. Pac-bayesian model averaging. In COLT, volume 99, pp. 164-170. Citeseer,
1999a.
David A McAllester. Some pac-bayesian theorems. Machine Learning, 37(3):355-363, 1999b.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.
Vishal M Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Visual domain adaptation:
An overview of recent advances. IEEE Signal Processing Magazine, 2014.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In
2015 IEEE Information Theory Workshop (ITW), pp. 1-5. IEEE, 2015.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018.
Abraham Wald. Statistical decision functions which minimize the maximum risk. Annals of Math-
ematics, pp. 265-280, 1945.
Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312:
135-153, 2018.
11
Under review as a conference paper at ICLR 2020
Figure 5: Color MNIST training set (left) and out of distribution test set (right). Each class in
training set has two background colors and two foreground colors that are unique only to that class.
Each test image has a foreground and background color that is randomly picked out of 10 colors that
are chosen independently of the training set.
Figure 6: Random samples from MNIST (top), SVHN (middle) and MNIST-M (bottom) datasets
are shown to get a visual sense of the hardness of the out of distribution task.
Appendix
A Datasets
Colored MNIST Dataset: Randomly drawn training and test samples from the C-MNIST dataset
generated as described above are shown in figure 5. The colored MNIST dataset (C-MNIST), which
is used in experiments 1 and 2, uses all the 60,000 training image in the MNIST dataset to generate
the C-MNIST dataset, where we randomly do a 0.9-0.1 split to get the training set and validation
set respectively. Similarly, we use all the 10,000 test images in MNIST to generate the C-MNIST
test set. We vary both the foreground and background colors to generate our C-MNIST dataset. The
reason why we vary colors of both foreground and background is that we want the trained model to
avoid overfitting any color bias. A single color in foreground or background would constitute a low
variance feature, which as we study in section 2.1.2, leads models to prioritize learning it for both
training with vanilla MLE as well as MLE with entropy penalty.
The C-MNIST training/validation set is generated from its MNIST counter-part as follows:
1.	For each class, randomly assign two colors (RGB value) for foreground and two colors (RGB
value) for background.
2.	Binarize each image (pixels in 0-255 range) around threshold 150 so that pixel values are either
0 or 1 and replicate the channel in each image to have a three channel image.
3.	For each image in a class, randomly pick one of the two foreground colors assigned to that class
and replace all foreground pixel with that color. Similarly replace background pixels for all images.
4.	Add zero mean Gaussian noise with a small standard deviation (0.04 used in our experiments) to
all images.
To generate the test set, in step one, we randomly assign a foreground and background color to each
image irrespective of the class, and the colors for validation set are chosen independently of the
training set.
1
Under review as a conference paper at ICLR 2020
Ooooo
W 8 6 4 2
15	30	45	60	75	90
Epochs
AdaBN (LR 0.001)
BN (LR 0.001)
AdaBN (LR 0.0001)
BN (LR 0.0001)
No BN (LR 0.001)
No BN (LR 0.0001)
O
Figure 7: Batch Normalization is unstable without AdaBN when using Entropy Penalty (experiment
on C-MNIST dataset).
Other Datasets: During experiments with MNIST, the single channel in each image was replicated
to form a three channel image. For SVHN, we resized the image to have 28 × 28 hieght and width.
B Experimental Details
Experiment in Section 2.1.1: The input data is in 500 dimensions and set the value of pi for each
i to be uniformly from [0, 1] and fix it henceforth. We then randomly sample 15000 input-target
pairs from this dataset with σ2 = 0.0001. We train a linear regression model and a 3 hidden layer
perceptron (MLP) of width 200 with ReLU activation for 1000 iterations using Adam optimizer with
learning rate 0.001 and weight decay 0.00001.
Variational bottleneck method (VIB): As an implementation detail of VIB, we flatten the output of
the last convolution layer of ResNet-56 and separately pass it through two linear layer of width 256,
one that outputs a vector that We regard as mean μ, and the other that We regard as log-variance V
(similar to how it is done in variational auto-encoders (Kingma & Welling, 2013)). We then combine
their outputs as o = μ + exp(0.5ν) Θ e where e is sampled from a standard Gaussian distribution
of the same dimension as ν. This output is then passed through a linear layer that transforms it into
a vector of dimension same as the number of classes. These implementation details are similar to
those in the original VIB paper.
Further, We gradually ramp up the regularization coefficient β from 0.0001 to its final value by
doubling the current value at the end of every epoch until the final value is reached. This is a
popular prctice When minimizing the KL divergence term betWeen posterior and prior Which helps
optimization.
C Overfitting Color Features
The validation set of C-MNIST is a held out set that folloWs the same distribution as the training
set but the tWo are mutually exclusive. On the other hand, the test set of C-MNIST has different
foreground and background colors in all images that are sampled independently of the training set
as explained in section A and shoWn in figure 5. The hyper-parameter configuration chosen (from
among the ones used in experiment 1 in section 3) for each method is based on the condition that
training accuracy converged to 100% at the end of the training process. This ensures a fair compar-
ison of validation and test accuracy betWeen different methods.
D	Effects of Batch Normalization on Entropy Penalty
In this section We study the effect of batch normalization (BN, Ioffe & Szegedy (2015)) When using
it in conjunction With entropy penalty. We train a ResNet-56 on C-MNIST dataset identical to
the settings in experiment 1 in main text, With the exception that We use BN. While doing so, We
2
Under review as a conference paper at ICLR 2020
record the accuracy of the model on the C-MNIST test set at every epoch. In addition, we also
run experiments where under the same training settings, during evaluation at each epoch, we use
AdaBN (Li et al., 2016). These values are plotted in figure 7. We have also plotted runs using
entropy penalty without any batch normalization for reference. In all cases, we use a learning rates
(LR) of 0.001 and 0.0001. The plots show that without AdaBN, test accuracy is very unstable on
the distribution shifted test set. This seems to be a side-effect of using BN which can be fixed by
adaptive the running statistics of BN (used during evaluation) with the test set statistics. However,
this requires domain knowledge (i.e., samples) of the test set, which is not preferable for our goal.
E Proofs
ʌ , . .. ..
Proposition2 LIB (θ) = (1 - β)H(Y∣fθ (X))+ βH(fθ (X)Y) + C
where C is a positive constant for discrete Y, independent of θ.
Proof: We note that,
H(fθ(X)) = -	Pr(fθ(X))logPr(fθ(X))	(11)
= -X	Pr(fθ(X),Y)logPr(fθ(X))	(12)
Y
=-XZ Pr(fθ(X), Y) (logPr(fθ(X)Y)+ log PPff(XXY))	(13)
-X
Y
Pr(fθ(X), Y)(logPr(fθ(X)IY)- logPr(Y∣fθ(X))+logPr(Y)) (14)
H(fθ (X)|Y) — H(Y∣fθ (X)) + H(Y)
(15)
where the fourth equality is due to Bayes rule. Since Y is discrete, H(Y) is a positive constant.
Finally, substituting the above expression in the definition of LI B (θ), yields,
ʌ ,. .. .. .. ..
LIB(θ) = H(Y∣fθ(X)) + β(H(fθ(X)|Y) - H(Y∣fθ(X)) + H(Y))	(16)
= (1- β)H(Y∣fθ(X)) + βH(fθ(X) ∣Y) + βH(Y)	(17)
which proves the claim.
Lemma 1
E[xi|y = 1] = -E[xi|y = -1] = 2pi - 1	(18)
E[xi2|y = 1] = E[xi2|y = -1] = 1+σ2	(19)
Proof: Given the distribution of x, we can write each element xi as,
Xi = bin + (1 - bi)n	(20)
where b is sampled from the BernoiUi distribution with probability Pi, n 〜N (y,σ2), and n 〜
N (-y, σ2 ). Thus,
E[xi|y = 1] =pi - (1 -pi) = 2pi - 1	(21)
E[xi|y = -1] = -pi + (1 -pi) = 1 - 2pi	(22)
Next,
E[x2∣y =1]= E[b2n2 + (1 - bi)2n2 + 2bi(1 -庆)&%® = 1]	(23)
We know from the properties of Bernoulli and Gaussian distribution that,
E[bi2] = var(bi) + E[bi]2 = pi (1 - pi) + pi2 = pi	(24)
E[ni2|y = 1] = var(ni|y = 1) + E[ni|y = 1]2 = σ2 + 1	(25)
3
Under review as a conference paper at ICLR 2020
We similarly get,
	E[ni2|y = -1] =	σ2	+	1	(26) E[n2∣y = 1] =	σ2	+	1	(27) E[n2∣y = -1] =	σ2	+	1	(28)
Therefore, using the independence property between random variables,
	E[x2|y = 1] = E[b2]E[n2|y = 1] + (1 + E[b2] - 2E[bi])E[n2|y = 1] + 2(E[bi] - E[bi]2)E[ni∣y = 1]E[ni∣y = 1]	(29) = 1+σ2	(30)
We similarly have,
	E[xi2|y = -1] =1+σ2	(31)
Theorem 3	Let θ* be the minimizer of J(θ) in Eq. 8 where we have used synthetic dataset A. Then
for a large enough d, θ* is given by,
	θ* = M-1∣2p - 1|	(32)
where,	M := Σ + λI + β(σ2I + 4diag(p	(1 - p)))	(33)
such that Σ is a positive definite matrix if2 pi 6∈ {0, 0.5, 1} for all i.
Proof: We start by noting that,
H (fθ (X)Iy)=	=- X P Pr(fθ(x),y)logPr(fθ(x)|y)	(34) y∈{-1,1} x =-0.5 P Pr(fθ(x)|y = 1)logPr(fθ(x∣y = 1) + Pr(fθ(x)|y = -1)logPr(fθ(x∣y = -1) x (35)
Identifying that,
	Hfθ(x)∣y=ι ：= - P Pr(fθ(x)|y = 1)logPr(fθ(x|y = 1)	(36) x Hfθ(χ)∣y=-ι := - / Pr(fθ(x)|y = -1)logPr(fθ(x|y = -1)	(37) x
are the entropy of the class conditional distributions Pr(fθ (x)|y = 1) and Pr(fθ (x)|y = -1)
respectively, we have that,
	H(fθ(x)|y) = 0.5 (Hfθ(χ)∣y=ι + Hfθ(χ)∣y=-ι)	(38)
We note that,
d
fθ (X)Ky = I) = X θix∕y = 1	(39)
i=1
d
=X θi(bi(n*y = 1) + (1 - bi)(n∕y = 1))	(40)
i=1
For a large enough dimensionality d ofX, central limit theorem (CLT) applies to fθ (X)|(y = 1)
and it converges to a Gaussian distribution. Thus the entropy of this distribution is given by
2This assumption is needed due to technicality.
4
Under review as a conference paper at ICLR 2020
0.5 log(2πes12) where s12 is the variance of fθ (x)|y = 1. A similar argument applies to fθ (x)|y
-1, in which case we define s2-1 to be its variance. Thus,
d
S2 = Var(X θiXi∣y = 1)	(41)
i=1
d
= X var(θixi |y = 1)	(42)
i=1
d
=X θ2var(xi |y = 1)	(43)
i=1
where the second equality holds because xi ’s are independent of one another. Thus using lemma 1,
s12 =	θi2 (1 + σ2 - (2pi - 1)2)
i
= X θi2(σ2 +4pi(1 -pi))
i
We similarly get,
s2-1 = Xθi2(σ2 +4pi(1 -pi))
i
Since s12 and s2-1 are equal, we denote s2 = s21 = s2-1. Thus,
H fθ(χ)∣y=ι = 0.5log(2πes2)
Hfθ(χ)∣y=-1 = 0.5log(2πes2)
and,
H (fθ (x)|y) = 0.5(0.5 log(2πes2) + 0.5 log(2πes2))
= 0.5 log(2πes2)
Therefore, our objective becomes,
arg min E[(fθ (x) - y)2] + λkθk2 +βs2
θ
d
= arg min θTE[xxT]θ - 2θT E[xy] + λkθk2 + β X θi2(σ2 + 4pi(1 - pi))
θ	i=1
Define M as,
M := Σ + λI + β(σ2I + 4diag(p	(1 - p)))
where Σ := E[xxT], we can re-write our objective as,
arg min θTMθ - 2θT E[xy]
θ
whose solution is given by,
θ* = M-1E[xy]
(44)
(45)
(46)
(47)
(48)
(49)
(50)
(51)
(52)
(53)
(54)
(55)
Using lemma 1, we get,
E[xiy] = E[xi|y = 1] Pr(y = 1) - E[xi|y = -1] Pr(y = -1)	(56)
= 0.5(E[xi|y = 1] -E[xi|y = -1])	(57)
= 0.5(4pi -2) = 2pi - 1	(58)
Plugging this value in Eq. 55 yields θ*.
5
Under review as a conference paper at ICLR 2020
Now we prove that Σ is full rank. Note that it is positive semi-definite since it is a scatter matrix.
Next, due to conditional independence, for i 6= j,
E[xixj] = E[xixj|y = 1] Pr(y = 1) +E[xixj|y = -1] Pr(y = -1)	(59)
= E[xi|y = 1]E[xj|y = 1] Pr(y = 1) +E[xi|y = -1]E[xj |y = -1] Pr(y = -1) (60)
and for i = j,
E[xi2] = E[xi2|y = 1] Pr(y = 1) +E[xi2|y = -1] Pr(y = -1)	(61)
Using lemma 1, we get,
Σ =1+σ2
(1 - 2pi )(1 - 2pj )
ifi = j
otherwise
(62)
To prove that Σ is positive definite (and hence full rank), we need to prove that no two columns are
parallel. To show this, consider any two indices i and j such that i 6= j . We show that there exists
no α 6= 0 such that the columns Σi = αΣj . We prove this by contradiction. Suppose Σii = αΣji,
then αΣjj = N∑ljj. Substituting values from Eq. 62,
(1	(1 + σ2)2
αΣjj =  ---------------------- (63)
(I - 2Pi)(I - 2pj)
Thus αΣjj > 1. However, Σij = (1 - 2pi)(1 - 2pj) < 1. Thus there is no non-zero αfor which
Σi = αΣj . Hence Σ must be full rank and hence positive definite. Thus we have proved the claim.
Lemma 2
E[xi|y = 1] = -E[xi|y = -1] = 1	(64)
E[xi2|y = 1] = E[xi2|y = -1] = 1 + σ2(pi + ki(1 -pi))	(65)
Proof: Given the distribution of x, we can write each element xi as,
Xi = bin + (1 - bi)%	(66)
where b is sampled from the BernoiUi distribution with probability Pi, n 〜N (y,σ2), and % 〜
N(y, kiσ2 ). Thus,
E[xi|y = 1] =Pi + (1 -Pi) = 1	(67)
E[xi|y = -1] = -Pi - (1 -Pi) = -1	(68)
Next,
E[x2∣y =1]= E[b2n2 + (1 - bi)2n2 + 2bi(1 -庆)&%® = 1]	(69)
We know from the properties of Bernoulli and Gaussian distribution that,
E[bi2] = var(bi) + E[bi]2 = Pi (1 - Pi) + Pi2 = Pi	(70)
E[%i2|y = 1] = var(%i|y = 1) + E[%i|y = 1]2 = σ2 + 1	(71)
We similarly get,
E[%i2|y = -1] =σ2+1	(72)
E[n2∣y = 1] = kσ2 + 1	(73)
E[n2∣y = —1] = kσ2 + 1	(74)
Therefore, using the independence property between random variables,
E[x2|y = 1] = E[b2]E[n2|y = 1] + (I + E[b2] - 2E[bi])E[n2|y = 1]
+ 2(E[bi] - E[bi]2)E[ni∣y =1]E[ni∣y =1]	(75)
= Pi(1 + σ2) + (1 - Pi)(1 + kσ2)	(76)
We similarly have,
E[xi2|y = -1] = Pi(1 +σ2) + (1 -Pi)(1 + kσ2)	(77)
Rearranging these terms yields the claim.
6
Under review as a conference paper at ICLR 2020
Theorem 4	Let θ* be the minimizer of J(θ) in Eq. 8 where we have used synthetic dataset B. Then
for a large enough d, θ* is given by,
θ* = MT1	(78)
where,
M := Σ + λI + βσ2diag(p + k(1 - p))	(79)
such that Σ is a positive definite matrix.
Proof: Similar to theorem 1 we have that,
H(fθ(x)|y) = 0.5 1Hfθ(x)∖y=ι + Hfθ(x)∣y=-ι)	(80)
and,
d
fθ (X)Ky = I) = X θi bi(ni∖y = I) +(I- bi)(ni∖y = I)	(81)
i=1
For a large enough dimensionality d ofx, central limit theorem (CLT) applies to fθ (x)|(y = 1)
and it converges to a Gaussian distribution. Thus the entropy of this distribution is given by
0.5log(2πes2) where s2 is the variance of fθ(x)|y = 1. A similar argument applies to fθ(x)|y =
-1, in which case we define s2-1 to be its variance. Thus,
d
s2 = X θ2var(xi |y = 1)	(82)
i=1
Thus using lemma 2,
s12 = X θi2(σ2(pi + ki(1 -pi)))	(83)
i
We similarly get,
s2-1 = X θi2(σ2(pi + ki(1 -pi)))	(84)
i
Since s12 and s2-1 are equal, we denote s2 = s21 = s2-1. Thus,
Hfθ(x)∣y=ι = 0.5log(2πes2)	(85)
Hfθ (χ)∣y=-i =0.5log(2πes2)	(86)
and,
H (fθ (x)|y) = 0.5(0.5 log(2πes2) + 0.5 log(2πes2))	(87)
= 0.5 log(2πes2)	(88)
Therefore, our objective becomes,
arg min E[(fθ (x) - y)2] + λkθk2 + βs2	(89)
θ
= arg min θT E[xxT]θ - 2θT E[xy] + λkθk2 + βσ2 X θi2 (pi + ki(1 -pi))	(90)
θ
i
Define M as,
M := Σ + λI + βσ2diag(p + k(1 - p))	(91)
where Σ := E[xxT], we can re-write our objective as,
arg min θTMθ - 2θT E[xy]	(92)
θ
whose solution is given by,
θ* = M-1E[xy]	(93)
7
Under review as a conference paper at ICLR 2020
Using lemma 2, we get,
E[xiy] = E[xi|y = 1] Pr(y = 1) -E[xi|y = -1] Pr(y = -1)	(94)
= 0.5(E[xi|y = 1] -E[xi|y = -1])	(95)
= 1	(96)
Plugging this value in Eq. 93 yields θ*.
Now we prove that Σ is full rank. Note that it is positive semi-definite since it is a scatter matrix.
Next, due to conditional independence, for i 6= j,
E[xixj] = E[xixj|y = 1] Pr(y = 1) +E[xixj|y = -1] Pr(y = -1)	(97)
= E[xi|y = 1]E[xj|y = 1] Pr(y = 1) +E[xi|y = -1]E[xj |y = -1] Pr(y = -1) (98)
and for i = j,
E[xi2] = E[xi2|y = 1] Pr(y = 1) +E[xi2|y = -1] Pr(y = -1)	(99)
Using lemma 2, we get,
1 + σ2 (pi + ki (1 - pi))
Σij =	1
ifi=j
otherwise
(100)
To prove that Σ is positive definite (and hence full rank), we need to prove that no two columns are
parallel. To show this, consider any two indices i and j such that i 6= j. We show that there exists
no α 6= 0 such that the columns Σi = αΣj . We prove this by contradiction. Suppose Σii = αΣji,
then αΣjj = '∑2j. SubstitutingvaluesfromEq. 100,
αΣjj = (1 + σ2(pi + k(1 - pi)))(1 + σ2(pj + k(1 - pj)))	(101)
Thus αΣjj > 1. However, Σij = 1. Thus there is no non-zero α for which Σi = αΣj. Hence Σ
must be full rank and hence positive definite. Thus we have proved the claim.
Proposition 3 IfPr(fθ(X))follows a Gaussian distribution, then,
H(fθ(X)) = 0.5log(πeEχι,χ2〜Dg[(fθ(Xi) - fθ(X2))2])	(102)
where X1 and X2 are IID samples from the data distribution D(X).
Proof: We note that,
H(fθ(X)) = -Pr(fθ(X))logPr(fθ(X))	(103)
= -Pr(fθ(X))logPr(fθ(X))	(104)
= -(0.5 log(2πeσ2))	(105)
where σ2 is the variance of the Gaussian distribution Pr(fθ (X)).
Let μ := Eχι〜D(χ)[fθ(Xi]. Then note that Eχ2〜D(χ)[fθ(X2] = μ as well since Xi and X2 are
IID samples. Therefore,
Eχι,χ2〜D(χ)[fθ(XI)- fθ(Xz))2]	(106)
=EXI,χ2〜D(X) [((fθ(XI)- μ) - (fθ(X2) - μ))2]	(107)
=EXI〜D(χ)Mfθ(XI)- μ)2]+ Eχ2〜D(χ)[fθ(X2) - μ))2]
-2Eχι,χ2〜D(χ)[((fθ(XI)- μ)(fθ(X2)- μ)]	(108)
=2Eχ〜D(χ)[((fθ(X)- μ)2]	(109)
= 2σ2	(110)
Therefore substituting Eχ1,χ2〜D(χ)[(fθ(Xi) — fθ(X2))2] in Eq. 105 yields the claim. □
8