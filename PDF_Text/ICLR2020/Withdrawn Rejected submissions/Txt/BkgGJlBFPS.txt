Under review as a conference paper at ICLR 2020
Unsupervised Hierarchical Graph Represen-
tation Learning with Variational Bayes
Anonymous authors
Paper under double-blind review
Ab stract
Hierarchical graph representation learning is an emerging subject owing to the
increasingly popular adoption of graph neural networks in machine learning and
applications. Loosely speaking, work under this umbrella falls into two categories:
(a) use a predefined graph hierarchy to perform pooling; and (b) learn the hierar-
chy for a given graph through differentiable parameterization of the coarsening
process. These approaches are supervised; a predictive task with ground-truth
labels is used to drive the learning. In this work, we propose an unsupervised ap-
proach, BayesPool, with the use of variational Bayes. It produces graph repre-
sentations given a predefined hierarchy. Rather than relying on labels, the training
signal comes from the evidence lower bound of encoding a graph and decoding the
subsequent one in the hierarchy. Node features are treated latent in this variational
machinery, so that they are produced as a byproduct and are used in downstream
tasks. We demonstrate a comprehensive set of experiments to show the usefulness
of the learned representation in the context of graph classification.
1	Introduction
Graph representation learning has attracted a surge of interest recently, inspired by the widespread
success of representation learning in the image and language domains through the use of deep neu-
ral networks for parameterization. A substantial number of graph neural network (GNN) architec-
tures (Bruna et al., 2014; Henaff et al., 2015; Duvenaud et al., 2015; Defferrard et al., 2016; Kipf &
Welling, 2017; Hamilton et al., 2017; Chen et al., 2018; VelickoVic et al., 2018; Ying et al., 2018a;
Liao et al., 2019; Xu et al., 2019) extend the convolution filters for a regular grid of data (e.g., image
pixels, time series, and sequences) to irregularly connected graph neighborhoods. This extension
naturally stimulates the quest of also extending the pooling operation in conVolutional neural net-
works (CNN) to graphs. The challenge lies in the irregular connections as opposed to a regular grid
structure, whereby partitioning is straightforward.
Graph pooling is used in at least two scenarios. One is global pooling: it pools the Vector represen-
tations of all nodes to a single Vector as the graph representation. Simple operators such as max or
mean are applied. A slightly more complex operator is the weighted sum, wherein the weights are
computed through attention (VelickoVic et al., 2018; Lee et al., 2019). A recently proposed operator
is top-k pooling (Zhang et al., 2018), whereby a fixed number of node representations at the top of
a sorted list is retained so that conVolutions or feed-forward transformations are applied.
The second use of pooling is the creation of a graph hierarchy. In this scenario, pooling is local,
more similar to that in CNNs. It is interfaced with graph coarsening (also called graph reduction
or graph compression), a generic form of which is to cluster the nodes of the original graph into
a node of the coarse graph. Then, the representations of the nodes in the cluster are pooled. The
clustering may be obtained by using existing graph coarsening or graph clustering approaches, as
in Bruna et al. (2014); Defferrard et al. (2016); SimonoVsky & Komodakis (2017); or learned through
parameterization as in Ying et al. (2018b); Gao & Ji (2019); Lee et al. (2019). In either approach,
the result include both a hierarchy of graphs and the accompanying node representations.
Representation learning in these local pooling approaches is superVised, with the training signal
coming from labels of the downstream task. In this work, we propose an unsuperVised learning
approach, named BayesPool, through the use of Variational Bayes. We use an existing coarsening
1
Under review as a conference paper at ICLR 2020
Figure 1: Coarsening sequence of a graph by using the method of Loukas (2019).
method to obtain the graph hierarchy, an example of which is shown in Figure 1. Then, our con-
tribution is the learning of node representations for all graphs in this sequence. The high-level idea
is to employ an encoder-decoder architecture: the encoder takes the graph and its node features as
input and produces node features for the next graph in the hierarchy, whereas the decoder uses these
produced node features to construct the graph. The objective is to obtain a decoding result as close
to the given next graph as possible. The tool we use is variational Bayes. It is, however, slightly
different from variational autoencoders (Kingma & Welling, 2014), because our decoder does not
intend to reconstruct the input graph.
A clear benefit of unsupervised learning is that the learned representation is not tailored to a specific
downstream task and hence may be more generalizable. Moreover, the coarsening method we adopt
is a recent development that holds spectral guarantee (Loukas & Vandergheynst, 2018; Loukas,
2019) on the quality of the coarse graphs. We demonstrate the effectiveness of such a combination
of hierarchical production and node representation learning in the context of graph classification. In
particular, the classification performance is rather competitive with state-of-the-art supervised GNN
approaches.
2	Related Work
This work is in part based on graph coarsening that produces a hierarchy for a given graph. Denote
by G = (V, E) a graph with the vertex set V and the edge setE. Graph coarsening is concerned with
computing a smaller (coarse) graph Gc = (Vc, Ec) with |Vc| < |V | that retains the structure of G. A
multilevel coarsening technique recursively coarsens the graph, yielding a hierarchy. Graph coars-
ening has been studied in the context of graph partitioning (Karypis & Kumar, 1998; Dhillon et al.,
2007), graph visualization (Harel & Koren, 2000), machine learning (Lafon & Lee, 2006; Gavish
et al., 2010; Ubaru & Saad, 2019), and pooling in graph neural networks (Bruna et al., 2014; Def-
ferrard et al., 2016; Simonovsky & Komodakis, 2017). A variety of heuristic coarsening techniques
have been proposed in different disciplines, including matching (Hendrickson & Leland, 1995;
Ubaru & Saad, 2019), first choice (Cong & Shinnerl, 2013), contraction-based schemes (Dhillon
et al., 2005; Sanders & Schulz, 2011), and algebraic multigrid (AMG)-inspired schemes (Sharon
et al., 2000; Hu & Scott, 2001; Ron et al., 2011; Chen & Safro, 2011). Many well-known soft-
ware packages exist for graph coarsening, e.g., Jostle (Walshaw & Cross, 2007), Metis (Karypis &
Kumar, 1998), and DiBaP (Meyerhenke et al., 2008).
Recently, a few graph coarsening techniques achieving certain theoretical guarantees were pre-
sented (Moitra, 2011; Dorfler & Bullo, 2012; Loukas & Vandergheynst, 2018). Loukas (2019)
presented variational approaches for graph coarsening with spectral guarantees. In particular, it
was shown that the coarse graphs preserve the top eigenspace (whose dimension is an input to the
method) within a predefined error tolerance. Here, we use this variational approach to obtain the
graph hierarchy.
This work is concerned with unsupervised graph representation learning. Recent literature has fo-
cused on generative models to achieve the same; see. e.g., Kipf & Welling (2016); Li et al. (2018);
Ma et al. (2018); Simonovsky & Komodakis (2018); Zhang et al. (2019). For learning hierarchical
representations of graphs, most of the works that we are aware of are based on supervised learn-
ing, including Bruna et al. (2014); Defferrard et al. (2016); Simonovsky & Komodakis (2017); Ying
et al. (2018b); Gao & Ji (2019); Lee et al. (2019). Methods most relevant to our work include:
DiffPool (Ying et al., 2018b), where the coarsening matrices are learned in an end-to-end fashion;
Graph U-Net (Gao & Ji, 2019), where graph pooling is achieved using a learnable vector and
2
Under review as a conference paper at ICLR 2020
node ranking; and SAGPool (Lee et al., 2019), which is similar to Graph U-Net but uses graph
self-attention to compute the ranking.
3	Method
The proposed method BayesPool is an extension of variational autoencoders. As the name sug-
gests, the goal of an autoencoder is to reconstruct the original input object after encoding it in the
latent space. Our approach does not reconstruct the original input, but rather, aims as decoding an
output faithful to another prescribed object. To this end, we first revisit variational Bayes and justify
the use of variational lower bound for learning. Then, the machinery is applied to the graph context.
3.1	Variational Bayes
Let x be the observed (data) variable and z be the unobserved (latent) variable. A core subject of
Bayesian inference is concerned with estimating the posterior distribution p(z|x). It is related to the
priorp(z) and the likelihood p(x|z) through the Bayes theorem
p(z |x)
p(x|z)p(z)
p(x, z ) dz
The challenge lies in the marginalization over z in the denominator, which is generally computation-
ally intractable. Hence, various approximations were developed. Typically one adopts a surrogate
model q(z) independent of data; and recently in the context of VAEs, the data dependent distribution
q(z|x) is often used. In our setting, we introduce a new variable xe and consider q(z|xe).
The difference, in terms of the KUllback-Leibler divergence, between the surrogate (variational)
posterior q(z|xe) and the true posterior p(z|x) may be decomposed as
DKL q(z|xe) || p(z|x)
=/
<.
q(z∣e) log q(z|x) dz
p(z |x)
q(z∣e) log q(zX)dz + j q(z∣e) log p(x) dz — j q(z∣e) log p(x∣z) dz.
p(z)
} '-----------------{z--------------} '------------------V-----------
log p(x)	Eq(z|xe) log p(x|z)
}
It consists of three terms: the KL divergence between the variational posterior and the priorp(z), the
log-evidence log p(x), and the marginal log-likelihood logp(x|z) under the surrogate distribution.
Because the KL divergence is nonnegative, the log-evidence is lower bounded by the combination
of the other two terms:
log p(x) ≥ Eq(z|xe) logp(x|z) — DKL q(z|xe) ||p(z) .	(1)
The better the surrogate, the tigher the lower bound.
One sees that the right-hand side of (1) is almost the same as the usual log-evidence lower bound
(ELBO), except that the surrogate q(z|xe) appears in place of q(z|x). This observation is not surpris-
ing, because the marginalization is over the latent variable z and has nothing to do with x and xe. We
thus conclude that the usual machinery of VAE applies, with only a notational change of the varia-
tional posterior. In the usual VAE setting, the first term of the right-hand side of (1) is considered
the decoding accuracy, whereas the second term is a regularization in the latent space. Our setting
follows this interpretation.
3.2	Graph Representation Learning with Variational Bayes
In our setting, a pair of graphs—the original one and the coarse one—is given. Let A ∈ Rn×n
and Ac ∈ Rm×m be the corresponding graph adjacency matrices, respectively. Similarly, denote
by X ∈ Rn×d and Xc ∈ Rm×d0 the corresponding node feature matrices. We apply the encoder-
decoder formalism, whereby the encoder encodes A and X into the coarse graph features Xc that
we seek, such that the decoder can use Xc to decode a coarse graph as similar to Ac as possible. See
Figure 2 for an illustration.
3
Under review as a conference paper at ICLR 2020
encoding
Xc
decoding
Figure 2: Encoder-decoder architecture.
Specifically, in the language of generative modeling, the encoder is the parameterized inference
model that produces the parameters of q(X∕A, X) and the decoder is the parameterized generative
model that produces the parameters of p(Ac|Xc). Following (1), Ac plays the role of x, Xc plays
the role ofz, and (A, X) plays the role of xe. The variational lower bound for model learning is thus
ELBO = Eq(Xc|A,X) hlogp(Ac|Xc)i - DKLq(Xc|A,X) ||p(Xc).	(2)
Maxmizing the ELBO amounts to maximizing the likelihood of decoding the coarse Ac (given
coarse node features Xc resulting from the encoder), while minimizing a regularization of the vari-
ational posterior q(Xc |A, X) that departs from the latent distribution p(Xc).
3.3	Modeling and Parameterization
Generally, the latent space may be kept simple and unparameterized, with more emphasis placed
on the encoder and the decoder. Thus, we let the prior p(Xc) be the standard matrix normal
MN (Xc | 0m×d0, Im, Id0). Occasionally, specifying simple Gaussian structures on p(Xc) may
improve performance, such as letting p(Xc) be the factored Gaussian, with the mean and the di-
agonal variance being parameters to learn. We have not yet, however, obtained strong empirical
evidence of the benefit of using a parameterized factored Gaussian in this case.
For the decoder (generative model), a natural choice is to treat each element of the coarse graph
adjacency matrix as a Bernoulli variable (scaled to the magnitude of the corresponding edge weight),
with the success probability parameterized by a function of the corresponding coarse node features.
For notational convenience, let the column vector xic ≡ Xc(i, :)T and let Aicj ≡ Ac(i, j). Then,
p(Ac|Xc) is the product of independent Bernoulli distributions:
p(Ac|Xc) = Y p(Aicj | xic,xjc) = Y Bernoulli(1Aicj | pij),
i6=j	i6=j
where 1Ac is the indicator function that returns 1 if Aicj 6= 0 and 0 otherwise, and pij is a param-
eterized function that computes the success probability. A simple choice of the probability is an
(unparameterized) dot product:
pij = sigmoid(hxic, xjci),	(3)
but there exist several other straightforward parameterized variants. For example,
pij = sigmoid(hWTxic, WTxjci) and pij = sigmoid(wT (xic xjc)),	(4)
where W and w are a parameter matrix and vector, respectively. One may also replace them by
an MLP. Note that all these functions are symmetric with respect to i and j because the graph is
undirected.
For the encoder (inference model), we treat the variational posterior q(Xc|A, X) as a factored Gaus-
sian: each coarse node Xc is an independent Gaussian with vector mean μ% and diagonal variance
diag(σi2). Then,
q(Xc|A,X)=Yq(xic|A,X)=YN(xic |μi, diag(σ2)),
ii
4
Under review as a conference paper at ICLR 2020
where μi and σ% are parameterized functions of A and X. By doing so, the KL term in the ELBO(2)
admits a closed form. We transpose the column vectors μ% and stack them to form a matrix M. Sim-
ilarly, we proceed with the σi ’s and form a matrix S. In what follows, we model the parameterized
function for M. The one for S is analogous.
Let C be the set of coarse nodes; hence A(C, :) keeps only the rows of A that correspond to the
coarse nodes. We let
M = σ(HXW1),	(5)
where W1 is a parameter matrix, H has the same nonzero pattern as A(C, :), and σ is an activation
function. This expression is in form similar to one graph convolution layer in GCN (Kipf & Welling,
2017), except that the square normalized adjacency matrix is replaced by a fat rectangular matrix
H . Rather than basing H on the original adjacency matrix, we designate its nonzero elements to be
self-attention weights computed from the node features. Specifically, the nonzero elements of the
i-th row of H is computed as
softmax w2T tanh(W3xi + W4xj ) ,	(6)
j ∈neighbor(i)
where w2 is a parameter vector and W3 and W4 are parameter matrices. As an alternative, one may
replace the attention calculation by that in GAT (VeliCkOViC et al., 2018):
softmax	LeakyReLU(w2T [W3xi ; W3xj]) .	(7)
j∈neighbor(i)
To further enhance the representational power, in the parameterization (5) one may replace the orig-
inal feature matrix X by the node embedding matrix Z output from GCN:
M = σ(HZW1) with Z = GCN(A,X;W5).	(8)
The GCN introduces additional parameters W5 that may be useful for large data sets.
In Section 4, we experiment with the different variants and suggest a default choice that works
generally well.
3.4	Multilevel Learning
Coarsening may be done recursively, forming a sequence of increasingly coarse graphs. Let the
adjacency matrices of this sequence be A0, A1, . . . , AL, where A0 = A corresponds to the initial
given graph. Given this sequence and the initial feature matrix X0 = X, the goal is to obtain the
subsequent feature matrices X1 , . . . , XL.
To this end, model learning is conducted through maximizing the evidence of observing A1, . . . , AL,
treated independently. That is, We want to optimize logp(Aι) + …+ logp(Al). Following the
argument as before, the actual quantity to optimize is the evidence lower bound. Inserting the layer
index ` into (2), we have
ELBO' = Eq(X'∣A'-ι,X'-ι) [ log P(A'|X')i - DKL (q(X'lA'-1,χ'-I) || P(Xg)).
Then, the log-evidence lower bound is the sum
L
ELBO = X ELBO'.
'=1
The encoder and decoder parameters across coarsening levels differ but they are jointly learned
through maximizing the ELBO.
3.5	Downstream Tasks
The learned features X1 , . . . , XL, together with the original X0, may be used for predictive tasks
through learning a separate predictive model. We follow a common practice and define the model as
yp = MLP(Concat(readout(Xo),readout(Xι),…，readout(XL))),
where readout is a global pooling across graph nodes (e.g., a concatenation of the max pooling and
the mean pooling), concat denotes vector concatenation, MLP is self-explanatory, and yp is the class
probability vector. In this paper, we consider the graph classification task.
5
Under review as a conference paper at ICLR 2020
4	Experiments
In this section, we evaluate the performance of BayesPool through the task of graph classification.
Note again that BayesPool is an unsupervised method; but as we will see, it is rather competitive
with recently proposed supervised methods, even outperforming them on several data sets. We first
present the details of the experimented data sets, the compared methods, and the training proce-
dure. Then, we compare the graph classification accuracies. We also perform sensitivity analysis
regarding the number of coarsening levels and compare the performance of several variants in the
implementation of BayesPool.
Data sets: We consider the same data sets used by Lee et al. (2019). They are standard benchmarks
publicly available from Kersting et al. (2016). Table 1 summarizes the information.
Table 1: Data Set StatiStics.
Data Set	No. graphS	Nodes (max)	NodeS (avg)	EdgeS (avg)	ClaSSeS
DD	1178	574「	284.32	715.66	2
PROTEINS	1113	620	39.06	72.82	2
NCI1	4110	111	29.87	32.30	2
NCI109	4127	111	29.68	32.13	2
FRANKENSTEIN	4337	146	16.90	17.88	2
The first two data sets are related to protein structures. The graphs in DD (Dobson & Doig, 2003)
have different amino acidS aS nodeS; and the edgeS correSpond to the diStance between the nodeS.
LabelS indicate if the protein iS an enzyme or not. The PROTEINS (DobSon & Doig, 2003) graphS
have Secondary Structure elementS of proteinS aS nodeS. The edgeS indicate whether the nodeS are
in amino acidS. NCI1 and NCI109 are biological data SetS popularly uSed for anticancer activity
claSSification (Wale et al., 2008). Here, the graphS correSpond to chemical compoundS, with the
atomS and the chemical bondS repreSented aS the nodeS and edgeS, reSpectively. The FRANKEN-
STEIN (OrSini et al., 2015) data Set containS molecular graphS with 780 node featureS. The labelS
indicate if a molecule iS mutagen or non-mutagen. Data SetS DD, NCI1, and NCI109 do not come
with node attributeS for uSe aS featureS. Hence, we employ tranSformationS of node degreeS aS
featureS, following the practice of DiffPool.
Compared methods: We compare with three SuperviSed hierarchical graph repreSentation learn-
ing methodS, namely DiffPool, Graph U-Net, and SAGPool. We duplicate the teSt accuracieS
of theSe methodS reported by Lee et al. (2019).
DiffPool (Ying et al., 2018b) computeS hierarchical repreSentationS of graphS by uSing end-to-end
trainable pooling baSed on Soft cluStering. ThiS method iS expenSive becauSe of the computation
of the denSe projection matrix. Lee et al. (2019) report that the method ran out of memory for a
pooling ratio greater than 0.5. Graph U-Net (Gao & Ji, 2019) uSeS a learnable Scoring vector to
rank the graph nodeS and SelectS top-ranked nodeS for pooling. SAGPool (Lee et al., 2019) uSeS a
Similar approach aS Graph U-Net for pooling, but incorporateS a Self-attention layer for learning
the Scoring vector. In both methodS, the pooling ratio waS Set to be 0.5.
Training procedure: We follow Lee et al. (2019) and perform Several roundS of random SplitS
with 80% training, 10% validation and 10% teSt. The learning rate iS tuned over the range [1e-2,
5e-2, 1e-3, 5e-3, 1e-4]. The hidden dimenSionS are tuned over [10, 20, 32, 48]. For LeakyReLU,
the Slope coefficient iS Set to 0.01. The coarSening ratio ρ iS experimented with [0.25, 0.5, 0.75];
See Section 4.2 for SenSitivity analySiS. The dimenSion of the top eigenSpace to be preServed by the
coarSening procedure iS Set to K = 5. We implement the method in PyTorch and uSe Adam aS the
optimizer. The training employS early Stopping with a patience of 50 epochS. The training Setting
for other methodS are reported in Lee et al. (2019).
For a fair compariSon, we uSe the Same MLP claSSifier aS in Lee et al. (2019). For readout, we uSe
mean pooling and max pooling and concatenate the two outputS. We then uSe 3 feedforward layerS
along with Softmax for claSSification. The claSSifier iS trained for 150 epochS.
6
Under review as a conference paper at ICLR 2020
For architecture variation, the following combination consistently achieves the best results: the de-
coder uses an unparameterized dot product (3), the encoder uses parameterization (5) with the atten-
tion matrix H computed by using the GAT form (7). The classification results reported in Section 4.1
below follow this choice. The results of other variants are reported in the subsections that follow.
The code is available at https://anonymous.4open.science/r/
a50d6411-55f7-4e24-8f6c-6eecee118ea0/.
4.1	Graph Classification
We compare the performance of BayesPool with several high-performing supervised methods
recently proposed. These methods are all hierarchical methods. Table 2 lists the average test accu-
racies. The pooling/coarsening ratio is 0.5 in all cases.
Table 2: Graph classification accuracies.					
Method	Data set				
	DD	PROTEINS	NCn	NCI109	FRANKENSTEIN
DiffPool	66.95	68.20	62.32	61.98	60.60
Graph U-Net	75.01	71.10	67.02	66.12	61.46
SAGPOOL	76.45	71.86	67.45	67.86	61.73
BayesPool	78.12	76.85	61.86	61.04	62.65
BayesPool outperforms other methods on three out of five data sets. Its performance is also on par
with DiffPool on the other two data sets, although infereior to Graph U-Net and SAGPool.
The graphs in DD and PROTEINS are relatively large; with a coarsening ratio 50% there does
not seem to cause information loss. Hence, BayesPool works rather appealingly. On the other
hand, although the graphs are smaller in FRANKENSTEIN, the data set contains a large number
(780) of features, which possibly dwarf the graph structure information. Therefore, all methods
yield similar results (with ours slightly outperforming the others). Encouragingly, BayesPool
is an unsupervised method; hence, these results show that the method is highly competitive for
downstream tasks such as graph classification. It enables incorporating sophisticated coarsening
techniques such as Loukas (2019) for graph representation learning.
Note that the adjacency matrices of the graphs are typically sparse. BayesPool leverages sparse
matrix computation, as opposed to DiffPool where the projection matrix does not have an a priori
sparsity structure. The coarsening procedure used by BayesPool is implemented in sparse matrix
format, along with the calculations of the neural network. This implementation results in a lower
time cost and space complexity.
4.2	Effect of Coarsening Ratio
One of the key factors that affects the performance of BayesPool is the amount of graph reduction
(the ratio of the coarse graph size to the initial size), or equivalently, the number of coarsening levels.
This ratio is an input parameter to the coarsening method of Loukas (2019) that we use. In Table 3,
we evaluate the performance of BayesPool on two data sets with respect to the coarsening ratio.
These data sets have relatively large graphs so that aggressive coarsening is possible.
Table 3: Graph classification accuracies for different coarsening ratios.
Coarsening ratio	DD	PROTEINS
ρ= 0.75	77.87	77.27
ρ = 0.5	78.12	76.85
ρ=0.25	68.75	66.96
In the table, we report the results for three different levels of coarsening with ratio ρ = m/n, where
m is the number nodes in the coarse graph andn the original graph. We observe that the performance
of BAYESPOOL is relatively stable when ρ ≥ 0.5, but degrades as ρ becomes smaller. As we lower
ρ, more and more nodes and edges are removed, causing significant loss of information. However,
7
Under review as a conference paper at ICLR 2020
even for ρ = 0.25, BAYESPOOL still yields comparable results to DIFFPOOL according to Table 2.
We conclude that ρ = 0.5 appears to be the right tradeoff.
4.3	Variants of Architecture
As discussed in Section 3, a few parameterizations of the encoder and the decoder are possible.
In this subsection, we comprehensively study the different variants, with the aim of obtaining a
combination that generally works well. The results are reported in Table 4.
Table 4: Graph classification accuracies for different architecture variants.
Encoder
Variants	DD	PROTEINS
LeakyReLU + plain [(7) + (5)]	78.12	76.85
LeakyReLU + GCN [ (7) + (8) ]	75.68	74.51
tanh + plain [ (6) + (5) ]	75.48	73.78
tanh + GCN [(6) + (8)]	74.80	73.52
Decoder
Variants	DD	PROTEINS
Plain dot product (3)	78.12	76.85
Parameterized dot product (4) left	71.06	73.58
Parameterized dot product (4) right	71.69	70.58
Replace W in (4) left by 2-layer MLP	72.83	75.65
In the top part of Table 4 we compare the performance of four variants of the encoder output M
(as well as S). The variants include (i) the attention calculation and (ii) whether or not to apply
GCN before attention. The former contains two versions (6) and (7) and the latter also admits two
versions (5) and (8), hence four combinations in total.
From the table, we observe that the GAT approach with LeakyReLU yields a better performance;
and interestingly, using additionally GCN for parameterization lowers the performance. The intro-
duction of the additional parameters inside GCN does not seem helpful.
In the bottom part of Table 4 we compare the performance of four variants of the decoder output
pij . Along with the three variants discussed in Section 3.3 (unparamterized (3) and matrix/vector-
paramterized (4)), we also consider replacing the matrix W in (4) by a 2-layer MLP.
From the table, we observe that the plain dot product performs the best for both data sets. Again, the
introduction of additional parameters does not seem helpful; rather, the accuracies deteriorate. This
observation is consistent for both the encoder and the decoder. It is possible that the use of many
parameters adversely affects the performance on data sets of a scale that we experimented with here.
5	Conclusion
We have presented an unsupervised approach BayesPool for hierarchical graph representation
learning. Compared with supervised approaches, a clear benefit is that the learned representations
are generalizable to different downstream tasks. BayesPool consists of an encoder-decoder ar-
chitecture and adopts variational Bayes for training, but it is different from standard VAEs in that it
does not attempt to reconstruct the input graph; rather, the decoder aims at producing the next graph
in the hierarchy. Together with the use of the graph coarsening approach of Loukas (2019), we per-
form empirical evaluations that show that the learned representations yield competitive classification
accuracies with state-of-the-art supervised GNN methods.
References
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. In ICLR, 2014.
8
Under review as a conference paper at ICLR 2020
Jie Chen and Ilya Safro. Algebraic distance on graphs. SIAM Journal on Scientific Computing, 33
(6):3468-3490, 2011.
Jie Chen, Tengfei Ma, and Cao Xiao. FastGCN: Fast learning with graph convolutional networks
via importance sampling. In ICLR, 2018.
Jingsheng Jason Cong and Joseph R Shinnerl. Multilevel optimization in VLSICAD, volume 14.
Springer Science & Business Media, 2013.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In NIPS, 2016.
Inderjit Dhillon, Yuqiang Guan, and Brian Kulis. A fast kernel-based multilevel algorithm for graph
clustering. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge
discovery in data mining, pp. 629-634. ACM, 2005.
Inderjit S Dhillon, Yuqiang Guan, and Brian Kulis. Weighted graph cuts without eigenvectors a
multilevel approach. IEEE transactions on pattern analysis and machine intelligence, 29(11):
1944-1957, 2007.
Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without
alignments. Journal of molecular biology, 330(4):771-783, 2003.
Florian Dorfler and Francesco Bullo. Kron reduction of graphs with applications to electrical net-
works. IEEE Transactions on Circuits and Systems I: Regular Papers, 60(1):150-163, 2012.
David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gomez-Bombarelli, Tim-
othy Hirzel, Alan Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for
learning molecular fingerprints. In NIPS, 2015.
Hongyang Gao and Shuiwang Ji. Graph U-Nets. In ICML, 2019.
Matan Gavish, Boaz Nadler, and Ronald R Coifman. Multiscale wavelets on trees, graphs and high
dimensional data: theory and applications to semi supervised learning. In Proceedings of the
27th International Conference on International Conference on Machine Learning, pp. 367-374.
Omnipress, 2010.
William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In NIPS, 2017.
David Harel and Yehuda Koren. A fast multi-scale method for drawing large graphs. In International
symposium on graph drawing, pp. 183-196. Springer, 2000.
Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured
data. arXiv:1506.05163, 2015.
B Hendrickson and R Leland. A multi-level algorithm for partitioning graphs. In Supercomput-
ing’95: Proceedings of the 1995 ACM/IEEE Conference on Supercomputing, pp. 28-28. IEEE,
1995.
YF Hu and Jennifer A Scott. A multilevel algorithm for wavefront reduction. SIAM Journal on
Scientific Computing, 23(4):1352-1375, 2001.
George Karypis and Vipin Kumar. A fast and high quality multilevel scheme for partitioning irreg-
ular graphs. SIAM Journal on scientific Computing, 20(1):359-392, 1998.
Kristian Kersting, Nils M. Kriege, Christopher Morris, Petra Mutzel, and Marion Neumann. Bench-
mark data sets for graph kernels, 2016. URL http://graphkernels.cs.tu-dortmund.
de.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.
Thomas N. Kipf and Max Welling. Variational graph auto-encoders. In NIPS Workshop on Bayesian
Deep Learning, 2016.
9
Under review as a conference paper at ICLR 2020
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In ICLR, 2017.
Stephane Lafon and Ann B Lee. Diffusion maps and coarse-graining: A unified framework for
dimensionality reduction, graph partitioning, and data set parameterization. IEEE transactions on
pattern analysis and machine intelligence, 28(9):1393-1403, 2006.
Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In ICML, 2019.
Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep generative
models of graphs. In ICML, 2018.
Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard Zemel. Lanczosnet: Multi-scale deep
graph convolutional networks. In ICLR, 2019.
Andreas Loukas. Graph reduction with spectral and cut guarantees. JMLR, 2019.
Andreas Loukas and Pierre Vandergheynst. Spectrally approximating large graphs with smaller
graphs. In ICML, 2018.
Tengfei Ma, Jie Chen, and Cao Xiao. Constrained generation of semantically valid graphs via
regularizing variational autoencoders. In NeurIPS, 2018.
Henning Meyerhenke, Burkhard Monien, and Thomas Sauerwald. A new diffusion-based multi-
level algorithm for computing graph partitions of very high quality. In 2008 IEEE International
Symposium on Parallel and Distributed Processing, pp. 1-13. IEEE, 2008.
Ankur Moitra. Vertex sparsification and universal rounding algorithms. PhD thesis, Massachusetts
Institute of Technology, 2011.
Francesco Orsini, Paolo Frasconi, and Luc De Raedt. Graph invariant kernels. In Twenty-Fourth
International Joint Conference on Artificial Intelligence, 2015.
Dorit Ron, Ilya Safro, and Achi Brandt. Relaxation-based coarsening and multiscale graph organi-
zation. Multiscale Modeling & Simulation, 9(1):407-423, 2011.
Peter Sanders and Christian Schulz. Engineering multilevel graph partitioning algorithms. In Euro-
pean Symposium on Algorithms, pp. 469-480. Springer, 2011.
Eitan Sharon, Achi Brandt, and Ronen Basri. Fast multiscale image segmentation. In Proceedings
IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662),
volume 1, pp. 70-77. IEEE, 2000.
Martin Simonovsky and Nikos Komodakis. Dynamic edge-conditioned filters in convolutional neu-
ral networks on graphs. In CVPR, 2017.
Martin Simonovsky and Nikos Komodakis. GraphVAE: Towards generation of small graphs using
variational autoencoders. In ICANN, 2018.
Shashanka Ubaru and Yousef Saad. Sampling and multilevel coarsening algorithms for fast matrix
approximations. Numerical Linear Algebra with Applications, 26(3):e2234, 2019.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In ICLR, 2018.
Nikil Wale, Ian A Watson, and George Karypis. Comparison of descriptor spaces for chemical com-
poUnd retrieval and classification. Knowledge and Information Systems, 14(3):347-375, 2008.
Chris Walshaw and Mark Cross. Jostle: parallel mUltilevel graph-partitioning software-an overview.
2007.
KeyUlU XU, WeihUa HU, JUre Leskovec, and Stefanie Jegelka. How powerfUl are graph neUral
networks? In ICLR, 2019.
10
Under review as a conference paper at ICLR 2020
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure
Leskovec. Graph convolutional neural networks for web-scale recommender systems. In KDD,
2018a.
Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, and Jure Leskovec.
Hierarchical graph representation learning with differentiable pooling. In NIPS, 2018b.
Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning
architecture for graph classification. In AAAI, 2018.
Muhan Zhang, Shali Jiang, Zhicheng Cui, Roman Garnett, and Yixin Chen. D-VAE: A variational
autoencoder for directed acyclic graphs. In NeurIPS, 2019.
11