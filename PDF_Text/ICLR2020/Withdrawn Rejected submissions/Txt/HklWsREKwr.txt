Under review as a conference paper at ICLR 2020
Training Deep Neural Networks with Par-
tially Adaptive Momentum
Anonymous authors
Paper under double-blind review
Ab stract
Adaptive gradient methods, which adopt historical gradient information to auto-
matically adjust the learning rate, despite the nice property of fast convergence,
have been observed to generalize worse than stochastic gradient descent (SGD)
with momentum in training deep neural networks. This leaves how to close the
generalization gap of adaptive gradient methods an open problem. In this work,
we show that adaptive gradient methods such as Adam, Amsgrad, are sometimes
“over adapted”. We design a new algorithm, called Partially adaptive momentum
estimation method, which unifies the Adam/Amsgrad with SGD by introducing a
partial adaptive parameter p, to achieve the best from both worlds. We also prove
the convergence rate of our proposed algorithm to a stationary point in the stochas-
tic nonconvex optimization setting. Experiments on standard benchmarks show
that our proposed algorithm can maintain fast convergence rate as Adam/Amsgrad
while generalizing as well as SGD in training deep neural networks. These re-
sults would suggest practitioners pick up adaptive gradient methods once again
for faster training of deep neural networks.
1 Introduction
Stochastic gradient descent (SGD) is now one of the most dominant approaches for training deep
neural networks (Goodfellow et al., 2016). In each iteration, SGD only performs one parameter
update on a mini-batch of training examples. SGD is simple and has been proved to be efficient,
especially for tasks on large datasets. In recent years, adaptive variants of SGD have emerged and
shown their successes for their convenient automatic learning rate adjustment mechanism. Adagrad
(Duchi et al., 2011) is probably the first along this line of research, and significantly outperforms
vanilla SGD in the sparse gradient scenario. Despite the first success, Adagrad was later found
to demonstrate degraded performance especially in cases where the loss function is nonconvex or
the gradient is dense. Many variants of Adagrad, such as RMSprop (Hinton et al., 2012), Adam
(Kingma & Ba, 2015), Adadelta (Zeiler, 2012), Nadam (Dozat, 2016), were then proposed to address
these challenges by adopting exponential moving average rather than the arithmetic average used in
Adagrad. This change largely mitigates the rapid decay of learning rate in Adagrad and hence
makes this family of algorithms, especially Adam, particularly popular on various tasks. Recently,
it has also been observed (Reddi et al., 2018) that Adam does not converge in some settings where
rarely encountered large gradient information quickly dies out due to the “short momery” problem of
exponential moving average. To address this issue, Amsgrad (Reddi et al., 2018) has been proposed
to keep an extra “long term memory” variable to preserve the past gradient information and to correct
the potential convergence issue in Adam. There are also some other variants of adaptive gradient
method such as SC-Adagrad / SC-RMSprop (Mukkamala & Hein, 2017), which derives logarithmic
regret bounds for strongly convex functions.
On the other hand, people recently found that for largely over-parameterized neural networks, e.g.,
more complex modern convolutional neural network (CNN) architectures such as VGGNet (He
et al., 2016), ResNet (He et al., 2016), Wide ResNet (Zagoruyko & Komodakis, 2016), DenseNet
(Huang et al., 2017), training with Adam or its variants typically generalizes worse than SGD
with momentum, even when the training performance is better. In particular, people found that
carefully-tuned SGD, combined with proper momentum, weight decay and appropriate learning rate
decay schedules, can significantly outperform adaptive gradient algorithms eventually (Wilson et al.,
2017). As a result, even though adaptive gradient methods are relatively easy to tune and converge
1
Under review as a conference paper at ICLR 2020
faster at the early stage, recent advances in designing neural network structures are all reporting
their performances by training their models with SGD-momentum (He et al., 2016; Zagoruyko &
Komodakis, 2016; Huang et al., 2017; Simonyan & Zisserman, 2014; Ren et al., 2015; Xie et al.,
2017; Howard et al., 2017). Different from SGD, which adopts a universal learning rate for all
coordinates, the effective learning rate of adaptive gradient methods, i.e., the universal base learning
rate divided by the second order moment term, is different for different coordinates. Due to the
normalization of the second order moment, some coordinates will have very large effective learning
rates. To alleviate this problem, one usually chooses a smaller base learning rate for adaptive gradi-
ent methods than SGD with momentum. This makes the learning rate decay schedule less effective
when applied to adaptive gradient methods, since a much smaller base learning rate will soon di-
minish after several rounds of decay. We refer to the above phenomenon as the “small learning rate
dilemma” (see more details in Section 3).
With all these observations, a natural question is:
Can we take the best from both Adam and SGD-momentum, i.e., design an algorithm that not only
enjoys the fast convergence rate as Adam, but also generalizes as well as SGD-momentum?
In this paper, we answer this question affirmatively. We close the generalization gap of adaptive
gradient methods by proposing a new algorithm, called partially adaptive momentum estimation
(Padam) method, which unifies Adam/Amsgrad with SGD-momentum to achieve the best of both
worlds, by a partially adaptive parameter. The intuition behind our algorithm is: by controlling
the degree of adaptiveness, the base learning rate in Padam does not need to be as small as other
adaptive gradient methods. Therefore, it can maintain a larger learning rate while preventing the
gradient explosion. We note that there exist several studies (Zaheer et al., 2018; Loshchilov &
Hutter, 2019; Luo et al., 2019) that also attempted to address the same research question. In detail,
Yogi (Zaheer et al., 2018) studied the effect of adaptive denominator constant and minibatch size in
the convergence of adaptive gradient methods. AdamW (Loshchilov & Hutter, 2019) proposed to fix
the weight decay regularization in Adam by decoupling the weight decay from the gradient update
and this improves the generalization performance of Adam. AdaBound (Luo et al., 2019) applies
dynamic bound of learning rate on Adam and make them smoothly converge to a constant final step
size as in SGD. Our algorithm is very different from Yogi, AdamW and AdaBound. Padam is built
upon a simple modification of Adam without extra complicated algorithmic design and it comes
with a rigorous convergence guarantee in the nonconvex stochastic optimization setting.
We highlight the main contributions of our work as follows:
•	We propose a novel and simple algorithm Padam with a partially adaptive parameter, which re-
solves the “small learning rate dilemma” for adaptive gradient methods and allows for faster con-
vergence, hence closing the gap of generalization.
•	We provide a convergence guarantee for Padam in nonconvex optimization. Specifically, we prove
that the convergence rate of Padam to a stationary point for stochastic nonconvex optimization is
(1.1)
where g1, . . . , gT are the stochastic gradients and g1:T,i = [g1,i, g2,i, . . . , gT,i]>. When the stochas-
tic gradients are '∞-bounded, (1.1) matches the convergence rate of SGD in terms of the rate of T.
•	We also provide thorough experiments about our proposed Padam method on training modern
deep neural architectures. We empirically show that Padam achieves the fastest convergence speed
while generalizing as well as SGD with momentum. These results suggest that practitioners should
pick up adaptive gradient methods once again for faster training of deep neural networks.
•	Last but not least, compared with the recent work on adaptive gradient methods, such as Yogi
(Zaheer et al., 2018), AdamW (Loshchilov & Hutter, 2019), AdaBound (Luo et al., 2019), our
proposed Padam achieves better generalization performance than these methods in our experiments.
1.1 Additional related work
Here we review additional related work that is not covered before. Zhang et al. (2017) proposed
a normalized direction-preserving Adam (ND-Adam), which changes the adaptive terms from in-
dividual dimensions to the whole gradient vector. Keskar & Socher (2017) proposed to improve
2
Under review as a conference paper at ICLR 2020
the generalization performance by switching from Adam to SGD. On the other hand, despite the
great successes of adaptive gradient methods for training deep neural networks, the convergence
guarantees for these algorithms are still understudied. Most convergence analyses of adaptive gra-
dient methods are restricted to online convex optimization (Duchi et al., 2011; Kingma & Ba, 2015;
Mukkamala & Hein, 2017; Reddi et al., 2018). A few recent attempts have been made to ana-
lyze adaptive gradient methods for stochastic nonconvex optimization. More specifically, Basu et al.
(2018) proved the convergence rate of RMSProp and Adam when using deterministic gradient rather
than stochastic gradient. Li & Orabona (2018) analyzed convergence rate of AdaGrad under both
convex and nonconvex settings but did not consider more complicated Adam-type algorithms. Ward
et al. (2018) also proved the convergence rate of AdaGrad under both convex and nonconvex settings
without considering the effect of stochastic momentum. Chen et al. (2018) provided a convergence
analysis for a class of Adam-type algorithms for nonconvex optimization. Zou & Shen (2018) ana-
lyzed the convergence rate of AdaHB and AdaNAG, two modified version of AdaGrad with the use
of momentum. However, none of these results are directly applicable to Padam. Our convergence
analysis in Section 4 is quite general and implies the convergence rate of Adam/AMSGrad for non-
convex optimization. In terms of learning rate decay schedule, Wu et al. (2018) studied the learning
rate schedule via short-horizon bias. Xu et al. (2016); Davis et al. (2019) analyzed the convergence
of stochastic algorithms with geometric learning rate decay. Ge et al. (2019) studied the learning
rate schedule for quadratic functions.
Notation: Scalars are denoted by lower case letters, vectors by lower case bold face letters, and
matrices by upper case bold face letters. For a vector X ∈ Rd, We denote the '2 norm of X by
∣∣xk2 = JPd=I χ2, the'∞ norm of X by ∣∣x∣∣∞ = maχd=1 ∣χi∣. Forasequenceofvectors {xj}j=1,
We denote by xj,i the i-th element in Xj. We also denote X1:t,i = [x1,i , . . . , xt,i ]>. With slightly
abuse of notation, for tWo vectors a and b, We denote a1 2 as the element-Wise square, ap as the
element-Wise poWer operation, a/b as the element-Wise division and max(a, b) as the element-
Wise maximum. We denote by diag(a) a diagonal matrix With diagonal entries a1 , . . . , ad. Given
tWo sequences {an} and {bn}, We Write an = O(bn) if there exists a positive constant C such that
an ≤ Cbn and an = o(bn) if an/bn → 0 as n → ∞. Notation O(∙) hides logarithmic factors.
2 Review of Adaptive Gradient Methods
Various adaptive gradient methods have been proposed in order to achieve better performance on
various stochastic optimization tasks. Adagrad (Duchi et al., 2011) is among the first methods
With adaptive learning rate for each individual dimension, Which motivates the research on adaptive
gradient methods in the machine learning community. In detail, Adagrad1 adopts the folloWing
update form:
gt	1 t 2
Xt+1 = Xt — at √v, where Vt = - 2g2,	(Adagrad)
where gt stands for the stochastic gradient Nft(Xt), and at = α∕√7 is the step size. In this paper,
we call at base learning rate, which is the same for all coordinates of Xt, and we call aj√ξ7
effective learning rate for the i-th coordinate of Xt, which varies across the coordinates. Adagrad is
proved to enjoy a huge gain in terms of convergence especially in sparse gradient situations. Empir-
ical studies also show a performance gain even for non-sparse gradient settings. RMSprop (Hinton
et al., 2012) follows the idea of adaptive learning rate and it changes the arithmetic averages used
for Vt in Adagrad to exponential moving averages. Even though RMSprop is an empirical method
with no theoretical guarantee, the outstanding empirical performance of RMSprop raised people’s
interests in exponential moving average variants of Adagrad. Adam (Kingma & Ba, 2015)2 is the
most popular exponential moving average variant of Adagrad. It combines the idea of RMSprop and
momentum acceleration, and takes the following update form:
Xt+1	= Xt	— atmL where	mt	=	βimt-i	+ (1 — βι)gt, Vt	=	β2Vt-l + (1 —	β2)g2.	(Adam)
Vt
1The formula is equivalent to the one from the original paper (Duchi et al., 2011) after simple manipulations.
2Here for simplicity and consistency, we ignore the bias correction step in the original paper of Adam. Yet
adding the bias correction step will not affect the argument in the paper.
3
Under review as a conference paper at ICLR 2020
Adam also requires at = α∕√7 for the sake of convergence analysis. In practice, any decaying step
size or even constant step size works well for Adam. Note that if we choose β1 = 0, Adam basically
reduces to RMSprop. Reddi et al. (2018) identified a non-convergence issue in Adam. Specifically,
Adam does not collect long-term memory of past gradients and therefore the effective learning rate
could be increasing in some cases. They proposed a modified algorithm namely Amsgrad. More
specifically, AmSgrad adopts an additional step to ensure the decay of the effective learning rate
at/√vt, and its key update formula is as follows:
xt+ι = Xt - at ^m, where Qt = max(Vι, vj
(Amsgrad)
mt and Vt are the same as Adam. By introducing the V term, Reddi et al. (2018) corrected some
mistakes in the original proof of Adam and proved an O(1∕√T) convergence rate of Amsgrad for
convex optimization. Note that all the theoretical guarantees on adaptive gradient methods (Adagrad,
Adam, Amsgrad) are only proved for convex functions.
3 The Proposed Algorithm
In this section, we propose a new algorithm for bridging the generalization gap for adaptive gradient
methods. Specifically, we introduce a partial adaptive parameter p to control the level of adaptive-
ness of the optimization procedure. The proposed algorithm is displayed in Algorithm 1.
Algorithm 1 Partially adaptive momentum estimation method (Padam)
input: initial point X1 ∈ X; step sizes {at}; adaptive parameters β1, β2, p ∈ (0, 1/2]
set m0 = 0, v0 = 0, vb0 = 0
for t = 1, . . . , T do
gt = Vf(Xt,ξt)
mt = β1mt-1 + (1 - β1)gt
vt = β2vt-1 + (1 - β2)gt2
vbt = max(vbt-1, vt)
Xt+1 = Xt — at ∙ mt∕Vp
end for
Output: Choose Xout from {Xt}, 2 ≤ t ≤ T with probabilityat-i∕(PTIIai)
In Algorithm 1, gt denotes the stochastic gradient and vbt can be seen as a moving average over
the second order moment of the stochastic gradients. As we can see from Algorithm 1, the key
difference between Padam and Amsgrad (Reddi et al., 2018) is that: while mt is still the momentum
as in Adam/Amsgrad, it is now “partially adapted” by the second order moment, i.e.,
xt+ι = Xt - at mt, where Vt = max(vt-ι, vj
(Padam)
We call p ∈ [0, 1/2] the partially adaptive parameter. Note that 1/2 is the largest possible value
for p and a larger p will result in non-convergence in the proof (see the proof details in the sup-
plementary materials). When p → 0, Algorithm 1 reduces to SGD with momentum3 and when
p = 1/2, Algorithm 1 is exactly Amsgrad. Therefore, Padam indeed unifies Amsgrad and SGD
with momentum.
With the notations defined above, we are able to formally explain the “small learning rate dilemma”.
In order to make things clear, we first emphasize the relationship between adaptiveness and learning
rate decay. We refer the actual learning rate applied to mt as the effective learning rate, i.e., at/vbtp.
Now suppose that a learning rate decay schedule is applied to at. Ifp is large, then at early stages,
the effective learning rate at/vbtp,i could be fairly large for certain coordinates with small vbt,i value4.
To prevent those coordinates from overshooting we need to enforce a smaller at, and therefore the
base learning rate must be set small (Keskar & Socher, 2017; Wilson et al., 2017). As a result,
after several rounds of decaying, the learning rates of the adaptive gradient methods are too small
3The only difference between Padam with p = 0 and SGD-Momentum is an extra constant factor (1 - β1),
which can be moved into the learning rate such that the update rules for these two algorithms are identical.
4The coordinate vbt,i’s are much less than 1 for most commonly used network architectures. See Figure 5 in
the supplementary materials.
4
Under review as a conference paper at ICLR 2020
to make any significant progress in the training process5. We call this phenomenon “small learn-
ing rate dilemma”. It is also easy to see that the larger p is, the more severe “small learning rate
dilemma” is. This suggests that intuitively, we should consider using Padam with a proper adaptive
parameter p, and choosing p < 1/2 can potentially make Padam suffer less from the “small learning
rate dilemma” than Amsgrad, which justifies the range of p in Algorithm 1. We will show in our
experiments (Section 5) that Padam with p < 1/2 can adopt an equally large base learning rate as
SGD with momentum.
Note that even though in Algorithm 1, the choice ofαt covers different choices of learning rate decay
schedule, the main focus of this paper is not about finding the best learning rate decay schedule, but
designing a new algorithm to control the adaptiveness for better empirical generalization result. In
other words, our focus is not on αt , but on vbt . For this reason, we simply fix the learning rate decay
schedule for all methods in the experiments to provide a fair comparison for different methods.
Figure 1 shows the comparison of
test error performances under the
different partial adaptive parame-
ter p for ResNet on both CIFAR-
10 and CIFAR-100 datasets. We
can observe that a larger p will
lead to fast convergence at early
stages and worse generalization per-
formance later, while a smaller p be-
haves more like SGD with momen-
(b) CIFAR-100
tum: slow in early stages but finally Figure 1: Performance comparison of Padam with different
catch up. With a proper choice ofp choices of p for training ResNet on CIFAR-10 / CIFAR-100.
(e.g., 1/8 in this case), Padam can
obtain the best of both worlds. Note that besides Algorithm 1, our partially adaptive idea can also be
applied to other adaptive gradient methods such as Adagrad, Adadelta, RMSprop, AdaMax (Kingma
& Ba, 2015). For the sake of conciseness, we do not list the partially adaptive versions for other
adaptive gradient methods here. We also would like to comment that Padam is totally different from
the p-norm generalized version of Adam in Kingma & Ba (2015), which induces AdaMax method
when p → ∞. In their case, p-norm is used to generalize 2-norm of their current and past gradients
while keeping the scale of adaptation unchanged. In sharp contrast, we intentionally change (reduce)
the scale of the adaptive term in Padam to get better generalization performance. Finally, note that
in Algorithm 1 we remove the bias correction step used in the original Adam paper following Reddi
et al. (2018). Nevertheless, our arguments and theory are applicable to the bias correction version
as well.
4 Convergence Analysis of the Proposed Algorithm
In this section, we establish the convergence theory of Algorithm 1 in the stochastic nonconvex op-
timization setting, i.e., we aim at solving the following stochastic nonconvex optimization problem
min f(x) ：= Eξ [f(x； ξ)],
x∈Rd
where ξ is a random variable satisfying certain distribution, f(x; ξ) : Rd → R is a L-smooth
nonconvex function. In the stochastic setting, one cannot directly access the full gradient of f (x).
Instead, one can only get unbiased estimators of the gradient of f (x), which is Vf (x; ξ). This setting
has been studied in Ghadimi & Lan (2013; 2016). We first introduce the following assumptions.
Assumption 4.1 (Bounded Gradient). f(x) = Eξf(x; ξ) has G∞-bounded stochastic gradient.
That is, for any ξ, we assume that kVf (x; ξ)k∞ ≤ G∞.
It is worth mentioning that Assumption 4.1 is slightly weaker than the '2-boundedness assump-
tion IlVf (x; ξ)k2 ≤ G2 used in Reddi et al. (2016); Chen et al. (2018). Since IlVf (x;ξ)k∞ ≤
∣∣Vf(x; ξ)∣∣2 ≤ √d∣∣Vf(x; ξ)k∞, the '2-boundedness assumption implies Assumption 4.1 with
5This does not mean the learning rate decay schedule weakens adaptive gradient method. On the contrary,
applying the learning rate decay schedule still gives performance boost to the adaptive gradient methods in
general but this performance boost is not as significant as SGD + momentum.
5
Under review as a conference paper at ICLR 2020
G∞ = G2. Meanwhile, G∞ will be tighter than G2 by a factor of √d when each coordinate of
Vf (x; ξ) almost equals to each other.
Assumption 4.2 (L-smooth). f(x) = Eξf(x; ξ) is L-smooth: for any x, y ∈ Rd, it satisfied that
If(X)- f(y) - hvf (y),X - yil ≤ Lkχ - yk2∙
Assumption 4.2 is frequently used in analysis of gradient-based algorithms. It is equivalent to the
L-gradient Lipschitz condition, which is often written as kVf (X) - Vf (y)k2 ≤ LkX - yk2. Next
we provide the main convergence rate result for our proposed algorithm.
Theorem 4.3 (Padam). In Algorithm 1, suppose that p ∈ [0, 1/2], β1 < β22p and αt = α for t =
1, . . . , T, under Assumptions 4.1 and 4.2, let ∆f = f(X1) - infx f (X), for any q ∈ [max{0, 4p -
1}, 1], the output Xout of Algorithm 1 satisfies that
EhIRf(XOut)II2] ≤ Ma + MTd + τM-dq2E(Xkg1zik2)1 q,	(4.1)
where
M1 = 2G2∞p∆f,
4G2+2PE||b-P||i + 4G2 m = 4LG∞+q-2p + 8LG∞+q-2p(1 -仇)(βι Y
d(1-βι)	+	∞,	3= (l-β2)2p + (1 - β2)2P(l - β1∕β∣p)U — βj .
Remark 4.4. From Theorem 4.3, we can see that M1 and M3 are independent of the number of
iterations T and dimension d. In addition, if kvb1-1 k∞ = O(1), it is easy to see that M2 also has an
upper bound that is independent of T and d.
The following corollary is a special case of Theorem 4.3 when p ∈ [0, 1/4] and q = 0.
Corollary 4.5. Under the same conditions in Theorem 4.3, ifp ∈ [0, 1/4], Padam’s output satisfies
kg1:T,i k2
(4.2)
where M1 and M2 and ∆f are the same as in Theorem 4.3, and M30 is defined as follows:
M30
4LG∞-2p +	8LG∞-2p(1 - βι)	( βι Y
(1 - β2)2p	(1 — β2)2P(l — βι∕β∣p) U - βι)
Remark 4.6. Corollary 4.5 simplifies the result of Theorem 4.3 by choosing q = 0 under the
condition p ∈ [0, 1/4]. We remark that this choice of q is optimal in an important special case
studied in Duchi et al. (2011); Reddi et al. (2018): when the gradient vectors are sparse, we assume
that Pd=IIlgLτ,i∣∣2《√dT. Then for q > 0, it follows that
PLkg1：T,ik2 « d (Pd=1kgib,ik2) j
T	«	T 1-q∕2
(4.3)
(4.3) implies that the upper bound provided by (4.2) is strictly better than (4.1) with q > 0. There-
fore when the gradient vectors are sparse, Padam achieves faster convergence when p ∈ [0, 1/4].
Remark 4.7. We show the convergence rate under different choices of step size α. If
then by (4.2), we have
-1
EhIWf(XOut)I∣2] = O((Pd="TM2)" + T).
(4.4)
Note that the convergence rate given by (4.4) is related to the sum of gradient norms Pid=1 Ig1:T,i I2.
As we mentioned in Remark 4.6, when the stochastic gradients g1:T,i, i = 1, . . . ,d are sparse, we
have Pd=I ∣∣g±τ,i∣∣2 ≪ √dT (DUChi et al. (2011)). More specifically, suppose Pd=I ∣∣g±τ,i∣∣2 =
O(ds√T) for some 0 ≤ S ≤ 1/2. We have E[∣Vf(xout)k2] = O(ds/2/T1/2 + d/T). When
S = 1/2, We have E[∣Vf(xout)k2] = O{d1/4/√T + d/T), which matches the rate O(1∕√T)
achieved by nonconvex SGD (Ghadimi & Lan, 2016), considering the dependence of T. It is worth
noting that nonconvex SGD achieves O(maxχ,ξ ∣∣Vf (x,; ξ)k2/√T) = O(，d/T) convergence rate
under Assumption 4.1, thus the convergence rate of our algorithm is strictly better than that of SGD
by a factor of d1/4 when T is large.
6
Under review as a conference paper at ICLR 2020
Remark 4.8. If We set α = 1∕√T which is not related to Pd=i ∣∣g±τ,i∣∣2, then (4.2) suggests that
Eh∣∣vf (χout)∣∣2i = O( √τ + T + T X llgi：T,i k2).
When Pd=I ∣∣g±τ,i∣∣2 ≪ √dT (Duchietal., 2011; ReddietaL,2018),we further have
E h∣∣ Vf(XOut) ∣∣2i = o( √T + T + rT)
which matches the convergence result in nonconvex SGD (Ghadimi & Lan, 2016) considering the
dependence of T.
5 Experiments
In this section, we empirically evaluate our proposed algorithm for training various modern deep
learning models and test them on several standard benchmarks 6. We show that for nonconvex
loss functions in deep learning, our proposed algorithm still enjoys a fast convergence rate, while
its generalization performance is as good as SGD with momentum and much better than existing
adaptive gradient methods such as Adam and Amsgrad. All experiments are conducted on Amazon
AWS p3.8xlarge servers which come with Intel Xeon E5 CPU and 4 NVIDIA Tesla V100 GPUs
(16G RAM per GPU). All experiments are implemented in Pytorch version 0.4.1 within Python
3.6.4.
(a) Train Loss for VGGNet
0.5
0.4
g 03-
(b) Train Loss for ResNet
0	20 40	60	∞ IM 120 1« 160 180 200
Epochs
(d) Test Error for VGGNet
0.1
0.0...........................................
0	20	40	60	∞ 100 120 1« 160 180 200
Epochs
(e) Test Error for ResNet
(c) Train Loss for WideResNet
(f) Test Error for WideResNet
Figure 2: Train loss and test error (top-1) plots of three CNN architectures on CIFAR-10 dataset.
We compare Padam against several state-of-the-art algorithms, including: (1) SGD-momentum, (2)
Adam (Kingma & Ba, 2015), (3) Amsgrad (Reddi et al., 2018), (4) AdamW (Loshchilov & Hutter,
2019) (5) Yogi (Zaheer et al., 2018) and (6) AdaBound (Luo et al., 2019) . We use several popular
datasets for image classifications and language modeling: CIFAR-10 (Krizhevsky & Hinton, 2009),
CIFAR-100 (Krizhevsky & Hinton, 2009), ImageNet dataset (ILSVRC2012) (Deng et al., 2009) and
Penn Treebank dataset (Marcus et al., 1993). We adopt three popular CNN architectures to evaluate
the performance of our proposed algorithms on image classification task: VGGNet-16 (Simonyan &
Zisserman, 2014), Residual Neural Network (ResNet-18) (He et al., 2016), Wide Residual Network
(WRN-16-4) (Zagoruyko & Komodakis, 2016). We test the language modeling task using 2-layer
and 3-layer Long Short-Term Memory (LSTM) network (Hochreiter & Schmidhuber, 1997). We
test CIFAR-10, CIFAR-100 and Penn Treebank tasks for 200 epochs. For all experiments, we use a
fixed multi-stage learning rate decaying scheme: we decay the learning rate by 0.1 at the 100th and
150th epochs. We test ImageNet tasks for 100 epochs with similar multi-stage learning rate decaying
6The code is available in the anonymous link
7
Under review as a conference paper at ICLR 2020
scheme at the 30th, 60th and 80th epochs. We perform grid search on validation set to choose the
best hyper-parameters for each algorithm. Details about the datasets, CNN architectures and all the
specific model parameters used in the following experiments can be found in the supplementary
materials.
5.1 Experimental Results
We compare our proposed algorithm
with other baselines on training the
aforementioned three modern CNN
architectures for image classifica-
tion on the CIFAR-10, CIFAR-100
and also ImageNet datasets. Due to
the paper length limit, we leave all
our experimental results regarding
CIFAR-100 dataset and all test re-
sult tables in the supplementary ma-
terials. Figure 2 plots the train loss
and test error (top-1 error) against
training epochs on the CIFAR-10
dataset. As we can see from the
figure, at the early stage of the
training process, (partially) adaptive
gradient methods including Padam,
make rapid progress lowing both
the train loss and the test error,
0.6
0.5
O 0∙4
UJ
⅛ 03
α)
0.2
0.1
0	10 20 30	«	50 60 70 80	90 100	0	10 20 30	«	50 60	70	∞	90 1∞
Epochs	Epochs
(a) Top-1 Error, VGGNet (b) Top-5 Error, VGGNet
(c) Top-1 Error, ResNet
Figure 3: Top-1 and Top-5 test error for VGGNet and ResNet
on ImageNet dataset.
0.35
0.30
S 0.25
LU
[S 0.20
0.15
0.10
Epochs
(d) Top-5 Error, ResNet
while SGD with momentum con-
verges relatively slowly. After the
first learning rate decaying at the
100-th epoch, different algorithms start to behave differently. SGD with momentum makes a huge
drop while fully adaptive gradient methods (Adam and Amsgrad) start to generalize badly. Padam,
on the other hand, maintains relatively good generalization performance and also holds the lead over
other algorithms. After the second decaying at the 150-th epoch, Adam and Amsgrad basically lose
all the power of traversing through the parameter space due to the “small learning dilemma”, while
the performance of SGD with momentum finally catches up with Padam. AdamW, Yogi and Ad-
aBound indeed improve the performance compared with original Adam but the performance is still
worse than Padam.
Overall we can see that Padam
achieves the best of both worlds
(i.e., Adam and SGD with momen-
tum): it maintains faster conver-
gence rate while also generalizing
as well as SGD with momentum in
the end. Figure 3 plots the Top-
1 and Top-5 error against training
epochs on the ImageNet dataset for
both VGGNet and ResNet. We can
see that on ImageNet, all methods
behave similarly as in our CIFAR-10
experiments. Padam method again
(a) 2-layer LSTM
Figure 4: Test perplexity for 2-layer and 3-layer LSTM
model on Penn Treebank dataset.
(b) 3-layer LSTM
obtains the best from both worlds by achieving the fastest convergence while generalizing as well as
SGD with momentum. Even though methods such as AdamW, Yogi and AdaBound have better per-
formance than standard Adam, they still suffer from a big generalization gap on ImageNet dataset.
Note that we did not conduct WideResNet experiment on Imagenet dataset due to GPU memory
limits.
We also perform experiments on the language modeling tasks to test our proposed algorithm on
Long Short-Term Memory (LSTM) network (Hochreiter & Schmidhuber, 1997), where adaptive
gradient methods such as Adam are currently the mainstream optimizers for these tasks. Figure 4
8
Under review as a conference paper at ICLR 2020
plots the test perplexity against training epochs on the Penn Treebank dataset (Marcus et al., 1993)
for both 2-layer LSTM and 3-layer LSTM models. We can observe that the differences on simpler
2-layer LSTM model is not very obvious but on more complicated 3-layer LSTM model, different
algorithms have quite different optimizing behaviors. Even though Adam, Amsgrad and AdamW
have faster convergence in the early stages, Padam achieves the best final test perplexity on this
language modeling task for both of our experiments.
6 Conclusions and Future Work
In this paper, we proposed Padam, which unifies Adam/Amsgrad with SGD-momentum. With an
appropriate choice of the partially adaptive parameter, we show that Padam can achieve the best
from both worlds, i.e., maintaining fast convergence rate while closing the generalization gap. We
also provide a theoretical analysis towards the convergence rate of Padam to a stationary point for
stochastic nonconvex optimization.
While the empirical generalization performances achieved by Padam backup our hypothesis of
“small learning rate dilemma”, it is still unclear how learning rate decay affects the performance
of adaptive gradient methods in theory. We leave it as an important future direction.
References
Amitabh Basu, Soham De, Anirbit Mukherjee, and Enayat Ullah. Convergence guarantees for rm-
sprop and adam in non-convex optimization and their comparison to nesterov acceleration on
autoencoders. arXiv preprint arXiv:1807.06766, 2018.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type
algorithms for nonconvex optimization. arXiv preprint arXiv:1808.02941, 2018.
Damek Davis, Dmitriy Drusvyatskiy, and Vasileios Charisopoulos. Stochastic algorithms with geo-
metric step decay converge linearly on sharp functions. arXiv preprint arXiv:1907.09547, 2019.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 248-255. Ieee, 2009.
Timothy Dozat. Incorporating nesterov momentum into adam. 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Rong Ge, Sham M Kakade, Rahul Kidambi, and Praneeth Netrapalli. The step decay schedule: A
near optimal, geometrically decaying learning rate procedure. arXiv preprint arXiv:1904.12838,
2019.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and
stochastic programming. Mathematical Programming, 156(1-2):59-99, 2016.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 770-778, 2016.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning
lecture 6a overview of mini-batch gradient descent, 2012.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
9
Under review as a conference paper at ICLR 2020
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from
adam to sgd. arXiv preprint arXiv:1712.07628, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive
stepsizes. arXiv preprint arXiv:1805.08114, 2018.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations, 2019.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. In Proceedings of the 7th International Conference on Learning Repre-
sentations, New Orleans, Louisiana, May 2019.
Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of english: The penn treebank. 1993.
Mahesh Chandra Mukkamala and Matthias Hein. Variants of rmsprop and adagrad with logarithmic
regret bounds. In International Conference on Machine Learning, pp. 2545-2553, 2017.
Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In International conference on machine learning, pp. 314-
323, 2016.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. In Advances in Neural Information Processing Systems,
pp. 91-99, 2015.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex
landscapes, from any initialization. arXiv preprint arXiv:1806.01811, 2018.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4151-4161, 2017.
Yuhuai Wu, Mengye Ren, Renjie Liao, and Roger Grosse. Understanding short-horizon bias in
stochastic meta-optimization. In International Conference on Learning Representations, 2018.
Saining Xie, Ross Girshick, Piotr Dollar, ZhUoWen Tu, and Kaiming He. Aggregated residual trans-
formations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 5987-5995. IEEE, 2017.
Yi Xu, Qihang Lin, and Tianbao Yang. Accelerate stochastic subgradient method by leveraging
local groWth condition. arXiv preprint arXiv:1607.01027, 2016.
Sergey Zagoruyko and Nikos Komodakis. Wide residual netWorks. arXiv preprint
arXiv:1605.07146, 2016.
10
Under review as a conference paper at ICLR 2020
Manzil Zaheer, Sashank Reddi, Devendra Singh Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive
methods for nonconvex optimization. In Advances in Neural Information Processing Systems,
2018.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Zijun Zhang, Lin Ma, Zongpeng Li, and Chuan Wu. Normalized direction-preserving adam. arXiv
preprint arXiv:1709.04546, 2017.
Fangyu Zou and Li Shen. On the convergence of adagrad with momentum for training deep neural
networks. arXiv preprint arXiv:1808.03408, 2018.
11
Under review as a conference paper at ICLR 2020
A Proof of the Main Theory
In this section, we provide a detailed version of proof of Theorem 4.3.
A.1 Proof of Theorem 4.3
Let x0 = x1 and
一,βι /	、	1	βι
Zt = "+I-/],*	Xt-I)= I./]"	i-βιxt-1,	(A.I)
we have the following lemmas:
Lemma A.1. Let Zt be defined in (A.1). For t ≥ 2, we have
zt+ι -	zt	= 1	-β	hI - (αt^Vt p)(αt-ιvt-i)	i (χt-ι	- Xt)	- αtVb-Pgt.	(AZ
and
Zt+1 - Zt = ~^-- (αt-]V-p] - at V-p)mt-i - αtV -Pgt∙	(A.3)
1 - /]
For t = 1, we have
z2 - z] = -α] Vb]-Pg].	(A.4)
Lemma A.2. Let zt be defined in (A.1). For t ≥ 2, we have
kzt+1 - ztk2 ≤ IlaVt pgt∣∣2 + [队R kxt-1 - xtk2∙
1 - /]
Lemma A.3. Let zt be defined in (A.1). For t ≥ 2, we have
IIVf (zt) - Vf(Xt)k2 ≤ L(T-^) ∙kxt - Xt-1k2.
Lemma A.4. Let vbt and mt be as defined in Algorithm 1. Then under Assumption 4.1, we have
kVf (x)k∞ ≤ G∞, kvbtk∞ ≤ G2∞ and kmtk∞ ≤ G∞.
Lemma A.5. Suppose that f has G∞-bounded stochastic gradient. Let /], /2 be the weight param-
eters, at, t = 1, . . . , T be the step sizes in Algorithm 1 and q ∈ [max{4p - 1, 0}, 1]. We denote
γ = /]//22P. Suppose that at = a and γ ≤ 1, then under Assumption 4.1, we have the following
two results:
XX 2f hllb-P ∣∣2i V T(1+q)∕2dqa2(1 - βl)G∞+q 4P)F X XX H	H V q
NatEl 11Vtp mtU2] ≤ ----------(1-β2)2p(1-γ )-----e( Nkg1"2J ,
and
G 2 甯「12-P ∣∣2] / T (1+q)∕2dq a2G∞+q-4pζ1 ∕Λl. ll V-q
2∑atEIJIVt gtH2] ≤	(1 -β2)2p	e(ɪzkgi：T，ik2j	.
Now we are ready to prove Theorem 4.3.
Proof of Theorem 4.3. Since f is L-smooth, we have:
f (zt+ι) ≤ f (zt)	+ Vf (zt)>(zt+1	- zt) +	^2kzt+1 - ztk2
=f (zt)	+ vf (xt)>(zt+1	- zt) +	(Vf(Zt) - Vf(Xt))>(zt+1	- zt) + W∣∣zt+1	- ztk2
{一一	{一一	2
Il	I2	S	;	}
I3
(A.5)
In the following, we bound I] , I2 and I3 separately.
12
Under review as a conference paper at ICLR 2020
Bounding term ʃɪ: When t = 1, we have
v/(x1)τ(z2 - Z1) = -Vf(Xι)>αιV-pgι.	(A.6)
For t ≥ 2, we have
Vf(Xt)T(Zt+1 - zt)
-I-	/1-1	,	∕∙~.	∕∙~.	∕∙~.
=Vf(Xt)T k⅛(……tv 尸)mt-1 - αtv-Pgtj
=JIa Vf (xt)τ (αt-iV-PI- at V-p)mt-i - Vf (Xt)TatV-pgt,	(A.7)
1 - βι
-p	-p
where the first equality holds due to (A.3) in Lemma A.1. For Vf (Xty (at-1Vt-p1 - atVt p)mt-ι
in (A.7), we have
Vf (Xt)T(αt-iV-pι - atV-p)mt-i ≤ ∣∣Vf (Xt)k∞ ∙ Ilat-IV-PI- at V-PII 1,1 ∙ ∣∣mt-i∣∣∞
≤ G∞[ I I at-iV-PIhI-UatV-PhIi
=G∞[ I I at-iv-p1II1 -IIatV-pIIi].	(A.8)
The first inequality holds because for a positive diagonal matrix A, we have xt Ay ≤ ∣∣x∣∣∞ ∙
-	-T	-T—T)
IlAk 1,1 ∙ ∣∣y∣∣∞. The second inequality holds due to at-iVt-i 占 atV-占 0. Next we bound
-Vf(Xt)TatV-pgt. We have
-Vf(Xt)TatV-pgt
=-Vf(Xt)Tat-iV-pιgt - Vf(Xt)T (atV-p - at-iV-pJgt
≤ -Vf(Xt)Tat-iV t-ιgt+ iiVf (Xt)km ∙ IIatVtP - at-iv t-1∣∣1,1 ∙ ∣gt∣∞
≤ -Vf(Xt)τat-1V-pigt + G∞(II at-1V-PIhI-IIatV-PhI)
=-Vf(Xt)Tat-iV-pιgt + G∞(U at-iV-pιIIι - IIatV-pIIi).	(A.9)
The first inequality holds because for a positive diagonal matrix A, we have xt Ay ≤ ∣∣x∣∞ ∙
-p	-p
IlAk 1,1 ∙ ∣∣y∣∣∞. The second inequality holds due to at-iVt-i 占 atVt p 占 0. Substituting (A.8)
and (A.9) into (A.7), we have
Vf (xt)τ(zt+1 - Zt) ≤-Vf(xt)τat-1V-PIgt + ɪ-ɪ G∞(II at-1V-p1II1-IIatV-pII1).
(A.10)
Bounding term /2： For t ≥ 1, we have
(Vf (zt) - Vf (xt))τ(zt+1 - Zt)
≤ IlVf(Zt)- Vf(Xt) I I 2 ∙ ∣zt+1 - zt∣2
≤(II atV-PgtII2 + 1队R l∣xt-1- xtll2) ∙]队R ∙ LkXt- Xt-1II2
1	- P1	1 1 - P1
=LI -\ IIatV-pgtII2 ∙ Ixt- xt-1∣2+ L(ɪɪ^) Iixt- xt-1∣2
≤ LllatV-pgtII2 + 2l(ɪɪ-) ∣Xt - Xt-1k2,	(A.11)
where the second inequality holds because of Lemma A.1 and Lemma A.2, the last inequality holds
due to Young,s inequality.
Bounding term /3： For t ≥ 1, we have
5∣∣zt+1 -	zt∣∣2	≤ y	[ i i atv-Pgt12	+ 1 氏R llxt-1	- xt∣∣2]
2	2	L	1 — p 1	J
≤ L∣∣atV-pgt∣∣2 +2l()2∣xt-1 - Xtk2.
1 - p1
(A.12)
13
Under review as a conference paper at ICLR 2020
The first inequality is obtained by introducing Lemma A.1.
For t = 1, substituting (A.6), (A.11) and (A.12) into (A.5), taking expectation and rearranging
terms, we have
E[f(z2)- f(zι)]
2
≤ E ] - Vf(XI)TaIV-pgι + 2L∖∖α1V-Pg川；+4L( T-^) |必-X0∣∣2
=E[-Vf(xι)TaIV-pgι + 2L∣∣αιV -pg1∖∖2]
≤ E[dɑ1G∞-2p + 2L∣∣αιV-pgι∖∖2],	(A.13)
where the last inequality holds because
-Vf(XI)TV-pgι ≤ d ∙ IIVf(Xι)k∞ ∙ ∣∣V-PgIh ≤ d ∙ Gg ∙ G∞-2p = dG∞;2p.
For t ≥ 2, substituting (A.10), (A.11) and (A.12) into (A.5), taking expectation and rearranging
terms, we have
E f (zt+1) +
Gg ∖∖ …∖∖ι
-1 - βι~
f (Zt) +
Gg W αt-i*∖∖ i
1 - βι
≤E
=E
≤E
-Vf (xt)>αt-iV-Plgt + 2L∖∖atVb-pgt∖∖2 +4L(ɪɪ-)	IlXt- Xt-Ik2
-Vf (xt)>αt-iV-pιVf(Xt)+ 2LlIatV-pgt∖∖2 +4L(ɪɪ-) ∖∖αt-iV-p1mt-1∖∖2]
-at-i∖∖Vf (Xt)∖∖2(G∞∞)-1 +2LIIatV-pgt∖∖2 +4L(T-^)2∖∖at-R-PImt-1∖∖2],
(A.14)
where the equality holds because E[gt] = Vf (Xt) conditioned on Vf (Xt) and V-p1, the second
inequality holds because of Lemma A.4. Telescoping (A.14) for t = 2 to T and adding with (A.13),
we have
T
(G∞P)-1 X αt-iE∖∖ Vf (Xt) ∖ ∖ 2
t=2
≤ E f (zι) + GgI二；。W l + daιG∞-2p -
Gg ∖∖ aτb-p ∖∖ ι λ
1 - β1	)
T	/ a ∖	2	T
+ 2L X E ∖ ∖ atV-pgt∖∖2	+4L( ]-β7)	X EhWat-IV-PImt-ι∖∖2i
t=l	∖ β"	t=2
≤ E ]∆f + GgI7β1∖∖l + daiGg2P + 2LXEWatV}gt∖∖2
+ 4L( T⅜ )2 X EhW atV-Pmt∖∖2].
(A.15)
By Lemma A.5, we have
T
Xa2EUV-Pmtk2] ≤
t=l
T(l+q”2dqa2(1 - βl)Gg+q-4P)F
(1- β2)2P(1- Y)
、l-q
llgl：T,ik2
(A.16)
where Y = βl∕β2P. We also have
T
Xa2E[∣∣V-Pgtk2] ≤
t=l
T (l+q"2dq a2Gg+q-4P)
(1 - β2)2P	E
∖l-q
l|gl：TMl2
(A.17)
—
14
Under review as a conference paper at ICLR 2020
Substituting (A.16) and (A.17) into (A.15), and rearranging (A.15), we have
Ek▽/(Xout)k2
1
2 αt-1 t=2
T
X αt-iE∣∣Vf (Xt)Il2
≤
PtT=
G2∞p
2 αt-1
EH + 「
+ dα1 G2∞-2p
2LG∞	T(I+q"2dq α2G∞+q-4p)
PtT=2 αt-1
(1 - β2)2p
E
1-q
kg1:T,ik2
+
PtT=
4LG2∞p
2 αt-1
βl
1 - βι
2T(I+q"2dqα2(1 - βι)G∞+q-4p)
(1 - β2)2p(1 - γ)
E
1-q
kg1:T,ik2
≤ Ta 2G∞∆f+T (空＞+
dqα
+ T (i-q)∕2E
kg1:T,ik2
1-q	4LG1∞+q-2p
(1 - β2)2p
+ 8LG∞+q-2p(1- βι) ( βι Y
+ (1 — β2)2p(l — Y)[1- βι)
(A.18)
+
where the second inequality holds because αt = α. Rearranging (A.18), we obtain
EkVf(Xout)k2 ≤ 黑 + MTd + TdqM/2E(X kg1Zik2)i ",
where {Mi}i3=1 are defined in Theorem 4.3. This completes the proof.
□
A.2 Proof of Corollary 4.5
Proof of Corollary 4.5. From Theorem 4.3, let p ∈ [0, 1/4], we have q ∈ [0, 1]. Setting q = 0, we
have
E IIVf(Xout)II22
M1
≤ — +
≤ Ta +
M2 ∙ d
T
+
kg1:T,ik2
where M1 and M2 are defined in Theorem 4.3 and M30 is defined in Corollary 4.5. This completes
the proof.
B Proof of Technical Lemmas
B.1 Proof of Lemma A.1
Proof. By definition, we have
	_	,	βι /	1	βι t+1 = t+1 +1-β1(t+1	t)=1-β1 t+1	1-β1 t.
Then we have	zt+1 - Zt = T⅛(Xt+1 - Xt)- T-⅛(Xt- XtT) =7⅛ ( - atV-Pmt) + √β⅛-at-ιV-p1mt-1. 1 - β1	1 - β1
15
Under review as a conference paper at ICLR 2020
The equities above are based on definition. Then we have
Zt+1 - Zt =	[βιmt-1 + (1 - βι)gt] +	β1 αt-iV-p1 mt-1
1 - P1 L	」1 - P1
=τ^λmmt-1(αt-1V--1 - αtV-P) - αtV-Pgt
1 - P1
=1-⅛αt-1V-PImt-I [l - (αtV-p)(αt-1V-PI)Ti - αtV-Pgt
=T-^ [l - (αtV-P)(at-1V-PI)Ti (xt-1 - Xt)- αtV-Pgt.
The equalities above follow by combining the like terms.	□
B.2 Proof OF Lemma A.2
Proof. By Lemma A.1, we have
kzt+1 - Zt∣∣2 = τ^⅛r [i - (αtV-P)(αt-1V-PI)Ti (Xt-I- Xt)- αtV-Pgt
1 - P1 l	」	2
≤ 1-PJI- (αtV-P)(αt-1V-PI)TL OO」Xt-1 - Xt∣∣2 + IIaV-Pgtll2，
where the inequality holds because the term β1∕(1 - β1) is positive, and triangle inequality. Con-
sidering that atb-j ≤ at-1b-P1 j, whenP > 0, we have ∣∣I - (atV-P)(at-1V-PI)T ∣∣	≤ 1.
With that fact, the term above can be bound as:
l∣zt+1 - zt∣∣2 ≤ ||aVt Pgt|12 + [队R llXt-1 - χt∣∣2.
1 - P1
This completes the proof.	□
B.3	Proof of Lemma A.3
Proof. For term ∣∣V∕(zt) - ▽/(Xt)Il2, we have:
INf (Zt) - Vf(Xt)∣∣2 ≤ LkZt- Xt∣∣2
≤ L
P1
1 - β1
(Xt-Xt-I) II2
≤ L( 1⅛ )…Xt-Xt-1k2,
where the last inequality holds because the term β1∕(1 - β1) is positive.
□
B.4	PROOF OF LEMMA A.4
ProofofLemma A.4. Since f has G∞-bounded stochastic gradient, for any X and ξ,
∣∣Vf (x; ξ)∣∞ ≤ G∞. Thus, we have
∣Vf(X)∣∞ =㈣Vf(x;ξ)∣∞ ≤ Eξ∣Vf(x;ξ)∣∞ ≤ G∞.
Next we bound ∣∣mt∣∞. We have ∣∣m0∣ = 0 ≤ G∞. SUPPOSethatkmth ≤ G∞, then for mt+1,
we have
l∣mt+1∣∞ = ∣∣β1mt + (1 - β1)gt+11∣∞
≤ β1∣lmt∣∣∞ + (1 - β1)∣∣gt+1∣∣∞
≤ β1G∞ + (1 - β1)G∞
G
∞CO .
16
Under review as a conference paper at ICLR 2020
Thus, for any t ≥ 0, we have Ilmt∣∣∞ ≤ G∞. Finally we bound IIvtIl∞. First we have ∣∣v0∣∣∞ =
∣∣V01∣∞ = 0 ≤ G∞. Suppose that ∣Vt∣∣∞ ≤ G∞ and IlVtI∣∞ ≤ G∞. Note that we have
llvt+ιk∞ = M2vt + (1 - β2)g2+ι∣∣∞
≤ β2∣∣vt∣∣∞ + (I- β2)∣∣g2+ι∣∣∞
≤ β2g∞ + (1 - β2)G∞
=G∞,
and by definition, we have ∣∣vt+ι∣∣∞ = max{∣∣Vt∣∣∞, ∣∣vt+ι∣∣∞} ≤ G∞. Thus, for any t ≥ 0, we
have IlVtI∣∞ ≤ G∞.	□
B.5 Proof of Lemma A.5
Proof. Recall that Vtj, mt,j, gt,j denote the j-th coordinate of Vt, mt and gt. We have
α2E UV-Pmtk2]
2E「X (Pj=ι(i-βι)βt-%∙,i)2]
=αt *(PL(I-β2)β2-%)2pJ,
where the first inequality holds because Vt,i ≥ Next we have
2E JX (Pt=ι(1-β1)βt-%∙,i)2 -
αt [⅛1 (Pt=ι(i-β2)β2一%”
≤ α2(i-βι)2EJXX (Ptt=Iβt-j∣gj,i∣(1+q-4P))(P"βt-jl”i∣(1-q+4P))-
≤ (1-伤产 吟	(P；=i β-jg2,i)2P	一
< α2(1-βι)2E JX (Pt=i βt-jG∞+q-4p))(Pt=ι βt-j∣gj,i∣(IT+4P))-
≤ (1-β2)2p	*	(Pj=ι β2-jg2,i)2p	-
≤ α2(1- β1)G∞+q-4P) E JX P；=1 βt-j∣gj,iI(I-q+4p)-
≤ -(1-β2)2P-吟(Pt = 1 β2-%i)2P	一，
(B.1)
(B.2)
where the first inequality holds due to Cauchy inequality, the second inequality holds because
∣%∙,i∣ ≤ G∞, the last inequality holds because P；=1 βt-j ≤ (1 - β1)-1. Note that
d
X
i=1
Pj=1 βt-j∣”i∣(I-q+4P)
(Pj=1 β2-j g2,i)2P
dt
≤ XX
i=1 j=1
βt-j∣gj,i∣(1-q+4P)
Ejgji )2p
dt
XX Yt-j ∣gj,i∣1-q,
i=1j=1
(B.3)
where the equality holds due to the definition of Y. Substituting (B.2) and (B.3) into (B.1), we have
叫忖-PmtI∣2] ≤ α2(I 二)；[。)EJXXYt-j∣gj,i∣1-q].	(B.4)
Telescoping (B.4) for t = 1 to T, we have
T	Γ	1 λv2∩ R S(I+q-4P)	「T d t	-I
X四忖-Pmtk2] ≤ α (11-βG∞P— E[XXXYt-j∣gj,i∣1-q
α2 (1 - β1)G∞+q-4P)
(1 - β2)2P
「d T	T
E [XX ki∣1-q X
Yt-j
dT
E XX ∣gj,i∣1-q
Li=1j=1	-
≤
α2 (1 - β1)G∞+q-4P)
(1- β2)2P(1- Y)
(B.5)
17
Under review as a conference paper at ICLR 2020
Finally, we have
d T	d T	(1 q)/2
XX ∣gj,i∣1-q ≤ X(X g2,i)(	•…/
i=1 j=1	i=1 j=1
d
= T(1+q)/2 X kg1:T,ik12-q
i=1
≤ T (1+q)/2dq Xd kg1:T,ik21-q,	(B.6)
where the first and second inequalities hold due to Holder,s inequality. Substituting (B.6) into (B.5),
we have
Xα2E[kV-Pmtk2i ≤ T(∖2d-β⅛-(β-G∞+i)E(X “皿厂.
Specifically, taking β1 = 0, we have mt = gt, then
G 2 甯 Γ∣∣b-P ll21 / T (1+q"2dq α2G∞+q-4pζ1 ∕Λl.	ll V-q
TE atE |_k Vt gtk2∣ ≤	(1 — β2 )2p	E( ʌ/ kgl：T,ik2 J	.
□
C Experiment Details
C.1 Datasets
We use several popular datasets for image classifications.
•	CIFAR-10 (Krizhevsky & Hinton, 2009): it consists of a training set of 50, 000 32 × 32
color images from 10 classes, and also 10, 000 test images.
•	CIFAR-100 (Krizhevsky & Hinton, 2009): itis similar to CIFAR-10 but contains 100 image
classes with 600 images for each.
•	ImageNet dataset (ILSVRC2012) (Deng et al., 2009): ILSVRC2012 contains 1.28 million
training images, and 50k validation images over 1000 classes.
In addition, we adopt Penn Treebank (PTB) dataset (Marcus et al., 1993), which is widely used in
Natural Language Processing (NLP) research. Note that word-level PTB dataset does not contain
capital letters, numbers, and punctuation. Models are evaluated based on the perplexity metric
(lower is better).
C.2 Architectures
VGGNet (Simonyan & Zisserman, 2014): We use a modified VGG-16 architecture for this exper-
iment. The VGG-16 network uses only 3 × 3 convolutional layers stacked on top of each other
for increasing depth and adopts max pooling to reduce volume size. Finally, two fully-connected
layers 7 are followed by a softmax classifier.
ResNet (He et al., 2016): Residual Neural Network (ResNet) introduces a novel architecture with
“skip connections” and features a heavy use of batch normalization. Such skip connections are
also known as gated units or gated recurrent units and have a strong similarity to recent successful
elements applied in RNNs. We use ResNet-18 for this experiment, which contains 2 blocks for each
type of basic convolutional building blocks in He et al. (2016).
7For CIFAR experiments, we change the ending two fully-connected layers from 2048 nodes to 512 nodes.
For ImageNet experiments, we use batch normalized version (vgg16,bn) provided in Pytorch official package
18
Under review as a conference paper at ICLR 2020
Figure 5: Plot of max and min values across all coordinates of vbt against the iteration number for
ResNet model on CIFAR10 dataset.
Wide ResNet (Zagoruyko & Komodakis, 2016): Wide Residual Network further exploits the “skip
connections” used in ResNet and in the meanwhile increases the width of residual networks. In
detail, we use the 16 layer Wide ResNet with 4 multipliers (WRN-16-4) in our experiments.
LSTM (Hochreiter & Schmidhuber, 1997): Long Short-Term Memory (LSTM) network is a special
kind of Recurrent Neural Network (RNN), capable of learning long-term dependencies. It was
introduced by Hochreiter & Schmidhuber (1997), and was refined and popularized in many followup
work.
C.3 Parameter Settings
We perform grid searches to choose the best hyper-parameters for all algorithms in both im-
age classification and language modeling tasks. For the base learning rate, we do grid search
over {0.0001, 0.001, 0.01, 0.1, 1, 10, 100} for all algorithms, and choose the partial adaptive pa-
rameter p from {2/5, 1/4, 1/5, 1/8, 1/16} and the second order moment parameter β2 from
{0.9, 0.99, 0.999}. Specifically, in terms of image classification experiments, for SGD with mo-
mentum, the base learning rate is set to be 0.1 with a momentum parameter of 0.9. For all adaptive
gradient methods except Padam, we set the base learning rate as 0.001. For AdaBound, the final
learning rate is set to be 0.1. For Padam, the base learning rate is set to be 0.1 and the partially
adaptive parameter p is set to be 1/8 due to its best empirical performance. For Adam, Amsgrad,
the momentum parameters are set to be β1 = 0.9, β2 = 0.99. And for other adaptive gradient
methods including Padam, we set β1 = 0.9, β2 = 0.999. The weight decay factor is set to 5 × 10-4
for all methods except AdamW which adopts a different weight decay scheme. For AdamW, the
normalized weight decay factor is set to 2.5 × 10-2 for CIFAR and 5 × 10-2 for ImageNet. For
Yogi, is set as 10-3 as suggested in the original paper. For AdaBound, the final learning rate is
set as 0.1 as suggested in Luo et al. (2019). The minibatch sizes for CIFAR-10 and CIFAR-100
are set to be 128 and for ImageNet dataset we set it to be 256. Regarding the LSTM experiments,
for SGD with momentum, the base learning rate is set to be 1 for 2-layer LSTM model and 10 for
3-layer LSTM. The momentum parameter is set to be 0.9 for both models. For all adaptive gradient
methods except Padam and Yogi, we set the base learning rate as 0.001. For Yogi, we set the base
learning rate as 0.01 for 2-layer LSTM model and 0.1 for 3-layer LSTM model. For Padam, we
set the base learning rate as 0.01 for 2-layer LSTM model and 1 for 3-layer LSTM model. For all
adaptive gradient methods, we set β1 = 0.9, β2 = 0.999. In terms of algorithm specific parameters,
for Padam, we set the partially adaptive parameter p as 0.4 for 2-layer LSTM model and 0.2 for
3-layer LSTM model. For AdaBound, we set the final learning rate as 10 for 2-layer LSTM model
and 100 for 3-layer LSTM model. For Yogi, is set as 10-3 as suggested in the original paper. The
weight decay factor is set to 1.2 × 10-6 for all methods except AdamW which adopts a different
weight decay scheme. For AdamW, the normalized weight decay factor is set to 4 × 10-4 . The
minibatch size is set to be 20 for all LSTM experiments.
D Additional Experiments
As a sanity check, we first plot the max and min of vbt across all the coordinates in Figure 5. In detail,
the maximum value of vbt is around 0.04 in the end and the minimum value is around 3 × 10-8,
19
Under review as a conference paper at ICLR 2020
max{(vbt)i1/8} - min{(vbt)i1/8} ≈ 0.55. We can see that the effective learning rates for different
coordinates are quite different.
.Ioxlwttc,J.
0.3 ∙
0.0
O 20	40 SO 80 1∞ 120
Epochs
160 180 200
160 180 200
O 20	40	60	80 IOO 120
Epochs
SSO*∣ C-EF
o.o.............................
O 20	40 so 80 IOO 120 140 160 180 200
Epochs
(c) Train Loss for WideResNet
(a) Train Loss for VGGNet
0	20	40	60	80 100 120 140 160 180 200
Epochs
(d) Test Error for VGGNet
0.25-
∙ioj∙i1*jtt3J,
(b) Train Loss for ResNet
0.20-
O 20	40	60	80 IOO 120 140 160 180 200
印 OChS
(e) Test Error for ResNet
∙ioj∙i1*jtt3J,
0.25
---SGD-Momentum
Adam
—Amsgrad
—AdamW
---Yogi
AdaBound
---Padam
O 20	40	60	80 IOO 120 140 160 180 200
印 OChS
(f) Test Error for WideResNet
Figure 6: Train loss and test error (top-1 error) of three CNN architectures on CIFAR-100. In all
cases, Padam achieves the fastest training procedure among all methods and generalizes as well as
SGD with momentum.
Figure 6 plots the train loss and test error (top-1 error) against training epochs on the CIFAR-100
dataset. We can see that Padam achieves the best from both worlds by maintaining faster conver-
gence rate while also generalizing as well as SGD with momentum in the end.
Tables 1 and 2 show the test accuracy of all algorithms on CIFAR-10 dataset and CIFAR-100 dataset
respectively. On CIFAR-10 dataset, methods such as Adam and Amsgrad have the lowest test ac-
curacies. Even though followup works such as AdamW, Yogi, AdaBound improve upon original
Adam, they still fall behind or barely match the performance of SGD with momentum. In contrast,
Padam achieves the highest test accuracy in VGGNet and WideResNet for CIFAR-10, VGGNet
and ResNet for CIFAR-100. On the other two tasks (ResNet for CIFAR-10 and WideResNet for
CIFAR-100), Padam is also on a par with SGD with momentum at the final epoch (differences less
than 0.2%). This suggests that practitioners should once again, use fast to converge partially adap-
tive gradient methods for training deep neural networks, without worrying about the generalization
performances.
Table 1: Final test accuracy of all algorithms on CIFAR-10 dataset. Bold number indicates the best
result.
Models	Test Accuracy (%)						
	SGDM	Adam	Amsgrad	AdamW	Yogi	AdaBound	Padam
VGGNet	93.71	92.21	92.54	93.54	92.94	93.28	93.78
ResNet	95.00	92.89	93.53	94.56	93.92	94.16	94.94
WideResNet	95.26	92.27	92.91	95.08	94.23	93.85	95.34
Table 3 shows the final test accuracy of all algorithms on ImageNet dataset. Again, we can observe
that Padam achieves the best test accuracy on VGGNet (both Top-1 and Top-5) and Top-1 accuracy
on ResNet. It stays very close to the best baseline of Top-1 accuracy on ResNet model.
Table 4 shows the final test perplexity of all algorithms on the Penn Treebank dataset. As we can
observe from the table, Padam achieves the best (lowest) test perplexity on both 2-layer LSTM and
3-layer LSTM models.
20
Under review as a conference paper at ICLR 2020
Table 2: Final test accuracy of all algorithms on CIFAR-100 dataset. Bold number indicates the best
result.
Models ___________________________TestAccUracy ⑸________________________
	SGDM	Adam	Amsgrad	AdamW	Yogi	AdaBoUnd	Padam
VGGNet	73.32	66.60	69.40	73.03	72.35	72.00	74.39
ResNet	77.77	71.72	72.62	76.69	74.55	75.29	77.85
WideResNet	76.66	71.83	73.02	76.04	74.47	73.49	76.42
Table 3: Final test accUracy on ImageNet dataset							
Models	Test AccUracy (%)						
	SGDM	Adam	Amsgrad	AdamW	Yogi	AdaBoUnd	Padam
ResNet Top-1	70.23	63.79	67.69	67.93	68.23	68.13	70.07
ResNet Top-5	89.40	85.61	88.15	88.47	88.59	88.55	89.47
VGGNet Top-1	73.93	69.52	69.61	69.89	71.56	70.00	74.04
VGGNet Top-5	91.82	89.12	89.19	89.35	90.25	89.27	91.93
Table 4: Final test perplexity (lower is better) on Penn Treebank dataset
Models	Test Perplexity						
	SGDM	Adam	Amsgrad	AdamW	Yogi	AdaBoUnd	Padam
2-layer LSTM	63.37	61.58	62.56	63.93	64.13	63.14	61.53
3-layer LSTM	61.22	60.44	61.92	63.24	60.01	60.89	58.48
21