Under review as a conference paper at ICLR 2020
Projected Canonical Decomposition
for Knowledge Base Completion
Anonymous authors
Paper under double-blind review
Ab stract
The leading approaches to tensor completion and link prediction are based on the
canonical polyadic (CP) decomposition of tensors. While these approaches were
originally motivated by low rank approximations, the best performances are usu-
ally obtained for ranks as high as permitted by computation constraints. For large
scale factorization problems where the factor dimensions have to be kept small,
the performances of these approaches tend to drop drastically. The other main
tensor factorization model, Tucker decomposition, is more flexible than CP for
fixed factor dimensions, so we expect Tucker-based approaches to yield better per-
formance under strong constraints on the number of parameters. However, as we
show in this paper through experiments on standard benchmarks of link prediction
in knowledge bases, ComplEx, (Trouillon et al., 2016), a variant of CP, achieves
similar performances to recent approaches based on Tucker decomposition on all
operating points in terms of number of parameters. In a control experiment, we
show that one problem in the practical application of Tucker decomposition to
large-scale tensor completion comes from the adaptive optimization algorithms
based on diagonal rescaling, such as Adagrad. We present a new algorithm for
a constrained version of Tucker which implicitly applies Adagrad to a CP-based
model with an additional projection of the embeddings onto a fixed lower dimen-
sional subspace. The resulting Tucker-style extension of ComplEx obtains similar
best performances as ComplEx, with substantial gains on some datasets under
constraints on the number of parameters.
1	Introduction
The problems of representation learning and link prediction in multi-relational data can be formu-
lated as a binary tensor completion problem, where the tensor is obtained by stacking the adjacency
matrices of every relations between entities. This tensor can then be intrepreted as a "knowledge
base", and contains triples (subject, predicate, object) representing facts about the world. Link pre-
diction in knowledge bases aims at automatically discovering missing facts (Bordes et al., 2011;
Nickel et al., 2011; Bordes et al., 2013; Nickel et al., 2016a; Nguyen, 2017).
State of the art methods use the canonical polyadic (CP) decomposition of tensors (Hitchcock, 1927)
or variants of it (Trouillon et al., 2016; Kazemi & Poole, 2018; Lacroix et al., 2018). While initially
motivated by low-rank assumptions on the underlying ground-truth tensor, the best performances
are obtained by setting the rank as high as permitted by computational constraints, using tensor
norms for regularization (Lacroix et al., 2018). However, for large scale data where computational
or memory constraints require ranks to be low (Lerer et al., 2019), performances drop drastically.
Tucker decomposition is another multilinear model which allows richer interactions between entities
and predicate vectors. A special case of Tucker decomposition is RESCAL (Nickel et al., 2011), in
which the relations are represented by matrices and entities factors are shared for subjects and ob-
jects. However, an evaluation of this model in Nickel et al. (2016b) shows that RESCAL lags behind
other methods on several benchmarks of interest. Recent work have obtained more competitive re-
suits with similar models (Balazevic et al., 2019b; Wang et al., 2019), using different regularizers or
deep learning heuristics such as dropout and label smoothing. Despite these recent efforts, learning
Tucker decompositions remains mostly unresolved. Wang et al. (2019) does not achieve state of the
art results on standard benchmarks, and we show (see Figure 3) that the performances reported by
1
Under review as a conference paper at ICLR 2020
Balazevic et al. (2019b) are actually matched by ComplEx (Trouillon et al., 2016; Lacroix et al.,
2018) optimized with Adam, which has less hyperparameters.
In this work, we overcome some of the difficulties associated with learning a Tucker model for
knowledge base completion. Balazevic et al. (2019b) use deep-learning mechanisms such as batch
normalization (Ioffe & Szegedy, 2015), dropout (Srivastava et al., 2014) or learning-rate annealing
to address both regularization and optimization issues. Our approach is different: We factorize the
core tensor of the Tucker decomposition with CP to obtain a formulation which is closer to CP
and better understand what difficulties appear. This yields a simple approach, which has a single
regularization hyperparameter to tune for a fixed model specification.
The main novelty of our approach is a more careful application of adaptive gradient techniques.
State-of-the-art methods for tensor completion use optimization algorithms with adaptive diagonal
rescaling such as Adagrad (Duchi et al., 2011) or Adam (Kingma & Ba, 2014). Through control
experiments in which our model is equivalent to CP up to a fixed rotation of the embeddings, we
show that one of the difficulties in training Tucker-style decompositions can be attributed to the lack
of invariance to rotation of the diagonal rescaling. Focusing on Adagrad, we propose a different
update rule that is equivalent to implicitely applying Adagrad to a CP model with a projection of the
embedding to a lower dimensional subspace.
Combining the Tucker formulation and the implicit Adagrad update, we obtain performances that
match state-of-the-art methods on the standard benchmarks and achieve significanly better results
for small embedding sizes on several datasets. Compared to the best current algorithm for Tucker
decomposition of Balazevic et al. (2019b), our approach has less hyperparameters, and We effec-
tively report better performances than the implementation of ComplEx of Lacroix et al. (2018) in
the regime of small embedding dimension.
We discuss the related work in the next section. In Section 3, we present a variant of the Tucker
decomposition which allows to interpolate between Tucker and CP. The extreme case of this vari-
ant, which is equivalent to CP up to a fixed rotation of the embedding, serves as control model to
highlight the deficiency of the diagonal rescaling of Adagrad for Tucker-style decompositions in
experiments reported in Section 4 . We present the modified version of Adagrad in Section 5 and
present experimental results on standard benchmarks of knowledge base completion in Section 7.
2	Link prediction in knowledge bases
Notation Tensors and matrices are denoted by uppercase letters. For a matrix U, ui is the vector
corresponding to the i-th row of U. The tensor product is written 0 and the Hadamard product (i.e.,
elementwise product) is written .
2.1	Learning setup
A knowledge base consists ofa set S of triples (subject, predicate, object) that represent (true) known
facts. The goal of link prediction is to recover facts that are true but not in the database. The data
is represented as a tensor X ∈ {0, 1 }N×L×N for N the number of entities and L the number of
predicates. Given a training set of triples, the goal is to provide a ranking of entities for queries of
the type (subject, predicate, ?) and (?, predicate, object). Following Lacroix et al. (2018), we use the
cross-entropy as a surrogate of the ranking loss. As proposed by Lacroix et al. (2018) and Kazemi
& Poole (2018), we include reciprocal predicates: for each predicate P in the original dataset, and
given an item o, each query of the form (?, P, o) is reformulated as a query (o, P-1, ?), where o is
now the subject ofP-1. This doubles the effective number of predicates but reduces the problem to
queries of the type (subject, predicate, ?) only.
For a given triple (i, j, k) ∈ S, the training loss function for a tensor X is then
'i,j,k (X) = -Xi,j,k + log ( X exp(xi,j,k0).	(1)
k06=k
2
Under review as a conference paper at ICLR 2020
ʌ
For a tensor decomposition model X(θ) parameterized by θ, the parameters θ are found by minimiz-
ing the regularized empirical risk with regularizer Λ:
1
θ = argmin L (θ) = argmin 而	'j (X (θ)) + νΛ( θ).	(2)
θ	θ | | (i,j,k)∈S
This work studies specific models for X(θ), inspired by CP and Tucker decomposition. We discuss
the related work on tensor decompositions and link prediction in knowledge bases below.
2.2	Related work
2.2.1	Canonical Decomposition and its variants
The canonical polyadic (CP) decomposition of a tensor X is defined entrywise by
∀i, j, k, Xi,j,k = hui , vj , wk i
d
uir vjr wkr .
r=1
The smallest value of d for which this decomposition exists is the rank of X. Each element Xi,j,k is
thus represented as a multi-linear product of the 3 embeddings in Rd associated respectively to the
ith subject, the jth predictate and the kth object.
CP currently achieves near state-of-the-art performances on standard benchmarks of knowledge base
completion (Kazemi & Poole, 2018; Lacroix et al., 2018). Nonetheless, the best reported results are
with the ComplEx model (Trouillon et al., 2016), which learns complex-valued embeddings and
sets the embeddings of the objects to be the complex conjugate of the embeddings of subjects, i.e.,
Wk = Irk. Prior to ComplEx, Dismult was proposed (Yang et al., 2014) as a variant of CP with
w = uk . While this model obtained good performances (Kadlec et al., 2017), it can only model
symmetric relations and does not perform as well as ComplEx. CP-based models are optimized with
vanilla Adam or Adagrad and a single regularization parameter (Trouillon et al., 2016; Kadlec et al.,
2017; Lacroix et al., 2018) and do not require additional heuristics for training.
2.2.2	Tucker Decomposition and its variants
Given a tensor X of size N × L ×N, the Tucker decomposition ofX is defined entrywise by
d1	d2	d3
∀i,j,k,	Xi,j,k = hui 於 Vj 於 Wk,Ci ：=ΣΣΣCr1,r2,r3 uir1 vjr2 wkr3 .
r1 =1 r2=1 r3=1
The triple (d1, d2, d3) are the rank parameters of the decomposition. We also use a multilinear
product notation X = [[C; U, V, W]], where U, V, W are the matrices whose rows are respectively
uj, vk, wl and C the three dimensional d1 × d2 × d3 core tensor. Note that the CP decomposition
is a Tucker decomposition in which d1 = d2 = d3 = d and C is the identity, which we write
[[U, V, W]]. With a non-trivial core tensor, Tucker decomposition is thus more flexible than CP for
fixed embedding size. In knowledge base applications, we typically have d ≤ L N, so the vast
majority of the model parameters are in the embedding matrices of the entities U and W . When
constraints on the number of model parameters arise (e.g., memory constraints), Tucker models
appear as natural candidates to increase the expressivity of the decomposition compared to CP with
limited impact on the total number of parameters.
While many variants of the Tucker decomposition have been proposed in the literature on tensor
factorization (see e.g., Kolda & Bader, 2009), the first approach based on Tucker for link prediction
in knowledge bases is RESCAL (Nickel et al., 2011). RESCAL uses a special form of Tucker
decomposition in which the object and subject embeddings are shared, i.e., U = W, and it does
not compress the relation matrices. In the multilinear product notation above, a RESCAL model
is thus written as X = [[C; U, I, U]]. Despite some success on a few smaller datasets, RESCAL
performances drop on larger datasets (Nickel et al., 2016b). This decrease in performances has
been attributed either to improper regularization (Nickel et al., 2011) or optimization issues (Xue
et al., 2018). Balazevic et al. (2019b) revisits Tucker decomposition in the context of large-scale
knowledge bases and resolves some of the optimization and regularization issues using learning rate
3
Under review as a conference paper at ICLR 2020
annealing, batch-normalization and dropout. It comes at the price of more hyperparameters to tune
for each dataset (label smoothing, three different dropouts and a learning rate decay), and as we
discuss in our experiments, the results they report are not better than ComplEx for the same number
of parameters.
Two methods were previously proposed to interpolate between the expressivity of RESCAL and
CP. Xue et al. (2018) expands the HolE model (Nickel et al., 2016b) (and thus the ComplEx model
(Hayashi & Shimbo, 2017)) based on cross-correlation of embeddings to close the gap in expressivity
with the Tucker decomposition for a fixed embedding size. Jenatton et al. (2012) express the relation
matrices in RESCAL as low-rank combination of a family of matrices. We describe the link between
these approaches and ours in Appendix 9.4. None of these approach however studied the effect of
their formulation on optimization, and reported results inferior to ours.
2.2.3	Other approaches
(Graph) neural networks for link prediction Several methods have introduced models that go
beyond the form of Tucker and canonical decompositions. ConvE (Dettmers et al., 2018) uses a
convolution on a 2D tiling of the subject and relation embeddings as input to a 2-layer neural net
that produces a new embedding for the pair, then compares to the object embedding. Graph neural
networks (Scarselli et al., 2009; Niepert et al., 2016; Li et al., 2016; Bruna et al., 2014) have recently
gained popularity and have been applied to link prediction in knowledge bases by Schlichtkrull et al.
(2018). This model uses a graph convolutional architecture to generate a variant of CP.
Poincare embeddings Poincare embeddings have been proposed as an alternative to usual tensor
decomposition approaches to learn smaller embeddings when the relations are hierarchical (Nickel
& Kiela, 2017). The method has recently been extended to link prediction in relational data with very
good performance trade-offs for small dimensional embeddings on the benchmark using WordNet
(Balazevic et al., 2019a), which contains relationships such as hypernyms and hyponyms which are
purely hierarchical. However, such good results do not extend to other benchmarks.
3	Interpolating between CP and Tucker
In order to better understand the underlying difficulties in learning (variants of) Tucker decompo-
sitions compared to CP, our analysis starts from a Tucker model in which the core tensor is itself
decomposed with CP. Given a N × L × N tensor, a fixed d and assuming a (d, d, d) Tucker de-
composition to simplify notation, a Tucker model where the core tensor is itself decomposed with a
rank-D CP can be written as (details are given in Appendix 9.3):
Xijk = hui 乳 Vj 乳 Wk, C i = hPI Ui, P2 Vj, P3 Wk i or equivalently X = [ UP>, VP2^> ,WP,> J,
where P1 , P2 , P3 are all D × d matrices. Since most knowledge bases have much fewer predicates
than entities (L N), the dimension of the predictate factors has little impact on the overall number
of model parameters. So in the remainder of the paper, we always consider P2 = I . Learning
matrices U, V, W, P1 , P3 of this decomposition simultaneously leads to the following model, which
we call CP-Tucker (CPT):
(CPT) Xijk = hP1 ui,Vj,P3Wki, ui,Wk ∈ Rd,Vj ∈ RD,Pi ∈ RD×d.
The CPT model is similar to a CP model except that the embedding matrices U and W have an
additional low-rank constraint (d instead of D). We say that the model interpolates between CP and
Tucker because for D = d it is equivalent to CP (as long as P1 and P3 are full rank), whereas for
D = d2 we recover a full Tucker model because the matrices P1 and P3 can be chosen such that
hP1 ui, Vj, P3Wki = uiMat(Vj)VjT, where Mat is the operator that maps a d2 vector to a d × d matrix
(see Appendix 9.5).
CPT is similar to CANDELINC (Carroll et al., 1980), except that in CANDELINC the factors U, V
and W are fixed and used to compress the data in order to efficiently learn the Pi . Closer to CPT,
Bro & Andersson (1998) first learn a Tucker3 decomposition of X before applying CANDELINC
using the learned factors. These methods are only applicable to least-square estimation, and for
tensors of smaller scale than knowledge bases.
4
Under review as a conference paper at ICLR 2020
(包Zq S二Eb0三)
101
102	103
Parameters per entities
(a) Performances on FB15K-237 in the control exper-
iments for D = d, i.e., when PCP is a reparameteriza-
tion of CP. We observe that PCP and CPT with vanilla
Adagrad, which are variants of Tucker, underperform
compared to CP. As expected in this case D = d, our
modification of the Adagrad update leads to the same
performances for PCP as for CP.
- -
O O
1 1
(ə-eɔs0-) 9e>
0	200	400
Sorted Adagrad coefficients
(b) Adagrad coefficients for subject/object embedding
matrices in the control experiments (D = d) for CP
and PCP, averaged by columns (i.e., embedding di-
mension) and sorted by values. Adagrad coefficients
decay exponentially for CP, but the values are similar
across most dimensions in PCP: the fixed unitary trans-
form in PCP removes the benefit of Adagrad.
Figure 1:	Results of the control experiment of Section 4.
Fixed projection matrices: The Projected Canonical Polyadic (PCP) Decomposition In order
to clarify the difficulty that arise when learning a CPT model compared to a CP model, we study
a simpler model in which the matrices P1 and P3 are not learned but rather fixed during training
and taken as random matrices with orthonormal columns. We call the resulting model the Projected
Canonical Polyadic (PCP) decomposition, since P1, P3 project the embeddings of dimension d into
a higher dimension D:
(PCP) Xijk = hP1ui,vj, P3wki, ui,wk ∈ Rd,vj ∈ RD, fixed P1, P3 ∈ RD×d
When D = d the matrices Pi are then fixed unitary transformations. The PCP (or CPT) model in
this case D = d is then equivalent to a CP model, up to a fixed invertible transformation of the
embeddings. The capacity of the model grows beyond that of CP as D increases up to d2 .
4	Motivation: optimization issues with CPT and PCP
As discussed in the related works, previous results suggest that Tucker models are more difficult to
train than CP models. The goal of this section is to isolate an issue faced with CPT/PCP models
when trained with vanilla adaptive gradient methods such as Adagrad or Adam.
4.1	CONTROL EXPERIMENT: UNITARY P1 AND P3 IN PCP
When D = d in PCP, the model becomes equivalent to CP. Indeed, the matrices P1 and P3 are
unitary (P1 P1> = P3 P3> = I)andso[[(UP1)P1>,V,(WP3)P3>]] = [[U,V,W]]. There is no practical
interest in considering this degenerate case of PCP, we only use it in the following toy experiment
to exhibit one of the difficulties encountered when training PCP.
We perform a simple control experiment in which we take one of the standard benchmarks of link
prediction in knowledge bases, called FB15K-237, and train a CP model for different values of
the rank D and a PCP model with D = d with vanilla Adagrad. The full experimental protocol,
including hyperparameter tuning, is similar to our main experiments and is described in Section 7.2.
Figure 1a plots the performances in terms of the standard metric mean reciprocal rank (higher is
better) as a function of D of CP (blue curve) and PCP (red curve, called PCP (Adagrad)).
We observe that CP obtains significantly better performances than CPT for larger embedding dimen-
sion D . Since in this toy experiment CP and PCP can represent exactly the same tensors and have
equivalent regularizations, the only difference between the algorithms that can explain the difference
in performances is in how the optimization is carried out, namely the diagonal rescaling performed
by Adagrad: Adagrad adapts the learning rate on a per-parameter basis, depending on previous and
5
Under review as a conference paper at ICLR 2020
current gradients, and is therefore not invariant by the addition of the matrices P1 and P2 even if
these are unitary (we provide the formal justification in the next section). This is shown experimen-
tally in Figure 1b where we plot the average Adagrad coefficients for each embedding dimensions
(i.e., adagrad coefficients of subject/object embedding matrices averaged by column). The addition
of the random P1 and P2 flattens the Adagrad weights, which in turn removes all the benefit of the
adaptive rescaling of the algorithm.
For reference, we also tried to directly learn all parameters including P1 and P3 (i.e., learn a CPT
model) with vanilla Adagrad. The performances obtained are also lower than those of CP, as shown
in Figure 1a (orange curve).
5	A rotation invariant AdaGrad: Adaimp
In this section, we study the optimization problem in more details, and more precisely the effect
of the diagonal rescaling performed by Adagrad. As a remainder, given a sequence of stochastic
gradients g(t) of L and denoting G(t) = I + Ptτ =1 g(τ)g(τ)>, the (practical) AdaGrad update is:
θPt+1) = θPt) - ηgPpt)/dGPp or equivalently θ(t+1) = θ(t) - ηDiag(G(t)) —112
where Diag(G) is the diagonal matrix obtained by extracting the diagonal elements of G.
5.1	Two equivalent parametrizations of PCP
The following decomposition is equivalent to PCP, but its embeddings are expressed in RD :
(PCPFULL) Xi,j,k = hP1P1>ui,vj,P3P3>wki, with ui,vj,wk ∈ RD, fixed P1, P2 ∈ RD×d.
Note that with Ui = P>Ui and Wk = PWk, We go from PCPfU∣∣ to PCP. The practical differences
between PCP and PCPfull are that PCPfull learn embeddings in the high-dimensional space, main-
taining the low-rank structure of the overall entity embeddings through the orthogonal projections
P1 P1T and P3 P3T . The practical interest of PCPfull is not in terms of modeling but rather from the
optimization perspective with Adagrad because it has a structure that is closer to that of CP.
Indeed, for d = D, P1 and P3 disappear in PCPfull, so that optimizing a PCPfull model with Adagrad
is equivalent to optimizing a CP model with Adagrad. This property suggests an alternative to the
vanilla PCP + Adagrad algorithm, which we call implicit Adagrad:
Implicit Adagrad: Adaimp The approach we propose is to effectively optimize PCPfull with Ada-
grad. However, when d < D, which is the interesting case for PCP, we notice that we do not need
to maintain embeddings in RD. Our approach, called Adaimp , computes the gradients and Adagrad
coefficients with respect to Ui, Wk ∈ RD, but the full dimension factor matrices U and W are never
explicitly stored in memory. Rather, we store Ui = P> Ui and Wk = Pr Wk ∈ Rd, which is all that
is required for any model computation in PCPfull since P1 and P3 are fixed. Overall, the effective
model parameters are exactly the same as in PCP, and we call this approach PCP +Adaimp .
An Adaimp update is described in Algorithm 4. While PCP +Adagrad and PCP +Adaimp work with
the same number of model parameters, the fundamental difference is the computation of Adagrad
coefficients. Since Adaimp effectively applies Adagrad to PCPfull, we need to maintain the Adagrad
coefficients in RD even when d < D: the overall update is first computed in RD and projected back
to Rd after the application of Adagrad rescaling. In constrat, in vanilla PCP +Adagrad, the gradient
is projected to Rd before Adagrad rescaling.
5.2	Implicit optimization of PCPFULL : Adaimp
In this section, we discuss more formally the Adaimp updates, and how they compare to PCP +Ada-
grad. In the following, U, WW are in RN×d, whereas U, V and W are in RN×D. Using d ≤ D,
P1, P3 ∈ RD×d, and we use the notation Π1 = P1P1r and Π3 = P3P3r.
6
Under review as a conference paper at ICLR 2020
projection to Rd
Z∙A∙{
×	P>g(tt
×	Π1> gi(t)
|{}
projection
to Im(Π)∈RD
The empirical risk L can be expressed as a function of three matrices M(1), M(2) and M(3) corre-
sponding to factors of a CP decomposition. We focus on the following problems :
(PCP) argminL(UP>,V^,WWP>) (PCPFULL) argminL(UΠ>,Vr,WΠ>)
U ,v,W	u,v,w
We focus on a step at time (t) on vectors Ui and Ui. We assume that at this time t, the tensor iterates
are the same, that is U = UP> (resp. WW = WP>) so that [UP>, V, WWP> J = [UΠ>, V, WΠ> J.
In this case, the gradient ^m⑴ L is the same in both optimization problems, We denote it by git).
i
Let Gi(t) = Id + Ptτ=1gi(τ)gi(τ) . The updates for (PCP) are:
Ut+1 = Ut - ηDiag(P>Ga)Pι)-1 /2P>git).	(3)
Note that due to the presence of P1 inside the Diag operator, the update (3) is not rotation invariant.
Moreover, for random P1, the matrix P1>Gi(t)P1 Will be far from diagonal With high probability,
making its diagonal meaningless. This is visualized in Figure 1b.
Similar updates can be derived for (PCPfull):
ut+1 = ut - ηDiag(∏>G(t)∏1)-112Π>g(t).	(4)
As a sanity check, clearly, for d = D and Π1 = I, the update (4) is equivalent to the Adagrad update
for the CP model. In the general case d ≤ D, in order to avoid storing U ∈ RN ×D, We apply these
updates implicitly with Adaimp, by storing Utt = P>u(t) in Rd. Let US compare the updates:
Adagrad in Rd
z--------A---------{
(Adagrad) Ut+1 = Ut - η	×	Diag(P>Gi)Pi)-111
(ADAimp) Ui+1 = Ut - η	×	P>	×	Diag(Π>G(t)∏1)-111
l{z}	|---------{---------}
projection to Rd	Adagrad in RD
Going back to our control experiment, we note on Figure 1a that PCP +Adaimp matches the perfor-
mances of CP+Adagrad for all D, indicating that we fixed this optimization issue.
5.3	Alternatives to Adaimp
Another solution would be to use Adagrad projected on the column space of Π, but we show in
Appendix 9.1 that even with the diagonal approximation, this is impractical. Note that the version
of Adagrad which uses the full matrix G(t) is rotation invariant (see Appendix 9.2 for details), but it
cannot be used at the scale of our problems.
It could be argued that the strength of the AdaGrad algorithm in our context mostly comes from
its adaptation to the different frequencies of updates of each embedding. In fact, this is one of the
examples chosen in Duchi et al. (2011) to display the advantages of AdaGrad compared to stochastic
gradient descent. A version of AdaGrad that would only keep one coefficient per embedding (we
call this version Adarow) would be invariant to unitary transforms by design and would adapt to
the various update frequencies of different embeddings. In fact, this version of AdaGrad is used
to save memory in Lerer et al. (2019). We test this algorithm in Appendix 9.8. The difference in
performances shows that this adaptation is not sufficient to recover the performances of the finer
diagonal AdaGrad in our setting. The claim that the approximations made in Adaimp are indeed
better is further backed by the experiments in the next section .
5.4	Complexity
The time complexity of our Ada imp update for a batch of size B is O (D ∙ d ∙ B) which is similar,
up to constants, to the complexity of updates for the AdaGrad algorithm. We do not notice any
runtime differences between our algorithm applied in dimensions (d, D) and a CP decomposition of
dimension D (see Section 7). The runtime for large enough D is dominated by the matrix product
(O(D1 ∙ B)) required to compute the cross-entropy in Equation (1).
7
Under review as a conference paper at ICLR 2020
Algorithm 1 Step of PComplEx training
(i, j, k) J sample from S
gi, gj,gk J gradients in (Pu，i, Puj,wk)
~ . ~
ui,Gi J Adamnp (η, ui, gu, Gi, P)
Uj, Gj J Adaimp (η, uj, gj, Gj, P)
wk, Gk J AdaGrad(η,wk, gk, Gk)
Algorithm 2 Adaimp
Input: η,x(t), g(t), GG(t- 1), P
g(t) J P(P>g(t))
G(t) j G(t-1) + g(t) θ g(t)
X(t +1) J x(t) - ηP>Diag(G(t))- 1 /2g(t)
return x (t+1) ,G (t)
Figure 2:	The two algorithms used in the training of PComplEx
6	Projected ComplEx
As the state-of-the-art variant of CP is ComplEx (Trouillon et al., 2016; Lacroix et al., 2018), we
propose the following alternative to PCP_base on ComplEx with Adaimp in practice. Given the
ComPlEx decomposition X = Re([U,V,U]), a low-rank decomposition of the entity factor U as
∙-v
PU leads to the model PComplEx we use in the experiments of Section 7:
(PCOMPLEX) Xijk = hPui,Vj,PUki
=hui, P>Diag(Vj)P, Ui, ui,wk ∈ Cd,vj ∈ CD, fixed P ∈ RD×d
PComplEx is similar to ComplEx but with interactions described by full matrices of rank D that
share a same basis. We learn this decomposition with Algorithms 1 and 4.
7	Experiments
In this section, we compare ComplEx optimized with AdaGrad and PComplEx optimized with
Adaimp . We optimize the regularized empirical risk of Equation (2). Following Lacroix et al.
(2018), we regularize ComplEx with the weighted nuclear-3 norm, which is equivalent to regulariz-
ing kuik33 + kuj k33 + kwk k33 for each training example (i, j, k). For PComplEx based models, we
regularize kPuik33 + kvj k33 + kPukk33 by analogy.
We conduct all experiments on a Quadro GP 100 GPU. The code for PComplEx and Adaimp is
available in the supplementary materials, experiments on ComplEx use the code1 from (Lacroix
et al., 2018). We include results from TuckER (Balazevic et al., 2019b), DRT and SRT which are
the two models considered in Wang et al. (2019), ConvE (Dettmers et al., 2018), HolEx (Xue et al.,
2018), LFM (Jenatton et al., 2012) and MurP (Balazevic et al., 2019a) without re-implementation
on our parts. All the parameters for the experiments in this section are reported in Appendix 9.11.
7.1	Datasets
WN18 (Bordes et al., 2013) is taken from the Wordnet database which contains words and relations
between them. WN18RR (Dettmers et al., 2018) is a filtering of this dataset which removes train/test
leakage. YAGO3-10 (Dettmers et al., 2018) is taken from the eponymous knowledge base. Finally,
SVO (Jenatton et al., 2012) contains observations of Subject, Verb, Object triples. All statistics for
these datasets can be found in Appendix 9.13. Experiments on the FB15K and FB15K-237 datasets
are deferred to Appendix 9.10.
7.2	Results
We report the filtered Mean Reciprocal Rank (Nickel et al., 2016b) on Figure 3. For SVO, we report
the only figure available in previous work which is the filtered hits at 5% (Jenatton et al., 2012).
These measures are detailed in Appendix 9.11. Only the grid-search parameters were given for
LFM, so we were not able to obtain a precise number of parameters for the number they report.
1available at https://github.com/facebookresearch/kbc
8
Under review as a conference paper at ICLR 2020
Figure 3: MRR as a function of #floats / entities (see Appendix 9.11) on four knowledge bases. We
plot the convex envelope of various operating points we tested, varying D for several values of d.
For some datasets (WN18, WN18RR), Adam (Kingma & Ba, 2014) is beneficial, in which case we
use the implicit adaptation of Adam detailed in Appendix 9.7.
WN18RR
On WN18, SVO and YAGO3-10 we observe sizable performance gains for low embedding sizes :
up to 0.14 MRR points on WN18, 0.05 MRR points on YAGO and 0.03 H@5% points on SVO.
The TUckER (Balazevic et al., 2019b) model performs similarly to PComplEx and ComPIEx except
on FB15K and WN18 where it underperforms (see Appendix 9.10). We expect this discrepancy to
come from a less extensive grid-search rather than any intrinsic differences in the models that are
both based on the TUcker decomposition. The consistency on all operating points of oUr method
with ComplEx shows an advantage of oUr method, which enjoys the same learning rate robUstness
as AdaGrad, and does not reqUire choosing a learning-rate decay, leading to easier experiments
with only the regularization strength to tune. The MurP model (Balazevic et al., 2019a) provides
good performances for low embedding sizes on WN18RR, bUt Underperforms on FB15K-237 (see
Appendix 9.10). All other models fail to match the performances of ComplEx and PComplEx with
equivalent number of parameters.
Variance of performances in PComplEx due to random choice of P is similar to the variance of
ComplEx. We present experiments on the WN18 dataset for 5 different seeds in Appendix 9.9.
8	Conclusion
By observing that the core tensor of the Tucker decomposition can itself be decomposed, we obtain
new models that are reminiscent of the canonical decomposition with low-rank factors. We provide
experimental evidence that a naive application of AdaGrad on this decomposition fails, due to in-
dividual coordinates losing their meaning. We propose a new algorithm, Adaimp , which fixes this
issue. Our model, when optimized with Adaimp , provides better performances than ComplEx in the
low-rank regime, and matches its performances in the other regimes.
9
Under review as a conference paper at ICLR 2020
References
Ivana Balazevic, Carl Allen, and Timothy HosPedales. Multi-relational Poincar∖'e graph embed-
dings. arXiv preprint arXiv:1905.09791, 2019a.
Ivana Balazevic, Carl Allen, and Timothy M Hospedales. Tucker: Tensor factorization for knowl-
edge graPh comPletion. arXiv preprint arXiv:1901.09590, 2019b.
Antoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. Learning structured embed-
dings of knowledge bases. In Conference on artificial intelligence, 2011.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating Embeddings for Modeling Multi-relational Data. In C. J. C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Pro-
cessing Systems 26, pp. 2787-2795. Curran Associates, Inc., 2013.
Rasmus Bro and Claus A Andersson. Improving the speed of multiway algorithms: Part ii: Com-
pression. Chemometrics and intelligent laboratory systems, 42(1-2):105-113, 1998.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and lo-
cally connected networks on graphs. In International Conference on Learning Representations
(ICLR2014), 2014.
J Douglas Carroll, Sandra Pruzansky, and Joseph B Kruskal. Candelinc: A general approach to mul-
tidimensional analysis of many-way arrays with linear constraints on parameters. Psychometrika,
45(1):3-24, 1980.
Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2D
knowledge graph embeddings. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Katsuhiko Hayashi and Masashi Shimbo. On the equivalence of holographic and complex embed-
dings for link prediction. arXiv preprint arXiv:1702.05563, 2017.
Frank L. Hitchcock. The expression of a tensor or a polyadic as a sum of products. Studies in
Applied Mathematics, 6(1-4):164-189, 1927.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Rodolphe Jenatton, Nicolas L Roux, Antoine Bordes, and Guillaume R Obozinski. A latent factor
model for highly multi-relational data. In Advances in Neural Information Processing Systems,
pp. 3167-3175, 2012.
Rudolf Kadlec, Ondrej Bajgar, and Jan Kleindienst. Knowledge base completion: Baselines strike
back. In Proceedings of the 2nd Workshop on Representation Learning for NLP, pp. 69-74, 2017.
Seyed Mehran Kazemi and David Poole. Simple embedding for link prediction in knowledge graphs.
In Advances in Neural Information Processing Systems 31, pp. 4289-4300. 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. SIAM review, 51(3):
455-500, 2009.
Timothee Lacroix, Nicolas Usunier, and Guillaume Obozinski. Canonical tensor decomposition for
knowledge base completion. In Proceedings of the 35th International Conference on Machine
Learning (ICML-18), pp. 2863-2872, 2018.
Adam Lerer, Ledell Wu, Jiajun Shen, Timothee Lacroix, Luca Wehrstedt, Abhijit Bose, and
Alex Peysakhovich. Pytorch-biggraph: A large-scale graph embedding system. arXiv preprint
arXiv:1903.12287, 2019.
10
Under review as a conference paper at ICLR 2020
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural
networks. In International Conference on Learning Representations (ICLR), 2016.
Dat Quoc Nguyen. An overview of embedding models of entities and relationships for knowledge
base completion. arXiv preprint arXiv:1703.08098, 2017.
Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learn-
ing on multi-relational data. In Proceedings of the 28th International Conference on Machine
Learning (ICML-11),pp. 809-816, 2011.
Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A Review of Relational
Machine Learning for Knowledge Graphs. Proceedings of the IEEE, 104(1):11-33, 2016a.
Maximilian Nickel, Lorenzo Rosasco, Tomaso A Poggio, et al. Holographic embeddings of knowl-
edge graphs. 2016b.
Maximillian Nickel and DoUWe Kiela. Poincare embeddings for learning hierarchical representa-
tions. In Advances in Neural Information Processing Systems, pp. 6338-6347, 2017.
Mathias Niepert, Mohamed Ahmed, and Konstantin KUtzkov. Learning convolUtional neUral net-
Works for graphs. In International Conference on Machine Learning, pp. 2014-2023, 2016.
Franco Scarselli, Marco Gori, Ah ChUng Tsoi, MarkUs HagenbUchner, and Gabriele Monfardini.
The graph neUral netWork model. IEEE Transactions on Neural Networks, 20(1):61-80, 2009.
Michael SchlichtkrUll, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max
Welling. Modeling relational data With graph convolUtional netWorks. In European Semantic Web
Conference, pp. 593-607. Springer, 2018.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya SUtskever, and RUslan SalakhUtdi-
nov. DropoUt: A simple Way to prevent neUral netWorks from overfitting. Journal of Ma-
chine Learning Research, 15:1929-1958, 2014. URL http://jmlr.org/papers/v15/
srivastava14a.html.
Kristina ToUtanova and Danqi Chen. Observed versUs latent featUres for knoWledge base and text
inference. In Proceedings of the 3rd Workshop on Continuous Vector Space Models and their
Compositionality, pp. 57-66, 2015.
Theo Trouillon, Johannes Welbl, Sebastian Riedel, Eric Gaussier, and Guillaume Bouchard. Com-
plex embeddings for simple link prediction. In International Conference on Machine Learning,
pp. 2071-2080, 2016.
Yanjie Wang, Samuel Broscheit, and Rainer Gemulla. A relational tucker decomposition for multi-
relational link prediction. arXiv preprint arXiv:1902.00898, 2019.
Yexiang Xue, Yang Yuan, Zhitian Xu, and Ashish SabharWal. Expanding holographic embeddings
for knoWledge completion. In Advances in Neural Information Processing Systems, pp. 4496-
4506, 2018.
Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and
relations for learning and inference in knoWledge bases. arXiv preprint arXiv:1412.6575, 2014.
11
Under review as a conference paper at ICLR 2020
9	Appendix
In subsections 9.1 and 9.2 where we discuss the Adagrad algorithm, we do so in a general setting,
where we study, for fixed P with orthonormal columns, the problem :
mθin f(Pθ).
9.1	Projected Adagrad update
Let D denote Diag(G)1/2 for the classical version of Adagrad and G1/2 itself for the “full” version
of the update. When the parameter θ is constrained to a set Θ, the update proposed in Eq.(1) in Duchi
et al. (2011) the one obtained by solving
main( G — Z) > D (θ — Z) with Z = θ(t) — ηD-1 g(t)
∙-v
To enforce the constraint that θ = Pθ, we can consider the Lagrangian
∙-v	∙-v	-I-	∙-v	-I- ∙-v
L(θ, θ; λ) = (θ — z)>D(θ — z) — λ>(θ — Pθ)
whose stationary points satisfy D(θG — Z) = λ and P>λ = 0. So this entails P>D(θG — θG(t)) =
ηP>g(t) and finally using θG = Pθ we obtain an update in θ as follows
θ(t+1) = θ(t) — η(P>DP>)-1Pg(t).
Clearly, PDP> is in general non-diagonal whether D is diagonal or not, and so this approach does
not provide a computationally efficient update.
If D = G1/2, then since P G1/2P > = (PGP> )1/2 the update is the same as the full Adagrad
update (5) that we derive in the following section and replacing (PGP> )1/2 by its diagonal approx-
imation recovers update (3).
9.2	The two Full Adagrad updates and the quality of approximations of the
Diag versions
If we consider the full versions of the Adagrad updates then (letting again Π = PP>) its application
to θ 7→ f(Pθ) and θG 7→ f (ΠθG) yield respectively
θG(t+1) = θG(t) — η P (P >G(t)P -1/2P >g(t) and	(5)
θ(t+1) = θ(t) — ηΠ (∏G(t)∏)-t∕2 Πg(t),	(6)
where Mt notes the pseudo-inverse of a matrix M .Asit turns out, the two updates are equivalent:
Indeed, first (ΠGΠ)1 /2 = P (P> GP)1 /2 P> because P>P = I implies that
P(P>GP)1/2P>P(P>GP)1/2P> = ΠGΠ,
and the p.s.d. squareroot is unique. Second, taking the pseudo-inverse of this identity, we have
(Π G Π)t/2 = (P (P >GP )1 / 2 P >)t = P (P >GP)- 1 / 2 P >.	(7)
because, if H = (P> GP)1/2 is an invertible matrix, then (PHP>)t = PH-1P> given that
PHP> PH-1P> = PP>. Finally multiplying both sides of Eq. (7) by Π shows that
Π(Π G(t )Π)t/2Π = P (P >G(t) P)- 1 / 2 P >.
This shows that although Adagrad is not invariant for any invertible P, it is invariant for any P such
that P>P = I. Eq.(5) seems in general simpler than (6), but note that if D = d, then Π is the
identity and (6) shows that both full updates are in that case actually equivalent to the full update of
Adagrad applied to plain CP.
Finally, the equivalence of the full updates discussed above strongly suggests that if our pro-
posed update performs better than the naive application of Adagrad to PCP, it is because
PDiag(ΠG(t)Π)- 1 /2P> is a better approximation of (G(t))- 1 /2 than Diag(PG(t)P>)- 1 /2 while
not being much more computationally expensive.
12
Under review as a conference paper at ICLR 2020
9.3	CP-TUCKER
We have X = [[C; U, V, W]] and C = [[P1, P2, P3]]. For all i,j, k, we have:
d
,k =	Cr1,r2,r3 Ui,r1 Vi,r2 Wk,r3
,r3 [P3]r3,s
hP1ui, P2vj,P3wki
9.4	HolEx and Latent factor model
9.4.1	HolEx
The HolEx model (Xue et al., 2018) writes Xi,j,k = PrR=1 h(cr ui) ?uj, wkr i, where ? denotes the
circular correlation2. Exploiting the equivalence between HolE and ComplEx (Hayashi & Shimbo,
2017), denoting by F the discrete Fourier transform (with values in C), we can write for embeddings
of size d:
Reie
d
R1
)：h(cr Θ ui) ? Ujl wki = dRe
=1
C Θ Ui),F(Uj),F(WTk)i
C) ? F(Ui), F(Uj), F(Wk) i
For all vectors, We write Ui = F(Ui) ∈ Cd. We can re-write the circular correlation F(Cr) ? F(Ui)
as CrUi where Cr ∈ Cd×d is the circulant matrix associated with Cr ∈ Rd. We have :
R
h(Cr	Ui) ?vj,wkri
r=1
1 Re
d
T- r .ʌr ∖
U i,Uj ,w k i
Finally, with C1 = [C1, . . . , CR] ∈ RRd×d the vertical stacking of all Cr, C2 = [Id, . . . , Id] and
W k = [ Wk ,∙∙∙,wR]:
R	1	_
Eh(Cr Θ Ui) ?Uj Wi = dRe (hC1Ui,C2Uj,Wk〉)
r=1
HolEx with embeddings of size d is close to the CPT with D = Rd, allowing for two different
complex matrices to act on left and right hand side embeddings.
9.4.2	Latent Factor model
The latent factor model defines the score for a triple (i, j, k) as:
D
Xi,j,k = h(si + z), Rj (ok + z )i, with Rj =	αrUrvr .
r=1
Removing the bias terms z and z0 and gathering Ur and vr into matrices P1 and P3 leads to the
model CPT. In the PCP model, we fix P1 and P3 instead of learning them. We do not use a sparsity
inducing penalty on α but rather a variational form of the nuclear norm on the whole tensor.
2[a?b]k=	id=-01aib(i+k)modd.
13
Under review as a conference paper at ICLR 2020
9.5	Tucker2 with CP-Tucker
Let for all 1 ≤ r ≤ d, M(r) be a matrix of zeros except its r - th column which is all one. Let P1 be
the vertical concatenation of all (M (r))r=1..d and P2 the vertical concatenation of d identity matrix
in Rd. Remember that for all k, wk is an element of Rd2. For all 0 ≤ r < d, let wkr be the restriction
of wk to its [rd, (r + 1)d] coordinates.
Then P1 and P2 are elements of Rd2 ×d and we have for all i,j, k:
d
hP1ui,P2vj,wki = X hM(r1)ui,Idvj,wkr1i
r1 =1
d
= X ui,r1 hvj, wkr1 i by definition of M(r1)
r1 =1
dd
=	ui,r1	vj,r2 wk,r1r2	by definition of wkr1
r1=1	r2=1
= ui> Mat(wk)vj
9.6	Complete PComplEx Algorithm
Let U ∈ CN ×d be the the entity embedding matrix and V ∈ C2L×D be the predicate embedding
matrix. Let P : RN×d 7→ RN×D such that P(Ui) = P Ui. Let GtU and GtV be the stochastic
gradients with respect to U and V at time t.
Algorithm 3 PComPlEx optimized with Ada imp
Input: learning rate η, (random) matrix P with orthogonal columns,
CU ,^v J 0
while U, V not converged do
GU JP(GU)
CU J CU + G u Θ G U
Cv J Cv + GV Θ GV ________
U J U — η ∙P > (G U / (√Cu+i))
V J V - η ∙ GV/(√CV7+:1)))
end while
return U
9.7 Adam - Implicit
For WN18RR, we used the ideas presented in this paper, but adapted to the Adam (Kingma &
Ba, 2014) optimization algorithm. Similar to Adaimp , first and second moment estimates are ac-
cumulated in RD and we project back to Rd only for the parameter update. For simplicity, we
present here the dense version of the algorithm applied to the entity embeddings U ∈ RN ×d . Let
P : RN×d 7→ RN×D such that P(Ui) = PUi. Let Gt be the stochastic gradient with respect to U
at time t.
9.8 ADArow
For the same control experiment as in Section 5, we observe the performances of Adarow which is
rotation invariant by design.
14
Under review as a conference paper at ICLR 2020
Algorithm 4 Adamimp applied to U
Input: η, β1 , β2 , P,
m o ,v o ,t — 0
while U not converged do
t V- t + 1
Gt J P (Gt)
mt J β 1 ` mt-1 + (1 — βι) ∙ Gt
Vt J β2 ∙ Vt-1 + (1 — β2)∙ Gt Θ Gt
mt J mt/(1 一 βι)
Vt J VtIQ - βt)	_____
u J U -η-P>(mt/(VV+^))
end while
return U
Figure 4: Adarow vs AdaGrad on FB15K-237.
9.9	Variance of PComplEx
We run 5 grid search and plot the 5 associated convex-hulls on the WN18 dataset optimized with
Adaimp. Note that despite the added randomness of the choice of P , there is not more variance in
PComplEx than in ComplEx.
9.10	FB15k datasets
We use two subsets of the Freebase knowledge base : FB15K (Bordes et al., 2013) and FB15K-
237 (Toutanova & Chen, 2015). FB15K-237 is a harder version of FB15K, where some triples have
been removed to avoid leakage between the train and test set. There is no difference of performances
between PComplEx and ComplEx on these datasets.
15
Under review as a conference paper at ICLR 2020
FB15K-237
9.11	Experimental details
ʌ ʌ ʌ
Metrics Let rank(Xi,j,:; k) be the rank of Xi,j,k in the sorted list of values of Xi,j,:. We report
the MRR for most datasets :
MRR(X)
-1 X ______________1_______.
⑸(i,j,k)∈S rank (Xi,j,：;k))
For SVO, the task is slightly different as the ranking happens on the predicate mode. The metric
reported in Jenatton et al. (2012) is Hits@5% defined as :
H@5%(X) = L X 1(rank(Xi,：,k; j) ≤ 227).
| | (i,j,k)∈S
The metrics we report are filtered by ignoring other true positives when computing the ranks, as
done in Bordes et al. (2013).
Number of parameters per entities We count the number of floats in the model, and divide by
the number of entities in the dataset. For different methods, the number of parameters are :
•	ComplEx: 2 ∙ d ∙ (N + 2L)
•	PComplEx: 2 ∙ d ∙ N + 2 ∙ D ∙ 2L + d ∙ D
•	TuckEr: N ∙ de + L ∙ dp + d∣ ∙ dp
•	MurP: N ∙ (d +1) + 2 ∙ L ∙ d
•	ConvE: taken from Dettmers et al. (2018)
•	D/SRT: taken from Wang et al. (2019)
•	HolEx: d ∙ (N + L)
Grid Search For SVO:
•	For Complex vary d in [5, 25, 50, 100, 500, 1000, 2000]. For PComplEx, we vary d in
[5, 25, 50, 100, 500].
•	The strength of the regularizer, ν varies in [5e - 4,1e - 3, 5e - 3,1e - 2, 5e - 2,1e - 1].
•	Finally, for PComplEx, we vary the dimension D in
[5, 25, 50, 100, 500, 1000, 2000, 4000, 8000].
For all other datasets, we run 500 epochs :
•	For FB15K and FB15K-237, we vary d in [5, 25, 50, 100, 500, 1000, 2000]. For YAGO3-10,
WN18 and WN18RR, we add ranks 8 and 16 to that list.
•	The strength of the regularizer, ν varies in [5e - 4,1e - 3, 5e - 3,1e - 2, 5e - 2,1e - 1].
•	Finally, for PComplEx, we vary the dimension D in [5, 25, 50, 100, 500, 1000, 2000].
16
Under review as a conference paper at ICLR 2020
For TuckEr, We use the hyperparameters described in Balazevic et al. (2019b) for each dataset. We
apply a multiplicative factor γ to the dropout rates and run a grid-search over this parameter.
•	For WN18RR, We vary de in [12, 15, 18, 20, 40, 100, 200]. For FB15K-237, We vary de in
[10, 20, 30, 40, 50, 80, 100, 200]. For WN18, de varies in [5, 8, 16, 25, 50, 100]. For FB15K
de varies in .
•	For WN18RR, WN18 We search γ over [0, 0.5, 1]. For FB15K-237 Where this lead to non
increasing performances as a function of number of parameters, We refined this grid to
[0, 0.1, 0.2, ..., 0.7] for de ≤ 80. For FB15K, We use a grid of [0, 0.2, 0.4, ...1].
For MurP, we use the hyperparameters described in Balazevic et al. (2019a) for each dataset and
vary din [10, 12, 15, 18, 20, 40, 100, 200] on WN18RR and in [10, 20, 40, 100, 200] for FB15K-237.
Other details Batch-size is fixed to 1000, the learning-rate is 1e - 1 both for Adagrad and for
Adaimp, we run 100 epochs to ensure convergence. The best model based on validation MRR is
selected and we report the corresponding test MRR.
9.12	Running times
We give here the running times of each method per epoch, on the WN18RR dataset for comparable
dimensionalities and run on a P100 GPU. We use the original implementation for MurP (BalaZeViC
et al., 2019a) and TuckEr (Balazevic et al., 2019b). For ComplEx, we use the implementation from
Lacroix et al. (2018).
•	ComplEx (d = 200) : 4s/epoch.
•	PComplEx (d = D = 200, Adaimp) : 5s/epoch.
•	MurP (d = 200): 38s/epoch.
•	TuckEr (de = 200, dr = 200): 24s/epoch
Note that these running times are implementation dependent. In the figure below, we give the
learning-curves of MurP, TuckEr, ComplEx and PComplEx for one operating point on the WN18RR
dataset. The convergence speed of these methods is given in the figure below at an operating point
on WN18RR where all methods are close in final performances.
17
Under review as a conference paper at ICLR 2020
9.13	Dataset statistics
Dataset	N	L	Train
FB15K	15k	1k	500k
FB15K-237	15k	237	272k
WN18	41k	18	141k
WN18	41k	11	141k
YAGO3-10	123k	37	1M
SVO	30k	4.5k	1M
Table 1: Dataset statistics.
18