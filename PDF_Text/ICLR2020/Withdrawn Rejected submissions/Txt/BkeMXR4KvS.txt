Under review as a conference paper at ICLR 2020
DASGRAD:
Double Adaptive Stochastic Gradient
Anonymous authors
Paper under double-blind review
Ab stract
Adaptive moment methods have been remarkably successful for optimization un-
der the presence of high dimensional or sparse gradients, in parallel to this, adap-
tive sampling probabilities for SGD have allowed optimizers to improve conver-
gence rates by prioritizing examples to learn efficiently. Numerous applications in
the past have implicitly combined adaptive moment methods with adaptive prob-
abilities yet the theoretical guarantees of such procedures have not been explored.
We formalize double adaptive stochastic gradient methods DASGrad as an op-
timization technique and analyze its convergence improvements in a stochastic
convex optimization setting, we provide empirical validation of our findings with
convex and non convex objectives. We observe that the benefits of the method in-
crease with the model complexity and variability of the gradients, and we explore
the resulting utility in extensions to transfer learning.
1	Introduction and Motivation
Stochastic gradient descent (SGD) is a widely used optimization method, and currently through
backpropagation this algorithm has propelled the success of many deep learning applications. The
Deep Learning community has particularly adopted variants of adaptive moment methods for SGD
that specialize in high-dimensional features and non convex objectives, examples include ADA-
GRAD, ADADELTA, RMSPROP, ADAM and AMSGRAD (Duchi et al. (2011); Zeiler (2012);
Tieleman & Hinton (2012); Kingma & Ba (2014); Reddi et al. (2018)). All these adaptive moment
methods relied on the efficient use of the information of the geometry of the problem to improve the
rate of convergence.
In parallel to the previous ideas, adaptive probabilities methods for SGD, traditionally focusing on
convex objectives, have shown advantages over its uniform sampling baselines, by allowing a more
efficient use of the gradient information (Zhu (2018); Shen et al. (2016); Bottou et al. (2016); Csiba
et al. (2015); Stich et al. (2017)).
Adaptive probabilities were introduced to the Deep Learning community by Hinton (2007) through
the discriminative fine tuning procedure. The method was further explored in a range of applications
like object detection, reinforcement learning and curriculum learning (Shrivastava et al. (2016);
Schaul et al. (2015); Bengio et al. (2009)). In these examples the combination of adaptive moments
and adaptive probabilities methods was implicit and its analysis as a pure optimization technique
was still an open question.
In this paper we analyze the asymptotic convergence properties of combining adaptive probabilities
and adaptive moments. To our knowledge such family has not yet been introduced as an optimization
procedure. We will refer to this family of optimization algorithms as Double Adaptive Stochastic
Gradient DASGRAD.
We show that the improvements of DASGrad depend on its flexibility to control the variance of
the adaptive moments methods. We prove the theoretical guarantees and improvements in a convex
setting and validate these observations empirically in convex and deep learning objectives. Finally
we demonstrate the generalization properties of the algorithm with a simple extension to importance
weight transfer learning.
1
Under review as a conference paper at ICLR 2020
2	Adaptive Gradient Methods
Notation. In order to facilitate the proofs and reading process we introduce some simplified notation
that will be common to the analyzed algorithms. Let a, b ∈ Rd and M ∈ S+d , then the multiplication
of vector a by the inverse of M will be M-1a = a/M. Let √a be the element-wise square root
of vector a, a2 the element-wise square, a/b the element-wise division, and max(a, b) the element-
wise max of vector a and vector b. Finally for any natural n the set {1, . . . , n} is denoted as [n].
Let T = {(xi , yi)}in=1 be a training set; let f : Θ × X × Y → R be a differentiable function that
represents the empirical risk of an agent over T for the parameters θ ∈ Θ, with Θ ⊆ Rd a convex
feasible set of parameters; let S+d the set of positive definite matrices in Rd×d, for a given matrix
M ∈ S+d and parameter θ0 ∈ Θ; let ΠΘ,M be the projection operator over the feasible domain
defined by ∏θ,m(θ0) = argmi□θ∈θ ||M1/2(θ - θ0)∣∣. 1
For the iterative stochastic optimization algorithm A, let it be a sampled index i at step t drawn from
the training set indices [n], with it 〜Pt and Pt ∈ △+ = {p ∈ Rn : Pi > 0 Σ%p% = 1}. We
denote the evaluated risk f(θ, Xi, yi) = fi(θ), the complete gradient Vf (θt) = ɪ∑itNfit (θt) and
the stochastic gradient Vfit (θt), analogous a full descent direction mt = n∑itm^ and a stochastic
descent direction mit .
Stochastic Optimization Framework. To analyze the convergence of the stochastic optimization
algorithm A we use the convex optimization setting where we assume that the objective function is
convex with bounded gradients, that is ∣∣Vfi(θ)∣∣∞ ≤ G for all i ∈ [n], θ ∈ Θ, and finally the
parameter space Θ has bounded diameter, that is ∣∣θ - θ0∣∣∞ ≤ D for all θ, θ0 ∈ Θ.
For our purposes, the algorithm A at time t chooses a distribution over the training set Pt ∈ △+,
obtains a training example it 〜 Pt and its importance weights Wit = Q/n)/Pit, then updates its
parameters θt ∈ Θ using the available data at time t and the importance weights Wit to unbias the
direction of the gradients. After the update, the algorithm incurs in a loss from an unknown function
f(θt). To assess the performance of the algorithm after T steps we use the expected regret, which
measures the difference of the loss at time t and the loss for optimal fixed parameter, along the
possible trajectories induced by the chosen probabilities.
R(A) = PtT=1 En [ fi(θt) -minθEn[fi(θ)] ]
The goal is to design an algorithm A that has sub linear expected regret R(A)T = O(T), which in
turn implies that the algorithm will converge on average to the optimal parameter.
Algorithm 1: Double Adaptive Methods
Input: θ1 ∈ Θ, step size {αt > 0}tT=1, functions {φt, ψt}tT=1
for t = 1 to T do
Choose Pt ∈ △+, and sample it 〜Pt
Calculate g鲤 = Vfit (θt) and Wit = (1∕n~)∕Pit
mit = φt (gi1 , . . . , git ) and Vit = ψt (gi1 , . . . , git )
Θt+1 = θt - αtWit Vi- 1/2mit
_ — ,
θt+ι = πθ,V1/2(仇+1)
Algorithm 1 constitutes the general family of double adaptive gradient methods. This algo-
rithm comprehends the classical stochastic gradient descent, adaptive moment methods family
Zeiler (2012); Tieleman & Hinton (2012); Kingma & Ba (2014); Reddi et al. (2018), and sec-
ond order methods Duchi et al. (2011), varying the averaging functions of the past gradients with
φt : Θt → Rd, and approximating the Hessian matrix with the functions ψt : Θt → S+d .
1The projection operator enables the algorithm to deal with constrained optimization over compact convex
domains that are equivalent to common regularization techniques like ridge and LASSO.
2
Under review as a conference paper at ICLR 2020
Adaptive Probabilities Methods. The classic stochastic gradient descent algorithm is recovered
with the following step size, sampling probabilities and functions:
at = a/y/t Pit = 1/n for all t ∈ [T], i ∈ [n]
φt (gi1, . . . ,git) = git ψt(gi1, . . . ,git) = I
(SGD)
Adaptive probabilities methods can be obtained simply by allowing the algorithm to choose a dif-
ferent probability Pt at any time t:
at = α∕√t Pt ∈ △+ for all t ∈ [T]
φt(gi1, . . . ,git) = git ψt(gi1, . . . ,git) = I
(AP-SGD)
Significant improvements in the convergence rate of the algorithm can be obtained by cleverly choos-
ing and computing such probabilities that in turn enables the algorithm to use data in a more efficient
manner Stich et al. (20l7). Fixed importance sampling is the case when Pt = P for all t ∈ [T].
Adaptive Moments Methods. Duchi et al. propelled interest and research on adaptive algorithms.
In their work they noticed that SGD lacked good convergence behavior in sparse settings, and pro-
posed a family of algorithms that allowed the methods to dynamically incorporate information about
the geometry of the data Duchi et al. (2011). Following huge gains obtained with ADAGrad, the
deep learning community proposed variants based on exponential moving average functions for ψt
like ADADELTA, RMSPROP, ADAM and most recently AMSGRAD (Zeiler (2012); Tieleman &
Hinton (2012); Kingma & Ba (2014); Reddi et al. (2018)).
The first algorithm ADAGrad is obtained by the following proximal functions:
at = 1∕√t pit = 1/n for all t ∈ [T],i ∈ [n]
φt (gi1, . . . ,git) = git
ψt(giι,..., git) = t diagNT =1 g2τ)
The ADAM/AMS Grad algorithm is obtained by setting:
at = 1∕√t Pit = 1/n ∀t ∈ [T], i ∈ [n]
φt(gi1, . . . ,git) = Σtτ =1 β1 (t)τ giτ
Vit = (1 - β2)H=1β2-τg2τ Vit = max(vit-ι, Vit)
ψt(giι,..., git) = diag (Vit)
(ADAGRAD)
(ADAM/AMSGRAD)
Fortunately a very simple and computationally efficient way to implement ADAM is given by a
recursion. RMSPROP is the particular case of ADAM when β1 = 0 and without maximum operator
for the second moments vector, while ADAM is recovered without the maximum operator.
Double Adaptive Methods. The key idea behind both the adaptive probabilities methods and adap-
tive moment methods is the efficient use of the information available in the training data to improve
the convergence of the algorithms. In the case of adaptive moment methods the diagonal approx-
imations of the Hessian matrix use the information about the geometry of the problem while the
adaptive sampling methods, the probabilities Pit, prioritize the examples with the highest impact on
the learning progress. As these improvements rely on complementary sources of information we can
combine them into a general framework described by the double adaptive methods in Algorithm 1.
To analyze the theoretical improvement guarantees of the double adaptive methods we first we ex-
tend the adaptive moments convergence guarantees to the stochastic case with uniform sampling and
then we compare them to the convergence guarantees using optimal probabilities.
3
Under review as a conference paper at ICLR 2020
3 Convergence Analysis
3.1 Convergence of Adaptive Moments Methods
We first provide a regret bound of ADAM for weakly convex objectives with uniform probabilities
adapting the arguments in Reddi et al. (2018) and Kingma & Ba (2014). Then we extend these
results to the adaptive probabilities case.
Theorem 1.	Let {θt}tT=1 be the sequence obtained with ADAM, then the regret bound is:
T1
R(ADAM) ≤ X 电(I-e)En [llV1 /4(& - θ*)ll2 -IIVI/4(仇十1 - θ*)II2]
T
+X
t=1
αtβ1t
2(1 - β1t)
T
+X
t=1
T
花 τ∕4mt-ιll2 + X：
t=1
αt
2(1 - β1t )
β1t
En [∣H-1∕4mit∣∣2]
2αt(1 - β1t )
∣H“4(θt- θ*)∣∣2
Corollary 1.1. Following the Sequence {θt}T=ι of ADAM with step size at = α∕√t, averaging
parameters βι = β匕, β人 ≤ βι for all t ∈ [T], Y = β1∕√β2 < 1 and uniform probabilities
Pit = 1/n. Ifwe assume that Θ has bounded diameter D, "▽/〃(θ)ll∞ ≤ G for all t ∈ [T] and
θ ∈ Θ, then the expected regret bound is:
R(ADAM) ≤
D2 √T
2α(1 - β1)
EnMirIH
α √1+log(T)
2(I- βI)2 VZ(I-尸2)(1 - Y)
d
X Ill g " Il
h=1
αGd
D2
*2α(1 - βι)3P(1 - β2)(l - γ) + 2α(I - βι)
T
X √βT-tllv1∕4ll2
t=1
+
3.2 Convergence of Double Adaptive Methods
Theorem 2.	Let {θt}tT=1 be a DA S GRAD sequence, for a trajectory pt ∈ ∆n+ the regret bound is:
T1
R(DASGrad) ≤ X	EpIjIIV；/4(d-θ*)ll2 TH1/4®+i - θ*)ll[
t=1 2αt(1 - β1t )	t	t
T
+X
t=1
αt
T
+X
t=1
αtβ1t
2(1 - β1t)
T
EpιJw2tllV-1∕4mitll[
2(1 - β1t )
花 τ4mit-1ll2 + X
t=1
β1t
2αt (1 - β1t )
I∣V14 (θt- θ*)ll2
Corollary 2.1. Following the Sequence {θt}T=ι of DASGrad, step size a = a/ʌ/t,averaging
parameters βι = βh, β匕 ≤ βι for all t ∈ [T], Y = β1∕√β2 < 1 and the optimal adaptive proba-
bilities Pit H ll V-"4mitll∙ Ifwe assume that Θ has bounded diameter D and "▽% (θ)ll∞ ≤ G
for all t ∈ [T] and θ ∈ Θ, then the expected regret bound is:
R(DASGRAD) ≤
D2 √T
2α(1 - β1)
Epi：TMT4 Il2]
α √1 + log(T)
2(I- βI)2 VZ(I-尸2) (I - Y)
X ||| g |1:T,h || - X Varn (llVrmij)
αGd
h=1
D2
*2α(1 - βι)3P(1 - β2)(l - γ) + 2α(I - βι)
t=1
T
X √tβT-tllv1/4ll2
t=1
+
d
T
4
Under review as a conference paper at ICLR 2020
3.3 Convergence Comparison
Adaptive moment methods can improve classical gradient descent by integrating the geometry of
the problem with a diagonal approximation of the Hessian an may achieve an exponentially smaller
bound for the expected regret with respect to the dimensions of the input data d, when dealing
with sparse features or small gradients in general. As shown by Duchi et al. (2011) for the adaptive
moment methods in the sparse setting, the potential component and error component of the expected
regret of Corollary 1.1 each will satisfy:
「d	]	d
Epi：T [∣∣V1T4II2] = EpLT X V1T2h ≪√d	and X Il Ig kτ,h Il ≪ √dT
h=1	h=1
which in turn translates to a much better expected regret bound than O(√dT) for classic SGD on
weakly convex objectives and sparse inputs.
In parallel to the previous improvements, the adaptive probabilities methods can further speed the
convergence by allowing the algorithm to evaluate the relative importance of each data point to
maximize the expected learning progress, and minimize the variance of the stochastic gradient at
each step. Given the optimal adaptive probabilities the error component of the expected regret in
Corollary 2.1 satisfies:
dT	d
X Ill g Ii：T,h Il- X Varn (M∕4mitll)w X Ill g Lτ,h Il
h=1	t=1	h=1
This shows how the optimal adaptive sampling probabilities on a convex setting can further improve
the convergence rate by allowing to flexibly control the variance of the gradients.
4	DAS GRAD Implementation
To obtain the optimal probabilities it is necessary to compute the norm of the gradient for each
training sample at each step. Given the deep learning libraries today, this calculation renders op-
timal adaptive probabilities methods impractical for real applications 2. Still, for completeness of
the exposition of the theoretical results we provide empirical evidence of the convergence improve-
ments, to do it we use an approximation of the optimal algorithm from the double adaptive methods,
following Algorithm 2.
Algorithm 2: DASGrad approximation
Input: θ1 ∈ Θ, functions {φt, ψt}tT=1, frequency J
for t = 1 to T do
if t mod J = 0 then
I Compute p^t ∈ △+ setting Pit X | M-1/4mit|| + e
Sample it 〜Pt using the segment tree
Calculate g鲤 = Vfit (θt) and Wit = (1∕n)∕Pit
mt = β1tmt-1 + (1 - β1t)gt and vt = β2vt-1 + (1 - β2)gt2
Vt = max(Vt-ι, Vt) and Vt = diag(vt)
θt+ι = θt - αtWit Vi- 1/2mit
_ — ,
θt+ι = πθ,V1/2(仇+1)
2Current dynamical computational graph libraries compute the average gradient batches by de-
fault and available workarounds are still slow. See https://github.com/pytorch/pytorch/issues/7786 and
https://github.com/tensorflow/tensorflow/issues/4897.
5
Under review as a conference paper at ICLR 2020
5	Empirical Results
In this section we provide empirical evidence of the convergence rates on classification problems
using logistic regression and deep neural networks, using ADAM, AMSGrad, and DASGrad.
Logistic Regression: For the convex setting we solve two classification problems with L2 regular-
ization. For the non sparse feature experiment we use the MNIST digit dataset, which is composed
of 60, 000 images of 28 × 28 hand written digits. For the sparse feature experiment we use the IMDB
movie rating dataset which is composed of 25, 000 highly polar movie reviews and the sentiment
label for the review Maas et al. (2011).3
Neural Networks: For the non convex setting we perform one experiment, we use the CIFAR10
dataset, which is composed of 60, 000 colour images of 32 × 32 pixels labeled in 10 classes. For
this multiclass classification problem we use a convolutional neural network following the small-
CIFARNET architecture, consisting of two convolution filters combined with max pooling and local
response normalization, followed by two fully connected layers of rectified linear units Krizhevsky
et al. (2012). 4
1500
DASGrad
AMSGrad
O 500 IOOO
IterationS
Aue」nuu< u«」l ueəz
əuu ①」ətoAue-Jmuv we,ll-
----DASGrad
AMSGrad
DASGrad
AMSGrad
>usɔwu< C-SH ueəz
①uualətoXM23UW< We-Jl
5000 7500 IOOOO 12500 15000
Iterations
Aue-Jnuuq u.e」.L Ue ①Σ
əuuə,ləito
Figure 1: Trajectories in convex and deep learning settings. First row 100 logistic regressions on
MNIST, second row 100 logistic regressions on IMDB, third row 10 convolutional neural networks
on CIFAR10. We show the mean over the trajectories for training loss (left), training accuracy
(center), and the accuracy improvement of DAS Grad with respect to AMSGrad and Adam with
95% confidence intervals (right).
3For both experiments, We use a batch of size 32, with a probability update every 10 steps, and the step size
at = ɑ/ʌ/t. We set βι = 0.9, β2 = 0.99, and choose α through a grid search. For the MNIST dataset, for all
three optimizers, the optimal learning rates are α = 0.01. For the IMDB dataset, we find the optimal learning
rates to be α = 0.005 for ADAM, α = 0.006 for AMSGRAD, andα = 0.02 for DASGRAD.
4For the experiment we use a batch size of 32, with a probability update every 300 steps, and step size of
at = ɑ/ʌ/t. We set βι = 0.9, β2 = 0.99, and choose α through a grid search, for which the optimal learning
rate for all optimizers is α = 0.001.
6
Under review as a conference paper at ICLR 2020
From the comparison in Figure 1, we observe that in all cases the DASGrad optimization algorithm
outperforms its adaptive moment counterparts represented by Adam and AMSGrad, as predicted
by the theoretical analysis. The improvement is more significant for the IMDB dataset than it is
for the MNIST dataset. From Figure 1 we can see that DASGrad continues to outperform Adam
and AMSGrad in the deep learning setting. These results reinforce the previous statement that the
benefits from DASGrad increase with the complexity of the data and the models.
6	Discussion
6.1	Convergence Improvements and Variance of Gradients
To further explore the relationship between variance and the improvements to the convergence rate
of the DASGrad algorithm, we implemented an online centroid learning experiment. Because of
the linear relationship between the features and the gradients, we are able to explicitly control their
variance. For this experiment, the empirical risk and gradients will be given by
Rn(θ) = 21n Pn=I ∣∣θ - Xill2 and Vf (θ, Xi) = θ - x〃
As we can see from Figure 2 the greater the variance of the gradients, the greater the benefit that one
can obtain from an adaptive probabilities method such as DASGrad in convex objectives, since
those probabilities will prioritize the data points with the most learning potential.
Figure 2: Trajectories of 100 random seeds, for the online centroid learning problem with different
variance for the features. Enhanced improvements of adaptive methods with higher variance of the
gradients.
6.2	Flexible Control of Variance
Recent insights on the generalization properties of minibatch SGD for non convex objectives suggest
that higher variance gradients tend to converge to flatter regions of the loss surfaces (Keskar et al.
(2016)). Applications like curriculum learning that shape the learning procedure, by gradually mak-
ing the task more difficult through importance sampling, may allow to maintain a higher variance
of the gradients for longer steps, this combined with the previous intuitions offers an explanation of
the mathematical basis of its success.
While curriculum learning is contrary to the optimal probabilities of the double adaptive methods
for convex settings, the underlying principle of flexible control of the variance of the gradients
operates as the mechanism behind both procedures. This observations strengthens the argument
that improving our understanding of the implicit optimization techniques in this algorithms can also
improve our understanding that so far has relied mostly on intuitive explanations of their success.
7
Under review as a conference paper at ICLR 2020
6.3	Importance Weight Transfer Learning
When the training T and test T0 set do not share the same distribution, we may face a distribution
mismatch problem. The DAS Grad algorithm is compatible with the cost re-weighting correction
technique Elkan (2001); Bowyer et al. (2011) as we can set the importance weights wt for any
trajectory of distributions pt , to unbias the gradients for the test distribution instead of the training.
T
R(DASGRAD)T 0 = XEpT0 fi(θt) -minθEpT0[fi(θ)]
t=1
T
= X Ep1:t witfi(θt) - minθEpT 0 [fi(θ)]
t=1
To test the generalization properties of the DAS Grad algorithm empirically, we unbalanced the
MNIST training data set by reducing ninety percent the observations from the 1 and 3 digit. We set
the importance weights to wit = (|Li |/m)/pit, where |Li| is the count of the label L associated
with index i in test over m, the number of test samples. As we see in Figure 3 using DAS GRAD
with the correct importance weights has the desired generalization properties when facing a domain
shift.
.0.9.8.7.6
2.LLLL
Sso-J-Sφl cs∑
——DASGrad
.--AMSGrad
∙- Adam
Iterations
Iterations
Figure 3: Trajectories of 20 random seeds for 2,000 iterations in convex optimization settings. Mul-
ticlass logistic regression on unbalanced MNIST dataset. We show the mean over 20 trajectories of
training loss (left), training accuracy (center), and the improvement in accuracy of DASGrad with
respect to AMSGrad and Adam with a 95% confidence interval (right)
7 Conclusion
Capability of learning from data efficiently is a prerequisite for practical success of complex learning
models across various problem settings and application contexts. We have shown how double adap-
tive stochastic gradient descent methods enable efficient learning in a generalizable manner, while
ensuring convergence improvement. We observed that DAS Grad algorithm outperforms currently
prevalent variants of adaptive moment algorithms such as Adam and AMS Grad overall, in the
context of the number of iterations required to achieve comparable performance, under the theoret-
ical convergence guarantees in a stochastic convex optimization setting. With empirical validation
in convex and non convex settings, we have shown that the advantages of DASGrad become more
prominent with the increasing complexity of data and models, and with more variance in the gradi-
ents. We have also broadened our results to demonstrate generalization properties of our approach
and its extensions to transfer learning, as well as intuitive connections to other learning scenarios.
8
Under review as a conference paper at ICLR 2020
References
Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pp.
41-48, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-516-1. doi: 10.1145/1553374.
1553380. URL http://doi.acm.org/10.1145/1553374.1553380.
Leon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning, 2016. URL http://arxiv.org/abs/1606.04838. cite arxiv:1606.04838.
Kevin W. Bowyer, Nitesh V. Chawla, Lawrence O. Hall, and W. Philip Kegelmeyer. SMOTE:
synthetic minority over-sampling technique. CoRR, abs/1106.1813, 2011.
Dominik Csiba, Zheng Qu, and Peter Richtrik. Stochastic dual coordinate ascent with adaptive prob-
abilities. In Proceedings of The 32nd International Conference on Machine Learning, ICML’15,
2015.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. J. Mach. Learn. Res., 12:2121-2159, July 2011. ISSN 1532-4435. URL
http://dl.acm.org/citation.cfm?id=1953048.2021068.
Charles Elkan. The foundations of cost-sensitive learning. In Proceedings of the 17th International
Joint Conference on Artificial Intelligence - Volume 2, IJCAI’01, pp. 973-978, San Francisco,
CA, USA, 2001. Morgan Kaufmann Publishers Inc. ISBN 1-55860-812-5, 978-1-558-60812-2.
URL http://dl.acm.org/citation.cfm?id=1642194.1642224.
Geoffrey E. Hinton. To recognize shapes, first learn to generate images. Progress in brain research,
165:535-47, 2007.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. CoRR,
abs/1609.04836, 2016.
Diederik P. Kingma and Jimmy Ba. ADAM: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convo-
lutional neural networks. In Proceedings of the 25th International Conference on Neural Infor-
mation Processing Systems - Volume 1, NIPS’12, pp. 1097-1105, USA, 2012. Curran Associates
Inc. URL http://dl.acm.org/citation.cfm?id=2999134.2999257.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies - Volume 1,
HLT ’11, pp. 142-150, Stroudsburg, PA, USA, 2011. Association for Computational Linguistics.
ISBN 978-1-932432-87-9. URL http://dl.acm.org/citation.cfm?id=2002472.
2002491.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of ADAM and beyond. In
International Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=ryQu7f-RZ.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay.
CoRR, abs/1511.05952, 2015. URL http://arxiv.org/abs/1511.05952.
Zebang Shen, Hui Qian, Tengfei Zhou, and Tongzhou Mu. Adaptive variance reducing for stochas-
tic gradient descent. In Proceedings of the 32nd International Joint Conference on Artificial
Intelligence, IJCAI’16, 2016.
Abhinav Shrivastava, Abhinav Gupta, and Ross B. Girshick. Training region-based object detectors
with online hard example mining. CoRR, abs/1604.03540, 2016. URL http://arxiv.org/
abs/1604.03540.
9
Under review as a conference paper at ICLR 2020
Sebastian U. Stich, Anant Raj, and Martin Jaggi. Safe adaptive importance sampling. CoRR,
abs/1711.02637, 2017. URL http://arxiv.org/abs/1711.02637.
T. Tieleman and G. Hinton. Lecture 6.5—RMSProp: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
Matthew D. Zeiler. ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701, 2012.
URL http://arxiv.org/abs/1212.5701.
R. Zhu. Gradient-based Sampling: An Adaptive Importance Sampling for Least-squares. ArXiv
e-prints, March 2018.
10
Under review as a conference paper at ICLR 2020
A	Appendix
A.1 Proof of Theorem 2
The proof of Theorem 2 assumes a convex differentiable objective function f , bounded diameter for
the parameters, and bounded norm of the gradients for any trajectory of probabilities pt ∈ ∆n+.
Proof. We build an upper bound of the expected regret using the convexity of the loss:
f (θt) - f (θ*) ≤ h gt, θt- θ* i = En Kgit, θt- θ*i]
While using DASGrad the update of the parameter will be given by the stochastic update dependent
on the training example it and the current parameter θt :
θt+1 = πθ,-^ 1/2 (θt+1) = πθ,-^ 1/2 (θt- atwit V-1"miJ = argmin || Vi1"(θt - αtwit Vi-1"miJ 11
To bound the expected regret of the algorithm, we use the fact that:
Θt+1 - θ* = (θt- θ*) - αtWitmit/，Vit
花 1/4 (θt+ι - θ*)∣∣2 =花 l(θt - θ*)∣∣2 - 2αtwit h mit, θt - θ*> + °2*2,花-1/4馆肩|2
=花 1"(θt - θ*)l∣2 - 2αtwit h βit mit-1 +(1- βι)g% θt - θ*i + α2w2∕H-1"mit ||2
We identify the first three components as the potential, the immediate cost, now with extra terms
associated to the moving average, and the error.
Lemma 1. For any M ∈ S+d and convex feasible set Θ ⊆ Rd with the projection operator ΠΘ,M
let u1 = ΠΘ,M (z1) and u2 = ΠΘ,M (z2) then:
||M1/2(u1-u2)|| ≤ ||M1/2(z1-z2)||
Taking the expectation at time t, and using the extended norm reduction property of the projections
from Lemma 1 we obtain the following inequality:
Epth 花 1"(θt+ι- θ)∣∣2∣θt i ≤ Epth 花 I/4©- θ*)∣∣2∣θt i
-Epth 2αtwit h β1t mit-ι + (1 - βIt)git, θt - θ*i I θt i
+ α2Ept hw2t∣H1 /4mitl∣2∣θt]
Since wt is such that the interior product will be unbiased, then:
Ept h∣∣vy4(θt+1- θ)∣∣2∣θt i ≤ Epth 花 Ie- θ*)∣∣2∣θt i
-2αth β'tmt-1 + (1 - β1t)gt, θt - θ*i
+ α2Ept h w2t∣∣Vt/4mit ||2 ∣ θt ]
Finally rearranging the terms, summing until time T and taking expectations:
T1
R(DASGRAD)T ≤ X 2也(I-e ) EpIJIHy4(θt- θ*)l∣2- llVy4(θt+ι- θ* )112 ]
T
+ X2τ⅜1 Epi：t hw2t∣H-1∕4mit∣∣2i (1)
t=1 2(1 - β1t)
TT
+ X Jαt"1"ι∣∣2 + X ^oɪM4(θt - θ*)∣∣2
Last line is Cauchy-Schwarz and Young’s inequality applied to the inner product of the extra terms
associated with the moving average in the immediate cost.
□
11
Under review as a conference paper at ICLR 2020
A.2 Proof of Corollary 2.1
Proof. The proof of Corollary 2 is in the line of the improvements provided by Reddi et al. to the
convergence proof of Kingma & Ba for ADAM, we adapt the arguments to the stochastic case. We
assess separately each component of the expected regret from Equation 1.
Lemma 2 addresses the potential, Lemma 3 the error, and Lemma 6 and Lemma 4 the moving
average terms. The proof of Corollary 2.1 is a consequence of all the previous Lemmas using the
optimal probabilities while Corollary 1.1 is the case with uniform probabilities.
Following the sequence {θt}T=ι of DASGrad, with step size α= = ɑ∕√7, averaging parameters
βι = βiι and β人 ≤ βι for all t ∈ [T] and Y = βι∕√β2 < 1. and bounded diameter D for Θ and
∣∣Vfit (θ)∣∣∞ ≤ G for all t ∈ [T] and θ ∈ Θ.
Lemma 2. From Equation 1 the potential component will satisfy:
XX 2O(1⅛) EpLthM/4(θt-⑹"2 - lιv1 /4(θt+ι- θ* )|12i ≤ 2ODΓ√β) Epi：TMT4112i
Proof
One can decompose the potential in the following manner:
T
X 2a(1-βJEpLthIlVy4(θt - θ*)∣∣2 - IIVt∕4(θt+ι - θ*)∣∣2 i ≤
201(⅛)EpIhM%1- θ*)H2i - 2M⅛)Epi：T [闻/4®+1- θ*)H2i
T
+2(T→1) X (%Epi：，h||VI/4® -θ*)H2i - - Epi- hM-4(θt-ι -θ*)H2.
≤ 20√⅛)EpIh 同/4 θ D Q 1112i
T
+2(T→1) X QEpI：，h 同4 Q D Q 1| 12 i- α-ι Epi：t-i h||v 廿 Q D Q 1|12 i)
≤ 2(⅛) (⅛Epi [||Vii||2]+ X & - α⅛) Ept 川vi"2)
=2M⅛)Epi：T hllvi1T4ll2i = 2OD2√⅛Epi：TMT4112i
The first inequality comes from rearranging and the definition of β1t, the second inequality comes
from the bounded diameter assumption applied to each entry of θt - θ* and using the Hadamard's
product to represent the original matrix multiplication, the third inequality5 comes from the defini-
tion of vit = max (Vi,-ι, vit), the last equality comes from the property of the telescopic sequence.
This completes the proof of Lemma 2.
5The third inequality is of particular importance since Reddi et al. showed that it is one of the main flaws in
the convergence analysis of ADAM and RM S Prop, and provided a simple fix to the adaptive moment methods
that guarantees the non increasing property needed to achieve the telescopic sequence upper bound.
12
Under review as a conference paper at ICLR 2020
Lemma 3. From Equation 1 the error component, once evaluated in the optimal probabilities Pt
will satisfy:
EpthW2t∣H1"mit∣∣2∣θti = Enh ∣∣V1 /4mit∣∣2∣θti - Wrn (∣H1"mit∣∣)
Proof
Creating a lower bound with Cauchy-Schwarz and showing that it is achievable with the optimal
probabilities Pit H ||立1/4mi」|.
Lemma 4. The first component of the extra terms associated with the moving average in Equation
1 will satisfy:
T
X
t=1
αtβit
2αt(I-尸It)
花—"mill2
αGd
≤-----------,	-----
2α(I- βI)3P(I-尸2)(I - Y)
Proof
Following very similar arguments as those from Lemma 6, we can get:
T
X
t=1
αtβit
2αt(I- βIt)
花 τ∕4mt-i∣∣2
T
2(1 - β1)2P(1β2)(1 - γ)
t=1
≤
≤
T
---------‰=--------X βT-t∣∣G Θ 11| 1
2(1 - βι)2p1β)(1-γ) = 1
αGd
≤-----------,	-----
2α(1 - β1)3P(I-尸2)(1 - Y)
This completes the proof of Lemma 4.
Lemma 5. To finish the second component of the extra terms associated with the moving average
in Equation 1, will satisfy:
T
X
t=1
βit
2αt(1 - βIt)
∣M1∕4(θt- θ*)∣∣2
D2
2α(1 - β1)
T
X √βT-tl∣v11Tl2
t=1
≤
Proof
TT
X 2d⅛ 花1/4(好 θ*)ll2 ≤ X 20d⅛ " θ D θ 1112
=D X √t o⅛)同 Ti2
D2 工― 一，
≤ 20(⅛)X √βL∣v1T∣2
This completes the proof of Lemma 5.
13
Under review as a conference paper at ICLR 2020
Lemma 6. From Equation 1 the error component, once evaluated in the optimal probabilities Pt,
and the total error will satisfy that:
T
X2(1¾)Epjw⅛M∕4mit∣∣2∣θti ≤
J p+‰ X ∣∣∣g"-X Varn (ME)
Proof
For Lemma 6 we follow Kingma & Ba, for every element at time t of the error component:
αt
2(1-β1t)
EnbH-1/4mi 用
αt
≤ 2(1-βit)
αt
En ||Vi-t 1/4mit||2
"ςT=IeI(t)τ giJ2-
2(1-β1t)
αt
≤ 2(1-βι)
αt
2(1-β1t)
√viT
αt
2(1-β1t)
αt
^II∑T=ιβι(t)T∕2βι(t)T∕2giJ2
≤ 2(1-βι)
αt
β1t-τ
l∣giτll2
02)ςT=ιβ2-τ g2τ
E	X上恒J!
[代 LgiTI
2 En I IΣ βt-
2(I - βI)2y∕(1-β2)
2(1 - βl)2 √(1 - β2)
En]√t (XYFgiJ1)[
≤
≤
α
The first inequality follows the definition of the auxiliary vectors Vt = max (Vt-ι,vt), the second
inequality comes from the non negativity ofβ1(t)τ. The third and fourth inequality comes from the
decreasing property β1 ≤ β11 and β12 ≤ β11 and the property of the geometric sequence. The fifth
inequality comes from the non negativity of β2 and ∣∣giτ∣∣2, the last equality uses the definition of
the step size. Using induction one can show that:
X J EnhMFl≤X - En( (X H"1"]
(2)
14
Under review as a conference paper at ICLR 2020
Continuing the proof of Lemma 6, let k
α
from Equation 2 we have that:
T
X
t=1
αt
2(1-β1t)
En [∣∣Vi1 /4mit∣∣2] ≤ Xk En
t=1
Tn1
=k XX 1∣∣git∣∣ι
t=1 it=1
Tn1
≤k XX加t∣∣ι
Y t-τ∣∣giτ∣∣j
t=1 it=1
k (X X n∣∣αt∣∣ι √t (X L))
d
k
(1 - γ)工
γ h=1
/ k
≤ (1-Y)
d
T
|git||1
n ιgit,h∣
|git,h |
≤ k，1 + log(T)
_ _(1 - Y)-
t=1
t
h1
2
n ∣git,h∣
≤ V) X N 3
The first equality comes from a change of indexes, the second inequality is an upper bound for
the arithmetic sequence that begins at t, the third inequality is an upper bound for the geometric
sequence, the fourth inequality comes is an application of Cauchy-Schwarz inequality, finally the
fifth inequality is an upper bound for the arithmetic sequence.
T
X Epth W2t∣∣Vt/4mit
t1
TT
∣∣2∣θt i = X En h∣∣V1 /4mit ∣∣2∣θt i - X Varn (∣∣V1 "mit∣∣)
t=1 t=1
≤
α /1 + log(T)
2(1 - βI)2P(I-尸2)(1 - Y)
dT
X ∣∣∣g ∣rτ,h ∣∣- XVarn (∣∣V—mij)
h1 t=1
With the optimal probabilities Pit, we complete the ProofofLemma 6.
15
Under review as a conference paper at ICLR 2020
Finally we can combine the results from Lemma 2 to 5 and obtain the following bound for the
expected regret of the general double adaptive algorithms:
Corollary 2.1
R(DASGRAD) ≤	J	Epi：T [||v1T41|2]
2α(1 - β1 )
+______α ,1 + log(T)
2(I- βI)2P(I-尸2)(I - Y)
dT
X Ill g li：T,h Il-X Varn (M ∕4m∙dl)
h=1	t=1
αGd
+ 2α(1 - βι)3P(1 - β2)(1 - Y
+
D2
2α(1 - βI)
T
X √tβT-t 同/4II2
t=1
□
16