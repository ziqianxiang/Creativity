Under review as a conference paper at ICLR 2020
Power up! Robust Graph Convolutional Net-
work based on Graph Powering
Anonymous authors
Paper under double-blind review
Ab stract
Graph convolutional networks (GCNs) are powerful tools for graph-structured
data. However, they have been recently shown to be vulnerable to topological
attacks. To enhance adversarial robustness, we go beyond spectral graph theory
to robust graph theory. By challenging the classical graph Laplacian, we propose
a new convolution operator that is provably robust in the spectral domain and is
incorporated in the GCN architecture to improve expressivity and interpretability.
By extending the original graph to a sequence of graphs, we also propose a robust
training paradigm that encourages transferability across graphs that span a range
of spatial and spectral characteristics. The proposed approaches are demonstrated
in extensive experiments to simultaneously improve performance in both benign
and adversarial situations.
1	Introduction
Graph convolutional networks (GCNs) are powerful extensions of convolutional neural networks
(CNN) to graph-structured data. Recently, GCNs and variants have been applied to a wide range of
domains, achieving state-of-the-art performances in social networks (Kipf & Welling, 2017), traffic
prediction (Rahimi et al., 2018), recommendation systems (Ying et al., 2018), applied chemistry
and biology (Kearnes et al., 2016; Fout et al., 2017), and natural language processing (Atwood &
Towsley, 2016; Hamilton et al., 2017; Bastings et al., 2017; Marcheggiani & Titov, 2017), just to
name a few (Zhou et al., 2018; Wu et al., 2019).
GCNs belong to a family of spectral methods that deal with spectral representations of graphs (Zhou
et al., 2018; Wu et al., 2019). A fundamental ingredient of GCNs is the graph convolution operation
defined by the graph Laplacian in the Fourier domain:
gθ ? X := gθ(L)x,	(1)
where X ∈ Rn is the graph signal on the set of vertices V and gθ is a spectral function applied to
the graph Laplacian L := D - A (where D and A are the degree matrix and the adjacency matrix,
respectively). Because this operation is computational intensive for large graphs and non-spatially
localized (Bruna et al., 2014), early attempts relied on a parameterization with smooth coefficients
(Henaff et al., 2015) or a truncated expansion in terms of of Chebyshev polynomials (Hammond
et al., 2011). By further restricting the Chebyshev polynomial order by 2, the approach in (Kipf &
Welling, 2017) referred henceforth as the vanilla GCN pushed the state-of-the-art performance of
semi-supervised learning. The network has the following layer-wise update rule:
H(l+1) := ψ AH(l)W (l) ,	(2)
where H(l) is the l-th layer hidden state (with H(1) := X as nodal features), W(l) is the l-th
layer weight matrix, ψ is the usual point-wise activation function, and A is the convolution operator
chosen to be the degree weighted Laplacian with some slight modifications (Kipf & Welling, 2017).
Subsequent GCN variants have different architectures, but they all share the use of the Laplacian
matrix as the convolution operator (Zhou et al., 2018; Wu et al., 2019).
1.1	Why not graph Laplacian?
Undoubtedly, the Laplacian operator (and its variants, e.g., normalized/powered Laplacian) plays a
central role in spectral theory, and is a natural choice for a variety of spectral algorithms such as
1
Under review as a conference paper at ICLR 2020
principal component analysis, clustering and linear embeddings (Chung & Graham, 1997; Belkin &
Niyogi, 2002). So what can be problematic?
From a spatial perspective, GCNs with d layers cannot acquire nodal information beyond its d-
distance neighbors; hence, it severely limits its scope of data fusion. Recent works (Lee et al., 2018;
Abu-El-Haija et al., 2018; 2019; Wu et al., 2019) alleviated this issue by directly powering the graph
Laplacian.
From a spectral perspective, one could demand better spectral properties, given that GCN is funda-
mentally a particular (yet effective) approximation of the spectral convolution (1). A key desirable
property for generic spectral methods is known as “spectral separation,” namely the spectrum should
comprise a few dominant eigenvalues whose associated eigenvectors reveal the sought structure in
the graph. A well-known prototype is the Ramanujan property, for which the second leading eigen-
value of a r-regular graph is no larger than 2√r - 1, which is also enjoyed asymptotically by random
r-regular graphs (Friedman, 2004) and ErdOS-Renyi graphs that are not too sparse (Feige & Ofek,
2005). In a more realistic scenario, consider the stochastic block model (SBM), which attempts
to capture the essence of many networks, including social graphs, citation graphs, and even brain
networks (Holland et al., 1983).
Definition 1 (Simplified stochastic block model). The graph G with n nodes is drawn under
SBM(n, k, aintra, ainter) if the nodes are evenly and randomly partitioned into k communities, and
nodes i andj are connected with probability aintra/n ∈ [0, 1] if they belong to the same community,
and ainter/n ∈ [0, 1] if they are from different communities.
It turns out that for community detection, the top k leading eigenvectors of the adjacency matrix A
play an important role. In particular, for the case of 2 communities, spectral bisection algorithms
simply take the second eigenvector to reveal the community structure. This can be also seen from
the expected adjacency matrix E[A] under SBM(n, 2, aintra, ainter), which is a rank-2 matrix with the
top eigenvalue 1 (aintra + ainter) and eigenvector 1, and the second eigenvalue 1 (aintra - ainter) and
eigenvector σ such that σi = 1 if i is in community 1 and σi = -1 otherwise. More generally, the
second eigenvalue is of particular theoretical interests because it controls at the first order how fast
heat diffuses through graph, as depicted by the discrete Cheeger inequality (Lee et al., 2014).
While one would expect taking the second eigenvector of the adjacency matrix suffices, it often fails
in practice (even when it is theoretically possible to recover the clusters given the signal-to-noise
ratio). This is especially true for sparse networks, whose average nodal degrees is a constant that
does not grow with the network size. This is because the spectrum of the Laplacian or adjacency
matrix is blurred by “outlier” eigenvalues in the sparse regime, which is often caused by high de-
gree nodes (Kaufmann et al., 2016). Unsurprisingly, powering the Laplacian would be of no avail,
because it does not change the eigenvectors or the ordering of eigenvalues. In fact, those outliers
can become more salient after powering, thereby weakening the useful spectral signal even further.
Besides, pruning the largest degree nodes in the adjacency matrix or normalizing the Laplacian can-
not solve the issue. To date, the best results for pruning does not apply down to the theoretical
recovery threshold (Coja-Oghlan, 2010; Mossel et al., 2012; Le et al., 2015); either outliers would
persist or one could prune too much that the graph is destroyed. As for normalized Laplacian, it may
overcorrect the large degree nodes, such that the leading eigenvectors would catch the “tails” of the
graph, i.e., components weakly connected to the main graph. See Figure A.3 in the appendix for an
experimental illustration.
In summary, graph Laplacian may not be the ideal choice due to its limited spatial scope of
information fusion, and its undesirable artefacts in the spectral domain.
1.2	If not Laplacian, then what?
In searching for alternatives, potential choices are many, so it is necessary to clarify the goals. In
view of the aforementioned pitfalls of the graph Laplacian, one would naturally ask the question:
Can we find an operator that has wider spatial scope, more robust spectral properties, and is
meanwhile interpretable and can increase the expressive power of GCNs?
From a perspective of graph data analytics, this question gauges how information is propagated and
fused on a graph, and how we should interpret “adjacency” in a much broader sense. An image can
2
Under review as a conference paper at ICLR 2020
be viewed as a regular grid, yet the operation of a CNN filter goes beyond the nearest pixel to a local
neighborhood to extract useful features. How to draw an analogy to graphs?
From a perspective of robust learning, this question sheds light on the basic observation that real-
world graphs are often noisy and even adversarial. The nice spectral properties of a graph topology
can be lost with the presence or absence of edges. What are some principled ways to robustify the
convolution operator and graph embeddings?
In this paper, we propose a graph learning paradigm that aims at achieving this goal, as illustrated in
Figure 1. The key idea is to generate a sequence of graphs from the given graph that capture a wide
range of spectral and spatial behaviors. We propose a new operator based on this derived sequence.
Definition 2 (Variable power operator). Consider an unweighted and undirected graph G. Let A[k]
denote the k-distance adjacency matrix, i.e., A[k] ij = 1 if and only if the shortest distance (in the
original graph) between nodes i and j is k. The variable power operator of order r is defined as:
r
A(θr) = X θkA[k],	(3)
k=0
where θ := (θ0, . . . , θr) is a set of parameters.
Clearly, A(θr) is a natural extension of the classical adjacency matrix (i.e., r = 1 and θ0 = θ1 = 1).
With power order r > 1, one can increase the spatial scope of information fusion on the graph
when applying the convolution operation. The parameters θk also has a natural explanation—the
magnitude and the sign of θk can be viewed as “global influence strength” and “global influence
propensity” at distance k, respectively, which also determines the participation factor of each graph
in the sequence in the aggregated operator.
Furthermore, we provide some theoretical justification of the proposed operator by establishing the
following asymptotic property of spectral separation under the important SBM setting, which is,
nevertheless, not enjoyed by the classical Laplacian operator or its normalized or powered versions.
(All proofs are given in the appendix.)
Theorem 3 (Asymptotic spectral separation of variable power operator). Consider a graph G drawn
from SBM(n, 2,。正如2, ainter). Assume that the signal-to-noise ratio ξ2∕ξι > 1, where ξι = 11 (aintra +
ainter) and ξ1 = 2 (。由值-ainter) (c.f., (Decelle et al., 2011)). Suppose r is on the order of C log(n)
for a constant c, such that c log(ξ1) < 1∕4. Given nonvanishing θk for k > r∕2, the variable power
operator A(θr) has the following spectral properties: (i) the leading eigenvalue is on the order of
Θ (kθk1ξ1r), the second leading eigenvalue is on the order of Θ (kθk1ξ2r), and the rest are bounded
by kθkιneξrr/2O(log(n)) for any fixed > 0; and (ii) the two leading eigenvectors are sufficient to
recover the two communities asymptotically (i.e., as n goes to infinity).
Intuitively, the above theoretical result suggests that the variable power operator is able to “magnify”
benign signals from the latent graph structure while “suppressing” noises due to random artefacts.
Figure 1: From the original graph, we generate a series of graphs, which are weighted by param-
eters that gauge the influence strengths, (optionally) sparsified, and eventually combined to form a
variable power graph.
3
Under review as a conference paper at ICLR 2020
This is expected to improve spectral methods in general, especially when the benign signals tend to
be overwhelmed by noises. For the rest of the paper, we will apply this insight to propose a robust
graph learning paradigm in Section 2, as well as a new GCN architecture in Section 3. We also
provide empirical evidence of the gain from this theory in Section 4 and conclude in Section 5.
1.3	Related work
Beyond nearest neighbors. Several works have been proposed to address the issue of limited spatial
scope by powering the adjacency matrix (Lee et al., 2018; Wu et al., 2019; Li et al., 2019). However,
simply powering the adjacency does not extract spectral gap and may even make the eigenspectrum
more sensitive to perturbations. Abu-El-Haija et al. (2018; 2019) also introduced weight matrices
for neighbors at different distances. But this could substantially increase the risk of overfitting in the
low-data regime and make the network vulnerable to adversarial attacks.
Robust spectral theory. The robustness of spectral methods has been extensively studied for graph
partitioning/clustering (Li et al., 2007; Balakrishnan et al., 2011; Chaudhuri et al.; Amini et al., 2013;
Joseph et al., 2016; Diakonikolas et al., 2019). Most recently, operators based on self-avoiding
or nonbacktracking walks have become popular for SBM (Massouli6, 2014; Mossel et al., 2013;
Bordenave et al., 2015), which provably achieve the detection threshold conjectured by Decelle et al.
(2011). Our work is partly motivated by the graph powering approach by Abbe et al. (2018), which
leveraged the result OfMaSSouli6 (2014); Bordenave et al. (2015) to prove the spectral gap. The
main difference with this line of work is that these operators are studied only for spectral clustering
without incorporating nodal features. Our proposed variable power operator can be viewed as a
kernel version of the graph powering operator (Abbe et al., 2018), thus substantially increasing the
capability of learning complex nodal interactions while maintaining the spectral property.
Robust graph neural network. While there is a surge of adversarial attacks on graph neural net-
works (GNNs) (Dai et al., 2018; Zugner & Gunnemann, 2019; Bojchevski & Gunnemann, 2019),
very few methods have been proposed for defense (Sun et al., 2018). Existing works employed
known techniques from computer vision (Szegedy et al., 2014; Goodfellow et al., 2015; Szegedy
et al., 2016), such as adversarial training with “soft labels” (Chen et al., 2019) or outlier detection
in the hidden layers (Zhu et al., 2019), but they do not exploit the unique characteristics of graph-
structured data. Importantly, our approach simultaneously improves performance in both the benign
and adversarial tests, as shown in Figure 2 (details are presented in Section 4).
Citeseer
72
71
70
69
68
(5s)
一∙J0φ>p<
70	70.5	71	71.5
Cora
80.8
80.4
80
79.6
81.4	81.6	81.8	82
Pubmed
78.8
78.4
78
77.6
78.8	79	79.2 79.4 79.6 79.8
Clean data accuracy (%)
・ GCN (baseline) ∙ VPN(ours) ▲ r-GCN (ours)
Figure 2: Our proposed framework can improve both clean and adversarial (10% attack by
ADW3 (Bojchevski & Gunnemann, 2019)) accuracy for semi-supervised learning benchmarks.
2	Graph augmentation: Robust training paradigm
Exploration of the spectrum of spectral and spatial behaviors. Given a graph G, consider a
family of its powered graphs, G(2) , . . . , G(r) , where G(k) is obtained by connecting nodes with
distance less than or equal to k. This “graph augmentation” procedure is similar to “data augmenta-
tion”, because instead of limiting the learning on a single graph that is given, we artificially create a
series of graphs that are closely related to each other in the spatial and spectral domains.
As we increase the power order, the graph also becomes more homogenized (Figure A.1). In partic-
ular, it can help near-isolated nodes (i.e., low-degree vertices), since they become connected beyond
their nearest neighbors. By comparison, simply raising the adjacency matrix to its r-th power will
4
Under review as a conference paper at ICLR 2020
make them appear even more isolated, because it inadvertently promotes nodes with high degrees
or nearby loops much more substantially as a result of feedback. Furthermore, the powered graphs
can extract spectral gaps in the original graph despite local irregularities, thus boosting the signal-
to-noise ratio in the spectral domain.
Transfer of knowledge from the powered graph sequence. Consider a generic learning task on a
graph G with data D. The loss function is denoted by '(W; G, D) for a particular GCN architecture
parametrized by W. For instance, in semi-supervised learning, D consists of features and labels on a
small set of nodes (see Table A.1), and ` is the cross-entropy loss over all labeled examples. Instead
of minimizing over '(W; G, D) alone, We use all the powered graphs:
r
'(W; G, D) +£ αk'(W; G(k) D),
(r-GCN)
k=2
where αk ≥ 0 gauges how much information one desires to transfer from powered graph G (k)
to the learning process. By minimizing the (r-GCN) objective, one seeks to optimize the network
parameter W on multiple graphs simultaneously, which is beneficial in two ways: (i) in the low-data
regime, like semi-supervised learning, it helps to reduce the variance to improve generalization and
transferability; (ii) in the adversarial setting, it robustifies the network since it is more likely that the
perturbed network is contained in the wider spectrum during training.
3	From fixed to variable power network
By using the variable power operator illustrated in Figure 1, we substantially increase the search
space of graph operators. The proposed operator also leads to a new family of graph algorithms
with broader scope of spatial fusion and enhanced spectral robustness. As the power grows, the
network eventually becomes dense. To manage this increased complexity and make the network
more robust against adversarial attacks in the feature domain, we propose a pruning mechanism.
Graph sparsification. Given a graph G := (V, E[1]), consider its powered version G(r) := (V, E(r))
and a sequence of intermediate graphs G[2] , . . . , G[r] , where G[k] := (V, E[k] ) is constructed by
connecting two vertices if and only if the shortest distance is k in G. Clearly, {E[k] }rk=1 forms a
partition of E(r). For each node i ∈ V, denote its r-neighborhood by Nr(i) := {j ∈ V | dG (i, j) ≤
r}, which is identical to the set of nodes adjacent to i in G[r] . Next, for each edge within this
neighborhood, we associate a value using some suitable distance metric φ to measure “aloofness.”
For instance, it can be the usual Euclidean distance or correlation distance in the feature space. Based
on this formulation, we prune an edge e := (i, j) in E(r) if the value is larger than a threshold τ (i, j),
and denote the edge set after pruning E(r). Then, we can construct a new sequence of sparsified
graphs, G[k] with edge sets £[k] = E[k] ∩ Em and adjacency matrix A[k]. Hence, the variable power
operator is given by A『)=Pk=0 θk A[k]. Due to the influence of high-degree nodes in the spectral
domain, one can adaptively choose the thresholds τ(i,j) to mitigate their effects. Specifically, we
choose τ to be a small number if either i or j are nodes with high degrees, thereby making the
sparsification more influential in densely connected neighborhoods than weakly connected parts.
Layer-wise update rule. To demonstrate the effectiveness of the proposed operator, we adopt the
vanilla GCN strategy (2). Importantly, we replace the graph convolutional operator A with the
variable power operator to obtain the variable power network (VPN):
A = D-2 (I + Aθr))D-2,
(VPN)
where Dii = 1 + |{j ∈ V | dG (i, j) = 1}|. The introduction of I is reminiscent of the “renormal-
ization trick” (Kipf & Welling, 2017), but it can be also viewed as a regularization strategy in this
context, which is well-known to improve the spectral robustness (Amini et al., 2013; Joseph et al.,
2016). This construction immediately increases the scope of data fusion by a factor of r.
Proposition 4. By choosing A with (VPN) in the layer-wise update rule (2), the output at each node
from a L-layer GCn depends on neighbors within L * r hops.
Since we proved that the variable power operator has nice spectral separation in Theorem 3, VPN is
expected to promote useful spectral signals from the graph topology (similar to the preservation of
5
Under review as a conference paper at ICLR 2020
useful information in images (Jacobsen et al., 2018), our method preserves useful information in the
graph topology). This claim is substantiated with the following proposition.
Proposition 5. Given a graph with two hidden communities. Consider a 2-layer GCN architecture
with layer-wise update rule (2). Suppose that A has a spectral gap. Further, assume that the leading
two eigenvectors are asymptotically aligned with 1 and ν, i.e., the community membership vector,
and that both are in the range of feature matrix X. Then, there exists a configuration of W(1) and
W (2) such that the GCN outputs can recover the community with high probability.
4	Experiments
The proposed methods are evaluated against several recent models, including vanilla GCN (Kipf &
Welling, 2017) and its variant PowerLaplacian where we simply replace the adjacent matrix with its
powered version, three baselines using powered Laplacian IGCN (Li et al., 2019), SGC (Wu et al.,
2019) and LNet (Liao et al., 2019), the recent method MixHop (Abu-El-Haija et al., 2019) which
attempts to increase spatial scope fusion, as well as a state-of-the-art baseline and RGCN (Zhu et al.,
2019), which is also aimed at improving the robustness of Vanilla GCN. All baseline methods on
based on their public codes.
4.1	Revisiting the stochastic block model
SBM dataset. We generated a set of networks under SBM with 4000 nodes and parameters such
that the SNRs range from 0.58 to 0.91. To disentangle the effects from nodal features with that from
the spectral signal, we set the nodal features to be one-hot vectors. The label rates are 0.1%, 0.5%
and 1%, the validation rate is 1%, and the rest of the nodes are testing points.
Performance. Since the nodal features do not contain any useful information, learning without
topology such as multi-layer perceptron (MLP) is only as good as random guessing. The incor-
poration of graph topology improves classification performance■—the higher the SNR (i.e., ξ22∕ξι,
see Theorem 3), the higher the accuracy. Overall, as shown in Figure 3, the performance of the pro-
posed method (VPN) is superior than other baselines, which either use Laplacian (GCN, Chebyshev,
RGCN, LNet) or its powered variants (IGCN, PowerLaplacian).
0 5 0 5
0 7 5 2
(％) AB,!n<
—VPN
—IGCN
→- PowerLaPIadan
→- Chebyshev
→- GCN
—RGCN
→- LanCZoSNet
→- MLP
Figure 3: Comparison for SBM dataset. Shaded area indicates standard deviation over 10 runs.
Spectral separation and Fourier modes. From the eigenspectrum of the convolution operators
(Figure A.3), we see that the spectral separation property is uniquely possessed by VPN, whose first
two leading eigenvectors carry useful information about the underlying communities: without the
help of nodal features, the accuracy is 87% even with label rate of 0.5%. Let Φ denote the Fourier
SNR: 0.58	SNR: 0.68	SNR: 0.79	SNR: 0.85	SNR: 0.91
Oooooo
9 8 7 6 5 4
(％) AOE,JnKX/
90
80
70
60
50
40
VPN
Adjacency
0	25 50 75 100	0	25 50 75 100
o	25	50	75	100	0	25	50	75	100	0	25	50	75	100
Index of frequency component (from low to high)
Figure 4: Accuracy of first 100 frequency components of VPN (red) and adjacency matrix (blue).
6
Under review as a conference paper at ICLR 2020
Table 1: Results for semi-supervised node classification. We highlighted the best and the second
best performances, where we broke the tie by choosing the one with the smallest standard deviation.
Model	Citeseer	Cora	Pubmed
ManiReg (Belkin et al., 2006)	601	59.5	70.7
SemiEmb (Weston et al., 2012)	59.6	59.0	71.1
LP (Zhu et al., 2003)	45.3	68.0	63.0
DeepWalk (Perozzi et al., 2014)	43.2	67.2	65.3
ICA (Lu & Getoor, 2003)	69.1	75.1	73.9
Planetoid (Yang et al., 2016)	64.7	75.7	77.2
Vanilla GCN (Kipf & Welling, 2017)	70.3	81.5	79.0
PowerLaplacian	70.5	80.5	78.3
IGCN(RNM) (Li et al., 2019)	69.0	80.9	77.3
IGCN(AR) (Li et al., 2019)	69.3	81.1	78.2
LNet (Liao et al., 2019)	66.2 ± 1.9	79.5 ± 1.8	78.3 ± 0.3
RGCN (Zhu et al., 2019)	71.2 ± 0.5	82.8 ± 0.6	79.1 ± 0.3
SGC (Wu et al., 2019)	71.9 ± 0.1	81.0 ± 0.0	78.9 ± 0.0
MixHop (Abu-El-Haija et al., 2019)	71.4 ± 0.8	81.9 ± 0.4	80.8 ± 0.6
r-GCN (this paper)	71.3 ± 0.54	81.7 ± 0.23	79.3 ± 0.31
VPN (this paper)	71.4 ± 0.57	81.9 ± 0.32	79.6 ± 0.39
modes of the the adjacency matrix or VPN, and X be the nodal features (i.e., identity matrix). We
analyze the information from spectral signals (e.g., the k-th and k+ 1-th eigenvectors) by estimating
the accuracy of an MLP with filtered nodal features, namely Φ：,k：(k+1)Φτk：(k+1)X, as shown in
Figure 4. The accuracy reflects the information content in the frequency components. We see that
the two leading eigenvectors of VPN are sufficient to perform classification, whereas those of the
adjacency matrix cannot make accurate inferences.
4.2	Semi-supervised node classification
Experimental setup. We followed the setup of (Yang et al., 2016; Kipf & Welling, 2017) for citation
networks Citeseer, Cora and Pubmed (please refer to the Appendix for more details).
Graph powering order can influence spatial and spectral
behaviors. Our theory suggests powering to the order of
log(n); in practice, orders of 2 to 4 suffice (Figure 5).
Here, we chose the power order to be 4 for r-GCN on
Citeseer and Cora, and 3 for Pubmed, and reduced the
order by 1 for VPN.
Performance. By replacing Laplacian with VPN, we see
an immediate improvement in performance (Table 1). We
also see that a succinct parametrization of the global in-
Benign
5% attack
10% attack
Figure 5: Benign and adversarial accu-
racy as power order increases. Dahsed
lines correspond to vanilla GCN.
fluence relation in VPN is able to increase the expressivity of the network. For instance, the learned
θ at distances 2 and 3 for Citeseer are 3.15e-3 and 3.11e-3 with p-value less than 1e-5. This implies
that the network tends to put more weights in closer neighbors.
4.3	Defense against evasion attacks
To evaluate the robustness of the learned network, we considered the setting of evasion attacks,
where the model is trained on benign data but tested on adversarial data.
Adversarial methods. Five strong global attack methods are considered, including DICE (ZUgner
& Gunnemann, 2019), Aabr and Adw3 (Bojchevski & Gunnemann, 2019), Meta-Train and Meta-
Self (ZUgner & GUnnemann, 2019). We further modulated the severity of attack methods by varying
the attack rate, which corresponds to the percentage of edges to be attacked.
Robustness evaluation. In general, both r-GCN and VPN are able to improve over baselines for the
defense against evasion attacks, e.g., Figure 6 for the ADW3 attack (detailed results for other attacks
are listed in the Appendix). It can be also observed that the proposed methods are more robust in
Citeseer and Cora than Pubmed. In addition to the low label rates, we conjectured that topological
7
Under review as a conference paper at ICLR 2020
IGCN(RNM) ■ IGCN(AR) ■ LanczosNet ■ PowerLap 2 ■ PowerLap 3 ≡SGC ■ MixHop ■ RGCN ■ r-GCN BVPN
(≡s) juəluə:ɔuɪuə
SsoUjSnqOX
Attack rate (%)
Attack rate (%)
post-attack	post-attack
Figure 6:	Robustness merit accuracyproposed-method - accuracyvanilla GCN reported in percentage un-
der ADW3 attack. The error bar indicates standard deviation over 20 independent simulations.
attacks are more difficult to defend for networks with prevalent high-degree nodes, because the
attacker can bring in more irrelevant vertices by simply adding a link to the high-degree nodes.
Informative and robust low-frequency spectral signal. It has been observed by Wu et al. (2019);
Maehara (2019) that GCNs share similar characteristics of low-pass filters, in the sense that nodal
features filtered by low-frequency Fourier modes lead to accurate classifiers (e.g., MLP). However,
one key question left unanswered is how to obtain the Fourier modes. In their experiments, they
derive it from the graph Laplacian. By using VPN to construct the Fourier modes, we show that the
information content in the low-frequency domain can be improved.
More specifically, we first perform eigendecomposition of the graph convolutional operator (i.e.,
graph Laplacian or VPN) to obtain the Fourier modes Φ. We then reconstruct the nodal features
X using only the k-th and the k + 1-th eigenvectors, i.e., Φ∖k[k+i)Φ>k.(k+i)X. We then use the
reconstructed features in MLP to perform the classification task in a supervised learning setting. As
Figure 7 shows, features filtered by the leading eigenvectors of VPN lead to higher classification
accuracy compared to the classical adjacency matrix.
For the adversarial testing, we construct a new basis Φ based on the attacked graph, and then use
>
Φ=,k[k+i)Φ>k[k+i)X as test points for the MLP trained in the clean data setting. As can be seen
in Figure 8, models trained based on VPN filtered features also have better adversarial robustness
in evasion attacks. Since the eigenvalues of the corresponding operator exhibit low-pass filtering
characteristics (Figure A.2), the enhanced benign and adversarial accuracy of VPN is attributed
to the increased signal-to-noise ratio in the low-frequency domain. This is in alignment with the
theoretical proof of spectral gap developed in this study.
(％) AυBJnυυv
Citeseer
40
35
30
25
20
15
0 100 200 300 400 500
Cora
50
40
30
20
0 100 200 300 400 500
Index of frequency component (from low to high)
Figure 7:	Accuracy of first 500 frequency components for VPN (red) and adjacency matrix (blue).
The X axis corresponds to the index k in Φ∖k[k+i)Φ>k.(k+i)X for signal reconstruction.
4.4 Discussions
In this paper, we proposed a new graph operator to replace the graph Laplacian in the classic GCN. It
is worth mentioning that our approach is very different from the k-th order polynomials of the Lapla-
cian developed in (Defferrard et al., 2016) or the high-order graph Laplacian employed in (Lee et al.,
2018; Li et al., 2019; Abu-El-Haija et al., 2019; Wu et al., 2019). If the graph Laplacian L is pow-
ered to the k-th order, the resulting matrix has the same eigenvectors as L, with only the eigenvalues
8
Under review as a conference paper at ICLR 2020
■ VPN HAdj
60
4.5
30
15
0
CiteSeer
Cora
75
PUbmed
60	H H
I l l I I I I I l I H I I 20 Illlll
0	5	10	15	20	25	30
Attack rate (%)
0	5	10	15	20	25	30
Attack rate (%)
a0 a5 a10 a15 a20 a25 a30
Attack rate (%)
0
0
Figure 8: Accuracy of the leading frequency components in adversarial testing. We use
>
Φ二,LkΦ> i:kX as reconstructed nodal features for testing, where k is 10 for Citeseer and 5 for Cora
and Pubmed. Error bar indicates standard deviation over 10 runs.
powered to the k-th order. Thus, the corresponding k-th order polynomial has the same eigenvec-
tors, with the polynomial function applied only to the eigenvalues. However, the proposed variable
power operator has radically different eigenvectors as the graph Laplacian or its powered variant.
This is an important difference because we observed empirically that the leading eigenvectors of a
graph Laplacian are extremely sensitive to outliers under the SBM, and they often correspond to
either tails or high-degree nodes (please see Figure A.3 for an illustration). However, our result in
Theorem 3 suggests that the leading eigenvectors of the proposed operator asymptotically recovers
the underlying community under SBM, and enjoys the “spectral gap” property.
We also remark that the meaning of robustness in the machine learning literature (“adversarial ro-
bustness”) and in the spectral theory literature (“spectral robustness”) appear to be different. How-
ever, in this work, we argue that spectral robustness is closely related to adversarial robustness. This
can be explained from a matrix perturbation point of view. Let λk and φk be the eigenvalue and the
corresponding eigenvector of the adjacency matrix A. Then, the sensitivity of the eigenvector to the
perturbation is given by (Trefethen & Bau III, 1997):
∂φk = XX φ'(i)φk(j)(2 - δ(ijp φ
dA(ij)	'=⅛=k	λk-λ'	',
where A(ij) is the (i,j)-th entry of A, φk(j) is the j-th entry of φk, δ(ij) is the perturbation of the
(i, j)-th entry of A, and n is the dimension of A. It can be seen that the perturbation of the con-
volutional operator is controlled by the inverse of the spectral gap. When the spectral gaps between
the leading eigenvalues (i.e., λ1 and λ2) and the rest of the eigenvalues are large, the perturbations
of the leading eigenvectors due to the adversarial attack are controlled. Since the eigenvalues of the
graph convolutional operator decays very quickly (see Figure A.2 as an illustration), the learned rep-
resentation is heavily influenced by the leading eigenvectors. Thus, by controlling the perturbations
of the leading eigenvectors, one can expect to control the perturbations of the outputs.
Our theoretical result in Theorem 3 was based on the standard SBM, which is a classic model
proposed in mathematical sociology and has been adopted to model and analyze real-world social
and biological networks (Funke & Becker, 2019). The main advantage of SBM is that it encodes the
structural information that nodes belong to the same community tend to be more connected with each
other. Nevertheless, we recognize one limitation of our analysis is that SBM might not be a suitable
model for some existing or future applications, in which case, models with more flexibility in the
degree distributions, such as the degree-corrected SBM proposed by Karrer & Newman (2011),
labeled SBM by Heimlicher et al. (2012), or hiearchical SBM proposed by Peixoto (2017) could
be more applicable. Also our analysis is limited to two communities, and the extension to more
than two communities is a challenging open problem. Nevertheless, through extensive experiments,
we observed that our theory has strong implications for real-world graphs. This is evident from the
informative and robust low-frequency spectral signal results in Figures 7 and 8, which show that
the proposed graph convolution operator has a clear advantage in the low-frequency regime over the
classic graph Laplacian in terms of classification accuracy.
5 Conclusion
This study goes beyond classical spectral graph theory to defend GCNs against adversarial attacks.
We challenge the central building block of existing methods, namely the graph Laplacian, which
9
Under review as a conference paper at ICLR 2020
is not robust to noisy links. For adversarial robustness, spectral separation is a desirable property.
We propose a new operator that enjoys this property and can be incorporated in GCNs to improve
expressivity and interpretability. Furthermore, by generating a sequence of powered graphs based on
the original graph, we can explore a spectrum of spectral and spatial behaviors and encourage trans-
ferability across graphs. The proposed methods are shown to improve both benign and adversarial
accuracy over various baselines evaluated against a comprehensive set of attack strategies.
References
Emmanuel Abbe, Enric Boix, Peter Ralli, and Colin Sandon. Graph powering and spectral robust-
ness. arXiv preprint arXiv:1809.04818, 2018.
Sami Abu-El-Haija, Amol Kapoor, Bryan Perozzi, and Joonseok Lee. N-GCN: Multi-scale graph
convolution for semi-supervised node classification. arXiv preprint arXiv:1802.08888, 2018.
Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Hrayr Harutyunyan, Nazanin Alipourfard,
Kristina Lerman, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolu-
tion architectures via sparsified neighborhood mixing. arXiv preprint arXiv:1905.00067, 2019.
Arash A Amini, Aiyou Chen, Peter J Bickel, Elizaveta Levina, et al. Pseudo-likelihood methods for
community detection in large sparse networks. The Annals ofStatistics, 41(4):2097-2122, 2013.
James Atwood and Don Towsley. Diffusion-convolutional neural networks. In NIPS 2016, pp.
1993-2001, 2016.
Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, and Aarti Singh. Noise thresholds for
spectral clustering. In NIPS 2011, pp. 954-962, 2011.
Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Sima’an. Graph convolu-
tional encoders for syntax-aware neural machine translation. In EMNLP 2017, pp. 1957-1967,
2017.
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and
clustering. In NIPS 2002, pp. 585-591, 2002.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples. Journal of Machine Learning Research,
7:2399-2434, 2006.
Aleksandar Bojchevski and StePhan Gunnemann. Adversarial attacks on node embeddings via graph
poisoning. In International Conference on Machine Learning, pp. 695-704, 2019.
Charles Bordenave, Marc Lelarge, and Laurent Massoulig Non-backtracking spectrum of random
graphs: community detection and non-regular ramanujan graphs. In FOCS 2015, pp. 1347-1357.
IEEE, 2015.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. In ICLR 2014, 2014.
Kamalika Chaudhuri, Fan Chung, and Alexander Tsiatas. Spectral clustering of graphs with general
degrees in the extended planted partition model. Journal of Machine Learning Research, 2012:
1-23.
Jinyin Chen, Yangyang Wu, Xiang Lin, and Qi Xuan. Can adversarial network attack be defended?
arXiv preprint arXiv:1903.05994, 2019.
Fan RK Chung and Fan Chung Graham. Spectral graph theory. Number 92. American Mathematical
Soc., 1997.
Amin Coja-Oghlan. Graph partitioning via adaptive spectral techniques. Combinatorics, Probability
and Computing, 19(2):227-284, 2010.
Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack on
graph structured data. In International Conference on Machine Learning, pp. 1123-1132, 2018.
10
Under review as a conference paper at ICLR 2020
Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturbation: III.
SIAM Journal on Numerical Analysis, 7(1):1-46,1970.
AUrelien Decelle, Florent Krzakala, CristoPher Moore, and Lenka Zdeborov叁 Asymptotic analysis
of the stochastic block model for modular networks and its algorithmic applications. Physical
Review E, 84(6):066106, 2011.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. neural information processing systems, pp. 3844-
3852, 2016.
Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, and Alistair Stewart.
Robust estimators in high-dimensions without the computational intractability. SIAM Journal on
Computing, 48(2):742-864, 2019.
Uriel Feige and Eran Ofek. Spectral techniques applied to sparse random graphs. Random Structures
& Algorithms, 27(2):251-275, 2005.
Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph
convolutional networks. In NIPS 2017, pp. 6530-6539, 2017.
Joel Friedman. A proof of alon’s second eigenvalue conjecture and related problems. Mem. Amer.
Math. Soc., (910), 2004.
Thorben Funke and Till Becker. Stochastic block models: A comparison of variants and inference
methods. PloS one, 14(4):e0215296, 2019.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In AISTATS 2010, pp. 249-256, 2010.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In ICLR 2015, 2015.
William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. NIPS 2017, pp. 1024-1034, 2017.
David K. Hammond, Pierre Vandergheynst, and Remi Gribonval. Wavelets on graphs via spectral
graph theory. Applied and Computational Harmonic Analysis, 30(2):129-150, 2011.
Simon Heimlicher, Marc Lelarge, and Laurent Massoulie. Community detection in the labelled
stochastic block model. arXiv preprint arXiv:1209.2910, 2012.
Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured
data. arXiv preprint arXiv:1506.05163, 2015.
Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: First
steps. Social Networks, 5(2):109-137, 1983.
Jorn-Henrik Jacobsen, Arnold Smeulders, and Edouard Oyallon. i-RevNet: Deep invertible net-
works. In ICLR 2018, 2018.
Antony Joseph, Bin Yu, et al. Impact of regularization on spectral clustering. The Annals of Statis-
tics, 44(4):1765-1791, 2016.
Brian Karrer and Mark EJ Newman. Stochastic blockmodels and community structure in networks.
Physical review E, 83(1):016107, 2011.
Emilie Kaufmann, Thomas Bonald, and Marc Lelarge. A spectral algorithm with additive clustering
for the recovery of overlapping communities in networks. In ALT 2016, pp. 355-370. Springer,
2016.
Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph
convolutions: moving beyond fingerprints. Journal of computer-aided molecular design, 30(8):
595-608, 2016.
11
Under review as a conference paper at ICLR 2020
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In ICLR 2017, 2017.
Can M Le, Elizaveta Levina, and Roman Vershynin. Sparse random graphs: regularization and
concentration of the laplacian. arXiv preprint arXiv:1502.03049, 2015.
James R Lee, Shayan Oveis Gharan, and Luca Trevisan. Multiway spectral partitioning and higher-
order cheeger inequalities. Journal of the ACM, 61(6):37, 2014.
John Boaz Lee, Ryan A Rossi, Xiangnan Kong, Sungchul Kim, Eunyee Koh, and Anup Rao. Higher-
order graph convolutional networks. arXiv preprint arXiv:1809.07697, 2018.
Jure Leskovec and Julian J. Mcauley. Learning to discover social circles in ego networks. In
Advances in Neural Information Processing Systems 25, pp. 539-547, 2012.
Qimai Li, Xiao-Ming Wu, Han Liu, Xiaotong Zhang, and Zhichao Guan. Label efficient semi-
supervised learning via graph filtering. In CVPR 2019, pp. 9582-9591, 2019.
Zhenguo Li, Jianzhuang Liu, Shifeng Chen, and Xiaoou Tang. Noise robust spectral clustering. In
ICCV 2007, pp. 1-8, 2007.
Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard S. Zemel. Lanczosnet: Multi-scale deep
graph convolutional networks. In ICLR 2019, 2019.
Qing Lu and Lise Getoor. Link-based classification. In ICML 2003, pp. 496-503, 2003.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(Nov):2579-2605, 2008.
Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters. arXiv preprint
arXiv:1905.09550, 2019.
Diego Marcheggiani and Ivan Titov. Encoding sentences with graph convolutional networks for
semantic role labeling. In EMNLP 2017, pp. 1506-1515, 2017.
LaUrent MassouliC. Community detection thresholds and the weak ramanujan property. In STOC
2014, pp. 694-703. ACM, 2014.
Elchanan Mossel, Joe Neeman, and Allan Sly. Stochastic block models and reconstruction. arXiv
preprint arXiv:1202.1499, 2012.
Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjecture.
arXiv preprint arXiv:1311.4115, 2013.
Tiago P Peixoto. Nonparametric bayesian inference of the microcanonical stochastic block model.
Physical Review E, 95(1):012317, 2017.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representa-
tions. In KDD 2014, pp. 701-710. ACM, 2014.
Afshin Rahimi, Trevor Cohn, and Timothy Baldwin. Semi-supervised user geolocation via graph
convolutional networks. In ACL 2018, volume 1, pp. 2009-2019, 2018.
Lichao Sun, Ji Wang, Philip S Yu, and Bo Li. Adversarial attack and defense on graph data: A
survey. arXiv preprint arXiv:1812.10528, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In ICLR 2014, 2014.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In CVPR 2016, pp. 2818-2826, 2016.
12
Under review as a conference paper at ICLR 2020
Lloyd N Trefethen and David Bau III. Numerical linear algebra, volume 50. SIAM, 1997.
Jason Weston, Frederic Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi-
supervised embedding. Neural Networks: Tricks of the Trade (2nd ed.), pp. 639-655, 2012.
Hermann Weyl. Das asymptotische verteilungsgesetz der eigenwerte linearer partieller differen-
tialgleichungen (mit einer anwendung auf die theorie der hohlraumstrahlung). Mathematische
Annalen, 71(4):441-479, 1912.
Felix Wu, Amauri H. Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Q. Weinberger.
Simplifying graph convolutional networks. In ICML 2019, pp. 6861-6871, 2019.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A
comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019.
Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning
with graph embeddings. In ICML 2016, pp. 40-48, 2016.
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec.
Graph convolutional neural networks for web-scale recommender systems. In KDD 2018, pp.
974-983. ACM, 2018.
Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Graph neural
networks: A review of methods and applications. arXiv preprint arXiv:1812.08434, 2018.
Dingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu Zhu. Robust graph convolutional networks
against adversarial attacks. In KDD 2019, pp. 1399-1407, 2019.
Xiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty. Semi-supervised learning using gaussian
fields and harmonic functions. In ICML 2003, pp. 912-919, 2003.
Daniel Zugner and Stephan Gunnemann. Adversarial attacks on graph neural networks via meta
learning. In ICLR 2019, 2019.
A	Appendix
A. 1 Proof of Proposition 4
To validate the claim, we prove a more general result. Consider a graph G and its powered graph
G(r), whose adjacency matrices are A and A(r), respectively. Since the support of A(θr) is identical
to that of A(r), we will focus on A(r) for notational simplicity. For any signal f : V 7→ R|V| on
a graph, denote the support of the signal fv on node v ∈ V by Svf, i.e., fv only depends on the
signals on nodes in Sf. Then, it is easy to see that the signal g，Af has support Sg ⊆ ∪u〜rV Su,
where U 〜r V denotes adjacency relation on G(r) defined by A(r). Clearly, the set defined by
{u ∈ V | u 〜r v} is identical to {u ∈ V | dg(u,v) ≤ r}, where dg(u,v) denotes the shortest
distance from u to v on G .
Also, we see that element-wise operation does not expand the support, i.e., for any element-wise
activation σ, the signal gσ , σ(f) has support Svgσ ⊆ Svf . In particular, we can prove the claim
by iteratively applying the function composition layer by layer, since each layer is of the form
σ(A(θr)H(l)W (l)), where H(l) is the hidden state at layer l and W(l) is the corresponding weight
matrix. Hence after L iterations of layer-wise updates, the support of the output on v is given by
{u ∈ V | dg(u, V) ≤ r * L}.
A.2 Proof of Proposition 5
Let λι and λ? be the two leading eigenvalues of A with corresponding eigenvectors φι and φ2,
respectively. Without loss of generality, assume that both eigenvalues are nonnegative. Under the
13
Under review as a conference paper at ICLR 2020
assumption that φ1 and φ2 lie in the range ofX, there exists a W(1) such that XW (1) = [φ1 φ2].
So the output of the first layer with ReLU activation is given by:
H⑴二σ (AXW⑴)=σ (A [φι φ2])	(4)
= [λ1σ(φ1) λ2σ(φ2)]	(5)
=[λι(1 + σ(hι)) λ2( 1 ν + 21 + σ(h2))] where IlhIII= o(1) and M2∣∣ = o(1) (6)
where (5) is due to the definition of eigenvectors and that σ is an element-wise operation, and (6)
is due to the asymptotic alignment condition and the fact that σ(ν) = 2V + 21. Therefore, by
choosing the last layer weight W(2) =	λ21λ2	λ1λ22 , the output of the last layer is given by:
_ λ2	-λ2j
Y = A H⑴W⑵
=A [λι(1 + σ(hι)) λ2( 1 v + 11 + σ(h2))] W⑵
=%1 + h01 λ2v + λ12λ21 + h2i W⑵
= [ν + h010 -ν + h020 ]
where Ih0kI2 = o(1), Ih0k0I2 = o(1) for k = 1, 2. Therefore, after the softmax operation, we can
see that the 2-layer output recovers the membership of the elements exactly.
One general remark is that while the above argument is not limited to the leading two eigenval-
ues/eigenvectors, from the form of W (2) whose elements depend inversely on the corresponding
eigenvalues, we can expect more numerical stability (and hence easier learning) when the corre-
sponding eigenvalues are large.
A.3 Proof of Theorem 3
Consider a stochastic block model with two communities, where the intra-community connection
probability is aintra/n and the inter-community connection probability is ainter/n, and assume that
aintra > ainter. Let ξι = aintra+ainter and ξ2 = aintra- ainter, which correspond to the first and second
eigenvalues of the expected adjacency matrix of the above model. Let v denote the community
label vector, with νi = ±1 depending on the membership of node i. For weak recovery, we assume
the detectability condition (Decelle et al., 2011):
ξ22 > ξ1 .	(7)
With the notations from the main paper, let A(θr) = Prk=0 θkA[k] denote the θ-weighted variable
[k]	1 if dG (i, j) = k
power operator, where A[k] ij = o.w.	is the k-distance adjacency matrix. Let
A{k} denote the nonbacktracking path counting matrix, where the (i,j)-th component indicate the
number of self-avoiding paths of graph edges of length k connecting i to j (Massouli6, 2014). Our
goal is to prove that the top eigenvectors of A(θr) are asymptotically aligned with those of A{k} for
k greater than log(n) up to a constant multiplicative factor. To do so, we first recall the result of
(Massouli6, 2014), which examines the behaviors of top eigenvectors of A{k}.
Lemma 6 ((MaSSouli6, 2014)). Assume that (7) holds. Then, w.h.p., for all k ∈ {r/2,..., r}:
(a)	The operator norm IA{k} I2 is up to logarithmic factors Θ(ξ1k). The second eigenvalue of
A{k} is UP to logarithmic factors Ω(ξk).
(b)	The leading eigenvector is asymptotically aligned with A{k} 1:
A{k}A{k}1	A{k}1
kA{k}A{k}1∣2 = ∣A{k}1∣2 + hk
where IhkI = o(1). The second eigenvector is asymptotically aligned with A{k}v:
A{k}A{k}v	A{k}v	0
∣A{k}A{k}v ∣∣2 = ∣∣A{k}v ∣∣2 + hk
where Ih0k I = o(1).
14
Under review as a conference paper at ICLR 2020
Now, we first define Γ{θr} = Prk=r/2 θkA{k}, so
r
Aθr) = Γθr} + ∆θr} = Γθr} + A*T) + X θk(A[k] - A{k}).
k=r/2
Also, by (Massouli6, 2014, Theorem 2.3), the local neighborhoods of vertices do not grow fast,
and w.h.p., the graph of A(T/2T) has maximum degree O(ξ[/2(logn)2). Let φ be the leading
eigenvector of A^r/2-1), and let V be the node where φ is maximum (i.e., φ(v) ≥ φ(u)). Without
loss of generality, assume that φ(v) > 0. We have a good control over the first eigenvalue λ1 of
A^/2-1) as follows:
λ1
(Aθ"2T) Φ)(v) = Pk=0 PP{u:dG (u,v) = k} θkφ(u)
Φ(v)	Φ(v)
r/2-1
X θk X
k=0	{u:dG (u,v)=k}
φ(U)
φ(v)
r/2-1
≤ X ∣θkI	X 1
k=0	{u:dG (u,v)=k}
≤ kθk∞	1
{u:dg (u,v)≤r∕2 -1}
=O(kθk*r∕2(log n)2)
Due to (Abbe et al., 2018, Theorem 2), which proves an identical result of Lemma 6 but for A[k],
we know that w.h.p., for r = log(n), we can bound the last term by
rr
k X θk(A[k] — A{k})k2 ≤ X ∣θk∣kA[k]- A{k}k2 = O (kθkιξr∕2(logn)4)
k=r∕2	k=0
Therefore, by triangle inequality, we can conclude that
k∆{θr}k2 = kA(θr) - Γ{θr}k2 = O kθk1ξ1r∕2(logn)4 .	(8)
Now, our goal is to show a similar result as Lemma 6 for Γ{θr} .	In particular,
krθr}A{r}1k2 = k X %A{k}	A{r}1 kA{r}1k2」除2 k	kA{r}1k2 2	(9)
="A{k} kA⅛ [+。仁如…	A{r}1	A{k}1	∣l∖ kA{r}1k2 — kA{k}1k2||J	( )
=|k=X2 θk A{k} ⅛ l + OQ^Mn)	(11)
k r	A{k}1 k =X θkA{k}kAF	+ O (kθkιξrn-") kk=r∕2	k2	(12)
X/k ( kA{⅛ + hk) kAAAiIk2	+ O (kθkι6 n-δ)for khk k2 =。⑴
2	(13)
15
Under review as a conference paper at ICLR 2020
r
X θk
k=r/2
A{r} 1	、∣∣A{k}A{k}ιk2
kA{F + hk)	∣A{k}1∣2
+
2
IIX ∣A{k} A{k}1∣2
Oki?	∣A{k}1k2
A{r}1	A{k}1
kA{r}1k2	kA{k}1k2
+ O (kθk*rn-δ)
—
2
(14)
”(W + hk) ^⅛{^2+ O U n-δ)
(XX ∣θk∣kA{A{Aj1k2) (1 + o(1)) + O (kθk*rn-δ)
k=r/2
O (kθkιξrn-δ),
(15)
(16)
(17)
where (10) and (14) are due to triangle inequalities, (11) and (15) are due to results in (Massouli6,
2014), and (13) is due to Lemma 6. Therefore, we have ∣∣rθr}A{r}1∣2 = Θ (∣∣θ∣∣ιξr∣∣ A{r}1∣2).
Similarly, we can prove that ∣rθr} A{r}ν∣∣2 = Θ (∣∣θ∣∣ιξr ∣∣ A{r}ν∣∣2).
{r}
Furthermore, we can show that Γθ satisfies the weak Ramanujan property (the proof is similar
to (Massoulie, 2014) and is deferred to Section A.4).
Lemma 7. For any fixed > 0, Γ{θr} satisfies the following weak Ramanujan property w.h.p.,
SUp	l∣rθr}u∣2 = |例"€；/20(1085))
kuk2=1,u>A{r}1=u>A{r}ν=0
By Lemma 7, the leading two eigenvectors of Γ{θr} will be asymptotically in the span of A{r} 1 and
A{r}ν according to the variational definition of eigenvalues. By our previous analysis, this means
that the top eigenvalue of Γ{θr} will be Θ (∣θ∣1ξ1r) with eigenvector asymptotically aligned with
A{r}1. Since by (Massoulie, 2014, Lemma 4.4), A{r} 1 and A{r}ν are asymptotically orthogonal,
the second eigenvalue of Γ{θr} will be Θ (∣θ∣1ξ2r) with eigenvector asymptotically aligned with
A{r}ν.
Since the perturbation term ∣∆{θr} ∣2 = o λ2(Γ{θr}) by (8), using Weyl’s inequality (Weyl, 1912),
we can conclude that the leading eigenvalue λ1 A(θr)	= Θ (∣θ∣1ξ1r), the second eigenvalue
λ2 (Aθr)) = Θ (∣∣θ∣∣ιξr), and the rest of the eigenvalues are bounded by |网|1n吒：/20(10且(n)).
Moreover, since ∣∆{θr}∣2 = o max nλ1(Γ{θr}) - λ2(Γ{θr}), λ2(Γ{θr}) - λ3(Γ{θr})o, by the
Davis-Kahan Theorem (Davis & Kahan, 1970), the leading two eigenvectors of A(θr) are asymp-
totically aligned with A{r} 1 and A{r} ν, which is shown to be enough for the rounding procedure
of (Massoulie, 2014) to achieve weak recovery.
A.4 Proof of Lemma 7
Denote A = antra [ɪ(11> + VV>) — I] + 翳(11> — VV>) as the expected adjacency matrix con-
ditioned on community labels ν. Let Pij denote the set of all self-avoiding paths i0k := {i0, . . . , ik}
from i to j, such that no nodes appear twice in the path, and define Qi{jm} := Pim ∈P Qtm=1(A—
Ae)it-1it. Let Timj be the concatenation of self-avoiding paths i0k-m and ikk-m+1, and let Rimj denote
Timj \Pij. Define Wi{jk,m} := Pi0k∈Rimj Qtk=-1m(A — Ae)it-1itAeik-mik-m+1 Qtk=k-m+2(A)it-1it.
16
Under review as a conference paper at ICLR 2020
Uo=nq-i=-po>-a-nujnu
CiteSeer
Degree
Cora
8642
Oooo
UOnq-po>--nujn
Degree
Figure A.1: Degree distribution in the original graph and the powered graphs for Citeseer and Cora.
The graph becomes more homogeneous and high degree nodes become more prevalent as we in-
crease the powers.
Then, by (Massouli6, 2014, Theorem 2.2), We can have
A{k} = Q{k} + X (Q{k-m}AeA{m-1}) - X W {k,m}.
m=1	m=1
Hence,
r
Γ{θr} = X θkA{k}
k=r/2
r	r	k	rk
= X θkQ{k} + X θk X Q{k-m}AeA{m-1} - X X θkW{k,m}
k=r/2	k=r/2	m=1	k=r/2 m=1
r	r	r	rk
X θkQ{k} + X I X	θkQ{k-m}N)A{m-1} - X X θkW{k,m}.
k=r/2	m=1	k=max(r/2,m)	k=r/2 m=1
In particular, for any u that is orthogonal to A{m} 1 and A{m}ν With norm-1 (i.e., a feasible vector
of the variational definition in Lemma 7), We have:
(r	r	r
X θkQ{k} I + X X	∣θkIp(Q{k-m})忸"-1}川2
k=r/2	m=1 k=max(r/2,m)
(r	k	∖
X θk X W {k,m} I
k=r/2	m=1
Since by (Massoulie, 2014), the terms P (Q{k}) and P(W{k,m}) are bounded by nξr/,
AeA{m-1}
x is bounded by O(log(n) +	log(n)ξ1m-1), With some elementary calculations,
we have that ∣ rθr}u∣∣ is bounded by |网|1"§：/20(10且(九)).
Table A.1: Citation datasets. Label rate denotes the fraction of training labels in each dataset.
Dataset	Nodes	Edges	Features	Classes	Label rates
Citeseer	3,327	4,732	3,703^^	6	0.036
Cora	2,708	5,429	1,433	7	0.052
Pubmed	19,717	44,338	500	3	0.003
A.5 More experimental details and results
Experimental setup. We followed the setup of (Yang et al., 2016; Kipf & Welling, 2017) for ci-
tation networks Citeseer, Cora and Pubmed. The statistics of datasets is in Table A.1. Degree
17
Under review as a conference paper at ICLR 2020
distribution in the original graph and the powered graphs for Citeseer and Cora is shown in Fig-
ure A.1. We used the same dataset splits as in (Yang et al., 2016; Kipf & Welling, 2017) with 20
labels per class for training, 500 labeled samples for hyper-parameter setting, and 1, 000 samples
for testing. To facilitate comparison, we use the same hyper-parameters as Vanilla GCN (Kipf &
Welling, 2017), including number of layers (2), number of hidden units (16), dropout rates (0.5),
weight decay (5 × 10-4), and weight initialization following (Glorot & Bengio, 2010) implemented
in Tensorflow. The training process is terminated if validation accuracy does not improve for 40
consecutive steps. We note that the citation datasets are extremely sensitive to initializations; as
such, we report the test accuracy for the top 50 runs out of 100 random initializations sorted by the
validation accuracy. We used Adam (Kingma & Ba, 2014) with a learning rate of 0.01 except for
θ (VPN), which was chosen to be 1 × 10-5 to stabilize training. For VPN, we consider two-step
training process, i.e., We first employ graph SParsification on adjacency matrix A[k] based on X
for the first-step training, then process second graph SParSification on adjacency matrix A[k] with
respect to the feature embedding from hidden layer and fetch them back into model for second-step
training with warm restart (continue training with the parameters from first-step). We set αk to be
0.5 for the corresponding power order and 0 other wise in r-GCN.
To enhance robustness against evasion attack, we adaptively chose the threshold of graph sparsifica-
tion. For each node, we consider all its neighbors within r hops, where r is the order of the powered
graph. We order these neighbors based on their Euclidean distance to the current node in the feature
space. We then set the threshold for this node such that only the first S * d neighbors with the smallest
distance are selected, where s is the sparsification rate, and dis the degree of the node in the original
graph. If this node is a high-degree node (determined by if the degree is higher than the average
degree by more than 2 standard deviations), we set s = 1; otherwise, we set s to be a small number
slightly larger than 1. Nevertheless, we do not observe that the robustness performance is sensitive
to this parameter. In addition, one can define their own distance function that better measures prox-
imity between nodes, such as correlation distance or cosine distance. As a side remark, one can
easily obtain A(r) with 1[(I + A)r > 0], and recursively calculate A[k] . The proposed methods
have similar computational complexity as vanilla GCN as a result of sparsification.
(a)

Citeseer	Cora	Pubmed
Ju---ya=① oυ
一袋υ①ds
0	1000 2000 3000	0	1000	2000	0	5000 10000 15000 2000(
Index of eigenvalues
■ ■ , power 1
—power 2
--power 3
--power 4
--power 5
Ju---ya=① oυ
一B-υ①ds
Index of eigenvalues
■ 一 power 1
—power 2
■ — power 3
—power 4
—power 5
Figure A.2: Eigenspectrum of (a) the adjacency matrix and (b) VPN raised to different powers. The
convolution operator becomes similar to low-pass filters as we increase powers. The eigenspectrum
of VPN also has sharper edges than the original adjacency matrix.
Revisiting SBM. To obtain networks with different SNRs (0.58, 0.68, 0.79, 0.85, 0.91), we set
the intra-degree average aintra = 2.1 and inter-degree average ainter to be 0.4, 0.3, 0.2, 0.15, 0.1
respectively. We generate 10 networks for each SNR. One of the networks with SNR of 0.91 is
analyzed in Figure A.3 for different convolution operators. As predicted by Theorem 3, the proposed
operators have nontrivial spectral gap. This is not enjoyed by other operators, so it is a quite unique
property. Furthermore, our learned operator w/ or w/o normalization obtained 81% accuracy based
18
Under review as a conference paper at ICLR 2020
Adjacency A
Powered normalized
Normalized LaplaCian	LaplaCian r=5	Variable operator r=5 Normalized VPN r=5
I — DT"4DT∕2	I-Dlrv2ArDrv2	A^ in ⑶ mainteXt	D*。+ 碟)D*
luniuBdsuB∙-3
βcug=as-u一reiuBds
-0.4	-0.2	0	0.2	0.4	-0.1
--0.08 -------'-----'-----'-----'
0	0	0.02	0.04	0.06	0.08
0.1
0.05
0
-0.05
-0.1
Figure A.3: Illustration of operators for SBM, showing (top to bottom) the eigenspectrum, plots
of the first 2 leading eigenvectors, and the detected clusters. We consider (from left to right) the
adjacency matrix A, normalized Laplacian I - D-1/2AD-1/2 , powered normalized Laplacian
I - Dr-1/2ArDr-1/2 (Dr is the degree matrix of Ar), VPN A(θr) with order 5 (θk is 1 when k = 1
and 0.1 otherwise), and the normalized version DT/2(I + Aθɔ)DT/2.
Figure A.4: t-SNE visualization of network embeddings learned by r-GCN.
on the second eigenvector, whereas adjacency matrix catches “high-degree nodes” and normalized
(or powered) Laplacians catch “tails”, with spectral clustering accuracy of only 51%.
Visualization of network embeddings. We used t-SNE (Maaten & Hinton, 2008) to visualize the
network embeddings learned by r-GCN (Figure A.4) and VPN (Figure A.5) for Citeseer, Cora and
Pubmed. As can be seen, with very limited labels, the proposed methods can learn an effective
embedding of the nodes.
Evasion experiment. Five representative global adversarial attack methods are considered as com-
parisons:
•	DICE (ZUgner & Gunnemann, 2019) (remove internally, insert externally): it randomly
disconnects nodes from the same class and connects nodes from different classes.
19
Under review as a conference paper at ICLR 2020
Figure A.5: t-SNE visualization of network embeddings learned by VPN.
•	Aabr and Adw3 (BojcheVski & Gunnemann, 2019): both attack methods are matrix per-
turbation theory based. ADW3 is the closed-form attack for edge deletion and Aabr is the
add-by-remoVe strategy based method for edge insertion.
•	Meta-Train and Meta-Self (Zugner & Gunnemann, 2019): both attack approaches are meta
learning based. The meta-gradient approach with self-training is denoted as Meta-Self and
the Variant without self-training is denoted as Meta-Train. Note that both attack methods
result in OOM (out of memory) on Pubmed, we choose to report the performance against
them only on Citeseer and Cora.
We proVide the post-eVasion accuracy in Table A.2 and Table A.3 and robustness enhancement in
Figure A.6, Figure A.7 and Figure A.8 for Citeseer, Cora, and Pubmed. Results are obtained with
20 random initializations on a giVen attacked network.
Computational efficiency. The adjacency matrix could be much denser after being powered seVeral
times, which poses a general challenge for all high-order matrix-based approaches. To alleViate this
issue, we employ a simple sparsification strategy as described in the main paper. To demonstrate
the computational efficiency of the proposed method, we introduce a running time comparison ex-
periment on the real world social network (Social circles: Facebook) (LeskoVec & Mcauley, 2012).
This small dataset consists of “circles” (or “friends lists”) from Facebook and becomes Very dense
from first order (4.71%) to second order (92.12%), thus is suitable to be used for computational
efficiency eValuation. The results are shown in Table A.4. We can find that the running efficiency of
the proposed method is compatible with baselines and the density does not affect the efficiency sig-
nificantly when the dataset is not too large. MoreoVer, in this paper, our primary goal is to improVe
the robustness. We leaVe the solution to resolVe the scalability issue as a future direction.
20
Under review as a conference paper at ICLR 2020
Table A.2: Summary of evasion attack performance on DICE, Aabr and ADW3 in terms of post-
evasion accuracy (in percent). Best performance under each setting is in bold.
	Attack Method						DICE					
Model												
	Attack Rate (%)	GCN	IGCN(RNM)	IGCN(AR)	LanczosNet	RGCN	PowerLap 2	PowerLap 3	SGC	MixHop	r-GCN	VPN
	5	68.5 ± 0.7	64.1 ± 0.0	64.4 ± 0.5	60.6 ± 0.9	69.9 ± 0.7	64.8 ± 0.4	65.9 ± 0.7	68.8 ± 0.5	69.9 ± 1.0	70.5 ± 1.0	70.0 ± 1.0
	10	66.3 ± 0.1	61.6 ± 0.3	61.9 ± 0.2	57.6 ± 0.4	67.9 ± 0.5	62.5 ± 0.4	62.9 ± 0.4	67.5 ± 0.7	67.7 ± 0.7	68.6 ± 0.6	68.3 ± 0.9
	15	63.4 ± 0.2	59.3 ± 0.5	59.4 ± 0.2	55.7 ± 0.5	66.0 ± 0.7	60.4 ± 0.4	60.2 ± 1.2	65.8 ± 0.7	65.4 ± 1.1	66.2 ± 0.4	66.4 ± 0.3
Citeseer	20	62.8 ± 0.4	56.9 ± 0.5	56.9 ± 0.2	53.0 ± 0.8	65.0 ± 1.0	57.8 ± 0.5	57.5 ± 0.7	64.7 ± 0.8	64.6 ± 1.0	65.4 ± 0.8	63.9 ± 0.9
	25	60.9 ± 0.6	54.6 ± 0.3	54.7 ± 0.7	51.8 ± 0.4	63.2 ± 1.0	55.5 ± 0.4	55.3 ± 1.2	63.1 ± 0.5	63.2 ± 0.4	63.9 ± 0.0	63.2 ± 0.6
	30	57.5 ± 1.1	51.9 ± 0.3	52.1 ± 0.7	49.7 ± 0.8	60.2 ± 0.5	52.4 ± 0.3	51.3 ± 1.9	59.2 ± 0.4	63.0 ± 0.7	59.6 ± 0.4	61.4 ± 0.4
	Attack Rate (%)	GCN	IGCN(RNM)	IGCN(AR)	LanczosNet	-RGCN-	PowerLap 2	PowerLap 3	SGC	MixHop	r-GCN	VPN
	5	79.2 ± 0.3	77.2 ± 0.9	78.0 ± 0.3	73.0 ± 0.9	79.5 ± 0.4	77.5 ± 1.1	74.0 ± 0.5	78.2 ± 0.2	80.1 ± 0.2	80.6 ± 0.8	78.4 ± 0.2
	10	76.7 ± 0.7	73.7 ± 0.2	74.5 ± 0.7	69.6 ± 1.5	77.5 ± 0.5	74.6 ± 0.8	70.9 ± 0.5	76.1 ± 0.1	76.9 ± 0.1	78.6 ± 0.8	75.0 ± 0.3
	15	74.3 ± 0.2	70.7 ± 0.1	71.5 ± 1.2	66.1 ± 0.0	74.6 ± 0.2	71.4 ± 1.1	67.0 ± 0.6	73.5 ± 1.7	73.1 ± 0.2	76.3 ± 0.8	72.6 ± 0.3
Cora	20	72.5 ± 0.6	68.9 ± 0.2	69.5 ± 0.6	62.8 ± 0.7	72.9 ± 0.2	69.9 ± 0.7	64.0 ± 0.3	72.1 ± 0.3	71.6 ± 1.0	74.6 ± 0.8	71.3 ± 0.3
	25	70.2 ± 0.9	66.2 ± 0.5	67.4 ± 0.9	60.4 ± 1.3	70.9 ± 0.4	66.6 ± 1.1	59.8 ± 0.7	70.1 ± 0.3	69.5 ± 0.6	72.7 ± 0.9	69.4 ± 0.3
	30	69.9 ± 0.4	67.5 ± 0.8	68.9 ± 1.4	58.7 ± 1.1	72.2 ± 0.6	66.7 ± 0.8	58.8 ± 0.8	70.8 ± 0.6	69.7 ± 0.3	72.9 ± 0.8	69.7 ± 0.5
	Attack Rate (%)	GCN	IGCN(RNM)	IGCN(AR)	LanczosNet	-RGCN-	PowerLap 2	PowerLap 3	SGC	MixHop	r-GCN	VPN
	5	75.3 ± 0.4	74.8 ± 0.8	75.8 ± 0.2	72.9 ± 1.3	76.3 ± 0.4	74.9 ± 0.9	68.0 ± 2.7	75.6 ± 0.5	76.6 ± 0.5	76.1 ± 0.5	77.1 ± 0.6
	10	73.0 ± 0.5	72.3 ± 0.5	73.0 ± 0.2	69.9 ± 1.0	73.9 ± 0.4	72.1 ± 0.9	65.4 ± 2.7	73.1 ± 0.2	73.0 ± 0.8	73.6 ± 0.3	74.4 ± 0.1
	15	71.6 ± 0.1	68.8 ± 0.2	70.4 ± 0.2	67.4 ± 0.1	70.9 ± 0.2	69.3 ± 1.0	62.5 ± 2.0	79.4 ± 0.2	70.7 ± 0.7	70.7 ± 0.1	71.3 ± 0.9
Pubmed	20	69.2 ± 0.6	66.1 ± 0.2	67.6 ± 0.4	63.7 ± 1.8	69.2 ± 0.2	67.5 ± 0.8	60.5 ± 1.3	69.1 ± 0.7	67.0 ± 0.8	69.8 ± 0.3	70.1 ± 0.7
	25	68.2 ± 0.6	63.6 ± 0.3	65.7 ± 0.2	61.5 ± 1.6	67.8 ± 0.5	64.4 ± 0.5	58.0 ± 1.2	67.3 ± 0.2	64.5 ± 0.2	68.3 ± 0.3	68.8 ± 0.1
	30	57.5 ± 0.9	63.7 ± 0.2	65.0 ± 1.0	62.2 ± 0.3	66.3 ± 0.2	63.5 ± 0.7	56.3 ± 1.0	66.0 ± 0.5	64.5 ± 0.3	66.5 ± 0.1	67.1 ± 0.3
	Attack Method						Aabr					
Model												
	Attack Rate (%)	GCN	IGCN(RNM)	IGCN(AR)	LanczosNet	RGCN	PowerLap 2	PowerLap 3	SGC	MixHop	r-GCN	VPN
	5	69.2 ± 0.2	61.7 ± 0.1	60.4 ± 0.1	57.3 ± 0.7	70.4 ± 0.8	68.3 ± 0.2	64.1 ± 2.0	70.7 ± 0.2	70.4 ± 0.8	70.8 ± 0.5	71.6 ± 0.6
	10	67.8 ± 0.3	59.1 ± 0.3	56.3 ± 0.3	55.0 ± 0.2	70.2 ± 0.8	66.0 ± 0.2	59.0 ± 2.7	69.6 ± 0.3	69.4 ± 0.9	70.6 ± 0.0	70.8 ± 1.0
	15	66.6 ± 0.4	53.6 ± 0.4	52.1 ± 0.4	50.7 ± 0.9	70.2 ± 1.0	63.9 ± 0.2	55.3 ± 2.3	69.1 ± 0.3	68.7 ± 0.4	70.4 ± 0.4	70.6 ± 0.7
Citeseer	20	66.4 ± 0.5	49.7 ± 0.5	49.5 ± 0.5	47.4 ± 0.8	69.9 ± 0.8	62.0 ± 0.3	52.2 ± 2.2	69.5 ± 0.5	67.9 ± 1.2	70.3 ± 0.2	70.1 ± 0.3
	25	65.6 ± 0.5	45.4 ± 0.6	45.3 ± 0.6	44.0 ± 1.0	69.1 ± 1.0	59.5 ± 0.3	48.6 ± 1.5	68.6 ± 0.4	67.7 ± 2.1	70.0 ± 0.9	69.7 ± 0.4
	30	63.8 ± 0.5	41.9 ± 0.6	42.6 ± 0.6	40.5 ± 0.9	68.7 ± 1.0	57.2 ± 0.4	44.8 ± 1.4	67.4 ± 0.3	67.3 ± 0.5	69.5 ± 0.4	69.1 ± 0.6
	Attack Rate (%)	GCN	IGCN(RNM)	IGCN(AR)	LanczosNet	-RGCN-	PowerLap 2	PowerLap 3	SGC	MixHop	r-GCN	VPN
	5	81.1 ± 0.1	77.6 ± 0.1	80.5 ± 0.1	75.3 ± 1.0	81.0 ± 0.4	79.8 ± 0.4	75.2 ± 1.1	79.7 ± 0.6	81.1 ± 0.3	80.9 ± 0.9	81.5 ± 0.2
	10	80.3 ± 0.1	76.4 ± 0.2	79.3 ± 0.1	74.0 ± 1.7	80.0 ± 0.6	78.9 ± 0.7	73.0 ± 1.5	78.7 ± 0.8	80.6 ± 0.3	79.6 ± 0.8	81.1 ± 0.8
	15	79.7 ± 0.2	74.3 ± 0.4	77.7 ± 0.2	72.7 ± 1.0	80.1 ± 0.4	77.1 ± 1.2	69.9 ± 1.7	78.6 ± 0.1	79.7 ± 1.5	79.7 ± 0.1	80.0 ± 0.5
Cora	20	78.9 ± 0.2	71.2 ± 0.5	73.2 ± 0.2	70.5 ± 0.2	79.2 ± 0.7	75.9 ± 1.6	66.8 ± 2.5	78.2 ± 0.4	79.0 ± 0.4	79.1 ± 0.2	79.3 ± 0.6
	25	77.4 ± 0.3	64.9 ± 0.6	70.1 ± 0.3	68.2 ± 0.6	78.5 ± 1.1	73.2 ± 1.4	61.7 ± 2.6	77.1 ± 0.3	78.0 ± 0.3	78.5 ± 0.1	78.4 ± 0.6
	30	75.6 ± 0.6	59.2 ± 0.7	66.2 ± 0.4	65.5 ± 0.8	77.7 ± 0.8	71.0 ± 1.9	57.4 ± 2.8	76.0 ± 0.3	77.1 ± 0.6	77.3 ± 0.8	77.6 ± 1.0
	Attack Rate (%)	GCN	IGCN(RNM)	IGCN(AR)	LanczosNet	-RGCN-	PowerLap 2	PowerLap 3	SGC	MixHop	r-GCN	VPN
	5	75.3 ± 0.8	76.4 ± 0.2	75.9 ± 0.4	75.0 ± 0.2	77.6 ± 0.1	76.9 ± 0.5	67.3 ± 1.2	77.8 ± 0.8	78.2 ± 0.3	78.2 ± 0.3	78.6 ± 0.6
	10	74.3 ± 0.5	74.3 ± 0.1	73.0 ± 0.6	72.7 ± 0.8	77.0 ± 0.1	75.6 ± 0.6	64.9 ± 1.7	77.5 ± 0.9	77.2 ± 2.2	77.7 ± 0.5	77.7 ± 0.4
	15	73.5 ± 0.7	71.9 ± 0.4	69.4 ± 0.0	70.9 ± 0.6	76.1 ± 0.3	74.1 ± 0.3	61.9 ± 1.8	76.5 ± 0.4	76.5 ± 0.5	76.9 ± 0.4	77.3 ± 0.8
Pubmed	20	73.7 ± 0.1	68.7 ± 0.7	66.2 ± 0.0	69.1 ± 0.8	76.5 ± 0.1	73.7 ± 0.3	59.4 ± 1.1	76.3 ± 1.1	77.3 ± 1.2	77.3 ± 0.4	77.7 ± 0.1
	25	72.9 ± 0.1	64.9 ± 0.2	63.2 ± 0.5	67.6 ± 0.9	75.8 ± 0.3	73.1 ± 0.5	58.0 ± 0.6	75.5 ± 0.4	75.7 ± 0.4	76.7 ± 0.7	77.2 ± 0.3
	30	72.8 ± 0.9	63.1 ± 0.6	61.1 ± 0.6	65.8 ± 1.2	75.9 ± 0.2	72.4 ± 0.6	55.4 ± 0.6	76.2 ± 0.3	75.9 ± 0.5	77.2 ± 0.6	76.7 ± 0.5
	Attack Method						ADW3					
Model												
	Attack Rate (%)	GCN	IGCN(RNM)	IGCN(AR)	LanczosNet	RGCN	PowerLap 2	PowerLap 3	SGC	MixHop	r-GCN	VPN
	5	68.8 ± 0.1	63.5 ± 0.4	65.1 ± 0.2	60.0 ± 0.5	70.3 ± 0.6	69.2 ± 1.0	70.0 ± 0.8	70.5 ± 0.5	70.4 ± 1.0	71.8 ± 0.3	70.6 ± 0.6
	10	68.4 ± 0.2	63.1 ± 0.6	63.6 ± 0.5	59.6 ± 0.6	70.1 ± 0.6	69.3 ± 1.1	69.6 ± 0.6	70.1 ± 0.8	69.4 ± 0.4	71.2 ± 0.7	70.2 ± 0.7
	15	68.9 ± 0.1	62.9 ± 0.7	63.2 ± 0.8	59.5 ± 0.9	70.3 ± 0.4	69.5 ± 1.1	69.8 ± 0.7	70.6 ± 0.8	68.7 ± 1.4	71.2 ± 0.1	70.6 ± 0.4
Citeseer	20	68.8 ± 0.5	63.5 ± 0.8	63.4 ± 0.7	59.1 ± 0.8	69.8 ± 0.4	69.4 ± 1.0	69.5 ± 0.7	70.3 ± 0.2	67.9 ± 1.4	71.1 ± 0.2	70.4 ± 0.7
	25	68.8 ± 0.4	63.6 ± 0.7	63.8 ± 0.6	59.3 ± 0.4	69.9 ± 0.5	69.2 ± 0.9	69.3 ± 0.9	70.2 ± 0.5	67.7 ± 0.8	71.2 ± 0.1	70.1 ± 0.2
	30	68.8 ± 0.4	63.6 ± 0.7	63.8 ± 0.6	59.3 ± 0.1	69.9 ± 0.5	69.2 ± 0.9	69.3 ± 0.9	70.2 ± 0.7	67.3 ± 0.1	71.2 ± 0.0	70.1 ± 0.9
	Attack Rate (%)	GCN	IGCN(RNM)	IGCN(AR)	LanczosNet	-RGCN-	PowerLap 2	PowerLap 3	SGC	MixHop	r-GCN	VPN
	5	80.0 ± 0.7	78.9 ± 0.2	79.2 ± 0.4	78.1 ± 0.8	80.5 ± 0.9	77.8 ± 0.9	77.0 ± 1.4	78.3 ± 0.3	80.3 ± 0.3	80.8 ± 0.7	80.8 ± 0.5
	10	79.7 ± 0.9	78.7 ± 0.2	78.9 ± 0.4	77.8 ± 1.0	80.1 ± 0.6	78.1 ± 1.1	77.8 ± 1.1	78.0 ± 0.9	80.1 ± 0.2	80.7 ± 0.8	80.5 ± 0.3
	15	79.3 ± 0.7	77.1 ± 0.2	77.7 ± 0.4	76.8 ± 1.2	79.7 ± 0.5	77.3 ± 1.2	77.9 ± 0.6	77.8 ± 0.5	80.0 ± 0.7	80.3 ± 0.9	80.6 ± 0.9
Cora	20	78.3 ± 1.0	75.5 ± 0.2	75.6 ± 0.3	75.7 ± 0.3	79.3 ± 0.6	76.9 ± 1.2	77.1 ± 0.5	78.0 ± 0.2	79.8 ± 1.0	80.3 ± 0.5	80.3 ± 0.2
	25	77.8 ± 0.6	75.3 ± 0.3	74.4 ± 0.7	74.6 ± 1.8	78.7 ± 0.5	76.1 ± 1.0	76.8 ± 0.2	77.3 ± 0.2	79.1 ± 0.7	79.2 ± 0.0	79.1 ± 0.5
	30	77.8 ± 0.8	75.0 ± 0.2	73.8 ± 0.7	74.3 ± 1.7	78.0 ± 0.5	75.6 ± 1.1	75.9 ± 0.6	76.7 ± 0.6	78.8 ± 0.4	79.3 ± 0.9	78.6 ± 1.0
	Attack Rate (%)	GCN	IGCN(RNM)	IGCN(AR)	LanczosNet	-RGCN-	PowerLap 2	PowerLap 3	SGC	MixHop	r-GCN	VPN
	5	78.3 ± 0.5	79.3 ± 0.4	79.2 ± 0.1	76.5 ± 1.8	77.8 ± 0.1	74.8 ± 1.7	76.5 ± 0.5	78.0 ± 0.4	78.2 ± 0.5	78.1 ± 0.7	79.0 ± 1.0
	10	77.8 ± 0.2	78.6 ± 0.2	78.6 ± 0.7	75.3 ± 1.0	77.9 ± 0.3	74.7 ± 1.9	77.4 ± 0.4	77.8 ± 1.0	78.0 ± 0.7	77.9 ± 0.9	78.6 ± 0.1
	15	77.4 ± 0.4	77.0 ± 0.4	77.2 ± 0.3	74.7 ± 0.3	77.5 ± 0.3	74.2 ± 1.5	76.9 ± 0.4	77.3 ± 0.4	77.3 ± 0.8	77.8 ± 0.3	78.3 ± 0.2
Pubmed	20	77.4 ± 0.4	75.8 ± 0.3	76.3 ± 0.4	73.5 ± 1.8	77.8 ± 0.4	73.8 ± 1.0	76.3 ± 0.4	77.3 ± 1.2	77.1 ± 0.3	77.6 ± 1.0	78.3 ± 0.4
	25	77.2 ± 0.5	75.2 ± 0.1	75.9 ± 0.4	73.7 ± 1.1	77.6 ± 0.3	73.7 ± 1.1	75.6 ± 0.7	77.1 ± 0.2	77.0 ± 0.3	78.1 ± 0.8	78.2 ± 0.2
	30	77.1 ± 0.2	74.5 ± 0.3	74.6 ± 0.3	73.4 ± 0.3	77.0 ± 0.2	72.9 ± 1.4	74.6 ± 0.4	77.1 ± 0.3	77.2 ± 0.8	77.6 ± 0.3	77.7 ± 0.9
21
Under review as a conference paper at ICLR 2020
Table A.3: Summary of evasion attack performance on Meta-Train and Meta-Self in terms of post-
evasion accuracy (in percent). OOM (out of memory) for both methods on Pubmed.
Attack Method							Meta-Train					
Model												
	Attack Rate (%)	GCN	IGCN(RNM)	IGCN(AR)	LanczosNet	RGCN	PowerLap 2	PowerLap 3	SGC	MixHop	r-GCN	VPN
	5	67.6 ± 0.3	62.8 ± 0.8	63.1 ± 1.2	57.7 ± 0.7	69.3 ± 0.5	66.8 ± 1.1	64.9 ± 1.5	70.0 ± 0.3	69.6 ± 0.6	70.4 ± 0.7	70.4 ± 0.6
	10	66.0 ± 1.0	61.0 ± 1.1	61.4 ± 1.0	56.4 ± 0.3	68.3 ± 0.7	65.2 ± 0.9	61.9 ± 1.4	68.3 ± 0.2	68.9 ± 1.0	68.7 ± 0.4	69.4 ± 1.0
	15	66.1 ± 0.6	60.9 ± 1.1	61.4 ± 1.0	55.4 ± 0.8	67.3 ± 0.4	64.5 ± 1.3	60.9 ± 1.9	67.3 ± 0.8	67.8 ± 0.8	67.9 ± 0.6	68.6 ± 0.3
Citeseer	20	64.9 ± 0.9	59.1 ± 1.4	59.7 ± 0.8	54.5 ± 0.1	66.6 ± 0.6	63.0 ± 1.2	58.2 ± 2.1	67.2 ± 0.5	67.1 ± 0.9	67.6 ± 0.4	68.5 ± 0.8
	25	64.3 ± 0.7	57.9 ± 1.8	59.8 ± 1.0	54.0 ± 0.4	66.3 ± 0.7	62.7 ± 1.1	55.8 ± 2.6	66.1 ± 0.5	66.5 ± 1.7	67.0 ± 0.5	67.0 ± 0.2
	30	64.8 ± 1.1	59.8 ± 1.0	59.0 ± 0.8	53.5 ± 0.8	65.2 ± 0.4	61.8 ± 0.8	56.1 ± 1.9	65.0 ± 0.8	65.5 ± 0.7	66.0 ± 0.8	66.1 ± 0.1
	Attack Rate (%)	GCN	IGCN(RNM)	IGCN(AR)	LanczosNet	-RGCN-	PowerLap 2	PowerLap 3	SGC	MixHop	r-GCN	VPN
	5	79.8 ± 0.9	76.5 ± 0.1	77.4 ± 0.3	75.1 ± 1.1	80.2 ± 0.7	75.2 ± 1.2	75.1 ± 1.1	79.5 ± 0.5	80.5 ± 0.8	79.8 ± 0.6	80.8 ± 0.2
	10	78.4 ± 0.7	75.8 ± 0.4	76.6 ± 0.3	73.3 ± 1.7	79.0 ± 0.5	74.2 ± 1.2	72.9 ± 1.0	78.1 ± 0.7	79.0 ± 0.1	78.5 ± 0.9	79.2 ± 0.1
	15	77.9 ± 0.6	73.8 ± 0.9	76.7 ± 0.6	71.8 ± 1.1	79.0 ± 0.7	73.4 ± 1.7	71.0 ± 1.1	77.9 ± 1.8	78.3 ± 0.6	78.2 ± 0.6	79.7 ± 0.1
Cora	20	76.0 ± 1.4	71.8 ± 1.0	73.8 ± 0.3	70.2 ± 1.9	77.1 ±0.5	71.0 ± 1.5	68.4 ± 1.0	76.3 ± 1.0	77.2 ± 0.2	76.6 ± 0.9	77.6 ± 1.0
	25	79.1 ± 0.6	77.7 ± 0.6	78.7 ± 0.6	74.3 ± 0.7	79.7 ± 0.6	74.3 ± 1.6	73.8 ± 1.0	78.0 ± 0.4	77.7 ± 1.0	78.9 ± 1.0	79.8 ± 0.9
	30	77.7 ± 0.9	75.6 ± 0.7	76.7 ± 0.6	73.8 ± 0.7	78.6 ± 0.4	73.5 ± 1.6	73.4 ± 0.7	78.3 ± 0.4	78.4 ± 0.9	78.6 ± 0.9	79.0 ± 0.0
Attack Method							Meta-Self					
Model												
	Attack Rate (%)	GCN	IGCN(RNM)	IGCN(AR)	LanczosNet	RGCN	PowerLap 2	PowerLap 3	SGC	MixHop	r-GCN	VPN
	5	66.2 ± 0.9	61.8 ± 0.1	60.9 ± 0.8	60.2 ± 0.2	69.1 ± 0.4	66.3 ± 0.9	66.0 ± 1.2	69.1 ± 0.3	69.6 ± 0.4	70.1 ± 0.7	69.1 ± 0.8
	10	64.7 ± 0.8	59.3 ± 0.4	59.1 ± 0.9	57.3 ± 0.1	67.3 ± 0.6	64.4 ± 0.8	62.5 ± 1.3	67.9 ± 0.7	67.1 ± 0.5	68.0 ± 0.0	68.2 ± 0.7
	15	64.0 ± 0.9	57.2 ± 0.3	58.1 ± 0.7	55.2 ± 0.2	65.9 ± 0.9	62.9 ± 1.7	60.6 ± 0.7	66.3 ± 0.9	66.2 ± 0.4	67.3 ± 0.9	66.2 ± 0.1
Citeseer	20	62.4 ± 1.2	55.5 ± 0.2	56.8 ± 0.8	55.9 ± 0.4	65.3 ± 1.2	61.2 ± 1.5	58.6 ± 1.9	65.3 ± 1.0	64.9 ± 0.1	66.3 ± 0.4	65.6 ± 0.4
	25	62.7 ± 1.0	56.6 ± 0.7	58.4 ± 1.1	55.3 ± 0.3	65.1 ± 0.7	60.5 ± 1.6	57.6 ± 1.9	65.6 ± 0.8	65.0 ± 0.6	65.9 ± 0.5	65.5 ± 0.5
	30	61.1 ± 0.9	55.9 ± 0.5	56.4 ± 0.7	54.9 ± 0.1	64.7 ± 0.3	60.3 ± 1.5	57.5 ± 1.4	64.1 ± 0.8	64.5 ± 1.2	66.0 ± 1.0	64.5 ± 0.9
	Attack Rate (%)	GCN	IGCN(RNM)	IGCN(AR)	LanczosNet	-RGCN-	PowerLap 2	PowerLap 3	SGC	MixHop	r-GCN	VPN
	5	78.7 ± 0.2	76.1 ± 0.3	75.0 ± 0.4	73.4 ± 0.5	79.6 ± 0.6	77.9 ± 1.4	74.7 ± 0.7	77.9 ± 0.6	79.0 ± 0.8	79.3 ± 0.8	79.1 ± 0.5
	10	76.8 ± 0.3	76.0 ± 0.5	74.3 ± 0.7	71.0 ± 1.8	78.4 ± 0.3	76.0 ± 0.9	72.6 ± 0.6	76.8 ± 0.2	77.2 ± 0.1	77.8 ± 0.9	78.0 ± 0.7
	15	75.3 ± 0.9	74.0 ± 0.5	72.1 ±0.4	69.4 ± 0.0	77.1 ± 0.7	74.2 ± 1.4	69.5 ± 1.2	76.6 ± 0.4	77.0 ± 0.6	77.0 ± 0.7	77.5 ± 0.9
Cora	20	75.8 ± 0.7	72.6 ± 0.7	71.1 ± 0.8	69.5 ± 1.4	76.3 ± 0.5	73.3 ± 1.2	70.4 ± 1.4	75.9 ± 0.3	76.3 ± 0.4	76.1 ± 0.9	76.4 ± 0.5
	25	77.5 ± 0.2	73.8 ± 0.2	73.4 ± 0.6	70.3 ± 0.2	78.4 ± 0.4	74.2 ± 1.0	70.9 ± 1.0	76.9 ± 0.3	77.6 ± 0.2	77.9 ± 0.2	77.8 ± 0.1
	30	76.7 ± 0.6	73.7 ± 0.6	72.2 ± 1.0	66.7 ± 0.0	78.4 ± 0.5	72.6 ± 1.2	67.0 ± 1.4	77.9 ± 0.2	78.0 ± 0.7	78.1 ± 0.5	78.5 ± 0.4
Table A.4: Running time comparison on a social network dataset (Leskovec & Mcauley, 2012).
Methods Vanilla GCN PowerLaplacian IGCN(RNM) IGCN(AR) LNet RGCN SGC MixHop r-GCN VPN
Running Time (sec) 6.02	6.36	3.33	7.21	5.56	18.14	0.231	11.38	6.73	7.18
22
Under review as a conference paper at ICLR 2020
CiteSeer attacked by Aabr
■ IGCN(RNM) ■ IGCN(AR) ■ LanczosNet ■ PowerLap 2 ■ PowerLap 3 ■ SGC ■ MixHop ■ RGCN ■ r-GCN ■ VPN
SS ① UiSnqO X
Attack rate (%)
4-4
(％) IU①UJ①。UEL{U①
SS①UISnqOX
10
CiteSeer attacked by DICE
■ IGCN(RNM) BIGCN(AR) ■ LanczosNet ■ PowerLap 2 ■ PowerLap 3 ∙SGC ■ MixHop ■ RGCN ■ r-GCN BVPN
15	20
Attack rate (%)
25	30
CiteSeer attacked by Meta-Self
■ IGCN(RNM) ■ IGCN(AR) ■ LanczosNet ■ PowerLap 2 ■ PowerLap 3 ∙SGC ■ MixHop ■ RGCN ■ r-GCN ■ VPN
6 O-6
(％) IU①UJ①。UELJUB
SS①UISnqOX
10
15	20
Attack rate (%)
25
30
CiteSeer attacked by Meta-Train
■ IGCN(RNM) ■ IGCN(AR) -LanczosNet ■ PowerLap 2 ■ PowerLap 3 BSGC ■ MixHop BRGCN ■ r-GCN BVPN
6 。-6
(％) IU①UJ①。UELJU①
SS①UISnqOX
Figure A.6: Illustration of evasion attack performance in terms of robustness enhancement on dif-
ferent attack methods (from top to bottom) for Citeseer: Aabr (BojcheVski & Gunnemann, 2019),
DICE (ZUgner & Gunnemann, 2019), Meta-Self and Meta-Train (ZUgner & Gunnemann, 2019).
EValuated methods include IGCN (Li et al., 2019), LanczosNet (Liao et al., 2019), SGC (Wu et al.,
2019), MixHop (Abu-El-Haija et al., 2019) and RGCN (Zhu et al., 2019). PowerLap 2 and 3 are
methods that replace the adjacency matrix in GCN by its powered Versions with orders 2 and 3,
respectiVely. PositiVe Values indicate improVement of robustness compared to Vanilla GCN and
negatiVe ones indicate deterioration.
23
Under review as a conference paper at ICLR 2020
Cora attacked by Aabr
(≡s) juəluə:ɔuɪuə
SsoUjSnqOX
5	10	15	20	25	30
Attack rate (%)
(≡s) juəluə:ɔuɪuə
SsoUjSnqOX
Cora attacked by ADW3
■ IGCN(RNM) HIGCN(AR) -LanczosNet ■ PowerLap 2 ■ PowerLap 3 BSGC HMixHop HRGCN Hr-GCN BVPN
3 --------------------------------------------------------------------------------------------
Cora attacked by DICE
SSoUlSnqoH
Attack rate (%)
Cora attacked by Meta-Self
■ IGCN(RNM) ■ IGCN(AR) ■ LanCZosNet ■ PoWerLap 2 ■ PoWerLap 3 ■ SGC ■ MixHop ■ RGCN ■ r-GCN ■ VPN
O-f'
()JU9UJ9uu9
SsoUjSnqoX
15	20
Attack rate (%)
Cora attacked by Meta-Train
■ IGCN(RNM) ■ IGCN(AR) ■ LanCZosNet ■ PoWerLap 2 ■ PoWerLap 3 ∙SGC ■ MixHop ■ RGCN ■ r-GCN ■ VPN
6 -------------------------------------------------------------------------------------------
(≡s)JU9UJ9uu"u9
SsoUjSnqOX
-12 ----------------------------------------------------------------------------------------------
5	10	15	20	25	30
Attack rate (%)
Figure A.7: Illustration of evasion attack performance in terms of robustness enhancement on dif-
ferent attack methods for Cora.
24
Under review as a conference paper at ICLR 2020
Robustness	Robustness
enhancement (%)	enhancement (%)
Pubmed attacked by Aabr
Attack rate (%)
3
-2
-7
-12
Pubmed attacked by DICE
Attack rate (%)
Figure A.8: Illustration of evasion attack performance in terms of robustness enhancement on dif-
ferent attack methods for Pubmed.
25