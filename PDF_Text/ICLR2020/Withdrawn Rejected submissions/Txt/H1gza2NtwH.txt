Under review as a conference paper at ICLR 2020

TOWARDS  UNDERSTANDING  THE  TRUE  LOSS  SURFACE
OF DEEP NEURAL NETWORKS USING RANDOM MATRIX
THEORY  AND  ITERATIVE  SPECTRAL  METHODS

Anonymous authors

Paper under double-blind review

ABSTRACT

The geometric properties of loss surfaces, such as the local flatness of a solution,
are associated with generalization in deep learning. The Hessian is often used to
understand these geometric properties. We investigate the differences between the
eigenvalues of the neural network Hessian evaluated over the empirical dataset, the
Empirical Hessian, and the eigenvalues of the Hessian under the data generating
distribution, which we term the True Hessian.  Under mild assumptions, we use
random matrix theory to show that the True Hessian has eigenvalues of smaller
absolute value than the Empirical Hessian. We support these results for different
SGD schedules on both a 110-Layer ResNet and VGG-16.   To perform these
experiments we propose a framework for spectral visualization, based on GPU
accelerated stochastic Lanczos quadrature. This approach is an order of magnitude
faster than state-of-the-art methods for spectral visualization, and can be generically
used to investigate the spectral properties of matrices in deep learning.

1    INTRODUCTION

The extraordinary success of deep learning in computer vision and natural language processing has
been accompanied by an explosion of theoretical (Choromanska et al., 2015a;b; Pennington & Bahri,
2017) and empirical interest in their loss surfaces, typically through the study of the Hessian and 
its
eigenspectrum (Ghorbani et al., 2019; Li et al., 2017; Sagun et al., 2016; 2017; Wu et al., 2017).

Exploratory work on the Hessian, and its evolution during training (e.g., Jastrze˛bski et al., 
2018),
attempts to understand why optimization procedures such as SGD can discover good solutions for
training neural networks, given complex non-convex loss surfaces.  For example, the ratio of the
largest to smallest eigenvalues, known as the condition number, determines the convergence rate for
first-order optimization methods on convex objectives (Nesterov, 2013). The presence of negative
eigenvalues indicates non-convexity even at a local scale. Hessian analysis has also been a primary
tool in further explaining the difference in generalization of solutions obtained, where under 
Bayesian
complexity frameworks, flatter minima, which require less information to store, generalize better 
than
sharp minima (Hochreiter & Schmidhuber, 1997). Further work has considered how large batch vs
small batch stochastic gradient descent (SGD) alters the sharpness of solutions (Keskar et al., 
2016),
with smaller batches leading to convergence to flatter solutions, leading to better generalization.
These geometrical insights have led to generalization procedures, such as taking the Cesàro mean of
the weights along the SGD trajectory (Garipov et al., 2018; Izmailov et al., 2018), and algorithms
that optimize the model to select for local flatness (Chaudhari et al., 2016). Flat regions of 
weight
space are more robust under adversarial attack (Yao et al., 2018). Moreover, the Hessian defines the
curvature of the posterior over weights in the Laplace approximation for Bayesian neural networks
(MacKay, 1992; 2003), and thus crucially determines its performance.

In this paper we use random matrix theory to analyze the spectral differences between the Empirical
Hessian, evaluated via a finite data sample (hence related to the empirical risk) and what we term 
the
True Hessian, given under the expectation of the true data generating distribution.¹

¹We consider loss surfaces that correspond to risk surfaces in statistical learning theory 
terminology.

1


Under review as a conference paper at ICLR 2020

In particular, we show that the differences in extremal eigenvalues between the True Hessian and
the Empirical Hessian depend on the ratio of model parameters to dataset size and the variance per
element of the Hessian. Moreover, we show that that the Empirical Hessian spectrum, relative to that
of  the True Hessian, is broadened; i.e. the largest eigenvalues are larger and the smallest 
smaller. We
support this theory with experiments on the CIFAR-10 and CIFAR-100 datasets for different learning
rate schedules using a large modern neural network, the 110 Layer PreResNet.

It is not currently known if key results, such as (1) the flatness or sharpness of good and bad 
optima,

(2) local non-convexity at the end of training, or (3) rank degeneracy hold for the True Hessian in
the same way as for the Empirical Hessian. We hence provide an investigation of these foundational
questions.

2    RELATED  WORK, CONTRIBUTIONS  AND  PAPER  STRUCTURE

Previous work has used random matrix theory to study the spectra of neural network Hessians under
assumptions such as normality of inputs and weights, to show results such as the decreasing 
difference
in loss value between the local and global minima (Choromanska et al., 2015a) and the fraction of
negative eigenvalues at final points in training (Pennington & Bahri, 2017). To the authors 
knowledge
no theoretical work has been done on the nature of the difference in spectra between the True 
Hessian
and Empirical Hessian in machine learning. Previous empirical work on neural network loss surfaces
(Ghorbani et al., 2019; Papyan, 2018; Sagun et al., 2017; 2016; Jastrze˛bski et al., 2018) has also
exclusively focused on the Empirical Hessian or a sub-sample thereof.

The work closest in spirit to our work is the spiked covariance literature, which studies the 
problem
of learning the true covariance matrix given by the generating distribution, from the noisy sample
covariance matrix. This problem is studied in mathematics and physics (Baik & Silverstein, 2004;
Bloemendal et al., 2016b;a) with applications leading to extensively improved results in both sparse
Principal Component Analysis (Johnstone & Lu, 2004), portfolio theory (Laloux et al., 1999) and
Bayesian covariance matrix spectrum estimation (Everson & Roberts, 2000). A key concept from
this literature, is the ratio of parameters to data samples and its effect on the observed 
spectrum; this
ratio is mentioned in (Pennington & Bahri, 2017), but it is not used to determine the perturbation
between the Empirical Hessian and True Hessian. (Kunstner et al., 2019) consider circumstances
under which the Empirical Fisher information matrix is a bad proxy to the True Fisher matrix, but do
not use a random matrix theory to characterise the spectral perturbations between the two. We list
our main contributions below.

We introduce the concept of the True Hessian, discuss its importance and investigate it both
theoretically and empirically.

We use random matrix theory to analytically derive the eigenvalue perturbations between
the True Hessian and the Empirical Hessian, showing the spectrum of the Empirical Hessian
to be broadened.

We visualize the True Hessian spectrum by combining a GPU accelerated stochastic Lanc-
zos quadrature (Gardner et al., 2018) with data-augmentation. Our spectral visualization
technique is an order of magnitude faster than recent iterative methods (Ghorbani et al.,
2019; Papyan, 2018), requires one less hand-tuned hyper-parameter, and is consistent with
the observed moment information. This procedure can be generically used to compute the
spectra of large matrices (e.g., Hessians, Fisher information matrices) in deep learning.

3    PROBLEM  FORMULATION

For an input x     Rdx  and output y     Rdy  we have a given prediction function h( ;  ) : Rdx     
   RP
Rdy ,  we  consider  the  family  of  prediction  functions  parameterised  by  a  weight  vector  
w,  i.e.,

:=   h( ; w) : w     RP    with a given loss l(h(x; w), y) : Rdx         Rdy            R.  Ideally 
we would
vary w such that we minimize the loss over our data generating distribution ψ(x, y), known as the
true risk.


Rtruₑ(w) = ∫

Rdx ×Rdy

l(h(x; w), y)dψ(x, y) = E[l(h(x; w), y)]                     (1)

2


Under review as a conference paper at ICLR 2020

with corresponding gradient gtruₑ(w) =     Rtruₑ(w) and Hessian Htruₑ(w) =        Rtruₑ(w). If
the loss is a negative log likelihood, then the true risk is the expected negative log likelihood 
per data
point under the data generating distribution. However, given a dataset of size N , we only have 
access
to the empirical or full risk

R     (w) =   1  Σ l(h(x ; w), y )                                              (2)

and the gradients gₑmp(w) and Hessians Hₑmp(w) thereof. The Hessian describes the curvature at
that point in weight space w and hence the risk surface can be studied through the Hessian. Weight
vectors which achieve low values of equation 2 do not necessarily achieve low values of equation 1.
Their difference is known as the generalization gap. We rewrite our Empirical Hessian as

Hₑmp(w) = Htruₑ(w) + ε(w)                                                (3)

where² ε(w)     Hₑmp(w)     Htruₑ(w). A symmetric matrix with independent normally distributed
elements of equal variance is known as the Gaussian Orthogonal Ensemble (GOE) and a matrix with
independent non-Gaussian elements of equal variance is known as the Wigner ensemble.

3.1    FLATNESS AND GENERALIZATION WITH THE EMPIRICAL HESSIAN

By the spectral theorem, we can rewrite Hₑmp in terms of its eigenvalue, eigenvector pairs, [λi, 
φi]:

P

T

i

i=1

The magnitude of the eigenvalues represent the magnitude of the curvature in the direction of the
corresponding eigenvectors. Often the magnitude of the largest eigenvalue λ₁, or the Frobenius norm

(given by the square root of the sum of all the eigenvalues squared (ΣP     λ²)¹/²), is used to 
define

the sharpness of an optimum (e.g., Jastrze˛bski et al., 2018; 2017). The normalized mean value of 
the

cases a larger value indicates greater sharpness. Other definitions of flatness have looked at 
counting
the number of 0 (Sagun et al., 2016), or close to 0 (Chaudhari et al., 2016), eigenvalues.

It is often argued that flat solutions provide better generalization because they are more robust to
shifts between the train and test loss surfaces (e.g., Keskar et al., 2016) that exist because of 
statistical
differences between the training and test samples. It is then compelling to understand whether flat
solutions associated with the True Hessian would also provide better generalization, since the True
Hessian is formed from the full data generating distribution.  If so, there may be other important
reasons why flat solutions provide better generalization.

Reparametrization can give the appearance of flatness or sharpness from the perspective of the
Hessian.  However,  it is common practice to hold the parametrization of the model fixed  when
comparing the Hessian at different parameter settings, or evaluated over different datasets.

4    ANALYZING  THE  TRUE  RISK  SURFACE  USING  RANDOM  MATRIX  THEORY

We show that the elementwise difference between the True and Empirical Hessian converges to a
zero mean normal random variable, assuming a Lipshitz bounded gradient. Moreover, we derive a
precise analytic relationship between the values of the extremal values for both the Empirical and
True Hessians, showing the eigenvalues for the Empirical Hessian to be larger in magnitude. This
result indicates that the true risk surface is flatter than its empirical counterpart.

4.1    REGULARITY CONDITIONS

We establish some weak regularity conditions on the loss function and data, under which the elements
of ε(w) converge to zero mean normal random variables.

²For a fixed dataset, the perturbing matrix ε(w) can be seen as a fixed instance of a random 
variable.

3


Under review as a conference paper at ICLR 2020

Lemma 1.  For an L-Lipshitz-continuous-gradient and almost everywhere twice differentia√ble loss

function√l(h(x; w), y), the True Hessian elements are strictly bounded in the range −   PL  ≤


Hj,k  ≤

PL.

Proof.  By the fundamental theorem of calculus and the definition of Lipshitz continuity λmₐₓ ≤ L


Tr(H²) = Σ λ² =

Σ  H2

= H²    '

'  +     Σ     H²


i

i=1

P

j,k

j,k=1

j=j ,k=k

j,k

j/=j',k   k'

(5)


H²    '

'  ≤ Σ λ² ≤ PL²


j=j ,k=k

√

i

i=1

−   PL ≤ Hj₌j',k=k'  ≤    PL

Lemma 2.  For unbiased independent samples drawn from the data generating distribution and
an L-Lipshitz loss l the difference between the True Hessian and Empirical Hessian converges
element-wise to a zero mean, normal random variable with variance ∝ 1/N .

Proof.  The difference between the Empirical and True Hessian ε(w) is given as

2                                                           N               2


[∇∇R

(w) − ∇∇R

(w)]

= E      ∂      l(h(x; w), y) −  1  Σ      ∂      l(h(x ; w), y )


true

emp         jk

∂wj∂wk

N

i=1

∂wj∂wk

i                i

(6)

By Lemma 1, the Hessian elements are bounded, hence the moments are bounded and using indepen-
dence and the central limit theorem, equation 6 converges almost surely to a normal random variable
P(µjk, σ²  /N ).

Remark.  For finite P and N         , i.e. q = P/N      0,  ε(w)        0 we recover the True 
Hessian.
Similarly in this limit our empirical risk converges almost surely to our true risk, i.e. we 
eliminate the
generalization gap. However, in deep learning typically the network size eclipses the dataset size 
by
orders        of magnitude.³

4.2    A RANDOM MATRIX THEORY APPROACH

In  order  to  derive  analytic  results,  we  move  to  the  large  Dimension  limit,  where  P, N

but P/N  =  q > 0 and employ the machinery of random matrix theory to derive results for the
perturbations on the eigenspectrum between the True Hessian and Empirical Hessian. This differs
from the classical statistical regime where q     0. We primarily focus on the regime when q     1.
In order to make the analysis tractable, we introduce two further assumptions on the nature of the
elements ε(w).

Assumption  1.  The  elements  of  ε(w)  are  identically  and  independently  Gaussian  
distributed

N (0, σ²) up to the Hermitian condition.

Assumption 2.  Htruₑ is of low rank r   P .

Under assumption 1, ε(w) becomes a Gaussian Orthogonal Ensemble (GOE) and for the GOE we
can prove the following Lemma 3. We discuss the necessity of the assumptions in Section 4.3.

Lemma 3.  The extremal eigenvalues [λ′1, λ′P ] of the matrix sum A + B/√P , where A ∈ RP ×P  is

a matrix of finite rank r with extremal eigenvalues [λ₁, λP ] and B     RP ×P  is a GOE matrix with

element variance σ² are given by


λ′1  =

.  λ1        σ

₁ > σs

Σ , λ′P  =

.  λP       σ

n < −σs  Σ

(7)


2σs,          otherwise

−2σs,           otherwise

³CIFAR datasets, which have 50, 000 examples, are routinely used to train networks with about 50 
million
parameters.

4


Under review as a conference paper at ICLR 2020

Proof.  See Appendix.

Theorem 1.  The extremal eigenvalues [λ′1, λ′P ] of the matrix sum Hₑmp, where λ′1         λ′2...   
  λ′P
(such that assumptions 1 and 2 are satisfied), where Htruₑ has extremal eigenvalues [λ₁, λP ], are
given by


  λ

+  P σ2 ,   if λ

 

> . P σ                  λ   +  P  σ2  ,   if λ   < −. P σ   

	 			

 

1          2. P σ ,             otherwise        P         −2. P σ ,               otherwise   

Proof.  The result follows directly from Lemmas 2 and 3.

Remark.  This result shows that the extremal eigenvalues of the Empirical Hessian are larger in
magnitude than those of the True Hessian:  Although we only state this result for the extremal
eigenvalues, it holds for any number of well separated outlier eigenvalues. This spectral broadening
effect has already been observed empirically when moving from (larger) training to the (smaller)
test set (Papyan, 2018). Intuitively variance of the Empirical Hessian eigenvalues can be seen as 
the
variance of the True Hessian eigenvalues plus the variance of the Hessian, as we show in Appendix 
F.

We also note that if the condition  λi  >    P/Nσs is not met, the value of the positive and 
negative
extremal eigenvalues are completely determined by the noise matrix.

4.3    NOTE ON GENERALIZING ASSUMPTIONS

For Assumption 1, we note that the results for the Wigner ensemble (of which the GOE is a special
case) can be extended to non-identical element variances (Tao, 2012) and element dependence (Götze
et al., 2012; Schenker & Schulz-Baldes, 2005). Hence, similarly to Pennington & Bahri (2017), under
extended technical conditions we expect our results to hold more generally. Assumption 2 can be
relaxed if the number of extremal eigenvalues is much smaller than P , and the bulk of Htruₑ(w)’s
eigenspectra also follows a Wigner ensemble which is mutually free with that of ε(w). Furthermore,
corrections to our results for finite P scale as P −¹/⁴ for matrices with finite 4’th moments and P 
−²/⁵

for all finite moments (Bai, 2008).

5    EFFICIENT  COMPUTATION  OF  HESSIAN  EIGENVALUES

In order to perform spectral analysis on the Hessian of typical neural networks, with tens of 
millions
of parameters, we avoid the infeasible    (P ³) eigen-decomposition and use the stochastic Lanczos
quadrature (SLQ) algorithm, in conjunction with GPU acceleration. Our procedure in this section is a
general-purpose approach for efficiently computing the spectra of large matrices in deep learning.

5.1    STOCHASTIC LANCZOS QUADRATURE

The Lanczos algorithm (Meurant & Strakoš, 2006) is a power iteration algorithm variant which by
enforcing orthogonality and storing the Krylov subspace,    m₊₁(H, v) =   v, Hv, .., Hᵐv  , opti-
mally approximates the extremal and interior eigenvalues (known as Ritz values). It requires Hessian
vector products, for which we use the Pearlmutter trick (Pearlmutter, 1994) with computational cost
(NP ), where N is the dataset size and P is the number of parameters.  Hence for m steps the
total computational complexity including re-orthogonalisation is     (NPm) and memory cost of
(Pm).  In order to obtain accurate spectral density estimates we re-orthogonalise at every step
(Meurant & Strakoš, 2006). We exploit the relationship between the Lanczos method and Gaussian
quadrature, using random vectors to allow us to learn a discrete approximation of the spectral 
density.

A quadrature rule is a relation of the form,


b

f (λ)dµ(λ) =

a

Σj=1

ρjf (tj) + R[f ]                                            (9)

for a function f , such that its Riemann-Stieltjes integral and all the moments exist on the measure
dµ(λ), on the interval [a, b] and where R[f ] denotes the unknown remainder.  The nodes tj of the
Gauss quadrature rule are given by the Ritz values and the weights (or mass) ρj by the squares of 
the

5


Under review as a conference paper at ICLR 2020

first elements of the normalized eigenvectors of the Lanczos tri-diagonal matrix (Golub & Meurant,
1994). For zero mean, unit variance random vectors, using the linearity of trace and expectation


EvTr(vT Hᵐv) = TrEv(vvN Hᵐ) = Tr(Hᵐ) =

Σi=1

λᵐ = P

∫λ∈D

λᵐdµ(λ)        (10)

hence the measure on the LHS of equation 9 corresponds to that of the underlying spectral density.
The error between the expectation over the set of all zero mean, unit variance vectors v and the 
Monte
Carlo sum used in practice can be bounded (Hutchinson, 1990; Roosta-Khorasani & Ascher, 2015),
but the bounds are too loose to be of any practical value (Granziol & Roberts, 2017). However, in

the high dimensional regime N1→ ∞, we expect the squared overlap of each random vector with an

eigenvector of H, |vT φi|2  ≈    ∀i, with high probability (Cai et al., 2013). This result can be 
seen

P

intuitively by looking at the normalized overlap squared between two random Rademacher vectors,
where each element is drawn from the distribution    1 with equal probability, which gives 1/P . The
Lanczos algorithm can also be easily parallelized, since it largely involves matrix multiplications 
—
a major advantage for computing eigenvalues of large matrices in deep learning. We exploit GPU
parallelization in performing Lanczos (Gardner et al., 2018), for significant acceleration.

5.2    WHY TO AVOID MULTIPLE RANDOM VECTORS AND KERNEL SMOOTHING

In recent work using SLQ for evaluating neural network spectra (Papyan, 2018; Ghorbani et al.,
2019), the Hessian vector product is averaged over a set of random vectors, typically      10.  The
resulting discrete moment matched approximation to the spectra in equation 9 is smoothed using a
Gaussian kernel     (λi, σ²). The use of multiple random vectors can be seen as reducing the 
variance
of the vector overlap in a geometric equivalent of the central limit theorem.  Whilst this may be
desirable for general spectra, if we believe the spectra to be composed of outliers and a bulk as is
empirically observed (Sagun et al., 2016; 2017), then the same self-averaging procedure happens
with the bulk. We converge to the spectral outliers quickly using the Lanczos algorithm, as shown
in the Appendix by the known convergence theorem 3. Hence all the spectral information we need
can be gleaned from a single random vector. Furthermore, it has been proven that kernel smoothing
biases the moment information obtained by the Lanczos method (Granziol et al., 2018). We hence
avoid smoothing (and the problem of choosing σ), along with the     10    increased computational
cost, by plotting the discrete spectral density implied by equation 9 for a single random vector, 
in the
form of a stem plot.

5.3    WEAKNESSES OF STOCHASTIC LANCZOS QUADRATURE

It is possible to determine what fraction of eigenvalues is near the origin by evaluating the weight
of the Ritz value(s) closest to the origin.  But due to the discrete nature of the Ritz values, 
which
give the nodes of the associated moment-matched spectral density in equation 9, it is not possible
to see exactly whether the Hessian of neural networks is rank degenerate. It is also not possible to
exactly determine what fraction of eigenvalues are negative. Furthermore, smoothing the density and
using quadrature has the problems discussed in Section 5.2. Instead, we determine the fraction of
negative eigenvalues as the weight of negative Ritz values which are well separated from the 
smallest
magnitude Ritz value(s), to avoid counting a split of the degenerate mass.

6    EXPERIMENTS

In order to test the validity of our theoretical results in Section 4.2, we control the parameter q 
= P/N
by applying data-augmentation and then running GPU powered Lanczos method (Gardner et al.,
2018) on the augmented dataset at the final set of weights wfinₐl at the end of the training 
procedure.
We use random horizontal flips, 4     4 padding with zeros and random 32     32 crops. We neglect
the dependence of the augmented dataset on the input dataset in our analysis, but we would expect
the effective number of data points Nₑff < N due to degeneracy in the samples. For the Emprical
Hessian,  we use the full training and test dataset N  =  50, 000 with no augmentation.  We use
PyTorch (Paszke et al., 2017).  We train a 110 layer pre-activated ResNet (He et al., 2016) neural
network architecture on CIFAR-10 and CIFAR-100 using SGD with momentum set at ρ =  0.9
and data-augmentation. We include further plots and detail the training procedure for the VGG-16
(Simonyan & Zisserman, 2014) in Appendix I.

6


Under review as a conference paper at ICLR 2020

10  ²

10  ⁵

10  ⁸

6.54                                                              2.49                              
                                11.53                                                             
20.56

Eigenvalue Size

Figure 1: Empirical Hessian spectrum for CIFAR-100 SGD.

10  ²

10  ⁵

10  ⁸

1.374                                                            1.357                              
                               4.089                                                             
6.821

Eigenvalue Size

Figure 2: Empirical Hessian spectrum for CIFAR-10 SGD.

In order to investigate whether empirically observed phenomena for the Empirical Hessian such as
sharp minima generalizing more poorly than their flatter counterparts also hold for the True 
Hessian,
we run two slightly different SGD decayed learning rate schedules (equation 11) on a 110-Layer
ResNet (Izmailov et al., 2018).

α₀,                                if  ᵗ  ≤ 0.5

	


α  =     α [1 − (1−r)( ᵗ −0.5) ]    if 0.5 <  t

≤ 0.9

(11)

T

                   0.4                                         T

α₀r,                          otherwise

For both schedules we use r =  0.01.  For the Normal schedule we use α₀  =  0.1 and T  =  225

whereas for the Overfit schedule we use α₀ = 0.001 and T = 1000.

We plot the training curves in the Appendices B, C and include the best training and test accuracies
and losses, along with the extremal eigenvalues of the Empirical Hessian and Augmented Hessian in
Table 1 for CIFAR-100 and Table 2 for CIFAR-10. The Normal schedule achieves a best test accuracy
of [77.24/95.32] on CIFAR-100/CIFAR-10 respectively and best training accuracy of [99.85/99.98].
The  Overfit  schedule  achieves  a  best  test  accuracy  of  [56.85/86.46]  on  CIFAR-100/CIFAR-10
respectively and best training accuracy of [99.62/99.91].

We then run SLQ with m = 100 on the final set of training weights to compute an approximation
to the spectrum for both schedules on the full 50, 000 training set (i.e the Empirical Hessian) and
with m  =  80 on an augmented 1, 500, 000 data-set as a proxy for the True Hessian, which we
denote the Augmented Hessian. The 110-Layer ResNet has 1, 169, 972/1, 146, 842 parameters for
CIFAR-100/CIFAR-10 respectively, hence for both we have q < 1. We primarily comment on the
extremal eigenvalues of the Augmented Hessian and their deviation from those of the Empirical
Hessian. We also investigate the spectral density at the origin and the change in negative spectral
mass. A sharp drop in negative spectral mass, indicates due to Theorem 1 that most of the negative
spectral mass is due to the perturbing GOE.

6.1    NORMAL SGD SCHEDULE - EMPIRICAL AND AUGMENTED HESSIAN ANALYSIS

10  ¹                                                                                               
                10  ¹


10  ³

10  ⁵

10  ⁷

10  ³

10  ⁵

10  ⁷


1.37                0.01  0.56

Eigenvalue Size

Figure 3:  Augmented 1, 500, 000 data Hes-
sian spectrum for CIFAR-10 SGD.

6.541                         0.014     3.026

Eigenvalue Size

Figure 4:  Augmented 1, 500, 000 data Hes-
sian spectrum for CIFAR-100 SGD.

7


Under review as a conference paper at ICLR 2020

We plot the Empirical Hessian spectrum for the end of training for CIFAR-100 in Figure 1 and the
Augmented Hessian in Figure 4. We plot the same discrete spectral density plots for the Empirical
Hessian for CIFAR-10 in Figure 2 and the Augmented Hessian in Figure 3. We note that the effects
predicted by Theorem 1 are empirically supported by the differences between the Empirical and
Augmented Hessian. For CIFAR-100 the extremal eigenvalues shrink from [   6.54, 20.56] (Figure 1)
for the full dataset to [   0.014, 3.026] (Figure 4) for the Augmented dataset. The Empirical 
Hessian
has 43/100 negative Ritz values with a negative spectral mass total of 0.05. The Augmented Hessian
has 5/80 negative Ritz values with total negative spectral mass of 0.00016.  For CIFAR-10 the
extremal eigenvalues shrink from [   1.374, 6.821] (Figure 2) to [   0.0061, 0.5825] (Figure 3). The
fraction of negative Ritz values shrinks from 35/100 with total weight 0.09 to 3/80 with total mass
0.03.

6.2    OVERFITTING SGD SCHEDULE - EMPIRICAL AND AUGMENTED HESSIAN ANALYSIS

10⁰

10  ²

10  ⁴

10  ⁶

10  ⁸

7.3                                                             88.9                                
                             185.1                                                            281.3

Eigenvalue Size

Figure 5: Empirical Hessian Spectrum C100 OverFit SGD.

10  ²

10  ⁵

10  ⁸

2.9                                                              59.7                               
                              122.2                                                            
184.8

Eigenvalue Size

Figure 6: Empirical Hessian Spectrum C10 OverFit SGD.

As can be seen from the spectral plots of the full Hessians for both CIFAR-100 and CIFAR-10
in Figures 6 & 5, and their respective Augmented Hessian 7 & 8, we also observe extreme spec-
tral shrinkage, with the largest eigenvalue decreasing from [281.32, 184.4] to [45.68, 15.6] and 
the

smallest increasing from [   7.33,   2.9] to [2    10−⁵,   9    10−⁴] respectively for 
CIFAR-100/CIFAR-

10. We note that despite reasonably extreme spectral shrinkage, the sharpest values of the Overfit
schedule augmented Hessians are still significantly larger than those of the Normal schedule, hence

the intuition of sharp minima generalizing poorly still holds for the True Hessian.  The fraction
of negative Ritz values for the Empirical Hessian is 43/100, with relative negative spectral mass
of 0.032.  For the Augmented Hessian, the number of negative Ritz values is 0.  For the CIFAR-
10 Overfit Augmented Hessian, there is one negative Ritz value very close to the origin with a
large relative spectral mass 0.086, hence it could also be part of the spectral peak at the origin.


10⁰

10  ²

10  ⁴

10  ⁶

10  ¹

10  ³

10  ⁵

10  ⁷


0.00       22.84      45.68

Eigenvalue Size

0.0 7.8 15.6

Eigenvalue Size


Figure 7:  Augmented Data 1, 500, 000 Hes-
sian Spectrum C100 OverFit SGD.

Figure 8:  Augmented Data 1, 500, 000 Hes-
sian Spectrum C10 OverFit SGD.

6.3    RANK DEGENERACY AND RIGHT SKEW

The vast majority of eigen-directions of the Augmented Hessian are, like those of the Empirical
Hessian, locally extremely close to flat. We show this by looking at the 3 Ritz values of largest 
weight

8


Under review as a conference paper at ICLR 2020

for all learning rate schedules and datasets, shown in Table 3. The majority of the spectral bulk is
carried by the 3 closest weights (    0.99) to the origin for all learning rate schedules and 
datasets. In
instances (CIFAR-100 SGD OverFit) where it looks like the largest weight has been reduced, the
second largest weight (which is close in spectral distance) is amplified massively.

We further note that, as we move from the q     1 regime to q < 1, we move from having a symmetric
to right skew bulk. Both have a significant spike near the origin. This experimentally corroborates 
the
result derived in (Pennington & Bahri, 2017), where under normality of weights, inputs and further
assumptions the symmetric bulk is given by the Wigner semi-circle law and the right skew bulk is
given      by the Marcenko-Pastur. The huge spectral peak near the origin, indicates that the 
result lies in
a low effective dimension.

For all Augmented spectra, the negative spectral mass shrinks drastically and we see for the Overfit
Schedule for CIFAR-100 that there is no spectral mass below 0 and for CIFAR-10 there is one
negative Ritz value (and 79 positive).  This Ritz value λn =    0.00093 has a weight of ρ = 0.085
compared to the largest spike at λ = 0.0001 with weight 0.889, hence we cannot rule out that both
values belong to a true spectral peak at the origin.

7    DISCUSSION

The geometric properties of loss landscapes in deep learning have a profound effect on 
generalization
performance. We introduced the True Hessian to investigate the difference between the landscapes
for        the true and empirical loss surfaces.  We derived analytic forms for the perturbation 
between
the extremal eigenvalues of the True and Empirical Hessians, modelling the difference between the
two as a Gaussian Orthogonal Ensemble.  Moreover, we developed a method for fast eigenvalue
computation and visualization, which we used in conjunction with data augmentation to approximate
the     True Hessian spectrum.

We show both theoretically and empirically that the True Hessian has smaller variation in 
eigenvalues
and that its extremal eigenvalues are smaller in magnitude than the Empirical Hessian. We also show
under our framework that we expect the Empirical Hessian to have a greater negative spectral density
than the True Hessian and our experiments support this conclusion. This result may provide some
insight as to why first order (curvature blind) methods perform so well on neural networks. Reported
non-convexity and pathological curvature is far worse for the empirical risk than the true risk, 
which
is          what we wish to descend.

The shape of the true risk is particularly crucial for understanding how to develop effective 
procedures
for Bayesian deep learning. With a Bayesian approach, we not only want to find a single point that
optimizes a risk,  but rather to integrate over a loss surface to form a Bayesian model average.
The geometric properties of the loss surface, rather than the specific location of optima, therefore
greatly influences the predictive distribution in a Bayesian procedure.  Furthermore, the posterior
representation for neural network weights with popular approaches such as the Laplace approximation
has curvature directly defined by the Hessian.

In future work, one could also replace the GOE noise matrix ε(w) with a positive semi-definite white
Wishart kernel in order to derive results for the empirical Gauss-Newton and Fisher information
matrices, which are by definition positive semi-definite and are commonly employed in second order
deep learning (Martens & Grosse, 2015).  Our approach to efficient eigenvalue computation and
visualization can be used as a general-purpose tool to empirically investigate spectral properties 
of
large matrices in deep learning, such as the Fisher information matrix.

REFERENCES

Zhi Dong Bai. Convergence rate of expected spectral distributions of large random matrices part i:
Wigner matrices. In Advances In Statistics, pp. 60–83. World Scientific, 2008.

Jinho Baik and Jack W Silverstein.   Eigenvalues of large sample covariance matrices of spiked
population models. arXiv preprint math/0408165, 2004.

Alex Bloemendal, Antti Knowles, Horng-Tzer Yau, and Jun Yin.  On the principal components of
sample covariance matrices. Probability theory and related fields, 164(1-2):459–552, 2016a.

9


Under review as a conference paper at ICLR 2020

Alex Bloemendal, Bálint Virág, et al. Limits of spiked random matrices ii. The Annals of 
Probability,
44(4):2726–2769, 2016b.

Joël Bun, Jean-Philippe Bouchaud, and Marc Potters. Cleaning large correlation matrices: tools from
random matrix theory. Physics Reports, 666:1–109, 2017.

Tony Cai, Jianqing Fan, and Tiefeng Jiang.  Distributions of angles in random packing on spheres.

The Journal of Machine Learning Research, 14(1):1837–1864, 2013.

Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs,
Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing gradient descent
into wide valleys. arXiv preprint arXiv:1611.01838, 2016.

Anna Choromanska, Mikael Henaff, Michael Mathieu, Gérard Ben Arous, and Yann LeCun.  The
loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192–204, 
2015a.

Anna Choromanska, Yann LeCun, and Gérard Ben Arous. Open problem: The landscape of the loss
surfaces of multilayer networks. In Conference on Learning Theory, pp. 1756–1760, 2015b.

Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for
deep nets. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 1019–1028. JMLR. org, 2017.

Richard Everson and Stephen Roberts. Inferring the eigenvalues of covariance matrices from limited,
noisy data. IEEE transactions on signal processing, 48(7):2083–2091, 2000.

Jacob Gardner, Geoff Pleiss, Kilian Q Weinberger, David Bindel, and Andrew G Wilson. Gpytorch:
Blackbox matrix-matrix Gaussian process inference with GPU acceleration. In Advances in Neural
Information Processing Systems, pp. 7576–7586, 2018.

Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss
surfaces, mode connectivity, and fast ensembling of DNNs. In Advances in Neural Information
Processing Systems, pp. 8789–8798, 2018.

Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization
via Hessian eigenvalue density. arXiv preprint arXiv:1901.10159, 2019.

Gene H Golub and Gérard Meurant. Matrices, moments and quadrature. Pitman Research Notes in
Mathematics Series, pp. 105–105, 1994.

Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU press, 2012.

Friedrich Götze, A Naumov, and A Tikhomirov. Semicircle law for a class of random matrices with
dependent entries. arXiv preprint arXiv:1211.0389, 2012.

Diego Granziol and Stephen J. Roberts. Entropic determinants of massive matrices. In 2017 IEEE
International Conference on Big Data, BigData 2017, Boston, MA, USA, December 11-14, 2017,
pp. 88–93, 2017.

Diego Granziol, Binxin Ru, Stefan Zohren, Xiaowen Dong, Michael Osborne, and Stephen Roberts.
Entropic spectral learning for large-scale graphs. arXiv preprint arXiv:1804.06802, 2018.

Kaiming He,  Xiangyu Zhang,  Shaoqing Ren,  and Jian Sun.   Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016.

Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997.

Michael F Hutchinson.  A stochastic estimator of the trace of the influence matrix for Laplacian
smoothing splines. Communications in Statistics-Simulation and Computation, 19(2):433–450,
1990.

Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Av-
eraging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407,
2018.

10


Under review as a conference paper at ICLR 2020

Stanisław Jastrze˛bski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio,
and Amos Storkey. Three factors influencing minima in SGD. arXiv preprint arXiv:1711.04623,
2017.

Stanisław Jastrze˛bski, Zachary Kenton, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos
Storkey.  On the relation between the sharpest directions of DNN loss and the SGD step length.
2018.

Iain M Johnstone and Arthur Yu Lu. Sparse Principal Components Analysis. Unpublished manuscript,
7, 2004.

Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang.  On large-batch training for deep learning:  Generalization gap and sharp minima.  arXiv
preprint arXiv:1609.04836, 2016.

Frederik Kunstner, Lukas Balles, and Philipp Hennig. Limitations of the empirical Fisher approxima-
tion. arXiv preprint arXiv:1905.12558, 2019.

Laurent Laloux, Pierre Cizeau, Jean-Philippe Bouchaud, and Marc Potters. Noise dressing of financial
correlation matrices. Physical review letters, 83(7):1467, 1999.

Olivier Ledoit and Michael Wolf.  A well-conditioned estimator for large-dimensional covariance
matrices. Journal of multivariate analysis, 88(2):365–411, 2004.

Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein. Visualizing the loss landscape of neural nets.

arXiv preprint arXiv:1712.09913, 2017.

David JC MacKay.   Bayesian methods for adaptive models.   PhD thesis, California Institute of
Technology, 1992.

David JC MacKay.  Information theory, inference and learning algorithms.  Cambridge university
press, 2003.

James Martens and Roger Grosse. Optimizing neural networks with Kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408–2417, 2015.

Gérard Meurant and Zdeneˇk Strakoš.   The Lanczos and conjugate gradient algorithms in finite
precision arithmetic. Acta Numerica, 15:471–542, 2006.

Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.

Vardan Papyan. The full spectrum of deep net Hessians at scale: Dynamics with sample size. arXiv
preprint arXiv:1811.07062, 2018.

Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin,  Alban Desmaison, Luca Antiga, and Adam Lerer.   Automatic differentiation in
Pytorch. 2017.

Barak A Pearlmutter. Fast exact multiplication by the Hessian. Neural computation, 6(1):147–160,
1994.

Jeffrey Pennington and Yasaman Bahri. Geometry of neural network loss surfaces via random matrix
theory. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp.
2798–2806. JMLR. org, 2017.

Farbod Roosta-Khorasani and Uri Ascher. Improved bounds on sample size for implicit matrix trace
estimators. Foundations of Computational Mathematics, 15(5):1187–1212, 2015.

Levent Sagun, Léon Bottou, and Yann LeCun. Eigenvalues of the Hessian in deep learning: Singularity
and beyond. arXiv preprint arXiv:1611.07476, 2016.

Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the
Hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.

11


Under review as a conference paper at ICLR 2020

Jeffrey H Schenker and Hermann Schulz-Baldes. Semicircle law and freeness for random matrices
with symmetries or correlations. arXiv preprint math-ph/0505003, 2005.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.

Terence Tao. Topics in random matrix theory, volume 132. American Mathematical Soc., 2012.

Lei Wu, Zhanxing Zhu, et al. Towards understanding generalization of deep learning: Perspective of
loss landscapes. arXiv preprint arXiv:1706.10239, 2017.

Zhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, and Michael W Mahoney. Hessian-based analysis
of large batch training and robustness to adversaries. In Advances in Neural Information Processing
Systems, pp. 4949–4959, 2018.

A    TABLE  SUMMARIES


Schedule
Normal
Overfit

λ′1

20.56

281.32

λ′n

-6.54

-7.33

λ₁

3.025

45.68

λn

-0.014

2 × 10−⁵

Test Acc
77.24

56.85

Train Acc
99.85

99.62

Test Loss
0.99

1.85

Train Loss
0.021

0.026

Table 1: CIFAR-100 statistics table including extremal eigenvalues of the Empirical Hessian [λ′₁, 
λ′n],
the True Hessian [λ₁, λn] and test/train accuracies and losses.


Schedule
Normal
Overfit

λ′1

6.82

184.4

λ′n

-1.37

-2.9

λ₁

3.025

15.6

λn

-0.014

−9 × 10−⁴

Test Acc
95.32

86.46

Train Acc
99.98

99.91

Test Loss
0.18

1.85

Train Loss
0.0034

0.026

Table 2: CIFAR-10 statistics table including extremal eigenvalues of the Empirical Hessian [λ′₁, 
λ′n],
the True Hessian [λ₁, λn] and test/train accuracies and losses

Learning Schedule & DataSet                    [ρ₀, λ₀]                 [ρ₁, λ₁]                    
[ρ₂, λ₂]

CIFAR-100 SGD Normal                           [0.92,0.0014]       [0.05,-0.03]             
[0.018,0.06]

CIFAR-100 SGD Normal (Augmented)     [0.84,0.0017]       [0.1,-0.0007]           [0.045,0.003]

CIFAR-10 SGD Normal                             [0.88,0.007]         [0.085,-0.01]           
[0.023,0.02]
CIFAR-10 SGD Normal (Augmented)       [0.88,-3.87e-6]     [0.073,0.0006]        [0.03,-0.0007]
CIFAR-100 SGD Overfit                            [0.65,-0.026]        [0.34,0.05]              
[0.006,0.58]
CIFAR-100 SGD Overfit (Augmented)      [0.86,1.67e-5]      [0.1,0.0039]            [0.019,0.018]
CIFAR-10 SGD Overfit                              [0.77,0.01]           [0.22,-0.04]             
[0.005,0.32]

CIFAR-10 SGD Overfit (Augmented)        [0.89,0.00013]     [0.086,-0.00094]     [0.016,0.0058]

Table 3: Largest 3 weight Ritz values for different learning schedules and datasets. [ρi, λi] 
denotes
the weight and corresponding eigenvalue ρ₀ ≥ ρ₁        ≥ ρP .

12


Under review as a conference paper at ICLR 2020

B    SGD TRAINING  CURVES

Figure 9: SGD CIFAR-100 training and test curves


100

80

4

          train accuracy

          test accuracy

3

train loss
test loss

60

2

40

1

20


0                      50                    100                  150                  200

(a) Test and Training accuracy c100 SGD

0

0                    50                  100                 150                 200

(b) Test and Training loss c100 SGD

Figure 10: SGD CIFAR-10 training and test curves


100

90

80

70

60

50

40

          train accuracy

          test accuracy

0                      50                    100                  150                  200

(a) Test and Training accuracy c10 SGD

1.50

1.25

1.00

0.75

0.50

0.25

0.00

          train loss

          test loss

0                      50                   100                  150                  200

(b) Test and Training loss c10 SGD

C    OVERFIT  TRAINING  CURVES

Figure 11: OverFitSGD CIFAR-100 training and test curves


100

80

          train accuracy

          test accuracy                                                                             
                          4

train loss
test loss

3

60

2

40

20                                                                                                  
                                            1


0

0                  200                400                600                800               1000

0

0                 200               400               600               800             1000


(a) Test and Training accuracy c100 OverFitSGD

(b) Test and Training loss c100 OverFitSGD

Figure 12: OverFitSGD CIFAR-10 training and test curves


100

80

2.0

1.5

          train loss

          test loss

60

1.0


40

          train accuracy

          test accuracy

20

0                  200                400                600                800               1000

0.5

0.0

0                  200                400                600                800               1000

(a) Test and Training accuracy c10 OverFitSGD           (b) Test and Training loss c10 OverFitSGD

13


Under review as a conference paper at ICLR 2020

D    RANDOM  MATRIX  THEORY

Following the notation of (Bun et al., 2017) the resolvent of a matrix H is defined as

GH (z) = (zIN − H)−¹                                                       (12)

with z = x + iη ∈ C. The normalised trace operator of the resolvent, in the N → ∞ limit

SN (z) =      Tr[GH (z)] −−− → S(z) = ∫           du                          (13)

 1                     N→∞                 ρ(u)

	

is known as the Stieltjes transform of ρ. The functional inverse of the Siteltjes transform, is 
denoted

the blue function B(S(z)) = z. The R transform is defined as

1

R(w) = B(w) − w                                                (14)

crucially for our calculations, it is known that the R transform of the Wigner ensemble is

RW (z) = σ  z                                                   (15)

Definition D.1.  Let {Yi} and {Zij}₁≤i≤j be two real-valued families of zero mean, i.i.d random

variables, Furthermore suppose that EZ²   = 1 and for each k ∈ N

max(E|Zᵏ , E|Y₁|k ) < ∞                                        (16)

Consider an n × n symettric matrix Mn, whose entries are given by


Mn(i, i) = Yi

Mn(i, j) = Zij = Mn(j, i),   if x ≥ 1

The Matrix Mn is known as a real symmetric Wigner matrix.

(17)

Theorem 2.  Let   Mn  ∞n=1  be a sequence of Wigner matrices, and for each n denote Xn = Mn/√n.

Then µXn , converges weakly, almost surely to the semi circle distribution,


σ(x)dx =   1  √4 − x21

(18)

the property of freeness for non commutative random matrices can be considered analogously to
the moment factorisation property of independent random variables. The normalized trace operator,
which is equal to the first moment of the spectral density


ψ(H) =

 1 TrH =

N

N

N

i=1

λi =

∫λ∈D

dµ(λ)λ                             (19)

We say matrices A&B for which ψ(A) = ψ(B) = 0⁴ are free if they satisfy for any integers n₁..nk

with k ∈ N+

ψ(Aⁿ1 Bⁿ2 Aⁿ3 Bⁿ4 ) = ψ(Aⁿ1 )ψ(Bⁿ2 )ψ(Aⁿ3 )ψ(Aⁿ4 )                            (20)

E    DERIVATION

The Stijeles transform of Wigners semi circle law, can be written as (Tao, 2012)

z ± √z2  − 4σ2

SW (z) =           2σ2                                                                              
             (21)


from the definition of the Blue transform, we hence have

(z)            2   (z)     4σ2

z =                        W              

2σ2

(2σ²z − BW (z))² = B2   (z) − 4σ²

(22)


∴ BW

(z) =  1 + σ²z
z

                                                    ∴ RW (z) = σ²z

4We can always consider the transform A − ψ(A)I

14


Under review as a conference paper at ICLR 2020

Computing the     transform of the rank 1 matrix Htruₑ, with largest non-trivial eigenvalue β, on 
the
effect of the spectrum of a matrix A, using the Stieltjes transform we easily find following (Bun 
et al.,
2017) that

S       (u) =   1      1     + .1 −  1 Σ 1  =  1 Σ1 +  1        β       Σ                   (23)


Htrue

N u − β

N    u     u

N 1 − u−1β

We can use perturbation theory similar to in equation equation 22 to find the blue transform which 
to
leading order gives

B       (ω) =   1  +         β        + O(N −²)


Htrue

w     N (1 − ωβ)

(24)


setting ω = SM (z)

RHtrue

(ω) =          β        +    (N −²)

N (1 − ωβ)


z = B

Htrue

(SM

(z)) +             β           +    (N −²)                              (25)

N (1 − βSM (z))

using the ansatz of SM (z) = S₀(z) + S1 (z)  + O(N −²) we find that S₀(z) = Ss₍w₎(z) and using


that BM′   (z) = 1/g′(z) , we conclude that

βS′s(w)(z)

S (z) = −

(26)


and hence

1 − S

s(w)

(z)β


S   (z) ≈ S

1

(z) −

βS′s(w)(z)

(27)


M                s(w)

N 1 − S

s(w)

(z)β

and hence in the large N limit the correction only survives if Ss₍w₎(z) = 1/β

1

Ss₍w₎(z) =  β


2σ²

β

∴

= z ±

                

z2  − 4σ2

σ2

(28)


clearly for β → −β we have

z = β +

β

σ2


z = −β −

(29)

β

F    AN  INTUITIVE  EXPLANATION  OF  THE  KEY  RESULT

An extensive linear algebraic and geometric discussion about spectral broadening for sample co-
variance matrices can be found in (Ledoit & Wolf, 2004) and whilst the Hessian is not a covariance
matrix (the generalized Gauss-Newton or Fisher matrices can be seen as a co-variance of gradients),
identical arguments will hold here ⁵. The dispersion of the Empirical Hessian eigenvalues around
their mean will equal the dispersion of the True Hessian eigenvalues around their mean plus the
variance of the Empirical Hessian (from its true value), which is in general > 0.  Our theoretical
assumptions in sections 4.1 and 4.2 allow us to derive analytic results for this broadening.  Our
assumptions are significantly stricter than those in (Ledoit & Wolf, 2004) and hence automatically
fulfil their requirements.

⁵the proof of Lemma1 (Ledoit & Wolf, 2004) does not depend on the matrix being PSD

15


Under review as a conference paper at ICLR 2020

G    LANCZOS  ALGORITHM

In order to empirically analyse properties of modern neural network spectra with tens of millions of
parameters N =     (10⁷), we use the Lanczos algorithm (Meurant & Strakoš, 2006) with Hessian
vector products using the Pearlmutter trick (Pearlmutter, 1994) with computational cost    (NTm),
where N is the dataset size and m is the number of Lanczos steps. The main properties of the Lanczos
algorithm are summarized in the theorems 3,4

Theorem 3.  Let HN×N be a symmetric matrix with eigenvalues λ₁      ..    λn and corresponding
orthonormal eigenvectors z₁, ..zn. If θ₁      ..     θm are the eigenvalues of the matrix Tm 
obtained
after        m Lanczos steps and q₁, ...qk the corresponding Ritz eigenvectors then

(λ₁ − λn) tan²(θ₁)


λ₁ ≥ θ₁ ≥ λ₁ −

(ck−1

(1 + 2ρ₁))2

(30)

(λ₁ − λn) tan²(θ₁)


λn ≤ θk ≤ λm +

where ck is the chebyshev polyomial of order k

Proof: see (Golub & Van Loan, 2012).

(ck−1

(1 + 2ρ₁))2

Theorem 4.  The eigenvalues of Tk are the nodes tj of the Gauss quadrature rule, the weights wj

are the squares of the first elements of the normalized eigenvectors of Tk

Proof: See (Golub & Meurant, 1994). The first term on the RHS of equation 9 using Theorem 4 can
be seen as a discrete approximation to the spectral density matching the first m moments vT Hᵐv
(Golub & Meurant, 1994; Golub & Van Loan, 2012), where v is the initial seed vector. Using the
expectation of quadratic forms, for zero mean, unit variance random vectors, using the linearity of
trace   and expectation


EvTr(vT Hᵐv) = TrEv(vvT Hᵐ) = Tr(Hᵐ) =

Σi=1

λi = N

∫λ∈D

λdµ(λ)             (31)

The error between the expectation over the set of all zero mean, unit variance vectors v and the
monte carlo sum used in practice can be bounded (Hutchinson, 1990; Roosta-Khorasani & Ascher,


2015).  However in the high dimensional regime N →1

random vector with an eigenvector of H, |vT φi|2  ≈

∞, we expect the squared overlap of each

∀i, with high probability. This result can be

seen by computing the moments of the overlap between Rademacher vectors, containing elements
P (vj =    1) = 0.5. Further analytical results for Gaussian vectors have been obtained (Cai et al.,
2013).

H    SPECTRAL  PLOTS  - CIFAR-100 PRERESNET

10⁰

10  ²

10  ⁴

10  ⁶

0.77                             2.52                               5.81                            
   9.11                              12.40

Eigenvalue Size

Figure 13: Augmented CIFAR-100 PreResNet 110 Eigenspectrum Epoch 300, 5 million samples and

m = 10 Lanczos steps

16


Under review as a conference paper at ICLR 2020

10  ¹

10  ³

10  ⁵

10  ⁷

194.5                                      155.5                                      116.4         
                              77.4                                        38.4                      
                  0.7

Eigenvalue Size

Figure 14: CIFAR-100 PreResNet110 0’th epoch full dataspectrum

10  ²

10  ⁵

10  ⁸

1.501                                     0.147                                      1.796          
                            3.445                                      5.093                        
              6.742

Eigenvalue Size

Figure 15: CIFAR-100 PreResNet110 25’th epoch full dataspectrum

10⁰

10  ²

10  ⁴

10  ⁶

10  ⁸

3.34                                        1.24                                       0.85         
                               2.95                                        5.04                     
                   7.14

Eigenvalue Size

Figure 16: CIFAR-100 PreResNet110 50’th epoch full dataspectrum

10  ¹

10  ³

10  ⁵

10  ⁷

1.413                                     0.063                                      1.539          
                            3.015                                      4.491                        
              5.967

Eigenvalue Size

Figure 17: CIFAR-100 PreResNet110 75’th epoch full dataspectrum

10⁰

10  ³

10  ⁶

10  ⁹

2.24                                       0.31                                        2.87         
                               5.42                                        7.98                     
                  10.54

Eigenvalue Size

Figure 18: CIFAR-100 PreResNet110 125’th epoch full dataspectrum

10⁰

10  ²

10  ⁴

10  ⁶

10  ⁸

6.37                                        0.60                                       5.17         
                              10.93                                      16.70                      
                22.47

Eigenvalue Size

Figure 19: CIFAR-100 PreResNet110 150’th epoch full dataspectrum

17


Under review as a conference paper at ICLR 2020

10⁰

10  ²

10  ⁴

10  ⁶

10  ⁸

6.37                                        0.60                                       5.17         
                              10.93                                      16.70                      
                22.47

Eigenvalue Size

Figure 20: CIFAR-100 PreResNet110 150’th epoch full dataspectrum

18


Under review as a conference paper at ICLR 2020

I    SPECTRAL  PLOTS  - CIFAR-100 VGG16BN

The number of Parameters in the V GG     16 network is P  =  15291300 so our augmentation
procedure is unable to probe the limit q < 1. The training procedure is identical to the PreResNet
except for an initial learning rate of 0.05 and T = 300 epochs. Here we also see a reduction in both
extremal eigenvalues.

10⁰

10  ²

10  ⁴

10  ⁶

10  ⁸

6.59                                        0.52                                       5.54         
                              11.61                                      17.67                      
                23.73

Eigenvalue Size

Figure 21: CIFAR-100 VGG16BN 300’th epoch full dataspectrum

10⁰

10  ²

10  ⁴

10  ⁶

0.344                                     1.072                                      2.488          
                            3.905                                      5.321                        
              6.737

Eigenvalue Size

Figure 22: Augmented CIFAR-100 VGG16BN Eigenspectrum Epoch 300, 5 million samples and

m = 10 Lanczos steps

epoch - Full Data Spectrum.pdf

10⁰

10  ³

10  ⁶

10  ⁹

2.799                                      1.590                                      0.382         
                            0.827                                      2.036                        
              3.245

Eigenvalue Size

Figure 23: CIFAR-100 VGG16BN 0’th epoch full dataspectrum

10⁰

10  ³

10  ⁶

10  ⁹

0.595                                     0.081                                      0.757          
                            1.434                                      2.110                        
              2.786

Eigenvalue Size

Figure 24: CIFAR-100 VGG16BN 25’th epoch full dataspectrum

19


Under review as a conference paper at ICLR 2020

10⁰

10  ²

10  ⁴

10  ⁶

10  ⁸

0.330                                     0.489                                      1.308          
                            2.126                                      2.945                        
              3.763

Eigenvalue Size

Figure 25: CIFAR-100 VGG16BN 50’th epoch full dataspectrum

10⁰

10  ³

10  ⁶

10  ⁹

0.312                                     0.254                                      0.820          
                            1.386                                      1.952                        
              2.518

Eigenvalue Size

Figure 26: CIFAR-100 VGG16BN 75’th epoch full dataspectrum

10⁰

10  ³

10  ⁶

10  ⁹

0.635                                     0.367                                      1.368          
                            2.369                                      3.370                        
              4.372

Eigenvalue Size

Figure 27: CIFAR-100 VGG16BN 125’th epoch full dataspectrum

10⁰

10  ²

10  ⁴

10  ⁶

10  ⁸

1.522                                     0.430                                      2.382          
                            4.335                                      6.287                        
              8.239

Eigenvalue Size

Figure 28: CIFAR-100 VGG16BN 150’th epoch full dataspectrum

10⁰

10  ²

10  ⁴

10  ⁶

10  ⁸

6.37                                        0.60                                       5.17         
                              10.93                                      16.70                      
                22.47

Eigenvalue Size

Figure 29: CIFAR-100 VGG16BN 150’th epoch full dataspectrum

J    SPECTRAL  PLOTS  - CIFAR-10 PRERESNET

10⁰

10  ²

10  ⁴

10  ⁶

10  ⁸

2.273                                      0.736                                     0.802          
                            2.340                                      3.878                        
              5.416

Eigenvalue Size

Figure 30: CIFAR-10 PreResNet110 50’th epoch full dataspectrum

20


Under review as a conference paper at ICLR 2020

10⁰

10  ²

10  ⁴

10  ⁶

10  ⁸

3.03                                        0.54                                       1.95         
                               4.44                                        6.94                     
                   9.43

Eigenvalue Size

Figure 31: CIFAR-10 PreResNet110 300’th epoch full dataspectrum

K    SPECTRAL  PLOTS  - CIFAR-10 VGG16BN

10  ²

10  ⁵

10  ⁸

1.647                                      0.295                                     1.057          
                            2.410                                      3.762                        
              5.114

Eigenvalue Size

Figure 32: CIFAR-10 VGG16BN 0’th epoch full dataspectrum

10  ²

10  ⁵

10  ⁸

1.110                                     0.454                                      2.017          
                            3.581                                      5.145                        
              6.709

Eigenvalue Size

Figure 33: CIFAR-10 VGG16BN 300’th epoch full dataspectrum

21

