Under review as a conference paper at ICLR 2020
Role of Two Learning Rates in Convergence
of Model-agnostic Meta-learning
Anonymous authors
Paper under double-blind review
Ab stract
Model-agnostic meta-learning (MAML) is known as a powerful meta-learning
method. However, MAML is notorious for being hard to train because of the
existence of two learning rates. Therefore, in this paper, we derived a sufficinent
condition of inner learning rate α and meta-learning rate β for a simplified MAML
to locally converge to local minima from any point in the vicinity of the local
minima. We find that the upper bound of β depends on α. Moreover, we show
that the threshold of β increases as α approaches its own upper bound. This result
is verified by experiments on various few-shot tasks and architectures; specifically,
we perform sinusoid regression and classification of Omniglot and MiniImagenet
datasets with a multilayer perceptron and a convolutional neural network. Based
on this outcome, we present a guideline for determining the learning rates: first,
search for the largest possible α; next, tune β based on the chosen value of α.
1	Introduction
A pillar of human intelligence is the ability to learn and adapt to unseen tasks quickly and based
on only a limited quantity of data. Although machine learning has achieved remarkable results,
many recent models require massive quantities of data and are designed for solving particular tasks.
Meta-learning, one of the ways of tackling this problem, tries to develop a model that can adapt
to new tasks quickly by learning to learn new concepts from few data points (Schmidhuber, 1987;
Thrun & Pratt, 1998).
Among meta-learning algorithms, model-agnostic meta-learning (MAML), a gradient-based meta-
learning method proposed by Finn et al. (2017), has recently been extensively studied. For
example, MAML is used for continual learning (Finn et al., 2019; Jerfel et al., 2019; Spigler,
2019; Al-Shedivat et al., 2018), reinforcement learning (Finn et al., 2017; Al-Shedivat et al., 2018;
Gupta et al., 2018; Deleu & Bengio, 2018; Liu & Theodorou, 2019) and probablistic inference
(Finn et al., 2018; Yoon et al., 2018; Grant et al., 2018). The reason why MAML is widely used
is because MAML is simple but efficient and applicable to a wide range of tasks independent of
model architecture and the loss function. However, MAML is notorious for being hard to train
(Antoniou et al., 2019). One of the reasons why training MAML is hard is the existence of two
learning rates in MAML: the inner learning rate α and meta-learning rate β . A learning rate is
known to be one of the most important parameters, and tuning this parameter may be challenging
even if the simple gradient descent (GD) method is used. Nevertheless, we do not yet know the
relationship between these two learning rates and have little guidance on how to tune them. Hence,
guidelines for choosing these parameters are urgently needed.
In this paper, we investigate the MAML algorithm and propose a guideline for selecting the learning
rates. First, in Section 2 we briefly explain by using an approximation how MAML can be regarded
as optimization with the negative gradient penalty. Because the gradient norm is related to the shape
of the loss surface, a bias towards a larger gradient norm can make training unstable. Next, based on
the approximation explained in Section 2, in Section 3, we derive a sufficinent condition of α and β
for a simplified MAML to locally converge to local minima from any point in the neighborhood of
the local minima. Furthermore, by removing a constraint, we derive a sufficient condition for local
convergence with fewer simplifications as well. We find that the upper bound βc of meta-learning
rate depends on inner learning rate α. In particular, βc of α ≈ αc is larger than that of α = 0,
1
Under review as a conference paper at ICLR 2020
where αc is the upper bound of α. This is verified by experiments in Section 5. These results imply
a guideline for selecting the learning rates: first, search for the largest possible α; next, tune β.
2	MAML as optimization with negative gradient penalty
2.1	MAML
The goal of MAML is to find a representation that can rapidly adapt to new tasks with a small
quantity of data. In other words, MAML performs optimization for parameters θ ∈ Rd that the
optimizer can use to quickly reach the optimal parameter θTr for task T with few data points. To
this end, MAML takes the following steps to update θ. First, it samples a batch of tasks from task
distribution P (τ) and updates θ for each task τ with stochastic gradient descent (SGD). Although
MAML allows multiple-step being taken to update θ, we will consider the case only one step being
taken for simplicity. The update equation is as follows:
θT = θ — K θ LT (θ),	(1)
where α is a step size referred to as the inner learning rate, LT (θ) is the loss of T, and Ne LT (θ) is
an estimate of the true gradient. The data used for this update is called training data. Next, MAML
resamples data, referred to as test data, from each T and computes the loss at the updated parameters
θT0 , obtaining LT (θT0 ) for each task. Finally, to determine θ that can be adapted to θT0 for all tasks, θ
is updated with the gradient of a sum of loss values LT (θT0 ) over all tasks. In other words,
θ J θ - βNe X LT (θT),	⑵
τ 2 P (τ)
where β is the learning rate called the meta-learning rate and Ne PT2 P(T)LT (θ1) is an estimate of
the true gradient by using the test data. Though learning rates α and β can be tuned during training
or different for each task in practice, we will think them as fixed scalar hyperparameters.
2.2	Negative gradient penalty
Unless otherwise noted, we will consider the case of only one step being made per update, and the
data are not resampled to compute the loss for updating θ . The case of multiple steps and that of
training data and test data being separated are considered in Appendix A. The gradient of the loss at
θT is gT (θT) = Ne LT (θT) = Ne θT ∂T, where g (∙) is the gradient of L (∙) with respect to θ. If α
is small, we can assume that I ∂LT = gT (θ); this seems to hold since α is usually small (Finn et al.,
2017). Then,	τ
Ne lt (θT) = Ne θT ∂Θt = (I - Cae LT) d^T-
≈ gT(θ) - αHT(θ)gT(θ).
(3)
(4)
The result but not the procedure with the approximation is the same as that with the well-known first-
order approximation as long as data are not resampled and only one step being taken. The first order
approximation has been mentioned by Finn et al. (2017) and extensively studied by Nichol et al.
(2018) and Fallah et al. (2019). It is known that the error induced by the first-order approximation
does not degrade the performance so much in practice (Finn et al., 2017). Also, Fallah et al. (2019)
theoretically proved that the first-order approximation of MAML does not affect convergence result
when each task is similar to each other or α is small enough.
For simplicity, we will assume that only one task is considered during training, omitting task index T.
Therefore, instead of PT2P(T) LT (θT0 ), we will consider L(θ0) as the loss of the simplified MAML.
Since the MAML loss is just a sum of task-specific loss, extension to the case of multiple tasks
being considered is straightforward, as provided in Appendix A.1.1. Because Ne(g(θ)>g(θ)) =
∙-v
2H(θ)g(θ), ifwe define L(θ) = L(θ0),
L(θ) ≈ L(θ) — Cg(θ)>g(θ).	(5)
2
Under review as a conference paper at ICLR 2020
The above means that the simplified MAML can be regarded as optimization with the negative
gradient penalty. We will analyze this simplified MAML loss in Section 3. It can also be interpreted
as a Taylor series expansion of the simplified MAML loss for the first-order term, up to scale:
L(θ)= L(θ - KeL(θ))	(6)
≈ L(θ) - αVθL(θ)>VθL(θ)	(Taylor series expansion)
= L(θ) - αg(θ)>g(θ).	(7)
The fact that the simplified MAML is optimization with the negative gradient penalty is worth keep-
ing in mind. Because the goal of gradient-based optimization is to find a point where the gradient
is zero, a bias that favors a larger gradient is highly likely to make training unstable; this can be a
cause of instability of MAML (Antoniou et al., 2019). In fact, as shown in Fig. 1, the gradient norm
becomes larger during training, as do the gradient inner products, as Guiroy et al. (2019) observed.
40-
E
p 30-
20-
10-
0	10000 20000 30000 40000 50000
Iterations
Figure 1. Gradient norm during training. We compute the norm per task and subsequently compute their
average. Joint training shows when α = 0, and MAML is when α = 1e-2. These results are computed using
training data, but those determined using test data behave similarly. The total number of iterations is 50000,
β = 1e-3 and the Adam optimizer is used. Other settings are the same as those in Section 5.2.
3	Learning rate for local convergence
In this section, we will derive a sufficient condition of learning rate αand β for local convergence
from any point in the vicinity of the local minima. To this end, we will assume that only one step is
taken for update and training data and test data are not distinguished as we did in Section 2. Also,
we do not consider SGD but steepest GD to derive the condition. In other words, same training data
are assumed to be used continuously for updating parameters during training. First, we will consider
the case of only single task being considered. Next, we will consider the case of multiple tasks being
considered.
3.1	Single task
3.1.1	CONDITION FOR INNER LEARNING RATE α
First, we derive the sufficient condition of learning rate α. To this end, we will consider the sufficinet
condition that a fixed point is a local minimum. Taking the Taylor series for the second-order term
at a fixed point θ*, the simplified MAML loss is
L(θ) ≈ L(θf-) + 2(θ - θ^)>H(θ - θ^).	(8)
where H = H - α (Tg + H2) is the Hessian matrix of L(θ) at θ* and g = NeL(θ*) ∈ Rd,
H = VθL(θ*) ∈ Rd×d, and T = VθL(θ*) ∈ Rd×d×d. The calculation of H is presented in
Appendix B. We calculated the magnitudes of Tg and H2 numerically and observed that Tg was
much smaller than H2 in practice. Hence, we will ignore Tg while deriving the condition and will
3
Under review as a conference paper at ICLR 2020
∙∙w
thus assume that H = H - αH2. Further details are provided in Appendix C, and the case of Tg
being considered is provided in Appendix D. Since PΛHP> = P[ΛH - αΛh]P> where ΛH
is a diagonal matrix with entries that are eigenvalues of HH and P is a matrix with rows that are
∙-v
eigenvectors of H, the sufficinet condition of α for θ* to be a local minimum is
∀i, λ(H)i = λ(H)i - αλ(H)2 > 0
⇒ ∀i,
α<
(9)
(10)
Note that λ(A)i represents the ith eigenvalue of matrix A. Hence, the sufficient condition of α for
θ* to be a local minimum is
∀i, α<λ⅛ ∙
(11)
Therefore, αc is the inverse of the largest eigenvalue of H.
3.1.2 CONDITION FOR META-LEARNING RATE β
Next, we derive the sufficient condition of meta-learning rate β for the simplified MAML to locally
converge to the local minima discussed above from any point in the vicinity of the local minimum.
This is an extension of research of LeCun et al. (1998). Since PP> = I, the simplified MAML
loss can be written as
L(θ) ≈ L(θ*) +1((θ - θ*)>p)P>HP(P>(θ - θ*))∙	(12)
By using the simplified loss defined in Eq. 8, the update equation of the parameter θ with GD is
∙-v
θ(t + 1)= θ(t) - βNθL(θ)
=θ(t) - βHH(θ - θr-)
(13)
(14)
where t (t = 0,…，M ) is iteration and M is the total number of iterations. Hence, θ(t + 1) - θ* =
(I - βH)(θ(t) - θ*). If we denote P>(θ - θ*) by v, the simplified MAML loss in Eq. 12 is
∙-v	∙-v	—	_l_	∙-v	∙~v
L(v) ≈	L(0)	+ 2v 1 ΛHv.	Because the gradient of L(v)	for v is NvL(v)	= ΛHV,	the update
equation of v is
v (t + 1) = v (t) - βΛHv (t) = (I - βΛH) v (t),	(15)
where v(t) is the value of v during iteration t. Assuming that Eq. 11 holds, the sufficinet condition
ofβ is as follows: for all i,
11 - βλ(H - aH2)i∣ = 11 - β(λ(H)i - αλ(H)2)| < 1	(16)
⇒ -1 + β (λ (H) i - αλ (H) 2) < 1 (∙,∙ λ (H) i - αλ (H) 2 > 0 holds because ofEq. 11) (17)
2
⇒ β < ʌ / τj-∖	ʌ 2 TT∖2 ∙	(18)
λ(H)i - αλ(H)i2
Consequently, the sufficient condition for the simplified MAML to locally converge to local minima
from any point in the vicinity of the local minima is as follows:
∀i, α<λ⅛ N β<
λ (H) i - αλ (H) 2 ∙
(19)
1
λ (H) i-
2
Vanilla GD with learning rate β corresponds to MAML if α = 0. In this case, β < λ2~ is the
λmax
condition of β, where λmaχ is the largest eigenvalue of H, because 2/λmaχ is smaller than any
other 2/λ% (LeCun et al., 1998). Though this holds for the simplified MAML as well, this is not
the case if α is close to ac. The reason is that βc diverges as α approaches λ(H)i, or ac as Eq. 18
indicates. Hence, for the simplified MAML we must consider not only the largest but also other
eigenvalues and in particular, the second-largest eigenvalue. In short, unlike when vanilla GD is
employed, βc depends on α in the case of the simplified MAML, and βc is expected to be larger if
α is close to αc, as shown in Fig. 2. This finding is validated by experiments presented in Section 5.
4
Under review as a conference paper at ICLR 2020
3.2	Multiple tasks
We discussed the case of only one task being available during training in Section 3.1. In this sec-
tion, We derive upper bounds of α and β that apply if multiple tasks T 〜P (T) are considered.
Assumptions, except that multiple tasks are considered, are the same as those in Section 3.1.
3.2.1	CONDITION FOR α
ʌ
NoW, We define the simplified meta-objective as a sum of task-specific objectives: L(θ) =
∙-v
ET 〜P( T )L τ (θ). Then, at a fixed point θ*,
L(θ) ≈ L(θ*) + 2(θ - θ*)>HH(θ - θ*)	(20)
= X (lt(θ^-) + 1(θ - θ*)(Ht - αHT)(θ - θ^-))	(21)
T 〜P (T) '	)
=X Lθ*) + 2(θ - θ*)>Pτ(ΛHT- αΛHτ)P>(θ - θ*)) ,	(22)
T~P (T) ∖	)
ʌ ʌ _______________________________________________________
Where H is the Hessian matrix of L(θ) at θ. Note that We ignore TTgT as We did in Section 3.1.1.
Since HT -αHT2 is not necessarily simultaneously diagonalizable for each taskT, We cannot exactly
express eigenvalues of H as function of λ(HT) and a. Therefore, instead of the exact value of αc,
We Will derive an upper bound of αc .
~ ʌ
If θ率 is a local minimum for all LT(θ*), θ率 is a local minimum for L(θ) as well. Hence, if all
eigenvalues of HT = HT - 2aHT2 are positive for all tasks, those of H are positive as well.
Therefore, if the inequality
Fa<λhi
(23)
holds, it guarantees that the condition that θ* is a local minimum of L(θ) is satisfied. Note that this
∙-v
is a sufficient condition if θ兴 is a local minimum for all LT(θ兴).Since θ兴 can be a local minimum
ʌ ~
of L(θ) even when it is not a local minimum for all LT (θ*), the upper bound of a seems to be larger
than that in Eq. 23 in practice.
3.2.2	CONDITION FOR β
The analysis for β is also basically the same as that performed in Section 3.1.2. Denoting PT> (θ -
θ*) by Vτ, we obtain
T vT(t+1) = T I-β ΛHτ - aΛ2Hτ	vT(t).	(24)
∙-v
Since HT is not always simultaneously diagonalizable for each task, as mentioned in Section 3.2.1,
PT differs from task to task in general. Hence, we will consider an upper bound of βc as we did for
ac in Section 3.2.1. Accordingly, if both Eq. 23 and
11 - β ( λ ( Ht ) i - aλ ( Ht ) 2) I < 1
(25)
hold for any eigenvalue λi of any task T, it guarantees that β satisfies the condition for local conver-
gence. Therefore, a sufficient for the simplified MAML to locally converge to local minima from
any point in the neighborhood of the local minima in the case of multiple tasks is as follows:
12
τ,i, a<λHT)i ∧ β < λ ( Ht ) i - aλ ( Ht ) 2
(26)
4	Related Works
Sevral papers have investigated MAML and proposed various algorithms (Nichol et al., 2018;
Guiroy et al., 2019; Eshratifar et al., 2018; Antoniou et al., 2019; Fallah et al., 2019; Khodak et al.,
5
Under review as a conference paper at ICLR 2020
Figure 2. Curves of βc as a function of α for eigen-
values of the Hessian, λ0 < ... < λ4. Parameter β
is supposed to be smaller than βc for both λ4 and
λ3. Hence, β should be chosen from the colored area.
Since α must satisfy α < 二，α should also be in the
λi
colored region. The dashed line shows βc if α = 0.
If α ≈ αc , βc is larger than that at α = 0.
Figure 3. Training loss of linear regression. The area
colored in black is when the loss is below 1e-2, and
that in gray is when the loss is over 1e-2. Uncolored
region is not considered. βc : λi shows βc of λi ,
where λ1 < λ2. The dashed line is αc. Theoretical
βc and αc correspond to empirical ones.
2019; Vuorio et al., 2018; Finn et al., 2019; Deleu & Bengio, 2018; Liu & Theodorou, 2019;
Deleu & Bengio, 2018; Grant et al., 2018). Nichol et al. (2018) studied the first-order MAML fam-
ily in detail and showed that the MAML gradient could be decomposed into two terms: a term
related to joint training and a term responsible for increasing the inner product between gradients
for different tasks.
Guiroy et al. (2019) investigated the generalization ability of MAML. The researchers observed that
generalization was correlated with the average gradient inner product and that flatness of the loss
surface, often thought to be an indicator of strong generalizability in normal neural network train-
ing, was not necessarily related to generalizability in the case of MAML. Eshratifar et al. (2018)
also noted that the average gradient inner product was important. Hence, the authors proposed an
algorithm that considered the relative importance of each parameter based on the magnitude of the
inner product between the task-specific gradient and the average gradient. Although the above stud-
ies were cognizant of the importance of the inner product of the gradients, they did not explicitly
insert the negative gradient inner product, which is the negative squared gradient norm with sim-
plifications, as a regularization term. To consider the simplified MAML as optimization with a
regularization term is a contribution of our study.
Antoniou et al. (2019) enumerated five factors that could cause training MAML to be difficult. Then,
they authors proposed an algorithm to address all of these problems and make training MAML
easier and more stable. Behl et al. (2019), like us, pointed out that tuning the inner learning rate α
and meta-learning rate β was troublesome. The authors approached this problem by proposing an
algorithm that tuned learning rates automatically during training.
Fallah et al. (2019) studied the convergence theory of MAML. They proposed a method for selecting
meta-learning rate by approximating smoothness of the loss. Based on this result, they proved that
MAML can find an ε-first-order stationary point after sufficient number of iterations. On the other
hand, we studied the relationship between the sufficient conditions of inner learning rate α and
meta-learning rate β and showed that how the largest possible β is affected by the value of α.
5	Experiments
In this section, we will present the results of experiments to confirm our expectation that MAML
allows larger β ifα is close to its upper bound. First, we will show the result of linear regression with
simplifications used in Section 3.1. Because linear regression is convex optimization, the result is
expected to exactly match the theory presented in Section 3.1. Second, to check if our expectation is
confirmed in practice as well, we will present results of the practical case without any simplification.
In particular, we conducted sinusoid regression and classification of Omniglot and MiniImagenet
6
Under review as a conference paper at ICLR 2020
datasets with a multilayer perceptron and a convolutional neural network (CNN). Note that the
meta-objective used for experiments is not a sum of task-specific objectives but a mean of them.
(a) Sinusoid
-4.8
YQ
-32
-2.4
TS
(b) Omniglot
3
-2.0
-3.0
-15
-1.0
-0.5
(c) MiniImagenet
-4.8
,
3.6
3.0
2.4
1.8
Figure 4. Training losses for (a) sinusoid regression, (b) Omniglot classification, and (c) MiniImagenet classi-
fication at various values of α and β after a fixed number of iterations. The area with no color represents the
diverged losses, and the dashed line indicates the values of β above which the loss diverges for α = 0. The
maximum possible β is larger if α is close to the value above which the losses diverge than that at α = 0.
5.1	Linear regression
We performed a linear regression, where the task is to regress a linear function with scale parameter
in the range of [0, 5.0] and bias parameter in the range of [0, 5.0] based on data points in the range
of [-5.0, 5.0]. The true function has the same architecture as that of the model. We employed the
steepest gradient descent method to minimize the mean squared loss, where 1 step was taken during
update. Only one task was considered during training and the same data was used to update the
task-specific parameter and the meta parameter as we did in Section 3.1. Using these settings, we
computed the training loss after 500 iterations with α in the range of [1e-5, 9e-2] and β in the range
of [5e-3, 9e+0]. The eigenvalues are those of the Hessian matrix of the training loss at the end of
the training, where α = 5e-2 and β = 7e-1. We chose this training loss because it was thought to
be the closest to minima. Fig. 3 shows the training losses at various values of α and β. Horizontal
axis indicates α and vertical axis indicates β . The curves are βc of two eigenvalues and the dashed
line shows αc . In the case of linear regression with simplifications used in Section 3.1, the result
of numerical experiment shows good agreement with upper bounds of βc and αc that we derived in
Section 3.1, as shown in Fig. 3.
5.2	Sinusoid Regression
We conducted a sinusoid regression, where each task is to regress a sine wave with amplitude in the
range of [0.1, 5.0] and phase in the range of [0, π] based on data points in the range of [-5.0, 5.0]. A
multilayer perceptron with two hidden units of size 40 and ReLU was trained with SGD. The batch
size of data was 10, the number of tasks was 100, and 1 step was taken for update. Using these
settings, we computed the training loss after 500 iterations with α in the range of [1e-4, 9e-1] and
β in the range of [1e-2, 9e-0]. Fig. 4 (a) shows the training losses with various values of α and β.
The dashed line indicates β of α = 0 over which training loss diverges. According to Fig. 4 (a), if
α is close to the value above which the losses diverge, a larger β can be used. As explained above,
we did not put any simplification as we did in Section 3 and 5.1 in the experiment, meaning that
we used different data for updating task-specific parameter and meta parameter, considered multiple
tasks, and employed not steepest GD but SGD as optimizer. Despite simplifications, surprisingly,
this result confirms the expectation that MAML allows larger β if α is close to αc .
5.3	Classification
We performed classification of the Omniglot and MiniImagenet datasets (Lake et al., 2011;
ravi & Larochelle, 2017), which are benchmark datasets for few-shot learning. The model used
was essentially the same as that Finn et al. (2017), and hence, Vinyals et al. (2016) used. The task
is a five-way one-shot classification, where the query size is 15, the number of update steps is two,
and the task batch size is 32 for Omniglot and four for MiniImagenet. In this setup, we computed
7
Under review as a conference paper at ICLR 2020
the training losses after 100 iterations for the Omniglot dataset and one epoch for the MiniImagenet
dataset with various values of α and β; for Omniglot, α was in the range of [1e-3, 9e-0] and β was
in the range of [1e-1, 9e+1], and for MiniImagenet, α was in the range of [1e-4, 9e-1] and β was in
the range of [1e-2, 9e-0]. Fig. 4 (b) and (c) show the training losses of classification task at various
values of α and β . The dashed line indicates β of α = 0 above which training loss diverges. As
shown in Fig. 4 (b) and (c), the maximum β is larger at large α. Even though the model architecture
is composed of convolutional layer, max-pooling, and batch normalization (Ioffe & Szegedy, 2015)
and practical dataset is used for training, our expectation is confirmed in this case as well. This
result confirms that our theory is applicable in practice.
Our experimental result confirms that larger α is good for stabilizing MAML training. According
to Fig. 4 (a), (b) and (c), moreover, while large β does not necessarily make training loss smaller,
employing large α leads to smaller training loss comparatively. These result has a practical impli-
cation for tuning the learning rates: first, the largest possible α should be identified, and β may be
subsequently tuned based on the value of α. Once you identify αc, MAML is likely to work well
even if meta-learning rate is roughly chosen. Taking large α is desirable for the goal of MAML as
well. The aim of MAML is to find a good initial parameter which quickly adapt to new tasks and
the quickness is determined by inner learning rate α because α determines the step size from initial
parameter to task-specific parameter when model is fine-tuned. Therefore, identifying αc is not only
good for robustifying the model against divergence but also good for finding good initial parameter.
6	Conclusions
We regard a simplified MAML as training with the negative gradient penalty. Based on this formu-
lation, we derived the sufficient condition of the inner learning rate α and the meta-learning rate β
for the simplified MAML to locally converge to local minima from any point in the vicinity of the
local minima. We showed that the upper bound of β required for the simplified MAML to locally
converge to local minima depends on α. Moreover, we found that if α is close to its upper bound
αc, the maximum possible meta-learning rate βc is larger than that used while training with ordinary
SGD. This finding is validated by experiments, confirming that our theory is applicable in practice.
According to this result, we propose a guideline for determining α and β ; first, search for α close to
αc ; next, tune β based on the selected value of α.
References
Maruan Al-Shedivat, Trapit Bansal, Yura Burda, Ilya Sutskever, and Igor Mordatch. Con-
tinuous Adaptation Via Meta-Learning In Nonstationary And Competetive Environments.
arXiv:1710.03641, 2018.
Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your maml. International
Conference on Learning Representations (ICLR), 2019.
Harkirat Singh Behl, Atilim GuneS Baydin, and Philip H.S. Torr. Alpha maml: Adaptive model-
agnostic meta-learning. 36th International Conference on Machine Learning, 2019.
Rajendra Bhatia. Linear algebra to quantum cohomology: The story of alfred horn’s inequalities.
The American Mathematical Monthly, 108:289-318, 2001.
Tristan Deleu and Yoshua Bengio. The effects of negative adaptation in Model-Agnostic Meta-
Learning. arXiv preprint arXiv:1812.02159, 2018.
Amir Erfan Eshratifar, David Eigen, and Massoud Pedram. Gradient agreement as an optimization
objective for meta-learning. arXiv preprint arXiv:1810.08178, 2018.
Alireza Fallah, Aryan Mokhtariy, and Asuman Ozdaglar. On the Convergence Theory of Gradient-
Based Model-Agnostic Meta-Learning Algorithms. arXiv preprint arXiv:1908.10400, 2019.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic metalearning for fast adaptation
of deep networks. International Conference on Machine Learning (ICML), 2017.
8
Under review as a conference paper at ICLR 2020
Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic Model-Agnostic Meta-Learning. Neural
Information Processing Systems 2018, 2018.
Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online Meta-Learning. arXiv
preprint arXiv:1902.08438, 2019.
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting Gradient-
Based Meta-Learning as Hierarchical Bayes. arXiv preprint arXiv:1801.08930, 2018.
Simon Guiroy, Vikas Verma, and Christopher Pal. Towards understanding generalization in gradient-
based meta-learning. arXiv preprint arXiv:1907.07287, 2019.
Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine. Unsupervised Meta-
Learning for Reinforcement Learning. arXiv:1806.04640, 2018.
Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training b
y Reducing Internal Covariate Shift. arXiv:1502.03167, 2015.
Ghassen Jerfel, Erin Grant, Thomas L. Griffiths, and Katherine Heller. Reconciling meta-learning
and continual learning with online mixtures of tasks. arXiv:1812.06080, 2019.
Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Provable Guarantees for Gradient-
Based Meta-Learning. arXiv preprint arXiv:1902.10644, 2019.
Diederik P. Kingma and Jimmy Lei Ba. Adam: A Method for Stochastic Optimization. arXiv
preprint arXiv:1412.6980, 2014.
Brenden M. Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B. Tenenbaum. One shot learning
of simple visual concepts. In Proceedings of the 33th Annual Meeting of the Cognitive Science
Society, CogSci 2011, Boston, Massachusetts, USA, July 20-23, 2011, 2011.
Yann LeCun, Leon Bottou, B. Genevieve Orr, and KlaUs-Robert Muller. Efficient backprop. Neural
networks: Tricks of the trade, pp. 9 - 50,1998.
Guan-Horng Liu and Evangelos A. Theodorou. Deep Learning Theory Review: An Optimal Control
And Dynamical Systems Perspective. arXiv preprint arXiv:1908.10920, 2019.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999, 2018.
Sachin ravi and Hugo Larochelle. Optimization as a Model for Few-shot Learning. The International
Conference on Learning Representations 2017, 2017.
Jurgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to
learn: the meta-meta-... hook. PhD thesis, Technische Universitat Munchen, 1987.
Giacomo Spigler. Meta-learnt priors slow down catastrophic forgetting in neural networks.
arXiv:1909.04170, 2019.
Sebastian Thrun and Lorien Pratt. Learning to learn. Springer, 1998.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning. In Advances in Neural Information Processing Systems 29,
pp. 3630-3638, 2016.
Risto Vuorio, Shao-Hua Sun, Hexiang Hu, and Joseph J. Lim. Toward Multimodal Model-Agnostic
Meta-Learning. arXiv preprint arXiv:1812.07172, 2018.
Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
Bayesian Model-Agnostic Meta-Learning. Neural Information Processing Systems 2018, 2018.
9
Under review as a conference paper at ICLR 2020
A Negative gradient penalty: General case
In Section 2, we explained the simplest case of training data and test data being the same, only one
task being considered and only one step being taken for update. In this section, we analyze the
cases of waiving one of these simplifications. Here, we will interpret the simplified MAML loss as
a Taylor series expansion of the MAML loss for the first-order term.
A. 1 When the training data and test data are different
When using different data to compute the losses for updating the task-specific parameters and the
meta parameters as done in practical applications, the simplified loss is as follows:
L(θ) = LteSt(θ - α^θL"n(θ))	(27)
≈ Ltest(θ) - αg(θ)test>g(θ)train.	(28)
where Ltrain and LteSt represent the loss values for training and test data, respectively, and gtrain
and gteSt are gradients of these losses.
A.1.1 When multiple tasks are considered
When multiple tasks are considered, as MAML does, the meta-loss L(θ) is just a sum of losses
∙-v
Lτ (θ) for all tasks τ:
L( θ )= X L τ (θ )= X (LM (θ) - αgτ (θ) test>gτ (θ)train) .	(29)
T ~P (T)	T ~P (T)
A.2 WHEN k STEPS ARE TAKEN DURING THE UPDATE
If task-specific parameters are updated with k-step SGD, the loss can be written as follows;
Lk (θ) = Lk (θ) - α (X g> (θ)gi (θ)) .	(30)
Note that Li(θ) is the loss computed with the data at the ith step, and gi(θ) is the gradient of the
loss.
B CALCULATION OF H
∙-v	∙-v
Because H is the Hessian matrix of L at θ*, We derive the Hessian ofEq. 8. Then,
H = V θ L( θ)	(31)
=Vθ (L(θ) - 2g(θ)>g(θ))	(32)
= H(θ) - αVθ (H(θ)g(θ))	(33)
= H(θ) - α(VθH(θ)g(θ) + H(θ)H(θ))	(34)
= H - α(Tg + H2).	(35)
C MAGNITUDE OF Tg AND H2
We conducted a sinusoid regression With essentially the same condition that We explain in Section
5.2 except that the total number of iterations is 50000 and learnig rates are fixed. Parameters αand
β are 1e-2 and 1e-3 respectively. We calculated Tg numerically With the training error at the end
∙-v
of the training. As We shoWed in Section 3, especially large eigenvalues of H are important for the
upper bounds of learning rates. Therefore, if λ(Tg + H2)max ≈ λ(H2)max, We can ignore Tg
When deriving the condition. We calculate the maximum and the second-largest eigenvalues of Tg ,
H2 and Tg + H2 of the trained model. As shoWn in Fig. 5 (a), λ(Tg + H2)max is almost equal to
10
Under review as a conference paper at ICLR 2020
λ(H2)max, and λ(Tg)max is by far smaller than them. Therefore, ignoring λ(Tg)max is reasonable
when the conditions are derived. Furthermore, we calculate the Frobenius norm of Tg and H2 . As
Fig. 5 (b) indicates, the Frobenius norm of Tg is much smaller than that of H2 , meaning that Tg
is negligible in the sense of the magnitude of the norm as well. These results confirm that we can
neglect Tg when considering H .
._4
3×10
(a) Maximum eigenvalue
(b) Second largest eigenvalue
Figure 5. (a): The maximum eigenvalues of Tg, H2 and Tg + H2. It is clear that the maximum eigenvalue of
Tg+H2 is almost the same as that of H2, while that ofTg is much smaller than them. (b): The second-largest
eigenvalues of Tg, H2 and Tg + H2 . Like (a), the second-largest eigenvalue of Tg + H2 is almost equal to
that of H2. (c): The Frobenius norm of Tg and H2 . The Frobenius norm of Tg is much smaller than that of
H2.
._4
2×10
(c) Frobenius norm
D CONVERGENCE CONDITION WHEN Tg IS CONSIDERED
We assumed that Tg was negligible in Section 3. In this section, we derive a sufficient condition of
α and β for the simplified MAML to locally converge to local minima from any point in the vicinity
of the local minima under some assumptions when Tg is considered.
D. 1 CONDITION FOR INNER LEARNING RATE α TO SATISFY
When Tg is considered, the Hessian matrix of the simplified MAML loss L is H = H -
α Tg + H2 . Because H - α Tg + H 2 is a real symmetric matrix, it can be diagonalized.
Then, the sufficient condition that a fixed point θ 率 is a local minimum is
∀i, λ(HH)i = λ(H - α (Tg + H2))i > 0.	(36)
Note that Tg and H are not simultaneously diagonalizable, so we cannot decompose λ(H )i into
eigenvalues of each matrix as we did in Section 3. Therefore, we have to consider the relationship
among λ(H)i, λ(H)i and λ(Tg)i. In general, it is known that n × n Hermitian matrices A and B
satisfy the following equation (Bhatia, 2001):
λ1 (A) + λt (B) Y λ (A + B),	(37)
where λ(A) represents a vector with elements that are eigenvalues of A, ↑ indicates the operation
of sorting a vector in the ascending order, and ] indicates that in the descending order. If two
real vectors x, y ∈ Rd are related in the following way, x is said to be majorized by y and the
relationship is written as x Y y:
kk
X x∣ ≤ X yj for 1 ≤ k ≤ d,	(38)
dd
i=1 xij = i=1 yij.	(39)
11
Under review as a conference paper at ICLR 2020
We define A = H - αH2 and B = -αTg; then,
λ1 (A) + λt (B) Y λ (A + B)	(40)
⇒ λ^ (A) - αλi- (Tg) Y λ(A + B)	(41)
kk
⇒ E(N(A)i — αλ^(Tg)i) ≤	λ^(A + B)i for 1 ≤ k ≤ d.	(42)
i=1	i=1
Let us suppose that for top k eigenvalues of A, Tg, and A + B, conditions λ(A + B)i ≈ cA+B,
λ(Tg)i ≈ cTg and λ(A)i ≈ cA hold, where cA+B, cA and cTg are some constant values. Then,
λ(A)i - αλ(Tg)i ≤ λ(A + B)i for 1 ≤ i ≤ k	(43)
⇒ λ(H - αH2)i - αλ(Tg)i ≤ λ(H - α(Tg + H2))i for 1 ≤ i ≤ k. (44)
The sufficient condition for a fixed point θ* to bea local minimum is
∀i, λ(H - αH2)i - αλ(Tg)i > 0	(45)
Now, we assume that all eigenvalues of H below a threshold are 0; λ(H)i ≈ 0 for k < i ≤ n.
In fact, many eigenvalues of the loss computed by a trained model such as a deep neural network
are known to be very small. Thus, if the following condition is satisfied, it is enough to say that the
condition is satisfied:
λ(H - αH2)i - αλ(Tg)i > 0 for 1 ≤ i ≤ k
αλ(Tg)i < 0	for k ≤ i ≤ d
λ(H)i - αλ(H)2min - αλ(Tg)i > 0 for 1 ≤i ≤ k
λ(Tg )i < 0	for k ≤ i ≤ d
λ(H)i - α(λ(H)2min + λ(Tg)i) >0 for 1 ≤i≤k
λ(Tg )i < 0	for k ≤ i ≤ d
(46)
(47)
(48)
λ(H)i - αλ(Tg)i >0
λ(Tg)i < 0
for 1 ≤ i ≤ k
for k ≤ i ≤ d
(,「λ(H)min = 0)
(49)
In other words, if the smaller d - k eigenvalues of Tg are not negative values, the fixed point is not
guaranteed to be a minimum, and if they are negative, satisfying Eq. 49 is sufficient.
D.2 CONDITION FOR META LEARNING RATE β TO SATISFY
Next, we derive a sufficient condition of meta-learning rate β. Our analysis will be based on the
assumptions identical to those mentioned above. If the inequalities in Eq. 49 hold, it is sufficient for
β to satisfy
∀i, -1+β(λ(H-αλH2)i - αλ(Tg)i ≤ -1+βλ(H-α(Tg+H2))i < 1	(50)
2
⇒ ∀i, β < ~∖7ττ∖ ∖r 、 (丁 λ(H)min = 0)	(51)
λ(H)i - αλ(Tg)i
for the condition of local convergence.
E	Experiment with Adam
We conducted the same experiment as that explained in Section 5.2 and 5.3, but this time, we used
Adam optimizer instead of SGD for the meta-optimizer (Kingma & Ba, 2014). The results are
different from those we showed in Section 5. For training with Adam, MAML no longer allows us
to use larger βc. Because Adam has more parameters that we have to consider than SGD does, this
result is not at all surprising. Nonetheless, it is important to keep these facts in mind when Adam is
used for MAML optimization.
12
Under review as a conference paper at ICLR 2020
(a) Sinusoid
(b) Omniglot	(c) MiniImagenet
Figure 6. Training losses for (a) the sinusoid regression, (b) Omniglot classification and (c) MiniImagenet
classification for various values of the inner learning rate α and meta-learning rate β after a fixed number of
iterations. The area with no color represents the diverged losses.
13