Under review as a conference paper at ICLR 2020
Semantics Preserving Adversarial Attacks
Anonymous authors
Paper under double-blind review
Ab stract
While progress has been made in crafting visually imperceptible adversarial exam-
ples, constructing semantically meaningful ones remains a challenge. In this paper,
we propose a framework to generate semantics preserving adversarial examples.
First, we present a manifold learning method to capture the semantics of the inputs.
The motivating principle is to learn the low-dimensional geometric summaries of
the inputs via statistical inference. Then, we perturb the elements of the learned
manifold using the Gram-Schmidt process to induce the perturbed elements to
remain in the manifold. To produce adversarial examples, we propose an efficient
algorithm whereby we leverage the semantics of the inputs as a source of knowl-
edge upon which we impose adversarial constraints. We apply our approach on toy
data, images and text, and show its effectiveness in producing semantics preserving
adversarial examples which evade existing defenses against adversarial attacks.
1	Introduction
In response to the susceptibility of deep neural networks to small adversarial perturbations (Szegedy
et al., 2014), several defenses have been proposed (Liu et al., 2019; Sinha et al., 2018; Raghunathan
et al., 2018; Madry et al., 2017; Kolter & Wong, 2017). Recent attacks have, however, cast serious
doubts on the robustness of these defenses (Athalye et al., 2018; Carlini & Wagner, 2016). A standard
way to increase robustness is to inject adversarial examples into the training inputs (Goodfellow
et al., 2014a). This method, known as adversarial training, is however sensitive to distributional shifts
between the inputs and their adversarial examples (Ilyas et al., 2019). Indeed, distortions, occlusions
or changes of illumination in an image, to name a few, do not always preserve the nature of the image.
In text, slight changes to a sentence often alter its readability or lead to substantial differences in
meaning. Constructing semantics preserving adversarial examples would provide reliable adversarial
training signals to robustify deep learning models, and make them generalize better. However, several
approaches in adversarial attacks fail to enforce the semantic relatedness that ought to exist between
the inputs and their adversarial counterparts. This is due to inadequate characterizations of the
semantics of the inputs and the adversarial examples — Song et al. (2018) and Zhao et al. (2018b)
confine the distribution of the latents of the adversarial examples to a Gaussian. Moreover, the search
for adversarial examples is customarily restricted to uniformly-bounded regions or conducted along
suboptimal gradient directions (Szegedy et al., 2014; Kurakin et al., 2016; Goodfellow et al., 2014b).
In this study, we introduce a method to address the limitations of previous approaches by constructing
adversarial examples that explicitly preserve the semantics of the inputs. We achieve this by char-
acterizing and aligning the low dimensional geometric summaries of the inputs and the adversarial
examples. The summaries capture the semantics of the inputs and the adversarial examples. The
alignment ensures that the adversarial examples reflect the unbiased semantics of the inputs. We
decompose our attack mechanism into: (i.) manifold learning, (ii.) perturbation invariance, and (iii.)
adversarial attack. The motivating principle behind step (i.) is to learn the low dimensional geometric
summaries of the inputs via statistical inference. Thus, we present a variational inference technique
that relaxes the rigid Gaussian prior assumption typically placed on VAEs encoder networks (Kingma
& Welling, 2014) to capture faithfully such summaries. In step (ii.), we develop an approach around
the manifold invariance concept of (Roussel, 2019) to perturb the elements of the learned manifold
while ensuring the perturbed elements remain within the manifold. Finally, in step (iii.), we propose
a learning algorithm whereby we leverage the rich semantics of the inputs and the perturbations as a
source of knowledge upon which we impose adversarial constraints to produce adversarial examples.
Unlike (Song et al., 2018; Carlini & Wagner, 2016; Zhao et al., 2018b; Goodfellow et al., 2014b) that
resort to a costly search of adversarial examples, our algorithm is efficient and end-to-end.
1
Under review as a conference paper at ICLR 2020
The main contributions of our work are thus: (i.) a variational inference method for manifold learning
in the presence of continuous latent variables with minimal assumptions about their distribution, (ii.)
an intuitive perturbation strategy that encourages perturbed elements of a manifold to remain within
the manifold, (iii.) an end-to-end and computationally efficient algorithm that combines (i.) and (ii.)
to generate adversarial examples in a black-box setting, and (iv.) illustration on toy data, images and
text, as well as empirical validation against strong certified and non-certified adversarial defenses.
2	Preliminaries & Architecture
Notations. Let x be a sample from the input space X, with label y from a set of possible labels
Y, and D = {xn}nN=1 a set of N such samples x. Also, let d be a distance measure on X capturing
closeness in input space, or on Z, the embedding space of X, capturing semantics similarity.
Adversarial Examples. Given a classifier g, and its loss function `, an adversarial example of x is
produced by maximizing the objective below over an -radius ball around x (Athalye et al., 2017).
X = arg max '(g(x0),y) such that X ∈ B(x; e)
x0∈X
Above, the search region for adversarial examples is confined to a uniformly-bounded ball B(X; e).
In reality, however, the shape imposed on B is quite restrictive as the optimal search region may have
a different topology. It is also common practice to produce adversarial examples in the input space
X — via an exhaustive and costly search procedure (Shaham et al., 2018; Song et al., 2018; Zhao
et al., 2018b; Athalye et al., 2017; Carlini & Wagner, 2016; Goodfellow et al., 2014b). Unlike these
approaches, however, we wish to operate in Z, the lower dimensional embedding space of X, with
minimal computational overhead. Our primary intuition is that Z captures well the semantics of D .
x ∈ D
Figure 1: Architecture. The set of model parameters
Θ = {θm }mM=1 and Θ0 = {θm0 }mM=1 are sampled from
the recognition networks fη and fη0 . Given an input x ∈
D, we use E to sample the latent codes z1 , ..., zM via
Θ. These codes are passed to E0 to learn their perturbed
versions z1,…,ZM using Θ0. The output X 〜pφ (x01Z)
is generated via posterior sampling of a z0 (in red).
Attack Model. Given a sample x ∈ D and its
class y ∈ Y, we want to construct an adversarial
example x0 that shares the same semantics as
x. We assume the semantics of x (resp. x0) is
modeled by a learned latent variable model p(z )
(resp. p0(z0)), where z, z0 ∈ Z. In this setting,
observing x (resp. x0) is conditioned on the
observation model p(x|z) (resp. p(x0|z0)), that
is: x 〜p(x∣z) and X 〜p(x0∣z0), with Z 〜p(z)
and z0 〜p0(z0). We learn this model in a way
that d(x, x0) is small and g(x) = y ∧ g(x0) 6= y
while ensuring also that d(z, z0) is small.
Intuitively, We get the latent Z 〜p(z) which en-
codes the semantics of x. Then, we perturb z in
a way that its perturbed version z0 〜p0(z0) lies
in the manifold that supports p(Z). We define
a manifold as a set of points in Z where every
point is locally Euclidean (Roussel, 2019). We
devise our perturbation procedure by generaliz-
ing the manifold invariance concept of (Roussel,
2019) to Z. For that, we consider two embedding maps h: X → Z and h0 : X → Z, parameterized
by θ and θ0, as surrogates for p(Z) andp(Z0). We assume θ and θ0 follow the implicit distributions p(θ)
and p(θ0). In the following, we consider M such embedding maps h and h0.1 If we let Z = h(x; θ)
and Z0 = h0(x; θ0), we ensure that Z0 is in the manifold that supports p(Z) by constraining d(Z, Z0) to
be small. Then, given a map decφ : Z → X, we craft x0 = decφ(Z0) in a way that d(x, x0) is small
and g(x) = y ∧ g(x0) 6= y. Then, we say that x0 is adversarial to x and preserves its semantics.
Model Architecture. To implement our attack model, we propose as a framework the architecture
illustrated in Figure 1. Our framework is essentially a variational auto-encoder with two encoders E
and E’ that learn the geometric summaries of D via statistical inference. We present two inference
mechanisms — implicit manifold learning via Stein variational gradient descent (Liu & Wang, 2016)
and Gram-Schmidt basis sign method (Dukes, 2014) — to draw instances of model parameters from
1The reason why we consider M instances of h and h0 will become apparent in Section 3.
2
Under review as a conference paper at ICLR 2020
the implicit distributions p(θ) and p(θ0) that we parameterize E and E’ with. Both encoders optimize
the uncertainty inherent to embedding D in Z while guaranteeing easy sampling via Bayesian
ensembling. Finally, the decoder pφ acts as a generative model for constructing adversarial examples.
Threat Model. We consider in this paper a black-box scenario where we, as an attacker, have only
access to the predictions of a classifier g. As the attacker, we want to construct adversarial examples
not knowing the intricacies of g such as its loss function, nor having access to its gradient. We focus
on this scenario because it is challenging and more plausible in real-life than the white-box case. This
threat model serves to evaluate both certified defenses and non-certified ones under our attack model
3 Implicit Manifold Learning
Manifold learning is based on the assumption that high dimensional data lies on or near lower
dimensional manifolds in a data embedding space. In the variational auto-encoder (VAE) (Kingma &
Welling, 2014) setting, the datapoints Xn ∈ D are modeled via a decoder Xn 〜p(xn|zn； φ). To learn
the parameters φ, one typically maximizes a variational approximation to the empirical expected
log-likelihood 1/N PnN=1 log p(Xn; φ), called evidence lower bound (ELBO), defined as:
Le(φ,ψ; x) = Ez|x㈤ log
p(X|z; φ)p(z)
.q(Z|x;ψ)
—KL(q(z∣x; ψ)kp(z∣x; φ)) + logp(x; φ).
(1)
The expectation Ez∣χjψ can be re-expressed as a sum of a reconstruction loss, or expected negative
log-likelihood of X, and a KL(q(z |X; ψ)kp(z)) term. The KL term acts as a regularizer and forces
the encoder q(z|X; ψ) to follow a distribution similar to p(z). In VAEs, p(z) is defined as a spherical
Gaussian. That is, VAEs learn an encoding function that maps the data manifold to an isotropic Gaus-
sian. However, Jimenez Rezende & Mohamed (2015) have shown that the Gaussian form imposed on
p(z) leads to uninformative latent codes; hence to poorly learning the semantics of D (Zhao et al.,
2017). To sidestep this issue, We minimize the divergence term KL(q(z∣x; ψ)∣∣p(z∣x; φ)) using Stein
Variational Gradient Descent (Liu & Wang, 2016) instead of explicitly optimizing the ELBO.
Stein Variational Gradient Descent (SVGD) is a nonparametric variational inference method that
combines the advantages of MCMC sampling and variational inference. Unlike ELBO (Kingma
& Welling, 2014), SVGD does not confine a target distribution p(z) it approximates to simple or
tractable parametric distributions. It remains yet an efficient algorithm. To approximate p(z), SVGD
maintains M particles z = {zm}mM=1, initially sampled from a simple distribution, it iteratively
transports via functional gradient descent. At iteration t, each particle zt ∈ zt is updated as folloWs:
1M
Zt+1 - Zt + αtτ(Zt) whereT(Zt) = M X [k(zt ,zt)Vzm logp(zt ) + Nzmk(zt ,zt)],
m=1
where αt is a step-size and k(., .) is a positive-definite kernel. In the equation above, each particle
determines its update direction by consulting with other particles and asking their gradients. The
importance of the latter particles is weighted according to the distance measure k(., .). Closer particles
are given higher consideration than those lying further away. The term Nzmk(zm, Z) is a regularizer
that acts as a repulsive force between the particles to prevent them from collapsing into one particle.
Upon convergence, the particles Zm will be unbiased samples of the true implicit distribution p(Z).
Manifold Learning via SVGD. To characterize the manifold of D, which we denote M, we
learn the encoding function q(.; ψ). Similar to Pu et al. (2017), we optimize the divergence
KL(q(z∣x;ψ)kp(z∣x;φ)) using SVGD. As an MCMC method, SVGD, however, induces inher-
ent uncertainty we ought to capture in order to learn M efficiently. To potentially capture such
uncertainty, Pu et al. (2017) use dropout. However, according to Hron et al. (2017), dropout is not
principled. Bayesian methods, on the contrary, provide a principled way to model uncertainty through
the posterior distribution over model parameters. Kim et al. (2018) have shown that SVGD can be
cast as a Bayesian approach for parameter estimation and uncertainty quantification. Since SVGD
always maintains M particles, we introduce thus M instances of model parameters Θ = {θm}mM=1,
where every θm ∈ Θ is a particle that defines the weights and biases of a Bayesian neural network.
For large M , however, maintaining Θ can be computationally prohibitive because of the memory
footprint. Furthermore, the need to generate the particles during inference for each test case is
undesirable. To sidestep these issues, we maintain only one (recognition) network fη that takes as
3
Under review as a conference paper at ICLR 2020
f (ξ; η)
A
ξ
X
Algorithm 1 Inversion with one particle θ.
Require: Input X ∈ D
Require: Model parameters η
1:	Sample ξ 〜N(0,I)
2:	Sample θ 〜f (ξ)
3:	Given x, sample Z 〜p(z∣x; θ)
4:	Sample X 〜p(x|z, φ)
5:	Sample Z 〜p(z∣X, θ)
6:	Use x and Z to computep(Z∣x; θ)
Figure 2: Inversion. Process for computing the likelihoodp(D∣θ). As the decoder pφ gets accurate,
the error ∣∣x 一 X∣∣2 becomes small (see Algorithm 2), and We get closer to sampling the optimal Z.
input ξm 〜N(0, I) and outputs a particle θm. The recognition network f learns the trajectories of
the particles as they get updated via SVGD. fη serves as a proxy to SVGD sampling strategy, and is
refined through a small number of gradient steps to get good generalization.
where
with θm+1 - θm+αtτ (θm),
1M
τ(θt) = MM X [k(θj ,θt)Vθj logp(θj) + Vθj k(θj,θt)].
j=1
(2)
We use the notation SVGDτ (Θ) to denote an SVGD update of Θ using the operator τ (.). As
the particles θ are Bayesian, upon observing D, we update the prior p(θjt ) to obtain the posterior
p(θj|D) a p(D∣θj)p(θj) which captures the uncertainty. We refer the reader to Appendix A for a
formulation of p(θj∣D) andp(D∣θj). The data likelihoodp(D∣θj) is evaluated over all pairs (x,Z)
where X ∈ D and Z is a dependent variable. However, Z is not given. Thus, we introduce the inversion
process described in Figure 2 to generate such Z using Algorithm 1. For any input X ∈ D, we sample
its latent code Z from p(Z|X; D), which we approximate by Monte Carlo over Θ; that is:
p(z|x; D)
/p(z∣x; θ)p(θ∣D)dz ≈
1M
M £ p(z∣x; θm) where θm 〜p(θ∣D).
m=1
(3)
4 Perturbation Invariance
Here, we focus on perturbing the elements of M. We want the perturbed elements to reside in M and
exhibit the semantics of D that M captures. Formally, we seek a linear mapping h0 : M → M such
that for any point Z ∈ M, a neighborhood U of Z is invariant under h0; that is: Z0 ∈ U ⇒ h0(Z0) ∈ U.
In this case, we say that M is preserved under h0 . Trivial examples of such mappings are linear
combinations of the basis vectors of subspaces S of M called linear spans of S .
Rather than finding a linear span h0 directly, we introduce a new set of instances of model parameters
Θ0 = {θm0 }mM=1 . Each θm0 denotes the weights and biases of a Bayesian neural network. Then,
for any input X ∈ D and its latent code Z 〜p(z∣x; D), a point in M, we set h0(z) = z0 where
Z0 〜p(z0∣x; D). We approximatep(z0∣x; D) by Monte Carlo using Θ0, as in Equation 3. We leverage
the local smoothness of M to learn each θm0 in a way to encourage Z0 to reside in M in a close
neighborhood of Z using a technique called Gram-Schmidt Basis Sign Method.
Gram-Schmidt Basis Sign Method (GBSM). Let X be a batch of samples of D, Zm a set of latent
codes Zm 〜p(z∣x; θm) where X ∈ X, and θm ∈ Θ. For any m ∈ {1.., M}, we learn θm to generate
perturbed versions of Zm ∈ Zm along the directions of an orthonormal basis Um . As M is locally
Euclidean, we compute the dimensions of the subspace Zm by applying Gram-Schmidt (Dukes,
2014) to orthogonalize the span of representative local points. We formalize GBSM as follows:
argmin %(δm,θm) := X||Zm - [Zm + δm © Sign(Uim)]||2 where Zm 〜P(ZIXi； θm).
δm, θm	zm
4
Under review as a conference paper at ICLR 2020
The intuition behind GBSM is to utilize the fact that topological spaces are closed under their basis
vectors to render M invariant to the perturbations δm. To elaborate more on GBSM, we first sample
a model instance θ0n. Then, We generate Zm 〜P(ZIx; θ0n) for all X ∈ X. We orthogonalize Zm and
find the perturbations δm that minimizes % along the directions of the basis vectors uim ∈ Um . We
Want the perturbations δm to be small. With δm fixed, We update θm0 by minimizing % again. We use
the notation GBSM(Θ0, ∆) Where ∆ = {δm}mM=1 to denote one update of Θ0 via GBSM.
Manifold Alignment. Although GBSM confers us latent noise imperceptibility and sampling speed,
Θ0 may deviate from Θ; in Which case the manifolds they learn Will mis-align. To mitigate this issue,
We regularize each θm0 ∈ Θ0 after every GBSM update. In essence, We apply one SVGD update on
Θ0to ensure that Θ0 folloWs the transform maps constructed by the particles Θ (Han & Liu, 2017).
1M
Θ0+1 - θ0 + αt∏(θ0) Whereπ(θ0) = M X [k(θ0,心θt logp(θm + Vemk(θ[, θmm)]	(4)
m=1
We use the notation SVGDπ(Θ0) to refer to the gradient update rule in Equation 4. In this rule, the
model instances Θ0 determine their oWn update direction by consulting only the particles Θ instead
of consulting each other. Maintaining Θ0 = {θm0 }mM=1 for large M is, hoWever, computationally
prohibitive. Thus, as in Section 3, We keep only one (recognition) netWork fη0 that takes as input
ξm 〜N(0, I) and outputs θml 〜f (ξmm; W). Here too we refine W through a small number of gradient
steps to learn the trajectories that Θ0 folloWs as it gets updated via GBSM and SVGDπ.
M
η0t+1-argminxUf(ξm；ηt)—θm+ι(	whereθm+1-θm+。视时)	⑸
η	m=1 F~'
5	Generating Adversarial Examples
In this paper, a black-box scenario is considered. In this scenario, We have only access to the
predictions of the classifier g . We produce adversarial examples by optimizing the loss beloW. The
first term is the reconstruction loss inherent to VAEs. This loss accounts here for the dissimilarity
betWeen any input x ∈ D and its adversarial counterpart x0, and is constrained to be smaller than
attack so that x0 resides Within an attack-radius ball of x. Unless otherWise specified, We shall use
norm L2 as reconstruction loss. The second term is an auxiliary log-likelihood loss (for g) of a target
class y0 ∈ Y \ {y} Where y is the class of x. This loss defines the cost incurred for failing to fool g.
Lχ0 = Ilx — X0k2 + min 1y=y，∙ log(1 — P(y0∣x0)) such that kX — x0∣∣2 ≤ EattaCk.	(6)
y0∈Y
In Algorithm 2, We shoW hoW We unify our manifold learning and perturbation strategy into one
learning procedure to generate adversarial examples Without resorting to an exhaustive search.
6	Related Work
Manifold Learning. VAEs are generally used to learn manifolds (Yu et al., 2018; Falorsi et al.,
2018; Higgins et al., 2016) by maximizing the ELBO of the data log-likelihood (Alemi et al.,
2017; Chen et al., 2017). Optimizing the ELBO entails reparameterizing the encoder to a Gaussian
distribution (Kingma & Welling, 2014). This reparameterization is, hoWever, restrictive (Jimenez
Rezende & Mohamed, 2015) as it may lead to learning poorly the manifold of the data (Zhao et al.,
2017). To alleviate this issue, We use SVGD, similar to Pu et al. (2017). While our approach and that
of Pu et al. (2017) may look similar, ours is more principled. As discussed in (Hron et al., 2017),
dropout Which Pu et al. (2017) use is not Bayesian. Since our model instances are Bayesian, We are
better equipped to capture the uncertainty. Capturing the uncertainty requires, hoWever, evaluating the
data likelihood. As We are operating in latent space, this raises the interesting challenge of assigning
target-dependent variables to the inputs. We overcome this challenge using our inversion process.
Adversarial Examples. Studies in adversarial deep learning (Athalye et al., 2018; Kurakin et al.,
2016; GoodfelloW et al., 2014b; Athalye et al., 2017) can be categorized into tWo groups. The first
group (Carlini & Wagner, 2016; Athalye et al., 2017; Moosavi-Dezfooli et al., 2016) proposes to
5
Under review as a conference paper at ICLR 2020
Algorithm 2 Generating Adversarial Examples. Lines 2 and 4 compute distances between sets
keeping a one-to-one mapping between them. x0 is adversarial to x when Lx0 ≤ attack and y 6= y0.
1:	function INNERTRAIN ing(Θ, Θ0, η, η, ∆, X)
Require: Learning rates β, β0
2:	η - η - βVηkΘ- SVGDτ(Θ)k2
3:	∆, Θ0 - GBSM(Θ0, ∆)
4：	η0 - η0 - β0Vη0kΘ0- SVGD∏(Θ0)k2
5:	return η, η0 , ∆
Require: Training samples (x, y) ∈ D × Y
Require: Number of model instances M
Require: Number of inner updates T
Require: Initialize weights η, η0 , φ
Require: Initialize perturbations ∆ := {δm }mM=1
Require: Learning rates , α, α0 , and noise margin attack
6:	Sample ξ1, ..., ξM from N (0, I)
7:	for t = 1 to T do
8： Sample Θ = {θm}M=ι where θm 〜f (ξm)
9：	SamPle θ = {θm}M=ι Where θm 〜fη,(ξm)
10:	Use Θ and Θ0 in Equation 3 to sample z and z0
11:	Sample x 〜p(x∣z, φ) and x0 〜p(x0∣z0, φ)
12:	η, η’, ∆ —InnerTraining(Θ, Θ0, η, η0, ∆, X)
13:	Lx：= IIX — x∣∣2; Lχo := IlX — χ0∣∣2
. local gradient updates of fη, fη0 , ∆
.apply inversion on X and update η
. update ∆ and Θ0 using GBSM
. align Θ0 with Θ and update η0
. recognition nets fη, fη0, decoder pφ
. latent (adversarial) perturbations
. inputs to recognition nets fη, fη0
. clean and perturbed reconstructions
.reconstruction losses on X and x0
L
Lxo :=
14:
Lx0,	if Lx0
> attack
Lxo + min [ly=yo ∙ log(1 — P(y0∣X0))], otherwise
y0∈Y
15:	η — η — αVηLx; η0 — η0 — α0Vη0Lxo	. SGD update using Adam optimizer
16:	φ — φ — eVφ(Lx + Lxo)	. SGD update using Adam optimizer
generate adversarial examples directly in the input space of the original data by distorting, occluding
or changing illumination in images to cause changes in classification. The second group (Song et al.,
2018; Zhao et al., 2018b), where our work belongs, uses generative models to search for adversarial
examples in the dense and continuous representations of the data rather than in its input space.
Adversarial Images. Song et al. (2018) propose to construct unrestricted adversarial examples in the
image domain by training a conditional GAN that constrains the search region for a latent code z0 in
the neighborhood of a target z. Zhao et al. (2018b) use also a GAN to map input images to a latent
space where they conduct their search for adversarial examples. These studies are the closest to ours.
Unlike in (Song et al., 2018) and (Zhao et al., 2018b), however, our adversarial perturbations are
learned, and we do not constrain the search for adversarial examples to uniformly-bounded regions.
In stark contrast to Song et al. (2018) and Zhao et al. (2018b) approaches also, where the search for
adversarial examples is exhaustive and decoupled from the training of the GANs, our approach is
efficient and end-to-end. Lastly, by capturing the uncertainty induced by embedding the data, we
characterize the semantics of the data better, allowing us thus to generate sound adversarial examples.
Adversarial Text. Previous studies on adversarial text generation (Zhao et al., 2018a; Jia & Liang,
2017; Alvarez-Melis & Jaakkola, 2017; Li et al., 2016) perform word erasures and replacements
directly in the input space using domain-specific rules or heuristics, or they require manual curation.
Similar to us, Zhao et al. (2018b) propose to search for textual adversarial examples in the latent
representation of the data. However, in addition to the differences aforementioned for images,
the search for adversarial examples is handled more gracefully in our case thanks to an efficient
gradient-based optimization method in lieu of a computationally expensive search in the latent space.
7	Experiments & Results
Before, we presented an attack model whereby we align the semantics of the inputs with their
adversarial counterparts. As a reminder, our attack model is black-box, restricted and non-targeted.
Our adversarial examples reside within an attack—radius ball of the inputs as our reconstruction
6
Under review as a conference paper at ICLR 2020
loss, which measures the amount of changes in the inputs, is bounded by attack (see Equation 6).
We validate the adversarial examples we produce based on three evaluation criteria: (i.) manifold
preservation, (ii.) adversarial strength, and (iii.) soundness via manual evaluation. We provide in
Appendix A examples of the adversarial images and sentences that we construct.
7.1	Manifold Preservation
We experiment with a 3D non-linear Swiss Roll dataset which comprises 1600 datapoints grouped in
4 classes. We show in Figure 3, on the left, the 2D plots of the manifold we learn. In the middle, we
plot the manifold and its elements that we perturbed and whose reconstructions are adversarial. On
the right, we show the manifold overlaid with the latent codes of the adversarial examples produced
by PGD (Goodfellow et al., 2014b) with attack ≤ 0.3. Observe in Figure 3, in the middle, how the
latent codes of our adversarial examples espouse the Swiss Roll manifold, unlike the plot on the right.
Figure 3: Invariance. Swiss Roll manifold learned with our encoder E (left), and after perturbing its
elements with our encoder E0 (middle) vs. that of PGD adversarial examples (right) learned using E .
7.2	Adversarial Strength
Setup. As argued in (Athalye et al., 2018), the strongest non-certified defense against adversarial
attacks is adversarial training with Projected Gradient Descent (PGD) (Goodfellow et al., 2014b).
Thus, we evaluate the strength of our MNIST, CelebA and SVHN adversarial examples against
adversarially trained ResNets (He et al., 2015) with a 40-step PGD and noise margin attack ≤ 0.3.
The ResNet models follow the architecture design of Song et al. (2018). Similar to Song et al.
(2018) — whose attack model resembles ours2 —, for MNIST, we also target the certified defenses
(Raghunathan et al., 2018; Kolter & Wong, 2017) with attack = 0.1 using norm L∞ as reconstruction
loss. For all the datasets, the accuracies of the models we target are higher than 96.3%. Next, we
present our attack success rates and give examples of our adversarial images in Figure 4.
Attack Success Rate (ASR) is the percentage of examples misclassified by the adversarially trained
Resnet models. For attack = 0.3, the publicly known ASR of PGD attacks on MNIST is 88.79%.
However, our ASR for MNIST is 97.2%, higher than PGD. Also, with attack ≈ 1.2, using norm L∞
as reconstruction loss in Equation 6, we achieve an ASR of 97.6% against (Kolter & Wong, 2017)
where PGD achieves 91.6%. Finally, we achieve an ASR of 87.6% for SVHN, and 84.4% for CelebA.
7.2.1	Adversarial Text
Datasets. For text, we consider the SNLI (Bowman et al., 2015) dataset. SNLI consists of sentence
pairs where each pair contains a premise (P) and a hypothesis (H), and a label indicating the
relationship (entailment, neutral, contradiction) between the premise and hypothesis. For instance,
the following pair is assigned the label entailment to indicate that the premise entails the hypothesis.
Premise: A soccer game with multiple males playing. Hypothesis: Some men are playing a sport.
2Note that our results are, however, not directly comparable with (Song et al., 2018) as their reported success
rates are for unrestricted adversarial examples, unlike ours, manually computed from Amazon MTurkers votes.
7
Under review as a conference paper at ICLR 2020
Table 1: Test samples and their perturbed versions. See more examples in Appendix A.
True Input 1 P: A group of people are gathered together. H: There is a group here. Label: Entailment
Adversary 1 H’: There is a group there. Label: Contradiction
True Input 2 P: A female lacrosse player jumps up. H: A football player sleeps. Label: Contradiction
Adversary 2 H’: A football player sits. Label: Neutral
True Input 3 P: Aman stands in a curvy corridor. H’: A man runs down a back alley. Label: Contradiction
Adversary 3 H’: A man runs down a ladder alley. Label: Neutral
Setup. We perturb the hypotheses while keeping
the premises unchanged. Similar to Zhao et al.
(2018b), we generate adversarial text at word level
using a vocabulary of 11,000 words. We also use
ARAE (Zhao et al., 2018a) for word embedding,
and a CNN for sentence embedding. To generate
perturbed hypotheses, we experiment with three
decoders: (i.) pφ is a transpose CNN, (ii.) pφ is a
language model, and (iii.) we use the decoder of a
pre-trained ARAE (Zhao et al., 2018a) model. We
detail their configurations in Appendix B.
以夕夕,邑/e
0r∕3sxcς>，
a /736 ORb
o-∙⅜3 32 7/5
Λ3∕3 0g77
qq3ws43Γn
/6 7Zfc A^ze T« /P
8 I esGbo 72
a>∙夕，6 / f
2∕3sa%
3/735 OEb
q∙⅛832 /，5
23/303,7
qq¾35433
/ 6 4›4 0 6 2 4
8 I O SG σo 72
The transpose CNN generates more meaningful
hypotheses than the language model and the pre-
trained ARAE model although we notice some-
times changes in the meaning of the original hy-
potheses. We discuss these limitations more in
details in Appendix A. Henceforward, we use the
transpose CNN to generate perturbed hypotheses.
See Table 1 for examples of generated hypotheses.
Attack Success Rate (ASR). We attack an SNLI
classifier that has a test accuracy of 89.42%. Given
a pair (P, H) with label l, its perturbed version (P,
H’) is adversarial if the classifier assigns the label
l to (P, H), (P, H’) is manually found to retain the
label of (P, H), and such label differs from the one
the classifier assigns to (P, H’). To compute the
ASR, we run a pilot study which we detail next.
7.3	Manual Evaluation
To validate our adversarial examples and assess
their soundness vs. Song et al. (2018), Zhao et al.
(2018b) and PGD (Madry et al., 2017) adversarial
examples, we carry out a pilot study whereby we
ask three yes-or-no questions: (Q1) are the adver-
sarial examples semantically sound?, (Q2) are the
(e)	(f)
Figure 4: Inputs (left) - Adversarial exam-
ples (right, inside red boxes). MNIST: (a)-(b),
CelebA: (c)-(d), SVHN: (e)-(f). See Appendix
A for more samples with higher resolution.
true inputs similar perceptually or in meaning to their adversarial counterparts? and (Q3) are there
any interpretable visual cues in the adversarial images that support their misclassification?
Pilot Study I. For MNIST, we pick 50 images (5 for each digit), generate their clean reconstructions,
and their adversarial examples against a 40-step PGD ResNet with attack ≤ 0.3. We target also the
certified defenses of Raghunathan et al. (2018) and Kolter & Wong (2017) with attack = 0.1. For
SVHN, we carry out a similar pilot study and attack a 40-step PGD ResNet. For CelebA, we pick 50
images (25 for each gender), generate adversarial examples against a 40-step PGD ResNet. For all
three datasets, we hand the images and the questionnaire to 10 human subjects for manual evaluation.
We report in Table 2 the results for MNIST and, in Table 4, the results for CelebA and SVHN.
We hand the same questionnaire to the subjects with 50 MNIST images, their clean reconstructions,
and the adversarial examples we craft with our method. We also handed the adversarial examples
8
Under review as a conference paper at ICLR 2020
generated using Song et al. (2018), Zhao et al. (2018b) and PGD methods. We ask the subjects
to assess the soundness of the adversarial examples based on the semantic features (e.g., shape,
distortion, contours, class) of the real MNIST images. We report the evaluation results in Table 3.
Table 2: Pilot Study I. Note that against the certified defenses of Raghunathan et al. (2018) and Kolter
& Wong (2017), Song et al. (2018) achieved (manual) success rates of 86.6% and 88.6%.
	MNIST				
Questionnaire	40-step PGD	Raghunathan et al.	(201 8)	KOLTER&Wong(2017)	
Question Q1: Yes	100 %	100 %		100 %	
Question Q2: Yes	100 %	100 %		100 %	
Question Q3: No	100 %	100 %		100 %	
Table 3: Pilot Study I. The adversarial images are generated against the adversarially trained Resnets.					
Questionnaire	OUR Method	SONG et al. (2018)	Zhao et al. (2018b)		PGD
Question Q1: Yes	100 %	85.9 %		97.8 %	76.7 %
Question Q2: Yes	100 %	79.3 %		89.7 %	66.8 %
Question Q3: NO	100 %	71.8 %		94.6 %	42.7 %
Pilot Study II - SNLI. Using the transpose CNN as decoder pφ, we generate adversarial hypotheses
for the SNLI sentence pairs with the premises kept unchanged. Then, we select manually 20 pairs of
clean sentences (premise, hypothesis), and adversarial hypotheses. We also pick 20 pairs of sentences
and adversarial hypotheses generated this time using Zhao et al. (2018b)’s method against their
treeLSTM classifier. We choose this classifier as its accuracy (89.04%) is close to ours (89.42%).
We carry out a pilot study where we ask two yes-or-no questions: (Q1) are the adversarial samples
semantically sound? and (Q2) are they similar to the true inputs? We report the results in Table 4.
Table 4: Pilot Studies. * Some adversarial images and original ones were found blurry to evaluate.
Questionnaire	Pilot I		Pilot II-SNLI	
	CelebA	SVHN	Our Method	Zhao et al. (2018b)
Question Q1: Yes	100 %	95t %	83.7 %	79.6%
Question Q2: Yes	100%	97 %	60.4 %	56.3%
Question Q3: NO	100%	100 %	N/A	N/A
Takeaways. As reflected in the pilot study and the attack success rates, we achieve good results in
the image and text classification tasks. In the image classification task, we achieve better results than
PGD and Song et al. (2018) both against the certified and non-certified defenses. The other takeaway
is: although the targeted certified defenses are resilient to adversarial examples crafted in the input
space, we can achieve manual success rates higher than the certified rates when the examples are
constructed in the latent space, and the search region is unrestricted. In text classification, we achieve
better results than Zhao et al. (2018b) when using their treeLSTM classifier as target model.
8	Conclusion
Many approaches in adversarial attacks fail to enforce the semantic relatedness that ought to exist
between original inputs and their adversarial counterparts. Motivated by this fact, we developed a
method tailored to ensuring that the original inputs and their adversarial examples exhibit similar
semantics by conducting the search for adversarial examples in the manifold of the inputs. Our success
rates against certified and non-certified defenses known to be resilient to traditional adversarial attacks
illustrate the effectiveness of our method in generating sound and strong adversarial examples.
Although in the text classification task we achieved good results and generated informative adversarial
sentences, as the transpose CNN gets more accurate — recall that it is partly trained to minimize a
9
Under review as a conference paper at ICLR 2020
reconstruction error —, generating adversarial sentences that are different from the input sentences
and yet preserve their semantic meaning becomes more challenging. In the future, we intend to build
upon the recent advances in text understanding to improve our text generation process.
References
Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, and Kevin Murphy.
An information-theoretic analysis of deep latent-variable models. CoRR, abs/1711.00464, 2017.
URL http://arxiv.org/abs/1711.00464.
David Alvarez-Melis and Tommi S. Jaakkola. A causal framework for explaining the predictions of
black-box sequence-to-sequence models. CoRR, abs/1707.01943, 2017. URL http://arxiv.
org/abs/1707.01943.
Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial
examples. CoRR, abs/1707.07397, 2017. URL http://arxiv.org/abs/1707.07397.
Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. CoRR, abs/1802.00420, 2018. URL
http://arxiv.org/abs/1802.00420.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. CoRR, abs/1508.05326, 2015. URL
http://arxiv.org/abs/1508.05326.
Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. CoRR,
abs/1608.04644, 2016. URL http://arxiv.org/abs/1608.04644.
Liqun Chen, Shuyang Dai, Yunchen Pu, Chunyuan Li, Qinliang Su, and Lawrence Carin. Sym-
metric Variational Autoencoder and Connections to Adversarial Learning. arXiv e-prints, art.
arXiv:1709.01846, Sep 2017.
Kimberly A. Dukes. GramSchmidt Process. American Cancer Society, 2014. ISBN 9781118445112.
doi: 10.1002/9781118445112.stat05633. URL https://onlinelibrary.wiley.com/
doi/abs/10.1002/9781118445112.stat05633.
Luca Falorsi, Pim de Haan, Tim R. Davidson, Nicola De Cao, Maurice Weiler, Patrick Forre, and
Taco S. Cohen. Explorations in Homeomorphic Variational Auto-Encoding. arXiv e-prints, art.
arXiv:1807.04689, Jul 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems,pp. 2672-2680, 2014a.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harvesting adversarial
examples. CoRR abs/1412.6572, 2014b.
Jun Han and Qiang Liu. Stein variational adaptive importance sampling. Conference on Uncertainty
in Artificial Intelligence (UAI), 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385.
Irina Higgins, Loic Matthey, Xavier Glorot, Arka Pal, Benigno Uria, Charles Blundell, Shakir
Mohamed, and Alexander Lerchner. Early Visual Concept Learning with Unsupervised Deep
Learning. arXiv e-prints, art. arXiv:1606.05579, Jun 2016.
Jiri Hron, Alexander G. de G. Matthews, and Zoubin Ghahramani. Variational Gaussian Dropout is
not Bayesian. arXiv e-prints, art. arXiv:1711.02989, Nov 2017.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Alek-
sander Madry. Adversarial Examples Are Not Bugs, They Are Features. arXiv e-prints, art.
arXiv:1905.02175, May 2019.
10
Under review as a conference paper at ICLR 2020
Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems.
CoRR, abs/1707.07328, 2017. URL http://arxiv.org/abs/1707.07328.
Danilo Jimenez Rezende and Shakir Mohamed. Variational Inference with Normalizing Flows. arXiv
e-prints, art. arXiv:1505.05770, May 2015.
Taesup Kim, Jaesik Yoon, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
Bayesian model-agnostic meta-learning. CoRR, abs/1806.03836, 2018. URL http://arxiv.
org/abs/1806.03836.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. International Conference
on Learning Representations (ICLR), 2014.
J. Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer
adversarial polytope. CoRR, abs/1711.00851, 2017. URL http://arxiv.org/abs/1711.
00851.
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
CoRR, abs/1607.02533, 2016. URL http://arxiv.org/abs/1607.02533.
Jiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation
erasure. CoRR, abs/1612.08220, 2016. URL http://arxiv.org/abs/1612.08220.
Chen Liu, Ryota Tomioka, and Volkan Cevher. On certifying non-uniform bound against adversarial
attacks. CoRR, abs/1903.06603, 2019. URL http://arxiv.org/abs/1903.06603.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose Bayesian inference
algorithm. Neural Information Processing Systems (NIPS), 2016.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. CoRR, abs/1706.06083, 2017. URL
http://arxiv.org/abs/1706.06083.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal
adversarial perturbations. CoRR, abs/1610.08401, 2016. URL http://arxiv.org/abs/
1610.08401.
Yunchen Pu, Zhe Gan, Ricardo Henao, Chunyuan Li, Shaobo Han, and Lawrence Carin. VAE
learning via Stein variational gradient descent. Neural Information Processing Systems (NIPS),
2017.
Alec Radford. Improving language understanding by generative pre-training. In arXiv, 2018.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial
examples. CoRR, abs/1801.09344, 2018. URL http://arxiv.org/abs/1801.09344.
Marc R Roussel. Invariant manifolds. In Nonlinear Dynamics, 2053-2571, pp. 6-1 to 6-20. Morgan
& Claypool Publishers, 2019. ISBN 978-1-64327-464-5. doi: 10.1088/2053-2571/ab0281ch6.
URL http://dx.doi.org/10.1088/2053-2571/ab0281ch6.
Uri Shaham, Yutaro Yamada, and Sahand Negahban. Understanding adversarial training: Increasing
local stability of supervised models through robust optimization. Neurocomputing, 307:195-204,
2018.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with
principled adversarial training. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=Hk6kPgZA-.
Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Constructing unrestricted adversarial
examples with generative models. In Advances in Neural Information Processing Systems 31:
Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December
2018, MOntreal, Canada., pp. 8322-8333, 2018. URL http://papers.nips.cc/paper/
8052-constructing-unrestricted-adversarial-examples-with-generative-models.
11
Under review as a conference paper at ICLR 2020
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations, 2014. URL http://arxiv.org/abs/1312.6199.
Bing Yu, Jingfeng Wu, and Zhanxing Zhu. Tangent-normal adversarial regularization for semi-
supervised learning. CoRR, abs/1808.06088, 2018. URL http://arxiv.org/abs/1808.
06088.
Junbo Zhao, Yoon Kim, Kelly Zhang, Alexander Rush, and Yann LeCun. Adversarially regularized
autoencoders. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,
pp. 5902-5911, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018a. PMLR. URL http:
//proceedings.mlr.press/v80/zhao18b.html.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. InfoVAE: Information maximizing variational
autoencoders. CoRR, abs/1706.02262, 2017. URL http://arxiv.org/abs/1706.02262.
Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. In
International Conference on Learning Representations, 2018b. URL https://openreview.
net/forum?id=H1BLjgZCb.
12
Under review as a conference paper at ICLR 2020
Appendix A: Discussion & Adversarial examples
Posterior Formulation. Similar to (Kim et al., 2018), We formalize p(θ∣D) for every θ ∈ Θ as:
p(θ∣D) Y p(D∣θ)p(θ) = Y p(z∣x; θ)p(θ) where X ∈ D and Z is generated using Algorithm 1
(x,z)
=Y N(z|fw(x),γ-1 )N(WIfn(ξ),λ-1 )Gamma(γ|a, b)Gamma(λ∣a0, b0)
(x,z)
For every θ0 ∈ Θ0, We compute p(θ0∣D) the same way. Note that θ (resp. θ0) consists in fact of
network parameters W 〜fn (resp. W0 〜 fn，) and scaling parameters Y and λ. For notational
simplicity, we used before the shorthands θ 〜fn and θ0 〜 fn，. The parameters Y and λ are initially
sampled from a Gamma distribution and updated as part of the learning process. In our experiments,
we set the hyper-parameters of the Gamma distributions a and b to 1.0 and 0.1, and a0 and b0 to 1.0.
Latent Noise Level. We measure the amount of noise ∆ we inject into the latent codes of our inputs
by computing the average spectral norm of the latent codes of their adversarial counterparts. The
input changes are captured by our reconstruction loss which is bounded by attack (see Equation 6). For
MNIST, CelebA, and SVHN, the noise levels are 0.004 ± 0.0003, 0.026 ± 0.005, and 0.033 ± 0.008.
The takeaways are: (i.) they are imperceptible, and (ii.) the distributions that Θ and Θ0 follow are
similar. To validate (ii.), we compute the marginals of clean and perturbed latent codes randomly
sampled from Θ and Θ0 . As shown in Figure 5, the marginal distributions overlap relatively well.
(a) MNIST	(b) CelebA	(c) SVHN	(d) SNLI
Figure 5: Marginal distributions of clean (blue) and perturbed (red) latent codes over few minibatches.
Discussion. We discuss the choices pertaining to the design of our approach and their limitations.
We discuss also the evaluation process of our approach against (Song et al., 2018; Zhao et al., 2018b).
Space/Time Complexity. As noted in (Jimenez Rezende & Mohamed, 2015), the Gaussian prior
assumption in VAEs might be too restrictive to generate meaningful enough latent codes (Zhao
et al., 2017). To relax this assumption and produce informative and diverse latent codes, we used
SVGD. To generate manifold preserving adversarial examples, we proposed GBSM. Both SVGD and
GBSM maintain a set of M model instances. As ensemble methods, both inherit the shortcomings
of ensemble models most notably in space/time complexity. Thus, instead of maintaining 2 * M
model instances, we maintain only fη and fη0 from which we sample these model instances. We
experimented with M set to 2, 5, 10 and 15. As M increases, we notice some increase in sample
quality at the expense of longer runtimes. The overhead that occurs as M takes on larger values
reduces, however, drastically during inference as we need only fη0 to sample the model instances
θm0 ∈ Θ0 in order to construct adversarial examples. One way to alleviate the overhead during training
is to enforce weight-sharing for θm ∈ Θ and θm0 ∈ Θ0 . However, we did not try this out.
Preserving Textual Meaning. To construct adversarial text, we experimented with three architecture
designs for the decoder pφ : (i.) a transpose CNN, (ii.) a language model, and (iii.) the decoder
of a pre-trained ARAE model (Zhao et al., 2018a). The transpose CNN generates more legible
text than the other two designs although we notice sometimes some changes in meaning in the
generated adversarial examples. Adversarial text generation is challenging in that small perturbations
in the latent codes can go unnoticed at generation whereas high noise levels can render the outputs
nonsensical. To produce adversarial sentences that faithfully preserve the meaning of the inputs, we
need good sentence generators, like GPT (Radford, 2018), trained on large corpora. Training such
large language models requires however time and resources. Furthermore, in our experiments, we
considered only a vocabulary of size 10,000 words and sentences of length no more than 10 words to
align our evaluation with the experimental choices of (Zhao et al., 2018b).
13
Under review as a conference paper at ICLR 2020
Measuring Perceptual Quality is desirable when the method relied upon to generate adversarial
examples uses GANs or VAEs; both known to produce often samples of limited quality. As Song
et al. (2018) perform unrestricted targeted attacks — their adversarial examples might totally differ
from the true inputs — and Zhao et al. (2018b) do not target certified defenses, a fair side-by-side
comparison of our results and theirs using metrics like mutual information or frechet inception
distance, seems unachievable. Thus, to measure the quality of our adversarial examples and compare
our results with (Song et al., 2018) and (Zhao et al., 2018b), we carried out the pilot study.
Adversarial Images: CelebA
Table 5:	CelebA samples, their clean reconstructions, and adversarial examples (in red boxes).
Inputs
Adversarial
Examples
Clean
Recon-
STRUCTIONS
14
Under review as a conference paper at ICLR 2020
Adversarial Images: SVHN
Here, we provide few random samples of non-targeted adversarial examples we generate with our
approach on the SVHN dataset as well as the clean reconstructions.
Table 6:	SVHN. Images in red boxes are all adversarial.
Clean
Recon-
STRUCTIONS
Adversarial
Examples
15
Under review as a conference paper at ICLR 2020
Adversarial Images: MNIST
Here, we provide few random samples of non-targeted adversarial examples we generate with
our approach on the MNIST dataset as well as the clean reconstructions. Both the reconstructed
and the adversarial images look realistic although we notice some artifacts on the latter. Basic
Iterative Methods (Kurakin et al., 2016), among others, also suffer from this. Because the marginal
distributions of the latent codes of the inputs and their perturbed versions overlap, we conclude that
indeed the adversarial images preserve the manifold of the inputs (see Figure 5).
Table 7: MNIST. Images in red boxes are all adversarial.
Inputs
U,或6t9r*∙彳
9G 37O
W 6 ? 305 /
ogy9Tq/2
/<///373
527bΓo CP
夕—夕，COgfq
937c>5a fsʃ
4 Yr'3以O 5
73/，夕/4
, ∕fq∖73 夕
7q Nog夕6 6
423?Z3 qz2
Q<5√^ 67夕 3 /
夕ʃQ6 4 S CCZ2
t(owJ?夕贯 gz
Clean
Recon-
STRUCTION
U92夕9n-夕
9G S 7—ʃð
WsOb 3 C 5 /
o∖s997rq∕ΛZ
/5///373
3975厂”Λoo
9I夕？3414
,37o537>夕
4 q「co。O 5
73/7夕/0
夕 ∕fq>73夕
7q a O gʃs 6
4R3>L3 <〃
{Γ54∖3795/
夕 Sq64scc^
t(0£ :，Mgy
Adversarial
Examples
U 9而夕夕Γ*- q
9G 07lʃQ
AS ? 3 夕 5 ,
e 孑99qq∕2
/r/ / / 3 7 3
5 >，S 厂。P-
夕F夕夕004Iq
£37 0537>s
Car∙¾X3 Os
73/，夕/C
夕/gq>75夕
Oq 2夕6 6
4Rm>zw q夕
Cr5/6795/
夕 6qcqsc‹■夕
E(o4J?, 4 g，
16
Under review as a conference paper at ICLR 2020
Adversarial Text: SNLI
Table 8: Examples of adversarially generated hypotheses with the true premises kept unchanged.
True Input 1	P: A white dog is running through the snow. H: A CAT STALKING THROUGH THE SNOW. Label: CONTRADICTION
Adversary	H’: A CAT HOPS IN THE SNOW. Label: NEUTRAL
True Input 2	P: Three dogs are searching for something outside. H: THERE ARE FOUR DOGS. Label: CONTRADICTION
Adversary	H’: THERE ARE FIVE DOGS. Label: NEUTRAL
True Input 3	P: A man waterskis while attached to a parachute. H: A BULLDOZER KNOCKS DOWN A HOUSE. Label: CONTRADICTION
Adversary	H’: A BULLDOZER KNOCKS DOWN A CAGE. Label: ENTAILMENT
True Input 4	P: A little girl playing with flowers. H: A LITTLE GIRL PLAYING WITH A BALL. Label: CONTRADICTION
Adversary	H’: A LITTLE GIRL IS RUNNING WITH A BALL. Label: NEUTRAL
True Input 5	P: People stand in front of a chalkboard. H: PEOPLE STAND OUTSIDE A PHOTOGRAPHY STORE.
Adversary	Label: CONTRADICTION H’: PEOPLE STAND IN FRONT OF A WORKSHOP. Label: NEUTRAL
True Input 6	P: Musician entertaining his audience. H: THE WOMAN PLAYED THE TRUMPET.
Adversary	Label: CONTRADICTION H’: THE WOMAN PLAYED THE DRUMS. Label: ENTAILMENT
True Input 7	P: A kid on a slip and slide. H: A SMALL CHILD IS INSIDE EATING THEIR DINNER.
Adversary	Label: CONTRADICTION H’: A SMALL CHILD IS EATING THEIR DINNER. Label: ENTAILMENT
True Input 8	P: A deer jumping over a fence. H: A DEER LAYING IN THE GRASS. Label: CONTRADICTION
Adversary	H’: A PONY LAYING IN THE GRASS. Label: ENTAILMENT
True Input 9	P: Two vendors are on a curb selling balloons. H: THREE PEOPLE SELL LEMONADE BY THE ROAD SIDE.
Adversary	Label: CONTRADICTION H’: THREE PEOPLE SELL ARTWORK BY THE ROAD SIDE. Label: ENTAILMENT
17
Under review as a conference paper at ICLR 2020
Table 9: Some generated examples deemed adversarial by our method that are not.
True Input 1	P: A man is operating some type of a vessel. H: A DOG IN KENNEL.
Generated	Label: CONTRADICTION H’: A DOG IN DISGUISE. Label: CONTRADICTION
True Input 2	P: A skier. H: SOMEONE IS SKIING.
Generated	Label: ENTAILMENT H’: MAN IS SKIING. Label: NEUTRAL
True Input 3	P: This is a bustling city street. H: THERE ARE A LOT OF PEOPLE WALKING ALONG. Label: ENTAILMENT
Generated	H’: THERE ARE A LOT GIRLS WALKING ALONG. Label: NEUTRAL
True Input 4	P: A soldier is looking out of a window. H: THE PRISONER’S CELL IS WINDOWLESS.
Generated	Label: CONTRADICTION H’: THE PRISONER’S HOME IS WINDOWLESS. Label: CONTRADICTION
True Input 5	P: Four people sitting on a low cement ledge. H: THERE ARE FOUR PEOPLE. Label: ENTAILMENT
Generated	H’: THERE ARE SEVERAL PEOPLE. Label: NEUTRAL
True Input 6	P: Three youngsters shovel a huge pile of snow. H: CHILDREN WORKING TO CLEAR SNOW. Label: ENTAILMENT
Generated	H’: KIDS WORKING TO CLEAR SNOW. Label: NEUTRAL
True Input 7	P: Boys at an amphitheater. H: BOYS AT A SHOW.
Generated	Label: ENTAILMENT H’: BOYS IN A SHOW. Label: NEUTRAL
True Input 8	P: Male child holding a yellow balloon. H: BOY HOLDING BIG BALLOON. Label: NEUTRAL
Generated	H’: BOY HOLDING LARGE BALLOON. Label: NEUTRAL
True Input 9	P: Women in their swimsuits sunbathe on the sand. H: WOMEN UNDER THE SUN ON THE SAND. Label: ENTAILMENT
Generated	H’: FAMILY UNDER THE SUN ON THE SAND. Label: NEUTRAL
18
Under review as a conference paper at ICLR 2020
Appendix B: Experimental Settings
Table 10: Model Configurations + SNLI Classifier + Hyper-parameters.
	NAME	Configuration
	 Recognition Networks	fη	Input Dim: 50, Hidden Layers: [60, 70], ouTpuT DiM: NuM WEiGHTs & BiAsEs iN θm
	fη	Input Dim: 50, Hidden Layers: [60, 70], ouTpuT DiM: NuM WEiGHTs & BiAsEs iN θm0
Model Instances	PARTICLES θm	iNpuT DiM: 28 × 28 (MNisT), 64 × 64 (CELEBA), 32 × 32 (sVHN), 300 (sNLi) Hidden Layers: [40, 40] Output Dim (latent code): 100
	PARAMETERS θm0	iNpuT DiM: 28 × 28 (MNisT), 64 × 64 (CELEBA), 32 × 32 (sVHN), 100 (sNLi) Hidden Layers: [40, 40] Output Dim (latent code): 100
Feature Extractor	INPUT DIM: 28 × 28 × 1 (MNIST), 64 × 64 × 3 (CELEBA), 32 × 32 × 3 (SVHN), 10 × 100 (SNLI) Hidden Layers: [40, 40] OUTPUT DIM: 28 × 28 (MNIST), 64 × 64 (CELEBA), 32 × 32 (SVHN), 100 (SNLI)	
Decoder	Transpose CNN	For CelebA & sVHN: [filters: 64, stride: 2, KERNEL: 5]× 3 For sNLi: [filters: 64, stride: 1, KERNEL: 5]× 3
	Language Model	Vocabulary Size: 11,000 words Max Sentence Length: 10 words
SNLI classifier	Input Dim: 200, Hidden Layers: [100, 100, 100], Output Dim: 3	
Learning Rates	ADAM Optimizer (δ = 5.10-4),α = 10-3, β = β0 = 10-2	
More settings	Batch size: 64, Inner-updates: 3, Training epochs: 1000, M = 5	
19