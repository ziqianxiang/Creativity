Under review as a conference paper at ICLR 2020
Simultaneous Attributed Network Embedding
and Clustering
Anonymous authors
Paper under double-blind review
Ab stract
To deal simultaneously with both, the attributed network embedding and cluster-
ing, we propose a new model. It exploits both content and structure information,
capitalising on their simultaneous use. The proposed model relies on the approxi-
mation of the relaxed continuous embedding solution by the true discrete cluster-
ing one. Thereby, we show that incorporating an embedding representation pro-
vides simpler and more interpretable solutions. Experiment results demonstrate
that the proposed algorithm performs better, in terms of clustering and embed-
ding, than the state-of-art algorithms, including deep learning methods devoted to
similar tasks for attributed network datasets with different proprieties.
1	Introduction
In recent years, Attributed Networks (AN) (Qi et al., 2012) have been used to model a large vari-
ety of real-world networks, such as academic and health care networks where both node links and
attributes/features are available for analysis. Unlike plain networks in which only node links and
dependencies are observed, with AN, each node is associated with a valuable set of features.
More recently, the learning representation has received a significant amount of attention as an im-
portant aim in many applications including social networks, academic citation networks and protein-
protein interaction networks. Hence, Attributed network Embedding (ANE) (Cai et al., 2018; Yan
et al., 2007; Tang et al., 2015) aims to seek a continuous low-dimensional matrix representation for
nodes in a network, such that original network topological structure and node attribute proximity
can be preserved in the new low-dimensional embedding.
Although, many approaches have emerged with Network Embedding (NE), the research on ANE
(Attributed Network Embedding) is still remains to be explored (Chang et al., 2015). Unlike NE that
learns from plain networks, ANE aims to capitalize both the proximity information of the network
and the affinity of node attributes. Note that, due to the heterogeneity of the two information sources,
it is difficult for the existing NE algorithms to be directly applied to ANE.
To sum up, the learned representation has been shown to be helpful in many learning tasks such as
network clustering (Wang et al., 2017), nodes visualization (Dai et al., 2018), nodes classification
(Zhu et al., 2007; Dai et al., 2018; Huang et al., 2017) and link prediction (Singh & Gordon, 2008;
Pan et al., 2018). Therefore ANE is a challenging research problem due to the high-dimensionality,
sparsity and non-linearity of the graph data.
2	Related Work
Frequently, embedding and clustering are used to better understand the content and structure infor-
mation from the clusters. Many approaches have been proposed for the learning representation and
clustering tasks (Bock, 1987; De Soete & Carroll, 1994; Vichi & Kiers, 2001; Yamamoto & Hwang,
2014) without available information from a network. With ANE we start to list some works (Pan
et al., 2018; Guo et al., 2019) where the proposed algorithms show the benefits of clustering. Al-
though existing AN clustering has been widely applied, they may achieve poor performance due to
the following drawbacks:
1
Under review as a conference paper at ICLR 2020
-	High risk of severe deviation of approximate continuous embedding solution from the good
discrete clustering.
-	Information loss among separate independent stages, i.e., continuous embedding genera-
tion and embedding discretization.
These problems result from the sequential way where the learned representation is obtained before
obtaining clusters using a technical clustering. This is implicitly due to the fact that the two tasks
do not aim to reach the same objective and, in addition, are carried out separately. To remedy
this weakness. we propose a novel simultaneous ANE and clustering scheme which jointly; (1)
learns embedding from both network and attributes information (2) learns continuous embedding
and discrete clustering labels. Specifically, we explicitly enforce a discrete transformation on the
intermediate continuous labels (embedding), which leads to a tractable optimization problem with a
discrete solution. The key challenge is to know how to integrate the information of both node links
and attributes for simultaneous node representation learning and discrete node clustering. In order to
alleviate the influence caused by the information loss during the relaxation of sequential clustering,
then to recover a discrete clustering solution, we use a smooth transformation (e.g., rotation) from
the relaxed continuous embedding to a discrete solution. In this sense, the continuous embedding
only serves as an intermediate product.
To our best knowledge, the adoption of simultaneous attributed network embedding and clustering
in a unified learning framework which has not been adequately investigated yet. The goal of this
work is to conduct investigations along this direction by considering matrix decomposition as the
embedding framework.
The rest of the paper is organized as follows. Section 2 introduces the AN embedding problem and
clustering for community detection. Section 3 provides a sound Simultaneous Attributed Network
Embedding and Clustering (SANEC) framework for embedding and clustering. Section 4 is devoted
to numerical experiments. Finally, the conclusion summarizes the advantages of our contribution.
3	Proposed method
In this section, we descibe the Simultaneous Attributes Network Embedding and Clustering
(SANEC) method. We will present the formulation of an objective function and an effective al-
gorithm for data embedding and clustering. But first, we show how to construct two matrices S and
M integrating both types of information -content and structure information- to reach our goal.
3.1	Content and Structure information
An attributed network G = (V, E, X) consists of V the set of nodes, E ⊆ V × V the set of links,
and X = [x1, x2, . . . , xn] where n = |V | and xi ∈ Rd is the feature/attribute vector of the node
vi . Formally, the graph can be represented by two types of information, the content information
X ∈ Rn×d and the structure information A ∈ Rn×n , where A is an adjacency matrix of G and
aij = 1 if eij ∈ E otherwise 0; we consider that each node is a neighbor of itself, then we set
aii = 1 for all nodes. Thereby, we model the nodes proximity by an (n × n) transition matrix W
given by W = D-1A, where D is the degree matrix of A defined by dii = Pin0=1 ai0i.
In order to exploit additional information about nodes similarity from X, we preprocessed the above
dataset X to produce similarity graph input WX of size (n × n); we construct a K-Nearest-Neighbor
(KNN) graph. To this end, we use the heat kernel and L2 distance, KNN neighborhood mode with
K = 15 and we set the width of the neighborhood σ = 1. Note that any appropriate distance or
dissimilarity measure can be used. Finally we combine in an (n × n) matrix S, nodes proximity
from both content information X and structure information W. In this way, we propose to perturb
the similarity W by adding the similarity from WX ; we choose to take S defined by
S = W + WX	(1)
In Figure 3.1, we illustrate the impact ofWX by applying Multidimensional scaling on both W and
S. Note that with S, the sparsity is overcome by the presence of WX . We will see later the interest
of its use in S.
2
Under review as a conference paper at ICLR 2020
-0.4	-0.4
Figure 1: MDS on W (left) and S (right): Cora dataset where W is of size (2708 × 2708) with true 7 clusters
As we aim to perform clustering, we propose to integrate it in the formulation of a new data repre-
sentation by assuming that nodes with the same label tend to similar social relations and similar node
attributes. This idea is inspired by the fact that, the labels are strongly influenced by both content
and structure information and inherently correlated to these both information sources. This reminds
us that the idea behind the Canonical Discriminant Analysis (CDA) which is a dimension-reduction
technique related to principal component analysis (PCA) and canonical correlation (Goodfellow
et al., 2016). Given groups of observations with measurements on attributes, CDA derives a linear
combination of the variables that has the highest possible multiple correlation with the groups. It
can be viewed as a particular PCA where the observations belonging to a same group are replaced
by their centroid. Thereby the new data representation referred to as M = (mij ) of size (n × d)
can be considered as a multiplicative integration of both W and X by replacing each node by the
centroid of their neighborhood (barycenter): i.e, mij = kn=1 wikxkj , ∀i, j or
M=WX.	(2)
In Figure 3.1, it is interesting to point out the impact of W in the formulation of M. We apply
CDA on X and M and indicate the seven true clusters of the Cora dataset. This leads to show
clusters better separated with M than with X and therefore that W already does a good job without
clustering.
Figure 2: CDA on X (left) and M (right): Cora dataset where X is of size (2708 × 1433) with true 7 clusters
In this way, given a graph G, a graph clustering aims to partition the nodes in G into k disjoint
clusters {C1, C2, ..., Ck}, so that: (1) nodes within the same cluster are close to each other while
nodes in different clusters are distant in terms of graph structure; and (2) the nodes within the same
cluster are more likely to have similar attribute values.
3.2	Model
Let k be the number of clusters and the number of components into which the data is embedded.
With M and S, the SANEC method that we propose aims to obtain the maximally informative em-
bedding according to the clustering structure in the attributed network data. Therefore, the proposed
3
Under review as a conference paper at ICLR 2020
objective function to optimize is given by
min M - BQ>2 + λS - GZB>2 B>B = I,Z>Z = I,G ∈ {0, 1}n×k (3)
B,Z,Q,G
where G = (gij ) of size (n × k) is a cluster membership matrix, B = (bij ) of size (n × k) is
the embedding matrix and Z = (zij) of size (k × k) is an orthonormal rotation matrix which most
closely maps B to G ∈ {0, 1}n×k. Q ∈ Rd×k is the features embedding matrix. Finally, The
parameter λ is a non-negative value and can be viewed as a regularization parameter.
The intuition behind the factorization of M and S is to encourage the nodes with similar proximity,
those with higher similarity in both matrices, to have closer representations in the latent space given
by B. In doing so, the optimisation of (3) leads to a clustering of the nodes into k clusters given by
G. Note that, both tasks -embedding and clustering- are performed simultaneously and supported
by Z; it is the key to attaining good embedding while taking into account the clustering structure.
3.3	Optimization
To infer the latent factor matrices Z, B, Q and G, we shall derive an alternating optimization
algorithm. To this end, we rely on the following proposition.
Proposition 1 LetbeS ∈ Rn×n, G ∈ {0, 1}n×k, Z ∈ Rk×k, B ∈ Rn×k,wehave
S - GZB>2 = S - BB>S2 + kSB - GZk2	(4)
Proof. We first expand the matrix norm of the left term of (4)
S - GZBT 2 = kSk2 + GZB>2 - 2T r(SGZB>)	(5)
In a similar way, we obtain from the two terms of the right term of (4)
S - SBBT 2 = kSk2 + SBB>2 - 2T r(SBB>S>)
= kSk2 + SBB>2 - 2||SB||2
= ||S||2 - ||SB||2 due to B>B = I	(6)
and kSB - GZk2 = kSBk2 + kGZk2 - 2Tr(SBZG>).
Due also to B>B = I, we have
kSB - GZk2 = ||SB||2 + ||GZB>||2 - 2Tr(SGZB>)	(7)
Summing the two terms of (6) and (7 ) leads to the left term of (4).
kSk2 + kGZk2 - 2T r(SGZB>) = S - GZBT 2	(8)
due to ∣∣GZk2 = ∣∣GZB>∣∣2.	口
Compute Z. Fixing G and B the problem which arises in (3) is equivalent to minZ S - GZB> 2.
From proposition 1., we deduce that
min ∣∣S - GZB>∣∣2 ⇔ min ∣∣S - BB>S∣∣2 + ∣SB - GZ∣2	(9)
which can be reduced to maxZ T r(G>SBZ) s.t. Z>Z = I. As proved in page 29 of ten Berge
(1993), let UΣV> be the SVD for G>SB, then
Z= UV>.	(10)
We can observe that this problem turns out to be similar to the well known orthogonal Procrustes
problem (Schonemann, 1966).
4
Under review as a conference paper at ICLR 2020
Compute Q.	Given G, Z and B, the opimization problem (3) is equivalent to
minQ M - BQ> 2, and we get
Q = M>B.
(11)
Thereby Q is somewhere an embedding of attributes.
Compute B. Given G, Q and Z, the problem (3) is equivalent to
max Tr((M>Q + λSGZ)B>) s.t. B>B = I.
B
In the same manner for the computation of Z, let UrΣV> be the SVD for (M>Q + λSGZ), We get
ʌ ʌ ~Γ
B = U V >.
(12)
It is important to emphasize that, at each step, B exploits the information from the matrices Q, G,
and Z. This highlights one of the aspects of the simultaneity of embedding and clustering.
Compute G: Finally, given B, Q and Z, the problem (3) is equivalent to minG kSB - GZk2 . As
G is a cluster membership matrix, its computation is done as folloWs: We fix Q, Z, B. Let B = SB
and calculate
1 1 if k = arg mink，||bi - z^ ||2
0 otherWise.
(13)
3.4	SANEC algorithm
In summary, the steps of the SANEC algorithm relying on S referred to as SANECS can be deduced
in Algorithm 1. The convergence of SANECS is guaranteed and depends on the initialization to
reach only a local optima. Hence, We start the algorithm several times and select the best result
Which minimizes the objective function (3).
Algorithm 1 : SANECS algorithm
Input: M and S from structure matrix W and content matrix X ;
Initialize: B, Q and Z With arbitrary orthonormal matrix;
repeat
(a)	- Compute G using (13)
(b)	- Compute B using (12)
(c)	- Compute Q using (11)
(d)	- Compute Z using (10)
until convergence
Output: G: clustering matrix, Z: rotation matrix, B: nodes embedding and Q: attributes em-
bedding
4	Numerical experiments
In our Work We focus on different clustering methods. In the sequel, We evaluate the SANEC algo-
rithm With some competitive methods including recent deep learning methods.
4.1	Compared methods
We compare our method With both embedding based approaches as Well as approaches directly for
graph clustering. We consider classical methods and also deep learning methods; they differ in the
use of available information. Some of them rely only on X such as K-means and others more recent
on X and W. Graph Encoder (Tian et al., 2014) learns graph embedding for spectral graph clus-
tering. DNGR (Cao et al., 2016) trains a stacked denoising autoencoder for graph embedding. RTM
(Chang & Blei, 2009) learns the topic distributions of each document from both text and citation.
RMSC(Xia et al., 2014) employs a multi-vieW learning approach for graph clustering. TADW (Yang
5
Under review as a conference paper at ICLR 2020
et al., 2015) applies matrix factorization for network representation learning. DeepWalk (Perozzi
et al., 2014) is a network representation approach which encodes social relations into a continuous
embedding space. Spectral Clustering (Tang & Liu, 2011) is a widely used approach for learning
social embedding. GAE (Kipf & Welling, 2016) is an autoencoder-based unsupervised framework
for attributed network data embedding. VGAE (Kipf & Welling, 2016) is a variational graph autoen-
coder approach for graph embedding with both node links and node attributes information. ARGA
(Pan et al., 2018) is the most recent adversarially regularized autoencoder algorithm which uses
graph autoencoder to learn the embedding. ARVGA (Pan et al., 2018) algorithm, which uses a varia-
tional graph autoencoder to learn the embedding. For embedding based approaches, we first learn the
graph embedding, and then perform k-means clustering algorithm based on the embedding. For
our proposed method, we set the regularization parameter λ ∈ {0, 10-6, 10-3, 10-1, 100, 101, 103},
and choose the best values as the final results.
4.2	Datasets
The performances of clustering methods are evaluated using real-world datasets commonly tested
with ANE where the clusters are known. Specifically, we consider three public citation network data
sets, Citeseer, Cora and Wiki, which contain sparse bag-of-words feature vector for each document
and a list of citation links between documents. Each document has a class label. We treat documents
as nodes and the citation links as the edges. The characteristics of the used datasets are summarized
in Table 1. The balance coefficient is defined as the ratio of the number of documents in the smallest
class to the number of documents in the largest class while nz denotes the percentage of sparsity.
Table 1: Description of datasets (#: the cardinality)						
datasets	# Nodes	# Attributes	# Edges	#Classes	nz(%)	Balance
Cora	2708^^	1433	-^5294	7	98.73	0.22
Citeseer	3312	3703	4732	6	99.14	0.35
Wiki	2405	4973	17981	17	86.46	0.02
4.3 SENSITIVITY ANALYSIS OF λ
With the SANEC model, the parameter λ controls the role of the second term ||S - GZB>||2
in (3). To measure its impact on the clustering performance of SANECS, we vary λ in
{0, 10-6, 10-3, 10-1, 100, 101, 103}. The performances in terms of accuracy (Acc), normalized
mutual information (NMI) and adjusted rand index are depicted in Figure 4; a Acc, NMI or
ARI corresponds to a better clustering result. First we note that with λ = 0 we rely only on
minB,Q M - BQ>2s.t. B>B = I. In this case, we observe poor results in terms of quality
of clustering, this leads to show the impact of the second term in (3). This quality increases accord-
ing to λ and we get better performance on all datasets with small values of λ. Around 10-2, it is
worthy to note that the clustering result is stable and less sensitive to λ. However, the performance
of SANEC degrades sharply for λ greater than 10, this can be explained by the fact that the initializa-
tion of G which is random and can often be far from the real solution. Through, many experiments
on the three datasets and others not reported here, we choose to take λ = 10-2.
4.4	Attributed network clustering
Compared to the true available clusters, in our experiments the clustering performance is assessed
by ACC NMI and ARI. We repeat the experiments 50 times and the averages (mean) are reported
in Table 2; the best performance for each dataset is highlighted in bold. First, we observe the
high performances of methods integrating information from W. For instance, RTM and RMSC are
better than classical methods using only either M or W. On the other hand, all methods including
deep learning algorithms relying on M and W are better yet. However, regarding SANEC with
both versions relying on W, referred to as SANECW or S referred to as SANECS, we note high
performances for all the datasets and with SANECS, we remark the impact of WX; it learns low-
dimensional representations while suits the clustering structure. To go further in our investigation
and given the sparsity of X we proceeded to standardization tf-idf followed by L2, as it is often used
to process document-term matrices; see e.g;, (Salah & Nadif, 2017; 2019), while in the construction
6
Under review as a conference paper at ICLR 2020
Cora
Citeseer
Figure 3: Sensitivity analysis of λ using Acc, NMI and ARI.
Wiki
of WX we used the cosine metric. In Figure 4 are reported the results where we observe a slight
improvement.
Table 2: Clustering performances (Acc % , NMI % and ARI % ) on Cora, Citeseer and Wiki datasets
Methods	Input	Datasets								
		Cora			Citeseer			Wiki		
		ACC	NMI	ARI	Acc	NMI	ARI	Acc	NMI	ARI
K-means	X^	49.22	32.10	22.96	54.01	30.54	27.86	41.72	44.02	15.07
Spectral	W	36.72	12.67	03.11	23.89	05.57	01.00	22.04	18.17	01.46
GraphEncoder	W	32.49	10.93	00.55	22.52	03.30	01.00	20.67	12.07	0.49
DeepWalk	W	48.40	32.70	24.27	33.65	08.78	09.22	38.46	32.38	17.03
DNGR	W	41.91	31.84	14.22	32.59	18.02	04.29	37.58	35.85	17.97
RTM	X, W	43.96	23.01	16.91	45.09	23.93	20.26	43.64	44.95	13.84
RMSC	X, W	40.66	25.51	08.95	29.50	13.87	04.88	39.76	41.50	11.16
TAWD	X, W	56.03	44.11	33.20	45.48	29.14	22.81	30.96	27.13	04.54
VGAE	X, W	50.20	32.92	25.47	46.70	26.05	20.56	45.09	46.76	26.34
ARGE	X, W	64.0	44.9	35.2	57.3	35.0	34.1	47.34	47.02	28.16
ARVGE	X, W	63.8	45.0	37.74	54.4	26.1	24.5	46.45	47.8	29.65
SANECW	X, W	64.47	43.30	36.19	64.71	38.61	39.20	46.21	42.83	28.30
SANECS	X, S	67.38	47.14	39.88	66.77	40.60	41.78	52.80	50.02	35.57
ESANEC_l2
SANECjfidf
60
I^MSANECj2~I
I	ISANEJtfidf
1	50
(%}0UeE-lod 6u--ln
Cora	Citeseer	Wiki
Figure 4: Evaluation of SANECS using tf-idf normalization of X and cosine metric for WX .
4.5	Attributed network visualization
The SANEC model, through B, offers an embedding from which we can also observe a 2d or
3d structure into clusters. To illustrate the quality of embedding, we consider the three attributed
network datasets described above and we use UMAP for data visualization (McInnes et al., 2018).
The UMAP algorithm leads to dimension reduction based on manifold learning techniques and
ideas from topological data analysis. As in the construction of WX , the number of neighbors that
we have chosen with UMAP is equal to 30. Figure 4.5 McInnes et al. (2018) shows the visualization
obtained on X, on M and finally on B, we easily observe the capability of SANECS to distinguish
the cluster-based structures compared to visualization with the true clusters based on X or M.
7
Under review as a conference paper at ICLR 2020
X
Figure 5: From top to bottom and from left to right, UMAP applied on X, M and B respectively.
M
Citeseer dataset with 6 clusters
Wiki dataset with 19 clusters
B
5	Conclusion
In this paper, we proposed a novel matrix decomposition framework for simultaneous attributed net-
work data embedding and clustering. Unlike known methods that combine the objective function
of AN embedding and the objective function of clustering separately, we proposed a new single
framework to perform SANECS for AN embedding and nodes clustering. We showed that the opti-
mized objective function can be decomposed into three terms, the first is the objective function of a
kind of PCA applied to M, the second is the graph embedding criterion in a low-dimensional space,
and the third is the clustering criterion. We also integrated a discrete rotation functionality, which
allows a smooth transformation from the relaxed continuous embedding to a discrete solution, and
guarantees a tractable optimization problem with a discrete solution. Thereby, we developed an ef-
fective algorithm capitalizing on learning representation and clustering. The obtained results show
the advantages of combining both tasks over other approaches. SANECS outperforms the all recent
methods devoted to the same tasks including deep learning methods which require deep models
pretraining.
The proposed framework offers several perspectives and investigations. We have noted that the
construction of M and S is important, it highlights the introduction of W. As for the WX we have
observed that it is fundamental as it makes possible to link the information from X to the network;
this has been verified by many experiments. Finally, as we have stressed that Q is an embedding of
attributes, this suggests to consider also a simultaneously ANE and co-clustering.
8
Under review as a conference paper at ICLR 2020
References
H.-H. Bock. On the interface between cluster analysis, principal component analysis, and multidi-
mensional scaling. In MuItivariate statistical modeling and data analysis, pp. 17-34. Springer,
1987.
HongYun Cai, Vincent W. Zheng, and Kevin Chen-Chuan Chang. A comprehensive survey of graph
embedding: Problems, techniques, and applications. IEEE Trans. Knowl. Data Eng., 30(9):1616-
1637, 2018.
Shaosheng Cao, Wei Lu, and Qiongkai Xu. Deep neural networks for learning graph representations.
2016.
Jonathan Chang and David Blei. Relational topic models for document networks. In David van
Dyk and Max Welling (eds.), Proceedings of the Twelth International Conference on Artificial
Intelligence and Statistics, volume 5, pp. 81-88, 2009.
Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C. Aggarwal, and Thomas S. Huang. Het-
erogeneous network embedding via deep architectures. In Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, KDD ’15, pp. 119-128,
2015. ISBN 978-1-4503-3664-2.
Quanyu Dai, Qiang Li, Jian Tang, and Dan Wang. Adversarial network embedding. In Proceedings
of the Thirty-Second AAAI Conference on Artificial Intelligence, 2-7, 2018, pp. 2167-2174, 2018.
G. De Soete and JD Carroll. K-means clustering in a low-dimensional euclidean space. In New
approaches in classification and data analysis, pp. 212-219. 1994.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.
T. Guo, S. Pan, X. Zhu, and C. Zhang. Cfond: Consensus factorization for co-clustering networked
data. IEEE Transactions on Knowledge Data Engineering, 31(04):706-719, 2019. ISSN 1041-
4347.
Xiao Huang, Jundong Li, and Xia Hu. Label informed attributed network embedding. In Proceed-
ings of the Tenth ACM International Conference on Web Search and Data Mining, WSDM ’17,
pp. 731-739, 2017. ISBN 978-1-4503-4675-7.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. NIPS Workshop on Bayesian
Deep Learning, 2016.
Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and
projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.
Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang. Adversarially
regularized graph autoencoder for graph embedding. In Proceedings of the Twenty-Seventh In-
ternational Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm,
Sweden., pp. 2609-2615, 2018.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre-
sentations. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’14, 2014.
Guo-Jun Qi, Charu C. Aggarwal, Qi Tian, Heng Ji, and Thomas S. Huang. Exploring context and
content links in social media: A latent space method. IEEE Trans. Pattern Anal. Mach. Intell., 34
(5):850-862, 2012.
Aghiles Salah and Mohamed Nadif. Model-based von mises-fisher co-clustering with a conscience.
In Proceedings of the 2017 SIAM International Conference on Data Mining, pp. 246-254. SIAM,
2017.
Aghiles Salah and Mohamed Nadif. Directional co-clustering. Advances in Data Analysis and
Classification, 13(3):591-620, 2019.
9
Under review as a conference paper at ICLR 2020
P. Schonemann. A generalized solution of the orthogonal procrustes problem. Psychometrika, 31
(1):1-10,1966.
Ajit P. Singh and Geoffrey J. Gordon. Relational learning via collective matrix factorization. In
Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, KDD ’08, 2008.
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale
information network embedding. In Proceedings of the 24th International Conference on World
Wide Web, WWW ’15, pp. 1067-1077, 2015. ISBN 978-1-4503-3469-3.
Lei Tang and Huan Liu. Leveraging social media networks for classification. Data Min. Knowl.
Discov., 23(3), November 2011.
Jos MF ten Berge. Least squares optimization in multivariate analysis. DSWO Press, Leiden
University Leiden, 1993.
Fei Tian, Bin Gao, Qing Cui, Enhong Chen, and Tie-Yan Liu. Learning deep representations for
graph clustering. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,
AAAI’14, 2014.
M. Vichi and H. Kiers. Factorial k-means analysis for two-way data. Computational Statistics &
Data Analysis, 37(1):49-64, 2001.
Chun Wang, Shirui Pan, Guodong Long, Xingquan Zhu, and Jing Jiang. Mgae: Marginalized graph
autoencoder for graph clustering. In Proceedings of the 2017 ACM on Conference on Information
and Knowledge Management, CIKM ’17, pp. 889-898, 2017. ISBN 978-1-4503-4918-5.
Rongkai Xia, Yan Pan, Lei Du, and Jian Yin. Robust multi-view spectral clustering via low-rank
and sparse decomposition. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial
Intelligence, AAAI’14, 2014.
M. Yamamoto and H. Hwang. A general formulation of cluster analysis with dimension reduction
and subspace separation. Behaviormetrika, 41(1):115-129, 2014.
Shuicheng Yan, Dong Xu, Benyu Zhang, Hong-Jiang Zhang, Qiang Yang, and Stephen Lin. Graph
embedding and extensions: A general framework for dimensionality reduction. IEEE Trans.
Pattern Anal. Mach. Intell., 29(1), January 2007.
Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y. Chang. Network representa-
tion learning with rich text information. In Proceedings of the 24th International Conference on
Artificial Intelligence, IJCAI’15, 2015.
Shenghuo Zhu, Kai Yu, Yun Chi, and Yihong Gong. Combining content and link for classifica-
tion using matrix factorization. In Proceedings of the 30th Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval, SIGIR ’07, 2007.
10