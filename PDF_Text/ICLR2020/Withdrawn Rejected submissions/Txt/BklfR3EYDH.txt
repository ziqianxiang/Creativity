Under review as a conference paper at ICLR 2020
Keyframing the Future:
Discovering Temporal Hierarchy with
Keyframe-Inpainter Prediction
Anonymous authors
Paper under double-blind review
Ab stract
To flexibly and efficiently reason about temporal sequences, abstract representations
that compactly represent the important information in the sequence are needed. One
way of constructing such representations is by focusing on the important events in
a sequence. In this paper, we propose a model that learns both to discover such key
events (or keyframes) as well as to represent the sequence in terms of them. We
do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates
keyframes and their temporal placement and then inpaints the sequences between
keyframes. We propose a fully differentiable formulation for efficiently learning
the keyframe placement. We show that KeyIn finds informative keyframes in
several datasets with diverse dynamics. When evaluated on a planning task, KeyIn
outperforms other recent proposals for learning hierarchical representations.
1	Introduction
When thinking about the future, humans focus their thoughts on the important things that may happen
(When will the plane depart?) without fretting about the minor details that fill each intervening
moment (What is the last word I will say to the taxi driver?). Because the vast majority of elements in
a temporal sequence contains redundant information, a temporal abstraction can make reasoning and
planning both easier and more efficient. How can we build such an abstraction? Consider the example
of a lead animator who wants to show what happens in the next scene of a cartoon. Before worrying
about every low-level detail, the animator first sketches out the story by keyframing, drawing the
moments in time when the important events occur. The scene can then be easily finished by other
animators who fill in the rest of the sequence from the story laid out by the keyframes. In this paper,
we argue that learning to discover such informative keyframes from raw sequences is an efficient and
powerful way to learn to reason about the future.
Our goal is to learn such an abstraction for future image prediction. In contrast, much of the work
on future image prediction has focused on frame-by-frame synthesis (Oh et al. (2015); Finn et al.
(2016)). This strategy puts an equal emphasis on each frame, irrespective of the redundant content it
may contain or its usefulness for reasoning relative to the other predicted frames. Other recent work
has considered predictions that “jump” more than one step into the future, but these approaches either
used fixed-offset jumps (Buesing et al., 2018) or used heuristics to select the predicted frames (Neitz
et al., 2018; Jayaraman et al., 2019; Gregor et al., 2019). In this work, we propose a method that
selects the keyframes that are most informative about the full sequence, so as to allow us to reason
about the sequence holistically while only using a small subset of the frames. We do so by ensuring
that the full sequence can be recovered from the keyframes with an inpainting strategy, similar to
how a supporting animator finishes the story keyframed by the lead.
One possible application for a model that discovers informative keyframes is in long-horizon planning.
Recently, predictive models have been employed for model-based planning and control (Ebert et al.
(2018)). However, they reason about every single future time step, limiting their applicability to short
horizon tasks. In contrast, we show that a model that reasons about the future using a small set of
informative keyframes enables visual predictive planning for horizons much greater than previously
possible by using keyframes as subgoals in a hierarchical planning framework.
1
Under review as a conference paper at ICLR 2020
Keyframe
Predictor
Sequence
Inpainter
Figure 1: Keyframing the future. Instead of predicting one frame after the other, we propose to
represent the sequence with the keyframes that depict the interesting moments of the sequence. The
remaining frames can be inpainted given the keyframes.
To discover informative frames in raw sequence data, we formulate a hierarchical probabilistic model
in which a sequence is represented by a subset of its frames (see Fig. 1). In this two-stage model,
a keyframing module represents the keyframes as well as their temporal placement with stochastic
latent variables. The images that occur at the timepoints between keyframes are then inferred by an
inpainting module. We parametrize this model with a neural network and formulate a variational
lower bound on the sequence log-likelihood. Optimizing the resulting objective leads to a model that
discovers informative future keyframes that can be easily inpainted to predict the full future sequence.
Our contributions are as follows. We formulate a hierarchical approach for the discovery of infor-
mative keyframes using joint keyframing and inpainting (KeyIn). We propose a soft objective that
allows us to train the model in a fully differentiable way. We first analyze our model on a simple
dataset with stochastic dynamics in a controlled setting and show that it can reliably recover the
underlying keyframe structure on visual data. We then show that our model discovers hierarchical
temporal structure on more complex datasets of demonstrations: an egocentric gridworld environ-
ment and a simulated robotic pushing dataset, which is challenging for current approaches to visual
planning. We demonstrate that the hierarchy discovered by KeyIn is useful for planning, and that the
resulting approach outperforms other proposed hierarchical and non-hierarchical planning schemes
on the pushing task. Specifically, we show that keyframes predicted by KeyIn can serve as useful
subgoals that can be reached by a low-level planner, enabling long-horizon, hierarchical control.
2	Related work
Hierarchical temporal structure. Hierarchical neural models for efficiently modeling sequences
were proposed in Liu et al. (2015); Buesing et al. (2018). These approaches were further extended to
predict with an adaptive step size so as to leverage the natural hierarchical structure in language data
(Chung et al., 2016; Kadar et al., 2018). However, these models rely on autoregressive techniques for
text generation and applying them to structured data, such as videos, might be impractical.
The video processing community has used keyframe representations as early as 1991 in the MPEG
codec (Gall, 1991). Wu et al. (2018) adapted this algorithm in the context of neural compression;
however, these approaches use constant offsets between keyframes and thus do not fully reflect the
temporal structure of the data. Recently, several neural methods were proposed to leverage such
temporal structure. Neitz et al. (2018) and Jayaraman et al. (2019) propose models that find and
predict the least uncertain “bottleneck” frames. Gregor et al. (2019) construct a representation that
can be used to predict any number of frames into the future. In contrast, we propose an approach for
hierarchical video representation that discovers the keyframes that best describe a certain sequence.
In parallel to our work, Kipf et al. (2019) propose a related method for video segmentation via
generative modeling. Kipf et al. (2019) focus on using the discovered task boundaries for training
hierarchical RL agents, while we show that our model can be used to perform efficient hierarchical
planning by representing the sequence with only a small set of keyframes. Also concurrently, Kim
et al. (2019) propose a similar method to KeyIn for learning temporal abstractions. While Kim et al.
(2019) focuses on learning hierarchical state-space models, we propose a model that operates directly
in the observation space and performs joint keyframing and inpainting.
2
Under review as a conference paper at ICLR 2020
Video modeling. Early approaches to probabilistic video modeling include autoregressive models
that factorize the distribution by considering pixels sequentially (Kalchbrenner et al., 2017; Reed
et al., 2017). To reason about the images in the video holistically, latent variable approaches were
developed based on variational inference (Chung et al., 2015; Rezende et al., 2014; Kingma &
Welling, 2014), including (Babaeizadeh et al., 2018; Denton & Fergus, 2018; Lee et al., 2018) and
large-scale models such as (Castrejon et al., 2019; Villegas et al., 2019). Kumar et al. (2019) is a
recently proposed approach that uses exact inference based on normalizing flows (Dinh et al., 2014;
Rezende & Mohamed, 2015). We build on existing video modeling approaches and show how they
can be used to learn temporal abstractions with a novel keyframe-based generative model.
Visual planning and model predictive control. We build on recent work that explored applica-
tions of learned visual predictive models to planning and control. Several groups (Oh et al., 2015;
Finn et al., 2016; Chiappa et al., 2017) have proposed models that predict the consequences of actions
taken by an agent given its control output. Recent work (Byravan et al., 2017; Hafner et al., 2018;
Ebert et al., 2018) has shown that visual model predictive control based on such models can be
applied to a variety of different settings. In this work, we show that the hierarchical representation of
a sequence in terms of keyframes improves planning performance in the hierarchical planning setting.
3	Keyframing the Future
Our goal is to develop a model that generates se-
quences by first predicting key observations and the
time steps when they occur and then filling in the
remaining observations in between. To achieve this
goal, in the following we (i) define a probabilistic
model for joint keyframing and inpainting, and (ii)
show how a maximum likelihood objective leads to
the discovery of keyframe structure.
3.1	A probabilistic model
for Joint Keyframing and Inpainting
We first describe a probabilistic model for joint
keyframing and inpainting of a sequence I1:T . The
model consists of two parts: the keyframe predictor
and the sequence inpainter (see Fig. 2).
Figure 2: A probabilistic model for jointly
keyframing and inpainting a future sequence.
First, a sequence of keyframes K1:N is gen-
erated, as well as corresponding temporal in-
dices τ 1:N , defining the structure of the un-
derlying sequence. In the second stage, for
each pair of keyframes Kn and Kn+1, the
frames ITn :tn+ι-1 are inpainted.
The keyframe predictor takes in C conditioning frames Ico and produces N keyframes K1:N as well
as the corresponding time indices τ 1:N :
P(KLN ,τ 1:N IIco) = ɪɪp(K n,τn |K1:nT,T1:nT,Ico).
(1)
n
From each pair of keyframes, the sequence inpainter generates the sequence of frames in between:
P(ITn :Tn + 1 - 1IKn, Kn+1,τn+1 - Tn) = Y p(It∣Kn, Kn+1,Iτn:t-i ,τn+1 - Tn),	⑵
which completes the generation of the full sequence. The inpainter additionally observes the number
of frames it needs to generate Tn+1 - Tn . The temporal spacing of the most informative keyframes is
data-dependent: shorter keyframe intervals might be required in cases of rapidly fluctuating motion,
while longer intervals can be sufficient for steadier motion. Our model handles this by predicting the
keyframe indices T and inpainting Tn+1 - Tn frames between each pair of keyframes. We parametrize
the prediction of Tn in relative terms by predicting offsets δn : Tn = Tn-1 + δn .
3.2	Keyframe Discovery
To produce a complex multimodal distribution over K we use a per-keyframe latent variable z with
prior distribution P(z) and approximate posterior q(zII, Ico).1 We construct a variational lower bound
1For simplicity, the variable representing the full sequence is written without indices (I is the same as I1:T).
3
Under review as a conference paper at ICLR 2020
on the likelihood of both I and K as follows2:
lnp(I,K|Ico) ≥ Eq(z|I,Ico)
ln Ep(T n,τ n + 1|z1：n,Ico ) [p(I. n：T "+"K n,n+1 ,Tn+1 - Tn )]
'---------------------------------------{------------------------------------------}
inpainting
(3)
+ lnp(K|z, Ico)
X------{------}
keyframing
- DKL (q(z|I, Ico)||p(z)) .
^^{^^^^^^™
regularization
|
}
In practice, we use a weight β on the KL-divergence term, as is common in amortized variational
inference (Higgins et al., 2017; Alemi et al., 2018; Denton & Fergus, 2018).
If a simple model is used for inpainting, most of the representational power of the model has to
come from the keyframe predictor. We use a relatively powerful latent variable model for the
keyframe predictor and a simpler Gaussian distribution produced with a neural network for inpainting.
Because of this structure, the keyframe predictor has to predict keyframes that describe the underlying
sequence well enough to allow a simpler inpainting process to maximize the likelihood. We will
show that pairing a more flexible keyframe predictor with a simpler inpainter allows our model to
discover semantically meaningful keyframes in video data.
4	Continuous relaxation by linear interpolation in time
Our model can dynamically predict
the keyframe placement Tn. However,
learning a distribution over the dis-
crete variable Tn is challenging due
to the expensive evaluation of the ex-
pectation over p(τn|z1:n, Ico) in the
objective in Eq. 3. To be able to eval-
uate this term efficiently and in a dif-
ferentiable manner while still learn-
ing the keyframe placement, We pro-
pose a continuous relaxation of the
objective. The placement distribution
τn defines a probability for each pre-
dicted frame to match to a certain
frame in the ground truth sequence.
Instead of sampling from this distri-
bution to pick a target frame we pro-
duce a soft target for each predicted
frame by computing the expected tar-
get frame, i.e. the weighted sum of all
frames in the true sequence, each mul-
tiplied with the probability of match-
ing to the predicted frame. When the
Ground Truth Sequence
Figure 3: Soft keyframe loss in the relaxed formulation.
For each predicted keyframe Kn we compute a target image
Kn as the sum of the ground truth images weighted with
the corresponding distribution over index Tn. Finally, we
compute the reconstruction loss between the estimated image
Kn and the soft target Kn .
entropy of Tn converges to zero, the continuous relaxation objective is equivalent to the original,
discrete objective. 3
-TT- i'	，	， E	F	1 i'	.	Ty-∏	1 ∙	1	1,1,	.1	F
Keyframe targets. To produce a keyframe target, Kn , we linearly interpolate between the ground
truth images according to the predicted distribution over the keyframe’s temporal placement Tn :
Kn = t Ttn It , where Ttn is the probability that the nth keyframe occurs at timestep t. This process
is depicted in Fig. 3.
We parametrize temporal placement prediction in terms of offsets δ with a maximum offset of J.
Because of this, the maximum possible length of the predicted sequence is NJ. it is desirable for J
to be large enough to be able to capture the distribution of keyframes in the data, but this may lead to
2Keyframes K and time indices τ are modeled as independent given the corresponding latent variable z such
that the latter captures the dependency between the former two.
3We find this occurs most of the time in practice.
4
Under review as a conference paper at ICLR 2020
Figure 4: Sequences generated by KEYIN and a method with constant temporal keyframe offset
(Jumpy) on Brownian Motion data. Generation is conditioned on the first five frames. The first half
of the sequence is shown. Movement direction changes are marked red in the ground truth sequence
and predicted keyframes are marked blue. We see that KEYIN can correctly reconstruct the motion as
it selects an informative set of keyframes. The sequence generated by the Jumpy method does not
reproduce the direction changes since they cannot be inferred from the selected keyframes.
the generation of sequences longer than the target NJ > T. To correctly compute the value of the
relaxed objective in this case, we discard predicted frames at times > T and normalize the placement
probability output by the network so that it sums to one over the first T steps. Specifically, for each
keyframe we compute this probability as cn: Cn = Ptt≤τ τn. The loss corresponding to the last two
terms of Eq. (3) then becomes:
PnCn (||Kn — KnII2 + β Dkl (q(zn∣I-c+ιτ,z1:n-1)||p(zn))^
Lkey =—— --------------------------Pcn------------------------------L .	(4)
Inpainting targets. To complete our relaxed objective, for each ground truth frame, we produce
a target image composed from the inpainted frames.4 We note that as offsets δ have a maximum
range of J, and in general have non-zero probability on each timestep, the inpainting network
needs to produce J frames InJ between each pair of keyframes (Kn, Kn+1). As in the previous
section, the targets for ground truth images are given as an interpolation between generated images
weighted by the probability of the predicted frame Ijn being matched to ground truth frame It :
It = ( n,j mjn,tIjn)/ n,j mjn,t. Here, mjn,t is the probability that the j-th predicted image in
segment n has an offset of t from the beginning of the predicted sequence, which can be computed
from τn . To obtain a probability distribution over produced frames, we normalize the result with
Pn,j mjn,t. The full loss for our model is:
Ltotal = Lkey + βl X ||It - It||2 .	(5)
t
5	Deep Video Keyframing
We show how to instantiate KeyIn with deep neural networks and train it on high-dimensional
observations, such as images. We further describe an effective training procedure for KeyIn.
5.1	Architecture
We use a common encoder-recurrent-decoder architecture (Denton & Fergus (2018); Hafner et al.
(2018)). Video frames are first processed with a convolutional encoder module to produce image
embeddings ∣t = CNNenc(It). Inferred frame embeddings ^ are decoded with a convolutional de-
coder In = CNNdec(In). The keyframe predictor p(KLN,τ 1:NIzLN, Ico) is parametrized with
a Long Short-Term Memory network (LSTM, Hochreiter & Schmidhuber (1997)). To condi-
tion the keyframe predictor on past frames, we initialize its state with the final state of another
LSTM that processes the conditioning frames. Similarly, we parametrize the sequence inpainter
p(Iτn:Tn+ι |Kn, Kn+1, Tn+1 — Tn) with an LSTM. We condition the inpainting on both keyframe
embeddings, ^n-1 and κn, as well as the temporal offset between the two, δn, by passing these inputs
through a multi-layer perceptron that produces the initial state of the inpainting LSTM.
4This makes sure that each ground truth frame contributes equally to the final loss.
5
Under review as a conference paper at ICLR 2020
Ground
Truth
Predicted
Keyframe	Keyframe
Segment 1	3	Segment 2	3
Dense Reconstruction	Keyframe Predictions
Ground
Truth
Predicted
Figure 5: Example generations by KEYIN on (top) Pushing and (bottom) Gridworld data. The
generation is conditioned on a single ground truth frame. Twelve of the 30 predicted frames are
shown. We observe that for each transition between pushes and each action of the Gridworld agent
our network predicts a keyframe either exactly at the timestep of the event or one timestep apart.
Note, although agent position is randomized, objects not visible in the first image can be predicted in
Gridworld because the maze is fixed across episodes.
We use a Gaussian distribution with identity variance as the output distribution for both the keyframe
predictor and the inpainting model and a multinomial distribution for δn. We parametrize the
inference q(z1:N|I-c+i：T) with an LSTM with attention over the entire input sequence. The
inference distribution is a diagonal covariance Gaussian, and the prior p(z1:N) is a unit Gaussian.
Further details of the inference procedure are given in Sec. B and Fig. 8 of the Appendix.
5.2	Training Procedure
We train our model in two stages. First, we train the sequence inpainter to inpaint between ground
truth frames sampled with random offsets, thus learning interpolation strategies for a variety of
different inputs. In the second stage, we train the keyframe predictor using the loss from Eq. 5 by
feeding the predicted keyframe embeddings to the inpainter. In this stage, the weights of the inpainter
are frozen and are only used to backpropagate errors to the rest of the model. We found that this
simple two-stage procedure improves optimization of the model.
We use L1 reconstruction losses to train the keyframe predictor. We found that this and adding
a reconstruction loss on the predicted embeddings of the keyframes, weighted with a factor βκ,
improved the ability of the model to produce informative keyframes. Target embeddings are computed
using the same soft relaxation used for the target keyframes. More details of the loss computation are
given in Sec. E and Algorithm 1 of the Appendix.
6	Experiments
We evaluate the quality of KeyIn’s representation for future sequences by addressing the following
questions: (i) Can it discover and predict informative keyframes? (ii) Can it model complex data
distributions? (iii) Is the discovered hierarchy useful for long-horizon hierarchical planning?
Datasets. We evaluate our model on three datasets containing structured long-term behavior. The
Structured Brownian motion (SBM) dataset consists of binary image sequences of size 32 × 32 pixels
in which a ball randomly changes directions after periods of straight movement of six to eight frames.
The Gridworld Dataset consists of 20k sequences of an agent traversing a maze with different objects.
The agent sequentially navigates to objects and interacts with them following a task sketch.We use
the same maze for all episodes and randomize the initial position of the agent and the task sketch. We
use 64 × 64 pixel image observations and further increase visual complexity by constraining the field
of view to a 5 × 5-cells egocentric window.
6
Under review as a conference paper at ICLR 2020
The Pushing Dataset consists of 50k sequences of a robot arm pushing a puck towards a goal on the
opposite side of a wall. Each sequence consists of six consecutive pushes. We vary start and target
position of the puck, as well as the placement of the wall. The demonstrations were generated with
the MuJoCo simulator (Todorov et al., 2012) at a resolution of 64 × 64 pixels. For more details on
the data generation process, see Sec.D of the Appendix.
Further details about the experimental setup are given in Sec. C of the Appendix.
6.1	Keyframe Discovery
To evaluate KEYIN’s ability to dis- Table 1: F1 accuracy score for keyframe discovery on all
cover keyframes, we train KeyIn on three datasets. Higher is better.
all three datasets with N = 6, which -------------------------------------------------------
	 Z can be interpreted as selecting the N	Method	Brownian	Push	Gridworld
most informative frames from a se-	Random	0.15	0.18	0.12
quence. We show qualitative exam-	Static	0.21	0.18	0.25
ples of keyframe discovery for the	Surprise	0.73	0.10	0.32
SBM dataset in Fig. 4 and for the Grid-	KeyIn (Ours)	0.94	0.43	0.42
world and Pushing datasets in Fig. 5.
On all datasets the model discovers meaningful keyframes which mark direction changes of the ball,
transitions between pushes or interactions with objects, adapting its keyframe prediction patterns
to the data. Consequently, the inpainter network is able to produce frames of high visual quality.
Misplaced keyframes yield blurry interpolations, as can be seen for thejumpy prediction in Fig. 4.
This suggests that keyframes found by KEYIN describe the overall sequences better.
To show that KEYIN discovers informative keyframes,
We compare keyframe predictions against an alternative
approach that measures the surprise associated with ob-
serving a frame given the previous frames. This ap-
proach selects keyframes as the N frames with the largest
peaks in “surprise” as measured by the KL-divergence
DKL [q(zt |Ii：t) ||p(zt)] between the prior and the posterior
of a stochastic predictor based on Denton & Fergus (2018)
(see Sec. F and Algorithm 2 of the Appendix for details).
We provide comparisons to alternative formulations of
surprise in Appendix Sec. F, Tab. 3.
For quantitative analysis, we define approximate ground
truth keyframes to be the points of direction change for
the SBM dataset, the moments when the robot lifts its arm
to transitions between pushes, or when the agent interacts
with objects in the gridworld. We report F1 scores that
capture both the precision and recall of keyframe discovery.
Ground Truth	Generated
Figure 6: Distribution of trajectories
sampled from KeyIn. Each black line
denotes one of 100 trajectories of the ma-
nipulated object. The obstacle is shown
in blue and the initial position in pink.
We see that our model covers both modes
of the distribution, producing both trajec-
tories that go to the right and to the left
of the obstacle.
We additionally compare to random keyframe placement, and a learned but static baseline that is
the same for all sequences. The evaluation in Tab. 1 shows that KeyIn discovers better keyframes
than alternative methods. The difference is especially large on the more complex Pushing and
Gridworld datasets. The surprise-based method does not reason about which frames are most helpful
to reconstruct the entire trajectory and thus is unable to discover the correct structure on the more
complex datasets. In addition to the F1 scores, we report temporal distance between predicted and
annotated keyframes in Appendix, Tab. 4, also indicating that KeyIn is better able to discover the
temporal structure in both datasets.
6.2	Keyframe-Based Video Modeling
Even though the focus of this work is on discovering temporal structure via keyframing and not on
improving video prediction quality, we verify that KeyIn can represent complex data distributions
in terms of discovered keyframes and attains high diversity and visual quality. We show sample
generations from our model on the Pushing and Gridworld datasets on the supplementary website5.
5https://sites.google.com/view/keyin
7
Under review as a conference paper at ICLR 2020
We see that KeyIn is able to faithfully model complex distributions of video sequences. We further
visualize multiple sampled Pushing sequences from our model conditioned on the same start position
in Fig. 6, showing that KeyIn is able to cover both modes of the demonstration distribution. We
further show that KeyIn compares favorably to prior work on video prediction metrics on sequence
modeling in Tab. 5 of the Appendix, and outperforms prior approaches in terms of keyframe modeling
in Appendix, Tab. 6.
6.3	Robustness of Keyframe Detection
In the previous sections, we showed that when
the sequence can indeed be summarized with
N keyframes, KEYIN predicts the keyframes
that correspond to our notion of salient frames.
However, what happens if we train KeyIn to
select a larger or a smaller amount of keyframes?
To evaluate this, we measure KeyIn recall
with extra and precision with fewer available
keyframes. We note that high precision is un-
achievable in the first case and high recall is un-
achievable in the second case, since these prob-
lems are misspecified. As these numbers are not
Table 2: Keyframe discovery for varied number of
predicted keyframes. The data has approximately
6 keyframes. Uninterpretable entries are omitted
for clarity: see the text for details.
# Keyframes		4	6	8
B rownian	Precision	0.92	0.92	-
	Recall	-	0.96	0.90
Push	Precision	0.30	0.38	-
	Recall	-	0.48	0.46
Gridworld	Precision	0.37	0.43	- 0.40
	Recall	-	0.41	
informative, we do not report them. In Tab. 2, we see that KeyIn is able to find informative keyframes
even when N does not exactly match the structure of the data. We further qualitatively show that
KeyIn selects a superset or a subset of the original keyframes respectively in Sec. G. This underlines
that our method’s ability to discover keyframe structure is robust to the choice of the number of
predicted keyframes.
As a first step towards analyzing the robustness of KeyIn under more realistic conditions we report
keyframe discovery when trained and tested on sequences with additive Gaussian noise, a noise
characteristic commonly found in real-world camera sensors. We find that KeyIn is still able to
discover the temporal structure on both the Pushing and the Gridworld dataset. For qualitative and
quantitative results, see Appendix Fig. 11 and Tab. 7.
6.4	Hierarchical Keyframe-Based Planning
We have seen that KeyIn can find frames that correspond to an intuitive notion of keyframes.
This demonstrates that the keyframes discovered by KeyIn do indeed capture an abstraction that
compactly describes the sequence. In light of this, we hypothesize that an informative set of keyframes
contains sufficient information about a sequence to effectively follow the trajectory it shows. To test
this, we use the inferred keyframes as subgoals for hierarchical planning in the pushing environment.
During task execution, we first plan a sequence of keyframes that reaches the target using our learned
keyframe predictor. Specifically, we generate keyframe trajectories from our model by sampling
latent variables z from the prior and using them to roll out the keyframe prediction model. We
optimize for a sequence of latent variables z that results in a keyframe trajectory which reaches the
goal using the Cross-Entropy Method (CEM, Rubinstein & Kroese (2004)). We then execute the plan
by using the keyframes as subgoals for a low-level planner. This planner reaches each subgoal via
model predictive control using ground truth dynamics, again employing CEM for optimization of the
action trajectory. This planning procedure is illustrated in Fig. 7 (left). For more details, see Sec. I
and Algs. 3 and 4 of the Appendix.
We find that KeyIn is able to plan coherent subgoal paths towards the final goal that often lead to
successful task execution (executions are shown on the supplementary website6). To quantitatively
evaluate the keyframes discovered, we compare to alternative subgoal selection schemes: fixed time
offset (Jumpy, similar to Buesing et al. (2018)), a method that determines points of peak surprise
(Surprise, see Sec. 6.1), and a bottleneck-based subgoal predictor (time-agnostic prediction or TAP,
Jayaraman et al. (2019)). We additionally compare to an approach that plans directly towards the
final goal using the low-level planner (Flat). We evaluate all methods with the shortest path between
6https://sites.google.com/view/keyin
8
Under review as a conference paper at ICLR 2020
	(	Ke'In	)	Method	Position error Success RATE
Keyframing	'	Xj"I	-pj*~P-' ʃo	K］…KN	Igoal 	Γ Iy 	 vJ	5r~~	INTITIAL	1.32 ± 0.06	- Random	1.32 ± 0.07	- Flat	0.90 ± 0.14	15.0% TAP	0.80 ± 0.16	23.3% Surprise	0.64 ± 0.28	50.8% Jumpy	0.62 ± 0.33	58.8% KEYIN(OURS)	0.50 ± 0.26	64.2%
Control	"∖ ∕~~∖ /7 ［国 …国］	［画 …国］	
Figure 7: Hierarchical planning on the Pushing dataset. Left: We use the model to produce keyframes
that represent the sequence between the current observation image and the goal. A low-level planner
based on model predictive control produces the actions, at , executed to reach each keyframe, until
the final goal is reached. Right: Planning performance on a Pushing task. The hierarchy discovered
by KeyIn outperforms comparable planning approaches.
the target and the actual position of the object after the plan is executed. All compared methods use
the same low-level planner as we only want to measure the quality of the predicted subgoals.
As shown in Fig. 7 (right), our method outperforms all prior approaches. TAP shows only a moderate
increase in performance over the Flat planner, which we attribute to the fact that it fails to predict
good subgoals and often simply predicts the final image as the bottleneck. This is likely due to
the relatively large stochasticity of our dataset and the absence of the clear bottlenecks that TAP is
designed to find. Our method outperforms the planners that use Jumpy and Surprise subgoals. This
further confirms that KeyIn is able to produce keyframes that are informative about the underlying
trajectory, such that planning toward these keyframes makes it easier to follow the trajectory.
7 Discussion
We presented KeyIn, a method for representing a sequence by its informative keyframes by jointly
keyframing and inpainting. KeyIn first generates the keyframes of a sequence and their temporal
placement and then produces the full sequence by inpainting between keyframes. We showed that
KeyIn discovers informative keyframes on several datasets with stochastic dynamics. Furthermore,
by using the keyframes for planning, we showed our method outperforms several other hierarchical
planning schemes. Our method opens several avenues for future work. First, an improved training
procedure that allows end-to-end training is desirable. Second, more powerful hierarchical planning
approaches can be designed using the keyframe representation to scale to long-term real-world tasks.
Finally, the proposed keyframing method can be applied to a variety of applications, including video
summarization, video understanding, and multi-stage hierarchical video prediction.
9
Under review as a conference paper at ICLR 2020
References
Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A. Saurous, and Kevin Murphy. Fixing
a broken ELBO. In Proceedings of International Conference on Machine Learning (ICML), 2018.
Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H. Campbell, and Sergey Levine.
Stochastic variational video prediction. In Proceedings of International Conference on Learning
Representations (ICLR), 2018.
D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and
translate. Proceedings of International Conference on Learning Representations (ICLR), 2015.
Lars Buesing, TheoPhane Weber, Sebastien Racaniere, S. M. Ali Eslami, Danilo Jimenez Rezende,
David P. Reichert, Fabio Viola, Frederic Besse, Karol Gregor, Demis Hassabis, and Daan Wierstra.
Learning and querying fast generative models for reinforcement learning. arXiv:1802.03006, 2018.
Arunkumar Byravan, Felix Leeb, Franziska Meier, and Dieter Fox. Se3-Pose-nets: Structured
deeP dynamics models for visuomotor Planning and control. Proceedings of IEEE International
Conference on Robotics and Automation, 2017.
Lluis Castrejon, Nicolas Ballas, and Aaron Courville. ImProved conditional vrnns for video Prediction.
arXiv preprint arXiv:1904.12165, 2019.
Silvia ChiaPPa, Sebastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent environment
simulators. In Proceedings of International Conference on Learning Representations (ICLR),
2017.
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio.
A recurrent latent variable model for sequential data. In Proceedings of Neural Information
Processing Systems (NeurIPS), 2015.
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks.
2016.
E. Denton and R. Fergus. Stochastic video generation with a learned Prior. In Proceedings of
International Conference on Machine Learning (ICML), 2018.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear indePendent comPonents
estimation. arXiv preprint arXiv:1410.8516, 2014.
Frederik Ebert, Chelsea Finn, Alex X Lee, and Sergey Levine. Self-suPervised visual Planning with
temPoral skiP connections. In Conference on Robotic Learning (CoRL), 2017.
Frederik Ebert, Chelsea Finn, SudeeP Dasari, Annie Xie, Alex Lee, and Sergey Levine. Vi-
sual foresight: Model-based deeP reinforcement learning for vision-based robotic control.
arXiv:1812.00568, 2018.
Chelsea Finn and Sergey Levine. DeeP visual foresight for Planning robot motion. In Proceedings of
IEEE International Conference on Robotics and Automation, 2017.
Chelsea Finn, Ian Goodfellow, and Sergey Levine. UnsuPervised learning for Physical interaction
through video Prediction. In Proceedings of Neural Information Processing Systems (NeurIPS),
2016.
D. Le Gall. MPEG: A video comPression standard for multimedia aPPlications. Commun. ACM, 34
(4):46-58,1991.
Karol Gregor, George PaPamakarios, Frederic Besse, Lars Buesing, and TheoPhane Weber. TemPoral
difference variational auto-encoder. 2019.
Danijar Hafner, Timothy LillicraP, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for Planning from Pixels. arXiv preprint arXiv:1811.04551,
2018.
10
Under review as a conference paper at ICLR 2020
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a
constrained variational framework. In Proceedings of International Conference on Learning
Representations (ICLR), 2017.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780,1997.
D. Jayaraman, F. Ebert, A. A. Efros, and S. Levine. Time-agnostic prediction: Predicting predictable
video frames. Proceedings of International Conference on Learning Representations (ICLR), 2019.
AkoS Kadar, Marc-Alexandre Cδte, Grzegorz Chrupala, and Afra Alishahi. Revisiting the hierarchical
multiscale lstm. arXiv preprint arXiv:1807.03595, 2018.
Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex
Graves, and Koray Kavukcuoglu. Video Pixel networks. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, PP. 1771-1779. JMLR. org, 2017.
TaesuP Kim, Sungjin Ahn, and Yoshua Bengio. Variational temPoral abstraction. In Proceedings of
Neural Information Processing Systems (NeurIPS), 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. In Proceedings of
International Conference on Learning Representations (ICLR), 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In Proceedings of Interna-
tional Conference on Learning Representations (ICLR), 2014.
Thomas KiPf, Yujia Li, Hanjun Dai, Vinicius Zambaldi, Edward Grefenstette, Pushmeet Kohli, and
Peter Battaglia. ComPositional imitation learning: ExPlaining and executing one task at a time.
Proceedings of International Conference on Machine Learning (ICML), 2019.
Manoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan, Chelsea Finn, Sergey Levine, Laurent
Dinh, and Durk Kingma. Videoflow: A flow-based generative model for video. arXiv preprint
arXiv:1903.01434, 2019.
A. X. Lee, R. Zhang, F. Ebert, P. Abbeel, C. Finn, and S. Levine. Stochastic adversarial video
Prediction. arXiv:1804.01523, abs/1804.01523, 2018.
Pengfei Liu, XiPeng Qiu, Xinchi Chen, Shiyu Wu, and Xuanjing Huang. Multi-timescale long
short-term memory neural network for modelling sentences and documents. In Proceedings of the
2015 conference on empirical methods in natural language processing, PP. 2326-2335, 2015.
Thang Luong, Hieu Pham, and ChristoPher D Manning. Effective aPProaches to attention-based
neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing, PP. 1412-1421, 2015.
Alexander Neitz, Giambattista Parascandolo, Stefan Bauer, and Bernhard Scholkopf. Adaptive
skiP intervals: TemPoral abstraction for recurrent dynamical models. In Proceedings of Neural
Information Processing Systems (NeurIPS), 2018.
Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, and Satinder Singh. Action-conditional
video prediction using deep networks in atari games. In Proceedings of Neural Information
Processing Systems (NeurIPS), 2015.
Scott Reed, Aaron van den Oord, Nal Kalchbrenner, Sergio G6mez Colmenarejo, Ziyu Wang, Yutian
Chen, Dan Belov, and Nando de Freitas. Parallel multiscale autoregressive density estimation. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 2912-2921.
JMLR. org, 2017.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Proceedings
of International Conference on Machine Learning (ICML), 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In Proceedings of International Conference on
Machine Learning (ICML), 2014.
11
Under review as a conference paper at ICLR 2020
Reuven Y. Rubinstein and Dirk P. Kroese. The Cross-Entropy Method: A Unified Approach to
Combinatorial Optimization, Monte-Carlo Simulation and Machine Learning. Springer-Verlag
New York, 2004.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems, pp.
5026-5033. IEEE, 2012.
Ruben Villegas, Arkanath Pathak, Harini Kannan, Dumitru Erhan, Quoc V. Le, and Honglak Lee.
High fidelity video prediction with large neural nets. In Proceedings of Neural Information
Processing Systems (NeurIPS), 2019.
Chao-Yuan Wu, Nayan Singhal, and Philipp Krahenbuhl. Video compression through image interpo-
lation. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 416-431,
2018.
12
Under review as a conference paper at ICLR 2020
Image (Future)
CNNenc
LSTMinf
Key-Value
Attention
Approximate
Posterior
q(z∣lo:N)
Keyframes
Figure 8: Structure of the keyframe inference network. This diagram depicts the procedure to infer
the embedding of the n-th keyframe, κn, given the previously inferred keyframe embedding κn-1
and the future images. The initial state of LSTMkey is produced by LSTMcond (not shown), which
takes the embedding of past images as input. This ensures that the See the text for more details.
A Videos
We include video results on the supplementary website at https://sites.google.com/
view/keyin. The website includes inference samples, prior samples, and exectued trajectories for
all methods.
B Architecture details
We found simple attention over LSTM outputs tobe an effective inference procedure. Our approximate
inference network LSTMinf outputs (κitnf,ζt)t≤T,where κinf is an embedding used to compute an
attention weight and the ζt are values to be attended over. We compute the posterior distribution over
zt using a key-value attention mechanism (Bahdanau et al., 2015; Luong et al., 2015):
an,t = exp(d(Kn-1 ,κinf))	(6)
μn,σn = (X an,tZt)/X ant∙	⑺
The distance metric, d, is the standard inner product. The architecture used for keyframe inference,
including the attention mechanism, is depicted in Supplemental Fig. 8.
C Experimental setup
We set the prediction horizon to T = 30 frames and predict N = 6 segments with J = 10 frames
each for the SBM dataset and 6 frames each for the Pushing dataset. We pre-train the interpolator
on segments of two to eight frames for Structured Brownian motion data, and two to six frames for
Pushing data. The weight on the KL-divergence term for the interpolator VAE is 1e-3. For training
13
Under review as a conference paper at ICLR 2020
the keyframe predictor, we set βK = 0, βκ = 1, β = 5e-2. The hyperparameters were hand-tuned.
We activate the generated images with a sigmoid and use BCE losses on each color channel to avoid
saturation. The convolutional encoder and decoder both have three layers for the Structured Brownian
motion dataset and four layers for the Pushing dataset. We use a simple two-layer LSTM with a
256-dimensional state in each layer for all recurrent modules. Each LSTM has a linear projection
layer before and after it that projects the observations to and from the correct dimensions. We use
the Adam optimizer (Kingma & Ba, 2015) with β1 = 0.9 and β2 = 0.999, batch size of 30, and a
learning rate of 2e-4. For more details please refer to the appendix. Each network was trained on a
single high-end NVIDIA GPU. We trained the interpolator for 100K iterations, and the keyframe
predictor for 200K iterations. The toal training time took about a day.
In the Pushing environment, we use a held-out test set of 120 of sequences. The Structured Brownian
Motion dataset is generated automatically and is potentially infinite. We used 1000 testing samples
on the Structured Brownian Motion generated using a different random seed.
D Data collection in the MuJoCo environment
The data collection for our pushing dataset was performed in an environment simulated in MuJoCo
Todorov et al. (2012). In the environment, a robot arm initialized at the center of the table pushes an
object to a goal position located at the other side of a wall-shaped obstacle.
The demonstrations followed a rule-based algorithm that first samples subgoals between the initial
position of the object and the goal and then runs a deterministic pushing procedure to the subgoals in
order. The ground truth keyframes of the demonstrations were defined by frames at which subgoals
were completed.
We subsampled demonstration videos by a factor of two when saving them to the dataset, dropping
every other frame in the trajectory and averaging actions of every two consecutive frames. For all
datasets we generated for this environment following a rule-based algorithm, we only kept successful
demonstrations and dropped the ones that fail to push the object to the goal position within a
predefined horizon.
E Details of the loss computation algorithm
We describe the details of the continuous relaxation loss computation in Algorithm 1.
Note that we efficiently implement computing of the cumulative distributions τ as a convolution,
which allows us to vectorize much of the computation. Computational complexity of the proposed
implementation scales linearly with the number of keyframes N, and number of allowed frames per
segment J and number of ground truth frames T. The final complexity is O(NTJ), which we find
in practice to be negligible compared to the time needed for the forward and backward pass.
F	Surprise baseline
Standard stochastic video prediction methods do not attempt to estimate keyframes, as they are
designed to densely estimate future videos frame-by-frame. Accordingly, they cannot be used directly
as baselines for keyframe prediction methods, such as KeyIn. However, Denton & Fergus (2018)
observe that the variance of the learned prior of a stochastic video prediction model tends to spike
before an uncertain event happens. We exploit this observation to find the points of high uncertainty
for our strong Surprise baseline. We use the KL divergence between the prior and the approximate
posterior KL[q(zt|I1:t)||p(zt)] to measure the surprise. This quantity can be interpreted as the number
of bits needed to encode the latent variable describing the next state, it will be larger if the next state
is more stochastic.
We train a stochastic video prediction network with a fixed prior (SVG-FP, Denton & Fergus (2018))
with the same architectures of encoder, decoder, and LSTM as our model. We found that selecting
the peaks of suprise works the best for finding true keyframes. The procedure we use to select the
keyframes is described in Algorithm 2. In order to find the keyframes in a sequence sampled from
the prior, we run the inference network on the generated sequence.
14
Under review as a conference paper at ICLR 2020
Algorithm 1 Continuous relaxation loss computation
Parameters: Number of ground truth frames T, Number of keyframes N
Input: Ground truth frames Ii：T, Generated frames It generated offset distributions δn
Convert the distributions of interframe offsets δn to keyframe timesteps τn . For the first keyframe,
τ1 = δ1 .
for t = 2 . . . M do
Compute further τn with chain rule. This can be efficiently computed via convolution:
Tn = TnT * δn,i.e. τn = x Tn-hen.
j
end for
Compute probabilities of keyframes being within the predicted sequence: cn = Pt≤T Ttn .
r-∖	.	1`, i r∙	.	. τy-n.	K~、 r> τ
Compute soft keyframe targets: Kn =	tTtn It.
Compute the keyframe loss: (Pn cn∣∣KKn - Kn∣∣2)/Pn cn.
Get probabilities of segments ending after particular frames: ejn =	j>iδjn.
Get distributions of individual frames timesteps: mn,t H TtT+ιen
Compute soft individual frames: It = Pti mn,tIt
Compute the sequence loss: t ||It - It||2.
Algorithm 2 Selecting keyframes via Surprise
Parameters: Number of ground truth frames N, Desired number of keyframes M
Input: Input sequence I1:T, Stochastic Video Prediction model SV G(.)
Run the inference network over the sequence: q(z1:T |I1:T) = SV G(I1:T).
Get the surprise measure: st = K L[q(zt|I1:t)||p(zt)].
Find the set of peak surprise points S where: st > st+1 ∧ st < st-1.
if |S| < M then
add M - |S | maximum surprise points to S.
end if
Return: The M keyframes from S with maximum surprise.
G Ablation of the number of predicted keyframes
We show qualitative results of training KEYIN with N = 4, 6 (optimal number), and 8 on the SBM
dataset in Fig. 9. We observe that if we train KeyIn to select a smaller or a larger number of
keyframes than needed, it learns to predict a subset or a superset of the true keyframes, respectively.
This property follows from the structure of the model, which encourages the model to predict the
keyframes that allow the full sequence to be inpainted. When too few keyframes are available, the
model will be unable to put keyframes at all important times, but those it picks must still be good for
inpainting. When more keyframes are available than necessary, the model can place the additional
keyframes at any time, as only a subset of the keyframes are needed to ensure good inpainting.
Figure 9: Qualitative keyframe discovery on the Structured Brownian Motion dataset for varying
number of predicted keyframes. Top: Ground truth sequence, keyframes with bold white frame.
Bottom: KEYIN keyframe predictions at their predicted temporal placement. Even if the number of
predicted keyframes does not match the true number of keyframes KEYIN correctly discovers the
keyframes and their temporal placement.
15
Under review as a conference paper at ICLR 2020
Table 3: We compare different formulations for a surprise-based keyframe detection method: (1)
detecting maxima of the KL divergence between prior and posterior in a stochastic prediction model,
(2) detecting maxima in the lower bound on data likelihood log p (ELBO) of a stochastic prediction
model, (3) the formulation proposed in Denton & Fergus (2018) that detects maxima of the variance
of a learned prior distribution.
Dataset	Push	Gridworld
Method	F1 ↑	min dKUE J min dKKED J	F1 ↑	min dKRUE J	min dKRED J
KL-SUPRISE	0.25	1.44	2.05 log p-SURPRISE	0.24	1.42	2.08 DENTON & FERGUS (2018)	0.17	1.73	2.01	0.32	1.42	1.86 0.31	1.45	1.83 0.35 1.10	1.53
Table 4: In addition to the F1 scores we report the minimal temporal distance to the next keyframe
as an additional metric that is more graceful with respect to "close misses". Specifically, we report
the distance to the next annotated keyframe averaged across predicted keyframes, min dtKruFe, and,
inversely, the distance to the next predicted keyframe for each annotated keyframe, mindpKrFed. For
both datasets the distance metrics support the F1 results: KeyIn discovers keyframes that are better
aligned with the annotated keyframes than the baselines.
Dataset Method	Push			Gridworld		
	F1 ↑	min dKRUE J	min dKRED J	F1 ↑	min dKRUE J	min dPKRFED J
Static	0.18	1.67	1.25	0.25	1.22	1.07
Surprise	0.25	1.44	2.05	0.32	1.42	1.86
KeyIn (Ours)	0.43	1.25	1.86	0.42	1.03	0.99
H	Video modeling performance
We further report quantitative results on standard video prediction metrics, Structural Similarity Index
(SSIM) and Peak Signal-to-Noise Ratio (PSNR), in Tab. 5. KeyIn is able to match performance of
two comparable prior approaches, showing that the keyframe-based modeling is able to represent
complex data distributions.
I Planning algorithm
Algorithm 3 Planning in the subgoal space.
Input: Keyframe model KEYIN(., .), cost function c
Input: Start and target images I1 and Itarget
Set the sampling distribution to the prior:
μi = 0, σi = I
for i = 1 . . . H do
Sample L sequences of latent variables:
Z1N 〜N(μi,σi)
Produce L subgoal plans: KK1:N = KEYIN(Iι,z1:N)
Compute cost between generated and true targets:
C(K N ,Itarget)
Choose L0 best plans, refit sampling distribution:
μi+1,σi+1 = fit(Zi)
end for
Return: Best subgoal plan K 1:N
16
Under review as a conference paper at ICLR 2020
Legend
Figure 10: A training sequence and two samples from our model on the Structured Brownian motion
dataset. Each image shows an entire trajectory. Our model first samples the keyframes (shown in
red), and then deterministically predicts the rest of the sequence. The image resolution was enhanced
for viewability.
Table 5: SSIM and PSNR scores on pushing and gridworld dataset. Higher is better.
Dataset	Push	Gridworld
Method	PSNR	SSIM	PSNR	SSIM
Denton & Fergus (2018) Jumpy KeyIn (Ours)	33.3 ± 0.1	0.956 ± 0.001	29.4 ± 0.1	0.812	± 0.01 33.7 ± 0.1	0.960 ± 0.001	29.9 ± 0.1	0.831	± 0.001 33.4 ± 0.7	0.959 ± 0.001	29.3 ± 0.1	0.820	± 0.001
To apply the KEYIN model for planning, We
USe the approach for visual planning outlined
in Algorithm 1. At the initial timestep, we use
the cross-entropy method (CEM) Rubinstein &
Kroese (2004) to select subgoals for the task. To
do so, We sample M latent sequences z0 from
Figure 11: Example keyframe detections on noisy sequences. Red frames mark annotated keyframes.
Top: Pushing dataset. Bottom: GridWorld dataset. Each triplet depicts top: ground truth sequence
With additive Gaussian noise, middle: predicted keyframes at the predicted time steps, bottom:
predicted full sequence. KeyIn is reliably able to detect keyframes and reconstruct the full sequence.
17
Under review as a conference paper at ICLR 2020
Table 6: Keyframe SSIM and PSNR scores on pushing and gridworld dataset. Higher is better.
Dataset	Push	Gridworld
Method	PSNR	SSIM	PSNR	SSIM
Jumpy KeyIn (Ours)	28.25 ± 0.11	0.904 ± 0.001	16.3 ± 0.17	0.632 ± 0.001 29.5 ± 0.16	0.911 ± 0.001	18.4 ± 0.17	0.636 ± 0.001
Table 7: F1 score and distance to closest annotated / predicted keyframe when trained and tested on
sequences with additive Gaussian noise. KeyIn is able to reliably find keyframes on both datasets
even when trained and tested on noisy sequences. Even though the F1 score is lower on the Pushing
dataset, the distances indicate that the discovered keyframes are well aligned with the annotated
keyframes even under noise.
Dataset Method	Push			Gridworld		
	F1 ↑	min dKRUE J	min dKRED J	F1 ↑	min dKRUE J	min dPKRFED J
KeyIn, no-noise	0.43	1.25	1.86	0.42	1.03	0.99
KeyIn, Gauss-noise	0.25	1.21	1.34	0.43	1.00	0.96
the prior N (0, I) and use the keyframe model to
retrieve M corresponding keyframe sequences,
each with L frames. We define the cost of an
image trajectory as the distance between the tar-
get image and the final image of each keyframe
sequence defined under a domain-specific dis-
tance function (see below). In the update step
of the CEM algorithm, we rank the trajectories
based on their cost and fit a diagonal Gaussian
distribution to the latents z0 that generated the
M 0 = rM best sequences, where r is the elite
ratio. We repeat the procedure above for a total
of N iterations.
We define the cost between two frames used during planning as the Euclidean distance between the
center pixels of the object in both frames. We recover the center pixel via color-based segmentation of
the object. While this cost function is designed for the particular planning environment we are testing
on, our algorithm can be easily extended to use alternative, more domain-agnostic cost formulations
that have been proposed in the literature Finn & Levine (2017); Ebert et al. (2017; 2018).
After subgoals are selected, we use a CEM based planner to produce rollout trajectories. Similar to
the subgoal generation procedure, at each time step, we initially sample M action sequences u0 from
the prior N (0, I) and use the ground truth dynamics of the simulator to retrieve M corresponding
image sequences, each with l frames7. We define the cost of an image trajectory as the distance
between the target image and the final image of each trajectory. In the update step, we rank the
trajectories based on their cost and fit a diagonal Gaussian distribution to the actions u0 that generated
the M0 = rM best sequences. After sampling a new set of actions un+1 from the fitted Gaussian
distributions we repeat the procedure above for a total of N iterations.
Finally, we execute the first action in the action sequence corresponding to the best rollout of the
final CEM iteration. The action at the next time step is chosen using the same procedure with the
next observation as input and reinitialized action distributions. The algorithm terminates when the
specified maximal number of planning steps Tmax has been executed or the distance to the goal is
below a set threshold.
7In practice, we clip the sampled actions to a maximal action range [-amax, +amax] before passing them to
the simulator.
18
Under review as a conference paper at ICLR 2020
Table 8: Hyperparameters for the visual planning experiments.
planning Parameters	
Max. planning timesteps (Tmax)	60
Max. per subgoal timesteps (Ts,max)	10
Keyframe prediction horizon (L)	6
# keyframe sequences (M)	200
planning horizon (l)	8
# planning sequences (M)	200
Elite fraction (r = M0/M)	0.05
# refit iterations (N)	3
max. action (amax )	1.0
dswitch	5
We switch between planned subgoals if (i) the subgoal is reached, i.e. the distance to the subgoal is
below a threshold dswitch measured in pixels, or (ii) the current subgoal was not reached for Ts,max
execution steps. We use the true goal image as an additional, final subgoal.
Algorithm 4 Full two-stage planning algorithm
Input: Keyframe model KK1：L = LSTMkey (I, zi：L)
Input: Video prediction model It：t+l = LSTMinter (I1：t-1, u2：t+l)
Input: Subgoal index update heuristics ixt+1 = f(ixt, It, K1：L)
Input: Start and goal images I1 and Igoal .
Initialize latents from prior: z0 〜N(0, I).
for i = 1 . . . Nit do
~ ʌ
Rollout keyframe model for L steps, obtain M future keyframe sequences K1：L .
Compute distance between final and goal image: C = dist(KKl, Igoal).
Choose M0 best sequences, refit Gaussian distribution: μR, σke1 = fit(K0).
Sample new latents from updated distribution: zi+i 〜N(μk+y, σk+y).
end for
Feed best sequence of latents into keyframe model to obtain subgoals: K；：L
LSTMkey (Il,zNit,o).
Set current subgoal to ixi = 1.
for t = 1 . . . Tplan do
Perform subgoal update ixt = f(ixt-i, It-i, Ki；：L).
Initialize latents from prior: uo 〜N(0, I).
for i = 0 . . . Nit do
Rollout prediction model for l steps, obtain M future sequences It：t+l .
Compute distance between final and subgoal image: C = dist(It+ι ,Kixj.
Choose M0 best sequences, refit Gaussian distribution: μi+ι, σi+ι = fit(ui).
Sample new latents from updated distribution: ui+ι 〜 N(μi+ι, σi+ι).
end for
Execute u；N ,0 and observe next image It .
end for
The parameters used for our visual planning experiments are listed in Supplemental Table 8.
19
Under review as a conference paper at ICLR 2020
Figure 13: Sample planning task executions from the test set. From a start state depicted on the left,
the robot arm successfully pushes the object into the goal position (semi-transparent object) guided
by the KEYIN subgoals. The right side of the figure shows intermediate frames of the execution
trajectories.
20