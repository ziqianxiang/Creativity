Under review as a conference paper at ICLR 2020
Auto-Encoding Explanatory Examples
Anonymous authors
Paper under double-blind review
Ab stract
In this paper we ask for the main factors that determine a classifier’s decision
making process and uncover such factors by studying latent codes produced by
auto-encoding frameworks. To deliver an explanation of a classifier’s behaviour,
we propose a method that provides series of examples highlighting semantic
differences between the classifier’s decisions. These examples are generated
through interpolations in latent space. We introduce and formalize the notion of a
semantic stochastic path, as a suitable stochastic process defined in feature (data)
space via latent code interpolations. We then introduce the concept of semantic
Lagrangians as a way to incorporate the desired classifier’s behaviour and find
that the solution of the associated variational problem allows for highlighting
differences in the classifier decision. Very importantly, within our framework the
classifier is used as a black-box, and only its evaluation is required.
1	Introduction
A considerable drawback of the deep classification paradigm is its inability to provide explanations
as to why a particular model arrives at a decision. This black-box nature of deep systems is one of
the main reasons why practitioners often hesitate to incorporate deep learning solutions in application
areas, where legal or regulatory requirements demand decision-making processes to be transparent. A
state-of-the-art approach to explain misclassification is saliency maps, which can reveal the sensitivity
of a classifier to its inputs. Recent work (Adebayo et al., 2018), however, indicates that such methods
can be misleading since their results are at times independent of the model, and therefore do not
provide explanations for its decisions. The failure to correctly provide explanations by some of
these methods lies in their sensibility to feature space changes, i.e. saliency maps do not leverage
higher semantic representations of the data. This motivates us to provide explanations that exploit the
semantic content of the data and its relationship with the classifier. Thus we are concerned with the
question: can one find semantic differences which characterize a classifier’s decision?
In this work we propose a formalism that differs from saliency maps. Instead of characterizing
particular data points, we aim at generating a set of examples which highlight differences in the
decision of a black-box model. Let us consider the task of image classification and assume a
misclassification has taken place. Imagine, for example, that a female individual was mistakenly
classified as male, or a smiling face was classified as not smiling. Our main idea is to articulate
explanations for such misclassifications through sets of semantically-connected examples which link
the misclassified image with a correctly classified one. In other words, starting with the misclassified
point, we change its features in a suitable way until we arrive at the correctly classified image.
Tracking the black-box output probability while changing these features can help articulate the
reasons why the misclassification happened in the first place. Now, how does one generate such a set
of semantically-connected examples? Here we propose a solution based on a variational auto-encoder
framework. We use interpolations in latent space to generate a set of examples in feature space
connecting the misclassified and the correctly classified points. We then condition the resulting
feature-space paths on the black-box classifier’s decisions via a user-defined functional. Optimizing
the latter over the space of paths allows us to find paths which highlight classification differences,
e.g. paths along which the classifier’s decision changes only once and as fast as possible. A basic
outline of our approach is given in Fig. 1. In what follows we introduce and formalize the notion of
stochastic semantic paths — stochastic processes on feature (data) space created by decoding latent
code interpolations. We formulate the corresponding path integral formalism which allows for a
Lagrangian formulation of the problem, viz. how to condition stochastic semantic paths on the output
1
Under review as a conference paper at ICLR 2020
(a) Paths in feature space with black-
box classifier level sets.
Figure 1: Auto-Encoding Examples Setup: Given a misclassified point x0 and representatives
x-T , xT, we construct suitable interpolations (stochastic processes) by means of an Auto-Encoder.
Sampling points along the interpolations produces a set of examples highlighting the classifier’s
decision making.
(b) Procedure for sampling paths in feature space with
auto-encoders: interpolations in latent space and decod-
ing of images.
probabilities of black-box models, and introduce an example Lagrangian which tracks the classifier’s
decision along the paths. We show the explanatory power of our approach on the MNIST and CelebA
datasets.
2	Explanations
We are concerned with the problem of explaining a particular decision of a black-box model. Many
recent works discuss the roll and provide definitions of explanations in the machine learning context
(Doshi-Velez et al., 2017; Gilpin et al., 2018; Abdul et al., 2018; Mittelstadt et al., 2019). Here we
follow Ribeiro et al. (2016) and, in broad terms, to explain we mean to provide textual or visual
artifacts that provide qualitative understanding of the relationship between the data points and the
model prediction. Attempts to clarify such a broad notion of explanation require the answers to
questions such as (1) what were the main factors in a decision?, as well as (2) would changing a
certain factor have changed the decision? (Doshi-Velez et al., 2017). To provide an answer to such
questions, one must be able to define a clear notion of factors. One can think of factors as the minimal
set of coordinates that allows us to describe the data points. This definition mirrors the behavior and
purpose of the variational auto-encoder (VAE) code — by training an auto-encoder one can find a code
which describes a particular data point. Our role here is to provide a connection between these latent
codes and the classifier’s decision. Changes on the code should change the classification decision in a
user-defined way. Defining such a code will allow us to formalize the framework required to provide
an answer to question (1) and (2) above. Following Ribeiro et al. (2016) we require explanations to
be model-agnostic, i.e independent of the classifier’s inner workings, interpretable, and expressing
local fidelity.
3	Semantics and Example Generation: VAE
Following the discussion above, we use the variational auto-encoder (VAE) formalism (Kingma &
Welling, 2013) to introduce a notion of semantics useful to qualitatively explain the decisions of a
black-box classifier.
Let us denote the feature (data) space by X and the latent linear space of codes (describing the data)
by Z, where usually dim(Z) dim(X). We consider a latent variable generative model whose
distribution Pθ(X) on X is defined implicitly through a two-step generation process: one first samples
a code Z from a fixed prior distribution P(Z) on Z and then (stochastically) maps Z to feature space
through a (decoder) distribution Pθ(X|Z), the latter being parametrized by neural networks With
parameters θ . This class of models are generically train by minimizing specific distances between the
empirical data distribution PD (X) and the model distribution Pθ (X). VAE approaches this problem
by introducing an encoder distribution Qφ(Z|X), parametrized by neural networks with parameters
φ, which approximates the true posterior distribution Pθ(Z|X) and minimizing a variational upper
2
Under review as a conference paper at ICLR 2020
bound on the Kullback-Leibler divergence DKL between Pθ(X) and PD(X). This bound reads
LVAE = EPD(X) {-EQφ(Z∣X) [log pθ (XIz)]+ DKL (Qφ(ZIX ),P (Z))},	⑴
where pθ (χ∣z) denotes the decoder's density and yields the likelihood function of the data given the
code1. Once the model is trained one can think of the inferred latent code as containing some high-
level description of the input data. Below we will use such inferred code to modify in a controlled
way the features of a given input data point.
4	Explaining Through Examples: A Plaintiff S cenario
We define a defendant black-box model b(l, x) as a classifier which yields the probability that the
data point x ∈ X in feature (data) space belongs to the class l ∈ L, where L is a set of classes.
Assume the model b(l, x) is expected to perform by its users or clients, in a dataset D = {(li, xi)},
where xi ∈ X and li ∈ L is the label that xi belongs to. 2 Suppose now that the following litigation
case emerges. The black-box model b has assigned the data point x0 to the class l0 . Accordingly, a
plaintiff presents a complaint as the point x0 should have been classified as lt . Furthermore, assume
we are given two additional representative data points x-T , xT which have been correctly classified
by the black-box model to the classes l-T, lT, respectively — as expected by e.g. the plaintiff, the
defendant (if agreed), or the institution upon which the complain or litigation case is presented (say,
the court). With this set-up in mind, we propose that an explanation why x0 was misclassified can
be articulated through an example Set E = {χ-τ,...,xo,..., XT}, where Xt 〜Pθ(X∣Z = zt).
Here Pθ(XIZ = zt) is a given decoder distribution and the index t runs over semantic changes
(properly defined below) that highlight classification decisions. This example set constitutes the
context revealing how factor changes impact the classification decision (see Section 2). One expects
that human oriented explanations are semantic in character. One can understand the expression bigger
eyes will change the classification. As opposed to changes in some specific pixels 3. The index t
would run over these changes e.g. would make the eyes bigger.
5	Stochastic Semantic Processes and Corresponding Paths
In this section we first formalize the notion of semantic change by introducing the concept of
(stochastic) semantic interpolations in feature space X . This will allow us to generate examples
which provide local fidelity, as the examples are smooth modifications of the latent code associated
to the plaintiff data point X0. We then define a collection of probability measures over semantic paths
in X . These measures will be used later in Section 6 to constrain the paths to be explanatory with
respect to the classifier’s decision.
5.1	Semantic Interpolations
One of the main motivations behind the VAE formalism is the ability of the inferred latent code z to
provide semantic high-level information over the data set. If one is to generate examples which have
characteristics common to two different data points, say X0 and XT from the litigation case, one can
perform interpolations between the latent codes of these points, that is z0 and zT , and then decode the
points along the interpolation. A main observation is that these interpolations in latent space can be
used to induce certain interpolating stochastic processes on feature space4 X. We refer to these
as stochastic semantic processes. In what follows, we first focus on linear latent interpolations, i.e.
z(t) := tz0 + (1 - t)zT,	(2)
and construct an interpolating stochastic semantic process Xt on X by using the decoder distribution
Pθ(XIZ = z(t)). In practice, the generation process of such stochastic interpolations consists then
1where the average over Qφ(Z|X) is performed using the reparametrization trick (Kingma & Welling, 2013).
2Notice that, as a true black-box classifier, one does not know the nature of the true dataset where the model
was trained.
3the behavior of say adversarial examples
4Moreover, under appropriate assumptions on the auto-encoder mappings (Pθ , Qφ) the proposed induced
stochastic processes could posses additional properties (e.g. trajectory regularity, controlled moments, etc).
3
Under review as a conference paper at ICLR 2020
of three steps: (i) sample Qφ(Z|X) at the end points xo and XT using the reparametrization trick
(Kingma & Welling, 2013), (ii) choose a set of points zt along the line connecting z0 and zT and (iii)
decode the Zt by sampling Pθ(X|Z = zt). A formal description of this procedure is given below, in
subsection 5.2, and an impression of the stochastic process thus constructed is presented in Fig. 1b.
5.2	An Approach via Explicit Family of Measures
We observe that for every sequence of points {ti}in=0 there is a natural measure on piecewise linear
paths starting at x0 ∈ X and terminating at xT ∈ X. More precisely, we define the probability of a
piecewise linear path x(t) with nodes x1 , x2 . . . , xn ∈ X as
dPt0,...,tn(x(t))
Pθ (Xi∣z(ti))
qφ(zo∖xo)qφ(zτ∣XT) dzo dzτ,
(3)
where qφ,pθ label the densities of Qφ, Pθ, respectively, and where z(t) is defined by eq. (2) 5.
In other words, for every pair of points xo and xT in feature space, and its corresponding code
samples zo 〜 Qφ(Z∖X = xo) and ZT 〜 Qφ(Z∖X = XT), the decoder Pθ (X ∖Z) induces a measure
over the space of paths {x(t)∖x(0) = xo, x(T) = xT}. Formally speaking, the collection of measures
dPt0,...,tn given by different choices of points {ti}in=o in (3) defines a family of consistent measures
(cf. Definition 2 in the Appendix, Subsection D.1). This implies that these different measures are
assembled into a stochastic process on feature space X over the continuous interval [0, T]:
Proposition 1. The collection of measures prescribed by (3) induces a corresponding continuous-time
stochastic process. Moreover, under appropriate reconstruction assumptions on the auto-encoder
mappings Pθ , Qφ, the sample paths are interpolations, that is, start and terminate respectively at
xo , xT almost surely.
The statement goes along the lines of classical results on existence of product measures. For the
sake of completeness we provide all the necessary technical details in the Appendix, Subsection D.
Another important remark is that the stochastic semantic process construction in Proposition 1 is just
one way to define such a process — there are other natural options, e.g. in terms of explicit transition
kernels or It6 processes.
6	Principle of Least Semantic Action
Having described a procedure to sample stochastic semantic processes in X, we need to discover auto-
encoding mappings (Pθ, Qφ) that give rise to reasonable and interesting stochastic paths. Specifically,
to generate examples which are able to explain the defendant black-box model b(l, x) in the current
litigation case (Section 4), one needs to ensure that semantic paths between the data points xo and xT
highlight classification differences, i.e. classifications of the model along this path are far apart in the
plaintiff pair of labels. Thus, to design auto-encoding mappings Pθ , Qφ accordingly, we propose an
optimization problem of the form
min SPθ,Qφ [Xt],	(4)
θ,φ
where Xt is a stochastic semantic process and SPθ ,Qφ is an appropriately selected functional that
extracts certain features of the black-box model b(l, x).
The minimization problem (4) can be seen in the context of Lagrangian mechanics. For a given
stochastic semantic process Xt , and given initial and final feature “states" xo and xT , we introduce
the following function, named the model-b semantic Lagrangian
L : [0, 1] × X × X → R,	(t, xo, xT) 7→ L[Xt, xo, xT],	(5)
which gives rise to the semantic model action:
S[Xt] := Z TL[Xt,xo,xT]dt.
o
(6)
5We remark that the integral (eq. 3) is, moreover, finite, if, for example, the densities pθ are bounded with
respect to z .
4
Under review as a conference paper at ICLR 2020
In mechanics, the optimization given by suitable Lagrangians delivers physically meaningful paths,
e.g. those specified by the equations of motion (Landau & Lifshitz, 2013). In our case, a guiding
intuition is that the semantic Lagrangian should reflect how the black-box model takes decisions
along the path6 Xt, starting at x0 and ending at xT. In this way, the minimization of the semantic
action (i.e. finding minimizing paths Xt) should make such classification aspects prominent along
the example set.
Our problem, viz. to find encoding mappings Pθ , Qφ which yield explainable semantic paths with
respect to a black-box model, is then a constrain optimization problem whose total objective function
we write as
L(θ, φ) := LVAE(θ, φ) + λ EdP[x(t)]S[x(t)],	(7)
where LVAE is given by eq. (1), S[x(t)] corresponds to the Lagrangian action and λ is an hyper
parameter controlling the action’ scale. The average over the paths (Majumdar, 2007; Feynman &
Hibbs, 1965) is taken with respect to the stochastic paths and the corresponding measure dP [x(t)]
from Proposition 1, that is, the path integral
Z1 Kn
L[x(t), x0, xT]dP [x(t)] ≈ nK XX L[χk, x0, xT],	(8)
n kt
where xtk labels the tth point along the the kth path, sampled as described in Section 5, n is the
number of points on each path, K is the total number of paths, and the estimator on the right hand
side corresponds to an explicit average over paths7.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Algorithm 1: PATH Auto-Encoder
Data: Dataset D = {(xi, li)} Litigation case
(x-T, xT, l0, x0, lt, B(l|x))
Encoder Pθ(x|z), Decoder Qφ(z∣χ)
while φ and θ not converged do
Draw {x1, ..., xn} from the training set
Calculate Auto-Encoder Loss LVAE(θ, φ)
Sample Litigation Codes
Z-T ~ Qφ(Z∣x-τ), zo ~ Qφ(Z∣xo),
ZT ~ Qφ(Z∣χτ)
Generate Latent Interpolations
tk ~ Sort(Uniform(0,1))
Zjk = Z-T × tjk + Z0 × (1 - tjk )
Sample k Paths in Feature Space
Xk ~ pθ (XIZk)
Evaluate Semantic Action for each path k
and average over k
LS = EdP[x(t)] [S(x(t))]
Update Pθ and Qφ by descending:
LVAE (θ, φ) + LS (θ, φ)
end
return Pθ, Qφ
classifier’s decisions along it changes as quickly
In practice, both LVAE and the action term are
optimized simultaneously. Note that the VAE
loss function LVAE is trained on the entire data
set on which the black-box performs. The ac-
tion term, in contrast, only sees the x0 and xT
points. This can be seen explicitly in Algorithm
1, which shows an overview of the auto-encoder
pair training algorithm. Let us finally note that,
drawing analogies with the adversarial formal-
ism (Goodfellow et al., 2015), the defendant
black-box model plays the role of a fixed dis-
criminator, not guiding the example generation,
but the interpolations among these examples.
6.1	The Choice of Lagrangians
There are plenty of options for Lagrangian
functionals that provide reasonable (stochastic)
example-paths — roughly speaking, we attempt
to define an objective value for a certain subjec-
tive notion of explanations. In what follows we
illustrate one particular such Lagrangian8
Minimum Hesitant Path
We want to find an example path such that the
as possible, as to highly certain regions in X . In
6For example, a valid Lagrangian should reflect whether the the decisions along the path were taken with
sufficient certainty, or whether the probability of the decisions were gradually changing.
7Note that, as mention in Sec. 5, one must resort to the reparametrization trick to sample from Qφ and
efficiently evaluate the gradients of the action term. Note also that Proposition 1 tells us that different choices of
the discrete (approximation) grids in the t integration are qualitatively related to the same underlying stochastic
interpolation process.
8For further suggestions concerning the Lagrangian choice we refer to the Appendix, Subsection D.3. There
we also compute the corresponding Euler-Lagrange equations that shed further light on the solutions of the
variational problem — moreover, their solution could be used in the training process as well.
5
Under review as a conference paper at ICLR 2020
222-22工工NNNNNK产产产产产产产U3W?■手孑干干干
VAE-EDGE
PATH-VAE
Figure 2: Probability Paths for the litigation case l0 = 2, lT = 7. Y axis corresponds to classification
probability and x axis corresponds to interpolation index. Interpolation images for a specific paths
are presented below the x axis.
other words, the path is forced to stay in regions where the black-box produces decisions with
maximum/minimum probability. An intuitive way to enforce this is via the simple Lagrangian
L1(x(t),x0,xT) := - (b(lT, x(t)) - b(l0, x(t)))2,	(9)
where l0 , lT are the labels of the litigation case in question. Roughly speaking, given the appropriate
initial conditions, the paths that minimize the action associated to L1 are paths that attempt to keep
L1 close to 1 over almost the entire interpolation interval.
Other regularizers
Additionally we require b(lT, x(t)) to be a monotonous function along the interpolating path x(t).
Furthermore, in accordance with Proposition 1 we require certain level of reconstruction at the end
points. To enforce these conditions we introduce the regularizers rm, re which are described in detail
in subsection D.4 of the Appendix.
The total objective function is therefore
L(θ, φ) := LVAE(θ, φ) + λ EdP [x(t)]S1[x(t)] +λmrm+λere,	(10)
where λ, λm , λe are hyper-parameters and S1 is the action associated to the minimum hesitant
Lagrangian L1 in eq. (9).
7	Experimental results
We evaluate our method in two real-world data sets: MNIST, consisting of 70k Handwriting digits,
(LeCun, 1998) and CelebA (Liu et al., 2015), with roughly 203k images. We use a vanilla version
of the VAE (Kingma & Welling, 2013) with Euclidean latent spaces Z = Rdz and an isotropic
Gaussian as a prior distribution P(Z) = N(Z|0, Idz). We used Gaussian encoders, i.e. Qφ(ZX)=
N (Z ∣μφ(X), Σφ(X)), where μφ, σφ are approximated with neural networks of parameters φ, and
Bernoulli decoders Pθ(X|Z). We compare the standard VAE, VAE-EDGE (VAE augmented with the
edge loss re) and PATH-VAE (our full model, eq. (10)). The black-box classifier b(l, x) is defined as
a deep network with convolutional layers and a final soft-max output layer for the labels. Details of
the specifics of the architectures as well as training procedure are left to the Appendix.
For MNIST we studied a litigation case wherein l-T , lT = 2, 7 and l0 = 2, whereas its
true label (i.e. that of x0) is lt = 7 (see Section 4). The results are presented in Fig. 2.
VAE delivers interpolations which provide uninformative examples, i.e. the changes in the out-
put probability b(l0 , x) cannot be associated with changes in feature space. In stark contrast,
PATH-VAE causes the output probability to change abruptly. This fact, together with the corre-
sponding generated examples, allows us to propose explanations of the form: what makes the
black-box model classify an image in the path as two or seven, is the shifting up of the lower
stroke in the digit two as to coincide with the middle bar of the digit seven. Similarly, the
upper bar of the digit seven (especially the upper left part) has a significant decision weight.
6
Under review as a conference paper at ICLR 2020
In order to provide a more quantitative analysis we demonstrate
the capability of our methodology to control the path action
while retaining the reconstruction capacity. Hereby, we use
not only the VAE as the underlying generative model, but also
Wasserstein Auto-Encoder (WAE) (Bousquet et al., 2017) and
Adversarial Auto-Encoder (AAE) (Goodfellow et al., 2015),
i.e. we simply change LVAE in eq. (7) with the loss of WAE or
AAE. The theoretical details and corresponding architectures
are presented in the Appendix. We present, in Fig. 3, the action
values defined over random litigation end pairs (x-T , xT). The
PATH version of the model indeed yields lower action values.
Furthermore, these models tend to reduce the variance within
the different paths. This is expected since there is one path that
minimizes the action, hence, the distribution will try to arrive
at this specific path for all samples. In order to compare with
other explanation models, we define a saliency map with the
interpolations obtained in our methodology. We defined the
interpolation saliency as the sum over the differences between
interpolation images weighted with the probability change of
the black-box classifier through the interpolation path. We see
in Fig. 4 the comparisons among different methods. While
the standard methods only show local contributions to a clas-
sification probability, our saliency maps show the minimum
changes that one is to apply to the dubious image in order to
change the decision to the desired label. Our approach reveals
that the curvature of the lower bar is decisive to be classified as
a two, while the style of the upper bar is important to be clas-
sified as a seven. Further, we provide a sanity check analysis
(Adebayo et al., 2018) by studying the rank correlation between
original saliency map and the one obtained for a randomized
layers of the black-box classifier, shown in Fig. 5. As desired,
our proposed saliency map decorrelates with the randomized
version.
For the CelebA dataset we use a black-box classifier based on
the ResNet18 architecture (He et al., 2016). We investigate
two specific misclassifications. In the first case, a smile was
not detected (Fig. 6 a). Here we only interpolate between the
misclassified image (left) and a correctly classified one (right),
of the same person. Interpolations obtained by VAE are not
informative: specific changes in feature space corresponding
to changes in the probability cannot be detected since the latter
changes rather slowly over the example path. This observation
also holds true for the VAE-EDGE model, except that the ex-
VAE VAE-PATHSWAEWAE-PATHSAAE AAE-PATHS
Figure 3: Average action for Mini-
mum Hesitant Lagrangian. PATHS-
architectures trained to minimize
semantic action.
Figure 4: Saliency Maps Com-
parison: From left to right col-
umn: Vanilla Gradients, Smooth
Gradients, Guided BackProp, Grad
CAMP, Interpolations, Difference
with Representative. Upper row
corresponds to l-T = 2, lower row
to lT = 7.
Figure 5: Rank correlation between
the original explanation and the ran-
domized explanation derived up to
that layer.
amples are sharper. Finally, our PATH-VAE model yields a sharp change in the probability along
with a change of the visible teeth (compare the third and fifth picture in the example path), revealing
that this feature (i.e. teeth visibility) could constitute, from a human standpoint, a decisive factor in
the probability of detecting a smile for the given black-box model. It is important to note that these
observations represent one of many possible path changes which could change the classifier decision.
This is constrained by the current realization and representative end points. The important result is
that our methods are able to shape the behavior of the classifier along the path. Further experimental
examples are provided in Section C of the Appendix.
8	Related Work
The bulk of the explanation literature for deep/black-box models relies on input dependent method-
ologies. Gradient Based approaches (Simonyan et al., 2013; Erhan et al., 2009) derive a sensibility
7
Under review as a conference paper at ICLR 2020
Figure 6: Probability Paths for the case of detecting a smile in images of celebrities. Y axis
corresponds to classification probability and x axis corresponds to interpolation index. Interpolation
images for a specific paths are presented below the x axis. The images are vertically aligned with a
corresponding tick in the x-axis determining the interpolation index of the image
score for a given input example and class label by computing the gradient of the classifier with
respect to each input dimension. Generalizations of this approach address gradient saturation by
incorporating gradients’ values in the saliency map (Shrikumar et al., 2017) or integrating scaled
versions of the input (Sundararajan et al., 2017). Ad hoc modifications of the gradient explanation
via selection of the required value (Springenberg et al., 2015),(Zeiler & Fergus, 2014), as well as
direct studies of final layers of the convolutions units of the classifiers (Selvaraju et al., 2016), are
also provided. In contrast to gradient based approaches, other categories of explanatory models rely
on reference based approaches which modify certain inputs with uninformative reference values
(Shrikumar et al., 2017). Bayesian approaches treat inputs as hidden variables and marginalize over
the distribution to obtain the saliency of the input (Zintgraf et al., 2017). More recent generaliza-
tions exploit a variational Bernoulli distribution over the pixels values (Chang et al., 2018). Other
successful methodologies include substitution of black-box model with locally interpretable linear
classifiers. This is further extended to select examples from the data points in such a way that the latter
reflect the most informative components in the linear explanations, (Ribeiro et al., 2016). Studies
of auto-encoder interpolations seek to guarantee reconstruction quality. In (Arvanitidis et al., 2018)
the authors characterize latent space distortions compared to the input space through a stochastic
Riemannian metric. Other solutions include adversarial cost on the interpolations such as to improve
interpolation quality compared to the reconstructions, (Berthelot et al., 2018). Examples which are
able to deceive the classifier’s decisions have been widely studied in the framework of adversarial
examples (Goodfellow et al., 2015). These methodologies, however, do not provide interpretable
explanations or highlight any semantic differences that lead to the classifier’s decisions. Finally,
the Auto-Encoder framework can also naturally be seen as a tool for dimensionality reduction.
Geometrically speaking, assuming that the data set approximately lies along a manifold embedded in
feature space X, one can interpret the encoder, decoder as the coordinate map (chart) and its inverse.
From this point of view, our approach above translates to finding coordinate charts with additional
constraints on mapping the segments from z0 to zT to appropriate (stochastic) curves between x0 and
xT .
9	Conclusion
In the present work we provide a novel framework to explain black-box classifiers through examples
obtained from deep generative models. To summarize, our formalism extends the auto-encoder
framework by focusing on the interpolation paths in feature space. We train the auto-encoder, not
only by guaranteeing reconstruction quality, but by imposing conditions on its interpolations. These
conditions are such that information about the classification decisions of the model B is encoded
in the example paths. Beyond the specific problem of generating explanatory examples, our work
formalizes the notion of a stochastic process induced in feature space by latent code interpolations,
as well as quantitative characterization of the interpolation through the semantic Lagrangian’s and
actions. Our methodology is not constrained to a specific Auto-Encoder framework provided that
mild regularity conditions are guaranteed for the auto-encoder.
8
Under review as a conference paper at ICLR 2020
References
Ashraf Abdul, Jo Vermeulen, Danding Wang, Brian Y. Lim, and Mohan Kankanhalli. Trends
and trajectories for explainable, accountable and intelligible systems: An hci research agenda.
In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, CHI
’18, pp. 582:1-582:18, New York, NY, USA, 2018. ACM. ISBN 978-1-4503-5620-6. doi:
10.1145/3173574.3174156. URL http://doi.acm.org/10.1145/3173574.3174156.
Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity
Checks for Saliency Maps. In Advances in Neural Information Processing Systems, pp. 9525-9536,
2018.
Georgios Arvanitidis, Lars Kai Hansen, and S0ren Hauberg. Latent Space Oddity: on the Curvature
of Deep Generative Models. International Conference on Learning Representations, 2018.
Christian Bar and Frank Pfaffle. Wiener Measures on Riemannian Manifolds and the Feynman-Kac
Formula. Preprints des Institutsfur Mathematik der Universitat Potsdam 1, 2012.
David Berthelot, Colin Raffel, Aurko Roy, and Ian J. Goodfellow. Understanding and Improving
Interpolation in Autoencoders via an Adversarial Regularizer. CoRR, abs/1807.07543, 2018.
Olivier Bousquet, Sylvain Gelly, Ilya Tolstikhin, Carl Johann Simon-Gabriel, and Bernhard Scholkopf.
From Optimal Transport to Generative Modeling: the VEGAN cookbook. Technical report, 2017.
Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duvenaud. Explaining Image
Classifiers by Counterfactual Generation. arXiv preprint arXiv:1807.08024 [cs.CV], 2018.
Manfredo P. do Carmo. Differential Geometry of Curves and Surfaces. Prentice Hall, 1976. ISBN
978-0-13-212589-5.
Finale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam Gershman, David O’Brien, Stuart
Schieber, James Waldo, David Weinberger, and Alexandra Wood. Accountability of AI Under the
Law: The Role of Explanation. CoRR, abs/1711.01134, 2017.
Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing Higher-Layer
Features of a Deep Network. University of Montreal, 1341(3):1, 2009.
Richard Feynman and Albert Hibbs. Quantum Mechanics and Path Integrals. International Series in
Pure and Applied Physics, McGraw-Hill, 1965.
L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, and L. Kagal. Explaining explanations: An
overview of interpretability of machine learning. In 2018 IEEE 5th International Conference on
Data Science and Advanced Analytics (DSAA), pp. 80-89, Oct 2018. doi: 10.1109/DSAA.2018.
00018.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adversarial
Examples. International Conference on Learning Representations, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv preprint
arXiv:1312.6114, 2013.
Lev Davidovich Landau and Evgenii Mikhailovich Lifshitz. Course of Theoretical Physics. Elsevier,
2013.
Yann LeCun. The MNIST Database of Handwritten Digits. NEC Research Institute, 1998.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep Learning Face Attributes in the Wild.
In Proceedings of the IEEE international conference on computer vision, pp. 3730-3738, 2015.
9
Under review as a conference paper at ICLR 2020
Satya N Majumdar. Brownian Functionals in Physics and Computer Science. In The Legacy Of
Albert Einstein: A Collection of Essays in Celebration of the Year of Physics ,pp. 93-129. World
Scientific, 2007.
Brent Mittelstadt, Chris Russell, and Sandra Wachter. Explaining explanations in ai. In Proceedings
of the Conference on Fairness, Accountability, and Transparency, FAT* ’19, pp. 279-288, New
York, NY, USA, 2019. ACM. ISBN 978-1-4503-6125-5. doi: 10.1145/3287560.3287574. URL
http://doi.acm.org/10.1145/3287560.3287574.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why Should I Trust You?: Explaining the
Predictions of any Classifier. In Proceedings of the 22nd ACM SIGKDD international conference
on knowledge discovery and data mining, pp. 1135-1144. ACM, 2016.
Ramprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh,
and Dhruv Batra. Grad-CAM: Why did you say that? arXiv preprint arXiv:1611.07450, 2016.
Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning Important Features Through
Propagating Activation Differences. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 3145-3153, 2017.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep Inside Convolutional Networks:
Visualising Image Classification Models and Saliency Maps. CoRR, abs/1312.6034, 2013.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A. Riedmiller. Striving for
Simplicity: The All Convolutional Net. International Conference on Learning Representations
(Workshop), 2015.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic Attribution for Deep Networks. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3319-3328,
2017.
Michael Taylor. Partial Differential Equations II: Qualitative Studies of Linear Equations. Applied
Mathematical Sciences 116, Springer-Verlag New York, 2011.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein Auto-
Encoders. In International Conference on Learning Representations, 2018.
Matthew D Zeiler and Rob Fergus. Visualizing and Understanding Convolutional Networks. In
European Conference on Computer Vision, pp. 818-833. Springer, 2014.
Luisa M. Zintgraf, Taco S. Cohen, Tameem Adel, and Max Welling. Visualizing Deep Neural
Network Decisions: Prediction Difference Analysis. 5th International Conference on Learning
Representations, 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.
A Appendix
B	Models Training Details
B.1 A Gaussian CNN Encoder and CNN Decoder: MNIST and CelebA
There was no preprocessing on the 28x28 MNIST images. The models were trained with up to 100
epochs with mini-batches of size 32 - we remark that in most cases, however, acceptable convergence
occurs much faster, e.g. requiring up to 15 epochs of training. Our choice of optimizer is Adam with
learning rate α = 10-3. The weight of the KL term of the VAE is λkl = 1, the path loss weight is
λp = 103 and the edge loss weight is λe = 10-1. We estimate the path and edge loss during training
by sampling 5 paths, each of those has 20 steps.
Encoder Architecture
x ∈ R28×28×3 → Conv64 → BN → ReLU
→ Conv128 → BN → ReLU
→ Conv256 → BN → ReLU → FC8
10
Under review as a conference paper at ICLR 2020
Decoder Architecture
z ∈ R8 → FC4×4×512
→ FSConv256 → BN → ReLU
→ FSConv128 → BN → ReLU
→ FSConv64 → Sigmoid
Both the encoder and decoder used fully convolutional architectures with 3x3 convolutional filters
with stride 2. Convk denotes the convolution with k filters, FSConvk the fractional strides convolution
with k filters (the first two of them doubling the resolution, the third one keeping it constant), BN
denotes batch normalization, and as above ReLU the rectified linear units, FCk the fully connected
layer to Rk.
The pre-processing of the CelebA images was done by first taking a 140x140 center crop and then
resizing the image to 64x64. The models are trained with up to 100 epochs and with mini-batches of
size 128. Our choice of optimizer is Adam with learning rate α = 10-3. The weight of the KL term
of the VAE is λkl = 0.5, the path loss weight is λp = 0.5 and the edge loss weight is λe = 10-3.
We estimate the path and edge loss during training by sampling 10 paths, each of those has 10 steps.
Encoder Architecture
x ∈ R64×64×3 → Conv64 → BN → ReLU
→ Conv128 → BN → ReLU
→ Conv256 → BN → ReLU
→ Conv512 → BN → ReLU → FC500
Decoder Architecture
z ∈ R500 → FC4×4×512
→ FSConv256 → BN → ReLU
→ FSConv128 → BN → ReLU
→ FSConv64 → Sigmoid
Both the encoder and decoder used fully convolutional architectures with 3x3 convolutional filters
with stride 2. Convk denotes the convolution with k filters, FSConvk the fractional strides convolution
with k filters (the first two of them doubling the resolution, the third one keeping it constant), BN
denotes batch normalization, and as above ReLU the rectified linear units, FCk the fully connected
layer to Rk.
11
Under review as a conference paper at ICLR 2020
C Further Results
C.1 MNIST
H∙2∙22 22 N工3K产产尸产产尸尸仔『下¥¥∙¥∙?■平¥孑
VAE-EDGE
PATH-VAE
，22222上22%IXA户产产产尹尹/宇竽早？学W不斗干
Figure 7: Interpolation between 2 and 7. It is seen that the Path-VAE interpolation optimizes both
probabilities (P(2) and P(7)) according to the chosen Lagrangian - in this case the minimum hesitant
L1.
6 Q6 GGGGGGGGGSassαααooe今夕夕夕夕夕吁夕
VAE-EDGE
G (o (a (a G C GCGGGqqGGGGGqqGGGOQ 9。夕夕夕
PATH-VAE
£ £ £ 6GCGGG⅛>GG⅛∣⅛⅝<⅛c⅝c⅛qqqqt?『勺吁?宁勺宁斤
Figure 8:	Interpolation between 6 and 9. The Path-VAE interpolation appears to emphasize the
"opening and closing" of the loop.
12
Under review as a conference paper at ICLR 2020
VAE-EDGE
PATH-VAE
3 3 333333333333 3333355SS35SS55S
Figure 9:	Interpolation between 3 and 5. The translation of the "upper bar" is a prominent feature of
the Path-VAE interpolation.
C
2
CELEBA
1
ω<> ω8ω1ω<> 3V>-Hlvd
QayaLLLL厂
Figure 10: Probability Paths for the case of detecting the sex in images of celebrities. Y axis
corresponds to classification probability and x axis corresponds to interpolation index. Interpolation
images for a specific paths are presented below the x axis. The images are vertically aligned with a
corresponding tick in the x-axis determining the interpolation index of the image
13
Under review as a conference paper at ICLR 2020
D Stochastic Semantic Processes: Proof of Proposition 1
Briefly put, the construction we utilize makes use of the well-known notion of consistent measures,
which are finite-dimensional projections that enjoy certain restriction compatibility; afterwards, we
show existence by employing the central extension result of Kolmogorov-Daniell.
D.1 Collections of Consistent Measures
We start with a couple of notational remarks.
Definition 1. Let S, F be two arbitrary sets. We denote
SF := {f : F → S},	(11)
that is, the set of all maps F → S.
Definition 2. Let (S, B) be a measurable space and let G ⊆ F ⊆ [0, T] for some positive number T.
We define the restriction projections πF,G by
∏F,G ： SF → SG,	f ∈ Sf → f |g ∈ SG	(12)
Moreover, for each F ⊆ [0, T] the restriction projections induce the σ-algebra BF which is the
smallest σ-algebra on SF so that all projections
πF,{t} : SF → S{t} = S, ∀t ∈ F,	(13)
are measurable. In particular, the projections πF,G are measurable with respect to BF , BG.
Definition 3. Let us denote by Fin([0, T]) the set of all finite-element subsets of [0, T]. A collection
of finite measures {(μF, BF), F ∈ Fin([0, T ])} is called consistent ifit is push-forward compatible
with respect to the restriction projection mappings, i.e.
(∏F,G)* μF = μG,	∀F, G ∈ Fin([0, T]), G ⊆ F∙	(14)
Here
(∏F,G)* μF(A) ：= μF(∏-1G(A)),	∀A ∈ BG	(15)
Proposition 2. Let F = {0 ≤ tι <t2 < …< tn ≤ T} ∈ Fin([0, T ]) be an arbitrary finite set.
The mapping
n
ɪɪpθ (xi∣Zi) qφ(zo∣xo)qφ(zτ ∣xτ )dzodzτ dx∖∙∙∙dxn (16)
i=1
defines a consistent collection of finite measures.
μF(A) := χA(x1,x2, ∙ ∙ ∙ ,xn)
Proof. Let us fix
FI= {0 ≤ t1 < t2 < …< tn ≤ T} ∈ Fin([0,T]),
F2 ：= {0 ≤ tι <t* <t2 < …<tn ≤ T}∈ Fin([0,T]),
Without loss of generality, it suffices to check consistency for the pair (F1 , F2 ). We have
(πFι,F2 )*μF2(A) = μF2
πF-11,F2 (A)
χπ-1	(A)(x1,s,x2, ∙ ∙ ∙,xn)
n
pθ (xi |zi )
=1
X Pθ(s|zt*)qφ(zo∣xo)qφ(zτ∣xτ)dsdzodzτdx1dx2 ∙∙∙dxn
(17)
(18)
(19)
(20)
(21)
χA(x1, x2, ∙ ∙ ∙ , xn)
YPθ(xi∣Zi)j qφ(zo∖xo)qφ(zτ∣xτ)dzodzτdxι∙∙∙dxn
=1	(22)
= μF1 (A),	(23)
where we have used L1 -finiteness and integrated out the s variable via Fubini’s theorem. Note also,
that by the definitions above
χπ-1 (A)(x1, s, x2, ∙ ∙ ∙ , xn) = χA(x1, x2, ∙ ∙ ∙ , xn)∙	(24)
□
for any fixed s ∈ X.
14
Under review as a conference paper at ICLR 2020
We briefly recall the following classical result due to Kolmogorov and Daniell:
Theorem 1 (Theorem 2.11, Bar & Pfaffle (2012)). Let (S, B(S)) be a measurable space with S
being compact and metrizable and let I be an index set. Assume that for each J ∈ Fin(I) there exists
a measure μJ on SJ, BJ, such that thefollowing compatibility conditions hold:
μJ1 = μJ2 ◦ π-1, ∀Jι ⊆ J2 ∈ Fin(I).	(25)
Here πJ1 : SJ2 → SJ1 denotes the canonical projection (obtained by restriction).
Then, there exists a unique measure μ on (SI, Bl) such thatfor all J ∈ Fin(I) one has
μ ◦ πJ1 = μJ.	(26)
We recall that a well-known way to construct the classical Wiener measure and Brownian motion
is precisely via the aid of Theorem 1 (Taylor (2011)). We are now in a position to construct the
following stochastic process.
Proposition 3. There exists a continuous-time stochastic process Xt : [0, T] → RD satisfying
P((Xt1,Xt2,...,Xtn) ∈A) =	χA(x1,x2,...,xn)	(27)
× (Ypθ(χi∣Zi)) qφ(zo∣χo)qφ(zτ∣χτ)dχι ...dxn.	(28)
=	(29)
Moreover, for small positive numbers , δ we have X0 ∈ Bδ(x0) with probability at least (1 - ),
provided the reconstruction error of encoding/decoding process is sufficiently small. In particular, if
x0 stays fixed after the application of encoder followed by decoder, then X0 = x0 almost surely. A
similar statement holds also for the terminal point Xt and xT respectively.
Proof. By applying Theorem 1 to the collection of consistent finite measures prescribed by Proposi-
tion 2 We obtain a measure μ on the measurable space (S[0,T], B[0,T]). Considering the probability
space (S[0,T], B[0,T], μ) We define stochastic process
Xt := π[0,T],{t} : S[0,T] →S.	(30)
It folloWs from the construction and the Theorem of Kolmogorov-Daniell that
P ((Xt1 , Xt2 , . . . , Xtn ) ∈ A) is expressed in the required Way. This shoWs the first claim of
the statement.
NoW, considering a small ball Bδ(x0) We have
P(Xo ∈ Bδ(xo)) = /XBδ(xo)(x)pθ(x∣zo)qφ(zo∣xo)qφ(zτ∣XT)dxdzodzτ	(31)
=/ XBδ (χo)(x)Pθ (x∣z0)qφ(z0∣x0)dxdz0	(32)
:= R(x0, χBδ(x0)).	(33)
Here, the function R(χ*, U) measures the probability that the input x* is decoded in the set U. Thus,
if the reconstruction error gets smaller, R converges to 1. This implies the second statement.
Finally, if We assume that the auto-encoder fixes x0 in the sense above, We similarly get
P(Xo = xo) = / X{χo}(x)pθ (x∣zo)qφ(zo∣X0 )qφ(zτ ∣xτ IdxdzOdzT	(34)
=/ X{χo}(x)pθ (x∣zo)qφ(zo∣X0 )dxdzo	(35)
= δx0(χ{x0})	(36)
= 1.	(37)
□
15
Under review as a conference paper at ICLR 2020
D.2 Concerning the Regularity of Sample Paths
An important remark related to the the variational problem (4) is the following: one could develop
plenty of meaningful functionals SPθ ,Qφ that involve taking velocities or higher derivatives - thus
one is supposed to work over spaces of curves with certain regularity assumptions. However, as
stated above we are working over stochastic paths Xt whose regularity is, in general, difficult to
guarantee. A straightforward way to alleviate this issue is to consider a "smooth" version of the curve
Xt - e.g. by sampling Xt through a decoder with controllable or negligible variance or by means of
an appropriate smoothing. Furthermore, one could also approach such stochastic variational analysis
via Malliavin calculus - however, we do not pursue this direction in the present work.
We now briefly discuss a few remarks about the regularity of the stochastic semantic process from
Proposition 1. First, we state a well-known result of Kolmogorov and Chentsov:
Theorem 2 (Theorem 2.17, Bar & Pfaffle (2012)). Let (M, ρ) be a metric measure space and let
Xt , t ∈ [0, T] be a stochastic process. Suppose that there exists positive numbers a, b, C, with the
property
E [ρ(Xs,Xt)a] ≤ C|t - SI(I+叱 ∀s,t,∣s - t∣<e	(38)
Then, there exists a version Yt, t ∈ [0, T] of the Stochastic process Xt whose paths are α-Holder
continuous for any α ∈ (0, b/a).
Thus, roughly speaking, an estimate on E [ρ(Xs, Xt)a] can be regarded as a measure of the extent to
which Theorem 2 fails. To give an intuitive perspective, let us consider the stochastic process given
by Proposition 1 and, considering only the points Xs, Xs+δ for a small positive number δ, let us
write the expectation in (38) as:
kxs+δ
-Xsk Pθ(xs+δ∣Zs+δ)pθ(xs∣Zs)qφ(zo∣xo)qφ(zτ∣xτ)dxsdxtdzodzτ,
(39)
where we have used the standard Euclidean distance. To estimate the integral further, let us for
simplicity assume that the encoder is deterministic and the decoder is defined via a Gaussian Ansatz
of the type μ(z) + σ(z) 0 E for a normal Gaussian variable e. Thus the last integral can be written as:
Il
kxs+δ - xsk
------,	二exp
(2∏)n √∣Σs+δ ∣∣∑s∣
-2[(Xs+δ - μ(zs + δ 力,4+台(Xs+ δ - μ(zs+δ ))
(40)
+(xs - μ(zs))T∑-1(xs - μ(zs))]) dxsdxt,	(41)
where we denote the covariance matrix at time S by Σs. Now, if Σs+δ becomes sufficiently small as δ
converges to 0, then the exponential factor will dominate and thus (38) holds. In other words, Holder
regularity of the process is verified provided that pθ (χ∣z) becomes localized in X and converges to a
Dirac measure (similarly to the case of the heat kernel propagator and Brownian motion). From this
point of view, the variance of the decoder can be considered as an indicator of how far the stochastic
process is from being Holder continuous.
Below We discuss two other stochastic process constructions, one of which is built upon Itδ diffusion
processes and enjoys further path-regularity properties.
D.3 Further Semantic Lagrangians and Associated Euler-Lagrange Equations
We briefly recall that, among other aspects, Lagrangian theory suggests a framework for optimization
of functionals (Lagrangians) defined over appropriate function spaces. Critical points of Lagrangians
are identified by means of the corresponding Euler-Lagrange equations (Landau & Lifshitz (2013)).
To obtain the Euler-Lagrange equations for the Lagrangians in (9, 45, 50) we compute in a straight-
forward manner the first variation
δS	d
-7—[φ] := -z-|e=0Si(x(t) + eφ(t)), i = 1, 2, 3,	(42)
δX	dE
where φ:	[0, T] → TX is a compactly supported deformation 9. This produces the following
conditions:
9By TX we mean the tangent bundle of X.
16
Under review as a conference paper at ICLR 2020
TIe=OSI(X⑴ + eφ(t)) = -ʃ |e=0
d	d
Z	(B(lT Ix(t) + φ(t)) - B(l0 Ix(t) + φ(t)))2 dt
0
T	T dim X
-2 / Nf, φidt = -2 / X fφi ∂ifdt,
(43)
(44)
where we have denoted f := (B(lT Ix(t)) - B(l0Ix(t))) and differentiated under the integral sign.
The notation〈•, •)denotes the standard Euclidean scalar product. Requiring that the first variation
vanishes for every choice of deformation φ implies that either f or Vf vanishes.
Minimum Decision Path
Contrary to the previous Lagrangian L1 where the minimizers force almost instantaneous jumps of
Bθ, one might prefer a path that illustrates a uniform change of Bθ - such a gradual change might be
implemented by requesting that Bθ changes linearly along x(t). To this end, we introduce
S2(x(t),x0,xt):= Z (NB(IT∣x(t)),X(t)i- T) dt := Z L2(x(t),x0,xt)dt.	(45)
Computing the first variation as above, one gets:
2
TIe=OS2 (x(t) + eφ(t)) = 7|e=0
d	d
B(lτ∣x(t) + eφ(t)), x(t) + e<φ(t)i
dt
(46)
2
hVB(lτ∣χ(t)),X(t)i- T)《V2B(IT∣χ(t)(φ(t)),χ(t))
+hVB(lT Ix(t)), φ(t)i) dt,
(47)
(48)
—
1
T
where We have denoted the Hessian by V2 and, as above, the notationh•,)denotes the standard
Euclidean scalar product.
Now, assuming that B is non-degenerate in the sense that the second factor in the integrand is not
identically vanishing for all choices of a deformation φ (i.e. the Hessian and gradient of B satisfy the
above relations for all t), one sees that the critical points satisfy
dB(lτ∣x(t)) = T,	∀t ∈ [0,T].	(49)
Such a condition resembles the conservation of angular momentum given, for instance, by the
classical formula of Clairaut in the case of surfaces of revolution.
Minimum Transformation Path
Another meaningful Lagrangian construction is given by following the geometry of B itself and
attempting to find paths that are close to being gradient-descent lines. This can be embodied by
defining
S3(x(t), xO, xt)
ττ
/ ∣∣VB(lτ ∣x(t)) - αX(t)∣∣2dt := / L3(x(t),x0,xj
OO
(50)
where α is a suitably chosen positive constant describing the extent to which the stochastic path
should follow the geometry of B. Computing the variation w.r.t. x(t) one easily sees that
~e~ |e=0S3(x(t) + eφ(t)) = ~e~ |e=0
d	d
τ
J	∣∣VB(lτ|x(t) + eφ(t)) 一 α (X(t) + cφ(t)) ∣∣2dt	(51)
2
VB(lτ∣x(t)) — αX(t), V2B(lτ∣x(t))(φ(t)) — αφ(t)) dt,
(52)
17
Under review as a conference paper at ICLR 2020
Assuming the Hessian is not identically vanishing along the curve, the critical points of the variational
problem are given by the condition
(VB) (lτ∣x(t)) = αX(t).	(53)
In addition to following the geometry of the black box B, one could also impose a natural condition
that the stochastic paths minimize distances on the manifold in feature space that the auto-encoder
pair induces. We recall from basic differential geometry that the image of the decoder as a subset of
the feature space is a submanifold with a Riemannian metric g induced by the ambient Euclidean
metric in the standard way (for background we refer to do Carmo (1976)). In the simple case of a
deterministic auto-encoder, one can think of g as the matrix JTJ where J denotes the Jacobian of
the decoder - thus g gives rise to scalar product g(X, Y ) := XJTJY . In the stochastic case, one
can use suitable approximations to obtain g in a similar manner - e.g. in Arvanitidis et al. (2018) the
authors decompose the decoder into a deterministic and a stochastic part, whose Jacobians J1, J2 are
summed as J1T J1 + J2T J2 to obtain the matrix g .
Now, having Riemannian structure (i.e. the notion of a distance) on the data submanifold, geodesic
curves naturally arise as minimizers of a suitable distance functional, namely:
S4 (x(t), x0, xt)
/Tkx(t)kg dt,
0
(54)
where the norm ∣∣ ∙ kg is computed with respect to the Riemannian metric g, that is ,g(∙, ∙). We note
that the utilization of geodesics for suitable latent space interpolations was thoroughly discussed in
Arvanitidis et al. (2018).
D.4 Other Regularizers
As mentioned already, we would like that classifier’s probabilities change in a monotonous fashion
along the paths - these paths are preferable in the sense that they provide examples following a
particular trend along the disputed labels. We enforce such monotonic behaviour along the paths with
the term
1 K n-1
Tm ：= v77----TTXX min(0, b(lτ,xk) - b(lτ,xk+ι)),	(55)
K(n - 1)
with n the number of points along the path and K the number of paths.
Further, and in accordance with Proposition 1, one can also require that the auto-encoder reconstructs
the endpoints with sufficiently large accuracy. We enforce this requirement with the edge term
Te := Pi (∣b(li, Xi)) - b(li, Xi)∣ + c(xi, Xi)), i = 0,T, -T,where C measures the reconstruction
error10 and Xi 〜Pθ(X|Z = zi), with Zi 〜Qφ(Z|X = Xi) and Xi the data points at i = 0, T, -T.
D.5 Wasserstein
In contrast to VAE, within the WAE framework Tolstikhin et al. (2018) one only needs to be able
to sample from Qφ(Z|X) and Pθ(X|Z) — i.e. their density is not needed. WAE is trained by
minimizing a (penalized) optimal transport divergence Bousquet et al. (2017) — the Wasserstein
distance, between the input data distribution PD(X) and the implicit latent variable model Pθ(X).
As in VAE, the latter is defined by first sampling Z from P(Z) and then mapping Z to X through
the decoder Pθ(X|Z). The loss function of WAE is given by
Lwae = EPD(x)Eqφ(z∣x) [c (X, Pθ(X|Z))]+ λDz (Qφ(Z), P(Z)),	(56)
where c is a distance function and DZ is an arbitrary divergence between the prior P(Z) and
the agregate posterior Qφ(Z) = EPD(X)[Qφ(Z|X)], weighted by a positive hyperparameter λ.
Minimizing Equation 56 corresponds to minimizing the Wasserstein distance if the decoder is
deterministic (i.e. Pθ(X|Z = z) = 6g6(Z)Vz ∈ Z, with the map gθ : Z → X) and the distance
term is optimized. If the decoder is stochastic Equation 56 yields an upper bound on the Wasserstein
10In our experiments we computed c either by means of the cross-entropy or the Euclidean norm.
18
Under review as a conference paper at ICLR 2020
distance Bousquet et al. (2017). In the present work, we use two different divergences DZ .
GAN Based DZ Here we calculate the Jensen Shannon divergence through an adversarial game,
where a discriminator separates true points sampled from the prior PZ from fake ones sampled from
the aggregated posterior. Notice that this corresponds to the classical adversarial cost Goodfellow
et al. (2015) performed in the latent space.
MMD-based DZ For a postive-definite reproducing kernel k : Z × Z → R we use the maximum
mean discrepancy MMD:
MMDk(PZ, Qz) = || 4 k(zQdPz(Z)- L k(z,∙)dQz(z)∣∣Hz
(57)
In our experiments we choose a squared cost function c(x, y) = ||x -y||22 and refer to the Wasserstein
with JS divergence for DZ as Adversarial Auto Encoders (AAEs) 11 whereas the model trained with
the mean discrepancy divergence is denoted in the main text as WAE. We use the inverse multi
quadratic kernel Kkx, y) = c+∣∣C-y∣∣2
11the equivalence is stablished in Tolstikhin et al. (2018)
19