Under review as a conference paper at ICLR 2020
Deep Black-Box Optimization with Influence
Functions
Anonymous authors
Paper under double-blind review
Ab stract
Deep neural networks are increasingly being used to model black-box functions.
Examples include modeling brain response to stimuli, material properties under
given synthesis conditions, and digital art. In these applications, often the model is
a surrogate and the goal is rather to optimize the black-box function to achieve the
desired brain response, material property, or digital art characteristics. Moreover,
resource constraints imply that, rather than training on a passive dataset, one
should focus subsequent sampling on the most informative data points. In the
Bayesian setting, this can be achieved by utilizing the ability of Bayesian models
such as Gaussian processes to model uncertainty in observed data via posterior
variance, which can guide subsequent sampling. However, uncertainty estimates
for deep neural networks are largely lacking or are very expensive to compute. For
example, bootstrap or cross-validation estimates require re-training the network
several times which is often computationally prohibitive. In this work, we use
influence functions to estimate the variance of neural network outputs, and design
a black-box optimization algorithm similar to confidence bound-based Bayesian
algorithms. We demonstrate the effectiveness of our method through experiments
on synthetic and real-world optimization problems.
1	Introduction
Black-box optimization, also known as zeroth order optimization, is the problem of finding the
global minima or maxima of a function given access to only (possibly noisy) evaluations of the
function. Perhaps the most popular black-box optimization approach is in the Bayesian setting, such
as Gaussian process (gp) optimization, which assumes that the black-box function is sampled from a
gp, and uses an acquisition function such as lower confidence bound (lcb) to guide sampling and
subsequently update the posterior mean and variance of the gp model in an iterative manner. However,
recently, deep neural networks are increasingly being used to model black-box functions. Examples
include modeling brain response to stimuli(Yamins et al., 2014; Agrawal et al., 2014; Kell et al., 2018),
material properties under given synthesis conditions(Xue et al., 2016), and digital art(Manovich,
2015). Often the goal in these problems is optimization of the black-box model, rather than learning
the entire model. For example, a human vision researcher might be interested in understanding which
images cause maximum activation in a specific brain region(Ponce et al., 2019; Bashivan et al., 2019);
a material scientist is interested in finding optimal experimental conditions that yield a material with
desired properties(Xue et al., 2016) or generate digital art with desired characteristics(Manovich,
2015). While a simple approach is to learn a deep model on passively acquired evaluations of the
function, and then report its optima, this is wasteful as often the black-box evaluations are expensive
(c.f., subject time in a brain scanner is limited, material synthesis experiments are expensive, etc.).
Also, often pre-trained models of black-boxes need to be updated subsequently to identify inputs
that may lead to novel outputs not explored in training set. For example, in material science a
model trained to predict the energy of a pure lattice may need to be updated to understand new
low-energy configurations achievable under defects, or deep neural net models of images may need
to be updated to achieve desired characteristics of synthetic digital images. Thus, it is of interest to
develop sequential optimization methods akin to Bayesian optimization for deep neural networks.
Sequential optimization of neural network models requires an acquisition function, similar to Bayesian
optimization. However, popular acquisition functions (lcb, expectation maximization, Thompson
1
Under review as a conference paper at ICLR 2020
sampling, etc.) are mostly based on an uncertainty measure or confidence bound which characterizes
the variability of the predictions. Unfortunately, formal methods for uncertainty quantification that are
also computationally feasible for deep neural network models are largely non-existent. For example,
bootstrap or cross-validation based estimates of uncertainty require re-training the network several
times, which is typically computationally prohibitive. In this paper, we seek to investigate principled
and computationally efficient estimators of uncertainty measures (like variance of prediction), that
can then be used to guide subsequent sampling for optimization of black-box functions.
Specifically, we use the concept of influence functions(Cook & Weisberg, 1980; 1982) from classical
statistics to approximate the leave-one-out cross-validation estimate of the prediction variance,
without having to re-train the model. It is known(Cook & Weisberg, 1982) that if the loss function is
twice-differentiable and strictly convex, then the influence function has a closed-form approximation,
and the influence-function based estimate provides an asymptotic approximation of the variance of
prediction. Even though the loss function of neural networks is non-differentiable and non-convex, it
was recently shown(Koh & Liang, 2017) that in practice, the approximation continues to hold for this
case. However, Koh & Liang (2017)(Koh & Liang, 2017) used influence functions to understand
the importance of each input on the prediction of a passively trained deep neural network, Influence
functions were not investigated for uncertainty quantification and estimation of prediction variance
for use in subsequent sampling.
A related line of work is activation maximization in neural networks (nns) where the goal is to find
input that maximizes the output of a particular unit in the network. However, since the corresponding
target functions are known and differentiable, gradient based optimization methods can be used. Still,
obtaining results suitable for visualization requires careful tuning and optimization hacks(Nguyen
et al., 2016). In this paper, we will consider the activation maximization problem in a black-box
setting, to mimic neuroscience and material science experiments, where the brain is the black-box
function. Furthermore, prior work is passive requiring learning a good model for all inputs, while we
focus on collecting new data to sequentially guide the model towards identifying the input which
leads to maximum output without necessarily learning a good model for all inputs.
There have also been attempts to directly extend Bayesian optimization to neural networks. Snoek
et al. (2015)(Snoek et al., 2015) add a Bayesian linear layer to neural networks, treating the network
outputs as basis function. Springenberg et al. (2016)(Springenberg et al., 2016) focus on scalability,
and use a Monte Carlo approach, combined with scale adaptation. However, our focus is to enable
sequential optimization of existing NN models being used in scientific domains. Our contributions
can be summarized as follows:
•	We use influence functions to obtain a computationally efficient approximation of prediction
variance in neural networks.
•	We propose a computationally efficient method to compute influence functions for neural
network predictions. Our approach uses a low-rank approximation of the Hessian, which is
represented using an auxillary network, and trained along with the main network.
•	We develop a deep black-box optimization method using these influence function based
uncertainty estimates that is valid in the non-Bayesian setting.
•	We demonstrate the efficacy of our method on synthetic and real datasets. Our method
can be comparable, and also outperform Bayesian optimization in settings where neural
networks may be able to model the underlying function better than GPs.
The rest of the paper is organized as follows. In sec. 2, we formally define the problem, and the
Bayesian setting we build upon in this work. Our proposed method is described in sec. 3, followed
by results on synthetic and real datasets in sec. 4. We conclude with discussion of open problems in
sec. 5.
2	Preliminaries
2.1	Problem Setting
We consider the problem of sequential optimization of a black-box function. Specifically, let
f : X → R be a cost function to be minimized. At each step t, we select a point xt ∈ X, and observe
a noisy evaluation yt = f (xt) + t, where t is independent 0-mean noise. This noisy evaluation is
2
Under review as a conference paper at ICLR 2020
the only way to interact with the function, and we don’t assume any prior knowledge of it. We will
use z to denote an input-output pair; z ≡ (x, y) ∈ X × R.
Practical functions in this category (like hyper-parameter optimization for instance) generally tend to
be “expensive”, either in terms of time, or resources, or both. This makes it impractical to do a dense
“grid search” to identify the minimum; algorithms must use as few evaluations as possible. With a
given time budget T, the objective is to minimize the simple regret, mint=ι…T f (Xt) - f (x*) where
x* ∈ arg minχ∈χ f (x) is a global minimum (not necessarily unique). This measures how close to
the optimum an algorithm gets in T steps, and is equivalent to minimizing mint=1...T f(xt).
2.2	Bayesian Optimization
Bayesian optimization is a popular method for solving black-box optimization problems, which uses
GP models to estimate the unknown cost function. At each step T, newly obtained data (xT, yT)
is used to update a gp prior, and the posterior distribution is used to define an acquisition function
αT : X → R. The next point to query, xT+1 is selected by minimizing the acquisition function;
xT+1 = arg minx∈X αT (x). Popular acquisition functions are expected improvement (EI), maximum
probability of improvement (mpi), and lcb. Here, we will particularly focus on lcb, which provides
the motivation for our method.
2.3	GP-LCB
Consider a GP model with mean function 0, and covariance function k(∙, ∙). After observing T
points, the model is updated to obtain a posterior mean function μτ, and a posterior covariance
function kτ(∙, ∙). The lcb acquisition function is aTCB(x) = μτ(x) - βj2στ(x), where στ(x) ≡
kT (x, x), and βT is a parameter for balancing exploration and exploitation. This expression is easily
interpretable; μτ is an estimate of expected cost, and στ is an estimate of uncertainty. The next point
is chosen taking both into consideration. Points with low expected cost are exploited, and points with
high uncertainty are explored. αT defines a “pessimistic” estimate (lower confidence bound) for the
cost, hence the name of the algorithm.
3	Method
Suppose we have a neural network g : X → R with parameters θ ∈ Θ, trained using a convex loss
function L : (XX R) X Θ → R+. At a particular step T, We get an estimate of the parameters θτ,
by minimizing (1/T) PtT=1 L(zt, θ). As noted earlier, zt ≡ (xt, yt), and we will use gθ to denote
the network with a particular set of parameters. Now, for any point x ∈ X, we have a prediction
for the cost function, i.e., gθτ (x). So, if We get an estimate στ(x), for the variance of gθτ (x), We
can define an acquisition function ατ(x) = gθτ (x) - σ^τ (x). Then optimization proceeds similar
to Bayesian optimization; where we select the next point for querying xT+1 by minimizing αT(X).
This auxiliary optimization problem is non-convex, but local minima can be obtained through gradient
based methods. In practice, it is also common to use multiple restarts when solving this problem,
and select the best solution. We now describe our method for estimating the variance using influence
functions.
3.1	Influence Functions
Intuitively, influence functions measure the effect of a small perturbation at a data point on the
parameters ofa model. We up-weight a particular point z+ from the training set {zt}tT=1, and obtain a
new set of parameters θT(z+,ν) by minimizing the reweighted loss function, (1/T) PT=1 L(zt, θ) +
νL(z+, θ). We define the influence of z+ on θτ as the change θT(z+, V) - θτ caused by an adding
an infinitesimal weight to z+ . Formally,
I^τ (z+) = lim
ν
Importantly, the influence function can be approximated using the following result.
Iθτ(z+) ≈-H-1VθL(z+,θτ),
3
Under review as a conference paper at ICLR 2020
where VθL(z+, θτ) is the gradient of the loss With respected to the parameters evaluated at (z+,θτ),
and H^τ ≡ (1/T) PT=I V2L(zt, θτ) is the Hessian. Now, we can use the chain rule to extend this
approximation to the influence on the prediction of gθτ. For a test point χ*, let Ig^τ (χ*, z+) be the
influence of z+ on gθτ (Xt). So,
τ ( t +ʌ	dgθ+(z+,ν)(xt)
Ig^τ (x',z+) = -----∂V------
=∂g^τ(xt) ∂θT(z+,V)
∂θ ∂ν
=dg⅛P Iθτ(z+)
≈- dg⅛^ H-1Vθ L(z+,θτ).
∂θ τ
3.2	Variance Estimation
Finally, we estimate the variance by computing the average squared influence over the training points.
1T
στ(X) = T EIgθT(X,zt)2.
t=1
In semi-parametric theory, influence is formalized through the behavior of asymptotically linear
estimators, and under regularity conditions, it can be shown that the average squared influence
converges to the asymptotic variance(Tsiatis, 2007).
3.3	Implementation
The procedure described above cannot be directly applied to neural networks since the Hessian is
not positive-definite in the general case. We address this issue by making a low-rank approximation,
H^ ≈ Q ≡ PPT. Let P = UΣVT be a singular value decomposition (SVD) of P. Then, Qt ≡
UΣt2UT is the Moore-Penrose pseudoinverse of Q, where Σt2 is a diagonal matrix with reciprocals
of the squared non-zero singular values. With this, for any vector v we can approximate the product
with the inverse Hessian.
H^v ≈ Qtv = UΣt2 UTv.
We represent the low-rank approximation using a second neural network with a single hidden layer.
The network uses shared weights similar to an autoencoder, and given input v , computes PPTv. We
train this network to approximate HθVθL(z, θ), using samples from the training data. The Hessian
vector product can be computed efficiently by performing two backward passes through the network
(Perlmutter’s method). After updating the network at each step T, the SVD of P is computed, which
allows efficient computation of H-1VθL(z+,θτ). The full algorithm (NN-INF) is shown in fig. 1.
3.4	GP-INF
The influence approximation for variance can also be applied to gp models, by viewing them
as performing kernel ridge regression. In this case, there is a closed form expression for the
influence(Ollerer et al., 2015), so we can directly compute variance approximation. This gives
a method similar to gp-lcb, where we use the influence approximation of variance instead of the
posterior variance. We term this method gp-inf, and use it as an additional baseline in our experiments.
4 Experiments
4.1	Synthetic function maximization
First, we compare our method with gp based algorithms using common test functions used in
optimization: five dimensional Ackley function(Ackley, 2012), and ten dimensional Rastrigin func-
tion(Rastrigin, 1974). For the Ackley function, we use a network with 3 hidden layers, with 8, 8, and
4
Under review as a conference paper at ICLR 2020
ι: hyper-parameter {βt}t=1…∞ > exploration-exploitation trade-off values
2： hyper-parameter np	> random samples used to pre-train the model
3： hyper-parameter r	> Hessian approximation rank
4： hyper-parameter nH > samples used for training Hessian approximation
5： hyper-parameter nɪ	> samples used for computing influence
6： procedure NNINF(f, X , T, gθ)
I Minimize f over X for T steps using the network gθ .
7：	D — {(x, f (x)) : x ∈ Sample(X, np)}	> samples for pre-training
8：	|θ| — number of parameters in θ
9：	P J Matrix(|θ|,r)	> for low rank Hessian approximation
10： for t — 1 . . . T do
11：	TrainNetwork(gθ, D)
12：	P,I— IHVP(gθ,D,P)
13：	xt — arg minx∈X Acquisition(x, gθ, I, βt)
14：	D — D ∪ {(xt, f(xt))}
15：	end for
16：	return arg min(x,y)∈D y
17： end procedure
18： procedure IHVP(gθ , D, P)
A Compute H-I▽ θL(z,θ) for Z ∈ D.
19：	∏p J FullyConnectedNetwork(P, PT)
20： LH J {▽ θL(z,θ) : Z ∈ Sample(D,nH)}
21：	Jθ J (1 /nH E V
v∈LH
22：	VJ J Vθ Jθ
23：	DH J {(V, VθVTVJ) : V ∈ LH }
24：	TrainNetwork(πP, DH)
25：	U, Σ, V J SVD(P)
26：	W J U∑t2
27： I J {WUTV : V ∈ Sample(LH, nI)}
28：	return P, I
29： end procedure
30： procedure Acquisition(x, gθ , I, β)
A compute the acquisition function at x
31： μ J gθ (χ)
32：	Vμ J Vθμ____________
33： σ J Jn⅛ E(IT")2
34： return μ 一 β1 /2σ
35： end procedure
Figure 1:	Algorithm
5
Under review as a conference paper at ICLR 2020
NN-INF GP-INF GP-LCB GP-MPI GP-EI
0 ---------------------------------------
t=0	100	200	300	400	500	0	100	200	300	400	500
(a) Ackley 5D	(b) Rastrigin 10D
Figure 2:	Optimization of synthetic functions
4	hidden units respectively. And for the Rastrigin function, we again use a network with 3 hidden
layers, but with 16, 16, and 8 hidden units. In both cases, we approximate the Hessian with a rank 5
matrix. We report two sets of results, using different schemes for setting the βt parameter (used in
inf and lcb methods).
fig. 2 (1) shows the instantaneous regret over 500 iterations with βt = c√7log2(10t) (based on the
theoretical results presented by Srinivas et al. (2009)(Srinivas et al., 2009)). We set c = 0.1 for GP
methods, and c = 0.01 for NN-INF. We did not find c to have a significant effect on performance, but
for consistency, we used scaled βt for NN-INF by 10 in all cases. fig. 2 (2) shows the same results, but
with βt held constant throughout the experiment. We have βt = 2 for GP methods, and βt = 0.2 for
nn-inf.
4	.2 Neural network output maximization
We now demonstrate results on a task inspired from neuroscience. To understand the properties of
a neuron, brain region etc., experimenters collect response signals (neuron firing rate, increase in
blood oxygenation etc.) to different stimuli in order to identify maximally activating inputs. This is
generally done in an ad-hoc manner, whereby experimenters hand pick, or manually create a restricted
set of images designed to address a given theoretical question. This can lead to biased results caused
by insufficient exploration of the input space. One way to address this issue is to perform adaptive
stimulus selection over the full input space.
To simulate the setting of stimulus selection in neuroscience, we first trained a convolutional neural
network (cnn) to classify images from the mnist dataset. The output layer of this cnn has 10 units,
each corresponding to one of the mnist digits (0 to 9). Given an input image, the output of each unit
6
Under review as a conference paper at ICLR 2020
12-----------------------------------------
0-------------------------- -----------------------------
t=0 100	200	300	400	500 0	100	200	300	400	500
(a) '3' neuron	(b) '2' neuron
Figure 3:	MNIST
is proportional to the probability (as predicted by the model), that the image belongs to the particular
class. With this, we can define an optimization task: find the image that maximizes the output of
a particular unit. This is similar to a neuroscience visual stimulus selection experiment, where the
output unit could be a single neuron in the visual cortex.
Given the difficultly of this optimization problem, it is important to exploit available prior knowledge.
For a visual experiment, this could be in the form of a pre-trained network. Here, we pre-train our cnn
model for binary classification of two digits different from the target digit; for example (classifying
‘5’ vs. ‘6’ when the target digit is ‘2’. For the model, we use a smaller cnn than the target; with two
convolution layers, each with a single filter. fig. 3 shows the target neuron output for two different
settings. In fig. 3 (a), the target digit is ‘2’, and the cnn is pre-trained for classifying ‘5’ vs. ‘6’. In
fig. 3 (b), the target digit is ‘3’, and the cnn is pre-trained for classifying ‘1’ vs. ‘8’. In both cases,
we see that the cnn model is able to exploit the prior information, and achieve better performance
compared to the gp-lcb baseline. This is a promising result showing the feasibility of large scale
adaptive sitmulus selection.
5	Discussion
In this paper, we use the notion of influence functions from classical statistics to estimate the
variance of prediction made using a neural network model, without having to retrain the model on
multiple subsets of data as in bootstrap or cross-validation based estimates. We additionally use these
uncertainty estimates to design a deep black-box optimization algorithm, that can be used to optimize
a black-box function such as brain response or desired material property with sequentially collected
data. We show the efficacy of our algorithm on synthetic and real datasets.
There are several directions for future work. First, the uncertainty estimates we propose are backed
by theoretical underpinning under convexity assumptions when the samples are assumed to be
independent and it is of interest to develop theoretical guarantees for the non-convex and sequentially
dependent samples setting which arises in optimization. The latter should be possible given parallel
analysis in the Bayesian setting. Such non-Bayesian confidence bounds that are valid for sequential
data can then also be used for active learning of black-box functions or deep models. Second, while
the method does not require retraining the NN model at each iteration for variance estimation, the
model does require retraining as new data is collected. While this is inevitable in optimization and
active learning settings,the computational complexity can be improved by not training the model
to convergence at each iteration. For example, in (Awasthi et al., 2017), and references therein,
computational efficiency is achieved for active learning of linear separators by training the model
to lower accuracy initially (e.g. it should be matched to the lower statistical accuracy due to limited
samples initially) and then increasing the computational accuracy at subsequent iterations. Finally,
we have only explored the notion of uncertainty (coupled with prediction maximization) to guide
7
Under review as a conference paper at ICLR 2020
subsequent sampling. However, since neural networks learn a feature representation, another way
to guide sampling is via the notion of expressiveness (c.f. (Sener & Savarese, 2017)) that selects
data points which help improve the learnt feature representation. It is interesting to compare and
potentially combine the notions of uncertainty and expressiveness to guide sampling for optimization
as well as active learning of black-box functions modeled via deep neural networks.
References
David Ackley. A connectionist machine for genetic hillclimbing, volume 28. Springer Science &
Business Media, 2012.
Pulkit Agrawal, Dustin Stansbury, Jitendra Malik, and Jack L Gallant. Pixels to voxels: Modeling
visual representation in the human brain. arXiv preprint arXiv:1407.5104, 2014.
Pranjal Awasthi, Maria Florina Balcan, and Philip M. Long. The power of localization for efficiently
learning linear separators with noise. J. ACM, 63(6):50:1-50:27, January 2017. ISSN 0004-54l1.
doi: 10.1145/3006384. URL http://doi.acm.org/10.1145/3006384.
Pouya Bashivan, Kohitij Kar, and James J DiCarlo. Neural population control via deep image
synthesis. Science, 364(6439):eaav9436, 2019.
R Dennis Cook and Sanford Weisberg. Characterizations of an empirical influence function for
detecting influential cases in regression. Technometrics, 22(4):495-508, 1980.
R Dennis Cook and Sanford Weisberg. Residuals and influence in regression. New York: Chapman
and Hall, 1982.
Alexander JE Kell, Daniel LK Yamins, Erica N Shook, Sam V Norman-Haignere, and Josh H
McDermott. A task-optimized neural network replicates human auditory behavior, predicts brain
responses, and reveals a cortical processing hierarchy. Neuron, 98(3):630-644, 2018.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1885-1894.
JMLR. org, 2017.
Lev Manovich. Data science and digital art history. International Journal for Digital Art History, (1),
2015.
Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and Jeff Clune. Synthesizing the
preferred inputs for neurons in neural networks via deep generator networks. In Advances in
Neural Information Processing Systems, pp. 3387-3395, 2016.
Viktoria Ollerer, Christophe Croux, and Andreas Alfons. The influence function of penalized
regression estimators. Statistics, 49(4):741-765, 2015.
Carlos R Ponce, Will Xiao, Peter Schade, Till S Hartmann, Gabriel Kreiman, and Margaret S
Livingstone. Evolving super stimuli for real neurons using deep generative networks. bioRxiv, pp.
516484, 2019.
LA Rastrigin. Systems of extremal control. Nauka, 1974.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. arXiv preprint arXiv:1708.00489, 2017.
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,
Mostofa Patwary, Mr Prabhat, and Ryan Adams. Scalable bayesian optimization using deep neural
networks. In International conference on machine learning, pp. 2171-2180, 2015.
Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimization
with robust bayesian neural networks. In Advances in Neural Information Processing Systems, pp.
4134-4142, 2016.
8
Under review as a conference paper at ICLR 2020
Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process opti-
mization in the bandit setting: No regret and experimental design. arXiv preprint arXiv:0912.3995,
2009.
Anastasios Tsiatis. Semiparametric theory and missing data. Springer Science & Business Media,
2007.
D. Xue, P. V. Balachandran, J. Hogden, J. Theiler, and T. Lookman. Accelerated search for materials
with targeted properties by adaptive design. Nature communications, 7:11241, 2016.
Daniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J
DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual
cortex. Proceedings of the NationalAcademy of Sciences ,111(23):8619-8624, 2014.
9