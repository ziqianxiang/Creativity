Under review as a conference paper at ICLR 2020

IMPROVING  GRADIENT  ESTIMATION  IN  EVOLUTION-
ARY  STRATEGIES  WITH  PAST  DESCENT  DIRECTIONS

Anonymous authors

Paper under double-blind review

ABSTRACT

Evolutionary Strategies (ES) are known to be an effective black-box optimization
technique for deep neural networks when the true gradients cannot be computed,
such   as in Reinforcement Learning. We continue a recent line of research that uses
surrogate gradients to improve the gradient estimation of ES. We propose a novel
method to optimally incorporate surrogate gradient information.  Our approach,
unlike previous work, needs no information about the quality of the surrogate
gradients and is always guaranteed to find a descent direction that is better than
the surrogate gradient. This allows to iteratively use the previous gradient estimate
as surrogate gradient for the current search point.  We theoretically prove that
this yields fast convergence to the true gradient for linear functions and show
under simplifying assumptions that it significantly improves gradient estimates for
general functions. Finally, we evaluate our approach empirically on MNIST and
reinforcement learning tasks and show that it considerably improves the gradient
estimation of ES at no extra computational cost.

1    INTRODUCTION

Evolutionary Strategies (ES) (1; 2; 3) are a black-box optimization technique, that estimate the 
gradi-
ent of some objective function with respect to the parameters by evaluating parameter perturbations
in random directions. The benefits of using ES in Reinforcement Learning (RL) were exhibited in
(4). ES approaches are highly parallelizable and account for robust learning, while having decent
data-efficiency. Moreover, black-box optimization techniques like ES do not require propagation of
gradients,  are tolerant to long time horizons, and do not suffer from sparse reward distributions 
(4).
This lead to a successful application of ES in variety of different RL settings (5; 6; 7; 8). 
Applications
of ES outside RL include for example meta learning (9).

In many scenarios, the true gradient is impossible to compute, however surrogate gradients are
available.  Here, we use the term surrogate gradients for directions that are correlated but usually
not equal to the true gradient, e.g. they might be biased or unbiased approximations of the 
gradient.
Such scenarios include models with discrete stochastic variables (10), learned models in RL like
Q-learning (11), truncated backpropagation through time (12) and feedback alignment (13), see (14)
for a detailed exhibition. If surrogate gradients are available, it is beneficial to preferentially 
sample
parameter perturbations from the subspace defined by these directions (14).  The proposed algo-
rithm (14) requires knowing in advance the quality of the surrogate gradient, does not always 
provide
a descent direction that is better than the surrogate gradient, and it remains open how to obtain 
such
surrogate gradients in general settings.

In deep learning in general, experimental evidence has established that higher order derivatives are
usually "well behaved", in which case gradients of consecutive parameter updates correlate and
applying momentum speeds up convergence (15; 16; 17). These observations suggest that past update
directions are promising candidates for surrogate gradients.

In this work, we extend the line of research of (14). Our contribution is threefold:

First, we show theoretically how to optimally combine the surrogate gradient directions
with random search directions.  More precisely, our approach computes the direction of
the subspace spanned by the evaluated search directions that is most aligned with the true
gradient. Our gradient estimator does not need to know the quality of the surrogate gradients

1


Under review as a conference paper at ICLR 2020

and always provides a descent direction that is more aligned with the true gradient than the
surrogate gradient.

Second, above properties of our gradient estimator allow us to iteratively use the last update
direction as a surrogate gradient for our gradient estimator. Repeatedly using the last update
direction as a surrogate gradient will aggregate information about the gradient over time and
results in improved gradient estimates. In order to demonstrate how the gradient estimate
improves over time, we prove fast convergence to the true gradient for linear functions and
show, that under simplifying assumptions, it offers an improvement over ES that depends on
the Hessian for general functions.

Third, we validate experimentally that these results transfer to practice, that is, the proposed
approach computes more accurate gradients than standard ES. We observe that our algorithm
considerably improves gradient estimation on the MNIST task compared to standard ES and
that           it improves convergence speed and performance on the tested Roboschool reinforcement
learning environments.

2    RELATED  WORK

Evolutionary strategies (1; 2; 3) are black box optimization techniques that approximate the 
gradient
by sampling finite differences in random directions in parameter space. Promising potential of ES 
for
the optimization of neural networks used for RL was demonstrated in (4). They showed that ES gives
rise to efficient training despite the noisy gradient estimates that are generated from a much 
smaller
number of samples than the dimensionality of parameter space. This placed ES on a prominent spot
in  the RL tool kit (5; 6; 7; 8).

The history of descent directions was previously used to adapt the search distribution in covariance
matrix adaptation ES (CMA-ES) (18). CMA-ES constructs a second-order model of the underlying
objective function and samples search directions and adapts step size according to it.  However,
maintaining the full covariance matrix makes the algorithm quadratic in the number of parameters,
and    thus impractical for high-dimensional spaces.  Linear time approximations of CMA-ES like
diagonal approximations of the covariance matrix (19) often do not work well, in the sense that 
their
gradient estimates do not converge to the true gradient even if the true gradient does not change 
over
time. Our approach differs as we simply improve the gradient estimation and then feed the gradient
estimate to a first-order optimization algorithm.

Our work is inspired by the line of research of (14), where surrogate gradient directions are used 
to
improve gradient estimations by ’elongating’ the search space along these directions. That approach
has two shortcomings.  First, the bias of the surrogate gradients needs to be known to adapt the
covariance matrix. Second, once the bias of the surrogate gradient is too small, the algorithm will 
not
find a better descent direction than the surrogate gradient.

Another related area of research investigates how to use momentum for the optimization of deep
neural networks. Applying different kinds of momentum has become one of the standard tools in
current deep learning and it has been shown to speed-up learning in a very wide range of tasks
(20; 16; 17).  This hints, that for many problems the higher-order terms in deep learning models
are "well-behaved" and thus, the gradients do not change too drastically after parameter updates.
While these approaches use momentum for parameter updates, our approach can be seen as a form of
momentum when sampling directions from the search space of ES.

3    GRADIENT  ESTIMATION

We aim at minimizing a function f  : Rn        R by steepest descent. In scenarios where the 
gradient

f does not exist or is inefficient to compute, we are interested in obtaining some estimate of the
(smoothed) gradient of f that provides a good parameter update direction.

3.1    THE ES GRADIENT ESTIMATOR

ES considers the function fσ that is obtained by Gaussian smoothing

fσ(θ) = Es∼N₍₀,I₎[f (θ + σϵ)] ,

2


Under review as a conference paper at ICLR 2020

where σ is a parameter modulating the size of the smoothing area and     (0, I) is the n-dimensional
Gaussian distribution with 0 being the all 0 vector and I being the n-dimensional identity matrix. 
The
gradient of fσ with respect to parameters θ is given by

1

Qfσ =  σ Es∼N₍₀,I₎[f (θ + σϵ)ϵ],

which can be sampled by a Monte Carlo estimator, see (5). Often antithetic sampling is used, as it
reduces variance (5). The antithetic ES gradient estimator using P  samples is given by

P


gES

=     1    Σ .f (θ + σϵⁱ) − f (θ − σϵⁱ)Σ ϵⁱ ,                                    (1)

i=1

where ϵⁱ are independently sampled from     (0, I) for i       1, . . . , P   . This gradient 
estimator has
been shown to be effective in RL settings (4).

3.2    OUR ONE STEP GRADIENT ESTIMATOR

We first give some intuition before presenting our gradient estimator formally. Given one surrogate
gradient direction ζ, our one step gradient estimator applies the following sampling strategy. 
First,
it estimates how much the gradient points into the direction of ζ by antithetically evaluating f  in
the direction of ζ. Second, it estimates the part of     f that is orthogonal to ζ by evaluating 
random,
pairwise orthogonal search directions that are orthogonal to ζ.  In this way, our estimator detects
the optimal lengths of the parameter update step into both the surrogate direction and the evaluated
orthogonal directions (e.g. if ζ  and     f  are parallel, the update step is parallel to ζ, and if 
they
are orthogonal the step into direction ζ has length 0).  Additionally, if the surrogate direction 
and
the gradient are not perfectly aligned, then the gradient estimate almost surely improves over the
surrogate direction due to the contribution from the evaluated directions orthogonal to ζ.  In the
following we define our estimator formally and prove that the estimated direction possesses best
possible alignment with the gradient that can be achieved with our sampling scheme.

We assume that k  pairwise orthogonal surrogate gradient directions ζ¹, . . . , ζᵏ are given to our
estimator. Denote by Rζ the subspace of Rn that is spanned by the ζⁱ, and by R⊥ζ the subspace that
is orthogonal to Rζ. Further, for vectors v and Qf , we denote by vˆ and Qˆ f the normalized vector


  v

ǁvǁ

and   Qf

ǁQf ǁ

, respectively. Let ϵˆ¹, . . . , ϵˆP be random orthogonal unit vectors from R⊥ζ. Then, our


estimator is defined as

k                    ˆi

ˆi              P                     i                               i

g    = Σ f (θ + σζ  ) − f (θ − σζ  ) ζˆi + Σ f (θ + σϵˆ ) − f (θ − σϵˆ ) ϵˆi .               (2)

	

We write Qf  = Qfǁζ + Qf⊥ζ, where Qfǁζ and Qf⊥ζ are the projections of Qf on Rζ and R⊥ζ,
respectively. In essence, the first sum in (2) computes Qfǁζ by assessing the quality of each 
surrogate
gradient direction, and the second sum estimates Qf⊥ζ similar to an orthogonalized antithetic ES
gradient estimator, that samples directions from R⊥ζ, see (5). We remark that we require pairwise

orthogonal unit directions ϵˆⁱ for the optimality proof. Due to the orthogonality of the 
directions, no
normalization factor like the 1/P  factor in (1) is required in (2). In practice, the 
dimensionality n
is often much larger than P . Then, sampling pairwise orthogonal unit vecotrs ϵⁱ is nearly identical
to sampling the ϵⁱs from a     (0, I) distribution, because in high-dimensional space the norm of
ϵⁱ        (0, I) is highly concentrated around 1 and the cosine of two such random vectors is highly
concentrated around 0.

For the sake of analysis, we assume that f is differentiable and that the second order approximation
f (x + ϵ)      f (x) +   ϵ,    f (x)   + ϵT H(x)ϵ, where H(x) denotes the Hessian matrix of f  at 
x, is
exact. This assumption implies that

f (θ + σϵˆ) − f (θ − σϵˆ)  = ⟨Qf (θ), ϵˆ⟩ ,                                           (3)

because the even terms cancel for antithetic sampling.  The following proposition and theorems
provide theoretical understanding, how our gradient estimation scheme improves gradient estimation

3


Under review as a conference paper at ICLR 2020

in this smooth, noise-free setting. In the following, we will omit the θ in Qf (θ). Our first 
proposition
states that g⊥ computes the direction in the subspace spanned by ζ¹, . . . , ζᵏ, ϵ¹, . . . , ϵP 
that is most
aligned with Qf .

Proposition 1 (Optimality of g⊥).  Let ζ¹, . . . , ζᵏ, ϵ¹, . . . , ϵP be pairwise orthogonal 
vectors in Rn.

Then, g⊥ = Σk    ⟨Qf, ζˆi⟩ζˆi + ΣP    ⟨Qf, ϵˆi⟩ϵˆi  computes the projection of Qf  on the subspace

spanned by ζ¹, . . . , ζᵏ, ϵ¹, . . . , ϵP . Especially, ϵ = g⊥ is the vector of that subspace that 
maximizes
the cosine ⟨Qˆ f, ϵˆ⟩ between Qf and ϵ. Moreover, the squared cosine between g⊥ and Qf is given by

k                             P

⟨Qˆ f, gˆₒur⟩2  = Σ⟨Qˆ f, ζˆi⟩2  + Σ⟨Qˆ f, ϵˆⁱ⟩2  .                                     (4)

	

We remark that when evaluating      f, vⁱ  for arbitrary directions vⁱ, no information about search
directions orthogonal to the subspace spanned by the vⁱs is obtained. Therefore, one can only hope
for finding the best approximation of     f  lying within the subspace spanned by the vⁱs, which is
accomplished by g   . The proof of Proposition 1 follows easily from the Cauchy-Schwarz inequality
and is given in the appendix.

3.3    ITERATIVE GRADIENT ESTIMATION USING PAST DESCENT DIRECTIONS

Our gradient estimation algorithm iteratively applies the one step gradient estimator g    by using 
the
gradient estimate of the last time step as surrogate direction for the current time step. 
Therefore, our
algorithm relies on the assumption that gradients are correlated from one time step to the next. 
This
assumption is justified since it is one of the reasons momentum-based optimizers (15; 16; 17) are
successful in deep learning. We also explicitly test this assumption experimentally, see Figure 1a.
The quality of the gradient estimate at any time step, depends on the quality of the surrogate 
gradient,
which might be restrained because (a) the previous gradient estimate might be very noisy, and (b)
the gradient changes with parameter updates. Our algorithm efficiently tackles problem (a), since it
improves the gradient estimate over the surrogate gradient at any time step. In order to 
theoretically
quantify this, we first analyse the case where the gradient does not change with parameter updates, 
i.e.
if a linear function is optimized. In this setting, we show that our algorithm needs a small factor 
more
than n (dimension) samples to align with the true gradient, see Theorem 1, which is close to 
optimal,
because at least n samples are required to determine the true gradient. Next, we incorporate 
problem

(b) into our analysis by considering general, non-linear functions, for which the gradient changes
with parameter updates. We show under some simplifying assumptions that, also in this case, our
algorithm builds up an improved gradient estimate over time, see Theorem 2.

We first need some notation.  Denote by θt the search point, by     ft =     f (θt) the gradient and
by ζt the parameter update step at time t, that is, θt₊₁ = θt + ζt. The iterative gradient 
estimation
algorithm obtains the gradient estimate ζt by computing g    with the last update direction ζt  ₁ as
surrogate gradient and P  new random directions ϵˆⁱ. Formally, let ϵˆ¹, . . . , ϵˆP be pairwise 
orthogonal

t             t

unit directions chosen uniformly from the unit sphere, that is, they are conditioned to be pairwise

orthogonal and are marginally uniformly distributed. By defining ϵt = ΣP    ⟨Qft, ϵˆⁱ⟩ϵˆⁱ, and 
setting

ζt = ⟨Qft, ζˆt−1⟩ζˆt−1  + Σ⟨Qft, ϵˆⁱ⟩ϵˆⁱ = ⟨Qft, ζˆt−1⟩ζˆt−1  + ⟨Qft, ϵˆt⟩ϵˆt ,               (5)

where we used ϵt  =    ϵt  ²/  ϵt     ϵˆt  =       ft, ϵt /  ϵt     ϵˆt  =       f, ϵˆt ϵˆt.  Then, 
Equation 4 of
Proposition 1 turns into

⟨Qˆ ft, ζt⟩2  = ⟨Qˆ ft, ζˆt−1⟩2  + ⟨Qˆ ft, ϵˆt⟩2  .                                         (6)

The next theorem quantifies how fast the cosine between ζt and     ft converges to 1, if     ft 
does not
change over time.

Theorem 1 (Convergence rate for linear functions).  Let ζt be iteratively computed, as in Equation 
(5),
using the past update direction and P  pairwise orthogonal random directions and let Xt = ⟨Qˆ f, 
ζˆt⟩
be the random variable that denotes the cosine between ζt and Qft at time t.  Then, the expected

4


Under review as a conference paper at ICLR 2020


drift of X² is E[X² − X²

|Xt−₁ = xt−₁] = (1 − x²

)    P   . Moreover, let ϵ > 0 and define T  to


t               t           t−1

t−1

N −1

be the first point in time t with X² ≥ 1 − δ. It holds

E[T ] ≤  N − 1 min{(1 − δ)/δ, 1 + ln(1/δ)} .

The first bound E[T ] ≤  N −1 1−δ  is tight for δ close to 1 and follows by an additive drift 
theorem,

while the second bound E[T ]      N−¹ (1 + ln(1/δ)) is tight for δ close to 0 and follows by a 
variable
drift theorem, see appendix. We remark that in a smooth and noise-free setting, where one can sample
the true directional derivative with Equation (3), a cosine squared of 1 − δ can be reached by 
sampling
(1 − δ)N  random orthogonal directions, see Proposition 2 in the appendix.  Since our algorithm

evaluates P + 1 directions per time step, it requires approximately min{1/δ, ¹⁺ˡⁿ⁽¹/δ⁾ } times more


samples to reach the same alignment.

1−δ

Naturally, the linear case is not the most interesting one. However, it is hard to rigorously 
analyse the
case of general f , because it is unpredictable how the gradient Qft differs from Qft−₁. Note that
Qft − Qft−₁ ≈ Hζt−₁, where H is the Hessian matrix of f at θt−₁. We define αt = ⟨Qˆ ft, Qˆ ft−₁⟩
and write Qˆ ft = αtQˆ ft−₁ + Qf⊥ where Qf⊥ is orthogonal to Qˆ ft and has squared norm 1 − α².

Then, the first term of (6) is equal to


⟨Qˆ f , ζˆ

⟩2  = ⟨α Qˆ f

+ Qf

, ζˆ

⟩2  = .α ⟨Qˆ f

, ζˆ

⟩ + ⟨Qf

, ζˆ

⟩Σ2   .

In the following, we assume that    f    is a direction orthogonal to    ft  ₁ chosen uniformly at 
random.
Though, this assumption is not entirely true, it allows to get a grasp on the approximate cosine 
that
our estimator is going to converge to.

Theorem 2.  Let ζt be iteratively computed using the past update direction and P pairwise orthogonal
random directions, see Equation (5), and let Xt = ⟨Qˆ f, ζˆt⟩ be the random variable that denotes 
the
cosine between ζt and Qft at time t. Further, write Qˆ ft = αtQˆ ft−₁ + Qf⊥, where 1 ≥ αt ≥ 0 and
assume that Qf⊥ has a random direction orthogonal to Qˆ ft−₁.  Choose ζt according to Equation (5)
and define Xt to be the cosine between Qˆ ft and ζˆt. Then,


E[X²|X

= x     ] = .α²x²

+ (1 − α²)(1 − x²

)     1     Σ .1 −      P     Σ +     P     .


t      t−1

t−1

t   t−1

t               t−1

N − 1

N − 1

N − 1

The last theorem implies that the evolution of the cosine depends heavily on the cosine αt between


consecutive gradients. Let

(1−α²)     ¹    (1−      P    )+     P  

. Then, the theorem implies that the drift

1−(α²+(1−α²)     ¹    )(1−      P    )

t               t   N −1            N −1

E[X² − X²    |Xt−₁ = xt−₁] is positive if xt−₁ ≤ A and negative otherwise. Thus, if αt would not

change over time, we would expect Xt to converge to A.

TODO:explain comparison to practise plot: The predicted alignement of the gradient estimate with
the true gradient of Theorem 2 is plotted in Figure 1b. Though the assumptions of Theorem 2 are
technically not true, the close fit of the theoretical prediction gives empirical evidence that the 
analysis
captures the general behaviour of the gradient estimation process.

4    EXPERIMENTS

In this section, we will empirically evaluate the performance of our gradient estimation scheme when
combined with deep neural networks. In Section 4.1, we show that it significantly improves gradient
estimation for digit classifiers on MNIST. In Section 4.2, we suggest how to overcome issues that
arise from function evaluation noise.  Finally, in Section 4.3, we evaluate our gradient estimation
scheme on RL environments and investigate further issues arising in this setting.

4.1    GRADIENT ESTIMATION AND PERFORMANCE ON MNIST

We observe that our approach significantly improves gradient estimation compared to standard ES.
Figure 1a shows that the key requisite of our iterative gradient estimation scheme is satisfied 
during

5


Under review as a conference paper at ICLR 2020

(a)                                                                                            (b)

Figure 1:  Improved gradient estimation.  (a) The gradients before and after a parameter update
are highly correlated.  The cosine between two consecutive gradients (blue line) and the cosine
between two random vectors (green line) are plotted. (b) A network is trained with parameter updates
according to gES using SGD (blue line) and Adam (yellow line) as optimizers.  At any step we
compute our gradient estimate with gₒur and the true gradient     f with backpropagation. The plot
shows that the ratio of the cosine between gₒur and     f  and the cosine between gES and     f  is
always strictly larger than 1.

training on MNIST, that is, that gradients between consecutive parameter update steps are 
correlated.
Figure 1b shows that our approach improves gradient estimation compared to ES during the whole
training process and strongly improves it in the beginning of training, where consecutive gradients 
are
most correlated, see Figure 1a. We observe that our approach strongly outperforms ES in convergence
speed and reaches better final performance for all hyperparameters we tested, see Figure 2 and
Table 1.

Implementation details:  For these experiments, we used a fully connected neural network with
two hidden layers with a tanh non-linearity and 1000 units each, to have a high dimensional model
(    1.8 million parameters) . For standard ES 128 random search directions are evaluated at each 
step.
For our algorithm the previous gradient estimate and 126 random search directions are evaluated. We
evaluated all directions on the same batch of images in order eliminate function evaluation noise 
and
we resampled after every update step. We used small parameter perturbations (σ = 0.001). This is
possible because no function evaluation noise is present and because the objective function is 
already
differentiable and therefore no smoothing is required. We test both SGD and Adam optimizers with
learning rates in the range 10⁰.⁵, 10⁰, . . . , 10−³.

Figure 2: Performance of ES (red lines) and our algorithm (blue lines) on MNIST classification. The
evolution of the training log-likelihood is plotted for the best three learning rates found for ES 
when
using the Adam optimizer (top) and SGD (bottom). Our algorithm uses the same learning rates and
hyper-parameters as ES.

6


Under review as a conference paper at ICLR 2020

Table 1: Results on the MNIST digit classification task. We report the best loss and the number of
steps until the training log-likelihood drops below 0.6, to observe the performance in the initial 
stages
of learning. The values are for the best performing learning rate for each optimizer.

Optimizer           Steps until loss < 0.6      Best loss
ES + Adam                       433                      0.242

Ours + Adam                    182                      0.216

ES + SGD                         727                      0.305

Ours + SGD                     295                      0.278

(a) Default ES                           (b) Ours with one past sample            (c) Ours with 
four past samples

Figure 3: Using several past descent directions improves robustness to function evaluation noise. 
The
plots show the performance on MNIST digit classification task with (blue lines) and without (red
lines) function evaluation noise. The noise is created artificially by randomly permuting the 
fitness
values of the evaluated search directions (see Equation 2) in 20% of parameter updates. The x-axis
represents the number of proper (i.e. non-permuted) parameter updates. (a) Standard ES does not
suffer from this.  (b) Function evaluation noise heavily impairs learning for our iterative gradient
estimation scheme when using one past update direction as surrogate gradient.  (c) Using 4 past
update directions as surrogate gradients makes our iterative gradient estimation scheme robust to
function evaluation noise.

4.2    ROBUSTNESS TO FUNCTION EVALUATION NOISE

In practice,  our iterative gradient estimation scheme may suffer from function evaluation noise
because it builds up good gradient estimates over several parameter update steps. Suppose that the
past update direction is a good descent direction but it performs poorly on the current batch used
for evaluation due to randomness in the batch selection or network evaluation process. Then, this
direction is weighted lightly when computing the new update direction, see Equation 5, and therefore
the information about this direction will be discarded. We empirically show, that our approach 
suffers
heavily from this issue when artificially injecting noise in the function evaluation process, see 
Figure
3b    .  Figure 3c shows, that this issue can be resolved by using the last k update directions for 
our
gradient estimator (see Equation 2). In this case, a good direction is only discarded, when it 
performs
poorly in k consecutive evaluation steps, which is very unlikely. We remark that the magnitude of
the parameter updates naturally limits k, because the k-th last update direction is only useful if 
it
is still correlated with the current gradient.  Concretely, we found that using the last 4 
parameters
updates was extremely helpful for smaller learning rates, even in the absence of noise (see Figure 
3c).
However, it did not offer an advantage for larger learning rates.

4.3    ROBOTIC RL ENVIRONMENTS

For the next set of experiments, we evaluate our algorithm on three robotics tasks of the Roboschool
environment: RoboschoolInvertedPendulum-v1, RoboschoolHalfCheetah-v1 and RoboschoolAnt-v1.
Our approach outperforms ES in the pendulum task, and offers a small improvement over ES in the
other two tasks, see Figure 4. The improvement of our approach over standard ES is smaller on RL
tasks than on the MNIST task. Therefore, we first empirically confirmed that past updated direction
are also in RL correlated with the gradient.  To test this, we kept track of the average difference
between random perturbations and the direction given by our algorithm, after normalizing the 
rewards.

7


Under review as a conference paper at ICLR 2020

We found that, the direction of our algorithm had an average weight of 1.11 versus the 0.65 of a
random direction.

RL  robotics  tasks  bring  two  additional  major  challenges  compared  to  the  MNIST  task.   
First,
exploration  is  crucial  to  escape  local  optima  and  find  new  solutions,  and  second,  the  
function
evaluation noise is huge due to each perturbation being tested only on a single trajectory.  Our
proposed solution of robustness against function evaluation noise intertwines with the exploration
issue. A rather small step size is necessary in order to use more past directions as surrogate 
gradients.
However, exploration in ES is driven by large perturbation sizes and noisy optimization 
trajectories.
We did not observe improvements when combining the approach of using several past directions with
standard hyperparameter settings. We believe that an exhaustive empirical study can shed light onto
the effect of our approach on exploration and may further improve the performance on RL tasks.
However, running extensive experiments for complex RL environment is computationally expensive.

Implementation details: We use most of the hyper-parameters from the OpenAI implementation ¹ .
That is, two hidden layers of 256 units each and with a tanh non-linearity. Further, we use a 
learning
rate of 0.01, a perturbation standard deviation of σ  = 0.02 and the Adam optimizer, and we also
apply fitness shaping (19). For standard ES 128 random perturbations are evaluated at each step. For
our algorithm the previous gradient estimate and 126 random perturbations are evaluated. For the
Ant and Cheetah environments, we observed with this setup, that agents often get stuck in a local
optima where they stay completely still, instead of running forward. As this happens for both, ES
and our algorithm, we tweaked the environments in order to ensure that a true solution to the task 
is
learned and not some some degenerate optima, we tweaked the environments in the following way.
We remove the penalty for using electricity and finish the episode if the agent does not make any
progress in a given amount of time. In this way, agents consistently escape the local minima. We use
a tanh non-linearity on the output of the network, which increased stability of training, as 
otherwise
the output of the network would become very large without an electricity penalty.

Figure 4: Performance of our algorithm (red line) and ES (blue line) on three different Roboschool
tasks: Ant (left), Cheetah (center) and Pendulum (right). The plot shows the mean average reward
over 9 repetitions as a function of time-steps (in thousands).

5    CONCLUSION

We proposed a gradient estimator that optimally incorporates surrogate gradient directions and
random search directions, in the sense that it determines the direction with maximal cosine to the
true gradient from the subspace of evaluated directions.  Such a method has many applications as
elucidated in (14).  Importantly, our estimator does not require information about the quality of
surrogate directions, which allows us to iteratively use past update directions as surrogate 
directions
for our gradient estimator. We theoretically quantified the benefits of the proposed iterative 
gradient
estimation scheme. Finally, we showed that our approach in combination with deep neural networks
considerably improves the gradient estimation capabilities of ES, at no extra computational cost. 
The
results on MNIST indicate that the speed of the Evolutionary Strategies themselves, a key part in 
the
current Reinforcement Learning toolbox, is greatly improved. Within Reinforcement Learning an out
of the box application of our algorithm yields some improvements. The smaller improvement in RL
compared to MNIST is likely due to the interaction of our approach and exploration that is essential
in RL. We leave it to future work to explicitly add and study appropriate exploration strategies 
which
might unlock the true potential of our approach in RL.

¹https://github.com/openai/evolution-strategies-starter

8


Under review as a conference paper at ICLR 2020

REFERENCES

[1]  Ingo Rechenberg. Evolution strategy: Optimization of technical systems by means of biological
evolution. Fromman-Holzboog, Stuttgart, 104:15–16, 1973.

[2]  Hans-Paul Schwefel.  Evolutionsstrategien für die numerische optimierung.  In Numerische
Optimierung von Computer-Modellen mittels der Evolutionsstrategie, pages 123–176. Springer,
1977.

[3]  Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions.

Foundations of Computational Mathematics, 17(2):527–566, 2017.

[4]  Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies
as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.

[5]  Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard E Turner, and Adrian Weller.
Structured evolution with compact architectures for scalable policy optimization. arXiv preprint
arXiv:1804.02395, 2018.

[6]  Xiaodong Cui, Wei Zhang, Zoltán Tüske, and Michael Picheny. Evolutionary stochastic gradient
descent for optimization of deep neural networks. In Advances in neural information processing
systems, pages 6048–6058, 2018.

[7]  Rein Houthooft, Yuhua Chen, Phillip Isola, Bradly Stadie, Filip Wolski, OpenAI Jonathan Ho,
and Pieter Abbeel. Evolved policy gradients. In Advances in Neural Information Processing
Systems, pages 5400–5409, 2018.

[8]  David Ha and Jürgen Schmidhuber.  Recurrent world models facilitate policy evolution.  In

Advances in Neural Information Processing Systems, pages 2450–2462, 2018.

[9]  Luke Metz, Niru Maheswaranathan, Jeremy Nixon, Daniel Freeman, and Jascha Sohl-dickstein.
Learned optimizers that outperform on wall-clock and validation loss. 2018.

[10]  Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.

[11]  Christopher JCH Watkins and Peter Dayan.  Q-learning.  Machine learning, 8(3-4):279–292,
1992.

[12]  David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.  Learning internal represen-
tations by error propagation.  Technical report, California Univ San Diego La Jolla Inst for
Cognitive Science, 1985.

[13]  Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman.  Random
feedback weights support learning in deep neural networks. arXiv preprint arXiv:1411.0247,
2014.

[14]  Niru Maheswaranathan, Luke Metz, George Tucker, Dami Choi, and Jascha Sohl-Dickstein.
Guided evolutionary strategies: augmenting random search with surrogate gradients. In Interna-
tional Conference on Machine Learning, pages 4264–4273, 2019.

[15]  Timothy Dozat. Incorporating nesterov momentum into adam. 2016.

[16]  Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.   On the importance of
initialization and momentum in deep learning. In International conference on machine learning,
pages 1139–1147, 2013.

[17]  Sebastian Ruder.  An overview of gradient descent optimization algorithms.  arXiv preprint
arXiv:1609.04747, 2016.

[18]  Nikolaus Hansen.  The cma evolution strategy: A tutorial.  arXiv preprint arXiv:1604.00772,
2016.

[19]  Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and Jürgen Schmidhuber.
Natural evolution strategies. The Journal of Machine Learning Research, 15(1):949–980, 2014.

9


Under review as a conference paper at ICLR 2020

[20]  Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.

[21]  Johannes Lengler and Angelika Steger.  Drift analysis and evolutionary algorithms revisited.

Combinatorics, Probability and Computing, 27(4):643–666, 2018.

A    PROOF  OF  THEOREMS

In this Section we prove the theorems from the main paper rigorously.

A.1    PROOF OF PROPOSITION 1

Note that for this theorem there is no distinction between the directions ζⁱ and ϵⁱ. For ease of 
notation,
we denote ζ¹, . . . , ζᵏ, ζ¹, . . . , ζP by ϵ¹, . . . , ϵᵐ. The theorem is a simple application of 
the Cauchy-

Schwarz inequality.  Denote by Qfǁsi   =  Σm   ⟨Qf, ϵˆⁱ⟩ϵˆⁱ the projection of Qf  on the subspace

	

spanned by the   ⁱs, and let        Σm          i       i=1

⟨Qf, ϵ⟩ = ⟨Qfǁsi , ϵ⟩ ≤ ǁ Q fǁsi ǁǁϵǁ .                                           (7)
Equality holds if and only if ϵ and Qfǁsi  have the same direction, which is equivalent to ϵ = 
αgₒur

for some α > 0. In particular, in this case the cosine squared between Qf and ϵ is


ˆ f, ϵˆ ² =  ǁ Q fǁsi ǁ

ǁ Q f ǁ2

m

=       ⟨Qˆ f, ϵˆⁱ⟩2                                                               (8)

i=1

A.2    EXPECTATION OF COSINE SQUARED OF RANDOM VECTORS FROM THE UNIT SPHERE

We need the following proposition for the proofs of Theorem 1 and 2.

Proposition 2.  Let uˆ be an N-dimensional unit vector, and let ϵˆ¹, . . . , ϵˆP be pairwise 
orthogonal
vectors sampled uniformly from the N-dimensional unit sphere, that is, they are marginally uniformly
distributed and conditioned to be pairwise orthogonal. Then, the expected cosine squared of uˆ and


P

i=1

⟨u, ϵˆⁱ⟩ϵˆⁱ is

E[⟨uˆ, ϵˆ⟩2] =  P


Proof.  Note that

₂         1   

          1           .ΣP

2

i  2               Σ      i  2


⟨uˆ, ϵˆ⟩

=  ǁϵǁ2 ⟨uˆ, ϵ⟩

=

i=1

⟨uˆ, ϵˆi⟩2

i=1

⟨uˆ, ϵˆ ⟩

=

i=1

⟨uˆ, ϵˆ ⟩

Denote by S the N -dimensional unit sphere. Linearity of expectation implies that

P

E[⟨uˆ, ϵˆ⟩2] =        E[⟨uˆ, ϵˆⁱ⟩2]

i=1


=       P     
V ol(S)

⟨uˆ, v⟩   dv

10


Under review as a conference paper at ICLR 2020

By rotational invariance of the unit sphere, we can replace uˆ by e₁ = (1, 0, . . . , 0) and obtain


E[  uˆ, ϵˆ ²] =       P     

V ol(S)

=       P     
V ol(S)

⟨e₁, v⟩   dv
v² dv

S


        P        

=

N · V ol(S)

S Σi=1

v² dv


=          P        
N · V ol(S)

P

1 dv

S

=

N

A.3    PROOF OF THEOREM 1

In order to prove Theorem 2, use Equation 6 to compute how X²  = ⟨Qˆ f, ζˆt⟩2  depends on X²    .


t

Then, we apply a variable transformation to X

t−1

t in order to be able to apply the additive and variable

drift theorems from (21), which are stated in Section B.


We can split the normalized gradient Qˆ f  = Qf⊥ζˆ

+ Qfǁζˆt−1

into an orthogonal to ζˆt−1  part and


a parallel to ζˆt−1  part. It holds ǁ Q f⊥ζˆ       ǁ2  = 1 − ⟨Qˆ f, ζˆt−1⟩2  and ⟨Qˆ fǁζˆ

, ϵˆt⟩ = 0 since ϵˆt is a

unit vector orthogonal to ζˆ     . Recall that  ˆ f     =    qf⊥ζ    , then

ǁQf⊥ζ ǁ


⟨Qˆ f, ϵˆt⟩2  = ⟨Qf

⊥ζˆt−1

, ϵˆt⟩2  = (1 − ⟨Qˆ f, ζˆt−1⟩2)⟨Qˆ f⊥ζ, ϵˆt⟩2  = (1 − X²

)⟨Qˆ f⊥ζ, ϵˆt⟩2  ,    (9)


and therefore by Equation 6

X² = ⟨Qˆ f, ζˆt−1⟩2  + ⟨Qˆ f, ϵˆt⟩2  = X²

+ (1 − X²

)⟨Qˆ f⊥ζ, ϵˆt⟩2  .                  (10)


t                                                                 t−1

Define the random process Yt = 1 − X². It holds

t−1


Yt = 1 − X²

+ (1 − X²

)⟨Qˆ f⊥ζ, ϵˆt⟩2  = Yt−₁(1 − ⟨Qˆ f⊥ζ, ϵˆt⟩2) ,  and             (11)


E[Yt|Y

t−1

= yt−1

] = y

t−1

.1 − E[⟨Qˆ f⊥ζ

, ϵˆt⟩2]Σ = y

t−1

1         P         ,             (12)

N − 1

where we used Proposition 2 in the N − 1 dimensional subspace that is orthogonal to ζt−₁.
In order to derive the first bound on T , we bound the drift of Yt for Yt ≥ δ.


E[Yt|Y

t−1

= yt−1

, yt−1

≥ δ] = y

t−1

    P    

− yt−₁ N − 1  ≤ y

t−1

    P    

− δ N − 1 ,

where we used Equation 12 and yt−₁  ≥  δ.  In order to apply Theorem 3, we define the auxiliary
process Zt = Yt − δ. Then, T  is the expected time that Zt hits 0. Since E[Zt|Zt−₁ = zt−₁, zt−₁ ≥

0] ≤ zt−₁ − δ   ᶜ    and Z₀ = 1 − δ, Theorem 3 implies that

1 − δ N − 1

E[T ] ≤

In order to apply Theorem 4, to show the second bound on T , we need to rescale Yt such that it 
takes
values in {0} ∪ [1, ∞). Define the auxiliary process Zt by


Z   =    Yt/δ     if Yt ≥ δ

0           if Yt < δ

.                                                   (13)

11


Under review as a conference paper at ICLR 2020

Then, T  is the expected time that Zt hits 0. The process Zt satisfies


E [Zt|Z

t−1

= zt−1

, zt−1

≥ 1] ≤ E [Yt/δ|Y

t−1

= δzt−₁

, zt−1

≥ 1] ≤ z

t−1

1         P         ,
N − 1

where we used Equations 13 and 12. Since Z₀ = 1/δ, Theorem 4 implies for h(z) = z    ᶜ   that

N −1


E[T ] ≤

N − 1 +

P

1/δ  N     1

du =

₁         Pu

N − 1 (1 + ln(1/δ)) .
P

A.4    PROOF OF THEOREM 2

In order to prove the theorem, we need to understand how Xt depends on the value of Xt−₁.  It

holds  X²  =  ⟨Qˆ ft, ζˆt−1⟩2  + ⟨Qˆ ft, ϵˆt⟩2.   As  in  Equation  12,  we  can  write  ⟨Qˆ ft, 
ϵˆt⟩2   =  (1 −

   P   

⟨Qˆ ft, ζˆt−1⟩2)⟨Qˆ f⊥ˆ       , ϵˆt⟩2, and note that E[⟨Qˆ f   ˆ     , ϵˆt⟩2] =          follows by 
Proposition 2 and


the definition of

ζt−1

ϵˆt. This implies that

⊥ζt−1

N −1


E[X²|X

= x     ] = .1 −      P     Σ E[⟨Qˆ f , ζˆ

⟩  |X

    P    

= x     ] +

(14)


t      t−1

t−1

N − 1

t    t−1

t−1

t−1

N − 1

To  understand  how  the  Xt  evolves  we  need  to  analyze  how  ⟨Qˆ ft, ζˆt−1⟩2   relates  to  
Xt−₁   =

⟨Qˆ ft−₁, ζˆt−1⟩2. To that end, we set αt = ⟨Qˆ ft, Qˆ ft−₁⟩ and write Qˆ ft = αtQˆ ft−₁ + Qf⊥Qf


where

Qf⊥Qft−1

is orthogonal to

Qft−1

and has norm

1 − α². Then,

t−1


⟨Qˆ f , ζˆ     ⟩2  = ⟨α Qˆ f

+ Qf            , ζˆ     ⟩2  = .α ⟨Qˆ f     , ζˆ     ⟩ + ⟨Qf            , ζˆ     ⟩Σ2   .


It follows that

E[⟨Qˆ ft, ζˆt−1⟩2|Xt−₁ = xt−₁]                                                                      
          (15)

= α²x²     + 2αtxt−₁E[⟨Qf⊥Qf    , ζˆt−1⟩] + E[⟨Qf⊥Qf    , ζˆt−1⟩2]                 (16)

    1    

= α²x²     + (1 − α²)(1 − x²)            ,                                                          
      (17)


t   t−1

N − 1

where we used E[⟨Qf⊥Qft−1 , ζˆt−1⟩] = 0, which follows from the assumption of Qf⊥Qft−1  being
a random direction orthogonal to Qft−₁, and that


E[⟨Qf⊥Qf

t−1

, ζˆt−1⟩2] = E[⟨Qf⊥Qf

, ζˆt−1⊥Qf

t−1 ⟩  ]


= ǁ Q f

ǁ2ǁζˆ

ǁ2E[⟨    Qf⊥Qft−1      ,

ζˆt−1⊥Qft−1     ⟩  ]


⊥Qft−1             t−1⊥Qft−1

ǁ Q f⊥Qf

t−1 ǁ

ǁζˆt−1⊥Qft−1 ǁ

= (1 − α²)(1 − x²)     1     ,

N − 1


which follows from ǁQ f⊥Qf

ǁ2  = 1 − α², ǁζˆt−1⊥Qf

ǁ2  = 1 − x² and Proposition 2 for P  = 1


using that Qf

t−1

t−1 f               t

ft   1  is a random direction orthogonal to      t  ₁. Then, plugging Equation (17) into
(14), implies the theorem.

B    DRIFT  THEOREMS

For the proof of Theorem 1, we use two drift theorems from (21), which we restate for completeness.

Theorem 3 (Additive Drift, Theorem 1 from (21)).  Let (Xt)t∈N0  be a Markov chain with state space
S ⊂ [0, ∞) and assume X₀ = n. Let T  be the earliest point in time t ≥ 0 such that Xt = 0. If there
exists c > 0 such that for all x ∈ S, x > 0 and for all t ≥ 0 we have

E[Xt₊₁|Xt = x] ≤ x − c .


Then,

E[T ]      n .

c

12


Under review as a conference paper at ICLR 2020

(a)                                                                                            (b)

Figure 5: Performance when optimizing a quadratic function. Performances of our approach (green),
standard ES (blue) and SNES (red). (a) On small dimensional functions (N  = 32 and 16 sampled
directions per time step) SNES outperforms our approach and standard ES. Our approach and ES
do     not improve further because they do not adapt their stepsize. (b) For high dimensional 
functions
(N  = 10000 and 16 sampled directions per time step), our approach outperforms SNES and standard
ES.

Theorem 4.  Variable Drift, Theorem 4 from (21)] Let (Xt)t∈N be a Markov chain with state space
S  ⊂  {0} ∪ [1, ∞) and with X₀  = n.  Let T  be the earliest point in time t ≥  0 such that Xt = 0.
Suppose furthermore that there is a positive, increasing function h : [1, ∞) → R>₀ such that for all
x ∈ S, x > 0 we have for all t ≥ 0

E[Xt₊₁|Xt = x] ≤ x − h(x) .


Then,

E[T ]         1    +

h(1)

ⁿ    1   

du .

₁    h(u)

C    ADDITIONAL  EXPERIMENTS

This section presents some additional experiments. We will incorporate a polished version of this
section into the experiments section of the main paper, for the camera ready version of the paper.

C.1    COMPARISON OF OUR APPROACH TO SEPARABLE NES AND GUIDED ES

We consider the toy task of optimizing a quadratic function f (x) =   Ax  ² as considered in (14).
Figure 5 compares our approach to a diagonal approximation of CMA-ES, that is, seperable natural
ES (SNES) from (19), that has the same runtime and memory complexity as our approach. While
SNES works verv well for small diminsional parameter space (N  = 32 , see Figrue 5a ), it is clearly
outperformed by our approach for high dimensional paramater spaces ( N  = 10000, see Figure 5b).

Further, we compare our one step gradient estimator g    to the guided-ES gradient estimator 
proposed
in (14). Again the goal is to optimize a quadratic function f (x) =   Ax  ². Both estimators 
receive a
surrogate gradient ζ for the optimization. The surrogate gradient is created as in (14): A 
normalized
random bias b (drawn once at the beginning of optimization) and a normalized noise direction n
(resampled at every iteration) are added to the true gradient. that is, ζ(x) =     f (x)+(b+n)      
f (x)  ₂
Figure   6 shows that our approach outperforms guided-ES no matter how the parameter α controlling
the bias variance trade-off of guided-ES is set.

Implementation details:  For Figure 5b, x has dimension N  = 10000 and is initalized by sampling
x₀          (0, I). A is a matrix of dimension 100   10000, and its entries are samples from a     
(0, 0.001)
distribution, where the variance is chosen such that the initial loss is approx 1. For Figure 5a, x 
has
dimension N  = 16 and A has dimension 16    16. The hyperparamters for our approach and standard
ES     are the learning rate β and perturbation size σ. The hyperparamters for SNES are learning 
rate

13


Under review as a conference paper at ICLR 2020

Figure 6:  Optimizing a quadratic function with biased surrogate gradients.  Performances of our
approach (green line), SGD using the surrugate gradient for the parameter update (purple line), ES
(blue line), guided-ES (yellow line) are plotted. Our approach (green line) improves sharply, making
use of the biased gradient, until it becomes useless (crossing with purple line).  For guided-ES,
we observed a binary behaviour when optimizing for the parameter that controls the bias-variance
trade-off analysed in (14). Either it follows the surrogate gradient very closely or it behaves 
similarly
to ES. This observation is consistent with the analysis conducted in (14). The optimal value that we
found for this parameter is very close to 1 and results in a performance very close to ES (yellow 
and
blue lines).

14


Under review as a conference paper at ICLR 2020

β, learning rate βσ for the step size adaptation and perturbation size initialization. We performed 
a
hyperparameter search for all these hyperparameters and plotted the best performance.

15

