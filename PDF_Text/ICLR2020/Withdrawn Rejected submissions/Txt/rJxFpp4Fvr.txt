Under review as a conference paper at ICLR 2020
Feature-Robustness, Flatness and General-
ization Error for Deep Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
The performance of deep neural networks is often attributed to their automated, task-
related feature construction. It remains an open question, though, why this leads to
solutions with good generalization, even in cases where the number of parameters
is larger than the number of samples. Back in the 90s, Hochreiter and Schmidhuber
observed that flatness of the loss surface around a local minimum correlates with
low generalization error. For several flatness measures, this correlation has been
empirically validated. However, it has recently been shown that existing measures
of flatness cannot theoretically be related to generalization: if a network uses ReLU
activations, the network function can be reparameterized without changing its
output in such a way that flatness is changed almost arbitrarily. This paper proposes
a natural modification of existing flatness measures that results in invariance to
reparameterization. The proposed measures imply a robustness of the network to
changes in the input and the hidden layers. Connecting this feature robustness to
generalization leads to a generalized definition of the representativeness of data .
With this, the generalization error of a model trained on representative data can be
bounded by its feature robustness which depends on our novel flatness measure.
1 Introduction
Neural networks (NNs) have become the state of the art machine learning approach in many appli-
cations. An explanation for their superior performance is attributed to their ability to automatically
learn suitable features from data. In supervised learning, these features are learned implicitly
through minimizing the empirical error Eemp(f, S) = 1/|S| P(x,y)∈S `(f (x), y) for a training set
S ⊂ X × Y drawn iid according to a target distribution D : X × Y → [0, 1], and a loss function
` : Y × Y → R+. Here, f : X → Y denotes the function represented by a neural network.
It is an open question why minimizing the empirical error during deep neural network training leads
to good generalization, even though in many cases the number of network parameters is higher than
the number of training examples. That is, why deep neural networks have a low generalization error
Egen = E(x,y)〜D ['(f (x),y)] - |SS| X '(f (x),y)
(x,y)∈S
(1)
which is the difference between expected error on the target distribution D and the empirical error on
a finite dataset S ⊂ X × Y .
It has been proposed that good generalization correlates with flat minima of the non-convex loss
surface (Hochreiter & Schmidhuber, 1997; 1995) and this correlation has been empirically vali-
dated (Keskar et al., 2016; Novak et al., 2018; Wang et al., 2018). Thus, for deep neural networks
trained with stochastic gradient descent (SGD), this could present a (partial) explanation for their
generalization performance (Zhang et al., 2016), since minibatch SGD tends to converge to flat local
minima (Zhang et al., 2018; Jastrzebski et al., 2017). This idea was elaborated on by ChaUdhari
et al. (2016) who suggest a new training method that favors flat over sharp minima even at the
cost of a slightly higher empirical error - indeed solutions found by this algorithm exhibit bet-
ter generalization performance. Similarly, DziUgaite & Roy (2017) aUgment the loss to improve
generalization and find that this promotes flat minima. However, as Dinh et al. (2017) remarked,
current flatness measures—which are based only on the Hessian of the loss function—cannot
1
Under review as a conference paper at ICLR 2020
theoretically be related to generalization: For deep neural networks with ReLU activation func-
tions, there are layer-wise reparameterizations that leave the network function unchanged (hence,
also the generalization performance), but change any measure derived only from the loss Hessian.
Another, more intuitive explanation for generalization is
that the function generalizes well if the extracted features
encode a semantic similarity of the input that is robust to
small changes—both in the input and the features. This al-
lows to generalize from the training set to novel, sufficiently
similar data. Starting from such a concept of robustness
with respect to changes of features, we derive a measure of
flatness that is invariant under the mentioned reparameteri-
zations and that reduces to the well-known ridge regression
penalty in the special case of a linear regression.
input	hidden	output
layer	layers	layer
φψ
Figure 1: Illustration of the decomposi-
tion of f = ψ ◦ φ.
This brings three seemingly related properties into our fo-
cus: flatness, robustness, and generalization. The exact
relationship, however, between flatness of the loss surface
around local minima (measuring changes of the empiri-
cal error for perturbations in parameter space), robustness
(measuring changes of the error for perturbations in either
input or feature space), and generalization (performance on unseen data from the target distribution)
is not well-understood. This paper provides new insights into this relationship.
The notion of feature robustness proposed in this paper measures the robustness of a function
f = ψ ◦ φ (e.g., a neural network) toward local changes in a feature space. That is, f can be split
into a composition of functions f(x) = (ψ ◦ φ)(x) for x ∈ X, φ : X → Rm and ψ : Rm → Y.
The function φ is considered as a feature extraction, mapping the input X into a feature space Rm,
while the function ψ corresponds to the model (e.g., a classifier) with Rm as its domain (see Figure 1
for illustration). It is the feature space defined by φ where we measure robustness toward small
perturbations. For neural networks, the activation values of any but the output layer can be viewed as
a feature space. A function f is called -feature robust on a dataset S ⊂ X × Y if small changes in
the feature space defined by φ do not change the empirical error by more than . This differs from
the notion of robustness defined by Xu & Mannor (2012) using a cover of the sample space, which
has been theoretically connected to generalization. Flatness of the loss surface, however, is a local
property and we require a more local version of robustness to derive a connection between flatness
and robustness. Then, indeed, feature-robustness is upper bounded by the proposed flatness measure.
To finally connect the two local properties of robustness and flatness to generalization, we necessarily
need a notion describing how representative the given samples are for the true distribution. We define
a suitable notion, leading to an upper bound for the generalization error given by feature robustness
together with representativeness.
In summary, our contributions are as follows: (i) For models of the form f(x) = (ψ ◦ φ)(x) (e.g.
most (deep) neural networks) that split up into a feature extractor φ and a model ψ on the feature
space defined by φ, we define a property of feature robustness that measures the change of the loss
function under small perturbations of the features. This property is strongly related to flatness of
the loss surface at local minima. (ii) We propose a novel flatness measure. For neural networks
with ReLU activation functions, it is invariant under layer-wise reparameterization, addressing a
shortcoming of previous measures of flatness. (iii) We define a suitable notion of representativeness
of a dataset connecting feature robustness to the generalization error in form of an upper bound. (iv)
The proposed flatness measure is empirically shown to strongly correlate with good generalization
performance. Thereby, we recover Hessian based quantities as measures of flatness.
2	Feature Robustness
We will define a notion of robustness in feature space Rm for the model f = (ψ ◦ φ) : X → Y,
which depends on a small number δ > 0, a training set S, and a feature selection defined by a matrix
A ∈ Rm×m of operator norm ||A|| ≤ 1. In the case of neural networks split into a composition
according to Figure 1, traditionally, the activation values φj (x) of neurons are considered as feature
2
Under review as a conference paper at ICLR 2020
values. The feature value defined by the j-th neuron in the feature space φ(x) ∈ Rm can be written
as φj(x) = hφ(χ), eji, where ej denotes the j-th unit vector and〈•，•〉the scalar product in Rm.
However, it was shown by Szegedy et al. (2013) that, for any other direction v ∈ Rm, ∣∣v∣∣ = 1, the
values hφ(x), vi = projvφ(x) obtained from the projection φ(x) onto v, can be likewise semantically
interpreted as a feature. We can single out the feature defined by v from φ(x) by multiplication with
the projection matrix Ev = vvT . Similarly, multiplication of φ(x) with a matrix A corresponds to a
weighted selection of rank(A)-many features in parallel (e.g., projection matrices on d-dimensional
subspaces correspond to the selection of d many features). This justifies our terminology considering
a matrix A as a feature selection. The same way that, for a sample input x, non-activated neurons
φj (x) = 0 are considered as non-expressed features, we call a selection of features defined by matrix
A as non-expressed whenever Aφ(x) = 0.
We define our notion of feature robustness. In words, feature robustness measures the mean change
in loss over a dataset under small changes of features in the feature space. Hereby, a matrix A
determines which features shall be perturbed. For each sample, the perturbation is linear in the
expression of the feature. Thereby, we only perturb features that are relevant for the output for a given
sample and leave feature values unchanged that are not expressed (in the sense explained above).
With
F(δ, s, A) := ISJ X ['(ψ(φ(x) + δAφ(x)),y) - '(f (χ),y)],
(x,y)∈S
(2)
the precise definition is given as follows:
Definition 1. Let ` : Y × Y → R+ denote a loss function, δ and two strictly positive (small)
real numbers, S = {(xi, yi) | i = 1, . . . , N} ⊆ X × Y a set, and A ∈ Rm×m a matrix such that
||A|| ≤ 1. A model f(x) = (ψ ◦ φ)(x), which is a composition of functions φ : X → Rm and
ψ : Rm → Y, is called ((δ,S, A),β)-feature robust, if |F(δ0, S, A)| ≤ E forall ∣δ0∣ ≤ δ.
More generally, if A ⊂ Rm×m denotes a probability space over matrices such that ||A|| ≤ 1
for all A ∈ A, then we call the model ((δ, S, A), E)-feature robust on average over A, if
EA 〜A [∣F (δ0,S,A)∣] ≤ E forall ∣δ0∣ ≤ δ.
We will bound feature robustness at local minima for a dataset S uniformly over all feature selections
A and dependent on δ . With our interpretation, this corresponds to an upper bound of the change in
loss when perturbing features in feature space Rm . In Appendix C.1 we note how feature robustness is
related to noise injection in the layer of consideration, which is known to be related to generalization
(An, 1996; Bishop, 1995).
3 Feature Robustness is Connected to Flatness of the Loss Curve
Consider a function f(x, w) = ψ(w, φ(x)) = g(wφ(x)), where ψ is the composition of a twice
differentiable function g : Rd → Y and a matrix product with a matrix w ∈ Rd×m . As before,
φ : X → Rm can be considered as a feature extractor. We fix a loss function ` : Y × Y → R+
for supervised learning and let w* denote a choice of parameters for which the empirical error
Eemp(w, S) = 1/|S| P(x,y)∈S `(f (x, w), y), considered as a function on w, is at a local minimum
on the training set S = {(xi, yi) ∣ i = 1, . . . , N}. In the following, we write z = φ(x).
For any matrix A ∈ Rm×m we have that
ψ(w, z + δAz) = g(w(z + δAz)) = g((w + δwA)z) = ψ(w + δwA, z).
Therefore,
F(δ, S, A) + Eemp(w, S) = |S| X '(ψ(w,z + Aδz),y)
(x,y)∈S
=|S| X '(ψ(w + δwA,z),y).
(x,y)∈S
(3)
(4)
The latter is the empirical error Eemp(w + δwA, S) of the model f on the dataset S at parameters
w + δwA. If δ is sufficiently small, then by Taylor expansion of Eemp(w, S) with respect to
3
Under review as a conference paper at ICLR 2020
parameters W around the critical point w*,we have that
Eemp(W* + δw*A, S) = Eemp(W*, S) + (δw*A, VEemp(W*, S))
+ 2hδw*A, HEemp(W*,S) ∙ (δw*A)i + O(δ3∣∣w*A∣∣F)	⑸
δ2
=Eemp(W*, S) + — hw*A, HEemp(W*, S) ∙ (w*A))+ O(δ3∣∣w*A∣∣F)
with HEemp(W*, S) denoting the Hessian of the empirical error with respect to w,(•，•) the scalar
product with vectorized versions of the parameters and ||w||F the Frobenius norm of w.
Subtracting Eemp(w*, S) from (5), maximizing over matrices ||A|| ≤ 1 and using (4), we get that,
for any feature selection A, the function (2) defining feature robustness is bounded by
δ2
∣maxιF (δ,S,A) ≤ ei1w*iif λmaχ(w*)+O(δ3)
(6)
where λHmax(w*) denotes the largest eigenvalue of the Hessian HEemp(w*, S) of the empirical error
at w*. Here we used the identity that max||x||=1 xTMx = λmMax for any symmetric matrix M, and
that for matrices of norm ||A|| ≤ 1, we have ||w*A||F ≤ ||w* ||F. We show details of the proof of
(6) in the appendix. We summarize the connection between feature robustness and flatness in terms
of the Hessian in the following theorem.
Theorem 2. Let ` : Y × Y → R+ denote a loss function, δ a strictly positive (small) real number,
A ∈ Rm×m a matrix with ||A|| ≤ 1, and let f(x, w) = g(wφ(x)) be a model with g an arbitrary
twice differentiable function on a matrix product of parameters w and the image ofx under a (feature)
function φ. Let w* denote a local minimum of the empirical error on a dataset S.
Then the model f(w*) is ((δ, S, A), )-feature robust for = δ2 llw*llF λHax(w * ) + O(δ3) ∙
4 Measures of Flatnes s of the Loss Curve
Motivated by the relation of feature robustness with the Hessian H, we define a novel measure of
flatness. Note that the Hessian is computed with respect to those parameters w that are applied
linearly on the feature space φ(X ) ⊆ Rm.
Definition 3. Let ` : Y × Y → R+ denote a loss function and f(x, w) = g(wφ(x)) be a model with
g : Rm → Y an arbitrary twice differentiable function on a matrix product of parameters w and the
image of x under a (feature) function φ : X → Rm. Then κφ (w) shall denote a flatness measure of
the loss surface defined by
κφ(w) := ||w||2 ∙ λmax(w).	⑺
Note that small values of κφ(w) indicate flatness and high values indicate sharpness.
Linear regression with squared loss In the case of linear regression, f(x, w) = wx ∈ R (X =
Rd, g = id and φ = id), for any loss function `, we compute second derivatives with respect to the
parameters w ∈ Rd as
∂2'	_	∂2'
∂wi∂wj	∂(f (x, w))2 Xixj
If ' is the squared loss function '(y, y) = (y 一 y)2, then ∂2'∕∂y2 = 2 and the Hessian is independent
of the parameters w. In this case, Kid = C ∙ ∣∣w∣∣2 with a constant C = 2λmaχ(Pχ∈s xxt) and the
measure κid reduces to (a constant multiple of) the well-known Tikhonov (ridge) regression penalty.
Layers of Neural Networks We consider neural network functions
f(x) = wLσ(. . . σ(w2σ(w1x + b1) + b2) . . .) + bL	(9)
of a neural network of L layers with nonlinear activation function σ . We hide a possible non-linearity
at the output by integrating it in a loss function ` chosen for neural network training. By letting
φl(x) = σ(wl-1σ(. . . σ(w2σ(w1x + b1) + b2) . . .) + bl-1) denote the output of the composition of
the first l-1 layers and gl(z) = WLσ(... σ(z+bι).. .)+bL the composition ofthe activation function
4
Under review as a conference paper at ICLR 2020
of the l-th layer together with the rest of layers, we can write for each layer l, f(x, wl) = gl(wlφl(x)).
Using (7) we obtain for each layer of the neural network a measure of flatness at parameter values w:
Kl(W) = ||wi||2 ∙ λmix(wι)	(IO)
with λHm,alx (wl ) the largest eigenvalue of the Hessian of the empirical error with respect to the
parameters of the l-th layer. By Theorem 2, κl is related to small changes of feature values in layer l.
Corollary 4. Let f denote a neural network function of an L-layer fully connected neural network.
For each layer l, 1 ≤ l ≤ L of size nl, let A ∈ Rnl ×nl with ||A|| ≤ 1 correspond to feature selections
offeatures in the l-th layer ofthe neural network. Let wl* denote weights of the l-th layer at a local
minimum of the empirical error.
Then the neural network is ((δ, S, A), e) -feature robust in layer l at w* for E = δ2^ κl(w*) + O(δ3).
For an everywhere well-defined Hessian of the loss function, we assumed our network function
to be twice differentiable. With the usual adjustments (equations only hold almost everywhere in
parameter space), we can also consider neural networks with ReLU activation functions. In this
case, Dinh et al. (2017) noted that a linear reparameterization of one layer, wl → λwl for λ > 0,
can lead to the same network function by simultaneously multiplying another layer by the inverse of
λ, Wk → 1∕λWk, k = l. Representing the same function, the generalization performance remains
unchanged. However, this linear reparameterization changes all common measures of the Hessian
of the loss. This constitutes an issue in relating flatness of the loss curve to generalization. We
counteract this behavior by the multiplication with ||wl ||2.
Theorem 5. Let f = f(w1, w2, . . . , wL) denote a neural network function parameterized
by weights wl of the l-th layer. Suppose there are positive numbers λ1 , . . . , λL such that
f(w1,w2, . . . , wL) = f(λ1w1, λ2w2, . . . , λLwL) for all wl. Then, with w = (w1,w2, . . . ,wL)
and wλ = (λ1w1, λ2w2, . . . , λLwL), we have
κl (w) = κl (wλ) for all 1 ≤ l ≤ L.	(11)
We provide a proof in Appendix A.2.
An Averaging Alternative Experimental work (Ghorbani et al., 2019) suggests that the spectrum
of the Hessian has a lot of small values and only a few large outliers. In this case, our flatness
measure serving as an upper bound for feature robustness is governed by the outlier. However,
feature robustness for different feature selections is governed by different eigenvalues of the Hessian,
according to (5). We therefore consider the trace as an average of the spectrum. We will show
that this tracial averaging corresponds to feature robustness on average over all orthogonal feature
selection matrices. The following theorem specifies this connection between feature robustness and
the unnormalized trace T r(H Eemp(w*)) of the empirical error at a local minimum w*. The details
and the proof can be found in Appendix A.3.
Theorem 6. Let ` : Y × Y → R+ denote a loss function, δ a strictly positive (small) real number,
and let f (x, w) = g(wφ(x)) be a model with g an arbitrary twice differentiable function on a
matrix product of parameters w ∈ Rd×m and the image of x under a (feature) function φ. Let w*
denote a local minimum of the empirical error on a dataset S and Om ⊂ Rm×m denote the set
of orthogonal matrices. Then, (i) for each feature selection matrix ||A|| ≤ 1 the model f(w*) is
((δ, S, A), E)-feature robust for E = δ~∣∣w*∣∣F Tr(HEemp(W*)) + O(δ3), and (ii) the model f (w*)
is ((δ, S, Om), e)-feature robust on average over Om for C = 2δm∣∣w*∣∣F Tr(HEemp(W*)) + O(δ3).
We therefore consider the unnormalized trace as a suitable and efficiently computable measure of
flatness and define for each layer l of a neural network
KITr(w) := IWlIlF ∙ Tr(HEemp(Wl,S)).	(12)
The same arguments as those used to prove Theorem 5 also show the measure κlTr to be independent
with respect to the same layer-wise reparameterizations. The analogue of Corollary 4 is as follows.
Corollary 7. Let f denote a neural network function of an L-layer fully connected neural network.
For each layer l, 1 ≤ l ≤ L of size nl, let Onl ⊂ Rnl ×nl denote the set of orthogonal feature
selections in the l-th layer of the neural network. Let wl* ∈ Rnl+1 ×nl denote weights of the l-th layer
at a local minimum of the empirical error. Then the neural network is ((δ, S, On), C)- feature robust
in layer l on average over On at w* for C = ɪKTr (w*) + O(δ3).
5
Under review as a conference paper at ICLR 2020
Remark 8. Other Hessian-based measures have been proposed that are invariant under the given
reparameterizations. Liang et al. (2019) consider the Fisher-Rao norm defined by ||wTH(w)w||
where H(W) = (Vw')(Vw')T denotes the Gauss-Newton approximation of the loss Hessian.
Therefore, the Fisher-Rao norm considers the second partial derivative only into the direction defined
by the given weight values W. Our measure considers all directions of moving away from a local
minimum. In particular, in contrast to these measures, we take the full spectrum of the Hessian into
account, which results in a natural measure of flatness around a local minimum. We also came across
preprints by Tsuzuku et al. (2019) and Rangamani et al. (2019), which propose a similar measure of
flatness. While the first one derives the flatness measure from a PAC Baysian approach, the latter
considers the Riemannian metric on the quotient manifold obtained from the equivalence relation
given by the refactorization of layers as above.
5 Feature Robustness and Generalization
In this section we aim to study the relation between flatness, feature robustness and the generalization
error (defined in (1)). The connection of flat local minima with generalization in PAC Baysian bounds
has been considered in several works (Neyshabur et al., 2017; Tsuzuku et al., 2019; Dziugaite & Roy,
2017). Arora et al. (2018) relates noise injection to generalization under the same setting. The work
of McAllester (1998; 1999) and Langford & Caruana (2001) initiated the PAC-Baysian approach
to generalization, which measures the generalization error (usually for the 0-1 loss) of stochastic
classifiers. This leads to bounds on the expected true error over a distribution of models Q in terms
of the expectation of the empirical error over Q and the Kullback-Leibler divergence between Q
and some prior distribution P. For example, Neyshabur et al. (2017) use work by McAllester (2003)
to derive an inequality relating the generalization error for the 0-1 loss ` and expected sharpness
γν := Eν [Eemp(W + ν, S)] - Eemp(W, S) over distribution ν: With probability (1 - ) over datasets
S of size N, the expected error Ew+ν := EV [E(x,y)〜D ['(f (x, W + V), y)]] over the “posterior”
distribution (w + ν) is bounded by Eemp(w, S) + YV + 4 J N (KL(W + ν∣∣P) + ln 含).Here, P
denotes a “prior” distribution which is fixed before seeing any data and KL denotes the Kullback-
Leibler divergence. If we aim to use distributions with local support (as considered in feature
robustness and the Taylor expansion relating feature robustness to flatness) with a data-independent
prior P, the KL term goes to infinity as the distribution ν becomes increasingly localized. Since
feature robustness as a local property is related to generalization (Morcos et al., 2018), we aim to
connect the local properties of flatness and feature robustness to generalization of a specific model by
following a different approach. Our approach will be independent of the loss function and work in
the sample space instead of averages over models in parameter space.
Since feature robustness is a local property in neighborhoods around the points (x, y) ∈ S, to connect
feature robustness to generalization we necessarily need an assumption of representativeness of the
given data samples S. A simple computation shows that
Egen(f, S )= EA 〜A [F (δ,S,A)]
+(E(x,y)〜D ['(f (x),y)]-ɪ X	EA 〜A ['(Ψ(Φ(Xi) + δAφ(Xi)),yi)]
(xi,yi)∈S
(13)
The first term is exactly feature robustness on average over a probability distribution A of feature
matrices. For the second term, we accordingly define a notion on datasets S that describes how
well the loss on the true distribution can be approximated by certain probability distributions. The
distributions we consider are composed of a dataset and (local) probability distributions around its
points suitably restricted to local distributions λi and νi centered around the origin 0.
Definition 9. Let ψ : Rm → Y be a model, ` : Y × Y → R+ denote a loss function, a
strictly positive (small) real number, and S = {(xi, yi) | i = 1, . . . , N} ⊆ X × Y a set. Let
Λ = (λi, νi)1≤i≤N denote a family of pairs of probability distributions on Rm × Y, where each
λi and νi have support contained in a neighborhood of the origin 0. (i) The pair (S, Λ) is called
6
Under review as a conference paper at ICLR 2020
€-representative for ψ (with respect to the loss ' and distribution D) if ∣Rep(S, Λ)∣ ≤ G where
ReP(S, A) := E(x,y)〜D ['3(X), y)] - S X E(ξχ,ξy)〜(λi×Vi) ['(ψ(xi + G y + ξy)].
(xi,yi)∈S
(14)
(ii) With Ω a collection of families Λ as above and H a hypothesis space, we say that S is (β, Ω)-
representative for H if for all ψ ∈ H there is Λψ ∈ Ω such that (S, Λψ) is E-representativefor ψ.
Interestingly, we naturally derived a definition of representativeness which is a generalization of
classical €-representativeness (see e.g. Definition 4.1 in (Shalev-Shwartz & Ben-David, 2014)),
justifying the terminology. Indeed, let Λ0 denote the family of probability distributions where
each λi = δ0 and νi = δ0 have full weight on the origin. Then S is (€, {Λ0})-representative
exactly when S is €-representative in the classical sense. Further, if S is €-representative and S is
(€0, Ω)-representative for some Ω containing Λo, then € ≤ €.
In our setting of a model f (x) = (ψ ◦ φ)(χ), which is split up into a feature extractor φ and a model ψ,
we consider (φ(S), Λ)-representativeness for model ψ and specific choices for Λ = Λδ,A. Here, Λδ,A
is a family of probability distributions induced by a distribution A on feature matrices A such that
||A|| ≤ δ as follows: We assume that a Borel measure μA is defined by a probability distribution A
on matrices Rm×m. We then define Borel measures μi on Rm by μi(C) = μa({A | Aφ(x∕ ∈ C})
for Borel sets C ⊆ Rm. Then λi is the probability distribution defined by μi. We fix the distributions
νi = δ0 and denote the set containing all families of distributions (λi, νi) that can be generated this
way by Aδ. The following result is a direct consequence of Equation 13 and our Definition 9.
Theorem 10. Let f(x) = (ψ ◦ φ)(x) be a model composed of functions φ : X → Rm and
ψ : Rm → Y. If f is ((δ, S, A), €)-feature robust for all ||A|| ≤ 1 and φ(S) is (€0, Aδ)-representative
for some H containing ψ, then the generalization error off is bounded by Egen(f, S) ≤ € + €0.
Hence, for generalization we need a model that is feature-robust and training data that is sampled
densely enough. In the trivial case with A = δ0 the distribution with full weight on the 0-matrix, we
can choose δ = 0 to obtain € = 0 and Egen ≤ €0 . The more feature robust a model is, the larger δ we
can consider to use the flexibility of choosing a nontrivial A to lower the bound on representativeness
and therefore the generalization error. We hope that in future work it will be possible to find suitable
distributions A that lead to computable generalization bounds.
6	Empirical Evaluation
In this section we empirically validate the practical usefulness of the proposed flatness measure. A
correlation between generalization and Hessian-based flatness measures at local minima has been
observed previously, but the results of Dinh et al. (2017) questioned the usefulness of these measures.
We show that our measure does not only overcome the theoretical issues, but also preserves the strong
correlation with the generalization error. Previous works mostly use accuracy of the trained model
on the testing dataset (Rangamani et al., 2019; Keskar et al., 2016) for evaluating the generalization
properties of the achieved minimum. Nevertheless this does not directly correspond to the theoretical
definition of the generalization error (1). For measuring the generalization error, we employ a Monte
Carlo approximation of the target distribution defined by the testing dataset and measure the difference
between loss value on this approximation and empirical error. In order to track the correlation of
the flatness measure to the generalization error, sufficiently different minima should be achieved by
training. The most popular technique is to train the model with small and large batch size (Rangamani
et al., 2019; Keskar et al., 2016; Novak et al., 2018; Wang et al., 2018), which we also employed.
A neural network (LeNet5 (LeCun et al.)) is trained on CIFAR10 multiple times until convergence
with various training setups. This way, we obtain network configurations in multiple local minima. In
particular four different initialization schemes were considered (Xavier normal, Kaiming uniform,
uniform in (—0.1,0.1), normal with μ = 0 and σ2 = 0.1), with four different mini-batch sizes (4, 32,
64, 512) and corresponding learning rates to keep the ration between them equal (0.001, 0.008, 0.02,
0.1) for the standard SGD optimizer. Each of the setups was run for 9 times with different random
initializations.
Here the generalization error is the difference between summed error values on test samples multiplied
by 5 (since the size of the training set is 5 times larger) and summed error values on the training
7
Under review as a conference paper at ICLR 2020
Figure 2: LeNet5 characteristics after training on CIFAR10. Each color corresponds to a different
setup of training, characterized by initialization strategy, mini batch size and learning rate. The setups
are ordered in ascending order by the mini batch size, with the largest corresponding to the brightest
color of the displayed points.
Trace X squared weight norm
Figure 3: LeNet5 configurations trained on CI-
FAR10 with random reparameterizations. The cor-
relation stays the same for the proposed measure,
while it breaks for classic Hessian-based measure.
Figure 4: Robustness and flatness for LeNet5 con-
figurations trained on CIFAR10. Results ordered
by flatness, showing that robustness is bound by
our flatness measure.
examples. Figure 2 shows the approximated generalization error with respect to the flatness measure
(for both κl and κlTr with l = 5 corresponding to the last hidden layer) for all network configurations.
The correlation is significant for both measures, and it is stronger (with ρ = 0.91) for κ5Tr. This
indicates that taking into account the full spectrum of the Hessian is beneficial. To investigate
the invariance of the proposed measure to reparameterization, we apply the reparameterization
discussed in Sec. 4 to all networks using random factors in the interval [5, 25]. The impact of the
reparameterization on the proposed flatness measure based on the trace in comparison to the traditional
one is shown in Figure 3. While the proposed flatness measure is not affected, the one purely based
on the Hessian has very weak correlation with the generalization error after the modifications. To
verify the relation described by Equation 6, we also compared feature robustness with δ = 0.001
and feature matrices A that have only one non-zero value 1 on the diagonal. Figure 4 shows that
up to outliers the robustness is bound by the flatness measure. Additional experiments conducted
on MNIST dataset are described in Appendix E, where we obtain correlation factors between the
generalization error and tracial flatness κlTr of 0.73, 0.70, 0.72, 0.71 for the network’s hidden layers
l = 1, 2, 3, 4 respectively.
7	Discussion and Conclusion
We established a theoretical connection between flatness, feature robustness and, under the assumption
of representative data, the generalization error. The relation between feature robustness and Hessian-
based flatness measures has been established for κl, which takes into account the maximum eigenvalue
of the Hessian, and κlTr, which uses the trace instead. Empirically, the measure κlTr based on the
trace of the Hessian shows a stronger correlation with the generalization error. This is not surprising,
8
Under review as a conference paper at ICLR 2020
since it takes into account the whole spectrum of the Hessian and every eigenvalue corresponds to a
feature selection matrix of feature robustness. The tracial measure can be related to feature robustness
by either bounding the maximum eigenvalue of the loss Hessian by its unnormalized trace or by
averaging feature robustness over all orthogonal matrices A ∈ Om . It is interesting to note that strong
feature robustness does not exclude the possibility of adversarial examples, first observed by Szegedy
et al. (2013), since large changes of loss for individual samples (i.e. adversarial examples) may be
hidden in the mean in the definition of feature robustness. In Appendix C.2 we briefly discuss the
freedom of perturbing individual points by suitable feature selection matrices A.
In contrast to existing measures of flatness, our proposed measure is invariant to layer-wise repa-
rameterizations of ReLU networks. However, we note that other reparameterizations are possible,
e.g., we can use the positive homogeneity and multiply all incoming weights into a single neuron
by a positive number λ > 0 and multiply all outgoing weights of the same neuron by 1∕λ. While
the Fisher-Rao norm suggested by Liang et al. (2019) is invariant to such reparameterizations, our
proposed measures of flatness κl and κlTr are in general not. In principle, variations of our flatness
measures can be found that are invariant to such reparameterizations as well (see Appendix B) but
their analysis, except for some empirical evaluations in Appendix E, is left for future work.
The second term in the generalization bound of Theorem 10 is given by our notion of representa-
tiveness. In order to find specific bounds for the -representativeness of (S, Aδ), a distribution over
matrices is required that induces a distribution which is similar to a localized kernel density estimation
(KDE). While our notion of representativeness is a generalization of classical representativeness, it
remains open whether it is efficiently computable. The more feature robust a model is, the more free-
dom there is to finding specific distributions over matrices that lead to bounds on the generalization
error. In Appendix D we give a computation of representativeness for a KDE with Gaussian kernels.
Taking things together, we proposed a novel and practically useful flatness measure that strongly
correlates with the generalization error. We theoretically investigated this connection by relating
this measure to feature robustness. This notion of robustness, together with a novel notion of
representativeness provides a link to the generalization error. To the best of our knowledge, this
yields the first theoretical connection between a notion of robustness, flatness of the loss surface, and
generalization error and can help to better understand the performance of deep neural networks.
References
Guozhong An. The effects of adding noise during backpropagation training on a generalization
performance. Neural computation, 8(3):643-674, 1996.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep
nets via a compression approach. Proceedings of the 34th International Conference on Machine
Learning, 2018.
Chris M Bishop. Training with noise is equivalent to tikhonov regularization. Neural computation, 7
(1):108-116, 1995.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs,
Jennifer T. Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent
into wide valleys. In ICLR 2017, 2016.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for
deep nets. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 1019-1028. JMLR. org, 2017.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. AAAI, 2017.
Gregory Fasshauer, Fred Hickernell, and Henryk Wozniakowski. On dimension-independent rates of
convergence for function approximation with gaussian kernels. Journal on Numerical Analysis, 50
(1):247-271, 2012.
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization
via hessian eigenvalue density. arXiv preprint arXiv:1901.10159, 2019.
9
Under review as a conference paper at ICLR 2020
SePP Hochreiter and Jurgen Schmidhuber. Simplifying neural nets by discovering flat minima. In
A.dvcm,ces in neural information processing Systems, pp. 529-536, 1995.
Sepp Hochreiter and Jurgen Schmidhuber. Flat minima. Neural Computation, 9(1):142, 1997.
StaniSlaW JaStrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, ASja Fischer, Yoshua Bengio,
and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623,
2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In ICLR
2018, 2016.
Steven G. Krantz and Harold R. Parks. Geometric integration theory. Springer Science and Business
Media, 2008.
John Langford and Rich Caruana. (not) bounding the true error. In Proceedings of the 14th
International Conference on Neural Information Processing Systems: Natural and Synthetic,
NIPS’01, pp. 809-816, 2001.
Yann LeCun et al. Lenet-5, convolutional neural netWorks.
Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geometry,
and complexity of neural netWorks. International Conference on Artificial Intelligence and
Statistics (AISTATS), 2019.
David McAllester. Simplified pac-bayesian margin bounds. In Learning theory and Kernel machines,
pp. 203-215. Springer, 2003.
David A McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh annual conference
on Computational learning theory,, volume 98, pp. 230-234, 1998.
David A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual conference
on Computational learning theory,, volume 99, pp. 164-170, 1999.
Ari S Morcos, David GT Barrett, Neil C RabinoWitz, and MattheW Botvinick. On the importance of
single directions for generalization. In ICLR 2018, 2018.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947-5956,
2017.
Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein.
Sensitivity and generalization in neural netWorks: an empirical study. In ICLR 2018, 2018.
Akshay Rangamani, Nam H. Nguyen, Abhishek Kumar, Dzung T. Phan, Sang H. Chin, and
Trac D. Tran. A scale invariant flatness measure for deep netWork minima. arXiv preprint
arXiv:1902.02434, 2019.
Shai Shalev-ShWartz and Shai Ben-David. Understanding machine learning: From theory to
algorithms. Cambridge university press, 2014.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian GoodfelloW,
and Rob Fergus. Intriguing properties of neural netWorks. arXiv preprint arXiv:1312.6199, 2013.
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Normalized flat minima: Exploring scale
invariant definition of flat minima for neural netWorks using pac-bayesian analysis. arXiv preprint
arXiv:1901.04653, 2019.
Huan Wang, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. Identifying generalization
properties in neural netWorks. arXiv preprint arXiv:1809.07402, 2018.
Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391-423, 2012.
10
Under review as a conference paper at ICLR 2020
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In ICLR 2017, 2016.
Chiyuan Zhang, Qianli Liao, Alexander Rakhlin, Brando Miranda, Noah Golowich, and Tomaso Pog-
gio. Theory of deep learning iib: Optimization properties of sgd. arXiv preprint arXiv:1801.02254,
2018.
11
Under review as a conference paper at ICLR 2020
A Proofs of Main Results
A.1 PROOF OF EQUATION 6
First note that for ||A|| ≤ 1,
||wA||F
m
λ X||wjA||2 ≤
∖ j=1
F
(15)
||w||F.
From (4) and (5) we get
max F(δ,S,A)
I∣A∣I≤1
(4=5)
δ
max 5(w*A, HEemp(W*,S) ∙ (w*A))+ O(δ3)
I∣A∣I≤1 2
；15)	δ2 T	°
≤ max	—zτ H Eemp(W*,S)z + O(δ3)
1 1 z 1 1 2≤ 1 1 wd 1 F 2
max —||w*||F ZTHEemP(WKSyZ + O(δ3)
1 1 Z 1 1 2 = 1 2
δ2..	“ L	C
-2^ ||w*||F λmαx(w* )+ O(δ ),
where we used the identity that max∣ ∣ X ∣ ∣ = ι xτMx = λMax for any symmetric matrix M.
A.2 PROOF OF THEOREM 5
In this section, we discuss the proof to Theorem 5. Before starting with the formal proof, we discuss
the idea in a simplified setting to separate the essential insight from the complicated notation in the
setting of neural networks.
Let F,F : Rd → R denote twice differentiable functions such that F(w) = F1(Xw) for all w and
all X > 0. Later, W will correspond to weights of a specific layer of the neural network and the
functions F and F will correspond respectively to the neural network functions before and after
reparameterizations of possibly all layers of the network. We show that
1
2h H (F (w)) = H (F(XW)).
X2
Indeed, the second derivative of F at Xw with respect to coordinates Wi) Wj IS given by the differential
quotient as
∂2F(Xw)	ɪ, F(XW + hei + hej) — F(Xw + he0 — F(XW + hej) + F(Xw)
∂wi∂wj	h→0	h2
lim
h→0
F(X(W + h ei + h ej )) - F(X(W + λ ei)) — F(X(W + λ ej)) + F(XW)
SrX
ɪlim
X2 h→0
F (W + h e + λ ej) — F (W + h ei) — F (W + 号 ej) + F (W)
(h )2
1 ∂ 2F (w)
X2 ∂wi∂wj
12
Under review as a conference paper at ICLR 2020
Since this holds for all combinations of coordinates, We see that HF(λw) = 1∕λ2HF(W) for the
Hessians of F and F, and hence
∣∣λw∣∣2HF(λw) = λ2∣∣w∣∣2 J HF (W) = ||w||2HF (w).
Formal Proof of Theorem 5 We are given a neural netWork function f(x; w1, w2, . . . , wL)
parameterized by Weights wi of the i-th layer and positive numbers λ1, . . . , λL such that
f(x; w1, w2, . . . , wL) = f(x; λ1w1, λ2w2, . . . , λLwL) for all wi and all x. With w defined by
w = (w1, w2, . . . , wL), wlλ = λlwl and wλ = (w1λ, w2λ, . . . , wλL), We aim to shoW that
κl(w) = κl(wλ),
Where κl (w) = ||wl||2 λHm,alx(wl) is the product of the squared norm of vectorized Weight matrix wl
With the maximal eigenvalue of the Hessian of the empirical error at w With respect to parameters wl .
Let F(u) :=	(x,y)∈S `(f (x; w1, w2, . . . , u, . . . , wL), y) denote the loss as a function on
the parameters of the l-th layer before reparameterization. Further, We let F(v) :=
P(x,y)∈S `(f (x; w1λ, w2λ, . . . , v, . . . , wLλ ), y) denote the loss as a function on the parameters of
the l-th layer after reparameterization. We define a linear function η by η(u) = λlu. By assumption,
We have that F(η(w∕) = F(Wl) for all w?. By the chain rule, We compute for any variable u(i,j) of
u,
_ , . _ ~ ,,..
dF (U) I	=dFm(U)) I
∂u(i,j) Iu=wl 一∂u(i,j) lu=wl
_ ~ , ..
X dF(n(U)) ∣
J ∂(η(u)(Am)) lη(u)=η(wι)
k,m
∂ (η(u)(k,m)) I
∂u(i,j)	∣η(u)=η(wι)
_ ~ ,.
∂F(v) ∣
∂v(i,j) Iv=λιWι
Similarily, for second derivatives, We get for all i, j, s, t,
d2F (U)	I = λ2 dF(V) I
∂u(i,j)∂u(s,t) Iu=wι	l ∂v(i,j)∂v(s,t) Iv=λιwι
Consequently, the Hessian H of the empirical error before reparameterization and the Hessian H
∖	订
after reparameterization satisfy H(wl, S) = λ ∙ H(λlwl,S) and also λmaχ(wl) = λ ∙ λHalχ(λlwl).
Therefore,
1 .	. . rɪ TTI	..cc ττ 1 .	. . rɪ τ~τ 1 ,	1 , 、.
Kl(W)= llwll1 λmL(wl) = llwlll2λ2 ∙ λmalx(λlWl) = llλl w||2 λmalx(λlWl) = Kl (wλ).
A.3 Proof of Theorem 6
Proof. (i) This is just a corollary of Theorem 2 using the trivial bound that the maximal eigenvalue is
bounded by the unnormalized trace (sum of eigenvalues) for positive semidefinte matrices (Where all
eigenvalues are positive).
(ii) We consider the set of orthogonal matrices A ∈ Om as equipped With the (unique) normalized
Haar measure. (For the definition of the Haar measure, see e.g. Krantz & Parks (2008).) We need to
show that Ea~θm [F(δ,S, A)] ≤ ∙2m∣∣w*∣∣F TT(HEemp(w*)) + O(δ3) with F(δ,S, A) defined
as in (2). Using (4) and (5) We get, similarly to (6),
δ2
EA~Om [F(δ,s,A)] ≤ EA~Om ɪ hw*A, HEemp(W*, S) ∙ (w*A)i + O(δ )
with h∙,∙ the scalar product with vectorized versions of
W wι* ∖	I wι*A
w*A =	. I A =	.
∖ Wd* )	∖ Wd*A
W ∈ R1×m
13
Under review as a conference paper at ICLR 2020
We consider the vectorization of w*A ∈ Rdm given by (w1*,..., WdGT. By Lemma 11 below, we
get
δ2 d
EA〜Om [F(δ, S, A)] ≤ EA〜Om - ∙ E (Wi*A)HEemP(Wj*,S)(w*A)T + O(δ3)
_	i,j=1	_
J2	d
=W ∙ E EA 〜Om [(Wi*A)H EemP(Wj* ,S )(wi.A)T] + O(δ3)
i,j = 1
(16)
Here, the notation HEemp(wj*,S) refers to the empirical error at w* but the derivatives are only
taken over the parameters in the row Wj * .
If Wi* = 0, then by Proposition 3.2.1 of Krantz & Parks (2008) and the change of variables formula
for measures, we get
EA〜Om [(Wi*A)HEemp(Wj*,S)(Wi*A)τ] = ∣∣Wi*∣∣2 EzeRm,||z|| = i [zTHEemp(Wj*,S)z] (17)
for all 1 ≤ i,j ≤ d, where the latter expectation is taken over the normalized (uniform) Hausdorff
measure over the sphere Sm-1 U Rm. Now, using the unnormalized trace Tr([hi,j]) = Pi hi,i we
compute with the help of the so-called Hutchinson,s trick:
EzeRm,||z|| = 1 [z HEemP(Wj*, S)z] = EIIz|| = 1 [Tr(Z HEemP(Wj*, S)z)]
=E||z||=i [Tr(HEemp(Wj*,S)zzτ)]	(18)
=Tr(HEemp(Wj*,S) E||z|| = i [zzT]).
Note that ZzT = [&Zj]i,j and due to symmetry E||z||=1 [ziZj] = E||z||=1 [zi(-Zj)] for i = j,
hence E||z||=i [&刁]=0 whenever i = j. Further E||z||=i [z2]=* E||z||=i [pm=1 z2]=
mm E||z||=i [∣∣Z∣∣2] = mm for all i. Therefore E||z||=1 [zzt] = mm ∙ Im is a constant multiple of
the identity matrix. Putting things together we have
(16) δ2 二	E	C
EA〜Om [F(δ,S,A)] ≤ E ∙ X EA〜Om [(Wi*A)HEemP(Wj*,S)(Wi*A)T] + O(δ3)
i,j=1
(17)	δ2
≤ —
-2
(18)	δ2
——
2
Q
^2^
d
∙ X ∣∣Wi*∣∣2 E||z|| = i [ztHEemP(Wj*,S)z] + O(δ3)
i=1,j
d1
∙ E ∣∣Wi*∣∣2 - ∙ Tr(H EemP(Wj*,S))+ O(δ3)
i,j=1	m
(X ∣∣wi*∣∣2) ∙ (χ -m ∙ Tr(HEemP(Wj*, S))) +O(S3)
δ2 (∣∣w*∣∣F) ∙	Tr(HEemP(W*,S))) + O(δ3)
δ2 ..	..c	o
「∣∣w*∣∣F ∙ Tr(HEemP(W*,S))+ O(δ3).
2m
□
Lemma 11. (i) Let H = [Hi,j]i,j be a positive semidefinite matrix in R2m×2m that consists of
submatrices Hi,j ∈ Rm×m, 1 ≤ i,j ≤ 2. Thenforall x =( ：； ) ∈ R2m with Xi ∈ Rm, we have
2XTH1,2X2 ≤ xτH2,2X1 + xτH1,1X2.
(ii) Let d, m ∈ N and H = [Hi,j]i,j be a positive definite matrix in Rdm×dm that consists of
submatrices Hi,j ∈ Rm×m, 1 ≤ i,j ≤ d. Thenfor all X = (xi, ...,Xd) ∈ Rdm with Xi ∈ Rm, we
have xτHx ≤ Pdj=I XTHi,iXj.
14
Under review as a conference paper at ICLR 2020
Proof. (i) By definition, H is positive Semidefinite if (H is symmetric and) ZTHz ≥ 0 for all z.
Choosing Z = (—x2,x1) gives xTH1,1x2 + XTH2,2x1 — 2XTH1, 2x2 ≥ 0, hence 2XTH1,2x2 ≤
XT H2,2Xl + XT H1,1X2.
(ii) Using that every submatrix Ha,b = ( H(Ta Ha,b ) is positive definite together with (i), we
obtain
xt Hx = ɪ2 XT Hi,iXi +	2XT Hij Xj
i	i=j
≤ EXT Hi,ixi+ E (XT Hjj xi+XT Hi,ix) = EXT Hjj xi
i	i=j	i,j
□
A.4 PROOF OF THEOREM 10
We are given a function f (x) = (ψ ◦ φ)(x). By assumption, f is ((δ, S, A), e)-feature robust for all
matrices | A|| ≤ 1, which implies that
∣1	X	W(ψ(φ(xi) + δAφ(xi )),yi)-'(f(xi),yi)] ≤ e foralll1All≤ L (19)
(χi,yi)∈S
Further, we are given that φ(S) is (ez, Aδ)-representative for a hypothesis space H such that ψ ∈ H.
By Definition 9 (ii) this means that there is some Λδ,A = (λi,δ0)i ∈ Aδ such that (S, A6/) is
EJrePreSentatiVe for ψ. That is, by Definition 9 (i),
E(x,y)~D ['(f(x),y)]一回	SX Eξχ~λi ['(ψ(φ(xi)+ ξx),yi)] ≤ 匕	(20)
(xi,yi)∈S
Since Aδ,/ = (λi, δ0)i ∈ Aδ, there exists a probability distribution A of matrices ∣∣A∣∣ ≤ 1 (so that
∣∣δA∣∣ ≤ δ) such that
∣SS∣	X	⅜c~λi	['(f (φ(xi)	+	ξx),yi)] =不 X	ea~a	W(ψ(φ(xi)	+	δAφ(xi)),yi)]
(xi,yi )∈S	(xi,yi)∈S
=EA~A 木 X '(ψ(φ(xi) + δAφ (Xi)),yi).
(χi,yi)∈S
(21)
Putting things together, we get for the generalization error Egen (f, S) of model f,
Egen (f, S )= E(χ,y)~D ['(f (x), y)] — ɪ X '(f (Xi), yi)
(χi,yi)∈s
-2i)	1
≤ E(x,y)~D ['(f(x),y)] — ∣s∣	工	旧力~尢['(ψ(φ(Xi) + zi),yi)]
(χi,yi)∈s
EA~A
+
ISI	X	'(ψ(φ(xi) + δAφ(Xi)),yi) -
(χi,yi)∈s
iS∣ X '(f (xi),yi)
(χi,yi)∈s
E(χ,y)~D ['(f (x), y)] — ɪ X	Ezi~ii ['(ψ(φ(xi) + Zi), yi)]
(χi,yi)∈s
+
Ea~/ ɪ X ['(ψ(φ(xi) + δAφ(xi )),yi) — 4(f(xi),yi)
(χi,yi)∈s
(19),(20),
≤ E + €.
15
Under review as a conference paper at ICLR 2020
B Additional Measures of Flatness
We present additional measures of flatness we have considered during our study. The original
motivation to study additional measures was given by the observation that there are other possible
reparameterizations of a fully connected ReLU network than suitable multiplication of layers by
positive scalars: We can use the positive homogeneity and multiply all incoming weights into a
single neuron by a positive number λ > 0 and multiply all outgoing weights of the same neuron
by 1∕λ. Our previous measures of flatness Kl and KTr are in general not independent of the latter
reparameterizations. We therefore consider, for a layer l of size nl , feature robustness only for
projection matrices Ej ∈ Rnl ×nl having zeros everywhere except a one at position (j,j). At a local
minimum w* of the empirical error, this leads to
δ2	T	3
Eemp(Wl* + δwl*Ej, S) - Eemp(Wl*, S) = y Wl*(j)T H Eemp(Wl*(j), S)wl*(j) + O(δ3)
where wl*(j) denotes the j-th column vector of weight matrix wl of layer l, and we only consider
the Hessian with respect to these weight parameters. We define for each layer l and neuron j in that
layer a flatness measure by
ρl (j)(W*) := Wl* (j)T HEemp(Wl* (j))Wl* (j)
For each l and j , this measure is invariant under all linear reparameterizations that do not change the
network function. The proof of the following theorem is given in Section B.1
Theorem 12.	Let f = f(W1, W2, . . . , WL) denote a neural network function parameterized by
(ij)	(ij)
weights Wi of the i-th layer. Suppose there are positive numbers λ1 , , . . . , λL, such that the
products Wlλ obtained from multiplying weight wl(i,j) at matrix position (i, j ) in layer l by λl(i,j)
satisfy that f(W1, W2, . . . , WL) = f(W1λ, W2λ, . . . , WLλ) for all Wi. Then ρl(j)(W) = ρl(j)(Wλ) for
each j and l.
We define a measure of flatness for a full layer by combinations of the measures of flatness for each
individual neuron.
ρl(W*) := maxρl(j)(W*) andρlσ(W*) :=	ρl(j)(W*)
j
j
Since each of the individual expressions is invariant under all linear reparameterizations, so are the
maximum and sum.
Analogous to Theorem 2, we get an upper bound for feature robustness for projection matrices Ej .
Theorem 13.	Let f denote a neural network function of a L-layer fully connected neural network.
For each layer l, 1 ≤ l ≤ L of size nl let Ej ∈ Rnl ×nl denote the projection matrix containing only
zeros except a 1 at position (j, j). Let Wl* denote weights of the l-th layer at a local minimum of the
empirical error.
Then the neural network is (δ, S, Ej), δ2∕2ρl (W*) + O(δ3) - feature robust for all j at W*.
One Value for all layers Our measure of flatness are strongly related to feature robustness, which
evaluates the sensitivity toward small changes of features. In a good predictor, generalization behavior
should correlate with the amount of change of the loss under changes of discriminating features. For
neural networks, we can consider the output of each layer as a feature representation. Each flatness
measure Kl is then related by Corollary 13 to changes of the features of the l-th layer. It is however
clear that a low value of Kl for a specific layer l alone cannot explain good performance. We therefore
specify a common bound for all layers.
Denoting by W* the set of weights from all layers combined, we have ||W*l ||F ≤ ||W* ||F for all l.
Further, if H(l) denotes the Hessian of the loss with respect to only the weights of the l-th layer, and
H the Hessian with respect to the weights of all layers, then λHm(alx),l (Wl*) ≤ λHmax(W*). (This holds
since
AD
λ(A) = max vTAv and (v, 0)T	T
v0	= vTAv.)
16
Under review as a conference paper at ICLR 2020
Table 1: Hessian based measures of flatness
Notation	Definition	One value per	Invariance
K	||w ||2 ∙λm°x(W)	network	none
Ki	||wi||2 ∙λmαx(Wl)	layer	layer-wise mult. by pos scalar
KiTr	||w||2 ∙ TT(HEemp(Wl, S)	layer	layer-wise mult. by pos scalar
max K	maxi Ki (w)	network	layer-wise mult. by pos scalar
KΣ	PlL=1 Kl(w)	network	layer-wise mult. by pos scalar
max KTr	maxl KlTr (w)	network	layer-wise mult. by pos scalar
KTΣr	PlL=1 KlTr(w)	network	layer-wise mult. by pos scalar
ρi(j)	wl (j)THEemp(wl(j), S)wl (j)	neuron	all linear reparameterizations
ρi	maxj ρl (j)(w)	layer	all linear reparameterizations
ρiσ	Pj ρl (j)(w)	layer	all linear reparameterizations
ρmax	maxl ρl (w)	network	all linear reparameterizations
ρΣ	PlL=1ρlσ(w)	network	all linear reparameterizations
Therefore, no matter which layer with activation values φl(xi) for each xi ∈ S we are perturbing
with a matrix ∣∣Aι∣∣ ≤ 1 to φl (Xi) + δAι ∙ φl (xi), We have that
δ2
F (δ, S, A) ≤ ~2 llw*"F ∙ λHax(w*) + O(δ3),
and κ(w*) = ||w* ||F ∙ λHaχ(w*) can be considered as a common measure for all layers.
However, κ(w*) is not invariant under the reparameterizations considered in Theorem 5. We therefore
consider more simple common bounds by combinations of the individual terms κl, e.g. by taking the
maximum of KI over all layers, Kmax(W*) := maxi κl(w*), or the sum kς(w*) := PL=I κl(w*).
Since each of the individual expressions are invariant under linear reparameterizations of full layers,
so are the maximum and sum.
Finally, we define Pmax(W*) := maxi ρl(w*) and pς(w*) := PL=I pσ(w*).
Table 1 summarizes all our measures of flatness, specifying whether each measure is defined per
network, layer or neuron, and whether it is invariant layer-wise multiplication by a positive scalar
(as considered in Theorem 5) or invariant under all linear reparameterization (as considered in
Theorem 12).
B.1 Proof of Theorem 12
As in Subsection A.2, we first present the idea in a simplified setting.
For the proof of Theorem 12 we need to consider the case when we multiply coordinates by different
scalars. Let F : R2 → R denote twice differentiable functions such that F(v, W) = F(λv, μw)
for all V ∈ R, W ∈ R and all λ,μ > 0. In the formal proof, v, W will correspond to two outgoing
weights for a specific neuron, while again F and F correspond to network functions before and after
reparameterizations of all possibly all weights of the neural network. Then
v
(v, W) ∙ HF(v, w) ∙
W
(λv,μw) ∙ HF(λv,μw) ∙ ∣ "
∖ μw
for allv, w and all λ,μ > 0.
17
Under review as a conference paper at ICLR 2020
Indeed, the second derivative of F at (λv,μw) With respect to coordinates v,w is given by the
differential quotient as
2
∂* 1 2F(λv, μw)	F	F(λv + h, μw + ke) — F(λv + h, μw) — F(λv, μw + k) + F(λv, W)
∂v∂w	h,k→0	hk
lim
h,k→0
F(λ(v + h), μ(w + μ)) — F(λ(v + λ, μw)) — F(λv, μ(w + k)) + F(λv, μw)
(h) (μ) λμ
1	,	F (V + h ,w + k) — F (V + λ ,w) — F (v,w + k) + F (v,w)
-- Iim --------------------------T-J-----------------------
λμ h,k→0	hk
1 ∂ 2F(v,w)
λμ ∂v∂w
From the calculation above, We also see that
∂2F(λv,μw)	1 ∂2F(v,w) d ∂2F(λv,μw)	1 ∂2F(v,w)
∂v∂v	λ2 ∂v∂v ，aɪɪ ∂w∂w	μ2 ∂w∂w
It folloWs that
(v, W) ∙ HF(v, W) •
2 ∂2F(v,w)	∂2F(v, w)	2 ∂2F(v,w)
∂v∂v + VW ∂v∂w + W ∂w∂w
-C ~	.
^vw)
∂V∂V
+ 2(λv)(μW)
∂2F(v, w)
∂V∂W
l 2	、2 d2F(V,w)
+ (μw)
(λv, μw) ∙ HF(λv, μw) •
λV
μw
Formal Proof of Theorem 12 We are given a neural netWork function f(x; w1, w2, . . . , wL)
parameterized by Weights wi of the i-th layer and positive numbers λ(1i,j), . . . , λ(Li,j) such that the
products wlλ obtained from multiplying Weight wl(i,j) at matrix position (i, j) in layer l by λl(i,j)
satisfies that f(x; w1, w2, . . . , wL) = f(x; w1λ , w2λ , . . . , wλL) for all wi and all x. We aim to shoW
that
ρl(j)(w) = ρl(j)(wλ )
for each j and l Where ρl (j)(w) = wl(j)THEemp(wl(j), S)wl(j), wl(j) denotes the j-th column
of the Weight matrix at the l-th layer and HEemp(wl(j), S) denotes the Hessian of the empirical
error With respect to the Weight parameters in wl(j). Similar to the above, We denote by wl(j)λ
the product obtained from multiplying Weight wl(j)i = wl(i,j) at matrix position (i, j) in layer l by
λ(i,j).
The proof is very similar to the proof of Theorem 5, only this time We have to take the different
parameters λl(i,j) into account. For fixed layer l, We denote the j-th column of wl and wl(j).
Let
F (u) := E '(f(x; wι, W2,..., [wι(1),..., Wl (j — 1), u, Wl (j + 1),... wι(nι)],
(x,y)∈S
. . . , wL), y)
denote the loss as a function on the parameters of the j-th column in the l-th layer before reparame-
terization and
F(v) := X '(f (Xi； w)1, W2λ2,..., [wι(1)λ,..., Wi (j — 1)λ, V, Wι(j + 1)λ,.. .Wi (nι)λ],
(x,y)∈S
...,wLλL),y)
18
Under review as a conference paper at ICLR 2020
denote the loss as a function on the parameters of the j-th neuron in the l-th layer after reparameteri-
zation.
We define a linear function η by
η(u) = η(u1, u2, . . . unl) = η(u1λl(1,j), u2λl(2,j), . . . unlλl(n,j)).
By assumption, we have that F (η(wl(j))) = F (wl (j)) for all wl(j). By the chain rule, we compute
for any variable ui of u,
_ _ ~ , .
dF (u) I	= dF(n(U)) |
∂u,i Iu=wι(j)	∂ui	Iu=wι(j)
X dF(n(U))I
■ ∂(η(u)k) ∣η(u)=η(wι(j))
∂(η(U)k)
∂ui	Iη(u)=η(wl(j))
_ ~ ,
∂F(v) I
∂Vi lv=wι(j)λ
Similarily, for second derivatives, we get for all i, s,
• λ(i,j).
d2F(U) I	= λ(i,j)λ(s,j)dF(v) I
∂Ui∂Us Iu=wι(j)	l l	∂Vi∂Vj Iv=wι(j)λ
Consequently, the Hessian HF of the empirical error before reparameterization and the Hessian H F
after reparameterization satisfy that at position (i, s) of the Hessian matrix,
HF(Wl)(i,s) = λ(i,j)λ(Sj) • HF(Wλ)(i,s).
Therefore,
ρl(j)(W) = Wl(j)T • HF(Wl) • Wl(j) = Xwl(i,j)wl(s,j)HF(Wl)(i,s)
i,s
=X w(i,j)w(s,j)λ(i,j)λ(Sj) • HF(Wλ)(i,s)
i,s
=X λ((i,j)wi,j)λ(Sj)W(Sj) • HF(Wλ)(i,s)
i,s
=(wl(j)λ)T ∙ HF(Wλ ∙ Wl(j)λ = ρl(j)(wλ)
C Additional properties of feature robustness
C.1 Relation to noise injection at the feature space
Feature robustness is related to noise injection in the layer of consideration. By defining a probability
measure PA on matrices A ∈ Rm×m of norm ||A|| ≤ 1, we can take expectations over matrices. An
expectation over such matrices induces for each sample x ∈ X an expectation over a probability
distribution of vectors ξ ∈ Rm with ∣∣ξ∣∣ ≤ ∣∣φ(χ) ||. We find the induced probability distribution Px
from the measure Px defined by Px(T) = PA({A | Aφ(x) ∈ T}) for a measurable subset T ⊆ Rm.
Then,
eA-Pa [F(δ, S, A)] = EA〜PA
∣sS∣ X ['(ψ(φ(x) + δAφ(x),y)) - '(f(x),y)]
(x,y)∈S
=iSi X Eξχ∈Pχ [ '(ψ(φ(χ) + δξx)-'(f(χ),y)].
(x,y)∈S
The latter is robustness to noise injection according to noise distribution Px for sample x in the
feature space defined by φ.
19
Under review as a conference paper at ICLR 2020
C.2 Adversarial examples
Large changes of loss (adversarial examples) can be hidden in the mean in the definition of
feature robustness. We have seen that flatness of the loss curve with respect to some weights is
related to the mean change in loss value when perturbing all data points xi into directions Axi for
some matrix A. For a common bound over different directions governed by the matrix A, we restrict
ourselves to matrices ||A|| ≤ 1. One may therefore wonder, what freedom of perturbing individual
points do we have?
At first, note that for each fixed sample xi0 and direction zi0 there is a matrix A such that Axi0 = zi0 ,
so each direction for each datapoint can be considered within a bound as above. We get little insight
over the change of loss for this perturbation however, since a larger change of the loss may go missing
in the mean change of loss over all data points considered in the same bound.
The bound involving κ(w*) from above does not directly allow to check the change of the loss
when perturbing the samples xi independently into arbitrary directions . For example, suppose we
have two samples close to each other and we are interested in the change of loss when perturbing
them into directions orthogonal to each other. Specifically, suppose our dataset contains the points
(1, 0, 0, . . . , 0) and (1, , 0, . . . , 0) for some small , and we aim to check how the loss changes
when perturbing (1, 0, 0, . . . , 0) into direction (1, 0, 0, . . . , 0) and (1, , 0, . . . , 0) orthogonally into
direction (0, 1, 0, . . . , 0). To allow for this simultaneous change, our matrix A has to be of the form
/	ι	T	...	ʌ
0	1	...
A =	0	'.
..
..
..
0	0	...
Then
/ 0 ʌ
1
l∣A∣l≥l∣A ∙	0 Il = 11(-ɪ, ɪ, 0,...)ll =乎.
.
.
.,
0
Hence, our desired alterations of the input necessarily lead to a large matrix norm ||A|| and our
attainable bound with ∣∣A∣∣2κ(w*) becomes almost vacuous.
C.3 Convolutional Layers
Feature robustness is not restricted to fully connected neural networks. In this section, we briefly
consider convolutional layers W * x. Using linearity, we get W * (X + δx) = (w + δw) * x. What
about changes (w + δwA) for some matrix A? Since convolution is a linear function, there is a
matrix W such that -w--*→x = Wx and there is a matrix WA such that -w-A--*→x = WAx. We assume
that the convolutional layer is dimensionality-reducing, W ∈ Rn×m , m < n and that the matrix W
has full rank, so that there is a matrix V with WV = Im.1 Then
(w + δwA) *x = Wx+δWAx = Wx+δWVWBx = W(x+δVWBx).
As a consequence, similar considerations of flatness and feature robustness can be considered for
convolutional layers.
1This holds for example for a convolutional filter with stride one without padding, as in this case W has a
Toeplitz submatrix of size (m × m).
20
Under review as a conference paper at ICLR 2020
D A LINK BETWEEN -REPRESENTATIVENESS AND KERNEL DENSITY
Estimation
Compared to classical representativeness, the definition of -representativeness is far more general,
allowing the choice of a family of distributions Λ = (λi, νi)1≤i≤N. A suitable restriction is to
consider only local distributions λi and νi centered around the origin 0. With this, the following
connection to kernel density estimation can be established: If a distribution can be approximated with
error by a kernel density estimation then this sample is representative.
Proposition 14. (i) If a distribution D can be -approximated by a Kernel Density Estimator with
Gaussian kernels and a diagonal bandwidth matrix using a sample S of size N ∈ N and if the loss
` : Y × Y → R+ is bounded by L, then for any such sample S there is some ΛN such that (S, ΛN) is
L-representative for f with respect to D and `. (ii) If the probability density function ofD lies inside
a d-dimensional kernel Hilbert space with Gaussian kernel Kh, i.e., PD (x, y) ∈ Hd, then (S, ΛN) is
L-representative with ∈ O N -1/4 .
Before we proof this proposition it is important to note that this result—in its current form—cannot
be used to obtain a generalization bound using Theorem 10: In Proposition 14, Λ = (λi × νi) is
chosen such that P(λi×νi)(z) = Kh(z), where Kh denotes the Gaussian kernel. Theorem 10 requires
the distribution to be induced by a probability distribution A on feature matrices A with kAk ≤ δ.
However, since Gaussians have support everywhere, the assumption that kAk ≤ δ for any finite δ > 0
does not hold. A possible solution would be to use truncated Gaussian kernels, for which kAk ≤ δ
can be ensured. However, it remains an open question whether there exists a probability distribution
A over feature matrices A that induces suitable truncated Gaussian distributions which would allow
to compute practical bounds on the generalization error.
We now provide the proof to Proposition 14.
Proof. Given a sample S 〜D with |S| = N, its representativeness is defined as
Rep(S) = E(χ,y)~D ['(f(X),y)] - N〉：	E(ξχ,ξy)~(λi,Vi) ['(f (Xi + ξx),yi + ξy )].
(xi,yi)∈S
We can rewrite E(x,y)〜D ['(f (x), y)[ as
E(x,y)〜D ['(f (X),y)] = / )彳 y'(f (X),y)PD(χ,y)d(χ,y) = / J(ZlPD(z)dz ,
where we abbreviate X × Y = Z, z = (X, y), and with slight abuse of notation write `(f (X), y)
`f (X, y) = `(z) for fixed f. Furthermore, since λi and νi are independent we can rewrite
N X	E(ξx,ξy)〜(λi,Vi) ['(f(xi + ξx),yi + ξy)]
(χi,yi)〜S
卷 X /	'(f(xi + ξx),yi + ξy)P(λi×Vi)(ξx,ξy)d(ξx,ξy) ∙
N (xi,yi)∈S (ξx,ξy)∈X×Y
`(zi +ξ))P(λi×νi)(ξ)dξ
By assumption, a Kernel Density Estimator on sample S, i.e.,
P(Z) = N X Kh(Z-ZiI
zi∈S
21
Under review as a conference paper at ICLR 2020
with kernel Kh, approximates PD(z) with approximation error . Thus, we get that
11	`(Z)PD(Z)dz- -1 XZ	`(Zi+ξ)P(λi×νi)(ξ)dξ
z∈Z	N zi∈S ξ∈Z
≤ I I	'⑶P(Z)dz - -1 XZ	'(Zi + ξ)P(λi×Vi)(ξ)dξ + emax('(Z))
z∈Z	N zi∈S ξ∈Z	z∈Z
≤ I I	'(Z)-1 X Kh(Z -ZadZ- -1 XZ	'(Zi + ξ)p(λi×Vi) (ξ)dξ + e max('(Z))
I z∈Z N zi∈S	N zi∈S ξ∈Z	I z∈Z
≤N ^X I I	'(Z)Kh(Z -Zi)dZ - Z	'(Zi + ξ)P(λi×νi)(ξ)dξ + eL .
zi∈S z∈Z	ξ∈Z
By substituting ζ = Z - Zi and choosing the (λi × νi) such that P(λi×νi)(Z) = Kh(Z) (which is
possible since we assumed the bandwidth matrix to be diagonal), we can further rewrite this as
ReP(S) ≤ -1 X I [	'(Z)Kh(Z - Zi)dZ - [	'(Zi + ξ)P(λi×Vi)(ξ)dξ + EL
N zi∈S I z∈Z	ξ∈Z	I
V7 X I [	'(Z + Zi)Kh(Z)dζ- Z	'(Zi + ξ)P(λi×Vi)(ξ)dξ + EL
N zi∈S I ζ∈Z	ξ∈Z	I
Kh(Z) -P(λi,νi)(Z) dZII +EL=EL
≤	+Zi)I
}
zi∈S
*{z
=0
If the probability density function of D lies inside a d-dimensional kernel Hilbert space with Gaussian
kernel Kh, i.e., PD(x, y) ∈ Hd, then it follows from Theorem 4 in Fasshauer et al. (2012) that
E
1 + ^^ )	∈ O (n- 4
2n-2)	'
□
E Additional experiments
In addition to the evaluation on the CIFAR10 dataset with LeNet5 network, we also conducted
experiments on the MNIST dataset. For learning with this data, we employed a custom fully
connected network with ReLU activations containing 4 hidden layers with 50, 50, 50, and 30 neurons
correspondingly. The output layer has 10 neurons with softmax activation. The networks were trained
till convergence on the training dataset of MNIST, moreover, the configurations that achieved larger
than 0.07 training error were filtered out. All the networks were initialized according to Xavier normal
scheme with random seed. For obtaining different convergence minima the batch size was varied
between 1000, 2000, 4000, 8000 with learning rate changed from 0.02 to 1.6 correspondingly to
keep the ratio constant. All the configurations were trained with SGD. Figure 5 shows the correlation
between the layer-wise flatness measure based on the trace of the Hessian for the corresponding layer.
The values for all four hidden layers are calculated (the trace is not normalized) and aligned with
values of generalization error (difference between normalized test error and train error). The observed
correlation is strong (with ρ ≥ 0.7) and varies slightly for different layers, nevertheless it is hard to
identify the most influential layer for identifying generalization properties.
We also calculated neuron-wise flatness measures described in Sec. B for this network configurations.
In Figure 6 we depicted correlation between ρlσ and generalization loss for each of the layers, and
in Figure 7-between Pl and generalization loss. The observed correlation is again significant, but
compared to the previous measure we can see that it might differ considerably depending on the layer.
The network-wise flatness measures can based both on layer-wise and neuron-wise measures as
defined in Sec. B. We computed κτmax, κτΣ, ρmax, and ρΣ and depicted them in Figure 8. Interesting
to note, that each of the network-wise measures has a larger correlation with generalization loss than
the original neuron-wise and layer-wise measures.
22
Under review as a conference paper at ICLR 2020
Figure 5: Layer-wise flatness measure calculated for MNIST trained fully-connected network. Four
plots correspond to four hidden layers of the network. For each of the layers a strong correlation with
generalization error can be observed.
Figure 6: Neuron-wise flatness measure ρlσ calculated for each of the hidden layers for the fully-
connected network trained on MNIST dataset. Each plot corresponds to a layer.
23
Under review as a conference paper at ICLR 2020
Figure 7: Neuron-wise flatness measure ρl calculated for each of the hidden layers for the fully-
connected network trained on MNIST dataset. Each plot corresponds to a layer.
Figure 8: Network-wise flatness measures based on various neuron-wise and trace layer-wise mea-
sures for the fully-connected network trained on MNIST dataset.
24