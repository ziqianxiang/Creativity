Under review as a conference paper at ICLR 2020
Towards Certified Defense for Unrestricted
Adversarial Attacks
Anonymous authors
Paper under double-blind review
Ab stract
Certified defenses against adversarial examples are very important in safety-critical
applications of machine learning. However, existing certified defense strategies
only safeguard against perturbation-based adversarial attacks, where the attacker
is only allowed to modify normal data points by adding small perturbations. In
this paper, we provide certified defenses under the more general threat model of
unrestricted adversarial attacks. We allow the attacker to generate arbitrary inputs to
fool the classifier, and assume the attacker knows everything except the classifiers’
parameters and the training dataset used to learn it. Lack of knowledge about the
classifiers parameters prevents an attacker from generating adversarial examples
successfully. Our defense draws inspiration from differential privacy, and is based
on intentionally adding noise to the classifier’s outputs to limit the attacker’s
knowledge about the parameters. We prove concrete bounds on the minimum
number of queries required for any attacker to generate a successful adversarial
attack. For a simple linear classifiers we prove that the bound is asymptotically
optimal up to a constant by exhibiting an attack algorithm that achieves this lower
bound. We empirically show the success of our defense strategy against strong
black box attack algorithms.
1	Introduction
Classifiers trained using machine learning can often achieve high accuracy on test data coming from
the same distribution as the training one. Unfortunately, for a class of inputs called adversarial
examples (Szegedy et al., 2014), many classifiers can be fooled to give wrong predictions with high
confidence. This poses a significant security risk for real world deployment of machine learning
models. To mitigate this threat, many methods have been proposed to defend against adversarial
examples (see, e.g., Papernot et al. (2016); Song et al. (2018a); Ma et al. (2018); Buckman et al.
(2018)). However, empirically grounded methods are often found to be ineffective using newer attack
methods that they had originally not considered (Carlini & Wagner, 2017; Athalye et al., 2018). To
end this arms race, defenses with certified theoretical guarantees against a threat model have recently
gained more traction (see, e.g., Raghunathan et al. (2018); Wong & Kolter (2017); Dvijotham et al.
(2018); Cohen et al. (2019)).
Existing certified defense methods are all restricted to perturbation-based attacks (Song et al., 2018a),
a threat model where the attacker is only allowed to generate an adversarial example by perturbing
existing data points using small adversarial noise. In addition, a certified defense for one type of
perturbation (e.g., based on l2 norm) can still be vulnerable to a different type of perturbation (e.g.
spatial transform (Xiao et al., 2018)). Following Song et al. (2018b) and Brown et al. (2018), we
consider the much more general threat model of unrestricted adversarial attacks, where the small
perturbation restriction is removed: any input is considered a valid adversarial example as long as it
induces the classifier to predict a different label than an oracle classifier. As shown in Song et al.
(2018b), existing certified defenses are not robust to unrestricted adversarial attacks.
In this paper, we provide a defense strategy with (asymptotically) certified guarantees against
unrestricted adversarial attacks. The attacker can use any strategy to choose any input, and the attack
is successful as long as the input is confidently classified into the wrong class by the target classifier.
Note that in the white-box setup, where the attacker has access to all parameters of the classifier,
defending against unrestricted adversarial examples is essentially impossible unless the classifier is
1
Under review as a conference paper at ICLR 2020
perfect, because the attacker can simply enumerate all possible inputs to find adversarial ones (we do
not assume any computational restriction of the attacker). As a result, we focus our discussion on
black-box unrestricted attacks, where the attacker does not know the exact parameters of the model
nor the exact training data (otherise, the attacker could recover the parameters by simulating the
learning algorithm), but can query the model for a limited number of times before making unrestricted
adversarial examples. For example, for online services, it is easy to limit the number of queries
allowed per user.
Consider a classifier learned using empirical risk minimization. Because the training data are
randomly drawn from the data-generating distribution, the parameters of the classifier will be random
even if the training algorithm is deterministic. Since we assume a black-box setting where the
attacker does not know the learned parameters or the training dataset, the attacker has to obtain
extra information by querying the model, even though the exact training algorithm and even the
data-generating distribution might be known to the attacker. As a result, we can adopt methods similar
to those used in differential privacy (Dwork et al., 2014) to reduce the amount of information about
the parameters leaked from each query issued by the attacker.
We give concrete lower bounds on the minimum number of queries an attacker needs in order to
successfully generate adversarial examples. For most classifiers the guarantees are asymptotic,
meaning that the guarantees are valid if the number of training examples is sufficiently large. For
very simple classifiers we prove finite sample guarantees. Our bound is also asymptotically minimax
optimal with respect to important parameters of the problem — the dimension of the input and the
size of the training set. We prove this by exhibiting a class of classifiers and a concrete attacking
algorithm that successfully generates adversarial examples while querying no more than the minimum
necessary according to the lower bounds (up to a constant).
Experimentally, we show that the defense strategy is effective against strong black-box unrestricted
attacks, even when we use complex classifiers for which no theoretical guarantee with finite sample
size is available. Compared to models without any defense, the number of queries needed to identify
vulnerabilities often increases by an order of magnitude using our defense strategy, with negligible
effect on the accuracy on “clean” inputs. Providing finite sample guarantees for complex models such
as deep neural networks is an open problem for future work.
2	Problem definition
2.1	Classification
Given a training dataset {(xi,yi) ∈ X × Y}n=ι i蚓 p*(x)p*(y | x), We consider the task of
learning a classifier to predict the label y for an input x. To simplify our discussion, we assume
that X = [-1,1]d, and Y = {-1,1}. Let {g(w, ∙) : X → R} be a class of score functions with
parameter w, Whose signs determine the class labels, i.e., y = sign(g(w, x)), Where We define
sign(x) = 1 when x ≥ 0, and sign(x) = -1 otherwise. The model family is assumed to be
well-specified, i.e., there exists a ground-truth parameter w* such that the true labeling function is
given by y = sign(g(w*, x)).
We can use any learning algorithm to obtain w based on a training dataset. Since the training data are
randomly generated from the underlying distribution p* (x)p* (y | x), w is also a random vector and
we denote its distribution as plearn (w). With a reasonable learning algorithm, plearn (w) should be
increasingly concentrated around w* while the size of the training dataset n grows. When deploying
the classifier to real world applications, we might want to use a (possibly stochastic) surrogate score
function f (w, ∙) : X → R which depends on the learned parameter W of our model. Usually, it is
just g(w, ∙), but to mitigate adversarial attacks we might choose f (w, ∙) to be some special classifier
with a defense strategy. To avoid low confidence incorrect predictions, we give no response when the
confidence is low. Specifically, we allow a threshold α ∈ R+ such that given an input query x, the
predicted label is
(1,	if f (w, x) > α
y = -1,	if f(w, x) < -α	(1)
I No response,	otherwise.
2
Under review as a conference paper at ICLR 2020
Protocol 1. The Attack-Defense Protocol
Attacker knowledge: w*, Plearn(w), T, g(∙, ∙), f(∙, ∙).
Interacting procedure:
1.	The attacker chooses the distributions qt,t = 1,…，T and Qattack defined as below.
2.	The defender samples W 〜Plearn (w).
3.	Fort — 1,…，T
(a)	The attacker observed oi：t-i = (xι,zι), ∙∙∙ , (xt-ι, zt-ι) and sample Xt 〜
qt(xt|o1:t-1) as the next query.
(b)	The defender sends zt = f(w, Xt) to the attacker.
4.	The attacker samples q 〜Qattack(q | oi：T), where q is a probability distribution on X. The
attacker outputs q(X).
Figure 1: The attack-defense protocol
2.2	Performance evaluation
We evaluate the expected performance of the classifier f (w, x) on a test distribution q(x)p* (y | x).
The traditional setting in statistical machine learning assumes that q(x) = p* (x). In adversarial
settings, however, q(x) is the distribution of adversarial examples and is very different from p* (x).
Since we consider unrestricted attacks, we assume no relationship between q(x) andp*(x).
The classifier f(w, x) can make two kinds of errors for a given input x. First, f(w, x) might give no
response. We call this the no-response error, which is denoted as
NR(x; α) , I(|f (w, x)| ≤ α),
where I[P] is the indicator function whose value is 1 when the property P holds; other it produces 0.
Secondly, f(w, x) can produce a wrong prediction with high confidence. We call this the margin
error, which is defined as
ME(x; α) = I(g(w*, x) < 0 ∩ f(w, x) > α) + I(g(w*, x) > 0 ∩ f(w, x) < -α).
An ideal classifier f(w, x) should minimize the expected no-response error Eq(x) [NR(x; α)] and
margin error Eq(x) [ME(x; α)] simultaneously. When defending against an adversary, the margin
errors are more important because a confident but wrong prediction is arguably more harmful than
giving no prediction at all. Consequently, we only focus on bounding the margin errors achievable by
attackers in the sequel. We will only consider no response error for clean data X 〜p* (x).
2.3	The attack-defense protocol
In Figure 1, we provide a specific protocol of attack and defense to capture our assumptions of the
threat model. Since we consider the threat model of unrestricted adversarial attacks, we assume
the attacker to be fully general: The attacker is allowed to use any adaptive set of intermediate
distributions {qt (x | o1:t-1)}tT=1 to issue queries based on all previous observations. For the final
attack distribution q, the attacker is allowed to sample it from any distribution of distributions
Qattack(q | o1:T). We call a specific choice of {qt(x | o1:t-1)}tT=1 and Qattack an attack strategy, and a
specific choice of Plearn(W) and f (∙, ∙) a defense strategy. We additionally allow the attacker to know
the exact defense strategy. Note that when the attack and defense strategies are fixed beforehand, the
attack-defense protocol defines a joint probability over w, o1:T and q.
The protocol can be modified in the defender’s favor if defender only sends yt according to Eq.(1)
instead of zt. However, using zt allows normal users (sampling from p* (x)) to get a confidence score,
and any certified defense results are more general (they are still true if attacker only has access to yt).
Intuitively, the attack strategy is successful under the attack-defense protocol, if with high probability
the final attack distribution q incurs large expected margin errors for f(w, x). To formalize this
intuition, we define a winning strategy of the attacker as follows.
3
Under review as a conference paper at ICLR 2020
Definition 1. For a given defense strategy, an attack strategy in the attack-defense protocol is
an (α, γ, δ)-winning strategy if with probability at least γ, the attacker outputs a q such that
Prq(x)[ME(x; α)] ≥ δ.
In the following part, we will propose a concrete defense strategy and prove that no winning strategies
can exist under some conditions.
3	Towards a certified defense
In order for a certified defense strategy to exist we need some reasonable conditions on the problem.
All the proofs are available in the appendix.
3.1	Defensibility
For an omniscient attacker who knows f (w, ∙) and g(w*, ∙) (this is beyond the capabilities granted
by our attack-defense protocol), there will not be any effective defense strategies unless f(w, x)
never produces wrong predictions. This is because the attacker can simply enumerate an input xbad
so that |f (w, Xbad) - g(w*, Xbɑd)∣ is maximal, and set q(x) = δχbad (here δχ denotes apointmass
distribution on x). This results in the maximal average margin error for f(w, x).
Under our attack-defense protocol, the attacker does not have access to w and hence do not know
f (w, ∙). The randomness of Plearn(W) makes it more difficult for the attacker to infer W through
queries. To guarantee the existance of a certified defense for all attacks, we need to make sure the
randomness of plearn(w) is sufficient such that no fixed distribution q(X) can incur large average
margin error for f (w, ∙) with high probability for a random draw of W ∈ Plearn(w). We formalize
this intuition as the following condition.
Condition 1. Given a classifier g(∙, ∙), margin a, and true parameter w*, a distribution Plearn(W) is
t-defensible if ∀x ∈ X, we have Eplearn(W)[|g(w, x) — g(w*, x)| ≥ α] ≤ e-t,
In other words, the above condition says that if Plearn(w) is t-defensible, there will not exist any
x ∈ X such that g(w, x) significantly deviates from the ground-truth score g(w*, x) with large
probability over W 〜Plearn(w).
3.2	Query Privacy
As discussed before, the attacker cannot find an attack distribution q a-priori to fool f (w, ∙) when
Plearn(w) is t-defensible. To strive for successful attacks, the attacker must then obtain more
information about the specific w used by the defender through queries. Therefore, a successful
defense strategy should not leak too much information through each query from the attacker. We
formalize this intuition as follows.
Condition 2. A defense strategy in the attack-defense protocol is s-query private if for any x ∈ X
we have I(z; w | x) ≤ s,
3.3	Certifying the defense strategy
To satisfy this condition and prevent leakage of information on w, we take inspiration from differential
privacy and perturb the scores with Gaussian noise. By adding the right amount of noise, we hope
to hit the sweet spot where the accuracy on clean input X 〜p* (x) is mostly preserved, while at
the same time the attacker obtains least information from queries. We use the following stochastic
surrogate function f (∙, ∙) as the key of our defense strategy.
f(w, x) = g(w, x) + N(0, τ2)	(2)
As the first main result of our paper, the following theorem asserts that under the above two conditions
no attacker has a winning strategy using a limited number of queries.
Theorem 1. Suppose Plearn(w) is t-defensible, and the defense strategy is s-private, ∀α > 0, 0 <
γ,δ < 1, andfor T2 ≤ 8lθg2∕δ, there is no (2α,γ, 2δ) winning strategy if T ≤ S (γt+γ log δ—log2),
4
Under review as a conference paper at ICLR 2020
Note that in the definition above s implicitly depends on τ2 . Adding more noise (larger τ2) reduces
the amount of leaked information on w.
In practice, it can be hard to verify the defensibility of plearn(w) or the query-privacy of the defense
strategy for complicated models. In the following, we show examples of simple models where these
two conditions are either strictly satisfied or asymptotically satisfied in the limit of infinite data. In
the next section, we will first show that for logistic regression and kernel logistic regression models,
the conditions hold asymptotically when the models are well-specified. After that, we show that the
conditions can hold exactly for a simple naive Bayes model. We leave the analysis of defensibility
and query-privacy for more complicated models as an open problem for future research.
3.4	Asymptotic analysis of (kernel) logistic regression
We start by considering a linear model g(w, x) = (w, x), where (∙, ∙) denotes the inner product
in Euclidean space. One common approach to learning this model is logistic regression. When
the model is well-specified, i.e., there exists w* such that p*(y | x) = i+exp(_[(w* 乂)), We can
prove thatPlearn(W) converges in distribution to a Gaussian distribution with mean w* as n → ∞.
Specifically, we have the following lemma.
Lemma 1. Assume that ∣∣w*k ≤ λ. Given i.i.d. samples {(xi,yi)}n=ι 〜p*(x)p*(y | x), the
estimator wn obtained by solving the following objective function
1n
max — 5"^log(1 + e-yi(w,xi))
kwk≤2λ n
i=1
is consistent. Moreover,
√n(wn — w*) → N(0, ∑),
where Σ = (Ex [p(y = 1 | x)(1 - p(y = 1 | x))xx|])-1 .
The asymptotic Gaussianity of plearn (w) greatly facilitates the investigation in its defensibility.
However, the assumption that a linear model g(w, x) being well-specified is rather restrictive. We
therefore additionally consider a generalization called kernel logistic regression, where g(w, x) =
hw, φ(x)i. Here φ(x) is the natural basis vector of the Reproducing Kernel Hilbert Space H
corresponding to a kernel k(∙, ∙), and we use〈•, •)to denote the inner product in H. When the
kernel is universal (Micchelli et al., 2006), e.g., the Gaussian RBF kernel, functions in RKHS can
approximate any bounded continuous function arbitrarily well w.r.t. the uniform norm. Therefore,
it is a much milder assumption that there exists w* ∈ H such that p*(y | x) = 5xp-hw* g(x))).
For kernel logistic regression, we can similarly prove that plearn(w) is asymptotically Gaussian (see
Lemma 6 in the appendix), with asymptotic variance
∑h = (Eχ[p(y = 11 x)(i — p(y = 11 x))φ(x)乳 Φ(x)])-1,
where 0 denotes the tensor product.
For both logistic regression and kernel logistic regression, under the assumptions of well-specification
and in the asymptotic regime, we can prove both defensibility and query privacy. Formally,
2
Proposition 1. For logistic regression, the asymptotic distribution of Plearn(W) is 2∣∣∑k 壮-defensible
and our defense strategy is 喙T⅞d -query private. For kernel logistic regression, the asymptotic
distribution of Plearn(W) is 2k∑αj——defensible, and our defense is k；HJop -query private.
Therefore, our defense strategy is asymptotically certified for logistic regression and kernel logistic
regression models.
3.5	non-Asymptotic Verification of Assumptions
Asymptotic conditions hold for sufficiently large training set size n, but to be fully verifiable we need
results that hold for finite n. These bounds are very difficult to obtain, and we show some bounds for
very simple classifiers (naive Bayes).
5
Under review as a conference paper at ICLR 2020
Before showing the results for naive Bayes classifiers, we first show defensibility of computing
averages.
Proposition 2. Let p(x) be any distribution with Covariance Σ and w* = Ep(X) [x]. Let
xι, •…，Xn 〜p(x) and W = 1 Pi Xi, then the distribution of W is 2d∣∣∑n+2∕3dα-defensible.
Note that the results of this theorem is almost as good as if we knew the distribution p(x) is Gaussian
2
N(0, Σ). In that case, We can apply Proposition 1 and conclude that it is 2n∑--defensible.
The following Corollary shows a similar bound for a naive Bayes classifier. This is a direct outcome
of the above proposition.
Corollary 1. Let p(x|y = 0) and p(x|y = 1) be distributions with covariance Σ, and w* =
Ep(χ∣y=i)[x] — Ep(χ∣y=0)[x]. Let Xi,…，Xn 〜p(x∣y = 0) and x；,…，Xn 〜p(x∣y = 1). Ifwe
choose W = n PiXi - n PiXi, then it is 4dk∑n+4∕3dα -defensible.
Therefore, We can use the training data to upper bound kΣk2 (El Karoui et al., 2008) and obtain a
non-asymptotic certificate of defensibly.
For query privacy non-asymptotic guarantees are easier to get. All We need is to upper bounding the
covariance spectral radius of plearn (w).
Proposition 3. If g(w, x) = (w, x) andPlearn(W) has covariance Σ, it is 怨k2 -query private.
4	Rate Analysis and Minimax Optimality
To demonstrate more concrete asymptotic rates, We Will analyze a simple setup. We study a classifier
With g(w, x) = (w, x) and plearn (w) = N(w*, σ2I) Where σ2 = 1/n. We also assume that
kw* k∞ < b for some b > 0 (We need b to be fixed as We analyze the asymptotic rate When d → ∞).
We give an example of a classifier Where this distribution could arise in the appendix. Based on this
setup, We have the folloWing specialization of Theorem 1.
α4n2
Corollary 2. There is no (2α,γ, 2δ) winning strategy if T < 2心 kιg	+ o(n) if we choose
T2 =	α2 .
2 log 2∕δ.
This bound is desirable if α 1 (there is a large margin) and n d (more data than input
dimensions).
4.1	An Attacker Strategy
Corollary 2 is the minimum number of queries any attacker strategy needs to Win. NoW We shoW a
concrete attacker strategy that (almost) achieves this loWer bound.
The concrete attacker strategy is shoWn beloW. Intuitively, the attacker queries each of the d dimen-
sions of the input x ∈ Rd to find out the sign ofw - w*. The attacker finds a x such that (x, w -w*)
is large but (x, w* ) < 0.
PROTOCOL 2. THE SIGN ATTACK PROTOCOL
1.	Defender samples W 〜Plearn(w).
2.	For round = t = 1, ∙ ∙ ∙ , T
(a)	Attacker chooses xt = t mod d, Where i is the i-th basis vector for Rd.
(b)	Defender sends zt = (xt, w) + N(0, τ2 ) to attacker.
3.	Attacker computes Wi = avg{zt∣xt = εi}. Without loss of generality (by re-indexing the
dimensions) |w*| > ∣w2∣ > …> |w*|. For i = 1,…，λdchoose Xi = -Sign(W*), and
for i = λd, ∙∙∙ , d choose Xi = Sign(W i 一 w*). Attacker chooses the smallest λ such that
(x, w*) < 0. She outputs a delta distribution on this x as his attack distribution.
6
Under review as a conference paper at ICLR 2020
Theorem 2. For any 0 < γ < 1, α > 0, if d is sufficiently large, the attacker in Protocol 2 is a
(α, 1,1-γ) winning strategy if T = 20α2n2 if n ≤ ⑹/羡浮 and we choose T2 = 2ɪθg^；i is a
(α, 1, I-Y) winning strategy when T = 2α√n if we choose T2 = 0.
The extra condition n ≤ i60α2diog2∕δ ensures that adversarial examples actually exist; if the classifier
is sufficiently accurate (i.e. k w - w* k ι ≤ α), then no example will be falsely classified.
4.2	Rate Summary and Comparison
Note that Corollary 2 is a lower bound on required number of queries: no attacker can have high
success rate unless T > 2总0务6 ： while Theorem 2 is an upper bound: with T = 20α4n2 there is at
least one attacker that can achieve high success rate. The two bounds have the same asymptotic with
respect to α, n and d, which shows that the bound is asymptotically optimal with respect to these
parameters.
To better study the asymptotic behavior for large d and large n, we will choose α = √d. For
both methods Pr?*(X)[NR(x)] can be arbitrarily small for sufficiently large d. This is proved in the
Appendix. We have the following rates for the smallest T such that an attacker can have a (α, 1, I-Y)
winning strategy for any γ > 0 and α = √d. With this choice of a, T no longer depend on d for
both upper and lower bounds, and we can get:
•	For defended classifier with T2 = 2iθg22∕δ we have T = Θ(n2) queries.
•	For undefended classifier with T 2 = 0 we have T = O(n).
5 Experiments
5.1	Linear Models
We will first verify our method empirically on linear classification tasks. We will use two classification
datasets.
Synthetic Gaussian. For this dataset we randomly sample w* from a standard Gaussian, sample
p(x∣y =1) = w* + N(0, σ2I) andp(x∣y = 0) = -w* + N(0, σ2I) and finally clip X to between
[-1,1]. We choose σ such that w* is the optimal logistic regression weight for p(y∣x)= +—(e⑷.
Mnist. Because linear models cannot classify MNIST with high accuracy, we use deep network
features (last layer of a pretrained AlexNet) as the input X instead of the pixel space. We train with
the entire dataset and use the resulting vector as w* (for the attacker).
Attack Methods. We will use two attacking method: the first one is the attack strategy of Protocol
2 (which we will denote the sign attack); the second one is Simba (Guo et al., 2019), a recently
proposed black-box attacking algorithm with very good empirical performance.
Classifiers. We will use two classifiers, the naive Bayes classifier defined in Corollary 1 and logistic
regression. For both classifiers we choose α such that the no-response rate on p* (X) (estimated using
the validation dataset) is no more than 10%. We either choose T2 = 0 (undefended) or T2
as in Theorem 1 (defended) where δ is chosen to be 5%.
ɑ2
2 log2∕δ
5.1.1	Results
The results are shown in Figure 2. For defended classifier, the number of queries required for
successful attack typically increases by an order of magnitude. In addition, the effect on the clean data
p* (X) is almost negligible. Another interesting observation is that, the naive Bayes classifier actually
performs much better in terms of adversarial robustness than logistic regression. We conjecture that
this is because logistic regression has high parameter estimation variance tr(Cov(w)) compared to
naive Bayes. This indicates that adversarial robustness has very different requirements compared
to classification accuracy. Even though logistic regression usually performs better in classification
unless the training data size is tiny (Ng & Jordan, 2002), its adversarial robustness is poor.
7
Under review as a conference paper at ICLR 2020
training set size = 16484
sə-dlutŋs ws⅛31 co .jbə
----NR defended
■ ■ ■ ME defended
NR undefended
■ ■ ■ ME undefended
IO2	IO3	IO4	IO3
number Oftraining samples
number of attacker queries
SSQUUnS XUEKg
IO3	104
number of attacker queries
----NR defended
■	■ ■ ME defended
NR undefended
■	■ ■ ME undefended
10s IO3 104 ioɔ
number Oftraining samples
Figure 2: Success rate of attacker vs. number of queries on synthetic dataset and MNIST. Left to
right: Left and Middle: the number of attacker queries vs. attack success rate for different training
set sizes. Right: the no-answer (NR) and margin error (ME) on clean data X 〜p* (x). Top to bottom:
Top: naive Bayes classifier trained on synthetic Gaussian; Middle: naive Bayes classifier trained on
MNIST; Bottom: logistic regression trained on synthetic Gaussian. Logistic regression trained on
MNIST fails to achieve robustness for the training set sizes we experimented (up to 10k). For both
cases, defended classifiers require orders of magnitude more queries for successful adversarial attack,
while the adverse effect on clean data X 〜p* (x) is negligible.
5.2	Deep Models
Our theoretical guarantees are far from effective for
deep models. It is unlikely the analysis will be ef-
fective for current network architectures, so special
architectures and analysis will be necessary for cer-
tified defense under out framework. Nevertheless
we perform simple experiments on deep networks
to empirically verify that our defense strategy can
effectively protect our model against black box attack
methods. The setup is identical to (Guo et al., 2019)
on ImageNet, except we add a threshold α to the out-
put sigmoid probabilities: if no class has a predicted
probability greater than the threshold α, the model
answers “I don’t know”. Here we choose α = 0.7.
The results are shown in Figure 3. Similar to linear
models, defended models require orders of magni-
tude more queries to attack. We hope these promising
experimental performance will motivate further the-
oretical analysis and empirical investigation.
Figure 3: Success rate vs. number of queries
for Simba attacker on Inception network. We
adjust the threshold such that both methods
have similar no response rate (NR) on the
clean data. The undefended network is signif-
icantly more vulnerable to attack.
6 Discussion
In this paper we propose a new type of adversarial defense guarantee, one based on the concept
of defensibility and query privacy. We show theoretical guarantees for simple classifiers and good
empirical performance for complex classifiers.
Several open questions remain to be answered. The first one to verify defensibility and query privacy
for more complex classifiers. The second one is to design classifiers with better defensibility and
query privacy properties. The third one is large scale empirical investigation of the defense strategy
or similar defense strategies, i.e. studying the performance of adding noise to different layers of a
deep neural network against a larger suite of black box attack algorithms. We hope our work can
serve as a first step in this exciting direction of research.
8
Under review as a conference paper at ICLR 2020
References
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.
Tom B Brown, Nicholas Carlini, Chiyuan Zhang, Catherine Olsson, Paul Christiano, and Ian Good-
fellow. Unrestricted adversarial examples. arXiv preprint arXiv:1809.08352, 2018.
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot way
to resist adversarial examples. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=S18Su--CW.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP),pp. 39-57. IEEE, 2017.
Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certified adversarial robustness via randomized
smoothing. arXiv preprint arXiv:1902.02918, 2019.
Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy A Mann, and Pushmeet Kohli. A
dual approach to scalable verification of deep networks. In UAI, pp. 550-559, 2018.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations
and TrendsR in Theoretical Computer Science, 9(3-4):211-407, 2014.
Noureddine El Karoui et al. Spectrum estimation for large dimensional covariance matrices using
random matrix theory. The Annals of Statistics, 36(6):2757-2790, 2008.
Chuan Guo, Jacob R Gardner, Yurong You, Andrew Gordon Wilson, and Kilian Q Weinberger.
Simple black-box adversarial attacks. arXiv preprint arXiv:1905.07121, 2019.
Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck,
Dawn Song, Michael E Houle, and James Bailey. Characterizing adversarial subspaces using local
intrinsic dimensionality. arXiv preprint arXiv:1801.02613, 2018.
Charles A Micchelli, Yuesheng Xu, and Haizhang Zhang. Universal kernels. Journal of Machine
Learning Research, 7(Dec):2651-2667, 2006.
Andrew Y Ng and Michael I Jordan. On discriminative vs. generative classifiers: A comparison of
logistic regression and naive bayes. In Advances in neural information processing systems, pp.
841-848, 2002.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on
Security and Privacy (SP), pp. 582-597. IEEE, 2016.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial
examples. In International Conference on Learning Representations, 2018. URL https://
openreview.net/forum?id=Bys4ob-Rb.
Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend:
Leveraging generative models to understand and defend against adversarial examples. In Interna-
tional Conference on Learning Representations, 2018a. URL https://openreview.net/
forum?id=rJUYGxbCW.
Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Constructing unrestricted adversarial
examples with generative models. In Advances in Neural Information Processing Systems, pp.
8312-8323, 2018b.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations, 2014. URL http://arxiv.org/abs/1312.6199.
Eric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.
Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially transformed
adversarial examples. arXiv preprint arXiv:1801.02612, 2018.
9
Under review as a conference paper at ICLR 2020
A Appendix
A.1 PROOFS RELATED TO THE LOWER BOUND OF T
Proof of Theorem 1. If a strategy is (2α, γ, 2δ) winning then with probability γ
qPX)[|g(W, X) -g(W*, X) + N (0,τ 2)l≥ 2α]≥ 2δ	⑶
Because |g(w, x) - g(w*,x) + N(0,τ2)∣ ≤ |g(w,x) - g(w*,x)| + |N(0,τ2)∣. IfEq.(3) is true
one of the following must be true
Pr[∣N(0,τ2)∣ ≥ α] ≥ δ	(4)
pr [∣g(w, x) - g(w*, x)| ≥ α] ≥ δ	(5)
q(x)
For Eq.(4) we can bound by
α2
Pr[∣N(0,τ2)| ≥ α] ≤ 2e-2T2
Therefore, Eq.(4) is false if the RHS is less than δ, which is when τ2 ≤
α2
2log2∕δ .
now we show the condition for Eq.(5) to be true. We first define the following notation.
•	X(w) = {x : |g(w,x) - g(w*,x)| ≥ α}
•	M(x) = {w : |g(w, x) - g(w*, x)| ≥ α}
•	Let q be any distribution on X, let q(X (w)) denote the probability mass q assigns to X (w).
The following lemma is needed.
Lemma 2. In out attack-defense protocol, if with probability at least γ, we have Pr[|g(w, x) -
g(w*, x)| ≥ α] ≥ δ ,then I (w; 01：T) ≥ γ(t + log δ) - log 2.
Therefore, if I(w; o1:T) < γ(t + log δ) - log 2, Eq. (5) is false with at least γ probability. If we
additionally have the condition that the attack-defense protocol is s-private, by a sequence of data
processing inequalities, and by the fact that xt only depends on w through o<t, and yt only depends
on xt and w, we have
I(w; o1:T) = I(w; o<T) +I(w;xT|o<T) +I(w;yT|xT,o<T) ≤ I(w; o<T) +I(w;yT|xT)
T
≤ …≤ XI(w; yt∣xt) ≤ TS
t=1
Therefore Eq. (5) is false with at least γ probability if
I(yt;w|xT) ≤ ST < γ(t +logδ) - log2
which is equivalent to T < ɪ (γt + Y log δ - log2). What remains is to prove Lemma 2, which We
do below.
Proof of Lemma 2. By the condition of t defensibility we have
Eplearn(w)[q(X (w))] = Eplearn(w)Eq(x) [I(x ∈ X (w))] = Eq(x) [I(w ∈ M (x))]
= Eq(x) Eplearn (w) [I(w ∈ M (x))] ≤ Eq(x) [e-t] = e-t
We abbreviate Eplearn(w) by Ew. Define S(q; w) = max(q(X (w)), e-t), then
Ew[S(q;w)] ≤ Ew[q(X (w))] + Ew[e-t] ≤ 2e-t = e-t+log 2
In addition we have Ew[EQ(q) [S(q; w))]] = EQ(q) [Ew[S(q; w))]] ≤ e-t+log 2. Finally by Donsker-
Varadhan inequality,
EwEQ(q|w)[log S(q; w)] ≤ EwKL(Q(q | w)kQ(q)) + Ew log EQ(q)[elog s(q;w)]
≤ I(q; w) + log EwEQ(q)[S(q; w)]
≤ I(q; w) - t + log 2
10
Under review as a conference paper at ICLR 2020
Suppose δ > e-t, log q(X (w)) ≥ log δ if and only if log s(q; w) ≥ log δ. In addition, because
for all q and w, log s(q; w) ≥ -t, so log s(q; w) + t is a non-negative random variable, and we
can apply Markov inequality to get (where the probability and expectation are taken with respect to
W 〜Plearn(W) and q 〜Q(q | W))
γ = Pr[q(X (w)) ≥ δ] = Pr [log q(X (w)) ≥ log δ] = Pr[logs(q;w) ≥ log δ]
Pr[log s(q; W) + t ≥ logδ + t] ≤
E[logs(q;W) +t]
log δ + t
I (q; w) +log2
log δ + t
Rearranging we get
I(q; W) ≥ γ(t+logδ) -log2
Finally because q is a function of o1:T , we have by data processing
I(W;o1:T) ≥ I(W;q) ≥ γ(t +logδ) - log2
□
□
ProofofCorollary 2. First by Lemma 4 We know that Plearn(W) is 2α2d-defensible and 型-query
private. We apply Theorem 1 and choose T2 = zlog；^ to conclude that there is no (2α, γ, 2δ)
winning strategy if
2τ2	α2	α2n
T ≤ σd 卜许+Y log δ - log2) = diogi/j
Y(Odn + Y log δ - log2
□
A.2 Proofs related to the sign attack
Proof of Theorem 2. Under the distribution in Protocol 2, denote
X = (Xi,…，Xd) = (Sign(W 1 - W；),…，Sign(Wd - w√))
Also denote the probability of attacker producing X given forecaster choose W as q(X | w). By observ-
ing the protocol we know that this distribution is factorized, i.e. q(X | w) = q(Xi | W) •…q(Xd | w).
Also denote b； = Sign(W - W；) as the true sign of the error.
Step 1: We show that if q(Xi = b； | w) ≥ 1/2 + β, (X, r；) is likely large
Lemma 3. Under the distribution defined in Protocol 2, suppose for all i we have q(Xi = b； | w) ≥
1/2 + β, then
Pr (X, r；) ≥ 2ββdσ ≥ 1 - e-2ed/n
√π
Proof of Lemma 3. The proof idea is to first lower bound the expectation, then lower bound the
deviation from the expectation.
μ := E[(X, r；)] = X E[Xir；] = X E[X ir；l Xi = b；]Pr[Xi = b；]+ E[Xir¾ = b；]Pr[X i = b；]
i
i
≥ XE[|r；|](1/2 + β) - E[|r；|](1/2 - β) = 2βE[什*旧=2√2βdσ
V	√π
Then because (X, r；) = Pi Xir；, where {Xir；, i = 1, ∙∙∙ ,d} are independent mixtures of Gaussian.
In addition Xir↑ has a sub-Gaussian tail
c2
Pr[Xir ≥ c] ≤ Pr[∣ri ∣≥ c] ≤ e 4σ2
so it is σ2∕2-sub Gaussian. Combined We have (X, r；) is σ2d∕2-sub Gaussian. Therefore
c2d2
Pr[(X, r；) ≤ μ — cd] ≤ e dσ2 = e

c2d
T2
11
Under review as a conference paper at ICLR 2020
If We plug in C = √2 βσ We have
Pr
(X, r*) ≤ √∏βdσ
2βd
≤ e--
Step 2: Lower bound on q(Xi = b* | w). The first step is to show that Xi = b* with non-negligible
probability. Because Wi - w* 〜N(r*,τ2∕T) and if τ2∕T ≥ 2σ2
Pr[Xi = b*] = Pr[N(0,τ2∕T) ≤ |N(0,σ2)∣] ≥ ； + σ10T	(6)
The proof of the above statement relies on the following Lemma
Lemma 4. Let X 〜N(0, a2), y 〜N(0, b2) be independent random variables, then if a ≥ 2b, then
1 + 10a ≤ Pr[x ≤ |y|] ≤ 2 + π∏a∙
Proof of Lemma 4∙
Pr[x ≤ |y|] = Ey [Pr[x ≤ |y||y]] = Ey
Because Φ is concave in [0, +∞) we have
-Φ0(θ) + 1
a2
For a lower bound we have for any u ≥ 0,
11	1
a √2π Ey [|y|] + 2
ɪ "2 + 1 = JL + 1
√2πa √π	2 πa 2
Ey Φ 0θ] ≥ 1 + Ey I(∣y∣≤ Ub) (∣y∣-1/2)] + Ey 取㈤ >武力(?) - 2
≥ 2 + φ(uvUb-1/2Ey [I(∣y∣≤ ub)∣y∣]
According to Wikipedia for truncated Gaussians, we know that Ey [y|0 < y < ub] = bΦ(U)-Φ(U)), So
Ey [I(|y| ≤ ub)∣y∣] = Ey [∣y∣l∣y∣ < ub] Pr[∣y∣ < ub] = bφ(0) -φu)2(Φ(u) - φ(0))
Φ(u) - Φ(0)
=2b(O(O) - φ(U)) = √≡=(I - e-u /2) ≡ g(U)b
2π
In addition by the concavity of Φ on [0, +∞) we have
Ub 1 Ub Ub 1 Ub Ub
φ( R ≥ 2 + φI ≥ 2+φH√
Combining the above results we have
≥ 1 +
一 2
Φ(Ub∕a) - 1∕2	1
—Ub—g(u)b ≥ 2 + g(u)φ
Now we can plug in numerical results. Let U = 1, and if b∕a ≤ 1∕2 we get the results in the
Lemma.
Step 3: Bound the agreement between X and X. note that X can have high value of (X, r*), but
this is not useful unless in the final X outputted, a large proportion of the dimensions is equal to X. In
other words, we need to show that λ cannot be too large.
Lemma 5. In Protocol 2, ∀s > 0, λ < SVbd+b with probability 1 - e-s2/2, where b is an upper
bound on kw* k∞∙
□
12
Under review as a conference paper at ICLR 2020
To simplify our analysis, we will assume that d is large enough such that λ ≤ 1/2 with probability
1 - γ∕2.
ProofofLemma 5. We know that ∣∣w*∣∣∞ < b and ∣∣wkι = d, then by Holder inequality have
I∣w*k2 = (w*, w*) ≤ kw*kι∣w* k∞ ≤ bd.
For any choice of s > 0 because the distribution of xλd-.d do not depend on w3d：d，and by symmetry
arguments Xi, i = λd, ∙∙∙ ,d are i.i.d. Bernoulli distributions with zero mean, so they are 1/2
sub-Gaussian. We know that (xλd∙.d, Wsdd) is Pd=λd (wi) ≤ kw2~k2 ≤ bd sub-Gaussian. We can
conclude for any s > 0
pr[(Xid：d, wid：d) ≥ s√bd] ≤ e-s22/	⑺
Because the assumption (without loss of generality) that |w； | ≥ |w/1 ≥ …≥ ∣w3∣ we know that
∣∣w1λdkι ≥ λ∣∣w*kι ≥ λd, so (xLλd, w1λd) = -∣w[λdkι < -λd, so with probability at least
1 - e-s2/2
(X, w) = (XLλd, w1λd) + (Xld:d, wɪd:d) ≤ -λd + s√bd
note that the algorithm will choose the smallest λ such that (X, w) < 0. In particular, it will not
choose a λ such that (x, w) < -b, so we have with probability at least 1-e-s /2, 一λd +s√bd ≥ -b,
or λ ≤ s√d+b.	□
- d
Step 4: Combining the Results. Finally we are in a position to bound ME(X) and prove Theorem 2.
We know that ∀i = 1, ∙∙∙ ,λd, the distribution of W is independent of the choice of Xi, (XLλd, r]λd)
is Piλ=d1 Xi2σ2 ≤ λdσ2 sub-Gaussian, so for any s > 0 we have
Pr[(X±λd, r[λd) ≥ s√λdσ] ≤ e-ss/2	(8)
By Lemma 3 we have
Pr (Xid：d, Md) ≥√^β(1- λ)dσ ≥ 1 - e2β(1-λ)d2π
Combined we have
Pr
..、F2 .	L
(x, r*) ≥ √= β(1 — λ)dσ — s√λdσ
≥ 1 - e2β(1-λ)d2π - e-s222
And if we plug in the above condition that λ < 1/2 (which is true with probability 1 一 γ∕2) we have
Pr (x, r*) ≥ Wβdσ - s√2dσ ≥ 1 - eβd2π - e-s2/
2π
By previous analysis we know β ≥ *T, so we can choose a sufficiently large S and given the choice
of s we can choose a sufficiently large d such that
Pr (x, r*) ≥ σ= σv^dσ ≥ 1 — γ∕2
(,) - 2√π 10τ - Y
So if 7⅛ σ√T dσ ≥ α we have
2 π 10τ
Pr [(X, rs) ≥ α] ≥ 1 - γ
note that ME(X) = 1 whenever (X, rs) + N(0, τ2) ≥ α. Because of the symmetry of the Gaussian
distribution, this happens with 1/2 probability if (X, rs) ≥ α. Therefore we have
Pr[ME(x) = 1] ≥ 2(1 - Y)
We can rewrite the condition as
400πα2n2τ 2	400πα4n2
T ≥ -----------=-------------
—	d2	64d2
13
Under review as a conference paper at ICLR 2020
In particular We can choose T = 20α4n2. This is true if the condition if Lemma 4 is true. That is
τ∕√T ≥ 2σ, or T ≤ T2∕4σ2 = 8^,/3. SUCh a T could exist if
α2n	20a4n2	d2
8 log 2∕δ —	d2	n — 160a2 log2∕δ
Conversely if T = 0, we have E|||ri：T∣∣ι] = v√∏σ. In addition by Lemma 5 for large d, λ → 0, so
When T < d We have (When d is sufficiently large)
Pr
(x,r*) ≥
Pr
kr1：Tkι ≥
→d→∞ 1
We can similarly show that Pr[ME(x) = 1] ≥ ɪ (1 一 Y) if Tσ∕√π ≥ α, which becomes the
condition T ≥ √πα√n. We choose T = 2α√n to satisfy it.
□
A.3 Proofs related to the conditions
Proposition 4 (Representer Theorem with Hard Constraints). The optimal solution
1n
wn , arg min —£log(1 + e-yihw,φ(Xi)i)
kwnk≤	i=1
can be represented as
n
wn =	αiφ(xi),
i=1
where αi is a function of {(xi, yi)}in=1.
Proof of Proposition 4. We first make a critical observation that l(x, y; w) = log(1 + e-yhw,φ(x)i )
is convex w.r.t. w. We can then use the convex duality theory to obtain the following equivalent
optimization problem.
1n
max min — Y^log(1 + e-yi hw,φ(Xi)i) + μ(∣w∣ — 2λ),	μ ≥ 0.
α wn
i=1
Due to the traditional representer theorem, we know that W = PNi ai(μ*)φ(xi), where (w*,μ*)
is a saddle point of the max-min problem. This completes our proof.	□
Proof of Proposition 1. For logistic regression, first we observe that because ∣x∣∞ ≤ 1, it must
be that ∣x∣2	≤	ʌ/d.	Let Σ	^Q	Λ^Q	Where	(Q	is OrthOnomal and Λ is diagonal. Because	^Q	is
orthonormal, ∣ Qx 12 ≤ √d, then
Λ∣2 d = ∣Σ∣2 d
xT Σx ≤ sup yTΛy = sup	Λiiyi2 ≤ sup
y:kyk22=d	y:kyk22=d i	i
now we can bound defensibility by (x, W — w*)〜N(0, XTΣx∕n) and
XT^) ≥ ɑ
n
Pr N 0,
C U ∣∣∑k9 d∖	]	--nɑ^
≤ Pr N(0, '' "2	)≥ α ≤ e 2IPkzd
We can also bound mutual information by
I(z; W | x) = H(z|x) 一 H(z|W, x) = H(N(0, xT Σx∕n + T2)) 一 H(N(0, T2))
= 2log
xT Σx∕n + T2
≤ 1log(l + W) ≤≡d
2	nT2	2nT2
T2
14
Under review as a conference paper at ICLR 2020
We can prove similar results for kernel logistic regression. note that Σ = Λ-1, where
Λ，Ep(x)[p(y = 1 | x)p(y = -1 | x)φ(x)乳 φ(x)] ∈ H ® H.
We will first show that Λ is a bounded operator. This is because for all f ∈ H, we have
kΛf k	=	kE[p(y = 1	| x)p(y = -1	| x)φ(x)hf, φ(x)i]k
≤	E[kp(y = 1	| x)p(y = -1	| x)φ(x)hf, φ(x)ik]
≤	E[p(y = 1 |	x)p(y = -1 |	x) kφ(x)k2 kfk]
≤	E[p(y = 1 |	x)p(y = -1 |	x)] kfk
< kfk
≤ 丁
EquivalentlyJAkop ≤ 4. Due to the Bounded Inverse Theorem, Σ = Λ-1 is also a bounded operator
and therefore kΣkop < ∞. Therefore, for any x ∈ X ,
hφ(x), Σφ(x)i ≤ kφ(x)k kΣφ(x)k ≤ kΣkop.
As a result,
一 (k∑IL∖ ]	nɑ2
Pr[N(0, hφ(x), Σφ(x)i) ≥ αn] ≤ Pr N I 0,----p I ≥ α ≤ e 2悟"叩.
The mutual information can be bounded by
I(z; W | x) = H(z|x) - H(z∣w, x) = H(N(0, hφ(x), Σφ(x)i∕n + τ2)) - H(N(0,τ2))
—1 lc。 hφ(x), *0(x)i/n + T2 < 1 lc/1 , k"kop! < k"kop
= 2log	T2	≤ 2log l + ~τ^-) ≤ 2nτ2
□
Proof of Proposition 2. We know that for any x0 ∈ X
E n12 X(xo, Xi)(x0, Xj)
E n12 X(xo, Xi)2 +E n12 XxTXiXTxo
nE[(xo, x)2] + n12 XXTE[xi]E[Xj]TX0 = 1 E[xTxxTxo] = 1 xTΣxo ≤ d ∣∣∑∣∣2
i6=j
Because E[(xo, r)] = 0, Var[(xo, r)] = E[(xo, r)2] ≤ d ∣∣∑k2,by Bernstein inequality
__________________________________________________________na2_____
Pr[(xo, r) ≥ α] ≤ e 2dk*k2+2/3da
□
Proof of Proposition 3. Proof of this proposition utilizes the fact that Gaussian distributions have
highest entropy among distributions with the same covariance. Let z = (w, x) + N(0, τ2), because
Var[z∣x] = XTΣx + τ2 ≤ dρ(Σ) + τ2, so H(z|x) ≤ 1 log (dρ(Σ) + T2).
1	dρ(Σ) + τ2	dρ(Σ)
I(z； w|x) = H(z|x) - H(z∣x,w) ≤ glog--τ2---- ≤ 2τ2
□
15
Under review as a conference paper at ICLR 2020
Proof of Lemma 1. Let us denote l(x, y; w) = log(1 + e-y(w,x)). In order to learn w, we minimize
the negative likelihood function Pnl(x, y; w), and denote wn , arg min Pnl(x, y; w). We will first
prove that wn →p w. As the first step, we prove the uniform law of large numbers of l(x, y; w).
notice that
E[ sup IPnl(X, y; w) — Pl(X, y; w)I]
kwk≤2λ
=E[ sup IE[Pnl(X, y; wn) — Pnl(X0, y0; w)]I]
kwk≤2λ
≤E[ sup IPnl(X, y; w) — l(X0, y0; w)I]
=E
kwk≤2λ
1
—sup
n kwk≤2λ
2
≤-E
n
(i)	4
≤-E
n
(ii)	8λ
≤ 一E
sup
kwk≤2λ
sup
kwk≤2λ
n
i(l(Xi, yi; w) — l(X0i, yi0; w*))
i=1
n
il(Xi, yi; w)
i=1
n
i(w, Xi)
i=1
sup
kwk≤2λ
n
(w,	iXi)
i=1
n
iXi
i=1
(iii)	8λ u
≤——
n
EIIX eixi
4 E
n
2
8λ
where (i) is due to the Ledoux-Talagrand contraction theorem, (ii) is due to Cauchy-Schwarz, and
(iii) is due to Jensen’s inequality. Using Markov’s inequality, we can conclude that with probability
1 - δ,
8λ
sup IPnl(X,y; W) — Pl(χ,y; w)| ≤ ~^∙
kwk≤2λ	nδ
Therefore, with probability 1 — δ, Pl(X, y; wn) — P l(X, y; w*) can be bounded as
0 ≤ P l(X, y; wn) —Pl(X,y;w*)
= Pl(X, y; wn) — Pnl(X, y; wn) + Pnl(X, y; wn) — Pnl(X, y; w*) + Pnl(X, y; w*) — P l(X, y; w*)
16λ
≤ -^τ.
Furthermore, the smoothness of Pl(X, y; w) guarantees that when kwn — w* k is small enough,
P l(X, y; wn) —Pl(X,y;w*) = VP l(X, y; w*)|(wn —w*)
+ 2(wn — w*)lV2Pl(x, y; w*)(wn — w*) + o(kwn — w*k)
≥ 4λmin(V2Pl(x,y; w*)) ∣∣wn — w*k2
(i) 1	2
≥ 4λmin(Cov[VPl(x, y; w*)]) ∣∣wn — w*k2 ,
where (i) is due to Bartlett’s identity, and λmin denotes the smallest eigenvalue. Combining the fact
that Pl(X, y; wn) →p Pl(X, y; w*) and P l(X, y; wn) — Pl(X, y; w*) ≥ O(∣wn — w*∣2), we can
conclude that with probability 1 — δ, there exists a large enough n such that ∀n ≥ n :
kwn —w*k2
64λ
≤ √nδλmin(Cov[VPl(x,y; w*)]),
and as a simple corollary,
wn →p w* .
16
Under review as a conference paper at ICLR 2020
Because Wn minimizes Pnl(X,y; w),we have that whenever ∣∣Wn - w*k < λ, VPnl(x, y; Wn) =0.
This is equivalent to saying that √nVPnl(x, y; Wn) →→ 0. Using Taylor expansion, We obtain
VPnl(X,y; W*) + V2Pnl(x,y; W*)(Wn - w*) + 0p(1)(Wn - w*) = VPnl(X,y; Wn) = Op( √ ),
which is equivalent to
√n(PV2l(x,y; W*) + (Pn - P)V2l(x,y; w*) + 0p(1))(Wn - w*) = -√nPnVl(x,y; w*) + Op(1).
Combining the central limit theorem and Slutsky’s theorem, we have
√n(wn - w*) →d N(0; E[V2l(x, y; w*)]-1 Cov[Vl(x, y; w*)]E[V2l(x, y; w*)]-1).
Bartlett’s identity asserts that E[V2l(x, y; w*)] = Cov[Vl(x, y; w*)]. Therefore, the asymptotic
variance of √n(wn - w*) can be simplified to
Σ , Cov[Vl(x, y; w*)]-1 = E
xx|
(1 + ey(W*,X))2
-1
e(W*,X)
------7----7——xxl
(1 + e(W*,X))2
(EX[pW* (1 -pW*)xx|])-1
where pW*
1
1+e-(w*,χ).
□
Lemma 6. Assume that there exists w* ∈ H such that p*(y | x) = i+exp(_y；w* g(x)))∙ Assume
further that ∣∣w* ∣ ≤ λ, and ∀x : k(x, x) ≤ L Suppose we are given a dataset {(xi, yi)}n=ι L吟
p*(x)p*(y | x). Let Wn be the solution to thefollowing objective
1n
min - 5"^log(1 + e-yihw,φ(Xi)i).
kWk≤2λ n
i=1
Then we have wn →p w* , and
√n(Wn - W*) → N(0, ∑h),
where
∑h = (Eχ[p(y = 11 x)(i -p(y = 11 x))φ(x)乳 Φ(x)])-1.
ProofofLemma 6. Let us denote l(x, y; w) =log(1 + e-yhw,φ(X)i), Ln(w) = 1 Pn=ι l(xi, yi； w),
and L(w) = E[l(x, y; w)]. Now, we prove the uniform convergence of Ln to L. Denote B ,
{w| ∣w∣H ≤ 2λ}, and consider the expected error
E sup |Ln(w) — L(w)∣ = E
w∈B
=E
≤E
sup |Ln(w) - E[l(x, y; w)]|
w∈B
sup
w∈B
1n
Zl(Xi,yi, w) - E
n
i=1
n X l(xi,y0;w)』
1n
sup - Vei[l(xi,yi; w) - l(xi,yi; w)]
w∈Bn
n
il(xi, yi; w)
i=1
≤ 2E sup 1
w∈Bn
= 2R[l(x, y; w)]
Now let X(w) ， n Pn=ι Eil(Xi,yi; w). note that l(x, y; w) is a 1-Lipschitz function w.r.t.
yhw, φ(x)i. Using the Ledoux-Talagrand contraction theorem, we conclude that R(l(x, y; w)) ≤
17
Under review as a conference paper at ICLR 2020
2R(yhw, φ(x)i), which can be bounded by noticing that
R(yhw, φ(x)i) = E
≤E
sup
w∈B
sup
w∈B
1n
~y>Z^y( hw,Φ(xi)i
n
i=1
1n
hw, n E∙¾yiφ(xi)i
kwkH
i=1
1n
n
i=1
E
H
λ2
-2 E
n2
≤
t
nn
ΣΣ ijyiyj k(xi , xj )
i=1 j=1
√λ2
=---，
n \
λ
≤『
n
k(xi, xi)
i=1
E
where we assume that k(x, x) ≤ 1, which is satisfied by many universal kernels, such as Gaussian
RBF. Taken together, we have
ʌ
E sup |Ln(w) — L(w)∣ ≤ 4
w∈B
With Markov inequality, we have ∀δ > 0,
λ
n
4λ
Pr[sup |Ln(w) — L(w)∣ > δ] ≤ 仄^n
The uniform convergence concludes that L(Wn) → L(w*). Using the same arguments as in the
proof of Lemma 1, We conclude that Wn → w*.
next, we prove the asymptotic normality of Wn. First, we note that l(x, y; w) is Frechet differentiable
w.r.t. w. This can be verified by the chain rule OfFreChet derivatives, because l(x, y; w) is FreChet
differentiable w.r.t. hw, φ(x)i, and hw, φ(x)i is Frechet differentiable w.r.t. w. Then, we need to
focus on the differentiability of
Vw l(x,y; W)
—
yhφ(X), ∙i
1 + ey〈W,。(X)i .
Let g(w)，VWl(x, y; w) : H → H. We can further take the Gateaux derivative of g(w) to get
dg(W0 + t(W - W0))
dt
eyhWO+t(W-WO),φ(X)i
(1 + ey(Wo + t(W-Wo),φ(x)i)2 hw - w0, φ(X)ihφ(X), ^i
and even compute higher-order Gateaux derivatives
d2g(w0 + t(w — wo))	eyhwo+t(W-WO),φ(X)i (eyhw0+t(W-WO),φ(X)i —1)
dt2
—
hw — W0,φ(x)i2hφ(x), .).
Using fundamental theorem of calculus for Gateaux derivatives, we obtain
g(w) = g(w0) +
1 dg(w0 + t(w - w0))
g(w0) +
0
Z0
dt
1 dg(w0 + t(w - w0))
dt
dt
dt +
t=0
2g(w0 + t(w - w0))
dt2
dudv.
t=u
18
Under review as a conference paper at ICLR 2020
note that
ZQ	ZQ	d	g(w0	+；2w_w0))	dudv	≤ 20	∣∣hw - wo,φ(X)i2hφ(X),∙i∣∣ =	o(kw - w*ID,
we have
eyhw0,φ(x)i
g(W) = g(wO) + (1 + eyhw0φ(x)i )2 φ(X)区 φ(X)(W - w0) + Op(I)(W - WO).
Using the same arguments as in the proof of Lemma 1, We can assert that g(wn) = op( √1n). Now
we can expand L(Wn) in the neighborhood of w* to get
VnPnVwl(x, y; Wn) = Op(1)
e	eyhw*,φ(X)i	∖
=√nPnvwl(x,y； w*)+(2 g + eyhw*,φ(x)i)2 φ(X)% φ(X) + Op(I)J (w — w*)
= √nPnVw l(X,y; W*)
e	eyhw*,φ(X)i	eyhw*,φ(X)i	∖
+ Fn (P (1 + ey(w*,φ(x)i)2 Φ(X)乳 Φ(X) + (Pn - P)(1 + ey(w*,φ(x)i)2 0(x)氧 °(x) + Op(I)I(W - W*)
e	eyhw*,φ(X)i	∖
=√nPnvwI(X, y; W*) + √n pP(1 + eyhw*,φ(X)i)2 φ(X)乳 φ(X) + Op(I)) (w - w*),
where (i) is due to the law of large numbers. Because of∀X : k(X, X) ≤ 1, the (weak) law of large
numbers and the central limit theorem still holds for random variables in H. For the former, we can
prove using McDiarmid’s inequality, symmetrization, and Jensen’s inequality. For the latter, the
proof is the same as the traditional proof of CLT using characteristic functions. Similarly, Slutsky's
theorem still holds. As a result, the asymptotic distribution of √n(wn - w*) is again Gaussian, with
mean 0 and variance
∑ = (Ex[Pw*(1 -Pw*)φ(x) 0 φ(x)])-1,
where
…	_	1
Pw* = 1 + e-hw*,φ(X)i
□
A.3.1 Additional Examples and Proofs for Section 4
An example where plearn (w) = N (w*, σ2I) Let Berd(p) be the d dimensional Bernoulli distri-
bution where each dimension independently takes 1 with probability p and -1 with probability
1 - p.
Let p*(x, y) be defined by p(y) = Ber1 (1/2) and p(x|y = 1) = Berd(3/4) and p(x| = 0) =
Berd(I/4). We choose w* = (∣, ∣, •…,∣) and the ground truth score function is gw* (x) = (x,w*).
We draw a dataset of size n from the distribution {x1, y1, •…，Xn, yn1}, to learn W the algorithm
Pi:y =1
simply takes the average ~P~y—
i:yi =1
Xi
—:— —
Pi：yi = -1
—
1 Pi-1
Xi
1.
Then by CLT we have plearn (w) = N (w*, σ2I)
where σ2 = 3/4n. note that it is possible to obtain non-asymptotic results because w is actually
distributed as a multinomial, but we will use the convenience of a Gaussian distribution.
Proof of no-response rate. When α = √d and T = 0 we have
Pr [NR(x)] ≤ Pr [∣(x,w)∣ ≤ α] ≤ Pr h∣(x,r*)∣ ≤ d/2 — √d]
p* (x)
=Pr h∣N (0, dσ2) I ≤ d/2 - √di →d→∞ 0
Similarly forα= √d and T = 2总22/6 we have
Pr [NR(x)] ≤ Pr [∣ (x, w) + N(0, τ2) ∣ ≤ α] ≤ Pr [∣ (x, r*) + N(0, τ2) ∣ ≤ d/2 — √d]
Pr N 0, dσ2 +
2logd2∕δ)) ∣ ≤ d/2 - √d] →d→∞ 0
19