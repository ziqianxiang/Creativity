Under review as a conference paper at ICLR 2020
Learning Curves for Deep Neural Networks:
A Field Theory Perspective
Anonymous authors
Paper under double-blind review
Ab stract
A series of recent works established a rigorous correspondence between very
wide deep neural networks (DNNs), trained in a particular manner, and noiseless
Bayesian Inference with a certain Gaussian Process (GP) known as the Neural
Tangent Kernel (NTK). Here we leverage this correspondence to provide explicit
analytical predictions for the learning curves of DNNs on any fixed target func-
tion. Focusing on datasets whose input measure is uniform on a hypersphere,
our predictions match experimental curves very well and reveal a strong implicit
bias towards functions which are low order polynomials of the input. Two novel
analytical tools underlay our approach. First a renormalization-group approach is
used to show that noiseless GP inference using NTK, which lacks a good analytical
handle, can be well approximated by noisy GP inference on a related kernel we
call the renormalized NTK. Second a field theory reformulation of the problem
allows a controlled perturbative expansion of the test error in the inverse dataset
size. We believe our approach lays the foundations for a more precise quantitative
understanding of DNNs.
1	Introduction
Several pleasant features underlay the success of deep learning: The scarcity of bad minima encoun-
tered in their optimization (Draxler et al., 2018; Choromanska et al., 2014), their ability to generalize
well despite being heavily over-parameterized (Neyshabur et al., 2018; 2014) and expressive (Zhang
et al., 2016), and their ability to generate internal representations which generalize across different
domains and tasks (Yosinski et al., 2014; Sermanet et al., 2013).
Due to the complexity of DNNs our current understanding of these features is still largely empirical.
Notwithstanding, progress has been made recently in the highly over-parametrized regime (Daniely
et al., 2016; Jacot et al., 2018) due to the fact that the networks’ parameters, in all non-linear layers,
change in a minor yet important manner during training. This facilitated the derivation of various
bounds (Allen-Zhu et al., 2018; Cao & Gu, 2019b;a) on generalization and, more relevant for this
work, the following correspondence with GPs: Considering finite-depth DNNs which are much wider
than the dataset-size, trained with MSE loss, no weight decay, and at vanishing learning rate (the
NTK-regime) one finds that the initialization-averaged predictions are the same as those of Gaussian
Processes Regression (GPR) with a kernel known as the NTK. Several subsequent works corroborated
these results empirically (Lee et al., 2018; Lee et al., 2019; Arora et al., 2019) and extended them
(Arora et al., 2019). For fully-connected DNNs, the NTK-regime (and GPs associated with DNNs in
general (Lee et al., 2018; Novak et al., 2018)) seems to faithfully capture the generalization power of
DNNs trained with MSE loss (Lee et al., 2019).
One of the most detailed objects quantifying generalization are learning-curves: graphs of how the
test error diminishes with the number of datapoints (N). There are currently no analytical predictions
or bounds we are aware of for DNN learning-curves which are tight even just in terms of their
scaling with N, let alone tight in an absolute sense. In contrast, the theory of GPR is rich with
analytical tools which have yielded in the past high accuracy predictions for learning curves. One of
the most transparent ones is the equivalence kernel (EK) (Rasmussen & Williams, 2005) which will
be introduced in section 3. In short, EK gives an intuitive functional prediction of the expected GPR
predictor of a fixed target function when averaged over datasets of size N , and consequently of the
learning curves.
1
Under review as a conference paper at ICLR 2020
Clearly such a detailed understanding of generalization in DNNs is desirable. However, several
technical issues prohibit the application of the EK and related results (Rasmussen & Williams, 2005;
Malzahn & Opper, 2001) to DNNs trained in the NTK-regime. First, the NTK-regime corresponds to
noiseless GPR where the DNN and corresponding GP both fit the training dataset exactly, whereas
EK assumes Gaussian measurement noise on the training labels. In the case of noiseless GPR, EK
and related approximations break down and predict perfect generalization with very small datasets.
Second, the eigenfunctions (features) and eigenvalues of the NTK are needed so that the EK can be
interpreted. Third, as EK results can be misleading, it is important to estimate the validity range of
these approaches and, in a related manner, derive sub-leading corrections.
In this work we make the following contributions:
I We establish that noiseless GP inference using NTK can be well approximated by noisy GP
inference on a related set of kernels we dub the renormalized NTKs.
II We obtain closed expression for the leading and sub-leading asymptotics of learning curves for
any fixed target function (fixed-teacher learning curves) using an extension of the field-theory
formalism of Malzahn & Opper (2001) for GPR.
III For uniform datasets, where the input is distributed uniformly on a hypersphere, these expression
simplify considerably and, together with the features and eigenvalues we obtain for the renor-
malized NTKs, lead to clear relations between deep fully-connected networks and polynomial
regression.
IV Again for uniform datasets, we provide analytical predictions for learning curves along with
estimates for their range of validity. We believe our learning curves estimates stand-out in terms
of accuracy as they get to within 3% of experimental generalization values. We emphasize that
our predictions can be applied to fully-connected DNNs of any depth, trained in the NTK regime,
and that our approach provides a systematic way of making further accuracy improvements.
2	Prior works
Learning curves for GPs have been analyzed using a variety of techniques (see Rasmussen & Williams
(2005) for a review) most of which focus on a GP-teacher averaged case where the target/teacher is
drawn from the same GP used for inference (matched priors) and is furthermore averaged over. Fixed-
teacher or fixed-target learning curves have been analyzed using a similar grand-canonical/Poisson-
averaged approach (Malzahn & Opper, 2001) as our, however, the treatment of the resulting partition
function was variational whereas we take a perturbation-theory approach. In addition previous cited
results for MSE-loss breakdown in the noiseless limit (Malzahn & Opper, 2001). To the best of
our knowledge, noiseless GPs learning-curves have been analyzed analytically only in the teacher-
averaged case and in the following settings: For matched priors, exact results are known for one
dimensional data (Williams & Vivarelli, 2000; Rasmussen & Williams, 2005) and two dimensional
data with some limitations of how one samples the inputs (in the context of optimal design) (Ritter,
2007; 1996). In addition Micchelli & Wahba (1979) derived a lower bound on generalization. For
noiseless inference with partially mismatched-priors (matching features, mismatching eigenvalues)
and at large input dimension the teacher and dataset averaging involved in obtained learning curves
has been performed analytically and the resulting matrix traces analyzed numerically Sollich (2001).
Notably none of these cited results apply in any straightforward manner in the NTK-regime.
Considering kernel eigenvalues, explicit expression for the features and eigenvalues of dot-product
kernels (K = K(X ∙ x0)) were given in Azevedo & Menegatto (2015). The fact that the l-th eigenvalue
of such kernels scales as d-l (d being the input dimension), which we used in our derivation of the
bound, has been noticed in Sollich (2001). Kernels with a trimmed spectrum where the spectrum is
trimmed after the first r’s leading eigenvalues, has previously been suggested as a way of reducing
the computational cost of GP inference (Ferrari-Trecate et al., 1998). In contrast we trim the Taylor
expansion of the kernel function rather than the spectrum (which has a very different effect) and show
that an effective observation noise compensates for our trimming/renormalization procedure.
Several interesting recent works give bounds on generalization (Allen-Zhu et al., 2018; Cao & Gu,
2019b;a) which show O(1∕√N) asymptotic decay of the learning-curve (at best). In contrast our
predictions are typically well below this bound.
2
Under review as a conference paper at ICLR 2020
3	Field Theory Formulation of GP learning-curves
3.1	GPR Definition and Problem Statement
We begin with standard definitions of GPs and Bayesian Inference on GPs. A GP is defined as
a stochastic process of which any finite subset of random variables follow a multivariate normal
distribution. In a similar fashion to multivariate normal variables, GPs are also determined by
their first and second moments. The first is typically taken to be zero, and second is known as the
covariance function or the kernel Kχχo = E[f (x)f (x0)], where E[∙] here denotes expectation under
the GP distribution. The main appeal of GPs is that Bayesian Inference with GP priors is tractable. In
GPR we use the posterior mean as the predictor g? , and it is given by:
N
g?(x?) = X Kx?,xn[K(D) + σ2I]n-m1 gm	(1)
n,m=1
where x? is a new datapoint, gm are the training targets, xn are the training data-points, [K(D)]nm =
Kxn,xm is the covariance-matrix (the covariance-function projected on the training dataset D), and
σ2 is the variance of the assumed Gaussian noise of the labels, which also acts as a regulator of the
prediction. Some intuition for this formula can be gained by verifying that in the noiseless case
(σ2 = 0) the prediction at some training point x? = xq coincides with that point’s label g? = gq .
The quantity of interest in this paper will be the dataset averaged generalization error which we define
now. Throughout this paper we will assume that both train and test points are i.i.d. random variables
drawn from a probability measure μ(χ). With this in mind we define the generalization error of a
prediction g? or MSE loss as
kg(X?) - g?(X?)k2 = / dμχ? (g(x?) - g?(X?))2	⑵
Note that g? is itself a function of N draws from μ which make up the training set DN. The dataset
averaged generalization error is (2) averaged over the ensemble of all possible N sized training sets.
We denote this average as h)dn , so for instance the dataset averaged generalization error would be
hkg? - gk2iDN. We see that in order to calculate learning-curves, one needs to average quantities
like g? and g?2 . These averages turn out to be difficult to handle analytically, so to facilitate their
computation we adopt the approach of Malzahn & Opper (2001) and instead consider a related
quantity given by the Poisson averaging of the former one
∞n
<…iη = e-η X η!〈…〉。“	(3)
n=0
where ... can be any quantity, in particular g? and g?2 . Borrowing jargon from physics we refer to the
original data ensemble as the canonical ensemble and to the above as the grand-canonical ensemble.
Taking η = N, means we are essentially averaging over values of N in an √N vicinity of N. This
means that as far as the leading asymptotic behavior is concerned, one can safely exchange N and η
as the differences would be sub-leading. In App. A we compare learning curves as a function of N
and η and show that they match very well.
Equation (1) determines the predictions, and therefore the learning-curves, but it is not very convenient
for analytic exploration of the expected predictions. This fact is due to the (potentially very) large
matrix inversion involved, and the additional averaging over DN required. Nonetheless there are
some results for the expected prediction g? , the most famous of which is the equivalence kernel (EK)
result for the prediction of a fixed target function as a function of the training set size N (Rasmussen
& Williams, 2005):
gE? K,N (X) = X λn⅛> gnφn(x)	(4)
3
Under review as a conference paper at ICLR 2020
Where λn and φn (x) here are the eigenvalues and eigenfunctions of the kernel w.r.t the input
probability measure (μ) and g(χ) = Pn gnφn(χ) is the target. One notices immediately that the
prediction breaks down completely in the noiseless case where (4) implies perfect estimation of the
target with just one datapoint. To gain some intuition as to why having σ2 = 0 hinders predictions
of hg?iDN one can view it as a hard constraint (f(xn) = g(xn)), and hard constraints are typically
less tractable than soft ones. Indeed this can also be seen as a motivation for considering the above
Grand-canonical dataset ensemble. In a related view, finite σ2 can be seen as a form of averaging
which smooths and regulates analytical expressions making them more tractable. Another limitation
of the EK result is that (to the best of our knowledge) there is no systematic way to extend it in orders
of 1/N and get a more detailed picture of GPR generalization.
3.2	Field Theory Formulation
To facilitate the analysis of Eq. (1) we formulate the problem from a statistical-field-theory/path-
integral viewpoint (Schulman, 1996). These are well-studied, powerful approaches for performing
integrations over a space of functions (the jargon is ”paths” when x in one dimensional and ”fields”
when x is higher dimensional). This section is somewhat technical and the reader who is not interested
in derivations is invited to skip to section 3.3. To get some familiarity with this formalism, consider
first averages over the (centered) GP itself with no dataset. Using the path-integral formalism we
define the probability density functional of a GP with kernel K:
P0[f]
exp (—1 R dxdx0f(x)K-1(x, x0)f(x0))
J DfexP (-1 J dxdx0 f(x)K-1(x, x0)f(x0)}
(5)
where Df denotes integration over the space of (well behaved) functions, for concreteness we
limit dx0 to some compact domain such as the hyper-sphere, K-1(x, x0) is the inverse covariance
function ( dx0K(x, x0)K-1(x0, x00) = δ(x - x00)). P0 is analogous to a PDF with the sample space
being the set of real functions. To define the path-integrals one first chooses an orthonormal basis
of functions φi(x) (with respect to dx) arranged in order of likeliness P0[φi] ≥ P0[φj] for i > j
(note that this comparison doesn’t require calculating the path integral in (5)). Second, one expands
f = Pi fiφi(x), and defines the path-integral as a series of simple integrals
DfF [f]
df1	df2 . . . F
fiφi
i
(6)
where F is some functional of f (like P0 in (5)). Finally, one makes this last expression well-defined
by taking a limit procedure where the number of integrals is gradually taken to infinity (Schulman,
1996).
Performing the above procedure we show in App. E that a stochastic process with functional
probability density P0 is equivalent to the original GP. Following a similar procedure, and denoting
kf k2K = R dxdx0f(x)K-1(x, x0)f(x0) one can show an alternative representation of (1)
g?(x?) = ZT Z Df ∙ f(x?) ∙ exp --1 kf kK - 2∣2 XX (f 的)-gn)j	⑺
Z = Z Df exp (-2 kfkK -	X(f(xn)- gn)j
where Z is known as the partition function (see Rasmussen & Williams (2005) for equivalence of (1)
and (7)). It is useful to define the partition function with a ”source term”:
z[α(x)] = / Df eχp(-1 kfkK +
dxα(x)f (x) -
(8)
1N
2σ2 Xf(Xn)- gn产
n=1
4
Under review as a conference paper at ICLR 2020
Then the GPR prediction is simply
g?(x?)
∂ log(Z[α(x?)])
dα(X?)	α(x*) = 0
(9)
where 就* = δ(x - x?). This form makes it clear that We Can get quantities like dataset averaged
predictions by finding the average of log Z . By using the grand canonical ensemble, averaging over
draws from the dataset can be carried using the ”replica trick” (see for instance (Gardner & Derrida,
1988)), which aids in averaging over expressions like log(Z) and their derivatives via the equality
log(Z) =m→o Z-M-1. Specifically:
hg*(x*)iη = (Mm0 d⅛J ZM-I L(χ?)=o) η
(10)
MM
hZMin =	Y Dfm exp - j X kfmkK + η	dμχ"212 P-=1(fm(X)-g(X))2
m=1	m=1
where, as standard in the replica formalism, the computation should be carried at positive integer M
and the analytical result extrapolated to zero at the end. Neatly, a Taylor expansion in η of the above
r.h.s. yields the h...iη averaging appearing on the l.h.s.
The main benefit of (10) over (1) is that it allows for a controlled expansion in 1∕η. At large η
(or similarly large N) we expect the fluctuations in fm (x) to be small and centered around g(x).
Indeed such a behavior is encouraged by the term multiplied by η in the exponent. We can therefore
systematically Taylor expand the inner exponent
P-=Ifm(x)-g(x))2
e	2σ2
PM=1(fm(x) - g(x))2
2σ2
2
+ ... (11)
and each term will yield a higher order ofhg?(x?)》n in 1∕η. The first order ofthe systematic expansion
we defined can be dealt with in an exact manner, in which case we recovers the aforementioned EK
result (4) with the slight difference that N gets replaced with η (see App. F for the computation). The
second order term and further terms render the theory non-Gaussian and cannot be dealt with exactly
but rather through the use of perturbation-theory/Feynman-diagrams.
3.3	Field Theory Formulation Predictions
Performing the calculation described in the previous section we obtain the following closed expression
for large N or η behavior of noisy GPR using any kernel, in particular kernels associated with deep
fully-connected networks and convolutional networks.
hg*(x*)in = gEκ,n (x?) + gSL,n(x*) + O(I∕η3)
(12)
gsL,n(X?)=σ x	λ,+σ2	(λ^+σ)	(λ⅛+σ)	§曲	(x?)	Z d〃x。，(X) % (X)“(X)
The first term is the familiar EK results but with N replaced by η. The second term (gs? L,η) is a
sub-leading correction which captures the behavior at lower N’s.
As shown App. F similar expressions for hg*2>n are obtained using two replica indices. Interestingly
we find that hg?2in =(g?)/ + O(1∕η3). Hence the averaged MSE error is simply (〈g?(X?))n -
g(X?))2 integrated over X?. Since the variance of g? came out to be O(1∕η3) one finds that g? - g,
which is O(1∕η), is asymptotically much larger than its standard deviation. This implies self averaging
at large η, or equivalently that our dataset-averaged results capture the behavior of a single fixed
dataset.
5
Under review as a conference paper at ICLR 2020
Equation (12) and the average MSE error are one of our key results. They provides us with closed
expressions for the dataset-averaged MSE loss as a function of η namely, the fixed-teacher learning
curve. They hold without any limitations on the dataset or the kernel and yield a variant of the EK
result along with its sub-leading correction. From an analytic perspective, once λi and φi (x) are
known, the above expressions provide clear insights to how well the GP learns each feature and what
unwanted cross-talk is generated between features due to the second sub-leading term. Notably for
the renormalized NTK introduced below, the number of non-zero λi ’s is finite, and so the above
infinite summations reduce to finite ones. This makes these expressions computationally superior to
directly performing the matrix-inversion in (1) along with an N -dimensional integral involved in
dataset-averaging. In addition having the sub-leading correction allows us to estimate the range of
validity of our approximation by comparing the sub-leading and leading contributions, as we shall do
for the uniform case below.
4 Uniform datasets
To make the result (12) interpretable, φi(x) and λi are required. This can be done most readily for
the case of datasets normalized to the hypersphere (kxnk = 1) with a uniform probability measure
and rotation-symmetric kernel functions. By the latter we mean Kx,x0 = KOx,Ox0 for any O, where
O is an orthogonal matrix over the space of inputs. Although beyond the scope of the current work
obvious extensions to consider are datasets which are uniform only in a sub-space of x and/or small
perturbations to uniformity.
Importantly, the NTK associated with any DNN with a fully connected first layer and weights
initialized from a normal distribution, has the above symmetry under rotations. This follows from
the recursion relations defining the NTK (Jacot et al., 2018) along with fact that the kernel of the
first fully-connected layer is only a function of X ∙ χ0. It follows that the NTK can be expanded
as Kχ,χo = Pn bn(x ∙ χ0)n. An additional corollary (Azevedo & Menegatto, 2015) is that its
features are hyperspherical harmonics (Ylm(x)) as these are the features of all dot product kernels.
Hyperspherical harmonics are a complete (and orthonormal w.r.t a uniform measure) basis for
functions on the hypersphere. For each l these can be written as a sum of polynomials in the input
coordinates of degree l. The extra index m enumerates an orthogonal set of such polynomials (of
size deg(l)). 1 For a kernel of the above form the eigenvalues are independent of m and given by
(Azevedo & Menegatto, 2015)
r(d) Xb (2s + l)! r(s + 1)
√∏ ∙ 2ls⅛2s+l (2s)!	Γ (S + l + d)
(13)
For ReLU and erf activations, the bn ’s, can be obtained analytically up to any desirable order.
Thus one can semi-analytically obtain the NTK eigenvalues up to any desired accuracy. For the
particular case of depth 2 ReLU networks, we report in the App. H closed expression where the above
summation can be carried out analytically. However as we shall argue soon, it is in fact desirable to
trim the NTK in the sense of cutting-off its Taylor expansion at some order m, resulting in what we
call the renormalized NTK. For such kernels, which would be our main focus next, the above result
can be seen as a closed analytical expression for the eigenvalues.
Interestingly, for any fully-connected network and uniform datasets of dimension d on the hypersphere,
there is a universal bound given by λl ≤ K/deg(l) ≈ O(d-l), where K is Kx,x which is a constant
in x. Indeed note that Kx,x is finite and therefore its integral over the hypersphere is also finite and
given by R dμχKχ,x = Kχ,x = Plm λι = Pl deg(l)λι. The degeneracy (deg(l)) is fixed from
properties of hyper spherical harmonics, and equals deg(l)= 飞：：—(l+d-2) (Frye & Efthimiou,
2012) which goes as O(dl) for l《d. This combined with the positivity of the λl's implies the above
bound.
1Note that usually the hyperspherical harmonics are normalized w.r.t Lebesgue measure on the hypersphere,
but in this context the normalization is w.r.t a probability measure on the hypersphere.
6
Under review as a conference paper at ICLR 2020
Expressing our target in this feature basis g(x) =	l,m glmYlm (x) (12) simplifies to
g? = gE? K,η (x?) -
l,m
η ICK,σ2∕η	λl
λι + σ2∕η λι + σ2∕η
glm Ylm (x? )
(14)
where CK,σ2∕η = Plm(λl-1 + η∕σ2)-1 and notably cross-talk between features has been eliminated
at this order since Pm φim(x)2 is constant yielding Pm R dμχφim(x)φιomo(x)φ2m(x) H δuoδmm0.
By splitting the sum, CK,σ2∕η , to cases in which λl < σ2∕η and their complement one has the bound
CK,σ2∕η < #FOTn + Plmg>σ2∕η λl, where #F is the number of non-zero kernel eigenvalues.
Thus for kernels with a finite number of non-zero λi ’s, as the renormalized NTK introduced below,
CK,σ2∕η has a η-1 asymptotic. This illustrates the fact the above terms are arranged by their orders
in η.
Taking the leading order term one obtains the aforementioned EK result with N replaced by η.
Equating the two contributions provides an estimate of when perturbation theory breaks down.
Focusing on λl > σ2 ∕η, the perturbation theory appears valid when CK,σ2∕η σ2 . In the limit
σ2 → 0, and for trimmed kernels, this yield #F η. Notably it means that the original non-trimmed
NTK cannot be analyzed perturbatively in the noiseless limit. In the next section we tackle this issue.
5	Generalization in the noiseless case and the renormalized NTK
The correspondence between DNNs trained in the NTK regime and GPR using NTK implies noiseless
GPR (σ 2 = 0) for which the perturbative analysis carried in previous sections fails. Here we show
that the fluctuations of fm(χ) associated with low λ's can be traded for noise on the fluctuations
of fm(χ) associated with high λ's thereby making our perturbative analysis applicable. As shown
in the previous section, the lower λ,s correspond to higher spherical harmonics and hence have
higher Fourier components. We argue that these higher Fourier modes can be marginalized over in a
controlled manner to generate both noise and corrections to the high λ's. This is very much in spirit of
the renormalization group technique common in physics wherein high Fourier modes are integrated
over to generate changes (renormalization) of some parameters in the probability distribution of the
low Fourier modes.
We begin by defining a set of renormalized NTKs. As argued, an NTK of any fully-connected DNN
can be expanded as KxH = P∞=0 bq(X ∙ χ0)q. The renormalized NTK at scale r is then simply
Kxrxo = PI=。bq (x ∙ x0 )q. Harmoniously with this notation We denote the prediction of GPR with
the original kernel as g∞? . Our claim is that GPR with K and a noise of σ2 can be well approximated
by GPR with K(r) and noise σ2 + σr2 (where σr2 = Pq∞=r+1 bq), for sufficiently large r. Specifically
that the discrepancy between GPR predictions scales as O(√Nd-(r+1"2∕Kxι,xι), where d is the
effective data-input dimension. As can be inferred from (13), the renormalized NTK at scale r has
zero eigenvalues for all spherical Harmonics with l > r. Thus, as advertised, these high Fourier
modes have been removed from the problem. In a related manner trimming the Taylor expansion
after (x ∙ x0)r effectively reduces our angular resolution and coarse grains the fine angular features
captured by these spherical Harmonics with l > r.
To justify this approximation we consider the difference matrix Kx(rn),xm - Kxn,xm, first for n 6= m
and Xn's drawn for a uniform distribution of a hypersphere of dimension d. The terms bq (Xn ∙ Xm)q
scale roughly as dq∕2 (see App. H for a more accurate expression) due to the tendency of random
vectors in high dimensions to be orthogonal. Consequently the above difference diminishes very
quickly with r. Notably this also applies for Kxr)Xm 一 Kx*,xn provided x* is a test point and
not a train point. In contrast the diagonal part of the different matrix is simply σr2δn,m and may
diminish more slowly depending on the coefficients bq>r. Upon neglecting Kx(r*),xm 一 Kx*,xn and
the off-diagonal elements of the difference matrix, one finds that (1) with these two GPRs yields
identical predictions. As shown in App. H, these neglected off-diagonal elements yield a discrepancy
which scales as √Nd-(r+1)/2 (since they sum incoherently). Consequently the MSE error between
the two GPRs should scale asN. This scaling with N should saturate when the accuracy is nearly
perfect since then the predictions remain largely constant as N is increased.
7
Under review as a conference paper at ICLR 2020
Focusing back on the question of how to tackle noiseless GPR, we thus find that as long as the bq’s
decays slowly enough with q, then at any finite N we can choose a large enough r such that two
desirable properties are maintained: A. The discrepancy between the GPRs is small and B. σr2 * is
large enough to ensure convergence to our perturbative analysis. The required slow decay of bq is
harmonious with the intuition that DNNs should be initialized at the edge of chaos (Schoenholz et al.,
2016) where the output of the network has a fine and multi-scale sensitivity to small changes in the
input. As Kx,x0 is the correlation of two outputs with inputs x and x0, having a power law decaying
bq implies such fine and multi-scale sensitivity. Establishing relations between good initialization and
effectiveness of our renormalized NTK is left for future work.
We have tested the accuracy of approximating noiseless NTK GPR with renormalized NTK GPR
with the appropriate σr2, both on artificial datasets (see next section) and on real world dataset such as
CIFAR10 (see App B.). In both cases we found an excellent agreement between the two GPRs for r’s
as small as 3 and 4.
6	Generalization in the NTK regime
Collecting the results of all the preceding sections, we can obtain a detailed and clear picture of
generalization in fully connected DNNs trained in the NTK-regime on datasets with a uniform
distribution normalized to some hypersphere in input space. We begin with a qualitative discussion
and consider some renormalized NTK at scale r. From Sec. 4, we have that the features of this
kernel are hyperspherical harmonics and that λl scales as d-l. We also recall that l is the maximal
degree of the polynomial appearing in the Harmonic and all Harmonics up to the degree l span
all polynomial on the hypersphere with degree up to l. Examining (14) we find that features with
λl σ2 /η are learnable and via the above scaling we find that we learn polynomials of degree
O(log(η∕σ2)/ log(d)) or less. In particular a function like parity, which is a polynomial of degree d
is very hard to learn whereas a linear function is the easiest to learn. Thus despite having infinitely
more parameters than data-points (due to infinite width) and despite being able to span almost any
function (due to the richness of the kernel’s features), deep neural networks avoid over fitting by
having a strong bias towards low degree polynomials.
To make more quantitative statements we now focus on a specific setting. We consider input
data in dimension d = 50 and a scalar target function g(x) = Pl=1,2;m glmYlm(x) such that
Pl=1,m gl2m = Pl=2,m gl2m = 1/2, but otherwise iid glm’s. We generate several toy datasets
DN consisting of N data points (xn) uniformly distributed on the hypersphere Sd-1 and their
corresponding targets (g(xn)). We consider training a fully-connected DNN consisting of 4 layer
with ReLU activations and width W which we initialize with variance (σw2 = σb2 = 1/d) for the
input layer and (σw2 = σb2 = 1/W) for the hidden layers (see for instance Lee et al. (2019) App. C
and App. E for how to compute the kernel). To be in the NTK correspondence regime we consider
training such a network at vanishing learning-rate, MSE loss, and with W N . One then has that
the predictions of the DNN are given by GPR with σ 2 = 0 and the K given by the NTK kernel (Jacot
et al., 2018) 2.
For each such DNN we obtained the expected MSE loss kg∞? - gk2 of GPR with the NTK kernel
by numerical integration over x? Repeating this process multiple times we obtained the dataset
averaged loss hkg∞? - gk2iDN for N = 1, 2, . . . , Nmax with a relative standard error of less then 5%
(this typically required averaging over 10 datasets). For direct comparison with our prediction of
the learning curve, we computed the Poisson averaged learning curve hkg∞? - g k2iη in accordance
with (3), neglecting the terms n > Nmaχ. We restricted ourselves to ηmaχ ≤ NmaX 一 5√Nmax to
make tail effects negligible. Notably the Poisson averaging makes the final statistical error negligible
relative to the discrepancies coming from our large η approximations (see A).
Since the target function involves l = 0, 1, 2 the minimal scale for the renormalized NTK is r ≥ 2.
To have some headroom we start from r = 3 which implies reasonable discrepancies of the MSE
between the two GPRs of the order of N/d4 = 5.6e—4, for the maximal N we have (N = 3500). Our
2To be more precise, Jacot et al. (2018) predict correspondence with GPR up to a random initialization factor,
so to get exact match with GPR one would also need to average over initialization seeds. Recent research (Lee
et al., 2019) suggests this caveat can be avoided under some conditions.
8
Under review as a conference paper at ICLR 2020
analytical expressions following (13) combined with known results (Jacot et al., 2018; Cho & Saul,
2009) about the Taylor coefficients (bn) yield λ0, ..., λ3 = {3.19, 7.27e - 3, 5.98e - 6, 1.62e - 7}
and σ2 = 0.018. Since λ0, λ1》σ2/η》λ2, λ3 for 50 < η < 3500, CKr,σ2/ησ-2 < [deg(0) +
deg(1)]σ2/η + O(deg(2)10-6), thus CKr,σ2/nσ-2 ≈ 51∕η. Thus we expect perturbation theory to
be valid for η》50. At η = 100 the l = 1 features are learned well since σ2∕η = 1.8e — 4》λ1
and the l = 2 features neglected, at η = 1000 they are learned but suppressed by a factor of about
3. Had the target contained l = 3 features, they would have been entirely neglected at these η scale.
Experimental learning curves along with our leading and sub-leading estimates are shown in Fig. 1.
left panel showing an excellent agreement between theory and experiment.
While no actual DNNs were trained in the above experiments, the NTK correspondence means that
this would be the exact behavior of a DNN trained in the NTK regime (Jacot et al., 2018; Lee et al.,
2019; Arora et al., 2019). Furthermore since our aim was to estimate what the DNNs would predict
rather than reach SOTA predictions, we focus on reasonable hyper-parameters but did not perform
any hyper-parameter optimization.
ReLU NTK Dataset Averaged MSE Loss
With r=3 Prediction (σ2=0.019)	Renormalized NTK Prediction Deviation
500	1000	1500	2000	2500	3000	3500	IO2	IO3
η	N
Figure 1: Left panel: The experimental learning curve (solid line) for a depth 4 ReLU network
trained in the NTK regime on quadratic target function on a d = 50 hypersphere is shown along with
our analytical predictions for the leading (dotted line) and leading plus sub-leading behavior (dashed
line). Right panel: For the same dataset, we plot the dataset-averaged difference between predictions
based on NTK (g∞? ) and the renormalized NTK at scale r (gr?) showing an excellent agreement as r
increases.
Lastly we argue that the asymptotic behavior of learning-curve we predict is more accurate than
the recent PAC based bounds (Allen-Zhu et al., 2018; Cao & Gu, 2019b;a). In App. C We show
a log-log plot of the learning-curves contrasted with a 1/√ which is the most rapidly decaying
bound appearing in those works. It can be seen that such an asymptotic cannot be made to fit the
experimental learning-curve with any precision close to ours.
7	Discussion and Outlook
In this work we laid out a formalism based on field theory tools for predicting learning-curves in the
NTK regime. Well within the validly regime of our perturbative analysis we find excellent agreement
to within 3% relative mismatch between our best estimate and the experimental curves. Central to our
analysis was the renormalization-group transformation on the NTK leading to effective observation
noise on the target. Our analysis could be readily extend in several ways: Going beyond the uniform
dataset case should be possible for multi-variate Gaussian input distribution with a set of similar
finite variances and a set of nearly zero variances. Adding weak randomness to Kx,x0 to study the
difference between empirical and averaged NTKs. It would also be interesting to extend our analysis
to simple CNNs. The renormalized kernel can also be used for spectral analysis of the NTK and other
kernels associated with DNNs.
9
Under review as a conference paper at ICLR 2020
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and Generalization in Overparameterized
Neural Networks, Going Beyond Two Layers. arXiv e-prints, art. arXiv:1811.04918, Nov 2018.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
Exact Computation with an Infinitely Wide Neural Net. arXiv e-prints, art. arXiv:1904.11955, Apr
2019.
Douglas Azevedo and Valdir A. Menegatto. Eigenvalues of dot-product kernels on the sphere. ArXiv
e-prints, 2015.
Yuan Cao and Quanquan Gu. Generalization Bounds of Stochastic Gradient Descent for Wide and
Deep Neural Networks. arXiv e-prints, art. arXiv:1905.13210, May 2019a.
Yuan Cao and Quanquan Gu. Generalization Error Bounds of Gradient Descent for Learning
Over-parameterized Deep ReLU Networks. arXiv e-prints, art. arXiv:1902.01384, Feb 2019b.
Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Proceedings of the
22Nd International Conference on Neural Information Processing Systems, NIPS'09,pp. 342-350,
USA, 2009. Curran Associates Inc. ISBN 978-1-61567-911-9. URL http://dl.acm.org/
citation.cfm?id=2984093.2984132.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
Loss Surfaces of Multilayer Networks. arXiv e-prints, art. arXiv:1412.0233, Nov 2014.
A. Daniely, R. Frostig, and Y. Singer. Toward Deeper Understanding of Neural Networks: The Power
of Initialization and a Dual View on Expressivity. ArXiv e-prints, February 2016.
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred A. Hamprecht. Essentially No
Barriers in Neural Network Energy Landscape. arXiv e-prints, art. arXiv:1803.00885, March 2018.
Giancarlo Ferrari-Trecate, Christopher K. I. Williams, and Manfred Opper. Finite-dimensional
approximation of gaussian processes. In NIPS, 1998.
Christopher Frye and Costas J. Efthimiou. Spherical Harmonics in p Dimensions. ArXiv e-prints,
May 2012.
E. Gardner and B. Derrida. Optimal storage properties of neural network models. Journal of Physics
A Mathematical General, 21:271-284, January 1988. doi: 10.1088/0305-4470/21/1/031.
A. Jacot, F. Gabriel, and C. Hongler. Neural Tangent Kernel: Convergence and Generalization in
Neural Networks. ArXiv e-prints, June 2018.
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep neural networks as gaussian processes. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=B1EA-M-0Z.
Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and
Jeffrey Pennington. Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient
Descent. arXiv e-prints, art. arXiv:1902.06720, Feb 2019.
Dorthe Malzahn and Manfred Opper. A variational approach to learning curves. In Proceedings
of the 14th International Conference on Neural Information Processing Systems: Natural and
Synthetic, NIPS’01, pp. 463-469, Cambridge, MA, USA, 2001. MIT Press. URL http://dl.
acm.org/citation.cfm?id=2980539.2980600.
Charles A. Micchelli and Grace Wahba. Design problems for optimal surface interpolation. 1979.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In Search of the Real Inductive Bias: On the
Role of Implicit Regularization in Deep Learning. arXiv e-prints, art. arXiv:1412.6614, December
2014.
10
Under review as a conference paper at ICLR 2020
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards
Understanding the Role of Over-Parametrization in Generalization of Neural Networks. arXiv
e-prints, art. arXiv:1805.12076, May 2018.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Daniel A. Abolafia, Jeffrey
Pennington, and Jascha Sohl-Dickstein. Bayesian Deep Convolutional Networks with Many
Channels are Gaussian Processes. arXiv e-prints, art. arXiv:1810.05148, October 2018.
Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning
(Adaptive Computation and Machine Learning). The MIT Press, 2005. ISBN 026218253X.
K. Ritter. Average-Case Analysis of Numerical Problems. Lecture Notes in Mathematics. Springer
Berlin Heidelberg, 2007. ISBN 9783540455929. URL https://books.google.co.il/
books?id=X_l6CwAAQBAJ.
KlaUs Ritter. Asymptotic optimality of regular sequence designs. Ann. Statist., 24(5):2081-
2096, 10 1996. doi: 10.1214/aos/1069362311. URL https://doi.org/10.1214/aos/
1069362311.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Information
Propagation. arXiv e-prints, art. arXiv:1611.01232, Nov 2016.
L.S. Schulman. Techniques and applications of path integration. 1996. URL https://books.
google.co.il/books?id=Cuc9AQAAIAAJ.
Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun.
OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks.
arXiv e-prints, art. arXiv:1312.6229, December 2013.
Peter Sollich. Gaussian Process Regression with Mismatched Models. arXiv e-prints, art. cond-
mat/0106475, Jun 2001.
Christopher K. I. Williams and Francesco Vivarelli. Upper and lower bounds on the learning
curve for gaussian processes. Mach. Learn., 40(1):77-102, July 2000. ISSN 0885-6125. doi:
10.1023/A:1007601601278. URL https://doi.org/10.1023/A:1007601601278.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? In Proceedings of the 27th International Conference on Neural Information
Processing Systems - Volume 2, NIPS’14, pp. 3320-3328, Cambridge, MA, USA, 2014. MIT Press.
URL http://dl.acm.org/citation.cfm?id=2969033.2969197.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv e-prints, art. arXiv:1611.03530, November
2016.
A Poisson Averaging Demonstration
Here we demonstrate that Poisson averaging has no substantial effect on the learning curve. To
this end we show the experimental learning curve from the main text pre- and post-averaging. It
is evident that other than the unintended consequence of eliminating the experimental noise, the
averaged learning curve is equivalent to the original for all practical intents.
11
Under review as a conference paper at ICLR 2020
---(l∣g∞ -9∣I2)z7
---{∣∣g∞ -9,II2)dw
Poission Averaging Effect Demonstration
1500	2000	2500	3000	3500
η∖N
O 500	1000
B Comparison of NTK and Renormalized NTK Predictions on
Non-Uniform Dataset
While our lack of knowledge of the NTK eigenvalues and eigenfunctions with respect to a non-uniform
measure prevents us from predicting learning curves, we would like to show that the renormalized
NTK is still a valid approximation in this setting. To this end we compare the prediction of the NTK
and renormalized NTK on the one-hot encoding of the cifar-10 dataset.
Renormalized NTK Prediction Deviation
CIFAR-IO
r=l,σ2 = o.OO38
——r= 2,/ = 0.0028
——r= 3, σf = 0.0024
—— r = 4, σ^ = 0.0021
--- r=5,σj = 0.0019
C Comparison with recent bounds
As mentioned in the main text, various recent bounds, relevant to the NTK regime, have been derived
recently. Notwithstanding importance and rigor of these works, their bounds have at best a 1/VzN
asymptotic scaling. Here we show that given a functional behavior of the experimental learning
curves such a bound cannot be nearly as tight as our predictions.
12
Under review as a conference paper at ICLR 2020
ReLU NTK Dataset Averaged MSE Loss
With r=3 Prediction (σ2=0.019)
b〈z__6—«86__〉
D Notations for the field theory derivation.
For completeness, here we re-state the notations used in the main-text.
Ω - Space pace of inputs.
x, x0, x* - Inputs (in Ω).
μx - Measure on Ω.
K (x, x0 ) - Kernel function (covariance) of a Gaussian process. Assumed to be symmetric and
positive-semi-definite.
φi (x) - i’th eigenfunction of K (x, x0). By the spectral theorem, the set {φi}i∞=1 can be assumed to
be orthonormal:
/ dμxφi (x) φj (x) = δij
x∈Ω
λi - i’th eigenvalue ofK (x, x0).
/ dμχ0K (x, x0) φi (x0) = λiφi (x)
x0 ∈Ω
|| ∙ ||hk - RKHS norm. If f (x) = Pi f φi (x) then ||f ||hk = Pf (where φi is an orthonormal
set). Note that this norm is independent of μx.
g (x) - The target function.
σ2 - Noise variance. The noise is assumed to be Gaussian.
N - Number of inputs in the data-set.
DN - Data-set of size N, DN = {x1, ..., xN}.
fD* ,σ2 (x) - The prediction function.
E	Explicit path integral computations
Here we wish to prove the probability function defined in (5) of the main text yields the
GP defined by a given kernel using explicit computation of the path integral. Denoting
R dμxdμχ0f(x)KT (x,x0) f (x0) as IIfkK and noting that kf kK = Pi fi:
13
Under review as a conference paper at ICLR 2020
Notably, all other higher correlation functions split into products of the above correlation function
due to standard properties of Gaussian integrals (Wick’s/Isserlis’ theorem).
R Df ∙	f(x)	∙ f(y)	∙	exp (-2 kfkK)	RQi	fi	∙ Pi f,Φ,	(χ) ∙ Pj	f Φj	(y)	∙	eχp (-1 Pl	f2)
R Df exp (-1 kfkK)	RQi dfiexp (-1 Plf)
=X Rdf∙ f2 ∙ exp (-f) φi(X φi (y) + X R f f exp (-f) ∙ Rdf∙ f exp (-f)
i R df exp (-f)	i=j R df exp (-f) R df exp (-2⅛)
--{z}	--}	}
λi	0	0
=	λiφi (x) φi (y) = K(x,y)
i
F Gaussian Process Prediction as a Field Theory
Let us assume a Gaussian process (GP) with mean 0 and co-variance function K (x, x0). For a
data-set DN of size N and noisy targets (σ2 6= 0) {g (xi)}iN=1, it is known that the posterior mean
obtained by Bayesian inference is
4	"	1 II r∣∣2	,	(f (Xi) - g (Xi))2
fDNH = argmin 2 kf kHκ + T ---------------2σ-------
xi∈DN
For a data-set DN of size N and noisy targets {g (Xi)}iN=1 , we present the GP canonical partition
function:
ZDN,σ [α (X)]
d=f Z Df exp (-2 kfkHκ + Za (X) f (X) dx- X (Jgi-2 (Xi))2
xi∈DN	σ
Where the Df notation stands for path integral. Notice that the functional derivative of
log (ZDN,σ2 [α (x)]) w.r.t a (x*) at a (x) = 0 yields:
∂	1	∂
E .(x) = 0 Iog(ZDN ,σ2	[a	(X)D =	ZDN ,H [a	(X)=0] ^ E	α(x)=0	(ZDN ,^	(X)D	=
R Df ∙ f(X*)exp -1 kfkHκ - P	f(xi)-g(xi))2
xi∈DN
RDf exp (-2 kfkHκ- P	(""i))2)
xi∈DN
arg min
f |x*
2kfkHκ
N
+X
i=1
(J (Xi) - g (Xi))2
2σ2
where the last equality is due to the fact that for Gaussian distributions, the expected value coincides
with the most probable value. Therefore, the exact baysian inference mean:
∂
fDN ,σ2 (x ) = da (x*)(尸Olog (ZDN ,σ2 [a (X)D
F.1 Canonical Ensemble Formalism
for evaluating the quality of a certain GP, we’re interested in the average prediction for all the data-sets
of size N, meaning:
14
Under review as a conference paper at ICLR 2020
fNσ (χ*) d=f fDN,σ (x*)〉
DN 〜μN
/ dμχi / dμχ2 …/ dμχN fD N ={X1,...,XN }σ (x*).
Using the replica trick we obtain:
fNCσ(X* ) = Mmo ∂0⅛
α(x)=0
(ZDNσ [α (X)DdnX- 1
M
Now, let us calculate ZDM ,σ2 [α (x)]
for an integer M :
DN 〜μN
ZDMN,σ2[α(x)]=ZZ...ZYM Dfj
、二L j=1
M
(1 ^X H ʃ ∣∣2	/	(、一 、小	1X	(fj (Xi) - g (Xi))2
eχp - 2 T kfj kHK +X α (X) fj (X) dx -TT -----2σ2-----
j=1	j=1	j=1 xi∈DN
ZN,M,σ2 =〈ZDN,σ2 [a (X)D°n〜μN
/ dμχι …/ dμχNZDN,σ [α (X)] =
Z …/ Y Dfj eχp j
M
1 M	M	∖ /	/ M
2XkfjIlHK + XJa(X)fj(X)dX I (eχp I-X
(fj (x) - g (x))2
2σ2
X〜μχ
N
so
fNC,σ2 (X*)
lim
M→0
daM.(x) = 0 DZMN,σ2 [a (X)]Edn〜“N
M
F.2 Grand Canonical Ensemble Formalism
We now wish to allow fluctuations in the value of N, meaning averaging over fNC,σ2 (X*) for different
values of N . The motivation is to simplify the calculations, while averaging around a relatively
confined region of N s. Let us average the canonical prediction while weighting N according to
Poisson distribution with expected value η:
∞	-η N
fησ (X*) d=f X V fNσ (X*).
N=0
and defining:
∞
Zη,M,σ2 [α (X)] =
N=0
e-η nN
N!
IN 〜μN
we get:
fGC (X*) = ∂a (x*)
Zη,M,σ2 [α (X)]
Iim ---l--------
α(x)=0 M→0	M
15
Under review as a conference paper at ICLR 2020
That is, the functional derivative w.r.t α (x*) at α (x) = 0 yields the average prediction, averaged
over different data-set sizes (the canonical averaging) and different data-sets for each size (the grand
canonical averaging).
For a given η, the standard deviation of N is η, so the relative error is √, decreases with η.
Substituting ZDMN ,σ2
[α (x)]
/ DN 〜μ;
in the expression for Zη,M,σ2 [α (x)] we obtain:
(ZDN ,σ2 [α (X)DDN 〜μN =
∞ LrlN r M m	/	1 M	Mf	'
=X T …∏ Dfj exp I-2 X kfjkHK + X a (x) fj (x) dx
N =0	' ^~{zS j=1	∖	j=1	j=1	,
M
(exp (-X fj (X)2-2g (X))2 J;
X〜μχ
M M M	/ ι M	Mf
/…/ Y Dfj exp J- 2 XkfjkHK + Xja (X) fj(X) dx
's~M'
X e-ηInN e /_ X (fj(X) - g (x))2
LN!	∖ p (乙	2σ2
N=0	j=1
=e-η ...	Df1...DfM
{"
M
X〜μχ
1 M 2 M	M (fj (X) - g (X))2
exp I -2 E kfj∣∣HK+£	a (X) fj(X) dX+n (eχp ∣-E ʌ—⅛σ^"—
j=1	j=1	j=1	σ
X〜μχ
F.3 Deriving the Equivalence Kernel using the Grand Canonical Formalism
We wish to get rid of the exponent inside the exponent. Expending it using (first order) Taylor series:
N
N
16
Under review as a conference paper at ICLR 2020
exp
Zη,M,σ2 [α (x)] =
e-ηZ ...Z YM Dfj
I{Z-} j = 1
M
-2 X kfjkHκ + XX Za (x) fj (x) dx + η *exp (- X f ⑺相。(x))2 1 + j ≈
≈e-ηZ...ZYM Dfj
|—{z-} j=1
M
exp
MM
-2 X kfj kHκ + X /
j=1	j=1
M (fj (x) - g (x))2
α (X) fj(X) dx+η( 1 -E —2σ2s— /
'	j=1	Xx~μx
/ Df exp (-1 kfkHκ + / a (x) f (x) dx - η Inxx- (X)))
∖	∖	/ x~μχ
ZEK2 [a (x)] = Z Df exp (-2 kfkHκ
+ / a (x) f (x) dμχ
dμχ (f (χ) - g (χ))2
and without any additional approximation:
fGC Q
∂a (x*)
lim Zη,M,σ2 [a (X)] - 1 ≈
α(x) = 0 M→0	M
∂
∂a (x*)
∂0⅛L(x)=0 log(ZEK2 [a (X)D = ZEK [al) = 0]
lim
α(χ)=0 M→0
∂
∂a (x*)
ZEKK [a1(x) = 0] ∙∕Df∙ f(X*)
M
d=f (ZEK2 [a (x)])M
[a (X)])	- 1
(ZEK2 [a (X)D
α(x)=0
exp (-2kf kHκ + / a (x) f (x) dx - 2σ2 / dμχ (f (x) - g (x))2
α(x)=0
R Df ∙ f (χ*)eχP (- 1 kf kHK - 2η2 R dμx (f (X)- g (X))2)
RDf eχp (-1 kf kHκ -务 R dμχ (f (χ) - g (χ))2)
arg min
f |x*
2kfkHκ
dμχ (f (χ) - g (χ))2
d=f fEK (χ*)
∂
M
M
and that is exactly the result for the equivalence kernel, where η is the data-set size (we regarded it as
the mean of the data-set size).
Let us derive it explicitly. For f (x) = Pi fiφi (x) and g (x) = Pigiφi (x):
17
Under review as a conference paper at ICLR 2020
fEK2 (x*)
R Df ∙ f (x*)exp (-1 kf kHκ -多 R dμχ (f (x) - g (x))2)
R Df exp (-1 kf kHκ -务 R dμχ (f (x) - g (x))2)
RQi dfi ∙ Pi fiφi (x*) ∙ exp (- 1 Pi (fi + σ2 (fi - gi)2))
RQi dfi exp (- 1 Pi (fi + σ⅛ (fi - gi)2))
X *	R dfi	∙	fi	∙exp (- f -8(fi-	gi)2)	X	λi
τ φi(X) -FT------L~T77---------------= T λ-Ξ2giφi(X )
i	d dfi exp (- 2λ^ - 2σ2 (fi - gi) J	i i + η
F.4 Equivalence Kernel as Free Field Theory
Regarding the Equivalence Kernel as the free (quadratic) theory, we can denote fηE,σK2 (x*)
hf (x*)i
f〜EK =hf (x*)i0 = Pi λ+⅛giφi(x*). ’
Let us calculate the correlations in the free theory:
RDf ∙f (x) f ⑹ exp (-11lf kHK - 2σ2 Rdμχ(f (X) - g(X))2)
hf (x) f (y)io =--------------r-ʌʒ---------------------------------2ξ—‘一
R Df exp (-2 kfkHK - 2σ2 R dμχ(f (X)- g(X))B
RQi dfi ∙Pij fifjφi (X) φj (y) ∙ exp (- 1 Pi (fi + σ (fi - gi)2))
RQi dfi exp (- 1 Pi (λ + σ⅛ (fi - gi)2))
X R dfi ∙ fi ∙ exp (- 2λi - 2σ2 (fi - gi )2)
i R dfi exp (- 2fii - 2σ2 (fi - gi)2)
'---------------------------------------------
}
λigiλj gj
(λi + σ2) (λj + σ2)
φi (X) φj (y)
Therefore:
z
X(λ + a)φi(X) φi(y)+X
iσ
i	i,j
i+Wj+V)φi(X) φj ⑻
{^^^^^^™
hf(x)i0hf(y)i0
}
CoV [f (X) ,f (y)] = X (λ1 + σ2)	φi (X) φi (y)
and We see that the correlations are O (1).
For rotationally invariant kernel, we get that
∞ deg(l)	1	-1
Var [f (X)] =XX	λ- + σ2	匕 2m (X) d=f Cκ,η,σ
l=0 m=0	l
is independent of x since
deg(l)
X Yl2,m (X) = deg (l)
m=0
18
Under review as a conference paper at ICLR 2020
so
∞ deg(l)	]
Ci=∑∑ λ⅛
l=0 m=0 l ，/
F.5 PERTUBATIVE CORRECTION FOR THE EQUIVALENCE KERNEL
F.5.1 Averaging f
Going to the next order in the expansion:
Zη,M,σ2 [α (x)]=
e-η / …/ ft Dfj
1^^sz-} j = 1
M
/ 1 M
exp 卜2 X IlfjkHκ +
£ Za (x) f (x) dx + η *exp (- X (fj ⑺-(X))2 ) +	) ≈
j = 1	∖ j = 1	x χzμj
≈ e-η / …/ f Dfj
1{√"} j = 1
M
exp
1M	M
eχp I-2 E kfji∣Hk + £ Ia (X) f (X) dx)
∖ j=ι	j=ι	)
1 X (fj(X) - g(X))211 (X (fj(X) - g(X))2 ∖ ∖ ʌ
1 * ~^σ~+2 B -^σ-) LJ
(*
η {
\ \
/…/仃Dfj
|~{√"} j = 1
M
eχp (X (-1 kfjkHK+ /α(X) fj(X)dX -轰 /dμx (f (X) -g(X))2
(MM	\
表X XJ dμx (fj(X)- g(X))2 .(力(X) -g(X))2 J
Note that:
fGC (X*)
∂
∂a (x* )
z,
1
lim Zη,M,σ2 [a (X)] - 1
α(x)=0 M→0	M
Mm0 (ZEK2 [a (X)= 0])
Mm0 (ZEK2 [a (X) = ODM
|---------------------'
19
Under review as a conference paper at ICLR 2020
Calculating the first order pertubative corrections:
fG,C W
lim el(x)=0
M→0 M ∙
Zη,M,σ2 [α (x)]
M
lim ----1―而[…[Y Dfj
M→0 MZEK2 [0]M J
η,σ2 L」	^_{^_}j=1
M
1M	η M
-2 X kfj iiHk - 2σ2 X J dμχ (Zj(X) - g (X))2 J
(MMr	M M
8σ4XX J dμχ (Zj(X) -g(X))2 Yfl(X) -g(X))2J ∙Xfi(X*)
lim
M→0 MZ
1
EK
η,σ
ZtztM	/	1 M	Mf
[0]Mj …/ Y Dfj exp I-1 XkfjkHK -轰 XJ dμχ (fj (x) - g (X))2
's~M'
2
η MM	M	1
11 + 8σ4 ΣΣ J dμχ (Zj(X)- g(X))2 .(力(X) -g(X))2 j ∙ ɪ^fi (x*) + O (m)=
I 〃	MM	∖ M	\
MmO M ∖ 11	+8σ4 ∑E	J dμχ (Zj(X)- g(X))2	∙ (fl(X)- g(X))2 J ∙ Efi(X*))
'∖	j=1l = 1	) i=1	f fι,...,fΜ 〜EK
+O G
1 η MM	M
=hf	wio+MmO	M (18σ4	ΣEJd〃x	(Zj(X)- g(X))2 ∙ (fl(X)- g(X))2 J ∙ Efi(X*))
'∖ j=1l = 1	) i=1	f fl,...,fΜ 〜EK
+O G
=hf(x*)io
1	MMM
+ MimO M 8σ4 J μxx	XXX
(Zj(X)- g (X))2 ∙ (Zl(X)- g (X))2 fi(X*))
∖j=1 l=1 i=1	/ fι,...,fM〜EK
+O
Calculating the correction:
MMM
XXX
(Zj (X)- g (X))2 •(力(X)- g (X))2 fi(X*))
∖j=1 l = 1 i=1	/ fl,...,fM〜EK
=M((f (x)-g (x))4 f (x*)〉o
+M (M - 1) [2 D(f (x) - g (x))2ΕoD(f (x) - g (x))2 f (x*)>o + D(f (x) - g (x))4E0 hf (x*)io]
+M (M - 1)(M - 2) D(f (x) - g lx))2)： hf (x*)io
Note that we eventually divide by M and take the limit M → , so we only care about O (M) terms:
20
Under review as a conference paper at ICLR 2020
fηG,σC2 (Xi) = fηE,σK2 (Xi)
+8σ4 / dμχ hD(f(X)- g(X))4 f(X*))o - 2 Df(X)- g(X))2)o Df(X)- g(X))2 f(X*))o
-D(f(X)-g(X))4E hf (Xi)i0 + 2 D(f (X) - g (X))2E2 hf (Xi)i0
+OG
These correlations can be calculated using Feynman diagrams, since the free theory (EK) is quadratic
(Gaussian):
(f(X)-g(X))4f(Xi)	=
=3fEK (x*)Var[f (x)]2 + 6阴(工*乂阴(X)- g (x))2 Var[f (x)]
+fEK (工*乂阴(X)- g (x))4 + 4 IfEK (X) - g (x))3 Cov [f (x) ,f (x*)]
+12 (fEK (x) - g (x)) Var [f (x)] CoV [f (x) , f (x*)]
((f(x)-g(X))4)o hf (XiOiO =
= 3fEK (x*)Var[f(x)]2
+6fE柒(x*MfEK (X)- g (x))2 Var[f (x)] + fEK (x*) fEK (X)- g (x))4
(f(X)-g(X))2	(f(X)-g(X))2f(Xi)	=
=2Var[f (x)] Cov [f (x) ,f (x*)] fE总(X)- g (x)) + fEK (x*) Var[f (x)]2
+2Cov[f (x) ,f(x*)]g(x) - g (x))3
+2fE尊(x*)Var[f (x)]心(X)- g (x))2 + fEK (x*) fEK (X)- g (x))4
D(f (X) - g (X))2E hf (Xi )i0 =
= fηE,σK2 (Xi ) Var [f (X)]2
+2fEK (x*) Var[f (x)] .(阴(x) - g (x))2 + fEK 6乂用(x) - g (x))4
Summing everything up:
D(f (x) - g (x))4 f (xi)E -D(f(x)-g(x))4E hf(xi)i0
-2 D(f (x) - g (x))2E D(f (x) - g (x))2 f (xi)E + 2 D(f (x) - g (x))2E2 hf (xi)i0
=8 (fEK2 (x) - g (x)) Var [f (x)] Cov [f (x), f (x*)]
and all the bubble diagrams cancel as expected.
So we get:
21
Under review as a conference paper at ICLR 2020
f,C (χ*) = fEK (χ*) + σ / Mχ f,K (χ) - g (χ)) var[f (χ)]	[f (χ), f (χ*)] + OG
Substituting the expressions for the variance and covariance:
f,C (x*)=
fEK(X*)- σ X 1 ; 对(λ+σ)(鼠+σ)	g血 (吟 J 加方办(X) %(X)渥(X)+O(齐
F.5.2 Averaging f2
This time we must use two different replica indices:
2
)
《fDN,σ2 (x* )]2〉
∂ log (ZDN,σ2 [α (x)])
lim lim —
M→0 M→0 MM
m=1
m=ι
DN 〜μN
∂α (x*)
α(x) = 0
(I lim ɪ •
∖ Im→0 M
∂ZDN,σ2 [α (x)]
∂α (x*)
•/ Df ... I DfM / DfI ... I DfM exp I -1 X IIfmlHK
M
-X
∖ m=1
(fm (x) - g (x))2
2σ2
—
M
X
m=ι
2σ2
1	M	0
2	Xf M
m=1
N
)
X〜Nx

DN 〜μN
M
〜
M
Efm (x*) E fm (x*)
卜P
—
DN 〜μN
Averaging w.r.t poisson distribution:
22
Under review as a conference paper at ICLR 2020
ɪ	∖	∞ 一 一n_N ,
W」NS PDfD N卢E…
lim lim -ɪ ∙ j Df ... [ DfM / Df ... [ Df面
M→o M→o MM J J J J M
exp
η (exp
/	1 M 1	1 M
exp I -η - 2 X kfmkHκ - 2 X
∖	m=1	m=ι
2
HK
X (Jm (X) - g (X))2 X
-工	2σ2	工
m m=1	m=ι
2σ2
Efm (X) Efm (X) ≈
X〜μx
m=1
m=ι
(
∖
≈ lim lim -ɪ ∙ [f)f1... ( DfM / Df ... / DfM
M→o M→o MM J	J J	J M
M
M
exp
M
-2 X IIfmkH
m=1
I M
K- 2 X
m=ι
2
HK
+η
_ X (Jm (X) - g (X))2
-L	2σ2
m=1
M
X
m=ι
M
X〜μx
(Jm (X) - g (X))2 _ X
m=1
2σ2
Efm (X)£ fm (X)
m=1
m=ι
∖
+X
M
-X
` m=1
2
)
∖
M
—
/
X〜μχ )
≈ M MmO 焉∙/DfI …IDfMIDf …JDfM
exp
1M
-2 X llfmkH
m=1
—
K2
(fm (x) - g (x))2
2σ2
M
X〜μx
M
X
、m=1
2σ2
+ lim lim —匚∙ -ɪ- / dμχ ( I
M→o M→o MM 8σ4 J 产 ''=
2σ2
∖
M
(fm (x) - g (X))2
m=1
/
M
+ X
m,=ι
M
-X
m=ι
M
-X
` m=1
Efm (X)E fm (X)
m=ι
+
c=1
2
M
X〜Nx
2σ2
EfC (X) E fd (X)
d=1
M
M
(X)
8(fEjK2 (x)-g(x))var[f (x)]Cov[f (x),f (x*)] as we saw in ⑺
dμx (需?"
α=1
b=1
∖
l
1+X
∖
1
2
)
M


23
Under review as a conference paper at ICLR 2020
and we’re left with:
MimO
MM
X (fa (X)- g (X))2 X fb (x*)
a=1	b=1
0
MimO M [m D(f (x) - g (χ))2 f (x*)〉。+M(M -1) hf (x*)〉D(f (χ) - g (χ))2E Oi =
«f (X)- g (x))2 f (x*))o-hf (x*)io «f (X)- g (x))2)O = 2 IfEK (x) - g (x)) Cov [f (x) ,f (x*)]
but this correction gives O
so:
Zf N .m)]2 EDNIN ")=
(阴(x*))2 + ση4fEK (x*) / dμχ IfEK (x) - g (x)) Var[f (x)] Cov [f (x) ,f (x*)]+ O (*
and notably
f2=hfi2+O
G)
F.6 Perubative Correction for Rotationally Invariant Kernel
We now wish to evaluate this expression for a rotationally invariant kernel and a uniform measure on
the hypersphere. This simplyfies the expression for hfi to:
fηG,σC2 (x*)
fEK(x*) + ση4 / dμχ (fEK(X)- g(x)) Var [f(x)] COV [f(x) ,f (x*)] + O
fEK(X*)+σ Cκ,
fEK2(X* ) - gCK,η,σ2 X
i,j
,η,σ2
σ2
η
/ dμχ (fEK (x) - g (x)) Cov [f (x) , f (x*)]=
λ- + W)	giφj (x*) / dμχφi (x) φj (x) +O (η1∙
'''^^^^^^^^^{^^^^^^^^^^
δij
σ2
fEK (x*) - σCK,η,σ2 X λ + σ2
i i ' η
gl.m
-1
giφi (x*) +O
fηE,σK2 (x*) - CK,η,σ2
Σ
l,m
σ2
Yl,m (x* ) + O
λi + σ
The expression for f2 is:
fD N - (X*)]2 EDNi) N "η)
gl.m
Yl,m (X*)+ o(a
24
Under review as a conference paper at ICLR 2020
G Various insights
G.1 Correction means worse generalization
The correction always means worse generalization than what the EK suggests. Indeed
gl,m
f,C (x* ) = fEK (x*) - CK,η,σ2 Σ σ2(2+g黑 + Q) Yl,m (x*) + O
X λl +lσ2 gι,mγl,m (X*)- CKnσ2 X σ2 (2 +g,m + CI)Ym (x*)+O
/
∖
Σ
l,m
λl
λι + σ2
lη
CK,η,σ2
gl,m Yl,m (x*)
—
∖
<.
{Z-
positive
^^^^^{^^^^^^^^^™
< -^lɪ <1
%+ση2
}
/
.}
G.2 EXACT EIGENVALUES FOR 2-LAYER RELU NTK WITH σb2 = 0
For the NTK associated with a 2-layer ReLU NTK without bias we were able to fined an exact
expression for the eigenvalues for all l:
λ2k
22
σW1σW2
2π
d(1 + 2k) + (1 - 2k)2"(k - 2) γ (d)丫	_ σW1σW。 π
8∏	1 r(k + 婴)),λ2k+1 = -^r ∙ dδk0
It is interesting to note that for all odd l > 1 λl = 0 so the expressive power of the kernel (and hence
the neural network) is greatly reduced.
H Accuracy of the renormalized NTK
For two normalized datapoints x and x0 , drawn from a uniform dataset on a hypersphere of radius 1,
and at large d the random variable (x ∙ χ0) is approximately Gaussian with variance O(d-1). Since
(x ∙ χ0) is bounded to [-1,1], the random variable (x ∙ χ0)r must have a standard deviation which is
decaying function of r. For r d and large d one can estimate the magnitude this standard deviation
from exact known expressions and a saddle-point approximation yielding O((d/r)-r/2) ≈ O(d-r/2)
3. Considering next the tail of the Taylor expansion Pqq>r bq(X ∙ χ0)q, projected on the dataset
(Pq>r bq(Xn ∙ xm)q). The resulting N by N matrix is Pq(l>r bq on the diagonal but O(d-(r+1)/2)
in all other entries. As we justified in the main text, our renormalization transformation amounts to
keeping only the diagonal piece of this matrix and interpreting it as noise.
Consider then (1) for g? in two scenarios: (I) g∞? with the full NTK (K(x, x0)) and no noise and (II) gr?
with the NTK trimmed after the r’th power (Kr(x, x0)) but with σr2 = Pq>r bq. The first K(x?, xn)
piece, for x? drawn from the dataset distribution, obeys K(x?, xn) - Kr(x?, xn) = O(d-(r+1)/2).
Next we compare Kr(xn, xm) + Inmσr2 and K(xn, xn). On their diagonal they agree exactly but
their off-diagonal terms agree only up to a O(d-(r+1)/2) discrepancy. Denoting by δK the difference
between these two matrices, we may expand K-1 = [Kr + σm2 I + δK]-1 = [Kr + σr2I]-1 [1 -
δK[Kr +σr2I]-1 +δK[Kr +σr2I]-1δK[Kr +σr2I]-1 + ...].
3A more accurate estimate is
r	∖r/
r+d—3)
d—3 ∖"4
r+d—3)
25
Under review as a conference paper at ICLR 2020
We next argue that δK[Kr + σr2I]-1 multiplied by target vector (g(xn)) is negligible compared to
the identity for large enough r thereby establishing the equivalence of the two scenarios. Indeed
consider the eigenvalues of δK[Kr + σr2I]-1. As δKnm is O(d-(r+1)/2) its typical eigenvalues are
O(√Nd-(r+1)/2) and bounded by O(Nd-(r+1)/2). The typical eigenvalues of [Kr +。京I]-1 are
of the same order as K(xn, xn) = K and bounded from below by σr2. Thus typical eigenvalues of
δK[Kr + σr2I]-1 are O(√Nd-(r+1')/2/K) and bounded from above by O(NdTr+1/?/σr). The
NTK has the desirable property that σr2 decays very slowly. Thus certainly in the typical case but
even in the worse case scenario we expect good agreement at large r. In Fig. 1, right panel, we
provide supporting numerical evidence.
We refer to Kr (x, χ0) as the renormalized NTKS at the scale r. As follows from (13), λ∕s with l ≥ r
are zero. Therefore, as advertised, the high-energy-sector has been removed and compensated by
noise on the target and a change of the remaining l < r (low-energy) eigenvalues. A proper choice of
r involve two considerations. Requiring perturbation theory to hold well (Cκr,σ2∕η <。楙)which
puts an η-depended upper bound on r and requiring small discrepancy in predictions puts another η
dependent lower bound on r (typically √Nd-(r+1)/2《1).
Lastly we comment that our renormalization NTK approach is not limited to uniform datasets. The
entire logic relies on having a rapidly decaying ratio of off-diagonal moments ((Xn ∙ Xm)2r) and
diagonal moments (Xn ∙ Xn)2r as one increases r. We expect this to hold in real-world distributions.
For instance for a multi-dimension Gaussian data distribution the input dimension (d) traded by an
effective dimension (def f) defined by the variance of (Xm ∙ Xn).
26