Under review as a conference paper at ICLR 2020
LabelFool:
A trick in the Label Space
Anonymous authors
Paper under double-blind review
Ab stract
It is widely known that well-designed perturbations can cause state-of-the-art ma-
chine learning classifiers to mis-label an image, with sufficiently small perturba-
tions that are imperceptible to the human eyes. However, by detecting the incon-
sistency between the image and wrong label, the human observer would be alerted
of the attack. In this paper, we aim to design attacks that not only make classifiers
generate wrong labels, but also make the wrong labels imperceptible to human
observers. To achieve this, we propose an algorithm called LabelFool which iden-
tifies a target label similar to the ground truth label and finds a perturbation of
the image for this target label. We first find the target label for an input image by
a probability model, then move the input in the feature space towards the target
label. Subjective studies on ImageNet show that in the label space, our attack is
much less recognizable by human observers, while objective experimental results
on ImageNet show that we maintain similar performance in the image space as
well as attack rates to state-of-the-art attack algorithms.
1	Introduction
Deep neural networks are powerful learning models that achieve state-of-the-art pattern recogni-
tion performance in classification tasks (Krizhevsky et al., 2012b; LeCun et al., 2010; He et al.,
2016). Nevertheless, it is found that adding well-designed perturbations to original samples can
make classifiers of deep neural networks fail (Szegedy et al., 2013). These kinds of samples are
called adversarial samples. Techniques for generating adversarial samples are called attackers.
We think the ideal attacker should satisfy three levels of requirements. The first requirement is
fooling networks which means making classifiers fail to classify an image successfully. For example,
a dog image can be classified as a cat after added some well-designed perturbations. There are a
number of methods for achieving a high attack rate (Goodfellow et al., 2015; Carlini & Wagner,
2017; Dong et al., 2018).
clean image
(church)
motivation
human observer
IverSariaIimageS
attacker 1
attacker 2
attacker 3
,attack fails
,too large ,
perturbations
attack fails
. apparent ,
misclassification
dome
imperceptible1
classifier
attack
successfully
imperceptible
monastery
imperceptible1
output terminal
(label SPaCe)
input terminal
(image SDaCe)
Figure 1: This graph illustrates the importance of the imperceptibility of adversarial samples in both
image space and label space. Triangles are three attackers. Circles represent human observers.
The second requirement for the ideal attacker is the imperceptibility in the image space. This means
the magnitude of perturbations in the pixel level needs to be as tiny as possible so that it is imper-
ceptible to human eyes. For example, additive perturbations are minimized with lp norm to generate
1
Under review as a conference paper at ICLR 2020
imperceptible adversarial samples (Seyed-Mohsen et al., 2016). Extreme cases also exist where
only changing one or a few pixels (Su et al., 2019; Modas et al., 2019) can make classifiers fail.
Moosavi-Dezfooli et al. (2017) even show the existence of universal (image-agnostic) perturbations.
The third requirement for the ideal attacker, which is newly proposed in this paper, is the imper-
ceptibility of the error made by the classifier in the label space. It means making the classifier to
mis-classify an image as the label which is similar to its ground truth, so that people won’t notice the
misclassification. For example, in Figure 1, a human user will probably ignore the mis-classification
if an attacker caused a “church” to be mis-classified as a “monastery” as the third attacker does.
However, a human user will easily notice the mistake if an attacker caused a “church” to be mis-
classified as a “dome” as the second attacker does or caused an apparent perturbation in the image
space as the first attacker does. In real applications, a human user will take defensive measures as
soon as he notices the attack. Therefore making the whole attack process imperceptible is crucial
for letting observers’ guard down. Tiny perturbations in the image space but large perturbations in
the label space can muddle through on the input terminal. But as soon as observers check on the
output terminal and see the obviously-incorrect label for an input, they will realize that the classifier
fail due to some attacks and take defensive measures immediately, just as Figure 1 shows. This
justifies the power of attacks which also confuse people in the label space. So the imperceptibility
in the label space is quite important. However, to our best knowledge, few attackers have realized
this point.
In this paper, we propose an untargeted-attack algorithm called LabelFool, to perturb an image to be
mis-classified as the label which is similar to its ground truth, so that people won’t notice the mis-
classification. In the meantime, LabelFool also guarantees the imperceptibility in the image space as
well as maintaining a high attack rate in fooling classifiers. There are two steps by which we accom-
plish our goal. The first step is to choose a target label which is similar to the input image’s ground
truth. The second step is to perturb the input to be classified as this target label. The way is finding
the classification boundary between the current label and the target label, and then moving the input
towards this boundary until it is classified as the target label. We conduct a subjective experiment
on ImageNet (Deng et al., 2009) which shows that adversarial samples generated by our method are
indeed much less recognizable in the label space by human observers than other attacks. We also
perform objective experiments on ImageNet to demonstrate that adversarial samples generated by
LabelFool still guarantee the imperceptibility in the image space as well as maintaining a high attack
rate in fooling classifiers.
2	Related Work
The phenomenon that neural networks are sensitive to adversarial samples was proposed by Szegedy
et al. (2013). Since then, many researchers have studied how to generat adversarial samples. FGSM
(Goodfellow et al., 2015) was proposed to maximize the classification error subject to l∞-norm
based distortion constraints. CW attack (Carlini & Wagner, 2017) generates adversarial samples
by solving an optimization problem based on l0 /l2 /l∞ constraint, and l0 CW attack is the first
proposed method that can cause targeted misclassification on the ImageNet dataset, meaning that
we can specify the label of adversarial samples. But this designation of the target label is arbitrary.
Until this paper, there has no guide about how to choose a target label such that it is difficult for a
person to notice that the network has failed.
Besides achieving the goal of misclassification, many researchers realize the importance of imper-
ceptibility in the image space (Xu et al., 2019). One-pixel attack (Su et al., 2019) and SparseFool
(Modas et al., 2019) attack networks in a scenario where perturbing only one/a few pixels can make
a big difference. Moosavi-Dezfooli et al. (2017) show the existence of universal image-agnostic per-
turbations for state-of-the-art deep neural networks. DeepFool (Seyed-Mohsen et al., 2016) seeks
the minimum image-level distortion. And for generating adversarial samples, it directly moves the
input sample to the nearest class in the feature space. This is the most closely related work to
ours, because features extracted from classification models can reflect images’ perceptual informa-
tion and the classes which are close in the feature space are often perceptually similar. However,
DeepFool approximates the multi-dimensional classification boundaries in two dimensions and this
might make big errors on finding the nearest class. All these attacks generate adversarial samples
by iteration and the algorithm stops as soon as an adversarial sample is born no matter what label it
2
Under review as a conference paper at ICLR 2020
belongs to. This will lead to an apparent misclassification so that observers will sound the defensive
alarm quickly.
In this paper, we will compare our method with three attacks: FGSM, DeepFool and SparseFool to
show the advantage of our method in the imperceptibility in the label space. We will also demon-
strate that the performance gain in the label space is not at the expense of the loss in the image space
or attack rate.
3	LabelFool
In this section, we will introduce our method about how to choose a target label which is undetectable
by human observers and how we can perturb the input image so that the classifier assigns this specific
label. The whole pipeline is shown in Figure 2. All the symbols and notations used in this paper are
summarized in Table 1. We use the same notation i(i = 1, 2, . . . ) for “class” and “label”, because
“class” and “label” are interchangeable in this paper. LabelFool contains two steps. The first step is
to choose a target label for the input image which is similar to its ground truth. The second step is
to perturb the input image to be classified as this label. Inspired by DeepFool (Seyed-Mohsen et al.,
2016), we make modifications at the feature level. We keep moving the input towards this chosen
class at the feature level until it is classified as the label we want.
3.1	Choose A Target Label
The first step of our method is choosing the target label tx for an input image x. As we want the
target label to be imperceptible in the label space to human observers, we need to find the most
“similar” label to the input image’s ground truth lx where the most “similar” means the nearest in
the perceptual distance metric. However, lx is usually unknown when an input image is given. So it
is important to estimate the probability distribution P of an input’s ground truth lx , based on which,
we can compute the distance between each class in the dataset and lx , then choose the nearest one
as the target class. We propose a weighted distance model to achieve this goal. Before introducing
the model, there are some preparations.
Given two image x, y, we choose pre-trained image classification models to extract features φx , φy
because these features can reflect some perceptual information. As we want to calculate the distance
in the perceptual distance metric and cosine distance has been used to measure perceptual similarity
in many works (Lin et al., 2016; Wang et al., 2019), we compute the distance between x and y
as d(x, y) = 1 - cos (φx , φy). After having the distance between two images, we can compute
the distance between classes. Each class is a set of images. To measure the distance between two
sets, we choose Hausdorff distance (Henrikson, 1999). The distance between class i and class j is
denoted as Di,j. Suppose a dataset has n classes. Then, we can construct a matrix D ∈ Rn×n by
calculating the distance between all pairs of classes in the dataset, and it will be used in the following
probability model to provide the distance we need. After these preparations, we can start to decide
the target label for an input image.
As introduced before, we need to estimate the probability distribution P of the ground truth lx
because we want to find the nearest label to lx which is unknown in the beginning. When an image
x is put into a classifier f, state-of-the-art machine learning classifiers usually output a predicted
label lχ and a probability vector P whose elements mean P(X ∈ class i) = pi. For simplicity, We
suppose the elements in P are sorted in the descending order. Meanwhile, P can be thought as P's
approximation. Furthermore, we define a distance function between lx and the class i in a n-classes
dataset as Di(lx). In order to choose the nearest label to lx as the target label tx, we need to estimate
the expectation of Di(lχ) which is denoted as Elx〜P[Di(lχ)](i = 1,...,n). In general, our target
function is Eq. (1).
tχ = arg minElx〜p[Di(lχ)]	(1)
i=1,...,n
Specifically, when pi is larger than some threshold δι, we use Maximum Likelihood Estimation
(MLE) (Pfanzagl, 2011) which means we believe the classifier and take the predicted label lχ as
3
Under review as a conference paper at ICLR 2020
Table 1: Notations used in this paper.
Symbol	Meaning
x	Input image
φx	The feature of image	x
D The perceptual distance matrix where Di,j represents the distance between class i and class j
f	Classifier
P	Probability vector (elements are in descending order)
lx	The ground truth of the input image x
lx	The predicted class of the input image x by the classifier
Di (lx)	A function calculating the distance between class i and the ground truth lx
tx	Target label for input x
δ1 , δ2	Two thresholds, in this paper, δ1 = 0.8, δ2 = 0.01
M The number of elements larger than δ2 in p, i.e. M = maxj=ι,…,n{j : Pj > δ2}
Fj	The classification boundary between class lx and class j of an image
the ground truth lχ, then choose the label (except lχ) nearest to lχ as the target label tχ. So in this
circumstance, We assume lχ = lχ and Elx〜P[Di(lχ)] = Di ^ . Therefore, tχ is
tχ = arg min D"x if Pi > δι.	(2)
i6=lx,i=1,...,n
When pi is smaller than the threshold δι, we are not sure whether lχ is equal to lχ. Instead, we
sample some labels and compute the Weighted distance betWeen each label i and these labels. We
sample all labels whose probability are larger than a threshold δ2 and we use M to represent the
number of sampled labels. We think the input image might belong to one of these M labels. The
labels whose probability are smaller than δ2 will be abandoned because we think the input image
can hardly fall into these categories. The weight and the distance is provided by the vector P and
matrix D respectively. So in this circumstance, as we are not sure which label is the ground truth, we
want to find a target label which has the minimum expected distance with all these possible labels.
Therefore, the value of Elx〜P[Di(lχ)] can be approximated as X `pj ∙ Dij and the target label
tx is shown in Eq. (3). This can be explained by Importance Sampling (Owen & Zhou, 2000)
because it is hard to sample from the real probability distribution P. We can only use the probability
distribution P which is an approximation of P to estimate the value of Elx〜P [Di(lχ)].
M
tχ = arg min XPj ∙ Dij if Pi ≤ δι	(3)
i=i,...,n j=i
In conclusion, the whole strategy for choosing the target label tx of an input image x is computed
as Eq. (4). The target label tχ minimizes Elx〜P[Di(lχ)] just as Figure 2 shows.
{argmin D, ^	if pi > δι
i6=lx,i=i,...,n
M	(4)
arg min 工 Pj ∙ Dij otherwise
i=i,...,n j=i
3.2 Generate Adversarial Samples
After having the target label, the second step is to attack the input image to be mis-classified as
this target label. It’s easy to achieve by taking the target label as a parameter and putting it into
the targeted-attack algorithm such as targeted-FGSM (Goodfellow et al., 2015) and targeted-CW
4
Under review as a conference paper at ICLR 2020
Figure 2: Pipeline of our method. There are two steps. In the first step, we first compute the
distance Di,j between every two class i, j in a n-classes dataset. Then we choose the target label tx
for an input image X by two strategies according to the value of p^ι. The second step is to attack
the input into this target label. Solid lines are the real boundaries between the current label and the
indicated label and dashed lines with notes F are the approximate two-dimensional boundaries. Red
indicates the target label while blue indicates other labels. Our method moves the input towards the
boundary Ftx until it is classified as tx .
(Carlini & Wagner, 2017), but this operation may suffer huge loss in the image space because of
large perturbations.
Inspired by DeepFool (Seyed-Mohsen et al., 2016), we propose a method which can not only attack
an input image to be mis-classified as the target label successfully, but also ensure tiny perturbations
in the image space. The mathematical derivation in this step is similar to DeepFool (Seyed-Mohsen
et al., 2016) and the only difference is that, we have a target label chosen in the first step while
DeepFool doesn’t. As introduced in DeepFool (Seyed-Mohsen et al., 2016), a high dimensional
classification boundary can be approximated by a line in two dimensions. As shown in Figure 2, for
an image x0 , Fj represents the 2D approximated boundary between its current predicted class and
class j and tx is the target class we choose in the first step. In the first iteration, we move x0 towards
Ftx and get a new point x1 . The direction of movement is perpendicular to Ftx . The distance of
the movement is the vertical distance from x0 to Ftx. If the predicted label lx1 of the new point
equals lx0, the classification boundaries are the same as those before moving x0. Otherwise, the
classification boundaries change. No matter whether the boundaries change or not, we repeatedly
move the current point towards Ftx until it is classified as label tx or the maximum number of
iterations has been reached. A pseudocode of the second step is shown in Algorithm 1.
Algorithm 1: Generate Adversarial Samples
Input: image x, classifier f, target label tχ	5: r一 |g| W
Output: Adversarial image X	i ∣∣wk2
1: initialize χo - χ,i - 0	6:	Xi+1 - Xi + ri, i - i + 1
2: while lχi = tχ and i < max_iter do	7: end while
3:	w — Vf^ (Xi) — Nftx(Xi)	8: return X = Xi+1 xi
4:	g — flx" (Xi) — ftx (Xi)
5
Under review as a conference paper at ICLR 2020
4	Experiments
In this paper, all experiments are conducted on ImageNet. ImageNet provides the CLS-LOC dataset
for classification tasks. Its train split contains about 1300 thousand images. There are 50 thousand
validation images and 100 thousand test images. Our experiments are conducted on the train split
of CLS-LOC dataset which will be noted as ImageNet-train split in the following part. We perform
extensive experiments to show LabelFool can satisfy all three levels of requirements as an attacker.
First we demonstrate the deceptiveness of samples generated by LabelFool to humans in the label
space through a subjective experiment. Then we calculate the perceptibility and image quality of
adversarial samples to show there is not much loss in the image space even compared to DeepFool
(Seyed-Mohsen et al., 2016), which is the state-of-the-art method in the image space. Finally, we
conduct attacks on several models to prove the exceptional ability of our method on fooling neural
networks which is the first requirement for an ideal attack.
4.1	Imperceptibility in the Label Space
Setup. In this part, we will compare LabelFool with three attack methods: DeepFool, FGSM and
SparseFool. We first sample 600 source images from ImageNet-train split randomly, and these
images are in different classes. Each source image will derive four adversarial images and a base-
line image, namely DeepFool-attacked image, LabelFool-attacked image, FGSM-attacked image,
SparseFool-attacked image and clean image. Each adversarial image has its mis-classified label and
each baseline image has the truth label. We then use term “puzzles” to describe the combination of
an image and its label, for 5 × 600 = 3000 puzzles. A human observer needs to determine whether
the label is correct for the image, answering “True” or “False” for each puzzle. To eliminate ob-
servers’ memory effects, we split 3000 puzzles into five groups, ensuring that 600 images in one
group come from different source images. An interface presentation of our subjective experiment is
shown in Appendix A. We have 10 observers (3 females and 7 males, age between 20-29) to do our
subjective experiment.
Evaluation. In this paper, we define an index named Performance Gain (PG) as the evaluation
index. Human observers answer “True” or “False” for each puzzle, and the rate with which they
answer incorrectly is called Confusion Rate (CR). So every observer has a CR for each attack method
or baseline. It is an absolute indicator demonstrating how much observers are confused by a set of
puzzles. But doing arithmetic on CR of different observers is meaningless, as different observers
have different baseline results. So we define a relative indicator called Performance Gain (PG),
which demonstrates how much improvement in the confusion rate after attacking comparing with
baseline. It is a kind of normalization. The formula for PG is shown in Eq. (5), where CRA means
the confusion rate of an attacker and CRB means the confusion rate of baseline. The higher PG an
attacker has, the better it confuses people in the label space.
CR	CRA - CRb( A .	…	小
PGA = ------——------ ( AiSan attacker, B = Baseline)	(5)
CRB
Results. We report the average performance gain of 10 observers in total images in the left of Figure
3. As a whole, there is a huge improvement compared with FGSM and SparseFool, about 25 percent
improvement and 30 percent improvement respectively. Compared with DeepFool, the gap in PG
is a little smaller because DeepFool finds the nearest class in the feature level and features usually
reflect images’ perceptual information as we introduced in Section 2. But there is still 3 percent
improvement in performance gain.
As animal classes are more fine-grained, the effects of the imperceptibility in the label space become
more pronounced. We attack an animal image so that its true animal label changed into a similar
animal label, it is difficult for humans to notice that our attack is taking place. Meanwhile, other
attacks change the label into an obviously-incorrect label such as a non-animal category or another
species (Some examples in Appendix B). In our subjective experiment, there are 247 animal images
out of 600 source images. The right graph in Figure 3 shows the average performance gain of 10
observers in animal images. The accurate data for both graphs in Figure 3 is shown in Appendix C.
As for animal images, the improvement is very obvious comparing with all three attack methods.
There are about nearly 90 percent improvement in performance gain comparing with FGSM and
6
Under review as a conference paper at ICLR 2020
£eo gUelU∙!OJ∙llud
1.10
DeePFool Label Fool FGSM SParseFool
50505050
20752075
♦ ♦♦♦♦♦♦-
44333322
UeE」。七d
Animal
DeeP Fool Label Fool
FGSM SparseFool
Figure 3: A line chart for average performance gain of 10 observers. The horizontal axis represents
four attack methods. The vertical axis represents the mean value of 10 human observers’ perfor-
mance gain. The graph in the left is for total results, and the right one is for animal images.
SparseFool. A significant improvement can also be seen when comparing with DeepFool, there are
about 50 percent improvement in average performance gain.
4.2	Imperceptibility in the Image Space
In this subsection, we will show our performance in the image space to demonstrate that our
improvement in the label space is not at the cost of huge loss in the image space. We use
three metrics to evaluate performance in the image space. One is perceptibility which is simi-
lar to the definition in previous works (Szegedy et al., 2013; Seyed-Mohsen et al., 2016) :p
1
WN X HN
XWN
w=1
XHN
h=1
k∆yw,hk2, where yw,h is a 3-dimensional vector representing the RGB
intensities (normalized in [0, 1]) of a pixel. The other two are perceptual similarity (Zhang et al.,
2018) and PieAPP (Prashnani et al., 2018). These two are metrics for image quality. Perceptual
similarity measures the perceptual distance between an image and its reference image while PieAPP
measures the perceptual error. In this paper, the reference image is the clean image. And the smaller
these three metrics are, the better the adversarial samples are.
Perceptibility
3.00E-03
Perceptual Similarity
0.5
I I LL I I LI
ResNet-50 ReSNet-34 VGG_19bn AlexNet	ReSNet-50 ReSNet-34 VGG_19bn AlexNet
■ DeepFool HLabelFool FGSM ,SparseFool	,DeepFool HLabelFool FGSM ,SparseFool
Figure 4: Mean value of perceptibility, perceptual similarity and PieAPP for adversarial samples
generated by different attack methods on different models.
We randomly choose 1000 images from ImageNet and attack the classifier to generate 1000 adver-
sarial samples. Then we compute mean value for these adversarial samples of three metrics. In this
experiment, we test four classifiers: ResNet-34, ResNet-50, VGG-19 (with batch normalization)
(Simonyan & Zisserman, 2014) and AlexNet (Krizhevsky et al., 2012a). The results are shown in
Figure 4 whose original data are reported in Appendix C. We can see although LabelFool is signifi-
cantly better than FGSM and SparseFool, it is still a little worse than DeepFool in all three metrics.
However, visual results (Figure 5) indicate that human observers can not notice the difference be-
tween LabelFool and DeepFool in the image space as the metric value is on such a small scale.
4.3	Fool Networks
We will show attack rate in the last experiment which is the most fundamental requirement for
an attacker. Results are shown in Table 2. The results are the average value of three groups of
7
Under review as a conference paper at ICLR 2020
Figure 5: Two visual results of adversarial samples generated from AlexNet in the image space.
In each result, the left image is the clean image, the middle one is DeepFool-attacked adversarial
sample and the right one is LabelFool-attacked adversarial sample. Above the images are the true
label/ the label after attacked. The three metrics of adversarial samples are reported in the table
below them.
goose	black grouse	crane
Clean Image
Perceptibility
Perceptual Similarity
PieAPP
DeepFool-Attacked
3.3e-5
0.02
0.07
LabelFool-Attacked
3.6e-5
0.03
1.01
Table 2: Attack rate of different methods on different models.
Model	DeepFool	LabelFool	FGSM	SparseFool
ResNet-34	92.67%	97.50%	95.03%	92.60%
ResNet-50	93.08%	97.88%	95.09%	92.53%
VGG-19(bn)	92.03%	97.48%	94.59%	83.70%
AlexNet	90.35%	97.38%	96.44%	89.11%
experiments. Each group has 1000 original images from ImageNet, we use these original images to
generate adversarial images for four models respectively. We surprisingly find that LabelFool has
the highest attack rate on all models comparing with other methods. This might benefit from our
probability model which is used to choose the target label. Because in our strategy, When p^ι ≤ δι,
we do not use the predicted label as the ground truth like other methods do. Instead, we consider
all labels whose probability are larger than δ2 and choose the label nearest to all these labels as the
target label. This operation can avoid some mistakes and improve the attack rate when the classifier
doesn’t give a correct classification result. An example is shown in Appendix D.
5	Conclusion and Further Discussion
Conclusion. In this study, we pay attention to tiny perturbations in the label space. To our best
knowledge, we are the first one who points out the importance of the imperceptibility in the label
space for adversarial samples. Furthermore, we explore a feasible method named LabelFool to
identify a target label “similar” with an input image’s ground truth and perturb the input image to
be mis-classified as this target label so that a human observer will overlook the misclassification
and lower the vigilance of defenses. Our experiments show that, while LabelFool is a little behind
DeepFool in the image space, it is much imperceptible in the label space to human observers. Since
we adopt Importance Sampling instead of MLE only in traditional method, the success rate of attack
also get gains.
Further discussion. In this paper, we just propose a feasible way to generate adversarial sam-
ples which can confuse people in the label space. However, there is room for improvement in our
approach. Our results provide the following avenues for future research.
•	The perceptual features can be optimized by a well-designed loss function which can im-
prove the accuracy rate in finding nearest label ulteriorly.
•	We only consider perceptual distance in this paper, but semantic distance also has its signif-
icance for reference of confusing people in the label space. We may take the semantic tree
into consideration and make a trade off between perceptual distance and semantic distance
in future research.
8
Under review as a conference paper at ICLR 2020
References
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE
Symposium on Security and Privacy, pp. 39-57. IEEE, 2017.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 248-255. Ieee, 2009.
Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boost-
ing adversarial attacks with momentum. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 9185-9193, 2018.
Ian Goodfellow, Shlens Jonathon, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference of Learning Representation, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770-778, 2016.
Jeff Henrikson. Completeness and total boundedness of the hausdorff metric. MIT Undergraduate
Journal of Mathematics, 1:69-80, 1999.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097-1105,
2012a.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097-1105,
2012b.
Yann LeCun, Koray KavUkcUoglu, and Clement Farabet. Convolutional networks and applications
in vision. In Proceedings of 2010 IEEE International Symposium on Circuits and Systems, pp.
253-256. IEEE, 2010.
Tsung Yu Lin, Aruni Roychowdhury, and Subhransu Maji. Bilinear cnn models for fine-grained
visual recognition. In IEEE International Conference on Computer Vision, 2016.
Apostolos Modas, Moosavi-Dezfooli Seyed-Mohsen, and Ecole Polytechnique Federale de Lau-
sanne Pascal Frossard. Sparsefool: a few pixels make a big difference. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2019.
S. Moosavi-Dezfooli, O. Fawzi A. Fawzi, and P. Frossard. Universal adversarial perturbations. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.
Art Owen and Yi Zhou. Safe and effective importance sampling. Journal of the American Statistical
Association, 95(449):135-143, 2000.
Johann Pfanzagl. Parametric statistical theory. Walter de Gruyter, 2011. ISBN 3-11-013863-8.
Ekta Prashnani, Hong Cai, Yasamin Mostofi, and Pradeep Sen. PieAPP: Perceptual image-error
assessment through pairwise preference. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 1808-1817, 2018.
Seyed-Mohsen, Moosavi-Dezfooli, Pascal Frossard Alhussein Fawzi, and Ecole Polytechnique Fed-
erale de Lausanne. DeepFool: a simple and accurate method to fool deep neural networks. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep
neural networks. IEEE Transactions on Evolutionary Computation, 2019. doi: 10.1109/TEVC.
2019.2890858.
9
Under review as a conference paper at ICLR 2020
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R Scott. Multi-similarity loss
with general pair weighting for deep metric learning. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5022-5030, 2019.
Kaidi Xu, Sijia Liu, Pu Zhao, Pin Yu Chen, Huan Zhang, Quanfu Fan, Deniz Erdogmus, Yanzhi
Wang, and Xue Lin. Structured adversarial attack: Towards general implementation and better
interpretability. In International Conference of Learning Representation, 2019.
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 586-595, 2018.
A An interface presentation of the subjective experiment
Figure 6 shows the interface of our subjective experiments.
Note: Please determine whether the label given is the correct label for the picture.
spaghetti squash
I Correct ∣∣ ln∞rrect j
If you don't know about this label, please click on the "Show Reference Pictures" button.
Show Reference Pictures
Figure 6:	Interface presentation of our subjective experiment.
B Examples for animal classes
Figure 7 shows three examples for animal classes to demonstrate that LabelFool makes fine-grained
changes but other methods make some ridiculous changes instead.
Image	Ground truth	DeepFool	LabelFool	FGSM	SparSeFool
^w	CoCk	partridge	hen	partridge	partridge
	peacock	sea urchin	gallinule	wool	poncho
	fire salamander	sea slug	spotted salamander	coral reef	courgette
					
Figure 7:	Three examples for animal images. The first column shows the clean image. The second
column shows the ground truth label and other columns show the label after attacked.
10
Under review as a conference paper at ICLR 2020
C Original data for Figure 3 and 4
Table 3 is the original data for Figure 3. The original data of perceptibility, perceptual similarity,
PieAPP in Figure 4 is reported in Table 4, 5, 6 respectively. It is provided for the sake of convince if
anyone wants to rewrite Figure 3 or 4.
Table 3: Data for Figure 3
Index		DeePFool	LabelFool	FGSM	SParseFool
Total	Performance Gain	1.68	1.71	1.39	1.34
Animal	Performance Gain	3.55	4.04	3.11	3.00
Table 4: Perceptibility Data for Figure 4
Model	DeepFool	LabelFool	FGSM	SparseFool
ResNet-50	3.36E-05	4.75E-05	6.55E-04	2.56E-03
ResNet-34	3.35E-05	4.84E-05	6.59E-04	2.56E-03
VGG-19bn	3.18E-05	3.80E-05	6.44E-04	2.56E-03
AlexNet	3.59E-05	7.24E-05	6.68E-04	2.56E-03
Table 5: Perceptual Similarity Data for Figure 4
Model	DeepFool	LabelFool	FGSM	SparseFool
ResNet-50	1.48E-3^^	^^5.64E-3~~	0.20	0.42
ResNet-34	1.39E-3^^	^^5.91E-3~~	0.20	0.41
VGG-19bn	6.86E-4^^	^^2.54E-3~~	0.18	0.41
AlexNet	1.34E-2^^	^^3.50E-2^^	0.30	0.40
Table 6: PieAPP Data for Figure 4
Model	DeepFool	LabelFool	FGSM	SparseFool
ResNet-50	007	0.14	1.35	1.57
ResNet-34	007	0.15	1.31	1.46
VGG-19bn	005	008	1.27	1.59
AlexNet	007	021	1.28	1.21
D An example for Section 4.3
This is an example to illustrate why our method has the highest attack rate. We only give an example
of DeepFool and LabelFool. SparseFool and FGSM have similar effects with DeepFool.
Figure 8 is an example where the classifier fail to give a correct classification for the input image x.
The ground truth of x is class 2 while the predicted class is class 3. In this example, DeepFool takes
class 3 as the true class. Then DeepFool finds the nearest class to class 3 in the feature space which
is class 2 in this example, and moves the input image towards class 2. When the perturbed image
is classified as class 2 which is different from the predicted class, DeepFool considers the attack
11
Under review as a conference paper at ICLR 2020
succeed and stops the algorithm. However, it fails to attack actually because class 2 is the true class
of x.
Different from DeepFool, LabelFool sample top 3 classes in this example because their probabilities
are larger than 0.01 and compute the expected distance between each class in the dataset and these 3
classes (Eq.(3)). Finally, LabelFool choose class 394 as the target class because it has the minimum
expected distance with top 3 classes. By moving x towards class 394, LabelFool attacks successfully.
Ground Truth Class: 2
0.70
0.53
0.6746
Ai三qeqo」d
0.0156
t 0.0050 0.0027 0.0016 0.0014 0.0010 0.0007 0.0003
Class
3	2	4	391	394	6	390	395	389	973
classifier: VGG_19
(with batch normalization)
Figure 8: The line chart in the right part shows the top 10 elements in p^, When an image X whose
ground truth class is class 2 is given into the classifier VGG_19bn. The horizontal axis represents
the class and the vertical axis represents the probability that x belongs to this class. The predicted
class lx is class 3 and it means the classifier fail to give a correct classification.
E Supplementary Examples for LabelFool
Figure 9 shows some supplementary examples to show what LabelFool actually does.
Image	Ground truth	DeepFool	LabelFool	FGSM	SparSeFool
K	tench	sea cucumber	coho	barracouta	barracouta
一	whiptail lizard	banded gecko	alligator lizard	stole	damselfly
!!	harvestman	wolf spider	long-horned beetle	wolf spider	wolf spider
E∣P	beer bottle	PoP bottle	pop bottle	pop bottle	wine bottle
IS	chain saw	lumbermill	power drill	lumbermill	lumbermill
	liner	dock	container ship	dock	dock
Figure 9: Some other examples for illustrating what LabelFool does. The first column shows the
clean image. The second column shows the ground truth label and other columns show the label
after attacked.
12
Under review as a conference paper at ICLR 2020
F	An application: Face recognition
Figure 10 is an application to show it is necessary to generate imperceptible adversarial examples in
the label space even for image classification tasks. We take face recognition system for entrance as
an example. In Figure 10, A is the person who is using the face system to go into the gate. LabelFool
aims to let the system misclassify A and B who is the one looks like A, but other untargeted attacks
may let the system misclassify A an C who looks totally different from A. The attack is easy to
be detected by the guard if the system misclassifies A and C, but it is hard to detect if the system
misclassifies A and B. Letting a fake B in will bring great potential risks to security and safety.
the person entering the gate
A
LabelFool's target id
B
Other methods' target id
C
Figure 10: A is the person who is using a face system to enter the gate. Green lines represent what
LabelFool aims to do, that is to miscalssify A and B who looks like A. Red lines represent what
other untargeted attacks do. They misclassify A and C who looks totally different from A and this
error is easy to be detected by the guard.
13