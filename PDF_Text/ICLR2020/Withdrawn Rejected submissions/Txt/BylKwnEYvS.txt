Under review as a conference paper at ICLR 2020
Star-Convexity in Non-Negative Matrix Fac-
TORIZATION
Anonymous authors
Paper under double-blind review
Ab stract
Non-negative matrix factorization (NMF) is a highly celebrated algorithm for ma-
trix decomposition that guarantees non-negative factors. The underlying optimiza-
tion problem is computationally intractable, yet in practice gradient descent based
solvers often find good solutions. This gap between computational hardness and
practical success mirrors recent observations in deep learning, where it has been
the focus of extensive discussion and analysis. In this paper we revisit the NMF
optimization problem and analyze its loss landscape in non-worst-case settings. It
has recently been observed that gradients in deep networks tend to point towards
the final minimizer throughout the optimization. We show that a similar property
holds (with high probability) for NMF, provably in a non-worst case model with
a planted solution, and empirically across an extensive suite of real-world NMF
problems. Our analysis predicts that this property becomes more likely with grow-
ing number of parameters, and experiments suggest that a similar trend might also
hold for deep neural networks — turning increasing data sets and models into a
blessing from an optimization perspective.
1	Introduction
Non-negative matrix factorization (NMF) is a ubiquitous technique for data analysis where one at-
tempts to factorize a measurement matrix X into the product of non-negative matrices U, V (Lee
and Seung, 1999). This simple problem has applications in recommender systems (Luo et al., 2014),
scientific analysis (Berne et al., 2007; Trindade et al., 2017), computer vision (Gillis, 2012), inter-
net distance prediction (Mao et al., 2006), audio processing (Schmidt et al., 2007) and many more
domains. Often, the non-negativity is crucial for interpretability, for example, in the context of crys-
tallography, the light sources, which are represented as matrix factors, have non-negative intensity
(Suram et al., 2016).
Like many other non-convex optimization problems, finding the exact solution to NMF is NP-hard
(Pardalos and Vavasis, 1991; Vavasis, 2009). NMF’s tremendous practical success is however at
odds with such worst-case analysis, and simple algorithms based upon gradient descent are known
to find good solutions in real-world settings (Lee and Seung, 2001). At the time when NMF was
proposed, most analyses of optimization problems within machine learning focused on convex for-
mulations such as SVMs (Cortes and Vapnik, 1995), but owing to the success of neural networks,
non-convex optimization has experienced a resurgence in interest. Here, we revisit NMF from a
fresh perspective, utilizing recent tools developed in the context of optimization in deep learning.
Specifically, our main inspiration is the recent work of Kleinberg et al. (2018) and Zhou et al. (2019)
that empirically demonstrate that gradients typically point towards the final minimizer for neural
networks trained on real-world datasets and analyze the implications of such convexity properties
for efficient optimization.
In this paper, we show theoretically and empirically that a similar property called star-convexity
holds in NMF. From a theoretical perspective, we consider an NMF instance with planted solution,
inspired by the stochastic block model for social networks (Holland et al., 1983; Decelle et al.,
2011) and the planted clique problem studied in sum-of-squares literature (Barak et al., 2016). We
prove that between two points the loss is convex with high probability, and conclude that the loss
surface is star-convex in the typical case — even if the loss is computed over unobserved data.
From an empirical perspective, we verify that our theoretical results hold for an extensive collection
1
Under review as a conference paper at ICLR 2020
Figure 1: A non-convex loss surface is illustrated in a). In general, the loss will be non-convex on
straight paths connecting random points x。, Xb and the global minimizer x*. We consider a model
of NMF with a planted solution, and as shown in b) the loss is typically convex on straight paths
between points xa and a planted solution x* . Additionally, as illustrated in c), the loss is typically
convex on straight paths between points xa and xb .
of real-world datasets spanning collaborative filtering (Zhou et al., 2008; Kula, 2017; Harper and
Konstan, 2016), signal decomposition (Zhu, 2016; Li and Ngom, 2013; Li et al., 2001; Erichson
et al., 2018) and audio processing (Flenner and Hunter, 2017; canto Foundation), and demonstrate
that the star-convex behavior results in efficient optimization. Finally, we show that star-convex
behavior becomes more likely with growing number of parameters, suggesting that a similar result
may hold as neural networks become wider. We provide supporting empirical evidence for this
hypothesis on modern network architectures.
2	NMF and star-convexity
The aim of NMF is to decompose some large measurement matrix X P Rn X m into two non-negative
matrices U P R'" and V P R''m SUCh that X « UV. The canonical formulation of NMF is
JminO 'PU, V), where '(U, V) “ 2}UV ´ X}F	(1)
NMF is commonly used in recommender systems where entries pi, jq of X for example correspond
to the rating user i has given to movie j (Luo et al., 2014). In such settings, data might be missing
as all users have not rated all movies. In those cases, it is common to only consider the loss over
ʌ
observed data (Zhang et al., 2006; Candes and Recht, 2009). We let Ipijq be an indicator variable
that is 1 if entry (i, j) is "observed" and 0 otherwise. The loss function is then
Um⅛ '(U, V) = 1 ∑ ips ^[UV‰ij ´ Xij) 2	⑵
NMF is similar to PCA which admits spectral strategies; however, the non-negative constraints in
NMF prevent such solutions and result in NP-hardness (Vavasis, 2009). Work on the computational
complexity of NMF has shown that the problem is tractable for small constant dimensions r via
algebraic methods (Arora et al., 2012). In practice, however, these algorithms are not used, and
simple variants of gradient descent, possibly via multiplicative updates (Lee and Seung, 2001), are
popular and are known to work reliably (Koren et al., 2009). This gap between theoretical hardness
and practical performance is also found in deep learning. Optimizing neural networks is NP-hard
in general (Blum and Rivest, 1989), but in practice they can be optimized with simple stochastic
gradient descent algorithms to outmatch humans in tasks such as face verification (Lu and Tang,
2015) and playing Atari-games (Mnih et al., 2015). Recent work on understanding the geometry of
neural network loss surfaces has promoted the idea of convexity properties. The work of Izmailov
et al. (2018) shows that the loss surface is convex around the local optimum, while Zhou et al. (2019)
and Kleinberg et al. (2018) show that the gradients during optimization typically point towards the
local minima the network eventually converges to. Of central importance in this line of work is
star-convexity, which is a property of a function f that guarantees that it is convex along straight
paths towards the optima x* . See Figure 2 for an example. Formally, it is defined as follows.
2
Under review as a conference paper at ICLR 2020
Definition 1. A function f : Rn → R is star-convex towards x * if for all λ P [0,1] and x P Rn,
we have f'λx + (1 — λ) x*) W λf(x) + (1 — Xqfpx*).
Optimizing star-convex functions can be done in polynomial
time (Lee and Valiant, 2016), in Kleinberg et al. (2018) it is
shown how the function only needs to be star-convex under a
natural noise model. NMF is not star-convex in general as it is
NP-hard, however, it is natural to conjecture that NMF is star-
convex in the typical case. Such a property could explain the
practical success of NMF on real-world datasets, which is not
worst-case. This will be the working hypothesis of this paper,
where the typical case is formalized in Theorem 1. Indeed, one
can verify numerically that NMF is typically star-convex for
natural distributions and realistically sized matrices, see Figure
1 where we consider a rank 10 decomposition of (100, 100q-
matrices with iid half-normal entries and a planted solution,
sampled as per Assumption 1 given in the next section. Fol-
lowing sections will be dedicated to proving that NMF is star-
convex with high probability in a planted model, and to con-
firm that this phenomenon generalizes to datasets from the real
world, which are far from worst-case.
Figure 2: The function (|x|p +
|y|pq1{p is an example of a star-
convex function for 0 < P < 1. It
is non-convex in general, but con-
vex towards (0, 0q.
3 Proving typical-case star-convexity
Our aim now is to prove that the NMF loss-function is typically star-convex for natural non-worst-
case distribution of NMF instances. We will consider a slighly weaker notation of star-convexity
where f'λx +(1 — λ) x*) W λf (x) + (1 — λ)f (x*) holds not for all x, but for random X with
high probability. This is in fact the best achievable, an NMF instance with u1 “ 1, u* “ 0 and
v1 “ 0, v* “ 1 is not star-convex. Our results hold with high probability in high dimensions,
similar to Dvoretzky’s theorem in convex geometry (Dvoredsky, 1961; Davis).
Inspired by the stochastic block model of social networks (Holland et al., 1983; Decelle et al., 2011)
and the planted clique problem (Barak et al., 2016), we will focus on a setting with a planted random
solution. In section 4 we verify conclusions drawn from this model transfers to real-world datasets.
We will assume that there is a planted optimal solution (U*, V*q, where entries of these matrices
are sampled iid from a class of distributions with good concentration properties that include the half-
normal distribution and bounded distributions. As is standard in random matrix theory (Vershynin,
2010), we will develop non-asymptotic results that hold with a probability that grows as the matrices
of shape (n, rq and (r, mq become large. For this reason, we will need to specify how r and m
depend on n.
Assumption 1. For (U, V) P RnXr ^ Rrxm we assume that the entries of the matrices U, V are
sampled iid from a continuous distribution with non-negative support that either (iq is bounded or
(ii) can be expressed as a 1-Lipschitz function of a Gaussian distribution. As n → 8, we assume
that r grows as nγ up to a constant factor for γ P r1{2, 1s, and m as n up to a constant factor.
We are now ready to state our main results, that the loss function equation 1 is convex on a straight
line between points samples as per Assumption 1, and thus satisfy our slightly weaker notion of
star-convexity, with high probability. The probability increases as the size of the problem increases,
suggesting a surprising benefit of high dimensionality. We also show similar results for the loss
function of equation 2 with unobserved data under the assumption that the event that any entry is
observed occurs independently with constant probability p. Below we sketch the proof idea and key
ingredients, the formal proof is given in Appendix D.
Theorem 1. (Main) Let matrices U1, V1, U2, V2, U*, V* be sampled according to Assumption
1. Then there exists positive constants ci, c?, such that with probability ≥ 1 — ci exp(—c2n1{3),
the loss function '(U, V) in equation 1 is convex on the straight line (Ui, Vi) → (U2, V2). The
same holds along the line (Ui, Vi) → (U*, V*). It also holds if any entry (i, j) is observed
independently with constant probability P, but with probability ≥ 1 — Ci exp(—c2ri{3).
3
Under review as a conference paper at ICLR 2020
3.1 Proof strategy
Let Us parametrize the NMF solution one gets along the line (U2, V2) → (U 1, Vi) as
ʌ
X (λq =
λU1 ` (1 ´
λ)U2
λV1 ` (1 ´
Proving Theorem 1 amounts to showing that the loss function '(λ) “ ɪ}X(λ) — X}F is convex
in λ with high probability, our strategy is to show that its second-derivate is non-negative. For
fixed matrices Ui, U2, U*, Vi, V2, V*, the function '(λ) is a fourth-degree polynomial in λ, so
its second derivate w.r.t. λ will be a second-degree polynomial in λ. For a general second-degree
polynomial p(x) “ ax2 ' bx ' C We have p(x) “ ɪ “'ax ' b) ' 'ac — b2)‰. If a > 0, which in
the case here (see Appendix D), showing that it,s positive could be done by showing ac》b2. This
is equivalent to showing that
2}W2}F(}Wi}F + 2χW0, W2〉) > 3'〈Wi, W2〉)2	(3)
Where the matrices W0, Wi, W2 are given as Wo = U2V2 — U*V*, Wi = 'Ui — U2)V2 +
U2 Vi — V2 , W2 “ Ui — U2 Vi — V2 . By slight abuse of notation, we have used 〈A, B〉 to
denote Tr(ABT ) for matrices A, B of the same shape. By exchanging terms in equation 3 by their
means one gets
2(4rmnσ4)'6rmnσ4 + 4rmnμVarσ2 + 2rmnσ4)23' — 4rmnσ4)2	(4)
Here, σ2 is the variance of the distribution of the entries in the matrices, while μ is the mean. By
just counting terms of order (rmnσ4)2, we see that the LHS has 64 such terms while the RHS has
only 48. Thus, if all matrices W0 , Wi and W2 would exactly be equal to their mean, inequality
equation 3 would hold. In proving that it holds in general, we will use concentration of measure
results from random matrix theory to show that the terms will be concentrated around their means
and that large deviations are exponentially unlikely.
Concentration of measure Consider the matrix W2 “ Ui — U2 Vi — V2 . Given that all
matrices are iid we can center the variables such that W2 “ (Ui — U2) (Vi — V2) “ (Ui —
U2) (Vi — V2), where the bar denotes the centered matrices. The term } W2 }F can then be written
as Tr (V i — V 2 )T'U i — U 2)T (U i — U 2)'V i — V 2). Given that all matrix entries are independent
as per Assumption 1, we would expect some concentration of measure to hold. Bernstein-type
inequalities turns out to be too weak for our purposes, but the field of random matrix theory offer
stronger results for matrices with independent sub-Gaussian entries (Ahlswede and Winter, 2002;
Tropp, 2012; Guionnet et al., 2000; Meckes and Szarek, 2012). Via such results one can achieve the
following concentration result, see Appendix D.
P'∣}W2}F — E[}W2}F‰∣ > trn2) ≤ c3 exp ' — c4 min(t2,ti/2) n)	(5)
where c3 , c4 are positive constants. In some expression we will however not be able to center all
variables, and for such expression one gets similar but slightly weaker concentration results where
the exponent scales as ni{3 instead ofn, see Appendix D.
Proof sketch Given that E }W2 }2F “ 4rmnσ4 , equation 5 says that the probability that the term
deviates from its mean by a relative factor is less than c3 exp — c52n for some small . By
applying similar arguments to terms 〈W0, W2〉 and 〈Wi, W2〉, one can show that the probability
that they will deviate by a relative factor is less than c6 exp(—c72ni{3). A problematic term
is }Wi}2F which contains a term of the type Tr 'Vi — V2)τμTμi'Vi — V2) which has weak
concentration properties. Matrices of type ATA are psd, and psd matrices have non-negative trace.
Hence, this term will be non-negative, and since it occurs on the LHS of equation 3, we can simply
omit it to lower bound the convexity. Using union bound, we bound the probability that at least on
term deviate with a relative factor by ci exp(—c82ni{3) for positive constants ci, c8. Now, set
“ 0.01 If neither variable deviates by a factor of more than 0.01, then equation 10 still holds since
0.992 * 6421.012 * 48. Thus, inequality is violated with probability at most ci exp (— c2ni{3} for
positive ci , c2 .
4
Under review as a conference paper at ICLR 2020
Figure 3: We here illustrate the loss surface of NMF on straight paths connecting two random points
for 8 real-world datasets. We overlap 5 independent lines for each dataset. Note that the curves are
always convex, suggesting that the loss surface is "typically" convex.
Table 1: Dataset details. References contain suggested rank r. See Appendix A for details.
name	shape (n, m, rq	sparsity	reference
birdsong	(5120, 1246, 88q		(Flenner and Hunter, 2017)
extragalactic	(2760, 2820, 10q		(Zhu, 2016)
goodbooks	(10000, 43461, 50q	0.0022	(Kula, 2017)
metabolic	(9335, 36, 3q		(Li and Ngom, 2013)
movielens	(3953, 6041, 20q	0.0419	(Harper and Konstan, 2016)
netflix	(47928, 8963, 20q	0.0121	(Zhou et al., 2008)
ORL faces	(400, 10304, 49q		(Li et al., 2001).
satellite	(162, 94249, 4q		(Erichson et al., 2018).
Proof sketch for unobserved data If the entries in equation 2 are "observed" independently with
probability p, for fixed matrices Ui, U2, U*, Vi, V2, V* such that Theorem 1 hold, We have
E['2(λ)s = El£ 1(i,j) (X12j + X2ij(Xij ´ Xijq
ij
ij
“ “ P ∑ 3 1ij + X2ij (Xij´ Xij)) > 0
ij
ʌ ʌ
ʌ ʌ
Thus, the expectation of '2(λ) is convex. To show that it is convex with high probability, one first
observes that With high probability, no entry (i, jq in `2 (λq is particularly large. Assuming this
holds via union bound, for fix matrices Ui, U2, U*, Vi, V2, V* with elements that are "observed"
independently with probability P, one gets that `2 (λq is concentrated around its convex mean via
Hoeffding bounds (Hoeffding, 1994).
4	Experiments
4.1	Verifying theoretical predictions
To verify that the conclusions from our planted model hold more broadly, we now consider real-
world datasets previously studied in NMF literature. Some have ranks outside the scope of our
theoretical model but still display star-convexity properties, indicating that it might be a general
phenomenon. We focus on a handful of representative datasets spanning image analysis, scientific
applications and collaborative filtering. In Table 1, we list these datasets together with references
and sparsity, the decomposition rank we use is based upon values previously used in the literature.
We perform a non-negative matrix factorization via gradient descent, starting with randomly initial-
ized data. To enable comparison between datasets, we scale all data matrices so that the variance of
5
Under review as a conference paper at ICLR 2020
Figure 4: The loss surface of NMF along the straight line from a random point w0 to the local optima
w* found via gradient descent from an independent starting point. We overlap 5 independent lines,
zoom in for detail. For all datasets, this loss surface is convex, which is in line with what one would
expect from Theorem 1.
observed entries is 1 and divide the loss function by the number of (observed) entries. The initializa-
tion is performed with half-normal distribution scaled so that the means match with the dataset. For
simplicity We use the same gradient descent parameters for all datasets, a learning rate of 1e—7 with
1e4 gradient steps which gives convergence for all datasets. For the collaborative filtering datasets
(movielens, netflix, and goodbooks) we have unobserved ratings, as is standard in NMF we only
compute the loss over observed ratings (Zhang et al., 2006). In Figure 3, we plot the loss function
between two random points drawn from the initialization distribution and see that the loss is convex.
In Figure 4 we plot the loss function from an initialization point to an independent local optima.
These results agree with our planted model; the NMF loss-surface of real-world datasets seem to be
largely convex along straight paths. Gradient descent of course only gives local optima but these still
display nice star-convexity properties, however, finding the true global optima remains a challenge.
4.2	Ablation experiments
Theorem 1 suggests that as the matrices become larger, NMF is increasingly likely to be star-convex.
We are interested to see if this is the case for our real-world datasets, and to this end, we perform
ablation experiments varying the dimensions of the matrices. We decrease the number of data points
n by subsampling rows and columns uniformly randomly. Our measure of curvature at a point x,
given some optimal solution x*, is
α(x) “ minɔ '2'λx* + (1 — λ)x)
(6)
Note that α20 implies star-convexity. In practice, X and x* are obtained by random initialization
and gradient descent as per the earlier section. For each dataset and subsample rate, we find 100
optima and evaluate the curvature from 50 random points, thus giving 5000 samples of α. Figure
5 show how the relative deviation, μ, of α decrease as the dataset becomes larger. As shown in
Appendix C, the curvature is always positive, thus the curvature becomes increasingly concentrated
around its positive mean for larger matrices. We are also interested in investigating whether the loss
surface is star-convex during training. In Figure 6, we show that the cosine similarity between the
negative gradients —V'(U, V) and the straight line from xo to x* is always positive, and how the
loss surface is always star-convex during training. As per (Lee and Valiant, 2016), star-convexity
implies efficient optimization. In Figure 7, we illustrate the spectrum of singular values for U*,
found for the birdsong dataset, and a random matrix of the same shape with iid entries from a half-
normal distribution. The spectra are similar, and while U* is not random, it seems to share structural
qualities with the random matrices used in our proofs. See Appendix C for figures on more datasets.
6
Under review as a conference paper at ICLR 2020
birdsong	metabolic
movielens
Figure 5: Illustration of how the relative deviation σ{μ of the curvature equation 6 depend on the
dataset size. For all datasets, the relative deviations decrease with more samples, suggesting that the
(positive) curvature become increasingly concentreated around its mean for larger matrices.
0.035
S
_： 0.028
S?
0.021
ORL
Figure 6: a) and b) show the cosine similarity between the negative gradient —V'(U, V) and the
straight line pU0, V0q → (U*, V*), during training, for the ORL and movielens dataset. Notice
that the cosine similarity is large and non-negative throughout training, showing that the gradients
largely follow a straight line. c) and d) illustrate how the curvature equation 6 varies during training
for the ORL and movielens dataset. Note that it is always positive, and thus that the function satisfies
star-convexity. Shaded regions represents 95 % confidence computed over 5 iterations.
4.3	Implications for neural networks
We have seen how increasing the number of parameters makes NMF problems more likely tobe star-
convex, and Figure 5 shows how more parameters make the curvature tend towards its positive mean.
Theorem 1 suggest that this is a result of concentration of measure, and it is natural to believe that
similar phenomenon would persist in the context of neural networks. It has previously been observed
how neural networks are locally convex (Izmailov et al., 2018; Kleinberg et al., 2018), and also how
overparameterization is important in deep learning (Arora et al., 2018; Frankle and Carbin, 2018).
Based upon our observations in NMF, we hypothesise that a major benefit of overparameterization
is in making the loss surface more convex via concentration of measure w.r.t. the weights.
To verify this hypothesis, we consider image classification on
CIFAR10 (Krizhevsky et al., 2014) with Resnet networks (He
et al., 2016) trained with standard parameters (see Appendix
B). Networks are typically only locally convex, a property we
quantify as the length of subsets of random "lines" in parame-
ter space, along which the loss is convex. We sample a random
direction r and then consider a subspace of length one along
this direction centered around the current parameters w. We
then define the convexity length scale as the length of the max-
imal sub-interval [λι,λ2] containing 0 on which '(w ' λr) is
convex. Directions are sampled from Gaussian distributions
and then normalized for each filter f to have the same norm
as the weights of f. Table 2 show how this length-scale of
local convexity varies with depth, width, and training, where
width is varied by multiplying the number of channels by k .
Increased width indeed makes the landscape increasingly lo-
cally convex for all but the most shallow networks, results that
support our hypothesis. See Appendix B for extended tables.
Figure 7: Histogram of singular
values σ% for found U* and a ran-
dom matrix. Note the similarity of
the spectrum. Best viewed in color.
7
Under review as a conference paper at ICLR 2020
Figure 8: The loss landscape of a 110-layer Resnet architecture at epoch 200 along two random
directions, visualized as in Li et al. (2018). The network in the right image is four times as wide,
and its loss landscape is increasingly convex. In Table 2 we generalize this idea, showing that the
length scale of local convexity increases with network width.
Table 2: Typical length scales of local convexity for Resnet networks with various depth and width
(indicated by k). We sample 25 random "lines" in parameter space of length 1 centered on current
parameters, and report average length of convex subset of such "lines". Increased width makes the
loss surface increasingly locally convex, while convexity decreases with depth and during training.
20-layers	44-layers	68-layers	110-layers
epoch	k=1	k=2	k=4	k=1	k=2	k=4	k=1	k=2	k=4	k=1	k=2	k=4
0	0.96	1.0	1.0	1.0	1.0	10	1.0	1.0	1.0	0.94	0.94	0.94
100	0.87	0.84	0.87	0.72	0.79	0.83	0.71	0.76	0.87	0.79	0.75	0.91
200	0.81	0.74	0.73	0.66	0.68	0.76	0.6	0.71	0.8	0.71	0.71	0.82
300	0.71	0.72	0.75	0.57	0.68	0.78	0.58	0.67	0.81	0.63	0.68	0.82
5	Related work
Our work was initially motivated by findings in Kleinberg et al. (2018) and Zhou et al. (2019) re-
garding star-convexity in neural networks. As the success of deep learning has become apparent, re-
searchers have empirically investigated neural network trained on real-world datasets (Li and Yuan,
2017; Keskar et al., 2016). In the context of sharp vs flat local minima (Keskar et al., 2016), Li et al.
(2018) illustrate how width improved flatness in a Resnet network, an observation Table 2 quanti-
fies. In general, real-world datasets seem to be far more well-behaved than the worst case, given
that training neural networks are NP-hard (Blum and Rivest, 1989). There is extensive work on non-
worst-case analysis of algorithms and machine learning models, and on what problem distributions
can guarantee tractability which addresses such gaps (Bilu and Linial, 2012; Afshani et al., 2017).
On the positive side Arora et al. (2012) has shown an exact algorithm for NMF that runs in poly-
nomial time for small constant r, and there are positive results for ’separable’ NMF (Donoho and
Stodden, 2004). Compressive sensing (Candes et al., 2004), smoothed analysis (Spielman and Teng,
2004) and problems with "planted" solutions (Barak et al., 2016) (Holland et al., 1983) similarly
makes assumptions on the input. Researchers have also been interested in theoretical convergence
properties of shallow and linear networks (Lu and Kawaguchi, 2017), where a common theme is
that functions with only saddle points and global minima can be effectively optimized (Ge et al.,
2015). In analysis of neural networks, random matrix theory often plays a role, directly or indirectly
(Choromanska et al., 2015; Glorot and Bengio, 2010; Pennington and Bahri, 2017; Xiao et al., 2018).
6	Conclusions
This paper revisits NMF, a non-convex optimization problem in machine learning. We have shown
that NMF is typically star-convex, provably for a natural average-case model and empirically on an
extensive set of real-world datasets. Our results support the counter-intuitive observation that opti-
mization might sometimes be easier in higher dimensions due to concentration of measure effects.
8
Under review as a conference paper at ICLR 2020
References
P. Afshani, J. Barbay, and T. M. Chan. Instance-optimal geometric algorithms. Journal of the ACM
(JACM), 64(1):3, 2017.
R.	Ahlswede and A. Winter. Strong converse for identification via quantum channels. IEEE Trans-
actions on Information Theory, 48(3):569-579, 2002.
N. Ampazis. Collaborative filtering via concept decomposition on the netflix dataset. In ECAI,
volume 8, pages 26-30, 2008.
S.	Arora, R. Ge, R. Kannan, and A. Moitra. Computing a nonnegative matrix factorization-provably.
In Proceedings of the forty-fourth annual ACM symposium on Theory of computing, pages 145-
162. ACM, 2012.
S.	Arora, N. Cohen, and E. Hazan. On the optimization of deep networks: Implicit acceleration by
overparameterization. arXiv preprint arXiv:1802.06509, 2018.
B.	Barak, S. B. Hopkins, J. Kelner, P. Kothari, A. Moitra, and A. Potechin. A nearly tight sum-of-
squares lower bound for the planted clique problem. In 2016 IEEE 57th Annual Symposium on
Foundations of Computer Science (FOCS), pages 428-437. IEEE, 2016.
O. Berne, C. Joblin, Y. Deville, J. Smith, M. Rapacioli, J. Bernard, J. Thomas, W. Reach, and
A. Abergel. Analysis of the emission of very small dust particles from spitzer spectro-imagery
data using blind signal separation methods. Astronomy & Astrophysics, 469(2):575-586, 2007.
Y. Bilu and N. Linial. Are stable instances easy? Combinatorics, Probability and Computing, 21
(5):643-660, 2012.
A. Blum and R. L. Rivest. Training a 3-node neural network is np-complete. In Advances in neural
information processing systems, pages 494-501, 1989.
D. D. Bourgin. Testing Models of Cognition at Scale. PhD thesis, University of California, Berkeley,
2018.
E. Candes, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from
highly incomplete frequency information. arXiv preprint math/0409186, 2004.
E. J. Candes and B. Recht. Exact matrix completion via convex optimization. Foundations of
Computational mathematics, 9(6):717, 2009.
X. canto Foundation. xeno-canto: Sharing bird sounds from around the world. https://www.
xeno-canto.org. Accessed: 2019-04-19.
A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surfaces of multi-
layer networks. In Artificial Intelligence and Statistics, pages 192-204, 2015.
C.	Cortes and V. Vapnik. Support-vector networks. Machine learning, 20(3):273-297, 1995.
D.	Davis. Mathematics of data science. https://people.orie.cornell.edu/dsd95/
orie6340.html. Accessed: 2019-04-19.
A. Decelle, F. Krzakala, C. Moore, and L. Zdeborovd. Asymptotic analysis of the stochastic block
model for modular networks and its algorithmic applications. Physical Review E, 84(6):066106,
2011.
D. Donoho and V. Stodden. When does non-negative matrix factorization give a correct decom-
position into parts? In Advances in neural information processing systems, pages 1141-1148,
2004.
A. Dvoredsky. Some results on convex bodies and banach spaces. 1961.
N. B. Erichson, A. Mendible, S. Wihlborn, and J. N. Kutz. Randomized nonnegative matrix factor-
ization. Pattern Recognition Letters, 104:1-7, 2018.
9
Under review as a conference paper at ICLR 2020
J. Flenner and B. Hunter. A deep non-negative matrix factorization neural network. 2017.
J.	Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks.
arXiv preprint arXiv:1803.03635, 2018.
R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points—online stochastic gradient for
tensor decomposition. In Conference on Learning Theory, pages 797-842, 2015.
N. Gillis. Sparse and unique nonnegative matrix factorization through data preprocessing. Journal
of Machine Learning Research, 13(Nov):3349-3386, 2012.
N. Gillis. The why and how of nonnegative matrix factorization. Regularization, Optimization,
Kernels, and Support Vector Machines, 12(257), 2014.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.
In Proceedings of the thirteenth international conference on artificial intelligence and statistics,
pages 249-256, 2010.
A. Guionnet, O. Zeitouni, et al. Concentration of the spectral measure for large matrices. Electronic
Communications in Probability, 5:119-136, 2000.
F. M. Harper and J. A. Konstan. The movielens datasets: History and context. Acm transactions on
interactive intelligent systems (tiis), 5(4):19, 2016.
K.	He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.
W. Hoeffding. Probability inequalities for sums of bounded random variables. In The Collected
Works of Wassily Hoeffding, pages 409-426. Springer, 1994.
P. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social networks,
5(2):109-137, 1983.
P. O. Hoyer. Non-negative matrix factorization with sparseness constraints. Journal of machine
learning research, 5(Nov):1457-1469, 2004.
P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. G. Wilson. Averaging weights leads to
wider optima and better generalization. arXiv preprint arXiv:1803.05407, 2018.
N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training
for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.
R. Kleinberg, Y. Li, and Y. Yuan. An alternative view: When does sgd escape local minima? arXiv
preprint arXiv:1802.06175, 2018.
Y. Koren, R. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems.
Computer, (8):30-37, 2009.
A. Krizhevsky, V. Nair, and G. Hinton. The cifar-10 dataset. online: http://www. cs. toronto.
edu/kriz/cifar. html, 55, 2014.
M. Kula. Mixture-of-tastes models for representing users with diverse interests. arXiv preprint
arXiv:1711.08379, 2017.
M. Ledoux. The concentration of measure phenomenon. Number 89. American Mathematical Soc.,
2001.
D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative matrix factorization.
Nature, 401(6755):788, 1999.
D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In Advances in neural
information processing systems, pages 556-562, 2001.
J. C. Lee and P. Valiant. Optimizing star-convex functions. In 2016 IEEE 57th Annual Symposium
on Foundations of Computer Science (FOCS), pages 603-614. IEEE, 2016.
10
Under review as a conference paper at ICLR 2020
H. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein. Visualizing the loss landscape of neural nets.
In Advances in Neural Information Processing Systems, pages 6389-6399, 20l8.
L.	Li, G. Lebanon, and H. Park. Fast bregman divergence nmf using taylor expansion and coordi-
nate descent. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 307-315. ACM, 2012.
S. Z. Li, X. Hou, H. Zhang, and Q. Cheng. Learning spatially localized, parts-based representation.
CVPR (1), 207:212, 2001.
Y. Li and A. Ngom. The non-negative matrix factorization toolbox for biological data mining.
Source code for biology and medicine, 8(1):10, 2013.
Y. Li and Y. Yuan. Convergence analysis of two-layer neural networks with relu activation. In
Advances in Neural Information Processing Systems, pages 597-607, 2017.
C. Lu and X. Tang. Surpassing human-level face verification performance on lfw with gaussianface.
In Twenty-ninth AAAI conference on artificial intelligence, 2015.
H. Lu and K. Kawaguchi. Depth creates no bad local minima. arXiv preprint arXiv:1702.08580,
2017.
X. Luo, M. Zhou, Y. Xia, and Q. Zhu. An efficient non-negative matrix-factorization-based approach
to collaborative filtering for recommender systems. IEEE Transactions on Industrial Informatics,
10(2):1273-1284, 2014.
Y. Mao, L. K. Saul, and J. M. Smith. Ides: An internet distance estimation service for large networks.
IEEE Journal on Selected Areas in Communications, 24(12):2273-2284, 2006.
M.	Meckes and S. Szarek. Concentration for noncommutative polynomials in random matrices.
Proceedings of the American Mathematical Society, 140(5):1803-1813, 2012.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
miller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement
learning. Nature, 518(7540):529, 2015.
P. M. Pardalos and S. A. Vavasis. Quadratic programming with one negative eigenvalue is np-hard.
Journal of Global Optimization, 1(1):15-22, 1991.
J. Pennington and Y. Bahri. Geometry of neural network loss surfaces via random matrix theory. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2798-
2806. JMLR. org, 2017.
M. N. Schmidt, J. Larsen, and F.-T. Hsiao. Wind noise reduction using non-negative sparse coding.
In 2007 IEEE workshop on machine learning for signal processing, pages 431-436. IEEE, 2007.
D. A. Spielman and S.-H. Teng. Smoothed analysis of algorithms: Why the simplex algorithm
usually takes polynomial time. Journal of the ACM (JACM), 51(3):385-463, 2004.
S. K. Suram, Y. Xue, J. Bai, R. Le Bras, B. Rappazzo, R. Bernstein, J. Bjorck, L. Zhou, R. B.
van Dover, C. P. Gomes, et al. Automated phase mapping with agilefd and its application to light
absorber discovery in the v-mn-nb oxide system. ACS combinatorial science, 19(1):37-46, 2016.
G. F. Trindade, M.-L. Abel, and J. F. Watts. Non-negative matrix factorisation of large mass spec-
trometry datasets. Chemometrics and Intelligent Laboratory Systems, 163:76-85, 2017.
J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational
mathematics, 12(4):389-434, 2012.
S. A. Vavasis. On the complexity of nonnegative matrix factorization. SIAM Journal on Optimiza-
tion, 20(3):1364-1377, 2009.
R.	Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
11
Under review as a conference paper at ICLR 2020
R. Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge University Press, 2018.
L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, and J. Pennington. Dynamical isometry and
a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks.
arXiv preprint arXiv:1806.05393, 2018.
S.	Zhang, W. Wang, J. Ford, and F. Makedon. Learning from incomplete ratings using non-negative
matrix factorization. In Proceedings of the 2006 SIAM international conference on data mining,
pages 549-553. SIAM, 2006.
Y. Zhou, D. Wilkinson, R. Schreiber, and R. Pan. Large-scale parallel collaborative filtering for
the netflix prize. In International conference on algorithmic applications in management, pages
337-348. Springer, 2008.
Y. Zhou, J. Yang, H. Zhang, Y. Liang, and V. Tarokh. Sgd converges to global minimum in deep
learning via star-convex path. arXiv preprint arXiv:1901.00451, 2019.
G. Zhu. Nonnegative matrix factorization (nmf) with heteroscedastic uncertainties and missing data.
arXiv preprint arXiv:1612.06037, 2016.
12
Under review as a conference paper at ICLR 2020
A Datasets
Here we describe the datasets we used to evaluate our results on and provide references for them.
Birdsong: time series for bird calls, see (Flenner and Hunter, 2017; canto Foundation).
Extragalactic: dataset for extragalactic light sources, see (Zhu, 2016).
Goodbooks: user ratings for books, see (Kula, 2017; Bourgin, 2018).
Metabolic: Yest metabolic activity time series, see (Li and Ngom, 2013).
Movielens: user ratings for movies , see (Harper and Konstan, 2016; Li et al., 2012; Zhang et al.,
2006).
Netflix: User ratings for movies, see (Zhou et al., 2008; Ampazis, 2008; Li et al., 2012).
ORL faces: black and white facial images, see (Li et al., 2001; Hoyer, 2004).
Satellite: satellite urban spectral-image, see (Erichson et al., 2018; Gillis, 2014).
B DNN parameters
Table 3 presents the hyper-parameters used for training the neural networks. Data augmentation
consists of randomized cropping with 4-padding and randomized horizontal flipping. The learning
rate is decreased by a factor of 10 after epochs p150, 225, 275q. We use the standard Resnet architec-
ture for CIFAR10 with three segments with p16, 32, 64q channels each, and use the standard blocks
(i.e. not bottleneck blocks) (He et al., 2016).
Table 3: Hyper-parameters used for training.
Parameter	Value
init. learning rate	ɪi
SGD momentum	0.9
batch size	128
weight decay	0.0005
initialization	Kaiming
loss	cross-entropy
C Extended NMF figures
D	Proofs
D.1 Overview
We here provide the proof of Theorem 1. In section D.2 we present the notation and derive the main
inequality. In Section D.3, we prove that the loss function is convex between any two random points
with high probability. Section D.4, we prove that this also holds towards the planted solution. For
the case with unobserved data, we prove that the loss function is convex between any two random
points with high probability in Section D.5. That this holds towards the planted solution in the case
of unobserved data follows from previous sections, and the proof of this fact omitted. Together,
these results form Theorem 1 as stated in the main text. Section D.6 presents the concentration
Table 4: ContinUation of Table 2.
epoch	32-layers			56-layers			80-layers		
	k=1	k=2	k=4	k=1	k=2	k=4	k=1	k=2	k=4
0	1.0	1.0	1.0	1.0	10	1.0	1.0	1.0	1.0
100	0.77	0.8	0.84	0.7	0.81	0.84	0.72	0.85	0.81
200	0.61	0.68	0.8	0.63	0.67	0.8	0.59	0.7	0.77
300	0.55	0.68	0.82	0.57	0.66	0.79	0.63	0.71	0.79
13
Under review as a conference paper at ICLR 2020
Table 5: Table 2 for architectures without skip-connections
epoch	20-layers			32			44-layers		
	k=1	k=2	k=4	k=1	k=2	k=4	k=1	k=2	k=4
0	0.15	0.62	0.15	0.2	0.84	1.0	0.43	0.33	1.0
100	0.71	0.79	0.93	0.68	0.7	0.83	0.57	0.61	0.6
200	0.74	0.67	0.8	0.63	0.63	0.86	0.57	0.6	0.67
300	0.69	0.63	0.82	0.5	0.63	0.84	0.47	0.56	0.67
alnsΛJno alnsΛJno
Figure 9: The curvature equation 6 during training, as in Figure 6, for various datasets. Note that it
is always positive, and thus that the function satisfies star-convexity. Note the small shaded regions,
which represents the 95 % confidence computed over 5 repetitions.
Table 6: Links for used datasets.
dataset	link
birdsong	https://www.kaggle.com/rtatman/british-birdsong-dataset/data
extragalactic	https://s3.us-east-2.amazonaws.com/setcoverproblem/Extra... …galacticTest/ExtragalatiJArchetype_testsample_spec.fits
goodbooks	https://www.kaggle.com/zygmunt/goodbooks-10k
metabolic	ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE3nnn /GSE3431/matrix/GSE3431_SerieS_matrix.txt.gz
movielens	https://www.kaggle.com/prajitdatta/movielens-100k-dataset
netflix	https://www.kaggle.com/netflix-inc/netflix-prize-data
ORL	https://github.com/chibuta /2Layer_convolution/blob/master/ORL_faces.npz.zip
satellite	http://www.escience.cn/SyStem/file?fileId=69117
results we need. The proofs include considerable amounts of elementary algebraic calculations. For
completeness and ease of reference, we present these in Appendix E.
14
Under review as a conference paper at ICLR 2020
extragalactic
goodbooks
birds
Figure 10: The cosine similarity between the negative gradient —V'(U, V) and the straight line
pU0,V0q → (U*, V*), during training as in Figure 6. Note the small shaded regions, which
represents the 95 % confidence computed over 5 repetitions.
Figure 11: Illustration of how the relative deviation σ{μ of the curvature equation 6 depend on
the dataset size as in Figure 6. Unfortunately, computing these figures for the larger Netflix and
Goodbooks datasets is computationally infeasible for us.
D.2 Notation and the Main Inequality
The proof will involve various constants c0, c1, c2 that do not depend on n. For convenience, we will
let exact values be context dependent and will reuse the symbols c0, c1, c2 and so on. The proofs
will not depend on the exact values of these constants. Constants in our main result can be improved
slightly, but at the cost of a more complicated expression. We present the non-optimized version. We
will let boldface capital letters denote matrices, boldface lower-case letter vectors and non-boldface
letters denote scalars. Let Ui, U2, U* be n-by-r matrices and Vι, V2, V* r-by-m matrices
sampled as per Assumption 1. Without loss of generality, we assume m “ ncm, where 0 < Cm ≤ 1.
We will let μvar be the expectation, and σj be the variance of the entry-wise distributions. We
will denote the centered variables by U1, Vi and so on, and the mean matrix of U and V resp
as 1n^r, 1r^m. Then U “ U ` 1n^r. By slight abuse of notation, we will use the convention
xX, Yy “ Tr XYT for matrices X, Y of the same shape. The loss function we wish to minimize,
subject to non-negativity constraints, is
15
Under review as a conference paper at ICLR 2020
rnd matrix
UT* matrix
] σ
log------
σmax
σmax
σmax
σmax
Figure 12: Histogram of singular value spectra for U* found via gradient descent and a random
matrix of the same shape with entries drawn iid from a half-normal distribution. For the datasets
with larger ranks, the spectra typically agree significantly.
extragalactic
goodbooks
metabolic
satellite
σmax
ORL
σmax
Figure 13: Histogram of singular value spectra for V* found via gradient descent and a random
matrix of the same shape with entries drawn iid from a half-normal distribution. For the datasets
with larger ranks, the spectra typically agree significantly.
16
Under review as a conference paper at ICLR 2020
`pu, V) = 2 }uv ´ x}F	⑺
When we interpolate the solution UV between any fixed two points pU1, V1q and pU2, V2q, we
write
X(λ) = 'λUι + (1 ´ λ)U2)'λV1 + (1 ´ λ)V2)
The loss is then
1
'(λ) = 2 }X (X) — X}F
Proving Theorem 1 in the case of two random points and completely observed data amounts to
showing that the continuous function '(λ) is convex in λ on the interval [0,1] with high probability.
We will show that the Second-derivate is non-negative. For fixed matrices Ui, U2, U*, Vι, V2, V*,
the loss '(λ) is a fourth-degree polynomial in λ, so its second derivate w.r.t. λ will be a second-
degree polynomial. For a general second-degree polynomial p(x) = ax2 + bx + c we have p(x) =
a [(ax + b)2 + 'ac — b2)‰. By inspecting equation 8 one can conclude that a > 0. Showing that
the polynomial is non-negative can then be done by showing ac》b2. Let X j (λ) be element i,j
ʌ
of the matrix X(λ). The derivates of '(λ) are then
'(λ) = 1 2(X ij (X) — Xij )2
'1(λ) = § (X ij (xq — Xij )Xj (λ)
'2(λ) = X X2 (λ)(X ij- (λ) — Xij) + Xij (λ)2
ij
'3 (λ) = 3 ∑ X2 (λ)Xj (λ)
ij
'4(λ) = 3 § X2j (λ)2	(8)
ij
Where we have
X (λ) = (λUι + (1 — λ)U2)(λVι + (1 — λ)V2)
X1(λ) = (u1 — U
2
Let us define
Wo = U2V2 — U*v*
W1
= U1 — U2 V2 + U2 V1 — V2
W2
17
Under review as a conference paper at ICLR 2020
By expanding '2(λ) in a McLaurin series around 0, We see that proving non-negativity of '2(λ) can
be done by proving 2'(0)2'4(0)2'3(0)2. Thus, proving Theorem 3 then amounts to showing that
2}W2}F'}Wι}F + 2χW0, W2〉)N 3(〈Wi, W2〉)2	(9)
D.3 Proving the Inequality Between Random Points
Theorem 2. Let (Ui, Vι), (U2, V2) and the planted solution (U*, V*) be sampled as per As-
sumption 1. The loss function equation 7 is convex on a straight line connecting (U1, V1q and
(U2, V2q with probability N 1 ´ c1 exp ´ c2 n1{3 for positive constants c1, c2.
Proof. As per section D.2, we only need to prove that equation 9 holds with probability N 1 ´
c1 exp ´ c2 n1{3 . Let us exhange all terms in equation 9 with their means, which are given in
Facts 7, 5, 6 and 8
2(4rmnσ4q '6rmnσ4 + 4rmnμ2arσ2 + 2rmnσ4) N 3' — 4rmnσ4)2	(10)
Clearly, equation 9 would hold if one naively exchanges the terms for their means. Just counting
terms of order rmnσ4 we have 64 on the LHS and 48 on the RHS. We need to show that deviations
sufficiently large to violate the inequality are unlikely. Our strategy will be to show that the factors
are exponentially unlikely to deviate substantially individually, and then use union bounds. The
concentration results and their derivations are detailed in Section D.6.
Consider the random variable }W2}2F. As per equation 14, }W2}2F can be rewritten into polynomi-
als of centered random matrices where we have to pad the matrices with rows/columns of all zeros.
We can then apply the concentration result of Fact 1 to conclude that the probability that }W2 }2F
deviates from its mean by a factor (1 + q is no more than c1 exp(—c22n .
Now, consider the random variable }W1 }2F. As in the proof of Fact 7, }W1 }2F can be rewritten
as the sum of expression equation 16 plus terms of centered variables. The latter expressions are
polynomials in centered random matrices, after padding some columns/rows with zeros we can
apply Fact 1 to conclude that the probability that it deviates from its mean by a factor (1 + q
is no more than c1 exp(—c22n . Consider the expression equation 16. Clearly the expressions
lT^m'Uι — U2)τ'Ui — U2)lr^m and 'Vi — V2)τlT^∕n^r 'Vi — V2) Willbepsd, and thus
their trace will be nonnegative. We can thus lower bound the LHS of equation 10 by omitting the
terms in equation 16, and will thus only consider terms scaling as rn2σ4.
Now, consider the random variable 〈W0, W2〉. As per equation 19, the variable 〈W0, W2〉 can
also be separated into two parts. After padding with some zero rows/columns to obtain square
matrices, the first is a polynomial in centered matrices for which Fact 1 applies which again bounds
the probability of deviations by a factor (1 + q by ci exp(—c22n . The second one is a sum of the
type 1XiX2X3 modulo permutations, under which the trace is invariant. We can thus apply Fact 2
which bounds the probability for deviations of size 〈W0, W2〉 by ci exp — c2ni{3 .
At last, consider the random variable 〈Wi, W2〉. By equation 18 can, just like 〈W0, W2〉, be
written as the sum of a polynomial in centered random matrices, and sums of variables of the type
1XiX2X3 modulo permutations. We can thus apply the same argument as for 〈W0, W2〉 to bounds
the probability for deviations of relative size by ci exp — c2ni{3 .
We can now apply union bound on the variables }Wi}2F, }W2 }2F, 〈W0, W2〉 and 〈Wi, W2〉,
modulu term equation 16, deviating from their respective mean by a factor . The probability that
either of them does is no more than ci exp — c22ni{3 by union bounds. Now, set “ 0.01 If
neither variable deviates by a factor of more than 0.01, then equation 10 still holds ifwe only count
terms scaling as rmrσ4 since 0.992 * 64 N 1.012 * 48. Thus, inequality is violated with probability
at most ci exp — c2ni{3 for positive constants ci, c2.
18
Under review as a conference paper at ICLR 2020
D.4 Proving the Inequality Towards the Planted Solution
Theorem 3.	Let (Ui, Vι) and the planted solution (U*, V*) be Sampled as per Assumption 1.
The loss function ofequation 7 is convex on a straight line connecting (Ui, Vi) and (U*, V*) with
probability21 — ci exp ( — c2n1{3} for constants c1,c2 > 0.
Proof. To prove this, we can repeat the argument used in the proof of Theorem 2. The terms }W2 }2,
}Wi }2 and xWi , W2 y will have the same mean and concentration since Ui and U* are identically
distributed. The only difference will be the term xW0 , W2y, which now will have mean 2rmnσ4
per Fact 10. This will only make the LHS of equation 10 larger, so the inequality will still hold and
the concentration of measure properties will ensure that equation 9 holds with high probability as in
the proof of theorem 2.
D.5 Proving the Inequality B etween Random Points for Unobserved Data
As explained in the main text, we will assume that entries are observed with independent probability
p. We formalize this in the following assumption
Assumption 2. For any n, let i P {0, l}nxm have entries iid drawn from a Bernoulli distribution
with probability p, which is constant w.r.t n.
ʌ
Given any set of observations 1, we define our loss function as
'(u, V)= ∑ i(i,j) ^[UV‰ij ´ Xij) 2	(11)
Theorem 4.	Let (Ui, Vi), (U2, V2) and the planted solution (U*, V*) be sampled as per As-
ʌ
sumption 1. Let the set of observations 1 be sampled as per Assumption 2. The loss func-
tion equation 11 is convex on a straight line connecting (Ui, Vi) and (U2, V2) with probability
21 — Ci exp ( — c2ri∕3) for constants ci,c2 > 0.
Proof. If no entries are observed, we can claim trivially that the function is convex. If some entries
are observed, then a > 0 since X2 (λ)2 > 0 for any (i,j) with probability 1. The second derivate
of equation 11 can be written as
'2(λq =∑1(i,j-q
ij
ij ' X"ij (Xij — Xij q
)
Using Lemma 1, We can assume by union bound that no entry in X, X1, X2, X is larger than O(r2{3)
with probability ≤ cin2 exp (— c2ri∕3). Note that this estimate gives the exp (— c2ri∕3) scaling of
2
the proof. Let assume that the entries are bounded like this, then no entry in X1 ij- ` X2j (Xij — Xij)
will have magnitude more than O(r4{3) for any λ. Standard Hoeffding bounds (Hoeffding, 1994)
states that for variables txiu bounded in rAi, Bis we have
1 n	2n2t2
PU ? Xi N tJ W exp (— ∑n(bi — ai)2 )
Let us define
y(λ) = £ X12j (λ) + X〃j (X ij (λ) — Xij)
ij
Let us now consider fixed matrices Ui, U2, U*, Vi, V2, V* satisfying Lemma 1, but let i remain
2
a random variable. We can then consider the product of any two variables out ofXij, X1ij, Xij, Xij
as fixed constants in the interval r—cir4{3, cir4{3s for some constant ci. We note that y(λ) is the
19
Under review as a conference paper at ICLR 2020
sum of cmn2 variables, each of size at most Opr4{3q. Now ypλq is a sum of independent variables
and Hoeffdinger applies with Pbi 一 ai)2 ≤ O(r8{3) for all i. Taking t “ cr2{3 gives Us
P ^ y(λ) ´ E[y(λ)S2cr2{3n2) ≤ exp ^ ´ 2c ："；-) ≤ exp ' ´ cn2/3)
This will hold for any λ. We can assUme that this holds for let’s say 3 evenly spaced λ by Union
bound. Using union bounds, We assume the following: 1) y(λ) > cιrmn for all λ, which follows
from the derivation of Theorem 2 with probability21 — ci exp(—c2n1{3). 2) |X j | ≤ r2{3 for all
i, j via Lemma 1. 3) |Y(λ) — E[y(λ)]∣ ≤ cr2{3n2 for 3 evenly spaced λ% by the above equation.
Now, since the `2 (λ) is a second-degree polynomial, if it’s close to its expectation at 3 places, it
must be close it its expectation everywhere. Formally, we can define the second-degree polynomial
y(λ) = y(λ) — E[y(λ)]. Now, our union bounds states that ∣y(λi)∣ V cr2{3n2 for three evenly
spaced λi . Now, let the three coefficients of the second-degree polynomial be described by the
vector p, its values at λi by the vector v and let the matrix A describe the values of the monomials
1, λ, λ2 at λi . We then have Ap = v. Now, A will have elements bounded by 1 but will be full rank,
this implies that if V is bounded by cr2{3n2 so must P be. The fact that the Coefficents in y have
absolute value bounded by O(r2{3n2), together with the fact that the mean scales as rn2 gives us
the following
min y(λ) = min y(λ) ' E[y(λ)]》min y(λ) ' min E[y(λ1 )S
》min y(λ) ' crn22c1 r2{3n2 ' crn2
Here c is positive as per our assumptions, whereas c1 might be negative. Now, for sufficiently large
r, the above quality will be non-negative. We can thus find new constants, based upon c and c1, to
complete the proof.
D.6 Concentration
This section contains the concentration results needed for the proof of Theorem 1. We will use
results from random matrix theory, which has stronger results for matrices with independent entries
that are concentrated (Ahlswede and Winter, 2002; Tropp, 2012; Guionnet et al., 2000). We will use
the concentration results of Meckes and Szarek (2012) which relies on the notation of CCP — convex
concentration property - which is a regularity condition.
Definition 2. A random matrix X in a normed vector space satisfies CCP iff there exists positive
constants c1,c2 such that P(If(X) — Mf(X)I2t) ≤ cie-c2* for all t and all convex I-LiPschitz
functions f . Here M is the median.
One can easily verify that independent Gaussian variables satisfies this (Ledoux, 2001), and it then
follows that 1-Lipschitz functions of Gaussians also satisfies CCP. One important class here is the
half-normal distribution which is just the absolute value of a Gaussuan variable, and absolute value is
a 1-Lipschitz function. Any vector of independent bounded variables will also satisfy CCP (Meckes
and Szarek, 2012).
Fact 1. Let P be a polynomial ofdegree 4 in Centered matrices Xi, Xj, Xk, Xi otherwise SamPled
as Per AssumPtion 1, where we allow i = j, j = l and any other arbitrary index relationshiP. Let
Z = Tr P(Xi, Xj, Xk, Xlqand let μ be then mean of Z, then P(|z — μ∣ > trn2) ≤ ci exp (—
c2 min(t2, ti{2) n for positive constants ci, c2.
Proof. Any vector CCP where some components are zero will still be CCP, this we can pad the
matrices Xi, Xj, Xk, Xl so that they are square. We then invoke Theorem 1 of Meckes and Szarek
(2012) which gives
P|
z — μ∣ > τn2)
≤ ci exp ( — C2 min(τ2
nτ i∕2))
20
Under review as a conference paper at ICLR 2020
Setting τ “ tr gives us
P|
Z ´ μ∣ > t r n2)
≤ ci exp ( — C2 min( t2 r2
t1{2 r1{2 n))
Using our assumption r “ Opnγ q for some γ P r1{2, 1s gives
P|
z — μ∣ > trn2)
≤ ci exp ' — C2 min(t2,t1/2) n)
Fact 2. Let z “ Tr 1Xi Xj Xk for centered matrices Xi, Xj , Xk otherwise sampled as per As-
sumption 1, and let μ be the mean of Z. 1 is a matrix of all 1S of the appropriate shape. Then
P(|z ´ μl》 trn2) ≤ ci exp ( — c? n1{3 min(t1{2, t2)) for positive constants c1,c2.
Proof. As before, we can pad the matrices to be square while retaining the CCP property. We note
that the singular value of the matrix 1 is at most n. This means that the shatten-norm }E 1}k is at
most n for all k. Let Us consider the matrix 1 “ n^ which then has Shatten-norm }E 1}d is at most
n2{3. For this matrix, Theorem 1 in Meckes and Szarek (2012) applies, so that we have
P
Tr(IXiXjXk ´ 〃)卜 tn2
≤ ci exp ' — c2 min 't2, nt1/2))
(12)
Let us take t “ torn´^3 so that tn7{3 “ torn2. We then have
I Tr (IXiXjXk — μ)∣》tn2 0 ∖ Tr (IXiXjXk — μ)∣》tn7{3
ð^ ∣ Tr (IXiXjXk — μ)∣》torn2
By substituting t “ torn—i{3 into equation 12 We then have
P (ITr (1 XiXjXk — μ)∣》torn2)
(13)
W Ci exp ( — c2 min (t0r2n-2/3,力1{2储{2储—1{6))
Recall our assumption r “ c3nγ for γ P r1{2, 1s. We then have
min (t2r2n-2/3,力0{2储{2储—1{6)2min (t0{2, t2) ni{3
Plugging this into equation 13 completes the proof
Lemma 1. With probability W cin2 exp (— c2ri{3), no entry in X j, X j, X ij, Xij has an absolute
value larger than cr2{3, for some positive constant c.
2
Proof. For a fixed i, j consider the terms Xij , X1ij , Xij , Xij . Fact 9 says that each one can be
expressed as a constant number of variables a with zero mean of the form
r
a “ ∑ Viiqvp2q
i“i
Since the variables themselves are sub-gaussian, by Lemma 2.7.7 in Vershynin (2018) the product
vipiqvip2q is sub-exponential. Thus, the variable a is a sum of iid sub-exponential variables, and
Theorem 2.8.1 in Vershynin (2018) states that
P(∣a∣2t) W 2exp
— c min
„ t2 t 1∖
rK2 ,k]J
21
Under review as a conference paper at ICLR 2020
Here K is the orlicz norm } ∙ }ψι which is a constant for our fixed distributions. In the above
expression, let us set t “ cr2{3 . This gives
P
a ´ Eras)
cr2/3) ≤ 2exp ' — Cr
We note that there is a polynomial number of entries a, so we can do a polynomial number (w.r.t.
n) of union bounds, which might change the constants, to show that no entry is further than c1r2{3
from its expectation for some positive c1.
E Algebraic calculations
E.1 Basic facts
Fact 3. E“ Tr UjTUVVt‰ “ rmnσ4
Proof. We have
E“ Tr U T U V V T ‰ = E £ U U j® V kV
ijkl
Taking the mean, and using linearity of expectations, we only have nonzero mean if i “ k. We get
“E XUjiUjiViiVil
ijl
Using linearity of expectation and independence, this becomes
∑ E“U jiU ji‰E[V nV ii‰
ijl
For a single matrix entry, E [U jiU j∙j “ σ2. i goes over the columns of U of which there are r, j
goes over the rows of U of which there are n and l goes over the columns of V of which there are
m, so we clearly get σ4rmn.
Fact 4. E TrUTUlr^m 1 T^m “ rmnμVar.σ2
Proof. We can can first contract 1r^m1T^m “ 1mμ2ar., where 1 is a r-by-r matrix of all ones. We
then want to find
E XUjiUjk 1ki
ijk
Now since the variables are centered, the expectations becomes zero unless i “ k. The sum thus
becomes
E XUjiUji1ii “ nrσ2
ij
This gives us the result of rmnμVarσ2. ■
E.2 Expectation
Fact 5. Er}W2 }2 s “ 4mnrσ4
22
Under review as a conference paper at ICLR 2020
Proof. Let us rewrite Er}W2 }2s as
E Tr
^Vι ´ V2) ^Uι ´ U2) ^Uι ´ U2)^VL V2)
Trace and expectations are linear operators, so we can reorder them. Let us consider the matrix
^Vι ´ V2) ^Uι ´ U2) ^Uι ´ U2)^VL V2)
We can easily add and subtract the mean μ of all matrices which gives
(Vl´ %) (U1 ´ U?) (UI ´ U2)(V1- %)
(14)
Expanding the parantheses gives us 16 matrices, however they will have zero mean unless both the
U-matrices and V-matrices ar the same. For calculating the mean, we can thus only consider four
matrices of the type VT UT UV. Permuting the indices cyclically and appealing to Fact 3 gives us
Er}W2}2s “ 4mnrσ4
Fact 6. ErxW0, W2ys “ rmnσ4
Proof. We can write ErxW0, W2ys as
E Tr
´ (V*)T(U*)
T) ('u
´ U2 V1 ´
1
Again, we want to add and subtract the matrix means 1n^r, 1r^m to get
E Tr ([V2T U2T +V2τiT^r + lT^mU2τ + lT^mlT^r‰TV*T U*T +V*TlT^r +lT^mU*T 'l" IT
(15)
^
‰
卜UI
´ 5)(Vl
As before, we can remove terms which are linear in any centered variable. This removes matrices
with index 1, 3 and leaves
ETr [SU2T + V2T^r + lT^mU2T)(U2V2)
Applying fact 3 to the only term that remain gives
ErXWo, W2ys “ E Tr VU2TU2V2 “ rmnσ4
Fact 7. E}W1 }F = 6rmnσ4 + 4rmnμ2arσ2
Proof. We rewrite E}W1 }2F as
E Tr
´ U2)T
+ 'Vι ´ V2)TUT)卜Ui
´ U2 V2 + U2 V1 ´
We will again want to center the variables, which gives
E Trl(V2T (Ui ´ U2)T + lT^m (Ui ´ U2)T + (Ui ´ V)TU2T + (Vl ´ V2)TlT^r)
23
Under review as a conference paper at ICLR 2020
X ((U1 — U2)V2 ' (U1 ´ U2)lr^m ' U2(V1 - V2) ' 1n^r (V1 ´ V2))
Let US first consider the terms involving constants. We remove any terms linear in centered variables.
We are then left with
1T^m(U1 — U?)' (U1 ´ U2)lr^m ' (V1 ´ V?), 1Lr 1n^r (V1 ´ V2)	(16)
As before, any expressions linear in centered variables vanish when we take expectations. Using
this and Fact 4 we get that the above expression is equal to
“ 4rmnμVarσ2
We now return to the terms without constants These are
V2τ(U1 — U2)T(U1 — U2)V2 ' (V1 — V2)τU2TU2(V1 — V2)	(17)
'V2τ(U1 — U2)τU2(V1 — V2) ' (V1 — V2)τU2τ(U1 — U2)V2
As before, we remove any expressions linear in centered variables. As it turns out, we are left with
6 terms of the type V『UzTU2V2. Applying Fact 3 gives us
E[}Wi}F‰ = 6rmnσ4'4rmnμVarσ2	■
Fact 8. E〈W2, Wi)“ —4rmnσ4
Proof. Let us rewrite E<W2, Wi) as
E Tr ((Vι — V2)T(Ui — U2)τ) ((Uι — U2)V2 ' U2(V1 — V2))
As per usual, we center the variables
((V1—V2)T(U1—U2)T)卜U1
(V1 —V2))
(18)
—U2) V2' (U1—U2) lr^m'U2(VLV2)'ln^r
Any terms involving the constant will be linear in a centered variable, and thus disappear under
expectations. Equating like terms and using Fact 3, we get
“ —4E
Tr VIT UIT U1V1]
“ —4rmnσ4
E.3 Other algebraic facts
ʌ ʌ ʌ
Fact 9. For any fixed i, j the entries of X, X1 and X2 are zero mean.
Proof. We have
X(λ) — X = (λUι ' (1 — λ)U2)(λVι ' (1 — λ)V2) — U* V*
X1 (λ) “ (U1 — U2)(λV1 '(1 — λ)V2) ' (λU1 '(1 — λ)U2)(V1 — V2)
X2(λ) = (U1 — U2)(V1 — V2)
24
Under review as a conference paper at ICLR 2020
The fact that U1 and U2 are iid implies that pU1 ´ U2q has zero mean, and the similar holds for
Vi and V2. This implies that all terms of XX and XX are zero-mean. Using the fact that Ui, U2
and U* are iid implies that
E (λU1 + (1 — λ)U2) (λV1 + (1 — λ)V2) “ E[U*V*S
ʌ
This, in turn, implies that ErXS = 0. ■
Fact 10. For Ui “ U* and Vi “ V*, we have ErxW0, W2ys “ 2rmnσ4
Proof. We can write ErxW0, W2ys as
E Tr
´ (V*qT (U*qT

Again, We want to add and subtract the matrix means 1n^r, 1r^m to get
E Tr (" U2T +V2τiT^r + lT^mU2τ + lT^mlT^r‰TV*T U*T +V*TlT^r +JU*T + l"以/)
(19)
^ ((U* ´ U2)'V* ´ V2))
As before, we can remove terms which are linear in any centered variable. The only term that remain
gives
ErxW0, W2〉S “ ETr V2TU2TU2V2 + ETr ∣V*TU*TU*V* “ 2rmnσ4	■
25