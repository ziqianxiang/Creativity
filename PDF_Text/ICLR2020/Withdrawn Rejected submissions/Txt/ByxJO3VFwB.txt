Under review as a conference paper at ICLR 2020
Probabilistic modeling the hidden layers of
Deep Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we demonstrate that the parameters of Deep Neural Networks
(DNNs) do not satisfy the i.i.d. prior assumption and activations being i.i.d. is
not valid for all the hidden layers of DNNs, thus the Gaussian process cannot
correctly model the distribution of the hidden layers. Alternatively, we propose a
novel probabilistic representation for the hidden layers of DNNs in two aspects:
(i) a hidden layer formulates a Gibbs distribution, of which the energy function
is determined by the activations, and (ii) the statistical connection between two
adjacent layers can be formulated as a product of experts model. Based on the
proposed probabilistic representation, we further confirm the equivalence between
the variational inference and the stochastic gradient descent, and prove the later
is also equivalent to the energy minimization optimization. As a result, the entire
architecture of DNNs can be viewed as a Bayesian network, in which the hidden
layers close to the input formulate the prior distributions of the training dataset. In
addition, we propose a new regularization approach to improve the generalization
performance of DNNs by pre-training the hidden layers corresponding to the prior
distributions. Simulation results validate the proposed theories.
1	Introduction
Recently, interpreting the hidden layers of Deep Neural Networks (DNNs) as the Gaussian Process
(GP) has attracted a great deal of attention because it provides a novel probabilistic perspective to
clarify the working mechanism of deep learning. Neal (1994) initially demonstrates the equivalence
between GP and neural networks with a single fully connected layer in the limit of infinite neurons.
Following this seminal work, Lee et al. (2018); Matthews et al. (2018) further extend the equivalence
to neural networks with multiple hidden layers and Garriga-Alonso et al. (2018); Novak et al. (2018)
establish the correspondence between the convolutional layers and GP.
It is important to note that the GP explanations rely on a fundamental probabilistic premise that the
activations of a hidden layer are independent and identically distributed (i.i.d.). More specifically,
based on the classical Central Limit Theorem (CLT), the prerequisite of an activation of a hidden
layer approaching a Gaussian distribution is that all the activations of the previous layer are i.i.d..
Though the Lyapunov CLT could relax the probabilistic premise to be independence only, all the
previous works do not thoroughly discuss the probabilistic premise except assuming an i.i.d. prior
for all the parameters of DNNs. Due to its extreme importance but unclear status, it is necessary to
thoroughly investigate the probabilistic premise for improving the interpretability of DNNs.
In this work, we demonstrate that the parameters of DNNs are correlated and depend on the training
dataset, i.e., they do not satisfy the i.i.d. prior. Moreover, we demonstrate that activations being
i.i.d. cannot hold for all the hidden layers of DNNs. In the context of Bayesian probability, we
theoretically derive the necessary conditions for the activations of a hidden layer being i.i.d. given
the assumption that the activations of the previous layer are i.i.d.. Subsequently, we experimentally
show that typical DNNs, such as the Multilayer Perceptron (MLP) and the Convolutional Neural
Networks (CNNs), cannot satisfy the necessary conditions. As a result, activations being i.i.d. is not
valid for all hidden layers. In other words, GP with the i.i.d. assumption for the parameters of DNNs
cannot correctly explain all the hidden layers of DNNs. In addition, some previous works show that
GP is sensitive to the curse of dimensionality (Bengio et al., 2005; Hinton et al., 2012) and cannot
clarify the hierarchical representation, an essential of deep learning (Matthews et al., 2018).
1
Under review as a conference paper at ICLR 2020
Alternatively, we propose a novel probabilistic representation for the hidden layers based on the
Gibbs distribution (LeCun et al., 2006) and the Product of Experts (PoE) model (Hinton, 2002). The
probabilistic representation explains a hidden layer in two aspects: (i) the distribution of a hidden
layer can be formulated as a Gibbs distribution, of which the energy function is determined by the
activations, and (ii) the connection between two adjacent layers can be modeled by a PoE model.
Specifically, the distribution of a single neuron or hidden unit (e.g., convolutional channel) can be
expressed as a PoE, in which all the experts are defined by the neurons or hidden units in the previous
hidden layer. Compared to the GP explanations, the proposed probabilistic representation provides
a more specific explanation for the hidden layers of DNNs without any assumption.
Since the output of a hidden layer is commonly the input of the next layer, DNNs form a Markov
chain, thereby corresponding to a joint distribution. Moreover, we confirm the equivalence between
the Stochastic Gradient Descent (SGD) and the variational inference (Mandt et al., 2017; Chaudhari
& Soatto, 2018) and prove the former is also equivalent to the energy minimization optimization
based on the proposed probabilistic representation. As a result, the entire architecture of DNNs can
be explained as a Bayesian network (Nielsen & Jensen, 2009; Koski & Noble, 2011). In particular,
we demonstrate that the hidden layers close to the input formulate prior distributions of the training
dataset and propose a novel regularization approach to improve the generalization performance of
DNNs through pre-training the hidden layers corresponding to prior distributions.
2	Preliminaries
We assume X and Y are two random variables and Pθ(X, Y ) = P(Y |X)P (X) is an unknown
joint distribution between X and Y , where P(X) describes the prior knowledge of X, P(Y |X)
describes the statistical connection between X and Y , and θ indicate the parameters of Pθ(X, Y ).
A dataset D = {(xj, yj)|xj ∈ RM, yj ∈ RL}jJ=1 is composed of i.i.d. samples generated from
Pθ(X, Y ). A neural network with I hidden layers is denoted as DNN = {x; f1; ...; fI; fY }
and trained by D, where (x, y) ∈ D are the input of the DNN and the corresponding training
label, and fγ is an estimation of the true distribution P(Y|X). As a result, X 〜P(X) and
fY ≈ P(Y|X). The meaning of fi is two-fold: (i) denoting all the neurons in the ith hidden layer,
and (ii) representing the activations of the ith hidden layer. In addition, Fi and FY are the random
variables corresponding to fi and fY, respectively. In this work, we use the MLP (Figure 1) to
derive most theoretical conclusions unless otherwise specified.
为1
^m
xM
/12
/22
Y2l.
/13
∣2K
f1N
∕y(1)
ML)
x
fl	fl	fγ
Figure 1: A MLP = {x; f1; f2; fY }, in which f1 has N neurons, i.e., f1 = {f1n = σ1[g1n(x)]}nN=1,
where gin(x) = PM=I αmn ∙ Xm + bin is the nth linear filter, amn is the weight of the edge between Xm
and f1n, b1n is the bias, and σ1 is a non-linear activation function. Similarly, f2 = {σ2 [g2k (f1)]}kK=1 where
g2k(fι) = Pn=I βnk ∙ fin + b2k. In addition, since fγ is the Softmax, fγ = {表exp[gyi(f2)]}N1 where
gyi(f2) = PK=I Ykl ∙ f2k + byi, and the partition function is ZY = PL=I exp[gyi(f2)].
2
Under review as a conference paper at ICLR 2020
3	Background
As a fundamental probabilistic model, Gibbs distribution (a.k.a., energy based model, Boltzmann
distribution, or renormalization group) formulates the dependence of random variables X through
associating an energy to each dependence structure (Geman & Geman, 1984).
P(X; θ) = Zιθ)exp[-E(x; θ)]	(1)
where E(x; θ) is the energy function, θ denotes the parameters, and Z(θ) = x exp[-E(x; θ)]dx
is the partition function. Especially, a Gibbs distribution can be easily reformulated as various
probabilistic models by redefining E(x; θ), e.g., it would become Markov Random Fields (MRFs)
model if the energy function is defined as the summation of multiple potential functions, i.e.,
E(x; θ) = - Pk fk(x; θk) (Wang et al., 2013; Goodfellow et al., 2016). Moreover, assuming a
single potential function defines an expert, i.e., Fk = exp[-fk(x; θk)], we can regard MRFs as a
PoE model, i.e., P(x; θ) = z^ Qk Fk, where Z(θ) = Qk Z(θk) (Hinton, 2002).
A Gibbs distribution has two appealing properties. First, E(x; θ) is the sufficient statistics of
P(x; θ) because Z(θ) only relies on θ, i.e., P(x; θ) entirely depends on E(x; θ). The property
establishes a connection between P(X; θ) and the deterministic function E(x; θ), which allow us
to explain a deterministic hidden layer in a probabilistic way. Second, the energy minimization is
the commonly used optimization for learning θ, i.e., θ* = argmin® E(x; θ) (LeCUn et al., 2006),
which is helpful to explain the training procedure of DNNs, because the energy minimization can
be implemented by the gradient descent algorithm as long as E(x; θ) is differentiable.
Numerous efforts have been devoted to explain DNNs from the viewpoint of Gibbs distribution.
The classical work is the Restricted Boltzmann Machine (RBM) (Salakhutdinov & Hinton, 2009),
which is a simplified Gibbs distribution. Specifically, RBM only considers the connections between
visible units x and hidden units h but overlooks the internal connections within x and h, thus it
cannot explain complex DNNs, e.g., CNNs. To the best of our knowledge, Mehta & Schwab (2014)
first explain the distribution of hidden layers as a Gibbs distribution, and Yaida (2019) indirectly
proves the distribution of fully connected layers is a Gibbs distribution. Lin et al. (2017) clarify
some properties of DNNs based on the Gibbs distribution. However, all previous works still cannot
derive an explicitly Gibbs explanation for complex hidden layers, e.g., convolutional layers.
4	Main results
4.1	The parameters of DNNs do not satisfy the i.i.d. assumption
We examine the statistical properties of the parameters of DNNs and show they are correlated. First,
we derive a necessary condition for the parameters of DNNs satisfying the i.i.d. prior, Second, we
experimentally show that typical DNNs cannot satisfy the necessary condition on the MNIST and
CIFAR-10 datasets. Hence, the parameters of DNNs do not satisfy the i.i.d. assumption.
If all the parameters of DNNs are i.i.d., the parameters of each neuron are identically distributed, e.g.,
{ɑmn}M=1 〜 P(αn) in the MLP (Figure 1). Since all the parameters are i.i.d., different neurons
should be uncorrelated, i.e., ∀n 6= n0, Corr(αn, αn0 ) = 0. In particular, the i.i.d. assumption
enables us to use the sample correlation r(αn, αn0 ) to estimate Corr(αn, αn0).
r(αn, an，) =	MPMM=Igmn-OPamn0-ɑn0)_ F ≈ Corr(αn, αQ	(2)
√Σnτ = i(amn-an ) Em=Igmn，-an0 )
where an = ~M PM=I αmn is the sample mean of {αmn}mM=1 . Therefore, taking into account the
estimation error, |r(an, an，)| close to zero is the necessary condition for all parameters being i.i.d..
We examine the necessary condition for all parameters being i.i.d. in various DNNs. First, we use
the MLP to classify the benchmark MNIST dataset. The information about the network architecture
and training methods is included in Appendix A.1. After training the MLP well, we calculate the
absolute sample correlations, namely, ∣r(an, an0)|, ∣r(βk, βk0)∣, and ∣r(γι, γι0)∣, and derive three
absolute correlation matrices, i.e., A0N×N, BK0 ×K, and CL0 ×L, shown in Figure 2. We find that most
∣r(an, an，)| are close to zero, but most ∣r(βk, βk0)| and ∣r(γι, γι0)| are non-zero. That indicates
the necessary condition cannot hold for all the parameters of the MLP.
3
Under review as a conference paper at ICLR 2020
Figure 2: Three absolute correlation matrices for the weights in f1, f2, and fY . In addition, 0.08, 0.26, and
0.31 are the average of the absolute correlation coefficients for all the weights in f1, f2, and fY , respectively.
Second, we construct a shallow neural network with a single fully connected hidden layer, namely,
NN = {x; f; fY }, for classifying the MNIST dataset based on the seminal work (Neal, 1994) in
Appendix A.2, and construct a CNN for classifying the CIFAR-10 dataset in Appendix A.3. Using
the same method in the MLP, we obtain the absolute sample correlations between the weights of
different nodes in the both neural networks. The results in Appendix A.2 and Appendix A.3 show
that the weights of DNNs do not satisfy the necessary condition.
Third, we visualize all the parameters of the NN = {x; f; fY } for classifying a synthetic dataset
in Appendix A.4. Since the synthetic dataset is much simpler than MNIST, we can directly show
the correlation of different parameters and their dependences on the dataset by visualizing them.
Overall, these simulations demonstrate that the parameters of DNNs are not i.i.d..
4.2	Activations being i.i.d. is not valid for all the hidden layers of DNNs
Although we demonstrate that the parameters of DNNs do not satisfy the i.i.d. assumption, we are
still unclear whether the activations of a hidden layer are i.i.d.. In this section, we demonstrate that
activations being i.i.d. is not valid for all the hidden layers of DNNs.
In the context of Bayesian probability, all the weights and biases of the MLP (Figure 1) have prior
distributions, i.e., αmn 〜P(Amn), βnk 〜P(Wnk), bin 〜P(Bin), and b2k 〜P(B2k), thus We
regard Gin = PM=I AmnXm + Bin as the random variable of gin(x) = PM=I αmn ∙ Xm + bin,
and G2k = PN=I WnkFin + B2k as the random variable of g2k(f1) = PN=I βnk ∙ fin + b2k.
Hence, the random variables of tWo arbitrary activations in f1 and f2, e.g., fin and f2k, can be
expressed as Fin = σi (Gin) and F2k = σ2(G2k), respectively.
Given tWo random variables A and B, the necessary conditions for them being i.i.d. are that they are
uncorrelated (i.e., Cov(A, B) = 0) and have the same expectation (i.e., E(A) = E(B)). Similarly,
assuming {Fin}N=i are i.i.d. and {fin}N=i 〜 P(Fi), we derive the following necessary conditions
for the activations of the second hidden layer, i.e. ,{F2k}kK=i, being i.i.d..
∀(k, k0) ∈ Si = {(k, k0) ∈Z2 |k = k0,1 ≤ k ≤ K, 1 ≤ k0 ≤ K}, we must have
Cov(G2k,G2k0) = Cov(Wk, Wk0)E2(Fi) + Cov(Wk, B2k0)E(Fi)	(3)
+ Cov(Wk0, B2k)E(Fi) + Cov(B2k, B2k0) =0
E(Fi)E(Wk)+E(B2k) =E(Fi)E(Wk0)+E(B2k0)	(4)
σ2(∙) is strictly increasing, differentiable, and invertible.	(5)
where Wk = PnN=i Wnk. Equation 3 and 4 are the necessary conditions for {G2k}kK=i being i.i.d..
Equation 5 constrains the activation function σ2(∙) to guarantee that if {G2k}/ are i.i.d. then
{F2k}kK=i are i.i.d. as well. The necessary conditions are valid for arbitrary fully connected layers
as long as properly changing the subscripts. The detailed derivations are included in Appendix B and
Appendix C. Moreover, we prove that the necessary conditions also hold for convolutional layers
based on the connection between fully connected layers and convolutional layers in Appendix E.
4
Under review as a conference paper at ICLR 2020
Table 1: The statistical measures for examining the necessary conditions
Layer	Activation expectation	Covariance	Weight expectation
f1	E(X)	Cov(An, An0)	E(An)
f2	E(F1)	Cov(Wk,Wk0)	E(Wk)
fY	E(F2)	Cov(Cl,Cl0)	E(Cl)
{Xm}M=1 〜P (X), {fln}N=i 〜P (Fl), {f2k }K=1 〜P H)
An = ^Pm=I Amn, Where αmn 〜P(Amn)
Wk = Pn=I Wnk, where βnk 〜P(Wnk)
Cl = PK=I Ckl, where Ykl 〜P(Ckl)
It is noteworthy that the i.i.d. assumption for the parameters of DNNs indeed satisfy the necessary
conditions for activations being i.i.d.. Nevertheless, since the i.i.d. prior is not an appropriate prior
for the parameters of DNNs, it cannot guarantee activations of a hidden layer being i.i.d..
The theoretical necessary conditions indicate an applicable approach to examine whether activations
of a hidden layer being i.i.d. given the assumption that the activations of the previous layer are i.i.d..
More specifically, since {Fin}N=ι are i.i.d., we can use the sample mean f1 to estimate E(F1).
Because the samples of the training dataset D are also assumed to be i.i.d., we can train the MLP
multiple times to draw multiple independent observations of Wnk and B2k, thus we can use the
sample covariance to estimate Cov(Wk, Wk0), Cov(Wk, B2k0), and etc.
Based on the empirical approach, we estimate the statistical measures (Table 1) to examine whether
the activations of the three layers (i.e., f1, f2, and fY ) being i.i.d. in the MLP for classifying the
MNIST dataset. Without loss of generality, we restrict all the layers of the MLP from using bias to
decrease computation complexity, thus the necessary conditions for f2 can be simplified as
∀(k, k0) ∈ S1 = {(k, k0) ∈ Z2|k 6= k0, 1 ≤ k ≤ K, 1 ≤ k0 ≤ K}, we must have
Cov(G2k, G2k0) = Cov(Wk, Wk0)E2(F1) =0
E(F1)E(Wk) = E(F1)E(Wk0)
(6)
(7)
First, Figure 3 shows the correlation matrices, i.e., AN×N, WK×K, and CL×L, to check if their cor-
responding covariances, i.e, C ov(An, An0), Cov(Wk, Wk0), and Cov(Ck, Ck0), are close to zero.
It shows that many correlations are far from zero, so their corresponding covariances are not close
to zero either. Second, we use the sample mean, e.g., X, to estimate the expectation, e.g., E(X).
Taking into account the estimation error, we can regard E(X) = 0 based on X = 0.131. However,
fι = 0.436 and f2 = 0.498 imply E(F1) = 0 and E(F2) = 0 even considering the estimation
error. Therefore, the activations of the last two hidden layer, i.e., f2 and fY , cannot be independent
because they cannot satisfy the first necessary condition (Equation 6). In addition, we show the
necessary condition for activations being identically distributed also cannot be satisfied by the MLP
in Appendix F.1. Furthermore, we demonstrate that the necessary conditions for activations being
i.i.d. also cannot be satisfied in the CNN for classifying the CIFAR-10 dataset in Appendix F.2
fι
An×n
-1.0
-0.8
-0.6
-0.4
L 0.2
Figure 3: Three absolute correlation matrices for showing whether Cov(An, An0), C ov(Wk, Wk0), and
C ov(Ck, Ck0) are close to zero.
5
Under review as a conference paper at ICLR 2020
Overall, we conclude that activations being i.i.d. is not valid for all the hidden layers of DNNs based
on the theoretical necessary conditions and the experimental simulations on typical DNNs. In other
words, we cannot use the CLT to establish the equivalence between GP and all the hidden layers of
DNNs, thus GP can not correctly clarify all the hidden layers of DNNs. That necessitates a more
general probabilistic representation for explaining the hidden layers of DNNs.
4.3	The distribution of a hidden layer is a Gibbs distribution
This section proves that the distribution of a hidden layer can be formulated as a Gibbs distribution
and the statistical connection between two adjacent layers can be modeled as a PoE model.
Assuming the output layer in the MLP (Figure 1) is the softmax, its distribution can be expressed as
P(FY) = {P(fyi) = ɪ exp(fyi)}L=ι	(8)
ZY
where ZY = PL=I exp(fyi) is the partition function. Since fyi = PK=I Yki ∙ f2k + byi, We have
P(FY) = {P (fyl )
1K
z— exP(E Yki ∙ f2k + byl)}iti
ZY	k=1
(9)
Based on the properties of the exponential function, i.e., exp (a + b) = exp(a) ∙ exp(b) and
exp(a ∙ b) = [exp(b)]a, we can reformulate P(FY) as
1K
P(FY) = {P(fyi) = Zr ∏[exp(f2k)]γkl}L=1
ZY k=1
(10)
where ZY0 = ZY /exp(byi). Since {exp(f2k)}kK=1 are scalar, we can introduce a new partition
function Zf? = PK=I exp(f2k) such that {Z^exp(f2k)}K=1 become a probability measure. As a
result, we can further reformulate P(FY) = {P (fyi)}iL=1 as a Product of Expert (PoE) model as
P(FY) = {P(fyi) = ɪ Y[占exp(f2k)]γkl }L=ι	(11)
where ZY = ZY/[exp(byi) ∙ QK=I [Zf2]γkl] and each expert is defined as Z1rexp(f2k).
It is noteworthy that all the experts { Z^exp(f2k)}K=ι form a probability measure and establish an
exact one-to-one correspondence to all the neurons in the second hidden layer f2, i.e., {f2k}kK=1.
Therefore, the distribution of f2 can be expressed as
P (F2) = {P(f2k) = ɪ exp(f2k )}KK=1	(12)
ZF2
Based on the definition of Gibbs distribution, Equation 8 and 12 show that fY and f2 formulate
two multivariate Gibbs distributions and their energy functions are equivalent to the negative of the
output nodes {fyi}iL=1 and the neurons {f2k}kK=1, respectively. In addition, Equation 11 implies that
the connection between P (fyi) and P(f2k) can be expressed as the PoE model, namely that P (fyi)
is equal to the product of all the experts {P (f2k)}kK=1 with their respective weights {Yki}kK=1.
Moreover, we prove that the probabilistic representation is valid for other hidden layers in the MLP.
Since {f2k = σ2 (PnN=1 βnk • fin + b2k)}KK=ι, P(F2) can be expressed as
1N
P (F2 ) = {P (f2k ) = —exp[σ2(V βnk • fin + b2k )]}K=1	(13)
ZF2	n=1
Based on the equivalence between the gradient descent algorithm and the first order approximation
(Battiti, 1992), we prove that σ2(PnN=i βnk • fin + b2k)] can be approximated as
NN
σ2 (X βnk •	fin	+	b2k)] ≈	C2i	•	[X βnk	•	fin + b2k] + C22	(14)
6
Under review as a conference paper at ICLR 2020
where C21 and C22 only depend on the activations {f1n}nN=1 in the previous training iteration, thus
they can be regarded as constants and absorbed by βnk and b2k. The proof for the approximation is
included in Appendix G. Therefore, P(F2) also can be modeled as a PoE model.
P(F2)
{P (f2k) ≈
exp(f1n)]βnk}kK=1
(15)
where ZF2 = Z&/[exp(b2k) ∙ QN=1lZιι1 ]βnk] and Zfi = PN=I exp(fin).
Finally, the distribution of the first hidden layer f1 is formulated as
P(FI) = {P(fin) = ɪ exp(fin)}N=ι	(16)
ZF1
Overall, we introduce a novel probabilistic representation for fully connected layers in two aspects.
First, a fully connected layer formulates a multivariate Gibbs distribution (Equation 8, 12, and 16)
and the energy functions is equivalent to the negative activation. Second, the connection between
two adjacent fully connected layers can be formulated as a PoE model (Equation 11 and 15).
The proposed probabilistic representation has two advantages over the GP explanations. First, the
FoE explanation does not require any assumption, e.g., we formulate P(f2k) as a PoE model, in
which all the experts {	exp(fin )}NN=ι are defined by all the neurons {fιn}N=ι. As a comparison,
the GP explanation derives P(f2k) as a Gaussian distribution in the limit of infinite neurons, i.e.,
{f1n }nN=→1∞, under the assumption that {f1n }nN=1 are i.i.d.. Second, the Gibbs distribution provides
a more explicit explanation for the functionality of a fully connected layer. Specifically, a Gibbs
distribution derives the probability of the multiple features defined by the neurons occurring in the
input, but the covariance function of a GP cannot clearly explain the functionality of each neuron.
Moreover, we prove that a convolutional layer formulates a specific Gibbs distribution, namely
the MRF model, in Appendix H. More specifically, a convolutional layer fi with K convolutional
channels, i.e., f = {fk = σi(PN=ι S(k,n) ◦ fi-ι + bk ∙ 1)}K=ι, formulateaMRF model as
1K	1 K
P(Fi) =汨 UeXP[谈(fi-ι)] = Z-exp[]q谈(fi-ι)]	(17)
where fi-ι = {fi-1}N=ι is the input, φk(fi-ι) = Pn=I S(Jkrn ◦ fi-1 + bk ∙ 1) is the kth linear
convolutional channel, S(k,n) is the kth convolutional filter applying to the nth input channel, bik
is the bias, σi(∙) is an non-linear operator, and the partition function ZFi =R exp[PK=ι wk(fi-1)]dfi-1.
The corresponding energy function EFi=- PK=I φk(fi-ι).
In summary, the distribution of a hidden layer fi can be formulated as a Gibbs distribution P(Fi),
especially the energy function EFi establish a connection between the deterministic function of the
hidden layer and the Gibbs distribution P(Fi). The correspondence between Gibbs distributions
and hidden layers can be summarized by Table 2.
Table 2: The correspondence between hidden layers and Gibbs distributions
Architecture	neuron f1n	fully connected layer fl = {fln}N=i	conv. channel fk	conv. layer fi = {fik}kK=1
Explanation	energy function	discrete Gibbs	energy function	MRF
	EF1n =-f1n	{P(f1n)}nN=1	EFi=-PK=1 g k(fi-1)	ZF-exp[pK=ι rk(fi-1)] Fi
We use Equation 16 and Equation 17 as examples to illustrate the correspondence.
7
Under review as a conference paper at ICLR 2020
Figure 4: The top-left figure shows a 32 ×32 synthetic image as the input, which is sampled from the Gaussian
distribution N (0, 1) and sorted in the primary diagonal direction by the descending order. The remaining
figures visualize all the weights {αmn}1m0=241 of the seven neurons. In addition, the subscript in each figure, i.e.,
(gn(x), fn(x), P (fn)), denotes the linear output gn(x), activation fn(x), and Gibbs probability P(fn).
5	Simulations
This section uses two synthetic datasets to demonstrate the proposed probabilistic representation:
(i) the distribution of a fully connected layer is a multivariate discrete Gibbs distribution and (ii) the
distribution a convolutional layer is a MRF model.
5.1	The distribution of a fully connected layer is a multivariate discrete
Gibbs distribution
A fully connected layer with N neurons, i.e., f = {fn}nN=1, formulates a multivariate discrete
Gibbs distribution to describe the probability of the N features defined by their respective neurons
occurring in the input. The energy function is equal to the negative activation, i.e., Efn = -fn(x).
P (F) = {P (fn) = 1Γ exΡ[fn(x)]}n=i	(18)
ZF
where fn(x) = σ[gn(x)], gn(χ)=PM=ι αmn∙xm+bn is a linear filter with αmn being the weights and
bn being the bias, σ(∙) is the activation function, and ZF=PN=I exp[fn(x)] is the partition function.
Since benchmark datasets consist of very complex features and typical DNNs have too complicated
architecture, it is very hard to directly demonstrate the Gibbs distribution explanation. Alternatively,
we choose the neural network for classifying the synthetic dataset in Appendix A.4, because the
neural network only has a single fully connected hidden layer with seven neurons, i.e., N = 7, and
the synthetic dataset only has four features. After the network is trained well, we choose a synthetic
image as the input and show all the information of the learned hidden layer in Figure 4.
Since the input image is sorted in the primary diagonal direction, the input feature can be described
as high values above the secondary diagonal (the red line) and low values below the red line. Figure
4 shows that Neuron 1, Neuron 2 and Neuron 4 correctly describe the input feature, thus they derive
large positive outputs (i.e., g1(x) = 122.09, g2(x) = 113.70, and g4(x) = 140.20). However,
Neuron 6 and Neuron 7 do not describe the input feature correctly, thus they derive large negative
outputs (i.e., g6(x) = -134.08, g7(x) = -133.36). After applying the sigmoid function, we have
f1(x) = f2(x) = f4(x) =1 and f6(x) = f7(x) = 0. Finally, we derive the probability of different
neurons, i.e., P(f1) = P(f2) = P(f4) = 0.20 and P(f6) = P(f7) = 0.07, which indicates that the
features defined by Neuron 1, Neuron 2 and Neuron 4 have high probability occurring in the input
but the features defined by Neuron 6 and Neuron 7 have low probability occurring in the input.
8
Under review as a conference paper at ICLR 2020
Digit 2
Digit 1
Digit 8
Digit 0
Digit 5
A
XXXXX
Figure 5: The first row shows five synthetic images of handwritten digits, the second row visualizes their
respective histograms, and the red curve indicates the Gaussian distribution N (0, 1024).
Λ
5.2	The distribution of a convolutional layer is a MRF model
Since the distributions of benchmark datasets are unknown, it is impossible to use them to verify
the probabilistic explanation for convolutional layers. Alternatively, we generate a synthetic dataset
obeying the Gaussian distribution N(0, 1024) based on the NIST dataset of handwritten digits. It
consists of 20,000 32 × 32 grayscale images in 10 classes (digits from 0 to 9), and each class has
1,000 training images and 1,000 testing images. Figure 5 shows five synthetic images and their
perspective histograms. The method for generating the synthetic dataset is included in Appendix I.
We design a CNN = {x; f1; f2; f3; f4; fY } for classifying the synthetic dataset. The CNN has
two convolutional layers with ReLU (i.e., f1 and f3), two max-pooling layers (i.e., f2 and f4) and
a softmax output layer fY . The detailed information about the CNN is included in Appendix I.
Based on the MRF explanation (Equation 17), the distribution of f1 can be formulated as
1	20	1	20
P (FI) =二 Y exp[φk(X)] = TeXP[X φk(X)]
F1 k=1	F1	k=1
(19)
where φk (x) = Sk ◦ X + bk ∙ 1 is the kth linear convolutional channel, Sk is the linear convolutional
filter and b1k is the bias. The energy function of P(F1) is equivalent to EF1 = - P2k0=1 f1k.
In order to model a high dimensional dataset, e.g., the 32 × 32 synthetic image, we typically need
a multivariate distribution with the same dimension, e.g., the dimension of the covariance function
of a Gaussian process should be 1024 × 1024. However, most structured probabilistic graphical
models, especially the MRF model, have two fundamental assumptions: stationary and Markov
(Lyu & Simoncelli, 2007). The former means that the distribution is independent on the location,
i.e., the distribution of piXels at different locations are identically distributed. The later indicates
that the distribution of a single piXel is independent on the other piXels given its neighbors. The two
assumptions allow us to use the histogram of samples to simulate the true distribution.
Digit
10 15 20 25 30
P(X) KLD:0.62
§ B
-2 -3-1
10 10
kx)d16o Z
30
20
P(Fi) KLD:0.83
-210 -310-1
一(£工一 6。Z
。力
Figure 6: The red curve indicates the truly prior distribution P (X) = N (0, 1024). The blue curves
are different histograms. (A) the synthetic image x is the input. (B) the histogram of x, i.e., P (x), and
KL[P (X)||P (x)] = 0.62. (C) the histogram of EF1 forestimatingP(F1) andKL[P(X)||P(F1)] = 0.83.
9
Under review as a conference paper at ICLR 2020
Digit	Energy
Channel2	Channel3	Channel4
Channel1
KLD:10.16
KLD:16.15
KLD:21.82
KLD:14.77
KLD:0.94	KLD:0.49
10-2
10-3
-100	0	100	-100	0	100	-100	0	100	-100	0	100	-100	0	100	-100	0	100
Figure 7: The firs row shows an synthetic image, the energy functions ofP(F1) and five different experts, i.e.,
{P(FT) = z1-exρ(fn)}n=ι. The second row shows their respective distributions, and the KLDs denote
F1n
their distances to the true distribution, i.e., KLD[P (X)||P (F1n)].
After the CNN is well trained, we randomly choose a testing image x as the input to derive P (F1).
Since X 〜 P(X), we can use the histogram of the testing image, i.e., P(x), to estimate P(X).
In addition, EF1 is a sufficient statistics for P(F1) such that we can use the histogram of EF1
to estimate P(F1). As a result, we can verify the probabilistic representation by calculating the
distance between P(F1) and P(X), i..e, KL[P (X)||P (F1)]. We visualize their distributions in
Figure 6, which shows that P(F1) is very close to P(X), i.e., KL(P (X)||q(F1)) = 0.83, which
demonstrates the distribution of a convolutional layer can be modeled as a MRF model.
In addition, the experiment provides a new viewpoint to demonstrate the activations of different
convolutional channels are not i.i.d.. Given another synthetic image, we visualize the output of five
convolutional channels in f1 in Figure 7. It shows that the activations of different channels at the
same position are correlated, i.e., they are not independent. We also estimate the distribution of each
convolutional channel based on the histograms of their respective energy functions. It shows that
the distributions of different channels are different, i.e., they are not identically distributed.
6	Discussion
Based on the proposed probabilistic explanations for hidden layers, we discuss certain theoretical
and practical topics of deep learning. First, we further confirm the equivalence between SGD and the
variational inference. Second, we demonstrate that the entire architecture of DNNs can be explained
as a Bayesian network, in which the hidden layers close to the input formulate prior distributions of
the training dataset. Third, we propose a novel regularization method to improve the generalization
performance of DNNs by pre-training the hidden layers corresponds to the prior distributions.
6.1	SGD can be explained as a variational inference
Recent works prove that SGD works as a specific Bayesian posterior inference, i.e., the variational
inference (Mandt et al., 2017; Chaudhari & Soatto, 2018). Here we further confirm the Bayesian
explanation for SGD based on the proposed probabilistic representation. Briefly speaking, given
a DNN = {x; f1; ...; fI; fY }, we prove that the conditional distribution of fY given x, i.e.,
P(FY |X), is also a Gibbs distribution and its energy function is equivalent to the deterministic
function formulated by the entire architecture of the DNN, i.e., the DNN describes a family of
Gibbs distribution Q. Based on the definition of variational inference, SGD aims to find a Gibbs
distribution P*(FY|X) such that it can minimize the distance between P(FY|X) and the true
posterior distribution P(Y |X) is determined by the training dataset D.
P *(Fγ |X) = argmin KL [P (Y |X )||P (Fγ |X)]	(20)
P∈Q
Moreover, we prove that SGD is also equivalent to the energy minimization optimization, which in
turn validates the Gibbs explanation for the hidden layer of DNNs. The proof is in Appendix J.
10
Under review as a conference paper at ICLR 2020
We justify the theoretical explanation for SGD based on the CNN classifying the synthetic dataset
and the MLP classifying the MNIST dataset in Appendix K.1. In addition, we demonstrate that the
learned parameters via SGD are not i.i.d. based on in Appendix K.2, which indirectly justifies that
GP with the i.i.d. assumption for the parameters of DNNs cannot correctly clarify DNNs.
6.2	The entire architecture of DNNs can be explained as a Bayesian network
Since the output of the hidden layer fi in the DNN = {x; f1; ...; fI; fY } is commonly the input
of the next layer fi+ι, the DNN formulates a Markov chain Fi → …→ FI → Fγ. As a result,
the distribution of a hidden layer in the DNN should be regarded as a conditional distribution and
the entire architecture of the DNN can be formulated as a joint distribution.
P(Fγ; FI..; Fi|X) = P(FYIFI) ∙... ∙ P(Fi+i㈤)∙... ∙ P(F1|X)	(21)
Richard & Lippmann (1991) prove that P(FY|X) is an estimation of the true posterior distribution
P (Y ∣X) a P (X ∣Y )P (Y) and we demonstrate that P (F1∣X) isa prior distribution of the training
dataset and the equivalence between SGD and the variational inference, thus the entire architecture
of DNNs can be explained as a Bayesian network. That provides a probabilistic explanation for the
hierarchy property of DNNs, which cannot be clarified by GP (Matthews et al., 2018).
6.3	A novel regularization algorithm for DNNs
Bayesian theory indicates that a better prior distribution corresponds to a better posterior distribution
given the same likelihood distribution, thus pre-training the hidden layers corresponding to prior
distributions should be an effective approach to improve the generalization performance of DNNs.
Erhan et al. (2010) first demonstrate that the unsupervised pre-training is an effective regularization
for DNNs through capturing the prior knowledge of the input. However, since they do not know
which hidden layer corresponds to a prior distribution, they have to pre-training all the hidden layers,
which restricts its wide application due to the computation complexity. In addition, Le et al. (2018)
prove that incorporating the prior knowledge of the input results in uniform stability and provides a
bound on generalization error for a specific DNN, i.e., the supervised auto-encoders.
Based on the proposed probabilistic representation, we first justify that a better prior distribution
indeed has a positive correlation to the generalization performance in Appendix K.3. Subsequently,
we propose a novel a novel regularization algorithm to improve the generalization performance of
DNNs through pre-training the hidden layers only corresponding to prior distributions and show
that it outperforms classical regularizations, e.g., dropout (Srivastava et al., 2014), for the image
recognition task on the benchmark CIFAR-10 dataset in Appendix L.
7	Conclusion
In this paper, we demonstrate that the parameters of DNNs do not satisfy the i.i.d assumption and
the activations being i.i.d. cannot be valid for all the hidden layers of DNNs. As a result, GP cannot
correctly explain all the hidden layer of DNNs. Alternatively, we propose a novel probabilistic
representation for DNNs in four aspects: (i) a hidden layer formulates a Gibbs distribution, of which
the energy function is determined by the activations, (ii) the connection between two adjacent layers
can be modeled by a PoE model, (iii) the entire architecture of DNNs can be explained as a Bayesian
network, and (iv) the SGD can be explained as the Bayesian variational inference algorithm.
There are two general directions for future research. The theoretical direction is to examine the
probabilistic representation in other popular DNNs, e.g., the residual networks, which can further
verify the proposed probabilistic representation. The practical direction is to validate the proposed
regularization in more complex DNNs and datasets.
11
Under review as a conference paper at ICLR 2020
References
Roberto Battiti. First and second order methods for learning: Between steepest descent and newton’s
method. Neural Computation, 4:141-166,1992.
Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux. The curse of dimensionality for local
kernel machines. Technical Report 1258, 2005.
David Blei, Alp Kucukelbir, and Jon MaAuliffe. Variational inference: A review for statisticians.
Journal of Machine Learning Research, 112:859-877, 2017.
M. A. Carreira-Perpinan and Geoffrey E. Hinton. On contrastive divergence learning. Artificial
Intelligence and Statistics, pp. 33-41, 2005.
Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference,
converges to limit cycles for deep networks. In 2018 Information Theory and Applications Work-
shop (ITA), pp. 1-10. IEEE, 2018.
Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and
Samy Bengio. Why does unsupervised pre-training help deep learning? Journal of Machine
Learning Research, 11(Feb):625-660, 2010.
Adri Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional net-
works as shallow gaussian processes. In ICLR, 2018.
S. Geman and D. Geman. Stochastic relaxation, gibbs distributions, and the bayesian restoration
of images. IEEE Transactions. on Pattern Analysis and Machine Intelligence, pp. 721-741, June
1984.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.
K He, X Zhang, S Ren, and J Sun. Deep residual learning for image recognition. In CVPR, pp.
770-778, 2016.
G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition.
IEEE Signal Processing Magazine, 2012.
Geoffrey E. Hinton. Training products of experts by minimizing contrastive divergence. Neural
Computation, 14:1771-1800, 2002.
Timo Koski and John Noble. Bayesian networks: an introduction, volume 924. John Wiley & Sons,
2011.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technique Report, 2009.
Lei Le, Andrew Patterson, and Martha White. Supervised autoencoders: Improving generaliza-
tion performance with unsupervised regularizers. In Advances in Neural Information Processing
Systems, pp. 107-117, 2018.
Y. LeCun, L. Bottou, and P. Haffner. Gradient-based learning applied to document recognition.
Proceedings of the IEEE, 11:2278-2324, 1998.
Yann LeCun, Sumit Chopra, Raia Hadsell, Marc’Aurelio Ranzato, and Fu Jie Huang. A tutorial on
energy-based learning. MIT Press, 2006.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. In ICLR, 2018.
Henry W Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work so well?
Journal of Statistical Physics, 168(6):1223-1247, 2017.
Siwei Lyu and Eero. P. Simoncelli. Statistical modeling of images with fields of gaussian scale
mixtures. In NeurIPS, 2007.
12
Under review as a conference paper at ICLR 2020
Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate
bayesian inference. The Journal of Machine Learning Research ,18(1):4873-4907, 2017.
Alexander Matthews, Mark Rowland, Jiri Hron, Richard E. Turner, and Zoubin Ghahramani. Gaus-
sian process behaviour in wide deep neural networks. In ICLR, 2018.
Pankaj Mehta and David J. Schwab. An exact mapping between the variational renormalization
group and deep learning. arXiv preprint arXiv:1410.3831, 2014.
Radford M. Neal. Priors for infinite networks. Technical report no. crg-tr-94-1, 1994.
Thomas Dyhre Nielsen and Finn Verner Jensen. Bayesian networks and decision graphs. Springer
Science & Business Media, 2009.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abo-
lafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with
many channels are gaussian processes. In ICLR, 2018.
M. D. Richard and R. Lippmann. Neural network classifiers estimate bayesian a posteriori proba-
bilities. Neural Compution, 3:461-483, 1991.
Stefan Roth and Michael J. Black. Fields of experts: A framework for learning image priors. IEEE
Conf. on CVPR, pp. 860-867, July 2005.
Stefan Roth and Michael J. Black. Fields of experts. International Journal of Computer Vision, 82:
205-229, 2008.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by
back-propagating errors. Nature, 323:533-536, October 1986.
Ruslan Salakhutdinov and Geoffrey Hinton. Deep boltzmann machines. In AISTATS 2009, pp.
448-455, 2009.
U. Schmidt, K. Schelten, and S. Roth. Bayesian deblurring with integrated noise estimation. In
IEEE Conf. on Computer Vison and Pattern Recognition (CVPR), pp. 2625-2632, 2011.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research, 15:1929-1958, 2014.
C. Wang, N. Komodakis, and N. Paragios. Markov random field modeling inference and learning in
computer vision and image understanding: A survey. Computer Vision and Image Understanding,
117:1610-1627, 2013.
Sho Yaida. Non-gaussian processes and neural networks at finite widths. arXiv preprint
arXiv:1910.00019, 2019.
13
Under review as a conference paper at ICLR 2020
Table 3: The architecture of the MLP for MNIST classification
Layer	Description	Dimension	Filter dimension (M × N)
x	Input	784	—
f1	FC(sigmoid)	128	128 × 784
f2	FC(sigmoid)	64	64 × 128
fY	Output(softmax)	10	10× 64
FC denotes the fully connected layer
Dimension: the number of nodes (i.e., neurons) in every layer.
Filter dimension (M × N): a layer has M filters with N × 1 dimension
A The parameters of DNNs cannot satisfy the i.i.d. prior
A.1 A MLP on the MNIST dataset
We design a MLP (Figure 1) to classify the benchmark MNIST dataset. The MLP has two hidden
layers, each hidden layer is a fully connected layer with the sigmoid activation function, and the
output layer is the softmax. Table 3 summarizes the architecture of the MLP.
After the MLP is well trained (its training and testing accuracy is 97.7% and 97.1%, respectively),
we quantify the correlation of different weights in the same layer based on Equation 22 and derive
the correlation matrix for all the weights in every hidden layer.
r(αn, αn0)
Pm=1 (αmn -αn )(αmn0 - an0 )
P^Pm=1 (αmn - αn) PPm=1 (αmn0 - an0 )
≈ C orr(αn, αn0)
(22)
where 瓦吐= M Pm=I amn is the sample mean of {αmn}M=ι.
To examine if different weights are correlated, we only need to check whether their correlation
coefficients are close to zero, so we take the absolute value of the correlation matrix element-wisely.
Figure 8 visualizes the correlation matrices of each hidden layer, i.e., A0N ×N , BK0 ×K, and CL0 ×L.
We find that most weights in f1 are uncorrelated because their correlation coefficients are very
close to zero, i.e., ∣r(αn, αno)| ≈ 0, but many weights in f2 and fγ are correlated to others, i.e.,
∣r(βk, βko)| = 0 and ∣r(γι, Yi，)| = 0. In addition, We derive the average of the absolute correlation
coefficients of all the weights in each layer and show them at the top of Figure 8, which further
validates that the weights of some hidden layer are correlated.
Therefore, the parameters of the MLP cannot satisfy the i.i.d. prior.
f1 (0.08)
-1.0
-0.8
-0.6
-0.4
kθ,2
f2 (0.26)
-1.0
-0.8
-0.6
-0.4
[0.2
fγ(031)
CLXL
-0.8
-0.6
1.0
0.4
0.2
Figure 8: Three absolute correlation matrices for the weights in f1, f2, and fY . In addition, 0.08, 0.26, 0.31
are the average of the absolute correlation coefficients for all the weights in f1, f2, and fY , respectively.
14
Under review as a conference paper at ICLR 2020
Table 4: The architecture of the NN = {x; f ; fY } for MNIST classification
Layer	Description	Dimension	Filter dimension (M × N)
x	Input	784	—
f	FC(sigmoid)	128	128 × 784
fY	Output(softmax)	10	10 × 128
FC denotes the fully connected layer
Dimension: the number of nodes (i.e., neurons) in every layer.
Filter dimension (M × N): a layer has M filters with 1 × N dimension
A.2 A Neural Network with a single hidden layer on the MNIST dataset
Based on the seminal work (Neal, 1994), we design a neural network with a single fully connected
hidden layer (Figure 9) for classifying the benchmark MNIST dataset (LeCun et al., 1998). The input
layer dimension is 1 × 784, the hidden layer has 128 neurons with the sigmoid activation function,
and the output layer is the softmax with 1 × 10 dimension. Table 4 summarizes the architecture of the
network. In addition, {αmn}mM=1 denote the weights of the nth linear filter in the fully conntected
hidden layer and {βnl}nN=1 are the weights of the lth linear filter in the output layer. Therefore, the
hidden layer has 128 linear filters with 1 × 784 dimension, i.e., {αmn}7m8=4 1 . The output layer has
totally 10 linear filters and their dimension is 1 × 128, i.e., {βnl}1n2=81.
%1
xm
xM
fγ(1)
fγ(L)
XffY
Figure 9: A neural network with a single hidden layer NN = {x; f ; fY }.
After the network is well trained (its training and testing accuracy is 97.4% and 96.9%, respectively),
We calculate the absolute sample correlations, i.e., ∣r(αn, an)| and ∣r(βk,βko)|, and derive two
absolute correlation matrices, i.e., A0N×N and BL0 ×L, based on Equation 2. They are shown in
Figure 10. Wefind thatmost ∣r(αn, an)| are close to zero, but ∣r(βk,0k，)| not. That indicates the
parameters of the NN do not satisfy the necessary condition, thus they cannot satisfy the i.i.d. prior.
Figure 10: Two absolute correlation matrices for the weights in f and fY . In addition, 0.08 and 0.29 are the
average of the absolute correlation coefficients for all the weights in f and fY , respectively.
15
Under review as a conference paper at ICLR 2020
Table 5: The architectures of the CNN for CIFAR-10 classification
Layer	Description	Output dimension Filters dimension Correlation matrix
xf1f2f3f4fY
Input
Conv (3 × 3) + Maxpool
Conv (3 × 3) + Maxpool
FC + Sigmoid
FC + Sigmoid
Output(softmax)
32 × 32 × 3
16 × 16 × 64
8 × 8 × 128
1024
256
10
3 × 3 × 3 × 64
3 × 3 × 64 × 128
8192 × 1024
1024 × 256
256 × 10
A192×192
B8192×8192
C1024×1024
D256×256
E10×10
A.3 A CNN on the CIFAR- 1 0 dataset
We design a CNN to classify the benchmark CIFAR-10 dataset (Krizhevsky, 2009). The CNN
has two convolutional layers without activation function, two max-pooling layers for dimension
reduction, and two fully connected layers with the sigmoid activation function. In addition, the
output layer is the softmax. Table 5 summarizes the architecture of the CNN.
After we train the CNN well (its training and testing accuracy are 98.0% and 69.4%, respectively).
We quantify the correlation of different weights in the same layer based on Equation 2, thereby
deriving the absolute correlation matrix for all the weights in every hidden layer.
Since x has 3 channels and f1 has 64 channels, f1 has totally 3 × 64 = 192 filters. Hence, the
dimension of the correlation matrix for f1 is 192 × 192. In particular, we flatten the output of the
second convolutional layer, i.e., reshaping 8 × 8 × 128 to 1 × 8192, thus the filter dimension of f3 is
8192 × 1024. Table 5 shows the dimension of the correlation matrices for all the layers in the CNN.
Figure 11 shows the absolute correlation matrices for every layer. We can find that most weights are
correlated in f1 and f2, i.e., the weights in the convolutional layers are correlated. In addition, the
weighs in the output layer are also shown to be correlated. However, the weights in f3 and f4 are not
correlated, because their correlation coefficients are close to zero. In other words, the weights in the
fully connected layers are uncorrelated. Moreover, we derive the average of the absolute correlation
coefficients of all the weights in each layer and show them at the top of Figure 11, which further
validates the correlation of the weights in each layer. Overall, the parameters of the CNN cannot
satisfy the necessary condition. Therefore, they cannot satisfy the i.i.d. prior.
f4 (0.17)
°256 × 256
I- 1.0
-0.8
-0.6
-0.4
-0.2
fγ(0.26)
ElOXlo
I- 1.0
0.8
-0.6
-0.4
-0.2
Figure 11: Five absolute correlation matrices for the weights in the layer of CNN. The number at the top of
the figure is the average of the absolute correlation coefficients for all the weights of the layer
16
Under review as a conference paper at ICLR 2020
Figure 12: Four synthetic images with different features. All images are sampled from N(0, 1). The only
difference is that they are sorted in different diagonal directions by the ascending or descending orders. Image0
and Image1 are sorted in the primary diagonal direction by the ascending order and the descending order,
respectively. Image2 and Image3 are sorted in the secondary diagonal direction by the ascending order and the
descending order, respectively.
A.4 Neural networks with a single hidden layer on a synthetic dataset
In order to further demonstrate that the parameters of DNNs cannot satisfy the i.i.d. assumption,
we construct a neural network, i.e., NN = {x; f; fY } shown in Figure 9, to classify a synthetic
dataset. The synthetic dataset consists of 4,800 32 × 32 grayscale images with 4 classes, and every
synthetic image is sampled from the Gaussian distribution N(0, 1). To make the synthetic image
have certain features that can be learned by neural networks, we sort the synthetic image in the
primary or secondary diagonal directions by the ascending or descending orders. Thus, the synthetic
dataset only has four features, i.e., four classes, and their corresponding images are drawn above.
There are two important reasons why we choose the synthetic dataset. Above all, compared to
benchmark datasets containing too complex features, the synthetic dataset only has four simple
features, thus we can easily show the correlation of different parameters by simply visualizing them.
In addition, since a single synthetic image only has a single feature, we can easily examine the
connection between the parameters of networks and the synthetic image.
NeuronO	Neuronl	Neuron2	Neuron3
Neuron4
Figure 13: Eight neurons in the fully connected hidden layer. Their dimension is 32 × 32
Since the synthetic dataset is very simple, we set the hidden layer f have only eights neurons and
the output layer fY have four output nodes. After training the network well, we show all the eight
neurons in Figure 13. In contrast to the i.i.d. assumption, their weights show strong correlation in
three aspects. First, weights show strong internal correlation within each neuron, i.e., if a weight has
large magnitude, its neighbors have high probability to be large, and vice versa. Second, weights
show strong external correlation between different neurons. For example, most weights of Neuron1
and Neuron5 at the same position show strong positive correlation, and most weights of Neuron3
and Neuron7 at the same position show strong negative correlation. Third, the weights show strong
dependence on the training dataset. More specifically, since all the features of the synthetic dataset
concentrate in the primary and secondary diagonal directions, most linear filters also demonstrate
related features in the same directions.
17
Under review as a conference paper at ICLR 2020
FiIterO
Filterl
Filter2
Filter3
Figure 14: The left figure visualizes the four linear filters in the output layer. The right figure visualizes the
correlation matrix of the four filters in the output layer.
Figure 14 visualizes the four linear filters in the output layer. Though they not show too strong
internal correlation because their dimension is too small, their weights at the same position still
show strong external correlation, e.g., the weights of Filter0 and Filter1 at the same position show
strong negative correlation. Moreover, we calculate the correlation coefficients of all the four filters
based on Equation 2 and derive the correlation matrix show in Figure 14. It shows that the correlation
coefficient of Filter0 and Filter1 is close to -1, which further validates that they have strong negative
correlation, hence the parameters of the output layer do not satisfy the i.i.d. prior.
Compared to the simulations using benchmark datasets, the synthetic dataset enable us to expose
the correlation of different weights and their dependence on the training dataset in the more clearly
way. Overall, we demonstrate that the parameters of neural networks are not i.i.d..
B Proof: The necessary condition for the activations of hidden
LAYERS BEING INDEPENDENT
%1
Xm
XM
/12
/22
i2K
fγ(1)
fγ(L)
尤	fl	f2	fγ
Figure 15: A MLP = {x; f1; f2; fY }, in which f1 has N neurons, i.e., f1 = {f1n = σ1[g1n(x)]}nN=1,
where g1n(x) = PM=I αmn ∙ xm + b1n is the nth linear filter, amn is the weight of the edge between xm
and f1n, b1n is the bias, and σ1 is a non-linear activation function. Similarly, f2 = {σ2 [g2k(f1)]}kK=1 where
g2k(fι) = Pn=I βnk ∙ fin + b2k. In addition, since fγ is Softmax, fγ = {Z^exp[gyi(f2)]}L=ι where
gyi(f2) = PK=I Yki ∙ f2k + byi, and the partition function is ZY = PL=I exp[gyi(f2)].
In the context of Bayesian probability, all the weights and the biases in the MLP (Figure 15) have
prior distributions, i.e., αmn 〜P(Amn), βnk 〜P(Wnk), bin 〜P(Bin), and b2k 〜PBk), thus
we regard Gin = Pm=I AmnXm +Bin as the random variable of gin (x) = Pm=I αmn ∙Xm+bin,
and G2k = Pn=I WnkFin + B2k as the random variable of g2k(f1) = Pn=I βnk ∙ fin + b2k.
Therefore, the random variables of arbitrary activations in f1 and f2 , e.g., fin and f2k , can be
expressed as Fin = σi (Gin) and F2k = σ2 (G2k), respectively.
18
Under review as a conference paper at ICLR 2020
Based on the theorem (Appendix D) that functions of independent random variables are independent,
We can derive that if {F2k}3ι are independent and the activation function σι(∙) is invertible, then
{G2k}kK=1 are independent. In other words, in order to prove {F2k}kK=1 are independent, we only
need to prove {G2k}K=1 being independent as long as σ2(∙) is invertible.
The necessary condition for {G2k}kK=1 being independent is ∀(k, k0) ∈ S1 = {(k, k0) ∈ Z2 |k 6=
k0, 1 ≤ k ≤ K, 1 ≤ k0 ≤ K}, the covariance Cov(G2k, G2k0) = 0, Which can be formulated as
NN
Cov(G2k, G2k0) = Cov(X WnkF1n + B2k, X Wn0k0 F1n0 + B2k0) = 0	(23)
n=1	n0 =1
Based on Cov(X + Y, W + Z) = Cov(X,W) + Cov(X, Z) + Cov(Y,W) + Cov(Y,Z),Wecan
derive that
NN	N
Cov(G2k,G2k0)=XX
Cov(WnkF1n, Wn0 k0 F1n0 ) + X Cov(WnkF1n, B2k0 )
n=1 n0=1	n=1
N
+ X Cov(Wn0k0F1n0 , B2k) + Cov(B2k, B2k0 )
n0=1
Based on the definition of covariance, i.e., Cov(A, B) = E(AB) - E(A)E(B), We have
Cov(Wnk Fin, Wn0k0 FInO)= E(Wnk Wn0k0 ∙ FInFInO)- E(Wnk FIn)E(Wn0k0 FInO)
Cov(WnkF1n, B2k0) = E(WnkB2k0F1n) - E(WnkF1n)E(B2k0)
(24)
(25)
Based on the laW of total expectation, i.e, E(AB) = EA(AEB|A(B)), We have
E(Wnk Wn0k0 ∙ FInFInO)= EWnkWnOkO (Wnk Wn0k0 ∙ EFinFino ∣Wnk Wn,小 (FInFInO))
E (Wnk B2kO FIn)= EWnkB^ (Wnk B2kO ∙ EFin∣Wnk B?" (FIn))	(26)
E(Wnk Fin)= EWnk (Wnk ∙石为九卬^ (FIn))
Since the statistical properties of the input {F1n}nN=1 are unrelated to the Weights βnk and biases b2k,
We have EFinFinO|WnkWnOkO (FinFinO) = EFinFinO (FinFinO), EFin|WnkB2kO (Fin) = EFin (Fin),
and EFin|Wnk(Fin) = EFin(Fin). As a result, We can derive that
E(WnkWnOkO ∙ FInFInO) = EWnk WnOkO (Wnk WnOkO )EFinFinO (FInFInO)
E(WnkB2kO Fin) = EWnkB2kO (Wnk B2kO)EFin (Fin)	(27)
E(WnkFin) = EWnk (Wnk)EFin(Fin)
Substituting Equation (27) for the corresponding expectations in Equation (25), We have
Cov(WnkFin, WnO kO FinO ) = E(WnkWnOkO)E(FinFinO) - E(Wnk)E(WnOkO)E(Fin)E(FinO)
Cov(WnkFin, B2kO ) = E(WnkB2kO)E(Fin) - E(Wnk)E(B2kO)E(Fin)
(28)
Assuming {Fin}N=ι are i.i.d. and Fin 〜P(Fi), We have E(FInFin，) = E(FIn)E(Fin，)
E2(Fi). Hence, We can derive that
Cov(WnkFin, WnOkO FinO) = Cov(Wnk, WnOkO)E2 (Fi)
Cov(WnkFin, B2kO) = Cov(Wnk, B2kO)E(Fi)
Substituting Equation (29) for the corresponding covariances in Equation (24), We have
NN	N
Cov(G2k,G2kO)=XX
Cov(Wnk, WnOkO)E2(Fi) + X Cov(Wnk, B2kO)E(Fi)
n=i nO=i	n=i
N
+ X Cov(WnOkO , B2k)E(Fi) + C ov(B2k, B2kO)
nO =i
(29)
(30)
19
Under review as a conference paper at ICLR 2020
Based on Cov(X + Y, W +Z) = Cov(X+W) +Cov(X+Z) +Cov(Y +W) +Cov(Y +Z),
we can derive that
NN	N
Cov(G2k, G2k0) = Cov(X Wnk, X Wn0k0)E2(F1) + Cov(X Wnk, B2k0)E(F1)
n=1	n0=1	n=1
N
+Cov(X Wn0k0,B2k)E(F1) +Cov(B2k,B2k0)
n0=1
(31)
Therefore, given σ2(∙) is invertible and {F1n}N=1 are i.i.d., the necessary condition for {F2k}K=1
being independent can be expressed as
Cov(G2k,G2k0) = Cov(Wk, Wk0)E2(F1) + Cov(Wk, B2k0)E(F1)
+ Cov(Wk0, B2k)E(F1) + Cov(B2k, B2k0) =0
(32)
where Wk = PnN=1 Wnk and Wk0 = PnN=1 Wnk0 .
Overall, in order to guarantee the activations of a hidden layer are independent with each other in
the context of Bayesian probability, the prior distributions of the weights and biases of the hidden
layer should satisfy the above necessary condition given the inputs of the hidden layer are i.i.d. and
the activation functions are invertible.
C Proof: The neces sary condition for the activations of hidden
LAYERS BEING IDENTICALLY DISTRIBUTED
Given the same DNN drawn in Figure 15, We assume σ2(∙) being strictly increasing and differ-
entiable, so σ2(∙) is invertible and its inverse σ-1(∙) is also strictly increasing. The cumulative
distribution function of F2k can be expressed as
ΦF2k(f)=φ(F2k ≤f)
= φ(σ1(G2k) ≤ f)
= φ(G2k ≤ σ1-1(f))	(33)
= ΦG2k (σ1-1(f))
where φ(F2k ≤ f) is the probability of F2k takes on a value less than or equal to f . Subsequently,
we can obtain that
P (f)	∂ ΦF2k (f)	∂ΦG2k (σ-1(f))
F2k f)	∂f	∂f
_ P	("-1(fYtdσ-1(f)
=pG2k (σ2 (f))	∂f
(34)
Equation (34) means that if the activation function σ2(∙) is strictly increasing and differentiable,
{F2k}kK=1 being identically distributed implies {G2k}kK=1 being identically distributed.
In the context of Bayesian probability, G2k = PnN=1 WnkF1n + B2k, we can derive that
N
E(G2k) = XE(WnkF1n)+E(B2k)	(35)
n=1
Based on the law of total expectation and Equation 27, we have
E(WnkF1n) = EWnk (Wnk)EF1n (F1n)	(36)
Assuming {F1in}N=ι are i.i.d. and Fin 〜 Fι, we have E(FIn) = E(Fι). Hence, we can derive that
N
E(G2k) =E(F1)XE(Wnk)+E(B2k)
n=1
(37)
20
Under review as a conference paper at ICLR 2020
Based on the property of expectation, i.e., E(A) + E(B) = E(A + B), we have
N
E(G2k) = E(F1)E(X Wnk) + E(B2k)	(38)
n=1
Therefore, given σ2(∙) is strictly increasing and differentiable and {F‰}N=1 are i.i.d., the necessary
condition for {F2k}kK=1 being identically distributed is that ∀(n, n0) ∈ S1, we have
E(F1)E(Wk) + E(B2k) =E(F1)E(Wk0)+E(B2k0)	(39)
where Wk = PnN=1 Wnk and Wk0 = PnN=1 Wnk0 .
Overall, in order to guarantee the activations of a hidden layer are identically distributed, the prior
distributions of the weights and the biases of the hidden layer should satisfy the above necessary
condition in the context of Bayesian probability given the inputs of the hidden layer are i.i.d. and
the activation functions are strictly increasing and differentiable.
D	Theorem: Functions of independent random variables are
INDEPENDENT
Let X and Y be independent random variables on a probability space (Ω, F, P). Let g and h be
real-valued functions defined on the codomains of X and Y, respectively. Then g(X) and h(Y)
are independent random variables.
Proof: Let A ⊆ R and B ⊆ R be the range of g and h, the joint distribution between g(X)
and h(Y) can be formulate as P(g(X) ∈ A, h(Y) ∈ B). Let g-1(A) and h-1(B) denote the
preimages of A and B, respectively, we have
P(g(X) ∈ A, h(Y) ∈B)=P(X∈g-1(A),Y∈h-1(B))
Based on the definition of independence, we can derive that
P(g(X) ∈ A, h(Y) ∈ B) =P(X ∈ g-1(A))P(Y ∈ h-1(B))
= P(g(X) ∈ A)P(h(Y) ∈ B)
Based on the definition of preimage, we can derive that
P(g(X) ∈ A, h(Y) ∈ B) = P(g(X) ∈ A)P(h(Y) ∈ B)
Therefore, g(X) and h(Y) are independent random variables.
E The necessary conditions for activations being i.i.d. are valid
FOR CONVOLUTIONAL LAYERS
Assuming a DNN = {x; f1; ...; fI; fY } has two adjacent convolutional layers fi and fi+1. The
first convolutional layer has N channels, i.e. fi = {fin}nN=1, and the second convolutional layer has
K channels, i.e. fi+1 = {fik+1}kK=1. Therefore, the dimension of the convolutional filters Si+1 in
fi+1 are W × H × N × K, i.e., the dimension ofa single filter connecting the fin channel and the
fik+1 channel are D(Si(+k,1n)) = W × H. As a result, the fik+1 channel can be formulated as
N
珞1 = σi+ι(X S(+,n) ◦ fin + bk+1 ∙ 1)	(43)
n=1
where σi+1 is the activation function and bi+1 is the bias for the (i + 1)th channel.
(40)
(41)
(42)
21
Under review as a conference paper at ICLR 2020
Output
Convolutional filters Input
+
+
b
y1	= A	∙	x1	+	B ∙ x2	+ C ∙	x4	+ D	∙ x5
y2	= A	∙	x2	+	B ∙ x3	+ C ∙	x5	+ D	∙ x6
y3	= A	∙	x4	+	B ∙ x5	+ C ∙	x7	+ D	∙ xq
y4	= A	∙	x5	+	B ∙ x6	+ C ∙	X8	+ D	∙ x9
b is the bias
A	B		C	D					E	F		G	H				
	A	B		C	D					E	F		G	H			
			A	B		C	D					E	F		G	H	
				A	B		C	D					E	F		G	H
Blank elements in the matrix W are zeros
Xs
X9
&
22
ZI
+ b =	^2
Z3
^∖
2
x
备
Figure 16: The equivalence between a convolution channel and a fully connected layer.
Since Garriga-Alonso et al. (2018) demonstrate the equivalence between a convolution operation and
the matrix multiplication, we can regard the convolutional channel fik+1 as a fully connected layer.
Specifically, the input of the virtual fully connected layer is the same as the input of the convolutional
channel, the output of the virtual fully connected layer is the vectorized output of the convolutional
channel, and the weights of the virtual fully connected layer depend on the convolutional filter and
the spatial location of the output of the convolutional channel.
Without considering the activation function, the equivalence between a convolutional channel and a
fully connected layer can be explained in Figure 16, in which the first convolutional layer has two
channels, i.e., f = {x; X}, where X = {xι,…，x9} and X = {Xι,…，X9}, and the kth channel
of the second convolutional layer is fik+1 = {z1, z2, z3, z4}. In addition, the dimension of the
convolutional filter Si+1 is 2 × 2 × 2 × 1, where Si(+k,11) = [A, B; C, D] and Si(+k,12) = [E, F; G, H].
Figure 16 shows that the output of fik+1 can be expressed as a fully connected layer. The input of
the fully connected layer is the vector X, the weights of each neuron is represented by each row of
the matrix W, and the bias is b, thus f+ = W ∙ x + b ∙ 1.
If we consider the activation function and make a precise statement, fik+1 can be formulated as
珞1= σi+1(W⅛1∙ fi + bk+1 ∙ 1)	(44)
where Wik+1 is the matrix corresponding to all the convolutional filters in the kth channel.
To derive the necessary conditions for two arbitrary different channels are i.i.d., we need to derive
the necessary conditions for two arbitrary activations in two arbitrary different channels are i.i.d. For
example, if the mth activation of the k channel and the m0th activation of the k0 channel in fi+1 are
i.i.d., the necessary conditions are Cov(fik+1(m), fik+0 1(m0)) = 0, E(fik+1(m)) = E(fik+0 1(m0)).
Since we demonstrate the equivalence between a convolutional channel and a fully connected layer,
the necessary conditions for activations being i.i.d. in a fully connected layer are also valid for a
convolutional layer. In other words, we can also use the Equation 3, Equation 4, and Equation 5 to
examine whether activations being i.i.d. is valid in a convolutional layer under the assumption that
all activations being i.i.d. in the previous layer.
22
Under review as a conference paper at ICLR 2020
Table 6: The statistical measures for examining the necessary conditions
Layer	Activation expectation	Covariance	Weight expectation
f1	E(X)	Cov(An, An0)	E(An)
f2	E(F1)	Cov(Wk,Wk0)	E(Wk)
fY	E(F2)	Cov(CI,Cl)	E(Cl)
{Xm}M=1 〜P (X), {fln}N=1 〜P (Fl), {f2k}K=1 ~ P ®
An = ^Pm = ι Amn, Where amn 〜P(Amn)
Wk = Pn=I Wnk, where βnk 〜P(Wnk)
Cl = PK=I Cki, where γki - P(Ckl)
F Activations being i.i.d. cannot be valid for all the hidden
layers of DNNs
In this section, we demonstrate that activations being i.i.d. cannot be valid for all the hidden layers
of DNNs through showing that the hidden layers of two typical DNNs, i.e, the MLP and the CNN,
cannot satisfy the necessary conditions for activations being i.i.d..
F.1 The MLP on the MNIST dataset
The architecture of the MLP for classifying the MNIST dataset is visualized in Figure 15, and its
specific parameters are summarized in Table 3. Since the activation functions of all the hidden layers
in the MLP are the sigmoid function, which is strictly increasing, differentiable, and invertible, we
only need to examine the first two necessary conditions in the MLP.
Since we restrict all the hidden layers from using the bias, the first two necessary conditions for
activations being i.i.d. can be simplified as
∀(k, k0) ∈ S1 = {(k, k0) ∈ Z2|k 6= k0, 1 ≤ k ≤ K, 1 ≤ k0 ≤ K}, we must have
Cov(G2k,G2k0) =Cov(Wk,Wk0)E2(F1) =0	(4)
E(F1)E(Wk) = E(F1)E(Wk0)
(46)
Based on the architecture of the MLP, Table 6 summarizes all the statistical measures for each layer
of the MLP. Therefore, the key to examine if the layers of the MLP satisfy the necessary conditions
is to estimate the statistical measures.
To estimate the expectations E(X), E(Fι), and E(F2), We use the sample means x, fι, and f2,
respectively. The definition of the sample mean is
MN	K
x = MM X xm, f = N Xf f2(n), f = K X f2(k)	(47)
After the MLP is well-trained, we derive the sample means as x = 0.131, fι = 0.436, and f2
0.498 given the benchmark MNIST testing dataset.
Taking into account the estimation error, we can regard E(X) = 0. As a result, we can derive
that ∀(n, n0), Cov(G1n, G1n0 ) = 0 and E(G1n) = E(G1n0) = 0, thus f1 satisfy the necessary
conditions for activations being i.i.d..
However, f 1 = 0.436 and f2 = 0.498 imply that E(Fi) = 0 and E(F2) = 0 even considering the
estimation error. Therefore, we need to estimate Cov(Wk, Wk0 ) and E(Wk) to examine whether
the activations {f2k}kK=1 being i.i.d..
Since the training dataset is commonly viewed as i.i.d., we can use the same training dataset to
train the MLP several times to derive multiple independent observation samples of {βnk(t)}tT=1 to
estimate the random variable Wnk, thereby estimating Cov(Wk, Wk0) and E(Wk).
23
Under review as a conference paper at ICLR 2020
Figure 17: The expectation of the weights summation, i.e., E(An), E(Wk), and E(Cl), in each layer. The
number at the top of the figure indicates the variance of the points drawn in the figure.
4-
2-
O-
-2-
0	2	4	6	8
Zy (4.06)
I
More specifically, We use the sample correlation r(βk, Qk，)to check whether Cov(Wk, Wk，)are
close to zero and use the sample mean βk to estimate E(Wk).
r(βk, βk" =	/ PJ⑴-彳)(wk，⑴-丽—2 ≈ Corr(Wk,Wk，)
√∑T=1(βk ⑴-而 ∑T=1(βk0 ⑴-而)
(48)
where Jk = 1 P=I βk(t) is the sample mean of {βk(t)}T=1 and βk(t) = Pn=I βnk (t).
In order to make a tradeoff between the estimation precision and computation complexity, we use
the same training dataset to train the MLP 20 times to derive the samples, i.e., {βnk(t)}t2=0 1. For an
activation f2k, it has 128 weights because its previous layer has 128 neurons. Hence, a single sample
βk (t) = P1n2=81 βnk (t). Based on the 20 samples, i.e.,{βk(t)}t2=01,we can calculate r(βk, Qk，) to
examine whether Cov(Wk, Wk，) are close to zero and derive the sample mean to estimate E(Wk).
We use the same method to estimate Cov(An , An，), E(An ), Cov(Cl, Cl，), and E(Cl).
Finally, we shows the absolute correlation matrices i.e., AN ×N , WK×K, and CL×L, in Figure 3.
Since many elements in the three matrices are far from zero, we conclude that their corresponding
covariances, i.e, Cov(An , An，), Cov(Wk, Wk，), and Cov(Ck, Ck，), are not close to zero based on
connection between correlation and covariance
Cov(Wk,Wk，)
Corr(Wk ,Wk，)=------------
σWk σWk0
(49)
where σWk > 0 is the standard derivation of Wk.
Figure 17 visualizes the expectations, i.e., E(An ), E(Wk), and E(Cl), for each layer in the MLP.
We find that some expectations show great variations, especially {E(An )}nN=1 and {E(Cl)}lL=1.
In other words, the MLP cannot guarantee that all the expectations of the weights summation are
equivalent even though we take into account the estimation error. Therefore, the MLP cannot satisfy
the necessary condition for activations being identically distributed.
Overall, we conclude that not all the hidden layers of the MLP can satisfy the necessary conditions
for activations being i.i.d. under the assumption that the activations of the previous layer are i.i.d.
F.2 The CNN on the CIFAR- 1 0 dataset
The architecture of the CNN for classifying the CIFAR-10 dataset is in Table 5. In particular, the
two convolutional layers do not have activations function, i.e., their activation functions can be
viewed as y = x, which is strictly increasing, differentiable, and invertible. The sigmoid activation
function is also strictly increasing, differentiable, and invertible, thus we only need to examine
the first two necessary conditions in the CNN. To simplify derivation and decrease computation
complexity, we assume the max-pooling layers are special activation functions and satisfy the third
necessary condition for activations being i.i.d.
24
Under review as a conference paper at ICLR 2020
Table 7: The statistical measures for examining the necessary conditions of each layer in the CNN
	f1	f2	f3	f4	fY
Description	Conv Maxpool	Conv Maxpool	FC Sigmoid	FC Sigmoid	Output Softmax
Activation Dim.	16×16×64	8×8× 128	1024×1	256×1	10×1
Filter Dim.	3×3×3×64	3×3×64× 128	8192×1024	1024×256	256×10
Weight Dim.	M×N (27×64)	N XK (576×128)	K×L (8192×1024)	L×Q (1024×256)	Q×R (256×10)
Weight PDF	amn~P (Amn)	bnk 〜P(Bnk )	Ckl 〜P (Ckl)	dlq 〜P(Dlq )	fqr 〜P (Fqr)
Sum. PDF	An =PmM=1 Amn	Bk = Pn=I Bnk	Cl =PkK=1 Ckl	Dq =PlL=1 Dlq	Fr =PqQ=1 Fqr
Covariance	Cov(An,An0)	COv(Bk ,Bk0 )	COv(Cl,Cl0)	COv(Dq,Dq0)	COv(Fr,Fr0)
Sum. Exp.	E(An)	E(Bk )	E(Cl)	E(Dq)	E(Fr)
Activation Exp.	E(X)	E(Fl)	E(F2)	E(F3)	E(F4)
Dim. is short for dimension. Sum. is short for Weights Summation. Exp. is short for Expectation.
Similar as the MLP, we also restrict all the layers of the CNN from using the bias, thus the first two
necessary conditions for activations being i.i.d. can be simplified as
∀(k, k0) ∈ S1 = {(k, k0) ∈ Z2|k 6= k0, 1 ≤ k ≤ K, 1 ≤ k0 ≤ K}, we must have
Cov(G2k,G2k0) =Cov(Wk,Wk0)E2(F1) =0	()
E(F1)E(Wk) =E(F1)E(Wk0)	(51)
Table 7 summarizes all the statistical measures for every layer of the CNN based on the architecture
of the CNN in Table 5. The definitions of most random variables are similar as the MLP except the
dimension of the weights. To check if two arbitrary convolutional channels are i.i.d., we need to
know all the weights of two convolutional channels. Based on the equivalence between a convolu-
tional channel and a fully connected layer (Appendix E), all the weights of a single convolutional
channel consist of all the convolutional filters related to the convolutional channel. For example, the
dimension of all the convolutional filters related to a convolutional channel in f1 are 3 × 3 × 3, thus
a single convolutional channel in f1 has 27 weights, so the weights dimension of f1 is 27 × 64.
After training the CNN well (its training accuracy is 96.2% and testing accuracy is 67.9%), we use
the same methods as the MLP to estimate the above statistical measures in Table 7.
fl (0.29)
^64 × 64
f3 (0.29)
C1024× 1024
∣-rl.0
-0.8
-0.6
[0.4
, 0.2
r-r 1.0
-0.8
-0.6
『0.4
0.2
Figure 18: Five absolute correlation matrices for the weight summations in each layer. The number at the top
is the average of the absolute correlation coefficients for all the weights summations of the layer
25
Under review as a conference paper at ICLR 2020
0.05
0.00
-0.05
-0.10
§
f3 (550.28)
京(13.91)
fγ (1.13)
-12

Figure 19: The expectation of the weights summation of each layer in the CNN. The number at the top of the
figure indicates the variance of the points drawn in the figure..
For the activation expectations of each layer in the CNN, We have X = -0.023, fι = -707.074,
f2 =—6347.836, f3 = 36.870, f4 = 15.131, thus all the expectations cannot be zero except the
input layer, i.e., E(X) = 0, E(F1) 6= 0, E(F2) 6= 0, E(F3) 6= 0, E(F4) 6= 0.
For the covariances, We shoW all the correlation matrices of each layer of the CNN in Figure 18.
We find that many correlation coefficients are far from zero in all the correlation matrices, thus their
corresponding covariances are not close zero either. Based on the estimations for the activation
expectation and the covariance betWeen different Weights summations in each layer, We can derive
that activations being independent cannot be valid for all the layers of the CNN.
For the Weights summation expectation, We visualize all the expectations for each layer in the CNN
in Figure 19. We find that some expectations have great variations, especially {E(Cl)}lL=1 and
{E(Dq)}qQ=1, Which means that most expectations of Weights summations of different neurons in
f3 and f4 cannot be equivalent. As a result, the activations of the tWo fully connected layers in the
CNN cannot be identically distributed.
Overall, We conclude that not all the hidden layers of the CNN can satisfy the necessary conditions
for activations being i.i.d. under the assumption that the activations of the previous layer are i.i.d.
G The equivalence between the Gradient descent algorithm and
THE FIRST ORDER APPROXIMATION
In the context of deep learning, most learning algorithms belong to the gradient descent algorithm.
Given a DNN = {x; f1; ...; fI; fY } and a cost function H[fY , P(Y |X)], Where P(Y |X) is
the true distribution of the training label given the corresponding training dataset. Let θ be the
parameters of the DNN, the gradient descent aims to optimize θ by minimizing H[fY , P(Y |X)]
(RUmeIhart et al., 1986). We typically update θ iteratively to derive θ*, which can be expressed as
θt+ι = θt- aVθtH[fY,P(Y|X)]	(52)
where Vθt H [fγ ,P (Y |X)] is the Jacobian matrix of H [fγ ,P(Y∣X)] with respect to θt at the tth
iteration, and α> 0 denotes a constant indicating the learning rate. Since P(Y|X) is constant, we
denote H(fY) as H[fY, P(Y|X)] for simplifying the following derivation.
Since the functions of all hidden layers are differentiable and the output of a hidden layer is the input
of its next layer, the Jacobian matrix ofH(fY) with respect to the parameters of the ith hidden layer,
i.e., Vθ(i)H(fY), can be expressed as follows based on the chain rule.
I
Vθ(i)H(fγ) = VfγH(fγ) ∙Vfιfγ∙ Y Vfj-Ifj∙Vθ(fi	(53)
j=i+1
where θ(i) denote the parameters of the ith hidden layer.
Based on Equation 52 and Equation 53, θ(i) can be learned by the gradient descent method as
θt+1(i) =θt(i)-α[Vθt(i)H(fY)]	(54)
Table 8 summarizes the backpropagation training procedure for the MLP in Figure 15.
26
Under review as a conference paper at ICLR 2020
Table 8: One iteration of the backpropagation training procedure for the MLP
Layer	Gradients update Vθt(i)H(fγ)	Parameters and activations update		
fY	vfγ H(fY )vθt (Y )fY	J	θt+1 (Y)=θt+1 (Y )-α[Vθt(Y)H(fY)], fY (f2,θt+1 (Y ))	↑
f2	Vfγ H(fY )Vf2 fγ Vθt(2)f2	J	θt+1(2)=θt+1(2)-α[Vθt(2)H(fY)], f2(f1,θt+1(2))	↑
f1	Vfγ H(fγ )Vf2 fγ Vfi f2Vθt(1)f1	J	θt+1(1)=θt+1(1)-α[Vθt(1)H(fY)], f1(x,θt+1(1))	↑
x	—		—	
The uparrow and downarrow indicate the order of gradients and parameters(activations) update, respectively.
If an arbitrary function f is differentiable at point p* in RN and its differential is represented by the
Jacobian matrix Vpf, the first order approximation of f near the point P can be formulated as
f(p) — f(p*) = (Vp*f) ∙ (p — P*) + o(||p - P*||)	(55)
where o(||p 一 p*||) is a quantity that approaches zero much faster than ||p 一 p*|| approaches zero.
Based on the equivalence between the gradient descent method and the first order approximation
(Battiti, 1992), updating the activations of the hidden layers of the MLP in Figure 15 during the
backpropagation training procedure can be approximated as
f2[f1, Θt+1 (2)] ≈ f2[f1, θt(2)] + (Vθt(2)f2) ∙ [θt+1(2) - θt(2)]
fι[x, θt+ι(1)] ≈ fι[x, θt(1)] + (Vθt(i)fι) ∙ [θt+ι⑴一 θt(1)]	()
where f2 [f1, θt(2)] denote the activations of the second hidden layer based on the parameters
learned in the tth iteration, i.e., θt(2), given the activations of the first hidden layer, i.e., f1. The
definitions off2[f1, θt+1(2)] and f1(x,θt(1)) are the same as f2[f1,θt(2)].
Because f2 has K neurons, i.e., f2 = {f2k = σ2(PN=I βnk ∙ fin + b2k)}3i, and each neuron
has N + 1 parameters, i.e., θ(2) = {[βik;…；βNk; b2k]}K=1, the dimension of Vθt(2)f2 is equal
to K × (N + 1) and Vθt(2)f2 can be expressed as
V%(2)f2 = (Vσ2f2) ∙ [fι; 1]T	(57)
where V,? f2 = f2%'："" ∙
Substituting (Vσ2f2) ∙ [fi； 1]T for Vθt(2)f2 in Equation 56, We can derive
f2[f1, θt+i(2)] ≈ f2[f1, θt(2)] + (V,?f2) ∙ [fi；1]T ∙ θt+ι(2) -(V,2f2) ∙ [fi； 1]T ∙ θt(2)
(58)
If we only consider a single neuron, such as f2k, in f2, θt+ι(2k) = [βik; •…;βNk b2k] and
θt(2k) = [β*k；…；βNk； b*k], thus [fi； 1]T ∙ θt+ι(2k) = PN=I βnk ∙ fin + b2k. As a result,
Equation 58 with respect to a single neuron can be expressed as
N
f2k[fi, θt+i(2k)] ≈ (Vσ2f2k) ∙ [£ βnk ∙ fin + b2k] (First order approximation)
N
+ f2k[fi, θt(2k)] 一(Vσ2f2k) ∙ [X β*k ∙ fin + b*k] (Error)
n=i
(59)
Equation 59 indicates that f2k[fi, θt+i(2k)] can be expressed as its first order approximation with
an error component based on the activation in the previous iteration, i.e., f2k [fi, θt(2k)]. Since
V,2f2k = df2kfσ"2)] is only related to fi and the parameters in the tth training iteration, i.e,
θt(2), it can be regarded as a constant. Also note the error component not contains any parameters
in the (t + 1)th training iteration. In summary, f2k(fi, θt+i(2k)) can be reformulated as
N
f2k (fi, θt+i(2k)) ≈ Ci ∙ [X βnk ∙ fin + b2k ]+。2	(60)
where Ci = V,? f2k and C 2 = f2k (fi, θt(2k)) 一(V,? f2k) ∙ [PN=i β*k ∙ fin + b*k ]. Similarly, the
activations in the first hidden layer, i.e, fi, also can be formulated as the first order approximation
in the context of the gradient descent learning algorithm.
27
Under review as a conference paper at ICLR 2020
I Conv.+ReLU+Pool. ∣ Conv.+ReLU+Pool. ∣ FIattening I OUtpUt I
Figure 20: The above CNN has two convolutional layers, i.e., f1, f2, and one softmax output layer, i.e., fY .
We flatten the output of f2 as the input of fY .

f2j
fγ(1)
fγ(^)
H The proposed probabilistic representation holds for
CONVOLUTIONAL LAYERS
This section proves that the proposed probabilistic representation is valid for convolutional layers.
In the above CNN, the input x has Q channels x = {xq}qQ=1 (e.g., Q = 3 ifx are color images), f1
has N convolutional channels f1 = {f1n}nN=1, f2 has K convolutional channels f2 = {f2k}kK=1.
The output layer fY is the softmax with L nodes, thus its distribution can be formulated as
P(FY) = {Pfyl) = ɪ eXPfyl)}L=I
ZFY
(61)
where ZFY = PlL=1 exp(fyl) denotes the partition function. Since{fyl = PJ=1 Yji ∙ f2j+ byi}L=ι,
we can derive
1J
P(FY) ={Pfyl) = ZFY exp(∑ γjl ∙ f2j + byl)}L=1
(62)
where {f2j }jJ=1 is the flattened output of f2, γjl is the weight of the edge between f2j and fyl,
and byl denotes the bias. Since f2 is a convolutional layer with K convolutional channels, it can be
reformulated as {f2j}jJ=1 = {f2k}kK=1. As a result, P(FY ) can be reformulated as
1K
P(FY) = {P (fyl) = Z— exp(E YTl ∙ f2 + byl)}L=1
ZFY	k=1
(63)
where Ykl is a subset of all the parameters {γjι}J=ι such that PK=I YT ∙ fk = Pj=I Yjl ∙ f2j, and
γkTl is the transpose of γkl. Therefore, P(FY ) can be reformulated as
1K
P(FY) = {P(fyl) = Z- ∏ exp(γT ∙ fk)}L=ι
FY k=1
(64)
where ZF0
ZFY /[exp(byl)]. Recall the element-wise matrix power, e.g., exp(a)b, where
a = [a1; a2; a3] and b = [b1; b2; b3], we can derive that eXp(a) = [eXp(a1); eXp(a2); eXp(a3)]
and eXp(a)b = [eXp(a1)b1 ; eXp(a2)b2; eXp(a3)b3] = [eXp(a1b1); eXp(a2b2); eXp(a3b3)]. As a result,
eXp(ab) = eXp(a1b1 + a2b2 + a3b3) = Q|a| eXp(a)b, where |a| is the element numbers of a.
Based on the element-wise matriX power, we can reformulate P(FY) as
1 K	1 K
P(FY) = {P(fyl) ≈ Ir ∏ exp(γT ∙ fk) = Z^n ∏[exp(fk)]γkl }=	(65)
FY k=1	FY k=1 |fk|
28
Under review as a conference paper at ICLR 2020
Moreover, we introduce a new partition function ZFk = Pfk exp(f2k) to guarantee that
Z1~exp(fk) is a probability
ZF2k
measure. As a result, P (FY ) can be reformulated as
P(FY)=3" Zb ∏∏[ ZFk exp(fk )]γkl }=
(66)
where ZFY = ZFγ/ QK=JZFk]γklTfkl. Overall, P(FY) can be reformulated as a PoE model, in
which each expert is defined by every convolutional channel in f2
P(fk) = ɪ exp(fk)	(67)
ZF2k
Since f2 has K convolutional channels, i.e., f2 = {f2k}kK=1, and each channel can be formulated
as the summation of all the convolutional channels in f1, i.e.,
N
f2 = {fk = σ2(χ s2k,n) ◦ fn+bk ∙i)}k=ι	◎)
n=1
where S2(k,n) ◦ f1n is the output of the kth convolutional filter applying into the nth channel of f1,
bk is the bias, and σ2(∙) is the activation function. Therefore, We have
1N
p(fk) = Z- expg(£ S 尸◦ fn + bk ∙ 1)]
ZF2k	n=1
(69)
Based on the equivalence between the gradient descent learning and the first order approximation
(Appendix G), Equation 69 can be approximated as
1N
P(fk) ≈ kexp(∑ S(k,n) ◦ fn + bk ∙ 1)	(70)
ZF2k	n=1
Let us define a linear function as 夕k (fι) = PnN=I S(k,n) ◦ fn + bkk ∙ 1, thus the distribution of the
kth convolutional channel in f2, i.e., P (f2k ), can be expressed as
P(H) ≈	exp[^k (fi)]	(71)
ZF2k
If we regard all the linear filters {夕k(fι)}3ι as potential functions for modeling signal structures
of fi, we can formulate P(F2) as a specific Gibbs distribution, i.e., the Markov Random Fields
(MRFs), which can be expressed as
1K	1 K
P(F2) = Z-	P(f2) = Z-exp[∑	(fi)]
F2 k=1	F2	k=1
(72)
where ZF? = Pfl exp [P K= 1 夕 k (fi)] is the partition function for P (F2). In summary, we prove
that the convolutional layer f2 formulates a MRF model to describe the statistical property of fi.
Here we prove that the convolutional layer fi also formulates a MRF model to describe the statistical
property of the input x. Based on Equation 70, we have
1N
P(fk) ≈ Z-exp(X S(k,n) ◦ fn)
ZF2k	n=1
(73)
where ZF0 k = ZF k /exp(b2k).
29
Under review as a conference paper at ICLR 2020
Convolutional filter s2"n^
A B
A convolution operation
C D
Input channel K
A matrix multiplication
w2^>n)
D
C
B
A
Blank elements are zeros
O
D
Output channel /2
三
些
二
%
y1 = A	- x1	+ B	- x2	+	C	- x4 +	D	-	xs
y2 = A	■ %2	+ B	- x3	+	C	- xs +	D	-	x6
y3 = A	- x4	+ B	- xs	+	C	- x7 +	D	-	xs
y4 = A	- x5	+ B	- x6	+	C	- x3 +	D	-	x9
Figure 21: The equivalence between a convolutional operation and a full connected layer.
Based on the equivalence between a convolutional channel and a fully connected layer proven in
Appendix E, we can regard a convolutional operation as a matrix multiplication, which is visualized
in Figure 21. As a result, we have
1N	1N
P(f2) ≈ Z- exp(X S(f'n) o fn) = Z- exp(X W^n ∙ f f)
ZF2	n=1	ZF2	n=1
where W>(f,n) is a matrix corresponding to the convolution filter S(f,n).
Based on the element-wise matrix power, P (fk) can be reformulated as
NN
P(f2) ≈ Z- ∏ exp(W,fa ∙ fn) = Z- ∏ exp(fn)Wy,n)
ZFk n=1	ZFk n=1
(74)
(75)
Similarly, we introduce
a new partition function ZFn = Efn exp(fn) to guarantee that
1r— exp(fn) is a probability measure. As a result, P(fk) can be reformulated as
ZFn
N
P(Jf )=PM ≈ ZJ ∏1[ 有exp(f n 心
(76)
where ZFfe = ZFfe/ ∏N=ι [Zfn]W2k,n). Therefore, P(及)can be reformulated as a PoE model,
in which all the experts are defined as P (fn)=之n exp(ff)
Since fι has N convolutional channels
Q
fι = {fn = MX S (n'q) o Xq + bn ∙ 1)}N=1
q=ι
(77)
Based on the same derivation as Equations 70, 71,72, P(FI) also can be expressed as a MRF model
P(FI)
1 N	I N
短∏ P(W =福exp[X成⑺]
1 n=1	1	n=1
(78)
% %
A
B
A
C
B
D
A
C
B
D
C
where 夕?(x) = PQ=I S(n,q) oxq+好∙ 1) and the partition function ZF1 = Px exp[PNL1 夕;(x)].
Overall, the distribution of a convolutional layer can be formulated as a MRF model.
30
Under review as a conference paper at ICLR 2020
Algorithm 1 The algorithm for generating the synthetic dataset
input: NIST dataset of handwritten digits by class
1:	repeat
2:	binarizing an image of NIST to obtain z
3:	extracting the central part of z to obtain zc with dimension 64	×	64
4:	downsampling zc to obtain zcd with dimension 32 × 32
5:	extracting the edge of zcd to obtain the mask image mcd
6:	decomposing mcd into four parts, i.e., moutside, moutside-boundary,	minside-boundary, and minside.
7:	sampling N(0, 1024) to derive a random vector x ∈ R1024×1
8:	sorting X in the descending order to derive X
9：	decomposing X into four parts, i.e., X = {XoUtSide, XoUtSide-boundary, XinSide-boundary, XinSide}
10：	placing each pixel of X = {Xoutside, XoUtSide-boundary, Xinside-boundary,	XinSide} into	the	above masks.
11: until (20,000 Synthetic imageS are generated)
output: The synthetic dataset
I The Approach to generate the synthetic dataset based on NIST
and the architecture of the CNN
In this section, we present a novel approach to generate a synthetic dataset obeying the Gaussian
distribution based on the NIST 1 dataset of handwritten digits by class. The synthetic dataset has
similar characteristics as the benchmark MNIST dataset. It consists of 20,000 32 × 32 grayscale
images in 10 classes (digits from 0 to 9), and each class has 1,000 training and 1,000 testing images.
More specifically, the approach to generate the synthetic dataset is summarized in Algorithm 1, and
Figure 22 ViSUaIize the SPatiaI relation of X = {χoutside, xoutside-boundary, xinside-boundary, xinside}.
Original image
Outside
Image edge
Synthetic image
Outside boundary
Inside boundary
Inside boundary
Inside
Outside
Outside boundary
O

Figure 22: The first row shows an original image, its edge, and the corresponding synthetic image based on
the original one. The second row uses white pixels to show the spatial position of the four mask parts, i.e.,
moutside, moutside-boundary, minside-boundary, and minside. The third row shows the synthetic image corresponding to
each mask part, i.e., X = {xoutside, xoutside-boundary, xinside-boundary, xinside }.
1 https://www.nist.gov/srd/nist- special- database- 19
31
Under review as a conference paper at ICLR 2020
Table 9: The architectures of the CNN for synthetic image classification
R.V. Layer
Description
CNN
xf1f2f3f4fY
XF1 F2 FY
Input
Conv (3 × 3) + ReLU
Maxpool
Conv (5 × 5) + ReLU
Maxpool
Output(softmax)
32 × 32 × 1
30 × 30 × 20
15 × 15 × 20
11 × 11 × 60
5 × 5 × 60
1×1×10
R.V. is the random variable of the hidden layer(s).
The architecture of the CNN for classifying the synthetic dataset is summarized in Table 9.
J The equivalence between SGD and variational inference
J.1 The distribution of the output layer given the input is a Gibbs distribution
In Section 6.2, we demonstrate the entire architecture of a DNN = {x; f1; ...; fI; fY } corresponds
to a joint distribution P(FY ; FI..; F1 |X). As a result, the conditional distribution of fY given x,
i.e., P(FY |X), can be formulated as
P (Fγ∣x ) = ∕∙∙∙∕
P(Fγ; Fi..; Fι∣X)dF∣ …dFι
(79)
Since P(FY; FI..; FJX) = P(FY |F1)∙ ... ∙ P(Fi+ι∣Fi) ∙ ... ∙ P(Fι∣X) and only F2 depends
on F1 , we can derive
P (Fγ |X) = ∕∙∙∙∕
P(Fγ|Fi) ∙... ∙ P(F3∣F2)dFι .…dF2 ∙
P(F2|F1)P(F1|X)dF1 (80)
Since / P(F2∣F1)P(Fι∣X)dF1 = /P(F2; Fι∣X)dF1 = P[F2∣fι(X)], where fι(∙) is the
deterministic function defined by the corresponding hidden layer, P(FY|X) can be simplified as
P(Fγ|X) = /…/P(Fγ|Fi) ∙ ... ∙ P(F3∣F2)P[F2∣f1(X)]dFι …dF2	(81)
Similarly, since only F3 depends on F2, we can obtain
P (Fγ |X) = /…/P (Fγ |Fi ) ∙...∙ P(F3∣f2(f1(X )))dFι …dF3	(82)
If we iteratively apply the integral with respect to F3 until Fi, we can obtain
P (Fγ |X )= P [Fγ ∣fι (…f2 (fι(X ))•••)]	(83)
Since the output layer fY is commonly a fully connected layer. we can formulate P(FY|X) as a
multivariate discrete Gibbs distribution based on Equation 8.
P(FyIX) = y 1. . exp[fγ(fι(…f2(f1(x))…))]	(84)
ZDNN(x)
It is important to note that the energy function Ednn(fγ|x) = -fγ(fɪ(…f2(f1(x))∙∙∙)),
i.e., Ednn(∙) is determined by the entire architecture of DNNs. In addition, since P(FY ∣X) is a
conditional distribution ofFY given X, the partition function ZDNN(x) should integrate all possible
activations of the output layer fY to guarantee P(FY|X) being a valid conditional probability, i.e.,
ZDNN(X) = R exp[fγ (fl (…f2(f1(x))…))]dfγ (fl (…f2(f1(x))…)).
For clarity, here we only consider integral no matter a Gibbs distribution is discrete or continuous,
because PfI…f exp[-EDNN(fγ |x)] is a special case of integral. All functions involving fi should
include parameters θfi in the conditioning set, but we omit for the same reason.
32
Under review as a conference paper at ICLR 2020
J.2 SGD can be explained as a variational inference algorithm
As a dominant paradigm for Bayesian posterior inference P (H |E) 8 P (E|H )P (H), the variational
inference converts the inference problem into an optimization problem, where the prior distribution
P(H) is the probability of arbitrary hypothesis H with respect to the observation E, and P(E|H)
is the likelihood distribution. Specifically, the variational inference posits a family of approximate
distributions Q and aims to find a distribution Q*(H) that minimizes the Kullback-Leibler (KL)
divergence between P(H|E) and Q(H) (Blei et al., 2017).
Q*(H) = argmin KL(P (H ∣E)∣∣Q(H))	(85)
Q∈Q
Notably, recent works demonstrate that SGD performs variational inference during training DNNs
(Mandt et al., 2017; Chaudhari & Soatto, 2018). Following these works, we further confirm the
Bayesian explanation for SGD based on the proposed probabilistic for DNNs.
Given a DNN = {x; f1; ...; fI; fY }, we choose the cross entropy H[P(Y |X), P(FY |X)] as the
loss function, where P(Y |X) is the true posterior distribution determined by the training dataset D
and P(FY |X) is the conditional distribution of fY given x derived from the DNN.
H[P(Y |X), P(FY |X)] = H[P(Y |X)] + KL[P(Y |X) kP(FY|X)]
L
- X P [y(l)|x]log[P (fY (l)|x)]
l=1
(86)
where H[P(Y |X)] is the entropy of P(Y |X) and P (y(l)|x) is the true posterior distribution
given the training data x. For example, if L = 4 and P (y|x) is [0, 1, 0, 0], so P [y(2)|x] = 1
and P [y(3)|x] = 0. P(fY |x) is the learned conditional distribution of fY given the same x, e.g.,
P(fY |x) could be [0.2, 0.7, 0.1, 0].
In the context of deep learning, SGD aims to find the P * (FY |X) such that it can minimize the loss
functionH[P(Y|X),P(FY|X)] given the training dataset D.
P*(FY|X) = arg min H[P(Y|X),P(FY|X)]	(87)
P (FY∣X)∈Q
Given the training dataset D, P(Y |X) is a constant, i.e., H[P(Y |X)] = 0, thus we can obtain
P*(FY|X) = arg min KL[P (Y |X) k P(FY|X)]	(88)
P (Fy |X )∈Q
Comparing the Equation 88 and 85, we can confirm the equivalence between SGD and variational
inference based on the proposed probabilistic explanations for DNNs. More specifically, the DNN
defines a family of conditional Gibbs distribution P(FY |X) (Equation 84) and SGD aims to find
P*(FY |X) such that it can minimize the distance to the true posterior distribution P(Y |X).
J.3 SGD is equivalent to the energy minimization optimization
In most deep learning applications, P (y|x) is commonly an one-hot vector, i.e., P(y|x) only has a
single non-zero element. If we assume that l* is the index of the non-zero element of P(y|x), i.e.,
P[y(l*)|x] = 1, we can simplify KL[P (Y |X) k P(FY|X)] = -log[P(fY(l*)|x)].
Based on the definition ofP(FY |X) (Equation 84), we can obtain
KL[P(Y |X) k P(FY|X)] =-log[P(fY(l*)|x)]
=-fyl*(fI(∙ ∙ ∙ f2(f1(X))…))+ log[ZDNN(X)]	(89)
=EDNN (fyl* |x) + log[ZDNN(x)]
where fyl* is the l*th output node of the output layer fY. In particular, since log[ZDNN(X)] is not
related to the output fY , it can be viewed as a constant with respect to the gradient calculation
indicated by Table 8. Finally, combining Equation 88 and 89, we can obtain
P*(FY|X) = arg min EDNN(fyl* |X)	(90)
P(FY |X)∈Q
The above equation indicates that SGD is equivalent to the energy minimization optimization, which
in turn validates the Gibbs explanation for the hidden layer of DNNs.
33
Under review as a conference paper at ICLR 2020
? LabeI 2	“ Label 3	卅 Label 8
CNN
)%( rorre gnitseT
15 25 3
-1 -2
10
©8na
Label 4	Label 7	Label 10
MLP
505050
33221 1
)%( rorre gnitseT
-5000
-1000
傲-2000
乌
-3000
-4000
0	10	20	30
-1000
-2000
-3000
-4000
-5000
0	10	20	30
a
Figure 23: The first row shows the testing error of the CNN and the variation of ECNN (fyi* | x) given three
testing datasets with different labels over 30 training epochs. The second row shows testing error of the MLP
and the variation of EMLP (fyl* |x) given three testing datasets with different labels over 30 training epochs.
K Experiments validating the theoretical explanation for SGD
K. 1 Experiment: SGD is equivalent to the energy minimization optimization
To validate the proof that SGD learns a P(FY |X) with the minimal energy from the family of Gibbs
distribution determined by a DNN, we use SGD to train the CNN defined in Table 9 for classifying
the synthetic dataset shown in Figure 5. Specifically, we use the entire training dataset to train the
CNN over 30 epochs. During each training epoch, we calculate the energy function of P(FY |X)
given a testing dataset with the same label. Since the testing dataset has the same label, we have
the same index of the single non-zero element of P [y|x]. As a result, we can easily examine if the
corresponding energy function ECNN (fyl* |x) is decreasing during the training procedure.
Based on the architecture of CNN in Table 9, ECNN (fyl* |x) can be formulated as
ECNN(fyl*|x)=-fyl*(f4(f3(f2(f1(x)))))	(91)
In Figure 23, the first row visualizes the variation of ECNN (fyl* |x) over 30 training epochs. Though
there are certain fluctuations, ECNN (fyl* |x) indeed shows a decreasing trend. Due to limited space,
we only visualize the energy minimization procedure given three labels, but the procedure is valid for
any label. In addition, we demonstrate the equivalence based on the MLP (Table 3) for classifying
the MNIST dataset. The energy function of P(FY |X) corresponding to the MLP is defined as
EMLP(fyl*|x) =-fyl*(f2(f1(x)))	(92)
In Figure 23, the second row shows the same energy decreasing trend during training the MLP.
Overall, we demonstrate the equivalence between SGD and the energy minimization optimization,
which in turn validates the Gibbs explanation for DNNs.
K.2 Experiment: The parameters learned by SGD are not i.i.d.
To validate the parameters learned by SGD not i.i.d., we use SGD to train the MLP defined in Table 3
and the CNN defined in Table 5. Following the commonly used initialization approach, we initialize
all the weights of the two DNNs by Gaussian random numbers, i.e., all the parameters can be viewed
as i.i.d. before training. We use the absolute value of the sample correlation defined by Equation 2
to quantify the dependence between different weights during the training procedure.
Figure 24 visualizes the the variation of the average correlation between different weights of the
three layers in the MLP. Initially, the average correlation for f1 and f2 are very small, because
parameters are Gaussian random values. As the training procedure continues, we find that all the
correlation values are increasing. Notably, the increase for f2 is huge, i.e., the weights of f2 cannot
be i.i.d. anymore. However, the increase for f1 is very small, i.e., all the weights of f1 still could
be i.i.d.. Therefore, not all the learned parameters in the MLP are i.i.d..
34
Under review as a conference paper at ICLR 2020
Figure 24: (A) shows the testing error over 15 training epochs. (B), (C), and (D) show the average correlation
between different weights of three layers in the MLP over 15 training epochs.
h
fγ
Figure 25 visualizes the variation of the average correlation between different weights of the five
layers in the CNN. Similar as the weights in the MLP, the average correlation for f1 and f2 initially
are very small, because they are initialized as Gaussian random values. As the training procedure
continues, they have a sharp increasing and quickly achieve stationary, thus the parameters of f1 and
f2 are not i.i.d. anymore. In addition, the average correlation for f3 and f4 also have an increasing
trend, but their increase are very small, i.e., the parameters of f3 and f4 still could be i.i.d.. Overall,
we conclude that not all the learned parameters in the CNN are i.i.d..
CNN
(B)
h
O 5	10	15	20	25	30
O 5 IO 15	20	25	30
(E)
(C)
fγ
(F)
Figure 25: (A) shows the testing error over 30 training epochs. (B), (C), (D), (E) and (F) show the average
correlation between different weights of five layers in the CNN over 30 training epochs.
K. 3 Experiment: A better prior distribution indicates a better generalization
PERFORMANCE
We design three CNNs = {x; f1; f2; f3; f4; f5; fY } in Table 10 to classify the synthetic dataset
shown in Figure 5. We use the entire training dataset to train the CNNs over 30 epochs and use the
entire testing dataset to examine the generalization performance of the CNNs.
Based on the MRF explanation for convolutional layers, the first convolutional layer f1 formulates
a MRF model as the prior distribution P(F1), which can be expressed as
P(FI) = Z⅛- QK=1 eχp[3k(X)] = zFγeχP[pK=ι 萩(X)]	(93)
where 夕k (x) = Sk ◦ X + bf ∙ 1 is the kth linear convolutional channel, Sk is the linear convolutional
filter and b1k is the bias.
35
Under review as a conference paper at ICLR 2020
Table 10: The architectures of CNNs for classifying the synthetic dataset
R.V.	Layer	Description	CNN1	CNN2	CNN3
X	x	Input	32 X 32 X 1	32 X 32 X 1	32 X 32 X 1
F1	f1	Conv (3 × 3) +ReLU	30 X 30 X 4	30 X 30 X 12	30 X 30 X 36
	f2	Maxpool	15 X 15 X 4	15 X 15 X 12	15 X 15 X 36
F F2	f3	Conv (5 × 5) + ReLU	11 X 11 X 20	11 X 11 X 20	11 X 11 X 20
	f4	Maxpool	5 X 5 X 20	5 X 5 X 20	5 X 5 X 20
F3	f5	Fully connected	1 X 1 X 20^^	1 X 1 X 20	1 X 1 X 20
FY	fY	Output(softmax)	-^1 X 1 X 10^^	1X1X10	1 X 1 X 10
R.V. is the random variable of the hidden layer(s).					
Since the synthetic dataset obeys the Gaussian distribution P (X), we can measure the quality of the
prior distribution by averaging KLD[P (X)||P (F1)] over all the testing images for every training
epoch. In addition, we use the testing error to measure the generalization performance. Also note
that the only difference between the tree CNNs is they have different K numbers of convolutional
channels in f1 , i.e., different CNNs have different prior distributions. Therefore, we can examine
the effect of the prior distribution by checking the generalization performance of the three CNNs.
Figure 26 shows the learned prior distributions and the corresponding testing error over 30 training
epochs. We can find that there is a positive correlation between KLD[P (X)||P (F1)] and the testing
error. Specifically, the testing error decreases as the learned prior distribution P (F1) approaching
to the truly prior distribution P (X), and the testing error becomes stable when P (F1) achieves
stationary for all the three CNNs.
Moreover, we find that the CNN with more convolutional channels achieves the better generalization
performance. In terms of Bayesian theory, that is because a convolutional layer with more convolu-
tional channels can formulate a more powerful prior distribution, such that it can convey more prior
knowledge of the dataset, thereby rendering better generalization performance. Overall, we justify
that a better prior distribution indicates a better generalization performance.
(A) Training epoch
Figure 26: (A) shows the average KL[P (X)||P (F1)] overall all testing images. (B) shows the generalization
performance of the three CNNs represented by the testing error. The number behind the three CNNs in the
legend denotes the number of convolutional channels of f1 .
(B) Training epoch
36
Under review as a conference paper at ICLR 2020
L A regularization algorithm based on the probabilistic
REPRESENTATION
L.1 The proposed regularization algorithm
Based on the probabilistic representation, the first hidden layer formulates a MRF model as a prior
distribution of the training dataset and other hidden layers recursively serve as prior distributions for
their respective next layer in a CNN. However, the backpropagation calculates the gradient in the
backward direction, i.e., updating the gradient of a hidden layer corresponding to a prior distribution
must go over all hidden layers behind it. In addition, the degradation problem could makes the
learned prior distribution not precise enough to regularize DNNs and results in overfitting, especially
in very deep neural networks (He et al., 2016).
In this section, we propose to a new regularization learning algorithm for DNNs, which includes two
steps: (i) we learn the prior distributions of DNNs directly from the training dataset {xj }jJ=1 as the
blue arrow shown in Figure 27 and (ii) we use the backpropagation to train the initialized DNNs by
the learned prior distributions as the red arrow shown in Figure 27.
Since directly learning prior distribution from {xj}jJ=1 is less complicated than the backpropagation
learning algorithm, we can relieve the effects of degradation problem and derive prior distributions
more precisely, thereby achieving the better generalization performance of DNNs.
An important question need to answer is how many hidden layers of DNNs need to be pre-trained in
the first step. Since most prior information is stored in the training dataset {xj}jJ=1, the hidden layer
closest to the input of DNNs, i.e., the first hidden layer f1, should learn the most prior information
from {xj }jJ=1 as long as f1 has powerful representation ability through including sufficient hidden
units, e.g., convolutional filters. Therefore, the proposed regularization method only pre-train the
first hidden layer f1 in order to decrease the computation complexity. Pre-training more hidden
layers could obtain better generalization performance, which can be a direction for future research.
We choose the Field of Experts (FoE) model to directly learn a prior distribution of {xj}jJ=1 because
of two reasons. Above all, FoE achieves great performance on learning prior knowledge of given
datasets on various applications, e.g., image denoising (Roth & Black, 2005), image restoration
Roth & Black (2008), and image inpainting (Schmidt et al., 2011). More importantly, FoE can be
regarded as a Convolutional Neural Networks (CNN) with a single convolutional layer, hence we
can easily use the learned FoE model to initialize the first hidden layer of CNNs as long as they have
the same architecture. A FoE model can be formulated as
pFoE(x)
1K
ɪ Y {fkxpert[fk (x)]},
ZFoE k=1
(94)
where fk(∙) is a linear filter, fkxpert is an expert function, and Zf°e = Rx QK=ι{fkxpert[fk(x)]}dx is
the partition function. We use the contrastive divergence learning method to train pFoE(x) under the
Kullback-Leibler divergence (KLD) criterion (Carreira-Perpinan & Hinton, 2005). More detail of
the training procedure is in (Roth & Black, 2005; 2008). After deriving a well-trained FoE model,
we use the learned linear filters {fk}kK=1 to initialize the first convolutional layer of CNNs. Finally,
we use the backpropagation algorithm to train the initialized CNN for a specific application, such as
image recognition. We summarize the proposed regularization learning algorithm in Algorithm 2.
{χj}ji=ι
(K+1IK)∙∙∙∙P(%∣F
{yj}{=ι
Figure 27: The training procedure of a DNN = {x; fi；…；fγ}. The red arrow indicates the
traditional training algorithm, e.g. backpropagaton, which derives an output of the DNN given
training dataset {xj }jJ=1 in the forward direction and calculate the gradient of parameters with
respect to the training labels Y for updating the parameters in the backward direction.
37
Under review as a conference paper at ICLR 2020
Algorithm 2 Regularization learning algorithm
input： D = {(xn, yn)}N=ι, DNN = {x; fi；…；fɪ； fγ}, andPFoE(X)
1:	setup
2:	initialize iteration k = 0, and training epochs K
3:	specify the architecture of the DNN and pFoE (x)
4:	regularization learning
5:	trainpFoE(x) given {xn}nN=1
6:	initialize f1 by the well-trained pFoE (x)
7:	repeat
8:	train the DNN given D
9:	until (k = K)
output: the well-trained DNN for image recognition
L.2 Simulations
In this section, we validate the proposed regularization learning algorithm for image recognition
application on the CIFAR-10 benchmark dataset, which includes 70,000 32 × 32 color images in
10 classes, and each class has 6,000 training images and 1,000 testing images. Specifically, we
randomly choose 2,000 training images of each class as a new training dataset and use the same
testing dataset. Smaller training dataset means overfitting being more likely to occur, which can
help us validate the proposed regularization learning method more convincingly. In addition, we
covert color images to grayscale to simplify computation.
In order to directly learn regularization from {xj}j2=00100, we design a FoE model with 12 Gaussian
Scale Mixture (GSM) experts, thus it can be expressed as
PFoE(X) =熹 Qk=ι{fGSMfk(x)]},	(95)
where fGSM[fk(x)] = PI=I ∏ki ∙ N(fk(x)；0,条),σ2 is a fixed base variance,
δ = {δ(1), ∙∙∙ , δ(11)} is a range of scales, fk(∙) is a 3 X 3 convolutional filter, and ∏ki denotes
the weight of each Gaussian distribution. The parameters of the FoE model is θ = {f, π}, where
f = [f1, ..., f12] and π = {πki}kk==11,2i,=i=111. The architecture of the CNN for image recognition is
summarized in Table 11. It is noteworthy that the number of convolutional filters in f1 and the filter
dimension are the same as the above FoE model, thus we can use the well-trained FoE to initialize
f1 as a regularization for the CNN classifying the CIFAR-10 dataset based on Algorithm 2.
In order to make a comprehensive comparison, we choose two commonly used regularizations, i.e.,
the dropout (drop rate is 0.3) and the L2 norm, as references. We use the testing error to exam
the generalization performances of different regularizations. Figure 28 visualizes that the proposed
regularization method (abbr. Bayes Prior (BP)) successfully decreases the testing error. In particular,
it outperforms dropout (32.26%) and achieve similar performance as L2 norm (29.58%). Therefore,
this experiment validates the effectiveness of the proposed regularization learning method.
Table 11: The architectures of CNN for CIFAR-10 classification
Layer	Description	Output
x	Input	32 × 32 × 1
f1	-_Conv (3 × 3)~~	30 × 30 × 12
f2	ReLU	30 × 30 × 12
f3	-_Conv (5 X 5)~~	26 × 26 × 32
f4	ReLU + Maxpool	13× 13× 32
f5	-_Conv (5 × 5)~~	9 × 9 × 128
f6	ReLU + Maxpool	4 × 4 × 128
f7	Fully connected	1 × 1 × 512
f8	ReLU	1 × 1 × 512
f9	Fully connected	1 × 1 × 32
f10	ReLU	1 × 1 × 32
fY	OUtPUt(SOftmax)	1×1×10
38
Under review as a conference paper at ICLR 2020
0.7
0.65
0.6
----Testing error of CNN without any regularization (33.95%)
----Testing error of CNN with droupout (32.26%)
——Testing error of CNN with L2 norm (29.58%)
----Testing error of CNN with Bayes prior (29.90%)
5554
.5 0. .4 0.
00
JOjjaboUI+≡SΦI
0.35
0.3
0.25 -----------1------------1-----------1------------1-----------1------------
0	5	10	15	20	25	30
Training epoch
Figure 28:	The average testing error of the CNN on the last 5 training epochs based on various regularizations.
Bayes prior indicates the proposed regularization and its testing error is 29.90%.
Moreover, the proposed regularization method can be easily combined with other regularizations
methods to improve the generalization performance further. We combine two arbitrary aforemen-
tioned regularization methods together and examine their generalization performances under the
same experimental conditions as above. Figure 29 shows that the combination of two regulariza-
tions together achieves better generalization performance than any single one. In particular, the
combination of BP and L2 norm achieves the lowest testing error (28.60%). Also note that if we
combine three regularizations together, we achieve the best generalization performance (27.69%).
This experiment further validate the proposed regularization learning algorithm.
0.7
0.65
0.6
0.25
0
---Testing error of CNN with Bayes Prior (BP) (29.90%)
---Testing error of CNN with droupout + L2 norm (29.29%)
---Testing error of CNN with BP + droupout (28.91%)
——Testing error of CNN with BP + L2 norm (28.60%)
---Testing error of CNN with BP + both (27.69%)
5	10	15	20	25	30
Training epoch
Figure 29:	The average testing error of the CNN on the last 5 training epochs based on various combined
regularizations. Bayes prior + both indicates that we combine all three regularizations together.
39