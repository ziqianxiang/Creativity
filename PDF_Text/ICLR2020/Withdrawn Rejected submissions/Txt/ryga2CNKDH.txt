Under review as a conference paper at ICLR 2020
Evaluating Lossy Compression Rates of
Deep Generative Models
Anonymous authors
Paper under double-blind review
Ab stract
Deep generative models have achieved remarkable progress in recent years. De-
spite this progress, quantitative evaluation and comparison of generative models
remains as one of the important challenges. One of the most popular metrics
for evaluating generative models is the log-likelihood. While the direct computa-
tion of log-likelihood can be intractable, it has been recently shown that the log-
likelihood of some of the most interesting generative models such as variational
autoencoders (VAE) or generative adversarial networks (GAN) can be efficiently
estimated using annealed importance sampling (AIS). In this work, we argue that
the log-likelihood metric by itself cannot represent all the different performance
characteristics of generative models, and propose to use rate distortion curves to
evaluate and compare deep generative models. We show that we can approxi-
mate the entire rate distortion curve using one single run of AIS for roughly the
same computational cost as a single log-likelihood estimate. We evaluate lossy
compression rates of different deep generative models such as VAEs, GANs (and
its variants) and adversarial autoencoders (AAE) on MNIST and CIFAR10, and
arrive at a number of insights not obtainable from log-likelihoods alone.
1	Introduction
Generative models of images represent one of the most exciting areas of rapid progress of AI (Brock
et al., 2019; Karras et al., 2018b;a). However, evaluating the performance of generative models
remains a significant challenge. Many of the most successful models, most notably Generative
Adversarial Networks (GANs) (Goodfellow et al., 2014), are implicit generative models for which
computation of log-likelihoods is intractable or even undefined. Evaluation typically focuses on
metrics such as the Inception score (Salimans et al., 2016) or the Frechet Inception Distance (FID)
score (Heusel et al., 2017), which do not have nearly the same degree of theoretical underpinning as
likelihood-based metrics.
Log-likelihoods are one of the most important measures of generative models. Their utility is evi-
denced by the fact that likelihoods (or equivalent metrics such as perplexity or bits-per-dimension)
are reported in nearly all cases where it’s convenient to compute them. Unfortunately, computa-
tion of log-likelihoods for implicit generative models remains a difficult problem. Furthermore,
log-likelihoods have important conceptual limitations. For continuous inputs in the image domain,
the metric is often dominated by the fine-grained distribution over pixels rather than the high-level
structure. For models with low-dimensional support, one needs to assign an observation model, such
as (rather arbitrary) isotropic Gaussian noise (Wu et al., 2016). Lossless compression metrics for
GANs often give absurdly large bits-per-dimension (e.g. 1014) which fails to reflect the true perfor-
mance of the model (Grover et al., 2018; Danihelka et al., 2017). See Theis et al. (2015) for more
discussion of limitations of likelihood-based evaluation.
Typically, one is not interested in describing the pixels ofan image directly, and it would be sufficient
to generate images close to the true data distribution in some metric such as Euclidean distance.
For this reason, there has been much interest in Wasserstein distance as a criterion for generative
models, since the measure exploits precisely this metric structure (Arjovsky et al., 2017; Gulrajani
et al., 2017; Salimans et al., 2018). However, Wasserstein distance remains difficult to approximate,
and hence it is not routinely used to evaluate generative models.
1
Under review as a conference paper at ICLR 2020
We aim to achieve the best of both worlds by measuring lossy compression rates of deep generative
models. In particular, we aim to estimate the rate distortion function, which measures the number
of bits required to match a distribution to within a given distortion. Like Wasserstein distance,
it can exploit the metric structure of the observation space, but like log-likelihoods, it connects
to the rich literature of probabilistic and information theoretic analysis of generative models. By
focusing on different parts of the rate distortion curve, one can achieve different tradeoffs between
the description length and the fidelity of reconstruction — thereby fixing the problem whereby
lossless compression focuses on the details at the expense of high-level structure. It has the further
advantage that the distortion metric need not have a probabilistic interpretation; hence, one is free
to use more perceptually valid distortion metrics such as structural similarity (SSIM) (Wang et al.,
2004) or distances between hidden representations of a convolutional network (Huang et al., 2018).
Algorithmically, computing rate distortion functions raises similar challenges to estimating log-
likelihoods. We show that the rate distortion curve can be computed by finding the normalizing
constants of a family of unnormalized probability distributions over the noise variables z. Inter-
estingly, when the distortion metric is squared error, these distributions correspond to the posterior
distributions ofz for Gaussian observation models with different variances; hence, the rate distortion
analysis generalizes the evaluation of log-likelihoods with Gaussian observation models.
Annealed Importance Sampling (AIS) (Neal, 2001) is currently the most effective general-purpose
method for estimating log-likelihoods of implicit generative models, and was used by Wu et al.
(2016) to compare log-likelihoods of a variety of implicit generative models. The algorithm is based
on gradually interpolating between a tractable initial distribution and an intractable target distri-
bution. We show that when AIS is used to estimate log-likelihoods under a Gaussian observation
model, the sequence of intermediate distributions corresponds to precisely the distributions needed
to compute the rate distortion curve. Since AIS maintains a stochastic lower bound on the nor-
malizing constants of these distributions, it automatically produces an upper bound on the entire
rate distortion curve. Furthermore, the tightness of the bound can be validated on simulated data
using bidirectional Monte Carlo (BDMC) (Grosse et al., 2015; Wu et al., 2016). Hence, we can
approximate the entire rate distortion curve for roughly the same computational cost as a single
log-likelihood estimate.
We use our rate distortion approximations to study a variety of variational autoencoders
(VAEs) (Kingma & Welling, 2013), GANs and adversarial autoencoders (AAE) (Makhzani et al.,
2015), and arrive at a number of insights not obtainable from log-likelihoods alone. For instance,
we observe that VAEs and GANs have different rate distortion tradeoffs: While VAEs with larger
code size can generally achieve better lossless compression rates, their performances drop at lossy
compression in the low-rate regime. Conversely, expanding the capacity of GANs appears to bring
substantial reductions in distortion at the high-rate regime without any corresponding deterioration
in quality in the low-rate regime. We find that increasing the capacity of GANs by increasing the
code size (width) has a qualitatively different effect on the rate distortion tradeoffs than increasing
the depth. We also find that that different GAN variants with the same code size achieve nearly
identical RD curves, and that the code size dominates the performance differences between GANs.
2	Background
2.1	Annealed Importance Sampling
Annealed importance sampling (AIS) (Neal, 2001) is a Monte Carlo algorithm based on constructing
a sequence of n intermediate distributions Pk (Z) = PZz), where k ∈ {0,...,n}, between a tractable
initial distribution p0 (z) and the intractable target distribution pn (z). At the the k-th state (0 ≤ k ≤
n), the forward distribution qf and the un-normalized backward distribution % are
qf(z0, . . . , zk) = p0(z0)T0(z1 |z0) . . . Tk-1(zk|zk-1),
Qb(z0, . . . , zk ) = Pk (Zk)Tk— 1 (Zk —1|Zk) ... T0 (Z0 |ZI),
(1)
(2)
where Tk is an MCMC kernel that leaves Pk (Z) invariant; and Tk is its reverse kernel. We run M
independent AIS chains, numbered i = 1, . . . , M. Let Zik be the k-th state of the i-th chain. The
2
Under review as a conference paper at ICLR 2020
importance weights and normalized importance weights are
i =布(zi,...,zk)= pι(zI)遢&)	Pk (Zk)	* =	Wk
k = f(zi,...,zk) = PO(Zi)而…Pk-TZD,	w	P=W.
(3)
At the k-th step, the unbiased partition function estimate of Pk (z) is Zk = M PMi Wk.
At the k-th step, we define the AIS distribution qkAIS(z) as the distribution obtained by first sampling
z1k, . . . , zkM from the M parallel chains using the forward distribution qf (zi1, . . . , ziM), and then
re-sampling these samples based on Wk. More formally, the AIS distribution is defined as follows:
M
qAIS(Z)= EQM=I qf(z1,…,zk)[X Wk S(Z- Zik )].	⑷
i=1
Bidirectional Monte Carlo. We know that the log partition function estimate log Z is a stochastic
lower bound on log Z (Jensen’s inequality). As the result, using the forward AIS distribution as the
proposal distribution results in a lower bound on the data log-likelihood. By running AIS in reverse,
however, we obtain an upper bound on log Z. However, in order to run the AIS in reverse, we need
exact samples from the true posterior, which is only possible on the simulated data. The combination
of the AIS lower bound and upper bound on the log partition function is called bidirectional Monte
Carlo (BDMC), and the gap between these bounds is called the BDMC gap (Grosse et al., 2015).
We note that AIS combined with BDMC has been used to estimate log-likelihoods for implicit
generative models (Wu et al., 2016). In this work, we validate our AIS experiments by using the
BDMC gap to measure the accuracy of our partition function estimators.
2.2	Rate Distortion Theory
Let x be a random variable that comes from the data distribution Pd(x). Shannon’s fundamental
compression theorem states that we can compress this random variable losslessly at the rate of
H(x). But if we allow lossy compression, we can compress x at the rate of R, where R ≤ H(x),
using the code z, and have a lossy reconstruction X = f (z) with the distortion of D, given a
distortion measure d(x, X) = d(x, f (z)). The rate distortion theory quantifies the trade-off between
the lossy compression rate R and the distortion D. The rate distortion function R(D) is defined as
the minimum number of bits per sample required to achieve lossy compression of the data such that
the average distortion measured by the distortion function is less than D. Shannon’s rate distortion
theorem states that R(D) equals the minimum of the following optimization problem:
min I(z;X)	s.t. Eq(x,z)[d(X, f (z))] ≤ D.	(5)
q(z|x)
where the optimization is over the channel conditional distribution q(z|X). Suppose the data-
distribution is Pd(X). The channel conditional q(z|X) induces the joint distribution q(z, X) =
Pd(X)q(z|X), which defines the mutual information I(z; X). q(z) is the marginal distribution over
z of the joint distribution q(z, X), and is called the output marginal distribution. We can rewrite the
optimization of Eq. 5 using the method of Lagrange multipliers as follows:
min I(z; X) + βEq(z,x)[d(X, f (z))].	(6)
q(z|x)
2.3	Implicit Generative Models
The goal of generative modeling is to learn a model distribution P(X) to approximate the data distri-
bution Pd(X). Implicit generative models define the model distribution P(X) using a latent variable
z with a fixed prior distribution P(z) such as a Gaussian distribution, and a decoder or generator net-
work which computes X = f (z). In some cases (e.g. VAEs, AAEs), the generator explicitly param-
eterizes a conditional distribution P(X|z), such as a Gaussian observation model N(X; f (z), σ2I).
But in implicit models such as GANs, the generator directly outputs X = f (z). In order to treat
VAEs and GANs under a consistent framework, we ignore the Gaussian observation model of VAEs
(thereby treating the VAE decoder as an implicit model), and use the squared error distortion of
d(X, f (z)) = kX - f (z)k22. However, we note that it is also possible to assume a Gaussian ob-
servation model with a fixed σ2 for GANs, and use the Gaussian negative log-likelihood (NLL)
as the distortion measure for both VAEs and GANs: d(X, f (z)) = - log N (X; f (z), σ2I). This is
equivalent to squared error distortion up to a linear transformation.
3
Under review as a conference paper at ICLR 2020
3	Rate-Prior Distortion Functions
In this section, we describe the rate-prior distortion function, as a variational upper bound on the
true rate distortion function.
3.1	Variational Bounds on Mutual Information
We must modify the standard rate distortion formalism slightly in order to match the goals of gener-
ative model evaluation. Specifically, we are interested in evaluating lossy compression with coding
schemes corresponding to particular trained generative models, including the fixed prior p(z). For
models such as VAEs, KL(q(z|x)kp(z)) is standardly interpreted as the description length of z.
Hence, we adjust the rate distortion formalism to use Epd(x)KL(q(z|x)kp(z)) in place of I(x, z),
and refer to this as the rate-prior objective. The rate-prior objective upper bounds the standard rate:
I(x; z) ≤ I(x; z) + KL(q(z)kp(z)) = Epd(x)KL(q(z|x)kp(z)).	(7)
In the context of variational inference, q(z|x) is the posterior, q(z) = pd(x)q(z|x)dx is the
aggregated posterior (Makhzani et al., 2015), and p(z) is the prior. In the context of rate distortion
theory, q(z|x) is the channel conditional, q(z) is the output marginal, and p(z) is the variational
output marginal distribution. The inequality is tight when p(z) = q(z), i.e., the variational output
marginal (prior) is equal to the output marginal (aggregated posterior). We note that the upper bound
of Eq. 7 has been used in other algorithms such as the Blahut-Arimoto algorithm (Arimoto, 1972)
or the variational information bottleneck algorithm (Alemi et al., 2016).
3.2	Rate-Prior Distortion Functions
Analogously to the rate distortion function, we define the rate-prior distortion function Rp (D) as
the minimum value of the rate-prior objective for a given distortion D. More precisely, Rp (D) is
the solution of
min Epd(x)KL(q(z|x)kp(z))	s.t. Eq(x,z)[d(x, f (z))] ≤ D.
q(z|x)
(8)
We can rewrite the optimization of Eq. 8 using the method of Lagrange multipliers as follows:
min Epd(x)KL(q(z|x)kp(z)) + βEq(x,z)[d(x,f(z))].	(9)
q(z|x)
Conveniently, the Lagrangian decomposes into independent optimization problems for each x, al-
lowing us to treat this as an optimization problem over q(z|x) for fixed x. We can compute the rate
distortion curve by sweeping over β rather than by sweeping over D.
Now we describe some of the properties of the rate-prior distortion function Rp (D), which are
straightforward analogues of well-known properties of the rate distortion function.
Proposition 1. Rp (D) has the following properties:
(a)	Rp(D) is non-increasing and convex function of D.
(b)	We have R(D) = minp(z) Rp(D). As a corollary, for any p(z), we have R(D) ≤ Rp(D).
(c)	The rate-prior distortion optimization of Eq. 9 has a unique global optimum which can be
expressed as q,β(z|x) = Ze(X)p(z)exp(-βd(x,f (z))).
Proof. The proofs are provided in Appendix C.1.
Prop. 1b states that for any prior p(z), Rp(D) is a variational upper-bound on R(D). More specif-
ically, we have R(D) = minp(z) R(D), which implies that for any given β, there exists a prior
Pβ(z), for which the variational gap between rate distortion and rate-prior distortion functions at β
is zero. Fig. 1a shows a geometrical illustration of Prop. 1b for three values of β ∈ {0.25, 1, 4}.
We can see in this figure that all Rp(D) curves are upper bounds on R(D), and for any given β,
Rp* (D) is tangent to both Rp(D) and to the line with the slope of β passing through the optimal
solution.
4
Under review as a conference paper at ICLR 2020
Figure 1:	The rate-prior distortion function with (a) arbitrary distortion function d(x, f (z)) and (b)
negative log-likelihood distortion function of - log p(x|z).
3.3 Rate-Prior Distortion Functions with NLL Distortion
If the decoder outputs a probability distribution (as in a VAE), we can define the distortion metric
to coincide with the negative log-likelihood (NLL): d(x, f (z)) = - log p(x|z). We now describe
some of the properties of the rate-prior distortion functions with NLL distortions.
Proposition 2. The rate-prior distortion function Rp(D) with NLL distortion of - log p(x|z) has
the following properties:
(a)	R(D) is lower bounded by the linear function of Hd - D, and upper bounded by the
rate-prior distortion function: Hd - D ≤ R(D) ≤ Rp(D).
(b)	The global optimum of rate-prior distortion optimization (Eq. 9) can be expressed as
qβ(ZIX) = Zβ1χ)P(Z)P(XIZ)β, where ZI(X) = RP(Z)P(XIZ)βdz.
(c)	At β = 1, the negative summation of rate-prior and distortion is the true log-likelihood:
Lp = Epd(x)[logP(X)] = -Rββ=1 - Dβ β=1.
Proof. The proofs are provided in Appendix C.2.
Fig. 1b shows the geometrical illustration of Prop. 2. We can see that according to Prop. 2a the
rate distortion function R(D) is sandwiched between the linear function Hd - D and the rate-prior
distortion Rp(D). At β = 1, let LI and Lp be the negative summation of rate and distortion on the
rate distortion and rate-prior distortion curves respectively (shown in Fig. 1b). From Prop. 2c we
know that Lp is the true log-likelihood of the generative model. From Prop. 1b, we can conclude
that LI = maxp(z) Lp . This reveals an important relationship between rate distortion theory and
generative modeling that was also observed in Lastras (2019): for a given generative model with a
fixed conditional P(XIZ), the best log-likelihood Lp that can be achieved by optimizing the prior P(Z)
is the LI , which can be found by solving the rate distortion problem. Furthermore, the corresponding
optimal prior PI(Z) is the output marginal of the optimal channel conditional of the rate distortion
problem at β = 1. Fig. 1b shows the rate-prior distortion function Rp* (D) corresponding to pl (z).
In a “good” generative model, where the model distribution is close to the data-distribution, the
negative log-likelihood -Lp is close to the entropy of data Hd, and the variational gap between
Rp(D) and R(D) is tight.
4 B ounding Rate-Prior Distortion Functions with AIS
In the previous section, we introduced the rate-prior distortion function Rp (D) and showed that it
upper bounds the true rate distortion function R(D). However, evaluating Rp(D) is also intractable.
In this section, we show how we can upper bound Rp(D) using a single run of the AIS algorithm.
AIS Chain. We fix a temperature schedule 0 = β0 < β1 < . . . < βn = ∞. For the k-th
intermediate distribution, we use the optimal channel conditional qk(ZIX) and partition function
Zk(X), corresponding to points along Rp(D) and derived in Prop. 1c:
qk(z∣x) = 1- qk (z|x),
Zk
where qk (z|x) = p(z)exp(-βkd(x,f(z))),
Zk(X) = / qk (z∖x)dz.
(10)
5
Under review as a conference paper at ICLR 2020
Conveniently, this choice coincides with geometric averages, the typical choice of intermediate dis-
tributions for AIS, i.e, the kth step happens to be the optimal solutions for βk . This chain is shown
in Fig. 2. For the transition operator, we use Hamiltonian Monte Carlo (Neal et al., 2011). At the
k-th step, the rate-prior Rk (x) and the distortion Dk (x) are
Rk(x) = KL(qk(z|x)kp(z)),	Dk(x) = Eqk (z|x) [d(x, f (z))].	(11)
AIS Rate-Prior Distortion Curve. For each data point x, we run M independent AIS chains,
numbered i = 1, . . . , M, in the forward direction. At the k-th state of the i-th chain, let zik be the
state, Wk be the AIS importance weights, and Wk be the normalized AIS importance weights. We
denote the AIS distribution at the k-th step as the distribution obtained by first sampling from all
the M forward distributions qf (zi1, . . . , zik |x)	, and then re-sampling the samples based on
their normalized importance weights Wk (see Section 2.1 and Appendix C.4 for more details). More
formally qkAIS(z|x) is
M
qAIS(ZIX)= EQM=I qf (z1,...,zk |x) [X Wk δ(Z - zik 1.	(I2)
i=1
Using the AIS distribution qkAIS(Z|X) defined in Eq. 12, we now define the AIS distortion DkAIS(X)
and the AIS rate-prior RkAIS (X) as follows:
DkAIS(X) = EqkAIS(z|x)[d(X,f(Z))]	RkAIS(X) = KL(qkAIS(Z|X)kp(Z))	(13)
We now define the AIS rate-prior distortion curve RpAIS(D) (shown in Fig. 1b) as the RD cruve
obtained by tracing pairs of RkAIS (X), DkAIS (X) .
Proposition 3. The AIS rate-prior distortion curve upper bounds the rate-prior distortion function:
RpAIS (D) ≥ Rp(D).
Proof. The proof is provided in Appendix C.4.
Estimated AIS Rate-Prior Distortion Curve. Although the AIS distribution can be easily sampled
from, its density is intractable to evaluate. As the result, evaluating RpAIS (D) is also intractable. We
now propose to evaluate an upper-bound on RpAIS(D) by finding an upper bound for RkAIS(X), and
an unbiased estimate for DkAIS(X). We use the AIS distribution samples Zik and their corresponding
weights Wk to obtain the following distortion and partition function estimates:
DDAIS(X) = X Wkd(x,f(zk))),	ZAIS(X) = M X Wk.	(14)
ii
Having found the estimates DDAIS(X) and ZAIS(X), We propose to estimate the rate as follows:
RAIS(X) = - log ZAIS(X)- βkDAis(x).	(15)
We define the estimated AIS rate-prior distortion curve RAIS (D) (shown in Fig. 1b) as an RD curve
obtained by tracing pairs of rate distortion estimates (RAIS (x) , DAIS (x)).
Proposition 4. The estimated AIS rate-prior distortion curve upper bounds the AIS rate-prior dis-
tortion curve in expectation: E[RAIS(D)] ≥ RAIS(D). More specifically, we have
E[RAIs(x)] ≥ RAIS(x),	E[DAIS(x)] = DAIS(x).	(16)
Proof. The proof is provided in Appendix C.4.
In summary, from Prop. 1, Prop. 3 and Prop. 4, we can conclude that the estimated AIS rate-prior
distortion curve upper bounds the true rate distortion curve in expectation (shown in Fig. 1b):
E[RAIS(D)] ≥ RAIS(D) ≥ RP(D) ≥ R(D).	(17)
In all the experiments, we plot the estimated AIS rate-prior distortion function RAIS(D).
6
Under review as a conference paper at ICLR 2020
SO = O
OO(ZlX) =P(Z)
βθ(≡∣x) =P(Z)
IOgZ(J(X) = O
7⅞(χ) = O
2⅛(x) = ¾(,)[-logp(x∣≡)]
⅛
以(ZlX) = P(ZwXlZ)
⅛(≡l^)=大 P(Z)PA'(x ㈤
lɑg 软(x) = Iog ʃ PG)Pfc(XIZ)
版(ZlX) =0(Z)P(XIZ)
央(ZIX)=P(ZlX)
log¾(x) = logp(x)
BaTOo
‰(z∣x) =p(z)p∞(x∣z)
‰(≡∣x) =WZ-ZML)
IogZn(X) → ∞
TJneX) → 8
AI(X) = — IOgP(XwML)
Figure 2:	AIS chain for estimating the rate-prior distortion function with NLL distortion.
Accuracy of AIS Estimates. While the above discussion focuses on obtaining upper bounds, we
note that AIS is one of the most accurate general-purpose methods for estimating partition functions,
and therefore we believe our AIS upper bounds to be fairly tight in practice. In theory, for large
number of intermediate distributions, the AIS variance is proportional to 1/MK (Neal, 2001; 2005),
where M is the number of AIS chains and K is the number of intermediate distributions. For the
main experiments of our paper, we evaluate the tightness of the AIS estimate by computing the
BDMC gap, and show that in practice our upper bounds are tight (Appendix D).
The Rate Distortion Tradeoff in the AIS Chain. Different values of β corresponds to differ-
ent tradeoffs between the compression rate and the distortion in a given generative model. β = 0
corresponds to the case where q0(z|x) = p(z). In this case, the compression rate is zero, and
the distortion would be large, since in order to reconstruct x, we simply sample from the prior
and generate a random X that is completely independent of x. In this case, the distortion would
be D0(x) = Ep(z) [d(x, f (z))]. In the case of probabilistic decoders with NLL distortion, another
interesting intermediate distribution is β' = 1, where the optimal channel conditional is the true
posterior of the generative model qg(z∣x) = p(z∣x). In this case, as shown in Prop. 2c, the summa-
tion of the rate-prior and the distortion term is the negative of true log-likelihood of the generative
model. As βn → ∞, the network cares more about the distortion and less about the compression
rate. In this case, the optimal channel conditional would be qn(z|x) = δ(z - zML(x)), where
zML(x) = arg minz d(x, f (z)). In other words, since the network only cares about the distortion,
the optimal channel condition puts all its mass on zML which minimizes the distortion. However,
the network would require infinitely many bits to precisely represent this delta function, and thus
the rate goes to infinity.
5	Related works
Evaluation of Implicit Generative Models . Quantitative evaluation of the performance of GANs
has been a challenge for the field since the beginning. Many heuristic measures have been proposed,
such as the Inception score (Salimans et al., 2016) and the FreChet Inception Distance (FID) (Heusel
et al., 2017). One of the main drawbacks of the IS or FID is that a model that simply memorizes the
training dataset would obtain a near-optimal score. Another, drawback of these methods is that they
use a pretrained ImageNet classifier weights which makes them sensitive to the weights (Barratt &
Sharma, 2018) of the classifier, and less applicable to other domains and datasets. Another evalua-
tion method that sometimes is being used is the Parzen window estimate, which can be shown to be
an instance of AIS with zero intermediate distributions, and thus has a very large variance. Another
evaluation method of GANs that was proposed in Metz et al. (2018) is measuiring the ability of
the generator network to reconstruct the samples from the data distribution. This metric is similar
to the distortion obtained at the high-rate regime of our rate distortion framework when β → ∞.
Another related work is GILBO (Alemi & Fischer, 2018), which similar to our framework does
not require the generative model to have a tractable posterior and thus allows direct comparison of
VAEs and GANs. However, GILBO can only evaluate the performance of the generative model on
the simulated data and not the true data distribution.
Rate Distortion Theory and Generative Models. Perhaps the closest work to ours is “Fixing a
Broken ELBO” (Alemi et al., 2018), which plots rate-prior distortion curves for VAEs. Our work
is different than Alemi et al. (2018) in two key aspects. First, in Alemi et al. (2018) the rate-prior
distortion function is evaluated by fixing the architecture of the neural network, and learning the
distortion measure d(x, f (z)) in addition to learning q(z|x). Whereas, in our definition of rate dis-
tortion, we assumed the distortion measure is fixed and given by a trained generative model. As the
result, we plot the rate-prior distortion curve for a particular generative model, rather than a par-
ticular architecture. The second key difference is that, consistent with the Shannon’s rate distortion
7
Under review as a conference paper at ICLR 2020
(a) MNIST
Figure 3: The rate distortion curves of GANs.
Comparing GANs on CIFAR10
Distortion (Squared Error)
(b) CIFAR-10
theorem, We find the optimal channel conditional q* (z|x) by using AIS; while in Alemi et al. (2018),
q(z|x) is a variational distribution that is restricted to a variational family.
See Appendix A for a discussion of related works about practical compression schemes, distortion-
perception tradeoffs, and precision-recall tradeoffs.
6	Experiments
In this section, we use our rate distortion approximations to answer the following questions: How do
different generative models such as VAEs, GANs and AAEs perform at different lossy compression
rates? What insights can we obtain from the rate distortion curves about different characteristics of
generative models? What is the effect of the code size (width), depth of the network, or the learning
algorithm on the rate distortion tradeoffs?
6.1	Rate Distortion Curves of Deep Generatiev Models
Rate Distortion Curves of GANs. Fig. 3 shows rate distortion curves for GANs trained on MNIST
and CIFAR-10. We varied the dimension of the noise vector z, as well as the depth of the decoder.
For the GAN experiments on MNIST (Fig. 3a), the label “deep” corresponds to three hidden layers
of size 1024, and the label “shallow” corresponds to one hidden layer of size 1024. We trained
shallow and deep GANs with Gradient Penalty (GAN-GP) (Gulrajani et al., 2017) with the code
size d ∈ {2, 5, 10, 100} on MNIST. For the GAN experiments on CIFAR-10 (Fig. 3b), we trained
the DCGAN (Radford et al., 2015), GAN with Gradient Penalty (GP) (Gulrajani et al., 2017), SN-
GAN (Miyato et al., 2018), and BRE-GAN (Cao et al., 2018), with the code size of d ∈ {2, 10, 100}.
In both the MNIST and CIFAR experiments, we observe that in general increasing the code size has
the effect of extending the curve leftwards. This is expected, since the high-rate regime is effectively
measuring reconstruction ability, and additional dimensions in z improves the reconstruction.
We can also observe from Fig. 3a that increasing the depth pushes the curves down and to the left.
In other words, the distortion in both high-rate and mid-rate regimes improves. In these regimes,
increasing the depth increases the capacity of the network, which enables the network to make a
better use of the information in the code space. In the low-rate regime, however, increasing the
depth, similar to increasing the latent size, does not improve the distortion.
Rate Distortion Curves of VAEs. Fig. 4 com-
pares VAEs, AAEs and GP-GANs (Gulrajani et al.,
2017) with the code size of d ∈ {2, 10, 100}, and the
same decoder architecture on the MNIST dataset. In
general, we can see that in the mid-rate to high-rate
regimes, VAEs achieve better distortions than GANs
with the same architecture. This is expected as the
VAE is trained with the ELBO objective, which en-
courages good reconstructions (in the case of factor-
ized Gaussian decoder). We can see from Fig. 4 that
in VAEs, increasing the latent capacity pushes the
rate distortion curve up and to the left. In other words, in contrast with GANs where increasing
the latent capacity always improves the rate distortion curve, in VAEs, there is a trade-off whereby
Comparing GANs, VAEs and AAEs
GP-GAN2
GP-GANlO
Gp-GANIOO
VAE2
VAElO
VAElOO
AAE2
AAElO
AAEloO
Distortion (Squared Error)
Figure 4: RD curves OfVAEs, GANs, AAEs.
8
Under review as a conference paper at ICLR 2020
Mixture of Priors
Distortion (Squared Error)
(a) Mixture Prior
(b) Blurring Conditional Likelihood
Figure 5: (a) Effect of damaging the prior of VAE by using a mixture with a bad prior. (b) Effect
damaging the conditional likelihood of VAE by adding a Gaussian kernel after the last decoder layer.
increasing the capacity reduces the distortion at the high-rate regime, at the expense of increas-
ing the distortion in the low-rate regime (or equivalently, increasing the rate required to adequately
approximate the data).
We believe the performance drop of VAEs in the low-rate regime is symptomatic of the “holes
problem” (Rezende & Viola, 2018; Makhzani et al., 2015) in the code space of VAEs with large
code size: because these VAEs allocate a large fraction of their latent spaces to garbage images, it
requires many bits to get close to the image manifold. Interestingly, this trade-off could also help
explain the well-known problem of blurry samples from VAEs: in order to avoid garbage samples
(corresponding to large distortion in the low-rate regime), one needs to reduce the capacity, thereby
increasing the distortion at the high-rate regime. By contrast, GANs do not suffer from this tradeoff,
and one can train high-capacity GANs without sacrificing performance in the low-rate regime.
Rate Distortion Curves of AAEs. The AAE was introduced by Makhzani et al. (2015) to address
the holes problem of VAEs, by directly matching the aggregated posterior to the prior in addition to
optimizing the reconstruction cost. Fig. 4 shows the RD curves of AAEs. In comparison to GANs,
AAEs can match the low-rate performane of GANs, but achieve a better high-rate performance.
This is expected as AAEs directly optimize the reconstruction cost as part of their objective. In
comparison to VAEs, AAEs perform slightly worse at the high-rate regime, which is expected as
the adversarial regularization of AAEs is stronger than the KL regularization of VAEs. But AAEs
perform slightly better in the low-rate regime, as they can alleviate the holes problem to some extent.
6.2	Distinguishing different failure modes in generative modeling
Since log-likelihoods constitute only a scalar value, they are unable to distinguish different aspects
of a generative model which could be good or bad, such as the prior or the observation model.
Here, we show that two manipulations which damage a trained VAE in different ways result in very
different behavior of the RD curves.
Our first manipulation, originally proposed by Theis et al. (2015), is to use a mixture of the
VAE’s density and another distribution concentrated away from the data distribution. As pointed
out by Theis et al. (2015), this results in a model which achieves high log-likelihood while gen-
erating poor samples. Specifically, after training the VAE10 on MNIST, we “damage” its prior
p(z) = N(0, I) by altering it to a mixture prior (1 - α)p(z) + αq(z), where q(z) = N(0, 10I) is a
“poor” prior, which is chosen to be far away from the original prior p(z); and α is close to 1. This
process would results in a “poor” generative model that generates garbage samples most of the time
(more precisely with the probability of α). Suppose p(x) and q(x) are the likelihood of the good
and the poor generative models. It is straightforward to see that log q(x) is at most 4.6 nats worse
that log p(x), and thus log-likelihood fails to tell these models apart:
log q (x)
log
0.01p(x) + 0.99
q(z)p(x|z)dz
> log(0.01p(x)) ≈ log p(x) - 4.6
(18)
Fig. 5a plots the rate distortion curves of this model for different values of α. We can see that
the high-rate and log-likelihood performance of the good and poor generative models are almost
identical, whereas in the low-rate regime, the RD curves show significant drop in the performance
and successfully detect this failure mode of log-likelihood.
9
Under review as a conference paper at ICLR 2020
Figure 6: The RD curves of GANs, VAEs and AAEs with MSE distortion on the deep feature
space. The behavior is qualitatively similar to the results for MSE in images (see Fig. 3 and Fig. 4),
suggesting that the RD analysis is not particularly sensitive to the particular choice of metric.
Our second manipulation is to damage the decoder by adding a Gaussian blur kernel after the output
layer. Fig. 5b shows the rate distortion curves for different standard deviations of the Gaussian
kernel. We can see that, in contrast to the mixture prior experiment, the high-rate performance of
the VAE drops due to inability of the decoder to output sharp images. However, we can also see an
improvement in the low-rate performance of the VAE. This is because (similarly to log-likelihoods
with Gaussian observation models) the data distribution does not necessarily achieve the minimal
distortion, and in fact, in the extremely low-rate regime, blurring appears to help by reducing the
average Euclidean distance between low-rate reconstructions and the input images. Hence, our two
manipulations result in very different effects to the RD curves, suggesting that these curves provide
a much richer picture of the performance of generative models, compared to scalar log-likelihoods.
6.3	Beyond Pixelwise Mean S quared Error
The experiments discussed above all used pixelwise MSE as the distortion metric. However, for
natural images, one could use more perceptually valid distortion metrics such as SSIM (Wang et al.,
2004), MSSIM (Wang et al., 2003), or distances between deep features of a CNN (Johnson et al.,
2016). Fig. 6 shows the RD curves of GANs, VAEs, and AAEs on the MNIST dataset, using the
MSE on the deep features of a CNN as distortion metric. In all cases, the qualitative behavior of the
RD curves with this distortion metric closely matches the qualitative behaviors for pixelwise MSE.
We can see from Fig. 6a that similar to the RD curves with MSE distortion, GANs with different
depths and code sizes have the same low-rate performance, but as the model gets deeper and wider,
the RD curves are pushed down and extended to the left. Similarly, we can see from Fig. 6b that
compared to GANs and AAEs, VAEs generally have a better high-rate performance, but worse low-
rate performance. The fact that the qualitative behaviors of RD curves with this metric closely
match those of pixelwise MSE indicates that the results of our analysis are not overly sensitive to
the particular choice of distortion metric.
7 Conclusion
In this work, we studied rate distortion approximations for evaluating different generative models
such as VAEs, GANs and AAEs. We showed that rate distortion curves provide more insights about
the model than the log-likelihood alone while requiring roughly the same computational cost. For
instance, we observed that while VAEs with larger code size can generally achieve better lossless
compression rates, their performances drop at lossy compression in the low-rate regime. Conversely,
expanding the capacity of GANs appears to bring substantial reductions in distortion at the high-rate
regime without any corresponding deterioration in quality in the low-rate regime. This may help
explain the success of large GAN architectures (Brock et al., 2019; Karras et al., 2018a;b). We
also discovered that increasing the capacity of GANs by increasing the code size (width) has a very
different effect than increasing the depth. The former extends the rate distortion curves leftwards,
while the latter pushes the curves down. We also found that different GAN variants with the same
code size has almost similar rate distortion curves, and that the code size dominates the algorithmic
differences of GANs. Overall, lossy compression yields a richer and more complete picture of
the distribution modeling performance of generative models. The ability to quantitatively measure
performance tradeoffs should lead to algorithmic insights which can improve these models.
10
Under review as a conference paper at ICLR 2020
References
Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A Saurous, and Kevin Murphy. Fixing
a broken ELBO. In International Conference on Machine Learning, pp. 159-168, 2018.
Alexander A Alemi and Ian Fischer. GILBO: one metric to measure them all. In Advances in Neural
Information Processing Systems, pp. 7037-7046, 2018.
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016.
Suguru Arimoto. An algorithm for computing the capacity of arbitrary discrete memoryless chan-
nels. IEEE Transactions on Information Theory, 18(1):14-20, 1972.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein GAN. arXiv preprint
arXiv:1701.07875, 2017.
Johannes Balle, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational
image compression with a scale hyperprior. arXiv preprint arXiv:1802.01436, 2018.
Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973,
2018.
Yochai Blau and Tomer Michaeli. Rethinking lossy compression: The rate-distortion-perception
tradeoff. arXiv preprint arXiv:1901.07821, 2019.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. In International Conference on Learning Representations, 2019.
Yanshuai Cao, Gavin Weiguang Ding, Kry Yik-Chau Lui, and Ruitong Huang. Improving GAN
training via binarized representation entropy (bre) regularization. ICLR, 2018. accepted as poster.
Ivo Danihelka, Balaji Lakshminarayanan, Benigno Uria, Daan Wierstra, and Peter Dayan. Com-
parison of maximum likelihood and GAN-based training of real NVPs. arXiv preprint
arXiv:1705.05263, 2017.
Justin Domke and Daniel R Sheldon. Importance weighting and variational inference. In Advances
in neural information processing systems, pp. 4470-4479, 2018.
Brendan J Frey and Geoffrey E Hinton. Free energy coding. In Proceedings of Data Compression
Conference-DCC’96, pp. 73-81. IEEE, 1996.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Roger B Grosse, Zoubin Ghahramani, and Ryan P Adams. Sandwiching the marginal likelihood
using bidirectional Monte Carlo. arXiv preprint arXiv:1511.02543, 2015.
Aditya Grover, Manik Dhar, and Stefano Ermon. Flow-GAN: Combining maximum likelihood
and adversarial learning in generative models. In Thirty-Second AAAI Conference on Artificial
Intelligence, 2018.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein GANs. In Advances in Neural Information Processing Systems,
pp. 5767-5777, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems 30, pp. 6626-6637. Curran Associates, Inc.,
2017.
11
Under review as a conference paper at ICLR 2020
Geoffrey Hinton and Drew Van Camp. Keeping neural networks simple by minimizing the descrip-
tion length of the weights. In in Proc. of the 6th Ann. ACM Conf. on Computational Learning
Theory. Citeseer, 1993.
Gao Huang, Yang Yuan, Qiantong Xu, Chuan Guo, Yu Sun, Felix Wu, and Kilian Weinberger. An
empirical study on evaluation metrics of generative adversarial networks, 2018.
Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and
super-resolution. In European conference on computer vision, pp. 694-711. Springer, 2016.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for im-
proved quality, stability, and variation. In International Conference on Learning Representations,
2018a.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. arXiv preprint arXiv:1812.04948, 2018b.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.	CoRR,
abs/1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint
arXiv:1312.6114, 2013.
Friso H Kingma, Pieter Abbeel, and Jonathan Ho. Bit-swap: Recursive bits-back coding for lossless
compression with hierarchical latent variables. arXiv preprint arXiv:1905.06845, 2019.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Luis A Lastras. Information theoretic lower bounds on negative log likelihood. arXiv preprint
arXiv:1904.06395, 2019.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are GANs
created equal? a large-scale study. In Advances in neural information processing systems, pp.
700-709, 2018.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. arXiv preprint arXiv:1511.05644, 2015.
L Metz, B Poole, D Pfau, and J Sohl-Dickstein. Unrolled generative adversarial networks. arXiv
preprint arXiv:1611.02163, 2018.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Radford M Neal. Annealed importance sampling. Statistics and computing, 11(2):125-139, 2001.
Radford M Neal. Estimating ratios of normalizing constants using linked importance sampling.
arXiv preprint math/0511216, 2005.
Radford M Neal et al. MCMC using Hamiltonian dynamics. Handbook of markov chain Monte
Carlo, 2(11):2, 2011.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Danilo Jimenez Rezende and Fabio Viola. Taming VAEs. arXiv preprint arXiv:1810.00597, 2018.
Mehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assess-
ing generative models via precision and recall. In Advances in Neural Information Processing
Systems, pp. 5228-5237, 2018.
12
Under review as a conference paper at ICLR 2020
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems,
pp. 2234-2242, 2016.
Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving GANs using optimal
transport. In International Conference on Learning Representations, 2018.
Lucas Theis, Aaron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models. arXiv preprint arXiv:1511.01844, 2015.
Lucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc HUszar Lossy image compression with
compressive autoencoders. arXiv preprint arXiv:1703.00395, 2017.
James Townsend, Tom Bird, and David Barber. Practical lossless compression with latent variables
using bits back coding. arXiv preprint arXiv:1901.04866, 2019.
Chris S Wallace. Classification by minimum-message-length inference. In International Conference
on Computing and Information, pp. 72-81. Springer, 1990.
Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality
assessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003,
volume 2, pp. 1398-1402. Ieee, 2003.
Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli, et al. Image quality assessment:
from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600-
612, 2004.
Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the quantitative analysis of
decoder-based generative models. arXiv preprint arXiv:1611.04273, 2016.
13
Under review as a conference paper at ICLR 2020
Appendix A	Related works
Practical Compression Schemes. We have justified our use of compression terminology in terms
of Shannon’s fundamental result implying that there exist a rate distortion code for any rate dis-
tortion pair that is achievable according to the rate distortion function. Interestingly, for lossless
compression with generative models, there is a practical compression scheme which nearly achieves
the theoretical rate (i.e. the negative ELBO): bits-back encoding. The basic scheme was proposed
by Wallace (1990); Hinton & Van Camp (1993), and later implemented by Frey & Hinton (1996).
Practical versions for modern deep generative models were developed by Townsend et al. (2019);
Kingma et al. (2019). We do not currently know of an analogous practical scheme for lossy com-
pression with deep generative models. Other researchers have developed practical coding schemes
achieving variational rate distortion bounds for particular latent variable models which exploited the
factorial structure of the variational posterior (Bane et al., 2018; Theis et al., 2017). These meth-
ods are not directly applicable in our setting, since we don’t assume an explicit encoder network,
and our variational posteriors lack a convenient factorized form. We don’t know whether our varia-
tional approximation will lead to a practical lossy compression scheme, but the successes for other
variational methods give us hope.
Relationship with the Rate-Distortion-Perception Tradeoff. Our work is related to Blau &
Michaeli (2019) which incorporates a perceptual quality loss function in the rate-distortion frame-
work and characterizes the triple tradeoff between rate distortion and perception. More specifically,
Blau & Michaeli (2019) defines the perceptual loss using a divergence between the marginal re-
construction distribution and the data distribution. This perceptual loss is then incorporated as an
additional constraint in the rate-distortion framework to encourage the reconstruction distribution to
perceptually look like the data distribution. It is shown that as the perceptual constraint becomes
tighter, the rate-distortion function elevates more. In our rate-prior distortion framework, we are
also enforcing a perceptual constraint on the reconstruction distribution by incorporating the regu-
larization term of KL(q(z)kp(z)) in the rate-distortion objective, which encourages matching the
aggregated posterior to the prior (Makhzani et al., 2015). More precisely, let us define the recon-
struction distribution r(x) as the the distribution obtained by passing the data distribution through
the encoder and then the decoder:
r(x, z) = q(z)p(x|z),
X 〜r(x) =	r(x, z)dz.
z
(19)
It can be shown that the regularization term KL(q(z)kp(z)) upper bounds KL(r(x)kp(x)):
KL(q(z)kp(z)) = KL(r(x, z))kp(x, z)) ≥ KL(r(x)kp(x)).	(20)
In other words, in the rate-prior distortion optimization, for a given distortion constraint, we are
not only interested in minimizing the rate I(x; z), but also at the same time, we are interested
in preserving the perceptual quality of the reconstruction distribution by matching it to the model
distribution. In the low-rate regime, when the model is allowed to have large distortions, the model
obtains small rates and at the same time preserves the perceptual distribution of the reconstruction
samples. As the distortion constraint becomes tighter, the model starts to trade off the rate I(x; z)
and the perceptual quality KL(q(z)kp(z)), which results in an elevated rate distortion curve.
Relationship with the Precision-Recall Tradeoff. One of the main drawbacks of the IS or FID is
that they can only provide a single scalar value that cannot distinguish the mode dropping behav-
ior from the mode inventing behavior (generating outlier or garbage samples) in generative mod-
els. In order to address this issue, Lucic et al. (2018); Sajjadi et al. (2018) propose to study the
precision-recall tradoff for evaluating generative models. In this context, high precision implies that
the samples from the model distribution are close to the data distribution, and high recall implies
the generative model can reconstruct (or generate a sample similar to) any sample from the data
distribution. The precision-recall curves enable us to identify both the mode dropping and the mode
inventing behavior of the generative model. More specifically, mode dropping drops the precision
of the model at the high-recall regime, and mode inventing drops the precision of the model in the
low-recall regime. Our rate-prior distortion framework can be thought as the information theoretic
analogue of the precision-recall curves, which extends the scalar notion of log-likelihood to rate
distortion curves. More specifically, in our framework, mode dropping drops the distortion perfor-
mance of the model at the high-rate regime, and mode inventing drops the distortion performance
of the model at the low-rate regime. In Section 6, we will empirically study the effect of mode
dropping and mode inventing on our rate-prior distortion curves.
14
Under review as a conference paper at ICLR 2020
Appendix B	AIS Validation Experiments
AIS Settings. All the AIS settings including the HMC parameters are provided in Appendix D.2.
AIS Validation. We conducted several experiments to
validate the correctness of our implementation and the
accuracy of the AIS estimates. Firstly, we compared our
AIS results with the analytical solution of rate-prior dis-
tortion curve on a linear VAE (derived in Appendix D.3.1)
trained on MNIST. As shown in Fig. 7, the RD curve esti-
mated by AIS agrees closely with the analytical solution.
Secondly, for the main experiments of the paper, we eval-
uated the tightness of the AIS estimate by computing the
BDMC gap. The largest BDMC gap for VAEs and AAEs
was 0.127 nats, and the largest BDMC gap for GANs was
1.649 nats, showing that our AIS upper bounds are tight.
More details are provided in Appendix D.3.2.
Figure 7: Analytical vs. AIS rate distortion
curves for a linear VAE.
Appendix C	Proofs
C.1 Proof of Prop. 1.
Proof of Prop. 1a. As D increases, Rp (D) is minimized over a larger set, so Rp(D) is non-
increasing function of D.
The distortion Eq(x,z) [d(x, f (z))] is a linear function of the channel conditional distribution q(z|x).
The mutual information is a convex function of q(z|x). The KL(q(z)kp(z)) is also convex
function of q(z), which itself is a linear function of q(z|x). Thus the rate-prior objective is
a convex function of q(z|x). Suppose for the distortions D1 and D2, q1(z|x) and q2(z|x)
achieve the optimal rates in Eq. 5 respectively. Suppose the conditional qλ(z∣x) is defined as
qλ(z∣x) = λqι(z∣x) + (1 - λ)q2(z∣x). The rate-prior objective that the conditional qλ(z∣x)
achieves is Iλ(z; x) + KL(qλ(z)kp(z)), and the distortion Dλ that this conditional achieves is
Dλ = λD1 + (1 - λ)D2. Now we have
Rp(Dλ) ≤ Iλ(z;x) +KL(qλ(z)kp(z))	(21)
≤ λI1(z; x) + λKL(qι(z)kp(z)) + (1 - λ)I2(z; x) + (1 - λ)KL(q2(z)∣∣p(z))
= λRp(D1) + (1 - λ)Rp(D2)
which proves the convexity of Rp(D).
Alternative Proof of Prop. 1a. We know the rate-prior term Epd(x)KL(q(z|x)kp(z)) is a convex
function of q(z|x), and Eq(x,z) [d(x, f (z))] is a linear and thus convex function of q(z|x). As the
result, the following optimization problem is a convex optimization problem.
min Epd(x)KL(q(z|x)kp(z))	s.t. Eq(x,z)[d(x, f (z))] ≤ 0.	(22)
q(z|x)
The rate distortion function Rp(D) is the perturbation function of the convex optimization problem
of Eq. 22. The convexity of Rp(D) follows from the fact that the perturbation function of any
convex optimization problem is a convex function (Boyd & Vandenberghe, 2004).
Proof of Prop. 1b. We have
mzn Rp(D)=mzn “包㈤：Emxn ⑸心DI(X; z)+KL(q(z)kp(z))
min
q(z|x):E[d(x,f (z))]≤D
min I(x; z) + KL(q(z)kp(z))
p(z)
= min	I(x; z)
q(z|x):E[d(x,f (z))]≤D
= R(D).
where in Eq. 24, we have used the fact that for any function f(x, y), we have
min min f(x, y) = min min f(x, y) = min f(x, y),
x y	y x	x,y
(23)
(24)
(25)
(26)
(27)
15
Under review as a conference paper at ICLR 2020
and in Eq. 25, we have used the fact that KL(q(z)kp(z)) is minimized when p(z) = q(z).
Proof of Prop. 1c. In Prop. 1a, we showed that the rate-prior term is a convex function of q(z|x),
and that the distortion is a linear function of q(z|x). So the summation of them in Eq. 9 will be a
convex function of q(z|x). The unique global optimum of this convex optimization can be found by
rewriting Eq. 9 as
KL(q(z∣x) kp(z)) + βEq(z∣χ) [d(x, f (z))] = KL(q(z∣x) k -^1-τp(z) exp(-βd(x, f (z)))) - log Ze (x)
Z(x)
(28)
where Zβ (x) = p(z) exp(-β d(x, f (z)))dz. The minimum of Eq. 28 is obtained when the KL di-
vergence is zero. Thus the optimal channel conditional is q%(z|x) = ZIx)P(Z) exp(-βd(x, f (z))).
C.2 Proof of Prop. 2.
Proof of Prop. 2a. R(D) ≤ Rp(D) was proved in Prop. 1b. To prove the first inequality, note that
the summation of rate and distortion is
Rp(D) + D = I(z; x) + Eq*(χ,z)[-logp(x∣z)] = Hd + Eq*(z)KL(q*(x|z)||p(x|z)) ≥ Hd.
(29)
where q* (x, Z) is the optimal joint channel conditional, and q*(z) and q*(x∣z) are its marginal
and conditional. The equality happens if there is a joint distribution q(x, z), whose conditional
q(x|Z) = p(x|Z), and whose marginal over x is pd(x). But note that such a joint distribution might
not exist for an arbitrary p(x|Z).
Proof of Prop. 2b. The proof can be easily obtained by using d(x, f (Z)) = - log p(x|Z) in
Prop. 1c.
Proof of Prop. 2c. Based on Prop. 2b, at β = 1, we have
Zβ*(x) =	p(Z)p(x|Z)dZ = p(x).	(30)
C.3 Proof of Prop. 3.
The set of pairs of RkAIS (x), DkAIS (x) are achievable rate-prior distortion pairs (achieved by
qkAIS(Z|x)). Thus, by the definition of Rp(D), RpAIS(D) falls in the achievable region of Rp(D)
and, thus maintains an upper bound on it: RpAIS (D) ≥ Rp (D).
C.4 Proof of Prop. 4.
AIS has the property that for any step k of the algorithm, the set of chains up to step k, and the
partial computation of their weights, can be viewed as the result of a complete run of AIS with
target distribution qk*(Z|x). Hence, we assume without loss of generality that we are looking at a
complete run of AIS (but our analysis applies to the intermediate distributions as well).
Let qkAIS(Z|x) denote the distribution of final samples produced by AIS. More precisely, it is a dis-
tribution encoded by the following procedure:
1. For each data point x, we run M independent AIS chains, numbered i = 1, . . . , M. Let
Z0ik denotes the k-th state of the i-th chain. The joint distribution of the forward pass up to
the k-th state is denoted by qf(Z0i1, . . . , Z0ik|x). The un-normalized joint distribution of the
backward pass is denoted by
花(z01,..., z0k |x) = P(ZOk) exp(-βk d(x,f (z0k))) qb(z01,..., z0k-i∣z0k, x).	(31)
2.
Compute the importance weights and normalized importance weights of each chain using
wik =%(z01,…,z0k 叫 and Wi
k	qf (z0 i1,...,z0 ik|x)	k
Wk
PM W
3.	Select a chain index S with probability of Wk.
16
Under review as a conference paper at ICLR 2020
4.	Assign the selected chain values to (z11, . . . , z1k):
(z11, . . . , z1k) = (z01 , . . . , z0k ).	(32)
5.	Keep the unselected chain values and re-label them as (z12:M, . . . , z2k:M):
(z12:M,...,z2k:M)=(z01-S,...,z0k-S).	(33)
where -S denotes the set of all indices except the selected index S.
6.	Return z = z1k .
More formally, the AIS distribution is
M
qAIS(ZIX)= EQM=I qf (z01,...,z0k∣x)[X Wk δ(Z- z0k 4	(34)
=1
Using the AIS distribution qkAIS(Z|X) defined as above, we define the AIS distortion DkAIS(X) and the
AIS rate-prior RkAIS(X) = KL(qkAIS(Z|X)kp(Z)) as follows:
DkAIS(X) =EqkAIS(z|x)[d(X,f(Z))]	(35)
RkAIS(X) = KL(qkAIS(Z|X)kp(Z)).	(36)
In order to estimate RkAIS (X) and DkAIS (X), we define
M
DD AIS(X) = X Wk d(x,f(z0k)),	(37)
i=1
1M
ZAIS(X) = MM X Wk,	(38)
i=1
RAIS(X) = - log ZAIS(X)- βkDDAIS (x).	(39)
We would like to prove that
EQM=I qf(z01,…,z0k∣χ)[DDAIS(X)I = DAIS(X)，	(40)
EQM=I qf(z，1，..,z，k|x)[RAIS(X)] ≥ RAIS(X).	(41)
The proof of Eq. 40 is straightforward:
DkAIS(X) = EqkAIS(z|x)[d(X,f(z))],
=	qkAIS(z|X)d(X,f(z))dz,
M
=J EQM=I qf(z01,…,z0k |x)[X Wkδ(z - Z0k)]d(X, f(z))dz,
i=1
M
=EQM=I qf (z01,...,z0k∣x) X Wik [ / δ(z - z0k )d(X,f (Z))dz∣,
=1
(42)
M
EQM=I qf (z01,…,z0k∣χ) X Wkd(X，f (ZOk))，
i=1
EQM=I qf (z0ι,...,z0k∣χ)[力AIS(X)].
Eq. 42 shows that DDAIS (x) is an unbiased estimate of DAIS (x) . We also know log ZAIS (x) obtained
by Eq. 38 is the estimate of the log partition function, and by the Jenson’s inequality lower bounds
in expectation the true log partition function: E[log ZAIS(x)] ≤ log Zk(X). After obtaining DAIS (x)
17
Under review as a conference paper at ICLR 2020
and logZAIS(X), We use Eq. 39 to obtain RAIS(X). Now, it remains to prove Eq. 41, which states
that RAIS (x) upper bounds the AIS rate term RAIS (x) in expectation.
Let qkAIS (z11:M, . . . , z1k:M |X) denote the joint AIS distribution over all states of {z11:M, . . . , z1k:M},
defined in Eq. 32 and Eq. 33. It can be shown that (see Domke & Sheldon (2018))
qkAIS(z11:M,...,z1k:M|x)
qb(z1,...,zk |x) QM=2 qf(z1,...,zk |x)
ZAIS(X)
(43)
p(zk )exp(-βk d(x,f(zk))) qb(z1,...,zk-ι∣zk, x) QM=2 qf(z1,...,zk |x)
ZAIS(X)	.
(44)
In order to simplify notation, suppose z1k
{z11:M, . . . , z1k:-M1 , z2k:M } are denoted by V.
qkAIS(z, V|X) as follows:
is denoted by z, and all the other variables
Using this notation, we define p(V|z, X) and
M
p(V|z, X) := qb(z11,. . . , z1k-1|z1k, X)	qf(zi1, . . . ,zik|X),
i=2
qkAIS(z,V|X) := qkAIS(z11:M,...,z1k:M|X)
Using the above notation, Eq. 44 can be re-written as
ZAIS(X)
p(z) exp(-βk d(X, f (z))) p(V|z, X)
qAIS(Z, VIX)
Hence,
E[log ZAIS(x)] = E[logp(z) - log qAIS(z, V|x) + logp(V∣x,z)] - βkE[d(x,f (z))]
= -KL(qkAIS(Z, V|X)kp(Z)p(V|Z, X)) - βkE[d(X, f (Z))]
≤ -KL(qkAIS(z|x)kp(z)) - βkE[d(x, f(z))],
(45)
(46)
(47)
(48)
where the inequality follows from the monotonicity of KL divergence. Rearranging terms, we bound
the rate:
RAIS(X) = KL(qAIS(z∣x)kp(z)) ≤ -E[logZAIS(x)] - βkE[d(x,f(z))]= E[RAIS(x)].	(49)
Eq. 49 shows that RAIS (x) upper bounds the AIS rate-prior RAIS (x) in expectation. We also showed
DDAIS (x) is an unbiased estimate of the AIS distortion DAIS(X). Hence, the estimated AIS rate-prior
curve upper bounds the AIS rate-prior distortion curve in expectation: E[RAIS(D)] ≥ RAIS(D).
Appendix D Experimental Details
The code for reproducing all the experiments of this paper will be open sourced publicly.
D. 1 Datasets and Models
We used MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky & Hinton, 2009) datasets in our
experiments.
Real-Valued MNIST. For the VAE experiments on the real-valued MNIST dataset (Fig. 4a), we
used the “VAE-50” architecture described in (Wu et al., 2016), and only changed the code size in our
experiments. The decoder variance is a global parameter learned during the training. The network
was trained for 1000 epochs with the learning rate of 0.0001 using the Adam optimizer (Kingma &
Ba, 2014).
For the GAN experiments on MNIST (Fig. 3a), we used the “GAN-50” architecture described in (Wu
et al., 2016). In order to stabilize the training dynamic, we used the gradient penalty (GP) (Salimans
et al., 2016). In our deep architectures, we used code sizes of d ∈ {2, 5, 10, 100} and three hidden
18
Under review as a conference paper at ICLR 2020
VAElO Preliminary and Loaded Runs
Figure 8: The rate-prior distortion curves obtained by adaptively tuning the HMC parameters in the
preliminary run, and pre-loading the HMC parameters in the second formal run. ”rs” in the legend
indicates the random seed used in the second run.
layers each having 1024 hidden units to obtain the following GAN models: Deep-GAN2, Deep-
GAN5, Deep-GAN10 and Deep-GAN100. The shallow GANs architectures are similar to the deep
architectures but with one layer of hidden units.
CIFAR-10. For the CIFAR-10 experiments (Fig. 3b), we experimented with different GAN mod-
els such as DCGAN (Radford et al., 2015), DCGAN with Gradient Penalty (GP-GAN) (Gulrajani
et al., 2017), Spectral Normalization (SN-GAN) (Miyato et al., 2018), and DCGAN with Binarized
Representation Entropy Regularization (BRE-GAN) (Cao et al., 2018). The numbers at the end of
each GAN name in Fig. 3b indicate the code size.
D.2 AIS settings for RD curves
For each RD curve, there are 1999 points computed with only one AIS chain, for 999 β0 s spaced
linearly from βmax to 1 and another 999 β0s spaced linearly from 1 to βmin, and plus β = 1, thus
1999 points. βmi∩ = = for all models. βmaχ = 0 0^ ≈ 3333 for 100 dimensional models such
as GAN100, VAE100 or AAE 100, and βmax = 0 0000I770224 ≈ 36098 for the rest (2, 5 and 10
dimensional models). The AIS temperature schedu.le (t for intermediate distributions) is generated
by three steps:
1.	Build a sigmoid temperature schedule used in Wu et al. (2016) for βmax with N intermediate
distributions.
2.	If there are fewer than 800 intermediate distributions before the first RD point in the temper-
ature schedule initialized in Step 1, overwrite this part of the schedule with 800 intermediate
distributions linearly spaced between β0 = 0 the first RD point (β1 ).
3.	If there are fewer than 10 intermediate distributions between two RD points in the temper-
ature schedule initialized in Step 1, overwrite this part of the schedule with 10 intermediate
distributions linearly spaced between two RD points.
For the 2, 5 and 10 dimensional models, N = 40000, and the above procedure will result in 60292
intemediate distributions in total. For 100 dimensional models, to ensure accuracy of our AIS es-
timator with a small BDMC gap, we used N = 1600000 and the above procedure will result in
1611463 intermediate distributions in total. We used 20 leap frog steps for HMC, 40 independent
chains, on a single batch of50 images. On MNIST, we also tested with a larger batch of 500 MNIST
images, but did not observe significant difference compared with a batch 50 images, thus we did all
of our experiments with a single batch 50 images. On a P100 GPU, for MNIST, it takes 4-7 hours
to compute an RD curve for 60292 intermediate distributions and takes around 7 days for 1611463
intermediate distributions. For all of the CIFAR experiments, we used the schedule with 60292
intermediate distributions, and each experiment takes about 7 days to complete.
Adaptive Tuning of HMC Parameters. While running the AIS chain, the parameters of the HMC
kernel cannot be adaptively tuned, since it would violate the Markovian property of the chain. So in
19
Under review as a conference paper at ICLR 2020
order to be able to adaptively tune HMC parameters such as the number of leapfrog steps and the step
size, in all our experiments, we first do a preliminary run where the HMC parameters are adaptively
tuned to yield an average acceptance probability of 65% as suggested in Neal (2001). Then in the
second “formal” run, we pre-load and fix the HMC parameters found in the preliminary run, and
start the chain with a new random seed to obtain our final results. Interestingly, we observed that
the difference in the RD curves obtained from the preliminary run and the formal runs with various
different random seeds is very small, as shown in Fig. 8. This figure shows that the AIS with the
HMC kernel is robust against different choices of random seeds for approximating the RD curve of
VAE10.
D.3 Validation of AIS experiments
We conducted several experiments to validate the correctness of our implementation and the accu-
racy of the AIS estimates.
D.3.1 Analytical Solution of the Rate-Prior Distortion Optimization on the
Linear VAE
We compared our AIS results with the analytical solution of the rate-prior distortion optimization
on a linear VAE trained on MNIST as shown in Fig. 7.
In order to derive the analytical solution, We first find the optimal distribution q& (z|x) from Prop. 2b.
For simplicity, we assume a fixed identity covariance matrix I at the output of the conditional likeli-
hood of the linear VAE decoder. In other Words, the decoder of the VAE is simply: x = Wz+b+,
Where x is the observation, z is the latent code vector, W is the decoder Weight matrix and b is the
bias. The observation noise of the decoder is e 〜N(0, I). It's easy to show that the conditional
likelihood raised to a power β is: p(x∣z)β = N(x|Wz + b, JI). Then, q%(z|x) = N(z∣μβ, ∑β),
where
μβ = Eq*(z|x [z]) = W|(WW| + β-1 I)T(X - b)
∑β = Covq*(z|x [z]) = I - W|(WW| + β-1I)TW
For numerical stability, we can further simplify thw above by taking the SVD of W : let W
UDV|, and then apply the Woodbury Matrix Identity to the matrix inversion, we can get:
μβ = VRe UT(X — b)
∑β = VSe V|
(51)
(52)
Where Re is a diagonal matrix with the ith diagonal entry being 壮广；ι and Se is a diagonal matrix
with the ith diagonal entry being [^1^], where di is the ith diagonal entry of D
Then, the analytical solution for optimum rate is:
DκL(qβ(z∣x)∣∣p(z)) = Dkl(N(z∣μe, ∑e)∣∣N(z∣0,I))	(53)
=2 (tr(£e) + (-μe)T(-μe) - k + ln ((det 4)-1))	(54)
=2 (tr(£e) + (μe)T(μe) - k - ln (det £e))	(55)
Where k is the dimension of the latent code z. With negative log-likelihood as the distortion metric,
the analytically form of distortion term is:
Eq* (z|x) [— logP(X |z)]
Z∞1
-log((2π) k/2 exp { - 2(x - (WZ + b))l(x -(WZ + b))})q万(z∣xdz
1∞
=-(log((2π)-k∕2) + -y {(x - (Wz + b))l(x - (Wz + b))}q；(z∣xdz)
k1	1
=2 log(2π) + 2(x - b)|(x - b) - (W〃e)|(x - b) + 2Eq*(z∣χ)[(WZ)T(Wz)]
(56)
(57)
(58)
(59)
20
Under review as a conference paper at ICLR 2020
where Eq以红皮)[(Wz)| (Wz)] can be obtained by change of variable: Let y = Wz, then:
Eq*(y)[y]= Wμβ = U(I- Se )U|(X - b)
Covq*(y) [y] = W∑βW| = UDReDU|
(60)
(61)
Eq*(z∣x) [(Wz)T(Wz)] = Eq*(y) [y|y] = Eq*(y) [y]| Eq*(y) [y] + tr(CoVq*(y)[y])	(62)
(63)
D.3.2 The BDMC Gap
We evaluated the tightness of the AIS estimate by computing the BDMC gaps using the same AIS
settings. Fig. 9, shows the BDMC gaps at diffrent compression rates for the VAE, GAN and AAE
experiments on the MNIST dataset. The largest BDMC gap for VAEs and AAEs is 0.127 nats, and
the largest BDMC gap for GANs is 1.649 nats, showing that our AIS upper bounds are tight.
D.4 High-Rate vs. Low-Rate Reconstructions
In this section, we visualize the high-rate (β ≈ 3500) and low-rate (β = 0) reconstructions of the
MNIST images for VAEs, GANs and AAEs with different hidden code sizes. The qualitative results
are shown in Fig. 10 and Fig. 11, which is in consistent with the quantitative results presented in
experiment section of the paper.
21
Under review as a conference paper at ICLR 2020
Distortion (Squared Error)
(a)
BDMCGapsfor GANs
(b)
(c)
Figure 9: The BDMC gaps annotated on estimated AIS Rate-Prior Distortion curves of (a) VAEs,
(b) GANs, and (c) AAEs.
22
Under review as a conference paper at ICLR 2020
OO .，，fc，34
636CO夕 Rq
/3UOJJ/?
S*7S e- 2 / HI
2。NΛ∕2,2G
3 Γ Ur «A G ) ʃ 7
qαNO32c~7
O夕s-≡>∖γ*32
(d) Low Rate GAN2
zpg∕6 7∕λ^
ɔ 5 / 7⅜4 0c
3>7,ΓH373
Γ-J~l3>5∙a 7<
Cnτ∕osA<"¾q
0*46。』34r
。。口一，4 703
〃5/47/t/5
(g) Low Rate GAN10
SIS/® Zv
O &∙7g。，夕
丁
7
q
3
n R
X 4
4 C
5 «
'3
7 6
g /
5 1
1 b
0 4
j7 G
? 5
q ʃ
7 1
W夕
(j) Low Rate GAN100
K5,∖∕2/O
，JS 3。\"彳
/。G ∖l U 5 O^
q彳夕07，/。
Ot 442Λ乙夕
∕o¾ n∕7w-7<
乙夕7。“7qκ
7W943—31
(a) Original MNIST test images
Z73 0/3 ,a
O * 04 r∙3~43
3g43F∕qo
3〃2〃r“3y
72 7AZ6SX
y566 0g∕o
o½qo∕%0 2
0q938367
(c) Low Rate AAE2
(f) Low Rate AAE10
(i) Low Rate AAE100
5<Γ 夕 06□-∙x
Ooʃq - 3r7 J
4 g J A
b q 5 5
I ∙Γ Q O
J ʃ J 3
SR 7 6
5 / S 5
，∖ q ?
q J A 5
(b) Low Rate VAE2
Λ17yo<3 23
9qAJOd27
oα3c>3Gq>
，,b qo5Q3
63÷3 4 9ga
3A8qJ5gq
34cg,719
KZCC 446 30
(e) Low Rate VAE10
(h) Low Rate VAE100
Figure 10: Low-rate reconstructions (β = 0) of VAEs, GANs and AAEs on MNIST.
23
Under review as a conference paper at ICLR 2020
cΓ 5夕 \ /ʃ/。
V133 3 ∖ q1
/061 I S0IO
q 7夕。7/^/0
Ob3 42X/夕
/。Sr- 7 V 7 G
277。Q 74 S
7 Oc √ 5 I U- 7
(d) High Rate GAN2
A.5s∖l 2/。
v-s 3L∖q 夕
∕∙ O G ∖tl H 3 O。
q 夕夕。73/。
0 044-24 乙夕
∕o¾γ*747 4
乙夕70。7q1Γ
7⅛r9q31yy*
(g) High Rate GAN10
K59∖la/O
/r O G »1— I 5 Ofo
497073/。
0 6 — q2Λ乙 CK
∕o¾ n∕7ur7<
乙 m70z√7qQ
7<9Q5)⅛7
K5f∖∕a/O
ZJe 3〃\qg
∕∙ O G M 1« 5。
qg，o73/。
。匕4q2Λ乙?
∕o⅛ n∕7ur7√
乙 m70z√7qκ
7√9431∙γ
(a) Original MNIST test images.
Q∙SC-∖I^F∕O
ylG33∖q 夕
/06 115 Ob
q0夕073/。
Ooqq2 23 夕
∕o% T7uf7c
Z夕7。"7qq
7Qqq3l3y
(c) High Rate AAE2
久s5∖la / O
71-3 32∖q3
/ O G tl∙ H 5 Ob
497073/。
Obqq2 /乙7
∕o¾r∙747G
乙77oα 7qQ
7G943}∙7
(f) High Rate AAE10
A∖ / O
7Ie 3〃\q9
∕∙ O /O ∖l H Ofo
q夕夕O73/。
Osqq2Λ乙 0<
/ O %「7 彳 7<
乙 0‹7o々 7qκ
as5∖l√/O
Urlu Λ3∖u∙9
/061I5OG
qg7o 7 70 / O
Ob yq<2a 乙夕
∕O⅞>Γ∙7ZT7S
乙夕70。7q<Γ
7 4 of u∙ ʒ I Zry
(b) High Rate VAE2
A 551 12 Z O
7JS 3z∖q 夕
/ G H l∙ 5 O^
40夕。73/。
Os:Tq2/乙7
∕o% n<7w∙7<
乙77 0"7q<<
7C9M¾)WY
(e) High Rate VAE10
K51Λ∖IJ / O
7JQ 3z∖qg
/ O fo M H 5。fo
"97073/。
ofo4q2-A 乙 OK
/ O ¾ ɔ 7 u< 7 -⅛
乙 <K7 04∕7qκ
7ς∙0-q3J 4，
(h) High Rate VAE100	(i) High Rate AAE100	(j) High Rate GAN100
Figure 11: High-rate reconstructions (βmax) of VAEs, GANs and AAEs on MNIST. βmax = 3333 for
100 dimensional models, and βmax = 36098 for the 2 and 10 dimensional models.
24