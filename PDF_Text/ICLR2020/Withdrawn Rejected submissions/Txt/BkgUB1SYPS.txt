Under review as a conference paper at ICLR 2020
Interpretable Network Structure for Model-
ing Contextual Dependency
Anonymous authors
Paper under double-blind review
Ab stract
Neural language models have achieved great success in many NLP tasks, to a large
extent, due to the ability to capture contextual dependencies among terms in a tex-
t. While many efforts have been devoted to empirically explain the connection
between the network hyperparameters and the ability to represent the contextual
dependency, the theoretical analysis is relatively insufficient. Inspired by the re-
cent research on the use of tensor space to explain the neural network architecture,
we explore the interpretable mechanism for neural language models. Specifical-
ly, we define the concept of separation rank in the language modeling process,
in order to theoretically measure the degree of contextual dependencies in a sen-
tence. Then, we show that the lower bound of such a separation rank can reveal
the quantitative relation between the network structure (e.g. depth/width) and the
modeling ability for the contextual dependency. Especially, increasing the depth
of the neural network can be more effective to improve the ability of modeling
contextual dependency. Therefore, it is important to design an adaptive network
to compute the adaptive depth in a task. Inspired by Adaptive Computation Time
(ACT), we design an adaptive recurrent network based on the separation rank
to model contextual dependency. Experiments on various NLP tasks have veri-
fied the proposed theoretical analysis. We also test our adaptive recurrent neural
network in the sentence classification task, and the experiments show that it can
achieve better results than the traditional bidirectional LSTM.
1	Introduction
Language modeling is a fundamental topic in natural language processing (NLP), and neural net-
work based language models have achieved great success in many NLP tasks (Cheng et al., 2016; Ko
et al., 2017; Peters et al., 2018a;b). For instance, ELMo (Peters et al., 2018b), as a contextualized
word representation model, significantly improves the results in a broad range of NLP problems.
One important reason is that the recurrent network and its variants can capture the long-range/short-
range contextual dependencies among terms in a text (Graves, 2013; Yang et al., 2017; Peters et al.,
2018a). Researchers are trying to empirically explain the relation between network hyperparameters
(such as the number of layers) and the ability to represent contextual dependencies (Karpathy et al.,
2015; Peters et al., 2018a). However, the theoretical explanations are relatively insufficient.
Recently, Peters et al. (2018a) investigated the effect of network depth on the range of contextual
dependencies represented in neural language models. Empirical evidences show that the bottom
layer can capture short-range dependencies, while deep layers are more capable to represent long-
range dependencies. It is observed that different NLP tasks require different layers of pre-trained
embeddings in (Peters et al., 2018a). Specifically, for the coreference resolution task requiring global
syntax structures, long-range dependencies among words should be modeled, and thus embeddings
vectors in deep layers are more useful than those in bottom layers. On the other hand, for the named
entity recognition task, it requires relatively local syntax to be modeled, and then the embeddings
in bottom layers can work well (Peters et al., 2018a). However, while the empirical observations
are insightful, it is challenging to theoretically derive the quantitative relation between the neural
network structure and the representation ability for contextual dependency.
Our work is inspired by these studies on the relation between neural network and tensor network (Co-
hen & Shashua, 2016a; Levine et al., 2017). These studies have shown that the correlation between
1
Under review as a conference paper at ICLR 2020
input regions of an image can be calculated by the separation rank. Based on such a measure-
ment and its analysis, the structure of convolutional neural network can be explained in a tensor
space (Cohen & Shashua, 2016a). Levine et al. (2017) further utilize the separation rank to mea-
sure the long-term memory capacity and analyze its relation to the network depth, by mapping the
recurrent neural network to a tensor network.
In language modeling tasks, tensors have been used for text representation and language model-
ing (Liu et al., 2005; Socher et al., 2013; Pennington et al., 2014; Qiu & Huang, 2015). Recently,
the tensor network (as a method to model high-order tensors) has been adopted in language model-
ing, and a tensor space language model (TSLM) has been proposed (Zhang et al., 2019). TSLM was
proved to be a generalization of the n-gram and RNN language models. TSLM bridges the gap be-
tween the tensor network and language modeling, and then paves the way for the further theoretical
analysis of neural language modeling in tensor space.
In this paper, we aim to provide quantitative analyses for explaining the mechanisms of modeling
contextual dependencies in recurrent neural network (RNN) and its variants (e.g., LSTM, EMLo).
We first define the concept of separation rank in the language model process, and show that this
separation rank can measure the dependencies in a sentence. We further derive the lower bound of
the separation rank in the tensor space language model, which is a combination number of network
depth and network width, etc.
We then provide a lower bound analysis for the separation rank defined in language modeling. It
shows that along with the increasing number of network layers L (representing the network depth),
the lower-bound can be increased by an exponential level, while increasing the number of hid-
den units R (representing the network width) can improve the lower-bound by a linear level. The
lower-bound is associated with the separation rank, which measures the degree of the contextual
dependencies.
The method of Adaptive Computation Time (ACT) (Graves, 2016) allows recurrent neural networks
to learn how many computational steps (i.e., the depth) to takes on an input. However, this method
implies an assumption that the states and outputs are approximately linear, and it lacks a theoretical
interpretation. The effect of depth on the ability of neural network to model contextual dependencies
is more direct, as mentioned above (Levine et al., 2017; Zia & Razzaq, 2018). For a natural language
processing task, it is a challenge task to get an adaptive neural network structure (i.e., the number
of layers). Inspired by this work (Graves, 2016), we design a bidirectional adaptive recurrent neural
network based on the guidance of the separation rank in language model.
In order to verify our theoretical analysis, we carry out various NLP tasks that reflect different
degrees/ranges of contextual dependencies. The experimental results can explain the effect of net-
work depth (represented by network layers) on the contextual dependencies modeled in the neural
language models. In addition to the network depth, our experiments show that increasing the net-
work width (i.e., the number of the hidden units) can also increase the modeling ability for both
long-range and short-range contextual dependencies. Moreover, increasing the network depth can
be more effective than increasing the network width, on the tasks requiring long-range dependency.
Furthermore, in order to search a minimum layers and effective modeling contextual dependency,
we train the new adaptive neural network on the sentence classification task. The results show that
our model achieves better results than a fixed depth recurrent neural network (i.e., BiLSTM).
2	Preliminaries
We first briefly introduce the basic language model, as well as the recently developed tensor space
language model (TSLM) (Zhang et al., 2019), which is proved to be a generalized approach to both
n-gram and recurrent language models.
2.1	Basic Language Model
Given a sentence s of n words denoted as (w1, w2, . . . , wn), a language model computes the joint
probability of a sentence. Its joint probability can be written as:
p(w1n) = p(w1, . . . , wn).	(1)
2
Under review as a conference paper at ICLR 2020
N -gram language model computes the joint probability of the sentence by calculating the probability
of a word wi given the history (w1 , . . . , wi-1).
2.2	Tensor Space Language Model
Recently, a Tensor Space Language Model (TSLM) (Zhang et al., 2019) was proposed, which can
consider all the combinatorial dependencies among words through the tensor product. Tensor prod-
Uct is a fundamental operator in tensor analysis, denoted by ⑼ which can map two low-order tensors
to a high-order tensor. It is proved that TSLM is a generalization of both n-gram (when the word
vectors are represented with one-hot vectors) and a recurrent language modeling process (when the
word vectors are represented with a low-dimensional dense word vector).
In TSLM, for a sentence S = {w1 , . . . , wn} with length n, each word wi can be represented by a
m-dimension word vector wi (i ∈ {1, . . . , n}). The basic formulation of TSLM can be written as:
m
p(w1n) =	Td1...dnAd1...dn	(2)
d1 ,...,dn=1
where Ad1 ,...,dn are the entries from tensor A which is essentially rank-one tensor, i.e., A =
wι 0 ... 0 Wn. Tdι,...,dn are entries from tensor T, which is a higher rank tensor. T can en-
code the network parameters, e.g., hidden layers matrix and states matrix, after TSLM is reduced to
a recurrent language modeling architecture.
3	Separation Rank as Measurement for Contextual Dependency
In this section, our aim is to define the separation rank in TSLM. In Sec. 3.1, we first describe
the basic ideas for the contextual dependency and expose the variables which are related to the
contextual dependencies. Then, we define the separation rank in tensor space language model in
Sec 3.2. For better readability, we put detailed proofs in supplementary appendices.
3.1	Basic Ideas for Contextual Dependency
The separation rank measures how far a function is from being separable (Cohen & Shashua,
2016b). p(w1n) in Eq. 1 can be considered as the probability density function, given a sentence
S = (w1, . . . , wn), where n is the number of words. Without loss of generality, we can split a
sentence into two disjoint context subsets, S1 = (w1, . . . , wi) and S2 = (wi+1, . . . , wn), where i is
a position for separating a sentence to S1 and S2. If S1 and S2 is independent. The function of joint
probability in Eq. 1 can be written as:
p(w1n) = p(w1i )p(win+1).	(3)
For considering the situation when S1 and S2 are not independent, we give the function as follows:
K
p(w1n) = Xp(yj)p(w1i |yj)p(win+1|yj)	(4)
j=1
where yj is the conditional variable, yi ∈ Y, and Y={y1, . . . , yK}. When K is equal to 1, Eq. 4 can
be reduced to Eq. 3, since p(y1) = 1.
In language modeling, if K is equal to 1, then p(w1n)=p(w1i )p(win+1), which means that there is no
dependency between two context pieces S1 and S2. The higher K is, the further two context pieces
(S1 and S2) are from being separable, meaning that S1 and S2 has stronger dependency.
3.2	Separation Rank in TSLM
In this section, we define the separation rank based on K in TSLM. First, we consider p(w1n) in
Eq. 2 as a probability function. Then, the separation rank is defined on the factorization of p(w1n).
Such a factorization is carried on the matricization of weight tensor T in Eq. 2. The matricization
process is to put the entries from a tensor into a matrix. It turns out that the factorization ofp(w1n) is
reduced to the factorization of of the matrix form of the tensor T, as shown in the following claim.
3
Under review as a conference paper at ICLR 2020
Claim 1 The factorization ofp(w1n) in Eq. 2 can be obtained by the Singular Value Decomposition
(SVD) on the matrix JTK(S1,S2). After the decomposition, the matrix JTK(S1,S2) is written as follows:
K
JTK(S1,S2) = XλjvjujT	(5)
j=1
where K is number of non-zero singular values.
Proof 1 Our proof can be found in Supplementary Appendices A.1 submitted.
In Eq. 4, each different position i corresponds to different partitions (S1, S2) and consequent ma-
tricizations of tensor T . Thus there could be different decomposition and different numbers of
non-zero singular values. In order to find a measurement for contextual dependency, we define the
separation rank as the minimum value of K in TSLM. The claim is given as follows.
Claim 2 Suppose a sentence S=(w1, . . . , wn) can be split into two disjoint subset S1 =(w1, . . . , wi)
and S2=(wi+1, . . . wn), where i ∈ {1, . . . , n - 1} is any position. The separation rank of function
in Eq. 2 in tensor space language model is formulated as:
K
sep(S1,S2)(p(w1n)) = min{K ∈ N | p(w1n) = X p(yj)p(w1i |yj)p(win+1 |yj)}	(6)
j=1
Proof 2 Our detailed proof can be found in Supplementary Appendices A.2 submitted.
In general,the separation rank in TSLM sep(S1 ,S2) (p(w1n))1 have been defined in Eq. 6.
4	Lower B ound Analysis of Separation Rank for Recurrent
Neural Network Structure
In this section, by providing the lower bound of the separation rank in TSLM, we investigate the
interpretable mechanism in recurrent neural network language modeling. Specifically, we analyze
the relation between the separation rank (reflecting the contextual complexity) and the network
architecture (e.g., network depth and width). Recall that tensor space language model (TSLM)
can derive a recurrent language modeling process from via the tensor decomposition of high-order
tensors (Zhang et al., 2019). Based on the above analysis, we will establish the relation between
separation rank in TSLM (see Sec.3.2) and parameters of recurrent neural language models.
Different partition position i in Eq. 4 corresponds to different partition (S1, S2) in Eq. 6. If the
partitions (S1, S2) are different, the values of K will be different. Without loss of generality, we set
an equal partition, i.e., i = 1 2 C. When a sentence is partitioned equally, K value in Eq. 6 is larger
than the actual separation rank since it is the minimal K.
Then, based on the equal partition criteria, step by step, we propose our theoretical results regarding
the lower bound of the separation rank. Such a lower bound is expected to reflect the relation
between the ability for modeling contextual dependencies and the model structure (e.g., network
depth and width) in the language modeling process. Specifically, if a sentence is modeled by a
recurrent network architecture, the lower bound of the separation rank in TSLM can be computed
by some parameters (i.e., hidden units, the number of layers and the number of words in a sentence)
in recurrent network architecture. In the following, we first show such quantitative result when the
number of layers is one (i.e., L = 1), and then we provide our findings when L > 1.
Claim 3 In tensor space language model (TSLM), when the number of layer is one (i.e., L=1,) the
separation rank in TSLM can be given as follows:
Sep(S1,S2"=b2C)(P(Wn)) ≥ R, for L =L	⑺
where R is the number of hidden units, (S1, S2) is the case of equal partition.
Proof 3 The detailed proof can be found in Supplementary Appendices A.3 submitted.
1The separation rank can not be accuracy computed in experiments.
4
Under review as a conference paper at ICLR 2020
Figure 1: The diagram about a bidirectional adaptive RNN. a) is a single direction adaptive RNN,
b) is a schematic diagram of bidirectional RNN.
In order to show the influence of network depth, we need to further extend the tensor space language
model using recurrent network with deep layers, i.e., L > 1. We derive the lower bound, i.e., the
separation rank in TSLM in case of the L ≥ 2. Further details will be expound in Supplementary
Appendices A.4. The lower bound will be shown as follows:
n	(R + F -I)!	b n C ]
Sep(Si,S2；i=b 2 D)(P(WI)) ≥ min{ F!(R - 1)! ,mb 2」}	⑻
where F = ((L-Vn-j)! ,R is the number of the hidden units, n is the number of words in a sentence,
m is the dimension of a word vectors, L is the number of layers and (.)! is the symbol of factorial.
(Si, S2; i = b 2 J) is the case of equal partition.
Next, we explore the relation between the lower bound and network parameters in detail. From
Eq. 8, we can find that increasing both R and L can all improve the lower bound of separation
rank in TSLM, i.e., the separation rank of a recurrent network language model will be increased. In
order to illustrate this quantitative relationship between separation rank in TSLM and the recurrent
network architecture (i.e., R and L). In Eq. 8, we see that if L was increased, it is observed that the
lower bound will be increased by an exponential level. Through increasing R, the lower bound is
increased by an linear level. Table 2 in supplementary appendices shows the numerical relationship.
5	B idirectional Adaptive Recurrent Neural Network
The effectiveness of neural network depth is an important research problem (Vinayakumar et al.,
2017). In Sec.4, the analysis of lower bound has described that increasing the number of layer can
be more effective to improve the ability of modeling contextual dependency. In the processing for
modeling a sequence of words, it is important to search an adaptive network depth. Under most
circumstances, researches choose a fixed number of network layers L to model text. In this section,
we describe how to construct an adaptive bidirectional recurrent neural network.
5.1	Structure Construction
As in Eq. 6, we use a backward recurrent neural network language model as the right function
p(win+1 |yj). After that, a forward recurrent neural network network language model Rf can be seen
as the left function p(w1i |yj). In experiment, we first train the backward function and get the hidden
output. When it predicts the next words in forward language model, the hidden vectors from the
backward process are used to multiply the hidden vectors from the forward process. In Figure 1, we
provide the diagram for the bidirectional adaptive RNN.
In Figure 1 a), wt-1 and wt are the inputs of word vectors. St-2, St-1 and St are some vectors
from the outputs of hidden layer. Eq. 9 shows how to compute the St .
L0
St = Xpit Sti	(9)
i=1
5
Under review as a conference paper at ICLR 2020


Named Entity Recognition (NER)
POS tagging	Constituency Parsing
Word Sense Disambiguation (WSD)
Figure 2: Results on the short-range (first row) and long-range (second row) dependencies tasks.
The displayed results show that the increasing of R is more helpful in short-range dependencies
tasks (NER, Constituency Parsing and POS tagging), while the increasing of L is more effective for
long-range dependencies tasks (WSD, Semantic classifier and Coreference Resolution), compared
with the increasing of R.
where pti is a scalar value and presents the probability value of Sti . The probability value can be
computed as: pit = vtT Sti, where vt is the trainable vector and can be initialized at the start of
training. pit is equal to the inner product of vt and Sti .
5.2	Dynamic Halting
Since we wish to explore a recurrent neural network with the minimum computational cost. Here,
we first give out some basic assumptions. Since Eq. 16 is to get the minimal K, and increasing
the depth L can improve the ability of modeling contextual dependency by an exponential level in
Eq. 8, we can assume the computing of the depth L have a straight relation. Inspired by Adaptive
Computation Time to dynamically halt changes in certain representations (Graves, 2016), we design
the condition of halting as follows. There are two kinds of halting conditional in our model. First,
the times of computing is raised to the maximum number of calculations. Second, the steps of
computing is satisfied as N (t)=min{l : Pli pit ≤ 1 - }2.
6	Experiments
The about analysis has shown that improving R and L can improve the lower bound at different
levels. The lower bound can imply the ability of recurrent neural network language models for
modeling the contextual dependencies. For different NLP tasks, we need to choose the network
structure with different modeling capabilities. We carry out six classification tasks and an intra-
sentence similarity experiment to verify our theoretical, respectively.
6.1	Natural Language Processing Tasks
This set of experiments involves six tasks including Named Entity Recognition (NER), POS tagging,
Constituency parsing, Word Sense Disambiguation (WSD), Sentiment analysis and Coreference
Resolution. According to previous study (Peters et al., 2018a), NER, constituency parsing and POS
tagging are more required to model short-range dependency, while WSD, Semantic classifier and
Coreference Resolution are required to model long-range dependency.
2 is generally taken as 0.001 in experiments.
6
Under review as a conference paper at ICLR 2020
Table 1: Results on sentences classification dataset. Accuracy (acc) is used as the evaluation criteri-
on. * represents the results achieved by ourself.
Model	MR	SST2	CR	SUBJ	MPQA
FastText (Joulin et al., 2016)	0.765^^	0.788	0.789	0.916	0.874
Sent2Vec (Gupta et al., 2019)	0.763	0.802	0.791	0.912	0.872
Transformer (Vaswani et al., 2017)	0.731*	0.761*	0.762*	0.863*	0.723*
BiLSTM (Conneau et al., 2017)	0.775	0.807	0.813	0.896	0.887
Adaptive-LSTM	0.784*	0.816*	0.821*	0.92*	0.898*
Adaptive-BiLSTM	0.790*	0.818*	0.820*	0.917*	0.904*
Based on our analysis in the previous section, we have two hypotheses to be tested. (1) For tasks with
short-range dependencies required, improving R can to some extent meet the modeling requirement,
while improving the network depth L could be not necessary; (2) For tasks requiring long-range
dependencies, it is necessary to improve L to improve the ability of modeling dependencies.
These tasks be considered as classification tasks, for which the experimental results are described in
Sec. 6.1.2. In language modeling process, LSTM is a commonly used recurrent network structure.
Therefore, we mainly carried out experiments based on the structure of LSTM and its variants.
6.1.1	Testing Tasks
POS tagging We conduct experiments on the Wall Street Journal of the Penn Treebank dataset
(PTB) (Marcus et al., 1993). The model Ling et al. (2015) is used, which using bidirectional LSTMs.
Named Entity Recognition The CoNLL 2003 NER task (Tjong Kim Sang & De Meulder, 2003)
consists of newswire from the Reuters RCV1 corpus tagged with four different entity types. We
follow the language model augmented sequence taggers (TagLM) (Peters et al., 2017).
Constituency parsing The Penn Treebank (Marcus et al., 1993) which contains phrase structure
annotation is used in this application. We use the model named Reconciled Span Parser, which used
the biLM representation.
Word Sense Disambiguation We follow this model (Kageback & Salomonsson, 2016) which used
the bidirectional LSTMs architecture to compute a probability distribution over the possible senses
corresponding to that word. The dataset of Senserval-2 (Edmonds & Cotton, 2001) is used.
Coreference Resolution An end-to-end coreference resolution model (Lee et al., 2017) has been
proposed. We conduct experiments on the OntoNotes Release 5.0 benchmark (Pradhan et al., 2012).
Sentiment analysis We use the IMDB dataset (Maas et al., 2011) with two categories. We perform
classification using a standard bidirectional LSTM with different hidden units and layers.
The experimental setup of the six testing tasks can be found in Supplementary Appendices A.5.1.
6.1.2	Experimental Results and Analysis
The results on six NLP tasks are reported in Figure 2. The results of short-range dependency and
long-range dependency tasks are shown as the first row and second row, respectively.
In short-range dependency tasks, using 1-layer LSTM, its F1 scores increase along the increasing of
the number of hidden units R. There are similar experimental phenomenons when using 2-layer and
3-layer LSTMs. For each R, after L be increased, we observe that increasing F1 score is not very
obvious. These phenomena mean that for the increasing of F1 , the increasing of R is more obvious
than the increasing ofL. Therefore, the results in the first row of Figure 2 verify our first hypothesis
about short-range dependent tasks.
In the second row of Figure 2, these trends of line charts show that the increasing for the number
of hidden units R and the number of layers L can help to increase the F1 score, while the effects
of increasing L is more obvious, for long-range dependency tasks. The F1 scores are the highest
through using 3-layer LSTMs in long-range dependency tasks. In order to investigate different
effects of increasing R and L on F1 scores, we have detailed observations: When L is the largest
(saying 3), the increasing of F1 score is getting relatively slow along with the increasing of R, as
shown in the results of semantic classifier (the middle in the second row in (2)). These phenomena
and trends verify the second hypothesis. It is that increasing L can improve the network modeling
7
Under review as a conference paper at ICLR 2020
ability more effectively than increasing R for long-range dependency tasks, In summary, R is more
necessary to be increased for the short-range dependency tasks, while L is relatively more necessary
to be increased in the long-range dependency tasks. Such experimental results and analysis are
consistent with our theoretical analysis in Sec 4 and testing hypotheses in this section.
6.2	Intra-Sentence Similarity Task
In this section, we present a detailed empirical study about the effect of different network architec-
ture (i.e., network width and the number of layers) on the representation ability of contextual word
embeddings. Based on previous analysis in Sec. 4, our hypothese are as follows. (1) Increasing R
can improve the similarity between short-range words. (2) Increasing L can improve the similarity
between long-range words, (3)Increasing L is more effective for modeling the similarity between
words than the increasing of R.
1-Billion Word Benchmark dataset Chelba et al. (2013) is used, and it has approximately 800M
tokens of news crawl data from WMT 2011. Different recurrent network architectures (i.e., using
different layers and the number of hidden units) are trained in EMLo, which uses bidirectional
LSTM structure. The training of EMLo model will be stopped when the average perplexity is 40
in test dataset. After that, we select a sentence from the test data and compute the intra-sentence
similarity. We use cosine similarity, which is a popular way to measure the similarity between word
vectors encoded in different layers of EMLo. Figure 3 in Supplementary Appendices shows the
intra-sentence contextual similarity among words in a sentence. The details of the experimental
results and analysis will be discussed in Supplementary Appendices A.5.2.
6.3	Bidirectional Adaptive Recurrent Neural Network
In this section, we test the bidirectional adaptive recurrent neural network on five text classification
tasks. Text classification task is to refer to the process of automatically determining the text category
based on the textual content under a given classification system. The Precision is used to evaluate
the quality of the model. We test our model on MR, SST-2, CR, MPQA and TREC, respectively.
The experiment results of this part is shown in Table 1.
Table 1 shows the results about the adaptive network in sentence classification tasks. Results shows
that the Adaptive-BiLSTM model achieves better accuracy (acc) in these datasets than BiLSTM. Our
method achieves a larger improvement on the MPQA dataset. The adaptive bi-LSTM (bidirectional
LSTM) and the model of adaptive LSTM achieve very close results.
7	Conclusion and Further Work
In this paper, we provide quantitative analysis for modeling contextual dependencies in recurrent
language model and its variants. First, we define the separation rank in tensor space language model
to measure the contextual dependencies of a sentence. Then, we analyze the lower bound of the
separation rank in the recurrent neural network based LM. Based on the lower-bound analysis, we
explain the quantitative relation between the value of separation rank and the network structure. For
certain NLP tasks requiring different ranges of contextual dependencies, the lower-bound analysis
will suggest different settings for the network depth and width. These suggestions based on the
quantitative lower-bound analysis, are verified by various NLP tasks. In order to find the number of
layers with both minimum cost and great performance results, we designed an adaptive RNN and
verify the effectiveness of the method on text classification tasks.
In the future work, we can design feasible methods to calculate the contextual dependencies, and use
such signals to design automatic language modeling approaches, which can be adaptively controlled
for specific NLP tasks with certain requirement for the contextual dependencies. In addition, we will
explore the use of tensor space to explain or advance the more recent language modeling approach,
e.g., Transformer and BERT.
8
Under review as a conference paper at ICLR 2020
References
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony
Robinson. One billion word benchmark for measuring progress in statistical language modeling.
arXiv preprint arXiv:1312.3005, 2013.
Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pp. 551-561, 2016. URL
http://aclweb.org/anthology/D/D16/D16-1053.pdf.
Nadav Cohen and Amnon Shashua. Inductive bias of deep convolutional networks through pooling
geometry. CoRR, abs/1605.06743, 2016a. URL http://arxiv.org/abs/1605.06743.
Nadav Cohen and Amnon Shashua. Inductive bias of deep convolutional networks through pooling
geometry. arXiv preprint arXiv:1605.06743, 2016b.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Super-
vised learning of universal sentence representations from natural language inference data. arXiv
preprint arXiv:1705.02364, 2017.
Philip Edmonds and Scott Cotton. Senseval-2: overview. In The Proceedings of the Second Inter-
national Workshop on Evaluating Word Sense Disambiguation Systems, pp. 1-5. Association for
Computational Linguistics, 2001.
Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013.
URL http://arxiv.org/abs/1308.0850.
Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arX-
iv:1603.08983, 2016.
Prakhar Gupta, Matteo Pagliardini, and Martin Jaggi. Better word embeddings by disentangling
contextual n-gram information. In NAACL-HLT (1), pp. 933-939. Association for Computational
Linguistics, 2019.
Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov, Armand Joulin, Edouard Grave,
Piotr Bojanowski, Tomas Mikolov, Armand Joulin, and Edouard Grave. Bag of tricks for efficient
text classification. 2016.
Mikael Kageback and Hans Salomonsson. Word sense disambiguation using a bidirectional lstm. In
Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V), pp. 51-56,
2016.
Andrej Karpathy, Justin Johnson, and Fei-Fei Li. Visualizing and understanding recurrent networks.
CoRR, abs/1506.02078, 2015. URL http://arxiv.org/abs/1506.02078.
Wei-Jen Ko, Bo-Hsiang Tseng, and Hung-yi Lee. Recurrent neural network based language mod-
eling with controllable external memory. In 2017 IEEE International Conference on Acoustic-
s, Speech and Signal Processing, ICASSP 2017, New Orleans, LA, USA, March 5-9, 2017, p-
p. 5705-5709, 2017. doi: 10.1109/ICASSP.2017.7953249. URL https://doi.org/10.
1109/ICASSP.2017.7953249.
Kenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. End-to-end neural coreference reso-
lution. 2017.
Yoav Levine, Or Sharir, Alon Ziv, and Amnon Shashua. On the long-term memory of deep recurrent
networks. 2017.
Wang Ling, Tiago LuIs, LuIs Marujo, Ramn Fernandez Astudillo, and Isabel Trancoso. Finding
function in form: Compositional character models for open vocabulary word representation. Com-
puter Science, pp. 1899-1907, 2015.
Ning Liu, Benyu Zhang, Jun Yan, and Zheng Chen. Text representation: from vector to tensor. In
IEEE International Conference on Data Mining, pp. 725-728, 2005.
9
Under review as a conference paper at ICLR 2020
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.
Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the
associationfor computational linguistics: Human language technologies-volume 1, pp. 142-150.
Association for Computational Linguistics, 2011.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of English: the penn treebank. 1993.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word
representation. In Conference on Empirical Methods in Natural Language Processing, pp. 1532-
1543, 2014.
Matthew Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. Semi-supervised se-
quence tagging with bidirectional language models. In Proceedings of the 55th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 1756-
1765, 2017.
Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. Dissecting contextual word
embeddings: Architecture and representation. In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pp. 1499-1509, 2018a.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. In Proc. of NAACL, 2018b.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. Conll-
2012 shared task: Modeling multilingual unrestricted coreference in ontonotes. In Joint Confer-
ence on Emnlp & Conll-shared Task, 2012.
Xipeng Qiu and Xuanjing Huang. Convolutional neural tensor network architecture for community-
based question answering. In IJCAI, pp. 1305-1311, 2015.
Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. Reasoning with neural
tensor networks for knowledge base completion. In International Conference on Neural Informa-
tion Processing Systems, pp. 926-934, 2013.
Erik F Tjong Kim Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceedings of the seventh conference on Natural lan-
guage learning at HLT-NAACL 2003-Volume 4, pp. 142-147. Association for Computational Lin-
guistics, 2003.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural In-
formation Processing Systems 30: Annual Conference on Neural Information Processing Sys-
tems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 6000-6010, 2017. URL http:
//papers.nips.cc/paper/7181-attention-is-all-you-need.
R. Vinayakumar, K. P. Soman, and P. Poornachandran. Evaluating effectiveness of shallow and
deep networks to intrusion detection system. In 2017 International Conference on Advances in
Computing, Communications and Informatics (ICACCI), pp. 1282-1289, Sep. 2017. doi: 10.
1109/ICACCI.2017.8126018.
Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. Breaking the softmax
bottleneck: A high-rank RNN language model. CoRR, abs/1711.03953, 2017. URL http:
//arxiv.org/abs/1711.03953.
Lipeng Zhang, Peng Zhang, Xindian Ma, Shuqin Gu, Zhan Su, and Dawei Song. A generalized
language model in tensor space. Proceedings of the 2019 Conference on AAAI Conference on
Artificial Intelligence, 2019.
Tehseen Zia and Saad Razzaq. Residual recurrent highway networks for learning deep sequence
prediction models. Journal of Grid Computing, pp. 1-8, 2018.
10
Under review as a conference paper at ICLR 2020
A Supplementary Appendices
A.1 Claim 1
The factorization of p(w1n) in Eq.2 (in paper) can be obtained by the Singular Value Decomposi-
tion (SVD) on the matrix JTK(S1,S2). After the decomposition, the matrix JTK(S1,S2) is written as
follows:
K
JTK(S1,S2) = XλjvjujT	(10)
j=1
where K is the number of non-zero singular values.
Proof sketch of this claim
In the tensor space language model, the input tensor is a rank-one tensor. To make it easy to under-
stand, we assume that the word vector is one hot vector. The high order tensor A can be given by the
operator of tensor product. Therefore, the tensor A is one hot tensor, which means that the tensor
only have one element is 1 and other elements are zero. The basic form of tensor space language
can be re-written an element Td1,...,dn from the tensor T. After that, the element can be consider a
value in a matrix which can be given by the matricization of the tensor T . For all elements of the
matrix, we can consider a method of SVD to get the value of K .
It would be helpful to understand if the definition of matricization was given. Therefore, There is
the definition of matricization as follows.
Definition 1 Suppose T ∈ Rm×...m is a tensor of order n, and let (S1, S2) be a paratition of a
set {1, . . . , n}, and S1 and S2 are disjoint subsets. The matricization of T denoted as JTKS1,S2, is
the m|S1|-by-m|S2| matrix holding the entries of Td1,...,dn is placed in row index 1 + P|tS=11| (dit -
1)m|S2|-t and column index 1 + P|tS=21| (djt - 1)m|S2|-t, where i ∈ S1 andj ∈ S2.
Proof 4 . In subsection 2.2, Eq.2 is the basic form of tensor space language model. In Eq.2, A is a
rank - one tensor A = wι 0 ... 0 Wn. We set each word as an one - hot vector. The tensor A is a
tensor with only one entry being one and the other entries being zero. The Eq.2 can be rewritten as:
|V|
p(w1n) =	Td1...dnAd1...dn
d1 ,...,dn=1
where
1, dk = index(wk , V ), ∀k ∈ [n]
d1...d2 =	0, otherwise
(11)
(12)
which means tensor A is an one-hot tensor, and index(w, V ) means the index of w in vocabulary
V. k ∈ [n] also is denoted as k ∈ {1, . . . , n}. The probability of a sentence s is:
|V|
p(w1n) =	Td1 ...dn Ad1 ...dn
d1 ,...,dn=1
= Td1,...,dn, dk = index(wk, V),∀k ∈ [n]
(13)
Therefore, the factorization ofp(w1n) in Eq. 2(in paper) can be achieved by two steps as follows.
Firstly, the matricization of weight tensor T can be achieved by the matricization definition 1 and
represented as JT K(S1,S2). Secondly, we can perform singular value decomposition (SVD) on matrix
JTK(S1,S2).
JTK(S1,S2)=USVT	(14)
Therefore, we can compute each element in the matrix JTK(S1,S2), in other words, we can get every
entry from tensor T and represent as:
K
T(d1...di),(di+1...dn) =	λv Uv,d1,...,di Vv,ddi+1,...,dn
v=1
(15)
11
Under review as a conference paper at ICLR 2020
where Uv,d1,...,di is the element from the matrix U and the Uv,d1,...,di is the element from the matrix
V. Let Vv,d1,...,di and Uv,d1,...,di can be given by p(w1i |yv) andp(win+1|yv), respectively. λv is the
singular value and is seed as p(yv). We can represent Eq. 15 as Eq. 4 (in paper).
A.2 CLAIM 2
Suppose a sentence S = (w1, . . . , wn) can be split into two disjoint subset S1 = (w1, . . . , wi)
and S2 = (wi+1, . . . wn), the separation rank of function Eq. 2 by tensor space language model is
formulated as following:
K
sep(S1,S2)(p(w1n)) = min{K ∈ N | p(w1n) = X p(yj)p(w1i |yj)p(win+1 |yj)}	(16)
j=1
where K is the number of no-zero singular values, i is the position of separation.
Before proving the claim, we should give out a definition as reported in (Cohen & Shashua, 2016b).
Definition 2 Let (I1, I2) be a partition of ordered input indexes, i.e. I1 and I2 are disjoint subsets
of [N] whose is the set of all indexes. I1 = i1, . . . , i|I1| where i1 < . . . < i|I|, and similarly
I2 = j1, . . . , j|I2| where j1 < . . . < j|J|. For a function h : (Rs)N → R, the separation rank w.r.t
the partition (I1, I2) is defined as follows:
sep(h; I1 , I2) := min{R ∈ N ∪ {0} : ∃g1 . . . gR, g10 . . . gR0 s.t. h(x1 , . . . , xN)
R
gv (xi1, . . . , xi|I
v=1
)gv0 (xi1
..,xiI2)}
It is the minimal number of summands that together give h, where each summand is separable and
equal to a product of two function. gi and gi0 are the functions, gi:(Rs)|I1 | and gi0:(Rs)|I2 |.
Proof sketch of this claim
According to Eq. 15 and Definition 2, it will be easy to prove the result that the separation rank
between two different inputs S1 and S2 is equal to the minimum value of K for different separation.
Proof 5 In Eq. 15, if we set the √λV 1几R,…启 is equal to gv (wι,..., Wi) and √λ^ 匕4,…& is
equal to gv0 (wi+1, . . . , wn). According to the definition 2, we can give the function of the separation
rank in tensor space language model.
sep(S1,S2)(p(w1n)) = min{R ∈ N : ∃g1 . . .gR,g10 . . .gR0 s.t. p(w1n) =
R
gv (w1, . . . , wi)gv0 (wi+1,...,wn)}
v=1
(17)
In a word, it is the minimal number of summands that together give p(w1n), where each summand
is separable and is equaltion to a product. After that, for different (S1, S2) on a sentence, K is
different. Therefore, we can define the minimum value of K as the separation rank in tensor space.
A.3 CLAIM 3
In tensor space language model (TSLM), when the number of layer is one (i.e., L=1,) the separation
rank in TSLM can be given as follows:
Sep(S1,S2"=b2C)(P(Wn)) ≥ R, forL =L	(18)
where R is the number of hidden units, (S1, S2) is the case of equal partition, i.e., the position i in
Eq.4(in paper) is equal to [ nC.
Proof sketch of this claim
We can consider the RNN is a function y. In addition, the function can be re-written as a new form
which is the sum of R groups of factor functions product. R is the number of hidden units, and also
the separation rank in RNNLM. It meets the definition of separation rank in definition 2.
12
Under review as a conference paper at ICLR 2020
Proof 6 The recurrent network language model of the number of layers L=1 have been given:
ht = W ht-1	Uwt
yt = V ht
(19)
where w is the current word inputs, ht is the current hidden state, wt is the word vector of t-th
word. yt is probability distribution of the current state (i.e., the joint probability distribution). The
denotes the element-wise multiplication between vector (i.e., Hadamard product). W, U are
square matrix in TSLM and the dimension of W is the number of hidden units R. We set the W = I.
V is the full connection matrix. Further, we can compute current hidden state as follows:
ht = Uwt	W ht-1
= UwtW(Uwt-1	ht-2)
⇒UwtUwt-1ht-2 (W =I)
(20)
⇒ UWt Θ ... Θ UWt-1 Θ ht/2
where we set the h t is the function g(wι,..., Wt/2) and the U Wt Θ ... Θ U Wt-1 is the function
g0(wt +ι,..., Wt). After that, we can compute each probability in the probability distribution yt.
The process is as follows:
yt = Vht
R
yt(i) = X Vi,jgj(W1t/2)gj0(Wtn/2+1)
j=1
(21)
where i ∈ 1, . . . , |V | is the index in the vector yt and |V | is the size of vocabulary. V is a matrix of R
by |V |. At the same time, i means also the position of the last word of a sentence in the vocabulary.
Therefore, the p(w1t) can be written as:
p(w1t ) = yt(i)
Let n be equal to t, the inputs can be written (W1, . . . , Wbn/2c ) = S1 and (Wbn/2c+1, . . . , Wn) =
S2 .According to the definition of the separation separation rank in the definition 2, we can get the
result as follow:
sep(S1,S2) (p(w1n)) ≥ R	(22)
Therefore, the separation rank in TSLM is R when we using the 1-layer recurrent network.
A.4 Separation Rank in Tensor Network
Let yL be the function computing the output after t time-steps of an recurrent tensor network with
L layers, R hidden layer channel numbers, weights denoted by tensor T and initial hidden states
h0,l, l ∈ [L]. The relations between start-end separation rank and the TN architecture (Levine et al.,
2017) are shown as follow:
seq(T1,T2)(y1) = min{R, mT/2},	L = 1
seq(T1,T2)(y2) ≥ minT{/m2,R}, L=2
where ^^min{mR}^T^/2^ is the multiset coefficient, given in the binomial form by
(min{m,R}+bT72c-1). R is the hidden channel numbers, T is the length of sequence inputs, and
T /2c
m is the dimensions of each model in high-order tensor. (T1, T2) is an average separation of time
steps T. For clearly understand the multi-set coefficient, We can understand that select [ TC types
from R0 = min{R, m} class elements. Now, a lower bound of (([τR2j)) on the separation rank
of depth L = 2 have been provided. In the following, The conjecture that a tighter lower bound
13
Under review as a conference paper at ICLR 2020
holds for networks of depth L > 2, the form of which implies that the dependency capacity of deep
recurrent networks grows combinatorially with the network depth:
seq(yL) ≥ min{
((((LR1」))))
mbT/2c} L≥ 2
(23)
where R is the number of hidden units, L is the number of layers. n is the number of words in a
sentence. In language model, for easy to analysis the relation between separation rank and parameter
from the recursive networks architecture. We set the R = m. We do not consider time series. Let’s
set the T in Eq. 23 to n. n is the numbers of words in a sentence. Considering the constraints
of tensor dimensions, the maximum of TSLM separation rank is mbT /2c . T is the temple in RNN,
we suppose that it is the equal n, n is the length of a sentence. In the case where R and L are
controllable, the value to the left of the minimum function 23 is less than the value to the right.
After that, we can get the relations as follow.
n n n、、、…∙ 「(R + F-I)! I n I
Sep(S1 ,S2；i= b2C))(P(WI)) ≥ min{ FKR _ 1» ,mb2C
where F
([n/2] + L — 2)!
(L - 1)!(bn∕2C)!
(24)
where the R is the number of hidden size, n is the number of words in a sentence, and L is the
number of layers. Note that ! is the symbol of factorial.
A.5 Experiment
Table 2: The quantitative relation between the separation rank in TSLM and the parameters (i.e., R and L). Increasing L and R approximate the separation rank of recurrent network exponentially					
and linearly, respectively. Example: n = {1, 2, 3,4}.					4 (i.e., sentence length), R ∈ {14, 15, 16, 17} and L ∈		
R	14		15	16	17
L=	1	14	15	16	17
L=	2	455	560	680	816
L=	3	8008	12376	18564	27132
L=	4	1.9*104	4.3*104	9.2*104	3.5*105
A.5.1 Experimental Setup
Sec. 4 analyzed the relation between contextual dependencies and recurrent network architec-
ture through L and R. Specifically, the lower bound of the recurrent language model model-
ing the sentence contextual dependencies will be increased by increasing L (L ∈ {1, 2, 3}) and
R (R ∈ {100, 125, 150, 175, 200, 225, 250, 300}) in long-range/short-range dependency tasks. Re-
call that our hypotheses are: For short-range dependent tasks, increasing R is sufficient to reflect
the network modeling ability, while for long-range dependent tasks, increasing L can improve the
network modeling ability more effectually than increasing R. We test our hypotheses through the
trend of F1 score. F1 is a mainly and common indicator of evaluation in these task.
A.5.2 Analysis for Intra-Sentence Similarity Task
Our experiments focus on the changing trends of the similarity between word embeddings along
with the increasing of R and L. This experiment is different from the experiments reported by
Peters et al. (2018a). They only consider the difference between upper layer and lower layer and do
not consider the changing of network depth R.
Figure 3 shows the intra-sentence contextual similarity between all words in a sentence. In Figure 3,
we obtain several observations. When the number of hidden units R is fixed, we observe that the
bright area is larger along with the increase of the number of layers L from 1 to 3. The bright area
indicates the contextual dependency between words in our work. Relatively speaking, the larger the
14
Under review as a conference paper at ICLR 2020
The Layer L=I
The Layer L=2
Who
made
these
wonderful
cars
that
people
drove
as
If
they
were
an
extension
Qf
tħelr
own
bodies
The Layer L-3
Who
made
these
wonderful
cars
that
people
drove
as
If
they
were
an
extension
Cf
tħelr
own
bodies
Hidden units 256
Hidden units 512
Hidden units 1024
Who
made
these
wonderful
Who
made
these
wonderful
cars
that
PeQPIe
drove
cars
that
people
drove
they
they
were
were
their
an
extension
an
extension
Cf
tħelr
own
bodies
own
bodies
as
as
If

Figure 3: Visualization of contextual similarity between all words pairs in a single sentence. For the
first row, we set the hidden layer units to be 256, with layers from 1 to 3. For the second row, we set
the number of layers to 1, and the hidden layer units (256, 512 and 1024, respectively). The lighter
yellow-colored areas indicate the higher contextual similarity.
Table 3: The statistical results about the average of the context similarity. Layers and hidden units
are represented by L and R respectively.
R(for L = 1) average similarity	256 0.25	512 0.36	1024 0.38
L(f or R= 256)	1	2	3
average similarity	0.25	0.39	0.48
bright region is, the stronger the ability of a language model modeling the contextual dependency.
When the number of layers is fixed, we observe that the bright area is increased along with the
increasing of the number of hidden units from 256 to 512. It may mean that short-range contextual
dependency can be captured by adding the number of hidden units. These experimental phenomena
verify the first hypothesis and the second hypothesis.
Correspondingly, we can also observe from Table 3 that the larger the bright area is, the larger the
average of contextual similarity is. In Table 3, when the L is equal to 3 (when R=256), the average
of contextual similarity is the largest. Combined Figure 3 and Table 3, we can find that the increasing
of the number of layers can more effective to capture long-range similarity information. The results
in Table 3 and in Figure 3 verify the third hypothesis.
15