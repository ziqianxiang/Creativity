Under review as a conference paper at ICLR 2020
Credible Sample Elicitation by Deep Learning,
for Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
It is important to collect credible training samples (x, y) for building data-
intensive learning systems (e.g., a deep learning system). In the literature, there is
a line of studies on eliciting distributional information from self-interested agents
who hold a relevant information. Asking people to report complex distribution
p(x), though theoretically viable, is challenging in practice. This is primarily
due to the heavy cognitive loads required for human agents to reason and report
this high dimensional information. Consider the example where we are inter-
ested in building an image classifier via first collecting a certain category of high-
dimensional image data. While classical elicitation results apply to eliciting a
complex and generative (and continuous) distribution p(x) for this image data, we
are interested in eliciting samples Xi 〜P(x) from agents. This paper introduces a
deep learning aided method to incentivize credible sample contributions from self-
ish and rational agents. The challenge to do so is to design an incentive-compatible
score function to score each reported sample to induce truthful reports, instead of
an arbitrary or even adversarial one. We show that with accurate estimation of a
certain f -divergence function we are able to achieve approximate incentive com-
patibility in eliciting truthful samples. We then present an efficient estimator with
theoretical guarantee via studying the variational forms of f -divergence function.
Our work complements the literature of information elicitation via introducing
the problem of sample elicitation. We also show a connection between this sam-
ple elicitation problem and f -GAN, and how this connection can help reconstruct
an estimator of the distribution based on collected samples.
1	Introduction
The availability of a large quantity of credible samples is crucial for building high-fidelity machine
learning models. This is particularly true for deep learning systems that are data-hungry. Arguably,
the most scalable way to collect a large amount of training samples is to crowdsource from a decen-
tralized population of agents who hold relevant sample information. The most popular example is
the build of ImageNet (Deng et al., 2009).
The main challenge in eliciting private information is to properly score reported information such
that the self-interested agent who holds a private information will be incentivized to report truthfully.
At a first look, this problem of eliciting quality data is readily solvable with the seminal solution
for eliciting distributional information, called the strictly proper scoring rule (Brier, 1950; Winkler,
1969; Savage, 1971; Matheson & Winkler, 1976; Jose et al., 2006; Gneiting & Raftery, 2007): sup-
pose we are interested in eliciting information about a random vector X = (X1, ..., Xd-1, Y ) ∈
Ω ⊆ Rd, whose probability density function is denoted by P with distribution P. As the mechanism
designer, ifwe have a sample x drawn from the true distribution P, we can apply strictly proper scor-
ing rules to elicit P: the agent who holds P will be scored using S(P, x). S is called strictly proper
if it holds for any P and q that Eχ~p[S(p, x)] > Eχ~P[S(q, x)]. The above elicitation approach has
two main caveats that limited its application:
•	When the outcome space ∣ Ω ∣ is large and is even possibly infinite, it is practically impossible for
any human agents to report such a distribution with reasonable efforts. This partially inspired a
line of follow-up works on eliciting property of the distributions, which we will discuss later.
•	The mechanism designer may not possess any ground truth samples.
1
Under review as a conference paper at ICLR 2020
In this work we aim to collect credible samples from self-interested agents via studying the problem
of sample elicitation. Instead of asking each agent to report the entire distribution p, we hope
to elicit samples drawn from the distribution P truthfully. We consider the samples Xp 〜P and
Xq 〜Q. In analogy to strictly proper scoring rules1, we aim to design a score function S s.t.
Eχzp[S(Xp,X)] > Eχzp[S(Xq,χ0)] for any q = p, where X is a reference answer that can be
defined using elicited reports. Often, this scoring procedure requires reports from multiple peer
agents, and X0 is chosen as a function of the reported samples from all other agents (e.g., the average
across all the reported Xs, or a randomly selected X). This setting will relax the requirements of high
reporting complexity, and has wide applications in collecting training samples for machine learning
tasks. Indeed our goal resembles similarity to property elicitation (Lambert et al., 2008; Steinwart
et al., 2014; Frongillo & Kash, 2015b), but we emphasize that our aims are different - property
elicitation aims to elicit statistical properties ofa distribution, while ours focus on eliciting samples
drawn from the distributions. In certain scenarios, when agents do not have the complete knowledge
or power to compute these properties, our setting enables elicitation of individual sample points.
Our challenge lies in accurately evaluating reported samples. We first observe that the f -divergence
function between two properly defined distributions of the samples can serve the purpose of incen-
tivizing truthful report of samples. We proceed with using deep learning techniques to solve the
score function design problem via a data-driven approach. We then propose a variational approach
that enables us to estimate the divergence function efficiently using reported samples, via a varia-
tional form of the f -divergence function, through a deep neutral network. These estimation results
help us establish an approximate incentive compatibility in eliciting truthful samples. It is worth
to note that our framework also generalizes to the setting where there is no access to ground truth
samples, where we can only rely on reported samples. There we show that our estimation results
admit an approximate Bayesian Nash Equilibrium for agents to report truthfully. Furthermore, in
our estimation framework, we use a generative adversarial approach to reconstruct the distribution
from the elicited samples.
We want to emphasize that the deep learning based estimators considered above are able to handle
complex data. And with our deep learning solution, we are further able to provide estimates for the
divergence functions used for our scoring mechanisms with provable finite sample complexity. In
this paper, we focus on developing theoretical guarantees - other parametric families either can not
handle complex data, e.g., it is hard to handle images using kernel methods, ordo not have provable
guarantees on the sample complexity.
Our contributions are three-folds. (1) We tackle the problem of eliciting complex distribution via
proposing a sample elicitation framework. Our deep learning aided solution concept makes it prac-
tical to solicit complex sample information from human agents. (2) Our framework covers the
case when the mechanism designer has no access to ground truth information, which adds contri-
bution to the peer prediction literature. (3) On the technical side, we develop estimators via deep
learning techniques with strong theoretical guarantees. This not only helps us establish approxi-
mate incentive-compatibility, but also enables the designer to recover the targeted distribution from
elicited samples. Our contribution can therefore be summarized as
“eliciting credible training samples by deep learning, for deep learning".
1.1	Related works
The most relevant literature to our paper is strictly proper scoring rules and property elicitation.
Scoring rules were developed for eliciting truthful prediction (probability) (Brier, 1950; Winkler,
1969; Savage, 1971; Matheson & Winkler, 1976; Jose et al., 2006; Gneiting & Raftery, 2007). Char-
acterization results for strictly proper scoring rules are given in McCarthy (1956); Savage (1971);
Gneiting & Raftery (2007). Property elicitation notices the challenge of eliciting complex distri-
butions (Lambert et al., 2008; Steinwart et al., 2014; Frongillo & Kash, 2015b). For instance,
Abernethy & Frongillo (2012) characterize the score functions for eliciting linear properties, and
Frongillo & Kash (2015a) study the complexity of eliciting properties. Another line of relevant
research is peer prediction, where solutions can help elicit private information when the ground
truth verification might be missing (De Alfaro et al., 2016; Gao et al., 2016; Kong et al., 2016;
1Our specific formulation and goal will be different in details.
2
Under review as a conference paper at ICLR 2020
Kong & Schoenebeck, 2018; 2019). Our work complements the information elicitation literature
via proposing and studying the question of sample elicitation via a variational approach to estimate
f -divergence functions.
Our work also extends the line of work on divergence estimation. The simplest way to estimate
divergence starts with the estimation of density function (Wang et al., 2005; Lee & Park, 2006;
Wang et al., 2009; Zhang & Grabchak, 2014; Han et al., 2016). Another method based on the
variational form (Donsker & Varadhan, 1975) of the divergence function comes into play (Bronia-
towski & Keziou, 2004; 2009; Nguyen et al., 2010; Kanamori et al., 2011; Ruderman et al., 2012;
Sugiyama et al., 2012), where the estimation of divergence is modeled as the estimation of density
ratio between two distributions. The variational form of the divergence function also motivates the
well-know Generative Adversarial Network (GAN) (Goodfellow et al., 2014), which learns the dis-
tribution by minimizing the Kullback-Leibler divergence. Follow-up works include Nowozin et al.
(2016); Arjovsky et al. (2017); Gulrajani et al. (2017); Bellemare et al. (2017), with theoretical anal-
ysis in Liu et al. (2017); Arora et al. (2017); Liang (2018); Gao et al. (2019). See also Gao et al.
(2017); Bu et al. (2018) for this line of work.
1.2	Notations
For the distribution P, we denote by Pn the empirical distribution given a set of samples {xi}in=1
following P, i.e., Pn = 1 /n ∙ Prn=1 δχi, where δχi is the Dirac measure at Xi. We denote by
kvks = (Pid=1 |v(i) |s)1/s the `s norm of the vector v ∈ Rd where 1 ≤ s < ∞ and v(i) is the i-th
entry of V. We also denote by IlvIlg = maxi≤i≤d ∣v(i)| the '∞ norm of V. For any real-valued
continuous function f : X → R, we denote by kfkLs(P) := [RX |f (x)|s dP]1/s the Ls(P) norm off
and ∣∣∕∣∣s := [ JX ∣f (x) F d μ ]1 /s the Ls (μ) norm of f (∙), where μ is the Lebesgue measure. Also, We
denote by ∣∣f∣∣∞ = SuP χ∈χ ∣f (x) ∣ the L∞ norm of f (∙). For any real-valued functions g (∙) and h (∙)
defined on some unbounded subset of the real positive numbers, such that h(α) is strictly positive
for all large enough values of α, We write g(α) . h(α) and g(α) = O(h(α)) if ∣g(α)| ≤ C ∙ h(α)
for some positive absolute constant c and any α > α0, where α0 is a real number. We denote by [n]
the set {1, 2, . . . , n}.
2 Preliminary
We formulate the question of sample elicitation.
2.1	Sample Elicitation
We consider two scenarios. We start with an easier case where we, as the mechanism designer, have
access to a certain number of group truth samples. This is a setting that resembles similarity to the
proper scoring rule setting. Then we move to the harder case where the inputs to our mechanism can
only be elicited samples from agents.
Multi-sample elicitation with ground truth samples. Suppose that the agent holds n samples,
with each of them independently drawn from P, i.e., Xi 〜 P2 for i ∈ [n]. The agent can report each
sample arbitrarily, which is denoted as r (Xi) : Ω → Ω. There are n data {χ↑}i∈[n] independently
drawn from the ground truth distribution Q3. We are interested in designing a score function S (∙)
that takes inputs of each r (∙) and {rj(Xj), xj}j∈[n]: S(r (Xi), {rj(Xj), xj}j∈[n]) such that if the
agent believes that Xj is drawn from the same distribution Xj 〜P, then for any {rj (∙) }j∈ [n], it holds
with probability at least 1 - δ that
n
Ex
i=1
χ* 〜P [S*(Xi, {Xj, Xj } j∈ [n])]
n
≥	Ex
i=1
[s (Ti ( Xi ), {rj ( Xj ) ,xj}j∈ [ n ])]—
n ∙ e.
,x*^ P
2Though we use x to denote the samples we are interested in, x potentially includes both the feature and
labels (x, y) as in the context of supervised learning.
3The number of ground truth samples can be different from n, but we keep them the same for simplicity of
presentation. It will mainly affect the terms δ and in our estimations.
3
Under review as a conference paper at ICLR 2020
We name the above as (δ, )-properness (per sample) for sample elicitation. When δ = = 0, it is
reduced to the one that is similar to the properness definition in scoring rule literature (Gneiting &
Raftery, 2007). We also shorthand ri = ri (xi) when there is no confusion. Agent believes that her
samples are generated from the same distribution as of the ground truth samples, i.e., P and Q are
same distributions.
Sample elicitation with peer samples. Suppose there are n agents each holding a sample Xi 〜Pi,
where the distributions {Pi }i∈[n] are not necessarily the same - this models the fact that agents can
have subjective biases or local observation biases. This is a more standard peer prediction setting.
We denote by their joint distribution as P = P1 × P2 × ..... × Pn.
Similar to the previous setting, each agent can report her sample arbitrarily, which is denoted as
r (Xi) : Ω → Ω for any i ∈ [n]. We are interested in designing and characterizing a score func-
tion S(∙) that takes inputs of each r (∙) and {rj(Xj)}j=i: S(r (Xi), {rj(Xj)}j=i) such that for any
{rj (∙)}j∈[n], it holds with probability at least 1 - δ that
ESP SS(Xi, {rj(Xj) = Xj}j=i∕∣ ≥ ESP SS(r(Xi), {rj(Xj) = Xj}j=i)] -e∙
We name the above as (δ, )-Bayesian Nash Equilibrium (BNE) in truthful elicitation. We only
require that agents are all aware of above information structure as common knowledge, but they do
not need to form beliefs about details of other agents’ sample distributions. Each agent’s sample is
private to herself.
2.2	f-DIVERGENCE
It is well known that maximizing the expected proper scores is equivalent to minimizing a corre-
sponding Bregman divergence (Gneiting & Raftery, 2007). More generically, we take the perspec-
tive that divergence functions have great potentials to serve as score functions for eliciting samples.
We define the f -divergence between two distributions P and Q with probability density function p
and q, respectively, as
Df (qkp)
p(X)f
d μ.
(2.1)
Here f (∙) is a function satisfying certain regularity conditions, which will be specified later. Solving
our elicitation problem involves evaluating the Df (qkp) successively based on the distributions P
and Q, without knowing the probability density functions p and q. Therefore, we have to resolve to
a form of Df (qkp) which does not involve the analytic forms ofp and q, but instead sample forms.
Following from Fenchel’s convex duality, it holds that
Df (qkp) = m(axESq[t(X)] - ESp[ft(t(X))],
(2.2)
where ft(∙) is the Fenchel duality of the function f (∙), which is defined as ft(U) = SuPv∈R{uv -
f (v)}, and the max is taken over all functions t (∙): Ω ⊂ Rd → R.
3 Sample Elicitation: A Generative Adversarial Approach
Recall from (2.2) that Df (qkp) admits the following variational form:
Df (qkp) = RaxEXZq[t(X)] - EXZp[ft(t(X))].	(3.1)
We highlight that via functional derivative, (3.1) is solved by t^(X;p, q) = f 0(θ*(X;p, q)), where
θ* (X; p, q) = q(X)/p(X) is the density ratio between P and q. Our elicitation builds upon such a
variational form (3.1) and the following estimators,
b(∙； P,q) = argmin EXZP」ft (t(X))] - EXZQn [t(X)],
t()
Dbf(qkp) =EXZQn[bt(X)] -EXZPn[ft(tb(X))].
4
Under review as a conference paper at ICLR 2020
3.1	Error B ound and Assumptions
Suppose we have the following error bound for estimating Df(qkp): for any probability density
functions p and q, it holds with probability at least 1 - δ(n) that
|Dbf(qkp) - Df (qkp)| ≤ (n),	(3.2)
where δ(n) and (n) will be specified later in Section 4. To obtain such an error bound, we need the
following assumptions.
Assumption 3.1 (Bounded Density Ratio). The density ratio θ* (x; p, q) = q(x)/p(x) is bounded
such that 0 < θo ≤ θ* ≤ θι holds for positive absolute constants θo and θι.
The above assumption is standard in related literature (Nguyen et al., 2010; Suzuki et al., 2008),
which requires that the probability density functions p and q lie on a same support. For simplicity of
presentation, we assume that this support is Ω ⊂ Rd. We define the β-Holder function class on Ω as
follows.
Definition 3.2 (β-Holder Function Class). The β-Holder function class with radius M is defined as
Cd(Ω,M) = ʃt(∙):ɑ ⊂ Rd → R: X M~h + X sup	7X‘ ：；；"" ≤ M
k	∣∣αkι<β	∣∣αk 1 = bβC x,y∈ωX=y	kx - yk∞
where ∂α = ∂α1 ∙ ∙ ∙ ∂αd with α = (α1, . . . , αd) ∈ Nd.
We assume that the function t^ (∙;p, q) is β-Holder, which guarantees the smoothness of t^ (∙; p, q).
Assumption 3.3 (β-Holder Condition). The function t (∙; p, q) ∈ Cβ(Ω, M) for some positive ab-
solute constants M and β, where Cβ(Ω, M) is the β-Holder function class in Definition 3.2.
In addition, we assume that the following regularity conditions hold for the function f(∙) in the
definition of f -divergence in (2.1).
Assumption 3.4 (Regularity of Divergence Function). The function f(∙) is smooth on [θ0, θ1] and
f(1) = 0. Also, it holds that
(i)	f is μo-strongly convex on [θo, θι], where μo is a positive absolute constant;
(ii)	f has L0-Lipschitz continuous gradient on [θ0, θ1], where L0 is a positve absolute constant.
We highlight that we only require that the conditions in Assumption 3.4 hold on the interval [θ0, θ1],
where the absolute constants θ0 and θ1 are specified in Assumption 3.1. Thus, Assumption 3.4 is
mild and it holds for many commonly used functions in the definition of f -divergence. For example,
in Kullback-Leibler (KL) divergence, we take f(u) = - log u, which satisfies Assumption 3.4; in
Jenson-Shannon divergence, we take f(u) = ulog u - (u + 1) log(u + 1), which also satisfies
Assumption 3.4.
We will show that under Assumptions 3.1, 3.3, and 3.4, the bound (3.2) holds. See Theorem 4.2 in
Section 4 for details.
3.2	Multi-sample elicitation with ground truth samples
In this section, we focus on multi-sample elicitation with ground truth samples. Under this setting,
as a reminder, the agent will report multiple samples. After the agent reported her samples, the
mechanism designer obtains a set of ground truth samples {χ^ }i∈[n]〜Q to serve the purpose of
evaluation. This falls into the standard strictly proper scoring rule setting.
Our mechanism is presented in Algorithm 1.
Algorithm 1 consists of two steps: step 1 is to compute the function tb(∙; p, q), which enables us, in
step 2, to pay agent using a linear-transformed estimated divergence between the reported samples
and the true samples. We have the following result.
Theorem 3.5. The f -scoring mechanism in Algorithm 1 achieves (2δ(n), 2b(n))-properness.
5
Under review as a conference paper at ICLR 2020
Algorithm 1 f -scoring mechanism for multiple-sample elicitation with ground truth
1.	Compute
b(∙；P, q) = argminESPjft(t(N))] — E方*〜Q」t(x^)]∙
t (∙)
2.	For i ∈ [n], pay reported sample ri using
S (ri, {rj ,xj}n=ι) := a —b (叽〜Q n [b(X; p,q)] — f t (b( ri; p,q)))
for some constants a, b > 0.
The proof is mainly based on the error bound in estimating f -divergence and its non-negativity.
Not surprisingly, if the agent believes her samples are generated from the same distribution as the
ground truth sample, and that our estimator can well characterize the difference between the two set
of samples, she will be incentivized to report truthfully to minimize the difference. We defer the
proof to Section B.1.
3.3 Single-task elicitation without ground truth samples
The above mechanism in Algorithm 1, while intuitive, has the following two caveats:
•	The agent needs to report multiple samples (multi-task/sample elicitation);
•	Multiple samples from the ground truth distribution are needed.
To deal with such caveats, we consider the single point elicitation in an elicitation without verifica-
tion setting. Suppose there are 2n agents each holding a sample Xi 〜 Pi 4. We randomly partition
the agents into two groups, and denote the joint distributions for each group’s samples as P and Q
with probability density functions p and q for each of the two groups. Correspondingly, there are a
set of n agents for each group, respectively, who are required to report their single data point accord-
ing to two distributions P and Q, i.e., each of them holds {^i }i∈ [n]〜P and {xq }i∈ [n]〜Q. As an
interesting note, this is also similar to the setup of a Generative Adversarial Network (GAN), where
one distribution corresponds to a generative distribution X | y = 1, and another X | y = 0. This is a
connection that we will further explore in Section 5 to recover distributions from elicited samples.
We denote by the joint distribution of P and q as P ㊉ q (distribution as P ㊉ Q), and the product of
the marginal distribution as p × q (distribution as P × Q). We consider the divergence between the
two distributions:
Df (p ㊉ qkp × q)
max Ex 〜P ㊉ q[ t (x)] — Ex 〜P × q[ ft (t (x))]∙
t (∙)	'
Motivated by the connection between mutual information and KL divergence, we define general-
ized f -mutual information in the follows, which characterizes the generic connection between a
generalized f -mutual information and f -divergence.
Definition 3.6 (Kong & Schoenebeck (2019)). The generalized f -mutual information between P
and q is defined as
If (p; q) = Df (p ㊉ qkp × q)
Further it is shown in Kong & Schoenebeck (2018; 2019) that the data processing inequality for
mutual information holds for If (p; q) when f is strictly convex. We define the following estimators,
b	(∙；P ㊉ q,P × q) = argminEx〜Pn×Q/ft(t(x))] — Ex〜P„®Qn [t(x)],
t (∙)
Df (p ㊉ qkP × q) = Ex〜P“©Qn[b(χ;P ㊉ q,P × q)] — Ex〜Pn×Q」ft(b(x;P ㊉ q,P × q))],	(3.3)
where Pn and Qn are empirical distributions of the reported samples. We denote X 〜Pn ㊉ Qn ∣ ri
as the conditional distribution when the first variable is fixed with realization ri . Our mechanism is
presented in Algorithm 2.
4	This choice of 2n is for the simplicity of presentation.
6
Under review as a conference paper at ICLR 2020
Algorithm 2 f -scoring mechanism for sample elicitation
1.	Compute b(∙；P ㊉ q,p × q) = argmint(∙)Ex〜Pn×Qn [ft (t(x))] 一 Ex〜P八φQ”[t(x)].
2.	Pay each reported sample ri using:
S(Ti,{rj}j=i) := a + b(Ex〜Pn®Qn∣ri t(x; P ㊉ q,P × q)] 一 Ex〜Pn×Qn∣ri[f t 仅x；P ㊉ q,P × Q川)
for some constants a, b > 0. * 4
Similar to Algorithm 1, the main step in Algorithm 2 is to estimate the f -divergence between Pn ×
Qn and Pn ㊉ Qn using reported samples. Then We pay agents using a linear-transformed form of it.
We have the following result.
Theorem 3.7. The f -scoring mechanism in Algorithm 2 achieves (2δ(n), 2b(n))-BNE.
The theorem is proved by error bound in estimating f -divergence, a max argument, and the data
processing inequality for f -mutual information. We defer the proof in Section B.2.
The job left for us is to establish the error bound in estimating the f -divergence to obtain (n) and
δ(n). Roughly speaking, if We solve the optimization problem (3.3) via deep neural netWorks With
proper structure, it holds that
δ(n) = exp{-n(d-2β)/(2β+d) log14 n},	C(n) = C ∙ n-2β/(2β+d) log7 n,
Where c is a positive absolute constant. We state and prove this result formally in Section 4.
Remark 3.8. (1) When the number of samples groWs, it holds that δ(n) and C(n) decrease to 0
at least polynomially fast, and our guaranteed approximate incentive-compatibility approaches a
strict one. (2) Our method or frameWork handles arbitrary complex information, Where the data
can be sampled from high dimensional continuous space. (3) The score function requires no prior
knoWledge. Instead, We design estimation methods purely based on reported sample data. (4) Our
frameWork also covers the case Where the mechanism designer has no access to the ground truth,
Which adds contribution to the peer prediction literature. So far peer prediction results focused
on eliciting simple categorical information. Besides handling complex information structure, our
approach can also be vieWed as a data-driven mechanism for peer prediction problems.
4	ESTIMATION OF f-DIVERGENCE
In this section, We introduce an estimator of f -divergence and establish the statistical rate of con-
vergence, Which characterizes C(n) and δ(n). For the simplicity of presentation, in the sequel, We
estimate the f -divergence Df (qkP) betWeen distributions P andQ With probability density functions
P and q, respectively. The rate of convergence of estimating f -divergence can be easily extended to
that of mutual information.
By Section 3, estimating f -divergence betWeen P and Q is equivalent to solving the folloWing opti-
mization problem,
t^ (∙； P,q) = argmin ESp[ft(t(N))] - ESq[t(N)],
t (∙)
Df (qkp) = Ex〜q[t* (N； p,q)] - Ex〜p[ft (t* (n; p,q))].	(4.1)
In What folloWs, We propose an estimator of Df (qkP). By Assumption 3.3, it suffices to solve (4.1)
on the function class Cd (Ω, M). To this end, we approximate solution to (4.1) by the family of deep
neural netWorks.
We now define the family of deep neural networks as follows.
Definition 4.1. Given a vector k = (k0, . . . , kL+1) ∈ NL+2, where k0 = d and kL+1 = 1, the
family of deep neural networks is defined as
Φ(L,k) = {φ(n; W^) = Wl+ισvL …W2σv 1 WiN: Wj ∈ Rk ×kj-1 ,vj ∈ Rkj}.
Here we write σv ( n ) as σ ( n — V) for notational convenience, where σ (∙) is the ReLU activation
function.
7
Under review as a conference paper at ICLR 2020
To avoid overfitting, the sparsity of the deep neural networks is a typical assumption in deep learn-
ing literature. In practice, such a sparsity property is achieved through certain techniques, e.g.,
dropout (Srivastava et al., 2014), or certain network architecture, e.g., convolutional neural network
(Krizhevsky et al., 2012). We now define the family of sparse networks as follows,
Φ M (L,k,s) = {φ (N ； W,v) ∈ Φ(L,d )： kφk∞ ≤ M, ∣∣Wjk∞ ≤ 1 for j ∈ [ L + 1],
L+1	L
kvjk∞ ≤ 1forj ∈ [L], XkWjk0+Xkvjk0 ≤ s},	(4.2)
j=1	j=1
where s is the sparsity. In contrast, another approach to avoid overfitting is to control the norm of
parameters. See Section A.2 for details.
We now propose the following estimators
b(x;p,q) = argmin ESPjft(t(X))] - EsQn [t(X)],
t∈ΦM (L,k,s)
Df (qkp) = Eχ~Qn [b(χ;p,q)] - Eχ~Pn [ft(b(X;p,q))]∙	(4.3)
The following theorem characterizes the statistical rate of convergence of the estimators defined in
(4.3).
Theorem 4.2. LetL = O(logn), s = O(N log n), andk = (d, d, O(dN), O(dN), ∙ ∙ ∙,O(dN),1)
in (4.2), where N = nd/(2β+d). Under Assumptions 3.1, 3.3, and 3.4, it holds with probability at
least 1 — exp{-n(d-2β)/(2β+d) log14 n} that
^	_	2 β	7
lDf (qkp) - Df (qkp) | . n2β+d log n∙
We defer the proof of the theorem in Section B.3. By Theorem 4.2, the estimators in (4.3) achieve
the optimal nonparametric rate of convergence (Stone, 1982) up to a logarithmic term. By (3.2) and
Theorem 4.2, we have
δ(n) = exp{-n(d-2β)/(2β+d) ∙ log14 n},	C(n) = C ∙ n-2β/(2β+d) ∙ log7 n,
where c is a positive absolute constant.
5	Connection to GAN and Reconstruction of Distribution
After sample elicitation, a natural question to ask is how to learn a representative probability density
function from the samples. Denote the probability density function from elicited samples as p. Then,
learning the probability density function p is to solve for
q' = argmin Df (qkp),	(5.1)
q∈Q
where Q is the probability density function space.
To see the connection between (5.1) and the formulation of f -GAN (Nowozin et al., 2016), by
combining (2.2) and (5.1), we have
q = argmin max EXzq[t(X)] - EXzp[ft (t(X))],
q∈Q t
which is the formulation of f -GAN. Here the probability density function q(∙) is the generator, while
the function t (∙) is the discriminator.
By the non-negativity of f -divergence, q* = P solves (5.1). We now propose the following estimator
qb= argmin Db f (qkp),	(5.2)
q∈Q
where Df (qkp) is given in (4.3).
We define covering number as follows.
8
Under review as a conference paper at ICLR 2020
Definition 5.1 (Covering Number). Let (V, ^ ∙ IIL2) be a normed space, and Q ⊂ V. We say that
{v1, . . . , vN} is a δ-covering over Q of size N if Q ⊂ ∪iN=1B(vi, δ), where B(vi, δ) is the δ-ball cen-
tered at vi. The covering number is defined as N2(δ, Q) = min{N : ∃-covering over Q of size N}.
We impose the following assumption on the covering number of the probability density function
space Q.
Assumption 5.2. It holds that N20, Q) = O(exp{1 /δd/(2㈤T}).
Recall that q* = P is the unique minimizer of the problem (5.1). Therefore, the f -divergence
Df (b∣p) characterizes the deviation of q from p*. The following theorem characterizes the error
bound of estimating q* by qb.
Theorem 5.3. Under the same assumptions in Theorem 4.2 and Assumption 5.2, for sufficiently
large sample size n, it holds with probability at least 1 - 1/n that
2β
Df (b∣P) . n-2β+d ∙ log7 n + min Df (e∖∣P).	(5.3)
We defer the proof of the theorem in Section B.4.
In Theorem 5.3, the first term on the RHS of (5.3) characterizes the generalization error of the
estimator in (5.2), while the second term characterizes the approximation error. If the approximation
error in (5.3) vanishes, then the estimator qb converges to the true density function q* = p at the
optimal nonparametric rate of convergence (Stone, 1982) up to a logarithmic term.
6	Concluding remarks
In this work, we introduce the problem of sample elicitation as an alternative to eliciting complicated
distribution. Our elicitation mechanism leverages the variational form of f -divergence functions to
achieve accurate estimation of the divergences using samples. We provide theoretical guarantee for
both our estimators and the achieved incentive compatibility.
It reminds an interesting problem to find out more “organic" mechanisms for sample elicitation that
requires (i) less elicited samples; and (ii) induced strict truthfulness instead of approximated ones.
References
Jacob D Abernethy and Rafael M Frongillo. A characterization of scoring rules for linear properties.
In Conference on Learning Theory, pp. 27-1, 2012.
Martin Arjovsky, Soumith Chintala, and L6on Bottou. Wasserstein GAN. arXiv preprint
arXiv:1701.07875, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (GANs). In International Conference on Machine Learning, pp.
224-232, 2017.
Marc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,
Stephan Hoyer, and Remi Munos. The Cramer distance as a solution to biased Wasserstein gradi-
ents. arXiv preprint arXiv:1705.10743, 2017.
Glenn W. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review,
78(1):1-3, 1950.
Michel Broniatowski and Amor Keziou. Parametric estimation and tests through divergences. Tech-
nical report, Citeseer, 2004.
Michel Broniatowski and Amor Keziou. Parametric estimation and tests through divergences and
the duality technique. Journal of Multivariate Analysis, 100(1):16-36, 2009.
Yuheng Bu, Shaofeng Zou, Yingbin Liang, and Venugopal V Veeravalli. Estimation of KL di-
vergence: Optimal minimax rate. IEEE Transactions on Information Theory, 64(4):2648-2674,
2018.
9
Under review as a conference paper at ICLR 2020
Luca De Alfaro, Michael Shavlovsky, and Vassilis Polychronopoulos. Incentives for truthful peer
grading. arXiv preprint arXiv:1604.03178, 2016.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale
hierarchical image database. In Conference on Computer Vision and Pattern Recognition, pp.
248-255, 2009.
Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain Markov process
expectations for large time. I. Communications on Pure and Applied Mathematics, 28(1):1-47,
1975.
Rafael Frongillo and Ian Kash. On elicitation complexity. In Advances in Neural Information
Processing Systems, pp. 3258-3266, 2015a.
Rafael Frongillo and Ian A Kash. Vector-valued property elicitation. In Conference on Learning
Theory, pp. 710-727, 2015b.
Alice Gao, James R Wright, and Kevin Leyton-Brown. Incentivizing evaluation via limited access
to ground truth: Peer-prediction makes things worse. arXiv preprint arXiv:1606.07042, 2016.
Chao Gao, Yuan Yao, and Weizhi Zhu. Generative adversarial nets for robust scatter estimation: A
proper scoring rule perspective. arXiv preprint arXiv:1903.01944, 2019.
Weihao Gao, Sewoong Oh, and Pramod Viswanath. Density functional estimators with k-nearest
neighbor bandwidths. In International Symposium on Information Theory, pp. 1351-1355, 2017.
Tilmann Gneiting and Adrian E. Raftery. Strictly proper scoring rules, prediction, and estimation.
Journal of the American Statistical Association, 102(477):359-378, 2007.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of Wasserstein GANs. In Advances in Neural Information Processing Systems,
pp. 5767-5777, 2017.
Yanjun Han, Jiantao Jiao, and Tsachy Weissman. Minimax rate-optimal estimation of divergences
between discrete distributions. arXiv preprint arXiv:1605.09124, 2016.
Victor Richmond Jose, Robert F. Nau, and Robert L. Winkler. Scoring rules, generalized entropy
and utility maximization. Working Paper, Fuqua School of Business, Duke University, 2006.
Takafumi Kanamori, Taiji Suzuki, and Masashi Sugiyama. f -divergence estimation and two-sample
homogeneity test under semiparametric density-ratio models. IEEE Transactions on Information
Theory, 58(2):708-720, 2011.
Yuqing Kong and Grant Schoenebeck. Water from two rocks: Maximizing the mutual information.
In Conference on Economics and Computation, pp. 177-194, 2018.
Yuqing Kong and Grant Schoenebeck. An information theoretic framework for designing informa-
tion elicitation mechanisms that reward truth-telling. Transactions on Economics and Computa-
tion, 7(1):2, 2019.
Yuqing Kong, Katrina Ligett, and Grant Schoenebeck. Putting peer prediction under the micro
(economic) scope and making truth-telling focal. In International Conference on Web and Internet
Economics, pp. 251-264. Springer, 2016.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
N.S. Lambert, D.M. Pennock, and Y. Shoham. Eliciting properties of probability distributions. In
Conference on Electronic Commerce, pp. 129-138, 2008.
10
Under review as a conference paper at ICLR 2020
Young KyUng Lee and Byeong U Park. Estimation of KUllback-Leibler divergence by local likeli-
hood. Annals ofthe Institute OfStatistical Mathematics, 58(2):327-340, 2006.
XinggUo Li, JUnwei LU, Zhaoran Wang, Jarvis HaUpt, and TUo Zhao. On tighter generalization
boUnd for deep neUral networks: CNNs, ResNets, and beyond. arXiv preprint arXiv:1806.05159,
2018.
TengyUan Liang. On how well generative adversarial networks learn densities: Nonparametric and
parametric resUlts. arXiv preprint arXiv:1811.03179, 2018.
ShUang LiU, Olivier BoUsqUet, and Kamalika ChaUdhUri. Approximation and convergence proper-
ties of generative adversarial learning. In Advances in Neural Information Processing Systems,
pp. 5545-5553, 2017.
James E. Matheson and Robert L. Winkler. Scoring rUles for continUoUs probability distribUtions.
Management Science, 22(10):1087-1096, 1976.
John McCarthy. MeasUres of the valUe of information. Proceedings of the National Academy of
Sciences of the United States of America, 42(9):654-655, 1956.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
MIT press, 2018.
XUanLong NgUyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence fUnctionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
56(11):5847-5861, 2010.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neUral samplers
Using variational divergence minimization. In Advances in neural information processing systems,
pp. 271-279, 2016.
Avraham Ruderman, Mark Reid, Dario Garcia-Garcia, and James Petterson. Tighter varia-
tional representations of f -divergences via restriction to probability measUres. arXiv preprint
arXiv:1206.4664, 2012.
Leonard J. Savage. Elicitation of personal probabilities and expectations. Journal of the American
Statistical Association, 66(336):783-801, 1971.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation
function. arXiv preprint arXiv:1708.06633, 2017.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine
Learning Research, 15(1):1929-1958, 2014.
Ingo Steinwart, Chlo6 Pasin, Robert Williamson, and Siyu Zhang. Elicitation and identification of
properties. In Conference on Learning Theory, pp. 482-526, 2014.
Charles J Stone. Optimal global rates of convergence for nonparametric regression. The annals of
statistics, pp. 1040-1053, 1982.
Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density Ratio Estimation in Machine
Learning. Cambridge University Press, 2012.
Taiji Suzuki, Masashi Sugiyama, Jun Sese, and Takafumi Kanamori. Approximating mutual infor-
mation by maximum likelihood density ratio estimation. In New challenges for feature selection
in data mining and knowledge discovery, pp. 5-20, 2008.
Sara A van de Geer and Sara van de Geer. Empirical Processes in M-estimation, volume 6. Cam-
bridge university press, 2000.
Qing Wang, Sanjeev R Kulkarni, and Sergio Verdu. Divergence estimation of continuous distri-
butions based on data-dependent partitions. IEEE Transactions on Information Theory, 51(9):
3064-3074, 2005.
11
Under review as a conference paper at ICLR 2020
Qing Wang, Sanjeev R Kulkarni, and Sergio Verdu. Divergence estimation for multidimensional
densities via k-nearest-neighbor distances. IEEE Transactions on Information Theory, 55(5):
2392-2405, 2009.
Robert L. Winkler. Scoring rules and the evaluation of probability assessors. Journal of the American
Statistical Association, 64(327):1073-1078, 1969.
Zhiyi Zhang and Michael Grabchak. Nonparametric estimation of Kullback-Leibler divergence.
Neural computation, 26(11):2570-2593, 2014.
Xingyu Zhou. On the Fenchel duality between strong convexity and Lipschitz continuous gradient.
arXiv preprint arXiv:1803.06573, 2018.
12
Under review as a conference paper at ICLR 2020
A Auxiliary Analysis
A. 1 Auxiliary Results on Sparsity Control
In this section, we provide some auxiliary results on (4.3). We first state an oracle inequality showing
the rate of convergence of bt(x; p, q).
Theorem A.1. Given 0 < ε < 1, for any sample size n satisfies that n & [Y + Y- 1 log(1 /ε)]2,
under Assumptions 3.1, 3.3, and 3.4, it holds that
Ilkb- t*kL2(P) .~ min	Ile- t*kL2(P) + Yn- 1 /2 logn + n- 1 /2PlOg(I/) + Y- 1 IOg(I/)]
t∈ΦM (L,k,s)
With probability at least 1 一 ε ∙ exp(-Y2). Here Y = s1/2 lOg(V 2L) and V = QjL=+01(kj + 1).
We defer the proof of to Section B.5.
As a by-product, note that t^ (x;p, q) = f 0(θ* (x;p, q)) = f 0(q(x)/p(x)), based on the error bound
established in Theorem A.1, We obtain the folloWing result.
Corollary A.2. Given 0 < ε < 1, for the sample size n & [y + Y-1 log(1 /ε)]2, under Assumptions
3.1,	3.3, and 3.4, it holds with probability at least 1 一 ε ∙ exp(-Y2) that
Ilb - θ*l∣L2(P) . ~ min	Ile- t*IL2(P) + Yn- 11 / logn + n- 1 /2[plog(1 /ε) + Y- 1 log(1 /ε)].
te∈ΦM (L,k,s)
Here Y = s1/2 lOg(V 2L) and V = QjL=+01(kj + 1).
Proof. Note that (f 0) -1 = (f *) 0 and f * has Lipschitz continuous gradient with parameter 1 /μo
from Assumption 3.4 and Lemma D.6, we obtain the result from Theorem A.1.	□
A.2 Error Bound using Norm Control
In this section, we consider using norm of the parameters (specifically speaking, the norm of Wj and
vj in (4.1)) to control the error bound, which is an alternative of the network model shown in (4.2).
We consider the family of L-layer neural networks with bounded spectral norm for weight matrices
W = {Wj ∈ Rkj ×kj-1 }jL=+11, where k0 = d and kL+1 = 1, and vector v = {vj ∈ Rkj }jL=1, which
is denoted as
Φnorm = Φ∏orm(L,k,A,B) = {夕(X；卬山)∈ Φ(L,k): M∣∣2 ≤ A.- forall j ∈ [L],
IWjI2 ≤ Bjforallj ∈ [L + 1]},	(A.1)
where σvj (x) is short for σ(x - vj) for any j ∈ [L]. We write the following optimization problem,
b(x；p, q) = argmin ESP」f *(t(X))] - EsQn [t(X)],
t∈Φnorm
Df (q∣p) = Ex〜Qn[b(X;p,q)] - Ex〜Pn[f*(b(X;p,q))]∙	(a.2)
Based on this formulation, we derive the error bound on the estimated f -divergence in the following
theorem. We only consider the generalization error bound in this setting. Therefore, we assume that
the ground truth t^ (x; p, q) = f 0(q(x)/p(x)) locates within Φnorm∙ Before we state the theorem, we
first define two parameters for the family of neural networks Φnorm(L, k, A, B) as follows
L+1	uL+1
Y 1 = B Y Bj ∙ t X k2,
j=1	j=0
Y2
L ∙( 后T2 + EL Aj) ∙ X Aj∙	(A.3)
P L=0 kj ∙ minj Bj	j=1
We proceed to state the theorem.
Theorem A.3. We assume that t (x;p, q) ∈ Φnorm∙ Then for any 0 < ε < 1, with probability at
least 1 - ε, it holds that
L +1
IDbf (q∣p) - Df (q∣p)| . Y 1 ∙ n- 112 log(Y2n) + ∏ Bj ∙ n- 112plog(1 /ε).
j=1
Here Y1 and Y2 are defined in (A.3).
13
Under review as a conference paper at ICLR 2020
We defer the proof to Section B.6.
The next theorem uses the results in Theorem A.3. Recall that in Section §A.2, we assume that
the minimizer t to the population version problem (4.1) lies within the norm-controlled family of
neural networks Φnorm(L, k, A, B).
Theorem A.4. Recall that we defined the parameter γ1 and γ2 of the family of neural networks
Φ∩orm(L, k, A, B) in (A.3), the estimated distribution q in (5.2), and the ground truth q* = p. We
denote the the covering number of the probability distribution function class Q as N2 (δ, Q), then
for any 0 < ε < 1, with probability at least 1 - ε, we have
L+1
Df (qbkp) . b2(n,γ1,γ2) +	Bj
j=1
- n- 1 /2 ∙ √log(N2[b2(n,γ 1 ,γ2), Q]/ε) + minDf (e∖kp),
qe∈Q
where b2(n, Y 1, Y2) = Y 1 n- 1 /2 log(γ2n).
We defer the proof to Section B.7.
B Proofs of Theorems
B.1 Proof of Theorem 3.5
If the player truthfully reports, she will receive the following expected payment per sample i: with
probability at least 1 - δ(n),
E[S(ri, ■)] ：= a — b(ESQ“[q(x)] - Eg〜P”[ft(q(Xi))])
=a — b ∙ Df(q∖p)
≥ a — b ∙ (Df(q∖p) + E(n)) (sample complexity guarantee)
≥ a — b ∙ (Df (Plp) + E(n)) (agent believesP = q)
= a — bE(n)
Similarly, any misreporting according to a distribution Pe with distribution P will lead to the following
derivation with probability at least 1 — δ
E[S(ri, ∙)] ：= a — b(ESQn [b(x)] — Eg〜Pn [ft(b:Xi))])
_ ^ , .. . i
=a —b ∙ Df(q∖p)
≤ a — b ∙ (Df ( p∖p) -E (n))
≤ a + bE(n) (non-negativity of Df)
Combining above, and using union bound, leads to (2δ(n), 2bE(n))-properness.
B.2 Proof of Theorem 3.7
Consider an arbitrary agent i. Suppose every other agent truthfully reports.
E[ S (∏,{rj}j=i)] = a + b (Ex 〜P “© Q “^ [b>)] —Ex 〜P n× Q n∖ri{ft(tb(x))})
=a + b E[Ex 〜P nφ Q n∖ri [b: X )] — Ex 〜P n× Q “|八{ft £(x)) }]
14
Under review as a conference paper at ICLR 2020
Consider the divergence term E[Eχ〜P„®Q“∣∏[b(x)] — Ex〜Pn×Q{ft(b(x))}]. Reporting a r 〜
P 6= P (denoting its distribution as pe) leads to the following score
E ri 〜e n [Ex 〜e nφ Q n|ri [b(x)] — Ex 〜e n× Q n∣∏	^) H
=Ex 〜e n® Q n 而 X)] — Ex 〜e n× Q 八{f t IX)) } (t0Wer ProPerty)
W m ax Ex 〜e n® Q n [ t(X)] — Ex 〜e n× Q ”{f t (t (X)) } CmaX)
^ . - - .. . . 、
=Dbf (P ㊉ q∣P × q)
≤ Df (P ㊉ qkp × q) + E(n)
= If (pe; q) + (n) (definition)
≤ If (P; q) + E(n) (data Processing inequality (Kong & Schoenebeck, 2019))
with probability at least 1 一 δ(n) (the other δ(n) probability with maximum score S).
NoW We Prove that truthful rePorting leads at least
If (P; q) 一 E(n)
of the divergence term:
EXiZPn [Ex〜Pn®Qn∣Xi [b((x)] 一 Ex 〜P n× Q n∖Xi {f t (b(x)) }]
= ExZPn®Qn [tb(X)] 一 ExZPn×Qn{ft(bt(X))} (tower property)
a / i U	∖
=Df (p ㊉ qkp × q)
≥ Df (p ㊉ qkp × q) — E(n)
= If (p; q) 一 E(n) (definition)
with probability at least 1 一 δ(n) (the other δ(n) probability with score at least 0). Therefore the
expected divergence terms differ at most by 2E(n) with probability at least 1 一 2δ(n) (via union
bound). The above combines to establish a (2δ(n), 2bE(n))-BNE.
B.3 Proof of Theorem 4.2
Step 1. We proceed to bound ∣∣t* — bL 2(P). We first proceed to find some t ∈ Φ M (L,k,s). Note that
the ground truth t^ lies on a finite support Ω ⊂ [a, b]d. To invoke Theorem D.5, we denote t0(y)=
t^((b — a)y + a 1 d), where 1 d = (1, 1,..., 1)T ∈ Rd. Then the support of t0 lies in the unit cube
[0, 1]d. We choose L0 = O(log n), s0 = O(N log n), k0 = (d,O(dN),O(dN),...,O(dN),1),and
m0 = log n, we then utilize Theorem D.5 to construct some te0 ∈ ΦM (L0, k0, s0) such that
Ilt0 一 t0kL∞([0, 1]d) . N -β/.
We further define t(∙) = t0 ◦ '(∙), where '(∙) is a linear mapping taking the following form
' (x)
x
b—a
a
b—a
, 1 d.
	
To this end, we know that t ∈ ΦM (L, k, s), with parameters L, k, and s given in the statement of
Theorem 4.2. We fix this t and invoke TheoremA.1, then with probability at least 1 — ε ∙ exp( 一q2),
we have
kb— t^kL 2 (P) . Il e — t*kL 2(P) + Tn-1 / 2 log n + n-1 / 2 Plog(I / ) + Y1 log(1 / )]
.N-e/d + γn-1 /2 logn + n-1 /2[plog(1 /ε) + γ-1 log(1 /ε)].	(B.1)
Note that γ takes the form γ = s1/2 log(V 2L), where V = O(dL , NL) and L, s given in the
statement of Theorem 4.2, it holds that γ = O(N1/2 log5/2 n). Moreover, by the choice N =
nd/(2β+d), combining (B.1) and taking ε = 1 /n, we know that
kb — t*l∣L2(P) . n-β/(2β+d) log7/2 n	(B.2)
15
Under review as a conference paper at ICLR 2020
with probability at least 1 — exp{-nd/(2β+d) log5 n}.
Step 2. We denote by L(t) = E方〜q[t(x)] — EsP[f *(t(x))] and L(t) = EsQ/t(x)]—
Eχ~p/f , (t(x))]. Then from Assumption 3.4 and LemmaD.6, we know that L(∙) is strongly convex
with a constant coefficient. Note that by triangular inequality, we have
-ɪʌɪ-	-ɪʌɪ- V⅛	.ɪʌɪ-	-ɪʌɪ- V⅛	.ɪʌɪ.
Df (qkp) — Df (qkp)| = |L(t) — L(t*)| ≤ |L(t*) — Lt)| + |L(t*) — L(t*)| =: 4 + Λ2.
We proceed to bound A1 and A2 .
Bound on A1： Recall that L(∙) is strongly convex. Consequently, we have
A1 . kt* — 和L2(P) . n-2β+d log7/2 n,
with probability at least 1 — exp{—nd/(2β+d) log5 n}, where the last inequality comes from (B.2).
Bound on A2: Note that both the functions t*(∙) and f,(t*(∙)) are bounded, then by HOeffding's
inequality, we obtain that
P(A2 ≤ n- 2β+ d log7/2 n) ≥ 1 — exp{—n(d-2β)/(2β+d) log14 n}.
Therefore, by combining the above two bounds, we obtain that
Df (qkp) — Df (qkp) | . n-2β+d log7/2n
with probability at least 1 — exp {—n (d-2β) / (2β+d) log14 n}. This concludes the proof of the theorem.
B.4 Proof of Theorem 5.3
We first need to bound the max deviation of the estimated f -divergence Df (qkp) among all q ∈ Q.
The following lemma provides such a bound.
Lemma B.1. Under the assumptions stated in Theorem 5.3, for any fixed density p, if the sample
size n is sufficiently large, it holds that
^	_	2 β	7
sup ∣Df (qkp) — Df (qkp) | . n-2β+d ∙ log n
q∈Q
with probability at least 1 — 1/n.
We defer the proof to Section C.1.
Now we turn to the proof of the theorem. We denote by qe0 = argminqe∈Q Df (qekp), then with
probability at least 1 — 1/n, we have
,... ,... ^ ,... ^ ,...
Df (qbkp) ≤ |Df (qbkp) — Dbf(qbkp)| +Dbf(qbkp)
2β
≤ SUp ∣Df (qkp) — Dbf (qkp) | + Dbf (ekp) . n-2β+d ∙ log7 n + Df (ekp).	(B.3)
Here in the second line we use the optimality of qb among all qe ∈ Q to the problem (5.2),
while the last inequality uses Lemma B.1 and Theorem 4.2. Moreover, note that Df (qe0 kp) =
minqe∈Q Df (qekp), combining (B.3), it holds that with probability at least 1 — 1/n,
Df (bp) . n- 2β+d ∙ log7 n + min Df (e∖∖p).
qe∈Q
This concludes the proof of the theorem.
B.5 Proof of Theorem A.1
For any real-valued function %, we write Ep(%) = Eχ~p[%(x)], Eq(%) = Eχ~q[%(x)], EPn(%)=
Eχzpn [%(x)], and EQn (%) = Eχ~Qn [%(x)] for notational convenience.
For any t ∈ ΦM (L, k, s), we establish the following lemma.
16
Under review as a conference paper at ICLR 2020
Lemma B.2. Under the assumptions stated in Theorem A.1, it holds that
1 / (4 L O) ∙ Ilb - ekL 2(P) ≤ 1 /μ 0 ∙ kt - tkL 2(P) ∙ k t - t IIl 2(P) + {EQ」(t - t e/ 2] - EQ[(t - t e/ 2] ]}
-{ EPjft ((b+ e) / 2) - ft (e)] - Ep [ ft ((b+ e) / 2) - ft (e)]}
Here μo and Lo are specified in Assumption 3.4.
We defer the proof to Section C.2.
Note that by Lemma B.2 and the fact that ft is Lipschitz continuous, we have
k t - ek L 2(P) . k t - tkL 2(P) ∙ k t - t^kL 2(P) + {EQ」(t - t) /2] - EQ[(t - e) /2] ]}
-{EPn[ft((bt+et)/2) -ft(et)]-EP[ft((tb+et)/2)-ft(et)]}.	(B.4)
Furthermore, to bound the RHS of the above inequality, we establish the following lemma.
Lemma B.3. We assume that the function ψ : R → R is Lipschitz continuous and bounded such
that ∣ψ(x)I ≤ Mo for any ∣χ∣ ≤ M. Then under the assumptions stated in Theorem A.1, for any
fixed t(x) ∈ ΦM, n & [Y + Y-1 log(1 /ε)]2 and 0 < ε < 1, we have the follows
∏J	IEP [ Ψ (t) - ψ (t)] - Ep [ ψ (t) - ψ (t )]|	/ 八
P< sup I Pn ψ '-ψ " 〜Pψ '-ψ 川 ≤ 16M0 ≥ ≥ 1 - ε ∙ exp(-γ2),
U(-)∈φM(L,k,s) η(n,Y,ε) ∙ kψ(t) — ψ(t)IIl2(P) ∨ λ(n,Y,ε)	J
where η (n, γ,ε) = n-112[ Y log n+Y-1 log(1 /ε)], λ (n, γ, ε) = n-1[ Y 2 +log(1 /ε)], and for any real
numbers c1 and c2, we denote by c1 ∨ c2 = max{c1, c2}. Here Y takes the form Y = s1/2 log(V 2L),
where V = QjL=+01(kj + 1).
We defer the proof to Section C.3.
Note that the results in Lemma B.3 also apply to the distribution Q, and by using the fact that the
true density ratio θ* (x; p, q) = q(x)/p(x) is bounded below and above, we know that L2 (Q) is
indeed equivalent to L2(P). We thus focus on L2 (P) here. By (B.4), Lemma B.3, and the Lipschitz
property of ft according to Lemma D.6, with probability at least 1 - ε ∙ exp(-Y2), we have the
following bound
kt - tkL2(P) . kb - ekL2(P) ∙ ke - t kL2(P)
+ O(n-112[Y log n + Y-1 log(1 /ε)] ∙ kt - ekL2(p) ∨ n-1[Y2 + log(1 /ε)]),	(B.5)
where we recall that the notation Y = s1/2 log(V 2L) is a parameter related with the family of neural
networks ΦM. We proceed to analyze the dominant part on the RHS of (B.5).
Case1.	If the term k b-ekL 2(P) ∙k t-t^kL 2(P) dominates, then with probability at least 1 -ε∙ exp(-Y2)
k t - tkL 2(P) . kt - t^kL 2 (P) ∙
Case 2.	If the term O (n-112[ y log n + Y T log(1 /ε)] ∙ k t - ekL 2(p)) dominates, then with probability
at least 1 - ε ∙ exp(-γ2)
kt - ekL2(P) . n- 112[Y log n + Y- 1 log(1 /ε)] ∙
Case 3.	If the term O (n-1[ γ2 +log(1 /ε)]) dominates, then with probability at least 1 - ε∙ exp(-γ2)
kt - ekL2(P) . n- 112[Y + Plog(1 /ε)] ∙
Therefore, by combining the above three cases, we have
k b - ekL 2 (P) . kt - t*kL 2 (P) + YnT12 log n + n- 112[plog(1 /ε ) + Y - 1 log(1 IE )] ∙
Further the triangular inequality gives us
kt - t*kL 2 (P) . ke - t*kL 2 (P) + γn-112 log n + n-112 [Plog(1 1) + γ-1 log(1 1)]
with probability at least 1 - ε ∙ exp(-γ2). Note that the above error bound holds for any t ∈
Φ M (L, k, S), especially for the choice t such that it minimizes k t - t^kL 2(p). Therefore, we have
k b - t*kL 2 (P) .~ min k e - t*kL 2 (P) + YnT12 log n + n- 112[plog(1 /ε ) + Y-1 log(1 /ε )]
te∈ΦM (L,k,s)
with probability at least 1 - ε ∙ exp(-γ2). This concludes the proof of the theorem.
17
Under review as a conference paper at ICLR 2020
B.6 Proof of Theorem A.3
We follow the proof in Li et al. (2018). We denote by the loss function in (A.2) as L[t(x)] =
f (t(xI)) 一 t(xII), where xI follows the distribution P and xII follows Q. To prove the theorem,
we first link the generalization error in our theorem to the empirical Rademacher complexity (ERC).
Given the data {xi}in=1, the ERC related with the class L(Φnorm) is defined as
Rn [L(Φnorm)] = EE	SuP
月 ∈ φnorm
1n
| nfεi∙ L [ 中(Xi ； W,v)] ι{xi}n=ι
i=1
(B.6)
where ε%'s are i.i.d. Rademacher random variables, i.e., P(ε% = 1) = P(ε% = 一 1) = 1 /2. Here the
expectation EE(∙) is taken over the Rademacher random variables 抬力记[n].
We introduce the following Lemma B.4 (Mohri et al., 2018), which links the ERC to the generaliza-
tion error bound.
Lemma B.4. Assume that SuPφ∈φnorm ∣L(φ)| ≤ Mι, then for any ε〉0, with probability at least
1 一 ε, we have
SUP /e χ{L [ φ ( X ； W,V )]} - - X L [ φ (Xi ； W,V )β> . R n [ L (Φnorm)] + M1 ∙ n-1 / 2plog(1 /ε ),
ψ∈Φnorm I	n M	J
where the expectation Eχ{∙} is taken over xI 〜 P and xII 〜 Q.
Equipped with the above lemma, we only need to bound the ERC defined in (B.6).
Lemma B.5. Let L be a Lipschitz continuous loss function and Φnorm be the family of networks
defined in (A.1). We assume that the input x ∈ Rd is bounded such that kxk2 ≤ B. Then it holds
that
Rn [L(Φnorm)] . Y1 - n- 112 lθg(γ2n),
where γ1 and γ2 are given in (A.3).
We defer the proof to Section C.4.
Now we proceed to prove the theorem. Recall that we assume that t^ ∈ Φnorm. For notational
convenience, we denote by
Hb(t) = Ex〜P」ft(t(X))] - Ex〜Q/t(X)],	H(t) = Ex〜p[f t(t(X))] - Ex〜q[t(X)].
Then E[ H (t)] = H (t). We proceed to bound ∣Db f(q∣p) - Df (qkp) | = H (t) - H (t^) |. Note that if
H (b) ≥ H (t), then we have
0 ≤ Hb(b) - H(t^) ≤ Hb(t^) - H(t^),	(B.7)
where the second inequality follows from the fact that t is the minimizer of H (∙). On the other hand,
if H(t) ≤ H(t^), we have
0 ≥ Hb(b) - H(t^) ≥ Hb(b) - H(b),	(B.8)
where the second inequality follows that fact that t^ is the minimizer of H(∙). Therefore, by (B.7),
(B.8), and the fact that L(φ) . QL+1 Bj for any φ ∈ Φ∩orm, we deduce that
L+1
H(b - H(t*)| ≤ 晨P |H(t) - H(t)| . Rn[L(Φnorm)] + j∏ Bj - n- 112√log(1 /ε)	(B.9)
with probability at least 1 - ε. Here the second inequality follows from Lemma B.4. By plugging
the result from Lemma B.5 into (B.9), we deduce that with probability at least 1 - ε, it holds that
L +1
∣Dbf (qkp) - Df (qkp)| = H(b) - H(t*)| . Yι ∙ n- 112 log(γ2n) + ɪɪ Bj ∙ n- 112Plog(1 /ε).
j =1
This concludes the proof of the theorem.
18
Under review as a conference paper at ICLR 2020
B.7 Proof of Theorem A.4
We first need to bound the max deviation of the estimated f -divergence Df (qkp) among all q ∈ Q.
We utilize the following lemma to provide such a bound.
Lemma B.6. Assume that the distribution q is in the set Q, and we denote its L2 covering number
as N2(δ, Q). Then for any target distribution p, we have
L+1
max|Df(qkp) - Dbf(qkp)| . b2(n,γ1,γ2) +	Bj
q∈Q
j=1
- n-1 /2 - √log(N2[b2(n,γ 1 ,γ2), Q]/ε)
with probability at least 1 一 ε. Here b2(n, Y1 ,γ2) = Y1 n-1 /2 log(Y2n) and C is a positive absolute
constant.
We defer the proof to Section C.5.
Now we turn to the proof of the theorem. We denote by qe0 = argminqe∈Q Df (qekp). Then with
probability at least 1 一 ε, we have
Df (qbkp) ≤ |Df (qbkp) 一 Dbf(qbkp)| +Dbf(qbkp)
≤ mq∈aQx |Df (qkp) 一 Dbf(qkp)| +Dbf(qe0kp)
L+1
.b2(n,γ 1,γ2) + ɪɪ Bj - n-1 /2 - Plog(N2 [b2(n, γ 1 ,γ2), Q]/ε) + Df(TIP),
j=1
where we use the optimality of qb among all qe ∈ Q to the problem (5.2) in the second inequal-
ity, and we uses Lemma B.6 and Theorem 4.2 in the last line. Moreover, note that Df(qe0kp) =
minqe∈Q Df (qekp), we obtain that
L+1
Df (qbkp) . b2(n,Y1,Y2) +	Bj
j=1
-n-1 /2√log(N2[b2(n,γ 1 ,γ2), Q]/ε) + minDf (e∖∖p).
This concludes the proof of the theorem.
C Lemmas and Proofs
C.1 Proof of Lemma B.1
Recall that the covering number of Q is N2 (δ, Q), we thus assume that there exists
q1, . . . , qN2(δ,Q) ∈ Q such that for any q ∈ Q, there exists some qk, where 1 ≤ k ≤ N2(δ, Q),
so that ∖q 一 qk∖2 ≤ δ. Moreover, by taking δ = δn = n-2β/(2β+d) and union bound, We have
^	_ 2 β	7
P [sup ∣Df (q∖p) — Df (q∖p) | ≥ c 1 - n- 2 β+d - log7 n ]
q∈Q
N2(δn,Q)	2β
≤ X	P[lDf(qk kp) 一 Df (qk kp)1 ≥ c 1 - n- 2ββd - log7 n]
k=1
d-2β	1 a
≤ N2(δn, Q) - exp( —n2β+d - log14 n),
where the last line comes from Theorem 4.2. Combining Assumption 5.2, when n is sufficiently
large, it holds that
^	_ 2 β	7
P[sup lDf(qkp) 一 Df(qkp)l ≥ c 1 - n- 2β+d - log7 n] ≤ 1 /n,
q∈Q
which concludes the proof of the lemma.
19
Under review as a conference paper at ICLR 2020
C.2 Proof of Lemma B.2
For any real-valued function %, We write Ep(%) = Eχ~p[%(x)], Eq(%) = Eχ~q[%(x)], EPn(%)
Eχ~pn [%(x)], and EQn (%) = Eχ~Qn [%(x)] for notational convenience.
1 ʌ .1	1	(`	∙ . ∙	C 个♦	/∕C∖	1
By the definition of t in (4.3), we have
EP n [ f ' (b)] - EQ n (t) ≤ EP n [ f ^ (t)] - EQ n (t) ∙
Note that the functional G(t) = EPn [ f * (t)] 一 EQn (t) is convex in t since f * is convex, we then have
G(b+e) - G(t) ≤ G⑶ 一 CG⑶ ≤ 0.
By re-arranging terms, we have
{EPn[f*((bt+et)/2) - f *(et)] - EP[f *((tb+ te)/2) - f *(et)]} - {EQn [(tb- et)/2] - EQ[(bt - et)/2]}
≤ EQ[(bt - et)/2] -EP[f*((bt+et)/2) - f*(te)].	(C.1)
We denote by
Bf (et, t) =EP[f*(t)-f*(et)] - EQ(t - et).	(C.2)
then the RHS of (C.1) is exactly -Bf (t, (t + t)/2). We proceed to establish the lower bound of
Bf (e,t) using L2(P) norm. From t^(x;p,q) = f (q(x)/p(x)) and (f *)0 o (f)(X) = X, we know
that q/p = ∂f * (t^)/∂t. Then by substituting the second term on the RHS of (C.2) using the above
relationship, we have
Bf (e,t) = EP f * (11- f * (t)-∂t~(t) , (t - t)]
=EP f *(t) - f * (e) - -KΓ(t ∙ (t - e∖ + ep([^7≡T (t - ~KΓ(t*) ∙ (t - 11
= A1+A2 .
We lower bound A1 and A2 in the sequel.
(C.3)
Bound on A1 . Note that by Assumption 3.4 and Lemma D.6, we know that the Fenchel duality f*
is strongly convex with parameter 1/L0. This gives that
f *(t(X)) - f *(e(X)) - ∂(t(x)) ∙ [t(X) - e(X)] ≥ 1 /L0 ∙ (t(X) - e(X))2
for any x. Consequently, it holds that
.	,_	—	∙ C
A1 ≥ 1/L0 ∙ Ilt - tkL2(P).	(C.4)
Bound on A2. By Cauchy-Schwarz inequality, it holds that
Again, by Assumption 3.4 and Lemma D.6, we know that the Fenchel duality f * has 1 /μO-Lipschitz
gradient, which gives that
I~∂t~(t(x)) —∂t~(t(x)) ≤1 /μo ∙ lt(x) -t(x)l
for any x. By this, the term A2 is lower bounded:
A2 ≥ -1 /μO ∙ kt - t* kL2(P) ∙ kt - ekL2(P).	(C∙5)
Plugging (C.4) and (C.5) into (C.3), we have
Bf (e,t) ≥ 1/L 0 ∙ kt - ek L 2(P) - 1 /μ 0 ∙ kt - t^kL 2(P) ∙ kt - ekL 2(P)∙
By this, together with (C.1), we conclude that
1 / (4L 0) ∙ kt - ekL 2(P) ≤ 1 /μ 0 ∙ kt - tkL 2 (P) ∙ kt - t*kL 2 (P) + { eQ n [(b - t)/2] - EQ[(t - t) / 2] }
-{EPn[f*((tb+et)/2)-f*(et)] -EP[f*((tb+te)/2)-f*(et)]}.
This concludes the proof of the lemma.
20
Under review as a conference paper at ICLR 2020
C.3 Proof of Lemma B.3
For any real-valued function %, We write Ep(%) = Eχ~p[%(x)], Eq(%) = Eχ~q[%(x)], EPn(%)=
Eχ~pn [%(x)], and EQn (%) = Eχ~Qn [%(x)] for notational convenience.
We first introduce the following concepts. For any K > 0, the Bernstein difference PK p(t) of t(∙)
with respect to the distribution P is defined to be
P K, p( t ) = 2 K 2 ∙ Ep[exp( ∣t∣∕K) - 1 — ∣t∣∕K ].
Correspondingly, we denote by HK,B the generalized entropy with bracketing induced by the Bern-
stein difference PK,P . We denote by Hs,B the entropy with bracketing induced by Ls norm, Hs the
entropy induced by Ls norm, HLs(P),B the entropy with bracketing induced by Ls (P) norm, and
HLs(P) the regular entropy induced by Ls (P) norm.
Since we focus on fixed L, k, and s, we denote by ΦM = ΦM(L, k, s) for notational convenience.
We consider the space
ΨM = ψ(ΦM) = {ψ(t) : t(x) ∈ ΦM}.
For any δ > 0, we denote the following space
ΨM (δ) = {ψ(t) ∈ ΨM : kψ(t) - ψ(et)kL2(P) ≤ δ},
Ψ0M(δ) = {∆ψ(t) = ψ(t) - ψ(te) : ψ(t) ∈ ΨM (δ)}.
Note that sup∆ψ(t)∈Ψ0M(δ) k∆ψ(t)k∞ ≤ 2M0 and sup∆ψ(t)∈Ψ0M(δ) k∆ψ(t)k∞ ≤ δ, by Lemma
D.4 we have
sup	P8M0,p[∆ψ(t)] ≤ √2δ.
∆ψ(t)∈Ψ0M (δ)
To invoke Theorem D.3 for G = ΨM(δ), we pick K = 8M0 and R = √2δ. Note that from the
fact that sup∆ψ(t)∈Ψ0M (δ) k∆ψ(t)k∞ ≤ 2M0, by Lemma D.1, Lemma D.2, and the fact that ψ is
Lipschitz continuous, we have
H8M0,b (U, ψM(δ), P) ≤ H∞(u/(2√2), ΨM(δ)) ≤ 2(S +1)log(4√2UT(L + 1) V2)
for any U > 0. Then, by algebra, we have the follows
R
1/2
H8M0,B
00
(u, ΨM (δ), P) du ≤ 3s112δ ∙ log(8V2L∕δ).
For any 0 < ε < 1, we take C = 1, and a, C1 and C0 in Theorem D.3 to be
a = 8Mo log(exp(γ2)/ε)γ-1 ∙ δ,
Co = 6 Mo Y - Iplog(exp(γ 2)∕ε),
C1 = 33M2Y-2 log(exp(γ2)∕ε).
Here γ = s1/2 log(V2L). Then it is straightforward to check that our choice above satisfies the
conditions in Theorem D.3 for any δ such that δ ≥ Yn-1/2, when n is sufficiently large such that
n & [Y + Y-1 log(1∕ε)]2. Consequently, by Theorem D.3, for δ ≥ Yn-1/2, we have
P{ sup	|Epn [Ψ(t)	—	Ψ(e)]	-	Ep[Ψ(t)	—	Ψ(e)]|	≥ 8Mo log(eXP(Y2)∕ε)Y-1	∙ δ ∙ n-112}
t(x)∈ΦM (δ)
=P{	sup	|Epn[∆ψ(t)] — Ep[∆ψ(t)]| ≥ 8Mo log(exp(Y2)∕ε)Y-1 ∙ δ ∙ n-112}
∆ψ(t)∈Ψ0M(δ)
≤ ε ∙ exp(—Y2).
By taking δ = δn = Yn-112, we have
P sup
t(x)∈ΦM (δ)
। EP n [ ψ (t) - ψ (t)] - EP [ ψ (t) - ψ (t)]|
n-1[ Y 2 +log(1 ∕ε)]
≤ 8Mo ≥ ≥ 1 — ε ∙ exp(-Y2)
(C.6)
21
Under review as a conference paper at ICLR 2020
On the other hand, We denote that S = min{s > 1 : 2-S(2M0) < 6" = O(log(Y-1 n1 /2)). For
notational convenience, we denote the set
AS =	{ψ(t) ∈	ΨM	:	ψ(t)	∈	ΨM (2-S+2M0), ψ(t)	∈/	ΨM(2-S+1M0)}.	(C.7)
Then by the peeling device, We have the folloWing
***** *****
p/ sup	IEP」ψ(()- ψ()] - EP[以()- ψ(5 ≥ 16Mo]
U (t) ∈ Ψ M ,ψ( t) ∈ ψ M (δn )	kψ (( ) - ψ ( ( ) l∣L 2(P) ∙ T ( n,Y,ε )	J
A	,	___ -	,	、	，< r	_ -	,	、	，< r .	、
≤ XX P{ψ(s u^≡≡^2≡=M≡≠≡ ≥ 16 M0 ∙ T (n,Y,ε)}
S
≤X	P{	sup	IEPn[ψ(()	-	ψ((e)]	- EP[ψ(() -	ψ(e()]I	≥ 8M0∙	(2-S+2M0)	∙T(n,γ,ε)}
S=1	ψ(t)∈As
S
≤X	P{	sup	IEPn[ψ(()	-	ψ(e()]	-EP[ψ(()	-	ψ(e()]I	≥	8M0∙ (2-S+2M0)	∙T(n,γ,ε)}
S=1	ψ(t)∈ΨM (2-s+2M0)
≤S ∙ ε ∙ exp(-Y2)/ log(Y-1 n112) = C ∙ ε ∙ exp(-Y2),
Where c is a positive absolute constant, and for notational convenience We denote by T(n, γ, ε) =
γ-1 ∙ n-1 /2 log(log(γ-1 n1 /2) exp(Y2)/ε). Here in the second line, we use the fact that for any
ψ(() ∈ AS, We have lψ(() - ψ(e()lL2(Q) ≥ 2-S+1M0 by the definition of AS in (C.7); in the forth
line, we use the argument that since AS ⊆ ΨM(2-S+2M0), the probability of supremum taken over
ΨM (2-S+2M0) is larger than the one overAS; in the last line we invoke Theorem D.3. Consequently,
this gives us
P
sup
ψ(t)∈ΨM
ψ (t )∈ Ψ M ( δn )
∣Ep n 产(()-ψ (()] - Ep[ψ (()-ψ (()]∣
kψ(() - ψ(e)IIl2(P) ∙ n-1 /2Ylogn + Y- 1log(1 /)]
≤ 16M0 ≥ 1 - ε ∙ exp(-Y 2 )
(C.8)
Combining (C.6) and (C.8), we finish the proof of the lemma.
C.4 Proof of Lemma B.5
The proof of the theorem utilizes following two lemmas. The first lemma characterizes the Lipschitz
property of φ (x; W, V) in the input x.
Lemma C.1. Given W and V, then for any φ(∙; W, V) ∈ Φnorm and x 1 ,x2 ∈ Rd, we have
L+1
∣∣^(x 1； W,v) - φ(x2; W,v)k2 ≤ ∣∣x 1 - X2 k2 ∙ ɪɪ Bj.
j=1
We defer the proof to Section C.6.
The following lemma characterizes the LiPschitz property of φ (x; W, V) in the network parameter
pair (W, V).
Lemma C.2. Given any bounded x ∈ Rd such that ∣x∣2 ≤ B, then for any weights
W1	=	{Wj1}jL=+11, W2	=	{Wj2}jL=+11, V1	=	{Vj1}jL=1, V2	=	{Vj2}jL=1,	and functions
φ(∙,W1 ,V1),中(∙,W2,V2) ∈ Φnorm, we have
Il^(x,W1 ,v1) - Ψ(x,W2,v2)k
≤ B'2L + 1 ∙ Qj=1 Bj XLy A
m	minj Bj	£ j ∖
L+1	L
∣Wj1 - Wj2∣2F +	∣Vj1 - Vj2∣22.
j =1	j =1
We defer the proof to Section C.7.
22
Under review as a conference paper at ICLR 2020
We now turn to the proof of Lemma B.5. Note that by Lemma C.2, We know that φ(x; W, V) is
Lw-Lipschitz in the parameter (W, v) ∈ Rb, where the dimension b takes the form
L+1	L	L+1
b=j=1kjkj-1+j=1kj≤j=0(kj+1)2,	(C.9)
and the Lipschitz constant Lw satisfies
B√2L+1 - Q L+1 Bj 3
Lw =--------.	=j=1 j ∙ YjAj.	(C.10)
minj Bj	j=1
In addition, we know that the covering number of W = {(W, v) ∈ Rb : PjL=+11 kWj kF +
PjL=1 kvjk2 ≤ K},where
uL+1	L
K= t	kj2Bj2 +	Aj,
j=1	j=1
(C.11)
satisfies
N(W,δ) ≤ (3Kδ-1)b.
By the above facts, we deduce that the covering number of L(Φnorm) satisfies
N[L(Φnorm),δ] ≤ (c1KLwδ-1)b,
for some positive absolute constant c1. Then by Dudley entropy integral bound on the ERC, we
know that
Rn [L(Φnorm)] ≤ 典0 T + √1n / /log N[L(Φn°πm) ,6] dS,
(C.12)
where 鲁 =SuPg(,；WQ)∈l(φnorm)q∈Rd ∣g(x; W,v) |. Moreover, from Lemma C.1 and the fact that the
loss function is Lipschitz continuous, we have
L+1
Id ≤ C 2 ∙ B ∙ Y Bj
j=1
(C.13)
for some positive absolute constant c2. Therefore, by calculations, we derive from (C.12) that
Rn[L(Φnorm)]
。*∙ /ɪgw),
then we conclude the proof of the lemma by plugging in (C.9), (C.10), (C.11), and (C.13), and using
the definition of γ1 and γ2 in (A.3).
C.5 Proof of Lemma B.6
Remember that the covering number of Q is N2(S, Q), we assume that there exists
q1, . . . , qN2(δ,Q) ∈ Q such that for any q ∈ Q, there exists some qk, where 1 ≤ k ≤ N2(S, Q),
so that kq — qkk2 ≤ S. Moreover, by taking S = Y1 n-1 /2 log(Y2n) = b2(n,γ 1 ,γ2) and
N2 = N2[b2(n, γ1,γ2), Q], we have
_ ， . .... ^ .................
P{mq∈aQx |Df (qkp) — Dbf (qkp)| ≥ c ∙
L +1
[b2(n,γ 1 ,γ2) + Y Bj ∙ n-1 /2 ∙ /log(N2/ε)]}
j=1
N2	L+1
≤ XP{∣Df (qkp) — Df (qkp)| ≥ C ∙步2(n,Y 1 ,γ2) + Y Bj ∙ n-1 /2 ∙ Plog(N2/ε)]}
k=1	j=1
≤ N2 ∙ ε∕N2 = ε,
where the second line comes from union bound, and the last line comes from Theorem A.3. By this,
we conclude the proof of the lemma.
23
Under review as a conference paper at ICLR 2020
C.6 Proof of Lemma C.1
The proof follows by applying the Lipschitz property and bounded spectral norm of Wj recursively:
Il夕(N1； W,v) - φ(N2； W,v)Il2 = IlWL +ι(σvL …W2σv】WιXι - Gvl …W2σvι WιX2)∣∣2
≤ IWl+1 12 ∙∣Gvl (Wl …W2σ,1W1X1 - Wl …W2σ,1W1X2)||2
≤ BL +1 ∙ IIWL …W2σv 1 W1x 1 - WL …W2σv 1 W1x2 ∣∣ 2
L+1
≤ …≤ HBj Ilx 1 - X2 Il 2 .
j=1
Here in the third line We uses the fact that ∣∣Wjk2 ≤ Bj and the 1 -LiPschitz property of。嵬$ (∙), and
in the last line we recursively apply the same argument as in the above lines. This concludes the
proof of the lemma.
C.7 Proof of Lemma C.2
Recall that φ (X; W, V) takes the form
φ(X； W,v) = Wl +1 Gvl Wl …。富]W1x.
For notational convenience, we denote by *(x) = σ^ (Wix) for i = 1, 2. By this, φ(x; W, V) has
the form φ(x; Wi, vi) = WL+1 4L ◦ ∙∙∙ ◦ φ∖(X). First, note that for any W1, W2, V1 and V2, by
triangular inequality, we have
照(x,w 1 ,vI)- φ(x, w 2 ,v2) Il 2 = ∣∣wL+1 φ L。…”1( X) - WL+1 ψ L。…◦中 2( X) Il 2
≤ l∣wL+1 ψL。…”1(X)- wL+1 ψL。…”1(X)Il2
+ l∣wL+1 ψ L。…”1(X)- wL+1 ψ L ◦•••◦ ψ2( X) Il 2
≤ l∣wL+1- wL+1 if•闷L。…”1(X)Il2
+ BL+1 ∙ ||比。…。君(X) - φ L。…。中 2( X) Il 2 ∙	(C.14)
Moreover, note that for any ' ∈ [L], we have the following bound on ∣∣夕L。•…。夕 1(X) ∣∣2:
底。…。夕1(X)Il2 ≤ IlW沐'-1。…。夕1(X)Il2 + ∣∣v'∣∣2
≤ b` ∙ Il夕'-1。…。夕1(χ) Il2 + a`
`	``
≤IxI2∙YBj + XAj Y Bi,	(C.15)
j=1	j=1	i=j+1
where the first inequality comes from the triangle inequality, and the second inequality comes from
the bounded spectral norm of Wji , while the last inequality simply applies the previous arguments
recursively. Therefore, combining (C.14), we have
L	LL
Ii^(χ,w 1,vI)-夕(χ,w2,v2)Ii2 ≤ (B∙ YBj+XAj Y Bij ∙1wl+1 -wL+1*
j=1	j=1	i=j+1
+ Bl+1 ∙ Il 夕 L。…。夕 1 (X) - φ	…O φ 2( X) Il 2 ∙	(C.16)
Similarly, by triangular inequality, we have
m L ◦∙∙∙◦ v1( χ)- ψ L ◦∙∙∙◦ ψ2( χ) Il 2
≤ !ψL。ψL-1。…。。1(X)- ψL。ψL-1。…。。1(X)Il2
+ 闷 L。ψ L-1。…。v1( χ)- ψ L。ψ L-1 ^∙∙∙◦ ψ2( χ) Il 2
≤ !ψi。中L-1。…。。1(X)-φL。中L-1。…。。1(X)Il2	(C.17)
+ BL ∙ IkL-I。…。。1(X) -忌-1。…。中2(X) Il2,
24
Under review as a conference paper at ICLR 2020
where the second inequality uses the bounded spectral norm of WL and 1-Lipschitz property of
σvL (∙). For notational convenience, We further denote y = φL-1 ◦•••◦ φ∖(χx), then
陷L(y)-忌(y)Il2 = ∣∣σ(W1 y -VL)-σ(WLy Y)}∣∣2
≤IVL -VL k 2 + ∣W1 - WL blyk 2，
where the inequality comes from the I-LiPschitz property of σ(∙). Moreover, combining (C.15), it
holds that
L-1	L-1	L-1
闷L(y)-中L(y)k2 ≤!v"l -vLk2 + kWL - wLh b ∙ ∏% +	∏ Bi . (c.i8)
j=1 j=1	i=j+1
By (c.17) and (c.18), we have
kψL ◦•••◦ ψ∖(χ)- ψL ◦•••◦ ψ2(χ)Il2
L-1	L-1	L-1
≤kvγL - vL 12 + ∣w1 - wL IF ∙ B ∙∏Bj +	∏ Bi
j=1	j=1	i=j+1
+ bL - kψL-1。…。Ψl(x) - ΨL-1。…。Ψ 1(X) Il2
LL
≤∑ ∏ Bi •忖-v212 +
j=1 i=j+1
B ∙Q L=II Bj
minj Bj
LL
WAjWWjI-W2 If
j =1 j=1
B QL+1 B	L L
≤	I jB ∙ N Aj ∙ >M-V212 + Wl-Wj2 If).
Here in the second inequality we recursively apply the previous arguments. Further combining
(c.16), we obtain that
kψ(x,W1 ,v1) - φ(x,W2,v2) Il2
B QL+1 B L	L+1	L
≤ F⅛ ∙ X Aj ∙(> 吗"If + X∣v1-V212)
B√2T∏ ∙ QL+11 Bj
minj Bj
L
• X Aj ∙
j=1
≤
L+1	L
IWj1 - Wj2I2F +	Ivj1 - vj2I22,
j =1	j =1
where we use cauchy-Schwarz inequality in the last line. This concludes the proof of the lemma.
D Auxiliary Results
Lemma D.1. The following statements for entropy hold.
1.	Suppose that supg∈G IgI∞ ≤ M, then
H4MB (√δ, G,Q) ≤ H2B (δ, G,Q)
for anyδ > 0.
2.	For 1 ≤ q < ∞, and Qa distribution, we have
Hp,b (δ, G, Q) ≤ H∞ (δ/2, G),
for anyδ > 0. Here H∞ is the entropy induced by infinity norm.
3.	Based on the above two statements, suppose that supg∈G IgI∞ ≤ M, we have
H4MB (√2 ∙ δ, G, Q) ≤ H∞ (δ/2, G),
by taking p = 2.
25
Under review as a conference paper at ICLR 2020
Proof. See van de Geer & van de Geer (2000) for a detailed proof.	口
Lemma D.2. The entropy of the neural network set defined in (4.1) satisfies
H∞[δ,ΦM(L,p,s)] ≤ (s + 1) log(2δ-1(L + 1)V 2),
where V = QlL=+01 (pl + 1).
Proof. See Schmidt-Hieber (2017) for a detailed proof.	口
Theorem D.3. Assume that supg∈G ρK (g) ≤ R. Take a, C, C0, and C1 satisfying that a ≤
C1 √R2/K, a ≤ 8√R,a ≥ C0 ∙ [RR HKBB (u, G, P)du ∨ R], and C2 ≥ C2(C1 + 1). It holds
that
P[sup IEp“(g) - Ep(g)1 ≥ a ∙ n-1 /2] ≤Ceχp--C2(0：+ 1)R2).
Proof. See van de Geer & van de Geer (2000) for a detailed proof.	口
Lemma D.4. Suppose that kgk∞ ≤ K, and kgk ≤ R, then ρ22K,p(g) ≤ 2R2. Moreover, for any
K0 ≥ K, we have ρ22K0,p(g) ≤ 2R2.
Proof. See van de Geer & van de Geer (2000) for a detailed proof.	口
Theorem D.5. For any function f in the Holder ball Cd ([0,1]d, K) and any integers m ≥ 1 and
N ≥ (β + 1)d∨ (K + 1), there exists a network f ∈ Φ(L, (d, 12dN, . . . , 12dN, 1), s) with number
of layers L = 8 + (m + 5)(1 + dlog2 de) and number of parameters s ≤ 94d2(β + 1)2dN(m +
6)(1 + dlog2 de), such that
kf - f 屈8([0,1产)≤ (2K + 1)3d +1 N2-m + K2βN-d/d.
Proof. See Schmidt-Hieber (2017) for a detailed proof.	口
Lemma D.6. If the function f is strongly convex with parameter μo > 0 and has Lipschitz continu-
ous gradient with parameter Lo > 0, then the Fenchel duality f * of f is 1 /Lo-strongly convex and
has 1 /μo-Lipschitz continuous gradient (therefore, f * itself is Lipschitz continuous).
Proof. See Zhou (2018) for a detailed proof.	口
26