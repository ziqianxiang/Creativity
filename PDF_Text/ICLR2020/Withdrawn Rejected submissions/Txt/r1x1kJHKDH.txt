Under review as a conference paper at ICLR 2020
PatchVAE: Learning Local Latent Codes for
Recognition
Anonymous authors
Paper under double-blind review
Ab stract
Unsupervised representation learning holds the promise of exploiting large amount
of available unlabeled data to learn general representations. A promising technique
for unsupervised learning is the framework of Variational Auto-encoders (VAEs).
However, unsupervised representations learned by VAEs are significantly outper-
formed by those learned by supervising for recognition. Our hypothesis is that to
learn useful representations for recognition the model needs to be encouraged to
learn about repeating and consistent patterns in data. Drawing inspiration from
the mid-level representation discovery work, we propose PatchVAE, that reasons
about images at patch level. Our key contribution is a bottleneck formulation in a
VAE framework that encourages mid-level style representations. Our experiments
demonstrate that representations learned by our method perform much better on
the recognition tasks compared to those learned by vanilla VAEs.
1	Introduction
Due to the availability of large labeled visual datasets, supervised learning has become the dominant
paradigm for visual recognition. That is, to learn about any new concept, the modus operandi is
to collect thousands of labeled examples for that concept and train a powerful classifier, such as
a deep neural network. This is necessary because the current generation of models based on deep
neural networks require large amounts of labeled data (Sun et al., 2017). This is in stark contrast
to the insights that we have from developmental psychology on how infants develop perception
and cognition without any explicit supervision (Smith & Gasser, 2005). Moreover, the supervised
learning paradigm is ill-suited for applications, such as health care and robotics, where annotated data
is hard to obtain either due to privacy concerns or high cost of expert human annotators. In such cases,
learning from very few labeled images or discovering underlying natural patterns in large amounts of
unlabeled data can have a large number of potential applications. Discovering such patterns from
unlabeled data is the standard setup of unsupervised learning.
Over the past few years, the field of unsupervised learning in computer vision has followed two
seemingly different tracks with different goals: generative modeling and self-supervised learning.
The goal of generative modeling is to learn the probability distribution from which data was generated,
given some training data. A learned model can draw samples from the same distribution or evaluate
the likelihoods of new data. Generative models are also useful for learning compact representation of
images. However, we argue that these representations are not as useful for visual recognition. This is
not surprising since the task of reconstructing images does not require the bottleneck representation
to sort out meaningful data useful for recognition and discard the rest; on the contrary, it encourages
preserving as much information as possible for reconstruction. In comparison, the goal in self-
supervised learning is to learn representations that are useful for recognition. The standard paradigm
is to establish proxy tasks that don’t require human-supervision but can provide signals useful for
recognition. Due to the mismatch in goals of unsupervised learning for visual recognition and the
representations learned from generative modeling, self-supervised learning is a more popular way of
learning representations from unlabeled data. However, fundamental limitation of this self-supervised
paradigm is that we need to define a proxy-task that can mimic the desired recognition. It is not
always possible to establish such a task, nor are these tasks generalizable across recognition tasks.
In this paper, we take the first steps towards enabling the unsupervised generative modeling approach
of VAEs to learn representations useful for recognition. Our key hypothesis is that for a representation
1
Under review as a conference paper at ICLR 2020
to be useful, it should capture just the interesting parts of the images, as opposed to everything in
the images. What constitutes an interesting image part has been defined and studied in earlier works
that pre-date the end-to-end trained deep network methods (Singh et al., 2012; Doersch et al., 2012;
Juneja et al., 2013). Taking inspiration from these works, we propose a novel representation that only
encodes such few parts of an image that are repetitive across the dataset, i.e., the patches that occur
often in images. By avoiding reconstruction of the entire image our method can focus on regions that
are repeating and consistent across many images. In an encoder-decoder based generative model, we
constrain the encoder architecture to learn such repetitive parts - both in terms of representations for
appearance of these parts (or patches in an image) and where these parts occur. We formulate this
using variational auto-encoder (β-VAEs) (Kingma & Welling, 2013; Matthey et al., 2017), where we
impose novel structure on the latent representations. We use discrete latents to model part presence
or absence and continuous latents to model their appearance. We present this approach, PatchVAE,
in Section 3 and demonstrate that it learns representations that are much better for recognition as
compared to those learned by the standard β-VAEs (Kingma & Welling, 2013; Matthey et al., 2017).
In addition, we propose in Section 3.4 that losses that favor foreground, which is more likely to
contain repetitive patterns, result in representations that are much better at recognition. In Section 4,
we present results on CIFAR100 (Krizhevsky et al., 2009), MIT Indoor Scene Recognition (Quattoni
& Torralba, 2009), Places (Zhou et al., 2017), and ImageNet (Deng et al., 2009) datasets. Our
contributions are as follows:
•	We propose a novel patch-based bottleneck in the VAE framework that learns representations
that can encode repetitive parts across images.
•	We demonstrate that our method, PatchVAE, learns unsupervised representations that are
better suited for recognition in comparison to traditional VAEs.
•	We show that losses that favor foreground are better for unsupervised learning of representa-
tions for recognition.
•	We perform extensive ablation analysis to understand the importance of different aspects of
the proposed PatchVAE architecture.
2	Related Work
Due to its potential impact, unsupervised learning (particularly for deep networks) is one of the
most researched topics in visual recognition over the past few years. Generative models such as
VAEs (Kingma & Welling, 2013; Matthey et al., 2017; Kingma et al., 2016; Gregor et al., 2015),
PixelRNN (van den Oord et al., 2016), PixelCNN (Gulrajani et al., 2016; Salimans et al., 2017), and
their variants have proven effective when it comes to learning compressed representation of images
while being able to faithfully reconstruct them as well as draw samples from the data distribution.
GANs (Goodfellow et al., 2014; Radford et al., 2015; Zhu et al., 2017; Arjovsky et al., 2017) on the
other hand, while don’t model the probability density explicitly, can still produce high quality image
samples from noise. There has been work combining VAEs and GANs to be able to simultaneously
learn image data distribution while being able to generate high quality samples from it (Khan et al.,
2018; Donahue et al., 2016; Larsen et al., 2015). Convolution sparse coding (Affara et al., 2018) is
an alternative approach for reconstruction or image in-painting problems. Our work complements
existing generative frameworks in that we provide a structured approach for VAEs that can learn
beyond low-level representations. We show the effectiveness of the representations learned by our
model by using them for standard visual recognition tasks.
There has been a lot of work in interpreting or disentangling representations learned using generative
models such as VAEs (Matthey et al., 2017; Fraccaro et al., 2017; Kim & Mnih, 2018). However,
there is little evidence of effectiveness of disentangled representations in visual recognition tasks. In
our work, we focus on incorporating inductive biases in these generative models (e.g., VAEs) such
that they can learn representations better suited for visual recognition tasks.
A related, but orthogonal, line of work is self-supervised learning where a proxy task is designed to
learn representation useful for recognition. These proxy tasks vary from simple tasks like arranging
patches in an image in the correct spatial order (Doersch et al., 2014; 2015) and arranging frames from
a video in correct temporal order (Wang & Gupta, 2015; Pathak et al., 2017), to more involved tasks
like in-painting (Pathak et al., 2016) and context prediction (Noroozi & Favaro, 2016; Wang et al.,
2017). We follow the best practices from this line of work for evaluating the learned representations.
2
Under review as a conference paper at ICLR 2020
X
Z 〜％(0,))
(a) VAE Architecture
(b) PatchVAE Architecture
Figure 1: (a) VAE Architecture. (b) Proposed PatChVAE Architecture: Our encoder network computes a
set of feature maps f using φ(x). This is followed by 2 independent single layer networks - bottom network
generates part visibility parameters Qv. We combine QV with output of top network to generate part appearance
parameters Qa. We sample ZViS and ZaPP to construct Z as described in Section 3.2 which is input to the decoder
network. We also visualize the corresponding priors for latents ZaPP and ZViS in the dashed gray boxes.

3	OUR Approach
Our work builds upon VAE framework proposed by Kingma & Welling (2013). We briefly review
relevant aspects of the VAE framework and then present our approach.
3.1	VAE Review
Standard VAE framework assumes a generative model for data where first a latent z is sampled from
a prior p(z) and then the data is generated from a conditional distribution G(x|z). A variational
approximation Q(z|x) to the true intractable posterior is introduced and the model is learned by
minimizing the following negative variational lower bound (ELBO).
LVAE(X) = — Ez〜Q(z∣x) [log G(x∣z)] + DKL [Q(z∣x) k p(z)]	(1)
Q(z|x) is often referred to as an encoder as it can be viewed as mapping data to the the latent space,
while G(x|z) is referred to as a decoder (or generator) that can be viewed as mapping latents to
the data space. Both Q and G are commonly paramterized as neural networks. Fig. 1a shows the
commonly used VAE architecture. If the conditional G(x|z) takes a gaussian form, negative log
likelihood in the first term of RHS of Eq. 1 becomes mean squared error between generator output
X = G(x∣z) and input data x. In the second term, prior p(z) is assumed to be a multi-variate normal
distribution with zero-mean and diagonal covariance N (0, I) and the loss simplifies to
LVAE(X) = ∣∣x - xk2 + DKL [Q(z∣x) k N(0, I)]	⑵
When G and Q are differentiable, entire model can be trained with SGD using reparameterization
trick (Kingma & Welling, 2013). Matthey et al. (2017) propose an extension for learning disentangled
representation by incorporating a weight factor β for the KL Divergence term yielding
LeVAE(X) = ∣∣x — x∣2 + βDκL [Q(z∣x) k N(0, I)]	⑶
VAE framework aims to learn a generative model for the images where the latents z represent
the corresponding low dimensional generating factors. The latents z can therefore be treated as
image representations that capture the necessary details about images. However, we postulate that
representations produced by the standard VAE framework are not ideal for recognition as they are
learned to capture all details, rather than capturing ‘interesting’ aspects of the data and dropping the
rest. This is not surprising since there formulation does not encourage learning semantic information.
For learning semantic representations, in the absence of any relevant supervision (as is available in
self-supervised approaches), inductive biases have to be introduced. Therefore, taking inspiration
from works on unsupervised mid-level pattern discovery (Singh et al., 2012; Doersch et al., 2012;
Juneja et al., 2013), we propose a formulation that encourages the encoder to only encode such few
parts of an image that are repetitive across the dataset, i.e., the patches that occur often in images.
Since the VAE framework provides a principled way of learning a mapping from image to latent space,
we consider it ideal for our proposed extension. We chose β-VAEs for their simplicity and widespread
use. In Section 3.2, we describe our approach in detail and in Section 3.4 propose a modification in
the reconstruction error computation to bias the error term towards foreground high-energy regions
(similar to the biased initial sampling of patterns in (Singh et al., 2012)).
3
Under review as a conference paper at ICLR 2020
3.2	PATCHVAE
Given an image x, let f = φ(x) be a deterministic mapping that produces a 3D representation f of
size h × w × de , with a total of L = h × w locations (grid-cells). We aim to encourage the encoder
network to only encode parts of an image that correspond to highly repetitive patches. For example, a
random patch of noise is unlikely to occur frequently, whereas patterns like faces, wheels, windows,
etc. repeat across multiple images. In order capture this intuition, we force the representation f to
be useful for predicting frequently occurring parts in an image, and use just these predicted parts to
reconstruct the image. We achieve this by transforming f to Z which encodes a set of parts at a small
subset of L locations on the grid cells. We refer to Z as “patch latent codes” for an image. Next we
describe how we re-tool the β-VAE framework to learn these local latent codes. We first describe our
setup for a single part and follow it up with a generalization to multiple parts (Section 3.3).
Image Encoding. Given the image representation f = φ(x), we would like to learn part representa-
tions at each grid location l (where l ∈ {1, . . . , L}). A part is parameterized by its appearance Zapp
and its visibility Zlvis (i.e., presence or absence of the part at grid location l). We use two networks, QfA
and QfV, to parameterize posterior distributions QfA(Zapp | f) and QfV(Zlvis | f) of the part parameters
Zapp and Zlvis respectively. Since the mapping f = φ(x) is deterministic, we can re-write these distribu-
tions as QfA(Zapp | φ(x)) and QfV(Zlvis | φ(x)); or simply QA(Zapp | x) and QV(Zlvis | x). Therefore,
given an image x the encoder networks estimate the posterior QA(Zapp | x) and QV(Zlvis | x). Note
that f is a deterministic feature map, whereas Zapp and Zlvis are stochastic.
Image Decoding. We utilize a generator or decoder network G, that given Zvis and Zapp, reconstructs
the image. First, we sample a part appearance ZaPP (dp dimensional, continuous) and then sample
part visibilities ZviS (L dimensional, binary) one for each location l from the posteriors
ZaPP 〜QA(ZaPP | X)
ZviS 〜QV(ZviS I x), where l ∈{1,...,L}
(4)
Next, we construct a 3D representation Z by placing ZaPP at every location l where the part is present
(i.e., Zvis = 1). This can be implemented by a broadcasted product of ZaPP and ZviS . We refer to Z as
patch latent code. Again note that f is deterministic and Z is stochastic. Finally, a deconvolutional
network takes Z as input and generates an image X. This image generation process can be written as
x 〜G (x I Z^is, Z≡is,…，ZLs, ZaPP)	(5)
Since all latent variables (Zlvis for all l and Zapp) are independent of each other, they can be stacked as
ZP = [Wis ； Z：is；…；ZLs ； Zapp].
This enables us to use a simplified the notation (refer to (4) and (5)):
ZP 〜Q{A,V}(Zp ∣ X)
x 〜G (x I ZP)
(6)
(7)
Note that despite the additional structure, our model still resembles the setup of variational auto-
encoders. The primary difference arises from: (1) use of discrete latents for part visibility, (2) patch-
based bottleneck imposing additional structure on latents, and (4) feature assembly for generator.
Training. We use the training setup of β-VAE and use the maximization of variational lower bound
to train the encoder and decoder jointly (described in Section 3.1). The posterior QA, which captures
the appearance of a part, is assumed to be a zero-mean Normal distribution with diagonal covariance
N (0, I). The posterior QV, which captures the presence or absence a part, is assumed to be a
Bernoulli distribution Bern Zvpirsior with prior Zpvirsior. Therefore, the ELBO for our approach can
written as (refer to (3)):
LPatchVAE (x) = -EZP〜Q{A,V}(zp | x) [G (x | ZP)] + βDKL [Q{A,V}(ZP | x) k p(Zp)]	⑻
where, the DKL term can be expanded as:
4
Under review as a conference paper at ICLR 2020
L
DKL	Q{A,V}(zp	|	x)	k	p(zp)	=βappXDKL	QV(zlvis	|x)	k Bern	zpvirsior
l=1
(9)
+ βvisDκL (QA(Zapp | X)k N (0, I ))
Implementation details. As discussed in Section 3.1, the first and second terms of the RHS of (8)
can be trained using L2 reconstruction loss and reparameterization trick (Kingma & Welling, 2013).
In addition, we also need to compute KL Divergence loss for part visibility. Learning discrete
probability distribution is a challenging task since there is no gradient defined to backpropagate
reconstruction loss through the stochastic layer at decoder even when using the reparameterization
trick. Therefore, we use the relaxed-bernoulli approximation (Maddison et al., 2016; Agustsson et al.,
2017) for training part visibility distributions Zlvis.
For an H × W image, network Q(f | X) first generates feature maps of size (h × w × de), where
(h, w) are spatial dimensions and de is the number of channels. Therefore, the number of locations
L = h × w. Encoders QfA(Zapp | f) and QfV(Zlvis | f) are single layer neural networks to compute Zapp
and Zlvis. Zlvis is (h × w × 1)-dimensional multivariate bernoulli parameter and Zapp is (1 × 1 × dp)-
dimensional multivariate gaussian. dp is length of the latent vector for a single part. Input to the
decoder Z is (h X W X dp)-dimensional. In all our experiments, we fix h = H and W =当
Constructing Zapp. Notice that f is an (h × w × de)-dimensional feature map and Zlvis is
(h X W X 1)-dimensional binary output, but Zapp is (1 X 1 X dp)-dimensional feature vector. If
Zlvis > 1, the part occurs at multiple locations in an image. Since all these locations correspond to
same part, their appearance should be the same. To incorporate this, we take the weighted average of
the part appearance feature at each location, weighted by the probability that the part is present. Since
we use the probability values for averaging the result is deterministic. This operation is encapsulated
by the QA encoder (refer to Figure 1b). During image generation, we sample Zapp once and replicate
it at each location where ZviS = L During training, this forces the model to: (1) only predict ZviS = 1
where similar looking parts occur, and (2) learn a common representation for the part that occurs at
these locations. Note that Zapp can be modeled as a mixture of distributions (e.g., mixture of gaussians)
to capture complicated appearances. However, in this work we assume that the convolutional neural
network based encoders are powerful enough to map variable appearance of semantic concepts to
similar feature representations. Therefore, we restrict ourselves to a single gaussian distribution.
3.3	PatchVAE with multiple parts
Next we extend the framework described above to use multiple parts. To use N parts, we use N X 2
encoder networks QA(i) Za(ip)p | X and QV (i) Zlv(isi) | X , where Za(ip)p and Zlv(isi) parameterize the
ith part. Again, this can be implemented efficiently as 2 networks by concatenating the outputs
together. The image generator samples Zapp and ^v(? from the outputs of these encoder networks
and constructs Z⑴. We obtain the final patch latent code Z by concatenating all Z⑴ in channel
dimension. Therefore, Z⑺ is (h X W X dp)-dimensional and Z is (h X W X (N X dp))-dimensional
stochastic feature map. For this multiple part case, (6) can be written as:
ZP =卜 p1); Zp1)；…；ZpN )i , where Zpi) = Hi(Si) ； Zjisi) ； ... ； ZL(i)； Wppi .	(IO)
Similarly, (8) and (9) can be written as:
NL
LMultiPatchVAE(X) = - EzP [G (X | ZP)] + βapp X XDκL QV(i) Zlv(isi) | X k Bern Zpvirsior
i=1 l=1
N
+ βvis X DκL QA(i) Z(aip)p |X k N(0,I) .	(11)
i=1
The training details and assumptions of posteriors follow the previous section.
5
Under review as a conference paper at ICLR 2020
Figure 2: Concepts captured by parts: We visualize a few representative examples for several parts to qualitatively
demonstrate the visual concepts captured by parts. For each part, We crop image patches centered on the part
location where it is predicted to be present. Selected patches are sorted by part visibility probability as score.
We have manually selected a diverse set from the top 50 occurrences from the training images. As visible, a
single part may capture diverse set of concepts that are similar in shape or texture or occur in similar context, but
belong to different categories. We show which categories the patches come from.
3.4	Improved Reconstruction Loss
The L2 reconstruction loss used for training β-VAEs (and other reconstruction based approaches)
gives equal importance to each region of an image. This might be reasonable for tasks like image
compression and image de-noising. However, for the purposes of learning semantic representations,
not all regions are equally important. For example, “sky” and “walls” occupy large portions of an
image, whereas concepts like “windows,” “wheels,”, “faces” are comparatively smaller, but arguably
more important. To incorporate this intuition, we use a simple and intuitive strategy to weigh the
regions in an image in proportion to the gradient energy in the region. More concretely, we compute
laplacian of an image to get the intensity of gradients per-pixel and average the gradient magnitudes
in 8 X 8 local patches. The weight multiplier for the reconstruction loss of each 8 X 8 patch in the
image is proportional to the average magnitude of the patch. All weights are normalized to sum to
one. We refer to this as weighted loss (Lw). Note that this is similar to the gradient-energy biased
sampling of mid-level patches used in singh et al. (2012); Doersch et al. (2012). In Appendix 6.1, we
show examples of weight masks for some of the images.
In addition, we also consider an adversarial training strategy from GANs to train VAEs as proposed
by Larsen et al. (2015), where the discriminator network from GAN implicitly learns to compare
images and gives a more abstract reconstruction error for the VAE. We refer to this variant by using
‘GAN’ suffix in experiments. In section 4, we demonstrate that the proposed weighted loss (Lw) is
complementary to the discriminator loss from adversarial training, and these losses result in better
recognition capabilities for both β-VAE and PatchVAE.
4	Experiments
Datasets. We evaluate our proposed model on cIFAR100 (Krizhevsky et al., 2009), MIT Indoor
scene Recognition (Quattoni & Torralba, 2009), and Places (Zhou et al., 2017) datasets. Details of
these datasets can be found in Appendix 6.2.
Learning paradigm. In order to evaluate the utility of features learned for recognition, we setup
the learning paradigm as follows: we will first train the model in an unsupervised manner on all the
images other than test set images. After that, we discard the generator network and use only part
of the encoder network φ(x) to train a supervised model on the classification task of the respective
dataset. We study different training strategies for the classification stage as discussed later.
Training details. In all experiments, we use the following architectures. For cIFAR100, Indoor67,
and Place205, φ(x) has a conv layer followed by two residual blocks (He et al., 2016). For ImageNet,
φ(x) is a ResNet18 model (a conv layer followed by four residual blocks). For all datasets, QA and
QV have a single conv layer each. For classification, we start from φ(x), and add a fully-connected
layer with 512 hidden units and a final fully-connected layer as classifier. More details can be found
in Appendix 6.2 and 6.3.
During the unsupervised learning part of training, all methods are trained for 90 epochs for cIFAR100
and Indoor67, 2 epochs for Places205, and 30 epochs for ImageNet dataset. All methods use ADAM
optimizer for training, with initial learning rate of 1 X 10-4 and a minibatch size of 128. For relaxed
bernoulli in QV, we start with the temperature of 1.0 with an annealing rate of 3 X 10-5 (details
in (Agustsson et al., 2017)). For training the classifier, all methods use stochastic gradient descent
(sGD) with momentum with a minibatch size of 128. Initial learning rate is 1 X 10-2 and we reduce
it by a factor of 10 every 30 epochs. All experiments are trained for 90 epochs for cIFAR100 and
Indoor67, 5 epochs for Places205, and 30 epochs for ImageNet datasets.
6
Under review as a conference paper at ICLR 2020
Table 1: Classification results on CIFAR100, Indoor67, and Places205. We initialize the classification model
with the representations φ(x) learned from unsupervised learning task. The model φ(x) comprises of a conv
layer followed by two residual blocks (each having 2 conv layers). First column (called ‘Conv1’) corresponds to
Top-1 classification accuracy with pre-trained model with the first conv layer frozen, second and third columns
correspond to results with 3 conv layers and 5 conv layers frozen respectively. Details in Section 4.1.
Model	CIFAR100			Conv1	Indoor67 Conv3	Conv5	Conv1	Places205 Conv3	Conv5
	Conv1	Conv3	Conv5						
β-VAE	44.12	39.65	28.57	20.08	17.76	13.06	28.29	24.34	8.89
β-VAE + Lw	44.96	40.30	28.33	21.34	19.48	13.96	29.43	24.93	9.41
β-VAE-GAN	44.69	40.13	29.89	19.10	17.84	13.06	28.48	24.51	9.72
β-VAE-GAN + Lw	45.61	41.35	31.53	20.45	18.36	14.33	29.63	25.26	10.66
PatchVAE	43.07	38.58	28.72	20.97	19.18	13.43	28.63	24.95	11.09
PatchVAE + Lw	43.75	40.37	30.55	23.21	21.87	15.45	29.39	26.29	12.07
PatchVAE-GAN	44.45	40.57	31.74	21.12	19.63	14.55	28.87	25.25	12.21
PatchVAE-GAN + Lw	45.39	41.74	32.65	22.46	21.87	16.42	29.36	26.30	13.39
BiGAN	47.72	41.89	31.58	21.64	17.09	9.70	30.06	25.11	10.82
Imagenet Pretrained	55.99	54.99	54.36	45.90	45.82	40.90	37.08	36.46	31.26
Table 2: ImageNet classification results using ResNet18. We initialize weights from using the unsupervised task and fine-tune the last 2 residual blocks. Details in Section 4.1.				Table 3: Effect of maximum number of patches (N)(left) and number of hidden units (dp ) for patch appearance ZaPP (right) on classification accuracy. Details in Sec- tion 4.2.					
Model	Top-1	Top-5		N CIFAR100		Indoor67 dp CIFAR100			Indoor67
β-VAE	44.45	69.67		4	27.59	14.40	3	28.63	14.25
PatchVAE	47.01	71.71		8	28.74	12.69	6	28.97	14.55
β-VAE + Lw PatchVAE + Lw	47.28	71.78		16	28.94	14.33	9	28.21	14.55
	47.87	72.49		32 64	27.78 29.00	13.28 12.76			
		83.79							
Imagenet Supervised 61.37									
Baselines. We use the β-VAE model (Section 3.1) as our primary baseline. In addition, we use
weighted loss and discriminator loss resulting in the β-VAE-* family of baselines. We also compare
against a BiGAN model from Donahue et al. (2016). We use similar backbone architectures for
encoder/decoder (and discriminator if present) across all methods, and tried to keep the number of
parameters in different approaches comparable to the best of our ability. Exact architecture details
can be found in Appendix 6.3.
4.1	Results
In Table 1, we report the top-1 classification results on CIFAR100, Indoor67, and Places205 datasets
for all methods with different training strategies for classification. First, we keep all the pre-trained
weights in φ(x) from the unsupervised task frozen and only train the two newly added conv layers
in the classification network (reported under column ‘Conv5’). We notice that our method (with
different losses) generally outperforms the β-VAE counterpart by a healthy margin. This shows
that the representations learned by PatchVAE framework are better for recognition compared to
β-VAEs. Moreover, better reconstruction losses (‘GAN’ and Lw) generally improve both β-VAE and
PatchVAE, and are complementary to each other.
Next, we fine-tune the last residual block along with the two conv layers (‘Conv3’ column). We
observe that PatchVAE performs better than VAE under all settings except the for CIFAR100 with
just L2 loss. However, when using better reconstruction losses, the performance of PatchVAE
improves over β-VAE. Similarly, we fine-tune all but the first conv layer and report the results in
‘Conv1’ column. Again, we notice similar trends, where our method generally performs better than
β-VAE on Indoor67 and Places205 dataset, but β-VAE performs better CIFAR100 by a small margin.
When compared to BiGAN, PatchVAE representations are better on all datasets (‘Conv5’) by a huge
margin. However, when fine-tuning the pre-trained weights, BiGAN performs better on two out of
four datasets. We also report results using pre-trained weights in φ(x) using supervised ImageNet
7
Under review as a conference paper at ICLR 2020
Table 5: Effect of βvis : Too high or too low βvis
can deteriorate the performance of learned repre-
sentations on classification.
Table 4: Effect of zpvirsior: Increasing the prior on
patch visibility has adverse effect on classification
performance.
prior zvis	CIFAR100	Indoor67	βvis	CIFAR100	Indoor67
0.01	28.86	14.33	0.06	30.11	14.10
0.05	28.67	14.25	0.3	30.37	15.67
0.1	28.31	14.03	0.6	28.90	13.51
classification task (last column, Table 1) for completeness. The results indicate that PatchVAE learns
better semantic representations compared to β-VAE.
ImageNet Results. Finally, we report results on the large-scale ImageNet benchmark in Table 2. For
these experiments, we use ResNet18 (He et al., 2016) architecture for all methods. All weights are
first learned using the unsupervised tasks. Then, we fine-tune the last two residual blocks and train
the two newly added conv layers in the classification network (therefore, first conv layer and the
following two residual blocks are frozen). We notice that PatchVAE framework outperforms β-VAE
under all settings, and the proposed weighted loss helps both approaches. Finally, the last row in
Table 2 reports classification results of same architecture randomly initialized and trained end-to-end
on ImageNet using supervised training for comparison.
4.2	Ablation Studies
We study the impact of various hyper-parameters used in our experiments. For the purpose of this
evaluation, we follow a similar approach as in the ‘Conv5’ column of Table 1 and all hyperparameters
from the previous section. We use CIFAR100 and Indoor67 datasets for ablation analysis.
Maximum number of patches. Maximum number of parts N used in our framework. Depending
on the dataset, higher value of N can provide wider pool of patches to pick from. However, it can
also make the unsupervised learning task harder, since in a minibatch of images, we might not get
too many repeat patches. Table 3(left) shows the effect of N on CIFAR100 and Indoor67 datasets.
We observe that while increasing number of patches improves the discriminative power in case of
CIFAR100, it has little or negative effect in case of Indoor67. A possible reason for this decline in
performance for Indoor67 can be smaller size of the dataset (i.e., fewer images to learn).
Number of hidden units for a patch appearance Zapp. Next, We study the impact of the number
of channels in the appearance feature ZaPP for each patch (dp). This parameter reflects the capacity
of individual patch’s latent representation. While this parameter impacts the reconstruction quality
of images. We observed that it has little or no effect on the classification performance of the base
features. Results are summarized in Table 3(right) for both CIFAR100 and Indoor67 datasets.
Prior probability for patch visibility zvpirsior. In all our experiments, prior probability for a patch is
fixed to 1/N, i.e., inverse of maximum number of patches. The intuition is to encourage each location
on visibility maps to fire for at most one patch. Increasing this patch visibility prior Will alloW all
patches to fire at the same location. While this Would make the reconstruction task easier, it Will
become harder for individual patches to capture anything meaningful. Table 4 shoWs the deterioration
prior
of classification performance on increasing zvis .
Patch visibility loss weight βvis. The Weight for patch visibility KL Divergence has to be chosen
carefully. If βvis is too loW, more patches can fire at same location and this harms the the learning
capability of patches; and if βvis is too high, decoder Will not receive any patches to reconstruct from
and both reconstruction and classification Will suffer. Table 5 summarizes the impact of varying βvis.
5	Conclusion
We presented a patch-based bottleneck in a VAE frameWork that encourages learning useful repre-
sentations for recognition. Our method, PatchVAE, constrains the encoder architecture to only learn
patches that are repetitive and consistent in images as opposed to learning everything, and therefore
results in representations that perform much better for recognition tasks compared to vanilla VAEs.
We also demonstrate that losses that favor high-energy foreground regions of an image are better for
unsupervised learning of representations for recognition.
8
Under review as a conference paper at ICLR 2020
References
Lama Affara, Bernard Ghanem, and Peter Wonka. Supervised convolutional sparse coding. CoRR,
abs/1804.02678, 2018. URL http://arxiv.org/abs/1804.02678.
Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca
Benini, and Luc V Gool. Soft-to-hard vector quantization for end-to-end learning compressible
representations. In Advances in Neural Information Processing Systems, pp. 1141-1151, 2017.
Martin Arjovsky, Soumith Chintala, and L6on Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Computer Vision and Pattern Recognition, 2009. IEEE Conference
on, pp. 248-255. Ieee, 2009.
Carl Doersch, Saurabh Singh, Abhinav Gupta, Josef Sivic, and Alexei A. Efros. What makes paris
look like paris? ACM Transactions on Graphics (SIGGRAPH), 31(4):101:1-101:9, 2012.
Carl Doersch, Abhinav Gupta, and Alexei A Efros. Context as supervisory signal: Discovering
objects with predictable context. In European Conference on Computer Vision, pp. 362-377.
Springer, 2014.
Carl Doersch, Abhinav Gupta, and Alexei A. Efros. Unsupervised visual representation learning by
context prediction. In International Conference on Computer Vision (ICCV), 2015.
Jeff Donahue, Philipp Krahenbuhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint
arXiv:1605.09782, 2016.
Marco Fraccaro, Simon Kamronn, Ulrich Paquet, and Ole Winther. A disentangled recognition and
nonlinear dynamics model for unsupervised learning. In NIPS, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pp. 2672-2680, 2014.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw: A
recurrent neural network for image generation. arXiv preprint arXiv:1502.04623, 2015.
Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez,
and Aaron Courville. Pixelvae: A latent variable model for natural images. arXiv preprint
arXiv:1611.05013, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Mayank Juneja, Andrea Vedaldi, CV Jawahar, and Andrew Zisserman. Blocks that shout: Distinctive
parts for scene classification. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 923-930, 2013.
Salman H Khan, Munawar Hayat, and Nick Barnes. Adversarial training of variational auto-encoders
for high fidelity image generation. arXiv preprint arXiv:1804.10323, 2018.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. arXiv preprint arXiv:1802.05983,
2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
9
Under review as a conference paper at ICLR 2020
Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improved variational inference with inverse autoregressive flow. In Advances in Neural Information
Processing Systems,pp. 4743-4751, 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Anders Boesen Lindbo Larsen, S0ren Kaae S0nderby, Hugo Larochelle, and Ole Winther. AU-
toencoding beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300,
2015.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.
Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed,
and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational
framework. In ICLR 2017, 2017.
Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw
puzzles. In European Conference on Computer Vision, pp. 69-84. Springer, 2016.
Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context
encoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2536-2544, 2016.
Deepak Pathak, Ross Girshick, Piotr Dollar, Trevor Darrell, and Bharath Hariharan. Learning features
by watching objects move. In CVPR, 2017.
Ariadna Quattoni and Antonio Torralba. Recognizing indoor scenes. In 2009 IEEE Conference on
Computer Vision and Pattern Recognition, pp. 413-420. IEEE, 2009.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. CoRR, abs/1511.06434, 2015. URL http:
//arxiv.org/abs/1511.06434.
Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. Pixelcnn++: A pixelcnn
implementation with discretized logistic mixture likelihood and other modifications. In ICLR,
2017.
Saurabh Singh, Abhinav Gupta, and Alexei A. Efros. Unsupervised discovery of mid-level
discriminative patches. In European Conference on Computer Vision, 2012. URL http:
//arxiv.org/abs/1205.3137.
Linda Smith and Michael Gasser. The development of embodied cognition: Six lessons from babies.
Artificial life, 11(1-2):13-29, 2005.
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable
effectiveness of data in deep learning era. 2017 IEEE International Conference on Computer
Vision (ICCV), pp. 843-852, 2017.
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.
CoRR, abs/1601.06759, 2016. URL http://arxiv.org/abs/1601.06759.
Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 2794-2802, 2015.
Xiaolong Wang, Kaiming He, and Abhinav Gupta. Transitive invariance for self-supervised visual
representation learning. In Proc. of Int’l Conf. on Computer Vision (ICCV), 2017.
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10
million image database for scene recognition. IEEE transactions on pattern analysis and machine
intelligence, 40(6):1452-1464, 2017.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE International Conference
on Computer Vision, pp. 2223-2232, 2017.
10
Under review as a conference paper at ICLR 2020
6	Appendix
6.1	Visualization of Weighted loss
Figure 3 shows an illustration of the reconstruction loss LW proposed in Section 3.4. Notice that in
first column, guitar has more weight that rest of the image. Similarly in second, fourth and sixth
columns that train, painting, and people are respectively weighed more heavily by LW than rest of the
image; thus favoring capturing the foreground regions.
Figure 3: Masks used for weighted reconstruction loss Lw. First row contains images randomly samples from
MIT Indoor datatset. Second and third rows have the corresponding image laplacians and final reconstruction
weight masks respectively. In the last row, we take the product of first and third row to highlight which parts of
image are getting more attention while reconstruction.
6.2	DATASET AND TRAINING DETAILS
CIFAR100 consists of 60000 32 X 32 color images in 100 classes, with 600 images per class. There
are 50000 training images and 10000 test images. Indoor dataset contains 67 categories, and a total of
15620 images ('Indoor67'). Train and test subsets consist of 80 and 20 images per class respectively.
Places dataset has 2.5 millions of images with 205 categories ('Places205'). Finally, we report
results on the large-scale ImageNet (Deng et al., 2009) dataset, which has 〜1.28M training and 50k
validation images spanning 1000 categories.
The generator network has two deconv layers with batchnorm (Ioffe & Szegedy, 2015) and a final
deconv layer with tanh activation. When training with ‘GAN, loss, the additional discriminator has
four conv layers, two of with have batchnorm.
6.3	Model Architectures
In this section, we share the exact architectures used in various experiments. As discussed in Section 4,
we evaluated our proposed model on CIFAR100, Indoor67, and Places205 datasets. We resize and
center-crop the images such that input image size for CIFAR100 datasets is 32 × 32 × 3 while
for Indoor67 and Places205 datasets input image size is 64 × 64 × 3. PatchVAE can treat images
of various input sizes in exactly same way allowing us to keep the architecture same for different
datasets. In case of VAE and BiGAN however, we have to go through a fixed size bottleneck layer
and hence architectures need to be a little different for different input image sizes. Wherever possible,
we have tried to keep the number of parameters in different architectures comparable.
6.3.1	Architecture for unsupervised learning task
Tables 6 and 7 show the architectures for encoders used in different models. In the unsupervised
learning task, encoder comprises of a fixed neural network backbone φ(x), that given an image of
size h × W × 3 generated feature maps of size 8 × W × de. This backbone architecture is common
to different models discussed in the paper and consists ofa single conv layer followed by 2 residual
blocks. We refer to this φ(x) as Resnet-9 and it is described as Conv1-5 layers in Table 10. Rest of
11
Under review as a conference paper at ICLR 2020
the encoder architecture varies depending on the model in consideration and is described in the tables
below.
Tables 8 and 9 show the architectures for decoders used in different models. We use a pyramid
like network for decoder where feature map size is doubled in consecutive layers, while number of
channels is halved. Final non-linearity used in each decoder is tanh.
Table 6: Encoder architecture for unsupervised learning task on CIFAR100 - All ‘convolutional’ layers are
represented as (kernel_size × kernel_size, channels, stride, pad). BN stands for batch normalization layer and
ReLU for Rectified Linear Units.
Layer	β-VAE	BiGAN	PatchVAE
Features φ	Resnet-9	Resnet-9	Resnet-9
QV		-	-	(3 × 3,16,1,1)
QA	(1 × 1,64,1,0) BN ReLU μ :(4 × 4, 96,1,0) σ2 : (4 × 4,96,1,0)	(1 × 1, 64, 1, 0) BN ReLU (4 × 4,96,1,0)	μ : (3 × 3, 96,1,1) σ2 : (3 × 3, 96, 1, 1)
# Parameters	888,192	789,792	922,896
6.3.2	Architecture for supervised learning task
As discussed in Section 4, during the supervised learning phase, we discard rest of the encoder model
and only keep φ(x) for classifier training. So the architectures for all baselines are exactly the same.
Tables 10 shows the architecture for classifier used in our experiments.
Table 7: Encoder architecture for unsupervised learning task on Indoor67 and Places205 - All ‘convolutional’
layers are represented as (kernel_size × kernel_size, channels, stride, pad). BN stands for batch normalization
layer and ReLU for Rectified Linear Units. Note that PatchVAE and β-VAE architectures are slightly different
to account for sizes.
Layer	β-VAE	BiGAN	PatchVAE
Features φ	Resnet-9	Resnet-9	Resnet-9
QV		-	-	(3× 3,16,1,1)
QA	(1 × 1, 64, 1, 0) BN ReLU μ : (8 × 8, 96,1,0) σ2 : (8 × 8, 96, 1, 0)	(1 × 1, 64, 1, 0) BN ReLU (8 × 8, 96, 1, 0)	μ : (3 × 3, 96,1,1) σ2 : (3 × 3, 96, 1, 1)
# Parameters	1,478,016	1,084,704	922,896
12
Under review as a conference paper at ICLR 2020
Table 8: Decoder architecture for unsupervised earning task on CIFAR100 - All ‘deconvolutional’ layers are
represented as (kernel_size × kernel_size, channels, stride, pad). BN stands for batch normalization layer and
ReLU for Rectified Linear Units.
	β-VAE	BiGAN	PatchVAE
	 Model	(4 × 4,64,1,0) BN LeakyReLU(0.2) (1 × 1, 256, 1, 0) BN LeakyReLU(0.2) (4 × 4, 128, 2, 1) BN LeakyReLU(0.2) (4 × 4,64,2,1) BN LeakyReLU(0.2) (4×4,3,2,1) tanh	(4 X 4, 64,1, 0) BN LeakyReLU(0.2) (1 × 1, 256, 1, 0) BN LeakyReLU(0.2) (4 × 4, 128, 2, 1) BN LeakyReLU(0.2) (4 × 4,64,2,1) BN LeakyReLU(0.2) (4×4,3,2,1) tanh	(1 × 1, 256, 1, 0) BN LeakyReLU(0.2) (4 × 4, 128, 2, 1) BN LeakyReLU(0.2) (4 × 4,64,2,1) BN LeakyReLU(0.2) (4×4,3,2,1) tanh
# Parameters	774,144	774,144	683,904
Table 9: Decoder architecture for unsupervised learning task on Indoor67 and Places205 - All ‘deconvolutional’
layers are represented as (kernel_size × kernel_size, channels, stride, pad). BN stands for batch normalization
layer and ReLU for Rectified Linear Units. Note that PatchVAE and β-VAE architectures are slightly different
to account for sizes.
	β-VAE	BiGAN	PatchVAE
		(8 × 8, 64, 1, 0) BN LeakyReLU(0.2) (1 × 1, 256, 1, 0) BN LeakyReLU(0.2)	(8 × 8, 64,1, 0) BN LeakyReLU(0.2) (1 × 1, 256, 1, 0) BN LeakyReLU(0.2)	(1 × 1, 256, 1, 0) BN LeakyReLU(0.2) (4 × 4, 128, 2, 1) BN
Model	(4 × 4, 128, 2, 1) BN LeakyReLU(0.2) (4 × 4,64,2,1) BN LeakyReLU(0.2) (4×4,3,2,1) tanh	(4 × 4, 128, 2, 1) BN LeakyReLU(0.2) (4 × 4,64,2,1) BN LeakyReLU(0.2) (4×4,3,2,1) tanh	LeakyReLU(0.2) (4 × 4,64,2,1) BN LeakyReLU(0.2) (4×4,3,2,1) tanh
# Parameters	1,069,056	1,069,056	683,904
13
Under review as a conference paper at ICLR 2020
Table 10: Architecture for supervised learning task - same for all baselines and our model. All convolutional
layers are represented as (kernel_size × kernel_size, channels, stride, pad). BN stands for batch bormalization
layer and ReLU for Rectified Linear Units. All pooling operations are MaxPool and are represented by
(kernel_size × kernel_size, stride, pad). Like Resnet-18, downsampling happens by convolutional layers that
have a stride of 2. In our model, downsampling happens during Conv1, Pool, and after Conv4-5.
Layer	CIFAR100 (32 × 32 × 3)		Indoor67 (64 × 64 × 3)		PlaCes205 (64 × 64 × 3)	
Conv1	1 ×	'(7 × 7, 64, 2, 3) BN ReLU 、Pool(3 × 3, 2,1)	1 ×	I( (7× 7,64,2,3) I BN I ReLU 、Pool(3 × 3, 2,1)	1 ×	I( (7 × 7,64,2,3) I BN I ReLU 、Pool(3 × 3, 2,1)
Conv2-3	2×	((3 × 3, 64,1,1) I BN ReLU III (3× 3,64,1,1) [ BN	2 ×	I( (3 × 3,64,1,1) III BN ReLU III (3 × 3,64,1,1) (BN	2 ×	I( (3× 3,64,1,1) III BN ReLU III (3× 3,64,1,1) (BN
Conv4-5	2 × <	( (3 × 3, 128, 1, 1) BN ReLU (3 × 3, 128, 1, 1) 、BN	2 × <	( (3 × 3, 128, 1, 1) BN ReLU (3 × 3, 128, 1, 1) 、BN	2 × <	( (3 × 3, 128, 1, 1) BN ReLU (3 × 3, 128, 1, 1) 、BN
FC		2048 × 512 512 × 100		8192 × 512 512 × 67		8192 × 512 512 × 205
# Parameters	一	1,783,460	一	4,912,259	一	4,983,053
14