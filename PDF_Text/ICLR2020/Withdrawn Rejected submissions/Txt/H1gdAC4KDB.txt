Under review as a conference paper at ICLR 2020
Adversarially Robust Generalization Just
Requires More Unlabeled Data
Anonymous authors
Paper under double-blind review
Ab stract
Neural network robustness has recently been highlighted by the existence of ad-
versarial examples. Many previous works show that the learned networks do not
perform well on perturbed test data, and significantly more labeled data is re-
quired to achieve adversarially robust generalization. In this paper, we theoret-
ically and empirically show that with just more unlabeled data, we can learn a
model with better adversarially robust generalization. The key insight of our re-
sults is based on a risk decomposition theorem, in which the expected robust risk
is separated into two parts: the stability part which measures the prediction sta-
bility in the presence of perturbations, and the accuracy part which evaluates the
standard classification accuracy. As the stability part does not depend on any la-
bel information, we can optimize this part using unlabeled data. We further prove
that for a specific Gaussian mixture problem illustrated by Schmidt et al. (2018),
adversarially robust generalization can be almost as easy as the standard gener-
alization in supervised learning if a sufficiently large amount of unlabeled data is
provided. Inspired by the theoretical findings, we further show that a practical ad-
versarial training algorithm that leverages unlabeled data can improve adversarial
robust generalization on MNIST and Cifar-10.
1 Introduction
Deep learning (LeCun et al., 2015), especially deep Convolutional Neural Network (CNN) (LeCun
et al., 1998), has led to state-of-the-art results spanning many machine learning fields, such as im-
age classification (Simonyan & Zisserman, 2014; He et al., 2016; Huang et al., 2017; Hu et al.,
2017), object detection (Ren et al., 2015; Redmon et al., 2016; Lin et al., 2018), semantic segmen-
tation (Long et al., 2015; Zhao et al., 2017; Chen et al., 2018) and action recognition (Tran et al.,
2015; Wang et al., 2016; 2018).
Despite the great success in numerous applications, recent studies show that deep CNNs are vulner-
able to some well-designed input samples named as Adversarial Examples (Szegedy et al., 2013;
Biggio et al., 2013). Take image classification as an example, for almost every commonly used
well-performed CNN, attackers are able to construct a small perturbation on an input image. The
perturbation is almost imperceptible to humans but can fool the model to make a wrong prediction.
The problem is serious as some designed adversarial examples can be transferred among different
kinds of CNN architectures (Papernot et al., 2016), which makes it possible to perform black-box
attack: an attacker has no access to the model parameters or even architecture, but can still easily
fool a machine learning system.
There is a rapidly growing body of work on studying how to obtain a robust neural network model.
Most of the successful methods are based on adversarial training (Szegedy et al., 2013; Madry et al.,
2017; Goodfellow et al., 2015; Huang et al., 2015). The high-level idea of these works is that during
training, we predict the strongest perturbation to each sample against the current model and use the
perturbed sample together with the correct label for gradient descent optimization. However, the
learned model tends to overfit on the training data and fails to keep robust on unseen testing data.
For example, using the state-of-the-art adversarial robust training method (Madry et al., 2017), the
defense success rate of the learned model on the testing data is below 60% while that on the train-
ing data is almost 100%, which indicates that the robustness fails to generalize. Some theoretical
results further show that it is challenging to achieve adversarially robust generalization. Fawzi et al.
1
Under review as a conference paper at ICLR 2020
(2018)	proves that adversarial examples exist for any classifiers and can be transferred across dif-
ferent models, making it impossible to design network architectures free from adversarial attacks.
Schmidt et al. (2018) shows that adversarially robust generalization requires much more labeled
data than standard generalization in certain cases. Tsipras et al. (2019) presents an inherent trade-off
between accuracy and robust accuracy and argues that the phenomenon comes from the fact that
robust classifiers learn different features. Therefore it is hard to reach high robustness for standard
training methods.
Given the challenge of the task and previous findings, in this paper, we provide several theoretical
and empirical results towards better adversarially robust generalization. In particular, we show that
we can learn an adversarially robust model which generalizes well if we have plenty of unlabeled
data, and the labeled sample complexity for adversarially robust generalization in Schmidt et al.
(2018) can be largely reduced if unlabeled data is used. First, we show that the expected robust
risk can be upper bounded by the sum of two terms: a stability term which measures whether the
model can output consistent predictions under perturbations, and an accuracy term which evaluates
whether the model can make correct predictions on natural samples. Given the stability term does not
rely on ground truth labels, unlabeled data can be used to minimize this term and thus improve the
generalization ability. Second, we prove that for the Gaussian mixture problem defined in Schmidt
et al. (2018), if unlabeled data can be used, adversarially robust generalization will be almost as easy
as the standard generalization in supervised learning (i.e. using the same number of labeled samples
under similar conditions). Inspired by the theoretical findings, we provide a practical algorithm
that can learn from both labeled and unlabeled data for better adversarially robust generalization.
Our experiments on MNIST and Cifar-10 show that the method achieves better performance, which
verifies our theoretical findings.
Our contributions are in three folds.
•	In Section 3.2.1, we provide a theorem to show that unlabeled data can be naturally used to
improve the expected robust risk in general setting and thus leveraging unlabeled data is a
way to improve adversarially robust generalization.
•	In Section 3.2.2, we discuss a specific Gaussian mixture problem introduced in Schmidt
et al. (2018). In Schmidt et al. (2018), the authors proved that in this case, the labeled
sample complexity for robust generalization is significantly larger than that for standard
generalization. As an extension of this work, we prove that in this case, the labeled sample
complexity for robust generalization can be the same as that for standard generalization if
we have enough unlabeled data.
•	Inspired by our theoretical findings, we provide an adversarial robust training algorithm
using both labeled and unlabeled data. Our experimental results show that the algorithm
achieves better performance than baseline algorithms on MNIST and Cifar-10, which em-
pirically proves that unlabeled data can help improve adversarially robust generalization.
2	Related works
Adversarial attacks and defense Most previous works study how to attack a neural network
model using small perturbations under certain norm constraints, such as l∞ norm or l2 norm. For
the l∞ constraint, Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015) finds a direction
to which the perturbation increases the classification loss at an input point to the greatest extent;
Projected Gradient Descent (PGD) (Madry et al., 2017) extends FGSM by updating the direction of
the attack in an iterative manner and clipping the modifications in the norm range after each iteration.
For the l2 constraint, DeepFool (Moosavi-Dezfooli et al., 2016) iteratively computes the minimal
norm of an adversarial perturbation by linearizing around the input in each iteration. C&W attack
(Carlini & Wagner, 2016) is a comprehensive approach that works under both norm constraints. In
this work, we focus on learning a robust model to defend the white-box attack, i.e. the attacker
knows the model parameters and thus can use the algorithms above to attack the model.
There are a large number of papers about defending against adversarial attacks, but the result is far
from satisfactory. Remarkably, Athalye et al. (2018) shows most defense methods take advantage of
so-called “gradient mask” and provides an attacking method called BPDA to correct the gradients.
So far, adversarial training (Madry et al., 2017) has been the most successful white-box defense al-
2
Under review as a conference paper at ICLR 2020
gorithm. By modeling the learning problem as a mini-max game between the attacker and defender,
the robust model can be trained using iterative optimization methods. Some recent papers (Wang
et al., 2019; Gao et al., 2019) theoretically prove the convergence of adversarial training. More-
over, Shafahi et al. (2019); Zhang et al. (2019a) propose ways to accelerate the speed of adversarial
training. Adversarial logit pairing (Kannan et al., 2018) and TRADES (Zhang et al., 2019b) further
improve adversarial training by decomposing the prediction error as the sum of classification error
and boundary error, and Wang et al. (2019) proposes to improve adversarial training by evaluating
the quality of adversarial examples using the FOSC metric.
Semi-supervised learning Using unlabeled data to help the learning process has been proved
promising in different applications (Rasmus et al., 2015; Zhang & Shi, 2011; Elworthy, 1994). Many
approaches use regularizers called “soft constraints” to make the model “behave” well on unlabeled
data. For example, transductive SVM (Joachims, 1999) uses prediction confidence as a soft con-
straint, and graph-based SSL (Belkin et al., 2006; Talukdar & Crammer, 2009) requires the model
to have similar outputs at endpoints of an edge. The most related work to ours is the consistency-
based SSL. It uses consistency as a soft constraint, which encourages the model to make consistent
predictions on unlabeled data when a small perturbation is added. The consistency metric can be ei-
ther computed by the model’s own predictions, such as the Π model (Sajjadi et al., 2016), Temporal
Ensembling (Laine & Aila, 2016) and Virtual Adversarial Training (Miyato et al., 2018), or by the
predictions of a teacher model, such as the mean teacher model (Tarvainen & Valpola, 2017).
Semi-supervised learning for adversarially robust generalization There are three other concur-
rent and independent works (Carmon et al., 2019; Uesato et al., 2019; Najafi et al., 2019) which also
explore how to use unlabeled data to help adversarially robust generalization. We describe the three
works below, and compare them with ours. See also Carmon et al. (2019) and Uesato et al. (2019)
for the comparison of all the four works from their perspective.
Najafi et al. (2019) investigate the robust semi-supervised learning from the distributionally robust
optimization perspective. They assign soft labels to the unlabeled data according to an adversarial
loss and train such images together with the labeled ones. Results on a wide range of tasks show that
the proposed algorithm improves the adversarially robust generalization. Both Najafi et al. (2019)
and we conduct semi-supervised experiments by removing labels from the training data.
Uesato et al. (2019) study the Gaussian mixture model of Schmidt et al. (2018) and theoretically
show that a self-training algorithm can successfully leverage unlabeled data to improve adversarial
robustness. They extend the self-training algorithm to the real image dataset Cifar10, augment it with
unlabeled Tiny Image dataset and improve state-of-the-art adversarial robustness. They show strong
improvements in the low labeled data regimes by removing most labels from CIFAR-10 and SVHN.
In our work, we also study the Gaussian mixture model and show that a slightly different algorithm
can improve adversarially robust generalization as well. We observe similar improvements using
our algorithm on Cifar-10 and MNIST.
Carmon et al. (2019) obtain similar theoretical and empirical results as in Uesato et al. (2019),
and offer a more comprehensive analysis of other aspects. They show that by using unlabeled
data and robust self-training, the learned models can obtain better certified robustness against all
possible attacks. Moreover, they study the impact of different training components on the final
model performance, such as the size of unlabeled data. We also study the influence of different
factors in our experiments and have similar observations.
3	Main results
In this section, we illustrate the benefits of using unlabeled data for robust generalization from a
theoretical perspective.
3.1	Notations and definitions
We consider a standard classification task with an underlying data distribution PXY over pairs of
examples X ∈ Rd and corresponding labels y ∈ {1, 2, •…，K}. Usually PXY is unknown and We
can only access to S = {(χι,yι),…，(xn, yn)} in which (xi, yi) is independent and identically
3
Under review as a conference paper at ICLR 2020
drawn from PXY, i = 1,2,… ,n. For ease of reference, We denote this empirical distribution as
PXY (i.e. the uniform distribution over i.i.d. sampled data). We also assume that we are given a
suitable loss function l(f (x), y), where f ∈ F is parameterized by θ. The standard loss function
is the zero-one loss, i.e. l0/1 (y0, y) = I[y0 6= y]. Due to its discontinuous and non-differentiable
nature, surrogate loss functions such as cross-entropy or mean square loss are commonly used during
optimization.
Our goal is to find an f ∈ F that minimizes the expected classification risk. Without loss of any
generality, our theory is mainly based on the binary classification problem, i.e. K = 2. All theorems
below can be easily extended to the multi-class classification problem. For a binary classification
problem, the expected classification risk is defined as below.
Definition 1. (Expected classification risk). Let PXY be a probability distribution over Rd × {±1}.
The expected classification risk R ofa classifier f : Rd → {-1, 1} under distribution PXY and loss
function l is defined as
R = E(χ,y)〜PXY l(f (x),y)	(I)
We use R(f) to denote the classification risk under the underlying distribution and use R(f) to
denote the classification risk under the empirical distribution. We use R0/1(f) to denote the risk
with the zero-one loss function. The classification risk characterizes whether the model f is accurate.
However, we also care about whether f is robust. For example, when input x is an image, we hope
a small change (perturbation) to x will not change the prediction of f . To this end, Schmidt et al.
(2018) defines expected robust classification risk as follows.
Definition 2. (Expected robust classification risk). Let PXY be a probability distribution over Rd ×
{±1} and B : Rd → P Rd be a perturbation set. Then the B-robust classification risk RB-robust
ofa classifier f : Rd → {-1, 1} under distribution PXY and loss function l is defined as
RB-robust = E(x,y)〜PXY SuP l(f (X ), y)	(2)
x0∈B(x)
Again, we use RB-robust(f) to denote the expected robust classification risk under the underlying
distribution and use RB-robust(f) to denote the expected robust classification risk under the empirical
distribution. We use RB0/-1robust (f) to denote the robust risk with the zero-one loss function. In real
practice, the most commonly used setting is the perturbation under -bounded l∞ norm constraint
B∞ (x) = x0 ∈ Rd | kx0 - xk∞ ≤ ε . For simplicity, we refer to the robustness defined by this
perturbation set as '∞-robustness.
3.2	Robust generalization analysis
Our first result (Section 3.2.1) shows that unlabeled data can be used to improve adversarially robust
generalization in general setting. Our second result (Section 3.2.2) shows that for a specific learning
problem defined on Gaussian mixture model, compared to previous work (Schmidt et al., 2018),
the sample complexity for robust generalization can be significantly reduced by using unlabeled
data. Both results suggest that using unlabeled data is a natural way to improve adversarially robust
generalization. All detailed proofs of the theorems and lemmas in this section can be found in the
appendix.
3.2.1	General results
In this subsection, we show that the expected robust classification risk can be bounded by the sum
of two terms. The first term only depends on the hypothesis space and the unlabeled data, and the
second term is a standard PAC bound.
Theorem 1. LetF be the hypothesis space. Let S = (xi, yi)in=1 be the set of n i.i.d. samples drawn
from the underlying distribution PXY. For any function f ∈ F, with probability at least 1 - δ over
the random draw of S, we have
RB-robust(f ) ≤ Ex〜PX
suP
x0∈B(x)
(I(f (x0) = f(x)) + R0∕1(f) + RadS (F) + 3
(3)
{z^
(1)
}|
*{z
(2)
}
4
Under review as a conference paper at ICLR 2020
where (1) is a term that can be optimized with only unlabeled data and (2) is the standard PAC
generalization bound. PX is the marginal distribution for PXY and RadS (F) is the empirical
Rademacher complexity of hypothesis space F.
From Theorem 1, we can see that the expected robust classification risk is bounded by the sum
of two terms: the first term only involves the marginal distribution PX and the second term is the
standard PAC generalization error bound. This shows that the expected robust risk minimization
can be achieved by jointly optimizing the two terms simultaneously: we can optimize the first term
using unlabeled data sampled from PX and optimize the second term using labeled data sampled
from PXY , which is the same as the standard supervised learning.
While Cullina et al. (2018) suggests that in the standard PAC learning scenario (only labeled data is
considered), the generalization gap of robust risk can be sometimes uncontrollable by the capacity
of hypothesis space F, our results show that we can mitigate this problem by introducing unlabeled
data. In fact, our following result shows that with enough unlabeled data, learning a robust model
can be almost as easy as learning a standard model.
3.2.2	Learning from Gaussian mixture model
The learning problem defined on Gaussian mixture model is illustrated in Schmidt et al. (2018) as
an example to show adversarially robust generalization needs much more labeled data compared to
standard generalization. In this subsection, we show that for this specific problem, just using more
unlabeled data is enough to achieve adversarially robust generalization. For completeness, we first
list the results in Schmidt et al. (2018) and then show our theoretical findings.
Definition 3. (Gaussian mixture model (Schmidt et al., 2018)). Let θ* ∈ Rd be the Per-Class mean
vector and let σ > 0 be the variance parameter. Then the (θ*, σ)-Gaussian mixture model is defined
by the following distribution PXY over (x, y) ∈ Rd × {±1}: First, draw a label y ∈ {±1} uniformly
at random. Then sample the data point X ∈ Rd from N(y ∙ θ*, σ2Id).
Given the samples from the distribution defined above, the learning problem is to find a linear
classifier to predict label y from x. Schmidt et al. (2018) proved the following sample complexity
bound for standard generalization.
Theorem 2. (Theorem 4 in Schmidt et al. (2018)). Let (x, y) be drawn from the (θ?, σ)-Gaussian
mixture model with ∣∣θ*∣∣2 = √d andσ ≤ c∙ d1/4 where C is a universal constant. Let W ∈ Rd be the
vector W = y ∙ X. Then with high probability, the expected classification risk of the linear classifier
fw using 0-1 loss is at most 1%.
Theorem 2 suggests that we can learn a linear classifier with low classification risk (e.g., 1%) even
if there is only one labeled data. However, the following theorem shows that for adversarially robust
generalization under '∞ perturbation, significantly more labeled data is required.
Theorem 3. (Theorem 6 in Schmidt et al. (2018)). Let gn be any learning algorithm, i.e. a function
from n samples to a binary classifier f. Moreover, let σ = ci ∙ d1/4, let E ≥ 0, and let θ ∈ Rd be
drawn from N(0, Id). We also draw n samples from the (θ, σ)-Gaussian mixture model. Then the
expected '∞-robust classification risk of fn using 0-1 loss is at least 1 (1 一 1/d) if the number of
labeled data n ≤ c2 ^ogd.
As We can see from above theorem, the sample complexity for robust generalization is larger than
that of standard generalization by √d. This shows that for high-dimensional problems, adversarial
robustness can provably require a significantly larger number of samples. We provide a neW result
which shows that the learned model can be robust if there is only one labeled data and sufficiently
many unlabeled data. Our theorem is stated as follow:
Theorem 4. Let (xL,yL) be a labeled point drawn from (θ*, σ)-Gaussian mixture model PXY
with ∣∣θ*k2 = √d and σ = O(d1/4). Let XU,…，xU be n UnlabeIedpoints drawn from PX. Let
v ∈ Rd such that v ∈ arg maxkvk=1 in=1(v>XU)2. Let W = sign(yL ∙ v>xL)v. Then there exists
a constant D such that for any d ≥ D, with high probability, the expected '∞-robust classification
risk of fw using 0-1 loss is at most 1% when the number of unlabeled points n = Ω(d) and E ≤ ɪ.
From Theorem 4, we can see that when the number of unlabeled points is significant, we can learn
a highly accurate and robust model using only one labeled point.
5
Under review as a conference paper at ICLR 2020
Proof sketch The learning process can be intuitively described as the following three steps: in
the first step, We use unlabeled data to estimate the direction of θ* although We do not know the
label that θ* (or -θ*) corresponds to. Specifically, we choose the direction V which maximizes the
quantity Pin=i(v>xiU)2 Which can be vieWed as a measure of the confidence at data points. In the
second step, we use the given labeled point to determine the sign of θ* with high probability, we
note that when the direction is correctly estimated in the first step, then the only one labeled point is
sufficient to give the correct sign with high probability. Finally, we give a good estimation of θ* by
combining the two steps above and learn a robust classifier. The three key lemmas corresponding to
the three steps are listed below (ci are constants for i = 0, 1, 2, 3).
Lemma 1. Under the same setting as Theorem 4, suppose that n > d and σ y σ+ < 击.Then,
i.1 i	cmi n，vdc3 ( vdc3 )2}
with probability at least 1 - cie c2n mini σ ,( σ ) }, there is a unique unit maximal eigenvector
v of the sample covariance matrix Σ = 1
θ*
v-√d
Pin=1 xiUxiU> such that
σ2 + d
2 ≤	+ c3
(4)
≤
2
Lemma 2. Under the same setting as Theorem 4, suppose v is a unit vector such that
T for some constant T < √2. Then with probability at least 1 — exp(- dɑ-ɪ) ), we have
sign(yL ∙ vτxL)vτθ* > 0
(5)
Lemma 3. (Lemma 20 in Schmidt et al. (2018)). Under the same setting as Theorem 4,foranyp ≥ 1
and E ≥ 0, and for any unit vector W such that(W, θ?) ≥ E Ilwkp , where ∣∣ ∙ ∣∣p is the dual norm of
k ∙ kp, the linear classifier fw has 'p -robust classification risk at most exp
Our theoretical findings suggest that we can improve the adversarially robust generalization using
unlabeled data. In the next section, we will present a practical algorithm for real applications, which
further verifies our main results.
Algorithm 1 Generalized Virtual Adversarial Training over labeled and unlabeled data
1:	Input: Datasets S* L Li and SU . Hypothesis space F. Coefficient λ. PGD step size δ. Number of
PGD steps k. Maximum l∞ norm of perturbation E.
2:	for each iteration do
3:	Sample a mini-batch of labeled data SL from SL.
4:	Sample a mini-batch of unlabeled data SU from SU.
5:	for each X ∈ SL ∪ SU do
6:	Fix f and attack x with PGD-(k, E, δ) on loss L1/L2 to obtain x0.
7:	Perform gradient descent on f over the perturbed samples on loss LSSL .
8:	end for
9:	end for
4 Algorithm and experiments
4.1 Practical algorithm
Let Sl = {(χL, yL),…，(xL, yL)} be a set of labeled data and SU = {xf,…，Xm } be a set of
unlabeled data. Motivated by the theory in the previous section, to achieve better adversarially robust
generalization, we can optimize the classifier to be accurate on SL and robust on SL ∪ SU. This is
also equivalent to making the classifier accurate and robust on SL and robust on SU. Therefore, we
design two loss terms on SL and SU separately.
For the labeled dataset Sl, we use the standard '∞-robust adversarial training objective function,
i.e.,
1n
Li(f,SL) = 一£ , max JCE(f(Xi),yi).	⑹
n i=1 x0i∈B∞ (xi)	i
6
Under review as a conference paper at ICLR 2020
Following the most common setting, during training, the classifier outputs a probability distribution
over categories and is evaluated by cross-entropy loss defined as
K
lCE(f(x),y) = - X log fk (x)I[y = k]	(7)
k=1
where fk(x) is the output probability for category k.
For unlabeled data SU, we use an objective function which measures robustness without ground
truth
1m
L2(f, SU) = — X z maχ	lCE(f (xi),yi), where yi = argmax{fk(xi)}.	⑻
m i=1 x0i ∈B∞ (xi)	i	k
Putting the two objective functions together, our training loss is defined as a combination of L1 and
L2 as follows:
LSSL(f,SL,SU)=L1(f,SL)+λL2(f,SU).	(9)
Here λ > 0 is a coefficient to trade off the two loss terms. In real practice, we use iterative optimiza-
tion methods to learn the function f . In the inner loop, we fix the model and use Projected Gradient
Descent (PGD) to learn the attack x0 for any x. In the outer loop, we use stochastic gradient descent
to optimize f on the perturbed x0 s. The general training process is shown in Algorithm 1.
Remark We notice that Algorithm 1 is a generalized version of Virtual Adversarial Training (VAT)
(Miyato et al., 2018). When setting the PGD step k = 1, the algorithm is almost equivalent to the
original VAT algorithm, which is particular useful for improving standard generalization. However,
according to our experimental results below, setting k = 1 does not help improve adversarial robust
generalization. The improvement of adversarial robust generalization using unlabeled data exists
when setting a relatively larger k.
4.2	Experimental setting
We verify Algorithm 1 on MNIST and Cifar-10. Following Madry et al. (2017), we use the Resnet
model and modify the network incorporating wider layers by a factor of 10. This results in a net-
work with five residual units with (16, 160, 320, 640) filters each. During training, we apply data
augmentation including random crops and flips, as well as per image standardization. The initial
learning rate is 0.1, and decay by a factor of 10 twice during training. In the inner loop, we run a
7-step PGD with step size 短 for each mini-batch. The perturbation is constrained to be 蔡 under
l∞ norm.
Following many previous works (Laine & Aila, 2016; Tarvainen & Valpola, 2017; Miyato et al.,
2018; Athiwaratkun et al., 2019), we sample 5k/10k labeled data from the training set and use them
as labeled data. We mask out the labels of the remaining images in the training set and use them
as unlabeled data. By doing this, we conduct two semi-supervised learning tasks and call them
the 5k/10k experiments. In a mini-batch, we sample 25/50 labeled images and 225/200 unlabeled
images for the 5k/10k experiment respectively. In both experiments, we use several different values
of λ as an ablation study for this hyperparameter by setting λ = 0.1, 0.2, 0.3. Learning rate is
decayed at the 60th and the 120th epoch. We use the original PGD-based adversarial training (Madry
et al., 2017) on the sampled 5k/10k labeled data as the baseline algorithm for comparison (referred
to as PGD-adv). Our algorithm is referred to as Ours.
4.3	Experimental results
We list all results of the 5k/10k experiments in Tables 1 and 2. We use five criteria to evaluate
the performance of the model: the natural training/test accuracy (NAtrain and NAtest), the robust
training/test accuracy using PGD-7 attack (RAtrain and RAtest) and the defense success rate (DSR).
First, we can see that in both experiments, the robust test accuracy is improved when we use un-
labeled data. For example, on Cifar-10 the robust test accuracy of the models trained under SSL
with λ = 0.3 for the 5k/10k experiments increase by 3.0/5.0 percents compared to the PGD-adv
baselines. We also check the defense success rate which evaluates whether the model is robust given
7
Under review as a conference paper at ICLR 2020
Table 1: SSL experiment with 5k/10k labeled points on MNIST (%)
	NAtrain	NAtest	RAtrain	RAtest	DSR
PGD-adv on 5k	98.31	98.38	96.95	96.89	98.49
5k	OUrs (k = 7,λ = 0.1)	98.36	98.54	97.82	97.19	98.63
5k	Ours (k = 7, λ = 0.2)	98.43	98.55	98.18	97.28	98.71
Ours (k = 7, λ = 0.3)	98.56	98.56	98.46	97.31	98.73
PGD-adv on 10k-	98.91	98.83	97.96	97.64	98.80
10k Ours (k = 7, λ = 0.1)	98.92	98.92	98.55	97.91	98.98
Ours (k = 7, λ = 0.2)	98.90	98.89	98.76	97.93	99.03
Ours (k = 7, λ = 0.3)	98.93	98.87	98.77	98.01	99.13
PGD-adv on 50k一	99.89	99.44	99.77	98.84	99.40
Table 2: SSL experiment with 5k/10k labeled points on Cifar-10 (%)
	NAtrain	NAtest	RAt rain	RAtest	DSR
PGD-adv on 5k	61.18	60.57	32.40	30.54	50.42
Ours (k = 7, λ = 0.1)	63.24	60.44	32.97	30.90	51.13
5k	Ours (k = 7, λ = 0.2)	61.73	60.71	35.20	32.96	54.29
Ours (k = 7, λ = 0.3)	61.88	60.46	35.07	33.54	55.47
Ours (k = 1, λ = 0.3)	68.15	67.14	0.13	0.12	0.00
PGD-adv on 10k-	78.80	73.79	45.60	37.48	50.79
Ours (k = 7, λ = 0.1)	78.24	72.92	47.96	38.86	53.29
10k Ours (k = 7, λ = 0.2)	78.74	73.16	51.20	41.18	56.29
Ours (k = 7, λ = 0.3)	78.95	73.35	52.24	42.48	57.91
Ours (k = 1, λ = 0.3)	81.43	78.64	2.22	2.27	0.03
PGD-adv on 50k一	99.91	85.40	96.71	49.99	58.54
the prediction is correct. As we can see from the last column in Tables 1 and 2, the defense success
rate of models trained using our proposed method is much higher than the baselines. In particular,
the defense success rate of the model trained with λ = 0.3 in the 10k experiment is competitive
to the model trained using PGD-adv on the whole dataset. This clearly shows the advantage of our
proposed algorithm.
Second, we can also see the influence of the value of λ. The model trained with a larger λ has higher
robust accuracy. For example, in the 10k experiment, the robust test accuracy of the model trained
with λ = 0.3 is more than 3% better than that with λ = 0.1. However, we observe that training will
become hard to converge if λ > 0.5.
Third, using larger k produces more robust models. As we can see from the table, in the 5k/10k
experiment, relatively higher natural training/test accuracy can be achieved by setting k = 1 (vanilla
VAT algorithm). However, the robust training/testing accuracy are significantly worse and are near
zero. This clearly shows that using a stronger attack on both labeled and unlabeled data leads to
better adversarially robust generalization, which is also consistent with our theory.
5 Conclusion
In this paper, we theoretically and empirically show that with just more unlabeled data, we can
learn models with better adversarially robust generalization. We first give an expected robust risk
decomposition theorem and then show that for a specific learning problem on the Gaussian mixture
model, the adversarially robust generalization can be almost as easy as standard generalization.
Based on these theoretical results, we develop an algorithm which leverages unlabeled data during
training and empirically show its advantage. As future work, we will study the sample complexity
of unlabeled data for broader function classes and solve more challenging real tasks.
8
Under review as a conference paper at ICLR 2020
References
Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. CoRR, abs/1802.00420, 2018.
Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, and Andrew Gordon Wilson. There are many con-
sistent explanations of unlabeled data: Why you should average. In International Conference on
Learning Representations, 2019.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples. Journal of machine learning research, 7
(Nov):2399-2434, 2006.
Battista Biggio, Igmo Corona, Davide Maiorca, Blame Nelson, Nedim Srndic, Pavel Laskov, Gior-
gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint
European conference on machine learning and knowledge discovery in databases, pp. 387-402.
Springer, 2013.
Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks.
CoRR, abs/1608.04644, 2016.
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John C Duchi. Unlabeled data
improves adversarial robustness. arXiv preprint arXiv:1905.13736, 2019.
Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-
decoder with atrous separable convolution for semantic image segmentation. arXiv preprint
arXiv:1802.02611, 2018.
Daniel Cullina, Arjun Nitin Bhagoji, and Prateek Mittal. Pac-learning in the presence of evasion
adversaries. arXiv preprint arXiv:1806.01471, 2018.
David Elworthy. Does baum-welch re-estimation help taggers? In Proceedings of the Fourth
Conference on Applied Natural Language Processing, ANLC ’94, pp. 53-58, Stroudsburg, PA,
USA, 1994. Association for Computational Linguistics. doi: 10.3115/974358.974371. URL
https://doi.org/10.3115/974358.974371.
Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. arXiv
preprint arXiv:1802.08686, 2018.
Ruiqi Gao, Tianle Cai, Haochuan Li, Liwei Wang, Cho-Jui Hsieh, and Jason D Lee. Convergence
of adversarial training in overparametrized networks. arXiv preprint arXiv:1906.07916, 2019.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. arXiv preprint arXiv:1709.01507,
7, 2017.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In CVPR, volume 1(2), pp. 3, 2017.
Ruitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesvari. Learning with a strong adver-
sary. arXiv preprint arXiv:1511.03034, 2015.
Thorsten Joachims. Transductive inference for text classification using support vector machines. In
Proceedings of the Sixteenth International Conference on Machine Learning, ICML ’99, pp. 200-
209, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc. ISBN 1-55860-612-2.
Harini Kannan, Alexey Kurakin, and Ian J. Goodfellow. Adversarial logit pairing. CoRR,
abs/1803.06373, 2018.
9
Under review as a conference paper at ICLR 2020
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning.	CoRR,
abs/1610.02242, 2016.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324,1998.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.
Tsung-Yi Lin, Priyal Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense
object detection. IEEE transactions on pattern analysis and machine intelligence, 2018.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic
segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 3431-3440, 2015.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Takeru Miyato, Shin-ichi Maeda, Shin Ishii, and Masanori Koyama. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE transactions on pattern
analysis and machine intelligence, 2018.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
MIT Press, 2012. ISBN 978-0-262-01825-8.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and
accurate method to fool deep neural networks. In The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
Amir Najafi, Shin-ichi Maeda, Masanori Koyama, and Takeru Miyato. Robustness to adversarial
perturbations in learning from incomplete data. arXiv preprint arXiv:1905.13021, 2019.
Nicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. Transferability in machine learning:
from phenomena to black-box attacks using adversarial samples. CoRR, abs/1605.07277, 2016.
Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-
supervised learning with ladder networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 3546-3554.
Curran Associates, Inc., 2015.
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified,
real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 779-788, 2016.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. In Advances in neural information processing systems,
pp. 91-99, 2015.
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transforma-
tions and perturbations for deep semi-supervised learning. CoRR, abs/1606.04586, 2016.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adver-
sarially robust generalization requires more data. In Advances in Neural Information Processing
Systems, pp. 5019-5031, 2018.
Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S
Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! arXiv preprint
arXiv:1904.12843, 2019.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013.
10
Under review as a conference paper at ICLR 2020
Partha Pratim Talukdar and Koby Crammer. New regularized algorithms for transductive learning.
In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp.
442-457. Springer, 2009.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged con-
sistency targets improve semi-supervised deep learning results. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 30, pp. 1195-1204. Curran Associates, Inc., 2017.
Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spa-
tiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international
conference on computer vision, pp. 4489-4497, 2015.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. In International Conference on Learning Representa-
tions, 2019.
Jonathan Uesato, Jean-Baptiste Alayrac, Po-Sen Huang, Robert Stanforth, Alhussein Fawzi,
and Pushmeet Kohli. Are labels required for improving adversarial robustness?	CoRR,
abs/1905.13725, 2019. URL http://arxiv.org/abs/1905.13725.
Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series
in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019. doi: 10.1017/
9781108627771.
Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool.
Temporal segment networks: Towards good practices for deep action recognition. In European
Conference on Computer Vision, pp. 20-36. Springer, 2016.
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1(3), pp. 4,
2018.
Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the
convergence and robustness of adversarial training. In Kamalika Chaudhuri and Ruslan Salakhut-
dinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pp. 6586-6595, Long Beach, California, USA,
09-15 Jun 2019. PMLR.
Bing Zhang and Mingguang Shi. Semi-supervised learning improves gene expression-based predic-
tion of cancer recurrence. Bioinformatics, 27(21):3017-3023, 09 2011. ISSN 1367-4803. doi:
10.1093/bioinformatics/btr502.
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate
once: Painless adversarial training using maximal principle. arXiv preprint arXiv:1905.00877,
2019a.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 7472-7482, Long Beach,
California, USA, 09-15 Jun 2019b. PMLR.
Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing
network. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pp. 2881-2890,
2017.
11
Under review as a conference paper at ICLR 2020
A Background on generalization and Rademacher complexity
The Rademacher complexity is a commonly used capacity measure for a hypothesis space.
Definition 4. Given a set S = {x1, ..., xn} of n samples, the empirical Rademacher complexity of
function class F (mapping from Rd to R) is defined as:
RadS (F) = -Ee
n
n
sup	if (xi)
f∈F i=1
(10)
where E = &,…，en)> contains i.i.d. random variables drawn from the Rademacher distribution
unif({1, -1}).
By using the Rademacher complexity, we can directly provide an upper bound on the generalization
error.
Theorem 5.	(Theorem 3.5inMohrietal. (2012)). Suppose l(∙, ∙) is the 0 — 1 loss, let S = (xi,yi)2ι
be the set of n i.i.d. samples drawn from the underlining distribution PXY . Let F be the hypothesis
space, then with probability at least 1 — δ over S, for any f ∈ F:
R(f) ≤ R(f)+ RadS (F ) + 3
(11)
B Proof of Theorem 1
Proof. For indicator function I, we have for any x,
I(f(x0) 6=y)	≤	I(f(x)	6=	y)	+ I(f(x)	6=	f(x0)).	(12)
According to Definition 2, we have
RB-robust(f) = E(χ,y)〜PXY SUp l(f(χ0),y) = E(χ,y)〜PXY SUp I(f(χ0) = y)
x0 ∈B(x)	x0∈B(x)
≤ E(χ,y)〜PXY sup (I(f (x) = y) + I(f (x) = f (x0)))	(13)
x0 ∈B(x)
=Ex〜PX sup (I(f(x0) = f(x)) + E(x,y)〜ρχγl(f(x), y)
x0∈B(x)
=Ex〜PX sup (I(f(x0) = f(x)) + R(f),
x0∈B(x)
where (13) is derived from (12). We further use Theorem 5 to bound R(f). It is easy to verify that
with probability at least 1 — δ, for any f ∈ F:
RB-robust(f) ≤ Ex〜PX SUp (I(f(χ0) = f(x)) + R(f)
x0 ∈B(x)
≤ Ex〜PX
SUp (I(f(x0) = f(x)) + R(f) + RadS(F) + 3
x0 ∈B(x)
which completes the proof.
□
C Proof of Theorem 4
For convenience, in this section, we use ci or c0i to denote some universal constants, where i =
0,1,2,3,4.
In the proof of Theorem 4, we will use the concentration bound for covariance estimation in Wain-
wright (2019). We first introduce the definition of spiked covariance ensemble.
Definition 5. (Spiked covariance ensemble). A sample xi ∈ Rd from the spiked covariance ensem-
ble takes the form
Xi = √νξiθ0 + Wi,	(14)
where ξi ∈ R is a zero-mean random variable with unit variance, ν ∈ R is a fixed scalar, θ0 ∈ Rd is
a fixed unit vector and wi ∈ Rd is a random vector independent ofξi , with zero mean and covariance
matrix Id .
12
Under review as a conference paper at ICLR 2020
To see why spiked covariance ensemble model is useful, we note that the Gaussian mixture model
is its special case. Specifically, let xiU’s be the unlabeled data in Theorem 4. Then xiU follows
the Gaussian mixture distribution 1N(θ*,σ2L) + 1N(-θ*,σ2L), and 号 is a spiked covariance
ensemble with parameter V = 袅,ξi uniformly distributed on {±1}, Wi 〜N(0, Id) and θo = √.
The following theorem from Wainwright (2019) characterizes the concentration property of spiked
covariance ensemble, which we will further use to bound the robust classification error. Intuitively,
the theorem says that we can approximately recover θ0 in the spiked covariance ensemble model
using the top eigenvector θ of the sample covariance matrix Σ.
Theorem 6.	(Concentration of covariance estimation, see Corollary 8.7 in Wainwright (2019)).
Given i.i.d. samples {xi}in=1 from the spiked covariance ensemble with sub-Gaussian tails (which
means both ξi and Wi are Sub-Gaussian with parameter at most one), suppose that n > d and
Vν+1 y∣~n < 128. Then, with probability at least 1 — cιe-c2n min{√νc3,νc3}, there is a unique
maximal eigenvector θ ofthe SamPIe CoVariance matrix Σ = 1 Pn=I Xix> SUch that
θ -θ0l2
≤ c0
+ c3 .
(15)
Using the theorem above, we can show that for the Gaussian mixture model, one of the top unit
*
eigenvector of the sample covariance matrix is approximately -θd. In other words, We can approxi-
mately recover the parameter θ up to a sign difference: the principal component analysis of Σ gives
*
either V or -v, while ⅛ is close to v.
d
Lemma 4. Under the same setting as Theorem 4, suppose that n > d and σ Jσ∣+d < 忐.Then,
-.1 7 …J —	—ccn mi n{3c3 (3c3 )2}	∙ τ ∙	「
With probability at least 1 — eɪe c2n m { G ,( G ) }, there IS a unique maximal eigenvector V of
the sample covariance matrix Σ = 1 Pn=I XUXU> With unit '2 norm such that
V
θ*
√d
≤ τ0
min{c0σ
+ c3, √2}
(16)
2
Proof. As discussed above, xi- is a spiked covariance ensemble. By Theorem 6 we have with
{ vdc3 ( vdc3 )2}
probability at least 1 - c1e c2n min{ σ ,( σ ) }, there is a unique maximal eigenvector V of the
sample covariance matrix Σ = 1 Pn=I XUXU> such that
(17)
LetT=c0σ jσndd+c3,we have∣∣v - √√d∣∣2
and τ > 1.
≤ τ. Below we need to consider two cases, τ ≤ 1
*
Case 1: τ ≤ 1. Let V = ∣⅛, since both V and ⅛ are unit vectors, we have
一	kvk ,	Vd	,
θ*
v-√d
2
=kVk2+
θ*
√d
2
- 2hV,
2 - 2hV,
(18)
Recall that ∣∣V - √* ∣∣ ≤ T, which is equivalent to
τ2 ≥kVk2 +
θ*
√d
2
-2〈V,
kVk2 + 1 - 2 kVkhv,
Rearranging the terms and using AM-GM inequality gives
2hV，√di ≥ kVk 十不"L
(19)
13
Under review as a conference paper at ICLR 2020
Therefore, by equation 18,
θ*
v-√d
Y-2hv, √di
≤ 2- - 2√1 - T2
_ 2	2τ2
y 1 + √1 — T2
≤ √2τ
=√2(c0σE+d + c3).
By substituting co = √2c0, c2 = 2 c2 and c3 = √2c3, We complete the proof.
Case 2: τ > 1. Let V be one of ±向 such that the the inner product(v, θ*) is nonnegative. Since
1	A*
both v and 力 are unit vectors, we have
d
θ*
v-√d
2
=kVk2+
θ*
√d
2
- 2hv,
2
2-√dhv,θ*i≤ 2
(20)
Therefore, ∣∣v - :√⅛∣∣ ≤ √2 = τo.
□
Now we have proved that by using the top eigenvector of Σ, we can recover the θ* up to a sign
difference. Next, we will show that it is possible to determine the sign using the labeled data.
Lemma 5. Under the same setting as Theorem 4, suppose v ∈ Rd is a unit vector such that
∣∣v — √√*∣∣ ≤ τo where 丁。 ≤ √2. Then with probability at least 1 — exp (-d(1-σ0) ), we
have sign(yL ∙ v>xL)v>θ* > 0.
Proof. Since ∣∣v -& ∣∣ ≤ √2, and both V and √ are unit vectors, We have v>θ* > 0. So the
event {sign(yL ∙ v>xL)v>θ* ≤ 0} is equivalent to the event {yL ∙ v>xL ≤ 0}, i.e.
P[sign(yL ∙ v>xL)v>θ* ≤ 0] = P[yL ∙ v>xL ≤ 0]
(21)
Recall that XL is sampled from the Gaussian distribution N(yL ∙ θ*,σ2Id), where yL is sampled
uniformly at random from {±1}, we have (yLxL) follows the Gaussian distribution N(θ*,σ2L).
Hence,
P[yL ∙ V>XL ≤ 0] = P(yLXL)〜N(θ*,σ2Id) [v>(yLxL) ≤ 0] = Pl
)	「	θ* ∙ v
1〜N(0,1) g ≤-------
(22)
Moreover, from ∣∣v -号 ∣∣ ≤ τ2 We can get
(θ*, Vi ≥ √d(i -
(23)
So, using the Gaussian tail bound PX〜N(o,i)[X ≤ -t] ≤ exp(-t2) for all t ∈ R, and combining
with equation 21, equation 22, equation 23, we have
P[sign(yL ∙ v>xL)v>θ* ≤ 0] ≤ exp
d(1 - T2)2
2σ2
(24)
—
as stated in the lemma.
□
14
Under review as a conference paper at ICLR 2020
Armed with Lemma 4 and Lemma 5, We now have a precise estimation of θ* in the Gaussian mixture
model. Then, we will show that the high precision of the estimation can be translated to low robust
risk. To achieve this, we need a lemma from Schmidt et al. (2018), which upper bounds the robust
classification risk of a linear classifier W in terms of its inner product with θ*.
Lemma 6. (Lemma 20 in Schmidt et al. (2018)). Under the same setting as in Theorem 4, for
any P ≥ 1 and E ≥ 0, and for any unit vector W ∈ Rd such that hW,θ? ≥ E Ilwkp , where
k ∙ kp is the dual norm of ∣∣ ∙ ∣∣p, the linear classifier fw has 'p -robust classification risk at most
Lemma 6 guarantees that if we can estimate θ? precisely, we can achieve small robust classification
risk. Combine with Lemma 4 and Lemma 5 which provide such estimation, we are now ready to
prove the robust classification risk bound stated in Theorem 4. We can actually prove a slightly more
general theorem below with some extra parameters, and obtain Theorem 4 as a corollary.
Theorem 7. Let (xL,yL) be a labeled data drawn from (θ*, σ)-Gaussian mixture model PXY
with ∣∣θ*∣2 = √d. Let XU, ∙∙∙ ,xU be n unlabeled data drawn from PX. Let τo be as Stated
in Lemma 4, and v ∈ Rd be the normalized eigenvector (i.e. ∣v∣2 = 1) with respect to
the maximal eigenvalue of PNι XUXU> such that ∣∣v 一 号 ∣∣	≤ τ0 with probability at least
1 — cιe-c2nmin{ʒ-3,()2}. Let W = sign(yL ∙ v>xL)v. Then with probability at least
_2
1 — cιe-c2n min{,(~σ^)2} — exp(-d(1-^) ), the linear classifier fw has '∞-robust classi-
fication risk at most β when
E ≤ ι - τ2
σ，2 log 1
√d
(25)
—
Proof. By the choice ofv we have equation 23 holds, i.e.
(θ*,vi≥√d(1- τ2),	(26)
with probability at least 1 - cιe-c2nmin{√σ3,(√σ3)2}.
Applying Lemma 5 to v yields
sign(yL ∙ v>xL)v>θ* > 0,	(27)
τ2
with probability at least 1 - exp(- d(1-^) ).
Notice that W = sign(yL ∙ v>χL)v. So by union bound on events equation 26 and equation 27, we
have
(θ*, Wi = sign(yL ∙ v>xL)(θ*, v)≥ √d(1 - τ0-),	(28)
T 2
with probability at least 1 - cιe-c2n min{~^3,(^j?3)2} - exp(-d(1-2) ).
Since ∣W∣2 = 1, we have
∣∣W∣∣∞ = IlWkI ≤√d.	(29)
By Lemma 6, we have the '∞-robust error is upper bounded by
RB-robust(fw^ ) ≤ eχp
(hW,θ*i-EkWk∞)2
2σ2
(30)
Combining this with equation 28, equation 29 and the assumption equation 25, we have
hW, θ*i- EkWk∞ ≥ √d(I - 20))- √d (1 - ^20
σ J2log 1
√d
(31)
—
15
Under review as a conference paper at ICLR 2020
Hence,	?
RBTobustfw) ≤ exp (- (σq2log β) ∖ = β,	(32)
2σ2
∖ /
τ2
with probability at least 1 - cιe-c2n min{c3,(cσ3 )2} - eχp(- d(1-F) ), as stated in the theorem. □
Now we are ready to prove Theorem 4.
Proof of Theorem 4: Let C be a constant such that σ ≤ C ∙ d1/4 for sufficiently large d. Notice that
the W in Theorem 4 is same as the W in Theorem 7 since the maximal eigenvector of PZ ι XUXU>
also maximizes Pin=1(v>xiU)2 over the unit sphere kvk = 1, v ∈ Rd. Theorem 7 guarantees that
一	T 2 D
with probability at least 1 - cιe-c2n min{~^3,(^⅛3)2} - exp(- d(1-2) ), '∞-robust classification
risk is less then β = 0.01 for
τ02	σq2iogι
C ≤ 1 一——----------
一	2	√d
τ02	cq2log β
=1 - T	dv^.
Choose c3 to be 2. Since n = Ω(d), ɪ ≤ τ0 = sσ Jσn++d + c3 ≤ d1/4 + ɪ, and consequently
τ02 ≤ 1 + -C/4 + √4. So by Theorem 7, with probability at least 1-cι exp (-c2d7/4)-exp (-c√d),
'∞-robust classification risk is less then β = 0.01 for
*	cq2i0gj
£ ≤ 1 -工-d”
3	C ,2log 1
≤ 4	d1/4-.
Since c4, c are numerical constants, there exists a constant D such that when d ≥ D, '∞-robust
classification risk is less then β = 0.01 for E ≤ ɪ, thus We have completed the proof.	□
16