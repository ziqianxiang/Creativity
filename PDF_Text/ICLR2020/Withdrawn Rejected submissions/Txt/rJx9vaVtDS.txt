Under review as a conference paper at ICLR 2020
Individualised Dose-Response Estimation
using Generative Adversarial Nets
Anonymous authors
Paper under double-blind review
Ab stract
The problem of estimating treatment responses from observational data is by
now a well-studied one. Less well studied, though, is the problem of treatment
response estimation when the treatments are accompanied by a continuous dosage
parameter. In this paper, we tackle this lesser studied problem by building on a
modification of the generative adversarial networks (GANs) framework that has
already demonstrated effectiveness in the former problem. Our model, DRGAN, is
flexible, capable of handling multiple treatments each accompanied by a dosage
parameter. The key idea is to use a significantly modified GAN model to generate
entire dose-response curves for each sample in the training data which will then
allow us to use standard supervised methods to learn an inference model capable
of estimating these curves for a new sample. Our model consists of 3 blocks: (1)
a generator, (2) a discriminator, (3) an inference block. In order to address the
challenge presented by the introduction of dosages, we propose novel architectures
for both our generator and discriminator. We model the generator as a multi-
task deep neural network. In order to address the increased complexity of the
treatment space (because of the addition of dosages), we develop a hierarchical
discriminator consisting of several networks: (a) a treatment discriminator, (b) a
dosage discriminator for each treatment. In the experiments section, we introduce
a new semi-synthetic data simulation for use in the dose-response setting and
demonstrate improvements over the existing benchmark models.
1	Introduction
Most of the methods developed in the causal inference literature focus on learning the effects of
binary or categorical treatments (Bertsimas et al., 2017; Alaa et al., 2017; Alaa & van der Schaar,
2017; Athey & Imbens, 2016; Wager & Athey, 2018; Yoon et al., 2018). These treatments, though,
are often administered at a certain dosage which can take on continuous values (such as vasopressors
(DOPP-Zemel & Groeneveld, 2013)). In medicine, using a high dosage of a drug can lead to toxic
effects while using a low dosage can result in no effect on the patient outcome (Wang et al., 2017).
Moreover, the dosage levels used when choosing between multiPle treatments for a Patient are crucial
for the decision (Rothwell et al., 2018).
While admissible dosage intervals for drugs are often determined from clinical trials (Cook et al.,
2015), these trials often have a small number of Patients and use simPlistic mathematical models to
assign dosage levels to Patients that do not take into account Patient heterogeneity (Ursino et al., 2017).
After drugs are aPProved through clinical trials, observational data collected about different treatment
dosages Prescribed to a diverse set of Patients offers us the oPPortunity to learn individualized
resPonses. As the relationshiPs between treatment dosage efficacy, toxicity and Patient features
become more comPlex, estimating dose-resPonse from observational data becomes Particularly
imPortant in order to identify oPtimal dosages for each Patient. Fortunately, there is a wealth of
observational data available in the medical domain from electronic health records (Henry et al., 2016).
Learning from observational data already Presents significant challenges in the binary treatment
setting. As exPlained by SPirtes (2009), in an observational treatment-effect dataset, only the factual
outcome is Present (i.e. the outcome for the treatment that was actually given) - the counterfactual
outcomes are not observed. This Problem is exacerbated in the dose-resPonse setting in which the
number of counterfactuals is no longer even finite. Moreover, the treatment assignment is non-random
1
Under review as a conference paper at ICLR 2020
DRGAN
Discriminator
ωluoanosωlsd
Dosage d	Dosage d
Treatment 1 Treatment 2 ↑
r	I	~
DRGAN Counterfactual +_________
Generator
Figure 1: Comparison of DRGAN and GANITE highlighting the key difference between the two dif-
ferent problems they address (dose-response estimation vs. standard treatment-response estimation).
and instead is assigned according to the features associated with each sample. Due to the continuous
nature of the dosage parameter, adjusting for the bias in the dosage assignments is significantly more
complex than for binary (or even multiple) treatments. Thus, standard methods for adjusting for
treatment selection bias cannot be easily extended to handle bias in the dosage parameter.
In this paper we address the problem of dose-response estimation from observational data by building
on the framework introduced in GANITE (Yoon et al., 2018). The key idea is to modify the GAN
framework (Goodfellow et al., 2014) to generate the unobserved counterfactual outcomes from a
standard treatment-effect dataset. Already, GANITE presents a significant modification to the original
GAN framework - rather than the discriminator discriminating between entirely real or entirely fake
samples, the discriminator is attempting to pick out the real component from a vector containing
the real (factual) outcome from the dataset and the fake (counterfactual) outcomes generated by the
generator. We also inherit this key difference from a standard GAN, but in addition we must make
further modifications to the original GANITE framework in order to address the dosage problem.
A naive attempt to extend Yoon et al. (2018) to the dosage setting might involve trying to define a
discriminator that takes as input an entire dose-response curve for each treatment from the generator
(with the outcome for the observed treatment-dosage pair replacing the generated one) and that
tries to determine the factual treatment-dosage pair. This fails for two reasons: (1) we do not wish
to assume prior knowledge of the functional form of the dose-response curves and so will have
access to the generated dose-response curves only by evaluating them at given points (and so "entire"
dose-response curves cannot be passed to the discriminator); (2) substituting the generator output for
the factual treatment-dosage pair with the factual outcome will almost always create a discontinuity
in the response curve and thus the factual treatment-dosage pair would be very easy to identify.
We overcome these two hurdles by defining a discriminator that, rather than acting on the entire
dose-response curves, acts on a finite set of points from each curve, as shown in Fig. 1. From among
the chosen points, the discriminator will then attempt to pick out the factual one. To ensure that the
entire dose-response curve is well-estimated, we sample the set of points randomly each time an
input would be passed to the discriminator. If we were to fix a set of points in advance to compare for
all treatments and samples then only the outcomes associated with these dosage levels would be well
estimated. As our discriminator will be taking as input a set of random dosage-outcome pairs, we
need to condition its behaviour to be like that of a function on a set. In particular, we draw on ideas
from Zaheer et al. (2017) to ensure that the discriminator acts as a function on sets and its output
does not depend on the order in which the elements of the set are given as input.
In addition, we model the generator as a multi-task deep network capable of taking dosages as an input;
this gives us the flexibility to learn heterogeneous dose-response curves for the different treatments.
We also develop a hierarchical discriminator which breaks down the job of the discriminator into
determining the factual treatment and determining the factual dosage using separate networks. We
show in the experiments section that this approach significantly improves performance and is more
stable than using a single network discriminator.
Our contributions in this paper are 3-fold: (1) we propose DRGAN, a significantly modified GAN
framework, capable of dose-response estimation, (2) we propose novel architectures for each of our
networks, (3) we propose a new semi-synthetic data simulation for use in the dose-response setting.
We show, using semi-synthetic experiments, that our model outperforms existing benchmarks.
2
Under review as a conference paper at ICLR 2020
2	Related work
Methods for estimating the outcomes of treatments with an exposure dosage parameter that only
employ observational data make use of the generalized propensity score (GPS) (Imbens, 2000; Imai &
Van Dyk, 2004; Hirano & Imbens, 2004) or build on top of balancing methods for multiple treatments.
Schwab et al. (2019) developed a neural network based method to estimate counterfactuals for
multiple treatments and continuous dosages. The proposed Dose Response networks (DRNets)
in Schwab et al. (2019) consist of a three level architecture with shared layers for all treatments,
multi-task layers for each treatment and additional multi-task layers for dosage sub-intervals. More
specifically, for each treatment w, the dosage interval [aw , bw] is subdivided into E equally sized
sub-intervals and a multi-task head is added for each sub-interval. Their model architecture extends
the one in Shalit et al. (2017) by adding the multi-task heads for the dosage strata. However, the main
advantage of using multi-task heads for dosage intervals would be the added flexibility in the model
to learn potentially very different functions over different regions of the dosage interval. DRNets
does not determine the dosage intervals dynamically and thus much of this flexbility is lost. We
demonstrate in our experiments that DRGAN outperforms both GPS and DRNets.
For a discussion of works that address treatment-response estimation without a dosage parameter, see
Appendix A. Note that for such methods we cannot treat the dosage as an additional input due to the
bias associated with its assignment.
3	Problem formulation
We consider receiving observations of the form (xi, tif, yfi ) for i = 1, ..., N, where, for each i, these
are independent realizations of the random variables (X, Tf, Yf). We refer to X as the feature vector
lying in some feature space X, containing pre-treatment covariates (such as age, weight and lab
test results). The treatment random variable, Tf , is in fact a pair of values Tf = (Wf , Df ) where
Wf ∈ W corresponds to the type of treatment being administered (e.g. chemotherapy or radiotherapy)
which lies in the discrete space of k treatments, W = {w1, ..., wk}, and Df corresponds to the dosage
of the treatment (e.g. number of cycles, amount of chemotherapy, intensity of radiotherapy), which,
for a given treatment w lies in the corresponding treatment’s dosage space, Dw, which in the most
general case is some continuous space (e.g. the interval [0, 1]). We define the set of all treatment-
dosage pairs to be T = {(w, d) : w ∈ W, d ∈ Dw }.
Following Rubin’s potential outcome framework (Rubin, 1984), we assume that for all treatment-
dosage pairs, (w, d), there is a potential outcome Y (w, d) ∈ Y (e.g. 1-year survival probability). The
observed outcome is then defined to be Yf = Y (Wf, Df). We will refer to the unobserved (potential)
outcomes as counterfactuals.
The goal of dose-response estimation is to derive unbiased estimates of the potential outcomes for a
given set of input covariates:
μ(t, X)= E[Y(t)∣X = x]	(1)
for each t ∈ T, X ∈ X. We refer to μ(∙) as the individualised dose-response function. In general, this
quantity is not the same as E[Y |X = x, Tf = t] (which can be easily estimated from observational
data) in the presence of selection bias which often presents itself in observational datasets. In order
for these two quantities to be equal, we must make the following common assumption.
Assumption 1. (Unconfoundedness) The treatment assignment, Tf, and potential outcomes, Y(w, d),
are conditionally independent given the covariates X, i.e.
{Y (w, d)|w ∈ W,d∈ Dw} ⊥⊥ Tf|X.	(2)
This assumption is also commonly referred to as no hidden confounding.
In addition, in order to make μ(∙) identifiable We must also assume that any treatment-dosage pair
could be assigned to any given sample.
Assumption 2. (Overlap) For each X ∈ X such that p(X) > 0, we have 1 > p(t|X) > 0 for each
t ∈ T.
3
Under review as a conference paper at ICLR 2020
4	Dose-Response GAN
We propose estimating μ by first training a generator to generate dose-response curves for each sample
within the training dataset. The learned generator can then be used to train an inference network using
standard supervised methods. We build on the idea presented in Yoon et al. (2018), using a modified
GAN framework to generate potential outcomes conditional on the observed features, treatment and
factual outcome. Several changes must be made to both the generator and discriminator architectures
and learning paradigms in order to produce a model capable of handling the dose-response setting.
4.1	Counterfactual Generator
Our generator, G : X × T × Y × Z → YT takes features, x ∈ X, factual outcome, yf ∈ Y, received
treatment and dosage, tf = (wf, df) ∈ T, and some noise, z ∈ Z (typically multivariate uniform or
Gaussian), as inputs. The output will be a dose-response curve for each treatment (as shown in Fig.
1), so that the output is a function from T to Y, i.e. G(x,tf ,yf, z)(∙) : T → Y. We can then write
ycf ⑴=G(X,tf ,yf,Z)⑴
(3)
to denote our generated counterfactual outcome for the treatment-dosage pair t. We will write
Ycf(t) = G(X,Tf ,Yf, Z)(t) (i.e. the random variable induced by G).
While the job of the counterfactual generator is to generate outcomes for the treatment-dosage pairs
which were not observed, Yoon et al. (2018) demonstrated that the performance of the counterfactual
generator is improved by adding a supervised loss term that regularises its output for the factual
treatment (in our case treatment-dosage pair). We define the supervised loss, LS, to be
LS(G)
E (Yf -G(X,Tf,Yf,Z)(Tf))2
(4)
where the expectation is taken over X, Tf , Yf and Z.
4.2	Counterfactual Discriminator
As noted in Section 1, our discriminator will act on a random set of points from each of the generated
dose-response curves. Similar to Yoon et al. (2018), we define a discriminator, D, that will attempt to
pick out the factual treatment-dosage pair from among the (random set of) generated ones.
Formally, let nw ∈ Z+ be the number of dosage levels we will compare for treatment w ∈ W1.
For each W ∈ W, let Dw = {DW,…,Dww } be a random subset2 of Dw of size nw, where for
the factual treatment, Wf, DWf contains nWf - 1 random elements along with Df. We define
∙-v	∙-v
Yw = (Diw, Yiw)in=w1 ∈ (Dw × Y)nw to be the vector of dosage-outcome pairs for treatment w where
YW = (Yf if Wf = W and Df = Dw
i = I 工f (w,Dw) else	()
∙-v	∙-v	∙-v
and will write Y = (Yw)w∈w. We will write dw, Nw and y to denote realisations of Dw, Yw and y.
Our discriminator, D : X × Qw∈W (Dw × Y)nw → [0, 1]Pnw, will take the features X ∈ X
together with the (random) set of generated outcomes y ∈ YE nw, and output a probability for each
treatment-dosage pair indicating the discriminator’s belief that that pair is the factual one.
As in the standard GAN framework, we define a minimax game by defining the value function to be
L(D, G) =E Σ Σ I{Tf=(w,d)}log Dw,d(X,Y)+I{Tf=(w,d)} log(1- 2Dw,d(X,Y)) , (6)
-w∈W d∈D
where the expectation is taken over X, Tf, Y and {Dw : W ∈ W}, Dw,d corresponds to the
discriminator output for treatment-dosage pair (W, d).
1In practice we set all nw to be the same. The default setting is 5 in the experiments.
2In practice, when Dw = [0, 1], each Djw is sampled independently and uniformly from [0, 1]. Note that for
each training iteration, DDw is resampled (see Section 1).
4
Under review as a conference paper at ICLR 2020
Random dosage samples
Cf for treatment W
Dosage to generate
Treatment to generate
Noise vector
Factual outcome
Factual dosage
Features
Factual treatment
yyjw G Generated dosage-outcome
w...Cf pairs for treatment W
： y : Generated dosage-outcome
pairs for all treatments
-----► Input/output
-A Backpropagation
Figure 2: Overview of our model for the setting with two treatments (Wf corresponds to the factual
treatment and wcf to the counterfactual treatment). The generator is used to generate an output for
each dosage level in each DDW, these outcomes together with the factual outcome, yf, are used to
create the set of dosage-outcome pairs, y, which is passed directly to the treatment discriminator.
Each dosage discriminator receives only the part of y corresponding to that treatment, i.e. yW. These
discriminators are combined (Eq. 11) to define DH which is used to give feedback to the generator.
The minimax game is then given by
minmaxL(D,G)+λLS(G),	(7)
where λ is used to control the trade-off between L and LS (we set λ = 1 in the experiments).
The task of the discriminator (i.e. picking out the factual dosage from Pjk=1 nWj treatment-dosage
pairs) becomes increasingly difficult as we increase nW or k because the dimension of the discrimi-
nator output space, E nw, increases. Although we control nw, if we set it too low, then the set yW
may not well-represent the dose-response curve, particularly if the dose-response curve is complex.
In practice we found that even for moderate settings of nW and only 2 treatments, modelling the
discriminator as a single function resulted in poor performance. In order to overcome this problem,
we introduce a novel hierarchical discriminator which involves a treatment discriminator with output
dimension k and several dosage discriminators, one for each treatment, with output dimensions nW .
First observe that the probability P((Wf, Df) = (w, d)|X, DW, Y) can be written as
P(Wf = w|X, DDW, Y) X P(Df = d|Wf = w, X,DDW, Y).	(8)
We can therefore break down the discriminator into a hierarchical model by learning one discriminator,
DW, that outputs P(Wf =w|X, DDW, Y) which we will refer to as the treatment discriminator, and
then a discriminator, DW, for each treatment, W ∈ W, that outputs P(Df = d∣Wf = w, X, DDW, Y)
which we will refer to as the dosage discriminator for treatment w .
The treatment discriminator, DW : X × W∈W(DW × Y)nw → [0, 1]k, takes the features, x, and
generated potential outcomes, y, and outputs a probability for each treatment, w1,…，wk. Writing
DWW to denote the output of DW corresponding to treatment w, we define the loss, LW , to be
X
I{Wf=W} log DW(X, Y) + I{Wf=W} log(1 - DW(X, Y)) ,	(9)
W∈W
where, again, the expectation is taken over X, Wf , Df , Y and {DW }W∈W .
Then, for each w ∈ W, DW : X × (DW × Y)nw → [0, 1]nw is a map that takes the features, x, and
generated potential outcomes, yW, corresponding to treatment W and outputs a probability for each
dosage level, d1W, ..., dnWw, in a given realisation of DW. Writing DjW to denote the output of DW
corresponding to dosage level DjW , we define the loss of each dosage discriminator to be
nw
X
I{Df =Dw } log Dw (X, Y W )+ I{Df=DW } lθg(l - Dw (X, Y W )),
j=1
(10)
5
Under review as a conference paper at ICLR 2020
where the expectation is taken over X, Dw, Yw, Wf and Df. The I{Wf =w} term ensures that only
samples for which the factual treatment is w are used to train dosage discriminator Dw (otherwise
there would be no factual dosage for that sample).
We define the overall discriminator DH : X × Qw∈W (Dw × Y )nw → [0, 1]P nw by defining its
output corresponding to the treatment-dosage pair (w, djw ) as
DHj (x, y) = DW(x, y) X Dw (χ, y).	(ii)
Instead of the minimax game in Eq. 7, the generator and discriminator are trained according to the
minimax game defined by seeking G*, DH that solve
G* = argmGn L(DH； G) + λLs(G)	DHwj = DWw X Dwj
DW = arg min LW(DW; G*)	Dw = arg min Ld(DW; G*),∀w ∈ W (12)
DW	Dw
Fig. 2 depicts our generator and hierarchical discriminator. Pseudo-code for our algorithm can be
found in Appendix C.
4.3	Inference Network
Once we have learned the counterfactual generator, we can use it only to access (generated) dose-
response curves for all samples in the dataset. To generate dose-response curves for a new sample
we use the counterfactual generator along with the original data to train an inference network,
I : X X T → Y. Details of the loss and pseudo-code can be found in Appendix D.
5	Architecture
In this section, we describe in detail the novel architectures that we adopt to model each of the
functions G, D, DW, Dw1 , ..., Dwk which draws from the ideas in Zaheer et al. (2017). The inference
network, I, has the same architecture as the generator, but does not receive Wf ,df, yf or Z as inputs.
5.1	Generator Architecture
We adopt a multi-task deep learning model for G by defining
a function g : XXT × Y × Z → H for some latent space H
(typically Rl for some l) and then for each treatment W ∈ W we
introduce a multitask "head", gw : H × Dw → Y taking inputs
from H and a dosage, d, to produce an outcome y(w, d) ∈ Y.
Given observations, (x,tf ,yf), a noise vector z, and a target
treatment-dosage pair, t = (w, d), we define
G(x,tf ,yf,z)(t) = gw(g(x,tf,yf, z),d).	(13)
Each of g, gwι,..., gwk are modelled as fully connected networks.
Fig. 3 depicts our generator architecture.
Figure 3: Generator architecture.
5.2	Discriminator Architectures
As noted in Section 1, our discriminators need to act as functions of sets (of randomly selected dosage-
outcome pairs). While we could require that our discriminators try to learn this during training, by
enforcing them to be functions of sets through their architecture, we reduce the complexity of learning
the discriminators (they no longer need to "rule out" functions which are not functions of sets). This
results in better performing discriminators, which in turn improves the performance of the generator.
In practice, the treatment discriminator receives all of the sets (i.e. one set for each treatment)
of dosage-outcome pairs and outputs a probability for each treatment (i.e. there is one output
corresponding to each set). In order to define such a function, we treat each input set as a vector but
require that the outputs be invariant to (i.e. should not depend on) the ordering of the set as a vector.
6
Under review as a conference paper at ICLR 2020
Each dosage discriminator receives the set corresponding to a given treatment and is tasked with
outputting a probability for each element in the set. In order to define such a function, we consider
the input and output as vectors but then require that if we permute the elements of the input vector,
the output should be permuted in the same way. We formalise the required notions - permutation
invariance and permutation equivariance (Zaheer et al., 2017) - in the following subsection.
5.2.1	Permutation Invariance and Permutation Equivariance
The notions of what it means for a function to be permutation invariant and permutation equivariant
with respect to (a subset of) its inputs are given below in definitions 1 and 2, respectively. Let U , V , C
be some spaces. Let m ∈ Z+.
Definition 1. A function f : Um × V → C is permutation invariant with respect to the space Um if
for every u = (u1, ..., um) ∈ Um, every v ∈ V and every permutation, σ, of {1, ..., m} we have
f(u1, ..., um, v) = f (uσ(1) , ..., uσ(m) , v) .	(14)
Definition 2. A function f : U m × V → C m is permutation equivariant with respect to the space U m
if for every u = (u1, ..., um) ∈ Um, every v ∈ V and every permutation, σ, of {1, ..., m} we have
f (uσ(1) , ..., uσ(m) , v) = (fσ(1) (u, v), ..., fσ(m) (u, v)) ,	(15)
where fj (u, v) is the jth element of f(u, v).
To build up functions that are permutation invariant and permutation equivariant we make the
following observations: (1) the composition of any function with a permutation invariant function is
permutation invariant, (2) the composition of two permutation equivariant functions is permutation
equivariant.
Zaheer et al. (2017) provide several possible building blocks to use to construct invariant and
equivariant deep networks. The basic building block we will use for invariant functions will be a
layer of the form
finv(u) = σ(1b1Tm(φ(u1), ...,φ(um))) ,	(16)
where 1l is a vector of 1s of dimension l, φ is any function φ : U → Rq for some q (in this paper we
use a standard fully connected layer) and σ is some non-linearity.
The basic building block for equivariant functions is defined in terms of an equivariance input, u, and
an auxiliary input, v, by
fequi(u,v) = σ(λImu+γ(1m1Tm)u+ (1mΘT)v) ,	(17)
where Im is the m X m identity matrix, λ and Y are scalar parameters and Θ is a vector of weights.
5.2.2	Hierarchical Discriminator Architecture
In the case of the hierarchical discrim-
inator, We want the treatment discrim-
inator, DW, to be permutation invari-
ant with respect to yW for each treat-
ment, W ∈ W. To achieve this we
define a function h1 : ∏w∈w(Dw X
Y)nw → HH and require that this
function be permutation invariant with
respect to each of the spaces (Dw X
Y)nw. We then concatenate the out-
put of h1 with the features X and pass
these through a fully connected net-
work h2 : XX HH → [0,1]k so that
DW (x, S) = h2(x,h1(y)). (18)
P(WI) P(W2)……'P(Wk-I) P(Wk)	----► Auxiliary input
(a) Treatment Discriminator (b) Dosage
Discriminator
Figure 4: Architecture of our discriminators.
To construct h1, we concatenate the
outputs of several invariant layers of
7
Under review as a conference paper at ICLR 2020
the form given in Eq. (16) that each individually act on the spaces (Dw × Y)nw. That is, for each
treatment, W ∈ W We define a map h^ : (Dw X Y)nw → HH by substituting yw for U in Eq. (16).
Wethendefine HH = Qw∈w HH and hi(y) = (hwniv (ywι ),∙∙∙,hwv (ywfc )).
We want each dosage discriminator, Dw ,to be permutation equivariant with respect to Nw. To achieve
this each Dw Will consist of tWo layers of the form given in Eq. (17) With the equivariance input, u,
to the first layer being yw and to the second layer being the output of the first layer and the auxiliary
input, v, to the first layer being the features, x, and then no auxiliary input to the second layer.
Diagrams depicting the architectures of the treatment discriminator and dosage discriminators can be
found in Fig. 4(a) and Fig. 4(b) respectively.
6	Evaluation
The nature of the treatment-effects estimation problem in even the binary treatments setting does not
allow for meaningful evaluation on real-world datasets. While there are well-established benchmark
synthetic models for use in the binary (or multiple) case, no such models exist for the dosage setting.
We propose our own semi-synthetic data simulation to evaluate our model against several benchmarks.
6.1	Experimental setup
Semi-synthetic data generation: We simulate data as follows. We obtain features, x, from a real
dataset (in this paper we use TCGA (Weinstein et al., 2013), News (Johansson et al., 2016; Schwab
et al., 2019)) and MIMIC III (Johnson et al., 2016))3. We consider 3 treatments each accompanied
by a dosage. Each treatment, w ∈ W, is associated with a set of parameters, v1w, v2w, v3w. For each
run of the experiment, these parameters are sampled randomly by first sampling a vector, uiw , from
N(0,1) and then setting Vw = Uw/||uw || where ∣∣∙∣∣ is the standard Euclidean norm. The shape of
the dose-response curve for each treatment, fw(x, d), is given in Table 1, along with a closed-form
expression for the optimal dosage. We add E 〜 N(0,0.2) noise to the outcomes.
We assign factual treatment-dosage pairs to each sample by first sampling a dosage, dw , for each
treatment from a beta distribution, dw |x 〜Beta(α, βw). The parameter ɑ ≥ 1 controls the dosage
selection bias4 and the parameter βw is set to βw = O-I + 2 一 α, with dw being the optimal dosage
dw
for each treatment5. This setting of βw ensures that the mode of the Beta distribution is the optimal
dosage. Once we have sampled a dosage for each treatment, we assign a treatment according to
Wf |x 〜Categorical(softmax(κf (x, dw)) where a higher K will result in a stronger selection bias,
and κ = 0 results in the treatments being assigned completely randomly. The factual treatment-dosage
pair is then given by (Wf, dwf). Unless otherwise specified, we set κ = 2 and α = 2.
We consider 3 different shapes for fw to demonstrate learning heterogeneous dose-response curves.
The first curve can be broken down into two terms, a linear (in d) increasing term (v11)T x+12(v21)T xd
and a quadratic (in d) decreasing term -12(v3)Txd2. This first term could, for example, represent
the improved efficacy of higher dosages of chemotherapy in reducing the size of a tumour, while the
quadratic term could represent the increasing toxicity of chemotherapy as the dosage increases. This
type of trade-off presents itself in many other settings where there are both costs and rewards.
For metrics, we use Mean Integrated Square Error (MISE), Dosage Policy Error (DPE) and Policy
Error (PE) (Silva, 2016; Schwab et al., 2019). Details can be found in Appendix F.
Benchmarks: We compare against two benchmarks: Generalized Propensity Score (GPS) (Imbens,
2000) and Dose Reponse Networks (DRNet) (Schwab et al., 2019). For DRNets, we compare against
both the standard model architecture described by Schwab et al. (2019) as well as with Wasserstein
regularization (DRN-W).
3Details of each dataset along with suggested interpretations of the synthetic treatments and outcomes for
each dataset can be found in Appendix G
4When α = 1, βw = 1 and Beta(α, βw) reduces to the uniform distribution. See Appendix H.
5If the optimal dosage is 0, we sample dw from 1 — Beta(α, βw) where βw is set as though dW = 1. This
results in the dosage being sampled symmetrically for dw = 0 and dw = 1.
8
Under review as a conference paper at ICLR 2020
Treatment	Dose-Response	Optimal dosage
1	fι(x,d) = C((VI)TX + 12(v1)TXd - 12(v1)Txd2)	d* _ (VI)TX d1 = 2(v1)T X
Z_~	_ 2T__ 2	f2(x,d) = C((v1 )T X + sin(∏( V2τχ )d))	d* _ (V2)TX d2 = 2(v2)t X
3	f3(x, d) = C((v3)τx + 12d(d — b)2, where b = 0.75 (：3；丁X)	^^b if b ≥ 0.75- 1 if b < 0.75
Table 1: Dose response curves used to generate semi-synthetic outcomes for patient features x. In the
experiments, we set C = 10. v1w, v2w, v3w are the parameters associated with each treatment w.
As a baseline for comparison, we also use a standard multilayer perceptron (MLP) that takes as input
the patient features, the treatment and dosage and estimates the patient outcome and a multitask
variant (MLP-M) that has a designated head for each treatment. See Appendix E for details of the
benchmark models and their hyperparameter optimisation.
6.2	Source of gain
Before comparing against the benchmarks, we investigate how each component of our model affects
performance. We start with a baseline model in which both the generator and discriminator consist of
a single fully connected network. One at a time, we add in the following components (cumulatively
until we reach our full model): (1) the supervised loss in Eq. 4 (+ LS), (2) multitask heads in the
generator (+ Multitask), (3) hierarchical discriminator (+ Hierarchical) and (4) invariance/equivariance
layers in the treatment and dosage discriminators (+Inv/Eqv). We report the results in Table 2 for
TCGA and News for all 3 error metrics (MISE, DPE and PE), computed over 30 runs.
	√MISE	TCGA √DPe	√Pe	√MIse	News √DPe	√Pe
Baseline	4.18 ± 0.32	2.06 ± 0.16	1.93 ± 0.12	6.17 ± 0.27	6.97 ± 0.27	6.20 ± 0.21
+ LS	3.37 ± 0.11	1.14 ± 0.05	0.84 ± 0.05	4.51 ± 0.16	4.46 ± 0.12	4.40 ± 0.11
+ Multitask	3.15 ± 0.12	0.85 ± 0.05	0.67 ± 0.05	4.11 ± 0.11	4.33 ± 0.11	4.31 ± 0.11
+ Hierarchical	2.54 ± 0.05	0.36 ± 0.05	0.45 ± 0.05	4.07 ± 0.05	4.24 ± 0.11	4.17 ± 0.12
+ Inv/Eqv	1.89 ± 0.05	0.31 ± 0.05	0.25 ± 0.05	3.71 ± 0.05	4.14 ± 0.11	3.90 ± 0.05
Table 2: Source of gain analysis for our model. Metrics are reported as Mean ± Std.
We see that the addition of each component results in a performance improvement for our model,
with the final row (which corresponds to our full model) demonstrating the best performance across
both datasets and for all metrics.
To further demonstrate the advantages of our hierarchical discriminator, in Fig. 5 we investigate how
our hierarchical discriminator compares with a single network discriminator (all other components
are included in both models, see Appendix B for details of the single discriminator) when we vary
the hyperparameter nw on TCGA. Similar results for News can be found in Appendix I.1.
Number of dosage samples
(a)，MISE
Number of dosage samples	Number of dosage samples
(c) √PE
Figure 5: Performance of single vs. hierarchical discriminator when increasing the number of dosage
samples (nw) on TCGA dataset.
9
Under review as a conference paper at ICLR 2020
The performance of the single discriminator causes significant performance drops around nw = 9
across all metrics. As previously noted, this is due to the dimension of the output space (which for
nw = 9 is 27) being too large. Conversely, we see that our hierarchical discriminator shows much
more stable performance even when nw = 19. We investigate in Appendix I.1 the hyperparameter λ.
6.3	Benchmarks comparison
We now compare DRGAN against the benchmarks on our 3 semi-synthetic datasets. For Mimic, due
to the low number of samples available, we use only two treatments - 2 and 3. We report √MISE and
√PE in Table 3, with results for √DPE given in Appendix I.3. We see that DRGAN demonstrates
a statistically significant improvement over every benchmark across all 3 datasets, confirming that
DRGAN is able to learn response-curves on top of very different underlying patient features.
Method	TCGA		News		MIMIC	
	√Mise	√pE	√Mise	√PE	√Mise	√PE
DRGAN	1.89 ± 0.05	0.25 ± 0.05	3.71 ± 0.05	3.90 ± 0.05	2.09 ± 0.12	0.32 ± 0.05
DRNet	3.64 ± 0.12	0.67 ± 0.05	4.98 ± 0.12	4.17 ± 0.11	4.45 ± 0.12	1.44 ± 0.05
DRN-W	3.71 ± 0.12	0.63 ± 0.05	5.07 ± 0.12	4.56 ± 0.12	4.47 ± 0.12	1.37 ± 0.05
GPS	4.83 ± 0.01	1.60 ± 0.01	6.97 ± 0.01	24.1 ± 0.05	7.39 ± 0.00	20.2 ± 0.01
MLP-M	3.96 ± 0.12	1.20 ± 0.05	5.17 ± 0.12	5.82 ± 0.16	4.97 ± 0.16	1.59 ± 0.05
MLP	4.31 ± 0.05	0.97 ± 0.05	5.48 ± 0.16	6.45 ± 0.21	5.34 ± 0.16	1.65 ± 0.05
Table 3: Performance of individualized treatment-dose response estimation on three datasets. Bold
indicates the method with the best performance for each dataset.
In Appendix I.4 we compare DRGAN with DRNET and GPS for an increasing number of treatments.
6.4	Treatment and dosage selection bias
In this section, we assess the robustness of each method to varying treatment and dosage bias. We
report results for √MISE on TCGA here. For the other metrics see Appendix I.2. Fig. 6(a) shows the
performance of the 4 methods for κ between 0 (no bias) and 10 (strong bias). Fig. 6(b) shows the
performance for α between 1 (no bias) and 8 (strong bias). We see that our model shows consistent
performance, significantly outperforming the benchmark methods across the entire ranges of K and α.
(a) Treatment selection bias
Figure 6: Performance of the 4 methods on datasets with varying bias levels.
(b) Dosage selection bias
7 Conclusion
In this paper we proposed a novel framework for estimating dose-response curves from observational
data. Our method modified the GAN framework, introducing a novel hierarchical discriminator for
use in the dose-response setting. We also proposed novel architectures for the networks involved in
our model and introduced a new semi-synthetic data simulation for use as a benchmark in this setting.
On this data we demonstrated significant improvements over the benchmarks.
10
Under review as a conference paper at ICLR 2020
References
Ahmed M Alaa and Mihaela van der Schaar. Bayesian inference of individualized treatment effects
using multi-task gaussian processes. In Advances in Neural Information Processing Systems, pp.
3424-3432, 2017.
Ahmed M Alaa, Michael Weisz, and Mihaela Van Der Schaar. Deep counterfactual networks with
propensity-dropout. arXiv preprint arXiv:1706.05966, 2017.
Susan Athey and Guido Imbens. Recursive partitioning for heterogeneous causal effects. Proceedings
ofthe National Academy ofSciences, 113(27):7353-7360, 2016.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of
Machine Learning Research, 13(Feb):281-305, 2012.
Dimitris Bertsimas, Nathan Kallus, Alexander M Weinstein, and Ying Daisy Zhuo. Personalized
diabetes management using electronic medical records. Diabetes Care, 40(2):210-217, 2017.
Hugh A Chipman, Edward I George, Robert E McCulloch, et al. BART: Bayesian additive regression
trees. The Annals of Applied Statistics, 4(1):266-298, 2010.
Natalie Cook, Aaron R Hansen, Lillian L Siu, and Albiruni R Abdul Razak. Early phase clinical
trials to identify optimal dosing and safety. Molecular Oncology, 9(5):997-1007, 2015.
Richard K Crump, V Joseph Hotz, Guido W Imbens, and Oscar A Mitnik. Nonparametric tests for
treatment effect heterogeneity. The Review of Economics and Statistics, 90(3):389-405, 2008.
Donna DOPP-Zemel and AB Johan Groeneveld. High-dose norepinephrine treatment: determinants
of mortality and futility in critically ill patients. American Journal of Critical Care, 22(1):22-32,
2013.
Douglas Galagate. Causal Inference with a Continuous Treatment and Outcome: Alternative
Estimators for Parametric Dose-Response function with Applications. PhD thesis, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems, pp. 2672-2680, 2014.
J Henry, Yuriy Pylypchuk, Talisha Searcy, and Vaishali Patel. Adoption of electronic health record
systems among US non-federal acute care hospitals: 2008-2015. ONC Data Brief, 35:1-9, 2016.
Keisuke Hirano and Guido W Imbens. The propensity score with continuous treatments. Applied
Bayesian Modeling and Causal Inference from Incomplete-Data Perspectives, 226164:73-84,
2004.
Kosuke Imai and David A Van Dyk. Causal inference with general treatment regimes: Generalizing
the propensity score. Journal of the American Statistical Association, 99(467):854-866, 2004.
Guido W Imbens. The role of the propensity score in estimating dose-response functions. Biometrika,
87(3):706-710, 2000.
Fredrik Johansson, Uri Shalit, and David Sontag. Learning representations for counterfactual
inference. In International Conference on Machine Learning, pp. 3020-3029, 2016.
Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-wei, Mengling Feng, Mohammad
Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a
freely accessible critical care database. Scientific Data, 3:160035, 2016.
Nathan Kallus. Recursive partitioning for personalization using observational data. In Proceedings of
the 34th International Conference on Machine Learning-Volume 70, pp. 1789-1798. JMLR. org,
2017.
Sheng Li and Yun Fu. Matching on balanced nonlinear representations for treatment effects estimation.
In Advances in Neural Information Processing Systems, pp. 929-939, 2017.
11
Under review as a conference paper at ICLR 2020
Min Qian and Susan A Murphy. Performance guarantees for individualized treatment rules. Annals
of Statistics, 39(2):1180, 2011.
Peter M Rothwell, Nancy R Cook, J Michael Gaziano, Jacqueline F Price, Jill FF Belch, Maria Carla
Roncaglioni, Takeshi Morimoto, and Ziyah Mehta. Effects of aspirin on risks of vascular events
and cancer according to bodyweight and dose: Analysis of individual patient data from randomised
trials. The Lancet, 392(10145):387-399, 2018.
Donald B Rubin. Bayesianly justifiable and relevant frequency calculations for the applies statistician.
The Annals of Statistics, pp. 1151-1172, 1984.
Patrick Schwab, Lorenz Linhardt, Stefan Bauer, Joachim M Buhmann, and Walter Karlen. Learning
counterfactual representations for estimating individual dose-response curves. arXiv preprint
arXiv:1902.00981, 2019.
Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect: General-
ization bounds and algorithms. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pp. 3076-3085. JMLR. org, 2017.
Claudia Shi, David M Blei, and Victor Veitch. Adapting neural networks for the estimation of
treatment effects. arXiv preprint arXiv:1906.02120, 2019.
Ricardo Silva. Observational-interventional priors for dose-response learning. In Advances in Neural
Information Processing Systems, pp. 1561-1569, 2016.
Peter Spirtes. A tutorial on causal inference. 2009.
J Stoehlmacher, DJ Park, W Zhang, D Yang, S Groshen, S Zahedy, and HJ Lenz. A multivariate anal-
ysis of genomic polymorphisms: Prediction of clinical outcome to 5-FU/Oxaliplatin combination
chemotherapy in refractory colorectal cancer. British Journal of Cancer, 91(2):344, 2004.
Moreno Ursino, Sarah Zohar, Frederike Lentz, Corinne Alberti, Tim Friede, Nigel Stallard, and
Emmanuelle Comets. Dose-finding methods for Phase I clinical trials using pharmacokinetics in
small populations. Biometrical Journal, 59(4):804-825, 2017.
Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using
random forests. Journal of the American Statistical Association, 113(523):1228-1242, 2018.
Kyle Wang, Michael J Eblan, Allison M Deal, Matthew Lipner, Timothy M Zagar, Yue Wang,
Panayiotis Mavroidis, Carrie B Lee, Brian C Jensen, Julian G Rosenman, et al. Cardiac toxicity
after radiotherapy for stage III non-small-cell lung cancer: Pooled analysis of dose-escalation
trials delivering 70 to 90 Gy. Journal of Clinical Oncology, 35(13):1387, 2017.
John N Weinstein, Eric A Collisson, Gordon B Mills, Kenna R Mills Shaw, Brad A Ozenberger, Kyle
Ellrott, Ilya Shmulevich, Chris Sander, Joshua M Stuart, Cancer Genome Atlas Research Network,
et al. The cancer genome atlas pan-cancer analysis project. Nature Genetics, 45(10):1113, 2013.
Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, and Aidong Zhang. Representation learning
for treatment effect estimation from observational data. In Advances in Neural Information
Processing Systems, pp. 2633-2643, 2018.
Jinsung Yoon, James Jordon, and Mihaela van der Schaar. GANITE: Estimation of individual-
ized treatment effects using generative adversarial nets. International Conference on Learning
Representations (ICLR), 2018.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov,
and Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems, pp.
3391-3401, 2017.
12
Under review as a conference paper at ICLR 2020
Appendix
A Expanded Related Works
Most methods for performing causal inference in the static setting focus on the scenario with two
or multiple treatment options and no dosage parameter. The approaches taken by such methods to
estimate the treatment effects involve either building a separate regression model for each treatment
(Stoehlmacher et al., 2004; Qian & Murphy, 2011; Bertsimas et al., 2017) or using the treatment as a
feature and adjusting for the imbalance between the different treatment populations. The former does
not generalise to the dosage setting due to the now infinite number of possible treatments available.
In the latter case, methods for handling the selection bias involve propensity weighting (Crump
et al., 2008; Alaa et al., 2017; Shi et al., 2019), building sub-populations using tree based methods
(Chipman et al., 2010; Athey & Imbens, 2016; Wager & Athey, 2018; Kallus, 2017) or building
balancing representations between patients receiving the different treatments (Johansson et al., 2016;
Shalit et al., 2017; Li & Fu, 2017; Yao et al., 2018). An additional approach involves modelling the
data distribution of the factual and counterfactual outcomes (Alaa & van der Schaar, 2017; Yoon
et al., 2018).
Silva (2016) leverages observational and interventional data to estimate the effects of discrete dosages
for a single treatment. In particular, Silva (2016) uses observational data to construct a non-stationary
covariance function and develop a hierarchical Gaussian process prior to build a distribution over the
dose response curve. Then, controlled interventions are employed to learn a non-parametric affine
transform to reshape this distribution. The setting in Silva (2016) differs significantly from ours as
we do not assume access to any interventional data.
13
Under review as a conference paper at ICLR 2020
B Single Discriminator Model
In the paper we developed a hierarchical discriminator and demonstrated that it performs significantly
better than the single discriminator setup that we now describe in this section.
B.1 Single Discriminator
In the single model, we will aim to learn a single discriminator, D, that outputs P((Wf , Df) =
(w, d) |X, DDW, Y) for each W ∈ W and d ∈ DDW. We Win write Dw,d(∙) to denote the output of D
that corresponds to the treatment-dosage pair (w, d). We define the loss, LD, to be
LD(D; G) = -E XX
I{Tf=(w,d)} log Dw,d(X, Y) + I{Tf=(w,d)} log(1 - Dw,d(X, Y))
-w∈W d∈DDW	-
(19)
where the expectation is taken over X, {DW}W∈W, Y, Wf and Df and we note that the dependence
on G is through Y. Our single discriminator will be trained to minimise this loss directly. The
generator GAN-loss, LG, is then defined by
LG(G) = -LD (D*; G)	(20)
where D* is the optimal discriminator given by minimising LD. The generator will be trained to
minimise LG + λLS .
B.2 Single Discriminator Architecture
In the case of the single discriminator, we want the
output of D corresponding to each treatment W ∈ W,
i.e. (DWj,…,Dw,nw), to be permutation equivariant
with respect to y W and permutation invariant with re-
spect to each yV for V ∈ W∖{w}. To achieve this, we
first define a function f : Qw∈W (DW X Y)nw → HS
and require that this function be permutation invari-
ant with respect to each of the spaces (DW × Y)nw.
For each treatment, W ∈ W, we introduce a multi-
task head, ∕w : X ×H × (DW × Y )nw → [0,1]nw,
and require that each of these functions be permuta-
tion equivariant with respect to their corresponding
input space (DW × Y)nw but they can depend on
the features, X ∈ X, and invariant latent representa-
tion coming from f arbitrarily. Writing fW to denote
the jth output of fW, the output of the discriminator
given input features, x, and generated outcomes, y,
is defined by
DW,j (χ, y) = fW (χ,f (y), y W).	(21)
Pij = P((Wf ,Df) = (Wi ,dWi))
Figure 7: Overview of the single discrimina-
tor architecture.
To construct the function f, we concatenate the
outputs of several invariant layers of the form given in Eq. (16) that each individually act
on the spaces (DW × Y)nw . That is, for each treatment, W ∈ W we define a map fiWnv :
(DW × Y)nw → HW by substituting yW for U in Eq. (16). We then define HS = Qw∈w HW
and f(y) = (fWnv (y W)…，fWv(y Wk)).
Each fW will consist of two layers of the form given in Eq. (17) with the equivariance input, u, to
first layer being y W and to the second layer being the output of the first layer and the auxiliary input,
v, to the first layer being the concatenation of the features and invariant representation, i.e. (x, f (y))
and then no auxiliary input to the second layer.
A diagram depicting the architecture of the single discriminator model can be found in Fig. 7.
14
Under review as a conference paper at ICLR 2020
C Counterfactual Generator Pseudo-code
Algorithm 1 Training of the generator in DRGAN
1:	Input: dataset C = {(xi, tif, yfi ) : i = 1, ..., N}, batch size nmb, number of dosages per
treatment nd, number of discriminator updates per iteration nD, number of generator updates per
iteration nG, dimensionality of noise nz, learning rate α
2:	Initialize: θG, θW, {θw }w∈W
3:	while G has not converged do
Discriminator updates
4:	for i = 1, ..., nD do
5:	Sample (x1, (w1,d1),y1), ..., (xnmb, (wnmb, dnmb), ynmb) from C
6:	Sample generator noise zj = (z1j, ..., znjz) from Unif([0, 1]nz) for j = 1, ..., nmb
7:	for w ∈ W do
8:	for j = 1, ..., nmb do
∙-v	.
9:	Sample D W = (dW,j,…,dWj) independently and uniformly from (Dw )nd
10:	Set yw according to Eq. 5
11:	Calculate gradient of dosage discriminator loss
nd
gw-Vθw- X	X%j=dw,j}logDw(Xj,yw) + I{dj=dw.j}log(1 -Dw(Xj,yw))
{j:wj =w} k=1
12:	Update dosage discriminator parameters θw J θw + αgw
13:	Set yj' = (yw )w∈w
14:	Calculate gradient of treatment discriminator loss
nmb
gw j Vθw - ΣΣI{wj =w} log DW(Xj, yj) + I{wj=w} log(1 - DW(Xj, Nj))
j=1 w∈W
15:	Update treatment discriminator parameters θW J θW + αgW
Generator updates
16:	for i = 1, ..., nG do
17:	Sample (X1, (w1,d1),y1), ..., (Xnmb, (wnmb, dnmb), ynmb) from C
18:	Sample generator noise zj = (z1j, ..., znjz) from Unif([0, 1]nz) for j = 1, ..., nmb
19:	Sample (Dwj )w∈W fromΠw∈W(Dw)nd forj = 1, ..., nmb
20:	Set y according to Eq. 5
21:	Calculate gradient of generator loss
nmb	nd
gG J Vθg X X X I{wj=w,dj=dw,j} log(Dw (Xj, yj )w × Dw (Xj, yw )ι)
j=1 w∈W l=1
+1{wj=w,dj=dw,j} log(1 - (Dw(Xj, yj) × Dw(Xj, yw)))
22:	Update generator parameters θG J θG + αgG
23:	Output: G
15
Under review as a conference paper at ICLR 2020
D Inference Network
To generate dose-response curves for new samples, we learn an inference network, I : X × T → Y .
This inference network is trained using the original dataset and the learned counterfactual generator.
As with the training of the generator and discriminator, We train using a random set of dosages, Dw.
The loss is given by
LI(D= E X X (Y(w,d)- I(X, (w,d)))2 ,	(22)
-w∈W d三Dw	-
where Y(w, d) is Yf if Tf = (w, d) or given by the generator if Tf = (w, d). The expectation is
taken over X, Tf , Yf , Z and Dw .
D.1 Pseudo-code for training the Inference Network
Algorithm 2 Training of the inference network in DRGAN
1: Input: dataset C = {(xi, tif, yfi ) : i = 1, ..., N}, trained generator G, batch size nmb, number
of dosages per treatment nd, dimensionality of noise nz, learning rate α
2: Initialize: θI,
3: while I has not converged do
4:	Sample (x1, (w1, d1), y1), ..., (xnmb, (wnmb, dnmb), ynmb) from C
5:	Sample generator noise zj = (z1j, ..., znjz) from Unif([0, 1]nz ) for j = 1, ..., nmb
6:	forj = 1, ..., nmb do
7:	for w ∈ W do
8:	Sample Dw = (dw,j,…，dwj) independently and uniformly from (Dw)nd
9:	Set yw according to Eq. 5 5
10:	Calculate gradient of inference network loss
nmb	nd
gI~Nθι E E ∑(yw )ι - I(Xj, (w, dwj))2 3 4 5 6 7 8 9 10 11 12
j=1 w∈W l=1
11:	Update inference network parameters θI J θI + αgI
12: Output: I
16
Under review as a conference paper at ICLR 2020
E	Benchmarks
We use the publicly available GitHub implementation of DRNet provided by Schwab et al. (2019):
https://github.com/d909b/drnet. Moreover, we also used a GPS implementation similar
to the one from https://github.com/d909b/drnet which uses the causaldrf R package
(Galagate, 2016). More spcifically, the GPS implementation uses a normal treatment model, a linear
treatment formula and a 2-nd degree polynomial for the outcome. Moreover, for the TCGA and News
datasets, we performed PCA and only used the 50 principal components as input to the GPS model to
reduce computational complexity.
Hyperparameter optimization: The validation split of the dataset is used for hyperparameter
optimization. For the DRNet benchmarks we use the same hyperparameter optimization proposed by
Schwab et al. (2019) with the hyperparameter search ranges described in Table 4. For DRGAN, we
use the hyperparameter optimization method proposed in GANITE (Yoon et al., 2018), where we
use the complete dataset from the counterfactual generator to evaluate the MISE on the inference
network. We perform a random search (Bergstra & Bengio, 2012) for hyperparameter optimization
over the search ranges in Table 5. For a fair comparison, for the MLP-M model we used the same
architecture used in the inference network of DRGAN. Similarly, for the MLP model we use the
same architecture as for the MLP-M, but without the multitask heads.
Hyperparameter	Search range
Batch size	32, 64, 128
Number of units per hidden layer	24, 48, 96, 192
Number of hidden layers	2, 3
Dropout percentage	0.0, 0.2
Imbalance penalty weight*	0.1, 1.0, 10.0
Fixed	
Number of dosage strata E	5
Table 4: Hyperparameters search range for DRNet. *: For the DRNet model using Wasserstein
regularization only.
Hyperparameter	Search range
Batch size	64, 128, 256
Number of units per hidden layer	32, 64, 128
Size of invariant and equivariant representations	16, 32, 64, 128
Fixed	
Number of hidden layers per multitask head	2
Number of dosage samples	5
λ	1
Optimization	Adam Moment Optimization
Table 5: Hyperparameters search range for DRGAN.
17
Under review as a conference paper at ICLR 2020
F Metrics
The Mean Integrated Square Error (MISE) measures how well the models estimates the patient
outcome across the entire dosage space:
11 N	2
MISE = Nk XX	(y (W,u) - y (W,U)) du.
w∈W i=1 Dw
(23)
In addition to this, we also compute the mean dosage policy error (DPE) (Schwab et al., 2019) to
assess the ability of the model to estimate the optimal dosage point for every treatment for each
individual:
11 N	2
DPE = Nk XXyy (w,dw)-y(W,dw)),
(24)
w∈W i=1
ʌ
where dW is the true optimal dosage and dW is the optimal dosage identified by the model. The
optimal dosage points for a model are computed using SciPy’s implementation of Sequential Least
SQuares Programming.
Finally, we compute the mean policy error (PE) (Schwab et al., 2019) which compares the outcome
of the true optimal treatment-dosage pair to the outcome of the optimal treatment-dosage pair as
selected by the model:
2
yi(w*
	
(25)
i=1
where w* is the true optimal treatment and W* is the optimal treatment identified by the model. The
optimal treatment-dosage pair for a model is selected by first computing the optimal dosage for each
treatment and then selecting the treatment with the best outcome for its optimal dosage.
Each of these metrics are computed on a held out test-set.
18
Under review as a conference paper at ICLR 2020
G Dataset descriptions
TCGA: The TCGA dataset consists of gene expression measurements for cancer patients (Wein-
stein et al., 2013). There are 9659 samples for which we used the measurements from the 4000
most variable genes. The gene expression data was log-normalized and each feature was scaled
in the [0, 1] interval. Moreover, for each patient, the features x were scaled to have norm 1. We
give meaning to our treatments and dosages by considering the treatment as being chemother-
apy/radiotherapy/immunotherapy and their corresponding dosages. The outcome can be thought of
as the risk of cancer recurrence (Schwab et al., 2019).
News: The News dataset consists of word counts for news items. We extracted 10000 samples each
with 2858 features. As in (Johansson et al., 2016; Schwab et al., 2019), we give meaning to our
treatments and dosages by considering the treatment as being the viewing device (e.g. phone, tablet
etc.) used to read the article and the dosage as being the amount of time spent reading it. The outcome
can be thought of as user satisfaction.
MIMIC III: The Medical Information Mart for Intensive Care (MIMIC III) (Johnson et al., 2016)
database consists of observational data from patients in the ICU. We extracted 3000 patients that
receive antibiotics treatment and we used as features 9 clinical covariates measured during the day
of ICU admission. Again, the features were scaled in the [0, 1] interval. In this setting, we can
considered as treatments the different antibiotics and their corresponding dosages.
For a summary description of the datasets, see table 6. The datasets are split into 64/16/20% for
training, validation and testing respectively. The validation dataset is used for hyperparameter
optimization.
	TCGA	News	MIMIC
Number of samples	9659	10000	3000
Number of features	4000	2858	9
Number of treatments	3*	3	2
Table 6: Summary description of datasets. *: for our final experiment in Appendix I.4 we increase
the number of treatments in TCGA to 6 and 9.
H Dosage bias
In order to create dosage-assignment bias in our dataset, We assign dosages according to dw |x 〜
Beta(α,βw). The selection bias is controlled by the parameter a ≥ 1. When we set βw = α-1+2-ɑ
dw
(which ensures that the mode of our distribution is dW), we can write the variance of dw in terms of ɑ
and dW as follows
α -α + 2α — α2	2
dw + 2α α	ca2
(ɑ-1+2)2( α-1 + 3)≈ dα3.
' dw 7 v dw	)
(26)
We see that the variance of our Beta distribution therefore decreases with α, resulting in the sampled
dosages being closer to the optimal dosage, thus resulting in higher dosage-selection bias. In addition
we note that the Beta(1, 1) distribution is in fact the uniform distribution, corresponding to the
dosages being sampled independently of the patient features, resulting in no selection bias when
α = 1.
19
Under review as a conference paper at ICLR 2020
I Additional results
I.1	INVESTIGATING HYPERPARAMETER SENSITIVITY (nw AND λ)
Here we present additional results for our investigation of the hyperparameters nw and λ. Fig. 8
reports each of the 3 performance metrics as we increase the number of dosage samples, nw , used to
train the discriminators on the News dataset. As with the TCGA results in the main paper we see that
the single discriminator suffers a significant performance decrease when nw is set too high.
(a) √MISE
(b) √DPE	(C) √PE
Figure 8:	Performance metrics when increasing the number of dosage samples on News dataset.
Fig.	9 and Fig. 10 report eaCh of the performanCe metriCs when we inCrease λ, the hyperparameter
that trades off between the GAN loss, L, and the supervised loss, LS. The results shown are on
the TCGA and News datasets, respeCtively. As we would expeCt, for λ = 0, the performanCe is
signifiCantly worse than for λ = 1, sinCe this Corresponds to no supervised loss (this result is also
baCked up by our sourCe of gain experiments). In addition, as we inCrease λ we see performanCe
slowly degrades, this is beCause the model starts to behave more and more like MLP-M sinCe the
supervised loss beComes dominant.
(a)，MISE
(b) √DPE	(c) √PE
(a)，MISE
Figure 9: PerformanCe metriCs when inCreasing λ on TCGA dataset.
(c) √PE
Figure 10:	PerformanCe metriCs when inCreasing λ on News dataset.
20
Under review as a conference paper at ICLR 2020
I.2	Additional results on selection bias
In Fig. 11 we report the DPE and PE for our treatment and dosage bias experiment from Section 6.4
of the main paper.
(a) Treatment selection bias
(b) Dosage selection bias
(c) Treatment selection bias
(d) Dosage selection bias
Figure 11: Additional performance metrics of the 4 methods on datasets with varying bias levels on
TCGA dataset.
I.3	Dosage Policy Error for Benchmark Comparison
In Table 7 we report the Dosage Policy Error (DPE) corresponding to Section 6.3 in the main paper.
Methods	TCGA √DPE	News √DPE	MIMIC √DPE
DRGAN	0.31 ± 0.05	4.14 ± 0.11	0.51 ± 0.05
DRNet	0.51 ± 0.05*	4.39 ± 0.11*	0.52 ± 0.05
DRN-W	0.50 ± 0.05*	4.21 ± 0.11	0.53 ± 0.05
GPS	1.38 ± 0.01*	6.40 ± 0.01*	1.41 ± 0.12*
MLP-M	0.92 ± 0.05*	4.94 ± 0.16*	0.77 ± 0.05*
MLP	1.04 ± 0.05*	5.18 ± 0.12*	0.80 ± 0.05*
Table 7: Performance of individualized treatment-dose response estimation on three datasets. Bold
indicates the method with the best performance for each dataset. *: performance improvement is
statistically significant.
21
Under review as a conference paper at ICLR 2020
I.4	Varying the number of treatments
In our final experiment, we increase the number of treatments by defining 3 or 6 additional treatments.
The parameters v1w , v2w , v3w are defined in exactly the same way as for 3 treatments. The outcome
shapes for treatments 4 and 7 are the same as for treatment 1, similarly for 5, 8 and 2 and for 6, 9
and 3. In Table 8 we report MISE, DPE and PE on the TCGA dataset with 6 treatments (TCGA-6)
and with 9 treatments (TCGA-9). Note that we use 3 dosage samples for training DRGAN in this
experiment.
IVfpthnrI				TCGA - 6 √DPe	√Pe	√MISE	TCGA - 9 √DPe	√Pe
	，MISE					
DRGAN	2.37 ± 0.12	0.43 ± 0.05	0.32 ± 0.05	2.79 ± 0.05	0.51 ± 0.05	0.54 ± 0.05
DRNET	4.09 ± 0.16	0.52 ± 0.05	0.71 ± 0.05	4.31 ± 0.12	0.59 ± 0.05	0.74 ± 0.05
GPS	6.62 ± 0.01	2.04 ± 0.01	2.61 ± 0.00	7.58 ± 0.01	3.14 ± 0.01	2.91 ± 0.01
Table 8: Performance of DRGAN and the benchmarks when we increase the number of treatments in
the dataset to 6 and 9. Bold indicates the method with the best performance for each dataset.
22