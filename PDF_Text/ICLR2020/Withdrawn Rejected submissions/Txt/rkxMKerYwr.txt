Under review as a conference paper at ICLR 2020
Towards Interpreting Deep Neural Networks
via Understanding Layer Behaviors
Anonymous authors
Paper under double-blind review
Ab stract
Deep neural networks (DNNs) have achieved unprecedented practical success in
many applications. However, how to interpret DNNs is still an open problem. In
particular, what do hidden layers behave is not clearly understood. In this paper,
relying on a teacher-student paradigm, we seek to understand the layer behaviors
of DNNs by “monitoring” both across-layer and single-layer distribution evolution.
Here, the “across-layer” and “single-layer” considers the layer behavior along the
depth and a specific layer along training epochs, respectively. Relying on optimal
transport theory, we employ the Wasserstein distance (W -distance) to measure the
divergence between the layer distribution and the target distribution. Theoretically,
we prove that i) the W -distance between the distribution of any layer and the
target distribution tends to decrease along the depth. ii) For a specific layer, the
W -distance between the distribution in an iteration and the target distribution tends
to decrease along training iterations. iii) However, a deep layer is not always better
than a shallow layer for some samples. Moreover, our results helps to analyze the
stability of layer distributions and explains why auxiliary losses help the training
of DNNs. Extensive experiments justify our theoretical findings.
1 Introduction
Deep neural networks (DNNs) have been successfully applied in computer vision, such as image
classification (Chen et al., 2019b; Hsu et al., 2019), image generation (Cao et al., 2019; Brock et al.,
2019; Chrysos et al., 2019) and speech recognition (Yeh et al., 2019; Chen et al., 2019a). Despite
their success, the internal mechanism of DNNs is still a black box. In particular, understanding what
do hidden layers do and how to achieve remarkable performance remain persistently elusive. To
answer these questions, we seek to understand across-layer and single-layer behavior.
For the across-layer behavior, we study the learning process by measuring the W-distance between
the distribution of any layer and the target distribution. Recently, most methods only focus on final
predictions in different tasks (He et al., 2016). Due to the end-to-end training, interpreting each
intermediate layer behaviors of a DNN, which, however, is still not clear. To provide interpretability,
existing works try to produce a single prediction and observe the classification performance of each
layer (Papernot & McDaniel, 2018; Szegedy et al., 2013; Kaya et al., 2019). For example, (Alain &
Bengio, 2016) experimentally observe that the linear separability of features increases monotonically
along the depth of a DNN. Unfortunately, there is no theoretical analysis to support the experimental
finding. To understand the learning process of DNNs, one can explore the across-layer behavior by
monitoring how the distributions propagate across different layers. However, how to open the internal
mechanism of DNNs and investigate the across-layer behavior of a DNN remains an open question.
For the single-layer behavior, we seek to measure the W -distance between the distribution in an
iteration and the target distribution. It is important to analyze the distributional stability of one
layer by measuring the change of the distributions. Recently, the distributional stability of a deep
neural network attracts extensive attention (Santurkar et al., 2018). Some studies try to visualize the
training behavior of one layer by plotting the mean and variance of features (Santurkar et al., 2018).
Unfortunately, the visualizations are subjective and lack of necessary theoretical justifications. To
this end, (Sonoda & Murata, 2019) propose a transport analysis method and state that a denoising
Autoencoder transports mass to decrease the Shannon entropy of the data distribution. However, the
1
Under review as a conference paper at ICLR 2020
(a) Network forward propagation
^(0	^(∕)
μt
Figure 1: Demonstration of a DNN forward, distributions propagation and the intuition of the label
distribution. (a) Given an input x, We train hidden layers and classifiers to output label distributions bl.
(b) Across-layer propagation: the label distribution b0t) propagate to bLt). Single-layer propagation:
the label distribution bk0) propagate to bkt) during the epochs in one layer. (c) Given an image with
tWo labels (i.e., cat and dog), the label distribution depicts their corresponding probabilities of 0.5.
across-layer propagation
需 single-layer
♦ propagation ..
…皿ʌllɪi
ʌ(θ	^(0
Rk	Ml 〃
(c) Label distribution
(b) Label distribution propagation
method is limited and inflexible for analyzing a general case of deep neural networks. Therefore, it is
very necessary and important to develop a new analytical method to interpret the stability of layers.
In this paper, we apply optimal transport theory to analyze the behavior of distributions. Specifically,
we exploit Wasserstein divergence (W -distance) to measure the difference between the distribution
of any layer and the target distribution. By monitoring the change of the W-distance, we are able to
study both across-layer and single-layer behaviors. Our contributions are summarized as follows.
•	We analyze the across-layer behavior and prove that the W -distance between the distribution of
any layer and the target distribution decreases along the depth of a DNN. It means that every layer
of the network can express the target distribution progressively.
•	We analyze the single-layer behavior and prove that for a specific layer, the W -distance between
the distribution in an iteration and the target distribution decreases across the training iterations
when introducing a loss in the layer.
•	Moreover, we provide experiments and theoretical justifications on these findings. The proposed
analytical framework provides a different view of understanding and interpreting neural networks.
2	Related Work
Many studies analyze the features of intermediate layers. (Bau et al., 2017) evaluates how hidden
units align a set of semantic concepts to quantifies the interpretability of latent representations of a
CNN. (Dosovitskiy & Brox, 2016) inverts image representations with up-convolutional networks
for studying image representations. (Zeiler & Fergus, 2014) presents a visualization technique to
investigate the function of intermediate feature layers and the classifier. (Zhang et al., 2018) learns an
explanatory graph to reveal the knowledge hierarchy hidden inside a pre-trained CNN. (Zhang et al.,
2019) learns a decision tree to clarify the specific reason for each semantic prediction of a CNN.
Some studies analyze the classification accuracy of a DNN. (Lee et al., 2015) introduces a classifica-
tion objective to the individual hidden layers, in addition to the overall objective at the output layer.
(Szegedy et al., 2015) uses auxiliary classifiers to improve the classification performance. (Kaya
et al., 2019) introduces internal classifiers into off-the-shelf DNNs to understand overthinking of
networks by studying how the prediction changes during a DNN's forward pass. (Gupta & Schutze,
2018) explains recurrent neural networks by understanding the layer-wise semantic accumulation
behavior. (Sonoda & Murata, 2019) investigates the feature map inside a DNN by tracking the
transport map, and prove that a deep Gaussian DAE transports mass to decrease Shannon entropy of
the data distribution. (Alain & Bengio, 2016) experimentally observes that the linear separability
of features increases monotonically along the depth of a DNN. However, there is no theoretical
result to analyze this phenomenon. Existing studies using information bottleneck methods (Tishby &
Zaslavsky, 2015; Bang et al., 2019) mainly analyze the dynamics of across different layers. However,
it is hard for these methods to analyze the dynamics of a specific layer through different iterations. In
contrast, our proposed method is able to analyze both single-layer and across-layer behaviors.
2
Under review as a conference paper at ICLR 2020
3	Problem Setting
Notation. We use bold lower-case letters (e.g., x) to denote vectors, and bold upper-case letters
(e.g., X) to denote matrices. We denote the transpose of a vector (e.g., xT) or matrix (e.g., XT)
by the superscript T. Let 1:=[1,..., 1]T and 1s(∙) be an indicator function of a set S, and let
[n]={0,1,...,n}. Let * be the convolution operator, grad be gradient operator.
Label distribution learning. In this paper, we consider a label distribution learning problem (Geng,
2016), especially multi-label classification. In this setting, the label distribution is defined as a
probability distribution to cover a certain number of labels, representing the degree to which each
label describes the instance (see Figure 1 (c)). Note that the sum of the label distribution is equal to
1. Specifically, given training samples S, we seek to learn a model f from data space X into the
∙-v	∙-v	∙-v
label space Y. In Figure 1(a), for L-layered neural networks, We denote fo：i=fi。…◦fo, l≤L as the
∙-v
output of the l-th layers, then the label distribution mapping can be defined as fi：=hi◦fo：i, where hi
is a probability function (e.g., FC+softmax (Frogner et al., 2015)). Note that fl, l∈[L] have the same
input domains and the same output domains. Our goal is to learn a model fl to let the predicted
distribution μι close to the target distribution μ. Then, we optimize the empirical risk:
minfι L(fi)：= ES [d(bi, μ)], where bi=fi#(Mo), l≤L,	(1)
where ES[∙] is the expectation w.r.t. the training set S, fi# denotes the pushforward operator of μo,
and d(∙, ∙) is some distribution divergence, such as Cross-entropy (CE) loss and Wasserstein loss. In
practice, the label distribution is obtained by training with CE loss. Because of the convexity of CE
loss, we derive the optimal label distribution for given features of every layer (Alain & Bengio, 2016).
In this sense, the label distribution reflects the actual distribution of feature maps in a specific layer.
Label distribution propagation. In this paper, our goal is to explore how the layer distributions
propagate in the across-layer and single-layer of a DNN. Specifically, we measure the divergence
(e.g., Wasserstein distance (Villani, 2008)) to observe how the layer distributions change across
different layers or iterations. In Figure 1(b), we explain the behaviors of layer from the following two
perspectives. For the across layers, measuring the distance between the distribution of any layer and
the target distribution helps to understand the learning process and distribution expression ability. For
a single layer, measuring the distance between the distribution in an training iteration and the target
distribution helps to understand the dynamic behavior and training process of a DNN.
4	Wasserstein Distribution Propagation Quantification
Optimal transport (OT). Recently, the performance of multi-label classification can be improved
by using W-distance (Frogner et al., 2015; Zhao & Zhou, 2018). However, DNNs on the multi-label
classification task still lack theoretical understanding. With the help of OT theory (Villani, 2008),
W-distance can be used to interpret DNNs. In contrast, some measures, such as Jensen-Shannon
divergence, which, however, often ignore any metric structure of distribution (Frogner et al., 2015).
Definition 1 (W-distance (Villani, 2008)) Given a prediction distribution μ and the target distribu-
tion μ, and the cost matrix C defined as Cκ,κ =dK(κ, κ0) with the metric dκ, where κ, κ0 are label
tags, then the W-distance seeks to find a transportation matrix T by transporting a mass μ to μ,
W 2 (b,μ)=inf τ ∈∏(b,μ)hT,C i,	⑵
where Π(b, μ) is the set ofcouplings and defined as Π(b, μ)={T∈RK×K: T 1=b, TT1=μ}.
In practice, the distance of two label tags in the cost matrix can be calculated by using word2vec
(Mikolov et al., 2013) in the Euclidean space. We calculate W-distance with entropic regularization
by Sinkhorn algorithm (Knight, 2008) with p=2. See more details in Supplementary materials.
Distribution propagation measure. In this paper, we use W -distance to measure the across-
layer and single-layer label distribution propagation, as shown in Figure 1(b). For an across-
layer, we measure W -distance between the distribution of any layer and the target distribution,
i.e., W2(b1,μ),..., W2(bL,μ). In this way, we can measure how the distributions propagate across
different layers. For the single-layer, we measure W -distance between the distribution in an training
iteration and the target distribution, i.e., W2(b(0),μ),..., W2(b(t),μ). Therefore, we can study the
behavior of distributions in the same layer.
3
Under review as a conference paper at ICLR 2020
j T T-net
S S S-net
,…	fι	,…	……[月| [g …	^∞~∖"	""	f；	G ,… C
(a) one-layer behavior analysis (b) infinite-layer behavior analysis (C) finite-layer behavior analysis
Figure 2: The teacher-student paradigm for analyzing layer behaviors of DNNs. We analyze one-layer
behavior in (a), infinite-layer behavior in (b) and finite-layer behavior in (c).
5	Teacher-student framework for layer BEHAVIOR Analysis
Teacher-student analysis framework is widely used to analyze and understand DNNs (Tian, 2017;
Du et al., 2018; Cuturi, 2013). For the one-layer behavior, this framework helps to understand the
dynamic of student network from the teacher network. For the multi-layer behavior, this framework
helps to study the ability of the student network to express distributions of the teacher network.
Therefore, teacher-student analysis framework is important and necessary to understand DNNs.
5.1	One-layer B ehavior Analysis
In our problem setting, the target distribution in the training loss (1) can be represented by an output
of a teacher network (T-net). Our goal is to make the output of the student network (s-net) to be close
to the output of T-net, as shown in Figure 2 (a). specifically, given a s-net f and T-net f0, our goal is
to find a function f that minimizes the following training loss,
L(f) ：= ES [d(bι,μ)] ：= Ex〜S [kf(X) — f0(x)k2],	⑶
where e=x+ε, ε〜Vt is the corruption of data, and Vt is a noise distribution w.r.t. the variance tI,
i.e., νt=N(0, tI). By optimizing the training loss, the distribution in one layer can be monitored by
using a transport map. Next, we provide the definition of transport map below.
Definition 2 (Transport map) A transport map ft : Rm→Rm can be defined as ft(x)=x+gt(x)
with an update direction gt when t>0, and f0(x)=x when t=0.
Based on the definition of the transport map, we analyze the layer behavior of ResNet (He et al.,
2016), i.e., T-net is a residual block. Note that our analysis method can be extended to a more general
case. Then, we solve the optimal transport map as follow.
Theorem 1 (Optimal transport map) If the teacher network is a residual unit f 0(xe)=x+σ(xe),
where σ contains FC layer and an activation function. When vt：=N(0, tI), the minimum ft is
ft(X) =x + tvlog(νt * μo)(e) + σ(e) := e + gt(e).
The minimizer ftt updates x along the update direction gt(x). Therefore, the optimal transport map
ftt with time t transports the mass at the activation x toward x+gt (x). To associate with continuity
equation (sonoda & Murata, 2019), we introduce Wasserstein gradient flow (WGF) as follows.
Proposition 1 (Wasserstein gradient flow) Assume μt satisfies the continuity equation with the
gradient vector field ∂tμt=-V∙[μtVVt] of a potential function Vt, and if the function F satisfies
∂tF(μt)= JRm -Vt(X)[∂tμt](x)dx, then WGF coincides with ∂tμt=-gradF(μ..
Using the definition of WGF, we explore how the distribution propagate in one layer as follows.
Theorem 2 Based on the equivalent condition in Theorem 2 (Belavkin, 2016), and let ∆ be the
Laplacian operator At the initial moment t→0, the pushforward f#Mt with Gaussian distribution
satisfies the backward heat equation (BHE), and evolves according to Wasserstein gradient flow:
∂tμt=o(x)= - ∆μo(x)= - grad W2(μt,μ), x∈Rm.
Note that W is the potential function. Theorem 2 states that by introducing an auxiliary loss in a
layer, the W -distance can be decreased in the layer. It means that the label distribution can be close
to the target distribution across training iterations.
4
Under review as a conference paper at ICLR 2020
5.2	Multi-layer Behavior Analysis
(i)	Infinitely deep neural networks. For an infinitely DNN, we assume the number of layers and
hidden units and the size of dataset is infinite. Specifically, let 0=to<tι< •…<tL+ι=t, and let fo be
trained on μo with the variance t1-t0, then xi：=fo(xo) and its distribution is Mi：=fo#Mo. Then,
we train fι on μι with the variance t2-t1. In the same way, We obtain fι(xι) and μι:=力—i#Mi-i.
The composition of residual blocks can be written as f0.L(x)：=fL ◦• ∙ •◦fo(x), where t is total time.
Then, the velocity of the composition coincides with the vector field ∂tf0=tl (X)=▽%% (x).
Furthermore, we extend one-layer behavior analysis to the case of infinite layers, as shown in Figure
2 (b). Based on Theorem 1, we have the dynamical systems as follows.
Lemma 1 When the noise distribution is a Gaussian distribution, the continuous residual units is
defined as a flow 夕t： Rm→Rm ofthefollowing dynamical systems associated with the vector field
VVt, i.e., ddtx(t)=V log(μt(x(t)))+σ(x(t)), where t≥0 and μt=夕t#M0.
Based on the proofs of Theorem 2, we can achieve a similar conclusion as follows.
Theorem 3 Based on the equivalent condition in Theorem 2 (Belavkin, 2016), and when the noise
distribution is a Gaussian distribution, then the pushforward measure μt：=φt#μ0 evolves according
to Wasserstein gradient flow as follows:第 μt(x)=-gradW2 [μt, μ], μt=0=μ0.
Theorem 3 states that the W -distance can be decreased across different layers. It means that the label
distribution can be close to the target distribution along the depth of the layer.
(ii)	Finitely deep neural networks. When the number of layers and the hidden unit number are
finite, the above analysis is not available. As shown in Figure 2 (c), given a composition of L T-nets,
we try to analyze the ability of each layer of S-net to express the distribution of T-net. Hence, we
derive an approximation bound for finitely deep neural networks. Here, Barron function is present a
T-net and its definition is provided as follows.
Definition 3 (Barron function (Sonoda & Murata, 2019)) Thefunction φ is Barron function on
B If ψ satisfies Ωb(C)={φ: B→R: ∃φ,φ∣B=φ, kΦkB≤C,Φ∈Fb}, kΦkB：= JRm ∣∣w∣∣b∣b(w)∣dw,
where kwkB= supx∈B |hw, xi| and φ is the Fourier transform ofφ, and FB is the set of functions
with the Fourier inversion formula holds on B.
Based on the definition of Barron function, (Barron, 1993) states that a Barron function can be
approximated by a neural network with one hidden layer. Given a fixed composition of L Barron
functions, we derive the error bound of the approximation between such composite function and a
neural network with l hidden layers. Note that l is not necessarily equal to L. The approximation
bound of w.r.t. Wasserstein distance can be provided as follows.
Theorem 4 (Wasserstein distribution approximation) Given a data distribution μo and afunction
4 i： Rmi-I →Rmi, and let Lι=log(l)+1, if SUPPOrt (μ0)⊂K0 and 0(Ki-1)⊆Ki, 1≤i≤l, thefunction
4i is (Ll-IT)-Lipschitz and is a Barron function, i.e.,2 1∈Ωκ0 (Co) and ψi∈Ωκi-ι+sBm^ɪ (Ci),
then there exists a network f with l hidden layers with 4Cl2 ml Ll2-1 Ll2 /2 neurons on the i-th layer,
W2(μι,μ) ≤ L2 ,2Cι√m^ + Dl)) s2 + 1) , l ≤ L.	(4)
where Dl is the diameter of the set Kl, and , δ, s>0 are parameters.
Theorem 4 provides an error bound when using a neural network with l hidden layers to approximate
a fixed composition of L Barron functions. Moreover, the upper bound decreases with the increasing
of layers under certain conditions. Here, we can conclude that deep layers are not always better than
shallow layer for some specific samples. Such samples are often classified correctly in the shallow
layer rather than the deep layer, then the diameter Dl can be large for this set of samples. Hence,
shallow layers can behave better than deep layers for these kinds of samples.
5
Under review as a conference paper at ICLR 2020
#layer (ResNet)	#layer (VGG)
Figure 3: Wasserstein distance across different layers for different networks.
IayerO
layer?
Iayer9
target
Φ Q 0∙5
■ Iayer3
・ laye∏3
target
target
0I
40
40
BO 1
40
# label
# label
# label
# label
# label
1。	15
1。	15
1。	15
IO 15	2。
20	40	60	80
40	60	80
10	15	2。
BO 1	20	40
1□	15 2Q 1
2。	4。	6。	8。
Iayer2
⅛> ：
°c 0⅛	5
oo0-5Γ
I
-0?
«0 0-5Γ
荏 ：
映0L
,nθ 0-5Γ
Iayer2
IayerO
15
Iayer6
■ Iayer3
40	60	80
Iayer6
15	29 1
■ Iayerll
laye∣7
20 40 60 89 L
Iayerll
15
Iayer9
20	40	60	8?
■ laye∏3 ：
15	29
target
1	5
Figure 4: Distribution propagation across different layers for different networks.
6	Experiment
Implementation Details. All experiments are implemented on PyTorch. In the training, we use SGD
optimizer with the initial learning rate as 0.01. The learning rate decays by a factor of 10 for every 40
epochs. The momentum and weight decay is set to be 0.9 and 10-4, respectively. We obtain models
(including ResNet-18, ResNet-50 (He et al., 2016) and VGG-16 (Simonyan & Zisserman, 2014))
pre-trained on ImageNet(Deng et al., 2009) and train them for 100 epochs with the batch size of 128.
Then, we freeze the model weights and train the internal classifiers (i.e., the network hl in Figure 1(a))
for 100 epochs. Each internal classifier contains fully connected layer and Sigmoid function. We train
the network using Binary Cross Entropy. We use training set to explore the distribution propagation
of the across- and single-layer. In addition, we set α=0.01 in Eqn. 17 to achieve the balanced results.
For simplicity, we use ResNet to represent the ResNet-18 model and VGG to represent the VGG-16
model in the main paper.
Data Sets and Evaluation Metrics. We conduct experiments on two benchmark multi-label clas-
sification datasets, i.e., VOC2007 (Everingham et al., 2010) and MS-COCO (Maas et al., 2013).
VOC2007 (Everingham et al., 2010) contains 9,963 images from 20 object categories, which is
divided into train, val and test sets. MS-COCO (Maas et al., 2013) contains 82,783 images as the
training set and 40,504 images as the validation set. The objects are categorized into 80 classes with
about 2.9 object labels per image. We use the classification accuracy, average per-class F1 (CF1),
average overall F1 (OF1) and mean average precision (mAP) as the evaluation metrics.
6.1	Across-layer Behavior Exploration
6.1.1	The behaviors of label distributions in different layers
How does every layer express the distribution? Here, we study the expression ability of every
layer, which is measured by Wasserstein distance (W-distance) between the label distribution of any
layer and the target distribution. As suggested in Theorem 4, deep layers have smaller error bound
than shallow layers, meaning that they have better expression ability. From Figure 3, deep layers have
smaller the W -distance than shallow layers, which verifies the conclusion in Theorem 4. Moreover,
the W -distance decreases from shallow layers to deep layers. The shallow layers have similar ability
to express the target distribution since they have similar values of the W-distance. When approaching
to the last layer, the W -distance drops to a very small value. It implies that sufficient layers are able
to express the target distribution. More results are shown in Section F in Supplementary materials.
6
Under review as a conference paper at ICLR 2020
Table 1: Improve performance of ResNet and VGG.
Method	VOC				COCO			
	accuracy	CF1	OF1	mAP	accuracy	CF1	OF1	mAP
ResNet	64.48	58.02	59.10	85.18	-2733-	55.65	60.30	64.36
ResNet+EarlyExits	66.01	58.67	58.76	85.49	30.03	57.49	61.38	67.28
VGG	68.96	58.58	59.71	88.48	32.41-	59.37	62.94	70.64
VGG-EarlyExits	69.85	59.49	59.94	88.57	33.95	60.32	63.73	71.95
Figure 5: Demonstration of properties on easy, hard and confused samples.
How does the distribution propagate across layers? In this experiment, we investigate how the
label distribution propagate across layers to understand the learning process from layer to layer in a
DNN. Note that we normalize the prediction of classifier as the probability distribution (i.e., the sum
of label prediction probabilities of one sample is 1.). Figure 4 shows that the label distribution of one
sample propagates from the first layer to the last layer. In the shallow layers, the label distribution is
far away from the target distribution, then it can be close to the target distribution in the deep layers.
Different from the decreasing tendency in Figure 3, the label distribution of one sample may not close
to the target distribution progressively. For example, in the first row of Figure 4, the probability of the
10-th class is large in the 0-th layer, but goes down in the 3-th layer.
6.1.2	The evaluation of across-layer’s behaviors
How to evaluate the quality of each layer? Existing methods attempt to visualize the feature maps
to evaluate the quality of each layer. However, observing the feature maps is difficult because of
its high dimensionality and complexity. Relying on the W -distance, we can evaluate the quality of
each layer by measuring the ability of expression. If the W -distance decreases rapidly in a layer, it
suggests that the quality of this layer is good. Taking ResNet on VOC in Figure 3 as an example,
0-5 layers have similar value of the W-distance, while 6-8 layers have quite different value of the
W-distance. It means that these deep layers (i.e., 6~8) are better than the shallow layers (i.e., 0~5).
6.1.3	How to exploit the across-layer behaviors
Does every sample contribute equally? In practice, a deep neural network construct more complex
features progressively throughout the layers (Lee et al., 2011). An interesting question arises: does
the contribution of each sample vary across different layers and training iterations? To answer this
question, we first define the concept of sample difficulty in terms of the W -distance. As shown in
Figure 5, the samples can be divided into three categories, including easy, hard and confused. The
intuition is that: 1) Easy samples should have small the W -distance in the first few layers, i.e., , they
should be classified correctly with high confidence in a shallow layer. 2) Hard samples would have
large the W -distance in a deeper layer, i.e., , it cannot be resolved at all, or only near the last layer. 3)
Confused samples have small the W -distance in a shallow layer, while have large the W -distance in
a deep layer. It means that although these samples can be classified correctly in the shallow layer,
they still become misclassified at the last layer. Considering the difficulty of samples would help to
interpret the training of models and design the training loss, which may speed up convergence and
improve the performance eventually. We show more results in Section I in Supplementary materials.
7
Under review as a conference paper at ICLR 2
1.8
1.0
go>)"nsw
uutt-p,M
#epoch
0,2 1	10	20	30	40	50	60	70	80	90 100
1.0
α,H∙
1∙8
5>
#epoch
0∙2 1	10	20	30	40	50	60	70	80	90 100
..O 1.2
眈
to。
⅛rr 0.8
:5 ω
* 0∙4
≡812
—layer 1	—layer 4	—layer 9
1	10	20	30	40	50	60	70	80	90 100
# epoch
0.8
队4
—Iayerl —layer 6	—layer 13
1	10	20	30	40	50	60	70	80	90 100
# epoch
翼
Figure 6: Wasserstein distance between the distribution in an epoch and the target distribution across
different training epochs for different networks. We choose the 1,4, 9-th layer for ResNet and the
1,6,13-layer for VGG.
HoW to improve ClaSSifiCation performance? We WilldiSCUSS how to exploit the behaviors of
different layers to improve the model Performance. In Practice, we observe that in deeP neural
networks, some samples are correctly classified in the intermediate layers but misclassified in the last
layer. For example, 35.52% and31.04% of the samples for ResNet and VGG are misclassified on
the test set of VOC2007, respectively. For these samples, 5.96% and 7.76% samples are correctly
classified in all intermediate layers of ResNet and VGG, respectively. Based on this phenomenon, we
can improve the performance by early exiting such confused samples. Different from the strategy of
SDN (Kaya et al., 2019), how to early exit for multi-label classification DNNs is very challenging.
Therefore, we design a new early-exits strategy for multi-label prediction. In Table 1, our proposed
method consistently outperforms the baseline methods. We give more details about early-exits
strategy and show more results in Section H in Supplementary materials.
6.2	Single-layer B ehavior Exploration
In this section, we study how the distribution of an intermediate layer change during the training
process. We consider to use W -distance to measure the stability of distribution. We propose
theoretical and empirical analysis for the distribution stability in one layer.
How the diStribution propagate during updating DNNS? In this experiment, we investigate how
distributions propagate when training DNNs. From Figure 6, the label distribution in the first layers of
ResNet or VGG often fluctuates significantly due to the limited discriminative power of very shallow
layers. When approaching the last layer, the supervision is sufficient to decrease the W -distance.
These experiments justify Theorem 2 that the W-distance can be decreased with sufficient supervision.
More results are shown in Section G in Supplementary materials.
How Stable iS the diStribution in one layer? The training stability is a key problem in DNNs and
can be largely addressed by the widely used Batch Normalization (BN) (Ioffe & Szegedy, 2015).
However, we prove that the stability of distribution is hard to be guaranteed even with BN (See proof
in supplementary). In this paper, we use the W-distance to measure the discrepancy between the
distribution in an iteration and the target distribution across different training iterations in the specific
layer. Motivated by this, we further measure stability of label distribution during training DNNs. The
conventional understanding of BN suggests that the W-distance should decrease. Note that, from
Figure 6, the first layer of ResNet or VGG may fluctuate during the training process, which indicates
that deep layers often have better ability than shallow layers to express the target distribution.
7	Conclusion
In this paper, we have interpreted deep neural networks (DNNs) via understanding layer behaviors.
With the help of optimal transport theory, we propose a unified teacher-student analysis framework
to investigate the across-layer and single-layer behaviors of a DNN. Theoretically, we prove that i)
the W -distance between the distribution of any layer and the target distribution decreases along the
depth. ii) the W-distance of a specific layer between the distribution in an iteration and the target
distribution decreases along training iterations. Extensive experiments justify our theoretical findings.
The proposed analytical framework can facilitate future researches to interpret a deep neural network.
8
Under review as a conference paper at ICLR 2020
References
Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes.
arXiv preprint arXiv:1610.01644, 2016.
Seojin Bang, Pengtao Xie, Wei Wu, and Eric Xing. Explaining a black-box using deep variational
information bottleneck approach. arXiv preprint arXiv:1902.06918, 2019.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information theory, 39(3):930-945,1993.
David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection:
Quantifying interpretability of deep visual representations. In Computer Vision and Pattern
Recognition, pp. 6541-6549, 2017.
Roman V Belavkin. Relation between the kantorovich-wasserstein metric and the kullback-leibler
divergence. In Information Geometry and its Applications IV, pp. 363-373, 2016.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. In International Conference on Learning Representations, 2019.
Jiezhang Cao, Langyuan Mo, Yifan Zhang, Kui Jia, Chunhua Shen, and Mingkui Tan. Multi-marginal
wasserstein gan. In Advances in Neural Information Processing Systems, 2019.
Chun-Fu (Richard) Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, and Rogerio Feris. Big-little net:
An efficient multi-scale feature representation for visual and speech recognition. In International
Conference on Learning Representations, 2019a.
Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look
at few-shot classification. In International Conference on Learning Representations, 2019b.
Grigorios G. Chrysos, Jean Kossaifi, and Stefanos Zafeiriou. Roc-GAN: Robust conditional GAN. In
International Conference on Learning Representations, 2019.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in
neural information processing systems, pp. 2292-2300, 2013.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255, 2009.
Alexey Dosovitskiy and Thomas Brox. Inverting visual representations with convolutional networks.
In Computer Vision and Pattern Recognition, pp. 4829-4837, 2016.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? 2018.
Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.
The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):
303-338, 2010.
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, and Tomaso A Poggio. Learning
with a wasserstein loss. In Advances in Neural Information Processing Systems, pp. 2053-2061,
2015.
Matthias Gelbrich. On a formula for the l2 wasserstein metric between measures on euclidean and
hilbert spaces. Mathematische Nachrichten, 147(1):185-203, 1990.
AUde Genevay, Gabriel Peyr6, and Marco Cuturi. Learning generative models with sinkhorn diver-
gences. arXiv preprint arXiv:1706.00292, 2017.
Xin Geng. Label distribution learning. IEEE Transactions on Knowledge and Data Engineering, 28
(7):1734-1748, 2016.
Pankaj GUPta and Hinrich Schutze. Lisa: Explaining recurrent neural network judgments via layer-
wise semantic accumulation and example to pattern transformation. Empirical Methods in Natural
Language Processing Workshop BlackboxNLP, 2018.
9
Under review as a conference paper at ICLR 2020
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Yen-Chang Hsu, Zhaoyang Lv, Joel Schlosser, Phillip Odom, and Zsolt Kira. Multi-class classification
without multi-class labels. In International Conference on Learning Representations, 2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, 2015.
Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras. Shallow-deep networks: Understanding and
mitigating network overthinking. In International Conference on Machine Learning, volume 97,
pp. 3301-3310, 2019.
Philip A Knight. The sinkhorn-knopp algorithm: convergence and applications. SIAM Journal on
Matrix Analysis and Applications, 30(1):261-275, 2008.
Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-supervised
nets. In Artificial intelligence and statistics, pp. 562-570, 2015.
Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y Ng. Unsupervised learning of
hierarchical representations with convolutional deep belief networks. Communications of the ACM,
54(10):95-103, 2011.
Jun S Liu. Siegel’s formula via stein’s identities. Statistics & Probability Letters, 21(3):247-251,
1994.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network
acoustic models. In International Conference on Machine Learning, 2013.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations
of words and phrases and their compositionality. In Advances in neural information processing
systems, pp. 3111-3119, 2013.
Nicolas Papernot and Patrick McDaniel. Deep k-nearest neighbors: Towards confident, interpretable
and robust deep learning. arXiv preprint arXiv:1803.04765, 2018.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal-
ization help optimization? In Advances in Neural Information Processing Systems, pp. 2483-2493,
2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Sho Sonoda and Noboru Murata. Transport analysis of infinitely deep neural network. The Journal
of Machine Learning Research, 20(1):31-82, 2019.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Computer
Vision and Pattern Recognition, pp. 1-9, 2015.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. In International Conference on Machine
Learning, 2017.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015
IEEE Information Theory Workshop (ITW), pp. 1-5. IEEE, 2015.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
10
Under review as a conference paper at ICLR 2020
Chih-Kuan Yeh, Jianshu Chen, Chengzhu Yu, and Dong Yu. Unsupervised speech recognition
via segmental empirical output distribution matching. In International Conference on Learning
Representations, 2019.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European conference on computer vision, pp. 818-833, 2014.
Quanshi Zhang, Ruiming Cao, Feng Shi, Ying Nian Wu, and Song-Chun Zhu. Interpreting cnn
knowledge via an explanatory graph. In AAAI Conference on Artificial Intelligence, 2018.
Quanshi Zhang, Yu Yang, Haotian Ma, and Ying Nian Wu. Interpreting cnns via decision trees. In
Computer Vision and Pattern Recognition, pp. 6261-6270, 2019.
Peng Zhao and Zhi-Hua Zhou. Label distribution learning by optimal transport. In Thirty-Second
AAAI Conference on Artificial Intelligence, 2018.
11
Under review as a conference paper at ICLR 2020
SUPPLEMENTARY ON “TOWARDS INTERPRETING DEEP NEURAL
NETWORKS VIA UNDERSTANDING LAYER BEHAVIORS”
A Preliminary
Notation. Specifically, we use bold lower-case letters (e.g., x) to denote vectors, and bold upper-
case letters (e.g., X) to denote matrices. We denote the transpose of a vector (e.g., xT) or matrix
(e.g., XT) by the superscript T. Let 1 := [1, . . . , 1]T. For two matrices A and B of the same size,
their inner-product ishA, B)=tr(ATB). Let * be the convolution operator. Let k ∙ ∣∣ = k ∙ ∣∣2 denote
Euclidean norm on vectors in Rn. For a function f, let f∨:=/(-x). Let ∆f = PZi ∂f denote
the Laplacian. Let Bn be the unit ball in n dimension, i.e., Bn={x∈Rn : ∣x∣≤1}. For two sets A
and B and a scalar r ∈ R, let A+B={x+y : x∈A, y∈B} and rA={rx : x∈A}.
Definition 4 (Lipschitz continuous) A function 夕：Rm→Rn is called L-LiPschitz continuous if
there exists a real constant L≥0 such that, for all x and y w.r.t. the L2 norm,
13(X)- ψ(y)k2 ≤ Lkx - y∣2.
Definition 5 (Fourier transform) For 夕∈L1(R), Fourier transform of 夕：Rn→R is defined as:
b(w) = (⅛ IJ(XLw,xidx.
(5)
For vector-valued functions 夕：Rn→Rm, We conduct component-wise Fourier transform.
Let b∨(x) := b(-x), the inverse Fourier transform is
(F-1φ)(x) := I 夕(w)eihw,xidx = (2π)n0∨.
Rn
(6)
Definition 6 For a bounded set B, the norm of a function f : Rn →R can be defined on a set B,
ʌ
kfkB := JRn IIwkB If(W)|dw, Where IIwkB =suPχ∈B |hw, xi| ∙
B	Proof of Theorem 4
Definition 7 (Barron function (Sonoda & Murata, 2019)) Thefunction 夕 is Barronfunction on B if
ψ satisfies Ωb(C)={φ: B→R: ∃φ,φ∣B=g, kφkB≤C, φ∈FB}, where kφkB= JRmkwkB∣b(w)∣dw,
where kwkB= supx∈B |hw, xi| and φ is the Fourier transform ofφ, and FB is the set of functions
with the Fourier inversion formula holds on B,i.e.,
FB =	φ :	Rm → R : ∀x	∈ B,	φ(x) =	φ(0) +	eihw,xi	- 1	φb(w)dw	.	(7)
Definition 8 (Sigmoidal function) A sigmoidal function is a bounded measurable function σ: R→R
such that limx→-∞ σ(x)=0 and limx→+∞ σ(x)=1.
Theorem 5 (Barron theorem (BarrOn, 1993)) Let B⊆Rd be a bounded set, and μ be any prob-
ability measure on B,夕∈Ωb(C) and f (x)= En=I ciσ(hwi, xi + bi), where σ(∙) be a sigmoidal
function, then there exist wi∈Rd, bi∈R, Ci∈R with En=1 ∣c∕≤2C such that
k(χx) — f(x)kμ := ZBW(X)- f(x))2μ(dx) ≤ (2C)-.	⑻
We extend the above theorem to the case of the composition of Barron functions denoted as 夕j” :=
夕j ◦夕j-i ◦•••◦ ψi.
Corollary 1 Let B be a bounded set, μ be any probability measure and the neural network
f (x)= En=I c%σ(hwi, xi+bi), where σ(∙) is a sigmoidal function. Ifthe composition of Barron
12
Under review as a conference paper at ICLR 2020
functions is restricted in the Setof Ωb(C), then there exists Wi∈Rd, bi∈R, Ci∈R with En=I ∣c∕≤2C
such that
ll^k：i(x) - f(x)kμ := / (2 k：i(x) - f(x))2 μ(dx) ≤ (2C)- ∙	(9)
Bn
Proof Directly using Lemma 1 and Theorem 2 of (Barron, 1993) can complete the conclusion.
Theorem 6 (Extend Barron theorem) Let B be a bounded set, μ be any probability measure
and f (X)= En=ι c%σ(hwi, x〉+bi), where σ(∙) be a sigmoidal function. If SUPPort (μ0)⊂K0 and
夕i(Ki-ι)⊆Ki, 1≤i≤l, thefunction ψi is lθg(l1)-Lipschitz and Barronfunction, i.e.,夕i∈Ωk0 (Co)
and ψi∈Ωκi-ι+sBm^ɪ (Ci), then there exists a neural network f with l hidden layers and S ⊂ Rd0
2
satisfying μo(S)≤1 - s2(log(l-1)+1)2 so that
(∕1s k” fk2dμ0)2 ≤ iog(^ɪ .
(10)
Proof For simplicity, let P=Wl：i=WlQ •…◦夕ι be the composition of Barron functions, especially
夕i=0k：i, where Wl is defined in Corollary 1. Let f :=力：i=fl◦力-ιQ …。fι be the composition of
k-layered neural network, where fi : Rdi-1 →Rdi-1 are functions defined as
ri
(fi(x))j = Ecijk ∙ σ(hwijk, Xi + bijk),	(11)
k=1
where cijk, bijk∈R and wijk∈Rdi-1 are parameters, and ri is the number of nodes in the i-th layer.
Note that the function fi is represented by a neural network with one hidden layer and a linear output
layer. We prove the theorem by induction on l.
i)	For l=1, using assumptions of the support of initial distribution (i.e., SUPPort(μ0)⊂K0) and the
definition of Barron function. Then, =(2C1)2/r1 and Pir=1 1 |ci| ≤ 2C1, we complete the results.
ii)	For l>1, by the induction step, we assume the functions f1, . . . , fl-1 satisfy the conclusion for
We1, . . . , Wel-1. Let Sl-1 be the set in the conclusion. Applying Corollary 1 to fl to get that for each
1 ≤ j ≤ mj, for any μ supported on a set K— ⊆ Rml-I and any rl ∈ N, there exists a neural net
fl,j with 1 hidden layer with rl nodes such that
∖ 2
(Wl,j - fl,j)2dμ
≤
2Cfl,Kl-1
√rl
Note that Theorem 1 applies to any distribution μ supported on the set Kl-∖.
Let Sl = Sl-ι ∩ {x : fl-Li(x)∈Kl-ι+sBm∣-ι}.	Applying Theorem 1 with
K0=Kl+sBml ,rl = 14c2ml(lοg(lτ!+1)2(lοg(l)+1)2 m and μ=力—i：i#(1slμο). We have that μ
is supported on gl-1:1(Sl) ⊆ Kl-1+sBml-1 =Kl0-1, and the function Wl is Cl-Barron function on
this set by the assumption. The conclusion of Theorem 1 gives (fl)j such that
Of	/	. 、2 …	,	、、\1	2Cl	E
Rml-I(Wlj- fl,j) d(fj1#(ISl"o))) ≤√rl ≤ √ml(iοg(i-i)+i)(iοg(i)+i) ∙
For all elements of Wl and fl, we have
(Zml-JWl- flk2d(fl-1:1#(ISl"ο)))	≤ (lοg(l-1)+l)(lοg(l)+1).
13
Under review as a conference paper at ICLR 2020
We bound by the triangle inequality
1
1
2
≤LipS) (∕m1slτMτ1 - flτ1k2dμ0) + (log(l-1)+l)(log(l)+1)
≤ log(l)+1 log(l-1)+1 + (log(l-1)+1)(log(l)+1)
l
≤ log(l)+1，
where the last inequality holds by the assumption (i.e., Lip(夕l)= lθg(l1)) and the induction hypothe-
sis.
As above analysis, we have
/ 1sl-ι k夕i-i：i - fl-Lidμo∣∣2dμo
2
≤ (Iog(i-1)+1)2,
by the induction hypothesis. Also,夕l—i：i ∈ Kl-ι for all X ∈ support(μ0) by the assumption of the
theorem. Therefore, by Markov’s inequality and the induction hypothesis on Sl-1,
μo (Sl-1 ∩ {x : X ∈ Kl-1 + sfl-l:l(Bml-1)})
≤μo (Sl-1 ∩ {x : I3l-I：i -力一i:ik ≥ s}) ≤
2
s2(log(l-1)+1)2 .
Therefore,
2
μo(Sl) ≤ μo(sι-ι) -
s2(log(l-1)+1)2 ,
2
the last equality holds by a fact that there exists a constant δ to satisfy μo(Sl) = 1- s2(iog(；—i)+i)2. 口
Based on the above conclusions, we prove Theorem 4 as follow.
Theorem 4 Given a data distribution μo and afunction ^: Rmi-I →Rmi, and let Ll =log(l)+1, if
SUPPOrt(μ0)⊂K0 and 夕 i(Ki-ι)⊆Ki, 1≤i≤l, the function 4 i is (Ll-IT) -Lipschitz and is a Barron
function, i.e.,夕i∈Ωk0 (Co) and ψi∈Ωκi-ι+sBm^ɪ (Ci), then there exists a network f with l hidden
layers with 4Cl2mlLl2-1Ll2/2 neurons on the i-th layer,
W2(μl,μ) ≤ L2 02Cl√m^ + Dl) S + 1) , l ≤ L.
where Dl is the diameter of the set Kl, and , δ, s>0 are parameters.
Proof The functions fι,…，fl in Theorem 6 satisfy that
(12)
/1Sl-IkAT:1 - fl-1:1d〃0k2d〃0
2
≤ (log(l-1)+1)2 .
The range of fl = ((fl) 1, ∙∙∙ , (fl )m1) is contained in a set of diameter 2Cl √ml because the activation
in a neural network is ranged in [0, 1] and the weights cijk in Theorem 6 satisfy Prk=1 |cijk| ≤ 2Cl.
14
Under review as a conference paper at ICLR 2020
Choose a constant vector k to minimize JSJl夕i：i(x) - fi：i - k∣∣2dμ0 and replace 力 with f + k. Note
2
that the range of 力 and 夕ι necessarily overlap. We still have JSk夕i：i(x) — fid∣∣2dμo ≤
Moreover, we have
(iog(i)+i)2.
Is (X)-力(χ)k≤ 2Ci √m1 + Di,
for x ∈ K0, where Di is the diameter of Ki. Let Sic be a complementary of Si, we have
μ(Sc)= s2(log(l-1)+1)2 ,
then we have
I k夕l:1 - fl:1 k2dμ0 ≤ I k夕l:1 - fl:1 k2dμ0 + / k2l:1 - fl:1 k2dμo
K0	Sl	Slc
e2	δ62
≤ (log(l) + 1)2 + (2Cl√ml + Di)) s2(log(l-1)+1)2
= (log(l) + 1)2 ((2Cl√mr + DI))S2 + 1).
For (2i：i(X),力：i(X)), X 〜μo define a coupling between the distributions. Based on the definition
of W-distance, we have
W 2(μk, μ) =EX 〜μok 夕 l:1 - fl:lk)
I	k夕i：i - fi：i||2dM0
K0
≤ (log(ŋ + 1)2 ((2Cll√m + Dl)2 S) + 1)∙

C Proofs of Layer B ehavior Analysis of Deep Neural Networks
Definition 9 (Flow) A flow ψt is g^ven by an ordinary differential equation (ODE), St(x)=vt(夕t(x))
with a velocity field Vt when t>0, and so(x)=x when t=0.
Proposition 2 (Continuity equation (Sonoda & Murata, 2019)) Let St be the flow of an ODE
with vector field Vt, then the distribution μt evolves according to the continuity equation
∂tμt(x)=-V∙[μt(x)vt(x)], x∈Rm,t≥0, where ▽• denotes the divergence operator.
Theorem 7 (Optimal transport map) The global minimum ft of L(f) is attained at
1
ftt(Xe) =Xe—
Vt * μo(e)
ε	ενt(ε)μo(e — ε)dε,
Rm
where * denotes the convolution operator.
Proof The proof follows from the calculus of variations.
L(f) = [ Eε [kf(x + ε) — f0(x)k2i μo(x)dx
Rm
=[Eε [kf(x0) — f0(x0 — ε)k2 μo(x0 — ε)i dx0,
Rm
where the second line holds by the calculus of variations, i.e., X0 = X +ε. Then, for an arbitrary
function h, the first variation δL(h) is given by
δL(h) = -d- L(f + Sh)
ds
s=0
=/ ∂SEε [kf(x) + Sh(X) — f0(x — ε)k2 μo(x — ε)i dx
=2 / Eε [(f (x) — f(x — ε)) μo(x — ε)] h(x)dx.
Rm
s=0
15
Under review as a conference paper at ICLR 2020
At a critical point f * of L, δL(h) = 0 for every h. Hence,
Eε [(f (x) — f0(x — ε)) μo(x — ε)] = 0, a.e. x,
by the fundamental lemma of calculus of variations for integrable functions, and we have
f*( ) = Eε [f0(X — ε)μo(X — ε)]
f ( )=	Eε [μo(x - ε)].
Note that f * attains the global minimum, because, for every function h,
L(f * + h) = [ Eε [kf*(x) - f(x — ε) + h(x)k2 μo(x - ε)] dx
Rm
=[Eε	hkf*(x) - f0(x — ε)k2	μo(x —	ε)i	dx + [	Eε	[kh(x)∣∣2 μo(x —	ε)]	dx
Rm	Rm
+ 2 [	Eε	[h(x)τ	(f *(x)	-	f0(x	一 ε))	μo(x	-	ε)]	dx
Rm
=L(f*)+ L(h) + 2 ∙ 0 ≥L(f*).

Theorem 1 If the target network is a residual unit f 0(xe)=x+σ(xe), where σ contains fully connected
layer and activation function. When Vt := N (0, tI), then the global minimum f is
f；(X) =X + tvlog(νt * μo)(x) + σ(x) := x + gt(x).
Proof Using Stein’s identity (Liu, 1994), -tvνt(ε) = ενt(ε), which is known to hold only for
Gaussians.
ft*(xX) =xX -
xe +
Vt * μo(X)
1
Vt * μo(X)
/
Rm
Z
Rm
ενt(ε)μo(X — ε)dε + σ(X)
tVνt(ε)μo(X — ε)dε + σ(X)
1
X + tvνt * *) + °(x)
Vt * μo(x)
X + tv log(Vt * μo(X)) + σ(X).

Theorem 2 Based on the equivalent condition in Theorem 2 (Belavkin, 2016), and let ∆ be the
LaPlaCian operator. At the initial moment t→0, the pushforward f#Mt with Gaussian distribution
satisfies the backward heat equation (BHE), and evolves according to Wasserstein gradient flow:
∂tμt=o(x)=-∆μo(x)=-grad W2(μ0, μ), x∈Rm.
Proof The initial velocity vector is given by
∂tft*=0(x) = limo fL(Xt_x = vlogμo(x) + σ(x).	(13)
Hence, by substituting the score (13) in the continuity equation of Proposition 2, we have
∂t μt=o(x) = -V∙[μo(x)(V log μo(x) + σ(x))] = —∆μ°(x).
We leave the proof of ∆μo(x) = W2(μt,μ) in Theorem 3.	□
Theorem 3 Based on the equivalent condition in Theorem 2 (Belavkin, 2016), and when the noise
distribution is a Gaussian distribution, then the pushforward measure μt:=夕t#Mo evolves according
to Wasserstein gradient flow as follows:
dtμt(x)=-∆μt(x) = —gradW 2(μt,μ), μt=o(x)=μo(x).	(14)
16
Under review as a conference paper at ICLR 2020
Proof Let the potential function be equal to KUllback-Leibler divergence be KL(μt,μ) =
R μt(x)log μ(x - μt(x) + μ(x)dx, then Vt = -(log(μt) - log(μ)),then
gradF(μt) = V∙(μtV(log(μt) - log(μ))) = ∆μt.
The above continuity also satisfies the case of Wasserstein distance, then using the equivalent
condition in Theorem 2 (Belavkin, 2016), we have
∂tμt=0(x)=-grad W2(μt,μ), x∈Rm.
We assume the distribution μl(t) is Gaussian distribution. The W-distance of two Gaussian distribution
is defined as follow.
Proposition 3 (GeIbrich, 1990) Given two Gaussian distributions μι〜N(uι, ∑ι) and
μ2〜N(u2, ∑2), then Wasserstein distance between μι and μ2 is
W2(μ1 ,μ2)=ku1 - u2k2 + tr (∑1+∑2 - 2 (∑2∑2∑2) 2).
Batch Normalization is proposed to stabilize the distributions of layer distribution by introducing the
parameters γt and βt in the t-th optimization step. Let Γl(t) =diag(γl(t) ) and Γl(t+1) =diag(γl(t+1) ).
Proposition 4 Assume the distributions in two iterations in the l-th layer are Gaussian distributions,
i.e., μ(t)〜N(β(t),Γ(t)) andμ(t+1)〜N(β(t+1), Γ(t+1)), then the W-distance between μ(t andμ(t+1)
is W2(μ2μ(t+1)) = kβ(t)-β尸 k2 + k(Γ(t)-Γ尸)2k2.
Proof Directly from 3, and let u1=βl(t) , u2=βl(t+1) and Σ1=Γl(t) , Σ2=Γl(t+1) , we have
W2(μl(t), μl(t+1))=kβl(t)-βl(t+1) k2+k(Γl(t)-Γl(t+1))2k2.
17
Under review as a conference paper at ICLR 2020
D Optimal Transport
Definition 10 (Optimal Transport (Villani, 2008)) Given a cost function c: K×K→R, the optimal
transport distance measures the optimal plan to transport the massfrom a probability measure μ to
another probability measure μ:
W2(b, μ) =	inf	/C	c(κ, κ)π(dK,dκ),	(15)
π∈π(μ,μ)J Jk×k
where Π(b, μ) is the set ofjointprobability measures π on KXK with the marginals μ and μ.
In this paper, we consider the multi-label classification problem. We first introduce the definition of
Wasserstein distance. For the case of probability measures, they are histograms in the simplex ∆K .
Definition 11 (W-distance) Given an input x∈X and any f: X →∆K, let f (x) be the predicted
label distribution b, and let y be the target distribution μ, then the W-distance between μ and μ is
W2(b,μ)=	inf	(T, Ci,	(16)
T ∈π(μ,μ)
where C ∈R+K ×K is the cost matrix whose the element can be defined as Cκ,κ0 =dpK (κ, κ0), and
the set of couplings is composed of joint probability distributions with their marginals μ and μ,
i.e., Π(b, μ)={T∈RK×k: T 1=b, TT1=μ}.
Entropic Wasserstein loss. Cuturi (2013) introduces an entropic regularization such that the non-
convex problem (2) can be turned to a strictly convex problem:
dα(b,μ):=	inf	hT, Ci- 1H (T),	(17)
T ∈Π(μ,μ)	a
whereH(T)=-Pκ,κ0Tκ,κ0(log(Tκ,κ0)-1).
With the help of the entropic regularization, we solve optimal transport problem efficiently using
Sinkhorn-Knopp matrix scaling algorithm (Knight, 2008).
Proposition 5 The transport matrix T* optimizing Problem (17) satisfies T* = diag(U)Kdiag(v),
where K = e-αM, u = eαa and v = eαb, where a and b are the Lagrange dual variables.
Theorem 8 (Genevay et al., 2017)Wasserstein sinkhorn loss between the predicted label distribution
b and the target label distribution μ is defined as:
—一一 一 一 … ,
W2(b,μ) := 2W2α(b, μ) - W2α(b, b) - W2α(μ, μ),	(18)
with thefollowιng limiting behavior as ɑ → 0: W2(μ, μ) → 2W2α(μ, μ).
Note that normalized Wasserstein loss is non-negative and W2 α (b, μ) = 0 if and only is μ = μ. In the
quantification, we use normalized Wasserstein loss to measure the divergence between distributions.
18
Under review as a conference paper at ICLR 2020
E Intermediate layer
We add auxiliary classifiers for the intermediate layers as mentioned below. For ResNet-18, we
truncate each block and add a classifier after its feature map. We label these intermediate layers from
0-th layer to 8-th layer. The truncation of ResNet-50 is analogous to ResNet-18. For VGG-16, we
add a classifier after each convolution layer and label them from 0-th layer to 12-th layer.
8ZI>UO。CnXm
Figure 7: Definition of intermediate layer of ResNet-18.
φωraE~
寸 9>uo。mxm
HH—
寸 9>uo。mxm
8ZI>ug mxm
d.-δod
J-δod
NTS>ug mxm
I ⅛
Z--δod
z'-δod
960寸蚂
Figure 8: Definition of intermediate layer of VGG-16.
■ I ⅜
z、-δod
85>UO3 mxm
9SZ>uou mxm
9sz>uou mxm
9SN>U8 mxm
ZTS>u。。mxm
ZTS>U8 mxm
S>UO3 mxm
ZlS>uo。mxm
S>uo□ mxm
F ACROSS-LAYER DISTRIBUTION PROPAGATION
We ShoW more results for different datasets, measures and networks across different layers.
gu£MP—M
1.5
Figure 9: Wasserstein distance across different layers of ResNet-18 (left) and VGG-16 (right) on
different test set. Specifically, we shuffle the VOC2007 dataset (including training set and test set)
and divide it into training set and test set following the ratio of original dataset. We shuffle twice
and define them as “trial_0" and “trial_1”，respectively. Besides, the “orig.” means that we use the
original dataset. From Figure 9, different experiments consistently have the same decreasing tendency
through the depth of a neural network.
ι.o
0.5
0 0 0 1 2 3 4 5 6 7 8 9 10 11 12 13
#layer (VGG)
19
Under review as a conference paper at ICLR 2020
8UB4MP—M
0 0 0123456789
⅛layer (ResNet)
Figure 10: Wasserstein distance across different layers of ResNet-18 (left) and VGG-16 (right) on
test set. The tendency of distribution propagation on test set is the same as the training set(Figure 3),
suggesting that the distribution propagation is irrelevant to the property of the dataset.
ι.o
0.5
0 1 2 3 4 5 6 7 8 9 10 11 12 13
# layer (VGG)
Figure 11: Chebyshev distance across different layers of ResNet-18 (left) and VGG-16 (right) on
training set. Although the tendency of Chebyshev distance is analogous to Wasserstein distance,
Chebyshev distance ignores any metric structure (Frogner et al., 2015). Therefore, we use Wasserstein
distance to quantify the discrepancy of label distributions.
ιoo
0.75
0.50
0.25
①。U①6」①>-pl∞
0 00 0	1	2	3	4	5	6	7	8	9
# layer (ResNet)
1.00
0.75
0.50
0.25
0 00 0 1 2 3 4 5 6 7 8 9 10 11 12 13
# layer (VGG)
1.5
Figure 12: Jensen-Shannon divergence across different layers of ResNet-18 (left) and VGG-16 (right)
On training set. The conclusion of JenSen-Shannon divergence is the same as the Chebyshev distance.
ι.o
0.5
CD
U
⊂
CD
+J
ω
z5
I
M
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
# layer (ResNet)
1.5
ooo ι
0.0
1.0
0.5
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
# layer (ResNet)
1.00
0.75
Figure 13: Wasserstein distance across different layers of ResNet-50 on training set (left) and test set
(Iight). We get the same COnClUSiOn on ResNet-50 that Wasserstein distance between the distribution
of any layer and the target distribution tends to decreases along the depth of a DNN.
0.50
0.25
→- VOC →- coco
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
# layer (ResNet)
0.00
叩 0.25
S
f 0.00
1.00
ω
u
U 0.75
ω
6
ω 0.50
Figure 14: Chebyshev distance (left) and Jensen-Shannon divergence (right) across different layers
of ResNet-50 on training set.
20
Under review as a conference paper at ICLR 2020
G Single-layer distribution propagation
We present more results about the single-layer distribution propagation when training DNNs. We
calculate the Wasserstein distance between the prediction distribution of a selected epoch i and the
next epoch i + 1 and show in Figure 15. Analogous to Figure 4, Figure 16 shows that the label
distribution of one sample propagates from the first epoch to the last epoch.
'D3uett-p∙M
1.6
1.2
08
04
0-0ι
—layer 1	—layer 4	—layer 9
10	20	30
40	50	60
# epoch
70	80	90
SG 1∙6
Sg 1∙2
的08
工 O 0.4
M> cc
—Iayerl —layer 6	—layer 13
10	20	30
40	50	60	70	80	90	100
# epoch
Figure 15:	Wasserstein distance between distributions of adjacent training epoch.
σ0>)
epoch=ι
1	5	10	15	20
epoch=30
1	5	10	15	20
epoch=100
I l -I .J L
10	15	20 1	5
U
10
target
15
- 0.5
epoch=ι
epoch=ι
5	10	15	20
epoch=30
epoch=30
15 IOI5	20
epoch=70
1	5	10	15	2(
epoch=70
epoch=70
15 IOI5	20
epoch=100
15	IOl5	2(
target
15	IOI5	20
epoch=100
target
rnθ°∙5 Γ
%1
1	20	40	60	80 1	20	40	60	80 1	20	40	60	80 1	20	40	60	80 1	20	40	60	80
epoch=ι
epoch=30
epoch=70
epoch=100
target
20	40	60	80 1	20	40	60	80 1	20	40	60	80 1	20	40	60	80 1	20	40	60	80
# label
# label
# label
# label
# label
1	5
Figure 16:	Distribution propagation across different training epochs ofResNet-18 and VGG-16.
21
Under review as a conference paper at ICLR 2020
H Early-exits strategy
Deep neural networks often occur the over-thinking issue, that is, the samples are correctly classified
in the intermediate layer but misclassified in the last layer. We call these samples as confused samples.
From Table 2 and 3, the accuracy of ResNet-18 (VOC) and VGG-16 (VOC) are 64.48% and 68.96%,
respectively. In other words, out of 35.52% and 31.04% of the samples for ResNet-18 and VGG-16
are misclassified on the test set of VOC2007, respectively. For these misclassified samples, 1.17%
are actually correctly classified at 0-th layer, 0.44% are at 1-th layer and so on. Ideally, if we early
exit these confused samples, the cumulative accuracy of ResNet-18 is 70.44% on VOC dataset.
Table 2: Over-thinking results of ResNet-18.
layer	0	1	2	3	4	5	67	8	9	ideal result
VOC	1.17%	0.44%	0.69%	0.71%	0.55%	0.50%	0.53% 0.85%	0.53%	64.48%	70.44%
COCO	0.49%	0.43%	0.47%	0.28%	0.30%	0.31%	0.37% 0.78%	0.50%	27.33%	31.26%
Table 3: Over-thinking results of VGG-16.										
layer	01	2	3	45	6	7	8	9	10	11	12	13	ideal result
VOC
COCO
T34%^^0.00%^^071%^^0.67%^^0.50%^^0.24%^^0.48%^^0.22%^^0.20%^^0.14%^^0.28%^^0.34%^^162%^^68.96% 76.72%
0.35% 0.35% 0.27% 0.38% 0.33% 0.19% 0.21% 0.17% 0.18% 0.13% 0.38% 0.37% 1.63% 32.41%	37.35%
Early-exits strategy. We propose a simple probability mechanism to exit confused samples. Specifi-
cally, we add an auxiliary classifier for a selected intermediate layer to get the prediction probability
distribution. Each auxiliary classifier contains fully connected layer and Sigmoid function. For each
sample, we denote the number of exceeding the threshold p = 0.5 as N. We also denote the number
of exceeding another threshold q as n. We denote the ratio as γ and γ = n/N . The threshold q
and ratio γ are range from 0.5 to 1. We search for the best values of them to improve classification
performance and show the results on Table 4.
Table 4: Improve performance of ResNet and VGG.
Method	VOC				COCO			
	accuracy	CF1	OF1	mAP	accuracy	CF1	OF1	mAP
ResNet-18	64.48-	58.02	59.10	85.18	27.33-	55.65	60.30	64.36
ResNet-18+EarlyExits	66.01	58.67	58.76	85.49	30.03	57.49	61.38	67.28
VGG-16	-68.96-	58.58	59.71	88.48	32.41-	59.37	62.94	70.64
VGG-16+EarlyExits	69.85	59.49	59.94	88.57	33.95	60.32	63.73	71.95
ResNet-50	69.79-	60.93	60.23	88.84	33.81-	61.25	64.01	71.77
ResNet-50+EarlyExits	70.98	62.84	60.21	89.14	35.70	62.39	64.76	73.68
22
Under review as a conference paper at ICLR 2020
I More quantitative results
We demonstrate more results on easy, hard and confused samples. For each sample, we show the
image in the upper left, distribution corresponding to the selected layer in the upper right and the
distribution propagation across different layers in the bottom.
target
prediction
target
prediction
(b) ha| sample ∣
5	10	15	20
# Iabel
target ■	prediction I	(a) ]s：sample	
(6jφa<5)
UORnqES-P
# Iabel
(c) <Bnfused sampl∣
20

(9」①固)O
UOAnqES-P
target
prediction
_U1
L 5	10	15
# Iabel
Figure 17: More results of easy, hard and confused samples.
23