Under review as a conference paper at ICLR 2020
A Hierarchy of Graph Neural Networks
Based on Learnable Local Features
Anonymous authors
Paper under double-blind review
Ab stract
Graph neural networks (GNNs) are a powerful tool to learn representations on
graphs by iteratively aggregating features from node neighbourhoods. Many variant
models have been proposed, but there is limited understanding on both how to
compare different architectures and how to construct GNNs systematically. Here,
we propose a hierarchy of GNNs based on their aggregation regions. We derive
theoretical results about the discriminative power and feature representation ca-
pabilities of each class. Then, we show how this framework can be utilized to
systematically construct arbitrarily powerful GNNs. As an example, we construct
a simple architecture that exceeds the expressiveness of the Weisfeiler-Lehman
graph isomorphism test. We empirically validate our theory on both synthetic and
real-world benchmarks, and demonstrate our example’s theoretical power translates
to strong results on node classification, graph classification, and graph regression
tasks.
1	Introduction
Graphs arise naturally in the world and are key to applications in chemistry, social media, finance,
and many other areas. Understanding graphs is important and learning graph representations is a key
step. Recently, there has been an explosion of interest in utilizing graph neural networks (GNNs),
which have shown outstanding performance across tasks (e.g. KiPf & Welling (2016), VeliCkovic
et al. (2017)). Generally, we consider node-feature GNNs which operate recursively to aggregate
representations from a neighbouring region (Gilmer et al., 2017).
In this work, we propose a representational hierarchy of GNNs, and derive the discriminative power
and feature representation capabilities in each class. Importantly, while most previous work has
focused on GNNs aggregating over vertices in the immediate neighbourhood, we consider GNNs
aggregating over arbitrary subgraphs containing the node. We show that, under mild conditions, there
is only in fact a small class of subgraphs that are valid aggregation regions. These subgraphs provide
a systematic way of defining a hierarchy for GNNs.
Using this hierarchy, we can derive theoretical results which provide insight into GNNs. For example,
we show that no matter how many layers are added, networks which only aggregate over immediate
neighbors cannot learn the number of triangles in a node’s neighbourhood. We demonstrate that many
popular frameworks, including GCN1 (KiPf & Welling, 2016), GAT (VeliCkoviC et al., 2017), and
N-GCN (Abu-El-Haija et al., 2018) are unified under our framework. We also compare each class
using the Weisfeiler-Lehman (WL) isomorphism test (Weisfeiler & Lehman, 1968), and conclude our
hierarchy is able to generate arbitrarily powerful GNNs. Then we utilize it to systematically generate
GNNs exceeding the discriminating power of the 1-WL test.
Experiments utilize both synthetic datasets and standard GNN benchmarks. We show that the method
is able to learn difficult graph properties where standard GCNs fail, even with multiple layers. On
benchmark datasets, our proposed GNNs are able to achieve strong results on multiple datasets
covering node classification, graph classification, and graph regression.
1Throughout the paper we use GCN to specifically refer to the model proposed in Kipf & Welling (2016).
1
Under review as a conference paper at ICLR 2020
2	Related Work
Numerous works (see Li et al. (2015), Atwood & Towsley (2016), Defferrard et al. (2016), Kipf
& Welling (2016), NiePert et al. (2016), Santoro et al. (2017), VeliCkovic et al. (2017), Verma &
Zhang (2018), Zhang et al. (2018a), Ivanov & Burnaev (2018), Wu et al. (2019a) for examples)
have constructed different architectures to learn graPh rePresentations. Collectively, GNNs have
Pushed the state-of-the-art on many different tasks on graPhs, including node classification, and
graPh classification/regression. However, there are relatively few works that attemPt to understand or
categorize GNNs theoretically.
Scarselli et al. (2009) Presented one of the first works that investigated the caPabilities of GNNs. They
showed that the GNNs are able to aPProximate a large class of functions (those satisfying Preservation
of the unfolding equivalence) on graPhs arbitrarily well. A recent work by Xu et al. (2018) also
exPlored the theoretical ProPerties of GNNs. Its definition of GNNs is limited to those that aggregate
features in the immediate neighbourhood, and thus is a sPecial case of our general framework. We
also show that the PaPer’s conclusion that GNNs are at most as Powerful as the Weisfeiler-Lehman
test fails to hold in a simPle extension.
Survey works including Zhou et al. (2018) and Wu et al. (2019b) give an overview of the current field
of research in GNNs, and Provide structural classifications of GNNs. We differ in our motivation to
categorize GNNs from a comPutational PersPective. We also note that our classification only covers
static node feature graPhs, though extensions to more general settings are Possible.
The disadvantages of GNNs using localized filter to ProPagate information are analyzed in Li et al.
(2018). One major Problem is their incaPability of exPloring global graPh structures. To alleviate
this, there has been two imPortant works in exPanding neighbourhoods: N-GCN (Abu-El-Haija et al.,
2018) feeds higher-degree Polynomials of adjacency matrix to multiPle instantiations of GCNs, and
Morris et al. (2018) generalizes GNNs to k-GNNs by constructing a set-based k-WL to consider
higher-order neighbourhoods and caPture information beyond node-level. We comPare architectures
constructed using our hierarchy to these Previous works in the exPeriments, and show that systematic
construction of higher-order neighbourhoods brings an advantage across different tasks.
3	Background
Let G = (V,E) denote an undirected and unweighted graph, where ∣V| = N, and |E| = Ω. Unless
otherwise sPecified, we include self-looPs for every node v ∈ V . Let A be the graPh’s adjacency
matrix. Denote d(u, v ) as the distance between two nodes u and v on a graph, defined as the minimum
length of walk between u and v. We further write dv as the degree of node v, and N(v) as the set of
nodes in the direct neighborhood of v (including v itself).
Graph Neural Networks (GNNs) utilize the structure of a graph G and node features X ∈ RN ×p to
learn a refined representation of each node, where p is input feature size, i.e. for each node v ∈ V,
we have features Xv ∈ Rp .
A GNN is a function that for every layer l at every node v aggregates features over a connected
subgraph Gv ⊆ G containing the node v, and updates a hidden representation H(l) = [hɪl),…，h$].
Formally, we can define the lth layer of a GNN (with h(v0) = Xv):
a(vl) = Agg G (H(l-1))	h(vl) =Com(h(vl-1),a(vl))
where | is the restriction symbol over the domain Gv , the aggregation subgraph. The aggregation
function Agg(∙) is invariant with respect to the labeling of the nodes. The aggregation function,
Agg(∙), summarizes information from a neighbouring region Gv, while the combination function
Com(∙) joins such information with the previous hidden features to produce a new representation.
For different tasks, these GNNs are combined with an output layer to coerce the final output into
an appropriate shape. Examples include fully-connected layers (Xu et al., 2018), convolutional
layers (Zhang et al., 2018a), and simple summation (Verma & Zhang, 2018). These output layers are
2
Under review as a conference paper at ICLR 2020
task-dependent and not graph-dependent, so we would omit these in our framework, and consider the
node level output H (L) of the final Lth layer as the output of the GNN.
We consider three representative GNN variants in terms of this notation, where W (l) is a learnable
weight matrix at layer l:2
•	Graph Convolutional Networks (GCNs) (Kipf & Welling, 2016):
Agg(∙) = X hU-1)	Com(∙) = ReLu(aVl)W (I))
u∈N (v)
•	Graph Attention Networks (GAT)(Velickovic et al., 2017):
Agg(∙) = SX Softmax(MLP(hgT), hVl-1))) hg-1)	Com(∙) = ReLu(aVl)W(I))
u∈N (v)	u v	u	v
u∈N (v)
•	N-GCN (Abu-El-Haija et al., 2018) (2-layer case):
Agg(∙) = X X	hUl-1)	Com(∙)=ReLu(aVl)W(I))
u1 ∈N (v) u2 ∈N (u1 )
4	Hierarchical Framework for Constructing GNNs
Our proposed framework uses walks to specify a hierarchy of aggregation ranges. The aggregation
function over a node v ∈ G is a permutation-invariant function over a connected subgraph GV .
Consider the simplest case, using the neighbouring vertices u ∈ N (v), utilized by many popular
architectures (e.g. GCN, GAT). Then GV in this case is a star-shaped subgraph, as illustrated below
in Figure 1. We refer to that as D1 (v), which in terms of walks, is the union of all edges and nodes in
length-2 walks that start and end at v .
To build a hierarchy, we consider benefits of longer walks. The next simplest graph feature is the
triangles in the neighbourhood of v. Knowledge on connections between the neighbouring nodes of v
are necessary for considering triangles. A natural formulation using walks would be length-3 walks
that start and end at v. A length-3 returning walk outlines a triangle, and the union of all length-3
returning walks induces a subgraph, formed by all nodes and edges included in those walks. This is
illustrated in Figure 1 as L1 (v).
Definition 1. Define the set of all walks of length ≤ m returning to v as Wm(v). For k ∈ Z+, we
define Dk (v) as the subgraph formed by all the edges and nodes in W2k (v), while Lk (v) is defined
as the subgraph formed by all the nodes and edges in W2k+1 (v).
Intuitively, Lk(v) is a subgraph of G consisting of all nodes and edges in the k-hop neighbourhood of
node v, and Dk(v) only differs from Lk(v) by excluding the edges between the distance-k neighbors
of v. We explore this further in Section 5. An example illustration of the neighbourhoods defined
above is shown in Figure 1.
This set of subgraphs naturally induces a hierarchy with increasing aggregation region:
Definition 2. The D-L hierarchy of aggregation regions for a node v, AD-L (v) in a graph G is, in
increasing order:
AD-L(V) = {D1(v),L1(v),…，Dk(V),Lk(v),…}	(1)
Where Dι(v) ⊆ Lι(v) ⊆ D2(v) ⊆ L2(v)….
Next, we consider the properties of this hierarchy. One important property is completeness - that the
hierarchy can classify every possible GNN. Note that there is no meaningful complete hierarchy if GV
is arbitrary. Therefore, we propose to limit our focus to those GV that can be defined as a function of
the distance from V. Absent specific graph structures, distance is a canonical metric between vertices
and this definition includes all examples listed in Section 3. With such assumption, we can show that
the D-L hierarchy is complete:
2For simplicity we present the version without feature normalization using node degrees.
3
Under review as a conference paper at ICLR 2020
Figure 1: Illustration of D-L aggregation regions. Dashed circles represent neighborhoods of different
hops. From left to right: D1, L1, D2, and L2. Both Dk and Lk include nodes within the k-hop
neighborhood, but Dk does not include edges between nodes on the outmost ring whereas Lk does.
Theorem 1. Consider a GNN defined by its action at each layer:
a(vl) = Agg G (H(l-1))	h(vl) = Com(h(vl-1), a(vl))	(2)
Assume Gv can be defined as a univariate function of the distance from v. Then both of the following
statements are true for all k ∈ Z+:
•	IfDk(v) ⊆ Gv ⊆ Lk(v), then Gv ∈ {Dk(v), Lk(v)}.
•	If Lk (v) ⊆ Gv ⊆ Dk+1 (v), then Gv ∈ {Lk(v), Dk+1(v)}.
This theorem shows that one cannot create an aggregation region based on node distance that is "in
between" the hierarchy defined. With Theorem 1, we can use the D-L aggregation hierarchy to create
a hierarchy of GNNs based on their aggregation regions.
Definition 3. For k ∈ Z+, G(Dk) is the set of all graph neural networks with aggregation region
Gv = Dk(v) that is not a member of G(Lk). G(Lk) is the set of all graph neural networks with
aggregation region Gv = Lk (v) that is not a member of G(Dk-1).
We explicitly exclude those belonging to a lower aggregation region in order to make the hierarchy
well-defined (otherwise a GNN of order G(D1) is trivially one of order G(L1)). We also implicitly
define Do = Lo = 0.
4.1	Constructing D-L GNNs
The D-L Hierarchy can be used both to classify existing GNNs and also to construct new models.
We first note that all GNNs which aggregate over immediate neighbouring nodes fall in the class of
G(D1). For example, Graph Convolutional Networks (GCNs) defined in Section 3 is in G(D1) since
its aggregation is Agg(∙) = Pu∈di(V) hU-11, and similarly the N-GCN example is in G(D2). Note
that these classes are defined by the subgraph used by Agg, but does not imply that these networks
reach the maximum discriminatory power of their class (defined in the next section).
We can use basic building blocks to implement different levels of GNNs. These examples are not
meant to be exhaustive and only serve as a glimpse of what could be achieved with this framework.
Examples. For every k ∈ Z+:
•	AnyGNNWith Agg(∙) =	^X (Ak )vuh(^-1) is a GNN of class G (Dk).
u∈Dk (v)
•	Any GNN with Agg(∙) =	^X (Ak+1)vu h(^-1) is a GNN of class G(Lk).
u∈Dk (v)
4
Under review as a conference paper at ICLR 2020
•	Any GNNwith Agg(∙) = (A2k+1)w ∙ hVl-1) is a GNN ofclass G(Lk).
Intuitively, (Ak )vu counts all k-length walks from v to u, which includes all nodes in the k-hop
neighbourhood. The difference between the first and the second example above is that in the second
one, we allow (k + 1)-length walks from the nodes in the k-hop neighbourhood, which promotes it
to be class of G(Lk). Note the simplicity of the first and the last examples: in matrix form the first is
AkH(IT) while the last form is Diag(A2k+1) ∙ H(IT).
The building blocks can be gradually added to the original aggregation function. This is particularly
useful if an experimenter knows there are higher-level properties that are necessary to compute, for
instance to incorporate knowledge of triangles, one can design the following network (see Section 6
for more details):
w1AH(IT) + w2 Diag(A3) ∙ H(IT)	(3)
where w1 , w2 ∈ R are learnable weights.
5	Theoretical Properties
We can prove interesting theoretical properties for each class of graph neural networks on this
hierarchy. To do this, we utilize the Weisfeiler-Lehman test, a powerful classical algorithm used to
discriminate between potentially isomorphic graphs. In interest of brevity, its introduction is included
in the Appendix in Section 8.1.
We define the terminology of "discriminating graphs" formally below:
Definition 4. The discriminative power of a function f over graphs G is the set of graphs SGf such
that for every pair of graphs G1,G2 ∈ SG, the function has f(G1) = f(G2) iff Gi = G2 and
f (Gi) = f (G2) iff Gi 袈 G2. We say f decides G1,G2 as isomorphic if f (Gi) = f (G2) and vice
versa.
Essentially, SGf is the set of graphs that f can decide correctly whether any two of them are isomorphic
or not. We say f has a greater discriminative power than g if SGf ) SGg . Now we first introduce a
theorem proven by Xu et al. (2018):
Theorem 2.	The maximum discriminative power of the set of GNNs in G(Di) is strictly less than or
equal to the 1-dimensional WL test.
Their framework only included G(Di) GNNs, and they upper bounded the discriminative power of
such GNNs. With our generalized framework, we are able to prove a slightly surprising result:
Theorem 3.	The maximum discriminative power of the set of GNNs in G(Li) is strictly greater than
the 1-dimensional WL test.
This result is central to understanding GNNs. Even though the discriminative power of G(Di) is
strictly less than or equal to the 1-WL test, Theorem 3 shows that just by adding the connections
between the immediate neighbors of each node (Li\Di), we can achieve theoretically greater
discriminative power.
One particular implication is that GNNs with maximal discriminative power in G(Li) can count the
number of triangles in a graph, while those in G(Di) cannot, no matter how many layers are added.
This goes against the intuition that more layers allow GNNs to aggregate information from further
nodes, as G(Di) is unable to aggregate the information of triangles from the Li region, which is
important in many applications (see Frank & Strauss (1986), Tsourakakis et al. (2011), Becchetti
et al. (2008), Eckmann & Moses (2002)).
Unfortunately, this is the only positive result we are able to establish regarding the WL test as the
k-dim WL-test is not a local method for k > 1. Nevertheless, we are able to prove that our hierarchy
admits arbitrarily powerful GNNs through the following theorem:
5
Under review as a conference paper at ICLR 2020
GNN Class	Computational Complexity	Maximum Discriminatory Power	Possible Learned Features
GD	≤ O(Ωp)	≤1-WL	Node Degree
G(Li)	≤ O(ΩN + Np)	>1-WL All graphs of ≤ 2 nodes	All Cliques Length 3 cycles (Triangles)
GD	≤ O(Ωp)	>1-WL All graphs of ≤ 2 nodes	Length 2 walks Length 4 cycles
G(Dk)	≤ O(kΩp)	>1-WL All graphs of ≤ k nodes	Length k walks Length 2k cycles
G(Lk)	≤ O(kΩN + Np)	>1-WL All graphs of ≤ k + 1 nodes	Length 2k + 1 cycles
Table 1: Properties of different GNN classes. Shows the upper bound computational complexity
when the maximum discriminatory power is obtained. Here we assume hidden size p is the same as
feature input size. Final column contains some examples of features that can be learned by each class.
Theorem 4.	For all k ∈ Z+, there exists a GNN within the class of G(Lk) that is able to discriminate
all graphs with ≤ k + 1 nodes.
This shows that as k → ∞, we are able to discriminate all graphs. We record the full set of results
proven in Table 1. The key ingredients for proving these results are contained in Appendix 8.3 and
8.4. Here we see that at the G(L1) class, theoretically we are able to learn all cliques (as cliques by
definition are fully connected). As we gradually move upward in the hierarchy, we are able to learn
more far-reaching features such as higher length walks and cycles, while the discriminatory power
improves. We also note that the theoretical complexity increases as k increases.
6	Experiments
We consider the capability of two specific GNNs instantiations that are motivated by this frame-
work: wιAH(IT) + w? Diag(A3) ∙ H(IT)(GCN-L1)and wιAH(IT) + w? Diag(A3) ∙ H(IT) 十
w3A2H(l-1) (GCN-D2). These can be seen as extensions of the GCN introduced in Kipf & Welling
(2016). The first, GCN-L1, equips the GNN with the ability to count triangles. The second, GCN-D2,
can further count the number of 4-cycles. We note their theoretical power below (proof follows from
Theorem 3):
Corollary 1. The maximum discriminative power of GCN-L1 and GCN-D2 is strictly greater than
the 1-dimensional WL test.
We compare the performance of GCN-L1, GCN-D2 and other state-of-art GNN variants on both
synthetic and real-world tasks.3 For the combine function of GCN, GCN-L1, and GCN-D2, we use
Com(∙) = MLP(aVk)), where MLP is a multi-layer perceptron (MLP) with LeakyReLu activation
similar to Xu et al. (2018).
All of our experiments are run with PyTorch 1.2.0, PyTorch-Geometric 1.2.1, and we use NVIDIA
Tesla P100 GPUs with 16GB memory.
6.1	Synthetic Experiments
To verify our previous claim that in our proposed hierarchy, GNNs from certain classes are able to
learn specific features more effectively, we created two tasks: predict the number of triangles and
number of 4-cycles in the graphs. For each task, the dataset contains 1000 graphs and is generated in
a procedure as follows: We fix the number of nodes in each graph to be 100 and use the ErdOs-Renyi
random graph model to generate random graphs with edge probability 0.07. Then we count the
number of patterns of interest. In the 4-cycle dataset, the average number of 4-cycles in each graph is
1350 and in the triangle dataset, there are 54 triangles on average in each graph.
3The code is available at a public github repository. Reviewers have anonymized access through supplemen-
tary materials. The synthetic datasets are included in the codebase as well.
6
Under review as a conference paper at ICLR 2020
	MSE # Triangles	MSE#4Cycles (×103)
Predict Mean (Baseline)	-125.4 ± 10.7-	36.4 ± 5.6
GCN (2-layer)	-506.2 ± 80.9-	142.3 ± 19.8
GCN (3-layer)	-485.0 ± 92.4-	136.7 ± 18.5
GCN-LI (1-layer)	-61.2 ± 11.6-	45.2 ± 6.0
GCN-D2 (1-layer)	-57.9 ± 18.0-	3.0 ± 1.0
Table 2: Results of experiments on synthetic datasets (i) Count the number of triangles in the graph
(ii) Count the number of 4 cycles in the graph. The reported metric is MSE over the testing set.
Dataset	Category	# Graphs	# Classes	# Nodes Avg.	# Edges Avg	Task
Cora (Yang et al., 2016)	Citation	1	7	2,708	5,429	NC
Citeseer (Yang et al., 2016)	Citation	1	6	3,327	4,732	NC
PubMed (Yang et al., 2016)	Citation	1	3	19,717	44,338	NC
NCI1 (Shervashidze et al., 2011)	Bio	-4,110-	2	29.87	32.30	GC
Proteins (Kersting et al., 2016)	Bio	-1,113-	2	39.06	72.82	GC
PTC-MR (KerSting et al., 2016)	Bio	344	2	14.29	14.69	GC
MUTAG (BOrgWardt et al., 2005)	Bio	188	2	17.93	19.79	GC
QM7b(Wuet al.,2018)	Bio	-7,210-	14	16.42	244.95	GR
QM9 (Wuet al.,2018)	Bio	133,246	12	18.26	37.73	GR
Table 3: Details of benchmark datasets used. Types of tasks are: NC for node classification, GC for
graph classification, GR for graph regression.
We perform 10-fold cross-validation and record the average and standard deviation of evaluation
metrics across the 10 folds within the cross-validation. We used 16 hidden features, and trained the
networks using Adam optimizer with 0.001 initial learning rate, L2 regularization λ = 0.0005. We
further apply early stopping on validation loss with a delay window size of 10. The dropout rate
is 0.1. The learning rate is scheduled to reduce 50% if the validation accuracy stops increasing for
10 epochs. We utilized a two-layer MLP in our combine function for GCN, GCN-L1 and GCN-L2,
similar to the implementation in Xu et al. (2018). For training stability, we limited w1 , w2 ∈ (0, 1) in
our models using the sigmoid function.
Results In our testing, we limited GCN-L1 and GCN-D2 to a 1-layer network. This prevents GCN-
L1 and GCN-D2 from utilizing higher order features to reverse predict the triangles and 4-cycles.
Simultaneously, we ensured GCN had the same receptive field as such networks by using 2-layer
and 3-layer GCNs, which provided GCN with additional feature representational capability. The
baseline is a model that predicts the training mean on the testing set. The results are in Table 2. GCN
completely fails to learn the features (worse than a simple mean prediction). However, we see that
GCN-L1 effectively learns the triangle counts and greatly outperforms the mean baseline, while
GCN-D2 is similarly able in providing a good approximation on the count of 4-cycles, without losing
the ability to count triangles. This validates the "possible features learned" in Table 1.
6.2	Real-World Benchmarks
We next consider standard benchmark datasets for (i) node classification, (ii) graph classification, (iii)
graph regression tasks. The details of these datasets are presented in Table 3.
The setup of the learning rate scheduling and L2 regularization rate are the same as in synthetic tasks.
For the citation tasks, we used 16 hidden features, while we used 64 for the biological datasets. Since
our main modification is the expansion of aggregation region, our main comparison benchmarks are
k-GNN (Morris et al., 2018) and N-GCN (Abu-El-Haija et al., 2018), two previous best attempts in
incorporating aggregation regions beyond the immediate nodes. Note that we can view a mth order
N-GCN as aggregating over D1,D2, •…，Dm.
We further include results on GAT (Verma & Zhang, 2018), GIN (Xu et al., 2018), RetGK (Zhang et al.,
2018b), GNTK (Du et al., 2019), WL-subtree (Shervashidze et al., 2011), SHT-PATH, (Borgwardt &
Kriegel, 2005) and PATCHYSAN (Niepert et al., 2016) to show some best performing architectures.
7
Under review as a conference paper at ICLR 2020
Dataset	Cora	Citeseer	PubMed	NCI1	Proteins	PTC-MR	MUTAG	QM7b	QM9
-GAT	83.0 ± 0.7	72.5 ± 0.7	79.0 ± 0.3	74.5 ± 3.5*	73.7 ± 5.6*	60.2 ± 3.0*	84.0 ± 8.0*	91.7 ± 5.5*	115.0 ± 17.5*
GIN	77.6 ± 1.1	66.1 ± 0.9	77.0 ± 1.2	82.7 ± 1.7	76.2 ± 2.8	64.6 ± 7.0	89.4 ± 8.6		
WL-OA				86.1	75.3	63.6	84.5		
P.SAN				78.5 ± 1.8	75.8 ± 2.7	60.0 ± 4.8	92.6 ± 4.2		
RetGK				84.5 ± 0.2	75.8 ± 0.6	62.5 ± 1.6	90.3 ± 1.1		
GNTK				84.2 ± 1.2	75.6 ± 4.2	67.9 ± 6.9	90.0 ± 8.5		
S-PATH				73.0 ± 0.5	75.0 ± 0.5	58.5 ± 2.5	85.7 ± 2.5		
N-GCN	830	72.2	79.5	75.8 ± 1.9*	76.5 ± 1.5*	61.0 ± 5.0*	85.0 ± 6.9*	82.0 ± 5.4*	120.7 ± 8.5*
k-GNN	81.6 ± 0.4*	71.5 ± 0.5*	79.8 ± 0.3*	76.2	75.5	60.9	86.1	75.1 ± 8.5*	104.2 ± 10.4
GCN	80.6 ± 1.4	70.3 ± 1.2	79.0 ± 0.4	73.2 ± 1.4	73.9 ± 2.8	59.0 ± 2.0	82.2 ± 5.1	104.3 ± 15.6	160.2 ± 15.4
GCN-L1	82.5 ± 0.3	72.0 ± 0.3	80.2 ± 0.2	79.5 ± 1.6	77.6 ± 3.8	64.1 ± 2.5	86.8 ± 8.3	52.4 ± 4.3	78.5 ± 8.6
GCN-D2	82.9 ± 1.0	72.3 ± 0.3	80.2 ± 0.3	77.0 ± 2.0	77.0 ± 3.0	64.6 ± 4.1	87.8 ± 5.6	49.0 ± 2.9	72.5 ± 13.0
Table 4: Results of experiments on real-world datasets. The reported metrics are accuracy on
classification tasks and MSE on regression tasks. Figures for comparative methods are from literature
except for those with *, which come from our own implementation. The best-performing architectures
are highlighted in bold.
Baseline neural network models use a 1-layer perceptron combine function, with the exception of
k-GNN, which uses a 2-layer perceptron combine function. Thus, to illustrate the effectiveness of the
framework, we only utilize a 1-layer perceptron combine function for all tasks for our GCN models,
with the exception of NCI1. 2-layer perceptrons seemed necessary for good performance in NCI1,
and thus we implemented all neural networks with 2-layer perceptrons for this task to ensure a fair
comparison. We tuned the learning rates ∈ {0.001, 0.005, 0.01, 0.05} and dropout rates ∈ {0.1, 0.5}.
For numerical stability, we normalize the aggregation function using the degree of v only. For the
node classification tasks, we directly utilized the final layer output, while we summed over the node
representations for the graph-level tasks.
Results Experimental results on real-world data are summarized in Table 4. According to our
experiments, GCN-L1 and GCN-D2 noticeably improve upon GCN across all datasets, due to
its ability to combine node features in more complex and nonlinear ways. The improvement is
statistically significant on the 5% level for all datasets except Proteins. The results of GCN-L1 and
GCN-D2 match the best performing architectures in most datasets, and lead in numerical averages
for Cora, Proteins, QM7b, and QM9 (though not statistically significant for all).
Importantly, the results also show a significant improvement from the two main comparison ar-
chitectures, k-GNN and N-GCN. We see that further expanding aggregation regions generates
diminishing returns on these datasets, and the majority of the benefit is gained in the first-order
extension G(L1). This is in contrast to N-GCN which skipped L1 to only used D-type aggregation
regions (D2, D3,…),which is an incomplete hierarchy of aggregation regions. The differential in
results illustrates the power of the complete hierarchy as proven in Theorem 1.
We especially would like to stress the outsized improvement of GCN-L1 on the biological datasets.
As described in 1, GCN-L1 is able to capture information about triangles, which are highly relevant
for the properties of biological molecules. The experimental results verify such intuition, and show
how knowledge about the task can lead to targeted GNN design using our framework.
7	Conclusion
We propose a theoretical framework to classify GNNs by their aggregation region and discriminative
power, proving that the presented framework defines a complete hierarchy for GNNs. We also
provide methods to construct powerful GNN models of any class with various building blocks. Our
experimental results show that example models constructed in the proposed way can effectively learn
the corresponding features exceeding the capability of 1-WL algorithm in graphs. Aligning with our
theoretical analysis, experimental results show that these stronger GNNs can better represent the
complex properties of a number of real-world graphs.
8
Under review as a conference paper at ICLR 2020
References
Sami Abu-El-Haija, Amol Kapoor, Bryan Perozzi, and Joonseok Lee. N-gcn: Multi-scale graph
convolution for semi-supervised node classification. arXiv preprint arXiv:1802.08888, 2018.
James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in Neural
Information Processing Systems, pp.1993-2001, 2016.
Luca Becchetti, Paolo Boldi, Carlos Castillo, and Aristides Gionis. Efficient semi-streaming algo-
rithms for local triangle counting in massive graphs. In Proceedings of the 14th ACM SIGKDD
international conference on Knowledge discovery and data mining, pp. 16-24. ACM, 2008.
Karsten M Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs. In Fifth IEEE
international conference on data mining (ICDM’05), pp. 8-pp. IEEE, 2005.
Karsten M Borgwardt, Cheng Soon Ong, Stefan Schonauer, SVN VishWanathan, Alex J Smola, and
Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(suppl_1):
i47-i56, 2005.
Jin-Yi Cai, Martin Furer, and Neil Immerman. An optimal lower bound on the number of variables
for graph identification. Combinatorica, 12(4):389-410, 1992.
Michael Defferrard, Xavier Bresson, and Pierre Vndergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Advances in neural information processing systems,
pp. 3844-3852, 2016.
Simon S Du, Kangcheng Hou, BarnabaS P6czos, Ruslan Salakhutdinov, Ruosong Wang, and Keyulu
Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. arXiv preprint
arXiv:1905.13192, 2019.
Jean-Pierre Eckmann and Elisha Moses. Curvature of co-links uncovers hidden thematic layers in the
world wide web. Proceedings of the national academy of sciences, 99(9):5825-5829, 2002.
Ove Frank and David Strauss. Markov graphs. Journal of the american Statistical association, 81
(395):832-842, 1986.
Martin Furer. On the combinatorial power of the weisfeiler-lehman algorithm. In International
Conference on Algorithms and Complexity, pp. 260-271. Springer, 2017.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 1263-1272. JMLR. org, 2017.
Frank Harary. A survey of the reconstruction conjecture. In Graphs and combinatorics, pp. 18-28.
Springer, 1974.
Sergey Ivanov and Evgeny Burnaev. Anonymous walk embeddings. arXiv preprint arXiv:1805.11921,
2018.
Kristian Kersting, Nils M. Kriege, Christopher Morris, Petra Mutzel, and Marion Neumann. Bench-
mark data sets for graph kernels, 2016. URL http://graphkernels.cs.tu-dortmund.
de.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
arXiv preprint arXiv:1609.02907, 2016.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. arXiv:1801.07606, 2018.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural
networks. arXiv preprint arXiv:1511.05493, 2015.
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.
arXiv preprint arXiv:1810.02244, 2018.
9
Under review as a conference paper at ICLR 2020
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks
for graphs. In International conference on machine learning, pp. 2014-2023, 2016.
Md Abdur Razzaque, Choong Seon Hong, Mohammad Abdullah-Al-Wadud, and Oksam Chae. A
fast algorithm to calculate powers of a boolean matrix for diameter computation of random graphs.
In International Workshop on Algorithms and Computation, pp. 58-69. Springer, 2008.
Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter
Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. In
Advances in neural information processing systems, pp. 4967-4976, 2017.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
Computational capabilities of graph neural networks. IEEE Transactions on Neural Networks, 20
(1):81-102, 2009.
Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M
Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(Sep):
2539-2561, 2011.
Charalampos E Tsourakakis, Petros Drineas, Eirinaios Michelakis, Ioannis Koutis, and Christos
Faloutsos. Spectral counting of triangles via element-wise sparsification and triangle-based link
recommendation. Social Network Analysis and Mining, 1(2):75-81, 2011.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
SaUrabh Verma and Zhi-Li Zhang. Graph capsUle convolUtional neUral networks. arXiv preprint
arXiv:1805.08090, 2018.
Edward Wagstaff, Fabian B FUchs, Martin Engelcke, Ingmar Posner, and Michael Osborne. On the
limitations of representing fUnctions on sets. arXiv preprint arXiv:1901.09006, 2019.
Boris Weisfeiler and Andrei A Lehman. A redUction of a graph to a canonical form and an algebra
arising dUring this redUction. Nauchno-Technicheskaya Informatsia, 2(9):12-16, 1968.
Felix WU, Tianyi Zhang, AmaUri Holanda de SoUza Jr, Christopher Fifty, Tao YU, and Kilian Q
Weinberger. Simplifying graph convolUtional networks. arXiv preprint arXiv:1902.07153, 2019a.
Zhenqin WU, Bharath RamsUndar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S
PappU, Karl Leswing, and Vijay Pande. MolecUlenet: a benchmark for molecUlar machine learning.
Chemical science, 9(2):513-530, 2018.
Zonghan WU, ShirUi Pan, Fengwen Chen, GUodong Long, Chengqi Zhang, and Philip S YU. A
comprehensive sUrvey on graph neUral networks. arXiv preprint arXiv:1901.00596, 2019b.
KeyUlU XU, WeihUa HU, JUre Leskovec, and Stefanie Jegelka. How powerfUl are graph neUral
networks? arXiv preprint arXiv:1810.00826, 2018.
Zhilin Yang, William W Cohen, and RUslan SalakhUtdinov. Revisiting semi-sUpervised learning with
graph embeddings. arXiv preprint arXiv:1603.08861, 2016.
MUhan Zhang, Zhicheng CUi, Marion NeUmann, and Yixin Chen. An end-to-end deep learning
architectUre for graph classification. In Thirty-Second AAAI Conference on Artificial Intelligence,
2018a.
Zhen Zhang, Mianzhi Wang, Yijian Xiang, Yan HUang, and Arye Nehorai. Retgk: Graph kernels
based on retUrn probabilities of random walks. In Advances in Neural Information Processing
Systems, pp. 3964-3974, 2018b.
Jie ZhoU, GanqU CUi, Zhengyan Zhang, Cheng Yang, ZhiyUan LiU, and Maosong SUn. Graph neUral
networks: A review of methods and applications. arXiv preprint arXiv:1812.08434, 2018.
10
Under review as a conference paper at ICLR 2020
8	Appendixes
8.1	Introduction to Weisfeiler-Lehman Test
The 1-dimensional Weisfeiler-Lehman (WL) test is an iterative vertex classification method widely
used in checking graph isomorphism. In the first iteration, the vertices are labeled by their valences.
Then at each following step, the labels of vertices are updated by the multiset of the labels of
themselves and their neighbors. The algorithm terminates when a stable set of labels is reached. The
details of 1-dimensional Weisfeiler-Lehman (WL) is shown in Algorithm 1. Regarding the limitation
of 1-dimensional Weisfeiler-Lehman (WL), Cai et al. (1992) described families of non-isomorphic
graphs which 1-dimensional WL test cannot distinguish.
Algorithm 1 1-dimensional Weisfeiler-Lehman (WL)
1	Input: G = (V, E), G0 = (V0, E0), initial labels lo(v) for all V ∈ V ∪ V0
2	: Output: stablized labels l(v) for all v ∈ V
3	: while li(v) has not converged do	. Until the labels reach stabalization
4	:	for v ∈ V ∪ V0 do
5	:	Mi(v) = multi-set {li-1(u)|u ∈ N(v)}
6	:	end for
7	:	Sort Mi(v) and concatenate them into string si(v)
8	li(v) = f (Si(v)), where f is any function s.t. f (Si(V) = f (Si(W)) ^⇒ Si(V) = Si(W)
9	: end while
The k-dimensional Weisfeiler-Lehman (WL) algorithm extends the above procedure from operations
on nodes to operations on tuples V (G)k .
11
Under review as a conference paper at ICLR 2020
8.2	Proof of Theorem 1
Theorem. Consider a GNN defined by its action at each layer:
a(vl) = Agg Gv(H(l-1))	hlv = Com(h(vl-1), a(vl))	(4)
Assume Gv can be defined as a univariate function of the distance from v. Then both of the following
statements are true for all k ∈ Z+:
•	If Dk (v) ⊆ Gv ⊆ Lk(v), then Gv ∈ {Dk(v), Lk(v)}.
•	IfLk(v) ⊆ Gv ⊆ Dk+1 (v), then Gv ∈ {Dk+1 (v), Lk(v)}.
Proof. We would prove by contradiction. Assume, in contrary, that one of the statements in Theorem
1 is false. Let k ∈ Z+ . Then we would separate these two cases as below:
Dk(v) ( Gv ( Lk (v) Assume that Gv satisfies such relationship. Since Lk (v) and Dk (v)
only differ by the set Ck (v) = {eij ∈ E | d(i, v) = d(j, v) = k}, Gv can only contain this set
partially. Let Mk (v) ( Ck (v) be the non-empty maximal subset of Ck (v) that is contained in
Gv. Since Mk(v) 6= Ck(v), there exists m, n with d(v, m) = d(v, n) = k such that emn ∈ Ck(v)
but emn 6∈ Mk (v). Consider a non-identity permutation of vertices fixing v. Then since Gv is
defined only using the distance function, and it needs to be permutation invariant, all eij with
d(i, v) = d(j, v) = k must be in Ck (v) and not in Mk(v). But then Mk(v) is empty, a contradiction.
Lk (v) ( Gv ( Dk+1 (v) Assume that Gv satisfies such relationship. Consider the set difference
between Dk+1(v) and Lk(v), denoted as a subgraph Ck(v):
Ck(V) = (VC (V),EC (V)) = Hu | d(U, V)=储NeijIm(V,i),d(Vj)) ∈ {(k,k+I), (k+1,k)}})
(5)
Then Gv can only contain this set partially. Let Mk(V) ( Ck(V) be the maximal subset of Ck(V) that
is contained in Gv. Since Mk(V) 6= Ck(V), at least one of the followings must be true:
•	There exists u with d(u, V) = k such that u ∈ Ck (V) but u 6∈ Mk (V).
•	There exists m, n with d(V, m) = k, d(V, n) = k + 1 such that emn ∈ Ck (V) but emn 6∈
Mk(V)
For the first case, consider a non-identity permutation of the vertices fixing V. Then since Gv is
permutation invariant and defined only using the distance, then thus all vertices w with d(w, V) = k
are in Ck (V) but not in Mk (V). This implies VkM (V) is empty.
Using the same logic for the second case, one can conclude that all eij with d(V, i) = k and
d(V, m) = k + 1 must be in Ck(V) but not in Mk(V). That means EkM (V) is empty.
Therefore, we can conclude that at least one of VkM (V) and EkM (V) is empty. Since both cannot
be empty (as that means Gv = Lk(v)), We must have either VkM(V) = 0 or EM(V) = 0. With the
former case Mk(V) is not a valid subgraph (as some edges to nodes with distance k + 1 from V are in
the set, but the nodes are not), and With the latter case it is not connected (as the nodes With distance
k + 1 from V are in the set but none of the edges are), so neither of them are valid aggregation regions
in our frameWork (our definition of GNN requires the region to be a connected graph). Thus, We
reach a contradiction.	□
12
Under review as a conference paper at ICLR 2020
8.3	Proof of Theorem 3
Theorem. The maximum discriminative power of the set of GNNs in G(L1) is strictly greater than
the 1-dimensional WL test.
Proof. We first note that a GNN with the aggregation function (in matrix notation):
Agg(∙) = Diag(A3) ∙ H(k)	(6)
is a G(L1)-class GNN. Then note that (A3)ii is the number of length 3 walks that start and end at i.
These walks must be simple 3-cycles, and thus (A3)ii is twice the number of triangles that contains i
(since for a triangle {i, j, k}, there would be two walks i → j → k → i and i → k → j → i). Furer
(2017) showed that 1-WL test cannot measure the number of triangles in a graph (while 2-WL test
can), so there exist graphs G1 and G2 such that 1-WL test cannot differentiate but the GNN above
can due to different number of triangles in these two graphs (an example are two regular graphs with
the same degree and same number of nodes).
Now Xu et al. (2018) proved that G(D1) has a maximum discriminatory power of 1-WL, and since
L1 ) D1, the maximum discriminatory power of G(L1) is at least as great than that of G(D1), which
is 1-WL.
Thus combining these two results give the required theorem.	□
We here note the computational complexity of A3 using naive matrix multiplication requires O(N3)
multiplications. However, by exploiting the sparsity and binary nature of A, there exist algorithms
that can calculate Ak with O(ΩN) additions (Razzaque et al. (2008)), and we thus derive a more
favorable bound.
8.4	Proof of Theorem 4
Theorem. For all k ∈ Z+, there exists a GNN within the class of G(Lk) that is able to discriminate
all graphs with k + 1 nodes.
Proof. We would prove by induction on k . The base case for k = 1 is simple.
Assume the statement is true for all k ≤ k0 - 1. Let us prove the case for k = k0 ≥ 2.
We would separate the proof into three cases:
•	G1 and G2 are both disconnected.
•	One of G1, G2 is disconnected.
•	G1 and G2 are both connected.
If we can create appropriate GNNs to discriminate graphs in each of the three cases (say f1, f2, f3),
then the concatenated function f := [f1, f2, f3] can discriminate all graphs. Therefore, we would
prove the induction step separately for the three cases below.
8.4.1	G1 AND G2 DISCONNECTED
We would use the Graph Reconstruction Conjecture as proved in the disconnected case (Harary,
1974):
Lemma 1. For all k ≥ 2, two disconnected graphs G1, G2 with |V1 | = |V2| = k are isomorphic if
and only if the set of k - 1 sized subgraphs for these two graphs are isomorphic.
13
Under review as a conference paper at ICLR 2020
Let G1 and G2 be any two disconnected graphs with k0 + 1 nodes. By the induction assumption,
there exist a GNN in G(Lk0 ) such that it discriminates all graphs with k0 nodes. Denote that
fk0 : G → Rk0 ×p.
Then by Lemma 1 above, we know that G1 and G2 are isomorphic iff:
{fk0(G1),…fko(Gk0+1)} = {fk0(G2),…fk0(Gk0+1)}	⑺
Where G1,…Gf+1 are the k0 + 1 subgraphs of G1 with size k0, and similarly for G2. Then we
define:
k0+1
fk，+i(G)= X (fko(Gi))i	i = 1,…k0 + 1
i=1
fk0+ι(G) = [f1o+ι(G),…，fk⅛1(G)]
Then, by Theorem 4.3 in Wagstaff et al. (2019), fk0+1(G) is an injective, permutation-invariant
function on {fk，(G1),…fk，(Gk0+1)}. Therefore, fk0+1(G) is a GNN with an aggregation region
of Lk0+1 that can discriminate G1 and G2. Thus, the induction step is proven.
8.4.2	G1 AND G2 CONNECTED
In the case where both G1 and G2 are connected, let v1 , u1 be two vertices from each of the two
graphs. Note that since G1 and G2 have k0 + 1 nodes, by definition of Lk，, we have Lk， (v1) = G1
and Lk， (u1) = G2. Since v1, u1 are arbitrary, every node in the two graphs has an aggregation region
of its entire graph. Then we define the Aggregation function as followed:
Agg(v) = LO(ALk，(v))
Where ALk， is the adjacency matrix restricted to the Lk， (v) subgraph, and LO(A) returns the
lexicographical smallest ordering of the adjacency matrix as a row-major vector among all isomorphic
permutations (where nodes are relabeled).4 For example, take:
A= 11 01
There are two isomorphic permutations of this adjacency matrix, which are:
11	10	01 11
The row-major vectors of these two adjacency matrices are [1, 1, 1, 0] and [0, 1, 1, 1], in which
[0, 1, 1, 1] is lexicographical smaller, so LO(A) = [0, 1, 1, 1]. Note that this function is permutation
invariant.
Then for G1 and G2 connected, AL ，(v) is always the adjacency matrix of the full graph. Therefore
if G1 and G2 are connected and isomorphic, then their adjacency matrices are permutations of each
other, and thus their lexicographical smallest ordering of the row-major vector form of the adjacency
matrix are identical. The converse is also clearly true as the adjacency matrix determines the graph.
Therefore, this function discriminates all connected graphs of k0 + 1 nodes, and the induction step is
proven.
8.4.3	G1 DISCONNECTED AND G2 CONNECTED
In the case where G1 is disconnected and G2 is connected, we define the aggregation function as the
number of vertices in Lk， (v), denoted |VL ，(v) |:
Agg(v) = |VLk， (v)|
4Strictly speaking, for consistency of the output length, we would require padding to a length of (k0 + 1)2 if
the adjacency matrix restricted to the Lk， (v) subgraph is not the full adjacency matrix. However, since we only
care about the behavior of the function when G1 and G2 are both connected, this scenario never happens, so it is
not material for any part of the proof below.
14
Under review as a conference paper at ICLR 2020
This is a permutation-invariant function. Note that for the connected graph G2 and any vertex v in
G2, this function returns Agg(v) = k0 + 1 as Lk0 (v) = G2. Therefore, every node has the same
embedding k0 + 1.
On the other hand, for the disconnected graph Gι, let [Ui,…，Um∖ be the connected components of
G1. Then for a vertex u ∈ Ui, it is clear that Lk0 (u) = Ui, and thus Agg(u) = |VUi | for all u ∈ Ui.
And since |VUi | < k0 + 1 by construction, Agg(u) < k0 + 1 for all u ∈ Ui, so the embedding of G1
and G2 are never equal when G1 is connected and G2 is disconnected.
Therefore, this function discriminate all graphs of k0 + 1 nodes in which one is connected and one is
disconnected, so the induction step is proven.
□
15