Under review as a conference paper at ICLR 2020
Knowledge Graph Embedding: A Probabilistic
Perspective and Generalization Bounds
Anonymous authors
Paper under double-blind review
Ab stract
We study theoretical properties of embedding methods for knowledge graph com-
pletion under the “missing completely at random” assumption. We prove gen-
eralization error bounds for this setting. Even though the missing completely at
random setting may seem naive, it is actually how knowledge graph embedding
methods are typically benchmarked in the literature. Our results provide, to a cer-
tain extent, an explanation for why knowledge graph embedding methods work
(as much as classical learning theory results provide explanations for classical
learning from i.i.d. data).
1	Introduction
Many large knowledge bases such as YAGO (Mahdisoltani et al., 2015) and NELL (Carl-
son et al., 2010) are represented as knowledge graphs. A knowledge graph is a set of
triples (head, predicate, tail) where head and tail are objects (also frequently called entities)
and predicate is a binary relation. Each such triple represents a fact about the world, e.g.
(NewYork, IsLocatedIn, US). The objects can be seen as vertices of a graph and the relations as
labeled directed edges (pointing from head to tail), hence the name knowledge graph. Most knowl-
edge bases are incomplete, i.e. there are facts that should be included in them but are not. A popular
task is to automatically complete such knowledge bases. This is often done by embedding the objects
and relations contained in them into some vector spaces and then using the embedding to predict the
missing tuples.
There has been a number of knowledge graph completion methods presented in the literature (we
give an overview of existing methods in Section 2.3). However, to our best knowledge, there has not
been any theoretical analysis of the generalization abilities of these methods in the spirit of classical
learning theory. We make the first steps in this direction in the present paper. We derive expected
error bounds for a class of knowledge graph embedding methods. A key challenge in deriving gen-
eralization bounds in this setting is that the training and test data do not satisfy assumptions typically
used in learning theory. The knowledge bases that we receive contain only positive examples, that
is, the facts already in the knowledge base. We do not know which of the facts not contained in the
knowledge base are just missing and which are not contained in the knowledge base because they
are actually false. This is known as learning from positive and unlabeled data (PU-learning); we
refer to (Bekker & Davis, 2018) for a recent survey. Only a few results have been obtained for gen-
eralization bounds for PU-learning so far, e.g. (Niu et al., 2016), but they are not directly applicable
to our setting (we discuss the details in Section 10).
The bounds that we derive in this paper rely on an assumption on how the triples are missing from
the knowledge graph. In particular, we assume that the triples are missing “completely at random”,
i.e. that each tuple may be missing with probability δ > 0 independently of the others. Even
though this setting may appear naive, it is actually exactly how some famous datasets’ training and
test sets are created, e.g. (Bordes et al., 2013). Thus, the theoretical results actually do relate to
what one observes in “practice” (i.e. in the respective papers that use these datasets). The bounds
that we derive here apply to knowledge graph embedding methods that use log-loss for learning
the embeddings such as ComplEx (Trouillon & Nickel, 2017), SimplE (Kazemi & Poole, 2018) or
ConvE (Dettmers et al., 2018).
1
Under review as a conference paper at ICLR 2020
Main Technical Contributions Our main technical results are expected error bounds on the num-
ber of triples (h, r, t) that are predicted incorrectly when using knowledge graph embedding for
knowledge graph completion (stated in Theorems 4 and 5), i.e. either to be in the knowledge graph
when they are not supposed to be (false positives) or not to be in the knowledge graph when they
are supposed to be (false negatives). The bound assumes that the knowledge graph embedding is
learned by maximizing the log-likelihood of a certain exponential-family distribution (introduced in
Section 5), which turns out to be equivalent to how several prominent knowledge graph embedding
methods (Trouillon & Nickel, 2017; Kazemi & Poole, 2018; Dettmers et al., 2018) are learned using
log-loss (this is discussed in detail in Section 10). Our results therefore provide a statistical learning
theory style justification for these knowledge graph embedding methods. In order to obtain the main
results we also derive several auxiliary technical results which might be of independent interest (in
particular, Theorems 2 and 3).
2	Preliminaries
2.1	Knowledge Graphs
Let O be a set of objects and R a set of relations. A knowledge graph is a set of triples (h, r, t)
where h and t are objects and r is a relation. Here h is called the head of the triple and t is called the
tail of the triple. In the formalism of first-order logic, a triple (h, r, t) represents a first-order logic
atom r(h, t). For instance the triple (Alice, Knows, Bob) corresponds to the first-order logic atom
Knows(Alice, Bob), which represents the fact that Alice knows Bob. Here, Alice and Bob are objects
and Knows is a relation.
Example 1. Let us have the following objects and relations:	O =
{France, Germany, Berlin, Paris}, R = {CapitalOf, Borders}. Then the knowledge graph, repre-
senting our geographical knowledge about these countries and cities, can look as G = {(France,
Borders, Germany), (Germany, Borders, France), (Berlin, CapitalOf, Germany), (Paris, CapitalOf,
France)}. Whereas, in this case, the knowledge graph is complete and accurate, in reality most
knowledge graphs will lack some of the true tuples. The task of knowledge base completion is to
add such missing facts.
2.2	Knowledge Graph Embedding
Knowledge graph embedding methods represent knowledge graphs geometrically. They do so by
representing objects and relations as vectors (or matrices). Given a knowledge graph G on some sets
of objects O and relations R, most knowledge graph embedding methods need three components:
vector representations of objects, vector representations of relations and a function ψ(xh, xr, xt)
which takes the representations of h, r and t and produces a score that is used to assess how likely it
is that the relation (h, r, t) is contained in the knowledge graph. One can then compare the output of
the function with a threshold to decide which triples are (predicted to be) in the knowledge graph.
Note on notation. For a set of objects O and a set of relations R we will normally denote their
vector representations by X. We will usually treat X as a look-up table. For an object o we will
typically use xo to denote its vector representation from X and analogically for a relation r we will
use xr , unless explicitly stated otherwise. With a slight abuse of notation, we will also sometimes
treat X as a set of vectors and, for instance, write x ∈ X. Relations can be mapped to matrices
(denoted Mr) or tensors instead of only vectors, and even if relations are mapped to vectors the
dimension may be different from the dimension of entity embeddings. However, for the sake of
simplicity, we will not introduce more notations, and X and ψ will also be used in these cases.
2.3	Existing Methods
One can roughly categorize knowledge graph (KG) embedding approaches into several groups based
on different scoring functions such as distance-based scoring functions and similarity-based scoring
functions.
2
Under review as a conference paper at ICLR 2020
The most well-known class of embedding methods using distance-based scoring functions is TransE
(Bordes et al., 2013) and its variants. The scoring function of TransE is defined as the negative
distance between xh + xr and xt, i.e., ψ(xh, xr, xt) = -||xh + xr - xt||2.
In order to effectively model more general relations rather than only one-to-one relations, TransH
has been proposed (Wang et al., 2014). Given a triple (h, r, t), the scoring function is defined as
-∣∣x⊥ + Xr - x⊥∣∣2 where x⊥ and x⊥ are the projections of entity embeddings Xh and Xt onto
some hyperplane, respectively. TransR (Lin et al., 2015), TransD (Ji et al., 2015) and TranSparse (Ji
et al., 2016) are similar to TransH, allowing relation-specific embeddings in different ways. TransM
(Fan et al., 2014; Xiao et al., 2015), ManifoldE (Xiao et al., 2016) and TransF (Feng et al., 2016)
relax the requirement Xh + Xr ≈ Xt.
SE (Bordes et al., 2011) uses two matrices to project head and tail entities for each relation r, and
the score is -||Mr1Xh - Mr2Xt||1.
RotatE (Sun et al., 2019) defines each relation as a rotation from the source entity to the target entity
in the complex vector space, and its scoring function is -||Xh ◦ Xr - Xt||2 where ◦ is the entry-wise
product.
In the class of similarity-based methods, RESCAL (Nickel et al., 2011) and its variants are among
the most well-known ones. RESCAL also assigns every entity a vector but every relation r a matrix
Mr1. The scoring function is defined as Xh>MrXt. Jenatton et al. (2012) further assumes that every
Mr has the same set of eigenvectors. GarcIa-DUran et al. (2014) adds other terms into the scoring
function Xh>MrXt+Xh>Xr +Xr>Xt +Xt>DXh where D is a diagonal matrix independent of relations.
DistMUlt (Yang et al., 2015) is a simplified version of RESCAL by restricting Mr being diagonal,
and hence improves the efficiency of RESCAL. To combine the advantages of both methods, i.e., the
expressive power of RESCAL and the simplicity of DistMUlt, HolE (Nickel et al., 2016) has been
proposed. ComplEx (Yang et al., 2015) extends DistMUlt by allowing complex-valUed embeddings,
which is more powerfUl in modeling asymmetric relations. Hayashi & Shimbo (2017) show that
ComplEx is in fact eqUivalent to HolE. To captUre analogical strUctUres, ANALOGY (LiU et al.,
2017) fUrther reqUires Mr in RESCAL to be normal and mUtUally commUtative.
A class of KG embeddings is based on tensor factorization approaches sUch as the Canonical
Polyadic (CP) decomposition (Hitchcock, 1927) whose scoring fUnction is similar to DistMUlt, bUt
every entity has two independent embeddings. SimplE (Kazemi & Poole, 2018) is a recent extension
of CP decomposition, which ensUres the two embeddings of every entity dependent.
There are also a groUp of algorithms Using neUral networks for embedding, e.g., SME (Bordes et al.,
2014), NTN and SLM (Bordes et al., 2014), MLP (Dong et al., 2014), NAM (LiU et al., 2016) and
ConvE (Dettmers et al., 2018).
All the aforementioned embedding methods map entities and relations in a deterministic way. In
KG2E (He et al., 2015), representations of entities and relations are random vectors sampled from
mUltivariate normal distribUtions,
Xh 〜N(μh ∑h) Xr 〜N(μr, ∑r) Xt 〜N(μt, ∑t).
A possible scoring fUnction is KL(Xt - Xh||Xr) and another is the probability inner prodUct (Jebara
et al., 2004) of Xt - Xh and Xr. TransG (Han et al., 2016) fUrther models relations Using mixtUres
of GaUssians.
3	Learning Setting
We work within the following learning setting. We assUme that there exists some “groUnd trUth”
knowledge graph G* and that We observe only a subset of the triples contained in it. We will denote
the observed knowledge graph consisting of a subset of G*,s triples by G. Furthermore, we assume
that G is obtained from G* by the following sampling process, called data-generating distribution.
Definition 1 (Data-generating distribution). Let G* be a ground-truth knowledge graph and δ > 0
be a real number. The data-generating distribution is given by the following process. Iterate over
1One can also treat the embedding of every relation as a vector xr , and the corresponding scoring function
becomes Xr ∙ Vec(Xhx>) where Vec is the vectorization operator.
3
Under review as a conference paper at ICLR 2020
all triples T ∈ G * and add each of them to G With probability 1 一 δ independently of the others. We
call δ the missingness parameter.
The task that we want to solve is then to reconstruct the ground-truth knowledge graph G * as well
as possible from a single sample from the data-generating distribution.
Remark 1. Some knowledge graph embedding methods use different representations for the same
object depending on whether it is used as a head or tail of a particular triple. It is easy to see
that our general setting also allows such representations;it is enough to double the dimension of the
vectors and assume that the function ψ(xh, xr, xt) depends only on the first half of dimensions ofxh
and the second half of the dimensions of xt. It is easy to see that we can also handle the case when
the vector representations of relations have different dimensions than those of objects similarly.
4	Intuition and Sketch of the Argument
Our main results are bounds for the expected number of incorrectly predicted triples for knowledge
graph completion. In particular, we show that, assuming certain technical assumption on the knowl-
edge graph embedding method and the way it is trained, there exists a constant C such that the ex-
pected error rate in the realizable setting is bounded by C (P∣G*∣∕(R∣∣O∣2) ∙ Pln ∣O∣∕∣O∣ + 1 / |O|),
where G* is the ground truth knowledge graph and O is the set of objects andR is the set of re-
lations. The first term in the product measures sparsity of the graph and the second term has the
familiar form of classical error bounds for learning from i.i.d. data. We also provide similar bounds
for the agnostic setting.
The strategy taken in this paper is based on viewing the problem as learning a distribution on knowl-
edge graphs. Suppose that we have not only one sample G (one knowledge graph) from the data-
generating distribution, but actually many such samples G1, G2, . . . , Gn (a collection of knowledge
graphs instead of just one). We could try to use the knowledge graphs G1, G2, . . . , Gn to estimate a
distribution over knowledge graphs. Then we would predict the triples with frequency higher than
some threshold t as true. In reality, we do not have a collection of knowledge graphs but only one.
However, to estimate the data-generating distribution, we can exploit the fact that we know its form
and the independencies it satisfies. In the next section, we define a distribution to approximate the
data-generating distribution, parameterized by the vector representations X of objects and relations.
For the approach sketched above to work, we need (i) the distribution to be expressive enough and (ii)
we need to show that the learned distribution will be close enough to the data-generating distribution
if we estimate it from a sufficiently large knowledge graph. The question of expressiveness has been
tackled in the literature (e.g. Kazemi & Poole (2018)) and we do not deal with it much in this paper.
We deal with the second question. Using a concentration inequality for log-likelihood (Theorem 1),
we prove a generalization bound for log-likelihood (Theorem 2) from which a bound on Kullback-
Leibler divergence follows. What then remains to be done is to show how to bound the number of
incorrectly predicted triples as a function of the Kullback-Leibler divergence, which we do with the
help of Pinsker’s inequality (Theorem 3). Combining these two latter results together then yields a
bound on the expected number of triples that are predicted incorrectly (both false positives and false
negatives) by the learning method sketched here (Theorem 4).
5	A Probabilistic Perspective
Following the intuition described in the previous section, we cast the knowledge graph embedding
problem as the problem of estimating the distribution induced by the data-generating process. We
seek an approximation of this distribution as an exponential-family model of the form given below
in Definition 2. The parameters of this exponential family model are the vector embeddings of the
objects and relations.
Definition 2 (KG-distribution). Let O be a set of objects, R be a set Ofrelations and Ω = 2o×r×o
be the set ofallpossible knowledge graphs on these objects and relations. Let G ∈ Ω be a knowledge
graph and X be the vector representation of the objects and relations. Then we define
PX(G) = zZ Y	eχp(ψ(χh, xr, χt)),
(h,r,t)∈G
4
Under review as a conference paper at ICLR 2020
where xh, xr and xt are the vector representations of h, r and t given by X, ψ : Rd ×
Rd × Rd is a function, d is the dimension of the vector representations, and finally Z =
∑G0∈ω ∏(h r t)∈G0 exp(ψ(xh, Xr, Xt)) is a normalization constant, ensuring that PX(G) is aprob-
ability distribution.
Unless We allow ∣ψ∣ to be infinite, KG-distributions cannot model the data-generating distribution
exactly. However, as we will show it is enough if the KG-distributions are close enough in Kullback-
Leibler divergence.
Remark 2. The KG-distribution from Definition 2 can equivalently be written as
PX(G)
Y	exp(ψ(xh) Xr, Xt))	Y
1 + exp (ψ(Xh, Xr, Xt))
(h,r,t)∈G	S八 h, r, "，(h,r,t)∈(o×R×o)∖G
1
1+exp(ψ(xh, Xr, Xt)).
6	Generalization Bounds for Log-Likelihood
In this section, we derive generalization bounds for log-likelihood of KG-distributions learned from
one sample (i.e. knowledge graph) from the data-generating distribution described in Section 3.
First, we define log-likelihood and normalized log-likelihood of the KG-distribution model.
Definition 3 (Normalized log-likelihood). Let G be a knowledge graph on a set of objects O and a
set of relations R andX be vector representations of the objects and relations. The log-likelihood of
this model is L(X|G) = lnPX(G) and its normalized log-likelihood is then defined as N L(X|G) =
∣O∣1∣R∣ ∙ L(X|G).
We start by proving a concentration inequality for the log-likelihood of KG-distributions, which is a
rather straightforward consequence of McDiarmid’s inequality.
Theorem 1.	Let G* be the ground-truth knowledge graph. Let G be sampled from the data-
generating distribution induced by G* and X be fixed vector representations. Then
P [∣L(X∣G) - E[L(X∣.)]∣ ≥ ε] ≤ 2exp (-M^)
where M ≥ suPh,t,r∈χ ∣Ψ(xh, Xr, Xt) | (we assume suPh,t,r∈χ ∣Ψ(xh, Xr, Xt) | to be finite) and the
probability P[. . . ] and the expectation E[. . . ] are w.r.t. the sampling of knowledge graphs from the
data-generating distribution.
The next theorem is the main result of this section. It bounds the expectation of the maximum
deviation of the log-likelihood of KG distributions from their mean, where the KG-distributions are
selected from a hypothesis class H (which has to satisfy certain assumptions listed in the theorem).
Theorem 2.	Let G * be a ground-truth knowledge graph on a set of objects O and a set of relations
R and let G be sampled by the data-generating distribution. Let X ⊆ Rd. Finally, let HX (“hy-
pothesis class”) denote the set of vector representations X, X ⊆ X |O|+|R|, of the objects from O
and relations from R. Let D, M and K be real numbers satisfying:
1.	∀X, X0 ∈ X : kX - X0 k ≤ D,
2.	∀Xh, Xr, Xt ∈ X ： ∣Ψ(Xh, Xr , Xt)| ≤ M,
3.	∀Xh,Xr,Xt,X0h,X0r,X0t ∈ X: kψ(Xh, Xr, Xt) - ψ(X0h,X0r,X0t)k ≤ K(kXh - X0hk + kXr -
X0rk + kXt -X0tk).
Then the following holds:
E [ sup ∣NL(X∣G) — E[NL(X∣.)]∣1 ≤ 启+ MSjfiZ ∙ SIK(IeDKO^H
X∈HX	|O|	|R||O|2	|O|
where both expectations on the l.h.s. are w.r.t. sampling of knowledge graphs from the data-
generating distribution.
5
Under review as a conference paper at ICLR 2020
7	Pinsker’s Inequality Comes into Play
In this section, we use Pinsker’s inequality to show how the Kullback-Leibler divergence between
the data-generating distribution, which generates the observed knowledge graph, and the learned
distribution can be used to bound the number of incorrectly predicted triples. In particular, we
assume that all triples with probability given by the learned distribution greater than a fixed threshold
t are predicted as positive (i.e. predicted to belong in the knowledge graph) and the rest as negative
(i.e. predicted not to belong in the knowledge graph). An incorrectly predicted triple is then either
a triple which is included in the ground-truth knowledge graph but we predict it as false (false
negative) or a triple which is not included in the ground-truth knowledge graph but we predict it as
true (false positive). In combination with the results from the previous section, this will allow us
to bound the expected number of incorrectly predicted triples (this is explained in detail in the next
section).
Theorem 3, which is the main result of this section, bounds the number of incorrectly predicted
triples as a function of the Kullback-Leibler divergence of the data-generating and the learned distri-
butions, of the threshold t (used to decide which triple is predicted as positive) and the missingness
parameter δ (the fraction of the triples missing from the ground-truth knowledge graph). The main
idea of the proof of this theorem (which is given in the appendix) is as follows. Whenever there
is an incorrectly predicted triple τ, the difference between its probability w.r.t. the data-generating
distribution and w.r.t. the learned distribution must be greater than some fixed ∆ (the value of ∆
depends on the threshold t and the missingness parameter δ), otherwise τ could not be incorrectly
predicted. Pinsker’s inequality (Pinsker, 1964), which relates total variation distance and Kullback-
Leibler divergence, then allows us to obtain a lower bound on the Kullback-Leibler divergence of
the data-generating and the learned marginal distributions corresponding to the incorrectly predicted
triple τ . Since triples are sampled independently of each other in both the data-generating and the
learned distributions, the sum of the Kullback-Leibler divergences of the marginal distributions cor-
responding to all possible individual triples will equal the Kullback-Leibler divergence of the data-
generating distribution and the learned distribution. This all together is what allows us to derive the
bound stated in the next theorem.
Theorem 3. Let G* be the ground-truth graph on a set of objects O and relations R. Let P denote
the data-generating distribution induced by G* with the missingness parameter δ and let Q denote
the learned distribution. Let t ∈ (0; 1 - δ) be a threshold for predicting which triples belong to the
knowledge graph. Finally, let F be the set of incorrectly predicted triples, i.e. the set of all triples
τ ∈ O ×R×O such that either τ ∈ G* and the probability ofτ given by the learned distribution Q is
smaller than t (false negatives) or τ 6∈ G * and the probability ofτ given by the learned distribution
Q is greater or equal to t (false positives). Then the following holds for the cardinality of the set of
incorrectly predicted triples F:
|F| ≤ max { 2t2, 2(1 -δ -1)2 } . KL(P||Q).
Note that the bound from this theorem is minimized for the threshold t :=(1 - δ)∕2. This suggests
how to set the threshold ifwe knew the missingness parameter δ.
8	Putting Everything So Far Together
Next we combine the results from the previous sections to obtain a bound on the expected number
of incorrectly predicted triples for vector representations X learned by maximizing the likelihood
of the respective KG-distribution parametrized by X. Here we assume that the domain X of the
vector representations is a subset of Rd of finite diameter and that there is some pre-fixed scoring
function ψ.
Theorem 4.	Let G* be a ground-truth knowledge graph on a set of objects O anda set of relations R
and let G be sampled by the data-generating distribution. Let X ⊆ Rd. Let HX (“hypothesis class”)
denote the set of vector representations X, X ⊆ X |O|+|R|, of the objects from O and relations
from R. Let K, M, D be as in Theorem 2 and t and δ be as in Theorem 3. Let X ∈ HX be a
representation of objects and relations learned by maximizing the log-likelihood L(X|G), i.e. X =
arg maxX0 ∈HX L(X0|G). Finally, let X* = arg maxX∈HX E[L(X|.)]. Then the following holds for
6
Under review as a conference paper at ICLR 2020
the sets of incorrectly predicted triples FX, obtained using the KG-distribution with X:
|Fx I
∣o∣2∣R∣
E
ln2 ln2	4
≤ mar 铲,(i—δ—1)2 ∫Λ∣oi
,∣G I ∣G*I	Idln(4eDK∣O∣√d) ι
+Mv WF V-----------O------+
KL(P ∣∣Pχ*)
∣O∣2∣R∣
where KL(P∣∣PX*) is the KuUback-Leibler divergence of the data-generating distribution P and
the best possible representable distribution Pχ*.
9 The “Realizable” Case
In learning theory, one usually speaks of the “realizable case” when the hypothesis class contains
the target concept. In general, the data generating distribution cannot be represented exactly using
the KG-distribution with vector representations from a given set X unless we allow ∣ψ∣ to be infinite
(which would be needed to model the triples that have zero probability of being sampled from the
data generating distribution because they are not contained in the ground-truth knowledge graph
G*). Therefore We cannot really speak of the “realizable case" in the usual sense. In particular it
may be the case that the given scoring function ψ and the given domain of vector representations
X are enough to represent the knoWledge graph in the sense that there exists a threshold t such
that (ψ(xh,Xr,Xt) ≥ t) ⇔ (h,r,t) ∈ G*), and yet the data-generating distribution could not
be represented by the corresponding KG-distribution exactly. Hence, it makes sense to define the
realizable case slightly differently for our setting.
Definition 4 (γ-Realizable case). For a given data-generating distribution P induced by a ground-
truth knowledge graph G* with the missingness parameter δ and a given scoring function ψ and a
domain of vector representations X, we say that a KG-distribution PX given by a scoring function ψ
and vector representations X is γ-admissible if the following is satisfied: exp(ψ(Xh, Xr, Xt))/(1 +
exp(ψ(xh, Xr, Xt))) ≥ 1 一 Y for all (h,r,t) ∈ G* and 1/(1 + exp(ψ(xh, Xr, Xt))) ≤ Y for all
(h,r,t) ∈ G*, where xh, Xr, Xt are vector representations given by X. We say that a learning
problem is Y-realizable if all vector representations X ∈ X |O|+|R| that maximize the expected
log-likelihood (i.e. the best vector representations) give rise to Y-admissible KG-distributions.
Next We present a version of Theorem 3 suitable for Y-realizable settings.
Theorem 5.	Let G * be the ground-truth graph on a set of objects O and relations R. Let P denote
the data-generating distribution induced by G * and let Pγ be a Y-admissible distribution. Let Q
denote the learned distribution and let t ∈ (Y; 1 — Y) be a threshold. Then the following holds for
the cardinality of the set of incorrectly predicted triples F:
IFI ≤ max { 2(t — Y)2 , 2(1 - γ - t)2 } ∙ KL(PY∣∣Q).
In the next theorem We present a tighter version of Theorem 4 for Y-realizable settings.
Theorem 6.	Let everything be as in Theorem 4 but assume a Y-realizable setting. Then the following
holds for the sets of incorrectly predicted triples FX, obtained using the KG-distribution with X:
眉 ∣FX∣ ] N ʃ ln2 ln2 1
Eom∖ ≤maxi(t-ʒy,a-t-γ)2ʃ
+M y∣RG⅛
Idln (4eDK∣O∣√d)
V O
The next example illustrates that the results from Theorem 6 are asymptotically non-trivial (Which
is non-obvious as We shoW beloW).
Example 2. Let us assume that we have some fixed HX, K, D and d and that the X* ∈ HX. For
simplicity, let us assume that this is a Y-realizable setting with Y = 0.1. Let the ground knowledge
graphs G be sparse and satisfy ∣G∣ = α(∣O∣) ∙ ∣O∣ where a is an integer-valued function and O is
the set of objects in the knowledge graph. We suppose that the threshold t satisfies t ∈ (Y, 1 - Y).
Then the expected fraction of triples incorrectly predicted by the learned model from our hypothesis
7
Under review as a conference paper at ICLR 2020
class can be bounded by: Ci ∙，α(∣O∣),ln |O|/|O| where Ci is a positive constant. Now consider
another “alternative prediction method” which always predicts all triples as false. The fraction of
triples incorrectly predicted by this method will be: E [∣F∣∕(∣O∣2∣R∣)] ≤ C2 ∙ α(∣O∣)∕∣O∣ where
C2 is a positive constant. It follows that whenever ɑ(∣O∣) grows faster than ln |O|, the bounds of
the maximum likelihood method, derived in this paper, become tighter than the error bound of this
trivial method. Here, we note that ɑ(∣O∣) growing only as a logarithm COrreSPOndS to very sparse
knowledge graphs (in which prediction of positive examples is likely hard).
10	Existing KGE Methods in the Light of Our Analysis
In this section, we discuss how existing knowledge graph embedding methods in general fit into our
analysis.
10.1	Scoring Functions
We assumed that predictions are made by comparing exp (ψ(xh, xr, xt)) with a threshold. In exist-
ing works, one directly compares the output of the scoring function ψ(xh, xr, xt) with a threshold.
This differs only by taking a logarithm and, hence, obviously does not harm applicability of our
analysis.
10.2	Loss Functions
Throughout this paper we assumed learning vector representations of objects and relations using
maximum likelihood. Here we show that this setting corresponds to learning knowledge graph
embeddings using a sigmoidal transformation of the scoring function ψ together with log-loss. This
combination is often used in the knowledge graph embedding literature (Trouillon et al., 2016;
Dettmers et al., 2018; Kazemi & Poole, 2018).2 In particular the loss used in the literature is (using
our notation):
l(X, G) =	ln(1 + exp(-ψ(xh, xr, xt)))+	ln(1 + exp(ψ(xh,xr,xt)))
(h,r,t)∈G	(h,r,t)∈(O×R×O)∖G
- ln
(h,r,t)∈G
1+expJψ(Xh，Xr，Xt)) ) +(h,r,t)∈(θ×R×O)∖G
1
1+exp(ψ(xh, Xr, Xt))
—
X ln( exp(ψ(Xh，Xr，Xt))、、)+ X ln
√⅛G	'1+eXp3(Xh，Xr，Xt)”	(h,r,t)∈(O×R×O)∖G
1
1 + exp(ψ(Xh, Xr, Xt))
It is not difficult to see that this is the same as the negative log-likelihood of the KG-distribution
with scoring function ψ. Thus, our analysis directly applies to methods that use this loss, such as
ComplEx (Trouillon et al., 2016), ConvE (Dettmers et al., 2018) and SimplE (Kazemi & Poole,
2018). For other methods, log-loss can in principle be also used and it was observed by Trouillon
& Nickel (2017) that the margin-based loss functions, used by many knowledge graph embedding
methods, are more prone to overfitting compared to log-likelihood. Our results may shed further
light on this latter observation, however, to verify it theoretically would require to perform a similar
type of analysis as we did in this paper for the margin-based loss functions (which might require
completely different techniques). Therefore we leave this comparison for future work.
10.3	Lipschitz Continuity and Bounded Domain Assumptions
Our result requires Lipschitz continuity. Most of the scoring functions ψ used by existing knowledge
graph methods are not globally Lipschitz continuous. However, they are Lipschitz continuous if we
2These works use regularized log-loss. Here we ignore the regularization term but note that bounding the
domain of the vector representations has to some extent a similar effect.
8
Under review as a conference paper at ICLR 2020
9 UIoJOoqI Ulo亡 PUnom
Figure 1: Illustration of the generalization bound from Theorem 6 applied on TransE (Bordes et al.,
2013) and SimplE (Kazemi & Poole, 2018). The bound for TransE is shown in red and the bound
for SimplE in blue. The parameters of the models are described in the main text.
restrict the domain of the embeddings to a bounded subset of Rd (this is among others true for the
approaches based on matrix or tensor factorization).
For obtaining the generalization bounds, we needed to assume that the learned vector representations
come from a set with a bounded diameter. Existing methods usually do not have such a constraint
explicitly but use a regularization term which has a similar effect in practice.
10.4	Numerical Examples
TransE Here we show how our results apply to TransE (Bordes et al., 2013), arguably the simplest
embedding method. In TransE, ψ(xh, xr, xt) = -kxh + xr - xtk. It is not difficult to show that
the Lipschitz constant K = 1 as follows (using the triangle inequality repeatedly):
|kxh +xr -xtk - kx0h +x0r -x0tk| = |kxh -x0h +xr - x0r -xt +x0t +x0h +x0r -x0tk
-kx0h +x0r -x0tk| ≤ kxh-x0h+xr-x0r -xt+x0tk ≤ kxh - x0hk + kxr-x0rk + kxt - x0tk.
Let the dimension of the vector embeddings be d = 3. Let us suppose that X = {x|kxk ≤ ln 10}.
Then the remaining parameters from Theorem 4 are D = ln 102 and M = ln 103. For simplicity, let
Us assume a γ-realizable setting with Y = 0.01. Let t = 0.5. Finally suppose that |G* | = |O| ln2 |O|
and |R| = 1. Now we have all parameters needed to apply Theorem 6. We plot the resulting bound
for a varying number of objects in Figure 1 in red. Note that, even though the number of dimensions
of the vector embeddings is low, the total number of parameters of the model is very high because
each object and each relation has its own parameters.
SimplE Next we illustrate how our analysis applies to SimpleE (Kazemi & Poole, 2018), which
is a fully-expressive knowledge graph embedding method. In SimplE, each object o ∈ O and each
relation r ∈ R are represented by two vectors x0o and x0o0, respectively, x0r and x0r0. In our formalism,
we represent both as single vectors: xo = [x0o, x0o0] and xr = [x0r, x0r0]. The scoring function of Sim-
pleE is ψ(xh, Xr, Xt) = 1 (hχh, Xr, xt0i + hxt, x；, Xhi) where(x, y,z)：= Pd=I Xiyizi. Clearly
the Lipschitz constant of ψ(xo, xr, xt) is not bounded for unbounded xh, xr and xt. We therefore
need to restrict the domain of the vector representations X, which our theorems assume anyways
and which is done implicitly through the L2-regularization term in the original work (Kazemi &
Poole, 2018). We show in the appendix that the Lipschitz constant K for SimplE can be bounded
by δ26 supχ∈χ ∣∣xk2. For the illustration, We will assume the same dimension d = 3 and the same
domain X as we did for TransE, i.e. X = {x|kxk ≤ ln 10}. Then the other parameters needed to
compute the generalization bound for SimplE are D = ln 102, M = (ln 10)3, K = 寻(ln 10)2.
Finally, We assume the same Y, |R| and |G*| as We did for TransE. The resulting generalization
bound for SimplE following from Theorem 5 is shown in Figure 1 in red together with the bound
for TransE (in blue). Not surprisingly, the generalization error bound is looser for SimplE as it is
9
Under review as a conference paper at ICLR 2020
a more flexible model (in fact, a fully-expressive model with a sufficiently large dimension of the
vector representations).
11	Related Work
Our work is very close in spirit to the theoretical analyses of learning from positive and unlabeled
examples, known as PU-learning (Niu et al., 2016). However, there are several distinctions that do
not allow direct application of the existing results to the problem that we analyze in this paper. First,
existing works on PU-learning assume the inductive setting whereas our setting is transductive. Sec-
ond, the work (Niu et al., 2016), which is one of the few papers analyzing generalization bounds
for PU-learning, only applies to losses that satisfy l(t, +1) + l(t, -1) = 1 which is not the case for
log-loss. Moreover the losses considered by Niu et al. (2016) are usually not used for knowledge
graph embedding in practice. Third, even if the previous two differences were not a problem (al-
though they are), it would still not be obvious how to apply the existing results to the knowledge
graph embedding setting in which we deal with interconnected examples; i.e. it is not clear what the
attributes of the examples and the hypothesis class should be so that we could fit knowledge graph
embedding setting into the standard PU learning setting. Lastly, an advantage of our analysis is that
it uses a simple probabilistic model which provides a possible, hopefully intuitive, explanation of
why knowledge graph embedding methods work (at least in the standard testing scenarios that use
test data generated completely at random).
Another line of works that are related to ours are works on generalization bounds for link prediction,
such as (London et al., 2013; Wang et al., 2018). The main differences w.r.t. these works is that
(i) they assume availability of both positive and negative examples (whereas we only have positive
examples), (ii) the links are predicted based on attributes of nodes and not on the structure of the
relations. Finally, our work is also related to works for tensor completion using tensor decomposi-
tion, such as (Nickel & Tresp, 2013). However, first, not all knowledge graph embedding methods
can be seen as some low-rank tensor decompositions and, second, as far as we know no analysis
of generalization bounds of tensor decomposition applies to the PU-learning setting, in which one
observes only positive examples (which is the setting studied in our work).
12	Conclusions
In this paper we have derived generalization bounds for the expected number of triples predicted
incorrectly by knowledge graph embedding methods. The main assumptions that we used were: (i)
facts are missing from the knowledge graph “completely at random” and (ii) the vector representa-
tions of objects and relations are learned by maximizing log-likelihood of the model (equivalently by
minimizing log-loss). While the “missing completely at random” assumption may seem simplistic,
it is actually how training and test sets are usually constructed, e.g. in (Bordes et al., 2013), and only
recently there have been works systematically looking beyond datasets constructed in this way, e.g.
(Trouillon et al., 2019). As for the second assumption, it would be interesting to see if one could
derive similar bounds for other loss functions that have been used for knowledge graph embedding
in the literature, which would then provide a more solid theoretical justification for their use.
References
Jessa Bekker and Jesse Davis. Learning from positive and unlabeled data: A survey. arXiv preprint
arXiv:1811.04820, 2018.
Antoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. Learning structured embed-
dings of knowledge bases. In Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in neural information
processing systems,pp. 2787-2795, 2013.
Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. A semantic matching energy
function for learning with multi-relational data. Machine Learning, 94(2):233-259, 2014.
10
Under review as a conference paper at ICLR 2020
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka, and Tom M
Mitchell. Toward an architecture for never-ending language learning. In Twenty-Fourth AAAI
Conference on Artificial Intelligence, 2010.
Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d
knowledge graph embeddings. In Proceedings of the Thirty-Second AAAI Conference on Arti-
ficialIntelligence, (AAAI-18),pp. 1811-1818,2018.
LUc Devroye, Laszlo Gyorfi, and Gabor Lugosi. A probabilistic theory of pattern recognition, Vol-
ume 31. Springer Science & Business Media, 2013.
Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas
Strohmann, Shaohua Sun, and Wei Zhang. Knowledge vault: a web-scale approach to proba-
bilistic knowledge fusion. In The 20th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’14, 2014, pp. 601-610, 2014.
Miao Fan, Qiang Zhou, Emily Chang, and Thomas Fang Zheng. Transition-based knowledge graph
embedding with relational mapping properties. In Proceedings of the 28th Pacific Asia Conference
on Language, Information and Computing, pp. 328-337, 2014.
Jun Feng, Minlie Huang, Mingdong Wang, Mantong Zhou, Yu Hao, and Xiaoyan Zhu. Knowledge
graph embedding by flexible translation. In Fifteenth International Conference on the Principles
of Knowledge Representation and Reasoning, 2016.
Alberto Garcla-Duran, Antoine Bordes, and Nicolas Usunier. Effective blending of two and three-
way interactions for modeling multi-relational data. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, 2014.
Xiao Han, Minlie Huang, Hao Yu, and Xiaoyan Zhu. Transg : A generative mixture model for
knowledge graph embedding. Computer Science, 2016.
Katsuhiko Hayashi and Masashi Shimbo. On the equivalence of holographic and complex em-
beddings for link prediction. In Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics, ACL 2017, Volume 2: Short Papers, pp. 554-559, 2017.
Shizhu He, Liu Kang, Guoliang Ji, and Jun Zhao. Learning to represent knowledge graphs with
gaussian embedding. In Acm International on Conference on Information & Knowledge Manage-
ment, 2015.
Frank L Hitchcock. The expression of a tensor or a polyadic as a sum of products. Journal of
Mathematics and Physics, 6(1-4):164-189, 1927.
Tony Jebara, Risi Kondor, and Andrew Howard. Probability product kernels. Journal of Machine
Learning Research, 5(5):819-844, 2004.
Rodolphe Jenatton, Nicolas Le Roux, Antoine Bordes, and Guillaume Obozinski. A latent factor
model for highly multi-relational data. Advances in Neural Information Processing Systems, 4:
3167-3175, 2012.
Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun Zhao. Knowledge graph embedding via
dynamic mapping matrix. In Proceedings of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pp. 687-696, 2015.
Guoliang Ji, Kang Liu, Shizhu He, and Jun Zhao. Knowledge graph completion with adaptive sparse
transfer matrix. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.
Seyed Mehran Kazemi and David Poole. Simple embedding for link prediction in knowledge graphs.
In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Infor-
mation Processing Systems 2018, NeurIPS., pp. 4289-4300, 2018.
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation
embeddings for knowledge graph completion. In Twenty-ninth AAAI conference on artificial
intelligence, 2015.
11
Under review as a conference paper at ICLR 2020
Hanxiao Liu, Yuexin Wu, and Yiming Yang. Analogical inference for multi-relational embeddings.
In Proceedings ofthe 34th International Conference on Machine Learning, ICML 2017, pp. 2168-
2178, 2017.
Quan Liu, Hui Jiang, Zhen-Hua Ling, Si Wei, and Yu Hu. Probabilistic reasoning via deep learning:
Neural association models. CoRR, abs/1603.07704, 2016. URL http://arxiv.org/abs/
1603.07704.
Ben London, Bert Huang, and Lise Getoor. Graph-based generalization bounds for learning binary
relations. CoRR, abs/1302.5348, 2013. URL http://arxiv.org/abs/1302.5348.
Farzaneh Mahdisoltani, Joanna Biega, and Fabian M Suchanek. YAGO3 : A Knowledge Base from
Multilingual Wikipedias. In Proceedings ofthe Conference on Innovative Data Systems Research,
CIDR ’15, 2015.
Colin McDiarmid. On the method of bounded differences. Surveys in combinatorics, 141(1):148-
188, 1989.
Maximilian Nickel and Volker Tresp. An analysis of tensor models for learning on structured data.
In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp.
272-287. Springer, 2013.
Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning
on multi-relational data. In ICML, volume 11, pp. 809-816, 2011.
Maximilian Nickel, Lorenzo Rosasco, and Tomaso A Poggio. Holographic embeddings of knowl-
edge graphs. In Thirtieth Aaai Conference on Artificial Intelligence, 2016.
Gang Niu, Marthinus Christoffel du Plessis, Tomoya Sakai, Yao Ma, and Masashi Sugiyama. Theo-
retical comparisons of positive-unlabeled learning against positive-negative learning. In Advances
in neural information processing systems, pp. 1199-1207, 2016.
Mark S Pinsker. Information and information stability of random variables and processes. Holden-
Day, 1964.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding
by relational rotation in complex space. In 7th International Conference on Learning Represen-
tations, ICLR 2019, 2019.
Theo Trouillon and Maximilian Nickel. Complex and holographic embeddings of knowledge
graphs: a comparison. arXiv preprint arXiv:1707.01475, 2017.
Theo Trouillon, Johannes Welbl, Sebastian Riedel, Enc GaUssier, and GUillaUme Bouchard. Com-
plex embeddings for simple link prediction. In Proceedings of the 33nd International Conference
on Machine Learning, ICML 2016, pp. 2071-2080, 2016.
Theo Trouillon, Enc Gaussier, Christopher R. Dance, and Guillaume Bouchard. On inductive abili-
ties of latent factor models for relational learning. J. Artif. Intell. Res., 64:21-53, 2019.
Yuanhong Wang, Yuyi Wang, Xingwu Liu, and Juhua Pu. On the ERM principle with networked
data. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18),
pp. 4244-4251, 2018.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by trans-
lating on hyperplanes. In Twenty-Eighth AAAI conference on artificial intelligence, 2014.
Han Xiao, Minlie Huang, Yu Hao, and Xiaoyan Zhu. Transa: An adaptive approach for knowledge
graph embedding. arXiv preprint arXiv:1509.05490, 2015.
Han Xiao, Minlie Huang, and Xiaoyan Zhu. From one point to a manifold: knowledge graph
embedding for precise link prediction. In Proceedings of the Twenty-Fifth International Joint
Conference on Artificial Intelligence, pp. 1315-1321. AAAI Press, 2016.
12
Under review as a conference paper at ICLR 2020
Bishan Yang, Scott Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities
and relations for learning and inference in knowledge bases. In Proceedings of the International
Conference on Learning Representations (ICLR) 2015, 2015.
A Useful Inequalities
We will need the following well-known inequalities.
Lemma 7 (Pinsker’s inequality (Pinsker, 1964)). Let P and Q be distributions. Then
KL(PIIQ) ≥ 2ιn2kP - Qk2.
Lemma 8 (McDiarmid’s Inequality (McDiarmid, 1989)). Let X1, . . . , Xn be independent random
variables with values from a set X and let f : Xn → R. If, for all i ∈ {1, . . . , n}, and for all
x1, , xn, x0i ∈ X, the function f satisfies If(x1, . . . , xi, . . . , xn) - f(x1, . . . , x0i, . . . , xn)I ≤ ci then:
P[If(X1,...,Xn)-E[f]I ≥ε] ≤2exp
(-P2⅛).
B Proof of Theorem 1
In this section, we prove Theorem 1.
Theorem 1. Let G* be the ground-truth knowledge graph. Let G be sampled from the data-
generating distribution induced by G* and X be fixed vector representations. Then
P [∣L(X∣G) - E[L(X∣.)]∣ ≥ ε] ≤ 2exp --M2^),
where M ≥ suph,t∈Xo,r∈Xr Iψ(xh, xr, xt)I.
Proof. Let F := IG* I. Let U1, . . . , UF be Bernoulli random variables such that P[Ui = 1] = 1 - δ.
Let Us denote by Ti the i-th tuple in G*. Let g : {0,1}F → 2g* be defined as g(uι,..., UF)=
{τi ∈ G* |ui = 1}, i.e. g(uι,..., UF) contains all Ti,s for which Ui = 1. It is not difficult to
see that G and g(U1, . . . , UF) have the same distribution. Hence we can bound the probability
P[IL(XIg(U1,. ..,UF)) - E[L(XI.)]I ≥ ε] instead of directly P [IL(XIG) - E[L(XI.)]I ≥ ε].
It follows from the assumptions of the theorem that for any (U1, . . . , UF) ∈ {0, 1}F the following
must hold:
IL (XIg(U1, . . . , Ui, . . . , UF)) - L (XIg(U1, . . . ,U0i, . . . , UF)) I
=	ψ(xh,xr,xt) - lnZ -	ψ(xh, xr, xt) + lnZ
(h,r,t)∈g(u1 ,...,ui,...,uF)	(h,r,t)∈g(u1 ,...,u0i ,...,uF)
≤ Iψ(xhi,xri,xti)I ≤M,
where xh, xr and xt denote vector representations of h, r and t, respectively.
Hence, we can use McDiarmid’s inequality to obtain P [IL (XIg(U1, . . . , UF)) - E[L(XI.)]I ≥ ε] ≤
2 exp ( - MF) from which the statement of the theorem follows directly.	□
C Proof of Theorem 2
In this section, we give a proof of Theorem 2. First, we prove several lemmas that we need to prove
this theorem.
Lemma 9. Let M > 0 be such that for every object or relation o it holds kxo - x0o k ≤ M where xo
and x0o are the vector representations ofo in X and X0, respectively. Ifψ satisfies the assumptions of
Theorem 2 then the following holds: IL(Xo,XrIG) -L(X0o,X0rIG)I ≤ 2KIOI2IRIMf.
13
Under review as a conference paper at ICLR 2020
Proof. Let us first introduce some notation. Let V := |O| and R = |R|. Whenever we write xh, xr
and xt, it denotes the vector representations of h, r and t from X, whereas when we write x0h, x0r
and x0t, it denotes the representations from X0 . We have
Z
∣L(Xo, Xr |G) - L(Xo, XrIG )| = E	(ψ(xh, Xr, Xt)- ψ(Xh, Xr , Xt))-足 Z
(h,r,t)∈G
Z
≤
(ψ(xh, Xr, Xt) - ψ(xh, Xr, Xt))I + Iln Zo 卜
(h,r,t)∈G
Next we bound the terms on the r.h.s. of the above inequality separately. We start by obtaining the
bound for the first term:
E (ψ(xh, Xr, Xt) - ψ(xh, Xr, Xt))
(h,r,t)∈G
≤ κv2R • (||Xh - Xhll + IIXr - Xrll + IIXt- XtII) ≤ κv2RMf. (1)
It remains to bound the second term. We have:
Z _ Pg∈ω Q(h,r,t)∈G Cgfx = Pg∈ω Q(h,r,t)∈G eSm ,χt)+ψ(XhMx))
Z7 = Pg∈Ω Q(h,r,t)∈G eψ(xh,χrM) =	Pg∈ω Q(h,r,t)∈G eψ(x0h,x0r ,x0t)
≤ JK-V2R∙M PG∈Ω Q(h,r,t)∈G e( h r， ' = eKV2RM(2)
Pg∈Ω Q(h,r,t)∈G eψ(x0h,x0r ,x0t) = e	(2)
where the last inequality follows from the inequality K (kXh - X7hk + kXr - X7r k + |Xt - X7tk) ≤
KM. Combining (1) and (2) We obtain the inequality that We needed to prove.	□
Lemma 10. Let G * be a ground-truth knowledge graph on a set of objects O and a set of relations R
and let G be sampled by the data-generating distribution. Let H (“hypothesis class”) be a finite set
of vector representations of the objects from O and relations from R. Then the folloWing inequality
holds
E sup |L(X|G) - E[L(X|.)|
X∈H
≤M
JG*I ln(2e∣H∣)
where M ≥ suPh,t∈Xo,r∈Xr |@(Xh, Xr, Xt )|.
Proof. We denote W := supx∈H ∣L(X∣G) - E[L(X∣.)∣ and B := 2∕(∣G*∣ ∙ M2). Then, using the
union bound and Theorem 1, we have: P [W ≥ ε] ≤ 2∣H∣ ∙ exp (一ε2 ∙ B). Next, we have
E[W] ≤ √E[W2]
P[W2 ≥ t]dt =
P [W ≥ √t]dt ≤
P [W ≥ √t]dt
≤
2∣H∣ ∙ exp (-t ∙ B)dt
2|H|
u+ + ɪ
• exp (-u ∙ B)
The above expression is minimized foru := ln (2|H|)/B (note that u may be arbitrary), yielding
the bound:
.ln(2e∣H∣)
^/|G*| ln(2e∣H∣)
Lemma 11 (Covering number, e.g., (Shalev-Shwartz & Ben-David, 2014)). Let S be a subset of
Rd of diameter at most k (i.e. for any X1, X2 ∈ S, kX1 - X2 k ≤ k). Then S can be covered by
d _	、d
(2k√d)∕ρ) balls ofradius P.
M
□
14
Under review as a conference paper at ICLR 2020
Now we are ready to prove Theorem 2.
Theorem 2. Let G* be a ground-truth knowledge graph on a set of objects O and a set of relations
R and let G be sampled by the data-generating distribution . Let X ⊆ Rd. Finally, let HX (“hy-
pothesis class”) denote the set of vector representations X, X ⊆ X |O|+|R|, of the objects from O
and relations from R. Let D, M and K be real numbers satisfying:
1.	∀x, x0 ∈ X : kx - x0 k ≤ D,
2.	∀Xh, Xr, Xt ∈ X： ∣ψ(Xh, Xr, Xt)| ≤ M,
3.	∀xh,xr,xt,x0h,x0r,x0t ∈ X: kψ(xh,xr,xt) - ψ(x0h,x0r,x0t)k ≤ K(kxh - x0hk + kxr -
X0rk + kXt -X0tk),
Then the following holds:
E [ sup ∣NL(X∣G) — E[NL(X∣.)" ≤ 备 + MSjfiZ∙ ∖^n(IeDKO2H.
X∈HX	|O|	|R||O|2	|O|
Proof. Let B be a finite set of vectors of dimension d such that if we place centers of balls of radius
ρ in all these points then these balls will cover the set X . Using Lemma 11, we know that it is
possible to find such a set B satisfying
|B| ≤ ((2D√d)∕ρ)d
(3)
because the diameter of the set X is at most D (due to the assumptions of the theorem) and the
dimension of the vectors in X is d. Next we assume that we are only searching for a maximum-
likelihood solution X consisting of vectors from B so that we could use Lemma 10 which expects a
finite hypothesis set H. Even though we have so far been thinking of the vector representations X
as lookup tables, it will be more convenient here to think of them as lists of vectors. Specifically,
we assume that the objects and relations are ordered arbitrarily. The hypothesis set will then be
represented as HB = B|O|+|R| . Hence, using (3), we obtain |HB| ≤
from Lemma 10 we have：
d(	L一 ∖d(∣o∣+∣R∣)
((2D√d)∕ρ)
. Then
E sup |L(X|G) - E[L(X|.)|
X∈B
≤M
JG *1 ln(；e|HbD
(4)
Using the bound on |HB|, we can bound the r.h.s. by：
IG*∣d(∣o∣ + ∣R∣)in (4eDP√d)
2
(5)
For fixed vector representations X0 of the objects and relations, let H(X0) denote the set of vector
representations X which satisfy that for every object or relation o： kXo - X0o k ≤ ρ where Xo and X0o
are vector representations of o from X and X0, respectively. Now, optimizing over the hypothesis set
HX of all possible vector representations from X instead of just the vectors from the set HB, would
15
Under review as a conference paper at ICLR 2020
yield the next bound:
E sup |L(X|G) - E[L(X|.)]|
X∈HX
=E sup sup |L(X|G) -E[L(X0|.)] +E[L(X0|.)] - E[L(X|.)]|
X0∈HB X∈H(X0)
≤ E sup sup |L(X|G) - L(X0|G) +L(X0|G) -E[L(X0|.)]|
X0∈HB X∈H(X0)
+ sup sup |E[L(X0|.)] - E[L(X|.)]|
X0∈HB X∈H(X0)
≤ E sup sup |L(X|G) - L(X0|G)| +E sup |L(X0|G) -E[L(X0|.)]|
X0∈HB X∈H(X0)	X0∈HB
+ sup sup |E[L(X0|.)] - E[L(X|.)]|
X0∈HB X∈H(X0)
Next, to bound the first and third term, we use Lemma 9 and, to bound the second term, we use
Equations (4) and (5). After setting P := ^o∣ and after simplifying, this gives Us the following
bound:
E [ SUp |L(X|G) - E[L(X∣.)]∣] ≤ 4∣O∣∣R∣ + M jlG*ld(QI + |R|)ln 0eDκlOl√¾
X∈HX	2
≤ 4∣O∣∣R∣ + My∣G*∣∣O∣∣R∣dln(4eDK∣O∣√d).
For the expected error of normalized log-likelihood we then obtain
E 卜Up INL(XIG) - E[NL(X∣.)]∣1 ≤ ɪ + MSj⅛ ∙ SIH(IeDKl^H
X∈HX	|O|	|R||O|2	|O|
which finishes the proof of the theorem.
□
D Proof of Theorem 3
In this section, we prove Theorem 3.
Theorem 3. Let G* be the ground-truth graph on a set of objects O and relations R. Let P denote
the data-generating distribution induced by G* with the missingness parameter δ and let Q denote
the learned distribution. Let t ∈ (0; 1 - δ) be a threshold for predicting which triples belong to the
knowledge graph. Finally, let F be the set of incorrectly predicted triples, i.e. the set of all triples
T ∈ O×R×O such that either T ∈ G * and the probability of T given by the learned distribution Q is
smaller than t (false negatives) or τ ∈ G * and the probability of T given by the learned distribution
Q is greater or equal to t (false positives). Then the following holds for the cardinality of the set of
incorrectly predicted triples F:
FI ≤ max { ln2, 2(i-n2- t)2 } ∙ KL(P||Q).
In the proof of the theorem, we will use the following simple lemma (which can be verified by
computing the discriminant of the respective quadratic equation).
Lemma 12. The following inequality holds for all x, y ∈ [0; 1]2 and α ∈ (0; ∞):
α(x - y)2 + 41α ≥ |x - y|.
16
Under review as a conference paper at ICLR 2020
Proof of Theorem 3. We assume that all possible triples (h, r, t) ∈ O × R × O are ordered (arbi-
trarily) and that Ti denotes the i-th triple from this set and I = {1,2,..., |O|2 ∙ |R|}. Let G be
sampled from the data-generating distribution and G0 be sampled from the learned model (i.e. the
KG-distribution). We define binary random variables Bi := 1(τi ∈ G) and Bi0 := 1(τi ∈ G0).
Hence the complete vectors B = (B1, . . . , Bn) and B0 = (B10 , . . . , Bn0 ) represent the knowledge
graphs sampled from the data-generating and modelled distributions, respectively. We denote by
PB and PB0 the distributions of B and B0 . Due to the independence properties of the two distri-
butions, the following holds: KL(PB||PB0) = Pi∈I KL(PBi ||PB0) where PBi and PB0 are the
distributions of Bi and Bi0 , respectively.
Next we bound the error that is made when we classify all triples τi with P [Bi0 = 1] > t as true (i.e.
belonging to the knowledge graph) and the rest as false. We may notice that when misclassifying a
triple τi , we must either have
P [Bi = 1] = 0 and P [Bi0 = 1] > t
(“false positive”) or
P[Bi = 1] = 1 - δandP[Bi0 = 1] < t
(“false negative”). Hence, if we denote by FP the set of triples τi for which we have false positive
errors, we must have for all α > 0
t|FP| ≤	P[Bi0= 1] ≤	(|P[Bi =1]-P[Bi0= 1]|)
≤ X ( 4α + α∣P [Bi = 1]- P [Bi = 1]∣2
τi ∈FP	α
where the last inequality follows from Lemma 12. It follows that
|FP|
t — 4L) ≤ X α∣P[Bi = 1] — P[B0 = 1]∣2.
4α	τi∈FP
Letting α = 1/(2t) yields the bound
|FP| ≤ t12 ∙ X |P[Bi = 1] — P[B0 = 1]∣2.
τi ∈FP
(6)
Similarly for false negatives, if we denote by FN the set of triples for which we have false negative
errors, we must have for all α0 > 0:
∣FN∣∙(1- δ - t) ≤ E |P [Bi = 1] — P [Bi = 1]|
τi∈FN
≤ X (T17 + αi∣P [Bi = 1] — P [Bi = 1]∣2
τi ∈FN 4α0	i
Hence, we also have:
∣FN∣∙ (4α⅛1 - δ - t) ≤ αi X |P [Bi = 1] - P [Bi = 1]∣2
4α	τi ∈FN
Letting α0 = 1/(2 — 2δ — 2t) gives us
|FN| ≤
1
(1 — δ — t)2
|P[Bi = 1] —P[Bi0= 1]|2.
τi∈FN
(7)
17
Under review as a conference paper at ICLR 2020
Next we combine (6) and (7) and apply Pinsker’s inequality:
|FP| + |FN| ≤ t12 ∙ X |P[Bi = 1] - P[B0 = 1]∣2
τi ∈FP
+ (I-(I-1)2 ∙ X IP [Bi = 1] - P [B0 = 1]∣2
( - - t)	τi∈FN
=~2 ∙ x 4kPBi - PBik2+(1 - δ-1)2 ∙ X 4kPBi - PBik2
τi∈FP	τi ∈FN
≤ 1max {t12, (1- δ-1)2} ∙ X kPBi - pB0 k2
≤ max { 22, 2(1-n2- t)2 } ∙ KL(PB||PB)⑻
This finishes the proof of this theorem.
□
E Proof of Theorem 4
Theorem 4 Let G* be a ground-truth knowledge graph on a set ofobjects O and a set ofrelations R
and let G be sampled by the data-generating distribution. Let X ⊆ Rd. Let HX (“hypothesis class”)
denote the set of vector representations X, X ⊆ X |O|+|R|, of the objects from O and relations
from R. Let K, M, D be as in Theorem 2 and t and δ be as in Theorem 3. Let X ∈ HX be a
representation of objects and relations learned by maximizing the log-likelihood L(X|G), i.e. X =
arg maxχo∈Hχ L(XIG). Finally, let X* = arg maxχ∈Hχ E[L(X∣.)]. Then the following holds for
the sets of incorrectly predicted triples FX, obtained using the KG-distribution with X:
|FX |
|O|2|R|
E
ln 2 ln 2	4
≤ maxI铲,(I - δ -1)2 ∫Λ∣Oi
l IG I |G*I	/dln(4eDK∣O∣√d) ι
+MVIRW V O +
KL(P ∣∣Pχ*)
∣o∣2∣R∣
where KL(P ∣∣Pχ*) is the KuUback-Leibler divergence of the data-generating distribution P and
the best possible representable distribution Pχ*.
Proof. Let G denote the knowledge graph sampled from the data-generating distribution and PX the
learned distribution. For a fixed Xfixed, the following equality holds: K L(P ∣∣PXfixed) = H(P) +
EP[L(Xfixed∣G)]. Thenwe also have KL(P∣∣PXfixed) - KL(P∣∣PX*) = EP[L(Xfixed∣G)]
- EP [L(X* ∣G)]. Next we can define g(G0) := arg maxX∈HX L(X∣G0), substitute g(G0) for Xfixed
and take the expectation over graphs G0, which are assumed to be sampled from the data-generating
distribution. This yields:
Eg，〜P[KL(P∣∣Pgg))- KL(P∣∣Pχ*)] = EGAP[Eg〜P[L(g(G0)∣G)] - EG〜P[L(X*∣G)]]∙
Next we can use Lemma 8.2 from (Devroye et al., 2013) to bound the r.h.s.:
Eg，〜p[Eg〜P[L(g(G0)∣G)] - EG〜P[L(X*∣G)]] ≤ 2Eg，〜P SUp ∣L(X∣G0) - E[L(X,∙)]∣ ∙
X∈HX
Finally, we use the bound from Theorem 2 to bound the r.h.s. of this inequality. Combining this with
Theorem 3 finishes the proof.	□
F Proof of Theorem 5
Theorem 5. Let G * be the ground-truth graph on a set of objects O and relations R. Let P denote
the data-generating distribution induced by G * and let Pγ be a γ-admissible distribution. Let Q
18
Under review as a conference paper at ICLR 2020
denote the learned distribution and let t ∈ (γ; 1 - γ) be a threshold. Then the following holds for
the cardinality of the set of incorrectly predicted triples F:
|F| ≤ max { 2(t - γ)2 , 2(1 - γ - t)2 } . KL(PY||Q).
Proof. The proof of this theorem is very similar to the proof of Theorem 3. We highlight the most
important differences in bold.
We assume that all possible triples (h, r, t) ∈ O × R × O are ordered (arbitrarily) and that τi
denotes the i-th triple from this set and I = {1, 2,..., |O|2 ∙ |R|}. Let G be sampled from the
distribution Pγ and G0 be sampled from the learned model (i.e. the KG-distribution). We define
binary random variables Bi := 1(τi ∈ G) and Bi0 := 1(τi ∈ G0). Hence the complete vectors
B = (B1, . . . , Bn) and B0 = (B10 , . . . , Bn0 ) represent the knowledge graphs sampled from the
distribution Pγ and the modelled distribution, respectively. We denote by PB and PB0 the distribu-
tions of B and B0 . Due to the independence properties of the two distributions (after all, Pγ is
still a KG-distribution and therefore inherits its independence structure), the following holds:
KL(PB||PB0) = Pi∈I KL(PBi ||PB0) where PBi and PB0 are the distributions of Bi and Bi0,
respectively.
Next we bound the error that is made when we classify all triples τi with P [Bi0 = 1] > t as true (i.e.
belonging to the knowledge graph) and the rest as false. We may notice that when misclassifying a
triple τi, we must either have
P[Bi = 1] ≤ γandP[Bi0 = 1] > t
(“false positive”) or
P[Bi = 1] ≥ 1 -γandP[Bi0 = 1] < t
(“false negative”).
The rest of differences w.r.t. the proof of Theorem 3 follows from the differences highlighted so far in
a straightforward way (details follow below).
Hence, if we denote by FP the set of triples τi for which we have false positive errors, we must
have for all α > 0
t|FP| ≤ 工 P [Bi = 1] ≤ ∑ (γ + |P [Bi = 1] - P [Bi = 1]|)
≤ X (Y +4α + α∣P [Bi = 1] - P [Bi = 1]∣2
τi∈FP	4α
where the last inequality follows from Lemma 12. It follows that
∣FP∣∙ (t-Y - 4α) ≤ X α∣P [Bi = 1] - P [Bi = 1]∣2.
α	τi∈FP
Letting 1/(2t - 2γ) yields the bound
∣fp∣ ≤ (I-^ ∙ X |P [Bi = 1] - P [Bi = 1]∣2.
(t - γ) τi∈FP
(9)
Similarly for false negatives, ifwe denote by FN the set of triples for which we have false negative
errors, we must have for all αi > 0:
∣FN∣∙ (1 - Y -1) ≤ E |P [Bi = 1] - P [Bi = 1]|
τi∈FN
≤ X (T17 + αi∣P [Bi = 1] - P [Bi = 1]∣2
τi ∈FN 4αi	i
19
Under review as a conference paper at ICLR 2020
Hence, we also have:
∣FN∣∙ (44≠ " Y - t) ≤ α0 X |P [Bi = 1]- P [Bi = 1]∣2
4α	τi∈FN
Letting α0 = 1/(2 - 2γ - 2t) gives us
|FN| ≤ a 1)2 ∙ X |P [Bi = 1] — P [Bi = 1]∣2.	(10)
(1-γ-t)	τi∈FN
Next, we combine (9) and (10) and apply Pinsker’s inequality:
|FP| + ∣FN∣≤ (tɪ)2 ∙ X |P [Bi = 1] - P [Bi = 1]∣2
(t - γ) τi∈FP
+ ∩~~∙ X |P[Bi = 1] - P[B0 = 1]∣2
(1-γ-t)2 τi∈FN	i
=(t-⅛∙ X 4kPBi-PBik2+(i-Y-1)2 ∙ X,4kPBi-PBik2
τi∈FP	τi∈FN
≤ 4 max {(t - Yy, (i - γ -1)2} ∙ X kPBi- PBi k2
≤ max {2(⅛, 2(i-nγ2-1)2 } ∙ KL(PB||PB)(II)
This finishes the proof of this theorem.	□
G Proof of Theorem 6
Theorem 6 Let everything be as in Theorem 4 but assume a Y-realizable setting. Then the following
holds for the sets of incorrectly predicted triples FX, obtained using the KG-distribution with X:
眉 ∣Fx∣ ] N ʃ ln2 ln2 1
EhOF司 ≤ maxi(t-ʒy,(1-t-γ)2 ʃ
+M y∣RG⅛
/dln (4eDK∣O∣√d)
V O
Proof. The proof of this theorem is practically identical to the proof of Theorem 4. The only differ-
ence is that in this proof We invoke Theorem 5 instead of Theorem 3.	□
H	The Lipschitz Constant of SimplE’s Scoring Function
In this section We derive a bound on the Lipschitz constant K of the scoring function of SimplE
(Kazemi & Poole, 2018) on a bounded domain X ⊆ R2d .3 The scoring function of SimplE is
ψ(χo, Xr, Xt) = 1 (Xh, Xr, xt0i + hxt, x；, Xhi) where〈x, Ya := Pd=I xiyizi. Let us denote
xh = [x0h, x0h0], xr = [x0r, x0r0], xt = [x0t, x0t0] and r = [xh, xr, xt]. Since the function ψ(r) is
differentiable everywhere, any K satisfying K ≥ ∣∣Vψ(r)k for all r ∈ X3 will be a Lipschitz
constant for ψ on the bounded domain X .
Next we compute the gradient of ψ(r). To simplify the notation, we use X[i] to denote the i-th
element of the vector X. We then have
3Here we use d to denote the dimension of the vector representations as used by Kazemi & Poole (2018).
Since SimplE defines two vectors for every object or relation, the dimension in our formalism is 2d, hence
X ⊆ R2d.
20
Under review as a conference paper at ICLR 2020
∂ψ(r) 	---= dχh[i] ∂ψ(r) 	= d χt[i]	1 =—■ 2 1 =—■ 2	- Xr [i] ∙ χt0[i],		∂ψ(r) -	TT"= ∂ Xr [i] ∂ψ(r) -	TT"= ∂ χro[i]	=1 ∙ χ0h[i] =1 ∙ χt[i] ∙	∙ χ0t0 [i], χ0h0[i],	∂ψ(r) dχt0 [i] ∂ψ(r) dχh[i] 一	1 =— 2 1 =— 2	∙ χ0h [i] ∙ χ0r [i], ∙ χ0t [i] ∙ χ0r0 [i].
		. χr0[i]	∙ Xh[i],						
From this We get									
1d
ι∣vψ(r)k2 = 4 X(Xr [i]2 ∙ χt0[i]2+χh[i]2 ∙ χt0[i]2+χh[i]2 ∙ Xr [i]2+xr0[i]2 ∙ xh[i]2
4 i=1
+χt[i]2 ∙ χh[i]2+χt[i]2 ∙ χr0[i]2)
16
≤ 4 (hχr, χt i2 + hχh, χt i2 + hχh, χri2 + hχr , Xhi + hχt, XGi + hχt, Xrf) ≤ 4 sup Ilχk4.
Thus, We get
l∣vψ(r)k ≤ 萼 SUP ∣∣χk2 := K,
2 x∈X
Let us noW denote yh = [yh0 ,yh00], yr = [y0r,y0r0], yt = [yt0,yt00], ands = [yh,yr,yt]. Then We have
kψ(r) — ψ(s)k ≤ Kkr 一 s∣ ≤ K ∙ (∣χh — yhk + l∣χr — y"l + l∣χt — ytk).
So K =堂 suρx∈X ∣∣χk2 is indeed the LiPsChitZ constant that We need in order to be able to obtain
generalization bounds using Theorems 4 and 5.
21