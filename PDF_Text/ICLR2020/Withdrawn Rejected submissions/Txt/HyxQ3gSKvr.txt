Under review as a conference paper at ICLR 2020
Variational Information Bottleneck for Un-
supervised Clustering: Deep Gaussian Mix-
ture Embedding
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we develop an unsupervised generative clustering framework that
combines variational information bottleneck and the Gaussian Mixture Model.
Specifically, in our approach we use the variational information bottleneck method
and model the latent space as a mixture of Gaussians. We derive a bound on the
cost function of our model that generalizes the evidence lower bound (ELBO); and
provide a variational inference type algorithm that allows to compute it. In the
algorithm, the coders’ mappings are parametrized using neural networks and the
bound is approximated by Markov sampling and optimized with stochastic gradient
descent. Numerical results on real datasets are provided to support the efficiency
of our method.
1	Introduction
Clustering consists in partitioning a given data set into various groups (clusters) based on some
similarity metric, such as Euclidean distance, L1 norm, L2 norm, L∞ norm, the popular logarithmic
loss measure or others. The principle is that each cluster should contain elements of the data that are
closer to each other than to any other element outside that cluster, in the sense of the defined similarity
measure. If the joint distribution of the clusters and data is not known, one should operate blindly in
doing so, i.e., using only the data elements at hand; and the approach is called unsupervised clustering.
Unsupervised clustering is perhaps one of the most important tasks of unsupervised machine learning
algorithms nowadays, due to a variety of application needs and connections with other problems.
Examples of unsupervised clustering algorithms include the so-popular K-means (Hartigan & Wong,
1979) and expectation maximization (EM) (Dempster et al., 1977). The K-means algorithm partitions
the data in a manner that the Euclidean distance among the members of each cluster is minimized.
With the EM algorithm, the underlying assumption is that the data comprises a mixture of Gaussian
samples, namely a Gaussian Mixture Model (GMM); and one estimates the parameters of each com-
ponent of the GMM while simultaneously associating each data sample to one of those components.
Although they offer some advantages in the context of clustering, these algorithms suffer from some
strong limitations. For example, it is well known that the K-means is highly sensitive to both the
order of the data and scaling; and the obtained accuracy depends strongly on the initial seeds (in
addition to that it does not predict the number of clusters or K-value). The EM algorithm suffers
mainly from low convergence, especially for high dimensional data.
Recently, a new approach has emerged that seeks to perform inference on a transformed domain
(generally referred to as latent space), not the data itself. The rationale is that because the latent space
often has fewer dimensions it is more convenient computationally to perform inference (clustering)
on it rather than on the high dimensional data directly. A key aspect then is how to design a latent
space that is amenable to accurate low-complex unsupervised clustering, i.e., one that preserves only
those features of the observed high dimensional data that are useful for clustering while removing
out all redundant or non-relevant information. Along this line of work, we can mention (Ding & He,
2004) which utilizes Principal Component Analysis (PCA) (Wold et al., 1987) for dimensionality
reduction followed by K-means for clustering the obtained reduced dimension data; or (Roweis,
1997) which uses a combination of PCA and the EM algorithm. Other works that use alternatives
1
Under review as a conference paper at ICLR 2020
for the linear PCA include Kernel PCA (Hofmann et al., 2008), which employs PCA in a non-linear
fashion to maximize variance in the data.
The usage of deep neural networks (DNN) for unsupervised clustering of high dimensional data
on a lower dimensional latent space has attracted considerable attention, especially with the advent
of autoencoder (AE) learning and the development of powerful tools to train them using standard
backpropagation techniques (Kingma & Welling, 2014; Rezende et al., 2014). Advanced forms
include Variational autoencoders (VAE) (Kingma & Welling, 2014; Rezende et al., 2014) which
are generative variants of AE that regularize the structure of the latent space and the more general
Variational Information Bottleneck (VIB) of (Alemi et al., 2017) which is a technique that is based
on the Information Bottleneck method (Tishby et al., 1999) and seeks a better trade-off between
accuracy and regularization than VAE via the introduction of a Lagrange-type parameter s which
controls that trade-off and whose optimization is similar to deterministic annealing (Slonim, 2002) or
stochastic relaxation.
In this paper, we develop an unsupervised generative clustering framework that combines VIB and the
Gaussian Mixture Model. Specifically, in our approach we use the variational information bottleneck
method and model the latent space as a mixture of Gaussians. The encoder and decoder of the model
are parametrized using neural networks (NN). The cost-function is calculated approximatively by
Markov sampling and optimized with stochastic gradient descent. Furthermore, the application of
our algorithm to the unsupervised clustering of various datasets, including the MNIST (Lecun et al.,
1998), REUTERS (Lewis et al., 2004) and STL-10 (Coates et al., 2011), allows a better clustering
accuracy than previous state of the art algorithms. For instance, we show that our algorithm performs
better than the variational deep embedding (VaDE) algorithm of (Jiang et al., 2017) which is based
on VAE and performs clustering by maximizes the ELBO and can be seen as a specific case of our
algorithm (Section 3.1). Our algorithm also generalizes the VIB of (Alemi et al., 2017) which models
the latent space as an isotropic Gaussian which is generally not expressive enough for the purpose of
unsupervised clustering. Other related works, but which are of lesser relevance to the contribution
of this paper, are the deep embedded clustering (DEC) of (Xie et al., 2016), the improved deep
embedded clustering (IDEC) of (Guo et al., 2017) and (Dilokthanakul et al., 2017). For a detailed
survey of clustering with deep learning, the readers may refer to (Min et al., 2018).
To the best of our knowledge, our algorithm performs the best in terms of clustering accuracy by using
deep neural networks without any prior knowledge regarding the labels (except the usual assumption
regarding the number of the classes) compared to the state-of-the-art algorithms of this category. In
order to achieve the aforementioned accuracy, i) we derive a cost-function that contains the IB hyper
parameter s that controls the trade-off between over-fit and generalization of the model and we used
an approximation of KL divergence that avoid assumptions which do not hold in the beginning of the
learning process and lead to convergence issues; ii) evaluate the hyper-parameter s by following an
annealing approach that improves both the convergence and the accuracy of the proposed algorithm.
Figure 1: Variational Information Bottleneck with Gaussian Mixtures.
2	Problem Definition and Model
Consider a dataset that is composed of N samples {xi}iN=1 which we wish to partition into |C| ≥ 1
clusters. Let C = {1, . . . , |C|} be the set of all possible clusters; and C designate a categorical
random variable that lies in C and stands for the index of the actual cluster. If X is a random variable
that models elements of the dataset, given X = xi induces a probability distribution on C which
the learner should learn. Thus, mathematically the problem is that of estimating the values of the
2
Under review as a conference paper at ICLR 2020
unknown conditional probability Pc∣χ(∙∣Xi) for all elements Xi of the dataset. The estimates are
sometimes referred to as the assignment probabilities.
As mentioned previously, we use the VIB framework and model the latent space as a GMM. The
resulting model is depicted in Figure 1, where the parameters ∏c, μo Σo for all values of C ∈ C,
are to be optimized jointly with those of the employed NNs as instantiation of the coders. Also,
the assignment probabilities are estimated based on the values of latent space vector instead of the
observation themselves, i.e., PC|U = QC|U. In the rest of this section, we elaborate on the inference
and generative network models for our method, which are illustrated below.
(Cy^χχ>^→0	(Cy^U^χuy^χ^χχ
Figure 2: Inference Network	Figure 3: Generative Network
2.1	Inference network model
We assume that an observed data X is generated from a GMM with |C| components. Then, the latent
representation u is inferred according the following procedure:
1.	One of the components of the GMM is chosen according to a categorical variable C .
∙-v
2.	The data X is generated from the c-th competent of the GMM, i.e., Pχ∣c 〜N(x; μc, ∑c).
3.	Encoder maps X to a latent representation U according to Pu∣χ 〜 N(μθ, ∑θ).
3.1.	The encoder is modeled with a DNN fθ which maps X to the parameters of a Gaussian
distribution, i.e., [μθ, ∑θ] = fθ(x).
3.2.	The representation U is sampled from N(μθ, ∑θ).
For the inference network, shown in Figure 2, the following Markov chain holds
C -- X -- U .	(1)
2.2	Generative network model
Since encoder extracts useful representations of the dataset and we assume that the dataset is generated
from a GMM, we model our latent space also with a mixture of Gaussians. To do so, the categorical
variable C is embedded with the latent variable U. The reconstruction of the dataset is generated
according to the following procedure:
1.	One of the components of the GMM is chosen according to a categorical variable C, with a
prior distribution QC.
2.	The representation U is generated from the c-th component, i.e., Qu∣c 〜N(u; μc, ∑c).
3.	The decoder maps the latent representation U to X which is the reconstruction of the source
x by using the mapping Qχ∣u.
3.1.	The decoder is modeled with aDNN gφ, that maps U to the estimate X,i.e., [X] = gφ(u).
For the generative network, shown in Figure 3, the following Markov chain holds
C -- U -- X .	(2)
3 Proposed Method
In this section we present our clustering method. First, we provide a general cost function for the
problem of the unsupervised clustering that we study here based on the variational IB framework; and
we show that it generalizes the ELBO bound developed in (Jiang et al., 2017). We then parametrize
our model using NNs whose parameters are optimized jointly with those of the GMM. Furthermore,
we discuss the influence of the hyper-parameter s that controls optimal trade-offs between accuracy
and regularization.
3
Under review as a conference paper at ICLR 2020
3.1	B rief review of variational Information Bottleneck for unsupervised
LEARNING
As described in Chapter 2, the stochastic encoder PU|X maps the observed data x to a representation
u. Similarly, the stochastic decoder QXU assigns an estimate X of X based on the vector u. As
per the IB method (Tishby et al., 1999) a suitable representation U should strike the right balance
between capturing all information about the categorical variable C that is contained in the observation
X and using the most concise representation for it. This leads to maximizing the following Lagrange
problem
Ls(P) = I(C; U) - sI(X; U) ,	(3)
where s ≥ 0 designates the Lagrange multiplier and for convenience P denotes the conditional
distribution PU|X .
Instead of equation 3 which is not always computable in our unsupervised clustering setting, we use
a modified version of it (so-called unsupervised IB objective (Alemi et al., 2017)) given by
∙-v
Ls(P): = -H(XIU)- s[H(U)- H(U|X)]	(4)
= EPX EPU|X [log PX|U + slogPU - slogPU|X] .	(5)
For a variational distribution QU on U (instead of the unknown PU) and a variational stochastic
decoder QX|U (instead of the unknown optimal decoder PX|U), let Q := {QX|U, QU}. Also, let
LsVB(P,Q) := EPX EPU|X [log QX|U] - sDKL(PU|XkQU) .	(6)
Lemma 1. For given P, we have
LVB(P, Q) ≤ Ls(P), for all Q .
In addition, there exists a unique Q that achieves the maximum maxQ LVB(P, Q) = Ls(P), and is
given by
QXIU = PXIU , QU = PU .	□
Using Lemma 1, maximization of equation 4 can be written in term of the variational IB cost as
follows
mPax L0s(P) = mPax mQax LsVB(P, Q) .	(7)
Remark 1. As we already mentioned in the beginning of this chapter, the related work (Jiang et al.,
2017) performs unsupervised clustering by combining VAE with GMM. Specifically, it maximizes the
following ELBO bound
L1VaDE := EPX EPU|X [log QXIU] - DKL(PCIXkQC) - EPC|X[DKL(PUIXkQUIC)] .	(8)
Let, for an arbitrary non-negative parameter s, LsVaDE be a generalization of the ELBO bound
in equation 8 of (Jiang et al., 2017) given by
LsVaDE := EPX EPU|X [logQXIU] - sDKL(PCIXkQC) - sEPC|X [DKL(PUIXkQUIC)] . (9)
Investigating the RHS of equation 9, we get
LsVB(P,Q) = LsVaDE + sEPX EPU|X[DKL(PCIXkQCIU)] .	(10)
Thus, by the non-negativity of relative entropy it is clear that LsVaDE is a lower bound on LsVB(P, Q).
Also, if variational distribution Q is such that the conditional marginal QCIU is equal to PCIX the
bound is tight since the relative entropy term is zero in this case.
3.2	Proposed algorithm: VIB - GMM
In order to compute equation 7, we parametrize the distributions PUIX and QXIU using DNNs. For
instance, let the stochastic encoder PUIX be a DNN fθ and the stochastic decoder QXIU be a DNN
gφ . That is
Pθ(u∣x) = N(u; μθ, Σθ) , where [μθ, ∑θ] = fθ(x),
Qφ(x∣u) = gφ(u) = [X],
(11)
4
Under review as a conference paper at ICLR 2020
where θ and φ are the weight and bias parameters of the DNNs. Furthermore, the latent space is
modeled as a GMM with |C| components with parameters ψ := {∏c, μc, ∑c}C=ι, i.e.,
Qψ (U) = X ∏c N(u; μc,∑c) ∙	(12)
c
Using the parametrizations above, the optimization of equation 7 can be rewritten as
max LsNN(θ, φ, ψ)	(13)
θ,φ,ψ
where the cost function LsNN (θ, φ, ψ) given by
LNN(θ,φ,ψ):= EPX 回θ(u∣χ)[logQφ(X∣U)] -SDKL(Pθ(UX)kQψ(U))] .	(14)
Then, for a given observations of N samples, i.e., {xi}iN=1, equation 13 can be approximated in terms
of an empirical cost as follows
1n
max — Lsmi Lsm p(θ, φ, ψ),	(15)
θ,φ,ψ ni=1
where Lesm,i p(θ, φ, ψ) is the empirical cost for the i-th observation xi , and given by
Lesm,i p(θ, φ, ψ) =EPθ(Ui∣χi)[logQφ(Xi |Ui )]-sDKL(Pθ(Ui |Xi )kQψ(Ui )) .	(16)
Furthermore, the first term of the RHS of equation 16 can be computed using Monte Carlo sampling
and the re-parametrization trick (Kingma & Welling, 2014). In particular, Pθ(u|x) can be sampled
by first sampling a random variable Z with distribution PZ, i.e., PZ = N(0, I), then transforming
the samples using some function fθ : X ×Z →U, i.e., U = fθ (x, z). Thus,
M
1	ι
EPθ (UiIXi) [log Qφ(XiIUi)] = ^M〉J log q(XiIUi,mb ui,m =从6履 + £3 ±m,	Em ~N(0, I),
m=1
where M is the number of samples for the Monte Carlo sampling step.
The second term of the RHS of equation 16 is the KL divergence between a single component
multivariate Gaussian and a Gaussian Mixture Model with ICI components. An exact closed-form
solution for the calculation of this term does not exist. However, a variational lower bound approxi-
mation (Hershey & Olsen, 2007) of it can be obtained as
∣C∣
DKL(Pθ(UiIXi)kQψ(Ui)) = TOgE∏cexp(-DκL(N(μθ,i, ∑θ,i)kN(μc, ∑c)) .	(17)
c=1
In particular, in the specific case in which the covariance matrices are diagonal, i.e., Σθ,i :=
diag({σθ2,i,j}jn=u1) and Σc := diag({σc2,j}jn=u1), withnu denoting the latent space dimension, equa-
tion 17 can be computed as follows
DKL(Pθ(UiIXi)kQψ(Ui))
=- log X ∏c exP (- 1 X h ^j-μj + log 妥 - 1 + 暮 i)，(18)
c=1	2j=1	σc,j σθ,i,j	σc,j
where μθ,ij and σθ2 分 j are the mean and variance of the i-th representation in the j-th dimension of
the latent space. Furthermore, μcj and σ2j represent the mean and variance of the c-th component
of the GMM in the j -th dimension of the latent space.
Finally, we train NNs to maximize the cost function equation 14 over the parameters θ, φ, as well
as those ψ of the GMM. For the training step, we use the ADAM optimization tool (Kingma & Ba,
2015). The training procedure is detailed in Algorithm 1.
5
Under review as a conference paper at ICLR 2020
Once our model is trained, we assign the given dataset into the clusters. As mentioned in Section 2,
we do the assignment from the latent representations, i.e., QC|U = PC|X. Hence, the probability that
the observed data xi belongs to the c-th cluster is computed as follows
p(c|xi) = q(c|ui)
qψ*(c)qψ*(ui∣c)
qψ*M)
πN(Ui；μ?见
Pcπ?N(Ui； μ?, ∑c)
(19)
where ? indicates optimal values of the parameters as found at the end of the training phase. Finally,
the right cluster is picked based on the largest assignment probability value.
Algorithm 1 VIB-GMM algorithm for unsupervised learning
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
input: Dataset D := {xi}in=1, parameter s ≥ 0.
output: Optimal DNN weights θ*, φ* and GMM parameters ψ* = {π?, μ?, £?}?「
initialization Initialize θ, φ, ψ.
repeat
Randomly select b mini-batch samples {xi}ib=1 from D.
Draw m random i.i.d samples {zj}jm=1 from PZ.
∙-v
Compute m samples Ui,j = fθ(xi, zj)
For the selected mini-batch, compute gradients of the empirical cost equation 15.
Update θ, φ, ψ using the estimated gradient (e.g. with SGD or ADAM).
until convergence of θ, φ, ψ .
Remark 2. It is worth to mention that with the use of the KL approximation in equation 17, our
algorithm does not use the assumption PC|U = QC|U (not as in Jiang et al. (2017)), which does not
hold in the beginning of the training phase and leads to convergence issues. This assumption is only
used in the final assignment after the training phase is over.
3.3 Effect of the hyper-parameter
As we already mentioned, the hyper-parameter
s controls the trade-off between the relevance
Algorithm 2 Annealing Algorithm Pseudo-Code
of the representation U and its complexity. As
it can be seen from equation 14 for small val-
ues of s, it is the cross-entropy term that dom-
inates, i.e., the algorithm trains the parameters
so as to reproduce X as accurate as possible.
For large values of s, however, it is most im-
portant for the NN to produce an encoded ver-
sion of X whose distribution matches the prior
distribution of the latent space, i.e., the term
Dkl(Pθ(U∣X)kQψ(U)) is nearly zero.
In the beginning of the training process, the
GMM components are randomly selected; and
input: Dataset D := {xi}in=1,
hyper-parameter interval [smin , smax].
output: Optimal DNN weights θ? , φ? , GMM
parameters ψ? = {π?, μ? ∑"3,
assignment probability PC|X .
initialization Initialize θ, φ, ψ .
repeat
Apply VIB-GMM algorithm.
Update ψ, θ, φ.
Update s, e.g., s = (1 + s)sold.
until s does not exceed smax .
so starting with a large value of the hyper-parameter s is likely to steer the solution towards an
irrelevant prior. Hence, for the tunning of the hyper-parameter s in practice it is more efficient to start
with a small value of s and gradually increase it with the number of epochs. This has the advantage to
avoid possible local minimas, an aspect that is reminiscent of deterministic annealing (Slonim, 2002),
where s plays the role of the temperature parameter. The experiments that will be reported in the
next section show that proceeding in the above described manner for the selection of the parameter
s helps getting better accuracy results and better robustness to the initialization (i.e., no need for a
strong pretraining). A pseudo-code for annealing is given in Algorithm 2. We note that tuning s is
very critical, such that the step size s in update of s should be chosen carefully, otherwise phase
transitions might be skipped that would cause a bad ACC score.
4 Experiments
4.1	Description of used datasets
In our empirical experiments, we apply our algorithm to the clustering of the following datasets.
6
Under review as a conference paper at ICLR 2020
MNIST: A dataset of gray-scale images of 70000 handwritten digits of dimensions 28 × 28 pixel.
STL-10: A dataset of color images collected from 10 categories. Each category consists of 1300
images of size of 96 × 96 (pixels) ×3 (rgb code). Hence, the original input dimension nx is 27648.
For this dataset, we use a pretrained convolutional NN model, i.e., ResNet-50 (He et al., 2016) to
reduce the dimensionality of the input. This preprocessing reduces the input dimension to 2048.
Then, our algorithm and other baselines are used for clustering.
REUTERS10K: A dataset that is composed of 810000 English stories labeled with a category
tree. As in (Xie et al., 2016), 4 root categories (corporate/industrial, government/social, markets,
economics) are selected as labels and all documents with multiple labels are discarded. Then, tf-idf
features are computed on the 2000 most frequently occurring words. Finally, 10000 samples are
taken randomly, which are referred to as REUTERS10K dataset.
4.2	Network settings and other parameters
We use the following network architecture: the encoder is modeled with NNs with 3 hidden layers
with dimensions nx -500-500-2000- J, where nx is the input dimension and nu is the dimension
of the latent space. The decoder consists of NNs with dimensions nu - 2000 - 500 - 500 - nx . All
layers are fully connected. For comparison purposes, we chose the architecture of the hidden layers as
well as the dimension of the latent space nu = 10 to coincide with those made for the DEC algorithm
of (Xie et al., 2016) and the VaDE algorithm of (Jiang et al., 2017). All except the last layers of the
encoder and decoder are activated with ReLU function. For the last (i.e., latent) layer of the encoder
we use a linear activation; and for the last (i.e., output) layer of the decoder we use sigmoid function
for MNIST and linear activation for the remaining datasets. The batch size is 100 and the variational
bound equation 15 is maximized by the Adam optimizer of (Kingma & Ba, 2015). The learning rate
is initialized with 0.002 and decreased gradually every 20 epochs with a decay rate of 0.9 until it
reaches a small value (0.0005 is our experiments). The reconstruction loss is calculated by using the
cross-entropy criterion for MNIST and mean squared error function for the other datasets.
	MNIST	STL-10	REUTERS10K
GMM 二	50.4	77.1	53.74 =
Dec	84.3*	80.6t	72.17*
VaDE	94.5»	84.3	798
VIB-GMM 一	96.2	91.6	80.4
t values are taken from VaDE (Jiang et al., 2017)
^ values are taken from DEC (Xie et al., 2016)
Table 1: Comparison of clustering accuracy of various algorithms.
)CCA( ycaruccA gniretsulC
0.5
一VIB-GMM
一VaDE
一DEC
一GMM
0.40
50	100	150	200	250	300
Epochs
350	400	450	500
3500
400
00 50 00 50
6 5 54
ssoL noitcurtsnoceR
2	4	6	8	10	12	14
KL Divergence Loss, I(X; U)
Figure 4:	Accuracy vs. number of epochs for the
STL-10 dataset.
Figure 5:	Information plane for the STL-10
dataset.
7
Under review as a conference paper at ICLR 2020
100
80
60
40
20
0
-20
-40
-60
-80
-100
-100 -80 -60 -40 -20	0	20	40	60	80	100
100
80
60
40
20
0
-	20
-	40
-	60
-	80
-80	-60	-40	-20	0	20	40	60
(b) 1-st epoch, accuracy = %41
(a) Initial accuracy = %10
100
80
60
40
20
0
-	20
-	40
-	60
-	80
-80	-60	-40	-20	0	20	40	60	80
(c)	5-th epoch, accuracy = %66
80
60
40
20
0
-	20
-	40
-	60
-	80
-80 -60 -40 -20	0	20	40	60	80
(d)	Final, accuracy = %91.6
Figure 6: Visualization of the latent space before training; and after 1, 5 and 500 epochs.
4.3	Clustering accuracy
We evaluate the performance of our algorithm in terms of the so-called unsupervised clustering
accuracy (ACC), which is a widely used metric in the context of unsupervised learning (Min et al.,
2018). For comparison purposes, we also present those of algorithms from previous art.
For each of the aforementioned datasets, we run our VIB-GMM algorithm for various values of the
hyper-parameter s inside an interval [smin , smax], starting from the smaller valuer s1 and gradually
increasing the value of s every nepoch epochs. For the MNIST dataset, we set (smin , smax , nepoch) =
(1, 5, 500); and for the STL-10 dataset and the REUTERS10K datset we choose these parameters to
be (1, 20, 500) and (1, 5, 100), respectively. The obtained ACC accuracy results are reported in the
Table 1 from which it can be seen that our algorithm outperforms significantly the DEC algorithm
of (Xie et al., 2016) as well as the VaDE algorithm of (Jiang et al., 2017) and GMM on the same
datsets. Important to note, for the MNIST dataset the reported ACC accuracy of 96.2% using our VIB-
GMM algorithm is obtained as the best case run out of ten times run all with random initializations.
For instance, we do not use any pretrained values for the initialization of our algorithm in sharp
contrast with the VaDE of (Jiang et al., 2017) and the DEC of (Xie et al., 2016). For the STL-10
dataset, none of the compared algorithms use a pretrained network except the intimal ResNet-50 for
dimensionality reduction. For REUTERS10K, we used the same pretrain parameters as DEC and
VaDE. Figure 4 depicts the evolution of the ACC accuracy with iterations (number of epochs) for the
four compared algorithms.
Figure 5 shows the evolution of the reconstruction loss of our VIB-GMM algorithm for the STL-10
dataset, as a function of simultaneously varying values of the hyper-parameter s and the number of
epochs (recall that, as per-the described methodology, we start with s = s1 and we increase its value
gradually every nepoch = 500 epochs). As it can be seen from the figure, the few first epochs are spent
8
Under review as a conference paper at ICLR 2020
almost entirely on reducing the reconstruction loss (i.e., a fitting phase) and most of the remaining
epochs are spent in making the found representation more concise (i.e., smaller KL-divergence). This
is reminiscent of the two-phase (fitting v.s. compression) that was observed for supervised learning
using VIB in (Schwartz-Ziv & Tishby, 2017).
4.4	Visualization on the latent space
In this section, we investigate the evolution of the unsupervised clustering of the STL-10 dataset on
the latent space using our VIB-GMM algorithm. For this purpose, we find it convenient to visualize
the latent space through application of the t-SNE algorithm of (van der Maaten & Hinton, 2008)
in order to generate meaningful representations in a two-dimensional space. Figure 6 shows 4000
randomly chosen latent representations before the start of the training process and respectively after
1, 5 and 500 epochs. The shown points (with a ∙ marker in the figure) represent latent representations
of data samples whose labels are identical. Colors are used to distinguish between clusters. Crosses
(with an x marker in the figure) correspond to the centroids of the clusters. More specifically,
Figure 6-(a) shows the initial latent space before the training process. If the clustering is performed
on the initial representations it allows ACC accuracy of as small as 10%, i.e., as bad as a random
assignment. Figure 6-(b) shows the latent space after one epoch, from which a partition of some of
the points starts to be already visible. With five epochs, that partitioning is significantly sharper and
the associated clusters can be recognized easily. Observe, however, that the cluster centers seem still
not to have converged. With 500 epochs, the ACC accuracy of our algorithm reaches %91.6 and the
clusters and their centroids are neater as visible from Figure 6-(d).
References
Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information
bottleneck. In Proceedings of the 5th International Conference on Learning Representations, 2017.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the 14th International Conference on Artificial Intelligence and
Statistics,pp.215- 223, 2011.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the
EM algorithm. Journal ofthe Royal Statistical Society, 39:1 一 38,1977.
Nat Dilokthanakul, Pedro A. M. Mediano, Marta Garnelo, Matthew C.H. Lee, Hugh Salimbeni,
Kai Arulkumaran, and Murray Shanahani. Deep unsupervised clustering with Gaussian mixture
variational autoencoders. arXiv: 1611.02648, 2017.
Chris Ding and Xiaofeng He. K-means clustering via principal component analysis. In Proceedings
of the 21st International Conference on Machine Learning, 2004.
Xifeng Guo, Long Gao, Xinwang Liu, and Jianping Yin. Improved deep embedded clustering with
local structure preservation. In Proceedings of the 26th International Joint Conference on Artificial
Intelligence, pp. 1753 - 1759, 2017.
J. A. Hartigan and M. A. Wong. Algorithm AS 136: A k-means clustering algorithm. Journal of the
Royal Statistical Society, 28:100 - 108, 1979.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pp.
770 - 778, 2016.
John R. Hershey and Peder A. Olsen. Approximating the Kullback Leibler divergence between
Gaussian mixture models. In Proc. of IEEE International Conference on Acoustics, Speech and
Signal Processing, pp. 317 - 320, April 2007.
Thomas Hofmann, Bernhard Scholkopf, and Alexander J. Smola. Kernel methods in machine learning.
The Annals of Statistics, 36:1171 - 1220, June 2008.
9
Under review as a conference paper at ICLR 2020
Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational deep
embedding: An unsupervised and generative approach to clustering. In Proceedings of the 26th
International Joint Conference on Artificial Intelligence,pp.1965 - 1972, 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of
the 3rd International Conference on Learning Representations, 2015.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the 2nd
International Conference on Learning Representations, 2014.
Yann Lecun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE, volume 86, pp. 2278 - 2324, 1998.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. A new benchmark collection for text
categorization research. The Journal of Machine Learning Research, 5:361 - 397, 2004.
Erxue Min, Xifeng Guo, Qiang Liu, Gen Zhang, Jianjing Cui, and Jun Long. A survey of clustering
with deep learning: From the perspective of network architecture. IEEE Access, 6:39501 - 39514,
2018.
Danilo J. Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approxi-
mate inference in deep generative models. In Proceedings of the 31st International Conference on
Machine Learning, pp. 1278 - 1286, 2014.
Sam Roweis. EM algorithms for PCA and SPCA. In Advances in Neural Information Processing
Systems 10, pp. 626 - 632, 1997.
Ravid Schwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via informa-
tion. arXiv: 1703.00810, 2017.
Noam Slonim. The information bottleneck: Theory and applications. PhD dissertation, Hebrew
University, 2002.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. In
Proceedings of the 37th Annual Allerton Conference on Communication, Control and Computing,
pp. 368 - 377, 1999.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine
Learning Research 9, pp. 2579 - 2605, November 2008.
Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometrics and
Intelligent Laboratory Systems, 2:37 - 52, August 1987.
Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis.
In Proceedings of the 33rd International Conference on Machine Learning, pp. 478 - 487, 2016.
A The Proof of Lemma 1
∙-v
First, we expand Ls (P) as follows
∙-v
Ls(P)= - H(XU)- SI(X; U)
= - H(X|U) - s[H(U) - H(U|X)]
=	p(u, x) log p(x|u) du dx
ux
+ s	p(u) log p(u) du - s	p(u, x) log p(u|x) du dx.
u	ux
Then, LsVB(P, Q) is defined as follows
LsVB(P, Q) :=	p(u, x) log q(x|u) du dx
ux
+ s	p(u) log q (u) du - s	p(u, x) log p(u|x) du dx.
u
(20)
10
Under review as a conference paper at ICLR 2020
Hence, we have the following relation
Ls(P)- LVB(P, Q) = EPX DKL (Pχ∣ukQx∣u)] + SDKL(PUkQU) ≥ 0
where equality holds under equalities QX|U = PX|U and QU = PU. We note that s ≥ 0.
B THE PROOF OF ALTERNATIVE EXPRESSION LsVADE
Here we show how we obtained equation 10.
To do so,
LVaDE
Ls
= EPX hEPU|X [log Qx|U] - SDKL(PU|xkQU) - SEPU|X DKL(PC|xkQC|U)i
=EPX [Epu∣x [log Qx∣u]] — s P P(x) [ p(u∣x) log P(UIX) du dx
x u	q(u)
—	s Z P(x) Z P(u∣x) XP(c∣x) log P(CIIX) du dx
x u c q(cIu)
=Epx [Epu∣x [log Qx∣u]] — s JJ P(x)p(u∣x) log Pq(UIX) dudx
—	s 〃 Xp(x)p(u∣c, x)p(c∣x) log P(CIIX) dudx
EPX EPU|X [log Qx|U]
-s
EPX EPU|X [log Qx|U]
-s
JL " χ)iog 喘陪
ZZ X p(u,c, x)log 半哗
uxc	q (c) q(u|c)
du dx
du dx
EPX EPU|X [log Qx|U]
— S / XP(c, x) log P(CIx) dx
xc	q(c)
— s 〃 XP(X)P(C∣x)p(u∣c, x) log I(UIx)) dudx
= EPX hEPU|X [log Qx|U] — sDKL (PC |x kQC) — sEPC|X [DKL (PU|x kQU|C)]i
(=c)LsVB(P,Q)—sEPXhEPU|X[DKL(PC|xkQC|U)]i ,
where (a) and (b) follow due to the Markov chain C —— X —— U; (C) follows from the definition of
LsVB(P, Q) in equation 6.
C KL Divergence B etween Multivariate Gaussian Distributions
The KL divergence between two multivariate Gaussian distributions Pi ~ N(μι, ∑ι) and
P2 ~ N(μ2, ∑2) in RJ is
DKL(PIIlP2) = 1 ((μι — μ2)τ∑-1(μι — μ2)+log I∑2∣ — log ∣∑1∣ — J + tr(∑-i∑ι)). (21)
For the case in which Σ1 and Σ2 covariance matrices are diagonal, i.e., Σ1 := diag({σ12,j}jJ=1) and
Σ2 := diag({σ22,j}jJ=1), equation 21 boils down to the following
DKL(PIkP2) = 1 (X jμj +log σ2j - 1 + σj).	(22)
2 j=1	σ2,j	σ1,j	σ2,j
11
Under review as a conference paper at ICLR 2020
D KL Divergence Between Gaussian Mixture Models
An exact close form for the calculation of the KL divergence between two Gaussian mixture models
does not exist. In this paper, we use a variational lower bound approximation for calculations of KL
between two Gaussian mixture models. Let f and g be GMMs and the marginal densities of x under
f and g are
MM
f (X)=): ωmN(x; μm, Σm) =): ωmfm (X)
m=1	m=1
CC
g (X) = E nN (x; μg, Σg) = Enc gc(x).
C=1	c=1
The KL divergence between two Gaussian mixtures f an g can be approximated as follows
M
DvKL(fkg) :=	ωm log
m=1
PPm0∈M∖m ωm0exp(-DκL(fmkfm0 ))
-Pc=I ∏c exp(-DκL(fmkgc))-
(23)
In this paper, we are interested, in particular, M = 1. Hence, equation 23 simplifies to
DvκL(fkg) = -logXπcexp(-DκL(fkgc)) ,	(24)
c=1
where Dkl(∙∣∣∙) is the KL divergence between single component multivariate Gaussian distribution,
defined as in equation 21.
12