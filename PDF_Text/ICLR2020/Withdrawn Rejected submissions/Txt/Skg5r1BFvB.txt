Under review as a conference paper at ICLR 2020
Provably	Efficient Contextual	Linear
Quadratic Regulator
Anonymous authors
Paper under double-blind review
Ab stract
A fundamental challenge in artificially intelligence is to build an agent that gen-
eralizes and adapts to unseen environments. A common strategy is to build a de-
coder that takes a context of the unseen new environment and generates a policy.
The current paper studies how to build a decoder for the fundamental continuous
control environment, linear quadratic regulator (LQR), which can model a wide
range of real world physical environments. We present a simple algorithm for this
problem, which uses upper confidence bound (UCB) to refine the estimate of the
decoder and balance the exploration-exploitation trade-off. Theoretically, our al-
gorithm enjoys a O (√T) regret bound in the online setting where T is the num-
ber of environments the agent played. This also implies after playing O(1/e2)
environments, the agent is able to transfer the learned knowledge to obtain an
e-suboptimal policy for an unseen environment. To our knowledge, this is first
provably efficient algorithm to build a decoder in the continuous control setting.
While our main focus is theoretical, we also present experiments that demonstrate
the effectiveness of our algorithm.
1	Introduction
Humans are able to solve a new task without any training based on previous experience in similar
tasks. Our intelligent agent should be able do the same, learning from previous experience, adapting
to the new ones and improving the performance as the agent gains more experience. This is a
challenging problem as we need to design an adaptation mechanism which is fundamentally different
from classical supervised learning methods.
A common approach is to build a decoder so that once the agent sees a description of new task,
i.e., the context of the new task, the decoder turns the context into a succinct representation of
the new task, based on which the agent is able to design a policy to solve the task. Note this
procedure resembles how a human solves a new task. For example, if a human wants to push an
object on a table, the human first sees the object and the table (context). Then, in his/her mind, the
context becomes a representation of this task, e.g., a sense of weight of the object. Based on this
representation, the human can easily reason about how much force to exert on the object.
This general approach has been applied in practice. For example, Wu et al. (2018) studied the visual
navigation task and built a Bayesian model that takes the context of new environments and outputs
the policy that enables the agent to navigate. Killian et al. (2016) used this approach to develop
personalized medicine policies for HIV treatment.
While this is a promising approach, currently we only have limited theoretical understanding. The
approach can be formulated in Contextual Markov Decision Process (CMDP) framework (Hallak
et al., 2015). Recently, there is a line of work gave provable guarantees for CMDP (Abbasi-Yadkori
& Neu, 2014; Hallak et al., 2015; Dann et al., 2018; Modi et al., 2018; Modi & Tewari, 2019). These
work all studied tabular MDPs, and use function approximation, e.g., linear functions, generalized
linear models, etc, to model the mapping from the context to the probability transition matrix. A
major drawback of these work is that they are restricted to the tabular setting and thus can only deal
with discrete environments. Therefore, they can hardly model real-world continuous control tasks,
like the task of pushsing an object as we described above. A natural question arises:
1
Under review as a conference paper at ICLR 2020
Can we design a provably efficient decoder for continuous control problems?
In this paper, we make an important step towards answering this question. We study the fundamental
setting in continuous control, linear quadratic regulator (LQR). LQR is arguably the most widely
used framework in continuous control, as LQR easily models real world physical phenomena, e.g.,
the pushing object task we described earlier. We propose a new algorithm that builds a decoder,
so that for a new LQR task, the decoder takes LQR’s context and outputs a representation based
on which the agent can easily infer a near-optimal policy for new continuous control tasks. In the
training phase, we build the decoder via a sequence of LQRs (in an online fashion) with unknown
parameters. For each new task, we first use the current decoder to build the representation of this
task, infer a policy based on this representation and use this policy to do control for this episode.
There are two crucial components in our algorithm. First, after each episode, we will refine the
estimate of the decoder based on the observations from this episode. Second, it is crucial to use a
upper confidence bound (UCB) estimator of the decoder to build the representation so that the agent
can perform a near-optimal trade-off between exploration and exploitation. In this way, we provably
show the decoder improves the performance as it experiences more training tasks. Formally, we
show our algorithm enjoys O (√T) regret (the difference between the cumulative rewards of our
algorithm and the unknown optimal policy on every seen environment) bound in the online setting.
Moreover, the algorithm is able to obtain an -suboptimal policy for an unseen LQR environment
after playing Oe -2 environments. To our knowledge, this is the first provably efficient algorithm
that builds a decoder for continuous control environments. Empirically, we simulate several physical
environments to illustrate the effectiveness of our algorithm.
Organization This paper is organized as follows. In Section 2, we discuss related work. In
Section 3, we formally describe the problem setup. In Section 4, we present our algorithm and
its theoretical guarantees. In Section 5, we use simulation on physical environments to demonstrate
the effectiveness of our approach. We conclude in Section 6 and defer most technical proofs to the
appendix.
2	Related Work
Recently there is a large body of literature focusing on learning for control in LQR systems. The
first work we are aware of is Fiechter (1997) which studies the sample complexity of LQR in the
offline setting. For the online setting, where the agent can only obtain the next state starting from the
present state, the first near-optimal regret bound (O(√T)) is due to Abbasi-Yadkori & SzePesvari
(2011), which studies the learning problem in the infinite-horizon average-case cost setting. Later
on, a sequence of papers (Tu & Recht, 2017; Dean et al., 2017; 2018; Tu & Recht, 2018; Abbasi-
Yadkori et al., 2018; Cohen et al., 2019) studied this problem in similar settings, improved efficiency
of the algorithms and characterized the gap between model-free and model-based approaches.
Building an agent that quickly adapts to new environment has received increasing interest in the
machine learning community. Taylor & Stone (2009) gave a summary for the literature status before
2009. More recently, a sequence of theory papers Lehnert & Littman (2018); Spector & Belongie
(2018); Abel et al. (2018); Lehnert et al. (2019) studied the transferability of reward knowledge,
state-abstraction, and model features for Markov decision processes. Please also refer to references
in paper cited above for more details. There are also some experimental works, e.g., Santara et al.
(2019); Yu et al. (2018); Wu et al. (2018); Gamrian & Goldberg (2018), studying how to transfer
knowledge from seen tasks to unseen tasks. Nevertheless, we are not aware of any study on how to
provably perform continuous control with contexts.
3	Preliminaries
Notations. We begin by introducing necessary notations. We write [h] to denote the set {1, . . . , h}.
d×d
We use Id ∈ R to denote the d-dimensional identity matrix. We use 0d×d0 to represent the all-
zero matrix in Rd×d0. If it is clear from the context, We omit the subscript d X d0. Let ∣∣∙k2 denote
the Euclidean norm of a vector in Rd . For a symmetric matrix A, let kAkop denote its operator
2
Under review as a conference paper at ICLR 2020
norm and λi (A) denote its i-th eigenvalue. Throughout the paper, all sets are multisets, i.e., a single
element can appear multiple times.
Finite Horizon Linear Quadratic Regulator. We now formally define the finite horizon Linear
Quadratic Regulator (LQR) problem. In the LQR problem, there is a state space X ⊂ Rd and a
closed action space U ⊂ Rd0. Suppose we always start from the initial state x1 = xinit ∈ X and
play for H steps. Then at a state xh ∈ X, if an action uh ∈ U is played, the next state is given by
xh+1 = Axh + Buh + wh+1,	(1)
where A, B are matrices of proper dimension and wh+1 is a zero-mean random vector. Here A, B
can be viewed as the succinct representation of this LQR because as will be explained below, given
A, B, we can easily infer the optimal policy for this LQR. For simplicity, we additionally denote
M = [A, B], and yh = [xh> , uh>]> ∈ Rd+d0 .
Now the state transition can be rewritten as xh+1 = Myh + wh+1. For the ease of presentation, we
assume that the covariance matrix of noise vector wh+1 is E(wh+1wh>+1) = Id. Our analysis follows
similarly if the covariance matrix is not Id (See e.g. Remark 3 of Abbasi-Yadkori & Szepesvari
(2011)). After each step, the player receives an immediate cost xh>Qhxh + uh>Rhuh, where Qh, Rh
are positive definite (PD) matrices of proper dimensions. Throughout the paper, we assume the
agent knows Qh and Rh for all h. At a terminal state xH, there is no action to be played, and the
player receives a terminal cost x>HQHxH, where QH is a PD matrix of proper dimension. The goal
of the player is to find a policy ∏ : (XXU )* ×X → U, which is a function that maps the trajectory
{(xi, ui)}ih=-11 ∪ {xh} to the next action uh, such that the following objectives are minimized:
Jhπ(M,x) := E	xh>Qhxh + uh>Rhuh + x>H Qf xH xh = x ,
h0=h	h∈[H]
where the action uh is given by uh = π[(x1, u1), (x2, u2), . . . , (xh-1, uh-1), xh], and the expecta-
tion is over the randomness of wh and π .
It is well-known that the optimal policy ∏ is Markovian Puterman (2014), i.e., it only depends on
the present state. For an unconstrained action space U, we have
∀x ∈ X,h ∈ [H — 1] : ∏h(M,x) := Kh(M)x
where M = [A, B] and Kh (M) is a matrix that will be defined shortly. It is also known (see e.g.
Bertsekas (1996)) that the optimal cost function Jh(X) = Jh* (x) is given by
Jh(M,x) := XTPh(M)x + Ch(M) =inf Jh(M,x)
π
where
P M	Qh + ATPh+1(M)A - ATPh+1B(Rh + BTPh+1(M)B)-1BTPh+1(M)A
Ph(M) = QH
and
Ch (M) =
We now define Kh(M) as
Kh(M) := -(Rh +BTPh+1(M)B)-1BTPh+1(M)A.
(2)
h<H
h=H
(3)
(4)
Ch+1(M) + Ewh+1 whT+1Ph+1(M)wh+1] h < H
0	h=H.
Note that the optimal value Equation (2) satisfies Bellman equations,
∀h ∈ [H — 1] :	Jh (M, x) = x>Qh,x + π*(x)>Rh∏*(x) + E[ Jh+ι(Ax + Bπ*(x) + w)]
and
∀h ∈ [H — 1] :	Jh(M, x) = x>Qhx + minE[u>Rhu + Jh^+i(Ax + Bu + w)].
Now we have shown that ifwe are given A and B, then we can obtain the optimal policy directly. In
this paper, we deal with setting where A and B are unknown and we need to use decoder to decode
A and B from the contexts of the current LQR, as specified below.
3
Under review as a conference paper at ICLR 2020
Learning to Control LQR with Contexts In the continuous control with contexts setting, in each
episode we observe a context
(C, D)〜μ,
where μ is a distribution on Rp ×d X Rp0×d0. The context [C, D] encodes the information of the
environment. Formally, the representation ([A, B]) of this environment can be decoded from the
context via a decoding matrix Θ* ∈ Rd×(P+pO):
[A,B] = Θ* J J, 0Dd0 ]-	(5)
0p0 ×d	D
From now on, to emphasize that the representation of LQR can be decoded from Θ*, we write
Me*。。=θ* ^ [ 0pC:d 0Dd0 ] = [A,B]∙	(6)
If it is clear from the context, we ignore [C, D] for notational simplicity. Note the optimal decoder
Θ* is unknown to the agent and the goal is to learn Θ* from contexts and interactions with the
environment. Below we formally define the problem that we study.
Definition 3.1 (Contextual Transfer Learning Problem). Build an agent that plays on K LQR games
(one trajectory per game) with context pairs {(C ⑴，D(I)), (C ⑵，D(2)), ∙ ∙ ∙, (C(K), D(K))}〜μ,
for some integer K ≥ 0 such that for another new context pair (C, D) 〜μ, the agent outputs a
policy π based on (C, D) which satisfies
E[Jπ(MΘ*,C,D, x1) - Jh(MΘ*,C,D, x1)] ≤ e
for some given target accuracy > 0.
Here K is the sample complexity which ideally scales polynomially with e and problem-dependent
parameters. The performance of the agent can also be measured by regret, as defined below.
K
Regret(KH) := ^X Jn (MΘ*,C(k),D(k), xl) - J1 (MΘ*,C(k) ,D(k) ,xl),	⑺
k=1
where πe(k) is the policy played at episode k by the agent. This quantity measurse the sub-optimality
of policies the agent played in the first K episodes.
Remark 3.1. We consider matrix-type linear maps from context to the representation only for sake
of presentation. Our algorithm and analysis can be readily extended to other linear maps, e.g.,
[A* (C), B* (D)] := f(C, D) for some unknown IinearfunCtion f.
4 Main Algorithm
In this section, we first describe the algorithm and then present its sample complexity guarantees.
Since the decoder is linear, a straightforward algorithm is first to estimate (A, B) using the trajectory
from the single episode with system identification techniques (Mehra, 1974), and then to use linear
regression to estimate the mapping from (C, D) to (A, B). However, this naive algorithm has two
drawbacks. First, estimating (A, B) accurately requires a long horizon. However, in our setup,
we do not have any restrictions on the horizon. To fix this, we stack contexts and observations to
construct a more direct estimate on the decoder (cf. Equation equation 8). Second, in order to achieve
√T-type regret guarantee, one needs to balance exploration and exploitation carefully, but the naive
algorithm does not have such an component. Our algorithm uses UCB to construct a confidence set
which helps balance exploration and exploitation.
Algorithm We describe the high-level idea of the algorithm below. The agent maintains a de-
coder that maps the context (C, D) to the representation (A, B). We denote Θ(k) the decoder at
the k-th episode. Initially, we know nothing about Θ*, so we initialize our decoder by setting
Θ(1) = 0 ∈ Rd×p. At the k-th episode, the agent plays policy π(k) and in each time step h ∈ [H-1],
it collects data
x(hk)
(k)
uh
(k)
h+1,
C(k)x(hk) ]
D(k)u(hk)
x
J
4
Under review as a conference paper at ICLR 2020
Algorithm 1 Linear Continuous Control with Contexts
1:	Input Total number of episodes K ;
2:	Initialize Θ⑴一0 ∈ Rd×2p, V⑴―I2p,2p, W⑴0 ∈ R2p×d;
3:	for episode k = 1, 2, . . . , K do
4:	Let x1k) — Xinit, V(k+1) — V(k), W(k+1) — W(k);
5:	Obtain context [C(k), D(k)]〜”;
6:	Solve for the present policy:
θ (k)=arg eminkJ； (Me。”。3
(10)
7:
8:
9:
10:
11:
12:
13:
14:
15:
where J； is given by Equation 2, and C(k) is defined in Equation 11;
for stage h = 1, 2, . . . , H - 1 do
Let the current state be x(hk) ;
Play action Uhk) J Kh (M0(k)°(k)。(矽)∙ Xhk, where Kh is defined in Equation 4;
Obtain the next state x(hk+)1;
(k)	C(k)X(hk)
Let zh J	h ;
h	D(k)u(hk)
Update: V (k+1) J V(k+1) + zh(k) zh(k)>;
Update: W (k+1) J W (k+1) + zh(k) X(hk+)1>;
Compute Θ(k+1)> J V(k+1)-1W(k+1);
16:	output Θe (k) where k is chosen from [K] uniformly at random.
(k)
where zh can be viewed as the context regularized observation. We now describe how to obtain
policy π(k). We first solve the following optimization problem,
Θ(k) = arg &n Ji (Μθ,c(k) ,D(k, XIk))
where Ji is given by Equation (2), and the confidence set CIk) will be defined shortly. CIk) represents
our confidence region on Θi . Since we choose the one that minimizes the cost, this represents
the principle “optimism in the face of uncertainty” and it is the key to balance exploration and
exploitation which will be more clear in the proof. Notice that the above optimization problem is a
polynomial optimization problem. Then the policy is given by
∏hk)(x) := Kh(M(k)) ∙ X where M(k) = Mθ(k),c(k),D(k) ：= Θ(k) ∙	COk) 蒜),
and Kh is given by Equation (4). After episode k ∈ [K], we use the following ridge regression
formulation to update decoder
>
Θ(k)
W (k+1)
(8)
where
k H-1	k H-1
V (k+1) = I + X X zh(k0)zh(k0)> and W(k+1) = X X zh(k0)X(hk+0)1>.
k0=1 h=1	k0=1 h=1
After playing K episodes, the algorithm outputs a Θe by picking one from {Θe (k)}k∈[K] uniformly at
random. Now for a new task with its context, our learned policy map is given by:
∀C,D 〜μ,x ∈ X,h ∈ [H - 1]:	ec,D,h(x) = Kh (θ ∙	C D ) ∙ x. (9)
The formal algorithm is presented in Algorithm 1.
5
Under review as a conference paper at ICLR 2020
4.1	Algorithm Analysis
To present the analysis of the algorithm, we first introduce some assumptions.
Assumption 4.1. The contexts and LQR satisfy the following properties.
•	∀h ∈ [H], kPh (M)k2 ≤ cq for some parameter cq > 0.
•	∣∣Θ*∣∣F ≤ CΘ；
•	∀h ∈ [2, H],i ∈ [d] :	Ilwhk2 < ∞ and∀γ > 0, E[γwh,i] ≤ exp(γ2cW/2);
•	∀x ∈ X,u ∈ U, (C, D) ∈ supp(μ) : IlCxk2 + IIDuk2 ≤ Cx，∣∣x∣2 + ∣∣u∣∣2 ≤ Cx；
•	∀(C,D) ∈ supp(μ),x ∈ X,h ∈ [H]: Kh(Mθ*,c,D) ∙ x ∈U.
where CΘ , Cw , Cx are some positive parameters.
The first assumption is standard to ensure controllability. The second is a regularity condition on the
optimal decoder Θ*. The third assumption imposes almost sure boundedness of the noise w. The
fourth assumption is a regularity condition on the observation. The last assumption guarantees the
optimal controller for the unconstrained LQR problem is realizable in our control set U. Given these
assumptions, We are now ready to define confidence set C(k) as follows.
Ik) = {㊀：tr[(θ - Θ(k))V(k)(θ - Θ(k))>] ≤ β(k),
and Vh ∈ [H], (C,D) ∈ supp(μ), ∣∣Ph(Mθ,c,d)∣] ≤ Cq},	(11)
where Ph is given by Equation (3) and β(k) is defined as follows,
β(k) = 9㊀ + Cw q，2d( log d + P log(1 + kHcx∕p)∕2 + log δ-1)) .	(12)
We remark thatC(k) is changing at every episode because we update Θ(k) and V(k) at every episode.
The size of C(k) is decreasing because V(k) is strictly increasing at every episode.
With the above assumptions, the guarantee of Algorithm 1 is formally presented in the next theorem.
Theorem 4.1. Suppose we run Algorithm 1 for
CH	∙ dp2 ∙ log3(dKδ-1)
H,cq ,cx ,cΘ ,cw
K ≥ -----------------2--------------
2
episodes, for some parameter C0H,c ,c ,c ,c depending polynomially on H, Cq, Cx, CΘ, Cw, Then with
probability at least 1 - δ, we have for πeC,D be defined in Equation 9.
QE	[~E JnC,d([Θ*C, Θ*D],xι)) — J∏[Θ*C, Θ*D],xι)] ≤ e.
(13)
Theorem 13 states after playing polynomial number of episodes, our agent can learn a decoder Θ
such that given a new LQR with contexts (C, D), this decoder can turns the contexts into a near-
optimal policy πeC,D without any training on the new LQR. Note this is the desired agent we want
to build as described in the introduction. We emphasize again that this is the first provably efficient
algorithm that builds a decoder for continuous control environments.
Remark 4.1. Via similar analysis, it is easy to show that if the output Θ is picked uniformly at
random from {Θ(k)}k∈[K], the policy achieves similar accuracy.
In fact, Theorem 4.1 is implies by the following regret bound of our algorithm.
Proposition 4.1. With probability at least 1 - δ,
Regret(KH) ≤ CH ∙ d1/2P ∙ log3/2(dKHCxST) ∙ √KH.
where C0H is a constant depending only polynomially on H, Cq , Cx , CM , Cw.
By the definition of regret, this proposition justifies that the performance of the agent actually im-
proves as it sees more environment.
6
Under review as a conference paper at ICLR 2020
5 Experiments
In this section, we validate the effectiveness of our algorithm via numerical simulations.
We perform experiments on a path-following task. In this task, we are given a trajectory
z；,zg,...,zH ∈ R2. Our goal is to exert forces u1,u2, ...,um ∈ R2 on objects with different
(measurable) masses to minimize the total squared distance PH=I ∣∣z⅛ - zM∣2 + IluiIl2∙ Each state
xh = [zh ; vh] ∈ R4 is a vector whose first two dimensions represent the current position and the last
two dimension represent the current velocity. In each stage h, we may exert a force uh ∈ R2 on the
object, which produces an accelerations uh ∈ R2. The dynamics of the system can be described as
zh+1 = zh + vh
∖vh+ι = k ∙ Vh + uh/m
(14)
where 0 < k ≤ 1 is the decay rate of velocity induced by resistance. In our setting, the decay rate
of velocity k is fixed (encoded in Θ*), where the mass of the object m is drawn from the uniform
distribution over [0.1, 10]. In our experiments, we set the noise vector wh in the dynamics of the LQR
system (cf. Equation 1) to be a Gaussian random vector with zero mean and covariance 10-4 ∙ I. In
each episode, we receive an object with mass m where m is draw from the uniform distribution over
[0.1, 10], train one trajectory using that object, and the goal is to recover the physical law described
in Equation 14 so that our model can deal with objects with unseen mass m. Please see Appendix B
for the concrete value of Θ*, Q and R and the distribution of C and D.
In our experiments, we use 100 different masses as training masses (fixed among all experiments),
and use 100 different masses as test masses (again fixed among all experiments). All the training
masses and test masses are drawn from the uniform distribution over [0.1, 10]. We implement a
practical version of Algorithm 1. In particular, instead of solving the optimization problem in Equa-
tion 10 exactly, we sample 100 different Θ from C(k) uniformly at random, and choose the Θ which
minimizes the objective function. Moreover, instead of using the theoretical bound for β(k) in Equa-
tion 12, we treat β(k) as a tunable parameter and set β(k) = 104 in our experiments to encourage
exploration at early stage of the algorithm. We use two different metrics to measure the accuracy
of the learned model. First, We use ∣∣Θk - Θ* ∣f where Θk is calculated in Line 15 to measure the
accuracy of the learned Θ. Moreover, using the learned Θ, we test on 100 objects whose masses are
the 100 test masses to calculate the control cost PH=I Ilzh - Zh k2 + IluiIl2. We compare the control
cost of the learned Θ and the optimal control cost, and use the mean value of the differences (named
mean control error) to measure the accuracy.
In all experiments we fix H = 20. We use three different types of trajectories: unit circle, parabola
y = x2 with x ∈ [0, 1] and Lemniscate of Bernoulli with a = 11. For all three types of trajectories
we use their parametric equation x = x(t) and y = y(t), divide the interval [0, 1] evenly into H
parts, and set t to be the endpoints of these parts. We use these t values to define the trajectory
z1h, z2h, . . . , zHh ∈ R2. We set the decay ratio k to be k = 1 or k = 0.7 in our experiments.
We plot the accuracy of the learned model in Figure 1. Here we vary the number of training episodes
(the number of training masses) and observe its effect on the accuracy. It can be observed that our
algorithm achieves an satisfactory accuracy using only 5 episodes. We also illustrate trajectories
obtained by our resulting controllers in Figure 2. From Figure 2, it is clear that as the agent plays
more environments, it can enjoy better performance.
6 Conclusion
In this paper, we give a provably efficient algorithm for learning LQR with contexts. Our re-
sult bridges two major fields, learning with contexts and continuous control from a theoretically-
principled view. For future work, it is interesting to study more complex settings, include non-linear
control. Another interesting direction is to design provable algorithm in our setting with safety
guarantees (Dann et al., 2018).
1https://en.wikipedia.org/wiki/Lemniscate_of_Bernoulli.
7
Under review as a conference paper at ICLR 2020
D 5 O 5
2 110
. . ■ .
Oooo
*①—①=
0.00
2	4	6	8
Number of Episodes K
,JOt山-otuoυueə5
0.0
2	4	6	8
Number of Episodes K
10
Figure 1: ∣∣Θ - Θ* “ and Mean Control Error.
k = 0.7, m = 1.0
ι.o-
0.8-
0.6-
0.4-
0.2-
0.0-
0：0	0：2	0：4	0：6	0：8 LO
k = 0.7, m = 1.0
1.00-
0.75-
0.50-
0.25-
o.oo-
-0.25-
-0.50-
-0.75-
-1.00-
-L00 -0：75 -0：50 -0：25 0.00 0.25 0.50 0.75 1.00
k = 0.7, m = 1.0
0.4-
0.2-
0.0-
-0.2-
-0.4-
-1.5	-1.0	-0.5	0.0	0.5	1.0	1.5
Figure 2: Example trajectories produced by the LQR controllers. We test the LQR policy to follow
three types of paths: parabola, circle, and lemniscate. We first train a decoder, then test it on systems
with m = 0.1, k = 0.7 (left column), and m = 1.0, k = 0.7 (right column). Dashed line with
circles:target trajectories. ?: optimal policy. ◦: decoder trained by 1 iteration on randomly drawn
contexts. 4: decoder trained by 3 iterations on randomly drawn contexts. ×: decoder trained by 10
iterations on randomly drawn contexts.
8
Under review as a conference paper at ICLR 2020
References
Yasin Abbasi-Yadkori and Gergely Neu. Online learning in mdps with side information. arXiv
preprint arXiv:1406.6812, 2014.
Yasin Abbasi-Yadkori and Csaba Szepesvari. Regret Bounds for the Adaptive Control of Linear
Quadratic Systems. Technical report, 2011. URL http://proceedings.mlr.press/
v19/abbasi- yadkori11a/abbasi- yadkori11a.pdf.
Yasin Abbasi-Yadkori, Nevena Lazic, and Csaba Szepesvari. Regret bounds for model-free linear
quadratic control. arXiv preprint arXiv:1804.06021, 2018.
David Abel, Dilip Arumugam, Lucas Lehnert, and Michael Littman. State abstractions for lifelong
reinforcement learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th In-
ternational Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 10-19, Stockholmsmassan, Stockholm Sweden, 10-15 JUl 2θ18. PMLR. URL
http://proceedings.mlr.press/v80/abel18a.html.
Dimitri P Bertsekas. Dynamic programming and optimal control. Journal of the Operational Re-
search Society, 47(6):833-833, 1996.
Alon Cohen, Tomer Koren, and Yishay Mansour. Learning linear-quadratic regulators efficiently
with only √T regret. arXiv preprint arXiv:1902.06223, 2019.
Christoph Dann, Lihong Li, Wei Wei, and Emma Brunskill. Policy certificates: Towards accountable
reinforcement learning. arXiv preprint arXiv:1811.03056, 2018.
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the sample com-
plexity of the linear quadratic regulator. arXiv preprint arXiv:1710.01688, 2017.
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu.	Re-
gret bounds for robust adaptive control of the linear quadratic regulator. In
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems 31, pp. 4188-
4197. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
7673- regret- bounds- for- robust- adaptive- control- of- the- linear- quadratic- regulator.
pdf.
Claude-Nicolas Fiechter. Pac adaptive control of linear systems. In Annual Workshop on Computa-
tional Learning Theory: Proceedings of the tenth annual conference on Computational learning
theory, volume 6, pp. 72-80. Citeseer, 1997.
Shani Gamrian and Yoav Goldberg. Transfer learning for related reinforcement learning tasks via
image-to-image translation. arXiv preprint arXiv:1806.07377, 2018.
Assaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual markov decision processes. arXiv
preprint arXiv:1502.02259, 2015.
Taylor Killian, George Konidaris, and Finale Doshi-Velez. Transfer learning across patient varia-
tions with hidden parameter markov decision processes. arXiv preprint arXiv:1612.00475, 2016.
Lucas Lehnert and Michael L Littman. Transfer with model features in reinforcement learning.
arXiv preprint arXiv:1807.01736, 2018.
Lucas Lehnert, Michael J Frank, and Michael L Littman. Reward predictive representations gener-
alize across tasks in reinforcement learning. BioRxiv, pp. 653493, 2019.
Raman Mehra. Optimal input signals for parameter estimation in dynamic systems-survey and new
results. IEEE Transactions on Automatic Control, 19(6):753-768, 1974.
Aditya Modi and Ambuj Tewari. Contextual markov decision processes using generalized linear
models. arXiv preprint arXiv:1903.06187, 2019.
Aditya Modi, Nan Jiang, Satinder Singh, and Ambuj Tewari. Markov decision processes with con-
tinuous side information. In Algorithmic Learning Theory, pp. 597-618, 2018.
9
Under review as a conference paper at ICLR 2020
Martin L Puterman. Markov Decision Processes.: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, 2014.
Anirban Santara, Rishabh Madan, Balaraman Ravindran, and Pabitra Mitra. Extra: Transfer-guided
exploration. arXiv preprint arXiv:1906.11785, 2019.
Benjamin Spector and Serge Belongie. Sample-efficient reinforcement learning through transfer and
architectural priors. arXiv preprint arXiv:1801.02268, 2018.
Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey.
Journal of Machine Learning Research ,10(JUl):1633-1685, 2009.
Stephen Tu and Benjamin Recht. Least-squares temporal difference learning for the linear quadratic
regUlator. arXiv preprint arXiv:1712.08642, 2017.
Stephen TU and Benjamin Recht. The gap between model-based and model-free methods on the
linear qUadratic regUlator: An asymptotic viewpoint. arXiv preprint arXiv:1812.03565, 2018.
Yi WU, YUxin WU, Aviv Tamar, StUart RUssell, Georgia Gkioxari, and YUandong Tian. Learning and
planning with a semantic model. arXiv preprint arXiv:1809.10842, 2018.
Lin F Yang and Mengdi Wang. Reinforcement leaning in featUre space: Matrix bandit, kernels, and
regret boUnd. arXiv preprint arXiv:1905.10389, 2019.
Yang YU, Shi-Yong Chen, Qing Da, and Zhi-HUa ZhoU. ReUsable reinforcement learning via shallow
trails. IEEE transactions on neural networks and learning systems, 29(6):2204-2215, 2018.
10
Under review as a conference paper at ICLR 2020
A Proof of Main Results
This sections devotes to proving the main results. Before we prove Proposition 4.1, let us use it to
prove Theorem 4.1.
Proof of Theorem 4.1. We rewrite the Equation equation 13 as follows.
EC,DEn [Jπ(MΘ*,C,D, xl)] - EC,D [Ji(MΘ*,C,D, x1i
1K
=K X EC,D [Jπ (MΘ*,C,D, xl)] - EC,D [Ji(MΘ* ,C,D, xl)]
1K	k	k	k
=K X (EC,D [J1 (MΘ*,C,D, xl)] - J1 (MΘ*,C(k),D(k), xl) + JI(MΘ*,C(k),D(k , xl)
k=1
-JT (MΘ*,C(k),D(k), xl) + JT(MΘ*,C(k),D(k), x1)-EC,D [Jτ(MΘ*,C,D, xl)])
= R1 + R2 + R3
where
R1
1K
K X (EC,D [J： (MΘ*,C,D, xl)] - Jn (MΘ*,C(k),D(k), x1
k=1
and
R2
1K
K X (JI(Mθ*,c(k),D(k), χl) - EC,D [J^Γ (MΘ*,C,D, xl)]}
k=1
R3
1K
K X (Jn (MΘ*,C(k),D(k), xl) - J1 (MΘ* ,C(k),D(k), x1
k=1
Let Fk be the filtration of fixing all randomness before episode k. We have R1 and R2 are Martingale
difference sum. Note that the magnitude of each summand in R1 or R2 is upper bounded by (proved
in Lemma A.3 and A.4),
Hcqcx
almost surely. Therefore, by Azuma’s inequality (Theorem A.1), we have, with probability greater
than 1 - δ∕2,
|Ri| + ∣R2∣ ≤ 2HcqCx ∙ j2lo%∕δ).
Moreover, by Proposition 4.1, we have with probability greater than 1 - δ∕2,
∣R3∣ ≤ C ∙ d1/2p ∙ log3/2(dKHcXδ-1) ∙，H,
where c is constant depending only polynomially on H, cq , cx , cM, and cw . Combining the above
two inequalities, and setting K appropriately, we complete the proof of Theorem 4.1.
□
A. 1 Useful Concentration Bounds
Before we prove the main proposition, we first recall some useful concentration bounds.
Theorem A.1 (Azuma’s inequality). Assume that {Xs}s≥0 is a martingale and |Xs - Xs-1 | ≤ cs
almost surely. Then for all t > 0 and all > 0,
Pr |Xt - X0| ≥	≤ 2exp
-2
2 PS=ι c2∕
11
Under review as a conference paper at ICLR 2020
Theorem A.2 (Martingale Concentration, Theorem 16 of Abbasi-Yadkori & Szepesvari (2011)). Let
Ft; t ≥ 0 be a filtration, (zt; t ≥ 0) be an Rd-valued stochastic process adapted to (Ft). Let (ηt; t ≥
1) be a real-valued martingale difference process adapted to Ft. Assume that ηt is conditionally
sub-Gaussian with constant L, i.e.,
∀γ > 0	: E[γηt∣Ft] ≤ exp(γ2L2∕2).
Consider the following martingale
t
St =	ητ zt-1
τ=1
and the matrix-valued processes
t
Vt = I + X zt-1zt>-1 .
τ=0
Then for any δ ∈ (0, 1), with probability at least 1 - δ,
∀t ≥ 0, kStk V-ι ≤ 2L2 log (det(?”2)
where kStk2V-1 :=St>Vt-1St.
A.2 Proof of Proposition 4.1
In this section, we prove the main proposition. We first bound det(V (k)) for any k.
Lemma A.1. For all k ∈ [K],
det(V(k)) ≤(1 + kHcX∕p)p.
Proof. Since V (k) is PD, we have,
p	k H-1	p
det(V (k)) ≤ tr(V (k))∕pp ≤ 1+ X X zh(k0)22∕p .
k0=1 h=1
By Assumption 4.1, we have Ilzhk) k2 ≤ c2c. This completes the proof.	□
Let us then define an event Ek as follows.
Definition A.1 (Good Event). We define event Ek as {∀k0 ≤ k : Θ* ∈ C(k0)}.
We then show that the event Ek happens with high probability.
Lemma A.2. For all k ∈ [K], we have Pr[Ek] ≥ 1 - δ.
Proof. Now we consider Θ* — Θ(k). We immediately have
k H-1
Θ> - Θ(k)> = θ> -(V⑻尸(X X zhk0)(θ,zhk0) + wh+1)>)
k0=1 h=1
k H-1	k H-1
= (I-(V(k))T X X zhk0)zhk0)>)Θ> + (VIk))T X X zhk%h+T.
k0=1 h=1	k0=1 h=1
Next, we have
(θ* - Θ(k))v(k)(e* - Θ(k))>
k H-1	>	k H-1
=θ* (I-(V (k))-1 XX 铲zh") v(k) (I - (V (k))-1 XX ZhkO)铲>)θ>
k0=1 h=1	k0=1 h=1
12
Under review as a conference paper at ICLR 2020
k H-1	> k H-1
+ Θ*(I - (VIk))T X X Z沪Zh)>)	X X zhk0)wh+1>
k0=1 h=1	k0=1 h=1
k H-1	k H-1
+ XX wh+1zhk0)> I- - (V (k))-1 X X zhk0)zhk0)>) Θ>
k0=1 h=1	k0=1 h=1
k H-1	k H-1
+XXwh(k+0)1Zh(k0)>(V(k))-1XXZh(k0)wh(k+0)1>
k0=1 h=1	k0=1 h=1
Note that Pkk0=1 PhH=-11 Zh(k0)Zh(k0)> = V (k) - I and thus (V (k))-1 Pkk0=1 PhH=-11 Zh(k0)Zh(k0)> =
I - (V(k))-1. Hence we have
tr[(Θ* - Θ(k))V(k)(Θ* - Θ(k))>]
k H-1	k H-1	2
=kΘ*k2v(k))-ι +2tr(θ*(V(k))-1 X X zhk')wh+T) + IlXX zhk0)wh+TII(V(k))-ι
k0=1 h=1	k0=1 h=1
k H-1	k H-1	2
≤ kθ*k2v (k))-ι+2∣∣θ*∣∣ (V (k))-ill X X zhk%h+T∣∣(v (k))-ι + Il X X 染 )wh+T∣∣(v (k))-ι
(	) k0=1 h=1	(	)	k0=1 h=1	(	)
where kXk2V := tr X>VX and the last inequality uses Cauchy-Schwartz inequality. Notice that
kθ*∣∣(V(k))-ι ≤ kθ*kF∙
Moreover, we have
k H-1
III X X Zh(k0)wh(k+0)1>
k0=1 h=1
k H-1
(V(k))-1/2XXZh(k0)wh(k+0)1>
k0=1 h=1
I	k H-1
X II(V (k))-1/2 X X wh(k+0)1,jZh(k0)
j∈[d]
k0=1 h=1
2
2
F
2
2
k H-1
X III X X wh(k+0)1,jZh(k0) III
j∈[d]	k0=1 h=1
2
By Theorem A.2, We have, for every j ∈ [d], with probability at least 1 - δ∕d,we have,
k H-1
III X Xwh(k+0)1,jZh(k0)III
k0=1 h=1
2
By an union bound, we have, with probability at least 1 - δ,
k H-1
III X X wh(k+0)1Zh(k0)
k0=1 h=1
2
Plugging to tr[(Θ* - Θ(k))V(k)(Θ* - Θ(k))>], we have, with probability at least 1 - δ,
tr[(Θ* - θ(k,V(k) (Θ* - Θ(k'>] ≤ (cθ + Cw J2d log(d det(V (k))1/2/6))2
≤ k㊀ + Cw y2d( log d + p log(1 + kHcX∕p)∕2 + log δ-1)).
This completes the proof.
□
We define IEK as the indicator for EK happens. We denote
My) = Mθ*,c(k),D(k, M(k) = Mθ(k),c(k) ,D(k),	and yhk) = [χhk)>,uhk)>]>.
13
Under review as a conference paper at ICLR 2020
On Ek, we have
Vk ∈ [κ] ：	J*(f(k),χk) ≤ j；(MyLxk)∙
We denote ∆(k) := Jhk (My),xι) - Jh(My),xι). We can rewrite equation 7 as
K	K
Regret(KH) = XIEk△(&)+ X(I- IEk)△(?
where the second term is non-zero with probability less than δ. For the first term, we have
IEk∆(k) ≤ IEk [JΓk(My),x1)- J*(f (Rx1))] =： IEk ∙ ∆ 1k),
where
△ Sk = Jn (My),xh)- J;(M(k,xh).
Let US consider △ h" We denote filtration Fk,h as fixing the trajectory UP to time (k, h) and all
{C (k，),D(k，)}k,<k.	'
We have
△ hk) =xhk)>Qhxhk) + Uhk)TRhUhk) +Ew(k)[Jh+i(M；k),xh+i) I Fk,h]
Wh+1	，	1
-	靖TQhxhk)-Uhk)TRhUhk)
-	Ew(k) [(f(k)Zhk) +wh+I)TPh+i(f(k))(f(k)Zhk) +wh+ι) I Fk,hi
h + 1
-	Ch+i(f(k))
=Ew(k) [Jh+ I(My),xh+ι) ∣Fk,h]
wh+1
-	Ew(k) [(f(k)Zhk) +wh+I)TPh+i(f(k))(f(k)Zhk) +wh+ι) I Fk,hi
h + 1
-	Ch+i(f(k))
=Ew(k) J；ι(M(k),xh+ι) ∣ Fk,h] - Jh+ ι(M(k),xh+ι) + J∏+ i(M；k),xh%)
h+1
-	(f(k)Zhk))TPh+i(f(k))(f(k)Zhk)) - Ch+ι(fhk))
-	Ew群」(wh；I)TPh+i(f(k))wh；i I Fk,h]
=δhk) + J⅛ I(My),xh+ι) - (f(k)Zhk))TPh+1 (f(k))(f(k)Zhk)) - Ch+i(f(k))
-	E	(xh+ι - My)Zhk) )TPh+i(f(k))(xh+i - Mhk)Zhk)) ∣ Fk,h]
=δhk) + Jh； I(My),xh+ι) - (f(k)Zhk))TPh+1 (f(k))(f(k)Zhk)) - Ch+1(Mhk))
-	E* [(xh；i)TPh+i(f(k))(xh；i) ∣ Fk,h] + (Mhk)Zhk))TPh+ι (f(k))(My)Zhk))
=δhk) + δh(k) + δh'(k) + Jh； 1(M*, xh；i) - Jh；i(f(k) ,xh+ι)
where
δhk) = Ew(k) J； i(M；k),xh；i) ∣Fk,h] -Jh； I(Mhk),xh+ι)	(15)
h+1
δhk) = (xh；i)TPh；i(Mhk))(xh；i) - Ew(k) [(xh+1)TPh+1(M(k))(xh+1) ∣ Fk,h]	(16)
h+1
δh(k) = (Mhk)Zhk))TPh；1(Mhk))(Mhk)Zhk)) -(Mhk)Zhk))TPh；1(Mhk))(Mhk)Zhk)).	(17)
By indUction, we have
k	k H-1
X △ 1k) ≤ X X (δhk)+δhhk) + δh0hk)).
k 0 = 1	k 0 = 1 h=1
Notice that δhk) and δ∖> are Martingale difference adapted to Fk,h. We can well bound the sum of
them via Azuma’s inequality.
14
Under review as a conference paper at ICLR 2020
Lemma A.3. For all h ∈ [H], ∖Jπ (Mp),XS))| ≤ (H — h +1) ∙ cq ∙ cx.
Proof. Prove by induction on h. The base case JH(My),xH)) = XH)TQHXH) ≤ CqCx holds
straightforwardly. Consider an arbitrary h < H, we have
Jn (MaXs) )= Xa)TQhXa)+4fc)τR%4fc)+Ew(k) [J∏+ 1(Mp),xh%) ∖Fk,h] ≤ Cq Cx + (H —h)∙Cq ∙cx
wh + 1
as desired.	□
Lemma A.4. For all X ∈ X, we have IEK |J京(M(k),X)| ≤ CqCx.
Proof. Follows from Assumption 4.1.	□
We are now ready to prove Proposition 4.1.
Proofof Proposition 4.1. Thus by Azuma’s inequality, we have, with probability at least 1 — δ,
k H-1	I-----------------------------------
IXX δ(k)∣≤√2kH∙ [(H — h + 1)qCx + CqCx]2 ∙ log -.
k0=1 h=1	V
And, with probability at least 1 — δ,
∣ x xδ产)। ≤ r8kH ∙CC ∙ ι0g-.
k0 = 1 h=1	V
For P δjk), we bound it here.
k H-1	k H-1	k H-1
I x x δ*)∣ ≤ X X ∣δ)k)∣ = XX IkPh+1(MW)1∕2(MW Sk2 - kPh+1(M(k))1/2(M * 靖川
k0=1 =1	k0=1 =1	k0=1 =1
k H-1
≤ X X ∣(kPh +1(MW)"(MWy(k))k2 - kPh+1(M(k))1/2(M*y(k))k2)
k0=1 h=1
•(kPh+1(M(k))1/2(M(k)y(k) )k2 + kPh+1(M(k))1/2(M *y(k))k2)∣
≤[∑2 I-1 (kPh+1(M(k))1/2(M(k)y(k))k2 - kPh+1(M(k))1/2(M*y(k))k2)T/2
k0=1 h=1
•[IL I-1 (kPh+1(M(k))1/2(M(k)yhk))k2 + kPh+1(M(k))1/2(M*yhk))k2)T/2
k0=1 h=1
Notice that ∣∣Ph+1 (M(k))1/2(M(k)yhk))k2 ≤ CqCxcθ and ||Ph+1(M(k))1/2(M*或))口2 ≤ CqCx.
Hence
[X HXL ∣ (kPh+1(MW)1∕2(MWyhk))k2 + kPh+1(MW)1/2(M*yhk))k2)2『/2
k0=1 h=1
≤ JkH ∙ (CqCx(1 + CΘ))2.
Moreover, by triangle inequality, we have
∣kPh+1(MW)1∕2(M(k)yhk))k2 - ∣∣Ph+1(MW)1∕2(M*yhk))∣2 ∣
≤ kPh+1 (M(k))1/2(M(k) - M*)yh,∣2
≤ Cqk(M(k)- M*)yhk)k2
≤ Cq||(M(k) - M*)(V(k))1/2(V(k))-1/2yhk)k2
15
Under review as a conference paper at ICLR 2020
≤ Cqk(f(k) — M*)(V(W/2k2k(V(k))T/2yhk)k2
≤ Cq ∙ pβk ∙k(v(k))T∕2yhk)k2.
ByAssUmPtion 2, we also have ∣∣(V(k))-1/2yhk)k2 ≤ k(yhk)k2 ≤ √cX. Hence,
卜|Ph+i(f(k))1/2(f(k)yhk))k2 -kPh+i(f(k))1/2(M*yhk))∣2∣
≤ Cq √cx ∙ PekT • min(k(V (k))T∕2yhk)k2,l)
Combining the above eqUations, we have,
k H-1	_____
∣∣ X X δh00(k)∣∣ ≤
kH ∙ (CqCχ(1 + Cθ))2 ∙ Cq√CX ∙ Pβ(k) •
k0=1 h=1
k H-1
XX min (k(V (k))T2yhk)k2,l)
k0=1 h=1
∖
____ k H-1
≤ 2cX/2c2cθ ・ pβk• t X X log(1 + k(V(k))-"yhk)k2) • √H.
k0=1 h=1
Lastly, by Lemma 8 of Yang & Wang (2019), we have
k H-1
XX
log 1 + ∣(V (k))-1/2yh(k) ∣22 ≤ 2H log det(V (k)).
k0=1 h=1
Together with Lemma A.1, we have
2Hlogdet(V(k)) ≤ 2Hp . log(1 + kHcX∕p).
Overall, we have,
k H-1
∣∣ X X δh00(k)∣∣ ≤
2c3∕2c22 cθ ∙ y2Hp∙l0g(l÷kHcX∕p)∙(β(k))∙kH.
k0=1 h=1
PUtting everything together, with Probability at least 1 - 2δ, we have
K H-1
reg(KH) ≤ X X (δhk) + δh(k) + δ00(k))
k0=1 h=1
≤
2KH • [(H — h + 1)CqCχ + CqCχ] • log δ +
88KH • cχc2 • log 2
+2cx42cθ • y2Hp^πog(TZKHcX∕pyιβ(KrKH
≤ CH • d1/2p • log3/2 (dKHcXδ-1) • √KH,
where CH is a constant dePending on H, Cq , CX , CΘ and Cw .
□
B Concrete Choice of the Parameters
We fUrther aUgment the state so that the first coordinate is a constant with valUe 1. More sPecifically,
we set the state xh = [1; zh; vh] ∈ R5. We set
∣z⅛k2 -Zh 0 ∖
-Zh I	0
0	00
so that for any state xh, xhT QhxH = ∣Zh - Zhh∣22. We set Rh = I with size 2 × 2. We set
1	0	0	0	0	0	0
0	1	0	1	0	0	0
0	0	1	0	1	0	0
0	0	0	k	0	1	0
0	0	0	0	k	0	1
C to be the 5 × 5 identity matrix and D to be I∕m with size 2 × 2 where m is samPled from the
Uniform distribUtion over [0.1, 10], to rePresent the Physical law in EqUation 14.
16