Under review as a conference paper at ICLR 2019
Learning RNNs with Commutative State Tran-
SITIONS
Anonymous authors
Paper under double-blind review
Ab stract
Many machine learning tasks involve analysis of set valued inputs, and thus the
learned functions are expected to be permutation invariant. Recent works (e.g.,
Deep Sets) have sought to characterize the neural architectures which result in
permutation invariance. These typically correspond to applying the same pointwise
function to all set components, followed by sum aggregation. Here we take a dif-
ferent approach to such architectures and focus on recursive architectures such as
RNNs, which are not permutation invariant in general, but can implement permuta-
tion invariant functions in a very compact manner. We first show that commutativity
and associativity of the state transition function result in permutation invariance.
Next, we derive a regularizer that minimizes the degree of non-commutativity in
the transitions. Finally, we demonstrate that the resulting method outperforms
other methods for learning permutation invariant models, due to its use of recursive
computation.
1	Introduction
Many of the successes of deep learning can be attributed to a choice of architecture that fits the
application domain. A key example of this are convolutional neural nets (Krizhevsky et al., 2012)
that employ extensive parameter tying across the image, reflecting the shift invariance of visual
labels. It is clear that such inductive bias is necessary for any domain if one needs to learn from a
limited number of examples. This has prompted much recent interest in the question of modeling
architectures that capture invariances of particular domains.
One such example is the setting where the input to the network is a set of objects and the true output
does not depend on the order of the objects in the set. In this case, the function that we seek to
learn is permutation invariant. It would thus be advantageous to retrict learning to models that are
permutation invariant by design. This was the observation in the work on Deep Sets (Zaheer et al.,
2017), which gave necessary and sufficient conditions for a network to be permutation invariant.
The underlying idea in many of the permutation invariant architectures is to apply the same network
N1 to all items in the set, then sum the outputs and apply another network N2 to the summation.
Clearly this is permutation invariant. Similarly other models using attention (Vinyals et al., 2016; Lee
et al., 2019) are permutation invariant since they only rely on comparisons between elements in the
set.
Although the above approaches results in invariant models, these are not clearly the smallest models
that can compute a given invariant function. For example, consider the problem of computing the
permutation invariant function fpx1, . . . , xnq “ maxi xi. In order to calculate it with a DeepSet
approach, one would need to set N1 pxq “ xk for large k and then N2pzq “ z1{k. This would
approximate the `8 metric and therefore the max function. However, in order to implement these
two functions Ni, N with ReLU networks, one would need O(∣) to achieve E accuracy (e.g., see
Telgarsky, 2017, Lemma 3.4), and thus calculating this seemingly simple function to good accuracy
would take a fairly large network. Clearly, if one uses max as the aggregation function in DeepSets,
this could be avoided, but the same would then be true for other aggregation functions.
On the other hand, the max function can be easily implemented via a simple RNN. Consider an
RNN with the following state update st`i “ max (s~i, Xt) and set s0 “ 一8. Then clearly we
would have sn “ maxi“1 xi . Namely, we will implement the permutation invariant max function.
1
Under review as a conference paper at ICLR 2019
Furthermore, the st`1 state update can be very easily be implemented using a one hidden layer ReLU
network, because max(s, x) “ ReLU(S — x)+ ReLU(X) — ReLU(—x).
The above example demonstrates that RNNs can in some cases be a natural computational model for
permutation invariant functions. The goal of this paper is to ask which RNNs compute permutation
invariant functions, and how they can be learned.
Clearly, most RNNs are not permutation invariant, as their state has the capability of tracking temporal
patterns, and is thus sensitive to changes in ordering. Here we show that by restricting RNN state
updates to commutative-associative functions, RNNs do become invariant, and the resulting set of
functions is very expressive.
After establishing the commutative-associative constraints, we turn to ask how one can learn RNNs
that satisfy the latter constraint. We show that for ReLU activation functions the commutative
constraint corresponds to a closed form regularizer on the parameters of the RNN and suggest
learning with this regularizer in order to achieve commutativity.
We show empirically that learning with commutative regularization leads to architectures that are
permutation invariant in practice and that outperform DeepSets on several benchmarks.
Taken together, our results highlight the importance of the function class used for learning permutation
invariant functions, and the important role of recursive computation for these tasks.
2	Problem Formulation
We consider the problem of learning functions that map a sequence of inputs x1, . . . , xn to an output
y. We note we could have also addressed the problem of generating an output sequence y1, . . . , yn
but use a single output for simplicity.
We say that such a function f(x1, . . . , xn) is permutation invariant if for any permutation π of rns
and for all x inputs we have:
f(x1,.. .,xn) “ f(xπp1q,...,xπpnq)	(2.1)
We next ask: how should an RNN be constructed such that it is guaranteed to be permutation invariant.
We consider standard RNNs, parameterized as follows. Let st P Rr be the state vector at time t.
Then the state update rule is:
st`1 “ Aσ (W ψ(xt ) ` Θst)	(2.2)
where ψ(x) P Rd is a “pre-processing” network (of arbitrary architecture), σ is a pointwise non-
linearity such as ReLU or tanh, and A, W, Θ are matrices which transform to the appropriate
dimensions, specifically, A P RrXk, W P RkXd and Θ P Rkxr. We note that We add a matrix A to
allow the non-linearity to be taken in Rk such that k ‰ r.
Finally, the output is obtained via:
y “ g(st)	(2.3)
where g is any multilayer neural net (the specific architecture will not be important for our derivation).
To simplify the derivation of permutation invariance we make the assumption that the dimension of
the RNN state, and the dimension of the transformed input ψ(x) are the same, i.e. r “ d.1
Next, we introduce the following notation for the RNN state transition:
st`i = Aσ (Wψ(xt) ' Θst) “ ψ(xt) o St	(2.4)
Thus, the o operation takes two vectors in Rd and outputs a vector in Rd. Namely o : Rd, Rd → Rd.2
With this notation it is clear that:
St “(...((S0 O ψ(xι)) O Ψ(x2)) ... O ψ(xt))	(2.5)
We use parentheses to emphasize that the order of applying o does matter and these operations are
neither commutative nor associative for general RNNs.
1It is always possible to achieve by padding either s or x by zeros as necessary.
2For the reminder of this paper we omit the ψ notation as it does not affect our derivations.
2
Under review as a conference paper at ICLR 2019
3	Permutation Invariant RNNs
In this section We relate properties of the state transition operator o to the permutation invariance
properties of the RNN. Without loss of generality we assume that the ψpxq “ x, since this is just a
pre-processing step.
Definition 3.1. An RNN is permutation invariant if the function fpx1, . . . , xnq “ gpstq implemented
by the RNN is permutation invariant.
We begin With basic definitions.
Definition 3.2. The operator o is commutative if for all x, x1 it holds that X o x1 “ x1 o X
Definition 3.3. The operator o is associative iffor all x, x1, X it holds that Px o x1)oX = X o(x1 o X)
Definition 3.4. If an RNNhas a o operator that is both commutative and associative we refer to it as
a Commutative-Associative RNNs.
Theorem 3.5.	If an RNN is Commutative-Associative then it is permutation invariant. Namely st
does not depend on the order ofX1, . . . , Xn.
Proof. The associativity property of the o operator implies that we can remove the parentheses in
Eq. 2.5 and commutativity implies that We can then sWitch the order of the Xi arbitrarily. Thus We
have that for all permutations π :
St “ S0 ɑ Xl ɑ X2 ... ɑ Xt “ S0 ɑ X∏piq ɑ X∏(2) ... ɑ Xnt	(3.1)
Therefore the state is invariant to the input order, and so is the output y.	□
A complementary question is that of universality. Namely, are the above permutation invariant RNNs
sufficiently expressive to capture any permutation invariant function?
Theorem 3.6.	Let fpX1, . . . , Xn) be a permutation invariant function. Then it can be implemented
with Commutative-Associative RNNs.
Proof. Since the function f is permutation invariant, it follows from Theorem 2 in the DeepSets paper
(Zaheer et al., 2017) that there exist two functions φ and P such that f (xi,..., Xn)= P PXi φ(Xi)).
We can now implement this architecture with a Commutative-Associative RNNs as follows: set
ψ = φ,g = ρ and x o x1 = X ` x1. Clearly this implements the DeepSet function and is Commutative-
Associative because o is just addition.	□
We conclude that Commutative-Associative RNNs implement permutation invariant functions and
are also sufficiently expressive to capture all permutation invariant functions.
4	Commutative Regularization
In the previous section we highlighted the importance of having an RNN operator o that is both
commutative and associative. This suggests that in order to learn a permutation invariant RNN
we would like to learn RNNs under the constraint that o is associative and commutative. In this
section and the remainder of the paper, we focus on the latter, namely introducing a constraint (or
equivalently, regularizer) that o is commutative.
Formally, the constraint that o is commutative corresponds to requiring:
x o X1 = X1 o x @x, X1 P Rd	(4.1)
Recall that o depends on the matrices A, W, Θ and thus the above constraint translates to a non-linear
constraint on these three matrices. However, it is not clear how to write this constraint in a way that
facilitates optimization. This will be our goal in what follows.
We begin by replacing requirement 4.1 with a more convenient proxy.
3
Under review as a conference paper at ICLR 2019
Lemma 4.1. If there exists a distribution, D, over Rd, Rd with non-zero density over its support,3
then
Eχ,χi~D [}x 0 X1 ´ X1 O x}2] = 0 ð^ P Ix o x1 “ x1 o x,	@x, X1 P
“1
(4.2)
Proof. The proof relies on the fact that for a continuous non-negative random variable, X, is holds
that (see Section A):
E∣x] = 0 ð^ PrX = 0] = 1	(4.3)
We now use 4.3 in	order to complete the proof. Denote by R the random	variable	RpX, X1q ”
}X O X1 ´ X1 O X}22.	Obviously, R is non-negative by definition. Note also that	for	any pair X, X1 we
have }x o x1 — x1 o	x}2 “ 0 ð^ X o x1 “ x1 o x. It follows that:
E ∣r] “ 0 ð^ PIR = θ] “ 1 ð^ PIX o x1 “ x1 o x,
@X, X1 P Rd “ 1
(4.4)
□
The above lemma already gives the strong result that with probability one the network is commutative.
In fact a stronger result is possible, stating that the network is commutative for all inputs. We only
need to add the restriction of the network A, W, Θ is Lipschitz. This will hold for example, if we
restrict A, W, Θ to any compact set (e.g., see similar argument in Arjovsky et al., 2017).
Lemma 4.2. Assume the network defined by A, W, Θ has a Lipschitz constant bounded by K, and
that Eq. 4.2 holds under the conditions therein. Then @X, X1 it holds that X O X1 “ X1 O X.
Proof. Denote fpX, X1q “ }X O X1 — X1 O X}22. Then f also has a Lipschitz constant bounded by L
that is a function of K. Assume in contradiction that there exists X0 , X10 such that f pX0, X10q “
for some E > 0. Let B(c,r) denote an '2 ball centered at C with radius r. Then because f is
Lipschitz it follows that for all Z P B([xo, x0], 2L) it holds that f (Z)22. Let Y denote the minimal
density value at B([xo, X0], 2L). By assumption on D we have that Y > 0. It therefore follows that
Plf(X)20.5e]》PlZ P B([xo, x0], 2L∣)20.5γe > 0 and therefore Plf(X) = 0] V 1 and we
have a contradiction with Lemma 4.1.	□
As a conclusion of lemma 4.1, we can express E }x O x1 — x1 O x}22 in terms of matrices A, W, Θ
for a specific distribution and derive conditions under which it is equal to zero.
Definition 4.3. Consider an RNN parameterized by matrices A, W, Θ as in 2.2 with ReLU activation.
∆(A, W,⑼三 Eχ,χi~D [}x O X1 — X1 O x}2]	(4.5)
Below we show that ∆(A, W, Θ) can be expressed as a simple function of A, W, Θ when using the
ReLU non-linearity.
Let D be a multivariate Gaussian distribution with zero mean and covariance identity. In order to
calculate ∆(A, W, Θ) we will make use of the following integral (Cho & Saul, 2009):
E∣σ(u ∙	x)σ(v	∙	x)]	“	∏1 }u}2}v}2	(Sin (θu,v) ' (π —	θu,v) Cos (θu,v)) ” g(u,	V)	(4.6)
It turns out that ∆(A, W, Θ) is a simple function of the above g(u, v) function, as stated next.
Let wi denote the ith row of W, θi the ith row of Θ and ai the ith column of A. Also, denote the
following horizontal stacking of the vectors as
ui “ rθi wis	, vj “ rwj θjs	(4.7)
and the matrices with these as columns by U and V respectively.
3e.g. @x, x1 P Rd, fD(x, x1q > 0
4
Under review as a conference paper at ICLR 2019
Theorem 4.4. Assume x, x1 are independent Gaussian vectors, each with a unit covariance. Then:
hh
△(A, W,⑼=£ £ ai ∙ aj (g(ui, Uj) — 2g(ui, Vj) + g(vi, Vj))
i“1 j“1
(4.8)
Proof. Define the stacked vector: z =
xx1
. The function △(A, W, Θ) is given by:
E
Aσ 'WX + Θx1) — Aσ (Wx1 + Θx)
(4.9)
We can write 4.9 using definitions in Eq. 4.7 as:
—2E∣σ (zTU) ATAσ (VTz)] + E∣σ 'zTV) ATAσ (VTz)]
Denoting U = σ (Uz), V = σ (Vz), We can now write:
E” UT AT AU — 2UT AT AV + VT AT Av].	(4.10)
Letting ATA = Q, we can now write the above as:
hh	hh	hh
E £ £ UiQijuj _ 2 £ £ UiQijVj + £ £ ViQijGj	(4.11)
i“1 j“1	i“1 j“1	i“1 j“1
and more compactly
hh
£ £ ElQij(Uiuj _ 2UiVj + Vivj)]	(4.12)
i“1 j “1
writing back Ui = σ(ui ∙ z) and Vj = σ(Vj ∙ z) We have
hh
£ £ QijE σ(Ui ∙ z)σ(Uj ∙ z) — 2σ(Ui ∙ z)σ(Vj ∙ z) + σ(Vi ∙ z)σ(Vj ∙ z)	(4.13)
i“1 j “1
Using Eq. 4.6, we have that Eq. 4.13 results in the expression in Eq. 4.8.
□
Theorem 4.4 provides an interesting characterization of when an RNN is commutative. As mentioned
earlier, an RNN is commutative iff △(A, W, Θ) = 0. Since the theorem gives a closed-form
expression for △ it provides a measure of non-commutativity which we will optimize in what follows.
The function in Eq. 4.8 does not provide clear structual constraints on the matrices A, W, Θ but a
rather elaborate dependence between their elements. The function △ indeed achieves a value of zero
for the ReLU implementation of the max function discussed in the intro, as well as (infinitely) many
other commutative state transition functions.
5	Learning Commutative RNNs
In the previous section we obtained a function △(A, W, Θ) whose value reflects the degree to which
the state transition operator o is commutative.
If we could enforce the constraint △(A, W, Θ) = 0 we could optimize over commutative RNNs.
Here we follow a standard approach to learning with constraints and add △(A, W, Θ) as a regularizer
multiplied by a regularization coefficient λ > 0 to whatever training loss is being optimized (e.g.,
squared error for regression, cross entropy for classification etc). We note that if we have training
data for which zero training loss can be achieved with a commutative RNN, then the optimum of the
regularized objective will be a commutative RNN regardless of the value of λ. We have indeed found
this to be the case in our experiments.
5
Under review as a conference paper at ICLR 2019
6	Related Work
In recent years, the question of invariances and network architecture has attracted considerable
attention, and in particular for various forms of permutation invariances. Several works have focused
on characterizing architectures that are “by-design” permutation invariant (Zaheer et al., 2017;
Vinyals et al., 2016; Qi et al., 2017; Hartford et al., 2018).
While the above works address invariance for sets, there has also been work on invariance of
computations on graphs (Maron et al., 2019; Herzig et al., 2018). In these, the focus is on problems
that take a graph as input, and the goal is for the output to be invariant to all equivalent representations
of the graph. Another approach to graph invariant computations are so called neural message passing
architectures (Gilmer et al., 2017). Finally, there is also recent work on graph-based representations
and their relation to graph-isomorphism (Xu et al., 2018).
Our work is a departure from these ideas in two respects. First, the typical approach is to consider
an architecture that is invariant by design. Here we depart from this by considering an architecture
that is not generally invariant, but can be invariant for a particular setting of its parameters. This also
allows us to handle cases of “near invariance” where ordering may affect the output in some cases.
Second, the typical approach is non temporal, whereas ours puts a specific emphasis on state based
computation. This is in fact a natural approach for processing data streams, where many algorithms
store a sketch of the data and update it as samples arrive (e.g., see Alon et al., 1999).
7	Experiments
In order to demonstrate the versatile nature of commutative RNNs we evaluate our method on
several tasks. For each task we compare performance to a relevant DeepSet architecture. In order to
fairly compare our method to Zaheer et al. (2017) we implemented our own version of DeepSets in
TensorFlow. We also explored two aggregation (i.e., pooling) methods for DeepSets: max and sum,
both of which were considered in Zaheer et al. (2017).
7.1	Sequences of Digits
Perhaps the simplest suite of problems over sets are basic operations involving sets of digits, as done
in Zaheer et al. (2017). We evaluate our method over three such tasks, Sum, Max and Parity (the latter
was not used in the DeepSets paper). we show that commutative RNNs are able to capture all functions
easily whereas other baselines struggle and require adaptations to the specific problem at hand. We
treat the sum and max experiments as regression problems. Given a set tx1, . . . , xnu where the output,
f ({x1,..., xn}) is 1-dimensional, We apply the mean square error loss, 'f ({x1,..., xn}) — y)2
where y is the ground truth. Namely, y “ ^nL1 Xi and y “ max{x1,... ,xn} for the Sum and Max
experiments respectively.
(a) Sum of Digits
(b) Max of Digits
(c) Parity of Digits
Figure 1: Experiments on digit sets. All graphs depict accuracy as a function of the sequence length.
In 1a and 1b, a prediction fp{x1, . . . , xn}q, is considered correct if its rounded value is equal to the
ground truth, i.e., |f ({x1,..., xn}) 一 y| ≤ 0.5.
6
Under review as a conference paper at ICLR 2019
7.1.1	SUM OF DIGITS
We follow the exact setting described in Zaheer et al. (2017). We randomly generate 100k sequences
of between 3 to 10 digits long where the label is the sum of the digits of the sequence. We evaluate
the performance over sequences of lengths of 10 up to 100 digits long. Results are shown in Figure
1a and demonstrate that DeepSets with sum-aggregation and commutative RNNs behave similarly, as
expected. However DeepSets with max aggregation does not generalize well.
7.1.2	MAX OF DIGITS
We perform the exact same experiment as 7.1.1 with the change that the label of a sequence is
assigned with the maximal digit in the specific sequence. Results are shown in Figure. 1b and are
similar to 7.1.1, except here the DeepSet sum does not generalize well.
7.1.3	PARITY
For the parity experiment we generate 100k binary sequences of lengths 1 up to 10 where the assigned
label is 1 if the number of 1’s in the binary sequence is odd and 0 otherwise. As before, we evaluate
over sequences with length up to 100. Since the parity is a binary classification problem, we apply the
cross-entropy loss. Results are shown in figure 1c. Here both variants of DeepSets do not generalize
well, whereas the commutative architecture has a simple solution for the parity aggregation, and
therefore generalizes well.
7.2 IMAGE BASED SEQUENCE OF DIGITS
MNIST8m (Loosli et al.) is a collection of 8 million 28 X 28 grey-scale images of digits {0,..., 9}.
We repeat experiments 7.1.1 and 7.1.2 where instead of sets of numerical values, we use sets of
images drawn randomly to create sequences of length up to 10. We evaluate the performance of the
learned models on sequences up to length 50. Figure. 2 shows that in both cases the commutative
architecture performs well, whereas DeepSets only work when using the true underlying aggregation
scheme.
(a) Sum of a set of MNIST digits.
Figure 2: Experiments on MNIST digits. As seen in the plots, Commutative RNNs without any
adaptations perform equally to a DeepSet architecture with a modified pooling scheme.
(b) Maxofa set of MNIST digits.
8	Evaluation on ECG Recordings
We also evaluate on the ECG5000 dataset (Chen et al., 2015), which is a sequence classification
problem between normal and abnormal ECG activity. There is no reason to expect this task is
pertmutation invariant, but since our method only regularizes for invariance, it can also handle variant
data. Results on this data for DeepSets, plain RNN and Commutative RNNs gives errors 0.9, 0.944,
0.952 respectively. Thus, indeed the regularizing effect of our methods leads to improved results on
variant data as well.
7
Under review as a conference paper at ICLR 2019
Table 1:	Point-cloud Classification
Method	Accuracy
DeePSets	0.83
Commutative RNNs 0.822
Table 2:	Accuracy on the ModeINet40 dataset.
8.1	Point Cloud Classification
Point clouds are sets of 3-dimensional vectors which rePresent objects. Such rePresentations of
objects are useful in various fields such as robotics, comPuter graPhics, 3D scanners, etc. The nature
of Point-clouds suggests that these are order-less objects and thus a natural candidate for evaluation of
methods designed to handle sets. As in Zaheer et al. (2017) we use ModelNet40 Wu et al. (2015), a
collection of 12k 3D rePresentations of40 different categories. We treat this as a 40-way classification
Problem. From each 3D rePresentation we Produce a Point-cloud which consists of 100 3-dimensional
vectors. Results are shown in Table 2, and are comParable for DeePSets and Commutative RNNs.
To summarize the emPirical comParisons above, our results demonstrate that using recurrent methods
to handle sets Provides results that are more robust to the underlying aggregation method, and can
outPerform DeePSets in several settings.
9 Discussion
We have introduced an aPProach to Permutation invariant comPutation that relies on recursive archi-
tectures. While recursive architectures are generally non Permutation-invariant we give conditions
on their Parameters such that they become invariant. The two conditions are that the state transition
is associative and commutative. Of these, we focus on the latter, which we show is equivalent to
minimizing a certain function of the RNN Parameters. To do so we integrate the “non-commutativity”
of inPuts with resPect to a Gaussian distribution. We believe this is a Promising aPProach for other
constraints on inPut sPace, which may be likewise rePresented via an aPProPriate measure over sPace.
A first natural extension of our aPProach is to seek a regularizer for the associative ProPerty. We
exPlored using the Gaussian integration for this PurPose but the integrals do not seem to have a
closed form exPression, and are related to integrating multivariate Gaussians under linear constraints,
which is considered a hard Problem (Miwa et al., 2003). However, it is Possible that under other
distributions the integral will become feasible, or that some distributions will allow faster mixing of
samPling techniques.
Another extension is to other activation functions. Our derivation for the commutative regularizer
relies on the structure of the ReLU function, but variants such as leaky ReLU can also be ana-
lyzed similarly. We leave analysis of other alternatives such as tanh, or more broadly LSTM like
architectures for further work.
Here we consider the case of a single hidden layer state transition, although we have no constraint
on the dePth of the networks for ψpxq (the inPut Processing stage) and the gpsq (the state to outPut
maPPing). We note that one layer transitions are often used in Practice, but it would be interesting to
exPlore regularizers for transitions with more layers.
Another interesting learning-theoretic question is what is the samPle comPlexity of learning with
commutative and associative regularizers. For examPle, it might haPPen that in some cases the com-
mutative constraint is sufficient to ensure a low enough samPle comPlexity such that generalization is
good even without enforcing associativity. Finally, it would be interesting to aPPly our aPProach to
Problems that are not “strictly Permutation invariant” where regularization is exPected to be more
effective than a hard constraint on Permutation invariance.
8
Under review as a conference paper at ICLR 2019
A Proof of Eq. 4.3.
Here We prove Eq. 4.3. Let (Ω, A, P) be a probability space. Let X be a continuous random variable
such that Vω P Ω it holds that X(ω)20 (non-negative). Assume E ∣x] “ 0, from Markov
inequality,
P”X > J] ≤ nE”X1 = 0
(A.1)
holds for any n > 0. To show that PIX = 0∣ = 1, given E > 0, for any n > ɪ we have
0 “ PIX > nl]2PIX > e]20, therefore PIX > e] “ 0 or equivalently, PlX V e] = 1.
On the other hand, suppose PIX “ 0∣ “ 1, denote by E U Ω the set on which X(ω) ‰ 0. We can
write
E
X(ω)dP(ω) “	X(ω)dP(ω) `	X(ω)dP(ω)
JE	Jω∖E
(A.2)
where both terms evaluate to zero. E X pωqdPpωq “ 0 since E has zero measure with respect to P
and ∖ω∖e X(ω)dP(ω) “ 0 since Vω P Ω∖E X(ω) “ 0.
References
Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the frequency
moments. Journal ofComputer and system sciences, 58(1):137-147, l999.
Mɑrt^n Arjovsky, Soumith Chintala, and L6on Bottou. Wasserstein generative adversarial networks.
In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney,
NSW, Australia, 6-11 August 2017, pp. 214-223, 2017. URL http://proceedings.mlr.
press/v70/arjovsky17a.html.
Yanping Chen, Eamonn Keogh, Bing Hu, Nurjahan Begum, Anthony Bagnall, Abdullah Mueen,
and Gustavo Batista. The ucr time series classification archive, July 2015. www.cs.ucr.edu/
~eamonn/time_series_data/.
Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Y. Bengio, D. Schu-
urmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta (eds.), Advances in Neural Information
Processing Systems 22, pp. 342-350. 2009.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning, pp. 1263-1272, 2017.
Jason Hartford, Devon Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep models of
interactions across sets. In International Conference on Machine Learning, pp. 1914-1923, 2018.
Roei Herzig, Moshiko Raboh, Gal Chechik, Jonathan Berant, and Amir Globerson. Mapping
images to scene graphs with permutation-invariant structured prediction. In Advances in Neural
Information Processing Systems, pp. 7211-7221, 2018.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R Kosiorek, Seungjin Choi, and Yee Whye Teh.
Set transformer: A framework for attention-based permutation-invariant neural networks. In
Proceedings of the 35th International Conference on Machine Learning, 2019.
Gaelle Loosli, StePhane Canu, and L6on Bottou. Training invariant support vector machines using
selective sampling.
Haggai Maron, Heli Ben Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. In 7th International Conference on Learning Representations, ICLR, 2019.
9
Under review as a conference paper at ICLR 2019
Tetsuhisa Miwa, AJ Hayter, and Satoshi Kuriki. The evaluation of general non-centred orthant
probabilities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 65(1):
223-234, 2003.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for
3d classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 652-660, 2017.
Matus Telgarsky. Neural networks and rational functions. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pp. 3387-3393. JMLR. org, 2017.
Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets.
In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico,
May 2-4, 2016, Conference Track Proceedings, 2016.
Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong
Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 1912-1920, 2015.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In 6th International Conference on Learning Representations, ICLR, 2018.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov,
and Alexander J Smola. Deep Sets. In Advances in neural information processing systems, pp.
3391-3401, 2017.
10