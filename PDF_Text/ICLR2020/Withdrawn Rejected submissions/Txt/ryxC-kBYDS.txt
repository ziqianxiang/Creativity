Under review as a conference paper at ICLR 2020
Gaussian Conditional Random Fields for
Classification
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, a Gaussian conditional random field model for structured binary
classification (GCRFBC) is proposed. The model is applicable to classification
problems with undirected graphs, intractable for standard classification CRFs.
The model representation of GCRFBC is extended by latent variables which yield
some appealing properties. Thanks to the GCRF latent structure, the model
becomes tractable, efficient, and open to improvements previously applied to
GCRF regression. Two different forms of the algorithm are presented: GCRF-
BCb (GCRGBC - Bayesian) and GCRFBCnb (GCRFBC - non-Bayesian). The
extended method of local variational approximation of sigmoid function is used
for solving empirical Bayes in GCRFBCb variant, whereas MAP value of latent
variables is the basis for learning and inference in the GCRFBCnb variant. The
inference in GCRFBCb is solved by Newton-Cotes formulas for one-dimensional
integration. Both models are evaluated on synthetic data and real-world data. It
was shown that both models achieve better prediction performance than relevant
baselines. Advantages and disadvantages of the proposed models are discussed.
1	Introduction
Increased quantity and variety of sources of data with correlated outputs, so called structured data,
created an opportunity for exploiting additional information between dependent outputs to achieve
better prediction performance. One of the most successful probabilistic models for structured out-
put classification problems are conditional random fields (CRF) (Sutton & McCallum, 2006). The
main advantages of CRFs lie in their discriminatory nature, resulting in the relaxation of indepen-
dence assumptions and the label bias problem that are present in many graphical models. Aside
of many advantages, CRFs also have many drawbacks mostly resulting in high computational cost
or intractability of inference and learning. A wide range of different approaches of tackling these
problems has been proposed, and they motivate our work, too.
One of the popular methods for structured regression based on CRFs - Gausian conditional random
fields (GCRF) - has the form of multivariate Gaussian distribution (RadoSavIjevic et al., 2010).
The main assumption of the model is that the relations between outputs are presented in quadratic
form. It has convex loss function and, consequently, efficient inference and learning, and expensive
sampling methods are not used.
In this paper, a new model of Gaussian conditional random fields for binary classification is pro-
posed (GCRFBC). GCRFBC builds upon regression GCRF model which is used to define latent
variables over which output dependencies are defined. The model assumes that discrete outputs yi
are conditionally independent conditioned on continuous latent variables zi which follow a distribu-
tion modeled by a GCRF. That way, relations between discrete outputs are not expressed directly.
Two different inference and learning approaches are proposed in this paper. The first one is based on
evaluating empirical Bayes by marginalizing latent variables (GCRFBCb), whereas MAP value of
latent variables is the basis for learning and inference in the second model (GCRFBCnb). In order
to derive GCRFBCb model and its learning procedure the variational approximation of Bayesian
logistic regression (Jaakkola & Jordan, 2000) is generalized.
Compared to CRFs and structured SVM classifiers, the GCRFBC models have some appealing
properties:
1
Under review as a conference paper at ICLR 2020
•	The model is applicable to classification problems with undirected graphs, intractable for
standard classification CRFs. Thanks to the GCRF latent structure, the model becomes
tractable, efficient and open to improvements previously applied to GCRF regression mod-
els.
•	Defining correlations directly between discrete outputs may introduce unnecessary noise to
the model (Tan et al., 2010). This problem can be solved by defining structured relations
on a latent continuous variable space.
•	In case that unstructured predictors are unreliable, which is signaled by their large variance
(diagonal elements in the covariance matrix), it is simple to marginalize over latent variable
space and obtain better results.
GCRFBC model is relying on the assumption that the underlying distribution of latent variables is
multivariate normal distribution, due to that in the case when this distribution cannot be fitted well
to the data (e.g. when the distribution of latent variables is multimodal) the model will not perform
as well as it is expected. The proposed models are experimentally tested on both synthetic and
real-world datasets in terms of predictive performance and computation time. In experiments with
synthetic datasets, the results clearly indicate that the the empirical Bayes approach (GCRFBCb)
better exploits output dependence structure, more so as the variance of the latent variables increases.
We also tested both approaches on real-world datasets of predicting ski lift congestion, gene function
classification, classification of music according to emotion and highway congestion. Both GCRFBC
models outperformed ridge logistic regression, lasso logistic regression, neural network, random
forest, and structured SVM classifiers, demonstrating that the proposed models can exploit output
dependencies in a real-world setting.
2	Related Work
An extensive review of binary and multi-label classification with structured output is provided in Su
(2015). A number of different studies related to graph based methods for regression can be found in
the literature (Fox, 2015). CRFs were successfully applied on a variety of different structured tasks
(Cotterell & Duh, 2017; Zhang et al., 2015; Masada & Bunescu, 2017; Zia et al., 2018) and different
model adaptations can be found in literature Kim (2017); Maaten et al. (2011). Recently, successful
unifications of deep learning and CRFs have been proposed Chen et al. (2016); Kosov et al. (2018).
Moreover, implementation of deep neural networks as potential functions is presented in form of
structure prediction energy networks (SPEN) Belanger & McCallum (2016); Belanger et al. (2017).
Adaptation of normalazing flows in SPEN structure is presented in Lu & Huang (2019).
An extensive review on topic of binary and multi-label classification with structured output is pro-
vided in Su (2015). Large number of different studies related to graph based methods for regression
can be found in the literature (Fox, 2015). CRFs were successfully applied on a variety of different
structured tasks, such as: low-resource named entity recognition (Cotterell & Duh, 2017), image
segmentation (Zhang et al., 2015), chord recognition (Masada & Bunescu, 2017) and word seg-
mentation (Zia et al., 2018). The mixture of CRFs capable to model data that come from multiple
different sources or domains is presented in Kim (2017). The method is related to the well known
hidden-unit CRF (HUCRF) (Maaten et al., 2011). The conditional likelihood and expectation min-
imization (EM) procedure for learning have been derived there. The mixtures of CRF models were
implemented on several real-world applications resulting in prediction improvement. Recently, a
model based on unification of deep learning and CRF was developed by Chen et al. (2016). The
deep CRF model showed better performance compared to either shallow CRFs or deep learning
methods on their own. Similarly, the combination of CRFs and deep convolutional neural networks
was evaluated on an example of environmental microorganisms labeling (Kosov et al., 2018). The
spatial relations among outputs were taken in consideration and experimental results have shown
satisfactory results.
The GCRF model was first implemented for the task of low-level computer vision (Tappen et al.,
2007). Since then, various different adaptations and approximations of GCRF were proposed (Ra-
dosavljevic et al., 2014). The parameter space for the GCRF model is extended to facilitate joint
modelling of positive and negative influences (Glass et al., 2016). In addition, the model is ex-
tended by bias term into link weight and solved as a part of convex optimization. Semi-supervised
2
Under review as a conference paper at ICLR 2020
marginalized Gaussian conditional random fields (MGCRF) model for dealing with missing vari-
ables was proposed by Stojanovic et al. (2015). The benefits of the model were proved on partially
observed data and showed better prediction performance than alternative semi-supervised structured
models.A comprehensive review of continuous conditional random fields (CCRF) was provided in
Radosavljevic et al. (2010). The sparse conditional random fields obtained by l1 regularization are
first proposed and evaluated by Wytock & Kolter (2013). Additionaly, Frot et al. (2018) presented
GCRF with the latent variable decomposition and derived convergence bounds for the estimator that
is well behaved in high dimensional regime. An adaptation of GCRF on discrete output was briefly
discussed in Radosavljevic (2011), as a part of future work. This discussion motivates our work, but
our approach is different in technical aspects.
3	Methodology
In this section we first present already known GCRF model for regression and then we propose
GCRFBC model for binary classification and two approaches to inference and learning.
3.1	Background Material
GCRF is a discriminative graph-based regression model (Radosavljevic et al., 2010). Nodes of the
graph are variables y = (y1, y2, . . . , yN), which need to be predicted given a set of features x.
The attributes x = (x1 , x2 , . . . , xN ) interact with each node yi independently of one another,
while the relations between outputs are expressed by pairwise interaction function. In order to learn
parameters of the model, a training set of vectors of attributes x and real-valued response variables
y are provided. The generalized form of the conditional distribution P y |x, α, β is:
P(y|x，α,β = Z (x,1α, β)exp
NK	L
-XX ak Mi- Rk (Xi)) 2 - X X βιSj 3-y )2
i=1 k=1	i6=j l=1
(1)
First sum models relations between outputs yi and corresponding input vector xi and the second
one models pairwise relations between nodes. Rk (xi)represents an unstructured predictor of yi for
each node in the graph and Sil is value that expresses similarity between nodes i and j in graph
i
l. Unstructured predictor can be any regression model that gives prediction of output yi for given
attributes xi . K is the total number of unstructured predictors. L is the total number of graphs
(similarity functions). Graphs can express any kind of binary relations between nodes e.g., spatial
and temporal correlations between outputs. Z is a partition function and vectors α and β are learn-
able parameters. One of the main advantages of GCRF is the ability to express different relations
between outputs by variety of graphs and ability to learn which graphs are significant for predic-
tion. The quadratic form of interaction and association potential enables conditional distribution
P(y|x, α, β)to be expressed as multivariate Gaussian distribution (Radosavljevic et al., 2010):
P(y|x, α, β) = (2π) 1园ι eχp (-2(y - μ)T∑-1(y - μ))	⑵
Precision matrix Σ-1 = 2Q and distribution mean μ = Σb are defined as, respectively:
Q= (PkK=1 αk + PhN=1 PlL=1 βlSilh, ifi=j	(3)
Q=	-PlL=1βlSilj,	ifi 6=j	(3)
bi =2 (X αkRk (Xi))	⑷
Due to concavity of multivariate Gaussian distribution, the inference task argmaxP (y|x, α, β)is
y
straightforward. The maximum posterior estimate of y is the distribution expectation μ.
The objective of the learning task is to optimize parameters α and β by maximizing conditional log
likelihood argmax Py logP (y|x, α, β). One way to ensure positive definiteness of the covariance
α,β	y
matrix of GCRF is to require diagonal dominance (Strang et al., 1993). This can be ensured by
imposing constraints that all elements of α and β be greater than 0 (Radosavljevic et al., 2010).
3
Under review as a conference paper at ICLR 2020
3.2	GCRFBC model representation
One way of adapting GCRF to classification problem is by approximating discrete outputs by suit-
ably defining continuous outputs. Namely, GCRF can provide dependence structure over contin-
uous variables which can be passed through sigmoid function. That way the relationship between
regression GCRF and classification GCRF is similar to the relationship between linear and logistic
regression, but with dependent variables. Aside from allowing us to define a classification variant
of GCRF, this may result in additional appealing properties: (i) The model is applicable to classifi-
cation problems with undirected graphs, intractable for standard classification CRFs. Thanks to the
GCRF latent structure, the model becomes tractable, efficient and open to improvements previously
applied to GCRF regression models. (ii) Defining correlations directly between discrete outputs
may introduce unnecessary noise to the model (Tan et al., 2010). We avoid this problem by defining
structured relations on a latent continuous variable space. (iii) In case that unstructured predictors
are unreliable, which is signaled by their large variance (diagonal elements in the covariance matrix),
it is simple to marginalize over latent variable space and obtain better results.
It is assumed that yi are discrete binary outputs and zi are continuous latent variables assigned to
each yi . Each output yi is conditionally independent of the others, given zi .
The conditional probability distribution P(yi|zi) is defined as Bernoulli distribution:
P(yi∖zi) = Ber(yi∣σ(zi)) = σ(zi)yi(1 - σ(zi))1-yi	(5)
where σ(∙) is sigmoid function. Due to conditional independence assumption, the joint distribution
of outputs yi can be expressed as:
N
P(y1,y2,. . . ,yN|z) = Y σ(zi)yi (1 - σ(zi))1-yi	(6)
i=1
Furthermore, the conditional distribution P (z|x) is the same as in the classical GCRF model and has
canonical form defined by multivariate Gaussian distribution. Hence, joint distribution of continuous
latent variables z and outputs y given x and θ = (α1, . . . , αK, β1, . . . , βL) is is the general form of
the GCRFBC model defined as:
N
P (y, z|x, θ) = Yσ(zi)yi(1 - σ(zi))1-yi
i=1
1
(2π)N/2 ∣Σ(x, θ)∣1/2
• exp (- 1(z - μ(x, θ))T∑-1(x, θ)(z - μ(x, θ))
(7)
We consider two ways of inference and learning in GCRFBC model: (i) GCRFBCb - with condi-
tional probability distribution P (y|x, θ), in which variables z are marginalized over, and (ii) GCRF-
BCnb - with conditional probability distribution P (y|x, θ, μZ), in which variables Z are substituted
by their expectations.
3.3	Inference in GCRFBCb Model
Prediction of discrete outputs y for given features x and parameters θ is analytically intractable due
to integration of the joint distribution P(y, z|x, θ) with respect to latent variables. However, due to
conditional independence between nodes, it is possible to obtain P(yi = 1|x, θ).
P(yi
ι∣χ, θ)
σ(zi)P(z∣x, θ)dz
(8)
z
where σ(zi) models P(yi|z). As a result of independence properties of the distribution, it holds
P(yi = 1|z) = P(yi = 1|zi), and it is possible to marginalize P(z|x, θ) with respect to latent
variables z0 = (z1, . . . , zi-1, zi+1, . . . , zN):
P(yi = 1|x, θ) =	σ(zi)	P(z0,zi|x, θ)dz0 dzi
(9)
4
Under review as a conference paper at ICLR 2020
where fz/ P(z0,zi∣x, θ)dz0 is normal distribution with mean μ = μ% and variance
fore, it holds:
σi2
P(yi
1|x, θ)
Z+∞
∞
Σii . There-
(10)
The evaluation of P(yi = 0|x, θ) is straightforward: P(yi = 0|x, θ) = 1 - P(yi = 1|x, θ).
The one-dimensional integral is still analytically intractable, but can be effectively evaluated by one-
dimensional numerical integration. The proposed inference approach can be effectively used in case
of huge number of nodes, due to low computational cost of one-dimensional numerical integration.
3.4	Inference in GCRFBCnb Model
The inference procedure in GCRFBCnb is much simpler, because marginalization with respect to
latent variables is not performed. To predict y, it is necessary to evaluate posterior maximum of
latent variable zmax = argmaxP(z|x, θ), which is straightforward due to normal form of GCRF.
z
Therefore, it holds ZmaX = μz,i. The conditional distribution P(y% = 1|x, μz,i, θ), where μz,i is
expectation of latent variable zi , can be expressed as:
P(yi = 1 |x, μz, θ) = σ(μz,i)
1
1 + exp(-μz,i)
(11)
3.5	Learning in GCRFBCb Model
In comparison with inference, learning procedure is more complicated. Evaluation of the condi-
tional log likelihood is intractable, since latent variables cannot be analytically marginalized. The
conditional log likelihood is expressed as:
MM
L (Y|X, θ) = logJzP(Y,Z|X, θ)dz = XlogJ P(yj,ZjX∙, θ)dzj = XLj(yjX∙, θ)
eχp(- 1 (Zj - μj)Tς-I(Zj - μj))
N
Lj(yj|xj,θ) = log / ∏σ(zji)yji(1 -σ(Zji)P—y”
zj i=1
(12)
(2π)N/2 ∣Σj∣1/2
(13)
where Y ∈ RM ×N is complete dataset of outputs, X ∈ RM ×N ×A is complete dataset of features,
M is the total number of instances and A is the total number of features. Please note that each
instance is structured, so while different instances are independent of each other, variables within
one instance are dependent.
One way to approximate integral in conditional log likelihood is by local variational approximation.
Jaakkola & Jordan (2000) derived lower bound for sigmoid function, which can be expressed as:
σ(x) > σ(ξ) exp{(x - ξ)∕2 - λ(ξ)(x2 - ξ2)}	(14)
where λ(ξ) = - 2ξ ∙ [σ(ξ) - 2] and ξ is a variational parameter. The Eq. 14 is called ξ ITansfOr-
mation of sigmoid function and it yields maximum value when ξ = x. This approximation can be
applied to the model defined by Eq. 13, but the variational approximation has to be further extended
because of the product of sigmoid functions, such that:
p(yj, Zjlxj, θ) = P(yj|zj)P(zj|xj, θ) ≥ p(yj∙, Zj|xj, θ,ξj)	(15)
p (yj, Zj |xj, θ, ξj) = Y σ(ξji) eχp (Zjiyji- ji 2 'ji - λ(ξji )(Z2i - ξ2i)) ∙
i=1	(16)
(2∏)N∕2∣Σj∣1∕2 eXP J 2(Zj-% )T ς-1 (Zj-% )I
The Eq. 16 can be arranged in the form suitable for integration. Detailed derivation of lower bound of
conditional log likelihood is presented in Appendix A. The lower bound of conditional log likelihood
5
Under review as a conference paper at ICLR 2020
L(yj |xj, θ, ξj) is defined as:
N
Lj (yj lxj, θ, ξj) = log P(yj lxj,θ, ξj) =
—	i=1
2 μT £-1〃j + 1 mT S-Imj +1 log ISj |
(log σ(ξji) 一导 十 λ(ξji)ξ2iJ -
where:
Sj-1	Σj-1 + 2Λj		mj =	Σj (	yj - 2 I) +，-14j	
		'λ(ξjl)	0	0 ..	.	0	-	
	Λj =	0 . .	λ(ξj2 ) . .	0 .. .. ..	.0 . .	
		. 0	. 0	. 0 ..	.. .	λ(ξjN)	
(17)
(18)
(19)
GCRFBCb uses the derivative of conditional log likelihood in order to find the optimal values for
parameters α, β and matrix of variational parameters ξ ∈ RM ×N . In order to ensure positive
definiteness of normal distribution involved, it is sufficient to constrain parameteres α > 0 and β >
0. The partial derivatives of lower bound of conditional log likelihood are presented in Appendix B.
For constrained optimization, the truncated Newton algorithm was used Nocedal & Wright (2006);
Facchinei et al. (2002). The target function is not convex, so finding a global optimum cannot be
guaranteed.
3.6	Learning in GCRFBCnb Model
In GCRFBCnb the mode of posterior distribution of continuous latent variable z is evaluated di-
rectly, so there is no need for approximation. The conditional log likelihood can be expressed as:
MN	MN
L (Y |X, θ, μ) = log P (Y 1X, θ, μ) = XX log P (yji|xj, θ,μji) = XX Lji(yjdxj, θ,μji)
j=1 i=1	j=1 i=1
(20)
Lji (yjilxj , θ, μji) = yji log σ(μji) + (1 - yji) log (1 - σ(μji))	QI)
The partial derivatives of conditional log likelihood are presented in Appendix C.
4	Experimental Evaluation
Both proposed models were tested and compared on synthetic data and real-world tasks.1 All com-
pared classifiers were compared in terms of the area under ROC curve (AUC) and accuracy 2 (ACC).
Moreover, the lower bound (in case of GCRFBCb) of conditional log likelihood L (Y X, θ, μ) and
actual value (in case of GCRFBCnb) of conditional log likelihood L Y |X , θ of obtained values
on synthetic test dataset were also reported.
4.1	Synthetic Dataset
The main goal of experiments on synthetic datasets was to examine models under various controlled
conditions, and show advantages and disadvantages of each. In all experiments on synthetic datasets
two different graphs were used (hence β ∈ R2 ) and two unstructured predictors (hence α ∈ R2 ).
The results of experiments on synthetic datasets are presented in Appendix D.
It can be noticed, that in cases where norm of the variances of latent variables is small, both mod-
els have equal performance considering AUC and conditional log likelihood L (Y |X, θ). This is
the case when values of parameters α used in data generating process are greater or equal to the
1Implementation can be found at https://github.com/andrijaster/GCRFBC_B_NB
2PyStruct package does not have option of returning SSVM and CRF confidence values for AUC evaluation
6
Under review as a conference paper at ICLR 2020
values of parameters β. This means that the information provided by unstructured predictors is
more important for classifications task than the information provided by output structure. There-
fore, conditional distribution P(y, z|x, θ) is concentrated around mean value and MAP estimate
is a satisfactory approximation. However, when data is generated from distribution with signifi-
cantly higher values of β than α, the GCRFBCb performs significantly better than GCRFBCnb.
For the larger values of variance norm, this difference is also large. This means that the structure
between outputs has significant contribution to solving the classification task. It can be concluded
that GCRFBCb has at least equal prediction performance as GCRFBCnb. Also, it can be argued
that the models were generally able to utilize most of the information (from both features and the
structure between outputs), which can be seen through AUC values. In addition, distribution of local
variational parameters were analyzed during learning. It is noticed that in each epoch, the variance
of this distribution is small and that the parameters can be clustered and their number significantly
reduced. Therefore, it is possible to significantly lower down computational and memory costs of
GCRFBCb learning procedure, but that’s out of the scope of this paper.
4.2	Performance on Real-World Datasets
4.2.1	Ski lifts congestion
Data used in this research includes information on ski lift gate entrances in Kopaonik ski resorts,
for the period March 15 to March 30 for the seasons from 2006 to 2011. The goal is to predict
occurrence of crowding on ski lifts for 40 minutes in advance. Total number of instances in dataset
was 4,850 for each ski lift, which is 33,950 in total.
Relatively simple method for crowding detection was devised for labelling data. We assume that, if
the crowding at some gate occurs, distributions of skiing times from other gates to that gate within
some time window get shifted towards larger values. We model probability distribution of skiing
time between two gates by the well-known parametric method of kernel density estimation (KDE)
(Silverman, 2018). The distribution shift is measured with respect to the mode of the distribution.
The dataset is generated by observing shifts in time windows of 5 minutes. When the mode of the
distribution of skiing times within that window is greater than the mode for the whole time-span,
the instance is labeled by 1 (crowding) and otherwise, it is labeled by 0 (no crowding). In order to
obtain more information from the data distribution, additional 18 features were extracted.
Four different unstructured predictors that were trained on each class separately were used: ridge lo-
gistic regression, LASSO logistic regression, neural network and random forest, whereas additional
two unstructured predictors: decision tree and neural network were trained on all nodes together.
Additionally, three structural support vector machine and two CRFs classifiers were used (Muller
& Behnke, 2014). Fully connected graph of SSVM and CRF models are defined as SSVM-full and
CRF-full, whereas Chow-Liu tree method for specifying edge connections are defined as SSVM-
tree and CRF-tree, respectively. In the SSVM-independent model the nodes of the graph are not
connected.
Six different weighted graphs were used to capture dependence structure between ski lifts (nodes):x2
statistics on labels of training set, mutual information between labels, correlation matrix between
outputs of over-fitted neural networks, norm of difference between vectors of labels and two graphs
were defined based on difference of vectors of historical labels and on differences of historical
averages of skier times.
The AUC score and ACC of structured and unstructured predictors, along with the total computa-
tional time are shown in Table 1. It can be observed that GCRFBCb and GCRBCnb outperformed
unstructured and other structured predictors in all cases. Based on evaluated parameters it could
be concluded that dependence structure has significant impact on overall prediction performance,
even though, due to low values of norm of variance, GCRFBCb and GCRFBCnb have equal AUC
scores. It can be summarized that advantages of structured models compared to unstructured are
obvious, but in this particular task due to equal prediction performance and its lower computational
and memory complexity, GCRFBCnb is the best choice for this specific application.
7
Under review as a conference paper at ICLR 2020
Table 1: Prediction performance and computation time of classifiers - Ski lifts congestion problem
Model	AUC	ACC	Calculation time [sec]
GCRFBCnb	0.831	0.749	119.554
GCRFBCb	一	0.831	0.749	3364.326
Ridge logistic	0.793	0.736	0.41
LASSO logistic	一	0.793	0.735	1.799
Neural network	0.790	0.720	151.571
Random forest	0.783	0.720	7.983
Decision tree - together	-	0.681	8.297
Neural network - together	-	0.711	13.997
SSVM - full	-	0.622	517.412
SSVM - tree	-	0.615	580.475
SSVM - independent	-	0.635	1029.172
CRF - tree	-	0.745	16415.723
CRF - full	一		0.740	13942.542
Table 2: Prediction performance and computation time of classifiers - Music classification
accordint to emotion
Model	AUC	ACC	Calculation time [sec]
GCRFBCnb	0.859	0.811	7.248
GCRFBCb	一	0.860	0.813	353.328
Ridge logistic	0.826	0.794	0T38
LASSO logistic	一	0.832	0.797	0.874
Neural network	0.811	0.783	98TT32
Random forest	0.843	0.798	2.469
Decision tree - together	-	0.736	0.564
Neural network - together	-	0.782	8.471
SSVM - full	-	0.755	76.817
SSVM - tree	-	0.795	75:93
SSVM - independent	-	0.784	146.867
4.2.2	Multi-label classification of music according to emotion
The dataset used for this work consists of 100 songs from 7 different genres. The collection was
created from 233 musical albums choosing three songs from each album. 8 rhythmic and 64 timbre
features are extracted. The music is labeled in 6 categories of emotions: amazed-surprised, happy-
pleased, relaxing-calm, quiet-still, sad-lonely and angry-fearful (Trohidis et al., 2008). Total number
of instances in dataset was 593. Four different weighted graphs were used: statistics on labels of
training set, mutual information between labels, correlation matrix between outputs of over-fitted
neural networks and norm of difference between vectors of labels. Same unstructured predictors
as in ski lift congestion problem were used, along with three structural support vector machine
classifiers.
The performances of models are evaluated by 10 fold cross validation. The AUC score and ACC
of structured and unstructured predictors, along with the total computational time are shown in
Table 2. It can be seen that GCRFBCb has achieved the best prediction performances. The ACC
of GCRFBC models are significantly better than the SSVM performances. The AUC score and
ACC of GCRGBCb are higher than the best result (AUC = 0.8237) presented in original paper
(Trohidis et al., 2008). As in previous cases, computational time of GCRFBCb is significantly
longer compared to GCRFBCnb and SSVM models.
4.2.3	Gene function classification
This dataset is formed by micro-array expression data and phylogenetic profiles with 2417 genes
(instances). The number of features is 103, whereas each gene is associated with the set of 14 groups
(Elisseeff & Weston, 2002). The same unstructured, structured predictors and weighted graphs, as
8
Under review as a conference paper at ICLR 2020
Table 3: Prediction performance and computation time of classifiers - Gene classification problem
Model	AUC	ACC	Calculation time [sec]
GCRFBCnb	0.775	0.766	48367
GCRFBCb	一	0.797	0.775	2297.727
Ridge logistic	0.582	0.539	0.079
LASSO logistic	一	0.583	0.540	0T88
Neural network	0.580	0.567	70.298
Random forest	0.601	0.615	5529
Decision tree - together	-	0.691	T2Γ8
Neural network - together	-	0.775	28.381
SSVM - full	-	0.771	10137.049
SSVM - tree	-	0.768	722.156
SSVM - independent	-	0.539	78.8870
in music according to emotion classification, were used. The 10-fold cross validation results of the
classification are shown in Table 3.
It can be observed that both GCRFBCb and GCRFBCnb achieved significantly better results in com-
parison with unstructured predictors. However, neural network trained on all data together achieved
the same ACC scores as GCRFBCb. The AUC of GCRFBCb has outperformed Random forest clas-
sifier by 19%, whereas SSVM - tree has better ACC compared to GCRFBCnb. It also outperformed
GCRFCnb, but as expected, its computation time was longer. In addition, the computation time of
CRFs models are longer compared to GCRFBCb
4.2.4	Highway congestion
The E70-E75 motorway is a major transit motorway in Serbia. With 504 kilometers, it is the one
the major transit motorway in Serbia. It crosses the country from north-west to south, starting at
Batrovci border crossing with the Republic of Croatia and ending with Presevo border crossing with
the Republic of North Macedonia.
One of the biggest problems in E70-E75 motorway is high congestion that frequently occurs. One of
the reasons lies in lack of open toll stations. In order to mitigate congestion problem, it is necessary
to predict its occurrence and open enough toll stations. Data used in this research includes infor-
mation of car entrance and exit for the year 2017. Two different sections were analyzed: Belgrade
- Adassevci and Niss - Belgrade. The section Belgrade - Adassevci was analyzed for the period of
January 2017, whereas section Niss - Belgrade was analyzed for the period of April - July 2017. The
congestion was labeled using the similar technique based on KDE as presented in the ski lifts con-
gestion problem. Based on raw datasets for sections Niss - Belgrade and Belgrade - Adassevci with
5,132,918 and 487,767 instances, respectively, anew dataset for section Niss - Belgrade is generated
by observing shifts in time windows of 10 minutes due to large number of vehicles, whereas in the
case of section Belgrade - Adassevci the shifts are observed in time windows of 20 minutes. Total
numbers of instances for sections Belgrade - Adassevci and Niss - Belgrade are 50,964 and 235,872,
whereas numbers of highway exits (outputs) are 6 and 18, respectively. The extracted features are
similar to the ones presented in ski congestion problem. The χ2 statistics, mutual information,
correlation matrix and difference of vectors of historical labels were used to capture dependence
structure, whereas the same unstructured predictors as in ski lifts congestion problem were evalu-
ated. The classification results, validated by 10 fold cross validation, are presented in Table 4.
The GCRFBCnb achieved the highest AUC and ACC scores in the section Belgrade - Adassevci,
whereas GCRFBCb has better prediction performance in section Niss - Belgrade. Moreover, in case
of section Niss - Belgrade, GCRFBCb has worse ACC score than fully connected CRF, whereas
CRF-tree outperformed GCRFBCnb in section Belgrade - Adassevci
9
Under review as a conference paper at ICLR 2020
Table 4: Prediction performance and computation time of classifiers - Highway congestion problem
	Nis - Belgrade			Belgrade - Adasevci		
	AUC	ACC	Calculation time [sec]	AUC	ACC	Calculation time [sec]
GCRFBCnb	0.740	0.684	344.166	0.974	0.925	907321
GCRFBCb	一	0.751	0.692	13818.874	0.956	0.895	2103.749
Ridge logistic	0.716	0.681	1073	0.917	0.856	1.771
LASSOlogistic 一	0.716	0.680	30712	0.917	0.856	17657
Neural network	0.72	0.682	857.602	0.956	0.904	125.339
Random forest	0.739	0.683	209.589	0.965	0.914	3.826
Decision tree - together	-	0.625	635.464	-	0.898	1.893
Neural network - together	-	0.664	125.441	-	0.880	167475
SSVM - full	-	0.588	7637.794	-	0.739	340.806
SSVM-tree	-	0.588	3684.138	-	0.755	392.597
SSVM - independent	-	0.602	3262.208	-	0.814	704.07
CRF - tree	-	0.685	29749.054	-	0.88	26539.250
CRF - full	一	-	0.683	52563.972	-	0.898	25339.97
5	Conclusion
In this paper, a new model, called Gaussian Conditional Random Fields for Binary Classification
(GCRFBC) is presented. The model is based on latent GCRF structure, which means that intractable
structured classification problem can become tractable and efficiently solved. Moreover, the im-
provements previously applied to regression GCRF can be easily extended to GCRFBC. Two differ-
ent variants of GCRFBC were derived: GCRFBCb and GCRFBCnb. Empirical Bayes (marginaliza-
tion of latent variables) by local variational methods is used in optimization procedure of GCFRBCb,
whereas MAP estimate of latent variables is applied in GCRFBCnb. Based on presented method-
ology and obtained experimental results on synthetic and real-world datasets it can be concluded
that both GCRFBCb and GCRFBCnb models have better prediction performance compared to the
analysed structured unstructured predictors. Additionaly, GCRFBCb has better performance con-
sidering AUC score, ACC and lower bound of conditional log likelihood L (Y|X, θ) compared
to GCRFBCnb, in cases where norm of the variances of latent variables is high. However, in cases
where norm of the variances is close to zero, both models have equal prediction performance. Due to
high memory and computational complexity of GCRFBCb compared to GCRFBCnb, in cases where
norm of the variances is close to zero, it is reasonable to use GCRFBCnb. Additionally, the trade
off between complexity and accuracy can be made in situation where norm of the variances is high.
Further studies should address extending GCRFBC to structured multi-label classification problems,
and lower computational complexity of GCRFBCb by considering efficient approximations.
References
David Belanger and Andrew McCallum. Structured prediction energy networks. In International
Conference on Machine Learning, pp. 983-992, 2016.
David Belanger, Bishan Yang, and Andrew McCallum. End-to-end learning for structured prediction
energy networks. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 429-439. JMLR. org, 2017.
Gang Chen, Yawei Li, and Sargur N Srihari. Word recognition with deep conditional random fields.
arXiv preprint arXiv:1612.01072, 2016.
Ryan Cotterell and Kevin Duh. Low-resource named entity recognition with cross-lingual,
character-level neural conditional random fields. In Proceedings of the Eighth International Joint
Conference on Natural Language Processing (Volume 2: Short Papers), volume 2, pp. 91-96,
2017.
Andre Elisseeff and Jason Weston. A kernel method for multi-labelled classification. In Advances
in neural information processing systems, pp. 681-687, 2002.
Francisco Facchinei, Stefano Lucidi, and Laura Palagi. A truncated newton algorithm for large scale
box constrained optimization. SIAM Journal on Optimization, 12(4):1100-1125, 2002.
10
Under review as a conference paper at ICLR 2020
John Fox. Applied regression analysis and generalized linear models. Sage Publications, 2015.
Benjamin Frot, Luke Jostins, and Gilean McVean. Graphical model selection for gaussian con-
ditional random fields in the presence of latent variables. Journal of the American Statistical
Association, (just-accepted), 2018.
Jesse Glass, Mohamed F Ghalwash, Milan Vukicevic, and Zoran Obradovic. Extending the mod-
elling capacity of gaussian conditional random fields while learning faster. In AAAI,, pp. 1596-
1602, 2016.
Tommi S Jaakkola and Michael I Jordan. Bayesian parameter estimation via variational methods.
Statistics and Computing, 10(1):25-37, 2000.
Minyoung Kim. Mixtures of conditional random fields for improved structured output prediction.
IEEE transactions on neural networks and learning systems, 28(5):1233-1240, 2017.
Sergey Kosov, Kimiaki Shirahama, Chen Li, and Marcin Grzegorzek. Environmental microorganism
classification using conditional random fields and deep convolutional neural networks. Pattern
Recognition, 77:248-261, 2018.
You Lu and Bert Huang. Structured output learning with conditional generative flows. arXiv preprint
arXiv:1905.13288, 2019.
Laurens Maaten, Max Welling, and Lawrence Saul. Hidden-unit conditional random fields. In
Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,
pp. 479-488, 2011.
Kristen Masada and Razvan C Bunescu. Chord recognition in symbolic music using semi-markov
conditional random fields. In ISMIR, pp. 272-278, 2017.
Andreas C Muller and Sven Behnke. Pystruct: learning structured prediction in python. The Journal
of Machine Learning Research, 15(1):2055-2060, 2014.
Jorge Nocedal and Stephen J Wright. Numerical optimization 2nd, 2006.
Vladan Radosavljevic. Gaussian conditional random fields for regression in remote sensing. Temple
University, 2011.
Vladan Radosavljevic, Slobodan Vucetic, and Zoran Obradovic. Continuous conditional random
fields for regression in remote sensing. In ECAI, pp. 809-814, 2010.
Vladan Radosavljevic, Slobodan Vucetic, and Zoran Obradovic. Neural gaussian conditional ran-
dom fields. In Joint European conference on machine learning and knowledge discovery in
databases, pp. 614-629. Springer, 2014.
Bernard W Silverman. Density estimation for statistics and data analysis. Routledge, 2018.
Jelena Stojanovic, Milos Jovanovic, Djordje Gligorijevic, and Zoran Obradovic. Semi-supervised
learning for structured regression on partially observed attributed graphs. In Proceedings of the
2015 SIAM International Conference on Data Mining, pp. 217-225. SIAM, 2015.
Gilbert Strang, Gilbert Strang, Gilbert Strang, and Gilbert Strang. Introduction to linear algebra,
volume 3. Wellesley-Cambridge Press Wellesley, MA, 1993.
Hongyu Su. Multilabel Classification through Structured Output Learning - Methods and Applica-
tions. Aalto University, 2015.
Charles Sutton and Andrew McCallum. An introduction to conditional random fields for relational
learning, volume 2. Introduction to statistical relational learning. MIT Press, 2006.
Chenhao Tan, Jie Tang, Jimeng Sun, Quan Lin, and Fengjiao Wang. Social action tracking via noise
tolerant time-varying factor graphs. In Proceedings of the 16th ACM SIGKDD international
conference on Knowledge discovery and data mining, pp. 1049-1058. ACM, 2010.
11
Under review as a conference paper at ICLR 2020
Marshall F Tappen, Ce Liu, Edward H Adelson, and William T Freeman. Learning gaussian con-
ditional random fields for low-level vision. In 2007 IEEE Conference on Computer Vision and
PatternRecognition,pp.1-8.1EEE, 2007.
Konstantinos Trohidis, Grigorios Tsoumakas, George Kalliris, and Ioannis P Vlahavas. Multi-label
classification of music into emotions. In ISMIR, volume 8, pp. 325-330, 2008.
Matt Wytock and Zico Kolter. Sparse gaussian conditional random fields: Algorithms, theory, and
application to energy forecasting. In International conference on machine learning, pp. 1265-
1273, 2013.
Peng Zhang, Ming Li, Yan Wu, and Hejing Li. Hierarchical conditional random fields model for
semisupervised sar image segmentation. IEEE Transactions on Geoscience and Remote Sensing,
53(9):4933-4951, 2015.
Haris Bin Zia, Agha Ali Raza, and Awais Athar. Urdu word segmentation using conditional random
fields (crfs). arXiv preprint arXiv:1806.05432, 2018.
12
Under review as a conference paper at ICLR 2020
A Derivation of lower bound of conditional likelihood
In this section we derive lower bound of conditional likelihood. In order to obtain suitable form
of joint distribution that can be easily integrated, the lower bound for sigmoid function was used
(Jaakkola & Jordan, 2000). The lower bound of joint distribution P(yj, zj |xj, θ) can be expressed
as:
P(yj, Zj |xj, θ) = P(yj |zj )P(Zj |xj, θ) ≥ P(yj, Zj |xj, θ, ξj)	(22)
P(yj, zj|xj, θ, ξj) = Y σ(ξji)exp fzjiyji - Zji + ξji — λ(ξji)(Z2i — ξ2i)) ∙
i=1	(23)
(2∏)N∕2 叫 W exp J 2(Zj - μj)T ς-1 (Zj-% )I
The simplified form of Eq. 23 can be represented by rearranging terms in the following form:
P(yj, zjlxj, θ, ξj) = T(ξj)eχp (zT(yj -11)- λzTzj -1ZTς-Izj + ZTς-IG	(24)
22
T(ξj)
(2∏)N∕2l∣∑J/2 Y °(MXP J1 ""-1〃j - P + λ(ξji)ξ2i)
(25)
The lower bound of likelihood P(yj |xj, θ, ξj) can be obtained by marginalization of Zj as:
P (yj |xj, θ, ξj) = / P (yj, zj |xj, θ, ξj )dzj
=T(ξj)/exp (ZT(yj -1 i)- AjzTzj -1ZTς-Izj + ZT£-1〃，dzj
=T(ξj) f exp (- jZT(∑-1 + 2Λj)Zj +
ZT(Σ-1 + 2Λj)(Σ-1 + 2Λj)-1((yj - 11) + Σ-1μj∙DdZj
(26)
The lower bound of likelihood P(yj |xj, θ, ξj) can be transformed in the following form:
P(yj|xj, θ, ξj) = T(ξj)/eχp 卜2(zj - mj)TS-I(zj - mj) + 1 mTSTmjdZj
=T(ξj)eχp (2mTS-Imj) /eχp (- 1(Zj - mj)TS-I(Zj - mj)) dzj
(27)
where S-1 = Σ-1 + 2Λj and mj = Σj ((yj∙ - 111) + Σ-1μj∙).
This integration is easily performed by noting that it is the integral over an unnormalized Gaussian
distribution, which yields:
P(yj|xj,θ,ξj) = (2n)N/2 区:/2T(ξj)exp(2mTS-1mj) |Sj|1/2	(28)
The final form of the lower bound of conditional log likelihood Lj(yj |xj, θ, ξj) is:
13
Under review as a conference paper at ICLR 2020
N
Lj(yj|xj,θ,ξj) = logP(yj|xj,θ,ξj) =E (logσ(ξji)-
i=1
2 〃州-1%+
争 + λKji)ξ2i)
—
(29)
2 mT S-Imj+ 2log ISj |
B Partial derivative of lower bound of conditional log
LIKELIHOOD
The partial derivative of lower bound of conditional log likelihood (GCRFBCb)
computed as:
dL (yj lxj,θ,ξj)
∂αk
is
where:
dL (yj |xj, θ, ξ)
dαk
1	dSj-1	dmjT	1	1 T
- 3Τr SjF+	Sj mj + mnj
2 j dαk	dαk j j 2 j
μT ∑-ι	1 T
- 百均 μj - 2 μj
江+
dak
∂S-
1	dΣ-
-Tr Σj-j
2	1dαk
dαk
1
1
mj
(30)
dmT
dak
—
∂S-1 _ dΣ-1
dak dak
yj - 2 I+μT ς-1') Sj
=
∂S-
2,
0,
1
if i = j
ifi 6=j
(31)
∂αk
∂μT _i	T
Sj + ∂0k J Sj + μj
dΣ-1
Yr Sj
(32)
dμ
dak
2αkRk(X)- *μ/丁 ∑T
dαk
(33)
Similarly partial derivatives with respect to β can be defined as:
dLj (yj |xj, θ, ξ)
∂βl
1	dS-i	dmT	1
- 2Tr SjM + 常S-Imj+ 2mT
μT _i 1 T ∂Σ-1
- 函 J μj - 2 μj 而T +
1	dΣ-
2 Trm F
dSj-i
可mj
i
(34)
where:
∂S-1 _ ∂Σ-1
^∂βΓ = ~∂K~
(
PnN=1 elinSiln(x),
-elijSilj(x), ifi 6= j
if i = j
(35)
dmjT
~βκ
—
1 T _A ∂S-1	dμT	T ∂Σ-1
yj-	2 i	+	μjJ )Sj 可 Sj	+ 访 J	Sj	+ μj FSj
(36)
dμT
—
dJj-i T T
~∂βΓ μj J
(37)
In the same manner partial derivatives of conditional log likelihood with respect to ξji are:
dL (yj |xj, θ, ξ)
dξji
-2 Tr 通
—
2 (yj - 2 I) Sj
Sj-1mj
T dΛj	N	1	1	dσ (ξji )	1	3
+ mj 两mj + X ((百 + 2j k + 2 (σ(ξji) - 4
(38)
14
Under review as a conference paper at ICLR 2020
where:
0	0	0	...	0
∂Λj
dξji
0
.
.
.
0
∂λ(ξji)
dξj
.
.
.
0
0
.
.
.
0
dσ(ξji)
∂ξij
σ(ξji)(1 - σ(ξji))
∂λ(ξji) _ 1 ∂σ(ξji)	1(“、	1、1
F=有 F- 2 ai)- 2)鼐
(39)
(40)
(41)
0
0
C Partial derivative of conditional log likelihood
The derivatives of the conditional log likelihood (GCRFBCnb) with respect to α and β are defined
as, respectively:	dLji(y"χj,仇 μji = (	"('a)誓	(42) ∂αk	∂αk dLji叼叼,θ,"ji) = (y∙i — σ(μji))等	(43) ∂αl	∂βl
where dj and dj are elements of the vectors 舞 and 符 and can be obtained by Eqs. 33 and 37,
respectively.
D Synthetic dataset results
In order to generate and label graph nodes, edge weights S and unstructured predictor values R were
randomly generated from uniform distribution. Besides, it was necessary to choose values of param-
eters α and β. Greater values of α indicate that the model is more confident about performance of
unstructured predictors, whereas for the larger value ofβ the model is putting more emphasis on the
dependence structure of output variables.
Six different values of parameters α andβ were used. In the first group α andβ have similar values,
so unstructured predictors and dependence structure between outputs have similar importance. In
the second group, α has higher values compared to β, which means that unstructured predictors
are more important than the dependence structure. In the third group β has higher values than α,
meaning that dependence structure is more important than unstructured predictors.
Along with the AUC and conditional log likelihood, norm of the variances of latent variables (di-
agonal elements in the covariance matrix) is evaluated and presented in Table 5. In addition, the
results of experiments are presented in Fig. 1, where for different values of α and β we show dif-
ferences between GCRFBCb and GCRFBCnb (a) AUC scores, (b) log likelihoods, and (c) norm of
the variances of latent variables.
15
Under review as a conference paper at ICLR 2020
Figure 1: Experimental evaluation of differences between GCRFBCb and GCRFBCnb (a) AUC
scores, (b) log likelihoods, and (c) norms of the variances of latent variables for different values of
α and β
Table 5: Comparison of GCRFBCb and GCRFBCnb prediction performance for different values of
α and β, as measured by AUC, log likelihood, and norm of diagonal elements of the covariance
matrix
No.	Parameters		GCRFBCb				GCRFBCnb	
		AUC	L (Y|X, θ)	kσk2	AUC	L (YX, θ
1	α = [5, 4] β =[5, 22]	0.812	-71.150	0.000	0.812	-71.151
2	α =[1,18] β = [1,18]	0.903	-75.033	0.001	0.902	-75.033
3	α = [22, 21] β = [5, 22]	0.988	-83.957	0.000	0.988	-83.957
4	α = [22, 21] β = [0.1,0.67]	0.866	-83.724	0.000	0.886	-83.466
5	α = [0.8,。5] β =[5, 22]	0.860	-83.353	34.827	0.817	-84.009
6	α = [0.2, 0.4] β = [1,18]	0.931	-70.692	35.754	0.821	-70.391
16