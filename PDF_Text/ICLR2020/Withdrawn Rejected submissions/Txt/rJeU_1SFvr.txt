Under review as a conference paper at ICLR 2020
LOGAN: Latent Optimisation for Generative
Adversarial Networks
Anonymous authors
Paper under double-blind review
Ab stract
Training generative adversarial networks requires balancing of delicate adversar-
ial dynamics. Even with careful tuning, training may diverge or end up in a bad
equilibrium with dropped modes. In this work, we introduce a new form of la-
tent optimisation inspired by the CS-GAN and show that it improves adversarial
dynamics by enhancing interactions between the discriminator and the generator.
We develop supporting theoretical analysis from the perspectives of differentiable
games and stochastic approximation. Our experiments demonstrate that latent op-
timisation can significantly improve GAN training, obtaining state-of-the-art per-
formance for the ImageNet (128 × 128) dataset. Our model achieves an Inception
Score (IS) of 148 and an Frechet Inception Distance (FID) of 3.4, an improve-
ment of 17% and 32% in IS and FID respectively, compared with the baseline
BigGAN-deep model with the same architecture and number of parameters.
1	Introduction
Generative Adversarial Nets (GANs) are implicit generative models that can be trained to match
a given data distribution. GANs were originally proposed and demonstrated for images by Good-
fellow et al. (2014). As the field of generative modelling has advanced, GANs have remained at
the frontier, generating high-fidelity images at large scale (Brock et al., 2018). However, despite
growing insights into the dynamics of GAN training, most recent advances in large-scale image
generation come from architectural improvements (Radford et al., 2015; Zhang et al., 2019), or reg-
ularisation focusing on particular parts of the model (Miyato et al., 2018; Miyato & Koyama, 2018).
Inspired by the compressed sensing GAN (CS-GAN; Wu et al., 2019), we further exploit the ben-
efit of latent optimisation in adversarial games using natural gradient descent to optimise the latent
variable z at each step of training, presenting a scalable and easy to implement approach to improve
the dynamical interaction between the discriminator and the generator. For clarity, we unify these
approaches as latent optimised GANs (LOGAN).
To summarise our contributions:
1.	We present a novel analysis of latent optimisation in GANs from the perspective of differ-
entiable games and stochastic approximation (Balduzzi et al., 2018; Heusel et al., 2017),
arguing that latent optimisation can improve the dynamics of adversarial training.
2.	Motivated by this analysis, we improve latent optimisation by taking advantage of efficient
second-order updates.
3.	Our algorithm improves the state-of-the-art BigGAN-deep model (Brock et al., 2018) by a
significant margin, without introducing any architectural change or additional parameters,
resulting in higher quality images and more diverse samples (Figure 1 and 2).
2	Background
2.1	Notation
We use θD and θG to denote the vectors representing parameters of the generator and discriminator.
We use x for images, and z for the latent source generating an image. The prime 0 is used to denote
1
Under review as a conference paper at ICLR 2020
(a)	(b)
(a)
Figure 2: Samples from BigGAN-deep (a) and LOGAN (b) with similarly low FID. Samples from
the two panels were drawn from truncation levels corresponding to points A and B in figure 3 b
respectively. (FID/IS: (a) 5.04/126.8, (b) 5.09/217.0)
Figure 1: Samples from BigGAN-deep (a) and LOGAN (b) with similarly high IS. Samples from
the two panels were drawn from truncation levels corresponding to points C and D in figure 3 b
respectively. (FID/IS: (a) 27.97/259.4, (b) 8.19/259.9)
(b)
a variable after one update step, e.g., θfD = θD - α df (ZdθDSG). p(x) and p(z) denote the data
distribution and source distribution respectively. Ep(x) [f (x)] indicates taking the expectation of
function f(x) over the distribution p(x).
2.2	Generative Adversarial Nets
A GAN consists of a generator that generates image X = G(Z; θG) from a latent source Z 〜p(z),
anda discriminator that scores the generated images as D(x; θD) (Goodfellow et al., 2014). Training
GANs involves an adversarial game: while the discriminator tries to distinguish generated samples
x = G (Z; θG) from data X 〜p(x), the generator tries to fool the discriminator. This procedure can
be summarised as the following min-max game:
min max Ez〜p(x) [hd (D (x； Θd))] + Ez〜p(z) [hg (D (G (Z； Θg) ； Θd))]	(1)
θD θG
2
Under review as a conference paper at ICLR 2020
Table 1: Comparison of model scores. BigGAN-deep results are reproduced from Brock et al.
(2018). “baseline” indicates our reproduced BigGAN-deep with small modifications. The 3rd and
4th columns are from the gradient descent (GD, ablated) and natural gradient descent (NGD) ver-
sions of LOGAN respectively. We report the Inception Score (IS, higher is better, Salimans et al.
2016) and Frechet Inception Distance (FID, lower is better, HeUsel et al. 2017).
BigGAN-DeeP baseline LOGAN (GD) LOGAN (NGD)
FID^^5.7 ± 0.3	4.92 ± 0.05^^4.86 ± 0.09	3.36 ± 0.14^^
IS 124.5 ± 2.0	126.6 ± 1.3 127.7 ± 3.5	148.2 ± 3.1
The exact form of h(∙) depends on the choice of loss function (Goodfellow et al., 2014; Arjovsky
et al., 2017; Nowozin et al., 2016). To simplify oUr presentation and analysis, we Use the Wasserstein
loss (Arjovsky et al., 2017), so that hD(t) = -t and hG(t) = t. Our experiments with BigGAN-
deep uses the hinge loss (Lim & Ye, 2017; Tran et al., 2017), which is identical to this form in its
linear regime. Our analysis can be generalised to other losses as in previous theoretical work (e.g.,
Arora et al. 2017). To simplify notation, we abbreviate f(z; θD, θG) = D (G (z; θG) ; θD), which
may be further simplified as f(z) when the explicit dependency on θD and θG can be omitted.
Training GANs requires carefully balancing updates to D and G, and is sensitive to both architecture
and algorithm choices (Salimans et al., 2016; Radford et al., 2015). A recent milestone is BigGAN
(and BigGAN-deep, Brock et al. 2018), which pushed the boundary of high fidelity image generation
by scaling up GANs to an unprecedented level. BigGANs use an architecture based on residual
blocks (He et al., 2016), in combination with regularisation mechanisms and self-attention (Saxe
et al., 2014; Miyato et al., 2018; Zhang et al., 2019).
Here we aim to improve the adversarial dynamics during training. We focus on the second term in
eq. 1 which is at the heart of the min-max game, with adversarial losses for D and G, which can be
written as
L(z)=[LD(z),LG(z)]T=[f(z),-f(z)]T	(2)
Computing the gradients with respect to θD and θG obtains the following gradient, which cannot be
expressed as the gradient of any single function (Balduzzi et al., 2018):
g
一∂Ld (Z) ∂Lg (Z)
_ ∂Θd ， ∂Θg
T
-∂f (Z)
_ ∂Θd ,
(3)
The fact that g is not the gradient of a function implies that gradient updates in GANs can exhibit
cycling behaviour which can slow down or prevent convergence. In Balduzzi et al. (2018), vector
fields of this form are referred to as the simultaneous gradient. Although many GAN models use
alternating update rules (e.g., Goodfellow et al. 2014; Brock et al. 2018), following the gradient
with respect to θD and θG alternatively in each step, they can still suffer from cycling, so we use the
simpler simultaneous gradient (eq. 3) for our analysis.
2.3	Latent Optimised GANs
Inspired by compressed sensing (Candes et al., 2006; Donoho, 2006), Wu et al. (2019) introduced
latent optimisation for GANs. We call this type of model latent-optimised GANs (LOGAN). Latent
optimization has been shown to improve the stability of training as well as the final performance for
medium-sized models such as DCGANs and Spectral Normalised GANs (Radford et al., 2015; Miy-
ato et al., 2018). Latent optimisation exploits knowledge from D to guide updates of Z. Intuitively,
the gradient Nz f (Z) = fz points in the direction that satisfies the discriminator D, which implies
better samples. Therefore, instead of using the randomly sampled Z 〜P(Z) Wu et al. (2019) uses
the optimised latent
Δz = α a，Z)	Z0 = Z + Δz	(4)
∂Z
in eq. 1 for training 1. The general algorithm is summarised in Algorithm 1 and illustrated in Figure 3
a. We develop the natural gradient descent form of latent update in Section 4.
1We use a single step of gradient-based optimisation during training, and justify this choice in section 3.
3
Under review as a conference paper at ICLR 2020
(a)
Figure 3: (a) Schematic of LOGAN. We first compute a forward pass through G and D with a
sampled latent z . Then, gradients from the generator loss (dashed red arrow) are used to compute an
improved latent, z0 . After this optimised latent code is used in a second forward pass, we compute
gradients of the discriminator back through the latent optimisation into the model parameters θD,
θG. These gradients are used to update the model. (b) Truncation curves illustrate the FID/IS trade-
off for each model by altering the range of the noise source p(z). GD: gradient descent. NGD:
natural gradient descent. Points A, B, C, D correspond to samples shown in Figure 1 and 2.
Algorithm 1 Latent Optimised GANs with Automatic Differentiation
Input: data distribution p(x), latent distribution p(z), D (∙; Θd), G (∙; Θg), learning rate a, batch
size N
repeat
Initialise discriminator and generator parameters θD, θG
for i = 1 to N do
Sample Z 〜p(z), X 〜p(x)
Compute the gradient D(GZz) and use it to obtain ∆z from eq. 4 (GD) or eq. 12 (NGD)
Optimise the latent z0 - [Z + ∆z], [∙] indicates clipping the value between -1 and 1
Compute generator loss L(Gi) = -D(G(z0))
Compute discriminator loss L(Di) = D(G(z0)) - D(x)
end for
Compute batch losses LG = N PN=I LG) and LD = ~N PN=I LD)
Update Θd and Θg with the gradients ∂θD, ∂LG
until reaches the maximum training steps
3	Analysis of the Algorithm
To understand how latent optimisation improves GAN training, we analyse LOGAN as a 2-player
differentiable game following Balduzzi et al. (2018); Gemp & Mahadevan (2018); Letcher et al.
(2019). The appendix provides a complementary analysis that relates LOGAN to unrolled GANs
(Metz et al., 2016) and stochastic approximation (Heusel et al., 2017; Borkar, 1997).
3.1	The Symplectic Gradient Adjustment (SGA)
An important problem with gradient-based optimization in GANs is that the vector-field generated
by the losses of the discriminator and generator is not a gradient vector field. It follows that gradient
descent is not guaranteed to find a local optimum and can cycle, which can slow down convergence
or lead to phenomena like mode collapse and mode hopping. Balduzzi et al. (2018); Gemp &
4
Under review as a conference paper at ICLR 2020
Mahadevan (2018) proposed Symplectic Gradient Adjustment (SGA) to improve the dynamics of
gradient-based methods in adversarial games. For a game with gradient g (eq. 3), define the Hessian
as the second order derivatives with respect to the parameters, H = Vθg. SGA uses the adjusted
gradient
g* = g + λ AT g where λ is a positive constant	(5)
and A = 2(H 一 HT) is the anti-symmetric component of the Hessian. Applying SGA to GANs
yields the adjusted updates (see Appendix B.1 for details):
g*
(∂2f (Z) AT ∂f(z)
∖∂Θg ∂Θd) ∂Θg
一 ∂f(z) , λ ( ∂2f(z) Y ∂f(z)
∂θG + ∖ΘDΘd ∂Θg)	∂Θd
T
(6)
Compared with g in eq. 3, the adjusted gradient g* has second-order terms reflecting the interactions
between D and G. SGA has been shown to significantly improve GAN training in basic examples
(Balduzzi et al., 2018), allowing faster and more robust convergence to stable fixed points (local
Nash equilibria). Unfortunately, SGA is expensive to scale because computing the second-order
derivatives with respect to all parameters is expensive.
Explicitly computing the gradients for the discriminator and generator at z0 after one step of latent
optimisation (eq. 4) obtains
dLD dLG
dθθD, dθG,
∂f (Z) ∂Θd	∂∆z + (∂θD	、T ∂f(z0) ∂	∂∆z ,	∂f (z0) 	：	 ∂Θg	一(∂∆z丫 ∖∂Θg )	∂f(z0) # ∂ ∆z	T	(7)
∂f(z0)	∂2 +α ∂z	f(z)、T ∂f(z)	∂f(z0)	_	(∂2f(z)	AT ∂f		#T
∂Θd		∂Θd )	∂z0				 ∂Θg	0 <∂z∂θG	∂	z	
(8)
In both equations, the first terms represent how f(z0) depends on the parameters directly and the
second terms represent how f(z0) depends on the parameters via the optimised latent source. For
the second equality, We substitute ∆z = α fz as the gradient-based update of Z and use df⅛) =
d∂Zz-). The original GAN's gradient (eq. 3) does not include any second-order term, since ∆z = 0
without latent optimisation. In LOGAN, these extra terms are computed by automatic differentiation
when back-propagating through the latent optimisation process (see Algorithm 1).
The SGA updates in eq. 6 and the LOGAN updates in eq. 8 are strikingly similar, suggesting that
the latent step used by LOGAN reduces the negative effects of cycling by introducing a symplectic
gradient adjustment into the optimization procedure. The role of the latent step can be formalized in
terms of a third player, whose goal is to help the generator, see appendix B for details.
Crucially, latent optimisation approximates SGA using only second-order derivatives with respect
to the latent z and parameters of the discriminator and generator separately. The second order terms
involving parameters of both the discriminator and the generator - which are extremely expensive
to compute - are not used. For latent z's with dimensions typically used in GANs (e.g., 128-256,
orders of magnitude less than the number of parameters), these can be computed efficiently. In short,
latent optimisation efficiently couples the gradients of the discriminator and generator, as prescribed
by SGA, but using the much lower-dimensional latent source z which makes the adjustment scalable.
An important consequence of reducing the rotational aspect of GAN dynamics is that it is possible to
use larger step sizes during training which suggests using stronger optimisers to fully take advantage
of latent optimisation. Latent optimisation can improve GAN training dynamics further by allowing
larger single steps ∆z towards the direction of fz) without overshooting.
3.2	Unrolling and stochastic approximation
Appendix B further explains how LOGAN relates to unrolled GANs (Metz et al., 2016) and stochas-
tic approximation. Our main finding is that latent optimisation accelerates the speed of updating D
relative to that of G, facilitating convergence according to Heusel et al. (2017) (see also Figure 4 b).
In particular, the generator requires less update compared with D to achieve the same reduction of
loss, because latent optimisation “helps” G.
5
Under review as a conference paper at ICLR 2020
Figure 4: (a) Scaling of gradients in natural gradient descent. We use β = 5 in BigGAN-Deep
experiments. (b) The update speed of the discriminator relative to the generator shown as the dif-
ference k∆θD k - k∆θG k after each update step. Lines are smoothed with moving average using
window size 20 (in total, there are 3007, 1659 and 1768 data points for each curve). For all curves
oscillation strongly after training collapsed.
4	LOGAN with Natural Gradient Descent
Although our analysis suggests using strong optimisers for optimising z, Wu et al. (2019) only used
basic gradient descent (GD) with a fixed step-size. This choice limits the size ∆z can take: in
order not to overshoot when the curvature is large, the step size would be too conservative when
the curvature is small. We hypothesis that GD is more detrimental for larger models, which have
complex loss surfaces with highly varying curvatures. Consistent with this hypothesis, we observed
only marginal improvement over the baseline using GD (section 5.3, Table 1, Figure 3 b).
In this work, we instead use natural gradient descent (NGD, Amari 1998) for latent optimisation.
NGD can be seen as an approximate second-order optimisation method (Pascanu & Bengio, 2013;
Martens, 2014), and has been applied successfully in many domains. By using the positive semi-
definite (PSD) Gauss-Newton matrix to approximate the (possibly negative definite) Hessian, NGD
often works even better than exact second-order methods. NGD is expensive in high dimensional pa-
rameter spaces, even with approximations (Martens, 2014). However, we demonstrate it is efficient
for latent optimisation, even in very large models.
Given the gradient of z, g = fZZ), NGD computes the update as
∆z = α F-1 g	(9)
where the Fisher information matrix F is defined as
F = Ep(t∣z) [V lnp(t∣z) Vlnp(t∣z)T]	(10)
The log-likelihood function lnp(t|z) typically corresponds to commonly used error functions such
as cross entropy loss. This correspondence is not necessary when NGD is interpreted as an ap-
proximate second-order method, as has long been used in practice (Martens, 2014). Nevertheless,
Appendix C provides a Poisson log-likelihood interpretation for the hinge loss commonly used in
GANs (Lim & Ye, 2017; Tran et al., 2017). An important difference between latent optimisation
and commonly seen senarios using NGD is that the expectation over the condition (z) is absent.
Since each z is only responsible for generating one image, it only minimises the loss LG(z) for this
particular instance. Computing per-sample Fisher this way is necessary to approximate SGA (see
Appendix B.1 for details).
More specifically, we use the empirical Fisher F0 with Tikhonov damping, as in TONGA (Roux
et al., 2008)
F0 = g ∙ gT + β I	(II)
F0 is cheaper to compute compared with the full Fisher, since g is already available. The damping
factor β regularises the step size, which is important when F0 only poorly approximates the Hessian
6
Under review as a conference paper at ICLR 2020
Sd8⅛UaAMBq φ>os
(a)
Figure 5: (a) The change from ∆z across training, in D’s output space and z’s Euclidean space.
The distances are normalised by their standard derivations computed from a moving window of size
20 (1007 data points in total). (b) Training curves from models with different “stop_gradient”
operations. For reference, the training curve from an unablated model is plotted as the dashed line.
All instances with stop_gradient collapsed (FID went UP) early in training.
or when the Hessian changes too much across the step. Using the Sherman-Morrison formula, the
NGD update can be simplified into the following closed form:
∆z = α
G- β2 +βgTg)
1-
kgk2 )
β + kgk2J
(12)
α
g = β
g
which does not involve any matrix inversion. Thus, NGD adapts the step size according to the
curvature estimate C = 1(1 -6+羽产).Figure 4 a illustrates the scaling for different values of
β . NGD automatically smooths the scale of updates by down-scaling the gradients as their norm
grows, which also contributes to the smoothed norms of updates (Figure 4 b). Since the NGD
update remains proportional to g, our analysis based on gradient descent in section 3 still holds.
5	Experiments and Analysis
We focus on large scale GANs based on BigGAN-deep (Brock et al., 2018) trained on 128 × 128
size images from the ImageNet dataset (Deng et al., 2009). In Appendix E, we present results from
applying our algorithm on Spectral Normalised GANs trained with CIFAR dataset (Krizhevsky et al.,
2009), which obtains state-of-the-art scores on this model.
5.1	Model Configuration
We used the standard BigGAN-deep architecture with three minor modifications: 1. We increased
the size of the latent source from 128 to 256, to compensate the randomness of the source lost
when optimising z. 2. We use the uniform distribution U (-1, 1) instead of the standard normal
distribution N(0, 1) forp(z), to be consistent with the clipping operation (Algorithm 1). 3. We use
leaky ReLU instead of ReLU as the non-linearity for smoother gradient flow for fz ∙
Consistent with detailed findings in Brock et al. (2018) that these changes have limited effect, our
experiment with this baseline model obtains only slightly better scores compared with those in Brock
et al. (2018) (Table 1, see also Figure 8). The FID and IS are computed as in Brock et al. (2018), and
IS values are computed from checkpoints with the lowest FIDs. The means and standard deviations
are computed from 5 models with different random seeds.
To apply latent optimisation, we use a damping factor β = 5.0 combined with a large step size of
α = 0.9. As an additional way of damping, we only optimise 50% of z’s dimensions. Optimising
the entire population of z was unstable in our experiments. Similar to Wu et al. (2019), we found
it was helpful to regularise the Euclidean norm of weight-change ∆z, with a regulariser weight of
7
Under review as a conference paper at ICLR 2020
300.0. All other hyper-parameters, including learning rates and a large batch size of 2048, remain
the same as in BigGAN-deep; we did not optimise these hyper-parameters. We call this model
LOGAN (NGD).
5.2	Basic Results
Employing the same architecture and number of parameters as the BigGAN-deep baseline, LOGAN
(NGD) achieved better FID and IS (Table 1). As observed by Brock et al. (2018), BigGAN training
always eventually collapsed. Training with LOGAN also collapsed, perhaps due to higher-order
dynamics beyond the scope we have analysed, but took significantly longer (600k steps versus 300k
steps with BigGAN-deep).
During training, LOGAN was 2 - 3 times slower per step compared with BigGAN-deep because
of the additional forward and backward pass. We found that optimising z during evaluation did
not improve sample scores (even up to 10 steps), so we do not optimise z for evaluation. There-
fore, LOGAN has the same evaluation cost as original BigGAN-deep. To help understand this
behaviour, we plot the change from ∆z during training in Figure 5 a. Although the movement in
Euclidean space k∆zk grew until training collapsed, the movement in D’s output space, measured
as kf(z + ∆z) - f(z)k, remained unchanged (see Appendix D for details). As shown in our anal-
ysis, optimising z improves the training dynamics, so LOGANs work well after training without
requiring latent optimisation.
5.3	Ablation Studies
We verify our theoretical analysis in section 3 by examining key components of Algorithm 1 via
ablation studies. First, we experimented with using basic GD to optimising z, as in Wu et al. (2019),
and call this model LOGAN (GD). A smaller step size of α = 0.0001 was required; larger values
were unstable and led to premature collapse of training. As shown in Table 1, the scores from
LOGAN (GD) were worse than LOGAN (NGD) and similar to the baseline model.
We then evaluate the effects of removing those terms depending on f(Z) in eq. 8, which are not in
the ordinary gradient (eq. 3). Since these terms were computed when back-propagating through
the latent optimisation procedure, we removed them by selectively blocking back-propagation
with “stop_gradient” operations (e.g., in TensorFlow Abadi et al. 2016). Figure 5 b shows
the change of FIDs for the three models corresponding to removing
∂f(z0)
-fτ2, removing
df∂zZ00) and removing both terms. As predicted by our analysis (section 3), both terms help
stabilise training; training diverged early for all three ablations.
5.4	Truncation and Samples
Truncation is a technique introduced by Brock et al. (2018) to illustrate the trade-off between the
FID and IS in a trained model. For a model trained with Z 〜P(Z) from a source distribution
symmetric around 0, such as the standard normal distribution N(0, 1) and the uniform distribution
U(-1,1), down-scaling (truncating) the source W = S ∙ Z with 0 ≤ S ≤ 1 gives samples with higher
visual quality but reduced diversity. This observation is quantified as higher IS and lower FID when
evaluating samples from truncated distributions.
Figure 3 b plots the truncation curves for the baseline BigGAN-deep model, LOGAN (GD) and
LOGAN (NGD), obtained by varying the truncation (value of S) from 1.0 (no truncation, upper-left
ends of the curves) to 0.02 (extreme truncation, bottom-right ends). Each curve shows the trade-off
between FID and IS for an individual model; curves towards the upper-right corner indicate better
overall sample quality. The relative positions of curves in Figure 3 (b) shows LOGAN (NGD) has
the best sample quality. Interestingly, although LOGAN (GD) and the baseline model have similar
scores without truncation (upper-left ends of the curves, see also Table 1), LOGAN (GD) was better
behaved with increasing truncation, suggesting LOGAN (GD) still converged to a better equilibrium.
For further reference, we plot truncation curves from additional baseline models in Figure 8.
8
Under review as a conference paper at ICLR 2020
Figure 1 and Figure 2 show samples from chosen points on the truncation curves. In the high IS
domain, C and D on the truncation curves both have similarly high IS of near 260. Samples from
batches with such high IS have almost photo-realistic image quality. Figure 1 show that while
the baseline model produced nearly uniform samples, LOGAN (NGD) could still generate highly
diverse samples. On the other hand, A and B from Figure 3 b have similarly low FID of near 5,
indicating high sample diversity. Samples in Figure 2 b show higher quality compared with those in
a (e.g., the interfaces between the elephants and ground, the contours around the pandas).
6	Conclusion
In this work we present the LOGAN model which significantly improves the state-of-the-art on large
scale GAN training for image generation by online optimising the latent source z. Our results illus-
trate improvements in quantitative evaluation and samples with higher quality and diversity. More-
over, our analysis suggests that LOGAN fundamentally improves adversarial training dynamics. We
therefore expect our method to be useful in other tasks that involve adversarial training, including
representation learning and inference (Donahue et al., 2017; Dumoulin et al., 2017), text generation
(Zhang et al., 2019), style learning (Zhu et al., 2017; Karras et al., 2019), audio generation (Donahue
et al., 2018) and video generation (Vondrick et al., 2016; Clark et al., 2019).
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-
scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Imple-
mentation ({OSDI} 16),pp. 265-283, 2016.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251—
276, 1998.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. arXiv preprint
arXiv:1701.07875, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pp. 224-232. JMLR. org, 2017.
David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Grae-
pel. The mechanics of n-player differentiable games. arXiv preprint arXiv:1802.05642, 2018.
Vivek S Borkar. Stochastic approximation with two time scales. Systems & Control Letters, 29(5):
291-294, 1997.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. arXiv preprint arXiv:1809.11096, 2018.
Emmanuel J Candes, Justin K Romberg, and Terence Tao. Stable signal recovery from incomplete
and inaccurate measurements. Communications on Pure and Applied Mathematics: A Journal
Issued by the Courant Institute of Mathematical Sciences, 59(8):1207-1223, 2006.
Aidan Clark, Jeff Donahue, and Karen Simonyan. Efficient video generation on complex datasets.
arXiv preprint arXiv:1907.06571, 2019.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09, 2009.
Chris Donahue, Julian McAuley, and Miller Puckette. Adversarial audio synthesis. arXiv preprint
arXiv:1802.04208, 2018.
JeffDonahue, Philipp Krahenbuhl, and Trevor Darrell. Adversarial feature learning. In ICLR, 2017.
David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289-
1306, 2006.
9
Under review as a conference paper at ICLR 2020
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Ar-
jovsky, and Aaron Courville. Advesarially learned inference. In ICLR, 2017.
Ian Gemp and Sridhar Mahadevan. Global Convergence to the Equilibrium of GANs using Varia-
tional Inequalities. In Arxiv:1808.01531, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing Systems,pp. 2672-2680, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems, pp. 6626-6637, 2017.
Morris W Hirsch. Convergent activation dynamics in continuous time networks. Neural networks,
2(5):331-349, 1989.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4401-4410, 2019.
Vijaymohan R Konda and Vivek S Borkar. Actor-critic-type learning algorithms for markov deci-
sion processes. SIAM Journal on control and Optimization, 38(1):94-123, 1999.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Alistair Letcher, David Balduzzi, Sebastien Racaniere,James Martens, Jakob N Foerster, Karl Tuyls,
and Thore Graepel. Differentiable game mechanics. Journal of Machine Learning Research, 20
(84):1-40, 2019.
Jae Hyun Lim and Jong Chul Ye. Geometric GAN. arXiv preprint arXiv:1705.02894, 2017.
James Martens. New insights and perspectives on the natural gradient method. arXiv preprint
arXiv:1412.1193, 2014.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. CoRR, abs/1611.02163, 2016. URL http://arxiv.org/abs/1611.02163.
Takeru Miyato and Masanori Koyama. cGANs with projection discriminator. arXiv preprint
arXiv:1802.05637, 2018.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f -GAN: Training generative neural sam-
plers using variational divergence minimization. In Advances in neural information processing
systems, pp. 271-279, 2016.
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv preprint
arXiv:1301.3584, 2013.
Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147-160,
1994.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Nicolas L Roux, Pierre-Antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural gra-
dient algorithm. In Advances in neural information processing systems, pp. 849-856, 2008.
10
Under review as a conference paper at ICLR 2020
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in neural information processing systems,
pp. 2234-2242, 2016.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. ICLR, 2014.
Dustin Tran, Rajesh Ranganath, and David Blei. Hierarchical implicit models and likelihood-free
variational inference. In Advances in Neural Information Processing Systems, pp. 5523-5533,
2017.
Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics.
In Advances In Neural Information Processing Systems, pp. 613-621, 2016.
Yan Wu, Mihaela Rosca, and Timothy Lillicrap. Deep compressed sensing. In International Con-
ference on Machine Learning, pp. 6850-6860, 2019.
Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative
adversarial networks. In International Conference on Machine Learning, pp. 7354-7363, 2019.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference
on computer vision, pp. 2223-2232, 2017.
A Additional Samples and Results
Figure 6 and 7 provide additional samples, organised similarly as in Figure 1 and 2. Figure 8 shows
additional truncation curves.
B Detailed Analysis of Latent Optimisation
In this section we present three complementary analyses of LOGAN. In particular, we show how the
algorithm brings together ideas from symplectic gradient adjustment, unrolled GANs and stochastic
approximation with two time scales.
B.1 Approximate Symplectic Gradient Adjustment
To analyse LOGAN as a differentiable game we treat the latent step ∆z as adding a third player
to the original game played by the discriminator and generator. The third player’s parameter, ∆z,
is optimised online for each Z 〜p(z). Together the three players (latent player, discriminator, and
generator) have losses averaged over a batch of samples:
L = [ηLG, LD,LG]T	(13)
where η = 得(N is the batch size) reflects the fact that each ∆z is only optimised for a single
sample z, so its contribution to the total loss across a batch is small compared with θD and θG which
are directly optimised for batch losses. This choice ofη is essential for the following derivation, and
has important practical implication. It means that the per-sample loss LG(z0), instead of the loss
summed over a batch PnN=1 LG (zn0 ), should be the only loss function guiding latent optimisation.
Therefore, when using natural gradient descent (Section 4), the Fisher information matrix should
only be computed using the current sample z .
The resulting simultaneous gradient is
(14)
“=In ∂Lg(z0) ∂Ld(z') ∂Lg(z') ]T =「_ ∂f (z') ∂f(z')	∂f (z') iT
g = η∣ ∂∆z , ∂Θd , ∂Θg J = η ∂∆z , ∂Θd , ∂Θg J
Following Balduzzi et al. (2018), we can write the Hessian of the game as:
H=	Γ-n d2f(z0) η ∂∆z2 ∂2f(z0)	d2f(z0) η ∂∆z∂θD ∂2f(z0)	d2f(z0)] η ∂∆z∂θG ∂2f(z0)
	∂θ d∂ ∆z	∂θD	∂Θd ∂Θg
	∂2f(z0)	∂2f(z0)	∂2f(z0)
		 -- - _ ∂Θg∂Δz		 -- -- ∂Θg∂Θd	--dθGΓ J
(15)
11
Under review as a conference paper at ICLR 2020
The presence of a non-zero anti-symmetric component in the Hessian
A =1(H — H T)
0
ι+η ∂2f(z0)
2 ∂θ D ∂∆z
i-η ∂2f(z')
----- -- -
2 ∂Θg∂Δz
ι+η ∂2f(z0)
2	∂Δz∂Θd
i-η ∂2f(z0),
0
∂2f(z0)
∂Θg∂Θd
2	∂∆z∂θG
∂2f(z0)
∂θD ∂θG
0
(16)
implies the dynamics have a rotational component which can cause cycling or slow down conver-
gence. Since η《1 for typical batch sizes (e.g.,* for DCGAN and ^^ for BigGAN-deep), We
abbreviate Y = 1++η ≈ 1-η to simplify notations.
Symplectic gradient adjustment (SGA) counteracts the rotational force by adding an adjustment
term to the gradient to obtain g* J g + λATg, which for the discriminator and generator has the
form:
*	= ∂f(z0)	ι λ	( ∂2f(z0) AT	∂f(z0)	ι λ ( ∂2f(z0) AT	∂f(z0)
gD = ∂Θd	+ λγ	∖∂Δz∂Θd)	∂∆z	+	[∂θG ∂Θd)	∂Θg
*	= ∂f(z0)	( ∂2f(z0) AT ∂f(Z)	( ∂f(z0) AT ∂f(z0)
gG =	∂Θg	Y V∂Δz∂ΘgJ	∂∆z + 1∂θD ∂Θg)	∂θD
(17)
(18)
The gradient with respect to ∆z is ignored since the convergence of training only depends on θD
and θG.
If we drop the last terms in eq.17 and 18, which are expensive to compute for large models with
high-dimensional θD and Θg, and use df∆Z) =	), the adjusted updates can be rewritten as
* 〜∂f(z0L∖ ∂22f(z')∖T ∂f(z0)
gD ≈ "∂θΓ + λγ (汨次D) 石厂
*
gG* ≈ -
∂f (z0) M
F-λγ
(∂2f (z0) AT ∂f(z0)
V∂z0∂ΘgJ	∂z0
(19)
(20)
Because of the third player, there are still the terms depend on ) to adjust the gradients. Effi-
ciently computing Bzfdz工 and "∂¾ is non-trivial (e.g., Pearlmutter 1994). However, if we intro-
duce the local approximation
∂2f(z0) ≈ ∂2f(z) ∂2f(z0) ≈ ∂2f(Z)
∂z0∂θD 〜∂z∂θD ∂z0∂θD 〜∂z∂θD
(21)
then the adjusted gradient becomes identical to 8 from latent optimisation.
In other words, automatic differentiation by commonly used machine learning packages can com-
pute the adjusted gradient for θD and θG when back-propagating through the latent optimisation
process. Despite the approximation involved in this analysis, both our experiments in section 5 and
the results from Wu et al. (2019) verified that latent optimisation can significantly improve GAN
training.
B.2	Relation with Unrolled GANs
Latent optimisation can be seen as unrolling GANs (Metz et al., 2016) in the space of the latent,
rather than the parameters. Unrolling in the latent space has the advantages that:
1.	LOGAN is more scalable than Unrolled GANs because it avoids second-order derivatives
over a potentially very large number of parameters.
2.	While unrolling the update of D only affects the parameters of G (as in Metz et al. 2016),
latent optimisation effects both D and G as shown in eq. 8.
We next formally present this connection by showing that SGA can be seen as approximating Un-
rolled GANs (Metz et al., 2016). For the update θD0 = θD ι ∆θD, we have the Taylor expansion
approximation at θD :
f (z; θD + ∆θD,Θg) ≈ f(z; ΘD,Θg)+ (df (ZdθD,θG) A	∆θD	(22)
12
Under review as a conference paper at ICLR 2020
Substitute ∆θ0 = -α df(z∂θDSGG, and take the derivatives with respect to Θg on both sides:
∂f(z; Θd + ΔΘd,Θg) ≈ ∂f (z; Θd,Θg ) _ 2 ( ∂ 2f(z; Θd,Θg) Y ∂f(z; Θd,Θg)
∂Θg	〜	∂Θg	∂ 1	∂Θd ∂Θg )	∂Θd
(23)
which is the same as eq. 18 (taking the negative sign). Compared with the exact gradient from the
unroll:
∂f(z; OD + ΔΘd,Θg) = ∂f (z; θD ,Θg ) - 2 ( ∂ 2f(z; Θd,Θg) A T ∂f(z; θD ,Θg)
∂Θg	∂Θg	Q V ∂Θd∂Θg )	∂(θD)
(24)
The approximation in eq. 23 comes from using df(z∂θDSGG ≈ "(¾3,θG) and "(啜?SGG ≈
"嚼D'θG) as a result of the linear approximation.
At this point, unrolling D update only affects ΘD . Although it is expensive to unroll both D and
G, in principle, we can unroll G update and compute the gradient of ΘD similarly using ∆ΘG
α
∂f(zfD ,Θg ).
∂θG	:
df(z； θd,θg + δθg) ≈ f (Z θD,θG) + 2α (df(Z； θD,θG)「df(z； θD,θG)
∂Θd	∂Θd	V ∂Θg∂Θd	j	∂Θg
(25)
which gives us the same update rule as SGA (eq. 17). This correspondence based on first order
Taylor expansion is unsurprising, as SGA is based on linearising the adversarial dynamics (Balduzzi
et al., 2018).
B.3	Stochastic Approximation with Two Time Scales
Heusel et al. (2017) used the theory of stochastic approximation to analyse GAN training. View-
ing the training process as stochastic approximation with two time scales (Borkar, 1997; Konda &
Borkar, 1999), they suggest that the update of D should be fast enough compared with that of G.
Under mild assumptions, Heusel et al. (2017) proved that such two time-scale update converges to
local Nash equilibrium. Their analysis follows the idea of (τ, δ) perturbation (Hirsch, 1989), where
the slow updates (G) is interpreted as a small perturbation over the ODE describing the fast update
(D). Importantly, the size of perturbation δ is measured in the magnitude of parameter change,
which is affected by both the learning rate and gradients.
Here we show that LOGAN accelerates discriminator updates and slows down generator updates,
thus helping the convergence of discriminator according to Heusel et al. (2017). We start from
analysing the change of ΘG. We assume that, without LO, it takes ∆ΘG = ΘG0 - ΘG to make a small
constant amount of reduction in loss LG:
ρ= -f(z; ΘD, ΘG + ∆ΘG) + f(z; ΘD, ΘG)	(26)
Now using the optimised z0 = z + ∆z, we assess the change δΘG required to achieve the same
amount of reduction:
P = -f (z + ∆z; Θd, Θg + δθG) + f (z; Θd, Θg)	(27)
Intuitively, when z “helps” ΘG to achieve the same goal ofincreasingf(z; ΘD, ΘG) by ρ, the respon-
sible of ΘG becomes smaller, so it does not need to change as much as ∆ΘG, thus kδΘGk < k∆ΘGk.
Formally, f(z; ΘD, ΘG) and f(z + ∆; ΘD, ΘG + δΘG) have the following Taylor expansions around
z and ΘG :
f (z; θd,θG + δθG)=f(z; Θd,Θg)+ ( df (Zd θD,θG)) ΔΘg + e(ΔΘg)	(28)
f (z + ∆z; θd, Θg + δθG) =f (z; Θd ,Θg) + ( f (*%))
八,∂df (z + δz; θD,θG )∖ TXA 「八 ʌn ʌ	zɔɑʌ
∆z + I -------∂θ^------ ) δθG + e(∆z, 6Θg)	(29)
13
Under review as a conference paper at ICLR 2020
Where e(∙),s are higher order terms of the increments. Using the assumption of eq. 26 and 27, We
can combine eq. 28 and 29:
(f∂DM )T ∆θG = (∂f⅛θG) )T ∆z + ( d (Z + δFθg )t δθG + e (30)
where C = e(∆z, δθc) - e(∆θc). Since ∆z H df(^θD,θG) in gradient descent (eq. 3),
f " W ∆Z> 0	(31)
∂z
Therefore, we have the inequality
(f⅛θ^)T∆θG < (df(Z + ∆θGθD,θG))T δθG + C	(32)
If we further assume ∆θG and δθG are obtained from stochastic gradient descent with identical
learning rate,
∂f (z; Θd,Θg)	∂f (z; Θd,Θg)
δθg = α —∂θG—	δθG = α —∂θG—	(33)
substituting eq. 33 into eq. 32 gives
k∆θG k < kδθG k + C
(34)
The same analysis applies to the discriminator. The similar intuition is that it takes the discriminator
additional effort to compensate the exploitation from the optimised Z0 . We then obtain
(∂f⅛θD^y ∆θD = (∂f¾^y ∆Z + ( df (Z + 蓝 θD,θG) )T δθD + C (35)
However, since the adversarial loss LD = -Lg, we have ΔΘd = -α df(ZdθDSG) and 5Θd =
-a df(z∂θD,θG) taking the opposite signs of eq.33. For sufficiently small Δz, ΔΘg and 6Θg, C is
close to zero, so k∆θDk < kδθD k under our assumptions of small ∆Z, ∆θG and δθG.
Importantly, the bigger the product fz Δz is, the more robust the inequality is to the error from
C. Moreover, bigger step increases the speed gap between updating D and G, further facilitating
convergence according to Heusel et al. (2017). Overall, our analysis suggests:
1.	More than one gradient descent step may not be helpful, since ∆Z from multiple GD steps
may deviate from the direction of fz).
2.	Large step of ∆Z is more helpful in facilitating convergence by widening the gap between
D and G updates (Heusel et al., 2017).
3.	However, the step of ∆Z cannot be too large. In addition to the linear approximation we
used throughout our analysis, the approximate SGA breaks down when eq.21 is strongly
violated when “overshoot” brings the gradients at "：0) to the opposite sign of dfz).
C Poisson Likelihood from Hinge loss
Here we provide a probabilistic interpretation of the hinge loss for the generator, which leads natu-
rally to the scenario of a family of discriminators. Although this interpretation is not necessary for
our current algorithm, it may provides useful guidance for incorporating multiple discriminators.
We introduce the label t = 1 for real data and t = 0 fake samples. This section shows that the
generator hinge loss
LG = -D (G(Z))	(36)
can be interpreted as a negative log-likelihood function:
LG = - ln p(t = 1; D, G(Z))	(37)
14
Under review as a conference paper at ICLR 2020
Here p(t = 1; z, D, G) is the probability that the generated image G(z) can fool the discriminator
D.
The original GAN’s discriminator can be interpreted as outputting a Bernoulli distribution
p(t; Bg) = βG ∙ (1 - Bg)∖t. In this case, if We parameterise Bg = D (G(Z)), the generator
loss is the negative log-likelihood
—lnP(t = 1; D, G(Z)) = — Inp(t = 1; Bg) = — InBg = — In D (G(Z))	(38)
Bernoulli, hoWever, is not the only valid choice as the discriminator’s output distribution. Instead of
sampling “1” or “0”, We assume that there are many identical discriminators that can independently
vote to reject an input sample as fake. The number of votes k in a given interval can be described by
a Poisson distribution With parameter λ With the folloWing PMF:
λk e-λ
P(k； λ) =	,∣	(39)
k!
The probability that a generated image can fool all the discriminators is the probability of G(Z)
receiving no vote for rejection
p(k = 0; λ) = e-λ	(40)
Therefore, We have the folloWing negative log-likelihood as the generator loss if We parameterise
λ= —D (G(Z)):
—lnp(k = 0; D,G(z)) = — lnp(k = 0; λ) = -D (G(z))	(41)
This interpretation has a caveat that When D (G(Z)) > 0 the Poisson distribution is not Well defined.
HoWever, in general the discriminator’s hinge loss
LD = — min(0, —1 + D(X)) — min(0, —1 — D(G(Z)))	(42)
pushes D (G(Z)) < 0 via training.
D Details in Computing Distances in Figure 5 A
For a temporal sequence x1, x2, . . . , xT (changes of Z or f(Z) at each training step in this paper), to
normalise its variance While accounting for the non-stationarity, We process it as folloWs. We first
compute the moving average and standard deviation over a WindoW of size N:
1 t+N -1
μt = N Σ X
u=
(43)
u 1	t+N -1
σt = t N _ 1 E (Xu — μu)2	(44)
u=t
Then normalise the sequence as:
Xt
Xt = —	(45)
σt
The result in Figure 5 a is robust to the choice of WindoW size. Our experiments With N from 10 to
50 yielded visually similar plots.
E	Experiments with DCGAN and CIFAR
To test if latent optimisation Works With models at more moderate scales, We applied it on SN-GANs
(Miyato et al., 2018). Although our experiments on this model are less thorough than in the main
paper With BigGAN-deep, We hope to provide basic guidelines for researchers interested in applying
latent optimisation on smaller models.
The experiments folloWs the same basic setup and hyper-parameter settings as the CS-GAN in Wu
et al. (2019). There is no class conditioning in this model. For NGD, We found a smaller damping
15
Under review as a conference paper at ICLR 2020
factor β = 0.1, a kzk regulariser weight of 3.0 (the same as in Wu et al. 2019), combined with
optimising 70% of the latent source (instead of 50% for BigGAN-deep) worked best for SN-GANs.
In addition, we found running extra latent optimisation steps benefited evaluation, so we use ten steps
of latent optimisation in evaluation for results in this section, although the models were still trained
with a single optimisation step. We reckon that smaller models might not be “over-parametrised”
enough to fully amortise the computation from optimising z, which can then further exploit the
architecture in evaluation time. On the other hand, the overhead from running multiple iterations of
latent optimisation is relatively small at this scale. We aim to further investigate this difference in
future studies.
Table 2 shows the FID and IS alongside SN-GAN and CS-CAN which used the same architec-
ture. Here we observe similarly significant improvement over the baseline SN-GAN model, with
an improvement of 16.8% in IS and 39.6% in FID. Figure 9 shows random samples from these two
models. Overall, samples from LOGAN (NGD) have higher contrasts and sharper contours.
Table 2: Comparison of Scores. The first and second columns are reproduced from Miyato et al.
(2018) and Wu et al. (2019) respectively. We report the Inception Score (IS, higher is better, Sali-
mans et al. 2016) and Frechet Inception Distance (FID, lower is better, HeUsel et al. 2017).
SN-GAN
CS-GAN
LOGAN (NGD)
FID
IS
29.3	23.1 ± 0.5	17.7 ± 0.4
7.42 ± 0.08	7.80 ± 0.05	8.67 ± 0.05
16
Under review as a conference paper at ICLR 2020
Figure 6: Samples from BigGAN-deep (a) and LOGAN (b) with the similarly high inception scores.
Samples from the two panels were draw from truncations correspond to points C, D in figure 3 b
respectively. (FID/IS: (a) 27.97/259.4, (b) 8.19/259.9)
17
(b)
Under review as a conference paper at ICLR 2020
(a)	(b)
Figure 7: Samples from BigGAN-deep (a) and LOGAN (b) with the similarly low FID. Samples
from the two panels were draw from truncations correspond to points A, B in figure 3 b respectively.
(FID/IS: (a) 5.04/126.8, (b) 5.09/217.0)
18
Under review as a conference paper at ICLR 2020
Figure 8: Truncation curves with additional baselines. In addition to the truncation curves reported
in Figure 3 b, here we also include the Spectral-Normalised GAN (Miyato et al., 2018), Self-
Attention GAN (Zhang et al., 2019), original BigGAN and BigGAN-deep as presented in Brock
et al. (2018).
(a)
(b)
Figure 9: (a) Samples from SN-GAN. (b) Samples from LOGAN.
19