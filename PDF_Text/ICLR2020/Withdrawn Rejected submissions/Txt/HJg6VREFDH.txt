Under review as a conference paper at ICLR 2020

IWGAN: AN  AUTOENCODER  WGAN FOR  INFERENCE

Anonymous authors

Paper under double-blind review

ABSTRACT

Generative Adversarial Networks (GANs) have been impactful on many problems
and applications but suffer from unstable training.  Wasserstein GAN (WGAN)
leverages the Wasserstein distance to avoid the caveats in the minmax two-player
training of GANs but has other defects such as mode collapse and lack of metric
to detect the convergence.  We introduce a novel inference WGAN (iWGAN)
model, which is a principled framework to fuse auto-encoders and WGANs. The
iWGAN jointly learns an encoder network and a generative network using an
iterative primal dual optimization process. We establish the generalization error
bound of iWGANs. We further provide a rigorous probabilistic interpretation of
our model under the framework of maximum likelihood estimation. The iWGAN,
with a clear stopping criteria, has many advantages over other autoencoder GANs.
The empirical experiments show that our model greatly mitigates the symptom of
mode collapse, speeds up the convergence, and is able to provide a measurement of
quality check for each individual sample. We illustrate the ability of iWGANs by
obtaining a competitive and stable performance with state-of-the-art for benchmark
datasets.

1    INTRODUCTION

One of the goals of generative modeling is to match the model distribution Pθ(x) with parameters
θ to the true data distribution PX for a random variable X         .  For latent variable models, 
the
data point X is generated from a latent variable Z         through a conditional distribution P (X 
Z).
Here      denotes the support for PX and     denotes the support for PZ.  There has been a surge of
research on deep generative networks in recent years and the literature is too vast to summarize 
here
(Kingma & Welling, 2013; Goodfellow et al., 2014; Li et al., 2015). These models have provided
a powerful framework for modeling complex high dimensional datasets. We start introducing two
main approaches for generative modeling. The first one is called variational auto-encoders (VAEs)
(Kingma & Welling, 2013), which use variational inference to learn a model by maximizing the
lower bound of the likelihood function. VAEs have elegant theoretical foundations but the drawback
is that they tend to produce blurry images.  The second approach is called generative adversarial
networks (GANs) (Goodfellow et al., 2014), which learn a model by using a powerful discriminator
to distinguish between real data points and generative data points.  GANs produce more visually
realistic images but suffer from the unstable training and the mode collapse problem. Although there
are many variants of generative models trying to take advantages of both VAEs and GANs (Tolstikhin
et al., 2017; Rosca et al., 2017), to the best of our knowledge, the model which provides a unifying
framework combining the best of VAEs and GANs in a principled way is yet to be discovered.

1.1    RELATED WORK

GANs and WGANs: The generative model is to learn a mapping, denoted by G, from     to      to
approximate the conditional distribution PG(X Z) of the data point X          given latent code Z   
     .
We consider the deterministic mapping G in this work. Both GANs and Wasserstein GANs (WGANs)
(Arjovsky et al., 2017) can be viewed as minimizing certain divergence between the data distribution
PX and the generative model distribution PG₍Z₎. For example, the Jensen-Shannon (JS) divergence
is implicitly used in GANs (Goodfellow et al., 2014). The 1-Wasserstein distance between PX and
PG₍Z₎, denoted by W₁(PX , PG₍Z₎), is employed in WGANs. Empirical experiments suggest that
the Wasserstein distance is a more sensible measure to differentiate probability measures supported
in low-dimensional manifold. In terms of training, it turns out that it is hard or even impossible 
to

1


Under review as a conference paper at ICLR 2020

compute these standard divergences in probability, especially when PX is unknown and PG₍Z₎ is
parameterized by deep neural networks (DNNs). The training of GANs is converted into playing a
game between two competing networks: the generator and the discriminator. The generator is to
fool the discriminator and the discriminator is to distinguish between true data samples and 
generated
samples. Instead, the training of WGANs is to study its dual problem because of the elegant form
of Kantorovich-Rubinstein duality (Villani, 2008). Analogous to GANs, the discriminator is now a
real-valued 1-Lipschitz function. Many techniques such as weight clipping (Arjovsky et al., 2017)
and gradient penalty (Gulrajani et al., 2017) are used to enforce the Lipschitz constraint.  More
discussions about WGANs will be presented in Section 2.

Autoencoder GANs: Larsen et al. (2016) first introduced the VAE-GAN, which is a hybrid of VAEs
and GANs. The VAE-GAN uses a GAN discriminator to replace a VAE’s decoder to learn the loss
function. The motivation behind this modification is that VAEs tend to produce blurry outputs during
the reconstruction phase. More recent VAE-GAN variants, such as Adversarial Generator Encoders
(AGE) (Ulyanov et al., 2018) and Auto-encoding GANs (α-GAN) (Rosca et al., 2017), use a separate
encoder to stabilize GAN training.  The main difference with standard GANs is that, besides the
generator G, there is an encoder Q :               which maps the data points into the latent 
space. This
encoder is to approximate the conditional distribution Q(Z X) of the latent variable Z given the 
data
point X.  Other encoder-decoder GANs are introduced in Adversarially Learned Inference (ALI)
(Dumoulin et al., 2016) and Bidirectional Generative Adversarial Networks (BiGAN) (Donahue et al.,
2016). The objective of both ALI and BiGAN is to match two joint distributions under the framework
of vanilla GANs, the joint distribution of (X, Q(X)) and the joint distribution of (G(Z), Z). When
the algorithm achieves equilibrium, these two joint distributions roughly match. We are able to 
obtain
more meaningful latent codes by Q(X), and this should improve the quality of the generator as
well. Adversarial Variational Bayes (AVB) (Mescheder et al., 2017) presented a more flexible latent
distribution to train Variational Autoencoders. Hu et al. (2017) provided new interpretations of 
GANs
and  VAEs and revealed strong connections between them which are linked by the classic wake-sleep
algorithm.

Duality in GANs: Regarding the optimization perspectives of GANs, (Chen et al., 2018; Zhao et al.,
2018) studied duality-based methods for improving algorithm performance for training. Primal-dual
Wasserstein GANs (PD-GANs) are introduced in (Gemici et al., 2018), which proposed a new penalty
term whose evaluation samples are obtained from the encoder Q. Farnia & Tse (2018) developed a
convex duality framework to address the case when the discriminator is constrained into a smaller
class. Grnarova et al. (2018) developed an evaluation metric to detect the non-convergence behavior
of vanilla GANs, which is the duality gap defined as the difference between the primal and the dual
objective functions. Husain et al. (2019) investigated the close relationship between WAE 
(Tolstikhin
et al., 2017) and f-GANs (Nowozin et al., 2016), and proved generalization results for autoencoder
models.

1.2    OUR CONTRIBUTIONS

Although there are many interesting works on autoencoder GANs,  it remains unclear what the
principles are underlying the fusion of auto-encoders and GANs. For example, do there even exist
these two mappings, the encoder Q and the decoder G, for any high-dimensional random variable X,
such that Q(X) has the same distribution as Z and G(Z) has the same distribution as X? Is there
any probabilistic interpretation such as the maximum likelihood principle on autoencoder GANs?
We introduce inference Wasserstein GANs (iWGANs), which provide satisfying answers for these
questions. We focus on the 1-Wasserstein distance, instead of the Kullback-Leibler divergence. We
borrow the strength from both the primal and the dual problems and demonstrate the synergistic 
effect
between these two optimizations.  The encoder component tends out to be a natural consequence
from our algorithm. Furthermore, the iWGAN has a rigorous probabilistic interpretation under the
maximum likelihood principle, and our learning algorithm is equivalent to the maximum likelihood
estimation when our model is defined as an energy-based model based on an autoencoder. Our main
contributions are listed as below:

1.  We propose a novel framework, called iWGAN, to learn both an encoder and a decoder
simultaneously. We prove the existence of meaningful encoder and decoder, establish an
equivalence between WGAN and iWGAN, and develop a generalization error bound for
iWGAN.

2


Under review as a conference paper at ICLR 2020

2.  We establish a rigorous probability interpretation for iWGANs and our training process is
exactly the same as the maximum likelihood estimation. As a byproduct, this interpretation
allows us to perform the quality check at the individual sample level.

3.  We demonstrate the natural use of the duality gap as a measure of convergence for iWGANs,
and show its effectiveness for various numerical settings. Our experiments do not experience
any mode collapse problem.

2    IWGAN

The autoencoder generative model consists of two parts:  an encoder Q and a generator G.  The
encoder Q maps a data sample x         to a latent variable z        , and the generator G takes a 
latent
variable z          to produce a sample G(z).  The autoencoder generative model should satisfy the
following three conditions simultaneously:  (a) The generator can generate images which have a
similar distribution with observed images; (b) The encoder can produce meaningful encodings in the
latent space; (c) The reconstruction errors of this model based on these meaningful encodings are
small. The benefit of using an autoencoder is to encourage the model to better represent all the 
data it
is trained with, so that it discourages mode-collapse.

We first show that, for any distribution residing on a smooth manifold, there always exists an 
encoder
Q∗ which guarantees meaningful encodings and exists a generator G∗ which generates samples with
the same distribution as data points by using these meaningful codes.

Theorem  2.1.  Consider  a  continuous  random  variable  X  ∈  X∗ ,  where  Xp is  a  
d-dimensional

smooth Riemannian manifold.  Then, there exist two mappings Q   : X  →  R   and G∗ : Rp  →  X ,

with p  =  max  d(d + 5)/2, d(d + 3)/2 + 5  ,  such that Q∗(X)  follows a multivariate normal
distribution with zero mean and identity covariance matrix and G∗   Q∗ is an identity mapping, i.e.,
X = G∗(Q∗(X)).

Learning Q∗ and G∗ from the data points is a challenging task. Recall that the 1-Wasserstein 
distance
between the data distribution PX and the generative model distribution PG₍Z₎ is defined as


W₁(PX , PG₍Z₎) =

π∈

inf

Π(PX ,PZ )

E₍X,Z₎∼πǁX − G(Z)ǁ,                               (1)

where          represents the L₂-norm and Π(PX , PZ) is a set of all joint distributions of (X, Z)
with marginal measures PX and PZ, respectively. The main difficulty in (1) is to find the optimal
coupling ∫π  and  this  is  a  constrained  optimization  with  PX (x)  =      π(x, z)dz  for  x  ∈ 
 X  and

Based on the Kantorovich-Rubinstein duality, the WGAN studies the 1-Wasserstein distance (1) from
the dual format

W1(PX, PG₍Z₎) = sup ,EX∼PX Σf (X)Σ − EZ∼PZ Σf (G(Z))Σ,,                        (2)

where      is the set of all bounded 1-Lipschitz functions.  Weight clipping (Arjovsky et al., 2017)
and gradient penalty (Gulrajani et al., 2017) have been used to satisfy the constraint of Lipschitz
continuity. The experiment of (Arjovsky et al., 2017) showed that the WGAN can avoid the problem
of gradient vanishment.  However, the WGAN does not produce meaningful encodings and many
experiments still display the problem of mode collapse (Arjovsky et al., 2017; Gulrajani et al., 
2017).
On       the other hand, the Wasserstein Autoencoder (WAE) (Tolstikhin et al., 2017), after 
introducing
an encoder Q  :                  to approximate the conditional distribution of Z  given X, studies 
the

reconstruction error infQ∈Q EX  X      G(Q(X))  , where      is a set of encoder mappings whose
elements satisfies PQ₍X₎ = PZ.  The penalty, such as    (PQ₍X₎, PZ), is added to the objective to
satisfy this constraint, where     is an arbitrary divergence between PQ₍X₎ and PZ.  The WAE can

produce meaningful encodings and controlled reconstruction error.  However, the WAE defines a
generative model in an implicit way and does not model the generator through G(Z) with Z      PZ
directly.

To take the advantages of both WGAN and WAE, we propose a new autoencoder GAN model, called
iWGAN, whose objective is


W ₁(PX , PG₍Z₎) =  inf

sup EX ǁX − G(Q(X))ǁ + EX Σf (G(Q(X)))Σ − EZ Σf (G(Z))Σ.       (3)

3


Under review as a conference paper at ICLR 2020

The term   X     G(Q(X))   can be treated as the autoencoder reconstruction error as well as a loss
to match the distributions between X  and G(Q(X)).  We note that the L₁-norm         ₁ has been
used for the reconstruction term by α-GAN (Rosca et al., 2017) and CycleGAN (Zhu et al., 2017).
Another term EX  PX f (G(Q(X)))      EZ  PZ f (G(Z)) can be treated as a loss for the generator
as well as a loss to match the distribution between G(Q(X)) and G(Z).  We emphasize that this
term is different with the objective function of the WGAN in (2). Furthermore, it is challenging for
practitioners to determine when to stop training GANs. Most of the GAN algorithms do not provide
any explicit standard for the convergence of the model. However, the measure of convergence for
iWGAN becomes very natural and we use the duality gap as the measure. The duality gap can be
defined as


DualGap(G˜, Q˜, f˜) = sup L(G˜, Q˜, f ) − G

inf      L(G, Q, f ),                         (4)

∈G,Q∈Q

where L(G, Q, f ) = EX ǁX − G(Q(X))ǁ + EX [f (G(Q(X)))] − EZ[f (G(Z))].

Theorem 2.2.  The iWGAN objective (3) is equivalent to


W ₁(PX , PG₍Z₎) =  inf

Q∈Q

,W1(PX, PG(Q(X))) + W1(PG(Q(X)), PG(Z)),.                   (5)

Therefore, W₁(PX , PG₍Z₎)  ≤  W ₁(PX , PG₍Z₎).  If there exists a Q∗ ∈  Q such that Q∗(X) has
the same distribution with Z, then W₁(PX , PG₍Z₎)  =  W ₁(PX , PG₍Z₎).  Let (Q, G, f ) be a fixed
solution and assume that the encoder, generator, and discriminator all have enough capacities. Then
the duality gap is larger than W₁(PX , PG(Q(X))) + W₁(PG(Q(X)), PG(Z)). Moreover, if G outputs
the same distribution as X  and Q outputs the same distribution as Z, both the duality gap and
W ₁(PX , PG(Z)) are zeros and X = G(Q(X)) for X ∼ PX .

According to Theorem 2.2, the iWGAN objective is in general the upper bound of W₁(PX , PG₍Z₎).
However, this upper bound is tight.  When the space      includes a special encoder Q∗ such that
Q∗(X) has the same distribution as Z, the iWGAN objective is the exactly same as W₁(PX , PG₍Z₎).
Theorem 2.2 also provides an appealing property from a practical point of view. The values of the

duality gap and W ₁(PX , P        ) give us a natural criteria to justify the algorithm 
convergence.

3    GENERALIZATION  ERROR  BOUND  AND  THE  ALGORITHM

In practice, we minimize the empirical version, W ₁(PX , PG₍Z₎), of W ₁(PX , PG₍Z₎) to learn both
the encoder and the decoder.  Before we present the details of the algorithm, we first develop the
generalization error bound for iWGANs. For discussions of generalization performance of classical
GANs,    see Arora et al. (2017) and Jiang et al. (2018).

Theorem 3.1.  Given a generator G ∈ G, and given n samples (x₁, . . . , xn) from X  = {x : ǁxǁ ≤

B}, with probability at least 1 − δ for any δ ∈ (0, 1), we have

W  (P   , P      ) ≤ W^  (P   , P      ) + 2R^  (F) + 3B. 2 log ∫ 2 ,,                      (6)


where R^ n(F) = Es Σsup

n−1 Σn

ϵif (xi)Σ is the empirical Rademacher complexity of the

Theorem  3.1  indicates  that  the  1-Wasserstein  distance  between  PX  and  PG₍Z₎ can  be  domi-
nantly upper bounded by the empirical W ₁(PX , PG₍Z₎) and Rademacher complexity of F. Since
W ₁(PX , PG₍Z₎) ≤ W₁(PX , PG₍Q₍X₎₎) + W₁(PG₍Q₍X₎₎, PG₍Z₎) for any Q ∈ Q, the capacity of Q

determines the value of W ₁(PX , PG₍Z₎).  On the other hand, there are several existing results on
the empirical Rademacher complexity of neural networks.  When F  is a set of 1-Lipschitz neural
networks, we can apply the conclusion from Bartlett et al. (2017) to Rn(F), which produces an
upper bound scaling as O(B√ L3/n). Here L denotes the depth of network f  ∈ F. Similar upper

4


Under review as a conference paper at ICLR 2020

In practice, we adopt the gradient penalty defined as      (f ) = EX  (      X f (X)  ₂  1)²  in 
(Gulrajani
et al., 2017) to enforce the 1-Lipschitz constraint on f          .  In addition, we use the maximum
mean discrepancy (MMD) penalty (Gretton et al., 2012), denoted by MMDk(PQ₍X₎, PZ), to enforce
Q(X) to converge to PZ, where k is a kernel function. The details of the algorithm are presented in
Algorithm 1.

Algorithm 1: The training algorithm of iWGAN

1   while  DualGap > ϵ₁ or L(Gⁱ, Qⁱ, f ⁱ) > ϵ₂ do

2              for t = 1, ..., ncritic do

3                        Sample real data xⁱ ∼ PX , latent variable zⁱ ∼ PZ and a random number ϵ ∼ 
U [0, 1]

4                        xˆⁱ     ϵxⁱ + (1     ϵ)Gⁱ(zⁱ)

5                        Calculate Lⁱ = L(Gⁱ, Qⁱ, f ⁱ xⁱ, zⁱ) and gradient of    Lⁱ

6                        Update f by Adam: f ⁱ⁺¹     Adam(       f Lⁱ)

7                        where for f ⁱ,


1 Σn      .  i      i     i              i      i      i     i

8                        −∇  L  = ∇                f  (G (z  )) − f  (G (Q (x  ))) + λ  (ǁ∇

	

fⁱ(xˆⁱ)ǁ

− 1)²Σ

10              for t = 1, ..., ncritic do

11                        Sample real data x′ⁱ     PX , latent variable z′ⁱ     PZ

12                        Calculate L′ⁱ = L(Gⁱ, Qⁱ, f ⁱ⁺¹ x′ⁱ, z′ⁱ) and gradient of L′ⁱ

13                        Update G, Q by Adam: Gⁱ⁺¹, Qⁱ⁺¹     Adam(   G,QL′ⁱ)

14                        where for Gⁱ, Qⁱ,


∇     L′ⁱ = ∇

1 Σ(ǁxⁱ − Gⁱ(Qⁱ(xⁱ ))ǁ + fⁱ⁺¹(Gⁱ(Qⁱ(xⁱ ))) − fⁱ⁺¹(Gⁱ(zⁱ )))


15              end

n(n − 1)  l   j

n(n − 1)

l/=j

l              j             n2

l            j

l,j

16   end

4    PROBABILISTIC  INTERPRETATION  AND  THE  MLE

The iWGAN has proposed an efficient framework to stably and automatically estimate both the
encoder and the generator. In this section, we provide a probabilistic interpretation of the iWGAN
under the framework of maximum likelihood estimation.

Maximum likelihood estimator (MLE) is a fundamental statistical framework for learning models
from data.  However,  for complex models,  MLE can be computationally prohibitive due to the
intractable normalization constant. MCMC has been used to approximate the intractable likelihood
function but do not work efficiently in practice. The iWGAN can be treated as an adaptive method
for MLE training, which not only provides computational advantages but also allows us to generate
more realistic-looking images.  Furthermore, this probabilistic interpretation enables other novel
applications such as image quality checking and outlier detection.

Let X denote the image. Define the density of X by an energy-based model based on an autoencoder
(Zhao et al., 2016; Berthelot et al., 2017):

p(x|θ) = exp    −   x − Gθ(Qθ(x))   − V (θ)  ,   V (θ) = log ∫  exp(−  x − Gθ(Qθ(x))  )dx,

where θ is the unknown parameter and V (θ) is the log normalization constant. The major difficulty
for the likelihood inference is due to the intractable function V (θ). Suppose that we have the 
observed

data {xi : i = 1, . . . , n}. The log-likelihood function of θ is l(θ) = n−¹ Σn     log  p(xi|θ), 
whose


gradient is

∇θl(θ) = −EˆobsΣ∂θ¨x − Gθ(Qθ(x))¨Σ + EθΣ∂θ¨x − Gθ(Qθ(x))¨Σ,                     (7)

where  Eˆobs[·]  denotes  the  empirical  average  on  the  observed  data  {xi}  and  Eθ[·]  
denotes  the
expectation under model p(x|θ). The key computational obstacle lies in the approximations of the

5


Under review as a conference paper at ICLR 2020

model expectation Eθ[ ]. To address this problem, we propose a novel dual approximation for this
expectation. By Theorem 5.10 of Villani (2008), there exists an optimal f ∗ such that

P₍ₓ,y₎∼π  f ∗(y) − f ∗(x) = ǁy − xǁ  = 1                                                   (8)

for the optimal coupling π. Therefore, there exists a f ∗ such that f ∗(x)     f ∗(Gθ(Qθ(x))) =   x
Gθ(Qθ(x))   with probability one with respect to the distribution of x. Since Eθ in (7) is taken 
under
the current estimated θ and we also require Gθ to be a good generator and the distributions of Gθ(z)
and Gθ(Qθ(x)) to be close, we approximate ǁx − Gθ(Qθ(x))ǁ by f ∗(Gθ(z)) − f ∗(Gθ(Qθ(x))).
We replace ǁx − Gθ(Qθ(x))ǁ in the second term of (7) by f ∗(Gθ(z)) − f ∗(Gθ(Qθ(x))), yielding a

gradient update for θ of form θ ← θ + ϵ∇ˆ θ l(θ), where

∇ˆ θl(θ) = −Eˆobs  ∂θ  x − Gθ(Qθ(x))     + Eθ  ∂θf ∗(Gθ(z)) − ∂θf ∗(Gθ(Qθ(x))  .             (9)

Here f ∗ needs to be learned and is solved by the corresponding dual problem at each iteration. We
approximate f ∗ by a network fη with an unknown parameter η, yielding a gradient update for η of
form

η ← η + g Eθ  ∂ηfη(Gθ(z)) − ∂ηfη(Gθ(Qθ(x))  .                                 (10)

The advantage of using expectations in (9) and (10) is that we can evaluate them by using only
marginal distributions of z and x.  The above iterative updating process is exactly the same as in
Algorithm 1. Therefore, the training of iWGAN is to seek the MLE. This probabilistic interpretation
provides a novel alternative method to tackle problems with the intractable normalization constant 
in
latent variable models. The MLE gradient update of p(x θ) decreases the energy of the training data
and increases the dual objective. Compare with original GANs or WGANs, our method gives much
faster convergence and simultaneously provides a higher quality generated images.

The probabilistic modeling opens a door for many interesting applications.  Next, we present a
completely new approach for determining a highest density region (HDR) estimate for the distribution
of X. What makes HDR distinct from other statistical methods is that it finds the smallest region,
denoted by U (α),  in the high dimensional space with a given probability coverage 1      α,  i.e.,
P(X     U (α)) = 1     α. We can use U (α) to assess each individual sample quality. Note that FID 
or
the Inception score are used to measure the whole sample quality, not at the individual sample 
level.
Let θˆ be the MLE. The density ratio at x₁ and x₂ is

ˆ

p(x1|θ)  = exp Σ − (ǁx  − G  (Q  (x  ))ǁ − ǁx  − G  (Q  (x  ))ǁ)Σ.

The smaller the reconstruction error is, the larger the density value is. We can define the HDR for 
x
through the HDR for the reconstruction error eₓ := ǁx − Gθˆ(Qθˆ(x))ǁ, which is simple because it is
a one-dimensional problem. Let U˜(α) be the HDR for eₓ. Then, U (α) =   x : eₓ    U˜(α)  . Here

Qθˆ(U (α)) defines the corresponding region in the latent space, which can be used to generate 
better
quality samples.

5    EXPERIMENTAL  RESULTS

5.1    MIXTURE OF GAUSSIANS

We train our iWGAN model on three toy datasets with an increasing difficulty shown on the right:
a).  RING: a mixture of 8 Gaussians,

b).  SPIRAL: a mixture of 20 Gaus-
sians and c).  GRID: a mixture of 25
Gaussians.  As the true data distribu-
tions  are known,  this setting allows
for tracking of convergence and mode
dropping.

Duality gap and convergence:  We


illustrate that as the duality gap con-
verges to 0, our model converges to

(a) RING                 (b) Swiss Roll                 (c) GRID

the generated samples from the true distribution. We keep track of the generated samples using G(z)
and record the duality gap at each iteration to check the corresponding generated samples. Figure 2
shows the generated samples converge to the true distribution very fast without the mode collapse

6


Under review as a conference paper at ICLR 2020

Figure 2: Duality gap and generated samples from iWGANs on mixture of Gaussians

problem.  We compare our method with WGAN-GP in Figure 2.  Both methods adopt the same
structure, learning rate, number of critical steps, and other hyper-parameters. The iWGAN surpasses
the performance of the WGAN-GP at very early stage and avoids the appearance of mode collapse.

Latent Space: We choose the latent distribution to be a five dimensional standard normal 
distribution
Z      N (0, I₅). After training, the distribution of Q(X) is expected to be close to the 
distribution of
Z. We plot the Q(X)i against Q(X)j for all i = j in Figure 3. We can tell that the joint 
distribution
of any two dimensions of Q(X) is close to a bivariate normal distribution.

(a) RING                                        (b) Swiss Roll                                      
 (c) GRID

Figure 3: Latent Space of Mixture of Gaussians

Individual    sample    quality    check:        From    the    probability    interpretation    of 
   iW-
GANs,    we   naturally   adopt   the   reconstruction   error     X         G(Q(X))  ,    or   the 
  qual-
ity    score    exp (     X     G(Q(X))  )    as    the    metric    of    the    quality    of    
any    individ-
ual   sample.        The   larger   the   quality   score   is,    the   better   quality   the   
sample   has.

Figure 4 shows their quality scores for
different samples. The quality scores
of samples near the modes of the true
distribution are close to 1, and become
smaller as the sample draw is away
from the modes.  This indicates that
the iWGAN does converge and learns
the distribution well, and the quality
score is a reliable metric for the indi-


vidual sample quality.

5.2    MNIST & CELEBA

We  experimentally  demonstrate  our

Figure 4: Quality Check

model’s ability on well-known datasets, MNIST and CelebA. The results by iWGAN on MNIST and
CelebA are shown in Figure 5 and Figure 6, respectively. For latent space interpolations between
MNIST or CelebA validation set examples. We sample pairs of validation set examples x₁ and x₂
and project them into z₁ and z₂ by the encoder. We then linearly interpolate between z₁ and z₂ and
pass the intermediary points through the decoder to plot the input-space interpolations.  Figure 7

7


Under review as a conference paper at ICLR 2020

displays images with high and low quality scores selected from CelebA. More experimental results,
including  reconstructed  images,  interpolations,  and  comparison  with  state-of-the-art  
results  by
WGAN-GP, are shown in Appendix.

(a) Generated samples                  (b) Reconstructed samples                       (c) 
Interpolations

Figure 5: iWGAN on MNIST

(a) Generated samples     (b) Reconstructed samples                       (c) Interpolations

Figure 6: iWGAN on CelebA

Figure 7: Images with high (left) and low (right) quality scores by iWGAN

6    CONCLUSION

We have developed a novel iWGAN model, which fuses auto-encoders and GANs in a principle way.
We have established a generalization error bound for iWGAN. We have provided a solid probabilistic
interpretation on iWGAN using the maximum likelihood principle. Our training algorithm with an
iterative primal and dual optimization has demonstrated an efficient and stable learning. We have
proposed a stopping criteria for our algorithm and a metric for individual sample quality checking.
The empirical results on both synthetic and benchmark datasets are state-of-the-art.

We now mention several future directions for research on iWGAN. First, one might be interested
in applying iWGAN into image-to-image translation, as the extension should be straightforward.
A second direction is to develop a formal hypothesis testing procedure to test whether the samples
generated from iWAGN is the same as the data distribution.

8


Under review as a conference paper at ICLR 2020

REFERENCES

Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein gan. arXiv preprint 
arXiv:1701.07875, 2017.

Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in 
generative
adversarial nets (gans). In Proceedings of the 34th International Conference on Machine 
Learning-Volume
70,      pp. 224–232. JMLR. org, 2017.

Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973, 
2018.

Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky.   Spectrally-normalized margin bounds for 
neural
networks. In Advances in Neural Information Processing Systems, pp. 6240–6249, 2017.

David Berthelot, Thomas Schumm, and Luke Metz.   Began:  Boundary equilibrium generative adversarial
networks. arXiv preprint arXiv:1703.10717, 2017.

Xu Chen, Jiang Wang, and Hao Ge.  Training generative adversarial networks via primal-dual 
subgradient
methods: a lagrangian perspective on gan. arXiv preprint arXiv:1802.01765, 2018.

Umberto Cherubini, Elisa Luciano, and Walter Vecchiato. Copula methods in finance. John Wiley & 
Sons, 2004.

Jeff  Donahue,  Philipp  Krähenbühl,  and  Trevor  Darrell.    Adversarial  feature  learning.    
arXiv  preprint
arXiv:1605.09782, 2016.

Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, 
and Aaron
Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.

Farzan Farnia and David Tse.   A convex duality framework for gans.   In Advances in Neural 
Information
Processing Systems, pp. 5248–5258, 2018.

Mevlana  Gemici,   Zeynep  Akata,   and  Max  Welling.     Primal-dual  wasserstein  gan.     arXiv 
 preprint
arXiv:1805.09575, 2018.

Ian  Goodfellow,  Jean  Pouget-Abadie,  Mehdi  Mirza,  Bing  Xu,  David  Warde-Farley,  Sherjil  
Ozair,  Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information 
processing
systems, pp. 2672–2680, 2014.

Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola.  A 
kernel
two-sample test. Journal of Machine Learning Research, 13(Mar):723–773, 2012.

Paulina Grnarova, Kfir Y Levy, Aurelien Lucchi, Nathanael Perraudin, Thomas Hofmann, and Andreas 
Krause.
Evaluating gans via duality. arXiv preprint arXiv:1811.05512, 2018.

Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved 
training
of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5767–5777, 2017.

Matthias Günther. Isometric embeddings of riemannian manifolds. In Proceedings of the International 
Congress
of Mathematicians, pp. 1137–1143, 1991.

Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, and Eric P Xing. On unifying deep generative models. 
arXiv
preprint arXiv:1706.00550, 2017.

Hisham Husain,  Richard Nock,  and Robert C Williamson.   Adversarial networks and autoencoders:  
The
primal-dual relationship and generalization bounds. arXiv preprint arXiv:1902.00985, 2019.

Haoming Jiang, Zhehui Chen, Minshuo Chen, Feng Liu, Dingding Wang, and Tuo Zhao. On computation and
generalization of generative adversarial networks under spectrum control. 2018.

Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 
2013.

Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoencoding 
beyond
pixels using a learned similarity metric. In International Conference on Machine Learning, pp. 
1558–1566,
2016.

Xingguo Li, Junwei Lu, Zhaoran Wang, Jarvis Haupt, and Tuo Zhao. On tighter generalization bound 
for deep
neural networks: Cnns, resnets, and beyond. arXiv preprint arXiv:1806.05159, 2018.

Yujia  Li,  Kevin  Swersky,  and  Richard  Zemel.    Generative  moment  matching  networks.    
arXiv  preprint
arXiv:1502.02761, 2015.

9


Under review as a conference paper at ICLR 2020

Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying 
variational
autoencoders and generative adversarial networks. In Proceedings of the 34th International 
Conference on
Machine Learning-Volume 70, pp. 2391–2400. JMLR. org, 2017.

Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.  Foundations of machine learning.  MIT 
press,
2018.

John Nash. The embedding problem for riemannina manifolds. Annals of Mathematics, pp. 20–63, 1956.

Sebastian Nowozin, Botond Cseke, and Ryota Tomioka.  f-gan:  Training generative neural samplers 
using
variational divergence minimization. In Advances in neural information processing systems, pp. 
271–279,
2016.

Mihaela Rosca, Balaji Lakshminarayanan, David Warde-Farley, and Shakir Mohamed. Variational 
approaches
for auto-encoding generative adversarial networks. arXiv preprint arXiv:1706.04987, 2017.

Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein 
auto-encoders. arXiv
preprint arXiv:1711.01558, 2017.

Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.  It takes (only) two: Adversarial 
generator-encoder
networks. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.

Cédric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 
2008.

Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. arXiv 
preprint
arXiv:1609.03126, 2016.

Shengjia  Zhao,  Jiaming  Song,  and  Stefano  Ermon.   The  information  autoencoding  family:  A  
lagrangian
perspective on latent variable generative models. arXiv preprint arXiv:1806.06514, 2018.

Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.  Unpaired image-to-image translation 
using
cycle-consistent adversarial networks.  In Proceedings of the IEEE international conference on 
computer
vision, pp. 2223–2232, 2017.

10


Under review as a conference paper at ICLR 2020

APPENDIX

APPENDIX  A: PROOF  OF  THEOREM  2.1

According to the Nash embedding theorem (Nash, 1956; Günther, 1991), every d-dimensional smooth 
Rieman-
nian manifold X possesses a smooth isometric embedding into Rᵖ with p = max{d(d + 5)/2, d(d + 3)/2 
+ 5}.
Therefore, there exists an injective mapping u :  X  → Rᵖ which preserves the metric in the sense 
that the
manifold metric on X is equal to the pullback of the usual Euclidean metric on Rᵖ by u.  The 
mapping u is
injective so that we can define the inverse mapping u−¹ : u(X ) → X .

Let X˜  = u(X)     Rᵖ, and write X˜  = (X˜1, . . . , X˜p). Let Fi(x) = P(X˜i       x), i = 1, . . . 
, p, be the marginal
cdfs. By applying the probability integral transformation to each component, the random vector

U1, U2, . . . , Up    :=   F1(X˜1), F2(X˜2), . . . , Fp(X˜p)

has uniformly distributed marginals. Let C : [0, 1]ᵖ     [0, 1] be the copula of X˜, which is 
defined as the joint
cdf of (U1, . . . , Up):

C(u1, u2, . . . , up) = P  U1  ≤ u1, U2  ≤ u2, . . . , Up  ≤ up   .

The copula C contains all information on the dependence structure among the components of X˜, while 
the
marginal cumulative distribution functions Fi  contain all information on the marginal 
distributions. Therefore,

the joint cdf of X˜  is


Define, for i = 2, . . . , p,

H(x˜1, x˜2, . . . , x˜p) = C.F1(x˜1), F2(x˜2), . . . , Fp(x˜p)Σ.

Ci(u1, u2, . . . , ui) = C.u1, u2, . . . , ui, 1, . . . , 1Σ.

The conditional distribution of Uk, given U₁, . . . , Uk−₁, is given by Cherubini et al. (2004)

Ck(uk|u₁, . . . , uk  ₁) = P  Uk ≤ uk|U₁ = u₁, . . . , Uk  ₁ = uk  ₁


=  Σ∂k−1C

(u  , . . . , u  )/∂u

· · · ∂u     Σ ,

We will construct Q∗ as follows.  First, we obtain X˜      Rᵖ by X˜  =  u(X).  Second, we transform 
X˜  into a
random vector with uniformly distributed marginals (U1, . . . , Up) by the marginal cdf Fi. Then, 
define U˜1  = U1
and

U˜k  = Ck  Uk|U₁, . . . , Uk−₁  ,   k = 2, . . . , p.

Hence, U˜1, . . . , U˜p  are independent uniform random variables.  Finally, let Zi  = Φ−¹(Ui) for 
i = 1, . . . , p.
This completes the transformation Q∗ from X to Z = (Z1, . . . , Zp).

The above process can be inverted to obtain G∗.  First, we transform Z into independent uniform 
random
variables by U˜i  = Φ(Zi) for i = 1, . . . , p. Next, let U1  = U˜1. Define

Uk = C−¹(U˜k |U˜1, . . . , U˜k−1),   i = 2, . . . , p,

where C−¹(·|u₁, . . . , uk) is the inverse of Ck and can be obtained by numerical root finding.  
Finally, let
X˜i  = F −¹(Ui) for i = 1, . . . , p and X = u−¹(X˜), where u−¹ : u(    )           is the inverse 
mapping of u. This
completes the transformation G∗ from Z to X.

APPENDIX  B: PROOF  OF  THEOREM  2.2

By  the  iWGAN  objective  (3),  (5)  holds.    Since  W1   is  a  distance  between  two  
probability  measures,
W₁(PX , PG₍Z₎)      W ₁(PX , PG₍Z₎). If there exists a Q∗        such that Q∗(X) has the same 
distribution as
PZ           , we have

W 1(PX, PG(Z)) ≤ W1(PX, PG(Q∗ (X))) + W1(PG(Q∗ (X)), PG(Z)) = W1(PX, PG(Z)).

Hence,  W₁(PX , PG₍Z₎)   =   W ₁(PX , PG₍Z₎).    Observe  that  supf  L(G, Q, f )   =   W₁(PX , 
PG(Q(X)))  +

W1(PG₍Q₍X₎₎, PG₍Z₎).  By Theorem 2.1,  we have infG,Q L(G, Q, f )  ≤  L(G∗, Q∗, f )  =  0 when G 
and

Q have enough capacity. Therefore, the duality gap is larger than W1(PX, PG₍Q₍X₎₎) + W1(PG₍Q₍X₎₎, 
PG₍Z₎).

It is easy to see that, if G outputs the same distribution as X and Q outputs the same distribution 
as Z, both the
duality gap and W ₁(PX , PG₍Z₎) are zeros and X = G˜(Q˜(X)) for X ∼ PX .

11


Under review as a conference paper at ICLR 2020

APPENDIX  C: PROOF  OF  THEOREM  3.1

We first consider the difference between population W₁(PX , PG₍Z₎) and empirical W₁(PX , PG₍Z₎) 
given n
samples S  =    x1, . . . , xn   .  Let f1  and f2  be their witness function respectively.  Using 
the dual form of
1-Wassertein distance, we have


=E   ∼

[f  (X)] − E   ∼

[f  (G(Z))] − 1 Σ f  (x ) + E   ∼

[f  (G(Z))]


X   PX      1

Z   PZ      1

n        2      i

i=1

n

Z   PZ      2


≤E   ∼

[f  (X)] − E   ∼

[f  (G(Z))] − 1 Σ f  (x ) + E   ∼

[f  (G(Z))]


X   PX      1

Z   PZ      1

n

n        1      i

i=1

Z   PZ      1


≤ sup E   ∼

[f (X)] − 1 Σ f (x ) , Φ(S).

Given another sample set S′ = {x₁, . . . , x′i, . . . , xn}, it is clear that

Φ(S) − Φ(S′) ≤ sup |f (xi) − f (x′i)| ≤ ǁxi − x′iǁ ≤ 2B ,

f             n                     n             n

where the second inequality is obtained since f is 1-Lipschitz continuous function. Applying 
McDiamond’s
Inequality, with probability at least 1 − δ/2 for any δ ∈ (0, 1), we have

Φ(S) ≤ E[Φ(S)] + B. 2 log ∫ 2 ,.                                          (11)

By the standard technique of symmetrization in Mohri et al. (2018), we have


E[Φ(S)] = E Σ

n

1

X   PX                             n

f (xi)Σ

≤ 2Rn(F).                        (12)

f                                               i=1

It has been proved in Mohri et al. (2018) that with probability at least 1 − δ/2 for any δ ∈ (0, 
1),

R  (F) ≤ R^  (F) + B. 2 log ∫ 2 ,.                                          (13)

Combining Equation (11), Equation (12) and Equation (13), we have


W  (P   , P

) ≤ W^ (P   , P

) + 2R^

(F) + 3B. 2 log ∫ 2 ,.

W  (P   , P      ) ≤ W^  (P   , P      ) + 2R^  (F) + 3B. 2 log ∫ 2 ,.

APPENDIX  D: EXTENSION  TO f -GANS

This framework can be easily extended to other types of GANs. We consider f -GAN (Nowozin et al., 
2016) here.
Let h : R       (        ,    ] be a convex function with h(1) = 0. The f -GAN minimizes the 
following objective for
the generator G:

GANh(PX , PG₍Z₎) = sup    EX [f (X)]     EZ [h∗(f (G(Z))]    ,

f ∈F

where h∗(x) = supy {x·y−h(y)} is the convex conjugate of h. If h(x) = x log(x)−(x+1) log(x+1)−2 log 
2,
f -GAN recovers the original GAN (Goodfellow et al., 2014). If h(x) = 0 when x = 1 and h(x) = ∞ 
otherwise,
we have h∗(x) = x. With the property that F is 1-Lipschitz function class, f -GAN turns to be WGAN.

Assume that      is the 1-Lipschitz function class.  We extent the iWGAN framework to the inference 
f-GAN
(ifGAN) framework and define the new objective function as follows:

W ₁,h(PX , PG₍Z₎) =  inf   sup EX ǁX − G(Q(X))ǁ + EX Σf (G(Q(X)))Σ − EZΣh∗(f (G(Z)))Σ.    (14)

12


Under review as a conference paper at ICLR 2020


Following this definition, we have

W ₁,h(PX , PG₍Z₎) =  inf

         Q∈Q

.W1(PX, PG(Q(X))) + GANh(PG(Q(X)), PG(Z))Σ .

We show GANh(PX , PG₍Z₎) ≤ W ₁,h(PX , PG₍Z₎). This is because
GANh(PX , PG₍Z₎) = sup EX [f (X)]     EZ [h∗(f (G(Z))]

f ∈F

≤  inf  .sup EX [f (X)] − EX [f (G(Q(X)))] + sup EX [f (G(Q(X)))] − EZ [h∗(f (G(Z))]Σ

= W 1,h(PX, PG(Z)).

This indicates that the ifGAN objective (14) is an upper bound of the f-GAN objective.

APPENDIX  E: ARCHITECTURES 1

E1: MIXTURE OF GUASSIANS

Encoder architecture:

x ∈ R² → FC1024  → RELU

→ FC512  → RELU

→ FC256  → RELU

→ FC128  → RELU → FC5


Generator architecture:

z ∈ R⁵ → FC512  → RELU

→ FC512  → RELU

→ FC512  → RELU → FC2


Discriminator architecture:

x ∈ R² → FC512  → RELU

→ FC512  → RELU

→ FC512  → RELU → FC1


E2: MNIST

Encoder architecture:

x ∈ R²⁸×²⁸ → Conv128  → RELU

→ Conv256  → RELU

→ Conv512  → RELU → FC8


Generator architecture:

z ∈ R⁸ → FC4×4×512  → RELU

→ ConvT rans256  → RELU

→ ConvT rans128  → RELU → ConvT rans1

Discriminator architecture:

28×28

x ∈ R           → Conv128  → RELU

→ Conv256  → RELU

→ Conv512  → RELU → FC1

¹Codes used for this paper will be available at: https://drive.google.com/drive/folders/
1-_vIrbOYwf2BH1lOrVEcEPJUxkyV5CiB?usp=sharing

13


Under review as a conference paper at ICLR 2020

E3: CELEBA

Encoder architecture:

x ∈ R⁶⁴×⁶⁴×³ → Conv128  → LeakyRELU

→ Conv256  → InstanceNorm → LeakyRELU

→ Conv512  → InstanceNorm → LeakyRELU → Conv1

Generator architecture:

z ∈ R64  → FC4×4×1024

→ ConvT rans512  → BN → RELU

→ ConvT rans256  → BN → RELU

→ ConvT rans128  → BN → RELU → ConvT rans3

Discriminator architecture:

x ∈ R⁶⁴×⁶⁴×³ → Conv128  → LeakyRELU

→ Conv256  → InstanceNorm → LeakyRELU

→ Conv512  → InstanceNorm → LeakyRELU → Conv1

14


Under review as a conference paper at ICLR 2020

APPENDIX  F: MORE  EXPERIMENTAL  RESULTS  ON  MIXTURE  OF  GAUSSIAN

We investigate the mode collapse problem for the iWGAN. If we draw two random samples in the latent 
space
z1, z2         N (0, I5), the interpolation, G(λz1 + (1     λ)z2), 0      λ     1, should fall 
around the mode to represent
a reasonable sample. In Figure 8, we select λ      0, 0.05, 0.10, . . . , 0.95, 1.0  , and do 
interpolations on two
random samples. We repeat this procedure several times on 3 datasets as demonstrated in Figure 8. 
No matter
where the interpolations start and end, the interpolations would fall around the modes other than 
the locations
where true distribution has a low density. There may still be some samples that appears in the 
middle of two
modes. This may be because the generator G is not able to approximate a step function well.

Figure 8:  Interpolation:  V and ▲ indicates the first and last samples in the interpolations, other
colored samples are the interpolations.

APPENDIX  G: MORE  EXPERIMENTAL  RESULTS  ON  MNIST

G.1: LATENT SPACE

Figure 9 shows the latent space of MNIST, i.e. Q(X)i  against Q(X)j  for all i /= j.

G.2: GENERATED SAMPLES

Figure 10 shows the comparison of random generated samples between WGAN-GP and iWGAN. Figure 11
shows examples of interpolations of two random generated samples.

G.3: RECONSTRUCTION

Figure 12 shows, based on the samples from validation dataset, the distribution of reconstruction 
error. Figure 13
shows examples of reconstructed samples. Figure 14 shows the best and worst samples based on 
quality scores
from the validation dataset.

15


Under review as a conference paper at ICLR 2020

Figure 9: Latent Space of MNIST dataset

(a) WGAN-GP

(b) iWGAN

Figure 10: Generated samples on MNIST

16


Under review as a conference paper at ICLR 2020

Figure 11: Interpolations on MNIST

Figure 12: Histogram of reconstruction error on MNIST

Figure 13: Reconstructions on MNIST

17


Under review as a conference paper at ICLR 2020

(a) Samples with high quality scores

(b) Samples with low quality scores

Figure 14: Sample quality check by iWGAN on the validation dataset of MNIST

18


Under review as a conference paper at ICLR 2020

APPENDIX  H: MORE  EXPERIMENTAL  RESULTS  ON  CELEBA

H.1 LATENT SPACE

Figure 15 shows the first 8 dimensions of the latent space calculated by Q(x) on CelebA.

H.2 RANDOM GENERATED SAMPLE

Figure 16, Figure 17, Figure 18 and Figure 19 display the random generated samples from Wasserstein 
GAN
gradient penalty(WGAN-GP), iWGAN, Wasserstein Autoencoder(WAE), and Adversarial Learning Inference
(ALI), respectively. Table 1 shows numerical comparison among these four methods based on inception 
scores
(IS), Fréchet inception distances (FID), reconstruction errors (RE), and maximum mean discrepancy 
(MMD)
between encodings and standard normal random variables.

The IS and FID scores are calculated based on pre-trained inception models. However, as discussed 
in Barratt
& Sharma (2018), inception score is not a reliable metric for the wellness of generated samples. 
This is also
consistent with our experiments. Although WAE delivers the best inception scores among four 
methods, WAE
also  has the worst FID scores.  The generated samples (Figure 18) show that WAE is not the best 
generative
model compared with other three methods.

The reconstruction error (RE) is defined as


RE =   1  Σ ǁXˆ

− X ǁ ,                                                 (15)

Xˆi  is the reconstructed sample for Xi. RE is used to measure if the method has generated 
meaningful latent
encodings. Smaller reconstruction errors indicate a more meaningful latent space which can be 
decoded into the
original samples.

The maximum mean discrepancy (MMD) is defined as

MMD =          1         Σ k(z , z ) +         1         Σ k(z˜ , z˜ ) −   2   Σ k(z , z˜ )         
     (16)

	 	

where k is a positive-definite reproducing kernel, zis are drawn from prior distribution PZ , and 
z˜i  = Q(xi)
are the latent encodings of real samples. MMD is used to measure the difference between 
distribution of latent
encodings and standard normal random variables. Smaller MMD indicates that the distribution of 
encodings is
close   to the standard normal distribution.

From Table 1, in terms of generative models, iWGAN and ALI are better models, where WGAN-GP comes
after, but WAE is suffering from generating clear pictures. In terms of RE and MMD, iWGAN and WAE 
are
better choices, where ALI cannot always reconstruct the sample to itself (Figure 21). In general, 
Table 1 shows
that iWGAN has successfully produce both meaningful encodings and reliable generator 
simultaneously.

Table 1: Comparison of iWGAN, ALI, WAE, WGAN-GP

Methods                         IS       FID                  RE           MMD
True                1.96(0.019)     18.63                     –                   –

iWGAN          1.51(0.017)     51.20     13.55(2.41)     6    10−³

ALI                 1.50(0.014)     51.12     34.49(8.23)              0.39

WAE               1.71(0.029)     77.53       9.88(1.42)     4    10−³

WGAN-GP     1.54(0.016)     61.39                     –                   –

Figure 20 shows the interpolation comparison between iWGAN, WAE and ALI.

H.3: RECONSTRUCTION

Figure 22 shows the distribution of reconstruction error of CelebA. Figure 21 shows the comparison 
between
real images and reconstructed images among three methods, iWGAN, WAE and ALI. Figure 23 shows 
samples
with high and low quality scores in CelebA validation sets.

19


Under review as a conference paper at ICLR 2020

Figure 15: Latent Space of CelebA dataset

Figure 16: Generated samples by WGAN-GP

20


Under review as a conference paper at ICLR 2020

Figure 17: Generated samples by iWGAN

Figure 18: Generated samples by WAE

21


Under review as a conference paper at ICLR 2020

Figure 19: Generated samples by ALI

Figure 20: Interpolations comparison among iWGAN, WAE and ALI

22


Under review as a conference paper at ICLR 2020

Figure 21: Reconstructed samples comparison among iWGAN, WAE and ALI

Figure 22: Histogram of reconstruction errors on CelebA

23


Under review as a conference paper at ICLR 2020

(a) Samples with high quality scores

(b) Samples with lower quality scores

Figure 23: Sample quality check by iWGAN on CelebA

24

