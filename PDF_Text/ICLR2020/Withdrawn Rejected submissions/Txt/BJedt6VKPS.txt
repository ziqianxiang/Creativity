Under review as a conference paper at ICLR 2020
Scaling Laws for the Principled Design, Ini-
tialization, and Preconditioning of ReLU Net-
WORKS
Anonymous authors
Paper under double-blind review
Ab stract
In this work, we describe a set of rules for the design and initialization of well-
conditioned neural networks, guided by the goal of naturally balancing the diagonal
blocks of the Hessian at the start of training. We show how our measure of
conditioning of a block relates to another natural measure of conditioning, the ratio
of weight gradients to the weights. We prove that for a ReLU-based deep multilayer
perceptron, a simple initialization scheme using the geometric mean of the fan-in
and fan-out satisfies our scaling rule. For more sophisticated architectures, we
show how our scaling principle can be used to guide design choices to produce
well-conditioned neural networks, reducing guess-work.
1	Introduction
The design of neural networks is often considered a black-art, driven by trial and error rather than
foundational principles. This is exemplified by the success of recent architecture random-search
techniques (Zoph and Le, 2016; Li and Talwalkar, 2019), which take the extreme of applying no
human guidance at all. Although as a field we are far from fully understanding the nature of learning
and generalization in neural networks, this does not mean that we should proceed blindly.
In this work we define a scaling quantity γl for each layer l that approximates the average squared
singular value of the corresponding diagonal block of the Hessian for layer l. This quantity is
easy to compute from the (non-central) second moments of the forward-propagated values and the
(non-central) second moments of the backward-propagated gradients. We argue that networks that
have constant γl are better conditioned than those that do not, and we analyze how common layer
types affect this quantity. We call networks that obey this rule preconditioned neural networks, in
analogy to preconditioning of linear systems.
As an example of some of the possible applications of our theory, we:
•	Propose a principled weight initialization scheme that can often provide an improvement
over existing schemes;
•	Show which common layer types automatically result in well-conditioned networks;
•	Show how to improve the conditioning of common structures such as bottlenecked residual
blocks by the addition of fixed scaling constants to the network (Detailed in Appendix E).
2	Notation
We will use the multilayer perceptron (i.e. a classical feed-forward deep neural network) as a running
example as it is the simplest non-trivial deep neural network structure. We use ReLU activation
functions, and use the following notation for layer l of L (following He et al., 2015):
yl = Wlxl + bl,
xl+1 = ReLU(yl),
where Wl is a nlout × niln matrix of weights, bl is the bias vector, yl the preactivation vector and xl
is the input activation vector for the layer. The quantities nlout and niln are called the fan-out and
1
Under review as a conference paper at ICLR 2020
Figure 1: Average singular value heat maps for the strided LeNet model, where each square represents
a block of the Hessian. The preconditioned network maintains an approximately constant block-
diagonal weight. The scale goes from Yellow (larger) through green to blue (smaller).
fan-in of the layer respectively. We also denote the gradient of a quantity with respect to the loss
(i.e. the back-propagated gradient) with the prefix ∆. We initially focus on the least-squares loss.
Additionally, we assume that each bias vector is initialized with zeros unless otherwise stated.
3	Conditioning by balancing the Hessian
Our proposed approach focuses on the singular values of the diagonal blocks of the Hessian. In the
case of a multilayer perceptron network (MLP) network, each diagonal block corresponds to the
weights from a single weight matrix Wl or bias vector bl . This block structure is used by existing
approaches such as K-FAC and variants (Martens and Grosse, 2015; Grosse and Martens, 2016;
Ba et al., 2017; George et al., 2018), which correct the gradient step using estimates of second-
order information. In contrast, our approach modifies the network to improve the Hessian without
modifying the step.
Estimates of the magnitude of the singular values σi(Gl) of the diagonal blocks G1,. . . ,GL of the
Hessian G provide information about the singular values of the full matrix.
Proposition 1. Let Gl be the lth diagonal block of a real symmetric matrix G : n × n. Then for all
i = 1 . . . n:
σmin(G) ≤ σi(Gl),
σmax(G) ≥ σi(Gl).
We can use this simple bound to provide some insight into the conditioning of the full matrix:
Corollary 2. Let S = {s1 , . . . , } be the union of the sets of singular values of the diago-
nal blocks G1 , . . . , Gl of a real symmetric matrix G : n × n. Then the condition number
K(G) = σmax(G)∕σmin(G) is bounded as:
κ(G) ≥ maxsi/ min si.
In particular, a Hessian matrix with a very large difference between the singular values of each block
must be ill-conditioned. This provides strong motivation for balancing the magnitude of the singular
values of each diagonal block, the goal of this work. Although ideally, we would like to state the
converse, that a matrix with balanced blocks is well conditioned, we can not make such a statement
without strong assumptions on the off-diagonal behavior of the matrix.
We use the average squared singular value of each block as a proxy for the full spectrum, as it is
particularly easy to estimate in expectation. Although the minimum and maximum for each block
would seem like a more natural quantity to work with, we found that any such bounds tend to be too
pessimistic to reflect the behavior of the actual singular values.
When using the ReLU activation function, as we consider in this work, a neural network is no longer
a smooth function of its inputs, and the Hessian becomes ill-defined at some points in the parameter
space. Fortunately, the spectrum is still well-defined at any twice-differentiable point, and this gives a
local measure of the curvature. ReLU networks are typically twice-differentiable almost everywhere.
We assume this throughout the remainder of this work.
2
Under review as a conference paper at ICLR 2020
Figure 2: Distributions of the ratio of theoretical scaling to actual for a strided LeNet network
3.1	GR scaling: A measure of Hessian average conditioning
Our analysis will proceed with batch-size 1 and a network with k outputs. We consider the network
at initialization, where weights are centered, symmetric and i.i.d random variables and biases are set
to zero.
ReLU networks have a particularly simple structure for the Hessian with respect to any set of
activations, as the network’s output is a piecewise-linear function g fed into a final layer consisting of
a loss. This structure results in greatly simplified expressions for diagonal blocks of the Hessian with
respect to the weights.
We will consider the output of the network as a composition two functions, the current layer g, and
the remainder of the network h. We write this as a function of the weights, i.e. f(Wl) = h(g(Wl)).
The dependence on the input to the network is implicit in this notation, and the network below layer l
does not need to be considered.
Let Rl = Vy； h(yl) be the Hessian of h, the remainder of the network after application of layer
l (recall yl = Wlxl). Let Jl be the Jacobian of yl with respect to Wl. The Jacobian has shape
Jl : nlout × nloutniln . Given these quantities, the diagonal block of the Hessian corresponding to Wl
is equal to:
Gl = JlT Rl Jl .
The lth diagonal block of the (Generalized) Gauss-Newton matrix G (Martens, 2014). We discuss
this decomposition further in Appendix A.1. We use the notation E[X2] for any matrix or vector X
to denote the expectation of the element-wise non-central second moment.
Proposition 3. (The GR scaling) Under the assumptions outlined in Section 3.2, the average squared
singular value of Gl is equal to the following quantity, which we call the GR scaling for MLP layers:
(GR scaling) Yl= ninElxT E：y； ].
l l	E[yl2]
We define a “balanced” or “preconditioned” network as one in which γl is equal for all l (full
derivation in Appendix A).
Balancing this theoretically derived GR scaling quantity in a network will produce an initial opti-
mization problem for which the blocks of the Hessian are expected to be approximately balanced
with respect to their average squared singular value.
Due to the large number of approximations needed for this derivation, which we discuss further in
the next section, we don’t claim that this theoretical approximation is accurate, or that the blocks
will be closely matched in practice. Rather, we make the lesser claim that a network with very
disproportionate values of γl between layers is likely to have convergence difficulties during the early
stages of optimization due to Cor. 2.
To check the quality of our approximation, we computed the ratio of the convolutional version of
the GR scaling equation (Equation 1) to the actual E[(Glr)2] product for a strided (rather than
max-pooled, see Table 1) LeNet model, where we use random input data and a random loss (i.e. for
outputs y we use yTRy for an i.i.d normal matrix R), with batch-size 1024, and 32 × 32 input images.
The results are shown in Figure 2 for 100 sampled setups; there is generally good agreement with the
theoretical expectation.
3
Under review as a conference paper at ICLR 2020
3.2	Assumptions
The following strong assumptions are used in the derivation of the GR scaling:
(A1) The input and target values are drawn element-wise i.i.d from a centered symmetric distribution
with known variance.
(A2) The Hessian of the remainder of the network above each block, with respect to the output, has
Frobenius norm much larger than 1. More concretely, we assume that all but the highest order terms
that are polynomial in this norm may be dropped.
(A3) All activations, pre-activations and gradients are independently distributed element-wise. In
practice due to the mixing effect of multiplication by random weight matrices, only the magnitudes
of these quantities are correlated, and the effect is small for wide networks due to the law of large
numbers. Independence assumptions of this kind are common when approximating second-order
methods; the block-diagonal variant of K-FAC (Martens and Grosse, 2015) makes similar assumptions
for instance.
Assumption A2 is the most problematic of these assumptions, and we make no claim that it holds
in practice. However, we are primarily interested in the properties of blocks and their scaling with
respect to each other, not their absolute scaling. Assumption A2 results in very simple expressions
for the scaling of the blocks without requiring a more complicated analysis of the top of the network.
Similar theory can be derived for other assumptions on the output structure, such as the assumption
that the target values are much smaller than the outputs of the network.
4	Preconditioning also balances weight-to-gradient ratios
We provide further motivation for the utility of preconditioning by comparing it to another simple
quantity of interest. Consider at network initialization, the ratio of the (element-wise non-central)
second moments of each weight-matrix gradient to the weight matrix itself:
.E[∆W2]
V	E[W2].
This ratio approximately captures the relative change that a single SGD step with unit step-size on
Wl will produce. We call this quantity the weight-to-gradient ratio. When E[∆Wl2] is very small
compared to E[Wl2], the weights will stay close to their initial values for longer than when E[∆Wl2]
is large. In contrast, if E[∆Wl2] is very large compared to E[Wl2], then learning can be expected
to be unstable, as the sign of the elements of W may change rapidly between optimization steps.
A network with constant νl is also well-behaved under weight-decay, as the ratio of weight-decay
second moments to gradient second moments will stay constant throughout the network, keeping the
push-pull of gradients and decay constant across the network. Remarkably, the weight-to-gradient
ratio νl turns out to be equivalent to the GR scaling for MLP networks:
Proposition 4. (Appendix 8) νl is equal to the GR scaling γl for i.i.d mean-zero randomly-initialized
multilayer perceptron layers under the independence assumptions of Appendix 3.2.
5	Convolutional networks and conditioning multipliers
The concept of GR scaling may be extended to scaled convolutional layers yl = αlConvWl (xl) + bl
with scaling factor αl, kernel width kl, batch-size b, and output resolution ρl × ρl . A straight-forward
derivation gives expressions for the convolution weight and biases of:
Yl = α4bnlnk2ρ2E [χ2]2 E∆φ,	Yb = P2 E=∆yP.	(1)
E[yl ]	E[yl ]
This requires an assumption of independence of the values of activations within a channel that is
not true in practice, so Yl tends to be further away from empirical estimates for convolutional layers
than for non-convolutional layers, although it is still a useful guide. The effect of padding is also
ignored here. Sequences of convolutions are well-scaled against each other along as the kernel size
remains the same. The scaling of layers involving differing kernel sizes can be corrected using the
alpha parameter (Appendix E), and more generally any imbalance between the conditioning of layers
4
Under review as a conference paper at ICLR 2020
can be fixed by modifying αl while at the same time changing the initialization of Wl so that the
forward variance remains the same as the unmodified version. This adjusts γl while leaving all other
γ the same.
6	Preconditioning of neural networks via initialization
For ReLU networks with a classical multilayer-perceptron (i.e. non-convolutional, non-residual)
structure, we show in this section that initialization using i.i.d mean-zero random variables with
(non-central) second moment inversely proportional to the geometric mean of the fans:
E[Wl2] =
c
q nιnnout
(2)
for some fixed constant c, results in a constant GR scaling throughout the network.
Proposition 5. Let W0 : m × n and W1 : p × m be weight matrices satisfying the geometric initial-
ization criteria of Equation 2, and let b0 , b1 be zero-initialized bias parameters. Then consider the
following sequence of two layers where x0 and ∆y1 are i.i.d, mean 0, uncorrelated and symmetrically
distributed:
y0 = W0x0 + b0, x1 = ReLU (y0),	y1 = W1x1 + b1.
Then ν0 = ν1 and so γ0 = γ1 .
Proof. Note that the ReLU operation halves both the forward and backward (non-central) second
moments, due to our assumptions on the distributions of x0 and ∆y1 . So:
E[χ2] = 2E[y2], Eay2] = 2Eax2].	⑶
Consider the first weight-gradient ratio, using E[∆Wl2] = E[xl2]E[∆yl2]:
EEWW^ = CE [x2 ]E[∆y0]√≡.
Under our assumptions, back-propagation to ∆x1 results in E[∆x21] = pE[W12]E[∆y12] , so:
1	1	1c
E[∆y02] = - E[∆x2] = - pE[W2]E[∆y2] = - p	E[∆y2],
2	2	2	mp
So:
EiWI = 2cp √mp 厮E[x2]Ej2] = - ^E[x0]Ej2].	(4)
Now consider the second weight-gradient ratio:
EIW^ = C √pmE [x2]E[ay2].
Under our assumptions, applying forward propagation gives E[y02] = nE[W02]E[x02], and so from
Equation 3 we have:
E[χ2] = - nE[W2]E[x2] = - n √^E[x2],
2	2 nm
... EEWWJ = 2c √pm ∙n √⅛E[x2]E [ay2]
=-√npE[x2]E[△y2],
which matches Equation 4, so ν0 = νι.	□
Remark 6. This relation also holds for sequences of (potentially) strided convolutions, but only if
the same kernel size is used everywhere and circular padding is used. The initialization should be
modified to include the kernel size, changing the expression to C/ kl2 nilnnlout .
5
Under review as a conference paper at ICLR 2020
6.1	Traditional Initialization schemes
The most common approaches are the Kaiming (He et al., 2015) and Xavier (Glorot and Bengio,
2010) initializations. The Kaiming technique for ReLU networks is actually one of two approaches:
(fan-in) Var[Wl]
2
nln
2
or (fan-out) Var [ Wι] = F.
nlout
(5)
For the feed-forward network above, assuming random activations, the forward-activation variance
will remain constant in expectation throughout the network if fan-in initialization of weights (LeCun
et al., 2012) is used, whereas the fan-out variant maintains a constant variance of the back-propagated
signal. The constant factor 2 in the above expressions corrects for the variance-reducing effect of the
ReLU activation. Although popularized by He et al. (2015), similar scaling was in use in early neural
network models that used tanh activation functions (Bottou, 1988).
These two principles are clearly in conflict; unless niln = nlout, either the forward variance or backward
variance will become non-constant, or as more commonly expressed, either explode or vanish. No
prima facie reason for preferring one initialization over the other is provided. Unfortunately, there
is some confusion in the literature as many works reference using Kaiming initialization without
specifying if the fan-in or fan-out variant is used.
The Xavier initialization (Glorot and Bengio, 2010) is the closest to our proposed approach. They
balance these conflicting objectives using the arithmetic mean:
Var[Wl]
4
nln + nout
(6)
to “... approximately satisfy our objectives of maintaining activation variances and back-propagated
gradients variance as one moves up or down the network”. This approach to balancing is essentially
heuristic, in contrast to the geometric mean approach that our theory directly guides us to.
6.2	Geometric initialization balances biases
We can use the same proof technique to compute the GR scaling for the bias parameters in a network.
Our update equations change to include the bias term: yl = Wlxl + bl , with bl assumed to be
initialized at zero. We show in Appendix D that:
γlb
E[∆y2]
E[y2]
It is easy to show using the techniques of Section 6 that the biases of consecutive layers have equal
GR scaling as long as geometric initialization is used. However, unlike in the case of weights, we
have less flexibility in the choice of the numerator. Instead of allowing all weights to be scaled by c
for any positive c, we require that c = 2, so that:
E[Wl2] =
2
JnlnnoUt
(7)
Proposition 7. (Appendix D) Consider the setup of Proposition 5. As long as the weights are
initialized following Equation 7 with c = 2 and the biases are initialized to 0, we have that γ0b = γ1b .
6.3	Network input scaling balances weights against biases
It is traditional to normalize a dataset before applying a neural network so that the input vector has
mean 0 and variance 1 in expectation. This principle is rarely quested in modern neural networks,
even though there is no longer a good justification for its use in modern ReLU based networks.
In contrast, our theory provides direct guidance for the choice of input scaling. We show that the
(non-central) second moment of the input affects the GR scaling of bias and weight parameters
differently and that they can be balanced by careful choice of the initialization.
Consider the GR scaling values for the bias and weight parameters in the first layer of a ReLU-based
multilayer perceptron network, as considered in previous sections. We assume the data is already
6
Under review as a conference paper at ICLR 2020
Table 1: Scaling of common layers
Method	Maintains Scaling	Notes
Linear layer	X	Will not be well-conditioned against other layers unless geometric initialization is used
(Strided) convolution	X	As above, but only if all kernel sizes are the same
Skip connections	X	Operations in residual blocks will be scaled correctly against each other, but not against non-residual operations
Average pooling	X	
Max pooling	X	
Dropout	X	
ReLU/LeakyReLU	X	Any positively-homogenous function with degree 1
Sigmoid	X	
Tanhh	X	Maintains scaling if entirely within the linear regime
centered. Then the scaling factors for the weight and bias layers are:
_ in 7 2 2 77 Γ 2^∣ 2 EDy2]	_ 2 E@y2]
Yo = no koρ0E [xo]币7,	γ0b = Po -Ey『 ∙
We can cancel terms to find the value of E x20 that makes these two quantities equal:
E曷]=」∙
Pnonko
In common computer vision architectures, the input planes are the 3 color channels, and the kernel
size is k = 3, giving E x2o ≈ 0∙2. Using the traditional variance-one normalization will result in
the effective learning rate for the bias terms being lower than that of the weight terms. This will result
in potentially slower learning of the bias terms than for the input scaling we propose.
6.4 Output (non-central) second moments
A neural network’s behavior is also very sensitive to the (non-central) second moment of the outputs.
For a convolutional network without pooling layers (but potentially with strided dimensionality
reduction), if geometric-mean initialization is used the activation (non-central) second moments are
given by:
1	nin
E[x2+1] = 2 k2niηE [Wι2]E[x2] = ↑lφ E[x2]∙
The application of a sequence of these layers gives a telescoping product:
E [xL+1] = (YY s∕^ζ! E[χ2] = S^nn E[χ2]∙
l l+ij	〈廿ZnoUt)	o o NnoUt o o
We potentially have independent control over this (non-central) second moment at initialization, as
we can insert a fixed scalar multiplication factor at the end of the network that modifies it. This may
be necessary when adapting a network architecture that was designed and tested under a different
initialization scheme, as the success of the architecture may be partially due to the output scaling
that happens to be produced by that original initialization. We are not aware of any existing theory
guiding the choice of output variance at initialization for the case of log-softmax losses, where it has
a non-trivial effect on the back-propagated signals, although output variances of 0.01 to 0.1 appear to
work well. The output variance should always be checked and potentially corrected when switching
initialization schemes.
7 Designing well-conditioned neural networks
Consider a network where γl is constant throughout. We may add a layer between any two existing
layers without affecting this conditioning, as long as the new layer maintains the activation-gradient
(non-central) second-moment product:
E[∆xl2+1]E[xl2+1] = E[∆xl2]E[xl2],
7
Under review as a conference paper at ICLR 2020
Table 2: Comparison on 26 LIBSVM repository datasets
Method		Average Normalized loss (±0.01)	Worst in #	Best in #
Arithmetic mean	0.90	14	=	3	=
Fan-in	084	3	5
Fan-out	0.88	9	12
Geometric mean	0.81	0	-	6	—
and dimensionality; this follows from Equation 1. For instance, adding a simple scaling layer of the
form xl+1 = 2xl doubles the (non-central) second moment during the forward pass and doubles the
backward (non-central) second moment during back-propagation, which maintains this product:
EAx2+1]E[x2+1] = 2EAx2] ∙ 2E[x2].
When spatial dimensionality changes between layers we can see that the GR scaling is no longer
maintained just by balancing this product, as γ depends directly on the square of the spatial dimension.
Instead, a pooling operation that changes the forward and backward signals in a way that counteracts
the change in spatial dimension is needed. The use of stride-2 convolutions, as well as average
pooling, results in the correct scaling, but other common types of spatial reduction generally do not.
Table 1 lists operations that preserve scaling when inserted into an existing preconditioned network.
Operations such as linear layers preserve the scaling of existing layers but are only themselves well-
scaled if they are initialized correctly. For an architecture such as ResNet-50 that uses operations that
break scaling, αl values can be introduced to correct scaling. In a ResNet-50, residual connections,
max-pooling, and varying kernel sizes need to be corrected for (we describe this procedure in
Appendix E).
It is particularly interesting to note that the evolution in state-of-the-art architectures corresponds
closely to a move from poorly scaled building blocks to well-scaled ones. Early shallow architectures
like LeNet-5 used tanh nonlinearities, which were replaced by the (well-scaled) ReLU, used for
instance in the seminal AlexNet architecture (Krizhevsky et al., 2012). AlexNet and the latter VGG
architectures made heavy use of max-pooling and reshaping before the final layers, both operations
which have been replaced in modern fully-convolutional architectures with (well-scaled) striding
and average-pooling respectively. The use of large kernel sizes is also in decline. The AlexNet
architecture used kernel sizes of 11, 5 and 3, whereas modern ResNet (He et al., 2016) architectures
only use 7, 3 and 1. Furthermore, recent research has shown that replacing the single 7x7 convolution
used with a sequence of three 3x3 convolutions improves performance (He et al., 2018).
8	Experimental Results
We considered a selection of dense and moderate-sparsity multi-class classification datasets from
the LibSVM repository, 26 in total. The same model was used for all datasets, a non-convolutional
ReLU network with 3 weight layers total. The inner two layer widths were fixed at 384 and 64 nodes
respectively. These numbers were chosen to result in a larger gap between the optimization methods,
less difference could be expected if a more typical 2× gap was used. Our results are otherwise
generally robust to the choice of layer widths.
For every dataset, learning rate and initialization combination we ran 10 seeds and picked the median
loss after 5 epochs as the focus of our study (The largest differences can be expected early in training).
Learning rates in the range 21 to 2-12 (in powers of2) were checked for each dataset and initialization
combination, with the best learning rate chosen in each case based off of the median of the 10 seeds.
Training loss was used as the basis of our comparison as we care primarily about convergence
rate, and are comparing identical network architectures. Some additional details concerning the
experimental setup and which datasets were used is available in the Appendix.
Table 1 shows that geometric initialization is the most consistent of the initialization approaches
considered. It has the lowest loss, after normalizing each dataset, and it is never the worst of the
4 methods on any dataset. Interestingly, the fan-out method is most often the best method, but
consideration of the per-dataset plots (Appendix F) shows that it often completely fails to learn for
some problems, which pulls up its average loss and results in it being the worst for 9 datasets.
8
Under review as a conference paper at ICLR 2020
Figure 3: CIFAR-10 training loss for a strided AlexNet architecture. An average of 10 seeds is shown
for each initialization, where for each seed a sliding window of minibatch training loss over 400 steps
was used.
8.1	Convolutional experiments
Testing an initialization method on modern computer vision problems is problematic due to the
heavy architecture search, both automated and manual, that is behind the current best methods. This
search will fit the architecture to the initialization method, in a sense, so any other initialization is
at a disadvantage compared to the one used during architecture search. This is further complicated
by the prevalence of BatchNorm which is not handled in our theory. Instead, to provide a clear
comparison we use an older network with a large variability in kernel sizes, the AlexNet architecture.
This architecture has a large variety of filter sizes (11, 5, 3, linear), which according to our theory will
affect the conditioning adversely, and which should highlight the differences between the methods.
We found that a network with consistent kernel sizes through-out showed only negligible differences
between the initialization methods. The network was modified to replace max-pooling with striding
as max-pooling is not well-scaled by our theory (further details in Appendix F).
Following Section 6.4, we normalize the output of the network at initialization by running a single
batch through the network and adding a fixed scaling factor to the network to produce output
standard deviation 0.05. For our preconditioned variant, we added alpha correction factors following
Section 5 in conjunction with geometric initialization, and compared this against other common
initialization methods. We tested on CIFAR-10 following the standard practice as closely as possible,
as detailed in Appendix F. We performed a geometric learning rate sweep over a power-of-two grid.
Results are shown in Figure 3 for an average of 10 seeds for each initialization. Preconditioning
improves training loss over all other initialization schemes tested, although only by a small margin.
9	Conclusion
Although not a panacea, by using the scaling principle we have introduced, neural networks can be
designed with a reasonable expectation that they will be optimizable by stochastic gradient methods,
minimizing the amount of guess-and-check neural network design. As a consequence of our scaling
principle, we have derived an initialization scheme that automatically preconditions common network
architectures. Most developments in neural network theory attempt to explain the success of existing
techniques post-hoc. Instead, we show the power of the scaling law approach by deriving a new
initialization technique from theory directly.
References
Jimmy Ba, Roger Grosse, and James Martens. Distributed second-order optimization using kronecker-
factored approximations. International Conference On Learning Representations (ICLR2017),
9
Under review as a conference paper at ICLR 2020
2017.
Lukas Balles and Philipp Hennig. Dissecting adam: The sign, magnitude and variance of stochastic
gradients. International conference on machine learning (ICML), 2018.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar. signSGD:
Compressed optimisation for non-convex problems. International conference on machine learning
(ICML), 2018.
Leon Bottou. Reconnaissance de la parole par reseaUx Connexionnistes. In Proceedings of
NeuroNimes 88, pages 197-218, Nimes, France, 1988. URL http://leon.bottou.org/
papers/bottou-88b.
Simon S. Du, Wei Hu, and Jason D. Lee. Algorithmic regularization in learning deep homogeneous
models: Layers are automatically balanced. Neural Information Processing Systems (NIPS), 2018.
Thomas George, Cesar Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. Fast
approximate natural gradient descent in a kronecker-factored eigenbasis, 2018.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and
Statistics, 2010.
Roger Grosse and James Martens. A kronecker-factored approximate fisher matrix for convolution
layers. International Conference on Machine Learning (ICML2016), 2016.
Boris Hanin. Which neural net architectures give rise to exploding and vanishing gradients? In
Advances in Neural Information Processing Systems 31, pages 582-591. Curran Associates, Inc.,
2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Sur-
passing human-level performance on imagenet classification. In Proceedings of the 2015 IEEE
International Conference on Computer Vision (ICCV), 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770-778, 2016.
Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for
image classification with convolutional neural networks. Technical report, Amazon Web Services,
2018.
C. G. J. Jacobi. Ueber eine neue auflosungsart der bei der methode der kleinsten quadrate vorkom-
menden Iinearen gleichungen. Astron. Nachrichten, 1845.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference for Learning Representations (ICLR2015), 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pages 1097-1105,
2012.
Jean Lafond, Nicolas Vasilache, and Leon Bottou. Diagonal rescaling for neural networks. ArXiv
e-prints, 2017.
Rafal Latala. Some estimates of norms of random matrices. Proc. Amer. Math. Soc.,, 2005.
Yann A. LeCun, L6on Bottou, Genevieve B. Orr, and Klaus-Robert Muller. Neural Networks: Tricks
of the Trade, chapter Efficient BackProp. Springer, 2012.
Liam Li and Ameet S. Talwalkar. Random search and reproducibility for neural architecture search.
CoRR, abs/1902.07638, 2019.
James Martens. New insights and perspectives on the natural gradient method. In ArXiv e-prints,
2014.
10
Under review as a conference paper at ICLR 2020
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. International conference on machine learning (ICML), 2015.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In ArXiv e-prints, 2015.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F.
Frangi, editors, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015.
Springer, 2015.
Mark Rudelson and Roman Vershynin. Non-asymptotic theory of random matrices: extreme singular
values. Proceedings of the International Congress of Mathematicians, 2010.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115
(3):211-252,2015. doi: 10.1007∕s11263-015-0816-y.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks. In ArXiv e-prints, 2014.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. international conference on learning representations (ICLR2015), 2015.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. In International conference on machine learning (ICML), pages
1139-1147, 2013.
Terrance Tao. Topics in Random Matrix Theory. American Mathematical Soc.„ 2012.
T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. Compressed
sensing, 2012.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S. Schoenholz, and Jeffrey Penning-
ton. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla
convolutional neural networks. ICML, 2018.
Jure Zbontar, Florian Knoll, Anuroop Sriram, Matthew J. Muckley, Mary Bruno, Aaron Defazio, Marc
Parente, Krzysztof J. Geras, Joe Katsnelson, Hersh Chandarana, Zizhao Zhang, Michal Drozdzal,
Adriana Romero, Michael Rabbat, Pascal Vincent, James Pinkerton, Duo Wang, Nafissa Yakubova,
Erich Owens, C. Lawrence Zitnick, Michael P. Recht, Daniel K. Sodickson, and Yvonne W. Lui.
fastMRI: An open dataset and benchmarks for accelerated MRI. In ArXiv e-prints, 2018.
Hongyi Zhang, Yann N. Dauphin, and Tengyu Ma. Residual learning without normalization via
better initialization. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=H1gsz30cKX.
Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. CoRR,
abs/1611.01578, 2016. URL http://arxiv.org/abs/1611.01578.
11
Under review as a conference paper at ICLR 2020
A GR scaling derivation
Our quantity of interest is the average squared singular value of Gl, which is simply equal to the
(element-wise) non-central second moment of the product of G with a i.i.d normal random vector r:
E [(Gi r)2] = E[JT RJir)2].
Recall that our notation E[X2] refers to the element-wise non-central second moment of the vector.
Through-out the remainder of this work we use non-central moments unless otherwise stated. To
compute the second moment of the elements of Gir, we can calculate the second moment of matrix-
random-vector products against Ji, Ri and JiT separately since R is uncorrelated with Ji, and the
back-propagated gradient ∆yi is uncorrelated with yi (Assumption A3).
JACOBIAN PRODUCTS Ji AND JiT
Note that each row of Ji has niin non-zero elements, each containing a value from xi . This structure
can be written as a block matrix,
「Xl	0	0	-
Ji	=	0	xl	0	,	(8)
0	0	...
Where each xi is a 1 × niin row vector.	This can also be written	as a Kronecker product with an identity
matrix as Inout 0 xi. The value xi is i.i.d random at the bottom layer of the network (Assumption A1).
For layers further up, the multiplication by a random weight matrix from the previous layer ensures
that the entries of xi are identically distributed (Assumption A3). So we have:
E h(Jir)2i = niinE[r2]E[xi2] =niinE[xi2].	(9)
Note that we didn’t assume that the input xi is mean zero, so V ar[xi] 6= E[xi2]. This is needed as
often the input to a layer is the output from a ReLU operation, which will not be mean zero.
For the transposed case, we have a single entry per column, so when multiplying by an i.i.d random
vector u we have:
E [(JiTu)2i = E[u2]E[χ2].	(10)
UPPER HESSIAN Ri PRODUCT
Instead of using Riu, for a random u, we will instead compute it for u = yi/E[yi2], it will have the
same expectation since both Jir and yi are uncorrelated with Ri (Assumption A3). The piecewise
linear structure of the network above yi with respect to the yi makes the structure of Ri particularly
simple. It is a least-squares problem g(yi) = 1 ∣∣Φyi 一 t『for some Φ that is the linearization of the
remainder of the network. The gradient is ∆y = ΦT (Φy - t) and the Hessian is simply R = ΦTΦ.
So we have that
(Uncorr. A1)
(Assumption A2)
Applying this gives:
E (Riu)2 = E[u2]E[(Riyi)2]/E[y2] = E[u2]EAy2]/E[y2].
(11)
12
Under review as a conference paper at ICLR 2020
Combining
To compute E[(Gιr)2] = E[(J∣τRlJlr)2] We then combine the simplifications from Equations 9, 10,
and 11 to give:
E[(Glr)2] = nlnE[x2]2 E^ ∙
E [yl ]
A. 1 The Gauss-Newton matrix
Standard ReLU classification and regression netWorks have a particularly simple structure for the
Hessian With respect to the input, as the netWork’s output is a pieceWise-linear function g feed into a
final layer consisting of a convex log-softmax operation, or a least-squares loss. This structure results
in the Hessian With respect to the input being equivalent to its Gauss-Newton approximation. The
Gauss-NeWton matrix can be Written in a factored form, Which is used in the analysis We perform in
this Work. We emphasize that this is just used as a convenience When Working With diagonal blocks,
the GN representation is not an approximation in this case.
The (Generalized) Gauss-NeWton matrix G is a positive semi-definite approximation of the Hessian of
a non-convex function f, given by factoring f into the composition of tWo functions f(x) = h(g(x))
Where h is convex, and g is approximated by its Jacobian matrix J at x, for the purpose of computing
G:
G = JT (V2h(g(x))) J.
The GN matrix also has close ties to the Fisher information matrix (Martens, 2014), providing another
justification for its use.
Surprisingly, the Gauss-NeWton decomposition can be used to compute diagonal blocks of the Hessian
With respect to the Weights Wl as Well as the inputs (Martens, 2014). To see this, note that for any
activation yl, the layers above may be treated in a combined fashion as the h in a f(Wl) = h(g(Wl))
decomposition of the netWork structure, as they are the composition of a (locally) linear function and
a convex function and thus convex. In this decomposition g(Wl) =Wlxl + bl is a function of Wl
With xl fixed, and as this is linear in Wl, the Gauss-NeWton approximation to the block is thus not an
approximation.
B	Forward and backward second moments
We make heavy use of the equations for forWard propagation and backWard propagation of second
moments, under the assumption that the Weights are uncorrelated to the activations or gradients. For
a convolution
y = CW (x)
With input channels nin, output channels nout , and square k × k kernels, these formulas are (recall our
notation for the second moments is element-Wise for vectors and matrices):
E[y2] = nink2E[W 2]E[x2],
E[∆x2] = noutk2E[W2]E[∆y2].
C	The Weight gradient ratio is equal to GR scaling for MLP
MODELS
Proposition 8. The weight-gradient ratio νl is equal to the GR scaling γl for i.i.d mean-zero
randomly-initialized multilayer perceptron layers under the independence assumptions of Appendix
3.2.
Proof. To see the equivalence, note that under the zero-bias initialization, We have from yl = Wlxl
that:
E[yl2] = nilnE[Wl2]E[xl2],	(12)
13
Under review as a conference paper at ICLR 2020
and so:
E [W2] = E [yl]
Wl ]	nlnE[x2].
The gradient of the weights is given by ∆Wij E[∆Wl2]	∆ylixlj and so its second moment is: E[xl2]E[∆yl2].	(13)
Combining these quantities gives:		
E[∆Wl2] Vl = WlT =	"2『EiF.	□
D Bias scaling
We consider the case of a convolutional neural network with spatial resolution ρ × ρ for greater
generality. Consider the Jacobian of yl with respect to the bias. It has shape Jlb : (nloutρl2) × (nlout).
Each row corresponds to a yl output, and each column a bias weight. As before, we will approximate
the product of G with a random i.i.d unit variance vector r:
Glbr = JlbT RlJlbr,
The structure of Jlb is that each block of ρ2 rows has the same set of 1s in the same column. Only a
single 1 per row. It follows that:
E [(Jbr)2i = L
The calculation of the product of Rl with Jlbr is approximated in the same way as in the weight
scaling calculation. For the J bT product, note that there is an additional ρ2 as each column has ρ2
non-zero entries, each equal to 1. Combining these three quantities gives:
Yb = P2 2.
Proposition 9. Consider the setup of Proposition 5, with the addition of biases:
y0 = W0x0 + b0 ,
x1 = ReLU (y0),
y1 = W1 + b1 .
As long as the weights are initialized following Equation 7 and the biases are initialized to 0, we have
that
γ0b = γ1b .
We will include c = 2 as a variable as it clarifies it’s relation to other quantities. We reuse some
calculations from Proposition 5. Namely that:
E[y2] = %伍即0],
m
E[∆y0] = 1 cχppE[∆yi].
2m
Plugging these into the definition of γ0b :
b= E∆y2] = 2 CpmE E∆y2] = √pE ∆y2]
Y0 = E[y0] = Cpm^ E[x2 ]	= 2√nE[x0].
14
Under review as a conference paper at ICLR 2020
For γ1b, we require the additional quantity:
E[y12] = mE[x12]E W12
=m (1P
12 Vm
=Cr r E[χ0].
Again plugging this in:
E[∆y2]
γ1 = E
=E[∆y2]
c2 qp E[χ0]
=√PE [∆y2]
C2 √nE[χ0].
So comparing these expressions for γ0b and γ1b, we see that γ0b = γ1b if and only if c = 2.
E Conditioning of ResNets Without Normalization Layers
There has been significant recent interest in training residual networks without the use of batch-
normalization or other normalization layers (Zhang et al., 2019). In this section, we explore the
modifications that are necessary to a network for this to be possible and show how to apply our
preconditioning principle to these networks.
The building block of a ResNet model is the residual block:
zl+1 = ReLU (F (zl) + zl) ,
where F is a composition of layers. Unlike classical feedforward architectures, the pass-through
connection results in an exponential increase in the variance of the activations in the network as
the depth increases. A side effect of this is the output of the network becomes exponentially more
sensitive to the input of the network as depth increases, a property characterized by the Lipschitz
constant of the network (Hanin, 2018).
This exponential dependence can be reduced by the introduction of scaling constants sl to each block:
zl+1 = ReLU (slF(zl) + zl) .
The introduction of these constants requires a modification of the block structure to ensure constant
conditioning between blocks. A standard bottleneck block, as used in the ResNet-50 architecture, has
the following form:
y0 = C0 (x0 ),
x1 = ReLU(y0),
y1 = C1 (x1 ),
x2 = ReLU(y1),
y2 = C2(x2),
x3 = ReLU(y2 +x0).
In this notation, C0 is a 1 × 1 convolution that reduces the number of channels 4 fold, C1 is a 3 × 3
convolution with equal input and output channels, and C2 is a 1 × 1 convolution at increases the
number of channels back up 4 fold to the original input count.
If we introduce a scaling factor sl to each block l, then we must also add conditioning multipliers βl
to each convolution to change their GR scaling, as we described in Section 5. The correct scaling
15
Under review as a conference paper at ICLR 2020
constant depends on the scaling constant of the previous block. A simple calculation gives the
equation:
β2 = β2-111+2-7.
The initial β0 and s0 may be chosen arbitrarily. If a flat sl = sis used for all l, then we may use
βl = 1. The block structure including the βl factors is:
y0 =万 CO(XO),
β
x1 = ReLU(yO),
y1=√3βc1(xι),
x2 = ReLU(y1),
y = 1 C2(x2),
x3 = ReLU (sy2 +xO)
The weights of each convolution must then be initialized with the standard deviation modified such
that the combined convolution-scaling operation gives the same output variance as would be given
if the geometric-mean initialization scheme is used without extra scaling constants. For instance,
the initialization of the C convolution must have standard deviation scaled down by dividing by -7
so that the multiplication by - during the forward pass results in the correct forward variance. The
1 /√3 factor is an α correction that corrects for change in kernel shape for the middle convolution.
The variance at initialization must be scaled to correct for the α factor also.
E.1 Correction for mixed residual and non-residual blocks
Since the initial convolution in a ResNet-50 model is also not within a residual block, it’s GR scaling
is different from the convolutions within residual blocks. Consider the composition of a non-residual
followed by a residual block, without max-pooling or ReLUs for simplicity of exposition:
yO = αCO(xO),	x1 = yO,
y1 =s1C1(x1),	z1 = y1+x1.
Without loss of generality, we assume that E x2O = 1, and assume a single channel input and output.
Our goal is to find a constant α, so that γO = γ1 . Recall that when using α scaling factors we must
initialize CO so that the variance of yO is independent of the choice of α. Our scaling factor will also
depend on the kernel sizes used in the two convolutions, so we must include those in the calculations.
From Equation 1, the GR scaling for CO is
γo = α4nlnk2E [x0]2 E^
E[yO]
= α4kO2E[∆yO2].
Note that E[∆y2] = (1 + s2) E[∆z2] so:
Yo = (1 + s2) α4k2E[∆z2],
For the residual convolution, we need to use a modification of the standard GR equation due to the
residual branch. The derivation of γ for non-residual convolutions assumes that the remainder of
the network above the convolution responds linearly (locally) with the scaling of the convolution,
but here due to the residual connection, this is no longer the case. For instance, if the weight were
scaled to zero, the output of the network would not also become zero (recall our assumption of
16
Under review as a conference paper at ICLR 2020
zero-initialization for bias terms). This can be avoided by noting that the ratio E[∆y2]∕E[y2] in the
GR scaling may be computed further up the network, as long as any scaling in between is corrected
for. In particular, we may compute this ratio at the point after the residual addition, as long as we
include the factor s41 to account for this. So we in fact have:
γ1 = s14nilnk12E x122
E[∆z2]
E[z2]
s4 k2 EA2].
We now equate γ0 = γ1 :
s4k2E[∆zj = (1 + s2) O4k0E[∆zi],
1	+ s1
k1	s4	— 4
林∙ E)2 =α.
Therefore to ensure that γ0 = γ1 we need:
2	k1	s21
=k0 ∙ (1 + s1).
Final layer
A similar calculation applies when the residual block is before the non-residual convolution, as in the
last layer linear in the ResNet network, giving a scaling factor for the linear layer (effective kernel
size 1) of:
s2
2	L-1
α = (1+⅛-1) kLi
F	Full experimental results
Details of LibSVM dataset input/output scaling
To prevent the results from being skewed by the number of classes and the number of inputs affecting
the output variance, the logit output of the network was scaled to have standard deviation 0.05 after
the first minibatch evaluation for every method, with the scaling constant fixed thereafter. LayerNorm
was used on the input to whiten the data. Weight decay of 0.00001 was used for every dataset. To
aggregate the losses across datasets we divided by the worst loss across the initializations before
averaging.
LIBSVM PLOTS
Figure 4 shows the interquartile range (25%, 50% and 75% quantiles) of the best learning rate for
each case.
CIFAR10
Following standard practice, training used random augmentations consisting of horizontal flips
and random crops to 32x32, as well as normalization to the interval [-1,+1]. We used SGD with
momentum 0.9, a learning rate schedule of decreases at 150 and 225 epochs, and no weight decay.
The network architecture is the following sequence, with circular “equal” padding used and ReLU
nonlinearities after each convolution:
1.	11x11 stride-1 convolution with 3 input and 64 output channels,
2.	5x5 stride-2 convolution with 64 input and 192 output channels,
3.	3x3 stride-2 convolution with 192 input and 384 output channels,
17
Under review as a conference paper at ICLR 2020
4.	3x3 convolution with 384 input and 256 output channels,
5.	3x3 convolution with 256 input and 256 output channels,
6.	Average pooling to 1x1 followed by reshaping to non-spatial,
7.	Linear layer with 256 input and 4096 output features,
8.	Linear layer with 4096 input and 4096 output features,
9.	Linear layer with 4096 input and 10 output features,
18
Under review as a conference paper at ICLR 2020
0J5 -
OJO -
0.05 -
0.070 -
0.07 -
0.8 -
0.065 -
0.06 -
0.060 -
0.05 -
0.055 -
0.050 -
0.04 -
0.045 -
0.03 -
0.040 -
0.02 -
0.48 -
0.46 -
0.44 -
0.42 -
0.40 -
0.38 -
0.36 -
0.M -
0.57
0.56 -
0.55 -
0.54 -
0.53 -
0.52 -
0.51 -
0.50 -
0.49 -
----aritħ
----faoio
----(Ieem
----fβa-out
—aritt>
---f¾n in
---9βom
—faa out
—aritħ
--f¾n in
--9βom
—fan out
0.15 -
o.ιo -
0.00 -
D.22 -
D.6 -
D.16 -
0.12 -
D.2 -
D.O35 -
D.1O -
Epoch
Epoch
0.8 -
0.7 -
D.6 -
Epoch
Epoch
∞nnect-4
1
o -
0
0.6 -
D.3 -
Epoch
a∞ustac scale
0.96 -
0.92 -
0.90 -
0.08 -
Epoch
2
Epoch
Epoch
0.075 -
1.0 -
0.55 -
0.50 -
0.45 -
0.40 -
0.35 -
0.30 -
0J5 -
0.500 -
0.475 -
0.450 -
0.425 -
0.4Do -
0.375 -
0.350 -
SVlngVide2
Epoch
Epoth
Epoch
0.425 -
0.400 -
0.375 -
0.350 -
0.325 -
0.300 -
0.275 -
0.250
12
1
1 -
1
.0 -
0.9 -
0.8 -
0
7 -
0.6 -
0.5 -
0.4 -
0.3
0.20 -
0.18 -

aloi.scale
—αtiθ>.
--f¾n in
--9βom
—εan-out
0.8 -
0.7 -
iris
---ante
---f¾n in
---9βom
——fanj>ut
vehicle
0.9 -
0.8 -
7 -
0.6 -
0.5 -
0.4 -
Epoch
Epoch
POkeT
1.02 -
1.00 -
0.98 -
Figure 4: Full results for the 26 datasets
19