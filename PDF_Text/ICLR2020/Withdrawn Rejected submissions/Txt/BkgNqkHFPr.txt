Under review as a conference paper at ICLR 2020
Enhanced Convolutional Neural Kernels
Anonymous authors
Paper under double-blind review
Ab stract
Recent research shows that for training with `2 loss, convolutional neural net-
works (CNNs) whose width (number of channels in convolutional layers) goes to
infinity correspond to regression with respect to the CNN Gaussian Process kernel
(CNN-GP) (Novak et al., 2019) if only the last layer is trained, and correspond
to regression with respect to the Convolutional Neural Tangent Kernel (CNTK) if
all layers are trained. An exact algorithm to compute CNTK (Arora et al., 2019)
yielded the finding that classification accuracy of CNTK on CIFAR-10 is within
6-7% of that of the corresponding CNN architecture (best figure being around
78%) which is interesting performance for a fixed kernel.
Here we show how to significantly enhance the performance of these kernels using
two ideas. (1) Modifying the kernel using a new operation called Local Average
Pooling (LAP) which preserves efficient computability of the kernel and inherits
the spirit of standard data augmentation using pixel shifts. Earlier papers were
unable to incorporate naive data augmentation because of the quadratic training
cost of kernel regression. This idea is inspired by Global Average Pooling (GAP),
which we show for CNN-GP and CNTK is equivalent to full translation data aug-
mentation. (2) Representing the input image using a pre-processing technique
proposed by Coates et al. (2011), which uses a single convolutional layer com-
posed of random image patches.
On CIFAR-10, the resulting kernel, CNN-GP with LAP and horizontal flip
data augmentation, achieves 89% accuracy, matching the performance of
AlexNet (Krizhevsky et al., 2012) , and outperforms the best previous classifier
that is not a trained neural network (Mairal, 2016). Similar improvements are
obtained for Fashion-MNIST.
1 Introduction
Recent research shows that for training with `2 loss, convolutional neural networks (CNNs) whose
width (number of channels in convolutional layers) goes to infinity, correspond to regression with
respect to the CNN Gaussian Process kernel (CNN-GP) if only the last layer is trained (Novak et al.,
2019; Garriga-Alonso et al., 2019), and correspond to regression with respect to the Convolutional
Neural Tangent Kernel (CNTK) if all layers are trained (Jacot et al., 2018; Allen-Zhu et al., 2018;
Du et al., 2019b; Arora et al., 2019). Novak et al. (2019); Garriga-Alonso et al. (2019) also im-
plemented CNN-GP and tested its empirical performance. An efficient exact algorithm was given
(Arora et al., 2019) to compute CNTK for CNN architectures, as well as those that include a Global
Average Pooling (GAP) layer (defined below). This is a fixed kernel that inherits some benefits of
CNNs, including exploitation of locality via convolution, as well as multiple layers of processing.
For CIFAR-10, incorporating GAP into the kernel improves classification accuracy by up to 10%
compared to pure convolutional CNTK.
While this performance is encouraging for a fixed kernel, the best accuracy is still under 78%,
which is disappointing even compared to AlexNet. One hope for improving the accuracy further
is to somehow capture modern innovations such as batch normalization, data augmentation, resid-
ual layers, etc. in CNTK. The current paper shows how to incorporate simple data augmentation.
Specifically, the idea of creating new training images from existing images using pixel translation
and flips, while assuming that these operations should not change the label. Since deep learning uses
stochastic gradient descent (SGD), it is trivial to do such data augmentation on the fly. However, it’s
1
Under review as a conference paper at ICLR 2020
unclear how to efficiently incorporate data augmentation in kernel regression, since training time is
quadratic in the number of training images. 1
Thus somehow data augmentation has to be incorporated into the computation of the kernel itself.
The main observation here is that the above-mentioned algorithm for computing CNTK involves a
dynamic programming whose recursion depth is equal to the depth of the corresponding finite CNN.
It is possible to impose symmetry constraints at any desired layer during this computation. In this
viewpoint, it can be shown that prediction using CNTK/CNN-GP with GAP is equivalent to predic-
tion using CNTK/CNN-GP without GAP but with full translation data augmentation with wrap-
around at the boundary. The translation invariance property implicitly assumed in data augmentation
is exactly equivalent to an imposed symmetry constraint in the computation of the CNTK which in
turn is derived from the pooling layer in the CNN. See Section 4 for more details.
Thus GAP corresponds to full translation data augmentation scheme, but in practice such data aug-
mentation creates unrealistic images (cf. Figure 2) and training on them can harm performance.
However, the idea of incorporating symmetry in the dynamic programming leads to a variant we
call Local Average Pooling (LAP). This implicitly is like data augmentation where image labels are
assumed to be invariant to small translation, say by a few pixels. Interestingly, LAP corresponds to
a average pooling layer for CNNs, named box filtering (Szeliski, 2010).
Experimentally, we find LAP significantly enhances the performance as discussed below.
•	In extensive experiments on CIFAR-10 and Fashion-MNIST, we find LAP consistently improves
performance of CNN-GP and CNTK. In particular, we find CNN-GP with LAP achieves 81%
on CIFAR-10 dataset, outperforming the best previous kernel predictor by 3%.
•	When using the technique proposed by Coates et al. (2011), which uses randomly sampled
patches from training data as filters to do pre-processing,2 CNN-GP with LAP and horizon-
tal flip data augmentation achieves 89% accuracy on CIFAR-10, matching the performance of
AlexNet (Krizhevsky et al., 2012) and is the strongest classifier that is not a trained neural net-
work.3
•	We also test performance of CNNs with an extra layer corresponding to LAP and observe that it
improves the performance on certain architectures.
2	Related Work
Data augmentation has long been known to improve the performance of neural network and kernel
methods (Sietsma & Dow, 1991; ScholkoPf et al., 1996). Theoretical study of data augmentation
dates back to Chapelle et al. (2001). Recently, Dao et al. (2018) proposed a theoretical framework
for understanding data augmentation and showed data augmentation with a kernel classifier can
have feature averaging and variance regularization effects. More recently, Chen et al. (2019) quanti-
tatively shows in certain settings, data augmentation Provably imProves the classifier Performance.
For more comPrehensive discussion on data augmentation and its ProPerties, we refer readers to Dao
et al. (2018); Chen et al. (2019) and references therein.
CNN-GP and CNTK corresPond to infinitely wide CNN with different training strategies (only
training the toP layer or training all layers jointly). The corresPondence between infinite neural
networks and kernel machines was first noted by Neal (1996). More recently, this was extended to
deeP and convolutional neural networks (Lee et al., 2018; Matthews et al., 2018; Novak et al., 2019;
Garriga-Alonso et al., 2019). These kernels corresPond to infinitely wide neural networks where
only the last layer is trained. A recent line of work studied overParameterized neural networks
where all layers are trained (Allen-Zhu et al., 2018; Du et al., 2019b; 2018; Li & Liang, 2018; Zou
et al., 2018). Their Proofs imPly the gradient kernel is close to a fixed kernel which only dePends
the training data and neural network architecture. These kernels thus corresPond to infinitely wide
neural networks where are all layers are trained. Jacot et al. (2018) named this kernel, neural tangent
kernel (NTK). Arora et al. (2019) formally Proved infinitely wide neural net Predictor trained by
1For CIFAR 10, the bottleneck of using CNN-GP and CNTK is not solving least square but constructing
kernel. The time comPlexity is O(p2n2) where p is the number of Pixels in each image and n is number of data
Point.
2See Section 6.2 for the Precise Procedure.
3httPs://benchmarks.ai/cifar-10
2
Under review as a conference paper at ICLR 2020
gradient descent is equivalent to NTK predictor. Recently, NTKs induced by various neural network
architectures are derived and shown to achieve strong empirical performance (Arora et al., 2019;
Yang, 2019; Du et al., 2019a).
Global Average Pooling (GAP) was first proposed in Lin et al. (2013) and is common in modern
CNN design (Springenberg et al., 2014; He et al., 2016; Huang et al., 2017). However, current the-
oretical understanding on GAP is still rather limited. It has been conjectured in Lin et al. (2013)
that GAP reduces the number of parameters in the last fully-connected layer and thus avoids over-
fitting, and that GAP is more robust to spatial translations of the input since it sums out the spatial
information. In this work, we study GAP from the CNN-GP and CNTK perspective, and draw an
interesting connection between GAP and data augmentation.
Here we are interested in methods that are not trained neural networks. If the features are predefined
before seeing the data, Oyallon & Mallat (2015) proposed the scattering network which achieves
82% classification accuracy on CIFAR-10. If one uses unsupervised learning methods to extract
features, the method proposed in Coates et al. (2011) is one of the best-performing approaches on
CIFAR-10 preceding modern CNNs. To our knowledge, the best result via unsupervised learning
method in this line is by Mairal (2016), who used the convolutional kernel network to achieve 86%
accuracy on CIFAR-10. In this work we combine CNTK with LAP and the idea in Coates et al.
(2011) to achieve the best performance for classifiers that are not trained neural networks.
3	Preliminaries
3.1	Notation
We use bold-faced letters for vectors, matrices and tensors. For a vector a, let [a]i be its i-th
entry; for a matrix A, let [A]i,j be its (i, j)-th entry; for an order 4 tensor T , let [T]ij,i0j0 be its
(i, j, i0,j0)-th entry. For a symmetric tensor, wet let tr (T) = Pi,j Tij,ij. For an order d tensor
T ∈ RC1 ×C2×...×Cd and an integer α ∈ [Cd], we use T(α) ∈ RC1×C2×...×Cd-1 to denote the order
d - 1 tensor formed by fixing the coordinate of the last dimension of T to be α.
3.2	CNN, CNN-GP AND CNTK
In this section we give formal definitions of CNN, CNN-GP and CNTK that we study in this paper.
Throughout the paper, we let P be the width and Q be the height of the image. We use q ∈ Z+ to
denote the filter size. In practice, q = 1, 3, 5 or 7.
Padding Schemes. In the definition of CNN, CNTK and CNN-GP, we may use different padding
schemes. Let x ∈ RP ×Q be an matrix. For a given index pair (i, j) with i ≤ 0, i ≥ P + 1, j ≤ 0
or j ≥ Q + 1, different padding schemes define different value for [x]i,j. For circular padding, we
define [x]i,j to be [x]i mod P,j mod Q . For zero padding, we simply define [x]i,j to be 0. Note the
difference between circular padding and zero padding occurs only on the boundary of images. We
will prove our theoretical results for the circular padding scheme to avoid boundary effects.
CNN. Now we describe CNN with and without GAP. For any input image x, after L intermediate
layers, we obtain x(L) ∈ RP×Q×C(L) where C(L) is the number of channels of the last layer. See
Section A for the definition of x(L). For the output, there are two choices: with and without GAP.
• Without GAP: the final output is defined as f (θ, x)
PαC=(L1) DW((αL)+1), x((αL))E where x((αL)) ∈
RP×Q, and W((αL)+1) ∈ RP×Q is the weight of the last fully-connected layer.
•	With GAP: the final output is defined as f(θ, x) = PQ PC=1) WL)+1)∙P(i,j)∈[P]×[Q] hx(L)ii,j
where W((αL)+1) ∈ R is the weight of the last fully-connected layer.
CNN-GP and CNTK. Now we describe CNN-GP and CNTK. Let x, x0 be two input images. We
denote the L-th layer’s CNN-GP kernel as Σ(L) (x, x0) ∈ R[P]×[Q]×[P]×[Q] and the L-th layer’s
CNTK kernel as Θ(L) (x, x0) ∈ R[P]×[Q]×[P]×[Q]. See Section A for the precise definitions of
3
Under review as a conference paper at ICLR 2020
Σ(L) (x, x0) and Θ(L) (x, x0). For the output kernel value, again, there are two choices, without
GAP (equivalent to using a fully-connected layer) or with GAP.
•	Without GAP: the output of CNN-GP is ∑fc (x, x0) = tr (∑(L)(x, x0)) and the output of
CNTK is Θfc (x, x0) = tr (Θ(L) (x, x0)).
•	With GAP:	the output of CNN-GP is	ΣGAP (x, x0)	=
P2Q2 Pij,i0,j0∈[P]×[Q]×[P]×[Q] E(L) (X, XO)]i",j0 , and the	OUtpUt Of	CNTK is
θGAP(X, XO) = P2Q2 Pi,j,i0,j0∈[p]×[Q]×[P]×[Q] [θ(L) (X, XO)]i,j,i0,j0 .
Kernel Prediction. Lastly, we recall the formula for kernel regression. For simplicity, throughout
the paper, we will assume all kernels are invertible. Given a kernel K (X, XO) and a dataset (X, y)
with data {(Xi, yi)}iN=1 , define KX ∈ RN×N where [KX]i,j = K(Xi, Xj). The prediction for
unseen data XO is PiN=1 αiK(XO, Xi), where α = K-X1y.
3.3	Data Augmentation Schemes
In this paper we consider two types of data augmentation schemes: translation and horizontal flip.
Translation. Given (i, j) ∈ [P] × [Q], we define the translation operator Tij : RP ×Q×C →
RP ×Q×C: for an image X ∈ RP ×Q×C, [Tij (X)]i0,j0,c = [X]i0+i,j0+j,c for (iO, jO, c) ∈ [P] × [Q] ×
[C]. Here the precise definition of [X]i0+i,j0+j,c depends on the padding scheme. Given a dataset
D = {(Xi, yi)}iN=1 , the full translation data augmentation scheme creates a new dataset DT =
{(Tij (Xi),yi)}(i,j,n)∈[P]×[Q]×[N] and training is performed on DT.
Horizontal Flip. Flip operator F : RP ×Q×C → RP ×Q×C: for an image X ∈ RP ×Q×C,
[F (X)]i,j,c = [X]P +1-i,j,c for (i, j, c) ∈ [P] × [Q] × [C]. Given a dataset D = {(Xi,yi)}iN=1,
the horizontal flip augmentation scheme creates anew dataset of the form DF = {(F (Xi) , yi)}iN=1
and training is performed on DF ∪ D.
4	Equivalence B etween Augmented Kernel and Data
Augmentation
In this section, we demonstrate the equivalence between using data augmentation and using a aug-
mented kernel. To formally discuss the equivalence, we use group theory to describe translation and
horizontal flip operators. We provide the definition of group in Section B for completeness.
It is easyto verifythat {F,I}, {Ti,j}(i,j)∈[p]×[Q], {Ti,j ◦ F}(i,j)∈[p]×[q] ∪ {Ti,j}(i,j)∈[P]×[Q] are
groups, where I is the identity map. From now on, given a dataset (X, y) with data {(Xi, yi)}iN=1
and a group G, the augmented dataset (XG, yG) is defined to be {g(Xi), yi}g∈G,i∈[N] . For
kernel prediction for unseen data XO on the augmented dataset, we have the following formula:
Pi∈[N],g∈G αei,gK(XO, g(Xi)), where αe = K-X1G yG.
To proceed, we define the concept of augmented kernel. Let G be a finite group. Define the aug-
mented kernel KG as KG (X, XO) = Eg∈GEg0∈GK(g(X), gO(XO)) where X, XO are two inputs images.
A key observation is that for CNTK and CNN-GP, when circular padding and GAP is adopted,
these are actually the augmented kernels with the group G = {Ti,j}(i,j)∈[p]×[Q] . Formally, we have
∑gap (x, xo) = PQ∑Gc (x, xo) and Θgap (x, xo) = PQ。，0 (x, xo). The proof for these two
equations is just by checking the formula of these kernels and using definition of circular padding.
By similar proof, one can observe the following invariance property of ΣGAP, ΣFC, ΘGAP and ΘFC,
under all groups mentioned above, including {F, I} and {Ti,j}(i,j)∈[P]×[Q] .
Definition 4.1. A kernel K is invariant under a group G if and only if for any g ∈ G,
K(g(X), g(XO)) = K(X, XO).
Now the following theorem formally states the equivalence between using an augmented kernel on
the dataset and using the kernel on the augmented dataset.
4
Under review as a conference paper at ICLR 2020
Theorem 4.1. Given a group G and a kernel K such that K is invariant under G, then the prediction
of augmented kernel KG with dataset (X, y) is equal to that of kernel K and augmented dataset
(XG,yG). Namely, for any x0 ∈ RP ×Q×C, PiN=1 αiKG(x0,xi) = Pi∈[N],g∈G αei,gK(x0, g(xi))
where a = (KX) 1 y, α = (KXG)-1 y.
The proof is deferred to Appendix B. Two corollaries are directly followed.
Corollary 4.1. For G = {Ti,j}(i,j)∈[P]×[Q], for any given dataset D, the prediction of ΣGAP (or
ΘGAP) with dataset D is equal to the prediction of ΣFC (or ΘFC) with augmented dataset DT .
Corollary 4.2. For G = {F,I}, for any given dataset D, the prediction of ΣGGAP (or ΘGGAP) with
dataset D is equal to the prediction of ΣGAP (or ΘGAP) with augmented dataset DF ∪ D.
Now we discuss implications of Theorem 4.1 and its corollaries. Naively applying data augmen-
tation, with full translation on CNTK or CNN-GP for example, one needs to create a P2Q2 times
larger kernel matrix since there are P Q translation operators, which is often computationally in-
feasible. Instead, we can directly use the augmented kernel (ΣGAP or ΘGAP for the case of full
translation on CNTK or CNN-GP) for prediction, for which one only needs to create a kernel ma-
trix that is as large as the original one. For horizontal flip, although the augmentation kernel is not
as conveniently computed as full translation, Corollary 4.2 still provides a more efficient method for
computing kernel value and solving kernel regression, since the augmented dataset is twice as large
as the original dataset.
5 Local Average Pooling
In this section, we introduce a new operation called Local Average Pooling (LAP). As discussed in
the introduction, full translation data augmentation can create unrealistic images. A natural idea is to
do local translation data augmentation, i.e., restricting the distance of translation. More specifically,
we only allow translation operations T∆i,∆j (cf. Section 3.3) for (∆i, ∆j) ∈ [-c, c]×[-c, c] where c
is a parameter to control the amount of allowed translation. With a proper choice of the parameter c,
translation data augmentation will not create unrealistic images (cf. Figure 2). However, naive local
translation data augmentation is computationally infeasible for kernel methods, even for moderate
choice of c. To remedy this issue, in this section we introduce LAP, which is inspired by the
connection between full translation data augmentation and GAP on CNN-GP and CNTK. Here,
for simplicity, we assume P = Q and derive the formula only for CNTK. Our formula can be
generalized to CNN-GP in a straightforward manner.
Recall that for two given images x and x0, without GAP, the formula for out-
put of CNTK is tr (Θ(x, x0)).	With GAP, the formula for output of CNTK is
pɪ Piji0j0∈[p]4 [Θ (x, x0)]iji0jo. With circular padding, the formula can be rewritten
as p2 E∆i,∆i,∆j,∆”[p]4 Pi,j∈[P]×[P] [θ (x, x0儿+∆i,j+∆j,i+∆i,j+∆j , WhiCh is again equal to
P12 Ε∆i,∆i,∆j,∆j〜[P]4 tr (θ (j∆"∆j (x), T∆i,∆j (x'))} We ignore the 1/P2 scaling factor since
it plays no role in kernel regression.
Now we consider restricted translation operations T∆i,∆j with (∆i, ∆j) ∈ [-c, c] × [-c, c] and
derive the formula for LAP. Assuming circular padding, we have
E∆i,∆i,∆j,∆j〜[-c,c]4 tr (θ (T∆i,∆j (X), T∆<,∆j (XO)))
=(2c +1)4 X∆i,∆i,∆j,∆j∈[-c,c]4 Xij∈[P]2 [θ(x, X XiMijM/Mijf .	⑴
Now we have derived the formula for LAP which is given in Equation 1. Notice that the formula in
Equation 1 is a well-defined quantity for all padding schemes. In particular, assuming zero padding,
when c = P, LAP is equivalent to GAP. When c = 0, LAP is equivalent to no pooling layer.
Another advantage of LAP is that it does not incur significant additional computational cost, since
the formula in Equation 1 can be rewritten as Pi j if jo∈[p]4 [w]i,j,i0,j0 ∙ [Θ(x, x0)]i j if ,, where each
entry in the weight tensor w can be calculated in O(1) time.
Note that the GAP operation in CNN-GP and CNTK corresponds to the GAP layer in CNNs. Here
we observe the following box filtering layer that corresponds to LAP in CNNs. Box filtering layer
5
Under review as a conference paper at ICLR 2020
(BF) is a function RP×Q → RP×Q such that [BF(X)]i,j = (2c++i)2 P∆i,∆j∈[-c,乎 xi+∆i,j+∆j.
This is in fact a standard average pooling layer but with stride 1 and pooling size 2c + 1. We prove
the equivalence between LAP and box filtering layer in Appendix C. In Section 6.3, we test BF on
CNNs to verify its effectiveness.
6	Experiments
In this section we present our empirical findings on CIFAR-10 (Krizhevsky, 2009) and Fashion-
MNIST (Xiao et al., 2017). The detailed experimental setup is reported in Appendix D. When
reporting test accuracies, the best result on the test set is in boldface and the result that corresponds
to the hyper-parameter chosen by cross-validation is underlined.
6.1	Ablation Study on CIFAR-10 and Fashion-MNIST
We perform experiments to study the effect of different values of the c parameter in LAP and hori-
zontal flip data argumentation on CNTK and CNN-GP. For experiments in this section we set the
bias term in CNTK and CNN-GP to be β = 0 (cf. Section A). We use the same architecture for
CNTK and CNN-GP as in Arora et al. (2019). I.e., we stack multiple convolutional layers before
the final pooling layer. We use dto denote the number of convolutions layers, and in our experiments
we set d to be 5, 8, 11 or 14, to study the effect of depth on CNTK and CNN-GP. For CIFAR-10,
we set the c parameter in LAP to be 0, 4, . . . , 32, while for Fashion-MNIST we set the c parameter
in LAP to be 0, 4, . . . , 28. Notice that when c = 32 for CIFAR-10 or c = 28 for Fashion-MNIST,
LAP is equivalent to GAP, and when c = 0, LAP is equivalent to no pooling layer. Results on
CIFAR-10 are reported in Tables 1 and 3. Due to space constraint, results on Fashion-MNIST are
reported in Tables 5 and 6 in Appendix E. In each table, for each combination of c and d, the first
number is the test accuracy without horizontal flip data augmentation (in percentage), and the sec-
ond number (in parentheses) is the test accuracy with horizontal flip data augmentation. To perform
cross-validation to choose the hyper-parameters, we use the last 10000 samples in the training set
of CIFAR-10 and Fashion-MNIST as the validation set and the rest samples as the training set. We
then use the full training set to report the test accuracy. To perform cross-validation, we choose c,
d, CNN or CNN-GP, and whether or not to adopt horizontal flip based on the validation accuracy
(shown in Appendix F). With cross-validation, the resulting accuracy is 82.09% on CIFAR-10 and
94.07% on Fashion-MNIST.
We made the following observations regarding our experimental results.
•	LAP with a proper choice of the parameter c significantly improves the performance of CNTK and
CNN-GP. On CIFAR-10, the best-performing value of c is c = 12 or 16, while on Fashion-
MNIST the best-performing value of c is c = 4. We suspect this difference is due to the nature
of the two datasets: CIFAR-10 contains real-life images and thus allow more translation, while
Fashion-MNIST contains images with centered clothes and thus allow less translation. For both
datasets, the best-performing value of c is consistent across all settings (depth, CNTK or CNN-
GP) that we have considered.
•	Horizontal flip data augmentation is less effective on Fashion-MNIST than on CIFAR-10. There
are two possible explanations for this phenomenon. First, most images in Fashion-MNIST are
nearly horizontally symmetric (e.g., T-shirts and bags). Second, CNTK and CNN-GP have al-
ready achieved a relatively high accuracy on Fashion-MNIST, and thus it is reasonable for hori-
zontal flip data augmentation to be less effective on this dataset.
•	Finally, for CNTK, when c = 0 (no pooling layer) and c = 32 (GAP) our reported test accuracies
are close to those in Arora et al. (2019) on CIFAR-10. For CNN-GP, when c = 0 (no pooling
layer) our reported test accuracies are close to those in Novak et al. (2019) on CIFAR-10 and
Fashion-MNIST. This suggests that we have reproduced previous reported results.
6.2	Improving Performance on CIFAR- 1 0 Using Random Patches Layer
Finally, we explore another interesting question: what is the best performance achievable via
a method that is not a trained neural network? To further improve the performance, we com-
bine CNTK and CNN-GP with LAP, together with the unsupervised learning approach developed
in Coates et al. (2011). Here we use the variant implemented in Recht et al. (2019). More specifi-
cally, we first sample 2048 random image patches with size 5 × 5 from all training images. Then for
the sampled images patches, we subtract the mean of the patches, then normalize them to have unit
6
Under review as a conference paper at ICLR 2020
不	5	8	11	14
0	66.55 (69.87)	66.27 (69.87)	65.85 (69.37)	65.47 (68.90)
4	77.06(79.08)	77.14 (78.96)	77.06 (78.98)	76.52 (78.74)
8	79.24(80.95)	79.25 (81.03)	78.98 (80.94)	78.65 (80.35)
12	80.11 (81.34)	79.79 (81.28)	79.29 (81.14)	79.13 (80.91)
16	79.80 (81.21)	79.71 (81.40)	79.74 (81.09)	79.42 (81.00)
20	79.24(80.67)	79.27 (80.88)	79.30 (80.76)	78.92 (80.39)
24	78.07(79.88)	78.16 (79.79)	78.14 (80.06)	77.87 (80.07)
28	76.91(78.69)	77.33 (79.20)	77.65 (79.56)	77.65 (79.74)
32	76.79(78.53)	77.39 (79.13)	77.63 (79.51)	77.63 (79.74)
Table 1: Test accuracy of CNTK on CIFAR-10.
K	5	8	11	14
4	84.63 (86.64)	84.07 (86.23)	83.29 (85.53)	82.57 (84.81)
8	86.36 (88.32)	85.80 (87.81)	85.01 (87.08)	84.57 (86.53)
12	86.74 (88.35)	86.20 (87.90)	85.60 (87.36)	84.95 (86.99)
16	86.77 (88.36)	86.17 (87.85)	85.60 (87.44)	84.92 (86.98)
20	86.17 (87.77)	85.71 (87.50)	85.14 (87.07)	84.59 (86.84)
Table 2: Test accuracy of random patches layer + CNTK on CIFAR-10.
norm, and finally perform ZCA transformation to the resulting patches. We use the resulting patches
as 2048 filters of a convolutional layer with kernel size 5, stride 1 and no dilation or padding. For an
input image x, we use conv(x) to denote the output of the convolutional layer. As in the implemen-
tation in Recht et al. (2019), we use ReLU(conv(x) - βfeature) and ReLU(-conv(x) - βfeature)
as the input feature for CNTK and CNN-GP. Here we fix βfeature = 1 as in Recht et al. (2019) and
the bias term β in CNTK and CNN-GP to be β = 1. To make the output kernel value invariant
under horizontal flip (cf. Defintion 4.1), for each image patch, we horizontally flipped it and add the
flipped patch into the convolutional layer as a new filter. Thus, for an input CIFAR-10 image of size
32 × 32, the dimension of the output feature is 8192 × 28 × 28. To isolate the effect of randomness in
the choices of the image patches, we fix the random seed to be 0 throughout the experiment. In this
experiment, we set the value of the c parameter in LAP to be 4, 8, 12, . . . , 20 to avoid small and large
values of c. The results are reported in Tables 2 and 4. Similar to the experiments in Section 6.1,
again we set the hyper-parameters by cross-validation, and the resulting accuracy is 88.91%. See
Appendix F for the validation accuracy for different hyper-parameters.
From our experimental results, it is evident that combining CNTK or CNN-GP with additional
feature extractor can significantly improve upon the performance of using solely CNTK or CNN-
GP, and that of using solely the feature extractor Coates et al. (2011). Previously, it has been
reported in Recht et al. (2019) that using solely the feature extractor Coates et al. (2011) (together
with appropriate pooling layer) can only achieve a test accuracy of 84.2% using 256, 000 image
patches, or 83.3% using 32, 000 image patches. Even with the help of horizontal data augmentation,
the feature extractor Coates et al. (2011) can only achieve a test accuracy of 85.6% using 256,
000 image patches, or 85.0% using 32, 000 image patches. Here we use significantly less image
patches (only 2048) but achieve a much better performance, with the help of CNTK and CNN-GP.
In particular, we achieve a performance of 88.91% on CIFAR-10, matching the performance of
AlexNet on the same dataset. In the setting reported in Coates et al. (2011), increasing the number
of sampled image patches will further improve the performance. Here we also conjecture that in
our setting, further increasing the number of sampled image patches can improve the performance
and get close to modern CNNs. However, due the limitation on computational resources, we leave
exploring the effect of number of sampled image patches as a future research direction.
6.3	EXPERIMENTS ON CNN WITH BOX FILTERING LAYER
In Figure 1, we verify the effectiveness of BF on a 10-layer CNN (with Batch Normalization) on
CIFAR-10. The setting of this experiment is reported in Appendix G. Our network structure has no
pooling layer except for the BF layer before the last fully-connected layer. The fully-connected layer
is fixed during the training. Our experiment illustrates that even with a fixed last FC layer, using
7
Under review as a conference paper at ICLR 2020
K	5	8	11	14
0	63.53 (67.90)	65.54 (69.43)	66.42 (70.30)	66.81 (70.48)
4	76.35(78.79)	77.03 (79.30)	77.39 (79.52)	77.35(79.65)
8	79.48(81.32)	79.82 (81.49)	79.76 (81.71)	79.69(81.53)
12	80.40(82.13)	80.64(82.09)	80.58 (82.06)	80.32(81.95)
16	80.36(81.73)	80.78(82.20)	80.59 (82.06)	80.41(81.83)
20	79.87(81.50)	80.15 (81.33)	79.87 (81.46)	79.98(81.35)
24	78.60(79.98)	78.91 (80.48)	79.22 (80.53)	78.94(80.46)
28	77.18(78.84)	78.03 (79.86)	78.45 (79.87)	78.48(80.07)
32	77.00(78.49)	77.85(79.65)	78.49 (80.04)	78.45(80.01)
Table 3: Test accuracy of CNN-GP on CIFAR-10.
K	5	8	11	14
4	85.49 (87.32)	85.37 (87.22)	85.16 (87.11)	84.79 (86.81)
8	87.07 (88.64)	86.82 (88.68)	86.53 (88.40)	86.39 (88.15)
12	87.23 (88.91)	87.12 (88.92)	86.87 (88.66)	86.62 (88.29)
16	87.28 (88.90)	87.11 (88.66)	86.92 (88.61)	86.74 (88.24)
20	86.81 (88.26)	86.77 (88.24)	86.61 (88.14)	86.26 (87.84)
Table 4: Test accuracy of random patches layer + CNN-GP on CIFAR-10.
GAP could improve the performance of CNN. Our experiments also show that BF with appropriate
choice of c achieves better performance than GAP.
(a) With Flip Agumentation
Figure 1: Test accuracy of 10-layer CNN with various values for the c parameter in BF.
(b) Without Flip Agumentation
7	Conclusion
In this paper, inspired by the connection between full translation data augmentation and GAP, we
derive anew operation, LAP, on CNTK and CNN-GP, which consistently improves the performance
on image classification tasks. Combining CNN-GP with LAP and the pre-processing technique pro-
posed by Coates et al. (2011), the resulting kernel achieves 89% accuracy on CIFAR-10, matching
the performance of AlexNet and is the strongest classifier that is not a trained neural network.
Here we list a few future research directions. Is it possible to develop analogs of CNTK or CNN-
GP incorporating modern techniques such as batch norm and residual layers, to further improve the
performance? Moreover, it is an interesting direction to study other components in modern CNNs
through the lens of CNTK and CNN-GP.
8
Under review as a conference paper at ICLR 2020
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019.
Olivier Chapelle, Jason Weston, Leon Bottou, and Vladimir Vapnik. Vicinal risk minimization. In
Advances in neural information processing systems, pp. 416-422, 2001.
Shuxiao Chen, Edgar Dobriban, and Jane H Lee. Invariance reduces variance: Understanding data
augmentation in deep learning and beyond. arXiv preprint arXiv:1907.10905, 2019.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artificial intelli-
gence and statistics, pp. 215-223, 2011.
Tri Dao, Albert Gu, Alexander J Ratner, Virginia Smith, Christopher De Sa, and Christopher Re. A
kernel theory of modern data augmentation. arXiv preprint arXiv:1803.06084, 2018.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018.
Simon S. Du, Kangcheng Hou, BarnabaS Poczos, Ruslan Salakhutdinov, Ruosong Wang, and Keyulu
Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. ArXiv,
abs/1905.13192, 2019a.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019b.
Adria Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional net-
works as shallow gaussian processes. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=Bklfsi0cKm.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep neural networks as gaussian processes. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
B1EA-M-0Z.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. arXiv preprint arXiv:1808.01204, 2018.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400,
2013.
9
Under review as a conference paper at ICLR 2020
Julien Mairal. End-to-end kernel learning with supervised convolutional kernel networks. In Ad-
Vances in neural information processing Systems, pp. 1399-1407, 2016.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271,
2018.
Radford M Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks, pp. 29-53.
Springer, 1996.
Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel A. Abolafia, Jeffrey
Pennington, and Jascha Sohl-dickstein. Bayesian deep convolutional networks with many chan-
nels are gaussian processes. In International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=B1g30j0qF7.
EdoUard Oyallon and StePhane Mallat. Deep roto-translation scattering for object classification.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2865-
2873, 2015.
Benjamin Recht, Rebecca Roelofs, LUdwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers
generalize to imagenet? arXiv preprint arXiv:1902.10811, 2019.
Bernhard Scholkopf, Chris Burges, and Vladimir Vapnik. Incorporating invariances in support VeC-
tor learning machines. In International Conference on Artificial Neural Networks, pp. 47-52.
Springer, 1996.
Jocelyn Sietsma and Robert JF Dow. Creating artificial neural networks that generalize. Neural
networks, 4(1):67-79, 1991.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for
simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.
Richard Szeliski. Computer vision: algorithms and applications. Springer Science & Business
Media, 2010.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms, 2017.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep ReLU networks. arXiv preprint arXiv:1811.08888, 2018.
10
Under review as a conference paper at ICLR 2020
A FORMAL DEFINITIONS OF CNN-GP AND CNTK
We add some additional notations. Let I be the identity matrix, and [n] = {1, 2, . . . , n}. Let ei
be an indicator vector with i-th entry being 1 and other entries being 0, and let 1 denote the all-one
vector. We use Θ to denote the pointwise product and 0 to denote the tensor product. We use diag( ∙)
to transform a vector to a diagonal matrix. We use σ (∙) to denote the activation function, such as the
rectified linear unit (ReLU) function: σ (Z) = max{z, 0}, and σ (∙) to denote the derivative of σ (∙).
We set cσ = 2. Denote by N(μ, Σ) the Gaussian distribution with mean μ and covariance Σ.
Equation equation 2 shows patch [w * x]j depends on [χ∖i-q-ι^+q-1 j-q-1 j+q-ι. For
(i,j,i0,j0) ∈ [P∖ X [Q∖ X [P∖ X [Q∖, define
Dij,i0j0 = {(i + a,j + b, i0 + a0,j0 + b0) ∈ [P∖ X [Q∖ X [P∖ X [Q∖ | -(q - 1)/2 ≤ a, b, a0, b0 ≤ (q - 1)/2}.
Now we define the convolution operation. For a convolutional filter w ∈ Rq×q and an image
x ∈ RP×Q, the convolution operator is defined as
q — 1	q — 1
~~2	~~2
[w * x∖ij = Σ Σ [w∖a+ q+1 ,b+ q+1 [x∖a+i,b+j for i ∈ [P∖,j ∈ [Q].	(2)
a=-q-ɪ b=-q-ɪ
Now we formally define CNN.
•	Let x(0) = x ∈ RP ×Q×C (0) be the input image where C(0) is the initial number of channels.
•	For h = 1, . . . , L, β = 1, . . . , C(h), the intermediate outputs are defined as
C(h—1)	_____________
X(h) —	X	W(h)	*	x(h-1)	+ V ∙	b，, X㈤= /	cσ	σ	(X㈤)
x(β)=2	W(α),(β) *	x(α)十 Y	b⑶，x(β)	= VC(h)	X q X qσ(x(β)J
α=1
where each W((αh)),(β) ∈ Rq×q is a filter with Gaussian initialization and b(β) is a bias term with
Gaussian initialization scaled by γ .
CNN-GP and CNTK
•	For α = 1, . . . , C(0), (i, j, i0, j0) ∈ [P∖ X [Q∖ X [P∖ X [Q∖, define
K((α0)) (X, X0) = X(α) 0 X0(α) and Σ(0) (X, X0)	0 0
C(0)
J XItr "a, B]",)+ β2.
•	For h ∈ [L∖,
-For (i,j, i0,j0) ∈ [P∖ X [Q∖ X [P∖ X [Q∖, define
Λ(h) (X X0) =	Σ(h-1)(X,X)ij,ij
Λij,i0j0(X,X)= Σ(h-1)(X0,X)i0j0,ij
Σ(h-1)(X, X0)ij,i0j0	R2×2
Σ(h-1) (X0, X0)i0j0,i0j0	∈R
-Define K(h)(x, x0), K(h)(x, x0) ∈ RP×q×p×Q,for (i,j,i0,j0) ∈ [P∖ X [Q∖ X [P∖ X [Q∖
K(h)(X,X0) ij,i0j0 =cσ
K(h)(X，XO)L,jo=，，
E	[∙σ (U)σ (v)∖,
(Sv) ~N (0Wo (XH))
E	[σ(u) σ(v)∖ .
(u,V) ~N OAHiOjO (XH))
(3)
(4)
- Define Σ(h)(X, X0) ∈ RP×Q×P×Q, for (i, j, i0, j0) ∈ [P∖ X [Q∖ X [P∖ X [Q∖
h*(h)(X, x0)iij,iθjθ =q2tr
(X, X0)i	+β2.
Dij,iOjO
TLT ,	/	f ∖	1	(	f∖ 1	∙ ∙1	. 1	K l-ΓI X	,	. zɪ , , 1
Note that Σ(X, X0) and Σ(X, X0) share similar structures as their NTK counterparts (Jacot et al.,
2018). The only difference is that we have one more step, taking the trace over patches. This step
represents the convolution operation in the corresponding CNN. Next, we can use a recursion to
compute the final kernel value.
11
Under review as a conference paper at ICLR 2020
1.	First, we define Θ(0) (x, x0) = Σ(0) (x, x0).
2.	For h = 1, . . . , L and (i, j, i0, j0) ∈ [P] × [Q] × [P] × [Q], we define
hθ(h)(x, x0)i	= ɪtr (hiK(h)(x, x0) Θ Θ(I)(x, x0) + K(h)(x, x0)i [+ β2.
ij,i0j0	q2	Dij,i0j0
B Additional definition and proof for Section 4
Definition B.1 (Group). (G, ◦) is a group, if and only if
1.	each element g ∈ G is a operator: RP ×Q×C →	RP ×Q×C;
2.	∀g1,g2 ∈ G,g1 ◦ g2 ∈ G, where (g1 ◦ g2)(x) is	defined as g1(g2(x)).
3.	∀g1,g2,g3 ∈ G, (g1 ◦ g2) ◦ g3 = g1 ◦ (g2 ◦ g3).
4.	∃e ∈ G, such that ∀g ∈ G, e ◦ g = g ◦ e = g.
5.	∀g1 ∈ G, ∃g2 ∈ G, such that g1 ◦ g2 = g2 ◦ g1	= e. We denote	g2	as the inverse of g1, namely,
g1-1.
Proof of Theorem 4.1. Since we assume KGX and KXG are invertible, both α and αe are uniquely
defined. Now We claim &g = {(¾,g}i∈[N] ∈ RN is equal to 战 for all g ∈ G.
By the invariance of K under G, for all j ∈ [N] and g0 ∈ G,
X	|GK(OO(Xj),g(xi)) = X	1GK((g-1。g0)(xj), xi)
i∈[N],g∈G	i∈[N],g∈G
=	αiEg∈GK(g(xj), xi)
i∈[N]
=	αiKG(xj, xi)
i∈[N]
= yj .
Note that αe is defined as the unique solution of KXG αe = yG , the claim has been verified.
Similarly, we have
X	∣αGi∣ K(x0,g(xi)) = X 0iEg∈GK(g-1(x0), xi) = X αiKG(x0, xi).
i∈[N],g∈G	i∈[N]	i∈[N]
□
C EQUIVALENCE B ETWEEN LAP AND BOX FILTERING LAYER.
For a CNN with abox filtering layer before the final fully-connected layer, the final output is defined
as f(θ, x) = PC(L) DW(L+1), BF x(L)E, where x(L) ∈ RP×Q, andW(L+1) ∈ RP×Q is the
,	α=1	(α)	,	(α)	,	(α)	,	(α)
weight of the last fully-connected layer.
Now we establish the equivalence between BF and LAP on CNTK. The equivalence on CNN-
GP can be derived similarly. Let ΘBF (x, x0) ∈ R[P]×[Q]×[P]×[Q] be the CNTK kernel of
BF x((αL)) . Since BF is just a linear operation, we have
[Θbf (x, x0)] - --o -O = ---^X	∣^Θ(L) (x, x0)i	.
i,j,i ,j	(2c +1)4∆i,∆j ,∆⅛ ∈[-C,c]4	S+∆M-+∆
By the formula of the output kernel value for CNTK without GAP, we obtain
tr(θBF (x, x0)) =(2c + ipXX
[Θ(x, x0)]i+∆i,j+∆j ,i+∆0i,j+∆0j .
∆i,∆0i,∆j,∆0j∈[-c,c]4 i,j∈[P]×[Q]
12
Under review as a conference paper at ICLR 2020
(a) GAP
Figure 2: Randomly sampled images with full translation data augmentation and local translation
data augmentation from CIFAR-10. Full translation data augmentation can create unrealistic images
that harm the performance whereas local translation data augmentation creates more realistic images.
D Experimental Setup in Section 6
For both CIFAR-10 and Fashion-MNIST we use the full training set and report the test accuracy on
the full test set. Throughout this section we only consider 3 × 3 convolutional filters with stride 1 and
no dilation. In the convolutional layers in CNTK and CNN-GP, we use zero padding with pad size
1 to ensure the input of each layer has the same size. We use zero padding for LAP throughout the
experiment. We perform standard preprocessing (mean subtraction and standard deviation division)
for all images.
In all experiments, we perform kernel ridge regression to utilize the calculated kernel values4. We
normalize the kernel matrices so that all diagonal entries are ones. Equivalently, we ensure all
features have unit norm in RKHS. Since the resulting kernel matrices are usually ill-conditioned,
we set the regularization term λ = 5 × 10-5, to make inverting kernel matrices numerically stable.
We use one-hot encodings of the labels as regression targets. We use scipy.linalg.solve to
solve the corresponding kernel ridge regression problem.
The kernel value of CNTK and CNN-GP are calculated using the CuPy package. We write native
CUDA codes to speed up the calculation of the kernel values. All experiments are performed on
Amazon Web Services (AWS), using (possibly multiple) NVIDIA Tesla V100 GPUs. For efficiency
considerations, all kernel values are computed with 32-bit precision.
One unique advantage of the dynamic programming algorithm for calculating CNTK and CNN-
GP is that we do not need repeat experiments for, say, different values of c in LAP and different
depths. With our highly-optimized native CUDA codes, we spend roughly 1,000 GPU hours on
calculating all kernel values for each dataset.
4We also tried kernel SVM but found it significantly degrading the performance, and thus do not include
the results.
13
Under review as a conference paper at ICLR 2020
E TEST ACCURACY OF CNTK AND CNN-GP ON FASHION-MNIST
不	5	8	11	14
0	92.25 (92.56)	92.22 (92.51)	92.11 (92.29)	91.76 (92.17)
4	93.76(94.07)	93.69 (93.86)	93.55 (93.74)	93.37(93.58)
8	93.72(93.96)	93.67 (93.78)	93.50 (93.58)	93.32(93.51)
12	93.59(93.80)	93.58 (93.70)	93.35 (93.44)	93.21(93.40)
16	93.50(93.62)	93.42 (93.63)	93.27 (93.40)	93.10(93.25)
20	93.10(93.34)	93.17 (93.49)	93.20 (93.34)	92.99(93.18)
24	92.77(93.04)	93.07 (93.44)	93.11 (93.31)	93.02(93.21)
28	92.80(92.98)	93.08(93.42)	93.12 (93.28)	92.97(93.19)
Table 5: Test accuracy of CNTK on Fashion-MNIST.
不	5	8	11	14
0	91.47 (91.81)	91.96 (92.37)	92.09 (92.60)	92.22 (92.72)
4	93.44 (93.60)	93.59 (93.79)	93.63 (93.76)	93.59 (93.64)
8	93.26 (93.16)	93.41 (93.51)	93.31 (93.52)	93.39 (93.46)
12	92.83 (92.94)	93.07 (93.20)	93.11 (93.15)	92.94 (93.09)
16	92.46 (92.51)	92.58 (92.83)	92.64 (92.92)	92.68 (93.07)
20	91.83 (91.72)	92.35 (92.42)	92.49 (92.79)	92.51 (92.69)
24	91.15 (91.40)	92.10 (92.18)	92.29 (92.60)	92.41 (92.77)
28	91.30 (91.37)	92.03 (92.27)	92.41 (92.79)	92.41 (92.74)
Table 6: Test accuracy of CNN-GP on Fashion-MNIST.
F VALIDATION ACCURACY OF CNTK AND CNN-GP ON CIFAR-10 AND
FASHION-MNIST
X	5	8	11	14
0	64.26 (68.42)	64.47 (68.23)	63.94 (67.80)	63.29 (67.00)
4	75.97 (78.87)	75.89 (78.99)	75.65 (78.56)	75.40 (78.19)
8	77.93 (80.65)	77.90 (80.69)	77.65 (80.41)	76.92 (79.94)
12	78.51 (80.73)	78.47 (80.85)	78.18 (80.57)	77.71 (80.19)
16	78.47 (80.39)	78.69 (80.56)	78.34 (80.17)	77.74 (79.97)
20	77.86 (79.69)	77.81 (79.81)	77.38 (79.55)	76.88 (79.46)
24	76.59 (78.12)	76.80 (78.63)	76.44 (78.79)	76.18 (78.73)
28	75.44 (77.08)	76.15 (78.20)	76.10 (78.30)	75.95 (78.37)
32	75.33 (76.99)	76.04 (78.09)	76.08 (78.27)	75.99 (78.32)
Table 7: Validation accuracy of CNTK on CIFAR-10.
14
Under review as a conference paper at ICLR 2020
K	5	8	11	14
0	62.49 (66.63)	64.25 (68.20)	64.94 (69.01)	65.35 (69.29)
4	75.31(78.59)	76.05 (79.20)	76.05 (79.17)	76.20 (79.09)
8	78.17(81.02)	78.53 (81.29)	78.36 (81.20)	78.05 (80.98)
12	79.19(81.38)	79.08 (81.66)	79.13 (81.52)	78.90 (81.10)
16	79.26 (81.18)	79.24 (81.37)	78.85 (81.33)	78.82 (80.84)
20	78.72 (80.61)	78.72 (80.85)	78.45 (80.60)	78.08 (80.22)
24	77.31(79.01)	77.59 (79.49)	77.41 (79.56)	77.26 (79.38)
28	76.01(77.60)	76.60 (78.32)	76.57 (78.76)	76.86 (79.01)
32	75.72(77.54)	76.42 (78.47)	76.56 (78.94)	76.63 (78.87)
Table 8: Validation accuracy of CNN-GP on CIFAR-10.
	5	8	11	14
4	83.89 (85.76)	83.13 (85.40)	82.62 (84.95)	82.02 (84.43)
8	85.52 (87.59)	84.88 (87.12)	84.30 (86.69)	83.84 (86.10)
12	85.71 (87.85)	85.32 (87.42)	84.81 (87.02)	84.26 (86.58)
16	85.68 (87.76)	85.19 (87.30)	84.71 (86.83)	84.47 (86.40)
20	85.26 (87.11)	84.91 (86.67)	84.44 (86.40)	84.09 (86.17)
Table 9: Validation accuracy of additional feature extractor + CNTK on CIFAR-10.
K	5	8	11	14
4	84.03 (86.16)	84.21 (86.38)	84.15 (86.33)	83.98 (86.04)
8	85.85 (87.94)	85.87 (88.03)	85.70 (87.87)	85.49 (87.62)
12	86.37 (88.33)	86.38 (88.25)	86.06 (88.12)	85.69 (87.82)
16	86.06 (88.27)	86.21 (88.05)	86.01 (87.87)	85.58 (87.74)
20	85.73 (87.71)	85.79 (87.60)	85.73 (87.54)	85.27 (87.21)
Table 10: Validation accuracy of additional feature extractor + CNN-GP on CIFAR-10.
K	5	8	11	14
0	92.07 (92.30)	92.08 (92.21)	91.79 (91.99)	91.51 (91.72)
4	93.84 (93.96)	93.82 (93.83)	93.58 (93.74)	93.40 (93.57)
8	93.82 (93.96)	93.80 (93.77)	93.56 (93.71)	93.37 (93.57)
12	93.71 (93.83)	93.60 (93.72)	93.45 (93.58)	93.41 (93.45)
16	93.59 (93.73)	93.39 (93.63)	93.34 (93.53)	93.21 (93.45)
20	93.24 (93.44)	93.29 (93.42)	93.26 (93.30)	93.19 (93.31)
24	93.16 (93.28)	93.21 (93.39)	93.30 (93.32)	93.22 (93.32)
28	93.11 (93.23)	93.21 (93.33)	93.29 (93.29)	93.28 (93.31)
Table 11: Validation accuracy of CNTK on Fashion-MNIST.
15
Under review as a conference paper at ICLR 2020
X	5	8	11	14
0	91.13 (91.43)	91.57 (91.77)	91.85 (91.92)	91.94 (92.08)
4	93.44(93.55)	93.57 (93.54)	93.69 (93.68)	93.58(93.64)
8	93.57(93.67)	93.51 (93.68)	93.52 (93.72)	93.44(93.58)
12	93.15(93.36)	93.49 (93.59)	93.25 (93.52)	93.23(93.44)
16	92.83(92.84)	93.01 (93.19)	93.01 (93.27)	92.95(93.18)
20	92.29(92.45)	92.60 (92.82)	92.60 (92.93)	92.78(93.10)
24	91.76(92.04)	92.28 (92.63)	92.57 (92.86)	92.58(92.78)
28	91.79(92.00)	92.32(92.56)	92.56 (92.77)	92.70(93.00)
Table 12: Validation accuracy of CNN-GP on Fashion-MNIST.
G Setting of the Experiment in Section 6.3
The total number of training epochs is 80, and the learning rate is 0.1 initially, decayed by 10 at
epoch 40 and 60 respectively. The momentum is 0.9 and the weight decay factor is 0.0005. In
Figure 1, the blue line reports the average test accuracy of the last 10 epochs, while the red line
reports the best test accuracy of the total 80 epochs. Each experiment is repeated for 3 times. We
use circular padding for both convolutional layers and the BF layer. The last data point with largest
x-coordinate reported in Figure 1 corresponds to GAP.
16