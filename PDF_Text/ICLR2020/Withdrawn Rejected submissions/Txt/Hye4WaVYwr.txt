Under review as a conference paper at ICLR 2020
B ootstrapping the Expressivity with Model-
based Planning
Anonymous authors
Paper under double-blind review
Ab stract
We compare the model-free reinforcement learning with the model-based ap-
proaches through the lens of the expressive power of neural networks for poli-
cies, Q-functions, and dynamics. We show, theoretically and empirically, that
even for one-dimensional continuous state space, there are many MDPs whose
optimal Q-functions and policies are much more complex than the dynamics.
We hypothesize many real-world MDPs also have a similar property. For these
MDPs, model-based planning is a favorable algorithm, because the resulting poli-
cies can approximate the optimal policy significantly better than a neural network
parameterization can, and model-free or model-based policy optimization rely on
policy parameterization. Motivated by the theory, we apply a simple multi-step
model-based bootstrapping planner (BOOTS) to bootstrap a weak Q-function into
a stronger policy. Empirical results show that applying BOOTS on top of model-
based or model-free policy optimization algorithms at the test time improves the
performance on MuJoCo benchmark tasks.
1 Introduction
Model-based deep reinforcement learning (RL) algorithms offer a lot of potentials in achieving sig-
nificantly better sample efficiency than the model-free algorithms for continuous control tasks. We
can largely categorize the model-based deep RL algorithms into two types: 1. model-based policy
optimization algorithms which learn policies or Q-functions, parameterized by neural networks, on
the estimated dynamics, using off-the-shelf model-free algorithms or their variants (Luo et al., 2019;
Janner et al., 2019; Kaiser et al., 2019; Kurutach et al., 2018; Feinberg et al., 2018; Buckman et al.,
2018), and 2. model-based planning algorithms, which plan with the estimated dynamics Nagabandi
et al. (2018); Chua et al. (2018); Wang & Ba (2019).
A deeper theoretical understanding of the pros and cons of model-based and the model-free algo-
rithms in the continuous state space case will provide guiding principles for designing and applying
new sample-efficient methods. The prior work on the comparisons of model-based and model-free
algorithms mostly focuses on their sample efficiency gap, in the case of tabular MDPs (Zanette
& Brunskill, 2019; Jin et al., 2018), linear quadratic regulator (Tu & Recht, 2018), and contextual
decision process with sparse reward (Sun et al., 2019).
In this paper, we theoretically compare model-based RL and model-free RL in the continuous state
space through the lens of approximability by neural networks, and then use the insight to design
practical algorithms. What is the representation power of neural networks for expressing the Q-
function, the policy, and the dynamics? How do the model-based and model-free algorithms utilize
the expressivity of neural networks?
Our main finding is that even for the case of one-dimensional continuous state space, there can be a
massive gap between the approximability of Q-function and the policy and that of the dynamics:
The optimal Q-function and policy can be significantly more complex than the dynamics.
We construct environments where the dynamics are simply piecewise linear functions with constant
pieces, but the optimal Q-functions and the optimal policy require an exponential (in the horizon)
* indicates equal contribution
1
Under review as a conference paper at ICLR 2020
Figure 1: Left: The dynam-
ics of two randomly gener-
ated MDPs (from the RAND,
and SEMI-RAND methods
outlined in Section 4.3 and
detailed in Appendix D.1).
Right: The corresponding
Q-functions which are more
complex than the dynamics
(more details in Section 4.3).
number of linear pieces, or exponentially wide neural networks, to approximate.1 The approximabil-
ity gap can also be observed empirically on (semi-) randomly generated piecewise linear dynamics
with a decent chance. (See Figure 1 for two examples.)
When the approximability gap occurs, any deep RL algorithms with policies parameterized by neural
networks will suffer from a sub-optimal performance. These algorithms include both model-free
algorithms such as DQN (Mnih et al., 2015) and SAC (Haarnoja et al., 2018), and model-based
policy optimization algorithms such as SLBO (Luo et al., 2019) and MBPO (Janner et al., 2019).
To validate the intuition, we empirically apply these algorithms to the constructed or the randomly
generated MDPs. Indeed, they fail to converge to the optimal rewards even with sufficient samples,
which suggests that they suffer from the lack of expressivity.
However, in such cases, model-based planning algorithms should not suffer from the lack of ex-
pressivity, because they only use the learned, parameterized dynamics, which are easy to express.
The policy obtained from the planning is the maximizer of the total future reward on the learned
dynamics, and can have an exponential (in the horizon) number of pieces even if the dynamics has
only a constant number of pieces. In fact, even a partial planner can help improve the expressivity of
the policy. If we plan for k steps and then resort to some Q-function for estimating the total reward
of the remaining steps, we can obtain a policy with 2k more pieces than what Q-function has.
We hypothesize that the real-world continuous control tasks also have a more complex optimal Q-
function and a policy than the dynamics. The theoretical analysis of the synthetic dynamics suggests
that a model-based few-steps planner on top of a parameterized Q-function will outperform the orig-
inal Q-function because of the addtional expressivity introduced by the planning. We empirically
verify the intuition on MuJoCo benchmark tasks. We show that applying a model-based planner on
top of Q-functions learned from model-based or model-free policy optimization algorithms in the
test time leads to significant gains over the original Q-function or policy.
In summary, our contributions are:
1.	We construct continuous state space MDPs whose Q-functions and policies are proved to
be more complex than the dynamics (Sections 4.1 and 4.2.)
2.	We empirically show that with a decent chance, (semi-)randomly generated piecewise lin-
ear MDPs also have complex Q-functions (Section 4.3.)
3.	We show theoretically and empirically that the model-free RL or model-based policy op-
timization algorithms suffer from the lack of expressivity for the constructed MDPs (Sec-
tions 4.3), whereas model-based planning solve the problem efficiently (Section 5.2.)
4.	Inspired by the theory, we propose a simple model-based bootstrapping planner (BOOTS),
which can be applied on top of any model-free or model-based Q-learning algorithms at
1In turn, the dynamics can also be much more complex than the Q-function. Consider the following situa-
tion: a subset of the coordinates of the state space can be arbitrarily difficult to express by neural networks, but
the reward function can only depend on the rest of the coordinates and remain simple.
2
Under review as a conference paper at ICLR 2020
the test time. Empirical results show that BOOTS improves the performance on MuJoCo
benchmark tasks, and outperforms previous state-of-the-art on MuJoCo humanoid environ-
ment.
2	Additional Related Work
Comparisons with Prior Theoretical Work. Model-based RL has been extensively studied in
the tabular case (see (Zanette & Brunskill, 2019; Azar et al., 2017) and the references therein), but
much less so in the context of deep neural networks approximators and continuous state space. (Luo
et al., 2019) give sample complexity and convergence guarantees suing principle of optimism in the
face of uncertainty for non-linear dynamics.
Below we review several prior results regarding model-based versus model-free dichotomy in var-
ious settings. We note that our work focuses on the angle of expressivity, whereas the work below
focuses on the sample efficiency.
Tabular MDPs. The extensive study in tabular MDP setting leaves little gap in their sample com-
plexity of model-based and model-free algorithms, whereas the space complexity seems to be the
main difference. (Strehl et al., 2006). The best sample complexity bounds for model-based tabular
RL (Azar et al., 2017; Zanette & Brunskill, 2019) and model-free tabular RL (Jin et al., 2018) only
differ by a poly(H) multiplicative factor (where H is the horizon.)
Linear Quadratic Regulator. Dean et al. (2018) and Dean et al. (2017) provided sample complexity
bound for model-based LQR. Recently, Tu & Recht (2018) analyzed sample efficiency of the model-
based and model-free problem in the setting of Linear Quadratic Regulator, and proved an O(d) gap
in sample complexity, where d is the dimension of state space. Unlike tabular MDP case, the space
complexity of model-based and model-free algorithms has little difference. The sample-efficiency
gap mostly comes from that dynamics learning has d-dimensional supervisions, whereas Q-learning
has only one-dimensional supervision.
Contextual Decision Process (with function approximator). Sun et al. (2019) prove an ex-
ponential information-theoretical gap between mode-based and model-free algorithms in the fac-
tored MDP setting. Their definition of model-free algorithms requires an exact parameterization:
the value-function hypothesis class should be exactly the family of optimal value-functions in-
duced by the MDP family. This limits the application to deep reinforcement learning where over-
parameterized neural networks are frequently used. Moreover, a crucial reason for the failure of the
model-free algorithms is that the reward is designed to be sparse.
Related Empirical Work. A large family of model-based RL algorithms uses existing model-free
algorithms of its variant on the learned dynamics. MBPO (Janner et al., 2019), STEVE (Buckman
et al., 2018), and MVE (Feinberg et al., 2018) are model-based Q-learning-based policy optimization
algorithms, which can be viewed as modern extensions and improvements of the early model-based
Q-learning framework, Dyna (Sutton, 1990). SLBO (Luo et al., 2019) is a model-based policy
optimization algorithm using TRPO as the algorithm in the learned environment.
Another way to exploit the dynamics is to use it to perform model-based planning. Racaniere et al.
(2017) and Du & Narasimhan (2019) use the model to generated additional extra data to do plan-
ning implicitly. Chua et al. (2018) study how to combine an ensemble of probabilistic models and
planning, which is followed by Wang & Ba (2019), which introduces a policy network to distill
knowledge from a planner and provides a prior for the planner. Piche et al. (2018) uses methods in
Sequential Monte Carlo in the context of control as inference. Oh et al. (2017) trains a Q-function
and then perform lookahead planning. Nagabandi et al. (2018) uses random shooting as the plan-
ning algorithm. Lowrey et al. (2018) uses the dynamics to improve the performance of model-free
algorithms.
Heess et al. (2015) backprops through a stochastic computation graph with a stochastic gradient
to optimize the policy under the learned dynamics. Levine & Koltun (2013) distills a policy from
trajectory optimization. Rajeswaran et al. (2016) trains a policy adversarially robust to the worst dy-
namics in the ensemble. Clavera et al. (2018) reformulates the problem as a meta-learning problem
and using meta-learning algorithms. Predictron (Silver et al., 2017) learns a dynamics and value
function and then use them to predict the future reward sequences.
3
Under review as a conference paper at ICLR 2020
Another line of work focus on how to improve the learned dynamics model. Many of them use an
ensemble of models (Kurutach et al., 2018; Rajeswaran et al., 2016; Clavera et al., 2018), which
are further extended to an ensemble of probabilistic models (Chua et al., 2018; Wang & Ba, 2019).
Luo et al. (2019) designs a discrepancy bound for learning the dynamics model. Talvitie (2014)
augments the data for model training in a way that the model can output a real observation from
its own prediction. Malik et al. (2019) calibrates the model’s uncertainty so that the model’s output
distribution should match the frequency of predicted states. Oh et al. (2017) learns a representation
of states by predicting rewards and future returns using representation.
3	Preliminaries
Markov Decision Process. A Markov Decision Process (MDP) is a tuple hS, A, f, r, γi, where S
is the state space, A the action space, f : S × A → ∆(S) the transition dynamics that maps a state
action pair to a probability distribution of the next state, γ the discount factor, and r ∈ RS×A the
reward function. Throughout this paper, we will consider deterministic dynamics, which, with slight
abuse of notation, will be denoted by f : S × A → S .
A deterministic policy π : S → A maps a state to an action. The value function for the policy
is defined as is defined V π(s) d=ef Ph∞=1 γh-1r(sh, ah). where ah = π(sh), s1 = s and sh+1 =
f(sh, ah).
An RL agent aims to find a policy π that maximizes the expected total reward defined as
η(∏)=ef Esι~μ [V π(sι)],
where μ is the distribution of the initial state.
Bellman Equation. Let π? be the optimal policy, and V? the optimal value function (that is, the
value function for policy π?). The value function Vπ for policy π and optimal value function V?
satisfy the Bellman equation and Bellman optimality equation, respectively. Let Qπ and Q? defines
the state-action value function for policy π and optimal state-action value function. Then, for a
deterministic dynamics f , we have
J Vn(S)= Qπ(s,π(S)),	and J V?(S)= maχa∈A Q*(s,a),	⑴
I Qn(S, a) = r(4 s,a) + YVπ(f(s, a)),	1 Q?(s,a) = r(s,a) + YV*(f(s,a)).
Denote the Bellman operator for dynamics f by Bf :	(Bf [Q]) (S, a)	= r(S, a) +
maχa0 YQ(f(S, a), a0).
Neural Networks. We focus on fully-connected neural nets with ReLU function as activations. A
one-dimensional input and one-dimensional output ReLU neural net represents a piecewise linear
function. A two-layer ReLU neural net with d hidden neurons represents a piecewise linear function
with at most (d + 1) pieces. Similarly, an H-layer neural net with d hidden neurons in each layer
represents a piecewise linear function with at most (d + 1)H pieces (Pascanu et al., 2013).
Problem Setting and Notations. In this paper, we focus on continuous state space, discrete action
space MDPs with S ⊂ R. We assume the dynamics is deterministic (that is, St+1 = f(St, at)), and
the reward is known to the agent. Let bxc denote the floor function of x, that is, the greatest integer
less than or equal to x. We use IH to denote the indicator function.
4 APPROXIMABILITY OF Q-FUNCTIONS AND DYNAMICS
We show that there exist MDPs in one-dimensional continuous state space that have simple dynam-
ics but complex Q-functions and policies. Moreover, any polynomial-size neural network function
approximator of the Q-function or policy will result in a sub-optimal expected total reward, and
learning Q-functions parameterized by neural networks requires fundamentally an exponential num-
ber of samples (Section 4.2). Section 4.3 illustrates the phenomena that Q-function is more complex
than the dynamics occurring frequently and naturally even with random MDP, beyond the theoretical
construction.
4
Under review as a conference paper at ICLR 2020
Action a
Action a
Action a = 0
Action a = 1
dynamics
reward function
(a) Visualization of dynamics for
action a = 0, 1.
(b) The reward function r(s, 0)
and r(s, 1).
Figure 2: A visualization of the dynamics, the reward function in the MDP defined in Definition 4.1,
and the approximation of its optimal Q-function for the effective horizon H = 4. We can also
construct slightly more involved construction with Lipschitz dynamics and very similar properties.
Please see Appendix C.
(c) Approximation of optimal Q-
function Q? (s, a)
4.1	A PROVABLE CONSTRUCTION OF MDPS WITH COMPLEX Q
Recall that we consider the infinite horizon case and 0 < γ < 1 is the discount factor. Let H =
(1 - γ)-1 be the “effective horizon” — the rewards after H steps become negligible due to the
discount factor. For simplicity, we assume that H > 3 and it is an integer. (Otherwise we take just
take H = b(1 - γ)-1c.) Throughout this section, we assume that the state space S = [0, 1) and the
action space A = {0, 1}.
Definition 4.1. Given the effective horizon H = (1 - γ)-1, we define an MDP MH as follows. Let
κ = 2-H. The dynamics f by the following piecewise linear functions with at most three pieces.
2 ς if ς < 1/2	2s + K	if s < (I - K)/2
f(s, 0)=	2S1 if S < 1/2	f(s, 1)=	2s + κ - 1 if (1 - κ)∕2 ≤ S ≤ (2-κ)∕2
2s-	1	ifs ≥ 1/2	2s+K-2	otherwise.
The reward function is defined as
r(S, 0) = I[1/2 ≤ S < 1]
r(S, 1) = I[1/2 ≤ S < 1] - 2(γH-1 - γH)
The initial state distribution μ is uniform distribution over the state space [0,1).
The dynamics and the reward function for H = 4 are visualized in Figures 2a, 2b. Note that by the
definition, the transition function for a fixed action a is a piecewise linear function with at most 3
pieces. Our construction can be modified so that the dynamics is Lipschitz and the same conclusion
holds (see Appendix C).
Attentive readers may also realize that the dynamics can be also be written succinctly as f(S, 0) =
2S mod 1 and f(S, 1) = 2S + K mod 12, which are key properties that we use in the proof of
Theorem 4.2 below.
Optimal Q-function Q? and the optimal policy π?. Even though the dynamics of the MDP
constructed in Definition 4.1 has only a constant number of pieces, the Q-function and policy are
very complex: (1) the policy is a piecewise linear function with exponentially number of pieces,
(2) the optimal Q-function Q? and the optimal value function V ? are actually fractals that are not
continuous anywhere. These are formalized in the theorem below.
Theorem 4.2. For S ∈ [0, 1), let S(k) denotes the k-th bit of S in the binary representation.3 The
optimal policy π? for the MDP defined in Definition 4.1 has 2H+1 number of pieces. In particular,
π 夫(S) = I[s(H+1) =0].	(2)
2The mod function is defined as: x mod 1 , x - bxc. More generally, for positive real k, we define x
mod k , x - kbx/kc.
3Or more precisely, we define s(h) , b2hsc mod 2.
5
Under review as a conference paper at ICLR 2020
And the optimal value function is a fractal with the expression:
H∞
V ?(s) = X γh-1s(h) + X γh-1 1 + 2(s(h+1) - s(h)) + γH-1 2s(H+1) - 2 .	(3)
h=1	h=H +1
The close-form expression of Q? can be computed by Q?(s, a) = r(s, a) + V ?(f(s, a)), which is
also a fractal.
We approximate the optimal Q-function by truncating the infinite sum to 2H terms, and visualize it
in Figure 2c. We discuss the main intuitions behind the construction in the following proof sketch
of the Theorem. A rigorous proof of Theorem 4.2) is deferred to Appendix B.1.
Proof Sketch. The key observation is that the dynamics f essentially shift the binary representation
of the states with some addition. We can verify that the dynamics satisfies f (s, 0) = 2s mod 1 and
f (s, 1) = 2s + K mod 1 where K = 2-H. In other words, suppose S = 0.s(1)s(2) •…is the binary
representation of s, and let left-shift(s) = 0.s(2)s⑶∙∙∙.
f(s,0) = left-shift(s)	(4)
f(s, 1) = (left-shift(s) + 2-H) mod 1	(5)
Moreover, the reward function is approximately equal to the first bit of the binary representation
r(s, 0) = s(1), r(s, a) ≈ s(1)	(6)
(Here the small negative drift of reward for action a = 1, -2(γH-1 - γH), is only mostly designed
for the convenience of the proof, and casual readers can ignore it for simplicity.) Ignoring carries,
the policy pretty much can only affect the H-th bit of the next state s0 = f(s, a): the H-th bit of s0
is either equal to (H + 1)-th bit of s when action is 0, or equal its flip when action is 1. Because
the bits will eventually be shifted left and the reward is higher if the first bit of a future state is
1, towards getting higher future reward, the policy should aim to create more 1’s. Therefore, the
optimal policy should choose action 0 if the (H + 1)-th bit of s is already 1, and otherwise choose
to flip the (H + 1)-th bit by taking action 1.
A more delicate calculation that addresses the carries properly would lead us to the form of the
optimal policy (Equation (2).) Computing the total reward by executing the optimal policy will lead
us to the form of the optimal value function (equation (3).) (This step does require some elementary
but sophisticated algebraic manipulation.)
With the form of the V ?, a shortcut to a formal, rigorous proof would be to verify that it satisfies the
Bellman equation, and verify π? is consistent with it. We follow this route in the formal proof of
Theorem 4.2) in Appendix B.1.	口
4.2	THE APPROXIMABILITY OF Q-FUNCTION
A priori, the complexity of Q? or π? does not rule out the possibility that there exists an approxi-
mation of them that do an equally good job in terms of maximizing the rewards. However, we show
that in this section, indeed, there is no neural network approximation ofQ? or π? with a polynomial
width. We prove this by showing any piecewise linear function with a sub-exponential number of
pieces cannot approximate either Q? or π? with a near-optimal total reward.
Theorem 4.3. Let MH be the MDP constructed in Definition 4.1. Suppose a piecewise linear
policy π has a near optimal reward in the sense that η(π) ≥ 0.92 ∙ η(π?), then it has to have at
least Ω (exp(cH )/H) pieces for some universal ConStant c > 0. As a corollary, no constant depth
neural networks with polynomial width (in H) can approximate the optimal policy with near optimal
rewards.
Consider a policy π induced by a value function Q, that is, π(s) = arg maxa∈A Q(s, a). Then,when
there are two actions, the number of pieces of the policy is bounded by twice the number of pieces
ofQ. This observation and the theorem above implies the following inapproximability result of Q?.
Corollary 4.4. In the setting of Theorem 4.3, let π be the policy induced by some Q. If π is near-
optimal in a sense that η(π) ≥ 0.92 ∙ η(π?), then Q has at least Ω (exp(cH )/H) pieces for some
universal constant c > 0.
6
Under review as a conference paper at ICLR 2020
The intuition behind the proof of Theorem 4.3 is as follows. Recall that the optimal policy has the
form ∏?(S) = I[s(H+1) = 0]. One can expect that any polynomial-pieces policy ∏ behaves SUboPti-
mally in most of the states, which leads to the suboptimality ofπ. Detailed proof of Theorem 4.3 is
deferred to Appendix B.2.
Beyond the expressivity lower boUnd, we also provide an exponential sample complexity lower
boUnd for Q-learning algorithms parameterized with neUral networks (see Appendix B.4).
4.3 THE APPROXIMABILITY OF Q-FUNCTIONS OF RANDOMLY GENERATED MDPS
In this section, we show the phenomena that the Q-fUnction not only occUrs in the crafted cases as
in the previoUs sUbsection, bUt also occUrs more robUstly with a decent chance for (semi-) randomly
generated MDPs. (Mathematically, this says that the family of MDPs with sUch a property is not a
degenerate measUre-zero set.)
It is challenging and perhaps reqUires deep math to characterize the fractal strUctUre of Q-fUnctions
for random dynamics, which is beyond the scope of this paper. Instead, we take an empirical ap-
proach here. We generate random piecewise linear and Lipschitz dynamics, and compUte their
Q-fUnctions for the finite horizon, and then visUalize the Q-fUnctions or coUnt the nUmber of pieces
in the Q-fUnctions. We also Use DQN algorithm (Mnih et al., 2015) with a finite-size neUral network
to learn the Q-fUnction.
We set horizon H = 10 for simplicity and compUtational feasibility. The state and action space
are [0, 1) and {0, 1} respectively. We design two methods to generate random or semi-random
piecewise dynamics with at most foUr pieces. First, we have a Uniformly random method, called
RAND, where we independently generate two piecewise linear fUnctions for f(s, 0) and f(s, 1), by
generating random positions for the kinks, generating random oUtpUts for the kinks, and connecting
the kinks by linear lines (See Appendix D.1 for a detailed description.)
In the second method, called SEMI-RAND, we introdUce a bit more strUctUre in the generation
process, towards increasing the chance to see the phenomenon. The fUnctions f(s, 0) and f(s, 1)
have 3 pieces with shared kinks. We also design the generating process of the oUtpUts at the kinks
so that the fUnctions have more flUctUations. The reward for both of the two methods is r(s, a) =
s, ∀a ∈ A. (See Appendix D.1 for a detailed description.)
FigUre 1 illUstrates the dynamics of the generated MDPs from SEMI-RAND. More details of empir-
ical settings can be foUnd in Appendix D.1. The optimal policy and Q can have a large number
of pieces. BecaUse the state space has one dimension, and the horizon is 10, we can compUte the
exact Q-fUnctions by recUrsively applying Bellman operators, and coUnt the nUmber of pieces. We
foUnd that, 8.6% fraction of the 1000 MDPs independently generated from the RAND method has
policies with more than 100 pieces, mUch larger than the nUmber of pieces in the dynamics (which is
4). Using the SEMI-RAND method, a 68.7% fraction of the MDPs has polices with more than 103
pieces. In Section D.1, we plot the histogram of the nUmber of pieces of the Q-fUnctions. FigUre 1
visUalize the Q-fUnctions and dynamics of two MDPs generated from RAND and SEMI-RAND
method. These resUlts sUggest that the phenomenon that Q-fUnction is more complex than dynamics
is not a degenerate phenomenon and can occUr with non-zero measUre. For more empirical resUlts,
see Appendix D.2.
Model-based policy optimization methods also suffer from a lack of expressivity. As an impli-
cation of oUr theory in the previoUs section, when the Q-fUnction or the policy are too complex to
be approximated by a reasonable size neUral network, both model-free algorithms or model-based
policy optimization algorithms will sUffer from the lack of expressivity, and as a conseqUence, the
sUb-optimal rewards. We verify this claim on the randomly generated MDPs discUssed in Sec-
tion 4.3, by rUnning DQN (Mnih et al., 2015), SLBO (LUo et al., 2019), and MBPO (Janner et al.,
2019) with varioUs architectUre size.
For the ease of exposition, we Use the MDP visUalized in the bottom half of FigUre 1. The optimal
policy for this specific MDP has 765 pieces, and the optimal Q-fUnction has aboUt 4 × 104 nUmber
of pieces, and we can compUte the optimal total rewards.
First, we apply DQN to this environment by Using a two-layer neUral network with varioUs widths to
parameterize the Q-fUnction. The training cUrve is shown in FigUre 3 (Left). Model-free algorithms
7
Under review as a conference paper at ICLR 2020
trajectories
Figure 3: (Left): The performance of DQN, SLBO, and MBPO on the bottom dynamics in Figure 1.
The number after the acronym is the width of the neural network used in the parameterization of Q.
We see that even with sufficiently large neural networks and sufficiently many steps, these algorithms
still suffers from bad approximability and cannot achieve optimal reward. (Right): Performance of
BOOTS + DQN with various planning steps. A near-optimal reward is achieved with even k = 3,
indicating that the bootstrapping with the learned dynamics improves the expressivity of the policy
significantly.
optimal
——DQN-32
—DQN-128
——DQN-512
—DQN-16384
SLBO-512
MBPO-512
Algorithm 1 Model-based Bootstrapping Planner (BOOTS) + RL Algorithm X
1:	training: run Algorithm X, store the all samples in the set R, store the learned Q-function Q,
and the learned dynamics f if it is available in Algorithm X.
2:	testing:
3:	if f is not available, learn f from the data in R
4:	execute the policy BOOTS(s) at every state s
5:
1:	function BOOTS(s)
2:	Given: query oracle for function Q and f
3:	Compute
∏bo0t* s^(s) = argmax max r(s,a)+-------+ YkTr(Sk-ι,ak-i)+ YkQ(sk,ak)
k,Q,f	a	a1 ,...,ak
using a zero-th order optimization algorithm (which only requires oracle query of the function
value) such as cross-entropy method or random shooting.
can not find near-optimal policy even with 214 hidden neurons and 1M trajectories, which suggests
that there is a fundamental approximation issue. This result is consistent with Fu et al. (2019), in a
sense that enlarging Q-network improves the performance of DQN algorithm at convergence.
Second, we apply SLBO and MBPO in the same environment. Because the policy network and
Q-function in SLOBO and MBPO cannot approximate the optimal policy and value function, we
see that they fail to achieve near-optimal rewards, as shown in Figure 3 (Left).
5 Model-based Bootstrapping Planner
Our theory and experiments in Section 4.2 and 4.3 demonstrate that when the Q-function or the
policy is complex, model-free or model-based policy optimization algorithms will suffer from the
lack of expressivity. The intuition suggests that model-based planning algorithms will not suffer
from the lack of expressivity because the final policy is not represented by a neural network. For the
construction in Section 4.1, we can actually prove that even a few-steps planner can bootstrap the
expressivity of the Q-function (formalized in Theorem 5.1 below).
Inspired the theoretical result, we apply a simple k-step model-based bootstrapping planner on top
of existing Q-functions (trained from either model-based or model-free approach) in the test time,
on either the one-dimensional MDPs considered in Section 4 or the continuous control benchmark
tasks in MuJoCo. The bootstrapping planner is reminiscent of MCTS using in AlphaGo (Silver
et al., 2016; 2018). However, here, we use the learned dynamics and deal with the continuous state
space.
8
Under review as a conference paper at ICLR 2020
Ant-v2
4000
3000
2000
1000
0
e33h 36eJ3><
Figure 4: Comparison of BOOTS-MBSAC vs MBSAC and BOOTS-SAC vs SAC on Ant and Hu-
manoid. Particularly on the Humanoid environment, BOOTS improves the performance signifi-
cantly. The test policies for MBSAC and SAC are the deterministic policy that takes the mean of the
output of the policy network, because the deterministic policy performs better than the stochastic
policy in the test time.
5.1	B OOTSTRAPPING THE Q-FUNCTION
Given a function Q that is potentially not expressive enough to approximate the optimal Q-function,
we can apply the Bellman operator with a learned dynamics f for k times to get a bootstrapped
version of Q:
Bf^[Q] = Bf^[∙…[Bf^[Q]]],	⑺
'-------V------}
k times
or	B^[Q](s,a) = max r(s0,a0) +---------+ Yk-1 r(sk-i,ak-i) + YkQ(Sk,a，k)	(8)
J	aι,…,ak
where s0 = s, a0 = a and sh+1 = f (sh , ah ).
Given the bootstrapped version, we can derive a greedy policy w.r.t it:
城等 ^(s)=max Bki∖Q](s,a)	(9)
k,Q,f	a f
Algorithm 1, called BOOTS summarizes how to apply the planner on top of any RL algorithm with
a Q-function (straightforwardly).
For the MDPs constructed in Section 4.1, we can prove that representing the optimal Q-function by
Bk^[Q] requires fewer pieces in Q than representing the optimal Q-function by Q directly.
Theorem 5.1. Consider the MDP MH defined in Definition 4.1. There exists a constant-piece piece-
wise linear dynamics f and a 2H-k+1 -piecepiecewise Iinearfunction Q, such that the bootstrapped
policy ∏boots^(s) achieves the optimal total rewards.
By contrast, recall that in Theorem 4.3, we show that approximating the optimal Q-function directly
with a piecewise linear function requires ≈ 2H piecewise. Thus we have a multiplicative factor
of 2k gain in the expressivity by using the bootstrapped policy. Here the exponential gain is only
magnificent enough when k is close to H because the gap of approximability is huge. However,
in more realistic settings — the randomly-generated MDPs and the MuJoCo environment — the
bootstrapping planner improvs the performance significantly as shown in the next subsection.
5.2	Experiments
BOOTS on random piecewise linear MDPs. We implement BOOTS (Algorithm 1) with various
steps of planning and with the learned dynamics.4 . The planner is an exponential-time planner
which enumerates all the possible future sequence of actions. We also implement bootstrapping
with partial planner with varying planning horizon. As shown in Figure 3, BOOTS + DQN not
only has the best sample-efficiency, but also achieves the optimal reward. In the meantime, even a
partial planner helps to improve both the sample-efficiency and performance. More details of this
experiment are deferred to Appendix D.3.
4Our code is available at https://github.com/roosephu/boots.
9
Under review as a conference paper at ICLR 2020
Figure 5: BOOTS-MBSAC or BOOTS-
MBPO outperforms previous state-of-the-art
algorithms on Humanoid. The results are av-
eraged over 5 random seeds and shadow area
indicates a single standard deviation from the
mean.
BOOTS on MuJoCo environments. We work with the OpenAI Gym environments (Brockman
et al., 2016) based on the Mujoco simulator (Todorov et al., 2012) with maximum horizon 1000 and
discount factor 1. We apply BOOTS on top of three algorithms: (a) SAC (Haarnoja et al., 2018),
the state-of-the-art model-free RL algorithm; (b) MBPO (Janner et al., 2019), a model-based Q-
learning algorithm, and an extension of Dyna (Sutton, 1990); (c) a computationally efficient variant
of MBPO that we develop using ideas from SLBO (Luo et al., 2019), which is called MBSAC. The
main difference here from MBPO and other works such as (Wang & Ba, 2019; Kurutach et al., 2018)
is that we don’t use model ensemble. Instead, we occasionally optimize the dynamics by one step
of Adam to introduce stochasticity in the dynamics, following the technique in SLBO Luo et al.
(2019). Our algorithm is a few times faster than MBPO in wall-clock time. It performs similarly to
MBPO on Humanoid, but generally worse than MBPO on other environments. See Appendix A.1
for details.
We use k = 4 steps of planning unless explicitly mentioned otherwise in the ablation study (Sec-
tion A.2). In Figure 4, we compare BOOTS+SAC with SAC, and BOOTS + MBSAC with MBSAC
on Gym Ant and Humanoid environments, and demonstrate that BOOTS can be used on top of ex-
isting strong baselines. We found that BOOTS has little help for other simpler environments, and
we suspect that those environments have much less complex Q-functions so that our theory and
intuitions do not necessarily apply. (See Section A.2 for more ablation study.)
In Figure 5, we compare BOOTS+MBSAC and BOOTS+MBPO with other MBPO, SAC, and
STEVE (Buckman et al., 2018)5 on the humanoid environment. We see a strong performance sur-
passing the previous state-of-the-art MBPO.
6 Conclusion
Our study suggests that there exists a significant representation power gap of neural networks be-
tween for expressing Q-function, the policy, and the dynamics in both constructed examples and em-
pirical benchmarking environments. We show that our model-based bootstrapping planner BOOTS
helps to overcome the approximation issue and improves the performance in synthetic settings and
in the difficult MuJoCo environments. We raise some interesting open questions.
•	Can we theoretically generalize our results to high-dimensional state space, or continuous
actions space? Can we theoretically analyze the number of pieces of the optimal Q-function
of a stochastic dynamics?
•	In this paper, we measure the complexity by the size of the neural networks. It’s conceivable
that for real-life problems, the complexity of a neural network can be better measured by its
weights norm. Could we build a more realistic theory with another measure of complexity?
•	The BOOTS planner comes with a cost of longer test time. How do we efficiently plan in
high-dimensional dynamics with a long planning horizon?
•	The dynamics can also be more complex (perhaps in another sense) than the Q-function
in certain cases. How do we efficiently identify the complexity of the optimal Q-function,
policy, and the dynamics, and how do we deploy the best algorithms for problems with
different characteristics?
5For STEVE, we use the official code at https://github.com/tensorflow/models/tree/
master/research/steve
10
Under review as a conference paper at ICLR 2020
References
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. In Proceedings of the 34th International Conference on Machine Learning-Volume
70,pp. 263-272. JMLR. org, 2017.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-
efficient reinforcement learning with stochastic ensemble value expansion. In Advances in Neural
Information Processing Systems, pp. 8224-8234, 2018.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learn-
ing in a handful of trials using probabilistic dynamics models. In Advances in Neural Information
Processing Systems, pp. 4754-4765, 2018.
Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel.
Model-based reinforcement learning via meta-policy optimization. In Conference on Robot
Learning, pp. 617-629, 2018.
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the sample
complexity of the linear quadratic regulator. CoRR, abs/1710.01688, 2017. URL http:
//arxiv.org/abs/1710.01688.
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. Regret bounds for robust
adaptive control of the linear quadratic regulator. In Advances in Neural Information Processing
Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
3-8 December 2018, Montreal, Canada., pp. 4192T201, 2018.
Yilun Du and Karthik Narasimhan. Task-agnostic dynamics priors for deep reinforcement learning.
arXiv preprint arXiv:1905.04819, 2019.
V Feinberg, A Wan, I Stoica, MI Jordan, JE Gonzalez, and S Levine. Model-based value expan-
sion for efficient model-free reinforcement learning. In Proceedings of the 35th International
Conference on Machine Learning (ICML 2018), 2018.
Justin Fu, Aviral Kumar, Matthew Soh, and Sergey Levine. Diagnosing bottlenecks in deep q-
learning algorithms. In International Conference on Machine Learning, pp. 2021-2030, 2019.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning, pp. 1856-1865, 2018.
Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa. Learn-
ing continuous control policies by stochastic value gradients. In Advances in Neural Information
Processing Systems, pp. 2944-2952, 2015.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. ArXiv, abs/1906.08253, 2019.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably effi-
cient? In Advances in Neural Information Processing Systems, pp. 4863-4873, 2018.
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H. Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Ryan Sepassi,
George Tucker, and Henryk Michalewski. Model-based reinforcement learning for atari. ArXiv,
abs/1903.00374, 2019.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning.
In Proceedings of the Nineteenth International Conference on Machine Learning, pp. 267-274.
Morgan Kaufmann Publishers Inc., 2002.
11
Under review as a conference paper at ICLR 2020
Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble
trust-region policy optimization. arXiv preprint arXiv:1802.10592, 2018.
Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on Machine
Learning, pp. 1-9, 2013.
Kendall Lowrey, Aravind Rajeswaran, Sham M. Kakade, Emanuel Todorov, and Igor Mordatch.
Plan online, learn offline: Efficient learning and exploration via model-based control. ArXiv,
abs/1811.01848, 2018.
Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic
framework for model-based deep reinforcement learning with theoretical guarantees. In 7th In-
ternational Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019, 2019.
Ali Malik, Volodymyr Kuleshov, Jiaming Song, Danny Nemer, Harlan Seymour, and Stefano Er-
mon. Calibrated model-based deep reinforcement learning. arXiv preprint arXiv:1906.08312,
2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518:529-533, 2015.
Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics
for model-based deep reinforcement learning with model-free fine-tuning. In 2018 IEEE Interna-
tional Conference on Robotics and Automation (ICRA), pp. 7559-7566. IEEE, 2018.
Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In Advances in Neural
Information Processing Systems, pp. 6118-6128, 2017.
Razvan Pascanu, Guido F Montufar, and Yoshua Bengio. On the number of inference regions of
deep feed forward networks with piece-wise linear activations. 2013.
Alexandre Piche, Valentin Thomas, Cyril Ibrahim, Yoshua Bengio, and Chris Pal. Probabilistic
planning with sequential monte carlo methods. 2018.
SebaStien Racaniere, Theophane Weber, David Reichert, Lars Buesing, Arthur Guez,
Danilo Jimenez Rezende, Adria PUigdomeneCh Badia, Oriol Vinyals, Nicolas Heess, Yujia Li,
et al. Imagination-augmented agents for deep reinforcement learning. In Advances in neural
information processing systems, pp. 5690-5701, 2017.
Aravind Rajeswaran, Sarvjeet Ghotra, Balaraman Ravindran, and Sergey Levine. Epopt: Learning
robust neural network policies using model ensembles. arXiv preprint arXiv:1610.01283, 2016.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game
of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel
Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, et al. The predictron: End-to-
end learning and planning. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pp. 3191-3199. JMLR. org, 2017.
12
Under review as a conference paper at ICLR 2020
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Si-
monyan, and Demis Hassabis. A general reinforcement learning algorithm that masters chess,
shogi, and go through self-play. Science, 362(6419):1140-1144,2018. ISSN 0036-8075. doi:
10.1126/science.aar6404.
Alexander L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L Littman. Pac model-
free reinforcement learning. In Proceedings of the 23rd international conference on Machine
learning, pp. 881-888. ACM, 2006.
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
rl in contextual decision processes: Pac bounds and exponential improvements over model-free
approaches. In Conference on Learning Theory, pp. 2898-2933, 2019.
Richard S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. SIGART
Bulletin, 2:160-163, 1990.
Erik Talvitie. Model regularization for stable sample rollouts. In UAI, pp. 780-789, 2014.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Stephen Tu and Benjamin Recht. The gap between model-based and model-free methods on the
linear quadratic regulator: An asymptotic viewpoint. arXiv preprint arXiv:1812.03565, 2018.
Tingwu Wang and Jimmy Ba. Exploring model-based planning with policy networks. arXiv preprint
arXiv:1906.08649, 2019.
Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement
learning without domain knowledge using value function bounds. In International Conference on
Machine Learning, pp. 7304-7312, 2019.
Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without
normalization. arXiv preprint arXiv:1901.09321, 2019.
13
Under review as a conference paper at ICLR 2020
A Experiment Details in Section 5.2
A.1 MODEL-BASED SAC (MBSAC)
Here we describe our MBSAC algorithm in Algorithm 2, which is a model-based policy optimization
and is used in BOOTS-MBSAC. As mentioned in Section 5.2, the main difference from MBPO and
other works such as (Wang & Ba, 2019; Kurutach et al., 2018) is that we don’t use model ensemble.
Instead, we occasionally optimize the dynamics by one step of Adam to introduce stochasticity in
the dynamics, following the technique in SLBO (Luo et al., 2019). As argued in (Luo et al., 2019),
the stochasticity in the dynamics can play a similar role as the model ensemble. Our algorithm is
a few times faster than MBPO in wall-clock time. It performs similarlty to MBPO on Humanoid,
but a bit worse than MBPO in other environments. In MBSAC, we use SAC to optimize the policy
∏β and the Q-function Qφ. We choose SAC due to its SamPle-efficiency, simplicity and off-policy
nature. We mix the real data from the environment and the virtual data which are always fresh and
are generated by our learned dynamics model fθ .6
Algorithm 2 MBSAC
1:	Parameterize the policy ∏β, dynamics fθ, and the Q-function QW by neural networks. Initialize
replay buffer B with ninit steps of interactions with the environments by a random policy, and
pretrain the dynamics on the data in the replay buffer.
2:	t J 0, and sample so from the initial state distribution.
3:	for niter iterations do
4:	Perform action at ~ ∏β(∙∣st) in the environment, obtain s0 as the next state from the envi-
ronment.
5:	st+1 J s0, and add the transition (st, at, st+1, rt) to B.
6:	t J t + 1. If t = T or the trajectory is done, reset to t = 0 and sample s0 from the initial
state distribution.
7:	for npolicy iterations do
8:	for nmodel iterations do
9:	Optimize fθ with a mini-batch of data from B by one step of Adam.
10:	Sample nreal data Breal and nstart data Bstart from B.
11:	Perform q steps of virtual rollouts using fθ and policy ∏β starting from states in Bstat;
obtain Bvirtual.
12:	Update πβ and QW using the mini-batch of data in Breal ∪ Bvirtual by SAC.
For Ant, we modify the environment by adding the x and y axis to the observation space to make it
possible to compute the reward from observations and actions. For Humanoid, we add the position
of center of mass. We don’t have any other modifications. All environments have maximum horizon
1000.
For the policy network, we use an MLP with ReLU activation function and two hidden layers, each
of which contains 256 hidden units. For the dynamics model, we use a network with 2 Fixup blocks
(Zhang et al., 2019), with convolution layers replaced by a fully connected layer. We found out
that with similar number of parameters, fixup blocks leads to a more accurate model in terms of
validation loss. Each fixup block has 500 hidden units. We follow the model training algorithm in
Luo et al. (2019) in which non-squared `2 loss is used instead of the standard MSE loss.
A.2 Ablation Study
Planning with oracle dynamics and more environments. We found that BOOTS has smaller
improvements on top of MBSAC and SAC for the environment Cheetah and Walker. To diagnose
the issue, we also plan with an oracle dynamics (the true dynamics). This tells us whether the lack
of improvement comes from inaccurate learned dynamics. The results are presented in two ways
in Figure 6 and Figure 7. In Figure 6, we plot the mean rewards and the standard deviation of
various methods across the randomness of multiple seeds. However, the randomness from the seeds
6In the paper of MBPO (Janner et al., 2019), the authors don’t explicitly state their usage of real data in
SAC; the released code seems to make such use of real data, though.
14
Under review as a conference paper at ICLR 2020
UJnsH KBV><
O IOOK 200K	300K	400K
# steps
Walker
■WOK
# steps
■ ♦ ♦ ɪ ɪ
EmaU∙6eaΛV
Ant
Humanold
IOOK 200K	300K
# steps
Act
IMK 200K	3∞K
# steps
J , ɪ
Walker
5∞0
ε «00
"3000-
62000
* IMO


Figure 6: BOOTS with oracle dynamics on top of SAC (top) and MBSAC (bottom) on HalfCheetah,
Walker, Ant and Humanoid. The solid lines are average over 5 runs, and the shadow areas indicate
the standard deviation.
2000
IOOO
u≈9 vπBV><
——BOOTS + SAC with Qmde dynamics
—det-pollcy
——BOOTS + SAC with learned dynamics
HaIfCheetah
Walker
O IOOK	200K	300K	400 K
# steps
ToOK 200K	300K	«0K
# steps
U≈lsV8vv>d
u≈9 vπBV><
HaIfCheetah
BOOTS + MBSAC with OBde dynamics
det-pollcy
BOOTS÷ MBSAC with learned dynamics
Figure 7: The relative gains of BOOTS over SAC (top) and MBSAC (bottom) on HalfCheetah,
Walker, Ant and Humanoid. The solid lines are average over 5 runs, and the shadow areas indicate
the standard deviation.
20W
3000
,2000
IOOO
O
O
IMK	200K	300K	WOK
# steps
somewhat obscures the gains of BOOTS on each individual run. Therefore, for completeness, we
also plot the relative gain of BOOTS on top of MBSAC and SAC, and the standard deviation of the
gains in Figure 7.
From Figure 7 we can see planning with the oracle dynamics improves the performance in most of
the cases (but with various amount of improvements). However, the learned dynamics sometimes
not always can give an improvement similar to the oracle dynamics. This suggests the learned
dynamics is not perfect, but oftentimes can lead to good planning. This suggests the expressivity of
the Q-functions varies depending on the particular environment. How and when to learn and use a
learned dynamics for planning is a very interesting future open question.
Figure 8: Different BOOTS planning horizon k on top of SAC (left) and MBSAC (right) on Hu-
manoid. The solid lines are average over 5 runs, and the shadow areas indicate the standard devia-
tion.
15
Under review as a conference paper at ICLR 2020
The effect of planning horizon. We experimented with different planning horizons in Figure 8.
By planning with a longer horizon, we can earn slightly higher total rewards for both MBSAC and
SAC. Planning horizon k = 16, however, does not work well. We suspect that it’s caused by the
compounding effect of the errors in the dynamics.
B Omitted Proofs in Section 4
In this section we provide the proofs omitted in Section 4.
B.1 Proof of Theorem 4.2
Proof of Theorem 4.2. Since the solution to Bellman optimal equations is unique, we only need to
verify that V ? and π? defined in equation (1) satisfy the following,
V?(s) = r(s, ∏*(s)) + γV*(f (s, n?(s))),	(10)
V?(s) ≥ r(s, a) + YV?(f (s, a)),	∀a = ∏*(s).	(11)
Recall that s(i) is the i-th bit in the binary representation of s, that is, s(i) = b2i sc mod 2. Let
S = f (s, n?(s)). Since ∏?(S) = I[s(H+1) = 0], which ensures the H-bit of the next state is 1, We
have
s(i+1), i 6= H,
1, i = H.
(12)
For simplicity, define ε = 2(γH-1 - γH). The definition of r(s, a) implies that
r(s,π?(S)) = I[1∕2 ≤ s < 1] — I[π*(s) = 1]ε = S(I) —(1 — S(H+1)) ε.
By elementary manipulation, Eq. (3) is equivalent to
H∞
V ?(S) = X γi-1S(i) + X	γi-1-2(γi-2-γi-1) 1-S(i)	,	(13)
i=1	i=H+1
Now, we verify Eq. (10) by plugging in the proposed solution (namely, Eq. (13)). As a result,
r(s,π?(S)) + γV*(S)
H∞
S(I)-(1-S(H+1)) ε + γ X YiTI[^⑴=1] + γ X (YiT-(1 - S(i)) 2(γi-2 - γi-1
i=1	i=H+1
H∞
S(1) - 1 - S(H+1) ε+XYi-1S(i) +YH+ X	Yi-1 - 1 - S(i) 2(Yi-2 -Yi-1)
i=2	i=H+2
H∞
XYi-1S(i)+ X	Yi-1 - 1 - S(i) 2(Yi-2 - Yi-1)
i=1	i=H+1
V?(S),
which verifies Eq. (10).
16
Under review as a conference paper at ICLR 2020
In the following We verify Eq. (11). Consider any a = π*(s). Let S = f (s, a) for shorthand. Note
that S(i) = s(i+1) for i > H. As a result,
V?(s) - YV?(S)
H∞
= Xγi-1s(i)+ X γi-1 - 1 - s(i) 2(γi-2 - γi-1)
i=1	i=H+1
H∞
-Xγi-1sS(i) - X	γi-1 - 1 - sS(i) 2(γi-2 - γi-1)
i=1	i=H+1
H-1
=S(I) + X γi (s(i+1) - S⑺)-YHS(H) + γH - 2(1 -S(H+I))(YHT-YH)
i=1
For the case where S(H+1) = 0, we have π?(S) = 1. For a = 0, S(i) = s(i+1) for all i ≥ 1.
Consequently,
V?(S) -YV?(SS) = S(1) +YH -ε > S(1) = r(S, 0),
where the last inequality holds when YH - ε > 0, or equivalently, Y > 2/3.
For the case where S(H+1)= 1, We have ∏?(S) = 0. For a = 1, we have S(H+1)= 1 and S(H) = 0.
Let p = max{i ≤ H : S(i) = 0}, where we define the max of an empty set is 0. The dynamics
f(S, 1) implies that
(S(i+1), i + 1 < p or i > H,
SS(i) =	1, i+1 =p,
[0,	p < i + 1 ≤ H + 1.
Therefore,
H-1
V?(S) - YV?(SS) = S(1) + YH + X Yi S(i+1) - SS(i) > S(1) - ε = r(S, 1).
i=1
In both cases, we have V? - YV?(S)> r(S, a) for a = π*(s), which proves Eq. (11).	□
B.2 Proof of Theorem 4.3
For a fixed parameter H, let z(π) be the number of pieces in π. For a policy π, define the state
distribution when acting policy π at step h as μ∏.
In order to prove Theorem 4.3, we show that if 1/2 - 2Hz(π)∕2H < 0.3, then η(π) < 0.92η(π?).
The proof is based on the advantage decomposition lemma.
Lemma B.1 (Advantage Decomposition Lemma (Schulman et al., 2015; Kakade & Langford,
2002)). Define Aπ (S, a) = r(S, a) + YVπ (f (S, a)) - Vπ(S) = Qπ (S, a) - Vπ (S). Given poli-
cies π and π, we have
∞
η(π) = η(π) + X Yh-1 Es〜“n [Aπ(s,π(s))].	(14)
h=1
Corollary B.2. For any policy π, we have
∞
η(π?) - η(π) = X Yh-1Es〜“n [V*(S)- Q*(s, π(s))].	(15)
h=1
Intuitively speaking, since π* = I[S(H+1) = 0], the a policy π with polynomial pieces behaves
suboptimally in most of the states. Lemma B.3 shows that the single-step suboptimality gap V*(S) -
Q*(S, π(S)) is large for a constant portion of the states. On the other hand, Lemma B.4 proves that
17
Under review as a conference paper at ICLR 2020
the state distribution μ∏ is near uniform, which means that SUboPtimal states can not be avoided.
Combining with Corollary B.2, the suboptimal gap of policy π is large.
The next lemma shows that, if π does not change its action for states from a certain interval, the
average advantage term V ?(s) - Q?(s, π(s)) in this interval is large. Proof of this lemma is deferred
of Section B.3.
Lemma B.3. Let `k = [k/2H, (k + 1)/2H), and K = {0 ≤ k < 2H : k mod 2 = 1}. Then for
k ∈ K, ifpolicy π does not change its action at interval 'k (that is, ∣{∏(s) : S ∈ 'k}| = 1), we have
ɪ I	(V*(s)- Q*(s,∏(s))) ds ≥ 0.183	(16)
|'k| JsEkk
for H > 500.
Next lemma shows that when the number of pieces in ∏ is not too large, the distribution μ∏ is close
to uniform distribution for steP 1 ≤ h ≤ H. Proof of this lemma is deferred of Section B.3
Lemma B.4. Let z(π) be the number of pieces of policy π. For k ∈ [2H], define interval
'k = [k/2H, (k + 1)/2H). Let ν%(k) = inf§三八 μ∏(s), If the initial state distribution μ is Uni-
form distribution, then for any h ≥ 1,
X 2-H ∙ νh(k) ≥ 1 - 2h-2H-.	(17)
0≤k<2H
Now we present the proof for Theorem 4.3.
Proof of Theorem 4.3. For any k ∈ [2H], consider the interval `k = [k/2H, (k + 1)/2H). Let K =
{k ∈ [AH] : k mod 2 = 1}. If π does not change at interval 'k (that is, ∣{π(s) : S ∈ 'k}| = 1), by
Lemma B.3 we have
/	(V?(s) - Q*(s,π(s))) ds ≥ 0.183 ∙ 2-H
J s∈'k
(18)
Let νh(k) = inf§*热 μh(s), then by advantage decomposition lemma (namely, Corollary B.2), We
have
η(π?) - η(π)
∞
Xγ
h=1
10H
Xγ
h=1
10H
Xγ
h=1
10H
Xγ
h=1
…「V *(S)--(S))) dμπ (S)
h-1
h-1
h-1
(V*(s)- Q*(s,∏(s))) dμ∏(s)
Vh(k)(V*(s)- Q*(s,∏(s))) ds
0.183 ∙ 2-h ∙νh(k).
≥
≥
≥
By Lemma B.4 and union bound, we get
X 2-h ∙Vh(k) ≥ 1 -
(19)
k∈K
For the sake of contradiction, we assume z(π) = o (exp(cH)/H), then for large enough H we have,
20Hz(π)
1/2 ——2≠2 ≥ 0.49,
which means that Pk∈κ 2-H ∙ ν%(k) ≥ 0.49 for all h ≤ 10H. Consequently, for H > 500, We have
η(π?) - η(π) ≥ X(0.183 × 0.49)γh-1
h=1
≥ 0.089 ∙
1 - γ10H
1 - Y
0.088
≥ ----
—1 - Y
Now, since η(π?) ≤ 1/(1 - γ), we have η(π) < 0.92η(π?). Therefore for near-optimal policy π,
z(π) = Ω (exp(cH)/H).	□
18
Under review as a conference paper at ICLR 2020
B.3 Proofs of Lemma B.3 and Lemma B.4
In this section, we present the proofs of two lemmas used in Section B.1
Proof of Lemma B.3. Note that for any k ∈ K, s(H) = 1, ∀s ∈ `k. Now fix a parameter k ∈ K.
Suppose π(s) = ai for s ∈ `k. Then for any s such that s(H+1) + i 6= 1, we have
V?(s) - Q*(s,∏(s)) ≥ YH -ε.
For H > 500, we have γH - ε > 0.366. Therefore,
((V*(s)-Q*(s,π(s))) ds ≥ (	0.366∙I[s(H+1)
S S ∈'k	S S ∈'k
=1-i] ds ≥ 0.366∙2-HT = 0.183∙2-h.
□
Proof of Lemma B.4. Now let us fix a parameter H and policy π. For every h, we prove by induction
that there exists a function ξh(s), such that
(a)	0 ≤ ξh(s) ≤ min{μ∏(s), 1},
(b)	infs∈'k ξh(s) = suPs∈'k ξh(s),	∀k ∈ [AH],
(C)RS∈[0,1) dξh(S) ≥ 1 - h ∙ z(n)/2H-1.
For the base case h = 1, we define ξh(s) = μ∏(s) = 1 for all S ∈ [0,1). Now We construct ξh+ι
from ξh .
For a fixed k ∈ [2H], define Ik = k ∙ 2-H, τ% = (k + 1) ∙ 2-H as the left and right endpoints of
interval `k. Let {x(ki)}i2=1 be the set of 2 solutions of equation
2x + 2-H ≡ lk mod 1
where 0 ≤ x < 1, and we define yk(i) = x(ki) + 2-H mod 1. By definition, only states from the
set ∪i2=1 [x(ki), yk(i)) can reach states in interval `k by a single transition. We define a set Ik = {i :
1 ≤ i ≤ 2, ∣{∏(s) : s ∈ [xki),yki))}∣ = 1}. That is, the intervals where policy π acts unanimously.
Consequently, for i ∈ Ik, the set {s : S ∈ [xf),y也,f (s,π(s)) ∈ 'k} is an interval of length
2-H-1, and has the form
Uki) =f[xki) + wki) ∙ 2-H-1,xki) + (wki) + 1) ∙ 2-H-1)
for some integer wk(i) ∈ {0, 1}. By statement (b) of induction hypothesis,
inf ξh(S) = sup ξh(S).	(20)
S∈u(i)	(i)
S∈uk	S∈uk
Now, the density ξh+1(S) for S ∈ `k is defined as,
ξh+ι(s) =f X 2 ∙ξh(x"w? ∙ 2-HT)
i∈Ik
The intuition of the construction is that, we discard those density that cause non-uniform behavior
(that is, the density in intervals [x(ki), yk(i)) where i 6∈ Ik). When the number of pieces ofπ is small,
we can keep most of the density. Now, statement (b) is naturally satisfied by definition of ξh+1 . We
verify statement (a) and (c) below.
19
Under review as a conference paper at ICLR 2020
For any set B ⊆ `k, let (T π)-1 (B) = {s ∈ S : f(s, π(s)) ∈ B} be the inverse of Markov
transition Tπ . Then we have,
(Tπξh)(B)d=efξh(Tπ)-1(B) = X ξh(Tπ)-1(B)∩[x(ki),yk(i))
i∈{1,2}
≥Xξh (Tπ)-1(B)∩[x(ki),yk(i))
i∈Ik
=XI(Tn)-1 (B) ∩ [xki),yki))∣ξh 卜ki) + wki) ∙ 2-H-1)	(ByEq. (20))
i∈Ik
=X 号ξh 收+wki)∙ 2-H-1)，
i∈Ik
where | ∙ | is the shorthand for standard LebesgUe measure.
By definition, we have
ξh+1(B) = X1Bξh (xki) + Wki) ∙ 2-H-1) ≤ (Tnξh)(B) ≤ (Tnμ∏)(B) = μ∏+1(B),
i∈Ik
which verifies statement (a).
For statement (c), recall that S = [0, 1) is the state space. Note that Tn preserve the overall density.
That is (Tnξh) (S) = ξh(S). We only need to prove that
(Tnξh)(S) - ξh+1(S) ≤ h ∙ z(π)∕2H-1	(21)
and statement (c) follows by induction.
By definition of ξh+1 (s) and the induction hypothesis that ξh(s) ≤ 1, we have
(Tnξh)('k) - ξh+1('k) ≤ (2 -∣Ik∣)2-H.
On the other hand, for any s ∈ S, the set {k ∈ [2H] : s ∈ ∪i2=1 [x(ki), yk(i))} has cardinality 2, which
means that one intermittent point of πcan correspond to at most 2 intervals that are not in Ik for
some k. Thus, we have
X ∣Ik∣ ≥ 2H+1 - X ∣{k ∈ [2H ]: s ∈ ∪2=1[xki),yki))}∣ ≥ 2H+1 - 2 ∙ z(π).
0≤k<2H	s:n-(s)6=n+(s)
Consequently
(Tnξh)(S) - ξh+1(S) = X ((Tnξh)('k) - ξh+1('k)) ≤ z(π)2-H+1,
0≤k<2H
which proves statement (c).	口
B.4	Sample Complexity Lower Bound of Q-learning
Recall that corollary 4.4 says that in order to find a near-optimal policy by a Q-learning algorithm, an
exponentially large Q-network is required. In this subsection, we show that even ifan exponentially
large Q-network is applied for Q learning, still we need to collect an exponentially large number
of samples, ruling out the possibility of efficiently solving the constructed MDPs with Q-learning
algorithms.
Towards proving the sample complexity lower bound, we consider a stronger family of Q-learning
algorithm, Q-learning with Oracle (Algorithm 3). We assume that the algorithm has access to a
Q-ORACLE, which returns the optimal Q-function upon querying any pair (s, a) during the training
process. Q-learning with Oracle is conceptually a stronger computation model than the vanilla
Q-learning algorithm, because it can directly fit the Q functions with supervised learning, without
relying on the rollouts or the previous Q function to estimate the target Q value. Theorem B.5 proves
a sample complexity lower bound for Q-learning algorithm on the constructed example.
20
Under review as a conference paper at ICLR 2020
Algorithm 3 Q-LEARNING WITH ORACLE
Require: A hypothesis space Q of Q-function parameterization.
1:	Sample so 〜μ from the initial state distribution μ
2:	for i = 1, 2,…,n do
3:	Decide whether to restart the trajectory by setting Si 〜μ based on historical information
4:	Query Q-ORACLE to get the function Q?(si, ∙).
5:	Apply any action a% (according to any rule) and sample Si+ι 〜f (si,ai).
6:	Learn the Q-function that fit all the data the best:
Q — arg min — T (Q(si, ai) - Q?(si,ai))2 + λR(Q)
Q∈Q n i=1
7:	Return the greedy policy according to Q.
Theorem B.5 (Informal Version of Theorem B.7). Suppose Q is an infinitely-wide two-layer neural
networks, and R(Q) is `1 norm of the parameters and serves as a tiebreaker. Then, any instantiation
of the Q-LEARNING WITH O RACLE algorithm requires exponentially many samples to find a policy
π such that η(π) > 0.99η(π?).
Formal proof of Theorem B.5 is given in Appendix B.5. The proof of Theorem B.5 is to exploit the
sparsity of the solution found by minimal-norm tie-breaker. It can be proven that there are at most
O(n) non-zero neurons in the minimal-norm solution, where n is the number of data points. The
proof is completed by combining with Theorem 4.3.
B.5	Proof of Theorem B.5
A two-layer ReLU neural net Q(s, ∙) with input S is of the following form,
d
Q(S, a) =	wi,a [kiS+ bi]+ +ca,	(22)
i=1
where d is the number of hidden neurons. wi,a , ca, ki , bi are parameters of this neural net, where
ci,a, bi are bias terms. [x]+ is a shorthand for ReLU activation I[x > 0]x. Now we define the norm
of a neural net.
Definition B.6 (Norm ofa Neural Net). The norm of a two-layer ReLU neural net is defined as,
d
X|wi,a| + |ki|.	(23)
i=1
Recall that the Q-learning with oracle algorithm finds the solution by the following supervised
learning problem,
1n
min — (Q( (Q(St, at) - Q?(st, at))2 .	(24)
Q∈Q n t=1
Then, we present the formal version of theorem B.5.
Theorem B.7. Let Q be the minimal `1 norm solution to Eq. (24), andπ the greedy policy according
to Q. When n = o(exp(cH)/H), we have η(π) < 0.99η(π?).
The proof of Theorem B.5 is by characterizing the minimal-norm solution, namely the sparsity of
the minimal-norm solution as stated in the next lemma.
Lemma B.8. The minimal-norm solution to Eq. (24) has at most 32n + 1 non-zero neurons. That
is, |{i : ki 6= 0}| ≤ 32n + 1.
We first present the proof of Theorem B.7, followed by the proof of Theorem B.8.
Proof of Theorem B.7. Recall that the policy is given by π(S) = arg maxa∈A Q(S, a). For a Q-
function with 32n + 2 pieces, the greedy policy according to Q(S, a) has at most 64n + 4 pieces.
Combining with Theorem 4.3, in order to find a policy π such that η(π) > 0.99η(π?), n needs to be
exponentially large (in effective horizon H).	□
21
Under review as a conference paper at ICLR 2020
Proof of Lemma B.8 is based on merging neurons. Let xi = -bi/ki, wi = (wi,1, wi,2), and
c = (c1, c2). In vector form, neural net defined in Eq. (22) can be written as,
d
Q(s, •) = £wi [ki(s — Xi)]+ + C.
i=1
First we show that neurons with the same xi can be merged together.
Lemma B.9. Consider the following two neurons,
k1 [s - x1]+ w1, k2 [s - x2]+ w2.
with k1 > 0, k2 > 0. If x1 = x2, then we can replace them with one single neuron of the form
k0 [x - x1]+ w0 without changing the output of the network. Furthermore, if w1 6= 0, w2 6= 0, the
norm strictly decreases after replacement.
Proof. We set k0 = ∣∕∖Wn+ + k2W2∣1, and w0 = (k1W1 + k2W2)∕k0, where |w|i represents the
1-norm of vector w. Then, for all s ∈ R,
k0 [x - x1]+ W0 = (k1W1 +k2W2) [s - x1]+ = k1 [s - x1]+ W1 +k2 [s - x1]+ W2.
The norm of the new neuron is ∖k0∖ + ∖W0∖1. By calculation we have,
∖k0∖ + ∖w0∖1 = 2p∖kιwι + k2W2∖1 ≤ 2p∖kιwι∖ι + ∖k2W2∖1
(a)	/  ____________________、
≤ 2	∖k1w1∖1 +	∖k2w2∖1 ≤ ∖k1∖ + ∖w1∖1 + ∖k2∖ + ∖w2∖1.
Note that the inequality (a) is strictly less when ∖k1W1∖1 = 0 and ∖k2W2 ∖ι=0.	口
Next we consider merging two neurons with different intercepts between two data points. Without
loss of generality, assume the data points are listed in ascending order. That is, si ≤ si+1.
Lemma B.10. Consider two neurons
k1 [s - x0]+ W1, k2 [s - x0 - δ]+ W2.
with k1 > 0, k2 > 0. If si ≤ x0 < x0 + δ ≤ si+1 for some 1 ≤ i ≤ n, then the two neurons can
replaced by a set of three neurons,
00
k [s — xo]+ w , k [s — si]+ W, k [s — Si+ι1 (―w)
such that for s ≤ si or s ≥ si+1, the output of the network is unchanged. Furthermore, ifδ ≤
(si+1 — si)/16 and ∖W1 ∖1 6= 0, ∖W2 ∖1 6= 0, the norm decreases strictly.
Proof. For simplicity, define ∆ = si+1 — si . We set
k = √∖k1W1 + k2W2∖1,
W0 = (k1W1 + k2W2)/k0,
k = p∖k2W2∖1δ∕∆,
W = —k2W2δ∕(∆k).
Note that for s ≤ si, all of the neurons are inactive. For s ≥ si+1, all of the neurons are active, and
00
k0W0(s — x0) + kWk (s — si) — kWk (s — si+1)
= (k1W1 + k2W2)(s — x0) — k2W2δ
= k1(s — x0)W1 + k2(s — x0 — δ)W2,
which means that the output of the network is unchanged. Now consider the norm of the two
networks. Without loss of generality, assume ∖k1W1 ∖1 > ∖k2W2 ∖1. The original network has norm
∖k1 ∖ + ∖W1 ∖1 + ∖k2∖ + ∖W2∖1. And the new network has norm
∖k0∖ + ∖w0∖1 + 2∖k∖ + 2∖W ∖ι = 2P∖k1W1 + k2W2∖1 + 4p∖k2W2∖1δ∕∆
(a)	(	1----------- 1	\
≤ ∖kι ∖ + ∖w1∖1 + ∖k2∖ + ∖w2∖1 +(4 √∖k2W2∖1δ∕∆ — 2(∖k2∖ + ∖w2∖1) J ,
22
Under review as a conference paper at ICLR 2020
where the inequality (a) is a result of Lemma E.1, and is strictly less when |w1 |1 6= 0, |w2|1 6= 0.
When δ/∆ < 1/16, We have 0p∣k2W2∣ιδ∕∆ - ɪ(隹| + ∣w2∣1)) < 0, which implies that
.. ∙ ∙ . ~.
|k | + |w |i + 2肉 +2∣w|i < ∣kι∣ + ∣wι∣ι + 隹| + ∣W2∣1.
□
Similarly, two neurons with k1 < 0 and k2 < 0 can be merged together.
Now we are ready to prove Lemma B.8. As hinted by previous lemmas, we show that between two
data points, there are at most 34 non-zero neurons in the minimal norm solution.
Proof of Lemma B.8. Consider the solution to Eq. (24). Without loss of generality, assume that
si ≤ si+1. In the minimal norm solution, it is obvious that |wi|1 = 0 if and only if ki = 0.
Therefore we only consider those neurons with ki 6= 0, denoted by index 1 ≤ i ≤ d0 .
Let Bt = {-bi/ki : 1 ≤ i ≤ d0, st < -bi/ki < st+1, ki > 0}. Next we prove that in the minimal
norm solution, |Bt| ≤ 15. For the sake of contradiction, suppse |Bt| > 15. Then there exists i, j
such that, st < -bi/ki < st+1, st < -bj /kj < st+1, |bi/ki - bj/kj| < (st+1 - si)/16, and
ki > 0, kj > 0. By Lemma B.10, we can obtain a neural net with smaller norm by merging neurons
i, j together without violating Eq. (24), which leads to contradiction.
By Lemma B.9, |Bt| ≤ 15 implies that there are at most 15 non-zero neurons with st < -bi/ki <
st+1 and ki > 0. For the same reason, there are at most 15 non-zero neurons with st < -bi/ki <
st+1 and ki < 0.
On the other hand, there are at most 2 non-zero neurons with st = -bi/ki for all t ≤ n, and there
are at most 1 non-zero neurons with -bi∕ki < si. Therefore, we have d0 ≤ 32n + 1.	□
B.6 Proof of Theorem 5.1
In this section we present the full proof of Theorem 5.1.
Proof. First we define the true trajectory estimator
k-1
η(s0,ao,aι,…，ak) = EYjr(sj,aj) + YkQ?(sk,ak),
j=0
the true optimal action sequence
a?, a?,…，a? = arg max η(s0,a0,a1,…，ak),
a0,a1,…，ak
and the true optimal trajectory
S? = S0, Sj = f(sj-i,a?-i), ∀j > 1.
It follows from the definition of optimal policy that, a? = ∏?(Sj). Consequently we have
Sk(H-k+1) = Sk (H-k+2) =…=Sk(H ) = 1.
Define the set G = {s : S(H-k+i) = S(H-k+2)= … =S(H) = 1}. We claim that the following
function satisfies the statement of Theorem 5.1
2
Q(s, a) = I[s ∈ G] ∙ 1-.
Since S?k ∈ G, and Sk 6∈ G for Sk generated by non-optimal action sequence, we have
Q(Sk, a) > Q (Sk, a) ≥ Q (Sk, a) > Q(Sk, a),
where the second inequality comes from the optimality of action sequence a?h . As a consequence,
for any (a0,a1,…，ak) = (a?,a?,…，a?)
n(So, a?, a?,…，ak) > n(So, a?, a?,…，a?) ≥ n(So, ao, ai,…，a，k) > n(So, ao, ai,…，a，k).
Therefore, (^?, ^?,…，^?) = (a0, a?,…，ak).	□
23
Under review as a conference paper at ICLR 2020
C Extension of the Constructed Family
In this section, we present an extension to our construction such that the dynamics is Lipschitz. The
action space is A = {0, 1, 2, 3, 4}. We define CLIP(x) = max{min{x, 1}, 0}.
Definition C.1. Given effective horizon H = (1 - γ)-1, we define an MDP MH0 as follows. Let
κ = 2-H. The dynamics is defined as
f(s,0) = CLIP(2s), f(s,1) = CLIP(2s - 1),
f(s,2) = CLIP(2s + κ), f(s, 3) = CLIP(2s + κ - 1),	f(s,4) = CLIP(2s + κ - 2).
Reward function is given by
r(s, 0) = r(s, 1) = I[1/2 ≤ s < 1]
r(s, 2) = r(s, 3) = r(s, 4) = I[1/2 ≤ s < 1] - 2(γH-1 - γH)
The intuition behind the extension is that, we perform the mod operation manually. The following
theorem is an analog to Theorem 4.2.
Theorem C.2. The optimal policy π? for MH0 is defined by,
0, I[s(H+1) = 0] and 2s < 1,
1, I[s(H+1) = 0] and 1 ≤ 2s < 2,
π?(S) = < 2, I[s(H+1) = 1] and 2s + θ < 1,	(25)
3,	l[s(H+1) = 1] and 1 ≤ 2s + θ < 2,
4,	I[s(H+1) = 1] and 2 < 2s + θ.
And the corresponding optimal value function is,
H∞
V?(s) = Xγh-1s(h) + X γh-1 1 + 2(s(h+1) - s(h)) +γH-1 2s(H+1) - 2 .	(26)
h=1	h=H+1
We can obtain a similar upper bound on the performance of policies with polynomial pieces.
Theorem C.3. Let MH be the MDP constructed in Definition C.1. Suppose a piecewise linear
policy π has a near optimal reward in the sense that η(π) ≥ 0.99 ∙ η(π?), then it has to have at least
Ω (exp(cH)/H) pieces for some universal constant c > 0.
The proof is very similar to that for Theorem 4.3. One of the difference here is to consider the case
where f(s, a) = 0 or f(s, a) = 1 separately. Attentive readers may notice that the dynamics where
f (s, a) = 0 or f (s, a) = 1 may destroy the “near uniform” behavior of state distribution μ∏ (see
Lemma B.4). Here we show that such destroy comes with high cost. Formally speaking, if the clip
is triggered in an interval, then the averaged single-step suboptimality gap is 0.1/(1 - γ).
Lemma C.4. Let `k = [k/2H/2, (k + 1)/2H/2). For k ∈ [2H/2], if policy π does not change its
action at interval 'k (that is, ∣{∏(s) : S ∈ 'k}| = 1) and f (s, π(s)) = 0, ∀s ∈ 'k or f (s, π(s))=
1, ∀s ∈ `k . We have
(V ?(S) - Q?(S, π(S))) dS ≥
0.1
1 - Y
(27)
for large enough H.
Proof. Without loss of generality, we consider the case where f(S, π(S)) = 0. The proof for
f(S, π(S)) = 1 is essentially the same.
By elementary manipulation, we have
H
V?(S) -V?(0) ≥ Xγi-1S(i).
i=1
24
Under review as a conference paper at ICLR 2020
Let S = f (s, π*(s)). It follows from Bellman equation (1) that
V?(s) = r(s, ∏*(s)) + YV?(S),
Q?(s, ∏(s)) = r(s, ∏(s)) + γV*(0).
Recall that we define = 2 γH-1 - γH . As a consequence,
(V?(S)- Q?(s, ∏(s))) > r(s, ∏*(s))-r(s, ∏(s)) + Y(V?(S)- V?(0))
H
≥- + γ X γi-1S⑴.
i=1
Plugging into Eq (27), we have
7V^∣ [	(V*(S)- Q*(S,π(S))) ds ≥ -〜
|'k| JsEkk
≥ - C + X Yi (ɪ Z	S⑶ dS↑ ≥- +
≥	=1以i Is%	) ≥
|'k| Js∈'k
Yi ^(i) ds
YH/2 - YH
1 - Y
Lemma 27 is proved by noticing for large enough H ,
YH/2 - YH
-e +	i-------
1-Y
0.1
1 - Y
>
□
Let D = {0,1} for simplicity. For any policy ∏, We define a transition operator Tπ, such that
(TπQ(Z) = μ ({s : p(s, a) ∈ Z, f (s, π(Sy) ∈ D),
and the state distribution induced by it, defined recursively by
μπ (S) = 1,
_	ʌ-- _
μh = TT μh-1.
We also define the density function for states that are truncated as follows,
Ph(S)= I[f(S,n(S)) ∈ D]μπ (s).
Following advantage decomposition lemma (Corollary B.2), the key step for proving Theorem C.3
is
∞∞
η(∏?) - η(∏) ≥ X Yh-1Es〜μπ [V*(S)- Q*(s,∏(s))] + X YhES〜Ph [V*(S)- Q*(s,∏(s))].
h=1	h=1
(28)
Similar to Lemma B.4, the following lemma shows that the density for most of the small intervals is
either uniformly clipped, or uniformly spread over this interval.
Lemma C.5. Let z(π) be the number of pieces of policy π. For k ∈ [2H/2], define interval `k =
[k∕2H∕2, (k + 1)∕2H∕2). Let ν%(k) = inf§三热 μ∏(s) and ω%(k) = inf§三热 p∏(s). Ifthe initial state
distribution μ is uniform distribution, thenfor any h ≥ 1,
2H/2	h-1 2H/2
X 2-H∕2 ∙ Vh(k) + XX 2-H∕2 ∙ 3h (k) ≥ 1 - 2hz(π⅛≡.	(29)
k=0	h0=1 k=0	2
Proof. Omitted. The proof is similar to Lemma B.4.	□
Now we present the proof for Theorem C.3.
25
Under review as a conference paper at ICLR 2020
Proof of Theorem C.3. For any k ∈ [2H/2], consider the interval `k = [k/2H/2, (k + 1)/2H/2).. If
∏ does not change at interval 'k (that is, ∣{∏(s) : S ∈ 'k}| = 1), by Lemma B.3 We have
/	(V?(S) - Q?(s, π(s))) ds ≥ 0.075 ∙ 2-H/2.
s s∈'k
(30)
By Eq (28), Eq (30) and Lemma (27), we have
η(π?) - η(π)
H
≥ XYh-1
h=1
2H/2
H 2H/2
E 0.075 ∙ 2-H/2 ∙ νh(k) + £ EYh ∙ 2-H/2 ∙ 3h(k) ∙
k=0
h=1 k=0
0.1
1 - Y
(31)
By Lemma C.5, we get
2H/2
h-1 2H/2
E 2-H/2 ∙ νh(k) +££ 2-H/2 ∙3h, (k) ≥ 1 - 2h
k=0
h0=1 k=0
z(π) + 10
2H/2
(32)
For the sake of contradiction, we assume z(π) = o (exp(cH)/H), then for large enough H we have,
Hz(π) + 10
1 -2	2H/2	> 0.8.
Consequently,
k=0
Plugging in Eq (31), we get
2H/2	h-1 2H/2
X 2-H/2 ∙ νh(k) > 0.8 -XX
2 — H/2 ∙ 3h0 (k).
h0=1 k=0
(33)
η(π?) - η(π)
H
≥ X 0.075Yh-1
h=1
2H/2
H 2H/2
∑ 2-H/2Vh(k)	+£ EYh ∙ 2-H/2 ∙ ωh(k) ∙
k=0
h=1 k=0
0.1
1 - Y
H
≥ X 0.075Yh-1
h=1
h-1 2H/2
H 2H/2
0.8- E E 2-H/2 ∙ 3h，(k)+ £ EYh ∙ 2-H/2 ∙ 3h(k)∙
1 - YH
≥ 0.06....-	+
1-Y
h，=1 k=0
H 2H/2
XX ∙2-H/2 ∙ 3h(k)
h=1 k=0
H 2H/2	h-1
XX ∙2-H/2 ∙ 3h(k)Y-Y
h=1 k=0	Y
h=1 k=0
0.1
1 - Y
H
- 0.075 X Y
h，=h
h0-1
1 - YH
≥ 0.06....-	+
1-Y
(0.1γ - 0.075 (1 - YHfX
When Y > 1/4, we have 0.1Y - 0.075(1 - YH-h) > 0. As a consequence,
?	1 - YH 0.01
η(∏ ) — η(∏) > 0.06-------≥ -—.
Now, since η(π?) ≤ 1/(1 - γ), we have η(π) < 0.99η(π?). Therefore for near-optimal policy π,
z(π) = Ω (exp(cH)/H).	□
D Omitted Details of Empirical Results in the Toy Example
D. 1 Two Methods to Generate MDPs
In this section we present two methods of generating MDPs. In both methods, the dynamics p(s, a)
has three pieces and is Lipschitz. The dynamics is generated by connecting kinks by linear lines.
26
Under review as a conference paper at ICLR 2020
RAND method. As stated in Section 4.3, the RAND method generates kinks {xi } and the corre-
sponding values {x0i} randomly. In this method, the generated MDPs are with less structure. The
details are shown as follows.
•	State space S = [0, 1).
•	Action space A = {0, 1}.
•	Number of pieces is fixed to 3. The positions of the kinks are generated by, Xi 〜U(0,1)
for i = 1, 2 and x0 = 0, x1 = 1. The values are generated by x0i 〜 U (0, 1).
•	The reward function is given by r(s, a) = s, ∀s ∈ S, a ∈ A.
•	The horizon is fixed as H = 10.
•	Initial state distribution is U(0, 1).
Figure 1 visualizes one of the RAND-generated MDPs with complex Q-functions.
SEMI-RAND method. In this method, we add some structures to the dynamics, resulting in a
more significant probability that the optimal policy is complex. We generate dynamics with fix and
shared kinks, generate the output at the kinks to make the functions fluctuating. The details are
shown as follows.
•	State space S = [0, 1).
•	Action space A = {0, 1}.
•	Number of pieces is fixed to 3. The positions of the kinks are generated by, xi = i/3, ∀0 ≤
i ≤ 3. And the values are generated by Xi 〜0.65 × I[i mod 2 = 0] + 0.35 × U(0,1).
•	The reward function is r(s, a) = s for all a ∈ A.
•	The horizon is fixed as H = 10.
•	Initial state distribution is U(0, 1).
Figure 1 visualizes one of the MDPs generated by SEMI-RAND method.
D.2 The Complexity of Optimal Policies in Randomly Generated MDPs
We randomly generate 103 1-dimensional MDPs whose dynamics has constant number of pieces.
The histogram of number of pieces in optimal policy π? is plotted. As shown in Figure 9, even for
horizon H = 10, the optimal policy tends to have much more pieces than the dynamics.
Figure 9: The histogram of number of pieces in optimal policy π? in random method (left) and
semi-random method(right).
D.3 Implementation Details of Algorithms in Randomly Generated MDP
SEMI-RAND MDP The MDP where we run the experiment is given by the SEMI-RAND
method, described in Section D.1. We list the dynamics of this MDP in the following.
r(s, a) = s,	∀s ∈ S, a ∈ A,
27
Under review as a conference paper at ICLR 2020
((0.131 - 0.690) ∙x∕0.333 + 0.690,	0 ≤ x < 0.333,
f(s, 0) =	( (0.907	-	0.131)	∙	(x	-	0.333)/0.334 + 0.131,	0.333	≤	X < 0.667,
[(0.079	-	0.907)	∙	(x	-	0.667)/0.333 + 0.907,	0.667	≤	x,
f (0.134 - 0.865) ∙ x/0.333 + 0.865,	0 ≤ x < 0.333,
f (s, 1) =	( (0.750	-	0.134)	∙	(x	-	0.333)/0.334 + 0.134,	0.333	≤	x < 0.667,
[(0.053	-	0.750)	∙	(x	-	0.667)/0.333 + 0.750,	0.667	≤	x,
Implementation details of DQN algorithm We present the hyper-parameters of DQN algorithm.
Our implementation is based on PyTorch tutorials7.
•	The Q-network is a fully connected neural net with one hidden-layer. The width of the
hidden-layer is varying.
•	The optimizer is SGD with learning rate 0.001 and momentum 0.9.
•	The size of replay buffer is 104.
•	Target-net update frequency is 50.
•	Batch size in policy optimization is 128.
•	The behavior policy is greedy policy according to the current Q-network with -greedy.
exponentially decays from 0.9 to 0.01. Specifically, = 0.01 + 0.89 exp(-t/200) at the
t-th episode.
Implementation details of MBPO algorithm For the model-learning step, we use `2 loss to train
our model, and we use Soft Actor-Critic (SAC) (Haarnoja et al., 2018) in the policy optimization
step. The parameters are set as,
•	number of hidden neurons in model-net: 32,
•	number of hidden neurons in value-net: 512,
•	optimizer for model-learning: Adam with learning rate 0.001.
•	temperature: τ = 0.01,
•	the model rollout steps: M = 5,
•	the length of the rollout: k = 5,
•	number of policy optimization step: G = 5.
Other hyper-parameters are kept the same as DQN algorithm.
Implementation details of TRPO algorithm For the model-learning step, we use `2 loss to train
our model. Instead of TRPO (Schulman et al., 2015), we use PPO (Schulman et al., 2017) as policy
optimizer. The parameters are set as,
•	number of hidden neurons in model-net: 32,
•	number of hidden neurons in policy-net: 512,
•	number of hidden neurons in value-net: 512,
•	optimizer: Adam with learning rate 0.001,
•	number of policy optimization step: 5.
•	The behavior policy is -greedy policy according to the current policy network. expo-
nential decays from 0.9 to 0.01. Specifically, = 0.01 + 0.89 exp(-t/20000) at the t-th
episode.
7https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.
html
28
Under review as a conference paper at ICLR 2020
Implementation details of Model-based Planning algorithm The perfect model-based planning
algorithm iterates between learning the dynamics from sampled trajectories, and planning with the
learned dynamics (with an exponential time algorithm which enumerates all the possible future
sequence of actions). The parameters are set as,
•	number of hidden neurons in model-net: 32,
•	optimizer for model-learning: Adam with learning rate 0.001.
Implementation details of bootstrapping The training time behavior of the algorithm is exactly
like DQN algorithm, except that the number of hidden neurons in the Q-net is set to 64. Other
parameters are set as,
•	number of hidden neurons in model-net: 32,
•	optimizer for model-learning: Adam with learning rate 0.001.
•	planning horizon varies.
E Technical Lemmas
In this section, we present the technical lemmas used in this paper.
Lemma E.1. For A, B, C, D ≥ 0 and AC ≥ BD, we have
A + C + 1(B + D) ≥ 2√AC + BD.
Furthermore, when BD > 0, the inequality is strict.
Proof. Note that A + B + 2(C + D) ≥ 2√AC + √BD. And We have,
(2√AC + √BD)2 - 0√AC + BD) 2 = 4√AC ∙ BD - 3BD ≥ BD ≥ 0.
And When BD > 0, the inequality is strict.	□
29