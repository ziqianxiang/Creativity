Under review as a conference paper at ICLR 2020
SDNet: Contextualized Attention-based Deep
Network for Conversational Question An-
SWERING
Anonymous authors
Paper under double-blind review
Ab stract
Conversational question answering (CQA) is a novel QA task that requires the
understanding of dialogue context. Different from traditional single-turn ma-
chine reading comprehension (MRC), CQA is a comprehensive task comprised
of passage reading, coreference resolution, and contextual understanding. In this
paper, we propose an innovative contextualized attention-based deep neural net-
work, SDNet, to fuse context into traditional MRC models. Our model lever-
ages both inter-attention and self-attention to comprehend the conversation and
passage. Furthermore, we demonstrate a novel method to integrate the BERT
contextual model as a sub-module in our network. Empirical results show the ef-
fectiveness of SDNet. On the CoQA leaderboard, it outperforms the previous best
model’s F1 score by 1.6%. Our ensemble model further improves the F1 score by
2.7%.
1 Introduction
Machine reading comprehension (MRC) is a core NLP task in which a machine reads a passage
and then answers related questions. It requires a deep understanding of both the article and the
question, as well as the ability to reason about the passage and make inferences. These capabilities
are essential in applications like search engines and conversational agents. In recent years, there have
been numerous studies in this field (Huang et al., 2017; Seo et al., 2016; Chen et al., 2017; Liu et al.,
2017), with various innovations in text encoding, attention mechanisms and answer verification.
However, traditional MRC tasks often take the form of single-turn question answering. In other
words, there is no connection between different questions and answers to the same passage. This
oversimplifies the conversational manner humans naturally take when probing a passage, where
question turns are assumed to be remembered as context to subsequent queries. Figure 1 demon-
strates an example of conversational question answering in which one needs to correctly refer “she”
in the last two rounds of questions to its antecedent in the first question, “Cotton.” To accomplish
this kind of task, the machine must comprehend both the current round’s question and previous
rounds of utterances in order to perform coreference resolution, pragmatic reasoning and semantic
implication.
To facilitate research in conversation question answering (CQA), several public datasets have been
published that evaluate a model’s efficacy in this field, such as CoQA (Reddy et al., 2018), QuAC
(Choi et al., 2018) and QBLink (Elgohary et al., 2018). In these datasets, to generate correct re-
sponses, models need to fully understand the given passage as well as the dialogue context. Thus,
traditional MRC models are not suitable to be directly applied to this scenario.
Therefore, a number of models have been proposed to tackle the conversational QA task.
DrQA+PGNet (Reddy et al., 2018) combines evidence finding and answer generation to produce
answers. BiDAF++ (Yatskar, 2018) achieves better results by employing answer marking and con-
textualized word embeddings on the MRC model BiDAF (Seo et al., 2016). FlowQA (Huang et al.,
2018) leverages a recurrent neural network over previous rounds of questions and answers to absorb
information from its history context.
1
Under review as a conference paper at ICLR 2020
Once upon a time, in a barn near a farm house, there lived a little white kitten named
Cotton. Cotton lived high up in a nice warm place above the barn where all of the
farmer’s horses slept. But Cotton wasn’t alone in her little home above the barn, oh no.
She shared her hay bed with her mommy and 5 other sisters...
Q1 : What color was Cotton?
A1 : white
Q2 : Where did she live?
A2 : in a barn
Q3 : Did she live alone?
A3: no
Figure 1: Example passage and first three rounds of question and answers from CoQA dataset
(Reddy et al., 2018). Pronouns requiring coreference resolution is marked in bold.
In this paper, we propose SDNet, a contextual attention-based deep neural network for the conver-
sational question answering task. Our network stems from machine reading comprehension models,
but it has several unique characteristics to tackle context understanding. First, we apply both inter-
attention and self-attention on the passage and question to obtain a more effective understanding of
the passage and dialogue history. Second, we prepend previous rounds of questions and answers
to the current question to incorporate contextual information. Third, SDNet leverages the latest
breakthrough in NLP: BERT contextual embeddings (Devlin et al., 2018).
Different from the canonical way of employing BERT as a monolithic structure with a thin linear
task-specific layer, we utilize BERT as a contextualized embedder and absorb its structure into our
network. To accomplish this, we align the traditional tokenizer with the Byte Pair Encoding (BPE)
tokenizer in BERT. Furthermore, instead of using only the last layer’s output from BERT (Devlin
et al., 2018), we employ a weighted sum of BERT layer outputs to take advantage of all levels of
semantic abstraction. Finally, we lock the internal parameters of BERT during training, which saves
considerable computational cost. These techniques are also applicable to other NLP tasks.
We evaluate SDNet on the CoQA dataset, and it improves on the previous state-of-the-art F1 score
by 1.6% (from 75.0% to 76.6%). The ensemble model further increases the F1 score to 79.3%.
2	Approach
In this section, we propose our neural model, SDNet, for the conversational question answering task.
We first formulate the problem and then present an overview of the model before delving into the
details of the model structure.
2.1	Problem formulation
Given a passage/context C, and question-answer pairs from previous rounds of conversation
Q1, A1, Q2, A2, ..., Qk-1, Ak-1, the task is to generate response Ak given the latest question Qk.
The response is dependent on both the passage and historic questions and answers.
To incorporate conversation history into response generation, we employ the idea from
DrQA+PGNet (Reddy et al., 2018) to prepend the latest N rounds of QAs to the current ques-
tion Qk . The problem is then converted into a single-turn machine reading comprehension task,
where the reformulated question is Qk = {Qk-N; Ak-N; ..., Qk-1; Ak-1; Qk}1.
2.2	Model Overview
Encoding layer encodes each token in passage and question into a fixed-length vector, which in-
cludes both word embeddings and contextualized embeddings. For contextualized embedding, we
1To differentiate between question and answering, we add a special word hQi before each question and hAi
before each answer.
2
Under review as a conference paper at ICLR 2020
RNN
Answer Span
Prediction
Context Final
Representation
RNN
Self-Attention
RNN
Inter-Attention
Contextualized
Word Embedding
BERT
Word-level
Attention
Word Embedding
GloVe
Start End Yes No Unknown
↑ t t t t
Xi	x2	Xm∣	I	q1	q2	qn
Context	Question
Question Final
Representation
Question
Understanding
Figure 2: SDNet model structure.
Output layer
Integration layer
Encoding layer
utilize the pretrained language understanding model BERT (Devlin et al., 2018). Different from
previous work, we fix the parameters in BERT model and use the linear combination of embeddings
from different layers in BERT.
Integration layer uses multi-layer recurrent neural networks (RNN) to capture contextual informa-
tion within passage and question. To characterize the relationship between passage and question, we
conduct word-level attention from question to passage both before and after the RNNs. We employ
the idea of history-of-word from FusionNet (Huang et al., 2017) to reduce the dimension of output
hidden vectors. Furthermore, we conduct self-attention to extract relationship between words at
different positions of context and question.
Output layer computes the final answer span. It uses attention to condense the question into a fixed-
length vector, which is then used in a bilinear projection to obtain the probability that the answer
should start and end at each position.
An illustration of our model SDNet is in Figure 2.
2.3	Encoding layer
We first use GloVe (Pennington et al., 2014) embedding for each word in the context and question.
Additionally, we compute a feature vector fw for each context word, following the approach in
DrQA (Chen et al., 2017). This feature vector contains a 12-dim POS embedding, an 8-dim NER
embedding, a 3-dim exact matching vector emi indicating whether this word, its lower-case form or
its stem appears in the question, and a 1-dim normalized term frequency.
BERT as Contextual Embedder. We design a number of methods to leverage BERT (Devlin et al.,
2018) as a contextualized embedder in our model.
First, because BERT uses Byte Pair Encoding (BPE) (Sennrich et al., 2015) as the tokenizer, the
generated tokens are sub-words and may not align with traditional tokenizer results. To incorporate
BERT into our network, we first use a conventional tokenizer (e.g. spaCy) to get word sequences,
and then apply the BPE tokenizer from BERT to partition each word w in the sequence into sub-
words w = (b1, ..., bs). This alignment makes it possible to concurrently use BERT embeddings
and other word-level features. The contextual embedding of w is defined to be the averaged BERT
embedding of all sub-words bj , 1≤j≤s.
3
Under review as a conference paper at ICLR 2020
Second, Devlin et al. (2018) proposes the method to append thin task-specific linear layers to BERT,
which takes the result from the last transformer layer as input. However, as BERT contains multiple
layers, we employ a weighted sum of these layer outputs to take advantage of information from all
levels of semantic abstraction. This can help boost the performance compared with using only the
last transformer’s output.
Third, as BERT contains hundreds of millions of parameters, it takes a lot of time and space to
compute and store their gradients during optimization. To tackle this problem, we lock the in-
ternal weights of BERT during training, only updating the linear combination weights. This can
significantly increase the efficiency during training, which can be especially useful when computing
resource is limited.
To summarize, suppose a word w is tokenized to s BPE tokens w = (b1, b2, ..., bs), and BERT has
L layers that generate L embedding outputs for each BPE token, hlt , 1 ≤ l ≤ L, 1 ≤ t ≤ s. The
contextual embedding BERTw for word w is computed as:
BERT = XX C Ps=1 ht	(1)
BERTw = / , αl	,	(1)
s
l=1
where α1, ..., αL are trainable parameters.
2.4	Integration layer
Word-level Inter-Attention. We conduct attention from question to context (passage) based on
GloVe word embeddings. Suppose the context word embeddings are {h1C, ..., hCm} ⊂ Rd, and the
question word embeddings are {h1Q, ..., hnQ} ⊂ Rd. Then the attended vectors from question to
context are {hf,…,h£}:
Sij = ReLU(UhiC)DReLU(UhjQ)	(2)
ɑij H esij,	(3)
h C = X αjQ	(4)
j
where D ∈ Rk×k is a diagonal matrix and U ∈ Rd×k, k is the attention hidden size.
To simplify notation, we denote the above attention function as Attn(A, B, C), which linearly com-
bines the vector set C using attention scores computed from vector sets A and B . This resembles
the definition of attention in transformer (Vaswani et al., 2017). It follows that the word-level inter-
attention can be rewritten as Attn({hiC}im=1, {hiQ}in=1}, {hiQ}in=1}).
Therefore, the input vector for each context word and question word is:
WC = [GloVe(wCC); BERTwC; hC; fwc],	(5)
ii
WQ = [GloVe(wQ); BERTwQ ]	(6)
i
RNN. In this component, we use two separate bidirectional LSTMs (Hochreiter & Schmidhuber,
1997) to form the contextualized understanding for C and Q:
h1C,k,...,hCm,k=RNN(h1C,k-1,...,hCm,k-1)	(7)
h1Q,k, ..., hnQ,k = RNN(h1Q,k-1, ...,hnQ,k-1),	(8)
where hc,0 = WC, h(Qa = WQ, 1 ≤ k ≤ K and K is the number of RNN layers. We Use
variational dropout (Kingma et al., 2015) for the input vector to each layer of RNN, i.e. the dropout
mask is shared over different timesteps.
Question Understanding. For each question word in Q, we employ one more RNN layer to gener-
ate a higher level of understanding of the question.
h1Q,K+1,...,hnQ,K+1 =RNN(h1Q,...,hnQ),	(9)
hiQ = [hiQ,1; ...; hiQ,K]	(10)
4
Under review as a conference paper at ICLR 2020
Self-Attention on Question. As the question has integrated previous utterances, the model needs to
directly relate the previously mentioned concept with the current question for context understanding.
Therefore we employ self-attention on question:
{uiQ}in=1 = Attn({hiQ,K+1}in=1, {hiQ,K+1}in=1, {hiQ,K+1}in=1)	(11)
{uiQ }in=1 is the final representation of question words.
Multilevel Inter-Attention. After multiple RNN layers extract different levels of semantic abstrac-
tion, we conduct inter-attention from question to context based on these representations.
However, the cumulative output dimensions from all previous layers can be very large and computa-
tionally inefficient. Here we leverage the history-of-word idea from FusionNet (Huang et al., 2017):
the attention uses all previous layers to compute scores, but only linearly combines one RNN layer
output.
In detail, we conduct K + 1 times of multilevel inter-attention from each RNN layer output of
question to context {mi(k),C}im=1 = Attn({HoWiC}im=1, {HoWiQ}in=1, {hiQ,k}in=1), 1 ≤ k ≤ K + 1,
where HoW is the history-of-word vector:
HoWiC = [GloVe(wiC); BERTwiC; hiC,1;..., hiC,k],	(12)
An additional RNN layer is added to context C :
yiC = [hiC,1; ...; hiC,k; mi(1),C; ...;mi(K+1),C],	(13)
v1C,..., vmC =RNN(y1C,...,ymC)	(14)
Self Attention on the Context. Similar to questions, SDNet applies self-attention to the context.
Again, it uses the history-of-word concept to reduce the output dimensionality:
siC = [HoWiC;mi(1),Q;...;mi(K+1),Q;viC],	(15)
{VC }m=1 = Attn({sC }m=ι, {sC }Nι, {vC }m=ι).	(16)
The self-attention is followed by an additional RNN layer to generate the final representation of
context words:
{uC}m=ι = RNN([vC; VC],…,[vm; Vm])	(17)
2.5	Output layer
Question Condensation. The question is condensed into a single representation vector:
uQ =	βiuiQ ,
i
βi H exp (WTuQ),
(18)
(19)
where w is a trainable vector.
Generating answer span. As SDNet outputs answers of interval forms, the output layer generates
the probability that the answer starts and ends at the i-th context word, 1 ≤ i ≤ m:
PiS H exp ((uQ)T WS uiC),	(20)
tQ = GRU(uQ,XPiSuiC),	(21)
i
PiE H exp ((tQ)TWEuiC),	(22)
where WS, WE are parameters. The use of GRU is to transfer information from start position to end
position computation.
5
Under review as a conference paper at ICLR 2020
Table 1: Domain distribution in CoQA dataset.
Domain	#Passage	#QA turn
Child Story	750	14.0
Literature	1,815	15.6
Mid/High Sc.	1,911	15.0
News	1,902	15.1
Wikipedia	1,821	15.4
Out of domain		
Science	100	15.3
Reddit	100	16.6
Total	8,399	15.2
Special answer types. SDNet can also output special types of answer, such as affirmation “yes”,
negation “no” or no answer “unknown”. We separately generate the probabilities of these three
answers: PY , PN, PU. For instance, the probability that the answer is “yes”, PY , is computed as:
PY H exp((uQ)TWγUC)	(23)
PY = (XPiYuiC)TwY	(24)
i
where Wγ and wγ are parametrized matrix and vector, respectively.
2.6	Training and Inference
During training, all rounds of questions and answers for the same passage form a batch. The goal
is to maximize the probability of the ground-truth answer, including span start/end position, affir-
mation, negation and no-answer situations. Therefore, we minimize the cross-entropy loss function
L:
L= -	IkS(log(PiSsk)+log(PiEek))+IkγlogPkγ+IkNlogPkN+IkUlogPkU,	(25)
k
where isk and iek are the ground-truth span start and end position for the k-th question. IkS, Ikγ, IkN, IkU
indicate whether the k-th ground-truth answer is a passage span, “yes”, “no” and “unknown”, re-
spectively.
During inference, we pick the largest span/yes/no/unknown probability. The span is constrained to
have a maximum length of 15.
3	Experiments
We evaluated our model on CoQA (Reddy et al., 2018), a large-scale conversational question an-
swering dataset. In CoQA, many questions require understanding of both the passage and previous
rounds of questions and answers, which poses challenge to conventional machine reading models.
Table 1 summarizes the domain distribution in CoQA. As shown, CoQA contains passages from
multiple domains, and the average number of question answering turns is more than 15 per passage.
For each in-domain dataset, 100 passages are in the development set, and 100 passages are in the
test set. The rest in-domain dataset are in the training set. The test set also includes all of the
out-of-domain passages.
3.1	Implementation Details
We use spaCy for word tokenization and employ the uncased BERT-large model to generate contex-
tual embedding.
During training, we use a dropout rate of 0.4 for BERT layer outputs and 0.3 for other layers. We use
Adamax (Kingma & Ba, 2014) as the optimizer, with a learning rate of α = 0.002, β = (0.9, 0.999)
and = 10-8. We train the model for 30 epochs. The gradient is clipped at 10.
6
Under review as a conference paper at ICLR 2020
Table 2: Model and human performance (% in F1 score) on the CoQA test set.
I Child. Liter. Mid-High. NeWs Wiki Reddit Science ∣ Overall
PGNet	49.0	43.3	47.5	47.5	45.1	38.6	38.1	44.1
DrQA	46.7	53.9	54.1	57.8	59.4	45.0	51.0	52.6
DrQA+PGNet	64.2	63.7	67.1	68.3	71.4	57.8	63.1	65.1
BiDAF++	66.5	65.7	70.2	71.6	72.6	60.8	67.1	67.8
FloWQA	73.7	71.6	76.8	79.0	80.2	67.8	76.1	75.0
SDNet (single)	75.4	73.9	77.1	80.3	83.1	69.8	76.8	76.6
SDNet (ensemble)	78.7	77.1	80.2	81.9	85.2	72.3	79.7	79.3
Human	90.2	88.4	89.8	88.6	89.9	86.7	88.1	88.8
The word-level attention has a hidden size of 300. The self attention layer for question words has
a hidden size of 300. The RNNs for question and context have K = 2 layers and each layer has a
hidden size of 125. The multilevel attention from question to context has a hidden size of 250. The
self attention layer for context has a hidden size of 250. The final RNN layer for context Words has
a hidden size of 125.
3.2	Baseline Models and Results
We compare SDNet2 With the folloWing baseline models: DrQA+PGNet (Reddy et al., 2018),
BiDAF++ (Yatskar, 2018) and FloWQA (Huang et al., 2018). Aligned With the official leaderboard,
We use F1 as the evaluation metric, Which is the harmonic mean of precision and recall at Word level
betWeen the predicted ansWer and ground truth.3
Table 2 shoWs the performance of SDNet and baseline models.4 As shoWn, SDNet achieves signifi-
cantly better results than baseline models. In detail, the single SDNet model improves overall F1 by
1.6%, compared With previous state-of-art model on CoQA, FloWQA. We also trained an ensemble
model consisting of 12 SDNet models With the same structure but different random seeds for ini-
tialization. The ensemble model uses the ansWer from the most number of models as its predicted
ansWer. Ensemble SDNet model further improves overall F1 score by 2.7%.
Figure 3 shoWs the F1 score of SDNet on development set during training. As seen, SDNet over-
passes all but one baseline models after the second epoch, and achieves state-of-the-art results after
8 epochs.
Ablation Studies. We conduct ablation studies on SDNet to verify the effectiveness of different
parts of the model. As Table 3 shoWs, our proposed Weighted sum of per-layer output from BERT
is crucial, boosting the performance by 1.75% compared With the canonical method of using only
the last layer’s output. This shoWs that the output from each layer in BERT is useful in doWn-
stream tasks. Using BERT-base instead of the BERT-large pretrained model hurts the F1 score by
2.61%.Variational dropout and self attention can each improve the performance by 0.24% and 0.75%
respectively.
Contextual history. In SDNet, We utilize conversation history via prepending the current question
With previous N rounds of questions and ground-truth ansWers. We experimented With the effect of
N and present the result in Table 4.
As shoWn, excluding dialogue history (N = 0) can reduce the F1 score by as much as 8.56%,
manifesting the importance of contextual information in conversational QA task. The performance
of our model peaks When N = 2, Which Was used in the final SDNet model.
2We have open-sourced our SDNet code. For anonymity, We Will share the link afterWards.
3According to official evaluation of CoQA, When there are more than one ground-truth ansWers, the final
score is the average of max F1 against all-but-one ground-truth ansWers.
4Result Was taken from official CoQA leaderboard on Nov. 30, 2018.
7
Under review as a conference paper at ICLR 2020
Figure 3: F1 score on CoQA dev set over training epochs. Note that for BERT-base model, we use
the number on test set from the leaderboard.
Table 3: Ablation study of SDNet on CoQA development dataset.
Model	F1
SDNet	77.99
-Variational dropout	77.75
-Question self attention	77.24
Using last layer of BERT output (no weighted sum)	76.24
BERT-base	75.38
Table 4: Performance of SDNet on development set when prepending different number of rounds of
history questions and answers to the question. The model uses BERT-Large contextual embedding
and fixes BERT’s weights.
#previous QA rounds N	F1
0	69.43
1	76.70
2	77.99
3	77.39
4 Conclusions
In this paper, we propose a novel contextual attention-based deep neural network, SDNet, to tackle
the conversational question answering task. By leveraging inter-attention and self-attention on pas-
sage and conversation history, the model is able to comprehend dialogue flow and the passage.
Furthermore, we leverage the latest breakthrough in NLP, BERT, as a contextual embedder. We de-
sign the alignment of tokenizers, linear combination and weight-locking techniques to adapt BERT
into our model in a computation-efficient way. SDNet achieves superior results over previous ap-
proaches. On the public dataset CoQA, SDNet outperforms previous state-of-the-art model by 1.6%
in overall F1 score and the ensemble model further improves the F1 by 2.7%.
Our future work is to apply this model to open-domain multiturn QA problem with large corpus or
knowledge base, where the target passage may not be directly available. This will be a more realistic
setting to human question answering.
8
Under review as a conference paper at ICLR 2020
References
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-
domain questions. arXiv preprint arXiv:1704.00051, 2017.
Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke
Zettlemoyer. Quac: Question answering in context. arXiv preprint arXiv:1808.07036, 2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Ahmed Elgohary, Chen Zhao, and Jordan Boyd-Graber. A dataset and baselines for sequential
open-domain question answering. In Proceedings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pp. 1077-1083, 2018.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Hsin-Yuan Huang, Chenguang Zhu, Yelong Shen, and Weizhu Chen. Fusionnet: Fusing via fully-
aware attention with aPPlication to machine comPrehension. arXiv preprint arXiv:1711.07341,
2017.
Hsin-Yuan Huang, Eunsol Choi, and Wen-tau Yih. Flowqa: GrasPing flow in history for conversa-
tional machine comPrehension. arXiv preprint arXiv:1810.06683, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma, Tim Salimans, and Max Welling. Variational droPout and the local reParame-
terization trick. In Advances in Neural Information Processing Systems, PP. 2575-2583, 2015.
Xiaodong Liu, Yelong Shen, Kevin Duh, and Jianfeng Gao. Stochastic answer networks for machine
reading comPrehension. arXiv preprint arXiv:1712.03556, 2017.
Jeffrey Pennington, Richard Socher, and ChristoPher Manning. Glove: Global vectors for word
rePresentation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), PP. 1532-1543, 2014.
Siva Reddy, Danqi Chen, and ChristoPher D Manning. Coqa: A conversational question answering
challenge. arXiv preprint arXiv:1808.07042, 2018.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. arXiv preprint arXiv:1508.07909, 2015.
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention
flow for machine comPrehension. arXiv preprint arXiv:1611.01603, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. pp. 5998-6008, 2017.
Mark Yatskar. A qualitative comParison of coqa, squad 2.0 and quac. arXiv preprint
arXiv:1809.10735, 2018.
9