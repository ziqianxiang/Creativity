Under review as a conference paper at ICLR 2020
Adapt-to-Learn:	Policy Transfer in Rein-
forcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Efficient and robust policy transfer remains a key challenge in reinforcement
learning. Policy transfer through warm initialization, imitation, or interacting over
a large set of agents with randomized instances, have been commonly applied to
solve a variety of Reinforcement Learning (RL) tasks. However, this is far from
how behavior transfer happens in the biological world: Humans and animals are
able to quickly adapt the learned behaviors between similar tasks and learn new
skills when presented with new situations. Here we seek to answer the question:
Will learning to combine adaptation reward with environmental reward lead to a
more efficient transfer of policies between domains? We introduce a principled
mechanism that can “Adapt-to-Learn”, that is adapt the source policy to learn
to solve a target task with significant transition differences and uncertainties. We
show through theory and experiments that our method leads to a significantly re-
duced sample complexity of transferring the policies between the tasks.
1	Introduction
Lack of principled mechanisms to quickly and efficiently transfer policies learned between domains
has become the major bottleneck in Reinforcement Learning (RL). This inability to transfer or adapt
policies is one major reason why RL has still not proliferated physical application like robotics.
Since RL agents cannot quickly transfer policies, the agent is forced to learn every task from scratch,
which is both time and sample expensive. Warm-start, a method in which weights from one neural
network are transferred to another, has been reasonably successful for supervised learning. However,
this method can often lead to mixed and even negative results in RL (Joshi & Chowdhary, 2018;
Taylor & Stone, 2009).
Our main contribution is an algorithm to transfer policies between tasks with significant differences
in state transitions via a policy adaptation mechanism. Unlike the majority of existing work in trans-
fer learning for RL, our approach does not merely use the transferred policy to warm start (initialize
the parameter of the target network with learned source network) policy learning in the target do-
main. Neither does it rely on a multitude of simulations across randomly generated source domains.
Instead, we combine supervised reference trajectory tracking and unsupervised reinforcement learn-
ing to adapt the source policy to the target domain directly. We show through theory and experiments
that our method enjoys significantly reduced sample complexity in solving the task.
Adapt-to-Learn is inspired by the fact that combined adaptation of behaviors and learning through
experience is a primary mechanism of learning in biological creatures (Krakauer & Mazzoni, 2011;
Fryling et al., 2011).Inspired by this ability of biological creatures, we seek to answer the question:
Will learning to combine intrinsic adaptation reward with environment reward lead to more efficient
transfer of policies between domains? Imitation Learning (IL) (Duan et al., 2017; Zhu et al., 2018)
seems to play a crucial part in biological learning, and as such has been widely studied in RL. How-
ever, the key is, when presented with a new situation, animals do not just imitate, but quickly adapt
existing behaviors, and improve them through further experience. In particular, an animal learning
to walk on a different terrain does not just imitate its existing gait, but adapts it to the new environ-
ment.The theory behind such adaptation in reference tracking control problems has been typically
restricted to deterministic dynamical systems with well-defined reference trajectories (Astrom &
Wittenmark, 2013; Chowdhary et al., 2013). This ability to adapt and incorporate further learning
through optimization on the environment reward is one key difference between our method and ex-
1
Under review as a conference paper at ICLR 2020
isting imitation learning and Guided Policy Search (GPS) methods (Levine & Koltun, 2013). Unlike
IL and GPS, our method transfers policies between task with significant differences in the transition
models. Moreover, by mixing environment reward with intrinsic adaptation rewards, we ensure that
the agent quickly adapts and also learns to acquire skills beyond what the source policy can teach.
We posit that the presented method can be the foundation of a broader class of RL algorithms that
can choose seamlessly between learning through RL to supervised adaptive imitation. Our empir-
ical results show that approach is capable of robustly transferring policies between tasks, even in
the presence of nonlinear and time-varying differences in the dynamic model of the systems. In
particular, we show that it suffices to execute adapted greedy policies to ensure -optimal behavior
in the target domain.
Related work: D-RL has recently enabled agents to learn policies for complex robotic tasks in
simulation (Peng et al., 2016; 2017b; Liu & Hodgins, 2017; Heess et al., 2017). However, D-RL
has been plagued by the curse of sample complexity. Therefore, the capabilities demonstrated in the
simulated environment are hard to replicate in the real world. This learning inefficiency of RL has
led to significant work in the field of TL (Taylor & Stone, 2009). A significant body of literature
on transfer in RL is focused on initialized RL in the target domain using learned source policy;
known as jump-start/warm-start methods (Taylor et al., 2005; Ammar et al., 2012; 2015). Some
examples of these transfer architectures include transfer between similar tasks (Banerjee & Stone,
2007), transfer from human demonstrations (Peters & Schaal, 2006) and transfer from simulation
to real (Peng et al., 2017a; Ross et al., 2011; Yan et al., 2017). Efforts have also been made in
exploring accelerated learning directly on real robots, through Guided Policy Search (GPS) (Levine
et al., 2015) and parallelizing the training across multiple agents using meta-learning (Levine et al.,
2016; Nagabandi et al., 2018; Zhu et al., 2018). Sim-to-Real transfers have been widely adopted
in the recent works and can be viewed as a subset of same domain transfer problems. Daftry et al.
(Daftry et al., 2016) demonstrated the policy transfer for control of aerial vehicles across different
vehicle models and environments. Christiano et al. (Christiano et al., 2016) transferred policies
from simulation to real using an inverse dynamics model estimated interacting with the real robot.
Through learning over an adversarial loss, the agents are trained to achieve robust policies across
various environments (Wulfmeier et al., 2017). However, these and other reported architectures do
not necessarily lead to improved sample efficiency, handle relatively minor changes in the transition
model, and are even known to cause negative transfer. In contrast, our approach directly adapts
the source policies to target with significant transition model difference while interacting with the
environment. It enjoys empirically and rigorously proven sample efficiency guarantees of order
O(nH), depending polynomially on the horizon length “H”.
2	Preliminaries
Consider a finite horizon MDP defined as a tuple M = (S, A, P, R, ρ0 , γ), where S denote set of
continuous states; A is a set of continuous bounded actions, P : S × A × S → R+ is state transition
probability distribution of reaching s0 upon taking action a in s, ρ0 : S → R+ is the distribution
over initial states s0 and R : S × A → R+ is deterministic reward function and H be the finite
horizon of the problem.
Let ∏(a∣s) : S ×A→ [0,1] be stochastic policy over continuous state and action space. The action
from policy is a draw from this distribution a% 〜∏(a∕si). The agent,s goal is to find a policy π?
which maximize the total return. The total return starting from states so 〜ρo under a policy ∏ is
ηπ(s0) = Es0,a0,... X r(st, at)	(1)
where, so 〜ρo, at 〜∏(at∣st) and st+ι 〜p(st+ι∣st, at).
We formalize the underlying problem of policy transfer by considering a source and target MDP as
follows, MS = (S, A, P, R, ρo, γ)S, MT = (S, A, P, R, ρo, γ)T, each with its own state, action
space and transition model respectively. We will mainly focus on the problem of same domain
transfer in this paper, where the state and action space are analogous S(S) = S(T) = S ∈ Rm
and A(T) = A(S) = A ∈ Rk, but the source and target state transition models differ significantly
due to unmodeled dynamics or external environment interactions. Furthermore, an example of how
the method can extend to cross-domain transfer using manifold alignment (Joshi & Chowdhary,
2
Under review as a conference paper at ICLR 2020
2018; Wang & Mahadevan, 2009) is in the supplementary. Let ∏ be a parameterized optimal
stochastic policy for source MDP MS. The policy ∏* can be obtained using any available RL
methods (Sutton et al., 2000; Schulman et al., 2015; 2017). In this work, we have used the Proximal
Policy Optimization(PPO) (Schulman et al., 2017) algorithm to generate the source optimal policy.
We will use the following definition of the state value function V π defined under any policy π
Vπ(st)
= Eat ,st+1 ,at+1 ,... X r(si+t, ai+t)
The associated optimal value function fo source MDP MS is Vπ* for all s ∈ S. Let T : Rn → Rn
be the Bellman update operator defined as
(Tf)(s) = max r(s, a) + γ E	f (s0)	(2)
a∈A	s0 〜P (s,a)
Optimal value function Vπ* satisfies the Bellman Equation-(2) such that Vπ* (s) = TVπ* (s), ∀ s.
3	Algorithmic contribution: Transfer through Adaptation
3.1	Adaptation as a mechanism for policy transfer in RL
In this paper, we approach the problem of transfer learning for RL through adaptation of previously
learned policies to related tasks. Our approach to adaptation is by enabling the agent to learn the
best mixture of imitation and learning from environment reward.Our method differs from RL transfer
methods that rely on jump-starts or direct imitation (Zhu et al., 2018; Duan et al., 2017) in a key
way: We do not aim to emulate the source optimal policy itself in the target domain, but the source
transitions under optimal policy. To make this point, we demonstrate in our empirical results that
the source policy, when used directly, does not produce sensible behaviors of the agent in the target
domain when transition models are significantly different.On the other hand, our method uses the
source transitions projected onto the target task as reference exploration trajectories, helping to adapt
and optimize the source policy to the target domain efficiently.
In canonical reinforcement learning, the goal is to learn the optimal policy ∏* which maximizes
the future cumulative reward when the reward function and the transition function of the MDP are
unknown. In statistical RL, this leads to the (in)famous explore-exploit tradeoff.Yet, this learn-
through-experience approach has demonstrated significant potential to enable general solutions to
control of complex physical systems for which first principles-based models are not easily obtain-
able. However, one of the key challenges in RL has been the high sample complexity of obtaining a
reasonable policy. In other words, RL, and specifically Deep RL, require an absurdly huge amount
of interactions with the environment (experience) to find reasonable policies. This high learning
inefficiency is in apparent contrast with the efficient learning demonstrated by humans and many
biological creatures. For examples, octopuses are known to learn to solve complex problems such as
opening jars and squeezing through puzzles with just afew trials. This ability to quickly transfer and
adapt learned policies between related tasks is likely a key to general intelligence that seems to be
missing from RL. Our goal in the rest of this paper is to show that an algorithm that can judiciously
combine adaptation to changes and learning new skills is capable of avoiding brute force random
exploration to a large extent and be significantly less sample expensive.
3.2	Adapt-to -Learn: Policy transfer
We begin by mathematically describing adaptation for policy transfer in RL and state all the neces-
sary assumptions in Section 3.2.1. We then develop the Adapt-to-Learn algorithm in Section 3.2.2.
3.2.1	Adaptation
The proposed Adapt-to-Learn (ATL) algorithm objective is to accumulate higher total returns
through maximizing the likelihood of the target policies πθ, which emulate the optimal source transi-
tion behavior in the target MDP. We achieve this objective by minimizing the KL divergence between
point-wise local trajectories realized using the target policy and source optimal policy in the target
3
Under review as a conference paper at ICLR 2020
domain (Refer Figure-3 in Appendix-C). This adaptation objective can be formalized as minimizing
the KL-divergence between source and target transition trajectories:
ηκL(πθ,π*) =	E	(p∏θ(T)log (Ppn(T))) ,	(3)
st,at~τ ∖	∖qp*(t)))
where T is the trajectory in the target domain under the policy πθ defined as
T = (s0,a0,s1,a1,...),
and probability of the trajectory Pπθ (T) under policy πθ and target transition PT (.|st, at) can be
written as
H
P∏θ (τ) = ρ(so) U∏θ (at∣st)pT (st+ι∣st ,at).	(4)
t=0
Similarly qπ* (T) can be defined as probability of trajectory deviations at every state st ∈ T, when
the source optimal policy ∏ is used in place of target policy ∏, and the states evolve according to
the source transitions modelPS(.|st, at) (Refer Figure-3 in Appendix-C):
Assumption-1: We assume that optimal source policy is available, such that the source policy vari-
ance can be assumed to be zero. That is, the action probabilities ∏*(a]st) = 1, ∀st. This is a
reasonable assumption, since optimal source policy is available it can be treated as deterministic
policy and actions can be chosen greedily. We need this assumption only for deriving the expression
for intrinsic reward and theoretical analysis. In the empirical evaluation of the algorithm, we treat
∏*(.) as a stochastic policy.
The transition probabilities in the KL divergence in (3) term are treated as transition likelihoods and
the transitioned state st+1 as a random variable. Since s0t+1 is the optimal state reached starting in st
under source transition model using ∏* (at |st), We try to emulate this behavior in the target domain
and hence evaluate the transition likelihoods of the target trajectory at {s0i}iH=1. Using the definition
of the probabilities of the trajectories under ∏ and ∏ Equation-(4) & (5) the log term in the KL
divergence of the trajectory (3) is simplified as folloWs
log
ρ(s0)π(a0∣s0)pτ(s1∣s0, ao)π(aι∣sι)pτ(s2∣sι,αι) ...
ρ(so)π*(a0∣so)ps(s1∣s0, a0)n*(a；|si)ps⑸同，。；)...
(6)
Using (6) and with assumption-1 i.e. π*(at∣st) = 1∀st the KL term can simplified as follows
/*、 m (	/	∏πθ(atIst)PT(st+ιlst,at)
ηκLE,π ) = E	p∏θ(T)工log ---------sγo^^------------
st,at〜τ ∖	绘 ∖	PS (st+1lst,at)
ηκL (πθ ,π*) =	E	p∏)τ((τ (T) X q]	⑺
st,at~τ ∖	t=o )
where Zt = log ("'7^^:^,3)
If calculating the above expectation is feasible, it is possible to minimize the KL divergence and
move the policy parameters in the direction of emulating source transition behavior in the target
domain. However, this is not generally the case since the true expectation is intractable. Therefore a
common practice is to use an empirical estimate of the expectation to do approximate planning. We
minimize the trajectory KL divergence by handling the term ζt as the intrinsic adaptation reward,
which captures the local deviation in trajectory in the form of the shaped reward function.
Assumption-2: We do not assume to know the true transition distribution for source and the target,
but only have access to simulator model of the source. However, we assume both source and target
transition models follow a Gaussian distribution centered at the next propagated state and fixed
variance "σ”.
4
Under review as a conference paper at ICLR 2020
Assumption-2 is not very restrictive since we empirically show that for any deterministic model, a
bootstrapped Gaussian transition assumption is sufficient for ATL agent to learn the task. Using the
above assumption, we can approximate the KL term as
Zt = log 卜θ(at∣St)e-(st+1-st+1)2/2,2).	⑻
The individual terms in the expectation ζt represent the distance between two transition likelihoods
of landing in the next state s0t+1 starting in st and under actions at, a0t. The target agent is encouraged
to take actions that lead to states which are close in expectation to a reference state provided by an
optimal baseline policy operating on the source model. By doing so, we are providing a possible
direction of search for the higher environmental rewards “rt”.
We can, therefore, solve the following optimization problem to generate adaptive policy updates:
πθT = arg min (ηκL) .	⑼
πθ∈Π
The distance between two transition models ζt is used as an intrinsic reward to calculate the total
trajectory intrinsic return, and further, any policy update algorithm can be used (Sutton et al., 2000;
Schulman et al., 2017; 2015) to update the policy in direction optimizing this objective. However, it
is to be noted that though we use policy gradient kind of update for adaptive policy, the optimization
is more akin to supervised learning. For every state-action pair, the KL-distance is the true metric
to be minimized, and unlike RL, we do not engage in optimal value search in the adaptation part of
learning. Hence the algorithm is more sample efficient compared to any RL policy search methods.
3.2.2	Adaptation and Learning
We achieve Adaption and Learning simultaneously by augmenting environment reward rt with in-
trinsic reward ζt . By doing so, we achieve transferred policies which try to both optimize optimal
reference tracking and also maximize the cumulative future environmental reward to acquire skills
beyond what source can teach. This trade-off between learning by exploration and learning by
adaptation can be realized as follows:
ηKL = E pπθ (τ) X((1 - β)rt - βζt) .
(10)
Where the term β is the mixing coefficient. We make a heuristic choice for an appropriate β to start
with and annealed over episodes for optimal mixing of adaptation and learning. For consistency of
the reward mixing, the rewards rt, ζt are normalized to form the total reward rt0 = (1 - β)rt + βζt.
Algorithm 1 Adaptive-to-Learn Policy Transfer in RL
Require: ∏*(.),ps	. Inputs: Source Policy, source simulator
1:	Initialize s0T ∈ ρ0.	. Draw initial state from the given distribution in target task
2:	for i = 1 ≤ K do
3:	while si 6= terminal do
4:	ai 〜 ∏*(si)	. Generate the optimal action using Source policy
5:	ai 〜∏θ (Si)	. Generate the action using the ∏
6:	Si+ι 〜PT(si,ai)	. Propagate the target task model at state Si and action a%
7:	si+ι 〜PS(si, ai)	. Propagate the source task model at state Si and action ai
8:	Zt = ∏θ(ai∣Si)e-(si+1-si+1) 2/2 . Calculate the KL divergence intrinsic reward term
9:	Zi = ({Si, ri, ζi, ai, a0i})	. Incrementally store the trajectory for policy update
10:	Pzn(ηκL) = n Pn=I (PH=0 Vθ log ∏Θ@,小行) P∞=0 τi,) . Empirical loss to optimize
11:	∏θ = arg max∏∈∏ PZn (ηκL)	. Minimize the loss to obtain a adaptive policy
3.3	Sample-Based estimation of the gradient
The previous section proposed an optimization method to find the adaptive policy using KL-
divergence as an intrinsic reward, enforcing the target transition model to mimic the source tran-
sitions. This section describes how this objective can be approximated using a Monte Carlo simu-
lation. The adaptive policy update methods work by computing an estimator of the gradient of the
5
Under review as a conference paper at ICLR 2020
return and plugging it into a stochastic gradient ascent algorithm.
∏ΘT = arg maχ PZn Skl )	(11)
πθ ∈Π
θ 一 θ + αg.
Where α is the learning rate and g is the empirical estimate of the gradient (Refer APPendix-A) of
the total return ηKL . The gradient estimate over i.i.d data from the collected trajectories is computed
as follows:
XH∞
Vθ logπθ(at|St) Xrt),
1n H	∞
PZn(V θη KL) = n X (X vθ log πθ (ai,t|si,t) X ri,t),	(12)
where PZn is emPirical distribution over the data (Zn : {Si, ai, a0i}in).
4	Theoretical bounds on sample complexity
Although there is some emPirical evidence that transfer can imProve Performance in subsequent
reinforcement-learning tasks, there are not many theoretical guarantees. Since many of the existing
transfer algorithms aPProach the Problem of transfer as a method of Providing good initialization to
target task RL, we can exPect the samPle comPlexity of those algorithms to still be a function of the
cardinality of state-action Pairs |N| = |S| × |A|. On the other hand, in suPervised learning setting,
the theoretical guarantees of most algorithm have no dePendency on size (or dimensionality) of the
inPut domain (which is analogous to |N| in RL domains). Having formulated a Policy transfer algo-
rithm using labeled reference trajectories derived from oPtimal source Policy in ATL, we construct
suPervised learning like PAC ProPerty of the ProPosed method. For the samPle comPlexity analysis
we consider only the adaPtation Part of learning i.e. β = 1 in Equation-(16). This is because in ATL,
the adaPtive learning is akin to suPervised learning, since the source reference trajectories Provide
the target states given every (St, at) Pair.
SuPPose we are given the learning Problem sPecified with training set Zn = (Z1, . . . Zn) where
each Zi = ({Si, ai, a0i})iH=0 are indePendently drawn according to some distribution P. Given the
data Zn we can comPute the emPirical return PZn (ηKL ) for every πθ ∈ Π, and we will show that
the following holds:
kPZn(ηKL)-P(ηKL)k ≤	(13)
with Probability at least 1 - δ, for some very small δ s.t 0 ≤ δ ≤ 1. We can claim that the emPirical
return for all πθ is a sufficiently accurate estimate of the true return function. Thus a reasonable
learning strategy is to find a πθ ∈ Π that would minimize emPirical estimate of the objective (10).
Theorem 4.1 If the induced class of the policy ∏θ :Ln has uniform convergence property in empir-
ical mean; then the empirical risk minimization is PAC. s.t
Pn(P(ηκL∏) — P(ηκL,∏) ≥e) ≤ δ	(14)
and number of trajectory samples required can be lower bounded as
n(, δ) ≥
2H2C2
0
log
(15)
For the Proof of the above theorem refer the APPendix-B.
4.1	-OPTIMALITY RESULT UNDER ADAPTIVE TRANSFER-LEARNING
Consider MDP M* and M which differ in their transition models. For the sake of analysis, let
M* be the MDP with ideal transition model, such that target follows source transition p* precisely.
Let P be the transition model achieved (tracked) using the estimated adapted policy learned over
data interacting with the target model and the associated MDP be denoted as M. We analyze the
-optimality of return under adapted source optimal policy through proposed policy transfer.
6
Under review as a conference paper at ICLR 2020
Note for the sake of analysis we are deviating from finite horizon assumption and using discout factor
Y. To be consistent with previous section We can assume the horizon length H = log^ ( ^v-),
γ 2Vmax
where v is truncation error in total infinite return.
Definition 4.2 Given the ValuefUnction V * = Vπ* and model Mi and M2, which only differ in the
corresponding transition models p1 and p2. Lets define ∀s, a ∈ S × A
dM1,M2
sup
s,a∈S×A s0〜Pι(s,a)
E	[V*(s0)]-
E	[V*(s0)]
s0 〜P2(s,a)
Lemma 4.3 Given M*, M and value function VM：, VM the following bound holds
where maxs,a ∣∣p^(.∣s, a) - p*(.∣s, a)k ≤ E andP andp* are transition of MDP M, M* respectively.
The proof of this lemma is based on the simulation lemma (see Appendix-C). Similar results for RL
with imperfect models were reported by (Jiang, 2018).
5 Policy transfer in simulated rob otic locomotion tasks
To evaluate Adapt-to-Learn Policy Transfer in reinforcement learning, we design our experiments
using sets of tasks based on the continuous control environments in MuJoCo simulator (Todorov
et al., 2012). Our experimental results demonstrate that ATL can adapt to significant changes in
transition dynamics. Therefore, we perturb the parameters of the simulated target models for the
policy transfer experiments (see Table-1 for original and perturbed parameters of the target mode).
To create a challenging training environment, we changed the parameters of the model such that
the optimal source policy alone without any learning cannot produce any stable results (see source
policy performance in Figure-1). We compare our results against two baselines: (a) Initialized
Reinforcement learning (initialized PPO) (Jumpstart-RL (Wang & Mahadevan, 2009)) (b) stand-
alone reinforcement policy learning (PPO) (Schulman et al., 2017).
We experiment with the ATL algorithm on Hopper, Walker2d, and HalfCheetah Environments. The
states of the robots are their generalized positions and velocities, and the actions are joint torques.
High dimensionality, non-smooth dynamics due to contacts and being under-actuated systems make
these tasks very challenging. We use deep neural networks to represent the source and target pol-
icy, the details of which are in the Table-2 Appendix-D. The following models are included in our
evaluation:
Slippery Hopper: is defined through 11-dimensional state space and 3-dimension action space, with
reward function defined as r(t) = (st+1 - st)/dt - 10-3 ∣a∣2, and a bonus of +1 for being in a
non-terminal state. The simulation is terminated upon reaching 1000 steps or hopper toppling. The
target model differs in the floor friction and foot joint damping.
Slippery Fat-Walker2d: is defined through 17-dimensional state space and 6-dimension action
space, with reward function and termination condition defined same as Hopper. The target model
differs in the model density and floor friction.
Fat HalfCheetah: is defined through 17-dimensional state space and 6-dimension action space, with
reward function defined as r(t) = (st+1 - st)/dt - 0.1∣a∣2. The simulation is terminated upon
reaching 1000 steps. The target model differs in the floor friction coefficient, gravity, and mass.
To establish a standard baseline, we also included the classic cart-pole and Inverted pendulum bal-
ancing tasks, based on the formulation (Barto et al., 1983). We also demonstrate the cross-domain
transfer capabilities using a model-based variant of the proposed algorithm. The results of policy
transfer for Cart-Pole to Inverted Pendulum and Inverted Pendulum to Bicycle transfers are provided
in the Appendix-D. Learning curves showing the total reward averaged across three runs of each al-
gorithm are provided in Figure-1. Adapt-to Learn policy transfer solved all the three tasks, yielding
quicker learning compared to other baseline methods. These results provide empirical evidence of
our hypothesis. Using trajectory KL divergence as intrinsic adaptation reward to adapt source pol-
icy to the target, we achieve a more robust and sample efficient policy transfer between two tasks,
7
Under review as a conference paper at ICLR 2020
EnV	Property	source	Target	%Change
Hopper	Floor Friction	1.0	2.0	+100%
HalfCheetah	gravity	-9.81	-15	+52%
	Total Mass	14	35	+150%
	BaCk-Foot Damping	3.0	1.5	-100%
	Floor Friction	0.4	0.1	-75%
Walker2d	Density	1000	1500	+50%
	Right-Foot Friction	0.9	0.45	-50%
	Left-Foot Friction	1.9	1.0	-47.37%
Table 1: Transition Model and environment properties for Source and Target task and % change
compared to using a warm-start or standalone RL method. Note that the target domain perturba-
tions introduced are significant enough such that source policy alone without any adaptation in the
target domain produced no meaningful results (Figure-1). This notion of adaptation in the face of
uncertainty is a key advancement over traditional policy transfer, meta-learning, or adversarial RL
methods aiming to improve performance by learning a policy over a set of lightly perturbed tasks.
(a)	(b)
(c)
Figure 1:	Learning curves for locomotion tasks, averaged across three runs of each algorithm with
random Initialization for RL, warm initialization using source policy for ATL and Jumpstart(Warm-
Start) methods and Source policy performance in Target Task without any adaptation.
(a)	(b)	(c)
Figure 2:	Trajectory KL divergence Total Intrinsic Return -	eζt averaged across three runs.
6 Conclusion
We introduced a new transfer learning technique for RL: Adapt-to-Learn, that utilizes adaptation of
the source policy to target tasks. We demonstrated on nonlinear and continuous robotic locomotion
tasks that learning to adapt source policy to the target domain leads to a significant reduction in
sample complexity over the prevalent jump-start based approaches. We further also proved theo-
retical guarantees on the reduced sample complexity of our proposed architecture. There are many
exciting directions for future work. A network of policies that can generalize across multiple tasks
could be learned based on each new adapted policies. How to train this end-to-end is an important
question for meta-learning. The ability of Adapt-to-Learn to handle significant perturbations to the
8
Under review as a conference paper at ICLR 2020
transition model indicates that it should naturally extend to sim-to-real transfer. Indeed we argue that
such adaptation is necessary for real-world robotics, as has been established previously in classical
domains like flight control. Another exciting direction is to extend the work to other combinatorial
domains (e.g., multiplayer games). We expect, therefore follow on work will find other exciting
ways of exploiting such adaptation in RL and machine learning.
References
Haitham B Ammar, Karl Tuyls, Matthew E Taylor, Kurt Driessens, and Gerhard Weiss. Reinforce-
ment learning transfer via sparse coding. In Proceedings of the 11th International Conference on
Autonomous Agents and Multiagent Systems-Volume 1, pp. 383-390. International Foundation for
Autonomous Agents and Multiagent Systems, 2012.
Haitham Bou Ammar, Eric Eaton, Paul Ruvolo, and Matthew E Taylor. Unsupervised cross-domain
transfer in policy gradient reinforcement learning via manifold alignment. In Proc. of AAAI, 2015.
Karl J Astrom and Bjorn Wittenmark. Adaptive control. Courier Corporation, 2013.
Karl J Astrom, Richard E Klein, and Anders Lennartsson. Bicycle dynamics and control. IEEE
Control Systems Magazine, 25(4):26-47, 2005.
Bikramjit Banerjee and Peter Stone. General game learning using knowledge transfer. In IJCAI, pp.
672-677, 2007.
Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can
solve difficult learning control problems. IEEE transactions on systems, man, and cybernetics,
(5):834-846, 1983.
Girish Chowdhary, Tongbin Wu, Mark Cutler, and Jonathan P How. Rapid transfer of controllers
between uavs using learning-based adaptive control. In Robotics and Automation (ICRA), 2013
IEEE International Conference on, pp. 5409-5416. IEEE, 2013.
Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor Blackwell, Joshua Tobin, Pieter
Abbeel, and Wojciech Zaremba. Transfer from simulation to real world through learning deep
inverse dynamics model. arXiv preprint arXiv:1610.03518, 2016.
Shreyansh Daftry, J Andrew Bagnell, and Martial Hebert. Learning transferable policies for monoc-
ular reactive mav control. In International Symposium on Experimental Robotics, pp. 3-11.
Springer, 2016.
Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya
Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. In Advances
in neural information processing systems, pp. 1087-1098, 2017.
Mitch J Fryling, Cristin Johnston, and Linda J Hayes. Understanding observational learning: An
interbehavioral approach. The Analysis of Verbal Behavior, 27(1):191-203, 2011.
Nicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez,
Ziyu Wang, Ali Eslami, Martin Riedmiller, et al. Emergence of locomotion behaviours in rich
environments. arXiv preprint arXiv:1707.02286, 2017.
Nan Jiang. Pac reinforcement learning with an imperfect model. In Proc. of AAAI, 2018.
Girish Joshi and Girish Chowdhary. Cross-domain transfer in reinforcement learning using target
apprentice. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pp.
7525-7532. IEEE, 2018.
John W Krakauer and Pietro Mazzoni. Human sensorimotor learning: adaptation, skill, and beyond.
Current opinion in neurobiology, 21(4):636-644, 2011.
Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on Machine
Learning, pp. 1-9, 2013.
9
Under review as a conference paper at ICLR 2020
Sergey Levine, Nolan Wagener, and Pieter Abbeel. Learning contact-rich manipulation skills with
guided policy search. In Robotics and Automation (ICRA), 2015 IEEE International Conference
on,pp.156-163.IEEE, 2015.
Sergey Levine, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen. Learning hand-eye coordination
for robotic grasping with large-scale data collection. In International Symposium on Experimental
Robotics, pp. l73-184. Springer, 2016.
Libin Liu and Jessica Hodgins. Learning to schedule control fragments for physics-based characters
using deep q-learning. ACM Transactions on Graphics (TOG), 36(3):29, 2017.
Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics
for model-based deep reinforcement learning with model-free fine-tuning. In 2018 IEEE Interna-
tional Conference on Robotics and Automation (ICRA), pp. 7559-7566. IEEE, 2018.
Xue Bin Peng, Glen Berseth, and Michiel Van de Panne. Terrain-adaptive locomotion skills using
deep reinforcement learning. ACM Transactions on Graphics (TOG), 35(4):81, 2016.
Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer
of robotic control with dynamics randomization. arXiv preprint arXiv:1710.06537, 2017a.
Xue Bin Peng, Glen Berseth, KangKang Yin, and Michiel Van De Panne. Deeploco: Dynamic
locomotion skills using hierarchical deep reinforcement learning. ACM Transactions on Graphics
(TOG), 36(4):41, 2017b.
Jan Peters and Stefan Schaal. Policy gradient methods for robotics. In Intelligent Robots and
Systems, 2006 IEEE/RSJ International Conference on, pp. 2219-2225. IEEE, 2006.
Jette Randlov and Preben Alstrom. Learning to drive a bicycle using reinforcement learning and
shaping. In Proceedings of the Fifteenth International Conference on Machine Learning, pp.
463-471, 1998.
Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and StrUc-
tured prediction to no-regret online learning. In Proceedings of the fourteenth international con-
ference on artificial intelligence and statistics, pp. 627-635, 2011.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural informa-
tion processing systems, pp. 1057-1063, 2000.
Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey.
Journal of Machine Learning Research, 10(Jul):1633-1685, 2009.
Matthew E Taylor, Peter Stone, and Yaxin Liu. Value functions for rl-based behavior transfer: A
comparative study. In Proceedings of the National Conference on Artificial Intelligence, vol-
ume 20, pp. 880. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999,
2005.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Chang Wang and Sridhar Mahadevan. Manifold alignment without correspondence. In IJCAI,
volume 2, pp. 3, 2009.
Markus Wulfmeier, Ingmar Posner, and Pieter Abbeel. Mutual alignment transfer learning. arXiv
preprint arXiv:1707.07907, 2017.
10
Under review as a conference paper at ICLR 2020
Mengyuan Yan, Iuri Frosio, Stephen Tyree, and Jan Kautz. Sim-to-real transfer of accurate grasping
with eye-in-hand observations and continuous control. arXiv preprint arXiv:1712.03303, 2017.
Henry Zhu, Abhishek Gupta, Aravind Rajeswaran, Sergey Levine, and Vikash Kumar. Dexterous
manipulation with deep reinforcement learning: Efficient, general, and low-cost. arXiv preprint
arXiv:1810.06045, 2018.
11
Under review as a conference paper at ICLR 2020
A Total return gradient with respect to policy parameters
The total return which we aim to maximize in adapting the source policy to target is the mixture of
environmental rewards and Intrinsic KL divergence reward as follows,
ηκL (πθ ,π*) = E	(P∏θ (T) X rt)	(16)
Skaat5 ∖	t=0 )
he adaptive policy update methods work by computing an estimator of the gradient of the return and
plugging it into a stochastic gradient ascent algorithm.
∏θτ = arg maχ PZn Wkl )	(17)
πθ ∈Π
θ = θ + αg
where α is the learning rate and g is the empirical estimate of the gradient of the total discounted
return ηKL . The derivation of the gradient term is exactly same as for policy gradient methods.
Taking the derivative of the total return term
Ve (η
KL ) = E	Vθpπθ(τ )Xrt0 .
st,attτ	t=0
Consider the term Vθ pπθ (τ ) and evaluate the derivative. Lets consider the term log pπθ (τ ). Using
the definition of pπθ (τ ) and we can write the log term as
logP∏θ(T) = log(ρo∏θ(ao|S0)PT(sι∣S0,ao) ...)
HH
log P∏θ (T) = log(ρo) + £log ∏θ (at∣st) + EpT (st+ι∣st ,at).
Taking the derivative wrto θ we can write,
H
Ve logp∏θ(τ) = £ Ve log∏θ(at∣st)
t=0
H
θp∏θ : = X Ve log∏e(at∣st).
pπθ (T)	t=0
Therefore we can write
H
Vep∏θ(τ) = p∏θ(τ) E Ve log∏e(at∣st)
t=0
Substituting the above expression we can write the total gradient as,
XHH
Ve log∏e(at∣st) XTr
B Proof of Theorem-4. 1
Theorem B.1 If the induced class LΠ has uniform convergence in empirical mean property then
empirical risk minimization is PAC.
For notation simplicity we drop the superscript T (for Target domain) and subscript θ (policy pa-
rameters) in further analysis. Unless stated We will using following simplifications ∏* = ∏:T and
一*	*T
π* = πe*T.
12
Under review as a conference paper at ICLR 2020
Proof Fix , δ > 0 we will show that for sufficiently large n ≥ n(, δ)
Prn(P(ηκL,∏*) - P(ηκL∏) ≥e) ≤ δ	(18)
Let ∏* ∈ Π be the minimizer of true return P(n5), further adding and subtracting the terms
PZn (nκL,∏*) and PZn (nκL,π*) We can write
P((IKL∏ ) - P((IKL,π* ) =
P(nKL,n* ) - PZn 5κL,Π* ) + PZn (nKL,n* ) - PZn (nKL,n* ) + PZn 5kl,∏* ) - P(nKL,n* )
(19)
We can divide the above expression in three terms
1.	P5κL,∏* ) - PZn (nKL,n* )
2.	PZn (nKL,π* ) - PZn 5κL,π* )
3.	PZn (ηKL,π*) - P(ηKL,π*)
Lets consider the term PZn (n5 ∏*) 一PZn 5kl π*) in the above expression is always negative semi-
definite, since ∏* is a maximizer wrto PZn (n5), hence PZn (n5 ∏*) ≤ PZn (n5 π*) always, i.e
PZn (nKL,n* ) - PZn (nKL,∏* ) ≤ 0
Next the 1st term can be bounded as
P(nKL,n* ) - PZn (nκL,∏* ) ≤ suP[PZn SKL ) - PmKL )] ≤ SUp ||PZn (脸)- PmKL )||
π∈Π	π∈Π
Similarly upper bound can be written for the 3rd term Therefore we can upper bound the above
expression as
P(nKL,∏*) - P(nKL,∏*) ≤ 2sup kPZn (nKL) - Pg。) k	(20)
π∈Π
From Equation-(18) we have
SUp kPZn(nKL) - P(nKL )k≥e∕2	(21)
π∈Π
Using McDiarmids inequality and union bound, we can state the probability of this event as
n2
P n(kPZn (nKL) - P(nKL )k ≥e∕2) ≤ 2∣∏∣e-2C2H2	(22)
where C is bound on state space S andH is horizon length. The finite difference bound C is obtained
assuming transition pT , pS follow a Gaussian distributions and the i.i.d trajectories collected are of
maximum H length.
Equating the RHS of the expression to δ and solving for n we get
n(e,δ) ≥ 2H2C2 log (芈)	(23)
forn ≥ n(, δ) the probability of receiving abad sample is less than δ. The total number of observed
transition is of order O(nH). Refer supplementary document for details of the proof.
C Proof of Lemma-4. 3
and value function VMπ** ,
Lemma C.1 Given M*, M
VM the following bound holds
Here maxs,a ∣∣p(∙∣s, a) -p*(∙∣s, a)k ≤ E andP andp* are transition of MDP M, M* respectively.
Note we are deviating from finite horizon assumption and using γ . To be consistent with previous
section we can assume the horizon length H = logγ (2Vlv-). Where Ev is truncation error in total
infinite return.
13
Under review as a conference paper at ICLR 2020
∞
Add and subtract the term Y (p*(s0∣s, a), VM (s0))
=∣γ	Dp(s0∣s,	a),	VMM (s0)E	- Y (P*(S0∣S, a), VM (s0))+ γ	(p*(s0∣s, a), VM (s1)E	- γ	(p*(s0∣s,	a), VM** (s0))∣∞
≤	Yl	Dp(SlS,	a),	VMM (s0))	- (p*(s0∣s, a), VMM (s0)) | + γ∣	(p*(s0∣s, a), VMM (s0))	- γ	(p*(s0∣s,	a), VM** (s0))	∣∞
≤ YIp(Sls,a) -p*(SlS,a)l∞lVM*(SO)l∞ + YIVM*(S)- VM**(S)l∞
Using the definition of in above expression, we can write
∣VM* (s) - VM** (s)∣∞ ≤ YeIVM* (s0)∣∞ + YIVM* (s) - VM** (s)∣∞
Therefore
*	*	YeIVπ* (s0)I∞
IVM (s) - VM*(s)I∞ ≤ M Y
Now We solve for expression IVM* (s0) I∞. We know that this term is bounded as
IVM* (s0)∣∞ ≤ R-ax
where Rmax = 1, therefore we can write the complete expression as
γ
≤ (1-γ)
2
|VMM (S)- VMM * (S)l∞
Figure 3: Target Trajectory under policy πθ and local trajectory deviation produced by source opti-
mal policy π* and source transition PS
D Additional Results and Details of Experimental Domain
To establish a standard baseline, we also include the results for classic cart-pole and Inverted pen-
dulum balancing tasks, based on the formulation Barto et al. (1983). We also demonstrate the cross-
domain transfer capabilities using a variant of the proposed algorithm Joshi & Chowdhary (2018)
and the results for Cart-Pole to Inverted Pendulum and Inverted Pendulum to Bicycle transfers are
provided in the following sections.
14
Under review as a conference paper at ICLR 2020
	Hopper	Walker2d	HalfCheetah
State Space	12	18	17
Control Space	3	6	6
Number of layers	3	3	3
Layer Activations	tanh	tanh	tanh
Total num. of network params	10530	28320	26250
Discount	0.995	0.995	0.995
Learning rate	1.5×10-5	8.7×10-6	9×10-6
β init	1.0	1.0	1.0
β-anneal coeff	0.998	0.998	0.998
Batch size	20	20	5
Policy Iter	3000	5000	1500
Table 2: Policy Network details and Network learning parameter details
Figure 4: Policy Transfer from Inverted Pendulum to Non-stationary Inverted pendulum: (a) Aver-
age Rewards and (b) Training length, TA-TL(ATL ours), UMA-TL(Jumpstart-RL) and Stand-alone
RL
D.1 Inverted Pendulum (IP) to time-varying IP
We demonstrate our approach for a continuous state domain, Inverted Pendulum (IP) swing-up and
balance Figure-4. The source task is the conventional IP domain. The target task differs from the
source task in the transition model. The target task is a non-stationary inverted pendulum, where the
length and mass of the pendulum are continuously time varying with function Li = L0 + 0.5cos( ∏0i)
and Mi = M0 + 0.5cos(5∏i∙), where L0 = 1, M0 = 1 and i = 1 ...N. The state variables
describing the system are angle and angular velocity {θ, θ} ∈ [-π, π]. The RL objective is to
swing-up and balance the pendulum upright such that θ = 0, θ = 0. The reward function is selected
as R(θ,θ) = -10∣θ∣2 - 5∣θ∣2, which yields maximum value at upright position and minimum at the
down-most position. The continuous action space is bounded by T ∈ [-1, 1]. Note that the domain
is tricky, since full throttle action is assumed to not generate enough torque to be able to swing the
pendulum to the upright position, hence, the agent must learn to swing the pendulum back and forth
and leverages angular momentum to go to the upright position.
D.2 Robustness to negative transfer
We demonstrate that the proposed transfer is robust to negative transfers, that is, cases in trans-
fer learning where naive initialization of the target using source policy could be detrimental. We
demonstrate this through an inverted pendulum upright balance task with the sign of control flipped.
That is, we use an inverted pendulum model as both source and target systems, but the target is the
inverse of the source model with the sign of the control action flipped. It should be noted that deal-
ing with a change of sign in control has been considered a highly challenging problem in adaptive
control Chowdhary et al. (2013). We demonstrate that ATL is immune to negative transfers. Figure
15
Under review as a conference paper at ICLR 2020
Figure 5: Negative Transfer Inverted Pendulum: (a) Average Reward and (b) Training length
(a)	(b)
Figure 6: Adap-to-Learn Policy Transfer (a) Inverted-Pendulum (b) Cart-Pole
5a and 5b demonstrate convergence to average maximum reward with lesser training samples for
proposed ATL method compared to Initialized-RL(UMA-TL) and standalone-RL methods. It is to
be observed that UMA-TL method suffers from a negative transfer phenomenon. The agent under
UMA-TL converges to a much lower average reward by getting stuck in local minima and never
achieves the upright balance of the pendulum.
Also, the samples observed by UMA-TL in learning the task is much higher compared to no transfer
(RL) and proposed ATL methods. If the source and target are not sufficiently related or the features
of source task do not correspond to the target, the transfer may not improve or even decrease the
performance in target task leading to negative transfer. We show that the UMA-TL suffers from
a negative transfer in these results, whereas the performance of presented ATL is much superior
compared to UMA-TL and RL(learning from scratch).
E Cross Domain Transfer
E.1 Cart-Pole to Bicycle:
Bicycle balancing is a challenging physical problem, especially when the bicycle velocity is below
the critical velocity Vc = 4m/s to 5m/s. We set the bicycle velocity to be V < Vc i.e., V =
2.778m/s such that the bicycle becomes unstable, and active control is required to maintain stability.
The simulation itself is very high fidelity and realistic, which was designed for studying the physics
of the bicycle.
16
Under review as a conference paper at ICLR 2020
(a)
(b)
Figure 7: (a) Cart-Pole and Bicycle Domain (b)Average Rewards for TA-TL (ours), UMA-TL (jump-
start) and RL
(a)
(b)
Figure 8: Policy Transfer from Cart-Pole to Bike Balancing: (a) Total simulation time (in seconds)
the agent was able to balance the bike in training (b) Total time required to solve the task for TA-TL
(ours), UMA-TL (jumpstart) and RL
The states of the bicycle task are angle and angular velocity of the handlebar and the bike from
vertical (θ, θ, ω, ω) respectively. For the given state the agent is in, it chooses a continuous action of
applying torque to the handlebar, T ∈ [-2N m, 2Nm] trying to keep the bike upright. The details
of bicycle dynamics are beyond the scope of this paper; interested readers are referred to Randlov &
AlStrom (1998); AStrom et al. (2005), and references therein.
We use the Cart-Pole as source task for learning to balance a bicycle. The bicycle balance problem
in principle is similar to that of cart-pole, since the objective is to keep the unstable system upright.
The objective of balance is achieved in both the systems by moving in the direction of fall, which is
termed as a non-minimum phase behavior in the controls system literature. The control in the cart
pole affects the angle of the pole, by moving the cart such that it is always under the pole. In the
bicycle, the control is to move the handlebar in the direction of fall. However, balancing the bike
is not straightforward, to turn the bike under itself, one must first steer in the other direction before
turning in the direction of fall; this is called counter-steering AStrom et al. (2005). We observe that
both cart pole and bicycle have this commonality in their dynamical behaviors, as both the system
have a non-minimum phase that is the presence of unstable zero. This similarity qualifies the cart-
pole system as an appropriate choice of source model for bicycle balance task.
Cart pole is characterized by state vector [x,x,θ,θ], i.e., position, the velocity of cart and angle,
angular velocity of the pendulum. The action space is the force applied to the cart F ∈ [-1N, 1N].
For the mapping between the state space of bicycle and cart pole model, we use Unsupervised
Manifold Alignment(UMA) to obtain this mapping Wang & Mahadevan (2009). We do not report
17
Under review as a conference paper at ICLR 2020
the training time to learn the intertask mapping since it is common to both ATL and UMA-TL
methods. Figure 7b and 8 shows the quality of transfer for ATL through faster convergence to
average maximum reward with lesser training samples compared to UMA-TL and RL methods.
E.2 Mountain Car (MC) to Inverted Pendulum (IP)
Figure 9: Policy Transfer from Mountain Car to Inverted Pendulum: (a) Average Rewards and (b)
Training length
We also demonstrate the cross-domain transfer between mountain car to an inverted pendulum. The
source and target task are characterized by different state and action space. The source task MC
is a benchmark RL problem of driving an underpowered car up a hill. The dynamics of MC are
described by two continuous state variables (x,X) where X ∈ [-1.2, 0.6] and X ∈ [-0.07, 0.07]
and one continuous action F ∈ [-1, 1]. The reward function is proportional to the negative of
the squared distance of the car from goal position. The target task is conventional IP with state
(θ, θ) ∈ (-π, π) and action T ∈ [-1, 1]. We present the performance of transfer methods based
on sample efficiency in learning the target task and speed of convergence to the maximum average
reward. Similar to the bicycle domain transfer Figure 9a and 9b shows the quality of transfer for
ATL through faster convergence to average maximum reward with lesser training samples compared
to UMA-TL and RL methods.
18