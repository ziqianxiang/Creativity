Under review as a conference paper at ICLR 2020
Unsupervised Learning of Node Embeddings by
Detecting Communities
Anonymous authors
Paper under double-blind review
Ab stract
We present Deep MinCut (DMC), an unsupervised approach to learn node em-
beddings for graph-structured data. It derives node representations based on their
membership in communities. As such, the embeddings directly provide interest-
ing insights into the graph structure, so that the separate node clustering step of
existing methods is no longer needed. DMC learns both, node embeddings and
communities, simultaneously by minimizing the mincut loss, which captures the
number of connections between communities. Striving for high scalability, we
also propose a training process for DMC based on minibatches. We provide em-
pirical evidence that the communities learned by DMC are meaningful and that
the node embeddings are competitive in different node classification benchmarks.
1	Introduction
Graphs are a natural representation of relations between entities in complex systems, such as social
networks or information networks. To enable inference on graphs, a graph embedding may be
learned. It comprises node embeddings, each being a vector-based representation of a graph’s node
that incorporates its relations to other nodes (Goyal & Ferrara, 2018; Hamilton et al., 2017b). While
supervised node embeddings have received alot of attention, most real-world graphs are not labelled,
which calls for unsupervised learning techniques.
The main principle in unsupervised learning of node embeddings is that “similar” nodes have close
embeddings in the embedding space. The similarity of nodes is often defined based on their distance
in a graph, e.g., based on their co-occurrence probability in a random walk (Goyal & Ferrara, 2018;
Perozzi et al., 2014; Grover & Leskovec, 2016). Recently, it was also argued that two nodes should
be similar, if they are similar to a graph summary representation (VelickoVic et al., 2018).
In this work, we argue that node embeddings shall not only be of high quality for inference tasks,
but shall also be meaningful. That is, they shall directly proVide insights into interesting structures
in a graph in order to aVoid a potentially biased post-analysis step, e.g., through clustering of the
embeddings. We therefore assess node similarity from the perspectiVe of node communities, where
the dimensions of embeddings are some unknown communities instead of some unknown latent
features as in traditional techniques. Considering a community as a set of densely connected nodes
with sparse connections to outside nodes, the homophily principle is restated as follows: Nodes with
similar community membership characteristics shall haVe close embeddings. Specifically, for each
node, we incorporate membership information as a probability distribution oVer a set of communi-
ties. Then, nodes are similar, if they are both likely and unlikely to be part of the same communities.
Since communities are generally unknown, we propose to minimize the mincut loss for unsupervised
learning of communities and node embeddings simultaneously. Mincut loss leVerages the principle
that communities are well-separated if there are few connections between them (Fortunato, 2010).
It is theoretically motiVated as its optimal closed-form solution can be found, while its Variant, the
normalized cut, is a well-studied problem.
Aiming at a realisation of the aboVe idea, we propose Deep MinCut (DMC), a neural network ap-
proach to minimize mincut loss. We learn node embeddings to sample one-hot Vectors that represent
the assignment of nodes to communities. The Vectors are drawn from distributions parameterized by
continuous node embeddings using Gumbel-Softmax (Jang et al., 2016; Maddison et al., 2016). This
renders the process differentiable and, thus, enables joint learning of embeddings and communities.
1
Under review as a conference paper at ICLR 2020
We demonstrate the applicability of DMC in various applications. In node classification, our node
embeddings turn out to outperform traditional embedding techniques (Grover & Leskovec, 2016;
Perozzi et al., 2014; VeIickoVic et al., 2018), while also revealing the graph's community structure.
In community detection, by stacking mincut losses, we are able to learn a hierarchy of communities,
e.g., when generating word embeddings, we can link words to topics, and topics to abstract themes.
2	Related work
Graph embedding constructs a low-dimensional model of the nodes of a graph that incorporates its
structure (Hamilton et al., 2017b; Goyal & Ferrara, 2018). Embedding techniques can be classified
into shallow (Perozzi et al., 2014; Grover & Leskovec, 2016) and deep approaches (Hamilton et al.,
2017a; Kipf & Welling, 2016; Wu et al., 2019). Shallow approaches rely on an embedding lookup
table to map nodes to embeddings. On the other hand, deep models construct a node’s embedding
by performing aggregation of its neighbours’ embeddings.
Unsupervised node embeddings. While different models may be employed to embed nodes (Per-
ozzi et al., 2014; Grover & Leskovec, 2016), the majority of unsupervised learning techniques
leverage a contrastive loss function, such as skipgram loss (Hamilton et al., 2017b) or infomax
loss (VeIickoVic et al., 2017). The encoder is trained such that, given a scoring function, a high
score is given to positive samples, whereas negative samples receive a low score. For skipgram
loss, the positive samples are nodes that are close in a random walk, while the negative samples are
randomly selected from other graph nodes. A drawback of the random-walk objective is that it can
only capture local information around a node (Perozzi et al., 2014; Grover & Leskovec, 2016).
For infomax loss, positive samples are nodes in the original graph, whereas negative samples are
nodes in randomly corrupted graphs. While infomax loss can capture the global structure, its perfor-
mance is highly dependent on the corruption strategy (Velickovic et al., 2018). Since embeddings
also need to be learned for the corrupted graphs, it further suffers from high training time. While
our proposed method also considers the global graph structure, it differs in that our approach is non-
constrastive, i.e., it does not require unnecessary learning of negative samples. Moreover, due to the
nature of our loss function, we are able to learn the node embeddings as well as their clusters.
Community detection is a well-studied problem with many applications (Fortunato, 2010; Fortu-
nato & Hric, 2016). While numerous techniques for community detection have been proposed, we
focus on those that generate node embeddings, such as spectral methods (Newman, 2006b;a; White
& Smyth, 2005). Spectral methods operate either on the modularity matrix (Newman, 2006b) or
the Laplacian matrix (White & Smyth, 2005). While embeddings may be learned by reconstructing
these matrices (Wang et al., 2016), existing methods leverage matrix factorization, which does not
scale to large graphs. Closest to our work is (Nazi et al., 2019), which proposes a partition loss
function for graph partitioning. As the work focuses on graph partitioning, the loss function aims
for balanced partitions based on the number of nodes and, therefore, is not applicable in our setting.
3	Embeddings and Community Detection
Graphs. We consider a directed, weighted graph G = {V, F} with nodes V = {vi } and edges
F = {(vi, vj) | vi ∈ V ∧ vj ∈ V}, each edge (vi, vj) being assigned a weight s(vi,vj) ∈ R. Such a
graph can also be represented by its adjacency matrix A of size n × n, where each row and column
represents a node in G and a cell Aij denotes the edge weight. Note that we allow self-loops in the
graph, but not multi-edges between nodes. Also, edge weights can be negative. We also consider
attributed graphs in which nodes have features. We denote the node features matrix as F ∈ Rn×D .
Communities. We denote by C = {Cι, C2, ∙∙∙ , Ck } the set of k disjoint communities of graph G,
where Uk=I Ci = V and ∀ Ci = Cj, Ci ∩ Cj = 0. The assignment of nodes to communities is
captured by a membership matrix P ∈ {0, 1}n×k with rows representing nodes in G and columns
representing communities in C. As each node is only assigned to one community, the rows of P are
one-hot vectors, where Pij = 1 if node vi is assigned to community Cj.
Assuming the membership matrix P is already known, the number of cross-connections between
communities Ci and Cj can be captured by the non-diagonal elements of the adjacency matrix C of
2
Under review as a conference paper at ICLR 2020
the quotient graph, where the nodes are communities:
C = PTAP	(1)
On the other hand, elements Cii capture the number of connections within community Ci .
Mincut loss. While community detection is a well-studied problem, there is no consensus on the
precise notion of a community (Fortunato, 2010). A common principle is that well-separated com-
munities have more connections inside than across communities. Hence, communities are detected
by minimizing the number of connections between them, as captured by the following loss function:
k
LP(A)=-XCii=-Tr(PTAP)	(2)
i
where Tr(X) is the trace of matrix X. We call the loss function in Equation 2 mincut loss, as it aims
to minimize the number of connections between communities.
Degenerated cases. Minimizing Equation 2 may lead to degenerated cases, where all nodes are
assigned to one community while the others are empty (Fortunato, 2010). In practice, there are
two solutions to this problem. If there is prior knowledge on the communities (e.g., they shall have
equal size), a respective constraint is added to the mincut loss. Another approach is to minimize the
normalized cut (Shi & Malik, 2000; Zhang & Rohe, 2018), which is defined as follows:
k
ncut(C) = X
i=1
cut (Ci, C-i)
assoc(Ci, C-i)
XX PTi(D - A)P:,i
全	PTiDP. i
i=1	:,i ,
= Tr(
PT(D - A)P
-PT DP-)
T/ PTLP)
(PtDP)
where C-i denotes the set of communities except Ci and assoc(Ci, C-i) is the total degree of nodes
in community Ci . Then, the normalized cut (normcut) loss can be captured as follows:
PT LsymP
LP(A) = Tr( —PTP- )	⑶
where Lsym = I - D-1/2AD1/2 is the symmetric Laplacian matrix with D be the degree matrix
of A and and L = D - A is the Laplacian matrix.
Graph-like data. For ease of presentation, mincut loss is formulated based on graphs. Yet, it can
be applied to any problem comprising a set T of items and a kernel function k : T × T → R that
assigns weights to item pairs. This creates a kernel matrix K that can be considered as the adjacency
matrix of the items. Mincut loss is designed to separate items into subsets such that the connection
strength between every pair of subsets is minimized, which also means the coherence of each subset
is maximized. Hence, mincut loss is applicable to a wide range of unsupervised problems.
4 Deep MinCut
We first discuss the spectral approach to find optimal solutions for normcut and mincut loss, as
it provides a baseline technique for comparison. We then introduce Deep MinCut along with an
efficient training process based on minibatches.
4.1	Spectral approach
Since P is a binary matrix, optimizing the normcut loss to find P is an NP-hard problem (Fortunato,
2010). Following traditional approaches in community detection (Fortunato, 2010; White & Smyth,
2005), we relax P from a binary matrix to a real matrix H ∈ Rn×k. However, for the relaxed matrix
H to be meaningful, it needs to retain the following semantic constraint from matrix P that each
node belongs to only one community: hei hejT = 0 where hei is the i-th column of the matrix H. The
matrix H that minimizes Equation 3 can be found by eigendecomposition of the adjacency matrix
Lsym . This is captured by the following theorem.
Theorem 1. Let Lsym be the normalized Laplacian matrix of a graph G of size n and its eigende-
composition Lsym = QΛQT. We also denote λι, ∙∙∙ ,λk to be k smallest eigenvalues of Lsym and
3
Under review as a conference paper at ICLR 2020
Node	Membership Membership
embedding matrix E likelihood matrix matrix P
Figure 1: Learning embeddings by detecting communities.
their respective eigenvectors qι, •一，qk. Let H ∈ Rn×k be a matrix that satisfies hihT = 0. Then,
HT Lsym H
LH(Lsym) = Tr( HTH ) is minimized when the i -th column vector of H is parallel with the i -th
eigenvector, i.e., hi ↑↑ qi.
It is worth noting that closed-form solutions for the mincut loss can be found in a similar manner
where the matrix H that minimizes the mincut loss can be constructed from the largest eigenvectors
of the adjacency matrix A. The proofs of these theorems can be found in Appendix C.
4.2	Deep MinCut - A neural network approach
Although analytical solutions to Equation 2-3 can be found by eigendecomposition, such approach
to minimizing the normcut loss is infeasible for large graphs. We therefore propose a neural network
approach called Deep MinCut to learn node embeddings and communities at the same time. Our
framework is illustrated in Figure 1, which we explain in detail in the remainder.
Learning the membership matrix. To detect communities, we want to learn the membership
matrix P that minimizes the normalized cut LP(A) = Tr(P PTPmP). Recall that P captures
the assignment of nodes to communities. Intuitively, this assignment is based on a node’s role in
the graph and the graph structure, which is also the information that shall be encoded in a node
embedding (Grover & Leskovec, 2016; Hamilton et al., 2017a). Hence, we propose to compute the
membership matrix P based on node embeddings.
An embedding matrix H, which contains all the node embeddings, is derived by an encoder
Eθ : V → Rd . The encoder Eθ encodes every node in V to a d-dimensional space, where θ denote
parameters. Any existing node embedding techniques such as graph convolutional encoders (Hamil-
ton et al., 2017b; Wu et al., 2019) or shallow encoders (Grover & Leskovec, 2016; Perozzi et al.,
2014) can be used to realize Eθ , since the parameters θ can be learned during optimization of the
normalized cut. The node embeddings in H can also be used for downstream tasks such as node
classification or link prediction.
Differential sampling. To obtain P from H, we sample a one-hot vector p of P from the corre-
sponding node embedding h of H. Intuitively, Hij shall capture the likelihood that the i-th node is
assigned to the j-th community. As such, we consider the elements h1, . . . , hk of h to be the unnor-
malized class probabilities of a k-dimensional categorical distribution. Then, by sampling from this
distribution, we are able to obtain the one-hot vector p. Sampling from this distribution can be done
using the Gumbel-max trick (Gu et al., 2018; Niu et al., 2019) where the non-zero element of the
one-hot vector p = (p1, . . . , pk) is found as follows:
(1, if i = arg maxj (hj + g)
0, otherwise
where g 〜Gumbel(0,1) is a sample from the standard GUmbel distribution.
Note that the sampling process is still non-differentiable as the arg max operation is discontinuous
since the Gumbel-max trick only makes sampling from a categorical distribution an optimization
problem. By replacing the arg max function with the differentiable softmax, however, we render
4
Under review as a conference paper at ICLR 2020
Algorithm 1: Computation of normcut loss.
input : Adjacency matrix A, τ, straight-through st, embedding matrix H
output: Normcut loss L
1	P = gumbel _softmax (H, τ, st) ;	// Sample one-hot vectors from node embeddings
2	C = HT AH ;	// Adjacency matrix of the quotient graph.
3	q = 1C;	// Compute the association of communities. 1 is the vector of ones.
4	d = diagonal (C) ;	// Diagonal vector of C.
5	l = (q - d)/q ;	// Compute the normcut loss
6	L=Pl;
7	return L;
the whole process differentiable (Maddison et al., 2016; Jang et al., 2016). That is, an element pi of
p can be sampled from the corresponding row h of H as follows:
exp(xi∕τ)
Pi = x→	~(一1
i exp(xi∕τ)
where xi = hi + g and τ is a temperature hyperparameter. Gumbel-Softmax (GS) not only allows
to sample discrete values from a continuous distribution, but it is also differentiable. The latter is
important for learning the parameters of Eθ using backpropagation. As the temperature τ → 0, the
row p approaches the one-hot vector. The whole computation process of the normcut loss from an
adjacency matrix and the node embeddings is shown in Algorithm 1.
Straight-through Gumbel-Softmax. Since we want p to be close to a one-hot vector, we need to
set τ to be close to 0. However, a small temperature leads to a high variance of gradients which
makes the training process slow to converge (Jang et al., 2016). To this end, we use the Straight-
through Gumbel-Softmax which allows us to set a high temperature while obtaining one-hot vector
for p. This is done by taking the arg max of pi to construct the one-hot vector in the forward
pass, while in the backward pass, pi is used to compute the gradients. The trade-off is that there
is a mismatch between the forward and backward pass which makes Straight-through GS a biased
estimator. However, it performs well in practice (Choi et al., 2018; Gu et al., 2018; Niu et al., 2019).
Hierarchical community detection. Several mincut losses may be stacked to learn a hierarchy of
items. For instance, another mincut loss can be applied to the adjacency matrix C to learn super-
communities. Then, communities and super-communities can be learned in an end-to-end manner,
similar to the hierarchical pooling framework in (Ying et al., 2018). However, our approach is
unsupervised and may thus be used in applications where labels are not available.
4.3 Minibatch training
Sampling method. To improve scalability of DMC, parameters shall be learned with batches of
nodes, instead of the whole adjacency matrix. This is equivalent to approximating mincut loss with
sampled subgraphs. However, a simple random sampling of nodes to construct a subgraph is not
sufficient as the subgraph may not be connected. Hence, optimization of normcut loss is non-trivial.
Against this background, we propose to construct an ego-network for each node in the graph and
use this ego-network as the subgraph. This sampling procedure is similar to the neighbourhood
sampling method proposed by Hamilton et al. (2017a). Given a node v ∈ V, its ego-network of
depth d is the induced subgraph obtained from a sample of all nodes with a distance of at most d
to v . Sampling is done at each level, with replacement of a fixed amount of neighbours. This is to
make the subgraphs to have equal size. To create a batch of size b, we create b such ego-networks.
Theoretical motivation. We provide a theoretical motivation on why it is possible to approximate
the normcut loss function with sampled subgraphs. Let K ∈ Rm×k be the embedding matrix of
the subgraph S of size m. We also denote the adjacency matrix of this subgraph as A, its degree
matrix as D, and its Laplacian matrix as L. The following theorem shows that we can approximate
normcut loss to a certain degree with high probability using the subgraph of size m < n where n is
the number of nodes in the original graph.
HT LH	KT Le K
Theorem 2. Let L = Pi=1 T —, L = Pi=1 T* i	— be the normcut loss of the graph G
i=1 H:,iDH:,i	i=1 K:T,iDK:,i
and subgraph S with adjacency matrices A, A respectively. Let a, b ∈ R be the upper and lower
5
Under review as a conference paper at ICLR 2020
Table 1: Statistics of the datasets
Dataset	Nodes	Edges	Features	Classes
Cora (Sen et al., 2008)	2,708	5,429	1,433	7
Citeseer (Sen et al., 2008)	3,327	4,732	3,703	6
Pubmed (Namata et al., 2012)	19,717	44,338	500	3
Wiki (Grover & Leskovec, 2016)	4,777	184,812	n/a	40
bound of A then if H is a binary matrix and elements of A and A are i.i.d then given an ε ≥ 0,
P(| L-L | ≤ ε) ≥ 1 —2k(eχτ~>( (nG )+eχτ~ι(—(——mε) )+eχτ~ι(———n_ε— )+eχ∙∩( — m_ε—) +
Pu L | ≤ ε) ≥ 1 2k(eχp( i28k2(b-a)2 ) + exp( I28k2(b-a)2 ) + exp( 64k(b-a)2 ) + exp( 64k(b-a)2 ) +
2
2eχp(-mk ) + 2exP(-n)).
Theorem 2 (proof in Appendix C) states that we can choose a batch size such that the difference
in the approximated loss and the true loss is small with high probability. Other factors that affect
this probability are the bounds a, b of its adjacency matrix and the embedding size k. Finally, the
probability also depends on how accurately we approximate the loss function, as controlled by ε.
5	Experiments
5.1	Node embedding experiments
Setup. We evaluate the quality of our embeddings for node classification on four datasets, see
Table 1. Cora, Citeseer and Pubmed are paper citation networks where a label represents the domain
of a paper. Wiki is a word adjacency graph with the word labels being their POS tags.
In this experiment, we use a one-layer Graph Convolutional Network (Kipf & Welling, 2016) as the
encoder for DMC and also report the results obtained with the spectral approach. We compare DMC
with several baselines. First, we compare against community detection techniques that generate node
embeddings, such as DANMF (Ye et al., 2018), M-NMF (Wang et al., 2017) and GAP (Nazi et al.,
2019). Second, we include unsupervised node embedding methods that use contrastive loss, such
as DeePWalk (Perozzi et al., 2014) and DGI(VeIickoVic et al., 2018). For all methods that involve
randomization, we train three models with different seeds. The node embedding size is consistently
set to 128. The obtained node embeddings are used to learn a logistic classifier. Instead of a fixed
train/test sPlit, we use 50 random sPlits and rePort the mean accuracy and standard deviation as
suggested by Shchur et al. (2018).
Results. Table 2 highlights the benefits of generating embeddings by community detection for node
classification. Our technique outPerforms the baseline methods in three out of four benchmarks.
The largest gaP is observed for the Wiki dataset, which can be exPlained by the non-attributed
nature of this dataset. DMC considers the whole structure of the graPh, whereas most baseline
techniques consider only the neighbourhood surrounding a node. While DGI is able to incorPorate
the whole graPh, it relies more on node features, which is less beneficial for non-attributed graPhs.
The sPectral aPProach underPerforms significantly on the bibliograPhic datasets as it only uses the
structure information in the graPh. In addition, the oPtimal solution to normcut loss may not be the
best embeddings for node classification, as the node labels are more correlated to node features.
In addition to node embeddings, DMC also learns how to cluster the embeddings. We comPare the
cluster quality obtained by our aPProach with the best baseline for node classification, which is DGI.
For DGI, we use k-means to cluster the node embeddings into several clusters where the number of
clusters is the number of classes. Then, we comPare the cluster quality obtained using DMC and
DGI on two metrics: Normalized Mutual Information (NMI) and Homogeneity (HG).
Table 3 shows that the cluster quality obtained by DMC is significantly higher than the one by DGI.
For instance, the NMI scores with DMC are 7× better than those with DGI on the Cora dataset. This
illustrates the benefits of jointly learning node embeddings and their clusters. This is Particularly
imPortant in unsuPervised settings, where the learned embeddings are fed into a downstream task,
such as node classification or graPh analysis through clustering.
6
Under review as a conference paper at ICLR 2020
Table 2: Node classification results.
	Method	Cora	Citeseer	Pubmed	Wiki
	GAP	0.768±1.1e-2	0.663±4.8e-3	0.754±5.9e-3	0.596±4.1e-3
Community	M-NMF	0.775±0.9e-3	0.562±0.9e-3	0.763±0.3e-3	0.64±1.3e-2
detection	NMF	0.514±1.4e-2	0.443±9.3e-3	0.672±7.4e-3	0.485±4.9e-3
	Feat.+Comm.	0.749±0.8e-3	0.709±0.9e-3	0.851±0.2e-3	0.659±0.1e-2
Contrastive	DeepWalk	0.670±2.6e-3	0.479±3.3e-3	0.722±4.9e-3	0.491±2.6e-3
loss	DGI	0.813±0.3e-3	0.711±7.4e-3	0.835±3.6e-3	0.568±1.2e-3
Normcut	DMC (Ours)	0.839±1.8e-3	0.713±4.1e-3	0.833±2.2e-3	0.659±4.5e-3
loss	Spectral	0.303	0.206	0.397	0.581
Dataset	Metric	DGI	DMC
Cora	NMI	0.061	0.429
	HG	0.068	0.475
Citeseer	NMI	0.059	0.212
	HG	0.067	0.264
Pubmed	NMI	0.172	0.215
	HG	0.191	0.277
Wiki	NMI	0.272	0.286
	HG	0.345	0.339
Table 3: DMC vs. DGI on cluster quality
Figure 2: Effect of batch size
5.2	Community detection evaluation
Setup. Next, we aim to show the applicability of normcut loss to a graph-like setting, such as
learning word embeddings. Here, the nodes are words and the connection weight between two
words is measured by the following “kernel” function:
k(wi, wj)
log(#(wi, wj))
ιog( log(#Wi)log(#Wj),
0)
where #wi is the number of times word wi appears in the corpus, while #(wi , wj ) is the number
of times the words appear together. The adjacency matrix obtained using the above function is
the PPMI matrix, a well-established concept in NLP (Levy & Goldberg, 2014). Following Yin
& Shen (2018), we construct a word corpus of 10000 words that appear >100 times in the Text8
corpus (Mahoney, 2011). Words are said to appear together if they are within a window of five.
We further analyse the quality of the communities constructed by DMC. As those are represented by
the dimensions of the node embeddings (in this case, word embeddings), high-quality communities
correspond to explainable word topics. We evaluate explainability by a word intrusion test. We
create a set of five words for each dimension by selecting the top-4 words and a single low-ranked
word. Human workers on MTurk are asked to detect one word per dimension that does not belong to
the respective set. The detection precision then measures explainability. We compare DMC against
explainable word embeddings techniques, OIWE (Luo et al., 2015), Sparse Coding (SC) (Faruqui
et al., 2015), and Non-Negative Sparse Coding (NNSC) (Faruqui et al., 2015). For a qualitative
analysis, we also report the words with the largest embedding values along exemplary dimensions.
Results. Table 4 shows that DMC outperforms state-of-the-art methods in precision of the word
intrusion test. Note that NNSC, SC, and OIWE require additional data such as existing word em-
beddings as input, whereas our methods learn explainable word embeddings directly on a word
corpus. Table 5 shows that the top-ranked words indeed assign a meaning to each dimension (here,
the first three dimensions concern medieval literature, DC comics, and transportation). By stacking
two normcut losses, DMC is also able to learn a hierarchy of words and topics. Figure 4 gives an
example, where super-topic #18 captures IT-related words, while supertopic #3 is related to religion.
7
Under review as a conference paper at ICLR 2020
Table 4: Test Precision
Table 5: Top-5 words for the first five dimensions
	Precision	Dim #1	Dim #2	Dim #3	Dim #4	Dim #5
NNSC	35.85%	medieval	created	flight	full	internet
SC	47%	earliest	features	pilot	job	network
OIWE	91.01%	scholars	dc adaptation batman	navy	fair offered calling	client
DMC	95.24%	renaissance classical		passenger aviation		server servers
Figure 3: Embedding size vs. #communities
Figure 4: Hierarchy of words/topics
5.3	Effects of minibatch training
Setup. We evaluate the effects of minibatch training on the classification accuracy on three citation
networks by varying the batch size from 20 to 500.
Results. Confirming our theoretical analysis, Figure 2 shows that the difference between the approx-
imated loss and the true loss depends on the batch size. With increasing batch sizes, the accuracy
on the Cora and Citeseer datasets increases as well. However, after an initial sharp increase, the
differences become smaller. This shows the robustness of our approach to the batch size. Moreover,
we observe a reversed trend on the Pubmed dataset, which is the largest among the citation graphs.
We believe that this can be attributed to the stochasticity of minibatch training, which renders outlier
nodes to be less important as the subgraphs can only cover parts of the whole graph.
5.4	Relation between embedding size and number of communities
Setup. The aim of this experiment is two-fold. First, we aim to show the merit of the mincut loss.
While the normcut prevents degenerate cases, the mincut loss is useful if we have prior knowledge
about the graph structure. Second, we want to analyze the effect of the embedding size w.r.t the
number of communities. For this experiment, we need a ground truth number of communities, which
is why we rely on the Stochastic Block Model (SBM), a well-established benchmark for community
detection (Chen et al., 2019; Fortunato & Hric, 2016; Fortunato, 2010). Using SBM, a graph with
a known number of communities (#com) is generated. Then, we construct node embeddings with
varying dimensionality using the spectral approach for mincut loss with an additional balancing
constraint on the community sizes. The parameters of SBM are discussed in detail in Appendix B.
Node embeddings are assessed for link prediction with the ROC metric (avg over 100 runs). We
chose two values for parameter p, the probability of an edge between two nodes in a community.
Results. Figure 3 confirms that “there is a sweet spot for the dimensionality, [..] neither too small,
nor too large” (Arora et al., 2016). Our results provide one possible explanation for the dimen-
sionality trade-off. If the embedding size is smaller than the number of communities, unrelated
communities are combined, lowering the ROC. If the embedding size is larger than the number of
communities, communities are split up further. This also decreases the ROC, but not as drastically,
since the model has higher capacity. In practice, the embedding quality tends to increase and then
stabilize, with increasing embedding sizes, due to the communities often having different sizes.
8
Under review as a conference paper at ICLR 2020
6	Conclusion
We presented a novel perspective on unsupervised learning of node embeddings. Following the idea
of community detection, we proposed Deep MinCut (DMC), an approach to minimize the mincut
loss function to learn node embeddings and communities simultaneously. DMC learns node embed-
dings that are not only of high quality, but are also meaningful as they capture the graph’s structure.
We demonstrated the value of node embeddings learned with mincut loss in diverse experiments.
References
Karim M Abadir and Jan R Magnus. Matrix algebra, volume 1. Cambridge University Press, 2005.
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence
embeddings. In ICLR, pp. 1-16, 2016.
Zhengdao Chen, Lisha Li, and Joan Bruna. Supervised community detection with line graph neural
networks. In ICLR, pp. 1-23, 2019.
Jihun Choi, Kang Min Yoo, and Sang-goo Lee. Learning to compose task-specific tree structures.
In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer, and Noah Smith. Sparse overcomplete
word vector representations. arXiv preprint arXiv:1506.02004, 2015.
Santo Fortunato. Community detection in graphs. Physics reports, 486(3-5):75-174, 2010.
Santo Fortunato and Darko Hric. Community detection in networks: A user guide. Physics reports,
659:1-44, 2016.
Palash Goyal and Emilio Ferrara. Graph embedding techniques, applications, and performance: A
survey. Knowledge-Based Systems, 151:78-94, 2018.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 855-864. ACM, 2016.
Jiatao Gu, Daniel Jiwoong Im, and Victor OK Li. Neural machine translation with gumbel-greedy
decoding. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in Neural Information Processing Systems, pp. 1024-1034, 2017a.
William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods
and applications. IEEE Data Engineering Bulletin, 2017b.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Ad-
vances in neural information processing systems, pp. 2177-2185, 2014.
Hongyin Luo, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. Online learning of interpretable word
embeddings. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language
Processing, pp. 1687-1692, 2015.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.
Matt Mahoney. Large text compression benchmark. URL: http://www. mattmahoney. net/text/text.
html, 2011.
9
Under review as a conference paper at ICLR 2020
Galileo Namata, Ben London, Lise Getoor, Bert Huang, and UMD EDU. Query-driven active
surveying for collective classification. In 10th International Workshop on Mining and Learning
with Graphs, pp. 8, 2012.
Azade Nazi, Will Hang, Anna Goldie, Sujith Ravi, and Azalia Mirhoseini. Gap: Generalizable
approximate graph partitioning framework. arXiv preprint arXiv:1903.00614, 2019.
Mark EJ Newman. Finding community structure in networks using the eigenvectors of matrices.
Physical review E, 74(3):036104, 2006a.
Mark EJ Newman. Modularity and community structure in networks. Proceedings of the national
academyofsciences,103(23):8577-8582, 2006b.
Xing Niu, Weijia Xu, and Marine Carpuat. Bi-directional differentiable input reconstruction for
low-resource neural machine translation. In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pp. 442-448, 2019.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representa-
tions. In SIGKDD, pp. 701-710. ACM, 2014.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93-93, 2008.
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and StePhan Gunnemann. Pitfalls
of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.
Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. Departmental Papers
(CIS), PP. 107, 2000.
Petar VeliCkovic, Guillem CucurulL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. GraPh attention networks. arXiv preprint arXiv:1710.10903, 2017.
Petar VeliCkoviC, William Fedus, William L Hamilton, Pietro Lid, Yoshua Bengio, and R Devon
Hjelm. DeeP graPh infomax. arXiv preprint arXiv:1809.10341, 2018.
Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deeP network embedding. In Proceedings of
the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, PP.
1225-1234. ACM, 2016.
Xiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, and Shiqiang Yang. Community Preserving
network embedding. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.
Scott White and Padhraic Smyth. A sPectral clustering aPProach to finding communities in graPhs.
In Proceedings of the 2005 SIAM international conference on data mining, PP. 274-285. SIAM,
2005.
Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr, ChristoPher Fifty, Tao Yu, and Kilian Q
Weinberger. SimPlifying graPh convolutional networks. arXiv preprint arXiv:1902.07153, 2019.
Fanghua Ye, Chuan Chen, and Zibin Zheng. DeeP autoencoder-like nonnegative matrix factoriza-
tion for community detection. In Proceedings of the 27th ACM International Conference on
Information and Knowledge Management, PP. 1393-1402. ACM, 2018.
Zi Yin and Yuanyuan Shen. On the dimensionality of word embedding. In Advances in Neural
Information Processing Systems, PP. 887-898, 2018.
Zhitao Ying, Jiaxuan You, ChristoPher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hi-
erarchical graPh rePresentation learning with differentiable Pooling. In Advances in Neural Infor-
mation Processing Systems, PP. 4800-4810, 2018.
Yilin Zhang and Karl Rohe. Understanding regularized sPectral clustering via graPh conductance.
In Advances in Neural Information Processing Systems, PP. 10631-10640, 2018.
10
Under review as a conference paper at ICLR 2020
巾#Com=20 φ⅛com=25 φ⅛com=30 本#COm=35
巾#Com=20 φ≠com=25 φ⅛com=30 6#Com=35
Embedding size
Figure 5: Relationship between embedding size and #communities
A	Additional experimental results
A. 1 Relationship between embedding size and number of communities
Taking up the discussion in Section 5.4, Figure 5 provides additional results with different values
of p for the Stochastic Block Model. We observe that the ROC scores increase with p. This is
expected as larger p values correlate with a clearer community structure, which yields better link
prediction. On the other hand, with smaller p values, we observe the dimensionality trade-off better.
As p decreases, the graphs converge towards random graphs with no community structure. As a
result, it is easier for the model to cluster nodes into communities incorrectly. This effect becomes
more severe when the gap between the embedding size and the number of communities is larger.
The reason being that the model is forced to cluster nodes into communities that may be larger or
smaller than actual communities.
B	Details on Datasets and Hyperparameters
B.1	Hyperparameters
The embedding sizes of all methods are set to 128 to achieve a fair comparison. For methods that
use deep models (DMC, DGI, GAP), we use a learning rate of 0.001 and Adam as the optimizer. All
methods are trained for a fixed number of epochs, namely 3000. For DMC, since we use Straight-
Through Gumbel-Softmax, we fix the temperature to be 1. For minibatch DMC, we use a batch size
of 100 and two-layer neighbourhood sampling, where 10 and 25 neighbours are used in the first and
second layer. For word embedding experiment, we use a shallow encoder for DMC. For experiments
with k-means, we take the average scores (NMI and HG) over 10 runs.
B.2	Datasets
Node embedding experiments. Cora (Sen et al., 2008), Citeseer (Sen et al., 2008) and Pubmed (Na-
mata et al., 2012) are three standard benchmark datasets for node classification. Traditionally, a fixed
train/test split is used to evaluate the node embeddings. However, Shchur et al. (2018) showed that
using random splits for evaluation is fairer for different methods. As these datasets are attributed,
we also include the non-attributed Wiki dataset Grover & Leskovec (2016). For this dataset, we
initialize the node features using truncated SVD, setting the feature size to 128.
Word embedding experiments. To learn the embeddings, we use the Text8 corpus Mahoney (2011)
which is a standard large-scale benchmark for various NLP tasks. To construct the PPMI matrix,
we leverage a library that is publicly available1 and use the same setting as in Yin & Shen (2018)
to build the vocabulary and count the co-occurrences of word pairs: A window size of 5 and a
minimum occurrence of a word of 100.
1https://github.com/ziyin-dl/word-embedding-dimensionality-selection
11
Under review as a conference paper at ICLR 2020
Stochastic Block Model (SBM). To build graphs from the SBM, we leverage the networkx library.2
The parameters of SBM include the number of communities #ncom, community sizes and the prob-
ability of edges between nodes in a community p. We use the balanced SBM where the community
sizes are set to be equal to 100, while we vary p from 0.3 to 0.5 to generate the graphs. From p, we
can compute the probability of connection across communities as P= _#-Pom. When P 〜p, the
graph becomes a random graph with no community structure.
B.3	Hardware
Experiments were conducted on a workstation with an Intel Core i7-6700K CPU @ 4.00GHz with
32 GB RAM and an Nvidia GTX 1080Ti GPU with 12 GB anda server with 2 nodes. Each node
includes an AMD Ryzen 1900X CPU with 64GB RAM and 1 Nvidia GTX 1080Ti GPU with 12 GB.
C Proofs
C.1 Analytical solution for mincut loss
Theorem 1.	Let M be a positive semi-definite matrix of size n × n and its eigendecomposition
M = QΛQT. We also denote λι, ∙∙∙ ,λm, to be m largest eigenvalues of M and their respective
n×m
eigenvectors qι, ∙∙∙ , qm,. Let H ∈ Rn×m be a matrix that satisfies(hi, hji = 0 then LH(M)=
-Tr(HTMH) is minimized when the i-th column vector of H is parallel with the i-th eigenvector
i.e. hi ↑↑ qi.
Proof. Note that we can factorize any matrix H = UX by a unitary matrix U of size n × m and a
diagonal matrix X of size m. We denote XXT = diag(x1, x2, ..., xm) and U1:k is the matrix of k
leading columns of U. We also denote I1:k to be the identity matrix of size k × k. Then, we have:
LH(M) = -Tr(HTMH) = -T r(HHT M) = -T r(UXXT UT M)
mm
= -Tr(X(U1:k(xk - xk+1)I1:kU1T:kM)) = - X(xk - xk+1)Tr(U1T:kMU1:k)
By applying the Poincare Separation Theorem Abadir & Magnus (2005), we have ∀k =
1, m, Tr(U1T:kMUi：k) is maximized iff ∀k = 1,m, the column vectors of Ui：k are proportional
to k leading eigenvectors of M. In addition, as the column vectors of U or U1:k are proportional
to the column vectors of H, LH(M) is minimized iff the column vectors of H are proportional to
m leading eigenvectors of M. In other words, the column vectors of H are parallel with m leading
eigenvectors of M.	□
C.2 Analytical solution for normcut loss
Theorem 2.	Let Lsym be the normalized Laplacian matrix of a graph G of size n and its eigende-
composition Lsym = QΛQT. We also denote λι,…，λk to be k smallest eigenvalues of Lsym and
their respective eigenvectors qi, ∙∙∙ , qk. Let H ∈ Rn×k be a matrix that satisfies hihʃ = 0 then
HT Lsym H
LH(Lsym) = Tr (—HTH—) is minimized when the i-th column vector of H is parallel with the i-th
eigenvector i.e.hi ↑↑ qi.
Proof. We can see that if we multiply hi with an arbitrary number, the value of LH (Lsym) is
unchanged. So that we can assume HTH = I. With this assumption, the problem becomes finding
H to minimize Tr(HT LsymH).
Let μι ≥ μ2 ≥ ∙∙∙ ≥ μk be the eigenvalues of HTLsymH and λι ≥ λ? ≥ ∙∙∙ ≥ λn be the
eigenvalues of Lsym . Then, according to Poincare’seperation theorem, we have:
2https://networkx.github.io/
12
Under review as a conference paper at ICLR 2020
k	kk
∑λn-k+i ≤ £〃i ≤ £%	(4)
Noting that Pk=ι μk = Tr(HTLsymH). This means Tr(HTLsymH) is minimized at Pk=I λn-k+i
when hi is proportion with k smallest eigenvectors of LSym.	□
While the closed-form solutions for the mincut and normcut loss can both be constructed from
eigendecomposition, there are difference in application. The normcut loss is able to prevent degen-
erated cases since they do not correspond to the optimal loss value. On the other hand, for these
degenerated cases, the mincut loss is minimal.
C.3 Approximated normcut loss
Before proving Theorem 2, we provide the following lemmas.
Lemma 1. Given X = a ,y = d. If P (∣ a-c ∣ ≤ ε) ≥ 1 一 P and P (∣ b-d ∣ ≤ ε) ≥ 1 - q with small ε
then P(∣ x-y ∣ ≤ 2ε) ≥ 1 -(P + q)
Proof. We consider the case where x, y ≥ 0 as similar result can be proved for x, y < 0. Let
a = (1 + α)c and b = (1 + β)d. Then, we can rewrite the above statements as
P(∣β∣≤ ε) ≥ 1 - P
P(∣α∣ ≤ ε) ≥ 1 - q
This also means:
⇒ P(lβl ≤ ε,∣α∣ ≤ ε) ≥ 1 - (p + q)
Moreover, when ∣β∣ ≤ ε and|a| ≤ ε with small ε, we have
Similarly,
(1 + a)c
(1 + β)d
⇒x
≥ (I - ε)c ≥ (I - ε)
-(1+ ε)d - (1+ ε)
y ≥ (1 - 2ε)y
x≤
(1 + ε)c
(1 — ε)d
≤ (1 + 2ε)y
x
So that, if ∣β∣ ≤ ε and∣α∣ ≤ ε, and a small value of ε, We have
x-y
——y ≤ 2ε
∣y∣
Therefore, We have P (∣x-y ∣ ≤ 2ε) ≥ 1 — (p + q) with small ε	□
Lemma 2. Let x1, x2, ..., xm be m independent random variables such that P(xi = 1) = bi. Let
X = pn=o Xi and μ = E[X]. Then, for 0 < δ < 1, P(X ≤ (1 - δ)μ) ≤ exp(-μδ2∕2)
Corollary 1. Let H ∈ {0, 1}n×k be an arbitrary binary matrix where the number of1 elements in
H is n. Let pi be the ratio of 1 in column hi, then, P (Pi ≥ /)≥ 1 - exp(就)
Proof. Applying Lemma 2 with Xi is Pi ∀i = 1,k, μ = E[pi] = ɪ and δ = 1/2.
□
Lemma 3. (Chernoff bound) Let x1, x2, ..., xm is m random variables with Poisson distribution
with P(Xj = 1) = Pi,X = Pm=I Xj and, μ = E[X]. We have, ∀δ ∈ (0,1):
X — μ	δ2
P(------- ≥ δ) ≤ 2exP(-μ-)
μ	3
13
Under review as a conference paper at ICLR 2020
Lemma 4. (Hoefding's bound) Let x1,x2,…,xm, is m independent random variables. E[x∕ = μ,
P(a ≤ Yi ≤ b) = 1∀i. With arbitrary reals a and b, We have,
1n
P (1 X
i=1
∣∣	-2nδ2
Xi - μ ≤ δ) ≥ 1 - 2exp((b - 句2 )
Theorem 3. Let G be a graph with its adjacency matrix A and its subgraph S with adjacency
，* ɪ τ	7 一 亦 7	,∖	it	1	1 r a .» ∙rττ r;^>	ι τ/ 一
matrix A. Let a, b ∈ R be the upper and lower bound of A then if H ∈ {0, 1}n×k and K ∈
e> ee	e> e
{0,1}m×k and elements of A and A are i.i.d then given an ε ≥ 0, P(|」-e>ʌe-----1 ≤ 2) ≥
1 - 2(exp( 128k2nε-a)2 ) + exp( 128kim(b-a)2
the i-th column vectorsofH,K and c
∖ .	/ 一mε2 ∖ .	/ 一n \ \
) + exp(-394k) + exp(-n)) ∀i = 1,n where hi, k are
2
n2 .
Proof. Let n(x) be the counting function for the number of 1 elements in a binary vector or matrix.
We also denote pi to be the ratio of 1 elements in vector hi . As K is sampled uniformly from H and
With n large enough, We can assume that each element of ki is equally chosen With probability pi .
Then, n(ki) is a random variable for the number of 1 elements in vector ki. We also have n(ki)
__… _ ~ . .
m
j=1 Kj,i Which means E[n(ki)] = mpi With P (Kj,i = 1) = pi.
Then, by applying bound (Lemma 3 with X
P(
/T ∖
n(ki) - mpi
mpi
=n(ki), δ = ε and μ = E[n(ki)] = mpi, We have
≥ ε) ≤ 2eχp(-mpi-ε-)
8	192
~
n
m
〜


2
Denote C =m,we have n(hi) = np which means mpi = √C.n(hi). Therefore, We have:
~
~
P(
n(ki) - √cn(hi)
Γ~ / rΓ ∖
√cn(hi)
~	. C
≤ ε) ≥ 1 - 2eχp(-mPiN)
8	192
22
n(ki)2 - cn(hi)2
∙~	. C
cn(hei)2
ε	ε2
≤ 4) ≥ 1 - 2eχp(-mpi 192)
(5)
>2	>
Note that n(kiki>) = n(ki)2 and n(hihi>)
with:
2
n(hi)2 which make the above inequality equivalent
P(
>>
n(kiki> ) -cn(hihi> )
>
n(hihi> )
2
ε	ε2
≤ 4) ≥ 1 - 2eχp(-mPi 192)
(6)
⇔ P (
~
Since elements of A are i.i.d and bounded by a, b, we have hei>Ahei = Pj∈J Yj with J ⊆ [1, n2]
and|J| = n(hi). Similarly, ki>Aeki = Pj∈T Yj with T ⊆ J and|T| = n(ki).
>>	ε
Applying lemma 4, With h> Ahi, k>Bki and δ = ε, we have
>	>2
P(∣h>Ahi -n(hih>)μ∣ ≤ nh8h^) ≥ 1 - 2exp( -；；" )
and,
>	>2
P(∣e>Aκ - n%e>)μ∣ ≤ n^) ≥ 1 - 2eχp( -m(kie>))ε)
Moreover, since n(keikei>) = (m.pi)2 and n(heihei>) = (n.pi)2, we have
P(
>>
k> Aki - n(kik>)μ
>
n(kiki> )
>>
h> Ahi - n(hih> )μ
>
n(hihi> )
ε
≤ -)
≤ 8)
ε
≤ 8,
≥ 1 - 2(exp(
32(b - a)2
)+exP(⅛mp⅛)) 0)
32(b - a)2
14
Under review as a conference paper at ICLR 2020
,〜~ _
kei>Ae kei -
n(hihi)	"'
,~	~ -I-,
nm h >Ah i
n(hihi> ) i ii
ε
≤ -)
≤ 4)
≥ 1 - 2(exp(
32(b - a)2
)+exP(3-≡⅜))
(8)
Applying Lemma 1 for inequality 6 and 8 , we have,
P(|
Tt TT Tt A T
ki>Aki - c.hi>Ahi
c.he i> Ahe i
| ≤ 2ε) ≥ 1 - 2(eχp(
32(b - a)2
(-mpiε)2	-mpiε2
+exp(32(b-τρ )+exp(F^))
(9)
From Corollary 1, we also have
⇒ P (
)
1n
P (pi ≥ 2k) ≥ 1 - exp(8k )
(10)
From inequality (9) and (10) , we have,
P(
Tτ TT Tt A T
ki>Aki - c.hi>Ahi
c.hei>Ahei
≤ ε) ≥ 1 - 2(exp(128-2(nε- ɑ)2 )
+ exp(12W‰) + exp( -m2) + exp( In))
(11)
□
Theorem 4.	Let G be a graph with its adjacency matrix A and its subgraph S with adjacency
matrix A. Let a,b ∈ R be the upper and lower bound of A then if H ∈ {0, 1}n×k
and K
∈
{0, 1}m×k and elements of A and A are i.i.d then given an ε ≥ 0, P(
~-Γ ≈	~ -Γ .
e>A 1-c.h>A1
c.h> AI
22、	22、	2一	^一	---
1 - 2(exP( 64-(b-a)2 ) + exP( 64k(b-a)2 ) + exP( -mk )) + exP( -n )) ∀i = 1,n and C
≤ 2)
m2
n2 ,
≥
Proof. (Proof sketch) Theorem 4 can be proven in the same manner as Theorem 3.
□
Given the results in Theorem 4 and Theorem 3， We can noW prove Theorem 2.
eT e	eT e e
Theorem 5.	Let L = Pi=ι ∣T ∣i, L = Pi=ι ∣Te∣i be the normcut loss of the graph G and
hi Dhi	ki Dki
subgraph S with adjacency matrices A, A respectively. Let a, b ∈ R be the upper and lower
bound of A then if H is a binary matrix and elements of A and A are i.i.d then given an ε ≥ 0,
P(| L-L | ≤ ε) ≥ 1 —2k(exn(------(nε)--)+ex^rι( —(——mε)----)+exn( ———n_ε— )+exn( — m_ε—) +
Pu L | ≤ ε) ≥ 1 2k(exp( I28k2(b-a)2 )+exp( 128k2(b-a)2 )+exp( 64k(b-a)2 )+exp( 64k(b-a)2 ) +
2
2exP( -mk ) + 2exP( -n))
TT	T
Proof. First，since L = D - A，L = P3 ⅛⅛ - P3 请=k - P3,h⅛≡i ∙ This
means optimizing L, L is equivalent to optimizing L = Pk=I ITAhi, L = Pk=ι IrAHi.
hi Dhi	ki Dki
Second, observe that both L and Le are the sum of d terms. Thus, proving T ≤ ε is similar to
prove
〜
Li-Li
Li
≤ ε, ∀i = 1, k
(12)
>>
h Ahi	k Aki
In WhiCh Li = h>Dhi, Li == k>Dki.
ii
15
Under review as a conference paper at ICLR 2020
Algorithm 2: Minibatch forward propagation of Deep MinCut
input : Adjacency matrix A of graph G = {V, F}
Temperature τ
Straight-through st
Encoder Eθ
Node feature matrix F
Sampling method f
output: Node embedding matrix H
1
2
3
4
5
6
7
8
9
10
	Community assignment P	
//	1. Sampling minibatches of nodes	
U=	f(V)	
A0	= A[U, U]	
F0	= F[U]	
//	2. Compute the node embeddings	
H	=Eθ(A0,F0);	// Embed nodes in batch using the encoder
//	3. Compute the mincut loss	
P	gumbel _softmax (H, τ, st) ;	// Sample one-hot vectors from node embeddings
C	= HTA0H ;	// Adjacency matrix of the quotient graph.
q=	1C ;	// Compute the	association of communities. 1 is the vector of ones.
d=	diagonal (C) ;	// Diagonal vector of C.
l=	(q - d)/q ;	// Compute the normcut loss
L	Pl	
From Lemma 1, to prove inequality 12, we need to have the following results:
	>> k> Aki-C.h> Ahi	ε |-Ch7Ah- | ≤ 2	(13)
And, the second is	>> I k> Dki - ch Dki I V ε	门4、 |Ch7Dkl≤ 2	( ) ⇔l e>A ” > AI l≤ ε, ∀i F	(15) C.hi> A1	2
2
where C = m.Inequality 13 is proven in Theorem 3 while Inequality 15 is proven in Theorem 4.
' □
D Algorithm
Algorithm 2 illustrates one iteration in the forward pass of DMC. The forward pass include 3 steps.
In the first step, we sample a subset of nodes of the graph (Line 1) using a sampling strategy. In
our work, we use a neighbourhood sampling strategy as discussed in Section 4.3. From the sampled
nodes, we extract a submatrix from the adjacency matrix A in Line 2 and from the node feature
matrix F in Line 3. The second step involves constructing the node embeddings for the sampled
nodes. We use an encoder Eθ to embed the nodes in Line 3. The encoder can be shallow which does
not use the node features or a deep encoder. In our node classification experiment, we use 1-layer
GCN as discussed in Section 5.1 and a shallow encoder for our word embedding experiment. In
the last step, we use the node embeddings to compute the mincut loss as shown in Figure 1. From
the loss, we can backpropagate and update the parameters θ of the encoder using any automatic
differentation framework.
16