Under review as a conference paper at ICLR 2020
Differential Privacy in Adversarial Learning
with Provab le Robustness
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we aim to develop a novel mechanism to preserve differential privacy
(DP) in adversarial learning for deep neural networks, with provable robustness
to adversarial examples. We leverage the sequential composition theory in DP, to
establish a new connection between DP preservation and provable robustness. To
address the trade-off among model utility, privacy loss, and robustness, we design
an original, differentially private, adversarial objective function, based on the post-
processing property in DP, to tighten the sensitivity of our model. An end-to-end
theoretical analysis and thorough evaluations show that our mechanism notably
improves the robustness of DP deep neural networks.
1	Introduction
The pervasiveness of machine learning exposes new vulnerabilities in software systems, in which
deployed machine learning models can be used (a) to reveal sensitive information in private training
data (Fredrikson et al., 2015), and/or (b) to make the models misclassify, such as adversarial exam-
ples (Carlini & Wagner, 2017). Efforts to prevent such attacks typically seek one of three solutions:
(1) Models which preserve differential privacy (DP) (Dwork et al., 2006), a rigorous formulation
of privacy in probabilistic terms; (2) Adversarial training algorithms, which augment training data
to consist of benign examples and adversarial examples crafted during the training process, thereby
empirically increasing the classification accuracy given adversarial examples (Kardan & Stanley,
2017; Matyasko & Chau, 2017); and (3) Provable robustness, in which the model classification
given adversarial examples is theoretically guaranteed to be consistent, i.e., a small perturbation in
the input does not change the predicted label (Cisse et al., 2017; Kolter & Wong, 2017).
On the one hand, private models, trained with existing privacy-preserving mechanisms (Abadi et al.,
2016; Shokri & Shmatikov, 2015; Phan et al., 2016; 2017b;a; Yu et al., 2019; Lee & Kifer, 2018),
are unshielded under adversarial examples. On the other hand, robust models, trained with adversar-
ial learning algorithms (with or without provable robustness to adversarial examples), do not offer
privacy protections to the training data. That one-sided approach poses serious risks to machine
learning-based systems; since adversaries can attack a deployed model by using both privacy infer-
ence attacks and adversarial examples. To be safe, a model must be i) private to protect the training
data, and ii) robust to adversarial examples. Unfortunately, there has not yet been research on how
to develop such a model, which thus remains a largely open challenge.
Simply combining existing DP-preserving mechanisms and provable robustness conditions (Cisse
et al., 2017; Kolter & Wong, 2017; Raghunathan et al., 2018) cannot solve the problem, for many
reasons. (a) Existing sensitivity bounds (Phan et al., 2016; 2017b;a) and designs (Yu et al., 2019;
Lee & Kifer, 2018) have not been developed to protect the training data in adversarial training. It
is obvious that using adversarial examples crafted from the private training data to train our models
introduces a previously unknown privacy risk, disclosing the participation of the benign examples
(Song et al., 2019). (b) There is an unrevealed interplay among DP preservation, adversarial learn-
ing, and robustness bounds. (c) Existing algorithms cannot be readily applied to address the trade-off
among model utility, privacy loss, and robustness. Therefore, theoretically bounding the robustness
of a model (which both protects the privacy and is robust against adversarial examples) is nontrivial.
Our Contributions. Motivated by this open problem, we propose to develop a novel differentially
private adversarial learning (DPAL) mechanism to: 1) preserve DP of the training data, 2) be prov-
ably and practically robust to adversarial examples, and 3) retain high model utility. In our mech-
1
Under review as a conference paper at ICLR 2020
anism, privacy-preserving noise is injected into inputs and hidden layers to achieve DP in learning
private model parameters (Theorem 1). Then, we incorporate ensemble adversarial learning into
our mechanism to improve the decision boundary under DP protections. To do this, we introduce
a concept of DP adversarial examples crafted using benign examples in the private training data
under DP guarantees (Eq. 9). To address the trade-off between model utility and privacy loss, we
propose a new DP adversarial objective function to tighten the model’s global sensitivity (Theorem
3); thus, we significantly reduce the amount of noise injected into our function, compared with ex-
isting works (Phan et al., 2016; 2017b;a). In addition, ensemble DP adversarial examples with a
dynamic perturbation size μ° are introduced into the training process to further improve the robust-
ness of our mechanism under different attack algorithms. An end-to-end privacy analysis shows
that, by slitting the private training data into disjoint and fixed batches across epochs, the privacy
budget in our DPAL is not accumulated across training steps (Theorem 4).
After preserving DP in learning model parameters, we establish a solid connection among privacy
preservation, adversarial learning, and provable robustness. Noise injected into different layers is
considered as a sequence of randomizing mechanisms, providing different levels of robustness. By
leveraging the sequential composition theory in DP (Dwork & Roth, 2014), we derive a novel gener-
alized robustness bound, which essentially is a composition of these levels of robustness (Theorem 5
and Proposition 1). To our knowledge, our mechanism establishes the first connection between DP
preservation and provable robustness against adversarial examples in adversarial learning. Rigor-
ous experiments conducted on MNIST and CIFAR-10 datasets (Lecun et al., 1998; Krizhevsky &
Hinton, 2009) show that our mechanism notably enhances the robustness of DP deep neural net-
works, compared with existing mechanisms.
2	Background
In this section, we revisit adversarial learning, DP, and our problem definition. Let D be a database
that contains N tuples, each of which contains data x ∈ [-1, 1]d and a ground-truth label y ∈ ZK,
with K possible categorical outcomes. Each y is a one-hot vector ofK categories y = {y1 , . . . , yK}.
A single true class label yx ∈ y given x ∈ D is assigned to only one of the K categories. On
input x and parameters θ, a model outputs class scores f : Rd → RK that maps d-dimensional
inputs x to a vector of scores f(x) = {f1 (x), . . . , fK (x)} s.t. ∀k ∈ [1, K] : fk (x) ∈ [0, 1] and
PkK=1 fk(x) = 1. The class with the highest score value is selected as the predicted label for the
data tuple, denoted as y(x) = maxk∈K fk (x). A loss function L(f (x), y) presents the penalty for
mismatching between the predicted values f(x) and original values y. For the sake of clarity, the
notations and terminologies frequently used in this paper are summarized in Table 1 (Appendix A).
Let us briefly revisit DP-preserving techniques in deep learning, starting with the definition of DP.
Definition 1 (, δ)-DP (Dwork et al., 2006). A randomized algorithm A fulfills (, δ)-DP, if for any
two databases D and D0 differing at most one tuple, and for all O ⊆ Range(A), we have:
P r[A(D) = O] ≤ eP r[A(D0) = O] + δ	(1)
A smaller enforces a stronger privacy guarantee.
Here, controls the amount by which the distributions induced by D and D0 may differ, δ is a
broken probability. DP also applies to general metrics ρ(D, D0) ≤ 1, where ρ can be lp-norms
(Chatzikokolakis et al., 2013). DP-preserving algorithms in deep learning can be categorized into
two lines: 1) introducing noise into gradients of parameters (Abadi et al., 2016; Shokri & Shmatikov,
2015; Abadi et al., 2017; Yu et al., 2019; Lee & Kifer, 2018; Phan et al., 2019), 2) injecting noise into
objective functions (Phan et al., 2016; 2017b;a), and 3) injecting noise into labels (Papernot et al.,
2018). In Lemmas 2 and 4, we will show that our mechanism achieves better sensitivity bounds
compared with existing works (Phan et al., 2016; 2017b;a).
Adversarial Learning. For some target model f and inputs (x, yx), the adversary’s goal is to find
an adversarial example xadv = x + α, where α is the perturbation introduced by the attacker, such
that: (1) xadv and x are close, and (2) the model misclassifies xadv, i.e., y(xadv) 6= y(x). In this paper,
we consider well-known lp∈{i,2,∞}-norm bounded attacks (Goodfellow et al., 2014b). Let lp(μ)=
{α ∈ Rd : ∣∣αkp ≤ μ} be the lp-norm ball of radius μ. One of the goals in adversarial learning is
to minimize the risk over adversarial examples: θ* = argminθ E(x,ytrue)〜D [maxgkp≤μ Lf (X +
α, θ), yx , where an attack is used to approximate solutions to the inner maximization problem,
2
Under review as a conference paper at ICLR 2020
and the outer minimization problem corresponds to training the model f with parameters θ over
these adversarial examples xadv = x + α. There are two basic adversarial example attacks. The
first one is a single-step algorithm, in which only a single gradient computation is required. For
instance, FGSM algorithm (Goodfellow et al., 2014b) finds adversarial examples by solving the
inner maximization maxgkp≤μ Lf (X + α, θ), yχ). The second one is an iterative algorithm, in
which multiple gradients are computed and updated. For instance, in (Kurakin et al., 2016a), FGSM
is applied multiple times with Tμ small steps, each of which has a size of μ∕Tμ.
To improve the robustness of models, prior work focused on two directions: 1) Producing cor-
rect predictions on adversarial examples, while not compromising the accuracy on legitimate inputs
(Kardan & Stanley, 2017; Matyasko & Chau, 2017; Wang et al., 2016; Papernot et al., 2016b;a; Gu
& Rigazio, 2014; Papernot & McDaniel, 2017; Hosseini et al., 2017); and 2) Detecting adversarial
examples (Metzen et al., 2017; Grosse et al., 2017; XU et al., 2017; Abbasi & Gagne, 2017; Gao
et al., 2017). Among existing solutions, adversarial training appears to hold the greatest promise for
learning robust models (Tramer et al., 2017). One of the well-known algorithms was proposed in
(Kurakin et al., 2016b). At every training step, new adversarial examples are generated and injected
into batches containing both benign and adversarial examples. The typical adversarial learning in
(Kurakin et al., 2016b) is presented in Alg. 2 (Appendix B).
DP and Provable Robustness. Recently, some algorithms (Cisse et al., 2017; Kolter & Wong,
2017; Raghunathan et al., 2018; Cohen et al., 2019; Li et al., 2018) have been proposed to derive
provable robustness, in which each prediction is guaranteed to be consistent under the perturbation
α, if a robustness condition is held. Given a benign example x, we focus on achieving a robustness
condition to attacks of lp(μ)-norm, as follows:
∀α ∈ lp(μ) : fk(x + α) > max fi(x + α)	(2)
i:i6=k
where k = y(x), indicating that a small perturbation α in the input does not change the predicted
label y(x). To achieve the robustness condition in Eq. 2, Lecuyer et al. (Lecuyer et al., 2018)
introduce an algorithm, called PixelDP. By considering an input x (e.g., images) as databases in
DP parlance, and individual features (e.g., pixels) as tuples, PixelDP shows that randomizing the
scoring functionf (x) to enforce DP on a small number of pixels in an image guarantees robustness
of predictions against adversarial examples. To randomizef (x), random noise σr is injected into
either input x or an arbitrary hidden layer, resulting in the following (r, δr)-PixelDP condition:
Lemma 1 (r, δr)-PixelDP (Lecuyer et al., 2018). Given a randomized scoring functionf(x) sat-
isfying (r, δr)-PixelDP w.r.t. a lp-norm metric, we have:
∀k, ∀α ∈ lp(1) : Efk (x) ≤ er Efk (x + α) + δr	(3)
where Efk (x) is the expected value offk(x), r is a predefined budget, δr is a broken probability.
At the prediction time, a certified robustness check is implemented for each prediction. A general-
ized robustness condition is proposed as follows:
Elbfk (x) > e2 r max EUbfi(X) + (1 + e r )δr	(4)
i:i6=k
where Elb and Eub are the lower and upper bounds of the expected value Ef (x) = n En f (χ)n,
derived from the Monte Carlo estimation with an η-confidence, given n is the number of invocations
of f(x) with independent draws in the noise σr. Passing the check for a given input guarantees that
no perturbation up to lp(1)-norm can change the model’s prediction. PixelDP does not preserve DP
in learning private parameters θ to protect the training data. That is different from our goal.
3 DPAL with Provab le Robustness
Our new DPAL mechanism is presented in Alg. 1. Our network (Figure 1) can be represented
as: f(x) = g(a(x, θ1), θ2), where a(x, θ1) is a feature representation learning model with x as
an input, and g will take the output of a(x, θ1) and return the class scores f (x). At a high level,
DPAL has three key components: (1) DP a(x, θ1), which is to preserve DP in learning the feature
representation model a(x, θ1); (2) DP Adversarial Learning, which focuses on preserving DP in
adversarial learning, given DP a(x, θ1); and (3) Provable Robustness and Verified Inferring, which
are to compute robustness bounds given an input at the inference time. In particular, given a deep
neural network f with model parameters θ (Lines 2-3), the network is trained over T training steps.
In each step, a batch of m perturbed training examples and a batch of m DP adversarial examples
derived from D are used to train our network (Lines 4-12).
3
Under review as a conference paper at ICLR 2020
Algorithm 1 DPAL Mechanism
Input: Database D, loss function L, parameters θ, batch size m, learning rate %t, privacy budgets: e1 and
C, robustness parameters: e「, ∆x, and ∆h, adversarial attack size μa, the number of invocations n, ensemble
attacks A, parameters ψ and ξ, and the size ∣h∏ | of h∏
1:	Draw Noise χι ・ [Lap(普)]d, χ — [Lap(等)]β, χ3 - [Lap(∆2)]lhπ|
2:	Randomly Initialize θ = {θ1, θ2}, B = {B1, . . . , BN/m} s.t. ∀B ∈ B : B is a batch with the size m,
Bi ∩ ... ∩ BN/m = 0, and Bi ∪ ... ∪ Bn. = D, B = {B 1,..., %/m } where Vi ∈ [1, N/m] : Bi =
{x — x + χm }x∈Bi
3:	Construct a deep network f with hidden layers {hi + 2m2,..., h∏}, where h∏ is the last hidden layer
4:	for t ∈ [T] do
5:	Take a batch Bi ∈ B where i = t%(N∕m), Assign Bt - Bi
6:	Ensemble DP Adversarial Examples:
7:	Draw Random Perturbation Value μt ∈ (0,1]
8:	Take a batch B>+ι ∈ B, Assign Btdv - 0
9:	for l ∈ A do	_	_
10:	Take the next batch B a ⊂ B i+i with the size m/|A|
11:	∀Xj ∈ Ba: Craft xjdv by using attack algorithm A[l] with l∞ (μt), Btdv - Btdv ∪ xjdv
12:	Descent: θi — θi — %t Vθ1 RBt UBadV (θi); θ2 ― θ2 — %t Vθ2 LBt UBadV (θ2) With the noise Xm
Output: (ci + ci/γχ + ci/γ + £2)-DP parameters θ = {θi, θ2}, robust model with an e「budget
13:	Verified Inferring: (an input x, attack size μa)
14:	Compute robustness size (κ + 3)maχ in Eq. 15 of X
15:	if (κ + φ)maχ ≥ μa then
16:	Return isRobust(x) = True, label k, (κ + 3)maχ
17:	else
18:	Return isRobust(x) = False, label k, (κ + φ)max
3.1	DP Feature Representation Learning
Our idea is to use auto-encoder to
simultaneously learn DP parameters θ1
and ensure that the output of a(x, θ1)
is DP. The reasons we choose an
auto-encoder are: (1) It is easier to
train, given its small size; and (2) It
can be reused for different predictive
models. A typical data reconstruction
function (cross-entropy), given a batch
Bt at the training step t of the input xi ,
is as follows:
Figure 1: An instance of DPAL.
d
RBt (θi)= X X [xijlog(1+ e-θ1jhi) + (1 —Xij)log(1 + eθ1jhi)]
xi ∈Bt j=i
(5)
where the transformation of xi is hi = θ1Txi, the hidden layer h1 of a(x, θ1) given the batch Bt is
denoted as h1Bt = {θ1Txi}xi∈Bt, and xei = θ1hi is the reconstruction of xi.
To preserve 1 -DP in learning θ1 where 1 is a privacy budget, we first derive the 1st-order
polynomial approximation of RBt (θ1) by applying Taylor Expansion (Arfken, 1985), denoted as
RBt (θ1). Then, Functional Mechanism (Zhang et al., 2012) is employed to inject noise into co-
efficients of the approximated function RBt (θ1)
j0) (θijhi Γ,
~
d21
xi∈Bt j=1 l=1	r=0
where F1j (z) = xij log(1 + e-z), F2j (z) = (1 - xij) log(1 + ez), we have that: RBt (θ1) =
PxGBt Pd=1 [ log 2 + θ1j (2 — Xj) hi]. In TRBt (θ1), parameters θ1j derived from the function op-
timization need to be e1-DP. To achieve that, Laplace noise mlLap( ∆R) is injected into coefficients
(1 - xij)hi, where ∆R is the sensitivity of RBt (θ1), as follows:
ReBt (θi) = X X[θij((；-Xij)hi + mLap(乎川=X [X(2θijhi)— xi xei i	(6)
xi ∈Bt j=i	i	xi ∈Bt j=i
4
Under review as a conference paper at ICLR 2020
To ensure that the computation of xei does not access the original data, we further inject Laplace
noise mLap(∆R) into Xi. This can be done as a preprocessing step for all the benign examples
in D to construct a set of disjoint batches B of perturbed benign examples (Lines 2 and 5). The
perturbed function now becomes:
d ι
RBt (θι)= X [X(2θijhi) - Xiei∖	⑺
Xi∈Bt j=1
where Xi = Xi + ~~Lap(^IR), hi = θTxi,hi = hi + m2Lap(^IR), and Xi = θ1hi.
Let us denote β as the number of neurons in h1, and hi is bounded in [-1, 1], the global sensitivity
∆R is as follows:
Lemma 2 The global sensitivity of R over any two neighboring batches, Bt and Bt0, is as follows:
∆R ≤ d(β + 2).
All the proofs are in our Appendix. By setting ∆R = d(β + 2), we show that the output of
a(∙), which is the perturbed affine transformation h1Bt = {θ1 Xi + m2Lap(∆R)}x,∈^, is (∈ι∕γ)-
DP, given Y = J/R- and ∣∣θ1∣∣1,1 is the maximum 1-norm of θ1,s columns (Operator norm,
2018). This is important to tighten the privacy budget consumption in computing the remaining
hidden layers g(a(X, θ1 ), θ2). In fact, without using additional information from the original data,
the computation of g(a(X, θι), θ2) is also (s∕γ )-DP (the post-processing property ofDP). Similarly,
We observe that the perturbation of a batch Bt = {Xi J Xi + J }xi∈Bt achieves (s∕Yx)-DP, with
Yx = ∆R. Note that we do not use the post-processing property of DP to estimate the DP guarantee
of h1Bt based upon the DP guarantee of Bt, since e∖∕γ < e1 ∕γx in practice. As a result, the (6ι∕γ)-
DP h1Bt provides a more rigorous DP protection to the computation of g(∙) and to the output layer.
Lemma 3 The computation of the affine transformation h1Bt is (∈ι∕γ) -DP and the computation of
the batch Bt as the input layer is (HYx)-DP.
The following Theorem shows that optimizing RB=(θ1) is (∈ι∕γx + EI)-DP in learning θ1 given an
(s∕Yx)-DP Bt batch.
Theorem 1 The optimization of RBt (θι) preserves (cι∕τx + CI)-DP in learning θɪ.
3.2	DP Adversarial Learning
To integrate adversarial learning, we first draft DP adversarial examples XjdV using perturbed benign
examples Xj, with an ensemble of attack algorithms A and a random perturbation budget μt ∈ (0,1],
at each step t (Lines 6-11). This will significantly enhances the robustness of our models under
different types of adversarial examples with an unknown adversarial attack size μ.
xadv = Xj + μ ∙ SignKxjLf (Xj,θ),y(Xj)))	(8)
with y(Xj∙) is the class prediction result of f (Xj) to avoid label leaking of the benign examples Xj
during the adversarial example crafting. Given a set of DP adversarial examples Badv, training the
auto-encoder with BadV preserves (cι∕Yx + CI)-Dp
Theorem 2 The optimization of RBad(θι) preserves (c∖∕Yx + CI)-DP in learning θι.
The proof of Theorem 2 is in Appendix H, Result 4. It can be extended to iterative attacks as
Xad(V=Xj ,χadt+ι=Xadtv+T ∙ Sign«唠，(/号:⑼方*")	⑼
where y(Xad；) is the prediction result of f (Xad；, θ), t ∈ [0, Tμ 一 1].
Second, we propose a novel DP adversarial objective function LBt (θ2), in which the loss function
L for benign examples is combined with an additional loss function Υ for DP adversarial examples,
5
Under review as a conference paper at ICLR 2020
to optimize the parameters θ2. The objective function LBt (θ2) is defined as follows:
LBt∪Badv (θ2) = m(i1+ ξ)
(X L(f(Xi,θ2),yi) + ξ X Υ(f(xadv,θ2),yj))	(10)
Xi∈Bt
where ξ is a hyper-parameter. For the sake of clarity, in Eq. 10, we denote yi and yj as the true class
labels yχi and yχj of examples Xi and Xj. Note that Xadv and Xjshare the same label yχj.
Now We are ready to preserve DP in objective functions Lff (Xi, θ2), y, and Y ff (Xadv, θ2), yj∙) in
order to achieve DP in learning θ2 . Since the objective functions use the true class labels yi and yj ,
we need to protect the labels at the output layer. Let us first present our approach to preserve DP in
the objective function L for benign examples. Given h∏ computed from the Xi through the network
with Wπ is the parameter at the last hidden layer hπ . Cross-entropy function is approximated as
K	11
LBt(θ2) UE Σ [h∏iW∏k-(h∏iW∏k)yik-2∣h∏iW∏k∣ + g(h∏iW∏k)2] U LIBt (θ2)-L2瓦施)
k=1 Xi
where LIBt 依)=PK=1 Pxi ∖h∏iW∏k - 2 ∣h∏i Wnk | + 8 (h∏i Wnk )2], and L® (θ2)=
PK=I Pxa(h∏iyik)W∏k. Based on the post-processing property of DP (DWOrk & Roth, 2014),
h∏Bt = {h∏i}χa∈Bt is (“/Y)-DP, since the computation of h]B∙方 is (s/Y)-DP (Lemma 3). As a
result, the optimization of the function L^Bt (θ2) does not disclose any information from the training
Λ .	Λ Pr(LIBt (θ2))	Pr(hπBt ) J eι /-y ∙	♦ KK ♦ K . iʌ "fɔ	AF0 TL	1
data, and Pr(L Bt @)) = 尸丁⑴ Bt) ≤ ee1/7, given neighboring batches Bt and Bt. Thus, we only
need to preserve €2-DP in the function L2B(θ2), which accesses the ground-truth label yijk. Given
coefficients h∏iyijk, the sensitivity ∆L2 of L2Bt (θ2) is computed as:
Lemma 4 Let Bt and Bt be neighboring batches of benign examples, we have the following in-
equality: ∆L2 ≤ 2|hn |, where |hn | is the number of hidden neurons in hn.
The sensitivity of our objective function is notably smaller than the state-of-the-art bound (Phan
et al., 2017a), which is crucial to improve our model utility. The perturbed functions are as follows:
LB,(θ2) = L1B, (θ2) -~L2Bt (θ2), where ~L2Bt (θ2) = XX (hniyik + -1 Lap(δl2 ))Wnk
t	t	t	t	m	€2
k = 1 Xi
Theorem 3 Algorithm 1 preserves (6ι∕γ + €2)-DP in the optimization of LB(θ2).
We apply the same technique to preserve (€1/Y + €2)-DP in the optimization of the function
Y(f (Xadv,θ2),yj) over the DP adversarial examples Xjdv ∈ B adv. As the perturbed functions L
and Y are always optimized given two disjoint batches Bt and Badv, the privacy budget used to
preserve DP in the adversarial objective function LBt (θ2) is (€1/Y + €2), following the parallel
composition property of DP (Dwork & Roth, 2014). The total budget to learn private parameters
θ = {θι,θ2} = argmin{θ1,θ2}(RBt」后；~丫(θι) + LBt∪Bdv(θ2)) is (∈1 + ∈ι∕γ + €2) (Line 12).
We have shown that our mechanism achieves DP at the batch level Bt ∪ Badv given a specific training
step t. By constructing disjoint and fixed batches from the training data D, we leverage both parallel
composition and post-processing properties of DP (Dwork & Roth, 2014) to extend the result to
(€1 + €i/yx + €i/y + €2)-DP in learning θ = {θι, θ2} on D across T training steps. There are three
key properties in our approach: (1) It only reads perturbed inputs Bt and perturbed coefficients
hi, which are DP across T training steps; (2) Given N/m disjoint batches in each epoch, for any
example X, X is included in one and only one batch, denoted Bx ∈ B. As a result, the DP guarantee
to X in D is equivalent to the DP guarantee to X in Bx; since the optimization using any other batches
does not affect the DP guarantee of X; and (3) All the batches are fixed across T training steps to
prevent additional privacy leakage, caused by generating new and overlapping batches (which are
considered overlapping datasets in the parlance of DP) in the typical training approach.
Theorem 4 Algorithm 1 achieves (€1 + €i/Yx + €i/y + €2)-DPparameters θ = {θι, θ2} on the
private training data D across T training steps.
6
Under review as a conference paper at ICLR 2020
3.3 Provable Robustness
Now, we establish the correlation between our mechanism and provable robustness. In the inference
time, to derive the provable robustness condition against adversarial examples x+α, i.e., ∀α ∈ lp(1),
PixelDP mechanism randomizes the scoring function f (x) by injecting robustness noise σr into
∆x	∆h
either input X or a hidden layer, i.e., x0 = X + Lap(^r) or h0 = h + Lap(U), where ∆χ and ∆h
are the sensitivities of x and h, measuring how much x and h can be changed given the perturbation
α ∈ lp(1) in the input X. Monte Carlo estimation of the expected values Ef (X), Elbfk (X), and
Eubfk(X) are used to derive the robustness condition in Eq. 4.
On the other hand, in our mechanism, the privacy noise σp includes Laplace noise injected into
both input x, i.e., mlLap(∆R), and its affine transformation h, i.e., mLap(∆R). Note that the
perturbation of L® 依)is equivalent to L® (θ2) = PK=I Pxi (h∏iyik Wnk+ml Lap( ∆L2 )W∏k).
This helps us to avoid injecting the noise directly into the coefficients hπiyik . The correlation
between our DP preservation and provable robustness lies in the correlation between the privacy
noise σp and the robustness noise σr .
We can derive a robustness bound by projecting the privacy noise σp on the scale of the ro-
∆x
bustness noise σr. Given the input x, let K = Δr / _r, in our mechanism We have that:
m1 r
x = x + Lap(κ∆χ∕6r). By applying a group privacy size K (DWork & Roth, 2014; Lecuyer
et al., 2018), the scoring function f(x) satisfies r-PixelDP given α ∈ lp(κ), or equivalently is
Kr-PixelDP given α ∈ lp(1), δr = 0. By applying Lemma 1, We have
∀k, ∀α ∈ lp(K) : Efk(x) ≤ erEfk(x+α),
or ∀k, ∀α ∈ lp(1) : Efk (x) ≤ e(κr) Efk (x + α)
With that, We can achieve a robustness condition against lp(K)-norm attacks, as folloWs:
2
Elbfk (x) > e2r max Eubfi (x)
(11)
(12)
With the probability ≥ ηx-confidence, derived from the Monte Carlo estimation of Ef (x).
Our mechanism also perturbs h (Eq. 7). Given 夕 =2m∆R/芬,We further have h = h + Lap(哈1).
Therefore, the scoring function f (x) also satisfies Er-PixeIDP given the perturbation α ∈ lp(φ). In
addition to the robustness to the lp (K)-norm attacks, We achieve an additional robustness bound in
Eq. 12 against lp(夕)-norm attacks. Similar to PixelDP, these robustness conditions can be achieved
as randomization processes in the inference time. They can be considered as two independent and
provable defensive mechanisms applied against two lp-norm attacks, i.e., lp(κ) and lp(φ).
One challenging question here is: “What is the general robustness bound, given K and 夕?” Intu-
itively, our model is robust to attacks with α ∈ lp(κ + 夕). We leverage the theory of sequential
composition in DP (DWork & Roth, 2014) to theoretically ansWer this question. Given S indepen-
dent mechanisms M1, . . . , MS, whose privacy guarantees are E1, . . . , ES -DP with α ∈ lp(1). Each
mechanism Ms, which takes the input x and outputs the value of f(x) with the Laplace noise only
injected to randomize the layer s (i.e., no randomization at any other layers), denoted as fs (x), is
defined as: ∀s ∈ [1, S], Msf(x) : Rd → fs (x) ∈ RK . We aim to derive a generalized robustness
of any composition scoring function f(M1, . . . , Ms|x) bounded in [0, 1], defined as follows:
S
S
f (Mi,..., Ms |x) : ɪɪ Msf (x) ⇔ f (Mi,..., MS |x) : Rd → ∏ fs(x) ∈ RK (13)
Our setting follows the sequential composition in DP (Dwork & Roth, 2014). Thus, we can prove
that the expected value Ef(Mi, . . . , MS |x) is insensitive to small perturbations α ∈ lp(1) in
Lemma 5, and we derive our composition of robustness in Theorem 5, as follows:
Lemma 5 Given S independent mechanisms Mi, . . . , MS, which are Ei, . . . , ES -DP w.r.t a
lp -norm metric, then the expected output value of any sequential function f of them, i.e.,
f(Mi, . . . , MS |x) ∈ [0, 1], meets the following property:
∀α ∈ lp(1): Ef (Mi,..., Ms |x) ≤ e(PS= es)Ef (Mi,..., MS |x + α)
7
Under review as a conference paper at ICLR 2020
Theorem 5 (Composition of Robustness) Given S independent mechanisms M1 , . . . , MS.
ʌ
ʌ
Given any sequential function f(M1, . . . , MS |x), and let Elb and Eub are lower and up-
per bounds with an η-confidence, for the Monte Carlo estimation of Ef (Mi,..., MS|x) =
n Pn(QS=I f s(x)n)∙ Forany input X,
n Pn f(M1,..., MS Ix)n
__________ -- ʌ ʌ .
2(PS=1 es) max E Ubfi(M1,..., MS |x),	(14)
i:i6=k
if ∃k ∈ K ： E ibfk(Mι,..., Ms Ix) >e
一 一一一 一一 ʌ ʌ ,.
then the predicted label k = arg maxk Efk (M1, . . . , MS Ix), is robust to adversarial exam-
ples x + α, ∀α ∈ lp(1), with probability ≥ η, by satisfying: Efk (M1, . . . , MSIx + α) >
maxi:i6=k Efi(M1, . . . , MSIx + α), which is the targeted robustness condition in Eq. 2.
It is worth noting that there is no ηs -confidence for each mechanism s, since we do not estimate the
expected value Efs (x) independently. To apply the composition of robustness in our mechanism, the
noise injections into the input x and its affine transformation h can be considered as two mechanisms
Mx and Mh, sequentially applied as (Mh(x), Mx(x)). When Mh(x) is applied by invoking f(x)
with independent draws in the noise χ2 , the noise χ1 injected into x is fixed; and vice-versa. By
applying group privacy (DWork & Roth, 2014) with sizes K and 夕，the scoring functions fx(x) and
f h(x), given Mx and Mh, are Ka-DP and 夕金-DP given α ∈ lp(1). With Theorem 5, We have a
generalized bound as follows:
Proposition 1 (DPAL Robustness). For any input x, if ∃k ∈ K ： Elbfk(Mh, MxIx) >
e2r maxi:i6=k EUbfi(Mh, MxIx) (i.e., Eq. 14), then the predicted label k of our function
f (Mh, Mx∣x) is robust to perturbations α ∈ lp(κ + 夕)with the probability ≥ η, by satisfying
ʌ

∀α ∈ Ip(K + 夕)：Efk(Mh, Mχ∣x + α) > maxEfi(Mh, Mχ∣x + α)
i:i6=k
3.4 Training and Verified Inferring
Our model is trained similarly to training typical deep neural netWorks. Parameters θ1 and θ2 are
independently updated by applying gradient descent (Line 12). Regarding the inference time, We
implement a verified inference procedure as a post-processing step (Lines 13-18). Our verified
inference returns a robustness size guarantee for each example x, Which is the maximal value of
κ + 夕，for which the robustness condition in Proposition 1 holds. Maximizing K + 夕 is equivalent
to maximizing the robustness epsilon 金,which is the only parameter controlling the size of K + 夕；
since, all the other hyper-parameters, i.e., ∆R, m, 1, 2, θ1, θ2, ∆rx, and ∆rh are fixed given a
well-trained model f (x):
(k + φ)max = max ^Rr (ɪ + -2h) s.t. Elbfk (x) > e2'r max EUbfi(X) (i.e., Eq. 14)	(15)
r m1 ∆rx	∆rh	i:i6=k
The prediction on an example x is robust to attacks up to (K + 夕)max. The failure probability 1-η
can be made arbitrarily small by increasing the number of invocations of f (x), with independent
draws in the noise. Similar to (Lecuyer et al., 2018), Hoeffding’s inequality is applied to bound
the approximation error in Efk(x) and to search for the robustness bound (K + φ)max. We use the
following sensitivity bounds ∆rh = β kθ1 k∞ where kθ1 k∞ is the maximum 1-norm of θ1 ’s rows,
and ∆x = μd for l∞ attacks. We also propose a new way to draw independent noise following the
distribution of χι + *Lap(0, ∆R/ψ) for the input x and 2χ2 + Lap(0, ∆R/ψ) for the transfor-
mation h, where χ1 and χ2 are the fixed noise used to train the network, and ψ is a parameter to
control the distribution shifts between training and inferring. This new Monte Carlo Estimation of
Ef (x) works better without affecting the DP bounds and the robustness (Appendix L).
4 Experimental Results
We have conducted an extensive experiment on the MNIST and CIFAR-10 datasets. We consider
the class of l∞-bounded adversaries to see whether our mechanism could retain high model utility,
while providing strong DP guarantees and protections against adversarial examples.
Baseline Approaches. Our DPAL mechanism is evaluated in comparison with state-of-the-art
mechanisms in: (1) DP-preserving algorithms in deep learning, i.e., DP-SGD (Abadi et al., 2016),
AdLM (Phan et al., 2017a); in (2) Provable robustness, i.e., PixelDP (Lecuyer et al., 2018); and in
8
Under review as a conference paper at ICLR 2020
(3) DP-preserving algorithms with provable robustness, i.e., SecureSGD given heterogeneous noise
(Phan et al., 2019), and SecureSGD-AGM (Phan et al., 2019) given the Analytic Gaussian Mecha-
nism (AGM) (Balle & Wang, 2018). To preserve DP, DP-SGD injects random noise into gradients of
parameters, while AdLM is a Functional Mechanism-based approach. PixelDP is one of the state-of-
the-art mechanisms providing provable robustness using DP bounds. SecureSGD is a combination
of PixelDP and DP-SGD with an advanced heterogeneous noise distribution; i.e., “more noise” is in-
jected into “more vulnerable” latent features, to improve the robustness. The baseline models share
the same design in our experiment. Four white-box attacks were used, including FGSM, I-FGSM,
Momentum Iterative Method (MIM) (Dong et al., 2017), and MadryEtAl (Madry et al., 2018).
Model Configuration. It is important to note that x ∈ [-1, 1]d in our setting, which is different
from a common setting, X ∈ [0,1]d. Thus, a given attack size μ0 = 0.3 in the setting of X ∈ [0,1]d
is equivalent to an attack size 2μ0 = 0.6 in our setting. The reason for using X ∈ [-1,1]d is to
achieve better model utility, while retaining the same global sensitivities to preserve DP, compared
with X ∈ [0, 1]d. Our model configurations are in Appendix M and our approximation error bound
analysis is presented in Appendix N. As in (Lecuyer et al., 2018), we apply two accuracy metrics:
P|it=e1st| isCorrect(Xi)	P|it=e1st| isCorrect(Xi) & isRobust(Xi)
conventional acc = -=----：---：-------; certified acc = -=----------：----：--------------
|test|	|test|
where |test| is the number of test cases, isCorrect(∙) returns 1 if the model makes a correct predic-
tion (else, returns 0), and isRobust(∙) returns 1 if the robustness size is larger than a given attack
size μa (else, returns 0). Our task of validation focuses on shedding light into the interplay among
model utility, privacy loss, and robustness bounds, by learning 1) the impact of the privacy budget
Et = (∈ι + eι∕γχ + e∖∕γ + €2), and 2) the impact of attack sizes μ0. All statistical tests are 2-tail
t-tests. All experimental Figures are in Appendix O.
Results on the MNIST Dataset. Figure 2 illustrates the conventional accuracy of each model as a
function of the privacy budget Et on the MNIST dataset under l∞(μa)-norm attacks, with μ0 = 0.2
(a pretty strong attack). It is clear that our DPAL outperforms AdLM, DP-SGD, SecureSGD, and
SecureSGD-AGM, in all cases, with p < 1.32e - 4. On average, we register a 22.36% improvement
over SecureSGD (p < 1.32e - 4), a 46.84% improvement over SecureSGD-AGM (p < 1.83e - 6),
a 56.21% improvement over AdLM (p < 2.05e - 10), and a 77.26% improvement over DP-SGD
(p < 5.20e - 14), given our DPAL mechanism. AdLM and DP-SGD achieve the worst conventional
accuracies. There is no guarantee provided in AdLM and DP-SGD. Thus, the accuracy of the AdLM
and DPSGD algorithms seem to show no effect against adversarial examples, when the privacy
budget is varied. This is in contrast to our DPAL model, the SecureSGD model, and the SecureSGD-
AGM model, whose accuracies are proportional to the privacy budget.
When the privacy budget Et = 0.2 (a tight DP protection), there are significant drops, in terms
of conventional accuracy, given the baseline approaches. By contrast, our DPAL mechanism only
shows a small degradation in the conventional accuracy (6.89%, from 89.59% to 82.7%), compared
with a 37% drop in SecureSGD (from 78.64% to 41.64%), and a 32.89% drop in SecureSGD-
AGM (from 44.1% to 11.2%) on average, when the privacy budget Et goes from 2.0 to 0.2. At
Et = 0.2, our DPAL mechanism achieves 82.7%, compared with 11.2% and 41.64% correspondingly
for SecureSGD-AGM and SecureSGD. This is an important result, showing the ability to offer tight
DP protections under adversarial example attacks in our model, compared with existing algorithms.
•	Figure 4 presents the conventional accuracy of each model as a function of the attack size μ° on
the MNIST dataset, under a strong DP guarantee, Et = 0.2. It is clear that our DPAL mechanism
outperforms the baseline approaches in all cases. On average, our DPAL model improves 44.91%
over SecureSGD (p < 7.43e - 31), a 61.13% over SecureSGD-AGM (p < 2.56e - 22), a 52.21%
over AdLM (p < 2.81e - 23), and a 62.20% over DP-SGD (p < 2.57e - 22). More importantly,
our DPAL model is resistant to different adversarial example algorithms with different attack sizes.
When μa ≥ 0.2, AdLM, DP-SGD, SecureSGD, and SecureSGD-AGM become defenseless. We
further register significantly drops in terms of accuracy, when μ° is increased from 0.05 (a weak
attack) to 0.6 (a strong attack), i.e., 19.87% on average given our DPAL, across all attacks, compared
with 27.76% (AdLM), 29.79% (DP-SGD), 34.14% (SecureSGD-AGM), and 17.07% (SecureSGD).
•	Figure 6 demonstrates the certified accuracy as a function of μ0. The privacy budget is set to 1.0,
offering a reasonable privacy protection. In PixelDP, the construction attack bound Er is set to 0.1,
which is a pretty reasonable defense. With (small perturbation) μ0 ≤ 0.2, PixelDP achieves better
certified accuracies under all attacks; since PixelDP does not preserve DP to protect the training
9
Under review as a conference paper at ICLR 2020
data, compared with other models. Meanwhile, our DPAL model outperforms all the other models
when μa ≥ 0.3, indicating a stronger defense to more aggressive attacks. More importantly, our
DPAL has a consistent certified accuracy to different attacks given different attack sizes, compared
with baseline approaches. In fact, when μ° is increased from 0.05 to 0.6, our DPAL shows a small
drop (11.88% on average, from 84.29%(μα = 0.05) to 72.41%(μa = 0.6)), compared with a huge
drop of the PixelDP, i.e., from 94.19%(μα = 0.05) to 9.08%(μa = 0.6) on average under I-FGSM,
MIM, and MadryEtAl attacks, and to 77.47%(μ0 = 0.6) under FGSM attack. Similarly, We also
register significant drops in terms of certified accuracy for SecureSGD (78.74%, from 86.74% to
7.99%) and SecureSGD-AGM (81.97%, from 87.23% to 5.26%) on average. This is promising.
Our key observations are as follows. (1) Incorporating ensemble adversarial learning into DP preser-
vation, with tightened sensitivity bounds and a random perturbation size μt ∈ [0,1] at each training
step, does enhance the consistency, robustness, and accuracy of our model against different attack
algorithms with different levels of perturbations. (2) Our DPAL model outperforms baseline algo-
rithms, including both DP-preserving and non-private approaches, in terms of conventional accuracy
and certified accuracy in most of the cases. It is clear that existing DP-preserving approaches have
not been designed to withstand against adversarial examples.
Results on the CIFAR-10 Dataset further strengthen our observations. In Figure 3, our DPAL
clearly outperforms baseline models in all cases (p < 6.17e - 9), especially when the privacy budget
is small (t < 4), yielding strong privacy protections. On average conventional accuracy, our DPAL
mechanism has an improvement of 10.42% over SecureSGD (p < 2.59e - 7), an improvement of
14.08% over SecureSGD-AGM (p < 5.03e - 9), an improvement of 29.22% over AdLM (p <
5.28e - 26), and a 14.62% improvement over DP-SGD (p < 4.31e - 9). When the privacy budget
is increased from 2 to 10, the conventional accuracy of our DPAL model increases from 42.02%
to 46.76%, showing a 4.74% improvement on average. However, the conventional accuracy of
our model under adversarial example attacks is still low, i.e., 44.22% on average given the privacy
budget at 2.0. This opens a long-term research avenue to achieve better robustness under strong
privacy guarantees in adversarial learning.
•	The accuracy of our model is consistent given different attacks with different adversarial perturba-
tions μa under a rigorous DP protection (Q = 2.0), compared with baseline approaches (Figure 5).
In fact, when the attack size μ° increases from 0.05 to 0.5, the conventional accuracies of the base-
line approaches are remarkably reduced, i.e., a drop of 25.26% on average given the most effective
baseline approach, SecureSGD. Meanwhile, there is a much smaller degradation (4.79% on average)
in terms of the conventional accuracy observed in our DPAL model. Our model also achieves better
accuracies compared with baseline approaches in all cases (p < 8.2e - 10). Figure 7 further shows
that our DPAL model is more accurate than baseline approaches (i.e., r is set to 0.1 in PixelDP) in
terms of certified accuracy in all cases, with a tight privacy budget set to 2.0 (p < 2.04e - 18). We
register an improvement of 21.01% in our DPAL model given the certified accuracy over SecureSGD
model, which is the most effective baseline approach (p < 2.04e - 18).
5	Conclusion
In this paper, we established a connection among DP preservation to protect the training data, adver-
sarial learning, and provable robustness. A sequential composition robustness theory was introduced
to generalize robustness given any sequential and bounded function of independent defensive mech-
anisms. An original DP-preserving mechanism was designed to address the trade-off among model
utility, privacy loss, and robustness by tightening the global sensitivity bounds. A new Monte Carlo
Estimation was proposed to improve and stabilize the estimation of the robustness bounds; thus
improving the certified accuracy under adversarial example attacks.
However, there are several limitations. First, the accuracy of our model under adversarial example
attacks is still very low. Second, the mechanism scalability is dependent on the model structures.
Third, further study is needed to address the threats from adversarial examples crafted by unseen
attack algorithms. Fourth, in this study, our goal is to illustrate the difficulties in providing DP
protections to the training data in adversarial learning with robustness bounds. The problem is more
challenging when working with complex and large networks, such as ResNet (He et al., 2015),
VGG16 (Zhang et al., 2015), LSTM (Hochreiter & Schmidhuber, 1997), and GAN (Goodfellow
et al., 2014a). Fifth, there can be alternative approaches to draft and to use DP adversarial examples.
Addressing these limitations needs significant efforts from both research and practice communities.
10
Under review as a conference paper at ICLR 2020
References
Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. arXiv:1607.00133, 2016.
Martin Abadi, Ulfar Erlingsson, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Nicolas Pa-
pernot, Kunal Talwar, and Li Zhang. On the protection of private information in machine learning
systems: Two recent approches. In 2017 IEEE 30th Computer Security Foundations Symposium
(CSF), pp. 1-6. IEEE, 2017.
Mahdieh Abbasi and Christian Gagne. Robustness to adversarial examples through an ensemble of
specialists. CoRR, abs/1702.06856, 2017. URL http://arxiv.org/abs/1702.06856.
T. Apostol. Calculus. John Wiley & Sons, 1967.
GEORGE Arfken. In Mathematical Methods for Physicists (Third Edition). Academic Press, 1985.
Borja Balle and Yu-Xiang Wang. Improving the Gaussian mechanism for differential privacy: Ana-
lytical calibration and optimal denoising. In Jennifer Dy and Andreas Krause (eds.), Proceedings
of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Ma-
chine Learning Research,pp. 394T03, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018.
PMLR. URL http://proceedings.mlr.press/v80/balle18a.html.
N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE
Symposium on Security and Privacy (SP), pp. 39-57, May 2017. doi: 10.1109/SP.2017.49.
Konstantinos Chatzikokolakis, Miguel E. Andres, Nicolas Emilio Bordenabe, and Catuscia
Palamidessi. Broadening the scope of differential privacy using metrics. In Emiliano De Cristo-
faro and Matthew Wright (eds.), Privacy Enhancing Technologies, pp. 82-102, 2013.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parse-
val networks: Improving robustness to adversarial examples. In Doina Precup and Yee Whye
Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70
of Proceedings of Machine Learning Research, pp. 854-863, International Convention Centre,
Sydney, Australia, 06-11 Aug 2017.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pp. 1310-1320, Long Beach, California, USA, 09-15 Jun 2019.
Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Xiaolin Hu, and Jun Zhu. Discovering adversarial
examples with momentum. CoRR, abs/1710.06081, 2017.
C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data
analysis. Theory of Cryptography, pp. 265-284, 2006.
Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Found. Trends
Theor. Comput. Sci., 9(3&#8211;4):211-407, August 2014. ISSN 1551-305X. doi: 10.1561/
0400000042. URL http://dx.doi.org/10.1561/0400000042.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit con-
fidence information and basic countermeasures. In Proceedings of the 22Nd ACM SIGSAC
Conference on Computer and Communications Security, CCS ’15, pp. 1322-1333, 2015. doi:
10.1145/2810103.2813677.
Ji Gao, Beilun Wang, and Yanjun Qi. Deepmask: Masking DNN models for robustness against
adversarial samples. CoRR, abs/1702.06763, 2017. URL http://arxiv.org/abs/1702.
06763.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems 27, pp. 2672-2680. 2014a.
11
Under review as a conference paper at ICLR 2020
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. CoRR, abs/1412.6572, 2014b.
Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick D. McDaniel.
On the (statistical) detection of adversarial examples. CoRR, abs/1702.06280, 2017. URL http:
//arxiv.org/abs/1702.06280.
Shixiang Gu and Luca Rigazio. Towards deep neural network architectures robust to adversarial
examples. CoRR, abs/1412.5068, 2014. URL http://arxiv.org/abs/1412.5068.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. CoRR, abs/1512.03385, 2015.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780,1997. doi: 10.1162/neco.1997.9.8.1735.
Hossein Hosseini, Yize Chen, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. Block-
ing transferability of adversarial examPles in black-box learning systems. arXiv preprint
arXiv:1703.04318, 2017.
N. Kardan and K. O. Stanley. Mitigating fooling with comPetitive overcomPlete outPut layer neural
networks. In 2017 International Joint Conference on Neural Networks (IJCNN), PP. 518-525,
2017.
J. Zico Kolter and Eric Wong. Provable defenses against adversarial examPles via the convex outer
adversarial PolytoPe. CoRR, abs/1711.00851, 2017. URL http://arxiv.org/abs/1711.
00851.
Alex Krizhevsky and Geoffrey Hinton. Learning multiPle layers of features from tiny images. 2009.
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examPles in the Physical world.
CoRR, abs/1607.02533, 2016a.
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. CoRR,
abs/1611.01236, 2016b.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning aPPlied to document recog-
nition. Proceedings of the IEEE, 86(11):2278-2324, 1998. doi: 10.1109/5.726791.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examPles with differential Privacy. In arXiv:1802.03471, 2018. URL
https://arxiv.org/abs/1802.03471.
Jaewoo Lee and Daniel Kifer. Concentrated differentially Private gradient descent with adaPtive
Per-iteration Privacy budget. In Proceedings of the 24th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, PP. 1656-1665, 2018.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Second-order adversarial attack
and certifiable robustness. CoRR, abs/1809.03113, 2018. URL http://arxiv.org/abs/
1809.03113.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris TsiPras, and Adrian Vladu. To-
wards deeP learning models resistant to adversarial attacks. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.
A. Matyasko and L. P. Chau. Margin maximization for robust classification using deeP learning. In
2017 International Joint Conference on Neural Networks (IJCNN), PP. 300-307, 2017.
Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting adversar-
ial Perturbations. In Proceedings of 5th International Conference on Learning Representations
(ICLR), 2017. URL https://arxiv.org/abs/1702.04267.
N. PaPernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The limitations of deeP
learning in adversarial settings. In 2016 IEEE European Symposium on Security and Privacy, PP.
372-387, March 2016a. doi: 10.1109/EuroSP.2016.36.
12
Under review as a conference paper at ICLR 2020
N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial
perturbations against deep neural networks. In 2016 IEEE Symposium on Security and Privacy
(SP),pp.582-597,May2016b. doi: 10.1109/SP.2016.41.
Nicolas Papernot and Patrick McDaniel. Extending defensive distillation. arXiv preprint
arXiv:1705.05264, 2017.
Nicolas PaPemoL Shuang Song, Ilya Mironov, Ananth RaghUnathan, KUnal Talwar, and UJlfar Er-
lingsson. Scalable private learning with pate. arXiv preprint arXiv:1802.08908, 2018.
N. Phan, X. WU, H. HU, and D. DoU. Adaptive laplace mechanism: Differential privacy preservation
in deep learning. In IEEE ICDM’17, 2017a.
NhatHai Phan, YUe Wang, Xintao WU, and Dejing DoU. Differential privacy preservation for deep
auto-encoders: an application of human behavior prediction. In AΛAi,16, pp. 1309-1316, 2016.
NhatHai Phan, Xintao WU, and Dejing DoU. Preserving differential privacy in convolUtional deep
belief networks. Machine Learning, 2017b. doi: 10.1007/s10994-017-5656-2.
NhatHai Phan, Minh N. Vu, Yang Liu, Ruoming Jin, Dejing Dou, Xintao Wu, and My T. Thai. Het-
erogeneous gaussian mechanism: Preserving differential privacy in deep learning with provable
robustness. In Proceedings of the 28th International Joint Conference on Λrtificial Intelligence
(IJCA119),pp. 47534759,10-16 August 2019.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial exam-
ples. CoRR, abs/1801.09344, 2018. URL http://arxiv.org/abs/1801.09344.
Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In CCS’15, pp. 1310-1321,
2015.
Liwei Song, Reza Shokri, and Prateek Mittal. Privacy Risks of Securing Machine Learning Models
against Adversarial Examples. arXiv e-prints, art. arXiv:1905.10291, May 2019.
Operator norm. Operator norm, 2018. URL https://en.wikipedia.org/wiki/
Operator_norm.
Florian Tramer, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick McDaniel. Ensemble
adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204, 2017.
Qinglong Wang, Wenbo Guo, Kaixuan Zhang, Alexander G. Ororbia II, Xinyu Xing, C. Lee Giles,
and Xue Liu. Learning adversary-resistant deep neural networks. CoRR, abs/1612.01401, 2016.
Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep
neural networks. CoRR, abs/1704.01155, 2017. URL http://arxiv.org/abs/1704.
01155.
L. Yu, L. Liu, C. Pu, M. Gursoy, and S. Truex. Differentially private model publishing for deep
learning. In 2019 IEEE Symposium on Security and Privacy (SP), pp. 326-343, 2019.
Jun Zhang, Zhenjie Zhang, Xiaokui Xiao, Yin Yang, and Marianne Winslett. Functional mechanism:
regression analysis under differential privacy. PVLDB, 5(11):1364-1375, 2012.
Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun. Accelerating very deep convolutional
networks for classification and detection. CoRR, abs/1505.06798, 2015.
13
Under review as a conference paper at ICLR 2020
A Notations and Terminologies
Table 1: Notations and Terminologies.
D and X	Training data with benign examples x ∈ [-1,1]d
	y = {yι,... ,yκ}		One-hot label vector of K categories
f : Rd → RK	Function/model f that maps inputs X to a vector of scores f (x) = {∕i(x), ..., fK (x)}
	yx ∈ y		A single true class label of example X
y(x) = maxk∈K fk(x)	Predicted label for the example X given the function f
XadV = X + α	Adversarial example where α is the perturbation
lp(μ) = {α ∈ Rd : kαkp ≤ μ}	The Ip -norm ball of attack radius μ
(Cr , δr)	Robustness budget Cr and broken probability δr
Efk(X)	一	The expected value of fk (x)
ʌ . ^ Elb and EUb	Lower and upper bounds of the expected value Ef (x) = 1 Pn f (X)n
a(x,θι)	Feature representation learning model with X and parameters θι
Bt	A batch of benign examples Xi
RBt(θl)	Data reconstruction function given Bt in a(X, θι)
h1Bt = {θT xi}xi∈Bt	The values of all hidden neurons in the hidden layer hi of a(X, θι) given the batch Bt
R Bt(θι)and Rmt(θι) 一	Approximated and perturbed functions of RBt (θi)
Xi and Xi	Perturbed and reconstructed inputs Xi
	Δr = d(β + 2)		Sensitivity of the approximated function RBt (θi)
h1Bt			Perturbed affine transformation hiBt
Xdv = XjV + "ap( ∆R)	DP adversarial examples crafting from benign example Xj
Bt and BtdV	SetS of PertUrbed inputs X and DP adversarial examples XjdV
	* LBt (θ2 )		Loss function of perturbed benign examples in Bt, given θ2
Yf (Xdv,θ2),yj)	Loss function of DP adversarial examples Xadv, given θ2
	LBt (θ2 )		DP loss function for perturbed benign examples Bt
	L2Bt(。2		A Part of the loss function LB 他)that needs to be DP
f(M1,..., Ms∣X)	Composition scoring function given independent randomizing mechanisms M1,..., Ms
∆X and ∆r	Sensitivities of X and h, given the perturbation α ∈ lp(1)
(q + α∕γx + α∕γ + ◎)	Privacy budget to protect the training data D
(K + ψ)max	Robustness size guarantee given an input X at the inference time
B Pseudo-code of Adversarial Training (Kurakin et al., 2016b)
Given a loss function:
L(θ) = m +1 ξm (X L(f(χi,θ),yi) + ξ X γ(f(χadv,θ),yj))	(16)
1	2 xi ∈Bt	xadv∈Badv
xj	t
where m1 and m2 correspondingly are the numbers of examples in Bt and Btadv at each training
step.
Algorithm 2 Adversarial Training (Kurakin et al., 2016b)
Input: Database D, loss function L, parameters θ, batch sizes mi and m2, learning rate %t, param-
eter ξ
1: Initialize θ randomly
2: for t ∈ [T] do
3: Take a random batch Bt with the size m1, and a random batch Ba with the size m2
4: Craft adversarial examples Btadv = {xajdv}j∈[1,m2] from corresponding benign examples xj ∈
Ba
5:	Descent: θ — θ - %NθL(θ)
14
Under review as a conference paper at ICLR 2020
C Proof of Lemma 2
Proof 1 Assume that Bt and Bt0 differ in the last tuple, xm (x0m). Then,
d
ZR = Xhu X 2hi- X 2hi∣lι + Il X Xij- X xij∣lιi
j=1	xi∈Bt	x0i ∈Bt0	xi ∈Bt	x0i∈Bt0
d1
≤ 2max^(k2hikι + l∣xijkι) ≤ d(β + 2)
xi j=1
D Proof of Lemma 3
Proof 2 Regarding the computation of h]B = {θι xi}χ,∈b针 we can see that hi = θι Xi is a linear
function of x. The sensitivity of a function h is defined as the maximum change in output, that can
be generated by a change in the input (Lecuyer et al., 2018). Therefore, the global sensitivity of h1
can be computed as follows:
∆h1
kP%∈Bt θTXi- Pχi∈Bt θTXikl
k∑Xi∈Bt Xi- ∑xi∈Bt Xiki
≤ max
xi∈Bt
kθT Xiki
kXiki
≤kθT ki,i
following matrix norms (Operator norm, 2018): ∣∣θι ki,i is the maximum 1-norm of θi s columns.
By injecting Laplace noise Lap(δ^1 ) into hiBt, i.e., h-iBt = {θι Xi + Lap(~h1 )}χi∈b产 we Can
preserve Ei-DP in the computation of h↑Bt. Let us set △% = ∣∣θTki,i, Y = m∆R , and χ drawn
as a Laplace noise [Lap(δr)]β, in our mechanism, the perturbed affine transformation 也豆= is
presented as:
hiBt = {θi Xi +	}χi∈瓦={θi Xi + —[Lap(--^)]β}Xi∈Bt
m	m	Ei
={θT Xi + [Lap(Yz1 )]β }χi∈Bt = {θT Xi + [Lap(Zl )]β }5。
Ei	i t	E∖/ γ i
This results in an (∈ι∕γ)-DP affine transformation hiBt = {θTXi + [Lap(δ∕y)]β}χ.∈b广
Similarly, the perturbed inputs Bt = {Xi}χi∈Bt = {Xi + X1 }χi∈Bt = {Xi + [Lap( //^ )]d}χi∈Bt,
where △x is the sensitivity measuring the maximum change in the input layer that can be generated
by a change in the batch Bt and Yx = mR. Following (Lecuyer et al., 2018), ∆χ can be computed
kPχi∈Bt Xi-Pxi∈B0 Xiki
kPχi∈Bt Xi-Pxi∈B0 xi k 1
as follows: △x
1. As a result, the computation of B t is (e∖∕ γx)-DP.
Consequently, Lemma 3 does hold.
E Proof of Theorem 1
Proof 3 Given χi drawn as a Laplace noise [Lap(δr)]d and X drawn as a Laplace noise
[Lap(δr )]β, the perturbation ofthe coefficient φ ∈ Φ = { 2hi, Xi}, denoted as φ, can be rewritten
as follows:
for φ ∈ {Xi} : φ = X (φχi + χi) = X φχi + Xi = X φχi + [LaP( —)]d
m	Ei
χi∈B	χi∈B	χi∈B
forφ ∈ {2 hi}:φ = X 2(hi+~2))=X (φχi+χ2)
mm
χi∈B	χi∈B
=X φXi + X2 = X φXi + [LaP(ZR)]β
χi∈B	χi∈B	i
15
Under review as a conference paper at ICLR 2020
we have
d
Pr(RBt (θι)) = YY exP (-
j=1 φ∈Φ
EIkPxi∈Bt φXi - φk1
Δr
∆R is set to d(β + 2), we have that:
Pr(RBt (θι)) = Qd=ιQφ∈Φ exp ( -	xi∈∆Rφxi-φk1 )
Pr(RBO (θI))	Qd Q e ( - e1kPχi∈B0 φxi-φk1)
t	Uj=Iuφ∈Φ expI	∆r	)
d
≤ YY exp(⅛ IlX φxi- X φxi∣lι)
j=1 φ∈Φ	R xi∈Bt	x0i∈Bt0
d
≤YY exp(∆⅛2 m∈B
R xi∈Bt
j=1 φ∈Φ
∣∣φxi∣∣1) ≤ exp('1d∆；2)) = exP(EI)
(17)
Consequently, the computation of RBt (θι) preserves ∈ι-DP in Alg. 1. In addition, the parameter
optimization of RBt (θι) only uses the perturbed data Bt, which is (ei/Yx)-DP (Lemma 3), in the
computations of hi, hi, ei, parameter gradients, and gradient descents at each step. These opera-
tions do not access the original dataset Bt; therefore, they do not incur any additional information
from the original data (the post-processing property in DP Dwork & Roth (2014)). As a result, the
total privacy budget to learn the perturbed optimal parameters θι inAlg. 1 is (6ι∕γx + EI)-DP
F Proof of Lemma 4
Proof 4 Assume that Bt and Bt differ in the last tuple, and Xm (Xm) be the last tuple in Bt (Bt),
we have that
KK
∆L2 = XiX (h∏iyik) - X (h∏iy0k)∣∣ι= X ∣∣hπmymk - hπmymk ∣∣1
Since ymk and ymm卜 are one-hot encoding, we have that Δl2 ≤ 2maxχi kh∏ikι. Given h∏i ∈
[-1, 1], we have
∆L2 ≤ 2∣h∏|	(18)
Lemma 4 does hold.
G Proof of Theorem 3
Proof 5 Let Bt and Bt be neighboring batches ofbenign examples, and χ3 drawn as Laplace noise
[Lap( δl2 )]lhπ |, the perturbations of the COeffiCientS h∏iyik can be rewritten as:
h∏i 国 ik = X(h∏iyik + χ3) = X(h∏iyik) + [Lap( ^L2 )]lhπ 1
m^	m ⅜-z	E2
xi	xi
Since all the COeffiCientS are perturbed, andgiven Δl2 = 2∣h∏ |, we have that
Pr(LBt (θ2)) = Pr(LIBt 阴))	PrgBt ⑶)
Pr(LBt(θ2)) = Pr(LIBt闻)) × Pr(LB^
K	/ e2∣∣Pχ. hπiyik-hπiyik k 1 >
≤ e£ι∕γ X exp(-	∆L2 _ _	-)
一	乙 / e2kPχ0 hπiyik-hπiyikk1、
k=1 exP(-----FL2-----------)
K
≤ e□∕γ Xexp(除∣∣ X h∏iyik - X h∏iyik∣∣ 1)
k=1	L2	Xi	Xi
≤ e'l/Yexp(2maxkh∏ikι) = e'Mγ+'2
∆L2	xi
16
Under review as a conference paper at ICLR 2020
The computation of L2Bt (θ2) preserves (∈ι∕γ + €2)-differential privacy. The optimization of
L2Bt (θ2) does not access additional information from the original input Xi ∈ Bt. Consequently,
the optimal perturbed parameters θ2 derivedfrom L2Bt (θ2) are (∈ι∕γ + €2)-DP
H Proof of Theorem 4
Proof 6 First, we optimize for a single draw of noise during training (Line 3) and all the batches of
perturbed benign examples are disjoint and fixed across epochs. As a result, the computation of Xi is
equivalent to a data preprocessing step with DP, which does not incur any additional privacy budget
consumption over T training steps (the post-processing property of DP) (Result 1). That is different
from repeatedly applying a DP mechanism on either the same or overlapping datasets causing the
accumulation of the privacy budget.
Now, we show that our algorithm achieves DP at the dataset level D. Let us consider the compu-
tation of the first hidden layer, given any two neighboring datasets D and D0 differing at most one
tuple Xe ∈ D and Xe ∈ D0. Forany O = QNm oi ∈ QNm h]Bi (∈ Rβ×m), we have that
P (hlD = O)	P (h1 瓦=OI) ...	P (h1BN∕m	= QNlmm
P(h1D0 = O)	P(hlBl	= o1) …P(hlBN/m	= (ONIm
By having disjoint and fixed batches, we have that:
一.二 =	二 一.二/	=0 /	二/
∃!B ∈ B s.t.	Xe ∈ B and ∃!B0 ∈ B s.t. X0e	∈ B0
(19)
(20)
From Eqs. 19, 20, and Lemma 3, we have that
∀B ∈ B,B = B : B = B0 ⇒ P"B = , = 1
P (hιB, = o)
Eqs. 20 and 21 ⇒
P(hiD = O) _ P(hιB = O)
P (hiD，= O) — P (RB, = O)
≤ e1Iγ
(21)
(22)
As a result, the computation of hiD is (e` / γ) -DP given the data D, since the Eq. 22 does hold for
any tuple Xe ∈ D. That is consistent with the parallel composition property ofDP, in which batches
can be considered disjoint datasets given h]B as a DP mechanism (DwOrk & Roth, 2014).
This does hold across epochs, since batches B are disjoint andfixed among epochs. At each training
step t ∈ [1,T], the computation of h]B∙= does not access the original data. It only reads the per-
turbed batch ofinputs Bt, which is (eι∕γχ)-DP (Lemma 3). Following the post-processing property
in DP (DWOrk & Roth, 2014), the computation of h-ιBt does not incur any additional information
from the original data across T training steps. (Result 2)
Similarly, we show that the optimization of the function RBt (θι) is (∈ι∕γχ + €1)-DP across T
training steps. As in Theorem 1 and Proof 3, we have that Pr(RB(θι)) = Q；=i Qφ∈φ exp ( 一
‘1k'"代黑X Φ∣"), where B ∈ B. Given any two perturbed neighboring datasets D and D differ-
ing at most one tuple Xe ∈ D and Xe ∈ D:
Pr(RD(θι)) = Pr(RBI (θι))…Pr(RBN/m(θι))
Pr(RDO(θι)) = Pr(RBI (θι))…Pr(RBN/m(θι))
From Eqs. 20, 23, and Theorem 1, we have that
(23)
∀B ∈ B,B = B: B = B0 ⇒ P 股 (θ1)! = 1	(24)
P (RB, (θι))
Eqs. 23 and 24 ⇒
P (RD (θl))
P (RD，(θl))
P (RB (θl)) ≤ eeι
P (RB, (θι))一
(25)
17
Under review as a conference paper at ICLR 2020
As a result, the optimization of RD(θι) is (e∖∕γχ + €1)-DP given the data D (which is € \/γχ-DP
(Lemma 3)), since the Eq. 25 does hold for any tuple Xe ∈ D. This is consistent with the parallel
composition property in DP (Dwork & Roth, 2014), in which batches can be considered disjoint
datasets and the optimization of the function on one batch doesnot affect the privacy guarantee in
any other batch. In addition, ∀t ∈ [1, T], the optimization of RBt (θι) does not use any additional
information from the original data D. Consequently, the privacy budget is (tι∕γχ + €1) across T
training steps, following the post-processing property in DP (Dwork & Roth, 2014) (Result 3).
Similarly, we can also prove that optimizing the data reconstruction function RBadv (θι) given the
DP adversarial examples crafted in Eqs. 8 and 9, i.e., Xjdv, is also (一∖∕γχ + €1)-DP given t ∈ [1, T]
on the training data D. First, DP adversarial examples Xadv are crafted from perturbed benign
examples Xj. As a result, the computation of the batch BeadV of DP adversarial examples is 1)
(eι∕γx)-DP (the post-processing property of DP (DWOrk & Roth, 2014)), and 2) does not access
the original data ∀t ∈ [1,T]. In addition, the computation of h]Badv and the optimization of
RBadv (θι) correspondingly are e∖∕γ-DP and €1 -DP. Infact, the data reconstructionfunction RBadv
is presented as follows:
RBadv (θl)
Σ
Xj∈Badv
Σ
X.dv ∈Badv
d
「 X	hx(2θ1^vT谭i
X. ∈Badv i=1
d 1
[X(2θ1ihjdv) - XjXjadv - μ ∙ sign(VX.L(f (Xj,θ), y(Xj)))XaT
i=1 2
d 1
[X(2θ冏d) -XjXa1^ - X μ ∙sign(Vxaf(Xj,θ),y(Xj)))ejdv (26)
i=1	X.dv ∈Badv
where had = θτXadv, hjd = had + m2Lap(普),and Xad = θihadv. The right summation compo-
nent in Eq. 26 does not disclose any additional information, since the sign(∙) function is computed
from perturbed benign examples (the post-processing property in DP (Dwork & Roth, 2014)). Mean-
while, the left summation component has the same form with RBt (θι) in Eq. 7. Therefore, we can
employ the Proof3 in Theorem 1, by replacing the coefficients Φ = {ɪhi,Xi} with Φ = { ɪhad, Xj}
to prove that the optimization of R^adv (θι) is (tι∕γχ + €1)-DP. As a result, Theorem 2 does hold.
Bt
(Result 4)
In addition to the Result 4, by applying the same analysis in Result 3, we can further show that
the optimization of RDadv(θ1) is (e`∕γχ + €1)-DP given the DP adversarial examples Dadv crafted
using the data D across T training steps, since batches used to created DP adversarial examples are
disjoint and fixed across epochs. It is also straightforward to conduct the same analysis in Result
2, in order to prove that the computation of the first affine transformation h]Badv = {θTXMv +
「adv given the batch of DP adversarial examples Baυ, is (∈ι∕γ)-DP with t ∈
[1, T ] training steps. This is also true given the data level Dadv. (Result 5)
Regarding the output layer, theAlgorithm 1 preserves (s∕γ + €2)-DP in optimizing the adversarial
objective function LBt ∪B皿(02)(Theorem3). We apply the same technique to preserve (e∖∕γ + €2)-
DP across T training steps given disjoint and fixed batches derived from the private training data
D. In addition, as our objective functions R and L are always optimized given two disjoint batches
adv	a adrddv „	, ,	,	_ _ _ C	/	，
B t and Bt , the privacy budget used to preserve DP in thesefunctιons is(€1 + e∖∕γ + €2) ,following
the parallel composition property in DP (Dwork & Roth, 2014). (Result 6)
With the Results 1-6, all the computations and optimizations in the Algorithm 1 are DP following
the post-processing property in DP (Dwork & Roth, 2014), by working on perturbed inputs and
perturbed coefficients. The crafting and utilizing processes of DP adversarial examples based on
the perturbed benign examples do not disclose any additional information. The optimization of
18
Under review as a conference paper at ICLR 2020
our DP adversarial objective function at the output layer is DP to protect the ground-truth labels.
More importantly, the DP guarantee in learning given the whole dataset level D is equivalent to
the DP guarantee in learning on disjoint and fixed batches across epochs. Consequently, Algorithm
1 preserves (6ι + e∖∕γχ + e∖∕γ + €2)-DP in learning private parameters θ = {θι, θ2} given the
training data D across T training steps. Note that the 1 /γx is counted for the perturbation on the
benign examples. Theorem 4 does hold.
I Proof of Lemma 5
Proof 7 Thanks to the sequential composition theory in DP (Dwork & Roth, 2014),
f(M1, . . . , MS |x) is (Ps €s)-DP, since for any O = QsS=1 os ∈ QsS=1 f s (x)(∈ RK), we have
that
P(f(Mι,∙∙∙,MS|x) = O)	=	P(Mlf(X) = 01)...P(MSf(x) = OS)
P (f(Mι,..., MS |x + α) = O)	P(M1f(x + α) = 01)…P (MS f(x + α) = os)
S
≤ exp(€s) = e(Ps=1 s)
s=1
As a result, we have
P(f(Mi,...,Ms|x)) ≤ e(Piei)P(f(Mi,...,MS|x + α))
The sequential composition of the expected output is as:
Ef (Mi,..., Ms |x)= [i P (f (Mi,..., MS |x) >t)dt
0
≤ e( s es
) Z Pf(Mι,...,Ms|x + α) >t)dt
0
)Ef (Mi,..., Ms |x + α)
= e( s es
Lemma 5 does hold.
J	Proof of Theorem 5
Proof 8 ∀α ∈ lp(1),from Lemma 5, with probability ≥ η, we have that
ʌ ʌ , . . . . . . ʌ ʌ ,.. ....
^ ʃ / ʌj	A j i , 、、Efk (Mi,.∙∙, MS Ix)	Elbfk(Mi,..∙, MS Ix)
Efk (Mi,..∙, MS |x + α) ≥ ----eOɪs)-------- ≥ -----£(Ps ]金)------ (27)
In addition, we also have
∀i = k ： Efi：i=k (Mi,..., Ms |x + α) ≤ e(PS=i es)E%i=k (Mi,..., MS ∣x)
⇒∀i = k : E fi (Mi,..., Ms ∣x + α) ≤ e(PS=ι es) max E Ubfi(Mi,..., MS ∣x)	(28)
i:i6=k
Using the hypothesis (Eq. 14) and the first inequality (Eq. 27), we have that
E fk (Mi,..., Ms Ix + a) > e2"s)maxi:\E Ubfi(Mi,…，MS |x)
e( s=1 es)
> e(PS=1 es) maxEubfi(Mi,..., Ms|x)
i:i6=k
Now, we apply the third inequality (Eq. 28), we have that
...,.ʌ ʌ , . . . . . ʌ ʌ ,.. .. .
∀i = k : Efk(Mi,..., MsIx + α) > Efi(Mi,..., MSIx + a)
τπ /■	/ ɪ y	ι y I	∖	τπ /■ / ɪ y	ι y I	∖
⇔ Efk(Mi, . . . , MSIx + α) > maxEfi(Mi, . . . , MSIx + α)
i:i6=k
The Theorem 5 does hold.
19
Under review as a conference paper at ICLR 2020
K	Proof of Proposition 1
Proof 9 ∀α ∈ lp(1), by applying Theorem 5, we have
Eibfk(Mh, Mx|x) > e2(Ker+φer) maxEUbfi(Mh, Mχ∣x)
i:i6=k
> e2(K"B maxEUbfi(Mh, Mχ∣x)
i:i6=k
Furthermore, by applying group privacy, we have that
∀α ∈ Ip(K + 夕)：Elbfk(Mh, Mx|x) > e2 r maxEUbfi(Mh, Mx|x)
i:i6=k
By applying Proof 8, it is straight to have
∀α ∈ Ip(K + 夕)：Efk(Mh, Mx|x + α) > maxEfk(Mh Mx|x + α)
i:i6=k
with probability ≥ η. Proposition 1 does hold.
L EFFECTIVE MONTE CARLO ESTIMATION OF Ef (x)
Recall that the Monte Carlo estimation is applied to estimate the expected value Ef (x) =
n Pn f (χ)n, where n is the number of invocations of f (x) with independent draws in the noise, i.e.,
mlLap(0, ∆eR) and Laρ(0, 警)in our case. When ∈ι is small (indicating a strong privacy Protec-
tion), it causes a notably large distribution shift between training and inference, given independent
draws of the Laplace noise.
In fact, let US denote a single draw in the noise as χ1 = mlLap(0, ∆R) used to train the function
f (x), the model converges to the point that the noise χ1 and 2χ2 need to be correspondingly added
into x and h in order to make correct predictions. χl can be approximated as Lap(χl, %), where
% → 0. It is clear that independent draws of the noise mlLap(0, ∆R) have distribution shifts with
the fixed noise χl u Lap(χl, %). These distribution shifts can also be large, when noise is large.
We have experienced that these distribution shifts in having independent draws of noise to estimate
Ef (x) can notably degrade the inference accuracy of the scoring function, when privacy budget e1
is small resulting in a large amount of noise injected to provide strong privacy guarantees.
To address this, one solution is to increase the number of invocations of f (x), i.e., n, to a huge
number per prediction. However, this is impractical in real-world scenarios. We propose a novel
way to draw independent noise following the distribution of χ1 + mlLap(0,等/Ψ) for the input
X and 2χ2 + m2Lap(0, ∆R/ψ) for the affine transformation h, where ψ is a hyper-parameter to
control the distribution shifts. This approach works well and does not affect the DP bounds and the
provable robustness condition, since: (1) Our mechanism achieves both DP and provable robustness
in the training process; and (2) It is clear that Ef (x) = 1 Pn f (χ)n = 1 Pn g(a(χ + χ1 +
m1 Lapn(0, δR/ψ),θι) + 2χ2 + m2Lapn(0, δR/ψ), θ2), where Lapn(0,等/Ψ) is the n-th draw
of the noise. When n → ∞, Ef (x) will converge to n Pn g(a(x + χ1,θ1) + 2χ2,θ2), which
aligns well with the convergence point of the scoring function f (x). Injecting χ1 and 2χ2 to x
and h during the estimation of Ef (x) yields better performance, without affecting the DP and the
robustness bounds.
M Model Configurations
The MNIST database consists of handwritten digits (Lecun et al., 1998). Each example is a 28 ×
28 size gray-level image. The CIFAR-10 dataset consists of color images belonging to 10 classes,
i.e., airplanes, dogs, etc. The dataset is split into 50,000 training samples and 10,000 test samples
(Krizhevsky & Hinton, 2009). The experiments were conducted on a single GPU, i.e., NVIDIA
GTX TITAN X, 12 GB with 3,072 CUDA cores. All the models share the same structure, consisting
of2 and 3 convolutional layers, respectively for MNIST and CIFAR-10 datasets.
20
Under review as a conference paper at ICLR 2020
Both fully-connected and convolution layers can be applied in the representation learning model
a(x, θ1). Given convolution layer, the computation of each feature map needs to be DP; since each
of them independently reads a local region of input neurons. Therefore, the sensitivity ∆R can be
considered the maximal sensitivity given any single feature map in the first affine transformation
layer. In addition, each hidden neuron can only be used to reconstruct a unit patch of input units.
That results in d (Lemma 2) being the size of the unit patch connected to each hidden neuron, e.g.,
d = 9 given a 3 × 3 unit patch, and β is the number of hidden neurons in a feature map.
MNIST: We used two convolutional layers (32 and 64 features). Each hidden neuron connects with
a 5x5 unit patch. A fully-connected layer has 256 units. The batch size m was set to 2,499, ξ = 1,
ψ = 2. I-FGSM, MIM, and MadryEtAl were used to draft l∞(μ) adversarial examples in training,
with Tμ = 10. Learning rate %t was set to 1e - 4. Given a predefined total privacy budget j,⑦ is
set to be 0.1, and 印 is computed as: 6ι =(+/方/)). This will guarantee that (6ι + s∕γχ +
eι∕γ + £2)= Q. ∆R = (142 + 2) × 25 and Δl2 =2 × 256.
CIFAR-10: We used three convolutional layers (128, 128, and 256 features). Each hidden neuron
connects with a 4x4 unit patch in the first layer, and a 5x5 unit patch in other layers. One fully-
connected layer has 256 neurons. The batch size m was set to 1,851, ξ = 1.5, ψ = 10, and Tμ = 3.
The ensemble of attacks A includes I-FGSM, MIM, and MadryEtAl. We use data augmentation,
including random crop, random flip, and random contrast. Learning rate %t was set to 5e - 2. In the
CIFAR-10 dataset, £2 is set to (1 + r∕3.0) and £1 = (1 + 2r∕3.0)∕(1 + 1∕γ + 1∕γx), where r ≥ 0
is a ratio to control the total privacy budget £t in our experiment. For instance, given r = 0, we have
that Et =(£i + 6ι∕γχ + 6ι∕γ + £2) = 2. Δr = 3 X (142 + 2) X 16 and Δl2 = 2 X 256.
Computational Efficiency and Scalability. In terms of computation efficiency, our mechanism
does not consume any extra computational resources to train the model, compared with existing
DP-preserving algorithms in deep learning (Phan et al., 2016; 2017b;a). The model invocations
to approximate the robustness bounds can further be efficiently performed in a parallel process.
Regarding the scalability, with remarkably tightened global sensitivities, the impact of the size of
deep neural networks in terms of the number of hidden layers and hidden neurons is significantly
remedied, since 1) ∆R and ∆L2 are small, 2) we do not need to inject any noise into the computation
of the network g(∙), and 3) We do not redraw the noise in each training step t. In addition, our
mechanism is not restricted to the type of activation functions. That is similar to (Lecuyer et al.,
2018; Phan et al., 2019). As a result, our mechanism has a great potential to be applied in larger
deep neural networks using larger datasets. Extensively investigating this property requires further
study from both research and practice communities.
N Approximation Error Bounds
To compute how much error our polynomial approximation approaches (i.e., truncated Taylor ex-
pansions), RBt (θι) (Eq. 6) and LBt 依),incur, we directly apply Lemma 4 in (Phan et al., 2016),
Lemma 3 in (Zhang et al., 2012), and the well-known error bound results in (Apostol, 1967). Note
that RBt (θι) is the 1st-order Taylor series and LBt 他)is the 2nd-order Taylor series. Let us closely
follow (Phan et al., 2016; Zhang et al., 2012; Apostol, 1967) to adapt their results into our scenario,
as follows:
Given the truncated function RBt(θι) = Px.∈Bt Pjd= P2=ι P1=o Fjr!⑼(θjhi)r, the original
TaylOr polynomial function RBt(θι) = Pχi∈Bt Pjd=I P∞=ι p1=o j0) (θ∖jhiY, the average
error of the approximation is bounded as
ɪRBt(eι) -RBt(bι)l ≤ 74e×*	(29)
|Bt|	t	t	(1+e)2
1	e + 2e - 1
TB-∣ ILBt (θ2) -LBt (θ2)1 ≤	门 J 、2 X K	(30)
|Bt |	e(1 + e)
where θ1 = arg minθ1 RBt (θ1), θ1 = arg minθ1 RBt (θ1), LBt (θ2) is the original Taylor polyno-
mial function of Pxii∈Bit L(f (Xi,θ2),yj, b2 = argminj? LBt (θ2), θ2 = argminj? LBt (θ2).
21
Under review as a conference paper at ICLR 2020
Proof 10 Let U = maxθ1 (Rb, (θι) -TRBt (θι)) and S = minθ1 (Rb, (θι) -TRBt (θι)).
We have that U ≥ RBt(θι) — RBt(θι) and∀θj : S ≤ RBt(θɪ) 一 RBt(θɪ). Therefore, we have
RBt(θι) — RBt(θι) — RBt(θ;) + RBt(θ;) ≤ U - S	(31)
⇔RBt (θl) - RBt (θ;) ≤ U - S + (RBt (θl) - RBt (θ;))	(32)
T	/C*∖∕C∙,∙
In addition, RBt (θι) - RBt(θɪ) ≤ 0, it is straightforward to have:
^ ~ ∙^∙
RBt (θι)-RBt (θT) ≤ U - S	(33)
If U ≥ 0 and S ≤ 0 then we have:
.^	√~ , ^ ,,.
IRBt(θι)-RBt(θ"l≤ U - S	(34)
^
Eq. 34 holds for every θɪ, including θι. Eq. 34 shows that the error incurred by truncating the Taylor
series approximate function depends on the maximum and minimum values of RBt (θ1) - RBt (θ1).
This is consistent with (Phan et al., 2016; Zhang et al., 2012). To quantify the magnitude of the
error, we rewrite RBt (θ1) - RBt (θ1) as:
d
RBt (θl) - RRBt (θl) = X (RBt (θlj ) - RRBt (θlj ))	(35)
j=1
d |Bt| 2	∞ F(r) (z )
=XXXXFlj
rɪ (gij (Xi,θij) - zij )r)	(36)
j=1	i=1 l=1 r=3	r
where g1j (xi, θ1j ) = θ1j hi and g2j (xi, θ1j) = θ1jhi.
By looking into the remainder of Taylor expansion for each j (i.e., following (Phan et al., 2016;
Apostol, 1967)), with Zj ∈ [zj - 1,zj + 1],信〃 (RBt (θij) - RBt (θij)) must be in the
interval
P P minzj F(2)(Zj )(Zj-Zlj )2 P
[乙i	2!	，乙i
maxzj
FlJ (Zj)(Zj-Zlj )2
2!
.	IfPi
maχzj F(Ij)(Zj)(Zj-Zlj)2
2!
0 and Pi mmzj Flj (Zj)(Zj Zlj) ≤ o, then we have that ∣在(RBt(θι) - RBt(θι))I ≤
pd=ι Pi maxzj Flj (Zj)(Zj Zlj) 2!minzj Flj (Zj)(Zj Zlj) . This can be applied to the case ofourauto-
encoder, as follows:
≥
For the functions F1j (zj) = xij log(1 + e-Zj) and F2j (zj) = (1 - xij) log(1 + eZj ), we
have FIj)(Zj) = (Xj-Zjj)j and Fgj) (Zj) = (1 - Xij) (i+Zjj)j. It can be verified that
argminZj∙ FIj)(Zj) = (*产 < 0, argmaXZj FIj)(Zj) = (*e)2 > 0, argminZj∙ Fgj) (zj) = 0,
and argmaXZj Fgj)(Zj) = (广产 > 0. Therefore, the average error of the approximation is at
most:
高IRBt(ei)-RBt(θι)l≤ h((ɪ^ - (ɪ-ep) + (1⅛7i ×d = (4e×d2	(37)
Consequently, Eq. 29 does hold. Similarly, by looking into the remainder of Taylor expansion for
each label k, Eq. 30 can be proved straightforwardly. In fact,
1
with K categories, we have that:	∣Lb,(θ2) - LBt(θ2)∣ ≤
by using the 2nd-order Taylor series
2
e2+2e-1
e(1+e)2
× K.
O Complete Experimental Results
22
Under review as a conference paper at ICLR 2020
(a) I-FGSM attacks
(c) MIM attacks
(d) MadryEtAl attacks
(a) I-FGSM attacks
Figure 2: Conventional accuracy on the MNIST dataset given the privacy budget, under l∞ (μa =
0.2).
(c) MIM attacks
Figure 3: Conventional accuracy on the CIFAR-10 dataset given the privacy budget, under l∞ (μa =
0.2).
(b) FGSM attacks
(d) MadryEtAl attacks
23
Under review as a conference paper at ICLR 2020
Aoaln。。B-BUow ① >uo。
19876543210
0.0.0.0.60.0.0.0.
A。0 n。。B-BUo+3U ① >uo。
Figure 4: Conventional accuracy on the MNIST dataset given the attack size μa, under (e1 + e1 /γx +
e1 /γ + e2) = 0.2 (tight DP protection).
9876543210
0.60.0.0.0.0.0.0.
AoaIrIsB-BUOWeAUOO
(a) I-FGSM attacks
(b) FGSM attacks
(c) MIM attacks
Figure 5: Conventional accuracy on the CIFAR-10 dataset given the attack size μa, under (e1 +
e1 /γx + e1 /γ + e2) = 2 (tight DP protection).
(d) MadryEtAl attacks
24
Under review as a conference paper at ICLR 2020
19876543210
60.0.660.0.66
>os⊃ooω pωJt①。
0.050.1	0.2	0.3	0.4	0.5	0.6
%
(a) I-FGSM attacks
98765432
660.0.660.0.
>os⊃ooω pωJt①。
0.1
0
0.050.1
(c) MIM attacks
9876543210
60.6666666
Aoal no。B pφJt8
(b) FGSM attacks
(d) MadryEtAl attacks
Figure 6: Certified accuracy on the MNIST dataset. The privacy budget is set to 1.0 (tight DP
protection).
BDPAL
PixeIDP
SecureSGD-AGM
SecureSGD
0.5
0.15
0.1---------------------------------------
0.05 0.1	0.2	0.3	0.4	0.5
%
(a) I-FGSM attacks
0.1
0.05 0.1	0.2	0.3	0.4	0.5
%
(b) FGSM attacks
0.5 I——l——l------1-------1------1-------
一DPAL
--PixeIDP
..SecureSGD-AGM
—SecureSGD
%
(c) MIM attacks
%
(d) MadryEtAl attacks
Figure 7: Certified accuracy on the CIFAR-10 dataset. The privacy budget is set to 2 (tight DP
protection).
25