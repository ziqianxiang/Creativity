Under review as a conference paper at ICLR 2020
Recognizing Plans by Learning Embeddings
from Observed Action Distributions
Handling Uncertainty in Visual Perception for Plan Recognition
Anonymous authors
Paper under double-blind review
Ab stract
Plan recognition aims to look for target plans to best explain the observed actions
based on plan libraries and/or domain models. Despite the success of previous
approaches on plan recognition, they mostly rely on correct action observations.
Recent advances in visual activity recognition have the potential of enabling appli-
cations such as automated video surveillance. Effective approaches for such prob-
lems would require the ability to recognize the plans of agents from video infor-
mation. Traditional plan recognition algorithms rely on access to detailed planning
domain models. One recent promising direction involves learning approximate (or
shallow) domain models directly from the observed activity sequences. Such plan
recognition approaches expect observed action sequences as inputs. However, vi-
sual inference results are often noisy and uncertain, typically represented as a dis-
tribution over possible actions. In this work, we develop a visual plan recognition
framework that recognizes plans with an approximate domain model learned from
uncertain visual data.
1	Introduction
Plan recognition aims to look for target plans to best explain the observed actions based on plan
libraries (Kautz and Allen (1986)) and/or domain models (Ramirez and Geffner (2009); Zhuo, Yang,
and Kambhampati (2012)). Despite the success of previous approaches proposed to handle plan
recognition problems, they generally rely on the assumption that the observed actions are correct
and try to discover missing actions (c.f. Tian, Zhuo, and Kambhampati (2016)). Even though there
have been previous works on noisy observations, they only consider the case that the observed
actions (c.f. Zhuo and Kambhampati (2013)) or states (c.f. Mourao et al. (2012)) have a probability
to be incorrect, instead of considering full distribution of actions (or states). In many real-world
applications, such as video surveillance ( c.f. Collins et al. (2000)), a full distribution of actions,
which can be directly estimated by off-the-shelf approaches (e.g., Zhong, Shi, and Visontai (2004)),
is however available and can be explored to help recognize plans.
Consider the example in Figure 1, where a surveillance system attempts to recognize plans from
captured video clips of salad-making activities. In the first two frames, the activity recognition mod-
ule may generate a distribution over three activities, and determine that the human grasps a pestle
with the highest confidence (probability), a cucumber and a pen with lower confidences. In such a
case there is a perceptual error (PE), assuming the ground-truth activity of picking up a cucumber.
If the plan recognition module only takes the most likely activity (i.e. grasping a pestle), only erro-
neous information is used by the plan recognition module, leading to a poor performance. Hence
the plan recognition module should consider the information of less likely activities as well.
In this work, we consider the problem of recognizing plans from videos, namely visual plan recogni-
tion, based on the distribution of actions at each step of the underlying plan to be discovered. Visual
plan recognition is challenging due to the complex affinities of candidate actions among multiple
steps of the underlying plan based on distributions of actions. In this paper we propose to learn ac-
tion embeddings to capture affinities according to the distributions of actions, and then exploit the
action embeddings to recognize plans.
Specifically, to learn action embeddings from distributions of actions, we propose three candidate
approaches. The first one is to greedily take the most probable action from each distribution, namely
1
Under review as a conference paper at ICLR 2020
Figure 1: This figure illustrates the framework of VPR, which does plan recognition on uncertain ob-
servations. Those uncertain observations come from a visual recognition model, Video2Vec, whose
training is explained in the appendix (SectionB). The number ’18’，‘24’，and ‘36' means the 18-th,
24-th, and 36-th action in action vocabulary A (explained in Section 3)
GM (Greedy Model). GM may suffer from noisy observations because of greedy sampling. The sec-
ond, in order to use more information from uncertain observations, is to sample action sequences
from each sequence of action distributions, and then build a Word2Vec model, namely RS2Vec
( Resampling-to-Vector Model). RS2Vec can be quite inefficient in that it will have to generate
many samples to capture the distribution. The required sample size increases with the length of the
sequence and the number of actions in the distribution of each step. We thus propose the third way,
which is to directly handle distributions of actions, namely Distr2Vec (Distribution-to-Vector
model). Distr2Vec requires a revision to the Word2Vec to let it learn embeddings for distribu-
tions over words as opposed to single words. To train the Distr2Vec affinity model, we introduce
a loss function based on combining Kullback-Leibler (KL) divergence and (Hierarchical-Softmax)
H-Softmax Mikolov et al. (2013).
The framework, namely VPR (visual plan recognition), is shown in Figure 1. A pretrained action
recognition module converts a sequence of video clips from a camera, to a sequence of observed ac-
tion distributions (i.e., observed plans). Then we could do plan recognition by using one of the three
affinity models, GM, RS2Vec, or Distr2Vec. Those affinity models serve as approximated plan-
ning domain models and generate affinity scores for plan recognition. An affinity score represents
how likely an action is going to happen given observed actions.
To the best of our knowledge, we are the first to learn approximated domain (action affinities) models
from sequences of action distributions that are gathered from an activity recognition model trained
on real-world videos. Further, the potential of Distr2Vec and RS2Vec is not restricted to visual
plan recognition. They may also be useful in any categorical data sequence learning scenario where
there is uncertainty at each step of the input data such as in speech recognition.
2	Related work
In the planning literature, there has been a long history of leveraging models for plan recognition
but without considering connections to low-level perception. In Sohrabi, Riabov, and Udrea (2016)
and Ramlrez and Geffner (2009), solving a plan recognition problem is transformed to solving a
planning problem. These two plan recognition works assume that a planning domain model is given,
whose description strictly follows certain rules or syntaxes, like those in PDDL McDermott et al.
(1998). Recently an interesting direction that has emerged, is to learn an approximated planning
model. These models could be flexibly represented in various formats, to do plan recognition, as in
Tian, Zhuo, and Kambhampati (2016), which exploits Word2Vec to learn approximated planning
models from plan corpora. In contrast to these works, our VPR can handle uncertain information in
action recognition from a visual processing model. Our work is also different from existing works of
probabilistic plan recognition (Bui (2003); Rarnfrez and Geffner (2010); Geib and Goldman (2009)).
These works require a predefined set of rules or a domain model, which is instead approximately
learned from plan traces in our VPR.
Another relevant body of research is intention recognition from visual observations (Xie et al.
(2018b); Kitani et al. (2012); Holtzen et al. (2016)). The intention recognition modules in these
works remove observation uncertainty by taking only the most likely observed (or demonstrated)
trajectories and object states from a tracking module in a similar sense of our GM. They will there-
fore lose the information in the distribution-style observation sequences. Our work may be used to
2
Under review as a conference paper at ICLR 2020
Iadd-O让-PreP	0.7]	j add-oil-care	0.7]	jadd-vinegar-prep	0.6]	jadd-b加egar-core	0.6]	j add-pepper-prep
add-v>Eegar-prep	0.2	add-vinegar-prep	0.2	add-oil-prep	0.3	add-oil-core	0.3	∖add-cccambr-∙-peep
add-water-prep	0.1J	I. add-water-prep	0.1J	L add-water-prep	0.1J	I. add-water-care	0.1」L add-salt-prep
0.81
0.11
0.11
Figure 2:	An uncertain Plan p, which is a sequence of observed action distributions.
r add-pepper-prep
Observation: \add-cucumi)er-prep
a add-salt-prep
0.7-∣
0.2∖φφ
0.1J
Iadd-Vinegar-Core
add-oil-core
add-water-core
0.6
0.3
0.1
Hadd-SaIt-PreP
add-vinegar-prep
add-oil-prep
0.5
0.3
0.2
I
Figure 3:	An observed plan O, that has two steps of missing observations (action distributions)
marked with SymbOl φ.
augment these and similar visual intention recognition works because our approach Preserves the
information from the uncertainty in the data.
Using sensor data as input, Hodges and Pollack designed machine learning-based systems Hodges
and Pollack (2007) for identifying individuals as they perform routine daily activities such as mak-
ing coffee. Liao et al. proposed to infer user transportation modes Liao et al. (2007) from readings
of radio-frequency identifiers (RFID) and global positioning systems (GPS). Freedman et al. ex-
plore the application of natural language processing (NLP) techniques Freedman, Jung, and Zilber-
stein (2014), i.e., Latent Dirichlet Allocation topic models, to human skeletal data of plan execution
traces obtained from a RGB-D sensor. Bulling et al. discussed the key research challenges Bulling,
Blanke, and Schiele (2014) that human activity recognition shared with general pattern recognition.
When activity recognition is performed indoors and in cities using the widely available Wi-Fi sig-
nals, there is much noise and uncertainty. Xie et al. proposed a temporal-then-spatial recalibration
scheme to build an end-to-end Memory Attention Networks (MANs) Xie et al. (2018a) for solving
skeleton-based action recognition task. Chen et al. propose a multi-agent spatial-temporal attention
model Chen et al. (2019) to jointly recognize activities of multiple agents. To tackle the limita-
tions of feature extraction and training data labeling effort, Qian et al. propose a distribution based
semi-supervised learning approach Qian, Pan, and Miao (2019) to recognize human activities. Dif-
ferent from the above-mentioned sensor-based activity recognition, we aim to recognize plans, i.e.,
sequences of actions, instead of predicting single activities.
3 Visual Plan Recognition
Definition 1. (Action Space) We define the action space as A = A U φ. The action set A consists
of all possible grounded action symbols (in action vocabulary A), and a symbol φ which denotes a
step with missing observation (action distribution).
Definition 2. (Action Distribution) An observed action distribution of size K at time step t is de-
noted as DiStr(at) which equals to h(α1,c1), (α2, c2),…，(aK, CK)). Note that each action a has
a corresponding confidence value c. C is a probability given by a visual recognition module, and
PK ck = L
Definition 3. (Uncertain Plan) We usep to define an observed plan with uncertainty, and T to denote
its length or number of steps. A plan P with T steps would be a sequence of T action distributions,
defined as the matrix below.
Time	)
/ aI,c1	a2,c2	a3,c3	…	aT,cT ∖
Actions a3 ,c3	a2,c3	a3,c3	…	aT,cT
.	.	...
.	.	.	.	∙ ∙ ∙
y ∖aκ, cκ aκ, cκ aκ, cκ ∙ ∙ ∙	aK, cK/
Definition 4. (Visual Plan Recognition) The visual plan recognition problem is defined as R =
(L, O,A). L is a library of uncertain plan traces P (Figure 2). A denotes the action space as ex-
plained. O is an observed plan captured by running visual activity recognition modules on video
3
Under review as a conference paper at ICLR 2020
Iadd-PePPer-PreP
add-cucumber-prep
add-salt-PreP
0-7∖	^ddd^c>uτe<aar-^ree 0.61 Γ add-salt-PreP
0.2 add-PePPer-core add-vinegar-PreP	add-oil-core	0.3∣ ∣add-τi九egar-PreP
0.1J	L add-water-core	0.1J L	add-oil-PreP
0.5
0.3
0.2
]
Figure 4:	The completed plan after running visual plan recognition on observed plan O.
inputs. O may contain missing observations ot at certain steps t (Figure 3). To recognize a plan we
need to search for proper actions to fill them in those steps. The searching requires using action
affinity models, whose training will be explained later. Action affinity model works as a subrou-
tine in VPR. The solution R, as illustrated in Figure 4, could be either a completed plan with all
missing actions filled in (plan completion), or a plan that includes future actions of an agent (plan
recognition). Note that plan completion is a more general version of plan recognition problem such
that missing observation can be at any step in a plan. For plan recognition, missing observations are
only at the end of a plan. In some earlier plan recognition works Ramirez and Geffner (2009); Zhuo,
Yang, and Kambhampati (2012) they did solve plan completion problems as well, as pointed out
by Tian, Zhuo, and Kambhampati (2016). Similar to their works, we also mean to solve the more
general problem of plan completion and assume that the missing observations can be at any plan
step.
3.1	Formulation of Learning Action Affinity Models
We can now formulate the learning of action affinity model, as maximizing the mean of log proba-
bility of distributions over the steps of plans in L:
1	T
TE E	log Pr(Distr(at+j )∖Distr(at))	(1)
T t=1 -W≤j≤W,j=0
where W denotes the window size. DiStr(at) denotes the input observation distribution (input) at
the current step. DiStr(at+j) denotes the target observation distribution (target, which is the ex-
pected output) at step t + j, which provides training signals. Pr(DiStr(at+j)∖Distr(at)) represents
the similarity between DiStr(at+j) and DiStr(at), computed by an affinity model.
Also, when using a DiStr(at), the affinity model encodes DiStr(at) into a vector
enc(DiStr(at)) = uι,…,u^. The size of A equals to the number of nodes in input layer. DiStr (at)
is encoded into a vector by having a unique index i associated to each action in A. The value Ui
at each index i in the input vector, is the confidence value associated to the matching action in
DiStr(at). The rest of the units in input layer, whose corresponding actions are not in DiStr(at),
have zero probability values.
3.2	Recognizing Plans with a Learned Affinity Model
For an observed plan O (Definition 4.), if there are totally M steps or observations (o1,..., oM), the
length of O is M. If there are x steps in O with missing observed action distributions, there are x
φ symbols in O (Definition 1.). To recognize the plan O we try different actions within A to fill in
each of the x steps, and select the actions that maximize overall affinities. Note that x is a variable
that may vary plan by plan.
For example, consider when x = 1 that there is only one missing observation (at step j) in the
observed plan O. All observations at other steps are known (action distributions). To determine the
action for a step of missing observation, we calculate A × 2W pairwise affinities for all possible
actions that could complete O. W is the context window size (a hyperparameter) assumed to be set
to one in this example. Each candidate plan completion is denoted as p. P is also a M-step plan but
with symbols φ replaced by actions in A. To score P with one filled-in action in our example, we
need to compute the pairwise affinities between all possible actions in A, and an observed action
distribution within -W ≤ j ≤ W. The selected action would have the highest pairwise affinity to
observations in the window. As a result, because the plan only has one step with missing observation,
the selected action completes the pP and we obtain a recognized plan.
4
Under review as a conference paper at ICLR 2020
The score of a plan completion pe, denoted as F (pe),
is calculated using the sum of log probabilities
P r(target|input) (measuring a pairwise affinity) over all
M steps:
M
F(pe) =	log P r(target|input) (2)
t=1 -W≤j≤W,j 6=0
where both input and target are encodings of either a sin-
gle action (a one-hot vector), enumerated from the ac-
tion vocabulary, that could be put in a step with miss-
ing observation, or an observed action distribution (e.g.,
enc(Distr(at))) in the window W. Note that target ac-
tion or action distribution is the expected output. Each pe
(has length M) is a candidate plan completion, in which
x actions would be filled in the x steps with missing ob-
servations in an observed plan O (Definition 4.). j is the
Algorithm 1 Plan Recognition with an Affinity Model
INPUT: An observed plan O, and an affinity model MA
OUTPUT: A plan completion P
1:	for i = 1; i ≤ M; i + + do
2:	for steps in -W ≤ i ≤ W do
3:	if any step j has the value φ then
4:	for each action a from A do
5:	φ —a	> Replace φ with a
6:	Enumerate input-target pairs and com-
pute their affinity values with MA
7:	Store the action a and corresponding
affinity value
8:	end for
9:	end if
10:	end for
11:	end for
12:	Select actions with maximum affinity values to form P
13:	return P
index inside context window W. The plan completion for 万 that has the highest score, is treated as
the solution R for visual plan recognition. The general procedure is described in Algorithm. 1.
4	Action Affinity Models
In this section, we formally introduce our action affinity models, Distr2Vec and RS2Vec, that
could be used in VPR framework for visual plan recognition.
4.1	DISTRBUTION-TO-VECTOR (DISTR2VEC)
Input
Figure 5: Our Distr2Vec for learn-
ing action embeddings (approximated
planning domain models) directly from
distribution sequences. The embedding
matrix WE has the size of action vo-
cabulary A times embedding size. The
hidden-Output Matrix has the shape
of embedding length times size of A,
whose output is a vector of size A.
FigUre 5 illustrates our Distr2vec model, that takes
a pair of action distribution encodings to update its em-
beddings. The pair of distributions are enumerated within
the context window. Distr 2Vec takes one as input and
treats the other as the target. Each row in WE is the em-
bedding of an action symbol.
We train Distr2Vec by minimizing the KL-divergence
between the target distribution Distr(at+j), and the pre-
dicted output distribution Distr (at+j). KL-divergence is
an often-used measure of the distance between two distri-
butions. So, if we use it as the loss function we can keep
the input data as is (no resampling). Using KL-divergence
as the loss function, Distr2Vec is trained by minimiz-
ing:
ʌ
DKL(enc(Distr(at+j))||enc(Distr(at+j)))	(3)
where DKL represents the KL divergence. enc(Distr(at)) are defined in Section. 3. KL-divergence
DKL is calculated in Equation 4.
K	KK
KL(p∣∣q)，EPk logpk = EPk logPk - EPk logq®	(4)
k=1	qk	k=1	k=1
where q is the output probability distribution of our Distr2Vec model. q is also an approximation
ofP, the target distribution Distr(at+j) in the plan trace. Equation 4 allows us to avoid computing
the derivative of the entropy ofP when taking partial derivative of KL(P||q) with respect to model
parameters. This is because the probability values for P (which is Distr(at+j )), are constants with
5
Under review as a conference paper at ICLR 2020
respect to the model parameters. Using this information, we can obtain Equation 5.
ʌ
DKL(enc(DiStr(at+j))||enc(DiStr(at+j)))
K
= Z (DiStr(at+j)) - X ctk+j log P r(atk+j|h(DiStr(at)))
(5)
k=1
where Z(Distr(at+j)) = PkK=1 ctk+j log(ctk+j) is a constant, and h(Distr(at)) is the embed-
ding computed by multiplying the embedding matrix WE and the action-distribution input vec-
tor enc(Distr(at)) = hu1,u2,…，〃区〉 = (0…0,c1,0,…，0, c2, 0,...0,cK, 0...〉encoded from
Distr(at), as done in Equation 6. u, A,ct, and K are defined in Section. 3.
h(Distr(at)) = WE × enc(Distr(at))
(6)
4.2	Combining with Hierarchical Soft-max
We adopted the H-Softmax introduced in Mikolov et al. (2013), which has been shown to have ad-
vantages over the non-hierarchical counterpart in both the accuracy, and computational efficiency1.
H-Softmax decomposes the logits computation of each word to computing a sequence of nodes
(logit values) in a tree data structure.
If we combine Equation 5 with H-Softmax, with distribution input Distr(at), we obtain the proba-
bility of an action atk+j in the target observed action distribution Distr(at+j):
L(atk+j)-1
P r(atk+j |h(Distr(at))) =	Y	σ(I(n(atk+j,i + 1)
i=1
(7)
=child(n(at+j ,i))) ∙ vn(ak+j,i) ∙ h(DiStNat))) }
where I(x) is an identity function. L(atk+j) is the length of path from root to the leaf node, i.e., an
action, in a H-Softmax tree. vn(ak ,i) is the weights vector of ith node along the path in the tree,
and h is the embedding computed using Equation 6. We encourage readers to read a brief review of
the Word2Vec and H-Softmax in the Section D of appendix, to understand Equation 7 better.
And if we substitute Equation 7 into Equation 5, we obtain the error function in Equation 8. This is
the error function as we are trying to minimize the KL divergence of Equation 5.
K	L(atk+j )-1
E = Z (Distr(at+j )) - X ck+j	X {log σ (I(Jvn(ak+j,i) ∙ h(Distr(at/}	⑻
i=1
k=1
Note that we also use Equation 8 to calculate the logarithmic posterior probability of obtaining target
encoding given input encoding, log P r(target|input), as in Equation 2. Readers should refer to the
Section E in appendix for detailed derivation of gradient descent for Equation 8.
4.3	Resampling-to-Vector (RS2Vec)
In RS2Vec, we first perform a beam search sampling on each uncertain plan p from library L, to
obtain a set of deterministic samples (plans). Each sample includes a sequence of actions, and a
probability value calculated by multiplying the confidence values of all actions along a sequence.
This probability value could be used to represent the overall uncertainty or score of a sampled
plan. After doing beam search sampling in which the beam size is set to S, the top S samples
1The first version that tried to improve Word2Vec with H-Softmax in Morin and Bengio (2005), reports that
the new model requires less training time, but also results in a degraded accuracy. However, the work Mnih and
Hinton (2009) introduces an approach to automatically grow a tree to organize words, for H-Softmax, which
outperforms non-hierarchical Word2Vec. In the recent work Mikolov et al. (2013), the advantage is confirmed
and it selects the binary Huffman tree as the basic tree data structure.
6
Under review as a conference paper at ICLR 2020
(action sequences) with their scores are selected. We then resample them by using the roulette wheel
resampling approach of Lipowski and Lipowska (2012), which lets us drop some samples that have
low scores, and increase the amount of samples with high scores. After doing resampling we use the
action sequences in all stored samples after performing beam search, to train a Word2Vec model
as an action affinity model for plan recognition.
That said, the potential problem for RS2Vec are the following. First, the resampling would make
the algorithm more computationally expensive, and the training time would be closely related to
the number of samples and length of the training data. This makes it hard to apply RS2Vec to a
real-time plan recognition system. Secondly, the resampling approach could lose some information,
whereas Distr2Vec directly uses the entire distribution sequence.
5	Evaluation
We evaluate Distr2Vec and RS2Vec in VPR by observing training time and the plan recognition
performance of VPR. VPR takes Distr2Vec, RS2Vec, and the baseline model GM. In GM, we take
sequences of most likely actions from distributions across each plan to build a plan corpus. With the
corpus we train an action affinity (Word2Vec) model. This is as opposed to the RS2Vec model
which takes more samples per sequence for training. We ran our experiments on a machine with
a Quad-Core CPU (Intel Xeon 3.4GHz), a 64GB RAM, a GeForce GTX 1080 GPU, and Ubuntu
16.04 OS.
5.1	Dataset Collection and Experiment Design
We collected two plan corpora based on processing the 50 Salads Dataset Stein and McKenna
(2013). One is from real world videos (visually grounded corpus), for evaluating the real world
value of our model in connecting high-level plan recognition and low-level perception. The other
corpus is a controlled synthetic plan corpus. In order to fully assess the validity and effectiveness of
our approaches, we need to test with different types of distributions in input data. Thus we generated
synthetic plans (distribution sequences), as they would allow us to maximally evaluate the approach
by testing the effects of various factors. We evaluate the effect of factors like distribution entropies,
perception error rates (PER), and the length of observation sequences. Note that we simulate the
perceptual error (PE) in an action distribution by exchanging the highest confidence action with an-
other action in that distribution. A specified PER is achieved by simulating PE in a proportion of
the action distributions in each plan trace. For experiments on synthetic data we evaluate affinity
models with H-Softmax. We also explore the performance difference from using their counterparts
without H-Softmax in terms of training time and accuracy in Section 5.3. We set the size of window
W to one, embedding size to 100 and train each model for 60 epochs. For each position of miss-
ing observation, we ask models to predict top-three actions in all experiments except for Figure. 8.
The prediction is counted as correct if at least one of the three is correct. We denote the number of
candidate predictions as top-k-prediction. For details of dataset collection and experiment settings,
readers should refer to section B and C in the appendix.
We evaluate different approximated domain models (Distr2Vec, RS2Vec, and GM) as used in
VPR using 6-fold cross validation. For each test, we randomly remove some distributions at some
steps. Then we compare the performance by the average accuracy. We define the accuracy following
the work Tian, Zhuo, and Kambhampati (2016), using Equation 9.
acc
1 Z #hCorrectRecognitionii
Zi=I	Xi
(9)
Where Z is the total number of plan traces in testing set, and xi is the number of steps with missing
actions for the plan i. #hCorrectRecognitionii is the number of correct predicted actions in i-th
uncertain plan in testing set.
5.2	Experiments and Analysis on Synthetic Plans
Figure 6 shows accuracy results for synthetic plans of length 10, 20 and 30, with varying PER and
fixed entropy, in the four sub-plots on the left. The RS2Vec-S, RS2Vec-M, and RS2Vec-L are
7
Under review as a conference paper at ICLR 2020
1.0
0.9
0.8
0.7
?0.6
g0∙5
青0.4
0.3
0.2
0.1
10	20	30
length of sequences
10	20	30
length of sequences
_
Xɔejnɔɔd
ACC of Data with 50% PER
10	20	30
length of sequences
-B -GM
⅛ Distr2Vec
I RS2Vec-S
Il RS2Vec-M
K RS2Vec-L
_ __
X3"jn3u"
ACC of Data with 25% PER
* Distr2Vec
♦ RS2Vec-S
—2—RS2Vec-M
RS RS2Vec-L
300.0
250.0
200.0
150.0
100.0
50.0
Training Time
(Sauιjι
1 3 5 7 9 11 13 15 17 19
number of samples
10	20	30
length of sequences

Figure 6: This figure demonstrates the accuracy of GM, Distr2Vec, and three RS2Vec instances
on synthetic data of different PERs, with respect to increasing plan (distribution sequence) lengths.
The three RS2Vecs (RS2Vec-S, RS2Vec-M, and RS2Vec-L), are RS2Vec instances that are
trained on small (S), medium (M), and large (L) numbers of samples.
three RS2Vec instances trained on three levels of samples (small, medium, and large). We set the
sample numbers of small, medium, and large levels to 3, 9, and 27 respectively. In the rightmost
sub-plot, We show a comparison of training time required by GM, Distr 2Vec, and RS2Vec with
respect to increasing number of samples that an RS2Vec instance takes, when sequence length is
20. We did training time experiments for length 10, 20, and 30 of sequences, but because the three
training time results are very similar to each other, we only report the length 20 results.
We observe that the training time of RS2Vec increases
linearly with the number of samples per plan. This is eas-
ily explained by the fact that increasing the number of
samples, is increasing the training cost. In contrast, the
training time for Distr2Vec and GM stay constant as
there is no sampling step.
For accuracy performances under all PER settings,
Distr2Vec generally gets better performances for
longer training sequences. As the length of plan increases,
RS2Vec requires more samples from the distribution se-
quence to match or outperform the Distr2Vec (this is
more obvious when PER is high). For RS2Vec, increas-
ing the number of samples could be beneficial when PER
is higher. Higher PER means more observed action distri-
butions in a plan would have lower probabilities for cor-
rect actions (assuming each distribution must have one
action that matches the ground-truth). Thus, taking more
samples would give RS2Vec more accesses to correct in-
Figure 7: Accuracy results of GM,
Distr2Vec, and three RS2Vec in-
stances on synthetic data of different en-
tropies. The RS2Vec-S, RS2Vec-M,
and RS2Vec-L are explained in the de-
scription of Figure 6.
formation. We can observe clearly in sub-plots of 100% and 75% PER, that RS2Vec could getbetter
performance by training on more samples.
As for the GM model, its accuracy for 100% PER across all lengths is 0%.
This matches our expectation sicne the training data for the GM model
does not have the ground-truth action in any step for 100% PER. There-
fore, the GM would not capture any relevant information. For lower PER
cases the GM performance is higher. This matches our expectations since
the GM gets more of the correct information from highest-probability
sampling of low PER plans.
As for data with high and low entropies, the experimental results are
shown in Figure. 7. We set PER to zero, and sample numbers of
RS2Vec-S, RS2Vec-M, and RS2Vec-L, to 7, 11, and 15 respectively.
Then the actions which match ground-truth ones are most probable ones
in all distributions. As expected, GM performs the best when entropy
of distributions in training distribution sequences (plan corpus) is high,
since it gets the correct plan when PER is zero. The performance of
Distr2Vec is expectedly lower than GM, yet it gets appreciably bet-
Acc on Comparing Algo. 1
and LSTM in VPR with
1.0 Distr2Vec Embeddings
0 0.8
(Q
0 0.7
0 0.6
0.5
^^Distr2Vec+LSTM
^^Distr2Vec+Algo. 1
top-k-predictions
Figure 8: Algorithm. 1
and LSTM comparison
in VPR framework for
plan recognition.
1	2	3	4	5
8
Under review as a conference paper at ICLR 2020
ter than RS2Vec. In low entropy cases all models perform comparably
well.
We also evaluated Algorithm 1 in VPR (Section 3.2) by comparing it with the Long-shot Term Mem-
ory networks (LSTM) approach Graves (2013). Our LSTM based approach is essentially a gener-
ative model, implemented based on 2. It first learns action representations based on Distr2Vec
and then uses the learnt representations to train the LSTM for predicting an action. Different from
Algorithm 1, LSTM can do plan recognition by approximating an optimal prediction, which may
require a slower training time but do faster in testing (plan recognition) phase. We would like to
investigate the differences in accuracy and running time between the two approaches. The accuracy
result is shown in Figure. 8 and we used the synthetic data of 30% PER. The VPR with Algorithm. 1
and VPR with LSTM performs comparably well with top-k-prediction value increases. In addition,
VPR with LSTM requires 29.5 Secs to train and 0.02 Secs to recognize one action in testing phase.
VPR with Algorithm 1 requires 8.0 Secs and 0.80 Secs respectively. Hence VPR with Algorithm 1
overall has a less running cost.
5.3	Experiments and Analysis on visually grounded Plans
In Figure 9, the left sub-plot shows how Distr 2Vec, GM, and RS2Vec which takes 9 (as RS2Vec-
S) and 27 (as RS2Vec-L) samples would perform, when PER is high (69%) and low (8%), mean-
while setting distribution size to 5. The right sub-plot shows how sizes of distributions per step
influence the accuracy (setting RS2Vec to takes 15 samples). We observe that, when PER is high,
VPR with Distr2Vec clearly performs the best, whereas when PER is low, all models perform
comparably well. This is consistent to results of synthetic data evaluation. We also observe that,
overall the distribution size is not a key factor for deciding model performance.
Comparison of Training Time between
¾ 500
4 400
-B 300
詈200
E 100
≡	0
Using Hierarchical Softmax or not
■ GM-nohs
■ RS2Vec-nohs
■ GM
■ RS2Vec
■ DiStr2Vec-nohs	■ DiStr2Vec
1
δ∙
0 0.5
-0
Comparison of Accuracy between Using	ACC on Visual Grounded ACC on Visual Grounded
0.3
5	9	13	17	10% 2C% 30% 40% 5C%	1	2	3	4	5
number of samples	pct. of missing observations context window sizes
Data
岩 0.25
总0.2
0.15
0.1
Distr2Vec
1	5	9	13	17
number of samples
Figure 10: The left two subplots show a comparison between using H-Softmax or not with respect to
training time and accuracy for affinity models on the visually grounded corpus of8% PER. The suffix
“-nohs" denotes models without H-Softmax. The right two subplots show accuracy with respect to
the percentage of missing observations and window sizes on the visually grounded corpus of 69%
PER.
Acc on Visual Grounded Data	0" IACC on ViSUal GrOUnded Data (PER=69%,
1	SampleS=15)
The left two subplots in Figure 10 show that the H-
Softmax in Distr2Vec slightly decreases the train-
ing time, and vice versa for GM. This is probably be-
cause our action space is small (52). H-Softmax is faster
for corpora with larger vocabularies as the tree traver-
sal way of updating weights will eventually take less
time than updating the entire hidden-output matrix. The
accuracy advantage of using H-Softmax is obvious for
RS2Vec with more samples. The reason is RS2Vec-
nohs with more samples tends to sample more incorrect
data. RS2Vec tends to be more robust to this noise.
From the right two subplots we observe that the opti-
mal window size is one, and 50% of missing observa-
tions per plan is the lower bound for Distr2Vec to
maintain its advantage over other models.
0.8
2 0.6
O 0.4
0.2
0
■ GM
・RS2VeaL W
T GM	RS2Vec	Distr2Vec
8%
PER
69%
0.2
1	2	3	4 „ 5. 10,,15 20 25 30 35 40 45 50
SiZe of distributions per step
Figure 9: Results of evaluating the
GM, RS2Vec, and Distr2Vec on
videos. The left sub-plot shows how
Distr2Vec, GM, and RS2Vec with
small and large number of samples
(RS2Vec-S and RS2Vec-L) performs
with data of 8% and 69% PERs. The right
subplot shows the accuracy in terms of
varying distribution sizes on the 69% PER
data.
2https://github.com/pytorch/examples/tree/master/word_language_model
9
Under review as a conference paper at ICLR 2020
6	Conclusion and Future Work
Our work aims to make plan recognition more practi-
cally useful in scenarios with observational error in ac-
tion recognition. We introduce our VPR framework that uses Distr2Vec and RS2Vec as ac-
tion affinity models, to do visual plan recognition which may suffer from perceptual uncertainty.
Distr2Vec and RS2Vec learn embeddings for distributions over actions to preserve the infor-
mation from the visual recognition module. Our VPR framework can thus handle the uncertainty in
visual data and perform plan recognition. We evaluated our models (Distr2Vec and RS2Vec) as
well as a baseline GM model in the context of plan recognition in the VPR framework. We evaluated
models with/without H-Softmax on plans in which action distributions are generated from real-
world videos. We also evaluated the H-Softmax version of models on synthetic plans with different
types of action distributions. We conclude that when there is a higher PER, VPR with Distr2Vec
outperforms other models. When PER is lower, and sequence length is longer, all models perform
comparably well. Another advantage of using Distr2Vec to handle distributions within the VPR
framework is that the training time is shorter as compared to the RS2Vec model in which we have to
sample more action sequences for comparable performance. For future work we would be interested
in exploring other domains that provide uncertain observations, like speech recognition.
References
Reh u rek, R., and Sojka, P. 2010. Software Framework for Topic Modelling with Large Corpora.
In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, 45-50.
Valletta, Malta: ELRA.
Bui, H. H. 2003. A general model for online probabilistic plan recognition. In IJCAI, volume 3,
1309-1315. Citeseer.
Bulling, A.; Blanke, U.; and Schiele, B. 2014. A tutorial on human activity recognition using
body-worn inertial sensors. ACM Comput. Surv. 46(3):33.
Chen, K.; Yao, L.; Zhang, D.; Guo, B.; and Yu, Z. 2019. Multi-agent attentional activity recognition.
CoRR abs/1905.08948.
Collins, R. T.; Biernacki, C.; Celeux, G.; Lipton, A. J.; Govaert, G.; and Kanade, T. 2000. In-
troduction to the special section on video surveillance. IEEE Trans. Pattern Anal. Mach. Intell.
22(8):745-746.
Freedman, R. G.; Jung, H.-T.; and Zilberstein, S. 2014. Plan and activity recognition from a topic
modeling perspective. In Proceedings of ICAPS.
Geib, C. W., and Goldman, R. P. 2009. A probabilistic plan recognition algorithm based on plan
tree grammars. Artificial Intelligence 173(11):1101-1132.
Graves, A. 2013. Generating sequences with recurrent neural networks. arXiv preprint
arXiv:1308.0850.
Hodges, M. R., and Pollack, M. E. 2007. An ’object-use fingerprint’: The use of electronic sensors
for human identification. In Ubicomp, 289-303.
Holtzen, S.; Zhao, Y.; Gao, T.; Tenenbaum, J. B.; and Zhu, S.-C. 2016. Inferring human intent from
video by sampling hierarchical plans. In Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ
International Conference on, 1489-1496. IEEE.
Hu, S.-H.; Li, Y.; and Li, B. 2016. Video2vec: Learning semantic spatial-temporal embeddings for
video representation.
Kautz, H. A., and Allen, J. F. 1986. Generalized plan recognition. In Proceedings of AAAI, 32-37.
Kitani, K. M.; Ziebart, B. D.; Bagnell, J. A.; and Hebert, M. 2012. Activity forecasting. In European
Conference on Computer Vision, 201-214. Springer.
10
Under review as a conference paper at ICLR 2020
Liao, L.; Patterson, D. J.; Fox, D.; and Kautz, H. A. 2007. Learning and inferring transportation
routines. Artif. Intell. 171(5-6):311-331.
Lipowski, A., and Lipowska, D. 2012. Roulette-wheel selection via stochastic acceptance. Physica
A: Statistical Mechanics and its Applications 391(6):2193-2196.
McDermott, D.; Ghallab, M.; Howe, A.; Knoblock, C.; Ram, A.; Veloso, M.; Weld, D.; and Wilkins,
D. 1998. Pddl-the planning domain definition language.
Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations
of words and phrases and their compositionality. In NIPS, 3111-3119.
Mnih, A., and Hinton, G. E. 2009. A scalable hierarchical distributed language model. In Advances
in neural information processing systems, 1081-1088.
Morin, F., and Bengio, Y. 2005. Hierarchical probabilistic neural network language model. In
Aistats, volume 5, 246-252. Citeseer.
Mourao, K.; Zettlemoyer, L. S.; Petrick, R. P. A.; and Steedman, M. 2012. Learning STRIPS
operators from noisy and incomplete observations. In UAI, 614-623.
Qian, H.; Pan, S. J.; and Miao, C. 2019. Distribution-based semi-supervised learning for activ-
ity recognition. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019,
The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The
Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu,
Hawaii, USA, January 27 - February 1, 2019., 7699-7706.
Ramirez, M., and Geffner, H. 2009. Plan recognition as planning. In IJCAI 2009, Proceedings
of the 21st International Joint Conference on Artificial Intelligence, Pasadena, California, USA,
July 11-17, 2009, 1778-1783.
Ramirez, M., and Geffner, H. 2010. Probabilistic plan recognition using off-the-shelf classical
planners. In Twenty-Fourth AAAI Conference on Artificial Intelligence.
Sohrabi, S.; Riabov, A. V.; and Udrea, O. 2016. Plan recognition as planning revisited. In IJCAI,
3258-3264.
Stein, S., and McKenna, S. J. 2013. Combining embedded accelerometers with computer vision for
recognizing food preparation activities. In Proceedings of the 2013 ACM International Joint Con-
ference on Pervasive and Ubiquitous Computing (UbiComp 2013), Zurich, Switzerland. ACM.
Tian, X.; Zhuo, H. H.; and Kambhampati, S. 2016. Discovering underlying plans based on dis-
tributed representations of actions. In Proceedings of the 2016 International Conference on Au-
tonomous Agents & Multiagent Systems, 1135-1143. International Foundation for Autonomous
Agents and Multiagent Systems.
Xie, C.; Li, C.; Zhang, B.; Chen, C.; Han, J.; and Liu, J. 2018a. Memory attention networks
for skeleton-based action recognition. In Proceedings of the Twenty-Seventh International Joint
Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden., 1639-
1645.
Xie, D.; Shu, T.; Todorovic, S.; and Zhu, S.-C. 2018b. Learning and inferring “dark matter” and
predicting human intents and trajectories in videos. IEEE transactions on pattern analysis and
machine intelligence 40(7):1639-1652.
Zhong, H.; Shi, J.; and Visontai, M. 2004. Detecting unusual activity in video. In 2004 IEEE
Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2004), with
CD-ROM, 27 June - 2 July 2004, Washington, DC, USA, 819-826.
Zhuo, H. H., and Kambhampati, S. 2013. Action-model acquisition from noisy plan traces. In
IJCAI, 2444-2450.
Zhuo, H. H.; Yang, Q.; and Kambhampati, S. 2012. Action-model based multi-agent plan recogni-
tion. In Advances in Neural Information Processing Systems, 368-376.
11
Under review as a conference paper at ICLR 2020
A Appendix
B	Dataset Collection
B.1	Collection of Visually Grounded Plan Corpus
The procedure for making the visually grounded corpus is illustrated in Figure 1. First, we converted
all videos into video clips, and each clip matches a ground-truth action. Secondly, we applied a video
processing model, Video2Vec Hu, Li, and Li (2016), on those video clips, to output action distribu-
tion sequences. For each clip it outputs a distribution over all actions. We concatenate distribution
sequences together for each video to make plan corpora that correspond to real-world videos. We
trained two visual recognition models. The first is trained on 90% of video clips with 450 epochs.
The second one is trained on 70% of video clips with 100 epochs. With the two visual models trained
by Video2Vec, we obtain two plan corpora. We found the PER for plan corpora collected by running
the first visual model, is (8%), and the PER for plans collected by running the second visual model
is 79%.
B.2	Collection of Synthetic Plan Corpus
We also augmented the ground-truth sequences in 50 Salads Dataset Stein and McKenna (2013) to
create a synthetic plan corpus of action distribution sequences. There are totally 54 ground-truth,
low-level action sequences, a；：T (T is sequence length). We synthesize a distribution per action
step, by adding actions to each step and assigning a probability distribution over the actions. We
add K - 1 additional actions to each observation and assign a probability distribution such that the
ground truth has the highest confidence (probability).
In order to generate the K - 1 additional actions, we search for the K - 1 most similar actions
by using a Word2Vec model that is pretrained on Google News corpus 3. We use the semantic
correlation of actions in this model, to simulate their visual correlation. As for assigning confidence
values for each of the K - 1 additional actions, we followed the Equation 10.
C(Ok) = 1~	[ PK-I -	(IO)
1 + wentropy +	i=1 si
where atk is the kth additional action in a distribution at step t. We search for atk in the Word2Vec
model based on the ground truth action a%. Sk is the similarity between ajc, and ak, which can be
computed by using the pretrained Word2Vec of gensim library Reh u rek and Sojka (2010). These
similarity values are used as in the Equation 10. The +1 in the denominator denotes the similarity
between the ground-truth action and itself. We also use a parameter wentropy which we can use to
increase or decrease probability differences between actions. Thus it could be used to adjust the
entropy of each action distribution. We set up wentropy for the computation of each c(atk) to either
zero or one, to slightly increase the variance of confidences at each step. The confidence for ground-
truth action aJ= is initially the highest and is equal to Equation 11.
K-1
c(at=) = 1 - X c(atk)	(11)
k=1
where c(atk) is the confidence of one of the K - 1 additional actions in the distribution at a step.
Thus far, the synthetic data generated will have the ground truth as the action with the highest
confidence. We would like to vary this, and simulate perception errors. We define a perception error
(PE) as when the action at that has the highest confidence in Distr(at) does not match the ground-
truth action at= . The perception error rate (PER) is the percentage of action distributions that have
perception errors. We simulate the PE in a particular action distribution by exchanging the highest
confident action with another action in that distribution. A specified PER is achieved by simulating
PE in a proportion of the action distributions in each plan trace. The action distributions in which
we simulate PE are randomly chosen. In this way, data of different distribution types is generated
for testing the effectiveness of our model.
3https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit
12
Under review as a conference paper at ICLR 2020
C Experiments Settings
All of our experiments have the following hyperparameters (thus fair to all models): We set the win-
dow W to one, embedding size to 100 and train each model for 60 epochs. We considered other
training epochs from 20 to 80, and decide to use 60 in our experiments. The number of missing
actions is one in synthetic evaluation and 10% in video based evaluation, the number of recommen-
dations for each missing action is three, and the number of threads when running the experiment
is eight. Specifically, in accuracy evaluation on synthetic dataset, we evaluate the data under two
categories: 1) Fixed entropy but varying PERs between (25%, 50%, 75%, and 100%); 2) Zero PER
but test with both high entropy (all confidence values each time step are uniformly distributed) and
low entropy. We set PER to zero in order to evaluate with only the influence of entropy. For low
entropy, We set the confidence for aJ= to 0.9, and the two simulated actions to 0.05. And on real
world dataset (consists of whole distribution sequences with varying lengths), we also evaluate the
data under two categories: 1) Fixed sequence length (30) but varying number of samples; 2) Fixed
number of samples (15) but varying sizes of distributions.
We also look at the effect of the number of sampled paths on the training time, on the synthetic
dataset. Please note that in results where the number of samples are varied, it only affects the
RS2Vec model. The other models are unaffected by resampling and thus only have their average
value plotted as a line.
D Word2Vec and Hierarchical Softmax
In this section we make a brief review about how the Skip-gram Word2Vec works, based on
Mikolov et al. (2013), and how it could be used for plan recognition as introduced in Tian, Zhuo,
and Kambhampati (2016). To be consistent with the whole paper, we use the term “action” and treat
it an equivalence to the term “word” in other papers which introduces Word2Vec.
Given a corpora which contains action (word) sequences, a Skip-gram model is trained by maximiz-
ing the average log probability:
1T
TE E	log p(at+j |at)	(12)
t=1 -W≤j≤W,j 6=0
where T is the length of a sequence, W is the context window size, at is fed as the model input, aI ,
and at+j is used as the target action (word), aO. The probability p(aO |aI) is computed as:
p(aO |aI)
exp(VOTO Va)
PaA=1 exp(v0aT vaI)
(13)
which is essentially a softmax function. A denotes a vocabulary of all possible actions. Other sym-
bols follows the definition in Equation 12. We can use this equation that computes p(aO |aI) to
compute p(at+j |at) in Equation 12.
The p(at+j|at) in the Word2Vec with hierarchical softmax is calculated in the following manner.
The output layer’s weight matrix of the regular Word2Vec is replaced by a binary tree whose leaf
nodes are words in the trained vocabulary. Every node on the path from the root node until the leaf
node has a vector, excluding the leaf node. The input action ai is converted into an embedding h
which is the input into the binary tree component. The probability of this input vector h that could
go to a particular leaf node is calculated by following the path from the root node to the target leaf
node, using the following formula:
p(at+j|at)
L(at+j)-1
Y
i=1
σ(I(n(at+j, i + 1)
child(n(at+j,i))) ∙ Vn(
αt+j,i)∙h) },
(14)
where I(x) is a function that returns 1 if the next node on the path to the target leaf node is on the
left of the current node, and -1 if the next node is to the right. L(at+j) is the length of path from
13
Under review as a conference paper at ICLR 2020
root to the leaf node at+j, vn(at+j,i) is the vector of the ith node along the path. h is the embedding
obtained by multiplying the embedding matrix and vector of the input action at . h is the vector that
represents the input action in the embedding space. In order to maximize the probability, the vectors
of the intermediary nodes are updated with each training sample which has the target action at and
an action in its context at+j .
E Derivation of Gradient Update Equations
Figure 11: Details of Distr2Vec model for Error Propagation.
We back-propagate the error E as shown in Figure 11, where the dashed lines show the path of
back-propagation. For this derivation, we shorten h(Distr(at)) as h. We start by computing the
derivative of E with respect to (。4(。，十 ㈤ * h) as in Equation 15. We also shorten our notation of
vn(ak ,i) as vk,i, which is the vector of i-th node in the tree path to the leaf node of the k-th action
in the vocabulary as illustrated in Figure 11.
dσ(I(Jvk,ih)
∂E _ _ k dvk,ih
∂νk,ih	t+j3 σ(I(.)vk,ih)
=- k	σ(I(.)vk,ih)(1 - σ(I(.)vk,ih)I(.)
t+j3	σ(I(.)vk,ih)
k ctk+j	(σ(I(.)vk,ih) - 1), (I(.) =	1)
= ct+j	(σ(I(.)vk,ih)	- 1)I(.)	= cttk++jj	(σ(I(.)vk,ih), (I(.) = -1)
= ct+j (σ (vk,i h) - ti )
(15)
where ti = 1 if I(.) = 1 and ti = 0 if I(.) = -1. Recall that I(.) is the identity function defined in
Equation 14.
Then we calculate the derivative with respect to each vk,i along the path to a specific action, at the
time step t + j :
∂E
∂Vk,i
(16)
With this, we update each vk,i as follows:
∂E
vk,i = vk,i - α^^
∂vk,i
(17)
Note that each node’s vector vk,i could get updated more than once, as each node could be on the
path to more than one action as show in the right side of Figure 11.
14
Under review as a conference paper at ICLR 2020
Then we compute the back-propagated error δh by substituting Equation 15 as follows:
K L(atk+j )-1
δ = dE = ^X	^X	dE dvk,ih
h ∂h	dv	∂vk ih ∂h
k=1	i=1	k,i
K	L(atk+j )-1
=	ctk+j	σ(vk,ih) - ti)vk,i	(18)
k=1	i=1
We can understand this equation by imagining that there are multiple channels coming back from
each leaf node, passing thorough a sequence of child nodes in the hierarchical softmax tree, towards
the output of the embedding matrix WE . So doing the back-propagation means summing errors of
these channels together, and that is why vk,i could get updated more than once.
We derive the Equation 18 leveraging Equation 15. However, we could also go directly from the
original error function (Equation 8). Thus here we provide another derivation of δh which is equally
valid:
∂E	∂[- PK=Ick+jPL(ak+j)τ logσ(I(.)Vn(ak+j,i) ∙ h)]
---=---------------------------:--------------------------
∂h	∂h
K	L(atk+j )-1
-	ctk+j [
k=1	i=1
d log σ(I(Jvn(ak+j,i) ∙ h)
∂h
]
K	L(atk+j )-1
-	ctk+j [	(σ(vn(atk+j ,i)h) - ti)vn(atk+j ,i)]
k=1	i=1
d log σ(I(Jvn(ak i) ∙h)
where-----------∂h t+j'-----has already been derived as a part of Equation 15.
Finally, we update the weights in the embedding matrix WE :
∂E _ ∂E ∂h _	.t
∂We =而∂WE = hva
(19)
(20)
where v1a = hc1,c2,…，cK〉is the confidence values from DiStr(at), and ∂W can be used to update
the values in WE as follows:
WE = WE — α *
∂E
∂We
(21)
15