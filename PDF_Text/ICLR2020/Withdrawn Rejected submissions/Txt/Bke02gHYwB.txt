Under review as a conference paper at ICLR 2020
Learn Interpretable Word Embeddings Effi-
ciently with von Mises-Fisher Distribution
Anonymous authors
Paper under double-blind review
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
Ab stract
Word embedding plays a key role in various tasks of natural language processing.
However, the dominant word embedding model don’t explain what information
is carried with the resulting embeddings. To generate interpretable word embed-
dings we intend to replace the word vector with a probability density distribution.
The insight here is that if we regularize the mixture distribution of all words to
be uniform, then we can prove that the inner product between word embeddings
represent the point-wise mutual information between words. Moreover, our model
can also handle polysemy. Each word’s probability density distribution will gen-
erate different vectors for its various meanings. We have evaluated our model in
several word similarity tasks. Results show that our model can outperform the
dominant models consistently in these tasks.
1 Introduction
Word embedding is a widespread technique in boosting the performance of modern NLP systems
by learning a vector for each word as its semantic feature. The general idea of word embedding is to
assign each word with a dense vector having lower dimensionality than the vocabularies’ cardinal-
ity. In a qualified word embedding model, the vector-similarity tends to reflect the word-similarity.
Therefore, feeding these vectors as features of words into the other NLP systems will always boost
the performance of them in many downstream tasks (Turian et al., 2010; Socher et al., 2013).
One such qualified model is the skip-gram with negative sampling (SGNS) model proposed in
word2vec (Mikolov et al., 2013; Joulin et al., 2016), which is very popular in various NLP tasks
with expressive performance. The SGNS model propose to represent each word with vectors and
estimate these vectors by applying maximum likelyhood estimation method. It implicitly factorize
a word-context matrix containing a co-occurrence statistic. This would assign each word wc in the
vocabulary with a ”word” vector vc ∈ Rd and a ”context” vector uc ∈ Rd , so as to model the con-
ditional probabilities p(wk|wc) separately for different words. By maximizing the log likelyhood of
p(wk|wc), the SGNS model can estimate the ”word” vector and ”context” vector of each word.
However, the SGNS model’s main problem is that it doesn’t build interpretable model for the em-
beddings themself, and therefore, people don’t understand how word vectors can express useful
information. For example, previous work emphasized that the inner product between one word’s
”word” vector and another word’s ”context” vector represents the point-wise mutual information
between the two words (Levy & Goldberg, 2014). However, people never use the ”word-context”
inner product in practise. Instead, people will simply chose ”word” vectors as the word embeddings
and drop the other one or vice versa. Therefore, we should care about the behaviour of ”word-word”
or ”context-context” inner product, which are rarely analyzed and are never guaranteed to have any
good properties.
In this paper, we propose a variational inference based framework to learn more interpretable word
embeddings. That is, we would estimate a probability density distribution for each word instead of
estimating a vector from the training corpus. To be specific, we propose to replace the ”word” vector
with a probability density distribution, namely the von Mises-Fisher (vMF) distribution, and keep
the ”context” vector for each word. As what we will show in this paper, representing the word with
a probability density distribution can result in more interpretable word embeddings. Besides, the
probability density representations can provide other benefits too. For example, such representations
can model the polysemy phenomenon when we train our model. To be specific, we can sample a
1
Under review as a conference paper at ICLR 2020
Figure 1: Each word Wc would generate a latent meaning vector ^c, and then Wk is picked by Vc.
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
vector from the vMF distribution to represent the specific meaning of this word in a particular context
during training.
The main inspiration for this work is the analysis in Ma (2017)’s work, which can also be found in
Arora et al. (2016)’s paper. They assumed that the word vectors would obey the uniform distribution
over a sphere during the analysis. This assumption is critical in their analysis and yet it turns out to
be wrong as reported by Mimno & Thompson (2017). In fact, the word frequency obey the Zipf’s
law, which means that it’s impossible for word vectors to obey the uniform distribution when we
represent each word by a vector. However, when we represent each word by a probability density
distribution, it’s possible that the mixture distribution of all words is uniform if we adjust each
word’s probability density distribution’s position and shape carefully.
To estimate each word’s probability density representation, we need to adopt the Bayesian varia-
tional inference technique, and this would result in a Bayesian version of SGNS model. We are
not the first to propose a Bayesian version of word embedding model (Zhang et al., 2014; Sakaya
et al., 2015; Barkan, 2017). Among them, the state-of-the-art model is the BSG model introduced
by Brazinskas et al. (2017), and it is also the most related work with us. However, our model is dif-
ferent from the BSG model in many aspects. The BSG model represents each word with a Gaussian
distribution while we adopt the vMF distribution. It’s important to notice that the vMF distribution
is defined over a unit hyper-sphere, which means that we will sample a unit vector for each word
during training. Further more, the BSG model is based on the variational autoencoding framework
(VAE). As reported by Davidson et al. (2018), it’s impossible for the native VAE to work well when
the prior is defined over a hyper-sphere. In contrast, we didn’t adopt the VAE framework, and yet we
would still talk about the reparameterization tricks. At last but not least, the BSG model focuses on
how to build a more reasonable model using the VAE techniques, while we focus on how to generate
more interpretable word embeddings using the vMF distribution.
Our main contributions can be summarized as follows. First, we proposed to represent each word
with a vMF distribution. Second, we generated highly interpretable word embeddings, and show
that our model’s ”context-context” inner-product would represent the point-wise mutual information
between words. At last, our word embeddings out-perform the dominant models and the state-of-
the-art model in various tasks.
2 Our framework
In this section, we intend to generate interpretable word embeddings by adopting the vMF represen-
tation for each word. Specifically, we will show that the ”context-context” inner-product between
word embeddings would represent the point-wise mutual information between words.
2.1	Model definition
The SGNS model intends to maximize the probability of a context word appearing around a center
word. It assumes this probability is proportional to the inner product between two fixed vectors. In
contrast, we assume that the probability of a context word appearing around a center word should
be the average probability when the center word take different meanings. That is, we are suggesting
a generative model as pictured in 1. Assuming there are T words Wc in a training corpus, then for
2
Under review as a conference paper at ICLR 2020
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
each word we would like to predict the possible words wk appearing around it within a K length
word window. However, it’s hard to determine what exactly the current word wc means, therefore,
We propose to take the average probability over all possible Vc for each observed Wk.
The training objective of our model is to find vector representations and probability density repre-
sentations for Words that are useful for predicting the surrounding Words in a sentence. Given a
sequence of training words wι,… ,wτ, we are meant to maximize the average log likelyhood
1 T	K/2
arg max —XX
log J p(wt+j|Vt,wt； θ)p(Vt∣wt; θ)dV + L(θ), j = 0,	(1)
t=1 j=-K/2
where θ is the set of all parameters to be optimized. What's more, Vt ∈ Rd denotes a vector
representation for a patential meaning of wt, andP(Vt∣wt; θ) is the probability density representaton
for Wt. We sample Vt according top(Vt|wt； θ). At last, p(wt+j |Vt, wt； θ) denotes the probability of
word wt+j appearing given Vt, and L(θ) denotes the possible regularization term.
Unfortunately, it’s intractable to calculate the integration in (1). Therefore, we propose to git rid of
the integration by applying the jensen inequality considering that log is concate and we are doing
maximization. By doing so, our model can be defined as
1T n
arg max —XX
E	logP(wt+j |Vm,wt； θ) + L(θ),j = 0.	⑵
θ T t=1 j=-n vt~p(vm∣wt)
t= j=-n
More details can be found in the appendix. We will call the p(Vm∣wt) as a prior for each word
wt . Although the equation (2) is very similar to the ELBo in the standard varitional inference
auto-encoder (VAE) technique, the actual meaning of it is quite different from ELBo. First, the ex-
pectation term in (2) doesn’t involve a ”encoder” as what VAE would do. Second, the regularization
term in (2) is also different from the VAE’s KL-divergence term, as what we will show.
2.2	Expectation term
The most troublesome component of equation (2) is the epctation term. For each Vt sampled from
word wt’s prior, we choose the softmax function to calculate the odds of word wt+j appearing
around it. That is, we can decompose the expectation term into
E	logP(Wt+j |Vm,wt； θ) =	E
^t 〜P(VmIwt)	Vt 〜P(Vm ∣wt)
1	eχp(Ut+j Vt)
.°g PiVI exp(u>^t)
(3)
where Ui ∈ Rd denotes the ”context” vector for word wi , and |V | is the cardinality of our vocabu-
lary. As what’s been suggested in the SGNS model, we extend the negative sampling technique to
our model to avoid the computation of the softmax function’s denominator. It’s worth to notice that
different words would have different denominators in theory. We will use Zt to denote the denom-
inator of word wt. According to the theory proved by Gutmann & Hyvarinen (2012), maximizing
objective (3) is equivalent to minimizing
E	[logσ(u>+jVt) + NEi〜p(wi) log σ(-u>Vt)] ,	(4)
Vt 〜P(VmIwt)
where Ui is the context vector of word wi , and wi is the negative word sampled according to the
empirical unigram probability P(wi). There are N negative samples.
At last, we also need to sample Vt, and we choose von Mises-Fisher (VMF) distribution to calculate
the probability ofwt taking this particular meaning. The vMF distribution is an analogy of Gaussian
distribution over the unit sphere. It’s parameterized by a mean vector and a concentration parameter
κ ≥ 0. For word wt, it’s corresponding mean vector Vt can be interpreted as the vector repre-
sentation for its average meaning. Formally, the probability of wt taking one particular meaning
is
p(^t∣Wt)	= Cd(Kt) exp(κtV>Vt),
κ"∕2-1
Cd(Kt)	= (2n)d/2tId/2-i(Kt),
(5)
where d is the dimension of embedding space and Id∕2-1(∙) denotes the modified Bessel function
of the first kind Sra (2016).
3
Under review as a conference paper at ICLR 2020
Figure 2: By adjusting the position and shape of each word’s vMF distribution, the mixture distri-
bution of all words can be uniform.
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
2.3	Regularization term
We will introduce an regularization term in this subsection, and show that this term will link the
”context-context” inner-product directly to the point-wise mutual information. The general idea is
to regularize the mixture distribution of all words to be uniform, then we can achieve our goal. This
can be done by simply applying the maximum entropy algorithm which would result in a uniform
distribution naturally. Therefore, the regularization term we are seeking for is
arg max p(V) logp(V),
θ
where P(V) = PiVI P(Wi)p(V∣wJ.
(6)
Objective (4) + (6) is the final objective that we want to optimize. We claim that by doing so, for
each pair of words wc and wk , we have
uk>uc	P(wk, wc)
-≈— ≈ PMI(Wk, wc) ：= log ------7—7—ʌ
d	P(wk)P(wc)
where PMI(Wk, Wc) is called the point-wise mutual information (PMI) between word Wk and word
Wc, andP(Wk, Wc) denotes the probability for them to appear in the same word window.
Before we show why our claim holds, we would like to emphasize why PMI is important. PMI indi-
cates how much more possible that word Wk, Wc co-occur than by chance (Church & Hanks, 1990).
Then, people developed the notion of word window to help define when two words ”co-occur”, i.e.,
Wk and Wc co-occur, only when they appear in the same word window. Most of the word embedding
model will take advantage of such co-occurrence statistics. Indeed, the PMI evaluated from co-
occurrence counts has a strong linear relationship with human semantic similarity judgments from
survey data (Hashimoto et al., 2016). In conclusion, it’s reasonable to relate word embedding with
the point-wise mutual information.
Then, we will show how to link the ”context-context” inner-product directly to the PMI by two steps.
The first step is to show that
logP(Wk,Wc)	≈ kuk+duck2 - 2log(Z),
logP(Wk) ≈ k2d2 - log(Z),	G)
logp(wc)	≈ ⅛k2 - log(Z),
where Z is the constant that most Zc approximate to. This is a conclusion proved by Ma (2017).
The second step is quite obvious because
uk> uc
PMI(Wk ,Wc) = logP(Wk ,Wc) - logP(Wk) - logp(wc) ≈ kd .
4
Under review as a conference paper at ICLR 2020
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
We will show (7) by a series equations briefly, and reveal why the regularization term is very impor-
tant. More details can be found in the appendix. We start with p(wk , wc)
p(wk , wc)

|V|
p(wi)p(wk, wc|wi)
i=1
|V|
p(wi)p(wk|wi)p(wc|wi)
i=1
E p(wk|wi)p(wc|wi)
i 〜P(Wi)
E R euZvip(v0∣Wi)ds0 R euzvip(Vi∣Wi)ds
ii
i 〜P(Wi)
Z12 E R R eu>v0eu>vip(Vi∣Wi)p(^i∣Wi)dsds0
i〜P(Wi)
Z12 E J exp [(uk + Uc)>Vi] p(Vi∣Wi)ds
i〜P(Wi)
(8)

|V|
Z ∑p(wi) * {e exp [(uk + Uc)>Vi] p(Vi∣Wi)ds}
i=1
Z R exp [(uk + Uc)>v] [Pi=] p(wi)p(V∣Wi)] ds
Z R exp [(uk + Uc)>V] p(V)ds
Z E {exp [(uk + Uc)>^]}
^ 〜P(V)
Z12Ex〜N(0,kuk + uck2∕d) {exp(X)}
ɪ exD k kuk+uck2s∣
Z2 exp [	2d ∙
Step ten is why we need a regularization term, but before that, we would like to explain all the
equations above. The first step of (8) says that wk and wc co-occur iff they appear in another word
wi ’s word window together. This is true for the Skip-gram model. The second step is also true
when we considering the definition of p(wk |wi) and p(wc|wi) in the Skip-gram model. Step four
is just by definition and We use slightly different notations here to indicate that ^i, Vi are different
variable. Step six is a strong claim which needs rigorous prove. We put this prove in the appendix.
The key insight is that Vi, V0 obey the same VMF distribution, which means that when this VMF is
concentrate enough, then the probability of Vi,, V0 being very different is small. In the eighth step, we
omit i to emphasize that every word can generate the same vector V and that,s why the summation
and integration can exchange with each other in this way.
If we regularize the V to be uniformly distributed over the unit sphere in step ten, then (Uk + uj> v
will obey Gaussian distribution approximately (Ma, 2017). This means that step eleven holds. More
details can be found in the appendix. The last step is the result ofa famous calculation practise
Ex 〜N (0,σ2){exp(x)} = exp(σ2∕2).
Obviously, by replacing the ”word” vector with the vMF distribution, we can eliminate the assump-
tion that p(wi ) being uniform.
2.4	Reparameterization trick
There is one problem to solve before our model becomes practical. P(VtIwt) is difficult to optimize
because the operation of sampling is nondifferentiable. We can solve this problem by applying the
reparameterization trick. The vMF distribution’s reparameterization trick is usually discussed in
the context of hyperspherical variational auto-encoders (Davidson et al., 2018; Xu & Durrett, 2018;
Guu et al., 2018). To simplify our model, we propose to fix the concentration parameter κt as a
constant during training for each word wt. This is because the gradient estimation of κ is complex
and computational expensive.
When it comes to each word wt ’s mean vector Vt, we follow the technique used in Xu & Durrett’s
work. Firstly, we sample an auxiliary random variable ω according to the rejection sampling scheme
of Wood (1994). The distribution of ω is controlled by κ. Specifically, the probability of ω being
sampled is
p(ω; K) H exp(ωκ)(1 — ω2).
5
Under review as a conference paper at ICLR 2020
Figure 3: The illustration about our reparameterization trick.
Then We draw our Vt in the following way
161
162
163
164
165
166
167
168
169
170
171
172
173
^t = ωvt + Z √1 - ω1 2 * 4 5 6 7 * * 10 11 * 13,
where z is a random unit vector and is tangent to the unit sphere Sd-1 at vt. Figure 3 illustrates the
geometric vision. Because of applying this trick, we can take gradient with respect to vt as usual.
2.5 Final algorithm
Putting all the details together, we have the following objective to minimize
logσ(u>V)+ NEi〜p(wi) [logσ(-u>v)] -P(V)Iogp(^),
X-------------------V-------------------} '-------{-----}
L1	L2
where ^ = ωVc + Zʌ/l — ω2, ∣∣Vck2 = 1
ω 〜p(ω; KC) H exp(ωκj(1 — ω2),
kZk2 = 1, Z is tangent to Sd-1 at Vc ,
Pg) = PjV=I P(Wj )p(vlwj)	⑼
p(V∣Wj) = Cd(Kj )exp(κj v>V),
d/2-1
CMKj) = (2n)d/2jId/2-i(Kj ).
Based on our final model, we propose algorithm (1) to train the word embeddings. The for loop
started from line 1 controls how many times we will go through the entire training corpus. Then, for
each epoch, we would iterate over each word in the corpus as what the for loop in line 2 suggests. In
line 3, we will take wc ’s mean vector vc and concentrate parameter Kc from dictionaries. The third
layer of for loop will iterate over all the words in a word window centered around wc . For each word
Wk in this word window, We will take its context vector Uk from the dictionary, and sample a V from
wc ’s prior with the help of our reparameterization trick (line 5 to line 6). Then form line 7 to line
10, we will sample N negative samples for each Wk, and then calculate the gradients of L1. From
line 11 to line 13, we will calculate L2 based on the V, and update all word,s prior accordingly.
noend 1 PDF (N, Ui, V Ki,p(w%),i = 1,…,|V|)
1: for ePoCh in ePoChs do
2:	for Wc in CorPus do
3：	VC — {Vi}i=ι,…，|V|, KC — {κi}i=ι,…，|V|
4:	for Wk in W indoW(WC) do
5:	ω 〜p(ω; Kc), Z 〜U(Sd-2)
6:	V - ωVc + z√1 - ω2, Uk — {ui}i=ι,…,∣V∣
7:	for i in range(N) do
8：	Wi 〜P(Wi), Ui — {ui}i=ι,…，|V|
9：	Li — Lι(uk, Ui, V)
10:	Update Vc, Uk according to L1 ’s gradient.
11:	L2 J L2({Vj}j=1,…,|V|, V)
12： for j in range(|V |) do
13:	Update Vj according to L2 ’s gradient.
6
Under review as a conference paper at ICLR 2020
Model	Embedding	WS353	WS353-SIM	WS353-REL	RG65	MEN
SGNS	context	0.5648	-0.6184-	0.4832-	0.491	0.479
GloVe	context	0.5686	-062T9-	0.4864-	0.4959	0.4921
Ours	context	0.6378	-06932-	-05961-	0.522	0.646
SGNS	word	0.6606	-07071-	0.6237	0.669	0.676
GloVe	word	0.6436	-0.7085-	0.5865	0.6606	0.666
Ours	mean	0.6637	0.7444 —	0.6404	—	0.596	0.68
Table 1: Results on the word similarity tasks.
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
3	Experiments
We will now experimentally validate our embedding by comparing the performance of our model
with the dominant word embedding models on the word similarity tasks. Subsection 3.1 presents all
the experiment settings for different training corpus and different word embedding models. Subsec-
tion 3.2 introduces several word similarity benchmarks, and how we evaluate model’s performance
on them. Subsection 3.3 compares our model with the STOA model on several benchmarks.
3.1	Experimet Settings
We use part of massachusetts bay transportation authority (mbta) web crawled corpus because it
is well cleaned and is large enough. The mbta corpus contains about 600 million tokens. We
preprocessed all corpora by removing non-English characters, numbers and lower-casing all the
text. The vocabulary was restricted to the 100K most frequent words in each corpus.
We trained embeddings using three methods: word2vec Mikolov et al. (2013), GloVe Pennington
et al. (2014), and our model. This is because these models are implemented with C/C++, and
therefore are fast enough to be evaluated on mbta corpus. For fairness we fix all hyperparameters
for word2vec, GloVe and our model. Specifically, we trained for 5 epochs for each model using 75
threads for parallel computation; the word embedding dimension is 100; the window size is 5.
For the word2vec, and our model, the negative sampling number is 5; We adopt the Hogwild! algo-
rithm to train models, and the learning rate decays linearly from 0.0025 to 0. The noise distribution
is set as the same as used in Mikolov et al., Pn(W) 8 p(ww)"75. We also use a rejection threshold
of 10-4 to subsample the most frequent words.
For the GloVe model, we follow the original inplementation’s default settings for the other hyperpa-
rameters. This means the initial learning rate is 0.05.
We also use the news corpus1 with about 15 million tokens to evaluate the BSG model. This is
because the original implementation of BSG is based on theano, and it’s slow to be trained on the
mbta corpus. Given the small volume of this corpus, we trained for 25 epochs for each model using
12 threads. we also restrict each model’s vocabulary to be 10K. For both the BSG, word2vec and
our model, we set the negative number to be 10; the window width is 10.
3.2	Performance Evaluation
We test the quality of the word embeddings by checking if our word embeddings agree with the
human judgement on word similarity / relatedness.
For the mbta corpus, we test the performance of our embeddings on three benchmarks: the Word
Similarity353 data set Finkelstein et al. (2002), the RG65 data set Luong et al. (2013), and the MEN
data set Bruni et al. (2014). Taking the WS353 data set for example, it contains 353 word pairs along
with their similarity scores assigned by 29 subjects. These subjects possessed near-native command
of English and they are instructed to estimate the relatedness of the words in pairs on a scale from 0
to 10. During experiments, we will cumpute the Spearman’s rank correlation coefficient Spearman
(1904) between human judgement and the inner product between the vector representations. The
larger this coefficient is, the better this embedding is and the mximume value is 1. Some words in
1 https://drive.google.eom/file/d/1QWC2x6qq8KyHFUCgyvVJJoGHexZrw7gO/view
7
Under review as a conference paper at ICLR 2020
Datasets	Ours	BSG	GloVe	SGNS
MC-30	0.6190	0.5818	0.4524	0.4524
MEN-Tr-3k	0.4905	0.3937	0.3607	0.5182
MTUrk-287	0.5343	0.5147	0.3334	0.5351
MTUrk-771	0.4221	0.3693	0.2647	0.4177
RG65	0.5727	0.6036	0.3455	0.4000
RW-STNFRD	0.4172	0.3830	0.2710	0.3725
SIMLEX-999	0.2110	0.1717	0.1914	0.2339
VERB-143	0.3127	0.1371	0.0712	0.2376
WS353-ALL	0.4567	0.4350	0.2379	0.4387
WS353-Rel	0.3485	0.4197	0.1583	0.3591
WS353-SIM-	0.6070	0.5018	0.3277	0.5549
Table 2: Results on the word similarity tasks.
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
these testing datasets do not appear in our training corpora, and this means we can’t calculate the
inner product between vectors for those words. In order to provide comparable results, we propose
to use the mean vectors of the rest words for these missing words. Also, the mean vector of each
word’s prior is similar to the ”word” vector in the SGNS model, and we also test the performance of
this vector too.
We also evaluate the performance over more Benchmarks. Table 2 presents similarity results com-
puted using the online tool of Faruqui & Dyer (2014). Since the BSG model can only generate the
mean vectors from its prior, we only evaluate the ”word” or mean vectors in these experiments.
3.3	Compare with the dominant models
Table 1 shows the results on these benchmarks. As we can see, our ”context” word embedding can
constantly outperform the counterpart of the dominant word embedding models by a large margin.
This is exactly what we expect according to our theory. It’s interesting that the ”word” vector of our
model can still outperform ours ”context” vector. Another exception is that on the RG65 test set, the
SGNS model can outperform our model by a large margin. We argue this may be caused by its bias
-it only contains 65 noun pairs after all. Besides, our model's "context" vector can still outperform
SGNS on this test set.
Also, the gap between the "context" vectors and the "word" vectors in our model is much smaller
than the dominant models’ gaps. These results demonstrate that our model is more specific about
which part of our embedding contains the useful information.
3.4	Compare with the BSG model
First, we observe that our model can out-perform the BSG model for almost every task except for the
RG65 data set as before. Although the BSG model can perform better than our model in the WS353-
Rel data set, we would like to point out that this data set is a subset of WS353-ALL. Therefore, this
may also be caused by the bias of small data set. Second, the performance of BSG model is weaker
than the SGNS model for some data sets. Given that we used their released implementation directly,
this may be the result of small training data set, i.e., the SGNS model may perform better in the case
of small training data set because of its simplicity.
4	Conclusions
We generated interpretable word embeddings by represent each word with a von Mises-Fisher dis-
tribution. We have demonstrated that our word embeddings can be linked to the point-wise mutual
information directly without making any unrealistic assumptions. The experiments over different
training and testing data sets demonstrate that our model can outperform both the dominant and the
STOA models. We argue that our insight into the interpretable word embeddings is important. For
example, as we are sure that the unit word vectors can encode the semantic similarity between words,
it’s possible to encode the syntactic information between words into the norm of word vectors.
8
Under review as a conference paper at ICLR 2020
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
References
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model
approach to pmi-based word embeddings. Transactions of the Association for Computational
Linguistics, 4:385-399, 2016.
Oren Barkan. Bayesian neural word embedding. In Thirty-First AAAI Conference on Artificial
Intelligence, 2017.
Arthur Brazinskas, Serhii Havrylov, and Ivan Titov. Embedding words as distributions with a
bayesian skip-gram model. arXiv preprint arXiv:1711.11027, 2017.
Elia Bruni, Nam-Khanh Tran, and Marco Baroni. Multimodal distributional semantics. Journal of
Artificial Intelligence Research, 49:1-47, 2014.
Kenneth Ward Church and Patrick Hanks. Word association norms, mutual information, and lexi-
cography. Computational linguistics, 16(1):22-29, 1990.
Tim R Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, and Jakub M Tomczak. Hyperspheri-
cal variational auto-encoders. arXiv preprint arXiv:1804.00891, 2018.
Manaal Faruqui and Chris Dyer. Community evaluation and exchange of word vectors at wordvec-
tors. org. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguis-
tics: System Demonstrations, pp. 19-24, 2014.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and
Eytan Ruppin. Placing search in context: The concept revisited. ACM Transactions on informa-
tion systems, 20(1):116-131, 2002.
Michael U Gutmann and AaPo Hyvarinen. Noise-contrastive estimation of unnormalized statistical
models, with applications to natural image statistics. Journal of Machine Learning Research, 13
(Feb):307-361, 2012.
Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by
editing PrototyPes. Transactions of the Association for Computational Linguistics, 6:437-450,
2018.
Tatsunori B Hashimoto, David Alvarez-Melis, and Tommi S Jaakkola. Word embeddings as metric
recovery in semantic sPaces. Transactions of the Association for Computational Linguistics, 4:
273-286, 2016.
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient
text classification. arXiv preprint arXiv:1607.01759, 2016.
Omer Levy and Yoav Goldberg. Neural word embedding as imPlicit matrix factorization. In Ad-
vances in neural information processing systems, PP. 2177-2185, 2014.
Thang Luong, Richard Socher, and ChristoPher Manning. Better word rePresentations with recursive
neural networks for morPhology. In Proceedings of the Seventeenth Conference on Computational
Natural Language Learning, PP. 104-113, 2013.
Tengyu Ma. Non-convex Optimization for Machine Learning: Design, Analysis, and Understanding.
PhD thesis, Princeton University, 2017.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed rePresen-
tations of words and Phrases and their comPositionality. In Advances in neural information pro-
cessing systems, PP. 3111-3119, 2013.
David Mimno and Laure ThomPson. The strange geometry of skiP-gram with negative samPling. In
Empirical Methods in Natural Language Processing, 2017.
Jeffrey Pennington, Richard Socher, and ChristoPher Manning. Glove: Global vectors for word
rePresentation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), PP. 1532-1543, 2014.
9
Under review as a conference paper at ICLR 2020
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
Joseph Hosanna Sakaya et al. Scalable bayesian induction of word embeddings. 2015.
Richard Socher, John Bauer, Christopher D Manning, et al. Parsing with compositional vector
grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 455-465, 2013.
Charles Spearman. The proof and measurement of association between two things. American
journal of Psychology, 15(1):72-101, 1904.
Suvrit Sra. Directional statistics in machine learning: a brief review. 2016.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for
computational linguistics, pp. 384-394. Association for Computational Linguistics, 2010.
Andrew TA Wood. Simulation of the von mises fisher distribution. Communications in statistics-
simulation and computation, 23(1):157-164, 1994.
Jiacheng Xu and Greg Durrett. Spherical latent spaces for stable variational autoencoders. arXiv
preprint arXiv:1808.10805, 2018.
Jingwei Zhang, Jeremy Salwen, Michael Glass, and Alfio Gliozzo. Word semantic representations
using bayesian probabilistic tensor factorization. In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing (EMNLP), pp. 1522-1531, 2014.
10