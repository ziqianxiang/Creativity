Under review as a conference paper at ICLR 2020
Poincare WASSERSTEIN AUTOENCODER
Anonymous authors
Paper under double-blind review
Ab stract
This work presents the Poincare Wasserstein Autoencoder, a reformulation of
the recently proposed Wasserstein autoencoder framework on a non-Euclidean
manifold, the Poincare ball model of the hyperbolic space Hn . By assuming the
latent space to be hyperbolic, we can use its intrinsic hierarchy to impose structure
on the learned latent space representations. We show that for datasets with latent
hierarchies, we can recover the structure in a low-dimensional latent space. We
also demonstrate the model in the visual domain to analyze some of its properties
and show competitive results on a graph link prediction task.
1	Introduction
Variational Autoencoders (VAE) (17; 28) are an established class of unsupervised machine learning
models, which make use of amortized approximate inference to parametrize the otherwise intractable
posterior distribution. They provide an elegant, theoretically sound generative model used in various
data domains.
Typically, the latent variables are assumed to follow a Gaussian standard prior, a formulation which
allows for a closed form evidence lower bound formula and is easy to sample from. However, this
constraint on the generative process can be limiting. Real world datasets often possess a notion of
structure such as object hierarchies within images or implicit graphs. This notion is often reflected
in the interdependence of latent generative factors or multimodality of the latent code distribution.
The standard VAE posterior parametrizes a unimodal distribution which does not allow structural
assumptions. Attempts at resolving this limitation have been made by either "upgrading" the posterior
to be more expressive (27) or imposing structure by using various structured priors (34), (36).
Furthermore, the explicit treatment of the latent space as a Riemannian manifold has been considered.
For instance, the authors of (5) show that the standard VAE framework fails to model data with a
latent spherical structure and propose to use a hyperspherical latent space to alleviate this problem.
Similarly, we believe that for datasets with a latent tree-like structure, using a hyperbolic latent space,
which imbues the latent codes with a notion of hierarchy, is beneficial.
There has recently been a number of works which explicitly make use of properties of non-Euclidean
geometry in order to perform machine learning tasks. The use of hyperbolic spaces in particular
has been shown to yield improved results on datasets which either present a hierarchical tree-like
structure such as word ontologies (24) or feature some form of partial ordering (4). However, most of
these approaches have solely considered deterministic hyperbolic embeddings.
In this work, we propose the Poincare Wasserstein Autoencoder (PWA), a Wasserstein autoencoder
(33) model which parametrizes a Gaussian distribution in the Poincare ball model of the hyperbolic
space Hn . By treating the latent space as a Riemannian manifold with constant negative curvature,
we can use the norm ranking property of hyperbolic spaces to impose a notion of hierarchy on the
latent space representation, which is better suited for applications where the dataset is hypothesized
to possess a latent hierarchy. We demonstrate this aspect on a synthetic dataset and evaluate it using
a distortion measure for Euclidean and hyperbolic spaces. We derive a closed form definition of a
Gaussian distribution in hyperbolic space Hn and sampling procedures for the prior and posterior
distributions, which are matched using the Maximum Mean Discrepancy (MMD) objective. We
also compare the PWA to the Euclidean VAE visually on an MNIST digit generation task as well
quantitatively on a semi-supervised link prediction task.
The rest of this paper is structured as follows: we review related work in Section 2, give an overview
of the mathematical tools required to work with Riemannian manifolds as well as define the notion
1
Under review as a conference paper at ICLR 2020
of probability distributions on Riemannian manifolds in Section 3. Section 4 describes the model
architecture as well as the intuition behind the Wasserstein autoencoder approach. Furthermore, we
derive a method to obtain samples from prior and posterior distributions in order to estimate the PWA
objective. We present the performed experiments in and discuss the observed results in Section 5 and
a summary of our results in Section 6.
2	Related Work
Amortized variational inference There has been a number of extensions to the original VAE
framework (17). These extensions address various problematic aspects of the original model. The
first type aims at improving the approximation of the posterior by selecting a richer family of
distributions. Some prominent examples include the Normalizing Flow model (27) as well as its
derivates (20), (16), (7). A second direction aims at imposing structure on the latent space by selecting
structured priors such as the mixture prior (6), (34), learned autoregressive priors (36) or imposing
informational constraints on the objective (12), (38). The use of discrete latent variables has been
explored in a number of works (13) (36). The approach conceptually most similar to ours but with a
hyperspherical latent space and a von-Mises variational distribution has been presented in (5).
Hyperbolic geometry The idea of graph generation in hyperbolic space and analysis of complex
network properties has been studied in (19). The authors of (24) have recently used both the Poincare
model and the Lorentz model (25) of the hyperbolic space to develop word ontology embeddings
which carry hierarchical information encoded by the embedding norm. The general idea of treating
the latent space as a Riemannian manifold has been explored in (2). A model for Bayesian inference
for Riemannian manifolds relying on particle approximations has been proposed in (21). Finally the
natural gradient method is a prime example for using the underlying information geometry imposed
by the Fisher information metric to enhance learning performance (1).
Three concurrent works have explored an idea similar to ours. (22) propose to train a VAE with
a hyperbolic latent space using the traditional evidence lower bound (ELBO) formulation. They
approximate the ELBO using MCMC samples as opposed to our approach, which uses a Wasserstein
formulation of the problem. (23) propose to use a wrapped Gaussian distribution to obtain samples
on the Lorentz model of hyperbolic latent space. The samples are generated in Euclidean space using
classical methods and then projected onto the manifold under a concatenation of a parallel transport
and the exponential map at the mean. The authors of (10) also propose a similar approach but use an
adversarial autoencoder model in their work instead.
3	Hyperb olic geometry
In this section, we briefly outline some of the concepts from differential geometry, which are necessary
to formally define our model.
3.1	Riemannian geometry: a short overview
A Riemannian manifold is defined as a the tuple (M, g), where for every point x belonging to
the manifold M, a tangent space TxM is defined, which corresponds to a first order local linear
approximation of M at point x. The Riemannian metric g is a collection of inner products h∙∣∙)χ :
TxM × TxM → R on the tangent spaces TxM. We denote by α(t) ∈ M to be smooth curves on
the manifold. By computing the speed vector α(t) at every point of the curve, the Riemannian metric
allows the computation of the curve length:
L(a,b)(α)
Zb
a
VZgH (α(t),α(t))dt
Given a smooth curve α(a, b) → M, the distance is defined by the infimum over α(t): d =
inf L(a,b)(α). The smooth curves of shortest distance between two points on a manifold are called
geodesics.
2
Under review as a conference paper at ICLR 2020
Given a point x ∈ M, the exponential map expx(v) : TxM → M gives a way map a vector v in the
tangent space TxM at point X to the corresponding point on the manifold M. For the Poincare ball
model of the hyperbolic space, which is geodesically complete, this map is well defined on the whole
tangent space TxM. The logarithmic map logx(v) is the inverse mapping from the manifold to the
tangent space. The parallel transport Px0→x : Tx0M → TxM defines a linear isometry between
two tangent spaces of the manifold and allows to move tangent vectors along geodesics.
3.2	POINCAREBALL
Hyperbolic spaces are one of three existing types of isotropic spaces: the Euclidean spaces with zero
curvature, the spherical spaces with constant positive curvature and the hyperbolic spaces which
feature constant negative curvature. The Poincare ball is one of the five isometric models of the
hyperbolic space. The model is defined by the tuple (Bn, gH) where Bn is the open ball of radius 1, 1
gH is the hyperbolic metric and gE = In is the Riemannian metric on the flat Euclidean manifold.
Bn={X∈Rn | ||X|| < 1}
gH
The geodesic distance on the Poincare ball is given by
d(X )= = arccosh A + 2	||x-μ"2 ʌ
d(x, μ) = arccosh C + 2(1 - ∣∣X∣∣2)(1 - ∣∣μ∣∣2))
(1)
3.3	Gyrovector spaces framework and operators
In order to perform arithmetic operations on the Poincare ball model, we rely on the concept of
gyrovector spaces, which is a generalization of Euclidean vector spaces to models of hyperbolic space
based on Mobius transformations. First proposed by (35), they have been recently used to describe
typical neural network operations in the Poincare ball model of hyperbolic space (9). In order to
perform the reparametrization in hyperbolic space, we use the gyrovector addition and Hadamard
product defined as a diagonal matrix-gyrovector multiplication. Furthermore, we make use of the
exponential expμ and logarithm logμ map operators in order to map points onto the manifold and
perform the inverse mapping back to the tangent space. The Gaussian decoder network is symmetric
to the encoder network.
4	Model
4.1	GAUSSIAN DISTRIBUTION IN Hn
The Gaussian distribution is a common choice of prior for VAE style models. Similarly to the VAE,
we can select a generalization of the Gaussian distribution in the hyperbolic space as prior for our
model. In particular, we choose the maximum entropy generalization of the Gaussian distribution
(26) on the Poincare ball model. The Gaussian probability density function in hyperbolic space is
defined via the FreChet mean μ and dispersion parameter σ > 0, analogously to the density in the
Euclidean space.
1
zσe
NH (χ∣μ,σ)
d2(χ,μ)
2σ2
(2)
The main difference compared to Euclidean space is the use of the geodesic distance d(x, μ) in
the exponent and a different dispersion dependent normalization constant Z(σ) which accounts
for the underlying geometry. In order to compute the normalization constant, we use hyperbolic
1this can be generalized to radius √1c for curvature c. Throughout this paper, We assume the POinCare ball
radius to be c = 1 and omit it from the notation.
3
Under review as a conference paper at ICLR 2020
polar coordinates where r = d(x, μ) is the geodesic distance between the X and μ. This allows the
decomposition of Z(σ) into radial and angular components.
Z(σ) = Zα(σ)Zr(σ) = Vol(Sn-1) ×
∞
0
r2
e-2σ2 Sinhn-1(r) dr
We derive the closed form of the normalization constant in appendix A. For a two-dimensional space,
the normalization constant is given by (30):
Z(σ)
C ∏ σ2	.
2π^ —σe 2 erf
V 2
Dispersion representation The closed form of the hyperbolic Gaussian distribution (2) is only de-
fined for a scalar dispersion value. This can be a limitation on the expressivity of the learned represen-
tations. However, the variational family which is implicitly given by the hyperbolic reparametrization
allows for vectorial or even full covariance matrix representations, which can be more expressive.
Since the maximum mean discrepancy can be estimated via samples, we do not require a closed form
definition of the posterior density as is the case with training using the evidence lower bound. This
allows the model to learn richer latent space representations.
4.2	Model architecture
Our model mimics the general architecture of a variational autoencoder. The encoder parametrizes
the posterior variational distribution qφ(z∣x) and the decoder parametrizes the unit variance Gaussian
likelihood pθ (x|z). In order to accomodate the change in the underlying geometry of the latent space,
we introduce the maps into hyperbolic space and back to the tangent space. Both the encoder and
decoder network consist of three fully-connected layers with ReLU activations. We use the recently
proposed hyperbolic feedforward layer (9) for the encoding of the variational family parameters
(μH, σ). For the decoder fθ(x|z), we use the logarithm map at the origin log°(z) to map the posterior
sample z back into the tangent space.
Mean and variance parametrization In order to obtain posterior samples in hyperbolic space,
the parametrization of the mean uses a hyperbolic feedforward layer (W, bH) as the last layer of
the encoder network (proposed in (9)). The weight matrix parameters are Euclidean and are subject
to standard Euclidean optimization procedures (we use Adam (15)) while the bias parameters are
hyperbolic, requiring the use of Riemannian stochastic gradient descent (RSGD) (3). The outputs
of the underlying Euclidean network h are projected using the exponential map at the origin and
transformed using the hyperbolic feedforward layer map where φh, is the hyperbolic nonlinearity2:
fh(h)=2h(W0 expo(h)㊉ bh)夕h(x) = expo(夕(logo x))
4.3	Hyperbolic reparametrization trick
The reparametrization trick is a common method to make the sampling operation differentiable by
using a differentiable function g(, θ) to obtain a reparametrization gradient for backpropagation
through the stochastic layer of the network. For the location-scale family of distributions, the
reparametrization function g(e,θ) can be written as Z = μ + σ Θ E in the Euclidean space where
E 〜N(0, I). We adapt the reparametrization trick for the Gaussian distribution in the hyperbolic
space by using the framework of gyrovector operators. We obtain the posterior samples for the
parametrized mean 林口(x) and dispersion σ(x) using the following relation:
Z = μH(x)㊉ diag(σ(x))0e	(3)
We can motivate the reparametrization (3) with the help of Fig. 1, which depicts the reparametrization
in a graphical fashion. In a first step, we sample E from the hyperbolic standard prior E 〜 NH (0, 1)
using a rejection sampling procedure we describe in Algorithm 1. The samples are projected to the
2see Appendix C for the operator definitions
4
Under review as a conference paper at ICLR 2020
tangent space using the logarithm map log0 at the origin, where they are scaled using the dispersion
parameter. The scaled samples are then projected back to the manifold using the exponential map
expo and translated using μ.
Figure 1: Hyperbolic Gaussian reparametrization
4.3.1	Sampling from prior in hyperbolic space
We choose the hyperbolic standard prior NH (0, I) as prior p(z). In order to generate samples from
the standard prior, we use an approach based on the volume ratio of spheres in Hd to obtain the
quasi-uniform samples on the Poincare disk (19) and subsequently use a rejection sampling procedure
to obtain radius samples. We use the quasi-uniform distribution
g(r) = αeα(r-R)
as a proposal distribution for the radius. Using the decomposition into radial and angular components,
we can sample a direction from the unit sphere uniformly and simply scale using the sampled radius to
obtain the samples from the prior. An alternative choice of prior is the wrapped Gaussian distribution.
The samples are obtained by sampling z 〜N(0,1) in the tangent space and projecting them onto
the latent space manifold. Empirically, we have found the prior obtained via the rejection sampling
procedure and the exponential map prior to perform similarly in the context of the PWA objective. A
comparison of samples from both distributions is presented in Appendix D.
4.4	Optimization
Evidence Lower Bound The variational autoencoder relies on the evidence lower bound (ELBO)
reformulation in order to perform tractable optimization of the Kullback-Leibler divergence (KLD)
between the true and approximate posteriors. In the Euclidean VAE formulation, the KLD integral
has a closed-form expression, which simplifies the optimization procedure considerably.
The definition of the evidence lower bound can be extended to non-Euclidean spaces by using the
following formulation with the volume element of the manifold dvolgH induced by the Riemannian
metric gH .
logp(x) = log/ p(x∣z)p(z)dvolgH = log/ 噂^"，qφ(z∣x)p(z)dvolgH
≥ / log PLp)Z) qφ(z∣x)dvolgH = Ez 〜qφ[log p(x∣z)+log P(Z) — log qφ(z∣x)]
By substituting the hyperbolic Gaussian (2) into (4) we obtain the following expressions for
Eqφ(z0) log qφ(z∣x):
1	d2 (x,α),	_ L 9,	----τ∏
Eqφ(z0) [log qφ(zlx)] = Eqφ(z0) [log Z(σ)--^22^z] = const - Eqφ(z0)[log2(x + V- 1)]
Due to the nonlinearity of the geodesic distance in the exponent, we cannot derive a closed form
solution of the expectation expression Eqφ(z) [logqφ(z)]. One possibility is to use a Taylor expansion
of the first two moments of the expectation of the squared logarithm. This is however problematic
from a numerical standpoint due to the small convergence radius of the Taylor expansion. The
5
Under review as a conference paper at ICLR 2020
ELBO can be approximated using Monte-Carlo samples, as is done in (22). We have considered this
approach to be suboptimal due to large variance associated with one-sample MC approximations of
the integral.
Wasserstein metric In order to circumvent the high variance associated with the MC approximation
we propose to use a Wasserstein Autoencoder (WAE) formulation of the variational inference problem.
The authors of the WAE framework propose to solve the optimal transport problem for matching
distributions in the latent space instead of the more difficult problem of matching the data distribution
p(x) to the distribution generated by the model py (z) as is done in the generative adversarial network
(GAN) literature. Kantorovich’s formulation of the optimal transport problem is given by:
Wc(px, pg)
inf
γ∈P(X 〜Px,y 〜Py)
E(x,y)〜Γ[c(X, 丫)]
(4)
where c(x, y) is the cost function, p(x 〜pχ, y 〜Py) is the set of joint distributions of the variables
x 〜Px and y 〜Py. Solving this problem requires a search over all possible couplings Γ of the two
distributions which is very difficult from an optimization perspective. The issue is circumvented in a
WAE model as follows. The generative model of a variational autoencoder is defined by two steps.
First we sample a latent variable z from the latent space distribution P(z). In a second step, we map it
to the output space using a deterministic parametric decoder fθ(x|z). The resulting density is given
by:
P(x) =
Z
fθ (x∣z)p(z)
Under this model, the optimal transport cost (5) takes the following simpler form due to the fact that
the transportation plan factors through the map fθ .
Wc (Px,Py) = r . inf	、E(x,y)〜r[c(x, y)] =	inf	EPx Eqφ [c(x,fθ ®z))]
γ∈P(x〜Px,y〜Py )	q^φ(z)=P(Z)
The optimization procedure is over the encoders qφ(x) instead of the couplings between Px andPy.
The WAE objective is derived from the optimal transport cost (5) by relaxing the constraint on the
posterior q. The constraint is relaxed by using a Lagrangian multiplier and an appropriate divergence
measure.
LWAE =	inf	EP(x)Eq(z|x)(log P(x|z)) +βDMMD	(5)
qφ (z∣x)∈Q
The Maximum Mean Discrepancy (MMD) metric with an appropriate positive definite RKHS
3 kernel is an example of such a divergence measure. MMD is known to perform well when
matching high-dimensional standard normal distributions (11). MMD is a metric on the space
of probability distributions under the condition that the selected RKHS kernel is characteristic.
Geodesic kernels are generally not positive definite, however it has been shown that the Laplacian
kernel k(x, y) = exp(-λ(dH (x, y))) is positive definite if the metric of the underlying space is
conditionally negative definite (8). In particular, this holds for hyperbolic spaces (14). In practice,
there is a high probability that a geodesic RBF kernel is also positive definite depending on the dataset
topology (8). We choose the Laplacian kernel as it also features heavier tails than the Gaussian RBF
kernel, which has a positive effect on outlier gradients (33). The MMD loss function is defined over
two probability measures P and q in an RKHS unit ball F as follows:
DMMD(P, qφ)
|| Lk(z, ∙)dp(z) - Lk(z, ∙)dqφ(z)l∣F
(6)
There exists an unbiased estimator for DMMD(P, qφ). A finite sample estimate can be computed
based on minibatch samples from the prior Z 〜p(z) via the rejection sampling procedure described
3RKHS = Reproducing Kernel Hilbert Space
6
Under review as a conference paper at ICLR 2020
Table 1: Average distortion measure
Dataset	Metric	Model		
		T-SNE	N -VAE	PWA
Synthetic trees	Avg	0.73	0.82	0.49
in Appendix A and the approximate posterior samples Z 〜qφ(z) obtained via the hyperbolic
reparametrization:
DM( M)D (p(z), qφ(z))
占 X k"j)+f X k(zi, Zj)-
n2 X k(zi, Zj) (7)
i,j
Parameter updates The hyperbolic geometry of the latent space requires us to perform Riemannian
stochastic gradient descent (RSGD) updates for a subset of the model parameters, specifically the
bias parameters of μ. We perform full exponential map updates using gyrovector arithmetic for
the gradients with respect to the hyperbolic parameters similar to (9) instead of using a retraction
approximation as in (24). In order to avoid numerical problems at the origin and far away from the
origin of the POincare ball, We perturb the operands if the norm is close to0 or1 respectively. The
Euclidean parameters are updated in parallel using the Adam optimization procedure (15).
5	Experiments
5.1	Distortion of tree-structured data
To determine the capability of the model to retrieve an underlying hierarchy, We have setup tWo
experiments in Which We measure the average distortion of the respective latent space embeddings.
We measure the distortion betWeen the input and latent spaces using the folloWing distortion metric,
Where subscript U denotes the distances in the input space and V the distances in the latent space.
∣dv (f (a),f(b)) - du (a, b)|
du (a, b)
Noisy trees The first dataset is a set of synthetically generated noisy binary trees. The vertices of the
main tree are generated from a normal distribution Where the mean of the child nodes corresponds to
the parent sample xi = N(xp(i), σi ) and p(i) denotes the index of the parent node. In addition to the
main tree, We add K noise samples Xj = N(xi, σj) for every vertex. The dataset D = {[xi, Xj]}i,j
is a concatenation of Xi and Xj. To encourage a good embedding in a hyperbolic space, we enforce
the norms of the tree vertices to groW monotonously With the depth of the tree by rejecting samples
Whose norms are smaller than the norm of the parent vertices. We have trained our model on 100
generated trees for 100 epochs. The tree vertex variance Was set to σi = 1 and the noise variance to
σj = 0.1. We have also normalized the generated vertices to zero mean and unit variance. Table 1
compares the distortion values of the test set latent space embeddings obtained by using the Euclidean
VAE model compared to the PWA model. We can see that the PWA model shoWs less distortion
When embedding trees into the latent space of dimension d = 2, Which confirms our hypothesis that a
hyperbolic latent space is better suited to data With latent hierarchies. As a reference, We provide the
distortion scores obtained by the classical T-SNE (37) dimensionality reduction technique.
5.2	MNIST
In this experiment, We apply our model to the task of generating MNIST digits in order to get an
intuition for the properties of the latent hyperbolic geometry. In particular, We are interested in the
visual distibution of the latent codes in the Poincare disk latent space. While the MNIST latent space
is not inherently hierarchically structured - there is no obvious norm ranking that can be imposed - We
can use it to compare our model to the Euclidean VAE approach. We train the models on dynamically
7
Under review as a conference paper at ICLR 2020
binarized MNIST digits and evaluate the generated samples qualitatively as well as quantitatively via
the reconstruction error scores. We can observe in Appendix B that the samples present a deteriorating
quality as the dimensionality increases despite the lower reconstruction error. This can be explained
by the issue of dimension mismatch between the selected latent space dimensionality dz and the
intrinsic latent space dimensionality dI documented in (29) and can be alleviated by an additional
p-norm penalty on the variance. We have not observed a significant improvement by applying the
L2-penalty for higher dimensions. We have also performed an experiment using a two-dimensional
latent space. We can observe that the structure imposed by the Poincare disk pushes the samples
towards the outside of the disk. This observation can be explained by the fact that hyperbolic spaces
grow exponentially. In order to generate quality samples using the prior, some overlap is required with
the approximate posterior in the latent space. The issue is somewhat alleviated in higher dimensions
as the distribution shifts towards the ball surface.
fo6⅛/Jqs0n3
N Gt≠∖37oo^z
?r-rx，771- n-⅛
60$71？33d3
I- S t/ S ɪl 5，*x9A
5-，，qo5 3 65「
/ O F25¾>1ql /
^∙s^zo*∕b2z*i
72 ^/¥4 X∕OQ4∕
7 ¥。I zc√⅛0 5
CoNS4∕qga01E
& O&5-c400
伊 qoQ7H4qα*7
O。q4α2 £F70
&37X。何ez4G
Ir5^∖5e0qG6Q O
/t O 5r- qob ?«.
Figure 2: A comparison of the Euclidean VAE (left) and PWA samples (right), |z| = 5
5.3 Link prediction on citation networks
In this experiment, we aim at exploring the advantages of using a hyperbolic latent space on the
task of predicting links in a graph. We train our model on three different citation network datasets:
Cora, Citeseer and Pubmed (32). We use the Variational Graph Auto-Encoder (VGAE) framework
(18) and train the model in an unsupervised fashion using a subset of the links. The performance is
measured in terms of average precision (AP) and area under curve (AUC) on a test set of links that
were masked during training. Table 1 shows a comparison to the baseline with a Euclidean latent
space (N -VGAE), showing improvements on the Cora and Citeseer datasets. We also compare our
results to the results obtained using a hyperspherical autoencoder (S-VGAE) (5). It should be noted
that we have used a smaller dimensionality for the hyperbolic latent space (16 vs 64 and 32 for the
Euclidean and hyperspherical cases respectively), which could be attributed to the fact that a dataset
with a hierarchical latent manifold requires latent space embeddings of smaller dimensionality to
efficiently encode the information (analogously to the results of (24)). We can observe that the PWA
outperforms the Euclidean VAE on two of the three datasets. The hyperspherical graph autoencoder
(S-VGAE) outperforms our model. One hypothesis which explains this is the fact that the structure
of the citation networks has a tendency towards a positive curvature rather than a negative one. It is
worth noting that it is not entirely transparent whether the use of Graph Convolutional Networks (18),
which present a very simple local approximation of the convolution operator on graphs, allows to
preserve the curvature of the input data.
8
Under review as a conference paper at ICLR 2020
Table 2: Performance on link prediction datasets
Dataset	Metric	Model		
		N -VGAE	S-VGAE	PWA
Cora	AUC	92.7±,~	94.1±.1	93.9±.2
	AP	93.2±.4	94.1±.3	93.2±.2
Citeseer	AUC	90.3±.5	94.7±.2	92.2±.2
	AP	91.5±.5	95.2±.2	91.8±.2
Pubmed	AUC	97.1±.0	96.0±.ι	95.9±.2
	AP	97.1±.o	96.0±.2	96.3±.2
6 Conclusion
We have presented an algorithm to perform amortized variational inference on the POinCare ball
model of the hyperbolic space. The underlying geometry of the hyperbolic space allows for an
improved performance on tasks which exhibit a partially hierarchical structure. We have discovered
certain issues related to the use of the MMD metric in hyperbolic space. Future work will aim to
circumvent these issues as well as extend the current results. In particular, we hope to demonstrate
the capabilities of our model on more tasks hypothesized to have a latent hyperbolic manifold and
explore this technique for mixed curvature settings.
References
[1]	S.-I. Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251-276,
1998.
[2]	G. Arvanitidis, L. K. Hansen, and S. Hauberg. Latent space oddity: on the curvature of deep
generative models. arXiv preprint arXiv:1710.11379, 2017.
[3]	S. Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on
Automatic Control, 58(9):2217-2229, 2013.
[4]	B. P. Chamberlain, J. Clough, and M. P. Deisenroth. Neural embeddings of graphs in hyperbolic
space. arXiv preprint arXiv:1705.10359, 2017.
[5]	T. R. Davidson, L. Falorsi, N. De Cao, T. Kipf, and J. M. Tomczak. Hyperspherical variational
auto-encoders. arXiv preprint arXiv:1804.00891, 2018.
[6]	N. Dilokthanakul, P. A. Mediano, M. Garnelo, M. C. Lee, H. Salimbeni, K. Arulkumaran, and
M. Shanahan. Deep unsupervised clustering with gaussian mixture variational autoencoders.
arXiv preprint arXiv:1611.02648, 2016.
[7]	L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density estimation using real nvp. arXiv preprint
arXiv:1605.08803, 2016.
[8]	A. Feragen, F. Lauze, and S. Hauberg. Geodesic exponential kernels: When curvature and
linearity conflict. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3032-3042, 2015.
[9]	O.-E. Ganea, G. Becigneul, and T. Hofmann. Hyperbolic neural networks. arXiv preprint
arXiv:1805.09112, 2018.
[10]	D. Grattarola, D. Zambon, C. Alippi, and L. Livi. Learning graph embeddings on constant-
curvature manifolds for change detection in graph streams. arXiv preprint arXiv:1805.06299,
2018.
[11]	A. Gretton, K. M. Borgwardt, M.J. Rasch, B. Scholkopf, and A. Smola. A kernel two-sample
test. Journal of Machine Learning Research, 13(Mar):723-773, 2012.
9
Under review as a conference paper at ICLR 2020
[12]	I. Higgins, L. Matthey, X. Glorot, A. Pal, B. Uria, C. Blundell, S. Mohamed, and A. Lerchner.
Early visual concept learning with unsupervised deep learning. arXiv preprint arXiv:1606.05579,
2016.
[13]	E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016.
[14]	P J6ziak. Conditionally strictly negative definite kernels. Linear and Multilinear Algebra,
63(12):2406-2418, 2015.
[15]	D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[16]	D. P. Kingma, T. Salimans, R. Jozefowicz, X. Chen, I. Sutskever, and M. Welling. Im-
proving variational inference with inverse autoregressive flow.(nips), 2016. URL http://arxiv.
org/abs/1606.04934.
[17]	D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,
2013.
[18]	T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks.
arXiv preprint arXiv:1609.02907, 2016.
[19]	D. Krioukov, F. Papadopoulos, M. Kitsak, A. Vahdat, and M. BOgun£ Hyperbolic geometry of
complex networks. Physical Review E, 82(3):036106, 2010.
[20]	H. Larochelle and I. Murray. The neural autoregressive distribution estimator. In Proceedings of
the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 29-37,
2011.
[21]	C. Liu and J. Zhu. Riemannian stein variational gradient descent for bayesian inference. In
Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
[22]	E. Mathieu, C. L. Lan, C. J. Maddison, R. Tomioka, and Y. W. Teh. Hierarchical representations
with poincar\’e variational auto-encoders. arXiv preprint arXiv:1901.06033, 2019.
[23]	Y. Nagano, S. Yamaguchi, Y. Fujita, and M. Koyama. A differentiable gaussian-like distribution
on hyperbolic space for gradient-based learning. arXiv preprint arXiv:1902.02992, 2019.
[24]	M. Nickel and D. Kiela. PoinCare embeddings for learning hierarchical representations. In
Advances in neural information processing systems, pages 6338-6347, 2017.
[25]	M. Nickel and D. Kiela. Learning continuous hierarchies in the lorentz model of hyperbolic
geometry. arXiv preprint arXiv:1806.03417, 2018.
[26]	X. Pennec. Intrinsic statistics on riemannian manifolds: Basic tools for geometric measurements.
Journal of Mathematical Imaging and Vision, 25(1):127, 2006.
[27]	D. J. Rezende and S. Mohamed. Variational inference with normalizing flows. arXiv preprint
arXiv:1505.05770, 2015.
[28]	D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate
inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
[29]	P. K. Rubenstein, B. Schoelkopf, and I. Tolstikhin. On the latent space of wasserstein auto-
encoders. arXiv preprint arXiv:1802.03761, 2018.
[30]	S. Said, L. Bombrun, and Y. Berthoumieu. New riemannian priors on the univariate normal
model. Entropy, 16(7):4015-4031, 2014.
[31]	S. Said, L. Bombrun, Y. Berthoumieu, and J. H. Manton. Riemannian gaussian distributions on
the space of symmetric positive definite matrices. IEEE Transactions on Information Theory,
63(4):2153-2170, 2017.
10
Under review as a conference paper at ICLR 2020
[32]	P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classifica-
tion in network data. AI magazine, 29(3):93, 2008.
[33]	I. Tolstikhin, O. Bousquet, S. Gelly, and B. Schoelkopf. Wasserstein auto-encoders. arXiv
preprint arXiv:1711.01558, 2017.
[34]	J. M. Tomczak and M. Welling. Vae with a vampprior. arXiv preprint arXiv:1705.07120, 2017.
[35]	A. A. Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis Lectures on
Mathematics and Statistics,1(1):1-194, 2008.
[36]	A. van den Oord, O. Vinyals, et al. Neural discrete representation learning. In Advances in
Neural Information Processing Systems, pages 6306-6315, 2017.
[37]	L. van der Maaten and G. Hinton. Visualizing data using t-SNE. Journal of Machine Learning
Research, 9:2579-2605, 2008.
[38]	S. Zhao, J. Song, and S. Ermon. Infovae: Information maximizing variational autoencoders.
arXiv preprint arXiv:1706.02262, 2017.
11
Under review as a conference paper at ICLR 2020
A Prior rejection sampling
 Algorithm 1: prior rejection sampling
Input: maximum support radius rmax , dimensionality d, quasi-uniform α parameter, hyperbolic
prior likelihood NH(r) (r|0, 1)
Result: n samples from prior p(z)
while i < n do
sample 中〜N(0, Id);
compute direction on the unit sphere 0 =阍∣;
sample U 〜U(0,1);
get uniform radius samples ri ∈ [0, rmax] via ratio of hyperspheres;
ri = (U * rm,ax) 1 ；
evaluate p(xi) = fr(ri);
M = max(pi);
g(r) = αeα(r-rmax);
sample U 〜U(0,1);
if u < M⅛) then
I accept sample Xi；
else
I reject sample;
end
end
Output: prior samples z = r0
B Normalization constant derivation
We can derive the normalization constant Z(σ) for the Gaussian distribution in Hn for curvature
c = -1 by using the hyperbolic polar coordinates. The integral form is given by (31):
Z(σ) = Zr(σ)Zα(σ) = Vol(Sn-1) ×
∞	r2	_1
e-2σ2 Sinhn-1(r) dr
0
The normalization constant can be factorized into the radius and the angle part. The volume of a unit
hypersphere is given by:
Zα(σ) = Vol(Sn-1)
n— 1
∏ F-
Γ(n-1 + 1)
We can derive a closed form for the normalization constant as follows:
12
Under review as a conference paper at ICLR 2020
∞	r2
Zr (σ) =	e- 2σ2 Sinhn-1(r)dr
Jo
∞	∕er - e-r∖n-1
=J0 e-ydd
f∞ -二 S (n - I),八k (e(n-1-k)re(-rk\ /
/ e 2σ2X( k )(-1) (	2n-1	ddr
⅛Γ X (n -1)(-i)k / ∞ e-2⅛ m>dτ
k=0 v	j	1°
2⅛Γ X (n -1)(-1)k [ e-(袤r2-(n-ir)dr
We use the following identity for the solution of the definite integral:
Γ° e-"+bx)dx
0
Setting a =a,b = 2k +1 - n, C = 0, we obtain
Zr(σ)
1
2n-1
n—1
X
k=0
((2k + 1 — n)σ ʌ
[-√2-)
where erfc is the complementary error function.
C List of gyrovector operations
In this list of gyrovector operations and throughout this paper, we assume the POinCare ball radius to
be C = 1 and omit it from the notation.
Gyrovector addition:
Q = (1 + 2hχ, yi + MyM2 )χ +(1 -UXU2)y
电 y =	1 + 2hχ, y)+ ∣∣χ∣∣2∣∣y∣∣2]
Matrix-gyrovector product:
M0χ = tanh (ɪɪiparctanh(∣∣χ∣∣)) -Mχ∣∣
∖ ∣∣x∣∣	) ∣∣mχ∣∣
Exponential map:
expx(v) = X ㊉(tanh CI
Logarithm map:
2	一χ ㊉ V
IOgX(V)=元arctanh(∣∣ - X㊉ v∣∣) ∣∣- X㊉ v∣∣
Parallel transport:
PXo→x(V) = IOgX(V ㊉ exPxo (V)) = *V
λx
13
Under review as a conference paper at ICLR 2020
D Hyperbolic Gaussian samples
This section presents a comparison of samples obtained from the hyperbolic Gaussian and the wrapped
Gaussian distributions.
The means and variances are given as follows. (μ1, σ 1) = ((0.0, 0.0), (1.0,1.0)), (μ2, σ2) =
((0.6, 0.0), (1.0,1.0)),	(μ3, σ3)	=	((0.6,	0.4), (1.0,1.0)),μ,σ4)	=	((0.6, 0.4), (0.7, 0.3)),
(μ5, σ5) = ((0.6, 0.4), (0.1, 0.4))
Figure 3: Hyperbolic Gaussian samples
Figure 4: Wrapped Gaussian samples
14
Under review as a conference paper at ICLR 2020
E MNIST Visual Samples
∈
∈
3。。Cðz31
7zδ5«TS377Z
∖>8gH t**,rζpo-r7-
NZ43 4 夕<?a /。
CnC3c5G3gαs
eRO4 ¥9 532 5
r),一b。AπsJ7%g"
zGQCX-5 4σ∕a
r9, *⅛/ / 2 20
1 //，3 ιΓ，，/ɔ
reconstruction error L
√ :34 Wy⅛⅛FW
天了七7ɜ*/W < L
2夕S-1 4 2√q q?
5Qe 5∙i5‹÷κ/
'Acfq⅛千 W T<& N
7ʃ-i*⅛ ¥ ,72q τ⅛
*√*Λ6∙⅛^ςv∕542
J∙J V⅛L%^>1 -ɔ
4∙l-⅛6τ Yq 由才∙y
£* ¢,?2 H,-r-A夕
夕彳。7a 171sq
7fΛ5703，13
α14G6∕43G∕9
656Γ⅛5Λr∕r4/
57 76 7 263nq
Zr OS∕X,6O33
d6o，l/5Go/“
夕 3g0GZa57Γ-
∈ {5, 10, 20}, training
〃.:--> 1J?< &£>，7
匕 X -VfG -，，： Y 儡 9C
tD t j 了/r乙U1
Γ17G 7。yJ>vi
crJ 5,7τz^⅛s>Z
∙%7。-∙ 5T∖‰JFJ∙
「4 7 7 7cγiJ% 53
77iu q T 6 乙37
∈	{5, 10, 20}, training reconstruction error L
O
2。76 J ZB-G
3-7J2a<J 0oz‰d
SO 64,5 qs3≡
Nj 2≠∖378c/
71?，77i⅛nJ
6 O 7i?33/u
(-S H 6 1 5#29 A
ΓF∕K5^^5Π
f OJ*z5^lul½
匕 5“ Z2/62，I
72z7⅛4 夕/24/
Figure 5: Euclidean VAE samples d
{109.01, 94.58, 93.36}
Figure 6: Poincare WAE samples d
{95.01, 69.70, 58.58}
15
Under review as a conference paper at ICLR 2020
Figure 7: MNIST samples from two-dimensional hyperbolic latent space
16