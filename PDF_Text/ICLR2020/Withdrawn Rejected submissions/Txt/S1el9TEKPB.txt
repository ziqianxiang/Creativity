Under review as a conference paper at ICLR 2020
Sparsity Meets Robustness: Channel Pruning
for the Feynman-Kac Formalism Principled
Robust Deep Neural Nets
Anonymous authors
Paper under double-blind review
Ab stract
Deep neural nets (DNNs) compression is crucial for adaptation to mobile devices.
Though many successful algorithms exist to compress naturally trained DNNs,
developing efficient and stable compression algorithms for robustly trained DNNs
remains widely open. In this paper, we focus on a co-design of efficient DNN com-
pression algorithms and sparse neural architectures for robust and accurate deep
learning. Such a co-design enables us to advance the goal of accommodating both
sparsity and robustness. With this objective in mind, we leverage the relaxed aug-
mented Lagrangian based algorithms to prune the weights of adversarially trained
DNNs, at both structured and unstructured levels. Using a Feynman-Kac formal-
ism principled robust and sparse DNNs, we can at least double the channel sparsity
of the adversarially trained ResNet20 for CIFAR10 classification, meanwhile, im-
prove the natural accuracy by 8.69% and the robust accuracy under the benchmark
20 iterations of IFGSM attack by 5.42%.
1 Introduction
Robust deep neural nets (DNNs) compression is a fundamental problem for secure AI applications in
resource-constrained environments such as biometric verification and facial login on mobile devices,
and computer vision tasks for the internet of things (IoT) (Cheng et al., 2017; Yao et al., 2017;
Mohammadi et al., 2018). Though compression and robustness have been separately addressed in
recent years, much less is studied when both players are present and must be satisfied.
To date, many successful techniques have been developed to compress naturally trained DNNs, in-
cluding neural architecture re-design or searching (Howard et al., 2017; Zhang et al., 2018b), pruning
including structured (weights sparsification) (Han et al., 2015; Srinivas & Babu, 2015) and unstruc-
tured (channel-, filter-, layer-wise sparsification) (Yang et al., 2019; He et al., 2017), quantization
(Zhou et al., 2017; Yin et al., 2019; Courbariaux et al., 2016), low-rank approximation (Denil et al.,
2013), knowledge distillation (Polino et al., 2018), and many more (Alvarez & Salzmann, 2016; Liu
et al., 2015).
(a) NT
(b) AT
Figure 1: Histograms of the ResNet20’s weights.
The adversarially trained (AT) DNN is more robust than the naturally trained (NT) DNN to adver-
sarial attacks (Madry et al., 2018; Athalye et al., 2018). However, adversarial training (denoted as
AT if no ambiguity arises, and the same for NT) also dramatically reduces the sparsity of the trained
DNN’s weights. As shown in Fig. 1, start from the same default initialization in PyTorch, the NT
ResNet20’s weights are much sparser than that of the AT counterpart, for instance, the percent of
1
Under review as a conference paper at ICLR 2020
weights that have magnitude less than 10-3 for NT and AT ResNet20 are 8.66% and 3.64% (aver-
aged over 10 trials), resp. This observation motivates us to consider the following two questions:
•	1. Can we re-design the neural architecture with minimal change on top of the existing one such
that the new DNN has sparser weights and better robustness and accuracy than the existing one?
•	2. Can we develop efficient compression algorithms to compress the AT DNNs with minimal
robustness and accuracy degradations?
We note that under the AT, the recently proposed Feynman-Kac formalism principled ResNet ensem-
ble (Wang et al., 2019a) has much sparser weights than the standard ResNet, which gives a natural
answer to the first question above. To answer the second question, we leverage state-of-the-art re-
laxed augmented Lagrangian based sparsification algorithms (Dinh & Xin, 2018; Yang et al., 2019)
to perform both structured and unstructured pruning for the AT DNNs. We focus on unstructured
and channel pruning in this work.
1.1	Notation
Throughout this paper we use bold upper-case letters A, B to denote matrices, bold lower-case
letters x, y to denote vectors, and lower case letters x, y and α, β to denote scalars. For vector
X = (χι,...,χd)>, we use ∣∣xk = ∣∣xk2 = /x2 H---------+ Xd to represent its '2-norm; ∣∣xkι =
Pd=ι |xi| to represent its 'ι-norm; and ∣∣x∣o = Pd=i χ{χi=o} to represent its 'o-norm. For a
function f : Rd → R, we use Vf (∙) to denote its gradient. Generally, Wt represents the set of all
parameters of the network being discussed at iteration t, e.g. wt = (w1t, w2t , ..., wMt ), where wjt
is the weight on the jth layer of the network at the t-th iteration. Similarly, ut = (ut1, ut2, ..., utM)
is a set of weights with the same dimension as wt, whose value depends on wt and will be defined
below. We use N(0, Id×d) to represent the d-dimensional Gaussian, and use notation O(∙) to hide
only absolute constants which do not depend on any problem parameter.
1.2	Organization
This paper is organized in the following way: In section 2, we list the most related work to this paper.
In section 3, we show that the weights of the recently proposed Feynman-Kac formalism principled
ResNet ensemble are much sparser than that of the baseline ResNet, providing greater efficiency
for compression. In section 4, we present relaxed augmented Lagrangian-based algorithms along
with theoretical analysis for both unstructured and channeling pruning of AT DNNs. The numerical
results are presented in section 5, followed by concluding remarks. Technical proofs and more
related results are provided in the appendix.
2 Related Work
Compression ofAT DNNs: Gui et al. (2019) considered a low-rank form of the DNN weight matrix
with `0 constraints on the matrix factors in the AT setting. Their training algorithm is a projected
gradient descent (PGD) (Madry et al., 2018) based on the worst adversary. In their paper, the sparsity
in matrix factors are unstructured and require additional memory.
Sparsity and Robustness: Guo et al. (2018) shows that there is a relationship between the sparsity
of weights in the DNN and its adversarial robustness. They showed that under certain conditions,
sparsity can improve the DNN’s adversarial robustness. The connection between sparsity and ro-
bustness has also been studied recently by Ye et al. (2019), Rakin et al. (2019), and et al. In our
paper, we focus on designing efficient pruning algorithms integrated with sparse neural architec-
tures to advance DNNs’ sparsity, accuracy, and robustness.
Feynman-Kac formalism principled Robust DNNs: Neural ordinary differential equations
(ODEs) (Chen et al., 2018) are a class of DNNs that use an ODE to describe the data flow of each
input data. Instead of focusing on modeling the data flow of each individual input data, Wang et al.
(2019a; 2018a); Li & Shi (2017) use a transport equation (TE) to model the flow for the whole input
distribution. In particular, from the TE viewpoint, Wang et al. (2019a) modeled training ResNet (He
2
Under review as a conference paper at ICLR 2020
et al., 2016) as finding the optimal control of the following TE
'∂U(x，t) + G(x, w(t)) ∙ Vu(x, t) = 0, X ∈ Rd,
u(x, 1) = g(x), x ∈ Rd,	(1)
、u(xi, 0) = yi, Xi ∈ T, with T being the training set.
where G(x, w(t)) encodes the architecture and weights of the underlying ResNet, u(x, 0) serves as
the classifier, g(X) is the output activation of ResNet, and yi is the label of Xi.
Wang et al. (2019a) interpreted adversarial vulnerability of ResNet as arising from the irregularity of
u(x, 0) of the above TE. To enhance u(x, 0)'s regularity, they added a diffusion term, 1 σ2∆u(x, t),
to the governing equation of (1) which resulting in the convection-diffusion equation (CDE). By the
Feynman-Kac formula, u(x, 0) of the CDE can be approximated by the following two steps:
•	Modify ResNet by injecting Gaussian noise to each residual mapping.
•	Average the output ofn jointly trained modified ResNets, and denote it as EnnResNet.
Wang et al. (2019a) have noticed that EnResNet can improve both natural and robust accuracies of
the AT DNNs. In this work, we leverage the sparsity advantage of EnResNet to push the sparsity
limit of the AT DNNs.
3	Regularity and Sparsity of the Feynman-Kac Formalism
Principled Robust DNNs’ Weights
From a partial differential equation (PDE) viewpoint, a diffusion term to the governing equation
(1) not only smooths u(x, 0), but can also enhance regularity of the velocity field G(x, w(t))
(Ladyzenskaja et al., 1988). As a DNN counterpart, We expect that when We plot the weights of
EnResNet and ResNet at a randomly select layer, the pattern of the former one will look smoother
than the latter one. To validate this, we follow the same AT with the same parameters as that used in
(Wang et al., 2019a) to train En5ResNet20 and ResNet20, resp. After the above two robust models
are trained, we randomly select and plot the weights of a convolutional layer of ResNet20 whose
shape is 64 × 64 × 3 × 3 and plot the weights at the same layer of the first ResNet20 in En5ResNet20.
As shown in Fig. 2 (a) and (b), most of En5ResNet20’s weights are close to 0 and they are more reg-
ularly distributed in the sense that the neighboring weights are closer to each other than ResNet20’s
weights. The complete visualization of this randomly selected layer’s weights is shown in the ap-
pendix. As shown in Fig. 2 (c) and (d), the weights of En5ResNet20 are more concentrated at zero
than that of ResNet20, and most of the En5ResNet20’s weights are close to zero.
(b) En5ResNet20 (AT)
(a) ResNet20 (AT)
Figure 2: (a) and (b): weights visualization; (c) and (d): histogram of weights.
20.0%
10.0%
°,0%-0.25 0.00	0.25
Value
(c) ResNet20 (AT)
0.0%
+j 20.0%
U
①
前 10.0%
(d) En5ResNet20 (AT)
0.00	0.25
Value
4	Unstructured and Channel Pruning with AT
4.1	Algorithms
In this subsection, we introduce relaxed, augmented Lagrangian-based, pruning algorithms to spar-
sify the AT DNNs. The algorithms of interest are the Relaxed Variable-Splitting Method (RVSM)
(Dinh & Xin, 2018) for weight pruning (Algorithm 1), and its variation, the Relaxed Group-wise
Splitting Method (RGSM) (Yang et al., 2019) for channel pruning (Algorithm 2).
Our approach is to apply the RVSM/RGSM algorithm together with robust PGD training to train
and sparsify the model from scratch. Namely, at each iteration, we apply a PGD attack to generate
3
Under review as a conference paper at ICLR 2020
adversarial images x0, which are then used in the forward-propagation process to generate predi-
tions y0 . The back-propagation process will then compute the appropriate loss function and apply
RVSM/RGSM to update the model. Previous work on RVSM mainly focused on a one-hidden layer
setting; In this paper, we extend this result to the general setting. To the best of our knowledge, this
is the first result that uses RVSM/RGSM in an adversarial training scenario.
To explain our choice of algorithm, we discuss a classical algorithm to promote sparsity of the
target weights, the alternating direction of multiplier method (ADMM) (Boyd et al., 2011; Goldstein
& Osher, 2009). In ADMM, instead of minimizing the original loss function f (w), we seek to
minimize the `1 regularized loss function, f(w) + λkuk1, by considering the following augmented
Lagrangian
L(w, u, Z)= f (w) + λ ∣∣ukι + hz, W - Ui + 2 ∣∣w - uk2, λ,β ≥ 0.	(2)
which can be easily solved by applying the following iterations
{wt+1 J argminw Le(w, ut, Zt)
ut+1 J argminuLe(wt+1, u, Zt)	(3)
Zt+1 J Zt + β(wt+1 - ut+1)
Although widely used in practice, ADMM has several drawbacks when it is used to regularize
DNN’s weights. First, one can improve the sparsity of the final learned weights by replacing ∣u∣1
with ∣∣u∣∣o; but k∙∣o is not differentiable, thus current theory of optimization does not apply (Wang
et al., 2018b). Second, the update wt+1 J arg minw Le(w, ut, Zt) is not a reasonable step in
practice, as one has to fully know how f(w) behaves. In most ADMM adaptation on DNN, this step
is replaced by a simple gradient descent. Third, the Lagrange multiplier term, hZ, w - ui, seeks to
close the gap between wt and ut, and this in turn reduces sparsity ofut.
The RVSM we will implement is a relaxation of ADMM. RVSM drops the Lagrangian multiplier,
and replaces λ∣u∣1 with λ∣u∣0, and resulting in the following relaxed augmented Lagrangian
Le(w, U) = f(w) + λ ∣∣u∣0 + 2 I∣w — U∣2.	(4)
The above relaxed augmented Lagrangian can be solved efficiently by the iteration in Algo-
rithm 1. RVSM can resolve all the three issues associated with ADMM listed above in train-
ing robust DNNs with sparse weights: First, by removing the linear term hZ, w - Ui, one has
a closed form formula for the update of Ut without requiring ∣U∣0 to be differentiable. Explic-
itly, Ut = H√2λ∕β(Wt) = (WtX{∣wι∣>√2λ∕β},…,wdX{∣wι∣>√2λ∕β}), where Has is the hard-
thresholding operator with parameter α. Second, the update of wt is a gradient descent step itself,
so the theoretical guarantees will not deviate from practice. Third, without the Lagrange multiplier
term Zt, there will be a gap between Wt and Ut at the limit (finally trained DNNs). However, the
limit ofUt is much sparser than that in the case of ADMM. At the end of each training epoch, we re-
place Wt by Ut for the validation process. Numerical results in Section 5 will show that the AT DNN
with parameters Ut usually outperforms the traditional ADMM in both accuracy and robustness.
Algorithm 2 RGSM
Algorithm 1 RVSM
Input: η, β, λ, maxepoch, maxbatch
Initialization: W0
Define： u0 = H√2λ∕β(WO)
for t = 0, 1, 2, ..., maxepoch do
for batch = 1, 2, ..., maxbatch do
wt+1 J wt — ηVf (Wt) — ηβ(wt - Ut)
Ut+1 J argminu Le(u, Wt) = H√λ∕β(Wt)
end for
end for
Input: η, β, λ1, λ2, maxepoch, maxbatch
Objective: f(w) = f(W) + λ2∣∣w∣∣GL
Initialization： Initialize W0, define U0
for g = 1, 2, ..., G do
U0g = P roxλ1 (Wg0)
end for
for t = 0, 1, 2, ..., maxepoch do
for batch = 1,2, ..., maxbatch do
Wt+1 = Wt-ηVf(Wt)-ηβ(Wt-ut)
for g = 1, 2, ..., G do
Utg+1 = P roxλ1 (Wgt )
end for
end for
end for
4
Under review as a conference paper at ICLR 2020
RGSM is a method that generalizes RVSM to structured pruning, in particular, channel pruning. Let
w = {w1, ..., wg, ..., wG} be the grouped weights of convolutional layers of a DNN, where G is the
total number of groups. Let Ig be the indices of w in group g. The group Lasso (GLasso) penalty
and group-`0 penalty (Yuan & Lin, 2007) are defined as
G
kwkGL := Xkwgk2,
g=1
G
kwkG'o =EIkwg k2 = 0
g=1
(5)
and the corresponding Proximal (projection) operators are
PrOxGL,λ(wg):= Sgn(Wg)max(∣∣Wg∣∣2 - λ, 0),	ProxG'0,λ(wg):= Wg1kwg k2 = √2λ	⑹
where Sgn(Wg) := Wg/∣∣Wg∣∣2. The RGSM method is described in Algorithm 2, which improves
on adding group Lasso penalty directly in the objective function (Wen et al., 2016) for natural DNN
training (Yang et al., 2019).
4.2	Theoretical Guarantees
We propose a convergence analysis of the RVSM algorithm to minimize the Lagrangian (4). Con-
sider the following empirical adversarial risk minimization (EARM)
1n
miH n i=1 kχi-χaχ∞≤e L(F(Xi, w),yi)
(7)
where the classifier F(∙, W) is a function in the hypothesis class H, e.g. ResNet and its ensembles,
parametrized by W. Here, L(F(xi, W), yi) is the appropriate loss function associated with F on the
data-label pair (xi, yi), e.g. cross-entropy for classification and root mean square error for regression
problem. Since our model is trained using PGD AT, let
f (w) = E(χ,y)〜D [∏ιa X L(F (x0, W),y)]	(8)
where x0 is obtained by applying the PGD attack to the clean data x (Wang et al., 2019a; Goodfellow
et al., 2014a; Madry et al., 2018; Na et al., 2018). In a nutshell, f(W) is the population adversarial
loss of the network parameterized by W = (W1, W2, ..., WM). Before proceeding, we first make the
following assumption:
Assumption 1. Let W1, W2, ..., WM be the weights in the M layers of the given DNN, then there
exists a positive constant L such that for all t,
∣Vf(∙, Wj+1, ∙) -Vf (∙, Wtj ,∙)∣ ≤ L∣Wj+1 - Wj∣, for j = 1,2,…，M.	(9)
Assumption 1 is a weaker version of that made by Wang et al. (2019b); Sinha et al. (2018), in which
the empirical adversarial loss function is smooth in both the input x and the parameters W. Here we
only require the population adversarial loss f to be smooth in each layer of the DNN in the region
of iterations. An important consequence of Assumption 1 is
f(∙, Wj+1, ∙) — f(∙, WjQ ≤hVf(∙, Wj, ∙),(0,…,Wj+1- Wj, 0,...)i + L ∣Wj+1 - Wjk2 (10)
Theorem 1. Under the Assumption 1, suppose also that the RVSM algorithm is initiated with a
small stepsize η such that η < β++L. Then the Lagrangian Le (Wt, ut) decreases monotonically and
converges Sub-Sequentiauy to a limit point (W, U).
The proof of Theorem 1 is provided in the Appendix. From the descent property of Lβ(Wt, ut),
classical results from optimization (Nesterov, 2014) can be used to show that after T = O(1/2)
iterations, we have Vwt Lβ(Wt, ut) = O(), for some t ∈ (0, T]. The term ∣u∣0 promotes sparsity
and β ∣∣w - u∣2 helps keep W close to u. Since U = H√2λ∕β(w)，it follows that W will have lots
of very small (and thus negligible) components. This result justifies the sparsity in the limit U.
5
Under review as a conference paper at ICLR 2020
5	Numerical Results
In this section, we verify the following advantages of the proposed algorithms:
•	RVSM/RGSM is efficient for unstructured/channel-wise pruning for the AT DNNs.
•	After pruning by RVSM and RGSM, EnResNet’s weights are significantly sparser than the base-
line ResNet’s, and more accurate in classifying both natural and adversarial images.
These two merits lead to the fact that a synergistic integration of RVSM/RGSM with the Feynman-
Kac formula principled EnResNet enables sparsity to meet robustness.
We perform AT by PGD integrated with RVSM, RGSM, or other sparsification algorithms on-the-
fly. For all the experiments below, we run 200 epochs of the PGD (10 iterations of the iterative fast
gradient sign method (IFGSM10) with α = 2/255 and = 8/255, and an initial random perturbation
of magnitude ). The initial learning rate of 0.1 decays by a factor of 10 at the 80th, 120th, and 160th
epochs, and the RVSM/RGSM/ADMM sparsification takes place in the back-propagation stage. We
split the training data into 45K/5K for training and validation, and the model with the best validation
accuracy is used for testing. We test the trained models on the clean images and attack them by
FGSM, IFGSM20, and C&W with the same parameters as that used in (Wang et al., 2019a; Zhang
et al., 2019; Madry et al., 2018). We denote the accuracy on the clean images and under the FGSM,
IFGSM20 (Goodfellow et al., 2014b), C&W (Carlini & Wagner, 2016), and NAttack (Li et al., 2019)
1 attacks as A1, A2, A3, A4, and A5, resp. A brief introduction of these attacks is available in the
appendix. We use both sparsity and channel sparsity to measure the performance of the pruning
algorithms, where the sparsity is defined to be the percentage of zero weights; the channel sparsity
is the percentage of channels whose weights’ `2 norm is less than 1E - 15.
5.1	Model Compression for AT ResNet and EnResNets
First, we show that RVSM is efficient to sparsify ResNet and EnResNet. Table 1 shows the accu-
racies of ResNet20 and En2ResNet20 under the unstructured sparsification with different sparsity
controlling parameter β. We see that after the unstructured pruning by RVSM, En2ResNet20 has
much sparser weights than ResNet20. Moreover, the sparsified En2ResNet20 is remarkably more ac-
curate and robust than ResNet20. For instance, when β = 0.5, En2ResNet20’s weights are 16.42%
sparser than ResNet20’s (56.34% vs. 39.92%). Meanwhile, En2ResNet20 boost the natural and
robust accuracies of ResNet20 from 74.08%, 50.64%, 46.67%, and 57.24% to 78.47%, 56.13%,
49.54%, and 65.57%, resp. We perform a few independent trials, and the random effects is small.
Table 1: Accuracy and sparsity of ResNet20 and En2ResNet20 under different attacks and β, with
λ = 1E - 6. (Unit: %, n/a: do not perform sparsification. Same for all the following tables.)
I	ResNet20							En2ResNet20			
β	AI	A2	A3	A4	Sparsity ∣ Ai		A2	A3	A4	Sparsity
n/a	76.07	51.24	47.25	59.30	0	80.34	57.11	50.02	66.77	0
0.01	70.26	46.68	43.79	55.59	80.91	72.81	51.98	46.62	63.10	89.86
0.1	73.45	49.48	45.79	57.72	56.88	77.78	55.48	49.26	65.56	70.55
0.5	74.08	50.64	46.67	57.24	39.92	78.47	56.13	49.54	65.57	56.34
Second, we verify the effectiveness of RGSM in channel pruning. We lists the accuracy and chan-
nel sparsity of ResNet20, En2ResNet20, and En5ResNet20 in Table 2. Without any sparsification,
En2ResNet20 improves the four type of accuracies by 4.27% (76.07% vs. 80.34%), 5.87% (51.24%
vs. 57.11%), 2.77% (47.25% vs. 50.02%), and 7.47% (59.30% vs. 66.77%), resp. When we set
β = 1, λ1 = 5e - 2, and λ2 = 1e - 5, after channel pruning both natural and robust accuracies of
ResNet20 and En2ResNet20 remain close to the unsparsified models, but En2ResNet20’s weights
are 33.48% (41.48% vs. 8%) sparser than that of ResNet20’s. When we increase the channel sparsity
level by increasing λ1 to 1e - 1, both the accuracy and channel sparsity gaps between ResNet20 and
1For NAttack, we use the default parameters in https://github.com/cmhcbb/attackbox.
6
Under review as a conference paper at ICLR 2020
En2ResNet20 are enlarged. En5ResNet20 can future improve both natural and robust accuracies on
top of En2ReSNet20. For instance, at 〜55% (53.36% vs. 56.74%) channel sparsity, En5ResNet20
can improve the four types of accuracy of En2ResNet20 by 4.66% (80.53% vs. 75.87%), 2.73%
(57.38% vs. 54.65%), 2.86% (50.63% vs. 47.77%), and 1.11% (66.52% vs. 65.41%), resp.
Table 2: Accuracy and sparsity of different EnResNet20. (Ch. Sp.: Channel Sparsity)
Net	β	λ1	λ2	Ai	A2	A3	A4	A5	Ch. Sp.
ResNet20	n/a	n/a	n/a	76.07	51.24	47.25	59.30	45.88	0
	1	5.E-02	1.E-05	75.91	51.52	47.14	58.77	45.02	8.00
	1	1.E-01	1.E-05	71.84	48.23	45.21	57.09	43.84	25.33
En2ResNet20	n/a	n/a	n/a	80.34	57.11	50.02	66.77	49.35	0
	1	5.E-02	1.E-05	78.28	56.53	49.58	66.56	49.11	41.48
	1	1.E-01	1.E-05	75.87	54.65	47.77	65.41	46.77	56.74
En5ResNet20	n/a	n/a	n/a	81.41	58.21	51.60	66.48	50.21	0
	1	1.E-02	1.E-05	81.46	58.34	51.35	66.84	50.07	19.76
	1	2.E-02	1.E-05	80.53	57.38	50.63	66.52	48.23	53.36
Third, we show that an ensemble of small ResNets via the Feynman-Kac formalism performs
better than a larger ResNet of roughly the same size in accuracy, robustness, and sparsity. We
AT En2ResNet20 (〜0.54M parameters) and ResNet38 (〜0.56M parameters) with and without
channel pruning. As shown in Table 3, under different sets of parameters, after RGSM pruning,
En2ResNet20 always has much more channel sparsity than ResNet38, also much more accurate and
robust. For instance, when we set β = 1, λ1 = 5e - 2, and λ2 = 1e - 5, the AT ResNet38 and
En2ResNet20 with channel pruning have channel sparsity 17.67% and 41.48%, resp. Meanwhile,
En2ResNet20 outperforms ResNet38 in the four types of accuracy by 0.36% (78.28% vs. 77.92%),
3.02% (56.53% vs. 53.51%), 0.23% (49.58% vs. 49.35%), and 6.34% (66.56% vs. 60.32%), resp.
When we increase λ1, the channel sparsity of two nets increase.. As shown in Fig. 3, En2ResNet20’s
channel sparsity growth much faster than ResNet38’s, and we plot the corresponding four types of
accuracies of the channel sparsified nets in Fig. 4.
Table 3: PerformanCe of En2ReSNet20 and ReSNet38 Under RVSM.
Net	β	λ1	λ2	Ai	A2	A3	A4	Ch. Sp.
En2ResNet20	n/a	n/a	n/a	80.34	57.11	50.02	66.77	0
ResNet38	n/a	n/a	n/a	78.03	54.09	49.81	61.72	0
En2ResNet20	1	5.E-02	1.E-05	78.28	56.53	49.58	66.56	41.48
ResNet38	1	5.E-02	1.E-05	77.92	53.51	49.35	60.32	17.67
En2ResNet20	1	1.E-01	1.E-05	76.30	54.65	47.77	65.41	56.74
ResNet38	1	1.E-01	1.E-05	72.95	49.78	46.48	57.92	43.80
Figure 3:	Sparsity of
En2ResNet20 and ResNet38
under different parameters
λ1. (5 runs)
61
61
Figure 4: Accuracy of En2ResNet20 and ResNet38 under
different parameters λ1. (5 runs)
7
Under review as a conference paper at ICLR 2020
Table 4: Contrasting ADMM versus RVSM for the AT ResNet20.
Unstructured Pruning	Channel Pruning
	AI	A2	A3	A4	Sp.	AI	A2	A3	A4	Ch. Sp.
	 RVSM	70.26	46.68	43.79	55.59	80.91	71.84	48.23	45.21	57.09	25.33
ADMM	71.55	47.37	44.30	55.79	10.92	63.99	42.06	39.75	51.90	4.44
5.2 RVSM/RGSM versus ADMM
In this subsection, we will compare RVSM, RGSM, and ADMM (Zhang et al., 2018a) 2 for unstruc-
tured and channel pruning for the AT ResNet20, and we will show that RVSM and RGSM iterations
can promote much higher sparsity with less natural and robust accuracies degradations than ADMM.
We list both natural/robust accuracies and sparsities of ResNet20 after ADMM, RVSM, and RGSM
pruning in Table 4. For unstructured pruning, ADMM retains slightly better natural (〜1.3%) and
robust (〜0.7%,〜0.5%, and 0.2% under FGSM, IFGSM20, and C&W attacks) accuracies. HoW-
ever, RVSM gives much better sparsity (80.91% vs. 10.89%). In the channel pruning scenario,
RVSM significantly outperforms ADMM in all criterion including natural and robust accuracies and
channel sparsity, as the accuracy gets improved by at least 5.19% and boost the channel sparsity
from 4.44% to 25.33%. Part of the reason for ADMM’s inefficiency in sparsifying DNN’s Weights
is due to the fact that the ADMM iterations try to close the gap betWeen the Weights wt and the
auxiliary variables ut, so the final result has a lot of Weights With small magnitude, but not small
enough to be regarded as zero (having norm less than 1e-15). The RVSM does not seek to close
this gap, instead it replaces the Weight wt by ut , Which is sparse, after each epoch. This results
in a much sparser final result, as shoWn in Figure 5: ADMM does result in a lot of channels With
small norms; but to completely prune these off, RVSM does a better job. Here, the channel norm is
defined to be the `2 norm of the Weights in each channel of the DNN (Wen et al., 2016).
loo
uno□
#zeros: 171
O
O
1
4unou
#zeros: 171
0.5	1.0
Ch. norm
1	2
Ch. norm le-7
8：0
(a) RVSM
(b) RVSM (Zoom in)
O
10
4unoo
≠zeros: 30
50
4un8
#zeros: 30
0.5
Ch. norm
1.0
1	2
Ch. norm le-7
β
O
(c) ADMM	(d) ADMM (Zoom in)
Figure 5: Channel norms of the AT ResNet20 under RVSM and ADMM.
5.3	Beyond ResNet Ensemble and Beyond CIFAR10
5.4	Beyond ResNet Ensemble
In this part, we show that the idea of average over noise injected ResNet generalizes to other types
of DNNs, that is, the natural and robust accuracies of DNNs can be improved by averaging over
noise injections. As a proof of concept, we remove all the skipping connections in ResNet20 and
2We use the code from: github.com/KaiqiZhang/admm-pruning
8
Under review as a conference paper at ICLR 2020
En2ResNet20, in this case, the model no longer involves a TE and CDE. As shown in Table 5,
removing the skip connection degrades the performance significantly, especially for En2ResNet20
(all the four accuracies of the ATEn2ResNet20 reduces more than the AT ResNet20’s). These results
confirm that skip connection is crucial, without it the TE model assumption breaks down, and the
utility of EnResNets reduces. However, the ensemble of noise injected DNNs still improves the
performance remarkably.
Table 5: Accuracies of ResNet20 and En2ResNet20 with and without skip connections.
Net	TyPe	A1	A2	A3	A4
ResNet20	Base Net	76.07	51.24	47.25	59.30
	Without skip connection	75.45	51.03	47.22	58.44
En2ResNet20	Base net	80.34	57.11	50.02	66.77
	Without skip connection	79.12	55.76	49.92	66.26
Next, let us consider the sparsity, robustness, and accuracies of ResNet20 and En2 ResNet20 without
any skip connection, when they are AT using weights sparsification by either RVSM or RGSM.
For RVSM pruning, we set β = 5E - 2 and λ = 1E - 6; for RGSM channel pruning, we set
β = 5E - 2, λ1 = 5E - 2, and λ2 = 1E - 5. We list the corresponding results in Table 6. We
see that under both RVSM and RGSM pruning, En2ResNet20 is remarkably more accurate on both
clean and adversarial images, and significantly sparser than ResNet20. When we compare the results
in Table 6 with that in Tables 1 and 2, we conclude that once we remove the skip connections, the
sparsity, robustness, and accuracy degrades dramatically.
Table 6: Sparsity and accuracies of ResNet20 and En2ResNet20 without skip connection under
different pruning algorithms.
Net	Pruning Algorithm	Ai	A2	A3	A4	Sp.	Ch. Sp.
ResNet20	RVSM	73.72	50.46	46.98	58.28	0.05	0.15
(no skip connection)	RGSM	74.63	50.44	46.86	58.05	1.64	9.04
En2ResNet20	RVSM	76.95	55.17	49.28	58.35	10.87	9.48
(no skip connection)	RGSM	78.51	56.55	49.71	67.08	7.95	15.48
5.5	BEYOND CIFAR 1 0
Besides CIFAR10, we further show the advantage of EnResNet + RVSM/RGSM in compressing
and improving accuracy/robustness of the AT DNNs for CIFAR100 classification. We list the natural
and robust accuracies and channel sparsities of the AT ResNet20 and En2ResNet20 with different
RGSM parameters (n/a stands for do not perform channel pruning) in Table 7. For λ1 = 0.05,
RGSM almost preserves the performance of the DNNs without channel pruning, while improving
channel sparsity by 7.11% for ResNet20, and 16.89% for En2ResNet20. As we increase λ1 to
0.1, the channel sparsity becomes 18.37% for ResNet20 and 39.23% for En2ResNet20. Without
any channel pruning, En2ResNet20 improves natural accuracy by 4.66% (50.68% vs. 46.02%),
and robust accuracies by 5.25% (30.2% vs. 24.77%), 3.02% (26.25% vs. 23.23%), and 7.64%
(40.06% vs. 32.42%), resp., under the FGSM, IFGSM20, and C&W attacks. Even in very high
channel sparsity scenario (λ1 = 0.05), En2ResNet20 still dramatically increase A1, A2, A3, and
A4 by 2.90%, 4.31%, 1.89%, and 5.86%, resp. These results are similar to the one obtained on
the CIFAR10 in Table 2. These results further confirm that RGSM together with the Feynman-Kac
formalism principled ResNets ensemble can significantly improve both natural/robust accuracy and
sparsity on top of the baseline ResNets.
6 Concluding Remarks
The Feynman-Kac formalism principled AT EnResNet’s weights are much sparser than the baseline
ResNet’s. Together with the relaxed augmented Lagrangian based unstructured/channel pruning
9
Under review as a conference paper at ICLR 2020
Table 7: Accuracy and sparsity of different Ensembles of ResNet20's on the CIFAR100.
Net	β	λ1	λ2	Ai	A2	A3	A4	Ch. Sp.
ResNet20	n/a	n/a	n/a	46.02	24.77	23.23	32.42	0
	1	5.E-02	1.E-05	45.74	25.34	23.55	33.53	7.11
	1	1.E-01	1.E-05	44.34	24.46	23.12	32.38	18.37
En2 ResNet20	n/a	n/a	n/a	50.68	30.2	26.25	40.06	0
	1	5.E-02	1.E-05	50.56	30.33	26.23	39.85	16.89
	1	1.E-01	1.E-05	47.24	28.77	25.01	38.24	39.23
algorithms, we can compress the AT DNNs much more efficiently, meanwhile significantly improves
both natural and robust accuracies of the compressed model. As future directions, we propose to
quantize EnResNets and to integrate neural ODE into our framework.
10
Under review as a conference paper at ICLR 2020
References
Jose M Alvarez and Mathieu Salzmann. Learning the number of neurons in deep networks. In
Advances in Neural Information Processing Systems, pp. 2270-2278, 2016.
A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false sense of security: Circum-
venting defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.
S.	Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein, et al. Distributed optimization and statistical
learning via the alternating direction method of multipliers. Foundations and TrendsR in Machine
learning, 3(1):1-122, 2011.
N. Carlini and D.A. Wagner. Towards evaluating the robustness of neural networks. IEEE European
Symposium on Security and Privacy, pp. 39-57, 2016.
T.	Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud. Neural ordinary differential equations. In
Advances in neural information processing systems, pp. 6571-6583, 2018.
Y. Cheng, D. Wang, P. Zhou, and T. Zhang. A survey of model compression and acceleration for
deep neural networks. arXiv preprint arXiv:1710.09282, 2017.
M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and Y. Bengio. Binarynet: Training deep neural
networks with weights and activations constrained to +1 or -1. ArXiv, abs/1602.02830, 2016.
M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas. Predicting parameters in deep
learning. In Proceedings of the 26th International Conference on Neural Information Process-
ing Systems - Volume 2, NIPS’13, pp. 2148-2156, USA, 2013. Curran Associates Inc. URL
http://dl.acm.org/citation.cfm?id=2999792.2999852.
T. Dinh and J. Xin. Convergence of a relaxed variable splitting method for learning sparse neural
networks via `1, `0, and transformed-`1 penalties. arXiv preprint arXiv:1812.05719, 2018.
T. Goldstein and S. Osher. The split bregman method for l1-regularized problems. SIAM journal on
imaging sciences, 2(2):323-343, 2009.
I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. CoRR,
abs/1412.6572, 2014a.
I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. arXiv
preprint arXiv:1412.6275, 2014b.
S. Gui, H. Wang, C. Yu, H. Yang, Z. Wang, and J. Liu. Adversarially trained model compression:
When robustness meets efficiency. arXiv preprint arXiv:1902.03538, 2019.
Y. Guo, C. Zhang, C. Zhang, and Y. Chen. Sparse dnns with improved adversarial robustness. In
Advances in neural information processing systems, pp. 242-251, 2018.
S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights and connections for efficient neural
network. In Advances in neural information processing systems, pp. 1135-1143, 2015.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.
Y. He, X. Zhang, and J. Sun. Channel pruning for accelerating very deep neural networks. In The
IEEE International Conference on Computer Vision (ICCV), Oct 2017.
A.	Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam.
Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861, 2017.
O. Ladyzenskaja, V. Solonnikov, and N. Urarceva. Linear and quasi-linear equations of parabolic
type, volume 23. American Mathematical Soc., 1988.
Y. Li, L. Li, L. Wang, T. Zhang, and B. Gong. Nattack: Learning the distributions of adver-
sarial examples for an improved black-box attack on deep neural networks. arXiv preprint
arXiv:1905.00441, 2019.
11
Under review as a conference paper at ICLR 2020
Z. Li and Z. Shi. Deep residual learning and pdes on manifold. arXiv preprint arXiv:1708.05115,
2017.
B.	Liu, M. Wang, H. Foroosh, M. Tappen, and M. Pensky. Sparse convolutional neural networks. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 806-814,
2015.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models
resistant to adversarial attacks. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=rJzIBfZAb.
M. Mohammadi, A. Al-Fuqaha, S. Sorour, and M. Guizani. Deep learning for iot big data and
streaming analytics: A survey. IEEE Communications Surveys & Tutorials, 20(4):2923-2960,
2018.
T. Na, J. Ko, and S. Mukhopadhyay. Cascade adversarial machine learning regularized with a unified
embedding. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=HyRVBzap-.
Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer Publishing
Company, Incorporated, 1 edition, 2014. ISBN 1461346916, 9781461346913.
A. Polino, R. Pascanu, and D. Alistarh. Model compression via distillation and quantization. arXiv
preprint arXiv:1802.05668, 2018.
A. Rakin, Z. He, L. Yang, Y. Wang, L. Wang, and D. Fan. Robust sparse regularization: Simultane-
ously optimizing neural network robustness and compactness. arXiv preprint arXiv:1905.13074,
2019.
A. Sinha, H. Namkoong, and J. Duchi. Certifiable distributional robustness with principled ad-
versarial training. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=Hk6kPgZA-.
S. Srinivas and R. V. Babu. Data-free parameter pruning for deep neural networks. arXiv preprint
arXiv:1507.06149, 2015.
B. Wang, X. Luo, Z. Li, W. Zhu, Z. Shi, and S. Osher. Deep neural nets with interpolating function
as output activation. In Advances in Neural Information Processing Systems, pp. 743-753, 2018a.
B. Wang, B. Yuan, Z. Shi, and S. Osher. ResNet ensemble via the Feynman-Kac formalism to
improve natural and robust acurcies. In Advances in Neural Information Processing Systems,
2019a.
Y. Wang, J. Zeng, and W. Yin. Global Convergence of ADMM in Nonconvex Nonsmooth Optimiza-
tion. Journal of Scientific Computing, online, 2018b. doi: 10.1007/s10915-018-0757-z.
Y. Wang, X. Ma, J. Bailey, J. Yi, B. Zhou, and Q. Gu. On the convergence and robustness of
adversarial training. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of
the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 6586-6595, Long Beach, California, USA, 09-15 Jun 2019b. PMLR.
URL http://proceedings.mlr.press/v97/wang19i.html.
W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning structured sparsity in deep neural networks.
In Advances in neural information processing systems, pp. 2074-2082, 2016.
B. Yang, J. Lyu, S. Zhang, Y-Y Qi, and J. Xin. Channel pruning for deep neural networks via a
relaxed group-wise splitting method. In Proc. of 2nd International Conference on AI for Industries
(AI4I), Laguna Hills, CA, 2019.
S. Yao, Y. Zhao, A. Zhang, L. Su, and T. Abdelzaher. Deepiot: Compressing deep neural network
structures for sensing systems with a compressor-critic framework. In Proceedings of the 15th
ACM Conference on Embedded Network Sensor Systems, pp. 4. ACM, 2017.
12
Under review as a conference paper at ICLR 2020
S. Ye, K. Xu, S. Liu, H. Cheng, J. Lambrechts, H. Zhang, A. Zhou, K. Ma, Y. Wang, and X. Lin.
Second rethinking of network pruning in the adversarial setting. arXiv preprint arXiv:1903.12561,
2019.
P. Yin, S. Zhang, J. Lyu, S. Osher, Y. Qi, and J. Xin. Blended coarse gradient descent for full
quantization of deep neural networks. Research in the Mathematical Sciences, 6(1):14, Jan 2019.
ISSN 2197-9847. doi: 10.1007/s40687-018-0177-6. URL https://doi.org/10.1007/
s40687-018-0177-6.
M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal
ofthe Royal Statistical Society, Series B, 68(1):49-67, 2007.
H. Zhang, Y. Yu, J. Jiao, E. Xing, L. Ghaoui, and M. Jordan. Theoretically principled trade-off
between robustness and accuracy. arXiv preprint arXiv:1901.08573, 2019.
T Zhang, S Ye, K Zhang, J Tang, W Wen, M Fardad, and Y Wang. A systematic dnn weight
pruning framework using alternating direction method of multipliers. arXiv preprint 1804.03294,
Jul 2018a. URL https://arxiv.org/abs/1804.03294.
X. Zhang, X. Zhou, M. Lin, and J. Sun. Shufflenet: An extremely efficient convolutional neural
network for mobile devices. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 6848-6856, 2018b.
A. Zhou, A. Yao, Y. Guo, L. Xu, and Y. Chen. Incremental network quantization: Towards lossless
cnns with low-precision weights. arXiv preprint arXiv:1702.03044, 2017.
13
Under review as a conference paper at ICLR 2020
A Proof of Theorem 1
By the arg min update of ut, Lβ (wt+1, ut+1) ≤ Lβ (wt+1, ut).	It remains to show
Lβ (wt+1 , ut) ≤ Lβ (wt, ut). To this end, we show
Lβ (w1t+1, w2t, ..., wMt ,ut) ≤ Lβ(w1t,w2t, ..., wMt ,ut)
the conclusion then follows by a repeated argument. Notice the only change occurs in the first layer
w1t . For simplicity of notation, only for the following chain of inequality, let wt = (w1t , ..., wMt )
and wt+1 = (w1t+1, w2t , ..., wMt ). Then for a fixed u := ut, we have
Lβ (wt+1, u) - Lβ (wt, u)
=f (wt+1) - f (wt) + 2 (kwt+1 - uk2 - kwt - uk2)
≤hVf(wt), wt+1 - Wti + 2kwt+1 - wtk2 + 2 (kwt+1 - uk2 - kwt - uk2
=1 hwt - wt+1, wt+1 - Wti - βhwt - u, wt+1 - Wti
η
+Lkwt+1 - wtk2 + β (kwt+1 - uk2 -kwt - uk2)
-Wti+(2+2) kwt+1 - wtk2
=1 hwt- wt+1, wt+1
η
+βkwt+1 - uk2 - βkwt - uk2 - βhwt - u, wt+1 - wti - βkwt+1 - wtk2
=(2+2 -I}3- wtk2
Thus, when η ≤ j+^, We have Le(wt+1, U) ≤ Le(wt, u). Apply the above argument repeatedly,
we arrive at
Le(wt+1, ut+1) ≤ Le(wt+1, ut) ≤ Le(w1t+1, w2t+1, ..., wMt+-1 1, wMt , ut) ≤ ... ≤ Le (wt, ut)
This implies Le (wt, ut) decreases monotonically. Since Le(wt, ut) ≥ 0, (wt, ut) must converge
SUb-SeqUentiany to a limit point (W, U). This completes the proof.
B Adversarial Attacks Used in This Work
We focus on the '∞ norm based untargeted attack. For a given image-label pair {x, y},a given ML
model F(x, w), and the associated loss L(x, y) :=L(F(x, w), y):
•	Fast gradient sign method (FGSM) searches an adversarial, x0, within an '∞-ball as
X0 = X + 〜sign (VχL(x, y)).
•	Iterative FGSM (IFGSMM) (Goodfellow et al., 2014b) iterates FGSM and clip the range as
X(m) = Clipχ,e {x(mT) + α ∙ sign(Vx(m-i)L(x(m-1), y))} , w/ x(0) = x, m = 1, ∙∙∙ ,M.
•	C&W attack (Carlini & Wagner, 2016) searches the minimal perturbation (δ) attack as
min ∣∣δ∣∣∞, subject to F(w, X + δ) = t, X + δ ∈ [0,1]d, for ∀t = y.
δ
•	NAttack (Li et al., 2019) is an effective gradient-free attack.
14
Under review as a conference paper at ICLR 2020
C More Visualizations of the DNNs’ Weights
In section 3, we showed some visualization results for part of the weights of a randomly selected
convolutional layer of the AT ResNet20 and En5ResNet20. The complete visualization results of
this selected layer are shown in Figs. 6 and 7, resp., for ResNet20 and En5ResNet20. These plots
further verifies that:
•	The magnitude of the weights of the adversarially trained En5ResNet20 is significantly smaller
than that of the robustly trained ResNet20.
•	The overall pattern of the weights of the adversarially trained En5ResNet20 is more regular than
that of the robustly trained ResNet20.
-0.10
(a)
(d)
(g)
-0.05
-0.05
-0.10
-0.10
(b)
(e)
-0.05
-0.10
(h)
-0.05
-0.05
-0.10
-0.10
(c)
-0.10
(f)
-0.05
-0.10
(i)
-0.□5
-0.05
Figure 6: Weights of a randomly selected convolutional layer of the PGD AT ResNet20.
15
Under review as a conference paper at ICLR 2020
(b)
(c)
(d)
(e)
(f)
-0.01
(h)
(g)
(i)
Figure 7: Weights of the PGD AT En5ResNet20 at the same layer as that shown in Fig. 6.
-0.01
-0.02
-0.03
-0.02
-0.03
-0.01
-0.02
-0.03
-0.04
-0.01
-0.02
-0.03
(a)
-0.01
-0.02
-0.03
-0.01
-0.02
-0.01
-0.02
16