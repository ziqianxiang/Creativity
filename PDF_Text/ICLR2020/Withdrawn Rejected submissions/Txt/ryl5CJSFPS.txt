Under review as a conference paper at ICLR 2020
Generalization Guarantees for Neural Nets
via Harnessing the Low-rankness of Jacobian
Anonymous authors
Paper under double-blind review
Ab stract
Modern neural network architectures often generalize well despite containing many
more parameters than the size of the training dataset. This paper explores the
generalization capabilities of neural networks trained via gradient descent. We de-
velop a data-dependent optimization and generalization theory which leverages the
low-rank structure of the Jacobian matrix associated with the network. Our results
help demystify why training and generalization is easier on clean and structured
datasets and harder on noisy and unstructured datasets as well as how the network
size affects the evolution of the train and test errors during training. Specifically, we
use a control knob to split the Jacobian spectum into “information" and “nuisance"
spaces associated with the large and small singular values. We show that over the
information space learning is fast and one can quickly train a model with zero
training loss that can also generalize well. Over the nuisance space training is
slower and early stopping can help with generalization at the expense of some bias.
We also show that the overall generalization capability of the network is controlled
by how well the labels are aligned with the information space. A key feature of
our results is that even constant width neural nets can provably generalize for suffi-
ciently nice datasets. We conduct various numerical experiments on deep networks
that corroborate our theoretical findings and demonstrate that: (i) the Jacobian of
typical neural networks exhibit low-rank structure with a few large singular values
and many small ones leading to a low-dimensional information space, (ii) over the
information space learning is fast and most of the labels falls on this space, and (iii)
label noise falls on the nuisance space and impedes optimization/generalization.
1	Introduction
1.1	Motivation and contributions
Deep neural networks (DNN) are ubiquitous in a growing number of domains ranging from computer
vision to healthcare. State-of-the-art DNN models are typically overparameterized and contain more
parameters than the size of the training dataset. It is well understood that in this overparameterized
regime, DNNs are highly expressive and have the capacity to (over)fit arbitrary training datasets
including pure noise Zhang et al. (2016). Mysteriously however neural network models trained via
simple algorithms such as (stochastic) gradient descent continue to predict well or generalize on yet
unseen test data. In this paper we wish to take a step towards demystifying this phenomenon and
help explain why neural nets can overfit to noise yet have the ability to generalize when real data sets
are used for training. In particular we explore the generalization dynamics of neural nets trained via
gradient descent. Using the Jacobian mapping associated with the neural network we characterize
directions where learning is fast and generalizable versus directions where learning is slow and leads
to overfitting. The main contributions of this work are as follows.
•	Leveraging dataset structure: We develop new optimization and generalization results that can
harness the low-rank representation of semantically meaningful datasets via the Jacobian mapping of
the neural net. This sheds light as to why training and generalization is easier using datasets where
the features and labels are semantically linked versus others where there is no meaningful relationship
between the features and labels (even when the same network is used for training).
•	Bias-variance tradeoffs: We develop a generalization theory based on the Jacobian which decou-
ples the learning process into information and nuisance spaces. We show that gradient descent almost
perfectly interpolates the data over the information space (incurring only a small bias). In contrast,
optimization over the nuisance space is slow and results in overfitting due to higher variance.
1
Under review as a conference paper at ICLR 2020
•	Network size vs prediction bias: We obtain data-dependent tradeoffs between the network size
and prediction bias. Specifically, we show that larger networks result in smaller prediction bias, but
small networks can still generalize well, especially when the dataset is sufficiently structured, but
typically incur a larger bias. This compares favorably with recent literature on optimization and
generalization of neural networks Jacot et al. (2018); Arora et al. (2019); Du et al. (2018b); Allen-Zhu
et al. (2018b); Cao & Gu (2019); Ma et al. (2019); Allen-Zhu et al. (2018a); Brutzkus et al. (2017)
where guarantees only hold for very wide networks with the width of the network growing inversely
proportional to the class margins or related notions. See Section 3 for further detail.
•	Pretrained models: Our framework does not require random initialization and our results con-
tinue to apply even with arbitrary initialization. Therefore, our results may shed light on the
generalization capabilities of networks initialized with pre-trained models commonly used in
meta/transfer learning. Our extensive experiments strongly suggest Jacobian adapts over time
in a favorable and data-dependent fashion shedding light on the properties of (pre)trained models.
1.2	Model and training
Our theoretical analysis will focus on neural net-
works consisting of one hidden layer with d in-
put features, k hidden neurons and K outputs as
depicted in Figure 1. We use W ∈ Rk×d and
V ∈ RK×k to denote the input-to-hidden and hidden-
to-output weights. The overall input-output relation-
ship of the network is a function f (∙; W) :Rd →
RK that maps an input x ∈ Rd to an output via
x ↦ f(x; W) := V φ(W x).	(1.1)
Given a dataset consisting of n feature/label pairs
(xi, yi ) with xi ∈ Rd representing the features
and yi ∈ RK the associated labels representing
one of K classes with one-hot encoding (i.e. yi ∈
{e1, e2, . . . , eK} where e` ∈ RK has all zero en-
h3
X2
X3
^
Xd
hk
W
xι	/
V
二 cat
y2;—►
y = v φ(Wx) y
output layer label
X
input layer
h=φ(Wx)
hidden layer
Figure 1: Illustration of a one-hidden layer neu-
ral net with d inputs, k hidden units and K
outputs along with a one-hot encoded label.
tries except for the `th entry which is equal to one).
To learn this dataset, we fix the output layer and train over W via1
n
WmRn×dL(W):= 2 ∑ IV φ (Wxi) 一 yi 吆.
(1.2)
It will be convenient to concatenate the labels and prediction vectors as follows
yi
URnK
「V f(xi; W)]
and f(W) =	⋮	∈ RnK
「V f(xn ； W )J
Using this shorthand we can rewrite the loss (1.2) as
12
WRn×d L(W):= 2 f(W) 一 y*.
(1.3)
(1.4)
To optimize this loss starting from an initialization W0 we run gradient descent iterations of the form
Wτ+1 = Wτ - ηvL(Wτ),
(1.5)
with a step size η. In this paper we wish to explore the theoretical properties of the model found by
such iterative updates with an emphasis on the generalization ability.
1.3	Information and Nuisance Spaces
In order to understand the generalization capabilities of models trained via gradient descent we need to
develop better insights into the form of the gradient updates and how it affects the training dynamics.
To this aim let US aggregate the weights at each iteration into one large vector wτ :=VeCt(Wr) ∈ Rkd,
define the misfit/residual vector r(w) := f(w) - y and note that the gradient updates take the form
wτ+1 = wτ - ηvL(wτ) where vL(w) = vL(w) = J (w)T r(w).
1For clarity of exposition, we focus only on optimizing the input layer. However, as shown in the supplementary,
the technical approach is quite general and applies to arbitrary multiclass nonlinear least-squares problems. In
particular, the proofs are stated so as to apply to one-hidden layer networks where both layers are trained.
2
Under review as a conference paper at ICLR 2020
1. Large singulars
1. Small singulars
2. Fast learning
2. Slow learning
murtceps naibocaJ
(a) Depiction via the Jacobian spectrum
(b) Depiction in parameter space
Figure 2: Depiction of the training and generalization dynamics of gradient methods
based on the information and nuisance spaces associated with the neural net Jacobian.
Here, J(W) ∈ RnK×kd denotes the Jacobian mapping associated with f defined as J(W) = fw).
Due to the form of the gradient updates the dynamics of training is dictated by the spectrum of the
Jacobian matrix as well as the interaction between the residual vector and the Jacobian. If the residual
vector is very well aligned with the singular vectors associated with the top singular values of J(W),
the gradient update significantly reduces the misfit allowing substantial reduction in the train error.
Thus to provide a more precise understanding of the training dynamics and generalization capabilities
of neural networks it is crucial to develop a better understanding of the interaction between the
Jacobian and the misfit and label vectors. To capture these interactions we require a few definitions.
Definition 1.1 (Information & Nuisance Spaces) Consider a matrix J ∈ RnK×p with singular
value decomposition given by
nK
J = ∑ λsusvsT = Udiag(λ1,λ2, . . . ,λnK) VT,
s=1
with λ1 ≥ λ2 ≥ . . . ≥ λnK denoting the singular values of J in decreasing order and {us }sn=K1 ∈ RnK
and {vs}sn=K1 ∈ Rp the corresponding left and right singular vectors forming the orthonormal basis
matrices U ∈ RnK×nK and V ∈ Rp×nK. Fora spectrum cutoff α obeying 0 ≤ α ≤ λ∖ let r ：= r(α)
denote the index of the smallest singular value above α. We define the information and nuisance
spaces associated with J as I ：= span({us}rs=1) andN ：= span({us}sK=nr+1).
In this paper we shall use either the expected value of the Jacobian at the random initialization or the
Jacobian at one of the iterates to define the matrix J and the corresponding information/nuisance
spaces. More, specifically we will set J to either J = (E[J(W0)JT(W0)])1/2 or J = J(Wτ).
Therefore, one can effectively think of the information space as the span of the prominent singular
vectors of the Jacobian and the nuisance space as its complement. In particular, as we demonstrate in
Section 4 the Jacobian mapping associated with neural networks exhibit low-rank structure with a
few large singular values and many small ones leading to natural choices for the cut-off value α as
well as the information and nuisance spaces. Furthermore, we demonstrate both (empirically and
theoretically) that learning is fast over the information space leading to a significant reduction in
both train/test accuracy in the early stages of training. However, after a certain number of iterations
learning shifts to the nuisance space and reduction in the training error significantly slows down (see
Fig. 2). Furthermore, subsequent iterations in this stage lead to a slight increase in test error.
2	Main results
Our main results establish multi-class generalization bounds for neural networks trained via gradient
descent. First, we will focus on networks where both layers are randomly initialized. Next we will
provide guarantees for arbitrary initialization with the goal of characterizing the generalization ability
of subsequent iterative updates for a given (possibly pre-trained) network in terms of its Jacobian
mapping. In this paper we focus on activations φ which are smooth and have bounded first and
3
Under review as a conference paper at ICLR 2020
second order derivatives. This would for instance apply to the softplus activation φ(z) = log (1 + ez).
We note that utilizing a proof technique developed in Oymak & Soltanolkotabi (2019) for going
from smooth to ReLU activations it is possible to extend our results to ReLU activations with proper
modifications. We avoid doing this in the current paper for clarity of exposition. Before we begin
discussing our main results we discuss some notation used throughout the paper. For a matrix
X ∈ Rn×d We use Smin(X) and SmaX(X) = IlXIlto denote the minimum and maximum singular
value of X. For two matrices A and B We use A Θ B and A ⑥ B to denote their Hadamard and
Kronecker products, respectively. For a PSD matrix A ∈ Rn×n With eigenvalue decomposition
A = ∑n=ι λiUiUτ, the square root matrix is defined as A1/2 ：= ∑n=ι √λiuiuτ. We also use At
to denote the pseudo-inverse of A. In this paper we mostly focus on label vectors y which are
one-hot encoded i.e. all entries are zero except one of them. For a subspace S ⊂ Rn and point x ∈ Rn,
ΠS (x) denotes the projection of x onto S. Finally, before stating our results we need to provide a
quantifiable measure of performance for a trained model. Given a sample (x, y) ∈ Rd × RK from a
distribution D, the classification error of the network W with respect to D is defined as
ErrD(W ) = P{
arg max n` ≠ arg max
1≤'≤K	1≤'≤K
f'(x; W)}.
(2.1)
2.1	Results for random initialization
To explore the generalization of randomly initialized networks, we utilize the neural tangent kernel.
Definition 2.1 (Multiclass Neural Tangent Kernel (M-NTK) Jacot et al. (2018)) Let w ∈ Rd be
a vector with N(0, Id) distribution. Consider a set of n input data points x1, x2, . . . , xn ∈ Rd
aggregated into the rows of a data matrix X ∈ Rn×d. Associated to the activation φ and the input
data matrix X we define the multiclass kernel matrix as
Σ(X) ：= IK Θ E [(φ' (Xw) φ' (Xw)T) Θ (XXT)],
where IK is the identity matrix of size K. Here, the ` th diagonal block of Σ(X) corresponds to
the kernel matrix associated with the ` th network output for 1 ≤ ` ≤ K . This kernel is intimately
related to the multiclass Jacobian mapping. In particular, suppose the initial input weights W0 are
distributed i.i.d. N(0,1) and the output layer V has i.i.d. zero-mean entries with V2/K variance.
Then E[J (W0)J (W0)T] = ν2Σ(X). We use the square root of this multiclass kernel matrix
(i.e. Σ(X)1/2) to define the information and nuisance spaces for our random initialization result.
The following theorem is a (non-rigorous) simplification of our main result Theorem 6.24 where we
ignore constants and log factors, and state a weaker but simpler generalization bound.
Theorem 2.2 Fix numbers Γ ≥ 1 and α > 0. Consider an i.i.d. training dataset {(xi, yi)}in=1 ∈
Rd × RK with unit length input samples and one-hot encoded labels. Consider the neural net in (1.1)
parameterized by W and initialized with Wo 吗 N(0,1) entries. Set V with i.i.d. Rademacher
entries (properly scaled). Define the information I and nuisance N spaces with respect to Σ(X)1/2
with spectrum cutoff α∕nK per Definition 1.1. Furthermore, assume
k > r4logn.
α8
Then after T J Γ∕α2 gradient iterations of (1.5), with high probability, training loss obeys
IIf(WT) - y∣'2 < IlnN(y)∣∣'2 + e-r√n,.
Furthermore, the classification error obeys
ErrD(WT) < -N]"'2 + e-r + -√=.
n	αn
(2.2)
(2.3)
This theorem shows that even networks of moderate width can achieve a small generalization error if
(1) the data has low-dimensional representation i.e. the kernel is approximately low-rank and (2) the
inputs and labels are semantically-linked i.e. the label vector y mostly lies on the information space.
4
Under review as a conference paper at ICLR 2020
• Generalization bound: The generalization error has two core components: bias and variance. The
bias component ∣∣∏n(y)∣∣'2∕√n + e-r arises from the training loss and corresponds to the portion
of the labels that falls over the nuisance space. The variance component Γ∕α√n corresponds to the
Rademacher complexity of the model space which connects to the distance ∣∣ WT - Wo IlF.
If y is aligned with the information space, the bias term ΠN (y) will be small. Additionally, if the
kernel matrix is low-rank, we can pick a large α to ensure small variance as well as small network
width. In particular with a constant α the required network width is logarithmic in n.
We note however that our results continue to apply even when the kernel is not approximately
low-rank. In particular, consider the extreme case where we select α√nK = ∖∕λ ：= ʌ/ʌmin (∑(X)).
This sets I = RKn and IlnN(y)∣∣'2 = 0. For this case, the more general Theorem 6.24 yields
ErrD(Wt) W ^~^√yτ∑-1 (X)y while requiring a width of k > K n Jogn.	(2.4)
n	λ4
We note that in this special case our results improve upon the required width in recent literature Arora
et al. (2019)2 that focuses on K = 1 and a conclusion of the form (2.4). However, as we demonstrate
in our numerical experiments in practice λ is very small or even zero (e.g. see the toy model in
Section 2.3) so that requirements of the form (2.4) may require unrealistically (or even infinitely)
wide networks. In contrast, our results apply to all Jacobian spectrums, however can further harness
the low-rank structure of the Jacobian to give even stronger bounds.
•	Small width is sufficient for generalization: Based on our simulations the M-NTK indeed has
low-rank structure with a few large eigenvalues and many smaller ones. As a result a reasonable
scaling choice of α is constant. In that case our result states that as soon as the number of hidden
nodes are logarithmic in n, good generalization can be achieved. This favorably compares to related
works Jacot et al. (2018); Arora et al. (2019); Du et al. (2018b); Allen-Zhu et al. (2018b); Cao & Gu
(2019) where network size is required to grow polynomial with n and inversely with the distance
between the inputs or other notions of margin.
•	Network Size-Bias tradeoff: Based on the requirement (2.2) if the network is large (in terms
of # of hidden units k), we can choose a small cut-off α. This in turn allows us to enlargen the
information space and reduce the training bias further. In summary, as the network capacity grows,
we can gradually interpolate finer detail and reduce bias.
•	Fast convergence: Note that the number of gradient iterations is upper bounded by Γ∕α2 . Hence,
the training speed is dictated by and is inversely proportional to the the smallest singular value over
the information space. Specifically, picking α to be a constant, convergence on the information space
will be fast requiring only a constant number of iterations to reach any fixed accuracy (see (2.3)).
2.2	Generalization guarantees with arbitrary initialization
Our next result provides generalization guarantees from an arbitrary initialization which applies to
pre-trained networks (e.g. those that arise in transfer learning applications) as well as intermediate
gradient iterates as the weights evolve. This result has a similar flavor to Theorem 2.2 with the key
difference that the information and nuisance spaces are defined with respect to any arbitrary initial
Jacobian. This shows that if a pre-trained model3 provides a better low-rank representation of the
data in terms of its Jacobian, it is more likely to generalize well. Furthermore, given its deterministic
nature the theorem can be applied at any iteration, implying that if the Jacobians of any of the iterates
provides a better low-rank representation of the data then one can provide sharper generalization
guarantees. The following theorem is a (non-rigorous) simplification of Theorem 6.21.
Theorem 2.3 LetΓ ≥ 1, α be arbitrary scalars. Consider i.i.d. training data {(xi, yi)}in=1 ∈ Rd × RK
with unit length inputs and one-hot encoded labels. Also consider a neural net with k hidden nodes
as in (1.1) parameterized by W. Let W0 be an arbitrary initial weight matrix and assume the
output matrix has bounded entries obeying IlV ∣)∞ ≤ √=. Define the nuisance space N associated
with J(Wo) based on spectrum cutoff a√n. Set the initial residual r° = f (Wo) - y ∈ RnK and
assume IlrOIl'2 W √n. Suppose k > Γ4∕α8. After T J Γ∕α2 iterations (1.5) with constant learning
rate,traininglossobeys: If(WT) - y∣K W IlnN(r0)∣∣'2 + e-r√n.
2 Based on our understanding Arora et al. (2019) requires the number of hidden units to be on the order of
k > n8/λ6. Hence our result reduces the dependence on width by a factor of at least n4/λ2.
3 e.g. obtained by training with data in a related problem as is common in transfer learning.
5
Under review as a conference paper at ICLR 2020
Also with high probability, classification error obeys: ErrD (WT) W ' πN√√n"'2 + e-r + OTn.
As with the random initialization result, this theorem shows that as long as the initial residual is
sufficiently correlated with the information space, then high accuracy can be achieved for neural
networks with reasonable size. As with its randomized counterpart this result also allows us to study
various tradeoffs between bias-variance and network size-bias. Crucially however this result does not
rely on random initialization. The reason this is particularly important is two fold. First, in many
scenarios neural networks are not initialized at random. For instance, in transfer learning the network
is pre-trained via data from a different domain. Second, as we demonstrate in Section 4 as the iterates
progress the Jacobian mapping develops more favorable properties with the labels/initial residuals
becoming more correlated with the information space of the Jacobian. As mentioned earlier, due
to its deterministic nature the theorem above applies in both of these scenarios. In particular, if a
pre-trained model provides a better low-rank representation of the data via its Jacobian, it is more
likely to generalize well. Furthermore, given its deterministic nature the theorem can be applied at
any iteration by setting W0 = Wτ , implying that if the Jacobians of any of the iterates provides a
better low-rank representation then one can provide better generalization guarantees. Our numerical
experiments demonstrate that the Jacobian of the neural network adapts to the dataset over time with
a more substantial amount of the labels lying on the information space. While we defer the rigorous
theory of this adaptation to future, Section D provides a proof sketch of evolution of Jacobian rank for
a simple dataset model. Such a result when combined with our result above can potentially provide
significantly tighter bounds. This is particularly important in light of recent literature Chizat & Bach
(2018b); Ghorbani et al. (2019c); Yehudai & Shamir (2019) suggesting a significant generalization
gap between kernel methods/linearized neural nets when compared with neural nets operating beyond
the linearized regime (e.g. mean field regime). As a result we view our deterministic result as a first
step towards moving beyond the NTK regime.
2.3	Case Study: Gaussian mixture model
To illustrate a concrete example, we consider a distribution based on multiclass mixture models.
Definition 2.4 (Gaussian mixture model) Consider a size n dataset {(xi, yi)}in=1 ∈ Rd × RK. We
assume this dataset consists of K classes each comprising of C clusters with a total of KC clusters.
We index each cluster with (`, `) denoting the `th cluster from the `th class. We assume the dataset
In cluster (',') is centered around a cluster center μtι ∈ Rd with unit Euclidian norm. We assume
the dataset is generated i.i.d. with the cluster membership assigned uniformly of the clusters with
probability KC and the input samples associated with the cluster (', £) are generated i.i.d. according
to N (μe 历 σ2Id∕d) with the corresponding label set to the one-hot encoding of the class ' i.e. e`.
Note that the cluster indexed by (',') contains 遥彳 data points satisfying E[吗']=近=n/KC.
This distribution is an ideal candidate to demonstrate why the Jacobian of the network exhibits
low-rank or bimodal structure. Let us consider the extreme case σ = 0 where we have a discrete input
distribution over the cluster centers. In this scenario, the multi-class Jacobian matrix is at most rank
K2C = # of output nodes × # of distinct inputs.
as there are (i) only KC distinct input vectors and (ii) K output nodes. We can thus set the information
space to be the top K2C eigenvectors of the multiclass kernel matrix Σ(X). As formalized in the
appendix, it can be shown that
•	The singular values of the information space grow proportionally with n/KC .
•	The concatenated label vector y perfectly lies on the information space.
In Figure 3 we numerically verify that the approximate rank and singular values of the Jacobian
indeed scale as above even when σ > 0. The following informal theorem leverages these observations
to establish a generalization bound for this mixture model. This informal statement is for exposition
purposes. See Theorem A.3 in Appendix A for a more detailed result capturing the exact dependencies
(e.g. Z, B, log n). In this theorem We use > to denote inequality UP to Constant/logarithmic factors.
Theorem 2.5 (Generalization for Gaussian Mixture Models-simplified) Consider a data set of
size n consisting of input/label pairs {(xi, yi)}in=1 ∈ Rd × RK generated according to Def. 2.4 with
the standard deviation obeying Q W K. Let M = [μ1,1 ... μκ,c]t be the matrix obtained by
aggregating all the cluster centers and let g Z N(0, Id). Also let Σ(M) ∈ RKCxKC be the M-NTK
6
Under review as a conference paper at ICLR 2020
associated with the cluster centers M per Def. 2.1. Furthermore, set λM = λmin(Σ(M)), and
Γ4K8C4	2ΓK2 C
assume λ. > 0. Ifthe number of hidden nodes obeys k > —KK——.after T = XM gradient
iterations, with high probability, the model obeys ErrD (WT) W Γ JKC.
We note that λM captures how diverse the cluster
centers are. In this sense λM > 0 intuitively means
that neural network, specifically the neural tangent
kernel, is sufficiently expressive to interpolate the
cluster centers. In fact when the cluster centers are
in generic position λM scales like a constant Oy-
mak & Soltanolkotabi (2019). This theorem focuses
on the regime where the noise level σ is small. In
this case one can achieve good generalization as
soon as the sample size scales as n > K2C which
is the effective rank of the M-NTK matrix. This
result follows from our main result with random ini-
tialization by setting the cutoff at a2 Z KC. This
demonstrates that in this model α does indeed scale
as a constant. Finally, the required network width
is independent of n and only depends on K and C
specifically k > K8C4. This compares favorably
with Arora et al. (2019) which concerns the K = 1
case. In particular, Arora et al. (2019) requires
k > n8∕λX which depends on n (in lieu of K and
C) and the minimum eigenvalue λX of the NTK
matrix Σ(X) (rather than λM). Furthermore, as
σ → 0, Σ(X) becomes rank deficient and λX → 0
so that Arora et al. (2019) requires infinite width.
3	Prior Art
Singular value index (x-axis marks K2C)
Figure 3: The singular values of the normalized
Jacobian spectrum KJCJ- J(Wo) of a neural
network with K = 3. Here, the data is gener-
ated according to the Def. 2.4 with K classes
and σ = 0.1. The cluster centers are picked so
that the distance between any two is at least
0.5. We consider two cases: n = 30C (solid
line) and n = 60C (dashed line). These plots
demonstrate that the top KC singular values
grow proportional to √n.
Neural networks have impressive generalization abilities even when they are trained with more
parameters than the data Zhang et al. (2016). Thus, optimization/generalization properties of neural
nets have been the topic of recent literature Zhang et al. (2016). Below we discuss the works on
statistical learning, optimization, and implicit bias.
Statistical learning theory: Statistical properties of neural networks have been studied since 1990’s
Anthony & Bartlett (2009); Bartlett et al. (1999); Bartlett (1998). With the success of deep networks,
there is a renewed interest in understanding capacity of the neural networks under different norm
constraints or network architectures Dziugaite & Roy (2017); Arora et al. (2018); Neyshabur et al.
(2017b); Golowich et al. (2017). Bartlett et al. (2017); Neyshabur et al. (2017a) established tight
sample complexity results for deep networks based on spectral norms. See also Nagarajan & Kolter
(2019) for improvements via leveraging various properties of the inter-layer Jacobian and Long
& Sedghi (2019) for results with convolutional networks. Related, Arora et al. (2018) leverages
compression techniques for constructing tighter bounds. Yin et al. (2018) jointly studies statistical
learning and adversarial robustness. These interesting results, provide generalization guarantees for
the optimal solution to the empirical risk minimizer.
Properties of gradient descent: There is a growing understanding that solutions found by first-order
methods such as gradient descent have often favorable properties. Generalization properties of
stochastic gradient descent is extensively studied empirically Keskar et al. (2016); Hardt et al. (2015);
Sagun et al. (2017); Chaudhari et al. (2016); Hoffer et al. (2017); Goel & Klivans (2017); Goel
et al. (2018). For linearly separable datasets, Soudry et al. (2018); Gunasekar et al. (2018); Brutzkus
et al. (2017); Ji & Telgarsky (2018a;b) show that first-order methods find solutions that generalize
well without an explicit regularization for logistic regression. An interesting line of work establish
connection between kernel methods and neural networks and study the generalization abilities of
kernel methods when the model interpolates the training data Dou & Liang (2019); Belkin et al.
(2018a;b; 2019); Liang & Rakhlin (2018); Belkin et al. (2018c). Chizat & Bach (2018a); Song et al.
(2018); Mei et al. (2018); Sirignano & Spiliopoulos (2018); Rotskoff & Vanden-Eijnden (2018) relate
the distribution of the network weights to Wasserstein gradient flows using mean field analysis.
7
Under review as a conference paper at ICLR 2020
Global convergence and generalization of neural nets: Closest to our work, recent literature Cao
& Gu (2019); Arora et al. (2019); Ma et al. (2019); Allen-Zhu et al. (2018a) provides generalization
bounds for overparameterized networks trained via gradient descent. Also see Li et al. (2018); Huang
et al. (2019) for interesting visualization of the optimization and generalization landscape. Jacot
et al. (2018) introduced NTK and observed that principal directions of NTK is optimized faster than
smaller eigendirections for infinitely wide networks. In connection to this, our Def 1.1 helps quantify
the low-rankness and bimodality of the Jacobian spectrum (same as NTK for random initialization).
Similar to Thm 2.2, Arora et al. (2019) uses the NTK to provide generalization guarantees in a similar
framework to Jacot et al. (2018) (see (2.4) for comparison). Li et al. (2019a) leverages low-rank
Jacobian structure to establish robustness to label noise. Very recent work Su & Yang (2019) uses
low-rankness to better capture approximation power of neural nets. These works build on global
convergence results of randomly initialized networks Du et al. (2018b;a); Allen-Zhu et al. (2018b);
Chizat & Bach (2018b); Zhang et al. (2019); Nitanda & Suzuki (2019); Oymak & Soltanolkotabi
(2018); Zou et al. (2018) which study the gradient descent trajectory via comparisons to a NTK
linearization. These results however typically require unrealistically wide networks for optimization
where the width grows poly in n and poly-inversely proportional to the distance between the input
samples. Example distance measures are class margin for logistic loss and minimum eigenvalue of
the NTK matrix for least-squares. Our work circumvents this issue by allowing a capacity-dependent
interpolation. We prove that even small networks (e.g. of constant width) can interpolate the data
over a low-dimensional information space without making restrictive assumptions on the input. This
approach also leads to faster convergence rates. In terms of generalization, our work has three
distinguishing features: (a) bias-variance tradeoffs by identifying information/nuisance spaces, (b)
no margin/distance/minimum eigenvalue assumptions on data, (c) the bounds apply to multiclass
classification as well as pre-trained networks (Theorem 2.3).
Finally, low-rankness of the Jacobian plays a central role in this work. Hessian and Jacobian of neural
nets are investigated by multiple papers which contain related findings on the bimodal (approximately
low-rank) spectrum Papyan (2018); Ghorbani et al. (2019b); Papyan (2019b); Sagun et al. (2017); Li
et al. (2019b); Javadi et al. (2019). Our key empirical contribution is establishing (in great detail) that
multiclass Jacobian adapts over time to align its information space with the labels to better represent
the data. This alignment leads to tighter generalization bounds in our analysis shedding light on
representation learning and gradient dynamics beyond NTK.
4	Numerical experiments
We present experiments demonstrating our theoretical
findings on two popular image classification datasets.
In this section we focus on a set of CIFAR-10 exper-
iments and discuss how our theory is strongly sup-
ported by what we observe in practice. To provide
more detail and show that our theory holds across
different datasets, in addition to the experiments dis-
cussed in this section we perform additional experi-
ments on a modified 3-class version of CIFAR-10 and
MNIST in Appendix C.	Top 1000 singular values
Experimental setup. The CIFAR-10 dataset consists Figure 4: Histogram of the top 1000 Jacobian
of 50k training images and 10k test images in 10 singular values on the CIFAR10 dataset.
classes. We demonstrate our results on ResNet20, a
state-of-the-art architecture with a fairly low test error on this dataset (8.75% test error reported)
and relatively few parameters (0.27M). In all of our experiments we set the information space
to be the span of the top 50 singular vectors (out of total dimension of Kn ≈ 500000) of the
neural network Jacobian. In order to be consistent with our theoretical formulation we make the
following modifications to the default ResNet20 architecture: (1) we scale the output of the final
fully connected layer to ensure that the output is small, consistent with Theorem 2.2 (2) we turn off
batch normalization and (3) we do not pass the network output through a soft-max function. We
train the network with SGD on least-squares loss with batch size 128 and without any form of data
augmentation. We set the initial learning rate to 0.1 and adjust the learning rate schedule and number
of epochs depending on the particular experiment so as to achieve a good fit to the training data
quickly. The figures in this section depict the minimum error over a window consisting of the last
10 epochs for visual clarity. We also conduct two sets of experiments to illustrate the results on
8
Under review as a conference paper at ICLR 2020
	InI (y)l'2 怙必	InN (y)必 怙必	InI (r0)lg2 什。必	InN (r0)必 什0必
τtrain- Jinit	0.38081	0.92465	0.37114	0.92858
Ttrain- Jfinal	0.9869	016131	0.98669	0.1626
Table 1: Depiction of the alignment of the initial residual with the information/nuisance
space using uncorrupted data and a Multi-class ResNet20 model trained with SGD.
ygrene laudiseR
12
-0 -
1
ygrene laudiseR
rorrE
0	100	200	300	400	0	100	200	300	400	0	100	200	300	400
Epochs	Epochs	Epochs
(a) Final train Jacobian.	(b) Final test Jacobian.	(c) Training and test error.
Figure 5: Evolution of the residual (rτ = f(Wτ ) - y) along the information/nuisance
spaces of the final Jacobian on (a) the training data and (b) the test data and c)
misclassification error on training and test. This experiment uses uncorrupted labels.
uncorrupted and corrupted data. In this section we highlight some of these results and relate them to
our theoretical framework. For the complete set of experiments we refer the reader to Appendix C.
Jacobian eigenstructure. Calculating the exact full singular value decomposition of the Jacobian at
this scale (500k × 270k) is not tractable due to computation/memory limitations. In order to verify
the bimodal structure of the Jacobian with exact singular values we plot the histogram of the top
1000 singular values of the Jacobian mapping at initalization and after training in Figure 4. This
figure clearly demonstrates that the Jacobian has low-rank structure. In both cases we observe that
singular values are concentrated around zero with a relatively small density distributed over higher
singular values. This observation serves as a natural basis for decomposition of the label space into
information I (large singular values, low-dimensional) and nuisance space N (small singular values,
high-dimensional). We note that while calculating all the eigenvalues is not possible, we verify the
bimodal structure of the entire Jacobian spectrum by approximating its spectral density in App. C.
Experiments without label corruption. First, we present experiments on the original training data
described above with no label corruption. We train the network for 400 epochs to achieve a good
fit to the training data. Our theory predicts that the sum of IIJI y'? and IlnN (g)ll`2 determines the
classification error (Theorems 2.2 and 6.24). Table 1 collects these values for the initial and final
Jacobian. These values demonstrate that the label vector is indeed correlated with the top eigenvectors
of both the initial and final Jacobians. An interesting aspect of these results is that this correlation
increases from the initial to the final Jacobian so that more of the label energy lies on the information
space of the final Jacobian in comparison with the initial Jacobian. Stated differently, we observe a
significant adaptation of the Jacobian to the labels after training compared to the initial Jacobian so
that our predictions become more and more accurate as the iterates progress. In particular, the first
column of Table 1 shows that the fraction of label energy lying on the information subspace of the
Jacobian drastically increases after training (from 0.38 to 0.99). Consequentially, less energy falls
on the nuisance space (decreases from 0.92 to 0.16 after training), while IlJIy 八七 remains relatively
small resulting in better generalization. Towards explaining these, Section D provides a preliminary
analysis showing Jacobian spectrum indeed adapts to data.
We also track the projection of the residual rτ on the information and nuisance subspaces throughout
training on both training and test data and depict the results in Figures 5a and 5b. In agreement with
our theory, these plots show that learning on I is fast and the residual energy decreases rapidly on
this space. On the other hand, residual energy on N goes down rather slowly and the decrease in
total residual energy is overwhelmingly governed by I, suggesting that most information relevant to
learning lies in this space. We also plot the training and test error in Figure 5c. We observe that as
learning progresses, the residual on both spaces decrease in tandem with training and test error.
9
Under review as a conference paper at ICLR 2020
ygrene laudiseR
ygrene laudiseR
1
-10
rorrE
0.8

0	200	400	600	800	0	200	400	600	800	0	200	400	600	800
Epochs	Epochs	Epochs
(a) 50 epochs train Jacobian. (b) 50 epochs test Jacobian.	(c) Training and test error
Figure 6: Evolution of the residual (rτ = f(Wτ ) - y) along the information/nuisance
spaces of the Jacobian at 50 epochs on (a) training data and (b) test data and (c)
misclassification error on training and test data. 50% of the labels have been corrupted.
	InI (y)必 回`2	InN (y 儿 2 回`2	InI N)必 什。I'2	InN (r0 儿 2 什0 1'2
τtrain- Jinit	0.32762	0.94481	0.32152	0.9469
Ttrain- Jfinal	0.8956	044487	0.89597	0.44412
Table 2: Depiction of the alignment of the initial residual with the information/nuisance
space using 50% label corrupted data and a Multi-class ResNet20 trained with SGD.
Experiments with label corruption. Our next experiments study the effect of
corruption. Specifically, we corrupt 50% of the labels by randomly picking a
label from a (strictly) different class. We train the network for 800 epochs
and divide the learning rate by 10 at epoch 760 to fit to the training data.
We again track the projection of the residual rτ on
the information/nuisance spaces throughout training on
both training and test data and depict the results in Figs	0.8
6a and 6b. We also track the train and test errors in
Figure 6c. From Figure 6c it is evident that while the
training error steadily decreases, test error exhibits a
very different behavior compared to the uncorrupted
experiment. In the first phase, test error drops rapidly
as the network learns from information contained in
the uncorrupted data, accompanied by a corresponding
decrease in residual energy on the information subspace
on the training data (Figure 6a). The lowest test error
is observed at epoch 50 after which a steady increase
follows. In the second phase, the network overfits to
the corrupted data resulting in more test error on the
uncorrupted test data (Figure 6b). More importantly,
the increase of the test error is due to the nuisance space
0.6
0.4
0.2
0	0.25	0.5	0.75	1
Label corruption
Figure 7: Fraction of the energy of the label
vector that lies on the nuisance space of the
initial Jacobian and final Jacobian as well
as the test error as a function of the amount
of label corruption.
as the error over information space is stable while it
increases over the nuisance. In particular the residual on N slowly increases while residual on I drops
sharply creating a dip in both test error and total residual energy around epoch 50. This phenomenon
is further explained in the appendix (see Sec. 5.1) via a linear model.
In Table 2 we again depict the fraction of the energy of the labels and the initial residual that lies on
the information/nuisance spaces. The Jacobian continues to adapt to the labels/initial residual even in
the presence of label corruption, albeit to a smaller degree. We note that due to corruption, labels
are less correlated with the information space of the Jacobian and the fraction of the energy on the
nuisance space is higher which results in worse generalization (as predicted by our theory).
To demonstrate the connection between generalization and information/nuisance spaces, we repeat
the experiment with 25%, 75% and 100% label corruption and depict the results after 800 epochs in
Fig. 7. As expected, the test error increases with the corruption. Furthermore, the corrupted labels
become less correlated with the information space with more of the label energy falling onto the
nuisance space. This is consistent with our theory which predicts worse generalization in this case.
10
Under review as a conference paper at ICLR 2020
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018b.
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge
university press, 2009.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019.
Peter Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. 06 2017. URL https://arxiv.org/pdf/1706.08498.
Peter L Bartlett. The sample complexity of pattern classification with neural networks: the size of the
weights is more important than the size of the network. IEEE transactions on Information Theory,
44(2):525-536,1998.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Peter L Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear vc dimension bounds for piecewise
polynomial networks. In Advances in Neural Information Processing Systems, pp. 190-196, 1999.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning
and the bias-variance trade-off. arXiv preprint arXiv:1812.11118, 2018a.
Mikhail Belkin, Daniel Hsu, and Partha Mitra. Overfitting or perfect fitting? risk bounds for
classification and regression rules that interpolate. 06 2018b. URL https://arxiv.org/
pdf/1806.05161.
Mikhail Belkin, Alexander Rakhlin, and Alexandre B. Tsybakov. Does data interpolation contradict
statistical optimality? 06 2018c. URL https://arxiv.org/pdf/1806.09471.
Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. arXiv
preprint arXiv:1903.07571, 2019.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. arXiv preprint
arXiv:1710.10174, 2017.
Yuan Cao and Quanquan Gu. A generalization theory of gradient descent for learning over-
parameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs,
Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent
into wide valleys. arXiv preprint arXiv:1611.01838, 2016.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. arXiv preprint arXiv:1805.09545, 2018a.
Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming.
arXiv preprint arXiv:1812.07956, 2018b.
Xialiang Dou and Tengyuan Liang. Training neural networks as learning data-adaptive kernels:
Provable representation and approximation benefits. arXiv preprint arXiv:1901.07114, 2019.
11
Under review as a conference paper at ICLR 2020
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018b.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization
via hessian eigenvalue density. Proceedings of the 36th International Conference on Machine
Learning, 2019a.
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization
via hessian eigenvalue density. arXiv preprint arXiv:1901.10159, 2019b.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers
neural networks in high dimension. arXiv preprint arXiv:1904.12191, 2019c.
Surbhi Goel and Adam Klivans. Learning neural networks with two nonlinear layers in polynomial
time. arXiv preprint arXiv:1709.06010, 2017.
Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping
patches. arXiv preprint arXiv:1802.02547, 2018.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. arXiv preprint arXiv:1712.06541, 2017.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent
on linear convolutional networks. In Advances in Neural Information Processing Systems, pp.
9461-9471,2018.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gap in large batch training of neural networks. In Advances in Neural Information Processing
Systems, pp. 1731-1741, 2017.
W. Ronny Huang, Zeyad Emam, Micah Goldblum, Liam Fowl, Justin K. Terry, Furong Huang, and
Tom Goldstein. Understanding generalization through visualizations. 2019.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
Hamid Javadi, Randall Balestriero, and Richard Baraniuk. A hessian based complexity measure for
deep networks. arXiv preprint arXiv:1905.11639, 2019.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv
preprint arXiv:1810.02032, 2018a.
Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint
arXiv:1803.07300, 2018b.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Cornelius Lanczos. An iteration method for the solution of the eigenvalue problem of linear differen-
tial and integral operators. Journal ofResearch of the National Bureau of Standards 45: 255-282,
1950.
12
Under review as a conference paper at ICLR 2020
M. Ledoux. The concentration of measure phenomenon. volume 89 of Mathematical Surveys and
Monographs. American Matheamtical Society, Providence, RI, 2001.
R. B. Lehoucq, D. C. Sorensen, and C. Yang. Arpack users guide: Solution of large scale eigenvalue
problems by implicitly restarted arnoldi methods. SIAM, Philadelphia, PA, 1998, 1998.
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape
of neural nets. In Advances in Neural Information Processing Systems, pp. 6389-6399, 2018.
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is prov-
ably robust to label noise for overparameterized neural networks. arXiv preprint arXiv:1903.11680,
2019a.
Xinyan Li, Qilong Gu, Yingxue Zhou, Tiancong Chen, and Arindam Banerjee. Hessian based analysis
of sgd for deep nets: Dynamics and generalization. arXiv preprint arXiv:1907.10732, 2019b.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. NeurIPS, 2018.
Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel "ridgeless" regression can generalize.
08 2018. URL https://arxiv.org/pdf/1808.00387.
Philip M Long and Hanie Sedghi. Size-free generalization bounds for convolutional neural networks.
arXiv preprint arXiv:1905.12600, 2019.
Chao Ma, Lei Wu, et al. A comparative analysis of the optimization and generalization property
of two-layer neural network and random feature models under gradient descent dynamics. arXiv
preprint arXiv:1904.04326, 2019.
Andreas Maurer. A vector-contraction inequality for rademacher complexities. In International
Conference on Algorithmic Learning Theory, pp. 3-17. Springer, 2016.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of
two-layers neural networks. arXiv preprint arXiv:1804.06561, 2018.
Vaishnavh Nagarajan and J Zico Kolter. Deterministic pac-bayesian generalization bounds for deep
networks via generalizing noise-resilience. arXiv preprint arXiv:1905.13344, 2019.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-
bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint
arXiv:1707.09564, 2017a.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947-5956,
2017b.
Atsushi Nitanda and Taiji Suzuki. Refined generalization analysis of gradient descent for over-
parameterized two-layer neural networks with smooth activations on classification problems. arXiv
preprint arXiv:1905.09870, 2019.
Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent
takes the shortest path? 12 2018. URL https://arxiv.org/pdf/1812.10004.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global con-
vergence guarantees for training shallow neural networks. arXiv preprint arXiv:1902.04674,
2019.
Vardan Papyan. The full spectrum of deep net hessians at scale: Dynamics with sample size. arXiv
preprint arXiv:1811.07062, 2018.
Vardan Papyan. The full spectrum of deepnet hessians at scale: Dynamics with sgd training and
sample size. arXiv preprint arXiv:1811.07062v2, 2019a.
Vardan Papyan. Measuring the spectrum of deepnet hessians. 2019b.
13
Under review as a conference paper at ICLR 2020
Grant M. Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems:
Asymptotic convexity of the loss landscape and universal scaling of the approximation error. 05
2018. URL https://arxiv.org/pdf/1805.00915.
Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the
hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.
Bernhard A Schmitt. Perturbation bounds for matrix square roots and pythagorean sums. Linear
algebra and its applications,174:215-227,1992.
J. Schur. BemerkUngen ZUr theorie der beschrankten bilinearformen mit Unendlich Vielen veran-
derlichen. Journal fur die reine und angewandte Mathematik, 140:1-28, 1911. URL http:
//eudml.org/doc/149352.
JUstin Sirignano and Konstantinos SpiliopoUlos. Mean field analysis of neUral networks: A central
limit theorem. 08 2018. URL https://arxiv.org/pdf/1808.09372.
Mei Song, A Montanari, and P NgUyen. A mean field View of the landscape of two-layers neUral
networks. In Proceedings of the National Academy of Sciences, VolUme 115, pp. E7665-E7671,
2018.
Daniel SoUdry, Elad Hoffer, Mor Shpigel Nacson, SUriya GUnasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822-2878, 2018.
Lili SU and PengkUn Yang. On learning oVer-parameterized neUral networks: A fUnctional approxi-
mation prospectiVe. arXiv preprint arXiv:1905.10826, 2019.
Gilad YehUdai and Ohad Shamir. On the power and limitations of random featUres for Understanding
neUral networks. arXiv preprint arXiv:1904.00687, 2019.
Dong Yin, Kannan Ramchandran, and Peter Bartlett. Rademacher complexity for adVersarially robUst
generalization. arXiv preprint arXiv:1810.11914, 2018.
Yi YU, Tengyao Wang, and Richard J Samworth. A UsefUl Variant of the daVis-kahan theorem for
statisticians. Biometrika, 102(2):315-323, 2014.
ChiyUan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning reqUires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
HUishUai Zhang, Da YU, Wei Chen, and Tie-Yan LiU. Training oVer-parameterized deep resnet is
almost as easy as training a two-layer network. arXiv preprint arXiv:1903.07120, 2019.
Difan ZoU, YUan Cao, DongrUo ZhoU, and QUanqUan GU. Stochastic gradient descent optimizes
oVer-parameterized deep relU networks. arXiv preprint arXiv:1811.08888, 2018.
14
Under review as a conference paper at ICLR 2020
54535
...
432
,IO.I,I ①+jsop
3210
0	50	100	150	200	0	50	100	150	200
Iterations (τ)	Iterations (τ)
(a) Total test error	(b) Test error along information and nuisance spaces
Figure 8: Plots of the (a) total test error and (b) the test error components for the
model in Section 5.1. The test error decreases rapidly over the information subspace
but slowly increases over the nuisance subspace.
5	Technical approach and General Theory
5.1	Prelude: fitting a linear model
To gain better insights into what governs the generalization capability of gradient based iterations let
us consider the simple problem of fitting a linear model via gradient descent. This model maps an
input/feature vector X ∈ Rd into a one-dimensional output/label via X ↦ f (x, W) ：= wτx. We wish
to fit a model of this form to n training data consisting of input/label pairs {(xi, yi)}in=1 ∈ Rd × R.
Aggregating this training data as rows of a feature matrix X ∈ Rn×d and label vector y ∈ Rn, the
training problem takes the form
L(W) = 2IXW - y%.	(5.1)
We focus on an overparameterized model where there are fewer training data than the number of
parameters i.e. n ≤ d. We assume the feature matrix can be decomposed into the form X = X + Z
where X is low-rank (i.e. rank(X) = r << n) with singular value decomposition X = UΣ VT with
U ∈ Rn×r, Σ ∈ Rr×r, V ∈ Rd×r, and Z ∈ Rn×d is a matrix with i.i.d. N(0, σX/n) entries. We shall
also assume the labels are equal to y = y + Z with y = Xw* for some w* ∈ Range (V) and Z ∈ Rn a
Gaussian random vector with i.i.d. N(0, σj/n) entries. One can think of this as a linear regression
model where the features and labels are corrupted with Gaussian noise. The goal of course is to learn
a model which fits to the clean uncorrupted data and not the corruption. In this case the population
loss (i.e. test error) takes the form
E[L(W)] = 2IIXW- yll22 + 2 σ2 HI1+1 σ2,
Now let us consider gradient descent iterations with a step size η which take the form
Wτ+1 = Wτ - ηVL(Wτ) = (I - ηXTX) WT + ηXTy.	(5.2)
To gain further insight into the generalization capabilities of the gradient descent iterations we shall
consider an instance of this problem where the subspaces U and V are chosen uniformly at random,
Σ = Ir with n = 200, d = 500, r = 5, and σx = 0.2, σy = 2. In Figure 8a we plot the population loss
evaluated at different iterations. We observe an interesting phenomenon, in the first few iterations
the test error goes down quickly but it then slowly increases. To better understand this behavior we
decompose the population loss into two parts by tracking the projection of the misfit XW - y on the
column space of the uncorrupted portion of the input data (U) and its complement. That is,
E L(W) = E LI(W) + E LN (W).
15
Under review as a conference paper at ICLR 2020
where
E Li(W) = EunI (Xw- y)(22 ] = UXw-砒 22 + 2r-σ2 HI1 + 2-σ2,
2	2n	2n
E LN (w) ：= E [∣∣∏N (Xw - y)% ] = 2(1 - nr )(σ2 ∣H22 + σy),
with ΠI = UUT and ΠN = I - UUT. In Figure 8b we plot these two components. This plot clearly
shows that ELI(w) goes down quickly while ELN(w) slowly increases with their sum creating
the dip in the test error. Since U is a basis for the range of the uncorrupted portion of the features
(X) one can think of span (U) as the “information" subspace and E LI (w) as the test error on this
information subspace. Similarly, one can think of the complement of this subspace as the “nuisance"
subspace and ELN(w) as the test error on this nuisance subspace. Therefore, one can interpret
Figure 8a as the test error decreasing rapidly in the first few iterations over the information subspace
but slowly increasing due to the contributions of the nuisance subspace.
To help demystify this behavior note that using the gradient descent updates from (5.2) the update in
terms of the misfit/residual rτ = Xwτ - y takes the form
rτ+1
(I - ηXXT)rτ =(I - ηXXT)(Xwτ - y)+ noise
Based on the form of this update when the information subspace is closely aligned with the prominent
singular vectors of X the test error on the information subspace (ELI(w) ≈ IlXwT -5122) quickly
decreases in the first few iterations. However, the further we iterate the parts of the residual aligned
with the less prominent eigen-directions of X (which correspond to the nuisance subspace) slowly
pick up more energy contributing to a larger total test error. In this section, we outline our approach to
proving robustness of over-parameterized neural networks. Towards this goal, we consider a general
formulation where we aim to fit a general nonlinear model of the form x ↦ f(x; θ) with x ∈ Rd
denoting the input features, θ ∈ Rp denoting the parameters, and f(x; θ) ∈ RK the K outputs of the
model denoted by f1(x; θ), f2(x; θ), . . . , fK (x; θ). For instance in the case of neural networks θ
represents its weights. Given a data set ofn input/label pairs {(xi, yi)}in=1 ⊂ Rd × RK, we fit to this
data by minimizing a nonlinear least-squares loss of the form
1n
L(θ) = 5 ∑ Wf (Xi； θ) - yi%.
2 i=1
(5.3)
To continue let us first aggregate the predictions and labels into larger vectors based on class. In
particular define
f'(x1;θ)
f`(θ) =	⋮	∈ Rn
LMxn； θ)J
and y(') =	⋮	∈ Rn for ' = 1,2,...,K.
L(yn)'J
Concatenating these vectors we arrive at
f(θ) =
L f" R
LfK (θ)j
y⑴
and y= K eRKn.
(5.4)
Using the latter we can rewrite the optimization problem (5.3) into the more compact form
L(θ) = 2 Wf (θ) - yW22.	(5.5)
To solve this problem we run gradient descent iterations with a learning rate η starting from an initial
point θ0 . These iterations take the form
θτ+1 = θτ - ηVL(θτ) with VL(θ) = JT(θ)(f(θ)- y).	(5.6)
As mentioned earlier due to the form of the gradient the convergence/generalization of gradient
descent naturally depends on the spectral properties of the Jacobian. To capture these spectral
properties we will use a reference Jacobian J (formally defined below) that is close to the Jacobian at
initialization J(θ0).
16
Under review as a conference paper at ICLR 2020
Definition 5.1 (Reference Jacobian and its SVD) Consider an initial point θ0 ∈ Rp and the Jaco-
bian mapping J (θ0) ∈ RKn×p. For ε0, β > 0, we call J ∈ RKn×max(Kn,p) an (ε0 , β) reference
Jacobian matrix if it obeys the following conditions,
JI ≤ β,	IlJ(θo) JT(θo) - JJTIl ≤ ε0, and	IIJ(θ°) - JIl ≤ ε°.
where J(θo) ∈ RKn×maX(Kn,P) is a matrix obtained by augmenting J(θo) with max(0, Kn - P)
zero columns. Furthermore, consider the singular value decomposition of J given by
Kn
J = U diag(λ)V T = ∑ λsusvsT.	(5.7)
s=1
where λ ∈ RKn are the vector of singular values and us ∈ RKn and vs ∈ Rp are the left/right singular
vectors.
One natural choice for this reference Jacobian is J = J(θo). However, We shall also use other
reference Jacobians in our results. We will compare the gradient iterations (5.6) to the iterations
associated with fitting a linearized model around θ0 defined as flin(θ) = f(θ0) + J(θ - θ0), where
θo ∈ RmaX(Kn,P) is obtained from θ° by adding max(Kn - p, 0) zero entries at the end of θo. The
optimization problem for fitting the linearized problem has the form
Llin(θ) = 2 flin(θ)- y∣∣22 .	(5.8)
Thus starting from θo = θo the iterates θτ on the linearized problem take the form
θτ+1= θτ - η^Llin(θτ),	(5.9)
=θτ - nJT(f(θo) + J(θτ - θo) - y),
=θτ - nJT J(ΘT - θo) - nJT (f (θo) - y).
The iterates based on the linearized problem will provide a useful reference to keep track of the evo-
lution of the original iterates (5.6). Specifically we study the evolution of misfit/residuals associated
with the two problems
Original residual: rτ = f(θτ) - y.	(5.10)
Linearized residual: ττ = flin(θτ) - y = (I - nJJT)τr0.	(5.11)
To better understand the dynamics of convergence of the linearized iterates next we define two
subspaces associated with the reference Jacobian and its spectrum.
Definition 5.2 (Information/Nuisance Subspaces) Let J denote the reference Jacobian per Defini-
tion 5.1 with eigenvalue decomposition J = U diag(λ)V T per (5.7). Fora spectrum cutoff α obeying
0 ≤ α ≤ λ1 let r(α) denote the index of the smallest singular value above the threshold α, that is,
r(α) = min ({s ∈ {1, 2, . . . , nK} such that λs ≥ α}) .
We define the information and nuisance subspaces associated with J as I ：= span({us}S=ι) and
N ：= span({us}Kn+ι)∙ We also define the truncated reference Jacobian
T
JI = [u1 u2	. . . ur] diag (λ1, λ2, . . . , λr) [v1 v2	. . . vr]
which is the part of the reference Jacobian that acts on the information subspace I.
We will show rigorously that the information and nuisance subspaces associated with the reference
Jacobian dictate the directions where learning is fast and generalizable versus the directions where
learning is slow and overfitting occurs. Before we make this precise we list two assumptions that will
be utilized in our result.
Assumption 1 (Bounded spectrum) For any θ ∈ RP the Jacobian mapping associated with the
nonlinearity f : RP ↦ Rn has bounded spectrum, i.e. ∖J (θ)∣∣ ≤ β.
17
Under review as a conference paper at ICLR 2020
Assumption 2 (Bounded perturbation) Consider a point θ0 ∈ Rp and positive scalars ε, R > 0.
Assume that for any θ obeying ∣∣ θ - θo1屋 ≤ R,we have
J(θ)- J(θ0 )11 ≤ ε.
With these assumptions in place we are now ready to discuss our meta theorem that demonstrates that
the misfit/residuals associated to the original and linearized iterates do in fact track each other rather
closely.
Theorem 5.3 (Meta Theorem) Consider a nonlinear least squares problem of the form L(θ ) =
1 Wf (θ) - y∖∖222 with f : Rp ↦ RnK the multi-class nonlinear mapping, θ ∈ Rp the parameters of
the model, and y ∈ RnK the concatenated labels as in (5.4). Let θ be zero-padding of θ till size
max(Kn, p). Also, consider a point θ0 ∈ Rp with J an (0, β) reference Jacobian associated with
J(θ0) per Definition 5.1 and fitting the linearized problem flin(θ) = f(θ0) + J(θ - θ0) via the loss
Liin (θ) = 1 Wflin (θ) - y W22. Furthermore, define the information I and nuisance N subspaces and
the truncated Jacobian JI associated with the reference Jacobian J based on a cut-off spectrum
value of α per Definition 5.2. Furthermore, assume the Jacobian mapping J(θ) ∈ RnK×p associated
with f obeys Assumptions 1 and 2 for all θ ∈ Rp obeying
∖∖θ - θo∖∖'2 ≤ R := 2 QJIr0L + Γ W∏N (r0)W'2 + δα Wro|必)，	(5.12)
around a point θ0 ∈ Rp for a tolerance level δ obeying 0 < δ ≤ 1 and stopping time Γ obeying Γ ≥ 1.
Finally, assume the following inequalities hold
ε0 ≤
miη(δɑ, ∖∕δɑ3∕Γβ)
5
and
δα3
ε ≤ 5Γβ2 .
(5.13)
We run gradient descent iterations of the form θτ+1 = θτ - ηVL(θτ) and θτ+1 = θτ - ηYLiin(θτ)
on the original and linearized problems startingfrom θo with step size η obeying η ≤ 1∕β2. Thenfor
Γ
all iterates T obeying 0 ≤ T ≤ T ：= ^02 the iterates ofthe original (θτ) and linearized (θτ) problems
and the corresponding residuals r ：= f (θτ) 一 y and rτ ：= fnn(θτ) - y closely track each other.
That is,
WrT- rτW'2 ≤ 3δαWr0W'2 and 1瓦-GTW^ ≤ δr∣∣r0W'2	(5.14)
5β	α
Furthermore, for all iterates T obeying 0 ≤ T ≤ T ：= n02
wθt - θ0∣∣'2 ≤ 2 = IIJI r0∣∣'2+ α WnN (r0)W'2+δα Wr0W22.	(5.15)
and after T = T iteration we have
WrTW'2 ≤ e-r W∏I(r0)W'2 + W∏N(r0)W'2 + δαWr0W'2.	(5.16)
6 Proofs
Before we proceed with the proof let us briefly discuss some notation used throughout. For a
matrix W ∈ Rk×d we use vect(W) ∈ Rkd to denote a vector obtained by concatenating the rows
T
w1, w2, . . . , wk ∈ Rd of W. That is, vect(W) = [w1T w2T . . . wkT] . Similarly, we use
mat(w) ∈ Rk×d to denote a k × d matrix obtained by reshaping the vector w ∈ Rkd across its rows.
Throughout, for a differentiable function φ ： R ↦ R We use φ' and φ'' to denote the first and second
derivative.
6.1	Proofs for General Theory (Proof of Theorem 5.3)
In this section We prove our result for general nonlinearities. We begin With a feW notations and
definitions and preliminary lemmas in Section 6.1.1. Next in Section 6.1.2 We prove some key
lemmas regarding the evolution of the linearized residuals rGT . In Section 6.3 We establish some key
Rademacher complexity results used in our generalization bounds. Finally, in Section 6.1.3 We use
these results to complete the proof of Theorem 5.3.
18
Under review as a conference paper at ICLR 2020
6.1.1	Preliminary definitions and lemmas
Throughout we use
UI = [u1	u2 . . .	ur]	∈	RnK×r	and	UN	=	[ur+1	ur+2	. . .	unK] ∈	RnK×(nK-r).
to denote the basis matrices for the information and nuisance subspaces from Definition 5.2. Similarly,
we define the information and nuisance spectrum as
λI = [λ1	λ2 . . .	λr]T	and	λN	= [λr+1	λr+2	. . .	λnK]T	.
We also define the diagonal matrices
Λ = diag(λ), ΛI = diag(λI), and ΛN = diag(λN).
Definition 6.1 (early stopping value and distance) Consider Definition 5.2 and let Γ > 0 be a
positive scalar. Associated with the initial residual r0 = f(θ0) - y and the information/nuisance
subspaces of the reference Jacobian J (with a cut-off level α) we define the (α, Γ) early stopping
value as
nK λ2	1/2
(us, r0))2 + r2 Σ 2 ((Us, ro))2)
s=r+1 α
BaJ = (sΣi % (
(6.1)
We also define the early stopping distance as
Dα
Bα,Γ
γ = ɪ
The goal of early stopping value/distance is understanding the behavior of the algorithm at a particular
stopping time that depends on Γ and the spectrum cutoff α. In particular, as we will see later on
the early stopping distance characterizes the distance from initialization at an appropriate early
stopping time. We continue by stating and proving a few simple lemmas. The first Lemma provides
upper/lower bounds on the early stopping value.
Lemma 6.2 (Bounds on Early-Stopping Value) The early stopping value Bα,Γ from Definition 6.1
obeys
Bα,r ≤ (IlnI (r°)a+Γ2∣∣∏N (ro)a)"≤「忻以	(6.2)
α
Bα,Γ ≥ T IInI(r0)∣∣`2 .	(6.3)
Proof To prove the upper bound we use the fact that α ≤ λs for s ≤ r and α ≥ λs for s ≥ r to
conclude that
r	nK	1/2
Bα,Γ ≤ I ∑ ((us, r0))2 + r2 Σ ((Us,咐)]
s=1	s=r+1
=QnI (ro)%+Γ2∣∣∏N (ro 温)1/2
≤「MOL .
To prove the lower bound, We use the facts that ɑ2∕λ22 ≥ a2 /λ2 to conclude that
r α2	nK λ2	1/2
Bα,Γ = I ∑ λ2 ((us, r0))2 + r2 Σ	22 ((Us,rθ})2),
s=1 λs	s=r+1 α
r 2	1/2
≥ (sΣl 京((Us, ri)	,
α
≥ OnI (r0 儿 2.
It is of course well known that the mapping (I - ηAAT ) is a contraction for sufficiently small values
of η. The next lemma shows that if we replace one of the A matrices with a matrix B which is close
to A the resulting matrix (I - ηABT ), while may not be contractive, is not too expansive.
19
Under review as a conference paper at ICLR 2020
Lemma 6.3 (Asymmetric PSD increase) Let A, B ∈ Rn×p be matrices obeying
Ilal ≤ β,	IIBll ≤ β, and IIB - All ≤ ε.
Then, for all r ∈ Rn and η ≤ 1∕β2 we have
U(I-ηABT) r∣∣e2 ≤ (1 + ηε2)W∙.
Proof Note that using η ≤ 1∕β2 and IlB - AIl ≤ ε we conclude that
II(I - ηABT) r『2 = II(I - ηBBT - η(A - B)Bt) r*
=∣∣r-η(A - B + B)BT r∣∣22
=∣∣r∣∣22 - 2ηrτ(A - B + B)Btr + η2 ∣∣ABtr*
≤ ml- 2ηIBτ 唱+ 2η Il(A - B)t r®BT 我 + ―⑷2IlBT 唱
=什脸土田?*+ 2η Il(A- B)t叱2 IIBTr∣K + (η2IIAlI2∣∣BT班,jIIBT班2)
η≤≤β2 IrI22 - ηIBT班2 + 2η Il(A - B)t叱2 ∣∣Bt喂
IA-BMe
≤	m1-ηIBT 班2+ 2ηεIBT 8鼠―
=(1 + η≡2) 什%-η (ε mh- UBT W')
≤(1 + ηε2) m22 ,
completing the proof.	■
The next lemma shows that if two PSD matrices are close to each other then an appropriate square
root of these matrices will also be close.
Lemma 6.4 Let A and B be n × n positive semi-definite matrices satisfying IlA - Bil ≤ α2 for a
scalar α ≥ 0. Thenfor any X e Rn×p with P ≥ n obeying A = XXt, there exists a matrix Y ∈ Rn×p
obeying B = YYT such that
IIY - XI ≤ 2α
2
Proof First we note that for any two PSD matrices A+, B+ ∈ Rn×n obeying A+, B+ > 宏In, Lemma
2.2 of Schmitt (1992) guarantees that
N2- B?2] ≤
IA+ - B+ I
α
In the above for a PSD matrix A ∈ Rn×n with an eigenvalue decomposition A = UΛUT we use
A1/2 to denote the square root of the matrix given by A = UΛ1∕2UT. We shall use this result with
22
A+ = A + 宏 In and B+ = B + OP In to conclude that
1A+/2 - B1/2] ≤
∣∣A+-B+I JA - BI
α
α
Furthermore, using the fact that the eigenvalues of A+ and B+ are just shifted versions of the
eigenvalues of A and B by α2∕4 we can conclude that
]A+/2-A1/2] ≤ 2 and ]B1/2-B1/2] ≤ ∣.
Combining the latter two inequalities with the assumption that IA - BI ≤ α2 we conclude that
||A1/2 - B1/21 ≤ ]A+/2 - B]/2] + ]A+/2 - A1/2] + ]B1/2 - B1/2]
≤ IA - BI
α
22
α α
+ — + —
≤2α.
(6.4)
20
Under review as a conference paper at ICLR 2020
Suppose p ≥ n and assume the matrices A and B have eigenvalue decompositions given by A =
UAAAUA and B = UBABUB. Then, any X ∈ Rn×p with P ≥ n has the form X = UaA∕ Vj
with VA ∈ Rp×n an orthonormal matrix. Now pick
Y = UBA1B/2UBTUAVAT.
Then clearly Y YT = B . Furthermore, we have
IX - Yll = IUAAAr2Vj - UbAb2UBUAVJ∣∣
=j UaA12 UA UAVJ - UB AB UB UAVJ ∣
=[(UaAA2UA - Ub A1B2UB) UAVJ j
=∣∣(A1/2 - B1/2) UAVJ Il
=|A1/2 - b1/21.
Combining the latter with (6.4) completes the proof.	■
6.1.2	Key lemmas for general nonlinearities
Throughout this section we assume J is the reference Jacobian per Definition 5.1 with eigenvalue
decomposition J = UAVT = ∑sK=n1 λsusvsT with A = diag(λ). We also define a = UTr0 =
Utro ∈ RnK be the coefficients of the initial residual in the span of the column space of this
reference Jacobian.
We shall first characterize the evolution of the linearized parameter θτ and residual rτ vectors from
(5.11) in the following lemma.
Lemma 6.5 The linearized residual vector rτ can be written in theform
τ nK
r = U (I - ηA2)τ a = ∑ (1 - ηλ2)τasus.	(6.5)
s=1
Furthermore, assuming η ≤ 1∕λ2 the linear updates θτ obey
r 2	nK
腐-θo∖∖l2 ≤ ∑ λ2 + T2η2 ∑ λ2a2.	(6.6)
s=1 λs	s=r+1
Proof Using the fact that JJT = UA2 UT we have
(I - ηJJT)τ = U(I-ηA2)τUT
Using the latter combined with (5.11) we thus have
rT =(I-ηJJT )τ ro,
=U(I-ηA2)τUTr0,
=U(I-ηA2)τa,
nK
= ∑ (1 - ηλs2 )τas us ,
s=1
completing the proof of (6.5).
We now turn our attention to proving (6.6) by tracking the representation of θτ in terms of the right
singular vectors of J. To do this note that using (6.5) we have
JT rt = V AUT rt = V A (I - ηA2)t a.
Using the latter together with the gradient update on the linearized problem we have
θτ- θo
-η
(∑ vCin(θt)) = -η (∑ JT Rt) = -ηV (∑ A (I - ηA2)t)
t=0	t=0	t=0
a.
21
Under review as a conference paper at ICLR 2020
Thus for any s ∈ {1, 2, . . . , nK}

瓦) = -ηλsas (∑ (I - ηλ2)t) = -ηλsas -~(	XS)
-as
Noting that for η ≤ 1∕λ ≤ 1∕λ2 We have 1 - ηλ2 ≥ 0, the latter identity implies that
∣vτ (θτ -瓦)∣≤ ∣as∣.
λs
Furthermore, using the fact that 1 - ηλ2s ≤ 1 We have
IvT (θτ- θoI = nλs ∣as∣ (∑ (I-ηλ2y) ≤ ηλs ∣o3∣T
Combining (6.7) for 1 ≤ s ≤ r and (6.8) for s > r We have
(6.7)
(6.8)

completing the proof of (6.6).
For future use we also state a simple corollary of the above Lemma below.
Corollary 6.6 Consider the setting and assumptions of Lemma 6.5. Then, after τ iterations we have
快τ∣∣e2 ≤ (-ngTllnI(TO)Ila + IlnN(TOML.	(6.9)
Furthermore, after T =器 iterations we have
IzT圾 ≤ e-r∣∏ι(To)® + IlnN(ro)场.	(6.10)
and
两-θ⅛ ≤ Σ a∣+Γ2 ∑K1 λa = ⅞γ .
s=1 s	s=r+1 α α
with Bα,Γ given by (6.1) per Definition 6.2.
Proof To prove the first bound on the residual ((6.9))note that using (6.5) We have
UI不T = (I -nΛ)τ UI九 and UN落=(I - ηΛN) UNrro
Thus, using the fact that for s ≤ r We have λs ≥ α We have (1 - ηλs∣)τ ≤ (1 - ηα∣)τ and for s > r We
have (1 - ηλs∣)T ≤ 1, We can conclude that
IlUTH ≤ (- roll,	and	IlUNH ≤ IlUN皿.
Combining these With the triangular inequality We have
T
"=[UN"` ≤UUIxU'2 + IUNH≤(1-ηα2) IIUIro∣∣'2 + IlUN他,
concluding the proof of (6.9). The second bound on the residual simply folloWs from the fact that
(1 - ηα2)τ ≤ e-r. The bound on ∣∣θτ - θo. is trivially obtained by using T2 = nj^ in (6.6)..
The lemma above shoWs that With enough iterations, gradient descent on the linearized problem fits
the residual over the information space and the residual is (in the Worst case) unchanged over the
nuisance subspace N. Our hypothesis is that, When the model is generalizable the residual mostly lies
on the information space I Which contains the directions aligned With the top singular vectors. Hence,
the smaller term ∣∏n(r0)∣∣'2 over the nuisance space will not affect generalization significantly.
To make this intuition precise hoWever We need to connect the residual of the original problem to
that of the linearized problem. The following lemma sheds light on the evolution of the original
problem (5.6) by characterizing the evolution of the difference between the residuals of the original
and linearized problems from one iteration to the next.
22
Under review as a conference paper at ICLR 2020
Lemma 6.7 (Keeping track of perturbation - one step) Assume Assumptions 1 and 2 hold and θτ
and θτ+1 are within an R neighborhood of θ0, that is,
∣∣θτ- θo∣∣'2 ≤ R and |包+ι — θo- ≤ R.
Then with a learning rate obeying η ≤ 1/β2, the deviation in the residuals of the original and
linearized problems e「+1 = r +1 -宁丁+ι obey
Ileτ+ιIl'2 ≤ η(ε0+ εβ)TrτIl'2 + (I + ηε2VeτIL.	(6.11)
Proof For simplicity, denote B1 = J (θτ+1, θτ), B2 = J (θτ), A = J (θ0) where
J(b,a) = ∫0 J(tb
+ (1 - t)a)dt.
We can write the predictions due to θτ+1 as
f(θτ+1)= f(θτ-ηvL(θτ)) = f(θτ) + η J (θτ+1, θτ )vL(θτ)
=f(θτ)+ηJ(θτ+1,θτ)JT(θτ)(f(θτ)-y).
This implies that
rτ+1 = f(θτ+1) - y = (I - ηB1B2T)rτ.
Similarly, for linearized problem We have rτ+1 = (I - ηJJT)rrr. Thus,
IleT+1∣'2 = Il(I -ηB1BT )rτ-(I -JJ T )Fτ.
=II(I - ηB1BT)eτ-η(B1B, - JJT)rτ∣'2
≤ I (I -ηB1 BT)e∕'2 + ηI(B1BT - JJT)rτ I'2
≤ I (I -ηB1 BT)e∕'2 + η Il(BIBT - JJT) I'2.	(6.12)
We proceed by bounding each of these tWo terms. For the first term, We apply Lemma 6.3 With
A = B1 and B = B2 and use IB1 - B2 I ≤ ε to conclude that
I(I - ηB1B2T)eτ I'2 ≤(1+ηε2)IeτI'2.	(6.13)
Next We turn our attention to bounding the second term. To this aim note that
IB1B2T -JJTI = IB1B2T -AAT+AAT-JJTI
≤ IB1B2T-AATI + IAAT-JJTI
≤I(B1-A)B2TI +IA(B2-A)TI +IAAT-JJTI
≤ IB1 - AIIB2I +IB2-AIIAI+IAAT-JJTI
≤ β 2 + β 2 + ε0
= ε02 + εβ.	(6.14)
In the last inequality we use the fact that per Assumption 2 we have ∣∣ B1 - AIl ≤ ε∕2 and ∣∣ B2 - AIl ≤
ε∕2 as well as the fact that per Definition 5.1 IIAAT - JJT∣∣ ≤ ε2. Plugging (6.13) and (6.14) in
(6.12) completes the proof.	■
Next we prove a result about the growth of sequences obeying certain assumptions. As we will see
later on in the proofs this lemma allows us to control the growth of the perturbation between the
original and linearized residuals (eτ = Ieτ I'2).
Lemma 6.8 (Bounding residual perturbation growth for general nonlinearities) Consider posi-
tive scalars Γ,α,ε,η > 0. Also assume η ≤ 1∕α2 and a ≥ ∖∕2Γε and set T = η02. Assume the scalar
SeqUenceS e「(with eo = 0) and Er obey thefollowing identities
Hr ≤(1 - ηα2)τρ+ + ρ-,
eτ ≤(1 + ηε2)eτ-1 + ηΘrEτ -1,	(6.15)
for all 0 ≤ τ ≤ T and non-negative values ρ- , ρ+ ≥ 0. Then, for all 0 ≤ τ ≤ T,
eτ ≤ ΘΛ holds with A =2(，P-: P+).	(6.16)
α2
23
Under review as a conference paper at ICLR 2020
Proof We shall prove the result inductively. Suppose (6.16) holds for all t ≤ τ - 1. Consequently, we
have
et+1 ≤(1 + ηε2)et + ηΘrt
≤et + ηε2et +ηΘ ((1 - ηα2)tρ+ +ρ-)
≤et + ηΘ (ε2Λ + (1 - ηα2)tρ+ + ρ-) .
Thus
et+Θ et ≤ η (∙λ + (I - ηα2)tρ++ P-).
(6.17)
Summing up both sides of (6.17) for 0 ≤ t ≤ τ - 1 we conclude that
τ-1
eτ =et+1 - et
Θ = ∑0	Θ
τ-1
≤ -τ (ε2Λ + P-) + -P+ ∑ (1 - -α2)t
ητ (ε2Λ + ρ-) + ηρ+
t=0
1 - (1 - ηα2)τ
ηα2
≤ η (τε2Λ + -P+2 + τρ-)
=-τ(ε2Λ + ρ-) + ρ+2
α2
≤ -T(ε2Λ + ρ-) + P
α2
Γε2 Λ + ΓP- + P+
α2
Γε2Λ Λ
=+ 2
≤ Λ,
where in the last inequality we used the fact that α2 ≥2Γε2. This completes the proof of the induction
step and the proof of the lemma.	■
6.1.3 Completing the proof of Theorem 5.3
With the key lemmas in place in this section we wish to complete the proof of Theorem 5.3. We will
use induction to prove the result. Suppose the statement is true for some τ - 1 ≤ T - 1. In particular,
we assume the identities (5.14) and (5.15) hold for all 0 ≤ t ≤ τ - 1. We aim to prove these identities
continue to hold for iteration τ. We will prove this result in multiple steps.
Step I: Next iterate obeys 电-Θ0∣∣'2 ≤ R.
We first argue that θτ lies in the domain of interest as dictated by (5.12), i.e. ∣∣θτ - θo ≤ R. To do
this note that per the induction assumption (5.15) holds for iteration T - 1 and thus ∣θτ-ι - θo ［七 ≤
R/2. As a result using the triangular inequality to show ∣θτ - θo ［七 ≤ R holds it suffices to show
24
Under review as a conference paper at ICLR 2020
that Il θr - θτ-ι ∣∣'2 ≤ R/2 holds. To do this note that
电-θτ-1∣∣'2 =ηNL(θτ-ι)E
=η IIJ t (θτ-1)rτ-1∣l'2
=η朽T (/-加一』'2
(a)MIJT (θτ- + η∣∣JTT (θτ-ι)(rτ-ι- rτ-1)必
£ J T 落-1∣∣'2 + η ∣∣j (θτ-ι) - JIMT-IIl'2 + η IIJ (θτ-ι)"rτ-ι - FT-I-
(≤)η JT落-11'2 + ε0β2ε 忤-11'2 +11 rτ-ι-落-11'2
≤ η JTFT-IIl'2 + 7^2 ∣∣r0l'2 + 万 IIrT-1 - rτ-1卜2
5 P	P
⑵	T	2δα	3δα II II
≤ η|J rτ-1∣∣'2 + 7∑2 LK + 7∑2 lr0l'2
5p2	5p2
=η j T FT-Ill'2 + P WoIK
(f) Γ δα
≤ ηβ2 q + 询 Iro I'2
α p2	2
(g)&,Γ	δα	
≤——— ɑ	+ β2	,'2
(h)&,Γ	δΓ	
≤———	+——	, '2
ɑ	ɑ	
R		
=— . 2		
Here, (a) and (b) follow from a simple application of the triangular inequality, (c) from the fact that
IIJ(Θt-1) - JI ≤ IlJ(Θt-1) - J(θo)I + IlJ(θo) - JI ≤ ε + ɛo, (d) from combining the bounds in
(5.13), (e) from the induction hypothesis that postulates (5.14) holds for iteration T - 1, (f) from
considering the SVD J = UΛVT which implies that
JT FT-Ib JT(I -JJ T)T-1 ro∣22 = IlV A (I-ηA2 )T-1 U T ro[
=IIa (I-必2尸 U T ro∣∣22
nK
=∑ λ2(1-ηλ2)2(TT) ((US, ro))2
s = 1
nK
≤ Σ 用((US, ro))2
S=1
=Σ λs (Us, ro))2 + ∑ λS ((Us, ro))2
s=1	s=r+1
r	nK
≤β4 ∑ λ2 ((us, ro))2+ Σ λ2((us, ro))2
s=1 λs	s=r+1
(r	nK ʌ2
∑ 豆((us, ro))2 + Γ2 ∑ -4 ((us, ro))2∣
s=1 -S	s=r+1 Q
=p
（力
4
2
(g) from the fact that η ≤ 表,and (h) from the fact that Q ≤ β and Γ ≥ 1.
Step II: Original and linearized residuals are close (first part of (5.14)).
In this step we wish to show that the first part of (5.14) holds for iteration T. Since we established in
25
Under review as a conference paper at ICLR 2020
the previous step that ∣∣θτ - θo ∣∣'2 ≤ R the assumption of Lemma 6.7 holds for iterations T - 1 and T.
Hence, using Lemma 6.7 equation (6.11) we conclude that
IleTIIa ≤ η(ε2+ εβX∣rΓ-1∣∣'2 + (I + ηε2X∣eτ-ι∣K.
This combined with the induction assumption implies that
IetE ≤η(ε0+ εβ)lrt-ιl'2+ (1+ ηε2)l∣et-ιE,	(6∙18)
holds for all t ≤ T ≤ T. Furthermore, using Lemma 6.5 equation (6.9) for all t ≤ T ≤ T we have
lrt∖'2 ≤ (1 -ηα2)t ∣∣πi(r0)∣∣'2 + IlnN(r0)ll'2,	(6∙19)
To proceed, we shall apply Lemma 6.8 with the following variable substitutions
Θ ：= ε2 + εβ,	ρ+	= IInI(r0)也,P-	= IlnN(ro)∣K,	e, ：=	IleTII'2,	7T	：=	IlrT[e2 .	(6∙20)
We note that Lemma 6∙8 is applicable since (i) η ≤ 1∕β2 ≤ 1∕a2, (ii) based on (5∙13) We have
αε ≥ 5Γ α ≥ √2Γ, (iii) τ obeys T ≤ T =已,and (iv) (6∙15) holds based on (6∙18) and (6∙19)∙ Thus
using Lemma 6∙8 We can conclude that
'2 ≤2(ε2 + εβ)(I∏I(r0)I'2 +ΓI∏N(r0)I'2)
2	0	α2
≤ 2Γ(ε0 + εβ)∣∣r0)'2
≤	2
α2
≤(∣ξ+1) -α me ≤ 1 ɪ 忖。必,
25 5 β	5 β
(6∙21)
(6∙22)
Where in the last inequality We used (5∙13)∙ This completes the first part of (5∙14) via induction∙
Step III: Original and linearized parameters are close (second part of (5∙14)).
In this step we wish to show that the second part of (5∙14) holds for iteration τ∙ To do this we
begin by noting that by the fact that J is a reference Jacobian we have IIJ(θo) - J∣∣ ≤ ε0 where
J augments J (θo) by padding zero columns to match size of J∙ Also by Assumption 2 we have
IlJ(θ) - J(θo)I ≤ ε ∙ Combining the latter two via the triangular inequality we conclude that
IlJ(θτ) - JI ≤ εo + ε.	(6∙23)
Let θ and vL(θ) be vectors augmented by zero padding θ, vL(θ) so that they have dimension
max(Kn, p)∙ Now, we track the difference between θ and linearized θ as follows
®- O''2 = ∑ VL(θt)-VLlin(θt)
η	t=0	`2
T-1
=∑ J(θt)Trt- JTrt
t=0	'2
τ-1 __
≤ ∑ IJ(θt)Trt- JT列'2
t=0
τ-1
≤ ∑ I (J(θt) - J)trI'2 + IJ(θt)T(rt-到'2
t=0
τ-1	__ __________________
=∑ I (J(θt) - J)t可'2 + J(θt)TetI'2
t=0
T-1
≤ ∑(ε + εo)llrt∣l'2 +βlletE.	(6∙24)
t=0
In the last inequality we used the fact that IlJ(θt) - J∣∣ ≤ ε + ε0 and IlJ∣∣ ≤ β∙ We proceed by
bounding each of the two terms in (6∙24) above∙ For the first term we use the fact that ∣∣ rτ ∣∣ 缁 ≤ ∣∣ r0 ∣∣ 冬
to conclude
∑ 同I'2 ≤ τIr0I'2 ≤ TIr0I'2 = γ⅛.	(6∙25)
t=0	ηα2
26
Under review as a conference paper at ICLR 2020
To bound the second term in (6.24) We use (6.21) together with T ≤ T ≤ to conclude that
τ-1	2(εβ + ε20)	2Γ2(εβ + ε02)
∑ Ietl'2 ≤ T---γ-0-rM0l'2 ≤-------Iro3.	(6.26)
t=0	α2	ηα4
Combining (6.25) and (6.26) in (6.24), we conclude that
向-见'2 ≤ (τ(εβ[ε2β)+q) γ 什0 必
α3	α α
2Γβ2 , 2 2Γβ , ε + εo Γ ll ll
=(ε^3-+ε0 F+I α 什0必
夕/2	2 2Γβ ε + εo Γ
≤ (5δ+ε0R-+	) αlr0l'2
(b)	2	2 ε+ε0 Γ
≤ (5δ + 25δ+丁) αlr0l'2
(c)	2	2	1	ε0	Γ
≤ (5δ + 25δ+5δ+α) α 什山 2
(d)	2	2	1	1	Γ
≤ (5δ+25δ+5δ+5δ) α lr0l'2
=亦一rιr0i'2.
25 α
Here, (a) follows from ε ≤ 5δα2 Per Assumption (5.13), (b) from ε0 ≤ 1 ｛爵 per Assumption
(5.13), (c) from ε ≤ 5^ ≤ fα ≤ δα per Assumption (5.13), and (d) from ε0 ≤ δα Per Assumption
(5.13). Thus,
lθτ - θτ∣∣'2 ≤ —rllr0∣l'2 .
α
Combining the latter with the fact that ||。7 - θ01∣'2 ≤ Bor (which follows from Lemma 6.5 equation
(6.6)) we conclude that
lθτ- θ0∣l'2 = ∣∣θτ-θ01∣'2	≤ IBT-θ01∣'2 + ∣∣θτ-	θτ∣∣'2	≤ ^ar	+ /r∣lr0ll'2	≤ JIr0∣∣'2	+ ；	InN(r0)l'2+ -rlr0l'2
2	2	2 α α	2α	α
The completes the proof of the bound (5.15).
Step V: Bound on residual with early stopping.
In this step we wish to prove (5.16). To this aim note that
(a)
IrT l'2 ≤ lrT l'2 + IlrT - rT l'2
(b)	δα
≤ ITtl'2 + 豆Ir01'2
(c)	δα
≤ e-rI∏ι (r0)I'2 + I∏N (r0)I'2 + -J Ir0∣'2
where (a) follows from the triangular inequality, (b) from the conclusion of Step II (first part of
(5.14)), and (c) from Corollary 6.6 equation (6.10). This completes the proof of (5.16).
6.2 Key lemmas and identities for neural networks
In this section we prove some key lemmas and identities regarding the Jacobian of one-hidden layer
networks as well as the size of the initial residual that when combined with Theorem 5.3 allows
us to prove theorems involving neural networks. We begin with some preliminary identities and
calculations in Section 6.2.1. Next, in Section 6.2.2 we prove a few key properties of the Jacobian
mapping of a one-hidden layer neural network. Section 6.2.3 focuses on a few further properties of
the Jacobian at a random initialization. Finally, in Section 6.2.4 we provide bounds on the initial
misfit.
27
Under review as a conference paper at ICLR 2020
For two matrices
ΓAι]
A = A2 ∈ Rp×m
-Bi-
and B = B2 ∈ Rp×n,
乌一
We define their Khatri-Rao product as A * B = [Ai Θ Bi,..., Ap Θ Bp] ∈ Rp×mn, where ⑥ denotes
the Kronecker product.
6.2.1	Preliminary identities and calculations
We begin by discussing a few notations. Throughout we use w` and v` to denote the `th row of input
and output weight matrices W and V. Given a matrix M we use IIM ∣∣2,∞ to denote the largest
Euclidean norm of the rows of M . We begin by noting that for a one-hidden layer neural network of
the form x ↦ Vφ (W x), the Jacobian matrix with respect to vect(W) ∈ Rkd takes the form
J(W) =
[JI(W )1
JK (W)_
∈ RKn×kd
(6.27)
where J'(W) is the Jacobian matrix associated with the 'th class. In particular, J'(W) is given by
J'(W) = [j` (wi) ... J'(wk)] ∈ Rn×kd with J'(ws) ：= ½,sdiag (φf(Xws)) X.
Alternatively using Khatri-Rao products this can be rewritten in the more compact form
J'(W) = (φ'(XWT)diag(v`)) * X.	(6.28)
An alternative characterization of the Jacobian is via its matrix representation. Given a vector
u ∈ RKn let us partition it into K size n subvectors so that u = [uiT . . . uTK]T. We have
K
mat (J T (W )u) = ∑ diag(v`)θ' (WX T )diag(u')X.	(6.29)
'=i
6.2.2	Fundamental properties of the Jacobian of the neural network
In this section we prove a few key properties of the Jacobian mapping of a one-hidden layer neural
network.
Lemma 6.9 (Properties of Single Output Neural Net Jacobian) Let K = 1 so that VT = v ∈ Rn.
Suppose φ is an activation obeying ∣φ'(z)∣ ≤ B for all Z. Then, for any W ∈ Rk×d and any unit length
vector u, we have
J(W )11 ≤B √k MJXII
and
I mat (J T (W )u)∣2,∞ ≤B 同'∞∣X I	(6.30)
Furthermore, suppose φ is twice differentiable and ∣φ''(z)∣ ≤ B for all Z. Also assume all data points
have unit Euclidean norm (IlxiiI 上 =1). Then the Jacobian mapping is Lipschitz with respect to
spectral norm i.e. for all W, W ∈ Rk×d we have
J(W)- J (W )∣∣ ≤ b 同 ` ∞∣X HIW - WIIF.
Proof The result on spectral norm and Lipschitzness of J(W) have been proven in Oymak &
Soltanolkotabi (2019). To show the row-wise bound (6.30), we use (6.29) to conclude that
I mat (J T (W )u)∣2,∞ = IIdiag(v)φ'(WX T) diag(u)X ∣2,∞
≤ lvl'∞ max W(wTXT)diag(U)X /
i≤'≤k
≤ |v|'JX 1 max lφ'(wTXT)diag(u)|'2
i≤'≤k
≤ BHIeJX川WK
=BHIeJX I.
Next we extend the lemma above to the multi-class setting.
28
Under review as a conference paper at ICLR 2020
Lemma 6.10 (Properties of Multiclass Neural Net Jacobian) Suppose φ is an activation obeying
∣φ'(z)∣ ≤ B for all Z. Then, for any W ∈ Rk×d and any unit length vector U, we have
J(W )∣∣ ≤ B√Kk ML.	(6.31)
and
Il mat (J T (W )u)∣∣2,∞ ≤ B√K MIeJX/	(6.32)
Furthermore, suppose φ is twice differentiable and ∣φ''(z)∣ ≤ B for all Z. Also assume all data points
have unit Euclidean norm (IlxiII 上 =1). Then the Jacobian mapping is Lipschitz with respect to
spectral norm i.e. for all W, W ∈ Rk×d we have
IIJ(W)- J (W )∣∣ ≤ B√K I VIIeJX ll∣∣W - W ∣∣F.
Proof The proof will follow from Lemma 6.9. First, given A = [A1T . . . ATK]T and B =
[B1T . . . BKT ]T , observe that
IlAll ≤ √K sup ∣∣A'∣∣	and IIA- Bll ≤ Kκ SuP ∣∣A'- b`∣∣.
1≤e≤K	1≤e≤K
These two identities applied to the components Je(W) and Je(W) - Je(W) completes the proof
of the bound on the spectral norm and the perturbation. To prove the bound in (6.32) we use the
identity (6.29) to conclude that
K
Ilmat(J T (W )u)g,∞ = ∣∣∑ diag(v`)θ′ (WX T) diag(ue)X ∣∣2,∞
e=1
K
≤ ∑ Ildiag(ve)φ' (WXT)diag(u`)X∣∣2,∞
e=1
K
≤∑BIVIe∞IXIIueIe2
e=1
=BIVIe∞IXI (e∑K=1IueIe2)
K κ	1/2
≤ BIHeJXI√K(∑ W22)
=BIVI'∞ IXI√K,
where the penultimate inequality follows from Cauchy Schwarz, completing the proof.
6.2.3	Properties of the Jacobian at random initialization
In this section we prove a few lemmas characterizing the properties of the Jacobian at the random
initialization.
Lemma 6.11 (Multiclass covariance) Given input and output layer weights V and W, consider
the Jacobian described in (6.27). Given an Kn X Kn matrix M, for 1 ≤ ',' ≤ K, let M [','] denote
the (', ')th submatrix. For C (W) = J (W) J (W )T we have
k
C (W )[`a = ∑ (XX T) © (½,sVζsΦ' (Xws)φ'(Xws)T).
s=1
Suppose W i.i.d. N(0,1) and V has i.i.d. zero-mean entries with ν2 variance. Then E[C(W)] is a
block diagonal matrix given by the Kronecker product
E[C (W)] = kν2Σ(X).
where Σ(X) is equal to IK ⑥[(XXT) © E[φ'(Xws)φ'(Xws)T]].
29
Under review as a conference paper at ICLR 2020
Proof The (',')th submatrix of C (W) is given by
C(W)[`a = ((diag(v')φ'(WXT)) * XT)((diag(v~)φ'(WXT)) * XT)t
k
=∑ %(ws) J(Ws)T
s=1
k
=∑ U,s履s(diag(φ'(Xws))X)(diag(φ'(Xws))X)t
s=1
k
=∑ V',sV-s(XXT) © (φ'(Xws)φ'(Xws)T)
s=1
k
=∑ (XXT) © (V',s V,sφ'(Xws)φ'(Xws)τ).	(6.33)
s=1
Setting W i`i.d' N(0,1) and V with i.i.d. zero-mean and V2-variance entries, We conclude that
k
E[C(W)[',6] = ∑(XXT) © (E[¾sV,s] EW(Xws)φ'(Xws)τ])
s=1
k
=∑ V2δ('- ')[(XXT) © Ew(Xws)φ'(Xws)τ]]
s=1
=kδ(' -石ν2Σ(X),
where δ(x) is the discrete δ function which is 0 for x ≠ 0 and 1 for x = 0 and Σ(X) is single output
kernel matrix which concludes the proof.	■
Next we state a useful lemma from Schur (1911) which allows us to bound the eigenvalues of the
Hadamard product of the two PSD matrices.
Lemma 6.12 (Schur (1911)) Let A, B ∈ Rn×n be two Positive Semi-Definite (PSD) matrices. Then,
λmin (A © B ) ≥ (miin Bii ) λmin (A) ,
λmaχ (A © B) ≤ (max Bii) λmaχ (A).
Next we state a lemma regarding concentration of the Jacobian matrix at initialization.
Lemma 6.13 (Concentration of the Jacobian at initialization) Consider a one-hidden layer neu-
ral network model of the form x ↦ Vφ (Wx) where the activation φ obeys ∣φ(0)∣ ≤ B and
∣φ’(z)∣ ≤ B for all Z. Also assume we have n ≥ K data points xι, x2,..., Xn ∈ Rd with unit
euclidean norm (IlxiII七 =1) aggregated as the rows ofa matrix X ∈ Rn×d. Furthermore, suppose
V has i.i.d. V -scaled Rademacher entries (i.e. ±V equally-likely). Then, the Jacobian matrix at a
random point W0 ∈ Rk×d with i.i.d. N(0, 1) entries obeys
IIJ(W0) J(WO)T - E[J(W0)J(W0)τ]∣∣ ≤ 30K√kν2B2∣X∣2log(n).
with probability at least 1 - 1/n100. In particular, as long as
k ≥ 1000K2B41X∣4 log(n)
with the same probability, we have that
712 J(Wo)J(WO)T — ∑(X) ≤ δ.
kV2
Proof Define C = J(WO)J(WO)T. We begin by showing that the diagonal blocks of C are
concentrated. To do this first for 1 ≤ s ≤ k define the random matrices
As = (φ' (Xws) φ' (Xws)τ) © (XXT).
30
Under review as a conference paper at ICLR 2020
Now consider n X n diagonal blocks of C (denoted by C [',']) and note that We have
C[','] = (φ' (XWT) diag(v`)diag(v`)θ′ (WXT)) © (XXT)
k
=∑ v2sAs
s=1
k
=ν2∑As.
s=1
Furthermore, using Lemma 6.12
IlAsll ≤ (max (φ'(xTWs))2)∣∣X∣∣2 ≤ B2 ∣X『.
Also, using Jensen’s inequality
IE[As]I ≤EIAsI ≤B2IXI2.
Combining the latter two identities via the triangular inequality we conclude that
II(As-	E[As])2h IlAs-	E[As]∣2	≤	(IIAsll	+	∣E[As]∣)2 ≤	(2B2∣X∣∣2)2.	(6.34)
To proceed, we will bound the weighted sum
k
S=∑ν2(As-E[As])
s=1
in spectral norm. To this aim we utilize the Matrix Hoeffding inequality which states that
t2
P(Isl ≥ t)≤ 2ne-2∆2,
where ∆2 is an upper bound on I∑sk=1 ν4(As - E[As])2 I. Using (6.34) we can pick ∆2 =
∑k=ι(2ν2B2IIX∣2)2 = 4kν4B41X∣4. Setting t = 30√kν2B2 IlX∣2√log(n), we conclude that
p{ Q[',`] - E[C[',']H ≥ t} = P(ISl ≥ t) ≤ n-102
concluding the proof of concentration of the diagonal blocks of C .
For the off-diagonal blocks C [','] using (6.33) from the proof of Lemma 6.11We have that
k
__________________________________________
C['，'] = ∑ V',s%As.
s=1
Note that by construction {V',sV'^s}k=ι are i.i.d. ±ν2 Rademacher variables and thus C[','] is sum
of zero-mean i.i.d. matrices and we are again in the position to apply Hoeffding’s inequality. To this
aim note that
k	kk
∑ V2sV2sA2 =ν4 ∑ A2 ≤ V4 ∑ W2 ≤ ν4kB4 .4 ,
s=1	s=1	s=1
so that we can take ∆2 = V4kB4 ∣∣X ∣∣4 and again conclude that for t = 30√kν2B2∣∣X∣∣2 log(n) we
have
p{∣∣C[',4∣∣ ≥ t}≤ n-102
Using the fact that E[C[',']] = 0 and K ≤ n, combined with a union bound over all sub-matrices
1 ≤ `, ` ≤ K we conclude that
p{∣C[',?] - E [C[',即|| ≥ t}≤ K2n-102 ≤ n-100.
All that remains is to combine the concentration results for the sub-matrices to arrive at the complete
bound. In mathematical terms we need to bound D ：= IlC - E[C]∣∣.To this aim define D[', ：] to
denote the `th block row of D. Standard bounds on spectral norm in terms of sub-matrices allow us
to conclude that
∣∣D[', ：]|| ≤√K sup ∣∣D[',目I ≤ √Kt ⇒
1≤'≤K
IlDIl ≤√K SUp ∣∣D[', ：]I ≤ √K√Kt = Kt = 30K√kν2B2IX∣∣2 log(n),
1≤'≤K
concluding the proof. The result in terms of δ is obtained by using the population covariance Lemma
6.11.	■
31
Under review as a conference paper at ICLR 2020
6.2.4	Upper bound on initial residual
In this section we prove a lemma concerning the size of the initial misfit. The proof of this lemma
(stated below) follows from a similar argument in the proof of (Oymak & Soltanolkotabi, 2019,
Lemma 6.12).
Lemma 6.14 (Upper bound on initial residual) Consider a one-hidden layer neural network
model of the form x ↦ V φ (Wx) where the activation φ has bounded derivatives obeying
∣Φ(0)∣,∣Φ’(z)∣ ≤ B ∙ Also assume we have n data points xι, x2,..., Xn ∈ Rd with unit euclidean
norm (IlxiiI 和=1) aggregated as rows of a matrix X ∈ Rn×d and the corresponding labels given
by y ∈ RKn. Furthermore, assume the entries of V are i.i.d. Rademacher variables scaled by
50B√KK⅛g'22K)kn and the entries of W ∈ Rk×d are i.i.d. N(0,1). Then,
IVφ (WX T )∣∣F ≤ V ∣∣y∣∣'2,
holds with probability at least 1 - (2K)-100.
Proof We begin the proof by noting that
2K 2
UV φ (WX T )∣F = ∑ MT φ (WX T )『2.
'=1
We will show that for any row v of V, with probability at least 1 - (2K)-101,
MT φ (WXT )∣∣'2 ≤ √VK ∣∣y∣∣'2.	(6.35)
so that a simple union bound can conclude the proof. Therefore, all that remains is to show (6.35)
holds. To prove the latter, note that for any two matrices W, W ∈ Rk×d We have
∣ UΦ (xw T) v∣'2 - ∣φ (xw T) v∣'21 ≤ M (xw T) v - φ (xw T) v∣'2
≤ M (XW T) - Φ (XW T )UM2
≤ M (χW T) - φ (XW T )Uf 同'2
(a)U(φ' (S ® XWT + (ik×n - S) ® XWT)) ® (X(W - W)T)∣∣f ∣v∣'2
≤B∣∣χ(W - W)TIIF 同'2
≤b I x	'2∣w - W N
where in (a) we used the mean value theorem with S a matrix with entries obeying 0 ≤ Si,j ≤ 1
and 1k×n the matrix of all ones. Thus, Iφ (XWT) vI'2 is a B IXI IvI'2-Lipschitz function of W.
Thus, fixing v, for a matrix W with i.i.d. Gaussian entries
Iφ(XWT)vI'2≤E[Iφ(XWT)vI'2]+t,	(6.36)
- /
holds with probability at least 1 - e 2B2'v% 'x'2 . Next given g Z N(0, l),we have
∣E[φ(g)]∣≤∣E[φ(0)]∣+∣E[φ(g)-φ(0)]∣≤B+BE[∣g∣]≤2B and Var(φ(g))≤B2. (6.37)
where the latter follows from Poincare inequality (e.g. see (Ledoux, 2001, p. 49)). Furthermore, since
v has i.i.d. Rademacher entries, applying Bernstein bound, event
Ev ：= {∣1Tv∣2 ≤ 250 log K IwIl玄}	(6.38)
32
Under review as a conference paper at ICLR 2020
holds with probability 1 - (2K)-102. Conditioned on Ev, we now upper bound the expectation via
E[∣∣φ(XWT)v∣∣'2] (a)√E[∣∣φ(XWT)」脸]
∖
∑n E [ (vTφ(Wxi))2]
i=1
(=)√n∖JEg~N(0,Ik) [(vTφ(g))2 ]
(=)√n√Hl22 Eg~N(0,1) [(φ(g) - E[φ(g)])2 ] + (ITv)2(Eg~N(0,1) [φ(g)])2
(d)	/---------------------
≤ √n∣∣v∣∣'2 √250 × 4B2log(2K) + B2
≤32 Jnlog(2K)B ∣∣v∣∣'2 .
Here, (a) follows from Jensen’s inequality, (b) from linearity of expectation and the fact that for xi
With unit Euclidean norm Wxi 〜N(0, Ik), (C) from simple algebraic manipulations, (d) from the
inequalities (6.38) and (6.37). Thus using t = 18√nlog(2K)B [vll`2 in (6.36), conditioned on Ev
We conclude that
M(XWT)v∣∣'2 ≤50 Jnlog(2K)B 同七
50√n log(2K) B√k
V Iyh
50B√K log(2K )kn
V忖必
√K
(6.39)
holds with probability at least 1-exp(-102log(2K) JXnp∙) ≥ 1-(2K )-102 where we used n ≥ IIX ∣∣2.
Using a union bound over Ev and the conditional concentration over W, the overall probability of
success in (6.39) is at least 1 - (2K)-101 concluding the proof of (6.35) and the Lemma. ■
6.3 Rademacher complexity and generalization bounds
In this section we state and prove some Rademacher complexity results that will be used in our
generalization bounds. We begin with some basic notation regarding Rademacher complexity. Let
dK	n	K
F be a function class. Suppose f ∈ F maps R to R . Let {εi }i=1 be i.i.d. vectors in R with
i.i.d. Rademacher variables. Given i.i.d. samples S = {(xi, yi)}n=1 〜 D, we define the empirical
Rademacher complexity to be
1n
RS(F) = - E sup ∑ εTf(xi).
n	f∈F i=1
We begin by stating a vector contraction inequality by Maurer Maurer (2016). This is obtained by
setting hi(f(xi)) = h(yi, f(xi)) in Corollary 4 of Maurer (2016).
Lemma 6.15 Let f (∙) : Rd → RK and let' : RK X RK → R be a 1 Lipschitz loss function with respect
to second variable. Let {εi}in=1 be i.i.d. Rademacher variables. Given i.i.d. samples {(xi, yi)}in=1,
define
n
RS(', F) = E fiuF ∑ εi'(yi,f(xi)).
We have that
RS (', F) ≤ √2Rs (F).
Combining the above result with standard generalization bounds based on Rademacher complexity
Bartlett & Mendelson (2002) allows us to prove the following result.
Lemma 6.16 Let '(∙, ∙) ： RK X RK → [0, 1] be a 1 Lipschitz loss function. Given i.i.d. samples
{(xi, yi)}in=1, consider the empirical loss
1n
L(f,') = — ∑ '(yi,f (xi)).
n i=1
With probability 1 - δ over the samples, for all f ∈ F, we have that
E[L(f,')] ≤L(f,') + 2√2Rs (F) + ∖/5log(2∕δ)
n
33
Under review as a conference paper at ICLR 2020
Proof Based on Bartlett & Mendelson (2002),
E[L(f,')] ≤ L(f,') + 2Rs(',F) + ∖∕5lθg(2/')
n
holds with 1 - δ probability. Combining the latter with Lemma 6.15 completes the proof.
Lemma 6.17 Consider a neural network model of the form x ↦ f (x; V, W) = Vφ (Wx) with
W ∈ Rk×d and V ∈ RK×k denoting the input and output weight matrices. Suppose Vo ∈ RK×k
is a matrix obeying ∣∣V0∣∣'∞ ≤ V/，kK. Also let Wo ∈ Rk×d be a reference input weight matrix.
Furthermore, we define the neural network function space parameterized by the weights asfollows
Fv,w = {f(x; V, W) such that V ∈ V and W ∈ W} with V = {V ： ∣V - VOIlF ≤
and W = {w ：||W - WOilF ≤ MW and IlW - W0∣∣2,∞ ≤
(6.40)
Additionally, assume the training data {(xi, yi)}N1 are generated i.i.d. with the input data points of
unit Euclidean norm (i.e. ∣xi fe2 = 1). Also, define the average energy at Wo as
1 n	1/2
E = (kn ∑ M(Wox∙<).
Also let {ξi}n=ι ∈ RK be i.i.d. vectors with i.i.d. Rademacher entries and define the empirical
Rademacher complexity
Rs (Fv,w) ：=1 E sup ∑ ξTf(xi)
n	f∈Fv,w £1
Then,
MW + EMV R2 + MW Mv∖
RS(FV，W)≤VB (	√n	+ —√k-).
(6.41)
Proof We shall use w` to denote the rows of W (same for Wo, V, VO). We will approximate
φ ((w`, x“) by its linear approximation φ ((w?, x“) + φ’((w?, xn) ((w` - w?, xn) via the second
order Taylor,s mean value theorem. We thus have
RS (FV,W)≤ nE
n
∑ ξTVoφ (Woxi)
i=1
+ 1 E sup ∑ ξT%diag(" (WOXi))(W - Wo) Xi
n	W∈W M
+ — E iid
2n	&,产 ±1
n k K	2
sup ∑ ∑ ∑ ξT V0φ” ((1 - ti`)(wo, Xi)+ ti` (w', xi〉)((w'- wo, xi))
W∈W i=ι '=1 j=ι
R
+ 1 E sup ∑ξi(V - %)(φ(WXi)- φ(Woxi))
n LV ∈v ,w ∈W i=ι
1n
+ n e [既 ∑ξT(V - V))φ(WOXi)
苞
34
Under review as a conference paper at ICLR 2020
We proceed by bounding each of these four terms. For the first term note that
Ri ≤ 1 E
n
sup	∑ ξT V0diag (φ' (Woxi))(W - W0) Xi
IlW-W0∣∣F≤Mw i=1
≤1 E
n
sup (∑ diag (φ' (WoXi)) VOrξiX, W - Wo)
IlW-W0∣F≤Mw 'i=1	'
≤ MnW e[∣(∑ diag (VTξi) φ' (WoXi) XT )|
MW
≤--
n
E
2 I
[1(
n
∑ diag (VTξi) Φ' (WoXi)
i=1
XX )1
1/2
F
r	]1/2
=M ∑ E IIdiag(VTξi)φ'(WoXi) XTM
L i=i	J
「	^11/2
=MW ∑ E Miag (VT&)“ (WOXi)场
n	i=i
≤ BMW [∑ E IVTe⅛
n L i=1
≤ BMW网尸
n
BMW V
√ √n ,
where in the last inequality we used the fact that IIVOlIF ≤ ν. For the second term note that
InkK	2
R2 ≤-E	sup	∑∑∑ ξi,j vo,j'φff ((1- ti`)(w?, Xi) + ti`(w`, Xi))((wg- wo, Xih
2n L∣W-W0 ∣∣2,∞≤R i=1 '=1 j=1
1 Λ
≤ — ∑ E
2n £1
n I K
SUP	∑ ∑ ξi,jvo,j,e
∣∣w'-w0∣∣3 ≤R i=i j=i
1 k n [ K	I J
≤ W Σ i∑ 电 ξij MR2 B
≤ ɪ IVT ∣2,1
BR2V
≤----.
2 ∖fk
35
Under review as a conference paper at ICLR 2020
In the above We used IlM ∣∣ 2,1 for a matrix M to denote the sum of the Euclidean norm of the rows
of M. We also used the fact that ∣ V0T ∣2,1 ≤ ν∖∕k. To bound the third term note that
1n
R3 = — E SUp	∑ξi(V - V0)(φ(Wxi.)- φ(W0Xi))
n	V ∈V,W∈Wi=1
≤ 1 E	SUp	∑ IV - VWF 间'2Kφ(Wxi)-φ(W0xi)儿 2
n	V ∈V,W∈Wi=1	2	2
≤ j√M= E[ sup Σ ∣ξi∣'2 ∖∖Φ(Wxi) - φ(W0xi)∖∖'2]
n kK	W ∈W i=1
νM	n
≤ —τ≡ ∙ sup ∑ M(Wxi)- φ(W0xi)∖'2
n k W∈Wi=1
≤ VBMV ∙ sup ∑ II(W - W0)xi∣'2
n k W∈W i=1
≤ v√V, ∙ sup ∑ Il(W - wo)∣∣f
n k W∈W i=1
=VBMV ∙ sup Il(W- Wo)∣F
k	W∈W
VBMV MW
√k
Finally, to bound the fourth term note that we have
1n
R4 = n e[ V∈V •§&(V - V0)φ(W0xi)
=1 E sup	( ∑ξiφ(Woxi)τ, (V - Vo)∖
n LIVMF≤√MV i=1
=VMV= E [归 Φ(W0xi)ξT ]
n kK	i=1	F
≤ j√M= (E[∣∑Φ(W0xi)ξT2])1/2
n√kK ∖ Llli=I	F J/
=j√M= (∑ e [M(W0xi)ξT IlF])1/2
n kK i=1	F
=VMV 佶 ∑ E [∣Φ(W0 xi)∣F ])1/2
√n ∖knML J∕
VEMV
√n
VBEMV
≤ √n
Combining these four bounds we conclude that
RS (FV,W ) ≤ VB
+
R2
√k +
MVMW
concluding the proof of (6.41).
Next we state a crucial lemma that connects the test error measured by any Lipschitz loss to that of
the quadratic loss on the training data.
Lemma 6.18 Consider a one-hidden layer neural network with input to output mapping of the form
x ∈ Rd ↦ f(x; V, W) = Vφ (Wx) ∈ RK with W ∈ Rk×d denoting the input-to-hidden weights and
36
Under review as a conference paper at ICLR 2020
V ∈ Rk×K the hidden-to-output weights. Suppose V ∈ RK×k is a matrix obeying ∣∣ V0∣∣'∞ ≤ V/VkK.
Also let W0 ∈ Rk×d be a reference input weight matrix. Also define the empirical losses
1n
l(v, w) = - ∑lyi-f(g; V, w )唬,
and
-n
f = n Σ '(f(xi； V，W )，yi)，
with ' : RK X RK → [0,1] a one Lipschitz loss function obeying '(y, y) = 0. Additionally, assume
the training data {(xi, yi)}in=1are generated i.i.d. according to a distribution D with the input data
points of unit Euclidean norm (i.e. ↑Xi E = 1). Also, define the average energy at Wo as
E=
1n
(嬴∑
M(W0Xi)II22)
1/2
Then for all f in the function class FW given by (6.40)
E[L(f,')] ≤ √L(V^) + 2√2νB (M√W + √k + VBMMW + EMV) + J51,
(6.42)
holds with probability at least 1 - δ. Furthermore, Suppose labels are one-hot encoded and thus unit
Euclidian norm. Given a sample (x, y) ∈ Rd X RK generated according to the distribution D, define
the population classification error
ErrD (W) = P (arg max yi ≠ arg max fi(x, W)).
Then, we also have
ErD(W) ≤ 2 […+ 2√2vB (哭 +	TrW + 詈卜尸回
(6.43)
Proof To begin first note that any I-LiPschitz ' with '(y, y) = 0 obeys '(y, y) ≤ Ily 一 ry∣'2. Thus,
we have
1n	________
L(f,') ≤ n ∑ Iyi - f(Xi； V, W)∣'2 ≤ √L(V, W),
where the last inequality follows from Cauchy-Schwarz. Consequently, aPPlying Lemmas 6.16 and
6.17 we conclude that
E[L(f,')] ≤L(f,') + 2√2 ∙ Rs(F) + ∖∕⅛2≡,
n
≤ √L(W) + 2√2νB (MW + √2 + VBMMW + EMV) + 产等,
which yields the first statement.
To Prove the second statement on classification accuracy, we Pick the ` function as follows
'(y, y) = min(1Jy - y∣'2).
Note that, given a samPle (X, y) ∈ Rd X RK with one-hot encoded labels, if
arg max y' ≠ arg max f'(x; V, W),
this imPlies
' (y,f (x； V, W)) ≥ 0.5.
Combining the latter with Markov inequality we arrive at
ErrD(W) ≤ 2E(XM~。['(y,f(x; V, W))] = 2E[L(', W)].
Now since ` is 1 LiPschitz and bounded, it obeys (6.42), which combined with the above identity
yields (6.43), completing the proof.	■
37
Under review as a conference paper at ICLR 2020
6.4 Proofs for neural nets with arbitrary initialization (Proof of Theorem 2.3)
In this section we prove Theorem 2.3. We first discuss a preliminary optimization result in Section
6.4.1. Next, in Section 6.4.2 we build upon this result to prove our main optimization result. Finally,
in Section 6.4.3 we use these optimization results to prove our main generalization result, completing
the proof of Theorem 2.3.
6.4.1	Preliminary Optimization Result
Lemma 6.19 (Deterministic convergence guarantee) Consider a one-hidden layer neural net of
theform X ↦ f(x; W) ：= Vφ(Wx) with input weights W ∈ Rk×d and output weights V ∈ RK×k
and an activation φ obeying ∣φ(0)∣ ≤ B, ∣φ'(z)∣ ≤ B, and ∣φ'' (z)∣ ≤ B for all Z. Also assume V is
fixed with all entries bounded by IlV ∣∣g∞ ≤ √k= and we train over W based on the loss
1n
L(W) = 5 ∑ If (xi； w)-切脸∙
2 i=1
Also, consider a point W0 ∈ Rk×d with J an (0, νB IX I) reference Jacobian associated with
J (W0) per Definition 5.1. Furthermore, define the information I and nuisance N subspaces and
the truncated Jacobian JI associated with the reference Jacobian J based on a cut-off spectrum
value of α per Definition 5.2. Let the initial residual vector be r0 = y - f(W0) ∈ RnK. Furthermore,
assume
εo ≤ α min (δ, ʌ /，： ：)
5 N ΓνB∣∣x1
(6.44)
and
k ≥ 400 y6「2 (BaJ + 巾引'2 )2，	(6.物
with 0 ≤ δ ≤ 1 and Γ ≥ 1. We run gradient descent iterations ofthe form WT+ι = WT - η^L(Wτ)
startingfrom Wo with step size η obeying η ≤ 丫?621凶2. Thenfor all iterates T obeying 0 ≤ T ≤ T ：=
Γ
ηα2
IlWr - Wo IF ≤ Bar + δγ ∣∣ro∣K.	(6.46)
αα
IlWT- Wo∣∣2,∞ ≤2ν√r 1X1 IlrOlI'2∙	(6.47)
kα2
Furthermore, after T = T iteration we have
IlrTE ≤ e-r∣∣πι(ro)E + IInN(ro)E + jɑ ll IlrOll'2∙	(6.48)
2	2	2 νB IX I 2
Proof To prove this lemma we wish to apply Theorem 5.3. We thus need to ensure that the
assumptions of this theorem are satisfied. To do this note that by Lemma 6.10 Assumption 1 holds
with β = VB IlX ∣∣. Furthermore, We pick ε =	2喘 2 = 5δ02 which together With (6.44)
5Γ V B Il -X Il	5r β
guarantees (5.13) holds. We now turn our attention to verifying Assumption 2. To this aim note that
for all W ∈ Rk×d obeying
IlW - WoIlF ≤ R ：= 2 (虹 + δγIlrOll'2)
αα
38
Under review as a conference paper at ICLR 2020
as long as (6.45) holds by Lemma 6.10 we have
J(W)-J(Wo)I ≤B√K∣M'JXR
≤ξ√kB IX ∣ R
C 3	20Γν3B3∣∣X∣∣3 /
δα3	δa4	(&丁 +『ME)
10Γν 2B2∣X ∣2
δα3
≤-------------2
10Γν2B2 IX∣∣2
ε
=—.
2
Thus, Assumption 2 holds with IlW- W0IlF ≤ R ：= 2(Bar + δ£||roE). Now that We have
verified that the assumptions of Theorem 5.3 hold so do its conclusions and thus (6.46) and (6.48)
hold.
We now turn our attention to proving the row-wise bound (6.47). To this aim let w'T) denote the 'th
row of Wτ . Also note that
NL(Wi) = 'th row of mat( J(W)Tr「).
Hence, using Lemma 6.10 equation (6.32) we conclude that
卜 L(w'τ)儿2 ≤ B√K ∣M'JX∣∣⅛2 ≤ vb√X1 IIrT IK.
Consequently, for any row 1 ≤ ' ≤ k, we have
∣∣w'τ)- w'°]∣' ≤ ηνBiX i ∑ NK.	(6.49)
`2	k t=0
To bound the right-hand side we use the triangular inequality combined with (6.25) and (6.26) to
conclude that
T-1	T-1	T-1
η ∑ WTlI'2 ≤η ∑ 同必 + η ∑ IIrT-FK
t=0	t=0	t=0
Γ	2Γ2(εβ+ε02)
≤ α2 Iro I'2 + —k04~~ Iro I'2
2r(ε2 + εβ) + a2m~ U
=---------4---γ UroU'z
α4
≤2 F IroI'2,	(6.50)
α2
2	α2	δα3	α2
where in the last inequality we used the fact that εj ≤ 枭 Per (6.44) and Ce = δaβ ≤ Or Per OUr
choice of . Combining (6.49) and (6.50), we obtain
(τ)	(0)	2νBIX IΓll ll
llw'	w' ll'2≤	√ka2	Ir0I'2，
completing the proof of (6.47) and the theorem.	■
6.4.2 Main Optimization Result
Lemma 6.20 (Deterministic optimization guarantee) Consider the setting and assumptions of
Lemma 6.19. Also assume IΠI (ro)I'2 ≥ cIro I'2 for a constant c > 0 if εo > 0. Furthermore,
assume
2 α2	Bα,Γα
0 ≤ 25	(cνBΓ2IroI'2 IXI,
Z2V2B2IXI2 Z)
,Γ ),
α2
(6.51)
39
Under review as a conference paper at ICLR 2020
and
k ≥ 1600 (——α~- +
ZVB ,
Γ⅛ 2 V6B6 IX/Γ2Bα,Γ
Bα,Γ )
α8
(6.52)
and Γ ≥ 1. We run gradient descent iterations of the form WT+ι = WT - η^L(Wτ) Startingfrom
Wo with step size η obeying η ≤ ”2621因产.Thenfor all iterates T obeying 0 ≤ T ≤ T ：= ^02
IlWT- W0∣∣F ≤ 2Bαz.	(6.53)
α
IIWT-W0∣∣2,∞ ≤ 2v√γ 1X1 ∣r0∣'2.	(6.54)
kα2
Furthermore, after T = T iteration we have
IIf(WT) - y∣'2 ≤ e-rIInI(r0)∣∣'2 + IlnN(ro)∣'2 + Z∣r0∣'2.	(6.55)
Proof To prove this lemma we aim to substitute
δ = min ( ZVB≡
α
Bα,Γ
「什0卜2
)≤1,
(6.56)
in Theorem 6.19. To do this we need to verify the assumptions of Theorem 6.19. To this aim note
that the choice of δ from (6.56) combined with (6.52) ensures that
k ≥1600 (——α~- +
ZVB IXI
Γ⅛)2 V6B6 IXI6Γ2B0,γ
Bα,Γ )
ɑ	Γ∣∣r0∣∣'2 2 1600ν6B6 IIXFΓ2B0,「
≥ max(ZVB `, ~bo,γ~ )	α8
1	1600v6B6 IXI6Γ2B0,γ
≥min (ZVBiXI 卫J)2	α8
min(	α FroL)
1600Γ2V6B6 IXI6Bα2,Γ
=	δ2ɑ	—
Γ2v6B6 IXI6 (Bα,Γ + Bα,γ)2
=400----------δ2α-----------
>400Γ2V6B6 IXF (Bα,Γ + δΓ∣∣r0∣∣'2)2
≥	δ2α8	,
so that (6.45) holds. We thus turn our attention to proving (6.44). If ε0 = 0, the statement already
holds. Otherwise, note that based on Lemma 6.2 equation (6.3) we have
K)	aInI (⑹匕、aIInI (r0)I'2 , c«Ir0I'2 、
BrV Γ ≥	≥	≥	⇒
α,r ≥	λι	≥	νBIX I	≥ νBIX I	C
Bα,Γ ≥
Ir0I'2 ≥ vB IXI ∙
(6.57)
Recall that α = √√K, which implies that
• If δ = ZVBjX i: For (6.44) to hold it suffices to have ε0 ≤ 5 min ( ZVBjX i
α
40
Under review as a conference paper at ICLR 2020
• If δ = r*t : For (6.44) to hold it suffices to have ε0 ≤
(6.57) we have 得=√VBΓ⅛ ≤ 心
5ʌ/CνBΓ2*r∣∣: IlXll as based on
α
£0 ≤-
5
7
cBa,rα
νBΓ2∣∣r0E IlXIl
Combining the latter two cases as long as
2 , α2	Bα,Γα
0 ≤ 云	[cνBΓ2W0∣K IIXI'
ζ2ν2B2IIX∣2 Z)
α2
⇔	(6.51),
then (6.44) holds. As a result when (6.51) and (6.52) hold with δ = min ( "BX, r*t ) both
assumptions of Theorem 6.19 also hold and so do its conclusions. In particular, (6.53) follows
from (6.46) by noting that based on our choice of δ we have δΓ ∣r0|扃 ≤ BoL, (6.54) follows
immediately from (6.47), and (6.55) follows from (6.48) by noting that based on our choice of δ we
have VBlX ≤ Z.	■
6.4.3 Main generalization result (completing THE proof of Theorem 2.3)
We state the following rigorous and stronger version of Theorem 2.3. The differences (besides
constant terms) are the use of ∣ X ∣ rather than √n (we always have IlX ∣ ≤ √n) and use of the tighter
bound IIJIr01，ratherthan IlnZ(r0)∣h /α.
Theorem 6.21 Let ζ, Γ,α be scalars obeying ζ ≤ 1/2, Γ ≥ 1, and a ≥ 0 which determine
the overall precision, cut-off and learning duration, respectively.* 4 Consider a training data set
{(xi, yi)}n=ι ∈ Rd × RK generated i.i.d. according to a distribution D where the input samples have
unit Euclidean norm. Also consider a neural net with k hidden nodes as described in (1.1) parame-
terized by W where the activation function φ obeys ∣φ'(z)∣ ,∣φ"(z)∣ ≤ B. Let W0 be an arbitrary
initial weight matrix. Also assume the output matrix has bounded entries obeying IlV ∣)∞ ≤ √k=.
Furthermore, Set J ：= J(W0) and define the information I and nuisance N subspaces and the
truncated Jacobian JZ associated with the reference/initial Jacobian J based on a cut-off spectrum
value α = VBa √∕n / X ∣∣. Also define the initial residual r° = f (W0) - y ∈ RnK and pick Cr > 0
so that 与兽 ≤ Cr. Suppose number ofhidden nodes k obeys
k N ⅛⅛,
(6.58)
with Γ ≥ 1 and tolerance level ζ. Run gradient descent updates (1.5) with learning rate η ≤ ^5：X『.
Then, after T =5^五 iterations, training loss obeys
IIf(Wr) -y∣∣'2 ≤ IlnN(r0)I'2 + Cr (e-r + Z) √n.
and with probability at least 1 - δ, the generalization error obeys
ErrD(WT) ≤ 之阳咪层 + 1√VB (!JIr0∣`2 + ΓIInN(r%) +5 J胃* + 2C,(e-r + Z).
bias term	variance term
4Note that this theorem and its conclusions hold for any choice of these parameters in the specified range.
41
Under review as a conference paper at ICLR 2020
Theorem 6.21 immediately follows from Theorem 6.22 below by upper bounding D%γ (See Definition
6.1) using Lemma 6.2 equation (6.2).
Theorem 6.22 Consider a training data Set {(xi, yi)}n=ι € Rd × RK generated i.i.d. according to
a distribution D where the input samples have unit Euclidean norm. Also consider a neural net
with k hidden nodes as described in (1.1) parameterized by W where the activation function φ
obeys ∣φ'(z)∣ ,∣φ"(z)∣ ≤ B. Let Wo be an arbitrary initial weight matrix. Also assume the output
matrix has bounded entries obeying IlV∣∣g∞ ≤ √k=. Furthermore, set J ：= J(Wo) and define
the information I and nuisance N subspaces and the truncated Jacobian JI associated with the
reference/initial Jacobian J based on a CUt-OffSPeCtrUm value α = VBa √√nʌ/) X ∣. Also define
the initial residual ro = f (Wo) - y e RnK and pick Cr > 0 so that ∣∣7√^* 2 ≤ Cr. Also assume, the
number of hidden nodes k obeys
k ≥ 25600
C2γ4
α8ν 2B2Z2
(6.59)
with Γ ≥ 1 and tolerance level ζ ≤ 2. Run gradient descent updates (1.5) with learning rate
η ≤ ν2B21 因『.Then, after T = ^^ iterations, with probability at least 1 - δ, the generalization error
obeys
ErrD(WT) ≤ 2⅛(⅛ + -J +5	+ 2q(「r+ ¢),	(6.60)
ʌ/n	ʌ/n	V n
`----V-------' `----V----'
bias term	variance term
where D%γ is the early stopping distance as in Def. (6.1).
Proof First, note that using α ≤ β = VB IlX ∣, BTr ≤ Moh per(6.2), and IlrOll 七 ≤ Cr √n we have
α ≤ I ≤ rllroll'2 ≤ C γ√n
VBIX 「 ≤ Bα,r ≤ r Ba,Γ
This together with Z ≤ 2 implies that
α	+ rWoIK
2 νB .	&,γ
≤ α + r∣∣ro∣∣'2
^∣VB IlXll 2&,Γ
≤2 rHoII a
≤ IBr
(6.61)
Thus when
k ≥25600
C2Γ4
a8ν2B2ζ2
n2Γ2ν6B6 IIXI4
ɑ8
√n≥ixi	/CrΓ∖2 nν6B6 IlXI6
≥ 64001 ɪ) -0
=1600 (2 K ± W
Ba,ΓV6B6 IIXC
α8
(6≥1)1600/	α + r∣⅛Y Ba,γv6B6IXC
一 (2 νB IIXI &,γ )	α8
42
Under review as a conference paper at ICLR 2020
Thus, (6.52) holds. Also (6.51) trivially holds for ε0 . Thus applying Theorem 6.20 with ε0 = 0 the
following three conclusions hold
IlWT- Wo I F ≤ 2Bαr = 2Dα,Γ.
α
(6.62)
and
IlWT- Wo∣∣2,∞ ≤2VBr 'X' IroI'2
kα2
∣∣r0∣∣'2≤Cr √n 2 √ncr VBrIIX Il
√kα2
(6.63)
and
IIf(WT) - y∣'2 ≤e-r∣∏ι(r0)∣'2 + IlnN(r% + 2 同也
≤I∏N (ro)I'2 + (e-r + ζ2 )什。必
I∣r0∣∣3 ≤Cr√n	C L
≤	IlnN (r。)E + Cr (e一 + 2) √n.
(6.64)
Furthermore, using the assumption that IIV ∣.∞ ≤ √k= Lemma 6.18 applies and hence equation
(6.43) with TW = WT, √L(WT)
implies that
f(W√-yl'2, MW = 2Dα,r, MV = 0, and R
2√ncr VBrilX Il
ɑ2
ErrD (WT) ≤ 2
f(Wτ)- yl
`2 + 3VB
5log(2∕δ)
n
(6.65)
Also note that using (6.59) we have
3VBR2
——≤
√k
12C2Γ2 V 3B3n∣∣X ∣∣2
√kα4
≤ Cr 2
(6.66)
Plugging (6.64) and (6.66) into (6.65) completes the proof.
6.5 Proofs for neural network with random initialization (proof of Theorem
2.2)
In this section we prove Theorem 2.2. We first discuss and prove an optimization result in Section
6.5.1. Next, in Section 6.5.2 we build upon this result to complete the proof of Theorem 2.2.
6.5.1 Optimization result
Theorem 6.23 (Optimization guarantee for random initialization) Consider a training data set
{(xi, yi)}rn=ι ∈ Rd X RK generated i.i.d. according to a distribution D where the input samples
have unit Euclidean norm and the concatenated label vector obeys ∣∣y∣∣'2 = √n (e.g. one-hot
encoding). Consider a neural net with k hidden layers as described in (1.1) parameterized by W
where the activationfunction φ obeys ∣φ'(z)∣, ∣φ''(z)∣ ≤ B. Let Wo be the initial weight matrix with
i.i.d. N (0, 1) entries. Fix a precision level ζ and set
ζ
V =------ -
50 B ∖∕log(2K)
(6.67)
Also assume the output layer V has i.i.d. Rademacher entries scaled by -^=. Furthermore,
kK
set J ：= Σ(X)1/2 and define the information I and nuisance N spaces and the truncated
Jacobian JI associated with the reference Jacobian J based on a cut-off spectrum value of
α0 = α√n∖∕K ∣X∣B ≤ ByfK IlX∣ per Definition 1.1 so as to ensure IlnI(y)h ≥ C ∣∣yh for
some constant c. Assume
43
Under review as a conference paper at ICLR 2020
k≥12×107
Γ4K 4B8IX Fn log(n)
(6.68)
with Γ ≥ 1 and Z ≤ 2. We run gradient descent iterations of the form (1.5) with a learning rate
η ≤ v2b2：x『.Then，after T =55导 iterations，the following identities
If (Wτo) - y∣'2 ≤ IlnN (y)∣'2+e-r IInI (y)∣'2+4ζ√n,
IlWT - W0∣'2 ≤
2 √K (Bαo,r(y) + ΓZ √n
να0
IWτ - W0 I2,∞ ≤
4ΓBK IlX ∣
ναo
Pn
Vk
(6.69)
(6.70)
(6.71)
hold with probability at least 1 - (2K)-100.
Proof To prove this result we wish to apply Theorem 6.20. To do this we need to verify the
assumptions of this theorem. To start with, using Lemma 6.14 with probability at least 1 - (2K)-100,
the initial prediction vector f(W0 ) obeys
IIf(W0)I'2 ≤ ζ∣∣y∣∣'2= ζ√ ≤ 彳.	(6.72)
Hence the initial residual obeys ∣∣r0∣∣'2 ≤ 2√n. Furthermore, using Z ≤ c/2
Iro+ y∣l'2 ≤ζIlyE =⇒ IlnI(ro+ y)ll'2 ≤ ZIlyE∙	(6.73)
Thus,
IInI(ro)E ≥I∏I(y)I'2-I∏I(ro + y)E
≥I∏I (y)I'2-Z 团'2
≥ (c -Z)团'2
c
≥2 IyI'2	(6.74)
c
≥ 4IroI'2.	(6.75)
Thus the assumption on the ratio of information to total energy of residual holds and we can replace c
with 4 in Theorem 6.20. Furthermore, since Bɑ0,γ(∙) is Γ-Lipschitz function of its input vector in '2
norm hence we also have
Bα0,Γ(r0) ≤ Bα0,Γ(y) + ΓIr0 + yI'2 ≤ Bα0,Γ (y) + ΓZIyI'2 .	(6.76)
Next we wish to show that (6.51) holds. In particular we will show that there exists an ε0-
reference Jacobian J for J (W0) satisfying JJT = E[J (W0)J (W0)T]. Note that, such a J
will have exactly same information/nuisance spaces as the square-root of the multiclass kernel matrix
1
i.e.(E[J(Wo)J(Wo)T])2 since these subspaces are governed by the left eigenvectors. Applying
Lemmas 6.13 (with a scaling of the Jacobian by 1/VkK due to the different scaling of V), we find
that if
1000K2B4IXI4 log(n)
k ≥	δ2
(6.77)
then,
δν2
IIJ(WO)J(WO)T- E[J(Wo) J(Wo)T]∣∣ ≤ ~K~.	(6.78)
Let J(W) be obtained by adding max(Kn -p, 0) zero columns to J(W). Then, using (6.78) and
Lemma 6.4, there exists J satisfying JJT = E[J(Wo)J(Wo)T] and
IIJ(Wo) - JIl ≤ 2
K ^k,
44
Under review as a conference paper at ICLR 2020
Therefore, J is an ε0 = 4δK2 reference Jacobian. Now set
/	Bɑo,Γα0
Θ = mm C-----ɪ : ,
I BΓ2∣X Il √nκ
(一)2, Z)
and note that using α = √νκ α0 and ∣ r0 [七 ≤ 2√n
Θ = min
Bα0,Γα0
c---------:--
BΓ2∣X I √nκ
(一[Z)
min Ic TL	( VZB ' )2 Z)
∖ VBΓ2 ∣∣X∣∣√n, ( a ) , Γ)
Bα,Γα
≤ '	(CVBr2EIK IIXI,
Z2ν2B2 IIXI2 Z)
α2
(6.79)
To continue further, note that Ba0,r calculated with respect to Σ(X)1/2 with cutoff a° is ex-
actly same as Ba,r calculated with respect to J with cutoff α = √√κK which is a square-root of
E[J (W0)J (W0)T]. Thus, using (6.79) to ensure (6.51) holds it suffices to show
ε2 = 4Q ≤ aθ = %Θ.
0 K 25 2	50K
Hence, to ensure (6.51) holds we need to ensure that δ obeys
δ≤
2
」Θ.
200
Thus using δ = 2000Θ to ensure (6.51) we need to make sure k is sufficiently large so that (6.77) holds
with this value of δ. Thus it suffices to have
k≥12×107
Γ4K 4B81X Fn log(n)
c4 Z 4ɑ8
(6.80)
≥12 × 107
Γ4K 4B81XI8 log(n)
c4Z 4ɑ8
7	4K2B4Γ4∣∣X∣∣4	1 Γ2∖ K2B41X∣4log(n)
≥4 10'(-c40o —+ ζ4+ 1)	α
≥4 × 107 . (4KBT4JXl4
C α0
≥4 × S. ( 4KBT4JX l4
C α0
+ 1 + Γ2 ) K2B41X]4 log(n)
+	α4	+ Γ2) K2B4∣∣X∣∣4log(n)
+ Z4B4K2 IlXI4 + Z2)	α4
a)4 × 107 /nKB2Γ4 IXI2 + α4	+ ") K2B4∣∣X∣∣4log(n)
≥ I c2Bαo,ra0	Z4b4k2 IXI4 Z2 )	。4
≥4 × 107. maχ("K"4 IXI2	α0	Γ2) K2B4IXI4log(n)
-	∖ c2Bα0,「。2 ,Z4B4K2 IXI4, z2 ) α4
=4 × 107__________K2B4IXI4log(n)______________
α4 min (( ",rao )2 (ZB√K∣∣X∣∣ )4 U)
α0 min(( bγ2∣∣x∣∣√nκ) , ( α°	) , γ2 )
1000K2B4IXI4 log(n)
=	δ2
(6.81)
Here, (a) follows from the fact that I∑(X)1/21 ：= λι ≤ B∣∣XI, equation (6.3), and I∏i(rɔ)h ≥
C IyI'2 = C√n which combined imply
α	ɑ	α0 C IyI'2 _ α0c √n
α0,r ≥ λ InI( o)I'2 ≥ B IXI i I( o)I'2 ≥ "Γ BIX I = "Γ BIX I.
(6.82)
45
Under review as a conference paper at ICLR 2020
To be able to apply Theorem 6.20 we must also ensure (6.52) holds. Therefore, it suffices to have
k≥64× 106
K4B8 IlX∣∣6Γ4nlog(n)
(6.83)
(a)
≥ 25600
K4B6 IX I6 Γ4n
(6.84)
k ≥ 12800 (-α——; + 1)
Z2B2K IXI2
K4B6 IXI6Γ4n
(≥)3200 (Z⅛∣F
≥3200
4Γ2n ∖
+Bar)
Γ2 Ir0I
ν2α80
K4B6 IX I6 Γ2Bα2 ,Γ
≥1600
=1600
------------1------
Z2B2K IXI2	Bα2 ,Γ
ν2α08
I2 ∖ K4B6 ∣XI6 Γ2Bα,r
ν2
α0
γ⅛ 2 K4B6IXI6γ2Ba,γ
Ba,Γ )
ν2
α +［什0必)
ZVB IIXI	Ba,r )
2 ν6B6 IXI6Γ2Bα2,Γ
α8
(6.85)
(
(
Z √Kb ' +
Here, (a) follows from the fact that n ≥ K and the relationship between ζ and ν per (6.88) and (b)
follows from the fact that per equation (6.2) we have
Bα,r ≤ Γ ∣∣r0∣∣'2 ≤ 2Γ√n
Note that (6.80) and (6.84) are implied by
k≥12×107
Γ4K4B8IXI6n log(n)
(6.86)
which is the same as (6.68). What remains is stating the optimization bounds in terms of the labels
y. This follows by substituting (6.72), (6.76), and the fact that ∣ro ［七 ≤ 2√n into (6.55), (6.53), and
(6.54), respectively.	■
6.5.2 Generalization result (completing the proof of Theorem 2.2)
The theorem below is the formal statement and a more general version of Theorem 2.2. The
differences (besides constant terms) are the use of ∣ X ∣ rather than √n (we always have IIX ∣ ≤ √n)
and use of the tighter bound IIJI y 八七 rather than IlnI (y)∣'2 /α0.
Theorem 6.24 Let Z, Γ, α be scalars obeying Z ≤ 1/2, Γ ≥ 1, and α ≥ 0 which determine the overall
precision, cut-off and learning duration, respectively.5 Consider a training data set {(xi, yi)}in=1 ∈
Rd X RK generated i.i.d. according to a distribution D where the input samples have unit Euclidean
norm and the concatenated label vector obeys ∣y∣'2 = √n (e.g. one-hot encoding). Consider a
neural net with k hidden nodes as described in (1.1) parameterized by W where the activation
function φ obeys ∣φ'(z)∣, ∣φ''(z)∣ ≤ B. Let Wo be the initial weight matrix with i.i.d. N(0,1)
entries. Fix a precision level Z and set V = Z/(50B，log(2K)). Also assume the output layer V
has i.i.d. Rademacher entries scaled by √=k. Furthermore, Set J ：= (Σ(X))1/2 and define the
information I and nuisance N spaces and the truncated Jacobian JI associated with the Jacobian
J based on a CUt-OffSPeCtrUm value of α0 = α √n<K IlX ∣ B per Definition 1.1. Assume
Γ4 log n
k N T
(6.87)
5 Note that this theorem and its conclusions hold for any choice of these parameters in the specified range.
46
Under review as a conference paper at ICLR 2020
with Γ ≥ 1. We run gradient descent iterations oftheform (1.5) with a learning rate η ≤ 心温因/∙
Then, after T =.：& iterations, training loss obeys (6.69) and classification error ErrD(Wτ) is
upper bounded by
四誓 + ≡√K (JI 取 + 二 InN Wj+12(1+p⅛ )ζ+5 尸) +2e-r
√n	√n	'2	α0	α 4/ n∣∣X ∣∣2	n
、----Y---Z 、------------Y-------------Z
bias term	variance term
holds with probability at least 1 - (2K)-100 - δ.
The theorem below is a restatement of Theorem 6.24 after substituting the upper bound on the early
stopping distance Dαo,r of Def. (6.1).
Theorem 6.25 (Neural Net - Generalization) Consideratraining data set {(©, yi)}n=ι ∈ Rd×RK
generated i.i.d. according to a distribution D where the input samples have unit Euclidean norm and
the concatenated label vector obeys ∣∣y [尬 = √n (e.g. one-hot encoding). Consider a neural net with
k hidden nodes as described in (1.1) parameterized by W where the activation function φ obeys
∣φ’(z)∣, ∖φff(z)∖ ≤ B. Let W0 be the initial weight matrix with i.i.d. N (0,1) entries. Fix a precision
level ζ ≤ C and set
ζ
V =----- -
50B .log(2K)
(6.88)
Also assume the output layer V has i.i.d. Rademacher entries scaled by -^=. Furthermore, Set
√ kK
J ：= Σ(X)1/2 and define the information I and nuisance N spaces and the truncated Jacobian JI
associated with the Jacobian J based on a cut-off spectrum value of α0 = α √n.K IlX ∣∣B ≤ B IlX ∣∣
per Definition 1.1 ChOSen to ensure IlnI(y)∣)2 ≥ CllylI'2 for some COnStant c > 0. Assume
k ≥ 12 × 107
Γ4K4 B81X I4n2 log(n)
c4Z 4a0
(6.89)
with Γ ≥ 1. We run gradient descent iterations of the form (1.5) with a learning rate η ≤
Then, after T = /袅 iterations, classification error ErrD (WT) is upper bounded by
1
ν2B2∣∣X∣∣2 .
ErrD(Wt) ≤ 2 二⑻. +=叱⑷儿2
√n
12B√K	/ Γ \ 力	/log(2∕δ)
+ —『Dα0 γ + 12(1+—/	)ζ +10∖∕ gJ)
√n	,	、α 4/ n∣X I2	n
holds with probability at least 1 - (2K)-100 - δ.
Proof Under the stated assumptions, Theorem 6.23 holds with probability 1 - (2K)-100. The proof
will condition on outcomes of the Theorem 6.23. Specifically, we shall apply (6.43) of Lemma
6.18 with Mw and R dictated by Theorem 6.23 where the output layer V is fixed. Observe that
IIVIIF = √Kk∣∣V葭=V, we have
ErrD(Wt) ≤ 2 [≡^ + ^(Mn + X) + L
Theorem 6.23 yields
f(Wτ)- y∣∣'2 ≤ I∏n(y)∣∣'2 + £⅛(⅜ + 4ζ
√n ≤	√n	Z
Using (6.70) for MW
VBMW	2B√KDa0,r(y)	2B√K Γζ
√n -	√n	劭 ^
(6.90)
(6.91)
(6.92)
47
Under review as a conference paper at ICLR 2020
Using (6.71) on row bound R and lower bound on k
R 48nΓ2B3K2 IX∣∣2
√k	νa0√k
≤ CQ2
一 230VB log(n)
≤ζ.
(6.93)
Plugging in (6.91), (6.92), and (6.93) into (6.90) concludes the proof.
C clusters with the cluster centers given by {μ? g}(KC)(i 1)
A	The Jacobian of the Mixture Model is low-rank
(Proofs for Section 2.3)
The following theorem considers a simple noiseless mixture model and proves that its Jacobian
is low-rank and the concatenated multiclass label vectors lie on a rank K2C information space
associated with this Jacobian.
Theorem A.1 Consider a data set of size n consisting of input/label pairs {(xi, yi)}in=1 ∈ Rd × RK
generated according to the Gaussian mixture model of Definition 2.4 with K classes each consisting of
and σ = 0. Let Σ(X) be the multiclass
neural tangent kernel matrix associated with input matrix X = [x1 . . . xn]T with the standard
deviation ofthe output layer set to V = √. Also define the information space I to be the range space
of Σ(X). Also let M = [μ1,1 ... μκ,C]T be the matrix obtained by aggregating all the cluster
centers as rows and let g be a Gaussian random vector with distribution N(0, Id). Define the neural
tangent kernel matrix associated with the cluster centers as
Σ(M) = (MMT) Θ Eg~N(0,id)[φ'(Mg)φ'(Mg)T] ∈ rkc×kc,
and assume that Σ(M) is full rank. Then, the following properties hold with probability 1 一
KC eχp(-8κnc)
•	I is a K2C dimensional subspace.
•	The concatenated label vector y = [y1T y2T . . . ynT]T lies on I.
•	The nonzero eigenvalues (top K2C eigenvalues) of Σ(X) are between 2KcSmin(∑(X))
and Kkc ∣Σ(X )∣. Hence the eigenvalues ofthe information space grow with KC.
Proof First, we establish that each cluster has around the same size. Applying Chernoff bound and a
union bound, we find that with probability 1 - KC eχp(-8Knc)
0.5n ≤ n`,e ≤ 2n.
Note that based on Lemma 6.11, the multiclass covariance is given by
Σ(X) = kν2Iκ © Σ(X).
where Σ(X) = (XXT) Θ Eg/.i.d.N≡(0 1) [φ'(Xg)φ'(Xg)T]. Due to this Kronecker product represen-
tation, the range space of Σ(X) is separable. In particular, note that with
I = Range (Σ(X))
We have I = IK ® I Which also implies rank(I) = K ∙ rank (I). Hence, this identity allows us to
reduce the problem to a single output network. To complete the proof we will prove the following
three identities:
•	I has rank KC.
48
Under review as a conference paper at ICLR 2020
•	The nonzero eigenvalues of Σ (X) are between 0.5 为 Smin(Σ(M)) to 2 川 Σ(M )∣∣.
•	The portion of the label vector associated with class ' i.e. y(') ∈ Rn (see (5.4)) lies on I.
Hence, the concatenated vector y lies on I = IK ⑥ I.
To prove these statements let J'(X; Wo) and J'(M; Wo) be the Jacobian associated with the
`th output of the neural net (see (5.4)) for data matrices X and M. Observe that the columns of
J'(X; Wo) are chosen from J'(M; Wo) and in particular each column of J'(M; Wo) is repeated
between 0.5有 to 2n times. To mathematically relate this, define the KC dimensional subspace S of
Rn where for any v ∈ S, entries vi and vj of v are equal iff data point xi and xj are equal (i.e. belong
to the same class/cluster pair). Now, we define the orthonormal matrix US ∈ Rn×KC as the 0-1 matrix
with orthogonal rows that map RKC to S as follows. Assume the ith data point xi belongs to the
class/cluster pair ('i,'i). We then set the ith row of US as vect(e&eT ). Using US we have
US J'(M; Wo) = J'(X; Wo).
Now note that using the above identity we have
US ∑(M )UT = ∑(X).
Since US is tall and orthogonal, the range of Σ(X) is exactly the range of US hence I = S which
is KC dimensional. Furthermore, nonzero eigenvectors of Σ(X) lie on S and any eigenvector v
satisfies
VTΣ(X )v ≥ Smin(US )2Smin(Σ(M)) ≥ 0.5nSmin(Σ(M))
and similarly
VTΣ(X)v ≤ 2nllΣ(M)∣∣
which follows from the fact that '2-norm-squared of columns of U are between 0.5n to 2n. Finally,
we will argue that label vector y(') lies on S. Note that for all samples i that belong to the same
cluster y(') will be the same (either zero or one), thus y(') ∈ S.	■
Next lemma provides a perturbation analysis when there is noise.
Lemma A.2 Consider the single-output NTK kernel given by
Σ(X) = E [φ'(Xw)φ'(Xw)τ] Θ (XXT),
and assume that this matrix has rank r so that λr+ι (∑(X)) = λr+2 (∑(X)) = ... = λn (∑(X ))=
0. Also assume a noise corrupted version of X given by
√dZ
X = X +
with Z a matrix consisting ofi.i.d. N(0,1) entries. ThenJΣ(X) - Σ(X)∣∣ W △ where
∆ ：= σB2 logn IlXI2 + σ1B1(n∕d + 1) + √lθgn ∙σB2 ∣∣X∣2 + σB1√n∕d + 1 IlX∣	(A.1)
holds with probability at least 1 一 2ne- 2. Whenever σ ≤ √0gn, △ is upper bounded as
△ W B2σ√log n.	(A.2)
n
Furthermore, let V, V ∈ Rn×r be orthonormal matrices corresponding to the top r eigenvalues of
Σ(X) and Σ(X). Then,
IlVVT- VVT Il ≤
△
λr (Σ(X)) - △
Proof Note that
diag (φ' (XW)) X 一 diag (φ' (Xw)) X =diag (φ' (XW)) X 一 diag (φ' (Xw)) X
=diag (φ' (Xw) - φ' (Xw)) X
+ diag (φ' (Xw))(X - X)
49
Under review as a conference paper at ICLR 2020
Now define M = diag (φ'(XW)) X and M = diag (φ'(Xw)) X and note that using the above We
can conclude that
IlM- MIl ≤ Ildiag (φ' (XW)- φ' (Xw)) X∣∣
+ IIdiag (φ'(XW))(X - X)1
≤B Il(X - X)w∣'JXI + B IIX - XI
Now using the fact that
IMMMT - mmTI ≤ ∣∣M - M∣∣2 + 2 ∣∣M1 - M"m∣∣,
we conclude that
∣∣Σ(X) - Σ(X )I = U E [Md Mt - MM T]Q
2
≤e[(b II(X-X)w∣L' + BIIX-XI)]
+ 2BIIXII (B IlX II E[I(X - X )w∣L ] + B IlX - X I)
≤2B2 ∣∣X∣∣2 E[∣∣(X - X)wI2∞ ] + 2B2IX - X『
+ 2B2 ∣∣X∣∣2 E[I(X - X )wI'∞ ] + 2B2IX - XIllX I
To proceed further, with probability 1 - n exp(-d∕2), each row of X - X is upper bounded by 2σ.
Hence, using a standard tail bound over supremum of n Gaussian random variables (which follows
by union bounding) we have
E[I(X - X)wI2∞]1/2 ≤ 2σ√2logn
holds with the same probability. Furthermore, spectral norm bound on Gaussian random matrix
implies that
IX - XI2 ≤ (2(√n + √d))2 彳 ≤ 8(n∕d+1)σ2,
holds with probability at least 1 - e-2 (n+d). Plugging these two probabilistic bounds into the chain
of inequalities we conclude that
I∑(X) - Σ(X)I W σ2B2 logn IlX∣2 + σ2B2(n∕d + 1) + √lθgn ∙ σB2 ∣X∣2 + σB2√n∕d + 1 IlX∣
To establish (A.2), observe that B2σ√lognIlX∣∣2 dominates over other terms in the regime σ√logn
is small. The final bound is a standard application of Davis-Kahan Theorem Yu et al. (2014) when
we use the fact that Σ(X) is low-rank.	■
The following lemma plugs in the critical quantities of Theorem 2.2 for our mixture model to obtain
a generalization bound.
Theorem A.3 (Generalization for Mixture Model) Consider a dataset {xi, yi}in=1 generated
i.i.d. from the Gaussian mixture model in Definition 2.4. Let λM = λmin(Σ(M)) where M ∈ RKC×d
is the matrix of cluster centers. Suppose input noise level σ obeys
λmin
σ W --------
B2KC √lθgn
Consider the setup of Theorem 2.2 with quantities ζ and Γ. Suppose network width obeys
Γ4B8K8C4 log n
N-ζ4λmin-.
With probability 1 - ne-d/2 - KC exp(- ^^) - (2K )-100 - δ, running gradient descent for T =
2ΓK 2C	1
ην2nλ i with learning rate η ≤ ”262]阳产,we have that
ErrD(Wt) W IkB2KC 十 ΓBK√C 十眩十 √°⅛ + 2产.
λmin	nλmin	n
50
Under review as a conference paper at ICLR 2020
Proof The proof is an application of Lemma A.2 and Theorem A.1. Let I' be the information space
corresponding to noiseless dataset where input samples are identical to cluster centers. Let P', P
correspond to the projection matrices to I and I'. First, using Lemma A.2 and the bound on σ, we
have
IlP-Pll ≤ cσ^ogiBKC
λmin
for some constant C > 0. Next we quantify ∏i(y) using the fact that (i) ∏ι' (y) = y via Theorem A.1
as follows
I∏I(y)l'2 ≥ I∏I‛(y)∣κ - ∣∏I(y) -∏ι‛(y)L ≥ √n(i- J坐nB2KC).	(a.3)
λmin
In return, this implies that
I∏n(y)l'2 A
nσ √log nB 2KC
λmin
∖
To proceed, we pick α0 = ʌ/λmnn and corresponding α = √n√K0⑶B ≥ ∖∕2B⅛c and apply (??)
to find that, classification error is upper bounded by
ErrD (Wt )A∖ ° kB2KC 十 ΓBK^ 十吨十 5 A0g≡ + 23.
λmin	nλmin	n
B Joint input-output optimization
In this section we wish to provide the ingredients necessary to prove a result for the case where both
set of input and output weights W and V are trained. To this aim, we consider the combined neural
net Jacobian associated with input and output layers given by
X ↦ f(x; V, W) ：= Vφ(Wx).	(B.1)
Denoting the Jacobian associated with (B.1) by J (V, W) we have that
J(V,W) = [J (V) J(W)] ∈ RKn×k(K+d)
Here, J (W) is as before whereas J (V) is the Jacobian with respect to V and is given by
J(V)=[J(v1)J(v2) ... J(vK)].	(B.2)
where J(v`) ∈ RKn×k is so that its ''th block rows of size n X k is nonzero for 1 ≤ ' ≤ K i.e.
'th block row of J(v`)
{. .
0 if ' ≠ '
φ(XWT) else
Hence, J(V) is K X K block diagonal with blocks equal to φ(XWT). The following theorem
summarizes the properties of the joint Jacobian.
Theorem B.1 (Properties of the Combined Input/Output Jacobian) J(V, W) satisfies the fol-
lowing properties.
•	Upper bound： IIJ (V, W )∣ ≤ BllX NW IlF + √Kk ∣∣V∣∣'∞).
•	Row-bound: For unit length U: ∣ mat (J T (W )u)∣∣2,∞ ≤ B√K ∣∣V∣∣'∞∣∣X ∣.
•	Entry-bound: For unit length U: I mat (J T (V )u)l'∞ ≤ B∣W ∣2,∞∣X I
•	Lipschitzness: Given inputs V, V′ and outputs W, W'
Il J(V, W )-J (V', W ')l ≤ B IlXll (√Kk IlV - V '∣'∞+√K IV ∣'∞ IW ′ - W IlF +I W - W '∣F).
51
Under review as a conference paper at ICLR 2020
Proof First, we prove results concerning J(V). First, note that
IJ(V)I ≤Iφ(XWT)I ≤BIXIIWIF.
Next, note that for u = [u1 . . . uK] ∈ RKn we have
J T (V )ub∞ = ImaK M(WX T )u⅛∞
=max ∣φ(wsXT)u`l
1≤s≤k
=BIWI2,∞IXI.
Let J1, J2 be the Jacobian matrices restricted to V and W of J(V, W). To prove Lipschitzness,
first observe that
J(V, W)- J(V′, W')∣ ≤ Ji(v, W) - J1(V', W')∣ + ∣J2(V, W)- J2(V', W')∣.
Next, observe that
∣J1(V, W) - J1(V', W ')∣ ≤ M(XWT) - φ(XW 'T )∣∣ ≤ BIX 川 W - WIF.
We decompose J2 via
∣J2(V, W) - J2(V', W')∣ ≤ ∣J2(V, W) - J2(V, W')∣ + ∣J2(V, W') - J2(V', W')∣
≤ B√K ∣V∣'∞∣X 11W' - W IlF + ∣J2(V, W') - J2(V', W')∣.
To address the second term, note that, Jacobian is linear with respect to output layer hence
∣J2(V, W') - J2(V', W ')∣ = ∣J2(V - V', W' )∣ ≤ B√Kk IlV - V 1'∞∣X∣∣ .
Combining the latter two identities we arrive at
J2(V, W) - J2(V′, W. ≤ B ` (√Kk IlV - Vl`
completing the proof.
+ √K M∣'JW'-W IlF),
∞
C Further numerical experiments
We depict the approximate eigenstructure in Section C.1. Complementary to Section 4, in Appendix
C.2 we provide the complete set of experiments we performed on the original 10-class CIFAR-10
dataset covering various levels of label corruption. In Appendix C.3 we show numerical results on a
sub-sampled 3-class version of CIFAR-10. Moreover, we demonstrate that our theory holds across
different datasets by providing experimental results on MNIST in Appendix C.4.
C.1 Experiments on full (10-class) CIFAR- 1 0
As mentioned earlier while calculating all the eigenvalues is not possible, we also verify the bimodal
structure of the Jacobian using the entire spectrum by approximating its spectral density using the
method recently described in Ghorbani et al. (2019a) and Papyan (2019a) for Hessian eigenvalue
density estimation. We detail the parameters of the algorithm in Appendix C.5. Figure 9 depicts the
estimated spectrum before and after training. We observe a similar bimodal spectrum with a few
outliers. Moreover, in both depictions of the Jacobian eigenstructure we observe increasing separation
between the low and high end of the spectrum after training.
C.2 Experiments on full (10-class) CIFAR- 1 0
In addition to what has been described in Section 4 here we disclose the complete set of experiments
performed on the CIFAR-10 dataset. We trained the modified ResNet20 model described in Section 4
with SGD as longs as it was necessary to achieve a good fit to the training data. Information subspace
52
Under review as a conference paper at ICLR 2020
ytisned delac
105
104
103
102
101
100
10-1
10-2
r \ - 1
三r	∖	1
■4 ■ ； ■：■ r；i
装竣遥/镣推案港港返奈遵1漫。♦运至榭之
∖^-∖ ∖

...-
藜
MlIll IllIIL
10-1	100
101
102
Singular value
(a)	Scaled spectral density of initial train Jacobian
ytisned delacS
5311
0 0 0 -
111
100
101
102
103
Singular values
(b)	Scaled spectral density of final train Jacobian
Figure 9: Scaled spectral densities of the full train Jacobian at different stages of
training
	InI (y)∣∣'2 怙必	InN (y)l'2 怙必	JI Mh 怙必	InI （，0儿2 什。必	InN （，0儿2 什。必	JI M2 什。必
Ttrain *- Jinit	0.38081	0.92465	0.027224	0.37114	0.92858	0.027293
Ttrain *- J fin?l	0.9869	0.16131	0.00070893	0.98669	0.1626	0.00070354
τtest Jinit	0.38184	0.92423	0060229	0.37227	0.92812	0.06037
τtest J final	0.80926	0.58746	0.0013734	0.80912	0.58764	0.0013716
Table 3: Depiction of the alignment of the initial label/residual with the informa-
tion/nuisance space using uncorrupted CIFAR-10 data.
is spanned by the top 50 singular vectors. We marked figures and table entries also included in
Section 4 by asterisk.
Experiments without label corruption. We trained the modified ResNet20 model described in
Section 4 with SGD for 400 epochs with learning rate 0.1 on the original dataset without any form
of data augmentation. The network output after the last layer has been scaled by s = 0.025 in all
experiments to follow.
Experiments with label corruption. We corrupt the labels in the training data by switching each
label to a strictly different (incorrect) class. We train the network on the corrupted dataset for 800
epochs with initial step size 0.1 decayed to 0.01 after 760 epochs.
	InI (y"'2	InN (y)l'2	JI Mg2	InI (，0儿2	InN (r0)∣∣'2	JI M2
	怙必	怙必	怙必	什。必	什0必	什。必
Ttrain- Jinit	0.35328	0.93552	0021592	0.36954	0.92921	~0.021568
Ttrain- J final	0.92214	0.38685	000087246	0.92324	0.38423	0.00087304
Table 4: Depiction of the alignment of the initial label/residual with the informa-
tion/nuisance space using 25% label corruption on CIFAR-10 data.
53
Under review as a conference paper at ICLR 2020
一二一 一 一二 一 一二Ξ 一 一 一I≡^Ξ 一 一 一 一ΞΞ 一 一 一=三二一 一三一
012345
0 - - - - -
100000
11111
ygrene laudiseR
0	100	200	300	400
Epochs
(a) Residual projection on initial train Jacobian
100
123
---
10 10
ygrene laudiseR
0	100	200	300	400
Epochs
(b) Residual projection on final train Jacobian
100
-1 -2
-10 -
ygrene laudiseR
0	100	200	300	400
Epochs
(c)	Residual projection on initial test Jacobian
0	100	200	300	400
Epochs
(d)	Residual projection on final test JaCObian*
400
300
200
Oo
O
86420
......................
0000
rorrE
Epochs
*
(e) Training and test error of experiment*
Figure 10:	Experiments on the original, uncorrupted CIFAR-10 dataset. *: We discuss
these plots in Section 4.
	InI (y"'2 怙必	InN (y)l'2 怙必	JI Mh 怙必	InI （，0）必 什。必	InN (r0)∣∣'2 什0 L	JI M2 什。必
Ttrain *- Jinit	0.32762	0.94481	0.017556	0.32152	0.9469	0.017521
Ttrain *- J fin?l	0.8956	0.44487	0.00096413	0.89597	0.44412	0.00096652
τtest Jinit	0.38013	0.92493	0.080777	0.37454	0.92721	-0.080766
τtest J dip .		0.7041	-07W	0.0040147	0.70229	0.71189	0.0040423
τtest J final	0.44774	0.89416	0.0012216	0.44409	0.89598	0.0012157
Table 5: Depiction of the alignment of the initial label/residual With the informa-
tion/nuisance space using 50% label corruption on CIFAR-10 data.
54
Under review as a conference paper at ICLR 2020
0123456
0 - - - - - -
1000000
111111
ygrene laudiseR
012345
0- - - - -
10000
1111
ygrene laudiseR
0	200	400	600	800
Epochs
(a)	Residual projection on initial train Jacobian
1
10-6
0	200	400	600	800
Epochs
(b)	Residual projection on final train Jacobian
864
...
000
rorrE
Training error
Test error
0.2
0
0	200	400	600	800
Epochs
(c) Training and test error of experiment
Figure 11:	Experiments with 25% label corruption on
CIFAR-10 data
	InI (y"'2	InN (y)l'2	JI Mg2	InI (r0)∣∣g2	InN (r0)∣∣'2	JI M2
	怙必	怙必	怙必	什。必	什0必	什。必
Ttrain Jinit.	0.31756	0.94824	0.0056031	0.325	0.94571	0.005592
Ttrain- J final	0.50238	0.86465	000047518	0.50718	0.86184	0.00047967
Table 6: Depiction of the alignment of the initial label/residual with the informa-
tion/nuisance space using 75% label corruption on CIFAR-10 data.
55
Under review as a conference paper at ICLR 2020
0123456
0 - - - - - -
1000000
111111
ygrene laudiseR
012345
0 - - - - -
10000
1111
ygrene laudiseR
0
200	400	600	800
Epochs
0	200	400	600	800
Epochs
(a) Residual projection on initial train Jacobian
012345
0- - - - -
100000
11111
ygrene laudiseR
(b) Residual projection on train Jacobian at 50 epochs
(dip)*
100
1
-10
ygrene laudiseR
ygrene laudiseR
10-6	■
Cl	I	I	I	I	≡l
0	200	400	600	800
Epochs
(c) Residual projection on final train Jacobian
100
10-2
0	200	400	600	800
Epochs
(d)	Residual projection on initial test Jacobian
100
-1 -2
-10 -
ygrene laudiseR
0	200	400	600
Epochs
(e)	Residual projection on test Jacobian
(dip)*
800
at 50 epochs
0	200	400	600	800
Epochs
(f)	Residual projection on final test Jacobian
8642
........................
0000
rorrE
(g) Training and test error of experiment*
Figure 12:	Experiments with 50% label corruption on CIFAR-10 data. *: We discuss
these plots in Section 4.
56
Under review as a conference paper at ICLR 2020
01234
0 - - - -
1 10 10 10
ygrene laudiseR
l∏I (rτ)∣2
-lr0%
■	n∏N (rτ)唱
-lr0%
0	200	400	600	800
Epochs
(a)	Residual projection on initial train Jacobian
0	200	400	600	800
Epochs
(b)	Residual projection on final train Jacobian
0.8
0.6
Traming error
Test error
42
..
00
rorrE

CIFAR-10 data
0	200	400	600	800
Epochs
(c) Training and test error of experiment
Figure 13: Experiments with 75% label corruption on
100
1234
---------------
10 10 10
ygrene laudiseR
0	200	400	600	800
Epochs
100
123
---
10 10 10
ygrene laudiseR
0	200	400	600	800
Epochs
(a)	Residual projection on initial train Jacobian
(b)	Residual projection on final train Jacobian
(c) Training and test error of experiment
Figure 14: Experiments with 100% label corruption on CIFAR-10 data
57
Under review as a conference paper at ICLR 2020
	InI⑹必	InN (y)l'2	JI Mg2	InI (，0儿2	InN (r0)∣∣'2	JI M2
	怙必	怙必	怙必	什。必	什0必	什。必
Ttrain- Jinit	0.31581	0.94882	0.0094479	0.31854	0.94791	0.0094092
Ttrain- J final	0.47747	0.87865	000045241	0.47921	0.8777	0.00045344
Table 7: Depiction of the alignment of the initial label/residual with the informa-
tion/nuisance space using 100% label corruption on CIFAR-10 data.
Figure 15: Test error vs. final projection of labels on nuisance subspace for CIFAR-10
experiments. See discussion of this plot in Section 4
C.3 Experiments on subsampled 3-class CIFAR- 1 0
We created a new dataset of 3 classes by sub-sampling the original CIFAR-10 dataset. To do this, we
discarded all training and test data except those belonging to classes 0 (airplane), 1 (automobile) and
2 (bird) and sampled 3333 examples of each classes for a total of 9999 training images. We trained
the neural network model described in Section 4 with no output scaling using SGD and Adam. We
applied standard data augmentation (random crop and flip) to increase generalization. Information
subspace is spanned by the top 50 singular vectors.
Experiments without label corruption. First we trained the network on the sub-sampled 3-class
dataset keeping the original labels. For the SGD experiments, we used initial learning rate 0.01
decreased by a factor of 10 at epochs 260 and 360 for a total of 400 epochs with batch size 128. For
the Adam experiment learning rate 0.01 for 400 epochs was sufficient for a good fit to the training
data. We observed better Jacobian adaptation using Adam compared to SGD on this dataset (0.98743
from Table 8 vs. 0.9969 from Table 9).
Experiments with label corruption. We corrupt the training labels by the corruption model de-
scribed in C.2. We train the network on the corrupted dataset for 800 epochs with initial step size
0.01 and batch size 128. We decrease the learning rate by a factor of 10 at the following epochs: at
500 epochs for 25% corruption, at 700 epochs for 50% and 75% corruption and at 500 and 700 for
100% corruption.
58
Under review as a conference paper at ICLR 2020
01234
0 - - - -
1 10 10 10 10
ygrene laudiseR
0	100	200	300	400
Epochs
(a) Residual projection on initial train Jacobian
0	100	200	300	400
Epochs
(b) Residual projection on 200 epochs train Jacobian
0123
0 - - -
1 10 10 10
ygrene laudiseR
0	100	200	300	400
Epochs
(c) Residual projection on final train Jacobian
01
0-
10
1
ygrene laudiseR
0	100	200	300	400
Epochs
(d) Residual projection on initial test Jacobian
6420
...
000
rorrE
0	100	200	300	400
Epochs
(e)	Residual projection on final test Jacobian
0	100	200	300	400
Epochs
(f)	Training and test error of experiment
Figure 16: Experiments on 3-class uncorrupted CIFAR-10 dataset
	InI (y 儿 2 回`2	InN (y 儿 2 回`2	JI Mlg2 怙心	InI （，0 儿 2 什。北2	InN (r0 儿2 什。L2	JI恤 什。必
Ttrain Jinit		0.7239	0.68991	0.0054426	0.88552	0.4646	0.0040958
Ttrain J 200epochs	0.97266	0.23224	0.0026234	0.96849	0.24905	0.0030069
τtrain J fin?l		0.98743	0.15804	0.0031639	0.97606	0.21749	0.0034312
Ttest Jinit		0.73366	0.67951	0010328	0.88827	045932	0.0077364
τtest J final		0.8974	0.44123	00027082	0.89772	044057	0.0029383
Table 8: Depiction of the alignment of the initial label/residual with the informa-
tion/nuisance space using 3-class uncorrupted CIFAR-10 data.)
59
Under review as a conference paper at ICLR 2020
10-5
01234
0 - - - -
1 10 10 10
ygrene laudiseR
(a) Residual projection on initial train Jacobian
0	100	200	300	400
Epochs
10-4
0123
0 - - -
1 10 10 10
ygrene laudiseR
(b) Residual projection on final train Jacobian
0	100	200	300	400
Epochs
Figure 17: Experiments on 3-class uncorrupted CIFAR-10 data, trained with Adam
	InI (y"'2 怙必	InN (y"g2 怙必	JI Mh 忖必	InI （，0儿2 什。必	InN (r0)∣∣'2 什。必	JI川2 什。1'2
Ttrain Jinit.	0.7025	0.71169	0.0053554	0.8135	0.5815	0.0044353
Ttrain- Jfinal	0.9969	0078332	00030954	0.9907	0.1361	0.0030632
Table 9: Depiction of the alignment of the initial label/residual with the informa-
tion/nuisance space on 3-class uncorrupted CIFAR-10 data, trained with Adam.
0123
0 - - -
1 10 10 10
ygrene laudiseR
0	200	400	600	800
Epochs
(a) Residual projection on initial train Jacobian
Figure 18: Experiments with 25% label corruption on 3-class CIFAR-10 data
0123
0 - - -
1 10 10
ygrene laudiseR
10-4
0	200	400	600	800
Epochs
(b) Residual projection on final train Jacobian
	InI (y)∣∣'2 怙必	InN (y"g2 怙必	JI引1	InI (r0)∣∣3 什。I'2	InN (r0)∣∣'2 什。I'2	JI Tlg2 什。必
τtrain- Jinit	0.63965	0.76866	0.0047649	0.76602	0.64282	0.0043665
丁 train- Jfinal	0.90862	0.41763	00023974	0.9254	0.379	0.0021731
Table 10: Depiction of the alignment of the initial label/residual with the informa-
tion/nuisance space using 25% label corruption on 3-class CIFAR-10 data.
60
Under review as a conference paper at ICLR 2020
ygrene laudiseR
10-4
0
0123
0- - -
1 10 10
ygrene laudiseR
200	400	600	800
Epochs
0	200	400	600	800
Epochs
(a)	Residual projection on initial train Jacobian
(b)	Residual projection on train Jacobian at 100 epochs
(dip)
0123
0 - - -
1 10 10
ygrene laudiseR
01
0-
10
ygrene laudiseR
0	200	400	600	800
Epochs
(c)	Residual projection on final train Jacobian
0	200	400	600	800
Epochs
(d)	Residual projection on initial test Jacobian
------- i - 一 一
01
0-
10
ygrene laudiseR
0	200	400	600	800
Epochs
01
0-
10
ygrene laudiseR
0	200	400	600	800
Epochs
(e) Residual projection on test Jacobian at 100 epochs
(dip)
(f) Residual projection on
final test Jacobian
0.6
T Training error
■ Test error
rorrE
Epochs
(g) Training and test error of experiment
Figure 19:	Experiments with 50% label corruption on 3-class CIFAR-10 data
61
Under review as a conference paper at ICLR 2020
	InI (y)∣∣'2 怙必	InN (y"'2 怙必	JI Mh 忖必	InI (r0)∣∣'2 什。必	InN (r0)∣∣'2 什。必	JI川2 什。1'2
Ttrain Jinit	0.58664	0.80985	0.0017197	0.64281	0.76602	0.0019814
丁 train- Jdip	0.61125	0.79144	0.0026809	0.65671	0.75415	0.0031267
T train Jfinal	0.75143	0.65981	0.0018702	0.76311	0.64627	0.0012039
τtest J i"it.	0.7236	0.69021	0.012765	-07594-	0.65062	-0.01235
(test J dip .	0.81473	0.57984	0009931	-0.8322-	0.55447	0.010515
τtest Jfinal	0.75362	0.65731	00033476	0.77225	0.63532	0.0021479
Table 11: Depiction of the alignment of the initial label/residual with the informa-
tion/nuisance space using 50% label corruption on 3-class CIFAR-10 data.
100
------- -- -Ξ二二-E^Ξ --- -
1234
----------------------
10 10 10 10
ygrene laudiseR
Epochs
100
600
400
200
O
123
-10 -10 -10
ygrene laudiseR
Epochs
(a)	Residual projection on initial train Jacobian
(b)	Residual projection on final train Jacobian
Figure 20:	Experiments with 75% label corruption on 3-class CIFAR-10 data
	InI (y)l'2	InN (y 儿 2	JI Mg2	InI (r0)∣∣g2	InN (r0)∣∣'2	JI
	忖必	忖必	怙必	什。I'2	什0必	什。1'2
Ttrain Jinit	0.58032	0.81439	0.0014306	0.68898	0.72478	0.0015956
Ttrain- Jfinal	0.64704	0.76246	0.0015808	0.78391	0.62088	0.00091715
Table 12: Depiction of the alignment of the initial label/residual with the informa-
tion/nuisance space using 75% label corruption on 3-class CIFAR-10 data.
62
Under review as a conference paper at ICLR 2020
100
O
80
600
400
200
O
--Ξ 二--E^Ξ 二-E^Ξ - - - --E-
1234
-10 -10 -10 -10
ygrene laudiseR
100
600
400
200
O
- - -
10 10 10
ygrene laudiseR
Epochs
Epochs
(a) Residual projection on initial train Jacobian	(b) Residual projection on final train Jacobian
Figure 21: Experiments with 100% label corruption on 3-class CIFAR-10 data
	InI (y)l'2	InN (y 儿 2	JI y∣h	InI (r0)∣∣'2	InN (r0)∣∣'2	JI，1'2
	忖必	忖必	怙必	什。I'2	什。必	什。1'2
T train- Jinit	0.614	0.78931	0.0024274	0.78608	0.61813	0.0022444
Ttrain- Jfinal	0.60094	0.7993	0001845	0.7657	0.6432	0.00098981
Table 13: Depiction of the alignment of the initial label/residual with the informa-
tion/nuisance space using 100% label corruption on 3-class CIFAR-10 data.
C.4 Experiments on MNIST
The MNIST dataset contains handwritten digits in 10 classes divided into 60k training and 10k test
images. To demonstrate our theoretical findings on a dataset different from CIFAR-10 we repeat all
experiments on MNIST under various levels of label corruption. We run SGD with batch size 128 on
least-squares loss using the modified ResNet20 model from Section 4. In all of the following MNIST
experiments the model output after the last layer has been scaled by 0.1. Information subspace is
spanned by the top 50 singular vectors
Experiments without label corruption. We train the network on the original dataset for 100 epochs
with initial step size 0.1 decayed to 0.01 after 60 epochs.
Experiments with label corruption. We corrupt various portions of the labels in the training data
by switching the labels to a random incorrect class. We train the network on the corrupted dataset for
400 epochs with initial step size 0.1 decayed to 0.01 after 360 epochs.
	InI (y)∣∣'2 怙必	InN (y)l'2 怙必	JI Mg2 怙必	InI (r0)∣∣'2 什。L	InN (r0)∣∣'2 什0 L	JI M2 什。必
Ttrain- Jinit	0.43598	0.89996	0.028665	0.48168	0.87635	0.027921
Ttrain- J final	0.9946	0.10375	0.00066394	0.99321	0.11633	0.00076602
Ttest Jinit	0.43964	0.89817	0071577	0.48474	0.87466	-0.069724
τtest J final	0.99188	0.12716	0.0016387	0.99044	0.13796	0.0018745
Table 14: Depiction of the alignment of the initial label/residual with the informa-
tion/nuisance space using uncorrupted MNIST data.
63
Under review as a conference paper at ICLR 2020
01234
0 - - - -
1 10 10 10 10
ygrene laudiseR
0	20	40	60	80	100
Epochs
(a)	Residual projection on initial train Jacobian
----- -二- -=Ξ - - - 一Ξ Ξ - - h-Ξ - - - - = Ξ 二- -----
012345
0 - - - - -
100000
11111
ygrene laudiseR
0	20	40	60	80	100
Epochs
(b)	Residual projection on final train Jacobian
100
123
---
10 10
ygrene laudiseR
0	20	40	60	80	100
Epochs
0	20	40	60	80	100
Epochs
(c)	Residual projection on initial test Jacobian
I Γ
100 - ■
(d)	Residual projection on final test Jacobian
Training error
Test error
0	20	40	60	80	100
Epochs
(e) Training and test error of experiment
Figure 22:	Experiments on uncorrupted MNIST data
64
Under review as a conference paper at ICLR 2020
ygrene laudiseR
100
1
-0
0	100	200	300	400
Epochs
100
ygrene laudiseR
12
-0 -0
0	100	200	300	400
Epochs
(b) Residual projection on final train Jacobian
(a) Residual projection on initial train Jacobian
rorrE
86420
........................
0000
00
Training error
Test error
200	300	400
Epochs
(c) Training and test error of experiment
Figure 23:	Experiments with 25% label corruption on MNIST data
	InI (y"'2	InN (y"g2	JI Mh	InI (r0)∣∣'2	InN (r0)∣∣'2	JI恤
	怙心	怙心	忖必	什。I'2	什。I'2	lr0l'2
τtrain- Jinit	0.38906	0.92121	0.02197	0.38586	0.92256	0.022028
Ttrain- J final	0.94093	0.33861	00010507	-0.9391	0.34365	0.0010331
Table 15: Depiction of the alignment of the initial label/residual with the informa-
tion/nuisance space using 25% label corruption on MNIST data.
	InI (y)∣∣'2 怙心	InN (y)l'2 怙心	JI 训'2 怙心	InI (r0)∣∣'2 什。I'2	InN (W* 什。I'?	JI HR 什0 I'2
Ttrain- Jinit	0.34434	0.93885	0.013931	0.35864	0.93347	0.013967
Ttrain J fin?l	0.83136	0.55573	0.00081502	0.83235	0.55425	0.00081616
τtest Jinit	0.44458	0.89574	0.076224	0.45366	0.89117	-0.076101
τtest J dip .	0.97007	0.24284	0.0043166	-0.9705	0.24109	0.0043041
τtest J final	0.71536	0.69876	0.0018554	0.71632	0.69777	0.0018564
Table 16: Depiction of the alignment of the initial label/residual with the informa-
tion/nuisance space using 50% label corruption on MNIST data.
65
Under review as a conference paper at ICLR 2020
01234
0 - - - -
1 10 10 10
ygrene laudiseR
hΞ 二_- ____ 二__ _ h_Ξ 二- hΞ
0123
0 - - -
1 10 10 10
ygrene laudiseR
0	100	200	300	400
Epochs
(a)	Residual projection on initial train Jacobian
100
1
-10
ygrene laudise
0	100	200	300	400
Epochs
(b)	Residual projection on final train Jacobian
100
ygrene laudiseR
1
-10
0	100	200	300	400
Epochs
(c) Residual projection on initial test Jacobian
0	100	200	300	400
Epochs
(d) Residual projection on test Jacobian at 50 epochs
(dip)
8642
.......................
0000
rorrE
ygrene laudise
Figure 24: Experiments with 50% label corruption on MNIST data
0	100	200	300	400
Epochs
(f) Training and test error of experiment
	InI⑹必	InN (y)l'2	JI Mg2	InI (，0儿2	InN (r0)∣∣'2	JI M2
	怙必	怙必	怙必	什。必	什0必	什。必
Ttrain Jinit	0.31995	0.94744	0.0061168	0.32072	0.94718	0.0061618
Ttrain- J final	0.56007	0.82844	000063427	0.55869	0.82938	0.00062509
Table 17: Depiction of the alignment of the initial label/residual with the informa-
tion/nuisance space using 75% label corruption on MNIST data.
66
Under review as a conference paper at ICLR 2020
Residual energy	Residual energy
b_f
0123
0 - - -
1 10 10
10-4
0	100	200	300
Epochs
400
on initial train Jacobian
0
rorrE
10-3
012
0 - -
1 10 10
ygrene laudiseR
0	100	200	300	400
Epochs
(b) Residual projection on final train Jacobian
Training error
Test error
200	300	400
Epochs
(c) Training and test error of experiment
Figure 25: Experiments with 75% label corruption on MNIST data
0	100	200	300	400
Epochs
(a) Residual projection on initial train Jacobian
0123
0 - - -
1 10 10 10
ygrene laudiseR
0	100	200	300	400
Epochs
(b) Residual projection on final train Jacobian
Figure 26: Experiments with 100% label corruption on MNIST data
67
Under review as a conference paper at ICLR 2020
	InI⑹必	InN (y)l'2	JI Mg2	InI (，0儿2	InN (r0)∣∣'2	JI M2
	怙必	怙必	怙必	什。必	什0必	什。必
Ttrain- Jinit	0.31793	0.94811	0.0050114	0.34633	0.93811	0.0050949
Ttrain- J final	0.57769	0.81626	000060636	0.59174	0.80613	0.00062812
Table 18: Depiction of the alignment of the initial label/residual with the informa-
tion/nuisance space using 100% label corruption on MNIST data.
Figure 27: Test error vs. final projection of labels on nuisance subspace for MNIST
experiments
C.5 Singular value decomposition of the Jacobian
The matrix representation of the Jacobian arising in the numerical experiments is in general very
large and pose a computational challenge. In particular, the Jacobian matrix for a ResNet20 model is
500k × 270k (or 540 GB memory with float32 entries, more than 1 TB with float64) on the CIFAR-10
training data. Storing these matrices in memory for singular value decomposition is not feasible and
therefore other methods are necessary to approximate the singular values and singular vectors.
The Lanczos algorithm Lanczos (1950) is an adaptation of power methods, which guarantees fast
convergence to the largest eigenvalues of a hermitian matrix. More importantly, it only requires
matrix-vector products to estimate the top eigenvectors and therefore resolves the memory issues
with large Jacobians. Using the ARPACK implementation of the implicitly restarted Lanczos method
Lehoucq et al. (1998) it takes about 1 hour on 64 cores to find the top 50 eigenvalues and eigenvectors
of the 500k × 500k symmetric matrix JJT for CIFAR-10 training data. The same algorithm has
been used to calculate the histogram of top 1000 singular values.
The required computation time for Lanczos method is dominated by large Jacobian-vector products,
which increases linearly with the number of calculated eigenvalues. Therefore it is necessary to
depart from deterministic power iteration type algorithms and take a different approach. Stochastic
Lanczos quadrature algorithm has been recently introduced to the machine learning community in
Ghorbani et al. (2019a) and Papyan (2019a) to approximate the eigenvalue density of large neural
network Hessians. Finding the exact spectral density might be difficult therefore first the problem is
relaxed by approximating the spectrum convolved with a Gaussian density of variance σ2 = 10-5 . As
described in Ghorbani et al. (2019a), we draw k = 10 i.i.d. Gaussian random vectors and run m = 80
steps of the Lanczos algorithm starting from each random vector to estimate the parameters of the
spectral density. We modified the algorithm by replacing Hessian-vector products by J J T -vector
products. Finally, we normalize the density and scale it by the dimension of the Jacobian output
space.
68
Under review as a conference paper at ICLR 2020
D Jacobian Adaptation for Linear Data
In this section, we will (non-rigorously) argue that Jacobian and its singular value spectrum indeed
adapts to the data for simple linear datasets. Specifically, we will show that the very first gradient
iteration from a random initialization is already powerful enough to encourage an approximately
low-rank Jacobian.
Dataset: Let (xi)n=ι %. N(0, Id) and fix θ ∈ Rd. Fix labels yi = θτXi and the dataset (xi,yi)n=ι.
Neural net: Let Wo ∈ Rk×d	N(0,1) and fix V ∈ Rk with half +σ and half -σ entries. Set
activation to be ReLU.
(Non-rigorous) Statement: With small initialization (σ ≈ 0) and a single large gradient step,
Wi = Wo - ηVL(Wo) has a (properly scaled) Jacobian matrix with approximately rank 2d.
Instead, at random initialization W0 , Jacobian has full rank n (assuming kd ≫ n).
Specifically, at W1, we have
σ2d+1(J(X, Wi)) < √d∕n and σ2d(J(X, Wi)) > 1.	(D.1)
where σi is the ith largest singular value. In words, J(X, Wi) is approximately rank 2d when n > d.
This is in stark contrast to the properties of Jacobian at random initialization. For instance Li &
Liang (2018); Zou et al. (2018); Oymak & Soltanolkotabi (2019) actually show that Jacobian is well
conditioned with minimum singular value lower bounded by the minimum separation between the
samples (implying full rank of n). Such separation (independent of n) holds for Gaussian data even
when n is polynomially larger than d via standard concentration / packing bounds (also see Oymak &
Soltanolkotabi (2019); Li & Liang (2018)). Hence, as further formalized below, we argue that even a
single step of gradient descent can lead to an approximately low-rank Jacobian.
D. 1 Approach
Fixing weights of Wo, we study the gradient of the random data. Let Wr be rth row of Wo. Fixing
hidden node r, let x' = Xi if XTWr ≥ 0 (i.e. the rth node is active) and 0 else. The gradient of the
node r at Wo is given by
VL(W)
∂wr
vn
n ∑(f (M WO)- yi)xi.
If initialization is small enough (σ ≈ 0), we can approximate f(xi, Wo) - yi ≈ yi hence we study
VL(W)	vr n ′
Fr ≈ n ∑ yix'.
We will show that gradients of each row are ±σθ∕2 in expectation. Decompose a sample X Z N(0, Id)
as
wr
X =g jw∏'2 g
where g is standard normal vector over the orthogonal complement of vector wr and g Z N(0, 1).
Denoting label associated to X by y, consequently, we find
E[yx'] = E[yx ∣ XTWr ≥ 0]P(xTWr ≥ 0)
=1 E[(g wr— + g)(gwr + g)Tθ ∣ g ≥ 0]
2	Mrl'2
=1 E[ggT]θ + E[g2 ∣ g ≥ 0]WrWTθ
_ θ
=2 .
(D.2)
(D.3)
(D.4)
(D.5)
Hence
E["] ≈ Viθ∕2.
69
Under review as a conference paper at ICLR 2020
This implies that, in population (n → ∞), gradient vector has an extremely simple rank-one form:
EX [VL(W )∣ w0 ] = 1 vθT.
Standard results on non-asymptotic statistics then yield that with exponentially high-probability
(i.e. 1 - exp(-cn)),
ll VL(W)	VL(W)	/ɪ
∣ ~∂wk~- EX [ ~∂wk- ]∣'2 < dnn
As long as k < poly(n), union bounding over all rows, we find the spectral norm bound
IIVL(W) ∣ W0- EX [VL(W) ∣ W0]∣ ≤ σ√kd∕n
On the other hand, EX [VL(W) ∣ W。] is rank 1 with nonzero singular value σ∖∕k∕2.
Consequently, a large gradient step will ensure that
W1 = W0 - ηVL(W) ∣ W0
is approximately rank one as well. Specifically, set the rank one matrix
WI = EX [ηVL(W )∣ W0].
Then the tail of the spectrum is at most ∣ Wi - W11 < ,d∕n∣∣ W11 (i.e. Jd/n as large as the spectral
norm).
Finally, we need to move from showing low-rankness of the weight matrix to low-rankness of
Jacobian. Define the Jacobians associated with data at W1 and Wi by
J = J(X, WI) = diag(v)φ'(W1XT) * XT,	J = J(X, Wi) = diag(v)φ'(W1XT) * XT
The Jacobian associated with J is exactly rank 2d since matrix diag(v)φ'( WiXT) has exactly two
distinct rows and kronecker producting with XT will yield d rank for each (adding up to 2d). Since
X is well-conditioned with high probability (as it is N(0,1) and fat with n > d), top 2d singular
values and σ2d( J) are strictly positive with a lower bound independent of n and σ (after scaling by
1∕σIXI).
Finally, using smoothness of φ' (which excludes ReLU but includes smoother versions of ReLU such
as softplus), using Jacobian perturbation bound (6.31), we have (after scaling by 1∕σIXI).
J - Jll < ιwι-Wd∣ < √d∕n.
Together, this leads to the desired conclusion (D.1) since singular value perturbation inequality yields
σ2d+1( J) ≤ IIJ - JIl < √d∕n (as σ2d+1( J) = 0).
70