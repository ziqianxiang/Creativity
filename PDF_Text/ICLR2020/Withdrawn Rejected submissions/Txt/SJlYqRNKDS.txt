Under review as a conference paper at ICLR 2020
Blockwise Adaptivity: Faster Training and
Better Generalization in Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
Stochastic methods with coordinate-wise adaptive stepsize (such as RMSprop and
Adam) have been widely used in training deep neural networks. Despite their
fast convergence, they can generalize worse than stochastic gradient descent. In
this paper, by revisiting the design of Adagrad, we propose to split the network
parameters into blocks, and use a blockwise adaptive stepsize. Intuitively, block-
wise adaptivity is less aggressive than adaptivity to individual coordinates, and
can have a better balance between adaptivity and generalization. We show the-
oretically that the proposed blockwise adaptive gradient descent has comparable
regret in online convex learning and convergence rate for optimizing nonconvex
objective as its counterpart with coordinate-wise adaptive stepsize, but is better
up to some constant. We also study its uniform stability and show that blockwise
adaptivity can lead to lower generalization error than coordinate-wise adaptiv-
ity. Experimental results show that blockwise adaptive gradient descent converges
faster and improves generalization performance over Nesterov’s accelerated gra-
dient and Adam.
1 Introduction
Deep networks have achieved excellent performance in a variety of domains such as computer vision
(He et al., 2016), language modeling (Zaremba et al., 2014), and speech recognition (Graves et al.,
2013). The most popular optimizer is stochastic gradient decent (SGD) (Robbins & Monro, 1951),
which is simple and has low per-iteration complexity. Its convergence rate is also well-established
(Ghadimi & Lan, 2013; Bottou et al., 2018). However, vanilla SGD is sensitive to the choice of
stepsize, and requires careful tuning.
To improve the efficiency and robustness of SGD, many variants based on coordinate-wise adaptive
stepsize (Almeida et al., 1999; Duchi et al., 2011; Schaul et al., 2013; Kingma & Ba, 2015; Tieleman
& Hinton, 2012; Zeiler, 2012; Zheng & Kwok, 2017; Reddi et al., 2018; Zaheer et al., 2018; Zou
& Shen, 2018; Chen et al., 2019; Zou et al., 2019) have been proposed. Though this is effective
in accelerating convergence, its generalization performance is often worse than SGD (Wilson et al.,
2017). Recently, it is shown that coordinate-wise adaptive gradient descent is closely related to sign-
based gradient descent (Balles & Hennig, 2018; Bernstein et al., 2018). Theoretical arguments and
empirical evidence suggest that the gradient sign impedes generalization (Balles & Hennig, 2018).
To contract the generalization gap, a partial adaptive parameter for the second-order momentum is
proposed (Chen & Gu, 2018). By using a smaller partial adaptive parameter, the adaptive gradient
algorithm behaves less like sign descent and more like SGD. Based on the similar motivation, Luo
et al. (2019) proposed to gradually transform Adam Kingma & Ba (2015) to SGD. Moreover, to
avoid numerical problems in practical implementation, a small (= 10-8) parameter is typically
used in these methods. This parameter controls adaptivity of the algorithm (Zaheer et al., 2018). A
larger value (say, = 10-3) can reduce adaptivity and empirically helps Adam to match its general-
ization performance with SGD. This implies that coordinate-wise adaptivity may be too aggressive
for good generalization performance.
To improve generalization performance, attempts have been made to use a layer-wise stepsize (Singh
et al., 2015; You et al., 2017; Yu et al., 2017; Zhou et al., 2018), which assign different stepsizes to
different layers or normalize the layer-wise gradient. However, there has been no theoretical analysis
for its empirical success.
1
Under review as a conference paper at ICLR 2020
In this paper, we consider the more general case in which model parameters are partitioned into
blocks, instead of simply into layers. By revisiting the derivation of Adagrad, we propose the use
of a blockwise stepsize which depends on the corresponding gradient block. Such blockwise adap-
tivity is less aggressive than coordinate-wise adaptivity, as it adapts to parameter blocks instead of
to individual parameters. This can thus have a better balance between adaptivity and generaliza-
tion. Moreover, unlike coordinate-wise adaptivity, blockwise adaptivity is not sign-based gradient
descent, and so does not suffer from its performance deterioration.
As in (Bernstein et al., 2018; Ghadimi & Lan, 2013; Ward et al., 2018; Zaheer et al., 2018; Zou &
Shen, 2018; Zou et al., 2019), we will focus on the expected risk minimization problem:
min F(θ) = Ez[f(θ; z)],	(1)
θ
where f is some possibly nonconvex loss function, z is a random sample, θ is the model parameter,
and the expectation is taken w.r.t. the underlying sample distribution. The expected risk measures
the generalization performance on unseen data (Bottou et al., 2018), and reduces to the empirical risk
when a finite training set is considered. We show theoretically that the proposed blockwise adaptive
gradient descent can be faster than its coordinate-wise counterpart. Using tools on uniform stability
(Bousquet & Elisseeff, 2002; Hardt et al., 2016), we also show that blockwise adaptivity has poten-
tially lower generalization error than coordinate-wise adaptivity. Empirically, blockwise adaptive
gradient descent converges faster and obtains better generalization performance than coordinate-
wise descent (Adam) and Nesterov’s accelerated gradient (NAG) (Sutskever et al., 2013).
Notations. For an integer n, [n] = {1, 2,..., n}. For a vector x, XT denotes its transpose, Diag(X)
is a diagonal matrix with X on its diagonal, √χ is the element-wise square root of x, x2 is the
coordinate-wise square of x, ∣∣x∣∣2 = λ∕xtX, ∣∣x∣∣∞ = maxi |xi|, ∣∣x∣∣Q = XTQx, where Q is a
positive semidefinite (psd) matrix, and X ≥ 0 means Xi ≥ 0 for all i. For two vectors X and y, X/y,
and hX, yi denote the element-wise division and dot product, respectively. For a square matrix X,
X-1 is its inverse, and X 0 means that X is psd. Moreover, 1d = [1, 1, . . . , 1]T ∈ Rd.
2 Related Work
2.1 Adagrad
Adagrad (shown in Algorithm 1) is an adaptive gradient method in online convex learning with
coordinate-wise stepsize (Duchi et al., 2011; McMahan & Streeter, 2010). Itis particularly useful for
sparse learning, as parameters for the rare features can take large steps. Recently, Ward et al. (2018)
established its convergence properties with a global adaptive stepsize in nonconvex optimization. It
is shown that Adagrad converges to a stationary point at the optimal O(1/√T) rate (UP to a factor
log(T)), where T is the total number of iterations.
Recall that the SGD iterate is the solution to the problem: θt+ι = argminθhgt, θ) + 击∣∣θ 一 θtk2,
where gt is the gradient of the loss ft at iteration t, and θt ∈ Rd is the parameter vector. To
incorporate information about the curvature of sequence {ft}, the '2-norm in the SGD update can
be replaced by the Mahalanobis norm, leading to (Duchi et al., 2011):
θt+1 = argminhgt,θi + 2η kθ 一 θtkDiag(st)-ι,	⑵
where st ≥ 0. This is an instance of mirror descent (Nemirovski & Yudin, 1983). Its regret bound
has a gradient-related term PtT=1 ∣gt∣2Diag(s )-1 . Adagrad’s stepsize can be obtained by examining
a similar objective (Duchi et al., 2011):
T
ms∈iSn	∣gt∣2Diag(s)-1,	(3)
t=1
where S = {s : S ≥ 0,〈s, 1〉 ≤ c}, and C is some constant. At optimality, s*,i =
ckgLT,ik2/Pj=I kgi：T,jI∣2, where gi：T,i = [gι,i,..., gτ,i]T. AS St cannot depend on gj’s with
j > t, this suggests st,i a ∣∣gLt,i∣∣2∙ Theoretically, this choice of St leads to a regret bound that is
competitive with the best post-hoc optimal bound (McMahan & Streeter, 2010).
2
Under review as a conference paper at ICLR 2020
To solve the expected risk minimization problem in (1), an Adagrad variant called weighted
AdaEMA is recently proposed in (Zou et al., 2019). It employs weighted averaging of gt2,i’s for
stepsize. Moreover, weighted AdaEMA is a general coordinate-wise adaptive method and includes
many Adagrad variants, including Adam and RMSprop, as special cases.
2.2 Uniform Stability
Given n samples S = {zi}in=1 drawn i.i.d. from an underlying data distribution D, one often learns
the model by minimizing the empirical risk: min6 Φs(θ) ≡ 1 Pn=I f (θ; Zi). Let M(S) be the
output of a possibly randomized algorithm M (e.g., SGD) running on data S.
Definition 1. (Hardt et al., 2016) Let S and S0 be two data sets of size n that differ in only
one sample. Algorithm M is u-uniformly stable if stab ≡ supS,S0 supz∈D EM [f(M(S); z) -
f(M(S0); z)] ≤ %	,
The generalization error is defined as gen ≡ ES,M [ΦS (M (S)) - F (M(S))], where the expectation
is taken w.r.t. the set S and randomness of M (Hardt et al., 2016). It is shown that the generalization
error is bounded by the uniform stability of M, i.e., |gen | ≤ stab (Hardt et al., 2016). In other
words, the more uniformly stable an algorithm is, the lower is its generalization error.
3 Blockwise Adaptive Descent
Consider the under-determined least squares problem:
min kXθ - yk22,	(4)
θ
where X ∈ Rn×d is the input matrix (with sample size n, and dimensionality d > n), and output y ∈
Rn. We assume that XXT is invertible. As pointed out in (Zhang et al., 2017; Wilson et al., 2017),
any stochastic gradient descent method on problem (4) with a global stepsize outputs a trajectory
with iterates lying in the span of the rows of X. One solution of (4) is XT (XXT )-1y, which
happens to be the solution with minimum '2-norm among all possible global minimizers. This
minimum-norm solution has the largest margin, and maximizing margin typically leads to lower
generalization error (Boser et al., 1992).
It is known that SGD converges to the minimum '2-norm solution of problem (4) (Zhang et al.,
2017). On the other hand, coordinate-wise adaptive methods (such as Adagrad, RMSprop, and
Adam) fail to find the minimum '2-norm solution, but converge to solutions with low '∞-norm
instead (Wilson et al., 2017). Examples in (Wilson et al., 2017) show that solutions obtained by
these adaptive methods can generalize arbitrarily poorly, while the SGD solution makes no error.
In Section 3.1, we first show that blockwise adaptivity, unlike coordinate-wise adaptivity, can find
the minimum '2-norm solution of a nonlinear least squares problem in layer-wise training of a neural
network. This motivates us to further exploit blockwise adaptivity in end-to-end neural network
training (Section 3.2). To provide more analysis, Section 3.3 studies the proposed algorithm in the
online convex learning setting as in (Duchi et al., 2011; Kingma & Ba, 2015; Reddi et al., 2018).
3.1	Blockwise vs Coordinate-wise Adaptivity
Consider a L-layer neural network with output Φl-i(•…Φ2(Φ1(XW1)W2) ∙ ∙ ∙ WL-I)Wl, where
{Wl ∈ Rdl-1 ×dl}lL=1 are weight matrices with d0 = d and dL = m, the output dimensionality.
Assume that the nonlinear activation functions {φl }lL=-11 are bijective with nonzero derivatives on
R (e.g., tanh and leaky ReLU). For simplicity, we further assume that dl = d = m > n for all l.
Training this neural network with the square loss corresponds to solving the nonlinear optimization
problem: min{w此[∣∣Φl-i(…Φ2(Φ1(XW1)W2) ∙ ∙ ∙ WL-I)WL - Y∣∣2, where Y ∈ Rn×m is the
label matrix. Consider training the network layer-by-layer, starting from the bottom one. For layer
l, its optimization subproblem can be rewritten as
min∣Φl(Hl-1Wl)-Y∣22,	(5)
Wl
where φl (∙) = φL-1(∙ ∙ ∙ φl + 1(φl(∙)Wl+1) ∙ ∙ ∙ WL-I)WL, Hl-I = φl-1(∙ ∙ ∙ φ1 (XWI)…Wl-1) is
the input activation to the lth layer, and H0 = X . Note that (4) is a special case of (5) with L = 1
3
Under review as a conference paper at ICLR 2020
and identity mapping. To minimize (5), the weights for this layer are updated as
Wt+1,l = Wt,l - ηt,lgt,l,	(6)
where gt,l is a stochastic gradient evaluated at Wt,l at time t, and ηt,l is the stepsize which may be
adaptive in that it depends on gt,l .
Proposition 1. Assume that Wl0 ’s (with l0 > l) are invertible. If Wl is initialized to zero, and Hl-1
has full row rank, the critical point that (6) converges to is the minimum '2-norm solution of (5) in
expectation.
Another benefit of using a blockwise stepsize is that the optimizer’s extra memory cost can be
reduced. Using a coordinate-wise stepsize requires an additional O(d) memory for storing estimates
of the second moment, while the blockwise stepsize only needs an extra O(B) memory, where B is
the number of blocks. A deep network generally has millions of parameters but only tens of layers.
If we set B to be the number of layers, memory reduction can be significant.
3.2	Blockwise Adaptive Gradient (BAG)
Let the gradient gt ∈ Rd be partitioned to {gt,Gb ∈ Rdb : b = 1, . . . , B}, where Gb is the set of
indices in block b, and gt,Gb is the corresponding subvector of gt . Inspired by problem (3) in the
derivation of Adagrad, we consider the following variant which imposes a block structure on s:
T
sm∈iSn0	kgtk2Diag(s)-1,	(7)
t=1
where S0 = {s : s = [q1 1dT , . . . , qB 1dT ]T ≥ 0, hs, 1i ≤ c} for some qi ∈ R. We assume the
indices in Gb are consecutive; otherwise, we can simply reorder the elements of the gradient. Note
that reordering does not change the result, as the objective is invariant to ordering of the coordinates.
Itcan be easily shown that at optimality of (7), qb = ckgi：T,Gb k2∕(√db PB=I √di∣∣gLτ,Gi∣∣2),where
gi：T,Gb = [gτGb,..., gT,Gb ]t . The optimal qb is thus proportional to kgιτ^b k2∕√db. When St in
(2) is partitioned by the same block structure, the optimal qb suggests to incorporate kgi：t,Gb k2∕√db
into st for block b at time t.
Algorithm 1 Adagrad: Adaptive gradient for		Algorithm 2 BAG: Blockwise adaptive gradient	
online convex learning (Duchi et al., 2011).		for online convex learning.	
1	Input: η > 0; e > 0.	1	Input: η > 0; E > 0.
2	initialize θι; v0 — 0	2	initialize θι; v0 — 0
3	: for t = 1, 2, . . . , T do	3	: for t = 1, 2, . . . , T do
4	: Receive subgradient gt ∈ ∂ft (θt)	4	: Receive subgradient gt ∈ ∂ft (θt)
5	: for i = 1, 2, . . . , d do	5	: for b = 1, 2, . . . , B do
6	vt,i = vt-1,i + ∣∣gt,i k2	6	vt,b = vt-1,b + kgt,Gbl∣2∕db
7	θt+1,i = θt,i - ηgt,i/(√vt,i + E)	7	θt+1,Gb = θt,Gb - ηgt,Gb/(√vtb + E)
8	: end for	8	: end for
9	: end for	9	: end for
The proposed procedure, which will be called blockwise adaptive gradient (BAG), is shown in
Algorithm 2. Compared to Adagrad, each block, instead of each coordinate, has its own learn-
ing rate. When B = d (i.e., each block has only one coordinate), BAG reduces to Adagrad.
When B = 1 (i.e., all coordinates are grouped together), Algorithm 2 produces the update:
θt+ι = θt - n(gt/(kgi:tkz/ʌ/d + E)) with a global adaptive learning rate, which is equivalent
to AdaGrad-Norm (Ward et al., 2018).
Returning to the underdetermined least squares problem in (4), the following Proposition shows that
when B > 1, BAG finds the minimum '2-norm solution in each subspace induced by the group
structure. When B = 1, BAG converges to the minimum '2 -norm solution of (4).
Proposition 2. Assume that for each b ∈ [B], each submatrix X:,Gb ∈ Rn×db has full row rank.
BAG (with θι initialized to 0) converges to an optimal solution θ* of (4). For each b ∈ [B], the
SubVector。*& of θ* equals XTGb(X：,GbXTGb)Tub for some Ub ∈ Rn and ∑b∈[B] Ub = y.
4
Under review as a conference paper at ICLR 2020
3.3	Regret Analysis
To further illustrate the advantages of blockwise adaptivity over coordinate-wise adaptivity, we con-
sider the online convex learning setting. At round t, the learner picks θt, and suffers a loss ft(θt).
After T rounds, the learner wants to achieve a low regret w.r.t. an optimal θ* = arg minθ PT=I ft (θ)
in hindsight:
TTT	T
R(T)≡ X ft@)- X ft(θ*) ≡ X ft(θt) — inf X ft(θ).	(8)
We make the following assumptions.
Assumption 1. Each ft in (8) is convex but possibly nonsmooth. There exists a subgradient g ∈
∂ft (θ) such that ft (θ0) ≥ ft (θ) + hg, θ0 - θi for all θ, θ0.
Assumption 2. Each parameter block is in a ball of the corresponding optimal block throughout
the iterations. In other words, for all b ∈ [B], maxt ∣∣θt,Gb 一 θ*,Gb k2 ≤ Db for some Db, where
θ*,Gb is the subvector of θ* in block b.
When B = 1, this reduces to the common assumption in online convex learning Duchi & Singer
(2009). When B > 1, it naturally encodes the heterogeneity of model parameters.
Theorem 1. Suppose that Assumptions 1 and2 hold. Then,
R(T) ≤ X ]2η√⅞D2 + ηPdb] kgi：T，Gbk2.	⑼
Assume that maxt ∣∣θt 一 θ* k∞ ≤ D∞ for some constant D∞. When B = d, the above regret bound
reduces to that of Adagrad (Theorem 5 of (Duchi et al., 2011)) by setting Db = D∞ for all b ∈ [B].
In the following, we show that when gradient magnitudes for elements in the same block have the
same upper bound, blockwise adaptive learning can have lower regret than coordinate-wise adap-
tive learning. As a deep network can be naturally divided into blocks (examples will be given in
Section 5.2) and parameters in the same block are likely to have gradients with similar magnitudes
(which is verified empirically in Appendix B), blockwise adaptivity can be more beneficial.
Corollary 1. Assume that E[gt2,i] ≤ σb2 for all i ∈ Gb. The expectation of (9) can be bounded as:
:一三「1c IL
E[R(T)] ≤ »b — Db + ηdb TT.	(10)
2η
b=1
When B = d (Which corresponds to Adagrad), Assumption 2 becomes maxt(θt,i 一 θ*,i) ≤ Di for
some Di. Expectation of the bound in (9) then reduces to
B 「1	]
E[R(T)] ≤ N σb 2η gDb + ηdb TT.	(11)
Assuming that Assumption 2 is tight in the sense that Db2 ≤ Pi∈G Di2. The bound in (10) is
then smaller than that in (11). Intuitively, when gradients in a block have similar magnitudes in
expectation, we have from the weak law of large numbers that vt,b = Pi∈G vt,i/db (where vt,b and
vt,i are as defined in Algorithms 2 and 1, respectively) is a better estimate of E[vt,i] than vt,i for a
single coordinate i. This implies using blockwise adaptivity may lead to better performance.
4 Blockwise Adaptive Gradient with Momentum (BAGM)
In Algorithm 2, vt,b’s are increasing w.r.t. t. The update suffers from vanishing stepsize, making
slow progress on nonconvex problems such as deep network training. To alleviate this problem,
many Adagrad variants (such as RMSprop, Adam and weighted AdaEMA (Zou et al., 2019)) use
weighted moving average momentum. In this paper, we extend the use of blockwise adaptive step-
size to weighted AdaEMA (Section 4.1), which includes Adam and RMSprop as special cases.
The proposed algorithm will be called blockwise adaptive gradient with momentum (BAGM). Sec-
tions 4.2 and 4.3 then study its convergence and generalization properties. Note that as BAG is a
special case of BAGM, the analysis there also apply to BAG.
5
Under review as a conference paper at ICLR 2020
4.1	Proposed Algorithm
The proposed BAGM is shown in Algorithm 4. Here, mt serves as an exponential moving averaged
momentum, {βt} is a sequence of momentum parameters, and at ’s assign different weights to the
past gradients in the accumulation of variance. Note from Algorithm 4 that the variance estimate
can be rewritten as
Vt,b
t
X
i=1
ai kgi,Gbk2
At	db
1
Pj= aj
t
X ai
i=1
kgi,Gbk2
db
(12)
In particular, we will consider the three weight sequences {at} introduced in (Zou & Shen, 2018).
S.1: at = a for some a > 0; S.2: at = tτ for some τ > 0; The fraction at/At in (12) then decreases
as O(1/t). S.3: at = α-t for some 0 < α < 1: It can be shown that this is equivalent to using the
exponential moving average estimate: vt,b = αvt-ι,b + (1 - α) kgtdb k2, and vtb = Iv-Obi.
With βt = 0, weight sequence S.1, and = 0, BAGM reduces BAG. When B = d and = 0,
BAGM reduces to weighted AdaEMA. As weighted AdaEMA includes many Adagrad variants, the
proposed BAGM also covers the corresponding blockwise variants.
Algorithm 3 Weighted AdaEMA for stochastic
nonconvex optimization.
1:	Input: {ηt}; {at}; {βt}; E > 0.
2:	initialize θι; vo J 0; m° J 0; A° J E
3:	fort = 1,2,. . . ,T do
4:	Sample an unbiased stochastic gradient
gt
5:	At = At-1 + at
6:	for i = 1, 2, . . . , d do
7:	vt,i = vt-1,i + atgt,i
8:	Vt,i = vt,i/At
9:	mt,i = ∣βtmt-1,i + (I - βt)gt,i
10:	θt+1,i = θt,i - ηtmt,i/pvt,i
11:	end for
12:	end for
Algorithm 4 BAGM: Blockwise adaptive gra-
dient with momentum for stochastic nonconvex
optimization.
1:	Input: {ηt}; {at}; {βt}; E > 0.
2:	initialize θ1 ; v0 J 0; m0 J 0; A0 J 0
3:	for t = 1, 2, . . . , T do
4:	Sample an unbiased stochastic gradient
gt
5:	At = At-1 + at
6:	for b = 1, 2, . . . , B do
7:	vt,b = vt-1,b + atkgt,Gb k2/db
8:	Vt,b = vt,b/At
9:	mt,Gb = βtmt-l,Gb + (I - βt)gt,Gb
10:	θt+1,Gb = θt,Gb - ηtmt,Gb/(pvt,b + E)
11:	end for
12:	end for
4.2	Convergence Analysis on Nonconvex Problems
Assumption 3. F in ⑴ is Iower-bounded (i.e., F(θ*) = infθ F(θ) > -∞) and L-smooth.
Assumption 4. Each block of stochastic gradient has bounded second moment, i.e.,
Et[kgt,Gb k22]/db ≤ σb2, ∀b ∈ [B], ∀t, where the expectation is taken w.r.t. the random ft.
Assumption 4 implies the variance of each block of stochastic gradient is upper-bounded by dbσb2
(i.e., Et[∣∣gt,Gb-VGb F (θt)k2] = Et[kgt,Gbk2]-∣∣VGb F (θt)k2 ≤ dbσ2). This naturally encodes the
notion of heterogeneous gradient in a multi-layer neural network. When B = 1, this reduces to the
usual second moment bound in stochastic approximation (Shamir & Zhang, 2013; Zou et al., 2019).
Assumption 5. 0 ≤ βt ≤ β for some 0 ≤ β < 1.
Assumption 5 allows us to use, for example, a constant βt = β, a decreasing sequence βt = β∕tτ,
or an increasing sequence βt = β(1 - 1∕tτ).
Assumption 6. (i) {at} is non-decreasing; (ii) at grows slowly such that {At-1/At} is non-
decreasing and At/(At-1 + a1) ≤ ω for some ω ≥ 0; (iii) p ≡ limt→∞ At-1/At > β2.
Assumption 6 is satisfied by the three weight sequences introduced above. Specifically, for S.1:
ω = 1 andP = 1; S.2: ω = (1 + 2τ)/2 andP = 1; S.3: ω = (1 + 1∕α)∕2 andP = α > β2.
Assumption 7. (Zou et al., 2019) The stepsize η is chosen such that Wt = ηt/，at/At is “almost”
non-increasing, i.e., there exists a non-increasing sequence {zt} and positive constants C1 and C2
such that C1 zt ≤ wt ≤ C2zt for all t.
6
Under review as a conference paper at ICLR 2020
Assumption 7 is satisfied by the weights sequences S.1, S.2, S.3 when
ηt = η∕√t	(13)
for some η > 0. Interested readers are referred to (Zou et al., 2019) for details.
Proposition 3. Suppose that Assumptions 3-7 hold. With probability at least 1 一 δ2/3,
minι≤t≤τ kVF(θt)k2 ≤ δO(log(T)∕√T) for S.1 and SNand minι≤t≤τ ∣∣VF(θt)k2 ≤ 1O⑴
for S.3.
Note that SGD, with the decreasing stepsize in (13), converges at a rate of O(log(T)/T) (Ghadimi
& Lan, 2013). Thus, the rates for S.1 and S.2 are as good as SGD. Though S.3 only leads to an O(1)
bound, it has good performance in practice (Kingma & Ba, 2015; Zaheer et al., 2018). Moreover,
recall that when B = d and = 0, BAGM reduces to weighted AdaEMA. In this case, Proposition 3
obtains the same convergence rates as in (Zou et al., 2019).
Next, we consider B = B for some B 6= d (blockwise stepsize), and compare it with B = d
(coordinate-wise stepsize). This requires the following assumption, which is slightly stronger than
Assumption 4 (that only bounds the expectation).
Assumption 8. kgt,Gb k22/db ≤ Gb2, ∀b ∈ [B] and ∀t.
Note that when B = d, Assumption 8 becomes gt2,i ≤ Gi2 for some Gi, and Assumption 4 becomes
Et [gt2,i] ≤ σi2 for some σi. Let Gb be the set of indices in block b when B = B. The follow-
ing Corollary shows that BAGM has faster convergence than its coordinate-wise counterpart when
{02}i∈Cb have low variability. This also agrees with our observation in Section 3.3 that blockwise
adaptivity can have lower regret under this condition.
Corollary 2. Suppose that Assumptions 3-8 hold.
Define r1 ≡
Pb=1 Pi∈Gb σi and r _ Pb=ι~Pi∈Gb σilog8i Ie +1)
PB=I σbdb	3 —	PB=I σbdb log(σ2∕e2 + 1)
P=1 Pi∈Gb log(σ2∕e2 + 1)
PB=I db log(σ2∕e2 + 1)
r2 ≡
^r.___ _	^r.	___ .
Let Cd(T)∕δ (resp. CB (T)∕δ) be the
high probability upper bound on min1≤t≤T kVF(θt)k22 when B = d (resp. B = B). If
√maxb max -U卢 G2+e2	~ ,	-
ImaxbGG+e2i	≥ Lthen Cd(T) ≥ CB (T).
√maxb max-U卢 G2 +e2
maxbiGG+e2i
≥ 1 when Assumption 8 is tight. By comparing the denominator
and numerator in r1, r2, r3, it can be seen that min(r1, r2, r3) is close to or greater than 1 when
{σ2h∈Cb have low variability. These will be verified empirically in Appendix B.
4.3	Uniform Stability and Generalization Error
As in Definition 1, let S, S0 be two data sets of size n that differ in only one sample, and the tth
iterates of BAGM on S and S0 by θt and θt0, respectively. Let ∆t = kθt 一 θt0 k2, and ∆t(z) =
|f (θt; Z) - f (θt; z)|. The following Proposition allows US to study how B affects the growth of
E[∆t(z)], where the expectation is taken w.r.t. randomness of the algorithm.
Proposition 4. Suppose that Assumptions 3-7 hold. Assume that f is Y-Lipschitz1,
βt = 0, and the initial θ1 , θ10 values are the same. We have supS,S0 supz E[∆t+1(z)] ≤
2≡ ∕hw2PB=I db	log (σi/^2+ι)+dω	Pk=I	η2i	t +	(1-n)YWt,	Where	Wt	=
Y Pk=I ηk EImaXb 卜/(P¾b + C)- 1/(qvkb + e)∣i + L Ptk=I ηk E [δ⅛ / (Pminb vk,b + e)].
Recall that σb2 ≤ d1 Pi∈gb σf. If σb2 = dl Pi∈久 σf, the first term on the RHS of the above bound
is smallest when B = d; otherwise, some B < d will make this term smallest. For the minb vk,b
R
term inside Wt, this reduces to -R Pb=ι ^k,b when B = 1, and to minb mι□i∈Gb ^k,i when B = d.
1In other words, |f (θ; z) — f (θ0; z)| ≤ γ∣∣θ — θ0∣∣2 for any z.
7
Under review as a conference paper at ICLR 2020
R^	..	一 一
As B Pb=I ^k,b ≥ minb ^k,b ≥ minb mini∈Gb ^k,i ,this minb ^k,b term is the smallest When B = d,
and is largest when B = 1. As for the other terms in Wt, the first term is small when B is close
to d, and the second term is small When B approaches 1. Hence, for B equals some 1 < B < d,
supS,S0 supz E[∆t+1 (z)], and thus the generalization error, groWs sloWer than those of B = d and
B = 1. Besides, Proposition 4 also indicates that a larger makes the bound smaller.
Note that the uniform stability bound of SGD (Theorem 3.8 in [12]) is not directly comparable With
our Proposition 4. HoWever, as SGD and B = 1 both depend on the same second moment upper
bound (Assumption 4), by shoWing B = B is better than B = 1, We expect blockWise adaptivity to
be also better than SGD.
5 Experiments
In Section 5.1, We first empirically validate the regret analysis results of BAG (Section 3.3) on a
linear model. In Section 5.2, We run deep netWorks on a number of standard benchmark data sets
including CIFAR-10 (Section 5.2.1), ImageNet (Section 5.3), and WikiText-2 (Section 5.4). As the
focus is on deep learning, We only use BAGM (instead of BAG) in this section.
5.1	Illustration of the Regret Analysis Results
In this section, We use BAG on the linear model ft(θt) = max(0, 1 -ythθt, xti), With input xt ∈ Rd
generated in a blockWise manner and yt ∈ {-1, 1} is the label for xt. Assume that xt is partitioned
into B blocks, whose structure may be different from that of the B gradient blocks. With probability
pb, each element xt,i in input block b is sampled from N (cbyt , γb2) for some scaling factor cb and
variance γb2, and xt,i = 0 otherwise. It can be easily shown that for elements in the same input block
b, their expected gradient magnitudes have the same upper bound (E[gt2,i] ≤ pb(cb2 + γb2)).
We generate a synthetic data set, with d = 100, using the above procedure. The first 50 features of
xt are sampled independently fromN(10yt, 100) with probability 0.5, and zero otherwise. The last
50 features are sampled independently from N (-5yt , 25) with probability 0.4, and zero otherwise.
The class label yt ∈ {-1, 1} are sampled randomly with equal probabilities.
We study BAG with B = 1, 2, 3, 4 and 100. For
B = 2, gradient gt is partitioned in the same way as
the input. For B = 3, we form the first block using
the first 35 coordinates, the second block with the
next 30 coordinates, and the third block with the
remaining 35 elements. For B = 4, gt is divided
into four blocks each of 25 elements. We initialize
θ1 to zero, fix = 10-8 and η = 0.01.
Figure 1 compares the expected regret, which is
estimated by averaging the regrets over 100 rep-
etitions. BAG with B = 2 and 4 achieve lower
regrets than the others (as B = 4 covers the case
for B = 2). BAG with B = 3 is a little worse as
its block structure is different from the input block
structure, but still performs better than B = d. For
B = 1, the mismatch in block structures is severe
and its performance is worst.
Figure 1: Expected regret with different B ’s
(note that curves for B = 2 and 4 overlap).
5.2	Real-World Data Sets
We introduce four block construction strategies: B.1: Use a single adaptive stepsize for each pa-
rameter tensor/matrix/vector. A parameter tensor can be the kernel tensor in a convolution layer, a
parameter matrix can be the weight matrix in a fully-connected layer, and a parameter vector can be a
bias vector; B.2: Use an adaptive stepsize for each output dimension of the parameter matrix/vector
in a fully connected layer, and an adaptive stepsize for each output channel in the convolution layer;
B.3: Use an adaptive stepsize for each output dimension of the parameter matrix/vector in a fully
8
Under review as a conference paper at ICLR 2020
connected layer, and an adaptive stepsize for each kernel in the convolution layer; B.4: Use an adap-
tive stepsize for each input dimension of the parameter tensor/matrix, and an adaptive stepsize for
each parameter vector. More details on the implementation can be found in Appendix A
We compare the proposed BAGM (with block construction approaches B.1, B.2, B.3, B.4) with
the following baselines: (i) Nesterov’s accelerated gradient (NAG) (Sutskever et al., 2013); and
(ii) Adam (Kingma & Ba, 2015). These two algorithms are widely used in deep networks. NAG
provides a strong baseline with good generalization performance, while Adam serves as a fast coun-
terpart with coordinate-wise adaptive stepsize.
As grid search for all hyper-parameters is very computationally expensive, we only tune the most
important ones using a validation set and fix the rest. We use a constant βt = β (momentum
parameter) and exponential increasing sequence S.3 with α = 0.999 for BAGM. For Adam, we
fix its second moment parameter to 0.999 and tune its momentum parameter. Note that with such
configurations, Adam is a special case of BAGM with B = d (i.e., weighted AdaEMA). For all the
adaptive methods, we use = 10-3 as suggested in (Zaheer et al., 2018). All the experiments are
run on a AWS p3.16 instance with 8 NVIDIA V100 GPUs.
5.2.1	CIFAR- 1 0
We train a deep residual network from the MXNet Gluon CV model zoo Guo et al. (2019) on the
CIFAR-10 data set. We use the 56-layer and 110-layer networks as in (He et al., 2016). 10% of
the training data are carved out as validation set. We perform grid search using the validation set
for the initial stepsize η and momentum parameter β on ResNet56. The obtained hyperparameters
are then also used on ResNet110. We follow a similar setup as in (He et al., 2016). Details are in
Appendix A.2. To reduce statistical variance, results are averaged over 5 repetitions.
	ResNet56	ResNet110	top-1 error (%)	top-5 error (%)
NAG	6.91 ± 0.15	6.28 ± 0.23	NAG	204	551
Adam	6.64 ± 0.30	6.35 ± 0.18	Adam	21.04	5.47
BAGM-B.1	6.26 ± 0.12	5.94 ± 0.09	BAGM-B.1	2079	543
BAGM-B.2	6.51 ± 0.14	6.27 ± 0.18	BAGM-B.2	20.90	5.39
BAGM-B.3	6.52 ± 0.33	6.31 ± 0.06	BAGM-B.3	20.88	5.52
BAGM-B.4	6.38 ± 0.40	6.02 ± 0.15	BAGM-B.4	20.82	5.48
Table 1: Testing errors (%) on CIFAR-10. Table 2: Validation set errors on ImageNet. The
The best results are bolded.	best results are bolded.
Table 1 shows the testing errors of the various methods. With a large = 10-3, the testing perfor-
mance of Adam matches that of NAG. This agrees with (Zaheer et al., 2018) that a larger reduces
adaptivity and improves generalization performance. It also agrees with Proposition 4 that the bound
is smaller when is larger. As for BAGM, it outperforms Adam for all block construction schemes
used. It also outperforms NAG with schemes B.1, B.2 and B.4.
Convergence of the training, testing, and generalization errors (absolute difference between training
error and testing error) are shown in Figure 2.2 As can be seen, on both ResNet models, BAGM-B.1
converges to a lower training error rate than Adam. This agrees with Corollary 2 that blockwise
adaptive methods can have faster convergence than their counterparts with element-wise adaptivity.
Moreover, the generalization error of BAGM-B.1 is smaller than Adam, which agrees with Proposi-
tion 4 that blockwise adaptivity can have a slower growth of generalization error. On both models,
BAGM-B.1 gives the smallest generalization error, while NAG has the highest generalization error
on ResNet56. Hence, the proposed methods can accelerate convergence and improve generalization.
5.3	ImageNet
In this experiment, we train a 50-layer ResNet model on ImageNet (Russakovsky et al., 2015). The
data set has 1000 classes, 1.28M training samples, and 50,000 validation images. As the data set
does not come with labels for its test set, we evaluate its generalization performance on the validation
2To reduce clutterness, we only show results of the block construction scheme BAGM-B.1, which gives the
lowest testing error among the proposed block schemes. The full results are shown in Figure 3.
9
Under review as a conference paper at ICLR 2020
Figure 2: Results on CIFAR-10. Top: ResNet56; Bottom: ResNet110. Note that the training error
(%) is plotted in log scale.
set. We use the ResNet50_v1d network from the MXNet Gluon CV model zoo. We train the FP16
(half precision) model on 8 GPUs, each of which processes 128 images in each iteration. More
details are in Appendix A.3. As it takes long time to train on ImageNet, we only run each algorithm
once.
Performance on the validation set is shown in Table 2. As can be seen, BAGM with all the block
schemes (particularly BAGM-B.1) achieve lower top-1 errors than Adam and NAG. As for the top-5
error, BAGM-B.2 obtains the lowest, which is then followed by BAGM-B.1. Overall, BAGM-B.1
has the best performance on both CIFAR-10 and ImageNet.
5.4	Word-Level Language Modeling
In this section, we train the AWD-LSTM word-level language model (Merity et al., 2018) on the
WikiText-2 (WT2) data set (Merity et al., 2017). We use the publicly available implementation in
the Gluon NLP toolkit Guo et al. (2019). We perform grid search on the initial learning rate and
momentum parameter as in Section 5.2.1, and set the weight decay to 1.2 × 10-6 as in (Merity et al.,
2018). Results are averaged over 3 repetitions. More details on the setup are in Appendix A.4. As
there is no convolutional layer, B.2 and B.3 are the same.
Table 3 shows the testing perplexities, the lower the better. As can be seen, all adaptive methods
achieve lower test perplexities than NAG, and BAGM-B.2 obtains the best result.
NAG	Adam	BAGM-B.1	BAGM-B.2	BAGM-B.4
65.75 ± 0.10^^65.40 ± 0.13^^65.42 ± 0.10^^65.29 ± 0.14^^65.55 ± 0.07
Table 3: Testing perplexities on the WikiText-2 data set. The best results are bolded.
6 Conclusion
In this paper, we proposed adapting the stepsize for each parameter block, instead of for each in-
dividual parameter as in Adam and RMSprop. Regret, convergence and uniform stability analyses
show that it can have lower regret, faster convergence and lower generalization error than its counter-
part with coordinate-wise adaptive stepsize. Experiments on synthetic dataset, image classification
and language modeling confirm these theoretical results.
10
Under review as a conference paper at ICLR 2020
References
L. B. Almeida, T. Langlois, J. D. Amaral, and A. Plakhov. Parameter adaptation in stochastic
optimization. In On-line learning in neural networks, pp. 111-134. Cambridge University Press,
1999.
L. Balles and P. Hennig. Dissecting adam: The sign, magnitude and variance of stochastic gradients.
In Proceedings of the International Conference on Machine Learning, pp. 404-413, 2018.
J. Bernstein, Y. Wang, K. Azizzadenesheli, and A. Anandkumar. signSGD: Compressed optimi-
sation for non-convex problems. In Proceedings of the International Conference on Machine
Learning, pp. 560-569, 2018.
B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin classifiers.
In Proceedings of the annual workshop on Computational learning theory, pp. 144-152. ACM,
1992.
L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning.
SIAM Review, 60(2):223-311, 2018.
O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research,
2(3):499-526, 2002.
N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning
algorithms. IEEE Transactions on Information Theory, 50(9):2050-2057, 2004.
J. Chen and Q. Gu. Closing the generalization gap of adaptive gradient methods in training deep
neural networks. arXiv preprint arXiv:1806.06763, 2018.
X. Chen, S. Liu, R. Sun, and M. Hong. On the convergence of a class of adam-type algorithms for
non-convex optimization. In Proceedings of the International Conference for Learning Represen-
tations, 2019.
J. Duchi and Y. Singer. Efficient online and batch learning using forward backward splitting. Journal
of Machine Learning Research, 10(12):2899-2934, 2009.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12(7):2121-2159, 2011.
B. S. Everitt. The Cambridge dictionary of statistics. Cambridge University Press, 2006.
S. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic pro-
gramming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks.
In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, pp.
6645-6649, 2013.
J.	Guo, H. He, T. He, L. Lausen, M. Li, H. Lin, X. Shi, C. Wang, J. Xie, S. Zha, A. Zhang, H. Zhang,
Z. Zhang, Z. Zhang, and S. Zheng. Gluoncv and gluonnlp: Deep learning in computer vision and
natural language processing. arXiv preprint arXiv:1907.04433, 2019.
M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient
descent. In Proceedings of the International Conference on Machine Learning, pp. 1225-1234,
2016.
K.	He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the International Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.
D.	Kingma and J. Ba. Adam: A method for stochastic optimization. In Proceedings of the Interna-
tional Conference for Learning Representations, 2015.
I. Loshchilov and F. Hutter. SGDR: Stochastic gradient descent with warm restarts. In Proceedings
of the International Conference on Learning Representations, 2017.
11
Under review as a conference paper at ICLR 2020
L. Luo, Y. Xiong, Y. Liu, and X. Sun. Adaptive gradient methods with dynamic bound of learning
rate. Proceedings of the International Conference on Learning Representations, 2019.
H. B. McMahan and M. Streeter. Adaptive bound optimization for online convex optimization. In
Proceedings of the Annual Conference on Computational Learning Theory, pp. 244, 2010.
S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. In Proceedings
of the International Conference on Learning Representations, 2017.
S. Merity, N. S. Keskar, and R. Socher. Regularizing and optimizing LSTM language models. In
Proceedings of the International Conference on Learning Representations, 2018.
A. Nemirovski and D.B. Yudin. Problem Complexity and Method Efficiency in Optimization. Wiley,
1983.
S.	J. Reddi, S. Kale, and S. Kumar. On the convergence of adam and beyond. In Proceedings of the
International Conference for Learning Representations, 2018.
H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statis-
tics, 22(3):400-407,1951.
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,
M. Bernstein, A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recognition challenge.
International journal of computer vision, 115(3):211-252, 2015.
T.	Schaul, S. Zhang, and Y. Lecun. No more pesky learning rates. In Proceedings of the International
Conference on Machine Learning, pp. 343-351, 2013.
O. Shamir and T. Zhang. Stochastic gradient descent for non-smooth optimization: Convergence re-
sults and optimal averaging schemes. In Proceedings of the International Conference on Machine
Learning, pp. 71-79, 2013.
B. Singh, S. De, Y. Zhang, T. Goldstein, and G. Taylor. Layer-specific adaptive learning rates
for deep networks. In Proceedings of the International Conference on Machine Learning and
Applications, pp. 364-368, 2015.
I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum
in deep learning. In Proceedings of the International Conference on Machine Learning, pp. 1139-
1147, 2013.
T. Tieleman and G. Hinton. Lecture 6.5 - RMSProp, COURSERA: Neural networks for machine
learning, 2012.
R. Ward, X. Wu, and L. Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes,
from any initialization. arXiv preprint arXiv:1806.01811, 2018.
A. C. Wilson, R. Roelofs, M. Stern, N. Srebro, and B. Recht. The marginal value of adaptive
gradient methods in machine learning. In Advances in Neural Information Processing Systems,
pp. 4148-4158, 2017.
Y. You, I. Gitman, and B. Ginsburg. Large batch training of convolutional networks. arXiv preprint
arXiv:1707.03888, 2017.
A. W. Yu, Q. Lin, R. Salakhutdinov, and J. Carbonell. Normalized gradient with adaptive stepsize
method for deep neural network training. arXiv preprint arXiv:1707.04822, 2017.
M. Zaheer, S. Reddi, D. Sachan, S. Kale, and S. Kumar. Adaptive methods for nonconvex optimiza-
tion. In Advances in Neural Information Processing Systems, pp. 9793-9803, 2018.
W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neural network regularization. arXiv preprint
arXiv:1409.2329, 2014.
M. D. Zeiler. ADADELTA: An adaptive learning rate method. Preprint arXiv:1212.5701, 2012.
12
Under review as a conference paper at ICLR 2020
C. Zhang, Q. Liao, A. Rakhlin, K. Sridharan, B. Miranda, N. Golowich, and T. Poggio. Theory of
deep learning iii: Generalization properties of sgd. Technical report, Center for Brains, Minds
and Machines (CBMM), 2017.
H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization.
In Proceedings of the International Conference on Learning Representations, 2018.
S. Zheng and J. T. Kwok. Follow the moving leader in deep learning. In Proceedings of the Inter-
national Conference on Machine Learning,pp. 4110-4119, 2017.
Z. Zhou, Q. Zhang, G. Lu, H. Wang, W. Zhang, and Y. Yu. Adashift: Decorrelation and convergence
of adaptive learning rate methods. arXiv preprint arXiv:1810.00143, 2018.
F. Zou and L. Shen. On the convergence of weighted adagrad with momentum for training deep
neural networks. arXiv preprint arXiv:1808.03408v2, 2018.
F. Zou, L. Shen, Z. Jie, W. Zhang, and W. Liu. A sufficient condition for convergences of adam and
rmsprop. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 11127-11135, 2019.
A Experimental Setup
A. 1 Implementation Details
As {at} is non-decreasing, the accumulated sum At can grow significantly, which may potentially
cause some numerical issue. In practice, using steps 7 and 8 in Algorithms 4, we equivalently rewrite
the update in (12) as the following exponentially moving update:
vt,b = αtvt-1,b + (I - αt) "'t,Gb "2 ,
db
where α= = 1 一 at/At. If at = α-t, then α= 二 a(1 一 αt-1)∕(1 — at). Based on Proposition 3,
this setting leads to an O(1) bound. On the other hand, if at = tτ, then we have at/At = O(1/t).
This suggests that we can use polynomial-decay averaging αt = 1 一 (c+ 1)/(t + c) for some c ≥ 0
(Shamir & Zhang, 2013), whereas c > 0 reduces the weight of earlier iterates compared to later
ones. The larger c corresponds to the larger τ. In this case, as PtT=1 at = O(Tγ) for some γ > 0,
we have a convergence rate of O(log(T )/T ).
A.2 CIFAR-10
The CIFAR-10 data set has 50,000 training images and 10,000 testing images. As in (He et al.,
2016), we employ data augmentation for training. We first pad the input picture by adding 4 pixels
on each side of the image. Then, a 32 × 32 crop is randomly sampled from the padded image with
random horizontal flipping. A mini-batch size of 128 is used. The stepsize is divided by 10 at the
39k-th and 59k-th iterations. We use a weight decay of 0.0001.
For NAG, the initial stepsize η is chosen from {0.01, 0.05, 0.1, 0.5, 1}. For the adaptive methods,
we have η ∈ {0.0001, 0.0005, 0.001, 0.005, 0.01}. The momentum parameter is searched over
{0, 0.5, 0.9}. The learning rate is multiplied by 0.1 at the 100th and 150th epochs. We perform
grid search on the hyper-parameters by running each algorithm for 200 epochs on ResNet56. The
hyper-parameters that give the highest accuracy on the validation set are employed. The testing
performance is obtained by running each algorithm with its best hyper-parameters on full training
set for 400 epochs. The same obtained hyperparameters are then used on training ResNet110. When
NAG is applied to ResNet110, we use a smaller learning rate at the beginning to warm up the
training. Specifically, the obtained learning rate is divided by 10 in the first 4000 iterations, and then
go back to the original one and continue training. The grid search results are shown in Table 4.
Figure 3 shows that, on ResNet56, BAGM converges to a lower training error rate than Adam for all
schemes used. For the deeper ResNet100 model, BAGM-B.1 and B.4 have faster convergence than
Adam, while BAGM-B.2 and B.3 show the same convergence speed with Adam.
13
Under review as a conference paper at ICLR 2020
	η	β
NAG	0.5	0.9
Adam	0.005	0
BAGM-B.1	0.005	0
BAGM-B.2	0.005	0
BAGM-B.3	0.005	0
BAGM-B.4	0.005	0
Table 4: The best η and β obtained by grid search on CIFAR-10.
Figure 3: Results on CIFAR-10 for the proposed method with all four block construction schemes
and the baselines. Top: ResNet56; Bottom: ResNet110. The training error (%) is plotted on a
logarithmic scale.
A.3 ImageNet
In this experiment, we employ label smoothing and mixup (Zhang et al., 2018). The cosine schedule
(Loshchilov & Hutter, 2017) for learning rate is used. A warmup of 5 epochs is applied. During
validation, we use the center crop. Hyperparameter tuning is based on the obtained results in Sec-
tion A.2. Specifically, for NAG, the initial learning rate is chosen from {0.4, 0.5}, and momentum
parameter is fixed to 0.9. For Adam and BAGM, we have the initial learning rate η ∈ {0.004, 0.005},
and we use momentum parameter β = 0. A weight decay of 0.0001 is used (weight decay is not
applied to bias vectors, and parameters for batch normalization layers) 3. The best learning rates for
each method are shown in Table 5.
A.4 Word Language Modeling
In this experiment, we follow the same setting in (Merity et al., 2018). A 3-layer AWD-LSTM
is considered. The model is unrolled for 70 steps, and a mini-batch of size 80 is used. We
clip the norm of the gradients at 0.25. The details of the configuration used in this experiment
can be found in https://github.com/dmlc/gluon-nlp/blob/master/scripts/
3The example script for running NAG with η =	0.4 can be found in https://raw.
githubusercontent.com/dmlc/web-data/master/gluoncv/logs/classification/
imagenet/resnet50_v1d-mixup.sh. Details of the data augmentation can be found in
https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/
imagenet/train_imagenet.py.
14
Under review as a conference paper at ICLR 2020
	η
NAG	0.4
Adam	0.004
BAGM-B.1	0.004
BAGM-B.2	0.004
BAGM-B.3	0.004
BAGM-B.4	0.004
Table 5: The best η obtained by grid search on ImageNet.
language_model/word_language_model.py. For completeness, we show the model con-
figuration in Table 6.
	dimensionality/dropout rate
embedding size	400
hidden size	H50
dropout	04
dropout for RNN layers	0.2
dropout for input embedding layers	065
dropout to remove words from embedding layer	01
weight dropout	0.5	―
Table 6: Model configuration of AWD-LSTM model.
As the WikiText-2 data set comes with a validation set, we perform grid search by evaluating the
performance on the validation set. For NAG, the initial stepsize is chosen from {1, 3, 10, 30}. For
the adaptive methods, we select η ∈ {0.1, 0.03, 0.01, 0.003}. The momentum parameter varies
in {0, 0.5, 0.9}. The learning rate is multiplied by 0.1 when the validation performance does not
improve for 30 consecutive epochs. We tie the word embeddings and softmax weights. For each
algorithm, we employ the iterate averaging scheme proposed in (Merity et al., 2018). The model
is trained for 750 epochs. The hyper-parameters obtained by grid search are shown in Table 7. In
general, B.1 and B.4 are not suitable for updating the word embedding matrix as word frequency
varies a lot and thus the gradient is highly sparse. However, the gradient becomes dense when we
use weight tying. In modern toolkits such as Tensorflow, MXNet, and Pytorch, the weight matrices
of the LSTM gates are concatenated to speed up matrix-vector multiplication. We need to apply B.1
and B.4 to these weight matrices separately.
	η			β
NAG	30	0
Adam	0.03	0.5
BAGM-B.1	^003^	^O^
BAGM-B.2	0.03	0.5
BAGM-B.4	0.03	0.5
Table 7: The best η and β obtained by grid search on the word language modeling experiment.
B Gradients in Parameter Block
As discussed in Corollary 2, BAGM can have faster convergence than its coordinate-wise counter-
part when {。2}记@b have low variability. In this section, We verify this experimentally. Using the
setup in Section 5.2.1, we focus on BAGM-B.1, which shows the fastest convergence. At the end of
each epoch, we perform 10 full data passes with random shuffle and data augmentation (as described
in Appendix A.2) to compute E[g2] and E[∣∣gg∕∣2]∕db. We then approximate σ2 and σb2 by their
empirical maxima over all epochs. Figure 4 shows the coefficient of variation of {σl}i∈Qb, which
is defined as the ratio of the standard deviation to the mean (Everitt, 2006). As can be seen, around
86% (resp. 75%) of all blocks for ResNet56 (resp. ResNet110) have coefficient of variation smaller
than 1, indicating that {σ2}i∈g5 have low variance and concentrate around the mean.
15
Under review as a conference paper at ICLR 2020
We also compute the empirical values of rmin ≡ min(r1, r2, r3) and
maxb maXi∈Gb
maxb G2 + e2
Gi2+2
in
Corollary 2. We use the two symbols (vT,B and C(T)) that are defined in Appendix F for the
corresponding main theorem. Moreover, We estimate A ∕vτ,d f, where VTd = VTB=d and
vτ,B+ e	,	,
VTB = VTB=B, instead of J
maxb maXi∈Gb。2 + e2
maxb G2 + e2
~ / 一、	,	.	H. / 一、
,as C(T) is tighter than C(T). We estimate
VT B using max1≤t≤T maxb Vt,b. The results are shown in Table B.
		ReSNet56				ReSNet110		
	/ Vτ,d + e2 V vτ,B + e2	min(r1,r2,r3)	/ Vτ,d + e2 V vτ,B +e2	min(r1,r2 ,r3)
B.1	3.70	102	3.30	101
B.2	1.50	1.06	1.73	1.05
B.3	1.47	1.01	1.39	1.01
B.4	2.27	1.17	2.47	1.13
Table 8: Empirical estimate of JVT"；；： and min(r1, r2, r3).
As can been seen, all min(r1,r2,r3) are larger than 1, indicating {σ2}i∈gb have low variability
for all the proposed blocks, which agree with Corollary 2 and explain why the proposed blockwise
adaptivity leads to faster convergence. Moreover, B.1 has largest JVT,d+；2. Theoretically, it means
B.1 should lead to fastest convergence, which is empirically verified in Figure 3. Similarly, B.2 has
second largest
and is therefore the second fastest.
/ Vτ,d + e2-
V vτ,B + e2
Figure 4: Coefficient of variation of {σ2}i∈gb for all the blocks with B.1. The blocks with higher
indices in the abscissa belong to deeper layers.
C Proof of Proposition 1
Proof. In this proof, we use denominator layout for matrix calculus. As all the activation func-
tions are bijective and {Wk}kL=l+1 are invertible, Φl is bijective and has an inverse function Φl-1.
Specifically, Φl-1 is given by
Φ-(Y) = φ-(…Φ--2(Φ--1(YW-1 )W--1)…W1+1).
Then, with the assumption that Hl-1 has full row rank, the nonconvex objective (5) can be reformu-
lated as the following convex problem:
minkHl-1Wl -Φl-1(Y)k22.	(14)
Wl
It is obvious that its large margin solution is HlT-1 (Hl-1HlT-1)-1Φl-1 (Y). In the sequel, we will
see that every critical point of (5) is a global optimal solution. Let hi,l-1 denotes a column vector
16
Under review as a conference paper at ICLR 2020
that is the i-th row of Hl-1 and Z:,i be the i-th column of matrix Z. The gradient of (5) is
d
2Hl-ι XDiag(Φι(H1-1 Wi):,k - Y,QGk,ι = HLEι,
k=1
where Gk,ι =	[Vχ=hτ,ι-ιwlφι(χ)k；…；Vχ=hτ,ι-1wlφι(χ)k]	∈ Rn×d and El =
2 Pk=ι Diag(Φl(Hl-ιWl1,k — K,k)Gk,ι to be the error matrix. As Hl-ι has full row rank, then
clearly gradient is zero if only and if El = 0. By the definition of Gk,l, we can see that El = 0 if
only and if Φl(Hl-1Wl) = Y when Vx=hT WlΦl(x) has full row rank for all i ∈ [n]. Note that
the gradient Vx=hT WlΦl(x) is of the following form:
vx=hTl-1Wι φl(X) = (WL ◦ φL-l(hTL-2 WL-I)T IT )T …(Wl + 1 ◦ φ'l(hT,l-1Wl )TIT )T,
where ◦ is the Hadamard product. For all k ∈ {l, . . . , L - 1}, as Wk+1 has full rank and φ0k(z) 6= 0
for any Z ∈ R, we have that Wk+ι ◦ φk (hTk-ιWk )T 1T has full rank. Applying the fact that the mul-
tiplication of a number of invertible matrices preserves full rank, we obtain that Vx=hT Wl Φl (x)
has full rank. Therefore, every critical point satisfies Φl(Hl-1Wl) = Y and every critical point is a
global optimal solution.
Let it be the index chosen at iteration t and yit be the it-th row of Y . Let us define et,l =
2 Pdk=1(Φl(hiTt,l-1Wt,l)k -yit,k)Vx=hiT,l- Wt,lΦl(x)k. Now, we prove that if the following update
rule applied on (5) finds a critical point, then the iterate converges to the largest margin solution.
Wt+1,l = Wt,l -ηt,lhit,l-1et,l = H-I 卜 X ηj,lEj,l) ,	(15)
where we use Wl,1 = 0, ηt,l is the stepsize for l-th layer at iteration t, and Ej,l is a matrix in which
its ik-th row is ej,l and all the other rows are zeros. Then, the solution found by (15) lies in the span
of rows of Hl-1. In other words, the solution has the following parametric form:
Wl = HlT-1 αl
for some αl ∈ Rn. Thus, if (15) is converging to a critical point in expectation, then we have
Wt,l → W*,l as t → ∞, where W*,l = H-ια* l for some optimal a*,l. Since every critical point
is an optimal solution, then W*,l is also a solution to (14), and We have
Φ-1(Y) = Hl-iW.,l = Hl-iH-ια*,l.
We solve for a*,l and obtain
a*,，= (Hl-iHTjT Φ-1(Y).
Therefore, W*,l = Hl-1(Hl-1Hl-1)Tφ-1(Y).	□
D	Proof of Proposition 2
Proof. Let (xit, yit) be the pair of sample selected at iteration t. The stochastic gradient of least
square problem (4) at the t-th iteration is
2(xit θt - yit )xit = X et,
where we define et to be the error vector with value 2(xiT θt - yit) in the it-th coordinate and zeros
elsewhere. For each block b, BAG with θ1 = 0 uses the following update rule:
θt+1,Gb = θt,Gb - ηt,b X:T,Gb et = X:T,Gb - Xηi,bei ,
17
Under review as a conference paper at ICLR 2020
where %b = η∕( JPt=ι ∣∣XTGbeik2/db + e). Then, each subvector of the solution found by BAG
lies in the span of rows of X:,Gb. In other words, each subvector of the solution is of the following
parametric form:
θGb = X:T,Gb αb
for some αb ∈ Rn. Combining with Theorem 1 and online-to-batch conversion (Cesa-Bianchi et al.,
2004), BAG is converging in expectation ɪ Pt=1 θi → θ* as t → ∞, where。*& = XTGba*,b for
some optimal α*,› Since θ* is a solution to (4), We have
B
y = χθ* = £X：,Gb XTGb α*,b.
b=1
Assume that each submatrix X:,Gb has full row rank, then X:,Gb X:T,G is invertible, we can solve for
α*,b'sandobtain
α*,b = (X：,Gb XTGb)Tub
for some ub ∈ Rn and PB=I ub = y.	□
E Proof of Theorem 1
Lemma 1. Let {θt} be the Sequence generated by the Algorithm 2. Define St = [(√vt,ι +
e)1τ,..., (√vt,B + E)ITB ]T. Let Ht = Diag (St). Then, for any θ, we have
ft(θt) - ft(θ) ≤ 2-kθt - θkHt - 2llθt+ι - θkHt + 2kgtkH-ι.
2η	2η	2 t
Proof. For any θ, the convexity of ft indicates that
ft (θt) - ft (θ)
≤ hgt, θt - θi
hgt, θt+1 - θi + hgt, θt - θt+1i
一 hθt+ι - θ, Ht(θt - θt+ι)i + hgt,θt - θt+1i
1功1功 42
--+
ttt
2H 2H 2H
kkk
θθθ
---
111
+++
ttt
θθθ
1而1而1而
-
2Ht
kθ
-
kθt
切 I
=≤=
tt
H 2H
kθ
-
tt
θ
1 - θtk2Ht + hgt, θt - θt+1i
+ι- θMHt + 2η llθt+ι- θMHt + 2 llgtkH -1
2
Ht-1,
where the second to last inequality follows from Fenchel’s inequality applied to the conjugate func-
tions 看k∙kHt and 2卜哙”	口
Lemma 2. Considering an arbitrary R-valued sequence {ai} and its vector representation a1:t =
[a1, . . . , at], we have
T
X
t=1
a
kalM∣2
≤ 2ka1:Tk2.
Proof. The lemma can be proved by induction. The lemma trivially holds when T = 1. Assume the
lemma holds for T - 1, we get
X U at U	≤ 2kai:T-1∣∣2 + U a、]
t=1 ka1:tk2	ka1:T k2
=2√Z - X + √z,
18
Under review as a conference paper at ICLR 2020
where we define Z = ka1:T k22 and x = a2T . As the RHS is non-increasing for x ≥ 0. We can set
X = 0 to maximize the bound and obtain 2 √Z.	□
Lemma 3. Let Ht be defined as in Lemma 1. Denote g1:t,Gb = [g1T,G , . . . , gtT,G ]T. We have
T	B
X kgtkH-1 ≤ 2 Xpdbkg1：T,Gbk2.
Proof.
TT
X kgtk2H-1	≤ Xhgt,Diag(st)-1gti
TB
Xt=1Xb=1
√dbkgt,Gbk2
kg1:t,Gb k2
B	T
Xpdb X
b=1	t=1
kgt,Gbk2
kg1:t,Gb k2
≤
B
2E dbdb'1(T^Tkb k2.
b=1
where the last inequality follows from the Lemma 2 by setting ai = kgi,Gb k22.
□
E.1 Proof of Theorem 1
Proof. By summing UP the equation in Lemma 1 with θ = θ*,we obtain
T	1	1 T-1	T
Xft(θt) - ft(θ*) ≤ 2ηkθι - θ*kHι + 2η X [kθt+ι - θ*kHt+ι - kθt+ι - θ*kHt] + 2 X IIgtkH-1.
By the construction of Ht, we have that Ht+1	Ht . Then, we get
kθt+ι- θ*kHt+ι - kθt+ι- θ*kHt
=hθt+1 - θ*, Diag(St+1 - st)(θt+1 - θ*)i
B
= E kθt+1,Gb - θ*,Gb ∣∣2(√vt+1,b - √vtb)∙
b=1
Given the above result, we have
T-1
X [kθt+ι- θ*kHt+ι - kθt+ι- θ*kHt]
t=1
B T-1
ΣΣ kθt+1,Gb - θ*,Gb k2(√vt+1,b - √vt≠)
B T-1	B
ΣΣ kθt+1,Gb - θ*,Gbk2(√vt+ι,b- √vt≠) + 三 kθι,Gb- θ*,Gbk2(Vw,b - √v1,b)
BB
≤ X Db√VTb — X kθl,Gb-θ*,Gbk2g∙
b=1	b=1
Recall that vτ,b = kgi：T,Gbk2∕db. Let E = 0. Combining Lemma 3 Withthefactthat |仇 一 θ*kH
PB=I kθl,Gb -θ*,Gbk2√17,we have
T
X ft(θt) - ft(θ*)
t=1
1 Λ D2	Λ L
≤	2- £ √^kgLT,Gb k2 + n^2Vdbkg1：T,Gb k2 ∙
2η b=1	db	b=1
□
19
Under review as a conference paper at ICLR 2020
E.2 Proof of Corollary 1
Proof. Taking expectation of the gradient terms in (9), we have, for all b’s,
T
E[kg1:T,Gb k2] ≤ t X X E[g2,i] ≤ QhpdT
i∈Gb t=1
Let B = B . Then, (9) reduces to
一 一亘「1 C IL
e[R(t)] ≤ V Qb 钎D + nd Tτ.
2η
b=1
□
F Main Theorem
Let σt,b = /E∕∣gt,Gbk2]. We define a sequence of virtual estimates of the second moment:
vt,b = A (vt-ι,b + dt Et[∣∣gt,Gb∣∣2D
1 (X	kgi,Gb k2 ,	σ2,b∖ V7 γr1
=工邑 ai -^b-+at 匹l∀b ∈ [B].
Let ^t,b ≡ maxι≤t≤τ Emaxb vt,b]
Theorem 2. Suppose that Assumptions 3-7 hold. Let P = β2/p. We have
1m≤T(E[kVF(θt)k4/3])3/2 ≤ C(T),
(16)
where4
B
T
C(T)
a∕2(Vt,B + e2)
nτ T
B
+	Cb0 w1 log
b=1
2C2
(1 -β)C1
C0 + C4
β =
P)
Co = F(θι) - F(θ*), C4
β∕(1-β)	+ 1.
√Ca A1 /A2 (1-P)
(σ22+1)+ω X
2C2
Cf √Ca(i-√ρ)(i-β),
LC23w1db
C(1 _.P2 +
Qbdb	wt
b=1	t=2
2C3C2σbd
Ci
AAb-1
, and C3
(17)
When B = d, the bound here is tighter than that in (Zou et al., 2019), as we exploit heteroge-
neous second-order upper bound (Assumption 4). Note that constants C0 , C3, C4 are not related to
block partition. They only depend on initial optimality gap and sequences {ηt }, {at }, {βt }. In the
following, we introduce several lemmas to prove the Theorem 2.
In the sequel, we define Ht as
Ht = Diag(st),
where	____ ___________________
st = [(PvtJ + E)ITL , . . . , (Pvt,B + E)ITB ]T.
Let δt = θt+ι 一 θt = -ntmt/st. We introduce Ht as
Ht = Diag(St),
where vt,b is defined in (16) and
st =	[(PvtJ + E)ITL , . . . , (Pvt,B + E)ITB ]T .
Assume that σt,b/√db ≤ Qb for all t and let Σ = Diag([σ21T,..., σB ITB]t).
4When T = 1, the second term in C(T) (involving summation from t = 2 to T) disappears.
20
Under review as a conference paper at ICLR 2020
Lemma 4. Let St = S0 + Pit=1 ai, where {at } is a non-negative sequence and S0 > 0. We have
T
X tτ ≤ Iog(ST) - Iog(SO)
t=1 St
Proof. The concavity of log leads to log(b) ≤ log(a) + a (b - a) for all a, b > 0. This suggests that
a-b
a
≤ log(a) - log(b)
Hence, we have
T
T
X S=X
t=1 t	t=1
St - St-1
-St-
≤ Xlog (SStɪ) = log(Sτ) - log(So).
□
Lemma 5. Let {at} and {st} be two real number sequences, and let St = Pit=1 si. Then, we have
T
atst
t=1
T-1
X(at -
t=1
at+1)St + aTST.
Proof. Let S0 = 0. Expanding the summation, we obtain
T
atst
t=1
T
X at(St - St-1)
t=1
T-1	T-1
atSt -
t=1	t=1
at+1 St + aT ST
T-1
(at - at+1)St + atST
t=1
□
Lemma 6. Assume {at} is non-decreasing such that {At-1/At} is non-decreasing. Define wt =
ηt/ʌ/AF. Assume Wt is "almost” non-increasing. This means there exists another non-increasing
sequence {zt} and positive constants C1 and C2 such that C1zt ≤ wt ≤ C2zt. Then,
Wt ≤ C2/C1 Wi and ηt ≤ C2∕C1η
for all i < t.
Proof. For any i < t,
Wt ≤ C2zt ≤ C2zi ≤ C2/C1Wi.
Then,
ηt ≤
Ci √oiTAi
C2，1 - At-l/At
CI p1 - Ai-1/Ai
ηi ≤ C2/Cim.
□
Lemma 7. Assume that {at} is non-decreasing. For any block diagonal matrix C
Diag([c11dT , . . . , cB 1dT ]T) with cb ≥ 0for all b, we have
T
X E AIHTgtkC
t=1	t
B
≤	cbdb log
b=1
+1)+log (⅛ X ai+1)].
21
Under review as a conference paper at ICLR 2020
Proof.
T
X A kH-1gtkC
t=1 At
TB
Xt=1Xb=1
TB
Xt=1Xb=1
BT
Xb=1Xt=1
B
CbA kgt,Gb k2
Vt,b + e2
cbatkgt,Gbk22
Pt=I a，ikgi,Gbk2∕db + Ate2
cbdb
b=1 t=1
BT
XcbdbX
b=1 t=1
atkgt,Gbk2
Pi=I aikgi,Gbk2 + dbΑte2
atkgt,Gbk2
Pti=1 aikgi,Gbk22 + dba1e2
≤
≤
T
2
2
2
Hence,
T
X A kH-1gtkC
t=1 At
B
≤	cbdb
b=1
log X aikgi,Gb k22 + dba1e2 - log(dba1e2) ,
where the inequality follows from Lemma 4. Using Jensen’s inequality, we get
XE AkH-1gtk
t
t=1
B
cbdbE log
b=1
B
cbdb
b=1
B
cbdb
b=1
B
X aikgi,Gb k22 + dba1e2 - log(dba1e2)
log XaiE[kgi,Gbk22] + dba1e2 - log(dba1e2)
log dbσb2	ai + dba1e2	- log(dba1e2)
cbdb log
b=1
i=1
XT ai+1! .
i=1
T
2
C
≤
≤
≤
Using the inequality log(1 + ab) ≤ log(1 + a + b + ab) = log(1 + a) + log(1 + b) for a, b ≥ 0, we
have
T
XE AkH-1gtkC
T - I	*—
B
cbdb
b=1
+1
i(aɪX ai+3
≤
□
Lemma 8. Assume that {at} is non-decreasing. Define Wt = nt/，A. Assume Wt is ”al-
most” non-increasing. This means there exists another non-increasing sequence {zt} and posi-
tive constants C1 and C2 such that C1zt ≤ Wt ≤ C2zt for all t. For any block diagonal matrix
C = Diag([c11dT , . . . , cB 1dT ]T) with cb ≥ 0 for all b, we have
XntE ]∏kH-1gtkC
≤	Cl
B
W1	cbdb log
b=1
+1
BT
+	cbdb
b=1	t=1
I at	At
A∖ At At-I + a1
22
Under review as a conference paper at ICLR 2020
Proof. Let ξt = T IlH-Ig⅛∣∣C, then ζt = Pt=ι ξi. Lemma 5 indicates that we have
τ	,——	T
X 阴号 M1gtkC = X wtξt
t=ι Vt	t=ι
T
≤。2 X Ztξt
t=1
-T-1	-
= C2 E(Zt - Zt+1)ζt + ZTζτ
t=1
Define Mt = PB=ICbdb
Mt. Then,
[log (σ⅛ + 1) + log g Pi=I ai + 1)]. ByLemma 7, we have E[&] ≤
X ηte]∏IlHtTgt IC
「T-1
≤	C2	^X (Zt- Zt+1)E[Ct] + zTE[Ct]
t=1
≤	C2
Zt - zt+ι)Mt + ZTMT ,
where the last inequality follows from the assumption that Zt ≥ zt+ι. Then,
XηtE ]∏∣H-1gt∣C
-T-1	-
X (Zt- Zt+1)Mt + ZTMt
t=1
T
X Zt(Mt- Mt-I) + z1m0
t=1
B
Z1	Cbdb log
b=1
B
(σ2 + 1) + X Zt X Cbdb log
∖'	)	t=1	b=1
At + ai
√4t-ι + ai
W1 ECbdb log
b=1
TB
+ 1 I + EwtE Cbdb log
At + a1
t=1
b=1
4-1 + α1
≤ C
≤	C1
AS log(1 + x) ≤ x for x > -1 and the fact that At ≥ At-1, We get
log
At +。1
At-I + q1
1 Λ l At + q1	八 / At + q1
log 1 + ɪ 1 ≤ ~ι τ
∖	At-1 + q1	a At-I + q1
-1
Qt
At-I + q1
Hence,
T	L I——
XηtE ]ʌ/A^kH-IgtIlC
W1
W1
B
Cbdb log
b=1
B
Cbdb log
b=1
+ 1) + ɪ^wtɪ^ Cbdb log
At + Q1
At-I + q1
At
Lemma 9. Let %,b
.For each block b and t ≥ 2, we have
≤ ⅛
≤ H
T
B
□
δt - h	βn: — δt-1)	= -(1 - βt)ηt,bgt,Gb + ηt,bAd⅜t,Gbk2 Xt,b + ηt,b等匕,b + Zt,b,
√1 - at∕Atηt-1	G G	√vt,b+ E	√db
23
Under review as a conference paper at ICLR 2020
where
Xt,b
_______________________ + (I - Bt)gt,Gb
√vtj + VZAt-1vt-1,b∕At_√¾b + √¾T
Btmt-IG
Yt,b
A√d kgt,Gbk2	βtmt-1,Gb
Atbkg kgt,Gb k2
品 σt,b
Pvt,b + C	PAt-I逸t-1,b∕At + C Pvt,b + PAt-Ivt-1,b∕At pvt,b + PAt-Ivt- 1,b∕At
—
A gt,Gb
't)受
Zt,b
βtηtmt-ιG G
t-1vt-1,b/At + c)(、/ At-Ivt-1,b∕At + A At-1/AtC)
Proof. For any t ≥ 2,
δt -
βtηt
A At-ι∕ Atηt-
—
ηtmt +
St
-δt-1
1
βtηtmt-i
-ηt
—
mt
St
、/ At-I/At st-1
βtmt-1
—
、/At-1/AtSt-I
(1 - βt)ηtgt
F + C
-βtηtmt-i
1
1
—
-βtηtmt-i
—
1
Vvt + C	、/ At-Ivt-1/At + C
1
—
、/ At-I St-IlAt + C	、/ At-ISt-1/At + 、/ At-1/AtC
(1 - βt)ηtgt
F + C
-βtηtmt-i
1
1
—
+βtηtmt-i
Vvt + C	、/ At-Ivt-1/At + C
(1 - PAt-1/At) C
(、/ At-I vt-1∕At + €)(、/ At-Ivt-1∕At + 、/ At-I IAt €)
Let expand the first term of (18) as
(1 - βt)ηtgt
(1 - βt)ηtgt
F + C
Vvt + C
(1 — βt)ηtgt
+ (I - βt)ηtgt
—
1
For each block b, we have
(18)
VvH + C
+ (1 - βt)ηtgt
C Vt + C
vt - vt
(I - Bt)ηtgt,Gb
VzvtJ + C
=上普”+ (1-βt)ηtgt,Gb
V vt,b + C
a⅛ e；b- kgt,gj∣2)
(1 - βt)ηt,b9t,Qb + ηt,b
σt,b	Agt,Gb	(I - Bt)√⅛
*t* 亨⅛* P-4)
V vt,b + C V vt,b + V vt,b
24
Under review as a conference paper at ICLR 2020
Then, we expand the second term of (18):
1
βtηtmt-ι」I
√vt + E
、/ At-Ivt-1∕At + E
、/At-Ivt-1/At - √vt
βtηtmt-i
—
βtηtmt-i
(√vt + E)( ʌ/At-Ivt-1/At + E)
At-Ivt-1/At - vt
(√vt + E)(PAt-Ivt IAt + e)(√vt + PAt-Ivt-1/At)
Similarly, for each block b, we have
βtηtmt-i,G
11
ʌ/ At-Ivt-1,b/At + E
-β η m ：__________________ atl∣gt*k2/(Atdb)	_
，b (Pvt,b + E)(PAt-Ivt-1,b∕At + E)(Pvt,b + PAt-Ivt-1,b∕At)
-βtηtmt-1,Gb
________________αt∣∣gt,Gb∣∣2/(Atdb)
(Pvt,b + E)( Pvt,b + E)( Pvt,b + PAt-Ivt-1,b∕At)
αtkgt,Gb k2/(Atdb)	1
(Pvt,b + E)(Pvt,b + PAt-Ivt-1,b∕At) I PAt-Ivt-1,b∕At + E
))
〜A⅛τ∣∣gt,Gb k2
-ηt,b	F + E
βtmt-1,Gb_______
vt,b + P At-Ivt-1,b∕At
~	σt,b
-%b淘
Aa⅛ ∣gt,Gbk2	βtmt-1,Gb
yAt⅛F ιιgt,Gb ∣2
A⅛τσt.b
Pvt,b + E	PAt-I逸t-1,b∕At + E Pvt,b + PAt-Ivt-1,b∕At Pvt,b + PAt-Ivt-1,b.
—
1
V7¾b + E
Combining (19) and (20) into (18), we obtain the result.
□
Lemma 10. Suppose that {at} is a non-decreasing SeqUenCe and At = P；=1 at SUCh that
{At/At+1} is non-decreasing and limt→∞ AAt^ = p > 0. Let At,i = Qj=-+1 AA-I for 1 ≤ i < t
and At,t = 1. Forafixed constant P such that β2 < p < p, we have
At,i ≥ CaPt-i ,
where Ca =(QN=2 Af-L) and N is the maximum of the indices for which Aj-1∕Aj < P. When
there are no such indices, i.e., A1∕A2 ≥ P, we use Ca = 1 by convention.
Proof.
At,i
t
π
j=i+1
N
π
j=i+1
N
π
j=i+1
Aj-1
AjT >
----≥
Aj	-
□
Lemma 11. Suppose that 0 ≤ βt ≤ β < 1 for all t. Let P :=	, where P is defined in Lemma 10.
Then, for all t, we have
kmt,Gbk2 ≤ Caat/(Atdb)(1-ρ)vt,b,
where Ca is defined in Lemma 10.
25
Under review as a conference paper at ICLR 2020
_	_	_ ʌ	π-≠	一公	.
Proof. Let βt,i = ∏j=i+ι βj for i < t and βt,t = 1
kmt,Gbk22
E(I - βi)lβt,igi,Gb
i=1	2
kgi,Gb k22
(21)
Then, with Lemma 10, we get
t
X
i=1
(i-βi)2β2,i
ai
Atdb
XX (i-βi)2β2,i
i=ι	Aadb At,i
;XX (U )t-i≤」一
(22)
≤
Then, combining (21) and (22), we obtain the result.
□
Lemma 12. Assume F is L-smooth, {at} is non-decreasing such that {At-1 /At} is non-decreasing
and limt→∞ AAt^ = p > 0. Let P be a constant such that β2 < p < p. Assume Et[∣∣gt,Gb∣∣2]=
σ2b ≤ dbσ2. Define Wt = %/↑JA. Assume Wt is "almost” non-increasing. This means there
exists another non-increasing sequence {zt} and positive constants C1 and C2 such that C1zt ≤
Wt ≤ C2 zt for all t. Assume 0 ≤ βt ≤ β < 1 for all t. Define following Lyapunov function:
Mt = E[hVF (θt),M + Lkδtk2].
Let C3 ≡ √° β∕(1-β))+ 1，where P ：= βp. Then, for any t ≥ 2, we have
Mt ≤ -y=tΓr— MtT- ^2βηtE hkVF(θt)kH-ιi + 2wtCjE [AkH-1gtk∑1∕2
7 At-IlAtnt-1	2	L	」	LAt	.
短现同囱+√cβw-7) (SAE-1! X σbdb,	(23)
and for t = 1, we have
Ml ≤ - 1-βλnιE hkVF(θ1)kH-1 i +2wiC2E A1 ∣∣H-1g1k∑"+ LE[∣∣διk2]. (24)
Proof. For any t ≥ 2,
E[hVF(θt),δti] =	βt% — E[hVF(θt),δt-ιi] + E (VF(θt),δt -	” 瓦-J .(25)
V At-1∕Atnt-1	A ∖	A Αt-1∕Αtnt-1	/ _|
Then, for the first term of (25), we have
hVF(θt), δt-1i = hVF(θt-1),δt-1i+hVF(θt)-VF(θt-1),δt-1i
≤ hVF(θt-1),δt-1i+Lkθt-θt-1k2kδt-1k2
= hVF(θt-1),δt-1i+Lkδt-1k22,
where the first inequality follows from Schwartz inequality and the smoothness of the function F .
Hence, we have
βt nt
，At-1/Atnt-I
E[hVF(θt), δt-1i]
≤
βtnt
，At-ι∕Atnt-I
E hVF(θt-1),δt-1i+Lkδt-1k22
βtnt
，At-ι∕Atnt-I
Mt-1.
26
Under review as a conference paper at ICLR 2020
Now, we estimate the second term of (25). By Lemma 9, for each block b, we get
E	VGbF(θt),δt,Gb -
βtηt
A At-i/Atnt-
δt-1,Gb
-(1 - βt)E[hVGbF(θt),nt,bgt,Gbi] + EKVGbF(θt),nt,b 吗gt+bk2Xt,bj
+E [(VGbF(Ot),%,b√d= Yt,b
+ EKVGb F (θt),Zt,bi].
(26)
For the first term of (26), we have
-(1- βt)E[hVGbF(θt),ηt,bgt,Gbi]	= -(1- βt)E[hVGbF(θt),ηt,bVGbF(θt)i]
=-(1- βt)ηt,bE[kVGbF(θt)k2].
(27)
For the second term of (26), we have
口「/▽ m八〜 Aadbkgt,Gbk2 v ∖1
E ( VGbF(θt),nt,b Pvg + e Xt,b/
≤E
pnt,b l∣VGb F(θt )k 2 kgt,Gb k2∕ √db PntJAAt kgt,Gb l∣2∕ √dbσt,b/√dbkXt,bk2
σt,b∕√d⅛
. (28)
Note that
√n>t,b∕√db=yp⅛⅞db;
ηtσ2,b∕db
∖,atJΑtσ2b/d
≤ / ntσb
/ /at/At
√wtσb.
(29)
≤
Besides, we have
kXt,bk2
βtmt-1,Gb
(I - 8t)gt,Gb
2
Btmt-LGb
≤
+
2
(I - βt)gt,Gb
p/vt,b + p/ At-1vt-1,b/At
With Lemma 11, we have
mt-1,Gb
p/vt,b + // At-Ivt-1,b∕At
≤
2
2
1
≤	/	=, (30)
/CaAt-l/Atat/(Atdb)(I - P)
gt,Gb
mt-1,Gb
/ At-Ivt-1,b∕At
≤
2
gt,Gb
√αt∕Atkgt,Gb k2∕db
=-^d=. (31)
2	p/at/At
Then, we get
kXt,b k2	≤
βt
(I- βt)√db
YCaAt-11Atat 11Atdb)Q - P
+
≤
≤
βt∕(1 - βt)
p∕CaAt-l∕At(I - P)
β∕(1- β)
/CaAt-1/At(I - P)
+1
+1
β∕(1- β)	+1
/CaA1/A2(1 - P)
(I- βt)√db
√αt∕At
(1 — βt)√db
√at∕At
(1 - βt)√db .
(I- βt)√db
27
Under review as a conference paper at ICLR 2020
where the last-to-second inequality follows from the assumption that βt ≤ β, and the last inequal-
ity holds as we assume {at} is chosen such that {At-1/At} is non-decreasing for all t. Hence,
combining the above result with (29) and (28), we have
E KVGbF(θt),ηt,b Apvgt+bk2 Xt,b，
V‹bkVGb F (θt)∣∣2∣∣gt,Gb∣∣2∕√db
√wtσbC3(I - βt)
A llgt,Gb k2
1 - β %,b∣∣vGbFR)k2kgt,Gbk2∕db
4	σ2,b∕db
+ wtσbC32 (1 - βt)
A llgt,Gb k2
(ptb + 疔
1 - βt ηt,bkVGb F(θt)∣∣2Et[∣∣gt,Gb∣∣2]2∕db
4	σ2,b∕db
+ wtσbC32(1 - βt)
A kgt,Gbk2
(PVtJ + E)2
ɪ nt，bkVGb F (θt)k2 + wtσbC2 PB!
(32)
where the second inequality follows from ab ≤ aC + c22 for any c > 0. Now, We estimate the third
term of (26):
E KvGb F (θt),ηt,b √db Yt)	≤ E ]pηtj IlVGb F (θt)k2 PntJ √d= IIYt,bk2 .
Similarly, with (30) and (31), by expanding lYt,bl2, we have
IYt,bI2	≤
A√d kgt,Gbk2	βt"1,Gbk2	∕⅞ "bk2	y⅞Fσt,b
Pvt,b + C	PAt-IVt-i,b∕At + € PVt,b + PAt-1Vt-1,b/At pvt,b + PAt-IVt-i,b∕At
≤
≤
≤
≤
E
E
E
E
≤
Aad kgt,Gbk2 βt+ A kgt,Gbk2 1 - βt
E + €	CCaA^^∕AAt∕(KAbdb)(1 - ρ)	E + € √atTAt
βt
æ kgt,Gb ∣2
+ E+€ QTtt
βt ∕(1 - βt )
∖/CaAt-1/At(I - P)
+ 1 (1 - βt )
≤
C3(1 - βt),
■x/CaA-iTAtr1-"^
where C3 is the constant defined above. Hence, together with (29), we obtain
E KVGb F (θt), nt,b √⅛ Yt,b)
y-	y-σtb ∖PA kgt,Gb k2
≤ E	PtkVGbF(θt)k2Pt√dbC3(1-βt
≤ ;nt,tEhkVGbF(θt)k2i + wtσbC2(1 - βt)E Bg≡⅛_
≤ 1-βtnt,bE h∣VGbF(θt)k2i + wtσtC3E (PCj2 .	⑸)
28
Under review as a conference paper at ICLR 2020
The last term of (26) can be bounded as follows
EKVGbF(θt),Zt,bi] ≤ E[∣∣VGbF(θt)k2kZt,bk2] ≤ E[σb√db∣∣Zt,bk2],
and with (30), we get
kZt,bk2	≤
βtηt kmt-i,Gbk2_________(1 - PAt-1∕At) e
PAt-Ivt-1,b/At + PAt-l/AtE (PAt-1vt-1,b/At + E)
≤
___________βtηt___________
PCaAt-1/Atat/(Atdb)(1 - P)
≤
βηt VAtdb/at
PCa(1 - P)
-1
βwt Vdb
PCa(I - P)
At
At-1
y
Hence,
EKVGbF(θt),Zt,bi] ≤ *dbWt	(∖F — 1! .	(34)
Ca (1 - P)	At-1
Combining (26), (27), (32), (33), and (34), we get
E "*VGbF(θt),δt,Gb - pAtUAtnt-I δt-1,Gb +]
≤ --幅EhkVGbF (θt)k2i+2wtME" (W+1 ]+√⅛ (SAE - 1
Summing from b = 1 to B, we obtain
E "*vf(θt),δt - p==— δt-1 +]
L∖	∙√At-i∕Atηt-i	∕ J
≤	——ηtE [kVF(θt)kH-ιi +2wtC2e ]A∣∣H-1gtk∑1∕2
+	βwt
PCa (1 — P)
Then, with (25), we have
AA-1 — 1) X σbdb.
E[hVF(θt), δti]
≤ PAeAη…Mt-1 - 1—βtηtE [kVF(θt)kH-1 i + 2wtC3E A∣∣H-1gtk∑1∕2
βwt
AAT-1) X σbdb.
We obtain (23) by adding the term LE[kδt k22] to both sides of the above equation. When t = 1, we
have
Mi = E[-hVF (θι),ηιmι∕(PV1 + e) + L∣∣διk2]
=E[-hVF(θι),ηι(1 - βι)gι∕(PV7 + e))+ Lkδιk2].	(35)
Then, following the derivation of (19), for each block b, we have
(1 - βι)ηιgι,Gb
Pvι7 + E
(1 - β1)ηi,bg1,Gb + ηi,b
σι,b Α⅛gi,Gb	(1 - βi)√b
--	Αa⅛kg1,Gbk2 (1 - βι)gι,Gb
11'b P^7 + E Pvι,b + P^7
29
Under review as a conference paper at ICLR 2020
Hence, with similar argument, we get
E -〈VF (Θ1),η1 (√-β+)g1 R ≤ - 1-2β1 ηιE[kVF (θ1)kH-1 i +2wιC2E A1 ∣∣H-1g1k∑1∕2
Combining above with (35), and adding LE[kδ∣∣2], We obtain (24).	□
Lemma 13. With the same assumptions in Lemma 12, we have
X⑹2 ≤ CC¾XWtAkH-1gtk2.
Proof. For each block b,
kmt,Gbk2 = X(Y βj j (1 - βi )9i,Gb
i=1 j=i+1	2
≤ Xt ( Yt βj(1-βi)j kgi,Gbk2
i=1	j=i+1
t
≤ Xβt-i kgi,Gbk2 .
i=1
Then,
kmt,Gbk2 ≤ X βt-i kgi,Gbk2
√^t,b + E — i=ι	√^t,b + E
Since vt,b ≥ At-1vt-1,b∕At, We have Vt,b ≥ (Qj=i+1 Aj-1/Aj) vi,b = At,ivi,b ≥ Capt-ivi,b by
Lemma 10. It folloWs that
kmt,Gbk2 ≤	^X	βt-i	kgi,Gb k2 ≤	1	^X	(_P_ A t i	kgi,Gbk2	=	1	1X	√-t-i	kgi,Gbk2
√^t,b + E ~	i=ι	√^t,b	+ E √	√Cα	i=ι √VP∕	√^i,b	+ E	√Ca、=、P	√^i,b + E
Then, as at/At = 1 - At-1/At is non-decreasing, We have
2
2
kδt k22
B
X
b=1
ηtmt,Gb
p^j + E
B
X
b=1
w2	(t-i r-t-i Pai/Ai kgi,Gb k2 ʌ
Cafe 匕 7-	E + E )
2
w2	(t-i r-t-i PaJAt kgi,Gb k2 A
^ Cab=1 ∖h P E+ E )
X (Pj=I √-tj
w2 ∖~y (t-j 厂 t-j∖ V^^ r~t-i ai∕Ai kgi,Gbk
*怦 √-	) is √-	RW
2t
C (1wt 5)X√-t-iAkHiTgik2∙
Ca(1 - -) i=1	Ai
Pai∕Ai kgi,Gb k2 ∖
PviJ + E I
2 ≤ w2	—t-i ai∕Ai kgi,Gb k2
，-Ca(I-√P) b=l=r	(PvZb+E)2
≤
≤
≤
2
2
As wt ≤ C2 ∕C1 wi for any i ≤ t by Lemma 6, then We have
同2 ≤ C⅛¾ X √-t-iWiAikHiTgik2∙
30
Under review as a conference paper at ICLR 2020
Hence,
T
Xkδtk22
t=1
≤
≤
C2∕Ci2wι，
Ca(I- √ρ) :
C2∕C2wι，
Ca(I- √P)；
C27Clwl
Ca(1-√P)2
Tt
XX √ρt-i
t=1 i=1
TT
XX √ρt-i
i=1 t=i
T
wi~Γ kHi 1gik2
Ai
wi A1 kHTgikl
X wi Ai kH-1gik2.
i=1 Ai
□
Lemma 14.
With the same assumptions in Lemma 12, let Mt = E[hVF(θt), δ/ + L∣∣δtk2], we have
T
XMt
t=1
≤
Cl √Cα(1 -√P)
+pd⅛
B
T
2C32 X wtE
t=1
T
~τ IlH-Igtll∑" +
At
b=1
σbdb	wt
t=2
At
At-1
lc27c2w1
ca(i -√ρ)
T
T
XWtE AkH-1gtk
t
t=1
XηtE [kVF(θt)kH-ι].
t=1
Proof. Let define following quantity
Nt
2wtC2E a ∣H-1gtk∑ι∕2
At
+ LE[Iδt I22] +
βwt
At
At-1
1	XB σbdb , ∀t ≥ 2,
b=1
N1
2wιC2E ?∣H-1gιk
A1
2
Σ1/2
+ LE[Iδ1 I22].
Then, by Lemma 12, for any t ≥ 2, we have
Mt	≤
βtηt
-∖∕At-ι7Atnt-ι
Mt-I- 1-βtηtE h∣VF(θt)kH-ii + Nt
2
2
≤
βtηt
、/ At-IlAtnt-
Mt-1 + Nt
1
and M1 ≤ N1 . Then, by recursively applying above relation, we get
t
Mt	≤	^n- Mi + X	Ni
At,1η1	i=2	At,iηi
—
一ηtE h∣VF(θt)kH-ιi
t
≤X
i=1
βt,Mt
/ J
At,iηi
Ni- 1-βtηtE h∣VF(θt)kH-i，
where βt,i = Qj=i+ι βj for i < t and βt,t = 1 and At,i = Qj=i+ιA- for i < t and A^t,t = 1.
Note that βt,i ≤ βt-i, and ηt ≤ C2∕C1ηi. By Lemma 10, We have A^t,i ≥ Capt- . Then,
Mt	≤
Cl √Cα
t
X
t i Ni- 1-βtηtE h∣VF(θt)kH-1 i
Cl √Cα
i=1
X √Pt-iNi - 1-βtηtE h∣VF(θt)kH-ιi .
i=1
31
Under review as a conference paper at ICLR 2020
It can be verified that the above inequality holds for t = 1 as C2∕(C1√Ca) ≥ 1. Then, summing
from t = 1 to t = T , we obtain
T
XMt	≤
t=1
Tt	T
C√χ XX√Pt-iNi - XɪηtEhkVF(θt)kH-ιi
TT	T
C√χ XX √Pt-iNi - X -⅛βt ηtEhkVF (θt)kH-ι i
1 a i=1 t=i	t=1
≤
C2	X
Cl√Cα(1 - √P)=
T
--β XηtE [kVF(θt)kH-ιi.
t=1
(36)
With Lemma 13, we get
T
XNt
t=1
X "C2E [ At kHTgtkM+LE[kδt T+X pc=w---7) (/AE - 1) X σbdb
≤
2C2 X WtE [AkH-1gtk∑1∕2] + CC12-√w2 X WtE [AkH-1gtk2
BT
XbdbX
b=1
t
At
At-1
Combining the above with (36), we obtain the result.	□
Lemma 15. Assume {at} is non-decreasing such that {At-1/At} is non-decreasing. Define Wt =
ηt/ʌ/AF∙ Assume Wt is "almost” non-increasing. This means there exists another non-increasing
sequence {zt} and positive constants C1 and C2 such that C1zt ≤ Wt ≤ C2zt. We have
T
T X(E[kVF(θt)k2/3])	≤
t=1
√2 (maxι≤t≤τ E [maxb Vt,b] + e2)
C1∕C2ητ T
T
XηtE [kVF(θt)kH-1].
t=1
Proof. By Holder,s inequality, we have E[∣XY|] ≤ (E[|X|p])1/p(E[|Y|q])1/q for any 0 < p,q < 1
with 1/p + 1/q = 1. Taking p = 3/2, q = 3, and
X = (p≡≡-!2/3, Y =(一+『3
∖ √maXb vt,b + €)	∖V b	)
we obtain
E[kVF (θt)k2/3] ≤ (e "√mFbTb) I)* (E "(√max^+J#)”.
Hence,
(E[kVF P)"2 ≤ (e "√v⅛ #) (E "(qmbax^+”[J.
Note that
IVF (%u∣2
√maχb vt,b + €
B
X
i=1
IkVGi F (θtj k2
√maχb vt,b + €
≤
X kVGbF(θt)k2
b=1	PtJ+€
kVF (θt)∣H-1
32
Under review as a conference paper at ICLR 2020
We also have
2
E
max
vt,b + C
≤
2E
max Vt,b +
2 (E max Vt,b
+ C2
Then, for any t ≤ T , we get
(E[kVF (θt)k4/3])3/2	≤
≤
2E
]max VtJ + c2JEhkVF (θt)kH -1 i
p2(E max, vt,b] +，2) ntEhkVF @)kH -i i
p2(maχι≤t≤TE[maxb⅛+22ηtE hkVF(θt)kH-1
Cl∕C2ητ	L	Ht
where the last inequality follows from Lemma 6. Taking average from t = 1 to T , we get
T
T χ(E[kVF (θt )k2/3])	≤
t=1
√2 (maxι≤t≤τ E [maxb Vt,b] + c2)
C1∕C2ητ T
T
XηtE [kVF(θt)kH-1].
t=1
□
F.1 Proof of Theorem 2
Proof. As F is L-smooth, then we have
F(θt+ι) ≤ F(θt) + hVF(θt),θt+ι - θti + Lkθt+ι - θtk2.
Recursively applying the above relation, we get
T
F(θ*) ≤ E[F(θτ +1)] ≤ F(θι)+ XMt,
t=1
where Mt = E[hVF(θt), δti + Lkδtk22]. By Lemma 14, we have
T
--β XηtE hkVF(θt)kH-ιi
t=1
≤
F(θι) - F(θ*) +
C2
a" (i-√p)
T
2C2 X WtE A kH-lgtk∑1∕2
t=1 At
LC22/C12w1
+ Ca(I- √P)2
T
X wtE
t=1
F(θι) - F(θ*) +
A kH-igtk2
Cl √Cα (1 -√ρ)
+
BT
XσbdbX
b=1
t
2C3 X ηt E ]∏ kH-lgtk∑1∕2
At
At-1
LC2∕C2w1 X P Γ ∕^aΓ∣∣ p-- 1 ∣∣2
+ Ca(1-√P )2 工	kHt gtk2
βBT
+ PCK N σbdbt=2Wt
33
Under review as a conference paper at ICLR 2020
Applying Lemma 8, we have
1 — β
2
≤
T
XηtE [kVF(θt)kH-ι]
t=1
F (θι) — F (θ*)
+
+
Cl √Cα(1 — √P)
LC23 /C13w1
Ca(1 — √P)2
B
2C2 C2 wι fσbdb log
C1
b=1
At
-1 + a1
≤
+pd⅛
F (θι) — F (θ*)
+
+
B
w1 db log
b=1
BT
Xσbdb X
b=1
t
At
At-I
B
At
Cl √Cα(1 — √P)
LC23 /C13w1
2C2 C w1 ^X σbdb log
C1
b=1
(σ2+1)+ω X σbdb X ηt∏]
Ca(1 — √P)2
w1
B	2	BT
X db log (哭 + 1)+ ω X dbX ηt
at
W
+pd⅛
B
b=1
F(θι)- F(θ*) +
β
Cl√Ca (1-√P) VZCa(I-P)
B
+X
b=1
-LC3∕C3w1db	2C32C2σbdb
_Ca(1 -√p)2 +
C1
w1 log
BT
σbdb	wt
b=1	t=2
2b + 1 +ω Xηt
At
At-ι
at
W
Combining above with Lemma 15, we have
T
1m% (E[kVF(θt)k2/3])	≤ T χ(E[kVF(θt)k2/3
t=1
3/2
≤ 2，2 (maxι≤t≤τ E [maxb Vt,b] + 邛
C1/C2(1 - β)ηTT
[F(θι)- F(θ*)
+
β
Cl √Ca(1 — √P) VzCa(I-P)
X σbdb X wt(AA^ - 1
B
+X
b=1
-LC2∕Cw1db	2C3C2σbdb
Ca(I-√P)2 +
C1
w1 log (σ+1)+ω X ηt∏j]
√2 (maxι≤t≤τ E [maxb Vt,b] + e2) Γ	2C2
+
ηTT
2C22
β
(1 — β)C1
B
[F (θι) — F (θ*)]
T
C2 √Ca(1 -√P)(1 — β) VzCa(I-P)
σbdb	wt
b=1	t=2
B
+X
b=1
LC23w1db
C3Ca(1 -√p)2 +
2C32C2σbdb
At
Αt-ι
T
C1
W1 log (* + 1)+ ω^2ηt
at
W
(37)
.(38)
□
34
Under review as a conference paper at ICLR 2020
F.2 Proof of Proposition 3
To prove the result, we use the following high probability bound.
Proposition 5. With probability at least 1 - δ2/3, minι≤t≤τ ∣∣VF (θt )k2 ≤ C(T )∕δ.
Proof. By the concavity of the minimum, we have
3/2	3/2
E [1≤ni∏τ ∣VF(θt)∣∣4]	≤ 1nrninT (e[∣VF(仇川广).
Let X = min1≤t≤T ∣VF(θt)∣22. The Theorem 2 suggests that we have E[X2/3] ≤ C(T)2/3. By
Markov’s inequality, we get
P(X2/3 >>) ≤ EXaδ2∕3 ≤ δ2∕3.
∖	δ2∕3 J	C(T )2 2/3
Hence, P(X > C(T)) ≤ δ223, and We have P(X ≤ C(δτ)) ≥ 1 - δ2/3.	□
Proof. (of Proposition 3) Recall the definition of C(T) in (17). When at = atτ, We have At
O(t1+τ). This suggests that
and
Hence,
X ηtr≡
O(1),
1
O (log(T)),
On the other hand, When at
t1
= α-t, We have
百 η 1 1 -α η
ηt∖ W - 1 - β V (1 - αt)t - √t,
O (log(T)) , and C(T) = O
and
Then, We get
wt
1 -
(1 -
1 — αt	/
(1 - αt-1 )α ≤
αt	η
α)t	(1 — β) p(1 — a)t
1+α
α
τ
X	ηt
t=1
O
and C(T) = O(1).
Combining the results With Proposition 5, We complete the proof.
□
35
Under review as a conference paper at ICLR 2020
F.3 Proof of Corollary 2
Proof. As kgt,Gb k22 /db ≤ Gb2, then we have
Vt,b = (vt-i,b + atEt[kgt,Gbk2]∕db])/At ≤ G2,
and therefore VTB ≡ max1≤t≤T E [maxb Vt,b] ≤ maxb G2. Arranging the terms in C(T), We
obtain
~
C(T)
√2(maxb G2 + e2)
ητ T
(τ2Cβh[F(θι) - F("*)]
1	2C2
C2√Ca(1 -√ρ)(1 - β)
口 PCa； -P) X Wt
T
X ηt
t=1
X σbdb log (σb + 1
at
At
B
σbdb
b=1
+ C3CLC2w√5)2 X db log (IT + 1) +
C1 Ca (1 - ρ) b=1
2C32C2W1
C1
LC23W1 dω
+ofc” (i-√ρ)2
T
X ηt
t=1
When B = d, We have
~ .
Cd(T)
,2 (maxbmaxL G2+,2) 17r⅛[F(θι) - F(θ.)]
ηTT	(1 - β)C1
+ C2√Ca(1 -√ρ)(1 - β) ""pCα(1 - P) X Wt (SA-I
ηTT
2C22
LC23w2
CfCa (1-√P)2
2C2
d2
X log α+ι)+
2C32C2W1
1C^ ∑σi log
i=1
LC23W1 dω
+ C3C” (1-√P)2
ʌ/2 (maxb maxi∈Gb G2 + e2)
ητ T
(⅛ [F (θι)
-F(θ*)]
2C32C2
kω S ηt
σ2 + ι
2 I ɪ
2
at
At
d
Xσi
i=1
2C2	「I" β X A I At	A	2C2C2 X	/-at^
+ C2√Ca(i -√P)(1 - β) [ [pCo(T-T) Swt (" - 1Γ 丁ω⅛ηtV A
]X X σ
」b=1 i∈Gb
+ C1⅛C‰ XXbg (⅛ + 1) +
2C2C2w1
~C-
B	∕zτ2
X X σi log(方 + 1
b=1 i∈Gb
LC23w1 dω
+ CfCa (1-√P)
T
X ηt
t=1
+
2
36
Under review as a conference paper at ICLR 2020
fɔ	/ C C 、
Pb=ι Pi,∈gb log(σi /e +I)
PB=I dblog(σ2∕e2 + 1)	,
Substituting r1
___ iɔ __ / c - C ∖
PB=I Pi∈Gb σi log(σ2∕e2 + 1)	.
PB=1 σbdb Iog(σ2∕e2 + 1) , We get
一 d 一
「F and
~ ， .
Cd(T)
J2 (maxb maχi∈Gb G2 + e2)
ητ τ
2C2
(1 - β)C1
[F(θι)- F(θ*)]
C2 √Ca(1 -√P)(1 - β)	VZCa(I-P)
X " V - I鬻 ω
x ^rAt# r2 X Qbdb
t=1	t	b=1
+C3CL C2w√P)2 r1 X db log (- + 1) +
2C32C2w1
C1
r3 £ σbdb log ( -b + 1
b=1
LC23 w1 dω
+ c3Ca (1-√P)2
+
β
≥
.门 、ImaXb maxi∈Gb G2 + 昌 C G
min(1,rmin)V	maXh G2 + m	CB(T),
maxb Gb +
Where rmin = min(r1, r2, r3). Then,
~ , l
Cd(T)
CB (T)
≥ min(1,rmin) JmaxmZxGG+G三.When B = B,
We assume that Assumption 4 is tight in the sense that -2 ≤ 今 Pi∈G5 -2,5 Thus, rmin Can be larger
than 1 as -Ib ≤ 太 Pii∈gb -2. Corollary 2 then indicates that blockwise adaptive stepsize will lead
to improvement if
J(max∣ maXi∈Gb G2 + e2)∕(max∣ Glb) + e2) > r1-. Similarly,
assume that the
upper bound Gb is tight so that Gb ≤ 太 Pii∈Qb G2∙ Thus, max∣ max髭g^ G2 ≥ max∣ Glb), and the
above condition is likely to hold when rmin is close to 1 or is larger than 1. From the definitions of
ri, r and r3, we can see that they get close to 1 or is larger than 1 when {-2 }i∈Qb have sufficiently
low variability.	□
F.4 Proof of Proposition 4
Proof. As function is γ-Lipschitz, we have the following result:
SUpEM[f (M(S); Z)- f (M(S0); z)] ≤ 浊M[kM(S) - M(夕)|图.
z
Therefore, we can consider bounding EM [kM(S) - M(S0)k2]. Let βt = 0 for all t.
t
θt+1 = θ1 -	ηkHk-1mk
k=1
t
= θ1 -	ηkHk-1gk
k=1
t
=θi- XηkH-1Vf(θk; Zik),
k=1
5NOtethat 卷Et[kgt,Gbk2] = db Pi∈Gb Et[g2,i] ≤ d1b Pi∈Gb σ2. OntheOther hand,表Et[|匝4闿 ≤
σ2. Thus, this bound is tight in the sense that σ2 ≤ d1 Pi∈gb σ2.
37
Under review as a conference paper at ICLR 2020
where ik ∈ [n] is the example index selected at iteration k. Then, we can bound ∆t+1 = kθt+1 -
θt0+1 k2 as follows
E[∆t+1] = E[kθt+1-θt0+1k2]
tt
=E[kθι-θl - XηkH-IVf(θk；Zik) + XηkHkTVf(θk; W)k2]
k=1	k=1
t
≤ E[kθ1-θlk2] + XηkE[kH-1Vf(θk; Zik) - HkTVf(θk； Zik)k2]
k=1
t
=XηE[kH-1Vf(θk; Zik) - Hk-1Vf (θk； zik)k2].	(39)
k=1
Note that Zik = Zi0 with probability 1 - 1/n. Then, we can bound each term E[kHk-1Vf(θk; Zik) -
HkTVfM ； Zik )k2] as follows
E[kH-1Vf (θk; Zik) — HkTVf (θk； Zik)k2]
≤ 2E[∣∣H-1Vf(θk; Zik)k2]+ (l - J) E[∣∣H-1Vf(θk; Zik) - HkTVf (θk； Zik)k2]
≤ 2E[∣∣H-1Vf(θk; Zik)k2]+ (l — n) E[∣∣H-1Vf(θk; Zik) — HkTVf (θk； Zik)k2]
+(1 — J) E[kHk-1Vf (θk; Zik) — Hk-1Vf (θk; Zik)k2].	(40)
The second term is bounded as
E[kH-1Vf (θk; Zik) — HkTVf (θk; Zik)k2]
≤ E[kH-1— HkTk2kVf(θk; Zik)k2]
≤ YE[kH-1 — HkTk2]
YE max
b
We expand the third term of (40) as
E[kHk-1Vf (θk; Zik) — HkTVf (θk; Zik)k2]
≤	E[∣∣Hk-1k2 ∣∣Vf(θk; Zik) — Vf (θk; Zik )k2]
≤	LE[kHk0-1k2kθk — θk0 k2]
≤	LE	/」，kθk — θkk2
VZminb ^k,b + E
=LE — .	=---∆k .
VZminb ^k,b + E
Substituting the above results into (40) and combining with (39), we obtain
E[∆t+1]
2t
≤	- EnkE[kH-1Vf(θk; Zik)k2]
k=1
+
t
L X ηkE
k=1
+
max
b
∆k
1
Pminb Vk,b + E
38
Under review as a conference paper at ICLR 2020
Note that if Wt = nt//at/At is "almost” non-increasing w.r.t. another non-increasing sequence
{zt } and positive constant C1 and C2, then wt2 is also ”almost” non-increasing w.r.t. another non-
increasing sequence {zt2 } and positive constant C12 and C22 . Using Lemma 8 with C = I, we have
t
X nkE[∣∣H-1Vf(θk; Zik)k2]
k=1
t
≤
√∖ En2E[∣∣H-1vf(θk；Zik)k2]
k=1
t
≤
≤
√t ∑n2
k=1
AE [晨 kH-1vf (θk ； Zik )k2
U C2 ,
t C2∣
B
w12	db log
C2
C11 ：
b=1
B
w12	db log
b=1
(σ⅛+1)+d x n2
Ak
Ak-1 + a1
t.
Then, we get
E[∆t+1]
2C2
nC11 ：
w2 X db log (Ib +1) + dω X n2
≤
t
max
b
1
vk~b + e
—
1
,mi□b Vk,b +
∆k
As the bound holds for any S, S0 , Z, it also holds for its supreme.
□
39