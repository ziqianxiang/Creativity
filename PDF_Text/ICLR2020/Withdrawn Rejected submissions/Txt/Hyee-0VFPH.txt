Under review as a conference paper at ICLR 2020
Stochastic Geodesic Optimization
for Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
We develop a novel and efficient algorithm for optimizing neural networks in-
spired by a recently proposed geodesic optimization algorithm. Our algorithm,
which we call Stochastic Geodesic Optimization (SGeO), utilizes an adaptive co-
efficient on top of Polyak’s Heavy Ball method effectively controlling the amount
of weight put on the previous update to the parameters based on the change of
direction in the optimization path. Experimental results on strongly convex func-
tions with Lipschitz gradients and deep Autoencoder benchmarks show that SGeO
reaches lower errors than established first-order methods and competes well with
lower or similar errors to a recent second-order method called K-FAC (Kronecker-
Factored Approximate Curvature). We also incorporate Nesterov style lookahead
gradient into our algorithm (SGeO-N) and observe notable improvements.
1	Introduction
First order methods such as Stochastic Gradient Descent (SGD) with Momentum (Sutskever et al.,
2013) and their variants are the methods of choice for optimizing neural networks. While there
has been extensive work on developing second-order methods such as Hessian-Free optimization
(Martens, 2010) and Natural Gradients (Amari, 1998; Martens & Grosse, 2015), they have not been
successful in replacing them due to their large per-iteration costs, in particular, time and memory.
Although Nesterov’s accelerated gradient and its modifications have been very effective in deep
neural network optimization (Sutskever et al., 2013), some research have shown that Nesterov’s
method might perform suboptimal for strongly convex functions (Aujol et al., 2018) without looking
at local geometry of the objective function. Further, in order to get the best of both worlds, search for
optimization methods which combine the the efficiency of first-order methods and the effectiveness
of second-order updates is still underway.
In this work, we introduce an adaptive coefficient for the momentum term in the Heavy Ball method
as an effort to combine first-order and second-order methods. We call our algorithm Geodesic
Optimization (GeO) and Stochastic Geodesic Optimization (SGeO) (for the stochastic case) since it
is inspired by a geodesic optimization algorithm proposed recently (Fok et al., 2017). The adaptive
coefficient effectively weights the momentum term based on the change in direction on the loss
surface in the optimization process. The change in direction can contribute as implicit local curvature
information without resorting to the expensive second-order information such as the Hessian or the
Fisher Information Matrix.
Our experiments show the effectiveness of the adaptive coefficient on both strongly-convex functions
with Lipschitz gradients and general non-convex problems, in our case, deep Autoencoders. GeO
can speed up the convergence process significantly in convex problems and SGeO can deal with ill-
conditioned curvature such as local minima effectively as shown in our deep autoencoder benchmark
experiments. SGeO has similar time-efficiency as first-order methods (e.g. Heavy Ball, Nesterov)
while reaching lower reconstruction error. Compared to second-order methods (e.g., K-FAC), SGeO
has better or similar reconstruction errors while consuming less memory.
The structure of the paper is as follows: In section 2, we give a brief background on the origi-
nal geodesic and contour optimization introduced in Fok et al. (2017), neural network optimization
methods and the conjugate gradient method. In section 3, we introduce our adaptive coefficient
specifically designed for strongly-convex problems and then modify it for general non-convex cases.
1
Under review as a conference paper at ICLR 2020
In section 4, we discuss some of the related work in the literature. Section 5 illustrates the algo-
rithm’s performance on convex and non-convex benchmarks. More details and insights regarding
the algorithm and the experiments can be found in the Appendix.
2	Background
2.1	Geodesic and Contour Optimization
The goal is to solve the optimization problem minθ∈R f(θ) where f : RD → R is a differentiable
function. Fok et al. (2017) approach the problem by following the geodesics on the loss surface
guided by the gradient. In order to solve the geodesic equation iteratively, the authors approxi-
mate it using a quadratic. In the neighbourhood of θt , the solution of the geodesic equation can be
approximated as:
θt+1 = θt + vtδt - ctδt2	(1)
where θt is the current point, vt is the unit tangent vector of the geodesic at θt , δt is the step size
and:
Ct = 2 [Vf (θt) - 2(vt∙Vf (θt))vt]	(2)
and θt+1 is the next point. The tangent vector is estimated by the normalized difference vector
between the current point and the previous point vt = θt - θt-1 and the first tangent vector is set to
the gradient v1 = Vf(θ1). For a detailed explanation we refer the reader to Fok et al. (2017).
2.2	Neural Networks
We consider a neural network with a differentiable loss function f : RD → R with the set of
parameters θ. The objective is to minimize the loss function f with a set of iterative updates to the
parameters θ. Gradient descent methods propose the following update:
θt+1 = θt - Vf(θt)	(3)
where is the learning rate. However, this update can be very slow and determining the learning
rate can be hard. A large learning rate can cause oscillations and overshooting. A small learning
rate can slow down the convergence drastically.
Heavy Ball method In order to speed up the convergence of gradient descent, one can add a
momentum term (Polyak, 1964).
dt+ι = μdt - eVf (θt);	θt+ι = θt + dt+ι	(4)
where d is the velocity and μ is the momentum parameter.
Nestrov’s method Nesterov’s accelerated gradient (Nesterov, 1983) according to Sutskever et al.
(2013) can be rewritten as a momentum method:
dt+ι = μdt - eVf (θt + μdt); θt+ι = θt + dt+ι	(5)
Nesterov’s momentum is different from the heavy ball method only in where we take the gradient.
For the class of convex functions with Lipschitz gradients, Nesterov’s momentum is shown to be
optimal with a convergence rate of O(1/t2) (Nesterov, 1983). Note that in both these methods dt is
the previous update θt - θt-1.
2.3	Conjugate Gradients
The conjugate gradient algorithm (Hestenes & Stiefel, 1952) attempts to solve the quadratic problem
minθ∈Rn 1 θτQθ 一 bTθ where Q is a n × n positive definite matrix and the update is:
θt+1 = θt + td0t;	d0t+1 = -gt+1 + γtd0t	(6)
where and γ are step sizes and d0 is the search direction. In the non-quadratic case, minθ∈Rn f (θ),
the function is locally approximated using a quadratic where gt = Vf(θt) and Q = V2f(θt) and
and γ are:
=	gT dt	= g%[V2f(θt)]dt
Q=	dtτ[V2f(θt)]dt;	Yt= dtτ[V2f(θt)]dt	()
2
Under review as a conference paper at ICLR 2020
Clearly, one can see the conjugate gradient method as a momentum method where t is the learning
rate and tγt-1 is the momentum parameter:
θt+1 = θt - tgt + t γt-1 d0t-1
(8)
Note that d0t = (θt+1 - θt)/t (We added the prime notation to avoid confusion with dt = θt+1 - θt
throughout the paper). To avoid calculating the Hessian V2f which can be very expensive in terms
of computation and memory, is usually determined using a line search, i.e. by approximately
calculating t = arg min f(θt + dt) and several approximations to γ have been proposed. For
example, Fletcher & Reeves (1964) have proposed the following:
FR =	kgtk2
Yt	= ≡-1F
(9)
Note that γFR (with an exact line search) is equivalent to the original conjugate gradient algorithm
in the quadratic case.
3 Stochastic Geodesic Optimization
The adaptive coefficient that appears before the unit tangent vector in equation 2 has an intuitive
geometric interpretation:
gt ∙ dt = cos (∏ 一 Φt)
where
gt
gt
;
dt
时
(10)
where φt is the angle between the previous update dt = θt 一 θt-1 and the negative of the current
gradient -gt. Since 0 ≤ φ ≤ ∏, thus -1 ≤ cos(∏ 一 φt) ≤ 1. The adaptive coefficient em-
beds a notion of direction change on the path of the algorithm which can be interpreted as implicit
second-order information. The change in direction at the current point tells us how much the current
gradient’s direction is different from the previous gradients which is similar to what second-order
information (e.g. the Hessian) provide. For more details on the adaptive coefficient we refer the
reader to Appendix C.
3.1 Strongly Convex Functions with Lipschitz gradients
We propose to apply this implicit second-order information to the Heavy Ball method of Polyak
(1964) as an adaptive coefficient for the momentum term such that, in strongly-convex functions
with Lipschitz gradients, we reinforce the effect of the previous update when the directions align,
i.e. in the extreme case: φ = 0 and decrease when they don’t, i.e. the other extreme case: φ = π.
Thus, we write the coefficient as
YC = 1 - gt ∙ dt	(II)
with C indicating “convex”. It’s obvious that 0 ≤ γtC ≤ 2. Note that we will use the bar notation
(e.g. d) throughout the paper indicating normalization by magnitude. A "-strongly convex function
f with L-Lipschitz gradients has the following properties:
f(θ0) ≥ f(θ) + f0(θ) ∙ (θ0 - θ) + μkθ0 - θk2; kf0(θ) - f0(θ0)k2 ≤ L(θ - θ0)2	(12)
Applying the proposed coefficient to the Heavy Ball method, we have the following algorithm which
we call GeO (Geodesic Optimization):
Algorithm 1: GEO (STRONGLY CONVEX AND LIPSCHITZ)
1	Initialize θ1
2	Setd1= Vf(θ1)
3	for t = 1 to T do
4	Calculate the gradient gt = Vf(θt)
5	Calculate adaptive coefficient YC = 1 - gt ∙ dt
6	Calculate update dt+1 = αYtC dt - gt
7	Update parameters θt+1 = θt + dt+1
where T is total number of iterations and α is a tunable parameter set based on the function being
optimized and is the learning rate.
3
Under review as a conference paper at ICLR 2020
Incorporating Nesterov’s momentum We can easily incorporate Nesterov’s lookahead gradient
into GeO by modifying line 4 to gt = Vf (θt + μdt) which We call GeO-N. In GeO-N the gradient
is taken at a further point θt + μdt where μ is a tunable parameter usually set to a value close to 1.
3.2 Non-convex functions
However, the algorithm proposed in the previous section would be problematic for non-convex func-
tions such as the loss function when optimizing neural networks. Even if the gradient information
was not partial (due to minibatching), the current direction of the gradient cannot be trusted because
of non-convexity and poor curvature (such as local minima, saddle points, etc). To overcome this
issue, we propose to alter the adaptive coefficient to
YNC = 1 + gt ∙ dt
(13)
with N C indicating “non-convex”. By applying this small change we are reinforcing the previous
direction when the directions do not agree thus avoiding sudden and unexpected changes of direction
(i.e. gradient). In other words, we choose to trust the previous history of the path already taken more,
thus acting more conservatively. To increase efficiency, we use minibatches, calling the following
algorithm SGeO (Stochastic Geodesic Optimization):
Algorithm 2: SGEO (NON-CONVEX)
1	Initialize θ1
2	Set d1 =Vf(θ1)
3	for t = 1 to T do
4	Draw minibatch from training set
5	Calculate the gradient gt = Vf(θt)
6	Calculate adaptive coefficient γtN C = 1 + gt ∙ dt
7	Calculate update dt+1 = tγtNCdgt - tggt
8	Update parameters θt+ι = θt + dt+ι
Further we found that using the unit vectors for the gradient gg and the previous update dg, when
calculating the next update in the non-convex case makes the algorithm more stable. In other words,
the algorithm behaves more robustly when we ignore the magnitude of the gradient and the momen-
tum term and only pay attention to their directions. Thus, the magnitudes of the updates are solely
determined by the corresponding step sizes, which are in our case, the learning rate and the adap-
tive geodesic coefficient. Same as the strongly convex case, we can integrate Nesterov’s lookahead
gradient into SGeO by replacing line 5 with gt = Vf(θt + μdt) which we call SGeO-N.
4	Related Work
There has been extensive work on large-scale optimization techniques for neural networks in recent
years. A good overview can be found in Bottou et al. (2018). Here, we discuss some of the work
more related to ours in three parts.
4.1	Gradient Descent Variants
Adagrad (Duchi et al., 2011) is an optimization technique that extends gradient descent and adapts
the learning rate according to the parameters. Adadelta (Zeiler, 2012) and RMSprop (Tieleman
& Hinton, 2012) improve upon Adagrad by reducing its aggressive deduction of the learning rate.
Adam (Kingma & Ba, 2014) improves upon the previous methods by keeping an additional average
of the past gradients which is similar to what momentum does. Adaptive Restart (Odonoghue &
Candes, 2015) proposes to reset the momentum whenever rippling behaviour is observed in accel-
erated gradient schemes. AggMo (Lucas et al., 2018) keeps several velocity vectors with distinct
parameters in order to damp oscillations. AMSGrad (Reddi et al., 2018) on the other hand, keeps
a longer memory of the past gradients to overcome the suboptimality of the previous algorithms on
simple convex problems. We note that these techniques are orthogonal to our approach and can be
adapted to our geodesic update to further improve performance.
4
Under review as a conference paper at ICLR 2020
4.2	Accelerated Methods
Several recent works have been focusing on acceleration for gradient descent methods. Meng &
Chen (2011) propose an adaptive method to accelerate Nesterov’s algorithm in order to close a
small gap in its convergence rate for strongly convex functions with Lipschitz gradients adding a
possibility of more than one gradient call per iteration. In Su et al. (2014), the authors propose a
differential equation for modeling Nesterov inspired by the continuous version of gradient descent,
a.k.a. gradient flow. Wibisono et al. (2016) take this further and suggest that all accelerated methods
have a continuous time equivalent defined by a Lagrangian functional, which they call the Bregman
Lagrangian. They also show that acceleration in continuous time corresponds to traveling on the
same curve in spacetime at different speeds. It would be of great interest to study the differential
equation of geodesics in the same way. In a recent work, Defazio (2018) proposes a differential ge-
ometric interpretation of Nesterov’s method for strongly-convex functions with links to continuous
time differential equations mentioned earlier and their Euler discretization.
4.3	Second-order Methods
Second-order methods are desirable because of their fine convergence properties due to dealing
with bad-conditioned curvature by using local second-order information. Hessian-Free optimization
(Martens, 2010) is based on the truncated-Newton approach where the conjugate gradient algorithm
is used to optimize the quadratic approximation of the objective function. The natural gradient
method (Amari, 1998) reformulates the gradient descent in the space of the prediction functions
instead of the parameters. This space is then studied using concepts in differential geometry. K-FAC
(Martens & Grosse, 2015) approximates the Fisher information matrix which is based on the natural
gradient method. Our method is different since we are not using explicit second-order information
but rather implicitly deriving curvature information using the change in direction.
5	Experiments
We evaluated SGeO on strongly convex functions with Lipschitz gradients and benchmark deep
autoencoder problems and compared with the Heavy-Ball and Nesterov’s algorithms and K-FAC.
5.1	Strongly Convex and Lipschitz functions
We borrow these three minimization problems from Meng & Chen (2011) where they try to accel-
erate Nesterov’s method by using adaptive step sizes. The problems are Anisotropic Bowl, Ridge
Regression and Smooth-BPDN. The learning rate E for all methods is set to + except for Nesterov
which is set to 3工+& and the momentum parameter μ for Heavy Ball, Nesterov and GeO-N is set to
the following:
=1 - K一
μ = 1 + √μ
where L is the Lipschitz parameter and μ is the strong-convexity parameter. The adaptive parameter
Yt for Fletcher-Reeves is set to YFR and for GeO and GeO-N is YC = 1 - gt ∙ <¾. The function-
specific parameter α is set to 1, 0.5 and 0.9 in that order for the following problems. It’s important
to note that the approximate conjugate gradient method is only exact when an exact line search is
used, which is not the case in our experiments with a quadratic function (Ridge Regression).
Anisotropic Bowl The Anisotropic Bowl is a bowl-shaped function with a constraint to get Lips-
chitz continuous gradients:
n1
f(θ) = Ei ∙ θfi) + 2 kθk2
i=1
subject to kθk2 ≤ τ
(14)
As in Meng & Chen (2011), We set n = 500, τ = 4 and θ0 = √* 1. Thus L = 12nτ2 + 1 = 96001
and μ = 1. Figure 1 shows the convergence results for our algorithms and the baselines. The al-
gorithms terminate when f(θ) - f * < 10-12. GeO-N and GeO take only 82 and 205 iterations
5
Under review as a conference paper at ICLR 2020
0	1000	2000	3000	4000	5000
Iterations
Figure 1: AnisotroPicBoWl
Iterations
Figure 2: Ridge Regression
Figure 3: Smooth-BPDN
Figures 1 to 3: Results from experiments on
strongly convex functions with Lipschitz gradi-
ents. Geodesic and Geodesic-N are our methods
and the baselines are Gradient Descent, Heavy
Ball, Nesterov and Fletcher-Reeves. All meth-
ods start from the same point and are terminated
within a tolerance of the global optimum. The
horizontal axes show the iterations and the ver-
tical axes show the distance to the optimal value.
Figures are best viewed in color.
to converge, while the closest result is that of Heavy-Ball and Fletcher-Reeves which take approxi-
mately 2500 and 3000 iterations respectively.
Ridge Regression The Ridge Regression problem is a linear least squares function with Tikhonov
regularization:
f(θ) = 1 kAθ - bk2 + 2kθk2	(15)
where A ∈ Rm×n is a measurement matrix, b ∈ Rm is the response vector and γ > 0 is the ridge
parameter. The function f (θ) is a positive definite quadratic function with the unique solution of
θ* = (ATA + λI)-1 ATb along with Lipschitz parameter L = ∣∣Ak2 + λ and strong convexity
parameter μ = λ.
Following Meng & Chen (2011), m = 1200, n = 2000 and λ = 1. A is generated from U ΣV T
where U ∈ Rm×m and V ∈ Rn×m are random orthonormal matrices and Σ ∈ Rm×m is diagonal
with entries linearly distanced in [100, 1] while b = randn(m, 1) is drawn (i.i.d) from the standard
normal distribution. Thus μ = 1 and L ≈ 1001. Figure 2 shows the results where Fletcher-Reeves,
which is a conjugate gradient algorithm, performs better than other methods but we observe similar
performances overall except for gradient descent. The tolerance is set to f (θ) — f * < 10-13.
Smooth-BPDN Smooth-BPDN is a smooth and strongly convex version of the BPDN (basis pur-
suit denoising) problem:
f(θ) = 1 ∣Aθ - b∣2 + λ∣θk'ι,τ + P kθ∣2
where kθk'ι,τ = {lθlθ-
T if∣θ∣≥ T
if ∣θ∣ < τ
(16)
6
Under review as a conference paper at ICLR 2020
O
O
IoJ 2 3
Wwooo
(0-eos—60-)①
2 1
1010
(3-eos,6o-) -10」」3
Oo
8
Oo
6
Oo
4
Oo
2
O
50
S
4
S
3
∞
2
Figure 4:	MNIST dataset.
Figure 5:	FACES dataset.
O
O
10
(0-eos—60-)①
Figure 6: CURVES dataset
Figures 4 to 6: Results from autoencoder ex-
periments on three datasets. The horizontal axis
shows computation time and the vertical axis
shows log-scale training error. Our methods are
SGeO and SGeO-N and the baselines are SGD-
HB and SGD-N, variants of SGD that use the
Heavy Ball momentum and Nesterov,s momen-
tum respectively, along with K-FAC. All meth-
ods use the same initialization. SGeO-N is able to
outperform other methods on the MNIST dataset
and performs similarly to K-FAC on the other two
while outperforming other baselines. Figures are
best viewed in color.
and ∣∣∙∣∣'ι,τ is a smoothed version of the '1 norm also known as Huber penalty function with half-
width of τ. The function is strongly convex μ = P because of the quadratic term P∣∣θk2 with
Lipschitz constant L = ∣∣A∣2 + T + P.
As in Meng & Chen (2011), we set A = √n ∙ randn(m, 1) where m = 800, n = 2000, λ = 0.05,
T = 0.0001 and μ = 0.05. The real signal is a random vector with 40 nonzeros and b = Aθ* + e
where e = 0.01 √2 ∙ randn(m, 1) is Gaussian noise. Also L = ∣∣A∣2 + T + P ≈ 502.7. Since we
cannot find the solution analytically, Nesterov’s method is used as an approximation to the solution
(fN) and the tolerance is set to f (θ) 一 fN < 10-12. Figure 3 shows the results for the algorithms.
GeO-N and GeO converge in 308 and 414 iterations respectively, outperforming all other methods.
Closest to these two is Fletcher-Reeves with 569 iterations and Nesterov and Heavy Ball converge
similarly in 788 iterations.
5.2	Deep Autoencoders
To evaluate the performance of SGeO, we apply it to 3 benchmark deep autoencoder problems
first introduced in Hinton & Salakhutdinov (2006) which use three datasets, MNIST, FACES and
CURVES. Due to the difficulty of training these networks, they have become standard benchmarks
for neural network optimization. To be consistent with previous literature (Martens, 2010; Sutskever
et al., 2013; Martens & Grosse, 2015), we use the same network architectures as in Hinton &
Salakhutdinov (2006) and also report the reconstruction error instead of the log-likelihood objec-
tive. The layer structure for MNIST, FACES and CURVES are [1000 500 250 30 250 500 1000],
[2000 1000 500 30 500 1000 2000] and [400 200 100 50 25 625 50 100 200 400] respectively. All
layers use sigmoid activations except the middle layer for MNIST, the middle and the last layer for
FACES and the middle layer for CURVES which use linear activation.
7
Under review as a conference paper at ICLR 2020
Our baselines are the Heavy Ball algorithm (SGD-HB) (Polyak, 1964), SGD with Nesterov’s Mo-
mentum (SGD-N) (Sutskever et al., 2013) and K-FAC (Martens & Grosse, 2015), a second-order
method utilizing natural gradients using an approximation of the Fisher information matrix. Both
the baselines and SGeO were implemented using MATLAB on GPU with single precision on a sin-
gle machine with a 3.6 GHz Intel CPU and an NVIDIA GeForce GTX 1080 Ti GPU with 11 GBs
of memory.
The results are shown in Figures 4 to 6. Since we are mainly interested in optimization and not in
generalization, we only report the training error, although we have included the test set performances
in the Appendix B. We report the reconstruction relative to the computation time to be able to
compare with K-FAC, since each iteration of K-FAC takes orders of magnitude longer than SGD
and SGeO. The per-iteration graphs can be found in the Appendix A.
All methods use the same parameter initialization scheme known as ”sparse initialization” intro-
duced in Martens (2010). The experiments for the Heavy Ball algorithm and SGD with Nesterov’s
momentum follow Sutskever et al. (2013) which were tuned to maximize performance for these
problems. For SGeO, we chose a fixed momentum parameter and used a simple multiplicative
schedule for the learning rate:
Et = ∈1 X βb KC
where the initial value (1) was chosen from {0.1,0.15,0.2,0.3,0.4,0.5} and is decayed (K) every
2000 iterations (parameter updates). The decay parameter (β) was set to 0.95. For the momentum
parameter μ, We did a search in {0.999,0.995,0.99}. The minibatch size was set to 500 for all
methods except K-FAC which uses an exponentially increasing schedule for the minibatch size. For
K-FAC we used the official code provided 1 by the authors with default parameters to reproduce
the results. The version of K-FAC we ran was the Blk-Tri-Diag approach which achieves the best
results in all three cases. To do a fair comparison with other methods, we disabled iterate averaging
for K-FAC. It is also worth noting that K-FAC uses a form of momentum (Martens & Grosse, 2015).
In all three experiments, SGeO-N is able to outperform the baselines (in terms of reconstruction
error) and performs similarly as (if not better than) K-FAC. We can see the effect of the adaptive
coefficient on the Heavy Ball method, i.e. SGeO, which also outperforms SGD with Nesterov’s
momentum in two of the experiments, MNIST and FACES, and also outperforms K-FAC in the
MNIST experiment. Use of Nesterov style lookahead gradient significantly accelerates training for
the MNIST and CURVES dataset, while we see this to a lesser extent in the FACES dataset. This
is also the case for the other baselines (Sutskever et al., 2013; Martens & Grosse, 2015). Further,
we notice an interesting phenomena for the MNIST dataset (Figure 4). Both SGeO and SGeO-N
reach very low error rates, after only 900 seconds of training, SGeO and SGeO-N arrive at an error
of 0.004 and 0.0002 respectively.
6	Conclusion and future work
We proposed a novel and efficient algorithm based on adaptive coefficients for the Heavy Ball
method inspired by a geodesic optimization algorithm. We compared SGeO against SGD with
Nesterov’s Momentum and regular momentum (Heavy Ball) and a recently proposed second-order
method, K-FAC, on three deep autoencoder optimization benchmarks and three strongly convex
functions with Lipschitz gradients. We saw that SGeO is able to outperform all first-order methods
that we compared to, by a notable margin. SGeO is easy to implement and the computational over-
head it has over the first-order methods, which is calculating the dot product, is marginal. It can also
perform as effectively as or better than second-order methods (here, K-FAC) without the need for
expensive higher-order operations in terms of time and memory. We believe that SGeO opens new
and promising directions in high dimensional optimization research and in particular, neural net-
work optimization. We are working on applying SGeO to other machine learning paradigms such as
CNNs, RNNs and Reinforcement Learning. It remains to analyse the theoretical properties of SGeO
such as its convergence rate in convex and non-convex cases which we leave for future work.
1http://www.cs.toronto.edu/ jmartens/docs/KFAC3-MATLAB.zip
8
Under review as a conference paper at ICLR 2020
References
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251-
276, 1998.
Jean Francois AUjoL AUde Rondepierre, JF Aujol, Charles Dossal, et al. Optimal convergence rates
for nesterov acceleration. arXiv preprint arXiv:1805.05719, 2018.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223-311, 2018.
Aaron Defazio. On the curved geometry of accelerated optimization. arXiv preprint
arXiv:1812.04634, 2018.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Reeves Fletcher and Colin M Reeves. Function minimization by conjugate gradients. The computer
journal, 7(2):149-154, 1964.
Ricky Fok, Aijun An, and Xiaogong Wang. Geodesic and contour optimization using conformal
mapping. Journal of Global Optimization, 69(1):23-44, 2017.
Magnus Rudolph Hestenes and Eduard Stiefel. Methods of conjugate gradients for solving linear
systems, volume 49. NBS Washington, DC, 1952.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504-507, 2006.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Manuel Laguna and Rafael Marti. Experimental testing of advanced scatter search designs for global
optimization of multimodal functions. Journal of Global Optimization, 33(2):235-255, 2005.
James Lucas, Shengyang Sun, Richard Zemel, and Roger Grosse. Aggregated momentum: Stability
through passive damping. arXiv preprint arXiv:1804.00325, 2018.
James Martens. Deep learning via hessian-free optimization. In ICML, volume 27, pp. 735-742,
2010.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
Xiangrui Meng and Hao Chen. Accelerating nesterov’s method for strongly convex functions with
lipschitz gradient. arXiv preprint arXiv:1109.6058, 2011.
Yurii E Nesterov. A method for solving the convex programming problem with convergence rate o
(1∕k^ 2). In Dokl. akad. nauk Sssr, volume 269, pp. 543-547,1983.
Brendan Odonoghue and Emmanuel Candes. Adaptive restart for accelerated gradient schemes.
Foundations of computational mathematics, 15(3):715-732, 2015.
Victor Picheny, Tobias Wagner, and David Ginsbourger. A benchmark of kriging-based infill criteria
for noisy optimization. Structural and Multidisciplinary Optimization, 48(3):607-626, 2013.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-
putational Mathematics and Mathematical Physics, 4(5):1-17, 1964.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. 2018.
Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nesterovs
accelerated gradient method: Theory and insights. In Advances in Neural Information Processing
Systems, pp. 2510-2518, 2014.
9
Under review as a conference paper at ICLR 2020
S. Surjanovic and D. Bingham. Virtual library of simulation experiments: Test functions and
datasets. Retrieved April 14, 2019, from http://www.sfu.ca/~ssurjano.
Ilya Sutskever, James Martens, George E Dahl, and Geoffrey E Hinton. On the importance of
initialization and momentum in deep learning. ICML (3), 28(1139-1147):5, 2013.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26—
31, 2012.
Andre Wibisono, Ashia C Wilson, and Michael I Jordan. A variational perspective on accelerated
methods in optimization. proceedings of the National Academy of Sciences, 113(47):E7351-
E7358, 2016.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
A	Autoencoder Per-Iteration Results
Here we include the per-iteration results for the autoencoder experiments in Figures 7 to 9. We
reported the reconstruction error vs. running time in the main text to make it easier to compare
to K-FAC. K-FAC, which is a second-order algorithm, converges in fewer iterations but has a high
per-iteration cost. All other methods are first-order and have similar per-iteration costs.
Ja-1-2-3/
Wwoooo
(0-eos—60-)①
0	0.5	1	1.5	2
iteration	x105
Figure 7: MNIST dataset.
2 1
W W
(①-BoS—60-)」0」」①
0	2	4	6	8
iteration	x104
Figure 8: FACES dataset.
O
O
O
(0-eos—60-)①
2	4	6	8	10
iteration	x104
Figure 9: CURVES dataset
Figures 7 to 9: Results from autoencoder ex-
periments on three datasets. The horizontal axis
shows iterations and the vertical axis shows log-
scale training error. K-FAC is a second-order
method and takes much fewer iterations to con-
verge. However, the per-iteration cost is much
higher than the other methods. SGeO and SGeO-
N are our methods and the baselines are Nesterov
(SGD-N), Heavy Ball (SGD-HB) and KFAC-M
(M indicating a form of momentum). Figures are
best viewed in color.
10
Under review as a conference paper at ICLR 2020
Io-1-2-3-4
1 01 00°°°
(0-eos—60-)」0」」①
-SGeO
SGeO (test)
-SGeO-N
SGeO-N (test)
2 1
W W
(①-BoS—60-)」0」」①
0	2	4	6	8
iteration	x104
Figure 11: FACES dataset.
0	0.5	1	1.5	2
iteration	x105
Figure 10: MNIST dataset.
I O
W W
(0-B0S—60-)」0」」①
I-SGeO
1	- SGeO (test)
1	-SGeO-N
R	- SGeO-N (test)
2	4	6	8	10
iteration	x104
Figures 10 to 12 show the performance of the au-
toencoders evaluated on the test set while training.
Note that in all cases the algorithms are tuned to
maximize performance on the training set. The
dashed lines show the test error while the continu-
ous lines show the training error. The letter ”N” in
SGeO-N indicates the use ofNesterov style looka-
head gradient. Figures are best viewed in color.

Figure 12:	CURVES dataset
B Generalization Experiments
We include generalization experiments on the test set here. However, as mentioned before, our
focus is optimization and not generalization, we are aware that the choice of optimizer can have a
significant effect on the performance of a trained model in practise. Results are shown in Figures
10 to 12. SGeO-N shows a significant better predictive performance than SGeO on the CURVES
data set and both perform similarly on the two other datasets.. Note that the algorithms are tuned
for best performance on the training set. Overfitting can be dealt with in various ways such as using
appropriate regularization during training and using a small validation set to tune the parameters.
C Adaptive Coefficient behaviour
C.1 Geometric interpretation
Figure 13 (b) shows the dot product value g ∙ d which is equivalent to cos (∏ - φ) (where φ is the
angle between the previous update and the negative of the current gradient) for different values of φ.
Figure 13 (a) shows the adaptive coefficient (γ) behaviour for different values of φ for both convex
and non-convex cases. Recall that the adaptive coefficient is used on top the Heavy Ball method.
For strongly convex function with Lipschitz gradients we set YC = 1 - g ∙ d and for non-convex
cases γnc = 1 + g ∙ d.
C.2 Strongly convex functions with Lipschitz gradients
Here we include the values of the adaptive coefficient during optimization from our experiments.
The plots in Figures 14 to 16 show γt at each iteration for GeO and GeO-N. For the Anisotropic
11
Under review as a conference paper at ICLR 2020
O
2 5 15 0
6
luoooo ①≥ldep4
O
5 O 5-
□ 。
IUROyJOOO ①>4dBP4
Tr
-	/	I-COS(Tr-O)I -
__________________


(a) Adaptive coefficient
(b) Cosine value
Figure 13:	Adaptive Coefficient and cosine (dot product) values for the strongly convex YC = 1-g∙d
and non-convex YNC = 1 + g ∙ d cases where g ∙ d is equal to cos (∏ - φ).
——Geodesic
--Geodesic-N
IU0一。E①0。①≥ldep<
2 5 15 0
6
2.5
——Geodesic
--GeOdeSiC-N
50
100
150
200
0l
0
500
1000
1500	2000
2500
5
N
O
5
Iterations
Iterations
Figure 14:	Anisotropic Bowl
Figure 15:	Ridge Regression
F - - - 1 O
2 5 15 0
6
IU①一。①Oo ①≥ldep<
Figure 16: Smooth-BPDN
Figures 14 to 16 show the value of the adap-
tive coefficient per iteration during training for the
strongly convex functions with LiPschitz gradi-
ents from our experiments. Note that here YC =
1 - gt ∙ dt. Figures are best viewed in color.
Bowl and the Smooth BPDN problem, the values fluctuates between 0 and 2 periodically, with the
Smooth-BPDN behaving more sinusoidal and the Anisotropic Bowl more pulse-like. However, for
the ridge regression problem, the values gradually increase from 0 in the beginning and stay close
to 2 thereafter, indicating φ ≈ 0 (convergence) since YC = 1 - cos (π - φ)
12
Under review as a conference paper at ICLR 2020
2
2
U
φ
(D
O
O
1.5
1
-SGeO
-SGeO-N
(D
>
'¾0.5
E
P
<
0
U
Φ
⅛
(D
O
O
(D
>
1.5

0	0.5	1	1.5	2
'¾0.5
E
P
<
0
-SGeO
-SGeO-N
0	2	4	6	8
iteration
x105
iteration
x104
Figure 17:	MNIST dataset.
Figure 18:	FACES dataset.
1
Figure 19: CURVES dataset
Figures 17 to 19 show the behaviour of the adap-
tive coefficient per iteration for the autoencoder
experiments. Note that here YNC = 1 + gt ∙ dt
and the experiments are taken from optimization
on the training set. We can see the effect of
Nesterov,s lookahead gradient on the autoencoder
benchmarks. Figures are best viewed in color.
C.3 Autoencoders
The adaptive coefficient values during training for the autoencoder experiments is shown in Figures
17 to 19. The values for all three problem stay close to values between 1 and 1.5 indicating 2 <
φ < 2∏. However, interpreting these values would not be as exact as our convex experiments due
to partial gradient information, stochasticity and the non-convex nature of the problems. Further,
adding lookahead gradient clearly decreases fluctuations in the adaptive coefficient value, thus a
more stable training process. [t]
D	Global Optimization benchmarks
We compared our method (non-stochastic version using γNC) with Nesterov’s method on global
optimization benchmarks with two parameters to facilitate visualization. The results showing the
contour plots and the paths taken by the algorithms can be found in Figure 20.
Levy Function The Levy function (Surjanovic & Bingham; Laguna & Marti, 2005) features a
wavy surface in both directions, multiple ravines and local minima. The global minimum is at
(0, 0). The function is:
f(θ) = sin2 (πw1) + (w1 - 1)2[1 + 10 sin2 (πw1 + 1)] + (w2 - 1)2[1 + sin2 (2πw2)]	(17)
where Wi = 1 + θi-1 for i = 1, 2. We initialize all three methods at (9,10).
Scaled Goldstein-Price Function The scaled Godstein-Price function (Surjanovic & Bingham;
Picheny et al., 2013) features several local minima, ravines and plateaus which can be representative
13
Under review as a conference paper at ICLR 2020
CN
10
5
0
-5
-10
-10	-5	0
5	10
Θ
1
(b) Goldstein-Price function.
Figure 20: Results from the scaled Goldstein-Price and the Levy function experiments. The figures
show the contour plots of the functions and the paths taken by the methods in different colors.
The numbers in front of each method’s name in the legend show the total iterations it took for the
algorithm to get to a tolerance of 0.001 from the global minimum or reached maximum iterations
(1000). Geodesic is our method and N indicates the use of Nesterov’s lookahead gradient. The
global minimum for (a) is at (0,0) and for (b) is at (0.5,0.25). All methods start from the same
point and the hyper-parameters are tuned for best performance. Although the algorithms used are
not stochastic, the adaptive coefficient used for these problems is γNC since the functions are non-
convex. Figures are best viewed in color.
(a) Levy function.
肉
of a deep neural network’s loss function. The global minimum is at (0.5, 0.25). The function is:
f(θ)
2 I?7 h log ([1 + (θι + θθ2 + I)2(19 - 14θι + 3Θ2 -
14& + 6Θ1Θ2 +30∣)]
[30+ (20 - 3θ2)2(18 - 32θ1 + 12θ2 +48(92 - 36θ1θ2 + 27θ∣)]) - 0.86931
(18)
where θ9i = 4θ9i - 2 for i = 1, 2. We initialize all methods at (1.5, 1.5).
Details The momentum parameter μ for both NesteroV and Geodesic-N was set to 0.9. The learn-
ing rate for all methods is fixed and is tuned for best performance. The results from both experiments
indicate that Geodesic is able to effectiVely escape local minima and recoVer from basins of attrac-
tion, while NesteroV’s method gets stuck at local minima in both cases. We can also obserVe the
effect of lookahead gradient on our method where the path taken by Geodesic-N is much smoother
than Geodesic.
14