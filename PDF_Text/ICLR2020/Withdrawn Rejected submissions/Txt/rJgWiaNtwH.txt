Under review as a conference paper at ICLR 2020
Bridging the ELBO and MMD
Anonymous authors
Paper under double-blind review
Ab stract
One of the challenges in training generative models such as the variational auto
encoder (VAE) is avoiding posterior collapse. When the generator has too much
capacity, it is prone to ignoring latent code. This problem is exacerbated when the
dataset is small, and the latent dimension is high. The root of the problem is the
ELBO objective, specifically the Kullback-Leibler (KL) divergence term in ob-
jective function (Zhao et al., 2019). This paper proposes a new objective function
to replace the KL term with one that emulates the maximum mean discrepancy
(MMD) objective. It also introduces a new technique, named latent clipping, that
is used to control distance between samples in latent space. A probabilistic au-
toencoder model, named μ-VAE, is designed and trained on MNIST and MNIST
Fashion datasets, using the new objective function and is shown to outperform
models trained with ELBO and β-VAE objective. The μ-VAE is less prone to Pos-
terior collapse, and can generate reconstructions and new samples in good quality.
Latent representations learned by μ-VAE are shown to be good and can be used
for downstream tasks such as classification.
1	Introduction
Autoencoders(AEs) are used to learn low-dimensional representation of data. They can be turned
into generative models by using adversarial, or variational training. In the adversarial approach, one
can directly shape the posterior distribution over the latent variables by either using an additional
network called a Discriminator (Makhzani et al., 2015), or using the encoder itself as a discriminator
(Huang et al., 2018). AEs trained with variational methods are called Variational Autoencoders
(VAEs) (Kingma & Ba, 2014; Rezende et al., 2014). Their objective maximizes the variational lower
bound (or evidence lower bound, ELBO) ofpθ(x). Similar to AEs, VAEs contain two networks:
Encoder - Approximate inference network: In the context of VAEs, the encoder is a recognition
model q@(z ∣χ)1,which is an approximation to the true posterior distribution over the latent variables,
Pθ (z|x). The encoder tries to map high-level representations of the input X onto latent variables such
that the salient features of x are encoded on z .
Decoder - Generative network: The decoder learns a conditional distribution pθ(x|z) and has
two tasks: i) For the task of reconstruction of input, it solves an inverse problem by taking mapped
latent z computed using output of encoder and predicts what the original input is (i.e. reconstruction
x0 ≈ x). ii) For generation of new data, it samples new data x0 , given the latent variables z.
During training, encoder learns to map the data distribution pd(x) to a simple distribution such
as Gaussian while the decoder learns to map it back to data distribution p(x) 2. VAE’s objective
function has two terms: log-likelihood term (reconstruction term of AE objective function) and a
prior regularization term 3. Hence, VAEs add an extra term to AE objective function, and approxi-
mately maximizes the log-likelihood of the data, log p(x), by maximizing the evidence lower bound
(ELBO):
1 φ refers to parameters of encoder while θ is parameters of decoder.
2Note that output distribution of decoder is model distribution p(x), not data distribution pd(x)
3One such term would be Kullback-Leibler (KL) divergence, which is a measure of how similar two proba-
bility distributions are.
1
Under review as a conference paper at ICLR 2020
LELBO = Epd(X) [Eqφ(z∣x) [logPθ(XIz)]] - Epd(X) [KL(q°(ZIx)kP(Z))]	⑴
Maximizing ELBO does two things:
•	Increase the probability of generating each observed data x.
•	Decrease distance between estimated posterior q(ZIx) and prior distribution p(Z), pushing
KL term to zero. Smaller KL term leads to less informative latent variable.
Pushing KL terms to zero encourages the model to ignore latent variable. This is especially true
when the decoder has a high capacity. This leads to a phenomenon called posterior collapse in
literature (Razavi et al., 2019; Chen et al., 2016; Dieng et al., 2018; Kim et al., 2018; van den Oord
et al., 2017; Bowman et al., 2015; Kingma et al., 2016; S0nderby et al., 2016; Zhao et al., 2017).
This work proposes a new method to mitigate posterior collapse. The main idea is to modify the KL
term of the ELBO such that it emulates the MMD objective (Gretton et al., 2007; Zhao et al., 2019).
In ELBO objective, minimizing KL divergence term pushes mean and variance parameters of each
sample at the output of encoder towards zero and one respectively. This , in turn, brings samples
closer, making them indistinguishable. The proposed method replaces the KL term in the ELBO
in order to encourage samples from latent variable to spread out while keeping the aggregate mean
of samples close to zero. This enables the model to learn a latent representation that is amenable
to clustering samples which are similar. As shown in later sections, the proposed method enables
learning good generative models as well as good representations of data. The details of the proposal
are discussed in Section 4.
2	Related Work
In the last few years, there have been multiple proposals on how to mitigate posterior collapse. These
proposals are concentrated around i) modifying the ELBO objective, ii) imposing a constraint on the
VAE architecture, iii) using complex priors, iv) changing distributions used for the prior and the
posterior v) or some combinations of these approaches. Modifications of the ELBO objective can be
done through annealing the KL term (S0nderby et al., 2016; Bowman et al., 2015), lower-bounding
the KL term to prevent it from getting pushed to zero (Razavi et al., 2019), controlling KL capacity
by upper bounding it to a pre-determined value (Burgess et al., 2018) or lower-bounding the mutual
information by adding skip connections between the latent layer and the layers of the decoder (Dieng
et al., 2018). Proposals that constrain the structure of the model do so by reducing the capacity of the
decoder (Bowman et al., 2015; Yang et al., 2017; Gulrajani et al., 2016), by adding skip connections
to each layer of the decoder (Dieng et al., 2018), or by imposing constraints on encoder structure
(Razavi et al., 2019). Taking a different approach, Tomczak & Welling (2017) and van den Oord
et al. (2017) replace simple Gaussian priors with more complex ones such as a mixture of Gaussians.
The most recent of these proposals are δ-VAE (Razavi et al., 2019) and SKIP-VAE (Dieng et al.,
2018). δ-VAE imposes a lower bound on KL term to prevent it from getting pushed to zero. One
of the drawbacks of this approach is the fact that it introduces yet another hyper-parameter to tune
carefully.Also, the model uses dropout to regularize the decoder, reducing the effective capacity
of the decoder during training. It is not clear how effective the proposed method is when training
more powerful decoders without such regularization. Moreover, the proposal includes an additional
constraint on encoder structure, named the anti-causal encoder.
SKIP-VAE , on the other hand, proposes to lower bound mutual information by adding skip connec-
tions from latent layers to each layer of decoder. One drawback of this approach is that it introduces
additional non-linear layer per each hidden layer, resulting in more parameters to optimize. More-
over, its advantage is not clear in cases, where one can increase capacity of decoder by increasing
number of units in each layer (or number of channels in CNN-based decoders) rather than adding
more layers.
3	The problem statement:
When we train a VAE model, we ideally want to end up with a model that can reconstruct a given
input well and can generate new samples in high quality. Good reconstruction requires extracting
2
Under review as a conference paper at ICLR 2020
the most salient features of data and storing them on latent variable (’Encoder + Latent layer’ part
of the model). Generating good samples requires a generative model (’Latent layer + Decoder’ part)
with a model distribution that is a good approximation to actual data distribution.
However, there tends to be a trade-off between reconstruction quality of a given input, and quality
of new samples. To understand why we have such a trade-off, we can start by looking at ELBO
objective function4:
LELBO=Eqφ(z∣x) [logPθ(x|z)] - KL(qφ(z∣x) ∣∣p(z))	⑵
Maximizing this objective function increases pθ (x), the probability of generating each observed data
x while decreasing distance between q(z|x) and prior p(z). Pushing q(z|x) closer to p(z) makes
latent code less informative i.e. z is influenced less by input data x.
The reason why the KL term can be problematic becomes more clear when we look at the KL loss
term typically modelled with log of variance during optimization:
D
KLioss = 2 X [μdi)2 + [eχp(logσ2*) - (logσ2)di) - 1]	⑶
d=1
where D is the dimension of latent variable, and i refers to ith sample. Noting that the mean is in
(i)
L2 norm, minimizing the KL term leads to pushing the each dimension of the mean,μd , to zero
while pushing σ2 towards 1. This makes estimated posterior less informative and less dependent
on input data. The problem gets worse when dimension of latent variable, D, increases, or when
the KL term is multiplied with a coefficient β > 1 (Higgins et al., 2017). Ideally, we want to be
able to distinguish between different input samples. This can be achieved by having distinctive
means and variances for clusters of samples. This is where MMD might have advantage over the
KL divergence. Matching distributions using MMD can match their sample means although their
variance might still differ.
4	Proposal： μ-VAE
We can emulate behaviour of MMD by modifying the KL term. We do so by changing L2 norm of
mean, pD=ι μdi)2 to L1 norm, |pD=i μdi)∣. Re-writing it for B samples, We have:
BD
B XX μdi)∣	(4)
i=1 d=1
It is important to note that we are taking absolute value of sum of sample means. This new formu-
lation results in aggregate mean of samples to be zero (i.e. same mean as that of prior distribution)
while allowing samples to spread out and enabling model to encode information about input data
onto z. It should be noted that this new L1 norm of μ can push individual mean estimates to very
high values if it is not constrained. To avoid that, L2 norm of means for each sample is clipped
by a pre-determined value during optimization. Based on experiments, it is found that clipping L2
norm of sample means by three times square root of latent dimension works well in general although
bigger values might help improve results in tasks such as classification:
∣∣μsamplek ≤ 3 * ʌ/zdim
(5)
This method will be referred as latent clipping for the rest of this work. In addition, the remaining
terms in the KL loss can be kept as is, i.e. exp (log σ2) - log σ2 - 1 , or we can just optimize
4Ignoring Epd(x) [.] term for clarity
3
Under review as a conference paper at ICLR 2020
for subset of it by using either ”log σ2”, or ” [exp (log σ2) - 1] ” term since each method will push
log σ2 towards zero (i.e. variance towards one). log σ2 is chosen in this work since it is simpler.
Finally, the μ-VAE objective function can be formulated as follows:
Lμ-VAE =万 B	-B J	B	D	B	D	] X X kxji) - xj(i) k2 + XX μdi) I + XX [log σ2] di i=1 j=1	i=1	d=1	i=1	d=1
where first term is reconstruction loss, B refers to batch size since aggregated mean is computed
over batch samples, J refers to dimension of data, D refers to dimension of latent variable, x is
original input, and x0 is reconstructions.
4.1	Latent Clipping:
To visualize the implications of the latent clipping, a toy VAE model shown in Table 2 in Appendix A
is used. Figure 1 compares three cases, in which a VAE model is trained on MNIST dataset using
ReLu, Tanh, and Leaky ReLu activation functions for each case, and the latent layer is visualized
to observe clustering of digits. Those three cases are: i) Standard VAE objective with the KL
divergence, ii) Standard VAE objective with the KL divergence + latent clipping, and iii) μ-VAE
objective function + latent clipping. Two observations can be made:
1.	Latent clipping might help improve smoothness of latent space, even in the case of standard
VAE objective, ELBO.
2.	μ-VAE objective function seems to work well.
Figure 1: Clustering of MNIST: From left to right: Tanh, ReLu, Leaky ReLu. Top row: Standard
VAE training with ELBO objective, Middle row: Standard VAE with ELBO objective and latent
clipping, Bottom row: μ-VAE objective function and latent clipping.
5	Experiments
To test the effectiveness of μ-VAE objective, a CNN-based VAE model is designed and trained on
MNIST and MNIST Fashion using same hyper-parameters for both datasets. Centered isotropic
4
Under review as a conference paper at ICLR 2020
Gaussian prior, p(z)〜N(0, 1.0), is used and the true posterior is approximated as Gaussian with
an approximately diagonal co-variance. No regularization methods such as dropout, or techniques
such as batch-normalization is used to avoid having any extra influence on the performance, and to
show the advantages of the new objective function.
The model is trained with four different objective functions: i) VAE (ELBO objective), ii) β-VAE
with β = 4, iii) 〃-VAE#1 s.t. kμsampiek ≤ 3 *√zdim andiv) 〃-VAE#2 s.t. kμsampiek ≤ 6 *√zdim,
where zdim = 10. Details of architecture, objective functions, hyper-parameters, and training are
described in Appendix B.
During training of the models, a simple three layer fully connected classifier is also trained over
10 dimensional latent variable to learn to classify data using features encoded on latent variable.
Classifier parameters are updated when encoder and decoder parameters are frozen and vice versa
so that classifier has no impact on how information is encoded on the latent variable.
5.1	Evaluation
Evaluation of the generative model is done qualitatively in the form of inspecting quality, and diver-
sity of samples. Posterior collapse is assessed by comparing reconstructions of input data to observe
whether the decoder ignores latent code encoded by input data and by comparing the KL diver-
gences obtained for each model. For all three objective functions, the KL divergence is measured
using standard KL formula in Equation 3. Moreover, the accuracy of the classifier trained on latent
variable is used as a measure of how well the latent variable represents data (Dieng et al., 2018).
Higher classification accuracy reflects a better representation of data and opens doors to use latent
representation for downstream tasks such as classification.
5.2	Results
Figure 2 shows training curves for MNIST Fashion dataset (MNIST results can be seen in Ap-
pendix C). The new objective function results in lower reconstruction loss, higher KL divergence,
and higher classification accuracy. Higher KL divergence and classification accuracy can be inter-
preted as a sign of learning a more informative latent code. β-VAE performs the worst across all
metrics as expected. The reason is that β factor encourages latent code to be less informative, and is
known to result in worse reconstruction quality (Higgins et al., 2017).
Figure 2:	Training curves of the model trained on MNIST Fashion using each of four objectives:
VAE (ELBO), β-VAE (β = 4), m-VAE#1 and m-VAE#2. Plots from left to right: reconstruc-
tion loss, KL divergence, classification accuracy and regularization loss of μ-VAE (PB=I μn∣ +
PnB=1 log σ2n) i.e. the term that replaces KL.
Figure 3 shows results of t-SNE (Maaten & Hinton, 2008) of samples obtained using test data for
both datasets. VAE seems to able to distinguish all ten digits, but performs worse in MNIST Fashion.
β-VAE pushes samples closer as expected, which explains why its performance is low in classifi-
cation task. μ-VAE, on the other hand, is able to cluster similar samples together in both datasets.
Moreover, when upper-bound on kμsampiek is increased, it spreads out clusters of samples, making
them easier to distinguish. Hence, upper-bound used in latent clipping can be a knob to control
distance between samples. Also, we should note that we can achieve similar clustering results to the
5
Under review as a conference paper at ICLR 2020
Figure 3:	Clustering of samples from latent layer obtained using t-SNE and test datasets of MNIST
(top row) and MNIST Fashion (bottom row). From left to right: VAE, β-VAE, m-VAE#1, and
M-VAE#2.
one obtained by VAE through reducing the upper-bound. Smaller upper-bound closes gaps between
the clusters.
Table 1 lists classification accuracy obtained using test datasets. μ-VAE performs the best as ex-
pected since it is able to push the clusters apart. Higher upper bound on ∣∣μsampiek results in a
higher classification accuracy. Also, it should be noted that reported accuracy numbers can be im-
proved, but the purpose of this test was to show that new objective function can reliably be used in
downstream tasks such as classification.
Table 1: Comparing test accuracy of classifiers trained on latent variables of each model.
Model	MNIST	MNIST Fashion
VAE	93.422	78.01
β-VAE	62.45	62.35
μ-VAE #1	95.28	82.71
μ-VAE #2	96.44	84.260
Figure 4 compares sample distributions obtained at each dimension of latent variable using test
dataset of MNIST Fashion for each objective function. β-VAE samples follow N(0, 1) prior very
closely, and hence resulting in the smallest KL divergence term. Sample distributions from the most
dimensions of VAE are also close to prior distribution although some of them show a multi-modal
behavior. Sample distributions from both m-VAE#1 & #2 result in zero mean, but they are more
spread out as expected. Spread is controlled by upper-bound on ∣μsampie∣. Similar to VAE, some
sample distributions show a multi-modal behavior.
Figure 5 shows reconstruction of input using test dataset. β-VAE reconstructions are either blurrier,
or wrong, the latter of which is a sign of posterior collapse. VAE performs better, and both versions
of μ-VAE gives the best reconstruction quality.
Figure 6 shows images generated using random samples drawn from multivariate Gaussian, N(0, σ),
where σ = 1 is for VAE and β-VAE while it is 3 for μ-VAE since their samples are more spread
out (MNIST results can be seen in Appendix C). We can observe that some samples generated
from μ-VAE models have dark spots. This is because the model is trying to generate texture on
these samples. This can also be observed in samples of VAE model, but it is less pronounced.
However, samples from β-VAE do not show any such phenomena since the model perhaps learns
global structure of shapes while ignoring local features. Failing to capture local structures is a known
problem in latent variable models (Larsen et al., 2015; Razavi et al., 2019).
Figure 7 shows latent traverse in each dimension of latent variable for MNIST Fashion (MNIST re-
sults can be seen in Appendix C). Each dimension of VAE and β-VAE is swept in [-2, 2] range lin-
early while other dimensions are kept at zero. For μ-VAE models, ranges of [-10,1θ] and [-20,20]
are used since samples are more spread out. β-VAE gives mostly similar classes of objects, a sign
6
Under review as a conference paper at ICLR 2020
Figure 4: Sample distribution obtained at each dimension of latent variable using test dataset of
MNIST Fashion. From top to bottom: VAE, β-VAE, m-VAE#1, and m-VAE#2.
Figure 5: Reconstructions of input data obtained using test dataset of MNIST Fashion. Top row:
vae (left), β-VAE (right). Bottom row: m-VAE#1 (left), m-VAE#2 (right).
用其一量七ɪ
■ fl j∣t Il ml 5
∣∣ ⅛t 4e * A ♦
•・，・h« ^flbι
曲上&翁∙fM*
KiII*华AE
f>A ^fβfr
Figure 6: Random samples drawn from multi-variate Gaussian, N(0, σ).From left to right, model
(σ): VAE (σ=1), β-VAE (σ=1), m-VAE#1 (σ=3), and m-VAE#2 (σ=3). Higger σ is used for μ-VAE
models since their samples are more spread out.
7
Under review as a conference paper at ICLR 2020
that most dimensions of latent variable are not very informative. VAE is slightly better. However,
both μ-VAE models learn diverse classes of objects across different dimensions. Moreover, they
learn different classes on opposite sides of same dimension. This is encouraging since it shows its
power to learn rich representations.
Traversing latent space
Bits 1 而
IIIIlIHn
3 4 5 6 7 8910
Icn JU £3
.∙4TWnAMlfAT
.∙M / BnftHitAY
∙>RAMAfflr
∙*RftBifAY
∙<^ΛA≡1∙⅞
∙4qκA∙J,身⅛*
∙4* RAHlfAY
*3 ∕x*1l上■周⅛∙
▲入餐・囿V
■4* R AHitWy
■办▲只AMJ∙βl*
*1/只*1IJ∙∙⅛
;;*■♦■«»
*
βΛΛ≡fil∙⅞
A*Λβ**l*∙>
Λ**aa4laa*β
鼻*■»«*aa*β
a∙ ∙∙1⅜^a&8■
Ax∙∙ifaaβa
^eA
n*β∙fva
ΛΛβ∙*swa ^fia
>4t*f« ^fia
飙x∙*，。红
^a
,7112,2 /B
A
S⅞1Q∙JC5I 地 ⅛
承*2∙04va*
≡π
23456789101234 5678
4-l-un JU2B~I1dun4->uαj%~l

;一 Jl
io
Range
Figure 7: Traversing each dimension of latent variable in 40 steps. From top to bottom, the model
and sweeping range used for the model: VAE [-2,2], β-VAE [-2,2], m-VAE#1 [-10,10], m-VAE#2
[-20,20].
6 Summary
In this work, a new objective function is proposed to mitigate posterior collapse observed in VAEs.
It is shown to give better reconstruction quality and to learn good representations of data in the
form of more informative latent codes. A method, named latent clipping, is introduced as a knob
to control distance between samples. Samples can be pushed apart for tasks such as classification,
or brought closer for smoother transition between different clusters of samples. Unlike prior work,
the proposed method is robust to parameter tuning, and does not constraint encoder, or decoder
structure. It can be used as a direct replacement for ELBO objective. Moreover, the proposed
method is demonstrated to learn representations of data that can work well in downstream tasks
such as classification. Applications of μ-VAE objective with more powerful decoders in various
settings can be considered as a future work.
8
Under review as a conference paper at ICLR 2020
References
Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.
Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Des-
jardins, and Alexander Lerchner. Understanding disentangling in β-vae. arXiv preprint
arXiv:1804.03599, 2018.
Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint arXiv:1611.02731,
2016.
Adji B Dieng, Yoon Kim, Alexander M Rush, and David M Blei. Avoiding latent variable collapse
with generative skip models. arXiv preprint arXiv:1807.04863, 2018.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Scholkopf, and Alex J Smola. A kernel
method for the two-sample-problem. In Advances in neural information processing systems, pp.
513-520, 2007.
Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez,
and Aaron Courville. Pixelvae: A latent variable model for natural images. arXiv preprint
arXiv:1611.05013, 2016.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. ICLR, 2(5):6, 2017.
Huaibo Huang, Ran He, Zhenan Sun, Tieniu Tan, et al. Introvae: Introspective variational autoen-
coders for photographic image synthesis. In Advances in Neural Information Processing Systems,
pp. 52-63, 2018.
Yoon Kim, Sam Wiseman, Andrew C Miller, David Sontag, and Alexander M Rush. Semi-amortized
variational autoencoders. arXiv preprint arXiv:1802.02550, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Im-
proved variational inference with inverse autoregressive flow. In Advances in neural information
processing systems, pp. 4743-4751, 2016.
Anders Boesen Lindbo Larsen, S0ren Kaae S0nderby, Hugo Larochelle, and Ole Winther. Autoen-
coding beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300, 2015.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(Nov):2579-2605, 2008.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. arXiv preprint arXiv:1511.05644, 2015.
Ali Razavi, Aaron van den Oord, Ben Poole, and Oriol Vinyals. Preventing posterior collapse with
delta-vaes. arXiv preprint arXiv:1901.03416, 2019.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Casper Kaae S0nderby, Tapani Raiko, Lars Maal0e, S0ren Kaae S0nderby, and Ole Winther. How
to train deep variational autoencoders and probabilistic ladder networks. In 33rd International
Conference on Machine Learning (ICML 2016), 2016.
9
Under review as a conference paper at ICLR 2020
Jakub M Tomczak and Max Welling. Vae with a vampprior. arXiv preprint arXiv:1705.07120, 2017.
Aaron van den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In Advances in
Neural Information Processing Systems,pp. 6306-6315, 2017.
Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. Improved variational
autoencoders for text modeling using dilated convolutions. In Proceedings of the 34th Interna-
tional Conference on Machine Learning-Volume 70, pp. 3881-3890. JMLR. org, 2017.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Towards deeper understanding of variational
autoencoding models. arXiv preprint arXiv:1702.08658, 2017.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Balancing learning and inference in
variational autoencoders. In Proceedings of the AAAI Conference on Artificial Intelligence, vol-
ume 33, pp. 5885-5892, 2019.
10
Under review as a conference paper at ICLR 2020
A Model architecture used for toy example of latent clipping
Table 2: Toy VAE Model for visualization of experiments
Function	Layer
Encoder	Fully Connected Layer 784x512 + Activation Fully Connected Layer 512x512 + Activation Fully Connected Layer 512x512 + Activation
Sampling	Linear layer with 2 units
Decoder	Fully Connected Layer 2x512 + Activation Fully Connected Layer 512x512 + Activation Fully Connected Layer 512x512 + Activation Fully Connected Layer 512x784 + Sigmoid
*Activation functions are all either Leaky ReLu (alpha=0.2), ReLu, or Tanh as part of the experimentation.	
B Model architecture, objective functions and details of
training.
Optimization: In all experiments, learning rate of 1e-4 and batch size of 64 are used. Adam al-
gorithm with high momentum (β1 = 0.9, β2 = 0.999) is used as optimizer. High momentum is
chosen mainly to let most of previous training samples influence the current update step.
For reconstruction loss, mean square error, kx-x0k2, is used for all cases. As for initialization, since
the model consists of convolutional layers with Leaky ReLu in both encoder and decoder, Xavier
initialization is used Glorot & Bengio (2010). Thus, initial weights are drawn from a Gaussian
distribution with standard deviation (Stdev) of，2/N, where N is number of nodes from previous
layer. For example, for a kernel size of 3x3 with 32 channels, N = 288, which results in stdev of
0.083.
Objective functions are shown in Table 3, where μ-VAE objective is written explicitly to avoid any
ambiguity in terms of how batch statistics are computed.
Table 4 shows model architecture as well as classifier used for all experiments. It consists of CNN-
based encoder and decoder while classifier is three layer fully connected neural network. They all
use Leaky Relu activation and learning rate of 1e-4.
Table 3: Objective functions5
Objective type			Objective	
ELBO	Le =	二 Eqφ(z∣x) [logPθ(XIz)]	-KLSφ(ZIX)kp(Z))
β-VAE	Lβ	二 Eqφ(z∣x) [logPθ (XIz)]	-β * KL(qφ(ZIX)kp(Z))
μ-VAE	Lμ 二	二 Eqφ(z∣x) [logPθ (XIz)]	-B h∣P3PD=1 μdi)I + Pi=IPD=1 [logσ2]di)i
5Note that each Eqφ(z∣χ)[.] and KL[.] is computed on expectation, Epdata(X)[」，but it is not shown explicitly
in the formulas above to make them more readable. Also, regularization term in μ-VAE is shown explicitly to
emphasize how sample means are computed over batches.
11
Under review as a conference paper at ICLR 2020
Table 4: Model used for comparing VAE, β-VAE and μ-VAE
Function	Layer
Encoder	2D Conv 28x28x64 + Leaky Relu 2D Conv 14x14x64 + Leaky Relu 2D Conv 7x7x64 + Leaky Relu Linear layer with (7x7x64)x10
Sampling	10 Latent layer units
Decoder	Linear Layer 10x(7x7x64) + Leaky Relu 2D DeConv 7x7x64 + Leaky Relu 2D DeConv 14x14x32 + Leaky Relu 2D DeConv 28x28x1 + Sigmoid
Classifier	Dense 10x1024 + Leaky Relu Dense 1024x1024 + Leaky Relu Dense 1024x1024 + Leaky Relu Dense 1024x10 + Softmax
*All Leaky ReLU layers use alpha=0.2.	
12
Under review as a conference paper at ICLR 2020
C MNIST Results
Figure 8: Training curves of the model trained on MNIST. Regularization loss of μ-VAE defined as
PB=I μn∣ + PB=1 [logσ2]n, i.e. term that replaces KL.
θ
Θ
Q
3
S S
8
8
θ
6
β
g g
b b
g
&
9
9
3
β
6
S
B
g
8
a
6
s
I 1
a 9
9
9
B
B
⅞ 3
8 6
8 g g g g
9 9 9 9 9
B
q
S
g
9
8 β B 6 θ
6
3
S 6 8 6 θ θ
B e 9 S 9 B
3 3 3 3 3 3
9 β
9 9
g7e∙0Ω2a 36 S
9/8∙0Ω2a 86 4
q g
Q g
q 8
W &
Cfg ∖
Ob,
0 6、
QL'
q8∖
Qg、
θ
a
55555556ftS83
aawx#WNaaWaW
令令3999999233
333333333333
9
S 5
9
9
ð
8
g
9
9
9
g
3
B
9
3
8
ð
∂
3
Θ
g
ð
9
9
9
g
6
β
θ
9
&
9
3 3
g。
99
3 8
g ð
3 3
& g
9 9
S S
g
6
θ
θ
G
0
0
8
9
0
9
β
g
6
B
Q
0
8
G
6
θ
β
θ
0
8 β 6 g
B 8 S 3
9
9
B
9
S
9
S
S
S
9
§ g g 9
9 9 6 9
g
9
8
0
9
g
8
8
0
g
0
9
g
9
S
0
g
β
g
β
8
g
β
β
g
β
O 7
0 OJ
g S
99Sa
U-
q
99
9
9
8
x⅛
C
/
λ7
9
q
z⅛
Zfc
-ɔ
-□
5 a
5 a
5 d
∖j α / ?45 a
/7q 54
6t< /。XSa
^4/ Oq 5a
4 4/ ς^q5a
匕 4 /。q 5 a
「 4 / Jq 5a

Θ
9
ð

3
θ
θ
θ
9
8
9
θ
B
B
B
B
B
β
6
6
Θ88888888888B888683
c> £?
b 5 5 b 5 5 5
1111111
a a w a a a a
3 3 9 3 3 3 3
3 3^3333

7 9
79
7<0
7to
79
79
79
79
79
M.
U-
4
y
U∙
U
3
g
9
9
g
9
3
9
3
ð
3
&
g
∂
θ
g
3 3333333333
©80900000000
9
&
q
s
9 3
& &
g g g g
9 3 3 3
◎ Q g 9
WQqCl
5 5 5 5
7 7 7 7
9
B
38gS53gS5SS5S
Sgggagggggggg
8666666666666
βθβθβ9βθ∂0βββ
ΘΘOOOOOOOOOOO
βBS6ββeβθ8β8β
999993998899698888
SggggggggEgggggggg
9^99^^99^997977777
Q3
as
a 3

73
72
7ΛJ
7?
7 QJ
73
72
73
73
7nJ
73
7?
73
7 ?
7 3
32
3 3
73
7 S
7 3
7 S
73
7 S
7 3
7 3
7 S
7 S
7 S
7s
7 S
7 3
3 3
，3
3 3
O LJ
O L
O 5
。5
O 7
O LJ
。5
Av 7
O 5
。S
F:
•A
5
-ɔ
=
Z
e
e
β
g
β
g
g
g
β
g
g
a
a
a
Q
Q
β
8
e
8
B
g
Figure 9: Traversing each dimension of latent variable in 40 steps. From top to bottom, model
[range]: VAE [-2,2], β-VAE [-2,2], 〃-VAE#1 [-10,10], and 〃-VAE#2 [-20,20].
13
Under review as a conference paper at ICLR 2020
6G0a9/ 7 y
207c)δ∕c5
5 7 y76
θ 73 7, 7r7
SQo27x 7。
厂 JkDN 6夕“4
ʒʒ tIL C4
Figure 10: Random samples drawn from multi-variate Gaussian, N(0, σ).From left to right, model
(σ): VAE (σ=1), β-VAE (σ=1), m-VAE#1 (σ=3), and m-VAE#2 (σ=3). Higger σ is used for μ-VAE
models since their samples are more spread out.
-2.5 0.0 2.5	-2.5 0.0 ZS -2.5 0.0 2.5	-2.5 0.0 2.5	-2.5 0.0 ZS -2.5 0.0 2.5	-2.5 0.0 2.5	-2.5 0.0 2.5	-2.5 0.0 2.5	-2.5 0.0 2.5
-2.5 0.0 25	-2 5 0.0 ZS -2.5 0.0 ΣΞ -2.5 0.0 ZS -2.5 0.0 25	-2.5 0.0 2.5	-2.5 0.0 25	-2.5 0.0 25	-2.5 0.0 25	-2.5 0.0 ZS
Figure 11: Sample distribution obtained at each dimension of latent variable using test dataset of
MNIST. From top to bottom: VAE, β-VAE, 〃-VAE#1, and 〃-VAE#2.
14