Under review as a conference paper at ICLR 2020
Amortized Nesterov’s Momentum: Robust and
Lightweight Momentum for Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
Stochastic Gradient Descent (SGD) with Nesterov’s momentum is a widely used
optimizer in deep learning, which is observed to have excellent generalization per-
formance. In this work, we propose Amortized Nesterov’s Momentum, which is a
special variant of Nesterov’s momentum. Compared with Nesterov’s momentum,
our new momentum has more robust iterates and higher efficiency. Our empir-
ical results show that it achieves faster early convergence and comparable final
generalization performance with little-to-no tuning. Just like Nesterov’s method,
the new schemes are also proved optimal in general convex setting. Our analysis
sheds light on the understanding of the new variant.
1	Introduction
In recent years, Gradient Descent (GD) (Cauchy, 1847) and its variants have been widely used
to solve large scale machine learning problems. Among them, Stochastic Gradient Descent (SGD)
(Robbins & Monro, 1951), which replaces gradient with an unbiased stochastic gradient estimator, is
a popular choice of optimizer especially for neural network training which requires lower precision.
Sutskever et al. (2013) found that using SGD with Nesterov’s momentum (Nesterov, 1983; 2013b),
which was originally designed to accelerate deterministic convex optimization, achieves substantial
speedups for training neural networks. This finding essentially turns SGD with Nesterov’s momen-
tum into the benchmarking method of neural network design, especially for classification tasks (He
et al., 2016b;a; Zagoruyko & Komodakis, 2016; Huang et al., 2017). It is observed that in these
tasks, the momentum technique plays a key role in achieving good generalization performance.
Adaptive methods (Duchi et al., 2011; Kingma & Ba, 2015; Tieleman & Hinton, 2012; Reddi et al.,
2018), which are also becoming increasingly popular in the deep learning community, diagonally
scale the gradient to speed up training. However, Wilson et al. (2017) show that these methods
always generalize poorly compared with SGD with momentum (both classical momentum (Polyak,
1964) and Nesterov’s momentum).
In this work, we introduce Amortized Nesterov’s Momentum, which is a special variant of Nes-
terov’s momentum. From users’ perspective, the new momentum has only one additional integer
hyper-parameter m to choose, which we call the amortization length. Learning rate and momentum
parameter of this variant are strictly aligned with Nesterov’s momentum and by choosing m = 1,
it recovers Nesterov’s momentum. This paper conducts an extensive study based on both empirical
evaluation and convex analysis to identify the benefits of the new variant (or from users’ angle, to
set m apart from 1). We list the advantages of Amortized Nesterov’s Momentum as follows:
•	Increasing m improves robustness1. This is an interesting property since the new momentum
not only provides acceleration, but also enhances the robustness. We provide an understanding
of this property by analyzing the relation between convergence rate and m in the convex setting.
•	Increasing m reduces (amortized) iteration complexity.
•	A suitably chosen m boosts the convergence rate in the early stage of training and produces
comparable final generalization performance.
1In this work, robustness refers to the probability of an optimizer significantly deviating from its expected
performance, which can be reflected by the deviations of accuracy or loss in the training process over multiple
runs that start with the same initial guess.
1
Under review as a conference paper at ICLR 2020
•	It is easy to tune m. The performances of the methods are stable for a wide range of m and we
prove that the methods converge for any valid choice of m in the convex setting.
•	If m is not too large, the methods obtain the optimal convergence rate in general convex setting,
just like Nesterov’s method.
The new variant does have some minor drawbacks: it requires one more memory buffer, which is
acceptable in most cases, and it shows some undesired behaviors when working with learning rate
schedulers, which can be addressed by a small modification. Considering these pros and cons, we
believe that the proposed variant can benefit many large-scale deep learning tasks.
Our high level idea is simple: the stochastic Nesterov’s momentum can be unreliable since it is pro-
vided only by the previous stochastic iterate. The iterate potentially has large variance, which may
lead to a false momentum that perturbs the training process. We thus propose to use the stochastic
Nesterov’s momentum based on several past iterates, which provides robust acceleration. In other
words, instead of immediately using an iterate to provide momentum, we put the iterate into an
“amortization plan” and use it later.
2	Preliminaries: SGD and Nes terov’ s Momentum
We start with a review of SGD and Nesterov’s momentum. We discuss some subtleties in the imple-
mentation and evaluation, which contributes to the interpretation of our methods.
Notations In this paper, We use X ∈ Rd to denote the vector of model parameters. ∣∣∙k and〈•，•)
denote the standard Euclidean norm and inner product, respectively. Scalar multiplication for v ∈
Rd and β ∈ R is denoted as β ∙ v. f : Rd → R denotes the loss function to be minimized and Vf (x)
represents the gradient of f evaluated at x. We denote the unbiased stochastic gradient estimator
of Vf (x) as Vfi(x) With the random variable i independent of x (e.g., using mini-batch). We use
x0 ∈ Rd to denote the initial guess.
SGD SGD has the folloWing simple iterative scheme, Where γ ∈ R denotes the learning rate:
Xk+1 = Xk 一 Y ∙ Vfik (Xk), for k ≥ 0.
Nesterov’s momentum The original Nesterov’s accelerated gradient (With constant step) (Nes-
terov, 1983; 2013b) has the folloWing scheme2 (y ∈ Rd, η, β ∈ R and y0 = X0):
yk+ι = Xk ― η ∙ Vf(Xk),
χk+ι = yk+ι + β ∙ (yk+ι — yk), for k ≥ 0,
(1)
where we call β ∙ (yk+ι 一 yk) the momentum. By simply replacing Vf (Xk) with Vfik (Xk), we
obtain the SGD With Nesterov’s momentum, Which is Widely used in deep learning. To make this
point clear, recall that the reformulation in Sutskever et al. (2013) (scheme (2), also the Tensorflow
(Abadi et al., 2016) version) and the PyTorch (Paszke et al., 2017) version (scheme (3)) have the
following schemes (v, vpt ∈ Rd and v0 = v0pt = 0): for k ≥ 0,
(2)	fvk+ι = β ∙ vk - η •Vfik (yk + β ∙ vk),	(3) (vp+1 = β ∙ Vpt + Vfik (Xk),
) (yk+ι =期k + vk+ι.	X Vk+i = Xk — η ∙ (β ∙ vp+ι + Vfik(Xk)).
Here the notations are modified based on their equivalence to scheme (1). It can be verified that
schemes (2) and (3) are equivalent to(1) through vk = β-1 ∙ (Xk — yk) and vpt = η-1β-1 ∙ (yk — Xk),
respectively (see Defazio (2018) for other equivalent forms of scheme (1)).
Interestingly, both PyTorch and Tensorflow3 track the values {Xk}, which we refer to as M-SGD.
This choice allows a consistent implementation when wrapped in a generic optimization layer (De-
fazio, 2018). However, the accelerated convergence rate (in the convex case) is built upon {yk}
(Nesterov, 2013b) and {Xk} may not possess such a theoretical improvement. We use OM-SGD to
refer to the Original M-SGD that outputs {yk}.
2We exchange the notations of x and y in Nesterov (2013b).
3Tensorflow tracks the values {yk + β ∙ Vk} = {xk}.
2
Under review as a conference paper at ICLR 2020
Test Accuracy
(a)
Test Accuracy STD
(b)	(c)
Figure 1: ResNet34 on CIFAR-10. Initial learning rate no = 0.1, momentum β = 0.9, run 5 seeds
(same χ0). In (a) (c), We plot mean curves with shaded bands indicating ±1 standard deviation. (b)
shows the standard deviation of test accuracy and its average over 90 epochs. Best viewed in color.
SGD and M-SGD In order to study the features of momentum, in this work, we regard momentum
as an add-on to plain SGD, which corresponds to fixing the learning rates4 Y = n. From the
interpretation in Allen-Zhu & Orecchia (2017), n represents the learning rate for the gradient descent
“inside” Nesterov’s method. To introduce the evaluation metrics of this paper, we report the results
of training ResNet34 (He et al., 2016b) on CIFAR-10 (Krizhevsky et al., 2009) (our basic case study)
using SGD and M-SGD in Figure 1. In this paper, all the multiple runs start with the same initial
guess x0. Figure 1a shows that Nesterov’s momentum hurts the convergence in the first 60 epochs
but accelerates the final convergence, which verifies the importance of momentum for achieving
high accuracy. Figure 1b depicts the robustness of M-SGD and SGD, which suggests that adding
Nesterov’s momentum slightly increases the uncertainty in the training process of SGD.
Train-batch loss vs. Full-batch loss In Figure 1c, train-batch loss stands for the average of batch
losses forwarded in an epoch, which is commonly used to indicate the training process in deep
learning. Full-batch loss is the average loss over the entire training dataset evaluated at the end of
each epoch. In terms of optimizer evaluation, full-batch loss is much more informative than train-
batch loss as it reveals the robustness of an optimizer. However, full-batch loss is too expensive to
evaluate and thus we only measure it on small datasets. On the other hand, test accuracy couples
optimization and generalization, but since it is also evaluated at the end of the epoch, its convergence
is similar to full-batch loss. Considering the basic usage of momentum in deep learning, we mainly
use test accuracy to evaluate optimizers. We provide more discussion on this issue in Appendix C.2.
M-SGD vs. OM-SGD We also include OM-SGD in Figure 1a. In comparison, the final accuracies
of M-SGD and OM-SGD are 94.606% ± 0.152% and 94.728% ± 0.111% with average deviations
at 1.040% and 0.634%, respectively. This difference can be explained following the interpretation
in Hinton (2012) that {xk} are the points after “jump” and {yk} are the points after “correction”.
3 Amortized Nes terov’ s Momentum
In this section, we formally introduce SGD with Amortized Nesterov’s Momentum (AM1-SGD) in
Algorithm 1 with the following remarks:
Options It can be verified that if m = 1, AM1-SGD with Option I degenerates to M-SGD and
Option II corresponds to OM-SGD. Just like the case for M-SGD and OM-SGD, the accelerated
convergence rate is built upon Option II while Option I is easier to be implemented in a generic
optimization layer5. Intuitively, Option I is SGD with amortized momentum and Option II applies
an m-iterations tail averaging on Option I.
4Ma & Yarats (2019) observed that when effective learning rates γ = η(1 - β)-1 are fixed, M-SGD and
SGD have similar performance. We provide a discussion on this observation in Appendix C.1.
5To implement Option II, we can either maintain another identical network for the shifted point X or tem-
porarily change the network parameters in the evaluation phase.
3
Under review as a conference paper at ICLR 2020
(a) Sweeping m in {3, 5, 7, 10, 20, 30}. Run 5 seeds.
(b) Fixing m = 5. Run 20 seeds.
Figure 2: ResNet34 on CIFAR-10. For all methods, no = 0.1, β = 0.9. Labels of AMI-SGD are
'AM1-SGD-{Option}’. Shaded bands (or bars) indicate ±1 standard deviation. Best viewed in color.
Algorithm 1 AMI-SGD
Input: Initial guess xo, learning rate n, momentum β, amortization length m, iteration number K.
Initialize: X J xo, X J xo, x+ J 0 {a running average}.
1:	for k = 0,. ..,K 一 1 do
2:	χ J χ 一 n ∙ Vfik (χ).
3:	X+ J X+ + — ∙ x.
m
4:	if (k + 1) mod m = 0 then
5:	x J x + β ∙ (X+ — X). {adding amortized momentum}
6:	X J X+, X+ J 0.
7:	end if
8:	end for
Output: Option I: X, Option II: xʃ.	*The Symbol'—' denotes assignment.
Efficiency We can improve the efficiency of Algorithm 1 by maintaining a running scaled mo-
mentum v+，m ∙ (X+ — X) instead of the running average X+, by replacing the following steps in
Algorithm 1:
Initialize:	X J xo, X J xo, v+ J-----m ∙ xo, Step 3:	v+ J v+ + x.
Step 5:	x J x + (β∕m) ∙ v+.	Step 6:	X J X+(1∕m)	∙ V+,V+	J--m ∙ X.
Then, in one m-iterations loop, for each of the first m 一 1 iterations, AM1-SGD requires 1 vector
addition and 1 scaled vector addition. At the m-th iteration, it requires 1 vector addition, 1 scalar-
vector multiplication and 3 scaled vector additions. In comparison, M-SGD (standard PyTorch)
requires 1 vector addition, 1 (in-place) scalar-vector multiplication and 2 scaled vector additions per
iteration. Thus, as long as m > 2, AM1-SGD has lower amortized cost than M-SGD. For memory
complexity, AM1-SGD requires one more auxiliary buffer than M-SGD.
Tuning m We did a parameter sweep for m in our basic case study. We plot the final and the
average deviation of test accuracies over 5 runs against m in Figure 2a. Note that m= 1 corresponds
to the results of M-SGD and OM-SGD, which are already given in Figure 1. From this empirical
result, m introduces a trade-off between final accuracy and robustness (the convergence behaviors
can be found in Appendix A.1). Figure 2a suggests that m = 5 is a good choice for this task. For
simplicity, and also as a recommended setting, we fix m=5 for the rest of experiments in this paper.
A momentum that increases robustness To provide a stronger justification, we ran 20 seeds with
m = 5 in Figure 2b and the detailed data are given in Figure 3 & Table 1. The results show that
the amortized momentum significantly increases the robustness. Intuitively, the gap between Option
I and Option II can be understood as the effect of tail averaging. However, the large gap between
Option I and SGD is somewhat mysterious: what Option I does is to inject a very large momentum6
into SGD every m iterations. It turns out that this momentum not only provides acceleration, but
also helps the algorithm become more robust than SGD. This observation basically differentiates
AM1-SGD from a simple interpolation in-between M-SGD and SGD.
6Amortized momentum β∙(x+ — x) is expected to be much large than Nesterov,s momentum β∙(yk+ι — yk).
4
Under review as a conference paper at ICLR 2020
METHOD	FINAL ACCURACY	Avg. STD
SGD 一	93.302% ± 0.199%	0928%
M-SGD 一	94.710% ± 0.169%	0995%
AMI-SGD-f^	94.675% ± 0177%	0.587%
AMI-SGD-Ir	94.619% ± 0.152%	0.314%
Figure 3 & Table 1: Detailed data of the curves in Figure 2b. Best viewed in color.
Learning rate scheduler issue We observed that when we use schedulers with a large decay
factor and the momentum β is too large for the task (e.g., 0.995 for the task of this section), there
would be a performance drop after the learning rate reduction. We believe that it is caused by the
different cardinalities of iterates being averaged in X+, which leads to a false momentum. This issue
is resolved by restarting the algorithm after each learning rate reduction inspired by (O’donoghue &
Candes, 2015). We include more discussion and evidence in Appendix A.4.
3.1	AM2 -S GD: A variant with identical iterations
Algorithm 2 AM2-SGD
Input: Initial guess xo, amortization length m, a point table φ = [φι •一 φm] ∈ Rd×m, learning
rate η, momentum β , iteration number K .
Initialize: φj0 = x0 , ∀j ∈ [m]*. {jk | jk ∈ [m]}kK=-01 is a sequence of uniformly random indexes.
If Option II is used, φ0 = x0. {a running average for the point table φ}
1:	for k = 0, . . . , K - 1 do
2:	φk+1 = Xk - η ∙ Vfifc (Xk) and keep other entries unchanged (i.e., φfj+1 = φj for j = jk).
3:	Xk+ι = φik+1 + β ∙ (Φk+1 - φjk). {adding amortized momentum}
4:	if Option II then φk+1 = φk + A ∙ /1 - φk).
5:	end for
Output: Option I (not recommended): XK, Option II: φκ.	* [m] denotes the set {1,..., m}.
While enjoying an improved efficiency, AM1-SGD does not have identical iterations7, which to
some extent limits its extensibility to other settings (e.g., asynchronous setting). In this section, we
propose a variant of Amortized Nesterov’s Momentum (AM2-SGD, Algorithm 2) to address this
problem. To show the characteristics of AM2-SGD, we make the following remarks:
Trading memory for extensibility In expectation, the point table φ stores the most recent m
iterations and thus the output φκ is an m-iterations tail average, which connects to AMI-SGD. The
relation between AM1-SGD and AM2-SGD resembles that of SVRG (Johnson & Zhang, 2013) and
SAGA (Defazio et al., 2014), the most popular methods in finite-sum convex optimization: to reuse
the information from several past iterates, we can either maintain a “snapshot” that aggregates the
information or keep the iterates in a table. A side-by-side comparison is given in Section 4.
Options and convergence As in the case of AM1-SGD, if m = 1, AM2-SGD with Option I
corresponds to M-SGD and Option II is OM-SGD. In our preliminary experiments, the convergence
of AM2-SGD is similar to AM1-SGD and it also has the learning rate scheduler issue. In our
preliminary experiments (can be found in Appendix A), we observed that Option I is consistently
worse than Option II and it does not seem to benefit from increasing m. Thus, we do not recommend
using Option I. We also set m = 5 for AM2-SGD for its evaluation due to the similarity.
7For AM1-SGD, the workload varies for different iteration k due to the if-clause at Step 4.
5
Under review as a conference paper at ICLR 2020
Additional randomness {jk} In our implementation, at each iteration, we sample an index in [m]
as jk+1 and obtain the stored index jk. We observed that with Option I, AM2-SGD has much larger
deviations than AM1-SGD, which we believe is caused by the additional random indexes {jk}.
4 Convergence Results
The original Nesterov’s accelerated gradient is famous for its optimal convergence rates for solving
convex problems. In this section, we analyze the convergence rates for AM1-SGD and AM2-SGD
in the convex case, which explicitly model the effect of amortization (i.e., m). While these rates do
not hold for deep learning problems in general, they help us understand the observed convergence
behaviors of the proposed methods, especially on how they differ from M-SGD (m = 1). Moreover,
the analysis also provides intuition on tuning m. Since the original Nesterov’s method is determin-
istic (Nesterov, 1983; 2013b), we follow the setting of its stochastic variants (Lan, 2012; Ghadimi
& Lan, 2012), in which Nesterov’s acceleration also achieves the optimal rates.
We consider the following convex composite problem (Beck & Teboulle, 2009; Nesterov, 2013a):
min
x∈X
F(x) , f(x) + h(x) ,
(4)
where X ⊆ Rd is a non-empty closed convex set and h is a proper convex function with its proximal
operator Proxah(∙)8 available. We impose the following assumptions on the regularity of f and the
stochastic oracle Nfi (identical to the ones in Ghadimi & Lan (2012) with μ = 0):
Assumptions. For some L ≥ 0, M ≥ 0, σ ≥ 0,
(a)	0 ≤ f (y) - f(χ) - hNf(χ),y — Xi ≤ L2 ky — χk2 + M ky — Xk,∀χ,y ∈ X.8 9
(b)	Ei [Nfi(x)] = Nf (x), ∀x ∈ X.
(c)	EikNfi(X)-Nf(X)k2 ≤σ2,∀X∈X.
The notation Eik [ ∙ ] is E [ ∙ I (io,.∙∙, ik-ι)] for a random process io, iι,.... These assumptions
cover several important classes of convex problems. For example, (a) covers the cases of f being
L-smooth (M = 0) or L0-Lipschitz continuous (M = 2L0, L = 0) convex functions and if σ = 0
in (c), the assumptions cover several classes of deterministic convex programming problems. We
denote X? ∈ X as a solution to problem (4) and X0 ∈ X as the initial guess.
Unlike its usage in deep learning, the momentum parameter β is always a variable in general convex
analysis. For the simplicity of analysis, we reformulate AM1-SGD (Algorithm 1) and AM2-SGD
(Algorithm 2) into the following schemes10(z ∈ X, α ∈ R):
	AM1-SGD (reformulated, proximal)		AM2-SGD (reformulated, proximal)
Initialize: Xo = z0 = x0, S = K/m.		Initialize: zo = φjo = Xo , ∀j ∈ [m].	
1	: for s = 0, . . . , S - 1 do	1	for k = 0, . . . , K - 1 do
2	: for j = 0, . . . , m - 1 do	2	Sample jk uniformly in [m].
3	:	k = sm +j.	3	Xkk = (I- βk) ∙ zk+βk ∙ φjk.
4 5 6	Xk = (1 - βS) ∙ Zk + Bs ∙ xs. zk+1 =ProXash {zk — αs Nfik(Xk)}. (Xk + 1 = (1 - βS) ∙ zk+1 + Bs ∙ Xsj	4 5	zk + 1 =ProXak h {zk - αk ∙Vfik (Xkk )}. φk+1 = (1 - Bk) ∙ zk + 1 + Bk ∙ φjk .
7	: end for	6	end for
8	1m Xs+1 = m 工 j=1 Xsm+j.	OUtPUt： φK = m Em=I φK.	
9	: end for		
Output: XS.
We show in Appendix B.1 that when h ≡ 0 and β is a constant, the reformulated schemes AM1-
SGD and AM2-SGD are equivalent to Algorithm 1 and Algorithm 2 through αs = η(1 - βs)-1 and
8∀x ∈ Rd, ProXah(X)，argminu∈x {2 l∣u — xk2 + αh(u)}, see Parikh et al. (2014).
9When M > 0, f is not necessarily differentiable and We keep using the notation Vf (x) to denote an
arbitrary subgradient of f at x for consistency.
10For simplicity, we assume K is divisible by m.
6
Under review as a conference paper at ICLR 2020
Figure 4 & Table 2: ResNet18 with pre-activation on CIFAR-10. For all methods, η0 = 0.1, β=0.9,
run 20 seeds. For AM1-SGD, m = 5 and its labels are formatted as ‘AM1-SGD-{Option}’. Shaded
bands indicate ±1 standard deviation. Best viewed in color.
METHOD	Avg. STD
SGD	00329
M-SGD	00339
AMI-SGD-I	0.0142
AMI-SGD-II	0.0056
αk = η(1 - βk)-1. These reformulations are basically how Nesterov’s momentum was migrated
into deep learning (Sutskever et al., 2013). Then we establish the convergence rates for AM1-SGD
and AM2-SGD as follows. All the proofs in this paper are given in Appendix B.2.
Theorem 1.	For the reformulated AM1-SGD, suppose we choose
βs = -s- and ɑs = λ1 with λι = min ʃ 2,-----------L kx0 x --------3 ].	(5)
S + 2 S	L(1- βs)	1	3, 2√m√σ2 + M2(S +1)2 ʃ
Then,
(a)	The output xs satisfies
?、V 3Lm kx0 - x?『工 8 kx0 - x?k √σ2 + M2 , K
E [F(XS)] - F(X) ≤	(K + m)2	+--------√κ+m----------= K0(m).
(b)	Ifthe variance has a “lighttail”, i.e., Ei kXP ∣kVfi(χ)-Vf (x)k2/σ2} ] ≤ exp{1},∀x ∈
X, and X is compact, denoting DX , maxx∈X kx - x? k, for any Λ ≥ 0, we have
?	4λo (3 ∣∣x0 - x?k + √6dx)[
Prob F F(XS) - F(x ) ≤ Ko(m) +---------C »	--------〉
3 K+m
≥ 1 — (exp{-Λ2∕3} +exp{-Λ}).
Remarks: (a) Regarding K0(m), its minimum is obtained at either m= 1 or m= K. Note that for
AM1-SGD, mis strictly constrained in {1, . . ., K}. It can be verified that when m= K, AM1-SGD
becomes the modified mirror descent SA (Lan, 2012), or under the Euclidean setting, the SGD that
outputs the average of the whole history, which is rarely used in practice. In this case, the conver-
gence rate in Theorem 1a becomes the corresponding O(L∕K + (σ + M)/√K) (cf. Theorem 1 in
Lan (2012)). Thus, we can regard AM1-SGD as a smooth transition between AC-SA and the mod-
ified mirror descent SA. (b) The additional compactness and “light tail” assumptions are similarly
required in Nemirovski et al. (2009); Lan (2012); Ghadimi & Lan (2012). Recently, Juditsky et al.
(2019) established similar bounds under weaker assumptions by truncating the gradient. However,
as indicated by the authors, their technique cannot be used for accelerated algorithms due to the
accumulation of bias.
Understandings: Theorem 1a gives the expected performance in terms of full-batch loss F(X)-
F(x?), from which the trade-off of mis clear: Increasing m improves the dependence on variance
σbut deteriorates the O(L/K2 ) term (i.e., the acceleration). Based on this trade-off, we can under-
stand the empirical results in Figure 2b: the faster convergence in the early stage could be the result
of a better control on σand the slightly lowered final accuracy is possibly caused by the reduced
acceleration effect. Theorem 1b provides the probability of the full-batch loss deviating from its
expected performance (i.e., K0(m)). It is clear that increasing m leads to smaller deviations with
the same probability, which sheds light on the understanding of the increased robustness observed
in Figure 2. Since the theorem is built on the full-batch loss, we did an experiments based on this
7
Under review as a conference paper at ICLR 2020
metric in Figure 4 & Table 2. Here we choose training a smaller ResNet18 with pre-activation (He
et al., 2016a) on CIFAR-10 as the case study (the test accuracy is reported in Appendix A.5).
For AM2-SGD, we only give the expected convergence results as follows.
Theorem 2.	For the reformulated AM2-SGD, if we choose
βk
k/m
k/m+2 and ak
λ
L(I - Bk)
with λ2 = min
L ∣∣χo 一 x?k
-,	3
√2m(σ+M)( ^m-ɪ+2) 2
the output φκ satisfies
E [F(ΦK)] 一 F(x?) ≤
4(m2-m)(F(XO)-F(x?)) +3Lm ∣∣xo一x?∣∣2	4√2 ∣∣xo-x?| (σ + M)
(K + 2m - 1)2	√K + 2m - 1
Remark: In comparison with Theorem 1a, Theorem 2 has an additional term F(x0) 一 F(x?) in the
upper bound, which is inevitable. This difference comes from different restrictions on the choice of
m. For AM2-SGD, m ≥ 1 is the only requirement. Since it is impossible to let m K to obtain an
improved rate, this additional term is inevitable. As a sanity check, we can let m → ∞ to obtain a
point table with almost all x0, and then the upper bound becomes exactly F(x0 ) 一 F(x?). In some
cases, there exists an optimal choice ofm > 1 in Theorem 2. However, the optimal choice could be
messy and thus we omit the discussion here.
Understanding: Comparing the rates, we see that when using the same m, AM2-SGD has slightly
better dependence on σ, which is related to the observation in Figure 5 that AM2-SGD is always
slightly faster than AM1-SGD. This difference is suggesting that randomly incorporating past iter-
ates beyond m iterations helps.
If m = O(1), Theorems 1 and 2 establish the optimal O(L∕K2 + (σ + M)∕√K) rate in the
convex setting (see Lan (2012) for optimality), which verifies AM1-SGD and AM2-SGD as variants
of the Nesterov’s method (Nesterov, 1983; 2013b). From the above analysis, the effect of m can
be understood as trading acceleration for variance control. However, since both acceleration and
variance control boost the convergence speed, the reduced final performance observed in the CIFAR
experiments may not always be the case as will be shown in Figure 5 and Table 3.
Connections with Katyusha Our original inspiration of AM1-SGD comes from the construction
of Katyusha (Allen-Zhu, 2018), the recent breakthrough in finite-sum convex optimization, which
uses a previously calculated “snapshot” point to provide momentum, i.e., Katyusha momentum.
AM1-SGD also uses an aggregated point to provide momentum and it shares many structural simi-
larities with Katyusha. We refer the interested readers to Appendix B.3.
5 Performance Evaluation
In this section, we evaluate AM1-SGD and AM2-SGD on more deep learning tasks. Our goal is to
show their potentials of serving as alternatives for M-SGD. Regarding the options: for AM1-SGD,
Option I is a nice choice, which has slightly better final performance as shown in Table 1; for AM2-
SGD, Option I is not recommended as mentioned before. Here we choose to evaluate Option II
for both methods for consistency, which also corresponds to the analysis in Section 4. AM1-SGD
and AM2-SGD use exactly the same values for (η, β) as M-SGD, which was tuned to optimize the
performance of M-SGD. Wesetm = 5 for AM1-SGD and AM2-SGD.
We trained ResNet50 and ResNet152 (He et al., 2016b) on the ILSVRC2012 dataset (“ImageNet”)
(Russakovsky et al., 2015) shown in Figure 5b. For this task, we used 0.1 initial learning rate and
0.9 momentum for all methods, which is a typical choice. We performed a restart after each learning
rate reduction as discussed in Appendix A.4. We believe that this helps the training process and also
does not incur any additional overhead. We report the final accuracy in Table 3.
We also did a language model experiment on Penn Treebank dataset (Marcus et al., 1993). We used
the LSTM (Hochreiter & Schmidhuber, 1997) model defined in Merity et al. (2017) and followed
the experimental setup in its released code. We only changed the learning rate and momentum in
8
Under review as a conference paper at ICLR 2020
(a) LSTM on Penn Treebank.
ResNetSO on ImageNet
ResNetl52 on ImageNet
(b) ResNet on ImageNet. Run 3 seeds.
Figure 5: Convergence of LSTM and ResNet. We plot the curve of validation perplexity and test
accuracy, respectively. Shaded bands indicate ±1 standard deviation. Best viewed in color.
Table 3: Detailed perplexity and accuracy results for Figure 5.
METHOD	Penn Treebank (Perplexity)		ImageNet (Final Accuracy)	
	Validation	Test	ReSNet50	ReSNet152
SGD (+ASGD-	61.283	59.068 —	72.783% ± 0.081%	74.361% ± 0.293%
M-SGD	60.747	58.355 —	75.711% ± 0062%	78.065% ± 0.103%
AMI-SGD~~	60.734	57.977 —	75.779% ± 0.105%	77.816% ± 0.287%
AM2-SGD~~	60.434	58.233 —	75.845% ± 0.073%	78.194% ± 0.147%
the setup. The baseline is SGD+ASGD11 (Polyak & Juditsky, 1992) with constant learning rate 30
as used in Merity et al. (2017). For the choice of (η, β), following Lucas et al. (2019), we chose
β = 0.99 and used the scheduler that reduces the learning rate by half when the validation loss has
not decreased for 15 epochs. We swept η from {5, 2.5, 1, 0.1, 0.01} and found that η = 2.5 resulted
in the lowest validation perplexity for M-SGD. We thus ran AM1-SGD and AM2-SGD with this
(η, β) and m = 5. Due to the small decay factor, we did not restart AM1-SGD and AM2-SGD
after learning rate reductions. The validation perplexity curve is plotted in Figure 5a. We report
validation perplexity and test perplexity in Table 3. This experiment is directly comparable with the
one in Lucas et al. (2019).
Extra results are provided in the appendices for interested readers: the robustness when using large β
(Appendix A.2), a CIFAR-100 experiment (Appendix A.6) and comparison with classical momen-
tum (Polyak, 1964), AggMo (Lucas et al., 2019) and QHM (Ma & Yarats, 2019) (Appendix A.3).
6	Conclusions
We presented Amortized Nesterov’s Momentum, which is a special variant of Nesterov’s momentum
that utilizes several past iterates to provide the momentum. Based on this idea, we designed two
different realizations, namely, AM1-SGD and AM2-SGD. Both of them are simple to implement
with little-to-no additional tuning overhead over M-SGD. Our empirical results demonstrate that
switching to AM1-SGD and AM2-SGD produces faster early convergence and comparable final
generalization performance. AM1-SGD is lightweight and has more robust iterates than M-SGD,
and thus can serve as a favorable alternative to M-SGD in large-scale deep learning tasks. AM2-SGD
could be favorable for more restrictive tasks (e.g., asynchronous training) due to its extensibility and
good performance. Both the methods are proved optimal in the convex case, just like M-SGD. Based
on the intuition from convex analysis, the proposed methods are trading acceleration for variance
control, which provides hints for the hyper-parameter tuning. 11
11SGD+ASGD is to run SGD and switch to averaged SGD (ASGD) when a threshold is met.
9
Under review as a conference paper at ICLR 2020
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-
scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Imple-
mentation ({OSDI} 16),pp. 265-283, 2016.
Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. Journal
of Machine Learning Research, 18(221):1-51, 2018. URL http://jmlr.org/papers/
v18/16-410.html.
Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate unification of gradient and
mirror descent. In ITCS, 2017.
Alfred Auslender and Marc Teboulle. Interior gradient and proximal methods for convex and conic
optimization. SIAM Journal on Optimization, 16(3):697-725, 2006.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM journal on imaging sciences, 2(1):183-202, 2009.
Augustin Cauchy. Methode generale pour la resolution des systemes d'equations simultanees.
Comp. Rend. Sci. Paris, 25(1847):536-538, 1847.
Aaron Defazio. On the curved geometry of accelerated optimization. arXiv preprint
arXiv:1812.04634, 2018.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in neural information
processing systems, pp. 1646-1654, 2014.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly con-
vex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on
Optimization, 22(4):1469-1492, 2012.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and
stochastic programming. Mathematical Programming, 156(1-2):59-99, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016b.
Geoffrey Hinton. Lecture 6C : The momentum method, 2012. URL http://www.cs.
toronto.edu/~hinton/Coursera_lectures.html.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Chonghai Hu, Weike Pan, and James T Kwok. Accelerated gradient methods for stochastic optimiza-
tion and online learning. In Advances in Neural Information Processing Systems, pp. 781-789,
2009.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
10
Under review as a conference paper at ICLR 2020
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in neural information processing Systems, pp. 315-323, 2013.
Anatoli Juditsky, Alexander Nazin, Arkadi Nemirovsky, and Alexandre Tsybakov. Algorithms of
robust stochastic optimization based on mirror descent method. arXiv preprint arXiv:1907.02707,
2019.
Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical Program-
ming, 133(1-2):365-397, 2012.
Guanghui Lan, Arkadi Nemirovski, and Alexander Shapiro. Validation analysis of mirror descent
stochastic approximation method. Mathematical programming, 134(2):425-458, 2012.
Guanghui Lan, Zhize Li, and Yi Zhou. A unified variance-reduced accelerated gradient method for
convex optimization. arXiv preprint arXiv:1905.12412, 2019.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016.
James Lucas, Shengyang Sun, Richard Zemel, and Roger Grosse. Aggregated momentum: Stability
through passive damping. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=Syxt5oC5YQ.
Jerry Ma and Denis Yarats. Quasi-hyperbolic momentum and adam for deep learning. In Interna-
tional Conference on Learning Representations, 2019. URL https://openreview.net/
forum?id=S1fUpoR5FQ.
Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of english: The penn treebank. 1993.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm lan-
guage models. arXiv preprint arXiv:1708.02182, 2017.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574-
1609, 2009.
Yu Nesterov. Gradient methods for minimizing composite functions. Mathematical Programming,
140(1):125-161, 2013a.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013b.
Yurii E Nesterov. A method for solving the convex programming problem with convergence rate
o(1/k2). In Dokl. Akad. Nauk SSSR, volume 269, pp. 543-547, 1983.
Brendan O’donoghue and Emmanuel Candes. Adaptive restart for accelerated gradient schemes.
Foundations of computational mathematics, 15(3):715-732, 2015.
Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and TrendsR in Optimization,
1(3):127-239, 2014.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-
putational Mathematics and Mathematical Physics, 4(5):1-17, 1964.
11
Under review as a conference paper at ICLR 2020
Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging.
SIAM Journal on Control and OPtimization, 30(4):838-855,1992.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning RePresentations, 2018. URL https://openreview.
net/forum?id=ryQu7f-RZ.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400-407, 1951.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of comPuter vision, 115(3):211-252, 2015.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In International conference on machine learning, pp.
1139-1147, 2013.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-
31, 2012.
Paul Tseng. On accelerated proximal gradient methods for convex-concave optimization. 2008.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4148-4158, 2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv PrePrint
arXiv:1605.07146, 2016.
Kaiwen Zhou, Fanhua Shang, and James Cheng. A simple stochastic variance reduced algorithm
with fast convergence rates. In International conference on machine learning, pp. 5980-5989,
2018.
Kaiwen Zhou, Qinghua Ding, Fanhua Shang, James Cheng, Danli Li, and Zhi-Quan Luo. Direct
Acceleration of SAGA using Sampled Negative Momentum. In AISTATS, pp. 1602-1610, 2019.
12
Under review as a conference paper at ICLR 2020
Appendices
A Extra Experimental Results	14
A.1	The effect of m on convergence ....................................... 14
A.2	Robustness on large momentum parameters .............................. 15
A.3	Comparison with other momentum ....................................... 15
A.4	Issues with learning rate schedulers ................................. 17
A.5	Test accuracy results of Figure 4 & Table 2 .......................... 17
A.6	CIFAR-100 experiment ................................................. 17
A.7	A sanity check ....................................................... 18
B Missing parts in Section 4	19
B.1	The reformulations ................................................... 19
B.2	Proofs of Theorem 1 and Theorem 2 .................................... 20
B.2.1	Proof of Lemma 1 ............................................... 20
B.2.2	Proof of Theorem 1a ............................................ 22
B.2.3	Proof of Theorem 1b ............................................ 23
B.2.4	Proof of Theorem 2 ............................................. 25
B.3	Connections between AM1-SGD and Katyusha ............................. 27
C	Miscellanies	28
C.1 Comparison of SGD and M-SGD ............................................ 28
C.2 Training evaluation .................................................... 29
D	Experimental Setup	29
D.1 Classification Setup ................................................... 29
D.2 Language Model Setup ................................................... 30
13
Under review as a conference paper at ICLR 2020
A Extra Experimental Results
In this appendix, we provide more experimental results to further evaluate the Amortized Nesterov’s
Momentum. Table 4 shows the detailed data of the parameter sweep experiments, where the con-
vergence curves of these results are given in Appendix A.1. In Appendix A.2, we compare the
robustness of AM1-SGD and M-SGD on large momentum parameters. In Appendix A.3, we em-
pirically compare the Amortized Nesterov’s Momentum with classical momentum (Polyak, 1964),
aggregated momentum (Lucas et al., 2019) and quasi-hyperbolic momentum (Ma & Yarats, 2019).
We discuss the issues with learning rate schedulers in Appendix A.4. We report the test accuracy
results of the ResNet18 experiment (in Section 4) in Appendix A.5. A CIFAR-100 experiment is
provided in Appendix A.6. We also provide a sanity check for our implementation in Appendix A.7.
Table 4: Final test accuracy and average accuracy STD of training ResNet34 on CIFAR-10 over
5 runs (including the detailed data of the curves in Figure 1 and Figure 2a). For all the methods,
η0 = 0.1, β = 0.9. Multiple runs start with the same x0.
METHOD	DESCRIPTION	FINAL ACCURACY	Avg. STD
SGD	Standard Pytorch	93.406% ± 0.149%	0.991%
M-SGD -	Standard Pytorch	94.606% ± 0.152%	1.040%
AMI-SGD '	Option I, m = 1, sanity check	94.672% ± 0.143%	0.912%
AMI-SGD -	Option I, m = 3	94.630% ± 0.025%	0.643%
AMI-SGD	Option I, m = 5	94.604% ± 0.104%	0.496%
AMI-SGD	Option I, m = 7	94.640% ± 0.127%	0.439%
AMI-SGD -	Option I, m = 10	94.542% ± 0.133%	0.441%
AMI-SGD -	Option I, m = 20	94.378% ± 0.217%	0.399%
AMI-SGD -	Option I, m = 30	94.300% ± 0.145%	0.429%
OM-SGD	AMI-SGD (Opt.II, m = "	94.728% ± 0.111%	0.634%
AMI-SGD	OPtion II, m = 3		94.662% ± 0.139%	0.411%
AMI-SGD -	Option II, m = 5	94.602% ± 0.083%	0.265%
AMI-SGD -	Option II, m = 7	94.514% ± 0.103%	0.282%
AMI-SGD -	Option II, m = 10	94.424% ± 0.117%	0.293%
AMI-SGD -	OPtion II, m = 20	94.358% ± 0.181%	0.306%
AMI-SGD -	OPtion II, m = 30	94.270% ± 0.134%	0.344%
AM2-SGD '	Option I, m = 1, sanity check	94.682% ± 0.212%	0.822%
AM2-SGD -	Option I, m = 5	94.572% ± 0.188%	0.591%
AM2-SGD -	Option I, m = 10	94.440% ± 0.138%	0.737%
AM2-SGD -	Option I, m = 20	94.312% ± 0.149%	0.741%
AM2-SGD ・	Option II, m = 5	94.664% ± 0.107%	0.263%
AM2-SGD -	Option II, m = 10	94.496% ± 0.211%	0.280%
AM2-SGD -	Option II, m = 20	94.412% ± 0.140%	0.246%
A.1 THE EFFECT OF m ON CONVERGENCE
We show in Figure 6 how m affects the convergence of test accuracy. The results show that in-
creasing m speeds up the convergence in the early stage. While for AM1-SGD the convergences of
Option I and Option II are similar, AM2-SGD with Option II is consistently better than with Option
I in this experiment. It seems that AM2-SGD with Option I does not benefit from increasing m and
the algorithm is not robust. Thus, we do not recommend using Option I for AM2-SGD.
14
Under review as a conference paper at ICLR 2020
Figure 6: Convergence of test accuracy from the parameter sweep experiments in Table 4. Labels
are formatted as Am1∕2-SGD-{Option}-{m}' . Best viewed in color.
A.2 ROBUSTNESS ON LARGE MOMENTUM PARAMETERS
We compare the robustness of M-SGD and AM1-SGD when β is large in Figure 7 & Table 5. For
fair comparison, AM1-SGD uses Option I. As We can see, the STD error of M-SGD scales UP
significantly when β is larger and the performance is more affected by a large β compared with
AM1-SGD.
50」
0
Test Accuracy
90
求
& 80
fl5
< 70
ω
g
60
20	40	60	80
Epoch
M-SGD-0.9
Figure 7 & Table 5: ResNet34 on CIFAR-10. η0 = 0.1,β ∈ {0.9,0.95,0.99}, run 5 seeds (the
β = 0.9 results are copied from Table 4). Labels are formatted as “{Algorithm}-{β}''. Best viewed
in color.
——M-SGD-0.95
——M-SGD-0.99
AM I-SG D-0.9
AMl-SG D-0.95
—— AM1-SGD-O.99
METHOD	FINAL ACCURACY	Avg. STD
M-SGD-0.9	94.606% ± 0.152%	1.040%
M-SGD-0.95	94.198% ± 0120%	1.200%
M-SGD-0.99	88.366% ± 0357%	2.561%
AMI-SGD-0.9	94.604% ± 0.104%	0496%
AMI-SGD-0.95	93.942% ± 0.074%	0.581%
AMI-SGD-0.99	90.640% ± 0.375%	0900%
A.3 Comparison with other momentum
Figure 8 & Table 6: ResNet34 on CIFAR-10. Run 5 seeds. The results of AM1-SGD and M-SGD
are copied from Table 4. Best viewed in color.
METHOD	FINAL ACCURACY	Avg. STD
CM-SGD	94.690% ± 0.212%	1.107%
AMI-SGD	94.604% ± 0.104%	0496%
AggMo	94.556% ± 0.142%	0777%
QHM	94.426% ± 0.226%	1.073%
M-SGD	94.606% ± 0.152%	1.040%
In this section, we compare AM1-SGD (Option I) with classical momentum (Polyak, 1964), AggMo
(Lucas et al., 2019) and QHM (Ma & Yarats, 2019) in our basic case study (training ResNet34 on
15
Under review as a conference paper at ICLR 2020
CIFAR-10). Since we are not aware of What makes a fair comparison with these methods (e.g., it is
not clear What is the effective learning rate for AMI-SGD), We compare them based on the default
hyper-parameter settings suggested by their papers.
Classical Momentum The SGD with classical momentum (CM-SGD) that is widely used in deep
learning has the following scheme (standard PyTorch) (Vcm ∈ Rd, Vcm = 0):
VcmI= β ∙ Vcm + Vfik (Xk),
Xk+1 = Xk — η ∙ Vcm1, for k ≥ 0.
CM-SGD with its typical hyper-parameter settings (η0 = 0.1, β = 0.9) is observed to achieve
similar generalization performance as M-SGD. However, CM-SGD is more unstable and prone to
oscillations (Lucas et al., 2019), which makes it less robust than M-SGD as shown in Table 6.
Aggregated Momentum (AggMo) AggMo combines multiple momentum buffers, which is in-
spiredby the passive damping from physics literature (Lucas et 12019). AggMo uses the following
update rules (for t = 1,...,T, V ⑴ ∈ Rd, V0t) = 0):
Vkt++ι = β(t) ∙ Vkt)- Vfik (xk), for t =1,...,T,
T
Xk+1 = Xk + T ∙ EVkt+1, for k ≥ 0.
t=1
We used the exponential hyper-parameter setting recommended in the original work with the scale-
factor a = 0.1 fixed, β(t) = 1 — at-1, for t = 1,...,T and choosing T in {2, 3,4}. We found that
T = 2 gave the best performance in this experiment. As shown in Figure 8 & Table 6, with the help
of passive damping, AggMo is more stable and robust compared with CM-SGD.
Quasi-hyperbolic Momentum (QHM) Ma & Yarats (2019) introduce the immediate discount
factor V ∈ R for the momentum scheme, which results in the QHM update rules (α ∈ R, Vqh ∈
Rd, V0h = 0):
Vq+i = β ∙ Vqh + (1 - β) ∙ Vfik (xk),
Xk+1 = Xk — α ∙ (ν ∙ Vq++1 + (1 — ν) ∙ Vfik (xk)), for k ≥ 0.
Here we used the recommended hyper-parameter setting for QHM (α0 = 1.0, β = 0.999, V = 0.7).
Figure 8 shows that AM1-SGD, AggMo and QHM achieve faster convergence in the early stage
while CM-SGD has the highest final accuracy. In terms of robustness, huge gaps are observed when
comparing AM1-SGD with the remaining methods in Table 6. Note that AM1-SGD is more efficient
than both QHM and AggMo, and is as efficient as CM-SGD.
We also plot the convergence of train-batch loss for all the methods in Figure 9. Despite of showing
worse generalization performance, both QHM and AggMo perform better on reducing the train-
batch loss in this experiment, which is consistent with the results reported in Ma & Yarats (2019);
Lucas etal. (2019).
Figure 9: Train-batch loss results. Best viewed in color.
16
Under review as a conference paper at ICLR 2020
A.4 ISSUES WITH LEARNING RATE SCHEDULERS
Train-Batch Loss	Train-Batch Loss	Train-Batch Loss
(a) β = 0.95
(b) β = 0.995
(c) β = 0.995
Figure 10: ResNet18 on CIFAR-10. η0 = 0.1, β ∈ {0.95, 0.995}. ‘+’ represents performing a
restart after each learning rate reduction.
We show in Figure 10 that when β is large for the task,
using step learning rate scheduler with decay factor 10, a
performance drop is observed after each reduction. Both
Option I and Option II have this issue and the curves are
basically identical. Here we only use Option II. We fix
this issue by performing a restart after each learning rate
reduction (labeled with ‘+’). We plot the train-batch loss
here because we find the phenomenon is clearer in this
way. If β = 0.9, there is no observable performance drop
in this experiment.
Train-Batch Loss
Epoch
Figure 11: ResNet18 on CIFAR-10.
Cosine annealing scheduler (without
restarts), η0 = 0.1, β = 0.995.
For smooth-changing schedulers such as the cosine an-
nealing scheduler (Loshchilov & Hutter, 2016), the amor-
tized momentum works well as shown in Figure 11.
A.5 Test accuracy res ults of Figure 4 & Table 2
We report the test accuracy results of the experiments in Section 4 in Figure 12 & Table 7. These
results are reminiscent of the ResNet34 experiments (Figure 3 & Table 1).
METHOD	FINAL ACCURACY	Avg. STD
SGD 一	92.810% ± 0.147%	1.005%
M-SGD 一	94.057% ± 0170%	1.102%
AMI-SGD-f^	93.968% ± 0.154%	0.543%
AMI-SGD-Ir	93.953% ± 0192%	0.315%
Figure 12&Table7: ResNet18WithPre-activationonCIFAR-10. For all methods, no = 0.1,β = 0.9,
run 20 seeds. For AM1-SGD, m = 5 and its labels are formatted as 'AM1-SGD-{Option}’. Shaded
bands indicate ±1 standard deviation. Best viewed in color.
A.6 CIFAR-100 EXPERIMENT
We rePort the results of training DenseNet121 (Huang et al., 2017) on CIFAR-100 in Figure 13,
which shows that both AM1-SGD and AM2-SGD Perform well before the final learning rate re-
duction. However, the final accuracies are lowered around 0.6% comPared with M-SGD. We also
notice that SGD reduces the train-batch loss at an incredibly fast rate and the losses it reaches are
consistently lower than other methods in the entire 300 ePochs. However, this Performance is not
17
Under review as a conference paper at ICLR 2020
reflected in the convergence of test accuracy. We believe that this phenomenon suggests that the
DenseNet model is actually “overfitting" M-SGD (since in the ResNet experiments, M-SGD always
achieves a lower train loss than SGD after the final learning rate reduction).
Test Accuracy
Epoch
Figure 13: DenseNet121 on CIFAR-100. For all methods, no = 0.1, β = 0.9, run 3 seeds. AM1-
SGD and AM2-SGD use Option II and m = 5. Shaded bands indicating ±1 standard deviation.
Best viewed in color.
Train-Batch Loss
Epoch
A.7 A SANITY CHECK
When m = 1, both AMI-SGD and AM2-SGD are equivalent to M-SGD, We plot their convergence
in Figure 14 as a sanity check (the detailed data is given in Table 4).
Figure 14: A sanity check. Labels are formatted as 'AM{1∕2}-SGD-{Option}-{m}'.
We observed that when m = 1, both AM1-SGD and AM2-SGD have a lower STD error than
M-SGD. We believe that it is because they both maintain the iterates without scaling, which is
numerically more stable than M-SGD (M-SGD in standard PyTorch maintains a scaled buffer, i.e.,
Vpt = n-1β-1 ∙ (yk - χk)).
18
Under review as a conference paper at ICLR 2020
B Missing parts in Section 4
B.1 The reformulations
When h ≡ 0 and β is a constant, we do the reformulations by eliminating the sequence {zk }.
For the reformulated AM2-SGD,
Xkk = (I- β) ∙ Zk + β ∙ φjk,
Zk+1 = Zk - α ∙ Vfik (Xkk),
Φk+1 = (1- β) ∙ Zk+1 + β ∙ φjk,
(xkk+ι1 =(1-β) ∙ Zk+1 + β ∙ Φk+11).
The reformulated AM2-SGD
α(1 - β) = η
Eliminate {zk}
φk+1 = Xkk- η ∙ vfik (Xkk),
χkk+11=Φk+1+β ∙ (ok+：- ΦkJ.
Algorithm 2
For the reformulated AM1-SGD, when h ≡ 0, the inner loops are basically SGD,
Xk = (1 - β) ∙ Zk + β ∙ Xs,
Zk+1 = Zk - α ∙ Vfik (Xk),
(Xk+1 = (1 - β) ∙ Zk + 1 + 8 ∙ Xs .)
α(1 - β) = η
Eliminate {zk}
Xk+1 = Xk - η ∙ Vfik (Xk).
At the end of each inner loop (i.e., when (k + 1) mod m = 0), we have
X(s + 1)m = (I - β) ∙ Z(s + 1)m + β ∙ Xs ,
while at the beginning of the next inner loop,
X(s + 1)m = (1 - β) ∙ Z(s + 1)m + β ∙ xs+1,
which means that We need to set Xk+1 J Xk+1 + β ∙ (Xs+1 一 Xs) (reassign the value of Xk+1).
We also give the reformulation of M-SGD (scheme (1)) to the Auslender & Teboulle (2006) scheme
for reference:
Xk = (1 - β) ∙ Zk + β ∙ yk,
Zk+1 = Zk - α ∙ Vfik (Xk),
yk+1 = (1 - β) ∙ Zk+1 + β ∙ yk,
(Xk+1 = (1 - β) ∙ Zk+1 + β ∙ yk+1).
Auslender & Teboulle (2006)
(AC-SA (Lan, 2012))
α(1 - β) = η
Eliminate {zk}
yk+1 = Xk - η ∙ Vfik (Xk),
Xk+1 = yk+1 + β ∙ (yk+1 - yk).
Nesterov (1983; 2013b)
AC-SA (in the Euclidean case) maps to the Auslender & Teboulle (2006) scheme through (in the
original notations)	X = Xmd Z = X
<	y = Xag	. 1 - β = β-1 尸=Yt
Intuition for the Auslender & Teboulle (2006) scheme can be found in Remark 2 in Lan (2012).
19
Under review as a conference paper at ICLR 2020
B.2 Proofs of Theorem 1 and Theorem 2
The reformulated schemes are copied here for reference:
AM1-SGD (reformulated, proximal)			AM2-SGD (reformulated, proximal)		
Initialize: Xo =		Zo = Xo, S = K/m.	Initialize: Z		o = φjo = Xo , ∀j ∈ [m].
1	: for s = 0, . .	.,S- 1do	1	for k =	0, . . . , K - 1 do
2	: for j = 0,	. . . , m - 1 do	2	Sample jk uniformly in [m].	
3	:	k = sm	+ j.	3	jk Xk =	(1 - βk ) ∙ Zk + βk ∙ φjk .
4 5 6	:	Xk = (1 :	Zk+1 = :	(Xk+1 =	-βs) ∙ Zk + Bs ∙ Xs . ProXash {zk - αs ∙vfik (Xk)}. (1 - βs) ∙ zk+1 + Bs ∙ XsJ	4 5	Zk+1 k+1 φjkk+1	k = proxαkh{Zk - αk∙Vfik(Xjkk)}. = (1 - βk) ∙ Zk+1 + βk ∙ φjkk .
7	: end for		6	end for	
8	1 Xs+1 = m	m j=1 Xsm+j .	Output: φ,		' =mi pm=ι φk .
9	: end for				
Output: XS.					
Comparing the reformulated schemes, we see that their iterations can be generalized as follows:
X = (1 - β) ∙ Z + β ∙ y,
Z+ = proxɑh{z - α ∙ Vfi(x)},
y+ = (1 - β) ∙ z+ + β ∙ y.
(6)
This type of scheme is first proposed in Auslender & Teboulle (2006), which represents one of the
simplest variants of the Nesterov’s methods (see Tseng (2008) for other variants). The scheme is
then modified into various settings (Hu et al., 2009; Lan, 2012; Ghadimi & Lan, 2012; 2016; Zhou
et al., 2019; Lan et al., 2019) to achieve acceleration. The following lemma serves as a cornerstone
for the convergence proofs of AM1-SGD and AM2-SGD.
Lemma 1. If α(1 - β) < 1/L, the update scheme (6) satisfies the following recursion:
ɪ (F(y+) - F(χ*)) ≤
1-β
ɪ(F(y) - F(x?)) + ɪ (kz - x?k2 - ∣∣z+ - x?『)
1 - β	2α
+
(kVf(x)-Vfi(x)k+ M)2
2(α-1 - L(1 - β))
+ hVf (x) - Vfi (x), Z - x?i .
B.2.1 Proof of Lemma 1
This Lemma is similarly provided in Lan (2012); Ghadimi & Lan (2012) under a more general
setting that allows non-Euclidean norms in the assumptions, we give a proof here for completeness.
Based on the convexity (Assumption (a)), we have
f(x) - f(x?) ≤ hVf (x), x - Zi + hVf(x) - Vfi(x), Z - x?i + Vfi(x),Z - Z+
'------{------} '------------{------------} `--------{-------}
R0	R1	R2	(7)
+ Vfi(x), Z+ - x? .
'-------{---------}
R3
We upper bound the terms on the right side one-by-one.
For R0 ,
Ro =占 hVf(χ),y -Xi ≤ 占(f(y)- f(χ)),	(8)
where (?) uses the relation between X and z, i.e., (1 - β) ∙ (x - Z)= β ∙ (y - x).
For R2 , based on Assumption (a), we have
f(y+) - f(x) + (Vf (x),x - y+) ≤ L ∣∣X - y+∣∣2 + MIIX - y+∣∣ .
20
Under review as a conference paper at ICLR 2020
Then, noting that x — y+ = (1 - β) ∙ (z - z+), we can arrange the above inequality as
R ≤ L(I;e) ∣∣z - z+∣∣2 + 占 f(χ)- f (y+)) +〈Vf(X)- Vfi(X), z+ -Z
+ M∖∖z - z+∣∣
≤ l⅛0 ∣∣z-z+∣∣2 + ɪ(f(χ) -f(y+)) + (kVf(χ)-Vfi(X)Il + m) ∣∣z-z+∣∣.
1	1 — P
Using Young,s inequality with Z > 0, we obtain
R2 ≤	卜一z+∣∣2 + ɪ(f(X) - f(y+)) + (W(X)-Vfi(X)k + M)2. (9)
1	1 - P	2ζ
For R3, based on the optimality condition of PrOXah{z - a ∙ Vfi(x)} and denoting ∂h(z+) as a
subgradient of h at z+, we have for any U ∈ X,
〈a ∙ ∂h(z+) + z+ — Z + α ∙ Vfi(X), U — z+)≥ 0,
CVfi(X), z+ — u) ≤ (∂h(z+), u — z+)+ —〈z+ — z,u — z+)
≤ h(u) — h(z+) + —〈z+ — z, u — z+).
Choosing U = x?,
R3 ≤ h(X?) — h(z+) + —〈z+ — z, X? — z+)
a '
=h(X?) - h(z+) + 2α (kz - X*k2 - ∣∣z+ - X*∣∣2 - ∣∣z+ - z∣∣2) ,	(10)
where (?) follows from ∣∣α + b∣∣2 = ∣∣α∣2 + ∣∣b∣2 + 2〈a, b).
Finally, by upper bounding (7) using (8), (9), (10), we conclude that
f (X) - f (X?) ≤ R1 + 占(f (y) - f (x)) + L(I-P) + Z - α-1 ∣∣z - z+∣∣2
+ 占(f (x) - f (y+)) + h(x?) - h(z+) + (IVf(X)-Vfi(X)k + M)2
+ 21a (∣z-x*k2-∣∣z+-x*∣∣2),
After simplification,
占(f (y+) - f (x?)) ≤ 占(f (y) - f (x?)) + L(I-P) + Z-α-1 ∣∣z - z+∣∣2
+ h(x?) - h(z+) + (kVf(X)-Vfi(X)k + M)2 + R1	(11)
2C
+ 2a (∣z -x*∣∣2-∣∣z+-x*∣∣2).
Note that with the convexity of h and y+ = (1 - P) ∙ z+ + P ∙ y, we have
h(y+) ≤ (1 - P)h(z+)+ Ph(y),
1P
h(z+) ≥ 1-ph(y+) - 1-ph(y)∙
Using the above inequality and choosing Z = α-1 - L(1 - P) > 0 ⇒ a(1 - P) < 1 /L, we can
arrange (11) as
ɪ(F(y+) - F(x?)) ≤ ɪ(F(y) - F(x?)) + ɪ (∣z - x*∣∣2 - ∣∣z+ - x*∣∣2)
1 - P	1 - P	2a
+ (IIVf(X)-Vfi(X)II+ M)2 + R
+	2(a-1 - L(1 - P))	+ R1.
21
Under review as a conference paper at ICLR 2020
B.2.2 Proof OF Theorem 1a
Using Assumption (c), Lemma 1 with
x = Xk
Z = Zk
z+ = zk + 1
y = Xs
y+ = χk+ι
Q = QS
由=βs
and taking expectation, if q§ (1 - βs) < 1 /L, we have
1 _ O (Eik [F(Xk+1)] - F(X*)) + 2— EiJllZk+1 - x*∣∣2]
1 - βS	2QS	L	」
< -s^- (F(XS) - F(x*)) + ɪ ∣∣zk - x*∣∣2 + ——,+ M)2——
- 1-βs' (S)	(	" + 2Qskk k + 2(Q-I-L(1-βs))
Summing the above inequality from k = sm,...,sm + m - 1, we obtain
r	m	r
(Γ-β> X(ElF (XSm+j )] - F (X*)) + RE [M + 1)m -XI2]
< -^βs-(F(¾3) - F(x*)) + -ɪ ∣zsm - x*∣∣2 +--(° + M)2---,
<1-βs'(	s)	(	" +	2Qsm k sm k	+ 2(Q-I-L(1-βs)),
Using the definition of 金s+ι and convexity,
7^⅛ (E [F(≡s + 1)] - F(X*)) + 21-E [∣∣Z(s+1)m - X*『]
1 β S	2/m L	」
<	QSβs	(F (X) _ F (X ?))	+	X	kz	_X*||2 + Qs (b2 + M 2)
<	1 - βs(FES) - (X "	+	2m	kzsm	- x k + Q-1 - l(1 - βs)
(12)
(13)
It can be verified that with the choices βs =吊 and QS = L(I-.), the following holds for S ≥ 0,
QS+1βs+1
1 - βs+1
Qs
< Kand β0=0∙
(14)
Note that since our analysis aims at providing intuition, we do not refine the choice of Qs as in (Hu
et al., 2009; Ghadimi & Lan, 2012). Thus, by telescoping (13) from S = S - 1,..., 0, we obtain
I QS-IT (E [F3s)] - F(x*)) + 2mE [∣∣zsm - x*『]
S-1
< ɪ ∣χ0-χ*k2 + X
2m	z—z
S=0
Qs(σ2 + M2)
Qs 1 — L(1 — βs)
and thus,
E [F(⅛)] - F(x?) <
2L	∣∣x - x*∣2 + 4L(b2 + M2) X	Q2
λ1m(S + 1)2 k 0 k + λ1(S +1)2 S⅛ 1 - Qs(1 - βs)L
(a)	2L
< λ1m(S + 1)2
∣∣X0 - χ*∣∣2 +
3λ1(σ2 + M2)
L(S +1)2
S-1
X (S + 2)2
S=0
(b)	2L
< λ1m(S + 1)2
∣∣X0 — χ*∣∣2 +
8λ1(σ2 + M 2)(S + 1)
L
22
Under review as a conference paper at ICLR 2020
where (a) follows from λι ≤ 3 and (b) holds because 0 ≤ x → (X + 2)2 is non-decreasing and thus
S-1	S
X (s + 2)2 ≤	(x + 2)2dx ≤
(S + 2)3 V 8(S +1)3
-3- ≤	3
Denoting
λ? ,	L kx0 - x*k
1	2√m√σ2 + M2 (S + 1)3，
and based on the choice of λι = min {3,入；}, if λ; ≤ 22, we have
E[F(Xs)] - F(x?) ≤
8 IM - x?|| √σ2 + M2
m2 (S + 1) 2
If λi > I,
E [F(XS)] - F(x?) ≤
3L ∣∣xo — x?k2	4 ∣∣xo — x?k √σ2 + M2
m(S +1)2	+ m1 (S + 1) 2
Thus, we conclude that
E [F(XS)] — F(x?) ≤
3L ∣∣xo — x?|2	8 ∣∣xo — x?| √σ2 + M2
m(S + 1)2	+ m1 (S + 1) 2
Substituting S = K/m completes the proof.
B.2.3 Proof of Theorem 1b
In order to prove Theorem 1b, we need the following known result for the martingale difference (cf.
Lemma 2 in Lan et al. (2012)):
Lemma 2. With N > 0, let ξ0, ξ1, . . . , ξN-1 be a sequence of i.i.d. random variables, for
t = 0, . . . , N — 1, σt > 0 be a deterministic number and ψt = ψt(ξ0, . . . , ξt) be a determinis-
tic measurablefUnction such that Eξt [ψt] = 0 a.s. and Eςt [exp{ψ2/σt2}] ≤ exp{1} a.s.. Thenfor
any Λ ≥ 0,
N-1	uN-1
≥ Λt X σ2 > ≤ exp{—Λ2∕3}.
t=0	t=0	
To start with, using Lemma 1 with the parameter mapping (12), we have
1⅛(F(xk+ι) — F(x?)) + 4 ∣zk+ι — x?k2
1 — βs	2αs
≤
T^sr(F(XS)- F(x?)) + 2^ ∣zk — x?k2
1 — βs	2αs
1 (∣Vf (Xk )—Vfik (Xk )k + M)2
2(αs 1 — L(I - βS))
+ hVf(xk) — Vfik(xk), zk — x?i
≤ 1-β;(F(Xs) — F(x?)) + 20- ∣zk — x?|2 +
M2
as 1 — L(1 — βs)
,kVf (Xk)-Vfik (Xk)I2 , /w 'W 一、	?\
+	-1 γτ. 7^7	+ hVf (Xk) -Vfik (Xk), zk - X i ∙
αs-1 — L(1 — βs )
23
Under review as a conference paper at ICLR 2020
Summing the above inequality from k = sm, . . . , sm + m - 1 and using the choice αs
with λι ≤ 3, we obtain
λl
L(1-βs)
1 asβ (F(xs + 1) - F(X*力 + 2^Ξ Ilz(S+ 1)m - x*∣∣2
-βs	m
≤ 1⅛(F (XS)- F (x?))+ 2m kzsm - x?k2+ 3α2M2
2 Sm+m-1	Sm+m-1
+ B X kVf(xk)-Vfik(Xk)k2 + αs X (Vf(Xk)-Vfik(Xk),zk - x?i.
m	km	k
k=Sm	k=Sm
With our parameter choices, the relations in (14) hold and thus we can telescope the above inequality
from s = S - 1, . . . , 0,
αs-i
1 - βs-ι
(F(XS) - F(x?)) ≤
ɪ kX0 -
2m
S-1
X?k2+3M2X αS2
S=0
3 K-1
+ m X α[k∕mJ kVf(Xk) -Vfik(Xk)k2
m k=0
X--------------{---------------}
R4
(15)
1 K-1
+---abk∕mC hV f (Xk ) - Vfik (Xk ), zk - X?i ∙
m k=0
V-------------------{-------------------}
R5
DenOting Vk , kVf(Xk) - Vfik (Xk )k2, α = PK-01 a2k∕mc = m PSS=-01 αS2, for R4, by Jensen’s
inequality, we have
1 K-1
E卜PI标X
K-1
k=0
α2k∕mCV27σ2
K-1	(?)
Σ αbk∕mcE [eχp {Vk7σ2}] ≤ eχp{1},
k=0
where (?) uses the additional assumption Eik [exp {V2∕σ2}] ≤ exp{1}.
Then, based on Markov’s inequality, we have for any Λ ≥ 0,
Prob exp lɪX
αb2k∕mcV2∕σ2} ≥ exp{Λ+ 1}} ≤ exp{-Λ},
Prob R4 ≥ (Λ + 1)σ2m X αS2 ≤ exp{-Λ}.
(16)
ForR5, since we have Eik αbk∕mc hVf (Xk) - Vfik (Xk), zk - X?i = 0 and
Eik exP ( abk/mc hv":?~[fD(Xk)，zk_X^- )# ≤ Eik [exp {V"σ2}] ≤ exp{1},
αbk∕mc σ	X
which is based on the “light tail” assumption, using Lemma 2, we obtain
f	^^S-- ∖
Prob R5 ≥ ΛσDX m	aS2	≤ exP{-Λ2∕3}.	(17)
I	∖ s=0
Combining (15), (16) and (17), based on the parameter setting (cf. (5)) and using the notation
δ 3Lm ∣∣X0 一 X?k2	8 ∣∣X0 一 X*∣∣√σ2 + M2
K0(m),	(K+m)2	+ —√K+m—,
R6
12Lσ2
λι(S +1)2
S-1
XaS2
S=0
4LσDχ
λι(s + i)2√m
+
24
Under review as a conference paper at ICLR 2020
we conclude that
Prob{F(Xs) — F(x*) ≤ K0(m) + ARg} ≥ 1 — (exp{-Λ2∕3} + exp{-Λ}).
For R6, using the choice of as and λι, we obtain
4√6σDχ	8λ1σ2(S +1)	4√6σDχ	4σ2 ∣∣x0 - x*∣∣
R6 ≤ --, 二 +---------------- ≤ --.	= +-,	-~~.	==
― 3y/K + m	L 3 3y/K + m	√K + m√σ2 + M2
V 4σ(3 ∣∣x0 - x*∣ + √6Dχ)
≤	3√K + m	,
which completes the proof.
B.2.4 PROOF OF THEOREM 2
Using Assumption (c), Lemma 1 with

jk
k
Z = Zk
z+ = Zk + 1
y = φjk
y+ =限1
α = αk
β = βk
and taking expectation, if ak (1 - βk) < 1∕L,we have
r⅛Eik,jk [F(φj+) - F(x*)] + ɪEik,jj∣zk+1 - x*k2]
1 — βk	2αk
≤	Ejk [F(φk) - F(x?)] + ɪ B - x*∣2 + °, « +M∖.
1 - βk Ljk	2以	2(αJ- L(1 - βk))
Note that
Eik,jk [F(j+1)- F(x?)]
m	m
=Eik,jk X (F(Φk+1) - F(x?)) - X (F(φk) - F(x?))
j=1	j=jk
(18)
m
Eik,jk X (F(Φk+1)- F(x?))
j=i
m
-Ejk	X (F就)-F(x?))
j=jk
Dividing both sides of (18) by m and then adding([，忆加Ejk
sides, we obtain
[∑m,jk (F(φj) - F(x?))] to both
1
1 - βk
m
m X F (若+1)- F (x?)
j=i
+ 20kmEik jk [∣zk+ι - x?k2]
1一，	1	1	、1
≤ -mEjk[F(φkk)-F(x?)] + E (m∙XF就)-F(x?)I +20kmB-x?k2
,	(σ + M )2
2m(αk 1 - L(1 - βk))
1 - 1-βk
_____m
1 - βk
m
m X F 就)-F (x?)
j=1
+ 20m
B-x*k2 + 2m(α-r MI-βk)). (19)
1
25
Under review as a conference paper at ICLR 2020
It can be verified that with our parameters choice: βk = ；^+ and ak
holds for k ≥ 0,
L(1-βk),
the following
1 - 1-βfe + 1	α.
Qk+11⅛ ≤ 占 and β0 = 0.
λ
Note that since our analysis aims at providing intuition, we do not refine the choice of a§ as in (HU
et al., 2009; Ghadimi & Lan, 2012). Then, we can telescope (19) from k = K - 1,..., 0, which
results in
QKT E
1 — βκ-ι
m
-X F(ΦK) - F(x*)
m
j=i
+ 2mE [∣zk - x*∣2]
‹ λ2(m - 1)
Lm
1	KT
(F(x0) - F(x?)) + K ∣∣x0- x*∣∣2 +£
k=0
2m(α-1
αk(σ + M )2
一 L(I - βk))
Using the definition of φκ and convexity, we obtain
1 - βκ-ι λλ2(m - 1)
oK-1	∖
+ 1 - βK-1
ακ-ι
Lm
K-1
Σ
k=0
(F(x0)- F(x*)) +
Qk (σ + M )2
2m(QkI- L(1 - βk))
(a)	4(m - 1)(F(x0) - F(x*))
m (K- + 2)2
3λ2(σ + M )2
2Lm (KmI + 2)2
12
而 ∣x0-x*∣2
+ 2L ∣∣x0 - x*∣2
λ2m (K-1 +2)2
K-I / 1	∖ 2
χ( m+2)
k=0 '	/
(b)	4(m — 1)(F(x0) — F(x*))
m (K-I
∖ m
+ 2L ∣x0 - x*∣2
λ2m (K-1 +2)2
(20)
+

+
4λ2(σ + M)2 (Km1 + 2)
L
where (a) uses λ2 ≤ 3, (b) follows from simple integration arguments and that m + 2 ≤
2 (K-1 + 2) since K ≥ 1, m ≥ 1.
Based on the choice of
in J 3,
L ∣∣χ0 - χ*∣∣
__	3
√2m(σ + M)( Km 1+ 2)2
(20) can be further upper bounded as
1
2
4(m — 1)(F(x0) — F(x*))	3L ∣∣x0 — x*∣2	4√2 ∣∣x0 — x*∣ (σ + M)
+	(K- + 2)2 +
m (K-1 +2)2
2
m1 (K- +2)
m
26
Under review as a conference paper at ICLR 2020
B.3 Connections between AM1-SGD and Katyusha
The discussion in this section aims to shed light on the understanding of the experimental results,
which also shows some interesting relations between AM1-SGD and Katyusha.
The high level idea of Katyusha momentum is that it works as a “magnet” inside an epoch of SVRG
updates, which “stabilizes” the iterates so as to make Nesterov’s momentum effective (Allen-Zhu,
2018). In theory, the key effect of Katyusha momentum is that it allows the tightest possible variance
bound for the stochastic gradient estimator of SVRG (cf. Lemma 2.4 and its comments in Allen-
Zhu (2018)). In this sense, we can interpret Katyusha momentum as a variance reducer that further
reduces the variance of SVRG. Below we show the similarity between the construction of Katyusha
and AM1-SGD, based on which we conjecture that the amortized momentum can also reduce the
variance of SGD (and thus increase the robustness). However, in theory, following a similar analysis
of Katyusha, we cannot guarantee a reduction of σ in the worst case.
Deriving AM1-SGD from Katyusha Katyusha has the following scheme (non-proximal, in the
original notations, σ is the strong convexity parameter, cf. Algorithm 1 with Option I in Allen-Zhu
(2018))12:
Initialize: e0 = yo = zo = xo, η = 3L, ω = 1 + ασ.
1:	for s = 0, . . . , S - 1 do
2:	Compute and store Vf (χs).
3:	for j = 0, . . . , m - 1 do
4:	k = sm + j.
5：	Xk = τι ∙ Zk + T2 ∙ es + (1 — Ti 一 T2)∙ yk.
6:	Vek =Vfik(xk)—Vfik(xes)+Vf(xes).
-1.	ʊ
7：	zk + 1 = Zk 一 α ∙ Vk.
C .	ʊ
8：	yk + i = Xk 一 η ∙vk.
9:	end for
10：	es+1 = (Pmo1 ωj) - ∙ Pm=o1 ωj ∙ ysm+j+1.
11： end for
Output: XeS .
We can eliminate the sequence {Zk} in this scheme. Note that in the parameter setting of Katyusha,
we have η = αT1 , and thus
Xk+1 = Ti ∙ Zk+1 + T2 ∙ es + (1 — Ti — T2) ∙ yk + i
s
=Ti ∙ Zk — η ∙ Vk + T2 ∙ X +(1 — Ti — T2) ∙ yk + (1 — Ti — T2) ∙ (yk+i — y)
=Xk — η ∙ Vk + (1 — Ti — T2) ∙ (yk+i — yk)
=yk+i + (I — Ti 一 T2) ∙ (yk+i 一 yk).
Hence, the inner loops can be written as
yk+i = Xk — η ∙ Vk,
xk+i = yk + i + (I 一 Ti 一 T2) ∙ (yk+i 一 yk),
which is the Nesterov’s scheme (scheme (1)). At the end of each inner loop (when k = sm+m— 1),
X(s+i)m = Ti ∙ z(s + i)m + t2 ∙ es + (I — Ti — t2 ) ∙ y(s+i)m,
while at the beginning of the next inner loop,
X(s+i)m = τi ∙ z(s + i)m + τ2 ∙ es+i + (I — τi — τ2 ) ∙ y(s+i)m,
which means that We need to set X(s+i)m — X(s+i)m + τ2 ∙ (es+i — es) (reassign the value of
X(s+i)m).
Then, the following is an equivalent scheme of Katyusha：
12We change the notation xk+1 to xk.
27
Under review as a conference paper at ICLR 2020
Initialize: xe0 = y0 =xo, η = 3L, ω = 1 + ασ.
1:	for s = 0, . . . , S - 1 do
2:	for j = 0, . . . , m - 1 do
3:	k = sm + j.
t	τ~7
4：	yk+ι = Xk -	η ∙ Vk.
5:	xk+1 = yk+1	+ (I - τ1	- τ2)	∙ (yk+1 -	yk ).
6： end for
7:	es+1 = (Pm01 ωj) -1 ∙ ∑m=01 ωj ∙ ysm+j+1.
8：	x(s + 1)m - x(s+1)m + τ2 ∙ (es + 1 - es).
9:	end for
Output: xeS .
Now it is clear that the inner loops use Nesterov’s momentum and the Katyusha momentum is
injected for every m iterations. If we replace the SVRG estimator Vk with Vfik (xk), set 1 - τ1 -
τ2 = 0, which is to eliminate Nesterov’s momentum, and use a uniform average for xes+1, the above
scheme becomes exactly AM1-SGD (Algorithm 1).
If we only replace the SVRG estimator Vek, the scheme can be regarded as adding amortized mo-
mentum to M-SGD. This scheme requires tuning the ratio of Nesterov’s momentum and amortized
momentum. In our preliminary experiments, after suitable tuning, we observed some performance
improvement. However, this scheme increases the complexity, which we do not consider it worth-
while.
A recent work (Zhou et al., 2018) shows that when 1 - τ1 - τ2 = 0, which is to solely use Katyusha
momentum, one can still derive optimal rates and the algorithm is greatly simplified. Their proposed
algorithm (i.e., MiG) is structurally more similar to AM1-SGD.
C Miscellanies
C.1 Comparison of SGD and M-SGD
Ma & Yarats (2019) normalize the momentum buffer of M-SGD, which results in the following
formulation (α ∈ R, vqh ∈ Rd , v0qh = 0):
vkq+1 = β ∙ vkqh + (1 - β) ∙ Vfik (Xk),
Xk+1 = Xk- α ∙ (β ∙ vq+1 + (1- β) ∙ Vfik (Xk)), for k ≥ 0.
This scheme is equivalent to the PyTorch formulation (scheme (3)) through Vpt = (1 - β)-1 ∙ Vqh
and η = α(1 - β).
Based on this formulation, α is understood as the effective learning rate (i.e., the vector it scales
has the same cardinality as a gradient) and the experiments in Ma & Yarats (2019) were conducted
with fixed α = 1. Their results indicate that when using the same effective learning rate, M-SGD
and SGD achieve similar performance and thus they suspect that the benefit of momentum basically
comes from using sensible learning rates.
Here we provide some intuition on their results based on convex analysis. For simplicity, we consider
deterministic smooth convex optimization. In theory, to obtain the optimal convergence rate, the
effective learning rate α is set to a very large O(k/L), which can be derived from Theorem 1 or
Theorem 2 by setting σ = 0,M = 0,m = 1 (then λ1 or λ? is always 2 since the other term is
∞). IfWe fix α = 3L for both methods, GD has an OQlK) convergence rate (cf. Theorem 2.1.13
in Nesterov (2013b)). For the Nesterov,s method, if we use βk = k+2, it has the convergence rate
(applying Lemma 1):
τ⅛(F(yk+1) - F(X?))+ 3L kzk+1 - X?k2 ≤ TT⅛T (F(yk) - F(X?)) + 3L kzk - X?k2,
1 - βk	4	1 - βk	4
F(yK) - F(X?) ≤
3L ∣∣xo — X?k2
2(K + 1)
28
Under review as a conference paper at ICLR 2020
Figure 15: Train-batch loss vs. full-batch loss. Best viewed in color.
Thus, in this case, both GD and the Nesterov’s method yield an O(1/K) rate, and thus we expect
them to have similar performance. This analysis suggests that the acceleration effect basically comes
from choosing a large effective learning rate, which corresponds to the observations in Ma & Yarats
(2019).
However, what is special about the Nesterov’s method is that it finds a legal way to adopt a large
α that breaks the 1/L limitation. If GD uses the same large α, we would expect it to be unstable
and potentially diverge. In this sense, Nesterov’s momentum can be understood as a “stabilizer”.
In our basic case study (ResNet34 on CIFAR-10), if we align the effective learning rate and set
γ = 1.0 for SGD, the final accuracy is improved but the performance is highly unstable and not
robust, which is 2.205% average STD of test accuracy over 5 runs. The significance of QHM (Ma
& Yarats, 2019) is that with suitable tuning, it achieves much faster convergence without changing
the effective learning rate. Our work uses the convergence behavior of SGD as a reference to reflect
and to understand the features of our proposed momentum, which is why we set γ = η.
C.2 Training evaluation
Due to the mechanism of back-propagation, evaluating train-batch loss basically incurs no overhead.
It can be efficiently used to indicate the training progress. However, it can only be treated as a coarse
approximation to the full-batch loss as shown in Figure 1c. If batch normalization (Ioffe & Szegedy,
2015) or dropout (Srivastava et al., 2014) is used in training, the model changes during the training
phase, which makes train-batch loss less accurate. More importantly, train-batch loss is always
observed to be statistically stable, which omits many important characteristics of an optimizer such
as robustness, oscillations, etc. We include a comparison of train-batch loss and full-batch loss on
training ResNet18 with pre-activation on CIFAR-10 (the experiment in Section 4) in Figure 15.
We also notice that for different optimizers, even if their convergences on train-batch loss are indis-
tinguishable, their convergences on test accuracy can vary greatly. We show two examples in Figure
16, where the ImageNet experiment is from Section 5 and the CIFAR-10 experiment is from Table
4 with m = 10.
D	Experimental Setup
All of our experiments were conducted using PyTorch (Paszke et al., 2017) library.
D. 1 Classification Setup
CIFAR-10 & CIFAR-100 Our implementation (e.g., ResNet and DenseNet implementations,
data pre-processing) generally follows the repository https://github.com/kuangliu/
pytorch-cifar. All the CIFAR experiments in this paper used a single GPU in a mix of
RTX2080Ti, TITAN Xp and TITAN V. The batch size is fixed to 128. We used cross-entropy loss
with 0.0005 weight decay and used batch normalization (Ioffe & Szegedy, 2015). Data augmen-
tation includes random 32-pixel crops with a padding of 4-pixel and random horizontal flips with
29
Under review as a conference paper at ICLR 2020
Epoch
Epoch
Figure 16: Train-batch loss vs. test accuracy. Best viewed in color.
0.5 probability. We used step (or multi-step) learning rate scheduler with a decay factor 10. For the
CIFAR-10 experiments, We trained 90 epochs and decayed the learning rate every 30 epochs. For
the CIFAR-100 experiments, We trained 300 epochs and decayed the learning rate at 150 epoch and
225 epoch following the settings in DenseNet (Huang et al., 2017).
ImageNet In the ImageNet experiments, we tried both ResNet50 and ResNet152 (He et al.,
2016b). The training strategy is the same as the PyTorch,s official repository https://github.
com/pytorch/examples/tree/master/imagenet, which uses a batch size of 256. The
learning rate starts at 0.1 and decays by a factor of 10 every 30 epochs. Also, we applied weight
decay with 0.0001 decay rate to the model during the training. For the data augmentation, we ap-
plied random 224-pixel crops and random horizontal flips with 0.5 probability. Here, we run Ml
experiments across 8 NVIDIA P100 GPUS for 90 epochs.
D.2 Language Model Setup
We followed the implementation in the repository https://github.com/salesforce/
awd-lstm-lm and trained word level Penn Treebank with LSTM without fine-tuning or con-
tinuous cache pointer augmentation for 750 epochs. The experiments were conducted on a single
RTX2080Ti. We used the default hyper-parameter tuning except for learning rate and momentum:
The LSTM has 3 layers containing 1150 hidden units each, embedding size is 400, gradient clipping
has a maximum norm 0.25, batch size is 80, using variable sequence length, dropout for the layers
has probability 0.4, dropout for the RNN layers has probability 0.3, dropout for the input embed-
ding layer has probability 0.65, dropout to remove words from embedding layer has probability 0.1,
weight drop (Merity et al., 2017) has probability 0.5, the amount of '2-regularization on the RNN
activation is 2.0, the amount of slowness regularization applied on the RNN activation is 1.0 and all
weights receive a weight decay of 0.0000012.
30