Under review as a conference paper at ICLR 2020
Safeguarded Learned Convex Optimization
with Guaranteed Convergence
Anonymous authors
Paper under double-blind review
Ab stract
Many applications require quickly and repeatedly solving a certain type of opti-
mization problem, each time with new (but similar) data. However, state-of-the-
art general-purpose optimization methods may converge too slowly for real-time
use. This shortcoming is addressed by learning to optimize (L2O) schemes, which
construct neural networks from parameterized forms of the update operations of
general-purpose methods. Inferences by each network form solution estimates,
and networks are trained to optimize these estimates for a particular distribution
of data. This results in task-specific algorithms (e.g., LISTA, ALISTA, and D-
LADMM) that can converge order(s) of magnitude faster than general-purpose
counterparts. We provide the first general L2O convergence theory by wrapping
all L2O schemes for convex optimization within a single framework. Existing
L2O schemes form special cases, and we give a practical guide for applying our
L2O framework to other problems. Using safeguarding, our theory proves, as the
number of network layers increases, the distance between inferences and the solu-
tion set goes to zero, i.e., each cluster point is a solution. Our numerical examples
demonstrate the efficacy of our approach for both existing and new L2O methods.
1	Introduction
Solving scientific computing problems often requires application of efficient and scalable optimiza-
tion algorithms. Despite the ever improving rates of convergence of state-of-the-art general purpose
algorithms, their ability to apply to real-time applications is still limited due to the relatively large
number of iterations that must be computed. To circumvent this shortcoming, a growing num-
ber of researchers use machine learning to develop task-specific algorithms from general-purpose
algorithms. For example, inspired by the iterative shrinkage thresholding algorithm (ISTA) for solv-
ing the LASSO problem, a sparse coding problem, Gregor & LeCun (2010) proposed to learn the
weights in the matrices of the ISTA updates that worked best for a given data set, rather than leave
these parameters fixed. They then truncated the method to K iterations, making their Learned
ISTA (LISTA) algorithm form a K-layer feed-forward neural network. Empirically, their examples
showed roughly a 20-fold reduction in computational cost compared to the traditional algorithms.
Several related works followed, also demonstrating numerical success (discussed below). While
classic optimization results often provide worst-case convergence rates, limited theory exists per-
taining to such instances of data drawn from a common distribution (e.g., data supported on a low-
dimensional manifold). As a step toward providing such theory, this work addresses the question:
Does there exist a universal method that encompasses all L2O algorithms and generates
iterates that approach the solution set with guarantees?
We provide an affirmative answer to this question by prescribing and proving properties of neural
networks generated within our L2O framework. Convergence is established by including any choice
among several practical safeguarding procedures, including nonmonotone options. Nonmonotone
safeguarding enables sequences to traverse portions of the underlying space where the objective
function value may increase for a few successive iterations as long as, on average, the sequence
approaches the solution set. Although counterintuitive, this ability may lead to faster convergence.
Furthermore, we provide a practical guide in our discussion for how practitioners may use our frame-
work to create and apply L2O schemes to their own problems.
1
Under review as a conference paper at ICLR 2020
The theoretical portion of this work is presented in the context of fixed point theory. This is done to
be sufficiently general and provide the desired convergence result for the wide class of optimization
methods that can be expressed as special cases of the Krasnose1'Skii-Mann (KM) method. For
concreteness and ease of application, we then provide the special-case results to several well-known
methods (e.g., proximal-gradient, Douglas-Rachford splitting, and ADMM).
Related Works.
Learning to learn methods date back decades (e.g., see (Thrun & Pratt, 1998) for a survey of earlier
works and references). A seminal L2O work in the context of sparse coding was by Gregor &
LeCun (2010). Numerous follow-up papers also demonstrated empirical success at constructing
rapid regressors approximating iterative sparse solvers, compression, `0 encoding, combining sparse
coding with clustering models, nonnegative matrix factorization, compressive sensing MRI, and
other applications (Sprechmann et al., 2015; Wang et al., 2016a;b;c;d; Hershey et al., 2014; Yang
et al., 2016). A nice summary of unfolded optimization procedures for sparse recovery is given by
Ablin et al. (2019) in Table A.1. However, the majority of L2O works pertain to sparse coding and
provide limited theoretical results. Some works have interpreted LISTA in various ways to provide
proofs of different convergence properties (Giryes et al., 2018; Moreau & Bruna, 2017). Others have
investigated structures related to LISTA (Xin et al., 2016; Blumensath & Davies, 2009; Borgerding
et al., 2017; Metzler et al., 2017), providing varying results dependent upon the assumptions made.
Chen et al. (2018) introduced necessary conditions for the LISTA weight structure to asymptotically
achieve a linear convergence rate. This was followed by Liu et al. (2019a), which proved linear
convergence of their ALISTA method for the LASSO problem and provided a result stating that, with
high probability, the convergence rate of LISTA is at most linear. The mentioned results are useful,
yet can require intricate assumptions and proofs specific to the relevant sparse coding problems.
L2O works have also taken other approaches. For example, the paper by Li & Malik (2016) used
reinforcement learning with an objective function f and a stochastic policy ∏* that encodes the
updates, which takes existing optimization algorithms as special cases. Our work is related to theirs
(cf. Method 1 below and Algorithm 1 in that paper), with the distinction that we include safeguarding
and work in the fixed point setting. The idea of Andrychowicz et al. (2016) is to use long short term
memory (LSTM) units in recurrent neural networks (RNNs). Additional learning approaches have
been applied in the discrete setting (Dai et al., 2018; Li et al., 2018; Bengio et al., 2018). Balcan
et al. (2019) reveal how many samples are needed for the average algorithm performance on the
training set to generalize over the entire distribution. This is practical for choosing training data and
may be used for training L2O networks within our framework.
Our Contribution. This is the first work to merge ideas from machine learning, safeguarded opti-
mization, and fixed point theory into a general framework for incorporating data-driven updates into
iterative convex optimization algorithms. In particular, given a collection of data and an update op-
erator from an established method (e.g., ADMM or proximal gradient) for solving an optimization
problem, we present procedures for creating a neural network that can be used to quickly infer solu-
tion estimates. The first novelty of this framework is the ability to incorporate several safeguarding
procedures in a general setting. The second is that we present a procedure for utilizing machine
learning methods to incorporate knowledge from particular data sets. However, our most significant
contribution to the L2O literature is to combine these results into a single, general framework for
use by practitioners on any convex optimization problem.
Outline. We first provide a brief overview of the fixed point setting of this work in Section 2. Then
we present the SKM method and convergence results in Section 3. The incorporation of the SKM
method into a neural network and subsequent training approach is presented in Section 4. This is
followed in Section 5 by numerical examples, discussion in Section 6, and conclusions in Section 7.
2	Fixed Point Methods
Let H be a finite dimensional Hilbert space (e.g., the Euclidean space Rn) with inner product(,, •)
and norm k∙∣∣. Denote the set of fixed points of each operator T : H → H by Fix(T) := {x ∈ H :
Tx = x}. In this work, for an operator T with a nonempty fixed point set (i.e., Fix(T) = 0), the
primary problem considered is the fixed point problem:
Find x? ∈ Fix(T).	(1)
2
Under review as a conference paper at ICLR 2020
Convex minimization problems, both constrained and unconstrained, may be equivalently rewritten
as the problem (1) for an appropriate mapping T . The method chosen for solving the minimization
problem determines the operator T in (1) (e.g., see Table 1 below for examples). We focus on
the fixed point formulation to provide a general approach, given T , for creating a sequence that
converges to a solution of (1) and, thus, also of the corresponding optimization problem.
The following definitions will be used in the sequel. A mapping T : H → H is nonexpansive if
kTx - Tyk ≤ kx - yk, for all x, y ∈ H.	(2)
An operator T : H → H is α-averaged if α ∈ (0, 1) and there is a nonexpansive operator Q :
H → H such that T = (1 - α)Id + αQ, where Id is the identity operator. If the constant α is not
important, then T may for brevity be called averaged. We also denote the distance between a point
x ∈ H and a set C by
dC (x) := inf{kx - yk : y ∈ C}.	(3)
Two operators frequently used in optimization are constructed from monotone relations. Letting
α > 0 and f : H → R be a function, the resolvent of the (possibly) multi-valued subgradient ∂f is
Jα∂f := (Id+α∂f)-1	(4)
and the reflected resolvent of ∂f is
Rα∂f := 2Jα∂f - Id.	(5)
If f is closed, convex, and proper, then the resolvent is precisely the proximal operator, i.e.,
Jadf = PrOxaf(x) ：= argminαf(z) + 2∣∣z — x∣∣2.	(6)
From these definitions, it can be shown that Rα∂f is nonexpansive and Jα∂f is averaged. Results
above may all be found in Bauschke & Combettes (2017) (e.g., see ProP. 4.4, Thm. 20.25, ExamPle
23.3, and ProP. 23.8). See Table 1 for examPles of these oPerators in oPtimization methods.
A classic theorem states that sequences generated by successively aPPlying an averaged oPerator
converges to a fixed Point. This method comes from Krasnosel’skii (1955) and Mann (1953), which
yielded adoption of the name Krasnosel，ski(-Mann (KM) method. This result is stated below and
can be found with various forms and Proofs in many works (e.g., see (Bauschke & Combettes, 2017,
Thm. 5.14), (Byrne, 2008, Thm. 5.2), (Cegielski, 2012, Thm. 3.5.4), and (Reich, 1979, Thm. 2)).
Theorem 2.1. If an averaged operator T : H → H has a nonempty fixed point set and a sequence
{xk}k∈N with arbitrary x1 ∈ H satisfies the update relation
xk+1 = T(xk), for all k ∈ N,	(7)
then there is x? ∈ Fix(T) such that {xk}k∈N converges to x?, i.e., xk → x?.
There are pathological cases where the result fails for operators that are only nonexpansive (e.g.,
when x1 6= 0 and either T = —Id or T is a rotation). However, this is easily remedied since any
convex combination of a nonexpansive operator with the identity is averaged.
3	Safeguarded KM Method
This section generalizes the classic KM iteration in (7). We accomplish this by defining an envelope
of operators TL20(•;•). For a parameter Z chosen from an appropriate set, We let TL20(∙; Z)
define an operator on H. Changing ζ may define a new operator with different properties. We
do not impose restrictions on TL20(∙; Z) other than it be well-defined, meaning TL20(∙; Z) may
fail to be averaged and/or fail to have a fixed point. This is illustrated by the following two examples.
Example 3.1.	Let Q : H → H be nonexpansive. Then define TL2O : H × R → R by
TL2O(x; Z) := (1 —Z)x+ZQ(x).	(8)
For Z ∈ (0,1), the operator TL20(∙; Z) defined in (8) is Z-averaged. Although using Z》1 may
result in an operator that fails to be averaged, this can be useful in accelerating the convergence of a
method (e.g., see (Giselsson et al., 2016)).	4
3
Under review as a conference paper at ICLR 2020
Table 1: Below are the averaged operators for well-known algorithms. We assume α > 0 and,
when α is multiplied by a gradient, we also assume α < 2/L, with L the Lipschitz constant for the
gradient. The dual of a function is denoted by a superscript * and Ω := {(x,z) : Ax + Bz = b}. The
block matrix M is M = (α-1Id, AT; -A, β-1Id). In each case, L is the associated Lagrangian.
Problem	Method	Averaged Operator T
min f (x)	Gradient Descent	Id - aVf
min f (x)	Proximal Point	Proxaf
min{g(x) : x ∈ C}	Projected Gradient	PrOjC ◦ (Id — aVg)
min f (x) + g(x)	Proximal Gradient	PrOxaf ◦ (Id — aVg)
min f (x) + g(x)	Peaceman-Rachford	Ra∂f ◦ Radg
min f (x) + g(x)	Douglas-Rachford	1(Id + Radfo Radg)
min f(x) + g(z)	ADMM	2 (Id + RaA∂f* (AT ∙) ◦ Ra(Bdg*( BT ∙)-b))
minf (x) s.t. Ax = b	Uzawa	Id + α (AVf *(—AT∙) — b)
minf (x) s.t. Ax = b	Proximal Method of Multipliers	JadL
minf (x) + g(Ax)	PDHG	JM TdL
Method 1 Safeguarded Krasnose1'Skii-Mann (SKM)
1:	Choose x1 ∈ H and δ ∈ (0, 1)
2:	for k = 1, 2, . . . do
3:	Choose parameter ζk
4:	Choose μk ∈ (0, ∞)
5:	yk = TL2O(xk; ζk)
6:	if kS(yk)k ≤ (1- δ)μk then
7:	xk+1 = yk
8:	else
9:	xk+1 = T(xk)
10:	end if
11:	end for
Example 3.2.	Let f : H → R be closed, convex and proper, and define TL2O : H × (0, ∞) → R by
TL2O(x; ζ) := proxζ f (x).	(9)
For fixed Z ∈ (0, ∞), the operator TL20(∙; Z) is averaged.	4
The practicality of TL2O is discussed and demonstrated in Sections 4 and 5, respectively. In the
remainder of this work, each operator T : H → H is assumed to be averaged and we set S :=
Id 一 T. Our proposed method below is called the Safeguarded KraSnoSe1'Skii-Mann (SKM) Method.
Explanation of the SKM Method is as follows. In Line 1, the initial iterate and parameter δ are
initialized. A common choice for the initial iterate is x1 = 0. The for loop on Lines 2 to 11
4
Under review as a conference paper at ICLR 2020
Table 2: Choices for μk updates that ensure Assumption 3 holds. Here α ∈ (0,1) and, for fixed
m ∈ N, Ξk is the set of the most recent min{m, k} indices for which the inequality in Line 6 holds.
Name
Geometric Sequence
GS(α)
Update Formula
(1 一 α)μk if Line 6 holds,
μk	otherwise.
Decrease μk by geometric factor whenever Line 6 holds.
Arithmetic Average
AA
μk+ι:
mk+1 :=
1
mk + 1
mk + 1	if Line 6 holds,
mk	otherwise.
(kS (xk+1)k + mk μk)	if Line 6 holds,
μk	otherwise.
Use mk to count how many times Line 6 holds and
μk is the average of the residuals among those times.
Exponential
Moving Average
EMA(α)
Recent Term
RT
Recent Max
RM(m)
:_ ( α∣∣S(xk+1)k + (1 - α)μk-ι	if Line 6 holds,
μk+1 :	ɪ	μk	otherwise.
Average μk with the latest residual whenever Line 6 holds.
kS(xk)k if Line 6 holds,
μk	otherwise.
Take μk to be most recent residual for which Line 6 holds.
μk+ι = max ∣∣S(x')k
'∈Ξk
Take μk to be max ofthe most recent residualsfor which Line 6 holds.
generates each update xk+1 in the sequence {xk}k∈N. The choice of parameter ζk in Line 3 may be
any value that results in a well-defined operator Tl2o(∙; Zk) in Line 5. The choice of μk in Line 4
defines the safeguarding procedure that is used to ensure convergence. Safeguarding is implemented
through a descent condition inequality in Line 6. When the inequality in Line 6 holds, TL2O(xk; ζk)
is used to update xk via Line 7. Otherwise, a KM update is used to update xk via Line 9. Notice
also the iteration indexed parameters may all be chosen dynamically (rather than precomputed).
Below are several standard assumptions used to prove our convergence result in Theorem 3.1.
Assumption 1. The operator T is nonexpansive with a nonempty fixed point set.
The following assumption ensures boundedness of sequences generated by the SKM method.
Assumption 2. The operator S is coercive, i.e.,
lim ∣S(x)∣ = ∞.	(10)
kxk→∞
Remark 3.1. Assumption 2 does not hold, in general, for nonexpansive operators. For example, if
T is the gradient operator (Id 一 αVf) for some α > 0 and f is a constant function, then S(x) = 0
for all x ∈ H. However, a minor perturbation to the f enables Assumption 2 to hold. In this
example, if one fixes small ε > 0 and sets f (x) := f (x) + W ∣∣χ∣∣2, then the associated S satisfies
∣S(x)∣ = ε∣x∣. This idea generalizes and, since this works for arbitrarily small ε, in practice it
may be reasonable to assume Assumption 2 holds when applying the SKM Method.
5
Under review as a conference paper at ICLR 2020
Algorithm 2 Learned SKM (LSKM)
1:	Stage 1: Initialization/Training.
2:	Choose envelope Tl2o(∙; ∙) and network structure C, parameterized by Θ = (Zk)3ι
3:	Choose training loss function φd
4:	Choose ‘optimal’ parameter
Θ? ∈ arg min Ed〜D
Θ∈C
φd(xK ) ,
assuming μk = ∞ at each layer k
5:	Choose δ and safeguarding scheme for {μk}3ι
6:	Define the neural network M = Mθ?,δ,μk.
7:	Stage 2: Inference.
8:	For input d return x = M(d)
Assumption 3. If the inequality in Line 6 is satisfied infinitely many times, then the sequence
{μk}k∈N converges to zero.	◊
Remark 3.2. Assumption 3 may be enforced by using various choices that are dependent upon
combinations of the previous residuals. This is illustrated by Table 2 and Corollary 3.1 below.
Our main convergence result is Theorem 3.1 below (proven in the Appendix).
Theorem 3.1. If {xk}k∈N is a sequence generated by the SKM method and Assumptions 1 to 3 hold,
then
lim dFix(τ) (Xk) =0.	(11)
k→∞
And, if {xk}k∈N contains a single cluster point, then {xk}k∈N converges to a point x? ∈ Fix(T).
We propose several methods for choosing the sequence {μk}k∈N in Table 2. These methods are
adaptive in the sense that each update to an iterate μk depends upon the current iterate Xk and
(possibly) previous iterates. These update schemes enable each μk to trail the value of the residual
norm kS (Xk)k. This implies there may exist j ∈ N for which
kS(xj)k < kS(xj+1)k = kS (TL2O(Xj; Zj)) k ≤ (1 - δ)μj.	(12)
Such leniency is desirable since it is possible that, even though the residual norms converge to zero,
constructing a sequence of iterates along the quickest route to the solution set requires traversing
portions of the underlying space where the residual norms increase for a few successive iterations.
The safeguarding schemes in Table 2 are justified by the Corollary below (proven in the Appendix).
Corollary 3.1. If {Xk}k∈N is a sequence generated by the SKM method and Assumptions 1 and 2
hold and {μk}k∈N is generated using a scheme outlined in Table 2, then Assumption 3 holds and,
by Theorem 3.1, the limit (33) holds.
4	A Neural Network View
The SKM method may be truncated and executed inferences with a neural network. The input into
the network is the data d, often in vector form. Each layer of the network consists of an iterate
Xk, with the output from the final layer K providing the inference estimate XK that approximates a
solution to (1). The feedforward operations between layers are formed by the operator Tl2o(∙, Zk)
if the descent condition in Line 6 of the SKM method holds and T otherwise. We encode all the
network parameters with Θ := (Zk)kK=1. The set C, over which Θ is minimized, may be chosen
with great flexibility, the only restriction being that C is a subset of the K-tuple set where each Zk
is such that Tl2o (∙; Zk) is well-defined. Since C determines the form of possible Θ, We refer to it
in Algorithm 2 as the network structure. For each application of the algorithm, the operators T and
S change, depending upon the particular data d. For example, the data d could correspond to the
measurement vector d = AX when attempting to recover X. Thus, to make explicit this dependence
of each operator on the data d, we henceforth incorporate a subscript to write Td and Sd .
6
Under review as a conference paper at ICLR 2020
The “optimal” choice of parameters Θ depends upon the particular application. Suppose each d is
drawn from a common distribution D. Then a choice of “optimal” parameters Θ? may be identified
as those for which the expected value of φd(xK) is minimized, where φd : H → R is an appropriate
cost function. In mathematical terms, this is expressed by stating Θ? forms a solution to the problem
minEd〜D [φd(xK(Θ, d))],	(13)
where the expectation Ed〜D is taken over all d ∈ D and We emphasize the dependence of XK on
Θ and d by writing xK = xK (Θ, d). In practice, the problem (13) is approximately solved by
using a sample set of data {dn}nN=1 taken from the distribution D and minimizing an empirical loss
function. We summarize the procedure for creating, training, and performing inferences with such
L2O networks in Algorithm 2. There M denotes the neural network, which is dependent upon Θ,
δ, and the choice of safeguarding used to construct each μk (see Line 6).
Remark 4.1. Different learning problems than (13) may be used (e.g., the min-max problem used
by adversarial networks (Goodfellow et al., 2014)).
Remark 4.2. Two particular choices of the set C are of interest. The first case is when each param-
eter ζk may be chosen independently, i.e., the network weights vary by layer. This is used in our
numerical examples below. The second case is when C is restricted such that each of its K entries
are identical, i.e., the parameters across all layers are fixed so that Z1 = Z1 = … =ZK. This latter
case corresponds to the structure of recurrent neural networks (RNNs).
5	Numerical Examples
This section presents example L2O schemes applied within the LSKM framework.1 We numeri-
cally investigate (i) the convergence rate of LSKM relative to corresponding KM iterations, (ii) the
efficacy of safeguarding procedures when inferences are performed on data for which L2O fails in-
termittently, and (iii) the convergence of LSKM schemes even when the application of TL2O is not
justified theoretically. We first use TL2O from ALISTA (Liu et al., 2019a) on the LASSO Problem.
Then we apply the differentiable linearized ADMM of Xie et al. (2019) to a similar problem. We
also implement a new L2O scheme for solving the nonnegative least squares (NNLS) problem.
In all experiments, we take μι = IlSd(X1)k2. Implementations of the LSKM Algorithm are abbre-
viated by the scheme in Table 2, e.g., we write GS(α) to mean the Geometric Sequence method
with parameter α is used to construct {μk}k∈N. As in Algorithm 2, training is completed without
safeguarding. For notational compactness, in each example we let fd? the optimal value of fd(X)
among all possible X. Note each objective function is dependent upon the data d, which implies it
it is only meaningful to compare curves below that are within the same plot (and not appropriate to
compare curves in separate plots). We illustrate the performance in plots using an approximation of
the expected objective error
Ed〜D [fd(xk) - f?] ,	(14)
using 1,000 test samples. The optimal value fd? for each sample d is estimated by running the KM
method for 15,000 iterations.
5.1	ALISTA FOR LASSO
Here we consider the popular LASSO problem for sparse coding. Consider an unknown sparse
vector X? ∈ Rn and a matrix A ∈ Rm×n, which is called the dictionary. Then assume we have
access to noisy linear measurements d ∈ Rm, where ε ∈ Rm is additive Gaussian white noise and
d = AX? + ε.	(15)
Even for underdetermined systems, when X? is sufficiently sparse andτ ∈ (0, ∞) is an appropriately
chosen regularization parameter, X? can often be recovered faithfully by solving the LASSO problem
min fd(x)：=IkAX — d∣2 + T∣∣x∣ι,	(16)
x∈Rn	2
1All of the codes for this work can be found on Github here: (link will be added after review process).
7
Under review as a conference paper at ICLR 2020
where k ∙∣∣2 and ∣∣ ∙ ∣∣ι are the '2 and '1 norms, respectively. A classic method for solving (16) is
the iterative shrinkage thresholding algorithm (ISTA) (e.g., see (Daubechies et al., 2004)), a special
case of the proximal-gradient method in Table 1. Given x1 ∈ Rn, this method iteratively computes
xk+1 := T(xk)
g/l (Xk — ；AT(Axk — d)) , for all k ∈ N,
(17)
where L = ∣AtA∣2 and ηθ is the soft-thresholding function defined by component-wise operations:
ηθ(x) := sgn(x) ∙ maχ{0, |x| — θ}.
(18)
Implementation details for the LASSO problem may be found in the Appendix.
We applied the LSKM Algorithm to the LASSO problem above by using T defined in (17) and the
update operation from ALISTA (Liu et al., 2019a). The tunable operator TL2O is parameterized by
ζ = (θ, γ) for positive scalars θ and γ and defined by
Tl2O (x； Z) ：= ηθ (x — YWT(Ax — d)) ,	(19)
with W defined in the Appendix. The parameter Θ used in Algorithm 2 is Θ = (θk, γk)kK=1, which
consists of 2K scalars. Note TL2O may fail to be averaged, depending on the choice of ζ = (θ, γ).
The primary illustration of the rapid convergence using LSKM-ALISTA relative to the KM counter-
part ISTA is in Figure 1a. There each xk estimating a solution to (16) is computed for data d drawn
from the same distribution Ds that was used to train the LSKM network. Figure 1b shows a plot
comparing performance of LSKM and KM with each d there drawn from a distribution Du that is
different than Ds. For this reason, we refer to Du as the unseen distribution. For d ∈ Du, the L2O
scheme ALISTA usually fails to converge without safeguarding whereas the safeguarded method
LSKM-ALISTA still converges and much faster than ISTA. The dotted plot with square markers
shows the percentage of safeguard activations that occurred in texting. The only two layers in Fig-
ure 1b where safeguarding kicked in were layers 2, 7 and 13, with respective percentages 36.7%,
99.6% and 99.2% among 1000 testing samples. Additionally summary results are provided in Table
3. These show the relative usefulness of difference choices of training loss function φd, safeguard-
ing method, and performance measured by different test loss functions. In particular, using φd = fd
and EMA(0.25) safeguarding yielded the lowest function value fd; however, instead using GS(0.1)
safeguarding resulted in estimates closer to the underlying sparse signals x? . In both the seen and
unseen cases, 20 iterations of LSKM-ALISTA yields better function values fd than thousands of
iterations of ISTA, which reveals orders of magnitude speedup by the safeguarded L2O schemes.
5.2	Linearized ADMM
Let A ∈ Rm×n, x? ∈ Rn, and d ∈ Rm be as in Subsection 5.1. Here we apply the L2O scheme
differentiatable linearized ADMM (D-LADMM) of Xie et al. (2019) to the closely related sparse
coding problem
min ∣Ax — d∣1 + τ ∣x∣1 .	(20)
x∈Rn
The operators defining T and TL2O for LSKM-D-LADMM are provided in the Appendix along with
further implementation details. Comparison plots are provided in Figure 2 and Table 4 summarizes
further results.
5.3	Projected Gradient for Nonnegative Least S quares
Let A ∈ Rm×n and d ∈ Rm . Here we consider the nonnegative least squares (NNLS) problem
min fd(x) := ɪkAx — d∣2 s.t. x ≥ 0.	(21)
We proceed by using the projected gradient method in Table 1, where C := {x ∈ Rn : x ≥ 0},
Vfd(x) = AT (Ax — d), and PrCjC (x) = max(x, 0), With the max applied component-wise. Then,
for α = 1/1IATA∣∣2, we take
T(x) := max (x — aAT(Ax — d), 0).	(22)
8
Under review as a conference paper at ICLR 2020
--- ISTA
→K LSKM-ALISTA
100	101	102	103	104
Iteration/Layer k
(a)	Performance on seen distribution, i.e., d 〜Ds
获; — (%) Sg?岗
630
000
111
2
lɪ
--- ISTA
-→- LSKM-ALISTA
---L2O-ALISTA
0%
%
80
%
60
40 20
%%
.qerF noitavitcA draugefaS
Iteration/Layer k
(b)	Performance on unseen distribution, i.e., d 〜Du
Figure 1:	Expected function value error versus iteration when applied to two different data distribu-
tions. Training used φd = fd. Inferences used δ = 0.01 and EMA(0.1).
We then relax T to obtain, for ζ = (α, β, W) with α ∈ R, W ∈ Rn×n, and β ∈ Rn, to obtain
Tl2o(x； Z) ：= max (X - aWT(Ax — d), β) .	(23)
In the special case that W = AT, α = 1/kATAk2, and β = 0, we recover (22). When safeguarding
does not kick in, (23) yields a familiar network structure with the max as the activation function.
Here Θ = (ζk)kK=1 = (Wk, Dk, βk)kK=1 consists of (n2 + 2n)K trainable parameters. During
training we force βK ≥ 0 to ensure xK ∈ C . However, note xk might not be in C for k < K .
In this example, each learned Wk is likely an approximation of the pseudo-inverse of A. Summary
plots are given in Figure 3 and more results in Table 5 of the Appendix.
6 Discussion
Our numerical examples provide several insights. Tables 3 to 5 in the Appendix reveal the choice of
training loss function φd results in different models and performance, even though the minimizers of
each φd are identical. This is due to the difference in gradient directions of each φd and reveals the
‘best’ choice of φd may depend on the application. Figure 1b shows the necessity of safeguarding to
ensure convergence and that inferences on unseen data can still be much more rapid than traditional
KM methods. For the space limit, we do not focus on timed results. We simply note that the relative
cost per iteration of the SKM method versus KM is roughly double in the worst case (since two
tentative update operations are needed). However, in cases like our ALISTA example, this relative
cost is greatly reduced because some computations (e.g., Axk — d) are common to both TL2O and
T. In each example, L2O schemes obtain an order of magnitude reduction in computational cost.
The results of this work reveal a practical procedure for creating and implementing L2O schemes
on new problems. Suppose one is presented a convex optimization problem and a standard general
purpose method for solving it. The method determines an averaged operator T (e.g., as in Table
9
Under review as a conference paper at ICLR 2020
21
00
11
W — (WP./Ta?岗
LADMM
.qerF snoitavitcA draugefaS
%%%%
0000%
86420
1
10
2
10
0
Iteration/Layer k
(a) Performance on seen distribution Ds
Iteration/Layer k
(b) Performance on unseen distribution, i.e., d 〜Du
Figure 2:	Expected relative function value error versus iteration when applied to two different dis-
tributions. Training used φd = fd. Inferences used δ = 0.01, and GS(0.1) in (a) and EMA(0.75) in
(b).
1). To construct TL2O, one may let each scalar, vector, and matrix that does not represent the data
d be parameterized (i.e., all of its entries learnable). We may further generalize the operations by
replacing scalar parameters with vectors and using element-wise products as in D-LADMM (see
(31)). Having T and TL2O, a network can be constructed via the LSKM algorithm. For training
the network, the learnable parameters may be initialized to the quantities that reduce TL2O to the
original operator T . Additional structures of parameters may be invoked (e.g., as done by ALISTA
with W in (26)). The number of terms to parameterize and how to structure parameterizations are
matters subject to the a practitioner’s needs/priorities for a given application.
7 Conclusion
This work establishes a framework for extending the L2O methodology to a wide class of iterative
optimization procedures (i.e., KM methods). We provide theoretical results that demonstrate se-
quences generated by our SKM method possess the property that the distance between each iterate
and fixed point set converges to zero. When there is a unique cluster point, this is equivalent to
convergence of the method to a solution. Furthermore, our LSKM algorithm provides a straightfor-
ward neural network design that inherits the theoretical properties from the SKM method. Practical
guidance is also given for constructing learnable operations for new problems. Our numerical ex-
periments demonstrate order(s) of magnitude faster convergence by LSKM implementations than
general-purpose counterparts and the efficacy of safeguarding in cases where L2O schemes would
otherwise fail to converge. Future work will provide a more efficacious fall-back method for using
data-driven updates than classic KM updates and will investigate stochastic extensions with quasi-
Fejer monotone operators.
References
Pierre Ablin, Thomas Moreau, Mathurin Massias, and Alexandre Gramfort. Learning step sizes for
unfolded sparse coding. arXiv:1905.11071, 2019.
Marcin Andrychowicz, Misha Denil, Sergio Gmez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando de Freitas. Learning to learn by gradient descent by gradient
10
Under review as a conference paper at ICLR 2020
descent. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in
Neural Information Processing Systems 29,pp. 3981-3989. Curran Associates, Inc., 2016.
Maria-Florina Balcan, Dan DeBlasio, Travis Dick, Carl Kingsford, Tuomas Sandholm, and Ellen
Vitercik. How much data is sufficient to learn high-performing algorithms? arXiv:1908.02894,
2019.
Heinz Bauschke and Patrick Combettes. Convex Analysis and Monotone Operator Theory in Hilbert
Spaace. Springer, 2nd. edition, 2017.
Heinz H. Bauschke, Francesco Iorio, and Valentin R. Koch. The Method of Cyclic Intrepid Pro-
jections: Convergence Analysis and Numerical Experiments. In Masato Wakayama, Robert S.
Anderssen, Jin Cheng, Yasuhide Fukumoto, Robert McKibbin, Konrad Polthier, Tsuyoshi Takagi,
and Kim-Chuan Toh (eds.), The Impact of Applications on Mathematics, volume 1, pp. 187-200.
Springer Japan, Tokyo, 2014.
Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimiza-
tion: a methodological tour d’horizon. arXiv:1811.06128, 2018.
Thomas Blumensath and Mike E. Davies. Iterative hard thresholding for compressed sensing. Ap-
plied and Computational Harmonic Analysis, 27(3):265-274, 2009. ISSN 1063-5203.
M. Borgerding, P. Schniter, and S. Rangan. AMP-Inspired Deep Networks for Sparse Linear Inverse
Problems. IEEE Transactions on Signal Processing, 65(16):4293-4308, 2017.
Charles L Byrne. Applied Iterative Methods. A K Peters, Ltd., 2008.
Andrzej Cegielski. Iterative Methods for Fixed Point Problems in Hilbert Spaces. Number 2057 in
Lecture Notes in Mathematics. Springer, 2012.
Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. Theoretical Linear Convergence of
Unfolded ISTA and Its Practical Weights and Thresholds. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing
Systems 31, pp. 9061-9071. Curran Associates, Inc., 2018.
Hanjun Dai, Elias B. Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning Combinatorial
Optimization Algorithms over Graphs. arXiv:1704.01665, February 2018.
I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse
problems with a sparsity constraint. Communications on Pure and Applied Mathematics, 57(11):
1413-1457, 2004.
R. Giryes, Y. C. Eldar, A. M. Bronstein, and G. Sapiro. Tradeoffs Between Convergence Speed and
Reconstruction Accuracy in Inverse Problems. IEEE Transactions on Signal Processing, 66(7):
1676-1690, 2018.
Pontus Giselsson, Mattias Falt, and Stephen Boyd. Line search for averaged operator iteration. In
2016 IEEE 55th Conference on Decision and Control (CDC), pp. 1015-1022, Las Vegas, NV,
USA, December 2016. IEEE.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Karol Gregor and Yann LeCun. Learning Fast Approximations of Sparse Coding. In Proceedings of
the 27th International Conference on International Conference on Machine Learning, ICML’10,
pp. 399-406, USA, 2010. Omnipress.
C.W Groetsch. A note on segmenting Mann iterates. Journal of Mathematical Analysis and Appli-
cations, 40(2):369-372, November 1972.
John R. Hershey, Jonathan Le Roux, and Felix Weninger. Deep Unfolding: Model-Based Inspiration
of Novel Deep Architectures. arXiv:1409.2574, 2014.
11
Under review as a conference paper at ICLR 2020
M.A. Krasnosel’skii. Two remarks about the method of successive approximations. Uspekhi Mat.
Nauk, 10:123-127, 1955.
Ke Li and Jitendra Malik. Learning to Optimize. arXiv:1606.01885, 2016.
Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional
networks and guided tree search. In Advances in Neural Information Processing Systems, pp.
539-548, 2018.
Jialin Liu, Xiaohan Chen, Zhangyang Wang, and Wotao Yin. ALISTA: ANALYTIC WEIGHTS
ARE AS GOOD AS LEARNED WEIGHTS IN LISTA. pp. 33, 2019a.
Q. Liu, X. Shen, and Y. Gu. Linearized ADMM for Nonconvex Nonsmooth Optimization With
Convergence Analysis. IEEE Access, 7:76131-76144, 2019b.
Robert Mann. Mean Value Methods in Iteration. 4(3):506-510, 1953.
Chris Metzler, Ali Mousavi, and Richard Baraniuk. Learned D-AMP: Principled Neural Network
based Compressive Image Recovery. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fer-
gus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems
30, pp. 1772-1783. Curran Associates, Inc., 2017.
Thomas Moreau and Joan Bruna. Understanding Trainable Sparse Coding with Matrix Factorization.
2017.
Simeon Reich. Weak convergence theorems for nonexpansive mappings in Banach spaces. Journal
of Mathematical Analysis and Applications, 67(2):274-276, 1979.
P. Sprechmann, A. M. Bronstein, and G. Sapiro. Learning Efficient Sparse and Low Rank Models.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(9):1821-1833, September
2015.
Sebastian Thrun and Lorien Pratt (eds.). Learning to Learn. Kluwer Academic Publishers, Norwell,
MA, USA, 1998.
Z. Wang, S. Chang, J. Zhou, M. Wang, and T. Huang. Learning A Task-Specific Deep Architecture
For Clustering. In Proceedings of the 2016 SIAM International Conference on Data Mining,
Proceedings, pp. 369-377. Society for Industrial and Applied Mathematics, June 2016a.
Zhangyang Wang, Qing Ling, and Thomas S. Huang. Learning Deep `0 Encoders. In Thirtieth AAAI
Conference on Artificial Intelligence, 2016b.
Zhangyang Wang, Ding Liu, Shiyu Chang, Qing Ling, Yingzhen Yang, and Thomas S. Huang. D3:
Deep Dual-Domain Based Fast Restoration of JPEG-Compressed Images. pp. 2764-2772, 2016c.
Zhangyang Wang, Yingzhen Yang, Shiyu Chang, Qing Ling, and Thomas S. Huang. Learning a
Deep l∞ Encoder for Hashing. In Proceedings of the Twenty-Fifth International Joint Conference
on Artificial Intelligence, IJCAI’16, pp. 2174-2180. AAAI Press, 2016d.
Xingyu Xie, Jianlong Wu, Zhisheng Zhong, Guangcan Liu, and Zhouchen Lin. Differentiable Lin-
earized ADMM. arXiv:1905.06179, 2019.
Bo Xin, Yizhou Wang, Wen Gao, David Wipf, and Baoyuan Wang. Maximal Sparsity with Deep
Networks? In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances
in Neural Information Processing Systems 29, pp. 4340-4348. Curran Associates, Inc., 2016.
Yan Yang, Jian Sun, Huibin Li, and Zongben Xu. Deep ADMM-Net for Compressive Sensing MRI.
In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 29, pp. 10-18. Curran Associates, Inc., 2016.
A Appendix
This Appendix contains a subsection with supplemental materials and a subsection with proofs of
the theoretical results.
12
Under review as a conference paper at ICLR 2020
Table 3: Summary of LASSO Problem Results on data d drawn from the seen distribution Ds .
K = 10, 000 iterations of ISTA are used by the KM reference method ‘ISTA 10K’ while K = 20
and δ = 0.01 for the LSKM schemes. ‘No guard’ refers to LSKM without safeguarding.
Method	ISTA 10K	No guard	EMA(0.1)	EMA(0.1)	EMA(0.1)	GS(0.1)	RM (3)
Loss φd	NA	fd	2 kSd(∙)kI	M(∙)kι	fd	fd	fd
Rf,Ds (xK)	7.72e-04	3.36e-04	4.65e-04	4.58e+00	3.33e-04	3.32e-04	6.68e-04
EIkS(XKiIr	2.11e-09	4.68e-07	3.75e-07	1.58e-05	4.68e-07	4.68e-07	4.95e-07
A. 1 Numerical Example Supplement Materials
Supplement to Subsection 5.1. In similar manner to (Chen et al., 2018) and (Liu et al., 2019a),
we use the following set up. We take m = 250, n = 500, and τ = 0.001. Each entry of the
dictionary A is sampled i.i.d from the standard Gaussian distribution, i.e., aj 〜N (0,1 /m). Having
these entries, we then normalize each column of A, with respect to the Euclidean norm. Each d
in the distribution Ds of data used to train the neural network is constructed using (15) with noise
ε 〜0.1∙N(0,1/m) and each entry of x? as the composition of Bernoulli and Gaussian distributions,
i.e., Xj 〜Ber(0.1) ◦N(0,1) for all j ∈ [n]. Each d in the unseen distribution Du is computed using
the same distribution of noise ε as before and using Xj 〜Ber(0.2) ◦ N(1, 2). Our data set consists
of 10,000 training samples and 1,000 test samples.
If τ is chosen too large, then the solution to the problem (16) is simply the zero vector, which is
undesirable. If τ is chosen too small, then the solution will not be sparse. Thus, appropriate choice
of τ is crucial. Since the goal of the LASSO problem is to faithfully recover the underlying sparse
vector X? used to create the measurement d, we find it reasonable to choose τ as a solution estimate
of the problem
min、Ed~D [kxK -xdk2] ,	(24)
τ ∈(0,∞)
where XdK is the output of ISTA after K = 15, 000 iterations for the problem (16) and X?d is the
sparse vector from which d was created. To simplify (24), we assume τ was of the form t = 10α
with integer α ∈ Z to obtain the problem
miyEd~D [kxK - xj∣∣2] ,	(25)
α∈Z
Approximately solving this discrete problem for our data set revealed the optimal choice yielded
α = -3andτ = 0.001.
In practice, such a choice of τ may be difficult to ascertain. However, if the practitioner has access
to each underlying xd? for their training data d, then one can circumvent the problem of choosing τ
by simply using the training loss function φd(x) := kx - x?dk22. This was precisely the approach
taken in Liu et al. (2019a).
To define the learned operations in TL2O, we let
W ∈ arg min ∣∣MTA∣∣f, s.t.(M：,')TA：,' = 1, for all ' ∈ [n],	(26)
M ∈Rm×n
where ∣∣∙∣∣f is the Frobenius norm and the Matlab notation M：,' is used to denote the 'th column of
the matrix M .
Supplement to Subsection 5.2. LADMM is used to solve problems of the form
min	f(x) + g(z) s.t. Ax + Bz = d,	(27)
x∈Rn,z∈Rm
for which LADMM generates sequences {xk}k∈N, {zk}k∈N and {νk}k∈N defined by the updates
xk+1 := proxβf (xk — βAT [νk + α (AXk + Bzk - d)]),
zk+1 := proxγg (Zk - YBT [νk + α (Axk+1 + Bzk - d)]) ,	(28)
Vk+1 := Vk + α (Axk+1 + Bzk+1 - d ,
13
Under review as a conference paper at ICLR 2020
Table 4: Summary of D-LADMM Problem Results on data d drawn from the seen distribution Ds .
K = 1, 000 iterations of LADMM are used by the KM reference method ‘LADMM 1K’ while
K = 20 and δ = 0.01 for the LSKM schemes. ‘No guard’ refers to LSKM without safeguarding.
Method	LADMM 1K	No guard	EMA(0.75)	GS(0.1)	RT
Loss φd	NA	fd	fd	fd	fd
Rf,Ds(xK)	3.48e+00	1.78e+00	1.52e+01	1.78e+00	1.28e+01
with given scalars α, β, γ ∈ (0, ∞). The problem (20) may be written in the form of (27) by taking
f = Tk ∙ kι, g = k ∙ ∣∣ι, and B = -Id. In this case, the proximal operators in (28) reduce to Soft-
thresholding operators. Although not given in Table 1, the update νk+1 is generated by applying
an averaged operator T to νk . (This follows since LADMM may expressed as a special case of
proximal ADMM, which itself is a special case of the ADMM method. The details of this derivation
are outside the scope of this paper.) This implies
∣S(νk)∣ = ∣νk - T(νk)∣ = ∣νk - νk+1∣ = α∣Axk+1 - zk+1 - d∣.	(29)
Because we compare methods that use differing choices of α, the residual comparison illustrations
presented below will, for consistency of interpretation, assume α = 1 in (29). And, although xk+1
and zk+1 form intermediate computations, for notational clarity, the term xk in the SKM and LSKM
schemes is replaced in this subsection by the tuple (xk, zk, νk). This is of practical importance too
since it is the sequence {xk}k∈N that converges to a solution of (20).
We now modify the iteration (28) for the problem (20) to create the D-LADMM L2O scheme. We
generalize soft-thresholding to vectorized soft-thresholding for β ∈ Rn by
ηβ(x) = (ηβ1 (x1), ηβ2(x2), . . . , ηβn(xn)).	(30)
We assume ηβ represents the scalar soft-thresholding in (18) when β ∈ R and the vector general-
ization (30) when β ∈ Rn. Combining ideas from ALISTA (Liu et al., 2019a) and D-LADMM	(Liu
et al., 2019b),	given (xk, zk, νk) ∈ Rn × Rm × Rm, αk, γk, ξk ∈ Rm, βk, σk ∈ Rn,	W1	∈	Rn×m,
and W2 ∈ Rm×m, set
Xk+1 := ηβk (Xk - σk ◦ (Wk)T [νk + αk O(Axk - zk - d)]),
Zk+1 := ηγk (zk - ξk o (Wf)T [νk + αk o (Axk+1 - Zk - d)]) ,	(31)
Vk+1 := Vk + αk O(Axk+1 - Zk+1 - d ,
with element-wise products denoted by o. For the parameter ζk := (αk, βk, γk, σk,ξk,W1k,W2k),
then define
TL2O(xk,zk, Vk; Zk) := (xk+1, Zk+1, i>k+1).	(32)
Fixing the number of iterations K, the learnable parameters from (31) used in the LSKM Algorithm
may be encoded by Θ = (Zk)3ι = (αk,βk, Yk, σk, ξk, Wk, Wk)K=ι, consisting of (2n + 3m +
mn + m2 )K scalars.
For data generation, we follow the same settings as in the experiments of Subsection 5.1. Although
the value of τ is not optimally chosen, this was used as it worked well for D-LADMM.
Supplement to Subsection 5.3. We take m = 500, n = 250, aj 〜Ber(0.1) ∙ rand[0,1]. We use
noise ε 〜0.1 ∙ N(0,1/m). Each d 〜Ds used for training the neural network is sampled using d =
Ax? + ε with x 〜 maχ(N(0,1), 0). For unseen data d 〜Du We sample x?〜 maχ(N(5,102), 0).
We sample 10,000 training samples from Ds to train the neural network and 1,000 samples from Ds
and Du , respectively, for testing.
A.2 Proofs
For ease of reference, we restate Theorem 3.1 below. Then provide a proof is provided for it.
Theorem 3.1. If {xk}k∈N is a sequence generated by the SKM method and Assumptions 1 to 3
hold, then
lim dFiχ(τ)(xk) = 0.	(33)
k→∞
14
Under review as a conference paper at ICLR 2020
记/ I (WP岗
--- PG
→K LSKM-PG
--- PG
LSk LSKM-PG
---L2O-PG
Iteration/Layer k
(a) Performance on seen distribution Ds
Iteration/Layer k
(b) Performance on unseen distribution, i.e., d 〜Du
Figure 3: Expected function value error versus iteration when applied to two different data distribu-
tions. Training used φd = fd. Inferences used δ = 0.01 and GS(0.1).
Table 5: Summary of NNLS Problem Results on data d drawn from the seen distribution Ds . K =
1, 500 iterations ofPG are used by the KM reference method ‘PG 1.5K’ while K = 20 and δ = 0.01
for the LSKM schemes. ‘No guard’ refers to LSKM without safeguarding.
Method	PG 1.5K	No guard	EMA(0.75)	GS(0.1)	RT
Loss φd	NA	fd	fd	fd	fd
Rf,Ds(xK)	3.80e+06	1.86e+06	2.37e+07	1.86e+06	1.65e+08
And, if {xk}k∈N contains a single cluster point, then {xk}k∈N converges to a point x? ∈ Fix(T).
Proof. If the inequality in Line 6 holds finitely many times, then there exists an index beyond
which Line 9 is always used to update xk . In this case, for large k the SKM Method takes the
form of the classic KM Method, which is known to converge (e.g., see (Cegielski, 2012, Theorem
3.7.1), (Groetsch, 1972, Corollary 3), and (Bauschke & Combettes, 2017, Theorem 5.15)). Thus, it
is sufficient to only consider the case where the inequality in Line 6 holds infinitely many times.
We proceed by first showing the sequence {xk}k∈N is bounded (Step 1). This is used to prove
{xk}k∈N has a cluster point in Fix(T) (Step 2). Results from these steps are then applied to obtain
the desired limit (33) (Step 3).
Step 1: By Assumption 2, there exists R ∈ (0, ∞) sufficiently large to ensure
kxk > R =⇒ kS(x)k > 1, for all x ∈ H.	(34)
Equivalently, we may write
kS (x)k ≤ 1 =⇒	kxk ≤ R, for all x ∈ H.	(35)
By Assumption 3, there also exists N1 ∈ N such that
μk ≤ 1, for all k ≥ Ni.	(36)
Fix any z ∈ Fix(T ). We claim
Ilxk 一 ZIl ≤ max {2R, kx' _ zk}, for all k ∈ N.	(37)
'∈[Nι]
The result (37) holds trivially for all k ∈ [N1]. Proceeding by induction, suppose (37) holds for
some k > N1. If the inequality in Line 6 holds, then (35), (36) and the update formula in Line 7
together imply Ixk+1 I ≤ R. Since IS(z)I = 0, (35) also implies IzI ≤ R. Thus,
kxk+1 - zk ≤ kxk+1k + kzk ≤ 2R ≤ max {2R, ∣∣x' - z∣∣}.	(38)
15
Under review as a conference paper at ICLR 2020
If instead the update in Line 9 is applied, the averagedness of T implies there is α ∈ (0, 1) such that
kxk+1 - zk2 ≤ kxk - zk2 - 1-αkS(xk)k2	(39)
α
(e.g., see Prop. 4.35 in (Bauschke & Combettes, 2017) or Cor. 2.2.15 and Cor. 2.2.17 in (Cegielski,
2012)), and so
kxk+1 一 Zk ≤ Ilxk 一 Zk ≤ max {2R, kx' _ zk}.	(40)
'∈[Nι]
Equations (38) and (40) together close the induction, from which (37) follows. Whence
kxkk ≤ kxk — zk + kzk ≤ max {2R, kx' — zk} + R, for all k ∈ N,	(41)
'∈[Nι]
which verifies the sequence {xk}k∈N is bounded.
Step 2: Because the inequality in Line 6 holds infinitely many times, there exists a subsequence
{xqk }k∈N ⊆ {xk}k∈N satisfying
0 ≤ lim kS(xqk)k ≤ lim (1 - δ)μk = 0,	(42)
k→∞	k→∞
from which the squeeze theorem asserts kS(xqk )k → 0. Since {xk}k∈N is bounded, so also is
{xqk}k∈N. Thus, there exists a subsequence {x'k}k∈N ⊆ {xqk }k∈N converging to a limit P ∈ H.
Then applying the fact S is 2-Lipschitz and ∣∣T∣ is continuous yields
0 = lim ∣∣S(x'k)k = S I lim x'k )11 = ∣∣S(p)k =⇒ P ∈ Fix(T).	(43)
k→∞	k→∞
That is, {xk}k∈N contains a cluster point P ∈ Fix(T).
Step 3: Let ε > 0 be given. Following (Bauschke et al., 2014, Def. 2), define the ε-enlargement
Fix(T)[ε] := {x ∈ H : dFix(T) (x) ≤ ε}.	(44)
Note Fix(T)[ε] is a nonempty closed and bounded subset ofH. Set
C := (B(0, R)- FiX(T)[ε∕2]) ∪ ∂Fix(T)归/2],	(45)
where B(0, R) is the closed ball of radius R centered at the origin. Because H is finite dimensional
and C is closed and bounded, C is compact. Thus, every continuous function obtains its infimum
over C. In particular, we may set
ζ = min kS(x)k.	(46)
x∈C
Note Z > 0 since C ∩ Fix(T) = 0. Consequently, letting Z := min{1, Z∕2} yields
，，一.，、，， ~ „. 一、 - ，、 . … _. ...
∣∣S(x)k ≤ Z =⇒ X ∈ Fix(T)[ε]	=⇒ dpix(τ)(x) ≤ ε, for all X ∈ H,	(47)
where the first implication holds because x ∈ B(0, R) by (35) and x ∈/ C by (46). By Assumption
3, there exists N2 ∈ N such that
μk ≤ Z, for all k ≥ N2.	(48)
By the result of Step 2, there exists N3 ≥ N2 such that
kxN3 - Pk ≤ ε =⇒ dFix(T) (xN3 ) ≤ ε.	(49)
We claim
dFix(T)(xk) ≤ ε, for all k ≥ N3,	(50)
which, by the arbitrariness ofε, implies (33) holds. Indeed, inductively suppose (50) holds for some
k ≥ N3. If the inequality in Line 6 holds, then (47) and (48) together imply
kS(xk+1)k≤ (1 - δ)μk ≤ Z =⇒ dFiχ(τ)(xk+1) ≤ ε.	(51)
Otherwise, letting P : H → H be the projection operator onto Fix(T), we deduce
kxk+1 - P (xk+1)k ≤kxk+1-P(xk)k ≤ kxk-P(xk)k=dFix(T)(xk) ≤ε, (52)
16
Under review as a conference paper at ICLR 2020
where the second inequality follows from (39), taking z = P (xk). Note the left hand side of (52)
equals the distance from xk+1 to Fix(T). Therefore, (51) and (52) close the induction in each case,
and (33) holds.
The limit (33) can be used in a similar manner to the work in Step 2 above to prove each cluster point
of {xk}k∈N is in Fix(T). Thus, if {xk}k∈N admits a unique cluster point, then the entire sequence
converges to a point x? ∈ Fix(T).	□
We restate Corollary 3.1 below and then provide a proof.
Corollary 3.1. If {xk }k∈N is a sequence generated by the SKM method and Assumptions 1 and 2
hold and {μk }k∈N is generated using a scheme outlined in Table 2, then Assumption 3 holds and,
by Theorem 3.1, the limit (33) holds.
Proof. The proof is parsed into four parts, one for each particular choice of the sequence {μk}k∈N
in Table 2, where we note “Recent Term” is a special case of “Recent Max” obtained by taking
m = 1. Each proof part is completely independent of the others and is separated by italic text.
However, to avoid excessive writing, in each section let Γ ⊆ N be the set of all indices for which the
descent condition in Line 6 holds, the sequence {tk}k∈N be an ascending enumeration of Γ, mk be
the number of times the descent condition has been satisfied by iteration k, and μι ∈ (0, ∞).
Geometric Seqeunce. Define the sequence {μk }k∈N using, for each k ∈ N, the update formula
[	μk	ifLine6holds,
μk+ι =	(53)
(1 - δ)μk otherwise.
This implies
μk = (1- δ)mkμι.	(54)
Since Γ is infinite, lim mk = ∞, and it follows that
k→∞
lim μk = lim (1 — δ)mkμι = 0 ∙ μι = 0,	(55)
k→∞	k→∞
i.e., Assumption 3 holds.
Arithmetic Average. Define the sequence {μk }k∈N using, for each k ∈ N, the update formula
∖ ——--- (IlS(Xk+1)k + mkμk) ifLine6 holds
μk+1 :=	mk + 1
I	μk	otherwise.
Then observe
0 ≤ μtk+1 ≤	=(1—m⅛) μtk ≤ μtk i也k ∈ n∙
(57)
Since μk+ι = μk whenever k ∈ Γ, (57) shows {μk }k∈N is monotonically decreasing. Consequently,
using induction reveals
0 ≤ μtk -
δ	δμt'
—fμtk ≤ μ1 -	~Γ
mtk + 1	'=1 mt` + 1
μι
k
-X
'=1
δμt'
'+1
for all k ∈ N,
(58)
where we note mt` = ` in the sum since m` increments once each time a modification occurs in the
sequence {μk }k∈N. By way of contradiction, suppose there exists α ∈ (0, ∞) such that
liminf μk ≥ α > 0.	(59)
k→∞
Then (58) implies
kk
X 鼻 ≤ X '+t1 ≤ μι, forallk ∈ N.	(6O)
17
Under review as a conference paper at ICLR 2020
However, the sum on the left hand side becomes a divergent harmonic series as k → ∞, contradict-
ing the finite upper bound on the right hand side. This contradiction proves assumption (59) is false,
from which it follows that
liminf μk = 0.	(61)
k→∞
By the monotone convergence theorem, We deduce μk → 0, i.e., Assumption 3 holds.
Exponential Moving Average. Given θ ∈ (0, 1), for all k ∈ N, define
:_	J	θ∣∣S(xk+1)k	+ (1	- θ)μk-ι	if Line 6 holds,
μk+1 :	[	μk	otherwise.
(62)
Now observe
μtk+ι = θkS(Xtk+1)k + (1 - θ)μtk ≤ θ(I- δ)μtk + (1 - θ)μtk = (1 - θδ)μtk.	(63)
This shows the sequence {μk}k∈N is nonincreasing and, when a decrease does occur, it is by a
geometric factor of the current iterate. Through induction, it follows that
μk ≤ (1 - θδ)mkμι, for all k ∈ N.	(64)
Since Γ is infinite, lim mk = ∞. This, combined with the fact (1 - θδ) ∈ (0, 1), implies
k→∞
0 ≤ lim μk ≤ lim (1 — θδ)mkμι = 0 ∙ μι = 0,	(65)
k→∞	k→∞
from which the squeeze theorem asserts Assumption 3 holds.
Recent Max. Let m ∈ N. Set Ξk to be the set of the most recent min{m, k} indices for which the
descent condition in Line 6 held, where {μk }k∈N is defined, for all k ∈ N, by the update formula
μk+ι = (mΞXkS(x')k
μk
if Line 6 holds,
otherwise.
(66)
The sequence {μk }k∈N is monotonically decreasing since the inequality in Line 6 implies, each time
anew term kS(xk)k is introduced so that kS(xk)k ∈ Ξk+1, the new term is no larger than the largest
term in Ξk .
All that remains is to show this sequence converges to zero. By way of contradiction,
suppose there exists α ∈ (0, ∞) such that
liminf μk = α > 0.
k→∞
Then choose
δα
ε =----------
2(1 - δ),
which implies
(1 - δ)(α + ε) < α.
By (67) and the fact Γ is infinite, there exists N ∈ N with N > m such that
kμtN - α∣ < ε	=⇒	μtR < α + ε∙
(67)
(68)
(69)
(70)
Then note each new element to Ξk is no larger than (1 - δ)μtW. And, for any k after m such
replacements occur, it follows that
μk = max ∣∣S(x')k ≤ (1 - δ)μtN ≤ (1 - δ)(α + ε) <α,	(71)
a contradiction to (67). This contradiction shows our assumption (67) must be false, and so
liminf μk = 0.	(72)
k→∞
By the monotone convergence theorem, we conclude Assumption 3 holds.	□
18