Under review as a conference paper at ICLR 2020
Optimal Attacks on Reinforcement Learning
Policies
Anonymous authors
Paper under double-blind review
Ab stract
Control policies, trained using the Deep Reinforcement Learning, have been re-
cently shown to be vulnerable to adversarial attacks introducing even very small
perturbations to the policy input. The attacks proposed so far have been designed
using heuristics, and build on existing adversarial example crafting techniques
used to dupe classifiers in supervised learning. In contrast, this paper investigates
the problem of devising optimal attacks, depending on a well-defined attacker’s
objective, e.g., to minimize the main agent average reward. When the policy and the
system dynamics, as well as rewards, are known to the attacker, a scenario referred
to as a white-box attack, designing optimal attacks amounts to solving a Markov
Decision Process. For what we call black-box attacks, where neither the policy nor
the system is known, optimal attacks can be trained using Reinforcement Learning
techniques. Through numerical experiments, we demonstrate the efficiency of
our attacks compared to existing attacks (usually based on Gradient methods).
We further quantify the potential impact of attacks and establish its connection
to the smoothness of the policy under attack. Smooth policies are naturally less
prone to attacks (this explains why Lipschitz policies, with respect to the state, are
more resilient). Finally, we show that from the main agent perspective, the system
uncertainties and the attacker can be modelled as a Partially Observable Markov
Decision Process. We actually demonstrate that using Reinforcement Learning
techniques tailored to POMDP (e.g. using Recurrent Neural Networks) leads to
more resilient policies.
1	Introduction
Advances in Deep Reinforcement Learning (RL) have made it possible to train end-to-end policies
achieving superhuman performance on a large variety of tasks, such as playing Atari games Mnih et al.
(2013; 2016), playing Go Silver et al. (2016; 2017), as well as controlling systems with continuous
state and action spaces Lillicrap et al. (2016); Schulman et al. (2015); Levine et al. (2016). Recently,
some of these policies have been shown to be vulnerable to adversarial attacks at test time see e.g.
Huang et al. (2017); Pattanaik et al. (2018). Even if these attacks only introduce small perturbations
to the successive inputs of the policies, they can significantly impact the average collected reward by
the agent.
So far, attacks on RL policies have been designed using heuristic techniques, based on gradient
methods, essentially the Fast Gradient Sign Method (FGSM). As such, they do not explicitly aim at
minimizing the reward collected by the agent. In contrast, in this paper, we investigate the problem
of casting optimal attacks with well-defined objectives, for example minimizing the average reward
collected by the agent. We believe that casting optimal attacks is crucial when assessing the robustness
of RL policies, since ideally, the agent should learn and apply policies that resist any possible attack
(of course with limited and reasonable amplitude).
For a given policy learnt by the agent, we show that the problem of devising an optimal attack can
be formulated as a Markov Decision Process (MDP), defined through the system dynamics, the
agent’s reward function and policy. We are mainly interested in black-box attacks: to devise them,
the adversary only observes the variables from which she builds her reward. For example, if the
objective is to lead the system to a certain set of (bad) states, the adversary may just observe the states
as the system evolves. If the goal is to minimize the cumulative reward collected by the agent, the
1
Under review as a conference paper at ICLR 2020
adversary may just observe the system states and the instantaneous rewards collected by the agent. In
black-box attacks, the aforementioned MDP is unknown, and optimal attacks can be trained using RL
algorithms. The action selected by the adversary in this MDP corresponds to a perturbed state to be
fed to the agent’s policy. As a consequence, the action space is typically very large. To deal with this
issue, our optimal attack is trained using DDPG Lillicrap et al. (2016), a Deep RL algorithm initially
tailored to continuous action space.
We train and evaluate optimal attacks for some OpenAI Gym Brockman et al. (2016) environments
with discrete action spaces (discrete MountainCar, Cartpole, and Pong), and continuous state-action
spaces (continuous MountainCar and continuous LunarLander). As expected, optimal attacks outper-
form existing attacks.
We discuss how the methods used to train RL policies impact their resilience to attacks. We show that
the damages caused by attacks are upper bounded by a quantity directly related to the smoothness
of the policy under attack1. Hence, training methods such as DDPG leading to smooth policies
should be more robust. Finally, we remark that when under attack, the agent faces an environment
with uncertain state feedback, and her sequential decision problem can be modelled as a Partially
Observable MDP (POMDP). This suggests that training methods specifically tailored to POMDP
(e.g. DRQN Hausknecht & Stone (2015)) result in more resistant policies. These observations are
confirmed by our experiments.
2	Related Work
Adversarial attacks on RL policies have received some attention over the last couple of years. As for
now, attacks concern mainly the system at test time, where the agent has trained a policy using some
Deep RL algorithm (except for Behzadan & Munir (2017), which considers an attack during training).
The design of these attacks Huang et al. (2017); Pattanaik et al. (2018); Lin et al. (2017) are inspired
by the Fast Gradient Sign Method (FGSM) Goodfellow et al. (2015), which was originally used to
fool Deep Learning classifiers. Fast Gradient Methods (FGMs) are gradient methods that changes the
input with the aim of optimizing some objective function. This function, and its gradient, are sampled
using observations: in case of an agent trained using Deep RL, this function can be hence related to
a distribution of actions given a state (the actor-network outputs a distribution over actions), or the
Q-value function (the critic outputs approximate Q-values). In Huang et al. (2017), FGM is used to
minimize the probability of the main agent selecting the optimal action according to the agent. On
the other hand, in Pattanaik et al. (2018), the authors develop a gradient method that maximizes the
probability of taking the action with minimal Q-value. The aforementioned attacks are based on the
unperturbed policy of the agent, or its Q-value function, and do not consider optimizing the attack
over all possible trajectories. Therefore, they can be considered a first-order approximation of an
optimal attack. If the adversary wishes to minimize the average cumulative reward of the agent, she
needs to work on the perturbed agent policy (to assess the impact of an attack, we need to compute
the rewards after the attack).
Proactive and reactive defence methods have been proposed to make RL policies robust to gradient
attacks. Proactive methods are essentially similar to those in supervised learning, i.e., they are based
on injecting adversarial inputs during training Kos & Song (2017); Pattanaik et al. (2018). A few
reactive methods have also been proposed Lin et al. (2017). There, the idea is to train a separate
network predicting the next state. This network is then used to replace the state input by the predicted
state if this input is believed to be too corrupted. But as we show in the paper, adversarial examples
introduce partial observability, and the problem is not Markov anymore. It is known Singh et al.
(1994) that deterministic memory-less policies are inadequate to deal with partial observability, and
some kind of memory is needed to find a robust policy. In this work, we make use of recurrent neural
networks to show that we can use techniques from the literature of Partially Observable MDPs to
robustify RL policies.
It is finally worth mentioning that there has been some work studying the problem of robust RL with
a game-theoretical perspective Morimoto & Doya (2005); Pinto et al. (2017). However, these papers
1A policy is smooth when the action it selects smoothly varies with the state. Refer to proposition 5.1 for
details.
2
Under review as a conference paper at ICLR 2020
consider a very different setting where the adversary has a direct impact on the system (not indirectly
through the agent as in our work). In a sense, they are more related to RL for multi-agent systems.
3	Preliminaries
This section provides a brief technical background on Markov Decision Process and Deep Reinforce-
ment Learning. The notions and methods introduced here are used extensively in the remainder of
the paper.
Markov Decision Process. A Markov Decision Process (MDP) defines a discrete-time controlled
Markov chain. It is defined by a tuple M = hS, (As, s ∈ S), P, p0, qi. S is the finite state space,
As is the finite set of actions available in state s, P represents the system dynamics where P(∙∣s,α)
is the distribution of the next state given that the current state is s and the selected action is a, p0
is the distribution of the initial state, and q characterizes rewards where q(∙∣s, a) is the distribution
of the collected reward in state is s when action a is selected. We denote by r(s, a) the expectation
of this reward. We assume that the average reward is bounded: for all (s, a), |r(s, a)| ≤ R. A
policy ∏ for M maps the state and to a distribution of the selected action. For a ∈ As, ∏(a∣s)
denotes the probability of choosing action a in state s. The value function V π of a policy π defines
for every s the average cumulative discounted reward under π when the system starts in state s:
Vπ(S) = E[Pt≥o YtIr(S,a∏)∣so = s] where s∏ and a∏ are the state and the selected action at
time t under policy π, and γ ∈ (0, 1) is the discount factor. The Q-value function Qπ of policy
π maps a (state, action) pair (s, a) to the average cumulative discounted reward obtained starting
in state s when the first selected action is a and subsequent actions are chosen according to π:
Qπ(s, a) = r(s, a) + γEs,~p(∙∣s,a)[Vπ (s0)]. For a given MDP M, the objective is to devise a policy
with maximal value in every state.
Deep Reinforcement Learning To train policies with maximal rewards for MPDs with large state
or action spaces, Deep Reinforcement Learning consists in parametrizing the set of policies or some
particular functions of interest such as the value function or the Q-value function of a given policy
by a deep neural network. In this paper, we consider various Deep RL techniques to train either the
policy of the agent or the attack. These include:
DQN Mnih et al. (2013): the network aims at approximating the Q-function of the optimal policy.
DDPG Lillicrap et al. (2016): an actor-critic method developed to deal with continuous state-action
spaces. To this aim, the actor returns a single action rather than a distribution over actions.
DRQN Hausknecht & Stone (2015): introduces memory in DQN by replacing the (post-convolutional)
layer by a recurrent LSTM. The use of this recurrent structure allows us to deal with partial observ-
ability, and DRQN works better with POMDPs.
4	Optimal Adversarial Attacks
To attack a given policy π trained by the main agent, an adversary can slightly modify the sequence
of inputs of this policy. The changes are imposed to be small, according to some distance, so that the
attack remains difficult to detect. The adversary aims at optimizing inputs with a precise objective in
mind, for example, to cause as much damage as possible to the agent.
We denote by M = hS, (As, s ∈ S), P,p0, qi the MDP solved by the agent, i.e., the agent policy π
has been trained for M.
4.1	The attack MDP
To attack the policy π the adversary proceeds as follows. At time t, the adversary observes the system
state st. She then selects a perturbed state st, which becomes the input of the agent policy ∏. The
agent hence chooses an action according to ∏(∙∣st). The adversary successively collects a random
reward defined depending on her objective, which is assumed to be a function of the true state and
the action selected by the agent. An attack is defined by a mapping φ : S → S where ss = φ(s) is the
perturbed state given that the true state is s.
3
Under review as a conference paper at ICLR 2020
Constrained perturbations. For the attack to be imperceptible, the input should be only slightly
modified. Formally, we assume that we can define a notion of distance d(s, s0) between two states
s, s0 ∈ S . The state space of most RL problems can be seen as a subset of a Euclidean space, in which
case d is just the Euclidean distance. We impose that given a state s of the system, the adversary can
only select a perturbed state S in As = {s ∈ S : d(s, s) ≤ e}. e is the maximal amplitude of the state
perturbation.
System dynamics and agent’s reward under attack. Given that the state is st = s at time t
and that the adversary selects a modified state sS ∈ ASs , the agent selects an action according
to the distribution ∏(∙∣s). Thus, the system state evolves to a random state with distribution
Pn(∙∣s,S) := Pa π(a∣S)P(∙∣s, a). The agent instantaneous reward is a random variable with
distribution Pa π(a∣S)q(∙∣s, a).
Adversary’s reward. The attack is shaped depending on the adversary’s objective. The adversary
typically defines her reward as a function of the true state and the action selected by the agent, or
more concisely as a direct function of the reward collected by the agent. The adversary might be
interested in guiding the agent to some specific states. In control systems, the adversary may wish
to induce oscillations in the system output or to reduce its controllability (this can be realized by
choosing a reward equal to the energy spent by the agent, i.e., proportional to a2 if the agent selects a).
The most natural objective for the adversary is to minimize the average cumulative reward collected
by the agent. In this case, the adversary would set her instantaneous reward equal to the opposite of
the agent’s reward.
We denote by qπ (∙∣s, s) the distribution of the reward collected by the adversary in state S when she
modifies the input to sS, and by rSπ(s, sS) its expectation. For example, when the adversary wishes to
minimize the agent,s average cumulative reward, we have Sn(s, S) = - Pa π(a∣S)r(s, a).
We have shown that designing an optimal attack corresponds to identifying a policy φ that solves the
following MDP: MS π = hS, (ASs, s ∈ S), PSπ,p0, qSπi. When the parameters of this MDP are known
to the adversary, a scenario referred to as white box attack, finding the optimal attack accounts to
solving the MDP, e.g., by using classical methods (value or policy iteration). More realistically, the
adversary may ignore the parameters of MS , and only observe the state evolution and her successive
instantaneous rewards. This scenario is called black-box attack, and in this case, the adversary can
identify the optimal attack using Reinforcement Learning algorithms.
For ease of notation, we will denote the value of the adversarial policy φ as V φ (for the MDP MS π),
even though it depends on π . Finally, observe that when the adversarial attack policy is φ, then the
system dynamics and rewards correspond to a scenario where the main agent applies the perturbed
policy π ◦ φ, which is defined by the distributions (π ◦ φ)(∙∣s) := EM〜φ(s)[π(∙∣S)], s ∈ S. Hence, we
can denote the value of the perturbed policy as Vπ°φ (for the MDP M).
4.2	Minimizing agent’ s average reward
Next, we give interesting properties of the MDP MS π . When the adversary aims at minimizing the
average cumulative reward of the agent, then the reward collected by the agent can be considered as
a cost by the adversary. In this case, Sn(s, s) = — Pa ∏(a∣s)r(s, a), and one can easily relate the
value function of an attack policy φ to that of the agent policy π ◦ φ:
Vφ(s) = -Vπcιφ(s), ∀s ∈ S.	(1)
The Q—value function ofan attack policy φ can also be related to the Q—value function of the agent
policy under attack π ◦ φ. Indeed: for all s, sS ∈ S.
Qφ(s, S) = -Sπ(s, S) + YEso〜P∏(∙∣s"vφ(s0)],
=-Ea〜π(,⑸ hr(s, a) + YEs0〜P(∙∣s,a)[V π°φ(s0)]] = -Ea 〜∏(同[Qπ°φ(s,a)].
In particular, if the agent policy is deterministic (π(s) ∈ As is the action selected under π in state s),
we simply have:
Qφ(s,s) = -Qπ°φ(s,π(S)), ∀s,S ∈S.	(2)
Equation (2) has an important consequence when we train an attack policy using Deep Learning
algorithms. It implies that evaluating the Q-value of an attack policy φ can be done by evaluating
4
Under review as a conference paper at ICLR 2020
the Q-value function of the perturbed agent policy π ◦ φ. In the case of off-policy learning, the
former evaluation would require to store in the replay buffer experiences of the type (s, s, r) (r is the
observed reward), whereas the latter just stores (s,∏(s),r) (but it requires to observe the actions of
the agent). This simplifies considerably when the state space is much larger than the action space.
4.3 Training optimal attacks
Training an optimal attack can be very complex, since it corresponds to solving an MDP where the
action space size is similar to that of the state space. In our experiments, we use two (potentially
combined) techniques to deal with this issue: (i) feature extraction when this is at all possible, and (ii)
specific Deep RL techniques tailored to very large (or even continuous) state and action spaces, such
as DDPG.
Feature extraction. One may extract important features of the system admitting a low-dimensional
representation. Formally, this means that by using some expert knowledge about the system, one can
identify a bijective mapping between the state s in S , and its features z in Z of lower dimension. In
general, this mapping can also be chosen such that its image of the ball As is easy to represent in Z
(typically this image would also be a ball around z, the feature representation of s). It is then enough
to work in Z . When Z is of reasonable size, one may then rely on existing value-based techniques
(e.g. DQN) to train the attack. An example where features can be easily extracted is the Atari game
of pong: the system state is just represented by the positions of the ball and the two bars. Finally,
we assume to be very likely that an adversary trying to disrupt the agent policy would conduct an
advanced feature engineering work before casting her attack.
Deep RL: DDPG. In many systems, it may not be easy to extract interesting features. In that case,
one may rely on Deep RL algorithms to train the attack. The main issue here is the size of the action
space. This size prevents us to use DQN (that would require to build a network with as many outputs
as the number of possible actions), but also policy-gradient and actor-critic methods that parametrize
randomized policies such as TRPO (indeed we can simply not maintain distributions over actions).
This leads us to use DDPG, an actor-critic method that parametrizes deterministic policies. In DDPG,
we consider policies parametrized by ω (φω is the policy parametrized by ω), and its approximated
Q—value function Qθ parametrized by θ. We also maintain two corresponding target networks. The
objective function to maximize is J(ω) = Es〜p[ [Vφω (s)]. φ is updated using the following gradient
Silver et al. (2014):
Vω J (ω) = Es 〜ρφe [▽@ Qθ (s,φω (s))Vω φω (s)],	(3)
where φe corresponds to φω with additional exploration noise, and ρφe denotes the discounted state
visitation distribution under the policy φe. In the case where the adversary’s objective is to minimize
the average reward of the agent, in view of (2), the above gradient becomes:
Vω J (ω) = -Es 〜ρφe [Va Qθ (s,∏(φω (S)))V s∏(φω (s))Vω φω (s)]∙	(4)
Finally, to ensure that the selected perturbation computed by φω belongs to As, we add a last fixed
layer to the network that projects onto the ball centred in 0, with radius . This is done by applying
the function X → min(1, 祢)X to X + e, where X is the output before the projection layer, and e is
the exploration noise. After this projection, we add the state s to get φe(s).
The pseudo-code of the algorithm is provided in Algorithm 1 in the Appendix. Algorithm 2 presents
a version of the algorithm using the gradient (4). A diagram of the actor and critic networks are also
provided in Figure 8 of the Appendix.
Gradient-based exploration. In case the adversary knows the policy π of the main agent, we can
leverage this knowledge to tune the exploration noise e. Similarly, to gradient methods Huang
et al. (2017); Pattanaik et al. (2018), we suggest using the quantity VsJ(∏(∙∣s), y) to improve the
exploration process , where J is the cross-entropy loss between ∏(∙∣s) and a one-hot vector y that
encodes the action with minimum probability in state s. This allows exploring directions that might
minimize the value of the unperturbed policy, which boosts the rate at which the optimal attack is
learnt. At time t, the exploration noise could be set to e0t , a convex combination of a white exploration
noise et and f (St) = g(-Vs J(∏(∙∣st), yt)), where g is a normalizing function. Following this
gradient introduces bias, hence we randomly choose when to use et or e0t (more details can be found
in the Appendix).
5
Under review as a conference paper at ICLR 2020
5	Robustness of Policies
In this section, we briefly discuss which training methods should lead to agent policies more resilient
to attacks. We first quantify the maximal impact of an attack on the cumulative reward of the agent
policy, and show that it is connected to the smoothness of the agent policy. This connection indicates
that the smoother the policy is, the more resilient it is to attacks. Next, we show that from the agent
perspective, an attack induces a POMDP. This suggests that if training the agent policy using Deep
RL algorithms tailored to POMDP yields more resilient policies.
Impact of an attack and policy smoothness. When the agent policy π is attacked using φ, one may
compute the cumulative reward gathered by the agent. Indeed, the system evolves as if the agent
policy was ∏ ◦ φ. The corresponding value function satisfies the Bellman equation: Vπ◦φ(s)=
Ea~∏oφ(∙∣s) [r(s, a) + γEs0~p(∙∣s,a)[Vπ°φ(s0)]]. Starting from this observation, we derive an upper
bound on the impact of an attack (see the Appendix for a proof):
Proposition 5.1. Forany S ∈ S, let2 α∏,ε(s) = max≤∈As ∣∣∏(∙∣s) 一 ∏(•恒)kτv. We have:
kVπ 一 V π°φ∣∣∞ ≤ 2 kαπ,εk∞ (R+YkV π∣∞).
1一 γ
(5)
Assume now that π is smooth in the sense that, for all s,s0 ∈ S, k∏(∙∣s) 一 ∏(∙∣s0)kτv ≤ Ld(s,s0),
then
Lε
kVπ - V π.φk∞ ≤2厂(R+YkV π k∞).
(6)
In the above, απ,ε (s) quantifies the potential impact of applying a feasible perturbation to the state s
on the distribution of actions selected by the agent in this state (note that it decreases to 0 as ε goes to
0). The proposition establishes the intuitive fact that when the agent policy is smooth (i.e., varies
smoothly with the input), it should be more resilient to attacks. Indeed, in our experiments, policies
trained using Deep RL methods known to lead to smooth policies (e.g. DDPG for RL problems with
continuous state and action spaces) resist better to attacks.
Induced POMDP and DRQN. When the agent policy is under the attack φ, the agent perceives the
true state s only through its perturbation φ(s). More precisely, the agent observes a perturbed state
S only, with probability O(引S) = l{φ(s)=s} (in general O could also be a distribution over states
if the attack is randomized). Therefore, an optimall perturbation policy φ induces a deterministic
POMDP. It is known Spaan (2012); Astrom (1965); Singh et al. (1994) that solving POMDPs requires
"memory", i.e., policies whose decisions depend on the past observations (non-Markovian). Hence,
we expect that training the agent policy using Deep RL algorithms tailored to POMDP produces
more resilient policies. In Hausknecht & Stone (2015) they empirically demonstrate that recurrent
controllers have a certain degree of robustness against missing information, even when trained with
full state information. We use this to pro-actively extract robust features, not biased by a specific
adversary policy φ, as in adversarial training. This is confirmed in our experiments, comparing the
impact of attacks on policies trained by DRQN to those trained using Deep RL algorithms without
recurrent structure (without memory).
6	Experimental Evaluation
We evaluate our attacks on four OpenAI Gym Brockman et al. (2016) environments (A: discrete
MountainCar, B: Cartpole, C: continuous MountainCar, and D: continuous LunarLander), and on E:
Pong, an Atari 2600 game in the Arcade Learning Environment Bellemare et al. (2013). In Appendix,
we also present the results of our attacks for the toy example of the Grid World problem, to illustrate
why gradient-based attack are sub-optimal.
Agent’s policies. The agent policies are trained using DQN or DRQN in case of discrete action spaces
(environments A, B, and E), and using DDPG when the action space is continuous (environments C
and D). In each environment and for each training algorithm, we have obtained from 3 policies for
DQN, 1 for DRQN (achieving at least 95% of the performance of the best policy reported in OpenAI
Gym).
2k ∙ kτv is the Total Variation distance, and for any V ∈ R1S1JV∣∣∞ = maxs∈s |V(s)|.
6
Under review as a conference paper at ICLR 2020
Figure 1: Average performance loss vs. attack amplitude ε: (top-left) discrete MountainCar, (top-
right) Cartpole, (bottom-left) continuous MountainCar, (bottom-right) LunarLander. The attack
policy φ has been trained only for ε = 0.05 In red is shown the FGM attack, in blue the attack
proposed in this paper.
Adversarial attack. For each environment A, B, C, and D, we train one adversarial attack using
DDPG against one of the agent’s policies, and for a perturbation constraint ε = 0.05 (we use the
same normalization method as in Huang et al. (2017)). For results presented below, we use uniformly
distributed noise; results with gradient-based noise are discussed in the Appendix. To get attacks for
other values of ε, we do not retrain our attack but only change the projection layer in our network.
For environment E, we represent the state as a 4-dimensional feature vector, and train the attack using
DQN in the feature space. The full experimental setup is presented in the Appendix.
6.1	Optimal vs. gradient-based attacks
We compare our attacks to the FGM attacks (gradient-based) proposed in Pattanaik et al. (2018) (we
found that these are the most efficient). To test a particular attack, we run it against the 3 agent’s
policies, using 10 random seeds (e.g. involved in the state initialization), and 30 episodes. We hence
generate up to 1800 episodes, and average the cumulative reward of the agent.
In Figure 1, we plot the performance loss for different the perturbation amplitudes ε in the 4
environments. Observe that the optimal attack consistently outperforms the gradient-based attack,
and significantly impact the performance of the agent. Also note that since we have trained our
attack for ε = 0.05, it seems that this attack generalizes well for various values of ε. In Appendix,
we present similar plots but not averaged over the 3 agent’s policies, and we do not see any real
difference — suggesting that attacks generalize well to different agent’s policies.
From Figure 1, for environments with discrete action spaces (the top two sets of curves), the resilience
of policies trained by DRQN is confirmed: these policies are harder to attack.
Policies trained using DDPG for environments with continuous action spaces (the bottom two sets of
curves in Figure 1) are more difficult to perturb than those trained in environments with discrete action
spaces. The gradient-based attack does not seem to be efficient at all, at least in the range of attack
amplitude ε considered. In the case of LunarLander some state components were not normalized,
which may explain why a bigger value of ε is needed in order to see a major performance loss (at
least ε = 0.1). Our optimal attack performs better, but the impact of the attack is not as significant as
in environments with discrete action spaces: For instance, for discrete and continuous MountainCar,
our attack with ε = 0.07 decreases the performance of the agent by 30% and 13%, respectively.
6.2	Attack on Pong, based on feature extraction
We consider the game of Pong where the state is an 84x84 pixel image. The agent’s policies were
trained using DQN using as inputs the successive images. As for the attack, we extracted a 4-
dimensional feature vector fully representing the state: this vector encodes the position of the centre
7
Under review as a conference paper at ICLR 2020
Unperturbed Perturbed
image	image
Unperturbed Perturbed
image	image
Adversarial actions
Figure 2:	Game of Pong. (Left part) Images and corresponding agent's action distributions before
and after the attack. (Right part) Average agent,s reward at different phases of the attack training.
of mass of the three relevant objects in the image (the ball and the two bars). Extracting the features
is done using a high pass filter to detect the contours. We need 4-dimensional vectors since the two
bars have a single degree of freedom, and the ball has two.
We limit the amplitude of the attack in the feature space directly. More precisely, we impose that the
adversary just changes one of the 4 components of the feature vector by one pixel only. For example,
the adversary can move up one bar by one pixel. We found that the amplitude of the resulting attack
in the true state space (full images) corresponds to ε = 0.013.
The attack is trained using DQN in the feature space. In Figure 2 (left part), we present a few true
and perturbed images, and below the corresponding distributions of the actions selected by the agent
without and with the attack (the probabilities are calculated from the output of the network using a
softmax function with temperature T = 1). Moving objects by one pixel can perturb considerably
the agent,s policy. In Figure 2 (right part), we show the evolution of the performance of the agent's
policy under our attack during the training of the latter. Initially, the agent,s policy achieves the
maximum score, but after 500 episodes of training, the agent,s score has become close to the lowest
possible score in this game (-20). This score can be reached in an even fewer number of episodes
when tuning the training parameters. The gradient-based attacks performed in Huang et al. (2017)
cannot be directly compared to ours, since these attacks change 4 frames at every time step, while
ours modifies only the last observation. They also modify the values of every pixel in frames (using
32-bit precision over the reals [0,1]), whereas we change only the values of just a few (using 8-bit
precision over the integers [0,255]).
7 Conclusion
In this paper we have formulated the problem of devising an optimal black-box attack that does not
require access to the underlying policy of the main agent. Previous attacks, such as FGM Huang et al.
(2017), make use of white-box assumptions, i.e., knowing the action-value function Q, or the policy,
of the main agent. In our formulation, we do not assume this knowledge. Deriving an optimal attack
is important in order to understand how to build RL policies robust to adversarial perturbations.
The problem has been formulated as a Reinforcement Learning problem, where the goal of the
adversary is encapsulated in the adversarial reward function. The problem becomes intractable when
we step out of toy problems: we propose a variation of DDPG to compute the optimal attack. In
the white-box case, instead of using FGM, we propose to use a gradient-based method to improve
exploration.
Adapting to such attacks requires solving a Partially Observable MDP, hence we have to resort to
non-markovian policies Singh et al. (1994). It can be achieved by adopting recurrent layers, as in
DRQN Hausknecht & Stone (2015). We also show that Lipschitz policies have desirable robustness
properties.
We validated our algorithm on different environments. In all cases we have found out that our attack
outperforms gradient methods. This is more evident in discrete action spaces, whilst in continuous
spaces it is more difficult to perturb for small values of ε, which may be explained by the bound we
provide for Lipschitz policies. Finally, policies that use memory seem to be more robust in general.
8
Under review as a conference paper at ICLR 2020
References
Karl J Astrom. Optimal control of markov processes with incomplete state information. Journal of
mathematical analysis and applications, 10(1):174-205,1965.
Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy induction
attacks. In International Conference on Machine Learning and Data Mining in Pattern Recognition,
pp. 262-275. Springer, 2017.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:
253-279, 2013.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. International Conference on Learning Representations, 2015.
Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps. In
2015 AAAI Fall Symposium Series, 2015.
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks
on neural network policies. Workshop Track of the 5th International Conference on Learning
Representations, Toulon, France, 2017.
Jernej Kos and Dawn Song. Delving into adversarial attacks on deep policies. Workshop Track of the
5th International Conference on Learning Representations, Toulon, France, 2017.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep
visuomotor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David
Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. International
Conference on Learning Representations, San Juan, Puerto Rico, 2016.
Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun.
Tactics of adversarial attack on deep reinforcement learning agents. Proceedings of the Twenty-
Sixth International Joint Conference on Artificial Intelligence, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. NIPS Deep
Learning Workshop, 2013.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928-1937, 2016.
Jun Morimoto and Kenji Doya. Robust reinforcement learning. Neural computation, 17(2):335-359,
2005.
Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust
deep reinforcement learning with adversarial attacks. In Proceedings of the 17th International Con-
ference on Autonomous Agents and MultiAgent Systems, pp. 2040-2042. International Foundation
for Autonomous Agents and Multiagent Systems, 2018.
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforce-
ment learning. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 2817-2826. JMLR. org, 2017.
Matthias Plappert. keras-rl. https://github.com/keras-rl/keras-rl, 2016.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.
9
Under review as a conference paper at ICLR 2020
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In ICML, 2014.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of Go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016. doi:
10.1038/nature16961.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan
Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering
the game of go without human knowledge. Nature, 550:354 EP -, Oct 2017. URL https:
//doi.org/10.1038/nature24270.
Satinder P Singh, Tommi Jaakkola, and Michael I Jordan. Learning without state-estimation in
partially observable markovian decision processes. In Machine Learning Proceedings 1994, pp.
284-292. Elsevier, 1994.
Matthijs TJ Spaan. Partially observable markov decision processes. In Reinforcement Learning, pp.
387-414. Springer, 2012.
10
Under review as a conference paper at ICLR 2020
A Appendix
A.1 Other experimental results
Gridworld. We use as a toy-example a 6 × 6 gridworld, with `1 distance, and perturbation magnitude
ε = 1.0. The goal of the game is to reach the top-left or bottom-right corner in the fewest number of
steps, with reward -1 for each step spent in the game. We implemented both the gradient methods
inspired by Huang et al. Huang et al. (2017) and Pattanaik et al. Pattanaik et al. (2018), and compared
them with our proposed method to find the optimal perturbation policy φ. The implementation of
those algorithms is shown in Algorithm 3, 4. In figure 3 is shown the undiscounted value of the
various attacks: We can see the value of the worst attack defined in Pattanaik et al. (2018) does not
achieve the same values that are achieved from the optimal perturbation φ.
	-∞	-∞	-∞	-∞	-∞
-1	-∞	-∞	-∞	-∞	-∞
-2	-∞	-∞	-∞	-∞	-∞
-3	-∞	-∞	-∞	-∞	-∞
-4	-∞	-∞	-∞	-∞	-∞
-∞	-∞	-3	-2	-1	
	-∞	-∞	-∞	-∞	-∞
-1	-2	-3	-4	-∞	-∞
-2	-3	-4	-5	-4	-∞
-3	-4	-7	-6	-3	-∞
-4	-9	-8	-3	-2	-∞
-11 -10		-3	-2	-1		
	-∞	-∞	-∞	-∞	-∞
-1	-∞	-∞	-∞	-∞	-∞
-2	-∞	-∞	-∞	-∞	-∞
-3	-∞	-∞	-∞	-∞	-∞
-4	-∞	-∞	-∞	-∞	-∞
-5	-∞	-3	-2	-1	
Figure 3:	Un-discounted value of the perturbed policy of the main agent. Left: our proposed attack,
middle: attack in Huang et al. (2017), right: attack in Pattanaik et al. (2018).
OpenAI Gym Environments. Figure 4 presents the results for the discrete MountainCar environment
A. There, We plot the distribution of the performance loss (in %) due to the attack for ε = 0.05, when
the agent policies have been trained using DQN (top) or DRQN (bottom).
Figure 4: Performance loss distribution under the optimal and gradient-based attacks with amplitude
ε = 0.05 for the discrete MountainCar environment. The attacks are against policies trained using
DQN (top) and DRQN (bottom).
11
Under review as a conference paper at ICLR 2020
⅛-su3α
----Gradient method
----Optimal attack
----No attack ∖
-60.00%	-40.00%	-20.00%
0.00%
Mean: -14.5%
20.00%
3一SU①。
Figure 5: Mountaincar environment - distribution of the average discounted reward performance
decrease for ε = 0.05, with respect to the main agent’s policy the attacker has trained for.
Mean： -16.45%
Mean: -6.34%
----Gradient metnod
----Optimal attack
No attack
-5.00%	0.00%	5.00%
⅛-su3α
Mean： -16.78%
Mean: -5.27%
----Gradient metnod
----Optimal attack
No attack
0--------------------------
-25.00%	-20.00%	-15.00%	-10.00%
ε = 0.05 - DRQN
20
至415 ------ Gradient method
c --------- Optimal attack
□ ɪɑ	No attack
5
0--------------------------------- 1	^
-25.00%	-20.00%	-15.00%	-10.00%	-5.00%	0.00%	5.00%
S = 0.05 - DRQN
Mean: -13.79%
Mean: -4.73%
-25.00%	-20.00%	-15.00%	-10.00%
Performance loss
20
----Gradient method
----Optimal attack
No attack
Mean: -14.52%
Mean: -3.78%
-25.00%	-20.00%	-15.00%	-10.00%	-5.00%	0.00%	5.00%
Performance loss
5 O
1 1
3一SU①。
Figure 6: Cartpole environment - distribution of the average discounted reward performance decrease
for ε = 0.05. On the left we show the distribution with respect to the main agent’s policy the attacker
was trained on, and on the right an average across all the main agent’s policies.
In Figure 5-8 we show the performance loss distribution for ε = 0.05, and in Figure 9 the average
performance loss for different values of ε, all computed when the adversary attacks the agent’s policy
it was trained for. Additionally, for environment B, C, D, we show the loss distribution averaged
across all the main agent’s policies.
0 5 0 5 0 5
4 3 3 2 2 1
⅛-su3α
Figure 7: Continuous MountainCar environment - distribution of the average discounted reward
performance decrease for ε = 0.05. On the left we show the distribution with respect to the main
agent’s policy the attacker was trained on, and on the right an average across all the main agent’s
policies.
0 5 0 5 0 5
4 3 3 2 2 1
⅛一 suaɑ
12
Under review as a conference paper at ICLR 2020
Figure 8: LunarLander environment - distribution of the average discounted reward performance
decrease for ε = 0.05. On the left we show the distribution with respect to the main agent’s policy
the attacker was trained on, and on the right an average across all the main agent’s policies.
-0%
S-10%
φ -20%
-50%
C
e
E -30%
Φ 40%
0.00	0.01	0.02	0.03	0.04	0.05	0.06	0.07
ε
5%
-60%
-35%
ω -5%
O
OJ
C
tŋ -15%
E
L-
€
q? -25%
Figure 9: Average performance loss vs. attack amplitude ε in the various environments, when the
adversary attacks the main agent’s policy it was trained on. (top-left) discrete MountainCar, (top-right)
Cartpole, (bottom-left) continuous MountainCar, (bottom-right) LunarLander. Attacks are against
using policies obtained using DQN, DRQN, or DDPG.
0.00	0.01	0.02	0.03	0.04	0.05	0.06	0.07
ε
Gradient based exploration Gradient-based exploration tries to exploit knowledge of the unper-
turbed policy π of the main agent. Caution should be taken, in order to reduce bias, and to minimize
the risk of exploring like an on-policy method. If we follow the information contained in π without
restriction we would end up in a sub-optimal solution, that minimizes the value of the unperturbed
policy π . The exploration noise is given by the following equation:
et ,	Xt = 0,
I(I - ωt)et + ωtgJηVsJ(n(a|St), y)), Xt = 1
(7)
where g is a normalizing function such as g(x)=窗,or g(x) = sign(x), Xt is a Bernoulli random
variable with P(Xt = 1) = p, and ωt = max。∏(a∣st) - min。∏(a∣st). J is the cross-entropy loss
between ∏(a∣st) and a one-hot vector y that encodes the action with minimum probability in state st.
13
Under review as a conference paper at ICLR 2020
If ∏ is not known, but the action-value function Qn is known, We can compute ∏(∙∣s) by applying the
softmax function on the action-values of a particular state s.
Figure 10: Mountaincar environment. On the left is shown the average episode loss when exploring
using a gradient-based exploration methods vs. uniform noise exploration. On the right are shown
statistics regarding episodes reward for ε = 0.05, and norm `2, after training.
The gradient of the cross-entropy loss computes the direction that increases the probability of taking
the action with minimum value when the policy π is unperturbed. The random variable Xt , instead, is
necessary in order to reduce bias induced by following this gradient, while ωt quantifies how strongly
an agent prefers the optimal action in state st. Intuitively, in case π is an optimal policy, the lower ωt
is, the less impact the optimal action has in that state. This guarantees that we follow the gradient of
J mostly when we are in critical states. In Figure 10 we show the low-pass filtered loss during the
training of the attack. We clearly see an improvement in performance, although it highly depends on
the selection of the parameters.
We have tested this type of exploration on Mountaincar, with p = 0.35, and temperature of the
softmax function set to 1. The exploration noise et is drawn from a Uniform distribution with
parameters shown in Table 3 (Mountaincar). In Figure 10 we compare gradient-based exploration vs.
uniform noise exploration. On the left, we show the low-pass filtered loss during the training of the
attack, with ε = 0.05 and `2 norm. On the right of Figure 10 we compare episodes reward statistics
after training. Results in the table were averaged across 5 different seeds, with 100 episodes for each
seed, for a total of 500 episodes for each method. We see an improvement in performance, although
it highly depends on the selection of the parameters.
A.2 Proof of Proposition 5.1
Proof. Let
g∏ (S, a = Es0~pG1s,O)[Vπ(s0)], and	gπ°φ(s,a)= Es，〜P(∙∣s,a)[Vπ°φ(s0)]∙	(8)
We will now proceed by bounding the quantity |(Vπ - Vπ^φ)(s)∣:
|(Vπ - Vπ°φ)(s)l = Ea〜∏(∙∣s)[r(s, a) + γgπ(s, a)] - Ea〜∏°φ(.∣s)[r(s, a) + γgπ°φ(s, a)]∣,	(9)
=嘎〜φ(∙∣s) IEa〜π(∙∣s)[r(S,a)+ Ygπ (S,a)] - Ea〜π(∙⑸[r(S,a)+ Yg"°φGa)]] |,
(10)
where in the last line we made use of the following equality Ea〜∏°φ(∙∣s) [∙] = EM〜φ(∙∣s) [E。〜∏(∙⑸[•]].
Now, by making use of |E[x]| ≤ E[|x|] we get:
|(Vπ - Vπ°φ)(s)l = ∣Em〜φ(∙∣s) [e。〜∏(∙∣s)[r(s,a) + γgπ(s, a)] - Ea〜∏(∙⑸[r(s, a) + γgπ°φ(s,a)]] ∣,
(11)
≤ EM〜φ(∙∣s)∣ [e。〜∏(∙∣s)[r(s,a) + γgπ(s, a)] - Ea〜∏(∙⑸[r(s, a) + γgπ°φ(s,a)] ∣.
(12)
14
Under review as a conference paper at ICLR 2020
At this point consider a random variable X that can assume values only in a bounded set K. Let X∞
be a bound on the absolute value of X. Then, for any two distributions /1,/2 whose support is K we
have ∣Efι[X] - Ef2[X]| ≤ 2X∞∣∣∕ι -川ITV:
∣Ef1 [χ ]-Ef2 [X ]∣ = IXx/1 (X)-X χ∕2(χ))∣ ≤ X∞ X ∣f1(χ)-f2(χ)∣ = 2X∞kfι-f2kτv.
x∈K	x∈K	x∈K
(13)
By using the previous inequality we can bound the reward difference:
嘎〜M.ls)]Ea〜π(∙∣s)[r(s,α)] - Ea〜∏(同[r(S, a)]1 ≤ 2R嘎〜既⑼ |忻(S)- π(S)∣∣TV,	(14)
≤ 2Rα∏ε(s),	(15)
where ɑπ,ε(s) = maxg∈χ≡ ∣∣π(s) - π(s)∣τv. The inequality becomes
∣(Vπ - Vπ°φ)(s)∣ ≤ 2ɑε(s)R + YEi(同| Eo^π(∙∣s)[gπ(s,α)] - E°~∏(同[尸(s, a)] ∣ .	(16)
Now, let gπ(s, a) = gπoφ(s, a) + ∆(s, a), and observe that gπ(s, a) is uniformly bounded by
V∞ = IlVπ∣∣∞, and ∆(s, a) by IlVπ - Vπoφ∣∣∞, thus
ES■〜φ(.∣ s)∣ Ea〜∏(.∣ s)[gπ(s,a)] - E皿∏(.⑸[gπoφ(s,a)] ∣ ,	(17)
=E5~φ(∙ ∣ S)IEa~π(∙ ∣ s)[g (s, a)] - Ea~π(∙ ∣ s)[g (s,a) - A(l^a)] | ,	(18)
≤Es∙2(. ∣ S) [2α∏,ε(s)V∞ + ∣Vπ - Vπoφ∣∞i = 2α∏,ε(s)V∞ + ∣Vπ - Vπoφ∣∞. (19)
Hence, we can deduce the following inequality:
∣(Vπ - Vπoφ)(s)∣ ≤ 2Rα∏,ε(s) + 27απ,ε(s)V∞ + YkVπ - Vπoφ∣∞,	(20)
=2α∏,ε(s)(R + γV∞) + YkVπ - Vπoφ∣∞.	(21)
By taking the term YkVπ - Vπoφ∣∞ on the left hand side, dividing by 1 - γ, and finally taking the
supremum over S ∈ S we obtain the result:
∣Vπ - Vπoφk∞ ≤ 2 kαπ,εk∞ (R + yV∞).	(22)
1 - Y
Assume now the main agent,s policy is smooth, such that ∣π(s) - π(s0)∣τv ≤ Ld(s, s0), L ∈ R>o.
We can now bound the term EM〜*∣ s)∣E0〜∏(∙∣ s)[r(s, a)] - Ea〜∏(∙∣ s) [r(s, a)]∣ as follows:
E百〜* ∣ s)∣Ea〜∏(∙ ∣ s)[r(s, a)] - Ea〜∏(∙ ∣ M)[r(s,a)]∣ ≤ E百〜*∣ s)[2R∣π(s) - π⑸IITV],	(23)
≤ 2LREm〜* ∣ s)[d(s,s)],	(24)
=2LRd(s),	(25)
where d(s) = E≤^φ(-1 S) [d(s, s)] denotes the average weighted distance between elements of Xf and
s, according to the distribution φ(∙∣s). Equivalently we also have
EM〜φ(∙ ∣ S)IEa〜π(∙ ∣ s)，(s, a)] — Ea〜π(∙ ∣ M) [d^G a)] ] ≤ EM〜φ(∙ ∣ s) [2V∞ IIn(S)- π⑸ IlTV (26)
+ IVπ - VπoφI∞],	(27)
≤ 2LV∞[d(s) + IVπ - Vπoφ∣∣∞], (28)
from which we obtain
∣(Vπ - Vπoφ)(s)∣ ≤ 2LRd(s) + Y2Ld(s)V∞ + Y∣∣Vπ - Vπoφ∣∣∞,	(29)
=2Ld(s)(R + yV∞) + YIVπ - Vπoφ∣∣∞,	(30)
and
IVπ - VπoφI∞ ≤ 2sup M(R + yV∞) ≤ 2占(R + yV∞).	(31)
s∈s 1 - Y	1 - Y
□
15
Under review as a conference paper at ICLR 2020
A.2.1 Algorithms
Attack training algorithm. In this section we describe the DDPG algorithms discussed in Section
4.3. We will denote the projection layer and the output before the projection layer respectively by
lΠ and xφ,t. Algorithm 1 refers to the black-box attack, whilst Algorithm 2 makes use of π, if it is
known. Apart from the projection layer, and the change in the gradient step in the white-box case,
both algorithms follow the same logic as the usual DDPG algorithm, explained in Lillicrap et al.
(2016). Moreover, projections have a regularization term λ in order to prevent division by 0.
Algorithm 1 Training adversarial attack
1:	procedure ATTACKTRAINING(π, Ttraining, NB)
2:	Initialize critic and actor network Qθ , φω with weights θ and φ.
3:	Initialize target critic and actor networks Qθ-, φφ- with weights θ- J θ,φ- J φ.
4:	Initialize replay buffer D.
5:	for episode 1:M do
6:	Initialize environment and observe state s0 , and set initial time t J 0.
7:	repeat
8:	Select a perturbation and add exploration noise St J St + l∏(χφ,t + et).
9:	Feed perturbation St to main agent and observe reward and state rt, st+ι.
10:	Append transition (St, SSt, rSt, St+1) to D.
11:	if t ≡ 0 mod Ttraining then
12:	Sample random minibatch B 〜D of NB elements.
13:	Set target for the i-th element of B to yi = rSi + γQθ- (Si+1, φφ- (Si+1)).
14:	Update critic by minimizing loss N^ PNι(yi 一 Qθ(si, Si))2.
15:	Update actor using sampled policy gradient
1 NB
▽sJ ≈ -Tj- X2 ▽百Qθ(si, S)▽3 φφ(si)∖s=φ(si).
NB
i=1
16:	Slowly update the target networks:
θ- J (1 一 τ)θ- + τθ, φ- J (1 一 τ)φ- + τφ.
17:	end if
18:	t J t+ 1.
19:	until episode is terminal.
20:	end for
21:	end procedure
A.2.2 Gradient-based attacks.
Gradient-based attacks can be adapted to MDPs by solving the optimization problem these methods
are trying to solve. There are two main techniques that were developed, one by Huang et al. Huang
et al. (2017), and the other one from Pattanaik et al. Pattanaik et al. (2018): the former tries to
minimize the probability of the optimal action a*, whilst the latter maximizes the probability of
taking the worst action, a*, defined as the action that attains the minimum value for the unperturbed
policy π. Pseudo-code for the former is given in Algorithm 3, while for the latter is in Algorithm 4.
A.3 Experimental setup
Hardware and software setup. All experiments were executed on a stationary desktop computer,
featuring an Intel Xeon Silver 4110 CPU, 48GB of RAM and a GeForce GTX 1080 graphical card.
Ubuntu 18.04 was installed on the computer, together with Tensorflow 1.13.1 and CUDA 10.0.
Deep-learning framework. We set up our experiments within the Keras-RL framework Plappert
(2016), together with OpenAI Gym Brockman et al. (2016) as interface to the test environments used
in this paper.
16
Under review as a conference paper at ICLR 2020
Algorithm 2 Training adversarial attack 2
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
procedure ATTACKTRAINING2(π, Ttraining , NB)
Initialize critic network Qθ and actor φω with weights θ and φ.
Initialize target critic and actor networks Qθ-, φφτ with weights θ- J θ,φ- J φ.
Initialize replay buffer D.
for episode 1:M do
Initialize environment and observe state s0 .
Set initial time t J 0.
repeat
Select a perturbation and add exploration noise St J S + l∏ (xφ,t + et).
Feed perturbation St to main agent and observe reward and state St, st+ι.
Append transition (St, at, rSt, St+1) to D, where at = π(SSt).
if t ≡ 0 mod Ttraining then
Sample random minibatch B 〜D of NB elements.
Set target for the i-th element of B to yi = rSi - γQθ- (si+1, π(φφ- (si+1))).
Update critic by minimizing loss 六 Pi=1(yi + Qθ⑶，a。)2.
Update actor using sampled policy gradient
N ω J ≈ -^~Γ~ X NaQθ (Si, a)Vsπ(s)Vω φφ(si)∖a=∏(φ(si)),S=φ(si) ∙
NB
i=1
17:	Slowly update the target networks:
θ- J (1 - τ)θ- + τθ,
φ- J (1 - τ)φ- + τφ.
18:	end if
19:	t J t + 1.
20:	until episode is terminal.
21:	end for
22:	end procedure
Algorithm 3 Adversarial Attack - Adaptation of Huang et al. (2017).
1:	procedure ATTACKPROCEDURE 1(s, π, Xsε)
2:	a* — argmaXa π(a∣s)	. Best action in state S
3:	s — s
4:	for all s0 ∈ Xsε do	. Loop trough the ε-neighbor states
5:	if π(a*∣s0) < π(a* 恒)then	. Evaluate if a* is not optimal in j
6:	S — s0
7:	end if
8:	end for
9:	return sS
10	: end procedure
Algorithm 4 Adversarial Attack - Adaptation of Pattanaik et al. (2018).
1:	procedure ATTACKPROCEDURE(s, π, Qπ , Xsε)
2:	q* - Qπ(s, argmaXa π(a∣s))
3:	S — S
4:	for all s0 ∈ Xsε do	. Loop trough the ε-neighbor states
5:	q — Qn(s, argmaXa π(a∣s0)))
6:	if q < q* then	. Evaluate if optimal action in S0 is worse in S
7:	q* — q
8:	S — S0
9:	end if
10	:	end for
11	:	return SS
12	: end procedure
17
Under review as a conference paper at ICLR 2020
All the adversarial methods used in this paper, including Gradient attacks, and DRQN, have been
implemented from scratch using the Keras-RL framework. All the code and experiments can be
found at the following url https://bit.ly/2JAD7CF.
Neural network architectures.
Depending on the environment we use a different neural network architecture, as shown in table
1. The network architecture was not optimised, but was chosen large enough also to have a better
comparison with Gradient methods since they depend on the dimensionality of the network, and may
fail for small-sized networks. For all environments we set the frame skip to 4. For the adversary,
Neural network structure	MountainCar DQN	MountainCar		LunarLander		Cartpole DQN
		Actor	Q-Critic	Actor	Q-Critic	
Layer 1	FC(512, ReLU)	FC(400, ReLU)	FC(400, ReLU)	FC(400, ReLU)	FC(400, ReLU)	FC(256, ReLU)
Layer 2	FC(256, ReLU)	FC(300, ReLU)	FC(300, ReLU)	FC(300, ReLU)	FC(300, ReLU)	FC(256, ReLU)
Layer 3	FC(64, ReLU)	FC(1, Tanh)	FC(1)	FC(2, Tanh)	FC(1)	FC(2)
Layer 4	FC(3)	-	-	-	-	-
Table 1: Neural network structure for the policies of the main agent. FC stands for Fully Connected
layer. Inside the parentheses is indicated how many units were used, and the activation function for
that layer.
instead, We used a different actor-critic architecture, although We have not tried to optimise it. The
model can be seen in figure 11. In the figure dim(S) denotes the dimensionality of the state space.
The neural network structure for the game of Pong is the same as the one used in Huang et al. (2017),
for both the adversary and the main agent.
Actor Network
State
Fully Connected Layer
512 units
ReLU
Fully Connected Layer
256 units
ReLU
Fully Connected Layer
128 units
ReLU
Fully Connected Layer
dim(S) units
Exploration
Noise
Critic Network
(Perturbed observation, State)
Q value
Projection layer
Perturbed
observation
Figure 11: Diagram of the adversarial agent perturbing observations of the environment. On theright
is shown the environment model for the malicious agent. The equivalent MDP is denoted by M.
Data collection and preprocessing. Every data point is an average across 10 different seeds, where
for each seed we have tested an attack against all of the main agent,s policies, and each test considers
30 episodes (10 in the case of FGSM for Continuous Mountaincar and LunarLander). In Table 2
18
Under review as a conference paper at ICLR 2020
is shown the number of policies for the main agent and the number of episodes collected. Density
plots were preprocessed using the function seaborn.kdeplot with Gaussian kernel and bandwidth 0.05
(0.15 for LunarLander). Gradient-based exploration data was preprocessed using a first order acausal
Butterworth filter with critical frequency ωn = 0.1.
Algorithm/Environment	MountainCar	MountainCar-Continuous	Cartpole	LunarLander
DQN	-3 (1800)^^	-	3 (1800)	-
DRQN	1 (600)	-	1 (600)	-
DDPG	-	3 (1200)	-	3 (1200)
Table 2: Number of policies for the main agent for each Algorithm/Environment pair. In parenthesis
is shown the total number of episodes taken.
Settings for gradient-attacks. For gradient-attacks we have always used the same norm as the one
used by the optimal attack. For the Softmax layer we have always used a temperature of T = 1.
We compute the gradient using the method from Pattanaik et al. Pattanaik et al. (2018), which has
proven to be better than the one proposed by Huang et al.Huang et al. (2017). In order to speed-up
the computation we do not sample the attack magnitude from a random distribution, but fix it to the
maximum value ε.
Training procedure and parameters. All the training parameters for the main agent are shown in
table 3. Parameters were chosen based on previous experience, so that the policy trains in a fixed
number of training steps. Policies were trained until they were within 5% of the optimal value
referenced in the OpenAI Gym documentation Brockman et al. (2016), across an average of the last
100 episodes. For the game of Pong we use same as Huang et al. (2017)
Parameters/Environment	MountainCar	MountainCar	LunarLander	Cartpole	Pong
Action space	Discrete	Continuous	Continuous	Discrete	Discrete
Algorithm used	DQN/DRQN	DDPG	DDPG	DQN/DRQN	DQN
Reference value	50.0	90.0	200.0	195.0	20
Number of steps	4 ∙ 104	2 ∙ 104	3 ∙ 104	22 ∙ 103	4 ∙ 106
Number of warmup steps	100	103	3 ∙ 103	103	5 ∙ 104
Replay memory size	4 ∙ 104	2 ∙ 104	15 ∙ 103	22 ∙ 103	5 ∙ 105
Discount factor	0.99	0.99	0.99	0.99	0.99
Learning rate	10-3	(10-4, 10-3)	(10-4, 10-3)	10-4	25∙10-4
Optimizer	Adam	Adam	Adam	Adam	Adam
Gradient clipping	Not set	1.0	1.0	1.0	Not set
Exploration style	e-greedy	Gaussian noise	Gaussian noise	-greedy	-greedy
Initial exploration rate/ Std (DDPG)	0.95	2	1	0.95	0.95
Final exploration rate/ Std (DDPG)	0.1	10-3	10-3	0.1	0.05
Exploration steps	28 ∙ 103	14 ∙ 103	21 ∙ 103	11 ∙ 103	35 ∙ 105
Batch size	32	64	192	256	32
Target model update	102	10-3	10-3	200	104
Initial random steps - Training	0	0	0	0	0
Initial random steps - Testing	10	10	10	10	50
Frame-skip	4	4	4	4	4
Table 3: Training settings for the policies of the main agent. When using DDPG we sometimes used
different parameters for the actor and the critic. In that case in the table are shown 2 parameters, the
first one for the actor, the second one for the critic.
For the adversary, we used similar parameters, shown in Table 4.
19
Under review as a conference paper at ICLR 2020
Parameters/Environment	MoUntainCar	MountainCar Continuous	LunarLander	Cartpole	Pong
Algorithm used	DDPG	DDPG	DDPG	DDPG	DQN+Features
Number of steps	4 ∙ 104	6 ∙ 104	5 ∙ 104	4 ∙ 104	2 ∙ 106
Number of warmup steps	4 ∙ 103	3 ∙ 103	5 ∙ 103	4 ∙ 103	104
Replay memory size	15∙103	3 ∙ 104	15∙103	15∙103	50 ∙ 104
Discount factor	0.99	0.99	0.99	0.99	0.99
Learning rate	(10-4,10-3)	(10-4, 10-3)	(10-4, 10-3)	(10-3, 10-2)	25∙10-4
Optimizer	Adam	Adam	Adam	Adam	Adam
Gradient clipping	1.0	1.0	1.0	1.0	Not set
Exploration style	Uniform noise	Uniform noise	Uniform noise	Uniform noise	-greedy
Initial exploration settings	[-0.5, 0.5]	2	[-0.5, 0.5]	[-0.3, 0.3]	0.95
Final exploration settings	[-10-3,10-3]	[-10-3, 10-3]	[-10-3, 10-3]	[-10-3, 10-3]	0.05
Exploration steps	28 ∙ 103	48 ∙ 103	35∙103	28 ∙ 103	15 ∙ 105
Batch size	164	32	256	92	32
Target model update	10-3	10-3	10-3	10-2	103
Initial random steps - Training	0	0	0	0	0
Initial random steps - Testing	10	10	10	10	10
Frame-skip	4	4	4	4	4
Projection - Regularization term	10-6	10-6	10-6	10-6	-
Table 4: Training settings for the attacker.
20