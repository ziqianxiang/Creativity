Under review as a conference paper at ICLR 2020 [Please do not distribute.]
Alleviating	Privacy	Attacks	via	Causal
Learning
Anonymous authors
Paper under double-blind review
Ab stract
Machine learning models, especially deep neural networks have been shown to
reveal membership information of inputs in the training data. Such membership
inference attacks are a serious privacy concern, for example, patients providing
medical records to build a model that detects HIV would not want their identity to
be leaked. Further, we show that the attack accuracy amplifies when the model is
used to predict samples that come from a different distribution than the training
set, which is often the case in real world applications. Therefore, we propose the
use of causal learning approaches where a model learns the causal relationship
between the input features and the outcome. An ideal causal model is known to be
invariant to the training distribution and hence generalizes well to shifts between
samples from the same distribution and across different distributions. First, we
prove that models learned using causal structure provide stronger differential pri-
vacy guarantees than associational models under reasonable assumptions. Next, we
show that causal models trained on sufficiently large samples are robust to mem-
bership inference attacks across different distributions of datasets and those trained
on smaller sample sizes always have lower attack accuracy than corresponding
associational models. Finally, we confirm our theoretical claims with experimental
evaluation on 4 datasets with moderately complex Bayesian networks. We observe
that neural network-based associational models exhibit upto 80% attack accuracy
under different test distributions and sample sizes whereas causal models exhibit
attack accuracy close to a random guess. Our results confirm the value of the
generalizability of causal models in reducing susceptibility to privacy attacks.
1 Introduction
Machine learning algorithms, especially deep neural networks (DNNs) have found diverse appli-
cations in various fields such as healthcare (Esteva et al., 2019), gaming (Mnih et al., 2013), and
finance (Tsantekidis et al., 2017; Fischer & Krauss, 2018). However, a line of recent research has
shown that deep learning algorithms are susceptible to privacy attacks that leak information about the
training dataset (Fredrikson et al., 2015; Rahman et al., 2018; Song & Shmatikov, 2018; Hayes et al.,
2017). Particularly, one such attack called membership inference reveals whether a particular data
sample was present in the training dataset (Shokri et al., 2017). The privacy risks due to membership
inference elevate when the DNNs are trained on sensitive data such as in healthcare applications. For
example, patients providing medical records to build a model that detects HIV would not want to
reveal their participation in the training dataset.
Membership inference attacks are shown to exploit overfitting of the model on the training
dataset (Yeom et al., 2018). Existing defenses propose the use of generalization techniques such as
adding learning rate decay, dropout or using adversarial regularization techniques (Nasr et al., 2018b;
Salem et al., 2018). All these approaches assume that the test data is from the same distribution as
the training dataset. In practice, a model trained using data from one distribution is often used on a
(slightly) different distribution. For example, hospitals in one region may train a model to detect HIV
and share it with hospitals in different regions. However, generalizing to a new context is a challenge
for any machine learning model. We extend the scope of membership privacy to different distributions
and show that the risk from membership attack increases further on DNNs as the test distribution is
changed. That is, the abiltity of an adversary to distinguish a member from a non-member improves
with change in test distributions.
To alleviate privacy attacks, we propose using models that depend on the causal relationship between
input features and the output. Causal learning has been extensively used to guarantee fairness and
1
Under review as a conference paper at ICLR 2020 [Please do not distribute.]
explainability properties of the predicted output (Kusner et al., 2017; Nabi & Shpitser, 2018; Datta
et al., 2016). However, the connection of causal learning to privacy is yet unexplored. To the best
of our knowledge, we provide the first analysis of privacy benefits of causal models. By definition,
causal relationships are invariant across input distributions (Peters et al., 2016), and hence make the
predictions of causal models independent of the observed data distribution, let alone the observed
dataset. Hence, causal models generalize better even with change in the distributions.
In this paper, we show that the generalizability property of causal models directly ensures better
privacy guarantees for the input data. Concretely, we prove that with reasonable assumptions, a
causal model always provides stronger (i.e., smaller value) differential privacy guarantees than a
corresponding associational model trained on the same features and the same amount of added noise
to the training dataset. Consequently, we show that membership inference attacks are ineffective
(equivalent to a random guess) on causal models trained on infinite samples. Empirical attack
accuracies on four different datasets confirm our theoretical claims. We find that 60K training
samples are sufficient to reduce the attack accuracy of a causal model to a random guess. In
contrast, membership attack accuracy for neural network-based associational models increase as test
distributions are changed. The attack accuracy reaches nearly 80% when the target associational
model is trained on 60K training samples and used to predict test data that belong to a different
distribution than the training data. Our results show that causal learning approaches are a promising
direction for training models on sensitive data. Section 2 describes the properties of causal models.
Section 3 proves the connections of causality to differential privacy and robustness to membership
attacks. Section 4 provides empirical results. To summarize, we make the following contributions:
•	For the same amount of added noise, models learned using causal structure provide stronger
-differential privacy guarantees than corresponding associational models.
•	Causal models are provably more robust to membership inference attacks than typical associational
models such as neural networks.
•	We simulate practical settings where the test distribution may not be the same as the training
distribution and find that the membership inference attack accuracy of causal models is close to a
“random guess” (i.e., 50%) while associational models exhibit upto 80% attack accuracy.
2	Properties of Causal Models
Causal models are shown to generalize well since the output of these models depend only on the
causal relationship between the input features and the outcomes instead of the associations between
them. From prior work, we know that the causal relationship between the features is invariant to the
their distribution (Peters et al., 2016). Using this property, we study its effects on the privacy of data.
2.1	Background: Causal Model
Intuitively, a causal model identifies a subset of features that have a causal relationship with the
outcome and learns a function from the subset to the outcome. To construct a causal model, one
may use a structural causal graph based on domain knowledge that defines causal features as parents
of the outcome under the graph. Alternatively, one may exploit the strong relevance property from
Pellet & Elisseeff (2008), use score-based learning algorithms (Scutari, 2009) or recent methods for
learning invariant relationships from training datasets from different distributions (Peters et al., 2016;
Bengio et al., 2019), or learn based on a combination of randomized experiments and observed data.
Note that this is different from training probabilistic graphical models, wherein an edge conveys an
associational relationship. Further details on causal models are in Pearl (2009); Peters et al. (2017).
For ease of exposition, we assume the structural causal graph framework throughout. Consider
data from a distribution (X, Y)〜P where X is a k-dimensional vector and Y ∈ {0,1}. Our goal
is to learn a function h(X) that predicts Y. Figure 1 shows causal graphs that denote the different
relationships between X and Y. Nodes of the graph represent variables and a directed edge represents
a direct causal relationship from a source to target node. Denote Xpa ⊆ X, the parents of Y in
the causal graph. Figure 1a shows the scenario where X contains variables XS0 that are correlated
to Xpa in P, but not necessarily connected to either Xpa or Y. These correlations may change in
the future, thus a generalizable model should not include these features. Similarly, Figure 1b
shows parents and children of Xpa . The d-separation principle states that a node is independent
of its ancestors conditioned on all its parents (Pearl, 2009). Thus, Y is independent of XS1 and
XS2 conditional on Xpa . Therefore, including them in a model does not add predictive value (and
further, avoids problems when the relationships between XS1 and XS2 may also change). Finally, for
2
Under review as a conference paper at ICLR 2020 [Please do not distribute.]
completeness, the exhaustive set of variables to include is known as the causal Markov Blanket1,
XC which includes Y’s parents, (Xpa), children (Ych) and parents of children. Conditioned on its
Markov blanket (Figure 1c), Y is independent of all other variables in the causal graph. When Y
has no descendants in the graph, then the effective Markov blanket includes only its parents, Xpa .
The key insight is that building a model for predict-
ing Y using the Markov Blanket XC ensures that the
model generalizes to other distributions ofX, and also
to changes in other causal relationships between X,
as long as the causal relationship of XC to Y is stable.
We call such a model as a causal model, the features
in (XC) as the causal features, and assume that all
the causal features for Y are observed. In contrast,
we call a model that uses all available features as an
associational model.
2.2	Generalization to new distributions
(a)
(b)
Figure 1: A causal predictive model includes only
the parents of Y (a) and (b). Panel (c) shows the
generalization to a Markov Blanket.
We state the generalization property of causal mod-
els and show how it results in a stronger differential privacy guarantee. We first define
In-distribution and Out-of-distribution generalization error. Throughout, L(., .)
refers to the loss on a single input and LP(., .) = EPL(., .) refers to the expected value of the loss
over a distribution P(X, Y). We refer f : X → Y as the ground-truth labeling function and h : X → Y
as the hypothesis function or simply the model. Then, L(h, h0) is any loss function quantifying the
difference between two models h and h0 .
Definition 1. In-Distribution Generalization Error (IDE). Consider a dataset S 〜P(X, Y). Then
for a model h : X → Y trained on S, the in-distribution generalization error is given by:
IDEp(h, f) = Lp(h, f) -LS〜p(h, f)
(1)
Definition 2. Out-of-Distribution Generalization Error (ODE). Consider a dataset S sampled from
a distribution P(X, Y). Then for a model h : X → Y trained on S, the out-of-distribution generalization
error with respect to another distribution P*(X, Y) is given by:
ODEp,P* (h, f) = Lp* (h, f) - LS〜p(h, f)	(2)
Definition 3. Discrepancy Distance (discL) (Def. 4 in Mansour et al. (2009)). Let H be a set of
hypotheses, h : X → Y. Let L : Y × Y → R+ define a loss function over Y for any such hypothesis h.
Then the discrepancy distance discL over any two distributions P(X, Y) and P*(X, Y) is given by:
dis“(P, P") = max |Lp(h, h0) -Lp*(h, h0)∣	(3)
Intuitively, the term discL(P, P") denotes the distance between the two distributions. Higher the
distance, higher is the chance of an error when transferring h from one distribution to another. Now,
we will state the theorem on the generalization property of causal models.
Theorem 1. Consider a structural causal graph G that connects X to Y, and causal features XC where
XC is a Markov Blanket of Y under G. Let P(X, Y) and P" (X, Y) be two distributions with arbitrary
P(X) and P"(X) such that the causal relationship between XC andY is preserved, which implies that
P(Y|XC) = P"(Y|XC). Let f : XC → Y be the resultant invariant labelling function such that y = f(XC).
Further, assume that HC represents the set of causal models hc : XC → Y that use all causal features
and HA represent the set of associational models ha : X → Y that may use all available features, such
that HC ⊆ HA and f ∈ HC.
Then, for any symmetric loss function L that obeys the triangle inequality, the upper bound of ODE
from a dataset S 〜P(X, Y) to P"(called ODE-Bound) for a causal model hc ∈ HC is less than or
equal to the upper bound ODE-Bound of an associational model ha ∈ HA, with probability at least
(1 - δ)2.
ODE-Boundp,p* (hc, f; δ) ≤ ODE-Boundp,p* (ha, f; δ)	(4)
1We call it the causal Markov Blanket since it is based on the structural causal graph, to distinguish it from
the associational Markov Blanket that is based on conditional probability distribution from a Bayesian network.
3
Under review as a conference paper at ICLR 2020 [Please do not distribute.]
Proof. As an example, consider a colored MNIST data distribution P where the classification task
is to detect whether a digit is greater than 5, and where all digits above 5 are colored with the same
color. Then, under a suitably expressive class of models, the loss-minimizing associational model
may use only the color feature to obtain zero error, while the loss-minimizing causal model will still
use the shape (causal) features to obtain zero error. On any new P * that does not follow the same
correlation of digits with color, we expect that the loss-minimizing associational model will have
higher error than the loss-minimizing causal model.
Formally, since P(Y|XC) = P*(Y|XC), the optimal causal model that minimizes loss over P is the same
as the loss-minimizing model over P*. That is, hCpT = hCpp*. However, for some associational models,
hapT = hapT* and thus there is an additional loss term when generalizing to data from P*. The rest of
the proof follows from triangle inequality of the loss function and the standard bounds for IDE from
past work. Detailed proof is in Appendix Section A.1.	□
Corollary 1. Consider a causal model hc : XC → Y and an associational model ha : X → Y trained
on a dataset S 〜P. Let (x, y) ∈ S and (x0, y0) ∈ S be two input instances such that they share the
same true labelling function y = f(xc) and y0 = f(x0c). Then, the worst-case generalization error
for a causal model on any such x0 is less than or equal to that for an associational model. [Proof in
Appendix Section A.2]
max	Lx0(hc,f) - Lx(hc, f) ≤ max	Lx0(ha,f) - Lx(ha,f)	(5)
x∈s,x0y=f (xC)	x∈s,x0y =f(x。
3	Main Result: Privacy Guarantees with Causality
We now present our main result on the privacy guarantees and attack robustness of causal models.
3.1	Differential Privacy Guarantees
Differential privacy (Dwork et al., 2014) provides one of the strongest notion of privacy guarantees to
hide the participation of an individual sample in the dataset. To state informally, it ensures that the
presence or absence of a single data point in the input dataset does not change the output by much.
Definition 4 (Differential Privacy). A mechanism M with domain I and range O satisfies -differential
privacy if for any two datasets d, d0 ∈ I that differ only in one input and for a setS ⊆ O, the following
holds: Pr(M(d) ∈ S) ≤ e Pr(M(d0) ∈ S)
Based on the generalization property, we show that causal models provide stronger differential
privacy guarantees than corresponding associational models. The standard approach to designing a
differentially private algorithm is by calculating the sensitivity of that algorithm and adding noise
proportional to the sensitivity. Sensitivity captures the change in the output of an algorithm due to
the change in a single data point in the input. Higher the sensitivity, larger is the amount of noise
required to make an algorithm differentially private with reasonable guarantees. We first provide
the formal definition of sensitivity and then show that the sensitivity of causal models is lower than or
equal to associational models.
Definition 5 (Sensitivity (From Def. 3.1 in Dwork et al. (2014)). LetF be a function that maps a
dataset to a vector in Rd. Let S, S0 be two datasets such that S0 differs from S in one data point. Then
the l1-sensitivity of a function F is defined as: ∆F = maxs,s0 ||F (S) - F (S0)||1
Lemma 1. Let S be a dataset over (X, Y) values, such that y(i) = f (x(ci)) ∀(x(i), y(i)) ∈ S, where f
is the true labelling function over the causal features XC. Consider a neighboring dataset S0 such that
S0 = S\(x, y) + (x0, y0) where (x, y) ∈ S, (x0, y0) ∈/ S, and (x0, y0) shares the same causal labelling
function y0 = f (xC). Let a model h be specified by a Set of parameters θ ∈ Ω ⊆ Rn. Let hmin(x; θs)
be a model learnt using S as training data and hms0in(x; θs0) be the model learnt using S0 as training
data. Then, the sensitivity of a causal learning function Fc that outputs learnt empirical hypothesis
hmiS1 J FC(S) and J Fc(S0) is lower than or equal to the sensitivity of an associational
learning function Fa that outputs hmiS1 J Fa(S) and haiSJ J Fa(S0) using a loss function L that is
strongly convex over Ω, symmetric and obeys the triangle inequality,
∆Fc = ma0x ||hcm,iSn - hmc,iSn0 ||1 ≤ ma0x ||ham,iSn - ham,iSn0 ||1 = ∆Fa	(6)
where the maximum is over all such datasets S and S0.
4
Under review as a conference paper at ICLR 2020 [Please do not distribute.]
Proof. We can write the empirical loss minimizers for the datasets S and S0 as:
1N
hmin = argmin Ls(h, f) = arg min N∑^Lχi (h, f)
1 N-1
hmin = arg min LS (h, f) = arg min NELxi (h, f) + [Lχo (h, f) - Lχ(h, f)]
i=1
(7)
From Corollary 1, for x0 ∈/ S and x ∈ S, we have:
maxLx0(hc,f) - Lx(hc, f) ≤ maxLx0(ha,f) - Lx(ha,f)	(8)
x,x0	x,x0
Since x ∈ S and x0 ∈ S0 and |S - S0| = 1 the above is true for any S and S0,
max Ls0(hc, f) - Ls(hc, f) ≤ max Ls0 (ha, f) - Ls(ha, f)	(9)
s,s0	s,s0
Since L is a strongly convex function over Ω, and since HC ⊆ HA ⇒ Ωc ⊆ Ωa,显^孑 and ^^
that minimize Ls (h, f) and Ls0 (h, f) respectively should also be closer to each other than ham,isn and
hma,isn0 (Boyd & Vandenberghe, 2004) (using Eqn. 9).
maxIi θmisι-emis，ι∣ι ≤ maxιι θaisι-壁孑，ι∣ι ⇒ maxιιhmin-hmin，ι∣ι ≤ maxιι hmin-hmin，ι∣ι(⑼
s,s	s,s	s,s	s,s
Hence, sensitivity of a causal model is lower than an associational model i.e., ∆Fc ≤ ∆Fa.	□
Theorem 2.	Let Fc and Fa be the differentially private algorithms corresponding to causal learning
and associational learning algorithms Fc and Fa respectively. Let Fc and Fa provide c-DP and
a-DP guarantees respectively. Then, for noise sampled from the same distribution, Lap(Z) for both
algorithms, we have c ≤ a.
Proof. According to the Def. 3.3 of Laplace mechanism from Dwork et al. (2014), we have,
FC= FC + K 〜Lap( —C)	Fα= Fa + K 〜Lap( —a)	(11)
c a
The noise is added to the output of the learning algorithm i.e., the model parameters. Since K is
sampled from the same noise distribution,
∆FC	∆Fa
Lap(----) = Lap(---)
C a
From Lemma 1, ∆Fc ≤ ∆Fa and hence C ≤ a.
(12)
□
While we prove the general result above, our central claim comparing differential privacy for causal
and associational models also holds true for models developed using recent work (Papernot et al.,
2017) that provides a tighter data-dependent differential privacy guarantee. The key idea is to produce
an output label based on voting from M teacher models, each trained on a disjoint subset of the
training data. We state the theorem below and provide the proof in Appendix B. Given datasets from
different domains, the below theorem provides a constructive proof to generate a differentially private
causal algorithm, following the method from Papernot et al. (2017).
Theorem 3.	Let D be a dataset generated from possibly a mixture of different distributions P(X, Y)
such that P(Y|XC) remains the same. Let nj be the votes for the jth class from M teacher models.
Let M be the mechanism that produces a noisy max, arg maxj {n7- + Lap(2∕γ)}. Then the privacy
budget for a causal model is lower than that for the associational model with the same accuracy.
3.2 Robustness to Membership Attacks
Deep learning models have shown to memorize or overfit on the training data during the learning
process (Carlini et al., 2018). Such overfitted models are susceptible to membership inference attacks
that can accurately predict whether a target input belongs to the training dataset or not (Shokri et al.,
2017). There are multiple variants of the attack depending on the information accessible to the
5
Under review as a conference paper at ICLR 2020 [Please do not distribute.]
adversary. An adversary with access to a black-box model only sees the confidence scores for the
predicted output whereas one with the white-box has access to the model parameters and observe
the output at each layer in the model (Nasr et al., 2018a). In the black-box setting, a membership
attack is possible whenever the distribution of output scores for training data is different from the test
data, and has been connected to model overfitting (Yeom et al., 2018). For the white-box setting, if
an adversary knows the true label for the target input, then they may guess the input to be a member
of the training set whenever the loss is lower, and vice-versa. Alternatively, if the adversary knows
the distribution of the training inputs, they may learn a “shadow” model based on synthetic inputs
and use the shadow model’s output to build a membership classifier for any new input (Salem et al.,
2018).
Most of the existing membership inference attacks have been demonstrated for test inputs from the
same data distribution as the training set. When test inputs are expected from the same distribution,
methods to reduce overfitting (such as adversarial regularization) can help reduce privacy risks (Nasr
et al., 2018b). However, in practice, this is seldom the case. For instance, in our example of a model
trained to detect HIV, the test inputs may come from different hospitals. Models trained to reduce the
generalization error for a specific test distribution are still susceptible to membership inference when
the distribution of features is changed. This is due to the problem of covariate shift that introduces a
domain adaptation error term (Mansour et al., 2009). That is, the loss-minimizing model that predicts
Y changes with a different distribution, and thus allows the adversary to detect differences in losses
for the test versus training datasets.As we show below, causal models alleviate the risk of membership
inference attacks. From Yeom et al. (2018), we first define a membership attack as:
Definition 6. Let model h be trained on a dataset S(X, Y) of size N. Let A be an adversary with
access to h and a input X. The advantage of an adversary in membership inference is the difference
between true and false positive rate in guessing whether the the input belongs to the training set.
Adv(A, h) = Pr[A = 1|b = 1] -Pr[A= 1|b = 0]	(13)
where b = 1 if the input is in the training set and 0 otherwise.
Lemma 2. [From Yeom et al. (2018)] Let M be a -differentially private mechanism based on a
model h. The membership advantage for an adversary is bounded by exp() - 1.
Theorem 4.	Given a structural causal network that connects X to Y, let S 〜P(X, Y) be a dataset
SamPledfrOm P, and let P* be any distribution such that P(Y∣Xc) = P*(Y∣Xc). Then, a causal model
hc trained on S yields lower membership advantage than an associational model ha even when the
test dataset is from a different distribution P* .
Proof. From Theorem 2 above, we can construct an c-DP mechanism based on a causal model, and
a a-DP mechanism based on an associational model, where c ≤ a. Further, this construction works
for different input distributions. From Lemma 2, the membership advantage of an adversary A is,
Adv(A, hc) ≤ exp(c) - 1	Adv(A, ha) ≤ exp(a) - 1	(14)
Thus, worst case advantage for a causal model is always lower than that of an associational model. 口
Corollary 2. Let hcm,iSn be a causal model trained using emPirical risk minimization on a dataset
S 〜P(X, Y) with sample size N. As N → ∞, membership advantage Adv( A, hmin) → 0.
The proof is the based on the result from Theorem 1 that h?? = h??* for a causal model. Crucially,
membership advantage does not go to zero as N → ∞ for associational models, since h?? = h!?*
in general. Detailed proof is in Appendix Section C.
Attribute Inference attacks. We prove similar results on the benefits of causal models for attribute
inference attacks in Appendix Section D.
4 Implementation and Evaluation
Table 1: Details of the benchmark datasets
Benchmark Datasets. To avoid errors in 						
learning causal structure from data, we per-	Dataset	Child	Alarm	(Sachs)	Water
	Output	XrayReport	-_BP^^	Akt	CKNI_12_45
form evaluation on datasets for which the					
	No. of classes	5	3	3	3
causal structure and the true conditional	Nodes	20	37	11	32
probabilities of the variables are known	Arcs	25	46	17	66
	Parameters	230	—	509	178	10083
6
Under review as a conference paper at ICLR 2020 [Please do not distribute.]
(a)
(b)
Attack accuracy for child dataset and XrayReport output
0.0	0.5	1.0	1.5	2.0
Amount of noise added to test distribution
(c)
Figure 2: Results for Child dataset with XrayReport as the output. ( a) is the target model accuracy. ( b) is the
attack accuracy for different dataset sizes on which the target model is trained and ( c) is the attack accuracy for
test distribution with varying amount of noise for total dataset size of 100K samples.
from prior research. We select 4 Bayesian
network datasets— Child, Sachs, Alarm and Water that range from 230-10k parameters (Table 1) (bnl).
Nodes represent the number of input features and arcs denote the causal connections between these
features in the network. Each causal connection is specified using a conditional probability table
P(Xi|Parents(Xi)); we consider these probability values as the parameters in our models. To create
a prediction task, we select a variable in each of these networks as the output Y . The number of
classes in Table 1 denote all the possible values for an output variable. For example, the variable BP
(blood pressure) in the alarm dataset takes 3 values i.e, LOW, NORMAL, HIGH. The causal model
uses only parents of Y whereas the associational model (DNN) uses all nodes except Y as features.
Implementation. We sample data using the causal structure and probabilities from the Bayesian
network, and use a 60:40% split for train-test datasets. We learn a causal model and a deep neural
network (DNN) on each training dataset. We implement the attacker model to perform membership
inference attack using the output confidences of both these models, based on past work (Salem et al.,
2018). The input features for the attacker model comprises of the output confidences from the target
model, and the output is membership prediction (member / non-member) in the training dataset of
the target model. In both the train and the test data for the attacker model, the number of members
and non-members are equal. The creation of the attacker dataset is described in Figure 4 in Appendix.
Note that the attack accuracies reported are an upper bound since we assume that the adversary has
white-box access to the ML model.
To train the causal model, we use the bnlearn library in R language that supports maximum likelihood
estimation of the parameters in Y ’s conditional probability table. For prediction, we use the parents
method to predict the class of any specific variable. To train the DNN model and the attacker model,
we build custom estimators in Python using Tensorflow v1.2 ten. The DNN model is a multilayer
perceptron (MLP) with 3 hidden layers of 128, 512 and 128 nodes respectively. The learning rate is
set to 0.0001 and the model is trained for 10000 steps. The attacker model has 2 hidden layers with 5
nodes each, a learning rate of 0.001, and is trained for 5000 steps. Both models use Adam optimizer,
ReLU for the activation function, and cross entropy as the loss function. We chose these parameters
to ensure model convergence.
4.1	Experimental Setup
We evaluate the DNN and the causal model sample sizes ranging from 1K to 1M dataset sizes. We
refer Test 1 as the test dataset which is drawn from the same distribution as the training data and Test
2 is generated from a completely different distribution except for the relationship of the output class
to its parents. To generate Test 2, we alter the true probabilities Pr(X) uniformly at random (later,
we consider adding noise to the original value). Our goal with generating Test 2 is to capture the
realistic behaviour of shift in distribution for input features. We refer the causal and DNN model as
the target on which the attack is perpetrated.
4.2	Results
We present results on the accuracy of target models (causal and DNN models) and the membership
attack accuracy for different dataset sizes and test distributions.
Accuracy comparison of DNN and Causal models. Figure 2a shows the target model accuracy
comparison for the DNN and the causal model trained on the Child dataset with XrayReport as the
output variable. We report the accuracy of the target models only for a single run since in practice the
attacker would have access to the outputs of only a single model. We observe that the DNN model
7
Under review as a conference paper at ICLR 2020 [Please do not distribute.]
(a)	(b)	(c)
Figure 3: (a)Target accuracy, (b) Attack accuracy, (c) Attack accuracy for true, learned causal model and DNN.
has a large difference between the train and the test accuracy (both Test 1 and Test 2) for smaller
dataset sizes (1K and 2K). This indicates that the model overfits on the training data for these dataset
sizes. However, after 10K samples, the model converges such that the train and Test 1 dataset have
the same accuracy. The accuracy for the Test 2 distribution stabilizes for a total dataset size of 10K
samples. In contrast, for the causal model, the train and Test 1 accuracy are similar for the causal
model even on smaller dataset sizes. However, after convergence at around 10K samples, the gap
between the accuracy of train and Test 2 dataset is the same for both the DNN and the causal model.
Figure 3a shows the accuracy comparison for all four datasets with similar results.
Attack Accuracy comparison of DNN and Causal models. A naive attacker classifier would
predict all the samples to be members and therefore achieve 0.5 prediction accuracy. Thus, we
consider 0.5 as the baseline attack accuracy which is equal to a random guess. Figure 2b shows the
attack accuracy comparison for Test 1 (same distribution) and Test 2 (different distribution) datasets.
Attack accuracy of the Test 1 dataset for the causal model is slightly above a random guess for smaller
dataset sizes, and then converges to 0.5. In comparison, attack accuracy for the DNN on Test 1 dataset
is over 0.6 for smaller samples sizes and reaches 0.5 after 10K datapoints. This confirms past work
that an overfitted DNN is susceptible to membership inference attacks even for test data generated
from the same distribution as the training data (Yeom et al., 2018). On Test 2, the attack accuracy
is always higher for the DNN than the causal model, indicating our main result that associational
models “overfit” to the training distribution, in addition to the training dataset. Membership inference
accuracy for DNNs is as high as 0.8 for total dataset size of 50K while that of causal models is below
0.6. Further, attack accuracy for DNN increases with sample size whereas attack accuracy for the
causal model reduces to 0.5 for total dataset size over 100k even when the gap between the train
and test accuracies is the same as DNNs ( as shown in Figure 2a). These results show that causal
models generalize better than DNNs across input distributions. Figure 3b shows a similar result for
all four datasets. The attack accuracy for DNNs and the causal model is close to 0.5 for the Test 1
dataset while for the Test 2 dataset the attack accuracy is significantly higher for DNNs than causal
model. This empirically confirms our claim that in general, causal models are robust to membership
inference attacks across test distributions as compared to associational models.
Attack Accuracy for Different Test Distributions. To understand the change in attack accuracy as
Pr(X) changes, we also generate test data from different distributions by adding varying amount of
noise to the true probabilities. We range the noise value between 0 to 2 and add it to the individual
probabilities which are then normalized to sum up to 1. Figure 2c shows the comparison of attack
accuracy for the causal model and the DNN on the child dataset for a total sample size of 100K
samples. We observe that the attack accuracy increases with increase in the noise values for the DNN.
Even for a small amount of noise, attack accuracies increase sharply. In contrast, attack accuracies
stay close to 0.5 for the causal model, demonstrating the robustness to membership attacks.
Results with learnt causal model. Finally, we perform experiments to understand the effect of
privacy guarantees on causal structures learned from data that might be different from the true causal
structure. We evaluate the attack accuracy for learned causal models on the Sachs, Child and
Alarm dataset2. For these datasets, a simple hill-climbing algorithm returned the true causal parents.
Hence, we evaluated attack accuracy for models with hand-crafted errors in learning the structure i.e.,
misestimation of causal parents, see Figure 3c. Specifically, we include two non-causal features as
parents of the output variable along with the true causal features. The attack risk increases as a learnt
model deviates from the true causal structure, however it still exhibits lower attack accuracy than the
corresponding associational model. Table 2 in Appendix gives a fine-grained analysis.
2We exclude the Water dataset as bn.fit in bnlearn library gives error due to the extreme probabilities.
8
Under review as a conference paper at ICLR 2020 [Please do not distribute.]
5	Related Work
Privacy attacks and defenses on ML models. Shokri et al. (2017) demonstrate the first membership
inference attacks on black box neural network models with access only to the confidence values.
Similar attacks have been shown on several other models such as GANs (Hayes et al., 2017), text
prediction generative models (Carlini et al., 2018; Song & Shmatikov, 2018) and federated learning
models (Nasr et al., 2018b). However, prior research does not focus on the severity of these attacks
with change in the distribution of the test dataset. We discussed in Section 2 that existing defenses
based on regularization (Nasr et al., 2018b) are not practical when models are evaluated on test
inputs from different distributions. Another line of defense is to add differentially private noise while
training the model. However, the values necessary to mitigate membership inference attacks in
deep neural networks require addition of large amount of noise that degrades the accuracy of the
output model (Rahman et al., 2018). Thus, there is a trade-off between privacy and utility when using
differential privacy for neural networks. In contrast, we show that causal models require lower amount
of noise to achieve the same differential privacy guarantees and hence retain accuracy closer to the
original model. Further, as training sample sizes become sufficiently large, as shown in Section 4,
causal models are, by definition, robust to membership inference attacks across distributions.
Causal learning and privacy.There is a substantial literature on learning causal models from data;
for a review see (Peters et al., 2017; Pearl, 2009). Kusner et al. (2015) proposed a method to privately
reveal parameters from a causal learning algorithm, using the framework of differential privacy.
Instead of a specific causal algorithm, our focus is on the privacy benefits of causal models for general
predictive tasks. While recent work applies causal models to study properties of machine learning
models such as providing explanations (Datta et al., 2016) or fairness (Kusner et al., 2017), the
relation of causality to privacy is yet unexplored. With this paper, we present the first result which
shows the privacy benefits of causal models.
6	Conclusion and Future Work
We conclude that causal learning is a promising approach to train models which are robust to privacy
attacks such as membership inference and model inversion. As our future work, we want to relax our
assumption of a known causal structure and investigate the privacy guarantees of causal models where
the causal features and the relationship between them is not known apriori (Peters et al., 2017).
References
Bayesian Network Repository. www.bnlearn.com/bnrepository/.
Tensorflow. https://www.tensorflow.org/.
Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Rosemary Ke, Sebastien Lachapelle, OleXa Bilaniuk,
Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle causal
mechanisms. arXiv preprint arXiv:1901.10912, 2019.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Nicholas Carlini, Chang Liu, Jernej Kos, Ulfar Erlingsson, and Dawn Song. The secret sharer:
Measuring unintended neural network memorization & eXtracting secrets. arXiv preprint
arXiv:1802.08232, 2018.
Anupam Datta, Shayak Sen, and Yair Zick. Algorithmic transparency via quantitative input influ-
ence: Theory and eXperiments with learning systems. In Security and Privacy (SP), 2016 IEEE
Symposium on, pp. 598-617. IEEE, 2016.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations
and TrendsR in Theoretical Computer Science, 9(3-4):211-407, 2014.
Andre Esteva, AleXandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo,
Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. A guide to deep
learning in healthcare. Nature medicine, 25(1):24, 2019.
Thomas Fischer and Christopher Krauss. Deep learning with long short-term memory networks for
financial market predictions. European Journal of Operational Research, 270(2):654-669, 2018.
9
Under review as a conference paper at ICLR 2020 [Please do not distribute.]
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence
information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Conference on
Computer and Communications Security, pp.1322-1333. ACM, 2015.
Jihun Hamm, Yingjun Cao, and Mikhail Belkin. Learning privately from multiparty data. In
International Conference on Machine Learning, pp. 555-563, 2016.
Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. Logan: Membership
inference attacks against generative models. arXiv preprint arXiv:1705.07663, 2017.
Matt J Kusner, Yu Sun, Karthik Sridharan, and Kilian Q Weinberger. Private causal inference. arXiv
preprint arXiv:1512.05469, 2015.
Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In Advances
in Neural Information Processing Systems, pp. 4066-4076, 2017.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds
and algorithms. arXiv preprint arXiv:0902.3430, 2009.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Razieh Nabi and Ilya Shpitser. Fair inference on outcomes. In Proceedings of the... AAAI Conference
on Artificial Intelligence. AAAI Conference on Artificial Intelligence, volume 2018, pp. 1931. NIH
Public Access, 2018.
Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning:
Stand-alone and federated learning under passive and active white-box inference attacks. arXiv
preprint arXiv:1812.00910, 2018a.
Milad Nasr, Reza Shokri, and Amir Houmansadr. Machine learning with membership privacy using
adversarial regularization. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and
Communications Security, pp. 634-646. ACM, 2018b.
Nicolas Papernot, Mardn Abadi, Ulfar Erlingsson, Ian Goodfellow, and KUnal Talwar. Semi-
supervised knowledge transfer for deep learning from private training data. In ICLR, 2017.
JUdea Pearl. Causality. Cambridge University press, 2009.
Jean-Philippe Pellet and Andre Elisseeff. Using markov blankets for causal structure learning. Journal
of Machine Learning Research, 9(JUl):1295-1342, 2008.
Jonas Peters, Peter Buhlmann, and Nicolai Meinshausen. Causal inference by using invariant
prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series
B (Statistical Methodology), 78(5):947-1012, 2016.
Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Elements of causal inference: foundations
and learning algorithms. MIT press, 2017.
Md Atiqur Rahman, Tanzila Rahman, Robert Laganiere, Noman Mohammed, and Yang Wang.
Membership inference attack against differentially private deep learning model. Transactions on
Data Privacy, 2018.
Ahmed Salem, Yang Zhang, Mathias Humbert, Mario Fritz, and Michael Backes. Ml-leaks: Model
and data independent membership inference attacks and defenses on machine learning models.
arXiv preprint arXiv:1806.01246, 2018.
Marco Scutari. Learning bayesian networks with the bnlearn r package. arXiv preprint
arXiv:0908.3817, 2009.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press, 2014. doi: 10.1017/CBO9781107298019.
10
Under review as a conference paper at ICLR 2020 [Please do not distribute.]
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks
against machine learning models. In Security and Privacy (SP), 2017 IEEE Symposium on, pp.
3-18.IEEE, 2017.
Congzheng Song and Vitaly Shmatikov. The natural auditor: How to tell if someone used your words
to train their model. arXiv preprint arXiv:1811.00513, 2018.
Avraam Tsantekidis, Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gabbouj, and
Alexandros Iosifidis. Using deep learning to detect price change indications in financial markets.
In 2017 25th European Signal Processing Conference (EUSIPCO),pp. 2511-2515. IEEE, 2017.
Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning:
Analyzing the connection to overfitting. In 2018 IEEE 31st Computer Security Foundations
Symposium (CSF), pp. 268-282. IEEE, 2018.
A Generalization Properties of Causal Model
A. 1 Generalization over Different Distributions
Theorem 1. Consider a structural causal graph G that connects X to Y, and causal features XC where
XC is a Markov Blanket of Y under G Let P(X, Y) and P* (X, Y) be two distributions with arbitrary
P(X) and P* (X) such that the causal relationship between XC and Y is preserved, which implies that
P(Y|XC) = P*(Y|XC). Let f : XC → Y be the resultant invariant labelling function such that y = f(XC).
Further, assume that HC represents the set of causal models hc : XC → Y that use all causal features
and HA represent the set of associational models ha : X → Y that may use all available features, such
that HC ⊆ HA and f ∈ HC.
Then, for any symmetric loss function L that obeys the triangle inequality, the upper bound of ODE
from a dataset S 〜P(X, Y) to P*(called ODE-Bound) for a causal model hc ∈ HC is less than or
equal to the upper bound ODE-Bound of an associational model ha ∈ HA, with probability at least
(1 - δ)2.
ODE-BoundP,P* (hc, f; δ) ≤ ODE-BoundP,p* (ha, f; δ)	(4)
Proof. Consider a model h : X → Y belonging to a set of models H, that was trained on S 〜P(X, Y).
From Def. 2 we write,
0DEp,p* (h, f) = Lp* (h, f) - LS〜p(h, f)
=Lp*(h, f) - Lp(h, f) + Lp(h, f) - LS-p(h, f)	(15)
= Lp* (h, f) - Lp(h, f) + IDEp(h, f)
where the last equation is to due to Def.1 of the in-distribution generalization error.
Let us denote the optimal loss-minimizing hypotheses over H for P and P* as hpOpT and hOpp*T.
hpOpT = arg min Lp(h, f)	hpOp*T = arg min Lp*(h, f)
Using the triangle inequality of the loss function, we can write:
Lp* (h, f) ≤ Lp* (h, hOppT) + Lp* (hOppT, hOp*pT) + Lp*(hOpp*T,f)
And,
Lp(h,f) ≥ Lp(h,hpOpT) -Lp(hpOpT,f)
⇒ -Lp(h,f) ≤ -Lp(h,hpOpT) +Lp(hpOpT,f)
Thus, combining Eq. 15, 17 and 18, we obtain,
ODEp,p* (h, f) ≤ IDEp(h,f) +Lp*(h,hOppT) +Lp*(hpOpT,hOpp*T) +Lp*(hpO*pT,f) - Lp(h,hOppT) + Lp(hpOpT,f)
= IDEp(h,f) + (Lp* (h, hOppT) - Lp(h,hpOpT)) + Lp*(hOp*pT,f) + Lp(hOppT,f) + Lp* (hpOpT, hpOp*T)
≤ IDEp(h,f) +discL,H(P,P*) +Lp*(hpO*pT,f) +Lp(hpOpT,f) +Lp*(hpOpT,hOp*pT)
(19)
where the last inequality is due to the definition of discrepancy distance (Definition 3). Equation 19
divides the out-of-distribution generalization error of a hypothesis h in four parts:
(16)
(17)
(18)
11
Under review as a conference paper at ICLR 2020 [Please do not distribute.]
1.	IDEP(h, f) denotes the in-distribution error of h. This can be bounded by typical generalization
bounds, such as the uniform error bound that depends only on the VC dimension and sample
size of S (Shalev-Shwartz & Ben-David, 2014). Using a uniform error bound based on the VC
dimension, we obtain, with probability at least 1 - δ,
I VCdim(H)(ln(2∣s∣) + l) + ln(4∕δ)	S t 厂、
IDE ≤ J8---------' 八 '	-----------=-l = IDE-Bound(H, S)	(20)
Since HC ⊆ HA , VC-dimension of causal models is not greater than that of associational models.
Thus,
VCDim(HC) ≤ VCDim(HA) ⇒ IDE-Bound(HC, S) ≤ IDE-Bound(HA, S)	(21)
2.	discL,H(P, P*) denotes the distance between the two distributions. Given two distributions, the
discrepancy distance does not depend on h, but only on the hypothesis class H.
3.	Lp* (hOPT, f) and Lp(hPPT, f) measure the error due to the true labeling function f being outside
the hypothesis class H.
4.	Lp* (hOppT, hOp*pT) denotes the loss (or difference) between the loss-minimizing function over P and
the loss-minimizing hypothesis over P*.
We next consider two classes of models, HC that contains models that uses all causal features (XC)
from a Markov Blanket over the structural causal graph, and HA that contains associational models
that may use all or a subset of all available features. Below we show that for a given distribution P
and another distribution P* such that P(Y|XC) = P*(Y|XC), the last term Lp(hOppT, hpO*pT) of Equation 19
vanishes for a causal model but may remain non-zero for an associational model.
Causal Model. Given a structural causal network, let us construct a model using all variables XC in
Y’s Markov Blanket. By property of the structural causal network, XC includes all parents of Y and
thus there are no backdoor paths, using Rule 2 of do-calculus from Pearl (2009):
Pr(Y|do(Xc = xc)) = P(Y|XC =xc) = P*(Y|XC =xc)	(22)
where the last equality is since data from P* also shares the same causal graph, (Y is independent of
X6C given XC in P* ). Since the conditional distribution of Y given XC features is the same across P
and P * , and the true labelling function f ∈ HC , the loss-minimizing models should also be the same.
Defining hOcppT = arg min Lp(hc, f) and hOcppT* = arg min Lp* (hc, f), we obtain,
hc∈HC	hc∈HC
hcO,ppT = hOc,ppT* = f	(23)
And therefore, the term Lp* (hOc,ppT, hcO,ppT* ) = 0. Also, Lp(hcO,ppT, f) = Lp* (hOc,ppT* , f) = 0.
Associational Model. In contrast, an associational model may use a subset of X, XS ⊆ X, that may
not include all variables in the Markov Blanket, or may include the Markov Blanket but also include
other extraneous variables. Let the set of such models be HAS . Note that we only consider HAS to
be the set of models for which Lp(haOps,Tp, f) = Lp* (hOaps,Tp* , f) = 0 (otherwise, the loss bound for a
model from HAS is trivially larger). This can happen when a feature xi ∈∕ XC does not belong to the
Markov Blanket under the causal graph, but belongs to the associational Markov Blanket under the
probability distribution over P or P*.
Then, by definition of	f,	f	∈∕	HAS .	Thus,	hOaps,Tp	6=	f	6=	haOps,Tp* .	Further, the loss-minimizing
hypotheses haOsp,Tp and haOps,Tp* depend on the different distributions P(Y|XS = xs) and P*(Y|XS = xs)
and thus not necessarily equal. Hence, haO,ppT 6= hOa,ppT* .
For the other subset of associational models, since XC ⊆ X, it is possible that an associational model
includes only the causal features of Y. Therefore, in general, Lp(hOa,ppT, haO,ppT* ) ≥ 0.
Loss Bounds. Hence, using Eq. 23, we write equation 19 for a causal model as:
ODEp,P* (hc, f) = Lp* (hc, f) - Ls~p(hc, f) ≤ discL,Hc(P, P*) + IDEp(hc, f)	(24)
Using Eqns. 20 and 21, we obtain, with probability at least 1 - δ:
ODEp,p* (hc, f) ≤ discL,HC (P, P*) + IDE-Boundp(He, S； δ)	(25)
≤ discL,HC (P, P*) + IDE-Boundp(HA,S; δ)	(26)
= ODE-Boundp,p* (hc, f; δ)	(27)
12
Under review as a conference paper at ICLR 2020 [Please do not distribute.]
Similarly, for an associational model, we obtain, with probability at least 1 - δ:
ODEp,P* (ha, f ) = Lp* (ha, f ) — LS〜p(ha, f )
≤ dis“,HA (P, P*) + IDE-BOUndp(Ha, S; δ) + Lp* (h??, h??*)
= ODE-Boundp,p* (ha, f; δ)
(28)
Finally, using Definition 3 for discrepancy distance, We can state HC ⊆ Ha ⇒ dis“,Hc (P, P*) ≤
discL,HA (P, P*). Therefore, from Eq. 24 and 28, we claim with probability (1 - δ)2,
ODE-BOundp,p* (hc, f; δ) ≤ ODE-BOundp,p* (ha, f; δ)	(29)
□
A.2 Generalization over a Single Datapoint
Corollary 1. Consider a causal model hc : XC → Y and an associational model ha : X → Y trained
on a dataset S 〜P. Let (x, y) ∈ S and (x0, y0) ∈ S be two input instances such that they share the
same true labelling function y = f(xc) and y0 = f(x0c). Then, the worst-case generalization error
for a causal model on any such x0 is less than or equal to that for an associational model. [Proof in
Appendix Section A.2]
max	Lx0 (hc, f) - Lx(hc, f) ≤ max	Lx0(ha,f) - Lx(ha,f)	(5)
x∈s,x0y=f (xc)	x∈s,x0y =f(χC)
Proof. Using the triangle inequality for the loss function, we obtain,
Lx0 (h, f) ≤ Lx0 (h, hOppT) +Lx0(hpOpT,hpO*pT)+Lx0(hOp*pT,f)	(30)
-Lx(h,f) ≤ -Lx (h, hOppT) +Lx(hpOpT,f)
Combining the two inequalities,
Lx0 (h, f) -Lx(h,f) ≤ Lx0(h,hpOpT) +Lx0(hpOpT,hOpp*T)+Lx0(hpOp*T,f)
- Lx(h, hOppT) + Lx(hpOpT, f)
≤distH(x,x0)+Lx0(hOpp*T,f)+Lx(hOppT,f)+Lx0(hOppT,hpO*pT)
where dist(x, x0) = max |Lx0 (h, h0) - Lx(h, h0)|, analogous to the discL for distributions.
h,h0∈H
(31)
(32)
For a causal model, we know that hcO,ppT = hOc,ppT* . Further, from Theorem 1 proof, Lx(hcO,ppT, f) =
Lx(hapT, f) = 0 and same for P* and loss on χ0.
Hence,
Lx0(hc, f) - Lx(hc, f) ≤ distHC (x, x0)	(33)
Lx0(ha,f) -Lx(ha,f) ≤distHA(x,x0)+Lx0(haO,ppT,haO,ppT*)	(34)
where distHC (x, x0) ≤ distHA (x, x0) since HC ⊆ HA.
Next, we show that these bounds are tight. That is, there exists an associational model ha whose
generalization error on x0 is exactly the RHS on Eqn. 34 and thus higher than the bound for any
causal model. Below we prove by constructing one such ha .
For simplicity in construction, let us select HC = {hOc,ppT, hcO,ppT* } and HA = {hOc,ppT, hcO,ppT* , haO,ppT, haO,ppT* }.
Thus distHC (x, x0) = 0 whereas distHA (x, x0) ≥ 0. We obtain the ODE-generalization bound for a
causal model,
Lx0(hc,f) -Lx(hc,f) ≤ Lx0(haO,ppT*,f)+Lx(hOa,ppT,f) = ODE-BOundx,x0(hc,f;HC)
Let us now construct an associational model, ha, such that:
Lχ0 (ha, f) = Lχ0(h?, hpPT) + Lχ0(hppτ, hp*τ) + Lχ0(hpPT, f)
-Lx (h?, f) = -Lχ(h?, hOpτ) + Lχ(hppτ, f)
(35)
(36)
13
Under review as a conference paper at ICLR 2020 [Please do not distribute.]
A simple construction for the above equalities is to select P and P*, and correspondingly Xt and χ0*
such that Lχt (h?pT，f) = Lχ,t (h??*, f) = 0 and Lχ,t (h??, h??*) > 0. Further, h] can be selected
such that Lχot (h[, hOPT) = 0 and Lχt (h], hPPT) = 0.
Then, using Eqn. 36,
Lχ0t (h1, f) — Lxt (ha, f) = Lχo(h?, hPPT)- Lχ(h[, hPPT) + Lxd (happ**, f) + Lxt 筑乎,f) + Lχ0t ”?,h??*)
=0+Lxot(haPT*, f)+Lxt (haP1T, f)+Lxot (haP1T, ha?*)
>Lx0t(hO?,PPT*,f)+Lxt(h?O,PPT,f)
=ODE-Boundx,xo(hc,f;HC)
(37)
where the last equality comes from Eqn. 35. Combining Eqns. 37 and 33, we obtain,
max Lxo (hc, f) - Lx(hc, f) ≤ maxODE-Boundx,xo(hc,f;HC) ≤ max Lxo (h?, f) - Lx(h?, f) (38)
x,xo	x,xo	x,xo
where the maximum is over X ∈ S and x0 ∈ S such that y0 = f (xC).	□
B Differential Privacy Guarantees with Tighter Data-dependent
Bounds
In this section we provide the differential privacy guarantee of a causal model based on a recent
method (Papernot et al., 2017) that provides tighter data-dependent bounds. We first present a Lemma
on generalization of causal models and then prove the main result.
As a consequence of Theorem 1, another generalization property of causal models is that the models
trained on data from two different distributions P (X) and P * (X) are likely to output the same value
for a new input.
Lemma 3. Under the conditions of Theorem 1, let hc,P be a causal model trained on distribution
P and let hc,P * be a model trained on P*. Similarly, let ha,P and ha,P * be correlational models
trained on P and P* respectively. Assume that the correlational and causal models report the same
accuracy α. Then for any new data input x,
Pr(hc,P (x) = hc,P * (x)) ≥ Pr(ha,P (x) = ha,P * (x))
As the size of the training sample N → ∞, the LHS→ 1.
Proof. Let haO,PPT = arg minh∈HA LP (h, f) and similarly let haO,PPT* = arg minh∈HA LP*(h, f) be
the loss-minimizing hypotheses under these two distributions, where H is the set of hypotheses.
We can analogously define hcO,PP T and hcO,PP*T . For a causal model, we know from Theorem 1 that
hcO,PP T = hcO,PP*T. As N → ∞, each of models on P and P* approach their loss-minimizing functions.
Then, for any input x,
L(hc,P (x), hc,P * (x)) = L(hcO,PP T (x), hcO,PP*T(x)) = 0	(39)
L(ha,P (x), ha,P * (x)) = L(haO,PP T (x), haO,PPT* (x)) ≥ 0	(40)
⇒ Pr(hc,P (x) = hc,P * (x)) = 1 ≥ Pr(ha,P (x) = ha,P * (x))	(41)
Under finite samples, let the accuracy of each model be α. Then for any new input, the causal models
predict the same output if they both match hcm,Pin = hcm,Pin* or both do not match. The probability
of this event can be captured by the accuracy of the models: α2 if both models match hcm,Pin, and
(1 - α)2 if they do not.
Pr(hc,P = hc,P * ) = α2 + (1 - α)2	(42)
For correlational models, haO,PPT 6= haO,PPT* for some x. For those x, assuming the same accuracy α,
Pr(h1 = h2) = 2α(1 - α)	(43)
Since 2α(1 - α) ≤ α2 + (1 - α)2 ∀α ∈ [0, 1], we have
Pr(hc,P = hc,P * ) ≥ Pr(ha,P = ha,P * )
□
14
Under review as a conference paper at ICLR 2020 [Please do not distribute.]
Based on the above generalization property, we now show that causal models provide stronger
differential privacy guarantees than corresponding associational models. We utilize the subsample
and aggregate technique (Dwork et al., 2014) that was extended for machine learning in Hamm et al.
(2016) and Papernot et al. (2017), for constructing a differentiably private model. The framework
considers M arbitrary teacher models that are trained on a separate subsample of the dataset without
replacement. Then, a student model is trained on some auxiliary unlabeled data with the (pseudo)
labels generated from a majority vote of the teachers. Differential privacy can be achieved by
either perturbing the number of votes for each class (Papernot et al., 2017), or perturbing the learnt
parameters of the student model (Hamm et al., 2016). For any new input, the output of the model is a
majority vote on the predicted labels from the M models. The privacy guarantees are better if a larger
number of teacher models agree on each input, since by definition the majority decision could not
have been changed by modifying a single data point (or a single teacher’s vote). Since causal models
generalize to new distributions, intuitively we expect causal models trained on separate samples to
agree more. Below we show that for a fixed amount of noise, a causal model is c-DP compared to
-DP for a associational model, where c ≤ .
Theorem 3. Let D be a dataset generated from possibly a mixture of different distributions P(X, Y)
such that P(Y|XC) remains the same. Let nj be the votes for the jth class from M teacher models.
Let M be the mechanism that produces a noisy max, arg maxj {n7- + Lap(2∕γ)}. Then the privacy
budget for a causal model is lower than that for the associational model with the same accuracy.
Proof. Consider a change in a single input example (x, y), leading to a new D0 dataset. Since
sub-datasets are sampled without replacement, only a single teacher model can change in D0 . Let n0j
be the vote counts for each class under D0 . Because the change in a single input can only affect one
model’s vote, |nj - n0j | ≤ 1.
Let the noise added to each class be r7-〜Lap(2∕γ). Let the majority class (class with the highest
votes) using data from D be i and the class with the second largest votes be j . Let us consider the
minimum noise r* required for class i to be the majority output under M over D. Then,
ni + r* > nj + rj
For i to have the maximum votes using M over D0 too, we need,
n0i + ri > n0j + rj
In the worst case, n0i = ni - 1 and n0j = nj + 1 for some j . Thus, we need,
ni - 1 + ri > nj + 1 + rj ⇒ ni + ri > nj + 2 + rj	(44)
which shows that r > r * +2. Note that r* > rj - (n - nj). We have two cases:
CASE I:	The noise rj < ni - nj, and therefore r* < 0. Writing Pr(i|D0) to denote the probability
that class i is chosen as the majority class under D0,
P(i∣D0) = P (ri ≥ r* + 2)	= 1 — 0.5exp(γ)exp( ^γr*)	(45)
= 1 - exp(γ)(1 - P(ri ≥ r*))	= 1 - exp(γ)(1 - P (i|D))	(46)
where the equations on the right are due to Laplace c.d.f. Using the above equation, we can write:
P(i∣D0)
P(i∣D)
exp(γ) +
1 - exp(γ)
P(iD)
exp(γ) +
1 - exp(γ)
P (ri ≥ r*)
≤ exp()
(47)
for some > 0. As P (i|D) = P(ri ≥ r*) increases, the ratio decreases and thus the effective
privacy budget () decreases. Thus, a model with a lower r* (effectively higher |r* |) will exhibit the
lowest .
Below, we show that |r* | is higher for a causal model, and thus P(ri ≥ r*) is higher. Intuitively, |r* |
is higher when there is more consensus between the M teacher models since |r* | is the difference
between the votes for the highest voted class with the votes for the second-highest class.
Let us consider two causal teacher models h1c and h2c, and two associational teacher models, h1 and
h2. From Lemma 3, for any new x, and for same accuracies of the models, there is more consensus
among causal models.
P(h1c(x) = h2c(x)) ≥ P (h1(x) = h2(x))	(48)
Hence rc* ≤ r * . From Equation 47, c ≤ .
15
Under review as a conference paper at ICLR 2020 [Please do not distribute.]
CASE II:	The noise r7- >= n - n7-, and therefore r* >= 0. Following the steps above, We obtain:
P(i∣D0) = P(r ≥ r* +2)
=exp(-γ)(P(ri ≥ r*))
0.5exp(一γ)exp(--γr*)	(49)
= exp(-γ)(P (i∣D))	(50)
Thus, the ratio does not depend on r*.
P(i∣D0)
P(i∣D)
exp(-γ)
(51)
Under CASE II when the noise is higher to the differences in votes between the highest and second
highest voted class, causal models provide the same privacy budget as associational models.
Thus, overall, c ≤ .
□
C Infinite S ample robustness to membership inference attacks
Corollary 2. Let hcm,iSn be a causal model trained using empirical risk minimization on a dataset
S 〜P(X, Y) with sample size N. As N → ∞, membership advantage Adv( A, hmin) → 0.
Proof. hmc,iSn can be obtained by empirical risk minimization.
1N
hc,s = arg mɪn LS〜P(h, f) = arg mɪn - E Lxi(h, f)
h∈HC	h∈HC N
i=1
(52)
As |S| = N → ∞, hcc,iSn → hOc,PPT. Suppose now that there exists another S0 of the same size such that
S0 〜P*. Then as ∣S01 → ∞, hmi台 → hOPr*.
From Theorem 1, hOPT = hOPp*. Thus,
lim
N→∞
lim
N→∞
(53)
Equation 53 implies that as N → ∞, the learnt hcc,iSn does not depend on the training set, as long as
the training set is sampled from any distribution P* such that P(Y|XC) = P*(Y|XC). That is, being
the global minimizer over distributions, hcc,iSn = hcO,PPT does not depend on its training set. Therefore,
hcc,iSn(x) is independent of whether x is in the training set.
lim Adv(A, hcc,iSn) = Pr(A = 1|b = 1) - Pr(A = 1|b = 0)
= E[A|b = 1] - E[A|b = 0] = E[A(hcc,iSn)|b = 1] - E[A(hcc,iSn)|b = 0] (54)
= E[A(hcc,iSn)] -E[A(hcc,iSn)] =0
where the second last equality follows since any function of hcc,iSn is independent of the training
dataset.	□
D	Robustness to Attribute Inference Attacks
In addition to revealing membership in the training set, a model may also reveal the value of individual
sensitive features of a test input, given partial knowledge of its features. For instance, given a training
dataset of HIV patients, an adversary may infer other attributes of a person (e.g., genetic information)
given that they know their demographics and other public features. As another example, it can
be possible to infer a person’s face based on hill climb on the output score for a face detection
model (Fredrikson et al., 2015). Model inversion is not always due to a fault in learning: a model may
learn a true, generalizable relationship between features and the outcome, but still be vulnerable to a
model inversion attack. This is because given (k-1) features and the true outcome label, it is possible
to guess the kth feature by brute-force search on output scores generated by the model. However,
inversion based on learning correlations between features, e.g., using some demographics to predict
disease, can be alleviated by causal models, since a feature will not be included in a model unless it
directly affects the outcome.
16
Under review as a conference paper at ICLR 2020 [Please do not distribute.]
Definition 7 (From Yeom et al. (2018)). Let h be a model trained on a dataset D(X,Y). Let A be an
adversary with access to h, and a partial test input xA ⊂ x. The attribute advantage of the adversary
is the difference between true and false positive rates in guessing the value of a sensitive feature
xs ∈/ xA. For a binary xs,
Adv(A, h) = Pr(A = 1|xs = 1) - Pr(A = 1|xs = 0)	(55)
Theorem 5. Given a dataset D(X, Y ) of size N and a structural causal network that connects X to
Y, a causal model hc makes it impossible to infer non-causal features.
Proof. The proof follows trivially from definition of a causal model. hc includes only causal features
during training. Thus, h(x) is independent of all features not in Xc.
Adv(A, h) = Pr(A = 1|xs = 1) - Pr(A = 1|xs = 0)	(56)
= Pr(A(h) = 1|xs = 1) - Pr(A(h) = 1|xs = 0) = Pr(A(h) = 1) - Pr(A(h) = 1) = 0
□
E Experiments
E.1	Dataset Distribution
The target model is trained using the synthetic training and test data generated using the bnlearn
library. We first divide the total dataset into training and test dataset in a 60:40 ratio. Further, the
output of the trained model for each of the training and test dataset is again divided into 50:50 ratio.
The training set for the attacker model consists of confidence values of the target model for the
training as well as the test dataset. The relation is explained in Figure 4. Note that the attacker model
is trained on the confidence output of the target models.
Figure 4: Dataset division for training target and attacker models.
E.2 Fine-grained attack analysis of learned causal models.
We expand the evaluation of the attack accuracy for a learnt causal model on the Sachs dataset
and Akt outcome. We increment the non-causal features to the learned model in addition to the
true causal features. Table 2 shows the attack and prediction accuracy for this model when trained
with the true causal, learned causal model with 1 and 2 non-causal features, and the results for the
corresponding DNN model.
	True Model	Learned Causal (2 causal +)		DNN
Acc. (%)	2 causal parents	1 non-causal parent	2 non-causal parents	
Attack	50	52	61	76
Pred.-	79	-	75	―	68.8	―	73
Table 2: Attack and Prediction accuracy comparison across models for Sachs dataset and Akt output variable.
17