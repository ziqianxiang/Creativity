Under review as a conference paper at ICLR 2020
Domain Aggregation Networks for
Multi-Source Domain Adaptation
Anonymous authors
Paper under double-blind review
Ab stract
In many real-world applications, we want to exploit multiple source datasets of
similar tasks to learn a model for a different but related target dataset - e.g.,
recognizing characters of a new font using a set of different fonts. While most
recent research has considered ad-hoc combination rules to address this prob-
lem, we extend previous work on domain discrepancy minimization to develop
a finite-sample generalization bound, and accordingly propose a theoretically jus-
tified optimization procedure. The algorithm we develop, Domain AggRegation
Network (DARN), is able to effectively adjust the weight of each source domain
during training to ensure relevant domains are given more importance for adap-
tation. We evaluate the proposed method on real-world sentiment analysis, digit
recognition and object recognition datasets and show that DARN can significantly
outperform the state-of-the-art alternatives.
1	Introduction
Many machine learning algorithms assume the learned predictor will be tested on the data from the
same distribution as the training data. This assumption, although reasonable, is not necessarily true
for many real-world applications. For example, patients from one hospital may have a different
distribution of gender, height and weight from another hospital. Consequently, a diagnostic model
constructed at one location may not be directly applicable to another location without proper adjust-
ment. The situation becomes even more challenging when we want to use data from multiple source
domains to build a model for a target domain, as this requires deciding, e.g., how to rank the source
domains and how to effectively aggregate these domains when training complex models like deep
neural networks. Including irrelevant or worse, adversarial, data from certain source domains can
severely reduce the performance on the target domain, leading to undesired consequences.
To address this problem, researchers have been exploring methods of transfer learning (Pan & Yang,
2009) or domain adaptation (Mansour et al., 2009a; Cortes et al., 2019), where a model is trained
based on labelled source data and unlabelled target data. Most existing works have focused on
single-source to single-target (“one-to-one”) domain adaptation, using different assumptions such
as covariate shift (Shimodaira, 2000; Gretton et al., 2009; Sugiyama & Kawanabe, 2012) or concept
drift (Jiang & Zhai, 2007; Gama et al., 2014). When dealing with multiple source domains, one may
attempt to directly use these approaches by combining all source data into a large joint dataset and
then apply one-to-one adaptation. This naive aggregation method will often fail as not all source
domains are equally important when transferring to a specific target domain. There are some works
on multi-source to single-target adaptation. Although many of them are theoretically motivated
with cross-domain generalization bounds, they either use ad-hoc aggregation rules when developing
actual algorithms (Zhao et al., 2018; Li et al., 2018) or lack finite-sample analysis (Mansour et al.,
2009b;c; Hoffman et al., 2018a). This leaves a gap between the theory for multi-source adaptation
and theoretically sound algorithm for domain aggregation.
This research has three contributions: First, we extend prior work on one-to-one adaptation using
discrepancy (Cortes et al., 2019) to develop a finite-sample cross-domain generalization bound for
multi-source adaptation. We show that in order to improve performance on the specific target do-
main, there is a trade-off between utilizing all source domains to increase effective sample size,
versus removing source domains that are underperforming or not similar to the target domain. Sec-
ond, motivated by our theory and domain adversarial method (Ganin & Lempitsky, 2015; Ganin
1
Under review as a conference paper at ICLR 2020
et al., 2016), we propose Domain AggRegation Network (DARN), that can effectively aggregate
multiple source domains dynamically during the course of training. Unlike previous works, our ag-
gregation scheme (Eq. (6)), which itself is of independent interest in some other contexts, is a direct
optimization of our generalization upper bound (Eq. (3)) without resorting to heuristics. Third, our
experiments on sentiment analysis, digit recognition and object recognition show that DARN can
significantly outperform state-of-the-art methods.
Section 2 introduces necessary background on one-to-one adaptation based on discrepancy. Then
Section 3 elaborates our theoretical analysis and the corresponding algorithm deployment. Section 4
discusses about related approaches in more detail, highlighting the key differences to the approach
developed here. Section 5 empirically compares the performance of the proposed method to other
alternatives. Finally, Section 6 concludes this work.
2	Background on Cross-domain Generalization
This section provides necessary background from previous work on one-to-one domain adapta-
tion (Cortes et al., 2019). Let X be the input space, Y ⊆ R be output space and H ⊆ {h : X 7→ Y}
be a hypothesis set. A loss function L : Y ×Y → R+ is μ-admissible1 if
∀y,y0 ,y0 ∈ Y	∣L(y0,y)-L(y00,y)l ≤ μ ∣y0-y00∣.	(1)
The discrepancy (Mansour et al., 2009a) between two distributions P, Q over X is defined as
disc(P, Q) =
max
h,h0∈H
|LP(h,h0)-LQ(h,h0)|
where LP(h, h0) = Ex〜P[L(h(x), h0(x))].
This quantity can be computed or approximated empirically given samples from both distributions.
For classification problems with 0-1 loss, it reduces to the well-known dA-distance (Kifer et al.,
2004; Blitzer et al., 2008; Ben-David et al., 2010) and can be approximated using a domain-classifier
loss w.r.t. H (Zhao et al., 2018; Ben-David et al., 2007, Sec.4), while for regression problems with
L2 loss, it reduces to a maximum eigenvalue (Cortes & Mohri, 2011, Sec.5); see Section 3.2 for
more details. For two domains (P, fP), (Q, fQ) where fP, fQ : X 7→ Y are the corresponding
labeling functions, we have the following cross-domain generalization bound:
Theorem 1 (Proposition 5 & 8, Cortes et al. (2019)) Let Rm (H) be the Rademacher complexity
of H given sample size m, HQ = {x 7→ L( h(x), fQ(x) ) : h ∈ H} be the set of functions mapping
x to its loss w.r.t. fQ and H,
ηH = μ X min max	|fp(x) — h(x)∣ + max ∣∕q(x) - h(x)∣ ,	(2)
h∈H x∈supp(Pb)	x∈supp(Qb)
be a constant measuring how well H can fit the true models where supp(P) means the support of
the empirical distribution P (using the μfrom Eq. (1)), and MQ 二 supx∈χ^∈u L( h(x), ∕q(x))
be the upper bound on loss for Q. Given Q with m points sampled iid from Q labelled according to
fQ,for δ ∈ (0, 1), ∀h ∈ H, w.p. at least 1 — δ,
LP(h,fp) ≤ Lq(h,fQ)+disc(P,Q) + 2Rm(HQ) + 〃丸 + MQ J"* .
This theorem provides away to generalize across domains when we have sample Q labelled accord-
ing to fQ and an unlabelled sample P. The first term is the usual loss function for the sample Q,
while the second term disc(P, Q) can be estimated based on the unlabelled data Q, P. ηH measures
how well the model family H can fit the example from the datasets, and it is not controllable once
H is given. The final term, as a function of sample size m, determines the convergence speed.
1For example, the common Lp losses are μ-admissible (Cortes et al., 2019, Lemma 23).
2
Under review as a conference paper at ICLR 2020
3	Domain Aggregation Networks
3.1	Theory
Suppose we are given k source domains {(Si, fSi) : i ∈ [k] d=ef {1, 2, . . . , k}} and a target domain
(T, fT) where Si, T are distributions over X and fSi , fT : X 7→ Y are their respective labelling
functions. For simplicity, assume that each sample Si has m points, drawn iid from Si and labelled
according to fSi . We are also given m unlabelled points T drawn iid from T . We want to leverage
all source domains’ information to learn a model h ∈ H minimizing LT (h, fT ).
One naive approach could be to combine all the source domains into a large joint dataset and Con-
duct one-to-one adaptation to the target domain using Theorem 1. However, including data from
irrelevant or even adversarial domains is likely to jeopardize the performance on the target domain,
which is sometimes referred to as negative transfer (Pan & Yang, 2009). Moreover, as certain source
domains may be more similar or relevant to the target domain than the others, it makes more sense
to adjust their importance according to their utilities. We propose to find domain weight αi ≥ 0
such that Pik=1 αi = 1 to achieve this. Our main theorem below sheds some light on how we should
combine the source domains (the proof is provided in Appendix A):
Theorem 2 Given k source domains datasets {(x(ji), yj(i)) : i ∈ [k], j ∈ [m]} with m iid examples
each where Sbi = {x(ji)} and yj(i) = fSi (x(ji)), for any α ∈ ∆ = {α : αi ≥ 0, Pi αi = 1}, δ ∈
(0, 1), and ∀h ∈ H, w.p. at least 1 - δ
LT(h,fT) ≤ X αi (LSi(h,fs) +disc(T,Si) +2Rm(HSi) + ηH,i) + IlaIl2MS ∖∣g2~^,
i
(3)
where HSi = {x 7→ L(h(x), fSi (x)) : h ∈ H} is the set of functions mapping x to the
corresponding loss, ηH,i is a constant similar to Eq. (2) with Q = Si , P = T and MS =
supi∈[k],x∈X,h∈H L(h(x), fSi (x)) is the upper bound on loss on the source domains.
There are several observations. (1) In the last term of the bound, m∕∣∣ɑ∣∣2 serves as the effective
sample size. Ifα is uniform (i.e., [1/k, . . . , 1/k]>), then the effective sample size is km; ifα is one-
hot, then we effectively only have m points from exactly one domain. (2) Rm(HSi) determines how
expressive the model family H is w.r.t. the source data Si. It can be estimated from samples (Bartlett
& Mendelson, 2002, Theorem 11), but the computation is non-trivial for a model family like deep
neural networks. The ηH,i is uncontrollable once the hypothesis class H (e.g., neural network archi-
tecture) is given. Using a richer H is not always beneficial. Richer H can reduce the ηH,i and also
help us find a better function h with smaller source losses LSb , but it will increase the Rm(HSi).
As removing these two terms does not seem to be empirically significant, we ignore them below
for simplicity. (3) Let gh,i d=ef LSb (h, fSi) + disc(T, Si). Small gh,i indicates that we can achieve
small loss on domain Si, and it is similar to the target domain (i.e., small disc(T, Si), estimated
from T, Si). We may want to emphasize on Si by setting αi close to 1, but this will reduce the
effective sample size. Therefore, we have to trade-off between the terms in this bound by choosing
a proper α. (4) When Si and T are only partially overlapped, it might be difficult to find a suitable
αi . In such cases, we can artificially split the given source domains into smaller datasets (e.g., by
using clustering) then apply our method on the finer scale. This strategy requires that the learning
algorithm has low computational complexities w.r.t. k, the number of source domains. As we will
see later in Section 3.3, this is indeed the case for our algorithm.
Before we proceed to develop an algorithm based on the theorem, let us compare Eq. (3) to existing
finite-sample bounds. The bound of Blitzer et al. (2008, Theorem 3) is informative when we have
access to a small set of labelled target examples. In such cases, we can improve our bound by
using this small labelled target set as an additional source domain Sk+1. We can also perform better
model selection using such labelled set. The bound of Zhao et al. (2018, Theorem 2) is based on the
dA-distance, which is a special case of disc in our bound. Moreover, our bound (Eq. (3)) use sample-
based Rademacher complexity, which is generally tighter than other complexity measures such as
VC-dimension (Bartlett & Mendelson, 2002; Koltchinskii et al., 2002; Bousquet et al., 2003).
3
Under review as a conference paper at ICLR 2020
3.2 Algorithm
In this section, we illustrate how to develop a practical algorithm based on Theorem 2. Ignoring the
constants, we would like to minimize the upper bound of Eq. (3):
min min Uh(α) = hgh,αi +τkαk2	(4)
h∈H α∈∆
where gh = [gh,1, . . . , gh,k]> and τ > 0 is a hyper-parameter. If we can solve the inner minimiza-
tion exactly given h, then We can treat the optimal α* (h) as a function of h and solve the outer
minimization over h effectively. In the following, we show how to achieve this.
Given gh, the inner minimization can be reformulated as a second-order cone programming prob-
lem, but it has no closed-form solution due to the τ kαk2 term. Consider the folloWing problem
(Z = -gh/τ recovers the inner minimization):
min	-hz, αi + kαk2	(5)
α∈∆
The Lagrangian for its dual problem is
Λ(α, λ, ν) = -z>α + kαk2 - λ>α + ν(1>α - 1) ν ∈ R, λ	0
Taking the derivative W.r.t. α and setting it to zero gives
∂Λ
不一=—z + α∕kα∣∣2 — λ + V1 = 0	=⇒	α /∣∣α ∣∣2 = Z — V1 + λ.
∂α
Notice that α = 0 so we have the constraint ∣∣z — V1 + λ∣2 = 1. Using this a* in A gives the
folloWing dual problem
max -ν s.t. kz - ν1 + λk2 = 1 and λ 0
ν,λ
We would like to decrease V as much as pos-
sible, and the best we can attain is the V* sat-
isfying k[z — v* 1] +1∣2 = 1 where [v]+ =
max(0, v). In this case, the optimal λ* can
be attained as λi* = 0 if zi — V* > 0, other-
wise λi* = V* — zi; see Fig. 1. Although we do
not have a closed-form expression for the op-
timal V*, we can use binary search to find it,
starting from the interval [zmin —1, zmax] where
zmin , zmax are the minimum and maximum of
z respectively. Then we can recover the primal
solution as
α* = [z-v * 1] + /∣[z — v * 1] + kι.	(6)
Figure 1: Optimal solution to Eq. (5) (best viewed
in color). λ* compensates what is below the V*,
while V* is chosen such that the vector of the
green bars has L2 norm of 1.
Eq. (6) gives rise to a new way to project any vector z to the probability simplex, which is of
independent interest and may be used in some other contexts.2 It resembles the standard projection
onto the simplex based on squared Euclidean distance (Duchi et al., 2008, Eq.(3)). One subtle but
crucial difference is that our Eq. (5) uses ∣∣α∣2 instead of ∣∣αk2. Recall that Z = — gh/τ. Here T
can be interpreted as a temperature parameter. On one hand, if τ 0, all z will have similar values
and thus the optimal V* will be close to zmax and α* will be close to uniform. On the other hand,
as τ → 0, zmax will stand out from the rest zi and eventually V* = zmax — 1. This means the gh,i
corresponding to the zmax is small enough so we focus solely on this domain and ignore all other
domains (even though this will reduce effective sample size as discussed in Section 3.1).
From a different perspective, if we define F^(α) = ∣∣ɑ∣2, then Eq. (5) equals — Fδ(z), where
F∆ , F∆* are convex conjugates of each other, where we use ∆ to emphasize their dependency on the
simplex domain. Then our objective Eq. (4) can be expressed as
min
h∈H
—Fδ (z) = —Fδ (—gh/τ)
2For example, if we consider z to be the logits of the final classification layer of a neural network, this
projection provides another way to produce class probabilities similar to the softmax transformation.
4
Under review as a conference paper at ICLR 2020
Figure 2: Model architecture with two source domains (best viewed in color). Mini-batches of xSi
and xT are fed to the network. xSi will go through the classification/regression path on the upper
hy box, while all x will go through to the corresponding discrepancies (in the lower hd box). The
gradients from the discrepancies will be reverted during backpropagation.
disc(Tb, Sbi) = 2 1 - min bT,Si (hd)
hd
We can optimize our objective and train a neural network h using gradient-based optimizer. The
optimal α*(h) is merely a function of h and We can backprop through it. To facilitate the gradient
computation and show that we can efficiently backprop through the projection of Eq. (6), we derive
the Jacobian J = ∂ɑ∕∂Z in Appendix B. This ensures effective end-to-end training.
NoW We elaborate more on hoW to compute gh,i. The LSb (h, fSi) is the task loss. The disc(T, Si)
depends on Whether the task is classification or regression. For classification, disc coincides With
the dA-distance, so We use the domain classification loss (Zhao et al., 2018; Ben-David et al., 2007):
1	2m
bT,Si (hd) = 2m∑ Ihd(X) - δx∈T
i=1
Where bT,Si (hd) is the sample domain classification loss of a domain classifier hd : X 7→ {0, 1}.
This minimization over hd Will become maximization once We move it outside of the disc due to
the minus sign. Then our objective consists of minh and maxhd, Which resembles adversarial train-
ing (GoodfelloW et al., 2014): learning a task classifier h to minimize loss and a domain classifier
hd to maximize domain confusion. More specifically, if We decompose the neural netWork h into a
feature extractor hfea and a label predictor hy (i.e., h(x) = hy (hfea(x))), We can learn a domain-
classifier hd on top of hfea to classify hfea(x) betWeen Si and T as a binary classification problem,
Where the domain itself is the label (see Fig. 2). To achieve this, We use the logistic loss to approxi-
mate bT,Si (hd) and apply the gradient reversal layer (Ganin & Lempitsky, 2015; Ganin et al., 2016)
When optimizing disc through backpropagation.
IfWe are solving regression problems With L2 loss, then disc(T, Si) = kMT - MSi k2 is the largest
eigenvalue (in magnitude) of the difference of two matrices MT = A Pj hfea(XjT))h>a(XjT)) and
MSi = mm Pj hfea(Xji) )h>a(Xji)) (Mansour et al.,2009a; Cortes & Mohri, 2011, Sec.5), which can
be conveniently approximated using mini-batches and a few steps of power iteration.
3.3 Complexity
Here we analyze the time and space complexities of the algorithm in each iteration. Similar to
MDAN (Zhao et al., 2018), in each gradient step, we need to compute the task loss LSb (h, fSi) and
the disc(T, Si) (or dA-distance) using mini-batches from each source domain i ∈ [k]. The question
is whether one can maintain the O(k) complexity given that we need to compute the weights using
Eq. (6) and backprop through it. For the forward computation of α*, in order to compute the
threshold V* to the e > 0 relative precision, the binary search will cost O(k log(1∕e)). As for the
backward pass of gradient computation, according to our calculation in Appendix B, the Jacobian
J = ∂α∕∂z has a concise form, meaning that it is possible to compute the matrix-vector product
Jv for a given vector v in O(k) time and space. Therefore, our space complexity is the same as
MDAN but our time complexity is slightly slower by a factor of log(1∕). In comparison, the time
5
Under review as a conference paper at ICLR 2020
complexity for MDMN (Li et al., 2018) is O(k2) because it requires computing the pairwise weights
within the k source domains. When we have a lot of source domains, MDMN will be noticeably
slower than MDAN and our DARN.
4	Related Work
The idea of utilizing data from the source domain (S, fS) to train a model for a different but related
target domain (T, fT ) has been explored extensively for the last decade using different assump-
tions (Pan & Yang, 2009; Zhang et al., 2015). For instance, the covariate shift scenario (Shimodaira,
2000; Gretton et al., 2009; Sugiyama & Kawanabe, 2012; Wen et al., 2014) assumes S 6= T but
fS = fT, while concept drift (Jiang & Zhai, 2007; Gama et al., 2014) assumes S = T but fS 6= fT.
More specifically, Theorem 1 (Cortes et al., 2019) shows that both covariate shift (as measured by
the discrepancy disc) and model misspecification (as controlled by ηH) contribute to the adaptation
performance (Wen et al., 2014).
Finding a domain-invariant feature space by minimizing a distance measure is common practice in
domain adaptation, especially for training neural networks. Tzeng et al. (2017) provided a com-
prehensive framework that subsumes several prior efforts on learning shared representations across
domains (Tzeng et al., 2015; Ganin et al., 2016). DARN uses adversarial domain classifier and the
gradient reversal trick from Ganin et al. (2016). Instead of proposing a new loss for each pair of
the source and target domains, our main contribution is the aggregation technique of computing the
mixing coefficients α, which is derived from theoretical guarantees. When dealing with multiple
source domains, our aggregation method can certainly be applied to other forms of discrepancies
such as MMD (Gretton et al., 2012; Long et al., 2015; 2016), and other model architectures such
as Domain Separation Network (Bousmalis et al., 2016), cycle-consistent model (Hoffman et al.,
2018b), class-dependent adversarial domain classifier (Pei et al., 2018) and Known Unknown Dis-
crimination (Schoenauer-Sebag et al., 2019).
Our work focuses on multi-source to single-target adaptation, which has been investigated in the
literature. Sun et al. (2011) developed a generalization bound but resorted to heuristic algorithms
to adjust distribution shifts. Zhao et al. (2018) proposed a certain ad-hoc scheme for the combina-
tion coefficients α, which, unlike ours, are not theoretically justified. Multiple Domain Matching
Network (MDMN) (Li et al., 2018) computes domain similarities not only between the source and
target domains but also within the source domain themselves based on Wasserstein-like measure.
Calculating such pairwise weights can be computationally demanding when we have a lot of source
domains. Their bound requires additional smooth assumptions on the labelling functions fSi , fT,
and is not a finite-sample bound, as opposed to ours. As for the actual algorithm, they also use ad-
hoc coefficients α without theoretical justification. Mansour et al. (2009b;c) consider multi-source
adaptation where T = Pi βiSi is a convex mixture of source distributions with some weights βi .
Our analysis does not require this assumption. Hoffman et al. (2018a) provides similar guarantees
with different assumptions, but unlike ours, their bounds are not finite-sample bounds.
5	Experiments
In this section, we compare DARN with several other baselines and state-of-the-art methods for the
popular tasks: sentiment analysis, digit recognition and object recognition. The following methods
are compared. (1) The SRC (for source) method uses only labelled source data to train the model. It
merges all available source examples to form a large dataset to perform training without adaptation.
(2) TAR (for target) is another baseline that uses only m labelled target data instances. It serves
as upper bound of the best we can achieve if we had access to the true label of the target data.
(3) Domain Adversarial Neural Network (DANN) (Ganin et al., 2016) is similar to our method in
that we both use adversarial training objectives. It is not obvious how to adapt DANN to the multi-
source setting. Here we follow previous protocol (Zhao et al., 2018) and merge all source data to
form a large joint source dataset of km instances for DANN to perform adaptation. (4) Moment
Matching for Multi-Source Domain Adaptation (M3SDA) (Peng et al., 2019) is a recent state-of-
the-art method that combines moment matching and maximizing classifier discrepancy (Saito et al.,
2018). We use their public code with a few necessary adjustments (change classification head based
on the number of classes etc.) (5) Multisource Domain Adversarial Network (MDAN) (Zhao et al.,
6
Under review as a conference paper at ICLR 2020
2018) resembles our method in that we both dynamically assign each source domain an importance
weight during training. However, unlike ours, their weights are not theoretically justified. We use
the soft version of MDAN since they reported that it performs better than the hard version, and
we use their code. (6) Multiple Domain Matching Network (MDMN) (Li et al., 2018) computes
weights not only between source and target domains but also within source domain themselves. We
use their code of computing weights in our implementation. All the methods are applied to the same
neural network structure to ensure fair comparison. Our PyTorch implementation will be available
online after acceptance.
5.1	Sentiment Analysis
Setup. We use the Amazon review dataset (Blitzer et al., 2007; Chen et al., 2012) that consists of
positive and negative product reviews from four domains (Books, DVD, Electronics and Kitchen).
Each of them is used in turn as the target domain and the other three are used as source domains.
Their sample sizes are 6465, 5586, 7681, 7945 respectively. We follow the common protocol (Chen
et al., 2012; Zhao et al., 2018) of using the top-5000 frequent unigrams/bigrams of all reviews as
bag-of-words features and train a fully connected model (MLP) with [1000, 500, 100] hidden units
for classifying positive versus negative reviews. The dropout drop rate is 0.7 for the input and
hidden layers. In each run, we randomly sample 2000 reviews from each domain as labelled source
or unlabelled target training examples, while the remaining instances are used as test examples for
evaluation. The hyper-parameters are chosen based on cross-validation. The model is trained for 50
epochs and the mini-batch size is 20 per domain. The optimizer is Adadelta with a learning rate of
1.0. The soft version of MDAN has an additional parameter Y = 1∕τ which is the inverse of our
temperature τ. The chosen parameters are γ = 10.0 for MDAN and γ = 0.9 for our DARN, which
are selected from a wide range of candidate values.
Results and Analysis. Table 1 summarizes the classification accuracies. The last column is the
average accuracy of four domains, and the standard errors are calculated based on 20 runs. (1) Most
of the time, DANN with joint source data performs worse than SRC which has no adaptation. This
may be because it does not adjust for the each source domain, and as a result, it fails to ignore
irrelevant data to avoid negative transfer. (2) Some domains are harder to adapt to than the others.
For example, the accuracies of SRC and TAR on the Electronics domain are very close to each
other, indicating that this requires little to no adaptation. Yet, our DARN is the closest to the TAR
performance here. The Books domain is more challenging as the improvement over the SRC method
is small, even though there exists a large gap between SRC and TAR. (3) DARN is always within the
best performing methods and significantly outperforms others in the Electronics domain. Note that
MDMN additionally computes similarities within source domains in each iteration, which can be
computationally expensive (O(k2) per iteration) if the number of source domains is large. Instead,
our method focuses on the discrepancy between source and target domains (O(k) per iteration) and
can achieve similar performance on this problem.
Table 1: Classification accuracy (%) of the target sentiment datasets. Mean and standard error over
20 runs. The best method(s) (excluding TAR) based on one-sided Wilcoxon signed-rank test at the
5% significance level is(are) shown in bold for each domain.
Method	Books	DVD	Electronics	Kitchen	Avg.
SRC	79.15 ± 0.39	80.38 ± 0.30	85.48 ± 0.10 二	85.46 ± 0.34	82.62 ± 0.20
DANN	79.13 ± 0.29	80.60 ± 0.29	85.27 ± 0.14	85.56 ± 0.28	82.64 ± 0.14
M3SDA	79.42 ± 0.17	80.82 ± 0.35	85.52 ± 0.19	86.45 ± 0.43	83.05 ± 0.14
MDAN	79.99 ± 0.20	81.66 ± 0.19	84.76 ± 0.17	86.82 ± 0.13	83.31 ± 0.08
MDMN	80.13 ± 0.20	81.58 ± 0.21	85.61 ± 0.13	87.13 ± 0.11	83.61 ± 0.07
DARN	79.93 ± 0.19	81.57 ± 0.16	85.75 ± 0.16	87.15 ± 0.14	83.60 ± 0.08
TAR	84.10 ± 0.13	83.68 ± 0.12	86.11 ± 0.32	88.72 ± 0.14	85.65 ± 0.09
5.2	Digit Recognition
Setup. Following the setting from previous work (Ganin et al., 2016; Zhao et al., 2018), we use the
four digit recognition datasets in this experiment (MNIST, MNIST-M, SVHN and Synth). MNIST
7
Under review as a conference paper at ICLR 2020
is a well-known gray-scale images for digit recognition, and MINST-M (Ganin & Lempitsky, 2015)
is a variant where the black and white pixels are masked with color patches. Street View House
Number (SVHN) (Netzer et al., 2011) is a standard digit dataset taken from house numbers in Google
Street View images. Synthetic Digits (Synth) (Ganin & Lempitsky, 2015) is a synthetic dataset that
mimic SVHN using various transformations. One of the four datasets is chosen as unlabelled target
domain in turn and the other three are used as labelled source domains.
MNIST images are resized to 32 × 32 and represented as 3-channel color images in order to match
the shape of the other three datasets. Each domain has its own given training and test sets when
downloaded. Their respective training sample sizes are 60000, 59001, 73257, 479400, and the re-
spective test sample sizes are 10000, 9001, 26032, 9553. In each run, 20000 images are randomly
sampled from each domain’s training set as actual labelled source or unlabelled target training ex-
amples, and 9000 images are randomly sampled from each domain’s test set as actual test examples
for evaluation. The model structure is shown in Appendix D. There is no dropout and the hyper-
parameters are chosen based on cross-validation. It is trained for 50 epochs and the mini-batch size
is 128 per domain. The optimizer is Adadelta with a learning rate of 1.0. Validation selected γ = 0.5
for MDAN and γ = 0.1 for DARN.
Results and Analysis. Table 2 shows the classification accuracy of each target dataset over 20
runs. (1) All methods except DANN can consistently outperform SRC. Again, without proper ad-
justment for each source domain, DANN with joint source data can perform worse than SRC. This
suggests the importance of ignoring irrelevant data to avoid negative transfer. (2) DARN signifi-
cantly outperforms MDAN and MDMN across all four domains, especially on the MNIST-M and
SVHN domains. Notice that even though MDAN and MDMN have generalization guarantees, they
both resort to ad-hoc aggregation rules to combine the source domains during training. Instead, our
aggregation (Eq. (6)) is a direct optimization of the upper bound (Theorem 2) thus is theoretically
justified and empirically superior for this problem.
Table 2: Classification accuracy (%) of the target digit datasets. Mean and standard error over 20
runs. The best method (excluding TAR) based on one-sided Wilcoxon signed-rank test at the 5%
significance level is shown in bold for each domain.
Method	MNIST	MNIST-M	SVHN	Synth	Avg.
SRC	96.78 ± 0.08	60.80 ± 0.21	68.99 ± 0.69	84.09 ± 0.27	77.66 ± 0.14
DANN	96.41 ± 0.13	60.10 ± 0.27	70.19 ± 1.30	83.83 ± 0.25	77.63 ± 0.35
M3SDA	96.95 ± 0.06	65.03 ± 0.80	71.66 ± 1.16	80.12 ± 0.56	78.44 ± 0.36
MDAN	97.10 ± 0.10	64.09 ± 0.31	77.72 ± 0.60	85.52 ± 0.19	81.11 ± 0.21
MDMN	97.15 ± 0.09	64.34 ± 0.27	76.43 ± 0.48	85.80 ± 0.21	80.93 ± 0.16
DARN	98.09 ± 0.03	67.06 ± 0.14	81.58 ± 0.14	86.79 ± 0.09	83.38 ± 0.06
TAR	99.02 ± 0.02	94.66 ± 0.10	87.40 ± 0.17	96.90 ± 0.09	94.49 ± 0.07
5.3	Object Recognition: Office-Home
To showcase the applicability of our method to more complicated real-world tasks, we use the chal-
lenging Office-Home dataset (Venkateswara et al., 2017). It contains images of 65 everyday objects
such as spoon, sink, mug and pen from four different domains: Art, Clipart, Product and Real-World.
One of the four datasets is chosen as unlabelled target domain in turn and the other three are used as
labelled source domains.
The respective sample sizes are 2427, 4365, 4439, 4357. In each run, 2000 images are randomly
sampled from each domain as labelled source or unlabelled target training examples, and the rest
images are used as test images for evaluation. We use the ResNet50 He et al. (2016) pretrained from
the ImageNet in PyTorch as the base network for feature learning and put an MLP with [1000, 500,
100, 65] units on top for classification. It is trained for 50 epochs and the mini-batch size is 32 per
domain. The optimizer is Adadelta with a learning rate of 1.0. MDAN uses γ = 1.0 while DARN
uses γ = 0.5.
Table 3 shows the classification accuracy of each target dataset over 20 runs. Most existing works
in the literature focused on single-source to single-target adaptation on this problem (e.g., see (Long
et al., 2018)). Compared to them, using multi-source methods can significantly boost performance,
8
Under review as a conference paper at ICLR 2020
revealing the importance of using multiple source domains when possible. Even though this is a
significantly more challenging problem with more classes and much fewer images compared to the
digits datasets, our DARN achieves state-of-the-art performance in this setting, excelling existing
methods by a noticeable margin.
Table 3: Classification accuracy (%) of the Office-Home datasets. Mean and standard error over 20
runs. The best method (excluding TAR) based on one-sided Wilcoxon signed-rank test at the 5%
significance level is shown in bold for each domain.
Method	Art	Clipart	Product	Real-World	Avg.
SRC	58.02 ± 0.47	57.29 ± 0.30	74.26 ± 0.22	77.98 ± 0.25	66.89 ± 0.16
DANN	57.39 ± 0.69	57.35 ± 0.35	73.78 ± 0.27	78.12 ± 0.21	66.66 ± 0.19
M3SDA	64.05 ± 0.61	62.79 ± 0.37	76.21 ± 0.30	78.63 ± 0.22	70.42 ± 0.18
MDAN	68.14 ± 0.58	67.04 ± 0.21	81.03 ± 0.22	82.79 ± 0.15	74.75 ± 0.18
MDMN	68.67 ± 0.55	67.75 ± 0.20	81.37 ± 0.18	83.32 ± 0.14	75.28 ± 0.15
DARN	70.00 ± 0.38	68.42 ± 0.14	82.75 ± 0.21	83.88 ± 0.16	76.26 ± 0.13
TAR	71.19 ± 0.38	79.16 ± 0.16	90.66 ± 0.15	85.60 ± 0.14	81.65 ± 0.12
5.4	Visualizing Domain Importance
To demonstrate how DARN can effectively aggregate multi-
ple source domains, we visualize the source domain weights
(i.e., α in DARN) for the Amazon dataset. We also com-
pared to the weights produced by MDMN, using the original
authors’ code.
Fig. 3 and Fig. 4 compare the evolution of source domain
weights during training. In each subfigure, every row corre-
sponds to the weights of the source domains when learning
for one target domain. The white stripe indicates there is no
target domain weight and darker stripe means less weight.
They are evaluated at the end of each epoch over 50 epochs.
To avoid noisy values due to small mini-batch size, the val-
ues are exponential moving averages with a decay rate of
0.95. (1) For DARN, as Electronics and Kitchen are more
related to each other than Books and DVD, their respec-
tive weights remain higher during training. This is reason-
able since we have overlapping products (e.g., blenders) in
both domains. The α is changing dynamically during train-
ing, showing the flexibility of our method to adjust domain
importance when needed. (2) In comparison, the domain
weights produced by MDMN are not very stable. We take
a closer look at the MDMN weights and notice that they
change drastically, especially towards the end of the train-
ing. It can produce alternating one-hot vectors α, changing
from one domain to a different domain and ignoring the rest.
This instability makes their domain weights hard to interpret.
Figure 3: Domain weights of DARN
for the Amazon data.
Figure 4: Domain weights of MDMN
for the Amazon data.
6	Conclusion and Future Work
Our work uses the discrepancy (Mansour et al., 2009a; Cortes et al., 2019) to derive a finite-sample
generalization bound for multi-source to single-target adaptation. Our theorem shows that, in order
to achieve the best possible generalization upper bound for a target domain, we need to trade-off be-
tween including all source domains to increase effective sample size and removing source domains
that are underperforming or not similar to the target domain. Based on this observation, we derive
an algorithm, Domain AggRegation Network (DARN), that can dynamically adjust the weight of
each source domain during end-to-end training. Experiments on sentiment analysis, digits recog-
nition and object recognition show that DARN outperforms state-of-the-art alternatives. Recent
9
Under review as a conference paper at ICLR 2020
analysis (Zhao et al., 2019; Johansson et al., 2019) show that solely focusing on learning domain
invariant features can be problematic when the marginal label distributions on Y between source and
target domains are significantly different. Thus it makes sense to take ηH into consideration when a
small amount of labelled data is available for the target domain, which we will explore in the future.
References
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
for domain adaptation. In Advances in neural information processing systems, pp. 137-144, 2007.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175,
2010.
John Blitzer, Mark Dredze, and Fernando Pereira. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classification. In Proceedings of the 45th annual
meeting of the association of computational linguistics, pp. 440-447, 2007.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman. Learning
bounds for domain adaptation. In Advances in neural information processing systems, pp. 129-
136, 2008.
Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan.
Domain separation networks. In Advances in Neural Information Processing Systems, pp. 343-
351, 2016.
Olivier Bousquet, StePhane Boucheron, and Gabor Lugosi. Introduction to statistical learning the-
ory. In Summer School on Machine Learning, pp. 169-207. Springer, 2003.
Minmin Chen, Zhixiang Xu, Kilian Q Weinberger, and Fei Sha. Marginalized denoising autoen-
coders for domain adaptation. In Proceedings of the 29th International Coference on Interna-
tional Conference on Machine Learning, pp. 1627-1634. Omnipress, 2012.
Corinna Cortes and Mehryar Mohri. Domain adaptation in regression. In International Conference
on Algorithmic Learning Theory, pp. 308-323. Springer, 2011.
Corinna Cortes, Mehryar Mohri, and Andres Munoz Medina. Adaptation based on generalized
discrepancy. The Journal of Machine Learning Research, 20(1):1-30, 2019.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efficient projections onto
the l 1-ball for learning in high dimensions. In Proceedings of the 25th international conference
on Machine learning, pp. 272-279. ACM, 2008.
Joao Gama, Indre Zliobaite, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid Bouchachia. A
survey on concept drift adaptation. ACM computing surveys (CSUR), 46(4):44, 2014.
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In
International Conference on Machine Learning, pp. 1180-1189, 2015.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. The Journal of Machine Learning Research, 17(1):2096-2030, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, and Bernhard
Scholkopf. Covariate shift by kernel mean matching. DataSet shift in machine learning, 3(4):5,
2009.
10
Under review as a conference paper at ICLR 2020
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. Journal ofMachine Learning Research,13(Mar):723-773, 2012.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Judy Hoffman, Mehryar Mohri, and Ningshan Zhang. Algorithms and theory for multiple-source
adaptation. In Advances in Neural Information Processing Systems, pp. 8246-8256, 2018a.
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros,
and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In International
Conference on Machine Learning, pp. 1994-2003, 2018b.
Jing Jiang and ChengXiang Zhai. Instance weighting for domain adaptation in NLP. In Proceedings
of the 45th annual meeting of the association of computational linguistics, pp. 264-271, 2007.
Fredrik Johansson, David Sontag, and Rajesh Ranganath. Support and invertibility in domain-
invariant representations. In The 22nd International Conference on Artificial Intelligence and
Statistics, pp. 527-536, 2019.
Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In Proceed-
ings of the Thirtieth international conference on Very large data bases-Volume 30, pp. 180-191.
VLDB Endowment, 2004.
Vladimir Koltchinskii, Dmitry Panchenko, et al. Empirical margin distributions and bounding the
generalization error of combined classifiers. The Annals of Statistics, 30(1):1-50, 2002.
Yitong Li, David E Carlson, et al. Extracting relationships by multi-domain matching. In Advances
in Neural Information Processing Systems, pp. 6798-6809, 2018.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with
deep adaptation networks. In International Conference on Machine Learning, pp. 97-105, 2015.
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation
with residual transfer networks. In Advances in Neural Information Processing Systems, pp. 136-
144, 2016.
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial
domain adaptation. In Advances in Neural Information Processing Systems, pp. 1640-1650, 2018.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds
and algorithms. In 22nd Conference on Learning Theory, COLT 2009, 2009a.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation with multiple
sources. In Advances in neural information processing systems, pp. 1041-1048, 2009b.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Multiple source adaptation and the
Renyi divergence. In UAI, pp. 367-374, 2009c.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
and Unsupervised Feature Learning 2011, 2011.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge
and data engineering, 22(10):1345-1359, 2009.
Zhongyi Pei, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Multi-adversarial domain adap-
tation. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching
for multi-source domain adaptation. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 1406-1415, 2019.
11
Under review as a conference paper at ICLR 2020
Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier dis-
crepancy for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Com-
Puter Vision and Pattern Recognition, pp. 3723-3732, 2018.
Alice Schoenauer-Sebag, Louise Heinrich, Marc Schoenauer, Michele Sebag, Lani Wu, and Steve
Altschuler. Multi-domain adversarial learning. In International Conference on Learning Repre-
sentations, 2019.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of statistical planning and inference, 90(2):227-244, 2000.
Masashi Sugiyama and Motoaki Kawanabe. Machine learning in non-stationary environments:
Introduction to covariate shift adaptation. MIT press, 2012.
Qian Sun, Rita Chattopadhyay, Sethuraman Panchanathan, and Jieping Ye. A two-stage weighting
framework for multi-source domain adaptation. In Advances in neural information processing
systems, pp. 505-513, 2011.
Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across
domains and tasks. In Proceedings of the IEEE International Conference on Computer Vision,
pp. 4068-4076, 2015.
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 7167-7176, 2017.
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep
hashing network for unsupervised domain adaptation. In (IEEE) Conference on Computer Vision
and Pattern Recognition (CVPR), 2017.
Junfeng Wen, Chun-Nam Yu, and Russell Greiner. Robust learning under uncertain test distributions:
Relating covariate shift to model misspecification. In ICML, pp. 631-639, 2014.
KUn Zhang, Mingming Gong, and Bernhard Scholkopf. Multi-source domain adaptation: A causal
view. In Twenty-ninth AAAI conference on artificial intelligence, 2015.
Han Zhao, Shanghang Zhang, Guanhang Wu, Jose MF Moura, Joao P Costeira, and Geoffrey J
Gordon. Adversarial multiple source domain adaptation. In Advances in Neural Information
Processing Systems, pp. 8559-8570, 2018.
Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant
representations for domain adaptation. In International Conference on Machine Learning, pp.
7523-7532, 2019.
12
Under review as a conference paper at ICLR 2020
A Proof of Theorem 2
Theorem 2 Given k source domains datasets {(x(ji), yj(i)) : i ∈ [k], j ∈ [m]} with m iid examples
each where Sbi = {x(ji)} and yj(i) = fSi (x(ji)), for any α ∈ ∆ = {α : αi ≥ 0, Pi αi = 1}, δ ∈
(0, 1), and ∀h ∈ H, w.p. at least 1 - δ
LT(h,fT) ≤Xαi LSbi(h,fSi)+disc(T,Si)+2Rm(HSi)+ηH,i + kαk2MS
i
where HSi = {x 7→ L(h(x), fSi (x)) : h ∈ H} is the set of functions mapping x to the
corresponding loss, ηH,i is a constant similar to Eq. (2) with Q = Si, P = T and MS =
supi∈[k],x∈X,h∈H L(h(x), fSi (x)) is the upper bound on loss on the source domains.
八og(1∕δ)
V	2m
Proof: The proof is similar to Cortes et al. (2019); Zhao et al. (2018). Given α ∈ ∆, the mixture
Sb = Pi αiSbi can be considered as the joint source data with km points, where a point x(i) from Sbi
has weight αi∕m. Define Φ = suph∈H LT (h, fT) - Pi αiLSb (h, fSi). Changing a point x(i) from
Si will change Φ at most Mmai. Using the McDiarmid’s inequality, We have Pr(Φ - E[Φ] > e) ≤
exp (- MjmI2). As a result, for δ ∈ (0,1), w.p. at least 1 - δ, the following holds for any h ∈ H
Lτ(h,fτ) ≤ XαiLsi(h,fsj + E[Φ] + ∣∣α∣∣2Ms Jlog曾∙
i
Now we bound E[Φ]. Let HSi = {x 7→ L(h(x), fSi (x)) : h ∈ H}.
E[Φ] =ESb hs∈uHpLT(h,fT)- i αiLSbi(h,fSi)
≤ ESb sup	αiLSi(h,fSi) -	αiLSbi(h, fSi) + sup LT(h,fT) -	αiLSi(h, fSi)
h∈H i	i	i	h∈H	i
≤ ESb
i
αi sup
h∈H
LSi(h,fSi)-LSbi(h,fSi)
XαiESbi sup LSi(h,fSi)-LSbi(h,fSi)
■j	-" e
+	αi sup (LT (h, fT) - LSi (h, fSi))
i h∈H
+ X αi sup (LT (h, fT) - LSi (h, fSi))
h∈H
≤ 2	αiRm (HSi ) +
αihs∈uHp(LT(h,fT)-LSi(h,fSi))
≤	αi (2Rm(HSi) + disc(T, Si) + ηH,i).
i
where first and second inequalities are using the subadditivity of sup, followed by the equality
using the independence between the domains {Si}, the second last inequality is due to the standard
“ghost sample” argument in terms of the Rademacher complexity and the last inequality is due to
Cortes et al. (2019, Proposition 8) for each individual S”	■
13
Under review as a conference paper at ICLR 2020
B Jacobian
Here we calculate the Jacobian Jij = ∂ɑi/∂zj for Eq. (6) in the main text:
α* = [z-ν *1]+∕∣∣[z-ν *1]+∣∣ι.
In the following, we write α = a*, ν = V* to simplify notations. Let S = {i : Zi — ν > 0} be
the support of the probability vector a. Jij = 0 if i / S or j ∈ S since a = 0 in the former
case while Zj does not contribute to the a in the latter case. Now consider the case i, j / S. Let
K = Il[z - V 1]+∣∣1 = PPj∈s(Zj - V). Then ai = (Zi - V) . K and
ʌ 1	1 ∂K .	.	1 Λ	∂ν ∂K ∖ f
)-K - K ∙ ∂Zj ∙ (Zi-V) = K (δi=j -西-∂Z- ∙ αi),⑺
where δi=j is the indicator or delta function. Now we compute 翁 and IK. By the definition of V,
we know that
E(Zj-V)2 = ∣s 1V2
j∈s
= Pj∈s Zj √A
V =	|S |	|S |
∂v	1 Bj
——=.-------
dZj	|S|	∣S∣√A
-2V»>j + £z2 = 1
j∈s	j∈S
where A =(X Zj)	- |S 1 (X ZI-1
v∈s )	v∈s
where Bj = Ez/―尸⑸
j0 ∈S
(8)
The first right-arrow is due to the quadratic formula and realizing that Ej∈s Zj∕∣S∣ is the mean of
the supported Zj so V must be smaller than it (i.e., we take - in the ± of the quadratic formula,
otherwise some of the Zj will not be in the support anymore). And
∂K
dZj
1 -|S l∙ ∂v
(9)
Plugging Eq. (8) and Eq. (9) in Eq. (7) gives
dai 1
——=——
∂Zj	K
1	Bj	1
δi=j - |S| + √A •(冏-αi
Note that
Bj =j j-∣S|Zj
K = j (Zj-V)
∑j∈s (Zj-V) + |S |(V-Zj)
Pj0∈S (ZjO - V)
1 - lSIaj.
Then
Jj = ^a = K (δi=j - |S1) + 号(∣S∣-ai) (∣S∣-aj
In matrix form,
Diag(s)
J
1
K
where S = [s1,..., sk]τ is a vector indicating the support si = δi∈s and o is element-wise multi-
plication. More often, we need to compute its multiplication with a vector V
Note that all quantities except A have been computed during the forward pass of calculating Eq. (6).
A can be computed in O(∣S∣) time so the overall computation is still O(k) since ∣S∣ ≤ k.
14
Under review as a conference paper at ICLR 2020
Figure 5: Regression experiment (best viewed in color).
C Regression on Synthetic Data
Due to the lack of publicly available multi-source regression datasets, we demonstrate our method
on a synthetic regression problem here.
Setup. We use eight source domains, where the ith (i = {0, 1, . . . , 7}) domain data is generated by
Xi 〜 N(∏4i - 78π, 0.22) and the output is y = Sin(X) + e where e 〜 N(0,0.052) is random noise.
These eight domains evenly cover the sin function on [-π, π] (see Fig. 5a). Next we construct four
target domains, where the jth (j = {0,1,2,3}) domain is generated by Xj 〜 N(∏j — 苧,0.42).
Similar to the source domains, these four target domains evenly cover [-π, π] (see Fig. 5b). Each
source/target domain has 100 data points.
We use labelled source data and unlabelled target data for learning. We take on one target domain
at a time, and learn a linear model from all eight source domains with MSE loss. The goal is to
see whether our method can focus on the relevant source domains and learn a linear model that can
perform well on the target domain.
Results and Analysis. First, Fig. 5b shows the learned models. The learned linear models can fit
the target data very well. This shows that our DARN can learn a meaningful model for a specific
target domain, using only labelled source data and unlabelled target data. Second, Fig. 5c shows the
source domain weights (the α) for each target domain after training. The weight colors correspond
to the colors in Fig. 5a. It is noticeable that DARN can focus well on the respective relevant source
domains for each target domain and ignore the rest. Note that these values are automatically learned
during the training of the model.
15
Under review as a conference paper at ICLR 2020
D Model Architecture for Digit Recognition
MNIST
Label prediction
Domain prediction 1
Domain prediction 2
Domain prediction 3
Figure 6: Model architecture for the digit recognition.
16