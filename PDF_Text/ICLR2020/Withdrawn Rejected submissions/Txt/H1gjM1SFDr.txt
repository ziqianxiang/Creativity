Under review as a conference paper at ICLR 2020
Noisy '0-Sparse SUBSPACE Clustering on DIMEN-
sionality Reduced Data
Anonymous authors
Paper under double-blind review
Ab stract
High-dimensional data often lie in or close to low-dimensional subspaces. Sparse
SUbsPace clustering methods with sparsity induced by '0-norm, such as '0-Sparse
SUbsPace Clustering ('0-SSC) Yang et al. (2016), are demonstrated to be more ef-
fective than its `1 counterpart such as Sparse Subspace Clustering (SSC) Elhamifar
& Vidal (2013). However, these '0-norm based subspace clustering methods are
restricted to clean data that lie exactly in subsPaces. Real data often suffer from
noise and they may lie close to subspaces. We propose noisy '0-SSC to handle
noisy data so as to improve the robustness. We show that the optimal solution to
the optimization problem of noisy '0-SSC achieves subspace detection property
(SDP), a key element with which data from different subspaces are separated, under
deterministic and randomized models. our results provide theoretical guarantee
on the correctness of noisy '0-SSC in terms of SDP on noisy data. We further pro-
pose NOiSy-DR-'0-SSC which provably recovers the subspaces on dimensionality
reduced data. NOiSy-DR-'0-SSC first projects the data onto a lower dimensional
space by linear transformation, then performs noisy '0-SSC on the dimensionality
reduced data so as to improve the efficiency. The experimental results demonstrate
the effectiveness of noisy '0-SSC and NOiSy-DR-'0-SSC.
1	Introduction
Clustering is an important unsupervised learning procedure for analyzing a broad class of scientific
data in biology, medicine, psychology and chemistry. on the other hand, high-dimensional data,
such as facial images and gene expression data, often lie in low-dimensional subspaces in many
cases, and clustering in accordance to the underlying subspace structure is particularly important.
For example, the well-known Principal Component Analysis (PCA) works perfectly if the data are
distributed around a single subspace. The subspace learning literature develops more general methods
that recover multiple subspaces in the original data, and subspace clustering algorithms Vidal (2011)
aim to partition the data such that data belonging to the same subspace are identified as one cluster.
Among various subspace clustering algorithms, the ones that employ sparsity prior, such as Sparse
Subspace Clustering (SSC) Elhamifar & Vidal (2013) and '0-Sparse Subspace Clustering ('0-SSC)
Yang et al. (2016), have been proven to be effective in separating the data in accordance with the
subspaces that the data lie in under certain assumptions.
Sparse subspace clustering methods construct the sparse similarity matrix by sparse representation
of the data. Subspace detection property (SDP) defined in Section 4.1 ensures that the similarity
between data from different subspaces vanishes in the sparse similarity matrix, and applying spectral
clustering Ng et al. (2001) on such sparse similarity matrix leads to compelling clustering performance.
Elhamifar and Vidal Elhamifar & Vidal (2013) prove that when the subspaces are independent or
disjoint, SDP can be satisfied by solving the canonical sparse linear representation problem using
data as the dictionary, under certain conditions on the rank, or singular value of the data matrix and
the principle angle between the subspaces. SSC has been successfully applied to a novel deep neural
network architecture, leading to the first deep sparse subspace clustering method Peng et al. (2016).
Under the independence assumption on the subspaces, low rank representation Liu et al. (2010; 2013)
is also proposed to recover the subspace structures. Relaxing the assumptions on the subspaces to
allowing overlapping subspaces, the Greedy Subspace Clustering Park et al. (2014) and the Low-
Rank Sparse Subspace Clustering Wang et al. (2013) achieve subspace detection property with high
1
Under review as a conference paper at ICLR 2020
probability. The geometric analysis in Soltanolkotabi & Cands (2012) shows the theoretical results
on subspace recovery by SSC. In the following text, We use the term SSC or '1-SSC exchangeably to
indicate the Sparse Subspace Clustering method in Elhamifar & Vidal (2013).
Real data often suffer from noise. Noisy SSC proposed in Wang & Xu (2013) handles noisy data
that lie close to disjoint or overlapping subspaces. While '0-SSC Yang et al. (2016) has guaranteed
clustering correctness via subspace detection property under much milder assumptions than previous
subspace clustering methods including SSC, it assumes that the observed data lie in exactly in the
subspaces and does not handle noisy data. In this paper, we present noisy '0-SSC, which enhances '0-
SSC by theoretical guarantee on the correctness of clustering on noisy data. It should be emphasized
that while '0-SSC on clean data Yang et al. (2016) empirically adopts a form of optimization problem
robust to noise, it lacks theoretical analysis on the correctness of '0-SSC on noisy data. In this
paper, the correctness of noisy '0-SSC on noisy data in terms of the subspace detection property is
established. Our analysis is under both deterministic model and randomized models, which is also the
model employed in the geometric analysis of SSC Soltanolkotabi & Cands (2012). Our randomized
analysis demonstrates potential advantage of noisy '0-SSC over its '1 counterpart as more general
assumption on data distribution can be adopted. Moreover, we present Noisy Dimensionality Reduced
'0-Sparse Subspace Clustering (NOiSy-DR-'0-SSC), an efficient version of noisy '0-SSC which also
enjoys robustness to noise. Noisy-DR-'0-SSC first projects the data onto a lower dimensional space
by random projection, then performs noisy '0-SSC on the dimensionality reduced data. Noisy-DR-'0-
SSC provably recovers the underlying subspace structure in the original data from the dimensionality
reduced data under deterministic model. Experimental results demonstrate the effectiveness of both
noisy '0-SSC and Noisy-DR-'0-SSC.
We use bold letters for matrices and vectors, and regular lower letter for scalars throughout this paper.
The bold letter with superscript indicates the corresponding column of a matrix, e.g. Ai is the i-th
column of matrix A, and the bold letter with subscript indicates the corresponding element of a
matrix or VeCtor.卜|旧 and ∣∣ ∙ ∣∣p denote the Frobenius norm and the vector 'p-norm or the matrix
p-norm, and diag(∙) indicates the diagonal elements of a matrix. HT ⊆ Rd indicates the subspace
spanned by the columns of T, and AI denotes a submatrix of A whose columns correspond to the
nonzero elements of I (or with indices in I without confusion). σt(∙) denotes the t-th largest singular
value of a matrix, and σmin(∙) indicates the smallest singular value of a matrix. supp(∙) is the support
of a vector, PS0 is an operator indicating projection onto the subspace S0 .
2	Problem Setup
2.1	Notations
We hereby introduce the notations for subspace clustering on noisy data considered in this paper. The
uncorrupted data matrix is denoted by Y = [y1, . . . , yn] ∈ Rd×n, where d is the dimensionality and
n is the size of the data. The uncorrupted data Y lie in a union of K distinct subspaces {Sk}kK=1 of
dimensions {dk}kK=1. The observed noisy data is X = Y + N, where N = [n1, . . . , nn] ∈ Rd×n
is the additive noise. xi = yi + ni is the noisy data point that is corrupted by the noise ni .
K
Let Y (k) ∈ Rd×nk denote the data belonging to subspace Sk with P nk = n, and denote the
k=1
corresponding columns in X by X(k). The data X are normalized such that each column has unit
'2-norm in our deterministic analysis. We consider deterministic noise model where the noise Z is
fixed and max ∣ni ∣ ≤ δ. Note that our analysis can be extended to a random noise model which is
common and also considered by noisy SSC Wang & Xu (2013), and the random noise model assumes
that columns of Z are sampled i.i.d. and max ∣ni∣ ≤ δ with high probability. Note that such random
noise model does not require spherical symmetric noise as that in Wang & Xu (2013).
2.2	Method
'0-SSC Yang et al. (2016) proposes to solve the following '0 sparse representation problem
min kZk0 s.t. X =XZ, diag(Z) =0,	(1)
and it proves that the subspace detection property defined in Definition 1 is satisfied with the globally
optimal solution to (1). We resort to solve the '0 regularized sparse approximation problem below to
2
Under review as a conference paper at ICLR 2020
handle noisy data for '0-SSC, which is the optimization problem of noisy '0-SSC:
min	L(Z) = kX -XZk2F + λkZk0.	(2)
Z∈Rn×n,diag(Z)=0
The definition of subspace detection property for noisy '0-SSC and noiseless '0-SSC, i.e. '0-SSC on
noiseless data, is defined in Definition 1 below.
Definition 1. (Subspace detection ProPertyfor noisy and noiseless '0-SSC) Let Z* be the optimal
solution to (2). The subspaces {Sk}kK=1 and the data X satisfy subspace detection property for noisy
'0-SSC if Zi is a nonzero vector, and nonzero elements of Zi correspond to the columns of X from
the same subspace as yi for all 1 ≤ i ≤ n.
Similarly, in the noiseless setting where X = Y, let Z be the optimal solution to(1). The subspaces
{Sk}K=ι and the data X satisfy the subspace detection property for noiseless '0-SSC if Zi is a
nonzero vector, and nonzero elements of Zi correspond to the columns of X that from the same
subspace as yi for all 1 ≤ i ≤ n.
We say that subspace detection property holdsfor Xi if nonzero elements of Z*i correspond to the
data that lie in the same subspace as yi, for either noisy '0-SSC or noiseless '0-SSC.
2.3	Models
Similar to Soltanolkotabi & Cands (2012), we introduce the deterministic, semi-random and fully-
random models for the analysis of noisy '0-SSC.
•	Deterministic Model: the subspaces and the data in each subspace are fixed.
•	Semi-Random Model: the subspaces are fixed but the data are independent and identically
distributed in each of the subspaces.
•	Fully-Random Model: both the subspaces and the data of each subspace are independent and
identically distributed.
The data in the above definitions refer to clean data without noise. We refer to semi-random model
and fully-random model as randomized models in this paper. All the three models are extensively
employed to analyze the subspace detection property in the subspace learning literature Soltanolkotabi
& Cands (2012); Wang et al. (2013); Wang &Xu (2013); Yining Wang & Singh (2016).
3	Theoretical Analysis FOR Noisy '0-SSC
The theoretical results on the subspace detection property for noisy '0-SSC are presented in this
section under deterministic model and randomized models.
Figure 1: illustration of an external subspace. All the data Y are normalized to have unit norm for
illustration purpose, so they lie on the surface of the sphere. S1 and S2 are two subspaces in the
three-dimensional ambient space. The subspace spanned by yi ∈ S1 and yj ∈ S2 is an external
subspace, and the intersection of this external subspace and S1 is a dashed line yi OA.
3.1 NOiSY `0 -SSC: DETERMiNiSTiC ANALYSiS
We introduce the definition of general position and external subspace before our analysis on noisy
'0-SSC.
3
Under review as a conference paper at ICLR 2020
Definition 2. (General position) For any 1 ≤ k ≤ K, the data Y (k) are in general position if any
subset of L ≤ dk data points (columns) of Y (k) are linearly independent. Y are in general position
if Y (k) are in general position for 1 ≤ k ≤ K.
The assumption of general condition is rather mild. In fact, if the data points in X(k) are independently
distributed according to any continuous distribution, then they almost surely in general position.
Let the distance between a point x ∈ Rd and a subspace S ⊆ Rd be defined as d(x, S) = infy∈S kx-
yk2, the definition of external subspaces is presented as follows. Figure 1 illustrates an example of
external subspace.
Definition 3. (External subspace) For a point y ∈ Y (k), a subspace H{yi }L spanned by a
set of linear independent points {yij }jL=1 ⊆ Y is defined to be an external subspace of y if
{yij }jL=1 6⊆ Y (k) and y ∈/ {yij }jL=1. The point y is said to be away from its external subspaces if
minH∈Hy,dk d(y, H) > 0, where Hy,d are the set of all external subspaces of y of dimension no
greater than dfory, i.e. Hy,d = {H: H = H{yi }L , dim[H] = L, L ≤ d, {yij }jL=1 6⊆ Y (k), y ∈/
{yij }jL=1}. All the data points in Y (k) are said to be away from the external subspaces if each of
them is away from the its associated external spaces.
Remark 1. (Subspace detection property holds for noiseless '0-SSC under the deterministic model)
It can be verified that the following statement is true. Under the deterministic model, suppose data is
noiseless, nk ≥ dk + 1, Y (k) is in general position. If all the data points in Y (k) are away from the
external SubsPacesfor any 1 ≤ k ≤ K, then the subspace detection ProPertyfor '0-SSC holds with
the optimal solution Z* to (1).
To present our theoretical results of the correctness of noisy '0-SSC, We also need the definitions of
the minimum restricted eigenvalue and the subspace separation margin, which are defined as follows.
In the folloWing analysis, We employ β to denote the sparse code of datum xi so that a simpler
notation other than Zi is dedicated to our analysis.
Definition 4. The minimum restricted eigenvalue of the uncorruPted data is defined as
σY ,r ,	min
σmin(Yβ)
β“∣βk0=r,rank(Ye )=kβk0
(3)
for r ≥ 1. In addition, the normalized minimum restricted eigenvalue of the uncorruPted data is
defined by
σY ,r
σY L √Γ
(4)
We have the folloWing perturbation bound for the distance betWeen a data point and the subspaces
spanned by noisy and noiseless data, Which is useful to establish the conditions When the subspace
detection property holds for noisy '0-SSC.
Lemma 1. Let β ∈ Rn and Ye has full column rank. Suppose δ < σγ ,r where r = kβko, then Xe
is a full column rank matrix, and
∣d(xi, HXe) - d(xi, HYe)| ≤ _ δ	(5)
σY ,r 一 δ
for any 1 ≤ i ≤ n.
The optimization problem of noisy '0-SSC (2) is separable. For each 1 ≤ i ≤ n, the optimization
problem With respect to the sparse code of i-th data point is
min
β∈Rn,βi=0
L(β)
kxi - Xβk22 + λkβk0.
(6)
Lemma 2 shows that the optimal solution to the noisy '0-SSC problem (6) is also that to a '0-
minimization problem With tolerance to noise.
Lemma 2. Let nonzero vector β* be the optimal solution to the noisy '0 -SSC problem (6) for point
Xi with ∣∣β*ko = r* > 1. If λ > τ0 where τ° is defined as
△ 2δ√r*
TL V + T1,
4
Under review as a conference paper at ICLR 2020
where
δ
σX , σmin (Xβ* ) ,
min σγ 产,
1≤r<r*
with δ < σγ, and σγ is defined as
;*
σγ
then β* is the optimal solution to the following sparse approximation problem with the uncorrupted
data as the dictionary:
.......... ..............2δ√r^
mm ∣∣βko s.t. ∣∣xi - Yβ∣∣2 ≤ C +------, βi = 0.	(7)
β	σX
where c*，∣∣Xi 一 Xβ*∣∣2.
Define B(xi, c0) = {x: kx - xik ≤ c0} be the ball centered at xi with radius c0. If B(xi, c0) is
away from the corresponding confusion area, i.e. all the external subspaces in Hyi,dk, then subspace
detection property holds with the solution to a proper sparse approximation problem where xi is
approximated by the uncorrupted data, as shown in the following Lemma.
Lemma 3. Suppose Y is in general position and yi ∈ Sk for some 1 ≤ k ≤ K. For positive number
co such that co ≥ d(xi, Sk), suppose B(xi,c0) ∩ H = 0 for any H ∈ Hyi,&%. Then the subspace
detection property holds for xi with the optimal solution to the following sparse approximation
problem, denoted by β*, i.e. nonzero elements of β* correspond to the columns of X from the same
subspace as yi.
min ∣β∣0 s.t. ∣xi - Yβ∣2 ≤ c0, βi = 0.	(8)
Now We use the above results to present the main result on the correctness of noisy '0-SSC.
Theorem 1.	(Subspace detection property holds for noisy '0-SSC) Let nonzero vector β* be the
optimal solution to the noisy '0-SSC problem (6) for point Xi with ∣∣β*∣o = r* > 1, and c* ，
∣∣Xi — Xβ*∣2. Suppose Y is in general position, Ni ∈ Sk for some 1 ≤ k ≤ K, δ < σγ, λ > τo,
B(yi, δ + c* + 2δσ*r*) ∩ H = 0 for any H ∈ Hyi,dk. Then the subspace detection property holds
for Xi with β*. Here τo, τι, σγ and σχ are defined in Lemma 2.
Remark 2. When δ = 0 and there is no noise in the data X, the conditions for the correctness of
noisy '0-SSC in Theorem 1 almost reduce to that for noiseless '0-SSC. To see this, the conditions
are reduced to B(yi, c*) ∩ H = 0, which are exactly the conditions required by noiseless '0-SSC,
namely data are away from the external subspaces by choosing λ → 0 and it follows that c* = 0.
While Theorem 1 establishes geometric conditions under which the subspace detection property holds
for noisy '0-SSC, it can be seen that these conditions are often coupled with the optimal solution
β* to the noisy '0-SSC problem (6). In the following theorem, the correctness of noisy '0-SSC is
guaranteed in terms of λ, the weight for the `o regularization term in (6), and the geometric conditions
independent of the optimal solution to (6).
Let Mi > 0 be the minimum distance between yi ∈ Sk and its external subspaces when yi is away
from its external subspaces, i.e.
Mi , min{d(yi, H): H ∈ Hyi,dk},	(9)
The following two quantities related to the spectrum of clean and noisy data, μr and σχ,r, are defined
as follows with r > 1 for the analysis in Theorem 2.
μr , ---------L--------7,	(10)
mini≤r0<r σγ,r - δ
σX,r , min{σmin(Xβ) : 1 ≤ ∣β∣0 ≤ r}	(11)
Theorem 2.	(Subspace detection property holds for noisy '0-SSC under deterministic model, with
conditions in terms of λ) Let nonzero vector β* be the optimal solution to the noisy '0-SSCproblem
(6) for point Xi with ∣β * ∣o = r* , nk ≥ dk + 1 for every 1 ≤ k ≤ K, and there exists 1 < ro ≤ d
5
Under review as a conference paper at ICLR 2020
such that 1 < r* ≤ ro. Suppose Y is in general position, Ni ∈ Sk for some 1 ≤ k ≤ K,
δ < min1≤r<r0 σγ/,and Mi,δ，Mi 一 δ. Suppose
Mi,δ > ~δ^,	(12)
σX,r0
and
2δ
μ=o < ι — σ—.	(13)
Then if
λ0 < λ < 1,	(14)
where λ0 , max{λ1, λ2} and
「一	,——一	2δ 一一、
λι，inf {0 < λ < 1: √1 — λ +-------√= < Mi,δ },	(15)
2δ	1
λ2，inf {0 <λ< 1: λ------------— > μro },	(16)
σX ,r0 λ
the subspace detection property holds for Xi with β*. Here Mi, μr° and σχ,r° are defined in (9),
(10) and (11) respectively.
Remark 3. The two conditions (12) and (13) are induced by the conditions that B(yi, δ + c* +
22^) ∩ H = 0 forany H ∈ Hyi^, and λ > τo in Theorem 1. Note that when (12) and (13) hold,
λ1 and λ2 can always be chosen in accordance with (15) and (16).
Remark 4. It can be observed from condition (14) that noisy `0 -SSC encourages sparse solution by
a relatively large λ so as to guarantee the subspace detection property. This theoretical finding is
consistent with the empirical study shown in the experimental results.
3.2 Noisy '0-SSC: Randomized Analysis
In this subsection, the correctness of noisy '0-SSC is analyzed when the clean data in each subspace
are distributed at random. We assume that the data in subspace S(k) are i.i.d. isotropic samples on
sphere of radius √dk centered at the origin according to some continuous distribution, for 1 ≤ k ≤ K.
A random variable y ∈ Sk is isotropic if E[yy>] = Idk, where Idk is the dk × dk identity matrix
(with y represented as a vector in Rdk). in addition, for each 1 ≤ k ≤ K, we assume that the
following condition holds:
(a) There exists a constant M ≥ 1 such that for any t > 0, any y ∈ Y (k), and any vector v with unit
'2 -norm,
Pr[|hy, vi| > t] ≤ M.	(17)
intuitively, condition (a) requires that the projection of any data point onto arbitrary unit vector is
bounded from both sides with relatively large probability. This condition is also required in Yaskov
(2014) to derive lower bound for the least singular value of a random matrix with independent
isotropic columns. in order to meet the conditions in Theorem 2 so as to guarantee the subspace
detection property under randomized models, the following lemma is presented and it provides the
geometric concentration inequality for the distance between a point y ∈ Y (k) and any of its external
subspaces. it renders a lower bound for Mi , namely the minimum distance between yi ∈ Sk and its
external subspaces.
Lemma 4. Under randomized models, given 1 ≤ k ≤ K andy ∈ Y (k), suppose H ∈ Hyi,dk is any
external subspace of y. Then for any t > 0,
一 .	d----- h	. dut2.
Pr[d(y, H) ≥ 1 — 2t∖Jdk — 1 — t ] ≥ 1 — 8exp(---2—).	(18)
We then have the following results regarding to the subspace detection property of noisy '0-SSC
under randomized models.
6
Under review as a conference paper at ICLR 2020
Theorem 3. (Subspace detection property holds for noisy '0-SSC under randomized models, with
conditions in terms of λ) Under randomized models, let nonzero vector β* be the optimal solution to
the noisy '0-SSCproblem (6)forpoint Xi with ∣∣β*∣∣o = r*, n ≥ dk + 1 for every 1 ≤ k ≤ K, and
there exists 1 < ro ≤ d such that 1 < r* ≤ r°. Suppose the data in each subspace are i.i.d. isotropic
samples according to some continuous distribution that satisfies condition (a). Let dmax , maxk dk,
1
c
√r0(√196Md+1+14√Md)'
For t > 0 such that 1 一 2t√dmaχ — 1 一 t2 > 0, suppose
δ < c,	(19)
δ +---------E ≤ 1 —— 2t√dmaχ — 1 — t,	(20)
√r0(c — δ)
δ	2δ ι
C — δ + √r0(c — δ) < ,
and
λ0 < λ < 1,
where λ00 , max{λ01, λ02 } and
2δ
λ1，inf {0 <λ< 1: V1-x +----------------
√ro(c — δ) √λ
< 1 — 2t√dmaχ — 1 — t — δ},
λ2 , inf{0 < λ < 1 : λ--1=7---√~> > -----ʌ }.
√r0(c — δ) √Λ	c — δ
(21)
(22)
(23)
(24)
K	d t2
Then with probability at least 1 — K exp(-d) — 8 P n exp(——12t-), the subspace detection property
k=1
holds for Xi with β* .
Remark 5. Note that there is no assumption on the distribution of subspaces in Theorem 3, so it is not
required that the subspaces should have uniform distribution, an is required in the geometric analysis
of '1 -SSC Soltanolkotabi & Cands (2012) and its noisy version Wang & XU (2013). In addition, while
Soltanolkotabi & Cands (2012); Wang & Xu (2013) require data in each subspace are i.i.d according
to uniform distribution on unit sphere, our randomized result requires data in each subspace are i.i.d.
isotropic random vectors on sphere ofradius ʌ/dk. Note that i.i.d samples uniformly distributed on
sphere ofradius ʌ/dk centered at the origin are infact isotropic, our assumption is less restrictive
after scaling the data by afactor of ʌ/dk.
4 Noisy '0-SSC ON Dimensionality Reduced Data： Noisy-DR-'0-SSC
Albeit the theoretical guarantee and compelling empirical performance of noisy '0-SSC to be shown in
the experimental results, the computational cost of noisy '0-SSC is high with the high dimensionality
of the data. In this section, we propose Noisy Dimensionality Reduced '0-SSC (NOiSy-DR-'0-SSC)
which performs noisy '0-SSC on dimensionality reduced data. The theoretical guarantee on the
correctness of Noisy-DR-'0-SSC under deterministic model as well as its empirical performance are
presented.
4.1	Method
NOiSy-DR-'0-SSC performs subspace clustering by the following two steps: 1) obtain the dimension
reduced data X = PX with a linear transformation P ∈ Rp×d (p < d). 2) perform noisy '0-SSC
on the compressed data X :
min L(β) = ∣∣x i — X βk2 + Xkek0.	(25)
β∈Rn,βi = 0
If p < d, NOiSy-DR-'0-SSC operates on the compressed data X rather than on the original data, so
that the efficiency is improved.
7
Under review as a conference paper at ICLR 2020
4.2	Analysis
High-dimensional data often exhibits low-dimensional structures, which often leads to low-rankness
of the data matrix. Intuitively, if the data is low rank, then it could be safe to perform noisy '0-SSC on
its dimensionality reduced version by the linear projection P, and it is expected that P can preserve
the information of the subspaces contained in the original data as much as possible, while effectively
removing uninformative dimensions.
To this end, we propose to choose P as a random projection induced by randomized low-rank
approximation of the data. The key idea is to obtain an approximate low-rank decomposition
of the data. Using the random projection induced by such low-rank approximation as the linear
transformation P, the clustering correctness hold for NOisy-DR-'0-SSC with a high probability.
Randomized algorithms are efficient and they have been extensively studied in the computer science
and numerical linear algebra literature. They have been employed to accelerate various numerical
matrix computation and matrix optimization problems, including random projection for matrix
decomposition Frieze et al. (2004); Drineas et al. (2004); Sarlos (2006); Drineas et al. (2006; 2008);
Mahoney & Drineas (2009); Drineas et al. (2011); Lu et al. (2013).
Formally, a random matrix T ∈ Rn×p is generated such that each element Tij is sampled indepen-
dently according to the Gaussian distribution N(0, 1). QR decomposition is then performed on XT
to obtain the basis of its column space, namely XT = QR where Q ∈ Rd×p is an orthogonal matrix
of rank p and R ∈ Rp×p is an upper triangle matrix. The columns of Q form the orthogonal basis for
the sample matrix XT. An approximation of X is then obtained by projecting X onto the column
space of XT: QQ>X = QW = X where W = Q>X ∈ Rp×n. In this manner, a randomized
low-rank decomposition of X is achieved as follows:
X = QW	(26)
We present probabilistic result on the correctness of Noisy-DR-'0-SSC using the random projection
induced by randomized low-rank decomposition of the data X, namely P = Q>, in Theorem 4.
In the sequel, X = PX for any X ∈ Rn. To guarantee the subspace detection property on the
dimensionality-reduced data X, it is crucial to make sure that the conditions, such as (12) and (13) in
Theorem 2, still hold after the linear transformation.
We denote by β* the optimal solution to (25). We also define the following quantities in the analysis
of the subspace detection property, which correspond to Mi, σγ产,σχ,『and μr used in the analysis
on the original data:
MMi，min{d(yi, H)： H ∈Hyi,dk},	(27)
where Hya 疏 is all the external subspaces of y with dimension no greater than dk in the transformed
space by P.
— Δ	♦	∖
σγr，	mιn~	σmin(Yβ),
,	β“∣βko=r,rank(Yβ)=kβko
(28)
ʌ - , . .. . .. 、
σχχ,r , min{σmin(Xβ): 1 ≤ ∣∣βko ≤ r},	(29)
二△	δ
μr , ^^^τ~ 三 P
-m-in1 ≤r0 <r σY r — δ
(30)
Theorem 4.	(Subspace detection property holds for Noisy-DR-'0-SSC under deterministic model)
Let nonzero vector β* be the optimal solution to the noisy '0-SSC problem (6) for point Xi with
∣∣β*ko = r*, nk ≥ dk + 1 for every 1 ≤ k ≤ K, and there exists 1 < r。≤ dsuch that 1 < r* ≤ r0.
Suppose Y is In general position, δ < mιnι≤r<ro σγ ,r, and Mi,δ，Mi 一 δ. Suppose the following
conditions hold:
(i)
Cp,po +2δ Jdmax < k^inK σ(k),	(31)
where dmaX，maxk dk, σ(k)，min{σmin(A): A ⊆ Y(k), A ∈ Rd×n0,n0 ≤ dk}.
8
Under review as a conference paper at ICLR 2020
(ii)	δ(1 + 2√r0) < min1≤r<r0 σγ,r - Cp,p0,
(iii)	mi∖≤r≤dk σγ,r > Cp,p0 - 2δ^∕dlk and
Mi - Cp,p0 (1 +
1
mini≤r≤dk σY,r - CP,P0
>δ+
2δ
σX,r0 - Cp,p0
for all yi ∈ Sk and 1 ≤ k ≤ K.
(iv) min1≤r<r0 行丫/。> Cp,p0 - 2δ√r0 - δ and
min1≤r<r0 σY,ro - Cp,po
-2δ√r0 - δ
<1-
2δ
σX,r0 - Cp,p0
If
r r 一
λ0 < λ < 1,
1
where λ0 = max{max{λι, λ2,	} and
λι =inf{0 < λ < 1: √1 - λ+ —2δ-= < MMi,δ},
OX ,ro C
2δ	1
λ2 = inf{0 <λ < 1: λ------------> > μro },
σX,r0 √λ
(32)
(33)
(34)
(35)
(36)
-^pX)
〜
then with probability at least 1 — 6e-p, the subspace detection property holds for Xi with β*. Here
717	~	1 ~	1 ∕'	7∙∕Cf∖∕CC∖	7∕CC∖	. ■ I
Mi, μr and σχ 丁。are defined in (27), (30) and (29) respectively.
Table 1: Clustering results on various data sets, with the best two results in bold
Data Set	Measure	KM	SC	Noisy SSC	Noisy DR-SSC	SMCE	SSC-OMP	NoisyT0-SSC	Noisy-DR-'0 -SSC
COIL-20	-AC	0.6554	0.4278	-07854-	07764	0.7549	-03389-	08472	0.8479
	-nmi-	0.7630	0.6217	-0.9148-	0.9219	0.8754	-0.4853-	0.9428	09433
COIL-100	-AC	0.4996	0.2835	-0.5275-	0.5013	0.5639	-0.1667-	07683	07039
	-nmi-	0.7539	0.5923	-0.8041-	0.8019	0.8064	-03757-	09182	08706
Yale-B	-AC	0.0954	0.1077	-07850-	07255	0.3293	-07789-	08480	08231
	NMI	0.1258	0.1485	0.7760 -	0.7311	0.3812	0.7024	^	0.8612	0.8533
Table 2: Clustering results on various data sets, with the best two results in bold
Data Set	Measure	KM	SC	Noisy SSC	Noisy DR-SSC	SMCE	SSC-OMP	NoisyT0-SSC	NOiSy-DR-20-SSC
MPIES1	AC	0.1164	0.1285	0.5892	03588	0.1721	0.1695	06741	06741
	-NMi-	0.5049	0.5292	0.7653	06806	0.5514	0.3395	08622	08622
MPIE S2	AC	0.1315	0.1410	0.6994	0.4611	0.1898	0.2093	07527	07527
	-nmi-	0.4834	0.5128	0.8149	07086	0.5293	0.4292	08939	07527
MPIE S3	AC	0.1291	0.1459	0.6316	0.4841	0.1856	0.1787	07050	07050
	-nmi-	0.4811	0.5185	0.7858	07340	0.5155	0.3415	08750	08750
MPIE S4	AC	0.1308	0.1463	0.6803	0.5511	0.1823	0.1680	07246	07246
	NMI	0.4866	0.5280	0.8063	0.7955	0.5294	0.3345	0.8837 —	0.8837
5 Optimization of Noisy '0-SSC and Noisy-DR-'0-SSC
We employ Proximal Gradient Descent (PGD) to optimize the objective function of noisy '0-SSC
and Noisy-DR-'0-SSC. For example, in the k-th iteration of PGD for problem (6), the variable β is
updated according to
β(k+1) = T√2λs(β(k) - sVg(β(k))),	(37)
9
Under review as a conference paper at ICLR 2020
where g(β) , kxi - Xβk22, Tθ is an element-wise hard thresholding operator:
[Tθ (u)]j =	0 : |uj | ≤ θ	, 1 ≤ j ≤ n.
j	uj : otherwise
It is proved in Yang & Yu (2019) that the sequence {β(k)} generated by PGD converges to a critical
point of (6), denoted by β. Let β be the optimal solution to (6). Theorem 5 in Yang & Yu (2019)
to problem (6) shows that the ∣∣β* - β∣∣2 is bounded. Theorem 5 establishes the conditions under
which β is also the optimal solution to (6).
Define S* ， supp(β*), H * ， maxι≤j≤n dist(x, Hx,*、、.}), μ，max{H * + ∣∣Xi —
Xβ*∣2,2∣Xi - Xβ∣2, 2∣Xi - Xβ* ∣2}, κ，σmin(Xs∪s* ) > 0 where S，supp(β⑼).The
following theorem demonstrates that β = β if λ is two-side bounded and βmin = min^e==。∣βt∣ is
sufficiently large.
Theorem 5.	(Conditions that the sub-optimal solution by PGD is also globally optimal) If
βmin ≥ 与	(38)
κ20
and
2
彳 ≤ λ ≤ (6min - *)μ,	(39)
κ0	κ0
then β = β*.
6	Experimental Results
We demonstrate the performance of noisy '0-SSC and NOiSy-DR-'0-SSC, with comparison to other
competing clustering methods including K-means (KM), Spectral Clustering (SC), noisy SSC, Sparse
Manifold Clustering and Embedding (SMCE) Elhamifar & Vidal (2011) and SSC-OMP Dyer et al.
(2013). With the coefficient matrix Z obtained by the optimization of noisy '0-SSC or Noisy-DR-'0-
|Z|+|Z>|
SSC, a sparse similarity matrix is built by W = kj+2——|, and spectral clustering is performed on
W to obtain the clustering results. Two measures are used to evaluate the performance of different
clustering methods, i.e. the Accuracy (AC) and the Normalized Mutual Information (NMI) Zheng
et al. (2004).
We use randomized rank-p decomposition of the data matrix in Noisy-DR-'0-SSC with P = min{d,n}.
It can be observed that noisy '0-SSC and Noisy-DR-'0-SSC always achieve better performance than
other methods in Table 1, including the noisy SSC on dimensionality reduced data (Noisy DR-SSC)
Wang et al. (2015). Throughout all the experiments we find that the best clustering accuracy is
achieved whenever λ is chosen by 0.5 < λ < 0.95, justifying our theoretical finding claimed in
Remark 4 and (39) in Theorem 5. More experimental results on the CMU Multi-PIE data are shown
in Table 2. For all the methods that involve random projection, we conduct the experiments for
30 times and report the average performance. Note that the cluster accuracy of SSC-OMP on the
extended Yale-B data set is reported according to You et al. (2016). The time complexity of running
PGD for noisy '0-SSC and Noisy-DR-'0-SSC are O(T nd) and O(T pd) respectively, where T is
the maximum iteration number. The actual running time of both algorithms confirms such time
complexity, and we observe that Noisy-DR-'0-SSC is always more than 8.7 times faster than noisy
'0-SSC with the same number of iterations.
7	Conclusion
We present provable noisy '0-SSC that recovers subspaces from noisy data through '0-induced
sparsity in a robust manner, with the theoretical guarantee on its correctness in terms of subspace
detection property under both deterministic and randomized models. Experimental results shows the
superior performance of noisy '0-SSC. We also propose Noisy-DR-'0-SSC which performs noisy
'0-SSC on dimensionality reduced data and still provably recovers the subspaces in the original data.
Experiment results demonstrate the effectiveness of both noisy '0-SSC and Noisy-DR-'0-SSC.
10
Under review as a conference paper at ICLR 2020
References
G. Aubrun and S.J. Szarek. Alice and Bob Meet Banach: The Interface of Asymptotic Geometric Analysis and
Quantum Information Theory. Mathematical Surveys and Monographs. American Mathematical Society,
2017.
Yan Mei Chen, Xiao Shan Chen, and Wen Li. On perturbation bounds for orthogonal projections. Numerical
Algorithms, 73(2):433-444, Oct 2016.
P. Drineas, A. Frieze, R. Kannan, S. Vempala, and V. Vinay. Clustering large graphs via the singular value
decomposition. Machine Learning, 56(1):9-33, 2004.
Petros Drineas, Ravi Kannan, and Michael W. Mahoney. Fast monte carlo algorithms for matrices ii: Computing
a low-rank approximation to a matrix. SIAM Journal on Computing, 36(1):158-183, 2006.
Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Relative-error $cur$ matrix decompositions. SIAM
Journal on Matrix Analysis and Applications, 30(2):844-881, 2008.
Petros Drineas, Michael W. Mahoney, S. Muthukrishnan, and Tamas Sarlos. Faster least squares approximation.
Numerische Mathematik, 117(2):219-249, 2011.
Eva L. Dyer, Aswin C. Sankaranarayanan, and Richard G. Baraniuk. Greedy feature selection for subspace
clustering. Journal of Machine Learning Research, 14:2487-2517, 2013.
Ehsan Elhamifar and Rene Vidal. Sparse manifold clustering and embedding. In NIPS, pp. 55-63, 2011.
Ehsan Elhamifar and Rene Vidal. Sparse subspace clustering: Algorithm, theory, and applications. IEEE Trans.
Pattern Anal. Mach. Intell., 35(11):2765-2781, 2013.
Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast monte-carlo algorithms for finding low-rank approxima-
tions. J. ACM, 51(6):1025-1041, November 2004.
N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for
constructing approximate matrix decompositions. SIAM Rev., 53(2):217-288, May 2011. ISSN 0036-1445.
Guangcan Liu, Zhouchen Lin, and Yong Yu. Robust subspace segmentation by low-rank representation. In
Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa,
Israel, pp. 663-670, 2010.
Guangcan Liu, Zhouchen Lin, Shuicheng Yan, Ju Sun, Yong Yu, and Yi Ma. Robust recovery of subspace
structures by low-rank representation. IEEE Trans. Pattern Anal. Mach. Intell., 35(1):171-184, January 2013.
Yichao Lu, Paramveer S. Dhillon, Dean Foster, and Lyle Ungar. Faster ridge regression via the subsampled
randomized hadamard transform. In Proceedings of the 26th International Conference on Neural Information
Processing Systems, NIPS’13, pp. 369-377, USA, 2013. Curran Associates Inc.
Michael W. Mahoney and Petros Drineas. Cur matrix decompositions for improved data analysis. Proceedings
of the National Academy of Sciences, 106(3):697-702, 2009.
Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. In NIPS,
pp. 849-856, 2001.
Dohyung Park, Constantine Caramanis, and Sujay Sanghavi. Greedy subspace clustering. In Advances in Neural
Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014,
December 8-13 2014, Montreal, Quebec, Canada, pp. 2753-2761, 2014.
Xi Peng, Shijie Xiao, Jiashi Feng, Wei-Yun Yau, and Zhang Yi. Deep subspace clustering with sparsity prior. In
Proceedings of the 25 International Joint Conference on Artificial Intelligence, pp. 1925-1931, New York,
NY, USA, 9-15 July 2016.
T. Sarlos. Improved approximation algorithms for large matrices via random projections. In 2006 47th Annual
IEEE Symposium on Foundations of Computer Science (FOCS’06), pp. 143-152, Oct 2006.
Mahdi Soltanolkotabi and Emmanuel J. Cands. A geometric analysis of subspace clustering with outliers. Ann.
Statist., 40(4):2195-2238, 08 2012.
G.	W. Stewart. On the perturbation of pseudo-inverses, projections and linear least squares problems. SIAM
Review, 19(4):634-662, 1977. ISSN 00361445.
R. Vidal. Subspace clustering. Signal Processing Magazine, IEEE, 28(2):52-68, March 2011.
11
Under review as a conference paper at ICLR 2020
Yining Wang, Yu-Xiang Wang, and Aarti Singh. A deterministic analysis of noisy sparse subspace clustering
for dimensionality-reduced data. In Proceedings of the 32Nd International Conference on International
Conference on Machine Learning - Volume 37, ICML’15, pp. 1422-1431. JMLR.org, 2015.
Yu-Xiang Wang and Huan Xu. Noisy sparse subspace clustering. In Proceedings of the 30th International
Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pp. 89-97, 2013.
Yu-Xiang Wang, Huan Xu, and Chenlei Leng. Provable subspace clustering: When lrr meets ssc. In C.J.C.
Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger (eds.), Advances in Neural Information
Processing Systems 26, pp. 64-72. Curran Associates, Inc., 2013.
H.	Weyl. Das asymptotische verteilungsgesetz der eigenwerte linearer partieller differentialgleichungen (mit
einer anwendung auf die theorie der hohlraumstrahlung). Mathematische Annalen, 71:441-479, 1912.
Yingzhen Yang and Jiahui Yu. Fast proximal gradient descent for A class of non-convex and non-smooth sparse
learning problems. In Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence,
UAI 2019, Tel Aviv, Israel, July 22-25, 2019, pp. 508, 2019.
Yingzhen Yang, Jiashi Feng, Nebojsa Jojic, Jianchao Yang, and Thomas S. Huang. L0-sparse subspace clustering.
In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14,
2016, Proceedings, Part II, pp. 731-747, 2016.
Pavel Yaskov. Lower bounds on the smallest eigenvalue of a sample covariance matrix. Electron. Commun.
Probab., 19:10 pp., 2014.
Yu-Xiang Wang Yining Wang and Aarti Singh. Parameter estimation of generalized linear models without
assuming their link function. In Proceedings of the Eighteenth International Conference on Artificial
Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016, 2016.
Chong You, Daniel P. Robinson, and Rene Vidal. Scalable sparse subspace clustering by orthogonal matching
pursuit. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV,
USA, June 27-30, 2016, pp. 3918-3927, 2016.
Xin Zheng, Deng Cai, Xiaofei He, Wei-Ying Ma, and Xueyin Lin. Locality preserving clustering for image
database. In Proceedings of the 12th Annual ACM International Conference on Multimedia, MULTIMEDIA
’04, pp. 885-891, New York, NY, USA, 2004. ACM.
A Appendix
We provide proofs to the lemmas and theorems in the paper in this appendix.
A.1 Proof of Remark 1
Lemma A. (Subspace detection property holds for '0-SSC under the deterministic model) Under the determin-
istic model, suppose data is noiseless, nk ≥ dk + 1, Y (k) is in general position. If all the data points in Y (k)
are away from the external subspaces for any 1 ≤ k ≤ K, then the subspace detection property for '0 -SSC
holds with the optimal SOlUtiOn Z to (1).
Proof. Let Xi ∈ Sk. Note that Z*i is the optimal solution to the following '0 sparse representation problem
minkZik0 s.t. xi = [X(k) \xi X(-k)]Zi, Zii =0,	(40)
Zi
where X(-k) denotes the data that lie in all subspaces except Sk. Let Z*i =	Ce	where a and β are sparse
codes corresponding to X(k) \ xi and X (-k) respectively.
Suppose β = 0, then Xi belongs to a subspace S0 = HXZ*4 spanned by the projected data points corresponding
to nonzero elements of Z*i, and S = Sk, dim[S ] ≤ dk. To see this, if S= Sk, then the data corresponding
to nonzero elements of β belong to Sk, which is contrary to the definition of X(-k). Also, if dim[S 0] > dk,
then any dk points in X(k) can be used to linearly represent Xi by the condition of general position, contradicting
with the optimality of Z*i. Since the data points (or columns) in Xz*i are linearly independent, it follows that
Xi lies in an external subspace HX *i spanned by linearly independent points in XZ*i, and dim[HX *i] =
dim[S 0] ≤ dk. This contradicts with the assumption that Xi is away from the external subspaces. Therefore,
12
Under review as a conference paper at ICLR 2020
β = 0. Perform the above analysis for all 1 ≤ i ≤ n, we can prove that the subspace detection property holds
for all 1 ≤ i ≤ n.
□
A.2 Proof of Lemma 1
The following proposition is used for proving Lemma 1.
Lemma B. (Perturbation of distance to subspaces) Let A, B ∈ Rm×n are two matrices and rank(A) = r,
rank(B) = s. Also, E = A — B and ∣∣E∣∣2 ≤ C, where ∣∣∙ ∣∣2 indicates the spectral norm. Thenfor any point
x ∈ Rm, the difference of the distance of x to the column space of A and B, i.e. |d(x, HA) - d(x, HB)|, is
bounded by
1d(x, HA) - d(x, HB)l≤ min{σCA)kσs(b)} ∙	(41)
Proof. Note that the projection of x onto the subspace HA is AA+x where A+ is the Moore-Penrose
pseudo-inverse of the matrix A, so d(x, HA) equals to the distance between x and its projection, namely
d(x, HA) = kx — AA+xk2 . Similarly, d(x, HB) = kx — BB+xk2 .
It follows that
|d(x, HA) — d(x, HB)| = |kx — AA+xk2 — kx — BB+xk2|
≤ kAA+x — BB+xk2 ≤ kAA+ — BB+k2kxk2.	(42)
According to the perturbation bound on the orthogonal projection in Chen et al. (2016); Stewart (1977),
kAA+ —BB+k2 ≤ max{kEA+k2,kEB+k2}.	(43)
Since kEA+k2 ≤ kEk2∣∣A+k2 ≤ 缶,kEB+k2 ≤ kEk2∣∣B+k2 ≤ 品，combining (42) and (43), we
have
|d(x, HA) — d(x, HB)| ≤ max{
C C
σr(A), σs(B)
}kxk2
Ckxk2	.
min{σr(A),σs(B)} .
(44)
So that (5) is proved.
□
ProofofLemma 1. We have Ni = xi 一 ni, and σmin(Yβ> Ye) = (σmin(Ye))2 ≥ σY产
ByWeylWeyl (1912), ∣σi(Yβ) — σi(Xβ)| ≤ IlNek2 ≤ IlNeIlF ≤ √rδ. Since √rδ < σγ,r ≤ σmin(Yβ) ≤
σi(Yβ), σ<Xβ) ≥ σi(Yβ) — √rδ ≥ σγ,r — √r > 0 for 1 ≤ i ≤ min{d, r}. It follows that σmin(Xe) ≥
σγ ,r — √rδ > 0 and Xβ has full column rank.
Also, ∣Xβ — Yβ∣2 ≤ ∣∣Xβ — Yβ ∣∣f ≤ √rδ. According to Lemma B,
|d(xi , HXβ ) — d(xi , HYβ )|
≤________________________________________√rδ___________
min{σmin(Xβ ) , σmin (Ye ) }
≤	√rδ	= δ
一σγ ,r — √rδ	σγ ,r — 6.
(45)
□
A.3 Proof of Lemma 2
Proof.
kxi — Xβ*k2 + λkβ*k0 ≤ kxi - Xo∣2 + λ∣0∣o = 1
⇒ C* = kxi — Xβ*k2 < 1.
13
Under review as a conference paper at ICLR 2020
We first prove that β* is the optimal solution to the sparse approximation problem
min kβko s.t. ∣∣xi - Xβ∣∣2 ≤ c*, βi = 0.	(46)
To see this, suppose there is a vector β such that kXi — Xβ[∣2 ≤ c* and kβ0∣∣0 < kβ*∣∣0, then L(β0) <
c* + λkβ* ko = L(β*), contradicting the fact that β* is the optimal solution to (6).
Note that X@* is a full column rank matrix, otherwise a sparser solution to (6) can be obtained as vector whose
support corresponds to the maximal linear independent set of columns of Xβ*.
Also, the distance between Xi and the subspace spanned by columns of Xβ* equals to c*,i.e. d(xi, Hxq* ) = c*.
*	∖∕*TC,1
To see this, it is clear that d(Xi, HXβ* ) ≤ c*. If there is a vector y = Xβ in HXβ* with supp(β) ⊆
supp(β*), and ∣∣Xi — y∣∣2 < c*, then L(β) < L(β*) which contradicts the optimality of β*. Therefore,
d(Xi, HXβ* ) ≥ c*, and it follows that d(Xi, HXβ* ) = c*.
To prove that the subspace separation margin HS(Xi, X, β*) > 0, suppose HS(Xi, X, β*) ≤ 0, so there exists
β0 such that ∣β0∣0 < r*, rank(Xβ0) = ∣β0∣0 and d(yi, HXβ0 ) ≤ d(yi, HXβ* ) ≤ c*. Then β0 is sparser
than β* and it satisfies the constraint of problem (46), contradicting the optimality of β* .
Since ∣Xi — Xβ*∣2 ≤ 1, ∣Xβ*∣2 ≤ 2. Also,
σmin(Xβ>* Xβ*)∣β*∣22 ≤ ∣Xβ*∣22 ≤ 4,
it follows that kβ*∣2 ≤ —4ɪ. By Cauchy-Schwarz inequality, kβ*kι ≤ 2√√r* and kNβ* k2 ≤ kβ*kιδ ≤
σX	σX
2δ√r*. Therefore,
σX
IlXi- Yβ*∣∣2 = IlXi- Xβ* + Nβ*k2
≤ kxi- Xβ*∣2 + ∣Nβ*∣2 ≤ c* + 2δ^r*,
σX
so that β* is a feasible for problem (7). To prove that β* is also the optimal solution to (7), suppose this
is not the case, and the optimal solution to (7) is a vector β0 such that IXi - Yβ0 I2 ≤ c* + 2δ√√r* and
Iβ0 I0 = r < r* . Yβ0 is a full column rank matrix, otherwise a sparser solution can be obtained as vector whose
support corresponds to the maximal linear independent set of columns of Yβ0 . We have
d(χi, HYeO ) ≤ kχi - Yβ0∣∣2 ≤ c* +-*——.
σX
According to Lemma 1, we have
|d(Xi, Hχβo) - d(Xi, HYeo)| ≤ —rδ√^-
β	β σY ,r - rδ
=≤，
σ Y ,r — δ	σ Y — δ
⇒ d(Xi, Hχeo) ≤ c* + 2δ√r* + 一 δ = c* + τo.
σ	σ X	σ Y 一 δ
However, according to the optimality of β* in the noisy '0-SSC problem (6), we have
d(Xi, HXe0) - c* = d(Xi, HXe0) - d(Xi,HXe* )
≥ (r* - r)λ ≥ λ > τ0
This contradiction shows that β* is the optimal solution to (7).	□
A.4 Proof of Lemma 3
Proof. (8) is equivalent to the following problem
min IβI0 s.t. y = Yβ, IXi - yI2 ≤ c0, βi = 0.	(47)
We show that the points (columns) of Yβ* must come from subspace Sk. To see this, suppose some columns of
Yβ* come from different subspaces. We first have Iβ* I0 ≤ dk. To see this, we can choose some y0 ∈ Sk such
14
Under review as a conference paper at ICLR 2020
that ky0 - xi k2 ≤ c0 since c0 ≥ d(xi, Sk). Also, dk points in Y (k) can linearly represent y0 since Y (k) is in
general position, and it follows that kβ* k 0 ≤ dk due to the optimality of β*.
Also, Yβ* has full column rank, so that subspace Hγβ* ∈ Hy^卜.Let y* = Yβ*, then y* ∈ Hγβ* ∩
B(xi, co) which contradicts the fact that B(xi,c0) ∩ H = 0 for any H ∈ Hyidk. Therefore, columns of Yβ*
must come from Sk.	□
A.5 Proof of Theorem 1
Proof. We first show that d(xi, Sk) ≤ c* + 2δ-√*. To see this, σX = σmin(Xβ*) ≤ 1 as the columns of X
have unit `2 -norm. It follows that
2	2δλ∕r*	,—	..	..	.	.
C +----；— ≥ 2δ√r* ≤ 2δ > ∣∣xi 一 yi∣∣ ≤ d(xi, Sk)
σX
(48)
By Lemma 2, it can be verified that β* is the optimal solution to the following problem
....	..	......2δ√r^
min IleIIO s.t. ∣∣xi — Yβ∣∣2 ≤ C +----------, βi = 0.	(49)
β	σX
The subspace detection property holds which follows from applying Lemma 3 with co = C- + 2δσ*r*.	□
A.6 Proof of Theorem 2
Proof. This theorem can be proved by checking that the conditions in Theorem 1 are satisfied.	□
A.7 Proof of Lemma 4
Proof. Let H be a fixed subspace of dimension de ≤ dk, and y ∈/ H. Since y ∈ Sk and y ∈/ H. Let
USk = I0dk ∈ Rd×dk be the orthonormal basis of Sk under which the isotropic random vector y in
Sk satisfies E[yy>] = I0dk 00 . It follows that more columns vectors can be added to USk to form a
orthonormal basis U ∈ Rd×d0 for the minimum subspace that contains Sk and H. It can be verified that
dk + 1 ≤ d0 ≤ min{dk + de , d} because H 6= Sk . Note that U can be represented as a block matrix as
U = I0dk U00 where U0 ∈ R(d-dk)×(d0 -dk) has orthonormal columns. It can be verified that the basis of
H can be represented as UH	=	Ide -0d0 +dk	U00	. Note that if de 一	d0	+	dk	=	0, UH	=	U00	. Then
PH (y) = UH U>H y, and we have
E[IPH (y)I22] = E[y>UHU>HUHU>Hy]
= E[Tr(y>UHU>Hy)]
= E[Tr(U>Hyy>UH)]
= Tr(U>HE[yy>]UH)
Idk 0	Ide -d0 +dk	0
0	0	0 U0
de 一 d0 + dk ≤ de 一 1 ≤ dk 一 1	(50)
According to the concentration inequality in section 5.2 of Aubrun & Szarek (2017), for any t > 0,
........ >----------------.	.	.dι,t2.
Pr[|kPH(y)k2 一	ʌ/de	- d0	+ dk |	≥ t]	≤ 8exp(-2~)
(51)
Now let H be spanned by data from Y , i.e. H = H{y }de , where {yij }jd=e 1 are any de linearly independent
points that does not contain y. For any fixed points {yij }jd=e 1, (51) holds. Let A be the event that |PH(y) 一
7de - d0 + dk | ≥ t, we aim to integrate the indicator function 1a with respect to the random vectors, i.e. y
and {yij }jd=e 1, to obtain the probability that A happens over these random vectors. Let y = yi, using Fubini
theorem, we have
15
Under review as a conference paper at ICLR 2020
Pr[A] = /	1A刈=即5
×jn=1S(j)
=×
≤Z×
j6=iS(j)
Pr[Al{yj }j=i 怯 j=id”(j)
8exp(- dkt- )0j=idμ(j) =8exp(-dkt-)
j 6=iS(j)	2	2
(52)
where S (j) ∈ {Sk }K=ι is the subspace that yj lies in, and μ(j) is the probabilistic measure of the distribution in
S(j). The last inequality is due to (51).
Note that for any y’s external subspace H
Pdk - ∣∣PH(y)k2∙ According to (52), we have
H{yij}jd=e 1, d(y, H)
√kyk2 -kPH(y)k2
一 .	/------ C.	. dut2.
Pr[d(y, H) ≥ 1 — 2t∖∕dk — 1 — t ] ≥ 1 — 8exp(-----2—).
(53)
□
A.8 Proof of Theorem 3
Proof. According to Yaskov (2014) and condition (a), with probability at least 1 — exp(—d), σmin (Yβ ≥
√196Md^+Γ — 14√Md for any β ∈ Rn such that kβko = r ≤ d, rank(Ye) = ∣∣βko. It follows that
σγ,r ≥ √196Md + 1 — 14√Md. ByWeyIWeyl (1912), ∣σmin(Xβ) — σmin(Yβ)| ≤ ∣∣Ne∣∣2 ≤ δ√r0.
Therefore, σmin(Xβ) ≥ √196Md^+Γ — 14√Md — δ√r0 > 0 if δ < "196Md++1-14√Md = c. It can be
verified that (20), (21) and (22) guarantee (12), (13) and (14) in Theorem 2 respectively, therefore, the conclusion
holds.	口
A.9 Proof of Theorem 4
It is proved that the low rank approximation X is close to X in terms of the spectral norm Halko et al. (2011):
Lemma C. (Corollary 10.9 in Halko et al. (2011)) Let p0 ≥ 2be an integer and p0 = p — p0 ≥ 4, then with
p
probability at least 1 — 6e-p, the spectral norm of X — X is bounded by
∣X - X ∣2 ≤ Cp,P0
(54)
where
Cp,po , (1 + 17y 1 + P)σp0+1 +
。2)2
(55)
and σ1 ≥ σ2 ≥ . . . are the singular values of X.
Before proving Theorem 4, we present the following lemma on the perturbation bound for the distance between a
data point and a subspace before and after the projection P. Each subspace Sk is transformed into Sk = P(Sk)
with dimension dk.
n
Lemma D. Let β ∈ R , y = Pyi, Hye is an external subspace of y, Ye = P(Ye) and Ye has full column
rank. Then
∣d(yi, Hγβ) — d(yi, HYe)|
1
≤ Cp,p0 (1 +
min1≤r≤淳 σY,r
Cp,po — 2δpdk)
(56)
—
for any 1 ≤ i ≤ n and yi ∈ Sk.
Proof. This lemma can be proved by applying Lemma B.
□
Proof of Theorem 4. For any matrix A ∈ Rp×q,
we first show that multiplying Q to the left of A would not
change its spectrum. To see this, let the singular value decomposition of A be A = UAΣVA> where UA
16
Under review as a conference paper at ICLR 2020
and VA have orthonormal columns with U>AUA = VA> VA = I. Then QA = UQAΣVQA is the singular
value decomposition of QA with UQA = QUA and VQA = VA . This is because the columns of UQA are
orthonormal since the columns Q are orthonormal: U>QA UQA = U>AQ>QUA = I, and Σ is a diagonal
matrix with nonnegative diagonal elements. It follows that σmin(QA) = σmin(A) for any A ∈ Rp×q.
For a point Xi = yi + ni, after projection via P, We have the projected noise ni = Pni. Because
knik2 = kPnik2 = kQ>nik2 ≤ kQk2knik2 ≤ kmk2 ≤ δ,	(57)
the magnitude of the noise in the projected data is also bounded by δ. Also,
IlxiIl2 = kQ>χi∣∣2 ≤ Ilxik2 ≤ 1,	(58)
_ _ _ __________ .. ... , _ . ,..
Let β ∈ R , Yβ = PYβ with kβko = r. Then σmin(QY⅞) = σmin(环)).Since
1	f -i^r ∖	/■< 7- ∖ 1	1	/ W ∖	/■< 7- ∖ 1
lσmin(Yβ) -
σmin(Ye )| = ।σmin(QYβ)-
σmin(Ye )|
≤ kQYβ - Yek2
= IQQ>Yβ -YβI2
= IQQ>Xβ -Xβ+Nβ - QQ>NβI2
≤ Cp,p0+ kNβ kF + kQQ>Nβ kF
≤ Cp,P0 + 2δ√r	(59)
Therefore, it follows from (59) that if
Cp,P0 +2δ/dmax < min σYk),	(60)
k=1,...,K
then Y is also in general position.
In addition, since λ ≥ -1, we have λ∣∣β*ko ≤ L(0) ≤ 1, and it follows that kβ*∣∣0 ≤ 1 ≤ ro.
Based on (59) we have
∣σγ,r — σγ,r| ≤ Cp,po + 2δ√r0,	(61)
it follows that δ < minι≤-<-o σ-γ - because δ < minι≤-<-o σγ,- - Cpp — 2δ√r0.
Again, for β ∈ Rn with kβk0 = r ≤ r0, we have
|σmin(Xβ) - σmin(Xβ) | = 1σmin(Qχβ) - σmin(Xβ)[
≤ kQXβ - Xβk2
=kQQ>Xβ - Xβk2 = kX - Xβk2
≤ Cp,p0	(62)
It can be verified that
lσXX ,r - σX,r | ≤ CP,PO	(63)
Combining (63) and Lemma D, noting that σX,r0 - Cp,p0, since
Mi - Cp,p0 (1 +
-------------L^K )
mini≤r≤⅛ σY,r - CP,P0 - 2δ√dk
we have
where yi ∈ Sk .
>δ+
2δ
σX
,r0 - Cp,p0
Mi,δ，Mi —
2δ
δ > -——
σX ,ro
Based on (61) and (63), we have
μro
<1-
2δ
σX ,ro
because
δ	2δ
-........Z------------ -- < 1-------------------
min1≤r<ro σY,ro - Cp,po - 2δy∕r0 - δ	σX,ro - Cp,po
(64)
(65)
(66)
(67)
□
17
Under review as a conference paper at ICLR 2020
A.10 S ketch of Proof of Theorem 5
We first present Theorem 5 in Yang & Yu (2019). Let g(x) = ky - Dxk22, y ∈ Rd, D is the design matrix of
dimension d × n. Let x* be the globally optimal solution to
minn F(x) = ky - Dxk22 + λkxk0,	(68)
**
S = SuPP(X ), X be the SUbOPtimal solution to (68) obtained by Proximal Gradient Descent (PGD), S =
SuPP(X). The following theorem presents the bound between X and x*.
Theorem A. (Theorem 5 in Yang & Yu (2019)) Suppose Ds∪s* has full column rank with κo ，
σmin (Ds∪s* ) > 0 where S is the support of the initialization for PGD on problem (68). Let κ > 0 such that
2κ02 > κ and b is chosen according to (69) as below:
0 <b< min{min |xj |,---λ∂τ∣—I, min|x* |,--------λ∂τ∣--T}.
j∈s	maxj∈^ | ∂Xj |x=x | j∈S	maχj∈s* | ∂Xj |x=x*|
**	*
Let F = (S \ S*) ∪ (S* \ S) be the symmetric difference between S and S*, then
(69)
kx - X*k2 ≤ 2R21 A
2κ0 - κ
(X (maχ{0,jb - K|Xj - b|})2 + X (maχ{0, λ — κbD2)1
j∈F∩^
___
j∈F∖S
(70)
Sketch GfProofofTheorem 5. It can be verified that max{0, λ — κ∖∕βj - b|} = 0 and max{0, λ — κb} = 0
1	，1	1-i-	∕CC∖	1 ∕CC∖ i 1	*	*	* * 1	1	■	El	A	I--1
under the conditions	(38) and (39), therefore,	β =	β by applying	Theorem A.	□
18