Under review as a conference paper at ICLR 2020
ODE Analysis of Stochastic Gradient Meth-
ods with Optimism and Anchoring for Minimax
Problems and GANs
Anonymous authors
Paper under double-blind review
Ab stract
Despite remarkable empirical success, the training dynamics of generative adver-
sarial networks (GAN), which involves solving a minimax game using stochastic
gradients, is still poorly understood. In this work, we analyze last-iterate con-
vergence of simultaneous gradient descent (simGD) and its variants under the
assumption of convex-concavity, guided by a continuous-time analysis with dif-
ferential equations. First, we show that simGD, as is, converges with stochastic
sub-gradients under strict convexity in the primal variable. Second, we generalize
optimistic simGD to accommodate an optimism rate separate from the learning
rate and show its convergence with full gradients. Finally, we present anchored
simGD, a new method, and show convergence with stochastic subgradients.
1	Introduction
Training of generative adversarial networks (GAN) (Goodfellow et al., 2014), solving a minimax
game using stochastic gradients, is known to be difficult. Despite the remarkable empirical success
of GANs, further understanding the global training dynamics empirically and theoretically is consid-
ered a major open problem (Goodfellow, 2016; Radford et al., 2016; Metz et al., 2017; Mescheder
et al., 2018; Odena, 2019).
The local training dynamics of GANs is understood reasonably well. Several works have analyzed
convergence assuming the loss functions have linear gradients and assuming the training uses full
(deterministic) gradients. Although the linear gradient assumption is reasonable for local analysis
(even though the loss functions may not be continuously differentiable due to ReLU activation func-
tions) such results say very little about global convergence. Although the full gradient assumption
is reasonable when the learning rate is small, such results say very little about how the randomness
affects the training.
This work investigates global convergence of simultaneous gradient descent (simGD) and its vari-
ants for zero-sum games with a convex-concave cost using using stochastic subgradients. We
specifically study convergence of the last iterates as opposed to the averaged iterates.
Organization. Section 2 presents convergence of simGD with stochastic subgradients under strict
convexity in the primal variable. The goal is to establish a minimal sufficient condition of global
convergence for simGD without modifications. Section 3 presents a generalization of optimistic
simGD (Daskalakis et al., 2018), which allows an optimism rate separate from the learning rate. We
prove the generalized optimistic simGD using full gradients converges, and experimentally demon-
strate that the optimism rate must be tuned separately from the learning rate when using stochastic
gradients. However, it is unclear whether optimistic simGD is theoretically compatible with stochas-
tic gradients. Section 4 presents anchored simGD, a new method, and presents its convergence with
stochastic subgradients. Anchoring represents what we consider to be the strongest contribution of
this work. The presentation and analyses of Sections 2, 3, and 4 are guided by continuous-time first-
order ordinary differential equations (ODE). In particular, we interpret optimism and anchoring as
discretizations of certain regularized dynamics. Section 5 experimentally demonstrates the benefit
of optimism and anchoring for training GANs in some setups.
1
Under review as a conference paper at ICLR 2020
Prior work. There are several independent directions for improving the training of GANs such
as designing better architectures, choosing good loss functions, or adding appropriate regularizers
(Radford et al., 2016; Arjovsky et al., 2017; S0nderby et al., 2017; Arjovsky & Bottou, 2017; Gul-
rajani et al., 2017; Wei et al., 2018; Roth et al., 2017; Mescheder et al., 2018; 2017; Miyato et al.,
2018). In this work, we accept these factors as a given and focus on how to train (optimize) the
model effectively.
Optimism is a simple modification to remedy the cycling behavior of simGD, which can occur
even under the bilinear convex-concave setup (Daskalakis et al., 2018; Daskalakis & Panageas,
2018; 2019; Mertikopoulos et al., 2019; Gidel et al., 2019a; Liang & Stokes, 2019; Mokhtari et al.,
2019; Peng et al., 2019). These prior work assume the gradients are linear and use full gradients.
Although the recent name ‘optimism’ originates from its use in online optimization (Chiang et al.,
2012; Rakhlin & Sridharan, 2013a;b; Syrgkanis et al., 2015), the idea dates back to Popov’s work
in the 1980s (Popov, 1980) and has been studied independently in the mathematical programming
community (Malitsky & Semenov, 2014; Malitsky, 2015; Malitsky & Tam, 2018; Malitsky, 2019;
Csetnek et al., 2019).
We note that there are other mechanisms similar to optimism and anchoring such as “prediction”
(Yadav et al., 2018), “negative momentum” (Gidel et al., 2019b), and “extragradient” (Korpelevich,
1976; Tseng, 2000; Chavdarova et al., 2019). In this work, we focus on optimism and anchoring.
Classical literature analyze convergence of the Polyak-averaged iterates (which assigns less weight
to newer iterates) when solving convex-concave saddle point problems using stochastic subgradients
(Bruck, 1977; Nemirovski & Yudin, 1978; Nemirovski et al., 2009; Juditsky et al., 2011; Gidel et al.,
2019a). For GANs, however, last iterates or exponentially averaged iterates (YazIcI et al., 2019)
(which assigns more weight to newer iterates) are used in practice. Therefore, the classical work
with Polyak averaging do not fully explain the empirical success of GANs.
We point out that we are not the first to utilize classical techniques for analyzing the training of
GANs. In particular, the stochastic approximation technique (Heusel et al., 2017; Duchi & Ruan,
2018), control theoretic techniques (Heusel et al., 2017; Nagarajan & Kolter, 2017), ideas from
variational inequalities and monotone operator theory (Gemp & Mahadevan, 2018; Gidel et al.,
2019a), and continuous-time ODE analysis (Heusel et al., 2017; Csetnek et al., 2019) have been
utilized for analyzing GANs.
2	Stochastic simultaneous subgradient descent
Consider the cost function L : Rm × Rn → R and the minimax game minx maxu L(x, u). We say
(x?, u?) ∈ Rm × Rn is a solution to the minimax game ora saddle point of L if
L(x?, u) ≤ L(x?, u?) ≤ L(x, u?),	∀x ∈ Rm, u ∈ Rn .
We assume
L is convex-concave and has a saddle point.	(A0)
By convex-concave, we mean L(x, u) is a convex function in x for fixed u and a concave function
in u for fixed x. Define
∂x L(x, u)
G(x, u) = ∂u(-L(x, u)) ,
where ∂x and ∂u respectively denote the subdifferential with respect to x and u. For simplicity,
write z = (x, u) ∈ Rm+n and G(z) = G(x, u). Note that 0 ∈ G(z) if and only if z is a saddle
point. Since L is convex-concave, the operator G is monotone (Rockafellar, 1970):
(g1 - g2)T (z1	-	z2)	≥ 0	∀g1	∈ G(z1),	g2	∈	G(z2),	z1, z2 ∈	Rm+n.	(1)
Let g(z; ω) be a stochastic subgradient oracle, i.e., Eωg(z; ω) ∈ G(z) for all z ∈ Rm+n, where ω is
a random variable. Consider Simultaneous Stochastic Sub-Gradient Descent
zk+1 = zk - αkg(zk; ωk)	(SSSGD)
for k = 0, 1, . . . , where z0 ∈ Rm+n is a starting point, α0, α1, . . . are positive learning rates, and
ω0, ω1, . . . are IID random variables. (We read SSSGD as “triple-SGD”.) In this section, we provide
convergence of SSSGD when L(x, u) is strictly convex in x.
2
Under review as a conference paper at ICLR 2020
Figure 1: z(t) with Z(t) = -G(z(t)). (Left) L(x,u) = xu. All points satisfy G(Z)T(Z - z?) = 0
so kz(t) - z?k does not decrease and z(t) forms a cycle. (Right) L(x, u) = 0.2x2 +xu. The dashed
line denotes where G(Z)T(Z - Z?) = 0, but it is visually clear that Z? = 0 is the only cluster point.
2.1	Continuous-time illustration
To understand the asymptotic dynamics of the stochastic discrete-time system, we consider a cor-
responding deterministic continuous-time system. For simplicity, assume G is single-valued and
smooth. Consider
Z(t) = -g(t),	g(t) = G(z(t))
with an initial value Z(0) = Z0. (We introduce g(t) for notational simplicity.) Let Z? be a saddle
point, i.e., G(Z?) = 0. Then Z(t) does not move away from Z?:
dt2l∣z(t) - z?k = -g⑴T(Z⑴-z?) ≤ 0,
where we used (1). However, there is no mechanism forcing Z(t) to converge to a solution.
Consider the two examples Lo(χ, U) = Xu and Lρ(χ, U) = (ρ∕2)x2 + Xu with
G0(x, u) = -01 10 ux , Gρ (x, u) = -ρ1 01 ux	(2)
where X ∈ R and u ∈ R and ρ > 0. Note that L0 is the canonical counter example that also arises
as the Dirac-GAN (Mescheder et al., 2018). See Figure 1.
The classical LaSane-Krasnovskii invariance principle (Krasovskii, 1959; LaSalle, 1960) states
(paraphrased) if Z∞ is a cluster point of Z(t), then the dynamics starting at Z∞ will have a constant
distance to z?. On the left of Figure 1, we can see ∣∣z(t) - z?12 is constant as d 1 ∣∣z(t) - z? ∣∣2 = 0
for all t. On the right ofFigure 1, we can see that although 舟 2 ∣∣z(t) - z*∣2 = 0 when z(t) = (0,u)
for u 6= 0 (the dotted line) this 0 derivative is temporary as z(t) will soon move past the dotted line.
Therefore, z(t) can maintain a constant constant distance to z? only ifit starts at 0, and 0 is the only
cluster point of z(t).
2.2	Discrete-time convergence analysis
Consider the further assumptions
∞∞
αk = ∞,	α2k < ∞	(A1)
k=0	k=0
Eω1 ,ω2 ∣g(z1; ω1)	-	g(z2 ; ω2 )∣2 ≤	R21∣z1	- z2 ∣2 + R22	∀ z1, z2	∈	Rm+n,	(A2)
where ω1 and ω2 are independent random variables and R1 ≥ 0 and R2 ≥ 0. These assumptions
are standard in the sense that analogous assumptions are used in convex minimization to establish
almost sure convergence of stochastic gradient descent.
Theorem 1. Assume (A0), (A1), and (A2). Furthermore, assume L(X, u) is strictly convex in X for
all u. Then SSSGD converges in the sense of zk →a.s. z? where z? is a saddle point of L.
We can alternatively assume L(X, u) is strictly concave in u for all X and obtain the same result.
3
Under review as a conference paper at ICLR 2020
The proof uses the stochastic approximation technique of (Duchi & Ruan, 2018). We show that
the discrete-time process converges (in an appropriate topology) to a continuous-time trajectory
satisfying a differential inclusion and use the LaSane-Krasnovskiiinvariance principle to argue that
cluster points are solutions.
Related prior work. Theorem 3.1 of (Mertikopoulos et al., 2019) considers the more general
mirror descent setup and proves convergence under the assumption of “strict coherence”, which is
analogous to the stronger assumption of strict convex-concavity in both x and u.
3	Simultaneous GD with optimism
Consider the setup where L is continuously differentiable and we access full (deterministic) gradi-
ents
G(x, u)
VχL(x,u)
-NuL(x,u)
Consider Optimistic Simultaneous Gradient Descent
zk+1 = zk - αG(zk) - β (G(zk) - G(zk-1))	(SimGD-O)
for k ≥ 0, where z0 ∈ Rm+n is a starting point, z-1 = z0, α > 0 is learning rate, and β > 0 is
the optimism rate. Optimism is a modification to simGD that remedies the cycling behavior; for the
bilinear example L0 of (2), simGD (case β = 0) diverges while SimGD-O with appropriate β > 0
converges. In this section, we provide a continuous-time interpretation of SimGD-O as a regularized
dynamics and provide convergence for the deterministic setup.
3.1	Continuous-time illustration
Consider the continuous-time dynamics
z(t) = -αg(t) - βg(t),	g(t) = G(z(t)).
The discretization Z(t) ≈ zk+ι — Zk and g(t) ≈ G(Zk) 一 G(zk-ι) yields SimGD-O. We discuss
how this system arises as a certain regularized dynamics and derive the convergence rate
kg(t)k2 ≤ O(1/t).
Regularized gradient mapping. The Moreau-Yosida (Moreau, 1965; Yosida, 1948) regulariza-
tion of G with parameter β > 0 is
Gβ = β-1(I - (I + βG)-1).
To clarify, I : Rm+n → Rm+n is the identity mapping and (I+βG)-1 is the inverse (as a function)
of I + βG, which is well-defined by Minty’s theorem (Minty, 1962). It is straightforward to verify
that Gβ (Z) = 0 if and only if G(Z) = 0, i.e., Gβ and G share the same equilibrium points. For
small β, we can think of Gβ as an approximation G that is better-behaved. Specifically, G is merely
monotone (satisfies (1)), but Gβ is furthermore β-cocoercive, i.e.,
(Gβ (Z1) - Gβ (Z2))T (Z1 - Z2) ≥ βkGβ(Z1) - Gβ (Z2)k2	∀Z1, Z2 ∈ Rm+n.	(3)
Regularized dynamics. Consider the regularized dynamics
,,. ,..
ζ(t) = -αGβ (Z(t)).
Reparameterize the dynamics Z(t) = -αG.(Z(t)) with z(t) = (I + βG)-1 (Z(t)) and g(t)=
G(Z(t)) to get ζ(t) = Z(t) + βg(t) and
α
z(t) + βg(t) = Z(t)=-用(ζ(t) - z(t)) = -αg(t).
β
This gives US Z(t) = -αg(t) — βg(t).
4
Under review as a conference paper at ICLR 2020
Rate of convergence. We now derive a rate of convergence. Let z? satisfy G(z?) = 0 (and
therefore Gβ (z?) = 0). Then
ddt 2 kZ (t) — z?k2 = (Z (t) — z? )T Z(t) = -α(Z (t) — z?)T Ge (Z (t))
≤ -αβkGβ(ζ(t))k2,
where we use cocoercivity, (3). This translates to
dt2kz(t) + βg⑴- z?k2 ≤ -αβkg⑴k2.
The quantity kg(t)k2 is nonincreasing since
万 5kg⑴ k2 = Z⑴T g⑴=-Jm 2( at+h) — Z(U)T (Ge at+h)) — Ge (Z(U))
dt2	α	α h→0 h2
≤ — α hhim0 hkGe(Z(t + h)) — Ge(Z(U)k = -akg(t)k2 ≤ 0,
where we use cocoercivity, (3). Finally, integrating (4) on both sides gives us
11	t
2kz(t) + βg(t) — z?k2 — 2kz(0) + βg(0) — z?k2 ≤ —αβ J kg(s)k2 ds ≤ —αβtkg(t)k2
kg(t)k2 ≤ ɪkz(0) + βg(0) — z?k2.
2αβt
(4)
Related prior work. The use of the MoreaU-Yoshida regularization for the continuous-time anal-
ysis was inspired by Attouch et al. (Attouch et al., 2002; Attouch & Peypouquet, 2019) who first
used the Moreau-Yosida regularization in continuous-time dynamics and Csetnek et al. (2019) who
interpreted a forward-backward-forward-type method as a discretization of continuous-time dynam-
ics with the Douglas-Rachford operator. Daskalakis et al. (2018) interprets optimism as augmenting
“follow the regularized leader” with the (optimistic) prediction that the next gradient will be the same
as the current gradient in online learning setup. Peng et al. (2019) interprets optimism as “centripetal
acceleration” but does not provide a formal analysis with differential equations.
3.2	Discrete-time convergenece analysis
The discrete-time method SimGD-O converges under the assumption
L is differentiable and VL is R-LiPschitz continuous.
(A3)
Theorem 2. Assume (A0) and (A3). If 0 < α < 2β (1— 2βR), then SimGD-O converges in the
sense of
min kG(zk)k2 ≤
i=0,...,k
2 + 2β 2R2
α(2β — α — 4β2R)k
kz0 + βG(z0) — z?k2.
Furthermore, zk → z?, where z? is a saddle point of L.
The Proof can be considered a discretization of the continuous-time analysis. We further discuss the
similarities and differences between the continuous and discrete analyses in Section A.
Corollary 1. In the setup of Theorem 2, the choice α = 1/(8R) and β = 2α yields
min ∣∣G(zk)k2 ≤ 136R2 kz0 + βG(z0) - z?k2 ≤ 289R2 IE - z?k2 ∙
i=0,...,k	k	k
Related prior work. Peng et al. (2019) show convergence of simGD-O for α 6= β and bilinear
L. Malitsky & Tam (2018) and Csetnek et al. (2019) show convergence of simGD-O for α = β
and convex-concave L. Theorem 2 establishes convergence for α 6= β and convex-concave L and
Presents an exPlicit rate.
5
Under review as a conference paper at ICLR 2020
Figure 2: Plot of kzk - z? k2 vs. iteration count for simGD-OS (left) and SSSGD-A (right) with
αk = 1/kp and βk = 1/kq. We use L0 of (2) and Gaussian random noise. The shaded region
denotes ± standard error. For simGD-OS, we see that neither q = 0 nor q = p leads to convergence.
Rather, q must satisfy 0 < q < p so that the learning rate diminishes faster than the optimism rate.
3.3	Difficulty with stochastic gradients
Training in machine learning usually relies on stochastic gradients, rather than full gradients. We
can consider a stochastic variation of SimGD-O:
zk+1 = zk - αkg(zk; ωk) - βk(g(zk; ωk) - g(zk-1; ωk-1))	(SimGD-OS)
with learning rate αk and optimism rate βk .
Figure 2 presents experiments of SimGD-OS on a simple bilinear problem. The choice βk = αk
where αk → 0 does not lead to convergence. Discretizing Z(t) = -αg(t) -βg(t) with a diminishing
step hk leads to the choice αk = αhk and βk = β, but this choice does not lead to convergence
either. Rather, it is necessary to tune αk and βk separately as in Theorem 2 to obtain convergence
and dynamics appear to be sensitive to the choice of αk and βk. In particular, both αk and βk
must diminish and αk must diminish faster than βk. One explanation of this difficulty is that the
finite difference approximation a-1(g(zk; ωk) - g(zk-ι; ωk-ι)) ≈ g(t) is unreliable when using
stochastic gradients.
Whether the observed convergence holds generally in the nonlinear convex-concave setup and
whether optimism is compatible with subgradients is unclear. This motivates anchoring of the fol-
lowing section which is provably compatible with stochastic subgradients.
Related prior work. Gidel et al. (2019a) show averaged iterates of SimGD-OS converge if iterates
are projected onto a compact set. Mertikopoulos et al. (2019) show almost sure convergence of
SimGD-OS under strict convex-concavity (and more generally under “strict coherence”). However,
such analyses do not provide a compelling reason to use optimism since SimGD without optimism
already converges under these setups.
4	Simultaneous GD with anchoring
Consider setup of Section 3. We propose Anchored Simultaneous Gradient Descent
zk + 1 = zk - fk SzJG(Zk) +( L JF (z0 - zk)	(SimGD-A)
(k+	1)p k+	1
for k ≥ 0, where z0 ∈ Rm+n is a starting point, p ∈ (1/2, 1), and γ > 0 is the anchor rate. In this
section, we provide a continuous-time illustration of SimGD-A and provide convergence for both
the deterministic and stochastic setups.
4.1	Continuous -time illustration
Consider the continuous-time dynamics
z(t) = -g(t) + Y(zo - z(t)),	g(t) = G(z(t)).
6
Under review as a conference paper at ICLR 2020
for t ≥ 0, where γ ≥ 1 and z(0) = z0. We will derive the convergence rate
kg(t)k2 ≤ O(1/t2).
Discretizing the continuous-time ODE with diminishing steps (1 - p)/(k + 1)p leads to SimGD-A.
Rate of convergence. First note
0 ≤ h12 hz(t + h) - z(t), g(t + h) - g(t)i → hz(t), g(t)i	as h → 0.
Using this, we have
dt 2 kz⑴k2 =-Dz⑴,g⑴+∕z⑴+~2 (ZO—Ht))E
=-hz(t),g(t)i- Jkz(t)k2 + γhz(t) — zo,zi
≤ — Y kz(t)k2 + t2 hz(t) — z0,zi.
Using γ ≥ 1, we have
d2 kz(t)k2 + 7kz(t)k2 ≤ τ2hz⑴- zo,zi.
dt 2	t	t
Multiplying by t2 and integrating both sides gives us
■2 kz⑴k2 ≤ 2kz⑴ — zok2.
Reorganizing, we get
-2llg(t)k2 — Ythg⑴,zo — z⑴〉+ Y2kz⑴- zok2 ≤ 2kz⑴- zok2
Using γ ≥ 1, the monotonicity inequality, and Young’s inequality, we get
kg(t)k2 ≤ -γ-hg⑴,zo - z⑴〉≤ 十hg⑴,zo - z?i ≤ 2kg(t)k2 + -γτkzo — z?k2
and conclude
kg(t)k2 ≤ ~7k~kzo - z?k2.
t2
Interestingly, anchoring leads to a faster rate O(1/t2 ) compared to the rate O(1/t) of optimism in
continuous time. The discretized method, however, is not faster than O(1/k). We further discuss
this difference in Section A.
Related prior work. Anchoring was inspired by Halpern’s method (Halpern, 1967; Wittmann,
1992; Lieder, 2017) and James-Stein estimator (Stein, 1956; James & Stein, 1961); these methods
pull/shrink the iterates/estimator towards a specified point zo .
4.2	Discrete-time convergenece analysis
We now present convergence results with anchoring. In Theorem 3, we use deterministic gradients,
and in Theorem 4, we use stochastic subgradients.
Theorem 3. Assume (A0) and (A3). If p ∈ (1/2, 1) and Y ≥ 2, then SimGD-A converges in the
sense of
kG(zk)k2 ≤O(k2-2P)
The proof can be considered a discretization of the continuous-time analysis.
Consider the setup of Section 2. We propose Anchored Simultaneous Stochastic SubGradient De-
scent
zk+1 = zk — (k + 1)p g(Zk； ωk) + (k+ 1p)-ε (z0 — Zk)
(SSSGD-A)
(The small ε > 0 is introduced for the proof of Theorem 4. See Section A for further discussion.)
7
Under review as a conference paper at ICLR 2020
Figure 3: Samples of generated MNIST and CIFAR-10 images at the end of the training periods of
anchored Adam.
O	50000	100000	150000	200000 O	50000	1OOOOO 150000	200000
Iterations	Iterations
Figure 4: FID score vs. iteration on MNIST (left) and CIFAR-10 (right). Optimism rate ofβ = 1 and
anchor rate of γ = 1 was used. The MNIST setup benefits from optimism but not from anchoring,
while the CIFAR-10 setup benefits from optimism but not from anchoring.
Theorem 4. Assume (A0) and (A2). If p ∈ (1/2, 1), ε ∈ (0, 1/2), and γ > 0, then SSSGD-A
converges in the sense of zk →L z?, where z? is a saddle point.
(To clarify, we do not assume L is differentiable.)
Main contribution. To the best of our knowledge, Theorem 4 is the first result establishing last-
iterate convergence for convex-concave cost functions using stochastic subgradients without assum-
ing strict convexity or analogous assumptions.
5	Experiments
In this section, we experimentally demonstrate the effectiveness of optimism and anchoring for
training GANs. We train Wasserstein-GANs (Arjovsky et al., 2017) with gradient penalty (Gulrajani
et al., 2017) on the MNIST and CIFAR-10 dataset and plot the Frechet Inception Distance (FID)
(Heusel et al., 2017; Lucic et al., 2018). The experiments were implemented in PyTorch (Paszke
et al., 2017). We combine Adam with optimism and anchoring (described precisely in Appendix G)
and compare it against the baseline Adam optimizer (Kingma & Ba, 2015). The generator and
discriminator architectures and the hyperparameters are described in Appendix G. For optimistic and
anchored Adam, we roughly tune the optimism and anchor rates and show the curve corresponding
to the best parameter choice.
Figure 4 shows that the MNIST setup benefits from anchoring but not from optimism, while the
CIFAR-10 setup benefits from optimism but not from anchoring. We leave comparing the effects of
optimism and anchoring in practical GAN training (where the cost function is not convex-concave)
as a topic of future work.
8
Under review as a conference paper at ICLR 2020
6	Conclusion
In this work, we analyzed the convergence of SSSGD, Optimistic simGD, and Anchored SSSGD.
Under the assumption that the cost L is convex-concave, Anchored SSSGD provably converges
under the most general setup. Through experiments, we showed that the practical GAN training
benefits from optimism and anchoring in some (but not all) setups.
Generalizing these results to accommodate projections and proximal operators, analogous to pro-
jected and proximal gradient methods, is an interesting direction of future work. Weight clipping
(Arjovsky et al., 2017) and spectral normalization (Miyato et al., 2018) are instances where projec-
tions are used in training GANs.
References
M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial net-
works. ICLR, 2017.
M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. ICML, 2017.
H. Attouch and J. Peypouquet. Convergence of inertial dynamics and proximal algorithms governed
by maximally monotone operators. Math. Program.,174(1):391-432, 2019.
H. Attouch, A. Cabot, and P. Redont. The dynamics of elastic shocks via epigraphical regularization
of a differential inclusion. Advances in Mathematical Sciences and Applications, 12(1):273306,
2002.
J. P. Aubin and A. Cellina. Differential Inclusions: Set-Valued Maps and Viability Theory. Springer-
Verlag, 1984.
H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert
Spaces. Springer-Verlag, 2nd edition, 2017.
R. E. Bruck. On the weak convergence ofan ergodic iteration for the solution of variational inequali-
ties for monotone operators in Hilbert space. Journal of Mathematical Analysis and Applications,
61(1):159-164, 1977.
T. Chavdarova, G. Gidel, F. Fleuret, and S. Lacoste-Julien. Reducing noise in GAN training with
variance reduced extragradient. NeurIPS, 2019.
C.-K. Chiang, T. Yang, C.-J. Lee, M. Mahdavi, C.-J. Lu, R. Jin, and S. Zhu. Online optimization
with gradual variations. COLT, 2012.
E. R. Csetnek, Y. Malitsky, and M. K. Tam. Shadow Douglas-Rachford splitting for monotone
inclusions. Applied Mathematics & Optimization, 2019.
C. Daskalakis and I. Panageas. The limit points of (optimistic) gradient descent in min-max opti-
mization. NeurIPS, 2018.
C. Daskalakis and I. Panageas. Last-iterate convergence: Zero-sum games and constrained min-max
optimization. ITCS, 2019.
C. Daskalakis, A. Ilyas, V. Syrgkanis, and H. Zeng. Training GANs with optimism. ICLR, 2018.
Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D. Lee. Stochastic subgradient
method converges on tame functions. Foundations of Computational Mathematics, 2019.
A. Dembo. Lecture notes on probability theory: Stanford statistics 310. http://statweb.
stanford.edu/~adembo/stat-310b/lnotes.pdf, 2019. Accessed: 2019-05-10.
J. Duchi and F. Ruan. Stochastic methods for composite and weakly convex optimization problems.
SIAM J. Optim., 28(4):3229-3259, 2018.
I. Gemp and S. Mahadevan. Global convergence to the equilibrium of GANs using variational
inequalities. arXiv:1808.01531, 2018.
9
Under review as a conference paper at ICLR 2020
G. Gidel, H. Berard, G. Vignoud, P. Vincent, and S. Lacoste-Julien. A variational inequality per-
spective on generative adversarial networks. ICLR, 2019a.
G. Gidel, M. Pezeshki R. A. Hemmat, R. Lepriol, G. Huang, S. Lacoste-Julien, and I. Mitliagkas.
Negative momentum for improved game dynamics. AISTATS, 2019b.
I. Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv:1701.00160, 2016.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. NeurIPS, 2014.
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of
Wasserstein GANs. NeurIPS, 2017.
B. Halpern. Fixed points of nonexpanding maps. Bull. Amer. Math. Soc., 73(6):957-961, 1967.
M.	Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by a two
time-scale update rule converge to a local Nash equilibrium. NeurIPS, 2017.
W. James and C. Stein. Estimation with quadratic loss. In J. Neyman (ed.), Proc. Fourth Berkeley
Symp. Math. Statist. Prob, volume 1, pp. 361-379, 1961.
A. Juditsky, A. Nemirovski, and C. Tauvel. Solving variational inequalities with stochastic mirror-
prox algorithm. Stoch. Syst., 1(1):17-58, 2011.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. ICLR, 2015.
G. M. Korpelevich. The extragradient method for finding saddle points and other problems.
Ekonomika Mat. Metody, 12:747-756, 1976.
N.	N. Krasovskii. Some Problems in the Theory of Motion Stability. Fizmatgiz, Moscow, 1959.
J. LaSalle. Some extensions of Liapunov’s second method. IRE Trans. Circuit Theory, 7(4):520-
527, 1960.
T. Liang and J. Stokes. Interaction matters: A note on non-asymptotic local convergence of genera-
tive adversarial networks. AISTATS, 2019.
F. Lieder. On the convergence rate of the Halpern-iteration. Optimization Online:2017-11-6336,
2017.
M. Lucic, K. Kurach, M. Michalski, O. Bousquet, and S. Gelly. Are GANs created equal? a large-
scale study. NeurIPS, 2018.
Y. Malitsky. Projected reflected gradient methods for monotone variational inequalities. SIAM J.
Optim., 25(1):502-520, 2015.
Y. Malitsky. Golden ratio algorithms for variational inequalities. Mathematical Programming, 2019.
Y. Malitsky and M. K. Tam. A forward-backward splitting method for monotone inclusions without
cocoercivity. arXiv:1808.04162, 2018.
Y. V. Malitsky and V. V. Semenov. An extragradient algorithm for monotone variational inequalities.
Cybern. Syst. Anal., 50(2):271-277, 2014.
P. Mertikopoulos, B. Lecouat, H. Zenati, C.-S. Foo, V. Chandrasekhar, and G. Piliouras. Optimistic
mirror descent in saddle-point problems: Going the extra(-gradient) mile. ICLR, 2019.
L. Mescheder, S. Nowozin, and A. Geiger. The numerics of GANs. NeurIPS, 2017.
L. Mescheder, A. Geiger, and S. Nowozin. Which training methods for GANs do actually converge?
ICML, 2018.
L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks. ICLR,
2017.
10
Under review as a conference paper at ICLR 2020
G.J. Minty. Monotone (nonlinear) operators in Hilbert space. Duke Math. J., 29(3):341-346,1962.
T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversar-
ial networks. ICLR, 2018.
A. Mokhtari, A. Ozdaglar, and S. Pattathil. A unified analysis of extra-gradient and optimistic
gradient methods for saddle point problems: Proximal point approach. arXiv:1901.08511, 2019.
J.J. Moreau. Proximite et dualite dans Un espace hilbertien. Bulletin de la Societe Mathematique de
France, 93:273-299, 1965.
E. Moulines and F. Bach. Non-asymptotic analysis of stochastic approximation algorithms for ma-
chine learning. NeurIPS, 2011.
V. Nagarajan and J. Z. Kolter. Gradient descent GAN optimization is locally stable. NeurIPS, 2017.
A. Nemirovski and D. Yudin. On Cezari’s convergence of the steepest descent method for ap-
proximating saddle point of convex-concave functions. Doklady Akademii Nauk SSSR, 239(5):
483-486, 1978.
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to
stochastic programming. SIAM J. Optim., 19(4):1574-1609, 2009.
A. Odena. Open questions about generative adversarial networks (online article). https://
distill.pub/2019/gan-open-problems/, 2019. Accessed: 2019-05-10.
A.	Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga,
and A. Lerer. Automatic differentiation in PyTorch. NeurIPS Autodiff Workshop, 2017.
W. Peng, Y. Dai, H. Zhang, and L. Cheng. Training GANs with centripetal acceleration.
arXiv:1902.08949, 2019.
B.	T. Polyak. Introduction to optimization. Optimization Software, 1987.
L. D. Popov. A modification of the Arrow-Hurwicz method for search of saddle points. Mat.
Zametki, 28(5):777-784, 1980.
A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional
generative adversarial networks. ICLR, 2016.
A. Rakhlin and K. Sridharan. Online learning with predictable sequences. COLT, 2013a.
A. Rakhlin and K. Sridharan. Optimization, learning, and games with predictable sequences.
NeurIPS, 2013b.
H. Robbins and D. Siegmund. A convergence theorem for non negative almost supermartingales
and some applications. In Jagdish S. Rustagi (ed.), Optimizing Methods in Statistics, pp. 233-
257. Academic Press, 1971.
R. T. Rockafellar. Monotone operators associated with saddle-functions and minimax problems.
In F. E. Browder (ed.), Nonlinear Functional Analysis, Part 1, volume 18 of Proceedings of
Symposia in Pure Mathematics, pp. 241-250. American Mathematical Society, 1970.
K. Roth, A. Lucchi, S. Nowozin, and T. Hofmann. Stabilizing training of generative adversarial
networks through regularization. NeurIPS, 2017.
E. K. Ryu and S. P. Boyd. Primer on monotone operator methods. Appl. Comput. Math., 15:3-43,
2016.
C. K. S0nderby, J. Caballero, L. Theis, W. Shi, and F. Huszar. Amortised MAP inference for image
super-resolution. ICLR, 2017.
C. Stein. Inadmissibility of the usual estimator for the mean of a multivariate normal distribution. In
J. Neyman (ed.), Proc. Third Berkeley Symp. Math. Statist. Prob., volume 1, pp. 197-206, 1956.
11
Under review as a conference paper at ICLR 2020
V. Syrgkanis, A. Agarwal, H. Luo, and R. E. Schapire. Fast convergence of regularized learning in
games. NeurIPS, 2015.
A. Taylor and F. Bach. Stochastic first-order methods: non-asymptotic and computer-aided analyses
via potential functions. COLT, 2019.
P. Tseng. A modified forward-backward splitting method for maximal monotone mappings. SIAM
J. Control Optim., 38(2):431-446, 2000.
X. Wei, B. Gong, Z. Liu, W. Lu., and L. Wang. Improving the improved training of Wasserstein
GANs: A consistency term and its dual effect. ICLR, 2018.
R. Wittmann. Approximation of fixed points of nonexpansive mappings. Arch. Math., 58(5):486-
491, 1992.
A. Yadav, S. Shah, Z. Xu, D. Jacobs, and T. Goldstein. Stabilizing adversarial nets with prediction
methods. ICLR, 2018.
Y. Yazici, C.-S. Foo, S. Winkler, K.-H. Yap, G. Piliouras, and V. Chandrasekhar. The unusual
effectiveness of averaging in GAN training. ICLR, 2019.
K. Yosida. On the differentiability and the representation of one-parameter semi-group of linear
operators. J. Math. Soc. Japan, 1(1):15-21, 1948.
12
Under review as a conference paper at ICLR 2020
A	Further discussion on the convergence results
Theorems 1, 2, 3, and 4 use related but different notions of convergence. Theorems 1 and 4 are
asymptotic (has no rate) while Theorems 2 and 3 are non-asymptotic (has a rate). Theorems 1
and 3 respectively show almost sure and L2 convergence of the iterates. Theorems 2 and 3 show
convergence of the squared gradient norm for the best and last iterates, respectively. We did not
make these choices. The choices were dictated by what we can prove based on the analysis.
The discrete-time analysis of SimGD-O of Theorem 2 bounds the squared gradient norm of the best
iterate, while the continuous-time analysis bounds the squared gradient norm of the “last iterate”
(at terminal time). The discrepancy comes from the fact that while we have monotonic decrease of
kg(t)k in continuous-time, we have no analogous monotonicity condition on kgkk in discrete-time.
To the best of our knowledge, there is no result establishing a O(1/k) rate on the squared gradient
norm of the last iterate for SimGD-O or the related “extragradient method” Korpelevich (1976).
Theorem 3 is the first result showing a rate close to O(1/k) on the last literate.
For SimGD-O and Corollary 1, the parameter choices are almost optimal. The optimal choices that
minimize the bound of Theorem 2 are α = 0.124897/R and β = 1.94431α; they provide a factor
of 135.771, a very small improvement over the factor 136 of Corollary 1.
For SimGD-A and Theorem 3, there is a discrepancy in the rate between the continuous time anal-
ysis O(1/t2) and the discrete time rate O(1/k2-2p) for p ∈ (1/2, 1), which is slightly slower than
O(1/k). In discretizing the continuous-time calculations to obtain a discrete proof, errors accumu-
late and prevent the rate from being better than O(1/k). This is not an artifact of the proof. Simple
tests on bilinear examples show divergence when p < 1/2.
SSSGD-A and Theorem 4 involves the parameter ε. While the proof requires ε > 0, we believe this
is an artifact of the proof. In particular, We conjecture that Lemma 17 holds with o(s∕τ) rather than
O(s∕τ), and, if so, it is possible to establish convergence with ε = 0.
In Figure 2, it seems that that the choice ε = 0 and p = 2/3 is optimal for SSSGD-A. While we
do not have a theoretical explanation for this, we point out that this is not surprising as p = 2/3 is
known to be optimal in stochastic convex minimization (Moulines & Bach, 2011; Taylor & Bach,
2019).
Theorems 2, 3, and 4 extend to monotone operators (Ryu & Boyd, 2016; Bauschke & Combettes,
2017) without any modification to their proofs. In infinite dimensional setups (which is of interest
in the field of monotone operators) Theorem 4 establishes strong convergence, while many conver-
gence results (including Theorems 2 and 3) establish weak convergence. However, Theorem 1 does
not extend to monotone operators, as the use of the LaSalle-Krasnovskii principle is particular to
convex-concave saddle functions.
B Notation and preliminaries
Write R+ to denote the set of nonnegative real numbers and〈•，•〉to denote inner product, i.e.,
hu, vi = uT v for u, v ∈ Rm+n.
We say A is a point-to-set mapping on Rd if A maps points of Rd to subsets of Rd . For notational
simplicity, we write
hA(x) - A(y), x - yi = {hu - v,x - yi |u ∈ A(x), v ∈ A(y)}.
Using this notation, we define monotonicity of A with
hA(x) - A(y), x - yi ≥ 0 ∀x, y ∈ Rd,
where the inequality requires every member of the set to be nonnegative. We say a monotone
operator A is maximal if there is no other monotone operator B such that the containment
{(x, u) | u ∈ A(x)} ⊂ {(x, u) | u ∈ B(x)}
is proper. If L : Rm × Rn → R is convex-concave, then the subdifferential operator
G(x, u)
∂xL(x, u)
∂u(-L)(x, u)
13
Under review as a conference paper at ICLR 2020
is maximal monotone (Rockafellar, 1970). By Bauschke & Combettes (2017) Proposition 20.36,
G(z) is closed-convex for any z ∈ Rm+n. By Bauschke & Combettes (2017) Proposition 20.38(iii),
maximal monotone operators are upper semicontinuous in the sense that if G is maximal monotone,
then gk ∈ G(zk) for k = 0, 1, . . . and (zk,gk) → (z∞,g∞) imply g∞ ∈ G(z∞). (In other words,
the graph of G is closed.) Define Zer(G) = {z ∈ Rd | 0 ∈ G(z)}, which is the set of saddle-points
or equilibrium points. When G is maximal monotone, Zer(G) is a closed convex set. Write
PZer(G)(z0) = arg min kz - z0k
z∈Zer(G)
for the projection onto Zer(G).
Write C(R+, Rd) for the space of Rd-valued continuous functions on R+. For fk : R+ → Rm+n,
we say fk → f in C(R+ , Rd) if fk → f uniformly on bounded intervals, i.e., for all T < ∞, we
have
lim sup kfk(t) - f (t)k = 0.
k→∞ t∈[0,T]
In other words, we consider the topology of uniform convergence on compact sets.
We rely on the following inequalities, which hold for any a, b ∈ Rm+n any ε > 0.
ha, bi ≤ 2εl∣ak2 + 2Ilbk2	(5)
ka+bk2 ≤ 2kak2 + 2kbk2.	(6)
Both inequalities are called Young’s inequality. (Note, (6) follows from (5) with ε = 1.)
Lemma 1 (Theorem 5.3.33 of Dembo (2019)). Let {Fk}k∈N+ be an increasing sequence of σ-
algebras. Let (mk , Fk) be a martingale such that
E[kmkk2] < ∞
for all k ≥ 0 and
∞
E kmk+1 - mkk2 |Fk < ∞
k=0
then mk converges almost surely to a limit.
Lemma 2 (Robbins & Siegmund (1971)). Let {Fk}k∈N+ be an increasing sequence of σ-algebras.
Let {Vk}k∈N+, {Sk}k∈N+, {Uk}k∈N+, and {βk}k∈N+ be nonnegative Fk-measurable random se-
quences satisfying
E [Vk+1 |Fk] ≤ (1+βk)Vk -Sk+Uk.
If
∞∞
βk < ∞,	Uk < ∞
k=1	k=1
holds almost surely, then
Vk → V∞ ,	Sk → 0
almost surely, where V∞ is a random limit.
Define
≈. , 一 , 、 _.,
G(Z) = Eωg(z;ω) ∈ G(z).
Note that 0 6= G(z?) is possible even if 0 ∈ G(z?) when L is not continuously differentiable.
Lemma 3. Under Assumptions (A0) and (A2), we have
Eωkg(z； ω)k2 ≤ R3kz - z?k2 + R2
for some R3 > 0 and R4 > 0.
14
Under review as a conference paper at ICLR 2020
Proof. Let z? be a saddle point, which exists by Assumption (A0). Let ω and ω0 be independent and
identically distributed. Then
Eωkg(z; ω)k2 ≤ Eωkg(z; ω)k2 + Eω√kg(z*; ω0) - G(z*)∣∣2
=Eω,ω0 kg(z; ω) - g(z*; ω0) + G(Z?)k2
≤ Eω,ω0 2kg(z; ω) - g(z?; ω0)k2 +2∣∣G(z*)k2
≤ 2Rlkz — z?k2 + 2R2 + 2kG(z?)k2
where we use the fact that g(z?; ω0) - G(z?) is a zero-mean random variable, Assumption (A2), and
(6). The stated result holds with R2 = 2R2 and R = 2R2 + 2∣∣G(z*)k2.	□
C Analysis of Theorem 1
For convenience, we restate the update, assumptions, and the theorem:
zk+1 = zk - αkg(zk; ωk)	(SSSGD)
L is convex-concave and has a saddle point	(A0)
∞∞
αk = ∞,	α2k < ∞	(A1)
Eω1,ω2 kg(z1; ω1)	- g(z2; ω2)k2 ≤ R21 kz1 -	z2 k2 + R22	∀ z1, z2	∈ Rm+n,	(A2)
Theorem 1.	Assume (A0), (A1), and (A2). Furthermore, assume L(x, u) is strictly convex in x
for all u. Then SSSGD converges in the sense of zk →a.s. z? where z? is a saddle point of L.
Differential inclusion technique. We use the differential inclusion technique of Duchi & Ruan
(2018), also recently used in Davis et al. (2019). The high-level summary of the technique is very
simple and elegant: (i) show the discrete-time process converges to a continuous-time trajectory
satisfying a differential inclusion, (ii) show any solution of the differential inclusion has a desirable
property, and (iii) translate the conclusion in continuous-time to discrete-time. However, the actual
execution of this technique does require careful and technical considerations.
Proof outline. For step (i), We adapt the LaSane-Krasnovskii principle to show that a solution of
the continuous-time differential inclusion converges to a saddle point. (Lemma 5.) Then we carry
out step (ii) showing the time-shifted interpolated discrete time process converges to a solution of
the differential inclusion. (Lemma 6.) Finally, step (iii), the “Continuous convergence to discrete
convergence”, combines these two pieces to conclude that the discrete time process converges to a
saddle point. The contribution and novelty of our proof is in our steps (i) and (iii).
Preliminary definitions and results. Consider the differential inclusion
Z(t) ∈ -G(z(t))	(7)
with the initial condition z(0) = z0. We say z : [0, ∞) → Rm+n satisfies (7) if there is a Lebesgue
integrable ζ : [0, ∞) → Rm+n such that
z(t)
z0 +	ζ(s) ds,
0
ζ(t) ∈ -G(z(t)), ∀t ≥ 0.
(8)
Write z(t) = φt(z0) and call φt : Rm+n → Rm+n the time evolution operator. In other words, φt
maps the initial condition of the differential inclusion to the point at time t, which is well defined by
the following result.
Lemma 4 (Theorem 5.2.1 of Aubin & Cellina (1984)). If G is maximal monotone, the solution to
(7) exists and is unique. Furthermore, φt : Rm+n → Rm+n is 1-Lipschitz continuous for all t ≥ 0.
15
Under review as a conference paper at ICLR 2020
C.1 Proof of Theorem 1
Lemma 5 and its proof can be considered an adaptation of the LaSalle-Krasnovskii invariance Prin-
ciple (Krasovskii, 1959; LaSalle, 1960) to the setup of differential inclusions. The standard result
applies to differential equations.
Lemma 5 (LaSalle-Krasnovskii). Assume (A0). Assume L(x, u) is strictly convex in x for all u. If
z(∙) satisfies (7) ,then z(t) → z∞ as t → ∞ and z∞ ∈ Zer(G).
Proof. Consider any z? ∈ Zer(G), which exists by Assumption (A0). Since z(t) is absolutely
continuous, so is kz(t) - z?k2, and we have
d2llz(t) - z?k2 = hζ⑴，z⑴一z?i ≤ 0
for almost all t > 0, where Z(∙) is as defined in (8) and the inequality follows from (1), monotonicity
of G. Therefore, lz(t) - z?l2 is a nonincreasing function oft, and
lim lz(t) - z?l = χ
t→∞
for some limit χ ≥ 0. Since z(t) is a bounded sequence, it has at least one cluster point.
Let tk → ∞ such that z(tk) → z∞, i.e., z∞ is a cluster point of z(∙). Then, ∣z∞ — z?12 = χ. Since
φt(∙) (with fixed t) is continuous by Lemma 4, We have
lim φs+tk (z0) = lim φs(φtk(z(0))) = φs(z∞)
k→∞	k→∞
for all S ≥ 0. This means φs (z∞) is also a cluster point of z(∙) and
lφs (z∞) - z? l = χ
for all s ≥ 0. Therefore
0 = ~φS(z∞zc- - z*ll2 ∈ -hG(Os(Zg)), φs(z∞) - z?i	(9)
ds
for almost all s ≥ 0.
Write z∞ = (x∞, u∞) and let z? = (x?, u?) ∈ Zer(G). Write (φsx(z?), φsu(z?)) = (φs(z?)). If
φsx (z?) 6= x?
hG(φs (z∞ )), φs (z∞ ) - z?i > 0
by strict convexity, and, in light of (9), we conclude φsx (z?) = x? for almost all s ≥ 0. Then for
almost all s ≥ 0, we have
0 ∈ hG(φs (z∞ )), φs (z∞ ) - z?i
=h∂u(-L)(x?, ΦU(z∞)), ΦU(z∞) - u?i
≥ -L(x?,φsu(z∞)) +L(x?, u?)
≥ 0,
where the first inequality follows from concavity of L(x, u) in u and the second inequality follows
from the fact that u? is a maximizer when x? is fixed. Therefore, we have equality throughout, and
L(x?, ΦU(z∞)) = L(x?,u?), i.e., φU(z∞) also maximizes L(x?, ∙).
Remember that φs(z∞) is a continuous function of s for all s ≥ 0. Therefore, that φsx(z∞) = x?
and that φU(z∞) maximizes L(x?, ∙) for almost all S ≥ 0 imply that the conditions hold for S = 0.
In other words, x∞ = x? and u∞ maximizes L(x?, ∙), and therefore z∞ ∈ ZerG.
Finally, since z∞ is a solution, lz(t) -z∞ l converges to a limit as t → ∞. Since lz(tk) -z∞ l → 0,
We conclude that ∣z(t) 一 z∞∣ → 0 as t → ∞.	□
The following lemma is the crux of the differential inclusion technique. It makes precise in what
sense the discrete-time process converges to a solution of the continuous-time differential inclusion.
16
Under review as a conference paper at ICLR 2020
Lemma 6 (Theorem 3.7 of Duchi & Ruan (2018)). Consider the update
zk+1 = zk - αk (ζk + ξk),	ζk ∈ G(zk).
Define tk = Pik=1 αi and
t - tk
ZinterP(t) = Zk + ~	(z (zk+1 - Zk ),	t ∈ [tk ,tk +1).
tk+1 - tk
Define the time-shifted process
Zτ
ZinterP
(∙) = ZinterP(T +，)•
Let the following conditions hold:
(i)	The iterates are bounded, i.e., supk kZk k < ∞ and supk kζk k < ∞.
(ii)	The stepsizes αk satisfy Assumption (A1).
(iii)	The weighted noise sequence converges:	k∞=0 αkξk = v for some v ∈ Rd.
(iv)	For any increasing sequence nk such that Znk → Z∞, we have
lim dist
n→∞
1m
一X Znk , G(z∞)
mk
k=1
0.
Thenfor any Sequence {τk }∞=ι ⊂ R+, the Sequence of functions {z⅛terp(∙)} is relatively compact
in C(R+, Rd). If Tk → ∞, all cluster points of {z⅛terp(∙)} satisfy the differential inclusion (8).
We verify the conditions of Lemma 6 and make the argument that the noisy discrete time process is
close to the noiseless continuous time process and the two processes converge to the same limit.
Verifying conditions of Lemma 6.
Condition (i).	Let	Z?	∈ Zer(G).	Write	Fk	for the σ-field generated by	ω0, .	. .	, ωk-1.	Write
G(Z) = Eg(z; ω) ∈ G(z). Then
kZk+1 - Z? k2 = kZk - Z? k2 - 2αk hZk - Z?, g(Zk; ωk)i + αk2 kg(Zk; ωk)k2
E [kZk+1 - Z?k2 | Fk] ≤ IlZk - Z?k2 - 2αk hZk - z? , G(Zk)i + αk (R3kZk - z? k 2 + R2)
= (1 + αk R3)kZk - Z?k2 - 2αk hZk - Z?, G(Zk )i + αk R4,
where we used Assumption (A2) and Lemma 3. Since Pk∞=0 αk2 < ∞ by Assumption (A1), this
inequality and Lemma 2 tells us
IZk - Z? I2 → limit
for some limit, which implies Zk is a bounded sequence. Since Zk is bounded, so is G(Zk) since
IIG(Zk)k2 ≤ EωIlg(Zk； ω)k2 ≤ R3 sup ∣∣Zk — Z?k2 + R2
k
by Lemma 3.
Condition (ii).	This condition is assumed.
Condition (iii).	Define
ξk = g(Zk; ωk) — G(Zk)
and
k
mk =	αi ξi .
i=0
17
Under review as a conference paper at ICLR 2020
Then (mk , Fk ) is a martingale and
∞∞
XE
kmk+1 - mkk2 | Fk =XakE [kξfck2 |Fk]
k=0	k=0
∞
≤ X αk E [kg(Zk ； ωk)k2 |Fk ]
k=0
∞
≤ Xαk (R3kzk- z?k2 + R2)
k=0
≤
∞
Xα2k sup2R23kzkk +2R32kz?k2 +
k=0	k
<∞
almost surely, where the first inequality is the second moment upper bounding the variance, the
second inequality is Lemma 3, and the third inequality is (6) and condition (i). Finally, we have (iii)
by Lemma 1.
Condition (iv). As discussed in Section B, G is maximal monotone, which implies G is upper
semicontinuous, i.e., (znk , gnk ) → (z∞, g∞) implies g∞ ∈ G(z∞), and G(z∞) is a closed convex
set. Therefore, dist(ζnk , G(z∞)) → 0 as otherwise we can find a further subsequence such that
converging to ζ∞ such that dist(ζ∞, G(z∞)) > 0. (Here we use the fact that ζk is bounded due to
condition (i)). Since G(z∞) is a convex set,
1m	1m
dist(Znk ,G(Z∞ )) → 0 ⇒ — ɪ2 dist(Znk , G(z∞)) → 0 ⇒ dist I ~	Znk , G(z∞) I → 0∙
m k=1	m k=1
In the main proof, We show that cluster points of Zinterp(∙) are solutions. We need the following
lemma to conclude that these cluster points are also cluster points of the original discrete time
process Zk .
Lemma 7. Under the conditions of Lemma 6, Zinterp(∙) and Zk share the same cluster points.
Proof. If z∞ is a cluster point of Zk, then it is a cluster point of ZinterP(∙) by definition. Assume z∞
is a cluster point of Zinterp(∙), i.e., assume there is a sequence Tj → ∞ such that ZinterP(Tj) → z∞.
Define kj → ∞ with
tkj ≤ Tj < tkj +1 .
Then
kZinterp (Tj ) - Zkj k ≤ αk kZkj +1 - Zkj k
≤ αk(kZkk + kξkk)
→0
where we use the assumption (i) which states that kZkk is bounded and assumption (iii) which states
that akξk → 0. We conclude Zkj → z∞.	□
Continuous convergence to discrete convergence. Let kj → ∞ be a subsequence such that
Zkj → Z∞. Let kj0 → ∞ be a further subsequence such that
0lim Zitnktj0erp(T) = φT (Z∞)
kj0 →∞
for all T ≥ 0, which exists by Lemma 6. (The time-shifted interpolated process converges to a
solution of the differential inclusion.) By Lemma 5,
lim φTZ∞ → φ∞Z∞
T→∞
where φtZ∞ → φ∞Z∞ as t → ∞ and φ∞Z∞ is a saddle point. (The solution to the differential
inclusion converges to a solution.)
18
Under review as a conference paper at ICLR 2020
These facts together imply that for any ε > 0, there exists kj0 and τj large enough that
tk0
kzinjerP(Tj ) - φT (z∞)k < ε/2
and
11 φτj z∞ - φ∞z∞k < ε/2.
Together, these imply
kzinterP (tkj0 + τj ) - φ∞z∞ k < ε.
since ZSIterp(∙) = ZinterP(T + ∙). Therefore, φ∞z∞ is a cluster point of Zinterp(∙), and, by Lemma 7,
φ∞z∞ is a cluster point of zk.
Since kZk -φ∞Z∞k converges to a limit and converges to 0 on this further subsequence, we conclude
IlZk - φ∞z∞ k → 0 almost surely.	□
D	Analysis of Theorem 2
For convenience, we restate the update, assumptions, and the theorem:
Zk+1 = Zk - αG(Zk) - β (G(Zk) - G(Zk-1))	(SimGD-O)
L is convex-concave and has a saddle point	(A0)
L is differentiable and VL is R-LiPSchitz continuous	(A3)
Theorem 2.
sense of
Assume (A0) and (A3). If 0 < α < 2β(1 - 2βR), then SimGD-O converges in the
min IG(Zk)I2 ≤
i=0,...,k
2 + 2β 2R2
α(2β — α — 4β2 R)k
IZ0 + βG(Z0) - Z?I2.
Furthermore, Zk → Z?, where Z? is a saddle point of L.
D.1 Proof of Theorem 2
Throughout this section, write gk = G(Zk) for k ≥ -1. Since We can define G = αG and β = β∕ɑ
and write the iteration as
≈. ,	、	~ , ,τ. ,	、	≈.,	一
Zk+1 = Zk - G(Zk ) - β(G(Zk ) - G(Zk-1)),
we assume α = 1 without loss of generality. Then
IZk+1	+ βgk	- Z?I2 = IZk + βgk-1 - Z? I2 - 2hgk, Zk - Z?i -	hgk, 2βgk-1	- gki
≤ IZk + βgk-1 - Z?I2 - hgk, 2βgk-1 - gki,
where the inequality follows from (1), monotonicity of G, and
-hgk,	2β gk-1 - gki	= 4β2 hgk - gk-1, Zk - Zk+1i - (2β - 1)IZk+1 -	Zk I2 - β2 (1	+ 2β)Igk - gk-1 I2
≤ 4β2hgk - gk-1, Zk - Zk+1i - (2β - 1)IZk+1 -	ZkI2.
We can bound
4β2 hgk - gk-1, Zk - Zk+1i ≤ ∕-∣ Igk - gk-1k2 + 2β2 RllZk+ 1 - Zk k 2
R
≤ 2β2RIZk - Zk-1I2 + 2β2RIZk+1 - ZkI2,
where the first inequality follows from (5), Young’s inequality, with ε = R and the second inequality
follows from Assumption (A3), R-Lipschitz continuity of G. Putting these together we get
IZk+1 + βgk - Z? I2 ≤ IZk + βgk-1 - Z? I2
+ 2e2RkZk - Zk-Ik2 - (2β - 1 - 2β2R) kZk+1 - Zk ∣∣2.	(IO)
Since β > 1/2 and R < (2β 一 1)∕(4β2) is assumed for Theorem 2, we have
2β2R < (2β - 1 一 2β2R).
19
Under review as a conference paper at ICLR 2020
By summing (10), we have
kk
(2β - 1 - 2β 2R) X Ilzi+1 - Zill2 - 2β2 R X Ilzi- zi-1k2 ≤ kz0 + βg-1 - z?k2
i=0	i=0
k
(2β - 1 - 4β2L) ^X ∣∣zi+1 - zi∣∣2 ≤ IIzO + βg-1 - z?k2,	(II)
i=0
where we use z0 = z-1.
Next,
kgk k2 = kzk+1 - zk + β(gk - gk-1)k2
≤ 2kzk+1 - zk k2 + 2β2 kgk - gk-1 k2
≤ 2kzk+1 - zkk2 + 2β2R2kzk - zk-1k2,
where we use (6). Using (11), we get
X (2kzi+1 - ζik2 + 2β2R2 kzi - zi-1∣∣2) ≤ 2β -1 - 4β2R kzO + βg-1 - z?k2.
Therefore, 2kzk+1 - zkk2 + 2β2R2 kzk - zk-1k2 → 0 and kgkk2 → 0. Moreover, we have
2	2+2β2R2	2
i=m,in,k kgik ≤ (2β - ι - 4β2R)k kz0 + βg-1 - z?k .
By scaling G by α, we get the first stated result.
By summing (10), we have
kzk + βgk-1 - z?k2 ≤ kz0 + βg-1 - z? k2,
and using the triangle inequality we get
kzk - z?k ≤ kz0 + βg-1 - z? k + β kgk-1 k → kz0 + βg-1 - z?k
as k → ∞. (Remember gk → 0.) So zk is a bounded sequence, and let z∞ be the limit of a
convergent subsequence znk. Since G is a continuous mapping with gnk = G(znk), znk → z∞, and
gnk → 0, we have G(z∞ ) = 0.
Finally, we show that the entire sequence zk converges to z∞. Reorganizing (10), we get
kzk+1 + βgk - z?k2 + 2β2Rkzk+1 - zkk2 ≤ kzk + βgk-1 - z?k2 + 2β2Rkzk - zk-1k2
-(2β - 1 - 4β2R) ∣∣zk + 1 - zk ∣∣2.
'-------{z------}
>0
So kzk+1 + βgk - z?k2 + 2β2Rkzk+1 - zk k2 is a nonincreasing sequence, and the following limit
exists
lim kzk + βgk-1 - z?k2 + 2β2Rkzk - zk-1 k2 = kz∞ - z?k2 .
k→∞
Since z? can be any equilibrium point, we let z? = z∞. This proves kzk - z∞k2 → 0, i.e.,
zk → z∞.	□
E	Analysis of Theorem 3
E.1 Preliminary lemmas
We quickly state a few identities and inequalities we later use. As the verification of these results
are elementary, we only provide a short summary of their proofs.
Lemma 8. Forp ∈ (0, 1) and k ≥ 1,
P	p(1 — P)	(k +1)p — kp P
k	2k2	<	kp	‹ k.
20
Under review as a conference paper at ICLR 2020
The proof follows from a basic application of the inequality
1 + PX — P(' 'I2 ≤ (1 + x)p ≤ 1 + PX
for x ∈ [0, 1] andp ∈ (0, 1).
Lemma 9. ForP ∈ (0, 1) and k ≥ 1,
p X (k +1)p — kp
k + 1 <	kp	.
The proof follows from integrating the decreasing function P/X1-p from k to k + 1.
Lemma 10. ForP ∈ (0, 1) and k ≥ 1,
0 ≤ P _ (k + 1)p — kp ≤ P(1 一 P)
-k(k + 1) kP(k + 1)	-	2k3.
The proof follows from Lemma 8.
Lemma 11. Given any V0, V1, . . . ∈ R, we have
X (— (Vj- Vj-1)+ jVj-J= BVk
j=1
The proof follows from basic calculations. This result can be thought of as the discrete analog of
t s2	t2V
/ 工V(s) + SV(S) ds =.
Lemma 12. Let z0, z1, . . . ∈ Rm+n be an arbitrary sequence. Then for any k = 0, 1, . . . ,
2 kzk+1 - z0k2 - 2 Ilzk - z0k2 = Ik+1 - zk, g(zk+1 + zk ) - zθ) ∙
The proof follows from basic calculations. This result can be thought of as the discrete analog of
dt2kz⑴-zok2 = hz⑴，z⑴-zoi.
E.2 Convergent sequence lemmas
In the proofs of Theorems 3 and 4, we establish certain descent inequalities. The following lemmas
state that these inequalities imply boundedness or convergence.
Lemma 13. Let {Vk}k∈N+ and {Uk}k∈N+ be nonnegative (deterministic) sequences satisfying
Vk+1 ≤ (1 - k-ε + f(k)) Vk + k- PVk + Uk
where Ci > 0, C? > 0, f (k) = o(1∕k1-ε) with ε ∈ [0,1), and
∞
Uk < ∞.
k=1
Then lim supk→∞ Vk ≤ C22/C12.
Proof. For any δ ∈ (0, C1), there is a large enough K ≥ 0 such that for all k ≥ K,
Ci - δ∕2
k1-ε
Define
kC⅛ - f (k) ≥
k
21
Under review as a conference paper at ICLR 2020
for k ≥ 0. Then
Vk + 1 ≤ (1- C1k-δ/2) Vk + © -δ)k1-ε max (rVk,Vk ) + Uk
Vk + 1-ν ≤ (1 - ⅛ε/2) (Vk-V) - 2ki；：- δ)2
+ (Ci -δ)k1-ε max (rVk - 1, Vk - 1) + Uk
Note that max{√X 一 1, x 一 1} ≤ max{0, x _ 1} for all x ≥ 0. So
Vk+1 - ν ≤	1
≤
(Vk - ν ) +
1
—
max {0, Vk - ν } +
(Vk -V )+(Ci -δ)ki-ε max S'》一 1} + Uk
k1ε
max {0, Vk - ν} + Uk
C1 - δ
k1ε
max {0, Vk - ν} + Uk
2kδ-ε ) max {0,Vk — v} + Uk
for large enough k. Since
0 ≤ (1 — o,fg ) max {0,Vk — v} + Uk
2k1-ε
for large enough k, we have
max {0, Vk+ι - v} ≤ (1 — 2kδ-ε ) max {0, Vk — v} + Uk
With a standard recursion argument (e.g. Lemma 3 of (Polyak, 1987)) we conclude
max {0, Vk — v} → 0. Since this holds for any δ > 0, we conclude lim supk→∞ Vk ≤ C2/C22. □
Lemma 14. Let ε ∈ (0, 1). Let {Vk}k∈N+ and {Uk}k∈N+ be nonnegative (deterministic) sequences
satisfying
Vk+1 ≤ (1 - ki-ε + f (k)) Vk + g(k)PVk + Uk
where C > 0, f(k) = o(1∕k1-ε), g(k) = O(1∕k), and
∞
Uk < ∞.
k=1
Then Vk → 0.
Proof. For any δ > 0, there is a large enough K ≥ 0 such that
Vk+1 ≤
1 - C-δ)Vk + 指 PVk +Uk
k1-ε	k1-ε
for all k ≥ K. By Lemma 13, we conclude lim supk→∞ Vk ≤ δ2 ∕(C - δ)2 . Since this holds for all
δ > 0, we conclude Vk → 0.
E.3 Proof of Theorem 3
For convenience, we restate the update, assumptions, and the theorem:
zk+i = Zk - /_- PP G(zk) + (1 -P))Y (zo - Zk)	(SimGD-A)
(k+1)p	k+1
L is convex-concave and has a saddle point	(A0)
L is differentiable and VL is R-LiPsChitz continuous	(A3)
22
Under review as a conference paper at ICLR 2020
Theorem 3. Assume (A0) and (A3). Ifp ∈ (1/2, 1) and γ ≥ 2, then SimGD-A converges in the
sense of
kG(zk)k2 ≤O(k⅛
Proof outline. Lemma 15 shows the iterates zk are bounded. Lemma 16 shows that kzk+1 -2zk +
zk-1k2, the analog of k Z k2 ,is small. The second-order derivative Z does not arise in the ContinUoUs-
time analysis of Section 4.1. In the discrete-time setup, kzk+1 - 2zk + zk-1k2 does arise, but we
Use Lemma 16 to show that its contribUtion is small. The main proof follows by mimicking the
continUoUs-time analysis by boUnding the higher-order terms.
ThroUghoUt this section, write gk = G(Zk) fork ≥ -1.
Lemma 15. For SimGD-A,
kZk - Z? k2 ≤ C
for all k ≥ 0 for some C > 0. (This result depends on assumption p > 1/2.)
Proof.
IlZk+1 - z*∣∣2	= IlZk	- z*∣∣2	- (k +	1)P	hgk, Zk - z? i	+-Ik + J，hz0	- Zk, Zk	- z?i
,	i -P	, Y(I -p) f 、2
+ EFg	fT(Z0-Zk)
≤
1	2Y(I- P)∖ II	∣∣2 , 2Y(I- P) /
1 - k + 1 JkZk-Z*k + k + 1	hZ0 - Z?,Zk -
l 2(I- P)2 II ∣∣2 l 2Y2 (I- P)2 II	∣∣2
+ (k + 1)2p kgk k +	(k + 1)2 kZ0-Zkk
≤
A 2Y(I- P) , 4Y2(1 - p)2∖∣∣	∣∣2 , 2Y(I- P)Il mi II
(1 - k + 1 + (k + 1)2 ) kZk - Z?k + k + 1 kZ0 - Z?kkZk - Z?k
+ 2k⅛"ρ R2kZk - Z0k2 +
(k+1)
4Y2 (1 - P)2
(k+1)2
IZ0 - Z? I2
2Y(1 - P) + 4Y2(1 - P)2 + R24(1 - P)2 A
k + 1	+	(k +1)2	+ 1 (k + 1)2P)
IZk - Z? I2
l 2Y(1 - P) II	IIII	II l 4Y2 (1 - P)2 II	∣∣2
+	k +1- kZ0 - Z?kkZk - Z?k +	(k + 1)2 kZ0 - Z?k
where the first ineqUality follows from (1), the monotonicity ineqUality, and (6) and the second
inequality follows from Assumption A3. We conclude the statement with Lemma 13.	□
Lemma 16. For SimGD-A,
IZk+1 - 2Zk + Zk-1I2
≤ 4(1 - P)2 (Y2 + k2p) kZk - ZkTk2 +4(I - P)2 (kP2+2p + Y4) kZ0 - Zk k2
23
Under review as a conference paper at ICLR 2020
Proof.
kzk+1 - 2zk + zk-1 k2
2
≤2
1-p 1-p
(k + 1)p gk	kp~ gk-1 -
1-p 1-p
(k + 1)p gk - ~pkΓgk-1
(I - P)Y
k + 1
2
+2
(z0 - zk ) +
匚pγ (z0-zk-i)
k
Y^(Z0-Zk) - γ⅛0(Z0-Zk-1)
k+1	k
2
≤ 4(1k2pp) kgk - gk-1k2 +4 ( (k+ 1p)p - 1-T) kgkk2
l 4Y2 (1 - P)2 II	∣∣2 一，Y(I- P)	Y(I- P)、2 II	∣∣2
+-----k-----kzk - ZkTk +4( k + 1----------k-) kz0 - Zkk
≤ 4(I -P (Y2 + 3) kzk - ZkTk2 +4(I -P (P+2p + γ4) kz0 - Zkk2
where the first and second inequalities follow from (6) and the third inequality follows from As-
sumptions (A3) and Lemma 8.	□
24
Under review as a conference paper at ICLR 2020
Main proof. In Section 4.1, we showed
d 2 kz(t)k2 ≤ - ； kz(t)k2 + -P2 hz⑴-z0, Z i
dt 2	t	t
in continuous time. We mimic analogous calculations in the discrete-time setup:
2IIzk+ι - zk∣∣2 - 2IIzk - zk-ι∣∣2
= 2 2(zk + 1 - zk-1), zk+1 - 2zk + zk-1
—
—
1 — Pj	(	∕1 ʌ (k + 1)p — kk1	γ γ(1 — p) H	∣∣2
h h	hzk	- zk-1,gk - gk-1i + (I - P)	I fI	1、	hzk	- zk-1,gk〉-----1-----Ilzk	- zk-1 Il
kp	kp(k +	1)p	k
γ(1 — P)	1	2
k(k + 1) hzk - zk-1,z0 - zki + 2 k zk+1 - 2zk + zk-1∣∣
(k + 1)p — kk	γ(1 — p)	2
≤ (1 - p) kp(k + 1)p hzk - zk-1,gki-------k- kzk - zk-1k
—
γ(1 — p)	1	2
k(k + 1) hzk - zk-1, z0 - zki + 2 kzk+1 - 2zk + zk-1 Il
Y Y(I - P) , (k + 1)p - k _ Y(I - P) , Y(I - P) (k + 1)p - k ∖ II _	∣∣2
V-k- +	kp	2k(k + 1) + -2	kp(k + 1) Jkzk - zk-11
(k + 1)p - kp,	C	x 1ll	C	ll2
--------Tz	hzk	-	zk-1, zk + 1 - 2zk	+	zk-1i	+ χ∣∣zk+1	-	2zk	+	zk-1∣∣
kp----------------------------------------------------------------2
,	、(	1	(k + 1)p - kp∖ /	1,	、\
-	Y(I-p)	-	kp(k + 1) ) ∖zk - zk-1,z0 - 2(zk + zk1)/
/Y(I - P) p _ P(I - P) _ Y(I - P)	YP(I - P) ∖ ll _ ll2
≤ - V —k — + k 质	2k(k + 1) + 2(k + 1)2 ) Hzk - zk-1h
—
(k + 1)p - kp∕	C	χ 1ll	C	„2
------TT	hzk	-	zk-1,	zk + 1 -	2zk	+ zk-1i + χ∣∣zk+1	- 2zk	+	zk-1∣∣
kp-----------------------------------------------------------------2
-	Y(I - P) (k(k 1+1) - "K;-： ) (zk - zk-1,z0 - 1(zk + zk1))
k(k + 1)	k (k + 1)	2
≤ _ Y Y(1 — P) + p _ p(1 — P) _ Y(1 — P) + YP(1 — P) ʌ
≤ - V -k —+ k 质 2k(k + 1)+ 2(k + 1)2 )
Izk - zk-1I2
2k2 Ilzk - zk-1 Il2 + 2 llzk+1 - 2zk + zk-1∣∣2
+ 2 llzk+1 - 2zk + zk-1 Il2
-	Y(I-p) (⅛(⅛) - (*(2+ 1)k ) (zk - zk-1,z0 - 1(zk + zk1))
—
YY(I -P)
P(I -P)
2k2
Y(I -P)
2k(k + 1)
+ YP(1 — P)
+ 2(k + 1)2
Izk - zk-1I2
,p
+ k -
—
+
—
-2^— ∣∣zk + 1 - 2zk + zk-1∣∣2
Y(I-P)2 /	1r , ʌ
k(k + 1) ∖zk - zk-1,z0 - 2(zk + zk1)
-Y(I- p) (k(⅛)
(k + 1)p - kp
kp(k + 1)
—
—
1-P	1
⅛(⅛;ιyJ ∖zk - zk-1,z0 - 2(zk + zk1)
=cι(k,p)
where the first inequality follows from (1), the monotonicity inequality, the second inequality fol-
lows from Lemma 8 and (6), and the third inequality follows from Lemma 8 and (5), Young's
inequality, with ε = k.
25
Under review as a conference paper at ICLR 2020
By Lemma 10, ∖Cι(k,p)∖ ≤ p1-p. Using (5), Young,s inequality, with ε = 1/k and (6) we get
-Y(I- p) (k(⅛)
(k + 1)p- kp	1 - p∖∕	1, ι 、
-kP(k + 1) - k(k+I)) Vfc - zfc-1,z0 - 2(Zk + ZkI)
≤ γp⅛p^B-Jk2 + γp¾≠kzo - 1(Zk + Zk-i)k2
≤ γp¾≠3-zιk2 + γp¾≠ 0∣zo - Zkk2 + kzo - zιk2).
4k	ok
Putting these together we get
1 ll	ll2	1 ll	ll2	1 ll	ll2 Y(1 - p)2 /	1 /	、
2 kzk+1 - Nk k - 2 IlZk - Zk-IIl + k +1 kzk - Zk-IIl k(k + 1) ∖ zk - zk-1, }(Zk + ZkI) - z0
-((Y	- 1)(1 - P)	+	γp(1 - P)	- p(1 - P)	- γ(1 - P)	-	JP_	_ YP(I - p)2 ʌ
-V k	+	2(k +1)2	2k2	2k(k + 1)	-	2k2	4k2 一)
+	y-P llZk + 1 - 2Zk + Zk-Ik2 + YP(a Pi P) (Hz0 - Zk12 + IIz0 - Zk-Ik2)
2	8k4
||Zk - Zk-Ik2
With Lemma 15 and Lemma 16, we get
1 ll	ll2	1 ll	ll2	1 ll	ll2 γ(1 - p)2 /	1 /	、
2 llZk+1 - Zk k - 2 llZk - Zk-Ik + k +1 llZk - Zk-Ik -成 + 1) \纵-Zk-1, 2(Zk + ZkI) - z0
-2γ2(1+k^2(I-P)2)kZk - Zk-ιk2
(Y-I)(I-P)
k
2(1+P)(I-P)2R I YP(I-P) _ P(I-P) _ Y(I-P) _ P _ YP(I-P)2
k2P	+ 2(k+1)2	2k2	2k(k+1)-肃	4k2
+(2(1+ p)(1 -p)2 (黑 + Y) + γp¾≠)。2
∖	∖k2十2p	k4 J	8k4	J
≤
(一 (Y 1T	P) + O (卷))- ZkTk2 + O
'---------V------------}
= C3(k,Y,P,R)
1
k2+2p
Note that there is a K ∈ N such that C3(k, Y,p, R) ≤ 0 for all k ≥ K (with Y, p, and R fixed).
In Section 4.1, we multiplied the established inequality by t2 and integrating both sides to get
ɪ 眨(力)『≤ 2llZ(t) - z0ll2.
We mimic analogous calculations in the discrete-time setup. Multiply both sides with k(k + 1) and
sum both sides from k = 1 to k = k, and apply Lemma 11 and Lemma 12 to get
HUkzk+1 - Zkk2 ≤ Y(I-P)2 kZk - Z0k2 + C4 + O (VK)
2	2	k p
where C4 < ∞ since C3(k, Y,p, R) > 0 for only finitely many k. Reorganizing we get
k(k + 1)(1 - p)2
2(k + 1)2P
≤ Y(1 - P)2
_	2
Il ∣∣2 . k(1 - P)2Y2 U	∣∣2 k(1 - P)2Y/	∖
kgk1	+	2(k + 1)	kz0	-	zkk----(k +	1)p	hgk,	z0	-	Zki
kzk-z0k2 + C4 + O (码二
Reorganizing yet again we get
k(k + 1)(1 - P)2 II ∣∣2 k(1 - P)2Y∕	∖
2(k + 1)2p kgk k - (k +1)P ®, Z0 - Zki
≤ 也铲	kZk - Z0k2 + C4 + O (-rɪɪ
2	k2p 1
≤ C4 + O
k2p-1
26
Under review as a conference paper at ICLR 2020
where we use the assumption that γ ≥ 2. Reorganizing again, we get
kgk k2 ≤ (k + 1)1-P hgk,z0 - zki + (1- p)2k(k4+1)i-2p + O (J
2γ	4C4	1
≤ (k + 1)1-P hgk,z0 - z?i + (1- p)2(k +1)2-2P + O ⑴
1	2	2γ2	2	4C4	1
≤ 2 kgkk +(k + 1)2-2p kz0-z*k +(1- p)2(k + 1)2-2p + O (J
for k ≥ 1, where the second inequality follows from (1), the monotonicity inequality, and the third
inequality follows from (5), Young,s inquality, with ε = γ∕(k + 1)1-p. Finally, We have
kgkk2 ≤
C
k2-2p
+O
with C = 4γ2 +8C4∕(I - Py.	□
F Analysis of Theorem 4
For convenience, we restate the update, assumptions, and the theorem:
zk+1 = Zk - (k + 1)p g(zk; ωk) + (k： lp)-ε (ZO - Zk)
(SSSGD-A)
L is convex-concave and has a saddle point	(A0)
Eω1,ω2 kg(Z1; ω1) - g(Z2; ω2)k2 ≤ R12 kZ1 - Z2 k2 + R22 ∀ Z1, Z2 ∈ Rm+n	(A2)
Theorem 4. Assume (A0) and (A2). If P ∈ (1∕2, 1), ε ∈ (0, 1∕2), and γ > 0, then SSSGD-A
converges in the sense of Zk →L Z?, where Z? is a saddle point.
To clarify, we do not assume L is differentiable for Theorem 4.
Proof outline. The key insight is to define ζk to be something like a “fixed point” of the k-th
iteration of SSSGD-A and then to show Zk shrinks towards to ζk in the following sense
kZk+1 - ζk+1k2 ≤ (1 - something)kZk - ζkk2 + (something small).
Lemma 17 states that ζk slowly (stably) converges to a solution. Using the fact that Zk shrinks
towards ζk and the fact that ζk is a slowly moving target converging to a solution, we conclude Zk
converges to a solution.
Preliminary definition and result. More precisely, we define ζk to satisfy
Zk ∈Zk- (⅛yG(Zk)+ (k1+ ιp1-(ZO- Zk).
(However, ζk is not actually a fixed point, since SSSGD-A has noise and since G is a multi-valued
operator.) We equivalently write
Zk+ι=(l +——— G)T(Z0).
Lemma 17 (Proposition 23.31 and Theorem 23.44 of Bauschke & Combettes (2017)). Let G be a
maximal monotone operator such that Zer(G) = 0. Then (I + tG)-1(zο) → PZer(G)(Z0) and
k(I +(τ + S)G)T(ZO) - (I + TG)T(Z0)k ≤ O (S)
for any s ≥ 0 as τ → ∞.
27
Under review as a conference paper at ICLR 2020
F.1 Proof of Theorem 4
Main proof. Since 1 - p - ε > 0, Lemma 17 gives us
ζk → PZer(G) (z0).
Then we have
E hkzk+1 - ζk+1 k2 Fki
zk - ζk -
kzk - ζk k -
1-p
(k + 1)p
1-p
(1 - p)γ	2
g(Zk; ωk) + (k + ι)1-ε (ZO - Zk ) + Zk - ζk+1 || | Fk
(k+1)p
(1 - p)γ
G(Zk ) + (k + ι)1-ε (Zk - ZO), Zk - Zk
E
+ hzk - ζk, ζk - ζk+1i
+E
1 - p	(1 - p)γ
(k + 1)p g(Zk; ωk ) - (k + 1)1-ε (z0 - Zk ) - Zk + Zk+1
2 Fk
≤ (1 - ∖ J lPlYε ) kZk - Zk k2 + llZk - ZkkkZk - ζk + 1k
(k+1)
+ E O ( (k +1)2p ) kg(Zk; ωk )k2|Fk_ + O ( (k +1)2(1-ε) ) kZ0 - Zk k2 + O ( (k + i)2 )
≤ (l - T二p)γε ) kZk - Zk k2 + O(1∕k) kZk - Zkk
(k+1)
+ O ( (k +11)2P ) (R3kZ0 - Zk k2 + R2) + O ( (k +I12(1-ε) ) kZ0 - Zk k2 + O ( (⅛⅛) ,
where the first inequality follows from (1), the monotonicity inequality, Cauchy-Schwartz inequality,
and (6), Now we take the full expectation to get
E hkZk+1 - Zk+1 k i
≤ (1 -O ( (k^⅛) + O ( (≡F) + O ( (≡⅛-) )) E hkZk - Zk k2i
+ O(1/k)E hkZk - Zkk2i1/2
+ O ( (k +1)2p ) (kZ0 - Z?k2 + 1) + O ( (k +1)2(1-ε) ) kZ0 - Z?k2 + O ( (k +1)2 ),
where we used E[kZk - Zkk]2 ≤ E[kZk -Zkk2]. Applying Lemma 14, wegetE kZk - Zkk2 → 0.
Since Zk → PZer(G)(Zo), We conclude Zk → PZer(G)(zo).	□
G	Experiment details
In this section, We prodvide further details of the experiments of Section 5. Our Optimistic Adam is
a variation of the Optimistic Adam of (Daskalakis et al., 2018), Which uses β = 1 While We alloW
for a general optimism rate β > 0. For Anchored Adam, We do not diminish the strength of the
anchor proportional to 1 /k1-ε since Adam does not diminish the learning rate. Rather, we maintain
a constant anchor strength Y but refresh the anchor point every T iterations. The notation V2 in
algorithm tables denote the element-wise square operation.
28
Under review as a conference paper at ICLR 2020
Generator
latent space 100 (Gaussian noise)
dense 128 lReLU
dense 256 batchnorm lReLU
dense 512 batchnorm lReLU
dense 1024 batchnorm lReLU
dense 1024 tanh
Discriminator
Resize the input image 28 X 28 to 32 X 32
dense 512 lReLU
dense 256 lReLU
dense 1
Table 1:	Generator and discriminator architectures for the MNIST experiment
Optimistic Adam
Parameters: learning rate η, exponential decay rates for moment estimates β1, β2 ∈ [0, 1), optimism
rate ρ > 0, and initial parameters z0
Repeat k = 0, 1, 2 . . . , K (iteration):
Compute stochastic gradient Vz,k = G(Zk)
Update biased estimate of first moment: mk = β1mk-1 + (1 — βι )Vzk
Update biased estimate of second moment: vk = β2vk-1 + (1 - β2)Vz2,k
Scale the step-size: ^ = ηp1 - β2/(1 一 βk)
Perform optimistic gradient step: Zk = Zk-i —m(1 + P) -m=+^ + ηk-ιp√mk-+e
Return zK
Anchored Adam
Parameters: learning rate η, exponential decay rates for moment estimates β1, β2 ∈ [0, 1), anchor
rate γ > 0, anchor update period T , and initial parameters Z0
Repeat k = 0, 1, 2 . . . , K (iteration):
set anchor ak = Zk if mod(k, T) = 0 else ak = ak-1
Compute stochastic gradient Vz,k = G(Zk)
Update biased estimate of first moment: mk = β1mk-1 + (1 - β1)Vz,k
Update biased estimate of second moment: vk = β2vk-1 + (1 - β2)Vz2,k
Scale the step-size: ^ = ηp1 — β2/(1 — βk)
Perform anchored gradient step: Zk = Zk-i —m √=^ + Yak — Zk-i)
Return ZK
29
Under review as a conference paper at ICLR 2020
batch size = 64
Adam learning rate = 0.0002
Adam β1 = 0.5
Adam β2 = 0.999
max iteration = 200000
GAN objctive = “WGAN-GP”
Gradient penalty parameter λ = 10
ndis = 5
Optimizer = “Adam”, “Optimistic Adam”, or “Anchored Adam
Optimism rate ρ = 1
Anchor rate γ = 1
Anchor refresh period T = 10000
Table 2:	Hyperparameters for the MNIST experiment
Generator
latent space 128 (Gaussian noise)
dense 4 × 4 × 512 batchnorm ReLU
4 × 4 conv.T stride=2 256 batchnorm ReLU
4 × 4 conv.T stride=2 128 batchnorm ReLU
4 × 4 conv.T stride=2 64 batchnorm ReLU
4 X 4 conv.T Stride=1 3 weightnorm tanh
Discriminator
Input Image 32 × 32 × 3
3 × 3 conv. stride=1 64 lReLU
3 × 3 conv. stride=2 128 lReLU
3 conv. stride=1 128 lReLU
3 conv. stride=2 256 lReLU
3 conv. stride=1 256 lReLU
3 conv. stride=2 512 lReLU
3 conv. stride=1 512 lReLU
dense 1
Table 3:	Generator and discriminator architectures for the CIFAR-10 experiment
batch size = 64
Adam learning rate = 0.0001
Adam β1 = 0.0
Adam β2 = 0.9
max iteration = 100000
GAN objctive = “WGAN-GP”
Gradient penalty parameter λ = 1
ndis = 1
Optimizer = “Adam”, “Optimistic Adam”, or “Anchored Adam
Optimism rate ρ = 1
Anchor rate γ = 1
Anchor refresh period T = 10000
Table 4:	Hyperparameters for the CIFAR-10 experiment
30