Under review as a conference paper at ICLR 2020

STUDENT    SPECIALIZATION    IN    DEEP    RELU   NET-
WORKS  WITH  FINITE  WIDTH  AND  INPUT  DIMENSION

Anonymous authors

Paper under double-blind review

ABSTRACT

To analyze deep ReLU network, we adopt a student-teacher setting in which an
over-parameterized student network learns from the output of a fixed teacher net-
work  of  the same  depth,  with  Stochastic  Gradient  Descent (SGD).  Our  contri-
butions  are  two-fold.   First,  we  prove  that  when  the  gradient  is  small  at  every
training sample, student node specializes to teacher nodes in the lowest layer un-
der mild conditions. Second, analysis of noisy recovery and training dynamics in
2-layer network shows that strong teacher nodes (with large fan-out weights) are
learned first and subtle teacher nodes are left unlearned until late stage of train-
ing.  As a result, it could take a long time to converge into these small-gradient
critical points. Our analysis shows that over-parameterization is a necessary con-
dition for specialization to happen at the critical points, and helps student nodes
cover more teacher nodes with fewer iterations. Both improve generalization. Dif-
ferent from Neural Tangent Kernel (Jacot et al., 2018) and statistical mechanics
approach (Goldt et al., 2019), our approach operates on finite width, mild over-
parameterization (as long as there are more student nodes than teacher) and finite
input dimension. Experiments justify our finding.

1    INTRODUCTION

Deep Learning has achieved great success in the recent years (Silver et al., 2016; He et al., 2016;
Devlin et al., 2018).  Although networks with even one-hidden layer can fit any function (Hornik
et al., 1989), it remains an open question how such networks can generalize to new data.  Different
from  what  traditional  machine  learning  theory  predicts,  empirical  evidence  (Zhang  et  
al.,  2017)
shows more parameters in neural network lead to better generalization. How over-parameterization
yields strong generalization is an important question for understanding how deep learning works.

In this paper, we analyze deep ReLU networks with teacher-student setting: a fixed teacher network
provides the output for a student to learn via SGD. Both teacher and student are deep ReLU net-
works. Similar to (Goldt et al., 2019), the student is over-realized compared to the teacher: at 
each
layer l, the number nl of student nodes is larger than the number ml of teacher (nl  >  ml).  Al-
though over-realization is different from over-parameterization, i.e., the total number of 
parameters
in the student model is larger than the training set size N , over-realization directly correlates 
with
the width of networks and is a measure of over-parameterization.

The student-teacher setting has a long history (Saad & Solla, 1996; 1995; Freeman & Saad, 1997;
Mace & Coolen, 1998) and recently gains increasing interest (Goldt et al., 2019; Aubin et al., 2018)
in analyzing 2-layered network.  While worst-case performance on arbitrary data distributions may
not be a good model for real structured dataset and can be hard to analyze, using a teacher network
implicitly enforces an inductive bias and could potentially lead to better generalization bound.

Specialization, that is, a student node becomes increasingly correlated with a teacher node during
training (Saad & Solla, 1996), is one of the important topic in this setup.  If all student nodes 
are
specialized to the teacher, then student tends to output the same as the teacher and generalization
performance can be expected. Empirically, it has been observed in 2-layer networks (Saad & Solla,
1996; Goldt et al., 2019) and multi-layer networks (Tian et al., 2019; Li et al., 2016), in both 
synthetic
and real dataset.  In contrast, theoretical analysis is limited with strong assumptions (e.g., 
Gaussian
inputs, infinite input dimension, local convergence, 2-layer setting, small number of hidden nodes).
In this paper, with arbitrary training distribution and finite input dimension, we show rigorously 
that
when gradient at each training sample is small (i.e., the interpolation setting as suggested in (Ma

1


Under review as a conference paper at ICLR 2020

et al., 2017; Liu & Belkin, 2018; Bassily et al., 2018)), the student node at the lowest layer can 
be
proven to specialize to the teacher nodes: each teacher node is aligned with at least one student
node in the lowest layer.  This explains one-to-many mapping between teacher and student nodes
and the existence of un-specialized student nodes, as observed empirically in (Saad & Solla, 1996).
Furthermore, from the proof condition, more over-realization encourages specialization.

Our  setting  is  different  from  previous  works.   (1)  While  statistical  mechanics  
approaches  (Saad
& Solla, 1996; Goldt et al., 2019; Gardner & Derrida, 1989; Aubin et al., 2018) assume both the
training set size N  and the input dimension d goes to infinite (i.e., the thermodynamics limits) 
and
assume Gaussian inputs, our analysis allows finite d and impose no parametric constraints on the
input data distribution.  (2) While Neural Tangent Kernel (Jacot et al., 2018; Du et al., 2018b) and
mean-field approaches (Mei et al., 2018) requires infinite (or very large) width, our setting 
applies to
finite width as long as student is slightly over-realized (nl     ml). In this paper we study the 
infinite
training sample case (the training set is a region), and leave finite sample analysis as the future 
work.

In addition, we further analyze the training dynamics and show that most student nodes converge
first towards strong teacher nodes with large fan-out weights in magnitude. While this makes train-
ing robust to dataset noise and naturally explains implicit regularization, the same mechanism also
leaves weak teacher nodes unexplained until very late stage of training, yielding high 
generalization
error with finite iterations.  In this situation, we show that over-realization plays another 
important
role:  once the strong teacher nodes have been covered, there are always spare student nodes ready
to switch to weak teacher nodes quickly. Empirically, we show more teacher nodes are covered with
the same number of iterations, and generalization is also improved.

We verify our findings with numerical experiments.  Starting with 2-layer setting, we justify Theo-
rem 2 and Theorem 3 with Gaussian inputs, showing one-to-many specialization and existence of
un-specialized nodes.  For deep ReLU networks, we show specialization happens not only in the
lowest  layer, as suggested by Theorem 4, but also in other hidden layers, on both Gaussian inputs
and CIFAR10.  We also perform ablation studies about the effect of student over-realization.  For
training dynamics, we show the strong/weak teacher effects in 2-layer settings and over-realization
could improve specialization and generalization.

2    RELATED  WORKS

Student-teacher setting.  This setting has a long history (Engel & Van den Broeck, 2001; Gard-
ner & Derrida, 1989).  The seminar works (Saad & Solla, 1996; 1995) studies 1-hidden layer case
from statistical mechanics point of view in which the input dimension goes to infinity, or so-called
thermodynamics limits.  They study symmetric solutions and locally analyze the symmetric break-
ing     behavior and onset of specialization of the student nodes towards the teacher. Recent 
follow-up
works (Goldt et al., 2019) makes the analysis rigorous and empirically shows that random initial-
ization and training with SGD indeed gives student specialization in 1-hidden layer case, which is
consistent with our experiments. With the same assumption,  (Aubin et al., 2018) studies phase tran-
sition property of specialization in 2-layer networks with small number of hidden nodes using 
replica
formula. In these works, inputs are assumed to be Gaussian and step or Gauss error function is used
as nonlinearity.   Few works study teacher-student setting with more than two layers. (Allen-Zhu
et al., 2019a) shows the recovery results for 2 and 3 layer networks, with modified SGD, batchsize
1 and heavy over-parameterization.

In comparison, our work shows that specialization happens around the SGD critical points in the
lowest layer for deep ReLU networks, without any parametric assumptions of input distribution.

Local minima is Global.  While in deep linear network, all local minima are global (Laurent &
Brecht, 2018; Kawaguchi, 2016), situations are quite complicated with nonlinear activations. While
local minima is global when the network has invertible activation function and distinct training 
sam-
ples (Nguyen & Hein, 2017; Yun et al., 2018) or Leaky ReLU with linear separate input data (Lau-
rent & von Brecht, 2017), multiple works (Du et al., 2018a; Ge et al., 2017; Safran & Shamir, 2017;
Yun et al., 2019) show that in GD case with population or empirical loss, spurious local minima
can happen even in two-layered network.  Many are specific to two-layer and hard to generalize to
multi-layer setting. In contrast, our work brings about a generic formulation for deep ReLU network
and gives recovery properties in the student-teacher setting.

2


Under review as a conference paper at ICLR 2020


(a)

(b)

Layer l

(ùëõ"  nodes)


Teacher Network

(Fixed parameters)

Student Network

(Learnable Parameters)

Layer l ‚Äì 1

(ùëõ"#$ nodes)

Figure 1:  Problem Setup.  (a) Student-teacher setting.  The student network learns from the output 
of a fixed
teacher network via stochastic gradient descent (SGD). (b) Notations. All low cases are scalar, 
bolds are column
vectors and upper cases are matrices.

Learning wild networks.  Recent works on Neural Tangent Kernel (Jacot et al., 2018; Du et al.,
2018b; Allen-Zhu et al., 2019b) show the global convergence of GD for multi-layer networks with
infinite width.   (Li & Liang, 2018) shows the convergence in one-hidden layer ReLU network us-
ing GD/SGD to solution with good generalization, when the input data are assumed to be clustered
into classes.  Both lines of work assume heavily over-parameterized network, requiring polynomial
growth of number of nodes with respect to the number of samples.   (Chizat & Bach, 2018) shows
global convergence of over-parameterized network with optimal transport.   (Tian et al., 2019) as-
sumes mild over-realization and gives convergence results for 2-layer network when a subset of the
student network is close to the teacher. Our work extends it with much weaker assumptions.

Deep Linear networks.  For deep linear networks, multiple works (Lampinen & Ganguli, 2019;
Saxe et al., 2013; Arora et al., 2019; Advani & Saxe, 2017) have shown interesting training dynam-
ics.  One common assumption is that the singular spaces of weights at nearby layers are aligned
at initialization, which decouples the training dynamics.  Such a nice property would not hold for
nonlinear network.  (Lampinen & Ganguli, 2019) shows that under this assumption, weight compo-
nents with large singular value are learned first, while we analyze and observe empirically similar
behaviors on the student node level. Generalization property of linear networks can also be analyzed
in    the limit of infinite input dimension with teacher-student setting (Lampinen & Ganguli, 2019).
However, deep linear networks lack specialization which plays a crucial role in the nonlinear case.
To    our knowledge, we are the first to analyze specialization rigorously in deep ReLU networks.

3    MATHEMATICAL  FRAMEWORK

Notation.  Consider a student network and its associated teacher network (Fig. 1(a)).  Denote the
input as x. We focus on multi-layered networks with œÉ( ) as ReLU nonlinearity. We use the follow-
ing equality extensively: œÉ(x) = I[x > 0]x, where I[ ] is the indicator function. For node j, fj(x),
zj(x) and gj(x) are its activation, gating function and backpropagated gradient after the gating.

Both teacher and student networks have L layers.  The input layer is layer 0 and the topmost layer
(layer that is closest to the output) is layer L.  For layer l, let ml be the number of teacher node
while nl be the number of student node.  The weights Wl ‚àà Rnl‚àí1 √ónl   refers to the weight matrix

that connects layer l ‚àí 1 to layer l on the student side.  Wl  =  [wl,‚ÇÅ, wl,‚ÇÇ, . . . , wl,nl ] 
where each


w  ‚àà  Rnl‚àí1   is the weight vector.   Similarly we have teacher weight Wl‚àó

W = {W‚ÇÅ, W‚ÇÇ, . . . , WL} as the collection of all trainable parameters.

‚àà  Rml‚àí1 √óml .   Denote

Let  fl(x)   =   [fl,‚ÇÅ(x), . . . , fl,n (x)];  ‚àà   Rnl    be  the  activation  vector  of  layer  
l,  Dl(x)   =

diag[zl,‚ÇÅ(x), . . . , zl,n (x)]  ‚àà  Rnl√ónl   be  the  diagonal  matrix  of  gating  function  (for  
ReLU  it  is


either 0 or 1), and

l

gl(x) = [gl,‚ÇÅ(x), . . . , gl,nl

(x)]; ‚àà Rnl  be the backpropated gradient vector.  By

definition, the input layer has f‚ÇÄ(x) = x ‚àà Rn0  and m‚ÇÄ = n‚ÇÄ. Note that fl(x), gl(x) and Dl(x) are
all dependent on W. For brevity, we often use fl(x) rather than fl(x; W).

All notations with superscript ‚àó are from the teacher, only dependent on the teacher and remains the
same throughout the training.  DL‚àó (x) = DL(x)      IC  C since there is no ReLU gating.  Note that
C is the dimension of output for both teacher and student. With the notation, gradient descent is:

WÀô l  = E‚Çì [fl‚àí‚ÇÅ(x)g;(x)]                                                       (1)

In SGD, the expectation E‚Çì [¬∑] is taken over a batch. In GD, it is over the entire dataset.

3


Under review as a conference paper at ICLR 2020

Bias  term.    With  the  same  notation  we  can  also  include  the  bias  term.    In  this  
case,  Wl  ‚àà
R(nl‚àí1 +1)√ónl , wl,‚ÇÅ  =  [wÀú ; b]  ‚àà Rnl‚àí1 +1, fl  ‚àà Rnl+1  (last column is all one), gl  ‚àà Rnl+1  
and
Dl ‚àà R(nl+1)√ó(nl+1)  (last diagonal element is always 1).

Objective.  We assume that both the teacher and the student output a vector.  We use the output of
teacher as the input of the student and the objective is:


1

min J(W) =    E‚Çì

Œ£«ÅfL‚àó (x) ‚àí fL

(x)«Å2Œ£                                   (2)

W                          2

We want to ask the following qeustion:

Are student nodes specialized to teacher nodes at the same layers after training?      (*)

One might wonder this is hard since the student‚Äôs intermediate layer receives no direct supervision
from the corresponding teacher layer, but relies only on backpropagated gradient. Surprisingly, the
following theorem shows that it is possible for every intermediate layer:

Lemma 1 (Recursive Gradient Rule).  At layer l, the backpropagated gl(x) satisfies

gl(x) = Dl(x) [Al(x)fl‚àó(x) ‚àí Bl(x)fl(x)] ,                                       (3)

where  the  mixture  coefficient  Al(x)  =  V ;(x)Vl‚àó(x)  ‚àà  Rnl√óml   and  Bl(x)  =  V ;(x)Vl(x)  ‚àà


Rnl√ónl . The matrices Vl(x) ‚àà RC√ónl  and Vl‚àó(x) ‚àà RC√óml  are defined in a top-down manner:

Vl‚àí‚ÇÅ(x) = Vl(x)Dl(x)W ;,     Vl‚àó 1(x) = Vl‚àó(x)Dl‚àó(x)W ‚àó;

(4)

l             ‚àí                                           l

In particular, VL(x) = VL‚àó(x) = IC√óC.

For convenience,  we can write Vl(x)  =  [vl,‚ÇÅ(x), vl,‚ÇÇ(x), . . . , vl,nl (x)],  then we have each 
ele-
ment of Al, Œ±l,jj' (x) = v; (x)vl‚àó,j' (x) and element of Bl, Œ≤l,jj' (x) = v; (x)vl,j' (x).  Note 
that

Lemma 1 applies to arbitrarily deep ReLU networks and allows different number of nodes for the

teacher and student. In particular, student can be over-parameterized (or over-realized).

Let R‚ÇÄ  =   x : œÅ(x) > 0   be the infinite training set, where œÅ(x) is the input data distribution. 
Let
Rl =   fl(x) : x      R‚ÇÄ  , which is the image of the training set at the output of layer l, and 
also a
convex polytope. Then the mixture coefficient Al(x) and Bl(x) have the following property:

Corollary 1 (Piecewise constant).  R‚ÇÄ can be decomposed into a finite (but potentially exponential)


set of regions Rl‚àí‚ÇÅ = {R¬π

, R¬≤

l‚àí1

, . . . , RJ

l‚àí1

}. Al(x) and Bl(x) are constant in Rl‚àí1.

4    CRITICAL  POINT  ANALYSIS

We first show that due to property of ReLU node and subset sampling in SGD, at SGD critical point,
under mild condition, the teacher node aligns with at least one student node and the goal (*) can be
reached in the lowest layer.

Definition 1 (SGD critical point).  WÀÜ  is a SGD critical point if for any batch, WÀô l  = 0 for 1 ‚â§ 
l ‚â§ L.

Theorem 1 (Interpolation).  Denote D = {xi} as a dataset of N  samples.  If WÀÜ  is a critical point
for SGD, then either gl(xi; WÀÜ ) = 0 or fl‚àí‚ÇÅ(xi; WÀÜ ) = 0.

Such critical points exist since over-realized student can mimic teacher perfectly.  Note that 
critical
points in SGD is much stronger than those in GD, where the gradient is always averaged at a fixed
data distribution.  If fl‚àí‚ÇÅ has a bias term (and fl‚àí‚ÇÅ  /= 0 always), then gl(xi; WÀÜ ) = 0.  For 
topmost
layer, immediately we have gL(xi;  ÀÜ) = fL‚àó (xi)     fL(xi) = 0, which is global optimum with zero
training loss. In the following, we want to check whether this condition leads to specialization, 
i.e.,

whether the teacher‚Äôs weights are recovered/aligned by the student, i.e., whether for teacher j, 
there
exists a student k at the same layer so that wk = Œ≥wj for some Œ≥ > 0.

Note that gl(xi; WÀÜ )  =  0 might be a strong assumption since in practicŒ£e the gradient isŒ£ small 
but

	

4


Under review as a conference paper at ICLR 2020


Iteration 0

Iteration 2

Iteration 10

Iteration 29


0.6

0.4

0.2

0.03

0.04

0.04

0.04

0.04

0.03

0.6

0.4

0.2

1.73

0.78

0.24

0.18

2.12

1.78

0.6

0.4

0.2

2.27

0.67

0.01

0.01

2.19

1.83

0.6

0.4

0.2

2.27

0.66

0.01

0.00

2.19

1.83


0.0

0.0

0.0

0.0


0.2

0.2

0.2

0.2


0.4

0.4

0.4

0.4


0.6

0.6

0.6

0.6


0.6       0.4       0.2     0.0       0.2       0.4       0.6

0.6       0.4       0.2     0.0       0.2       0.4       0.6

0.6       0.4       0.2     0.0       0.2       0.4       0.6

0.6       0.4       0.2     0.0       0.2       0.4       0.6

Figure 2:  Convergence (2 dimension) for 2 teachers (solid line) and 6 students (dashed line).  
Legend shows

«Åvk«Å for student node k. «Åvk«Å ‚Üí 0 for nodes that are not aligned with teacher.

4.1    ASSUMPTION OF TEACHER NETWORK

Obviously, an arbitrary teacher network won‚Äôt be reconstructed.  A trivial example is that a teacher
network always output 0 since all the training samples lie in the inactive halfspace of its ReLU
nodes. Therefore, we need to impose condition on the teacher network.

Let Ej = {x : fj(x) > 0} be the activation region of node j. Note that the halfspace Ej is an open
set. Let ‚àÇEj = {x : fj(x) = 0} be the decision boundary of node j.

Definition 2 (Observer).  Node k is an observer of node j if Ek ‚à© ‚àÇEj /= √ò.

Assumption 1 (Teacher Network).  For each layer l, we require that (1) the teacher weights wl‚àó,j

are not co-linear. and (2) the boundary of wl‚àó,j  is visible in the training set: ‚àÇEl‚àó,j ‚à© Rl‚àí‚ÇÅ /= 
√ò.

Assumption 1 is our assumption of the teacher.  The first requirement is trivial.  The second one is
reasonable since two teacher nodes who behaves linearly in the training set are indistinguishable.

4.2    ALIGNMENT OF TEACHER WITH STUDENT, 2-LAYER CASE

We first start with 2-layer case, in which A‚ÇÅ(x) and B‚ÇÅ(x) are constant with respect to x, since
there is no ReLU gating at the top layer l  = 2.  In this case, from the SGD critical point at l  = 
1,
g‚ÇÅ(x) = D‚ÇÅ(x) [A‚ÇÅf1‚àó(x) ‚àí B‚ÇÅf‚ÇÅ(x)] = 0, alignment between teacher and student can be achieved:

Theorem 2 (Student-teacher Alignment, 2-layers).  With Assumption 1, at SGD critical point, if a
teacher node j is observed by a student node k and Œ±kj = 0, then there exists at least one student
node k‚Ä≤ aligned with j.

The intuition is that if the input x takes sufficiently diverse values, ReLU activations œÉ(w;x) can
be proven to be mutually linear independent.  On the other hand, the gradient of each student node

k when active, is Œ±;f‚ÇÅ(x) ‚àí b;f‚ÇÅ(x) = 0, a linear combination of teacher and student nodes (note


Œ±; and Œ≤; are k   k

k

and B  ).  Therefore, zero gradient means that the summation of

k              k            -th rows of A‚ÇÅ                 ‚ÇÅ

coefficients of co-linear ReLU nodes is zero.  Since teachers are not co-linear, any teacher node is
co-linear with at least one student node. Alignment with multiple student nodes is also possible. If
there is no nonlinearity (e.g., deep linear models), alignment won‚Äôt happen since a linear subspace
has many representations.

Note that a necessary condition of a reconstructed teacher node is that its boundary is in the ac-
tive region of student, or is observed (Definition 2).  This is intuitive since a teacher node which
behaves like a linear node is partly indistinguishable from a bias term. This also suggests that 
over-
parameterization (more student nodes) are important.  More student nodes mean more observers,
and the existence argument in Theorem 4 is more likely to happen and more teacher nodes can be
covered by student, yielding better generalization.

For student nodes that are not aligned with the teacher, if they are observed by other student 
nodes,
then following a similar logic, we have the following:

Theorem 3 (Prunable Un-specialized Student Nodes).  With Assumption 1, at SGD critical point,
if an unaligned student k has C independent observers (concatenating v yields a full rank matrix),
then     k'‚ààco-linear(k) vk' «Åwk' «Å = 0. If node k is not co-linear with any other student, then vk 
= 0.

Corollary 2.  With sufficient observers, the contribution of all unaligned student nodes is zero.

5


Under review as a conference paper at ICLR 2020

Theorem 3 and Corollary 2 open the way of network pruning (LeCun et al., 1990; Hassibi et al.,
1993; Hu et al., 2016).  This is consistent with Theorem 5 in (Tian et al., 2019) which also shows
the fan-out weights are zero up on convergence in 2-layer networks, if the initialization is close. 
In
contrast, Theorem 3 analyzes the critical point rather than the dynamics.

Note that a relate theorem (Theorem 6) in (Laurent & von Brecht, 2017) studies 2-layer network
with scalar output and linear separable input, and discusses characteristics of individual data 
point
contributing loss in a local minima of GD. Here no linear separable condition is imposed.

4.3    MULTI-LAYER CASE

Thanks to Lemma 1 which holds for deep ReLU networks, we can use similar intuition to analyze the
behavior of the lowest layer (l = 1) in the multiple layer case. The difference here is that A‚ÇÅ(x) 
and
B‚ÇÅ(x) are no longer constant over x. Fortunately, using Corollary 1, we know that A‚ÇÅ(x) and B‚ÇÅ(x)
are piece-wise constant that separate the input region R‚ÇÄ into a finite (but potentially 
exponential)
set of constant regions R‚ÇÄ  =  {R¬π, R¬≤, . . . , RJ } plus a zero-measure set.  This suggests that 
we

could check each region separately. If the boundary of a teacher j and a student k lies in the 
region,

similar logic applies (here Œ±kj is the (k, j) entry of A‚ÇÅ(x) and is constant in a region R ‚àà R‚ÇÄ).

Theorem  4  (Student-teacher  Alignment,  Multiple  Layers).  With  Assumption  1,  at  SGD  
critical
points, for any teacher node j at l = 1, if there exists a region R ‚àà R and a student observer k so

that ‚àÇEj‚àó ‚à© Ek ‚à© R /= √ò and Œ±kj(R)     0, then node j aligns with at least one student node k‚Ä≤.

Note that even with random V‚ÇÅ(x) (e.g., at initialization), Theorem 4 still holds with high 
probability
(when Œ±kj  =  0) and teacher f1‚àó(x) can still align with student f‚ÇÅ(x).  This suggests a picture of
bottom-up training in backpropagation:  After the alignment of activations at layer 1, we just treat
layer     1 as the low-level features and the procedure repeats until the student matches with the 
teacher
at all layers.   This is consistent with many previous works that empirically show the network is
learned in a bottom-up manner (Li et al., 2018).

Note that the alignment may happen concurrently across layers:  if the activations of layer 1 start
to align, then activations of layer 2, which depends on activations of layer 1, will also start to 
align
since there now exists a W‚ÇÇ  that yields strong alignments, and so on.  This creates a critical path
from important student nodes at the lowest layer all the way to the output,  and this critical path
accelerates the convergence of that student node. We leave a formal analysis to the future work.

Small  Gradient  Case.   In  practice,  stochastic  gradient  (or  its  expectation  over  time)  
fluctuates
around zero (  g‚ÇÅ(x)            œµ,  or Et [  g‚ÇÅ(x)     ]       œµ),  but never zero.   In this case, 
 Theorem 5
shows that a rough specialization still follows. The ratio of recovery is also shown for 
weights/biases
separately, as a function of œµ. Note Œ∏Àújj'  is the angle of two weights wÀú j and wÀú j' .

Theorem 5 (Noisy Recovery).  If Assumption 1 holds and any two teachers wj‚àó, wj‚àó'  satisfy Œ∏Àújj'  ‚â•
Œ∏‚ÇÄ > 0 or |b‚àój' ‚àí b‚àój | ‚â• b‚ÇÄ > 0. Suppose «Åg‚ÇÅ(x, WÀÜ )«Å‚àû ‚â§ œµ for any x ‚àà R‚ÇÄ with œµ ‚â§ œµ‚ÇÄ, then for any
teacher j at l = 1, if there exists a region R ‚àà R and a student observer k so that ‚àÇEj‚àó ‚à©Ek ‚à©R /= 
√ò,


and Œ±kj(R) /= 0, then j is roughly aligned with a student k‚Ä≤: sin Œ∏jk'  = O

s1‚àíŒ¥

|Œ±kj |

and |b‚àój ‚àí bk' | =


s1‚àí2Œ¥       for any Œ¥ > 0. The hidden constants depends on Œ¥, œµ

|Œ±kj |                                                                                              
                            0

and the size of region ‚àÇEj‚àó ‚à©Ek

‚à©R.

Note  that Et [  g‚ÇÅ(x)     ]       œµ  leads  to   g‚ÇÅ(x)             œµ  at  least for  some  
iteration  t.   Therefore,
Theorem  5  still  applies  since  it  does  not  rely  on  past  history  of  the  
weight/gradient.   Note  that
Theorem 5 assumes infinite number of data points and leave finite sample case to future work.

5    ANALYSIS  ON  TRAINING  DYNAMICS

Our analysis so far shows student specialization happens at SGD critical points under mild condi-
tions.  A natural question arises:  is running SGD long enough sufficient to achieve these critical
points?  Some previous works (Ge et al., 2017; Livni et al., 2014) show that empirically SGD does
not recover the parameters of a teacher network up to permutation, while other works (Saad & Solla,
1996; Goldt et al., 2019) show specialization happens. Why there is a discrepancy? There are several
reasons.  First, from Theorem 3, there exist un-specialized student nodes, so a simple permutation
test  on student weights might fail.  Second, as suggested by Theorem 5, it can take a long time to

6


Under review as a conference paper at ICLR 2020

recover a teacher node k with small   vk‚àó    (since Œ±kj  =  vk‚àó;vj).  In fact, if vk‚àó  =  0 then it 
has no
contribution to the output and recovery never happens. This is particularly problematic if the 
output
dimension is 1 (scalar output), since a single small teacher weight vk‚àó  would block the recovery of
the entire teacher node k.  Previous works (Lampinen & Ganguli, 2019) shows similar behaviors

in the dynamics of singular values in deep linear networks in teacher-student setting, which lack
student specialization. Here we study these behaviors in deep ReLU networks.

In the following, we analyze various local dynamic behaviors of 2-layer ReLU network. Due to the
complexity, we leave a formal characterization of the entire training procedure for future work.

Definition 3.  A teacher node j is strong (or weak), if «Åvj‚àó«Å is large (or small).

In this case, the dynamics can be written as the following:

wÀô k = E‚Çì [fl‚àí‚ÇÅzk[fl‚àó;Œ±k ‚àí f ;Œ≤k ]] = E‚Çì [fl‚àí‚ÇÅzk[Vl‚àófl‚àó ‚àí Vlfl];vk] = E‚Çì [fl‚àí‚ÇÅzkr;vk] ,       (5)
where V‚ÇÅ and V1‚àó are constant, Œ±k = Vl‚àó;vk, Œ≤k  = V ;vk and residue rl = Vl‚àófl‚àó ‚àí Vlfl ‚àà RC .

5.1    WEIGHT MAGNITUDE

From Eqn. 5, we know that for both ReLU and linear network (since fk(x) = zk(x)w;fl‚àí‚ÇÅ(x)):

1 d«Åwk«Å2              ;                       ;

2      dt      = wk wÀô k = E‚Çì [fkrl vk]                                             (6)

When there is only a single output, rl is a scalar and Eqn. 6 is simply an inner product between the
residue and the activation of node k, over the batch.  So if the node k has activation which aligns
well with the residual, the inner product is larger and «Åwk«Å grows faster.

5.2    ANGLES BETWEEN TEACHER AND STUDENT WEIGHTS

Note that Eqn. 6 only tell that the weight norm would increase, but didn‚Äôt tell whether wk converges
to any teacher node wj‚àó. It could be the case that   wk   goes up but doesn‚Äôt move towards the 
teacher.
To see that, let‚Äôs check the quantity:


Ex Œ£fl‚àí1zk fj‚àóŒ£ = Ex Œ£fl‚àí1zk zj‚àóf ; 1

Œ£ wj‚àó = Gkj wj‚àó

(7)

where Gkj = E‚Çì Œ£fl‚àí‚ÇÅzkzj‚àóf ;  Œ£. Putting it in another way, we want to check the spectrum property

of the PSD matrix Gkj.  Intuitively, the direction of E‚Çì  fl  ‚ÇÅzkfj‚àó   should lie between wk and 
wj‚àó,

and the magnitude is large when wk and wj‚àó are close to each other. This means that if r is 
dominated

by a teacher j (i.e.,   vj‚àó    is large), then wÀô k would push wk towards wj‚àó. This also shows that 
SGD
will first try fitting strong teacher nodes, then weak teacher nodes.

Theorem 6 confirms this intuition if fl‚àí‚ÇÅ follows spherical symmetric distribution (e.g., N (0, 
I)).

Theorem   6.   If   fl  ‚ÇÅ    follows   spherical   symmetric   distribution,    then   E‚Çì  fl  ‚ÇÅzkf 
‚àó
«Åwj‚àó «Å«Åwk «Å Œ£(œÄ ‚àí Œ∏)w‚àó + sin Œ∏w  Œ£, where Œ∏ is the angle between w‚àó and w  .

As a result, for all Œ∏     [0, œÄ], E‚Çì  fl  ‚ÇÅzkfj‚àó   is always between wj‚àó and wk since œÄ    Œ∏ and 
sin Œ∏ are
always non-negative. Without such symmetry, we assume the following holds:

Assumption 2.  E‚Çì [fl‚àí‚ÇÅzkfj] = œà(Œ∏jk)wj + œà‚Ä≤(Œ∏jk)wk, where œà(œÄ) = 0.

Note that critical point analysis is applicable to any batch size,  including 1.   On the other 
hand,
Assumption 2 holds when a moderately large batchsize leads to a decent estimation of the terms.

With this assumption, we can write the dynamics as wÀô k =   wk  rk, where the time-varying residue

rk of node k is defined as the following (ŒΩ is a scalar related to œà‚Ä≤):

rk = Œ£ Œ±jkœà(Œ∏jk)wj‚àó ‚àí Œ£ Œ≤k'k œà(Œ∏k'k )wk'  ‚àí ŒΩwk                          (8)

	

5.3    SYMMETRIC BREAKING, WINNERS-TAKE-ALL AND FOCUS SHIFTING

We could show that for two nodes k = k‚Ä≤, regardless of the form of rk, we have (note that w¬Ø  is the
length-normalized version of w):

7


Under review as a conference paper at ICLR 2020


1x, loss=0.00

2x, loss=0.00

5x, loss=0.00

10x, loss=0.00

3                                                             3                                     
                        3                                                             3

2                                                             2                                     
                        2                                                             2

1                                                             1                                     
                        1                                                             1


0

0.0     0.2     0.4     0.6     0.8     1.0

0

0.0     0.2     0.4     0.6     0.8     1.0

0

0.0     0.2     0.4     0.6     0.8     1.0

0

0.0     0.2     0.4     0.6     0.8     1.0

Figure  3:  Student  specialization  of  a  2-layered  network  with  10  teacher  nodes  and  
1x/2x/5x/10x  student
nodes. p is teacher polarity factor (Eqn. 9). For a student node k, we plot its normalized 
correlation (in terms of
activation vector evaluated in a separate evaluation set) to its best correlated teacher as the x 
coordinate and the

fan-out weight norm   vk   as the y coordinate. We plot results from 32 random seed. Student nodes 
of different

seeds are in different color. An un-specialized student node has low fan-out weight norm (Theorem 
3).

.


Theorem 7.  For dynamics wÀô

= «Åw  «År  , we have  ·µà  ln  «Å ∑k «Å

= w¬Ø ;r

‚àí w¬Ø    r  ' .


k             k     k

dt       «Åwk' «Å

k   k          k'   k

We consider a special (and symmetric) case: rk = r = w‚àó ‚àí Œ£k akwk with all ak > 0, where w‚àó

 d         ;            ;

dt (w¬Ø k rk    w¬Ø k' rk' ) < 0 and vice versa. So the system provides negative feedback until w¬Ø k 
= w¬Ø k'

and according to Eqn. 7, the ratio between   wk   and   wk'    remains constant, after initial 
transition.
We can also show that w¬Ø k will align with w‚àó and every student node goes to w‚àó.

However, due to Theorem 6, the net effect w‚àó can be different for different students and thus rk are
different. This opens the door for complicated dynamic behavior of neural network training.

Symmetry breaking. As one example, if we add a very small delta to some node, say k = 1 so that

r   = r + œµw‚àó. Then to make  ·µà (w¬Ø ;r   ‚àí w¬Ø    r  ' ) = 0, we have w¬Ø   r   > w¬Ø    r  '  and thus 
according

to Theorem 7,    wk  /  wk'     grows exponentially.   This symmetric breaking behavior provides a
potential winners-take-all mechanism, since according to Theorem 6, the coefficient of w‚àó depends
critically on the initial angle between wk and w‚àó.

Strong teacher nodes are learned first. If   vj‚àó    is the largest among teacher nodes, then the 
joint
w‚àó heavily biases towards teacher j  and all student nodes move towards teacher j.   As a result,
strong teacher learns first and is often covered by multiple co-linear students (Fig. 6, 
teacher-0).

Focus shifting to weak teacher nodes. The process above continues until residual along the direc-
tion of wj‚àó quickly shrinks and residual corresponding to other teacher node (e.g., wj‚àó'  for j‚Ä≤ =/ 
   j)
becomes  dominant.   Since  each  rk is  different,  student  node  k  whose  direction  is  closer 
 to  wj‚àó'

(j‚Ä≤     j) will shift their focus towards wj‚àó' , as shown in the green (shift to teacher-2) and 
magenta

(shift to teacher-5) curves in Fig. 6.

Possible slow convergence to weak teacher nodes.  While expected angle between two weights
from initialization is œÄ/2, shifting a student node wk from chasing after a strong teacher node wj‚àó
to  a weaker one wj‚àó'  could yield a large initial angle (e.g., close to œÄ) between wk and wj' .  
For
example, all student nodes have been attracted to the opposite direction of a weak teacher node.  
In

this case, the convergence can be arbitrarily slow.  In fact, if there is only one teacher node and 
Œ∏

is the angle between teacher and student, then from Eqn. 8 we arrive at Œ∏Àô  ‚àù ‚àíœà(Œ∏) sin Œ∏.  Since

œà(Œ∏) sin Œ∏ ‚àº (œÄ ‚àí Œ∏)¬≤ around Œ∏ = œÄ, the time spent from Œ∏ = œÄ ‚àí œµ to some Œ∏‚ÇÄ is t‚ÇÄ ‚àº 1  ‚àí     1     
 ‚Üí


+‚àû when œµ ‚Üí 0

s      œÄ‚àíŒ∏0

. In this case, over-realization helps by having more student nodes that are possibly
ready for shifting towards weaker teachers, and thus accelerate convergence (Fig. 7). Alternatively,
we could reinitialize those student nodes (Prakash et al., 2019).

6    EXPERIMENTS

We first verify our theoretical finding on synthetic dataset.  We generate the input using     (0, 
œÉ¬≤I)
with œÉ  =  10 and we sample 10k as training and another 10k as evaluation.  For deep ReLU net-
works, we regenerate the dataset after every epoch to mimic infinite sample setting.  The details of
teacher/student construction is in Appendix (Sec. 8.16). The normalized correlation between nodes
is computed in terms of activation vectors evaluated on a separate evaluation set.

Two layer networks. First we verify Theorem 2 and Theorem 3 in the 2-layer setting. Fig. 6 shows
student nodes correlate with different teacher nodes over time. Fig. 3 shows for different degrees 
of

8


Under review as a conference paper at ICLR 2020

Layer 4

Layer 3

Layer 2

Layer 1

Figure 4:  The strength of student specialization versus their fan-out coefficients in 4 layer ReLU 
network.
Number of hidden teacher nodes is 50-75-100-125.  Student is 10x over-realized.  The dataset is 
regenerated

with the input distribution after every epoch.  For node k, y-axis is     E‚Çì [Œ≤kk(x)], equivalent 
to the fan-out
weight norm «Åvk«Å in 2-layer case, and x-axis is its max correlation to the teachers. The lower 
layer learns first.


p = 0.5

p = 1

p = 1.5

p = 2

p = 2.5

1.00


0.75

0.50

0.25

0.00

1x

         2x

5x
10x

0       4       8      12     16

Teacher idx

0       4       8      12     16

Teacher idx

0       4       8      12     16

Teacher idx

0       4       8      12     16

Teacher idx

0       4       8      12     16

Teacher idx

Figure 5: Success rate (over 32 trials with different random seeds) of recovery of 20 teacher nodes 
on 2-layer
network at different teacher polarity p (Eqn. 9) and different over-realization. Dotted line: 
successful rate after
5 epochs. Solid line: successful rate after 100 epochs.

over-realization (1   /2   /5   /10   ), for nodes with weak specialization (i.e., its normalized 
correla-
tion to the most correlated teacher is low), their magnitudes of fan-out weights are small. 
Otherwise
the nodes with strong specialization have high fan-out weights.

Deep Networks. For deep ReLU networks, we observe specialization not only at the lowest layer, as
suggested by Theorem 4, but also at multiple hidden layers. This is shown in Fig. 4. For each 
student
node k, the x-axis is its best normalized correlation to teacher nodes, and y-axis is     E‚Çì 
[Œ≤kk(x)],
which is equivalent to   vk   in 2-layer case. In the plot, we can also see the lowest layer learns 
first
(the ‚ÄúL-shape‚Äù curve was established at epoch 10), then the top layers follow.

Ablation on the effect of over-realization.  To further understand the role of over-realization, we
plot the average rate of a teacher node that is matched with at least one student node successfully
(i.e., correlation > 0.95).  Fig. 5 shows that stronger teacher nodes are more likely to be matched,
while weaker ones may not be explained well, in particular when the strength of the teacher nodes
are polarized (p is large).  Over-realized student can explain more teacher nodes, while a student
with 1√ó nodes has sufficient capacity to fit the teacher perfectly, it gets stuck despite long 
training.

In addition, the evaluation loss (Appendix Fig. 11) shows that over-realization yields better 
general-
ization, in particular with large teacher node polarity (p is large), where weak teacher nodes are 
hard
to capture. For good performance on real datasets, getting weak teacher nodes can be important.

Training Dynamics. We set up a diverse strength of teacher node by constructing the fanout weights
of teacher node j as follows:

«Åvj‚àó«Å ‚àº 1/j·µñ,                                                               (9)

where p is the teacher polarity factor that controls how strong the energy decays across different
teacher nodes. p = 0 means all teacher nodes are symmetric, and large p means that the strength of
teacher nodes are more polarized.

9


Under review as a conference paper at ICLR 2020


teacher-0

teacher-1

teacher-2

teacher-3

teacher-4


1.0

1.0

1.0

1.0

1.0


0.8

0.8

0.8

0.8

0.8


0.6

0.6

0.6

0.6

0.6


0.4

0.4

0.4

0.4

0.4


0.2

0.2

0.2

0.2

0.2


0.0

0.0

0.0

0.0

0.0


0.2

0.2

0.2

0.2

0.2


0.4

0           10          20          30          40

epoch

0.4

0           10          20          30          40

epoch

0.4

0           10          20          30          40

epoch

0.4

0           10          20          30          40

epoch

0.4

0           10          20          30          40

epoch

Figure 6:  Student specialization with teacher polarity p  =  1 (Eqn. 9).   Same students are 
represented by
the same color across plots.  Three rows represent three different random seeds.  We can see more 
students
nodes specialize to teacher-1 first. In contrast, teacher-5 was not specialized until later by a 
node (e.g.,
magenta in the first row) that first chases after teacher-1 then shifts its focus.


p = 0.5, logloss = -6.45

p = 1, logloss = -6.50

p = 1.5, logloss = -6.50

p = 2, logloss = -5.09

p = 2.5, logloss = -5.30


1.0

1.0

1.0

1.0

1.0


0.8

0.8

0.8

0.8

0.8


0.6

0.6

0.6

0.6

0.6


0.4

0.4

0.4

0.4

0.4


0.2

0.2

0.2

0.2

0.2


0.0

0.0

0                  50                100

Epoch

0.0

0                  50                100

Epoch

0.0

0                  50                100

Epoch

0.0

0                  50                100

Epoch

0                  50                100

Epoch

Figure 7:  Evolution of best student correlation to teacher over iterations.  Each rainbow color 
represents one
of the 20 teachers (blue: strongest, red: weakest). 5x over-parameterization on 2-layer network.

Fig. 6 and Fig. 7 show that many student nodes specialize to a strong teacher node first.  Once the
strong teacher node was covered well, weaker teacher nodes are covered after many epochs.

CIFAR-10. We also experiment on CIFAR-10. We first pre-train a teacher network with 64-64-64-
64 ConvNet (64 are channel sizes of the hidden layers, L  =  5) on CIFAR-10 training set.  Then
the teacher network is pruned in a structured manner to keep strong teacher nodes.  The student is
over-realized based on teacher‚Äôs remaining channels.

The  convergence  and  specialization  behaviors  of  student  network  is  shown  in  Fig.  8.    
Spe-
cialization  happens  at  all  layers  for  different  degree  of  over-realization.      
Over-realization
boosts  student  specialization,  measured  by  mean  of  maximal  normalized  correlation  œÅm‚Çë‚Çên   
=
meanj   t‚Çë‚Çêch‚Çër maxj'    student Àúfj‚àó;Àúfj'  at each layer (Àúfj is the normalized activation of node 
j  over
N evaluation samples), and improves generalization, evaluated on CIFAR-10 evaluation set.

7    CONCLUSION  AND  FUTURE  WORK

In this paper, we use student-teacher setting to analyze how an (over-parameterized) deep ReLU
student network trained with SGD learns from the output of a teacher. When the magnitude of gra-
dient per sample is small (student weights are near the critical points), the teacher can be proven 
to
be covered by (possibly multiple) students and thus the teacher network is recovered in the lowest

layer.  By analyzing training dynamics, we also show that strong teacher node with large   v‚àó   is
reconstructed first, while weak teacher node is reconstructed slowly. This reveals one important 
rea-
son why the training takes long to reconstruct all teacher weights and why generalization improves

with more training. As the next step, we would like to extend our analysis to finite sample case, 
and
analyze the training dynamics in a more formal way. Verifying the insights from theoretical analysis
on a large dataset (e.g., ImageNet) is also the next step.


1x, Avg/Max eval accuracy: 79.67% / 80.15%

2x, Avg/Max eval accuracy: 83.09% / 83.48%

5x, Avg/Max eval accuracy: 83.85% / 84.18%


1.0

0.8

1.0

0.8

1.0

0.8

layer-0
layer-1
layer-2
layer-3


0.6

0.6

0.6


0.4

0.4

0.4


0.2

0.2

0.2


0                 20                40                60                80               100

Epoch

0                 20                40                60                80               100

Epoch

0                 20                40                60                80               100

Epoch

Figure 8:  Mean of the max teacher correlation œÅmean  with student nodes over epochs in CIFAR10.  
More
over-realization gives better student specialization across all layers and achieves strong 
generalization (higher
evaluation accuracy on CIFAR-10 evaluation set).

10


Under review as a conference paper at ICLR 2020

REFERENCES

Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017.

Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.  Learning and generalization in overparameter-
ized neural networks, going beyond two layers. NeurIPS, 2019a.

Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.  A convergence theory for deep learning via over-
parameterization.  In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the
36th  International  Conference  on  Machine  Learning,  volume  97  of  Proceedings  of  Machine
Learning Research, pp. 242‚Äì252, Long Beach, California, USA, 09‚Äì15 Jun 2019b. PMLR. URL
http://proceedings.mlr.press/v97/allen-zhu19a.html.

Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu.   A convergence analysis of gradient
descent for deep linear neural networks. In ICLR, 2019. URL https://openreview.net/
forum?id=SkMQg3C5K7.

Benjamin Aubin, Antoine Maillard, Florent Krzakala, Nicolas Macris, Lenka Zdeborova¬¥, et al. The
committee machine: Computational to statistical gaps in learning a two-layers neural network. In
Advances in Neural Information Processing Systems, pp. 3223‚Äì3234, 2018.

Raef Bassily, Mikhail Belkin, and Siyuan Ma.  On exponential convergence of sgd in non-convex
over-parametrized learning. arXiv preprint arXiv:1811.02564, 2018.

Lenaic  Chizat  and  Francis  Bach.     On  the  global  convergence  of  gradient  descent  for  
over-
parameterized models using optimal transport.   In Advances in neural information processing
systems, pp. 3036‚Äì3046, 2018.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.  Bert:  Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh.  Gradient descent
learns one-hidden-layer cnn: Don‚Äôt be afraid of spurious local minima. ICML, 2018a.

Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.  Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018b.

Andreas Engel and Christian Van den Broeck.  Statistical mechanics of learning.  Cambridge Uni-
versity Press, 2001.

Jason AS Freeman and David Saad.   Online learning in radial basis function networks.   Neural
Computation, 9(7):1601‚Äì1622, 1997.

E Gardner and B Derrida.   Three unfinished works on the optimal storage capacity of networks.
Journal  of  Physics  A:  Mathematical  and  General,  22(12):1983‚Äì1994,  jun  1989.    doi:   10.
1088/0305-4470/22/12/004.  URL https://doi.org/10.1088%2F0305-4470%2F22%

2F12%2F004.

Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. arXiv preprint arXiv:1711.00501, 2017.

Sebastian Goldt, Madhu S Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborova¬¥.  Dy-
namics of stochastic gradient descent for two-layer neural networks in the teacher-student setup.
NeurIPS, 2019.

Babak Hassibi, David G Stork, and Gregory J Wolff.  Optimal brain surgeon and general network
pruning. In IEEE international conference on neural networks, pp. 293‚Äì299. IEEE, 1993.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition.  In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770‚Äì778, 2016.

Kurt Hornik, Maxwell Stinchcombe, and Halbert White.  Multilayer feedforward networks are uni-
versal approximators. Neural networks, 2(5):359‚Äì366, 1989.

11


Under review as a conference paper at ICLR 2020

Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang.   Network trimming:  A data-driven
neuron pruning approach towards efficient deep architectures.  arXiv preprint arXiv:1607.03250,
2016.

Arthur Jacot, Franck Gabriel, and Cle¬¥ment Hongler.  Neural tangent kernel: Convergence and gen-
eralization in neural networks.  In Advances in neural information processing systems, pp. 8571‚Äì
8580, 2018.

Kenji Kawaguchi.  Deep learning without poor local minima.  In Advances in neural information
processing systems, pp. 586‚Äì594, 2016.

Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and transfer
learning in deep linear networks. ICLR, 2019.

Thomas Laurent and James Brecht.  Deep linear networks with arbitrary loss: All local minima are
global. In International Conference on Machine Learning, pp. 2908‚Äì2913, 2018.

Thomas Laurent and James von Brecht.  The multilinear structure of relu networks.  arXiv preprint
arXiv:1712.10132, 2017.

Yann LeCun,  John S Denker,  and Sara A Solla.   Optimal brain damage.   In Advances in neural
information processing systems, pp. 598‚Äì605, 1990.

Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension
of objective landscapes. ICLR, 2018.

Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John E Hopcroft.  Convergent learning: Do
different neural networks learn the same representations?  In ICLR, 2016.

Yuanzhi Li and Yingyu Liang.  Learning overparameterized neural networks via stochastic gradient
descent on structured data.  In Advances in Neural Information Processing Systems, pp. 8157‚Äì
8166, 2018.

Chaoyue Liu and Mikhail Belkin.  Mass:  an accelerated stochastic method for over-parametrized
learning. arXiv preprint arXiv:1810.13395, 2018.

Roi Livni,  Shai Shalev-Shwartz,  and Ohad Shamir.   On the computational efficiency of training
neural networks. In Advances in neural information processing systems, pp. 855‚Äì863, 2014.

Siyuan  Ma,  Raef  Bassily,  and  Mikhail  Belkin.   The  power  of  interpolation:  Understanding  
the
effectiveness of sgd in modern over-parametrized learning.   arXiv preprint arXiv:1712.06559,
2017.

CWH Mace and ACC Coolen.  Statistical mechanical analysis of the dynamics of learning in per-
ceptrons. Statistics and Computing, 8(1):55‚Äì88, 1998.

Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665‚ÄìE7671,
2018.

Quynh Nguyen and Matthias Hein.   The loss surface of deep and wide neural networks.   In Pro-
ceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 2603‚Äì2612.
JMLR. org, 2017.

Aaditya Prakash, James Storer, Dinei Florencio, and Cha Zhang. Repr: Improved training of convo-
lutional filters.  In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition, pp. 10666‚Äì10675, 2019.

David Saad and Sara A Solla.  On-line learning in soft committee machines.  Physical Review E, 52
(4):4225, 1995.

David Saad and Sara A Solla.  Dynamics of on-line gradient descent learning for multilayer neural
networks. In Advances in neural information processing systems, pp. 302‚Äì308, 1996.

12


Under review as a conference paper at ICLR 2020

Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks.

arXiv preprint arXiv:1712.08968, 2017.

Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al.  Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.

Yuandong Tian, Tina Jiang, Qucheng Gong, and Ari Morcos. Luck matters: Understanding training
dynamics of deep relu networks. arXiv preprint arXiv:1905.13405, 2019.

Chulhee  Yun,  Suvrit  Sra,  and  Ali  Jadbabaie.   Global  optimality  conditions  for  deep  
neural  net-
works.    In  International  Conference  on  Learning  Representations,  2018.    URL  https://
openreview.net/forum?id=BJk7Gf-CZ.

Chulhee Yun, Suvrit Sra, and Ali Jadbabaie.  Small nonlinearities in activation functions create bad
local minima in neural networks. In International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=rke_YiRct7.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.  Understanding
deep learning requires rethinking generalization. ICLR, 2017.

13


Under review as a conference paper at ICLR 2020

8    APPENDIX

8.1    LEMMA 1

Proof.  We prove by induction.  When l  =  L we know that gL(x)  =  fL‚àó (x)     fL(x), by setting
VL‚àó(x) = VL(x) = IC  C and the fact that DL(x) = IC  C (no ReLU gating in the last layer), the
condition holds.

Now suppose for layer l, we have:

gl(x)    =    Dl(x) [Al(x)fl‚àó(x) ‚àí Bl(x)fl(x)]                                         (10)

=    Dl(x)V ;(x) [Vl‚àó(x)fl‚àó(x) ‚àí Vl(x)fl(x)]                              (11)


Using

fl(x)    =    Dl(x)W ;fl‚àí‚ÇÅ(x)                                               (12)

fl‚àó(x)    =    Dl‚àó(x)W ‚àó;fl‚àó    (x)                                             (13)

l       ‚àí1


we have:

gl‚àí‚ÇÅ(x)    =    Dl‚àí‚ÇÅ(x)Wlgl(x)                                                (14)


gl‚àí‚ÇÅ(x)    =    Dl‚àí‚ÇÅ(x)Wlgl(x)     ;

(15)

=    Dl‚àí‚ÇÅ(x) WlDl(x)Vl  (x) [Vl‚àó(x)fl‚àó(x) ‚àí Vl(x)fl(x)]                                   (16)

V ;   (x)

l‚àí1

=    Dl‚àí‚ÇÅ(x)V ;  (x) Ô£ØÔ£ÆVl‚àó(x)Dl‚àó(x)W ‚àó; fl‚àó  1(x) ‚àí Vl(x)Dl(x)W ; fl‚àí‚ÇÅ(x)Ô£∫Ô£π (17)

		

l‚àí1         Ô£ØÔ£∞`         Àõ¬∏         x               `        Àõ¬∏        x           Ô£∫Ô£ª

Vl‚àó 1 (x)                                                   Vl‚àí‚ÇÅ(x)

=    Dl‚àí‚ÇÅ(x)V ;  (x) Œ£Vl‚àó 1(x)fl‚àó  1(x) ‚àí Vl‚àí‚ÇÅ(x)fl‚àí‚ÇÅ(x)Œ£                            (18)

=    Dl‚àí‚ÇÅ(x) Œ£Al‚àí‚ÇÅ(x)fl‚àó  1(x) ‚àí Bl‚àí‚ÇÅ(x)fl‚àí‚ÇÅ(x)Œ£                                     (19)

8.2    THEOREM 1

Proof.  By definition of SGD critical point, we know that for any batch Bj, Eqn. 1 vanishes:


WÀô l  = E‚Çì Œ£gl(x; WÀÜ )f ;

(x;  ÀÜ)   =         g (x  ;  ÀÜ)f ;

l‚àí1

i‚ààBj

(xi; WÀÜ ) =         Ui = 0          (20)

i‚ààBj

where  Ui  =  gl(xi; WÀÜ )f ;  (xi; WÀÜ ).   Note  that  Bj  can  be  any  subset  of  samples  from  
the  data

distribution. Therefore, for a dataset of size N , Eqn. 20 holds for all . N Œ£ batches, but there 
are only

  

Plug that into Eqn. 20 we know U  = 0 and thus for any i, Ui = 0. Since Ui is an outer product, the

theorem follows.

Note that if «ÅWÀô l«Å‚àû ‚â§ œµ, which is «Å Œ£i‚ààB   Ui«Å‚àû ‚â§ œµ, then with simple Gaussian elimination for

two batchesŒ£B‚ÇÅ  and B‚ÇÇ  wŒ£ith only two samŒ£ple difference,  weŒ£will have for any i‚ÇÅ  /=  i‚ÇÇ,  «ÅUi1  
‚àí

in and we have |B|«ÅUi«Å‚àû ‚â§ [2(|B| ‚àí 1) + 1]œµ, which is «ÅUi«Å‚àû ‚â§ 2œµ.  If fl‚àí‚ÇÅ(x; WÀÜ ) has the bias
term, then immediately we have «Ågl(x; WÀÜ )«Å‚àû ‚â§ œµ.

14


Under review as a conference paper at ICLR 2020

(a)                                             (b)                                                 
           (c)

Figure 9: Proof illustration for (a) Lemma 2, (b) Lemma 3 and (c) Theorem 4.

8.3    COROLLARY 1

Proof.  The base case is that VL(x)  =  VL‚àó(x)  =  IC√óC,  which is constant (and thus piece-wise
constant) over the entire input space.  If for layer l, Vl(x) and Vl‚àó(x) are piece-wise constant, 
then
by Eqn. 4 (rewrite it here):


Vl‚àí‚ÇÅ(x) = Vl(x)Dl(x)W ;,     Vl‚àó 1(x) = Vl‚àó(x)Dl‚àó(x)W ‚àó;

(21)

l             ‚àí    ;                                 l

since Dl(x) and Dl‚àó(x) are piece-wise constant and Wl   and Wl‚àó; are constant, we know that for
layer l ‚àí 1, Vl‚àí‚ÇÅ(x) and Vl‚àó 1(x) are piece-wise constant. Therefore, for all l = 1, . . . L, Vl(x) 
and
V   (x)

Therefore,  Al(x) and Bl(x) are piece-wise constant with respect to input x.   They separate the
region R‚ÇÄ into constant regions with boundary points in a zero-measured set.

8.4    LEMMA 2

Lemma 2.  Consider K ReLU activation functions f  (x)  =  œÉ(w;x) for j  =  1 . . . K.  If w    =  0

and no two weights are co-linear, then Œ£j' cj' fj' (x) = 0 for all x ‚àà Rd+1  suggests that all cj = 
0.

so that w;x‚ÇÄ  =  0 but all w; x‚ÇÄ  /=  0 for j‚Ä≤  /=  j,  which is possible due to the distinct 
weight

conditions.  Consider an œµ-ball B‚Çì0 ,s  =  {x  :  «Åx ‚àí x‚ÇÄ«Å ‚â§ œµ}.  We pick œµ so that sign(wj' x) for

all j‚Ä≤     j remains the same within B‚Çì ,s (Fig. 9(a)). Denote [j‚Å∫] as the indices of activated 
ReLU


functions in B‚Çì0 ,s except j.

Then for all x ‚àà B‚Çì0 ,s ‚à© Ej, we have:

h(x) ‚â° Œ£ c ' f ' (x) = c  w;x +   Œ£

c ' w   x = 0                             (22)

Since B‚Çì0 ,s  is a d-dimensional object rather than a subspace, for x‚ÇÄ and x‚ÇÄ + œµek     B(x‚ÇÄ, œµ), we
have


h(x‚ÇÄ + œµek) ‚àí h(x‚ÇÄ) = œµ(cjwjk +

j'‚àà[j+]

where ek is axis-aligned unit vector (1 ‚â§ k ‚â§ d). This yields

cj' wj'k ) = 0                            (23)


Plug it back to Eqn. 22 yields

cjwÀú j +

j'‚àà[j+]

cjbj +   Œ£

cj' wÀú j'  = 0d                                          (24)

cj' bj'  = 0                                                     (25)

j'‚àà[j+]

where means that for the (augmented) d + 1 dimensional weight:


cjwj +

j'‚àà[j+]

cj' wj'  = 0d‚Çä‚ÇÅ                                                  (26)

15


Under review as a conference paper at ICLR 2020


However, if we pick x‚Ä≤  =  x

   wÀú j     

‚à© EC, then f  (x‚Ä≤)  =  0 but Œ£

f ‚Ä≤(x‚Ä≤)  =


0

‚àícjw;x‚Ä≤ = œµcj and thus

«ÅwÀú j «Å2

x‚ÇÄ,s          j                j

j'‚àà[j+]    j


which is a contradiction.

cj' fj' (x‚Ä≤) = œµcj /= 0                                                    (27)

j'

8.5    LEMMA 3

Lemma 3 (Local ReLU Independence).  Let R be an open set.  Consider K ReLU nodes fj(x) =

œÉ(w;x), j = 1, . . . , K. wj /= 0, wj /= Œ≥wj'  for j /= j‚Ä≤ with any Œ≥ > 0.

If there exists c‚ÇÅ, . . . , cK, c‚Ä¢  so that the following is true:

Œ£ cjfj(x) + c‚Ä¢ w;x = 0,     ‚àÄx ‚àà R                                          (28)

j

and for node j, ‚àÇEj ‚à© R /= √ò, then cj = 0.

Proof.  We can apply the same logic as Lemma 2 to the region R (Fig. 9(b)). For any node j, since
its boundary ‚àÇEj is in R, we can find a similar x‚ÇÄ so that x‚ÇÄ      ‚àÇEj    R and x‚ÇÄ  /  ‚àÇEj'  for 
any

j‚Ä≤ = j.  We construct B‚Çì0 ,s.  Since R is an open set, we can always find œµ > 0 so that B‚Çì0 ,s      
 R

and no other boundary is in this œµ-ball. Following similar logic of Lemma 2, cj = 0.

8.6    LEMMA 4

Lemma 4 (Relation between Hyperplanes).  Let wj and wj'  two distinct hyperplanes with «ÅwÀú j«Å =
«ÅwÀú j' «Å  =  1.   Denote  Œ∏jj'  as  the  angle  between  the  two  vectors  wj  and  wj' .   Then  
there  exists
uÀúj'  ‚ä• wÀú j and wj' uÀúj'  = sin Œ∏jj' .

Proof.  Note that the projection of wÀú j'  onto wÀú j is:


1

uÀúj'  =

sin Œ∏jj'

PwÀú‚ä• wÀú

j'                                                                 (29)

It is easy to verify that «ÅuÀúj' «Å = 1 and wj' uÀúj'  = sin Œ∏jj' .

8.7    LEMMA 5

Lemma 5 (Evidence of Data points on Misalignment).  Let R      Rd  be an open set.  Consider K
ReLU nodes fj(x) = œÉ(w;x), j  = 1, . . . , K.  «ÅwÀú j«Å = 1, wj are not co-linear.  Then for a node j
with ‚àÇEj ‚à© R /= √ò, and œµ ‚â§ œµ‚ÇÄ, either of the conditions holds:


(1)  There exists node j‚Ä≤

j so that sin Œ∏jj'  ‚â§ MKœµ¬π‚àíŒ¥/|cj| and |bj'  ‚àí bj| ‚â§ M‚ÇÇœµ¬π‚àí¬≤Œ¥/|cj|.


(2)  There exists x   ‚àà ‚àÇE

‚à© R so that for any j‚Ä≤ /= j, |w; x  | > 5œµ/|c  |.

where Œ∏jj'  is the angle between wÀú.j  and wÀú j' , Œ¥  >  0, r is the radius of a d ‚àí 1 dimensional 
ball

Proof.  Define q   = 5œµ/|c  |. For each j‚Ä≤ /= j, define I '  = {x : |w; x| ‚â§ q  ,  x ‚àà ‚àÇE  }. We 
prove

by contradiction.  Suppose for any j‚Ä≤ = j, sin Œ∏jj'  > KMœµ¬π‚àíŒ¥/ cj  or  bj'      bj  > M‚ÇÇœµ¬π‚àí¬≤Œ¥/ cj .
Otherwise the theorem already holds.

Case 1. When sin Œ∏jj'  > KMœµ¬π‚àíŒ¥/|cj| holds.

From Lemma 4, we know that for any x  ‚àà ‚àÇE  , if w; x  = ‚àíq  , with a '  ‚â§ 2q  |c  |/MKœµ¬π‚àíŒ¥ =


j      ;   j'

j               j             j    j

10œµŒ¥/MK, we have x‚Ä≤ = x + aj' uj'  ‚àà ‚àÇEj and wj' x‚Ä≤ = +qj.

16


Under review as a conference paper at ICLR 2020

Consider a d ‚àí 1-dimensional sphere B  ‚äÜ ‚Ñ¶j and its intersection of Ij'  ‚à© B for j‚Ä≤ /= j.  Suppose
the sphere has radius r. For each Ij'  ‚à© B, its d ‚àí 1-dimensional volume is upper bounded by:

Œ¥   10

V (Ij'  ‚à© B) ‚â§ aj' Vd‚àí‚ÇÇ(r) ‚â§ œµ  MK Vd‚àí‚ÇÇ(r)                                    (30)

where Vd‚àí‚ÇÇ(r) is the d ‚àí 2-dimensional volume of a sphere of radius r. Intuitively, the 
intersection


;

j'

at most aj' .

j and B is at most a d ‚àí 2-dimensional sphere of radius r, and the ‚Äúheight‚Äù is

Case 2. When sin Œ∏jj'  ‚â§ KMœµ¬π‚àíŒ¥/|cj| but |bj'  ‚àí bj| > M‚ÇÇœµ¬π‚àí¬≤Œ¥/|cj| holds.

In this case, we want to show that for any x ;‚àà ‚Ñ¶j, |wj' x| > qj and thus Ij'  ‚à© B = √ò. If this is 
not
the case, then there exists x ‚àà ‚Ñ¶j so that |wj' x| ‚â§ qj. Then since x ‚àà ‚àÇEj, we have:


|w; x| = |(w  '  ‚àí w  );x| = |(wÀú

'  ‚àí wÀú

);xÀú + (b‚Ä≤j ‚àí b  )| ‚â§ q

(31)

Therefore, from Cauchy inequality and triangle inequality, we have:


«ÅwÀú

'  ‚àí wÀú

«Å«ÅxÀú«Å ‚â• |(wÀú

'  ‚àí wÀú

);xÀú| ‚â• |b‚Ä≤j ‚àí b  | ‚àí |w; x|                     (32)

Œ∏jj'                                             1‚àíŒ¥

From the condition, we have «ÅwÀú j'  ‚àí wÀú  «Å = 2 sin        ‚â§ 2 sin Œ∏   '  ‚â§ 2KMœµ      /|cj|. Then

2M‚ÇÄMKœµ¬π‚àíŒ¥/|cj| ‚â• |(wÀú j'  ‚àí wÀú j);xÀú| ‚â• |bj'  ‚àí bj| ‚àí qj > M‚ÇÇœµ¬π‚àí¬≤Œ¥/|cj| ‚àí 5œµ/|cj|       (33)
which is equivalent to:


which means that

2M‚ÇÄMKœµŒ¥ > M‚ÇÇ ‚àí 5œµ¬≤Œ¥                                           (34)


M‚ÇÇ < 2M‚ÇÄMKœµŒ¥ + 5œµ¬≤Œ¥ ‚â§ 2M‚ÇÄMKœµŒ¥ + 5œµ¬≤Œ¥

(35)

for œµ ‚â§ œµ‚ÇÄ. This is a contradiction. Therefore, Ij'  ‚à© B = √ò and thus V (Ij'  ‚à© B) = 0.


d                        M   0

V (B) =  10 œµŒ¥V

(r) >          Œ£

V (I '  ‚à© B)                               (36)


This means that there exists xj ‚àà B ‚äÜ ‚Ñ¶j so that xj ‚àà/ Ij' ‚à© B for any j‚Ä≤

j and j‚Ä≤ in case 1. That


is,

|w; x  | > q                                                                (37)

On the other hand, for j‚Ä≤ in case 2, the above condition holds for entire ‚Ñ¶j, and thus hold for the
chosen xj.

8.8    LEMMA 6

Lemma 6 (Local ReLU Independence, Noisy case).  Let R be an open set. Consider K ReLU nodes
fj(x) = œÉ(w;x), j = 1, . . . , K. «ÅwÀú j«Å = 1, wj are not co-linear. If there exists c‚ÇÅ, . . . , cK, 
c‚Ä¢  and
œµ  ‚â§ œµ‚ÇÄ so that the following is true:

  Œ£ cjfj(x) + c‚Ä¢ w;x  ‚â§ œµ,     ‚àÄx ‚àà R                                         (38)

j

and for a node j, ‚àÇEj ‚à© R /= √ò. Then there exists node j‚Ä≤ /= j so that sin Œ∏jj'  ‚â§ MKœµ¬π‚àíŒ¥/|cj| and

|bj'  ‚àí bj| ‚â§ M‚ÇÇœµ¬π‚àí¬≤Œ¥/|cj|, where r, Œ¥, M, M‚ÇÇ are defined in Lemma 5 but with r‚Ä≤ = r ‚àí 5œµ/|cj|.

Proof.  Let qj = 5œµ/|cj| and ‚Ñ¶j = {x : x ‚àà ‚àÇEj ‚à© R,   B(x, qj) ‚äÜ R}. If situation (1) in Lemma 5
happens then the theorem holds. Otherwise, applying Lemma 5 with R‚Ä≤ = {x : x ‚àà R,  B(x, qj) ‚äÜ
R} and there exists xj ‚àà ‚Ñ¶j so that

|w; x  | ‚â• q   = 5œµ/|c  |                                          (39)

17


Under review as a conference paper at ICLR 2020

(a)                                                                            (b)

Figure 10: (a) Lemma 5. (b) Lemma 6.

Let two points x¬±j    =  xj ¬± qjwÀú j  ‚àà  R.   In the following we show that the three points xj  and
x¬±j    are  on  the  same  side  of  ‚àÇEj'  for  any  j‚Ä≤  /=  j.   This  can  be  achieved  by  
checking  whether
(w; x  )(w; x¬±) ‚â• 0 (Fig. 10):

j'   j        j'

(w; x  )(w; x¬±)    =    (w; x  ) Œ£w; (x   ¬± q  wÀú  )Œ£                           (40)

j


=    (w; x  )¬≤ ¬± q  (w; x  )w; wÀú

(41)


=    |w; x  |(|w; x  | ¬± q  w; wÀú

)                              (42)


Since |w; wÀú  | ‚â§ 1, it is clear that (w; x  )(w; x¬±j  ) ‚â• 0. Therefore the three points x

and x¬±j   are

on the same side of ‚àÇEj'  for any j‚Ä≤ /= j.

Let h(x) = Œ£  cjfj(x) + c‚Ä¢ w;x, then |h(x)| ‚â§ œµ for x ‚àà R. Since x‚Å∫ + x‚àí = 2xj, we know that

all terms related to w‚Ä¢  and wj'  with j = j will cancel out (they are in the same side of the 
boundary

‚àÇEj' ) and thus:

4œµ ‚â• |h(x‚Å∫) + h(x‚àí) ‚àí 2h(xj)| = |cjqjw;wj| = |cj|qj = 5œµ                      (43)

j                 j                                            j

which is a contradiction.

8.9    THEOREM 2

Proof.  In this situation,  because D‚ÇÇ(x)  =  D2‚àó(x)  =  I,  according to Eqn. 4,  V‚ÇÅ(x)  =  W ; 
and

V1‚àó(x) = W1‚àó; are independent of input x. Therefore, both A‚ÇÅ and B‚ÇÅ are independent of input x.

From Assumption 1, since œÅ(x) > 0 in R‚ÇÄ, from Theorem. 1 we know that the SGD critical points
gives g‚ÇÅ(x)  =  D‚ÇÅ(x) [A‚ÇÅf1‚àó(x) ‚àí B‚ÇÅf‚ÇÅ(x)]  =  0.  Picking node k, the following holds for every


node k and every x ‚àà R‚ÇÄ ‚à© Ek:

Œ±;f

‚àó(x) ‚àí Œ≤;f (x) = 0                                                            (44)

Here Œ±; is the k-th row of A‚ÇÅ, A‚ÇÅ  = [Œ±‚ÇÅ, . . . , Œ±n ]; and similarly for Œ≤;.  Note here layer 
index

k                                                                                 1                 
                                 k

l = 1 is omitted for brevity.

For teacher j, suppose it is observed by student k, i.e., ‚àÇEj‚àó     Ek =   . Given all teacher and 
student
nodes,  note that co-linearity is a equivalent relation,  we could partition these nodes into 
disjoint
groups. Suppose node j is in group s. In Eqn. 44, if we combine all coefficients in group s together
into one term cswj‚àó (with «Åwj‚àó«Å = 1), we have:


cs = Œ±kj ‚àí

k'‚ààco-linear(j)

18

«Åwk' «ÅŒ≤kk'                                                    (45)


Under review as a conference paper at ICLR 2020

‚ÄúAt most‚Äù because from Assumption 1, all teacher weights are not co-linear. Note that co-linear(j)

might be an empty set.

By Assumption 1, ‚àÇEj‚àó ‚à© R‚ÇÄ  /= √ò and by observation property, ‚àÇEj‚àó ‚à© Ek /= √ò, we know that for
R = R‚ÇÄ ‚à© Ek, ‚àÇEj‚àó ‚à© R /= √ò. Applying Lemma 3, we know that cs = 0. Since Œ±kj /= 0, we know
co-linear(j) /= √ò and there exists at least one student k‚Ä≤ that is aligned with the teacher j.

8.10    THEOREM 3

Proof.  We  basically  apply  the  same  logic  as  in  Theorem  2.     Consider  the  colinear  
group
co-linear(k).  If for all k‚Ä≤     co-linear(k), Œ≤k'k'           vk'   2   =  0, then vk'   =  0  and 
the proof is
complete.

Otherwise, if there exists some student k so that vk = 0.  By the condition, it is observed by some
student node k‚Çí, then with the same logic we will have


which is

k'‚ààcoŒ£-linear(k)

Œ≤ko,k' «Åwk' «Å = 0                                                (46)


;

ko

k'‚ààco-linear(k)

vk' «Åwk' «Å = 0                                               (47)

Since k is observed by C students k¬π, k¬≤, . . . , kJ , then we have:


o     o             o

v;          Œ£

v  ' «Åw  ' «Å = 0                                               (48)


By the condition, all the C vectors v;

o

‚àà RC  are linear independent, then we know that


k'‚ààcoŒ£-linear(k)

vk' «Åwk' «Å = 0                                                        (49)

8.11    COROLLARY 2

Proof.  We can write the contribution of all student nodes which are not aligned with any teacher
nodes as follows:


Œ£s   k‚ààco-Œ£linear(s)

vkfk(x)    =

s

k‚ààco-Œ£linear(s)

vk«Åwk«ÅœÉ(ws‚Ä≤ ;x)                   (50)


=    Œ£ œÉ(ws‚Ä≤ ;x)        Œ£

vk«Åwk«Å               (51)

where ws‚Ä≤  is the unit vector that represents the common direction of the co-linear group s.  From

Theorem 3, for group s that is not aligned with any teacher,     k   co-linear(s) vk  wk   = 0 and 
thus
the net contribution is zero.

8.12    THEOREM 4

Proof.  In multi-layer case, Al(x) and Bl(x) are no longer constant over input x. Fortunately, 
thanks
to the recursive definition (Eqn. 4) which only contains input-independent terms (weights) and 
gating
function, Al(x) and Bl(x) are piece-wise constant function over the input R‚ÇÄ.

Note that R‚ÇÄ can be partitioned into R = {R¬π, R¬≤, . . . , RJ } and a zero-measure set. Each of them

is constant region for Al(x) and Bl(x). Since R ≤ is an intersection of finite open half-planes 
(from

k‚Äôs parent nodes), R ≤ is still an open set.

From the condition, there exists open set R ‚àà R and a student observer node k so that ‚àÇEj‚àó ‚à© Ek ‚à©

R      √ò ((Fig. 9(c)).  Let HR and similarly HR‚àó  be the student and teacher nodes whose boundary

19


Under review as a conference paper at ICLR 2020

intersects with R.  Therefore j      H‚àó .  For other teacher/student nodes, they are linear 
functions

within R and thus can be combined together into w;x. For all weights in HR, HR‚àó  and w‚Ä¢, applying

Lemma 3 on R ‚à© E  , we know that the SGD criti‚Ä¢cal point Œ±;   f ‚àó(x) ‚àí Œ≤;   f  (x)  =  0 leads to

alignment between HR and HR‚àó . Let group s be the one that contains all weights that are co-linear 
to
teacher node j (note that no other teacher nodes are involved), and cs its coefficient. Since j     
HR‚àó ,
cs = 0. Since Œ±kj(R) = 0, there exists at least one student node k‚Ä≤ that is co-linear to teacher 
node
j.

8.13    THEOREM 5

Proof.  We follow the logic of Theorem 4. Instead of applying Lemma 3, for gradient that is not zero
but bounded within œµ, we pick the student observer k and we have for Ek ‚à© R:

|Œ±;f ‚àó(x) ‚àí Œ≤;f (x)| ‚â§ œµ,                                                    (52)

we use Lemma 6 and know that there exists a node k‚Ä≤     j  so that sin Œ∏k'j  =  O .œµ¬π‚àíŒ¥/|cj|Œ£ and

bk'       b‚àój    =        œµ¬π‚àí¬≤Œ¥/ cj    for any Œ¥  >  0.   Under the observation of student k,  the 
teacher j
has coefficient cj = Œ±kj.  Since all teacher weights are distant to each other with positive 
constant
b‚ÇÄ > 0 and Œ∏‚ÇÄ > 0, with sufficiently small œµ‚ÇÄ and œµ      œµ‚ÇÄ, this node k‚Ä≤ has to be a student node 
and
the bound follows.

8.14    THEOREM 6

Proof.  From the expression we can see that it is positive homogeneous with respect to «Åwj‚àó«Å and
wk  . So we can assume   wj‚àó    =   wk   = 1. Without loss of generality, we set up the coordinate
system so that wj‚àó = [1, 0]; and wk = [cos Œ∏, sin Œ∏];. Then


Ex Œ£fl‚àí1zk fj‚àóŒ£   =    Ex Œ£fl‚àí1zk zj‚àóf ;

Œ£ wj‚àó =             Œ£

fl‚àí1f ;

wj‚àó

(53)


+‚àû

=              r¬≤p(r)

0

œÄ

2

‚àí ‚ÇÇ +Œ∏

cos Œ∏‚Ä≤

sin Œ∏‚Ä≤

Œ£ cos Œ∏‚Ä≤p(Œ∏‚Ä≤|r)dŒ∏‚Ä≤ + s         (54)

where s is the term reflecting the asymmetry of the data distribution p(fl‚àí‚ÇÅ) with respect to the 
plane
spanned by the vectors wk and wj‚àó.

If the data distribution p(fl‚àí‚ÇÅ) is scale invariant (rescaling the data point won‚Äôt change the 
angular
distribution), then p(Œ∏‚Ä≤|r) = p(Œ∏‚Ä≤) and we only need to check the angular integral:


I(Œ∏) =

œÄ

2

‚àí ‚ÇÇ +Œ∏

cos Œ∏‚Ä≤

sin Œ∏‚Ä≤

Œ£ cos Œ∏‚Ä≤p(Œ∏‚Ä≤)dŒ∏‚Ä≤                                (55)

Note that cos¬≤ Œ∏ =  ¬π (1 + cos 2Œ∏) and sin Œ∏ cos Œ∏ =  ¬π sin 2Œ∏, so we have:


2

2I(Œ∏)    =

œÄ

2

‚àí ‚ÇÇ +Œ∏

p(Œ∏‚Ä≤)dŒ∏‚Ä≤Œ£

wj‚àó +

2

œÄ

2

‚àí ‚ÇÇ +Œ∏

cos 2Œ∏‚Ä≤
sin 2Œ∏‚Ä≤

Œ£ p(Œ∏‚Ä≤)dŒ∏‚Ä≤                    (56)


.‚à´  œÄ

p(Œ∏‚Ä≤)dŒ∏‚Ä≤Œ£

wj‚àó +

1     2œÄ      cos Œ∏‚Ä≤‚Ä≤

sin Œ∏‚Ä≤‚Ä≤

Œ£ p . Œ∏‚Ä≤‚Ä≤

‚àí œÄ Œ£

dŒ∏‚Ä≤‚Ä≤         (57)


‚àí ‚ÇÇ +Œ∏

2   2Œ∏                                   2       2


1

I  (Œ∏)w   +    I

1

‚àí   I  (Œ∏)                                                                        (58)

20


Under review as a conference paper at ICLR 2020


where Œ∏‚Ä≤‚Ä≤ = 2Œ∏‚Ä≤ + œÄ and

I‚ÇÅ(Œ∏)    =

œÄ

2

‚àí ‚ÇÇ +Œ∏

p(Œ∏‚Ä≤)dŒ∏‚Ä≤                                                                  (59)


‚à´ 2œÄ Œ£  cos Œ∏‚Ä≤‚Ä≤  Œ£

. Œ∏‚Ä≤‚Ä≤

œÄ Œ£    ‚Ä≤‚Ä≤


‚à´ 2Œ∏ Œ£  cos Œ∏‚Ä≤‚Ä≤  Œ£

. Œ∏‚Ä≤‚Ä≤

œÄ Œ£    ‚Ä≤‚Ä≤


I‚ÇÇ(Œ∏)    =

.‚à´ Œ∏ Œ£

sin Œ∏‚Ä≤‚Ä≤

. Œ∏‚Ä≤

p     2  ‚àí 2    dŒ∏

œÄ Œ£      .      Œ∏‚Ä≤

œÄ Œ£Œ£

‚Ä≤     ‚Ä≤Œ£


.‚à´ Œ∏ Œ£

. Œ∏‚Ä≤     œÄ Œ£

.      Œ∏‚Ä≤

œÄ Œ£Œ£

‚Ä≤     ‚Ä≤Œ£   ‚ä•

where wk‚ä•  is the unit vector that is perpendicular to wk but still in the plane spanned by wk and
wj‚àó.  Note I‚ÇÄ is the fixed integral of unit vectors weighted by angular distribution of input data 
on
activated half-plane Ej‚àó of teacher node j.

If  p(fl‚àí‚ÇÅ)  is  rotational  symmetric,  then  s =  0,  p(Œ∏‚Ä≤)  =    ¬π  ,  then  we  can  compute  
these  terms
analytically: I‚ÇÄ = 0, I‚ÇÅ(Œ∏) =   ¬π  (œÄ ‚àí Œ∏) and I‚ÇÇ(Œ∏) =  ¬π sin Œ∏wk.


2œÄ                                          œÄ

8.15    THEOREM 7

Proof.  Note that we have:

                                ;

«Åw  «Å =          «Åw  «Å2  =       ·µè      =           w;wÀô

= w;r

(62)


dt      ·µè       dt          ·µè

Therefore, we have

2«Åwk«Å

«Åwk«Å


 d  ln «Åw  «Å = w¬Ø ;r

(63)


and

dt          k             k   k


 d            w                 d

ln                =      (ln «Åw  «Å ‚àí ln «Åw  ' «Å) = w¬Ø   r

‚àí w¬Ø    r  '                          (64)


dt

Note that we have:

 d

«Åwk' «Å       dt

 d  .   wk   Œ£

k                    k

w;rk

k   k          k'   k

;              ‚ä•


dt w¬Ø k  =  dt

«Åwk«Å

= rk ‚àí wk

k

«Åwk«Å2

= (I ‚àí w¬Ø kw¬Ø k )rk = Pwk rk             (65)

Let hk = w¬Ø ;rk. We assume all hk > 0 (positive correlation), then we have:


 d h   = r;P ‚ä• r

+ w¬Ø ;rÀô

= «År  «Å2  ‚àí h¬≤ + w¬Ø ;rÀô

(66)

If rk = r = w‚àó ‚àí Œ£k akwk, then we have:


 d

h   = «År«Å2  ‚àí h¬≤ ‚àí Sh

(67)

where S = (Œ£k ak«Åwk«Å) > 0 is independent of k. So

		

dt (hk ‚àí hk' ) = (hk'  ‚àí hk) + S(hk'  ‚àí hk) = (hk'  ‚àí hk)(hk'  + hk + S)             (68)

if hk ‚àí hk'  > 0, then   ·µà (hk ‚àí hk' ) < 0 and vice versa.  This means that Eqn. 64 is zero when the
system enters the stable region.  On the other hand, if   rk  ¬≤  =    rk'   2  + œµ (e.g., rk has 
stronger
teacher component), then we have:

d

dt (hk ‚àí hk' ) = (hk'  ‚àí hk)(hk'  + hk + S) + œµ                                  (69)

which is only zero when hk > hk' . This yields exponential growth of «Åwk«Å compared to «Åwk' «Å.

21


Under review as a conference paper at ICLR 2020


p = 0

0

          1x

p = 0.5

p = 1

1                                                                                2x

2                                                                                5x

          10x

3

4

5

6

7


8

0              20             40             60             80            100

p = 1.5

0

0              20             40             60             80            100

p = 2

0              20             40             60             80            100

p = 2.5

1

2

3

4

5

6

7


8

0              20             40             60             80            100

0              20             40             60             80            100

Iteration

0              20             40             60             80            100

Iteration

Figure 11: Evaluation loss convergence curve.

8.16    DETAILS IN TEACHER/STUDENT CONSTRUCTION AND TRAINING

We construct teacher networks in the following manner.  For two-layered network, the output di-
mension C  =  50 and input dimension d  =  m‚ÇÄ  =  n‚ÇÄ  =  100.  For multi-layered network, we use
50-75-100-125 (i.e, m‚ÇÅ  =  50, m‚ÇÇ  =  75, m‚ÇÉ  =  100, m‚ÇÑ  =  125, L  =  5, d  =  m‚ÇÄ  =  n‚ÇÄ  =  100

and C  =  m‚ÇÖ  =  n‚ÇÖ  =  50).  The teacher network is constructed to satisfy Assumption 1:  at each
layer, teacher filters are distinct from each other and their bias is set so that     50% of the 
input data
activate the nodes. This makes their boundary (maximally) visible in the dataset.

To train the model, we use vanilla SGD with learning rate 0.01 and batchsize 16.

8.17    ADDITIONAL FIGURES

Fig. 11 shows how the loss changes over iterations. With high teacher polarity (Eqn. 9), it becomes
harder to learn the weak teacher nodes and over-realization helps in getting low evaluation loss (in
particular for p = 2.5).

Besides Gaussian distribution we also test on uniform distribution x       U [   15, 15].  For 
training,
we sample 100k data points in each epoch. Fig. 12 shows that the results on 4 layer ReLU network
(50-75-100-125) are similar.  Note that in multi-layer setting, Theorem 3 might not hold since it is
for         2-layer so there could be un-specialized student nodes with large Œ≤kk(x).

22


Under review as a conference paper at ICLR 2020


1.00

Epoch 0

Epoch 25

Epoch 50

Epoch 75

Epoch 100

0.75

0.50

0.25


0.00

1.00

0.0       0.2       0.4       0.6       0.8       1.0

0.0       0.2       0.4       0.6       0.8       1.0

0.0       0.2       0.4       0.6       0.8       1.0

0.0       0.2       0.4       0.6       0.8       1.0

0.0       0.2       0.4       0.6       0.8       1.0

0.75

0.50

0.25


0.00

1.00

0.0       0.2       0.4       0.6       0.8       1.0

0.0       0.2       0.4       0.6       0.8       1.0              0.0       0.2       0.4       
0.6       0.8       1.0              0.0       0.2       0.4       0.6       0.8       1.0          
    0.0       0.2       0.4       0.6       0.8       1.0

0.75

0.50

0.25


0.00

1.00

0.0       0.2       0.4       0.6       0.8       1.0

0.0       0.2       0.4       0.6       0.8       1.0              0.0       0.2       0.4       
0.6       0.8       1.0              0.0       0.2       0.4       0.6       0.8       1.0          
    0.0       0.2       0.4       0.6       0.8       1.0

0.75

0.50

0.25


0.00

0.0       0.2       0.4       0.6       0.8       1.0

Max correlation among teacher

0.0       0.2       0.4       0.6       0.8       1.0

Max correlation among teacher

0.0       0.2       0.4       0.6       0.8       1.0

Max correlation among teacher

0.0       0.2       0.4       0.6       0.8       1.0

Max correlation among teacher

0.0       0.2       0.4       0.6       0.8       1.0

Max correlation among teacher

Figure 12: Strength of student specialization for 4 layer network (50-75-100-125) when each entry
of the input dimension is uniform distributed in U [‚àí15, 15].  For all teacher nodes, the normalized
correlations are all close to 1.0 (œÅm‚Çë‚Çên ‚â• 0.998 at all layers).

23

