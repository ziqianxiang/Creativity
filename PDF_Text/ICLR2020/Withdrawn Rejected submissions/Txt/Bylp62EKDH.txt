Under review as a conference paper at ICLR 2020
Extreme Triplet Learning: Effectively Opti-
mizing Easy Positives and Hard Negatives
Anonymous authors
Paper under double-blind review
Ab stract
The Triplet Loss approach to Distance Metric Learning is defined by the strategy
to select triplets and the loss function through which those triplets are optimized.
During optimization, two especially important cases are easy positive and hard
negative mining which consider, the closest example of the same and different
classes. We characterize how triplets behave based during optimization as a func-
tion of these similarities, and highlight that these important cases have technical
problems where standard gradient descent behaves poorly, pulling the negative
example closer and/or pushing the positive example farther away. We derive an
updated loss function that fixes these problems and shows improvements to the
state of the art for CUB, CAR, SOP, In-Shop Clothes datasets.
1	Introduction
Deep metric learning optimizes an embedding function that maps semantically similar images to
relatively nearby locations and maps semantically dissimilar images to distant locations. A number
of approaches have been proposed for this problem (Schroff et al., 2015a; Sohn, 2016; Movshovitz-
Attias et al., 2017; Song et al., 2016; Xuan et al., 2018; Kim et al., 2018; Ge, 2018). A common
way to learn the mapping is to define a loss function based on triplets of images: an anchor image, a
positive image from the same class, and a negative image from a different class. The loss penalizes
cases where the anchor is mapped closer to the negative image than it is to the positive image.
One common variant of these functions (e.g. (Wang et al., 2018; Sohn, 2016)), uses a Deep Learning
framework to map images to a feature vector, and computes similarity between normalized feature
vectors based on the dot-product. This approach forces the features to lie on a hypersphere and has
advantages of making the feature comparison intuitive and efficient.
In this work we explore standard implementations of these loss functions and show there are two
problems. First, when the gradient of the loss function does not consider the normalization to a
hypersphere, a large part of the gradient is lost when points are re-projected back to the sphere,
especially in the easy-positive/hard-negative cases of triplets including nearby points. Second, when
optimizing the parameters (the weights) of the network, when points are already mapped close
together, it may be difficult to find gradient directions that effectively separate nearby images.
We give systematic derivation showing when and where these challenging triplets arise, and di-
agram the sets of triplets where standard gradient descent makes the loss increase. We find that
this explains problems previously reported in the literature such as the difficulty in optimizing hard-
negative triplets (Harwood et al., 2017). Furthermore, these problems are mostly introduced because
the loss-function of the triplets is based on the differences between the anchor-positive and anchor-
negative distances, so there is an equivalent effect of encouraging the positive image to be closer
or the negative image to be further. We create a new loss function that breaks this symmetry and
weights the importance of changing the anchor-positive and anchor-negative distances. Briefly, our
main contributions are:
•	A systematic characterization of triplet selection strategies and a visualization that high-
lights regions of bad gradient behavior.
•	A simple modification to a standard loss function to fix bad gradient behavior.
•	Improvements to current state of the art results across a range of datasets.
1
Under review as a conference paper at ICLR 2020
Figure 1: The triplet scatter diagram plots a triplet as a point defined by the Anchor-Positive sim-
ilarity Sap and the Anchor-Negative similarity San . (left) Points below the diagonal correspond to
triplets that are ”correct” in the sense that the anchor image could get the correct label because the
same class example is closer than the different class example. Triplets along the top of the diagram
are candidates for hard-negative mining, triplets along the right edge are candidates for easy-positive
mining, and triplets mapped near but below the diagonal are candidates for semi-hard triplet mining.
(right) Two important cases are the extremes of the hardest negatives and the easiest-positives.
2	Background
There is a large body of work in distance metric learning and they are leading with two main ideas.
One of the idea is to increase the intra-class cluster density and keep the inter-class clusters as far
as possible. Pairwise loss functions like contrastive divergence (Carreira-Perpinan & Hinton, 2005;
ChoPra et al., 2005; RadenoVic et al., 2016) directly optimize for this constraint, and ”No fuss metric
learning” (Movshovitz-Attias et al., 2017), implicitly optimizes for this constraint by assigning each
class to different location and penalizes the failure of any example to go to its assigned location.
The other approach more directly reflects the fact that for image retrieVal applications, it is not
necessary for all elements of a class to be clustered, but instead that the distance to elements of the
same class should be smaller than the distance to elements of different classes. Directly optimizing
for this is based on triplets of images. The triplets are drawn from the training data and include an
anchor image, a positiVe image from the same class, and a negatiVe image from a different class.
Key questions for these approaches explore how to select the triplets that are used. Choices such as
hard or semi-hard triplet mining (Schroff et al., 2015b; Simo-Serra et al., 2015; Wang et al., 2014)
focus on triplets with negatiVe examples that are closest (hard negatiVe mining) or nearly as close
to the anchor as positiVe images (semi-hard negatiVe mining) and emphasize creating separations
between classes in the embedding space. Recent work such as easy positiVe triplet mining (Xuan
et al., 2019) selects the closest anchor-positiVe pairs and ensures that at least they are closer than the
nearest negatiVes.
The next section introduces a diagram to systematically organize these triplet selection approaches,
and to explore where different loss functions fail to improVe the triplets.
3	Triplet Scatter Diagram
Triplet loss is trained with triplets of images, (xa , xp, xn), where xa is an anchor image, xp is a
positiVe image of the same class as the anchor, and xn is a negatiVe image of a different class.
We consider a convolution neural network, f (∙) that embeds the images on a unit hypersphere,
(f (xa), f (xp), f (xn)). We use (fa, fp , fn) to simplify the representation of the normalized feature
vectors. When embedded on a hypersphere, the cosine similarity is a convenient metric to measure
2
Under review as a conference paper at ICLR 2020
the similarity of anchor-positive pair Sap = fa|fp and anchor-negative pair San = fa|fn, and this
similarity is bounded in the range [-1, 1].
The triplet scatter diagram is an approach to characterizing a given set of triplets. Figure 1 represents
each triplet as a 2D point (Sap , San), describing how similar the positive and negative images are to
the anchor. This diagram is useful because the location on the diagram describes important features
of the triplet:
•	Triplets that are already in the correct configuration, where the similarity between anchor
and positive is greater than the similarity between anchor and negative images are below
the San = Sap diagonal. Dots representing triplets in the correct configuration are drawn
in blue, dots where the negative is closer are drawn in red.
•	Triplets that include an anchor and the most similar of the possible positive examples are
the ”Easy Positives” and are on the right side of the diagram because Sap tends to be close
to 1. We circle these with a red ring.
•	Hard negatives are cases where the anchor is very similar to a negative example, so San is
close to 1, depicted as red dots circled with a blue ring.
•	Hard negatives are cases where the anchor is very similar to a negative example, so San is
close to 1, depicted as red dots circled with a blue ring.
•	One very selective mining strategy is ”Easy-Positive, Semi-Hard Negative”, where an an-
chor is matched with closest possible positive match, and a negative example which has a
similar similarity. The blue dot circled with red dashed circle highlights one such example.
•	Another selective mining strategy is ”Easy-Positive, Hard Negative”, which selects, for an
anchor, the most similar positive and negative examples. The red circle surrounded by a
blue dashed circle represents one such example.
During the later discussion, We may show a subset area of Ω, Ωs = [0,1] X [0,1], because it is rare
that the hardest negative or positive pairs have a similarity less than 0.
Figure 1 (right) calls out two specific regions of points that we analyze in the next section; the
extremes of hard negatives and easy positives, and the region that only includes positive similari-
ties Ωs = [0,1] × [0,1], that includes nearly all triplets constructed with easy positives and hard
negatives.
4	Diagramming why some triplets are hard to optimize
The triplet scatter diagram offers the ability to understand when the gradient based optimization of
the network parameters is effective and when it fails. The triplets are used to train a network whose
loss function encourages the anchor to be more similar to its positive example (drawn from the same
class) than to its negative example (drawn from a different class), encouraging Sap to be greater
than San . While there are several possible choices, we consider NCA (Goldberger et al., 2005) as
the loss function, and denote this as L1st to differentiate it from an updated loss function introduced
later:
“"(fa, fp, fn) = -log eXp(SaPp+aXP(San)	⑴
All of the following derivation can also be done for the triplet loss formulation used in (Schroff et al.,
2015a); this has a very similar form and is derived in the Appendix.
The gradient of triplets loss L1st (fa, fp, fn) can be decomposed into two parts: a single gradient
with respect to feature vectors fa, fp , fn :
∆L
∂L∆fa + ∂L ∆fp + ∂L∆fn,
∂ fa	∂fp	∂ fn
(2)
and subsequently being clear that these feature vectors respond to changes in the model parameters
(the CNN network weights) θ:
∆L
dLdfa ∆θ+Mf ∆θ+胃 ∆θ.
∂ fa ∂ θ	∂ fp ∂θ	∂ fn ∂ θ
(3)
3
Under review as a conference paper at ICLR 2020
The gradient optimization only affects the feature embedding through variations in θ, but we first
highlight problems with hypersphere embedding assuming that the optimization could directly affect
the embedding location. To do this we derive the loss gradient with respect to the feature vector fa,
fp , fn and use this gradient to update the feature locations where should decrease the error:
	∂L fpnew	=	fp	-	αgp	=	fp	- αf =	fp	+ βfa	(4) ∂L fn	=	fn	-	αgn	=	fn	- α 而-=fn	- Bfa	(5) ∂fn ∂L fa	= fa -	αga	=	fa	- α 万F =	fa	- βfn +	βfp	(6) ∂fa
where β	二 ɑ exp(展Xp)(San)(S~)and ɑ is the learning rate. This gradient update has a clear geometric
meaning: the positive point fp is encouraged to move along the direction of the vector fa ; the
negative point fn is encouraged to move along the opposite direction of the vector fa ; the anchor
point fa is encouraged to move along the direction of the sum of fp and negative fn . All of these
are weighted by the same weighting factor β. Then we can get the new similarity of anchor-positive
and anchor-negative (The derivation is given in the Appendix):
Sanpew = (1+β2)Sap+2β-βSpn -β2San	(7)
Sannew = (1+β2)San-2β+βSpn-β2Sap	(8)
These gradients ga, gp, gn have components that move them off the sphere, computing the cosine
similarity requires that we compute the norm of fanew , fpnew and fnnew (the derivation for them
is shown in Appendix). Given the norm of updated feature vector, we can calculate the similarity
change after the gradient update.
∆Sap
∆San
new
Sap_______
kfanewkkfp newk
- San
kfa new
new
San
IFn
new
(9)
(10)
I
Because the gradient of the loss function does not consider this normalization, following the negative
gradient can actually cause some triplets to push the anchor closer to the negative example or push
the anchor away from the positive example, even assuming that you can directly push the anchor,
positive and negative features vectors in any direction.
The direction in which fa, fp, and fn move depends on the relative position of the fa , fp , fn on the
hypersphere. We use γ (fully defined in the Appendix) as a term to describe their relative orientation;
when fa, fn, and fp are close enough so that locally the hypersphere is a plane, then γ is the dot-
product of normalized vector from fa to fp and fa to fn . Therefore, if fp , fa , fn are co-planer then
γ = 1, and if moving from fa to fp is orthogonal to the direction from fa to fn, then γ = 0. Given
this description of the relative positions of the anchor, positive and negative points, Figure 2 shows
calculations of the change in similarity between the anchor positive and anchor negative for γ = 0.5.
There is an area along the right side of the ∆Sap plot highlighting locations where the anchor and
positive are pushed farther apart(∆Sap < 0), and along the top of the ∆San plot highlighting
locations where the anchor and negative are pulled closer together(∆San > 0). This behavior arises
because the gradient is pushing the feature off the hypersphere and therefore, after normalization,
the effect is lost.
This discussion so far considers the derivative of the loss as a function of the position of the feature
vectors, but the optimization can only control the feature vectors based on the network parameters
θ . Changes the θ are likely to affect nearby points in similar ways. For example, if there is a hard
negative example with easy positive where the anchor is close to both the positive and the negative
image, then changing θ to move the anchor closer to the positive is likely to pull the negative example
along with it. We call this effect ”entanglement” and propose a simple model to capture its effect on
how the gradient update affects the similarities.
We use a scalar P and a factor q =，SapSan to quantify this entanglement. As for the factor q,
when anchor, positive and negative are nearby to each other, both Sap and San will be large and q
will increase the entanglement effect; when positive or negative is far away to anchor, one of Sap
4
Under review as a conference paper at ICLR 2020
1st order Loss
ΔSaJv=0.5)	ΔSS*a,(y= 0.5, p =0.4)	ΔS*Stβ,(v=0,5,p = 0.8)
Figure 2: First two rows: numerical simulation for ∆Sap, ∆San, ∆Satoptal, ∆Satontal in equation 9,
10, 11 and 12 of γ = 0.5 and p = 0.4, 0.8. Thrid row: Vector field of γ = 0.5 andp = 0.4, 0.8.
2nd order Loss
and San will be small and q will reduce the entanglement effect. The total similarity changes with
entanglement will modelled as follows:
∆saptal =∆Sap+PPSapSnas。,
∆santa =∆San+PPSapSanas。；
(11)
(12)
n
p
Figure 2 shows (in color) problematic regions where this model of gradient entanglement indicates
that anchor and positive images become less similar (Sap < 0) and regions where the anchor nega-
tive images become more similar for different parameters of the entanglement (San > 0).
While figure 2 captures problematic regions on the scatter diagram, we can create a more complete
description. The bottom row plots of figure2 shows the vector field on the scatter diagram, indicating
that triplets move based on the gradient of their loss function. The figure shows several vector field
plots with aSatoptal and aSatontal with γ = 0.5 andP = 0.4, 0.8 settings and indicates the 3 types of
movement direction for dots on the triplet scatter. When aSap > 0 and aSan > 0, the movement
direction will point to up-right. When aSap < 0 and aSan < 0, the movement direction will point
to bottom-left. When aSap > 0 and aSan > 0, the movement direction will point to bottom-
right. In fact, the entanglement strength may be varied in different situation during the optimization.
The exact value will not be discussed in this paper. We only use the entanglement phenomenon to
demonstrate problems that happen optimizing triplets with Easy Positives and Hard Negatives.
Problem 1: Hard Negative Mining For a given anchor image, hard negative mining chooses the
negative example that maximizes San. The vector field computed in the case of entanglement shows
that most locations with large San (near the top of the plot) have vectors with an upward component,
meaning the gradient update for a hard negative triplet will push the negative even closer to anchor.
The result is that a network cannot effectively separate the negative pairs and tending to make all
points close to each other. Initializing a fine-grained visualization tasks (e.g. CARS196) with a
generic pre-trained network like ImageNet often creates a starting conditions where all points are
mapped nearby to begin with, leading to an optimization failure where all triplets move towards
(1,1) on the triplet scatter diagram and all images are mapped to the same feature location.
Problem 2: Easy Positive Mining For a given anchor image, easy positive mining chooses the
positive example that maximizes Sap . The vector field computed in the case of entanglement shows
that most locations with large Sap (near the right of the plot) have vectors with a strong downward
component, meaning the gradient updates pushes the anchor and negative to be negative related,
which leads over-training. A general idea for a pair of images to be different is their similarity to be
zero, the negative similarity value still means the pair of images are a kind of ’related’. In the later
5
Under review as a conference paper at ICLR 2020
phase of the optimization, as the optimization proceeds the triplet scatter diagram will not effectively
keep triplets close to the ideal (1,0) point.
5	Weight Gradient by S imilarity
Triplets that have problem 1 or problem 2 create gradients that move fa, fp , fn in wrong directions.
When the anchor is very close to either the positive or the negative points, the gradients defined by
their interaction is largely lost because of the hypersphere normalization, and the remaining effective
gradient is dominated by the entanglement.
Specifically, for problem 1, the anchor and the negative image are close together, and not effectively
encouraged to move apart, and pull of the anchor image towards the positive pulls the negative image
in the same direction. The fix for this problem is to emphasize more the part of gradient pushing the
anchor and negative apart when San is close to 1.
For problem 2, the anchor and positive image are close together, and there is a more distant negative
example. The effect of the distant negative example may push the anchor and positive example
further apart. The fix for this problem is to emphasize decrease the weight of the gradient related to
the distant negative example Sap is close to 1.
A simple weighting strategy addresses both problems. We weight the gp and gn gradients with
scalars wap = 1 - Sap and wan = San respectively. When a negative pair in a triplet is close but
the positive in the triplet is relative distant to anchor, we want to emphasize pushing the negative
pair apart, so we weight gp by 1 - Sap . Similarly, when a positive pair in a triplet is close but the
negative in the triplet is distant to anchor, we want to decrease the effect of the negative example,
then we weight gn with San . Our new gradients have the following form:
αgp	= -βw wap fa
αgn	= βw wan fa
αga	= βw wan fn - βw wap fp
(13)
(14)
(15)
α________exP( 2 San)______
exP (Sap-2 Sap ) + exP ( 2 San)
where βw
These can be integrated to find a loss function with these
gradients. We call this a 2nd-order triplet loss because in the exponent the similarities are squared.
L2nd(fa, fp, fn) = -log - exp，：:； -J2Sap) ι ,	(16)
eχp (Sap — 2 Sap) + eχp( 2 San)
The calculation of the terms needed to solve for ∆Sap or ∆San for the new loss function in the
Appendix, and Figure 2 (the right column) shows that how vector field for the new loss function. In
the area near (1,1), there is no longer the challenge that triplets are always moved towards (1,1) even
when including the effects of entanglement. This helps overcome the problem that the optimization
pushes all points towards the same place and we will show this effect in the experimental section.
6	Experimental Results
We to run a set of experiments on CUB200 (Welinder et al., 2010), CAR196 (Krause et al., 2013),
Stanford online products (Song et al., 2016) and In-shop cloth (Ziwei Liu & Tang, 2016) datasets.
All tests are run on the PyTorch platform (Paszke et al., 2017), using ResNet18 and ResNet50 (He
et al., 2016) architectures, pre-trained on ILSVRC 2012-CLS data (Russakovsky et al., 2015). Train-
ing images are re-sized to 256 by 256 pixels. We adopt a standard data augmentation scheme (ran-
dom horizontal flip and random crops padded by 10 pixels on each side). For pre-processing, we
normalize the images using the channel means and standard deviations. All networks are trained
using stochastic gradient descent (SGD) with 40 epochs. We set initial learning rate 0.005 for CAR,
SOP and In-shop cloth dataset and 0.0025 for CUB dataset, and divided by 10 after 20th and 30th
epochs. The batch size is 128 and each batch of images contains n examples from c classes, ran-
domly selected from the training data. Throughout the paper, we refer to the n examples per class
as a group.
Comparing the performance of 1st order and 2nd order triplet loss only requires changing one
line of code in a PyTorch implementation, substituting the loss from Equation 16 for the loss from
6
Under review as a conference paper at ICLR 2020
1st order loss
2nd order loss
Figure 3: The locations of dots moves top-right, top-left, bottom-left and bottom-right.The blue
color represents the initial phase iterations and the red color represents the later phase iterations
Equation 1. Then we calculate Recall@K as the measurement for retrieval quality. In the CUB,
CAR and SOP datasets, both the query set and gallery set refer to the testing set. During the query
process, the top-K retrieved images exclude the query image itself. In the In-Shop dataset, the query
set and gallery set are predefined by the original paper.
6.1	Verification of vector field
We implement two variants from EPHN paper (Xuan et al., 2019), Hard-Positive with Hard-
Negative(HPHN) to mimic the Problem 1 and Easy-Positive with Easy-Negative(EPEH) to mimic
the Problem 2 with CAR dataset and set n = 16 so that there will be enough positive and negative
for easy or hard selecting. Also, in the batch, we only allow each feature vector can only be in
one triplet to prevent the affection among the triplet. Figure 3 shows two sets experiment results
of training a network with 2 epochs(73 batches per epoch) at initial phase and later phase of the
optimization. We set the learning rate very low(0.00001) so as to prevent the model updating too
much in this experiment and better observe the problem happen in the wrong area.
We plot where the triplet dot move to all possible 4 direction(top-right, top-left, bottom-left and
bottom-right). This result highly match our proposal model. 1) there is no top-left movement which
is not possible in our model. 2) In the initial phase, most of the movement for the L1st is top-right
and for the L2nd is bottom-right, which fit the our proposal fix to the vector field 3) In the later phase,
most of the movement for the L1st is bottom-right and for the L2nd is bottom-right, bottom-left and
top-right. 4Although the learning rate is small, there is still a patter change with the dot in early
phase. L1st move to top-right and L2nd move to bottom-right.
6.2	Comparative Results for EPHN
Hard-Negative mining optimization problem depicts the specific cause of the challenges reported
(for example (Harwood et al., 2017)) for optimizing with hard negative examples. Also, in the
EPHN work (Xuan et al., 2019), when n = 2, there is only one positive example to choose from, so
Sap is more likely to be far away to 1. The negative is chosen as the closest negative, so San is more
likely to be close to 1. This situation is similar to Problem 1. The L1st loss leads to a singularity at
the location (1,1).
In addition, we re-implement the test in EPHN work to plot the recall@1 accuracy versus n across
CUB, CAR, SOP, and In-shop dataset. Figure 4 shows a clear gap of L1st and L2nd with EPHN
mining. We hypothesize that the small number of examples in most classes in SOP leads to the
inconsistent behavior we observe. Moreover, the figure shows Problem 1 happened on the CUB
when n = 2 and CAR when n = 2, 4, but L2nd loss perform well on with the same setting.
6.3	Comparison: EPHN vs EPSHN
Semi-hard negative mining samples triplets choosing the most similar negative where Sap > San .
It is therefore most affected by the Problem 2. We compare the result of easy positive and semi-hard
7
Under review as a conference paper at ICLR 2020
Figure 4: Recall@1 accuracy vs group size n on CUB, CAR, SOP and In-shop datasets with 1st/2nd
order EPHN/EPSHN
Dataset	CUB	CAR	SOP	In-shop
Method	R@1 R@2 R@4	R@1 R@2 R@4	R @1 R @10 R @100	R@1 R@10 R@20
HTL512	57.1 68.8 78.7	81.4 88.0 92.7	74.8 88.3^^941-	--	-
ABE512	60.6 71.5 79.8	85.2 90.5 94.0	76.3 88.4	94.8	87.3 96.7	97.9
DREML576	63.9 75.0 83.1	86.0 91.7 95.0	--	-	--	-
FastAP512	---	---	76.4 89.0	95.1	90.9 97.7	98.5
EPHN-Ist512	64.9 75.3 83.5	82.7 89.3 93.0	78.0 90.6	96.3	87.1 96.9	97.9
EPHN-2nd512	65.2 753 82.9	83.2 89.2 93.2	78.8 90.8	96.3	89.0 96.9	97.8
Table 1: Retrieval Performance on the CUB, CAR, SOP and In-shop datasets comparing to the best
reported results for more complex approaches and/or ensembles. All test are trained with ResNet50
negative mining (EPSHN) and easy-positive hard-negative mining (EPHN) for both 1st order and
2nd order loss functions on CUB, CAR, SOP and In-shop dataset. Figure 4 shows a another gap of
L2nd with EPHN and EPSHN on CUB, CAR and In-shop dataset for most choices of the group size
n of elements per class in each batch. This result indicates that both Problem 1 and 2 are important
for metric learning.
6.4	Comparing to the state of art
Finally we compare our results with current state-of-the-art embedding approaches, including more
complex triplet loss approaches (Yuan et al., 2017; Ge, 2018; Cakir et al., 2019) and ensemble based
approaches (Opitz et al., 2017; Kim et al., 2018; Xuan et al., 2018). Our embeddings are trained
with ResNet50 and an output embedding size of 512. For CUB, CAR, SOP and In-shop, the optimal
group size is 8,16,2 and 2. In Table 1, the 2nd order Easy Positive Hard Negative approach achieves a
new record on the CUB and SOP dataset. On the CAR and In-shop dataset, our result is comparable
to the ensemble methods.
7	Conclusion
This paper uses the triplet scatter diagram as a way to describe triplet selection strategies. The dia-
gram offers the ability to characterize how triplets change as the Deep Metric Learning progresses,
and we explore the behavior of the gradient descent optimization for the common case where points
are normalized to a hypersphere. We find that important classes of triplets have an effective gradient
forces negative examples closer to the anchor, or positive examples farther from the anchor, and situ-
ations that encourage triplets of images that are all similar to become even more similar. This explain
previously observed bad behavior for hard-negative triplet mining. We suggest a simple modifica-
tion to the desired gradients, and derive a loss function that gives those gradients. Experimentally
we show that this improves the convergence for hard negative triplet selection strategies.
With this modification, we no longer observe challenges in optimization with the Easy-Positive
Hard-Negative triplet mining strategies and show that easy-positive hard negative mining gives re-
sults that exceed or are competitive with state of the art approaches that including complicated
network architectures and ensembles.
8
Under review as a conference paper at ICLR 2020
References
Fatih Cakir, Kun He, Xide Xia, Brian Kulis, and Stan Sclaroff. Deep metric learning to rank. In
Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Miguel A Carreira-Perpinan and Geoffrey E Hinton. On contrastive divergence learning. In AIS-
TATS, volume 10,pp. 33-40, 2005.
Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with
application to face verification. In Proc. IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), volume 1, pp. 539-546, 2005.
Weifeng Ge. Deep metric learning with hierarchical triplet loss. In Proc. European Conference on
Computer Vision (ECCV), September 2018.
Jacob Goldberger, Geoffrey E Hinton, Sam T. Roweis, and Ruslan R Salakhutdinov. Neighbour-
hood components analysis. In L. K. Saul, Y. Weiss, and L. Bottou (eds.), Advances in Neural
Information Processing Systems 17, pp. 513-520. MIT Press, 2005. URL http://papers.
nips.cc/paper/2566-neighbourhood-components-analysis.pdf.
Ben Harwood, BG Kumar, Gustavo Carneiro, Ian Reid, Tom Drummond, et al. Smart mining for
deep metric learning. In Proceedings of the IEEE International Conference on Computer Vision,
pp. 2821-2829, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Wonsik Kim, Bhavya Goyal, Kunal Chawla, Jungmin Lee, and Keunjoo Kwon. Attention-based
ensemble for deep metric learning. In Proc. European Conference on Computer Vision (ECCV),
September 2018.
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained
categorization. In 4th International IEEE Workshop on 3D Representation and Recognition
(3dRR-13), Sydney, Australia, 2013.
Yair Movshovitz-Attias, Alexander Toshev, Thomas K. Leung, Sergey Ioffe, and Saurabh Singh.
No fuss distance metric learning using proxies. In Proc. International Conference on Computer
Vision (ICCV), Oct 2017.
Michael Opitz, Georg Waltner, Horst Possegger, and Horst Bischof. Bier - boosting independent
embeddings robustly. In Proc. International Conference on Computer Vision (ICCV), Oct 2017.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. In NIPS-W, 2017.
FiliP Radenovic, Giorgos Tolias, and Ondrej Chum. CNN image retrieval learns from bow: UnsU-
pervised fine-tuning with hard examples. In European Conference on Computer Vision, pp. 3-20.
Springer, 2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211-252, 2015.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face
recognition and clustering. In Proc. IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2015a.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face
recognition and clustering. In Proc. IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pp. 815-823, 2015b.
9
Under review as a conference paper at ICLR 2020
Edgar Simo-Serra, Eduard Trulls, Luis Ferraz, Iasonas Kokkinos, Pascal Fua, and Francesc Moreno-
Noguer. Discriminative learning of deep convolutional feature point descriptors. In Proc. Inter-
national Conference on Computer Vision (ICCV), pp. 118-126, 2015.
Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information
Processing Systems 29, pp. 1857-1865. 2016. URL http://papers.nips.cc/paper/
6200-improved-deep-metric-learning-with-multi-class-n-pair-loss-objective.
pdf.
Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted
structured feature embedding. In Proc. IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2016.
Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Zhifeng Li, Dihong Gong, Jingchao Zhou, and Wei
Liu. Cosface: Large margin cosine loss for deep face recognition. CoRR, abs/1801.09414, 2018.
URL http://arxiv.org/abs/1801.09414.
Jiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg, Jingbin Wang, James Philbin, Bo Chen,
and Ying Wu. Learning fine-grained image similarity with deep ranking. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1386-1393, 2014.
P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD
Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.
Hong Xuan, Richard Souvenir, and Robert Pless. Deep randomized ensembles for metric learning.
In Proc. European Conference on Computer Vision (ECCV), September 2018.
Hong Xuan, Abby Stylianou, and Robert Pless. Improved embeddings with easy positive triplet
mining. arXiv preprint arXiv:1904.04370, 2019.
Yuhui Yuan, Kuiyuan Yang, and Chao Zhang. Hard-aware deeply cascaded embedding. In Proc.
International Conference on Computer Vision (ICCV), Oct 2017.
Shi Qiu Xiaogang Wang Ziwei Liu, Ping Luo and Xiaoou Tang. Deepfashion: Powering robust
clothes recognition and retrieval with rich annotations. In Proc. IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
A Appendix: Similarity after gradient updating for 1st order
LOSS
The following derivation shows how to get Sanpew and Sannew in equation 7 and 8.
new
Sap
=fan ew|f new
=(1 + β2)falfp + βfalfa + β fplfp — βfjfp — β2fnlfa
=(1+β2)Sap+2β-βSpn-β2San
=fan ew|f new
=(1 + β2)falfn - βfalfa- βfn|fn + βfplfn — β2fplfa
=(1+β2)Sap-2β+βSpn-β2Sap
(17)
(18)
We construct two planes: Pap spanned by fa and fp, and Pan spanned by fa and fn . On Pap, fp can
be decomposed into two components: fpak (the direction along fa) and fpa⊥(the direction vertical to
fa). On Pan, fn can be decomposed into two components: fnak (the direction along fa) and fna⊥(the
direction vertical to fa). Then the Spn should be:
Spn = fp|fn =(塔k + 针)(fak + fa⊥) = SapSan + Y g Sap P^Sn	(⑼
fa⊥ fa⊥
where Y = ∣f a⊥k∣f a⊥∣∣ which represents the projection factor between Pap and Pan
10
Under review as a conference paper at ICLR 2020
B	Appendix: Norm of updated features for 1st order loss
The following derivation shows how to derive kfanewk, kfpnewk and kfnnewk in equation 9 and 10.
On Pap, gp can be decomposed into the direction along fp and the direction vertical to fp . On Pan ,
gn can be decomposed into the direction along fn and the direction vertical to fn . Then,
kfpnewk2 = (1+βSap)2+β2(1-Sa2p)	(20)
kfnnewk2 = (1 - βSan)2 + β2(1 - Sa2n)	(21)
On Pap, ga can be decomposed into 3 components: component in the plane and along fa, component
in the plane and vertical fa , and component vertical to Pap . Then,
kfanew k2 = (1+βSap-βSan)2 + (β∕l- Sap-7e«—标)2 + (β√1 — Y2 P^n)2 (22)
C Appendix: Similarity after gradient updating for 2nd order
LOSS
The following derivation shows how to get Sanpew and Sannew after gradient update with L2nd
Sap = (1 + wap βw )Sap + 2wap βw - wan βw Spn - wap wan βw San
San = (1 + wanβw )Sap - 2wanβw + wapβw Spn - wap wan βw Sap
(23)
(24)
D Appendix: Norm of updated features for 2nd order loss
The following derivation shows how to derive kfanewk, kfpnewk and kfnnewk after gradient update
with L2nd .
kfpnewk2	= (1 + wapβwSap)2 + wa2pβw2 (1 - Sa2p)
kfnnewk2 = (1 - wanβwSan)2 + wa2nβw2 (1 - Sa2n)
kfanewk2 = (1+wapβwSap-wanβwSan)2
+ (Wapew JI- Sap -YWan∕βw √1 - San)2
+ (wan% √1 - γ2 √1 - San)2
(25)
(26)
(27)
E Appendix: Gradient updates for vanilla triplet loss
The main paper uses NCA to define the loss. Another common vanilla triplet-loss function is derived
to guarantee a specific margin. This can be expressed in terms of fa,fp ,fn as:
L	max(kfa - fpk2 - kfa - fnk2 + α, 0) = max(D, 0)					(28)
	gp =	dL =《 ∂fρ = <	-β(fa 0	- fp )	if D > 0 otherwise	(29)
	gn =	∂L _ ∂fn =	β (fa - 0	fn)	if D > 0 otherwise	(30)
	ga =	∂L —— ∂fa	β (fn	- fp )	if D > 0 otherwise	(31)
where D = (kfa - fp k2 - kfa - fn k2 + α) and β = 2. For simplicity, in the following discussion,
we set D > 0 for vanilla triplet loss. Then we can get the fanew, fpnew and fnnew and their norm:
fpnew = fp+β(fa-fp) = (1 -β)fp+βfa
fnnew = fn-β(fa-fn) = (1+β)fn-βfa
fanew = fa - βfn + βfp
(32)
(33)
(34)
11
Under review as a conference paper at ICLR 2020
ΔSal(y = 0∙2)
ΔSan(y = 0.2)
1st order Loss
.ΔSjg"(y = 0∙2,p = 0.4)
Δ¾⅞3'(V=O∙2,P = O.8)
∆⅞l叫 y = 0∙2,p = 0.4).
δs缪“ (丫： 0.2, P = 0.8)

ΔSal(y = (λ5)
.AS 器叫 y=05 p = 0.4)
Δ⅞s'(V=05p = 0.8)
■ioia
ΔSan(y= 0.5)
∆⅞i叫 y = O5p = O.4).
Δ¾%(y=05p = 0.8).
≡ΠΓI
∆⅞i*'(y=o.8,p=o.4)3
ASw(y =,.8, p = 0.8).
ΔSan(y= 0.8)
≡ΠΓI
Vanilla Loss
βoια
ΔS3p(y = O∙2)
ΔSan(y= 0.2)
.ΔSjg"(y = 0∙2,p = 0.4)
Δ¾⅞3'(V=O∙2,P = O.8)
,"叫y=0.2,P = ?4) 3
Δ⅞'*'(γ=02p = 0.8)
■m
ΔSal(y = (λ5)
ΔSan(y= 0.5)
∆⅝p(y = 0.8)
.∆Sjg"(y=θ5p = o.4)
Δ¾⅞3'(V=05p = 0.8)
Δ⅞“"f0∙5,p = 0.4).
Δ⅞s,y=05p = α8).
.ΔSjg"(y = 0∙8,p = 0.4)
Δ¾⅞3'(V=O∙8,P = O.8)
■OIH
ΔSan(y= 0.8)
Δ⅞s'(y = 0.8,p = 0.4) -
Δ⅞By=018,p = α8).
2nd order Loss
ASMy=OZ)	ΔS^'(y = 0,2,p = 0.4)ι ΔSi∏y = 0,2, p = 0.8),
≡πιoι
ΔSan(y= 0.2)
・二 I
ΔSm(y= 0.5)	. ΔS^(F = 0,5,p = 0.4) ASgM,(y= 0,5,p = 0.8)
B□□
■i 1∣ 1∣
Δ⅛>(y=0.8) I Δsy,(y=0,8,p = 0-4)	ΔS^'(y=0,8,p = 0.8)
■□ICI
Figure 5:	Numerical simulation for ∆Sap, ∆San, ∆Satoptal and ∆Satontal change of L1st , Lv and
L2nd with γ = 0.2, 0.5, 0.8 and p = 0.4, 0.8.
kfpnewk2 = (1-β+βSap)2+β2(1-Sa2p)	(35)
kfnnewk2 = (1+β-βSan)2+β2(1-Sa2n)	(36)
kfanew k2 = (1+ βSap-βSan)2 + (β J- Sap-γβ Pr-Sin )2 + (β Pi-¥ Pi-Sn )2 (37)
The updated similarity Sanpew and Sannew will be:
Sanpew = (1-β+β2)Sap+2β-β2-β(1-β)Spn-β2San	(38)
Sannew = (1+β+β2)San-2β-β2+β(1+β)Spn-β2Sap	(39)
Comparing to the equation 7 and 8, vanilla triplet behavior is similar to the triplet-NCA. And we
simulate the ∆Sap and ∆San with the vanilla triplet loss in figure 5.
We show results for EPHN for n = 2 and n = 8 on CAR dataset when the hard negative mining
problem happens during the optimization. In both cases, all points are initially pushed towards
the same location (1,1), which matches our predicted movement on the vector field of L1st with
entanglement in figure 2.
F Appendix: Numerical simulation for similarity change and
their entanglement result for 1st order, vanilla and 2nd
ORDER LOSS
Figure 5 shows the numerical simulation ∆Sap and ∆San with L1st, Lv and L2nd. These show that
the problematic regions in terms are qualitatively similar for different values of p, the parameter in
our model of entanglement.
G Appendix: EPHN for 1st and 2nd order loss
Figure 6	shows the training of the embedding function observed through the triplet scatter diagram,
showing for each point the triplet containing the closest positive and the closest negative from whole
12
Under review as a conference paper at ICLR 2020
Figure 6: Triplet scatter plots after training epoch 0,5,10,15,40(epoch 0 represents the output of
imagenet initialization). 1st row: 1st order EPHN with n = 2;2nd row: 1st order EPHN with
n = 8;3rd row: 2nd order EPHN with n = 2;4th row: 2nd order EPHN with n = 8
dataset after each epoch training. We observe the dynamic movement to the singularity at the loca-
tion (1,1) results for EPHN for n = 2. The L2nd loss for EPHN never has this problem because the
weighted gradient approach more effectively separates hard negative pairs early on in the optimiza-
tion.
When n = 8, the easy-positive part of the triplet is easier because each can pick the closest of 7
neighbors from the same class instead of the randomly assigned same class element and Sap will
be closer to 1. Meanwhile, the negative is likely to be less similar because there are fewer other
examples to choose from. Therefore optimization recovers because triplets are less likely to have
large San and get trapped in the part of the vector field that lead to (1,1).
13