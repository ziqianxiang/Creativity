Under review as a conference paper at ICLR 2020
Multi-source Multi-view Transfer Learning
in Neural Topic Modeling with Pretrained
Topic and Word Embeddings
Anonymous authors
Paper under double-blind review
Ab stract
Though word embeddings and topics are complementary representations, several
past works have only used pretrained word embeddings in (neural) topic model-
ing to address data sparsity problem in short text or small collection of documents.
However, no prior work has employed (pretrained latent) topics in transfer learn-
ing paradigm. In this paper, we propose a framework to perform transfer learning
in neural topic modeling using (1) pretrained (latent) topics obtained from a large
source corpus, and (2) pretrained word and topic embeddings jointly (i.e., multi-
view) in order to improve topic quality, better deal with polysemy and data sparsity
issues in a target corpus. In doing so, we first accumulate topics and word repre-
sentations from one or many source corpora to build respective pools of pretrained
topic (i.e., TopicPool) and word embeddings (i.e., WordPool). Then, we identify
one or multiple relevant source domain(s) and take advantage of corresponding
topics and word features via the respective pools to guide meaningful learning
in the sparse target domain. We quantify the quality of topic and document rep-
resentations via generalization (perplexity), interpretability (topic coherence) and
information retrieval (IR) using short-text, long-text, small and large document
collections from news and medical domains. We have demonstrated the state-of-
the-art results on topic modeling with the proposed transfer learning approaches.
1	Introduction
Probabilistic topic models, such as LDA (Blei et al., 2003), Replicated Softmax (RSM) (Salakhut-
dinov & Hinton, 2009) and Document Neural Autoregressive Distribution Estimator (DocNADE)
(Larochelle & Lauly, 2012) are often used to extract topics from text collections and learn latent
document representations to perform natural language processing tasks, such as information re-
trieval (IR). Though they have been shown to be powerful in modeling large text corpora, the topic
modeling (TM) still remains challenging especially in the sparse-data setting, especially for the
cases where word co-occurrence data is insufficient e.g., on short text or a corpus of few docu-
ments. To this end, several works (Das et al., 2015; Nguyen et al., 2015; Gupta et al., 2019) have
introduced external knowledge in traditional topic models via word embeddings Pennington et al.
(2014). However, no prior work in topic modeling has employed topical embeddings (obtained from
large document collection(s)), complementary to word embeddings.
Local vs Global Views: Though word embeddings (Pennington et al., 2014) and topics are com-
plementary in how they represent the meaning, they are distinctive in how they learn from word
occurrences observed in text corpora. Word embeddings have local context (view) in the sense that
they are learned based on local collocation pattern in a text corpus, where the representation of each
word either depends on a local context window (Mikolov et al., 2013) or is a function of its sen-
tence(s) (Peters et al., 2018). Consequently, the word occurrences are modeled in a fine-granularity.
On other hand, a topic (Blei et al., 2003; Gupta et al., 2019) has a global word context (view): TM in-
fers topic distributions across documents in the corpus and assigns a topic to each word occurrence,
where the assignment is equally dependent on all other words appearing in the same document.
Therefore, it learns from word occurrences across documents and encodes a coarse-granularity de-
scription. Unlike topics, the word embeddings do not capture thematic structures (topical semantics)
underlying in the document collection.
1
Under review as a conference paper at ICLR 2020
Notation	Description	Notation	Description
LVT, GVT	Local-view Transfer, Global-view Transfer	Ak ∈ RH×H	Topic-alignment in T and Zk
MVT, MST	Multi-view Transfer, Multi-source Transfer	K, D	Vocabulary size, document size
T,S	A target domain, a set of source domains	E, H	Word embedding dimension, #topics
λk	Degree of relevance of Ek in T	b ∈ RK,c ∈ RH	Visible-bias, hidden-bias
γk	Degree of imitation of Zk by W	v, k, L	An input document, kth source, loss
Ek ∈ RE×K,	Word embeddings of kth source	W ∈ RH×K	Encoding matrix of DocNADE in T
Zk ∈ RH×K	Topic embeddings of kth source	U ∈ RK×H	Decoding matrix of DocNADE
Table 1: Description of the notations used in this work
Consider the following topics (Z1-Z4), where (Z1-Z3) are respectively obtained from different
(high-resource) source (S1 -S3) domains whereas Z4 from the (low-resource) target domain T in
the data-sparsity setting:
Z1 (S1): profit, growth, stocks, apple, fall, consumer, buy, billion, shares → Trading
Z2(S2): smartphone, ipad, apple, app, iphone, devices, phone, tablet → Product Line
Z3 (S3 ): microsoft, mac, linux, ibm, ios, apple, xp, windows → Operating System/Company
Z4 (T): apple, talk, computers, shares, disease, driver, electronics, profit, ios → ?
Usually, the top words associated with topics learned on a large corpus are semantically coherent and
represent meaningful semantics, e.g., Trading, Product Line, etc. However in sparse-data setting,
topics (e.g., Z4) are incoherent (noisy) and therefore, it is difficult to infer meaningful semantics.
Additionally, notice that the word apple is topically/thematically contextualized (topic-word associ-
ation) by different semantics in S 1 -S3 and referring to a Company.
Unlike the topics, word embeddings encode syntactic and semantic relatedness in fine-granularity
and therefore, do not capture thematic structures. For instance, the top-5 nearest neighbors (NN)
of apple (below) in the embeddings (Mikolov et al., 2013) space suggest that it refers to a fruit;
however, they do not express anything about its thematic context, e.g., Health.
apple =N=⇒N apples, pear, fruit, berry, pears, strawberry
Motivation (1) Knowledge transfer using pretrained word and topic embeddings: Essentially,
the application of TM aims to discover hidden thematic structures (i.e., topics) in text collection;
however, it is challenging in data sparsity settings, e.g, in a short and/or small collection. This leads
to suboptimal text representations and incoherent topics (e.g., topic Z4).
To alleviate the data sparsity issues, recent works (Das et al., 2015; Nguyen et al., 2015; Gupta
et al., 2019) have shown that TM can be improved by introducing external knowledge, where they
leverage pretrained word embeddings (i.e., local view) only. However, the word embeddings ignore
the thematically contextualized structures (i.e., document-level semantics), and can not deal with
ambiguity. Given that the word and topic representations encode complementary information, no
prior work has explored transfer learning in TM using pretrained topics obtained from a large corpus.
Motivation (2) Knowledge transfer from multiple sources of word and topic embeddings:
Knowledge transfer via word embeddings is vulnerable to negative transfer (Cao et al., 2010) on
the target domain when domains are shifted and not handled properly. For instance, consider a
short-text document v: [apple gained its US market shares] in the target domain T.
Here, the word apple refers to a company, and hence the word vector of apple (about fruit) is an
irrelevant source of prior knowledge for both v and the topic Z4 . In contrast, one can better model
v and amend the noisy Z4 for coherence, given the meaningful word and topic embeddings.
Often, there are several topic-word associations in different domains, e.g., in topics Z1-Z3. Given a
noisy topic Z4 in T and meaningful topics Z1-Z3 of S1-S3, we identify multiple relevant (source)
domains and advantageously transfer their word and topic embeddings in order to facilitate mean-
ingful and positive transfer learning in the sparse corpus, T.
Contribution (1) To our knowledge, it is the first work in unsupervised topic modeling framework
that introduces (external) knowledge transfer using (a) Global-view Transfer: Pretrained topic em-
2
Under review as a conference paper at ICLR 2020
ith autoregressive
conditional, p(Vj | v<)
visible-bias, b ∈ Rk
6
WordPool: KB of
Topic
Proportion
Embedding
Word Embeddings
U ∈ rk× 尘「十二.........
hi(v<i) ŋ J⅛ f
λ∣s∣
λ1
v1
hidden-bias,
C ∈ RH
W ∈ rh×k
columns: Word embeddings^Z
rows: Topic embeddings /
v1 vi-1	vk
v1	vi-1	VK
Embedding lookups
'∖ visible units,
> ∈ {1, ..., K}D
E1
E∣s∣
Collections (DC)
Target
Target topic
embeddings
TopicPool: KB of Source-target topic alignments in
pretrained topics projection space for GVT+MST



罔甲 λ√≤kmv
Figure 1: (Left) DocNADE (LVT+MST): Multi-source transfer learning in TM by introducing pre-
trained word embeddings from a WordPool at each autoregressive step i. Double circle → multino-
mial (softmax) unit. (Right) Multi-source transfer learning in TM by introducing pretrained (latent)
topic embeddings from a TopicPool, illustrating topic alignments between source and target cor-
pora in GVT+MST configuration. Each outgoing row from Zk signify a topic embedding of the
corresponding source corpus, DCk . Here, TM refers to a DocNADE topic model.
beddings instead of using word embeddings exclusively, and (b) Multi-view Transfer: Pretrained
word and topic embeddings jointly obtained from a large source corpus in order to deal with poly-
semy and alleviate data sparsity issues in a small target corpus.
Contribution (2) Multi-source Transfer: Moreover, we first learn word and topic representations on
multiple source domains to build WordPool and TopicPool, respectively and then perform multi-view
and multi-source transfer learning within neural topic modeling by jointly using the complementary
representations. In doing so, we guide the (unsupervised) generative process of learning hidden
topics of the target domain by embeddings in WordPool and TopicPool such that the hidden topics
become more meaningful and representative in explaining the target corpus.
We evaluate the effectiveness of our transfer learning approaches in neural topic modeling using 7 (5
low-resource and 2 high-resource) target and 5 (high-resource) source corpora from news and med-
ical domains, consisting of short-text, long-text, small and large document collections. Particularly,
we quantify the quality of text representations via generalization (perplexity), interpretability (topic
coherence) and text retrieval. The code is provided with the supplementary.
2	Knowledge Transfer in Neural Topic Modeling
Consider a sparse target domain T anda set of |S| source domains S, we first prepare two knowledge
bases (KBs) of representations (or embeddings) from each of the sources: (1) WordPool: Pretrained
word embeddings matrices {E1, ..., E|S|}, where Ek ∈ RE×K, and (2) TopicPool: Pretrained latent
topic embeddings {Z1, ..., Z|S|}, where Zk ∈ RH×K encodes a distribution over a vocabulary of
K words. E and H are word embedding and latent topic dimensions, respectively. While topic
modeling on T, we introduce the two types of knowledge transfers from one or many sources:
Local (LVT) and Global (GVT) View Transfer using the two KBs of pretrained word (i.e., WordPool)
and topic (i.e., TopicPool) embeddings, respectively. Specially, we employ a neural autoregressive
topic model (i.e., DocNADE (Larochelle & Lauly, 2012)) to build the WordPool and TopicPool.
Notice that a superscript indicates a source. See Table 1 for the notations used in this work.
2.1	Neural Autoregres sive Topic Models
DocNADE (Larochelle & Lauly, 2012) is an unsupervised neural-network based topic model that is
inspired by the benefits of NADE (Larochelle & Murray, 2011) and RSM (Salakhutdinov & Hinton,
2009) architectures. RSM has difficulties due to intractability leading to approximate gradients of
the negative log-likelihood, while NADE does not require such approximations. On other hand,
RSM is a generative model of word count, while NADE is limited to binary data. Specifically, Doc-
3
Under review as a conference paper at ICLR 2020
NADE factorizes the joint probability distribution of words in a document as a product of conditional
distributions and efficiently models each conditional via a feed-forward neural network.
Algorithm 1 Computation of log p(v) and Loss L(v)
Input: A target training document v, |S | source domains
Input: WordPool: KB of pretrained word embedding matrices {E1, ..., E|S| }
Input: TopicPool: KB of pretrained latent topics {Z1, ..., Z|S|}
Parameters: Θ = {b, c, W, U, A1, ..., A|S|}
Hyper-parameters: θ = {λ1,…，λlSl,γ1,…，γlSl, H}
Initialize: a J C and p(v) J 1
for i from 1 to D do
hi(v<i) J g(a), where g = {sigmoid, tanh}
p(v = ?wlv—) J______exP(bw+uw,:hi(VVi))_
我 i I <i' Pw0 exP(bw0 +Uw0,: hi (v<i))
p(v) J p(v)p(vi |v<i )
compute pre-activation at step, i: a J a + W:,vi
if LVT then
get word embedding for vi from source domain(s)
aJ a+pk=ι λk Ekvi
L(v) J - log p(v)
if GVT then
L(V)J L(V) + Pk=ι Yk PH=I ||Ak,W - Zjk,：||2
DocNADE Formulation: For a document v = (v1 , ..., vD) of size D, each word index vi takes
value in {1, ..., K } of vocabulary size K . DocNADE learns topics in a language modeling fashion
(Bengio et al., 2003) and decomposes the joint distribution p(V)=QiD=1 p(vi |V<i ) such that each
autoregressive conditional p(vi |V<i ) is modeled by a feed-forward neural network using preceding
words V<i in the sequence:
hi (VVi) = g(C + X W：,vq ) and P(Vi = WIVVi) = P eXP(bW + U;,) (V<")
q<i q	w0 exp(bw0 + Uw0,:hi	(V<i	))
for i ∈ {1, ...D}, where VVi is the subvector consisting of all vq such that q < i i.e., VVi ∈
{v1,...,vi-1}, g(∙) is a non-linear activation function, W ∈ Rh×k and U ∈ Rk×h are weight
matrices, C ∈ RH and b ∈ RK are bias parameter vectors. H is the number of hidden units (topics).
Figure 1 (left) (without WordPool) provides an illustration of the ith autoregressive step of the Doc-
NADE architecture, where the parameter W is shared in the feed-forward networks and hi encodes
topic-proportion embedding. Importantly, the topic-word matrix W has a property that the column
vector W:,vi corresponds to embedding of the word vi, whereas the row vector Wj,: encodes thejth
topic. We leverage this property to introduce external knowledge via word and topic embeddings.
Additionally, DocNADE has shown to outperform traditional models such as LDA (Blei et al., 2003)
and RSM (Salakhutdinov & Hinton, 2009) in terms of both the log-probability on unseen documents
and retrieval accuracy. Recently, Gupta et al. (2019) has improved topic modeling on short texts by
introducing word embeddings (Pennington et al., 2014) in DocNADE architecture. Thus, we adopt
DocNADE to perform transfer learning within the neural topic modeling framework.
Algorithm 1 (for DocNADE, set LVT and GVT to False) demonstrates the computation of log p(V)
and negative log-likelihood L(V) that is minimized using gradient descent. Moreover, computing hi
is efficient (linear complexity) due to NADE architecture that leverages the pre-activation ai-1 of
(i - 1)th step in computing the pre-activation ai. See Larochelle & Lauly (2012) for further details.
2.2	Multi-View (MVT) and Multi-Source Transfers (MST) in Topic Modeling
Here, we describe a transfer learning framework in topic modeling that jointly exploits the comple-
mentary knowledge using the WordPool and TopicPool, the KBs of pretrained word and (latent) topic
embeddings, respectively obtained from large document collections (DCs) from several sources. In
4
Under review as a conference paper at ICLR 2020
Target Domain Corpora
1T2T3T4T5T6T
Data	Train Val Test K LC
20NSshort	1.3k 0.1k 0.5k 1.4k 13.5 20
20NSsmall	0.4k 0.2k 0.2k 2k 187.5 20
TMNtitle	22.8k 2.0k 7.8k 2k 4.9 7
R21578title	7.3k 0.5k 3.0k 2k 7.3 90
Ohsumedtitle	8.3k 2.1k 12.7k 2k 11.9 23
Ohsumed	8.3k 2.1k 12.7k 3k 159.1 23
Source Domain Corpora
ID Data Train Val Test K L
S1
S2
S3
S4
S5
20NS
R21578
TMN
AGNews
PubMed
7.9k 1.6k 5.2k 2k 107.5
7.3k 0.5k 3.0k 2k 128
22.8k 2.0k 7.8k 2k 19
118k 2.0k 7.6k 5k 38
15.0k 2.5k 2.5k 3k 254.8
C
20
90
7
4
Table 2:	Data statistics: Short/long texts and/or small/large corpora in target
and source domains. Symbols- K: vocabulary size, L: average text length
(#words), C: number of classes and k: thousand. For short-text, L<15. S3
is also used in target domain. ‘-’: unlabeled data.
T1
T2
T3
T4
T5
T6
S1II IlIlRIDIDlD
^S2 DDDID 方
^S3 RRI 方
^S4 RRR 方
^S5 DDD 方
DD
DD
Table 3:	Domain
overlap in source-
target corpora. I :
Identical, R: Re-
lated and D: Distant
domains.
doing so, we first apply the DocNADE to generate a topic-word matrix for each of the DCs, where
its column-vector and row-vector generate Ek and Zk, respectively for the kth source.
LVT+MST Formulation for Multi-source Word Embedding Transfer: As illustrated in Figure
1 (left) and Algorithm 1 with LVT=True, we perform transfer learning on a target T using the
WordPool of pretrained word embeddings {E1, ..., E|S|} from several sources S (i.e., multi-source):
|S|
hi(v<i) = g(c + X W：,Vq + XX λk Ekvq)
Here, k refers to the kth source and λk is a weight for Ek that controls the amount of knowledge
transferred in T, based on domain overlap between target and source(s). Recently, DocNADEe
(Gupta et al., 2019) has incorporated word embeddings (Pennington et al., 2014) in extending Doc-
NADE; however, it is based on a single source.
GVT+MST Formulation for Multi-source Topic Embedding Transfer: Next, we perform knowl-
edge transfer exclusively using the TopicPool of pretrained topic embeddings (e.g., Zk) from one or
several sources, S. In doing so, we add a regularization term to the loss function L(v) and require
DocNADE to minimize the overall loss in a way that the (latent) topic features in W simultane-
ously inherit relevant topical features from each of the source domains S , and generate meaningful
representations for the target T. The overall loss L(v) due to GVT+MST in DocNADE is given by:
|S| H
L(v) = - log p(v) +Xγk X||Ajk,:W-Zjk,:||22
k=1	j=1
Here, Ak∈RH×H aligns latent topics in the target T and kth source, and γk governs the degree
of imitation of topic features Zk by W in T. Consequently, the generative process of learning
meaningful topics in W of T is guided by relevant features in {Z}|1S| to address data-sparsity.
Algorithm 1 describes the computation of the loss, when GVT = True and LVT = False.
Moreover, Figure 1 (right) illustrates the need for topic alignments between target and source(s).
Here, j indicates the topic (i.e., row) index in a topic matrix, e.g., Zk. Observe that the first topic
(gray curve), i.e., Zj1=1 ∈ Z1 of the first source aligns with the first row-vector (i.e., topic) of W (of
target). However, the other two topics Zj1=2 , Zj1=3 ∈ Z1 need alignment with the target topics.
MVT+MST Formulation for Multi-source Word and Topic Embeddings Transfer: When LVT
and GVT are True (Algorithm 1) for many sources, the two complementary representations are jointly
used in transfer learning using WordPool and TopicPool, and therefore, the name multi-view and
multi-source transfers.
3	Evaluation and Analysis
Datasets: Table 2 describes the datasets used in high-resource source and low-and high-resource
target domains for our experiments. The target domain T consists of four short-text corpora
(20NSshort, TMNtitle, R21578title and Ohsumedtitle), one small corpus (20NSsmall) and
5
Under review as a conference paper at ICLR 2020
Baselines (Related Works)
LDA (Blei et al., 2003)
RSM (Salakhutdinov & Hinton, 2009)
DocNADE (Larochelle & Lauly, 2012)
NVDM (Miao et al., 2016)
ProdLDA (Srivastava & Sutton, 2017)
Gauss-LDA(Daseta匚2015)
glove-DMM (Nguyen et al., 2015)
DocNADEe(GuPta et al., 2019)
EmbSum
doc2vec (Le & Mikolov, 2014)
this work
Features
NTM AuR LVT GVT|MVT|MST
X
X
XXX
X X X XXX
X
X
X
X
Table 4	: Baselines (related works) vs this work. Here, NTM and AuR refer to neural network-based
TM and autoregressive assumPtion, resPectively. DocNADEe → DocNADE+Glove embeddings.
two large corPora (TMN and Ohsumed). However in source S, we use five large corPora (20NS,
R21578, TMN, AGnews and PubMed) in different label sPaces (i.e, domains). Here, the corPora (T5,
T6 and S5) belong to medical and others to news.
Additionally, Table 3 suggests domain overlaP (in terms of label match) in the target and source
corPora, where we define three tyPes of overlaP: I (identical) if all labels match, R (related) if some
labels match, and D (distant) if a very few or no labels match. Note, our modeling aPProaches are
comPletely unsuPervised and do not use the data labels. See the data labels in appendices.
Reproducibility: For evaluations in the following sections, we follow the exPerimental setuP similar
to DocNADE (Larochelle & Lauly, 2012) and DocNADEe (GuPta et al., 2019), where the number
of toPics (H) is set to 200. While DocNADEe requires the dimension (i.e., E) of word embeddings
be the same as the latent toPic (i.e., H), we first aPPly a Projection on the concatenation of the Pre-
trained word embeddings obtained from several sources and then, introduce the Prior knowledge in
each of the autoregressive steP following DocNADEe. We aPPly it in configurations where Glove
and/or FastText (E=300) (Bojanowski et al., 2017) are emPloyed. See appendices for the exPerimen-
tal setuP, hyPerParameters1 and oPtimal values of λk ∈ [0.1, 0.5, 1.0] and γk ∈ [0.1, 0.01, 0.001]
(determined using develoPment set) in different source-target configurations. (code Provided)
Baselines: As summarized in Table 4, we consider several baselines including (1) LDA-based and
neural network-based toPic models that use the target data, (2) toPic models using Pretrained word
embeddings (i.e., LVT) from Pennington et al. (2014) (Glove), (3) unsuPervised document rePresen-
tation, where we emPloy doc2vec (Le & Mikolov, 2014) and EmbSum (to rePresent a document by
summing the embedding vectors of its words using Glove) in order to quantify the quality of doc-
ument rePresentations, (4) zero-shot toPic modeling, where we use all source corPora and no target
corPus, and (5) data-augmentation, where we use all source corPora along with a target corPus for
TM on T. Using DocNADE, we first PrePare the two KBs: WordPool and TopicPool from each of
the source corPora and then use them in knowledge transfer to T.
Tables	5 and 6 show the comParison of our ProPosed transfer learning aPProaches (i.e., LVT using
WordPool, GVT using TopicPool, MVT and MST) with the baselines TMs that (1) do not, and (2)
do emPloy Pretrained word embeddings (e.g., DocNADE and DocNADEe, resPectively).
3.1	Generalization: Perplexity (PPL)
To evaluate generative Performance of TM, we estimate the log-Probabilities for the test documents
and compute the average held-out perplexity per word as, PPL = exp (-寺 PN=I 隔 logP(Vt)),
where N and |vt | are the number of documents and words in a document vt , resPectively.
1selected with grid search; suboptimal results (see appendices) by learning λ and γ with backpropagation
6
Under review as a conference paper at ICLR 2020
KBs from Source Corpus	Model/ Transfer Type	Scores on Target Corpus (in sparse-data and sufficient-data settings)													
		20NSshort			TMNtitle			R21578title			20NSsmall			TMN	
		PPL	COH	IR	PPL	COH	IR	PPL	COH	IR	PPL	COH	IR	PPL	COH
Baseline TM	NVDM	1047	.736	.076	973	.740	.190	372	.735	.271	957	.515	.090	833	.673
without Word-	ProdLDA	923	.689	.062	1527	.744	.170	480	.742	.200	1181	.394	.062	1519	.577
Embeddings	DocNADE	646	.667	.290	706	.709	.521	192	.713	.657	594	.462	.270	584	.636
	LVT	630	.673	.298	705	.709	.523	194	.708	.656	594	.455	.288	582	.649
20NS	GVT	646	.690	.303	718	.720	.527	184	.698	.660	594	.500	.310	590	.652
	MVT	638	.690	.314	714	.718	.528	188	.715	.655	600	.499	.311	588	.650
	LVT	649	.668	.296	655	.731	.548	187	.703	.659	593	.460	.273	-	-
TMN	GVT	661	.692	.294	689	.728	.555	191	.709	.660	596	.521	.276	-	-
	MVT	658	.687	.297	663	.747	.553	195	.720	.660	599	.507	.292	-	-
	LVT	656	.667	.292	704	.715	.522	186	.715	.676	593	.458	.267	581	.636
R21578	GVT	654	.672	.293	716	.719	.526	194	.706	.672	595	.485	.279	591	.646
	MVT	650	.670	.296	716	.720	.528	194	.724	.676	599	.490	.280	589	.650
	LVT	650	.677	.297	682	.723	.533	185	.710	.659	592	.458	.260	564	.668
AGnews	GVT	667	.695	.300	728	.735	.534	190	.717	.663	598	.563	.282	601	.684
	MVT	659	.696	.290	718	.740	.533	189	.727	.659	599	.566	.279	592	.686
	LVT	640	.678	.308	663	.732	.547	182	.739	.673	594	.542	.277	568	.674
MST	GVT	658	.705	.305	704	.746	.550	192	.727	.673	599	.585	.326	602	.680
	MVT	656	.740	.314	680	.752	.569	188	.745	.685	600	.637	.285	600	.690
Gain% (vs DocNADE)		1.23	10.9	8.28	7.22	6.06	9.21	5.20	4.49	4.26	0.34	37.9	20.7	3.42	8.50
Table 5: State-of-the-art comparisons with TMs: Perplexity (PPL), topic coherence (COH) and
precision (IR) at retrieval fraction 0.02. Scores are reported on each of the target, given KBs from
one or several sources. Please read column-wise. Bold: best in column. Gain%: Bold vs DocNADE.
KBs from Source Corpus	Model/ Transfer Type	Scores on Target Corpus (in sparse-data and sufficient-data settings)													
		20NSshort			TMNtitle			R21578title			20NSsmall			TMN	
		PPL	COH	IR	PPL	COH	IR	PPL	COH	IR	PPL	COH	IR	PPL	COH
	doc2vec	-	-	.090	-	-	.190	-	-	.518	-	-	.200	-	-
	EmbSum	-	-	.236	-	-	.513	-	-	.587	-	-	.214	-	-
Baseline TM	Gauss-LDA	-	-	".080"	-	-	^.408	-	-	：367	-	-	^.090"	-	-
with Word-	glove-DMM	-	.512	.183	-	.633	.445	-	.364	.273	-	.578	.090	-	.705
Embeddings	DocNADEe	629	.674	.294	680	.719	.540	187	.721	.663	590	.455	.274	572	.664
20NS	MVT+Glove	630	.721	.320	688	.741	.565	183	.724	.667	597	.561	.306	570	.693
TMN	MVT+Glove	640	.731	.295	673	.750	.576	184	.716	.672	599	.594	.261	-	-
R21578	MVT+Glove	633	.705	.295	689	.738	.540	185	.737	.691	595	.485	.255	577	.697
AGnews	MVT+Glove	642	.734	.302	706	.748	.565	190	.734	.675	598	.573	.284	585	.703
MST	MVT+Glove	644	.739	.304	673	.752	.570	183	.742	.684	598	.631	.282	582	.710
	+ FastText	654	.741	.313	673	.751	.578	183	.744	.684	599	.634	.254	582	.711
Gain% (vs DocNADEe)		-	9.95	8.84	1.03	4.60	7.04	2.14	3.20	4.22	-	39.3	2.92	.35	7.08
Table 6: State-of-the-art comparisons with TMs using word embeddings: PPL, COH and IR at re-
trieval fraction 0.02. Scores are reported on each of the target, given KBs. Here, MVT: LVT+GVT
(Table 5), DocNADEe: DocNADE+Glove and Gain%: Bold vs DocNADEe. For all the configura-
tions, we apply a projection on word embeddings concatenated from several sources.
Tables 5 and 6 quantitatively show PPL scores on the five target corpora (four short-text and one
long-text) by the baselines and proposed transfer learning approaches (i.e., GVT, MVT and MST)
using one or four sources. In Table 5 using TMN (as a single source) for LVT, GVT and MVT on
TMNtitle, we see improved (reduced) PPL scores: (655 vs 706), (689 vs 706) and (663 vs 706)
respectively in comparison to DocNADE. We also observe gains due to MST+LVT, MST+GVT and
MST+MVT configurations on TMNtitle. Similarly in MST+LVT for R21578title, we observe
a gain of 5.2% (182 vs 192), suggesting that transfer learning using pretrained word and topic em-
beddings (jointly) from one or many sources helps due to positive knowledge transfer, and it also
verifies domain relatedness (e.g., in TMN-TMNtitle and AGnews-TMN). Similarly, Table 6 shows
gains in PPL (e.g., on TMNtitle, R21578title, etc.) compared to DocNADEe.
7
Under review as a conference paper at ICLR 2020
KBs from Source Corpus	Model/ Transfer Type	Scores on Target Corpus	
		Ohsumedtitle PPL COH IR	Ohsumed PPL COH IR
baselines	ProdLDA DocNADE EmbSum DocNADEe	1121 ~.734 .080 1321 .728 .160 -	-	.150 1534 .738 .175	1677 .646 .080 1706 .662 .184 -	-	.148 1637 .674 .183
AGnews	LVT GVT MVT + BioEmb	1587~.732 .160 1529 .732 .160 1528 .734 .160 1488 .747 .176	1717 .657 .184 1594 .665 .185 1598 .666 .184 1595 .681 .187
PubMed	LVT GVT MVT + BioEmb	1268^^.732 .172 1392 .740 .173 1408 .743 .178 1364 .753 .182	1535 .669 .190 1718 .671 .192 1514 .674 .191 1633 .689 .191
MST	LVT GVT MVT + BioEmb + BioFastText	1268^^.733 .172 1391	.740	.172 1399 .744 .177 1375 .751 .180 1350 .753 .178	1536 .668 .190 1504 .666 .192 1607 .679 .191 1497 .693 .190 1641 .688 .187
Gain% (vs DocNADE) Gain% (vs DocNADEe)		-4.01 ~3.43 13.8 17.3	2.03 4.00	12.3~4.08 4.35 8.55	2.22 4.91
Table 7: PPL, COH, IR at retrieval fraction 0.02.
BioEmb and BioFastText: 200-dimensional word
vectors from large biomedical corpus (Moen &
Ananiadou, 2013). + BioEmb: MVT+BioEmb.
T	S	Model	Topic-words (Top 5)
20NSshort	20NS	DNE	shipping, sale, prices, expensive, price
		-GVT 一 +GVT	sale, price, monitor, site, setup shipping, sale, price, expensive, subscribe
	AGnews	DNE	microsoft, software, ibm, linux, computer
		-GVT^ +GVT	apple, modem, side, baud, perform microsoft, software, desktop, computer, apple
TMNtitle	AGnews	DNE	miners, earthquake, explosion, stormed, quake
	TMN	DNE ^	tsunami, quake, japan, earthquake, radiation
		-GVT^ +GVT	strike, jackson, kill, earthquake, injures earthquake, radiation, explosion, wildfire
Table 8: Source S and target T topics before (-)
and after (+) topic transfer(s) (GVT) from one or
more sources. DNE: DocNADE
chip				
source corpora			target corpus	
20NS	R21578	AGnews	20NSshort	
			-GVT	+GVT
key	chips	chips	virus	chips
encrypted	semiconductor	chipmaker	intel	technology
encryption	miti	processors	gosh	intel
clipper	makers	semiconductor	crash	encryption
keys	semiconductors	intel	chips	clipper
Table 9: Five nearest neighbors of the word chip
in source and target semantic spaces before (-)
and after (+) knowledge transfer (MST+GVT)
In Table 7, we show PPL scores on two medical target corpora: Ohsumtitle and Ohsumed using
two sources: AGnews (news corpus) and PubMed (medical abstracts) to perform cross-domain and
in-domain knowledge transfers. We see that using PubMed for LVT on both the target corpora
improves generalization. Overall, we report a gain of 17.3% (1268 vs 1534) on Ohsumtitle and
8.55% (1497 vs 1637) on Ohsumtitle, compared to DocNADEe. Additionally, MST+GVT and
MST+MVT boost generalization performance compared to DocNADE(e).
3.2	Interpretab ilty: Topic Coherence (COH)
While PPL is used for model selection, adjusting parameters (e.g. H) and quantitative comparisons,
Chang et al. (2009) showed in some cases humans preferred TMs (based on the semantic quality of
topics) with higher (worse) PPLs. Thus beyond perplexity, we compute topic coherence to estimate
the meaningfulness of words in each of the topics captured. In doing so, we choose the coherence
measure proposed by Roder et al. (2015) that identifies context features for each topic word using
a sliding window over the reference corpus. We follow Gupta et al. (2019) and compute COH with
the top 10 words in each topic. Essentially, the higher scores imply the more coherent topics.
Tables 5 and 6 (under COH column) demonstrate that our proposed approaches (GVT, MVT and
MST) of transfer learning in TMs show noticeable gains in COH and thus, improve topic quality.
For instance in Table 5, when AGnews is used as a single source for 20NSsmall datatset, we observe
a gain in COH due to GVT (.563 vs .462) and MVT (.566 vs .462). Additionally, noticeable gains
are reported due to MST+LVT (.542 vs .462), MST+GVT (.585 vs .462) and MST+MVT (.637
vs .462), compared to DocNADE. Importantly, we find a trend MVT>GVT>LVT in COH scores
for both the single-source and multi-source transfers. Similarly, Table 6 show noticeable gains
(e.g., 39.3%, 9.95%, 7.08%, etc.) in COH due to MST and MVT with Glove and FastText word
embeddings. Moreover, Table 7 shows gains in COH due to GVT on Ohsumedtitle and Ohsumed,
using pretrained knowledge from PubMed. Overall, the GVT, MVT and MST boost COH for all the
five target corpora compared to the baseline TMs (i.e., DocNADE and DocNADEe). This suggests
that there is a need for the two complementary (pretrained word and topics) representations and
multi-source transfer learning in order to guide meaningful topic learning in T. The results on both
the low- and high-resource targets across domains conclude that the proposed modeling scales.
8
Under review as a conference paper at ICLR 2020
noisicerP
57 56 5
.7 0. 6. 0. .5
000
62843628
4433 .221
.............. ...
0000 000
noisicerP
MST+MVT
DocNADEe
—EmbSum
—b— zero-shot
—data-augment
0.001 0.002 0.005 0.01	0.02 0.05	0.1
Fraction of Retrieved Documents (Recall)
(a) IR: 20NSshort
uoɪ-oajd
Fraction of Retrieved Documents (Recall)
UOI-OaJd
Fraction of Retrieved Documents (Recall)
÷ MST+MVT
DocNADEe
—EmbSum
—b— zero-shot
—data-augment
0.002 0.005 0.01	0.02 0.05	0.1	0.2
Fraction of Retrieved Documents (Recall)
(d) IR: R21578title
(b) IR: 20NSsmall
8
4
0.
an
20%	40%	60%	80%
Fraction of training set
(e) IR: TMNtitle
100%
(c) IR: TMNtitle
57 56
.7 0. .6 0.
00
)HOC( ecnerehoc cipoT
0.55
TMNtitle	Ohsumed
(f) COH: Zero-shot & DA
Figure 2: (a, b, c, d) Retrieval performance (precision) on four datasets. (e) Precision at recall
fraction 0.02, each for a fraction (20%, 40%, 60%, 80%, 100%) of the training set of TMNtitle. (f)
Zero-shot and data-augmentation (DA) experiments for topic coherence on TMNtitle and Ohsumed.
3.3	Applicability: Information Retrieval (IR)
For a greater impact of TMs, we further evaluate the quality of document representations and per-
form a document retrieval task on the target datasets, using their label information only to compute
precision. We follow the experimental setup similar to Lauly et al. (2017), where all test documents
are treated as queries to retrieve a fraction of the closest documents in the original training set using
cosine similarity between their document vectors. To compute retrieval precision for each fraction
(e.g., 0.02), we average the number of retrieved training documents with the same label as the query.
Tables 5 and 6 depict precision scores at retrieval fraction 0.02 (similar to Gupta et al. (2019)), where
the configuration MST+MVT outperforms both the DocNADE and DocNADEe, respectively in
retrieval performance on the four target (short-text) datasets. A gain in IR performance is noticeable
for highly overlapping domains, e.g., TMN-TMNtitle (.555 vs .521 in Table 5 and .576 vs .540 in
Table 6) than the related, e.g., AGnews-TMNtitle (.534 vs .521 in Table 5 and .565 vs .540 in Table
6). We observe large gains in precision at retrieval fraction 0.02: (a) Table 5: 20.7% (.326 vs .270)
on 20NSsmall, 9.21% (.569 vs .521) on TMNtitle and 8.28% (.314 vs .290) on 20NSshort, (b)
Table 6: 8.84% (.320 vs .294) on 20NSshort and 9.21% (.578 vs.540) on TMNtitle, and (c) Table
7: 4.91% (.192 vs .183) on Ohsumed and 4.0% (.182 vs .175) on Ohsumedtitle.
Additionally, Figures 2a, 2b, 2c and 2d illustrate precision on 20NSshort, 20NSsmall, TMNtitle
and R21578title, respectively, where our approaches (MST+GVT and MST+MVT) consistently
outperform the baselines at all fractions. Moreover, we split the training data of TMNtitle into
several sets: 20%, 40%, 60%, 80% of the training set and then retrain DocNADE, DocNADEe
and DocNADE+MST+MVT. We demonstrate the impact of transfer learning in sparse-data settings
using WordPool and TopicPool jointly on IR task. Figure 2e plots precision at retrieval (recall)
fraction 0.02 and demonstrates that the proposed modeling consistently outperform DocNADE(e).
3.4	Zero-shot and Data-augmentation Evaluations
Figures 2a, 2b, 2c and 2d show precision in the zero-shot (source-only training) and data-
augmentation (source+target training) configurations. Observe that the latter helps in learning
9
Under review as a conference paper at ICLR 2020
meaningful representations and performs better than zero-shot; however, it is outperformed by
MST+MVT, suggesting that a naive (data space) augmentation does not add sufficient prior or rel-
evant information to the sparse target. Thus, we find that it is beneficial to augment training data
in feature space (e.g., LVT, GVT and MVT) especially for unsupervised TMs using WordPool and
TopicPool. Beyond IR, we further investigate computing topic coherence (COH) for zero-shot and
data-augmentation baselines, where the COH scores (Figure 2f) suggest that MST+MVT outper-
forms DocNADEe, zero-shot and data-augmentation.
3.5	Qualitative Analysis: Topics and Nearest Neighbors (NN)
For topic level inspection, we first extract topics using the rows of W of source and target corpora.
Table 8 shows the topics (top-5 words) from source and target domains. Observe that the target
topics become more coherent after transfer learning (i.e., +GVT) from one or more sources. The
blue color signifies that a target topic has imitated certain topic words from the source. Observe that
we also show topics from source domain(s) that align with the topics from target.
For word level inspection, we extract word representations using the columns of W. Table 9 shows
nearest neighbors (NNs) of the word chip in 20NSshort (target) corpus, before and after GVT using
three knowledge sources. Observe that the NNs in the target become more meaningful.
4 Conclusion
Within neural topic modeling, we have introduced transfer learning approaches using complemen-
tary representations: pretrained word (local semantics) and topic (global semantics) embeddings
exclusively or jointly from one or many sources (i.e., multi-view and multi-source). We have shown
that the proposed approaches better deal with data-sparsity issues, especially in a short-text and/or
small document collection. We have demonstrated learning meaningful topics and quality document
representations on 7 (low- and high-resource) target corpora from news and medical domains.
References
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic
language model. Journal of Machine Learning Research, 3:1137-1155, 2003. URL http:
//www.jmlr.org/papers/v3/bengio03a.html.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993-1022, 2003. URL http://www.jmlr.org/papers/v3/
blei03a.html.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors
with subword information. TACL, 5:135-146, 2017.
Bin Cao, Sinno Jialin Pan, Yu Zhang, Dit-Yan Yeung, and Qiang Yang. Adaptive transfer learning. In
Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta,
Georgia, USA, July 11-15, 2010. AAAI Press, 2010. URL http://www.aaai.org/ocs/
index.php/AAAI/AAAI10/paper/view/1823.
Jonathan Chang, Jordan L. Boyd-Graber, Sean Gerrish, Chong Wang, and David M. Blei. Reading
tea leaves: How humans interpret topic models. In Advances in Neural Information Processing
Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceed-
ings of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada., pp. 288-296,
2009.
Rajarshi Das, Manzil Zaheer, and Chris Dyer. Gaussian lda for topic models with word embeddings.
In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and
the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),
pp. 795-804. Association for Computational Linguistics, 2015. doi: 10.3115/v1/P15- 1077. URL
http://aclweb.org/anthology/P15-1077.
10
Under review as a conference paper at ICLR 2020
Pankaj Gupta, Yatin Chaudhary, Florian Buettner, and Hinrich Schutze. Document informed neural
autoregressive topic models with distributional prior. In Proceedings of the Thirty-Third AAAI
Conference on Artificial Intelligence, 2019. URL http://arxiv.org/abs/1809.06709.
Hugo Larochelle and Stanislas Lauly. A neural autoregressive topic model. In Peter L. Bartlett, Fer-
nando C. N. Pereira, Christopher J. C. Burges, Leon Bottou, and Kilian Q. Weinberger (eds.),
Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural
Information Processing Systems, pp. 2717-2725, 2012. URL http://papers.nips.cc/
paper/4613- a- neural- autoregressive- topic- model.
Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Geoffrey J.
Gordon, David B. Dunson, and Miroslav Dudlk (eds.), Proceedings of the Fourteenth Interna-
tional Conference on Artificial Intelligence and Statistics, AISTATS, volume 15 of JMLR Pro-
ceedings, pp. 29-37. JMLR.org, 2011.
Stanislas Lauly, Yin Zheng, Alexandre Allauzen, and Hugo Larochelle. Document neural autore-
gressive distribution estimation. Journal of Machine Learning Research, 18:113:1-113:24, 2017.
URL http://jmlr.org/papers/v18/16-017.html.
Quoc V. Le and Tomas Mikolov. Distributed representations of sentences and documents. In
Proceedings of the 31th International Conference on Machine Learning, ICML, volume 32 of
JMLR Workshop and Conference Proceedings, pp. 1188-1196. JMLR.org, 2014. URL http:
//jmlr.org/proceedings/papers/v32/le14.html.
Yishu Miao, Lei Yu, and Phil Blunsom. Neural variational inference for text processing. In Pro-
ceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City,
NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pp.
1727-1736. JMLR.org, 2016.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed
representations of words and phrases and their compositionality. In Christopher J. C.
Burges, Leon Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger (eds.), Advances in
Neural Information Processing Systems 26: 27th Annual Conference on Neural Informa-
tion Processing Systems, pp. 3111-3119, 2013. URL http://papers.nips.cc/paper/
5021-distributed-representations-of-words-and-phrases-and-their-compositionality.
SPFGH Moen and Tapio Salakoski2 Sophia Ananiadou. Distributional semantics resources for
biomedical text processing. Proceedings of LBM, pp. 39-44, 2013.
Dat Quoc Nguyen, Richard Billingsley, Lan Du, and Mark Johnson. Improving topic models with
latent feature word representations. TACL, 3:299-313, 2015. URL https://tacl2013.cs.
columbia.edu/ojs/index.php/tacl/article/view/582.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pp. 1532-1543. Association for Computational Linguistics, 2014.
doi: 10.3115/v1/D14-1162. URL http://aclweb.org/anthology/D14-1162.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers), pp. 2227-2237. Association for Computational
Linguistics, 2018. doi: 10.18653/v1/N18-1202. URL http://aclweb.org/anthology/
N18-1202.
Michael Roder, Andreas Both, and Alexander Hinneburg. Exploring the space of topic coherence
measures. In Proceedings of the Eighth ACM International Conference on Web Search and Data
Mining, WSDM 2015, Shanghai, China, February 2-6, 2015, pp. 399-408. ACM, 2015. URL
https://doi.org/10.1145/2684822.2685324.
Ruslan Salakhutdinov and Geoffrey E. Hinton. Replicated softmax: an undirected topic
model. In Yoshua Bengio, Dale Schuurmans, John D. Lafferty, Christopher K. I.
11
Under review as a conference paper at ICLR 2020
Williams, and Aron Culotta (eds.), Advances in Neural Information Processing Sys-
tems 22: 23rd Annual Conference on Neural Information Processing Systems, pp.
1607-1614. Curran Associates, Inc., 2009. URL http://papers.nips.cc/paper/
3856-replicated-softmax-an-undirected-topic-model.
Akash Srivastava and Charles Sutton. Autoencoding variational inference for topic models. In 5th
International Conference on Learning Representations, ICLR, 2017. URL https://arxiv.
org/pdf/1703.01488.pdf.
A Data Description
In order to evaluate knowledge transfer within unsupervised neural topic modeling, we use the fol-
lowing seven datasets in the target domain T following the similar experimental setup as in Doc-
NADEe:
1.	20NSshort: We take documents from 20NewsGroups data, with document size less (in
terms of number of words) than 20.
2.	20NSsmall: We sample 20 document (each having more than 200 words) for training
from each class of the 20NS dataset. For validation and test, 10 document for each class.
Therefore, it is a corpus of few (long) documents.
3.	TMNtitle: Titles of the Tag My News (TMN) news dataset.
4.	R21578title: Reuters corpus, a collection of new stories from nltk.corpus. We
take titles of the documents.
5.	Ohsumedtitle: Titles of Ohsumed abstracts. Source: disi.unitn.it/
moschitti/corpora.htm.
6.	Ohsumed: Ohsumed dataset, collection of medical abstracts. Source: disi.unitn.
it/moschitti/corpora.htm.
7.	TMN: The Tag My News (TMN) news dataset.
To prepare knowledge base of word embedings (local semantics) and latent topics (global semantics)
features, we use the following six datasets in the source S:
1.	20NS: 20NewsGroups corpus, a collection of news stories from nltk.corpus.
2.	TMN: The Tag My News (TMN) news dataset.
3.	R21578: Reuters corpus, a collection of new stories from nltk.corpus.
4.	AGnews: AGnews data sellection.
5.	PubMed: Medical abstracts of randomized controlled trials. Source: https://
github.com/Franck-Dernoncourt/pubmed-rct.
B Getting Word and Latent Topic Representations from
Source(s)
Since in DocNADE, the column of W:,vi gives a word vector of the word vi, therefore the dimension
of word embeddings in each of the Ek is same (i.e., H = 200). Thus, we prepare the knowledge
base of word representations Ek from kth source using DocNADE, where each word vector is of
H = 200 dimension.
Since the row vector of Wj,: in DocNADE encodes jth topic feature, therefore each latent topic
(i.e., row) in feature matrix W is a vector of K dimension, corresponding the definition of topics
that it is a distribution over vocabulary. H is the number of latent topics and K is the vocabulary
size, where K varies across corpora. Thus, we train DocNADE to learn a feature matrix specific to
each of the source corpora, e.g. Wk ∈ RH×K of kth source.
For a target corpus of vocabulary size K0, the DocNADE learns a feature matrix WT ∈ RH×K0.
Similarly, Wk ∈ RH×K for kth source of vocabulary size K. Since in the sparse-data setting for the
12
Under review as a conference paper at ICLR 2020
data	labels / classes
TMNtitle and TMN	world, us, sport, business, ScLtech, entertainment, health
AGnews	business, ScLtech, sports, world
20NSshort, 20NSsmall, 20NS	misc.forsale, comp.graphics, rec.autos, comp.windows.x, rec.sport.baseball, sci.space, rec.sport.hockey, soc.religion.christian, rec.motorcycles, comp.sys.mac.hardware, talk.religion.misc, sci.electronics, comp.os.ms-windows.misc, sci.med, comp.sys.ibm.pc.hardware, talk.politics.mideast, talk.politics.guns, talk.politics.misc, alt.atheism, sci.crypt
R21578title and R21578	trade, grain, crude, nat-gas, corn, rice, rubber, sugar, tin, palm-oil, veg-oil, ship, coffee, lumber, wheat, gold, acq, interest, money-fx, copper, ipi, carcass, livestock, oilseed, soybean, earn, bop, gas, lead, jobs, zinc, cpi, gnp, soy-oil, dlr, yen, nickel, groundnut, heat, sorghum, sunseed, pet-chem, cocoa, rapeseed, cotton, money-supply, iron-steel, l-cattle, alum, palladium, platinum, strategic-metal, reserves, groundnut-oil, lin-oil, meal-feed, rape-oil, sun-meal, sun-oil, hog, barley, potato, orange, retail, soy-meal, cotton-oil, oat, fuel, silver, income, wpi, tea, lei, coconut, coconut-oil, copra-cake, dfl, dmk, naphtha, propane, instal-debt, nzdlr, housing, nkr, rye, castor-oil, jet, palmkernel, cpu, rand
Table 10: Label space of the corpora used
Hyperparameter	Search Space
retrieval fraction	[0.02]
learning rate	[0.001]
hidden units, H	[200]
activation function (g)	sigmoid
iterations	[100]
λk	[1.0, 0.5, 0.1]
γk	[0.1, 0.01, 0.001]
Table 11: Hyperparameters in Generalization in DocNADE, DocNADEe, LVT, GVT and MVT
configurations for 200 topics
target, K0 << K due to additional word in the source. In order to perform GVT, we need the same
topic feature dimensions in the target and source, i.e., K0 of the target. Therefore, we remove those
k H ×K
column vectors from W ∈ R	of the kth source for which there is no corresponding word in
the vocabulary of the target domain. As a result, we obtain Zk as a latent topic feature matrix to
be used in knowledge transfer to the target domain. Following the similar steps, we prepare a KB
of Zs such that each latent topic feature matrix from a source domain gets the same topic feature
dimension as the target.
C Experimental Setup
For DocNADE and DocNADEe in different knowledge transfer configurations, we follow the same
experimental setup as in DocNADE and DocNADEe. We rerun DocNADE and DocNADEe using
the code released for DocNADEe.
C.1 Experimental Setup for Generalization
We set the maximum number of training passes to 100, topics to 200 and the learning rate to 0.001
with sigmoid hidden activation. Since the baseline DocNADE and DocNADEe reported better scores
in PPL for H = 200 topics than using 50, therefore we use H = 200 in our experiments.
See Table 11 for hyperparameters used in generalization task, i.e., computing PPL.
See section C.4 to reproduce scores of Table 1.
C.2 Experimental Setup for IR Task
We set the maximum number of training passes to 100, topics to 200 and the learning rate to 0.001
with tanh hidden activation. Since the baseline DocNADE and DocNADEe reported better scores in
13
Under review as a conference paper at ICLR 2020
Hyperparameter	Search Space
retrieval fraction	[0.02]
learning rate	[0.001]
hidden units, H	[200]
activation function (g)	tanh
iterations	[100]
λk	[1.0, 0.5, 0.1]
γk	[0.1, 0.01, 0.001]
Table 12: Hyperparameters search in the Information (text) Retrieval task, where λk and γk are
weights for kth source. We use the same grid-search for all the source domains. Hyperparameters
in IR task in DocNADE, DocNADEe, LVT, GVT and MVT configurations for 200 topics
setting	KBs from Source Corpus	Model/	Scores on Target Corpus (in sparse-data setting)											
		Transfer Type	20NSshort			TMNtitle			R21578title			20NSsmall		
			PPL	COH	IR	PPL	COH	IR	PPL	COH	IR	PPL	COH	IR
		LVT	667	.661	.308	670	.730	.535	183	.716	.661	610	.440	.286
parameterized	MST	GVT	651	.658	.285	701	.712	.523	190	.701	.656	602	.460	.273
		MVT	667	.660	.309	667	.730	.535	183	.714	.661	608	.441	.293
		+ Glove	662	.677	.296	672	.731	.540	183	.716	.662	634	.412	.207
		LVT	640	.678	.308	663	.732	.547	182	.739	.673	594	.542	.277
hyper-parameterized	MST	GVT	658	.705	.305	704	.746	.550	192	.727	.673	599	.585	.326
		MVT	656	.740	.314	680	.752	.569	188	.745	.685	600	.637	.285
Table 13: {λ, γ} as Parameter vs Hyperparameters: Perplexity (PPL), topic coherence (COH) and
precision (IR) at retrieval fraction 0.02, when λ and γ are (1) learned with backpropagation, and (2)
treated as hyperparameters. The experimental results suggest that the second configuration performs
better the former. Therefore, in the paper we have reported scores considering {λ, γ} as hyperpa-
rameters. + Glove: MVT+Glove embeddings. Please read column-wise. Bold: best in column.
precision for the retrieval task for H = 200 topics than using 50, therefore we use H = 200 in our
experiments. We follow the similar experimental setup as in DocNADEe. For model selection, we
used the validation set as the query set and used the average precision at 0.02 retrieved documents
as the performance measure. Note that the labels are not used during training. The class labels are
only used to check if the retrieved documents have the same class label as the query document. To
perform document retrieval, we use the same (Table 2) train/development/test split of documents for
all the datasets during learning.
Given DocNADE, the representation of a document of size D can be computed by taking the last
hidden vector hD at the autoregressive step D. Since, the RSM and DocNADE strictly outperformed
LDA, therefore we only compare DocNADE and its recent extension DocNADEe. We use the same
number of topic dimensions (H = 200) across all the source domains and the target in training with
DocNADE.
See Table 12 for the hyperparameters in the document retrieval task, where λk and γk are weights
for kth source. We use the same grid-search for all the source domains. We set γk smaller than λk to
control the degree of imitation of the source domain(s) by the target domain. We use the development
set of the target corpus to find the optimal setting in different configurations of knowledge transfers
from several sources.
See section C.4 to reproduce scores of Table 5.
C.3 {λ, γ} AS PARAMETER VS HYPERPARAMETERS
Here, we treat λ and γ as parameters of the model, instead of hyperparameters and learn them with
backpropagation. We initialize each λk = 0.5 and γk = 0.01 for each of the sources. We perform
experiments on short-text datasets in MST+LVT, MST+GVT and MST+MVT configurations. We
evaluate the topic modeling using PPL, topic coherence and retrieval accuracy.
14
Under review as a conference paper at ICLR 2020
Table 13 reports the scores, when λ and γ are (1) learned with backpropagation, and (2) treated as
hyperparameters. The experimental results suggest that the second configuration performs better the
former. Therefore in the work, we have reported scores considering {λ, γ} as hyperparameters.
C.4 REPRODUCIBILITY: OPTIMAL CONFIGURATIONS OF λ AND γ
As mentioned in Tables 11 and 12, the hyper-parameter λk takes on values in [1.0, 0.5, 0.1] for each
of the word embeddings matrix Ek and γk in [0.1, 0.01, 0.001] for each of the latent topic features
Zk, respectively for the kth source domain. To determine an optimal configuration, we perform
grid-search over the values and use the scores on the development set to determine the best setting.
We have a common model for PPL and COH scores due to generalization criteria.
To reproduce scores (best in Table 5), we mentioned the best settings of (λk , γk) in MST+MVT
configuration for each of the target and source combinations:
1.	Generalization (PPL and COH) in MST+MVT when target is 20NSshort: (λ20N S =
1.0, γ20NS = 0.001, λTMN = 0.1, γTMN = 0.001, λR21578 = 0.5, γR21578 = 0.001,
λAGnews = 0.1, γAGnews = 0.001
2.	Generalization (PPL and COH) in MST+MVT when target is TMNtitle: (λ20NS = 0.1,
γ20NS = 0.001, λTMN = 1.0, γTMN = 0.001, λR21578 = 0.5, γR21578 = 0.001,
λAGnews = 1.0, γAGnews = 0.001
3.	Generalization (PPL and COH) in MST+MVT when target is R21578title: (λ20N S =
0.1, γ20NS = 0.001, λTMN = 0.5, γTMN = 0.001, λR21578 = 1.0, γR21578 = 0.001,
λAGnews = 1.0, γAGnews = 0.001
4.	Generalization (PPL and COH) in MST+MVT when target is 20NSsmall: (λ20N S =
0.5, γ20NS = 0.001, λTMN = 0.1, γTMN = 0.001, λR21578 = 0.1, γR21578 = 0.001,
λAGnews = 0.1, γAGnews = 0.001
5.
Generalization (PPL and COH) in MST+MVT when target is Ohsumedtitle:
(λAGnews = 0.1, γAGnews = 0.001, λPubMed = 1.0,γPubMed = 0.001
6.	Generalization (PPL and COH) in MST+MVT when target is Ohsumed: (λAGnews
0.1, γAGnews = 0.001, λPubMed = 1.0,γPubMed = 0.001
7.	IR in MST+MVT when target is 20NSshort: (λ20NS = 1.0, γ20NS = 0.1, λTMN =
0.5, γTMN = 0.01, λR21578 = 0.1, γR21578 = 0.001, λAGnews = 1.0,γAGnews = 0.01
8.	IR in MST+MVT when target is TMNtitle: (λ20N S = 0.1, γ20NS = 0.01, λTMN =
1.0,γTMN = 0.01, λR21578 = 0.1, γR21578 = 0.01, λAGnews = 0.5, γAGnews = 0.001
9.	IR in MST+MVT when target is R21578title: (λ20N S = 0.1, γ20NS = 0.01,
λTMN = 1.0, γTMN = 0.01, λR21578 = 1.0, γR21578 = 0.01, λAGnews = 0.5,
γAGnews = 0.001
10.	IR in MST+GVT when target is 20NSsmall: (γ20NS = 0.01, γTMN = 0.01, γR21578 =
0.1, γAGnews = 0.01
11.	IR in MST+MVT when target is Ohsumedtitle: (λAGnews = 0.1, γAGnews = 0.001,
λP ubM ed = 1.0, γPubMed = 0.1
12.	IR in MST+MVT when target is Ohsumed: (λAGnews = 0.1, γAGnews = 0.001,
λP ubM ed = 0.5, γPubMed = 0.1
The hyper-parameters mentioned above also applies to a single source transfer configuration.
Additionally, we have also provided the code.
While DocNADEe requires the dimension (i.e., E) of word embeddings be the same as the latent
topic (i.e., H), we first apply a projection on the concatenation of the pretrained word embeddings
obtained from several sources and then, introduce the prior knowledge in each of the autoregressive
step following DocNADEe. We apply it in configurations where Glove and/or FastText (E=300) are
employed. IN these settings, we use a single mixture weight λ ∈ [1.0, 0.5, 0.1] over the projected
vector and then, introduced in TM following DocNADEe.
15
Under review as a conference paper at ICLR 2020
C.5 Experimental Setup for NVDM and ProdLDA
For NVDM, we run the code availale at github.com/ysmiao/nvdm and train for 200 topics.
For ProdLDA, we run the code availale at github.com/akashgit/autoencoding_vi_
for_topic_models and train for 200 topics.
C.6 Experimental Setup for glove-DMM
We used LFTM (https://github.com/datquocnguyen/LFTM) to train glove-DMM
model. It is trained for 200 iterations with 2000 initial iterations using 200 topics. For short texts,
we set the hyperparameter beta to 0.1, for long texts to 0.01; the mixture parameter lambda was set
to 0.6 for all datasets. IR task was performed using relative topic proportions as input, where we
inferred the topic distribution of the training and test documents and used the relative distribution as
input in computing similarities in documents based on the inferred relative topic distribution.
C.7 Experimental Setup for doc2vec
We used gensim (https://github.com/RaRe-Technologies/gensim) to train
Doc2Vec models. Models were trained with distributed bag of words, for 1000 iterations using a
window size of 5 and a vector size of 500.
16