Under review as a conference paper at ICLR 2020
Multi-step Greedy Policies in Model-Free
Deep Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Multi-step greedy policies have been extensively used in model-based Reinforce-
ment Learning (RL) and in the case when a model of the environment is available
(e.g., in the game of Go). In this work, we explore the benefits of multi-step
greedy policies in model-free RL when employed in the framework of multi-step
Dynamic Programming (DP): multi-step Policy and Value Iteration. These al-
gorithms iteratively solve short-horizon decision problems and converge to the
optimal solution of the original one. By using model-free algorithms as solvers
of the short-horizon problems we derive fully model-free algorithms which are
instances of the multi-step DP framework. As model-free algorithms are prone
to instabilities w.r.t. the decision problem horizon, this simple approach can help
in mitigating these instabilities and results in an improved model-free algorithms.
We test this approach and show results on both discrete and continuous control
problems.
1	Introduction
The field of Reinforcement learning (RL) span a wide variety of algorithms for solving decision-
making problems through repeated interaction with the environment. By incorporating deep neural
networks into RL algorithms, the field of RL has recently witnessed remarkable empirical success
(e.g., Mnih et al. 2015; Lillicrap et al. 2015; Levine et al. 2016; Silver et al. 2017). Much of this
success had been achieved by model-free RL algorithms, such as Q-learning and policy gradient.
These algorithms are known to suffer from high variance in their estimations (Greensmith et al.,
2004) and to have difficulties handling function approximation (e.g., Thrun & Schwartz 1993; Baird
1995; Van Hasselt et al. 2016; Lu et al. 2018). These problems are intensified in decision problems
with long horizon, i.e., when the discount factor, γ, is large. Although using smaller values of
γ addresses the γ-dependent issues and leads to more stable algorithms (Petrik & Scherrer, 2009;
Jiang et al., 2015), it comes with a cost, as the algorithm may return a biased solution, i.e., it may
not converge to an optimal solution of the original decision problem (the one with large value of γ).
Efroni et al. (2018a) recently proposed another approach to mitigate the γ-dependant instabilities in
RL in which they study a multi-step greedy versions of the well-known dynamic programming (DP)
algorithms policy iteration (PI) and value iteration (VI) (Bertsekas & Tsitsiklis, 1996). Efroni et al.
(2018a) also proposed an alternative formulation of the multi-step greedy policy, called κ-greedy
policy, and studied the convergence of the resulted PI and VI algorithms: κ-PI and κ-VI. These two
algorithms iteratively solve γκ-discounted decision problems, whose reward has been shaped by the
solution of the decision problem at the previous iteration. Unlike the biased solution obtained by
solving the decision problem with a smaller value of γ, by iteratively solving decision problems
with a smaller γκ horizon, the κ-PI and κ-VI algorithms could converge to an optimal policy of the
original decision problem.
In this work, we derive and empirically validate model-free deep RL (DRL) implementations of κ-PI
and κ-VI. In these implementations, we use DQN (Mnih et al., 2015) and TRPO (Schulman et al.,
2015) for (approximately) solving γκ-discounted decision problems (with shaped reward), which is
the main component of the κ-PI and κ-VI algorithms. The experiments illustrate the performance
of model-free algorithms can be improved by using them as solvers of multi-step greedy PI and VI
schemes, as well as emphasize important implementation details while doing so.
2	Preliminaries
In this paper, we assume that the agent’s interaction with the environment is modeled as a discrete
time Y-discounted Markov Decision Process (MDP), defined by MY = (S, A, P, R, γ, μ), where
S and A are the state and action spaces; P ≡ P(s0|s, a) is the transition kernel; R ≡ r(s, a)
1
Under review as a conference paper at ICLR 2020
is the reward function with the maximum value of Rmax; γ ∈ (0, 1) is the discount factor; and
μ is the initial state distribution. Let π : S → P(A) be a stationary Markovian policy, where
P(A) is a probability distribution on the set A. The value of π in any state s ∈ S is defined as
Vπ(S) ≡ E[Pt≥o Ytr(st, ∏(st))∣so = s, ∏], where the expectation is over all the randomness in
policy, dynamics, and rewards. Similarly, the action-value function of π is defined as Qπ (s, a) =
E[Pt≥o Ytr(St,∏(st))∣so = s,ao = a, ∏]. Since the rewards have the maximum value of Rmax,
both V and Q functions have the maximum value of Vmax = Rmax/(1 - γ). An optimal policy ∏ is
the policy with maximum value at every state. We call the value of ∏* the OPtimaI value, and define
it as V*(s) = max∏ E[Pt≥o Ytr(St,∏(st))∣so = s,∏], ∀s ∈ S. Furthermore, we denote the state-
action value of π* as Q*(s, a) and remind the following relation holds V*(s) = max。Q*(s, a) for
all S. The algorithms by which an is be solved (obtain an optimal policy) are mainly based on two
popular DP algorithms: Policy Iteration (PI) and Value Iteration (VI). While VI relies on iteratively
computing the optimal Bellman operator T applied to the current value function V (Eq. 1), PI relies
on (iteratively) calculating a 1-step greedy policy π1-step w.r.t. to the value function of the current
policy V (Eq. 2):
(TV)(S) = max E[r(S0, a) + YV(S1) | S0 = S],	∀S ∈ S,	(1)
a∈A
π1-step(S) ∈ arg max E[r(S0, a) + YV(S1) | S0 = S], ∀S ∈ S.	(2)
a∈A
It is known that T is a γ-contraction w.r.t. the max norm and its unique fixed point is V *, and the
1-step greedy policy w.r.t. V * is an optimal policy ∏*. In practice, the state space is often large, and
thus, we can only approximately compute Eqs. 1 and 2, which results in approximate PI (API) and VI
(AVI) algorithms. These approximation errors then propagate through the iterations of the API and
AVI algorithms. However, it has been shown that this (propagated) error can be controlled (Munos,
2003; 2005; Farahmand et al., 2010) and after N steps, the algorithms approximately converge to a
solution πN whose difference with the optimal value is bounded (see e.g., Scherrer 2014 for API):
η(∏*) - η(∏N) ≤ Cδ∕(i - γ)2 + γN‰aχ.	(3)
In Eq. 3, the scalar η(∏) = Es〜μ[Vπ (s)] is the expected value function at the initial state,1 2 3 4 5 6 δ rep-
resents the per-iteration error, and C upper-bounds the mismatch between the sampling distribution
and the distribution according to which the final value function is evaluated (μ in Eq. 3), and de-
pends heavily on the dynamics. Finally, the second term on the RHS of Eq. 3 is the error due to
initial values of policy/value, and decays with the number of iterations N .
3 κ-GREEDY POLICY & κ-PI AND κ-VI ALGORITHMS
Algorithm 1 κ-Policy Iteration
1	Initialize: K ∈ [0,1], ∏0, N(κ)
2	: for i = 0, 1, . . . , N(K) — 1 do
3	:	Vπi = E[Pt≥0 γtrt | πi]
4	∏i+ι ― arg max E[P∞=0(κγ)trt (κ,Vπi) ∣π] π
5	: end for
6	: Return πN (κ)
Algorithm 2 κ-Value Iteration
1: Initialize: K ∈ [0,1], V0 , N(K)
2: for i = 0, 1, . . . , N(κ) - 1 do
3:	¼+ι =max∏ E[Pt≥0(γκ)trt(κ,Vi)∣π]
4: end for
5: ∏N(κ) — arg max E[Pt≥o(KY)trt(κ,VN(κ))∣∏]
π
6: Return πN(κ)
The optimal Bellman operator T (Eq. 1) and 1-step greedy policy π1-step (Eq. 2) can be generalized
to multi-step. The most straightforward form of this generalization is by replacing T and π1-step
with h-optimal Bellman operator and h-step greedy policy (i.e., a lookahead of horizon h) that are
defined by substituting the 1-step return in Eqs. 1 and 2, r(S0, a) + YV(S1), with h-step return,
Pth=-01 r(St, at) + YhV(Sh), and computing the maximum over actions a0, . . . , ah-1, instead of just
a0 (Bertsekas & Tsitsiklis, 1996). Efroni et al. (2018a) proposed an alternative form of multi-step
optimal Bellman operator and multi-step greedy policy, called κ-optimal Bellman operator, Tκ, and
κ-greedy policy, πκ, for κ ∈ [0, 1], i.e.,
(TκV)(S) = max E[X(Yκ)trt(κ, V) | S0 = S,π],	∀S ∈ S,	(4)
π
t≥0
πκ(S) ∈ arg max E[	(Yκ)trt(κ, V) | S0 = S, π], ∀S ∈ S,	(5)
π
t≥0
1Note that the LHS of Eq. 3 is the '1 -norm Of(Vπ* — VπN) w.r.t. the initial state distribution μ.
2
Under review as a conference paper at ICLR 2020
where the shaped reward rt (κ, V ) w.r.t. the value function V is defined as
rt(κ,V) ≡ r(st, at) + (1 - κ)γV(st+1).	(6)
It can be shown that the κ-greedy policy w.r.t. the value function V is the optimal policy w.r.t. a κ-
weighted geometric average of all future h-step returns (from h = 0 to ∞). This can be interpreted
as TD(λ) (Sutton & Barto, 2018) for policy improvement (see Efroni et al., 2018a, Sec. 6). The
important difference is that TD(λ) is used for policy evaluation and not for policy improvement.
From Eqs. 4 and 5, it is easy to see that solving these equations is equivalent to solving a surrogate
γκ-discounted MDP with the shaped reward rt(κ, V ), which we denote by Mγκ(V ) throughout
the paper. The optimal value of Mγκ(V ) (the surrogate MDP) is TκV and its optimal policy is the
κ-greedy policy, πκ. Using the notions of κ-optimal Bellman operator, Tκ, and κ-greedy policy,
πκ, Efroni et al. (2018a) derived κ-PI and κ-VI algorithms, whose pseudocode is shown in Algo-
rithms 1 and 2. κ-PI iteratively (i) evaluates the value of the current policy πi , and (ii) set the new
policy, πi+1, to the κ-greedy policy w.r.t. the value of the current policy Vπi, by solving Eq. 5. On
the other hand, κ-VI repeatedly applies the Tκ operator to the current value function Vi (solves Eq. 4)
to obtain the next value function, Vi+1, and returns the κ-greedy policy w.r.t. the final value VN(κ) .
Note that for κ = 0, the κ-greedy policy and κ-optimal Bellman operator are equivalent to their
1-step counterparts, defined by Eqs. 1 and 2, which indicates that κ-PI and κ-VI are generalizations
of the seminal PI and VI algorithms.
It has been shown that both PI and VI converge to the optimal value with an exponential rate that
depends on the discount factor Y, i.e., ∣∣V* - VπN∣∣∞ ≤ O(YN) (See e.g., Bertsekas & Tsitsiklis,
1996; Scherrer, 2013). Analogously, Efroni et al. (2018a) showed that κ-PI and κ-VI converge with
faster exponential rate of ξ(κ) =)[-；? ≤ Y, i.e., ∣V * - V KN(K) ∣∣∞ ≤ O(ξ(κ)N (κ)), with the cost
that each iteration of these algorithms is computationally more expensive than that of PI and VI.
Finally, we state the following two properties of κ-PI and κ-greedy policies that we use in our RL
implementations of κ-PI and κ-VI algorithms in Sections 4 and 5:
1)	Asymptotic performance depends on κ. The following bound that is similar to the one reported in
Eq. 3 was proved by Efroni et al. (2018b, Thm. 5) for the performance of κ-PI:
η(∏*) - η(∏N(K)) ≤ C(K)δ(κ)∕(i - γ)2 + ξ(κ)N(K)Vmax,	(7)
'--------{z-------} '------{z-----}
Asymptotic Term	Decaying Term
where δ(κ) and C(κ) are quantities similar to δ and C in Eq. 3. Note that the first term on the RHS
of Eq. 7 is independent of N (κ), while the second one decays with N(κ).
2)	Soft updates w.r.t. a κ-greedy policy does not necessarily improve the performance. Let πK be the
κ-greedy policy w.r.t. Vπ. Then, unlike for 1-step greedy policies, the performance of (1-α)π+απK
(soft update) is not necessarily better than that ofπ (Efroni et al., 2018b, Thm. 1). This hints that it
would be advantages to use κ-greedy policies with ‘hard’ updates (using πK as the new policy).
4 RL IMPLEMENTATIONS OF κ-PI AND κ-VI
As described in Sec. 3, implementing κ-PI and κ-VI requires iteratively solving a Yκ-discounted
surrogate MDP with a shaped reward. If a model of the environment is given, the surrogate MDP
can be solved using a DP algorithm (see Efroni et al., 2018a, Sec. 7). When the model is not
available, it can be approximately solved by any model-free RL algorithm. In this paper, we focus
on the case that the model is not available and propose RL implementations of κ-PI and κ-VI. The
main question we investigate in this work is how model-free RL algorithms should be implemented
to efficiently solve the surrogate MDP in κ-PI and κ-VI.
In this paper, we use DQN (Mnih et al., 2015) and TRPO (Schulman et al., 2015) as subroutines for
estimating a κ-greedy policy (Line 4 in κ-PI, Alg. 1 and Line 5 in κ-VI, Alg. 2) or for estimating an
optimal value of the surrogate MDP (Line 3 in κ-VI, Alg. 2). For estimating the value of the current
policy (Line 3, in κ-PI, Alg. 1), we use standard policy evaluation deep RL (DRL) algorithms.
To implement κ-PI and κ-VI, we shall set the value ofN(κ) ∈ N, i.e., the total number of iterations
of these algorithms, and determine the number of samples for each iteration. Since N(κ) only
appears in the second term of Eq. 7, an appropriate choice of N(K) is such that C(κ)δ(κ)∕(1 -
Y)2 ' ξ(κ)N(K)Vmax. Note that setting N(κ) to a higher value would not dramatically improve the
3
Under review as a conference paper at ICLR 2020
Algorithm 3 K-PI-DQN
1:	Initialize replay buffer D, Q-networks Qθ, Qφ, and target networks Q0θ, Q0φ;
2:	for i = 0, . . . , N (κ) - 1 do
3:	# Policy Improvement
4:	for t = 1, . . . , T (κ) do
5:	Act by an -greedy policy w.r.t. Qθ(st, a), observe rt, st+1, and store (st, at, rt, st+1) in D;
6:	Sample a batch {(sj , aj , rj , sj+1)}jN=1 from D;
7:	Update θ by DQN rule with {(sj , aj , rj (κ, Vφ), sj+1)}jN=1, where
8:	Vφ (sj+1) = Qφ(sj+1,πi-1(sj+1))	and πi-1(s) ∈ arg maxa Q0θ(s, a);
9:	Copy θ to θ0 occasionally (θ0 — θ);
10:	end for
11:	# Policy Evaluation of πi(s) ∈ arg maxa Q0θ (s, a)
12:	for t = 1, . . . , T (κ) do
13:	Sample a batch {(sj , aj , rj , sj+1)}jN=1 from D;
14:	Update φ by TD(0) off-policy rule with {(sj, aj, rj, sj+1)}jN=1, and πi(s) ∈ arg maxa Q0θ (s, a);
15:	Copy φ to φ0 occasionally	(φ0 — φ);
16:	end for
17:	end for
performance, because the asymptotic term in Eq. 7 is independent of N (κ). In practice, since δ(κ)
and C(κ) are unknown, we set N (κ) to satisfy the following equality:
ξ(κ)N(κ) = CFA,	(8)
where CFA is a hyper-parameter that depends on the final-accuracy we are aiming for. For example,
if we expect the final accuracy being 90%, we would set CFA = 0.1. Our results suggest that this
approach leads to a reasonable choice for N (κ), e.g., N(κ = 0.99) ' 4 and N(κ = 0.5) ' 115, for
CFA = 0.1 and γ = 0.99. As we increase κ, we expect less iterations are needed for κ-PI and κ-VI
to converge to a good policy. Another important observation is that since the discount factor of the
surrogate MDP that κ-PI and κ-VI solve at each iteration is γκ, the effective horizon (the effective
horizon of a γκ-discounted MDP is 1/(1 - γκ)) of the surrogate MDP increases with κ.
Lastly, we need to determine the number of samples for each iteration of κ-PI and κ-VI. We allocate
equal number of samples per iteration, denoted by T (κ). Since the total number of samples, T , is
known beforehand, we set the number of samples per iteration to
T(κ) = T /N (κ).	(9)
5 DQN AND TRPO IMPLEMENTATIONS OF κ-PI AND κ-VI
In this section, we study the use of DQN (Mnih et al., 2015) and TRPO (Schulman et al., 2015) in
κ-PI and κ-VI algorithms. We first derive our DQN and TRPO implementations of κ-PI and κ-VI
in Sections 5.1 and 5.2. We refer to the resulting algorithms as κ-PI-DQN, κ-VI-DQN, κ-PI-TRPO,
and κ-VI-TRPO. It is important to note that for κ = 1, κ-PI-DQN and κ-VI-DQN are reduced to
DQN, and κ-PI-TRPO and κ-VI-TRPO are reduced to TRPO. We then conduct a set of experiments
with these algorithms, in Sections 5.1.1 and 5.2.1, in which we carefully study the effect of κ and
N (κ) (or equivalently the hyper-parameter CFA , defined by Eq. 8) on their performance. In these
experiments, we specifically focus on answering the following questions:
1.	Is the performance of DQN and TRPO improve when using them as κ-greedy solvers in
κ-PI and κ-VI? Is there a performance tradeoff w.r.t. to κ?
2.	Following κ-PI and κ-VI, our DQN and TRPO implementations of these algorithms devote
a significant number of sample T(κ) to each iteration. Is this needed or a ‘naive’ choice of
T(κ) = 1, or equivalently N(κ) = T, works just well, for all values of κ?
5.1 DQN IMPLEMENTATION OF κ-PI AND κ-VI
Algorithm 3 contains the pseudo-code of κ-PI-DQN. Due to space constraints, we report its de-
tailed pseudo-code in Appendix A.1 (Alg. 5). In the policy improvement stage of κ-PI-DQN, we
use DQN to solve the γκ-discounted surrogate MDP with the shaped reward rt(κ, Vφ ' V πi-1),
4
Under review as a conference paper at ICLR 2020
Breakout, Nκ
Time steps
Time steps
Figure 1: Training performance of κ-PI-DQN (Top) and κ-VI-DQN (Bottom) on Breakout, for the
hyper-parameter CFA = 0.05 (right) and for the ‘naive’ baseline N(κ) = T (left).
Breakout, CFA
i.e., at the end of this stage Mγκ (Vφ). The output of the DQN is approximately the optimal Q-
function of MYK(Vφ), and thus, the κ-greedy policy w.r.t. Vφ is equal to arg maxa Qθ(∙, a). At the
policy evaluation stage, we use off-policy TD(0) to evaluate the Q-function of the current policy
πi, i.e., Qφ ' Qπi. Although what is needed on Line 8 is an estimate of the value function of the
current policy, Vφ ' Vπi-1, we chose to evaluate the Q-function ofπi: the data in our disposal (the
transitions stored in the replay buffer) is an off-policy data and the Q-function of a fixed policy can
be easily evaluated with this type of a data using off-policy TD(0), unlike the value function.
Remark 1 In order for Vφ to be an accurate estimate of the value function of πi-1 on Line 8, we
should use an additional target network, Q0θ, that remains unchanged during the policy improvement
stage. This network should be used in ∏i-ι (∙) = arg maxa Qθ(∙, a) on Lme 8, and be only updated
right after the improvement stage on Line 11. However, to reduce the space complexity of the
algorithm, we do not use this additional target network and compute πi-1 on Line 8 as arg max Q0θ,
despite the fact that Q0θ changes during the improvement stage.
We report the pseudo-code of κ-VI-DQN in Appendix A.1 (Alg. 6). Note that κ-VI simply repeats
V — TKV and computes TKV, which is the optimal value of the surrogate MDP MYK(V). In
κ-VI-DQN, we repeatedly solve Mγκ(V) by DQN, and use its optimal Q-function to shape the
reward of the next iteration. Let QYKV and vYK,V be the optimal Q and V functions of MYK (V).
Then, We have maxa Q；k,v (s, a) = VγK,v (S) = (TKV)(s), where the first equality is by definition
(Sec. 2) and the second one holds since TKV is the optimal value of MYK (V) (Sec. 3). Therefore,
in κ-VI-DQN, we shape the reward of each iteration by maxa Qφ(s, a), where Qφ is the output of
the DQN from the previous iteration, i.e., maxa Qφ (s, a) ' TKVi-1 .
5.1.1	κ-PI-DQN AND κ-VI-DQN EXPERIMENTS
In this section, we empirically analyze the performance of the κ-PI-DQN and κ-VI-DQN algorithms
on the Atari domains: Breakout, Seaquest, SpaceInvaders, and Enduro (Bellemare et al., 2013). We
start by performing an ablation test on three values of parameter CFA = {0.001, 0.05, 0.2} on the
Breakout domain. The value of CFA sets the number of samples per iteration T(κ) (Eq. 8) and
the total number of iterations N (κ) (Eq. 9). Aside from CFA , we set the total number of samples
to T ' 106 . This value represents the number of samples after which our DQN-based algorithms
approximately converge. For each value of CFA , we test κ-PI-DQN and κ-VI-DQN for several
κ values. In both algorithms, the best performance was obtained with CFA = 0.05, thus, we set
CFA = 0.05 in our experiments with other Atari domains.
5
Under review as a conference paper at ICLR 2020
Domain	Alg.	Kbest	K = 0	DQN, K = 1	N(K) = T, Kbest
Breakout	K-PI K-VI	224(±5), K=0.68 180(±5), K=0.68	160(±3): 179(±6)	131(±3)	171(±1), k=0.68
SpaceInv.	K-PI κ-VI	747(±23), K=0.84 707(±32), K=0.36	611(±15) 669(±16)	685(±24)	695(±16), k=0.92
Seaquest	K-PI κ-VI	5159(±509), K=0.84 3394(±86), K=0.36	2732(±281) 2631(±496)	3207(±248)	4371(±466), k=0.84
Enduro	K-PI κ-VI	544(±29), K=0.84 499(±18), K=0.84	371(±215) 492 (±28)	355(±52)	547(±17), k=0.68
BeamRider	K-PI K-VI	3968(±78), K=1.0 4077 (±303), K=0.68	3654(±778) 4052 (±462)	3968(±78)	3968(±78), κ=1.0
Qbert	K-PI K-VI	8276(±202), K=0.36 7924 (±267), K=0.68	6900(±149) 7585 (±587)	7322(±280)	8042(±442), κ=0.92
Table 1: The final training performance of κ-PI-DQN and κ-VI-DQN on the Atari domains, for
the hyper-parameter CFA = 0.05. The values represent the empirical mean ± empirical standard
deviation.
Figure 1 shows the training performance of κ-PI-DQN (Top) and κ-VI-DQN (Bottom) for the best
value of CFA = 0.05, as well as for the ‘naive’ baseline T(κ) = 1, or equivalently N(κ) = T, on
Breakout. The results on Breakout for the other values of CFA and the results on the other Atari
domains for CFA = 0.05 have been reported in Appendix A.2. Table 1 shows the final training
performance of κ-PI-DQN and κ-VI-DQN on the Atari domains with CFA = 0.05. Note that the
scores reported in Table 1 are the actual returns of the Atari domains, while the vertical axis in the
plots of Figure 1 corresponds to a scaled return. We plot the scaled return, since this way it would
be easier to reproduce our results using the OpenAI Baselines codebase (Hill et al., 2018).
The results of Fig. 1 and Table 1, as well as those in Appendix A.2, exhibit that both κ-PI-DQN and
κ-VI-DQN improve the performance of DQN (κ = 1). Moreover, they show that setting N(κ) = T
leads to a clear degradation of the final training performance on all of the domains expect Enduro,
which attains better performance for N(κ) = T. Although the performance degrades, the results for
N (κ) = T are still better than for DQN.
5.2	TRPO IMPLEMENTATION OF κ-PI AND κ-VI
Algorithm 4 contains the pseudo-code of κ-PI-TRPO (detailed pseudo-code in Appendix A.1).
TRPO iteratively updates the current policy using its return and an estimate of its value function. In
our κ-PI-TRPO, at each iteration i: 1) we use the estimate of the current policy Vφ0 ' V πi-1 (com-
puted in the previous iteration) to calculate the return R(κ, Vφ0) and an estimate of the value function
Vθ of the surrogate MDP Mγκ(Vφ0), 2) we use the return R(κ, Vφ0) and Vθ to compute the new policy
πi , and 3) we estimate the value of the new policy Vφ ' V πi on the original, γ discounted, MDP.
In Appendix B.1 we provide the pseudocode of κ-VI-TRPO derived by the κ-VI meta algorithm. As
previously noted, κ-VI iteratively solves the γκ discounted surrogate MDP and uses its optimal value
TκVi-1 to shape the reward of the surrogated MDP in the i’th iteration. With that in mind, consider
κ-PI-TRPO. Notice that as πθ converges to the optimal policy of the surrogate γκ discounted MDP,
Vq converges to the optimal value of the surrogate MDP, i.e., it converges to TKVi-I = TKVi-1.
Thus, κ-PI-TRPO can be turn to κ-VI-TRPO by eliminating the policy evaluation stage, and simply
copy φ — θ, meaning, Vφ — Vq = TκVφ.
5.2.1	κ-PI-TRPO AND κ-VI-TRPO EXPERIMENTS
In this section, we empirically analyze the performance of the κ-PI-TRPO and κ-VI-TRPO algo-
rithms on the MuJoCo domains: Walker2d-v2, Ant-v2, HalfCheetah-v2, HumanoidStandup-v2, and
Swimmer-v2, (Todorov et al., 2012). As in Section 5.1.1, we start by performing an ablation test
on the parameter CFA = {0.001, 0.05, 0.2} on the Walker domain. We set the total number of
iterations to 2000, with each iteration consisting 1000 samples. Thus, the total number of samples is
T ' 2 × 106. This is the number of samples after which our TRPO-based algorithms approximately
converge. For each value of CFA, we test κ-PI-TRPO and κ-VI-TRPO for several κ values. In both
algorithms, the best performance was obtained with CFA = 0.2, thus, we set CFA = 0.2 in our
experiments with other MuJoCo domains.
6
Under review as a conference paper at ICLR 2020
Algorithm 4 κ-PI-TRPO
1:
2:
3:
4:
5:
6:
7:
8:
9:
10
11
12
13
Initialize V -networks Vθ and Vφ, policy network πψ, and target network Vφ0;
for i = 0, . . . , N (κ) - 1 do
for t = 1, . . . , T (κ) do
Simulate the current policy πψ for M steps and calculate the following two returns for all steps j :
Rj(κ,Vφ0)=PtM=j(γκ)t-jrt(κ,Vφ0)	and	ρj =PtM=jγt-jrt;
Update θ by minimizing the batch loss function:	Lvθ = N PN=I (Vθ(Sj) - Rj(κ, Vφ0))2;
#	Policy Improvement
Update ψ using TRPO by the batch {(Rj (κ, Vφ0), Vθ (sj))}jN=1;
#	Policy Evaluation
Update φ by minimizing the batch loss function:	Lvφ = N PN=I (V≠(sj) — Pj)2;
end for
Copy Φ to φ0 (φ0 — φ);
end for
P-IeMəɑ QPOMd山 ulŋəw
Walker, CFA = 0.2
P-IeMəɑ QPOMd山 UEφw
Walker, CFA = 0.2
Figure 2: Training performance of κ-PI-TRPO (Top) and κ-VI-TRPO (Bottom) on Walker, for the
hyper-parameter CFA = 0.2 (right) and for the ‘naive’ baseline N(κ) = T (left).
Figure 2 shows the training performance of κ-PI-TRPO (Top) and κ-VI-TRPO (Bottom) for the best
value of CFA = 0.2, as well as for the ‘naive’ baseline T (κ) = 1, or equivalently N(κ) = T,
on Walker. The results on Walker for the other CFA values and the other MuJoCo domains for
CFA = 0.2 have been reported in Appendix B.3. Table 2 shows the final training performance of
κ-PI-TRPO and κ-VI-TRPO on the MuJoCo domains with CFA = 0.2.
The results of Figure 2 and Table 2, as well as those in Appendix B.3, exhibit that both κ-PI-TRPO
and κ-VI-TRPO yield better performance than TRPO (κ = 1). Furthermore, they show that the
algorithms with CFA = 0.2 perform better than with N(κ) = T. However, the improvement is less
significant relative to the DQN-based results in Section 5.1.1.
5.2.2	Comparison with the Generalized Advantage Estimation algorithm
There is an intimate relation between κ-PI and the GAE algorithm Schulman et al. (2016) which we
elaborate on in this section. In GAE the policy is updated by the gradient:
Vθ Es~μ[V πθ (s)]=
EsO ~dμ,π 卜θ log ∏θ(so) X(γλ)tδ(V) ; δ(V) = rt + YK+1 - Vt, (10)
t
which can be interpreted as a gradient step in a γλ discounted MDP with rewards δ(V), which we
refer here as Mδγ(λV). As noted in Efroni et al. (2018a), Section 6, the optimal policy of the MDP
Mδγ(λV ) is the optimal policy of Mγκ (V) with κ = λ, i.e., the κ-greedy policy w.r.t. V: thus, the
7
Under review as a conference paper at ICLR 2020
Domain	Alg.	Kbest	K = 0	TRPO, K = 1	N(K) = T, Kbest		GAE	
Walker	K-PI K-VI	1352(±233), k=0.68 827(±269), k=0.68	1205 (±99) 669(±i25)	560 (±ιi7)	1158(±75), k=0.36	1664(±3i8), λ=0.36
Ant	K-PI κ-VI	1359(±326), k=0.68 2916(±455), k=0.68	1083(±2O5) 1809(±342)	-18.47(±2)	1225(±i4i), k=0.0	1152(±255), λ=0.0
HalfCheetah	K-PI κ-VI	1367(±406), k=0.36 1735(±800), k=0.36	855(±i6O) 1078(±48)	74(±202)	1450(±2oo), k=0.36	1453(±203), λ=0.36
HumanoidStand	K-PI κ-VI	73743(±i988), k=0.99 74063(±i779), k=0.99	73486(±i2ii) 51323(±i805)	67545(±i545)	72588(±i929), k=0.98	71420(±14oi), λ=0.98
Swimmer	K-PI K-VI	108(±17), k=1.0 108(±17), k=1.0	43(±3) 46(±i)	108(±i7)	108(±17), k=1.0	108(±14), λ = 1.0
Hopper	K-PI K-VI	1872(±i9i), k=0.68 1043(±95), k=0.92	805(±291) 590(±246)	1193(±353)	1491(±i57), k=0.96	1745(±3oo), λ = 0.68
Table 2: The final training performance of κ-PI-TRPO and κ-VI-TRPO on the MuJoCo domains,
for the hyper-parameter CFA = 0.2. The values represent the empirical mean ± empirical standard
deviation.
P-IeMəɑ QPOMd山 UEφw
P-IeMəɑ QPOMd山 UEφw
p」rtJMa)」pə-eus 3POMdφUEφw
Spacelnvaders, Lowering γ
Figure 3: Lowering the discount factor
optimal policy of Mδγ(λV) is the κ-greedy policy w.r.t. V . GAE, instead of solving the κ-greedy
policy while keeping V fixed, changes the policy and updates V by the return concurrently. Thus,
this approach is conceptually similar to κ-PI-TRPO with N (κ) = T . There, the value and policy
are concurrently updated as well, without clear separation between the update of the policy and the
value.
In Figure 2 and Table 2 the performance of GAE is compared to the one of κ-PI-TRPO and κ-VI-
TRPO. The performance of the latter two is slightly better than the one of GAE.
Remark 2 (Implementation of GAE) We used the OpenAI baseline implementation of GAE with a
small modification. In the baseline code, the value network is updated w.r.t. to the target t(γλ)trt,
whereas in Schulman et al. (2016) the authors used the target t γtrt (see Schulman et al. (2016),
Eq.28). We chose the latter form in our implementation to be in accord with Schulman et al. (2016).
5.3	DQN and TRPO Performance Versus the Discount Factor
To supply with a more complete view on our experiments, we tested the performance of the “vanilla”
DQN and TRPO when trained with different γ values than the previously used one (γ = 0.99). As
evident in Figure 3, only for the Ant domain this approach resulted in improved performance when
for TRPO trained with γ = 0.68. It is interesting to observe that for the Ant domain the performance
of κ-PI-TRPO and especially of κ-VI-TRPO (Table 2) significantly surpassed the one of TRPO
8
Under review as a conference paper at ICLR 2020
trained with γ = 0.68. The performance of DQN and TRPO on the Breakout, SpaceInvaders and
Walker domains decreased or remained unchanged in the tested γ values. Thus, on these domains,
changing the discount factor does not improve the DQN and TRPO algorithms, as using κ-PI or
κ-VI with smaller κ value do.
It is interesting to observe that the performance on the Mujoco domains for small γ, e.g., γ =
0.68, achieved good performance, whereas for the Atari domains the performance degraded with
lowering γ. This fits the nature of these domains: in the Mujoco domains the decision problem
inherently has much shorter horizon than in the Atari domains.
Furthermore, it is important to stress that γ and κ are two different parameters an algorithm designer
may use. For example, one can perform a scan of γ value, fix γ to the one with optimal performance,
and then test the performance of different κ values.
6 Discussion and Future Work
In this work we formulated and empirically tested simple generalizations of DQN and TRPO de-
rived by the theory of multi-step DP and, specifically, of κ-PI and κ-VI algorithms. The empirical
investigation reveals several points worth emphasizing.
1.	κ-PI is better than κ-VI for the Atari domains.. In most of the experiments on the Atari
domains κ-PI-DQN has better performance than κ-VI-DQN. This might be expected as the
former uses extra information not used by the latter: κ-PI estimates the value of current
policy whereas κ-VI ignores this information.
2.	For the Gym domains κ-VI performs slightly better than κ-PI. For the Gym domains κ-
VI-TRPO performs slightly better than κ-PI-TRPO. We conjecture that the reason for the
discrepancy relatively to the Atari domains lies in the inherent structure of the tasks of
the Gym domains: they are inherently short horizon decision problems. For this reason,
the problems can be solved with smaller discount factor (as empirically demonstrated in
Section 5.3) and information on the policy’s value is not needed.
3.	Non trivial κ value improves the performance. In the vast majority of our experiments both
κ-PI and κ-VI improves over the performance of their vanilla counterparts (i.e., κ = 1),
except for the Swimmer and BeamRider domains from Mujoco and Atari suites. Impor-
tantly, the performance of the algorithms was shown to be ‘smooth’ in the parameter κ.
This suggests careful hyperparameter tuning of κ is not of great necessity.
4.	Using the ‘naive’ choice ofN(κ) = T deteriorates the performance. Choosing the number
of iteration by Eq. 8 improves the performance on the tested domains.
An interesting future work would be to test model-free algorithms which use other variants of greedy
policies (Bertsekas & Tsitsiklis, 1996; Bertsekas, 2018; Efroni et al., 2018a; Sun et al., 2018; Shani
et al., 2019). Furthermore, and although in this work we focused on model-free DRL, it is arguably
more natural to use multi-step DP in model-based DRL (e.g.,Kumar et al., 2016; Talvitie, 2017; Luo
et al., 2018; Janner et al., 2019). Taking this approach, the multi-step greedy policy would be solved
with an approximate model. We conjecture that in this case one may set K - or more generally, the
planning horizon - as a function of the approximate model's 'quality'： as the approximate model
gets closer to the real model larger κ can be used. We leave investigating such relation in theory
and practice to future work. Lastly, an important next step in continuation to our work is to study
algorithms with an adaptive κ parameter. This, we believe, would greatly improve the resulting
methods, and possibly be done by studying the relation between the different approximation errors
(i.e., errors in gradient and value estimation, Ilyas et al., 2018), the performance and the κ value that
should be used by the algorithm.
References
L.	Baird. Residual algorithms： Reinforcement learning with function approximation. In Proceedings
of the Twelfth International Conference on Machine Learning, pp. 30-37, 1995.
M.	Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment： An evalu-
ation platform for general agents. Journal of Artificial Intelligence Research, 47：253-279, 2013.
9
Under review as a conference paper at ICLR 2020
D. Bertsekas and J. Tsitsiklis. Neuro-dynamic programming, volume 5. 1996.
Dimitri P Bertsekas. Feature-based aggregation and deep reinforcement learning: A survey and
some new implementations. IEEE/CAA Journal OfAutomatica Sinica, 6(1):1-31, 2018.
Y. Efroni, G. Dalal, B. Scherrer, and S. Mannor. Beyond the one step greedy approach in rein-
forcement learning. In Proceedings of the 35th International Conference on Machine Learning,
2018a.
Y. Efroni, G. Dalal, B. Scherrer, and S. Mannor. Multiple-step greedy policies in approximate and
online reinforcement learning. In Advances in Neural Information Processing Systems, pp. 5238-
5247, 2018b.
A. Farahmand, C. Szepesvari, and R. Munos. Error propagation for approximate policy and value
iteration. In Advances in Neural Information Processing Systems, pp. 568-576, 2010.
E. Greensmith, P. Bartlett, and J. Baxter. Variance reduction techniques for gradient estimates in
reinforcement learning. Journal of Machine Learning Research, 5(Nov):1471-1530, 2004.
A. Hill, A. Raffin, M. Ernestus, A. Gleave, A. Kanervisto, R. Traore, P. Dhariwal, C. Hesse,
O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, and Y. Wu. Stable base-
lines. https://github.com/hill-a/stable-baselines, 2018.
Andrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry
Rudolph, and Aleksander Madry. Are deep policy gradient algorithms truly policy gradient algo-
rithms? arXiv preprint arXiv:1811.02553, 2018.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. arXiv preprint arXiv:1906.08253, 2019.
N.	Jiang, A. Kulesza, S. Singh, and R. Lewis. The dependence of effective planning horizon on
model accuracy. In Proceedings of the 2015 International Conference on Autonomous Agents
and Multiagent Systems, pp. 1181-1189, 2015.
Vikash Kumar, Emanuel Todorov, and Sergey Levine. Optimal control with learned local models:
Application to dexterous manipulation. In 2016 IEEE International Conference on Robotics and
Automation (ICRA), pp. 378-383. IEEE, 2016.
S.	Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies.
Journal of Machine Learning Research, 17(1):1334-1373, 2016.
T.	Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous
control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
T. Lu, D. Schuurmans, and C. Boutilier. Non-delusional q-learning and value iteration. In Pro-
ceedings of the 32nd International Conference on Neural Information Processing Systems, pp.
9971-9981, 2018.
Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorith-
mic framework for model-based deep reinforcement learning with theoretical guarantees. arXiv
preprint arXiv:1807.03858, 2018.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A Rusu, J. Veness, M. Bellemare, A. Graves, M. Riedmiller,
A. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning.
Nature, 518(7540):529, 2015.
R. Munos. Error bounds for approximate policy iteration. In Proceedings of the 20th International
Conference on Machine Learning, pp. 560-567, 2003.
R. Munos. Error bounds for approximate value iteration. In Proceedings of the 20th National
Conference on Artificial Intelligence, pp. 1006-1011, 2005.
M. Petrik and B. Scherrer. Biasing approximate dynamic programming with a lower discount factor.
In Advances in neural information processing systems, pp. 1265-1272, 2009.
10
Under review as a conference paper at ICLR 2020
B. Scherrer. Improved and generalized upper bounds on the complexity of policy iteration. In
Advances in Neural Information Processing Systems, pp. 386-394, 2013.
B. Scherrer. Approximate policy iteration schemes: a comparison. In International Conference on
Machine Learning, pp. 1314-1322, 2014.
J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust-region policy optimization. In
International conference on machine learning, pp. 1889-1897, 2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. In Proceedings of the
International Conference on Learning Representations (ICLR), 2016.
Lior Shani, Yonathan Efroni, and Shie Mannor. Exploration conscious reinforcement learning re-
visited. In International Conference on Machine Learning, pp. 5680-5689, 2019.
D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,
M. Lai, A. Bolton, et al. Mastering the game of Go without human knowledge. Nature, 550
(7676):354, 2017.
Wen Sun, Geoffrey J Gordon, Byron Boots, and J Bagnell. Dual policy iteration. In Advances in
Neural Information Processing Systems, pp. 7059-7069, 2018.
R. Sutton and A. Barto. Reinforcement learning: An introduction. 2018.
Erik Talvitie. Self-correcting models for model-based reinforcement learning. In Thirty-First AAAI
Conference on Artificial Intelligence, 2017.
S. Thrun and A. Schwartz. Issues in using function approximation for reinforcement learning. In
Proceedings of the Connectionist Models Summer School, pp. 255-263, 1993.
E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In IEEE/RSJ
International Conference on Intelligent Robots and Systems, pp. 5026-5033, 2012.
H. Van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double Q-learning. In
Thirtieth AAAI conference on artificial intelligence, 2016.
11
Under review as a conference paper at ICLR 2020
A DQN IMPLEMENTATION OF κ-PI AND κ-VI
A.1 Pseudo-codes
In this section, we report the detailed pseudo-codes of the κ-PI-DQN and κ-VI-DQN algorithms,
described in Section 5.1, side-by-side.
Algorithm 5 K-PI-DQN
1:	Initialize replay buffer D, and Q-networks Qθ and Qφ with random weights θ and φ;
2:	Initialize target networks Qθ and Qφ with weights θ0 一 θ and φ0 一 φ;
3:	for i = 0, . . . , N (κ) - 1 do
4:	# Policy Improvement
5:	for t = 1, . . . , T (κ) do
6:	Select at as an -greedy action w.r.t. Qθ (st , a);
7:	Execute at, observe rt and st+1, and store the tuple (st, at, rt, st+1) in D;
8:	Sample a random mini-batch {(sj, aj, rj, sj+1)}jN=1 from D;
9:	Update θ by minimizing the following loss function:
10:	Lqθ = N1 PN=I (Qθ(Sj,aj) - (rj(κ, Vφ) + γκ maXaQθ(sj+ι, a))) , where
11:	Vφ(sj+1) = Qφ(sj+1, πi-1(sj+1)) and πi-1(sj+1) ∈ arg maxa Q0θ(sj+1, a);
12:	Copy θ to θ0 occasionally	(θ0 - θ);
13:	end for
14:	# Policy Evaluation
15:	Set πi(s) ∈ arg maxa Q0θ (s, a);
16:	for t0 = 1, . . . , T(κ) do
17:	Sample a random mini-batch {(sj, aj, rj, sj+1)}jN=1from D;
18:	Update φ by minimizing the following loss function:
19:	LQφ = N Pj=I (Qφ(Sj，aj ) - (rj + γQφ(Sj+1, πi(Sj +1))));
20:	Copy φ to φ0 occasionally	(φ0 J φ);
21:	end for
22:	end for
Algorithm 6 K-VI-DQN
1:	Initialize replay buffer D, and Q-networks Qθ and Qφ with random weights θ and φ;
2:	Initialize target network Q0θ with weights θ0 J θ;
3:	for i = 0, . . . , N (K) - 1 do
4:	# Evaluate TκVφ and the K-greedy policy w.r.t. Vφ
5:	for t = 1, . . . , T (K) do
6:	Select at as an -greedy action w.r.t. Qθ (St, a);
7:	Execute at, observe rt and St+1, and store the tuple (St, at, rt, St+1) in D;
8:	Sample a random mini-batch {(Sj, aj, rj, Sj+1)}jN=1 from D;
9:	Update θ by minimizing the following loss function:
10:	Lqθ = N1 Pj=I (Qθ(sj, aj) — (rj(κ, Vφ) + KY max°Qθ(sj+ι, a))) , where
11:	Vφ(Sj+1) = Qφ(Sj+1, π(Sj+1))	and π(Sj+1) ∈ arg maxa Q0θ(Sj+1, a);
12:	Copy θ to θ0 occasionally (θ0 J θ);
13:	end for
14:	Copy θ to φ (φ J θ)
15:	end for
12
Under review as a conference paper at ICLR 2020
HyPerParameter	Value
Horizon (T)	-1000
Adam stePsize	1 X 10-4
Target network uPdate frequency	1000
RePlay memory size	100000
Discount factor	0.99
Total training time stePs	10000000
Minibatch size	32
Initial exPloration	1
Final exPloration	0.1
Final exPloration frame	1000000
#Runs used for Plot averages	4
Confidence interval for Plot runs	〜70%
Table 3: Hyperparameters for κ-PI-DQN and κ-VI-DQN.
A.2 ABLATION TEST FOR CFA
142 O 8 6 4 2
PJEMW.J PΦ-BX(υPOEdαJunɔ① W

P-IeM ①-pφ-eu Sφpos 一 dφUeaw
P」BM ①」P<υ-Π3US(υpos-d<υUrt5① W
Figure 4: Performance of κ-PI-DQN and κ-VI-DQN on Breakout for different values of CFA .
A.3 κ-PI-DQN AND κ-VI-DQN PLOTS
In this section, we report additional results of the application of κ-PI-DQN and κ-VI-DQN on the
Atari domains. A summary of these results has been reported in Table 1 in the main paper.
13
Under review as a conference paper at ICLR 2020
Spacelnvaders, Nκ
0.2	0.4	0.6
Time steps
0.2	0.4	0.6	0.8	1.0
JΠ5M ①」pə-eus 3POS,Q,① UE ① W
Ppə-euS ①PoMdB Ue①W
0.0
Time steps	le7
Figure 5: Training performance of κ-PI-DQN (Top) and κ-VI-DQN (Bottom) on SpaceInvaders, for
the hyper-parameter CFA = 0.05 (right) and for the ‘naive’ baseline N(κ) = T (left).
14
Under review as a conference paper at ICLR 2020
Ooooooo
6 5 4 3 2 1
P」rcsMW」PΦ-BX(υPOEdaluro① W
Seaquest, CFA = 0.05
Ooooooo
6 5 4 3 2 1
P」BM ①」p<υrous(υpos 一 ʤ Urt3B W
Figure 6: Training performance of κ-PI-DQN (Top) and κ-VI-DQN (Bottom) on Seaquest, for the
hyper-parameter CFA = 0.05 (right) and for the ‘naive’ baseline N(κ) = T (left).
15
Under review as a conference paper at ICLR 2020
Figure 7: Training performance of κ-PI-DQN (Top) and κ-VI-DQN (Bottom) on Enduro, for the
hyper-parameter CFA = 0.05 (right) and for the ‘naive’ baseline N(κ) = T (left).
16
Under review as a conference paper at ICLR 2020
BeamRider, NK =
0.4	0.6
Time steps
0
0.0
25
20
P」«3M<U」
p<υ-gs
0.0	0.2	0.4	0.6	0.8	1.0
Time steps	le7
Figure 8: Training performance of κ-PI-DQN (Top) and κ-VI-DQN (Bottom) on BeamRider, for
the hyper-parameter CFA = 0.05 (right) and for the ‘naive’ baseline N(κ) = T (left).
17
Under review as a conference paper at ICLR 2020
0 5 0 5 0 5
3 2 2 1 1
EroMW」PΦ-BX(υPOEd0lUE① W
0.4	0.6
Time steps
Qbert, NK = T
0 5 0 5 0 5
3 2 2 1 1
P」BM ①」P<υ-Bus(υpos 一 ʤ Urt3① W
0.4	0.6	0.8	1.0
Time steps	le7
Figure 9: Training performance of κ-PI-DQN (Top) and κ-VI-DQN (Bottom) on Qbert, for the
hyper-parameter CFA = 0.05 (right) and for the ‘naive’ baseline N(κ) = T (left).
18
Under review as a conference paper at ICLR 2020
B	TRPO IMPLEMENTATION OF κ-PI AND κ-VI
B.1	Pseudo-Codes
In this section, we report the detailed pseudo-codes of the κ-PI-TRPO and κ-VI-TRPO algorithms,
described in Section 5.2, side-by-side.
Algorithm 7 K-PI-TRPO
1:	Initialize V-networks Vθ and Vφ, and policy network ∏ψ with random weights θ, φ, and ψ
2:	Initialize target network Vφ with weights φ0 一 φ
3:	for i = 0, . . . , N (κ) - 1 do
4:	for t = 1, . . . , T (κ) do
5:	Simulate the current policy πψ for M time-steps;
6:	for j = 1, . . . , M do
7:	Calculate Rj(κ,Vφ0) = PtM=j(γκ)t-jrt(κ,Vφ0)	and ρj = PtM=jγt-jrt;
8:	end for
9:	Sample a random mini-batch {(sj, aj, rj, sj+1)}jN=1 from the simulated M time-steps;
10:	Update θ by minimizing the loss function: LV =& PN=ι(Vθ(Sj) - Rj(κ, V0)))2;
11:	# Policy Improvement
12:	Sample a random mini-batch {(sj, aj, rj, sj+1)}jN=1from the simulated M time-steps;
13:	Update ψ using TRPO with advantage function computed by {(Rj (κ, Vφ0), Vθ(sj))}jN=1;
14:	# Policy Evaluation
15:	Sample a random mini-batch {(sj, aj, rj, sj+1)}jN=1from the simulated M time-steps;
16:	Update φ by minimizing the loss function:	Lyφ = NN PN=I(Vφ(sj) - Pj)2;
17:	end for
18:	Copy φ to φ0 (φ0 J φ);
19:	end for
Algorithm 8 K-VI-TRPO
1:	Initialize V-network Vθ and policy network ∏ψ with random weights θ and ψ;
2:	Initialize target network Vφ0 with weights φ0 ; J θ;
3:	for i = 0, . . . , N (K) - 1 do
4:	# Evaluate TκVφ0 and the K-greedy policy w.r.t. Vφ0
5:	for t = 1, . . . , T (K) do
6:	Simulate the current policy πψ for M time-steps;
7:	for j = 1, . . . , M do
8:	Calculate Rj (K, Vφ0) = PtM=j (γK)t-jrt(K, Vφ0);
9:	end for
10:	Sample a random mini-batch {(sj, aj, rj, sj+1)}jN=1 from the simulated M time-steps
11:	Update θ by minimizing the loss function: LV = NN PN=1(Vθ(Sj) - Rj(κ, V0)))2;
12:	Sample a random mini-batch {(sj, aj, rj, sj+1)}jN=1from the simulated M time-steps
13:	Update ψ using TRPO with advantage function computed by {(Rj (K, Vφ0), Vθ(Sj))}jN=1;
14:	end for
15:	Copy θ to φ0 (φ0 J θ);
16:	end for
19
Under review as a conference paper at ICLR 2020
HyPerParameter	Value
Horizon (T)	-1000
Adam stePsize	1 X 10-3
Number of samPles Per Iteration	1024
EntroPy coefficient	0.01
Discount factor	0.99
Number of Iterations	2000
Minibatch size	128
#Runs used for Plot averages	5
Confidence interval for Plot runs	〜70%
Table 4: Hyper-parameters of κ-PI-TRPO and κ-VI-TRPO on the MuJoCo domains.
B.2	ABLATION TEST FOR CFA
2000
Walker, Cfa = 0.2
2000
250
0
Oooooo
5 0 5 0 5 0
7 5 2 0 7 5
p」EMBaɑjpos-d 山 ue① W
Figure 10: Performance of κ-PI-TRPO and κ-VI-TRPO on Walker2d-v2 for different values of
CFA.
B.3	κ-PI-TRPO AND κ-VI-TRPO PLOTS
In this section, we report additional results of the application of κ-PI-TRPO and κ-VI-TRPO on the
MuJoCo domains. A summary of these results has been reported in Table 2 in the main paper.
20
Under review as a conference paper at ICLR 2020
---κ=0.0
---K=O-36
---K=O-68
——κ=0.84
——κ=0.92
---κ=0.96
—κ=0.98
---κ=0.99
Ant-v2 GAE
Figure 11: Performance of GAE and ‘Naive’ baseline (Top row) and κ-PI-TRPO (Bottom left) and
κ-VI-TRPO (Bottom right) on Ant-v2.
21
Under review as a conference paper at ICLR 2020
HalfCheetah-v2 GAE
Iterations
Figure 12: Performance of GAE and ‘Naive’ baseline (Top row) and κ-PI-TRPO (Bottom left) and
κ-VI-TRPO (Bottom right) on HalfCheetah-v2.
22
Under review as a conference paper at ICLR 2020
HumanoidStandup-v2, CFA = 0 2
75000
ɔ 70000
υ 65000
ɔ 60000
j 55000
g 50000
45000
40000
75000
ɔ 70000
U 65000
? 60000
亨 55000
S 50000
45000
40000
0	250	500	750 1000 1250 1500 1750 2000
Iterations
HUmanOidStandUP-V2, CEA = 0.2
HumanoidStandup-v2, Λ∕(κ) = T
O 250	500	750 IOOO 1250 1500 1750 2000
Iterations
75000
ɔ 70000
D 65000
S 60000
} 55000
S 50000
45000
Figure 13: Performance of GAE and ‘Naive’ baseline (Top row) and κ-PI-TRPO (Bottom left) and
κ-VI-TRPO (Bottom right) on HumanoidStandup-v2.
23
Under review as a conference paper at ICLR 2020
Figure 14: Performance of GAE and ‘Naive’ baseline (Top row) and κ-PI-TRPO (Bottom left) and
κ-VI-TRPO (Bottom right) on Swimmer-v2.
O 250	500	750 IOOO 1250 1500 1750 2000
Iterations
Swimmer, Λ∕(κ) - T
Swimmer, CFA 0.2
24
Under review as a conference paper at ICLR 2020
Hopper GAE
250	500	750 1000 1250 1500 1750 2000
Iterations
Hopper, CFA = 0 2
Hopper, /V(κ) = T
Oooooooo
05050505
07520752
2 ɪ ɪ ɪ 1
PJeMφɑφpos- d 山 Ueφw
0	250	500	750 1000 1250 1500 1750 2000
Iterations
Hopper, CFA = 0.2
Figure 15: Performance of GAE and ‘Naive’ baseline (Top row) and κ-PI-TRPO (Bottom left) and
κ-VI-TRPO (Bottom right) on Hopper-v2.
25
Under review as a conference paper at ICLR 2020
C Rebuttal Results
C.1 CartPole
In this section, we analyze the role κ plays in the proposed methods by reporting results on the
simple CartPole environment for κ-PI TRPO. For all experiments, we use a single layered value
function network and a linear policy network. Each hyperparameter configuration is run for 10
different random seeds and plots are shown for a 50% confidence interval.
Figure 16: Training performance of κ-PI-TRPO (Left) and the ‘naive’ baseline N (κ) = T (Right)
on the CartPole environment.
Note that since the CartPole is extremely simple, we do not see a clear difference between the κ
values that are closer to 1.0 (see Figure 16). Below, we observe the performance when the discount
factor γ is lowered (see Figure 17). Since, there is a ceiling of R = 200 on the maximum achievable
return, it makes intuitive sense that observing the κ effect for a lower gamma value such as γ = 0.36
will allow us to see a clearer trade-off between κ values. To this end, we also plot the results for
when the discount factor is set to 0.36.
5 0 5 0 5 0 5
7 5 2 0 7 5 2
p」PM①Hcυpos五山 UP①W
Figure 17: Training performance when lowering the discount factor (Left) and of κ-PI-TRPO (Left)
when discount factor is set to 0.36 (Right) on the CartPole environment.
The intuitive idea behind κ-PI, and κ-VI similarly, is that at every time step, we wish to solve
a simpler sub-problem, i.e. the γκ discounted MDP. Although, we are solving an easier/shorter
horizon problem, in doing so, the bias induced is taken care of by the modified reward in this new
MDP. Therefore, it becomes interesting to look at how κ affects its two contributions, one being the
discounting, the other being the weighting of the shaped reward (see eq. 11). Below we look at what
happens when each of these terms are made κ independent, one at a time, while varying κ for the
other term. To make this clear, we introduce different notations for both such κ instances, one being
κd (responsible for discounting) and the other being κs (responsible for shaping).
26
Under review as a conference paper at ICLR 2020
πκ(s) ∈ arg max E[	(γκd)t [r(st,at) + (1 -
∏	I_{-J
t≥0』, Lr
discounting
κs)γV (st+1)] | s0 = s, π], ∀s ∈ S
-一	J
^^{^^™
shaping
5 0 5 0 5 0
7 5 2 0 7 5
EΠ3M ①c≤əpoud 山 UE ① W
0	25	50	75	100	125	150	175	200
Iterations
5 0 5 0 5 0
7 5 2 0 7 5
ΞfaMφαΦPOS'Q.山 upφw
(11)
Figure 18: Training performance when contribution of κ is fixed for a) the shaped reward (Left) and
b) the discount factor (Right) on the CartPole environment.
We see something interesting here. For the CartPole domain, the shaping term does not seem to
have any effect on the performance (Figure 18(b)), while the discounting term does. This implies
that the problem does not suffer from any bias issues. Thus, the correction provided by the shaped
term is not needed. However, this is not true for other more complex problems. This is also why we
see a similar result when lowering γ in this case, but not for more complex problems.
C.2 Mountain Car
In this section, we report results for the Mountain Car environment. Contrary to the CartPole results,
where lowering the κ values degraded the performance, we observe that performance deteriorates
when κ is increased. We also plot a bar graph, with the cumulative score on the y axis and different
κ values on the x axis.
Ooooo
1 2 3 4 5
- - - - -
ΞEMφɑΦPOS'Q.山 UP ① W
MountainCar, Λ∕(κ) = T
0	25	50	75	100	125	150	175	200
Iterations
O 25	50	75 IOO 125	150	175	200
Iterations
Figure 19: Training performance of κ-PI-TRPO (Top Left: training curves, Top Right: cumulative
return) and the ‘naive’ baseline N(κ) = T (Bottom) on the Mountain Car environment.
We use the continuous Mountain Car domain here, which has been shown to create exploration
issues. Therefore, without receiving any positive reward, using a κ value of 0 in the case of dis-
counting (solving the 1 step problem has the least negative reward) and of 1 in the case of shaping
results in the best performance.
27
Under review as a conference paper at ICLR 2020
Figure 20: Training performance when lowering the discount factor (Left) and of κ-PI-TRPO (Left)
when discount factor is set to 0.995 (Right) on the MountainCar environment.
Figure 21: Training performance when contribution of κ is fixed for a) the shaped reward (Left) and
b) the discount factor (Right) on the MountainCar environment.
C.3 Pendulum
In this section, we move to the Pendulum environment, a domain where we see a non-trivial best κ
value. This is due to there not being a ceiling on the maximum possible return, which is the case in
CartPole.
Figure 22: Training performance of κ-PI-TRPO (Top Left: training curves, Top Right: cumulative
return) and the ‘naive’ baseline N (κ) = T (Bottom) on the Pendulum environment.
Pendulum, Λ∕(κ) = T
28
Under review as a conference paper at ICLR 2020
Choosing the best γ value and running κ-PI on it results in an improved performance for all κ values
(see Figure 23).
Figure 23: Training performance when lowering the discount factor (Left) and of κ-PI-TRPO (Left)
when discount factor is set to 0.96 (Right) on the Pendulum environment.
Pendulum, CFA = 0.2, κs=l
Iterations
Figure 24: Training performance when contribution of κ is fixed for a) the shaped reward (Left) and
b) the discount factor (Right) on the Pendulum environment.
To summarize, we believe that in inherently short horizon domains (dense, per time step reward),
such as the Mujoco continuous control tasks, the discounting produced by κ-PI and VI is shown to
cause major improvement in performance over the TRPO baselines. This is reinforced by the results
of lowering the discount factor experiments. On the other hand, in inherently long horizon domains
(sparse, end of trajectory reward), such as in Atari, the shaping produced by κ-PI and VI is supposed
to cause the major improvement over the DQN baselines. Again, this is supported by the fact that
lowering the discount factor experiments actually result in deterioration in performance.
29
Under review as a conference paper at ICLR 2020
C.4 SMOOTHNESS INκ
Ant, CFA = 0∙2
HaIfCheetah, CFA = 0.2
0.0 0.36 0.68 0.84 0.92 0.96 0.98 0.99 1.0
κ values
0.0 0.36 0.68 0.84 0.92 0.96 0.98 0.99 0.995 1.0
κ values
Figure 25: Cumulative training performance of κ-PI-TRPO on HalfCheetah (Left, corresponds to
Figure 12) and Ant (Right, corresponds to Figure 11) environments.
30