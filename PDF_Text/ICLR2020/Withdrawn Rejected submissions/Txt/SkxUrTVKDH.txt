Under review as a conference paper at ICLR 2020
Split LBI for Deep Learning: Structural Spar-
sity via Differential Inclusion Paths
Anonymous authors
Paper under double-blind review
Ab stract
Over-parameterization is ubiquitous nowadays in training neural networks to ben-
efit both optimization in seeking global optima and generalization in reducing
prediction error. However, compressive networks are desired in many real world
applications and direct training of small networks may be trapped in local op-
tima. In this paper, instead of pruning or distilling over-parameterized models to
compressive ones, we propose a new approach based on differential inclusions of
inverse scale spaces, that generates a family of models from simple to complex
ones by coupling gradient descent and mirror descent to explore model structural
sparsity. It has a simple discretization, called the Split Linearized Bregman Iteration
(SplitLBI), whose global convergence analysis in deep learning is established that
from any initializations, algorithmic iterations converge to a critical point of empir-
ical risks. Experimental evidence shows that SplitLBI may achieve comparable
and even better performance than other training algorithms on ResNet-18 in large
scale training on ImageNet-2012 dataset etc., while with early stopping it unveils
effective subnet architecture with comparable test accuracy to dense models after
retraining instead of pruning well-trained ones.
1	Introduction
The expressive power of deep neural networks comes from the millions of parameters, which are
optimized by Stochastic Gradient Descent (SGD) (Bottou, 2010) and variants like Adam (Kingma &
Ba, 2015). Remarkably, model over-parameterization helps both optimization and generalization. For
optimization, over-parameterization may simplify the landscape of empirical risks toward locating
global optima efficiently by gradient descent method (Mei et al., 2018; 2019; Venturi et al., 2018;
Allen-Zhu et al., 2018; Du et al., 2018). On the other hand, over-parameterization does not necessarily
result in a bad generalization or overfitting (Zhang et al., 2017), especially when some weight-size
dependent complexities are controlled (Bartlett, 1997; Bartlett et al., 2017; Golowich et al., 2018;
Neyshabur et al., 2019).
However, compressive networks are desired in many real world applications, e.g. robotics, self-
driving cars, and augmented reality. Despite that `1 regularization has been applied to deep learning
to enforce the sparsity on weights toward compact, memory efficient networks, it sacrifices some
prediction performance (Collins & Kohli, 2014). This is because that the weights learned in neural
networks are highly correlated, and `1 regularization on such weights violates the incoherence or
irrepresentable conditions needed for sparse model selection (Donoho & Huo, 2001; Tropp, 2004;
Zhao & Yu, 2006), leading to spurious selections with poor generalization. On the other hand, `2
regularization is often utilized for correlated weights as some low-pass filtering, sometimes in the
form of weight decay (Loshchilov & Hutter, 2019) or early stopping (Yao et al., 2007; Wei et al.,
2017). Furthermore, group sparsity regularization (Yuan & Lin, 2006) has also been applied to neural
networks, such as finding optimal number of neuron groups (Alvarez & Salzmann, 2016) and exerting
good data locality with structured sparsity (Wen et al., 2016; Yoon & Hwang, 2017).
Yet, without the aid of over-parameterization, directly training a compressive model architecture
may meet the obstacle of being trapped in local optima in contemporary experience. Alternatively,
researchers in practice typically start from training a big model using common task datasets like
ImageNet, and then prune or distill such big models to small ones without sacrificing too much of the
performance (Jaderberg et al., 2014; Han et al., 2015; Zhu et al., 2017; Zhou et al., 2017; Zhang et al.,
1
Under review as a conference paper at ICLR 2020
2016; Li et al., 2017; Abbasi-Asl & Yu, 2017; Yang et al., 2018; Arora et al., 2018). In particular,
a recent study (Frankle & Carbin, 2019) created the lottery ticket hypothesis based on empirical
observations: “dense, randomly-initialized, feed-forward networks contain subnetworks (winning
tickets) that - when trained in isolation - reach test accuracy comparable to the original network in a
similar number of iterations". How to effectively reduce an over-parameterized model thus becomes
the key to compressive deep learning. Yet, Liu et al. (2019) raised a question, is it necessary to fully
train a dense, over-parameterized model before finding important structural sparsity?
In this paper, we provide a novel answer by exploiting a dynamic approach to deep learning with
structural sparsity. We are able to establish a family of neural networks, from simple to complex,
by following regularization paths as solutions of differential inclusions of inverse scale spaces. Our
key idea is to design some dynamics that simultaneously exploit over-parameterized models and
structural sparsity. To achieve this goal, the original network parameters are lifted to a coupled
pair, with one weight set W of parameters following the standard gradient descend to explore the
over-parameterized model space, while the other set of parameters learning structure sparsity in an
inverse scale space, i.e., structural sparsity set Γ. The large-scale important parameters are learned at
a fast speed while the small unimportant ones are learned at a slow speed. The two sets of parameters
are coupled in an `2 regularization. The dynamics enjoys a simple discretization, i.e. the Split
Linearized Bregman Iteration (SplitLBI), with provable global convergence guarantee shown in this
paper. Here, SplitLBI is a natural extension of SGD with structural sparsity exploration: SplitLBI
reduces to the standard gradient descent method when the coupling regularization is weak, while it
leads to a sparse mirror descent when the coupling is strong.
Critically, SplitLBI enjoys a nice property that important subnet architecture can be rapidly learned via
the structural sparsity parameter Γ following the iterative regularization path, without fully training a
dense network first. Particularly, the support set of structural sparsity parameter Γ learned in the early
stage of this inverse scale space discloses important sparse subnet architectures. Such architectures
can be fine-tuned or retrained to achieve comparable test accuracy as the dense, over-parameterized
networks. As a result, the structural sparsity parameter Γ may enable us to rapidly find “winning
tickets” in early training epochs for the “lottery” of identifying successful subnetworks that bear
comparable test accuracy to the dense ones. This point is empirically validated in our experiments.
Historically, the Linearized Bregman Iteration (LBI) was firstly proposed in applied mathematics as
iterative regularization paths for image reconstruction and compressed sensing (Osher et al., 2005; Yin
et al., 2008), later applied to logistic regression (Shi et al., 2013). The convergence analysis was given
for convex problems (Yin et al., 2008; Cai et al., 2009), yet remaining open for non-convex problems
met in deep learning. Osher et al. (2016) established statistical model selection consistency for high
dimensional linear regression under the same irrepresentable condition as Lasso, later extended to
generalized linear models (Huang & Yao, 2018). To relax such conditions, SplitLBI was proposed
by Huang et al. (2016) to learn structural sparsity in linear models under weaker conditions than
generalized Lasso, that was successfully applied in medical image analysis (Sun et al., 2017) and
computer vision (Zhao et al., 2018). In this paper, it is the first time that SplitLBI is exploited to
train highly non-convex neural networks with structural sparsity, together with a global convergence
analysis based on the KUrdyka-LOjasieWiCz framework LOjasieWiCz (1963).
Contributions. (1) SplitLBI, as an extension of SGD, is applied to deep learning by exploring
both over-parameterized models and structural sparsity in the inverse scale space. (2) Global
convergence of SplitLBI in such a nonconvex optimization is established based on the Kurdyka-
Lojasiewicz framework, that the whole iterative sequence converges to a critical point of the empirical
loss function from arbitrary initializations. (3) Stochastic variants of SplitLBI demonstrate the
comparable and even better performance than other training algorithms on ResNet-18 in large scale
training such as ImageNet-2012, among other datasets, together with additional structural sparsity
in successful models for interpretability. (4) Structural sparsity parameters in SplitLBI provide
important information about subnetwork architecture with comparable or even better accuracies than
dense models before and after retraining -- SplitLBI with early stopping can provide fast “winning
tickets” without fully training dense, over-parameterized models.
2
Under review as a conference paper at ICLR 2020
Figure 1: Visualization of solution path and filter patterns in the third convolutional layer (i.e., conv.c5) of
LetNet-5, trained on MNIST. The left figure shows the magnitude changes for each filter of the models trained
by SPlitLBI and SGD, where x-axis and y-axis indicate the training epochs, and filter magnitudes ('2-norm),
respectively. The SplitLBI path of filters selected in the support of Γ are drawn in blue color, while the red color
curves represent the filters that are not important and outside the support of Γ. We visualize the corresponding
learned filters by Erhan et al. (2009) at 20 (blue), 40 (green), and 80 (black) epochs, which are shown in the right
figure with the corresponding color bounding boxes, i.e., blue, green, and black, respectively. It shows that our
SplitLBI enjoys a sparse selection of filters without sacrificing accuracy (see Table 1).
2	Methodology
The supervised learning task learns a mapping ΦW : X → Y, from input space X to output space
Y , with a parameter W such as weights in neural networks, by minimizing certain loss functions
on training samples Ln(W) = 1 En=I '(yi, ΦW(Xi)). For example, a neural network of l-layer is
defined as ΦW(x) = σl(Wlσl-1 (Wl-1 ∙ ∙ ∙ σ1 (W1 x)), where W = {Wi}i=1, σi is the nonlinear
activation function of the i-th layer.
Differential Inclusion of Inverse Scale Space. Consider the following dynamics,
*
Wt	-
W = -Vw L(Wt, Γt)	(1a)
κ
•	— Z	、
匕=-Vr L(Wt, Γt)	(1b)
匕 ∈ ∂Ω(Γt)	(1c)
where V is a sub-gradient of Ω(Γ) := Ωλ(Γ) + 白 kΓ∣2 for some sparsity-enforced regularization
Ωλ(Γ) = λΩ1 (Γ) (λ ∈ R+) such as Lasso or group Lasso penalties for Ω1 (Γ), κ > 0 is a damping
parameter such that the solution path is continuous, and the augmented loss function is
1
L(W, Γ)= Ln (W) + 2νIIW - Γk2,
(2)
with ν > 0 controlling the gap admitted between W and Γ. Compared to the original loss function
Ln (W), the L (W, Γ) additionally adopt the variable splitting strategy, by lifting the original neural
network parameter W to (W, Γ) with Γ modeling the structural sparsity of W. For simplicity, we
assumed L is differentiable with respect to W here, otherwise the gradient in Eq. (1a) is understood
as subgradient and the equation becomes an inclusion.
The differential inclusion system (1), called Split Inverse Scale Space (SplitISS), can be understood as
a gradient descent flow of Wt in the proximity of Γt and a mirror descent flow (Nemirovski & Yudin,
1983) of Γt associated with a sparsity enforcement penalty Ω. In mirror descent flow, gradient descent
goes on the dual space consisting of sub-gradients Vt , driving the flow in sparse primal space of Γt .
For a large enough ν, it reduces to the gradient descent method for Wt . Yet the solution path of Γt
exhibits the following property in the separation of scales: starting at the zero, important parameters
of large scale will be learned fast, popping up to be nonzeros early, while unimportant parameters of
small scale will be learned slowly, appearing to be nonzeros late. In fact, taking Ωλ(Γ) = ∣∣Γ∣1 and
K → ∞ for simplicity, V as the subgradient of Ωt, undergoes a gradient descent flow before reaching
the '∞-unit box, which implies that Γt = 0 in this stage. The earlier a component in Vt reaches
3
Under review as a conference paper at ICLR 2020
the '∞ -unit box, the earlier a corresponding component in Γt becomes nonzero and rapidly evolves
toward a critical point of C under gradient flow. On the other hand, the Wt follows the gradient
descent with a standard '2-regularization. Therefore, Wt closely follows dynamics of Γt whose
important parameters are selected. Such a property is called as the inverse scale space in applied
mathematics (Burger et al., 2006) and recently was shown to achieve statistical model selection
consistency in high dimensional linear regression (Osher et al., 2016) and general linear models
(Huang & Yao, 2018), with a reduction of bias as κ increases. In this paper, we shall see that the
inverse scale space property still holds empirically for the highly nonconvex neural network training
via Eq. (1). For example, Fig. 1 shows a LeNet trained on MNIST by the discretized dynamics, where
important sparse filters are selected in early epochs while the popular SGD returns dense filters.
Compared with directly enforcing a penalty function such as `1 or `2 regularization
^ ^ .
minRn(W):= Lbn (W)+Ωλ (W), λ ∈ 股十.	(3)
W
SplitISS avoids the parameter correlation problem in over-parameterized models. In fact, a necessary
and sufficient condition for Lasso or `1 -type sparse model selection is the incoherence or irrepre-
sentable conditions (Tropp (2004); Zhao & Yu (2006)) that are violated for highly correlated weight
parameters, leading to spurious discoveries. In contrast, Huang et al. (2018) showed that equipped
with such a variable splitting where Γ enjoys an orthogonal design where the restricted Hessian of
the augmented loss on Γ is orthogonal, the SplitISS can achieve model selection consistency under
weaker conditions than generalized Lasso, relaxing the incoherence or irrepresentable conditions
when parameters are highly correlated. For weight parameter W, instead of directly being imposed
with '1-sparsity, it adopts '2-regularization in the proximity of the sparse path of Γ that admits
simultaneously exploring highly correlated parameters in over-parameterized models and sparsity
regularization.
Split Linearized Bregman Iterations. SplitISS admits an extremely simple discrete approximation,
using the Euler forward discretization of dynamics (1):
Wk+1 = Wk — Kak ∙ Vw L (Wk, Γ),	(4a)
Vk+1 = Vk — αk ∙ VrL (Wk, Γk),	(4b)
Γk+ι = κ ∙ ProxΩλ (Vk+1),	(4c)
where V0 = Γ0 = 0, W0 can be small random numbers such as Gaussian distribution in neural
networks, for some complex networks it can be initialized as common setting.The proximal map in
Eq. (4c) that controls the sparsity of Γ is given by
ProxΩλ (V) = arg min {1 ∣∣Γ — V∣∣2+Ωλ(Γ)} ,	(5)
We shall call such an iterative procedure as Split Linearized Bregman Iteration (SplitLBI), that was
firstly coined in Huang et al. (2016) as an iterative regularization path for sparse modeling in high
dimensional statistics. In the application to neural networks, the loss becomes highly non-convex, the
SplitLBI returns a sequence of sparse models from simple to complex ones whose global convergence
condition to be shown below, while solving Eq. (3) at various levels of λ might not be tractable
except for over-parameterized models.
The sparsity-enforcement penalty used in convolutional neural networks can be chosen as follows. Our
sparsity framework aims at regularizing the groups of weight parameters using group Lasso penalty
(Yuan & Lin, 2006), Ωι(Γ) = Pg Ilrgk2, where Ilrgk2 = JPi=[ (Γg] and ∣Γg| is the number of
weights in Γg. Thus Eq. (4c) has a closed form solution Γg = K ∙ max (0,1 一 1/k Vg ∣∣2) Vg for the
g-th filter. We treat convolutional and fully connected layers in different ways.
(1)	For a convolutional layer, rg = rg (cin, cout, size) denote the convolutional filters where size
denotes the kernel size and cin and cout denote the numbers of input channels and output channels,
respectively. When we regard each group as each convolutional filter, g = cout; otherwise for weight
sparsity, g can be every element in the filter that reduces to the Lasso.
(2)	For a fully connected layer, r = r(cin, cout) where cin and cout denote the numbers of inputs
and outputs of the fully connected layer. Each group g corresponds to each element (i, j), and the
group Lasso penalty degenerates to the Lasso penalty.
4
Under review as a conference paper at ICLR 2020
3 Global Convergence of SplitLBI for Neural Networks
We present a theorem that guarantees the global convergence of SplitLBI, i.e. from any intialization,
the SPlitLBI sequence converges to a critical point of L. Our treatment extends the block coordinate
descent (BCD) studied in Zeng et al. (2019), with a crucial difference being the mirror descent
involved in SplitLBI. Instead of the splitting loss in BCD (Zeng et al., 2019), a new Lyapunov
function is developed here to meet the KUrdyka-LojaSieWiCz property LojaSieWiCz (1963). Xue &
Xin (2018) studied convergence of variable splitting method for single hidden layer networks with
Gaussian inputs.
Let P := (W, Γ). FolloWing Huang & Yao (2018), the SplitLBI algorithm in Eq. (4a-4c) can be
reWritten as the folloWing standard Linearized Bregman Iteration,
Pk+1 = arg min {〈P - Pk,αVC(Pk))+ Bψk (P,Pk)} ,	(6)
Where
Ψ(P) = Ωλ(r) + ɪ kP k2 = Ωλ(r) + ɪ kW k2 + ɪ krk2,	⑺
2κ	2κ	2κ
pk ∈ ∂Ψ(Pk), and BΨq is the Bregman divergence associated With convex function Ψ, defined by
BΨq (P, Q) := Ψ(P) - Ψ(Q) - hq,P - Qi, for some q ∈ ∂Ψ(Q).	(8)
Without loss of generality, consider λ = 1in the sequel. One can establish the global convergence of
SplitLBI under the folloWing assumptions.
Assumption 1. Suppose that: (a) Lbn(W) = 1 PZi '(yi, Φw(Xi)) is continuous differentiable and
VLn IS Lipschitz continuous with a positive constant Lip; (b)Ln(W) has bounded level sets; (c)
Ln(W) is lower bounded (without loss of generality, we assume that the lower bound is 0); (d) Ω is
a proper lower semi-continuous convex function and has locally bounded subgradients, that is, for
every compact set S ⊂ Rn, there exists a constant C > 0 such thatfor all Γ ∈ S and all g ∈ ∂Ω(Γ),
there holds kg k ≤ C; and (e) the Lyapunov function
F(Pe) = αL(W,Γ) + BΩ(Γ, Γ),
(9)
is a KUrdyka-七OjaSiewiCzfUnCtiOn on any bounded set, where Bg (Γ, Γ) := Ω(Γ) — Ω(Γ) — hg, Γ — Γ),
Γ ∈ ∂Ω*(g), and Ω* is the conjugate of Ω defined as
Ω*(g) := sup {(U,g)- Ω(U)}.
U∈Rn
Remark 1. Assumption 1 (a)-(c) are regular in the analysis of nonconvex algorithm (see, Attouch
et al. (2013) for instance), while Assumption 1 (d) is also mild including all Lipschitz continuous
convex function over a compact set. Some typical examples satisfying Assumption 1(d) are the `1
norm, group `1 norm, and every continuously differentiable penalties. By Eq. (9) and the definition
of conjugate, the Lyapunov function F can be rewritten as follows,
F(W,Γ, g) = αL(W, Γ) + Ω(Γ) + Ω*(g) -<Γ, g).
(10)
NoW We are ready to present the main theorem.
Theorem 1. [Global Convergence of SplitLBI] Suppose that Assumption 1 holds. Let (Wk, Γk) be
the sequence generated by SplitLBI (Eq. (4a-4c)) with a finite initialization. If
2
0 <αk= α<κ(Lip + ν-I)，
then (Wk, Γk) converges to a critical point of L defined in Eq. (2), and {W k} converges to a critical
.^ _____________
point of Lbn (W).
Applying to the neural netWorks, typical examples are summarized in the folloWing corollary.
5
Under review as a conference paper at ICLR 2020
Corollary 1. Let {Wk , Γk, gk} be a sequence generated by SLBI (16a-16c) for neural network
training where (a) ` is any smooth definable loss function, such as the square loss (t2), exponential loss
(et), logistic loss log(1 + e-t), and cross-entropy loss; (b) σi is any smooth definable activation, such
as linear activation (t), sigmoid (ι+1-), hyperbolic tangent (e：+e-t), and SofPlus (C log(1 + ect)
for some c > 0)asa smooth approximation OfReLU; (c) Ω is the group Lasso. Then the sequence
{Wk} converges to a stationary point of Ln(W) under the conditions of Theorem 1.
Proofs of Theorem 1 and Corollary 1 are given in Appendix A.
4	Experiments with Stochastic SplitLBI
We begin with some stochastic variants of SplitLBI and implementations, followed by four groups of
experiments demonstrating the utilities of weight parameter Wt and structural sparsity parameter Γt
in prediction, interpretability, and capturing effective sparse subnetworks.
Batch Split LBI. For neural network training with large datasets, stochastic approximation of the
gradients in Split LBI over the mini-batch (X, Y)batcht is adopted to update the parameter W,
RW = VWLbn (W) | (X,Y)batcht .	(II)
SplitLBI with momentum (Mom). Inspired by the variants of SGD, the momentum term can be
also incorporated to the standard Split LBI that leads to the following updates of W by replacing Eq
(4a) with,
〜 -，____ _ ,
vt+1 = Tvt + V W L(Wt, Γt)	(12a)
Wt+1 = Wt - καvt+1	(12b)
where τ is the momentum factor, empirically setting as 0.9 in default. One immediate application of
such stochastic algorithms of SplitLBI is to “boost networks", i.e. growing a network from the null to
a complex one by sequentially applying our algorithm on subnets with increasing complexities.
SplitLBI with momentum and weight decay (Mom-Wd). The update formulation is,
〜 -，____ _ ,
vt+1 = Tvt + V W L(Wt, Γt)	(13)
Wt+1 = Wt - καvt+1 - βWt	(14)
where β is set as 1e-4.
Implementation. Various algorithms are evaluated over the various backbones - LeNet (LeCUn
et al., 2015), AlexNet (Krizhevsky et al., 2012), VGG (Simonyan & Zisserman, 2014), and ResNet
(He et al., 2016) etc., respectively. For MNIST and Cifar-10, the default hyper-parameters of Split
LBI are κ = 1, ν = 10 and αk is set as 0.1, decreased by 1/10 every 30 epochs. In ImageNet-2012,
the Split LBI utilizes κ = 1, ν = 1000, and αk is initially set as 0.1, decays 1/10 every 30 epochs.
We set λ = 1 in Eq. (5) by default, unless otherwise specified. On MNIST and Cifar-10, the batch
size is set as 128; and for all methods, the batch size of ImageNet 2012 is 256. The standard data
augmentation implemented in pytorch is applied to Cifar-10 and ImageNet2012 datasets, as He et al.
(2016). The weights of all models are initialized as He et al. (2015). In the following experiments, we
define sparsity as percentage of non-zero parameters, i.e. the number of non-zero weights dividing
the total number of weights in consideration, that equals to one minus the pruning rate of the network.
We also have the reproducible source codes 1.
4.1	Image Classification
In SplitLBI, the weight parameter Wt explores over-parameterized models that can achieve the
state-of-the-art performance in large scale training such as ImageNet-2012 classification.
Experimental Design. We compare different variants of SGD and Adam in the experiments. By
default, the learning rate of competitors is set as 0.1 for SGD and its variant and 0.001 for Adam and
its variants, and gradually decreased by 1/10 every 30 epochs. In particular, we have,
1https://anonymous.4open.science/repository/d22bbbc8-50d5-4e60-b2e8-4ded4e93db63/
Split_LBI_code
6
Under review as a conference paper at ICLR 2020
Dataset		MNIST	Cifar-10	ImageNet-2012	
Models	Variants	LeNet	ResNet-20	AlexNet	ResNet-18
	Naive	98.87^^	86.46	-/-	60.76/79.18
	l1	98.52	67.60	-/-	-/-
SGD	Mom	99.16	89.44	55.14/78.09	66.98/86.97
	Mom-Wd?	99.23	90.31	56.55/79.09	69.76/89.18
	Nesterov	99.23	90.18	-/-	70.19/89.30
	Naive	99.19^^	89.14	-/-	59.66/83.28
	Adabound	99.15	87.89	-/-	-/-
Adam	Adagrad	99.02	88.17	-/-	-/-
	Amsgrad	99.14	88.68	-/-	-/-
	Radam	99.08	88.44	-/-	-/-
	Naive	99.02^^	89.26	55.06/77.69	65.26/86.57
SplitLBI	Mom	99.19	89.72	56.23/78.48	68.55/87.85
	Mom-Wd	99.20	89.95	57.09/79.86	70.55/89.56
Table 1: Top-1/Top-5 accuracy(%) on ImageNet-2012 and test accuracy on MNIST/Cifar-10. ?:
results from the official pytorch website. We use the official pytorch codes to run the competitors. All
models are trained by 100 epochs. In this table, we run the experiment by ourselves except for SGD
Mom-Wd on ImageNet which is reported in https://pytorch.org/docs/stable/torchvision/models.html.
SGD: (1) Naive SGD: the standard SGD with batch input. (2) SGD with l1 penalty (Lasso). The
l1 norm is applied to penalize the weights of SGD by encouraging the sparsity of learned model,
with the regularization parameter of the l1 penalty term being set as 1e-3 (3) SGD with momentum
(Mom): we utilize momentum 0.9 in SGD. (4) SGD with momentum and weight decay (Mom-Wd):
we set the momentum 0.9 and the standard l2 weight decay with the coefficient weight 1e-4. (5)
SGD with Nesterov (Nesterov): the SGD uses nesterov momentum 0.9.
Adam: (1) Naive Adam: it refers to the standard version of Adam. We report the results of several
recent variants of Adam, including (2) Adabound, (3) Adagrad, (4) Amsgrad, and (5) Radam.
SplitLBI achieves the state-of-the-art performance on ImageNet-2012, etc. Tab. 1 shows the
experimental results on ImageNet-2012, Cifar-10, and MNIST of some classical networks -- LeNet,
AlexNet and ResNet. Our SplitLBI variants may achieve comparable or even better performance than
SGD variants in 100 epochs, indicating the efficacy in learning dense, over-parameterized models.
4.2	SplitLBI Learns Sparse Filters for Improved Interpretation
Figure 2: Visualization of the first convolutional layer filters of ResNet-18 trained on ImageNet-
2012. Given the input image and initial weights visualized in the middle, filter response gradients
at 20 (purple), 40 (green), and 60 (black) epochs are visualized by Springenberg et al. (2014). The
”SLBI-10” (”SLBI-1”) in the right figure refers to SplitLBI with κ = 10 and κ = 1, respectively.
7
Under review as a conference paper at ICLR 2020
In SplitLBI, the structural sparsity parameter Γt explores important sub-network architectures that
contributes significantly to the loss or error reduction in early training stages. Through the '2-coupling,
structural sparsity parameter Γt may guide the weight parameter to explore those sparse models
in favour of improved interpretabiity. For example, Fig. 1 visualizes some sparse filters learned
by SplitLBI of LeNet-5 trained on MNIST (with κ = 10 and weight decay every 40 epochs), in
comparison with dense filters learned by SGD. The activation pattern of such sparse filters favours
high order global correlations between pixels of input images. To further reveal the insights of learned
patterns of SplitLBI, we visualize the first convolutional layer of ResNet-18 on ImageNet-2012 along
the training path of our SplitLBI as in Fig. 2. The left figure compares the training and validation
accuracy of SplitLBI and SGD. The right figure compares visualizations of the filters learned by
SplitLBI and SGD using Springenberg et al. (2014).
Implementation. To be specific, denote the weights of an l-layer network as {W 1,W2, ∙∙∙ ,Wl}.
For the i-th layer weights Wi, denote the j-th channel Wji. Then we compute the gradient of the
sum of the feature map computed from each filter Wji with respect to the input image (here a snake
image). We further conduct the min-max normalization to the gradient image, and generate the final
visualization map. The right figure compares the visualized gradient images of first convolutional
layer of 64 filters with 7 × 7 receptive fields. We visualize the models parameters at 20 (purple), 40
(green), and 60 (black) epochs, respectively, which corresponds to the bounding boxes in the right
figure annotated by the corresponding colors, i.e., purple, green, and black. We order the gradient
images produced from 64 filters by the descending order of the magnitude ('2-norm) of filters, i.e.,
images are ordered from the upper left to the bottom right. For comparison, we also provide the
visualized gradient from random initialized weights.
Filters learned by ImageNet prefer to non-semantic texture rather than shape and color. The
filters of high norms mostly focus on the texture and shape information, while color information is
with the filters of small magnitudes. This phenomenon is in accordance with observation of Abbasi-
Asl & Yu (2017) that filters mainly of color information can be pruned for saving computational cost.
Moreover, among the filters of high magnitudes, most of them capture non-semantic textures while
few pursue shapes. This shows that the first convolutional layer of ResNet-18 trained on ImageNet
learned non-semantic textures rather than shape to do image classification tasks, in accordance with
recent studies (Geirhos et al., 2019). How to enhance the semantic shape invariance learning, is
arguably a key to improve the robustness of convolutional neural networks.
4.3	Global Convergence and Structural Sparsity of SplitLBI
We conduct ablation studies based on Cifar-10 dataset with VGG-16 and ResNet-56 to evaluate (i)
global convergence of L; and (ii) the structural sparsity learned by Γt via exploring test accuracies
of sparse models obtained by projecting Wt onto the support set of Γt (mask), by varying two key
hyper-parameters κ and ν.
Implementation. We choose SplitLBI with momentum and weight decay, since it achieves very
good performance on large-scale experiments. Specifically, we have two set of experiments, where
each experiment is repeated for 5 times: (1) we fix ν = 100 and vary κ = 1, 2, 5, 10, where sparsity
of Γt and validation accuracies of sparse models are shown in top row of Fig. 4. Note that we keep
K ∙ αk = 0.1 in Eq (1a), to make comparable learning rate of each variant, and also consistent with
SGD. Thus the learning rate αk will be adjusted by different κ values. (2) we fix κ = 1, and validate
the results of SplitLBI with ν = 10, 20, 50, 100, 200, 500, 1000, 2000 in the second row of Fig. 4
with the learning rate αk = 0.1. Moreover, rather than using sparse models associated with Γt, Fig.
6 in Appendix shows the validation accuracies of full models learned by Wt .
SplitLBI converges to Critical Point. Figure 3 shows the curves of training loss (Ln) and
accuracies, with each point representing the average and variance bar over 5 times. As shown, both
training loss and training accuracy will converge, which validates our theoretical result in Theorem 1.
Besides, larger κ brings in slower convergence, which agrees with the analysis the convergence rate
is inversely scale to κ in Lemma A.5.
Sparse subnetworks achieve comparable performance to dense models without fine-tuning or
retraining. From the experiments above, the sparsity of Γ grows as κ and ν increase. While large κ
may cause a small number of important parameters growing rapidly, large ν will decouple Wt and Γt
such that the growth of Wt does not affect Γt that may over-sparsify and deteriorate model accuracies.
8
Under review as a conference paper at ICLR 2020
Figure 3: Loss and training accuracy by different κ and ν validates the global convergence of SplitLBI.
The X-axis and Y-axis indicate the training epochs, and sparsity/accuracy. The results are repeated
for 5 rounds, by keeping the exactly same initialization for each model. In each round, we use the
same initialization for every hyperparameter. For all models, we train for 160 epochs with initial
learning rate (lr) of 0.1 and decrease by 0.1 at epoch 80 and 120.

Sparse Model validation Accuracy for Different υ -

SParSIty for DIfferent K for ReSNet56
Figure 4: Sparsity and validation accuracy by different κ and ν show that moderate sparse models
may achieve comparable test accuracies to dense models without fine-tuning. Sparsity is obtained as
the percentage of nonzeros in Γt and sparse model at epoch t is obtained by projection of Wt onto the
support set of Γt, i.e. pruning the weights corresponding to zeros in Γt. The best accuracies achieved
are recorded in Tab. 2 and 4 of Appendix for different κ and ν, respectively. X-axis and Y-axis
indicate the training epochs, and sparsity/accuracy. The results are repeated for 5 times. Shaded area
indicates the variance; and in each round, we keep the exactly same initialization for each model. In
each round, we use the same initialization for every hyperparameter. For all the model, we train for
160 epochs with initial learning rate (lr) of 0.1 and decrease by 0.1 at epoch 80 and 120.
Thus a moderate choice of κ and ν is preferred in practice. In all cases, one can see that moderate
sparse models can achieve comparable predictive power to dense models, even without fine-tuning or
retraining. This shows that the structural sparsity parameter Γt can indeed capture important weight
parameter Wt through their coupling.
4.4	SplitLBI with Early Stopping and Retrain Finds Effective Sub-Networks
Equipped with early stopping, Γt in early epochs may learn effective subnetworks (i.e. “winning
tickets” (Frankle & Carbin, 2019; Liu et al., 2019)) that achieve comparable or even better performance
after retraining than existing pruning strategies by SGD.
9
Under review as a conference paper at ICLR 2020
Figure 5: SplitLBI with early stopping finds sparse subnets whose test accuracies (stars) after retrain
are comparable or even better than the baselines (Network Slimming (reproduced by the released
codes from Liu et al. (2019) ) , Soft-Filter Pruning(Tab. 10), Scratch-B(Tab. 10), Scratch-E(Tab. 10),
and “Rethinking-Lottery” (Tab. 9a)) as reported in Liu et al. (2019). Sparse filters of VGG-16 and
ResNet-56 are show in (a) and (b), while sparse weights of VGG-16 and ResNet-50 are shown in (c)
and (d).
Experimental Design. We adopt a comparison baseline as the one-shot pruning strategy in Frankle
& Carbin (2019), which firstly trains a dense over-parameterized model by SGD for T = 160 epochs
and find the sparse structure by pruning weights or filters (Liu et al., 2019), then secondly retrains the
structure from the scratch with T epochs from the same initialization as the first step. For SplitLBI,
instead of pruning weights/filters from dense models, we directly utilize the structural sparsity Γt at
different training epochs to define the subnet architecture, followed by retrain-from-scratch (Fine-tune
is shown in Appendix Sec. D with preliminary results). Experiments are conducted on Cifar-10
dataset where We still use VGG-16, ResNet-50, and ResNet-56 as the networks to make direct
comparisons to previous works. SplitLBI uses momentum and weight decay with hyperparameters
shown in Tab. 10 in Appendix. In particular, we set λ = 0.1, and 0.05 for VGG-16, and ResNet-56
respectively, since ResNet-56 has less parameters than VGG-16. Furthermore, we introduce another
variant of our SplitLBI by using Lasso rather than group lasso penalty for Γt to sparsify the weights
of convolutional filters; and the corresponding models are denoted as VGG-16 (Lasso) and ResNet-50
(Lasso). Every experiment is repeated for five times and the results are shown in Fig. 5. Note that in
different runs of SplitLBI, the sparsity of Γt slightly varies.
Sparse subnets found by early stopping of SplitLBI achieve remarkably good accuracy after
retrain from scratch. In Fig.5 (a-b), sparse filters discovered by Γt at different epochs are compared
against the methods of Network Slimming (Liu et al., 2017), Soft Filter Pruning (Yang et al., 2018),
Scratch-B, and Scratch-E, whose results are reported from Liu et al. (2019). At similar sparsity
levels, SplitLBI can achieve comparable or even better accuracy than competitors, even with sparse
architecture learned from very early epochs (e.g. t = 20 or 10). Moreover in Fig.5 (c-d), we can
draw the same conclusion for the sparse weights of VGG-16 (Lasso) and ResNet-50 (Lasso), against
the results reported in Liu et al. (2019). These results shows that the structural sparsity parameter Γt
found by early stopping of SplitLBI already discloses important subnetwork architecture that may
achieve remarkably good accuracy after retrain from scratch. Therefore, it is not necessary to fully
train a dense model to find a successful sparse subnet architecture with comparable performance
to the dense ones -- one can early stop SplitLBI properly where the structural parameter Γt unveils
“winning tickets” (Frankle & Carbin, 2019).
5	Conclusion
In this paper, a parsimonious deep learning method is proposed based on differential inclusions of
inverse scale spaces. Implemented by a variable splitting scheme, such a dynamics system can exploit
over-parameterized models and structural sparsity simultaneously. Besides, its simple discretization,
i.e., the SplitLBI, has a proven global convergence and hence can be employed to train deep networks.
We have experimentally shown that it can achieve the state-of-the-art performance on many datasets
including ImageNet-2012, with better interpretability than SGD. What’s more, equipped with early
stopping, such a structural sparsity can unveil the ”winning tickets” - the architecture of sub-networks
which after re-training can achieve comparable and even better accuracy than original dense networks.
10
Under review as a conference paper at ICLR 2020
References
Reza Abbasi-Asl and Bin Yu. Structural compression of convolutional neural networks based on
greedy filter pruning. arXiv preprint arXiv:1705.07356, 2017. 1, 4.2
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. 2018. arXiv:1811.03962. 1
Jose M Alvarez and Mathieu Salzmann. Learning the number of neurons in deep networks. In NIPS,
2016. 1
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018. 1
Hedy Attouch, Jerome Bolte, and Benar Fux Svaiter. Convergence of descent methods for Semi-
algebraic and tame problems: proximal algorithms, forward-backward splitting, and regularized
Gauss-Seidel methods. Mathematical Programming,137:91-129, 2013. 1, A.1, A.3
Peter Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. In The 31st Conference on Neural Information Processing Systems (NIPS), Long Beach,
CA, USA. 2017. 1
Peter L. Bartlett. For valid generalization the size of the weights is more important than the size of
the network. In M. C. Mozer, M. I. Jordan, and T. Petsche (eds.), Advances in Neural Information
Processing Systems 9, pp. 134-140. MIT Press, 1997. 1
Martin Benning, MartaM. Betcke, Matthias J. Ehrhardt, and Carola-Bibiane SchonlieB. Choose your
path wisely: gradient descent in a bregman distance framework. arXiv preprint arXiv:1712.04045,
2017. A.3
Jacek Bochnak, Michel Coste, and Marie-Francoise Roy. Real algebraic geometry, volume 3. Ergeb.
Math. Grenzgeb. Springer-Verlag, Berlin, 1998. 3, A.1
Jerome Bolte, Aris Daniilidis, and Adrian Lewis. The LOjaSieWiCz inequality for nonsmooth subana-
lytic functions with applications to subgradient dynamical systems. SIAM Journal on Optimization,
17:1205-1223, 2007a. A.1, A.1
JerOme Bolte, Aris Daniilidis, Adrian Lewis, and Masahiro Shiota. Clark subgradients of Stratifiable
functions. SIAM Journal on Optimization, 18:556-572, 2007b. A.1, A.1, A.2, A.2
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In COMPSTAT, 2010. 1
Martin Burger, Guy Gilboa, Stanley Osher, and Jinjun Xu. Nonlinear inverse scale space methods.
Communications in Mathematical Sciences, 4(1):179-212, 2006. 2
Jian-Feng Cai, Stanley Osher, and Zuowei Shen. Convergence of the linearized bregman iteration for
l1-norm minimization. Mathematics of Computation, 2009. 1
Maxwell Collins and Pushmeet Kohli. Memory bounded deep convolutional networks. In arXiv
preprint arXiv:1412.1442, 2014, 2014. 1
M.	Coste. An introduction to o-minimal geometry. RAAG Notes, 81 pages, Institut de Recherche
Mathematiques de Rennes, 1999. A.2
David L. Donoho and Xiaoming Huo. Uncertainty principles and ideal atomic decomposition. IEEE
Transactions on Information Theory, 47(7):2845-2862, 2001. 1
Simon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. 2018. arXiv:1811.03804. 1
Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer
features of a deep network. University of Montreal, Technical Report, 1341, 2009. 1
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. International Conference on Learning Representations (ICLR), 2019. arXiv preprint
arXiv:1803.03635. 1, 4.4, 4.4, D
11
Under review as a conference paper at ICLR 2020
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. The lottery ticket
hypothesis at scale. arXiv preprint arXiv:1903.01611, 2019. D
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and
Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves
accuracy and robustness. International Conference on Learning Representations (ICLR), 2019.
arXiv preprint arXiv:1811.12231. 4.2
N.	Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks.
Conference on Learning Theory (COLT), 2018. arXiv preprint arXiv:1712.06541. 1
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. In NIPS, 2015. 1
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In ICCV, 2015. 4
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016. 4
Chendi Huang and Yuan Yao. A unified dynamic approach to sparse model selection. In The 21st
International Conference on Artificial Intelligence and Statistics (AISTATS), Lanzarote, Spain,
2018. 1,2, 3
Chendi Huang, Xinwei Sun, Jiechao Xiong, and Yuan Yao. Split lbi: An iterative regularization path
with structural sparsity. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett
(eds.), Advances in Neural Information Processing Systems (NIPS) 29, pp. 3369-3377. 2016. 1, 2
Chendi Huang, Xinwei Sun, Jiechao Xiong, and Yuan Yao. Boosting with structural sparsity: A
differential inclusion approach. Applied and Computational Harmonic Analysis, 2018. arXiv
preprint arXiv:1704.04833. 2
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks
with low rank expansions. In BMVC, 2014. 1
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 1
Steven Krantz and Harold R. Parks. A primer of real analyticfunctions. Birkhauser, second edition,
2002. 2, A.2
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012. 4
Krzysztof Kurdyka. On gradients of functions definable in o-minimal structures. Annales de l’institut
Fourier, 48:769-783, 1998. A.1, A.1, A.2, A.2
Yann LeCun et al. Lenet-5, convolutional neural networks. URL: http://yann. lecun. com/exdb/lenet,
20, 2015. 4
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for
efficient convnets. In ICLR, 2017. 1
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning
efficient convolutional networks through network slimming. In ICCV, 2017. 4.4
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of
network pruning. In ICLR, 2019. 1, 4.4, 5, 4.4
StaniSlaW Eojasiewicz. Une propriete topologique des sous-ensembles analytiques reels. In: LeS
EqUatiOns aux derivees partielles. EditiOnS du centre National de la Recherche Scientifique, Paris,
pp. 87-89, 1963. 1, 3, A.1
12
Under review as a conference paper at ICLR 2020
Stanislaw Eojasiewicz. Ensemblessemi-analytiques. InstitUtdes HaUtes Etudes Scientifiques, 1965.
A.1
Stanislaw Eojasiewicz. Sur la geometrie semi-et sous-analytique. Annales de l’institut Fourier, 43:
1575-1595,1993. A.1
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. International Conference
on Learning Representations (ICLR), 2019. arXiv preprint arXiv:1711.05101. 1
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of
two-layers neural network. Proceedings of the National Academy of Sciences (PNAS), 2018. 1
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural
networks: dimension-free bounds and kernel limit. Conference on Learning Theory (COLT), 2019.
1
Boris S. Mordukhovich. Variational analysis and generalized differentiation I: Basic Theory. Springer,
2006. A.1
Arkadi Nemirovski and David Yudin. Problem complexity and Method Efficiency in Optimization.
New York: Wiley, 1983. Nauka Publishers, Moscow (in Russian), 1978. 2
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The role
of over-parametrization in generalization of neural networks. In International Conference on
Learning Representations (ICLR), New Orleans, Louisiana, USA. 2019. 1
Stanley Osher, Martin Burger, Donald Goldfarb, Jinjun Xu, and Wotao Yin. An iterative regularization
method for total variation-based image restoration. Multiscale Modeling & Simulation, 4(2):460-
489, 2005. 1
Stanley Osher, Feng Ruan, Jiechao Xiong, Yuan Yao, and Wotao Yin. Sparse recovery via differential
inclusions. Applied and Computational Harmonic Analysis, 2016. 1, 2
R. Tyrrell Rockafellar and Roger J-B Wets. Variational analysis. Grundlehren Math. Wiss. 317,
Springer-Verlag, New York, 1998. A.1
Jianing V Shi, Wotao Yin, and Stanley J Osher. Linearized bregman for '1-regularized logistic
regression. In Proceedings of the 30th International Conference on Machine Learning (ICML),
2013. 1
Masahiro Shiota. Geometry of subanalytic and semialgebraic sets, volume 150 of Progress in
Mathematics. Birkhauser, Boston, 1997. A.1
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014. 4
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for
simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014. 2, 4.2
Xinwei Sun, Lingjing Hu, Yuan Yao, and Yizhou Wang. Gsplit lbi: Taming the procedural bias in
neuroimaging for disease prediction. In International Conference on Medical Image Computing
and Computer-Assisted Intervention (MICCAI), pp. 107-115. Springer, 2017. 1
Joel A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Trans. Inform.
Theory, 50(10):2231-2242, 2004. 1, 2
L. van den Dries. A generalization of the tarski-seidenberg theorem and some nondefinability results.
Bull. Amer. Math. Soc. (N.S.), 15:189-193, 1986. A.2
L. van den Dries and C. Miller. Geometric categories and o-minimal structures. Duke Mathematical
Journal, 84:497-540, 1996. A.2
Luca Venturi, Afonso S. Bandeira, and Joan Bruna. Spurious valleys in two-layer neural network
optimization landscapes. 2018. arXiv:1802.06384. 1
13
Under review as a conference paper at ICLR 2020
Yu Wang, Wotao Yin, and Jinshan Zeng. Global convergence of admm in nonconvex nonsmooth
optimization. Journal of Scientific Computing ,78(1):29-63, 2019. A.1
Yuting Wei, Fanny Yang, and Martin J. Wainwright. Early stopping for kernel boosting algorithms:
A general analysis with localized complexities. The 31st Conference on Neural Information
Processing Systems (NIPS), Long Beach, CA, USA, 2017. 1
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning the number of neurons in
deep networks. In NIPS, 2016. 1
Fanghui Xue and Jack Xin. Convergence of a relaxed variable splitting method for learning sparse
neural networks via `1, `0, and transformed-`1 penalties. arXiv:1812.05719v2, 2018. URL
http://arxiv.org/abs/1812.05719. 3
He Yang, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Y. Yang. Soft filter pruning for accelerating
deep convolutional neural networks. In IJCAI 2018, 2018. 1, 4.4
Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning.
Constructive Approximation, 26(2):289-315, 2007. 1
Wotao Yin, Stanley Osher, Jerome Darbon, and Donald Goldfarb. Bregman iterative algorithms for
compressed sensing and related problems. SIAM Journal on Imaging sciences, 1(1):143-168, 2008.
1
Jaehong Yoon and Sung Ju Hwang. Combined group and exclusive sparsity for deep neural networks.
In ICML, 2017. 1
Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49-67, 2006. 1, 2
Jinshan Zeng, Tim Tsz-Kit Lau, Shao-Bo Lin, and Yuan Yao. Global convergence of block coordinate
descent in deep learning. In Proceedings of the 36th International Conference on Machine Learning,
Long Beach, California, 2019. URL https://arxiv.org/abs/1803.00225. 3, A.2
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. International Conference on Learning Representations
(ICLR), 2017. arXiv:1611.03530. 1
Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun. Accelerating very deep convolutional
networks for classification and detection. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 38(10):1943-1955, 2016. 1
Bo Zhao, Xinwei Sun, Yanwei Fu, Yuan Yao, and Yizhou Wang. Msplit lbi: Realizing feature
selection and dense estimation simultaneously in few-shot and zero-shot learning. In International
Conference on Machine Learning (ICML), 2018. 1
Peng Zhao and Bin Yu. On model selection consistency of lasso. J. Machine Learning Research, 7:
2541-2567, 2006. 1, 2
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantization:
Towards lossless cnns with low-precision weights. ICLR, 2017. 1
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. ICLR,
2017. 1
14
Under review as a conference paper at ICLR 2020
APPENDIX TO SplitLBI for deep learning: structural sparsity via differential
inclusion paths
A Proof of Theorem 1
First of all, we reformulate Eq. (6) into an equivalent form. Without loss of generality, consider
Ω = Ωι in the sequel.
Denote R(P):= Ω(Γ), then Eq. (6) can be rewritten as,
Pk+1 = ProXκR(Pk + K(Pk - ɑ▽£(Pk))),	(15a)
Pk+1 = Pk - K-I(Pk+1 - Pk + KaVL(Pk )),	(15b)
where Pk = [0,gk]T ∈ ∂R(Pk) and gk ∈ ∂Ω(Γk). Thus SplitLBI is equivalent to the following
iterations,
Wk+1 = Wk - KaVW L(Wk , rk),	(16a)
Γk+1 = ProXκΩ(Γk + κ(gk - aVrL(Wk, Γ))),	(16b)
gk+1 = gk - K 1(rk+1 - rk + Ka •▽「L(Wk,1k)).	(16C)
Exploiting the equivalent reformulation (16a-16c), one can establish the global convergence of
(Wk, Γk,gk) based on the KUrdyka-LOjaSieWicz framework. In this section, the following extended
version of Theorem 1 is actually proved.
Theorem 2. [Global Convergence of SplitLBI] Suppose that Assumption 1 holds. Let (Wk, Γk, gk )
be the sequence generated by SplitLBI (Eq. (16a-16c)) with a finite initialization. If
0 < ak = a <
2
K(LiP + ν-1)
then (Wk, Γk, gk) converges to a critical point of F. Moreover, {(Wk, Γk)} converges to a stationary
k
point of L defined in Eq. 2, and {Wk} converges to a stationary point of Ln(W).
A. 1 Kurdyka-Łojasiewicz Property
To introduce the definition of the Kurdyka-Lojasiewicz (KL) property, we need some notions and
notations from variational analysis, which can be found in Rockafellar & Wets (1998).
The notion of subdifferential plays a central role in the following definitions. For each x ∈ dom(h) :=
{x ∈ Rp : h(x) < +∞}, the Frechet SubdiHerential of h at x, written ∂h(x), is the set of vectors
v ∈ Rp which satisfy
h(y) - h(x) - hv,y -xi
lim inf ----------------------------- ≥ 0.
y6=x,y→x	kx - yk
When X ∈ dom(h), we set ∂h(x) = 0. The Iimiting-Subdifferential (or simply Subdifferential) of h
introduced in Mordukhovich (2006), written ∂h(x) at x ∈ dom(h), is defined by
∂h(x) := {v ∈	Rp	:	∃xk	→ x,	h(xk)	→ h(x),	vk	∈	∂bh(xk) → v}.	(17)
A necessary (but not sufficient) condition for x ∈ Rp to be a minimizer of h is 0 ∈ ∂h(x). A point
that satisfies this inclusion is called limiting-critical or simply critical. The distance between a point
x to a subset S of Rp, written dist(x, S), is defined by dist(x, S) = inf{kx - sk : s ∈ S}, where
∣∣∙k represents the Euclidean norm.
Let h : Rp → R ∪ {+∞} be an extended-real-valued function (respectively, h : Rp ⇒ Rq be a
point-to-set mapping), its graph is defined by
Graph(h) := {(x, y) ∈ Rp × R : y = h(x)},
(resp. Graph(h) := {(x, y) ∈ Rp × Rq : y ∈ h(x)}),
15
Under review as a conference paper at ICLR 2020
and its domain by dom(h) := {x ∈ Rp : h(x) < +∞} (resp. dom(h) := {x ∈ Rp : h(x) = 0}).
When h is a proper function, i.e., when dom(h) = 0, the set of its global minimizers (possibly
empty) is denoted by
arg min h := {x ∈ Rp : h(x) = inf h}.
The KL property (LOjaSieWiCZ, 1963; 1993; KUrdyka,1998; Bolte et al., 2007a;b) plays a central role
in the convergence analysis of nonconvex algorithms (Attouch et al., 2013; Wang et al., 2019). The
following definition is adopted from Bolte et al. (2007b).
Definition 1. [Kurdyka-Lojasiewicz property] A function h is said to have the Kurdyka-Lojasiewicz
(KL) property at u ∈ dom(∂h) := {v ∈ Rn ∣∂h(v) = 0}, if there exists a constant η ∈ (0, ∞),
a neighborhood N of U and a function φ : [0, η) → R+, which is a concave function that is
continuous at 0 and satisfies φ(0) = 0, φ ∈ C1((0, η)), i.e., φ is continuous differentiable on (0, η),
and φ0(s) > 0 forall S ∈ (0, η), such that for all U ∈ N ∩{u ∈ Rn∣h(u) < h(u) < h(u) + η}, the
following inequality holds
φ0(h(u) — h(u)) ∙ dist(0, ∂h(u)) ≥ 1.	(18)
If h satisfies the KL property at each point of dom(∂h), h is called a KL function.
KL functions include real analytic functions, semialgebraic functions, tame functions defined in some
o-minimal structures (Kurdyka, 1998; Bolte et al., 2007b), continuous subanalytic functions (Bolte
et al., 2007a) and locally strongly convex functions. In the following, we provide some important
examples that satisfy the Kurdyka-Lojasiewicz property.
Definition 2. [Real analytic] A function h with domain an open set U ⊂ R and range the set of either
all real or complex numbers, is said to be real analytic atu if the function h may be represented by a
convergent power series on some interval of positive radius centered at u: h(x) = j∞=0 αj(x —u)j,
for some {αj } ⊂ R. The function is said to be real analytic on V ⊂ U ifit is real analytic at each
u ∈ V (Krantz & Parks, 2002, Definition 1.1.5). The real analytic function f over Rp for some
positive integer p > 1 can be defined similarly.
According to Krantz & Parks (2002), typical real analytic functions include polynomials, exponential
functions, and the logarithm, trigonometric and power functions on any open set of their domains.
One can verify whether a multivariable real function h(x) onRp is analytic by checking the analyticity
of g(t) := h(x + ty) for any x, y ∈ Rp.
Definition 3. [Semialgebraic]
(a)	A set D ⊂ Rp is called semialgebraic (Bochnak et al., 1998) if it can be represented as
st
D= [ \ {x∈Rp :Pij(x)=0,Qij(x) >0},
i=1 j=1
where Pij , Qij are real polynomial functions for 1 ≤ i ≤ S, 1 ≤ j ≤ t.
(b)	A function h : Rp → R ∪ {+∞} (resp. a point-to-set mapping h : Rp ⇒ Rq) is called
semialgebraic if its graph Graph(h) is semialgebraic.
According to (Lojasiewicz, 1965; Bochnak et al., 1998) and (Shiota, 1997, I.2.9, page 52), the class of
semialgebraic sets are stable under the operation of finite union, finite intersection, Cartesian product
or complementation. Some typical examples include polynomial functions, the indicator function of
a semialgebraic set, and the Euclidean norm (Bochnak et al., 1998, page 26).
A.2 KL Property in Deep Learning and Proof of Corollary 1
In the following, we consider the deep neural network training problem. Consider a l-layer feedfor-
ward neural network including l — 1 hidden layers of the neural network. Particularly, let di be the
number of hidden units in the i-th hidden layer for i = 1, . . . , l — 1. Let d0 and dl be the number of
units of input and output layers, respectively. Let Wi ∈ Rdi×di-1 be the weight matrix between the
(i — 1)-th layer and the i-th layer for any i = 1, . . . l2.
2To simplify notations, we regard the input and output layers as the 0-th and the l-th layers, respectively, and
absorb the bias of each layer into Wi.
16
Under review as a conference paper at ICLR 2020
According to Theorem 2, one major condition is to verify the introduced Lyapunov function F
defined in (9) satisfies the KUrdyka-LojasieWicz property. For this purpose, We need an extension
of semialgebraic set, called the o-minimal structure (see, for instance Coste (1999), van den Dries
(1986), Kurdyka (1998), Bolte et al. (2007b)). The folloWing definition is from Bolte et al. (2007b).
Definition 4. [o-minimal structure] An o-minimal structure on (R, +, ∙) is a SequenCe of boolean
algebras On of “definable” subsets of Rn, such that for each n ∈ N
(i)	if A belongs to On, then A × R and R × A belong to On+1;
(ii)	ifΠ : Rn+1 → Rn is the canonical projection onto Rn, then for any A in On+1, the set
Π(A) belongs to On;
(iii)	On contains the family of algebraic subsets of Rn, that is, every set of the form
{x ∈ Rn : p(x) = 0},
where p : Rn → R is a polynomial function.
(iv)	the elements of O1 are exactly finite unions of intervals and points.
Based on the definition of o-minimal structure, We can shoW the definition of the definable function.
Definition 5. [Definable function] Given an o-minimal structure O (over (R, +, ∙)), a function
f : Rn → R is said to be definable in O if its graph belongs to On+1.
According to van den Dries & Miller (1996); Bolte et al. (2007b), there are some important facts of
the o-minimal structure, shoWn as folloWs.
(i)	The collection of semialgebraic sets is an o-minimal structure. Recall the semialgebraic sets
are Bollean combinations of sets of the form
{x ∈ Rn : p(x) = 0, q1 (x) < 0, . . . , qm(x) < 0},
Where p and qi ’s are polynomial functions in Rn .
(ii)	There exists an o-minimal structure that contains the sets of the form
{(χ,t) ∈ [-1,1]n X R : f(χ)=t}
Where f is real-analytic around [-1, 1]n.
(iii)	There exists an o-minimal structure that contains simultaneously the graph of the exponential
function R 3 x 7→ exp(x) and all semialgebraic sets.
(iv)	The o-minimal structure is stable under the sum, composition, the inf-convolution and
several other classical operations of analysis.
The Kurdyka-LojasieWicz property for the smooth definable function and non-smooth definable
function Were established in (Kurdyka, 1998, Theorem 1) and (Bolte et al., 2007b, Theorem 11),
respectively. NoW We are ready to present the proof of Corollary 1.
Proof. [Proof of Corollary 1] To justify this corollary, We only need to verify the associated Lyapunov
function F satisfies Kurdyka-LojasieWicz inequality. In this case and by (10), F can be reWritten as
folloWs
F (W, Γ, G )= α (Ln(W, Γ) + 2V k W - Γ∣∣2) + Ω(Γ) + Ω*(g) - (W, g).
C	八 F,	FCKFK	,♦	,1介	∕TT7- T-∖∖	FCKF	∙ , ∙	C
Because ' and σ% S are definable by assumptions, then Ln(W, Γ) are definable as compositions of
definable functions. Moreover, according to Krantz & Parks (2002), kW - Γk2 and hW, gi are
semi-algebraic and thus definable. Since the group Lasso Ω(Γ) = Pg ∣∣Γk2 is the composition of
'2 and '1 norms, and the conjugate of group Lasso penalty is the maximum of group '2-norm, i.e.
Ω*(Γ) = max。∣Γg∣∣2, where the '2, '1, and '∞ norms are definable, hence the group Lasso and its
conjugate are definable as compositions of definable functions. Therefore, F is definable and hence
satisfies Kurdyka-LojasieWicz inequality by (Kurdyka, 1998, Theorem 1).
The verifications of other cases listed in assumptions can be found in the proof of (Zeng et al., 2019,
Proposition 1). This finishes the proof of this corollary.
□
17
Under review as a conference paper at ICLR 2020
A.3 Proof of Theorem 2
Our analysis is mainly motivated by a recent paper (Benning et al., 2017), as well as the influential
work (Attouch et al., 2013). According to (Attouch et al., 2013, Lemma 2.6), there are mainly four
ingredients in the analysis, that is, the sufficient descent property, relative error property, continuity
property of the generated sequence and the Kurdyka-Lojasiewicz property of the function. More
specifically, we first establish the sufficient descent property of the generated sequence via exploiting
the Lyapunov function F (see, (9)) in Lemma A.4 in Section A.4, and then show the relative error
property of the sequence in Lemma A.5 in Section A.5. The continuity property is guaranteed by the
continuity of C(W, Γ) and the relation limk→∞ BΩ (Γk+ι, Γ) = 0 established in Lemma l(i) in
Section A.4. Thus, together with the Kurdyka-Eojasiewicz assumption of F, We establish the global
convergence of SLBI following by (Attouch et al., 2013, Lemma 2.6).
Let (W, Γ, g) be a critical point of F, then the following holds
∂wF(W, Γ, g) = α(VLbn(W) + VT(W — Γ)) = 0,
∂γF(W, Γ, g) = αν-1(Γ — W) + ∂Ω(巧—g 3 0,	(19)
∂gF(W, Γ,g) = Γ - ∂Ω*(g) 3 0.
By the final inclusion and the convexity of Ω, it implies g ∈ ∂Ω(Γ). Plugging this inclusion into the
second inclusion yields αν-1 (Γg - Wg ) = 0. Together with the first equality imples
VLg(Wg ,Γg) =0,	VLbn(Wg ) = 0.
This finishes the proof of this theorem.
A.4 Sufficient Descent Property along Lyapunov Function
Let Pk := (Wk, Γk), and Qk := (Pk, gk-1), k ∈ N. In the following, we present the sufficient
descent property of Qk along the Lyapunov function F.
Lemma. Suppose that Ln is continuously differentiable and VLn is Lipschitz continuous with
a constant Lip > 0. Let {Qk} be a sequence generated by SLBI with a finite initialization. If
0 < α < K(Lip+ν-i)，then
F(Qk+1) ≤ F(Qk) - ρkQk+1 - Qk k22,
where P := 1 - a(Lip+VT).
Proof. By the optimality condition of (15a) and also the inclusion pk = [0, gk]T ∈ ∂R(Pk), there
holds
κ(αVLg(Pk) + pk+1 - pk) + Pk+1 - Pk = 0,
which implies
-hαVLg(Pk),Pk+1 -Pki = κ-1kPk+1 - Pkk22 + D(Γk+1, Γk)	(20)
where
D(Γk+1, Γk) := hgk+1 - gk, Γk+1 - Γki.
Noting that L(P) = Cn(W) + 2ν Il W 一 Γ∣2 and by the Lipschitz continuity of VLn(W) with a
constant Lip > 0 implies VLg is Lipschitz continuous with a constant Lip + ν-1. This implies
C(Pk+1) ≤ C(Pk) + hVC(Pk), Pk + 1 - Pki +----------kPk+1 - Pk k2.
Substituting the above inequality into (20) yields
αCg(Pk+1) + D(Γk+1, Γk) + ρIPk+1 -PkI22 ≤αCg(Pk).	(21)
Adding some terms in both sides of the above inequality and after some reformulations implies
αLg(Pk+l) + BΩ (rk+ι, rk)	(22)
≤ αC(Pk) + B"Γ, Γk-ι) - PIlPk+1 — Pkk2 一 (D(Γk+ι, Γ) + BWT (Γk,1一) - BΩ (Γk+ι, Γ))
=αC(Pk) + B"T(Ik, Γk-ι) - PIlPk+1 - Pkk2 - BΩk+1 (Γk,「一)- B号-'(Γk,1一),
18
Under review as a conference paper at ICLR 2020
where the final equality holds for D(Γk+ι, Γk) - BΩ (Γk+ι, Γ) = B^+1 (Γk, Γk-ι). That is,
F(Qk+ι) ≤ F(Qk) - pkPk+1	- Pkk2 -	BΩk+1 (Γk, Γk-ι)	- BΩk-1(Γk, Γk-ι)	(23)
≤ F(Qk) - ρkPk+1	-Pkk22,	(24)
where the final inequality holds for B普+1 (Γ, Γk-ι) ≥ 0 and Btgk-1 (Γk, Γk-ι) ≥ 0. Thus, We
finish the proof of this lemma.	□
Based on Lemma A.4, we directly obtain the following lemma.
Lemma 1. Suppose that assumptions of Lemma A.4 hold. Suppose further that Assumption 1 (b)-(d)
hold. Then
(i)	both α{L(Pk)} and {F(Qk)} converge to the Same finite value, and
limk→∞ BΩ (Γk+ι, Γk) = 0.
(ii)	the Sequence {(Wk, Γk, gk)} iS bounded,
(iii)	limk→∞ kPk+1 - Pkk22 = 0 and limk→∞ D(Γk+1, Γk) = 0,
(iv)	K Pf=O ∣∣Pk+ι - Pkk2 → 0 at a rate of O(1∕K).
Proof. By (21), L(Pk) is monotonically decreasing due to D(Γk+ι, Γk) ≥ 0. Similarly, by (24),
F(Qk) is also monotonically decreasing. By the lower boundedness assumption of Ln(W), both
L(P) and F(Q) are lower bounded by their definitions, i.e., (2) and (9), respectively. Therefore, both
{L(Pk)} and {F(Qk)} converge, and it is obvious that limk→∞ F(Qk) ≥ limk→∞ αL(Pk). By
(23),
BΩk-1 (Γk, Γk-1) ≤ F(Qk) - F(Qk+ι), k =1,....
By the convergence of F(Qk) and the nonegativeness of B"-1 (Γk, Γk-ι), there holds
lim BΩk-1(Γk,Γk-ι) =0.
k→∞
By the definition of F(Qk) = αL(Pk) + B詈-1 (Γk, Γk-ι) and the above equality, it yields
lim F(Qk) = lim ɑL(Pk).
k→∞	k→∞
〜	介 ∕TT7-∖ 1	1	FFl 1	, ,1	TTT- ∙ 1	FFc , F FC ♦ ,	f K / T T 厂 L∖	1 , F
Since Ln(W) has bounded level sets, then Wk is bounded. By the definition of L(W, Γ) and the
finiteness of L(Wk, Γk), Γk is also bounded due to Wk is bounded. The boundedness of gk is due to
gk ∈ ∂Ω(Γk), condition (d), and the boundedness of Γk.
By (24), summing up (24) over k = 0, 1, . . . , K yields
K
^X (PllPk+1 - Pkll2 + D(rk+1, rk)) < αL(PO) < ∞.	(25)
k=0
Letting K → ∞ and noting that both lPk+1 - Pkl2 and D(Γk+1, Γk) are nonnegative, thus
lim lPk+1 -Pkl2 =0,	lim D(Γk+1, Γk) =0.
k→∞	k→∞
Again by (25),
1K
K ɪ2 (PkPk+1 - Pk k + D(rk+1, rk)) < K αL(Pθ),
k=O
which implies KK PK=O ∣∣Pk+ι — Pk ∣∣2 → 0 at a rate of O(1∕K).
□
19
Under review as a conference paper at ICLR 2020
A.5 Relative Error Property
In this subsection, We provide the bound of subgradient by the discrepancy of two successive iterates.
By the definition of F (9),
/	^Vw C(Wk+ι,1-1)	ʌ
Hk+1 := I αvrC(wk+1, rk + 1)+ gk+1 - Qk I ∈ dF(Qk+1), k ∈ N∙	(26)
1k - 1k+1
Lemma. Under assumptions of Lemma 1, then
∣∣Hk+1∣∣≤ PIkQk+1 - Qkk, for Hk+1 ∈ ∂F(Qk+1), k ∈ N,
where ρ1 := 2κ-1 + 1 + α(Lip + 2ν-1). Moreover,去 PNIkHk ∣∣2 → 0 at a rate of O(1∕K).
Proof. Note that
VWC(Wk+1, Γk+1) = (VWC(Wk+1, Γk+1) -VwC(Wk+1, Γk))	(27)
+ (VwC(Wk+1, Γk) - VWC(Wk, Γk)) + VWC(Wk, Γk)∙
By the definition of C (see (2)),
∣VwC(Wk+1,Γk+1) - VwC(Wk+1, Γ)∣ = VTkrk - Γk+1∣,
∣VwC(Wk+1,Γk) -VwL(Wk, Γ)k = k(VJ√Wk+1) - VCn(Wk))+ VT(Wk+1 - Wk)∣
≤ (Lip + v-1)kWk+1 - Wkk,
where the last inequality holds for the Lipschitz continuity of VLn with a constant Lip > 0, and by
(16a),
∣VwL(Wk,Γk)k = (Ka)TkWk+1 - Wkk∙
Substituting the above (in)equalities into (27) yields
∣VwC(Wk+1, Γk+1)k ≤ [(Ka)-1 + Lip + V-1] ∙∣∣Wk+1- Wkk + VTkrk+1 - Γkk
Thus,
∣∣αvWC(Wk+1, “ + 1)11 ≤ [κ-1 + α(Lip + V-1)] ∙ k Wk+1 - Wk k + αv-Ikrk+ 1 - Γk k ∙ (28)
By (16c), it yields
Qk+1 - Qk = K 1(Γk - &+1 ) - aVrC(Wk , Γk )∙
Noting that VrL(Wk, Γk) = V-1(Γk - Wk), and after some simplifications yields
IlaVrC(Wk + 1, &+1 )+ Qk + 1 - Qk k = ∣∣(κ 1 - aV I) ∙ (Γk -1k+1) + aV I(Wk - Wk+1 )∣∣
≤ aV-IkWk - Wk + 1 k + (K-I- aV-1)||心-口+11∣,
(29)
where the last inequality holds for the triangle inequality and K-1 > aV-1 by the assumption.
By (28), (29), and the definition of Hk+1 (26), there holds
IlHk+111 ≤ [κ 1 + a(Lip + 2v 1)] ∙ k Wk+1 - Wk k +(k 1 + 1)||&+1 - Γk k
≤ [2κ 1 + 1 + a(Lip + 2v 1)] ∙ kPk+1 - Pk k	(30)
≤ [2κ 1 + 1 + a(Lip + 2v 1)] ∙∣∣Qk+1 - Qk k ∙
By (30) and Lemma 1(iv), N PNI ∣∣Hk ∣∣2 → 0 at a rate of O(1∕K).
This finishes the proof of this lemma.	□
20
Under review as a conference paper at ICLR 2020
Figure 6: Validation curves of dense models Wt for different κ and ν . For SLBI we find that the
model accuracy is robust to the hyperparameters both in terms of convergence rate and generalization
ability. Here validation accuracy means the accuracy on test set of Cifar10. The first one is the result
for VGG16 ablation study on κ, the second one is the result for ResNet56 ablation study on κ, the
third one is the result for VGG16 ablation study on ν and the forth one is the result for ResNet56
ablation study on ν.
Type	Model	K = 1	κ 二 2	κ 二 5	κ = 10	SGD
Full	Vgg16	93.46	93.27	92.77	92.03	93.57
	ResNet56	92.71	92.18	91.50	90.92	93.08
Sparse	Vgg16	93.31	93.00	92.36	76.25	-
	ResNet56	92.37	91.85	89.48	87.02	-
Table 2: This table shows results for different κ, the results are all the best test accuracy. Here we
test two widely-used models: VGG16 and ResNet56 on Cifar10. For results in this table, we keep
ν = 100. Full means that we use the trained model weights directly, Sparse means the model weights
are combined with mask generated by Γ support. Sparse result has no finetuning process, the result is
comparable to its Full counterpart. For this experiment, we propose that κ = 1 is a good choice. For
all the model, we train for 160 epochs with initial learning rate (lr) of 0. 1 and decrease by 0.1 at
epoch 80 and 120.
B S upplementary Experiments
B.1 Ablation Study of VGG16 and ResNet56 on Cifar10
To further study the influence of hyperparameters, we record performance of Wt for each epoch t
with different combinations of hyperparameters. The experiments is conducted 5 times each, we
show the mean in the table, the standard error can be found in the corresponding figure. We perform
experiments on Cifar10 and two commonly used network VGG16 and ResNet56.
On κ , we keep ν = 100 and try κ = 1, 2, 5, 10, the validation curves of models Wt are shown in
Fig. 6 and Table 2 summarizes the mean accuracies. Table 3 summarizes best validation accuracies
achieved at some epochs, together with their sparsity rates. These results show that larger kappa leads
to slightly lower validation accuracies, where the numerical results are shown in Table 2 . We can
find that κ = 1 achieves the best test accuracy.
On ν , we keep κ = 1 and try ν = 10, 20, 50, 100, 200, 500, 1000, 2000 the validation curve and mean
accuracies are show in Fig. 6 and Table 4. Table 5 summarizes best validation accuracies achieved at
some epochs, together with their sparsity rates. By carefully tuning ν we can achieve similar or even
better results compared to SGD. Different from κ, ν has less effect on the generalization performance.
By tuning it carefully, we can even get a sparse model with slightly better performance than SGD
trained model.
C Computational Cost of SplitLBI
We further compare the computational cost of different optimizers: SGD (Mom), SplitLBI (Mom)
and Adam (Naive). We test each optimizer on one GPU, and all the experiments are done on one
GTX2080. For computational cost, we judge them from two aspects : GPU memory usage and time
needed for one batch. The batch size here is 64, experiment is performed on VGG-16 as shown in
Table 6.
21
Under review as a conference paper at ICLR 2020
Model		Ep20		Ep40		Ep80		Ep160	
Vgg16	Term	Sparsity	ACC	Spasity	Acc	Spasity	Acc	Spasity	Acc
	K = 1	96.62	71.51	96.62	76.92	96.63	77.48	96.63	93.31
	K = 2	51.86	72.98	71.99	73.64	75.69	74.54	75.72	93.00
	κ = 5	8.19	10.00	17.64	34.25	29.76	69.92	30.03	92.36
	κ = 10	0.85	10.00	6.62	10.00	12.95	38.38	13.26	76.25
ResNet56	Term	SParSity	Acc	Spasity	Acc	Spasity	Acc	Spasity	Acc
	κ = 1	96.79	73.50	96.87	75.27	96.69	77.47	99.68	92.37
	κ = 2	76.21	72.85	81.41	74.72	84.17	75.64	84.30	91.85
	κ = 5	36.58	60.43	53.07	76.00	57.48	75.67	57.74	89.48
	κ = 10	3.12	10.20	29.43	53.36	41.18	74.56	41.14	87.02
Table 3: Sparsity rate and validation accuracy for different κ at different epochs. Here we pick the
test accuracy for specific epoch. In this experiment, we keep ν = 100. We pick epoch 20, 40, 80 and
160 to show the growth of sparsity and sparse model accuracy. Here Sparsity is defined in Sec. 4,
and Acc means the test accuracy for sparse model. A sparse model is a model at designated epoch t
combined with the mask as the support of Γt .
Type	Model	V =10	V = 20	V = 50	V = 100	V = 200	V = 500	V = 1000	V = 2000	SGD
Full	Vgg16	93.66	93.59	93.57	93.39	93.38	93.35	93.43	93.46	93.57
	ResNet56	93.12	92.68	92.78	92.45	92.95	93.11	93.16	93.31	93.08
Sparse	Vgg16	93.39	93.42	93.39	93.23	93.21	93.01	92.68	-10-	-
	ResNet56	92.81	92.19	92.40	92.10	92.68	92.81	92.84	88.96	-
Table 4: Results for different ν, the results are all the best test accuracy. Here we test two widely-used
model : VGG16 and ResNet56 on Cifar10. For results in this table, we keep κ = 1. Full means
that we use the trained model weights directly, Sparse means the model weights are combined with
mask generated by Γ support. Sparse result has no finetuning process, the result is comparable to its
Full counterpart. For all the model, we train for 160 epochs with initial learning rate (lr) of 0.1 and
decrease by 0.1 at epoch 80 and 120.
Model		Ep20		Ep40		Ep80		Ep160	
Vgg16	Term	Sparsity	Acc	Spasity	Acc	Spasity	Acc	Spasity	Acc
	V = 10	96.64	71.07	96.64	77.70	96.65	79.46	96.65	93.34
	V = 20	96.64	69.11	96.64	77.63	96.65	77.08	96.65	93.42
	v = 50	96.64	74.91	96.65	74.21	96.65	79.15	96.65	93.38
	v = 100	96.64	74.82	96.64	73.22	96.64	78.09	96.64	93.23
	v = 200	91.69	73.67	94.06	74.67	94.15	75.20	94.15	93.21
	v = 500	18.20	10.00	59.94	67.88	82.03	78.69	82.32	93.01
	v = 1000	6.43	10.00	17.88	10.00	49.75	61.31	51.21	92.68
	v = 2000	0.22	10.00	6.89	10.00	18.15	10.00	19.00	10.00
ResNet56	Term	Sparsity	Acc	Spasity	Acc	Spasity	Acc	Spasity	Acc
	V = 10	99.97	73.37	99.95	71.64	99.74	76.46	99.74	92.81
	V = 20	99.97	72.58	99.84	74.16	99.69	72.37	99.72	92.19
	V = 50	99.96	70.72	99.89	73.96	99.79	74.93	99.77	92.40
	V = 100	96.31	73.63	96.63	75.79	96.55	72.94	96.57	92.10
	v = 200	91.98	75.30	94.38	72.13	94.87	73.75	94.88	92.68
	v = 500	74.44	65.58	90.00	74.12	92.96	71.91	92.99	92.81
	v = 1000	24.32	10.85	75.68	70.23	88.56	79.67	88.80	92.48
	v = 2000	0.65	10.00	26.66	13.30	74.98	70.38	75.92	88.95
Table 5: Sparsity rate and validation accuracy for different ν at different epochs. Here we pick the
test accuracy for specific epoch. In this experiment, we keep κ = 1. We pick epoch 20, 40, 80 and
160 to show the growth of sparsity and sparse model accuracy. Here Sparsity is defined in Sec. 4 as
the percentage of nonzero parameters, and Acc means the test accuracy for sparse model. A sparse
model is a model at designated epoch t combined with mask as the support of Γt .
22
Under review as a conference paper at ICLR 2020
optimizer	SGD	SLBI	Adam
Mean Batch Time	0.0197	0.0221	0.0210
GPU Memory	1161MB	1459MB	1267MB
Table 6: Computational and Memory Costs. ( And GPU memory means the whole memory model
weights and the activation cache. )
Layer	FC1	FC2	FC3
Sparsity	0049	0.087	0.398
Number of Weights	235200	30000	1000
Table 7: This table shows the sparsity for every layer of Lenet-3. Here sparsity is defined in Sec. 4,
number of weights denotes the total number of parameters in the designated layer. It is interesting
that the Γ tends to put lower sparsity on layer with more parameters.
D	Fine-tuning of sparse subnetworks
We design the experiment on MNIST, inspired by Frankle & Carbin (2019). Here, we explore the
subnet obtained by ΓT after T = 100 epochs of training. As in Frankle et al. (2019), we adopt
the “rewind” trick: re-loading the subnet mask of Γ100 at different epochs, followed by fine-tuning.
In particular, along the training paths, we reload the subnet models at Epoch 0, Epoch 30, 60, 90,
and 100, and further fine-tune these models by SplitLBI (Mom-Wd). All the models use the same
initialization and hence the subnet model at Epoch 0 gives the retraining with the same random
initialization as proposed to find winning tickets of lottery in Frankle & Carbin (2019). We will
denote the rewinded fine-tuned model at epoch 0 as (Lottery), and those at epoch 30, 60, 90, and
100, as F-epoch30, F-epoch60, F-epoch90, and F-epoch100, respectively. Three networks are studied
here - LeNet-3, Conv-2, and Conv-4. LeNet-3 removes one convolutional layer of LeNet-5; and it is
thus less over-parameterized than the other two networks. Conv-2 and Conv-4, as the scaled-down
variants of VGG family as done in Frankle & Carbin (2019), have two and four fully-connected
layers, respectively, followed by max-pooling after every two convolutional layer.
The whole sparsity for Lenet-3 is 0.055, Conv-2 is 0.0185, and Conv-4 is 0.1378. Detailed sparsity for
every layer of the model is shown in Table 7, 8, 9. We find that fc-layers are sparser than conv-layers.
We compare SplitLBI variants to the SGD (Mom-Wd) and SGD (Lottery) (Frankle & Carbin, 2019)
in the same structural sparsity and the results are shown in Fig. 7. In this exploratory experiment, one
can see that for overparameterized networks - Conv-2 and Conv-4, fine-tuned rewinding subnets -
F-epoch30, F-epoch60, F-epoch90, and F-epoch100, can produce better results than the full models;
while for the less over-parameterized model LeNet-3, fine-tuned subnets may achieve less yet
still comparable performance to the dense models and remarkably better than the retrained sparse
subnets from beginning (i.e. SplitLBI/SGD (Lottery)). These phenomena suggest that the subnet
architecture disclosed by structural sparsity parameter ΓT is valuable, for fine-tuning sparse models
with comparable or even better performance than the dense models of WT .
E Retraining of sparse subnets found by S plitLB I (Lottery)
Here we provide more details on the experiments in Fig. 5. Table 10 gives the details on hyper-
parameter setting. Moreover, Figure 8 provides the sparsity variations during SplitLBI training in Fig.
5.
Layer	Conv1	Conv2	FC1	FC2	FC3
Sparsity	0.9375	-1-	00067	00284	01551
Number of Weights	576	36864	3211264	65536	2560
Table 8: This table shows the sparsity for every layer of Conv-2. Here sparsity is defined in Sec. 4,
number of weights denotes the total number of parameters in the designated layer. The sparsity is
more significant in fully connected (FC) layers than convolutional layers.
23
Under review as a conference paper at ICLR 2020
Layer	Conv1	Conv2	Conv3	Conv4	FC1	FC2	FC3
Sparsity	0.921875	-1-	-1-	1	0.0040	0.0094	01004
Number of Weights	576	36864	73728	147456	1605632	65536	2560
Table 9: This table shows the sparsity for every layer of Conv-4. Here sparsity is defined in Sec.
4, number of weights denotes the total number of parameters in the designated layer. Most of the
convolutional layers are kept while the FC layers are very sparse.
XdeJnusV
Figure 7: Fine-tuning of sparse subnets learned by SplitLBI may achieve comparable or better
performance than dense models. F-epochk indicates the fine-tuned model comes from the Epoch
k. SplitLBI (Lottery) and SGD (Lottery) use the same sparsity rate for each layer and the same
initialization for retrain.
Network	Penalty	Optimizer	α	V	K	λ	Momentum	Nesterov
VGG-16	Group Lasso	-SLBi	~0T~	∏G0-	τ^	0.1	0.9	Yes
ResNet-56	Group Lasso	SLBI	~~0T~	∏G0-	τ^	0.05	0.9	Yes
VGG-16(Lasso)	Lasso	-SLBI	~0T~	-3q0-	τ^	0.05	0.9	Yes
ResNet-50(Lasso)	Lasso	SLBI	~0Γ^	~^00~	τ^	0.03	0.9	Yes
Table 10: Hyperparameter setting for the experiments in Figure 5.
U
ɑɪ
SparaityFWhofVGG-W
« S0 RHOielse 2∞
⅛**Λ
(a) VGG-16
⅛ara*y fwħ b ⅛βNw-w
，＞
(b) ResNet-56
SWiraity FWh OfVGG-WUeso)
0 ZSSOk "0 IelSOkSMO
(c) VGG-16 (Lasso)
(d) ResNet-50 (Lasso)


Figure 8: Sparsity changing during training process of SplitLBI (Lottery) for VGG and ResNets
(corresponding to Fig. 5). We calculate the sparsity in every epoch and repeat five times. The black
curve represents the mean of the sparsity and shaded area shows the standard deviation of sparsity.
The vertical blue line shows the epochs that We choose to early stop. We choose the log-scale epochs
for achieve larger range of sparsity.
24