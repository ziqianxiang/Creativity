Under review as a conference paper at ICLR 2020
Boosting Generative Models by Leveraging
Cascaded Meta-Models
Anonymous authors
Paper under double-blind review
Ab stract
A deep generative model is a powerful method of learning a data distribution,
which has achieved tremendous success in numerous scenarios. However, it is
nontrivial for a single generative model to faithfully capture the distributions of
the complex data such as images with complicate structures. In this paper, we
propose a novel approach of cascaded boosting for boosting generative mod-
els, where meta-models (i.e., weak learners) are cascaded together to produce a
stronger model. Any hidden variable meta-model can be leveraged as long as
it can support the likelihood evaluation. We derive a decomposable variational
lower bound of the boosted model, which allows each meta-model to be trained
separately and greedily. We can further improve the learning power of the genera-
tive models by combing our cascaded boosting framework with the multiplicative
boosting framework.
1	Introduction
The past decade has witnessed tremendous success in the field of deep generative models (DGMs)
in both unsupervised learning (Goodfellow et al., 2014; Kingma & Welling, 2013; Radford et al.,
2015) and semi-supervised learning (Abbasnejad et al., 2017; Kingma et al., 2014; Li et al., 2018)
paradigms. DGMs learn the data distribution by combining the scalability of deep learning with the
generality of probabilistic reasoning. However, it is not easy for a single parametric model to learn
a complex distribution, since the upper limit of a model’s ability is determined by its fixed structure.
If a model with low capacity was adopted, the model would be likely to have a poor performance.
Straightforwardly increasing the model capacity (e.g., including more layers or more neurons) is
likely to cause serious challenges, such as vanishing gradient problem (Hochreiter et al., 2001) and
exploding gradient problem (Grosse, 2017).
An alternative approach is to integrate multiple weak models to achieve a strong one. The early
success was made on mixture models (Dempster et al., 1977; Figueiredo & Jain, 2002; Xu & Jor-
dan, 1996) and product-of-experts (Hinton, 1999; 2002). However, the weak models in such work
are typically shallow models with very limited capacity. Recent success has been made on boosting
generative models, where a set of meta-models (i.e., weak learners) are combined to construct a
stronger model. In particular, Grover & Ermon (2018) propose a method of multiplicative boosting,
which takes the geometric average of the meta-model distributions, with each assigned an expo-
nentiated weight. This boosting method improves performance on density estimation and sample
generation, compared to a single meta-model. However, the boosted model has an explicit parti-
tion function, which requires importance sampling (Rubinstein & Kroese, 2016) for an estimation.
In general, sampling from the boosted model is conducted based on Markov chain Monte Carlo
(MCMC) method (Hastings, 1970). As a result, it requires a high time complexity of likelihood
evaluation and sample generation.
Rosset & Segal (2003) propose another method of additive boosting, which takes the weighted
arithmetic mean of meta-models’ distributions. This method can sample fast, but the improvement
of performance on density estimation is not comparable to the multiplicative boosting, since addi-
tive boosting requires that the expected log-likelihood and likelihood of the current meta-model are
better-or-equal than those of the previous boosted model (Grover & Ermon, 2018), which is difficult
to satisfy. In summary, it is nontrivial for both of the previous boosting methods to balance well be-
tween improving the learning power and keeping the efficiency of sampling and density estimation.
1
Under review as a conference paper at ICLR 2020
To address the aforementioned issues, we propose a novel boosting framework, called cascaded
boosting, where meta-models are connected in cascade. The framework is inspired by the greedy
layer-wise training algorithm of DBNs (Deep Belief Networks) (Bengio et al., 2007; Hinton et al.,
2006), where an ensemble of RBMs (Restricted Boltzmann Machines) (Smolensky, 1986) are con-
verted to a stronger model. We propose a decomposable variational lower bound, which reveals the
principle behind the greedy layer-wise training algorithm. The decomposition allows us to incor-
porate any hidden variable meta-model, as long as it supports likelihood evaluation, and train these
meta-models separately and greedily, yielding a deep boosted model. Finally, We demonstrate that
our boosting framework can be integrated with the multiplicative boosting framework (Grover &
Ermon, 2018), yielding a hybrid boosting with an improved learning power of generative models.
To summary, we make the following contributions:
•	We propose a boosting framework to boost generative models, where meta-models are
cascaded together to produce a stronger model.
•	We give a decomposable variational lower bound of the boosted model, which reveals the
principle behind the greedy layer-wise training algorithm.
•	We finally demonstrate that our boosting framework can be extended to a hybrid model by
integrating it with the multiplicative boosting models, which further improves the learning
power of generative models.
2	Approach
In subsection 2.1, we review the current multiplicative boosting (Grover & Ermon, 2018). Then, we
present our cascaded boosting. We first figure out how to connect meta-models, and then propose
our boosting framework with its theoretical analysis. Afterwards, we discuss the convergence of our
cascaded boosting.
2.1	Multiplicative Boosting
Grover & Ermon (2018) introduced multiplicative boosting, which takes the geometric average of
meta-models’ distributions, with each assigned an exponentiated weight αi as
Pn(X) = ∏n=0Mα(χ)
Zn
(1)
where Mi(x) (0 ≤ i ≤ n) are distributions of meta-models, which are required to support likelihood
evaluation, Pn(x) is the distribution of the boosted model and Zn is the partition function. The first
meta-model M0 is trained on the empirical data distribution pD which is defined to be uniform over
the dataset D. The other meta-models Mi (1 ≤ i ≤ n) are trained on a reweighted data distribution
pDi as
max EpDi [logMi (x)] ,
(2)
where PDi α (PpD^)" with βi ∈ [0,1] being the hypermeter.
Grover & Ermon (2018) show that the expected log-likelihood of the boosted model Pi over the
dataset D will not decrease (i.e., EpD [logPi (x)] ≥ EpD [logPi-1 (x)]) if Equation 2 is maximized.
The multiplicative boosting succeeds in improving the learning power of generative models. Com-
pared to an individual meta-model, it has better performance on density estimation and generates
samples of higher quality. However, importance sampling and MCMC are required to evaluate the
partition function Zn and generate samples respectively, which limits its application in occasions
requiring fast density estimation and sampling. To overcome these shortcomings, we propose our
cascaded boosting framework.
2.2	How to Connect Meta-Models
In multiplicative boosting, distributions of meta-models are connected by multiplication, leading to
the troublesome partition function. To overcome this problem, we connect meta-models in cascade.
Suppose we have n meta-models {mi (xi, hi)}in=1, where xi is the visible variable, hi is the hidden
2
Under review as a conference paper at ICLR 2020
variable and mi(xi, hi) is their joint distribution. These meta-models can belong to different families
(e.g. RBMs and VAEs (Variational Autoencoders) (Kingma & Welling, 2013)), as long as they have
hidden variables and support likelihood evaluation. We replace xi with hi-1 and connect meta-
models in a top-down style to construct a boosted model as
k-1
Pk (x, hl,…，hk):= mk(hk-i, hk) ɪɪ mi(hi-i |hi),	(3)
i=1
where h0 = x and 1 ≤ k ≤ n. This formulation avoids the troublesome partition function and we
can sample from the boosted model in a top-down style using simple ancestral sampling.
The boosted model allows us to generate samples hereby. Then, we build the approximation of the
posterior distribution, which allows us to do inference. We connect meta-models in a bottom-up
style to construct the approximation of the posterior distribution as
k
qk(hι,…，hk |x) := ɪɪmi(hilhi-i),	(4)
i=1
where ho = X and 1 ≤ k ≤ n. Since qj(hi,…,hj|x) is exactly the marginal distribution
of qk(hι, ∙∙∙ , hk|x) when j < k, We can omit the subscript, thereby writing qk as q. The
approximation of the posterior distribution makes an assumption of conditional independence:
hi ⊥ hi,…,hi-2∣hi-ι (3 ≤ i ≤ k), thereby leading to the advantage that we don't need to
re-infer the whole boosted model after incorporating a new meta-model mk: we only need to infer
hk from mk(hk|hk-i) conditioned on hk-i inferred previously.
2.3	Decomposable Variational Lower B ound
Supposing D is the training set, we give a decomposable variational lower bound Lk to the marginal
likelihood ED [logpk(x)] of the boosted modelpk, as illustrated in Theorem 1.
Theorem 1. Let {mi(hi-ι, hi)}rn=ι be n meta-models, Pk(x, hi,…，hk) be the boosted model
constructed from {mi}k=ι, and q(hi,…,hk-ι∣x) be the approximate posterior constructed from
{mi }ik=-ii , then we have:
ED [logPk (X)] ≥ Lk (m1,…，mk ) =ED Eq(hi,…,hk-ι∣x)
k
Eli(mi,…，mi),
i=i
bgPk(x, hi,…，hk-i)'
.	q(hi,…，hk-i∣x)
(5)
where li(mi) = ED [logmi(X)] and
li (mi,…，mi) =ED Eq(hi,…,hi-ι∣x) [logmi(hi-i)]
— EDEq(hi,…,hi-i∣χ) [logmi-i(hi-i)](2 ≤ i ≤ k).
(6)
Proof: see Appendix A.
The lower bound	Lk	is decomposed to k terms	li	(1	≤ i ≤	k),	where	Lk	is only related to
mi,…,mk and li is only related to mi,…,m》Specifically, li is the marginal likelihood of
the first meta-model mi. li (2 ≤ i ≤ k) is the difference between the marginal likelihood of the
observable variable of mi and the hidden variable of mi-i.
When k = 1, there is only one meta-model and the lower bound is exactly equal to the marginal
likelihood of the boosted model. So the lower bound is tight when k = 1. Based on the initially
tight lower bound, we can further promote it by optimizing these decomposed terms sequentially,
yielding the greedy layer-wise training algorithm, as discussed in subsection 2.4.
2.4 The Greedy Layer-Wise Training Algorithm
The difference between Lk(mi,…，mk) and Lk-i(mi,…，mk-i) is
lk(mi,…，mk) =EDEq(hι,…，hk-ι∣χ) [logmk(hk-i)]
- ED Eq(hι,…，hk-ι∣χ) [logmk-i(hk-i)].
(7)
3
Under review as a conference paper at ICLR 2020
Algorithm 1 Cascaded Boosting
1:	Input: dataset D; meta-models {mi}in=1
2:	Let pD = Px(i)∈D δ(x - x(i))/|D| be the empirical distribution of D
3:	Train m1 by maximizing ED [log m1 (x)]
4:	k = 2
5:	while k ≤ n do
6:	Fix {mi}：=-： and let q(hι, ∙∙∙ , hk-ι∣x) = QQik=1 mi(hi∣hi-ι)
7:	Train m： by maximizing EDEq(hι,…”…㈤[logm：(h：-i)]
8:	k = k + 1
9:	end while
10:	LetPn(x,hι,…，hn) = mn(hn-1, hn) QQ mi(hi-1 |hi)
i=1
11:	returnpn(x,hι,…，hn)
To ensure the lower bound grows with mk incorporated, we only need to ensure lk is positive. When
We train the meta-model m：, We first fix rest meta-models {mi}：=-1, so that q(hι, ∙∙∙ , hk-ι∣χ)
is fixed and EDEq(hι,…，hk-ι∣χ) [logmk-1(hk-1)] is constant, and then train m： by maximizing
E°Eq(hι,…,hk-ι ∣x) [logm： (h：-i)]. AS aresult, we can train each meta-model separately and greed-
ily, as outlined in Alg. 1.
When {mi}in=2 are arbitrarily poWerful learners, We can derive the non-decreasing property of the
decomposable loWer bound, as given in Theorem 2.
Theorem 2.	When {mi}in=2 are arbitrarily powerful learners (i.e., mi is able to model any distri-
bution), we have Li ≤ L? ≤ … ≤ Ln during the greedy layer-wise training.
Proof. During the kth (2 ≤ k ≤ n) round of Alg. 1, {mi}：-1 are fixed, so q(hi,…，h：-i|x) is
fixed and EDEq(hι,… m一⑻[102恒：一1(九：一1)] is constant. After training m：, we have
ED Eq(hi,…，hk-ι |x) [logm： (h：-1)] = max ED Eq(hi,…，hk-ι∣x) [logP(h：-1)]
p	(8)
≥ EDEq(hi,…，hk-ι∣x) [logm：-1(h：-1)].
Thus, 1：(mi,…，m：) ≥ 0 and Lk(mi,…，m：) ≥ L：-i(mi,…，m：-i).	□
In practice, 1： (mi,…，m：) is likely to be negative under the following three cases:
•	m： is not well trained. In this case, 1：(mi,…，m：) is very negative, which indicates us
to tune hyperparameters of m： and retrain this meta-model.
•	m：-i(h：-i) is close to the marginal distribution of PD (x)q(hi,…，h：-i|x). In this case,
1：(mi, ∙∙∙ , m：) will be close to zero, and we can either keep training by incorporating
more powerful meta-models or just stop training.
•	The lower bound converges. In this case, the lower bound will stop growing even if more
meta-models are incorporated, which will be further discussed in subsection 2.5
For models with m：(h：-i) initialized from m：-i(h：-i), such as DBNs (Hinton et al., 2006),
1：(mi, ∙∙∙ ,m：)=0 after initializing m：. In this case, we can make sure that 1：(mi, ∙∙∙ ,m：) ≥ 0
after training m： .
2.5 Convergence
It’s impossible for the decomposable lower bound to grow infinitely. After training m： , if
E°Eq(hι,…，hk-ι∣x) [logm：(h：-i)] is maximized, then the lower bound will stop growing even if
we keep incorporating more meta-models. We call this phenomenon convergence of the boosted
model, which is formally described in Theorem 3.
Theorem 3.	If EDEq(hi,… ,hk-1 |x) [logm：(h：-i)] is maximized after training m：, then ∀j ∈
[k + 1,n] ∩ Z, Vm：+i,m：+2,…，mj, Lj(mi,…，mj) ≤ Lk(mi,…，m：).
Proof: see Appendix B.
4
Under review as a conference paper at ICLR 2020
It indicates that it’s unnecessary to incorporate meta-models as much as possible. To help judge
whether the boosted model has converged, a necessary condition is given in Theorem 4.
Theorem 4.	If EDEq(hι,…，hk-ι∣x) [logmk(hk-ι)] is maximized after training mk, then
ck(m1,…，mk ) := IED Eq(hi,…,hk |x) [logmk(hk )] - Emk(hk) [logmk (hk )] | =0.
Proof: see Appendix B.
We can use ck to help us judge whether the boosted model has converged after training mk. For
meta-models such as VAEs, mk(hk) is the standard normal distribution and Emk (hk) [logmk (hk)]
is analytically solvable, leading to a simple estimation of ck.
3	Hybrid B oosting
We can further consider a hybrid boosting by integrating our cascaded boosting with the multiplica-
tive boosting. It is not difficult to implement: we can think of the boosted model produced by our
method as the meta-model for multiplicative boosting. An open problem for hybrid boosting is to
determine what kind of meta-models to use and how meta-models are connected, which is closely
related to the specific dataset and task. Here we introduce some strategies for this problem.
For cascaded connection, if the dataset can be divided to several categories, it is appropriate to use
a GMM (Gaussian Mixture Model) (Smolensky, 1986) as the top-most meta-model. Other meta-
models can be selected as VAEs (Kingma & Welling, 2013) or their variants (Burda et al., 2015;
S0nderby et al., 2016). There are three reasons for this strategy: (1) the posterior of VAE is much
simpler than the dataset distribution, making a GMM enough to learn the posterior; (2) the posterior
ofaVAE is likely to consist of several components, with each corresponding to one category, making
a GMM which also consists of several components suitable; (3) Since mk-1(hk-1) is a standard
Gaussian distribution when mk-1 is a VAE, when mk(hk-1) is a GMM, which covers the standard
Gaussian distribution as a special case, we can make sure that Equation 7 will not be negative after
training mk.
For multiplicative connection, each meta-model should have enough learning power for the dataset,
since each meta-model is required to learn the distribution of the dataset or the reweighted dataset.
If any meta-model fails to learn the distribution, the performance of the boosted model will be
harmed. In subsection 4.5, we give a negative example, where a VAE and a GMM are connected by
multiplication and the overall performance is extremely bad.
4	Experiments
We now present experiments to verify the effectiveness of our method. We first validate that the
non-decreasing property of the decomposable lower bound holds in practice. Next, we give results
of boosting advanced models to show that our method can be used as a technique to further promote
the performance of state-of-the-art models. Then, we compare our method with naively increasing
model capacity. Finally, we make a comparison between different generative boosting methods.
4.1	Setup
We do experiments on static binarized mnist (LeCun & Cortes, 2010), which contains 60000 train-
ing data and 10000 testing data, as well as the more complex celebA dataset (Liu et al., 2015), which
contains 202599 face images, with each first resized to 64 × 64. The meta-models we use include
RBMs (Smolensky, 1986), GMMs (Reynolds, 2015), VAEs (Kingma & Welling, 2013), ConvVAEs
(i.e., VAEs with convolutional layers), IWAEs (Burda et al., 2015), and LVAEs (S0nderby et al.,
2016), with their architectures given in Appendix C. The marginal likelihoods of RBMs are es-
timated using importance sampling, and the marginal likelihoods of VAEs and their variants are
estimated using the variational lower bound (Kingma & Welling, 2013). All experiments are con-
ducted on one 2.60GHz CPU and one GeForce GTX TITAN X GPU.
4.2	Validating the Non-decreasing Property of Decomposable Lower Bound
The non-decreasing property (Theorem 2) of decomposable lower bound (Equation 5) is the theo-
retical guarantee of the greedy layer-wise training algorithm (subsection 2.4). We validate that the
non-decreasing property also holds in practice by using RBMs and VAEs as meta-models.
5
Under review as a conference paper at ICLR 2020
We evaluate the decomposable lower bound on 4 combinations of RBMs and VAEs on static bi-
narized mnist. Since the stochastic variables in RBMs are discrete, we put RBMs at bottom and
put VAEs at top. For each combination, we evaluate the lower bound (Equation 5) at different k
(1 ≤ k ≤ 6) on both training and testing dataset. As shown in Figure 1, both the training and
testing curves of the decomposable lower bound present the non-decreasing property. We also no-
tice a slight drop at the end of these curves when incorporating VAEs, which can be explained by
the convergence (subsection 2.5): if EDEq(h1,…，hk-1 ∣x) [logmk(hk-1)] is maximized after training
mk, then the lower bound will stop growing even ifwe keep incorporating more meta-models.
PUnoq jəMo 一
Figure 1: The lower bound (Equation 5) on different combinations of meta-models. The triangular and circular
markers correspond to RBMs and VAEs respectively. (1): All meta-models are VAEs. After incorporating two
VAEs, the lower bound becomes stable. (2): The first two meta-models are RBMs and the rest are VAEs. The
second RBM and the first VAE greatly promote the lower bound. (3): The first four meta-models are RBMs
and the rest are VAEs. The lower bound grows as the first two RBMs are incorporated, while the incorporation
of next two RBMs doesn’t help promote the lower bound. We further improve the lower bound by adding two
VAEs. (4): All meta-models are RBMs. After incorporating two RBMs, the lower bound becomes stable.
Besides, the quality of generated samples of the boosted model also has a non-decreasing property,
which is consistent with the non-decreasing property of the decomposable lower bound, as shown in
Table 1. Furthermore, we can get some evidence about the convergence of the boosted model from
ck (Theorem 4). We see that ck is the smallest when k = 3, which indicates that the boosted model
is likely to converge after incorporating 3 VAEs and the last VAE is redundant.
k
(4) 6 RBMs
k	1	2	3	4
mnist
lower bound
(train / test)
ck / 10-3
(train)
celebA
3 i -； ?，二〃 4 0 5
C J -4、入 XKX
.f75^-07t7i
八∕f3313N∕
匕 q'3032i%g
耳 65)4 609)0*
“ 〉W 孑 F，? q A S
U£2 7 / 7 X ʃ 6 ?
-90.24 / -102.40
518.0
0®。4/9弓 4 412
"C-/U4 5 1552
£ £
Z / ? 1
/3/3
工，q 3
3 / C √
se√ >
⅛<? ? 1
5 * q 1
15*7
夕1。J
57 /。94r2n7 ./ 4
479Z6 +343 7
个 GG5‹γ∕m9
夕 Ss。/ £32</，
72
7f^x^-60z* v7q>7
4o5f7 夕。Gqʃ
H6GOε4513。/
，G8 7e7∕q4o
ΛTJΠ∙87 0l√07¾l
G CΓ∕c<eys32 6
5rtλ‹^wSΓt⅛,
+7›∙33k-76C
7 78 56O->I2 9
sgzgə*7Og 彳？
7QI2OS匕232
30
G夕
7 b
72
。B
7
3
y
,≡6 q 3。u6 0-11
-87.44 / -100.24	-87.45 / -100.36	-87.68 / -100.59
7.9	0.7	26.3

82 5C
α
a
7
9
9
8

lower bound
ck / 10-3
-6273.92
708.6
-6267.30
48.7
-6268.39
19.6
-6270.33
88.6
Table 1: Samples generated from boosted models consisting of k VAEs (1 ≤ k ≤ 4). The lower bounds
(Equation 5) of these boosted models are also given. ck (Theorem 4) is an indicator to help judge whether a
boosted model has converged, where a small one supports convergence. For both mnist and celebA, the quality
of generated samples has a non-decreasing property. Besides, ck is the smallest when k = 3, which indicates
that the boosted model is likely to converge after incorporating 3 VAEs and the last VAE is redundant.
6
Under review as a conference paper at ICLR 2020
4.3	Boosting Advanced Models
We show that our cascaded boosting can be used as a technique to further promote the perfor-
mance of state-of-the-art models. We choose ConvVAE (i.e., VAE with convolutional layers),
LVAE (S0nderby et al., 2016), IWAE (Burda et al., 2015) as advanced models, which represent
current state-of-art methods. We use one advanced model and one GMM (Reynolds, 2015) to con-
struct a boosted model, with the advanced model at the bottom and the GMM at the top. The result
is given in Table 2. We see that the lower bound of each advanced model is further promoted by
incorporating a GMM, at the cost of a few seconds.
	≤ logp(x)	extra time cost / S
ConvVAE ConvVAE + GMM	-88.41 -87.42		+7.85
LVAE, 2-layer LVAE, 2-layer + GMM	-95.73 -95.50		+11.76
IWAE, k=5 IWAE, k=5 + GMM	-81.58 -80.38		+9.41
IWAE, k=10 IWAE, k=10 + GMM	-80.56 -79.20		+8.39
Table 2: Test set performance on mnist. The density log p(x) is estimated using Equation 5. LVAE has 2
stochastic hidden layers. The number of importance weighted samples (k) for IWAE is 5 and 10. The number
of components in GMM is set to 10. The extra time cost is the time cost for incorporating an extra GMM.
The performance improvement by incorporating a GMM is theoretically guaranteed: since m1 (h1)
is a standard Gaussian distribution in above four cases considered in Table 2, when m2(h1) is a
GMM, which covers the standard Gaussian distribution as a special case, we can ensure that l2
(Equation 7) will not be negative after training m2 . Besides, the dimension of hidden variable h1 is
much smaller than the dimension of observable variable h0 for VAEs and their variants, and thus the
training of m2 only requires very little time.
4.4	Comparison with Naively Increasing Model Capacity
We compare our cascaded boosting with the method of naively increasing model capacity. The con-
ventional method of increasing model capacity is either to add more deterministic hidden layers or to
increase the dimension of deterministic hidden layers, so we compare our boosted model (Boosted
VAEs) with a deeper model (Deeper VAE) and a wider model (Wider VAE). The Deeper VAE has
ten 500-dimensional deterministic hidden layers; the Wider VAE has two 2500-dimensional de-
terministic hidden layers; the Boosted VAEs is composed of 5 base VAEs, each of them has two
500-dimensional deterministic hidden layers. As a result, all the three models above have 5000 de-
terministic hidden units. Figure 2 shows the results. Wider VAE has the highest lower bound, but
its generated digits are usually undistinguishable. Meanwhile, the Deeper VAE is able to generate
distinguishable digits, but some digits are rather blurred and its lower bound is the lowest one. Only
the digits generated by Boosted VAEs are both distinguishable and sharp.
lower bound
-102.651
-140.54
-87.74
-100.91
methods
Base VAE
(2 hiddens; 500-dimensional)
Deeper VAE
(10 hiddens; 500-dimensional)
Wider VAE
(2 hiddens; 2500-dimensional)
Boosted VAEs
(5 Base VAEs)
Figure 2: Comparison between our cascaded boosting and the method of naively increasing model capacity.
The lower bounds are evaluated on mnist test dataset.
Since straightforwardly increasing the model capacity is likely to cause serious challenges, such as
vanishing gradient problem (Hochreiter et al., 2001) and exploding gradient problem (Grosse, 2017),
it often fails to achieve the desired results on improving models’ learning power. Our boosting
method avoids these challenges by leveraging the greedy layer-wise training algorithm.
7
Under review as a conference paper at ICLR 2020
4.5 Comparison between Different Generative Boosting Methods
We make a comparison between our cascaded boosting, multiplicative boosting and hybrid boosting.
The result is given in Table 3. The hybrid boosting produces the strongest models, but the time cost
of density estimation and sampling is high, due to the troublesome partition function. Our cascaded
boosting allows quick density estimation and sampling, but its boosted models are not as strong as
the hybrid boosting. It is also worth note that the multiplicative connection of one VAE and one
GMM produces a bad model, since the learning power of a GMM is too weak for directly learning
the distribution of mnist dataset and the training time of a GMM is long for high dimensional data.
		≈ logp(x)	time cost / s		
			train	density estimate	sample
cascaded	VAE+VAE	-99.53	223.85	O2^	0Γ3-
	VAE+GMM	-98.13	116.33	0.14	0.12
multiplicative	VAEkVAE	-95.72	225.21	50.91	543.82
	VAEkGMM	-506.76	2471.60	130.65	480.95
hybrid	(VAE+GMM)kVAE	-94.28	225.23	125.20	1681.77
	(VAE+GMM)k(VAE+GMM)	-93.94	226.86		147.82	2612.59
Table 3: Comparison between different boosting methods on mnist. The ‘+’ represents the cascaded connection
and the ‘k’ represents the multiplicative connection. The density log p(x) is estimated on the test set, using
Equation 5 for cascaded connection and importance sampling for multiplicative connection respectively. The
sampling time is the time cost for sampling 10000 samples.
5	Related Work
Deep Belief Networks. Our work is inspired by DBNs (Hinton et al., 2006). A DBN has a multi-
layer structure, whose basic components are RBMs (Smolensky, 1986). During training, each RBM
is learned separately, and stacked to the top of current structure. It is a classical example of our
cascaded boosting, since a group of RBMs are cascaded to produce a stronger model. Our decom-
posable variational lower bound reveals the principle behind the training algorithm of DBNs: since
mk(hk-1) is initialized from mk-1 (hk-1) for DBNs, lk = 0 (Equation 7) after the initialization.
After training mk by maximizing ED Eq(hι,∙ ,hk-ι∣x) [logmk (hk-ι )],we can make sure that lk ≥ 0,
assuring the non-decreasing property of the decomposable lower bound (Equation 5).
Deep Latent Gaussian Models. DLGMs (Deep Latent Gaussian Models) are deep directed graph-
ical models with multiple layers of hidden variables (Burda et al., 2015; Rezende et al., 2014). The
distribution of hidden variables in layer k conditioned on hidden variables in layer k+1 is a Gaussian
distribution. Rezende et al. (2014) introduce an approximate posterior distribution which factorises
across layers. Burda et al. (2015) introduce an approximate posterior distribution which is a di-
rected chain. Our work reveals that the variational lower bound of Burda et al. (2015) can be further
decomposed and optimized greedily and layer-wise.
Other methods of boosting generative models. Methods of boosting generative models have
been explored. Previous work can be divided into two categories: sum-of-experts (Figueiredo &
Jain, 2002; Rosset & Segal, 2003; Tolstikhin et al., 2017), which takes the arithmetic average of
meta-models’ distributions, and product-of-experts (Hinton, 2002; Grover & Ermon, 2018), which
takes the geometric average of meta-models’ distributions.
6	Conclusion
We propose a framework for boosting generative models by cascading meta-models. Any hidden
variable meta-model can be incorporated, as long as it supports likelihood evaluation. The decom-
posable lower bound allows us to train meta-models separately and greedily. Our cascaded boosting
can be integrated with the multiplicative boosting. In our experiments, we first validate that the
non-decreasing property of the decomposable variational lower bound (Equation 5) holds in prac-
tice, and next further promote the performance of some advanced models, which represent state-of-
the-art methods. Then, we show that our cascaded boosting has better performance of improving
models’ learning power, compared with naively increasing model capacity. Finally, we compare dif-
ferent generative boosting methods, validating the ability of the hybrid boosting in further improving
learning power of generative models.
8
Under review as a conference paper at ICLR 2020
References
M Ehsan Abbasnejad, Anthony Dick, and Anton van den Hengel. Infinite variational autoencoder for
semi-supervised learning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR),pp. 781-790. IEEE, 2017.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training
of deep networks. In Advances in neural information processing systems, pp. 153-160, 2007.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv
preprint arXiv:1509.00519, 2015.
Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1):
1-22, 1977.
Mario A. T. Figueiredo and Anil K. Jain. Unsupervised learning of finite mixture models. IEEE
Transactions on Pattern Analysis & Machine Intelligence, (3):381-396, 2002.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Roger Grosse. Lecture 15: Exploding and vanishing gradients. University of Toronto Computer
Science, 2017.
Aditya Grover and Stefano Ermon. Boosted generative models. In Thirty-Second AAAI Conference
on Artificial Intelligence, 2018.
W Keith Hastings. Monte carlo sampling methods using markov chains and their applications. 1970.
Geoffrey E Hinton. Products of experts. In 1999 Ninth International Conference on Artificial Neural
Networks ICANN 99.(Conf. Publ. No. 470), volume 1, pp. 1-6. IET, 1999.
Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
computation, 14(8):1771-1800, 2002.
Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief
nets. Neural computation, 18(7):1527-1554, 2006.
Sepp Hochreiter, YoshUa Bengio, Paolo Frasconi, Jurgen Schmidhuber, et al. Gradient flow in
recurrent nets: the difficulty of learning long-term dependencies, 2001.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the 2nd
International Conference on Learning Representations (ICLR), 2013.
Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In Advances in neural information processing systems, pp.
3581-3589, 2014.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Chongxuan Li, Jun Zhu, and Bo Zhang. Max-margin deep generative models for (semi-) supervised
learning. IEEE transactions on pattern analysis and machine intelligence, 40(11):2762-2775,
2018.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), 2015.
9
Under review as a conference paper at ICLR 2020
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Douglas Reynolds. Gaussian mixture models. Encyclopedia of biometrics, pp. 827-832, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-
proximate inference in deep generative models. In International Conference on Machine Learn-
ing, 2014.
Saharon Rosset and Eran Segal. Boosting density estimation. In Advances in neural information
processing systems, pp. 657-664, 2003.
Reuven Y Rubinstein and Dirk P Kroese. Simulation and the Monte Carlo method, volume 10. John
Wiley & Sons, 2016.
Paul Smolensky. Information processing in dynamical systems: Foundations of harmony theory.
Technical report, COLORADO UNIV AT BOULDER DEPT OF COMPUTER SCIENCE, 1986.
Casper Kaae S0nderby, TaPani Raiko, Lars Maal0e, S0ren Kaae S0nderby, and Ole Winther. Ladder
variational autoencoders. In Advances in neural information processing systems, pp. 3738-3746,
2016.
Ilya O Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard
Scholkopf. Adagan: Boosting generative models. In Advances in neural information process-
ing systems, pp. 5424-5433, 2017.
Lei Xu and Michael I Jordan. On convergence properties of the em algorithm for gaussian mixtures.
Neural computation, 8(1):129-151, 1996.
10
Under review as a conference paper at ICLR 2020
A Proof of Theorem 1
Proof. Using q(hι,…,hk-ι∣χ) as the approximate posterior, We have a variational lower bound
Lk(mi,…，mk) for logpk(x):
logpk(x) ≥ Lk(mi,…，mk)= Eq(hι,… 也一㈤
logPk(x, hi,…，hk-i)
.g q(hi,…，hk-i∣x).
with
Pk(x, hi,…,hk-i) = mk(hk-1) Qi=1 mim(hi)i) =	( ) π mi+i(hi)
q(hi, ∙∙∙ , hk-i∣x)	Qk-I mi(hi-1 ,hi)	m1 z ɪɪ mi(hi)
i=i mi(hi-1)	i=i
Thus, the lower bound is equal to:
Eq(h……{log [mi(x) ∏ m+^]}
k-i
=logmi (x) +EEq(hi,…也 ∣χ)
i=i
mi+i(hi)
og mi(hi) 一
k-i
=logmι(x) + E {Eq(h],…也∣χ) [logmi+i(hi)] - Eq(hi,…也㈤[logmi(hi)]}.
i=i
Thus,
k-i
logPk(x) ≥ logmi(x) + E {Eq(h],…，hi∣χ) [logmi+i(hi)] - Eq(hi,…也㈤[logmi(hi)]}.
i=i
Take the expection with respect to dataset D, we have
ED [logPk (x)]
k-i
≥Ed [logmι(x)] + E {EdEq(hi,…,hi∣χ) [logmi+i(hi)] - EDEq(hi,…也⑻[logmi(hi)]}
i=i
k
=E Lk(hi,…，hk).
i=i
□
B Proof of Theorem 3 and Theorem 4
Proof. Let qi(x, hi,…，hi) := PD(x)q(hi,…，hi|x) (1 ≤ i ≤ n), where n is the number of
meta-models. Since qi(x,hi,…，hi) is exactly the marginal distribution of qj (x,hi, ∙∙∙ , hj) when
1 ≤ i < j ≤ n, we can omit the subscript, thereby writing qi as q.
When EDEq(hi,… 小^⑻[logmk(hk-i)] = Eq®—]) [logmk(hk-i)] is maximized after training
mk, we have mk(hk-i) = q(hk-i) a.e..
For any j ∈ [k + 1,n] ∩ Z and any mk+i,mk+2,…，mj-, given i (k + 1 ≤ i ≤ j), we have
li(mi ,…，mi) =ED Eq(hi,…，hi-ι∣x) [log mi(hi-i)] - ED Eq(hi,…，hi-ι∣x) [log mi-i(hi-i)]
=Eq(X,hi,…，hi-i) [log mi(hi-i)] - Eq(x,hι,…，hi-i) [log mi-i(hi-i)]
=Eq(hk-i )Eq(hk,…，hi-i |hk(i) [log mi(hi-i)]
-Eq(hk-i )Eq(hk ,…，hi-i |hk(i) [log mi-i (hi-i )]
=Emk(hk-i)Eq(hk,…，hi-1∣hk-1) [logmi(hi-i)]
-Emk (hk-i)Eq(hk,…，hi-1∣hk-1) [logmi-i(hi-i)] .
11
Under review as a conference paper at ICLR 2020
Besides, we have
j-i
Pj(hk-1,…，hj) = mj(hj-1, hj) ɪɪ mi(hi-i∣hi)
i=k
and
j -i
q(hk,…，hj-i∣hk-l) = ɪɪmi(hilhi-i).
i=k
Let q(hk,…，hj-ι∣hk-ι) be the approximate posterior of Pj(hk-ι,…，hj-ι), according to The-
orem 1, we have
Emk(hk-i) [logPj(hk-i)] ≥Emk(hk-i) [logmk(hk-i)]
I X (Elm-kc(ihks--L')Elq(ihks∙- ,hi-ι∖hk-ι) [log mi(hi-1)]	)
i = k + 1 I - Emk (hk-1)Eq(hk∙∙∙ ,hi-i∖hk-1) [lOg mi-1(hi-1)] J
j
=Emk(hk-i) [logmk(hk-i)] + E li(mi,…，mi)
i=k+i
Since
Emk (hk-1) [logpj (hk-1 )] ≤ Emk (hk-1) [logmk (hk-1 )] ,
we have Pj=k+1 li(mι,…，mi) ≤ 0, and thereby Lj(mi,…，mj) ≤ Lk(mi,…，m®).
Finally, we have
ED Eq(hi,…,hk∣χ) [logmk (hk)] = Eq(X,hi,…，hk) [logmk (hk )]
= Eq(hk-1)Eq(hk|hk-1) [logmk(hk)]
= Emk(hk-i)Emk(hk|hk-i) [logmk(hk)]
= Emk (hk) [logmk (hk)] .
Thus, Ck(mi,…，mk) := |EdEq(hι,…,h%∣x) [logm®(hk)] - Emk(hQ [logm®(hk)] | = 0.
□
C Architectures of Meta-Models
The architectures of VAEs, ConvVAEs, IWAEs and LVAEs are given in this part.
C.1 Architectures of VAEs
All VAEs have two deterministic hidden layers for both generation, and inference and we add batch
normalization layers (Ioffe & Szegedy, 2015; S0nderby et al., 2016) after deterministic hidden lay-
ers. The dimension of deterministic hidden layers is set to 500 and 2500, and the dimension of
stochastic hidden variables is set to 20 and 100, for experiments on mnist and celebA respectively.
C.2 Architectures of ConvVAEs
The ConvVAE has one 500-dimensional deterministic hidden layer and one 50-dimensional stochas-
tic hidden variable, with four additional convolutional layers LeCun et al. (1998). All convolutional
layers have a kernel size of 4 × 4 and a stride of 2. Their channels are 32, 64, 128 and 256 respec-
tively. We add batch normalization layers after deterministic hidden layers.
C.3 Architectures of IWAEs
The IWAE has two 500-dimensional deterministic hidden layers and one 50-dimensional stochastic
hidden variable. The number of importance sampling is set to 5 and 10.
C.4 Architectures of LVAEs
The LVAE has four 1000-dimensional deterministic hidden layers and two 30-dimensional stochastic
hidden variables. We add batch normalization layers after deterministic hidden layers.
12