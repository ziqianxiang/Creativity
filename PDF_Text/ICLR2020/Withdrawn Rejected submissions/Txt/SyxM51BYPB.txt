Under review as a conference paper at ICLR 2020
A new perspective in understanding of Adam-
Type algorithms and beyond
Anonymous authors
Paper under double-blind review
Ab stract
First-order adaptive optimization algorithms such as Adam play an important role
in modern deep learning due to their super fast convergence speed in solving large
scale optimization problems. However, Adam’s non-convergence behavior and
disturbing generalization ability make it fall into a love-hate relationship to the
deep learning community. Previous studies on Adam and its variants (refer as
Adam-Type algorithms) mainly rely on theoretical regret bound analysis, which
overlooks the natural characteristics residing in Adam-Type algorithms and limits
our understanding. In this paper, we aim at seeking a different interpretation of
Adam-Type algorithms so that we can intuitively comprehend and improve them.
The way we chose is based on a traditional online convex optimization algorithm
scheme known as mirror descent method. By bridging Adam and mirror descent
algorithm, we receive a clear map of the functionality of each term in Adam. In ad-
dition, this new angle brings us a new insight on identifying the non-convergence
issue of Adam and explaining the superior convergence speed of Adam than other
first-order methods. Moreover, we provide a new variant of Adam-Type algo-
rithm, namely AdamAL which can naturally mitigate the non-convergence issue
of Adam and improve its performance. We further conduct experiments on various
popular deep learning tasks and models, and the results are quite promising.
1	Introduction
In recent years, first-order optimization algorithms with adaptive learning rate have become the
dominant method to train deep neural networks because these methods show extraordinary power
in solving large-scale machine learning optimization problems. By cooperating with first-order in-
formation, adaptive methods iteratively update parameters by moving them to the direction of the
negative gradient of the cost function with non-fixed learning rate. The first algorithm in this line of
research can be dated back to McMahan & Streeter (2010), where they adaptively choose regular-
ization functions for bounding objective function parameters based on the loss functions observed
at each iteration. Then, in Streeter & McMahan (2010), they demonstrate that the convergence rates
can often be dramatically improved through the use of preconditioning. The insight of these two
methods is parallel but effective, that is, they try to modify the gradients’ magnitude with adaptive
pre-coordinate learning rates. Later, AdaGrad (Duchi et al., 2011) carefully chooses the precon-
ditioner and provides the first practical adaptive algorithm with a tight theoretical guarantee based
on Zinkevich (2003) regret analysis. Although AdaGrad achieves a great success in the sparse
settings, the rapid decay of the adaptive learning rate limits its usage. This is due to AdaGrad ac-
cumulating all the past gradients as its learning rate. And this unbounded learning rate becomes
extremely small when one has a large number of training iterations. To address this, several variants
of AdaGrad, such as AdaDelta (Zeiler, 2012) and RMSProp (Hinton et al., 2012) have been pro-
posed to mitigate the rapid decay of the learning rate. Based on these, Adma (Kingma & Ba, 2014)
incorporating with first momentum correction accelerates the convergence speed of first-order op-
timization algorithms to a new height. The use of exponential moving average (EMA) in Adam
becomes a key to Adam’s success. Up to this point, we can summarize the adaptive or non-adaptive
first-order optimization algorithms as follows.
Denote gt ∈ Rd as the gradient of generic optimization problem f with respect to its parameters
x ∈ Rd at iteration t, then the generic updating rule of adaptive methods can be expressed as (Reddi
1
Under review as a conference paper at ICLR 2020
et al., 2019):
ηt
xt+1 = Xt-----尸 Θ mt	(I)
√Vt
where denotes the entry-wise or Hadamard product and the αt is the base learning rate. In the
equation above, mt = S(gι,…，gt) is a function related to the historical gradients UP to t; and
vt = υ(gι,…，gt) is a function that produces a d-dimensional vector with non-negative entry; The
design of vt is simPle of non-adaPtive methods, such as vanilla stochastic gradient descent (Vanilla
SGD), where we have vt = I, however, it is crucial for Adam-Type algorithms. For Adam, in
particular, the mt and vt are computed by EMA of gradient, with coefficient β1 and β2 where
i=t	i=t
mt=(1-β1)Xβ1t-igi and vt=(1-β2)Xβ2t-igi2	(2)
i=1	i=1
Adam, the most popular adaptive method, has been widely adopted by the deep learning community.
The root cause of the fast convergence of Adam and its variants in convex or non-convex optimiza-
tion problems remains an open question (Chen et al., 2018). In addition, the generalization ability
and out-of-sample behavior of Adam-Type methods are even worse than traditional non-adaptive
counterparts such as Vanilla SGD (Wilson et al., 2017).
In order to understand the insight behind those adaptive algorithms and close the generalization gap,
several Adam-Type algorithms have been proposed including (Reddi et al., 2019; Luo et al., 2019;
Zhou et al., 2018b; Balles & Hennig, 2017; Liu et al., 2019). Although they propose many different
kinds of viewpoints in understanding the performance of Adam and demonstrate a series of correc-
tion methods to improve Adam, we think the mechanism of Adam-Type algorithms is still unclear.
For example, one common thinking about about mt and vt in Adam is first and second moments
of unbiased estimator gt, however, why this second moment can be used as adaptive learning rate?
Does the vt really behave as the second moment of the gradient gt ? Also, another commonly asked
question is where the Adam adopts such a faster convergence speed than other first-order methods?
These mysterious questions on adaptive methods finally leads us to rethink the Adam-Type algo-
rithms in a neutral way. In this paper, we provide a new insight into Adam-Type algorithms, which
brings a new perspective of comprehension on Adam-Type algorithms and it allows us to easily
identify the misbehavior of Adam such as non-convergence issues. In the aforementioned works,
the analysis of Adam-Type methods is based on Kingma & Ba (2014) framework, which limits our
understanding. In fact, when we look back to the origin of adaptive learning rate methods mentioned
in Streeter & McMahan (2010), we notice that the design of Adam is highly related to adaptive regu-
larization of follow-the-proximally-regularized-leader (FTPRL) method and it can also be regarded
as a variant of traditional mirror descent method (Xiao, 2010). The more detail and our motivation
can be found in the next section. For simplicity, we use standard Adam as our touchstone through
the paper to convey our main idea, and their variants follow the same thoughts. Now, we summarize
our contribution in three folds:
1.	We provide a new perspective in understanding the non-convergence behavior of Adam-
Type algorithms based on mirror descent approach. Our analysis agrees well with the
previous works but much more intuitive and effective.
2.	Based on our observation, we identify potential faults in Adam-Type algorithms and we
provide a new Adam variant algorithm, named AdamAL.
3.	We conduct a series of experiments on different machine learning tasks and models by using
our AdamAL algorithm, and the results are promising and the performance of AdamAL is
never worse than Adam.
2	Preliminaries and motivations
Notations Given a vector x ∈ Rd we denote its i-th entry by xi; We use ||x|| to denote its l2 norm;
for a vector xt in the t-th iteration, the i-th coordinate of xt is denoted as xt,i . We also define
x1:t = Pti=1 xi. Given two vectors x, y ∈ R, we use hx, yi to denote their inner product, x y to
denote element-wise product, y to denote entry-wise division, the max(χ, y) to denote entry-wise
maximum and min(x, y) to denote entry-wise minimum. We use S+ to denote the set of all positive
definite matrices M. We use M 1 to denote √M.
2
Under review as a conference paper at ICLR 2020
2.1	Preliminaries
Nonlinear projected subgradient methods and mirror descent algorithm Iterative gradient de-
scent (GD) scheme, which can be traced back to (Cauchy, 1847), is the simplest strategy to minimize
convex optimization problems. It was further developed as Zinkevich Online Greedy Subgradient
Project (OGSP) (Zinkevich, 2003), which can be considered as a variation of project gradient de-
scent (PGD) algorithm with following updating rules:
xt+1 =PX(xt -ηf0(xt)) (standard PGD) xt+1 = PXA(xt - ηtgt) (Zinkevich OGSP) (3)
where PXA is a distance-based projector denoting the projection of a point y onto X by PXA (y) =
argmi□x∈x ||x - y∣∣A, and || ∙ ||a = (x,Ax), and gt ∈ ∂ft(xt) = f0(xt) is the subgradient of
the objective function ft(∙). The well-known bottleneck of using PGD is that it can only work in
Hilbert space H and it cannot be extended to more general cases (in modern machine learning) of
optimization plays in some Banach space B where the Euclidean norms cannot be computed. To this
end, the mirror descent algorithm (MDA) introduced by (Nemirovsky & Yudin, 1983) overcomes
such infeasibility by using the linearity on dual vector space B* and a carefully designed mirror
map (Bubeck, 2014). Hence, MDA has following updating schemes:
xt+1 = argmin{hf0(xt),xi +——Dψ (∙)}
x∈X	ηt
(4)
the nonlinear projection therefore encoded in the term Dψ . By replacing the Euclidean quadratic
norms in Equation. 3 with more general distance-liked settings such as Bregman distance function
Dψ(∙) define on ψ(∙), the equivalence of PGD algorithm and MDA has been proved by Beck &
Teboulle (2003). For example, the simplest version of MDA is that: taking ψ(x) = 2 ∣∣χ∣∣2, and
Dψ(x,xt) = 1 ||x - xt||2 then plugging into Equation. 4, We have:
xt+1 = argmin{hgt,xi + 丁||x - Xt∣∣2}
x∈X	2ηt
(5)
Update scheme in the above equation. 5 is also known as proximal point algorithm (Rockafellar,
1976). The equivalence of Equation. 5 and PGD can be achieved by taking the derivative of our
target function on x, and rearranging the formula:
xt+1 = x* = xt - ηtgt = PXI (xt - ηtgt)
(6)
Now, we see the last two expressions in Equation. 6 are well-known subgradient descent update. We
restate the proposition 3.2 in Beck & Teboulle (2003) as:
Proposition 1. Assume X is a closed convex subset in R with non-empty interior, and objective
function f : X → R is a convex and Lipschitz function. Suppose the optimal set ofx denoted by X*
is non-empty, we can compute the subgradient of f atx as g ∈ ∂f(x). Fora convex mirror mapping
function ψ : X → R with conjugate function ψ* defined by ψ* (y) = maxx∈X {hx, yi - ψ(x)}.
Then the sequence {xt} ⊆ X generated by MDA is equivalent to the sequence generated by PGD.
We state the equivalence of PGD algorithm and MDA, particularly, the general gradient descent (or
SGD) can be directly derived from the Equation. 6.
Follow the proximally-reqularized leader (FTPRL) FTPRL is introduced by McMahan & Streeter
(2010) belongs to the family of follow-the-regularized-leader (FTRL) algorithms such as Regular-
ized Dual Averaging (Xiao, 2010). In general, FTRL-Type has following update rule:
xt+1 = argmin{Q2 fT(XT)) ∙ x + Ri：t(x)
x∈X
(7)
where the subgradient of objective function fτ0 (xτ) is approximated by the gradient at xτ and
R1:t(x) is defined as regularization. Particularly, the formal FTPRL is:
1 t ɪ
xt+1 = argmin{gi：t ∙ x + Φι.t ∙ X + Ψ(x) + q EIIQa(X - XT)||2}
(8)
x∈X
with φι∙.t ∙ x + Ψ(x) can be considered as non-smooth composite term which is orthogonal to our
3
Under review as a conference paper at ICLR 2020
Table 1: An overview of first-order optimization methods using the generic framework
	SGD	SGDM	AdaGrad	RMSProP	Adam
mt	gt	Pit=1 βt-igi	gt	gt	(1 -β1)Pit=1β1t-igi
vt	I	I	Pit=1 gi2	(1-β) Pt=ι βt-⅞T	(1 - β2) Pit=1 β2t-igi2
lr	ηt	ηt	η √v	η √v	η √v	
paper, more details can be found in McMahan (2010b). The last term in the above equation is
stabilizing regularization to ensure low regret. We mention that the Qτ can be either regarded as
scale of regularization or generalized learning rate which plays crucial role in our paper. As we
can see, FTPRL appears quite different from MDA stated in Equation. 5, however, in McMahan
(2010a;b) they show that in the case of selecting quadratic stabilizing regularization, the FTPRL and
generalized MDA only has differences in parameter centering. In fact, MDA is illustrated in the
Equation. 4 regularizing the parameter to be close to the origin, on contrast, FTPRL is regularizing
the parameter at current feasible point. No surprising, McMahan (2010a) propose the equivalence
proof of FTPRL and a variation algorithm of the MDA as follow.
Proposition 2. (McMahan, 2010a) Let Rt be a sequence of differentiable convex functions
(VRt (0) = 0), and let Ψ be an arbitrary Convexfunction. Define the Proximal-MDA with updating
rule:
xt+1 = arg min{hgt(xt),xi + Ψ(x) + Dr. (∙)}	(9)
x∈X
where the Bregman distance function DRt with respect to Rt where Rt(x) = Rt(x - xt). And
applying FTPRL to the same objective function, with:
xt+1 = argmin{gi：t ∙ x + φι∙.t ∙ X + Ψ(x) + Ri：t}	(10)
x∈X
when φt ∈ ∂Ψ, Such that gi：t + φ∖∙.t + VR±t(xt) = 0. Then the two above update scheme are
equivalent.
At this moment, we state a series of equivalence proposition from FTPRL to proximal-MDA (vari-
ation of MDA) and MDA to PGD. Now, we can construct the equivalence proof of FTPRL and
PGD properly. Although the direct proof of equivalence between FTPRL and PGD is provided un-
officially in McMahan (2010a), we would like to elaborate the intuition behind each algorithm and
deduce our perspective in understanding the Adam-Type algorithms.
2.2 Motivation
Non-Adam-type and Adam-type algorithms to mirror descent (FTPRL) Non-Adam-type online
gradient descent algorithms including SGD, SGD with momentum, Polyak’s HB and Nesterov’s ac-
celerated gradient method can be easily understood as first order optimization with different momen-
tum function. And, interestingly, most of them have physical interpretations. However, as shown
in Table 1, Adam-Type algorithms do not rely on the non-increasing learning rate when iteratively
updating their parameters. Instead, they perform adaptively learning rate element-wise on parame-
ters according to the first order information. However, this leads to the most mysterious part in the
Adam-TyPe algorithm where the adaptive update is represented as --√= Θ mt.One common inter-
vt
pretation of this updating representation is regarding - -√V^ as adaptive learning rate, and regarding
mt as general first order gradient. However, we can not treat Adam-Type algorithm in this way be-
cause the first order information gt resides in both mt and vt and we are unable to simply decorrelate
them as common gradient with step-size scheme. Another possible interpretation of such updating
is related to the Newton’s second order method, but there is no free lunch for expressing in this way.
We will discuss it later.
In order to dissect Adam-Type algorithm, we recall the FTPRL algorithm mentioned in the previous
section. A natural question arises: can we interpret Adam-Type algorithm as a variant of mirror de-
scent method? Before answering the question, let us explain why we present Adam-Type algorithm
as MDA is beneficial. We summary below:
4
Under review as a conference paper at ICLR 2020
Table 2: An overview of first-order optimization methods using Mirror Descent expression
	SGD	SGDM		AdaGrad	Adam	
A	gi：t ∙ X	(g1:t	i=t,j=t -X βj+1-i) ∙ X i=1	gt∙ X	(g2:t	i=t,j =t -X βj+1-i)∙X i=2
C	2 ||x - xτ||2		2 ||x - xτ ||2	1 t 2 Eσ"lx - χτ ||2 T = 1	1 2	t X στ ||x - χτ ||2 τ=2
ρ	1		1 1-β	1		1
* Term A, C, ρ are defined in Equation 11. See below.
1.
2.
3.
Implicit updates: This concept was first derived by Kivinen & Warmuth (1997) and later
pointed by Kulis & Bartlett (2010). It refers to without using explicit first-order update
rules to efficiently compute the parameters. The mirror descent algorithms, in fact, adopt
implicit update rules very well, therefore, the explicit update will roll into some regularized-
liked terms which can elaborate more insights. In contrast, like Adam, the learning rate is
explicitly defined as √η= which is hard to identify from intuition unless relying on the
theoretical proofs.
First-order information dissection The entire adaptive update involves in Hadamard
product and division. Both numerator and denominator are highly related to the construc-
tion function with respect to the first-order gradient g. The underlying relation between
two functions mt and vt can not be decorrelated. However, as shown in Table 2, we rep-
resent Adam-Type algorithms in MDA way so that we can separate the numerator and
denominator as simple additive scheme where the hard-understanding division disappears
and meanwhile, the alignment of mt and vt gone.
Equivalence guarantee The original adaptive method AdaGrad is build upon non-linear
subgradient projection, and Adam actually inherit such design indicate by their regret proof.
To re-represent Adam-Type algorithm as FTPRL style, we do require the theoretical equiv-
alence guarantee so that we can transform safely. As we discuss in previous (Proposition
1& 2), we successfully build such a bridge in between two types of algorithm.
In general, these first-order algorithms can be written in FTPRL style (also see Table 2):
1 t ɪ
argmin{ρ(gi：t + Ci：t) ∙ x + Ψ(x) + - X ∣∣Q∙2(X - XT)||2}
χ∈X	X------{------} l{z} 2 M
B i
xt+1
(11)
{z
A
}
{^^^^^^^^^^^^^^^
C
The understanding of above representation is highly related to our analysis on Adam-Type algo-
rithms. Term A has two parts, the first part gi：t ∙ X is an approximation to fi：t based on the gradient;
the second part Ci：t ∙ X refers to first-order momentum correction or fault tolerant in this literature.
Term B is similar to the setting of FTPRL, but we usually consider it as l2-regularization. In ad-
dition, term C stabilizing regularization plays a crucial role in this transformation because (1) the
implicit updates from Xt to Xt+1 happen in this term; (2) the Qτ residing in norms can be regarded
as generalized learning rate or even more complicated format; (3) the rule of parameter centering
to the current feasible solution is figured. Finally, the leading ρ is a balancing coefficient, aims at
controlling the tendency of minimization between term A and C. Smaller ρ value will guide the
minimization process and relies more on term C, otherwise term A will dominate the minimization.
The simplest format transformation from Vanilla SGD to MD is illustrated in Equation. 5. Now, we
rewrite it as FTPRL style according to Equation. 8 (for the sake of simplicity, we do not consider
the Ψ):
1t
xt+1 = argmin{gi：t ∙ X + 2 2。丁||x - XT∣∣2}
(12)
where the ρ = 1, C1:t is zero, and Qτ sets to be Qτ = στ2I. If Vanilla SGD with learning rate
ηt = 1, the PT=ι στ = t. If it with fixed learning rate η = k, then PT=ι στ
k . To be
5
Under review as a conference paper at ICLR 2020
mentioned here, fixed learning rate SGD is kind of special, it can be easily explained as Equation. 5,
however, in FTPRL format (Equation. 12), we need to be more careful.
Mirror descent like Vanilla SGD with non-increasing learning rates shows the great insight of un-
derstanding current algorithms. First, we are always looking for the next step xt+1 in the opposite
direction of the current gradient because cos(π) = -1 minimize Equation. 12. Second, xt+1 is be-
ing bound in a region centered at previous step xt . This is due to the fact that we do not want the new
solution to be far away from the current feasible solution, otherwise, may cause slow convergence,
and McMahan (2010b) confirm this view. Third, Ptτ=1 στ should be non-decreasing alone time.
Similar, when στ gets smaller, the bounding region expands, reducing the convergence speed.
3 New Perspective on Adam
In this section, we deliver anew perspective on Adam-Type algorithm from the Mirror Descent point
of view. In this lecture, we mainly focus on Adam to demonstrate our analysis, for the other variants,
they have similar results.
3.1 Adam on Mirror Descent
According to the Table 2, the subgradient projected Adam can be written as follows (entry-wise). In
order to make a clear comparison, we also show SGDM alongside.
τ =t,j =t	1 t
Adam: xt+ι,i = argmin{ (gi：t,i - £ βj+1-igτ,i) ∙ x,i + 万 fστ,i∣∣x,i - Xτ,i∣∣2}
x∈X	τ=1	τ=1	(13)
1	τ=t,j =t	1 t
SGDM: xt+ι,i	= argmin{ -—ɪ (gi:t,i	- E	βj+1-igτ,i)	∙ x,i + q fσT∣∣x,i	- Xτ,i∣∣2}
x∈X 1 - β1	τ=1	2 τ=1
where both β1 ≤ 1 (commonly chose β1 = 0.9), the Ptτ =1 στ,i
√(1 - β2) ∑τ=1 β2-τ gT,i
in Adam settings with β2 = 0.999. In SGDM, PT =1 σT performs differently, (1) SGDM is non-
adaptive method, the T* applies to all entry on the x; (2) PT =1 σ^ = t if We have learning rate
ηt = 1, it also means σ* = 1 for all T ∈ T. First, let us recall Vanilla SGD and compare it with
SGDM:
Corollary 1. Compare to Vanilla SGD, SGDM employs first-order momentum correction defined
in Section. 2.2 to correct the possible wrong direction prediction, making a smooth optimization
trajectory. (This is a well known truth. We verify it by experiments show in Appendix.)
Besides, we also notice that Adam has P = 1 while SGDM has P = ɪ-^ = 10 (a common setting
ofβ = 0.9). Recall the previous section, by definition of ρ, we have following corollary:
Corollary 2. Compare to Vanilla SGD or Adam, SGDM has larger P value indicates that the SGDM
is more sensitive on the value change of loss function.
To explain this, we know one of the drawbacks of SGDM is that SGDM is prone to oscillation
around the optimal point, because SGDM has relatively weak bound in proximal terms, and is very
sensitive to small change of loss, that is, P factor amplifies this loss change up to tenfold. We notice
that Adam avoids such imbalance between objective function and its parameter regularization by
using EMA wisely.
Now, back to Adam, before we move forward, we define some terms in the Equation. 13.Adam,
Proximal Searching Region refers to D = ||x - xT ||22, this Euclidean quadratic norm
reflects the geometry of given constraints feasible set X . We can also treat it as a regu-
larization process. This region is inversely proportional (D α B) to the Regularization
Budget defined below.
t
Regularization Budget refer to B =	σT,i , it indicates total ”weight” we can distribute
T=1
to the Proximal Searching Region. More weight it has been given will lead to a stronger
regularization in bounding, and a small search space centering at xT .
6
Under review as a conference paper at ICLR 2020
In Hoffer et al. (2017), they define an ultra slow diffusion phenomenon when they evaluate the
distance from current weight to initial weight point with ||xt - xo∣∣2 〜logt. Interestingly, this
result is entirely consistent with our Proximal Searching Region analysis, because for any t ∈ T, we
have ∣∣χt+ι - xt|| 〜(log t) = 1. Now, we summarise our results as follows:
1.	Hyper-parameter β1 :	β1 exponential smooth eliminates the presence of imbalance
in between the goal of minimizing loss of function and the constraints of searching in
proximal region. In other words, Adam treats both conditions fairly.
2.	First-order momentum: the usage ofmt leads to a smooth optimization trajectory which
avoids the sharp twist such as SGD. It can benefit Adam if searching in the wrong direction,
the momentum correction Cτ can compensate the party of penalty directly from the loss
function.
3.	Adam-Type algorithm: Adam-Type algorithms such as AdamAL (this paper), AMSGrad
AdaBound, AdaShift, NosAda, etc. can be regarded as making a correction on one of
regularization term (most on ||QT ||x - Xτ||2).
4.	Learning rate: mirror descent corporate with implicit update makes learning rate (step
size) act as a scalar factor of regularization.
By transferring the Adam to MDA-liked method, we successfully disassemble the Adam updating
-√mt into two additive terms and identify their functionality separately. We mainly focus on the mt
part in this section, we will move our eye on √= in the next section.
3.2 ADAM Vt AND THE NON-CONVERGENCE OF ADAM
A common thinking of Adam,s adaptive learning rate √= will treat Vt as second moment approx-
imation. However, in our perspective, we regard it as adaptive regularize scalar and it performs
implicit updates by replacing the explicit learning rate. Two adaptive regularize from AdaGrad and
Adam show in below:
t
Adam:	στ,i = t
τ=1
AdaGrad:
t
στ,i
τ=1
τ=t
Xgτ2,i
τ=1
(14)
By guiding with this intuition and our regularization budget definition, we now can conclude that
1.	Strictly speaking, non-Adam-Type algorithm such as SGD(M) has only proximal search-
ing region center at xt, however, Adam-Type algorithms achieve globally stabilizing reg-
ulations via proximal searching region through {xι, x2,…，xt-ι,xt}. Therefore, in our
settings, SGD has σ function with a constant value where {στ = k∣τ = t}. A special case
is when η = t where the SDG has a descending step size t, we notice that PT =1 στ = t
and σt = 1. In fact, in our theory, we can regard SGD with decreasing learning rate as an
adaptive regularization with respect to training iterations.
2.	Adam-Type algorithms, in contrast, have stronger bonding constraints with each proximal
term ||x - Xτ∣∣2∙ With the training iterations increasing, in order to retain the similar
regularization strength, the natural way is increasing the regularization budget Ptτ =1 στ
such that Bt ≥ Bt-1. We have the following comments for Adam:
2.1	Pro: Non-decreasing regularization budget Bt can benefit for Adam, however, unbounded
budget causes the infinitely small proximal searching region. On the surface, training
will stop without parameters updates. Adam overcomes this problem wisely, expo-
nential moving average (EMA) performs as a sliding window. In other words, the
regularization budget is bounded.
2.2	Con: EMA, on the one hand, controls the regularization budget in a range, but it fails to sat-
isfy the primary requirement that regularization budget Bt should be a non-decreasing
manner. Lots of previous works point out this issue such as (Reddi et al., 2019; Luo
et al., 2019; Chen et al., 2018). However, the way they identify such problems is not
natural, for example, the objective function with periodicity gradient rarely seen in
real scenarios. Our way seems more easy to access.
7
Under review as a conference paper at ICLR 2020
17	122	86	17	134	96	77	31	213
Figure 1: The number indicates the total swapping by AMSGrad at vt,i. The darker heat map colors
reveal the lower average swapping interval.
The non-convergence of Adam This issue was first identified by Reddi et al. (2019), which points
out that the key issue in the convergence of Adam lies in the quantity
(15)
which assumes to be a non-negative value, but in training, this assumption does not always hold in
Adam. Reddi et al. (2019) construct an objective function with periodicity gradient to illustrate the
non-convergence of Adam which is hard to follow. And Luo et al. (2019) using a similar way but
conduct a heuristic experiment shows that Adam will generate extreme learning rates (also extreme
vt value) that can affect the convergence. Fundamentally, they are identifying the same problem in
different expressions. In Reddi et al. (2019) non-convergence Theorem.1 the repeated occurrence
gradient -C is, in fact, the extreme learning rate. In our analysis, the emergence of extreme learning
rates is due to decreasing regularization budget Bt - Bt-1 < 0 which is equivalent to Γ < 0,
the negative regularization coefficient στ < 0 makes the corresponding proximal searching region
||x - xτ ||22 to be arbitrarily large when minimizing the Equation. 13.Adam. Again, the parameters
behavior in out of the control manner for example ||x - xτ ||22 → +∞. To this end, Reddi et al.
(2019) proposes a very intuitive modification on Adam where
vt = max{vt, vt-1}
(16)
Although this setting solves the decreasing regularization budget issue, it still remains the problem
so called nonalignment projection, we state this problem in the next section.
3.3 non-alignment projection and AdamAL
AMSGrad (Reddi et al., 2019) uses Equation. 16 to ensure the Γt ≥ 0 for all t ∈ T. They derive it
mainly from an unrealistic objective function which has extreme gradient appearance in periodical
manner. Does AMSGrad really outperform Adam or does AMSGrad’s design follow the general
sketch of Adam-Type algorithm? We conduct an experiment to illustrate the fundamental problem
of AMSGrad (or its variant AdaShift). We call this problem as non-alignment projection. To
illustrate the non-alignment projection problem, we trace a series of entries generated by AMSGrad
and sample them from vector vt,i:j , our goal is counting the total number of times of that entry
being swap with vt,i:j and meanwhile, we record the interval between two swaps. We employ a
heat map to visualize this result. As shown in Figure 1, we can find that (1) the different entry
has very different swapping counts; (2) the swapping intervals are nonuniform. Note here sampling
vt,i from different neuron network layers and different batch size have different results, but we
demonstrate the presence of such problems. To be more specific, we present a simple one-step
AMSGrad swapping at iteration t and figure out the ill-condition problem.
Suppose in iteration t, we receive a corresponding vt,i at ith element. Then in the next step, we
would like to perform the update of vt,i to vt+1,i via vt+1,i = β2vt,i + (1 - β2)gt2+1,i based on
the original design of the Adam. However, if we choose AMSGrad to evaluate vt+2,i, we first
make the comparison between new vt+1,i and old vt,i to ensure Γt+1 to be non-negative and choose
vt+ι,i = max{vt+ι,i, vt,i}. If this SWaP happens, We will have vt+ι,i = vt,i and AMSGrad uses it
for t + 2 update, and results in
Vamsi = β2Vt+1,i +(1- β2)g∖2,i = β2vt,i +(1- β2)gt+2,i	(17)
HoWever, the true step for updating vt+2,i should be
vt+2,i = β2vt+1,i + (1 - β2)gt+2,i = β22vt,i + (1 - β2)β2gt2+1,i + (1 - β2)gt2+2,i	(18)
The difference is quite obvious shoW in Equation 17 and Equation 18. But another question is Why
AMSGrad still Working? AMSGrad can still Work is because We choose very big β2 value as 0.999
Which makes the vta+m2s,i ≈ vt+2,i . HoWever, iterative method Will accumulate this small error into
8
Under review as a conference paper at ICLR 2020
each step and as a consequence, the non-alignment of vt will lead the model to find a suspicious
local optimal. Another non-alignment refers to mt and vk, AMSGrad updates its ith parameter by
xt+1,i = xt,i - -,，' where k = t
√W
Recall in the Zinkevich,s greedy projection, we minimize the objective loss function relies on current
gradient approximation gt then We perform the projection by projector PX匹 to resolve the xt+i.
We see the vk is not the actual projection matrix for xt approximate by mt .
To address above issue, recall the define of non-decreasing regularization budget, in Adam setting,
Bt ≥ Bt-1 equivalent tovt ≥ vt-1 that is
vt - vt-1 = (1 - β2)(gt2 -vt-1) ≥ 0 ⇒ gt2 - vt-1 ≥ 0
Now, the solution for resolving Adam’s non-convergence and non-alignment of AMSGrad is clear.
That is before we evaluate vt, we modify gt to guarantee the non-decreasing condition of vt-1 to
t. We illustrate the update detail of AdamAL in Algorithm.1. Using a similar one-step example to
Algorithm 1 AdamAL
1	: Input X ∈ F, initial step size α, β1, β2,
2	: set mt = 0, vt = 0
3	: for t = 1 to T do
4	:	gt = Vft(Xt)
5	:	mt = β1mt-1 + (1 - β1)gt
6	gt = max{g2,vt-ι}
7	vt = β1vt-1 + (1 - βI)gt
8	xt+1 = Px√vt(Xt — √ηvt Gl mt)
9	: end for
illustrate how AdamAL can mitigate non-alignment issue when update vt+2 :
va+amal = β2vt+1 + (I- β2)gt+2 = β2vt + (I- β2)β2g2+l + (I- β2)g2+2	(19)
which reconstruct the vt+2 in correct expression. The major difference of AdamAL and AMSGrad
is we do not skip the gradient update for vt so that we guarantee the alignment of gt and vt so as
mt. We have the following bound for AdamAL.
Theorem 3.1. Let {xt} and {vt} be the sequences obtained from AdamAl (Algorithm 1). ηt =
η∕t1/2, βι,t = βι, βι,t non-decreasing for all t ∈ [T ]. Assume that feasible convex set F such for
all Xt ∈ F with bounded diameter D2 and ∣∣Vft(xt)∣∣ ≤ G. For Xt generated using the AdamAL,
we have the following bound on the regret:
T
X ft(Xt) - ft(x*) ≤
t=1
D2	X V1/2 +	D2	X X
2ηι(1-βι) i=1 1,i +2(1-βι) t=1 =
+
1/2
vt,i
ηt
D2
2(1 - βι)
T d	1/2
xx{ vηt--
t=2 i=1	ηt
1/2
vt-1,i
ηt-1
(20)
}
+ η√ι + iogT
(I + βI)(I - γ)p(1 - β2)
d
||g1:T,i||2
i=1
4 experiments
In this section, we turn to an empirical study of different models to compare new variants with
popular optimization methods including SGD(M), Adam, and AMSGrad. We focus on image clas-
sification tasks on CIFAR10 and CIFAR100 with different state-of-the-art deep models including
ResNet18, ResNet50, VGG16 and VGG19. From the experiment results shown in Figure. 2, we
notice that AdamAL outperforms Adam and AMSGrad! This result is desirable because we fix the
9
Under review as a conference paper at ICLR 2020




Figure 2: Top: Accuracy of Cifar10 on ResNet18 and VGG16; Cifar100 on ResNet50 and VGG16;
Top: Loss of Cifar10 on ResNet18 and VGG16; Cifar100 on ResNet50 and VGG16;(Best see in
color)
non-alignment projection issue residing in AMSGrad. In general, from the experiments, AdamAL
constantly achieve 1% more accuracy gain than Adam. Notice that we only conduct our experiments
with 80 epochs, this is due to that fact that we observe there is no further accuracy improvement
without performing any hyperparameter tuning. If we perform hyperparameter tuning, the results
show in Table.1. AdamAL can finally reach to 95% accuracy on average on test data, however, the
best run of Adam is still lower than AdamAL. It is also worth mentioning that AMSGrad have even
worse performance than Adam due to non-alignment. We also compare the result of AdamAL using
different mini-batch settings, the result shows that AdamAL is also not sensitive to min-batch size.
Algorithm	lr decay	Acc.
Adam	75,125,175	94.98
AMSGrad	75, 125, 175	94.60
AdamAL	75, 125, 175	95.13
VSGD	75,125,175	94.73
Table 3: hyperparameter tuning, learn-
ing rate halve at iteration 75, 125, 175.
Figure 3: AdamAL in different min-batch
5 discuss and conclusion
The Newton second-order method and vt The mystery of vt in Adam is fascinating. As we
discussed in the section 2.2, using second moment to describe vt runs counter to its primary purpose.
Because AdaMax (Kingma & Ba, 2014), p-NosAdam (Huang et al., 2018) and Padam (Zhou et al.,
2018a), they apply the p-norm to vt as vt1/p and Adam still performs well. Another evidence from
AdaShift (Zhou et al., 2018b), in their settings, vt is no longer acting as second moment, instead
they treat it as a learning rate scalar. In our point of view, recall the α-exp-concavity (Hazan et al.,
2007), it structures the relation from first-order to second-order Hessian with α ≤ H/G2 where
gradient upper bound is G and H > 0 is the lower bound of Hessian. This relaxed condition results
of gt2 to be an possible approximation of Hessian with scaling factor α at iteration t.
Beyond the Adam-Type method Assume the above assumption on gt2 is true, the fast convergence
speed of Adam-Type algorithms seems easy to be explained. We also notice that most of the previous
works improve the Adam by modifying (or correcting) the term C in Equation 11. Therefore, such
corrections cannot speed up the Adam, instead, they define different regularization budget. To make
one step furthers to Adam may rely on close approximations of Hessian.
In this paper, we present a new angle to look at the first-order method with adaptive learning rate.
We decouple the Adam updating rule as an addition of two regularized terms. In this way, we can
identify the intuition behind Adam-Type algorithm. Additionally, we naturally figure out the non-
convergence issue resides in AMSGrad and Adam, we propose another variant of Adam algorithm
to mitigate such problem.
10
Under review as a conference paper at ICLR 2020
References
Lukas Balles and Philipp Hennig. Dissecting adam: The sign, magnitude and variance of stochastic
gradients. arXiv preprint arXiv:1705.07774, 2017.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters, 31(3):167-175, 2003.
Sebastien Bubeck. Theory of convex optimization for machine learning. arXiv preprint
arXiv:1405.4980, 15, 2014.
Augustin Cauchy. Methode generale pour la resolution des systemes d'equations simultanees.
Comp. Rend. Sci. Paris, 1847.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence ofa class of adam-type
algorithms for non-convex optimization. arXiv preprint arXiv:1808.02941, 2018.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex
optimization. Machine Learning, 69(2-3):169-192, 2007.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning
lecture 6a overview of mini-batch gradient descent. lecture, 2012.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gap in large batch training of neural networks. In Advances in Neural Information Processing
Systems, pp. 1731-1741, 2017.
Haiwen Huang, Chang Wang, and Bin Dong. Nostalgic adam: Weighting more of the past gradients
when designing the adaptive learning rate. arXiv preprint arXiv:1805.07557, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Jyrki Kivinen and Manfred K Warmuth. Exponentiated gradient versus gradient descent for linear
predictors. information and computation, 132(1):1-63, 1997.
Brian Kulis and Peter L Bartlett. Implicit online learning. In Proceedings of the 27th International
Conference on Machine Learning (ICML-10), pp. 575-582, 2010.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265,
2019.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. arXiv preprint arXiv:1902.09843, 2019.
H. McMahan. Follow-the-regularized-leader and mirror descent: Equivalence theorems and implicit
updates. CoRR, abs/1009.3240, 01 2010a.
H. Brendan McMahan. Follow-the-regularized-leader and mirror descent: Equivalence theorems
and implicit updates. CoRR, abs/1009.3240, 2010b. URL http://arxiv.org/abs/1009.
3240.
H Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex opti-
mization. arXiv preprint arXiv:1002.4908, 2010.
Arkadii Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method
efficiency in optimization. J. Wiley, New York, 1983.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv
preprint arXiv:1904.09237, 2019.
11
Under review as a conference paper at ICLR 2020
R Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM journal on
control and optimization,14(5):877-898,1976.
Matthew Streeter and H Brendan McMahan. Less regret via online conditioning. arXiv preprint
arXiv:1002.4862, 2010.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4148-4158, 2017.
Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimization.
Journal of Machine Learning Research, 11(Oct):2543-2596, 2010.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. On the convergence of
adaptive gradient methods for nonconvex optimization. arXiv preprint arXiv:1808.05671, 2018a.
Zhiming Zhou, Qingru Zhang, Guansong Lu, Hongwei Wang, Weinan Zhang, and Yong Yu.
Adashift: Decorrelation and convergence of adaptive learning rate methods. arXiv preprint
arXiv:1810.00143, 2018b.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 928-936,
2003.
12
Under review as a conference paper at ICLR 2020
A	Auxiliary Lemmas
Lemma A.1. (McMahan & Streeter, 2010) Let Q ∈ Sn+ with A = Q 2. Let F be a convex set,
and let u1,u2 ∈ Rn with xι = PF(U 1) and x2 = PF(U2),then
∣∣A(x2 - X1)∣∣ ≤ ∣∣A(u2 - U1)||	(21)
Proof. Define:
B(X, U) = 21∣A(X - u)∣∣2 = I(X - U)TQ(X - U)	(22)
we can write as:
Xi = argmin B(x,ui)	(23)
x∈F
Then, note that VxB(x,u1) = Qx - Qu1, and so we must have (Qxi - QUI)T(x2 - Xi) ≥ 0;
otherwise for δ sufficiently small the point xi + δ(x2 - xi) would belong to F (by convexity) and
would be closer to ui than xi. Similarly, we must have (Qx2 - Qu2)τ(x1 - x2) ≥ 0. Thus
(Qxi - QUi)T(x2 - Xi) - (Qx2 - Qu2)τ(xi - X2) ≥ 0
-(X2 - Xi)τQ(x2 - Xi) + (U2 - Ui)τQ(x2 - Xi) ≥ 0
(U2 - Ui)>Q(x2 - xi) ≥ (x2 - xi)>Q(x2 - xi)
Letting U = u2 - ui, and X = x2 - xi, we have XτQX ≤ UτQX. Since Q is semi-definite, we have
(U — X)τQ(U — X) ≥ 0, or equivalently UτQU + XτQX — 2XτQU ≥ 0. Thus
UτQU ≥ -XτQX + 2XτQU ≥ -XτQX + 2XτQX = XτQX	(24)
and so
∣∣A(u2 - ui)∣∣2 = UτQU ≥ XτQX = ∣∣A(x2 - xi)∣∣2	(25)
□
B Proof of Theorem 3.1
Proof. Let x* = arg minx∈j- PT=I ft(x), exist and F is feasible convex set. We have
Xt+i = PFW(xt - ηtv-1 Θ mt) = min 用/4 Θ (x -(Xt- ”12 Θ mt))∣∣	(26)
x∈F
Applying Lemma A.1 we have:
∣∣C/4 Θ (xt+i - x*)∣∣2 ≤ ∣∣C/4 Θ (Xt - ηtv-1 Θ mt - x*)∣∣2
=∣∣vy4 θ (Xt- x*)∣∣2 + η2∣∣v-4 θ mt∣∣2 - 2ηthmt,xt - x*i	(27)
=∣∣vi/4θ(Xt-X*)∣∣2 + η2∣∣v-4 θmt∣∣2
-2ηthβimt-i + (1 - βi)gt,xt — x*i
That is
hgt,χt - x*)
≤ 2ηt(1 - βι)
Mvy4 Θ 3+1 - χ*)∣∣2 Tlvy4 Θ (Xt- χ*)∣∣2]
ηt
2(1 - βι)
llv-1/4
Θ mt∣∣2 +
βι
1 - βι
{mt-ι,xt -
x*)
+
1
≤ 2η⅛y[Hv1/4 θ (Xt+i- x*)|12-1|v1/4 θ (Xt- x*)112]
+
2(⅛)l l v-1/4 θ m ll2 + 2(1⅜) ηtl l v-1/4 θ mt-ι"2 +	l l vi1/4 θ (Xt- X
*
)l l 2
13
Under review as a conference paper at ICLR 2020
We rearrange the Equation 27 to have first inequality. And the second inequality follows the Cauchy-
Schwarz and Yong,s inequality. Next, we have
T	T
X ft(χt) - ft(χ*) ≤ Xhgt,χt- x*)
t=1	t=1
T
≤ X{ 9z11 zo√∣∣v1/4 ® (xt+1 - x*)||2 - ||v1/4 Θ (Xt- x*)||2] +	ηt	||v-1/4 Θ mt||2 (28)
=2ηt(I - βI)	2(I - βI)
+ 2(1⅜) nt||V-1/4 θ m j||2 + 2ηt(β- βι) U"' θ (Xt- x*)||2}
The first inequality is due to convexity of function f. To bound the above regret, we need the
following intermediate result.	□
Lemma B.1. For the parameter settings and conditions assumed in Theorem 3.1, we have:
T
Xηt∖∖v-1/4 © mt∣∣2 ≤
t=1
n(i - βι)√ι + iogT
(i + βι )(1 - γ)p(1 -伪)
d
£ ∖∖gLT,i∖∣2
i=1
(29)
Proof. To prove, we need
T	T-1 X nt∖∖v-1/4 © mt∖∖2 = X nt∖∖v-1/4 G	D mt
t=1	t=1	
T-1	
=X nt∖∖v-1/4 G	D mt
t=1	
T-1	
=X nt∖∖v-1∕4α	D mt
t=1	
T-1	
≤ X nt∖∖v-1∕4α	mt
t=1	
T-1	
=X nt∖∖v-1∕4α	mt
t=1	
T-1	
=X nt∖∖v-1∕4G	mt
t=1	
T-1	
=X nt∖∖v-1∕4G	mt
t=1	
T-1	
≤ X nt∖∖v-1∕4G	mt
t=1	
T-1	
=X nt∖∖v-1∕4G	mt
t=1	
T-1	
≤ X nt∖∖v-1∕4G t=1	mt
2
2 + X ((1- β1) PT=I βT-"i)2
i=1 PT(β2vt-1,i + (1 - β2)gt,i)
d
+ n(1-βι)2 X
i=1
d
+ n(1-βι)2 X
i=1
(PT=I βT-%∙,i)2
p∕τ(β2vt-1,i + (1 - β2)gt,i)
PT=1 β2(T -j
PT=ι j
τ(β(β2vt-1,i + (1 - β2)gt,i)
2 + n(I - β1)(I - β2T) X P = I g2,i
1 + β1	i=1 ∕β(β2vt-1,i + (1 - β2)gt,i)
2 + n(I - β1)(I - β2T) X _______P=I j__________
1+βι	i=ι q (1-β2) PT=I βT-j j
2 + n(1 - β1)(1 - β2T) X	Pj=1 gj,i
(1+ βι)√β (1-β2)	丘 T=ι βT-j j
2+
n(1- β1)(1- β2T)
(1 + β1) ∕τ (1 - β2)
dT
XX
i=1j=1
√βji
2 + n(1 - β1)(1 - β2T) X X T-j gj,i
(1 + β1)∕β∏二两 匕 士 石
2+
n(1- β1)(} - β2T)
(1 + β1) /∕τ(1 - β2)
dT
XX YT-jM∖
i=1 j=1
2
2
The first inequality follows from Cauchy-Schwarz inequality. We let Y = 1/√β2. The last inequal-
ity due to the Algorithm 1 that gj,i = max{gj,i, Vj-1,i}. By using similar upper bounds for all time
14
Under review as a conference paper at ICLR 2020
steps, the quantity in above can further be bounded as follows:
T
X l|v-1/4
t=1
T
Θ mtll2 = X
t=1
η(i- βι)(i- β2τ)
(1 + β1)pt(I -e2)
d t
XX YTMI
i=1j=1
η(i - βι)
(1 + β1)p(I - β2)
i- β2t
√t
T
X
t=1
dt
XX YTMI
i=1j=1
η(I - βI) X X I	I X (I - β2j)γj-1
(1 + β1)pΓ≡β) L ⅛ lgt,i l ⅛ 一√j—
η(i - β1)
(1 + βI)P(I - β2)
dT	t
XX EiIX
i=1t=1	j=1
________η(i - β1)_________
(1 + β1)(I - Y)p(I - β2)
dT	t
X XlY 十
< ______η(I - βI)_____ X Il Il X 1
-(1 + β1)(i - Y)√(T-127 ⅛1	τ,i 2∖⅛1t
η(1 - β1)√1+logT
(1 + β1)(I - Y)√(1 - β2)
d
X ll9LT,ill2
i=1
The first inequality due to the fact that 1 - β2j < 1 and PT=1 Yj-t ≤ 1/(1 - y). The second
inequality is due to simple application of Cauchy-Schwarz inequality. The final inequality is due to
the following harmonic sum bound with PT=I 1/t ≤ (1 + logT).	□
Now we return to the proof of Theorem 3.1, by using above lemma, we have:
Eft(Xt)- ft(x*) ≤ Ehgt,xt —x*〉
t=1
T
≤ X{
t=1
t=1
2η(]1- β]) [llv1" θ (xt+1 - x*)ll2 - llv1/4 θ (xt - x*)ll2]
β1
2ηt(1 - β1)
IlvJ4 Θ (xt-x*)ll2} +
η √1 + log T
(1 + β1)(I - Y)VZ(I - β2)
d
E IIg1：T,i ll2
i=1
≤
≤
T
T
+
T
2ηrallv1/4 θ (xi*)ll2 + τ→7 X 2≠r θ (xt-x*)ll2
1T
+ 2(1- β1) X(
Ilv1/4 θ (xt - x*)ll2 Ik-： Θ (xt - x*)ll2ι
--------------------------------------------}
η √1 + log T
ηt
d
ηt-1
(30)
(1 + β1)(I - Y)，(I - β2)
y? llg1：T,ill2
i=1
1
2η1(1 - β1)
d
X v1∕i2(
i=1
Td
(x1 i — x* )2 +----------
(1,i ,i) + 2(1 - β1)
X X v；/2(xt,i- x*i)2
=⅛1	ηt
+ 2(1- β1)
∑∑(xt,i -吟
t=2 i=1
v1/2	Ilv1/2
、2 ʃ vt,i	llvt-1,i1
) (Ir - It-T}
η √1 + log T
(1 + β1)(I - Y) V(1 - β2)
d
£ llgLT,ill2
i=1
+
+
1
15
Under review as a conference paper at ICLR 2020
For the sake of clarity, we further suppose that the feasible region of parameter x with bound D2,
the Equation 30 becomes to:
T
Xft(xt)- ft(x*) ≤
t=1
D2	X v1/2 + D	X X v/_
2ηι(i- βι) ± 1'i +2(1- βι) ±士 ηt
+
D22	T d vt1,/i2
21F X X {Fr -
MI-1,i}
ηt-ι ʃ
(31)
+
______η √ι + log T_______
(I + βI)(I - Y)P(I - β2)
d
||g1:T,i||2
i=1
Therefore, we finish the proof of Theorem 3.1 as desired.
16