Under review as a conference paper at ICLR 2020
Cyclic Graph Dynamic Multilayer Perceptron
for Periodic Signals
Anonymous authors
Paper under double-blind review
Ab stract
We propose a feature extraction for periodic signals. Virtually every mechanized
transportation vehicle, power generation, industrial machine, and robotic system
contains rotating shafts. It is possible to collect data about periodicity by mea-
suring a shaft’s rotation. However, it is difficult to perfectly control the collection
timing of the measurements. Imprecise timing creates phase shifts in the resulting
data. Although a phase shift does not materially affect the measurement of any
given data point collected, it does alter the order in which all of the points are col-
lected. It is difficult for classical methods, like multi-layer perceptron, to identify
or quantify these alterations because they depend on the order of the input vectors’
components. This paper proposes a robust method for extracting features from
phase shift data by adding a graph structure to each data point and constructing
a suitable machine learning architecture for graph data with cyclic permutation.
Simulation and experimental results illustrate its effectiveness.
1	Introduction
Understanding what phenomena, a rotating shaft is experiencing is critical for machine health mon-
itoring. From industrial manufacturing equipment, transportation systems, to consumer products,
rotating shafts are in many mechanical devices. Many issues such as long-term fatigue, wear related
issues, and acute failures can cause symptoms that are detectable from the shaft. The effort and
flow variables associated with the shaft are desirable state variables to measure in nearly all these
cases mentioned. Although these physical variables may provide useful information for detecting
anomalies and estimating symptoms, one should extract features hidden in these signals. Therefore,
an efficient feature extraction method plays an important role in anomaly detection and symptoms
recognition.
Deep learning networks achieved remarkable results compared to the traditional methods. The time-
frequency analysis, such as the short-time Fourier transform Xie et al. (2012) and the wavelet trans-
form YanPing et al. (2006); Al-Badour et al. (2011), are well known feature extraction methods.
For example, one can detect a certain bending mode by paying attention to the resonance frequency.
Namely, domain knowledge expertise is needed to extract features from a time-frequency represen-
tation associated with particular phenomena. Some deep convolutional neural network (CNN) ar-
chitectures achieved good results by taking time-frequency images as inputs Verstraete et al. (2017);
Guo et al. (2018). Although many machine learning methods with preprocessing schemes were
used to extract signal features, many of them do not really consider the specific characteristics of
signals. For example, the output from a general CNN is compressed by pooling regardless of time
or frequency direction.
A method that considers the relative order information of signals is necessary. Classical methods,
such as multi-layer perceptron (MLP), regard signals as vectors and accordingly use vectors as in-
puts. However, a vector does not give relationship information that might exist between coordinates.
Therefore, while classical methods can measure data points, it is difficult to detect whether they are
in proper order relative to each other. This could occur due to pooling even if the signal is converted
into an image by some method and input to the CNN. The relative order is very important to classify
them. For example, the data obtained from a rotating machinery is periodic as in Figure 5. Figure
5a shows noise associated with the rotation period, and so it is related to a rotation anomaly such
as a crack in a gear. However, Figure 5b shows noise that is different from the rotation period, and
1
Under review as a conference paper at ICLR 2020
thus there is a possibility that it is not related to rotation, but perhaps an abnormality of the sensor
system. Even with a classical method, it is possible to classify them if such data are included in the
training data. However industrial machines would require significant time and cost to run the nec-
essary experiments for collecting data. If the abnormality to be detected was rare, then the required
effort would be magnified.
The proposed method to solve this problem, considers a graph structure for each data point. This
scheme provides a relative order information about the vector coordinates. It then applies a graph
neural network, such as Atwood & Towsley (2016); Kipf & Welling (2017), to the graph structured
data. For example, the relative information can be obtained by calculating cross-correlations be-
tween the points. Since some deep learning approaches achieved results by concatenating different
numerical sequences such as different sensor signals and treating them as inputs, we can concate-
nate the original sensor signal with the cross-correlations. This defines the relative information, and
treat it as an input. However it is not natural to treat them in the same way by simply concatenating
them because the sensor signal represents a physical value and the cross-correlation represents their
relationship. Hence, the essential meaning is different. We deal with these different values simul-
taneously by using a graph structure that represents each point and their relationships. Then, the
obtained graph data is fed to a graph neural network for feature extraction. This enables the system
to learn by focusing on the relative relationship of each coordinate of the data point.
The main reason for using a graph structure is to give data additional information. The time-
frequency representation simply converts the original signals to other forms. Inspired by the success
of CNN in computer vision, Wang & Oates (2015); Zhu et al. (2019) proposed encoding time series
as different types of images using methods other than time-frequency analysis and inputting them
into CNN. Umeda (2017) proposed a method of converting the original signals to high dimensional
data cloud. While they are all categorized as information conversions, conversions meaning that they
do not add any other information, we add relationship information as edges to the original signals,
thus the graph hold richer information than the original signal and its conversions.
A key feature of the method is phase shift invariance. The application of our current research is for
industrial machines with rotating shafts. Virtually every mechanized transportation vehicle, power
generation, industrial machine, and robotic system contains rotating shafts. The shafts provide an
opportunity to collect periodic signals. In practice, most measuring instruments such as sensors,
processors and loggers along with their data acquisition systems show time delay respectively in the
availability of the data. There is a limitation to correct the delays by hardware design or implemen-
tation. Hence, phase shifts may occur in the obtained periodic signals. However, these phase shifted
signals are essentially the same. We identify them using a shift invariance method.
Our proposed method performs a cyclic permutation to a graph neural network. This method assures
that the results account for phase shift of the periodic measurements. It is not necessary to consider
the vertex order in the graph originally, but it is necessary to give the order for computability. Here,
we identify the graphs whose vertex orders are different due to phase shifts. The conventional graph
neural networks regard them different. Therefore, we propose a method that intentionally focuses
on shift invariance by acting a cyclic permutation to a graph neural network. The use of this method
in Section 3 shows that it offers predictions with sufficient accuracies for idealized data and the
experimental data obtained from a test setup Gest et al. (2019).
2	Machine Learning Methodology
In this section, we define our machine learning method. First we introduce necessary terminology
to review the graph theory. Then we describe the method of constructing a graph structure for each
data point, and introduce the learning model corresponding to the graph data. Finally we extend the
learning model to periodic data.
2.1	Notation and Terminology of Graphs
The notation and terminology of graphs is as follows. Let G be a graph. We denote the vertex set
and the edge set of G by the symbol V (G) and E(G). A simple graph is a graph containing no
graph loops or multiple edges. A complete graph is a graph in which each pair of graph vertices is
connected by an edge. An ordered graph is a graph with a total order over its vertices. If a graph
2
Under review as a conference paper at ICLR 2020
G is ordered with |V (G)| = N, then we regard V (G) as an ordered set {vi}1≤i≤N. The adjacency
matrix A(G) of a simple ordered graph G is a matrix with rows and columns labeled by graph
vertices, with a 1 or 0 in position (vi, vj) according to whether the vertices vi and vj are connected
by an edge or not, respectively. For a graph G, a vertex labeling is a function from V (G) to a set of
labels. A graph with such a function defined is called a vertex-labeled graph. For a vertex-labeled
graph G, we denote the vertex labeling by the symbol L(G). Unless otherwise stated we assume
that graphs are simple ordered graphs labeled by real numbers.
2.2	Construction of Graph Structure
Now we construct a graph structure on each data point. Let x be a data point, namely, x be an N-
dimensional real vector (x1, x2, . . . , xN) for some integer N. For simplicity of notation, we write
(xi)1≤i≤N instated of (x1, x2, . . . , xN). Fix integers ws and ss with 1 ≤ ws, ss ≤ N, we call
these a window size and a slide size. Let [∙J be the floor function, namely baC := max{dj ∈ Z ∣ n ≤
a}. Then we obtain the following N0-length sequence of ws-dimensional real vectors {vi}, where
N0= b(N - ws)/ssc +1.
Vi = (x(i— 1)*ss + 17 x(i— 1)*ss+2, . . . , χ(i— 1)*ss+Ws ) with 1 ≤ i ≤ N0
We fix a real number ε and a distance function d on the ws-dimensional real vector space, such as
the Euclidean distance or the correlation distance. Then we can define a graph G as follows:
•	V (G) = {vi}1≤i≤N 0,
•	E(G) = {(vi,vj) | vi,vj ∈ V (G) with d(vi,vj) < ε},
•	L(G) = Proj : Vi 1 x(i— 1)*ss+Ws.
If the slide size and the window size are small enough, then maximum, minimum and mean work
the same as a projection for the labeling function. However, in our case, vertices should be at least
one period sub-waves because we construct edges by their similarity. Then max and min become
constant values for every vertex. Also, periodically the window size becomes equal to a constant
multiple of the period because our experimental data consists of many different frequencies. Then
the mean becomes the same value for every vertex. Therefore, we use the projection as a vertex
labeling in this paper.
2.3	Learning Model for Graph Data
Here we define our machine learning model suitable for graph data set. Our model draws inspiration
from recent work on a graph neural network (Kipf & Welling, 2017). However, they consider the
problem of classifying vertices of a graph by sharing filter parameters for each vertex and treat all
vertices equal. On the other hand, since we consider graph-wise feature extraction and the vertices
of our graph have time information as index, we think that it is not suitable to treat the vertices in
the same way. Therefore, we avoid weight sharing by using the Hadamard product as shown below.
Set an integer N . Let GN be a set of graphs with N -vertices. Set an integer M, which is a number
of hidden layers. We take a finite sequence of N -by-N matrices {Wm}0≤m≤M, called trainable
weights, and a finite sequence of N -dimensional real vectors {bm}0≤m≤M, called trainable bias.
Then we define a function φ from GN to RNas follows:
φ(G) = LM+1 ,
where, L0 is the image of the labeling function L(G)(V (G)),
Lm +1 = T (Lm ∙ ^yi(G) O Wm^ + bm∙
Here, √i(G) = A(G) + IN with the rank N identity matrix IN and T is an activation function, such
as the ReLU, and ∙ means the matrix product and o means the Hadamard product. Graph neural
networks usually use the Laplacian as A(G), but our experimental results were almost the same, so
We used A(G) = A(G) + IN instead. This turned out to be simpler than the Laplacian.
This function can be regarded as a natural extension of a multi-layer perceptron (MLP) obtained
by admitting a graph structure on each data point. In fact, our function on the complete graph G,
namely all the elements of A(G) are 1, is equal to a MLP. Hence We call this function a graph
dynamic multi-layer perceptron (GDMLP).
3
Under review as a conference paper at ICLR 2020
Figure 1: Graph dynamic multilayer perceptron (GDMLP).
2.4	Extension of Learning Model to Periodic Data
In this section, we extend the above learning model to another model which is suitable for periodic
data. We consider its application to data obtained from industrial machineries containing rotating
shafts. Accordingly each data point x = (xi)1≤i≤N obtained from them is periodic defined as
follows. Let T be the sampling interval, that is time between which data is recorded. Then x =
(xi)1≤i≤N is periodic if there exists a period Tx = nxT with a positive integer nx such that xi = xj
if i ≡ j mod nx. On the other hand, it is difficult to perfectly control the collection timing of these
data points because there is a limitation to correct the delays by hardware design or implementation.
To account for this the data includes a phase shift x0 = (x0i)1≤i≤N of x, namely, for periodic data
points x and x0 with a period Tx = Tx0 = nT, there is a delay T(x,x0) = n(x,x0)T with an integer
n(x,x0) such that xi = x0j-n(x,x0) if i ≡ j - n(x,x0) mod n. A period and a delay are multiples
of the sampling interval T . This is a practical limitation and not a theoretical limitation. In general,
sensor measurements are discrete in time. The sampling interval T is the minimum interval when the
sensor actually measures. Although we use this notation to clarify this practical limitation, sampling
interval T is not a limitation in the theoretical claim (see the appendix for details).
(a)	(b)	(c)
Figure 2: (a): Two waves with a phase shift. The above wave x represented by Xt = Sin 篙 + 0.3 Sin 1j6∏t
with 0 ≤ t ≤ 140 and the below x0 is phase shifted wave of x with 30 delay. (b): The graphs
obtained from the left waves by the operation in Section 2.2 with ws = 20, ss = 15, euclidean
distance as d and ε = 4.5. (c): The subgraphs of the left graphs whose are equivalence up to cyclic
order of vertices.
Although phase shift data points are different as vectors, the graphs obtained from them by the
operation in Section 2.2 have “large” equivalent induced subgraphs up to cyclic order of vertices. To
state more precisely we make the following definition; pick a N -vertices graph G. Let {vi}1≤i≤N
be a vertex set V (G). Let σ be a cyclic permutation on V (G) such that σ(vi) = vi+1. Then the
cyclic permutated graph σ(G) of G is defined as follows:
•	V (σ(G)) = {σ(vi)}1≤i≤N,
•	E(。(G)) = {(。(Vi),σ(Vj)) I (Vi, Vj) ∈ E(G)},
•	L(σ(G)) : σ(vi) 7→ L(G)(vi).
4
Under review as a conference paper at ICLR 2020
Let x and x0 be phase shift data points. They are different as vectors (see Figure 2a). Hence the
graphs Gx , Gx0 obtained from them by the operation in Section 2.2 are not equivalent as ordered
graphs (see Figure 2b). However, most of them produce identical vertices to each other up to cyclic
order (see Figure 2c). In fact we can prove the following claim.
Claim . Let x and x0 be phase shifted data points with period Tx = Tx0 = nT, and delay T(x,x0) =
n(x,x0)T. Assume that x and x0 is more than 3 periods, namely, |x| = |x0| ≥ kn for an integer
k ≥ 3. Then there exists a window size ws and a slide size ss satisfying the following condition.
For graphs Gx and Gx0 which obtained from x and x0 by the operation in Section 2.2, there exists
an integer K(x,x0) such that both of Gx and σK(x,x0) (Gx0) have a induced ordered subgraph S(x,x0)
satisfying |V (S(x,x0))| ≥ |V (Gx)|k/(k + 1) = |V(Gx0)|k/(k+1).
We provide a proof of the claim in the appendix. By the claim, it is expected that the GDMLP
outputs of Gx and σK(x,x0) (Gx0) will be approximately the same.
For the above reason, we improve a GDMLP with a cyclic order as shown in Figure 3. We fix a
GDMLP model φ on GN . Then we define a function Φφ from GN to RN as follows:
Φφ(G)=ρ(φ(G),φ(σ(G))),φ(σ2(G)),...,φ(σN-1(G)),	(1)
where, ρ is a pooling function, such as the average-pooling. We call this function a cyclic graph
dynamic multilayer perceptron (CGDMLP).
CoPy the original graph
with cyclic order of vertices.
IGDMLPl
IGDMLPl-►
pooling
Figure 3: Cyclic graph dynamic multilayer perceptron (CGDMLP).
U
2.5	Learning Architecture
Since a GDMLP is a feature extraction of the graph vertex labels, it is necessary to compose a
function with a GDMLP according to the final processing to be performed, such as a classification
or a regression. Our main purpose is to extract similar features from phase shifted signals. In
our experiments, we confirm the performance of the proposed feature extraction by checking the
difference in accuracy with others. And this is independent of the classifier used. Therefore we
compose a MLP with a GDMLP, which is a simple method. The MLP input layer consists of
the same number of perceptrons as the dimension of the GDMLP output. Its output layer and the
hidden layer is optimized according to the sophistication of the problem, such as the number of
classifications desired. Similarly, in the case of a CGDMLP, we compose tailored a MLP according
to the output desired (see Figure 4). For simplicity of notation, we use the same letter GDMLP
and CGDMLP for the learning architecture whose feature extraction part are the feature extraction
GDMLP and CGDMLP respectively.
5
Under review as a conference paper at ICLR 2020
Graph Feature Extraction MLP	output
-►I CGDMLP"∣
Data point
Convert to graph
by Operation in Section 3.2.
Figure 4: Learning Architecture.
3	Experiments
We now give an example where we compare our method for analyzing periodic data.
3.1	Ideal Data
First, we apply our method to idealized data that abstracts the problem we are considering, such as
a crack in a shaft, sensor malfunction, or external force that would impede mechanical rotation. The
first of these would typically cause periodic noise, and the last two non-periodic noise. Reports of
noises and their recurrence from a sensor, then, can be used to diagnose each of those and other
issues. Our proposed method, which adds a graph structure to each data point to give a relative in-
formation, should effectively make such a diagnosis. Also, because our proposed CGDMLP is with
a focus on cyclic order invariance, it is expected that phase shift signals can be identified because
they produce approximately the same outputs in either unshifted or differently shifted signals.
Based on the above, we compare the classification result of each method for noisy sine waves
(xt )1≤t≤T with some integer T defined as follows:
xt = sin(2πf (t/T - t0)) + +δf,
where f is the frequency, t0 is a phase shift, is a random noise from a uniform distribution over the
half open interval [-0.05, 0.05) and δf is periodic or non-periodic noise (see Figure 5).
Figure 5: (a): Periodic noise, (b): Non-periodic noise.
Set the frequency f to three types, 3, 6 and 9. We then execute (xt)1≤t≤T over the range of six
times, once per each combination of frequency and periodic vs. non-periodic noise. Verification is
performed with and without phase shift t0. The validation set size for each result is fixed at 100. We
train with several training set sizes, as shown in Table 1.
The architecture used for comparison is the architecture GDMLP and CGDMLP defined in Section
2.5. In addition, we use the architecture MLP and CMLP which are fully-connected networks ob-
tained from GDMLP and CGDMLP by replacing all the elements of A(G) with 1 respectively. We
set the learning rate based on the LR range test (see Section 3.3 in Smith (2017)).
The results are summarized in Table 1. Our main purpose is to extract similar features from phase
shifted signals. We focused on the difference in accuracy with other methods. In the middle and
the bottom of Table 1 including phase shifted signals, when the training set size per class is 10, the
difference in accuracy is higher than 30 %. It indicates that the proposed method can be performed
with a small experimental dataset with phase shifts. On the other hand, the bottom third of Table 1
shows that the training data does not include phase shifted signals, however the proposed method
performs acceptable with the validation set which includes unknown delays in signals. This could
be considered as generalization performance when applied to phase shifted signals.
6
Under review as a conference paper at ICLR 2020
Table 1: Validation set accuracy for simple periodic data.
Training Set Size Per Class	Training Phase Shift: None, Validation Phase Shift: None			
	MLP	CMLP	GDMLP	CGDMLP
10	5128	74:32	7780	80:70
50	55:92	7917	8357	9148
100	57.78 —	84.97 —	86.62 —	92.73 —
Training Set Size Per Class	Training Phase Shift: Exist, Validation Phase Shift: Exist			
	MLP	CMLP	GDMLP	CGDMLP
10	4847	4737	50:22	8133
50	54!6	8800	68:39	92:00
100	58.89 —	84.89 —	70.50 —	94.72 —
Training Set Size Per Class	Training Phase Shift: None, Validation Phase Shift: Exist			
	MLP	CMLP	GDMLP	CGDMLP
10	27:06	52.83	32.89	89.89
50	26∏	86:39	33:89	9100
100	26.33 —	88.28 —	33.89 —	94.39 —
3 .2 Validation with Real Data
3.2.1	Test Setup
In order to validate our model on a real-world dataset, we use data obtained from the test setup
Gest et al. (2019) as shown in Figure 6. The shaft is attached at both ends to brushed DC motors
by compliant couplers. One motor, the driving motor, is connected to a power supply and electronic
speed control and is controlled by a computer. The controls permit electrical current to the motor at
any of five discrete voltage levels. The second motor, the damping motor, is attached to a resistor
array to create a variable rotary damper. In the resistor array, relays are used connect and bypass
individual resisters. The possible combinations allow for sixteen discrete levels of resistance. Five
different weights of different masses and sizes are attached to the shaft to simulate the shaft bending,
a possible real-world anomaly. Tests are also run and data collected with no weights attached. The
sensors mounted on the shaft collect triaxial acceleration and strain, and audio data is collected with
an external microphone. We collect a total of 400 cases of data from the above discrete variables.
Each case consists of 50 data points. Each data point consists of 1.5 seconds signals. This time
width is set to collect at least three rotations at the minimum rotation speed of 120 rpm. Due to the
computational cost, each data point is downsampled to consist of 150 points.
Figure 6: Experimental test setup.
3.2.2	Construct Models for the Test Setup Data
To confirm the performance of the proposed feature extraction, we construct classification models to
estimate the shaft bending. We use the measurements of tangental acceleration, radial acceleration
and strain, as input by permutation importance (see Altmann et al. (2010), Breiman (2001)).
7
Under review as a conference paper at ICLR 2020
The basic architecture is the one defined in Section 2.5, but this time there are three types of sensor
signals used for input, so each feature is extracted in parallel and the outputs concatenation is input
to the multi-layer perceptron, which is a classifier (see Figure 7).
As in Section 3.1, for comparison we use GDMLP and CGDMLP, and MLP and CMLP which are
fully-connected networks obtained from GDML and CGDMLP respectively.
In the preprocessing part each of the above three sensors data are converted to the graph data by the
operation introduced in Section 2.2 with the following parameters. The window size ws is set to 50,
which is set to include at least one period at minimum rotation speed of 120 rpm. The slide size ss
is set to 3, which is set to be smaller than one period at the maximum rotation speed of 1200 rpm.
We set the distance function d the correlation distance and ε = 0.3, namely, a pair of vertices are
connected if they have a strong positive linear relationship.
We set the learning rates in this instance based on the LR range test as in Section 3.1 and we use a
grid search optimization method for other parameters.
Preprocessing	Feature Extraction	Classification
Convert to graph
by Operation in Section 3.2
Figure 7: Learning Model for the Test Setup Data.
3.2.3	Results
Table 2 shows the resulting accuracy of each method. There are only 60 samples in each case of the
experimental data. For this limited data, our proposed method achieves sufficiently higher accuracy
than either of the based methods MLP and CMLP. Of the proposed methods CGDMLP in particular
achieves the highest accuracy. We suggest that our methods would be superior even with limited
data since relative order information on the time axis was assigned here. We further suggest that
CDGMLP, especially, would be superior even when evaluating data sets that include phase shift
because our proposed CGDMLP is with a focus on cyclic order invariance.
Table 2: Result of each classification method.
Model	Accuracy (%)
-MLP-	7348
-CMLP-	7162
GDMLP	80:96
CGDMLP	87.73 —
4 Conclusion
In this paper, we proposed a machine learning method for analyzing periodic data by admitting
a graph structure to each data point and constructing a machine learning model according to the
characters of the graph structure and the original data. Another point of importance is that adding a
certain structure to data is shown to be very effective for feature extraction. The paper demonstrates
experimentally the effectiveness of adding a graph structure to the data.
8
Under review as a conference paper at ICLR 2020
References
F. Al-Badour, M. Sunar, and L. Cheded. Vibration analysis of rotating machinery using time-
frequency analysis and wavelet techniques. Mechanical Systems and Signal Processing, 25
(6):2083 - 2101,2011. ISSN 0888-3270. doi: https:〃doi.org∕10.1016∕j.ymssp.2011.01.017. URL
http://www.sciencedirect.com/science/article/pii/S0888327011000276.
Interdisciplinary Aspects of Vehicle Dynamics.
Andre Altmann, Laura Toloyi, Oliver Sander, and Thomas Lengauer. Permutation im-
portance: a corrected feature importance measure. Bioinformatics, 26(10):1340-
1347, 04 2010. ISSN 1367-4803. doi: 10.1093/bioinformatics/btq134. URL
https://doi.org/10.1093/bioinformatics/btq134.
James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Proceedings
of the 30th International Conference on Neural Information Processing Systems, NIPS’16,
pp. 2001-2009, USA, 2016. Curran Associates Inc. ISBN 978-1-5108-3881-9. URL
http://dl.acm.org/citation.cfm?id=3157096.3157320.
Leo Breiman. Random forests. Machine Learning, 45(1):5-32, 2001. doi: 10.1023/a:
1010933404324. URL https://doi.org/10.1023/a:1010933404324.
E. Gest, M. Furokawa, T. Hirano, and K. Youcef-Toumi. Design of versatile and low-cost shaft
sensor for health monitoring. In 2019 International Conference on Robotics and Automation
(ICRA), pp. 1926-1932, May 2019. doi: 10.1109/ICRA.2019.8794408.
Sheng Guo, Tao Yang, Wei Gao, and Chen Zhang. A novel fault diagnosis method for rotating
machinery based on a convolutional neural network. Sensors, 18:1429, 05 2018. doi: 10.3390/
s18051429.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolu-
tional networks. In 5th International Conference on Learning Representations, ICLR
2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. URL
https://openreview.net/forum?id=SJU4ayYgl.
Leslie N. Smith. Cyclical learning rates for training neural networks. 2017 IEEE Winter Conference
on Applications of Computer Vision (WACV), pp. 464-472, 2017.
Yuhei Umeda. Time series classification via topological data analysis. Transactions of the Japanese
Society for Artificial Intelligence, 32(3), 2017.
David Verstraete, Andres Ferrada, Enrique Droguett, V. Meruane, and Mohammad Modarres. Deep
learning enabled fault diagnosis using time-frequency image analysis of rolling element bearings.
Shock and Vibration, 2017:1-17, 10 2017. doi: 10.1155/2017/5067651.
Zhiguang Wang and Tim Oates. Imaging time-series to improve classification and imputation. In
Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015.
Hang Xie, Jing Lin, Yaguo Lei, and Yuhe Liao. Fast-varying am-fm components ex-
traction based on an adaptive stft. Digital Signal Processing, 22(4):664 - 670,
2012. ISSN 1051-2004. doi: https://doi.org/10.1016/j.dsp.2012.02.007. URL
http://www.sciencedirect.com/science/article/pii/S1051200412000413.
Zhang YanPing, Huang ShuHong, Hou JingHong, Shen Tao, and Liu Wei.	Con-
tinuous wavelet grey moment approach for vibration analysis of rotating ma-
chinery. Mechanical Systems and Signal Processing, 20(5):1202 - 1220, 2006.
ISSN 0888-3270. doi:	https://doi.org/10.1016/j.ymssp.2005.04.009. URL
http://www.sciencedirect.com/science/article/pii/S0888327005000816.
Xiaoxun Zhu, Jianhong Zhao, Dongnan Hou, and Zhonghe Han. An sdp characteristic information
fusion-based cnn vibration fault diagnosis method. Shock and Vibration, 2019, 2019.
9
Under review as a conference paper at ICLR 2020
A Appendix
We give a proof of the claim in Section 2.4. First we recall the claim.
Claim . Let x and x0 be phase shifted data points with period Tx = Tx0 = nT , and delay T(x,x0) =
n(x,x0)T. Assume that x and x0 is more than 3 periods, namely, |x| = |x0| ≥ kn for an integer
k ≥ 3. Then there exists a window size ws and a slide size ss satisfying the following condition.
For graphs Gx and Gx0 which obtained from x and x0 by the operation in Section 2.2, there exists
an integer K(x,x0) such that both of Gx and σK(x,x0) (Gx0) have a induced ordered subgraph S(x,x0)
satisfying|V(S(x,x0))| ≥ |V (Gx)|k/(k + 1) = |V (Gx0)|k/(k + 1).
(By the definition of delay.)
(Here, q is the quotient when n(x,x0) is divided by n.)
(By the definition of periodic.)
Proof. If n = 1, it implies that x and x0 are constant sequences. Thus we assume n ≥ 2.
Let n0(x,x0) be the remainder when n(x,x0) is divided by n. By the definition of periodic and delay,
we have x0 = x0 0 as follows:
n(x,x0)
x0 = x0
0	n(x,x0)
= x0qn+n0(x,x0)
= xn0
(x,x0)
Similarly, we have x00 = xn-n0(x,x0) . Note that n0(x,x0) < n by the definition of the remainder. If
n0(x,x0) ≥ n/2, then n - n0(x,x0) < n/2. Thus, we can assume that there is an integer m(x,x0) <
n/2 such that xm(x,x0) = x00 by replacing x and x0 if needed. Then x and x0 have a continuous
subsequence s such that |s| > (k - 1)n.
Set ws < |s| - km(x,x0) and let ss be a divisor of m(x,x0). Note that we have |s| - km(x,x0) > 1
as follows:
n
网-km(⑦ H) > (k — 1)n — k q
By |s| > (k - 1)n and m(x,x0) < n/2.
By k ≥ 3 and n ≥ 2.
Hence we can define ws satisfying ws < |s| - km(x,x0).
Since ss is a divisor of m(x,x0), for each i > m(x,x0)/ss, the vertex vi of Gx is equivalent to some
vertex vj0 of Gx0 . Let S(x,x0) be the equivalent induced subgraphs consists of the above vertices.
Since the graph Gx is obtained by the operation in Section 2.2 and ss is a divisor of m(x,x0), we have
|V (G)| = |S(x,x0)|+m(x,x0)/ss and |S(x,x0)| = b(|s| -ws)/ssc. Since we set ws < |s|-km(x,x0)
and ss is a divisor of m(x,x0), we have |S(x,x0) | ≥ bkm(x,x0) /ssc = km(x,x0) /ss. Then we have
|S(x,x0)|/|V(G)| ≥ k/(k + 1) as follows:
|S(x,x0)|/|V(G)|
IS(吗 *，)I______
IS ( χ,χ0 ) 1 + m ( χ,χ0 ) / ss
(By IV (G)I = IS(x,x0) I + m(x,x0) /ss.)
m(x,x0)/ss
=1 — T-:------:----------； 
IS(x,x0 ) I + m(x,x0 ) /ss
m(x x0)/ss
≥ 1 - I----------------------T- (By IS(χ,χ0)I ≥ km(χ,χ0)/ss.)
km(x,x0)/ss + m(x,x0)/ss
_ k
=k + 1,
Then m(x,x0)/ss is the desired integer K(x,x0).
□
10