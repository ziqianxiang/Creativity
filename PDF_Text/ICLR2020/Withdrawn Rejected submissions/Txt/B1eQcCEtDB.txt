Under review as a conference paper at ICLR 2020
Calibration, Entropy Rates, and Memory in
Language Models
Anonymous authors
Paper under double-blind review
Ab stract
Building accurate language models that capture meaningful long-term dependen-
cies is a core challenge in natural language processing. Towards this end, we
present a calibration-based approach to measure long-term discrepancies between
a generative sequence model and the true distribution, and use these discrepancies
to improve the model. Empirically, we show that state-of-the-art language models,
including LSTMs and Transformers, are miscalibrated: the entropy rates of their
generations drift dramatically upward over time. We then provide provable meth-
ods to mitigate this phenomenon. Furthermore, we show how this calibration-
based approach can also be used to measure the amount of memory that language
models use for prediction.
1	Introduction
Recent advances in language modeling have resulted in significant improvements on a wide vari-
ety of benchmarks (Dai et al., 2018; Gong et al., 2018; Takase et al., 2018). Capturing long-term
dependencies has especially been a major focus, with approaches ranging from explicit memory-
based neural networks (Grave et al., 2016; Ke et al., 2018) to optimization improvements to stabilize
learning (Le et al., 2015; Trinh et al., 2018). However, while these techniques seem to improve on
standard metrics like perplexity and even produce remarkably coherent text (Radford et al., 2019),
we still do not have appropriate measures to assess long-term properties in language models, making
it difficult to choose between different model options for downstream tasks.
Starting from Shannon’s seminal work that essentially introduced statistical language modeling
(Shannon, 1951), the most classical and widely studied long-term property of a language model
is its entropy rate — the average amount of information contained per word, conditioned on the
preceding words. A learned model provides an upper bound for the entropy rate of a language, via
its cross-entropy loss. The exponential of the entropy rate can be interpreted as the effective support
size of the distribution of the next word (intuitively, the average number of “plausible” word choices
to continue a document), and the perplexity score of a model (the exponential of the cross entropy
loss) is an upper bound for this quantity. In state-of-the-art models trained on billion-scale corpora,
this number ranges between 10 and 30 (Melis et al., 2017; Radford et al., 2019). A natural diagnos-
tic question, with which we begin our work, is whether the long-term generations of these models
exhibit the same entropy rates as the underlying languages they are modeling predictively.
Model	Corpus	Test ppl.	eEntRate-
-AWD-LSTM (Merity et al., 2017)-	-PTB-	58.3	93.1-
CNN-LSTM (JozefoWiCz et al., 2016) 一	-GBW-	29.8	49.4
Transformer (VasWani et al., 2017b)	GBW	28.1	34.7-
GPT-2 (Radford et al., 2019)	-	WebText	23.7	61.2
Table 1: Perplexity degradations for generations from popular language models. State-of-the-art
performance is usually reported via perplexity with respect to the test corpus (one-step prediction
loss), but there is a striking blowup in the perplexity (i.e. exponential of the entropy) of these models’
long-term generations. Test ppl. is the exponential of the cross-entropy of the model with respect
to the test corpus.
1
Under review as a conference paper at ICLR 2020
Empirically, and perhaps surprisingly, it turns out that the entropy rate of generated text is substan-
tially higher than the estimate for true text derived from the model’s one-step predictions. As seen in
Table 1 (see also Figure 1), this is true for both state-of-the-art LSTMs and Transformers trained on a
variety of datasets. As a timely example, the GPT-2 model (Radford et al., 2019), the object of much
recent attention for its seemingly coherent and on-topic generations, suffers a dramatic degradation
in its entropy rate, from 23.7 to 61.2.
This empirical finding is notable since the neural attention- and memory-based techniques have been
steadily improving on standard metrics like perplexity and, in some cases, even produce remarkably
coherent text (often with some heuristics to reject poor generations). That the perplexity of generated
text is so much higher than it is under the true distribution suggests that there are significant gaps in
our current methodologies in accurately learning language models, particularly if we are interested
in generating text that globally resembles the modeled language itself.
Our contributions. The focus of this work is twofold: to improve generations based on any mea-
surement mismatch on a long-term property of the model (e.g. the entropy rate), and to quantify
the way a model’s predictions depend on the distant past. Central to both of these is a calibration-
based approach, which is utilized in statistics and other areas of machine learning (Dawid, 1982;
1985; Foster, 1991; Zadrozny and Elkan, 2002; Platt, 1999; Guo et al., 2017; Niculescu-Mizil and
Caruana, 2005).
First, we show that, from a worst-case perspective, even an extremely accurate model (with ε average
KL divergence from the true distribution) may have generated text with a substantially different
entropy rate as compared to the true distribution. Indeed, we show that this worst-case amplification
may occur for a variety of long-term properties of a probabilistic language model; this is because the
one-step KL divergence does not in general provide tight control over the expectation of a bounded
function. The observed entropy rate amplification (as seen in Table 1) demonstrates that this is not
only of theoretical concern. We then describe a calibration procedure to fix this mismatch while
simultaneously improving the perplexity of the language model. From a statistical perspective, the
procedure is simple, and we discuss approaches to make it computationally efficient.
Second, we provide a definition for long-term memory in language models as the mutual information
between the models predictions and the distant past in the input. We then provide an upper bound
on the amount of this mutual information using calibrated distributions (with a single-parameter
exponent). This allows us to estimate the amount of context used by a language model as a function
of the distance of past tokens from the current prediction time step.
We perform empirical studies to accompany our theoretical results. We first use the entropy rate
calibration algorithm to fix an LSTM language model, resulting in a drop of around 20 perplexity
points in the generated text (so that the entropy rate of the model more accurately matches that of
the language itself). Then, we empirically estimate and compare the long-term memory of state-of-
the-art language models. Our insights point towards new ways of assessing (and fixing) language
models, especially in terms of their long-term properties, in a manner complementary to existing
metrics like perplexity.
2	Related Work
Improving language modeling with long-term dependencies. Recent approaches to improving
language modeling have focused on several ways to better capture long-term dependencies, from
using manually-defined context representations (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and
Cho, 2016) or document-level topics (Wang et al., 2017) to using LSTM recurrent neural networks
with careful initialization (Le et al., 2015), auxiliary loss signals (Trinh et al., 2018) or augmented
memory structures (Grave et al., 2016; Ke et al., 2018). Wiseman and Rush (2016) use scoring
functions over sequences and search-based optimization to improve generation in seq2seq models.
More recent work has demonstrated the applicability of Transformer networks (Vaswani et al.,
2017a) to the task, potentially side-stepping issues in training recurrent networks (e.g. vanish-
ing/exploding gradients) and scaling to longer contexts (Dai et al., 2018; Radford et al., 2018).
All these papers propose either architectural or optimization innovations to improve language model
2
Under review as a conference paper at ICLR 2020
training. In contrast, we define and measure explicit long-term properties of language models and
show that calibrating them correctly can provide improvements to any black-box language model.
Recent empirical breakthroughs have stemmed from language models which do not specify a unique
autoregressive factorization (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019), and thus do not
specify a unique Pcr. It remains
an interesting problem to identify and sample from distributions
induced by these models (Wang and Cho, 2019); thus, our end-to-end theoretical guarantees do not
hold in this setting.
Information-theoretic approaches. While most language models aim to predict a distribution
over the next token conditioned on the context, there have been alternative approaches relying on
information-theoretic measures. Jost and Atwell (1994) propose a model which makes use of mutual
information between word pairs to generate word sequences that retain longer-term dependencies.
McAllester (2018) propose a training objective based on mutual information for predictive modeling,
and demonstrate its application for phoneme prediction. Clarkson and Robinson (1999) develop
a hybrid metric using both perplexity and entropy rate, and show that it correlates better with a
downstream metric like word error rate. Such works propose alternative optimization objectives;
in contrast, we show how to use information-theoretic measures to improve models with respect to
existing objectives like cross-entropy.
Measuring long-term statistics. Khandelwal et al. (2018) analyze LSTM-based language models
and empirically show that such models make use of a finite context for prediction. Lin and Tegmark
(2017) measure mutual information between any two symbols in human languages, and show that
it decays with distance, roughly following a power law distribution. Takahashi and Tanaka-Ishii
(2018) provide an upper bound for the entropy (character-level) of human languages by training
neural language models with various context and data sizes and extrapolating to infinity. While we
also make use of measures like entropy and mutual information across longer contexts, our goal is
to use these to better calibrate the language model and provably improve its perplexity.
Calibration and integral probability metrics. The idea of matching properties of the models’
predictions to the empirical outcomes, in an online setting, goes back (at least) to the “prequential
principle” of Dawid (1982; 1985), with subsequent work in online and game-theoretic settings (Fos-
ter, 1991; Vovk, 2001; Kalai et al., 1999). The idea of improving probability scores is also common
in machine learning (Zadrozny and Elkan, 2002; Platt, 1999; Guo et al., 2017; Niculescu-Mizil
and Caruana, 2005). Recently, Ott et al. (2018) assessed model calibration for machine translation
systems using word-level probabilities. The notion of examining the expectation of functions as a
metric for the distance between two distributions sometimes goes under the name of integral prob-
ability metrics (Mller, 1997; Sriperumbudur et al., 2009), and this notion is becoming increasingly
relevant again in unsupervised learning through the connections to GANs (Mroueh and Sercu, 2017).
In this work, we directly focus on the KL divergence, where our use of calibration is largely based
on basic facts about exponential families (Brown, 1986).
Relation to generation-improving heuristics. If the sole objective is to improve the qualitative
coherency of sampled generations, a wide variety of heuristics exist in the literature. The simplest of
these is a constant multiplicative adjustment to the model’s logits (known as softmax temperature Xie
(2017)). This is a specific version of our method (Algorithm 2) with a constant logistic regression
feature instead of the next-token conditional entropy. Relatedly, greedy and top-k sampling (used in
state-of-the-art works such as (Radford et al., 2019)) are heuristics which make local modifications
to the model’s conditional probabilities to decrease diversity and eliminate nonsensical generations.
Efforts to push the empirical state of the art in generation quality have given rise to more complex
heuristics. Bengio et al. (2015) propose retraining the network on its own generations with a care-
fully scheduled probability for each token. Some works regularize a model’s generations with an
auxiliary reverse language model Zhang et al. (2019); Liu et al. (2016). Yet others promote realism
using adversarial training protocols (Bahdanau et al., 2016; Lin et al., 2017; Fedus et al., 2018).
We stress that our calibration methods result in a provable improvement in the original training
objective (i.e. lower perplexity). As far as we know, none of the aforementioned heuristic meth-
ods can hope to provide such a strong guarantee, since they are fundamentally designed to bias
3
Under review as a conference paper at ICLR 2020
models towards a different objective. Our work mitigates model hallucinations for (almost) free1,
in the sense that the global objective (entropy rate drift) is improved without worsening the local
objective (perplexity). Furthermore, calibration preserves the computational efficiency of density
estimation: a conditional probability vector from Algorithm 2 can be computed using O(vocabulary
size) inferences on the original model. In the more advanced heuristics, the implied distribution over
sequences is lost, and is only accessible by black-box sampling.
3 Preliminaries
We first define some useful quantities for our analyses. Let Pr(W1, W2, . . . , WT) represent the true
underlying distribution over T length sequences of words, where the vocabulary is of size M . Let
W1:T denote a random sequence of length T, with distribution Pr(W1:T). For clarity of exposition,
we assume that all sequences (i.e. sentences or documents or books) are of equal length T.
For any distributions D and D0 over length-T sequences, recall that the entropy H(∙), KL-
divergence, and entropy rate are, respectively, defined by: H(D) := EwLT~d
D(W1:TL=WLT)],
KL(D	k	D0)	:=	Ewrτ~D	[log D(WIh=w1:T)) 1	, and EntRate(D)	:=	ɪH(D).	Let C(WrT)
w	w1:T	D0(W1:T =w1:T)	T T	T	:-
denote a learned distribution over sequences. In the typical sequential prediction setting, the proba-
bilistic model is implicitly defined by the conditional distributions Pr(Wt|W<t), which are typically
efficiently computable. It is standard for such a language model to be trained to minimize the cross
entropy objective:
.. --∙.
CE(Pr k Pcr) :
1 E
T wi：T~Pr
T
X log
t=L
1
I?' / I	∖
Pr(wt|w<t)
1 E
T wi：T~Pr
log
1
Pr(wL:T)
TL ɪ . .ι . Γ-	-I	1 1	1 1 i	.1	.	z-<-n / tλ Il i? ∖	「	，c ，	∕τ2Γ" ∖
Note that for an accurate language model, we would hope that: CE(Pr k Pr) ≈ EntRate(Pr),
i.e. the entropy rate of the sequences generated under the learned model is nearly that of the cross
entropy of the model (with respect to the true distribution Pr).
Throughout, we assume that
1
亍KL(Pr k Pr) = CE(Pr ∣∣ Pr) - EntRate(Pr) ≤ ε	(1)
holds for some ε. In other words, the (unknown) ε measures the degree of sub-optimality of the
learned model, this ε is often referred to as the Bayes regret.
4	Calibration and Entropy Rates
In this section, we assess the long-term properties of language models when generating text.
Specifically, we quantify the amplification in the entropy rate of generations under an ε-accurate
model (Eq. 1). We then provide a procedure to fix this amplification, without increasing the perplex-
ity of the model. Proofs for all statements are provided in the supplementary material.
For generality, consider a function f : [M]T → R, defined on T length sequences. Let the mean
and variance of f under distribution D be denoted by μD (f) and σD (f)
μD(f) ：=	E n[f (wi：T)],	σD(f) := E	[(f (wi：T) - μD(f))2].
wi：T ~D	wi：T ~D
4.1	Error amplification under our model
If our learned m
odel Pcr is accurate,
We may hope that μpr(f) ≈ μc(f) i.e. that the expected value
of f under the true distribution Pr is close to its expected value under our model. We can quantify
this gap as folloWs:
1Technically, at the statistical cost of fitting one more parameter.
4
Under review as a conference paper at ICLR 2020
Figure 1: Perplexity (exponential of conditional entropy, given the past) of the t-th generated word,
for two popular language models, averaged over more than 500 generation runs with different con-
texts. At t = 1, this is the model’s upper bound for the language’s perplexity. As t → ∞, this is the
exponential of the entropy rate of the model’s own generations. For a perfectly calibrated model,
this curve would be flat (gray dotted lines). Left: LSTM trained on Penn Treebank. Right: GPT-2
Transformer.
Lemma 4.1. (Pinsker's Inequality (Csiszar and Korner, 2011)) Suppose that for all wi：T,
f(wi:T) ≤ B∙ Then:	____________
∣μpr(f) - μc(f)∣ ≤ B，2KL(Prk C).
Since this holds for any bounded function, we can obtain the error amplification of the entropy rate
of Pr simply by choosing f = - log Pr.
Before we proceed, in order to rule out amplification of this entropy rate due to arbitrarily small
probabilities (which can blow up - log Pr), it is helpful to define the γ-mixture distribution as:
D(γ) := (1 - γ)D + γUni, where the Uni is the uniform distribution over all MT sequences.
We will then consider the model Pcr(ε), which has only a minor degradation in the cross entropy
compared to Pcr, and, yet, may have a large amplification in the entropy rate.
Corollary 4.2. (Entropy rate amplification under generations) Suppose the bound in equation 1
holds. The ε-mixture distribution has KL bounded as:
TκL(Pr k 四)≤ (l + T) ε.
We have that:
∣CE(Pr k Pr()) — EntRate(Pr)I ≤ (l + W) ε , and
∣CE(Pr k c(ε)) — EntRate(c(ε))∣ ≤ ,2ε(T + 1) (logM + log*ε)).
This bound shows that, in the worst case, even a small cross entropy may provide little control over
the generations under our model (in terms of entropy rate). In fact, for ε = O( T) (which We may
hope is an accurate model), the bound is vacuous; a later remark shows this worst case bound is
unimprovable, see the supplementary material.
The above theorems suggest that entropy rate amplification is a theoretical possibility in the worst
case, which our experiments show is in fact prevalent in pratice. These entropy rate amplifications
are evident from the plots in Figure 1. Regardless of the text corpus or the language model, we
observe that the entropy rate under the model’s generations quickly increases with time, indicating
that this is a persistent problem even for state-of-the-art language models while generating text.
4.2	Model calibration
We now describe a procedure to fix this error amplification. First, let us define a distribution Prα
such that:
___/ c (	∖ ∖ 2r*T? /	∖
Pra(wi：T) =------------1ZZ----------- where Za = X exp(αf (wi：T)) ∙ C(WLT).
Zα
w1:T
We can then recover a calibrated model that does not suffer from error amplification in f :
5
Under review as a conference paper at ICLR 2020
Algorithm 1 (Inefficient) Entropy Rate Calibration
1:	Input: Model Pcr(ε).
2:	Define a model class:
Pra(Wl:T) = ^Pr(Wl：T)(ε))	/Zɑ .
—. . ____________________ .. ^ 、
3:	Fit α*: α* = argmιna CE(Prk Pra)
4:	Return Pu
Lemma 4.3. (Calibration to f with model improvement) Suppose the variance of f is uniformly
bounded in that there exists σj such that the following holds for all α, σp% (f) ≤ σj . Let α* =
argminα CE(Pr k Pcrα) . We have
—
—
μpr(f) - μcα*(f )= 0, and CE(Pr k Pg) ≤ CE(Pr k Pr) -
l(μ(f)-μc(f ))2
T	2σ+
Entropy rate calibration. We can now apply the previous result to fix the entropy rate amplifica-
tion seen in Table 1. Note that it is trivial to avoid the entropy rate amplification if we were allowed
to degrade the quality of our model, in terms of perplexity (e.g. a unigram model does not have
this amplification. However, we show that it is possible to match the entropy rate without having
to sacrifice the quality of our model. In fact, we can both improve our model and more accurately
match the entropy rate, by fitting a family of one-parameter models.
Theorem 4.4. (Entropy rate calibration) Suppose equation 1 holds. Algorithm 1 returns a Pia*
such that: the following calibration property is satisfied:
___.. ≤^- _ ,≤^- 、
CE(Pr k Pcra* ) = EntRate(Pcra* ).
Furthermore, Pra* has entropy close to the true entropy rate as specified by:
∣EntRate(Pr) - EntRate(Prα*)| ≤ f 1 + ɪ) ε,
and Pra* is an improvement over the original model as characterized by:
CE(Pr k Pca*) ≤ CE(Prk cc®) - 1
CE(Prk cc(ε)) - EntRate(C) ʌ
log M + log抖
This result shows that we simply need a single parameter α to define a new model class that is a
powered up version of our original model. Then, we can fit this α to minimize the cross-entropy
of the new model with respect to the true distribution Pr, in order to eliminate the entropy rate
amplification.
Even though this algorithm fits only a single parameter, it is not easily implementable since it re-
quires an integration over sequences, at least in its exact form. One future direction would be to a
sample based approach. This may be an interesting alternative to ideas like beam search (Steinbiss
et al., 1994; Ortmanns and Ney, 2000; Antoniol et al., 1995), which also aims to minimize a global
cost function on sequences that is inconsistent with the token-level perplexity loss used to train the
underlying generative model.
Lookahead algorithms. In order to sidestep the computational issues of Algorithm 1, we provide
another simple approach based on what can be viewed as a “one-step” lookahead correction (Al-
gorithm 2). Let Wt be a random variable With conditional distribution Pr(∙∣W<t). H(Wt+ι ∣w≤t)
denotes the entropy of this conditional distribution, i.e.
1
H (Wt+ι∣w≤t) =	E log -------------------.
wt+ι~ PrGlw≤亦	Pr(wt+ι∣w≤t)
6
Under review as a conference paper at ICLR 2020
Algorithm 2 Local Entropy Rate Calibration
1:	Input: Model Pr(‘)，where Ct 〜Pr(‘)(∙∣W<t).
2:	Define a model class:
τi-⅛7? /	∖ S/ ∖6∕ I ∖
Prα(wi:T) = Pα(wi)Pα(w2∣Wl)…
where
/ I ∖ TΓΛ^ / I ∖	( τr ∕τr? I ∖λ / rz
Pra(Wt|w<t) = Pr(wt∣w<t) ∙ exp (-α ∙ H(Wt+ι∣w≤t)j /Zα∙
一 . 一一，一 .. 、
3:	Fit α*: α* = argmin. CE(Prk Pra)
4:	Return Pu
1T
μD = T X
t=1
一 .--~~~∙ . 一
[H (Ct+ι∣w≤t)]
E
E
w<t~Pr wt~D(∙∣w<t)
El — ∙ ,ι	。ττ∕ττ7 I ∖ ∙ ,ι	, , τ , ∙ι	i ∙ i	zrʌ。	ι ∙
Thus, μD is the average of H(Wt+ι∣w≤t) with respect to a distribution which uses D for sampling
.ι 1	.	1 ττr z .	..	. x	∙.∙	1 .ι	一	… k ∙.ι	∙.∙	一
the last word Wt (at every timestep). Intuitively, the resulting model Pra with a positive α would
suppress sampling words leading to larger entropy but rather encourage words that stablizes the
entropy 1-step ahead in the future. Therefore, if our learned language model Pcr was accurate, we
would hope that: μpr ≈ “c . The following corollary shows that this is achievable, along with
improving the model’s perplexity.
Corollary 4.5. Suppose Equation 1 holds. Then, Algorithm 2 returns a Pra* Such that:
■X T , ,ι TT ∕ττrr I	∖ ∙ ι ι ,ι	ι	∙	.ι	.	. .∙	,	. ι
Note that H(Wt+ι ∣w≤t) includes the word wt, so We require computing the entropy at time t + 1
when predicting Wt using a learned model.
For a conditional distribution, D(W1:T), let us define:
-	-	C	7	Z'il-∖ ∕TΓΛ Il TΓΛ^ ∖
μpr - μpcα* =0, and CE(Prk Pra*)
CE(Prk c(ε)) -1 (	口一午:)/)!
2 log M + logTε) J
≤
This result provides us with Algorithm 2, which
is computationally quite tractable. We first use
the learned model Pr to define a new model
class Pra, which scales Pr by an exponential
distribution over the weighted 1-step lookahead
entropy H(Wt+ι∣w≤t). Then, similar to Algo-
rithm 1, we simply fit the single parameter α to
minimize the cross-entropy of the new model
with respect to Pr, which fixes the entropy am-
plification in the resulting model Pra . We ob-
serve this empirically in Figure 2 - our cali-
bration results in a perplexity drop of almost
20 points over long-term generations under an
LSTM model. Model and implementation de-
tails are in the supplementary material.
Generations from a calibrated model. Ta-
ble 2 provides sample generations from a cali-
Generation length t
Figure 2: Effect of calibrating an LSTM genera-
tive model with 1-step lookahead. Blue: perplex-
ity curve (i.e. exponential of conditional entropy
H) from the setting of Figure 1. Green: the same
perplexity measurements after applying local cal-
ibration.
brated Transformer model trained on the GBW dataset, compared to its original version. Qualita-
tively, the calibrated generations: (1) are shorter and more concise, and (2) display a better grasp of
discourse structure across sentences. More generations are provided in the supplementary material.
5 Calibration and Memory
Defining a notion of memory in language models is challenging, and multiple equally sensible no-
tions may co-exist. Here we present our choice from first principles. Let us say
that Wct is a sample
7
Under review as a conference paper at ICLR 2020
Original model
Actual results could differ materially from those
indicated by these forward-looking statements as
a result of various important factors , including ,
without limitation : changes in general economic
and business conditions , including more diffi-
cult real estate environments ; [...174 tokens...]
risks related to investigations by other companies
; inadequate information systems ; the impact of
reduced availability of ; * assumptions upon such
companies using such as ours to gauge CNET ’s
financial condition ; and other factors .
Bluepoint Games , Inc. is a highly experienced and
multi-faceted publisher of licensed virtual worlds
for gamers , developers and technology profession-
als . [...114 tokens...] James Upon , CEO of
MyNetSheltetWeb and the three previous Develop-
ers of MySQL . Based in Redwood City , Califor-
nia , BlueMountain is the leader in franchise and
game development for the massively multiplayer
online game .
Calibrated model
Actual results could differ materially from those in-
dicated by these forward-looking statements as a
result of a variety of factors , including but not lim-
ited to ( i ) the risk that the tender offer could close
in one or more manner or at all ; ( ii ) risks asso-
ciated with conducting business in foreign jurisdic-
tions ; ( iii ) difficulties in combining some or all of
the businesses under one roof ; ( iv ) decreased de-
mand for electricity , natural gas and other energy
products , including adverse effects on the pricing
of oil and natural gas ; and ( v ) the risks associated
with doing business internationally .
Bluepoint Games , Inc. is a highly experienced li-
censing , gaming and entertainment firm focused
on developing the next generation of casual games
based on the PlayStation ( R ) BRAVIA family of
video game machines for the North American mar-
ket . Bluepoint is a wholly owned subsidiary of
Bluehill ID Holdings L.P.
Table 2: Sample generations from a calibrated,
state-of-the-art Transformer model trained on the
GBW corpus, seeded with prefixes of sentences (in italics) from the holdout validation set.
from a model at time t, i.e. Wt 〜Pr(W∕W<t). Let us also assume that W<t 〜Pr(W<t). WeWill
define the memory at gap τ as the mutual information between Wct and the distant past (those words
greater than T steps ago) conditioned on the subsequence Wt-τ：t—i. Precisely,
—
—
—
IT= I(Wt; W<t-τ|Wt-T：t-i) = H(WtIWt-T：t-i) - H(Wt∣W<t),
Where We are not explicitly denoting the t dependence in this definition2.
Intuitively, It can be vieWed as hoW much uncertainty (entropy) in the prediction Wt the model is
able to reduce by utilizing the deep past W<t-τ in addition to the recent past W--r：t—i.
The difficulty in estimating this mutual information is due to estimating H(WtIWt-^t-ι), which
requires the marginalized model Pr(Wt∣Wt-τ：t-i). To (even approximately) marginalize a model
distribution Pr(WtIW<t) over the deep past W<t-τ is statistically difficult, since it requires the
access to a pool of samples of W<t that share an common recent past Wt-T：t-i. Nevertheless,
we now show that it is possible to obtain an upper bound (which is computationally efficient to
estimate).
Upper bounding mutual information using calibrated models. In the above, we were consider-
ing the mutual information between Wct and W<t-T
conditioned on Wt-T:t-1. Let us now consider
a more general setting, where we have a distribution Pr(Z, Y, X) where Z, Y , and X are random
variables. We wil eventually consider Z,
Y, X to be Wct ,
Wt-T:t-1 W<t-T, respectively.
一. 一	_ ______ 一 rr_________ 一一	_ 一 一
For distributions D(∙∣Y,X) and D(∙∣Y,X) and for α ∈ R, define
Da(ZIY,X) := D(Z∣Y,X) ∙ (D(ZIY,X))α IZa.
We say that D(∙∣Y, X) is calibrated to D(∙∣Y, X), if D = Da=o is unimprovable in that for all ɑ
CE(Pr k D) ≤ CE(Pr k Da).
Note this condition is achievable due to that calibrating a model to D(∙∣Y, X) involves a one dimen-
sional (convex) estimation problem (over α).
2While we may attempt to estimate Iτ for a given t, we can remove the t dependence by either defining this
quantity by with an average over t or by using appropriate stationarity assumptions. In our experiments, we
average over t.
8
Under review as a conference paper at ICLR 2020
Figure 3: Left: Plot of the upper bound on Iτ derived from calibrated models. Right: The measure-
ments of the upper bound on mutual information, the cross entropy of the limited memory model
Pr as well as the optimal calibration coefficient a* for various time lengths T. Details of the model
used here can be found in the supplementary material.
T	CE(Pr ||Pr)	Iτ upper bound	a*
~5~	-4.8144	0.6180	0.003515
10^^	-4.5258	0.4226	-0.01041
15^^	-4.4166	0.2678	-0.00447
"^0^^	-4.3347	0.2485	-0.02268
"^5^^	-4.2777	0.2274	-0.01814
~30~	4.2408 —	0.2143	-0.02323
〜
~ ■—■
—
Theorem 5.1. Suppose we have a model Pr(Z|X), and suppose Z 〜Pr(∙∣X), where Z is depen-
X. Suppose that Pcr
dent only on
is calibrated to Pr. Then we have that:
I(Zb;X|Y) ≤ CE(Pr k Pfr) - H(Zb|Y,X) , where:
CE(Pr k Pfr) = E E
Y〜Pr Z〜Pr(∙∣Y)
log
1
Pfr(Z|Y)
)
Memory estimation. We first learn another Wt 〜 Pr(∙∣Wt-τ：t-i), and then calibrate Pr to Pr.
Corollary 5.2. Suppose Pr (∙∣W<t) is a model calibrated to Pr(∙∣Wt-τ：t-1). For a random
l cal
variable, WtCal 〜Pr (∙∣W<t), we have that:
I(CtCal； W<t-τ|Wt-T：t-i) ≤ CE(Pr k ff) - H(CtCal∣w<t), where:
CE(Pr k Pfr)
E
Wt-T-Pr
log
1
〜- . .
Pr(Wt∣Wt-τ ：t-i)
This corollary gives us a means to efficiently provide upper bounds on the mutual information. The
key is that since Pr is efficiently computable, we can directly estimate CE(Pr ||Pr) through Monte
Carlo estimation. We measure the upper bounds on Iτ of a LSTM model with trained limited-
memory models Pr (see details in the supplementary material) and report them in Figure 3. As
expected, the memory estimate gradually decays with longer τ, indicating that the models make
more use of the recent past to generate text.
6 Conclusion
We have introduced a calibration-based approach to detect and provably correct the discrepancies
between the long-term generations of language models and the true distributions they estimate se-
quentially. In particular, for state-of-the-art neural language models, we have observed large degra-
dations of the entropy rate under iterative generation, and a proposed first-order correction which is
both computationally tractable and effective. Using the same calibration approach, we have derived
estimators for the amount of information extracted by these models from the deep past.
Aside from the empirical findings and improvements, we hope that this work will inspire a more
principled line of discourse on the quality of long-term generations in language models. It remains
an interesting open problem to relate the plethora of “future-aware” generation-improving heuristics
to our calibration framework.
9
Under review as a conference paper at ICLR 2020
References
Giuliano Antoniol, Fabio Brugnara, Mauro Cettolo, and Marcello Federico. Language model rep-
resentations for beam-search decoding. In 1995 International Conference on Acoustics, Speech,
and Signal Processing, volume 1, pages 588-591. IEEE, 1995.
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. arXiv preprint
arXiv:1607.07086, 2016.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence
prediction with recurrent neural networks. In Advances in Neural Information Processing Sys-
tems, pages 1171-1179, 2015.
L. D. Brown. Fundamentals of Statistical Exponential Families: With Applications in Statistical
Decision Theory. Institute of Mathematical Statistics, Hayworth, CA, USA, 1986. ISBN 0-940-
60010-2.
Philip Clarkson and Tony Robinson. Towards improved language model evaluation measures. In
Sixth European Conference on Speech Communication and Technology, 1999.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecom-
munications and Signal Processing). Wiley-Interscience, New York, NY, USA, 2006. ISBN
0471241954.
Imre Csiszar and Janos Korner. Information theory: coding theorems for discrete memoryless sys-
tems. Cambridge University Press, 2011.
Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan
Salakhutdinov. Transformer-xl: Language modeling with longer-term dependency. 2018.
A. P. Dawid. The well-calibrated bayesian. Journal of the Am. Stat. Assoc, 77, 1982.
A. P. Dawid. The impossibility of inductive inference. Journal of the Am. Stat. Assoc, 80, 1985.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
William Fedus, Ian Goodfellow, and Andrew M Dai. Maskgan: better text generation via filling in
the_. arXivpreprint arXiv:180L07736, 2018.
D.	P. Foster. Prediction in the worst case. Annals of Statistics, 19, 1991.
Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. Frage: frequency-agnostic
word representation. In Advances in Neural Information Processing Systems, pages 1341-1352,
2018.
Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a
continuous cache. arXiv preprint arXiv:1612.04426, 2016.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pages 1321-1330. JMLR. org, 2017.
Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer, and Jacob Eisenstein. Document context
language models. arXiv preprint arXiv:1511.03962, 2015.
Uwe Jost and ES Atwell. Proposal for a mutual-information based language model. In Proceedings
of the 1994 AISB Workshop on Computational Linguistics for Speech and Handwriting Recogni-
tion. AISB, 1994.
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the
limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.
10
Under review as a conference paper at ICLR 2020
E.	Kalai, E. Lehrer, and R. Smorodinsky. Calibrated forecasting and merging. Games and Economic
Behavior, 29, 1999.
Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Michael C Mozer, Chris Pal,
and Yoshua Bengio. Sparse attentive backtracking: Temporal credit assignment through remind-
ing. In Advances in Neural Information Processing Systems, pages 7651-7662, 2018.
Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. Sharp nearby, fuzzy far away: How neural
language models use context. In Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), volume 1, pages 284-294, 2018.
Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. A simple way to initialize recurrent networks
of rectified linear units. arXiv preprint arXiv:1504.00941, 2015.
Henry Lin and Max Tegmark. Critical behavior in physics and probabilistic formal languages.
Entropy, 19(7):299, 2017.
Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, and Ming-Ting Sun. Adversarial ranking for
language generation. In Advances in Neural Information Processing Systems, pages 3155-3165,
2017.
Lemao Liu, Masao Utiyama, Andrew Finch, and Eiichiro Sumita. Agreement on target-bidirectional
neural machine translation. In Proceedings of the 2016 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, pages 411-
416, 2016.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of english: The penn treebank. 1993.
David McAllester. Information theoretic co-training. arXiv preprint arXiv:1802.07572, 2018.
Gabor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language
models. arXiv preprint arXiv:1707.05589, 2017.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and Optimizing LSTM
Language Models. arXiv preprint arXiv:1708.02182, 2017.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. An Analysis of Neural Language Mod-
eling at Multiple Scales. arXiv preprint arXiv:1803.08240, 2018.
Tomas Mikolov and Geoffrey Zweig. Context dependent recurrent neural network language model.
In 2012 IEEE Spoken Language Technology Workshop (SLT), pages 234-239. IEEE, 2012.
Youssef Mroueh and Tom Sercu. Fisher gan. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing
Systems 30, pages 2513-2523. Curran Associates, Inc., 2017. URL http://papers.nips.
cc/paper/6845-fisher-gan.pdf.
Alfred Mller. Integral probability metrics and their generating classes of functions. Advances in
Applied Probability, 29:429-443, 06 1997. doi: 10.2307/1428011.
Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learn-
ing. In Proceedings of the 22nd international conference on Machine learning, pages 625-632.
ACM, 2005.
Stefan Ortmanns and Hermann Ney. Look-ahead techniques for fast beam search. Computer Speech
& Language, 14(1):15-32, 2000.
Myle Ott, Michael Auli, David Grangier, et al. Analyzing uncertainty in neural machine translation.
In International Conference on Machine Learning, pages 3953-3962, 2018.
11
Under review as a conference paper at ICLR 2020
John C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized
likelihood methods. In Advances in Large Margin Classifiers, pages 61-74. MIT Press, 1999.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language un-
derstanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-
assets/research-covers/languageunsupervised/language understanding paper. pdf, 2018.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. 2019.
Claude E Shannon. Prediction and entropy of printed english. Bell system technical journal, 30(1):
50-64, 1951.
Bharath Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schlkopf, and Gert Lanckriet.
On integral probability metrics, phi-divergences and binary classification. 01 2009.
Volker Steinbiss, Bach-Hiep Tran, and Hermann Ney. Improvements in beam search. In Third
International Conference on Spoken Language Processing, 1994.
Shuntaro Takahashi and Kumiko Tanaka-Ishii. Cross entropy of neural language models at infinitya
new bound of the entropy rate. Entropy, 20(11):839, 2018.
Sho Takase, Jun Suzuki, and Masaaki Nagata. Direct output connection for a high-rank language
model. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Pro-
cessing, pages 4599-4609, 2018.
Trieu H Trinh, Andrew M Dai, Thang Luong, and Quoc V Le. Learning longer-term dependencies
in rnns with auxiliary losses. arXiv preprint arXiv:1803.00144, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, pages 5998-6008, 2017a.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998-6008, 2017b.
V. Vovk. Competitive on-line statistics. International Statistical Review, 69, 2001.
Alex Wang and Kyunghyun Cho. Bert has a mouth, and it must speak: Bert as a markov random
field language model. arXiv preprint arXiv:1902.04094, 2019.
Tian Wang and Kyunghyun Cho. Larger-context language modelling with recurrent neural network.
In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 1319-1329, 2016.
Wenlin Wang, Zhe Gan, Wenqi Wang, Dinghan Shen, Jiaji Huang, Wei Ping, Sanjeev Satheesh, and
Lawrence Carin. Topic compositional neural language model. arXiv preprint arXiv:1712.09783,
2017.
Sam Wiseman and Alexander M Rush. Sequence-to-sequence learning as beam-search optimization.
arXiv preprint arXiv:1606.02960, 2016.
Ziang Xie. Neural text generation: A practical guide. arXiv preprint arXiv:1711.09534, 2017.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V
Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint
arXiv:1906.08237, 2019.
Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass proba-
bility estimates, 2002.
Zhirui Zhang, Shuangzhi Wu, Shujie Liu, Mu Li, Ming Zhou, and Tong Xu. Regularizing neural
machine translation by target-bidirectional agreement. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 33, pages 443-450, 2019.
12
Under review as a conference paper at ICLR 2020
A Proofs for Section 4
Proof. (of Lemma 4.1) We have
Wmrfg)]- wι±cf(wι:T)]
= X Pr(w1:T ) - Pcr(w1:T ) f (w1:T )
w1:T
≤	Pr -Pcr B
≤ 2KL(Pr ||Pcr))B
where we have used Holder’s and Pinsker’s inequalities Cover and Thomas (2006).
Proof. (of Corollary 4.2) First observe:
□
1
1
1
log PPr(ε)
and that:
(w1:T)
≤ log
(1 - ε)Pr(wi:T)
log
Pr(w1:T)
- log(1 - ε) ≤ log
1— 十 2ε
Pr(w1:T)
log PPr(ε)
(w1:T)
≤ log MT.
ε
(2)
1
For the first claim, we have
Pr(W1:T)
ogPC‰)
2
≤ (I + τ)ε.
using our assumption in Equation 1.
For the second claim, taking f = log
CSXWLT )
with Lemma 4.1, we have:
1
CE(Pr ∣∣Pr(ε)) - EntRate(COS))
T√2KL(Pr ∣∣Pc(ε))
logP⅛
≤
∞
τ ,2ε(T+1) log ε~,
which completes the proof.
□
Proof. (of Lemma 4.3) By definition,
___. .≤^- 、 ____________,.
≤^-
CE(Pr ||Pra)= CE(Pr ||Pr)
E	f (w1:T)] + 不 log(Za) ,
wi：T 〜Pr	T
—
α
T
we have:
∂CE(Pr ||ca)
∂α
The first claim now follows from optimality of a*.
For the second claim,
T (一μPr(f) + μCa
.C	. .∙--.
∂2CE(Pr ∣∣Pra)
∂2α
1 ∂2 log(Za)
τ 一∂2α一
- C X~'	C /	∖	/ C /	∖ ∖ I ?' /	\
1 ∂ Ew1：T f(wi:T) exp(αf(wi：T)) ∙ Pr(wi：T)
T∂α
1 2
PC.
X~'	/ Ct	∖ ∖ I ?' /	∖
EwLT exp(αf (WLT)) ∙ Pr(WLT)
2
(f) ≤ -T+ .
By Taylor’s theorem, we have:
≤^-
≤^-
CE(Pr ∣∣Pra) ≤ CE(Pr ||Pr) - a ∙
1	α2 σ2
T (μPr(f) - μc(f)) + ^2^ ∙
Taking the the α which minimizes the upper bound, leads to the second claim.
□
13
Under review as a conference paper at ICLR 2020
Remark A.1. (Sharpness) If ε ≥ 1, then there exists a problem where the bound is sharp and
EntRate(Pr) takes on the maximal value of O(log M). As an example, consider a model Pr, that
starts by generating words under the true distribution Pr and has a T probability of transitioning
into a mode in which it generates words uniformly at random thereafter.
Proof. (of Theorem 4.4) We can apply the previous lemma using
f i	1
f = log K--------,
Pr(w1:T )
and so our calibration condition implies:
0 = μpr(f) - μPCα*(f) = - (〃Pr(Iogc)- μcα* (Iogc)).
Now observe that:
_ ____. .≤^- , . _ ≤^- _ 一 ≤^-, _ _
T ∙ CE(Pr ∣∣Prα*) = μpr(-(1 + α ) log Pr + log Za) = -(1 + α )μpr(log Pr) + log Za
and, similarly,
_ _ _ ,≤^- ≤^-, _ _
T ∙ EntRate(Pra*) = -(1 + α*)μc (logPr) + logZoc* .
Prα*
These imply:
1
CE(Pr ∣∣Pra*) - EntRate(Pu) = -T(1 + α ) (μpr(logPr)-从K* (logPr)J = 0,
which completes the proof of the first claim.
The proof of the second claim uses
μ(f) - μPc(f ) = T (CE(Pr ||c) - EntRate(Pc)),
and, by Equation 2,
σ+ ≤ T log M + log(1∕ε),
which completes the proof.	□
Now we move on to the proof of Corollary 4.5.
Suppose f(W≤t) be a function of W≤t. For a conditional distribution, D(W1:T ), let us now define:
1T
μD (f ) = τf∑ EP	JE	[f(w≤t)].
T M w<t~Pr Wt〜D(∙∣w<t)
Define:
1
Pt,a(wt∣w<t)：=节—exp(αf(w≤t)) ∙ Pr(wt∣w<t)
Za,t
and
Pra(w1:T ) := P1,a(w1)P2,a(w2|w1) .
Lemma A.1. Suppose f ≤ σ+2 . Let
__. .≤^-、
α = argmin CE(Pr ||Pra).
a
We have that:
and that
μPr(f) - μPCα*(f) = 0
. .∙-- . .∙-----------------∙.
CE(Pr ||PPra*) ≤ CE(Pr ||PPr) -
(μ(f)- μpc(f ))2
σ2
Proof. (sketch) The proof is identical to that of Lemma 4.3, with the addition of using linearity of
expectation.	□
14
Under review as a conference paper at ICLR 2020
B Proofs for Section 5
Proof. (of Theorem 5.1) It is convenient to define the distribution:
------------------------------------------ . .
D(Z, Y, X) = Pr(Z|X, Y) ∙ Pr(Y, X).
We then have:
I(Zb;X|Y) = H(Zb|Y) - H(Zb|Y, X)
by the defintion of the mutual information.
The proof consists of showing that:
1
H(Z|Y) = Ey,z〜D log D(ZY) ≤ CE(Pr ||fr).
α
Let Us take Pra(Z|X, Y) = Pr(Z|X, Y) ∙ (Pr(Z∣X)) /Zα. The zero gradient condition for the
optimality at α = 0 implies:
_	. .∙—■.
0 =	∂CE(Pr ∣∣Pra) I
∂α	α=0
=Ex,y〜Prh-EZ〜Pr(∙∣x,γ) logPf(Z|Y) + EZ〜C(∙∣χ,γ) logPf(Z|Y)]
_ ______ _ ________________ _ __________ _ ____________________
=-EY 〜Pr[Ez 〜Pr(∙∣γ) logf r(Z| Y)+ Ex,y 〜Pr[Ez 〜C(∙∣χ,γ) logPr(Z |Y)]
. .ʃ-~~\	-	-~~^ ∙ 一
=CE(Pr |fr) + Ex,y〜Pr[Ez〜C(∙∣χ,γ) log Pr(Z∣Y)] .
This implies:
1
CE(Pr ||Pr) = EX,Y~Pr[Ez〜C(∙∣χ,γ) log f “V、]
,	Pr(Z|Y)
= EX,Y,Z〜D log	~~-
,,	fr(Z∣Y)
=EY,Z〜D log 一：~~-
,	Pfr(Z|Y)
≥ EYZS log D(ZY)
= H(Zb|Y),
where the last step uses the definition of Z and Jensen s inequality.	□
C Experimental Details
In this section, we outline the experimental setups used to obtain the empirical results throughout
the paper. For the calibration and memory experiments (Table 1 row 1, Figure 1 (left), Figures 2, 3),
our base model is a 3-layer LSTM with with 400 embedding dimension and 1150 hidden nodes. We
train it on the Penn Treebank (PTB) corpus Marcus et al. (1993), following the setup of Merity et al.
(2017) and Merity et al. (2018) for 500 epochs using SGD with batch size 20 and BPTT length 70.
The trained base model achieves 64.3 validation perplexity and 58.3 test perplexity.
The limited-memory models Pr(∙∣Wt-τ：t—i) used for the memory estimation in Section 5 share the
same architecture as our base model while, during training, the hidden states is re-initialized after
reading every τ tokens (τ takes value from {5, 15, . . . , 30}).
Finally, for the entropy rate measurements of larger-scale state-of-the-art language models (Ta-
ble 1 rows 2-4, Figure 1 (right)), we used the pretrained weights published alongside Jozefow-
icz et al. (2016); Radford et al. (2019) for rows 2 and 4, while we trained the model using the
tensor2tensor framework. The model for row 2 is an LSTM with CNN-embedded inputs,
15
Under review as a conference paper at ICLR 2020
trained on the Google Billion Words (GBW) corpus. The other two are Transformer Vaswani et al.
(2017a) models trained on GBW (row 3), and an proprietary corpus derived from a web crawl (Web-
Text; row 4). For GPT-2, since the authors have not published training or validation data, we used
the text of several New York Times articles as a stand-in validation set; the cross entropy loss is
comparable to that reported on the validation set. The entropy rate amplification plot in Figure 1
(bottom) corresponds to the setup from row 4.
To measure the conditional entropy after t generations, we measured the empirical conditional en-
tropy of the t-th word over > 500 independent generations, which were produced by the standard
way of iteratively sampling from the next predicted conditional distribution, seeded with ground-
truth text up to > 100 random points in the validation set. We used the entropy rate at t = 700 as a
proxy for the asymptotic limit in Table 1.
D	Additional Generation S amples
In this section, to provide a better sense of the qualitative effect of calibration, we provide below
some additional generations, seeded by 10-token prefixes of the holdout (validation) sentences from
the Google Billion Words dataset. Here, we used the model we trained for row 3 of Table 1. To
identify a failure mode for the uncalibrated model, we selected the seed prefixes which resulted in
unusually long generations by the uncalibrated model.
Original model	Calibrated model
Actual results could differ materially from	Actual results could differ materially from
those indicated by these forward-looking state- ments as a result of numerous factors includ- ing the risks associated with the timely and ef- ficient completion and integration of the Tem- porary Liquidity Guarantee Department ’s su- pervision into the commercial , open market , solar energy , energy efficiency , electric util- ity transmission , and water demands of res- idential and business customers , Comcast ’s ability to successfully implement its business plan , timing of completion of the acquisition and the effectiveness of the efforts and strate- gies involved in the integration of Rhapsody , timing of regulatory and client approvals and availability of key enhancements .	those indicated by these forward-looking state- ments as a result of a variety of factors , in- cluding but not limited to ( i ) the risk that the tender offer could close in one or more manner or at all ; ( ii ) risks associated with conducting business in foreign jurisdictions ; ( iii ) difficul- ties in combining some or all of the businesses under one roof ; ( iv ) decreased demand for electricity , natural gas and other energy prod- ucts , including adverse effects on the pricing of oil and natural gas ; and ( v ) the risks asso- ciated with doing business internationally .
16
Under review as a conference paper at ICLR 2020
ActUal results could differ materially from	ACtUal results could differ materially from
those indicated by these forward-looking state- ments as a result of various important factors , including , without limitation : changes in general economic and business conditions , in- cluding more difficult real estate environments ; declines in information technology spend- ing ; continued availability of capital and gov- ernment regulations ; changes in general eco- nomic and business conditions ; the possibil- ity that extended unemployment and healthcare policies may change , or may reduce access to quality care services ; failure to obtain ad- equate and affordable medications ; changes in certain CME / CE product mix ; disruption in CME credit markets ; uncertainty of the out- comes of regulatory investigations of compa- nies in which the Company has an interest ; de- pendence on suppliers for most of its products ; consolidation among financial institutions ; ability to attract and retain skilled personnel ; changes in rapidly changing technology and regulatory environments ; arrogance and com- placency among financial analysts ; the impact of competition ; inability to retain and moti- vate senior management ; difficulties in the in- tegration of acquired businesses ; the effects of redundancy and loss of key employees ; litiga- tion , including claims and the challenge of in- surance practices ; uncertainties relating to lit- igation ; risks related to investigations by other companies ; inadequate information systems ; the impact of reduced availability of ; * as- sumptions upon such companies using such as ours to gauge CNET ’s financial condition ; and other factors .	those indicated by such forward-looking state- ments as a result of various important factors , including those discussed in the company ’s periodic reports that are filed with the Securi- ties and Exchange Commission and available on the SEC ’s website at www.sec.gov.
17
Under review as a conference paper at ICLR 2020
ActUal results could differ materially from
those indicated by SUch forward-looking state-
ments as a result of a variety of factors , in-
cluding our ability to improve our liquidity .
Among these factors are changes in the gen-
eral economy , changes in political and eco-
nomic conditions , changes in interest rates ,
changes in technology and implementation of
regulatory policies and legislation , the direc-
tion of interest rates and changes in the banking
industry , changes in loan prepayment activ-
ity , changes in consumer preferences and con-
sumer and business lending markets , legisla-
tion or public compliance with applicable laws
and regulations and changes in the business or
regulatory environment . We caution you that
there are many uncertainties that could cause
actual results to differ materially from those
indicated in the forward-looking statements .
Among them are the risk factors that could
cause results to differ from those expressed in
the forward-looking statements . These fac-
tors include , but are not limited to : gen-
eral economic and business conditions , includ-
ing the financial markets ; fluctuations in in-
terest rates ; government regulation of the fi-
nancial services industry and possible failures
; planning assumptions and estimates ; poten-
tial funding requirements ; unexpected changes
in cost increases ( including goodwill impair-
ment ) ; competition ; the potentially lengthy ,
protracted U.S. recession ; and migratory con-
sumer and business conditions .
Bluepoint GameS, Inc. is a highly experienced
and multi-faceted publisher of licensed virtual
worlds for gamers , developers and technology
professionals . The company is based in Van-
couver , Canada . BlueKai ’s innovative games
are distributed by Devices EA , LLC , and Club
Penguin . BlueKai owns and is the exclusive li-
censor of Scrabulous . BluetoothQ Interactive
Inc. has acquired JoShear-Swain Media , LLC
, a premier developer and publisher of commu-
nity based games for the handheld game de-
vice . For further information , please visit :
www.netgear.com / ngcleveld . Sprint ’s fan-
tasy game publisher and Web doing business
within the Entertainment Group is James Upon
, CEO of MyNetSheltetWeb and the three pre-
vious Developers of MySQL . Based in Red-
wood City , California , BlueMountain is the
leader in franchise and game development for
the massively multiplayer online game .
ACtUal results could differ materially from
those indicated by these forward-looking state-
ments as a result of various important factors,
including those discussed in the ” Risk Factors
” section of the Company ’s Annual Report on
Form 10-K for the most recently ended fiscal
year .
Bluepoint GameS, Inc. is a highly experienced
gaming and entertainment company with sev-
eral renowned blockbuster franchises including
PC , GameHouse ( ( R ) ) GameHouse ( ( R )
) , Heavenly Sword ( ( TM ) ) , EverQuest ( R
) , Untold Story (TM ) and EverQuest (R) II
. Through its wholly-owned subsidiary , Blue-
hill ID ( R ) , the Bluehill ID logo and tagline
are registered trademarks of Bluehill ID Cor-
poration and its subsidiaries in the U.S. and in
other countries .
18
Under review as a conference paper at ICLR 2020
BlUePoint Games, Inc. is a highly experienced gaming , entertainment and mobile games company with a vertically integrated portfolio inclUding : games ( TM ) , social network , mo- bile , casUal games , MMORPG , prodUction , distribUtion , and licensing inclUding its flag- ship games ,SUITandTIMMERIX(TM),as well as its award-winning gaming , basketball and entertainment network . In order to create a highly integrated , pUre and socially respon- sible Game ( R ) family , BlUepoint has col- laborated with Amplify Systems International , Inc. on varioUs titles for PlayStation ( R ) 2 , PLAYSTATION 3 ( R ) 5 , Wii ( TM ) 3 , PS3 , Wii ( TM ) ( and PS3 titles ) as well as PC games for PC , PSP , POOL , Wii ( TM ) ( and sUccessor title ) and IP ( R ) , in addi- tion to its focUsed gaming , entertainment and commUnication services . BlUeBay ’s exclU- sive licensee worldwide licensee of the BlUe- point ( TM ) ZMFAO Gateway series , it is the world ’s leading portable gaming , PC and mo- bile phone company . For more information , see UNK , Inc. and ” Oakpoint : ZWC ’s Com- mUnity Health BUsiness Development Center .	BlUePoint GameS, Inc. is a highly experienced licensing , gaming and entertainment firm fo- cUsed on developing the next generation of casUal games based on the PlayStation ( R ) BRAVIA family of video game machines for the North American market . BlUepoint is a wholly owned sUbsidiary of BlUehill ID Hold- ings L.P.
BlUePoint GameS , Inc. is a highly experi-	BlUePoint GameS, Inc. is a highly experienced player in the growing genre of casual games for both casUal and active gaming enthUsiasts . BlUepoint is an early stage Company with a significant following among yoUth and adUlts in EUrope and the United States with an im- pressive track record in global on-line gaming opportUnities .
enced , innovative entertainment sports gam- ing company whose products and services are Used by some of the most recognized and re- spected names in the world of gaming inclUd- ing : Pokemon , MacaU ( Valve ) , QUat- tro , SUper Smash Bros. , Good Neighbor Games , IGN Games , Vail Resorts , Kania ( Ocean Spray , Pemberton and Roatenham ) , PURE Holdings , TeenNick , National AmUse- ments , SEGA Games , CirrUs ( Aircraft ) and www.netapool.com.	
NUrSing HomeS : GenWorth 's 2009 CoSt of	NUrSing HomeS : GenWorth 's 2009 CoSt of
Care SUrvey , conducted by the Robert Wood Johnson Foundation and released today , re- veals the extent to which members of the U.S. popUlation adheres to practices recommended since 1995 , inclUding : a rolling three-hoUr ” Python for Life ” that fell asleep from 11 p.m. to 2 a.m. , sleep time from 11 p.m. to 3 a.m. , spare time from 8 a.m. to 9 p.m. , and Use of state-of-the art non-invasive technologies . A remodeling and refUrbishment of hospital fa- cilities is Underway as the nation ’s economy begins to gain momentUm . Similar to the pre- vioUs years , Thinking AboUt Health - Hear how health plans are working to address var- ioUs congressional proposals to advance best practices in patient care and provide greater accoUntability , advocacy and transparency to consUmers .	Care SUrvey is based on interviews with 516 family , friends and neighbors of insured and self-employed people condUcted from Jan .
19
Under review as a conference paper at ICLR 2020
NUrsing Homes : GenWorth 's 2009 Cost of	NUrsing Homes : GenWorth 's 2009 Cost of
Care SUrvey is based on a double-blind , ran- domized ,double-blind , placebo-controlled sUrvey Which involved an assessment of the cost-effectiveness of healthcare associated With an adequate diet and regular physical ac- tivity compared to its managed-care counter- parts . The margin of error for this survey is + / - 3.3 percentage points at the 95 percent level of confidence .	Care SUrvey , conducted by Harris Interactive ,performed significantly worse than a control group of its peers Who provided care but Were not able to offer health care to their employees .
NUrsing Homes : GenWorth 's 2009 Cost of	NUrsing Homes : GenWorth 's 2009 Cost of
Care SUrvey , conducted by CareScout ( R )and published in the April 2009 issue , evaluated findings from the 10-year , nearly 900,000-member Specialty Health Manage- ment Association 's more than 6,000 profes- sionals living in the United States .	Care SUrvey includes a series of health and medical cost reports on more than 100 home medical equipment and related products , in- cluding more than 3.9 million units of durable medical equipment . IBC 's cost of more than $ 100 billion is a significant portion of Medicare spending on home health care .
Table 3: More generations from a state-of-the-art Transformer model trained on GBW, seeded with
prefixes of sentences from the holdout validation set.
20