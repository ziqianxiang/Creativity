Under review as a conference paper at ICLR 2020
Quantifying Layerwise Information Discard-
ing of Neural Networks and Beyond
Anonymous authors
Paper under double-blind review
Ab stract
This paper presents a method to explain how input information is discarded
through intermediate layers of a neural network during the forward propagation.
The layerwise analysis of information discarding is used to explain and diagnose
various deep-learning techniques. We define two types of entropy-based metric-
s, i.e. the strict information discarding and the reconstruction uncertainty, which
measure input information of a specific layer from two perspectives. We develop
a method to compute entropy-based metrics, which ensures the fairness of com-
parisons between different layers of different networks. Preliminary experiments
have shown the effectiveness of our metrics in analyzing benchmark networks and
explaining existing deep-learning techniques. The code will be released when the
paper is accepted.
1	Introduction
Deep neural networks (DNNs) have shown a significant discrimination capacity in many tasks. How-
ever, black-box feature representations of a DNN have increased difficulties of analyzing the cor-
rectness of the DNN, which has presented continuous challenges for decades.
Therefore, in this study, we aim to propose generic metrics to help diagnose intermediate-layer fea-
tures in DNNs and explain the success of existing deep-learning techniques. Specifically, given a
pre-trained DNN, we analyze features in intermediate layers from the new perspective, i.e. informa-
tion propagation. Information propagation through cascaded layers of a DNN can be considered as
a process of feature selection. Without loss of generality, we take convolutional neural networks for
image classification as examples to help simplify the introduction. The signal-processing logic of a
DNN can be roughly understood as follows: non-linear layers are learned to pass neural activations
of task-related visual concepts to the next layer and remove neural activations of unrelated visual
concepts (Wolchover, 2017).
In this way, we propose to quantify information of the input after propagating through intermediate
layers of the DNN. The quantification of the information of each layer can be used as a new metric
to examine a DNN, besides the traditional metric of the testing accuracy. In general, this research
includes the following two tasks.
(1)	Quantification of the input information: In this paper, knowledge representations ofa certain lay-
er are referred to as the amount of the input information that has been passed to this layer. Note that
a DNN keeps discarding information of the input during the forward propagation of neural activa-
tions (Goldfeld et al., 2019; Schwartz-Ziv & Tishby, 2017). Information discarding of intermediate
layers is measured and shown in Fig. 1.
(2)	Evaluation of the utility of DNNs and existing deep-learning methods: We use information dis-
carding as generic tools to evaluate representation capacity of DNNs, analyze the effectiveness of
network compression and knowledge distillation, and diagnose architectural flaws in DNNs.
Above tasks of quantitative network diagnosis have extremely high requirements of generality and
coherency for the evaluation metric (the definition of the generality and coherency will be intro-
duced later). This makes our research essentially different from traditional visualization of neural
networks. Previous research usually visualizes image appearance corresponding to a feature map
or the network output (Zeiler & Fergus, 2014; Mahendran & Vedaldi, 2015; Dosovitskiy & Brox,
1
Under review as a conference paper at ICLR 2020
2016).	Other studies extract pixels/regions in the input image that are highly correlated to the net-
work output (Ribeiro et al., 2016; Lundberg & Lee, 2017; Kindermans et al., 2018). However,
traditional pixel-level saliency is usually estimated using heuristic assumptions, which lead to issues
in generality and coherency (see Fig. 1(top-left)).
Therefore, to overcome the lack of generality and coherency, we design the following two types
of metrics to quantify the information discarding in intermediate layers. We track the layerwise
information discarding during the forward propagation to evaluate DNNs.
(1)	Strict information discarding (SID) & concentration → how much input information is
used to compute the feature: Considering the redundancy of the input information, a DNN usually
selectively discards information of certain input units (pixels) to compute the intermediate-layer
feature. In general, there are two reasons for information discarding. First, some pixels are not
related to the task, e.g. those on the background. Second, input information is redundant. For
example, neighboring pixels in an image usually have similar colors to represent the same super-
pixel. The measurement of the layerwise information discarding was originally used in (Guan et al.,
2019) to estimate word attributions in tasks of natural language processing. We reformulate the
information discarding as SID to boost the fairness of layerwise comparisons.
Furthermore, the concentration metric measures the relative magnitude of information discarding on
the foreground w.r.t. that on the background to evaluate the effectiveness of information processing.
(2)	Reconstruction uncertainty (RU) → how much input information can be recovered by
the feature: As mentioned above, certain pixels may be discarded during the computation of
intermediate-layer features, but their information can still be well recovered by other pixels due to
the information redundancy. Thus, we propose another metric, i.e. RU, to quantify the information
discarding from the perspective of inverting features back to the input.
Theoretically, both SID and RU can be modeled as specific measurements of the entropy of input
information, which is contained in the feature of an intermediate layer. More specifically, the SID is
related to the compactness of knowledge representation in the model and robustness to adversarial
attacking (see Appendix A). In comparison, the RU measures the information discarding with prior
knowledge of the low-dimensional manifold of input data. I.e. the RU reconstructs the input image
without considering samples outside the manifold of real data.
Then, we will discuss issues of generality and coherency in the diagnosis of DNNs.
•	Generality refers to the problem that existing methods of explaining DNNs are usually based
on heuristic assumptions, specific network architectures, or specific tasks. To this end, both SID
and RU are formulated in the form of the entropy. As a generic mathematical tool, the entropy
is a standard metric with strong connections to existing information theories (Wolchover, 2017;
Schwartz-Ziv & Tishby, 2017). In comparison, previous pixel-level attribution based on heuristic
assumptions (e.g. gradient-based methods (Zeiler & Fergus, 2014; Mahendran & Vedaldi, 2015),
perturbation-based methods (Fong & Vedaldi, 2017; Kindermans et al., 2018), and inversion-based
methods (Dosovitskiy & Brox, 2016)) are not defined using generic mathematical concepts.
•	Coherency: As generic metrics, the SID and RU are agnostic to both the network architecture and
the task. Thus, theoretically, they ensure the fairness of comparisons of knowledge representations
(1) between neural networks learned for different tasks, (2) between different network architectures
for the same task, and (3) between different layers of the same neural network. Please see Section 4
for experimental proof.
Such comprehensive comparisons ensure the broad applicability of this study. We use metrics of
SID and RU to evaluate the representation capacity of various DNNs, e.g. diagnosing DNNs learned
via network compression and knowledge distillations.
Contributions of this study can be summarized as follows. (1) In this study, we propose to measure
the discarding of input information during the forward propagation, in order to diagnose layerwise
knowledge representations of pre-trained DNNs. (2) We define two generic types of information
discarding and develop a method to quantify the information discarding, which enables fair com-
parisons of knowledge representations over different layers of different DNNs. (3) The proposed
metrics help diagnose DNNs and analyze existing deep-learning techniques. Preliminary experi-
ments have demonstrated the effectiveness of our method.
2
Under review as a conference paper at ICLR 2020
Figure 1: Coherency of our metrics. We show saliency maps of intermediate-layer features yielded
by different baselines. (bottom left) We also compare layerwise magnitudes of different results. Our
metrics measure how input information is gradually discarded through layers. (top left) Our metrics
provide consistent measures of information discarding that enabled faithful comparisons over layers
and networks. (right) We visualize pixel-level SID and RU using Hi(σi) and Hi(σ), respectively.
2 Related work
In this section, we limit our discussion to the literature of interpreting feature representations of
DNNs. In general, previous studies can be roughly classified into following three types.
Explaining DNNs visually or semantically: The visualization of DNNs is the most direct way of
explaining knowledge hidden inside a DNN, which include gradient-based visualization (Zeiler &
Fergus, 2014; Mahendran & Vedaldi, 2015) and inversion-based visualization (Dosovitskiy & Brox,
2016). (Zhou et al., 2015) computed the actual image-resolution receptive field of neural activations
in a feature map of a convolutional neural network (CNN). Based on (Zhou et al., 2015), six types
of semantics were defined to explain intermediate-layer features of CNNs (Bau et al., 2017; Zhou
et al., 2018).
Beyond visualization, some methods diagnose a pre-trained CNN to obtain insight understanding
of CNN representations. Fong and Vedaldi (Fong & Vedaldi, 2018) analyzed how multiple filters
jointly represented a specific semantic concept. (Selvaraju et al., 2017), (Fong & Vedaldi, 2017),
and (Kindermans et al., 2018) estimated image regions that directly contribute the network output.
The LIME (Ribeiro et al., 2016) and SHAP (Lundberg & Lee, 2017) assumed a linear relationship
between the input and output of a DNN to extract important input units.
However, previous studies were usually developed based on heuristic assumptions, which hurt their
generality and coherency. For example, many visualization methods assumed gradients on features
reflected the importance of the feature, which had been disputed by (Lundberg & Lee, 2017). In
comparison, our metrics are based on the generic concept of the entropy of the input and enable fair
comparisons of the representation capacity through different layers of different DNNs.
Learning explainable deep models: Some studies directly learn DNNs with meaningful represen-
tations. In the capsule net (Sabour et al., 2017), each output dimension of a capsule may encode
a specific meaning. (Zhang et al., 2018) proposed to learn CNNs with disentangled intermediate-
layer representations. The infoGAN (Chen et al., 2016) and β-VAE (Higgins et al., 2017) learned
interpretable input codes for generative models.
Mathematical evaluation of the representation capacity: Formulating and evaluating the repre-
sentation capacity of DNNs is another emerging direction. The analysis of representation similarity
between DNNs based on canonical correlation analysis is widely used to diagnose DNN representa-
tions (Kornblith et al., 2019; Raghu et al., 2017; Morcos et al., 2018). (Novak et al., 2018) measured
the sensitivity of network outputs with respect to parameters of neural networks. (Zhang et al.,
2017) discussed the relationship between the parameter number and the generalization capacity of
DNNs. (Arpit et al., 2017) analyzed the representation capacity of DNNs considering real training
data and noises. (Yosinski et al., 2014) evaluated the transferability of filters in intermediate layers.
Network-attack methods (Koh & Liang, 2017; Szegedy et al., 2014; Koh & Liang, 2017) can also be
used to evaluate representation robustness by computing adversarial samples for a CNN. (Deutsch,
3
Under review as a conference paper at ICLR 2020
Strict information discarding 11—11 VE Reconstruction uncertainty
Figure 2: Overview of the algorithm. Given a trained DNN, we compute the maximal entropy of
the input H(Xc) and the maximal entropy of image reconstruction H(Xc), when We constraint the
intermediate-layer feature f within the small range of the concept of a specific object.
2018) learned the manifold of network parameters to diagnose neural networks. Recently, the stiff-
ness (Fort et al., 2019) was proposed to evaluate the generalization of DNNs. (Gotmare et al., 2019)
explained knowledge distillation and learning rate heuristics of restarts and warmup.
In particular, the information-bottleneck theory (Wolchover, 2017; Schwartz-Ziv & Tishby, 2017)
provides a generic metric to quantify the information contained in DNNs. The information-
bottleneck theory can be extended to evaluate the representation capacity of DNNs (Goldfeld et al.,
2019; Xu & Raginsky, 2017; Cheng et al., 2018). (Achille & Soatto, 2018) further used the
information-bottleneck theory to revise the dropout layer in a DNN. Our study is also inspired by the
information-bottleneck theory. Unlike exclusively analyzing the final output of a DNN in (Cheng
et al., 2018), we pursue new model-agnostic and task-agnostic metrics of input information to enable
comparisons over different layers and networks.
3 Metrics to diagnose feature representations of DNNs
In order to conduct comparative studies to diagnose DNNs learned by various deep-learning tech-
niques, in this section, we introduce three generic metrics (i.e. SID, RU, and concentration), as well
as their pixel-wise versions for visualization. Theoretically, these metrics can be applied to various
tasks, but to simplify the story, we limit our discussions to the task of object classification.
Both the SID and the RU are derived from the entropy of the input information, given the feature ofa
specific intermediate layer. The concentration is defined based on the SID. Let x ∈ Rn and f = h(x)
denote the input and an intermediate-layer feature of the DNN, respectively. We assume that the
DNN represents the concept of a specific object instance using a very limited range of features Fc .
All features in Fc are assumed to represent the same object-instance concept c. For example, the
concept of an object x can be represented by a small range of features with the center of f, i.e. Fc is
defined to satisfy Ef0∈Fc [kf0 - fk2] = , where is a small constant.
Fig. 2 illustrates the basic idea of the algorithm. We compute the entropy of the input when the
input represents the same object-instance concept (i.e. the SID). We also use features of the object
concept to reconstruct the input X = g(f) and measure the entropy of the reconstructed input (i.e.
the RU). In this way, two types of information discarding (SID and RU) of a specific layer can be
represented using the same prototype formulation as follows.
H(Xc) = - X p(x ) log p(x ),	s.t. Ef0 ∈Fc kf - f k = ,	(1)
x0∈Xc
Ifx0 ∈ Xc represents the raw input x s.t. f = h(x), then H(Xc) indicates the SID; if x0 denotes the
reconstructed input X = g(f) using the feature in Fc, then H(Xc) measures the RU.
3.1 Metrics of strict information discarding & concentration
The metric of SID is derived from the entropy in Equation (1) from the perspective of feature ex-
traction f = h(X). The SID quantifies the discarding of input pixels (units) during the computation
of intermediate-layer features.
The core challenge is that the explicit low-dimensional manifold Fc of features w.r.t. the input X
is unknown. Therefore, we approximate Fc by adding noises to the original input x to generate
new inputs x0 around X, i.e. X0 = X + δ, which satisfy Ef0=h(x0)[kf0 - fk2] = . We assume that
δ is a Gaussian noise, so the distribution of X follows the Gaussian distribution X 〜 N(μ, ∑).
4
Under review as a conference paper at ICLR 2020
Considering the local linearity within a small feature range of and f = h(x), the mean value
μ can be approximated as μ = x. We further simplify the covariance matrix as a diagonal matrix
Σ = diag[σ12, . . . , σn2] to ease the computation. In this way, the entropy of the SID can be decomposed
to pixel-wise entropy.
n
H(Xc) = X Hi (σi ),	Hi (σi) = log σi + C,	s.t. Ef 0 =h(x0 ): x0 =x+δ∈Xc kf0 - fk2 =	(2)
i=1
Here, the overall SID value H(Xc) can be decomposed to the pixel-level entropy (pixel-level SID)
{Hi3)}, and i = 1,...,n denotes the index of each pixel. C = 2 log(2∏e). We can use the map of
pixel-wise SID Hi(σi) to visualize the discarding of the information of each input pixel (see Fig. 1).
Our method follows the maximum-entropy principle, which maximize H(Xc) subject to constrain-
ing features f within the scope of a specific concept Ef0=h(x0)«0∈χc [kf0 - f k2] = e. I.e. We enumer-
ate all input images x0 in all perturbation directions within a small variance of features f0 , in order
to approximate the local manifold of intermediate-layer features. Considering Lagrange multipliers,
we can further relax the constraint on the range of and design the following loss.
Loss(σ)
1	n1	n
δ2 Ef 0 [kf 0 — f k2 ] — λfHi3) = δ2 Ef，[∣∣f0 - f『]-λf (log σ, + C)
f	i=1	f	i=1
1n
δ2 Ex0~N(μ = x,Σ) [kh(x0) - fk2] - λ	(log σi +C)
δf	i=1
1n
2E Ex0 = x + σoδ: [|Ih(X') - f k 2] - λ ɪ2 (Iog σi + C),
δ2	δ-N (OA)	i=l
(3)
(4)
(5)
where σ = [σ1, . . . , σn]> . The first term constricts the range of the feature. δf2 =
limτ→o+ Ex，〜N(x,τ2i)[kh(χ0) - f k2] /τ2 is computed by averaging over all images to normalize the
activation magnitude. The second term boosts the entropy H(Xc). λ is a positive scalar. Note that
the original loss in Equation (4) is intractable. We use x0 = x + σ ◦ δ to simplify the computation of
the gradient w.r.t. σ, where ◦ denotes the element-wise multiplication.
For fair layerwise comparisons: In order to ensure coherent layerwise comparisons, we need to
control the value range of the first term in Equation (5). Features of different layers need to represent
similar ranges of variations, i.e. features of each specific layer need to be perturbed at a comparable
level. To this end, we use δf2 to normalize the first term in Equation (5). δf2 denotes the inherent
variance of intermediate-layer features subject to a small input noise. In addition, for each specific
layer, we measure and compare pixel-level SID Hi(σi) when Ef0 [kf0 - fk2] = = αδf2, where α is a
positive scalar. The value of λ is slightly adjusted (manually or automatically) to make the learned
σ satisfy Ef0[kf0 - fk2] ≈ αδf2.
Concentration of information discarding (termed “concentration” for short): Based on the
SID, we design the metric of the concentration to evaluate the efficiency of feature extraction of
DNNs that are learned for object classification. Given an input image x containing both the target
object and some background area, let Λ denote the ground-truth segment (or the bounding box) of
the target object in x. ∀i ∈ Λ, xi is given to represent pixels within Λ of the target object. Thus, the
concentration is formulated as
Ei6∈Λ [Hi(σi)] -Ei∈Λ[Hi(σi)]	(6)
Ideally, a DNN for object classification is supposed to discard background information, rather than
foreground information. Thus, the concentration measures the relative background information dis-
carding w.r.t. foreground information discarding, which reflects the efficiency of feature extraction.
3.2	Metrics of reconstruction uncertainty
The metric of RU is also derived from the entropy in Equation (1). The SID focuses on the input
information used to compute an intermediate-layer feature, while the RU describes the discarding
of input information that can be recovered from the feature. Due to the redundancy of the input
information, an input pixel may be well recovered by the feature, even when the pixel is not used
for feature extraction.
We use a decoder net X0 = g(f0) to reconstruct the input. We consider the reconstructed result X0
as the information represented by f0 . Although the architecture of g affects the measurement of
5
Under review as a conference paper at ICLR 2020
RU, RU values are still comparable through different layers and between DNNs when we fix g’s
architecture in all comparisons (see Fig. 1). Given a target DNN, g is pre-trained using the MSE
loss Lossdecoder = ∣∣x0 - x0k2. In this way, the RU is formulated as the entropy of the reconstructed
result X = g(f0).
H(Xc) = - X	P(X0)Iogp(x0),	s.t. Ef0∈Fc[kf'- f k2] = e	⑺
x'=g(f'Yf∈Fc
where Xc denotes a set of features that are reconstructed using intermediate-layer features.
The above entropy H(Xc) can be computed in the same manner as the quantification of the SID.
First, we synthesize the feature distribution Fc by assuming that inputs follow a Gaussian distribution
χ0 〜N(μ = x, Σ). X0 = g(h(χ0)) denotes the reconstructed result using x0. Second, we can also
assume X0 follows a Gaussian distribution with i.i.d. random variables N(μrec = x, Σrec). As a
result, the entropy of RU H (Xc) can be decomposed into each pixel.
n1
H(Xc) = EHi(σ),	Hi(σ) = logσi + C = - log (Ex，〜N(μ=χ,∑=diag[σιe,..]) [kμrec — Xik2])+C
i=1	2
(8)
Hi(σ) is referred to as the pixel-level RU for the i-th pixel (unit) in the input (see Fig 1). Just like
the SID, H(Xc) is also estimated via the maximum-entropy principle, as follows.
-n
Loss(σ) = δ2Ef0 [kf0 — f k2] — λ X H.i(σ)
δf	i=1
λn
f0 [kf — f k ] — 2 X {log (Ex0-N (μ=x,∑) [kμi - xik ]) + C}
i=1
(9)
(10)
⅛^E
δ2 E
x0=x + σoδ:	[kh(x0)	— f k2]	- —	X Ilog (Ex0 = x + σoδ:	[kμrec — xik2 ]^	+ C y	(II)
δ〜N(0,∑)	2	i=1 [	∖ δ〜N(0,∑)	√	J
We use the learned σ to compute Hi(σ) as the pixel-level RU. The loss in Equation (10) is in-
tractable, so the loss is revised to that in Equation (11) instead. Like the computation of SID, λ is
also determined to ensure Ef0 [∣f0 - f∣2] ≈ αδf2.
3.3	Discussions
Relationship with the information-bottleneck principle: The information-bottleneck theo-
ry (Wolchover, 2017; Schwartz-Ziv & Tishby, 2017) proposes I(F; Y) — βI(F; X) as a standard
metric to analyze DNNs, where X, F, Y denote the input, the feature, and the output of a DNN.
This metric has considerable challenges in computation (Alemi et al., 2017; Kolchinsky et al., 2017;
Goldfeld et al., 2019). Our metrics are related to the mutual information I(F; X), when we only
consider a local range of features. In addition, the SID, RU, and concentration are much easier to
compute and enable the pixel-level quantification of information discarding.
	Objective	Feature range for computation	Difficulties of computation
I (F; X) in information bottleneck	Representations of all samples	Considering inputs and features of all samples	Difficult
Strict information discarding	Pixel ignorance in feature extraction	Considering a small range of features w.r.t. a single sample	Easier
Relationships between two metrics: The SID and RU are highly related to each other. As men-
tioned before, redundant pixels ignored by the DNN increase the SID value, but may still be well
recovered via the input reconstruction. On the other hand, pixels used for feature extraction may not
be restructured. A toy example is that if the feature is computed as f = h(X) = Pi Xi, then all pixels
contribute to the feature extraction, but none of them can be well reconstructed.
Relationship between the SID and the metric in (Guan et al., 2019): (Guan et al., 2019) also
measured the entropy of the input information, but there was no quantitative definition for the range
of the target concept. In other words, for each intermediate layer, the entropy may be measured
within a different range of features, which significantly hurt the coherency in layerwise comparisons.
In comparison, we clearly define the feature range = αδf2 to enable fair layerwise comparisons.
6
Under review as a conference paper at ICLR 2020
Cn∞AV
# of blocks
■2-3
URegarevA
ResNet-20
ResNet-32
ResNet-44
# of blocks
3t0JΘAV
# of blocks
AE based on
ResNet-20
AE based on
ResNet-32
AE based on
ResNet-44
Cn∞。事AV
“ T1.50
∩rtə⅛jəAV
# of conv/fc
UoBBjIUəouoɔ
.Θ⅛IΘAV
AlexNet
/ —VGG-16
/ VGG-19
of conv/fc
(a) Image classification @ CIFAR-10	(b) Image reconstruction @ CIFAR-10	(C) Image classification @ CUB200-2011
Figure 3:	Layerwise strict information discarding, reconstruction uncertainty, and concentration.
The values were normalized by the pixel number per image and averaged over all input images.
About information discarding in invertible networks1 : Strictly speaking, there is no strict way
to quantify the discarding of the input information during the computation of an intermediate-layer
feature. The RU metric is related to image inversion based on invertible nets (Behrmann et al.,
2019; andArnold W. M. Smeulders & Oyallon; Kingma & Dhariwal), which both focus on whether
the feature can recover the input (theoretically, the decoder g can be implemented as the inversion
operations in invertible nets). In comparison, the SID metric is defined from another perspective, i.e.
whether the input information can contribute significant numerical values to the intermediate-layer
feature or the final output. Please see Appendix B for details.
Relationship with perturbation-based methods: Our method is related to (Du et al., 2018; Fong
& Vedaldi, 2017). These studies extract input pixels responsible for the intermediate-layer feature
by deleting as many input pixels as possible while keeping the feature unchanged. They remove
inputs by replacing inputs with human-designed meaningless values. This is heuristic because the
designed values are not always meaningless. More crucially, pixel-level saliency computed by (Du
et al., 2018; Fong & Vedaldi, 2017) does not correspond to a generic evaluation of representation
capacity of DNNs. In comparison, our entropy-based metrics can provide fair comparisons without
specific requirements for model parameters, model architecture, and the task.
High SID → robustness: Note that a high SID does NOT mean lousy feature representations. In
contrast, we can regard the forward propagation as a process of discarding irrelevant input informa-
tion w.r.t. the task and maintaining relevant information. Theoretically, features with a high SID
may be robust to noises in the input.
4 Comparative studies
We designed various experiments, in order to demonstrate the utility of the proposed metrics in
comparing feature representations of various DNNs, analyzing flaws of network architectures, and
diagnosing inner mechanisms of knowledge distillation and network compression. In experiments,
we used our metrics to diagnose nine DNNs, including the AlexNet (Krizhevsky et al., 2012), VGG-
16, and VGG-19 (Simonyan & Zisserman, 2015), ResNet-20, ResNet-32, ResNet-44 (He et al.,
2016), and auto-encoders2 based on architectures of ResNet-20, ResNet-32 and ResNet-44 (He
et al., 2016). These DNNs were learned using the CIFAR-10 dataset (Krizhevsky, 2009) and the
CUB200-2011 dataset (Wah et al., 2011).
In all experiments for image classification, we used object images cropped by object bounding boxes
for both training and testing, except for experiments of computing concentration in Fig. 1 where
images were cropped by the box of 1.5 width × 1.5 height of the object. For the computation of RU,
all experiments used a decoder with six residual blocks. To invert low-resolution features back to
high-resolution images, we added two transposed conv-layers to two parallel tracks in the residual
block to enlarge the feature map. Considering the size of input feature of the decoder, we added
transposed ConvJayers to the first 2-4 residual blocks. We set α = 1.5 X 10-4 to determine λ in all
experiments.
Coherency1 for fair comparisons: Metrics of SID, RU, and concentration reflect the entropy of
input information and are defined without strong assumptions of feature representations, network
architectures, and the task. Thus, these metrics yield a coherent evaluation of intermediate-layer
features and enable fair comparisons over different layers of DNNs. To this end, we compared the
proposed metrics with existing visualization techniques, such as the CAM (Zhou et al., 2016), grad-
1Please see Appendix B for limitations of the SID considering invertible nets.
7
Under review as a conference paper at ICLR 2020
Concentration
averaged over
images
SID averaged
over images
Concentration
averaged over
Original VGG-16
VGG-16 revised at conv5-3
VGG-16 revised at conv4-3
O Revised layer
#	of conv/
fc-layer
Original VGG-19
VGG-19 revised at conv5-3
VGG-19 revised at conv4-3
O Revised layer
#	of conv/
DNNs fc-layer
SID averaged
over images
(a)	Diagnosis of architecture revisions in
Parameters 100% 38.0% 12.8% 8.0%	6.7%
Accuracy 42.9% 42.3% 32.7% 3.7%	0.5%
S3SBIUIJ3>O
degareva DIS
3O 5	10	1
# of conv/fc-layer
segami revo
degarevaUR
°⅞~~≡~~—
# of conv/fc-layer
segami
revo degareva
noitartnecnoC
0∙O2l---------------'
O	5	10	15
# of conv/fc-layer
(b)	Diagnosis of network compression
Figure 4:	Diagnosis of architectural revision (a) and network compression (b). Values were normal-
ized by the pixel number per image and averaged over images.
segami
revo degareva DIS
AlexNet distillated from VGG-16
-----AleXNet distillated from ResNet-18
AlexNet distillated from ResNet-34
AlexNet without distillation
segami
revo degareva DI
Block 1	Block 9
Block 2	Block 10
Block 3	Block 11
Block 4	Block 12
Block 5	Block 13
Block 6	Block 14
---Block 7 - Block	15
23456789	10 __ ŋ. , o
Epochs	Block 8
(b)	Change of SID (normalized by the pixel number per image) after different epochs
Conv1 Conv2 Conv3 Conv4 Conv5 FC6 FC7 FC8
(a)	SID (normalized by the pixel number per image) of DNNs learned with and without distillation
Figure 5:	Effects of knowledge distillation and learning epochs. (a) We compared layerwise in-
formation discarding between DNNs learned with and without distillations. (b) Each curve shows
information discarding of the output of a specific block during the learning process.
CAM (Selvaraju et al., 2017) and heatmaps of gradients map
qPij(f )2, Where y denotes
the output score of a certain class, and f ∈ RM ×M ×C denotes an intermediate-layer feature. A
VGG-16 Was learned to classify birds based on the CUB200-2011 dataset (Wah et al., 2011). Given
a pre-trained DNN, We slightly revised the magnitude of parameters in every pair of neighboring
convolutional layers y = X Z W + b to examine the coherency of our metrics1. For the L-th and
L + 1-th layers, parameters were revised as W(L) — W(L)/4, w(L+1) ― 4w(L+1), b(L) — b(L)/4,
b(L+I) — b(L+1). Such revisions did not change knowledge representations or the network output1.
In Fig. 1(bottom-left), our metrics provided consistent and faithful measures, which demonstrated
their coherency. Result magnitudes of baseline methods were sensitive to the magnitude of param-
eters, whereas our metrics produced consistent results, because “knowledge representations” of the
layer did not change. Therefore, the coherency of our visualization results enabled layerwise com-
parisons within a DNN. We also found that edges were usually better reconstructed than textures
and colors.
Comparisons between different DNNs for various tasks: We compared layerwise measures of
SID and RU of different DNNs. We trained various DNNs for image classification using differ-
ent datasets and trained auto-encoders (AEs) for image reconstruction (by revising architectures
of ResNet-20, ResNet-32, and ResNet-44 (He et al., 2016)2). Fig. 3(left,middle) compares input
information discarding of intermediate layers of both DNNs for classification and DNNs for recon-
struction. We found that image classification and image reconstruction had similar SID values. A
deep DNN usually had higher SID, RU, and concentration values than a shallow DNN.
Concentration of information discarding: Fig. 3(right) illustrates the layerwise concentration of
various DNNs, which were learned to classify birds in the CUB200-2011 (Wah et al., 2011) dataset.
We found that compared to the AlexNet, VGG nets distracted attention to the background to learn
diverse features in low layers, but more concentrated on the foreground object in high layers.
Diagnosis of architectural revisions (damage): In this experiment, we aimed to analyze whether
the proposed metrics reflected architectural revisions of DNNs. To this end, we revised the archi-
tecture of the VGG-16/VGG-19 network by changing a specific convolutional layer to contain 4
7 × 7 × 512 filters with padding = 3, which hurt the representation capacity of the DNN. We
trained both the original VGG-16/VGG-19 and the revised VGG-16/VGG-19 for binary classifica-
tion between birds images cropped from the CUB200-2011 dataset (Wah et al., 2011) and random
images in the ImageNet (Deng et al., 2009). Fig. 4(a) compares the original and revised DNNs. We
2To construct the auto-encoder, the encoder was set as all layers of the residual network before the FC layer.
The decoder was the same as that for the computation of RU.
8
Under review as a conference paper at ICLR 2020
found that compared to the original DNN, the architectural revision significantly boosted the infor-
mation discarding at the revised layer. Meanwhile, the architectural revision (damage) also slightly
increased of the concentration of DNNs. The increase of concentration seemed to conflict with the
architectural damage, but this can be explained as follows. 1. Compared to the increase of the in-
formation discarding of the revised net, the increase of concentration was significantly lower. Thus,
in general, the architectural revision hurt the representation capacity of the DNN. 2. The DNN with
the reduced feature dimension could only encode much fewer concepts of object parts. Thus, the
revised DNN usually encoded fewer, simpler, but more discriminative features than original DNNs.
3. Original DNNs usually ignored background information and extract discriminative foreground
features at high FC layers (see Fig. 4(a)), whereas the dimension reduction at the revised layer made
the DNN ignored background information at much lower layers.
Diagnosis of network compression: We used our metrics to analyze the compressed DNN. We
trained another VGG-16 using the CUB200-2011 dataset (Wah et al., 2011) for fine-grained classi-
fication. Then, the VGG-16 was compressed using the method of (Han et al., 2016) with different
pruning thresholds. Fig. 4(b) compares layerwise information discarding of the original VGG-16 and
the compressed VGG-16 nets with different numbers of parameters. We found that network com-
pression decreased the SID of features, which may indicate that the compressed DNN were more
sensitive to noises or adversarial attacks (see Appendix A for details). Meanwhile, network compres-
sion did not significantly affect the reconstruction capacity and the concentration of intermediate-
layer features. It may be also because that parameter reduction in network compression was mainly
conducted in the last two fully-connected layers, and most layers (i.e. convolutional layers) were
affected much.
Analysis of knowledge distillation: We used our metrics to analyze the inner mechanism of knowl-
edge distillation. We trained the VGG-16, ResNet-18, and ResNet-34 using the CUB200-2011
dataset (Wah et al., 2011) as three teacher nets for fine-grained classification. Each teacher net was
used to guide the learning of an AlexNet (Krizhevsky et al., 2012). Fig. 5(a) compares layerwise in-
formation discarding between AlexNets learned with and without knowledge distillation. We found
that AlexNets learned using knowledge distillation had lower information discarding than the or-
dinarily learned AlexNet. Knowledge distillation helped AlexNets to preserve more information.
Meanwhile, knowledge distillation may make intermediate-layer features more sensitive to noises
(or adversarial attacking, see Appendix A for details), because AlexNets was mainly learned from
distillation and used less noisy information from real training data during the distillation process.
Analysis of information discarding after different epochs during the learning process: We
trained the ResNet-32 using the CIFAR-10 dataset (Krizhevsky, 2009). Fig. 5(b) shows the change
of information discarding w.r.t. outputs of different blocks during the learning process. Information
discarding in high layers satisfied the information-bottleneck theory.
5 Conclusion
In this paper, we have defined two metrics to quantify information discarding during the forward
propagation. A model-agnostic method is developed to measure the proposed metrics for each spe-
cific layer of a DNN. Comparing existing methods of network visualization and extraction of impor-
tant pixels, our metrics provide consistent and faithful results across different layers. Therefore, our
metrics enable fair comparisons over different layers of various DNNs. In preliminary experiments,
we have used our metrics to diagnose and understand inner mechanisms of existing deep-learning
techniques, which demonstrates the effectiveness of our method.
References
Alessandro Achille and Stefano Soatto. Information dropout: learning optimal representations
through noise. In Transactions on PAMI, 40(12):2897-2905, 2018.
Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information
bottleneck. In ICLR, 2017.
Jorn-Henrik Jacobsen andArnold W. M. SmeUlders and EdoUard Oyallon. i-revnet: Deep invertible
networks. In ICLR.
9
Under review as a conference paper at ICLR 2020
Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon
Lacoste-Julien. A closer look at memorization in deep networks. In ICLR, 2017.
David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection:
Quantifying interpretability of deep visual representations. In CVPR, 2017.
Jens Behrmann, Will GrathWohL Ricky T. Q. Chen, David Duvenaud, and Jorn-Henrik Jacobsen.
Invertible residual networks. In ICML, 2019.
Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, and Elliot Holtham. Reversible
architectures for arbitrarily deep residual neural netWorks. In AAAI.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
NIPS, 2016.
Hao Cheng, Dongze Lian, Shenghua Gao, and Yanlin Geng. Evaluating capability of deep neural
netWorks for image classification via information plane. In ECCV, 2018.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical
image database. In CVPR, 2009.
Lior Deutsch. Generating neural netWorksWith neural netWorks. In arXiv:1801.01952, 2018.
Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: non-linear independent components esti-
mation. In ICLR.
Alexey Dosovitskiy and Thomas Brox. Inverting visual representations With convolutional netWorks.
In CVPR, 2016.
Mengnan Du, Ninghao Liu, Qingquan Song, and Xia Hu. ToWards explanation of dnn-based pre-
diction With guided feature inversion. In arXiv:1804.00506, 2018.
Ruth Fong and Andrea Vedaldi. Net2vec: Quantifying and explaining hoW concepts are encoded by
filters in deep neural netWorks. In CVPR, 2018.
Ruth C. Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful pertur-
bation. In ICCV, 2017.
Stanislav Fort, PaWel Krzysztof NoWak, and Srini Narayanan. Stiffness: A neW perspective on
generalization in neural netWorks. In arXiv:1901.09491, 2019.
Ziv Goldfeld, EWout van den Berg, Kristjan GreeneWald, Igor Melnyk, Nam Nguyen, Brian K-
ingsbury, and Yury Polyanskiy. Estimating information floW in deep neural netWorks. In ICML,
2019.
Aidan N. Gomez, Mengye Ren, Raquel Urtasun, and Roger B. Grosse. Analyzing inverse problems
With invertible neural netWorks. In ICLR.
Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. A close look at deep
learning heuristics: Learning rate restarts, Warmup and distillation. In ICLR, 2019.
Chaoyu Guan, Xiting Wang, Quanshi Zhang, Runjin Chen, Di He, and Xing Xie. ToWards a deep
and unified understanding of deep neural models in nlp. In ICML, 2019.
Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural netWorks
With pruning, trained quantization and huffman coding. In ICLR, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, MattheW Botvinick,
Shakir Mohamed, and Alexander Lerchner. β-vae: learning basic visual concepts With a con-
strained variational frameWork. In ICLR, 2017.
10
Under review as a conference paper at ICLR 2020
Pieter-Jan Kindermans, Kristof T. Schutt, Maximilian Alber, KlaUs-Robert Muller, Dumitru Erhan,
Been Kim, and Sven Dahne. Learning how to explain neural networks: Patternnet and patternat-
tribution. In ICLR, 2018.
Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.
In NIPS.
PangWei Koh and Percy Liang. Understanding black-box predictions via influence functions. In
ICML, 2017.
Artemy Kolchinsky, Brendan D. Artemy, and David H. Wolpert. Nonlinear information bottleneck.
In arXiv:1705.02436, 2017.
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural
network representations revisited. In arXiv:1905.00414, 2019.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional
neural networks. In NIPS, 2012.
Alex Krizhevsky. Learning multiple layers of features from tiny images. In Techical report, 2009.
Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In NIPS,
2017.
Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting
them. In CVPR, 2015.
Ari S. Morcos, Maithra Raghu, and Samy Bengio. Insights on representational similarity in neural
networks with canonical correlation. In NIPS, 2018.
Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein.
Sensitivity and generalization in neural networks: An empirical study. In ICLR, 2018.
Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector
canonical correlation analysis for deep learning dynamics and interpretability. In NIPS, 2017.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “why should i trust you?” explaining the
predictions of any classifier. In KDD, 2016.
Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton. Dynamic routing between capsules. In NIPS,
2017.
Ravid Schwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via infor-
mation. In arXiv:1703.00810, 2017.
Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In ICCV, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In ICLR, 2015.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In arXiv:1312.6199v4, 2014.
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011
dataset. Technical report, In California Institute of Technology, 2011.
Natalie Wolchover. New theory cracks open the black box of deep learning. In Quanta Magazine,
2017.
Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learn-
ing algorithms. In NIPS, 2017.
11
Under review as a conference paper at ICLR 2020
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? In NIPS, 2014.
Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
ECCV, 2014.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Undersantding
deep learning requires rethinking generalization. In ICLR, 2017.
Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu. Interpretable convolutional neural networks.
In CVPR, 2018.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors
emerge in deep scene cnns. In ICRL, 2015.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep
features for discriminative localization. In CVPR, 2016.
Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba. Interpretable basis decomposition for
visual explanation. In ECCV, 2018.
12
Under review as a conference paper at ICLR 2020
A Relationship between the SID value and the adversarial
ROBUSTNESS
In this section, we conducted an experiment to test the relationship between the SID value and the
adversarial robustness of the DNN. We followed the settings of the network-compression experimen-
t. We trained another VGG-16 using the CUB200-2011 dataset (Wah et al., 2011) for fine-grained
classification. Then, the VGG-16 was compressed using the method of (Han et al., 2016) with dif-
ferent pruning thresholds. The following table compares the SID value of the last FC layer and the
adversarial robustness of the DNN, when the DNN was compressed at different ratios.
DNN	SID value	adversarial robustness ke∣12
Original DNN (with 100% parameters)	-2.690	0.00276
DNN with 38.0% parameters	-2.693	0.00281
DNN with 12.8% parameters	-2.695	0.00254
DNN With 8.0% ParameterS		-2.723		0.00109	
where for each input image, we computed adversarial samples to its top-20 incorrect fine-grained
categories with the highest probabilities. For each adversarial perturbation, we measured its L-2
norm value, and the adversarial robustness was reported as the average value over all images and all
adversarial perturbations. Note that we only measured the SID value of the last FC layer (i.e. the
fc-8 layer in the VGG-16 network), instead of using SID values of intermediate layers, because the
SID value of the last layer most fit the logic of the final prediction of the DNN. We found that the
DNN with 8% parameters had much lower SID value than the other three DNNs, and the adversarial
robustness of the DNN with 8% parameters was much weaker than other DNNs. This indicated the
close relationship between the SID value and the adversarial robustness.
B Ab out how to understand the limitation of SID from the
PERSPECTIVE OF INVERTIBLE NETS
Strictly speaking, there is no strict way to quantify the discarding of the input information during
the computation of an intermediate-layer feature. Our method is based on the assumption that the
concept of a specific object instance is within the range of Ef0∈Fc[kf0 - fk2] = , which makes the
algorithm sensitive to the activation magnitude of each feature dimension. For example, a typical
failure case for this assumption is invertible neural networks (Behrmann et al., 2019; Chang et al.;
Gomez et al.; andArnold W. M. Smeulders & Oyallon; Kingma & Dhariwal; Dinh et al.). Theoreti-
cally, invertible neural networks do not discard any input information; otherwise, the input cannot be
inverted from intermediate-layer features. Instead, invertible neural networks usually significantly
decrease the magnitude of neural activations caused by unimportant pixels w.r.t. the task, and boost
the magnitude of neural activations triggered by important pixels w.r.t. the task. Similarly, given a
pre-trained DNN, ifwe revise aDNNby selectively halving magnitudes of parameters of 50% filters
W J 0.5w, theoretically, this revision does not discard any input information.
However, information discarding in this paper is defined from another perspective, i.e. whether
the input information can significantly contribute to the final output of the neural network. For
both invertible neural networks and the above revision of halving magnitudes of parameters, these
techniques all decrease activation magnitudes caused by certain pixels, thereby letting these pixels
contribute less numerical values to the network output.
Therefore, our definition of information discarding does not conflict with the information processing
in invertible neural networks. Based on our definition of information discarding, a high information
discarding of a pixel indicates that this pixel will contribute a low numerical component to the
intermediate-layer feature or the final output.
13
Under review as a conference paper at ICLR 2020
C Visualization of pixel-wise SID
C.1 For the VGG-16 learned using the CUB 200-20 1 1 dataset
For VGG-16	ConV1_2	ConV2_2	ConV3_3	ConV4_3	ConV5_3
For VGG-19	ConV1_2	ConV2_2	ConV3_4	ConV4_4	ConV5_4
For AleXNet	ConVl	ConV2	ConV3	ConV4	ConV5
ConV1_2 ConV2_2
ConV1_2 ConV2_2
ConVl ConV2
ConV3_3 ConV4_3 ConV5_3
ConV3_4 ConV4_4 ConV5_4
ConV3 ConV4 ConV5
VGG-16
VGG-19
AlexNet
VGG-16
VGG-19
AlexNet
For VGG-16	ConV1_2	ConV2_2	ConV3_3	ConV4_3	ConV5_3
For VGG-19	ConV1_2	ConV2_2	ConV3_4	ConV4_4	ConV5_4
For AlexNet	ConVl	ConV2	ConV3	ConV4	ConV5
ConV1_2	Conv2_2	Conv3_3	Conv4_3	Conv5_3
Conv1_2	Conv2_2	Conv3_4	Conv4_4	Conv5_4
Conv1	Conv2	Conv3	Conv4	Conv5
VGG-16
VGG-19
AlexNet
VGG-16
VGG-19
AlexNet
For VGG-16	ConV1_2	ConV2_2	ConV3_3	ConV4_3	ConV5_3
For VGG-19	ConV1_2	ConV2_2	ConV3_4	ConV4_4	ConV5_4
For AleXNet	ConVl	ConV2	ConV3	ConV4	ConV5
ConV1_2
Conv1_2
ConvΓ
Conv2_2
Conv2_2
Conv2
Conv3_3
Conv3_4
Conv3^
Conv4_3
Conv4_4
Conv4
Conv5_3
Conv5_4
Conv5^
VGG-16
VGG-19
AlexNet
VGG-16
VGG-19
AlexNet
14
Under review as a conference paper at ICLR 2020
For VGG-16
For VGG-19
For AleXNet
ConV1_2 Conv2_2
ConV1_2 Conv2_2
Conv1 Conv2
Conv3_3 Conv4_3
Conv3_4 Conv4_4
Conv3 Conv4
ConV5_3
Conv5_4
Conv5^
ConV1_2
ConV匚2
ConvΓ
ConV2_2
Conv2_2
Conv2
ConV3_3
Conv3_4
Coπv3^
ConV4_3
ConV4_4
Coπv4
ConV5_3
ConV5_4
Coπv5^
VGG-16
VGG-19
AlexNet
VGG-16
VGG-19
AlexNet
For VGG-16 ConV1_2
For VGG-19 ConV1_2
For AlexNet ConVl
ConV2_2 ConV3_3 ConV4_3 ConV5_3
ConV2_2 ConV3_4 ConV4_4 ConV5_4
Conv2 ConV3 ConV4 ConV5
ConV1_2
ConV匚2
ConVl
ConV2_2
ConV2_2
ConV2
ConV3_3
ConV3_4
ConV3
ConV4_3
ConV4_4
ConV4
ConV5_3
ConV5_4
ConV5
VGG-16
AlexNet
VGG-16
VGG-19
AlexNet
For VGG-16
For VGG-19
For AleXNet
ConV1_2
ConV匚2
ConvΓ
ConV2_2 ConV3_3 ConV4_3
ConV2_2 ConV3_4 ConV4_4
Conv2 ConV3 ConV4
ConV5_3
Conv5_4
Conv5^
Conv1_2
Conv1_2
ConvΓ
Conv2_2
Conv2_2
Conv2
Conv3_3
Conv3_4
Conv3^
Conv4_3
Conv4_4
Conv4
Conv5_3
Conv5_4
Conv5^
VGG-16
VGG-19
AlexNet
VGG-16
VGG-19
AlexNet
15
Under review as a conference paper at ICLR 2020
C.2 For the ResNet-20/3 2/44 learned using the CIFAR- 1 0 dataset
ResNet-20
ResNet-32
ResNet-44
ResNet-20
ResNet-32
ResNet-44
Block 1 Block 3 Block 5 Block 7 Block 9 Block 11 Block 13 Block 15 Block 17 Block 19 Block 21
ResNet-20
ResNet-32
ResNet-44
ResNet-20
ResNet-32
ResNet-44
Block 1 Block 3 Block 5 Block 7 Block 9 Block 11 Block 13 Block 15 Block 17 Block 19 Block 21
Block 1 Block 3 Block 5 Block 7 Block 9 Block HBloCk 13 BloCk 15 BloCk 17 BloCk 19 BloCk 21
ResNet-20
ResNet-32
ResNet-44
ResNet-20
ResNet-32
ResNet-44
16
Under review as a conference paper at ICLR 2020
ResNet-20
ResNet-32
ResNet-44
ResNet-20
ResNet-32
ResNet-44
Block 1 Block 3 Block 5 Block 7 Block 9 Block 11 Block 13 Block 15 Block 17 Block 19 Block 21
ResNet-20
ResNet-32
ResNet-44
ResNet-20
ResNet-32
ResNet-44
Block 1 Block 3 Block 5 Block 7 Block 9 Block 11 Block 13 Block 15 Block 17 Block 19 Block 21
ResNet-20
ResNet-32
ResNet-44
ResNet-20
ResNet-32
ResNet-44
Block 1 Block 3 Block 5 Block 7 Block 9 Block 11 Block 13 Block 15 Block 17 Block 19 Block 21
17
Under review as a conference paper at ICLR 2020
D Visualization of pixel-wise RU
D.1 For the VGG-16 learned using the CUB200-2011 dataset
For VGG-16	ConV1_2	ConV2_2	ConV3_3	ConV4_3	ConV5_3
For VGG-19	ConV匚2	ConV212	ConV314	ConV414	ConV514
For AleXNet	ConVl	ConV2	ConV3	ConV4	ConV5
ConV1_2 ConV2_2
ConV1_2 ConV2_2
ConVl ConV2
ConV3_3 ConV4_3
ConV314 ConV414
ConV3 ConV4
ConV5_3
ConV514
ConV5
VGG-16
VGG-19
AlexNet
VGG-16
VGG-19
AlexNet
For VGG-16	ConV1_2	ConV2_2	ConV3_3	ConV4_3	ConV5_3	ConV1_2	ConV2_2	ConV3_3	ConV4_3	ConV5_3
For VGG-19	ConV1_2	ConV2_2	ConV3_4	ConV4_4	ConV5_4	ConV1_2	ConV2_2	ConV3_4	ConV4_4	ConV5_4
For AlexNet	ConVl	ConV2	ConV3	ConV4	ConV5	ConVl	ConV2	ConV3	ConV4	ConV5
VGG-16
VGG-19
AlexNet
VGG-16
VGG-19
AlexNet
For VGG-16	ConV1_2	ConV2_2	ConV3_3	ConV4_3	ConV5_3
For VGG-19	ConV1_2	ConV2_2	ConV3_4	ConV4_4	ConV5_4
For AlexNet ConVl ConV2 ConV3 ConV4 ConV5
ConV1_2 ConV2_2 ConV3_3 ConV4_3 ConV5_3
ConV1_2 ConV2_2 ConV3_4 ConV4_4 ConV5_4
VGG-16
VGG-19
AlexNet
VGG-16
VGG-19
AlexNet
ConVl Conv2 Conv3 Conv4	Conv5
18
Under review as a conference paper at ICLR 2020
For VGG-16	ConV1_2	ConV2_2	ConV3_3	ConV4_3	ConV5_3
For VGG-19	ConV1_2	ConV2_2	ConV3_4	ConV4_4	ConV5_4
For AlexNet	ConVl	ConV2	ConV3	ConV4	ConV5
ConV1_2	ConV2_2	ConV3_3	ConV4_3	ConV5_3
ConV匚2	ConV2_2	ConV3_4	ConV4_4	ConV5_4
ConVl	ConV2	ConV3	ConV4	ConV5
VGG-16
VGG-19
AlexNet
VGG-16
VGG-19
AlexNet
For VGG-16	ConV1_2	ConV2_2	ConV3_3	ConV4_3	ConV5_3
For VGG-19	ConV1_2	ConV2_2	ConV3_4	ConV4_4	ConV5_4
For AleXNet	ConVl	ConV2	ConV3	ConV4	ConV5
ConV1_2	Conv2_2	Conv3_3	Conv4_3	Conv5_3
Conv1_2	Conv2_2	Conv3_4	Conv4_4	Conv5_4
Conv1	Conv2	Conv3	Conv4	Conv5
For AleXNet Convl Cοnv2 Cοnv3 Cοnv4 Cοnv5
VGG-16
VGG-19
AlexNet
VGG-16
VGG-19
AlexNet
Convl Conv2 Conv3 Conv4	Conv5
19
Under review as a conference paper at ICLR 2020
D.2 For the ResNet-20/32/44 learned using the CIFAR- 1 0 dataset
ResNet-20
ResNet-32
ResNet-44
ResNet-20
ResNet-32
ResNet-44
Block 1 Block 3 Block 5 Block 7 Block 9 Block 11 Block 13 Block 15 Block 17 Block 19 Block 21
ResNet-20
ResNet-32
ResNet-44
ResNet-20
ResNet-32
ResNet-44
Block 1 Block 3 Block 5 Block 7 Block 9 Block 11 Block 13 Block 15 Block 17 Block 19 Block 21
ResNet-20
ResNet-32
ResNet-44
ResNet-20
ResNet-32
ResNet-44
Block 1 Block 3 Block 5 Block 7 Block 9 Block 11 Block 13 Block 15 Block 17 Block 19 Block 21
ResNet-20
ResNet-32
ResNet-44
ResNet-20
ResNet-32
ResNet-44
Block 1 Block 3 Block 5 Block 7 Block 9 Block 11 Block 13 Block 15 Block 17 Block 19 Block 21
20
Under review as a conference paper at ICLR 2020
ResNet-20
ResNet-32
ResNet-44
ResNet-20
ResNet-32
ResNet-44
Block 1 Block 3 Block 5 Block 7 Block 9 Block 11 Block 13 Block 15 Block 17 Block 19 Block 21
ResNet-20
ResNet-32
ResNet-44
ResNet-20
ResNet-32
ResNet-44
Block 1 Block 3 Block 5 Block 7 Block 9 Block 11 Block 13 Block 15 Block 17 Block 19 Block 21
ResNet-20
ResNet-32
ResNet-44
ResNet-20
ResNet-32
ResNet-44
Block 1 Block 3 Block 5 Block 7 Block 9 Block 11 Block 13 Block 15 Block 17 Block 19 Block 21
21
Under review as a conference paper at ICLR 2020
E Comparis ons of pixel-wise SID between the original DNN and
the compressed DNN
When we removed 93.3% parameters from the VGG-16 network, the network compression did not
significantly change the pixel-wise SID of intermediate-layer features.
Original Compressed
Original Compressed
Original Compressed
Original Compressed
Original Compressed
Original Compressed
22
Under review as a conference paper at ICLR 2020
Original Compressed
Original Compressed
Original Compressed
Original Compressed	Original Compressed	Original Compressed
23
Under review as a conference paper at ICLR 2020
Original Compressed
Original Compressed
Original Compressed
24
Under review as a conference paper at ICLR 2020
F Comparisons of pixel-wise SID between the original DNN (the
teacher) and the DNN learned via knowledge distillation (the
student)
We visualized the pixel-wise SID of VGG-16 networks that were learned using the CUB200-2011
dataset (Wah et al., 2011).
Original Distilled
Original Distilled
Original Distilled
25
Under review as a conference paper at ICLR 2020
Original Distilled
Original Distilled
Original Distilled
Original Distilled
Original Distilled
Original Distilled
26
Under review as a conference paper at ICLR 2020
Original Distilled
Original Distilled
Original Distilled
27
Under review as a conference paper at ICLR 2020
G Comparisons of pixel-wise SID between the original and the
revised DNNs
We visualized the pixel-wise SID of the original and damaged networks that were learned using the
CUB200-2011 dataset (Wah et al., 2011). We focused on the VGG-16 and VGG-19 networks. For
each neural network, we revised either the last convolutional layer or the second last convolutional
layer to generated the revised networks.
Revised the last	Revised the 2nd
Ongmal Net	conv-layer	last conv-layer
The layer in which For VGG-16 Conv4-3 Conv5-3 Conv4-3 Conv5-3 Conv4-3 Conv5-3
the SID is measured For VGG-19 Conv4-4 Conv5-4 Conv4-4 Conv5-4 Conv4-4 Conv5-4
VGG-16
VGG-19
VGG-16
VGG-19
VGG-16
VGG-19
Revised the last Revised the 2nd
Ongmal Net	conv-layer	last conv-layer
The layer in which For VGG-16	Conv4-3	Conv5-3	Conv4-3	Conv5-3	Conv4-3	Conv5-3
the SID is measured For VGG-19	Conv4-4	Conv5-4	Conv4-4	Conv5-4	Conv4-4	Conv5-4
VGG-16
VGG-19
VGG-16
VGG-19
VGG-16
VGG-19
Revised the last	Revised the 2nd
Ongmal Net	conv-layer	last conv-layer
The layer in which For VGG-16	Conv4-3	Conv5-3	Conv4-3	Conv5-3	Conv4-3	Conv5-3
the SID is measured For VGG-19	Conv4-4	Conv5-4	Conv4-4	Conv5-4	Conv4-4	Conv5-4
VGG-16
VGG-19
VGG-16
VGG-19
VGG-16
VGG-19
28
Under review as a conference paper at ICLR 2020
Original Net
Revised the last
ConV-Iayer
Revised the 2nd
last conv-layer
The layer in which For VGG-16 Conv4-3 Conv5-3
the SID is measured For VGG-19 Conv4-4 Conv5-4
Conv4-3 Conv5-3
Conv4-4 Conv5-4
Conv4-3 Conv5-3
Conv4-4 Conv5-4
VGG-16
VGG-19
VGG-16
VGG-19
VGG-16
VGG-19
Revised the last	Revised the 2nd
Original Net	conv-layer	last conv-layer
The layer in which For VGG-16 Conv4-3 Conv5-3 Conv4-3 Conv5-3 Conv4-3 Conv5-3
Original Net
Revised the last
conv-layer
Revised the 2nd
last conv-layer
Conv4-3 Conv5-3
Conv4-3 Conv5-3
The layer in which For VGG-16 Conv4-3 Conv5-3
29