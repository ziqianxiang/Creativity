Under review as a conference paper at ICLR 2020
Improving Encoder-Decoder CNN for Inverse
Problems
Anonymous authors
Paper under double-blind review
Ab stract
Encoder-decoder convolutional neural networks (CNN) have been extensively
used for various inverse problems. However, their prediction error for unseen
test data is difficult to estimate a priori, since the neural networks are trained us-
ing only selected data and their architectures are largely considered blackboxes.
This poses a fundamental challenge in improving the performance of neural net-
works. Recently, it was shown that Stein’s unbiased risk estimator (SURE) can
be used as an unbiased estimator of the prediction error for denoising problems.
However, the computation of the divergence term in SURE is difficult to imple-
ment in a neural network framework, and the condition to avoid trivial identity
mapping is not well defined. In this paper, inspired by the finding that an encoder-
decoder CNN can be expressed as a piecewise linear representation, we provide a
close form expression of the unbiased estimator for the prediction error. The close
form representation leads to a novel bootstrap and aggregation scheme to prevent
a neural network from converging to an identity mapping so that it can enhance
the performance. Experimental results show that the proposed algorithm provides
consistent improvement in various inverse problems.
1	Introduction
Suppose that the measurement x is corrupted with additive noises:
X = μ + w, W 〜N(0,σ2I),
(1)
where μ ∈ Rn denotes the unknown mean vector, and σ2 is the variance. Consider a deep neural
network model F (x) := FΘ(x) with the weight Θ, which is trained with the data x producing an
estimate μ = FΘ(x). Then, our goal is to estimate the prediction error:
Err(μ) = Eχ,x* ∣∣x* - μk2 = Eχ,x* ∣∣x* - F(x)k2,
(2)
which quantifies how well μb can predict a test data x*, independently drawn from the same distribu-
tion ofx (Efron, 2004; Tibshirani & Rosset, 2018). The problem of estimating the prediction error is
closely related to the generalizability of neural network (Anthony & Bartlett, 2009). Moreover, this
problem is tightly linked to the classical approaches for model order selection in statistics literature
(Stoica & Selen, 2004). One of the most investigated statistical theories to address this question is
so-called covariance penalties approaches such as Mallow’s Cp (Mallows, 1973), Akaike’s informa-
tion criterion (AIC) (Akaike, 1974), Stein’s unbiased risk estimate (SURE) (Donoho & Johnstone,
1995), etc.
This paper is particularly interested in estimating the prediction error of encoder-decoder convo-
lutional neural networks (E-D CNNs) such as U-Net (Ronneberger et al., 2015; Han & Ye, 2018;
Ye & Sung, 2019). E-D CNNs have been extensively used for various inverse problems (Ye et al.,
2018). Recent theoretical results showed that, thanks to the ReLU nonlinearities, the input space is
partitioned into non-overlapping regions so that input images in each region share the same linear
representation, but not across different partitions (Ye & Sung, 2019).
One of the most important contributions of this paper is that this property can be exploited to derive
a simplified form of prediction error estimator that can be used for neural network training. In par-
ticular, we provide a close form expression for the divergence term in the SURE estimator, which
1
Under review as a conference paper at ICLR 2020
suggests that the divergence term can be neglected if a proper batch normalization is used. Further-
more, the simplified form of the SURE estimate for the prediction error leads to a bootstrap and
aggregation scheme to prevent the CNN from converging to a trivial solution so that it can improve
the neural network performance. The applicability of the new method is demonstrated using various
inverse problems such as accelerated MRI, energy-dispersive X-ray spectroscopic imaging, decon-
volution, and etc, which clearly show that our method can significantly improve the image quality
compared to the existing approaches.
2	Related works
2.1	Stein Risk Unbiased Estimation for Prediction Error
The Stein Risk Unbiased Estimator (SURE) for the prediction error can be represented by (Donoho
& Johnstone, 1995):
S\URE(x) := kx - F (x)k2 + 2σ2div{F (x)},	(3)
where div(∙) denotes the divergence. Recently, the authors in (Soltanayev & Chun, 2018) employed
the SURE for CNN-based image denoising without references by minimizing the following loss:
1P
'sure(Θ) := P E kx⑴-FΘ(x⑴)k2 + 2σ2div{Fθ(x⑺)}.	(4)
P i=1
where {x(i)}iP=1 denotes the training samples. Although the application of SURE for unsupervised
denoising is an important advance in theory, there are several practical limitations. In particular,
due to the difficulty of calculating the divergence term, the authors relied on MonteCarlo SURE
(Ramani et al., 2008) which calculates the divergence term using MonteCarlo simulation. This
introduces additional hyperparameters, on which the final results critically depend.
2.2	Piecewise Linear Representation by E-D CNN with ReLUs
Recently, the authors in (Ye & Sung, 2019) showed that the output of a CNN composed of κ layer
of encoder and decoder without skipped connection can be represented by the following basis-like
representation:
y ：= F(x) = Xhbi(x), XiBi(X) = B(X)B(X)>x,	(5)
i
1	J /	∖	∙>7∕∖∙>	.	. 1	■ . 1	1	Γ∙ .t 1' 11	♦ Γ∙	F ♦	1 ∙ .	1	1
where bi(X) and bi(X) denote the i-th column of the following frame basis and its dual:
B(x) = E1Σ1(x)E2 …Σκ-1(x)Eκ,	(6)
B(X) = D1Σ1 (x)D2 …ΣKT(X)Dκ,	(7)
where Σl(X) and ΣBl(X) denote the diagonal matrix with 0 and 1 values that are determined by the
ReLU output in the previous convolution steps. El and Dl refer to the encoder and decoder matrices,
respectively, which are determined by pooling (resp. unpooling) operation and convolution filters
(Ye & Sung, 2019). For the case of encoder-decoder CNN with the skipped connection, similar basis
expression can be obtained Ye & Sung (2019).
Note that the expression (5) is nonlinear due to the dependency on the input signal X. Moreover, as
the ReLU nonlinearity is applied after the convolution operation, the on-and-off activation pattern
of each ReLU determines a binary partition of the feature space across the hyperplane that is deter-
mined by the convolution. Accordingly, one of the most important observation in Ye & Sung (2019)
is that the input space X is partitioned into multiple non-overlapping regions so that input images
for each region share the same linear representation, but overall representation is still non-linear.
This implies that two different input images may be automatically switched to two distinct linear
representations that are different from each other.
2
Under review as a conference paper at ICLR 2020
3	Main Contribution
In (3) and (4), the calculation of the divergence term is not trivial for general neural networks. This
is why the authors in (Soltanayev & Chun, 2018) employed the MonteCarlo SURE. Thanks to the
basis-like expression in (5), here we show that there exists a simple explicit form of the divergence
term for the case of E-D CNNs. Then, the batch normalization is shown to make the divergence
term trivial.
3.1	Divergence simplification in SURE
一 ∙~	. .	- C    C r~.	4 C 一 一、 、 a	«	«	«	C -■—W / ∖ ，C	-T-λ / ∖ T-∙ / ∖ ^~Γ C	«
In Proposition 6 of (Ye & Sung, 2019), the authors show that ∂F(x)∕∂x = B(X)B(Xyfor the
case of E-D CNN with ReLUs. Accordingly, we have
div{F(x)} = Tr (F^
∂X
Tr (B(X)B(x)>) = Tr (B(X)>B(x))
Ehbi(X), bi(x)i,
i
(8)
where Tr(A) denotes the trace of A.
This close form representation (8) leads to a further simplification of divergence term by exploiting
the property of the batch normalization. Recall that batch normalization has been extensively used
to make the training stable (Ioffe & Szegedy, 2015; Hoffer et al., 2018; Cho & Lee, 2017; Miyato
et al., 2018; Ulyanov et al., 2016). It has been consistently shown that the batch normalization
is closely related to the norm of the Jacobian matrix ∂F(X)∕∂X, which is equal to B(X)B(X)>.
For example, in their original paper (Ioffe & Szegedy, 2015), the authors conjectured that “Batch
Normalization may lead the layer Jacobians to have singular values close to 1, which is known to
be beneficial for training”. By extending the idea in (Ioffe & Szegedy, 2015) to multiple layers, the
batch normalization can be understood as to make the covariance of the network output and input
similar. For example, for the uncorrelated input with Cov[X(i)] = σ2I, the batch normalization may
result in Cov[F(X(i))] ' σ2 I . Furthermore, for sufficiently smaller σ, we have
σ2I ' Cov[F(X⑴)]= CoV [B(x⑴)B(x⑴)>x⑺x>⑺B(X⑴)B(x⑴)>]
= B(X⑴)B(x⑺)>Cov Wi)XT⑺i B(X⑴)B(x⑴)>
=σ2B(X⑴)B(x⑺)> B (X⑴)B(x⑴)>,
because the corresponding piecewise linear representation does not change for the small perturbation
of the input (Ye & Sung, 2019). Therefore, we have
B(X⑴)B(x⑴)> ' I,
since B(X(C))B(X⑺)> is a square matrix. This suggests that
divEL{F(X⑺)} = Xhbi(X⑺),bi(X⑴))=Tr (B(X⑴)B(x⑴)>)` n,	(9)
i
where n is the dimension of the input signal. As the resulting divergence term is just a constant, the
contribution of the divergence term is considered trivial and can be neglected.
3.2	Bagging Estimator
Another important drawback of SURE-based denoising network (4) is that it is difficult to prevent the
network from learning a trivial identity mapping. More specifically, ifFΘ(X) = X, the cost function
in (4) becomes zero. One way to avoid this trivial solution is to guarantee that the divergence term at
the optimal network parameter should be negative. However, given that the divergence term comes
from the degree of the freedom (Efron, 2004) and the amount of excess optimism in estimating the
prediction error (Tibshirani & Rosset, 2018), enforcing negative value may be unnatural. Moreover,
with the divergence simplification in (9), the cost function (4) can be simplified as
1P
`sure-ed(Θ) := P EkX⑴一Fθ(x(i))k2 +2σ2n
i=1
(10)
3
Under review as a conference paper at ICLR 2020
so that it is much apparent that a trivial solution for (10) is FΘ (x) = x.
To prevent the neural network from converging to this trivial solution, here we propose a bootstrap
aggregation (bagging) scheme (Breiman, 1996). Bagging is a classical machine learning technique
which uses bootstrap sampling and aggregation of the results to reduce the variance and to improve
the accuracy of the base learner. The rationale for bagging is that it may be easier to train several
simple weak learners and combine them into a more complex learner than to learn a single strong
learner. More specifically, we use the following bagging estimator:
K
μ(x) = Ewk Fθ(Lk x),	(11)
k=1
and the corresponding prediction error estimate:
K
Err(μ) ：= Eχ,x* ∣∣x* - μk2 = Eχ,x* ∣∣x* - XwkF®(L®x)k2,
k=1
where {Lk}kK=1 denotes the diagonal matrix whose diagonal elements are either 0 or 1 depending on
the bootstrap subsampling patterns, and {wk}kK=1 is the corresponding weights. The corresponding
empirical prediction error estimate is then given by
1P	K
'(Θ,{wk}) := P Ekx⑴-EwkF®(LkX⑴)k2,	(12)
i=1	k=1
Note that x(i) 6= Lkx(i) due to the subsampling patterns so that the optimal neural network FΘ
that minimizes (12) cannot be the trivial identity mapping. Furthermore, the following proposition
shows that the prediction error for the bagging estimator (11) is not bigger than the average of the
individual error estimate.
Proposition 1. Let wk ≥ 0 and k wk = 1. Then, we have
PP
P XX wkkx⑴-F (Lk x(i))k2 ≥ P X 卜⑺-X wk F (Lkx⑺)∣∣ .
i=1 k	i=1
Proof. For any x ∈ {x(i)}iP=1, we have
Xwkkx - F(Lkx)k2 = x>x - 2x> X wkF(Lkx) + XwkkF(Lkx)k2
k	kk
≥ x>x - 2x> X wkF(Lkx) + k XwkF(Lkx)k2
k
= x - X wk F (Lk x)	,
where we use the Jensen’s inequality for the inequality. By summing up for all training data
{x(i)}iP=1,
We conclude the proof.	□
The gap increases more when the F(Lkx) provides diverse output for each realization of the index
Lk (Breiman, 1996). In fact, the authors in (Ye & Sung, 2019) shoWed that the input space results in
non-overlapping partitions With different linear representations so that by changing the subsampling
pattern Lk, the distinct representation may be selected, Which can make the corresponding bagging
estimate more accurate.
4	Implementation Details
The final loss function for our bagging estimator is given by
1P	K
' (Θ, Ξ) := P Ekx⑴-Ewk (Ξ)Fθ(x,Lk )k2,	(13)
i=1	k=1
4
Under review as a conference paper at ICLR 2020
Bootstrap subsampling
□oo∙ooo
Input data
Subsampled data
∙oooooo
□oo∙ooo
Qeee∙QQ
∙∙QOQ∙Q
QQQQ∙Q∙
Regression
Network
Figure 1: Overall network architecture of the proposed method. The acquired data is split into
multiple subset using bootstrap subsampling. The data is then processed using a regression network
followed by an aggregation module using an attention network.
OO∙Q∙OO
QOO∙OOQ
∙OQQQOO
QQO∙OQQ
∙0O∙OQO
□ ∙o□o
∙∙QOQ
where we also parameterize the weighting using a neural network with parameter Ξ. Although the
standard way of aggregation in bagging estimator is a simple average of the overall results of the
regression network, this may not be the best method when the number of bootstrap subsampling
is limited. Instead, we propose a weighted average scheme whose weight is calculated by data
attention module so that it efficiently combines all data by adaptively incorporating output from
various bootstrap sub-sampling patterns. A schematic diagram of the proposed method is illustrated
in Fig. 1, which consists of three building blocks: bootstrap subsampling, a regression network, and
an attention network to calculate the weight parameters.
Figure 2: Architecture for the regression network.
In particular, U-net (Ronneberger et al., 2015) in Fig. 2 was used as our regression network. The
network is composed of four stages with convolution, batch normalization, ReLU, and skip connec-
tion with concatenation. Each stage is composed of three 3 × 3 convolution layers followed by batch
normalization and ReLU, except for the last layer, which is 1 × 1 convolution layer. The number of
convolutional filters increases from 64 in the first stage to 1024 in the final stage. To estimate the
weight wk , our attention network consists of two fully connected layer. The input dimension of the
attention network is R1×1×K followed by the average pooling of the concatenated output of regres-
sion network. The number of hidden node is 64, and the final dimension of the output is R1×1×K
for aggregation.
The overall network was trained using Adam optimization with the momentum β1 = 0.9 and β2 =
0.999. The proposed network was implemented in Python using TensorFlow library and trained
using an NVidia GeForce GTX 1080-Ti graphics processing unit.
5 Experimental Results
Experiments were conducted for various inverse problems such as accelerated magnetic resonance
imaging (MRI), energy-dispersive X-ray spectroscopy (EDX) (Sole et al., 2007), and image SuPer-
resolution in Appendix.
5.1	Accelerated MRI
In accelerated MRI, the goal is to recover high quality MR images from sparsely sampled k-space
data to reduce the acquisition time. This problem has been extensively studied using compressed
sensing (Lustig et al., 2007), but recently deep learning approaches have been the main research
interest due to its excellent performance at significantly reduced run time complexity (Hammernik
et al., 2018; Han & Ye, 2019). A standard method for neural network training for accelerated MRI
is based on the supervised learning, where the MR images from fully sampled k-space data is used
5
Under review as a conference paper at ICLR 2020
as reference and subsampled and zero-filled k-space data is used as the input for the neural network.
Specifically, we use the R = 13.45 subsampled k-space data as neural network input and the goal
is to obtain high resolution images from the sub-sampled k-space data. As a baseline algorithm, the
state-of-the-art deep learning algorithm called k-space deep learning (Han & Ye, 2019) is employed,
which interpolates the missing k-data using a CNN.
In the proposed method, the accelerated k-space data is further subsampled using bootstrap sam-
pling with the subsampling ratio of 91 %, and the number of bootstrap sample set K was set to
10. The same k-space deep learning network (Han & Ye, 2019) was used for our regression net-
work using bootstrap samples. We trained the baseline algorithm and our method under the same
conditions, except that the proposed network was trained by minimize the loss (13). The initial
learning rate was set to 10-2, and it was divided by half at every 50 epochs until it reaches to
around 10-4. The training data was generated from Human Connectome Project (HCP) MR dataset
(https://db.humanconnectome.org). Among the 34 subject data sets, 28 subject data sets were used
for training and validation. The other subject data sets were used for test. We also provide re-
construction results by GRAPPA (Griswold et al., 2002), which is a standard k-space interpolation
method in MRI.
Figure 3: Reconstruction results using accelerated MR data at the acceleration factor of R = 13.45.
The PSNR and SSIM index values for each images are written at the corner.
Thanks to the bagging, the proposed method provided nearly perfect reconstruction results compared
to other algorithms as shown in Fig. 3. Moreover, its average peak signal-to-noise ratio (PSNR) and
structural similarity (SSIM) index for entire test dataset significantly outperform existing methods
as shown in Table 1.
Table 1: Quantitative comparison using PSNR and SSIM index.
GRAPPA Baseline (Han & Ye, 2019) OUrS
PSNR (dB)	34.52	38.67	42.72
SSIM	0.72	0.89	0.91
To demonstrate the optimality of the proposed architectUre, we performed extensive ablation stUdy.
For the ablation stUdy, we Used 13 sUbject data sets from HCP MR dataset, from which seven sUbject
data sets were Used for training and validation and the remaining dataset were Used for test. The
ablation stUdy was performed by exclUding some strUctUres from the network and applying the
same training processes. First, oUr theory says that the performance improvement of oUr method
increases with the nUmber of bootstrap sUbsamples K. To verify this, we performed comparative
stUdies with different K valUes sUch as 1, 2, 4 and 10. As shown in Table 2, the proposed network
provided better reconstrUction resUlts as the nUmber of bootstrap sample set K increased. This
clearly confirmed the importance of the bootstrap sUbsampling. Second, oUr derivation sUggests that
the batch normalization plays a key role to simplify the divergence term. As shown in Fig. 4 and
Table 3, the lack of batch normalization prodUces a significantly degraded image with an average
PSNR drop from 40.047 dB to 25.290 dB. This verified the important role of batch normalization.
Table 2: QUantitative comparison Using PSNR and SSIM index with respect to nUmber of bootstrap
sUbsample sets.
K	1	2	4	10
PSNR (dB)	38.68	39.81	40.05	40.27
SSIM	0.89	0.90	0.91	0.93
6
Under review as a conference paper at ICLR 2020
We also evaluated the importance of the attention network to calculate the weights. With standard
bagging, the final output is a simple average of the entire network output for each bootstrap subsam-
ples without the attention network. As shown in Fig. 4 and Table 3, the simple average produced
blurry results than by the attention network and the PSNR and SSIM values were worse. This con-
firmed that the important of attention network for the weight calculation.
Figure 4: Ablation study for the accelerated MR experiments at the acceleration ratio of R = 13.45.
The PSNR and SSIM index values for each images are written at the corner. w/o BN: reconstruc-
tion results from an ablated network without batch normalization, Standard bagging: reconstruction
results from an ablated network without an attention module.
Table 3: Quantitative comparison in terms of PSNR and SSIM index in our ablation study.
	without BN	Standard bagging	Ours
PSNR (dB)	25.29	35.91	40.05
SSIM	0.49	0.867	0.914
5.2 Energy-dispersive X-ray spectroscopy Imaging
As for another experiment, We use the energy-dispersive X-ray spectroscopy (EDX) (Sole et al.,
2007) data set which were measured by scanning transmission electron microscopy. EDX (Sole
et al., 2007) is an analytical technique used for spectroscopic characterization of a sample. It relies
on the emission of characteristic X-rays from a specimen that is generated by a high-energy beam of
electrons. Specifically, the incident beam may eject an electron from the inner shell of atoms. Then,
an electron from an outer shell then fills the hole, and the energy difference may be released in the
form of an X-ray. As the energies of the X-rays are characteristic of the energy differences that
are determined by the atomic structure of the emitting element, EDX is widely used for nano-scale
quantitative and qualitative elemental composition analysis by measuring X-ray radiation from the
interaction with high energy electron and the material (McDowell et al., 2012).
However, in EDX, the specimens can be quickly damaged by the high energy electrons, so the
acquisition time should be reduced to its minimum. This usually results in very noisy and even
incomplete images as shown in Fig. 5(a). Therefore, the goal of this experiment was to remove the
noise and reconstruct high resolution EDX images. The main technical difficulty is that there are
no noiseless reference data. Due to the lots of missing photons, a widely used approach for EDX
analysis is a kernel regression method using an average kernel as shown in Fig. 5(b). Unfortunately,
this often results in severe blurring. Existing unsupervised learning methods such as SURE network
(Soltanayev & Chun, 2018) and Noise2Noise (Lehtinen et al., 2018) cannot be used for this purpose,
since there are no specific noise models for the EDX and the noiseless clean data is not available
for Noise2Noise training. In fact, this difficulty of EDX denoising was our original motivation for
this work. For our network training and inference, we use bootstrap subsampled images from the
measurement image in Fig. 5(a). In addition, we used the measurement image in Fig. 5(a) as the
label for training.
We used 28 cases from the EDX dataset. The specimen are composed of quantum-dots, where core
and shell consist of Cadmium (Cd), Selenium (Se), Zinc (Zn), and Sulfur (S). The regression network
was trained using image patches, whose sizes was 128 × 128. For stable training, the intensity of
data was normalized between [0, 1]. The initial learning rate was set to 10-3, and it was divided in
7
Under review as a conference paper at ICLR 2020
half at every 50 epochs until it reaches around 10-5. For the bootstrap subsampling, the number of
random subsampling mask was K = 30. The regression network was optimized to minimize the loss
(13) with respect to Θ first, after which the attention network was trained to properly aggregate the
entire interpolated output. As shown in Fig. 5(d), our method successfully produced high resolution
images which are significantly better than existing kernel regression methods. Thanks to the bagging
procedure, the regression network is believed to learn the measurement statistics so that the network
not only removes the noise but also filled in the unmeasured data. We also compared the performance
between the standard bagging and the proposed method. As shown in Fig. 5(c) and (d), the weighted
averaging from the attention module provides better performance over the the standard bagging that
uses the simple average. This confirmed the efficacy of the proposed method.
Measured data Kernel regression Standard bagging Proposed
Figure 5: EDX experimental results. Green and blue particles refer to Zn and Cd, respectively. (a)
Input data, and the reconstruction results using (b) the existing kernel regression method, and the
proposed methods (c) with simple average (standard bagging) and (d) the attention based weighted
average.
6	Conclusion
In this paper, we proposed a novel bagging scheme of encoder-decoder CNNs for various inverse
problems. The algorithm was derived by the observation that the existing SURE-based unsuper-
vised denoising networks have several practical limitations due to the divergence term and potential
to converge to a trivial solution. Inspired by the recent discovery of basis-like representation of the
encoder-decoder CNN, we provide a simple approximation of the divergence term. This also led
to a novel bagging scheme to prevent from converging to the trivial identity mapping. In the im-
plementation, multiple input data were generated by bootstrap subsampling, after which final result
are obtained by aggregating the entire output of network using an attention network. Experimen-
tal results with accelerated MRI and EDX experiments showed that the proposed method provides
consistent improvement for various inverse problems.
References
Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution:
Dataset and study. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition Workshops, pp. 126-135, 2017.
8
Under review as a conference paper at ICLR 2020
Hirotugu Akaike. A new look at the statistical model identification. In Selected Papers of Hirotugu
Akaike,pp. 215-222. SPringer,1974.
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge
university press, 2009.
Leo Breiman. Bagging predictors. Machine learning, 24(2):123-140, 1996.
Minhyung Cho and Jaehyung Lee. Riemannian approach to batch normalization. In Advances in
Neural Information Processing Systems, pp. 5225-5235, 2017.
David L Donoho and Iain M Johnstone. Adapting to unknown smoothness via wavelet shrinkage.
Journal of the american statistical association, 90(432):1200-1224, 1995.
Bradley Efron. The estimation of prediction error: covariance penalties and cross-validation. Jour-
nal of the American Statistical Association, 99(467):619-632, 2004.
Mark A Griswold, Peter M Jakob, Robin M Heidemann, Mathias Nittka, Vladimir Jellus, Jianmin
Wang, Berthold Kiefer, and Axel Haase. Generalized autocalibrating partially parallel acquisi-
tions (GRAPPA). Magn. Reson. Med., 47(6):1202-1210, 2002.
Kerstin Hammernik, Teresa Klatzer, Erich Kobler, Michael P Recht, Daniel K Sodickson, Thomas
Pock, and Florian Knoll. Learning a variational network for reconstruction of accelerated MRI
data. Magnetic resonance in medicine, 79(6):3055-3071, 2018.
Yoseob Han and Jong Chul Ye. Framing U-Net via deep convolutional framelets: Application to
sparse-view CT. IEEE transactions on medical imaging, 37(6):1418-1429, 2018.
Yoseob Han and Jong Chul Ye. k-Space Deep Learning for Accelerated MRI. IEEE Transactions
on Medical Imaging (in press), also available as arXiv preprint arXiv:1805.03779, 2019.
Muhammad Haris, Gregory Shakhnarovich, and Norimichi Ukita. Deep back-projection networks
for super-resolution. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 1664-1673, 2018.
Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters: efficient and accurate
normalization schemes in deep networks. In Advances in Neural Information Processing Systems,
pp. 2160-2170, 2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448-456,
2015.
Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and
Timo Aila. Noise2Noise: learning image restoration without clean data. In International Confer-
ence on Machine Learning, pp. 2971-2980, 2018.
Michael Lustig, David Donoho, and John M Pauly. Sparse MRI: The application of compressed
sensing for rapid MR imaging. Magn. Reson. Med., 58(6):1182-1195, 2007.
Colin L Mallows. Some comments on c p. Technometrics, 15(4):661-675, 1973.
Matthew T McDowell, Ill Ryu, Seok Woo Lee, Chongmin Wang, William D Nix, and Yi Cui.
Studying the kinetics of crystalline silicon nanoparticle lithiation with in situ transmission electron
microscopy. Advanced Materials, 24(45):6034-6041, 2012.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Sathish Ramani, Thierry Blu, and Michael Unser. Monte-Carlo SURE: a black-box optimization
of regularization parameters for general denoising algorithms. IEEE Transactions on image pro-
cessing, 17(9):1540-1554, 2008.
9
Under review as a conference paper at ICLR 2020
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
cal image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234-241. Springer, 2015.
VA Sole, E Papillon, M Cotte, Ph Walter, and J Susini. A multiplatform Code for the analysis of
energy-dispersive X-ray fluorescence spectra. Spectrochimica Acta Part B: Atomic Spectroscopy,
62(1):63-68, 2007.
Shakarim Soltanayev and Se Young Chun. Training deep learning based denoisers without ground
truth data. In Advances in Neural Information Processing Systems, pp. 3257-3267, 2018.
Petre StoiCa and Yngve Selen. Model-order seleCtion: a review of information Criterion rules. IEEE
Signal Processing Magazine, 21(4):36-47, 2004.
Ryan J Tibshirani and Saharon Rosset. ExCess optimism: How biased is the apparent error of an
estimator tuned by SURE? Journal of the American Statistical Association, pp. 1-16, 2018.
Dmitry Ulyanov, Andrea Vedaldi, and ViCtor Lempitsky. InstanCe normalization: The missing in-
gredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
Jong Chul Ye and Woon Kyoung Sung. Understanding geometry of enCoder-deCoder CNNs. Pro-
ceedings of the 2019 International Conference on International Conference on Machine Learning
(ICML). also available as arXiv preprint arXiv:1901.07647, 2019.
Jong Chul Ye, Yoseob Han, and Eunju Cha. Deep Convolutional framelets: A general deep learning
framework for inverse problems. SIAM Journal on Imaging Sciences, 11(2):991-1048, 2018.
A Appendix: Super-resolution application
Single image super-resolution (SR) is a task to estimate a high-resolution (HR) image from its low-
resolution (LR) image. To demonstrate the effeCtiveness of the proposed method, the proposed
bagging sCheme was also applied to these problems. Deep BaCk-ProjeCtion Network (DBPN) (Haris
et al., 2018) is a reCent state-of-the-art method for this appliCations, so we employed DBPN as
our baseline model to improve its performanCe by the proposed method. The baseline network
is trained using DIV2K (Agustsson & Timofte, 2017) with totally 800 training images on the ×2
and ×4 (in both horizontal and vertiCal direCtions) super-resolution tasks. The network Consists of
three modules: initial feature extraCtion, baCk-projeCtion stages, and reConstruCtion. The initial LR
feature maps were extraCted using the Convolution layer of the initial feature extraCtion module. By
applying the sequenCe of projeCtion units whiCh alters between Configuration of LR and HR feature
maps, the extensive HR features maps Can be obtained. The final HR image was reConstruCted using
the overall HR feature maps from the projeCtion units.
Figure 6: Comparisons between DBPN and the proposed method for ×4 super-resolution task.
Training Conditions of the baseline algorithm and our method were followed as desCribed in (Haris
et al., 2018). SpeCifiCally, we used patCh proCessing whose size is 32×32 for LR image. The
initial learning rate was set to 10-4, and it was divided by 10 at every 5 × 105 iterations for total 106
iterations. For the training of our bagging network, the number of random subsampling mask K was
set to 32 and 8 for ×2 and ×4 task, respeCtively. The subsampling ratio for bootstrap sampling was
set to 80%. In addition, the entire networks were trained simultaneously to minimize the loss (13).
10
Under review as a conference paper at ICLR 2020
Thanks to the bootstrapping and aggregation using attention network, the data distribution can be
fully exploited to restore the high resolution components, which results in the properly reconstructed
details of the image as shown in Fig. 6. As described in Table 4, our method can also improve the
quantitative performance of the super-resolution task.
Table 4: Comparison of PSNR and SSIM index for super-resolution task
set14				bsd100	
Algorithm	Scale	PSNR	SSIM	PSNR	SSIM
DBPN (Haris et al., 2018)	2	30.748	0.937	31.689	0.950
Proposed	2	30.802	0.939	31.720	0.951
set14				bsd100	
Algorithm	Scale	PSNR	SSIM	PSNR	SSIM
DBPN (Haris et al., 2018)	4	26.191	0.837	26.803	0.855
Proposed	4	26.222	0.837	26.855	0.855
11