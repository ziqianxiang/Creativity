Under review as a conference paper at ICLR 2020
Being Bayesian, Even Just a Bit,
Fixes Overconfidence in ReLU Networks
Anonymous authors
Paper under double-blind review
Ab stract
The point estimates of ReLU classification networks, arguably the most widely
used neural network architecture, have recently been shown to have arbitrarily
high confidence far away from the training data. This architecture is thus not
robust, e.g., against out-of-distribution data. Approximate Bayesian posteriors on
the weight space have been empirically demonstrated to improve predictive uncer-
tainty in deep learning. The theoretical analysis of such Bayesian approximations
is limited, including for ReLU classification networks. We present an analysis of
approximate Gaussian posterior distributions on the weights of ReLU networks.
We show that even a simplistic (thus cheap), non-Bayesian Gaussian distribution
fixes the asymptotic overconfidence issue. Furthermore, when a Bayesian method,
even if a simple one, is employed to obtain the Gaussian, the confidence becomes
better calibrated. This theoretical result motivates a range of Laplace approxima-
tions along a fidelity-cost trade-off. We validate these findings empirically via
experiments using common deep ReLU networks.
1	Introduction
As neural networks have been successfully applied in ever more domains, including safety-critical
ones, the robustness of their predictions and the calibration of their predictive uncertainty have
moved into focus, subsumed under the notion of AI safety (Amodei et al., 2016). A principal
goal of uncertainty calibration is that learning machines (and neural networks in particular) should
assign low confidence to test cases not explained well by the training data or prior information (Gal,
2016). The most obvious such instance are test points that lie “far away” from the training data.
Many methods to achieve this goal have been proposed, both Bayesian (Gal & Ghahramani, 2016;
Blundell et al., 2015; Louizos & Welling, 2017) and non-Bayesian (Lakshminarayanan et al., 2017;
Liang et al., 2018; Hein et al., 2019).
ReLU networks are currently among the most widely used neural architectures. This class com-
prises any network that can be written as a composition of linear layers (including fully-connected,
convolutional, and residual layers) and a ReLU activation function. But while ReLU networks often
achieve high accuracy, the uncertainty of their predictions has been shown to be miscalibrated (Guo
et al., 2017). Indeed, Hein et al. (2019) demonstrated that ReLU networks are always overconfident
“far away from the data”: scaling a training point x (a vector in a Euclidean input space) with a
scalar δ yields predictions of arbitrarily high confidence in the limit δ → ∞. This means ReLU
networks are susceptible to adversarial or out-of-distribution (OOD) examples. Bayesian methods
have long been known empirically to improve predictive uncertainty calibration. MacKay (1992)
demonstrated empirically that the predictive uncertainty of Bayesian neural networks will naturally
be high in regions not covered by training data. Results like this raise the hope that the overconfi-
dence problem of ReLU networks, too, might be mitigated by the use of Bayesian methods.
This paper offers a theoretical analysis of the binary classification case of ReLU networks with
logistic output layer. We show that equipping such networks with virtually any Gaussian probability
distribution (i.e. regardless of whether it is motivated in a Bayesian fashion or not) mitigates the
aforementioned theoretical problem, so that predictive confidence far away from the training data
approaches a known constant, bounded away from one, whose value is controlled by the covariance
(cf. Figure 1). At the same time, this treatment does not change the decision boundary of the trained
network, so it has no negative effect on the predictive performance.
1
Under review as a conference paper at ICLR 2020
(a) MAP
(b) Isotropic O(1)
(c) Diagonal O(d)
(d) Laplace O(d3)
Figure 1: Binary classification on a toy dataset using a MAP estimate (a) and various Gaussian
approximations over the weights, sorted by their complexity of inverting the precision matrix. These
approximations are carried out only at the last layer of the network and d denotes the number of
hidden units at that layer. The shade of color represents the confidence of the prediction (darker
shade means higher confidence). The decision boundary is in thick black. Even an arbitrary (i.e. non-
Bayesian) isotropic (b) or diagonal (c) covariance makes the confidence bounded away from one.
Using the data in a more Bayesian fashion (d) calibrates the uncertainty further, in particular in
regions close to the data.
A central aspect of our result is that asymptotic overconfidence can be mitigated with an essentially
arbitrary Gaussian distribution on the weight space, including one of simple diagonal or even scalar
covariance, and one whose covariance need not even depend on the training data. Achieving cal-
ibration at finite distances from the training data requires increasing levels of fidelity towards full
Bayesian inference, for which our results also give some quantification. Our results thus answer
a question about “how Bayesian” one needs to be to achieve certain levels of calibration. This is
valuable because even approximate Bayesian treatments of deep learning, such as through Laplace
approximations, can have high computational cost.
We empirically validate our results through a simple Laplace approximation to only the last layer
of deep ReLU architectures, and find that this cheap procedure is already competitive to recently
proposed non-Bayesian methods specifically constructed to overcome the overconfidence problem
of ReLU networks. We also show that this cheap Bayesian approach yields good performance in
the multi-class classification setting, indicating that our analysis may carry over to this case. Sec-
tion 2 begins with a rigorous problem statement and assumptions, then develops the main theoretical
results. We discuss related work in Section 3, while empirical results are in Section 4.
2	Analysis
2.1	Preliminaries
Definitions We call a function f : Rn → R piecewise affine if there exists a finite set of poly-
topes {Qr}rR=1, referred to as linear regions of f, such that ∪rR=1Qr = Rn and f|Qr is an affine
function for every Qr . ReLU networks are networks that result in piecewise affine classifier func-
tions (Arora et al., 2018) which include networks with fully-connected, convolutional, and residual
layers where just ReLU or leaky-ReLU are used as activation functions and max or average pool-
ing are used as a convolution layer. Let D := {xi ∈ Rn, ti}im=1 be a dataset, where the targets
ti ∈ {0, 1} or ti ∈ {1, . . . , k} for the binary and multi-class case, respectively. We define the
logistic (sigmoid) function as σ(z) := 1/(1 + exp(-z)) for z ∈ R and the softmax function as
softmax(z, i) := exp(zi)/ Pj exp(zj) for z ∈ Rk. Given a linear classifier,1 we will consider
probability distributions p(w|D) or p(W|D) over the weight vector and matrix, respectively. We
call these distributions posterior if they arose from Bayes’ theorem or an approximation thereof.
1Unless stated otherwise, we assume the bias is absorbed into the weights.
2
Under review as a conference paper at ICLR 2020
The predictive distribution (also called the marginalized prediction) is
p(y
1|x,D):=
σ(wT x) p(w|D) dw
or
p(y = i|x, D) :=
softmax(Wx, i) p(W|D) dW ,
(1)
(2)
for the binary and multi-class cases, respectively. For Euclidean spaces we use the standard in-
ner product and norm. Finally, λ%(∙), λmaχ(∙), and λmi∩(∙) return the ith, maximum, and minimum
eigenvalue (which are assumed to exist) of their matrix argument, respectively.
Problem statement The following theorem from Hein et al. (2019) shows that ReLU networks
exhibit arbitrarily high confidence far away from the training data: If a training point x ∈ Rn is
scaled by a sufficiently large scalar δ > 0, the input δx attains arbitrarily high confidence.
Theorem 2.1 (Hein et al. (2019)). Let Rd = ∪rR=1Qr and f(x) = Vrx + ar be the piecewise
affine representation of the output of a ReLU network on Qr. Suppose that Vr does not contain
identical rows for all r = 1, . . . , R, then for almost any x ∈ Rn and > 0 there exists an
δ > 0 and a class i ∈ {1, . . . , K} such that it holds softmax(f (δx), i) ≥ 1 - . Moreover,
limδ→∞ Softmax(f (δx),i) = L	□
2.2	Assumptions
For binary classification tasks, it is standard to treat neural networks as probabilistic models of
the conditional distribution p(y|x, w). Standard deep training involves assigning a maximum a
posteriori (MAP) value wMAP to the weights. Doing so ignores potential uncertainty on w. We will
show that this lack of uncertainty is the primary cause of the overconfidence discussed in Hein et al.
(2019).
Unfortunately, there is generally no analytic solution for eq. (1). But for the logistic link func-
tion, good approximations exist when the distribution over the weights is Gaussian p(w|D) =
N(w; μ, Σ) with mean μ and covariance Σ. One such approximation (MacKay, 1992) is con-
structed by scaling the input of the probit function2 Φ by a constant λ = ,π∕8 . Using this
approximation and the Gaussian assumption, ifwe let a := wTx, we get
p(y = 1|x, D) ≈
J Φ(pπ∕8 a) N(a∣μTx, XTΣx) da
μ μ x
p8∕π + XT Σx
≈ σ (z(x)) ,
where the last step uses the approximation Φ(,∏∕8 x) ≈ σ(x) a second time, with
(3)
z(x) :
μτ x
pl + π∕8 xτ Σx
(4)
Φ
In the case of μ = wmap, eq. (3) can be seen as the “softened” version of the MAP prediction of the
classifier, using the covariance of the Gaussian. The principal aspect of interest ofin this paper will
be not so much any philosophical point about Bayesian inference, but that the approximate proba-
bilistic Gaussian formalism as outlined in eqs. (1) and (3) introduces the second set of parameters in
the form of Σ. We will find that at least asymptotic overconfidence problems can be fixed by setting
Σ to virtually any sensible value, regardless of whether they are motivated in a Bayesian fashion or
not.
As a first notable property of this approximation, we show below that, in contrast to some other
methods for uncertainty quantification (e.g. Monte Carlo dropout (Gal & Ghahramani, 2016)) it
preserves the decision boundary induced by the MAP estimate. Moreover, this property still holds
even if we use any feature map φ and define the linear classifier on the image of this map instead.
The implication is important in practice, as this gives a guarantee that if we apply this approximation
to the last layer of any MAP pre-trained neural networks, then the classification accuracy of the
marginalized prediction is exactly the same as the MAP classification accuracy.
2The probit function Φ is another sigmoid, the distribution function (CDF) of the standard Gaussian.
3
Under review as a conference paper at ICLR 2020
(a) μ X
(c) Confidence
Figure 2: An illustration of Proposition 2.3 for a linear classifier defined on R2 . The confidence of
the marginalized prediction of a linear classifier is the highest in the direction of the lowest curvature,
as described by Σ.
Proposition 2.2. Let f : Rd → R be a binary linear classifier defined by f ◦ φ(x) := wT φ(x)
where φ : Rn → Rd is any feature map and let p(w|D) := N (w|wMAP, Σ) be the distribution over
w. Then for any x ∈ Rn, we have σ(z ◦ φ(x)) = 0.5 if and only if σ (wMT AP φ(x)) = 0.5.
Proof. See Proposition A.1 in Appendix A.
□
Particularly in the Bayesian setting, the Gaussian approximate posterior required in the previ-
ous approximation can be obtained via various methods, such as a Laplace approximation. Let
p(w∣D) a P(W) Qχ t∈D p(y = t|x, W) be the posterior of a binary linear classifier.3 Then We can
obtain a Gaussian approximation p(w∣D) ≈ N(w∣μ, Σ) of the posterior by setting μ = WMAP and
Σ = (-V2∣wMAP logp(w∣D))-1, the inverse Hessian of the negative log-posterior. In our binary
classification case, p(y|x, w) is assumed to be Bernoulli(σ(wT x)) While p(w) is assumed to be
N(w|0, σ2l), leading to the standard '2-regularized binary cross-entropy loss.
2.3	Main results
As our central theoretical contribution, We shoW that, far aWay from the training points, z(x) goes
to a quantity that only depends on the mean and covariance of the Gaussian over the Weights. This
result implies that We can make p(y = 1|x, D) closer to one-half far aWay from the training points
ifWe can make z(x) closer to zero by controlling the Gaussian. Proposition 2.3 beloW shoWs this in
the case of linear classifiers (also cf. Figure 2), While Theorem 2.4 shoWs that the analysis actually
also holds in the case of ReLU netWorks.
Proposition 2.3. Let f : Rn → R be a binary linear classifier defined by f(x) := wTx and
p(w∣D) := N(w∣μ, Σ) be the distribution over W. Thenfor any X ∈ Rn,
|z(X)| ≤
_________|〃T x|_________
P1+ ∏∕8 λmin(Σ)∣∣X∣∣2
(5)
Furthermore, if x ∈ Rn then as δ > 0 goes to infinity
lim |z(SX)| ≤
δ→∞
kμk
P∏∕8 λmin(∑)
(6)
Proof. See Proposition A.3 in Appendix A.
□
Let φ : Rn → Rd be a ReLU netWork. Recall from the definition, φ is a pieceWise affine function.
Thus, we can write the input space as Rn = ∪R=ιQr and for every Qr, the restriction φ∣Qr : Qr →
Rd is an affine function φ∣Qr (x) := VrX + ar for some Vr ∈ Rd×n and ar ∈ Rd. Note that if
i, j ∈ {1, . . . , M} with i 6= j then in general Vi 6= Vj and ai 6= aj . Using this definition, we can
3We use a linear model here for simplicity and not as a requirement.
4
Under review as a conference paper at ICLR 2020
also show a similar result to Proposition 2.3, in the case when x is replaced by any feature vector in
the image of φ.
Theorem 2.4. Let f : Rd → R be a binary linear classifier defined by f ◦ φ(x) := wT φ(x) where
φ : Rn → Rd is a ReLU network and let p(w∣D) := N(w∣μ, ∑) be the distribution over W. Then
for any x ∈ Rn,
|z◦ Φ(χ)l ≤
|〃T(VX+a)|
pi+ π∕8 λmiη(Σ)kVx + ak2
(7)
where V ∈ Rd×n
goes to infinity
and a ∈ Rd are some matrix and vector that depend on x. Furthermore, as δ > 0
lim |z ◦ φ(δx)∣ ≤ —	"μ"-----.
δ→∞	P∏∕8 λmin(Σ).
(8)
Proof. See Theorem A.5 in Appendix A.
□
Given a target upper bound on the logit and confidence values of a ReLU network, we can concretely
pick the covariance Σ that respects the asymptotic bound of Theorem 2.4.
Corollary 2.5 (Σ from a desired upper confidence bound on ReLU networks). Let f ◦ φ, with
φ : Rn → Rd and f : Rd → R, be a ReLU network defined by f ◦ φ(x) := wT φ(x) and
N (w∣μ, Σ) be the distribution over W where the mean μ is fixed and Σ is any SPD matrix. Then:
(i)	For any > 0 there exists Σ such that for any x ∈ Rn far away from the training data, we
have that |z ◦ φ(x) | ≤ e.
(ii)	For any 0.5 < p < 1 there exists Σ such that for any x ∈ Rn far away from the training data,
we have that σ(∣z ◦ φ(x)∣) ≤ P.
Proof. See Corollary A.6 in Appendix A.	□
2.4 Being Bayesian (not just probabilistic) with Laplace approximations
Proposition 2.3 and Theorem 2.4 imply that the confidence of a binary linear classifier with ReLU
features can be bound closer to one-half by increasing the minimum eigenvalue of the posterior
covariance. In this section, we will move towards the Bayesian setting (i.e. using an explicit prior and
likelihood, not just an imposed probability measure on the weights). Specifically, we will present a
way to control the posterior through the prior in a Laplace approximation. Concretely, the following
proposition and its immediate corollary point out that the eigenvalues of the posterior covariance
can be increased (bringing ∣z(δx) | closer to zero) by increasing the prior variance.
Proposition 2.6. Let f ◦ φ, with φ : Rn → Rd and f : Rd → R, be a ReLU network defined by
f ◦ φ(x) := wT φ(x), modeling a Bernoulli distribution with p(y = 1|x, w) = σ(f ◦ φ(x)). Let
p(w∣D) := N(w∣μ, Σ) be the posterior over W, obtained via a Laplace approximation with prior
N (w|0, σ2I). Suppose H is the Hessian w.r.t. W at μ of the negative log-likelihood of the model.
Then
(i)	H = Px∈D β(x) φ(x)φ(x)T where β(x) := σ(f ◦ φ(x))(1 - σ(f ◦ φ(x))).
(ii)	For each i = 1, . . . , d, the ith eigenvalue λi(Σ) ofΣ is a non-decreasing function of σ02 with
limits 1∕λi(H) as σ02 → ∞ and 0 as σ02 → 0.
Proof. See Proposition A.7 in Appendix A.
□
We get an immediate corollary from Proposition 2.6 that relates its results to Theorem 2.4.
5
Under review as a conference paper at ICLR 2020
Corollary 2.7. Let f ◦ φ, with φ : Rn → Rd and f : Rd → R, be a ReLU network defined by
f ◦ φ(x) := WTφ(x), modeling p(y = 1|x, W) with σ(f ◦ φ(x)). Let p(w∣D) := N(w∣μ, ∑) be
the posterior over w, obtained via a Laplace approximation with prior N (w|0, σ02I). Then z ◦ φ(x)
is a non-increasing function of σ02 with limits
lim |z ◦ φ(x) | ≤
_________|〃T φ(X)I________
P1+ ∏∕(8 λmaχ(H))kφ(x)k2
lim Z ◦ φ(x) = μτφ(x),
σ02→0
where H is as defined in (i) of Proposition 2.6.
Proof. See Corollary A.8 in Appendix A.	□
Lastly, the following corollary formalizes the intuition that the marginalized prediction with the
inverse empirical features covariance C-1 as Σ will naturally have high uncertainty far away from
the training data. Furthermore, this property can also be observed for Laplace approximation if the
spectral properties of the Hessian ((i) of Proposition 2.6) are not too different to those of C.
Corollary 2.8. Let f ◦ φ, with φ : Rn → Rd and f : Rd → R, be a ReLU network defined by
f ◦ φ(x) := wTφ(x), modeling p(y = 1∣x, w) with σ(f ◦ φ(x)). Let p(w∣D) := N(w∣μ, Σ) be
the distribution over w. If either
(i)	ς = CT = (Pχ∈D φ(x)φ(X)T)-1, or
(ii)	Σ is obtained via a Laplace approximation w.r.t. a prior N (w|0, σ02I) with σ02 → ∞ and
suppose H defined in (i) of Proposition 2.6 is invertible and the ordering of its eigenvalues is
the same as that of C, while the eigenvectors are the same as those of C,
then on any level set ofμTφ(x), the confidence decreases faster in the direction where the training
data are sparser in the feature space Rd.
Proof. See Corollary A.9 in Appendix A.	□
Similar statements for multi-class classifiers are not as straight-forward due to the lack of a good
closed-form approximation of the integral of softmax under a Gaussian measure. However, as can
be seen in Appendix C, at least the application of the above analysis can easily be generalized to
the multi-class case. In fact, in the experiments (Section 4), we mainly use multi-class classifiers
and show empirically that they are effective in mitigating issues that arise from the overconfidence
problem.
3 Related work
The overconfidence problem of deep neural networks, and thus ReLU networks, has long been
known in the deep learning community (Nguyen et al., 2015). However, only recently this issue was
demonstrated formally (Hein et al., 2019). Many methods have been proposed to combat or at least
detect this issue. Post-hoc heuristics based on temperature or Platt scaling (Guo et al., 2017; Liang
et al., 2018) are unable to detect inputs with arbitrarily high confidence far away from the training
data (Hein et al., 2019). Hein et al. (2019) proposed enhanced training objectives based on robust
optimization to mitigate this issue.
Bayesian methods have long been thought to mitigate the overconfidence problem on any neural
network (MacKay, 1992). Empirical evidence supporting this intuition has also been presented (Liu
et al., 2019; Wu et al., 2019, etc.). Our results complement these with a theoretical justification for
the ReLU-logistic case. But while our work is theoretical in nature, we believe its application has
practical value since it shows that a full Bayesian (expensive) treatment is not necessary if one is
only worried about overconfidence. Indeed, fully Bayesian neural networks are often intractable and
crude approximations have to be used, resulting in undesirable results (Foong et al., 2019).
6
Under review as a conference paper at ICLR 2020
Figure 3: Binary (top) and multi-class (bottom) toy classification. The color represents either the
confidence (top) or entropy (bottom) of the prediction, with darker shade implies higher value.
(a) MAP confidence
(b) Laplace confidence
(c) Denominator of z
Figure 4: The zoomed-out version of the MAP’s and Laplace’s confidence from Figure 1d, along
with denominator of z.
4	Experiments
In this section we validate our theoretical results by applying a Laplace approximation only to the last
layer of various widely used ReLU networks and call this method last-layer Laplace approximation
(LLLA). We refer the reader to Appendix C for details. Note that since LLLA is the simplest
Laplace approximation that we can apply to deep networks, our results should also hold for more
general Laplace methods, e.g. Kronecker-factored Laplace (KFLA) (Ritter et al., 2018), where not
only the linear classifier’s posterior but also the posterior of the feature map is approximated. Note
however, these fully-Bayesian methods are significantly more expensive and require a significant
amount of implementation effort.
We will present our empirical results on (i) a 2D toy classification task and (ii) out-of-distribution
(OOD) data detection experiments. For the OOD experiment, we find the optimal prior variance
σ02 via a heuristic that follows directly from Corollary 2.7. Concretely, we pick the largest positive
integer that makes the drop on the mean maximum confidence (MMC) of the in-distribution dataset
to be within around 0.03 of the MAP’s MMC. Thus, we only set this once without seeing any of the
OOD datasets.
7
Under review as a conference paper at ICLR 2020
Table 1: OOD detection results. For MMC, the lower the better in the OOD case, but the higher the
better for the in-distribution case; for AUROC, the higher the better.
MAP	CEDA	ACET	LLLA
Train - Test	MMC	AUROC	MMC	AUROC	MMC	AUROC	MMC	AUROC
MNIST - MNIST	0.993	-	0.993	-	0.993	-	0.962	-
MNIST - notMNIST	0.715	0.976	0.676	0.979	0.649	0.983	0.511	0.978
MNIST - EMNIST	0.803	0.944	0.802	0.943	0.790	0.948	0.608	0.947
MNIST - FMNIST	0.570	0.993	0.458	0.995	0.471	0.995	0.383	0.995
CIFAR-10 - CIFAR-10	0.910	-	0.906	-	0.883	-	0.880	-
CIFAR-10 - SVHN	0.716	0.813	0.692	0.824	0.678	0.797	0.606	0.823
CIFAR-10 - LSUN-CR	0.732	0.810	0.699	0.824	0.649	0.829	0.623	0.818
4.1	Toy dataset
The dataset is constructed by sampling the input points from k Gaussians. The corresponding targets
indicate from which Gaussian the point was sampled. We use a 5-layer ReLU network with 100
hidden units at each layer as the feature map φ. The classifier, along with this feature map is trained
jointly. We show the results for the binary and multi-class (k = 4) case in Figure 3. As we can see,
the MAP predictions have high confidence (low entropy) everywhere except at the region close to the
decision boundary. The widely used MC-dropout does not remedy this issue. While ACET remedies
the overconfidence issue, it is expensive and in general does not preserve the decision boundary. In
contrast, LLLA yields better calibrated predictions: high confidence close to the training points and
high uncertainty otherwise, while maintaining the MAP’s decision boundary. We furthermore show
the zoomed-out version of LLLA prediction we have presented in Figure 1d, along with the contour
of the denominator of z (eq. (4)) in Figure 4. We see that the covariance acts as a “moderator” for the
MAP predictions: As a test point moves away from the training data, the denominator of z becomes
larger and the marginalized prediction goes to a constant close to one-half.
4.2	Out-of-distribution experiments
Following the experiment of Hein et al. (2019), we compare LLLA with CEDA and ACET (Hein
et al., 2019), two recently-proposed non-Bayesian methods for mitigating the overconfidence prob-
lem of ReLU networks. We use a variant of LeNet for MNIST experiments and a pre-activation
ResNet-20 architecture (He et al., 2016b) for the CIFAR-10 experiment. The CEDA’s and ACET’s
out-distribution of choice is the uniform distribution. For ACET specifically, we solve the inner op-
timization via a projected gradient descent with 10 iterations, radius 0.3, and step size 0.0075. More
detail about the training setup is presented in Appendix D. We use standard metrics for measur-
ing robustness to OOD data (Hendrycks & Gimpel, 2017), namely: (i) mean maximum confidence
(MMC) and (ii) area under the ROC curve (AUROC). As presented in Table 1, LLLA yields com-
petitive performance compared to both CEDA and ACET.
5	Conclusion
We have shown that even an extremely approximate and virtually non-Bayesian probabilistic Gaus-
sian treatment mitigates the most extreme aspects of overconfidence in ReLU networks. Our analyt-
ical results bound the confidence of the Bayesian prediction of linear classifiers and ReLU networks
far away from the training data away from one. This motivates a spectrum of approximations, from
ad-hoc isotropic to “full Bayesian” Laplace approximations. In the Laplace approximation case,
the bound asymptotically converges to a constant whose value can be controlled via the prior. We
validated our results experimentally by constructing a simple Laplace method that can still capture
the properties we have shown, specifically by only approximating the last-layer’s posterior distribu-
tion. In contrast to other approximations, this method is cheap and simple to implement, yet already
yields competitive performance compared to the more expensive, recently proposed non-Bayesian
method for combating the overconfidence problem. While more elaborate Laplace approximations
can improve fidelity the further, our results provide virtually any ReLU network with a simple and
computationally lightweight way to mitigate overconfidence.
8
Under review as a conference paper at ICLR 2020
References
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Con-
crete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural
networks with rectified linear units. In ICLR, 2018.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. In ICML, 2015.
Andrew YK Foong, Yingzhen Li, Jose MigUel Hernandez-Lobato, and Richard E Turner. ’in-
between’uncertainty in bayesian neural networks. arXiv preprint arXiv:1906.11537, 2019.
Yarin Gal. Uncertainty in deep learning. University of Cambridge, 2016.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In ICML, 2016.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In ICML, 2017.
Arjun K Gupta and Daya K Nagar. Matrix variate distributions. Chapman and Hall/CRC, 1999.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In ECCV, 2016b.
Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield high-
confidence predictions far away from the training data and how to mitigate the problem. In CVPR,
2019.
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
examples in neural networks. In ICLR, 2017.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In CVPR, 2017.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In NIPS, 2017.
Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image
detection in neural networks. In ICLR, 2018.
Xuanqing Liu, Yao Li, Chongruo Wu, and Cho-Jui Hsieh. Adv-BNN: Improved adversarial defense
through robust bayesian neural network. In ICLR, 2019.
Christos Louizos and Max Welling. Multiplicative normalizing flows for variational Bayesian neural
networks. In ICML, 2017.
David JC MacKay. The evidence framework applied to classification networks. Neural computation,
1992.
Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confi-
dence predictions for unrecognizable images. In CVPR, 2015.
Hippolyt Ritter, Aleksandar Botev, and David Barber. A scalable laplace approximation for neural
networks. In ICLR, 2018.
Anqi Wu, Sebastian Nowozin, Edward Meeds, Richard E. Turner, Jose Miguel Hernandez-Lobato,
and Alexander L. Gaunt. Deterministic variational inference for robust bayesian neural networks.
In ICLR, 2019.
9
Under review as a conference paper at ICLR 2020
Appendix A	Proofs
Proposition A.1. Let f : Rd → R be a binary linear classifier defined by f ◦ φ(x) := wT φ(x)
where φ : Rn → Rd is any feature map and let p(w|D) := N (w|wMAP, Σ) be the distribution over
w. Then for any x ∈ Rn, we have σ(z ◦ φ(x)) = 0.5 if and only if σ (wMT AP φ(x)) = 0.5.
Proof. Denote μf := wMapΦ(x) and σf := φ(x)TΣφ(x) and let X ∈ Rn be arbitrary. For
the forward direction, suppose that σ(μf) = 0.5. This implies μf = 0, and We have σ(0∕(1 +
π∕8 σf )1/2) = σ(0) = 0.5. For the reverse direction, suppose that σ(μ/(1 + π∕8 σf )1/2) = 0.5.
This implies μf/(1 + ∏∕8 σf )1/2 = 0. Notice, the denominator of the l.h.s. is positive. Thus, it
follows that μf must be 0, implying that σ(μf) = 0.5.	□
Lemma A.2. Let X ∈ Rn be a vector and A ∈ Rn×n be an SPD matrix. Ifλmin(A) is the minimum
eigenvalue of A, then XT AX ≥ λmin kXk2.
Proof. Since A is SPD, it admits an eigendecomposition A = QΛQt and Λ = Λ2 Λ2 makes
sense. Therefore, by keeping in mind that QTX is a vector in Rn, we have
n
xt Ax = xT QΛ 2 Λ 2 QT x = ∣∣Λ 2 QT x『=X λi(QT x)2
i=1
n
≥ λmin(A) X(QT x)i2 = λmin(A)∣QTx∣2
i=1
= λmin(A)∣x∣2 ,
where the last equality is obtained as ∣QT x∣2 = xT QT Qx and noting that Q is an orthogonal
matrix.	□
Proposition A.3. Let f : Rn → R be a binary linear classifier defined by f(x) := wTx and
p(w∣D) := N(w∣μ, Σ) be the distribution over W. Thenfor any X ∈ Rn,
|z(x)| ≤
________|.T x|_______
pi+ ∏∕8 λmin(Σ)kxk2
(9)
Furthermore, if x ∈ Rn then as δ > 0 goes to infinity
lim ∣z(δx)∣ ≤	,	kμk	=
δ→∞	P∏∕8 λmin(Σ)
(10)
Proof. The first result follows directly from Lemma A.2 and by noting that the denominator of
eq. (4) is positive since Σ is symmetric positive-definite (SPD) by definition. For the second result,
let x ∈ Rn be arbitrary. By computation and again since the denominator of eq. (4) is positive, we
have
∣z(δx)∣ := /	—X)1	= = /	l"Tx|	.
√1 + π∕8(δx)T Σ(δx)	√1∕δ2 + π∕8 XT Σx
We would like to inspect the asymptotic behavior of z(δx) with respect to δ. First, for the sake of
completeness, we can compute that limδ→o ∣z(δx)∣ = 0. This reflects the case when δx goes to the
decision boundary. Now, for the case when δ → ∞, we can see that
Jim lz(δ)1
δ→∞
WT x|
pπ∕8 XT Σx
since 1∕δ2 → 0 as δ → ∞. Therefore, using Lemma A.2 and Cauchy-Schwarz inequality, we have
lim ∣z(δx)∣ ≤ /	l"TX|	≤ / k”kkxk = / kμk ,
δ→∞	P∏∕8 λmin(Σ)kxk2 _ P∏∕8 λmin(Σ)kxk	P∏∕8 λmin(Σ) ,
thus the proof is complete.	□
10
Under review as a conference paper at ICLR 2020
Lemma A.4 (Hein et al. (2019)). Let {Qi}lR=1 be the set of linear regions associated to the ReLU
network φ : Rn → Rn. For any x ∈ Rn there exists α ∈ R with α > 0 and t ∈ {1, . . . , R} such
that δx ∈ Qt for all β ≥ α. Furthermore, the restriction of φ to Qt can be written as an affine
function.	□
Theorem A.5. Let f : Rd → R be a binary linear classifier defined by f ◦ φ(x) := wT φ(x) where
φ : Rn → Rd is a ReLU network and let p(w∣D) := N(w∣μ, ∑) be the distribution over W. Then
for any x ∈ Rn,
|z◦ Φ(χ)l ≤
|〃T(VX+a)|
pi+ π∕8 λmiη(Σ)kVx + ak2
(11)
where V ∈ Rd×n
goes to infinity
and a ∈ Rd are some matrix and vector that depend on x. Furthermore, as δ > 0
lim |z ◦ φ(δx)∣ ≤
δ→∞
kμk
P∏∕8 λmin(Σ)
(12)
Proof. Let x ∈ Rn be arbitrary. By definition of ReLU network, there exists a linear region Q ⊂ Rn
along with V ∈ Rd×n and b ∈ Rd such that X ∈ Q and Φ∣q(x) := Vx + a. Applying eq. (4) to
Φ∣q(x) and following the proof of Proposition 2.3 yield
|z ◦ 0|Q(X)I
|〃T(VX + a)| ≤	|〃T(VX + a)|
pi + π∕8(Vx + a)TΣ(Vx +	^，1 + π∕8λmin(Σ)∣∣Vx + a∣∣2
(13)
thus the first result is obtained.
For the second result, by Lemma 3.1 of Hein et al. (2019) (also presented in Lemma A.4) there exists
α > 0 and a linear region R, along with U ∈ Rd×n and c ∈ Rd, such that for any δ ≥ α, we have
that δx ∈ R and the restriction φ∣R can be written as Ux + c. Therefore, for any such δ,
|z ◦ Φ∣R(δx)∣
∣μτ (δUx + c)∣___
Pl + π∕8 (δUx + b)τΣ(δUx + C)
|NT(UX + 1 c)|_____
q δ2 + π/8 (UX + 1 C)T ς(UX + 1 C)
Now, notice that as δ → ∞, 1∕δ2 and 1∕δ goes to zero. So, in the limit, we have that
lim |z ◦ Φ∣R(δx)∣
δ→∞
∣μτ (Ux)I
P∏∕8(UX)T Σ(Ux)
Again, following the proof of Proposition 2.3 (i.e. using Cauchy-Schwarz and Lemma A.2), we can
upper-bound this limit with
lim Iz ◦ φIR(δx)I ≤
δ→∞
kμkkUxk	= kμk
Pπ∕8 λmin(Σ)kUxk2	Pπ∕8 λmin(Σ)
which concludes the proof.
□
Corollary A.6 (λmin(Σ) from a desired upper confidence bound on ReLU networks). Let f ◦ φ,
with φ : Rn → Rd and f : Rd → R, be a ReLU network defined by f ◦ φ(x) := wT φ(x) and
N (w∣μ, Σ) be the distribution over W where the mean μ is fixed and Σ is any SPD matrix. Then:
(i)	For any > 0 there exists Σ such that for any x ∈ Rn far away from the training data, we
have that Iz ◦ φ(x)I ≤ .
(ii)	For any 0.5 < p < 1 there exists Σ such that for any x ∈ Rn far away from the training data,
we have that σ(Iz ◦ φ(x)I) ≤ p.
11
Under review as a conference paper at ICLR 2020
Proof. We begin with (i). Let c > 0 and δ = ∏8 (9).Pick any Σ SPD with λmin(Σ) = δ. Then,
by eq. (12) of Theorem 2.4 and our choice of λmin(Σ), for any z ∈ Rn, asymPtotically We have that
lim Iz ◦ φ(δz)I ≤
δ→∞
kμk
kμk
-/	= ―/	= C ,
∙√∏∕8 λmin(Σ)	√∏∕8 δ
which is the desired result.
For (ii), let 0.5 < p < 1 be arbitrary. Observe that the inverse logistic function is given by σ-1(x) :=
log x∕(1 - x) for 0 < x < 1 and it is positive for 0.5 < x < 1. Therefore by setting C in (i) with
σ-1(p), We Can Pick Σ SPD With λmin(Σ) = ∏8 Qkμ(P)) and verify that for any X ∈ Rn this gives
Iz(X)I ≤ σ-1(p). Thus, for any X ∈ Rn far away from the training data, since σ is monotonic, we
have that
σ(Iz(X)I) ≤ σ(σ-1(P))
p,
and the proof is complete.
□
Proposition A.7. Let f ◦ φ, with φ : Rn → Rd and f : Rd → R, be a ReLU network defined by
f ◦ φ(X) := wT φ(X), modeling a Bernoulli distribution with p(y = 1IX, w) = σ(f ◦ φ(X)). Let
p(w∣D) := N(w∣μ, ∑) be the posterior over W, obtained via a Laplace approximation with prior
N (w∣0, σ2I). Suppose H is the Hessian w.r.t. W at μ of the negative log-likelihood of the model.
Then
(i)	H = Px∈D β(X) φ(X)φ(X)T where β(X) := σ(f ◦ φ(X))(1 - σ(f ◦ φ(X))).
(ii)	For each i = 1, . . . , d, the ith eigenvalue λi(Σ) ofΣ is a non-decreasing function of σ02 with
limits 1∕λi(H) as σ02 → ∞ and 0 as σ02 → 0.
Proof. The negative log-likelihood of Bernoulli distribution is given by
-log Y p(yIX, w) = - X t logσ(f ◦ φ(X)) + (1 - t) log(1 - σ(f ◦ φ(X))) .
x,t∈D	x,t∈D
Now, observing that σ0(x) = σ(x)(1 - σ(x)) for all x ∈ R, we can comPute
Vw logσ(f ◦ φ(x)) = Vw log(1 - σ(f ◦ φ(x))) = σ(f ◦ φ(x))(1 - σ(f ◦ φ(x))) φ(x)φ(X)T .
This implies Vw logp(y∣x, w) = σ(f ◦ φ(x))(1 一 σ(f ◦ φ(x))) φ(x)φ(X)T, since t ∈ {0,1} by
assumPtion. By considering all x, t ∈ D, we get (i).
For (ii), first we assume that all Hessians mentioned below are w.r.t. w. We note that the assumption
on the prior implies - log p(w) = 1∕2 wT(1∕σ02I)w + const, which has Hessian 1∕σ02I. Thus,
the Hessian of the negative log posterior - log p(wID) = - log p(w) - log Qx,t∈D p(yIx, w) is
1∕σ02I + H. This implies that the posterior covariance Σ of the Laplace approximation is given by
ς=& I+H)-1.	(14)
Therefore, the ith eigenvalue of Σ for any i = 1, . . . , n is
λi㈤=1∕σ2 + %(H) = 1+ σ2λi(H) .
For all i = 1, . . . , n, the derivative of λi(Σ) w.r.t. σ02 is 1∕(1 + σ02λi(H))2 which is non-negative.
This tells us that λi(Σ) is a non-decreasing function ofσ02. Furthermore, itis also clear that σ02∕(1 +
σ02λi(H)) goes to 1∕λi(H) as σ2 goes to infinity, while it goes to 0 as σ2 goes to zero.	□
12
Under review as a conference paper at ICLR 2020
Corollary A.8. Let f ◦ φ, with φ : Rn → Rd and f : Rd → R, be a ReLU network defined by
f ◦ φ(x) := WTφ(x), modelingp(y = 1|x, W) with σ(f ◦ φ(x)). Letp(w∣D) := N(w∣μ, ∑) be the
posterior over w, obtained via a Laplace approximation with priorN(w|0, σ02I). Then z ◦ φ(x) is
a non-increasing function of σ02 with limits
lim |z ◦ φ(x) | ≤
_________|〃T φ(X)I________
P1+ ∏∕(8 λmaχ(H))kφ(x)k2
lim Z ◦ φ(x) = μτφ(x),
σ02→0
where H is as defined in (i) of Proposition 2.6.
Proof. We can rewrite eq. (13) as follows.
lz ◦ 0|Q(X)I
___________lμτ φ(X)I_________
√1+ ∏∕8 Pd=ι λi(∑)(Qτφ(x))2
(15)
where Σ = Q diag(λi(Σ), . . . , λd(Σ)) QT is the eigendecomposition of Σ (cf. the proof of
Lemma A.2). It is therefore clear that the denominator of the r.h.s. is a non-decreasing function
of σ02 by virtue of Proposition 2.6. This implies Iz ◦ φIQ(x)I is a non-increasing function of σ02. For
the limits, Proposition 2.6 directly implies λmin(Σ) has limits 1∕λmax(H) and 0 whenever σ02 → ∞
and σ2 → 0, respectively. From these facts, the right limit is immediate from Lemma A.2 while the
left limit is directly obtained by noticing that the denominator of eq. (15) goes to 1 as σ02 → 0 and
hence also the denominator of Z ◦ φ∣Q (x).	□
Corollary A.9. Let f ◦ φ, with φ : Rn → Rd and f : Rd → R, be a ReLU network defined by
f ◦ φ(x) := wT φ(x), modeling p(y = 1Ix, w) with σ(f ◦ φ(x)). Let p(wID) := N (wIμ, Σ) be
the distribution over w. If either
(i)	∑ = C-1 ：= (∑x∈d Φ(x)Φ(x)T) — 1，or
(ii)	Σ is obtained via a Laplace approximation w.r.t. a prior N (wI0, σ02I) with σ02 → ∞ and
suppose H defined in (i) of Proposition 2.6 is invertible and the ordering of its eigenvalues is
the same as that of C, while the eigenvectors are the same as those of C,
then on any level set of μT φ(x), the confidence decreases faster in the direction where the training
data are sparser in the feature space Rd.
Proof. We start with (i). Let qi, qj be an arbitrary pair of eigenvectors of C where qi points in the
direction where the training data become sparser faster than in the direction qj . By the property of
covariance matrices, therefore φ(x)TC-1φ(x) increases faster along qi than along qj. Thus the
denominator of Z ◦ φ decrease faster in the direction of qi compared to those in the direction of qj.
This implies that on {x : μTx = c} for any constant c, the confidence Z ◦ φ decreases faster in the
direction qi than in the direction qj . Thus, (i) is proven.
For (ii), let qi, qj be an arbitrary pair of eigenvectors ofH where qi points in the direction where the
training data become sparser faster than in the direction qj . The assumption about the eigendecom-
position ofH means that the corresponding eigenvalues of qi and qj are λi(H) < λj(H), implying
λi(C) < λj (C). Therefore similar to φ(x)TC-1φ(x) (cf. the proof of (i)), φ(x)TH-1φ(x) in-
creases faster in the direction qi than in the direction qj . Now, since we assume that σ02 → ∞,
eq. (14) in Proposition 2.6 tells us that, Σ = H-1. Thus, it holds that in the direction qi, the de-
nominator of Z ◦ φ increases faster than in the direction qj . By following the same argument as the
proof of (i), We get the desired result.	□
Appendix B	Supplementary theoretical analysis
For completeness, We shoW that in the case of tWo-layer netWorks (i.e. netWorks With one hidden
layer) With saturating activation functions (e.g. sigmoid and tanh), the confidence goes to an exact
constant for any input point far aWay from the training points.
13
Under review as a conference paper at ICLR 2020
Proposition B.1. Let f : Rd → R be a linear classifier with a parameter w ∈ Rd and let g : Rd →
Rd be a component-wise function defined by g(z) := (h(z1), . . . , h(zd))T for some saturating
h : R → R, which has limx→∞ h(x) = l. Let also φ : Rn → Rd defined as φ(x) := g(Vx + a) for
some V ∈ Rd×n and a ∈ Rd be afeature map. Suppose p(w∣D) := N(w∣μ, ∑) is the distribution
over w. Then for any x ∈ D, as δ > 0 goes to infinity
|z ◦ φ(δx)∣
∣ι P= ”,I
qι+∏∕8 J pd,j=ι∑j
Proof. By definition,
|z ◦ φ(δx)∣
∣“τ g(δVx + a) ∣____
P + π∕8 g(δVx + a)T Σg(δVx + a)
By definition ofg, limδ→∞ g(δVx + a) = (l, . . . , l)T =: l, which implies
lim ∣z ◦ φ(δx)∣
δ→∞
∣“τ 1|	=	Il P=1 ”,∣
p1 + π/8 lTς1	√1 + π∕812 Pd,j=ι Σij
□
Appendix C	Last-layer Laplace Approximation
The theoretical results in the main text essentially tell us that if we have a Gaussian approximate
posterior that comes from a Laplace approximation, then using eq. (1) (and eq. (2)) when making
a prediction can remedy the overconfidence problem on any ReLU network. In this section we
describe a simple Laplace method that can still capture the properties that we have presented in
Section 2. Concretely, we apply the Laplace approximation only to the linear last layer of ReLU
networks, that have been trained via MAP estimation. For the sake of clarity, we omit the bias in the
following and revisit the case where the bias is included at the end of this section.
For the binary classification case, let g : Rn → R be a MAP-trained deep ReLU neural network
with a linear last-layer. We can decompose g into a feature map φ : Rn → Rd and a linear clas-
sifier f : Rd → Rn which is defined by φ(x) 7→ wMTAPφ(x). Based on Proposition 2.6, we can
simply perform a Laplace approximation to get the posterior of the weight of the linear classifier
f, i.e. p(wID) = N (wIwMAP, H-1) where H is the Hessian of the negative log-posterior w.r.t. w
at wMAP. This Hessian could be obtained via automatic differentiation or via the explicit formula
stated in (i) of Proposition 2.6. We emphasize that we only deal with the weight at the last layer of
g, i.e. the weight of f, and not the weight of the whole network, thus the inversion of H is rarely
a problem. For instance, large models such as DenseNet-201 (Huang et al., 2017) and ResNet-152
(He et al., 2016a) have d = 1920 and d = 2048 respectively, implying that we only need to do the
inversion of a single 1920 × 1920 or 2048 × 2048 matrix once.4
In the case of multi-class classification, we now have f : Rd → Rk : φ(x) → WMAPφ(x). We
obtain the posterior over a random matrix W ∈ Rk×d in the form N (vec(W)Ivec(WMAP), Σ) for
some Σ ∈ Rdk×dk SPD. The procedure is still similar to the one described above, since the exact
Hessian of the linear multi-class classifier can still be easily and efficiently obtained via automatic
differentiation. Note that in this case we need to invert a dk × dk matrix, which, depending on the
size of k, can be quite large.5
For a more efficient procedure, we can make a further approximation to the posterior in the multi-
class case by assuming the posterior is a matrix Gaussian distribution. We can use the Kronecker-
factored Laplace approximation (KFLA) Ritter et al. (2018), but only for the last layer of the net-
work. That is, we find the Kronecker factorization of the Hessian HT ≈ VT 0 UT via automatic
differentiation.6 Then by definition of a matrix Gaussian (Gupta & Nagar, 1999), we immediately
4Based on the implementations available in the TorchVision package.
5For example, the ImageNet dataset has k = 1000.
6In practice, we take the running average of the Kronecker factors of the Hessian over the mini-batches.
14
Under review as a conference paper at ICLR 2020
obtain the posterior MN(W|WMAP, U, V). The distribution of the latent functions is Gaussian,
since f := Wφ(x) and p(W|D) = MN (W|WMAP, U, V) imply
P(HD) = MN(f∣WMApφ(x), U, Φ(x)TVφ(x))
=N(f∣WmapΦ(x), (Φ(x)TVφ(x))乳 U)= N(f∣WmapΦ(x), (Φ(x)TVφ(x))U), (16)
where the last equality follows since (φ(x)T Vφ(x)) is a scalar. We then have the following integral
p(y = i∣Φ(x),D) = / Softmax(f,i) N(f∣WmapΦ(x), (Φ(x)TVφ(x))U) df,
which can be approximated via MC-integration.
While one can always assume that the bias trick is already used, i.e. it is absorbed in the weight
matrix/vector, in practice when dealing with pre-trained networks, one does not have such liberty.
In this case, one can simply assume that the bias b or b is independent of the weight w or W,
respectively in the two- and multi-class cases. By using the same Laplace approximation procedure,
one can easily getp(b∣D) := N(b, μb, σ2) orp(b∣D) := N(b∣μb, ∑b). This implies WTφ(x)+b =:
f and Wφ(x) + b =: f are also Gaussians given by
N(f ∣μTφ(x)+μb, Φ(x)TH-1φ(x)+σ2) and N(f ∣Mφ(x)+b, (φ(x)T③I)Σ(φ(x)③I) + ∑b),
(17)
respectively, with I ∈ Rk×k if W ∈ Rk×d and φ(x) ∈ Rd. Similarly, in the case when the
Kronecker-factored approximation is used, we have
p(f∣D) = N(f∣WmapΦ(x) + μb, (Φ(x)TVφ(x))U + ∑b).	(18)
Because of the construction above, which is simply done by applying Laplace approximation on the
last layer of a ReLU network, we call this method last layer Laplace approximation or LLLA for
short. We present the pseudocodes of LLLA in Algorithms 1 and 2.
Algorithm 1 LLLA with exact Hessian for binary classification.
Input:
A pre-trained network f ◦ φ with WMAp as the weight of f, (averaged) cross-entropy loss L,
training set Dtrain, test set Dtest, mini-batch size m, running average weighting ρ, and prior
precision τ0 = 1 /σ2.
Output:
predictions P containing p(y = 1|x, Dtrain ) ∀x ∈ Dtest.
1:	Λ = 0 ∈ Rd×d
2:	for i = 1, . . . , |Dtrain |/m do	. Compute the Hessian over mini-batches
3:	Xi, yi = sampleMinibatch(Dtrain, m)	. Sample a mini-batch from the training set
4:	Ai, Bi = getHessian(L(f ◦ φ(Xi), yi), WMAp)	. Via autodiff
5:	Λi = m Λi + τ0 I	. I is the identity matrix with the appropriate size
6:	Λ = ρΛ + (1 - ρ)Λi
7:	end for
8:	Σ = Λ-1
9:	p(W|D) = N(W|WMAp, Σ)	. Laplace approximation of the posterior
10:	P = 0
11:	for all x ∈ Dtest do	. predictions
12:	p = σ(wMapΦ(x)∕(1 + ∏∕8 φ(x)τΣφ(x))1/2)	. Equation (3)
13:	P = P ∪ {p}	. Collect the prediction
14:	end for
Appendix D Training detail
We train all networks we use in Table 1 for 100 epochs with batch size of 128. The initial learning
rates are 0.001 and 0.1 for MNIST and CIFAR-10 experiments, respectively, and we divide them by
10 at epoch 50, 75, and 95. We use ADAM and SGD with 0.9 momentum, respectively. Standard
data augmentations, i.e. random crop and standardization are also used for training the network on
CIFAR-10. Meanwhile, for LLLA, we use the Kronecker-factored Hessian.
15
Under review as a conference paper at ICLR 2020
Algorithm 2 LLLA with Kronecker-factored Hessian for multi-class classification.
Input:
A pre-trained network f ◦ φ with WMAP as the weight of f, (averaged) cross-entropy loss
L, training set Dtrain, test set Dtest, mini-batch size m, number of samples s, running average
weighting ρ, and prior precision τo = 1 /σ].
Output:
Predictions P containing p(y = i|x, Dtrain) ∀x ∈ Dtest ∀i ∈ {1, . . . , k}.
1:
2:
A = 0 ∈ Rk×k, B = 0 ∈ Rd×d
for i = 1, . . . , |Dtrain|/m do
, yi = sampleMinibatch(Dtrain , m)
iii i
XAA BA
3:4:5: 6:7:
. Compute the Hessian over mini-batches
. Sample a mini-batch from the training set
,Bi = getKroneckerFactors(L(f ◦ φ(Xi), yi), WMAP)	. E.g. via KFAC or KFRA
=√m Ai + √τo i
二 √m Bi + √τ0 i
ρA + (1 - ρ)Ai
ρB + (1 - ρ)Bi
. I is the identity matrix with the appropriate size
9	: end for	
10	: U=A-1,V=B-1	
11	: p(W|D) =MN(W|WMAP,U,V)	. Laplace approximation of the posterior
12	P = 0	
13	: for all x ∈ Dtest do	. Predictions
14	P(HD) = N(f WMAPφ(x), (φ(x)TVφ(x))U)	. Distribution over the outputs of f
15	:	p=0	
16	:	for j = 1, . . . , s do	. Monte-Carlo integration
17	fj 〜P(HD)	
18	:	p = p + softmax(fj )	
19	:	end for	
20	:	p = p/m	
21	:	P=P∪{p}	. Collect the prediction
22	: end for	
Appendix E Further experiments
We further compare the OOD detection performance of LLLA to the temperature scaling method.
To find the optimal temperature, we follow the method of Guo et al. (2017). In particular, we use
the implementation provided by https://github.com/JonathanWenger/pycalib.
Table 2: Additional OOD detection results. For MMC, the lower the better in the OOD case, but the
higher the better for the in-distribution case; for AUROC, the higher the better.
MAP	Temp. scaling	LLLA
Train - Test	MMC	AUROC	MMC	AUROC	MMC	AUROC
MNIST - MNIST	0.993	-	0.990	-	0.962	-
MNIST - notMNIST	0.715	0.976	0.715	0.961	0.511	0.978
MNIST - EMNIST	0.803	0.944	0.803	0.914	0.608	0.947
MNIST - FMNIST	0.570	0.993	0.570	0.989	0.383	0.995
CIFAR-10 - CIFAR-10	0.910	—	0.875	—	0.880	—
CIFAR-10 - SVHN	0.716	0.813	0.716	0.748	0.606	0.823
CIFAR-10 - LSUN-CR	0.732	0.810	0.732	0.824	0.623	0.818
16