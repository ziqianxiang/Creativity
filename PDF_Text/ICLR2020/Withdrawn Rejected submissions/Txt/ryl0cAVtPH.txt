Under review as a conference paper at ICLR 2020
Deep Batch Active Learning by
Diverse, Uncertain Gradient Lower Bounds
Anonymous authors
Paper under double-blind review
Ab stract
We design a new algorithm for batch active learning with deep neural network models. Our
algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples
groups of points that are disparate and high-magnitude when represented in a hallucinated
gradient space, a strategy designed to incorporate both predictive uncertainty and sample
diversity into every selected batch. Crucially, BADGE trades off between diversity and
uncertainty without requiring any hand-tuned hyperparameters. While other approaches
sometimes succeed for particular batch sizes or architectures, BADGE consistently
performs as well or better, making it a useful option for real world active learning problems.
1	Introduction
In recent years, deep neural networks have produced state-of-the-art results on a variety of important
supervised learning tasks. However, many of these successes have been in domains where large amounts
of data are available. A promising approach for minimizing labeling effort is active learning, a supervised
learning protocol where labels can be requested by the algorithm in a sequential feedback-driven fashion.
Active learning algorithms aim to identify and label only maximally informative samples, so that a high-
performing classifier can be trained with minimal labeling effort. As such, a robust active learning algorithm
for deep neural networks may considerably expand the domains where these models are applicable.
How should a practical, general-purpose, label-efficient active learning algorithm for deep neural networks be
designed? Theory for active learning suggests a version-space-based approach (Balcan et al., 2006), which
explicitly or implicitly maintains a set of plausible models, and queries examples for which two models
that make different predictions. But when using highly-expressive models, these algorithms degenerate to
querying every example. Further, the computational overhead of training deep neural networks preclude
algorithms that update the model to best fit the data after each label query, as is often done (exactly or
approximately) for linear methods (Beygelzimer et al., 2010; Cesa-Bianchi et al., 2009). Unfortunately, the
theory provides little guidance for these models.
One option is to use the network’s uncertainty to inform a query strategy, for example by labeling samples
for which the model is least confident. However, in a batch setting this creates a pathological scenario where
data in the batch are nearly identical, a clear inefficiency. Remedying this issue, we could select samples to
maximize batch diversity, but this might choose points that provide little useful new information to the model.
For these reasons, methods that exploit just uncertainty or diversity do not consistently work well across model
architectures, batch sizes, or datasets. An algorithm that performs well when using a ResNet, for example,
might perform poorly when using a multilayer-perceptron. A diversity-based approach might work well
when the batch size is very large, but poorly when the batch size is small. Even what in practice constitutes
a “big” or “small” batch size is largely a function of the statistical properties of the data in question. These
1
Under review as a conference paper at ICLR 2020
weaknesses pose a major problem for real, practical batch active learning situations, where data are unfamiliar
and potentially unstructured. There is no way to know which active learning algorithm is best to use.
Further, in a real active learning scenario, every change of hyperparameters typically causes the algorithm to
label examples not chosen under other hyperparameters, provoking substantial labeling inefficiency. That is,
hyperparameter sweeps in active learning can be label expensive. Because of this, active learning algorithms
need to “just work”, given fixed hyperparameters, to a greater extent than is typical for supervised learning.
Based on these observations, we design an approach which creates diverse batches of examples about
which the current model is uncertain. We measure uncertainty as the gradient magnitude with respect
to parameters in the final (output) layer, which is computed using the most likely label according to the
model. To capture diversity, we collect a batch of examples where these gradients span a diverse set of
directions. More specifically, we build up the batch of query points based on these hallucinated gradients
using the k-MEANS++ initialization (Arthur and Vassilvitskii, 2007), which simultaneously captures both the
magnitude of a candidate gradient and its distance from previously included points in the batch. We name the
resulting approach Batch Active learning by Diverse Gradient Embeddings (BADGE).
We show that BADGE is robust to architecture choice, batch size, and dataset, generally performing as well
as or better than the best baseline across our experiments, which vary all of the aforementioned environmental
conditions. We begin by introducing our notation and setting, followed by a description of the BADGE
algorithm in Section 3, and experiments in Section 4. We defer discussion of related work to Section 5.
2	Notation and setting
Define [K] := {1, 2, . . . , K}. Denote by X the instance space, and Y the label space. In this work we consider
multiclass classification, so that Y = [K]. Denote by D the distribution from which examples are drawn, DX
the unlabeled data distribution, and DY|X the conditional distribution over labels given examples. We consider
the pool-based active learning setup, where the learner receives an unlabeled dataset U sampled according to
DX, and can request labels sampled according to DY|X for any x ∈ U. We use ED to denote expectation
under the data distribution D. Given a classifier h : X → Y, which maps examples to labels, and a labeled
example (x, y), We denote the 0/1 error of h on (x, y) as '01(h(χ), y) = I(h(χ) = y). The performance of a
classifier h is measured by its expected 0/1 error, i.e. ED ['01(h(χ), y)] = Pr(χ,y)〜D (h(χ) = y). The goal of
pool-based active learning is to find a classifier With a small expected 0/1 error, using as feW label queries as
possible. Given a set S of labeled examples (x, y), Where each x ∈ S is picked from U, folloWed by a label
query, We use ES as the sample averages over S.
In this paper, We consider classifiers h parameterized by underlying neural netWorks f of fixed architecture,
With the Weights in the netWork denoted by θ. We abbreviate the classifier With parameters θ as hθ since the
architectures are fixed in any given context, and our classifiers take the form hθ(x) = argmaxy∈[K] f(x; θ)y,
Where f(x; θ) ∈ RK is a vector of scores assigned to candidate labels, given the example x and parameters
θ. We optimize the parameters by minimizing the cross-entropy loss ES['ce(∕(x; θ),y)] over the labeled
examples, where 'ce(p, y) = PK=I I(y = i) ln 1/pi.
3	Algorithm
BADGE, described in Algorithm 1, starts by drawing an initial set of M examples uniformly at random from
U and asking for their labels. It then proceeds iteratively, performing two main computations at each step t: a
gradient embedding computation and a sampling step. Specifically, at each step t, for every x in the pool
U, we compute the label y(x) preferred by the current model, and the gradient gχ of the loss on (x, y(χ))
with respect to the parameters of the last layer of the network. Given these gradient embedding vectors
{gx : x ∈ U}, BADGE selects a set of points by sampling via the k-MEANS++ initialization scheme (Arthur
and Vassilvitskii, 2007). The algorithm queries the labels of these examples, retrains the model, and repeats.
We now describe the main computations — the embedding and sampling steps — in more detail.
2
Under review as a conference paper at ICLR 2020
Algorithm 1 BADGE: Batch Active learning by Diverse Gradient Embeddings
Require: Neural network f(x; θ), unlabeled pool of examples U, initial number of examples M, number of
iterations T, number of examples in a batch B.
1:	Labeled dataset S J M examples drawn uniformly at random from U together with queried labels.
2:	Train an initial model θ1 on S by minimizing ES [`(f (x; θ), y)].
3:	for t = 1, 2, . . . , T : do
4:	For all examples x in U \ S :
1.	Compute its hypothetical label y(x) = hθt(x).
2.	Compute gradient embedding gχ = ∂∂^ '(f (x; θ),y(x))∣θ=θt, where θ∩ut refers to parameters
of the final (output) layer.
5:	Compute St, a random subset of U\S, using the k-MEANS++ seeding algorithm, on {gx : x ∈ U \ S},
and query for their labels.
6:	S J S ∪ St .
7:	Train a model θt+ι on S by minimizing ES['ce(∕(x; θ), y)].
8:	end for
9:	return Final model θT+1.
The gradient embedding. Since deep neural networks are optimized using gradient-based methods, we
capture uncertainty about an example through the lens of gradients. In particular, we consider the model
uncertain about an example if knowing the label induces a large gradient of the loss with respect to the model
parameter, and hence a large update to the model. A difficulty with this reasoning is that we need to know
the label to compute the gradient. As a proxy, we compute the gradient as if the model’s current prediction
on the example is the true label. We show in Proposition 1 that, assuming a common structure satisfied
by most natural loss functions, the gradient norm with respect to the last layer using this label provides a
lower bound on the gradient norm induced by any other label. In addition, under that assumption, the length
of this hypothetical gradient vector captures the uncertainty of the model on the example: if the model is
highly certain about the example’s label, then the example’s gradient embedding will have a small norm (see
example below). Thus, the gradient embedding conveys information both about the model’s uncertainty and
potential update direction upon receiving a label at an example.
Consider a neural network f where the last nonlinearity is a softmax, i.e. σ(o)i = eoi/PjK=1 eoj . Specifically,
f is parametrized by θ = (W, V ), where θout = W = (W1>, . . . , WK>)> ∈ RK×d are the weights of the last
layer, and V consists of weights of all earlier layers. This means that f (x; θ) = σ(W ∙ z(x; V)), where Z
is the nonlinear function that maps an input x to the output of the network’s penultimate layer. We define
gXy = ∂w'cE(f (x； θ), y) for a label y and gχ = gX^ as the gradient embedding in our algorithm, where
y = argmaxi∈[κ] p%. Fixing an unlabeled sample x, we define Pi = f (x; θ)i.
Proposition 1. For all y ∈ {1,..., K} ,denote by gX = ∂W 'ce(∕ (x； θ),y). Then
K
kgxyk2 = Xpi2 + 1 - 2py kz(x;V)k2.
i=1
Consequently, y = argminy∈[κ] ∣∣gy ∣∣.
Proof. Observe that by Equation (1),
KK
kgy k2 = X(Pi- I(y = i))2kz(x; V)k2 = (Xp2 + 1 - 2py) kz(x; V)k2.
i=1	i=1
The second part follows from the fact that y = argmaXy∈[κ] Py.	□
3
Under review as a conference paper at ICLR 2020
k-DPP -------- k^-means++
SVHN1 ResNet1 Batch size: 1000
Figure 1:	Left and center: Learning curves for BADGE versus k-DPP sampling with gradient embeddings
for different scenarios. Right: A run time comparison (seconds) for BADGE versus k-DPP sampling corre-
sponding to the middle scenario. The performance of the two sampling approaches nearly perfectly overlaps.
Each line is the average over five independent experiments. Standard errors are shown by shaded regions.
The sampling step. We want the newly-acquired labeled dataset to induce large and diverse changes to
the model. To this end, we want the selection procedure to favor both sample magnitude and batch diversity.
Specifically, we want to avoid the pathology of, for example, selecting a batch of k similar samples where
even just a single single label could alleviate our uncertainty on all remaining samples.
A natural way of making this selection without introducing additional hyperparameters is to sample from
a k-DPP (Kulesza and Taskar, 2011). That is, to select a batch of k points with a probability proportional to
the determinant of their Gram matrix. Recently, Derezinski and Warmuth (2018) showed that in experimental
design for least square linear regression settings, learning from samples drawn from a k-DPP can have
much smaller mean square prediction error than learning from any iid samples. In this process, when the
batch size is very low, the selection will naturally favor points with a large length, which corresponds to
uncertainty in our space. When the batch size is large, the sampler chooses points that are diverse because
linear dependence makes the Gram determinant equal to zero.
Unfortunately, sampling from a k-DPP is not trivial. Many sampling algorithms (Kang, 2013; Anari et al.,
2016) rely on MCMC, where mixing time poses a significant computational hurdle. The state-of-the-art
algorithm of (Derezinski, 2018) has a high-order polynomial running time in the batch size and the embedding
dimension. To overcome this computational hurdle, we suggest instead sampling using the k-MEANS++
seeding algorithm (Arthur and Vassilvitskii, 2007), originally made to produce a good initialization for
k-means clustering. k-MEANS++ seeding selects centroids by sampling points in proportion to their squared
distances from the nearest centroid that has already been chosen. For completeness, we give a formal
description of the k-MEANS++ seeding algorithm in Appendix A.
This simple sampler tends to produce diverse batches similar to a k-DPP. As shown in Figure 1, switching be-
tween the two samplers does not affect the active learner’s statistical performance while greatly improving the
computational performance. A thorough comparison on the running times and test accuracies of k-MEANS++
and k-DPP based gradient embedding sampling can be found in Appendix G.
Figure 2 describes the batch diversity and average gradient magnitude per selected batch for a variety of
sampling strategies. As expected, both k-DPPs and k-MEANS++ tend to select samples that are diverse (as
measured by the magnitude of their Gram determinant with batches) and high magnitude. Other samplers,
like furthest-first traversal for k-Center clustering (FF-k-CENTER), do not seem to have this property. The
FF-k-CENTER algorithm is the sampling choice of the CORESET approach to active learning, which we
describe in the proceeding section (Sener and Savarese, 2018). Appendix F discusses diversity with respect
to uncertainty-based approaches.
4
Under review as a conference paper at ICLR 2020
∕c-DPP
OpenML #6, MLP, Batch size: 100
10000	12000
o aoo 4on mo moo
#Labels queried
κ-meaπs++
----Rand
qaeq Jo≡eu 一 UUsφp 6o-∣
FF κ-ceπter
Figure 2:	A comparison of batch selection algorithms in gradient space. Left and center: Plots showing the
log determinant of the Gram matrix of the gradient embeddings within batches as learning progresses. Right:
The average embedding magnitude (a measurement of predictive uncertainty) in the selected batch. The
k-centers sampler finds points that are not as diverse or high-magnitude as other samplers. Notice also that
k-MEANS++ tends to actually select samples that are both more diverse and higher-magnitude than a k-DPP,
a potential pathology of the k-DPP’s degree of stochastisity. Standard errors are shown by shaded regions.
Example: multiclass classification with softmax activation. Recall that the neural network f has the
form f (x; θ) = σ(W ∙ z(x; V)), where σ(o)i = eo/Pj=I eoj, and the loss function is the cross entropy,
'ce(p, y) = PK=11(y = i)ln 1/pi. By substitution,
K
'cE(f (x; θ), y)=ln (X ewj∙z(Xv。- Wy ∙ z(χ; V).
j =1
Given this, the i-th block of gx is equal to
∂
(gx)i = ∂w'ce f(χ; θ),y) = (Pi -1 (y = O) z(χ; V).
(1)
Based on this expression, we can make the following observations:
1.	Each block of gx is a scaling of z(x; V), the output of the network’s penultimate layer. In this
respect, gx captures the representation information of z(x; V), the space in which diversity-based
algorithms operate (Sener and Savarese, 2018).
2.	Proposition 1 shows that the norm of gx is a lower bound on the norm of the loss gradient induced
by the example with true label y with respect to the last-layer weights, i.e. kgx k ≤ kgxy k. This
suggests that the norm of gx conservatively estimates the example’s influence on the current model.
3.	If the current model θ is highly confident about x, i.e. vector p is skewed towards a standard basis
vector ej, then y = j, and vector (Pi - I(y = i))K=ι has a small length. Therefore, gx has a small
length as well. Consequently, such high-confidence examples tend to have gradient embeddings
of small magnitude, which are unlikely to be repeatedly selected by k-MEANS++ at iteration t.
For the special case of binary logistic regression (K = 2 and z(x; V) = x), we provide further justifications
on why BADGE yields better updates than vanilla uncertainty sampling in Appendix B.
5
Under review as a conference paper at ICLR 2020
4	Experiments
We evaluate the performance of BADGE against several algorithms in the literature. In our experiments, we
seek to answer the following question: how robust are the learning algorithms to choices of neural network
architecture, batch size, and dataset?
To ensure a comprehensive comparison among all algorithms, we evaluate them in a batch mode active
learning setup, with M = 100 being the number of initial random labeled examples, and batch size B varying
from {100, 1000, 10000}. The following is a list of the baseline algorithms evaluated: the first algorithm
performs representative sampling; the next three algorithms are uncertainty based; the last algorithm is a
hybrid of representativeness and uncertainty based approaches.
1.	Coreset: active learning with coreset selection (Sener and Savarese, 2018), where the embedding
of each example is computed as the network’s output of the penultimate layer, and the samples at
each round are selected using a greedy furthest-first traversal conditioned on all labeled examples.
2.	Conf (Confidence Sampling (Wang and Shang, 2014)): uncertainty-based active learning algorithm
that selects B examples with the smallest predicted class probability, maxiK=1 f (x; θ)i .
3.	Marg (Margin Sampling (Roth and Small, 2006)):uncertainty-based active learning algorithm that
selects the bottom B examples sorted according to the example’s multiclass margin, defined as
f (x; θ)y - f (x; θ)yo, where y and y0 are the indices of the largest and second largest entries of
f(x; θ).
4.	Entropy (Wang and Shang, 2014): uncertainty-based active learning algorithm that selects the
top B examples according to the entropy of the example’s predictive class probability distribution,
defined as H ((f (x; θ)y)yK=1), where H(p) = PiK=1 pi ln 1/pi.
5.	ALBL (Active Learning by Learning (Hsu and Lin, 2015)): A bandit-style meta-active learning
algorithm that selects between Coreset and Conf at every round.
6.	RAND: the naive baseline of randomly selecting k examples to query at each round.
We consider three neural architectures: a two-layer Perceptron with ReLU activations (MLP), an 18-layer
convolutional ResNet (He et al., 2016), and an 11-layer VGG network (Simonyan and Zisserman, 2014). We
evaluate our algorithms using three image datasets, SVHN (Netzer et al., 2011), CIFAR10 (Krizhevsky, 2009)
and MNIST (LeCun et al., 1998), 1 and four non-image datasets from the OpenML repository (#6, #155,
#156, and #184). 2 We study each situation with 7 active learning algorithms, including BADGE, making for
231 total experiments.
For the image datasets, the embedding layer in the MLP is 256. For the openML datasets, the embedding
dimensionality of the MLP is 1024, as more capacity is helpful to fit the training data. We fit models using
the cross entropy loss and the Adam variant of SGD until the training accuracy of the algorithm exceeds 99%.
We use a learning rate of 0.001 for image data and of 0.0001 for non-image data. We avoid warm-starting
and retrain models from scratch every time new samples are queried (Ash and Adams, 2019). All experiments
are repeated five times. No learning rate schedules or data augmentation are used for training.
Baselines use implementations from the libact library (Yang et al., 2017). All models are trained in
PyTorch (Paszke et al., 2017).
1Because MNIST is a dataset that is extremely easy to classify, we only use MLPs, rather than convolutional networks,
to better study the differences between active learning algorithms.
2The OpenML datasets are from openml.org, and are selected on two criteria: first, they have at least 10000
samples; second, neural networks have a significantly smaller test error rate when compared to linear models.
6
Under review as a conference paper at ICLR 2020
ALBL -------- Conf -------- Coreset -------- BADGE ---------- Entropy ------- Marg --------- Rand
SVHN, ResNet, Batch size: 100
OPenML#156, MLP, Batch size: 1000
10000	1S000	20000	25000	30000	35∞0
#Labels queried
(a)
500	1000	1500	2000	25∞	3∞0	358	408
#Labels queried
(b)
CIFAR10, VGG, Batch size: 10000
5∞0	10000	1508	20000	25000	388	350∞	4080
#Labels queried
(c)
Figure 3: Active learning test accuracy versus the number of total labeled samples for a range of conditions.
Standard errors are shown by shaded regions.
Learning curves. Here we show examples of learning curves that highlight some of the phenomena we
observe related to the fragility of active learning algorithms with respect to batch size, architecture, and
dataset. Often, we see that in early rounds of training, it is better to do diversity sampling, and later in
training, it is better to do uncertainty sampling. This kind of event is demonstrated in Figure 3a, which
shows Coreset outperforming confidence-based methods at first, but then doing worse than these methods
later on. BADGE does about as well as representative sampling when that strategy does best, and as well
as uncertainty sampling once those methods start outpacing Coreset. This suggests that BADGE is a good
choice regardless of labeling budget.
Separately, We notice that diversity sampling only
seems to work well when either the model has good
priors (inductive biases) built in, or when the data
are easy to learn. Otherwise, penultimate layer repre-
sentations are not meaningful, and diverse sampling
can be deleterious. For this reason, Coreset often
performs worse than random on sufficiently complex
data when not using a convolutional network (Fig-
ure 3b). That is, the diversidy induced by uncondi-
tional random sampling can yield a batch that better
represents the data. Even when batch size is large
and the model has helpful inductive biases, the un-
certainty information in BADGE can give it an ad-
vantage over pure diversity approaches (Figure 3c).
Comprehensive plots of this kind, spanning archi-
tecture, dataset, and batch size are in Appendix C.
PairWise comparisons. We next give a compre-
hensive comparison over all pairs of algorithms over
all datasets (D), batch sizes (B), model architectures
(A), and label budgets (L). From the learning curves,
it can be observed that when the label budgets are
large enough, the algorithms eventually reach sim-
ilar performance, hence the comparison between
algorithms in the large sample limit is uninteresting.
For this reason, for each combination of (D, B, A),
BADGE ALBL Coreset Conf Marg Entropy Rand
-12
- 10
-8
-6
-4
2
-0
Figure 4: A pairwise penalty matrix over all experiments.
Element Pij corresponds roughly to the number of times
algorithm i outperforms algorithm j. Column-wise averages
at the bottom show overall performance (lower is better).
BADGE
ALBL
Coreset
Conf
Marg
Entropy
Rand
0.0
9.18
10.97
12.56
3.88
13.16
10.04
0.34	0.0
1.65	2.02
0.54
0.96
0.31
0.84
0.66
5.18
3.15
0.31
6.81
4.95
0.0
6.55
3.08
8.13
6.56
2.78
7.14
1.79
5.63
4.08
7.61
0.0
0.33
5.96	6.08
10.87
9.34
0.0
11.33
9.34
6.05
1.25
0.35
0.0
5.12
7.61
8.67
2.98
10.65
0.0
6.9	5.93
1.56
8.01
6.01
7
Under review as a conference paper at ICLR 2020
we select a set of labeling budgets L where learning is still progressing. As there are 3 choices of batch sizes,
and in total 11 choices of (dataset, architecture) combinations, the total number of (D, B, A) combinations is
3 × 11 = 33. Specifically, we compute n0, the smallest number of labels where RAND’s accuracy reaches
99% of its final accuracy, and choose label budget L from {M + 2i-1 B : i ∈ [[log((no - M )∕B)C]}. The
calculation of scores in the penalty matrix P follows the protocol: for each (D, B, A, L) combination and
algorithms i, j, we have 5 test errors (one for each repeated run) ei1, . . . , ei5 and ej1, . . . , ej5 for each
algorithm respectively.
We compute the t score as t = √5μ∕σ, where
ι 5	ι 5
μ = 5 X(ei- ej),σ = t 4 X(ei- ej- μ尸.
l=1	l=1
We use the two-sided t-test to compare pairs of algorithms: Algorithm i is said to beat algorithm j in this
setting ift > 2.776 (the critical point of p-value being 0.05), and similarly algorithm j beats algorithm i if
t < -2.776. For each (D, B, A) combination, suppose there are nD,B,A different values of L, then for each
L, if algorithm i beats algorithm j, we accumulate a penalty of 1/nD,B,A to Pi,j; otherwise, if algorithm j
beats algorithm i, we accumulate a penalty of 1/nD,B,A to Pj,i. The choice of the penalty value 1/nD,B,A is
to ensure that every (D, B, A) combination gets equal share in the aggregated matrix. Therefore, the largest
entry of P is at most 33, the total number of (D, B, A) combinations. Intuitively, each row i indicates the
number of settings where algorithm i beats other algorithms; and each column j indicates the number of
settings where algorithm j gets beaten by other algorithms.
The penalty matrix in Figure 4 summarizes all experiments, showing that BADGE generally outperforms
baselines. Matrices grouped by batch size and architecture are in Appendix D, each suggesting that BADGE
outperforms other algorithms.
Cumulative distribution functions of normalized errors.
For each (D, B, A, L) combination, we have five average
errors for each algorithm i: & = 5 P5=ι ei∙ To ensure
that the errors of these algorithms are on the same scale
in all settings, we compute the normalized error of every
algorithm i, defined as nei = &/却,where r is the index of
the Rand algorithm. By definition, the normalized errors of
the Rand algorithm are identically 1 in all settings. Same
as in the generation of penalty matrices, for each (D, B, A)
combination, we only consider a subset of L values from the
set M + 2i-1B : i ∈ [blog((n0 - M)/B)c]}; in addition, we
assign a weight proportional to 1/nD,B,A to each (D, B, A, L)
combination, where there are nD,B,A different L values for this
combination of (D, B, A). We then plot the cumulative distribu-
tion functions (CDFs) of the normalized errors of all algorithms:
for a value of x, the y value is the total weight of settings where
the algorithm has normalized error at most x; in general, an
algorithm that has a higher CDF value has better performance.
---BADGE ------- Coreset   Marg ---------- Rand
ALBL ----------- Conf ------ Entropy
Figure 5: Cumulative distribution function
of the normalized errors of all algorithms.
We show the generated CDFs in Figures 5, 22 and 23. We can see from Figure 5 that, BADGE has the best
overall performance. In addition, from Figures 22 and 23 in Appendix E, we can conclude that, when the
batch sizes are small (100 or 1000), or when the MLP model is used, both BADGE and Marg outperform
the rest. However, in the regime when the batch size is large (10000), Marg’s performance degrades, while
BADGE, ALBL and Coreset are the best performing approaches.
8
Under review as a conference paper at ICLR 2020
5	Related work
Active learning is a well-studied problem (Settles, 2010; Dasgupta, 2011; Hanneke, 2014). There are two
major strategies for active learning algorithms: representative sampling and uncertainty sampling.
In representative sampling, the algorithm selects a batch of examples that are representative of the unlabeled
set to ask for labels. The high-level idea is that the set of examples chosen, once labeled, can act as a
surrogate for the full dataset. Performing loss minimization on the surrogate suffices to ensure a low error
with respect to the full dataset. In the context of deep learning, (Sener and Savarese, 2018; Geifman and
El-Yaniv, 2017) select representative examples based on core-set construction, a fundamental problem in
computational geometry. Inspired by generative adversarial learning, (Gissin and Shalev-Shwartz, 2019)
selects samples that are maximally indistinguishable from the pool of unlabeled examples.
On the other hand, uncertainty sampling is based on a different principle: it tries to select new samples
that maximally reduce the uncertainty the algorithm has on the target classifier. In the context of linear
classification, (Tong and Koller, 2001; Schohn and Cohn; Tur et al., 2005) propose uncertainty sampling
methods that query examples that lie closest to the current decision boundary. Some uncertainty sampling
approaches have theoretical guarantees on consistency (Hanneke, 2014; Balcan et al., 2006). Such methods
have also been recently generalized to deep learning. For instance, (Gal et al., 2017) uses Dropout as an
approximation of the posterior of the model parameters, and develop information-based uncertainty reduction
criteria; inspired by recent advances on adversarial examples generation, (Ducoffe and Precioso, 2018) uses
the distance between an example and one of its adversarial examples as an approximation of its distance to
the current decision boundary, and uses it as the criterion of label queries. An ensemble of classifiers could
also be used to effectively estimate uncertainty (Beluch et al., 2018).
There are several existing approaches that support a hybrid of representative sampling and uncertainty sam-
pling. For example, (Baram et al., 2004; Hsu and Lin, 2015) present meta-active learning algorithms that
can combine the advantages of different active learning algorithms. Inspired by expected loss minimization,
(Huang et al., 2010) develops label query criteria that balances between the representativeness and informa-
tiveness of examples. Another method for this is Active Learning by Learning (Hsu and Lin, 2015), which
can select whether to exercise a diversity based algorithm or an uncertainty based algorithm at each round of
training as a sequential decision process.
There is also a large body of literature on batch mode active learning, where the learner is asked to select
a batch of samples within each round (Guo and Schuurmans, 2008; Wang and Ye, 2015; Chen and Krause,
2013; Wei et al., 2015). In these works, batch selection is often formulated as an optimization problem, with
objectives based on (upper bounds of) average log-likelihood, average squared loss, etc.
A different query criterion based on expected gradient length (EGL) has been proposed in the literature (Settles
et al., 2008). In recent work, (Huang et al., 2016) show that the EGL criterion is related to the T -optimality
criterion in experimental design; in addition, they show that the samples selected by EGL are very different
from those by entropy-based uncertainty criterion. (Zhang et al., 2017a) uses the EGL criterion in active
sentence and document classification with CNNs. These works differ most substatially from BADGE in that
they do not take into account the diversity of the examples queried within each batch.
There are many theoretical works that focus on the related problem of adaptive subsampling for fully-labeled
datasets in regression settings (Han et al., 2016; Wang et al., 2018; Ting and Brochu, 2018). Empirical studies
of batch stochastic gradient descent also employ adaptive sampling to “emphasize” hard or representative
examples (Zhang et al., 2017b; Chang et al., 2017). These works aim at reducing computation costs or finding
a better local optimal solution, as opposed to reducing label costs. Nevertheless, our work is inspired by their
sampling criteria, which also emphasizes samples that induce large updates to the model.
As mentioned earlier, our sampling criterion has resemblance to sampling from k-determinantal point
processes (k-DPPs) (Kulesza and Taskar, 2011). Note that in multiclass classification settings, our gradient-
9
Under review as a conference paper at ICLR 2020
based embedding of an example can be viewed as the outer product of the original embedding in the
penultimate layer and a probability score vector that encodes the uncertainty information on this example (see
Section 3). In this view, the penultimate layer embedding characterizes the diversity of each example, whereas
the probability score vector characterizes the quality of each example. The k-DPP is also a natural probabilistic
tool for sampling that trades off between quality and diversity (See Kulesza et al., 2012, Section 3.1).
6	Discussion
We have established that BADGE is empirically an effective deep active learning algorithm across different
architectures and batch sizes, performing similar to or better than other active learning algorithms. A funda-
mental remaining question is: “Why?” While deep learning is notoriously difficult to analyze theoretically,
there are several intuitively appealing properties of BADGE:
1.	The definition of uncertainty (a lower bound on the gradient magnitude of the last layer) guarantees
some update of parameters.
2.	It optimizes for diversity as well as uncertainty, eliminating a failure mode of choosing many
identical uncertain examples in a batch, and does so without requiring any hyperparameters.
3.	The randomization associated with the k-MEANS++ initialization sampler implies that even for
adversarially constructed datasets it eventually converges to a good solution.
The combination of these properties appears to generate the robustness that we observe empirically.
References
Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. In Machine Learning,
Proceedings of the Twenty-Third International Conference (ICML 2006), Pittsburgh, Pennsylvania, USA,
June 25-29, 2006, pages 65-72, 2006.
Alina Beygelzimer, Daniel J Hsu, John Langford, and Tong Zhang. Agnostic active learning without
constraints. In Advances in Neural Information Processing Systems, pages 199-207, 2010.
Nicolo Cesa-Bianchi, Claudio Gentile, and Francesco Orabona. Robust bounds for classification via selective
sampling. In Proceedings of the 26th annual international conference on machine learning, pages 121-128.
ACM, 2009.
David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In Proceedings of the
eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027-1035. Society for Industrial
and Applied Mathematics, 2007.
Alex Kulesza and Ben Taskar. k-dpps: Fixed-size determinantal point processes. In Proceedings of the 28th
International Conference on Machine Learning (ICML-11), pages 1193-1200, 2011.
MichaI Derezinski and Manfred K Warmuth. Reverse iterative volume sampling for linear regression. The
Journal of Machine Learning Research, 19(1):853-891, 2018.
Byungkon Kang. Fast determinantal point process sampling with application to clustering. In Advances in
Neural Information Processing Systems, pages 2319-2327, 2013.
Nima Anari, Shayan Oveis Gharan, and Alireza Rezaei. Monte carlo markov chain algorithms for sampling
strongly rayleigh distributions and determinantal point processes. In Conference on Learning Theory,
pages 103-115, 2016.
10
Under review as a conference paper at ICLR 2020
MichaI Derezinski. Fast determinantal point processes via distortion-free intermediate sampling. arXiv
preprint arXiv:1811.03717, 2018.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach.
In International Conference on Learning Representations, 2018. URL https://openreview.net/
forum?id=H1aIuk-RW.
Dan Wang and Yi Shang. A new active labeling method for deep learning. In 2014 International joint
conference on neural networks (IJCNN), pages 112-119. IEEE, 2014.
Dan Roth and Kevin Small. Margin-based active learning for structured output spaces. In European
Conference on Machine Learning, pages 413-424. Springer, 2006.
Wei-Ning Hsu and Hsuan-Tien Lin. Active learning by learning. In Twenty-Ninth AAAI conference on
artificial intelligence, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in
natural images with unsupervised feature learning. 2011.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Jordan T Ash and Ryan P Adams. On the difficulty of warm-starting neural network training. arXiv preprint
arXiv:1910.08475, 2019.
Yao-Yuan Yang, Shao-Chuan Lee, Yu-An Chung, Tung-En Wu, Si-An Chen, and Hsuan-Tien Lin. libact:
Pool-based active learning in python. arXiv preprint arXiv:1710.00379, 2017.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin,
Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
Burr Settles. Active learning literature survey. University of Wisconsin, Madison, 52(55-66):11, 2010.
Sanjoy Dasgupta. Two faces of active learning. Theoretical computer science, 412(19):1767-1781, 2011.
Steve Hanneke. Theory of disagreement-based active learning. Foundations and TrendsR in Machine
Learning, 7(2-3):131-309, 2014.
Yonatan Geifman and Ran El-Yaniv. Deep active learning over the long tail. arXiv preprint arXiv:1711.00941,
2017.
Daniel Gissin and Shai Shalev-Shwartz. Discriminative active learning, 2019. URL https://
openreview.net/forum?id=rJl-HsR9KX.
Simon Tong and Daphne Koller. Support vector machine active learning with applications to text classification.
pages 45-66, 2001.
Greg Schohn and David Cohn. Less is more: Active learning with support vector machines. Citeseer.
11
Under review as a conference paper at ICLR 2020
Gokhan Tur, Dilek Hakkani-Tur, and Robert E SchaPire. Combining active and semi-supervised learning for
spoken language understanding. Speech Communication, 45(2):171-186, 2005.
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1183-1192.
JMLR. org, 2017.
Melanie Ducoffe and Frederic Precioso. Adversarial active learning for deep networks: a margin based
approach. arXiv preprint arXiv:1802.09841, 2018.
William H Beluch, Tim Genewein, Andreas Nurnberger, and Jan M Kohler. The power of ensembles for
active learning in image classification. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 9368-9377, 2018.
Yoram Baram, Ran El Yaniv, and Kobi Luz. Online choice of active learning algorithms. Journal of Machine
Learning Research, 5(Mar):255-291, 2004.
Sheng-Jun Huang, Rong Jin, and Zhi-Hua Zhou. Active learning by querying informative and representative
examples. In Advances in neural information processing systems, pages 892-900, 2010.
Yuhong Guo and Dale Schuurmans. Discriminative batch mode active learning. In Advances in neural
information processing systems, pages 593-600, 2008.
Zheng Wang and Jieping Ye. Querying discriminative and representative samples for batch mode active
learning. ACM Transactions on Knowledge Discovery from Data (TKDD), 9(3):17, 2015.
Yuxin Chen and Andreas Krause. Near-optimal batch mode active learning and adaptive submodular
optimization. In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th International
Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 160-
168, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR. URL http://proceedings.mlr.press/
v28/chen13b.html.
Kai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data subset selection and active learning. In
International Conference on Machine Learning, pages 1954-1963, 2015.
Burr Settles, Mark Craven, and Soumya Ray. Multiple-instance active learning. In Advances in neural
information processing systems, pages 1289-1296, 2008.
Jiaji Huang, Rewon Child, and Vinay Rao. Active learning for speech recognition: the power of gradients.
arXiv preprint arXiv:1612.03226, 2016.
Ye Zhang, Matthew Lease, and Byron C Wallace. Active discriminative text representation learning. In
Thirty-First AAAI Conference on Artificial Intelligence, 2017a.
Lei Han, Kean Ming Tan, Ting Yang, and Tong Zhang. Local uncertainty sampling for large-scale multi-class
logistic regression. arXiv preprint arXiv:1604.08098, 2016.
HaiYing Wang, Rong Zhu, and Ping Ma. Optimal subsampling for large sample logistic regression. Journal
of the American Statistical Association, 113(522):829-844, 2018.
Daniel Ting and Eric Brochu. Optimal subsampling with influence functions. In Advances in Neural
Information Processing Systems, pages 3650-3659, 2018.
Cheng Zhang, Hedvig Kjellstrom, and Stephan Mandt. Determinantal point processes for mini-batch
diversification. arXiv preprint arXiv:1705.00607, 2017b.
12
Under review as a conference paper at ICLR 2020
Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. Active bias: Training more accurate neural
networks by emphasizing high variance samples. In Advances in Neural Information Processing Systems,
pages 1002-1012, 2017.
Alex Kulesza, Ben Taskar, et al. Determinantal point processes for machine learning. Foundations and
TrendsR in Machine Learning, 5(2-3):123-286, 2012.
Stephen Mussmann and Percy S Liang. Uncertainty sampling is preconditioned stochastic gradient descent
on zero-one loss. In Advances in Neural Information Processing Systems, pages 6955-6964, 2018.
A THE k-MEANS++ SEEDING ALGORITHM
Here we briefly review the k-MEANS++ seeding algorithm by (Arthur and Vassilvitskii, 2007). Its basic
idea is to perform sequential sampling of k centers, where each new center is sampled from the ground
set with probability proportional to the squared distance to its nearest center. It is shown in (Arthur and
Vassilvitskii, 2007) that the set of centers returned is guaranteed to approximate the k-means objective
function in expectation, thus ensuring diversity.
Algorithm 2 The k-MEANS++ seeding algorithm (Arthur and Vassilvitskii, 2007)
Require: Ground set G ⊂ Rd, size k.
Ensure: Center set C of size k.
Ci J {cι}, where ci is sampled uniformly at random from G.
for t = 2, . . . , k : do
DefineDt(x) := minc∈Ct-1 kx - ck2.
Ct J Sample X from G with probability P DtDt(X)2.
Ct J Ct-i ∪ {ct}.
end for
return Ck .
B BADGE for binary logistic regression
We consider instantiating BADGE for binary logistic regression, where Y = {-1, +1}. Given a linear
classifier w, we define the predictive probability of W on X as Pw(y∖x, θ) = σ(yw ∙ x), where σ(z) = ɪ+I-Z
is the sigmoid funciton.
Recall that y(x) is the hallucinated label:
(X)= +1 pw(+1∖X, θ) > 1/2
y(X) = -1 pw(+1∖X, θ) ≤ 1/2.
The logistic loss of classifier w on example (X, y) is defined as:
'(w, (x, y)) = ln(1 + exp(-y(x)w ∙ x)).
Now, given model W and example x, we define gχ = ∂∂w'(w, (x,y)) = (1 一Pw (y∖x, θ)) ∙ (-y ∙ x) as the loss
gradient induced by the example with hallucinated label, and Ox = ∂w '(w, (χ,y)) = (1-pw (y∖χ, θ))∙(-y ∙χ)
as the loss gradient induced by the example with true label.
13
Under review as a conference paper at ICLR 2020
- I I
A。BJnKjV
- t I
A。BJnMV
I I
A。BJnKjV
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
OPenML#L55, MLP, Batch size: 100
1JX)
10000
20000	30000	40000 BOOOO
#LabeIS queried
OR
0«
040
Figure 6: Full learning curves for OpenML #6 with MLP.
OPenML#155, MLP, BatCh size:1000
ιoooo
20000	30000
#LabeIS queried
4∞oo SmOo
0.70
0«
040
OPenML#155, MLPj Batch size: 10000
&BJnMV
10000
Z(XXX) 30000
#LabeIS queried
40000	e»00
---ALBL ------- Conf ----- Coreset --- BADGE ------ Entropy ---- Marg ----- Rand
Figure 7:	Full learning curves for OpenML #155 with MLP.
Suppose that BADGE only selects examples from region Sw = {x : W ∙ X = 0}, then as Pw (+1|x, θ)=
Pw (-1∣χ,θ) = 1, we have that for all X in Sw, gχ = Sx ∙ gχ for some Sx ∈ {±1}. This implies that, sampling
from a DPP induced by gx's is equivalent to sampling from a DPP induced by gx's. It is noted in Mussmann
and Liang (2018) that uncertainty sampling (i.e. sampling from D|Sw) implicitly performs preconditioned
stochastic gradient descent on the expected 0-1 loss. In addition, it has been shown that DPP sampling
over gradients may reduce the variance of the updates (Zhang et al., 2017b); this suggests that BADGE,
when restricted sampling over low-margin regions (Sw), improves over uncertainty sampling by collecting
examples that together induce lower-variance updates on the gradient direction of expected 0-1 loss.
C All learning curves
We plot all learning curves (test accuracy as a function of the number of labeled example queried) in Figures 6
to 12. In addition, we zoom into regions of the learning curves that discriminates the performance of all
algorithms in Figures 13 to 19.
D	Pairwise comparis ons of algorithms
In addition to Figure 4 in the main text, we also provide penalty matrices (Figures 20 and 21), where the
results are aggregated by conditioning on a fixed batch size (100, 1000 and 10000) or on a fixed neural
network model (MLP, ResNet and VGG). For each penalty matrix, the parenthesized number in its title is the
total number of (D, B , A) combinations aggregated; as discussed in Section 4, this is also an upper bound
on all its entries. It can be seen uncertainty-based methods (e.g. Marg) perform well only in small batch
size regimes (100) or when using MLP models; representative sampling based methods (e.g. Coreset)
only perform well in large batch size regimes (1000) or when using ResNet models. In contrast, BADGE’s
performance is competitive across all batch sizes and neural network models.
14
Under review as a conference paper at ICLR 2020
Oa
o«
&
国oao
Ion
Og
0«
oa
0.7
∣m
8 0∙e
04
Oa
92
__
&S3Q≤
OPenML#156, MLP, BatCh size: 1000
10000
20000	30000
#LabeIS queried
40000
&BJnMV
OpenML#156, MLP, Batch size: 10000
BOOO 10000 16000	20000 2BOOO 30000	3SOOO	40000
#LabeIS queried
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 8:	Full learning curves for OpenML #156 with MLP.
OPenML#184, MLP, Batch size: 100
BOOO 10000 ISXlO 20000
#LabeIS queried
OPenML#184, MLP, Batch size: 1000
AoeJn84
2BOO BOOO 7βW 10000 12S00 16000 17β00 20000
#LabeIS queried
OpenML#184, MLPj Batch size: 10000
Oa
AOeJn84
2β00 BOOO 780	10000 12BOO 1B000 17βOO 20000
#LabeIS queried

----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 9:	Full learning curves for OpenML #184 with MLP.
15
Under review as a conference paper at ICLR 2020
SVHN1 MLP1 Batch size: 100
SVHN1 MLP1 Batch size: 1000
6000
IWW	IWW	ZWW
#LabeIS queried
2608	30 OW
SVHN1 MLP1 Batch size: 10000
SVHNj ResNetj Batch size: 100
SVHN1 VGG1 Batch size: 1000
6000	10000 ISXlO 20000 2BOOO
#LabeIS queried
, W t
&S3O<
10000
20000	30000
#LabeIS queried
40000
SVHN1 VGG1 Batch size: 10000
BOOO 10000
16000 ZOOW 2BOOO 30000	3SOOO 40000
#LabeIS queried
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 10:	Full learning curves for SVHN with MLP, ResNet and VGG.
** t
MNISTj MLPj Batch size: 1000
αsβ
ffs3QΛ
20000	30000
#LabeIS queried
40000 SOOOO
OW
ιoooo
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 11:	Full learning curves for MNIST with MLP.
16
Under review as a conference paper at ICLR 2020
Accuracy	Accuracy	Accuracy	A*u 丝y
----	Oooooooo	OOOOO
CIFAR10, MLP, Batch size: 100
ιoo∞
20000	30000
#LabeIS queried
«0000 SOOOO
CIFARl10, ReSNeL BaIgh size: 100
10000	16000
#LabeIS
20000
queried
2680	30000
CIFAR10l MLP1 Batch size: 1000
92
0.1
10000
ιo∞o
ZWW	3 WW
#LabeIS queried
«000
, W t
&S3O<
20000	30000
#LabeIS queried
CIFAR10, ResNet1 Batch siæ: 1000
AoeJn84
AOeJn84
** t
&S3O<
BOOO 10000	16000	20000 2BOOO 30000	3SOOO	40000
#LabeIS queried
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 12: Full learning curves for CIFAR10 with MLP, ResNet and VGG.
OPenML#6, MLP, Batch size: 1000
OpenML#6, MLP, Batch size: IoO
Figure 13:
、 W t
，”，
04
2000	4000 βooo aooo
#LabeIS queried
I(XXX) 12000
OPenML#6, MLP, Batoh size: IoOoO
AOalrOOV
2000
4000 KOO aooo 10000
#LabeIS queried
---- Coreset -------- BADGE ---------- Entropy -------- Marg -------- Rand
Zoomed-in learning curves for OpenML #6 with MLP.
17
Under review as a conference paper at ICLR 2020
OpenML#155, MLP, Batch size: 100
1000	208	3000 WOO BOOO 88 TWO
#LabeIS queried
OPenML#155, MLPj Batch size: 1000
AOeJn84
,OPenML#155, MLP, BatCh size: 10000
1XX)	..	_	I_1
20000 XOOO 40000 SOOOC
#LabeIS queried
OM
IOOOO
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 14:	Zoomed-in learning curves for OpenML #155 with MLP.
OpenML#156, MLPj Batch size: 100
OpenML⅛156j MLP, Batch size: 1000
OPenML#156, MLPj Batch size: 10000
600	1000	1600	2000	2600 XKX
#LabeIS queried
600	1000	1600	2000	2∞0	3000	»00	«00
#LabeIS queried
6000 IOTOO 16000	20000	2∞00	30000	36000	«M»0
#LabeIS queried
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 15:	Zoomed-in learning curves for OpenML #156 with MLP.
AoalrOOV
OPenML#184, MLPj Batch size: 1000
OpenML#184, MLPj Batch size: 10000
Oa
0.7
δ,o,∙
£
F*
04
Oa
2β00 BOOO 7S>t>	10000 12BOO 1B0W 17β00	20000
#LabeIS queried
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 16:	Zoomed-in learning curves for OpenML #184 with MLP.
18
Under review as a conference paper at ICLR 2020
SVHN1 MLP1 Batch size: 100
^CU萼Cy o	o	Accuracy	Accuracy	Accuracy
--------	Ooooooooo	OOOOO
0.7
g
OS
04
Oa
92
09
MI
0.7
IM
04
Oa
92
).1
SVHN, MLP, Batch size: 1000
SVHN, MLP, Batch size: 10000
88	10000 IsXlO 20000 2BOOO
#LabeIS queried
10000
Z(XXX) 30000
#LabeIS queried
40000	e»00
2600	60w	7600 IUOW 126w ICOW 17600 2ww
#LabeIS queried
BOOO 10000 1BOOO 20000 2BOOO 30000 3S0>XI 40000
#LabeIS queried
SVHN1 VGGj Batch size: 1000
SVHNj ResNetj Batch size: 10000
10000
Z(XXX) 30000
#LabeIS queried
40000	e»00
SVHN, VGG, BatCh size: 100
10000 ISXlO Z(XXX) 2B000	30000
#LabeIS queried
88	10000 IsXlO 20000 2BOOO
#LabeIS queried
SVHN1 VGGj Batch size: 10000
BOOO 10000	16000	20000 2BOOO 30000	3SOOO	40000
#LabeIS queried
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 17: Zoomed-in learning curves for SVHN with MLP, ResNet and VGG.
MNIST1 MLP1 Batch size: 100
MNIST1 MLP1 Batch size: 1000
t).7β
0.7(∣
2e00 BOOO 7βtX) 10000 IzeX) 1BOOO 1780 ZQO8 2ZβOC
#LabeIS queried
ffs3QΛ
1BOOO 17βOO 20000
0.70
OAS
2β0t> BOOO 7βOO 188 12BOO
#LabeIS queried
ffε3aa
----ALBL ---------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg --------- Rand
Figure 18: Zoomed-in learning curves for MNIST with MLP.
19
Under review as a conference paper at ICLR 2020
Cl FAR 1 Oj MLPj Batch size： 100
IE
AoeJn84
CIFAR10l MLP1 Batch size: 1000
BOOO 10000 1BOOO 20000	2B000	30000	3∞∞
#LabeIS queried
040
(Me
AoeJn84
ιoo∞
AWJ	ΛWJ
#LabeIS queried
«000
CIFAR10l VGG1 Batch size: 100
ιoo∞
ZOwu
#Labels
30000
queried
«000
COW IOOW ICOW ZUOW ZWOQ 3U0W 36U0Q 4WW
#LabeIS queried
---ALBL ——Conf
----Coreset ——BADGE ------------------
Entropy
&S3O<
Oa
92
0.1
** t
&S3O<
----Marg -------- Rand
Figure 19: Zoomed-in learning curves for CIFAR10 with MLP, ResNet and VGG.
Batch size: IOOO(Il)
Batch size: IOOOO(Il)
BADGE ALBL CCreset Conf Marg Entropy Rand
BADGE
ALBL
Coreset
Conf
Marg
Entropy
Rand
0.0	3.58	3.96	4.37	1.96	4.85	4.37
0.14	0.0	2.14	1.02	0.0	2.12	2.2
0.62	1.19	0.0	2.0	1.04	3.02	23
0.0	1.12	3.26	0.0	0.0	2.71	2.65
0.17	2.24	4.1	3.51	0.0	4.58	3-99
0.0	0.62	2.45	0.33	0.0	0.0	1.95
0.14	2.2	2.5	3.05	134	3.95	0.0
0.15	1.56	2.63	2.04	0.69	3.03	2.57
2.5
2.0
1.5
1.0
0.5
0.0
0
Figure 20: Pairwise penalty matrices of the algorithms, grouped by different batch sizes. The parenthesized
number in the title is the total number of (D, B, A) combinations aggregated, which is also an upper bound on
all its entries. Element (i, j) corresponds roughly to the number of times algorithm i outperforms algorithm j.
Column-wise averages at the bottom show aggregate performance (lower is better). From left to right: batch
size = 100, 1000, 10000.
20
Under review as a conference paper at ICLR 2020
MLP(21)
ResNet(6)
BADGE ALBL COreSet Conf Marg Entropy Rand
BADGE ALBL CCreset Conf Marg Entropy Rand
10
8
6
4
2
0
1.6
1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0
VGG(6)
BAtXSE		ALBL Coreset		Conf	Marg Entropy Rand			
BADGE	0.0	0.59	0.0	0.53	0.29	0.92	l.02 '	-l.6
ALBL	0.0	0.0	0.0	0.33	0.l	0.73	l.3	-l.4
Coreset	0.29	0.39	0.0	0.2	0.14	0.96	l.69	-l.2
Conf	0.33	0.33	0.l4	0.0	0.33	0.0	0.68	-LO
Marg	0.33	0.l4	0.33	0.2	0.0	QA	l.25	-0.8
Entropy	0.l	O.I	0.0	0.0	0.0	0.0	0.63	-0.6
Rand	0.24	0.54	0.2	0.82	0.29	0.98	0.0	-0.4
		—		—	—	—		-0.2
	0.19	0.3	O.I	0.3	0.16	0.57	0.94	-o.o
Figure 21:	Pairwise penalty matrices of the algorithms, grouped by different neural network models. The
parenthesized number in the title is the total number of (D, B, A) combinations aggregated, which is also
an upper bound on all its entries. Element (i, j) corresponds roughly to the number of times algorithm i
outperforms algorithm j . Column-wise averages at the bottom show aggregate performance (lower is better).
From left to right: MLP, ResNet and VGG.
Batch size: 100
0.4	0.5	0«	0.7	0.8	0.9	1.0	1.1	1∙2
Normalized error
QW £ 4 2 Q
1 Ooooo
ΛXJ0nba⅛ ΘΛ⅛-nEno
Batch size: 1000
0 El⅞l.4NIXJ
1 O O O O O
ΛXJ0nba⅛ ΘΛ⅛-nEno
0.5	0.6	0.7	0.8	0.9	1.0	1.1	1∙2
Normalized error
Batch size: 10000
0.3	0.4	0.5	0.6	0.7	0.8	0.9	1.0	1.1	1.2
Normalized error
------ ALBL -------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg -------- Rand
Figure 22:	CDFs of normalized errors of the algorithms, group by different batch sizes. Higher CDF indicates
better performance. From left to right: batch size = 100, 1000, 10000.
E	CDFs of normalized errors of different algorithms
In addition to Figure 5 that aggregates over all settings, we show here the CDFs of normalized errors by
conditioning on fixed batch sizes (100, 1000 and 10000) in Figure 22, and show the CDFs of normalized
errors by conditioning on fixed neural network models (MLP, ResNet and VGG) in Figure 23.
F	Batch diversity
Figure 24 gives a comparison of sampling methods with gradient embedding in two settings (OpenML # 6,
MLP, batchsize 100 and SVHN, ResNet, batchsize 1000), in terms of uncertainty and diversity of examples
selected within batches. These two properties are measured by average `2 norm and determinant of the Gram
21
Under review as a conference paper at ICLR 2020
0.3	0.4	0.5	0«	0.7	0.8	0.9	1.0	1.1	1∙2
Normalized error
ResNet
IX)
gofg 8
ΛXJOnbθ4 ΘΛqB-nEno
0.3	0.4	0.5	0.6	0.7	0.8	0.9	1.0	1.1	1∙2
Normalized error
VGG
0.3	0.4	0.5	0.6	0.7	0.8	0.9	1.0	1.1	1.2
Normalized error
------ALBL --------- Conf -------- Coreset -------- BADGE ---------- Entropy -------- Marg -------- Rand
Figure 23:	CDFs of normalized errors of the algorithms, group by different neural network models. Higher
CDF indicates better performance. From left to right: MLP, ResNet and VGG.
matrix of gradient embedding, respectively. It can be seen that, k-MEANS++ (BADGE) induces good batch
diversity in both settings. Conf generally selects examples with high uncertainty, but in some iterations of
OpenML #6, the batch diversity is relatively low, as evidenced by the corresponding log Gram determinant
being -∞. These areas are indicated by gaps in the learning curve for CONF. Situations where there are
many gaps in the Conf plot seem to correspond to situations in which Conf performs poorly in terms of
accuracy. Both k-DPP and FF-k-CENTER ( an algorithm that approximately minimizes k-center objective)
select batches that have lower diversity than k-MEANS++ (BADGE).
G COMPARIS ON OF k-MEANS++ AND k-DPP IN BATCH SELECTION
In Figures 25 to 31, we give running time and test accuracy comparisons between k-MEANS++ and k-DPP
for selecting examples based on gradient embedding in batch mode active learning. We implement the k-DPP
sampling using the MCMC algorithm from (Kang, 2013), which has a time complexity of O(T ∙ (k2 + kd))
and space complexity of O(kd + k2), where τ is the number of sampling steps. We set τ as b5k ln kc in
our experiment. The comparisons for batch size 10000 are not shown here as the implementation of k-DPP
sampling runs out of memory.
It can be seen from the figures that, although k-DPP and k-MEANS++ are based on different sampling criteria,
the classification accuracies of their induced active learning algorithm are similar. In addition, when large
batch sizes are required (e.g. k = 1000), the running times of k-DPP sampling are generally much higher
than those of k-MEANS++.
22
Under review as a conference paper at ICLR 2020
OpenML #6, MLP, Batch size: 100
qυleq°Ee£LU」sgp 6o~∣
qυleq°Ee£LU」sgp 6o~∣
6	2∞0	4000	60∞	80∞	10000	120∞	14000
#Labels queried
(a)
OpenML #6, MLP, Batch size: 10O
SVHNj ResNetj Batch size: 1000
(b)
一 如3020,°
USeq C- Eoc-φɑ≡φ><
(c)
6000	8000	100∞	12000	14000
#Labels queried
0 8 6 4 2 0
qseqC-Eoc Vφɑ≡φ><
(d)
----k-DPP -------- k-means++ ------- Rand ------- FFkCenter ------- Conf
Figure 24:	A comparison of batch selection algorithms in gradient space. Plots a and b show the log
determinants of the Gram matrices of gradient embeddings within batches as learning progresses. Plots c
and d show the average embedding magnitude (a measurement of predictive uncertainty) in the selected
batch. The k-centers sampler finds points that are not as diverse or high-magnitude as other samplers. Notice
also that k-MEANS++ tends to actually select samples that are both more diverse and higher-magnitude than
a k-DPP, a potential pathology of the k-DPP’s degree of stochastisity. Among all algorithms, CONF has
the largest average norm of gradient embeddings within a batch; however, in OpenML #6, and the first few
interations of SVHN, some batches have a log Gram determinant of -∞ (shown as gaps in the curve), which
shows that Conf sometimes selects batches that are inferior in diversity.
OPenML#6, MLP, Batch size: 1000
k-DPP -----匕means++
Figure 25:	Learning curves and running times for OpenML #6 with MLP.
23
Under review as a conference paper at ICLR 2020
k-DPP
∕c-means++
OpenML#155, MLP, Batch size: 1000
Figure 26:	Learning curves and running times for OpenML #155 with MLP.
k-DPP
----∕c-means++
Figure 27:	Learning curves and running times for OpenML #156 with MLP.
k-DPP
----∕c-means++
Figure 28:	Learning curves and running times for OpenML #184 with MLP.
24
Under review as a conference paper at ICLR 2020
SVHN, MLP, BatCh size: 100
k-DPP
----∕c-means++
Figure 29:	Learning curves and running times for SVHN with MLP and ResNet.
k-DPP
----∕c-means++
Figure 30:	Learning curves and running times for MNIST with MLP.
25
Under review as a conference paper at ICLR 2020
k-DPP
----∕c-means++
IOOOO	20000	30000	40000
# Labds queried
Figure 31:	Learning curves and running times for CIFAR10 with MLP and ResNet.
26