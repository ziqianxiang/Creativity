Under review as a conference paper at ICLR 2020
Powered S GD: Powered Stochastic Gradient
Descent Methods for Accelerated
Non-Convex Optimization
Anonymous authors
Paper under double-blind review
Ab stract
We propose a novel technique for improving the stochastic gradient descent (SGD)
method to train deep networks, which we term PoweredSGD. The proposed Pow-
eredSGD method simply raises the stochastic gradient to a certain power γ ∈ [0, 1]
elementwise during iterations and introduces only one additional parameter, namely,
the power exponent γ (when γ = 1, PoweredSGD reduces to SGD). We further pro-
pose PoweredSGD with momentum, which we term PoweredSGDM, and provide
convergence rate analysis on both PoweredSGD and PoweredSGDM methods. Ex-
periments are conducted on popular deep learning models and benchmark datasets.
Empirical results show that the proposed PoweredSGD and PoweredSGDM obtain
faster initial training speed than adaptive gradient methods, comparable generaliza-
tion ability with SGD, and improved robustness to hyper-parameter selection and
vanishing gradients. PoweredSGD is essentially a gradient modifier via a nonlinear
transformation. As such, it is orthogonal and complementary to other techniques
for accelerating gradient-based optimization.
1	Introduction
Stochastic optimization as an essential part of deep learning has received much attention from both
the research and industry communities. High-dimensional parameter spaces and stochastic objective
functions make the training of deep neural network (DNN) extremely challenging. Stochastic gradient
descent (SGD) (Robbins & Monro, 1951) is the first widely used method in this field. It iteratively
updates the parameters of a model by moving them in the direction of the negative gradient of the
objective evaluated on a mini-batch. Based on SGD, other stochastic optimization algorithms, e.g.,
SGD with Momentum (SGDM) (Qian, 1999), AdaGrad (Duchi et al., 2011), RMSProp (Tieleman &
Hinton, 2012), Adam (Kingma & Ba, 2015) are proposed to train DNN more efficiently.
Despite the popularity of Adam, its generalization performance as an adaptive method has been
demonstrated to be worse than the non-adaptive ones. Adaptive methods (like AdaGrad, RMSProp
and Adam) often obtain faster convergence rates in the initial iterations of training process. Their
performance, however, quickly plateaus on the testing data (Wilson et al., 2017). In Reddi et al.
(2018), the authors provided a convex optimization example to demonstrate that the exponential
moving average technique can cause non-convergence in the RMSProp and Adam, and they proposed
a variant of Adam called AMSGrad, hoping to solve this problem. The authors provide a theoretical
guarantee of convergence but only illustrate its better performance on training data. However,
the generalization ability of AMSGrad on test data is found to be similar to that of Adam, and a
considerable performance gap still exists between AMSGrad and SGD (Keskar & Socher, 2017;
Chen et al., 2018). Indeed, the optimizer is chosen as SGD (or with Momentum) in several recent
state-of-the-art works in natural language processing and computer vision (Luo et al., 2018; Wu &
He, 2018), where in these instances SGD does perform better than adaptive methods. Despite the
practical success of SGD, obtaining sharp convergence results in the non-convex setting for SGD to
efficiently escape saddle points (i.e., convergence to second-order stationary points) remains a topic
of active research (Jin et al., 2019; Fang et al., 2019).
Related Works: SGD, as the first efficient stochastic optimizer for training deep networks, iteratively
updates the parameters of a model by moving them in the direction of the negative gradient of the
1
Under review as a conference paper at ICLR 2020
objective function evaluated on a mini-batch. SGDM brings a Momentum term from the physical
perspective, which obtains faster convergence speed than SGD. The Momentum idea can be seen
as a particular case of exponential moving average (EMA). Then the adaptive learning rate (ALR)
technique is widely adopted but also disputed in deep learning, which is first introduced by AdaGrad.
Contrast to the SGD, AdaGrad updates the parameters according to the square roots of the sum of
squared coordinates in all the past gradients. AdaGrad can potentially lead to huge gains in terms of
convergence (Duchi et al., 2011) when the gradients are sparse. However, it will also lead to rapid
learning rate decay when the gradients are dense. RMSProp, which first appeared in an unpublished
work (Tieleman & Hinton, 2012), was proposed to handle the aggressive, rapidly decreasing learning
rate in AdaGrad. It computes the exponential moving average of the past squared gradients, instead
of computing the sum of the squares of all the past gradients in AdaGrad. The idea of AdaGrad and
RMSProp propelled another representative algorithm: Adam, which updates the weights according to
the mean divided by the root mean square of recent gradients, and has achieved enormous success.
Recently, research to link discrete gradient-based optimization to continuous dynamic system theory
has received much attention (Yuan et al., 2016; Mazumdar & Ratliff, 2018). While the proposed
optimizer excels at improving initial training, it is completely complementary to the use of learning
rate schedules (Smith & Topin, 2019; Loshchilov & Hutter, 2016). We will explore how to combine
learning rate schedules with the PoweredSGD optimizer in future work.
While other popular techniques focus on modifying the learning rates and/or adopting momentum
terms in the iterations, we propose to modify the gradient terms via a nonlinear function called the
Powerball function by the authors of Yuan et al. (2016). In Yuan et al. (2016), the authors presented
the basic idea of applying the Powerball function in gradient descent methods. In this paper, we
1) systematically present the methods for stochastic optimization with and without momentum;
2) provide convergence proofs; 3) include experiments using popular deep learning models and
benchmark datasets. Another related work was presented in Bernstein et al. (2018), where the authors
presented a version of stochastic gradient descent which uses only the signs of gradients. This
essentially corresponds to the special case of PoweredSGD (or PoweredSGDM) when the power
exponential γ is set to 0. We also point out that despite the name resemblance, the power PowerSign
optimizer proposed in Bello et al. (2017) is a conditional scaling of the gradient, whereas the proposed
PoweredSGD optimizer applies a component-wise trasformation to the gradient.
Contributions: Inspired by the Powerball method in Yuan et al. (2016), this paper uses Powerball-
based stochastic optimizers for the training of deep networks. In particular, we make the following
major contributions:
1.	We propose the PoweredSGD, which is the first systematic application of the Powerball
function technique in stochastic optimization. PoweredSGD simply applies the Powerball
function (with only one additional parameter γ) on the stochastic gradient term in SGD.
Hence, it is easy to implement and requires no extra memory. We also propose the Pow-
eredSGDM as a variant of PoweredSGD with momentum to further improve its convergence
and generalization abilities.
2.	We have proved the convergence rates of the proposed PoweredSGD and PoweredSGDM. It
has been shown that both the proposed PoweredSGD and PoweredSGDM attain the best
known rates of convergence for SGD and SGDM on non-convex functions. In fact, to the
knowledge of the authors, the bounds we proved for SGD and SGDM (as special cases
of PoweredSGD and PoweredSGDM when γ = 1) provide the currently best convergence
bounds for SGD and SGDM in the non-convex setting in terms of both the constants and
rates of convergence (see, e.g. Yan et al. (2018)).
3.	Experimental studies are conducted on multiple popular deep learning tasks and benchmark
datasets. The results empirically demonstrate that our methods gain faster convergence
rate especially in the early train process compared with the adaptive gradient methods.
Meanwhile, the proposed methods show comparable generalization ability compared with
SGD and SGDM.
Outline: The remainder of the paper is organized as below. Section 2 proposes the PoweredSGD
and PoweredSGDM algorithms. Section 3 provides convergence results of the proposed algorithms
for non-convex optimization. Section 4 gives the experiment results of the proposed algorithms on
a variety of models and datasets to empirically demonstrate their superiority to other optimizers.
Finally, conclusions are drawn in section 5.
2
Under review as a conference paper at ICLR 2020
Notation: Given a vector a ∈ Rn , we denote its i-th coordinate by ai ; we use kak to denote its 2-norm
(Euclidean norm) and kak p to denote its p-norm for p ≥ 1. Given two vectors a, b ∈ Rn , we use
a ∙ b to denote their inner product. We denote by EH the expectation With respect to the underlying
probability space.
2	Algorithms
In this section, We present the main algorithms proposed in this paper: PoWeredSGD and PoWeredS-
GDM. PoWeredSGD combines the PoWerball function technique With stochastic gradient descent,
and PoWeredSGDM is an extension of PoWeredSGD to include a momentum term. We shall prove
in Section 3 that both methods converge and attain at least the best knoWn rates of convergence for
SGD and SGDM on non-convex functions, and demonstrate in Section 4 the advantages of using
PoWeredSGD and PoWeredSGDM compared to other popular stochastic optimizers for train deep
netWorks.
2.1	PoweredSGD
Train a DNN With n free parameters can be formulated as an unconstrained optimization problem
min f (x),
x∈Rn
(1)
where f (∙) : Rn → R is a function bounded from below. SGD proved itself an efficient and effective
solution for high-dimensional optimization problems. It optimizes f by iteratively updating the
parameter vector xt ∈ Rn at step t, in the opposite direction of a stochastic gradient g(xt , ξt) (where
ξt denotes a random variable), which is calculated on t-th mini-batch of train dataset. The update rule
of SGD for solving problem (1) is
xt+1 =xt - αtg(xt,ξt),	(2)
starting from an arbitrary initial point x1, where αt is known as the learning rate at step t . In the rest
of the article, let gt = g(xt , ξt ) for the sake of notation. We then introduce a nonlinear transformation
σγ(Z) = sign(Z)|z∣γ named as the Powerball function where sign(Z) returns the sign of z, or 0 if Z = 0.
For any vector z = (z1, . . . ,zn)T, the Powerball function σγ (z) is applied to all elements of z. A
parameter γ ∈ R is introduced to adjust the mechanism and intensity of the Powerball function.
Applying the Powerball function to the stochastic gradient term in the update rule (2) gives the
proposed PoweredSGD algorithm:
xt+1 = xt - αt σγ (gt ),	(3)
where γ ∈ [0, 1] is an additional parameter. Clearly, when γ = 1, we obtain the vanilla SGD (2). The
detailed pseudo-code of the proposed PoweredSGD is presented in Algorithm 1.
Algorithm 1 PowerSGD
Algorithm 2 PowerSGDM
1： Input: xι,	y∈ [0,1]		1:	Input: %i，vι, {αt}{=ι, β ∈ (O, l),y∈ [0,1]
2:	for / = ItOT do	2:	for / = ItOT do
3:	& =g(x*r)	3:	v?+i = βvt - atσγ(gt)
4:	Xt+ι = xt - o⅞σγ⅛,)	4:	⅜+l =X?+Vr+1
5:	end for	5:	end for
2.2	PoweredSGDM
The momentum trick inspired by physical processes Polyak (1964); Nesterov (1983) has been
successfully combined with SGD to give SGDM, which almost always gives better convergence rates
on train deep networks. We hereby follow this line to propose the PoweredSGD with Momentum
(PoweredSGDM), whose update rule is
vt+1 = βvt - αtσ(gt),
xt+1 = xt +vt+1 .
(4)
Clearly, when β = 0, PowerSDGM (4) reduces to PoweredSGD (3). Pseudo-code of the proposed
PoweredSGDM is detailed in Algorithm 2.
3
Under review as a conference paper at ICLR 2020
3	Convergence Analysis
In this section, we present convergence results of PoweredGD and PoweredSGDM in the non-convex
setting. We start with some standard technical assumptions. First, we assume that the gradient of the
objective function f is L-Lipschitz.
Assumption 3.1 There exists some L > 0 such that |Vf (X) — Vf (y)| ≤ LkX - y∣∣,forallx,y ∈ Rn.
We then assume that a stochastic first-order black-box oracle is accessible as a noisy estimate of the
gradient of f at any point X ∈ Rn, and the variance of the noise is bounded.
Assumption 3.2 The stochastic gradient oracle gives independent and unbiased estimate of the
gradient and satisfies:
E[g(x, ξ)]= Vf (x), E[kg(x, ξ) - Vf (x)k2] ≤ σ2, ∀x ∈ Rn,	(5)
where σ ≥ 0 is a constant.
We will be working with a mini-batch size in the proposed PoweredSGD and PoweredSGDM. Let
nt be the mini-batch size at the t-th iteration and the corresponding mini-batch stochastic gradient
be given by the average of nt calls to the above oracle. Then by Assumption 3.2 we can show that
E[kgt — Vf (xt) k2] ≤ σ2/n for all t ≥ 1. In other words, We can reduce variance by choosing a larger
mini-batch size (see Supplementary Material A.2).
3.1	Convergence analysis of PoweredS GD
We now state the main convergence result for the proposed PoweredSGD.
Theorem 3.1 Suppose that Assumptions 3.1 and 3.2 hold. Let T be the number of iterations.
PoweredSGD (3) with an adaptive learning rate and mini-batch size Bt = T (independent of a
particular step t) can lead to
1T
E T ∑ kgtk2+γ
≤ 2⅛ ΓL(f (X 1)- f) + σ
≤ T [	1 - ε + 2ε (1 - ε)
where ε ∈ (0,1), p = ∣+γ for any Y ∈ [0,1) and p = ∞ for Y = L
The proof of Theorem 3.1 can be found in the Supplementary Material A.2.
Remark 3.1 The proposed PoweredSGD and PoweredSGDM have the potential to outperform
popular stochastic optimizers by allowing the additional parameter Y that can be tuned for different
training cases, and they always reduce to other optimizers when setting Y= 1.
Remark 3.2 We leave ε ∈ (0, 1) to be a free parameter in the bound to provide trade-offs between
bounds given by the curvature L and StochaSticity σ .If σ = 0, we can choose ε → 0 and recover the
convergence bound for PoweredGD (see Supplementary Material A.1).
Remark 3.3 The above theorem provides a sharp estimate of the convergence of PoweredSGD in the
following sense. When Y = 1, the convergence bound reduces to the best known convergence rate for
SGD. Note that, because ofthe choice ofbatch size, it requires T2 gradient evaluations in T iterations.
So the convergence rate is effectively O (1/√T). This is the best known rate of convergence for SGD
Ge et al. (2015). When σ = 0 (i.e., exact gradients are used and Bt = 1), PoweredSGD can attain
convergence in the order O(1/T), which is consistent with the convergence rate of gradient descent.
3.2	Convergence analysis of PoweredS GDM
We now present convergence analysis for PoweredSGDM. The proof is again included in the Supple-
mentary Material B.2 due to the space limit.
4
Under review as a conference paper at ICLR 2020
Theorem 3.2 Suppose that Assumptions 3.1 and 3.2 hold. Let T be the number of iterations. For any
β ∈ [0, 1), PoweredSGDM (4) with an adaptive learning rate and mini-batch size Bt = T (independent
of a particular step t) can lead to
1T
T Σ k gt k1+γ
Tt=1
E
≤ 2⅛[L(f(X 1)- f) 1 + β + σ
≤ T [	1 - ε 1 - β + 2ε(1 - ε)
where ε ∈ (0,1), p = 1-+γ for any Y ∈ [0,1) and P = ∞ for Y = L
Remark 3.4 Convergence analysis of stochastic momentum methods for non-conveX optimization is
an important but under-eXplored topic. While our results on convergence analysis do not improve
the rate of convergence for stochastic momentum methods in a non-conveX setting, it does match
the currently best known rate of convergence (Yan et al., 2018; Bernstein et al., 2018) in special
cases (Y = 0, 1) and offers very concise upper bounds in terms of the constants. The upper bound
continuously interpolates the convergence rate for Y varying in [0, 1] and β varying in [0, 1). The
key technical result that made the results of Theorems 3.1 and 3.2 possible is Lemma B.1 in the
Supplementary Material, which provide a tight estimate of accumulated momentum terms. We also
note that the convergence rates for Y∈ (0, 1) are entirely new and not reported elsewhere before.
Even for the special case of Y= 0, 1, our proof differs from that of (Yan et al., 2018; Bernstein et al.,
2018) and seems more transparent.
Remark 3.5 A large mini-batch (Bt = T) is assumed for the convergence results to hold. This is
consistent with the convergence analysis in Bernstein et al. (2018) for the special case Y= 0. We
assume this because it enables us to put analysis of PoweredGD and PoweredSGD in a unified
framework so that we can obtain tighter bounds. In the stochastic setting, similar to Remark 3.3, we
note that our proof requires T2 gradient calls in T iterations and hence the effective convergence
rate is O(1/√T), which is consistent with the known rate OfConvergenCefor SGD (Ge et al., 2015).
4	Experiments
The propose of this section is to demonstrate the efficiency and effectiveness of the proposed Pow-
eredSGD and PoweredSGDM algorithms. We conduct experiments of different model architectures
on datasets in comparison with widely used optimization methods including the non-adaptive method
SGDM and three popular adaptive methods: AdaGrad, RMSprop and Adam. This section is mainly
composed of two parts: (1) the convergence and generalization experiments and (2) the Powerball
feature experiments. The setup for each experiment is detailed in Table 11. In the first part, we present
empirical study of different deep neural network architectures to see how the proposed methods
behave in terms of convergence speed and generalization. In the second part, the experiments are
conducted to explore the potential features of PoweredSGD and PoweredSGDM.
To ensure stability and reproducibility, we conduct each experiment at least 5 times from randomly
initializations and the average results are shown. The settings of hyper-parameters of a specific
optimization method that can achieve the best performance on the test set are chosen for comparisons.
When two settings achieve similar test performance, the setting which converges faster is adopted.
We can have the following findings from our experiments: (1) The proposed PoweredSGD and
PoweredSGDM methods exhibit better convergence rate than other adaptive methods such as Adam
and RMSprop. (2) Our proposed methods achieve better generalization performance than adaptive
methods although slightly worse than SGDM. (3) The Powerball function can help relieve the gradient
vanishing phenomenon.
1Architectures in generalization and convergence experiments can be found at the following
links: (1) ResNet-50 and DenseNet-121 on CIFAR-10: https://github.com/kuangliu/
pytorch-cifar; (2) ResNext-29 and WideResNet on CIFAR-100: https://github.com/
junyuseu/pytorch-cifar-models; (3) ResNet50 on ImageNet: https://github.com/
pytorch/examples/tree/master/imagenet
5
Under review as a conference paper at ICLR 2020
Experiments	Datasets	Architecture
Convergence and Generalization Experiments	CIFAR-10 Krizhevsky & Hinton (2009)	ResNet-50 He et al. (2016) DenseNet-121 Huang et al. (2017)
	CIFAR-100 Krizhevsky & Hinton (2009)	ResNeXt-29 (16x64d) Xie et al. (2017) WideResNet (depth=26, k=10) Zagoruyko & Komodakis (2016)
	ImageNet Russakovsky et al. (2015)	ResNet-50 He et al. (2016)
Powerball Feature Experiments	MNISTLeCunetaL(1998)	13-Layer Fully-Connected Neural Network
Table 1: Summaries of the models and datasets in our experiments.
4.1	Hyper-parameter tuning
Since the initial learning rate has a large impact on the performances of optimizers, we implement a
logarithmically-spaced grid search strategy around the default learning rate for each optimization
method, and leave the other hyper-parameters to their default settings.
SGDM: The default learning rate for SGDM is 0.01. We tune the learning rate on a logarithmic scale
from {1, 0.1, 0.01, 0.001, 0.0001}. The momentum value in all experiments is set to default value 0.9.
PoweredSGD, PoweredSGDM: The learning rates for PoweredSGD and PoweredSGDM are chosen
from the same range {1, 0.1, 0.01, 0.001, 0.0001} as SGDM. The momentum value for PoweredS-
GDM is also 0.9. Note that γ = 1 in Powerball function corresponds to the SGD or SGDM. Based on
extensive experiments, we empirically tune γ from {0.5, 0.6, 0.7, 0.8, 0.9}.
AdaGrad: The learning rates for AdaGrad are {1e-1, 5e-2, 1e-2, 5e-3, 1e-3} and we choose 0 for the
initial accumulator value.
RMSprop, Adam: Both have the default learning rate 1e-3 and their learning rates are searched from
{1e-2, 5e-3, 1e-3, 5e-4, 1e-4}. The parameters β1, β2 and the perturbation value ε are set to default.
As previous findings Wilson et al. (2017) show, adaptive methods generalize worse than non-adaptive
methods and carefully tuning the initial learning rate yields significant improvements for them. To
better compare with adaptive methods, once we have found the value that was best performing
in adaptive methods, we would try the learning rate between the best learning rate and its closest
neighbor. For example, if we tried learning rates {1e-2, 5e-3, 1e-3, 5e-4, 1e-4} and 1e-4 was best
performing, we would try the learning rate 2e-4 to see if performance was improved. We iteratively
update the learning rate until performance could not be improved any more. For all experiments, we
used a mini-batch size of 128.
4.2	Experiments: convergence and generalization
Fig. 1 shows the learning curves of three experiments we have conducted to observe the performance
of PoweredSGD and PoweredSGDM in comparison with other widely-used optimization methods.
ResNet-50 on CIFAR-10: We trained a ResNet-50 model on CIFAR-10 and our results are shown
in Fig. 1(a) and Fig. 1(b). We ran each experiment for a fixed budget of 160 epochs and reduced the
learning rate by a factor of 10 after every 60 epochs Wilson et al. (2017).
As the figure shows, the adaptive methods converged fast and appeared to be performing better than
the non-adaptive method SGDM as expected. For PoweredSGD and PoweredSGDM, we observed
the same tendency in convergence rate as AdaGrad, which outperformed Adam and RMSprop. For
the test performance, adaptive methods and our proposed methods still outperform SGDM in the
early stage. SGDM achieved a final best overall test accuracy of 94.75%. The PoweredSGD and
PoweredSGDM achieved test accuracies of 94.17% and 94.13% respectively, which are slightly
worse than SGDM. The best adaptive method, Adam, achieved a test accuracy of 93.38%.
WideResNet on CIFAR-100: Next, we conducted experiments on the CIFAR-100 dataset using
WideResNet model. The fixed budget here is 120 epochs and the learning rate reduces by a factor of
10 after every 60 epochs. The results are shown in Fig. 1(e) and Fig. 1(f).
The performance of the PoweredSGD and PoweredSGDM are still promising in both the train set and
test set. PoweredSGD, PoweredSGDM and AdaGrad had the fastest initial progress. In the test set,
PoweredSGD and PoweredSGDM had much better test accuracy than all other adaptive methods.
6
Under review as a conference paper at ICLR 2020
% Aoe-n。。V U_e-l
% A0e∙ln8< u-e-J_
。一。VU一四一
Epoch
(a) ReSNet-50 CIFAR10 (Train)
Epoch
(b) ResNet-50 CIFAR10 (Test)
Epoch
(C) DenseNet-121 CIFAR10 (Train)
求 AO0noo< U-B二
Epoch
(e) WideResNet CIFAR100 (Train)
Epoch
(g) ResNeXt CIFAR100 (Train)
Epoch
(i) ResNet-50 ImageNet (Train)
Figure 1: Train and test accuracy for different models and datasets. The annotations indicate the
best overall test accuracy for each optimization method. The γ values in five experiments are
0.8,0.8,0.8,0.7,0.8 for PoweredSGD and 0.8, 0.8, 0.8, 0.8, 0.7 for PoweredSGDM, respectively.
95
求90
85
80
75
150
50	100
Epoch
70
0
(d) DenseNet-121 CIFAR10 (Test)
Epoch
(f) WideResNet CIFAR100 (Test)
Epoch
(h) ResNeXt CIFAR100 (Test)
Epoch
(j) ResNet-50 ImageNet (Test)
7
Under review as a conference paper at ICLR 2020
SGDM surpassed PoweredSGD and PoweredSGDM by epoch 60 when the learning rate decayed.
SGDM had the best test accuracy of 76.78%, while PoweredSGD and PoweredSGDM achieved
accuracies of 76.29% and 76.10%, respectively, with approximately 0.5% gap in test performance
compared with SGDM. The best adaptive methods, still Adam, achieved test accuracy of 74.35% and
had much larger gap of 2.5% in performance compared to SGDM.
ResNet-50 on ImageNet: Finally, we conducted experiments on the ImageNet dataset using ResNet-
50 model. The fixed budget here is 120 epochs and the learning rate reduces by a factor of 10 after
every 30 epochs. The results are shown in Fig. 1(i) and Fig. 1(j). We observed that PoweredSGD and
PoweredSGDM gave better convergence rates than adaptive methods while AdaGrad quickly plateaus
due to too many parameter updates. For test set, we can notice that although SGDM achieved the best
test accuracy of 76.27%, PoweredSGD and PoweredSGDM gave the results of 73.71% and 73.96%,
which were better than those of adaptive methods.
Additional experiments (DenseNet-121 on CIFAR-10 and ResNeXt on CIFAR100) are shown
in Fig. 1(c)(d)(g)(h). We observed similar results as in the other experiments.
4.3 Experiments: Features of PoweredSGD
Ooooqqooq
098765432
% AOeJnOOa U_e」l
10
0	5	10	15
Epoch
(a) Train Accuracy of SGD and PoWerSGD
々 y-s
,l3Ae-S」U- S3-pe∙lJO E」0N，L
0	5	10	15	20	25
Epoch
(b) The I-Norm of Gradients of SGD and PowerSGD in the First Layer
Figure 2: (a) Train accuracy comparison between SGD (learning rate = 0.1) and PoweredSGD
(learning rate = 0.1, γ = 0.4) on the 13-layer fully-connected neural network. The arrows annotate
the accuracy values of both methods after 20 epochs. (b) The 1-norm of stochastic gradients of SGD
and PoweredSGD in the first layer of the fully-connected neural network at every epoch.
Gradient Vanishing: In deep learning, the phenomenon of gradient vanishing poses difficulties in
training very deep neural networks by SGD. During the training process, the stochastic gradients
in early layers can be extremely small due to the chain rule, and this can even completely stop the
networks from being trained. Our proposed PoweredSGD method can relieve the phenomenon of
gradient vanishing by effectively rescaling the stochastic gradient vectors.
To validate this, we conduct experiments on the MNIST dataset by using a 13-layer fully-connected
neural network with ReLU activation functions. The SGD and proposed PoweredSGD are compared
in terms of train accuracy and 1-norm of gradient vector. As can be observed in Fig. 2, SGD
completely cannot train such a deep network and its train accuracy remained below 11.24%. By
contrast, the train accuracy of PoweredSGD grows quickly to 98.91% after few epochs. We further
compare the 1-norm of stochastic gradient vector in the first layer for both SGD and PoweredSGD. It
is clear that the Powerball function amplifies the stochastic gradients and helps relieve the gradient
vanishing phenomenon. We have included more experimental results on gradient vanishing in the
Supplementary Material C.
5 Conclusion
In this paper, a Powerball function is introduced as a basic technique to improve SGD and SGDM
for deep neural network training. Their convergence rates have been proved in the non-convex opti-
mization settings. We discussed the choice of an important hyper-parameter γ in the Supplementary
Material E from an empirical point of view. The experiments on different neural network models and
benchmark datasets have demonstrated that the proposed PoweredSGD and PoweredSGDM empiri-
cally obtain faster training speed than adaptive gradient methods and good if not better generalization
ability compared with SGD. We also show that PoweredSGD can help alleviate gradient vanishing.
8
Under review as a conference paper at ICLR 2020
References
Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V Le. Neural optimizer search with reinforce-
ment learning. In Proceedings of the 34th International Conference on Machine Learning-Volume
70,pp. 459-468. JMLR. org, 2017.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar. signSGD:
Compressed optimisation for non-convex problems. arXiv preprint arXiv:1802.04434, 2018.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of Adam-type
algorithms for non-convex optimization. arXiv preprint arXiv:1808.02941, 2018.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Cong Fang, Zhouchen Lin, and Tong Zhang. Sharp analysis for nonconvex SGD escaping from
saddle points. arXiv preprint arXiv:1902.00247, 2019.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic
gradient for tensor decomposition. In Conference on Learning Theory, pp. 797-842, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. Stochastic gradient
descent escapes saddle points efficiently. arXiv preprint arXiv:1902.04811, 2019.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from
Adam to SGD. arXiv preprint arXiv:1712.07628, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Proceedings of the
3rd International Conference on Learning Representations, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016.
Liangchen Luo, Wenhao Huang, Qi Zeng, Zaiqing Nie, and Xu Sun. Learning personalized end-to-end
goal-oriented dialog. arXiv preprint arXiv:1811.04604, 2018.
Eric Mazumdar and Lillian J. Ratliff. On the convergence of gradient-based learning in continuous
games. arXiv preprint arXiv:1804.05464, 2018.
Yurii E Nesterov. A method for solving the convex programming problem with convergence rate
o(1/k2)). In Dokl. akad. nauk Sssr, volume 269, pp. 543-547, 1983.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR
Computational Mathematics and Mathematical Physics, 4(5):1-17, 1964.
Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1):
145-151, 1999.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. In
International Conference on Learning Representations, 2018.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical
Statistics, 22(3):400-407, 1951.
9
Under review as a conference paper at ICLR 2020
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International journal of computer vision, 115(3):211-252, 2015.
Leslie N. Smith. No more pesky learning rate guessing games. CoRR, abs/1506.01186, 2015. URL
http://arxiv.org/abs/1506.01186.
Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using
large learning rates. In Artificial Intelligence and Machine Learning for Multi-Domain Operations
Applications, volume 11006, pp. 1100612. International Society for Optics and Photonics, 2019.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSProp: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31,
2012.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4148-4158, 2017.
Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on
Computer Vision (ECCV), pp. 3-19, 2018.
Saining Xie, Ross Girshick, Piotr Doll念 Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 1492-1500, 2017.
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi Yang. A unified analysis of stochastic momentum
methods for deep learning. arXiv preprint arXiv:1808.10396, 2018.
Ye Yuan, Mu Li, Jun Liu, and Claire J Tomlin. On the Powerball method for optimization. arXiv
preprint arXiv:1603.07421, 2016.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016.
10
Under review as a conference paper at ICLR 2020
Supplementary Material
A Convergence analysis of Powered S GD and PoweredSGDM
A.1 Convergence of PoweredGD
The authors of Yuan et al. (2016) proposed the so-called Powerball accelerated gradient descent
algorithm, which was updated as follows,
Xt+1 = Xt- ασ(Vf (Xt)).	(6)
The authors of Yuan et al. (2016) then provided insights from finite convergence properties of ODEs
models for Powerball variants of gradient descent algorithms and presented empirical observations of
convergence acceleration of Powerball methods. Nonetheless, they did not provide any convergence
results for their algorithms. In fact, they highlighted the convergence analysis of their algorithms as
open theoretical questions. In this paper, we are going to analyze stochastic variants of Powerball
methods in the non-convex setting. We start with the analysis of PoweredGD (powered gradient
descent) in (6).
Theorem A.1 Suppose that Assumption 3.1 holds. The PoweredGD scheme (6) can lead to
T ∑ kVf (Xt))k2+γ ≤ 2LkT1kp(f (X1)- f?),
Tt=1	T
where T is the numberof iterations and p = ∣+Y for any Y ∈ [0,1) and p = ∞ for Y = L
Proof: Denote by X? the minimizer and f? = f(X?). Then, by the L-Lipschitz continuity of Vf and
(6),
L
f (Xt +1)	≤ f (Xt)+ V f (Xt) ∙ (Xt +ι - Xt) + 2 k Xt+ι - Xt k2
=f (Xt)- αVf (Xt) ∙σ(Vf(Xt)) + Lα2kσ(Vf (Xt))k2.	⑺
Let Ot = %(：(Vf(Vf)X少 > 0 (this holds if Xt = X?). Then
((γ 1) ≤ f(γ ) — (Vf(Xt>σ(Vf(Xt)))2	(8)
f(Xt+1) ≤ f(Xt)	2Lkσ(Vf (Xt))k2 ∙	(8)
By Holder’s inequality, for Y ∈ (0,1) and with P = 挣 and q = 1+γ, We have
n	nn
kσ(Vf (Xt))k2 = ∑ |(Vf )i(Xt产 ≤ (∑ 1P)p(∑(|(Vf)i(Xt)∣2γ)q)q
i=1	i=1	i=1
	n	2Y ≤	kikP(∑I(Vf)i(Xt)∣1+γ) 1+γ. i=1
It follows that	f(x I)	≤	f(X)- (Vf(xt)∙σ(VfXt)))2 f( t+1)	≤	f( t)	2Lkσ(Vf(Xt))k2 ≤	f (Xt) --(∑n=11(V f M (X )11+Y )2	2Y 2 L k1k P (∑ n=1 |(V f )i( Xt )∣1+γ)中 =f (Xt) - 2L1ιr (∑ l(V f )i( Xt )∣1+γ)系 2Lk1k P i=1 1 =f (Xt) - MIIIIl kV f (xt )k1+γ, 2Lk1k P
which, by a telescoping sum, gives
T ∑ kVf(Xt)k1+γ ≤ 7LTkp-(f (X1) - f(XT+1)) ≤ 7LTkp-(f (Xi) -f?),
where 1 is vector with entries all given by 1. It is easy to see that the estimate is also valid for Y= 1
with p = ∞ and for Y = 0. The proof is complete.	■
11
Under review as a conference paper at ICLR 2020
A.2 Convergence of PoweredSGD (Proof of Theorem 3.1)
To analyze the convergence of PoweredSGD, we need some preliminary on the relation between
mini-batch size and variance reduction of SGD.
Variance reduction by a larger mini-batch size
Let V f (X) be the gradient of f at X ∈ Rn. Suppose that We use the average of m calls to the stochastic
gradient oracle, denoted by g(X, ξi) (i = 1,…,m), to estimate Vf (X). By Assumption 3.2, we have
E ]∣∣ ∑m=λm^- V f (X)][=m E I ∑[g(X，ξi)- V f (X)]][
ι 「1 m	J ι ι C	σ2
=m E ]m∑k g (X, ξi)- V f (X )k J = mm (mσ ) = m `
where in the second equality we used the fact that g(x, ξi) (i = 1,…，m) are drawn independently and
all give unbiased estimate of Vf(X) (provided by Assumption 3.2).
Now we are ready to present the proof of Theorem 3.1.
Proof of Theorem 3.1
Proof: By the L-Lipschitz continuity of Vf and (6),
L
f (χt+1)	≤	f (χt)+ Vf (χt) ∙ (Xt+1 -χt)+ 2Ilχt+1 -χtIl
=f (Xt)- α V f (Xt) ∙σ (gt) + L α2 l∣σ (gt )k2
=	f (Xt)	— α gt	∙σ (gt) + L α2kσ (gt )k2 +	αt(gt- V f (Xt)) ∙σ (gt).
Let a =箫繇 > 0. Then
f (χt +1)	≤ f (Xt) - 2L ⅞⅞gk22 + αt(gt - Vf (Xt)) ∙ σ(gt).	(9)
Fix any iteration number T > 1 and let ε ∈ (0, 1) to be chosen. We can estimate
αt(gt — Vf (χt)) ∙σ(gt)	=	⅛σ⅛ (gt -V f (χt)) ∙σ (gt)
≤	⅛σf⅛ i gt- V f ⑷口归(gt)n ⅛σ≡ i gt- V f (χt )n
≤	2L (ε ⅛S⅛l+1 i gt - V f(χt)"2
where the last inequality followed from the elementary inequality 2ab ≤ εa2 + ɪ b2 for any positive
real number ε and real numbers a, b. Substituting this into (9) gives
f (Xt+1) ≤ f (Xt) -聂 ⅛σσ⅛)22 + 2Lε kgt - Vf (Xt)k2.	(10)
By the same argument in the proof for Theorem A.1, we can derive
f (χt+1)	≤ f (χt) - 2 L-ε P Il gt k2+γ + 2Lε Il gt - V f (χt )k2.
Taking conditional expectation from both sizes gives
E[f (Xt+1) -f (χt) | χt ]	≤ - 2 L-i P E[k gt Il2+γ ] + 2Lεσ2
≤ - 2L-εPE[Igt i2+Y] + LT,
12
Under review as a conference paper at ICLR 2020
where σt2 is the variance of the t-th stochastic gradient approximation computed using the chosen
2	σ2
mini-batch Size Bt = T, which therefore satisfies σ/ ≤ T. Taking expectation from both sides and
performing a telescoping sum give
E " 1 Σ kgtk2+γ]	≤ 拜⅛(f (X 1) - f?) + T⅛%σ2.
T t=1
The proof is complete.	■
B Convergence Analysis of PoweredS GDM
B.1 Convergence of PoweredGDM
We first analyze the deterministic version of PoweredSGDM (denoted by PoweredGDM). The update
rule for PoweredGDM is
V¾+1 = β Vt- α σ (V f (Xt)),
xt+1 = xt +vt+1 ,
(11)
where β ∈ [0, 1) is a momentum constant and v0 = 0. Clearly, when β = 0, the scheme also reduces
to PoweredGD.
Theorem B.1 Suppose that Assumption 3.1 holds. For any β ∈ [0, 1), the PoweredGDM scheme (11)
with an adaptive learning rate can lead to
T ∑ kVf (Xt)k2+γ ≤ 2LTkp 1+β(f (x 1) - f?),
T t=1	T 1 - β
where T is the numberof iterations and p = ∣+Y for any Y ∈ [0,1) and p = ∞ for Y = 1.
Proof: Let Zt = Xt + I-P Vt. It can be verified that the PoWeredGDM scheme satisfies
卜+1 = Zt - i¾σ(Vf (Xt)),	(12)
1 Vt+1 = βVt - Otσ(Vf (Xt)).
By the L-Lipschitz continuity of Vf and (12),
L
f (Zt+1)	≤	f (Zt) + V f (Zt) ∙(zt+1 - zt)+ 2 Il zt+1 - ZtIl
=f (Zt) - 1-βVf (Zt) ∙ σ(Vf (Xt)) + L ɑɪkσ(Vf (Xt))∣2
=f (zt) - 1-⅛Vf (xt) ∙ σ(Vf (xt)) - 1-p(Vf (zt) - Vf (xt)) ∙ σ(Vf (Xt))
+L (T-βy kσ (V f (xt ))k2.	(13)
We can estimate
-τ¾ (V f (zt) - V f (xt ))∙ σ (V f (xt)) ≤ —ɪɪ [ε ∣V f (Zt) - V f (Xt )∣2 +1 αt2∣σ (V f (Xt ))∣21 , (14)
1 - β	2(1 - β )	ε
where ε > 0 is to be chosen. By the L-Lipschitz continuity of Vf,
|V f (Zt) - V f (Xt )k2 ≤ L 2k Zt - Xt k2 = L 2k ɪ Vt k2 = L2 -βɪ-2 k Vtk2.	(15)
1 -β	(1-β)2
Lemma B.1 For T ≥ 1, we haVe
T	1T
∑ k vt k2 ≤ 1_(>2 ∑ α2kσ (V f(xt ))k2.
t=1	(1 - β) t=1
13
Under review as a conference paper at ICLR 2020
Proof: It is easy to show by induction that, for t ≥ 1,
t-1
Vt = - ∑ βITaiσ(Vf (xi)).
i=1
Indeed, we have v1 = 0 and v2 = -α1σ(Vf(x1)). Suppose that the above holds for t ≥ 1. Then
vt+1 =βvt-βαtσ(Vf(xt)) =β -∑βt-i-1αiσ(Vf(xi)) -βαtVf(xt) = -∑β(t+1)-i-1αiσ(Vf(xi)).
i=1	i=1
Hence
T	T	t-1
∑kvtk2 = ∑k-∑βt-i-1αiσ(Vf(xi))k2
≤ ∑T t∑-1βt-i-1kαiσ(Vf(xi))k!2
t=1 i=1
T t-1	t-1
≤ ∑ ∑βt-i-1 ∑βt-i-1kαiσ(Vf(xi))k2
t=1 i=1	i=1
1 T t-1
≤』∑∑ 01」@。(V f(xi))k2
1 -β t=1 i=1
1T	T
= E∑kα∙σ(Vf(Xi))k2 ∑ βt--1
1 -β i=1	t=i+1
1T
≤ (1-§ )2 ∑ α2kσ (V f (Xt)) IE
By Lemma B.1, inequalities (14), (15), and a telescoping sum on (13), we get
f? - f (Z 1) ≤ -~j—0
1-β
T
∑ atV f (xt) ∙σ (V f (Xt))
t=1
εL2β2	1	L T
+ |_2(1 -β)5 + 2ε(1 -β) + 2(1-β)2 J ∑ aa kσ(Vf (xt))k .
It is clear that ε =
of ε > 0) and give
would minimize the bound on the right-hand side (among different choices
1 T	Lβ	L T
f? - f(z 1) ≤ - 1-β ∑atVf(xt) ∙σ(Vf(Xt))+ [(1 -Pe尸 + 2(1 -§产]∑ a2kσ(Vf(Xt))k2.
For any β ∈ [0,1), we Can choose at = % (%，)；黑)) ；2))(1+)2 So that the bound reduces to
1§T
f - f (Z 1) ≤ - 2 L (1 + β) Σ
(V fXt )∙ σ (V f (Xt)))2
kσ (V f (Xt ))k2
which immediately gives the bound in the theorem by noting Z1 = X1 .
B.2 Convergence of PoweredSGDM (Proof of Theorem 3.2)
Proof: The proof is built on that for Theorem B.1. With Zt = Xt + 1§§Vt, it can be verified that the
PoweredSGDM scheme satisfies
at
zt +1 = zt — 1-β σ(gt),
Vt+1 = §Vt - atσ(gt).
(16)
14
Under review as a conference paper at ICLR 2020
By the L-LiPschitz continuity of Vf and (4),
L
f (zt +1)	≤	f (Zt)+ Vf (Zt) ∙ (zt+1 -zt)+ 2Ilzt+1 -Ztk
=	f (zt) — 1-tβVf (zt) ∙ σ(gt) + L(1-tβ)2 kσ(gt)k2
=	f (zt) — 1-tβgt ∙σ(gt) — 1αβ(vf (zt) — Vf (Xt))∙σ(gt)
+1 - β (gt- V f (χt)) ∙σ (gt)+ 2 (1 -β )2 kσ (gt )k2.	(17)
Similar to the Proof of Theorem B.1, we can estimate
-T-β (V f (zt) - V f (Xt)) ∙ σ (gt) ≤ 2(⅛ [ε1∣∣V f (zt) - V f (Xt )k2 + -t2∣σ (&)『], (18)
where ε1 > 0 is to be chosen. By the L-LiPschitz continuity of Vf,
|V f (zt) - V f (Xt )k2 ≤ L 2k zt - Xt k2 = L 2k ɪ Vt k2 = L2 7rβ‰ k vtk2.	(19)
1 -β	(1-β)2
Similar to Lemma B.1, we obtain
T	1T
∑ kVtk2 ≤ (1-β)2 ∑ -2kσ(gt)k2.	(20)
We can also bound
-j-α⅛(gt - Vf (Xt)) ∙σ(gt) ≤ T J、kgt - Vf (Xt)k2 + Lε 1 + β3α2kσ(gt)k2 ,	(21)
1 - β	2 Lε (1 + β )	(1 - β )3
where ε > 0. By inequalities (18)-(21), and a telescoPing sum on (17), we get
f? - f (z 1) ≤ -π⅛ ∑T=I -tgt ∙ σ(gt) + T hεLβ5 + ε1(11-β) + (i-L3)2 + ζ(l-+β3)i ∑t=1 -2kσ(gt)k2
+ 2Li(1+β) ∑T=Ikgt - Vf (Xt)k2.
Setting ε1 = (1-/ and choosing Ot = Lgt/g* (¾+* lead to
f?-f(z 1) ≤-⅛β⅛)∑T=1 (gσg)F + 2L⅛⅛)∑T=1 kg-Vf(Xt)k2.
which, by taking exPectation from both sides and by the same argument in the Proof for Theorem
A.1, leads to
f?_ f ( ) ≤	(1 - β )(1 - ε) E T k k2	+ σ2(1 - β)
f - f (z1) ≤ - 2L k1k P (1 + β) E [t∑ kgt k1+γ] + 2L ε (1 + β),
which immediately gives the bound in the theorem by noting z 1 = X1.	■
Remark B.1 Clearly, Theorem 3.2 eXactly reduces to Theorem B.1 when σ = 0 and ε → 0. Moreover,
when β = 0, Theorem 3.2 reduces eXactly to Theorem 3.1. This in a sense shows that our estimates
are sharp.
B.3 Estimates of true gradients vs estimates of stochastic gradients
A careful reader will notice that in Theorems 3.1 and 3.2, our estimates of convergence rates for
PoweredSGD and PoweredSGDM, resPectively, are in terms of the stochastic gradients gt . We now
show that this is without loss of generality in view of AssumPtion 3.2.
When γ = 1, we have
E[kgtk12+γ] =E[kgtk22] =E[kgt-Vf(Xt)+Vf(Xt)k22] =E[kgt-Vf(Xt)k22]+E[kVf(Xt)k22],
15
Under review as a conference paper at ICLR 2020
where in the last equality, we used Assumption 3.2. This would imply
E[kVf (xt)k2] ≤ E[kgtk2+γ] -E[kgt- Vf (xt)k2] ≤ E[kgtk2+γ].
When γ ∈ [0, 1), by the equivalence of norm in Rn, there exist positive constants Cγ and Dγ such that
Cγ kxk22 ≤ kxk12+γ ≤ Dγ kxk22 ,
for all x ∈ Rn. Hence
E[kgtk12+γ] ≥CγE[kgtk22] =CγE[kgt-Vf(xt)+Vf(xt)k22]
=CγE[kgt-Vf(xt)k22]+CγE[kVf(xt)k22]
≥ Cγ E[k gt - V f (Xt )k2] + 弃 E[kV f (Xt )k2+γ ],
Dγ
which implies that
E[kVf(Xt)k2+γ] ≤ DE[kgtk2+γ].
Cγ
In other words, the estimates are equivalent (modulo a constant factor). We prefer the versions in
Theorems 3.1 and 3.2, because the bounds are more elegant.
C Powered S GD helps alleviate the vanishing gradient problem
The vanishing gradient problem is quite common when training deep neural networks using gradient-
based methods and backpropagation. The gradients can become too small for updating the weight
values. Eventually, this may stop the networks from further training. The Powerball function can help
amplify the gradients especially when they approach zero. We visualized the amplification effects of
Powerball function in Fig. 3. Thus, the attributes of PoweredSGD can help alleviate the vanishing
gradient problem to some extent. We investigated the actual performance of PoweredSGD and SGD
when dealing with very deep networks.
We trained deep networks on the MNIST dataset using PoweredSGD with γ chosen from
{0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0} and learning rate η chosen from {1.0, 0.1, 0.01, 0.001}.
When γ = 1.0, the PoweredSGD becomes the vanilla SGD. The architecture of network depth which
ranges from 12 to 15 with ReLU as the activation function is shown in Table 2. The results are
visualized using heatmaps in Fig. 4.
	Hidden neurons
Input layer	784 → 256
Hidde 山yers (× 10/11/ 12∕Γ3T	256 → 256
Output layer	256 → 10
Table 2: The architecture of MLP in vanishing gradient experiments.
As we can observe in the visualisation, when the network depth is more than 13 layers, increasing or
decreasing the learning rate of SGD could not solve the vanishing gradient problem. For PoweredSGD,
the usage of the Powerball function enables it to amplify the gradients and thus allows to further train
deep networks with proper γ settings. This confirms our hypothesis that PoweredSGD helps alleviate
the vanishing gradient problem to some extent. We also note that, when the network increases to 15
layers, both SGD and PoweredSGD could not train the network further. We speculate that this is due
to the ratio of amplified gradients to the original gradients becomes too large (see Fig. 3) and a much
smaller learning rate is needed (this is also consistent with the change of theoretical learning rates
suggested in the convergence proofs as the gradient size decreases). Since PoweredSGD is essentially
a gradient modifier, it would also be interesting to see how to combine it with other techniques for
dealing with the vanishing gradient problem. Since PoweredSGD also reduces the gradient when the
gradient size is large, it may also help alleviate the exploding gradient problem. This gives another
interesting direction for future research.
16
Under review as a conference paper at ICLR 2020
O^u
6 4 2
WWW
S-(6) Lb
100
10
-8
10-6	10-4	10-2
100
9t
Figure 3:	The amplification effects of Powerball function with different γ and different gradient sizes.
The amplification ratio is larger when the gradient size is closer to 0.
14-Layer MLP
0.001
V= 1.0
V= 0.9
V= 0.8
V= 0.7
V= 0.6
V=0.5
y=0.4
V= 0.3
Y≈0.2
γ=0.1
Y=O
13-LayerMLP
rj = 1.0
rj=O∙l rj = 0.01 fj= 0.001
Y= 1.0
V= 0.9
V= 0.8
Y= 0.7
V= 0.6
V= 0.5
Y= 0.4
7=0.3
V= 0.2
V=O-I
Y=O
rj=LO
rj = 0.1
rj=0.01
rj = 0.001
15-LayerMLP
V	= 1.0
V	= 0.9
V	= 0.8
V	= 0.7
V= 0.6
V=0.5
y=0.4
V= 0.3
Y≈0.2
γ=0.1
Y=O
η = 1.0 η=O∙l η = 0.01 η= 0.001
Figure 4:	We trained deep networks using PoweredSGD with different hyper-parameter settings.
A dark block indicates that PoweredSGD with a specific setting succeeded in training the network
within 25 epochs, while a light block indicates that PoweredSGD (or SGD) failed to train. It is clear
that by tuning the hyper-parameter γ, PoweredSGD can train networks deeper than that by SGD.
17
Under review as a conference paper at ICLR 2020
D Improved robustness to hyper-parameter selection
The Powerball function is a nonlinear function with a tunable hyper-parameter γ applied to gradients,
which is introduced to accelerate optimization. To test the robustness of different γ, we trained ResNet-
50 and DenseNet-121 on the CIFAR-10 dataset with PoweredSGD and SGDM. The parameter γ
is chosen from {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0} and the learning rate is chosen from
{1.0, 0.1, 0.01, 0.001}. The PoweredSGD becomes the vanilla SGD when γ = 1. The maximum test
accuracy is recorded and the results are visualized in Fig. 5.
Although the γ that gets the best test accuracy depends on the choice of learning rates, we can
observe that γ can be selected within a wide range from 0.5 to 1.0 without much loss in test accuracy.
Moreover, the Powerball function with a hyper-parameter γ could help regularize the test performance
while the learning rate decreases. For example, when η = 0.001 and γ = 0.6, PoweredSGD get the
best test accuracy of 90.06% compared with 79.87% accuracy of SGD.
We also compare the convergence performance of different γ choice in Fig. 6. The training loss is
recorded when training ResNet-50 on CIFAR-10 dataset. As the initial learning rate decreases, the
range from which the hyper-parameter γ can be selected to accelerate training becomes wider. As a
practical guide, γ = 0.8 seems a proper setting in most cases. It is again observed that the choice of γ
in the range of 0.4-0.8 seems to provide improved robustness to the change of learning rates.
ClFAR-IO and ResNet-50
y	=0	10.00	83.93	89.35	91.95	M-90
Y =	0.1	10.00	88.41	91.85	93.38	11
Y =	0.2	83.86	91.54	92.52	93.28	IL
Y =	0.3	89.72	92.53	92.94	93.46	ι∣
Y =	0.4	92.46	93.29	93.86	92.59	M-60
Y =	0.5	93.58	93.73	93.76	91.40	
Y =	0.6	94.12	94.07	93.51	90.06	]
Y =	0.7	94.40	94.18	93.49	87.56	
Y =	0.8	94.74	94.12	92.69	85.63	
Y =	0.9	94.58	94.11	91.97	83.46	-30
Y =	1.0	94.89	94.16	90.67	79.87	
SGDM		26.57	94.96	94.27	90.77	-15
η = 1.0 η = 0.1 η = 0.01 η = 0.001
CIFAR-IO and DenseNet-121
γ= 0 y= 0.1	10.00 10.00	87.52	90.04	93.22	I H-90
		91.29	91.86	93.81	
y=0.2	87.12	92.01	93.37	93.84	iL
y=0.3	91.58	93.24	94.20	93.65	ι∣
γ=0A	93.36	93.77	94.13	92.77	Tr 60
					
y= 0.5	94.04	94.13	93.99	91.64	I
y= 0.6	94.35	94.37	93.87	90.16	]
y=0.7	94.62	94.42	93.55	89.09	
y=0.8	95.06	94.47	93.25	86.24	
y= 0.9	94.91	94.80	92.48	84.13	-30
γ= 1.0	95.27	94.36	91.46	80.14	
SGDM	68.25	95.13	94.73	91.42	-15
	η = 1.0	η = 0.1	η = 0.01	η = 0.001	
Figure 5:	Effects of different γ on test accuracy. We show the best Top-1 accuracy on CIFAR-10
dataset of ResNet-50 and DenseNet121 trained with PoweredSGD. Although the best choice of γ
depends on learning rates, the selections can be quite robust considering the test accuracy.
E Combining PoweredSGD with learning rate schedules
In the main part of the paper, we demonstrated through multiple experiments that PoweredSGD can
achieve faster initial training. In this section we demonstrate that PoweredSGD as a gradient modifier
is orthogonal and complementary to other techniques for improved learning.
The learning rate is the most important hyper-parameter to tune for deep neural networks. Motivated
by recent advancement in designing learning rate schedules such as CLR policies (Smith, 2015) and
SGDR (Loshchilov & Hutter, 2016), we conducted some preliminary experiments on combining
learning rate schedules with PoweredSGD to improve its performance. The results are shown in Fig.
7.
The selected learning rate schedule is warm restarts introduced in (Loshchilov & Hutter, 2016),
which reset the learning rate to the initial value after a cycle of decaying the learning rate with a
18
Under review as a conference paper at ICLR 2020
(a) Train Loss of PowerSGD [η = 0.1)
2 10 12 3 4
Www-O,O-O -O
SSOq UlEa
----7 = 0
----7 = 0.2
7 = 0.4
----7 = 0.6
----7 = 0.8
7 = 1-0
(b) Train Loss of PowerSGD {η = 0.01)
101
SSoa 用⅝L
----7 = 0
----7 = 0.2
----7 = 0.4
----7 = 0.6
----7 = 0.8
----7 = 1.0
.,.L-一
P
E
150
Ch
Epo
3
Figure 6:	Effects of different γ on convergence. We show the best train loss on CIFAR-10 dataset of
ResNet-50 trained with PoweredSGD. While the γ which achieves the best convergence performance
is closely related to the choice of learning rates, a Y chosen in the range of 0.4-0.6 Seem to provide
better robustness to change of learning rates.
cosine annealing for each batch. In Fig. 7, SGD with momentum combined with warm restarts
policy is named as SGDR. Similarly, PoweredSGDR indicates PoweredSGD combined with a warm
restarts policy. The hyper-parameter setting is T0 = 10 and Tmult = 2 for warm restarts. We test their
performance on CIFAR-10 dataset with ResNet-50.
The results showed that the learning rate policy can improve both the convergence and test accuracy
of PoweredSGD. Indeed, PoweredSGDR achieved the lowest training error compared with SGDM
and SGDR. The test accuracy for PoweredSGDR was also improved from the 94.12% accuracy of
PoweredSGD to 94.64%. The results demonstrate that the nonlinear transformation of gradients
given by the Powerball function is orthogonal and complementary to existing methods. As such, its
combination with other techniques could potentially further improve the performance.
(a) Tram Loss Comparisons
20	40
120	140	160
60	80	100
Epoch
(b) Test Accuracy Comparisons
50505050
99887766
Adŋjnɔov⅛0H
Figure 7: Train loss and test accuracy for SGDR and PoweredSGDR. Learning rate schedules also
help accelerate training of PoweredSGD and improve the test performance.
19
Under review as a conference paper at ICLR 2020
F Comparisons with vanilla SGD and SGDM
In this section, we compare PoweredSGD with the vanilla SGD and SGDM to show that how a
simple Powerball function can boost the performance. The first four experiments in Section 4.2 are
conducted for comparisons. The results are shown in Figure 8 below, in which the hyper-parameters
that lead to the best test accuracy are chosen and can be found in Table 3.
——SGD
OLPo 5 O
0 9 9 8 8
% A0e-m84 Se-JJ-
50	100	150
Epoch
(a) ResNet-50 CIFAR10 (Train)
SGDM-----POWerSGD
95
0 5 0 5
9 8 8 7
% Λ0e,Jn8< 1S81
% A0e_m8< u_e:
0 5 0 5 0
0 9 9 8 8
% A0e-m84 Se-JJ-
50	100	150
Epoch
(C) DenseNet-121 CIFAR10 (Train)
OLPo 5
0 9 9 8
% A0e,m8< u-e」
20	40	60	80	100	120
Epoch
(e) WideResNet CIFAR100 (Train)
(g) ResNeXt CIFAR100 (Train)
70
0	50	100	150
Epoch
(b) ResNet-50 CIFAR10 Clest)
95
0 5 0 5
9 8 8 7
% Λ0e-Jn8< IS。1
75
δ?
∣, 70
n
υ
□
〈 65
Tn
(υ
I- 60
70
0	50	100	150
Epoch
(d) DenseNet-121 CIFAR10 (Test)
80
0	20	40	60	80	100	120
Epoch
(f) WideResNet CIFAR100 (Test)
80
C'
< 65
ω
(υ
I- 60
55
0	20	40	60	80	100	120
Epoch
(h) ResNeXt CIFAR100 (Test)
Figure 8: Train and test accuracy for SGD, SGDM and PowerSGD. The annotations indicate the best
overall test accuracy for each optimization method.
	SGD			SGDM		PowerSGD	
ResNet-50 + CIFAR10	η=	二 1.0	η	0.1	η	=0.1,γ=0.8
DenseNet-121 + CIFAR10	η=	≡W	η	0.08	η	=0.1,γ=0.8
WideResNet + CIFAR100	η=	≡W	η	0.1	η	=0.1,γ=0.8
ResNeXt + CIFAR100	η=		η	0.08	η	= 0.1, γ = 0.7
Table 3: Hyper-parameter settings for experiments shown in Figure 8.
20