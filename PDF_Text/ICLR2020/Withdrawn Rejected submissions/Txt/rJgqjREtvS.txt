Under review as a conference paper at ICLR 2020
CRNet: Image Super-Resolution Using A Con-
volutional Sparse Coding Inspired Network
Anonymous authors
Paper under double-blind review
Ab stract
Convolutional Sparse Coding (CSC) has been attracting more and more attention
in recent years, for making full use of image global correlation to improve perfor-
mance on various computer vision applications. However, very few studies focus
on solving CSC based image Super-Resolution (SR) problem. As a consequence,
there is no significant progress in this area over a period of time. In this paper, we
exploit the natural connection between CSC and Convolutional Neural Networks
(CNN) to address CSC based image SR. Specifically, Convolutional Iterative Soft
Thresholding Algorithm (CISTA) is introduced to solve CSC problem and it can
be implemented using CNN architectures. Then we develop a novel CSC based
SR framework analogy to the traditional SC based SR methods. Two models in-
spired by this framework are proposed for pre-/post-upsampling SR, respectively.
Compared with recent state-of-the-art SR methods, both of our proposed models
show superior performance in terms of both quantitative and qualitative measure-
ments.
1	Introduction
Single Image Super-Resolution (SISR), which aims to restore a visually pleasing High-Resolution
(HR) image from its Low-Resolution (LR) version, is still a challenging task within computer vision
research community (Timofte et al., 2017; 2018). Since multiple solutions exist for the mapping
from LR to HR space, SISR is highly ill-posed. To regularize the solution of SISR, various priors
of natural images have been exploited, especially the current leading learning-based methods (Wang
et al., 2015; Dong et al., 2016; Mao et al., 2016; Kim et al., 2016a;b; Tai et al., 2017a;b; Lim et al.,
2017; Ahn et al., 2018; Haris et al., 2018; Li et al., 2018; Zhang et al., 2018) are proposed to directly
learn the non-linear LR-HR mapping.
By modeling the sparse prior in natural images, the Sparse Coding (SC) based methods for SR (Yang
et al., 2008; 2010; 2014) with strong theoretical support are widely used owing to their excellent per-
formance. Considering the complexity in images, these methods divide the image into overlapping
patches and aim to jointly train two over-complete dictionaries for LR/HR patches. There are usu-
ally three steps in these methods’ framework. First, overlapping patches are extracted from input
image. Then to reconstruct the HR patch, the sparse representation of LR patch can be applied to
the HR dictionary with the assumption that LR/HR patch pair shares similar sparse representation.
The final HR image is produced by aggregating the recovered HR patches.
Recently, with the development of Deep Learning (DL), many researchers attempt to combine the
advantages ofDL and SC for image SR. Dong et al. (Dong et al., 2016) firstly proposed the seminal
CNN model for SR termed as SRCNN, which exploits a shallow convolutional neural network to
learn a nonlinear LR-HR mapping in an end-to-end manner and dramatically overshadows conven-
tional methods (Yang et al., 2010; Timofte et al., 2014). However, sparse prior is ignored to a large
extent in SRCNN for it adopts a generic architecture without considering the domain expertise. To
address this issue, Wang et al. (Wang et al., 2015) implemented a Sparse Coding based Network
(SCN) for image SR, by combining the merits of sparse coding and deep learning, which fully ex-
ploits the approximation of sparse coding learned from the LISTA (Gregor & LeCun, 2010) based
sub-network.
It’s worth to note that most of SC based methods utilize the sparse prior locally (Papyan et al.,
2017b), i.e., coping with overlapping image patches. Thus the consistency of pixels in overlapped
1
Under review as a conference paper at ICLR 2020
Figure 1: PSNRs of recent state-of-the-arts for scale factor ×4 on Set5 (Bevilacqua et al., 2012)
and Urban100 (Huang et al., 2015). Red names represent our proposed models.
patches has been ignored (Gu et al., 2015; Papyan et al., 2017b). To address this issue, CSC is
proposed to serve sparse prior as a global prior (Zeiler et al., 2010; Papyan et al., 2017b; 2018) and
it furnishes a way to fill the local-global gap by working directly on the entire image by convolution
operation. Consequently, CSC has attained much attention from researchers (Zeiler et al., 2010;
Bristow et al., 2013; Heide et al., 2015; Gu et al., 2015; Sreter & Giryes, 2018; Garcia-Cardona &
Wohlberg, 2018a). However, very few studies focus on the validation of CSC for image SR (Gu
et al., 2015), resulting in no work been reported that CSC based image SR can achieve state-of-the-
art performance. Can CSC based image SR show highly competitive results with recent state-of-the-
art methods (Dong et al., 2016; Mao et al., 2016; Kim et al., 2016a;b; Tai et al., 2017a;b; Tong et al.,
2017; Lim et al., 2017; Li et al., 2018; Zhang et al., 2018)? To answer this question, the following
issues need to be considered:
Framework Issue. Compared with SC based image SR methods (Yang et al., 2008; 2010), the lack
of a unified framework has hindered progress towards improving the performance of CSC based
image SR.
Optimization Issue. The previous CSC based image SR method (Gu et al., 2015) contains several
steps and they are optimized independently. Hundreds of iterations are required to solve the CSC
problem in each step.
Memory Issue. To solve the CSC problem, ADMM (Boyd et al., 2011) is commonly employed
(Bristow et al., 2013; Wohlberg, 2014; Heide et al., 2015; Wohlberg, 2016; Garcia-Cardona &
Wohlberg, 2018a), where the whole training set needs to be loaded in memory. As a consequence,
it is not applicable to improve the performance by enlarging the training set.
Multi-Scale Issue. Training a single model for multiple scales is difficult for the previous CSC
based image SR method (Gu et al., 2015).
Based on these considerations, in this paper, we attempt to answer the aforementioned question.
Specifically, we exploit the advantages of CSC and the powerful learning ability of deep learning
to address image SR problem. Moreover, massive theoretical foundations for CSC (Papyan et al.,
2017b; 2018; Garcia-Cardona & Wohlberg, 2018b) make our proposed architectures interpretable
and also enable to theoretically analyze our SR performance. In the rest of this paper, we first in-
troduce CISTA, which can be naturally implemented using CNN architectures for solving the CSC
problem. Then we develop a framework for CSC based image SR, which can address the Frame-
work Issue. Subsequently, CRNet-A (CSC and Residual learning based Network) and CRNet-B in-
spired by this framework are proposed for image SR. They are classified as pre- and post-upsampling
models (Wang et al., 2019) respectively, as the former takes Interpolated LR (ILR) images as input
while the latter processes LR images directly. By adopting CNN architectures, Optimization Issue
and Memory Issue would be mitigated to some extent. For Multi-Scale Issue, with the help of the
recently introduced scale augmentation (Kim et al., 2016a;b) or scale-specific multi-path learning
(Lim et al., 2017; Wang et al., 2019) strategies, both of our models are capable of handling multi-
2
Under review as a conference paper at ICLR 2020
scale SR problem effectively, and achieve favorable performance against state-of-the-arts, as shown
in Fig. 1. The main contributions of this paper include:
•	We introduce CISTA, which can be naturally implemented using CNN architectures for
solving the CSC problem.
•	A novel framework for CSC based image SR is developed. Two models, CRNet-A and
CRNet-B, inspired by this framework are proposed for image SR.
•	The differences between our proposed models and several SR models with recursive learn-
ing strategy, e.g., DRRN (Tai et al., 2017a), SCN (Wang et al., 2015), DRCN (Kim et al.,
2016b), are discussed.
2	Related Work
Sparse coding has been widely used in a variety of applications (Zhang et al., 2015). As for SISR,
Yang et al. (Yang et al., 2008) proposed a representative Sparse coding based Super-Resolution
(ScSR) method. In the training stage, ScSR attempts to learn the LR/HR overcomplete dictionary
pair Dl/Dh jointly by given a group of LR/HR training patch pairs xl/xh. In the test stage, the
HR patch xh is reconstructed from its LR version xl by assuming they share the same sparse code.
Specifically, the optimal sparse code is obainted by minimizing the following sparsity-inducing `1-
norm regularized objective function
z* = arg min ∣∣xι - Dιz∣∣2 + λ∣∣zkι,	(1)
z
then the HR patch is obtained by xh = Dhz*. Finally, the HR image can be estimated by ag-
gregating all the reconstructed HR patches. Inspired by ScSR, many SC based methods have been
proposed with various constraints on sparse code or dictionary Yang et al. (2012); Wang et al. (2012).
Traditional SC based SR algorithms usually process images in a patch based manner to reduce
the burden of modeling and computation, resulting in the inconsistency problem (Papyan et al.,
2017b). As a special case of SC, CSC is inherently suitable (Zeiler et al., 2010) and proposed to
avoid the inconsistency problem by representing the whole image directly. Specifically, an image
y ∈ Rnr ×nc can be represented as the summation of m feature maps zi ∈ Rnr ×nc convolved with
the corresponding filters f ∈ Rs×s: y = Pm=I f 0 Zi, where 0 is the convolution operation.
Gu et al. (Gu et al., 2015) proposed the CSC-SR method and revealed the potential of CSC for image
SR. In (Gu et al., 2015), CSC-SR requires to solve the following CSC based optimization problem
in both the training and testing phase:
1
min -
f,z 2
mm
y - X1 fi 0 zi	+ λX1 ∣zi∣1 .
(2)
which is solved by alternatively optimizing the z and f subproblems (Wohlberg, 2014). The z sub-
problem is a standard CSC problem. Hundreds of iterations are required to solve the CSC problem
and the aforementioned Optimization Issue and Memory Issue cannot be completely avoided. In-
spired by the success of deep learning based sparse coding (Gregor & LeCun, 2010), we exploit the
natural connection between CSC and CNN to solve the CSC problem efficiently.
3	CISTA for solving CSC problem
CSC can be considered as a special case of conventional SC, due to the fact that convolution opera-
tion can be replaced with matrix multiplication, so the objective function of CSC can be formulated
as:
min
z
mm
y - XFizi	+ λX ∣zi∣1 .
i=1	2 i=1
(3)
y, zi are in vectorized form and Fi is a sparse convolution matrix with the following attributes:
Fi zi ≡ fi 0 zi
FiT zi ≡ flipud(fliplr(fi)) 0 zi	(4)
≡ flip(fi) 0 zi
3
Under review as a conference paper at ICLR 2020
Figure 2: The architecture of the pre-upsampling model CRNet-A. The proposed CISTA block with
K recursions is surrounded by the dashed box and its unfolded version is shown in the bottom. S is
shared across every recursion.
(二 3±
(二 3±
(二 3±

where fliplr(∙) and flipud(∙) are following the notations of Zeiler et al.(Zeiler et al., 2010),
representing that array is flipped in left/right or up/down direction.
Iterative Soft Thresholding Algorithm (ISTA) (Daubechies et al., 2004) can be utilized to solve (3),
at the kth iteration:
zk+1 = hθ
Zk + LFT (y - Fzk)
(5)
where L is the Lipschitz constant, F = [F1, F2, . . . , Fm] and Fz = Pim=1 Fizi. Using the relation
in (4) to replace the matrix multiplication with convolution operator, we can reformulate (5) as:
zk+1 = hθ
(Izk + Lflip(f)乳(y - f 乳 zk)
(6)
where f = [f1, f2, . . . , fm], flip(f) = [flip(f1), flip(f2), . . . , flip(fm)] andI is the
identity matrix. Note that identity matrix I is also a sparse convolution matrix, so according to (4),
there existing a filter n satisfies:
Iz = n 0 z,	(7)
so (6) becomes:
zk+1 =hθ(W 0y+S0zk),	(8)
where W = Lflip(f) and S = n 一 Lflip(f) 0 f. Even though (3) is for a single image with
one channel, the extension to multiple channels (for both image and filters) and multiple images is
mathematically straightforward. Thus for y ∈ Rb×c×nr ×nc representing b images of size nr × nc
with c channels, (8) is still true with W ∈ Rm×c×s×s and S ∈ Rm×m×s×s .
As for hθ, (Papyan et al., 2017a) reveals two important facts: (1) the expressiveness of the sparsity
inspired model is not affected even by restricting the coefficients to be nonnegative; (2) the ReLU
(Nair & Hinton, 2010) activation function and the soft nonnegative thresholding operator are equal,
that is:
hθ+ (α) = max(α - θ, 0) = ReLU (α - θ).	(9)
We set θ = 0 for simplicity. So the final form of (8) is:
zk+1 = ReLU(W0y+S0zk).
(10)
One can see that (10) is a convolutional form of (5), so we name it as CISTA. It provides the solution
of (3) with theoretical guarantees (Daubechies et al., 2004). Furthermore, this convolutional form
can be implemented employing CNN architectures. So W and S in (10) would be trainable.
4
Under review as a conference paper at ICLR 2020
4	Proposed Method
In this work, exploiting the basic CISTA block, we implement the SR using CNN techniques. With
more recursions used in CISTA, the network becomes deeper and tends to be bothered by the gradi-
ent vanishing/exploding problems. Residual learning (Kim et al., 2016a;b; Tai et al., 2017a) is such
a useful tool that not only mitigates these difficulties, but helps network converge faster (A.1).
4.1	CRNet-A Model for Pre-upsampling
As shown in Fig. 2, CRNet-A takes the ILR image Iy with c channels as input, and predicts the
output HR image as Ix . Two convolution layers, F0 ∈ Rn0 ×c×s×s consisting of n0 filters of spatial
size c × s × s and F1 ∈ Rn0×n0×s×s containing n0 filters of spatial size n0 × s × s are utilized for
hierarchical features extraction from ILR image:
y = ReLU (FI 乳 ReLU(F0 乳 Iy)) .	(11)
The ILR features are then fed into a CISTA block to learn the convolutional sparse codes. As stated
in (10), two convolutional layers Wl ∈ Rm0×n0×s×s and S ∈ Rm0×m0×s×s are needed:
Zk+1 = ReLU(Wl 区 y + S 0 Zk),	(12)
where z° is initialized to ReLU(Wl 0 y). The convolutional sparse codes Z are learned after
K recursions with S shared across every recursion. When the convolutional sparse codes z are
obtained, it is then passed through a convolution layer Wh ∈ Rn0 ×m0 ×s×s to recover the HR
feature maps. The last convolution layer H ∈ Rc×n0×s×s is used as HR filters:
R= H 0ReLU(Wh 0Z).	(13)
Note that we pad zeros before all convolution operations to keep all the feature maps to have the
same size, which is a common strategy used in a variety of methods (Kim et al., 2016a;b; Tai et al.,
2017a). So the residual image R has the same size as the input ILR image Iy, and the final HR
image Ix would be reconstructed by:
Ix = Iy + R.	(14)
4.2	CRNet-B Model for Post-upsampling
We extend CRNet-A to its post-upsampling version to further mine its potential. Notice that most
post-upsampling models (Ledig et al., 2017; Tong et al., 2017; Li et al., 2018; Zhang et al., 2018)
need to train and store many scale-dependent models for various scales without fully using the inter-
scale correlation, so we adopt the scale-specific multi-path learning strategy (Wang et al., 2019)
presented in MDSR (Lim et al., 2017) with minor modifications to address this issue. The complete
model is shown in A.2. The main branch is our CRNet-A module. The pre-processing modules are
used for reducing the variance from input images of different scales and only one residual unit with
3 × 3 kernels is used in each of the pre-processing module. At the end of CRNet-B, upsampling
modules are used for multi-scale reconstruction.
5	Experimental Results
5.1	Settings
Datasets and Metrics Being fair to (Kim et al., 2016a; Tai et al., 2017a; Zhang et al., 2018),
291(Yang et al., 2010; Martin et al., 2001) images are used for CRNet-A, while 800 (Timofte et al.,
2017) images for CRNet-B. During testing, Set5 (Bevilacqua et al., 2012), Set14 (Zeyde et al., 2010),
B100 (Martin et al., 2001), Urban100 (Huang et al., 2015) and Manga109 (Matsui et al., 2017) are
employed. Both PSNR and SSIM (Wang et al., 2004) on Y channel of transformed YCbCr space
are calculated for evaluation.
Parameter Settings We set n0 = 128, m0 = 256 for CRNet-A (i.e. every convolution layer
contains 128 filters while Wl and S have 256 filters), while n0 = 64, m0 = 1024 for CRNet-B. We
choose K = 25 in both of our models. We implement our models using the PyTorch (Paszke et al.,
2017) framework with NVIDIA Titan Xp. More training details and parameter study could be found
in A.4 and A.5.
5
Under review as a conference paper at ICLR 2020
Table 2: Average PSNR/SSIMs of Pre-upsampling models for scale factor ×2, ×3 and ×4 on
datasets Set5, Set14, BSD100 and Urban100. Red: the best; blue: the second best.
Dataset	Scale	Bicubic	VDSR (Kim et al., 2016a)	DRCN (Kim et al., 2016b)	DRRN (Tai et al., 2017a)	MemNet (Tai et al., 2017b)	CRNet-A (ours)
	×2	33.66/0.9299	37.53/0.9587	37.63/0.9588	37.74/0.9591	37.78/0.9597	37.79/0.9600
Set5	×3	30.39/0.8682	33.66/0.9213	33.82/0.9226	34.03/0.9244	34.09/0.9248	34.11/0.9254
	×4	28.42/0.8104	31.35/0.8838	31.53/0.8854	31.68/0.8888	31.74/0.8893	31.82/0.8907
	×2	30.24/0.8688	33.03/0.9124	33.04/0.9118	33.23/0.9136	33.28/0.9142	33.33/0.9152
Set14	×3	27.55/0.7742	29.77/0.8314	29.76/0.8311	29.96/0.8349	30.00/0.8350	29.99/0.8359
	×4	26.00/0.7027	28.01/0.7674	28.02/0.7670	28.21/0.7720	28.26/0.7723	28.29/0.7741
	×2	29.56/0.8431	31.90/0.8960	31.85/0.8942	32.05/0.8973	32.08/0.8978	32.09/0.8985
B100	×3	27.21/0.7385	28.82/0.7976	28.80/0.7963	28.95/0.8004	28.96/0.8001	28.99/0.8021
	×4	25.96/0.6675	27.29/0.7251	27.23/0.7233	27.38/0.7284	27.40/0.7281	27.44/0.7302
	×2	26.88/0.8403	30.76/0.9140	30.75/0.9133	31.23/0.9188	31.31/0.9195	31.36/0.9207
Urban100	×3	24.46/0.7349	27.14/0.8279	27.15/0.8276	27.53/0.8378	27.56/0.8376	27.64/0.8403
	×4	23.14/0.6577	25.18/0.7524	25.14/0.7510	25.44/0.7638	25.50/0.7630	25.59/0.7680
5.2	Comparison with CSC-SR
We first compare our proposed models with the existing CSC based image SR method, i.e., CSC-
SR (Gu et al., 2015). Since CSC-SR utilizes LR images as input image, it can be considered as a
post-upsampling method, thus CRNet-B is used for comparison. Tab. 1 presents that our CRNet-B
clearly outperforms CSC-SR by a large margin.
Table 1: Left Table: Average PSNR/SSIMs of CSC-SR (Gu et al., 2015) and CRNet-B for scale
factor ×2, ×3 and ×4 on Set5, Set14 and B100. Red: the best; blue: the second best. Right Table:
Ablation Study of LRL, PA and CC.
DataSet	Scale	Bicubic	CSC-SR	CRNet-B
	× 2	33.66/0.9299	36.62/0.9549	38.13 /0.9610
Set5	× 3	30.39 / 0.8682	32.65/0.9098	34.75 / 0.9296
	× 4	28.42/0.8104	30.36/0.8607	32.57/0.8991
	× 2	30.24/0.8688	32.30/0.9070	34.09/0.9219
Set14	× 3	27.55 / 0.7742	29.14/0.8208	30.58/0.8465
	× 4	26.00/0.7027	27.30 / 0.7499	28.79/0.7867
	× 2	29.56/0.8431	31.27/0.8876	32.32/0.9014
B100	× 3	27.21 / 0.7385	28.31 /0.7857	29.26/0.8091
	× 4	25.96/0.6675	26.82/0.7101	27.73 /0.7414
LRL	PA	CC	PSNR
-×-	-X-	-X-	33.53
✓	X	X	33.54
X	✓	X	33.52
X	X	✓	34.05
✓	✓	X	33.58
✓	X	✓	34.09
X	✓	✓	33.97
✓	✓	✓	34.15
5.3	Comparison with State of the Arts
We now compare the proposed models with other state-of-the-arts in recent years. We compare
CRNet-A with pre-upsampling models (i.e., VDSR (Kim et al., 2016a), DRCN (Kim et al., 2016b),
DRRN (Tai et al., 2017a), MemNet (Tai et al., 2017b)) while CRNet-B with post-upsampling ar-
chitectures (i.e., SRDenseNet (Tong et al., 2017), MSRN (Li et al., 2018), D-DBPN (Haris et al.,
2018), EDSR (Lim et al., 2017), RDN (Zhang et al., 2018)). Similar to (Lim et al., 2017; Zhang
et al., 2018), self-ensemble strategy (Lim et al., 2017) is also adopted to further improve the perfor-
mance of CRNet-B, and we denote the self-ensembled version as CRNet-B+.
Tab. 2 and Tab. 3 show the quantitative comparisons on the benchmark testing sets. Both of our
models achieve superior performance against the state-of-the-arts, which indicates the effectiveness
of our models. Qualitative results are provided in Fig. 3. Our methods tend to produce shaper edges
and more correct textures, while other images may be blurred or distorted. More visual comparisons
are available in A.6.
Fig. 4(a) and 4(b) shows the performance versus the number of parameters, our CRNet-B and
CRNet-B+ achieve better results with fewer parameters than EDSR (Lim et al., 2017) and RDN
(Zhang et al., 2018). It’s worth noting that EDSR/MDSR and RDN are far deeper than CRNet-
B (e.g., 169 vs. 36), but CRNet-B is quite wider (Wl and S have 1, 024 filters). As reported in
(Lim et al., 2017), when increasing the number of filters to a certain level, e.g., 256, the training
procedure of EDSR (for ×2) without residual scaling (Szegedy et al., 2018; Lim et al., 2017) is nu-
merically unstable, as shown in Fig. 4(c). However, CRNet-B is relieved from the residual scaling
6
Under review as a conference paper at ICLR 2020
Ground Truth	Bicubic	DRCN	DRRN	CRNet-A	EDSR	RDN	CRNet-B
(PSNR, SSIM)	(25.48,	(29.29,	(29.55,	(29.96,	(30.60,	(30.80,	(30.90,
0.8259)	0.9026)	0.9084)	0.9112)	0.9171)	0.9180)	0.9206)
Ground Truth	Bicubic	DRCN	DRRN	CRNet-A	EDSR	RDN	CRNet-B
(PSNR, SSIM)	(20.45,	(21.28,	(21.49,	(21.58,	(22.71,	(22.43,	(23.07,
	0.6556)	0.7238)	0.7391)	0.7423)	0.7905)	0.7831)	0.7994)
Figure 3: SR results of “img016” and “img059” from Urban100 with scale factor ×4. Red indicates
the best performance.
Table 3: Average PSNR/SSIMs of Post-upsampling models for scale factor ×2, ×3 and ×4 on
datasets Set5, Set14, BSD100, Urban100 and Manga109. Red: the best; blue: the second best.
Dataset	Scale	MSRN Li et al. (2018)	D-DBPN Haris et al. (2018)	EDSR (Lim et al., 2017)	RDN (Zhang et al., 2018)	CRNet-B (ours)	CRNet-B+ (ours)
	×2	38.08/0.9605	38.09/0.9600	38.11/0.9601	38.24/0.9614	38.13/0.9610	38.25/0.9614
Set5	×3	34.38/0.9262	-/-	34.65/0.9282	34.71/0.9296	34.75/0.9296	34.83/0.9303
	×4	32.07/0.8903	32.47/0.8980	32.46/0.8968	32.47/0.8990	32.57/0.8991	32.71/0.9008
	×2	33.74/0.9170	33.85/0.9190	33.92/0.9195	34.01/0.9212	34.09/0.9219	34.15/0.9227
Set14	×3	30.34/0.8395	-/-	30.52/0.8462	30.57/0.8468	30.58/0.8465	30.67/0.8481
	×4	28.60/0.7751	28.82/0.7860	28.80/0.7876	28.81/0.7871	28.79/0.7867	28.93/0.7894
	×2	32.23/0.9013	32.27/0.9000	32.32/0.9013	32.34/0.9017	32.32/0.9014	32.38/0.9020
B100	×3	29.08/0.8041	-/-	29.25/0.8093	29.26/0.8093	29.26/0.8091	29.32/0.8103
	×4	27.52/0.7273	27.72/0.7400	27.71/0.7420	27.72/0.7419	27.73/0.7414	27.80/0.7430
	×2	32.22/0.9326	32.55/0.9324	32.93/0.9351	32.89/0.9353	32.93/0.9355	33.14/0.9370
Urban100	×3	28.08/0.8554	-/-	28.80/0.8653	28.80/0.8653	28.87/0.8667	29.09/0.8697
	×4	26.04/0.7896	26.38/0.7946	26.64/0.8033	26.61/0.8028	26.69/0.8045	26.90/0.8089
	×2	38.82/0.9868	38.89/0.9775	39.10/0.9773	39.18/0.9780	39.07/0.9778	39.28/0.9784
Manga109	×3	33.44/0.9427	-/-	34.17/0.9476	34.13/0.9484	34.17/0.9481	34.52/0.9498
	×4	30.17/0.9034	30.91/0.9137	31.02/0.9148	31.00/0.9151	31.16/0.9154	31.52/0.9187
trick. The training loss of CRNet-B is depicted in Fig. 4(d), it converges fast at the begining, then
keeps decreasing and finally fluctuates at a certain range.
5.4	Ablation Study
The proposed networks combine the merits of CSC and Residual learning. Particularly, CSC prob-
lem is solved in a learnable manner, namely CISTA, where three techniques are naturally inferred,
i.e. Local Residual Learning (LRL), Pre-Activation (PA) and Consistency Constraint (CC).
Ablation studies are implemented to address the superiority of the proposed networks with the el-
ementary blocks of CISTA by combining LRL, PA and CC. It is shown in Tab. 1 that the network
simultaneously containing LRL, PA and CC gives the best performance.
7
Under review as a conference paper at ICLR 2020
32,0
31.2
31.S
31.6
.31.4
CRNet-A (30)
*SRCNNI
空
31.0
30.8
30.6
30.-
l'40
RED30 00).
500 108 158 2000 2500 380 358 4000 4500
Nunflxr of parametea (K)
(b) Post-upsampling
(d) CRNet-B
(c) EDSR
(a) Pre-upsampling
Figure 4: (a) and (b): PSNR of recent state-of-the-arts versus the number of parameters for scale
factor ×4 on Set5. The number of layers are marked in the parentheses; (c) and (d): Training loss
of EDSR without residual scaling and CRNet-B.
6 Discussions
We discuss the differences between our proposed models and several recent CNN models for SR
with recursive learning strategy, i.e., DRRN (Tai et al., 2017a), SCN (Wang et al., 2015) and DRCN
(Kim et al., 2016b). Due to the fact that CRNet-B is an extension of CRNet-A, i.e., the main part
of CRNet-B has the same structure as CRNet-A, so we use CRNet-A here for comparison. The
simplified structures of these models are shown in A.3, where the digits on the left of the recursion
line represent the number of recursions.
Difference to DRRN. The main part of DRRN (Tai et al., 2017a) is the recursive block structure,
where several residual units with BN layers are stacked. On the other hand, guided by (10), CRNet-
A contains no BN layers. Coinciding with EDSR/MDSR (Lim et al., 2017), by normalizing features,
BN layers get rid of range flexibility from networks. Furthermore, BN consumes much amount of
GPU memory and increases computational complexity. Experimental results on benchmark datasets
under common-used assessments demonstrate the superiority of CRNet-A.
Difference to SCN. There are two main differences between CRNet-A and SCN (Wang et al., 2015):
CISTA block and residual learning. Specifically, CRNet-A takes consistency constraint into consid-
eration with the help of CISTA block, while SCN uses linear layers and ignores the information from
the consistency prior. On the other hand, CRNet-A adopts residual learning, which is a powerful
tool for training deeper networks. CRNet-A (30 layers) is much deeper than SCN (5 layers). As
indicated in (Kim et al., 2016a), a deeper network has larger receptive fileds, so more contextual
information in an image would be utilized to infer high-frequency details. In A.1, we show that
more recursions, e.g., 48, can be used to achieve better performance.
Difference to DRCN. CRNet-A differs with DRCN (Kim et al., 2016b) in two aspects: recursive
block and training techniques. In the recursive block, both local residual learning (Tai et al., 2017a)
and pre-activation (He et al., 2016; Tai et al., 2017a) are utilized in CRNet-A, which are demon-
strated to be effective in (Tai et al., 2017a). As for training techniques, DRCN is not easy to train,
so recursive-supervision is introduced to facilitate the network to converge. Moreover, an ensemble
strategy (the final output is the weighted average of all intermediate predictions) is used to further
improve the performance. CRNet-A is relieved from these techniques and can be easily trained with
more recursions.
7 Conclusions
In this work, we propose two effective CSC based image SR models, i.e., CRNet-A and CRNet-B,
for pre-/post-upsampling SR, respectively. By combining the merits of CSC and CNN, we achieve
superior performance against recent state-of-the-arts. Furthermore, our framework and CISTA block
are expected to be applicable in various CSC based tasks, though in this paper we focus on CSC
based image SR.
8
Under review as a conference paper at ICLR 2020
References
Namhyuk Ahn, Byungkon Kang, and Kyung-Ah Sohn. Fast, accurate, and lightweight super-
resolution with cascading residual network. In ECCV, 2018.
Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie-Line Alberi-Morel. Low-
Complexity Single-Image Super-Resolution based on Nonnegative Neighbor Embedding. BMVC,
pp.135.1-135.10, 2012.
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed optimiza-
tion and statistical learning via the alternating direction method of multipliers. Foundations and
Trends in Machine learning, 3(1):1-122, 2011.
Hilton Bristow, Anders Eriksson, and Simon Lucey. Fast Convolutional Sparse Coding. In CVPR,
2013.
I Daubechies, M Defrise, and C De Mol. An iterative thresholding algorithm for linear inverse
problems with a sparsity constraint. Communications on Pure and Applied Mathematics, 57(11):
1413-1457, 2004.
Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep
convolutional networks. TPAMI, 38(2):295-307, 2016.
Cristina Garcia-Cardona and Brendt Wohlberg. Convolutional Dictionary Learning: A Comparative
Review and New Algorithms. IEEE Transactions on Computational Imaging, 4(3):366-381,
2018a.
Cristina Garcia-Cardona and Brendt Wohlberg. Convolutional dictionary learning: A comparative
review and new algorithms. IEEE Transactions on Computational Imaging, 4(3):366-381, 2018b.
Karol Gregor and Yann LeCun. Learning Fast Approximations of Sparse Coding. In ICML, 2010.
Shuhang Gu, Wangmeng Zuo, Qi Xie, Deyu Meng, Xiangchu Feng, and Lei Zhang. Convolutional
Sparse Coding for Image Super-Resolution. In ICCV, 2015.
Muhammad Haris, Gregory Shakhnarovich, and Norimichi Ukita. Deep back-projection networks
for super-resolution. In CVPR, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving Deep into Rectifiers - Surpass-
ing Human-Level Performance on ImageNet Classification. In ICCV, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In ECCV, 2016.
Felix Heide, Wolfgang Heidrich, and Gordon Wetzstein. Fast and flexible convolutional sparse
coding. In CVPR, 2015.
Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from trans-
formed self-exemplars. In CVPR, 2015.
Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep
convolutional networks. In CVPR, 2016a.
Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply-Recursive Convolutional Network for
Image Super-Resolution. In CVPR, 2016b.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2014.
Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro
Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. Photo-
Realistic Single Image Super-Resolution Using a Generative Adversarial Network. In CVPR,
2017.
Juncheng Li, Faming Fang, Kangfu Mei, and Guixu Zhang. Multi-scale residual network for image
super-resolution. In ECCV, 2018.
9
Under review as a conference paper at ICLR 2020
Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep resid-
ual networks for single image super-resolution. In CVPR Workshops, 2017.
Xiaojiao Mao, Chunhua Shen, and Yu-Bin Yang. Image restoration using very deep convolutional
encoder-decoder networks with symmetric skip connections. In NIPS, 2016.
David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented
natural images and its application to evaluating segmentation algorithms and measuring ecological
statistics. In ICCV, 2001.
Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto, Toru Ogawa, Toshihiko Yamasaki, and
Kiyoharu Aizawa. Sketch-based manga retrieval using manga109 dataset. Multimedia Tools and
Applications, 2017.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines.
In ICML, 2010.
Vardan Papyan, Yaniv Romano, and Michael Elad. Convolutional neural networks analyzed via con-
VolUtional sparse coding. The Journal of Machine Learning Research ,18(1):2887-2938, 2017a.
Vardan Papyan, Jeremias Sulam, and Michael Elad. Working locally thinking globally: Theoretical
gUarantees for convolUtional sparse coding. IEEE Transactions on Signal Processing, 65(21):
5687-5701, 2017b.
Vardan Papyan, Yaniv Romano, Jeremias SUlam, and Michael Elad. Theoretical foUndations of deep
learning via sparse representations: A mUltilayer sparse model and its connection to convolUtional
neUral networks. IEEE Signal Processing Magazine, 35(4):72-89, 2018.
Adam Paszke, Sam Gross, SoUmith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, et al. AUtomatic differentiation in pytorch. In NIPS-W, 2017.
Hillel Sreter and Raja Giryes. Learned convolUtional sparse coding. In ICASSP, 2018.
Christian Szegedy, Sergey Ioffe, and Vincent VanhoUcke. Inception-v4, inception-resnet and the
impact of residUal connections on learning. arXiv:11602.07261, 2018.
Ying Tai, Jian Yang, and Xiaoming LiU. Image SUper-ResolUtion via Deep RecUrsive ResidUal
Network. In CVPR, 2017a.
Ying Tai, Jian Yang, Xiaoming LiU, and ChUnyan XU. Memnet: A persistent memory network for
image restoration. In ICCV, 2017b.
RadU Timofte, Vincent De Smet, and LUc Van Gool. A+: AdjUsted anchored neighborhood regres-
sion for fast sUper-resolUtion. In ACCV, 2014.
RadU Timofte, EirikUr AgUstsson, LUc Van Gool, Ming-HsUan Yang, Lei Zhang, et al. Ntire 2017
challenge on single image sUper-resolUtion: Methods and resUlts. In CVPR Workshops, 2017.
RadU Timofte, ShUhang GU, Jiqing WU, and LUc Van Gool. NTIRE 2018 challenge on single image
sUper-resolUtion: methods and resUlts. In CVPR Workshops, 2018.
Tong Tong, Gen Li, Xiejie LiU, and QinqUan Gao. Image sUper-resolUtion Using dense skip connec-
tions. In ICCV, 2017.
Shenlong Wang, Lei Zhang, Yan Liang, and QUan Pan. Semi-coUpled dictionary learning with
applications to image sUper-resolUtion and photo-sketch synthesis. In CVPR, pp. 2216-2223,
2012.
Zhaowen Wang, Ding LiU, Jianchao Yang, Wei Han, and Thomas HUang. Deep networks for image
sUper-resolUtion with sparse prior. In ICCV, 2015.
Zhihao Wang, Jian Chen, and Steven C.H. Hoi. Deep learning for image sUper-resolUtion: A sUrvey.
arXiv:1902.06068, 2019.
10
Under review as a conference paper at ICLR 2020
Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli, et al. Image quality assessment:
from error visibility to structural similarity. IEEE TIP, 13(4):600-612, 2004.
Brendt Wohlberg. Efficient convolutional sparse coding. In ICASSP, 2014.
Brendt Wohlberg. Boundary handling for convolutional sparse representations. In ICIP, 2016.
Chih-Yuan Yang, Chao Ma, and Ming-Hsuan Yang. Single-image super-resolution: A benchmark.
In ECCV, 2014.
Jianchao Yang, John Wright, Thomas Huang, and Yi Ma. Image super-resolution as sparse repre-
sentation of raw image patches. In CVPR, 2008.
Jianchao Yang, John Wright, Thomas S Huang, and Yi Ma. Image super-resolution via sparse
representation. IEEE TIP, 19(11):2861-2873, 2010.
Jianchao Yang, Zhaowen Wang, Zhe Lin, Scott Cohen, and Thomas Huang. Coupled dictionary
training for image super-resolution. IEEE TIP, 21(8):3467-3478, 2012.
Matthew D Zeiler, Dilip Krishnan, Graham W Taylor, and Rob Fergus. Deconvolutional networks.
In CVPR, 2010.
Roman Zeyde, Michael Elad, and Matan Protter. On single image scale-up using sparse-
representations. In International conference on curves and surfaces, pp. 711-730. Springer, 2010.
Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image
super-resolution. In CVPR, 2018.
Zheng Zhang, Yong Xu, Jian Yang, Xuelong Li, and David Zhang. A Survey of Sparse Representa-
tion - Algorithms and Applications. IEEE Access, 3:490-530, 2015.
11
Under review as a conference paper at ICLR 2020
A Appendix
A.1 Residual vs. non-residual
(a) K = 1
(b) K = 5
A.2 CRNet-B
CISTA Block
on
Figure 5: Performance curve for residual/non-residual networks with different recursions. The tests
are conducted on Set5 for scale factor ×3.
Upsampling Modules
H
■■
个 Pre-processing Modules
Auoo
Auoo
a≡nqs
Auou
Auoo
a≡nqs
S
Figure 6:	The architecture of the post-upsampling model CRNet-B.
A.3 Graphical Comparison to other Recurrent CNNs in SR
(a) DRRN
(b) SCN
I Conv ]
匚 ReLU 口
一匕
[ Add J
[Conv ]
ReLU
ReLU
Conv
匚 ReLU 口
ωι ω2	…ωi6
[ Add J
(d) CRNet
(c) DRCN
Figure 7:	Simplified network structures of (a) DRRN (Tai et al., 2017a), (b) SCN (Wang et al.,
2015), (c) DRCN (Kim et al., 2016b), (d) our model CRNet.
A.4 Implementation details
CRNet-A To enlarge the training set, data augmentation, which includes flipping (horizontally and
vertically), rotating (90, 180, and 270 degrees), scaling (0.7, 0.5 and 0.4), is performed on each
image of 291 dataset, where 91 of these images are from Yang et al. (Yang et al., 2010) with the
12
Under review as a conference paper at ICLR 2020
addition of 200 images from Berkeley Segmentation Dataset (Martin et al., 2001). Scale augmen-
tation is exploited by combining images of different scales (×2, ×3 and ×4) into one training set.
Furthermore, all training images are partitioned into 33 × 33 patches with the stride of 33, provid-
ing a total of 1, 929, 728 ILR-HR training pairs. Every convolution layer in CRNet-A contains 128
filters (n0 = 128) of size 3 × 3 while Wl and S have 256 filters (m0 = 256). We follow the same
strategy as He et al. (He et al., 2015) for weight initialization where all weights are drawn from a
normal distribution with zero mean and variance 2/nout , where nout is the number of output units.
The network is optimized using SGD with mini-batch size of 128, momentum parameter of 0.9 and
weight decay of 10-4. The learning rate is initially set to 0.1 and then decreased by a factor of 10
every 10 epochs. We train a total of 35 epochs as no further decrease of the loss can be observed.
For maximal convergence speed, we utilize the adjustable gradient clipping strategy stated in (Kim
et al., 2016a), with gradients clipped to [-θ, θ], where θ = 0.4 is the gradient clipping parameter.
CRNet-B We use the 800 training images of the DIV2K (Timofte et al., 2017) dataset to train
CRNet-B, and all the images are pre-processed by substracting the mean RGB value of the DIV2K
dataset. Data augmentation includes random horizontal flips and rotations. Every weight layer in
CRNet-B has 64 filters (n0 = 64) with the size of 3 × 3 except Wl and S have 1, 024 filters
(m0 = 1, 024). CRNet-B is updated using Adam (Kingma & Ba, 2014) with β1 = 0.9, β2 = 0.999,
and = 10-8. The mini-batch size is 16. We use RGB LR patches of size 54 × 54 as inputs. For
each epoch, there are 103 iterations. The initial learning rate is 10-4 and halved every 200 epochs.
We train CRNet-B for 800 epochs. Unlike CRNet-A, CRNet-B is trained using L1 loss for better
convergence speed.
Recursion We choose K = 25 in both of our models. We implement our models using the PyTorch
(Paszke et al., 2017) framework with NVIDIA Titan Xp. It takes approximately 4.5 days to train
CRNet-A, and 15 days to train CRNet-B.
A.5 Parameter Study
The key parameters in both of our models are the number of filters (n0 , m0) and recursions K.
Number of Filters We set n0 = 128, m0 = 256, K = 25 for CRNet-A as stated in Section A.4.
In Fig. 8(a), CRNet-A with different number of filters are tested (DRCN (Kim et al., 2016b) is used
for reference). We find that even n0 is decreased from 128 to 64, the performance is not affected
greatly. On the other hand, if we decrease m0 from 256 to 128, the performance would suffer an
obvious drop, but still better than DRCN (Kim et al., 2016b). Based on these observations, we set
the parameters of CRNet-B by making m0 larger and n0 smaller for the trade off between model
size and performance. Specifically, we use n0 = 64, m0 = 1024, K = 25 for CRNet-B. As shown
in Fig. 8(b), the performance of CRNet-B can be significantly boosted with larger m0 (MDSR (Lim
et al., 2017) and MSRN (Li et al., 2018) are used for reference). Even with small m0, i.e., 256,
CRNet-B still outperforms MSRN (Li et al., 2018) with fewer parameters (2.0M vs. 6.1M).
Number of Recursions We also have trained and tested CRNet-A with 15, 20, 25, 48 recursions, so
the depth of the these models are 20, 25, 30, 53 respectively. The results are presented in Fig. 9(a).
It’s clear that CRNet-A with 20 layers still outperforms DRCN with the same depth and increasing
K can promote the final performance. The results of using different recursions in CRNet-B are
shown in Fig. 9(b), which demonstrate that more recursions facilitate the performance improved.
A.6 More Qualitative Results
In Fig. 10-15, we provide additional visual results on benchmark datasets to clearly show the
superiority of our proposed models. The pre-upsampling models (SRCNN (Dong et al., 2016),
VDSR (Kim et al., 2016a), DRCN (Kim et al., 2016b), DRRN (Tai et al., 2017a), MemNet (Tai
et al., 2017b)) and the post-upsampling models (MSRN (Li et al., 2018), EDSR/MDSR (Lim et al.,
2017), RDN (Zhang et al., 2018)) are used for comparisons. Red indicates the best performance.
13
Under review as a conference paper at ICLR 2020
2J∙09∙8∙7∙6∙5432Jα
4 4 4 3 3 3 3 3 3 3 3 3 3
(ap)偿Sd
(a) CRNet-A
(b) CRNet-B
Figure 8:	PSNR of proposed models versus different number of filters on Set5 with scale factor ×3.
(a) CRNet-A
(b) CRNet-B
Figure 9: PSNR of proposed models versus different number of recursions on Set5 with scale factor
×3.
Ground Truth	Bicubic	SRCNN	VDSR	DRCN	DRRN	CRNet-A
(PSNR, SSIM)	(21.11,	(22.17,	(22.42,	(22.68,	(22.62,	(23.07,
0.6810)	0.7668)	0.7955)	0.8016)	0.8077)	0.8241)
MemNet	MSRN	EDSR	MDSR	RDN	CRNet-B	CRNet-B+
(22.40,	(23.86,	(24.08,	(24.34,	(24.13,	(24.40,	(24.88,
0.8022)	0.8467)	0.8595)	0.8637)	0.8634)	0.8640)	0.8736)
Figure 10: SR results of “img004” from Urban100 with scale factor ×4.
14
Under review as a conference paper at ICLR 2020
Ground Truth	Bicubic	SRCNN	VDSR	DRCN	DRRN	CRNet-A
(PSNR, SSIM)	(25.47,	(26.58,	(27.45,	(27.29,	(28.05,	(28.00,
0.6149)	0.6453)	0.6892)	0.6882)	0.7057)	0.7058)
MemNet	MSRN	EDSR	MDSR	RDN	CRNet-B	CRNet-B+
(28.17,	(28.55,	(28.84,	(28.85,	(28.68,	(29.01,	(29.10,
0.7101)	0.7206)	0.7289)	0.7282)	0.7254)	0.7330)	0.7345)
Figure 11: SR results of “img026” from Urban100 with scale factor ×4.
Ground Truth	Bicubic	SRCNN	VDSR	DRCN	DRRN	CRNet-A
(PSNR, SSIM)	(28.48,	(30.23,	(31.01,	(30.78,	(31.08,	(31.30,
0.8120)	0.8502)	0.8683)	0.8638)	0.8699)	0.8761)
MemNet	MSRN	EDSR	MDSR	RDN	CRNet-B	CRNet-B+
(31.13,	(31.57,	(31.95,	(31.85,	(31.86,	(32.16,	(32.24,
0.8694)	0.8791)	0.8859)	0.8849)	0.8834)	0.8882)	0.8900)
Figure 12: SR results of “img028” from Urban100 with scale factor ×4.
15
Under review as a conference paper at ICLR 2020
Ground Truth	Bicubic	SRCNN	VDSR	DRCN	DRRN	CRNet-A
(PSNR, SSIM)	(19.87,	(20.61,	(20.75,	(20.62,	(20.89,	(20.81,
0.6467)	0.7205)	0.7476)	0.7437)	0.7636)	0.7650)
MemNet	MSRN	EDSR	MDSR	RDN	CRNet-B	CRNet-B+
(21.02,	(21.60,	(22.50,	(22.36,	(22.31,	(22.72,	(23.04,
0.7692)	0.8118)	0.8480)	0.8392)	0.8406)	0.8464)	0.8563)
Figure 13: SR results of “img062” from Urban100 with scale factor ×4.
Ground Truth Bicubic SRCNN
VDSR
DRCN DRRN CRNet-A
(PSNR, SSIM)	(26.95,
(31.65,
(32.81,
(32.39,	(34.00,	(34.24,
0.9481)
0.9809)	0.9871)
0.9869)	0.9904)	0.9910)
Everything
reκβ∣atκ	rcMfrtat∣(	rcMfrtπth	rex卅靠tit	reMcrtAtiι	ΓCM∏t∏ti(	rcuntΛti(
flake γu IIr	flδke YOIIr	flake your	flake YtIUr	flake γgur	ffəke YQlJr	flake your
Bf⅞ ⅝⅝ ■ j⅞ .■» J⅞j	τ⅞ e B⅞ ■ gfcifj	m ⅝⅝ g⅝ c . a⅝ ⅝⅞J	I m⅝⅝B⅝CB∣g⅝g⅝j	r⅝pι a⅝ e c ■ a⅝ Fijl	Fffl f⅝ F⅝ ■ .J⅝	m ι⅝	时 c Eil
MemNet	MSRN	EDSR	MDSR	RDN	CRNet-B	CRNet-B+
(34.46,	(35.12,	(36.20,	(35.14,	(35.89,	(36.76,	(36.84,
0.9885)	0.9920)	0.9933)	0.9925)	0.9933)	0.9935)	0.9937)
Figure 14: SR results of “ppt3” from Set14 with scale factor ×2.
16
Under review as a conference paper at ICLR 2020
Ground Truth	Bicubic	SRCNN	VDSR	DRCN	DRRN	CRNet-A
(PSNR, SSIM)	(28.50,	(29.40,	(29.54,	(30.29,	(29.70,	(30.54,
0.8288)	0.8565)	0.8654)	0.8656)	0.8673)	0.8704)
MemNet	MSRN	EDSR	MDSR	RDN	CRNet-B	CRNet-B+
(30.25,	(30.95,	(31.18,	(31.12,	(30.34,	(31.14,	(31.23,
0.8701)	0.8778)	0.8797)	0.8791)	0.8771)	0.8801)	0.8809)
Figure 15: SR results of “8023” from B100 with scale factor ×4.
Ground Truth	Bicubic	SRCNN	VDSR	DRCN	DRRN	CRNet-A
(PSNR, SSIM)	(24.13,	(25.00,	(25.66,	(25.69,	(26.10,	(26.17,
0.6923)	0.7450)	0.7769)	0.7776)	0.7938)	0.7956)
MemNet	MSRN	EDSR	MDSR	RDN	CRNet-B	CRNet-B+
(26.20,	(26.12,	(26.61,	(26.53,	(26.64,	(26.71,	(26.82,
0.7958)	0.8006)	0.8193)	0.8114)	0.8191)	0.8173)	0.8213)
Figure 16: SR results of “86000” from B100 with scale factor ×4.
17