Under review as a conference paper at ICLR 2020
MULTIPOLAR: Multi-Source Policy Aggrega-
tion for Transfer Reinforcement Learning be-
tween Diverse Environmental Dynamics
Anonymous authors
Paper under double-blind review
Abstract
Transfer reinforcement learning (RL) aims at improving learning efficiency of an
agent by exploiting knowledge from other source agents trained on relevant tasks.
However, it remains challenging to transfer knowledge between different environ-
mental dynamics without having access to the source environments. In this work,
we explore a new challenge in transfer RL, where only a set of source policies
collected under unknown diverse dynamics is available for learning a target task
efficiently. To address this problem, the proposed approach, MULTI-source POL-
icy AggRegation (MULTIPOLAR), comprises two key techniques. We learn to
aggregate the actions provided by the source policies adaptively to maximize the
target task performance. Meanwhile, we learn an auxiliary network that predicts
residuals around the aggregated actions, which ensures the target policy’s expres-
siveness even when some of the source policies perform poorly. We demonstrated
the effectiveness of MULTIPOLAR through an extensive experimental evaluation
across six simulated environments ranging from classic control problems to chal-
lenging robotics simulations, under both continuous and discrete action spaces.
1 Introduction
We envision a future scenario where a variety of robotic systems, which are each trained or manually
engineered to solve a similar task, provide their policies for a new robot to learn a relevant task
quickly. For example, imagine various pick-and-place robots working in factories all over the world.
Depending on the manufacturer, these robots will differ in their kinematics (e.g., link length, joint
orientations) and dynamics (e.g., link mass, joint damping, friction, inertia). They could provide
their policies to a new robot (Devin et al., 2017), even though their dynamics factors, on which the
policies are implicitly conditioned, are not typically available (Chen et al., 2018). Moreover, we
cannot rely on a history of their individual experiences, as they may be unavailable due to a lack of
communication between factories or prohibitively large dataset sizes. In such scenarios, we argue
that a key technique to develop is the ability to transfer knowledge from a collection of robots to a
new robot quickly only by exploiting their policies while being agnostic to their different kinematics
and dynamics, rather than collecting a vast amount of samples to train the new robot from scratch.
The scenario illustrated above poses a new
challenge in the transfer learning for reinforce-
ment learning (RL) domains. Formally, con-
sider multiple instances of a single environment
that differ in their state transition dynamics,
e.g., independent ant robots with different leg
designs in Figure 1, which reach different lo-
cations by executing the same walking actions.
These source agents interacting with one of the
environment instances provide their determin-
istic policy to a new target agent in another en-
vironment instance. Then, our problem is: can
we efficiently learn the policy of a target agent given only the collection of source policies? Note
that information about source environmental dynamics, such as the exact state transition distribu-
Source agents	Target agent
Figure 1: Ant Example. A policy of a target agent
(right) is learned by utilizing the policies of other
source agents with different leg designs (left).
1
Under review as a conference paper at ICLR 2020
State
St
Adaptive aggregation of source policies: Kgg(St； L, 0agg)
Source policies
Continuous action space:
"target ≡ "(F(sCgg,%ux),∑
Auxiliary network for predicting residuals: faux(s" Jaux)
冗target ≡ SoftmaX ("($t； L, "agg，"aux))
Figure 2: Overview of MULTIPOLAR. We formulate a target policy πtarget with the sum of 1)
the adaptive aggregation Fagg of deterministic actions from source policies L and 2) the auxiliary
network Faux for predicting residuals around Fagg .
tions and the history of environmental states, will not be visible to the target agent as mentioned
above. Also, the source policies are neither trained nor hand-engineered for the target environment
instance, and therefore not guaranteed to work optimally and may even fail (Chen et al., 2018).
These conditions prevent us from adopting existing work on transfer RL between different envi-
ronmental dynamics, as they require access to source environment instances or their dynamics for
training a target policy (e.g., Lazaric et al. (2008); Chen et al. (2018); Yu et al. (2019); Tirinzoni
et al. (2θ18)). Similarly, meta-learning approaches (Vanschoren, 2018; sæmundsson et al., 2018;
Clavera et al., 2019) cannot be used here because they typically train an agent on a diverse set of
tasks (i.e., environment instances). Also, existing techniques that utilize a collection of source poli-
cies, e.g., policy reuse frameworks (Fernandez & Veloso, 2006; Rosman et al., 2016; Zheng et al.,
2018) and option frameworks (Sutton et al., 1999; Bacon et al., 2017; Mankowitz et al., 2018), are
not a promising solution because, to our knowledge, they assume source policies have the same
environmental dynamics but have different goals.
As a solution to the problem, we propose a new transfer RL approach named MULTI-source POL-
icy AggRegation (MULTIPOLAR). As shown in Figure 2, our key idea is twofold; 1) In a target
policy, we adaptively aggregate the deterministic actions produced by a collection of source poli-
cies. By learning aggregation parameters to maximize the expected return at a target environment
instance, we can better adapt the aggregated actions to unseen environmental dynamics of the tar-
get instance without knowing source environmental dynamics nor source policy performances. 2)
We also train an auxiliary network that predicts a residual around the aggregated actions, which is
crucial for ensuring the expressiveness of the target policy even when some source policies are not
useful. As another notable advantage, the proposed MULTIPOLAR can be used for both continuous
and discrete action spaces with few modifications while allowing a target policy to be trained in a
principled fashion. Similar to Ammar et al. (2014); Song et al. (2016); Chen et al. (2018); Tirinzoni
et al. (2018); Yu et al. (2019), our method assumes that the environment structure (state/action space)
is identical between the source and target environments, while dynamics/kinematics parameters are
different. This assumption holds in many real-world applications such as in sim-to-real tasks (Tan
et al., 2018), industrial insertion tasks (Schoettler et al., 2019) (different dynamics comes from the
differences in parts), and wearable robots (Zhang et al., 2017) (with users as dynamics).
We evaluate MULTIPOLAR in a variety of environments ranging from classic control problems to
challenging robotics simulations. Our experimental results demonstrate the significant improvement
of sample efficiency with the proposed approach, compared to baselines that trained a target policy
from scratch or from a single source policy. We also conducted a detailed analysis of our approach
and found it works well even when some of the source policies performed poorly in their original
environment instance.
Main contributions: (1) a new transfer RL problem that leverages multiple source policies col-
lected under diverse environmental dynamics to train a target policy in another dynamics, and (2)
MULTIPOLAR, a simple yet principled and effective solution verified in our extensive experiments.
2
Under review as a conference paper at ICLR 2020
2	Preliminaries
Reinforcement Learning We formulate our problem under the standard RL framework (Sutton &
Barto, 1998), where an agent interacts with its environment modeled by a Markov decision process
(MDP). An MDP is represented by the tuple M = (ρ0 , γ, S, A, R, T ) where ρ0 is the initial state
distribution and γ is a discount factor. At each timestep t, given the current state st ∈ S , the
agent executes an action at ∈ A based on its policy π(at | st ; θ) that is parameterized by θ. The
environment returns a reward R(st , at ) ∈ R and transitions to the next state st+1 based on the state
transition distribution T (st+1 | st, at). In this framework, RL aims to maximize the expected return
with respect to the policy parameters θ.
Environment Instances In this work, we consider K instances of the same environment that differ
only in their state transition dynamics. We model each environment instance by an indexed MDP:
Mi = (ρ0, γ, S, A, R, Ti) where no two state transition distributions Ti, Tj; i 6= j are identical. We
also assume that each Ti is unknown when training a target policy, i.e., agents cannot access the
exact form of Ti nor a collection of states sampled from Ti .
Source Policies For each of the K environment instances, we are given a deterministic source
policy μi : S → A that only maps states to actions. Each source policy μ% can be either parame-
terized (e.g., learned from interacting with the environment modeled by Mi) or non-parameterized
(e.g., heuristically designed by humans). Either way, We assume no prior knowledge about μ% is
available for a target agent, such as their representations or original performances, except that they
were acquired in Mi with an unknown Ti .
Problem Statement Given the set of source policies L = {μι,...,μκ}, our goal is to train a
new target agent’s policy πtarget(at | st; L, θ) in a sample efficient fashion, where the target agent
interacts with another environment instance Mtarget = (ρ0, S, A, R, Ttarget ) and Ttarget is not
necessarily identical to Ti (i = 1 . . . , K).
3	Multi- S ource Policy Aggregation
As shown in Figure 2, with the Multi-Source Policy Aggregation (MULTIPOLAR), we formulate a
target policy πtarget using a) the adaptive aggregation of deterministic actions from the set of source
policies L, and b) the auxiliary network predicting residuals around the aggregated actions. We first
present our method for the continuous action space, and then extend it to the discrete space.
Adaptive Aggregation of Source Policies Let us denote by a(i) = μi(st) the action predicted
deterministically by source policy μ% given the current state st. For the continuous action space,
at(i) ∈ RD is a D-dimensional real-valued vector representing D actions performed jointly in each
timestep. For the collection of source policies L, we derive the matrix of their deterministic actions:
At= h(at(1))>,...,(at(K))>i ∈ RK×D.	(1)
The key idea of this work is to aggregate At adaptively in an RL loop, i.e., to maximize the expected
return. This adaptive aggregation gives us a “baseline” action that could introduce a strong inductive
bias in the training of a target policy, without knowing source environmental dynamics Ti . More
specifically, we define the adaptive aggregation function Fagg : S → A that produces the baseline
action based on the current state st as follows:
Fagg (st; L，θagg ) = K 1K (θagg ® At),	⑵
where θagg ∈ RK ×D is a matrix of trainable parameters, is the element-wise multiplication, and
1K is the all-ones vector of length K. θagg is neither normalized nor regularized, and can scale each
action of each policy independently. This means that we do not merely adaptively interpolate action
spaces, but more flexibly emphasize informative source actions while suppressing irrelevant ones.
Predicting Residuals around Aggregated Actions Moreover, we learn auxiliary network Faux :
S → A jointly with Fagg, to predict residuals around the aggregated actions. Faux is used to improve
the target policy training in two ways. 1) If the aggregated actions from Fagg are already useful in
3
Under review as a conference paper at ICLR 2020
the target environment instance, Faux will correct them for a higher expected return. 2) Otherwise,
Faux learns the target task while leveraging Fagg as a prior to have a guided exploration process.
Any network could be used for Faux as long as it is parameterized and fully differentiable. Finally,
the MULTIPOLAR function is formulated as:
F (st ; L, θagg, θaux) = Fagg (st ; L, θagg) + Faux (st ; θaux),	(3)
where θaux denotes a set of trainable parameters for Faux . Note that the idea of predicting residuals
for a source policy has also been presented by Silver et al. (2018); Johannink et al. (2019); Rana et al.
(2019). The main difference here is that, while these works just add raw action outputs provided
from a single hand-engineered source policy, we adaptively aggregate actions from multiple source
policies in order to obtain a more flexible and canonical representation.
Target Policy Target policy πtarget can be modeled by reparameterizing the MULTIPOLAR func-
tion as a Gaussian distribution, i.e., N (F (st; L, θagg, θaux), Σ), where Σ is a covariance matrix
estimated based on What the used RL algorithm requires. Since We regard μ% ∈ L as fixed func-
tions mapping states to actions, this Gaussian policy πtarget is differentiable with respect to θagg and
θaux, and hence could be trained With any RL algorithm that explicitly updates policy parameters.
Unlike Silver et al. (2018); Johannink et al. (2019); Rana et al. (2019), We can formulate the target
policy in a principled fashion for actions in a discrete space. Specifically, instead of a D-dimensional
real-valued vector, here We have a D-dimensional one-hot vector at(i) ∈ {0, 1}D, Pj(a(ti))j = 1 as
outputs of μi, where (a(i))j = 1 indicates that the j-th action is to be executed. Following Eqs. (2)
and (3), the output of F (st; L, θagg, θaux) can be vieWed as D-dimensional un-normalized action
scores, from which we can sample a discrete action after normalizing it by the softmax function.
4	Experimental Evaluation
We aim to empirically demonstrate the sample efficiency ofa target policy trained with MULTIPO-
LAR (denoted by “MULTIPOLAR policy”). To complete the experiments in a reasonable amount
of time, we set the number of source policies to be K = 4 unless mentioned otherwise. Moreover,
we investigate the factors that affect the performance of MULTIPOLAR. To ensure fair compar-
isons and reproducibility of experiments, we followed the guidelines introduced by Henderson et al.
(2018) and FranCoiS-Lavet et al. (2018) for conducting and evaluating all of our experiments.
4.1	Experimental Setup
Baseline Methods To show the benefits of leveraging source policies, we compared our MULTI-
POLAR policy to the standard multi-layer perceptron (MLP) trained from scratch, which is typically
used in RL literature (Schulman et al., 2θl7; FranCOiS-Lavet et al., 2018). As another baseline, we
also used MULTIPOLAR with K = 1, which is an extension of residual policy learning (Silver
et al., 2018; Johannink et al., 2019; Rana et al., 2019) (denoted by “RPL”) with adaptive residuals
as well as the ability to deal with both continuous and discrete action spaces. We stress here that
the existing transfer RL or meta RL approaches that train a universal policy network agnostic to
the environmental dynamics, such as Frans et al. (2018); Chen et al. (2018), cannot be used as a
baseline since they require a policy to be trained on a distribution of environment instances, which
is not possible in our problem setting. Also, other techniques using multiple source policies, such as
policy reuse frameworks, are not applicable because their source policies should be collected under
the target environmental dynamics.
Environments To show the general effectiveness of the MULTIPOLAR policy, we conducted
comparative evaluations of MULTIPOLAR on the following six OpenAI Gym environments: Ro-
boschool Hopper, Roboschool Ant, Roboschool InvertedPendulumSwingUp, Acrobot, CartPole,
and LunarLander. We chose these six environments because 1) the parameterization of their dynam-
ics and kinematics is flexible enough, 2) they cover discrete action space (Acrobot and CartPole) as
well as continuous action space, and 3) they are samples of three distinct categories of OpenAI Gym
environments, namely Box2d, Classic Control, and Roboschool.
Experimental Procedure For each of the six environments, we first created 100 environment
instances by randomly sampling the dynamics and kinematics parameters from a specific range.
For example, these parameters in the Hopper environment were link lengths, damping, friction,
4
Under review as a conference paper at ICLR 2020
Acrobot
CartPoIe
io
Oooo0.
Oooo
4 3 2 1
p.IEMcu0:u-pos-d山
0.25	0.50	0.75
Timesteps
Q
IoQOQOQOQO O
050505050
112233445
---------
P-IeMəɑ,ypos.d山
λ0°
0.5	1.0	1.5
Timesteps
LunarLandercontinuous
一
Ooooooooon^
Ooo Ooooo
321 12345
-----
pEMφα.ypo∞d山
1.25	2.50	3.75	5.00
Timesteps Ie5
Figure 3: Average Learning Curves of MLP, RPL, and MULTIPOLAR (K = 4) over all the
experiments for each environment. The shaded area represents 1 standard error.
armature, and link mass1 Then, for each environment instance, we trained an MLP policy. The
trained MLP policies were used in two ways: a) the baseline MLP policy for each environment
instance, and b) a pool of 100 source policy candidates from which we sample K of them to train
MULTIPOLAR policies and one of them to train RPL policies2. Specifically, for each environment
instance, we trained three MULTIPOLAR and three RPL policies with distinct sets of source policies
selected randomly from the candidate pool. The learning procedure explained above was done three
times with fixed different random seeds to reduce variance in results due to stochasticity. As a
result, for each of the six environments, we had 100 environment instances × 3 random seeds = 300
experiments for MLP and 100 environment instances × 3 choices of source policies × 3 random
seeds = 900 experiments for RPL and MULTIPOLAR. The aim of this large number of experiments
is to obtain correct insights into the distribution of performances (Henderson et al., 2018). Due to
the large number of experiments for all the environments, our detailed analysis and ablation study
of MULTIPOLAR components were conducted with only Hopper, as its sophisticated second-order
dynamics plays a crucial role in agent performance (Chen et al., 2018).
Implementation Details All the experiments were done using the Stable Baselines (Hill et al.,
2018) implementation of learning algorithms as well as its default hyperparameters and MLP net-
work architecture for each environment (see Appendix A.1 for more details). Based on the perfor-
mance of learning algorithms reported in the Hill et al. (2018), all the policies were trained with
Soft Actor-Critic (Haarnoja et al., 2018) in the LunarLander environment and with Proximal Pol-
icy Optimization (Schulman et al., 2017) in the rest of the environments. For fair comparisons, in
all experiments, auxiliary network Faux had an identical architecture to that of the MLP. There-
fore, the only difference between MLP and MULTIPOLAR was the aggregation part Fagg , which
made it possible to evaluate the contribution of transfer learning based on adaptive aggregation of
source policies. Also, we avoided any random seed optimization since it has been shown to alter the
policies’ performance (Henderson et al., 2018).
Evaluation Metric Following the guidelines of Henderson et al. (2018), to measure sampling ef-
ficiency of training policies, i.e., how quick the training progresses, we used the average episodic
reward over a various number of training samples. Also, to ensure that higher average episodic
reward is representative of better performance and to estimate the variation of it, we used the sam-
ple bootstrap method (Efron & Tibshirani, 1993) to estimate statistically relevant 95% confidence
bounds of the results of our experiments. Across all the experiments, we used 10K bootstrap itera-
tions and the pivotal method. Further details on evaluation method can be found in Appendix A.3.
1Details of sampling ranges for dynamics and kinematics are provided in Appendix A.2.
2Although we used trained MLPs as source policies for reducing experiment times, any type of policies
including hand-engineered ones could be used for MULTIPOLAR in principle.
5
Under review as a conference paper at ICLR 2020
Table 1: MULTIPOLAR vs. Baselines. Bootstrap mean and 95% confidence bounds of average
episodic rewards over various training samples across six environments.
Methods	CartPole			
	25K	50K	75K	100K
MLP	171 (164,179)	229 (220,237)	266 (258,275)	291 (282,300)
RPL	185 (179,192)	238 (231,245)	269 (262,276)	289 (282,296)
MULTIPOLAR (K=4)	202 (195,209)	252 (245,260)	283 (276,290)	299 (292,306)
		Acrobot		
	50K	100K	150K	200K
MLP	-305 (-317,-294)	-164 (-172,-156)	-127 (-133,-121)	-111 (-117,-106)
RPL	-154 (-159,-150)	-120 (-124,-116)	-105 (-109,-102)	-98 (-101,-95)
MULTIPOLAR (K=4)	-151 (-155,-146)	-117 (-121,-113)	-103 (-106,-100)	-96 (-99,-93)
		LunarLander		
	125K	250K	375K	500K
MLP	10 (2,18)	112(104,121)	178 (171,185)	216 (210,221)
RPL	92 (87,96)	178 (174,182)	223 (220,226)	246 (243,248)
MULTIPOLAR (K=4)	95 (90,99)	181 (177,185)	224 (221,228)	246 (244,249)
		Roboschool Hopper		
	0.5M	1M	1.5M	2M
MLP	26 (25,27)	43 (42,45)	67 (64,70)	92 (88,96)
RPL	37 (36,39)	75 (70,79)	114 (107,121)	152 (142,160)
MULTIPOLAR (K=4)	61 (59,64)	138 (132,143)	213 (206,221)	283 (273,292)
		Roboschool Ant		
	0.5M	1M	1.5M	2M
MLP	714 (674,756)	1088 (1030,1146)	1332 (1267,1399)	1500 (1430,1572)
RPL	807 (785,830)	1120 (1088,1152)	1307 (1269,1344)	1432 (1391,1473)
MULTIPOLAR (K=4)	1025 (995,1056)	1397 (1361,1432)	1606 (1568,1644)	1744 (1705,1783)
		Roboschool InvertedPendulumSwingup		
	0.5M	1M	1.5M	2M
MLP	159 (155,164)	267 (260,273)	347 (340,355)	409 (401,417)
RPL	111 (109,113)	195 (192,198)	265 (261,268)	322 (317,326)
MULTIPOLAR (K=4)	375 (355,395)	476 (456,495)	541 (522,559)	588 (571,605)
4.2	Results
Sample Efficiency of MULTIPOLAR Figure 3 and Table 1 clearly show that on average, in all
the environments, MULTIPOLAR outperformed baseline policies in terms of sample efficiency and
sometimes the final episodic reward3. For example, in Hopper over 2M training samples, MULTI-
POLAR with K = 4 achieved a mean of average episodic reward about three times higher than MLP
(i.e., training from scratch) and about twice higher than RPL (i.e., using only a single source policy).
It is also noteworthy that MULTIPOLAR with K = 4 had on par or better performance than RPL,
which indicates the effectiveness of leveraging multiple source policies4. Figure 7 in Appendix,
shows the individual average learning curve for each of the instances of Roboschool environments.
Ablation Study To demonstrate the importance of each component of MULTIPOLAR, we eval-
uated the following degraded versions: (1) θagg fixed to 1, which just averages the deterministic
actions from the source policies without adaptive weights (similar to the residual policy learning
3Episodic rewards in Figure 3 are averaged over 3 random seeds and 3 random source policy sets on 100
environment instances. Table 1 reports the mean of this average over multiple numbers of training samples.
4Video replays of source policies, as well as MULTIPOLAR vs. baseline MLP in the Ant environment is
available at: https://www.youtube.com/watch?v=3b0mGeT3sLo
6
Under review as a conference paper at ICLR 2020
Table 2: Results for MULTIPOLAR and its degraded versions in Hopper.
MULTIPOLAR (K=4)	0.5M	1M	1.5M	2M
Full version	61 (59,64)	138 (132,143)	213 (206,221)	283 (273,292)
θagg fixed to 1	56 (53,59)	118 (111,126)	180 (169,191)	237 (222,250)
Faux learned independent of st	53 (50,56)	101 (95,108)	146 (137,156)	187 (175,200)
Table 3: Results for MULTIPOLAR with different source policy sampling schemes in Hopper.
MULTIPOLAR (K=4)	0.5M	1M	1.5M	2M
Random	61 (59,64)	138 (132,143)	213 (206,221)	283 (273,292)
4 high performance	98 (95,101)	214 (208,220)	323 (314,331)	420 (409,430)
2 high & 2 low performance	45 (43,47)	98 (94,102)	154 (148,160)	208 (200,215)
4 low performance	27 (26,27)	45 (44,47)	68 (66,71)	92 (88,95)
Table 4: Results for MULTIPOLAR with different number of source policies in Hopper.
MULTIPOLAR	0.5M	1M	1.5M	2M
K=4	61 (59,64)	138 (132,143)	213 (206,221)	283 (273,292)
K=8	71 (68,74)	160 (154,167)	246 (236,255)	323 (312,335)
K=16	78 (75,80)	177 (172,182)	272 (264,279)	357 (348,367)
methods that used raw action outputs of a source policy), and (2) Faux learned independent of st ,
which replaces the state-dependent MLP with an adaptive “placeholder” parameter vector making
actions just a linear combination of source policy outputs. As shown in Table 2, the full version
of MULTIPOLAR significantly outperformed both of the degraded versions, suggesting that the
adaptive aggregation and predicting residuals are both critical.
Effect of Source Policy Performances Figure 4 illustrates an example of the histogram of fi-
nal episodic reward (average rewards of the last 100 training episodes) for the source policy can-
didates obtained in the Hopper environment. As shown in the figure, the source policies were
diverse in terms of the performance on their original environment instances5. In this setup,
we investigate the effect of source policies performances on MULTIPOLAR sample efficiency.
We created two separate pools of source policies, where
one contained only high-performing and the other only low-
performing source policies6. Table 3 summarizes the results
of sampling source policies from these pools (4 high, 2 high
& 2 low, and 4 low performances) and compares them to the
original MULTIPOLAR (shown as ‘Random’) also reported
in Table 1. Not surprisingly, MULTIPOLAR performed the
best when all the source policies were sampled from the high-
performance pool. However, we emphasize that such high-
quality policies are not always available in practice, due to the
variability of how they are learned or hand-crafted under their
own environment instance. Figure 6 in Appendix B.1 illus-
trates that MULTIPOLAR can successfully learn to suppress
the useless low-performing source policies.
Figure 4: Histogram of source pol-
icy performances in Hopper.
Effect of Number of Source Policies Finally, we show how the number of source policies con-
tributes to MULTIPOLAR’s sample efficiency in Table 4. Specifically, we trained MULTIPOLAR
policies up to K = 16 to study how the mean of average episodic rewards changes. The monotonic
performance improvement over K (for K ≤ 16), is achieved at the cost of increased training and
inference time. In practice, we suggest balancing this speed-performance trade-off by using as many
source policies as possible before reaching the inference time limit required by the application.
5Histograms for the other environments can be found in Appendix A.4.
6Here, policies with final episodic reward over 2K are high-performing and below 1K are low-performing.
7
Under review as a conference paper at ICLR 2020
5	Discussion and Related Work
Our work is broadly categorized as an instance of transfer RL (Taylor & Stone, 2009), in which
a policy for a target task is trained using information collected from source tasks. In this section,
we highlight how our work is different from the existing approaches and also discuss the current
limitations as well as future directions.
Transfer between Different Dynamics There has been very limited work on transferring knowl-
edge between agents in different environmental dynamics. As introduced briefly in Section 1, some
methods require training samples collected from source tasks. These sampled experiences are then
used for measuring the similarity between environment instances (Lazaric et al., 2008; Ammar et al.,
2014; Tirinzoni et al., 2018) or for conditioning a target policy to predict actions (Chen et al., 2018).
Alternative means to quantify the similarity is to use a full specification of MDPs (Song et al., 2016;
Wang et al., 2019) or environmental dynamics Yu et al. (2019). In contrast, the proposed MULTI-
POLAR allows the knowledge transfer only through the policies acquired from source environment
instances, which is beneficial when source and target environments are not always connected to
exchange information about their environmental dynamics and training samples.
Leveraging Multiple Policies The idea of utilizing multiple source policies can be found in the
literature of policy reuse frameworks (Fernandez & Veloso, 2006; Rosman et al., 2016; Li & Zhang,
2018; Zheng et al., 2018; Li et al., 2019). The basic motivation behind these works is to provide
“nearly-optimal solutions” (Rosman et al., 2016) for short-duration tasks by reusing one of the
source policies, where each source would perform well on environment instances with different
rewards (e.g., different goals in maze tasks). In our problem setting, where environmental dynamics
behind each source policy are different, reusing a single policy without an adaptation is not the right
approach, as described in (Chen et al., 2018) and also demonstrated in our experiment. Another
relevant idea is hierarchical RL (Barto & Mahadevan, 2003; Kulkarni et al., 2016; Osa et al., 2019)
that involves a hierarchy of policies (or action-value functions) to enable temporal abstraction. In
particular, option frameworks (Sutton et al., 1999; Bacon et al., 2017; Mankowitz et al., 2018) make
use of a collection of policies as a part of “options”. However, they assumed all the policies in the
hierarchy to be learned in a single environment instance. Another relevant work along this line of
research is (Frans et al., 2018), which meta-learns a hierarchy of multiple sub-policies by training a
master policy over the distribution of tasks. Nevertheless, hierarchical RL approaches are not useful
for leveraging multiple source policies each acquired under diverse environmental dynamics.
Learning Residuals in RL Finally, some recent works adopt residual learning to mitigate the
limited performance of hand-engineered policies (Silver et al., 2018; Johannink et al., 2019; Rana
et al., 2019). We are interested in a more extended scenario where various source policies with
unknown performances are provided instead of a single sub-optimal policy. Also, these approaches
focus only on RL problems for robotic tasks in the continuous action space, while our approach
could work on both of continuous and discrete action spaces in a broad range of environments.
Limitations and Future Directions Currently, our work has several limitations. First, MULTI-
POLAR may not be scalable to a large number of source policies, as its training and testing times
will increase almost linearly with the number of source policies. One possible solution for this issue
would be pre-screening source policies before starting to train a target agent, for example, by testing
each source on the target task and taking them into account in the training phase only when they are
found useful. Moreover, our work assumes source and target environment instances to be different
only in their state transition distribution. An interesting direction for future work is to involve other
types of environmental differences, such as dissimilar rewards and state/action spaces.
6	Conclusion
We presented a new problem setting of transfer RL that aimed to train a policy efficiently using
a collection of source policies acquired under diverse environmental dynamics. We demonstrated
that the proposed MULTIPOLAR is, despite its simplicity, a principled approach with high training
sample efficiency on a variety of environments. Our transfer RL approach is advantageous when
one does not have access to a distribution of diverse environmental dynamics. Future work will seek
to adapt our approach to more challenging domains such as a real-world robotics task.
8
Under review as a conference paper at ICLR 2020
References
Haitham Bou Ammar, Eric Eaton, Matthew Taylor, Decebal Constantin Mocanu, Kurt Driessens,
Gerhard Weiss, and Karl Tuyls. An Automated Measure of MDP Similarity for Transfer in Rein-
forcement Learning. In Workshops at the AAAI Conference on Artificial Intelligence, 2014.
Pierre-Luc Bacon, Jean Harb, and Doina Precup. The Option-Critic Architecture. In AAAI Confer-
ence on Artificial Intelligence, 2017.
Andrew G Barto and Sridhar Mahadevan. Recent Advances in Hierarchical Reinforcement Learning.
Discrete Event Dynamic Systems,13(1-2):41-77, 2003.
Tao Chen, Adithyavairavan Murali, and Abhinav Gupta. Hardware Conditioned Policies for Multi-
Robot Transfer Learning. In Advances in Neural Information Processing Systems, pp. 9355-9366,
2018.
Ignasi Clavera, Anusha Nagabandi, Simin Liu, Ronald S. Fearing, Pieter Abbeel, Sergey Levine,
and Chelsea Finn. Learning to Adapt in Dynamic, Real-World Environments through Meta-
Reinforcement Learning. In International Conference on Learning Representations, 2019.
Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, and Sergey Levine. Learning Mod-
ular Neural Network Policies for Multi-Task and Multi-Robot Transfer. In IEEE International
Conference on Robotics and Automation, pp. 2169-2176, 2017.
Bradley Efron and Robert Tibshirani. An Introduction to the Bootstrap. Springer, 1993.
Fernando Fernandez and Manuela Veloso. Probabilistic Policy Reuse in a Reinforcement Learning
Agent. In International Joint Conference on Autonomous Agents and Multiagent Systems, pp.
720-727, 2006.
Vincent Frangois-Lavet, Peter Henderson, Riashat Islam, Marc G. Bellemare, and Joelle Pineau. An
Introduction to Deep Reinforcement Learning. Foundations and Trends in Machine Learning, 11
(3-4):219-354, 2018.
Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared
hierarchies. In International Conference on Learning Representations, 2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-Policy
Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. In International Con-
ference on Machine Learning, pp. 1856-1865, 2018.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep Reinforcement Learning That Matters. In AAAI Conference on Artificial Intelligence, pp.
3207-3214, 2018.
Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Rene Traore, Prafulla Dhariwal,
Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schul-
man, Szymon Sidor, and Yuhuai Wu. Stable Baselines. https://github.com/hill-a/
stable-baselines, 2018.
Tobias Johannink, Shikhar Bahl, Ashvin Nair, Jianlan Luo, Avinash Kumar, Matthias Loskyll,
Juan Aparicio Ojea, Eugen Solowjow, and Sergey Levine. Residual Reinforcement Learning
for Robot Control. In International Conference on Robotics and Automation, pp. 6023-6029,
2019.
Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical Deep
Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation. In Advances
in Neural Information Processing Systems, pp. 3675-3683, 2016.
Alessandro Lazaric, Marcello Restelli, and Andrea Bonarini. Transfer of Samples in Batch Rein-
forcement Learning. In International Conference on Machine Learning, pp. 544-551, 2008.
Siyuan Li and Chongjie Zhang. An Optimal Online Method of Selecting Source Policies for Rein-
forcement Learning. In AAAI Conference on Artificial Intelligence, 2018.
9
Under review as a conference paper at ICLR 2020
Siyuan Li, Fangda Gu, Guangxiang Zhu, and Chongjie Zhang. Context-Aware Policy Reuse. In
International Conference on Autonomous Agents and MultiAgent Systems, pp. 989-997, 2019.
Daniel J Mankowitz, Timothy A Mann, Pierre-Luc Bacon, Doina Precup, and Shie Mannor. Learn-
ing Robust Options. In AAAI Conference on Artificial Intelligence, 2018.
Takayuki Osa, Voot Tangkaratt, and Masashi Sugiyama. Hierarchical Reinforcement Learning via
Advantage-Weighted Information Maximization. In International Conference on Learning Rep-
resentations, 2019.
Krishan Rana, Ben Talbot, Michael Milford, and Niko Sunderhauf. Residual Reactive Navigation:
Combining Classical and Learned Navigation Strategies For Deployment in Unknown Environ-
ments. arXiv preprint arXiv:1909.10972, 2019.
Benjamin Rosman, Majd Hawasly, and Subramanian Ramamoorthy. Bayesian Policy Reuse. Ma-
chine Learning, 104(1):99-127, 2016.
Steindor sæmundsson, Katja Hofmann, and Marc Peter Deisenroth. Meta Reinforcement Learning
with Latent Variable Gaussian Processes. In Conference on Uncertainty in Artificial Intelligence,
2018.
Gerrit Schoettler, Ashvin Nair, Jianlan Luo, Shikhar Bahl, Juan Aparicio Ojea, Eugen Solowjow,
and Sergey Levine. Deep reinforcement learning for industrial insertion tasks with visual inputs
and natural reward signals. 2019.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy
Optimization Algorithms. arXiv preprint arXiv:1707.06347, 2017.
Tom Silver, Kelsey Allen, Josh Tenenbaum, and Leslie Kaelbling. Residual Policy Learning. arXiv
preprint arXiv:1812.06298, 2018.
Jinhua Song, Yang Gao, Hao Wang, and Bo An. Measuring the Distance Between Finite Markov
Decision Processes. In International Conference on Autonomous Agents and Multiagent Systems,
pp. 468-476, 2016.
Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, 1st
edition, 1998.
Richard S Sutton, Doina Precup, and Satinder Singh. Between MDPs and Semi-MDPs: A Frame-
work for Temporal Abstraction in Reinforcement Learning. Artificial intelligence, 112(1-2):181-
211, 1999.
Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and
Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. In Robotics:
Science and Systems, 2018.
Ole Tange. GNU Parallel 2018. Ole Tange, 2018. URL https://doi.org/10.5281/
zenodo.1146014.
Matthew E. Taylor and Peter Stone. Transfer Learning for Reinforcement Learning Domains: A
Survey. Journal of Machine Learning Research, 10(Jul):1633-1685, 2009.
Andrea Tirinzoni, Andrea Sessa, Matteo Pirotta, and Marcello Restelli. Importance Weighted Trans-
fer of Samples in Reinforcement Learning. In International Conference on Machine Learning,
pp. 4936-4945, 2018.
Joaquin Vanschoren. Meta-Learning: A Survey. arXiv preprint arXiv:1810.03548, 2018.
Hao Wang, Shaokang Dong, and Ling Shao. Measuring Structural Similarities in Finite MDPs. In
International Joint Conference on Artificial Intelligence, pp. 3684-3690, 2019.
Wenhao Yu, C. Karen Liu, and Greg Turk. Policy Transfer with Strategy Optimization. In Interna-
tional Conference on Learning Representations, 2019.
10
Under review as a conference paper at ICLR 2020
Juanjuan Zhang, Pieter Fiers, Kirby A Witte, Rachel W Jackson, Katherine L Poggensee, Christo-
pher G Atkeson, and Steven H Collins. Human-in-the-loop optimization of exoskeleton assistance
during walking. Science, 356(6344):1280-1284, 2017.
Yan Zheng, Zhaopeng Meng, Jianye Hao, Zongzhang Zhang, Tianpei Yang, and Changjie Fan. A
Deep Bayesian Policy Reuse Approach against Non-Stationary Agents. In Advances in Neural
Information Processing Systems, pp. 954-964, 2018.
11
Under review as a conference paper at ICLR 2020
A Appendix: Further Experimental Details
In this section, we present all the experimental details for the six environments we used. Note that
we did not do any hyperparameter-tuning but followed the default parameters of Hill et al. (2018).
We used the Roboschool implementation of Hopper, Ant, and InvertedPendulumSwingup since they
are based on an open-source engine, which makes it possible for every researcher to reproduce our
experiments. To run our experiments in parallel, we used GNU Parallel tool (Tange, 2018).
A. 1 Hyperparameters
Tables 5 and 6 summarize all the hyperparameters used for experiments on each environment. As
done by Hill et al. (2018), to have a successful training, rewards and input observations are normal-
ized using their running average and standard deviation for all the environments except CartPole and
LunarLander. Also, in all of the experiments, θagg is initialized to be the all-ones matrix.
Table 5: Hyperparameters for Acrobot, CartPole, Hopper, Ant and InvertedPendulumSwingup.
PPO Parameters	Acrobot	CartPole	Hopper	Ant	InvertedPendulumSwingup
#Training samples	200K	100K	2M	2M	2M
#Updates per rollout	4	20	10	10	10
Learning rate	2.5e-4	1e-3	2.5e-4	2.5e-4	2.5e-4
Mini batch size	8	1	128	32	32
Discount factor	0.99	0.98	0.99	0.99	0.99
GAE λ	0.94	0.8	0.95	0.95	0.95
Clip ratio	0.2	0.2	0.2	0.2	0.2
Value function coefficient	0.5	0.5	0.5	0.5	0.5
Entropy coefficient	0	0	0	0	0
Gradient clipping value	0.5	0.5	0.5	0.5	0.5
Optimizer	Adam	Adam	Adam	Adam	Adam
MLP & Faux Parameters					
Hidden layers	64-64	64-64	64-64	16	64-64
Activation functions	tanh	tanh	tanh	tanh	tanh
Table 6: Hyperparameters for LunarLander.
SAC Parameters	LunarLander
#Training samples	500K
#Steps before learning starts	1K
Buffer size	50K
Learning rate	3e-4
Mini batch size	256
Discount factor	0.99
Soft update coefficient τ	5e-3
Entropy coefficient	learned automatically
Model training frequency	1
Target network training frequency	1
#Gradient updates after each step	1
Probability of taking a random action	0
Action noise	none
Optimizer	Adam
MLP & Faux Parameters	
Hidden layers	64-64
Activation functions	relu
12
Under review as a conference paper at ICLR 2020
A.2 Sampling Range of the Environmental Parameters
Sampling ranges for dynamics and kinematics of each environment are provided in Tables 7, 8, 9,
10, 11 and 12. We defined these sampling ranges such that the resulting environments are stable
enough for successfully training an MLP policy. To do so, we trained MLP policies across wide
ranges of environmental parameters and chose the ranges in which the policy converged.
Table 7: Sampling range for Ant kinematic and dynamic parameters.		Table 8: Sampling range for CartPole kine- matic and dynamic parameters.	
Kinematics		Kinematics	
Links	Length Range	Links	Length Range (m)
Legs	[0.4,1.4] X default length	Pole	[0.1,3]	一
	Dynamics		Dynamics
Damping	[0.1, 5]	Force	[6,13]
Friction	[0.4, 2.5]	Gravity	[-14, -6]
Armature	[0.25, 3]	Poll mass	[0.1, 3]
Links mass	[0.7, 1.1] × default mass	Cart mass	[0.3, 4]	
Table 9: Sampling range for Hopper kine-
matic and dynamic parameters.
Kinematics	
Links	Length Range (m)
Leg	[0.35, 0.65]
Foot	[0.29, 0.49]
Thigh	[0.35,0.55]
Torso	[0.3,0.5]
Dynamics	
Damping	[0.5, 4]
Friction	[0.5, 2]
Armature	[0.5, 2]
Links mass	[0.7, 1.1] × default mass
Table 10: Sampling range for InvertedPendu-
lumSwingup kinematic and dynamic param-
eters.
Kinematics	
Links	Length Range (m)
Pole	[0.2, 2]
Dynamics	
Damping	[0.1,5]
Friction	[0.5, 2]
Armature	[0.5, 3]
Gravity	[-11, -7]
Links mass	[0.4, 3] × default mass
Table 11: Sampling range for Acrobot kinematic and dynamic parameters.		Table 12: Sampling range for Lu- narLander kinematic and dynamic parameters.	
	Kinematics	Kinematics	
Links	Length Range (m)	Side engine height	[10, 20]
Link 1&2	[0.3, 1.3]	—	Dynamics
	Dynamics	Scale	[25, 50]
Links mass	[0.5, 1.5]	Initial Random	[500, 1500]
Links center mass	[0.05, 0.95] × link length	Main engine power	[10, 40]
Links inertia moments	[0.25, 1.5]	Side engine power	[0.5, 2]
			ΓQ 1 Ql
		Side engine away	[8, 18]
13
Under review as a conference paper at ICLR 2020
A.3 Evaluation Method
In this section, we explain how we calculated the mean of average episodic rewards in Tables 1, 2, 3,
and 4, over a specific number of training samples (the numbers at the header of the tables e.g., 25K,
50K, 75K, and 100K for the CartPole) which we denote by T in what follows. For each experiment
in an environment instance, we computed the average episodic reward by taking the average of the
rewards over all the episodes the agent played from the beginning of the training until collecting T
number of training samples. Then we collected the computed average episodic rewards of all the
experiments, i.e., all the combinations of three random seeds, three random sets of source policies
(for RPL and MULTIPOLAR), and 100 target environment instances. Finally, we used the sample
bootstrap method (Efron & Tibshirani, 1993) to estimate the mean and the 95% confidence bounds
of the collected average episodic rewards. We used the Facebook Boostrapped implementation:
https://github.com/facebookincubator/bootstrapped.
A.4 Source Policies Histograms
To generate environment instances, we uniformly sampled the dynamics and kinematics parameters
from the ranges defined in Section A.2. Figure 5 illustrates the histograms of the final episodic
rewards of source policies on the original environment instances in which they were acquired.
CartPoIe	Acrobot	Lu na rl_a nderContinuous
O
-150 -IOO -50
Final Episodic Reward
RoboschooIAnt
100	200	300
Final Episodic Reward
RoboschoollnvertedpenduIumSwingup
5 4 3 2 1
sωyod ①Q-lnos#
OO 200	300	40∣
Final Episodic Reward
RoboschooIHopper
O IOOO 2000	3000	4000	400	600	800 IOOO
Final Episodic Reward	Final Episodic Reward
Figure 5:	Histogram of final episodic rewards obtained by source policies per environment.
14
Under review as a conference paper at ICLR 2020
B	Appendix: Additional Results
B.1	learned aggregation parameters visualization
Figure 6 visualizes an example of how the aggregation parameters θagg for the four policies and
their three actions were learned during the 2M timestep training of MULTIPOLAR (K = 4) policy
in the Hopper environment. In this example, the source policies in the first and second rows were
sampled from low-performance pools whereas those in the third and fourth rows were sampled from
high-performance pools (see Section 4.2 for more details). It illustrates that MULTIPOLAR can
successfully suppress the two useless low-performing policies as the training progresses.
Source #1, Action #1
Source #1, Action #2
Source #1, Action #3
Source #2, Action #1
Source #2, Action #2
Source #2, Action #3
Source #3, Action #1
Source #3, Action #2
Source #3, Action #3
Source #4, Action #1
Source #4, Action #2
Source #4, Action #3
Figure 6:	Aggregation parameters θagg during the training of MULTIPOLAR (K = 4) in the Hopper
that has 3-dimensional actions. Here, the first two source policies are low-performing and the last
two are high-performing in their original environment instance.
B.2 MULTIPOLAR with randomly initialized policies
To further study how having low-performing source policies affects MULTIPOLAR sample effi-
ciency, we evaluated MULTIPOLAR (K=4) in the Hopper environment, where the sources are ran-
domly initialized policies, i.e., policies that predict actions randomly. Following our experimental
procedure explained in Section 4.1, Table 13 reports the bootstrap mean and 95% confidence bounds
15
Under review as a conference paper at ICLR 2020
of average episodic rewards over various training samples for this experiment and compares it with
MULTIPOLAR with four low-performing sources. This result suggests that the sample efficiency of
MULTIPOLAR (K = 4) with low-performing source policies (i.e., source policies which had low
final episodic rewards in their own environments) is almost the same as with randomly initialized
source policies.
Table 13: Results for MULTIPOLAR with low-performing source policies vs. with randomly ini-
tialized source policies in Hopper.
MULTIPOLAR (K=4)	0.5M	1M	1.5M	2M
4 randomly initialized	27 (26,28)	47 (45,49)	73 (70,76)	101 (96,106)
4 low performance [Table 3]	27 (26,27)	45 (44,47)	68 (66,71)	92 (88,95)
B.3 individual average learning curves
As an example, we visualized the individual average learning curve of policies for each of the
environment instances of the Roboschool environments. Figures 7, 8 and 9 compare the individual
average learning curves of MULTIPOLAR to the baseline policies in the 100 target environment
instances.
16
Under review as a conference paper at ICLR 2020
2500
2000
1500
IOOO
,500
O
RoboschooIHopper
Timesteps
2500
2000
1500
IOOO
,500
O
RoboschooIHopper
2500
2000
1500
IOOO
,500
O
RoboschooIHopper
Timesteps
RoboschooIHopper
I
2000
RoboschooIHopper
1500
10∞
500
O
0.0	0.5	1.0	1.5	2.0
Timesteps 1≡6
RoboschooIHopper
Timesteps
RoboschooIHopper
g 2000
S 1500
¾1000
I 500
O
0.0	0.5	1.0	1.5	2.0
Timesteps 1≡6
RoboschooIHopper
I
30∞
2500
2000
1500
IOOO
500
O
0.0	0.5	1.0	1.5	2.0
Timesteps 1≡6
RoboschooIHopper
RoboschooIHopper
500
O
P 2500
g 2000
S 1500
⅞ IOOO
I 2500
S 2000
„1500
g IOOO
I 500
uι O
0.0	0.5	1.0	1.5	2.0
Timesteps 1≡6
RoboschooIHopper
p 3000
I 2500
g 2000
u 1500
Q IOOO
I. 500
uι O
0.0	0.5	1.0	1.5	2.0
Timesteps 1≡6
RoboschooIHopper
I
∙g25OO
g 2000
S 1500
⅞ IOOO
i 500
uι O
1.0	1.5
Timesteps
RoboschoolHopper
§ 2500
Jl 2000
„1500
ζS IOOO
a. 500
uι o
0.0	0.5	1.0	1.5
Timesteps
⅝ i5(n
S
y IOOO
1 500
⅛
O
m 2500
I 2000
■ 1500
⅜ IOOO
∣5∞
uι O
3000
2500
I 2000
u 1500
⅝ iooo
a. 5∞
uι O
Timesteps
RoboschooIHopper
RoboschooIHopper
2000
2000
1500
10∞
500
Il
2000
2000
I
500
O
O
2000
1500
2500
500
O
1500
IOOO
,5∞
O
E 2500
S 2000
g 2000
S 1500
⅞ IOOO
Timesteps
RoboschooIHopper
Timesteps
RoboschooIHopper
P 2500
g 2000
Z 1500
⅞ IOOO
O
0.0	0.5	1.0	1.5	2.0
Timesteps 1≡6
RoboschooIHopper
1.0	1.5	2.0
Timesteps 1≡6
RoboschooIHopper
营 1500
M IOOO
∣15∞
MlooO
.8 500
g IOOO
$ 5∞
11500
1a iooo
500
O
r
Timesteps
RoboschooIHopper
»1500
r
O
Il
Timesteps
RoboschgIHopper
s-
s-
Timesteps
le6
Timesteps
RoboschoolHopper
Timesteps
RoboschooIHopper
Timesteps
RoboschooIHopper
Timesteps
RoboschooIHopper
2500
2000
1500
ð IOOO
500
O
2500
2000
1500
ð IOOO
500
Il
0.0	0.5	1.0	1.5
Timesteps
1.0	1.5	2.0
Timesteps le6
0.0	0.5	1.0	1.5	2.0
Timesteps le6
0.0	0.5	1.0	1.5	2.0
Timesteps le6
1.0	1.5	2.0
Timesteps 1≡6
RoboschgIHopper
⅛ 2000
M
g 1500
手 IOOO
S 500
1.0	1.5	2.0
Timesteps 1≡6
RoboschoolHopper
Timesteps
RoboschooIHopper
500
O
0.0	0.5	1.0	1.5	2.0
Timesteps 1≡6
RoboschooIHopper
RoboschooIHopoer
Timesteps
■g 2000
1500
.y ιooo
1.0	1.5	2.0
Timesteps le6
.3 500
∞ O
1500
y IOOO
500
O
0.0	0.5	1.0	1.5	2.0
Timesteps le6
RoboschooIHopper
Timesteps
RoboschooIHopper
2500
2000
1500
IOOO
,500
O
0.0
2500
2000
1500
IOOO
.500
O
⅛ 2000
g 1500
=IOOO
500
Timesteps
RoboschooIHopper
1.0	1.5
Timesteps
RoboschoolHopper
1500
IOOO
500
1.0	1.5
Timesteps
RotioschooIHopper
2000	
1500	
IOOO	/7
500	一一 一
O	
Timesteps
10∞
500
Timesteps
RoboschooIHopper
2000 1500 IOOO 500 O	
RoboschooIHopper

RoboschooIHopper
zaυυ 2000 1500 IOOO 500 O	
RoboschgIHopper
30∞ 2500 2000 1500 IOOO 500 O	
2000
1500
10∞
RoboschgIHopper

500
P 2500
§ 2000
^1500
⅞ IOOO
a. 5∞
uι o
1.0	1.5
Timesteps
RoboschooIHopper
1500
Timesteps
10∞
RoboschooIHopper
500
O
,£■
Timesteps
O


RoboschoolHopper
1500
IOOO
500
0.0	0.5	1.0	1.5	2.0
Timesteps le6
RoboschooIHopper
■g 2000
1500
.y ιooo
500
H IOOO
,£■
O
0.0	0.5	1.0	1.5	2.0
Timesteps le6
2500
2000
1500
IOOO
,500
O
2500
2000
1500
IOOO
5∞
O
1.0	1.5
Timesteps
RoboschoolHopper
p 2500
⅝ 2000
S 1500
IOOO
500
O
2000 1500 IOOO 500 O	一
RoboschooIHopper
Timesteps
RoboschgIHopper
30∞	
2500	
2000	
1500	/ q
IOOO	
500	
O
0.0	0.5	1.0	1.5	2.0
Timesteps le6
RoboschgIHopper
1500

500
O
,£■
Timesteps
le6

RoboschooIHopper
2000
500
11500
⅛ IOOO
O
0.0	0.5	1.0	1.5	2.0
Timesteps le6
RoboschoolHopper
P 3000
n 2500
I 2000
u 1500
IOOO
,500
O
0.0	0.5	1.0	1.5	2.0
Timesteps le6
17
Under review as a conference paper at ICLR 2020
RoboschooIHopper
PJeMaHypos-dUJ
RoboschooIHopper
0.5	1.0	1.5	2.，
Timesteps 1≡6
RoboschoolHopper
0.0	0.5	1.0	1.5	2.0
Timesteps le6
12000
隹 1500	/
号 1000	)
I 500
uι 0	I
0.0	0.5	1.0	1.5	2.0
0.5	1.0	1.5	2.，
Timesteps 1≡6
RoboschoolHopper
S 2500
⅝ 2000
g 1500
1g 1000
a. 500
uι o
RoboschoolHopper
0.0	0.5	1.0	1.5	2.0
Timesteps le6
pooo
雪 1500
S
u 1000
I 500
品QL
0.0
m 2000
11500
M ɪooo
"8
.S5 500
S »
0.5	1.0	1.5	2.，
Timesteps le6
m2000
11500
.«iooo ,匕
.2 5W	-
1S'	^
O bɪ-____________________
0.0	0.5	1.0	1.5	2.0
0.0	0.5	1.0	1.5	2.0
Timesteps le6
产
u 1000
RoboschoolHopper
0.5	1.0	1.5	2.，
Timesteps le6
P 3000
n 2500
着 2000
u 1500	.
⅝ iooo
a. 5∞	，A
uι o ~_______________________________
0.0	0.5	1.0	1.5	2.0
RoboschooIHopper
0.5	1.0	1.5	2.，
Timesteps le6
0.0	0.5	1.0	1.5	2.0
Timesteps le6
RoboschgIHoppgr
g 2000
f 1500
•OoO
≡ 500
0.0	0.5	1.0	1.5	2.0
Timesteps le6
iɪ500
H IOOO
500
O
g 2000
£ 1500
⅞ iooo
I 500
uj 如
0.0	0.5	1.0	1.5	2.0
Timesteps le6
P 2000
j 1500
考 IOOO
,i 500
岳C
0.0	0.5	1.0	1.5	2.0
Timesteps le6
0.0	0.5	1.0	1.5	2.0
Timesteps le6
RoboschooIHopper
0.5	1.0	1.5	2.，
Timesteps le6
RoboschooIHopper
M 2000
11500
1000
500
0 L
0.0
I I
PJeMəhypos-da
RoboschoolHopper
PJeMəhypos-d3
0.5	1.0	1.5	2.0
Timesteps le6
0.5	1.0	1.5
∕uuu
1500
1000
500
0
∙p2000
1OO
u IOOO
,ð 500
品一
0.0
I 2000
11500
IOOO
500
O
RoboschooIHopper
0.0	0.5	1.0	1.5	2.0
Timesteps le6
RoboschooIHopper
0.5	1.0	1.5	2.，
Timesteps 1≡6
I 2000
^1500
⅞ 1000	二X∙⅛√'~
I 500
uj 0 ~----------------, f
0.0	0.5	1.0	1.5	2.0
Timesteps le6
0.0	0.5	1.0	1.5	2.0
Timesteps le6
PJeMQHypos-da
RobcschgIHopper
Timesteps
P 2500
I 2000
整 1500
⅞ 1000
∙∣ 500
uι 0
RoboschooIHopper
0.5	1.0	1.5	2.0
Timesteps le6
Figure 7: Average learning curves of MULTIPOLAR with K = 4 in red, RPL in green and MLP
in blue over 3 random seeds and 3 random source policy sets for all the 100 target environment
instances of Hopper.
18
Under review as a conference paper at ICLR 2020
RotxjschooIAnt	Rotχjsch∞IAnt
Episodic Reward	Episodic Reward	Episodic Reward	Episodic Reward	Episodic Reward	Episodic Reward	Episodic Reward	Episodic Reward	Episodic Reward	Episodic Reward
xx)ooxx)oo
Q 5 Q 5
0.0	0.5	1.0	1.5	2.0
Timesteps le6
RobOSCh8∣Ant
2000
1500
1000
500
0
0.0
0.5	1.0	1.5	2.0
Timesteps 1≡6
RoboschoolAnt
2000
1500
1000
ebməh.ypos'a.ul
0.5	1.0	1.5
Tmesteps
RoboSCh8∣Ant
ɔuw
I 2500
S 2000
"C 1500
含 ɪooo
1 500
£- 0
-500
25∞
PJeMaHypos-dul
1.0
Timesteps
Roboschoo IAnt
0.0	0.5	1.0	1.5	2.0
Timesteps 1≡6
Roboschoo∣Ant
RoboschoolAnt
2500
2000
1500
1000
500
0.0	0.5	1.0	1.5	2.0
Timesteps 1≡6
Roboschoo∣Ant
0.0	0.5	1.0	1.5	2.0
Timesteps 1≡6
Roboschoo∣Ant
0.5	1.0	1.5
Tmesteps
Robosch8∣Ant
0.5	1.0	1.5
Tmesteps
RoboschooIAnt
0.5	1.0	1.5
Tmesteps
RoboschoolAnt
0.5	1.0	1.5
Timesteps
RoboschoolAnt
ɪ I I I I 1 , ■
PJeMaH.ypos'a.ul
0.5	1.0	1.5
Tmesteps
RoboschooIAnt
1.0	1
Tmesteps
Roboschoo IAnt
0.5	1.0	1.5
Tmesteps
Robosch8∣Ant
0.5	1.0	1.5
Tmesteps
RoboschoolAnt
g 2000
S 1500
ɪg 1000
s- 5∞
0.5	1.0	1.5
Tmesteps
RoboschoolAnt
0.5	1.0	1.5
Tmesteps
Roboschoo∣Ant
y 1000
1 500
ω
RotioschoolAnt
0.5	1.0	1.5
Tmesteps
RotioschoolAnt
0.0	0.5	1.0	1.5	2.0
Timesteps le6
0.0	0.5	1.0	1.5	2.0
Timesteps le6
0.0	0.5	1.0	1.5	2.0
Timesteps le6
RoboschooIAnt
ooxx)ooxx)oo
5 Q 5 Q 5
EeM3H.ypos'a.ul
RobOSChOOIAnt
0.5	1.0	1.5
Tmesteps
RoboschoolAnt
P-JeMaH.ypos'a.3
ɔuuv
2500
2000
1500
10∞
500
PJeMaH.ypos'a.ul
RoboschooIAnt
0.5	1.0	1.5	2.0
Timesteps le6
Timesteps
I 2000
S 1500
⅛ 1000
∙p 3500
n 30∞
蓄 2500
H 2000
⅞ 1500
I ιo∞
⅛ 500
0.0	0.5	1.0	1.5	2.0
Timesteps le6
RoboschoolAnt
RoboschooIAnt
P 2500
I 2000
≈ 1500
V 1000
I 500
0.5	1.0	1.5	2.0
Timesteps le6
■g 2000
11500
M 1000
I 500
Roboschoo∣Ant
0.0	0.5	1.0	1.5	2.0
Timesteps le6
19
Under review as a conference paper at ICLR 2020
PJeMəh U-PoS-dUJ
ɔuw
2500
2000
1500
1000
500
0
-500
Tmesteps
E 2000
jl500
铲000
a. 500
ebməh U=UOnKW
Robosch8∣Ant
Timesteps
RoboschooIAnt
Robosch8∣Ant
Robosch8∣Ant
PJeMəhypos-dul
Timesteps
le6
0.5	1.0	1.5
Tmesteps
P-JeMəhypos-da
Roboschoo IAnt
p 3500
o 3000
g 2500
K 2000
u 1500
ŋ 1000
« 500
£- 0
-500
E 3000
1 2500
S 2000
⅞1500
.∣ iooo
ω 500
而ooxx>oox)oooQ
-G 5 G 5 0 5
-32211
PJeMəhyposaUJ
RoboschooIAnt
Timesteps
Roboschoo IAnt
Roboschoo IAnt
Timesteps
PJeMaH.ypos - dul
0.5	1.0	1.5
Tmesteps
RobOSChOOIAnt
RoboschooIAnt

1 ■ 1 >
PJeMQHypos-dul
Timesteps
ie6
RoboschooIAnt
2500
2000
1500
0.5	1.0	1.5
Tmesteps
0.5	1.0	1.5
Tmesteps
RobOSChOOIAnt
0.0	0.5	1.0	1.5	2.0
Timesteps 1≡6
1.0	1.5	2.0
Timesteps 1≡6
RoboschooIAnt
■ 一
PJeMQH.ypsdul
0.5	1.0	1.5
Timesteps
P 3000
⅛ 2500
g 2000
H 1500
⅞ iooo
3 500
£- o
-500
0.0	0.5	1.0	1.5	2.0
Timesteps le6
RobOSChooIAnt
I I
PJeMaH.ypsdul
Timesteps
le6
1.0
Tmesteps
Roboschoo IAnt
Timesteps
Robo sc ho。IAnt
PJeMSH.ypos!d3
PJeMaH.ypos'a.ul
RoboschooIAnt
1.0
Tmesteps
Roboschoo IAnt
Roboschoo IAnt
Timesteps
Timesteps
RoboschooIAnt
0.5	1.0	1.5
Timesteps
⅛ 3000
着 2500
H 2000
B 1500
gιooo
⅛ 500
0
P 2500
I 2000
S 1500
=S 1000
ɪ 500
0
P-JaMaH.ypos'a.3

⅛25OO
S 20W
u 1500
B iooo
0. 500
1.0
Timesteps
Roboschoo IAnt
RotioschoolAnt
E 3000
S 2500
£ 2000
⅛ 1500
11000
S 500
Roboschoo IAnt
Timesteps
ie6
RoboschooIAnt
0.5	1.0	1.5
Tmesteps
RoboschoolAnt
0.5	1.0	1.5
Tmesteps
Robosch8∣Ant
0.5	1.0	1.5
Tmesteps
RoboschoolAnt
0.5	1.0	1.5
Timesteps
RoboschoolAnt
Timesteps
Timesteps
RoboschooIAnt
1.0	1.5	2.0
Timesteps le6
p 30∞
g 2500
2 2000
.a 1500
QooO
⅛ 500
0
■ 一
P-JeMəh.ypos'a.3
■g 2500
g 2000
S 1500
M IOOO
3 500
⅛ O
0.5	1.0	1.5
Timesteps
Robosch8∣Ant
1.0
Tmesteps
Roboschoo IAnt
Timesteps
Timesteps
RoboschooIAnt
P 2500
詈 2000
1 1500
a iooo
8 500
后。
-500
■ 一
PJeMəhypos-dul
■ 一
Posvtfypos-da
2500
2000
1500
1000
500
RotioschooIAnt
0.0	0.5	1.0	1.5	2.0
Timesteps 1≡6
2000
1500
1000
500
0
RoboschooIAnt
le6
Timesteps
Roboschoo IAnt
le6
Timesteps
Figure 8: Average learning curves of MULTIPOLAR with K = 4 in red, RPL in green and MLP
in blue over 3 random seeds and 3 random source policy sets for all the 100 target environment
instances of Ant.
20
Under review as a conference paper at ICLR 2020
Episodic Rewand	Episodic ReWaE	Episodic ReWaE	Episodic Reward	Episodic Rewand	Episodic ReWaE	Episodic ReWaE	Episodic Rewand	Episodic ReWaE	Episodic ReWaE
I ■	■ ■	■ ■
FUeMaH U-PoS-dm
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.
Timesteps ɪefi
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.
Timesteps ie6
RoboschMllnvertedperxlulumSwlngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ie6
FUeMaH U-POS-dm
RoboschMllnvertedperxlulumSwlngup
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
FUeMaH U-POS-dm
FUeMaH U-POS-dm
RoboschMllnvertedperxlulumSwIngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ie6
FUeMaH U-POS-dm
FUeMaH U-POS-dm
FUeMaH U-POS-dm
peM3H U-POS-dm
FUeMaH U-POS-dm
FUeMaH U-POS-dm
RoboschMllnvertedperxlulumSwlngup
1O∞	---------------- - ,n
800
600
400
200
0
FUeMaH U-POS-dm
FUeMaH U-POS-dm
RoboschMllnvertedperxlulumSwIngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
Timesteps
ROboSCh8∣ I n verted Perxlulu m5wl ngup
FUeMaH U-POS-dm FUeMaH U-Pos_dm FUeMaH U-POS-dm PLeMaH VPoS-dm FUeMaH VPoS-dm FUeMaH u_pos_dm FUeMaH U-POS-dlll
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ie6
ROboSCh8∣ I nve rted Perxlulu m5wl ngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ie6
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ie6
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
^Robosch∞ll n verted Perxlulu m5wl ngup
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ie6
RoboschMllnvertedperxlulumSwlngup
1000
800
600
400
200
0
Timesteps
FUeMaH U-POS-dm
FUeMaH U-PoS-dm FUeMaH U-POS-dm
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
ROboSCh8∣ I nve rted Perxlulu m5wl ngup
0.0	0.5	1.0	1.5	2.0
Timesteps ie6
RoboschmllnvertedpendulumSwlngup
1800
Loo
∣400
&oo
0
0.0	0.5	1.0	1.5	2.0
Timesteps ieδ
FUeMaH U-POS-dm
0.0	0.5	1.0	1.5	2
Timesteps ι≡6
peM3H U-POS-dm
FUeMaH U-PoS-dm FUeMaH U-POS-dm FUeMaH U-POS-dm
FUeMSH U-PoS-dm FUeMaH U-POS-dm
RoboschmllnvertedpendulumSwlngup
0.0	0.5	1.0	1.5	2.0
Timesteps ieδ
RoboschMllnvertedperxlulumSwlngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ie6
θRobosch∞l In verted Pe nd ul UmSwlngu p
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ie6
RoboschMllnvertedperxlulumSwlngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ie6
FUeMaH U-POS-dm FUeMaH U-PoS-dm FUeMaH U-POS-dm PLeMaH VPoS-dm FUeMaH VPoS-dm FUeMaH u_pos_dm FUeMaH U-POMd111
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ie6
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ie6
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ie6
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
ie6
Timesteps
ROWSCh8∣lnwerted Pend ulumSwl 埠 UP
1000
800
600
400
200
0
Vvvw
1.0	1.5	2.0
Timesteps ie6
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
Timesteps
21
Under review as a conference paper at ICLR 2020
FUeMaHU-POS-dm FUeM<UH U-Poqdm
RoboschMllnvertedperxlulumSwIngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
FUeMaHU-POS-dm FUeM<UH U-Poqdm
RoboschMllnvertedperxlulumSwIngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
ROboSCh8∣ I nve rted Perxlulu m5wl ngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
FUeMaHU-POS-dm FUeM<UH U-Poqdm
RoboschgllnwertecipencluIumSwI ngu P
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
Timesteps
fUeMaH U-PoS-dm
10∞
800
600
400
200
0
FUeMaH U-POS-dm
FUeMaH U-POS-dm
RoboschMllnvertedperxlulumSwIngup
Timesteps
ROt^oSCh8lInWerteClpenCIUIUmSWl 叩UP
1000
800
600
400
200
Γ

00.0
0.5	1.0	1.5	2.0
Timesteps ɪefi
FUeMaHU一 PeS-dm
FUeMaH u_pos_dm
FUeMQH u_pos_dm
0.0	0.5	1.0	1.5	2
Timesteps ι≡6
FUeMaH u_pos_dm
0.0	0.5	1.0	1.5	2.
Timesteps ie6
0.0	0.5	1.0	1.5	2
Timesteps ι≡6
0.0	0.5	1.0	1.5	2
Timesteps ι≡6
FUeMaHU一 PeS-dm p」emailU一 pos-dm
FUeMaH U-POS-dm
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
I：UeMaH U-POS-dm
fUeM<UH U-POS-dm
fUeMaH U-PoS-dm
ie6
Timesteps
1000
800
600
400
200
0
FUeMaH U-POS-dm
FUeMaHU-POS-dm FUeM<UH U-Poqdm
RoboschMllnvertedperxlulumSwIngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
fUeMaH U-POS-dm
fUeMaH U-PoS-dm
fUeMaH U-POS-dm
FUeMaH u_pos_dm

0.0	0.5	1.0	1.5	2
Timesteps ι≡6
fUeM<UH U-POS-dlll
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
∣:UeMaHU一 PeS-dm I:UeMaHU一 PeS-dm
0.0	0.5	1.0	1.5	2
Timesteps ι≡6
0.0	0.5	1.0	1.5	2.0
Timesteps ie6
fUeMaH U-POS-dlll
0.0	0.5	1.0	1.5	2
Timesteps ι≡6
∣:UeMeHU一 PeS-dm ∣:UeMaHU一 PeS-dm
0.0	0.5	1.0	1.5	2
Timesteps ι≡6
1000
800
600
400
200
0.5	1.0	1.5	2.0
Timesteps ɪefi
FUeMaH U 一 PoS-dm
fUeMaH U-POS-dm
FUeMaH U 一 PoS-dm
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ie6
FUeMaHU一 PeS-dm p」emailU一 pos-dm
fUeMaH U-PoS-dm
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
EeM3H U-PoS-dm
EeM3H U-PoS-dm
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0
0.0	0.5	1.0	1.5	2.0
Timesteps ɪefi
r
1000e00600400200n
I：UeMaH u_pos_dm
0.5	1.0	1.5	2.0
Timesteps ɪefi
fUeMaH U-PoS-dm
0.5	1.0	1.5	2.0
Timesteps ɪefi
FUeMaH U-POS-dm
ROboSCh8∣ I n verted Perxlulu m5wl ngup
1000
800
600
400
200
0.5	1.0	1.5	2.0
Timesteps ɪefi
Figure 9: Average learning curves of MULTIPOLAR with K = 4 in red, RPL in green and MLP
in blue over 3 random seeds and 3 random source policy sets for all the 100 target environment
instances of InvertedPendulumSwingup.
22