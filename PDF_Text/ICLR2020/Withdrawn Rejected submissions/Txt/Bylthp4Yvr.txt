Under review as a conference paper at ICLR 2020
Dropout: Explicit Forms and Capacity Con-
TROL
Anonymous authors
Paper under double-blind review
Ab stract
We investigate the capacity control provided by dropout in various machine learn-
ing problems. First, we study dropout for matrix sensing, where it induces a data-
dependent regularizer that, in expectation, equals the weighted trace-norm of the
product of the factors. In deep learning, we show that the data-dependent regular-
izer due to dropout directly controls the Rademacher complexity of the underly-
ing class of deep neural networks. These developments enable us to give concrete
generalization error bounds for the dropout algorithm in both matrix completion
as well as training deep neural networks. We evaluate our theoretical findings on
real-world datasets, including MovieLens, Fashion MNIST, and CIFAR-10.
1	Introduction
Dropout is a popular regularization technique for training deep neural networks that aims at “break-
ing co-adaptation” among neurons by randomly dropping them at training time (Hinton et al., 2012).
Dropout has been shown effective across a wide range of machine learning tasks, from classifica-
tion (Srivastava et al., 2014; Szegedy et al., 2015) to regression (Toshev & Szegedy, 2014). Notably,
it is considered as a major component in the design of AlexNet (Krizhevsky et al., 2012), which won
the prominent ImageNet challenge in 2012 with a significant margin and helped transform the field
of computer vision.
Following the empirical success of dropout, there have been several studies in recent years that focus
on understanding theoretical underpinnings of dropout (Baldi & Sadowski, 2013; Wager et al., 2013;
McAllester, 2013; Van Erven et al., 2014; Helmbold & Long, 2015; Gal & Ghahramani, 2016;
Gao & Zhou, 2016; Mou et al., 2018; Bank & Giryes, 2018; Cavazza et al., 2018; Mianjy et al.,
2018). However, none of these works adequately address the following basic question: how does
dropout control the capacity of deep neural networks? In this paper, we provide an answer to this
question. We focus on the task of deep regression using squared `2 -loss which yields state-of-
the-art results in human pose estimation (Toshev & Szegedy, 2014), facial landmark detection, age
estimation (LathUiliere et al., 2019), and more. We give precise generalization bounds for deep
regression with dropout; our bounds leverage recent results in understanding generalization in deep
learning and help explain practice with meaningful bounds. Along the way we also recover the
state-of-the-art bounds for matrix completion. The key contributions in this paper are as follows.
1.	We introduce dropout for matrix completion, a procedure that randomly drops the columns
of the factors during training. Interestingly, this algorithmic procedure induces a data-
dependent regularizer that, in expectation, equals weighted trace-norm - a complexity mea-
sure that enjoys strong generalization guarantees (Foygel et al., 2011).
2.	For two-layer neural networks with ReLU activation, when the input distribution is sym-
metric and isotropic, we show that dropout induces the `2 path-norm which has been shown
to provide scale-sensitive generalization guarantees in deep neural networks (Neyshabur
et al., 2015).
3.	We study deep regression, where a feed-forward neural network is trained with dropout
under squared loss. With no further assumptions on the data distribution and the network
architecture, we show that the dropout regularizer is a data-dependent measure whose ex-
pected value serves as a strong complexity measure for networks trained with dropout. In
1
Under review as a conference paper at ICLR 2020
particular, we show that a network with a small dropout regularizer enjoys a small general-
ization gap.
4.	We empirically evaluate our theoretical findings for matrix completion and deep regression
on real world datasets MovieLens, Fashion MNIST and CIFAR-10.
1.1 Notation
We denote matrices, vectors, scalar variables and sets by Roman capital letters, Roman small letters,
small letters, and script letters, respectively (e.g. X, x, x, and X). For any integer d, we represent the
set {1,...,d} by [d]. For any vector X ∈ Rd, diag(x) ∈ Rd×d represents the diagonal matrix with
diagonal elements equal to x, and √x is the elementwise squared root of x. Let ∣∣xk represent the
'2-norm of vector x, and ∣∣X∣, ∣X∣f, and ∣∣X∣∣* represent the spectral norm, the Frobenius norm,
and the nuclear norm of matrix X, respectively. Similarly, given a positive definite matrix C, we
denote the Mahalonobis norm as ∣∣x∣C = x>Cx. The standard inner product is represented by(•,•),
for vectors or matrices, where hX, X0i = Tr(X>X0).
2 Matrix Sensing
We begin with understanding dropout for matrix sensing, a problem which arguably is an impor-
tant instance of a matrix learning problem with lots of applications, and is well understood from a
theoretical perspective. Here is the problem setup.
LetM* ∈ Rd2 ×d0 be a matrix with rank r* := Rank(M*). Let A(I),..., A(n) be a set of measure-
ment matrices of the same size as M. The goal of matrix sensing is to recover the matrix M* from
n observations of the form yi = hM*, A(i)i such that n d2d0. A natural approach to solve this
problem is to enforce the rank constraints implicitly using the Burer-Monteiro factorization and to
solve the following empirical risk minimization problem:
min
U∈Rd2 ×d1 ,V∈Rd0 ×d1
n
L(U, V) :=1 X(yi-hUV>, A⑺i)2.
n
i=1
(1)
When d1	r* , there exist many “bad” empirical minimizers, i.e., those with a large true risk.
However, recently, Li et al. (2018) showed that under restricted isometry property, despite the ex-
istence of such poor ERM solutions, gradient descent with proper initialization is implicitly biased
towards finding solutions with minimum nuclear norm - this is an important result which was first
conjectured and empirically verified by Gunasekar et al. (2017). However, for the most part, mod-
ern machine learning systems employ explicit regularization techniques. In fact, as we show in the
experimental section, the implicit bias due to (stochastic) gradient descent does not prevent it from
blatant overfitting in the matrix completion problem. We argue that in such cases, there is a need for
“suitable” explicit regularization, such as trace-norm Recht et al. (2010); CandeS & Recht (2009),
weighted trace-norm Foygel et al. (2011), or max norm Srebro & Shraibman (2005).
We propose solving the ERM problem (1) with algorithmic regularization due to dropout, where at
training time, columns of U and V are dropped uniformly at random. As opposed to the implicit
effect of gradient descent, this dropout heuristic explicitly regularizes the empirical objective. It is
then natural to ask, in the case of matrix sensing, if dropout also biases the ERM towards certain low
norm solutions. To answer this question, we begin with the observation that dropout can be viewed
as an instance of SGD on the following objective:
1n
LdrOP(U, V) = - ]TEb(yj - hUBV>, A(j)i)2,
n j=1
(2)
where B ∈ Rd1 ×d1 is a diagonal matrix whose diagonal elements are Bernoulli random variables
distributed as Bii 〜ɪ--pBer(1 一 p). In this case, it is easy to show that for any P ∈ [0,1):
p
LdrOP(U, V) = L(U,V) +占 R(U, V),
d1 n
R(U, V) = X - X(u>Aj%)2
i=1 n j=1
2
Under review as a conference paper at ICLR 2020
where R(U, V) is a data-dependent term that captures the explicit regularizer due to dropout (see
Proposition 1 in the Appendix). Provided that the sample size n is large enough, the explicit reg-
ularizer is well concentrated around its mean (see Lemma 2 in the Appendix). Further, given that
we seek a minima of Ldrop, it suffices to consider the factors with the minimal value of the reg-
ularizer among all that yield the same empirical loss. This motivates studying the the following
distribution-dependent induced regularizer:
Θ(M) := min R(U, V), where R(U, V) := EA [Rb(U, V)].
UV> =M
Surprisingly, for a wide range of random measurements, Θ(∙) turns out to be a “suitable” regularizer.
Here, we instantiate two important examples (see Proposition 2 in the Appendix).
Gaussian measurements. For all j ∈ [n], let A(j) be standard Gaussian matrices. Then Θ(M) =
d1 ∣∣M∣∣2 is the standard trace-norm regularization studied in Srebro et al. (2005); Bach (2008);
Candes & Tao (2009).
Matrix completion. For all j ∈ [n], let A(j) be an indicator matrix whose (i, k)-th element is
selected randomly with probability p(i)q(k), where p(i) and q(k) denote the probability of choosing
the i-th row and the j -th column, respectively. Then
Θ(M) = 1T k diag(√P)UV> diag(√q)k!
d1
is the weighted trace-norm studied by Srebro & Salakhutdinov (2010) and Foygel et al. (2011).
These observations are specifically important because they connect dropout, an algorithmic heuristic
in deep learning, to strong complexity measures that are empirically effective as well as theoretically
well understood. To illustrate, here we give a generalization bound for matrix completion with
dropout in terms of the value of the explicit regularizer at the minimum of the empirical problem.
Theorem 1.	Without loss of generality, assume that d? ≥ do and ∣∣M*k ≤ 1. Furthermore, as-
sume that mini,jp(i)q(j) ≥ ngdd). Let (U, V) be a minimizer of the dropout ERM objective
in equation (2), and assume that maxi kU(i, :)k2 ≤ γ, maxi kV(i, :)k2 ≤ γ. Let α be such that
R(U, V) ≤ α∕d1. Then, for any δ ∈ (0,1), the following generalization bounds holds with proba-
bility at least 1 - 2δ over a sample of size n:
L(U, V) ≤ L(U, V) + C(1+ Y)r ɑd2 log(d2) + C0(1+ Y2)rlog(20
n	2n
as long as n = Ω ((d1γ2∕a)2 log(2∕δ)), where C, C0 are some absolute constants.
The proof of Theorem 1 follows from standard generalization bounds for `2 loss (Mohri et al., 2018)
based on the Rademacher complexity (Bartlett & Mendelson, 2002) of the class of functions with
weighted trace-norm bounded by √α, i.e. Ma ：= {M : k diag(√p)M diag(√q)∣( ≤ α}. A bound
on the Rademacher complexity of this class was established by Foygel et al. (2011). The technicali-
ties here include showing that the explicit regularizer is well concentrated around its expected value,
as well as deriving a bound on the supremum of the predictions. A few remarks are in order.
We require that the sampling distributions be non-degenerate, as specified by the condition
mini,j p(i)q(j) ≥ n√g^). This is a natural requirement for bounding the Rademacher complexity
ofMα, as discussed in Foygel et al. (2011).
We note that for large enough sample size, Rb(U, V) ≈ R(U, V) ≈ Θ(UV>) =
d1 k diag(√p)UV> diag(√q)k2, where the second approximation is due the fact that the pair (U, V)
is a minimizer. That is, compared to the weighted trace-norm, the value of the explicit regularizer at
the minimizer roughly scales as 1/di. Hence the assumption R(U, V) ≤ α∕d1 in the statement of
the corollary.
In practice, for models that are trained with dropout, the training error L(U, V) is negligible (see
Figure 1 for experiments on the MovieLens dataset). Moreover, given that the sample size is large
3
Under review as a conference paper at ICLR 2020
enough, the third term can be made arbitrarily small. Having said that, the second term, which is
O(γ,αd2∕n), dominates the right hand side of generalization error bound in Theorem 6.
The assumption maxi kU(i, :)k2 ≤ γ, maxi kV(i, :)k2 ≤ γ is motivated by the practice of deep
learning; such max-norm constraints are typically used with dropout, where the norm of the vector
of incoming weights at each hidden unit is constrained to be bound by a constant (Srivastava et al.,
2014). In this case, if a dropout update violates this constraint, the weights of the hidden unit are
projected back to the constraint norm ball. In proofs, we need this assumption to give a concentration
bound for the empirical explicit regularizer, as well as bound the supremum deviation between the
predictions and the true values. We remark that the value of γ also determines the complexity of
the function class. On one hand, the generalization gap explicitly depends on and increases with γ.
ɪ τ	1	∙ 1	,1	∙ ,	TT XT	∙1 1	.l	^T^ /ɪ τ ɪ τ∖	1111
However, when γ is large, the constraints on U, V are milder, so that L(U, V) can be made smaller.
Finally, the required sample size heavily depends on the value of the explicit regularizer at the
optima (α∕d1), and hence, on the dropout rate p. In particular, increasing the dropout rate increases
the regularization parameter λ := ι--p, thereby intensifies the penalty due to the explicit regularizer.
Intuitively, a larger dropout rate p results in a smaller α, thereby a tighter generalization gap can be
guaranteed. We show through experiments that that is indeed the case in practice.
3 Deep neural networks
Next, we focus on neural networks with multiple hidden layers. Let X ⊆ Rd0 and Y ⊆ Rdk denote
the input and output spaces, respectively. Let D denote the joint probability distribution on X × Y .
Given n examples {(xi, yj}n=ι 〜Dn drawn i.i.d. from the joint distribution and a loss function
` : Y × Y → R, the goal of learning is to find a hypothesis fw : X → Y, parameterized by w, that
has a small population risk L(W) := ED ['(fw(x), y)].
We focus on the squared '2 loss, i.e., '(y, y0) = ∣∣y - y0k2, and study the generalization proper-
ties of the dropout algorithm for minimizing the empirical risk L(W) = 1 PnLiUM - fw(xi)∣2].
We consider the hypothesis class associated with feed-forward neural networks with k layers, i.e.,
functions of the form fw(x) = Wkσ(Wk-ισ(…W2σ(W1 x)…)),where Wi ∈ Rdi×di-1, for
i ∈ [k], is the weight matrix at i-th layer. The parameter w is the collection of weight matrices
{Wk , Wk-1 , . . . , W1 } and σ : R → R is an activation function applied entrywise to an input vector.
In modern machine learning systems, rather than talk about a certain network topology, we should
think in terms of layer topology where each layer could have different characteristics - for exam-
ple, fully connected, locally connected, or convolutional. In convolutional neural networks, it is a
common practice to apply dropout only to the fully connected layers and not to the convolutional
layers. Furthermore, in deep regression, it has been observed that applying dropout to only one of
the hidden layers is most effective (LathUiIiere et al., 2019). In our study, dropout is applied on top
of the learned representations or features, i.e. the output of the top hidden layer. In this case, dropout
updates can be viewed as stochastic gradient descent iterates on the dropout objective:
1n
Ldrop(w) ：= - EEBkyi- WkBσ(Wk-ισ(∙ ∙ ∙ W2σ(W1Xi)…))∣2 (dropout objective)
n i=1
where B is a diagonal random matrix with diagonal elements distributed identically and indepen-
dently as Bii 〜 i-pBern(1 - p), i ∈ [dk-1], for some dropout rate p. We seek to understand the
explicit regularizer due to dropout:
R(w) := Ldrop(w) - L(w)	(explicit regularizer)
We denote the output of the i-th hidden node in the j -th hidden layer on an input vector x by
ai,j(x) ∈ R; for example, a1,2(x) = σ(W2(l, )>σ(Wιx)). Similarly, the veCtOraj(x) ∈ Rdj
denotes the activation of the j -th layer on input x. Using this notation, we can conveniently rewrite
the dropout objective as LdrOP(W) := 1 Pn=ι EBkyi - WkBak-ι(xi)∣2. ItiS then easy to show that
the explicit regularizer due to dropout is given as (see Propostion 3 in the Appendix):
dk-1
R(W) = ɪ X kWk(：,j)k2 涕
1 -p j=1
where abj
1n
n X aj,k-1(Xi产
i=1
∖
4
Under review as a conference paper at ICLR 2020
The explicit regularizer R(w) is the summation over hidden nodes, of the product of the squared
norm of the outgoing weights with the empirical second moment of the output of the corresponding
neuron. For a two layer neural network with ReLU, when the input distribution is symmetric and
isotropic, the expected regularizer is equal to (see Proposition 4 in the Appendix)
1 d0 ,d1 ,d2
R(W)= E[R(w)] = 2 X W2(i2,i1)2W1(i1,i0)2,	(3)
i0,i1,i2=1
Which is precisely the squared `2 path-norm of the netWork (Neyshabur et al., 2015). We note that
such a connection has been previously established for deep linear netWorks Mianjy et al. (2018);
Mianjy & Arora (2019); here We have extended that result to single hidden layer ReLU netWorks.
Next, in order to understand the generalization properties of the dropout algorithm, We bound the
Rademacher complexity of the function class With bounded expected explicit regularizer, i.e.
dk-1
Fα := {f : x 7→ fW(x) : X kWk(:,j)k2aj2 ≤ α},
j=1
Where aj2 := Ex[baj2] = Ex[aj,k-1(x)2] is the expected squared neural activation for the j-th hid-
den node. For simplicity, We focus on netWorks With one output neuron; extension to multi-
ple output neurons is rather straightforWard. Recall that the Rademacher complexity is a sam-
ple dependent measure of the capacity of a hypothesis class that bounds the generalization gap,
i.e., the difference betWeen the empirical and true risks (Bartlett & Mendelson, 2002). Let
S = {(x1,y1),…，(xn,yn)} be a sample of size n. The empirical Rademacher complexity of
Fα With respect to S, and the expected Rademacher complexity are defined, respectively, as:
1n
RS (F) = Eσ sup — σf(xif (Xi),	Rn (Fa) = Ex Rs (Fɑ)].
f∈F n i=1
The folloWing lemma gives an upper bound on the expected Rademacher complexity of Fα, Which
only depends on α and the Width of the top layer dk-1.
Lemma 1. For any Sample size n and any a > 0 it holds that Rn(Fa) ≤ JdknIa.
We Would like to remark that the expected regularizer due to dropout, i.e. R(W), appears naturally
When bounding the Rademacher complexity of deep neural netWorks. In particular, as We shoW in
the proof of Lemma 1, the folloWing upper bound holds for any class of neural netWorks F:
Rn(F) ≤
Σ
j=1
IWk(Ij)aj | ≤ √n ∖
dk-1
dk-1 X Wk(1,j)2aj2,
j=1
Where the second inequality is due to Cauchy-SchWartz. In particular, dropout directly controls the
summation in the right hand side above. HoWever, note that the second inequality above can in gen-
eral be very loose for large Width dk-1, specifically if a small subset of hidden nodes J ⊂ [dk-1]
co-adapt in a Way that for all j ∈ [dk-1] \ J, the other hidden nodes are almost inactive, i.e.
Wk (1, j )aj ≈ 0. In this sense, dropout breaks “co-adaptation” betWeen neurons by promoting so-
lutions With (almost) equally contributing hidden neurons. For tWo-layer linear netWorks and deep
linear netWorks With one output neuron, Mianjy et al. (2018) and Mianjy & Arora (2019), respec-
tively, shoWed that the minimum of the dropout objective is only achieved by equalized netWorks,
for Which |Wk(1, j)aj | = |Wk(1,j0)aj0 |, ∀j ∈ [dk-1]. For such equalized netWorks, the second in-
equality above holds With equality, Where the value of the explicit regularizer gives a tight bound on
the Rademacher complexity. Whether such equalization property extends beyond linear activations
is an interesting open question Which We leave for future Work.
The main result of this section, is the folloWing theorem that bounds the generalization gap in net-
Works trained by dropout.
Theorem 2.	Consider deep neural netWorks With ReLU activation functions. Let W = {Wi}ik=1
be a minimizer of the dropout objective. Assume that kWk kF Qik=-11 kWik ≤ √M, and that
5
Under review as a conference paper at ICLR 2020
suPx∈x llxk ≤ √B, suPy∈γ |y| ≤ 1. Let Rb(W) ≤ α∕2 be an upper bound on the explicit regu-
larizer at w. Then, with probability at least 1 - 2δ, over a sample of size n, we have that
L(W) ≤ L(W) + C(1 +	αdk-1)
+ C 0(1 + pdk-iα)2∖ ∕log(20
2n
as long as n = Ω(B2M2 log(2∕δ)∕ɑ2), where C and C0 are absolute constants.
In practice, modern over-parameterized deep neural netWorks are usually trained to zero empirical
loss, i.e., if w is a minimizer of the dropout objective, then L(w) is negligible (see, e.g. Figure 2 for
the empirical error of convolutional neural networks trained with dropout on Fashion MNIST and
CIFAR-10). In such settings, we expect the last two terms on the right hand side to be the domi-
nant terms - this implies that the right hand side above is in the order of O(αdk-ι，log(2/6)/n).
Interestingly, the upper bound presented in Theorem 2 has no explicit dependence on the network
depth, except through the assumption that kWk∣∣f Qk-IL IWik ≤ √M, which we argue is a very
mild assumption and somewhat inevitable, as discussed in Golowich et al. (2018). In particular, if
the spectral norm of the weight matrices are concentrated around 1, then M is very small. On the
contrary, most generalization bounds on deep networks depend on the product of the stable rank of
the layers, resulting in exponential dependence on the network depth (e.g., see Bartlett et al. (2017);
Neyshabur et al. (2017) and the references therein). Furthermore, there is no explicit dependence
on the widths of the hidden layers, except through Jadk-I We note that in most state-of-the-art
models, dk-1 is not large, and as we discussed earlier, dropout implicitly equalizes the network in a
way that Podk-I ≈ Pd=I1 ∣Wk(1, j)a,j |, which does not explicitly depend on the width. Finally,
we remark that Theorem 2 is not limited to ReLU and can be extended to any Lipschitz activation
function.
3.1	Linear Regression
We recall recent results from Mianjy et al. (2018) and Mianjy & Arora (2019) that characterize
the explicit regularizer R(w) in the simpler setting of deep linear networks, where σ(∙) is the identity
map and the network computes a linear mapping fw : x → Wk …W1x. In particular, they showed
that if f is a deep linear network with one output neuron, then it holds that
νlfl2Cb=fm=ifnwRb(w),
where ν is a regularization parameter independent of the parameters w. For linear regression (i.e.,
for k = 1 and u = W1> ∈ Rd0 ), this result implies that least squares regression using dropout
amounts to solving the following regularized problem:
1n
min — ∑(yi - u>xi)2 + VkUkb
u∈Rd0 n
All the minimizers of the above problem are solutions to the following system of linear equations
(1 + ν)X>Xu = X>y, where X = [x1,…，xn]> ∈ Rn×d0, y = [yι,…，yn]> ∈ Rn×1 are the
design matrix and the response vector, respectively. Recall that if standard Tikhonov regularization
with parameter ν were used instead of dropout, then the minimizer would have been the solutions
to the system of linear equations (X>X + νI)u = X>y. The spectral augmentation due to Tikhonov
regularization is helpful because it results in a well-posed problem, even if the the original (ν = 0)
system was under-determined. On the contrary, the dropout regularizer manifests itself merely as a
scaling of the parameters, under which the problem remains ill-posed e.g. if n < d0. More impor-
tantly, Tikhonov regularization discards the directions that account for small variance in data even
when they exhibit good discriminability, which is a useful prior for learning in general. Dropout,
however, does not seem to yield a useful inductive bias for linear regression. However, in the case
of deep neural networks, as we show in the previous section, dropout does yield a useful inductive
bias. In the rest of the paper, we confirm this theoretical result with an extensive empirical study.
6
Under review as a conference paper at ICLR 2020
Figure 1: MovieLens dataset: the training error (left), the test error (middle), and the generalization gap
(right) for plain SGD and dropout with p ∈ {0.1, 0.2, 0.3, 0.4} as a function of the number of iterations. The
factorization size is d1 = 70.
width	plain SGD		P = 0.1	dropout		p = 0.4
	last iterate	best iterate		p = 0.2	p = 0.3	
di = 30	-0.8041-	-07938-	0.7805	0.785	0.7991	0.8186
di = 70	0.8315	0.7897	0.7899	0.7771	0.7763	0.7833
di = 110	0.8431	0.7873	0.7988	0.7813	0.7742	0.7743
di = 150	0.8472	0.7858	0.8042	0.7852	0.7756	0.7722
di = 190	0.8473	0.7844	0.8069	0.7879	0.7772	0.772
Table 1: MovieLens dataset: Test RMSE of plain SGD as well as the dropout algorithm with various dropout
rates for various factorization sizes. The grey cells shows the best performance(s) in each row.
4	Experimental Results
In this section, we empirically evaluate our theoretical findings on several real world datasets1. All
results are averaged over 50 independent runs with random initialization. We report experiments for
plain SGD as well as dropout with various dropout rates. For deep neural networks, the activation
function is always ReLU.
4.1	Matrix Completion
In this section, we explore the generalization properties of matrix completion with dropout algorithm
on the MovieLens dataset Harper & Konstan (2016). MovieLens is a publicly available collabora-
tive filtering dataset that contains 10M ratings for 11K movies by 72K users of the online movie
recommender service MovieLens.
We initialize the factors using standard He initialization scheme. We train the model for 100 epochs
over the training data, where we use a fixed learning rate of lr = 1, and a batch size of 2000.
In our experiments, changing the learning rate or the batch size does not significantly improve the
performance of any of these algorithms. We report the results for plain SGD (p = 0.0) as well as
the dropout algorithm with p ∈ {0.1, 0.2, 0.3, 0.4}.
Figure 1 shows the progress in terms of the training and test error as well as the gap between them
as a function of the number of iterations for d1 = 70. It can be seen that plain SGD is the fastest
in minimizing the empirical risk. The dropout rate clearly determines the trade-off between the
approximation error and the estimation error: as the dropout rate p increases, the algorithm favors
less complex solutions that suffer larger empirical error (left figure) but enjoy smaller generalization
gap (right figure). The best trade-off here seems to be achieved by a moderate dropout rate of
p = 0.3. We observe similar behaviour for different factorization sizes; please see the Appendix for
additional plots with factorization sizes d1 ∈ {30, 110, 150, 190}.
It is remarkable, how even in the “simple” problem of matrix completion, plain SGD lacks a proper
inductive bias. As it is clearly depicted in the middle plot, without explicit regularization - in
particular early stopping or dropout in this figure - SGD suffers from gross overfitting. We further
illustrate this fact in Table 1, where we compare the test root-mean-squared-error (RMSE) of plain
SGD with the dropout algorithm, for various factorization sizes. To show the superiority of dropout
over SGD with early stopping, we give SGD the advantage of having access to the test set (and not
1To ensure reproducibility of the results, we have uploaded the code.
7
Under review as a conference paper at ICLR 2020
Figure 2: Fashion MNIST (top) and CIFAR-10 (bottom): the training error, the generalization gap, and the
Rademacher complexity bound in Lemma 1 for plain SGD and dropout with p ∈ {0.25, 0.50, 0.75} as a
function of the width of the top hidden layer.
a separate validation set), and report the best iterate in the third column. Even with this impractical
privilege, dropout performs significantly better (> 0.01 difference in test RMSE).
4.2	Deep Neural Networks
In this section, we report our experimental results for training convolutional neural networks with
and without dropout, on Fashion MNIST and CIFAR-10. The Fashion MNIST dataset of Zalando’s
article images contains 60K training examples and 10K test examples each, where each example is
a 28 × 28 grayscale image, associated with a label from 10 classes. The CIFAR-10 dataset consists
of 60K 32 × 32 color images in 10 classes, with 6k images per class, divided into a training set and
a test set of sizes 50K and 10K, respectively Krizhevsky et al. (2009).
For the Fashion MNIST dataset, we use a convolutional neural network with one convolutional layer
and two fully connected layers. The convolutional layer has 16 filters, padding and stride of 2, and
kernel size of 5. We report experiments on networks with the width of the top hidden layer chosen
from width ∈ {25, 26, •一212}. In all experiments, the linear layer weights are initialized using
standard Xavier initialization, and a fixed learning rate lr = 0.5 and a mini-batch of size 256 is
used to perform the updates. We train the models for 30 epochs over the whole training set.
For CIFAR-10, we use an AlexNet (Krizhevsky et al., 2012), where the layers are modified accord-
ingly to match the dataset. The only difference here is that we apply dropout to the top hidden layer,
whereas in Krizhevsky et al. (2012), dropout is used on the second and the third hidden layers from
the top. We report experiments on networks with the width of the top hidden layer chosen from
width ∈ {25, 26, . . . , 212}. In all the experiments, an initial learning rate lr = 5 and a mini-batch
of size 256 is used to perform the updates. We train the models for 100 epochs over the whole train-
ing set. We decay the learning rate by a factor of 10 every 30 epochs. We did not observe significant
changes in the training/test errors by running the experiments longer.
Figure 2 shows the test error, training error, the generalization gap, and the Rademacher complexity
bound in Lemma 1 as a function of the number of hidden nodes in the top hidden layer. It is not
at all surprising that on all datasets, the training error of plain SGD is always the smallest, whereas
dropout always enjoys a smaller generalization gap. For Fashion MNIST, which is a fairly “simple”
dataset, there is no significant difference between the test performance of plain SGD and dropout;
dropout performs better only when the network is highly over-parameterized. However, for CIFAR-
10, dropout is always helpful in achieving better test performance. More interestingly, with dropout,
the bound on the Rademacher complexity is predictive of the generalization gap, in the sense that 1)
both are roughly in the same range (≈ 0.015), and 2) a smaller bound corresponds to a curve with
smaller generalization gap.
8
Under review as a conference paper at ICLR 2020
5	Discussion
Motivated by the success of dropout in deep learning, we propose a dropout algorithm for matrix
sensing and show that it enjoys strong generalization guarantees as well as competitive test perfor-
mance on the MovieLens dataset. We then focus on deep regression under the squared loss and show
that the regularizer due to dropout serves as a strong complexity measure for the underlying class of
deep neural networks, using which we give a generalization error bound in terms of the value of the
regularizer. We evaluate our theoretical findings for training convolutional neural networks on real
world datasets including CIFAR-10.
We leave several important questions for future work. It would be interesting to study the optimiza-
tion theoretic aspects of training with dropout. In particular, the results presented in this paper is
based on the assumption that dropout finds an (approximate) empirical minimizer; although this is
widely supported by practice, giving precise convergence rates remains an open question. Another
direction for future work is to understand the function class approximated by neural networks with
dropout layer. For example, it is known that deep ReLU networks represent the class of piecewise
linear functions (Arora et al., 2018). What can we say about the function class approximated by
neural networks with norm-bounded weight matrices, due to dropout.
References
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural
networks with rectified linear units. In International Conference on Learning Representations
(ICLR), 2018. URL https://arxiv.org/abs/1611.01491.
Francis R Bach. Consistency of trace norm minimization. Journal of Machine Learning Research,
9(Jun):1019-1048,2008.
Pierre Baldi and Peter J Sadowski. Understanding dropout. In Advances in Neural Information
Processing Systems (NIPS),pp. 2814-2822, 2013.
Dor Bank and Raja Giryes. On the relationship between dropout and equiangular tight frames. arXiv
preprint arXiv:1810.06049, 2018.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Emmanuel J Candes and Benjamin Recht. Exact matrix completion via convex optimization. Foun-
dations of Computational mathematics, 9(6):717, 2009.
Emmanuel J Candes and Terence Tao. The power of convex relaxation: Near-optimal matrix Com-
pletion. arXiv preprint arXiv:0903.1476, 2009.
Jacopo Cavazza, Benjamin D. Haeffele, Connor Lane, Pietro Morerio, Vittorio Murino, and Rene
Vidal. Dropout as a low-rank regularizer for matrix factorization. Int. Conf. on Artificial Intelli-
gence and Statistics (AISTATS), 2018.
Rina Foygel, Ohad Shamir, Nati Srebro, and Ruslan R Salakhutdinov. Learning with the weighted
trace-norm under arbitrary sampling distributions. In Advances in Neural Information Processing
Systems, pp. 2133-2141, 2011.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In Int. Conf. Machine Learning (ICML), 2016.
Wei Gao and Zhi-Hua Zhou. Dropout rademacher complexity of deep neural networks. Science
China Information Sciences, 59(7):072104, 2016.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Conference On Learning Theory, pp. 297-299, 2018.
9
Under review as a conference paper at ICLR 2020
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems,pp. 6151-6159, 2017.
F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm
transactions on interactive intelligent systems (tiis), 5(4):19, 2016.
David P Helmbold and Philip M Long. On the inductive bias of dropout. Journal of Machine
Learning Research (JMLR), 16:3403-3454, 2015.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-
nov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580, 2012.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Stephane Lathuiliere, Pablo Mesejo, Xavier Alameda-Pineda, and Radu Horaud. A comprehensive
analysis of deep regression. IEEE transactions on pattern analysis and machine intelligence,
2019.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In Conference On Learning The-
ory, pp. 2-47, 2018.
David McAllester. A pac-bayesian tutorial with a dropout bound. arXiv preprint arXiv:1307.2118,
2013.
Poorya Mianjy and Raman Arora. On dropout and nuclear norm regularization. In International
Conference on Machine Learning, 2019.
Poorya Mianjy, Raman Arora, and Rene Vidal. On the implicit bias of dropout. In International
Conference on Machine Learning, pp. 3537-3545, 2018.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
MIT press, 2018.
Wenlong Mou, Yuchen Zhou, Jun Gao, and Liwei Wang. Dropout training, data-dependent reg-
ularization, and generalization bounds. In International Conference on Machine Learning, pp.
3642-3650, 2018.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564,
2017.
Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear
matrix equations via nuclear norm minimization. SIAM review, 52(3):471-501, 2010.
Nathan Srebro and Ruslan R Salakhutdinov. Collaborative filtering in a non-uniform world: Learn-
ing with the weighted trace norm. In Advances in Neural Information Processing Systems, pp.
2056-2064, 2010.
Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In International Conference
on Computational Learning Theory, pp. 545-560. Springer, 2005.
Nathan Srebro, Jason Rennie, and Tommi S Jaakkola. Maximum-margin matrix factorization. In
Advances in Neural Information Processing Systems (NIPS), 2005.
10
Under review as a conference paper at ICLR 2020
Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research (JMLR), 15(1), 2014.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings ofthe IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
Alexander Toshev and Christian Szegedy. Deeppose: Human pose estimation via deep neural net-
works. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2014.
Tim Van Erven, Wojciech Kotlowski, and Manfred K Warmuth. Follow the leader with dropout
perturbations. In Conference on Learning Theory, pp. 949-974, 2014.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge University Press, 2018.
Stefan Wager, Sida Wang, and Percy S Liang. Dropout training as adaptive regularization. In
Advances in Neural Information Processing Systems (NIPS), 2013.
11
Under review as a conference paper at ICLR 2020
Supplementary Materials for
“Dropout: Explicit Forms and Capacity Control”
A Auxiliary Results
Theorem 3 (Hoeffding’s inequality: Theorem 2.6.2 Vershynin (2018)). Let X1, . . . , XN be inde-
pendent, mean zero, sub-Gaussian random variables. Then, for every t ≥ 0, we have
/I 1 N	∖	-	ct2N2
P(卜 XXi ≥ t∖ ≤ 2e PN=1 kXikψ2
Theorem 4 (Theorem 10.3 of Mohri et al. (2018)). Assume that kh - fk∞ ≤ M for all h ∈ H.
Then, for any δ > 0, with probability at least 1 - δ over a sample {(xi,yi), i ∈ [n]} of size n, the
following inequalities holds uniformly for all h ∈ H.
E[∣h(ɪ) — f(x)|2] ≤ 1 X |h(xi) — f(xi)|2 +4MRn(H) + M2∖Fgt26
n	2n
i=1
Theorem 5 (Theorem 3.3 in Mianjy et al. (2018)). For any pair of matrices U ∈ Rd2×d1 , V ∈
Rd0 ×d1, there exist a rotation matrix Q ∈ SO (di) such that rotated matrices U := UQ, V := VQ
satisfy ∣∣uikk力ik = dik UV> ∣∣≠,forall i ∈ [di].
Theorem 6 (Theorem 1 in Foygel et al. (2011)). Assume thatp(i)q(j) ≥ :√d) for all i ∈ [d2],j ∈
[do].	For any α >	0,	let Ma ：=	{M	∈	Rd2×d1	:	∣∣ diag(√p)Mdiag(√q)∣, ≤	α}	be the class of
linear transformations with weighted trace-norm bounded with √a. Then the expected Rademacher
complexity ofMα is bounded as follows:
αd2 log(d2) ∖
n
B Matrix Sensing
Proposition 1 (Dropout regularizer in matrix sensing). The following holds for any p ∈ [0, 1):
Lbdrop(U, V) = Lb(U, V) + λRb(U, V),
d1	n
where R(U, V) = X 1 χ(U>A jv了.
i=i n j=i
(4)
where λ = ɪ-p is the regularization parameter
Proof of Proposition 1. The following equality follows from the definition of variance:
Eb[(yi — hUBV>,A(i)i)2] = Eb[yi — hUBV>, A(i)i]2 + Var(yi — hUBV>,A(i)i)
Recall that for a Bernoulli random variable Bii, We have E[Bii] = 1 and Var(Bii) = I-P. Thus, the
first term on right hand side is equal to (yi — hUV>, A(i)i)2. For the second term we have
d1	d1
Var(yi — hUBV>, A(i)i) = Var(XBjjuj>A(i)vj) =X(uj>A(i)vj)2 Var(Bjj)
d1
=1¾ X(u>A(i%j)2
j=i
12
Under review as a conference paper at ICLR 2020
Plugging the above into Equation (5) and averaging over samples we get
1n
Ldrop(U, V) = — £Eb[(yi - hUBV>, A⑴i)2]
n i=1
n	n	-1
=1 X(yi -hUV>, A(i)i)2 + 1 X 七 X(u>A⑴Vj )2
n	n -p
i=1	i=1	j=1
p
=L(U, V)+ LR(U, V).
1	- p
which completes the proof.
Proposition 2. [Induced regularizer] The followings hold true.
□
1.	Matrix completion. Forj ∈ [n], let A(j) be an indicator matrix whose (i, k)-th element is
selected randomly with probability p(i)q(k), where p(i) and q(k) denote the probability of
choosing the i-th row and the k-th column. Then Θ(M) =	∣∣ diag(√p) UV> diag(√q)∣].
2.	i.i.d. measurements. For all j ∈ [n], let the elements of A(j) be distributed i.i.d. with zero
mean and unit variance. Then Θ(M) = -1∙ IlMIlq.
Proof of Proposition 2. For any pair of factors (U, V) it holds that
-1
R(U, V) =XE(ui>Avi)2
i=1
-1 -2 -0
= X X X p(j)q(k)(ui>ejek>vi)2
i=1 j=1 k=1
-1	-2	-0
= X X X p(j)q(k)U(j, i)2V(k, i)2
-1
=Ek diag(√P)uik2k diag(√q)vik2
i=1
-2
≥ d1 (X k diag(√P)Uikk diag(√q)vik j
-2
=d1 (X k diag(√p)uivir diag(√q)k*)
-2
≥ d1 HI diag(√p) X uiv> diag(√q)k J
=ɪ k diag(√p)UVτ diag(√q) ∣∣2
d1
where the first ineqUality is dUe to CaUchy-Schwartz and the second ineqUality follows from the
triangle ineqUality. The eqUality right after the first ineqUality follows from the fact that for any two
vectors a, b, kabτ kq = kabτ k = kakkbk. Since the ineqUalities hold for any U, V, it implies that
Θ(UVτ) ≥ :k diag(√p)UVτ diag(√q)k2.
d1
Applying Theorem 5 on (diag(√p)U, diag(√p)V), there exist a rotation matrix Q such that
k diag(√p)Uqikk diag(√q)Vqik = ɪ k diag(√p)UVτ diag(√q)k*
d1
13
Under review as a conference paper at ICLR 2020
We evaluate the expected dropout regularizer at UQ, VQ:
d1
R(UQ, VQ) = X k diag(^)Uqik2k diag(yq)Vqik2
i=1
d1
=X d Il diag(^)UV> diag(√q)k*
i=1 d1
=ɪk diag(√p)UVτ diag(√q)k2
d1
≤ Θ(UVτ)
Which completes the proof of the first part.
Similarly for the second part, we first show that any pair of factors (U, V), R(U, V) geq/ ∣∣UVτ ∣∣2:
d1
R(U, V) =XE(uiτAvi)2
i=1
d1	d2 d2
= XE(XXUjiAjkVki)2
i=1 j=1j=1
d1	d2	d0
=	UjiUj0iVkiVk0iE[AjkAj0k0]
i=1 j,j0=1 k,k0=1
d1 d2 d0
= XXX Uj2iV2kiE[Aj2k]
i=1 j=1 k=1
d1 d2 d0
= XXX Uj2iV2ki
i=1 j=1 k=1
d1
= Xkuik2kvik2
i=1
1
≥ --
d1
1
d1
1
≥ --
d1
d1
Xkuikkvik
Xd1
kUiV>k.
/
d1
hι∑ uiv>k*
2
2
where the first and the second inequaliteis are due to Cauchy-Schwartz and the triangle inequality,
respectively. The equality right after the first inequality follows because for any pair of vectors
a,b, it holds that ∣∣ab>k* = ∣∣ab>∣∣ = IlaIll∣b∣∣. NoW again using Theorem 5 on (U, V), there
exist a rotation matrix Q such that ∣UqikkVqik =看 ∣∣UV> ∣ *. We evaluate the expected dropout
regularizer at UQ, VQ:
d1	d1	1	1
R(UQ, VQ)== kUqik2kVqik2 = ∑ dd2 kUV>k* =石kUV>k* ≤ Θ(UV>)
which completes the proof of the the second part.
□
Lemma 2. Assume U, V is such that maxi kU(i, :)k2 ≤ γ, maxi kV(i, :)k2 ≤ γ. Then, with proba-
bility at least 1 - δ over a sample of size n, we have that
|R(U, V) - Rb(U, V)| ≤
Cγ2 √log(2∕δ)
14
Under review as a conference paper at ICLR 2020
ProofofLemma 2. Define X' := PW1=ι(uWA(')Vw)2 and observe that
X' = X (X Uiw Vjw A(j))
w=1 i,j
d1
=	Uiw UiOw Vjw VjOw ARAFjO
w=1 i,i0 ,j,j0
d1
=∑∑ U2w V2w Aj
w=1 i,j
d1
≤ miajxXUi2wVj2w
w=1
≤ mi,ajxkU(i,:)k2kV(j, :)k2 ≤ Y2
where the third equality follows because for an indicator matrix A('), it holds that A(,A(3, = 0 if
(i, j) = (i0,j0). Thus, Xw,' is a SUb-GaUSSian (more strongly, bounded) random variable with mean
E[X'] = R(U, V) andsub-Gaussiannorm ∣∣X'∣ψ2 ≤ γ2∕ln(2). Furthermore, ∣∣X'-R(U, V)kψ2 ≤
C0∣∣X'kψ2 ≤ Cγ2, for some absolute constants C0, C (Lemma 2.6.8 of Vershynin (2018)). Using
Theorem 3, for t = Cd1
we get that:
P	Rb(U, V) - R(U, V) ≥ t = P
which completes the proof.
1n
—VX' - R(U, V) ≥ CY2
n
'=1
Proof of Theorem 1. We use Theorem 4 to give a bound on the generalization gap. From Lemma 2,
we have with probability at least 1 - δ that
-1 k diag(√P)UV> diag(√q)k2 = Θ(UV>) ≤ R(U, V)
d1
(definition of Θ(∙))
≤ Rb(U, V) +
Cγ2 √log(2∕δ)
(Lemma 2)
n
F ≤ δ
□
CY 2 √log(2∕δ)
α
≤而+
α
≤——
d1
(assumption of the Theorem)
(whenever n ≥ 4d2γ4C2 log(2∕δ)∕α2)
Define the class of predictors with weighted trace-norm bounded by √a, i.e. Ma = {M :
k diag(√p)Mdiag(√q)k2 ≤ α}. By Theorem 6, we have that Rn(Fa) ≤ Jad2 lng(d2). It re-
mains to bound the supremum deviation between elements of M* and UV> ∈ Mα:
max |〈M* - UV>, A)| =max |〈M* - UV>, Qe>i∣
≤ max ∣M*(i,j)∣ + max ∣(UV>, ge>i∣
i,j	i,j	j
≤ ∣∣M*k + max |〈U(i,:), V(j, :))|
i,j
≤ 1 + max kU(i, :)kkV(j, :)k ≤ 1+Y
i,j
Plugging the above results in Theorem 4, we get
L(U, V) ≤ L(U, V)+ C (1+ Y)r αd2 log(d2) + C 0(1+ γ2 )J log(20
n	2n
which completes the proof.
□
15
Under review as a conference paper at ICLR 2020
C Deep Neural Networks
Proposition 3 (Dropout regularizer in deep regression).
dk-1
Lbdrop(w) = Lb(w) + Rb(w), where Rb(w) = λX kWk(:,j)k2abj2.
j=1
where bj = JnI PZi aj,k-ι(xi)2 and λ = ι-p is the regularization parameter
Proofof Proposition 3. Recall that E[Bii] = 1 and Var(Bii) = i-p. Conditioned on x, y in the
current mini-batch, we have that
dk
EB[ky - WkBak-i(x)k2] = XEB(yi - Wk(i, :)>Bak-i(x))2.	(5)
i=i
where aj (x) ∈ Rdj is the activation vector of the j-th hidden layer for input x, i.e. aj (x)[i] = ai,j (x).
The following holds by the definition of variance for each of the summands above:
Eb(yi — Wk(i, :)>Bak-i(x))2 = (Eb [yi, - Wk(i, :)>Bak-i(x)])2 + Var(yi - Wk(i, :)>Bak-i(x))
Since E[B] = I, the first term on right hand side is equal to (yi -Wk(:, i)>ak-i(x))2. For the second
term we have
Var(yi - Wk(i, :)>Bak-i (x)) = Var(Wk(i, :)>Bak-i(x))
dk-1
= Var( X Wk(i, j)Bjjaj,k-i(x))
j=i
dk-1
= X(Wk(i,j)aj,k-i(x))2 Var(Bjj)
j=i
dk-1
=ɪ X Wk(i,j)2aj,k-i(x)2
1-pj=i
Plugging the above into Equation (5)
dk-1
EB[ky -WkBak-I(X)k2] = ∣∣y -Wkak-ι(χ)k2 + ɪ-p-p X kWk(：,j)『aj,k-i(x)2
Now taking the empirical average with respect to x, y, we get
dk-1
Ldrop(w) = L(W) +	— X ||Wk(：,j)k2b2 = L(W) + R(W)
1-pj=i
which completes the proof.	□
Proposition 4. Consider a two layer neural network fW(∙) with ReLU activation functions in the
hidden layer. Furthermore, assume that the marginal input distribution PX (x) is symmetric and
isotropic, i.e., PX (x) = PX (-x) and E[xx>] = I. Then the following holds for the expected explicit
regularizer due to dropout:
λ d0 ,d1 ,d2
R( W )：= E[R( W)] = 2 X	W2(i2,i1 )2 W1 (iι,io)2,
i0,i1 ,i2=i
(6)
Proof of Proposition 4. Using Proposition 3, We have that:
d1
R(W) = E[R(w)] = λX |W2(:,j)k2E[σ(W1(j, :)>x)2]
j=1
16
Under review as a conference paper at ICLR 2020
It remains to calculate the quantity Ex [σ(W1 (j, :)>x)2]. By symmetry assumption, we have that
PX (x) = PX (-x). As a result, for any v ∈ Rd0 , we have that P(v>x) = P(-v>x) as well. That
is, the random variable zj := W1 (j, :)>x is also symmetric about the origin. It is easy to see that
Ez [σ(z)2] = 2 Ez [z2].
Z∞
σ(z)2dμ(z)
∞
=/ σ(z)2dμ(z) = /	z2dμ(z)
00
=2/ z2dμ(z) = 1 Ez [z2].
Plugging back the above identity in the expression of R(w), we get that
d1	d1
R(W) = λX kW2(:,j)k2E[(Wι(j, :)>x)2] = λX kW2(:,j)k2kW1(j, :)k2
j=1	j=1
where the second equality follows from the assumption that the distribution is isotropic.	□
Proof of Lemma 1. Given a dataset S, We start by bounding the empirical Rademahcer complexity:
1n
RS(Fa) = Eσ sup - E σifw(xi)
fw∈Fα n i=1
1 n	dk-1
=Eσ sup 一 σi	Wk(1, j)aj,k-1(xi	)
f∈Fα ni=1 j=1
dk-1	n
=Eσ SUp X Wk(1, j)aj Xσi"jk- Xi
fw∈Fα n j=1	i=1	aj
dk-1	n
≤ ʃsup X |Wk(1,j)aj∣∙ nEσ ʃsup ^ax J X 0心二1 Xi |
The first term on the right hand side is bounded as:
dk-1
X |Wk(1,j)aj∣≤ Pnt
dk-1
X Wk(1,j)2aj2
j=1
(Cauchy-Schwartz)
≤ VZdk-Ia
We now consider the function class
F = {hw ： x → σ(vσ(Wk-2σ(∙∙∙ W2σ(W1x)…)),Ex[hw (x)2] = 1}
Plugging back the above and noting that aj ≥ 0, we have
Rs(Fa) ≤	-a
n
≤ VZdk-Ia
一 n
n
Eσ sup max | fσiaj,k-i(xi)|
aj2=1 j∈[dk-1] i=1
n
Eσ sup |	σi hw (Xi) |
hw∈F i=1
Let W* be the maximizer of the right hand side, i.e.
nn
Eσ | £bihw* (xi)| = sup」fσihw* (xi)|
i=1	hw∈F i=1
17
Under review as a conference paper at ICLR 2020
Then, it holds that
Rn (Fα) = Ex [RS (Fα)]
≤ PdkTaEx,σ| X σihw* (Xi)I
n	i=1
≤ PdnTat EχEσ (X Qhw* ⑷)
=pk-a tX Exhw* my=rdk-a
n	*n
i=1
which completes the proof of the Lemma.	口
Lemma 3. Assume that ∣∣ Wk ∣∣f Qk=-IIk Wik ≤ √M, and that supX∈χ kXk ≤ √B, supy∈γ ∣y∣ ≤ L
Then with probability at least 1 - δ over a sample of size n, we have that
CBM p∕log(2∕δ)
|R(W) - R(w)∣ ≤-----Vg( / ),
n
where C is an absolute constant.
ProofofLemma 3. Define X' := Pd=^11 ∣∣Wk(：,j)k2aj,k-i(x')2 and observe that
dk-1
X' = X ∣Wk(:,j)k%k-i(x')2
j=1
dk-1
=X l∣WkC,j)k2σ (Wk-i(j, ：)ak-i(x'))2
j=1
dk-1
≤ X ∣Wk(:,j)k2 (Wk-1(j, ：)ak-i(x'))2
j=1
dk-1
≤ X ∣Wk(:,j)『kWk-i(j, :)k2kai(x')k2
j=1
dk-1
≤ ∣ak-i(x')k2 max l∣Wk-1(i, :)|2 X ∣∣Wk(:, j)∣2
j=1
≤kx'k2kW1k2 …kWk-1k2 IIWk IlF
≤ BM
where the first inequality follows from the definition of ReLU, and the second inequality is
due to Cauchy-Schwartz. Thus, Xw,' is a SUb-GaUssian (more strongly, bounded) random vari-
able with mean E[X'] = R(w) and SUb-GaUssian norm ∣∣X'kψ2 ≤ BM/ln(2). Furthermore,
∣∣X' 一 R(w)kψ2 ≤ C0∣X'kψ2 ≤ CBM, for some absolute constants C0, C (Lemma 2.6.8 of Ver-
shynin (2018)). Using Theorem 3, for t = CBM JIog” we get that:
P	Rb(w) - R(w) ≥ t =P
1n
x' 一 R(W)
n
'=1
≥ CBM
which completes the proof.
□
Proof of Theorem 2. We use Theorem 4 to give a bound on the generalization gap. From Lemma 3,
we have with probability at least 1 一 δ that
_ , ^ ,
R(w) ≤ Rb(w) +
CBM √log(2∕δ)
aa
≤ 2 + 2 = ɑ
18
Under review as a conference paper at ICLR 2020
where the second inequality holds whenever n ≥ 4C2B2M2 log(2∕δ)∕α2. Define the class of deep
neural networks with dropout regularize] bounded by √α, i.e. Fa = {fw : R(W) ≤ α}. By
Lemma 1, we have that Rn(Fa) ≤ JadnT . It remains to bound the supremum deviation between
true labels and those predicted by networks that belong to Fα :
sup ky - fw(x)k ≤ sup kyk + sup	kfw(x)k
x,y〜D, fw∈Fα	x,y~D	x,y〜D, fw∈Fα
dk-1
≤ 1 + sup k X Wk(:,j)aj,k-1(x)k
x,y 〜D,fw∈Fα j=1
dk-1
≤ 1 + sup X kWk (:, j)aj,k-1 (x)k
x,y〜D, fw∈Fα j=1
u	dk-1
≤ 1 + sup udk-1 X kWk(:, j)k2aj,k-1(x)2
x,y〜D,fw∈Fα ∖	j=1
≤ 1 + Jdk—1a
where the second inequality follows from the assumption supy∈Y |y| ≤ 1, and third and the forth
inequalities are due to the triangle inequality and Cauchy-Schwartz, respectively. Plugging the above
results in Theorem 4 and using union bound we get
L(w) ≤ L(w) + C(1 + Po砌)\斗 + C0(1 + Pk-α)2Jl0手)
n	2n
holds with probability at least 1 - 2δ, which completes the proof.	□
19
Under review as a conference paper at ICLR 2020
d1 = 30
d1 = 110
d1 = 150
d1 = 190
Figure 1: MovieLens dataset: the training error (top), the test error (middle), and the generalization gap for
plain SGD as well as dropout with p ∈ {0.25, 0.50, 0.75} as a function of the number of iterates, for different
factorization sizes d1 = 30 (first column), d1 = 110 (second column), d1 = 150 (third column), and d1 = 190
(forth column).
D	Additional Experiments
In this section, we include additional plots which was not reported in the main paper due to the space
limitations. Figure 1 in the main paper shows comparisons between plain SGD and the dropout
algorithm on the MovieLens dataset for a factorization size of d1 = 70. The observation that we
make with regard to those plots is not at all limited to the specific choice of the factorization size.
In Figure 1 here, we report similar experiments with factorization sizes d1 ∈ {30, 110, 150, 190}. It
can be seen that the overall behaviour of plain SGD and dropout are very similar in all experiments.
In particular, plain SGD always achieves the best training error but it has the largest generalization
gap. Furthermore, increasing the dropout rate increases the training error but results in a tighter
generalization gap.
It can be seen that an appropriate choice of the dropout rate always perform better than the plain
SGD in terms of the test error. For instance, a dropout rate of p = 0.2 seems to always outperform
plain SGD. Moreover, as the factorization size increases, the function class becomes more complex,
and a larger value of the dropout rate is more helpful. For example, when d1 = 30, the dropout with
rates p = 0.3, 0.4 fail to achieve a good test performance, where as for larger factorization sizes
(d1 ∈ {110, 150, 190}), they consistently outperform plain SGD as well as other dropout rates.
20