Under review as a conference paper at ICLR 2020
Policy Optimization with Stochastic Mirror
Descent
Anonymous authors
Paper under double-blind review
Ab stract
Improving sample efficiency has been a longstanding goal in reinforcement learn-
ing. In this paper, we propose the VRMPO: a sample efficient policy gradient method
with stochastic mirror descent. A novel variance reduced policy gradient estimator
is the key of VRMPO to improve sample efficiency. Our VRMPO needs only O(-3)
sample trajectories to achieve an -approximate first-order stationary point, which
matches the best-known sample complexity. We conduct extensive experiments to
show our algorithm outperforms state-of-the-art policy gradient methods in vari-
ous settings.
1	Introduction
Reinforcement learning (RL) is one of the most wonderful fields of artificial intelligence, and it has
achieved great progress recently (Mnih et al., 2015; Silver et al., 2017). To learn the optimal policy
from the delayed reward decision system is the fundamental goal of RL. Policy gradient methods
(Williams, 1992; Sutton et al., 2000) are powerful algorithms to learn the optimal policy.
Despite the successes of policy gradient method, suffering from high sample complexity is still a
critical challenge for RL. Many existing popular methods require more samples to be collected for
each step to update the parameters (Silver et al., 2014; Lillicrap et al., 2016; Schulman et al., 2015;
Mnih et al., 2016; Haarnoja et al., 2018), which partially reduces the effectiveness of the sample.
Although all the above existing methods claim it improves sample efficiency, they are all empirical
results which lack a strong theory analysis of sample complexity.
To improve sample efficiency, in this paper, we explore how to design an efficient and stable algo-
rithm with stochastic mirror descent (SMD). Due to its advantage of the simplicity of implemen-
tation, low memory requirement, and low computational complexity (Nemirovsky & Yudin, 1983;
Beck & Teboulle, 2003; Lei & Tang, 2018), SMD is one of the most widely used methods in machine
learning. However, it is not sound to apply SMD to policy optimization directly, and the challenges
are two-fold: (I) The objective of policy-based RL is a typical non-convex function, but Ghadimi
et al. (2016) show that it may cause instability and even divergence when updating the parameter of
a non-convex objective function by SMD via a single batch sample. (II) Besides, the large variance
of gradient estimator is the other bottleneck of applying SMD to policy optimization for improving
sample efficiency. In fact, in reinforcement learning, the non-stationary sampling process with the
environment leads to the large variance of existing methods on the estimate of policy gradient, which
results in poor sample efficiency (Papini et al., 2018; Liu et al., 2018).
Contributions To address the above two problems correspondingly, in this paper (I) We analyze
the theoretical dilemma of applying SMD to policy optimization. Our analysis shows that under
the common Assumption 1, for policy-based RL, designing the algorithm via SMD directly can
not guarantee the convergence. Hence, we propose the MPO algorithm with a provable convergence
guarantee. Designing an efficiently computable, and unbiased gradient estimator by averaging its
historical policy gradient is the key to MPO. (II) We propose the VRMPO: a sample efficient policy op-
timization algorithm via constructing a variance reduced policy gradient estimator. Specifically, we
propose an efficiently computable policy gradient estimator, utilizing fresh information and yielding
a more accurate estimation of the gradient w.r.t the objective, which is the key to improve sample
efficiency. We prove VRMPO needs O(-3) sample trajectories to achieve an -approximate first-
order stationary point (-FOSP) (Nesterov, 2004). To our best knowledge, our VRMPO matches the
1
Under review as a conference paper at ICLR 2020
best-known sample complexity among the existing literature. Besides, we conduct extensive ex-
periments, which further show that our algorithm outperforms state-of-the-art bandit algorithms in
various settings.
2	Background and Notations
2.1	Policy-Based Reinforcement Learning
We consider the Markov decision processes M = (S, A, P, R, ρ0 , γ), where S is state space, A is
action space; At time t, the agent is in a state St ∈ S and takes an action At ∈ A, then it receives
a feedback Rt+1; Psas0 = P(s0 |s, a) ∈ P is the probability of the state transition from s to s0 under
taking a ∈ A; The bounded reward function R : S × A → [-R, R], Rsa 7→ E[Rt+1 |St = s, At =
a]; ρo : S → [0,1] is the initial state distribution and Y ∈ (0,1) is discounted factor. Policy ∏θ(a|s)
is a probability distribution on S × A with the parameter θ ∈ Rp. Let τ = {st, at, rt+1}tH=τ0 be a
trajectory, where so 〜ρo(so), at 〜π(∙∣st),rt+ι = R(st,at), st+ι 〜P(∙∣st,at), and HT is the
finite horizon ofτ. The expected return J(πθ) is defined as:
J(θ)
def
J(πθ) =
P (T ∣θ)R(τ )dτ = ET 〜∏θ[R(t )],
(1)
where P(τ∣θ) = ρo(so) QHO P(st+ι∣st, at)∏θ(at∣st) is the probability of generating T, R(T)
PtH=τo γtrt+1 is the accumulated discounted return.
Let J (θ) = -J (θ), the central problem of policy-based RL is to solve the problem:
θ* = arg max J(θ) ^⇒ θ* = arg min J(θ).	(2)
Computing the VJ(θ) analytically, We have
Hτ
VJ(θ) = ET〜∏θ[X Vθ log∏θ(at∣st)R(τ)].	(3)
t=o
For any trajectory T, let g(τ∣θ) = PHO Vθ log∏θ(at∣st)R(τ), which is an unbiased estimator of
VJ (θ). Vanilla policy gradient (VPG) is a straightforward way to solve problem (2):
θ — θ + αg(τ ∣θ),
where α is step size.
Assumption 1. (Sutton et al., 2000; Papini et al., 2018) For each pair (s, a), any θ ∈ Rp, and all
components i, j, there exists positive constants G, F s.t.,
∂2
∣Vθi log∏θ(a|s)| ≤ G, L	log∏θ(a|s)| ≤ F.	(4)
∂θi ∂θj
According to the Lemma B.2 of (Papini et al., 2018), Assumption 1 implies VJ (θ) is L-Lipschiz,
i.e., kVJ(θ1) -VJ(θ2)k ≤ Lkθ1 - θ2k, where
L = RH (HG2 + F )/(1 - γ),	(5)
Besides, Assumption 1 implies the following property of the policy gradient estimator.
Lemma 1 (Properties of stochastic differential estimators (Shen et al., 2019)). Under Assumption
1, for any policy ∏θ and T 〜∏θ, we have
kg(τ∣θ) - VJ(θ)k2 ≤ 7GR24 =f σ2.	(6)
(1 - γ)4
2.2 Stochastic Mirror Descent
Now, we review some basic concepts of SMD; in this section, the notation follows (Nemirovski
et al., 2009). Let’s consider the stochastic optimization problem,
min {f(θ)= E[F(θ; ξ)]},	(7)
θ∈Dθ
2
Under review as a conference paper at ICLR 2020
where Dθ ∈ Rn is a nonempty convex compact set, ξ is a random vector whose probability dis-
tribution, μ is supported on Ξ ∈ Rd and F : Dθ X Ξ → R. We assume that the expectation
E[F(θ; ξ)] = Rξ F(θ; ξ)dμ(ξ) is well defined and finite-valued for every θ ∈ Dθ.
Definition 1 (Proximal Operator (Moreau, 1965)). T is a function defined on a closed convex X,
and α > 0. Mαψ,T (z) is the proximal operator of T, which is defined as:
Mψτ(Z) = argmin{T(x) + 1 Dψ(x, z)},	(8)
,	x∈X	α
where ψ(x) is a Continuously-dferentiable, Z-StrictIy ConvexfUnction:hx — y, Vψ(x) 一 Vψ(y)i ≥
Z∣∣x — yk2, Z > 0, Dψ is Bregman distance: Dψ (x, y) = ψ(x) — ψ(y) — hVψ(y), x — y), ∀ x, y ∈ X.
Stochastic Mirror Descent The SMD solves (7) by generating an iterative solution as follows,
Θt+1 = Mψt,'(θ)(θt) = arg min{hgt,θ) + αDψ(θ, θt)},	(9)
where αt > 0 is step-size, `(θ) = hgt, θi is the first-order approximation off(θ) at θt, gt = g(θt, ξt)
is stochastic subgradient such that g(θt) = E[g(θt, ξt)] ∈ ∂f (θ)∣θ=θt, {ξt}t≥o represents a draw
form distribution μ, and ∂f(θ) = {g∣f (θ) — f(ω) ≤ gτ(θ — ω),∀ω ∈ dom(f)}.
If We choose ψ(x) = ∣∣∣x∣∣2, which implies Dψ(x, y) = ∣∣∣x — y∣2, since then iteration (9) is the
proximal gradient (Rockafellar, 1976) view of SGD. Thus, SMD is a generalization of SGD.
Convergence Criteria: Bregman Gradient Bregman gradient is a generation of projected gradi-
ent (Ghadimi et al., 2016). Recently, Zhang & He (2018); Davis & Grimmer (2019) develop it to
measure the convergence of an algorithm for the non-convex optimization problem. Evaluating the
difference between each candidate solution x and its proximity is the critical idea of Bregman gra-
dient to measure the stationarity ofx. Specifically, let X be a closed convex set on Rn, α > 0, T(x)
is defined on X . The Bregman gradient of T at x ∈ X is:
Gαψ,T (x) = α-1(x — Mαψ,T (x)),	(10)
where Mψ T(∙) is defined in Eq.(8).If ψ(x) = g∣∣x∣∣2, then x* is a critical point of T if and only if
Gψτ(x*) = VT(x*) = 0 (Bauschke et al.(2011);TheOrem 27.1). Thus, Bregman gradient (10) is a
generalization of gradient. The following Remark 1 is helpful for us to understand the significance
of Bregman gradient, and it gives us some insights to understand this convergence criterion.
Remark 1. Let T be a convex function, by the Proposition 5.4.7 of Bertsekas (2009): x* is a sta-
tionarity point of T if and only if
0∈∂(T+δX)(x*),	(11)
where δχ(∙) is the indicatorfunction on X. Furthermore, suppose ψ(x) is twice continuously differ-
entiable, let X = Ma τ (x), by the definition of proximal operator Mψ T (∙), we have
0 ∈ ∂(T + δχ)(x) + (Vψ(x) — Vψ(x)) ≈) ∂(T + δχ)(x) + αGψ,T(x)V2ψ(x), (12)
Eq.(?) holds due to the first-order Taylor expansion ofVψ (x). By the criteria of (11), if Gαψ,T (x) ≈ 0,
Eq.(12) implies the origin point 0 is near the set ∂(T + δχ )(x), i.e., X is close to a stationary point.
In practice, we choose T(θ) = h—VJ(θt), θ), since then discriminant criterion (12) is suitable to
RL problem (2). For the non-convex problem (2), we are satisfied with finding an -approximate
First-Order Stationary Point (-FOSP) (Nesterov, 2004), denoted by θ, such that
∣Gαψ,T(θ) (θ)∣ ≤ .	(13)
3 Policy Optimization with Stochastic Mirror Descent
In this section, we solve the problem (2) via SMD. Firstly, we analyze the theoretical dilemma
of applying SMD directly to policy optimization. Then, we propose a convergent mirror policy
optimization algorithm (MPO).
3
Under review as a conference paper at ICLR 2020
3.1 Theoretical Dilemma
Let T = {τk }N=-o1 be a collection of trajectories, Tk 〜∏θk, We receive gradient information:
Hτk
—g(Tk∖θk) = — £▽& log∏θ(at∖st)R(τk)∖θ=θk,
t=0
then by SMD (9), to solve (2), for each 0 ≤ k ≤ N — 1, We define the update rule as folloWs,
(14)
θk+1 = Mψk,h-g(τk∣θk),θ)(θk) = argmin{h-g(Tk ∖θk),θi + 0-Dψ (θ,θk)},	(15)
Where αk > 0 is step-size and other symbols are consistent to previous paragraphs. Due to —J (θ)
is non-convex, according to (Ghadimi et al., 2016), a standard strategy for analyzing non-convex
optimization methods is to pick up the output θN randomly according to the folloWing distribution
over {1, 2,…，N}:
P (θN=θn)=∑kN=α(⅛⅛i).
(16)
where Z is defined in Definition 1, 0 < On < L ,n = 1,2,…，N.
Theorem 1. (Ghadimi et al., 2016) Under Assumption 1, and the total trajectories are{Tk}kN=1.
Consider the Sequence {θk }N=ι generated by (15), the output Θn = θn follows the probability mass
distribution of (16). Let 0 < αk < L, '(g, U) = hg, Ui, the term L and σ are defined in Eq.(5) and
Eq.(6) correspondingly. Then, we have
E[kGΨ	(θ )k2] ≤ (J W)- J (θ1)) + PN=I θk
E[kGan,'…“Wk ] ≤	PN=I(Zak - Lα2k)
(17)
where gn is short for g(τn∖θn).
Unfortunately, it is worth to notice that the lower bound of (17) reaches
(Jo - J(&))+k PjN=I ak ≥ ɑɪ
—PN=1 (Zak-La2) — ≥ ζ2,
which can not guarantee the convergence of the iteration (15), no matter how the step-size αk is
specified. Thus, under the Assumption 1, generating the solution {θk}kN=1 according to (15) and the
output following (16) lack a strong convergence guarantee.
An Open Problem The iteration (15) is a very important and general scheme that unifies many ex-
isting algorithms. For example, if the mirror map ψ(θ) = -kθk22, then the update (15) is reduced to
policy gradient algorithm (Sutton et al., 2000) which is widely used in modern RL. The update (15)
is natural policy gradient (Kakade, 2002; Peters & Schaal, 2008; Thomas et al., 2013) ifwe choose
mirror map ψ(θ) = 2θ>F(θ)θ, where F = ET〜∏θVθ log∏θ(s, a)Vθ log∏θ(s,a)>] is Fisher in-
formation matrix. If ψ is Boltzmann-Shannon entropy function (Shannon, 1948), then Dψ is known
as KL divergence and update (15) is reduced to relative entropy policy search (Peters et al., 2010;
Fox et al., 2016; Chow et al., 2018). Despite the vast body of work around above specific meth-
ods, current works are scattered and fragmented in both theoretical and empirical aspects (Agarwal
et al., 2019). Thus, it is of great significance to establish the fundamental theoretical convergence
properties of iteration (15).
Please notice that for the non-convexity of problem (2), the lower bound of (18) holds under As-
sumption 1. It is natural to ask:
What conditions guarantee the convergence of scheme (15)?
This is an open problem. Although, the iteration (15) is intuitively a convergent scheme, as dis-
cussed above that particular mirror maps ψ can lead (15) to some popular empirically effective RL
algorithms; there is still no generally complete theoretical convergence analysis of (15). Such con-
vergence properties not only help us to understand better why those methods work but also inspire
us to design novel algorithms with the principled approaches. We leave this open problem and the
related questions, e.g., how fast the iteration (15) converges to global optimality or its finite sample
analysis, as future works.
4
Under review as a conference paper at ICLR 2020
Algorithm 1 Mirror Policy Optimization Algorithm (MPO)
1: Initialize: parameter θι, SteP-Sizeak > 0, go = 0, parametric policy ∏(a|s), and map ψ.
2: for k = 1 to N do
3:	Generate a trajectory Tk = {st, at, rt+ι}= 〜∏θk, temporary variable go = 0.
Hτk	
gk — £ Vθ log∏θ(at∣st)R(τk)∣θ=θk	(21)
t=o	
11 gk J kgk + (1 - k )gk-1	(22)
θk + 1 - argmin{h-gk,ωi +	dΨ (ω, θk)}	(23)
4: end for	k	
5: Output: Θn according to (16).	
3.2 An Implementation with Convergent Guarantee
In thiS Section, we propoSe a convergent implementation defined aS followS, for each Step k:
θk+ι=Mak h-gk θi(θk)=argmin~-0k ,θi+-1 Dψ (θ,θk)},
k , gk ,	θ∈Θ	αk
where gk is an arithmetic mean of previous episodes, gradient estimate {g(τ∕θi)}k=ι:
1k
gk = k£g(Ti∖θi)
(19)
(20)
i=1
We preSent the detailS of an implementation in Algorithm 1. Notice that Eq.(22) iS an incremental
implementation of the average (20), thuS, (22) enjoyS a lower Storage coSt than (20).
For a given episode, the gradient flow (20)/(22) of MPO is slightly different from the traditional VPG,
REINFORCE (Williams, 1992), A2C (Mnih et al., 2016) or DPG (Silver et al., 2014) whose gradient
estimator follows (14) that is according to the trajectory of current episode, while our MPO uses an
arithmetic mean of previous episodes’ gradients. The estimator (14) is a natural way to estimate
the term -VJ(θt) = -E[PH=0 Nθ log ∏θ(ak ∣Sk)R(τt)], i.e. using a single current trajectory to
estimate policy gradient. Unfortunately, under Assumption 1, the result of (18) shows using (14)
with SMD lacks a guarantee of convergence. This is exactly the reason why we abandon the way
(14) and turn to propose (20)/(22) to estimate policy gradient. We provide the convergence analysis
of our scheme (20)/(22) in the next Theorem 2.
Theorem 2 (Convergence Rate of Algorithm 1). Under Assumption 1, and the total trajectories are
{τ^k}N=ι. Consider the sequence {θk }NN=ι generated by Algorithm 1, and the output Θn = θn follows
the distribution ofEq.(16). Let 0 < ak < L, '(g,u) =(g,u), the term L and σ are defined in Eq.(5)
and Eq.⑹ correspondingly. Let gk = 1 Pk=I gi, where gi = PHO Vθ logπ(at∣st)R(τi)∣θ=θi.
Then we have
E[kGψn,'(-gn,θn) (^^] ≤
(J W- J (θι))+σ2 p3 α
-PN=ι(Zαk - Lak)-
(24)
We prove the proof in Appendix A. Let ak = Z/2L, then, Eq(24) is E[∣∣Gψ 〃一θ ∖(θn)∣∣2] ≤
αn ,`(-g
n ,θn )
4L(j(θ*)-J(θι))+2σ2 PN=I ⅛
Nζ2
O(ln N/N). Our scheme of MPO partially answers the previous
open problem through conducting a new policy gradient estimator.
4 VRMPO: A Variance Reduction Implementation of MPO
In this section, we propose a variance reduction version of MPO: VRMPO. In optimization commu-
nity, variance reduction gradient estimator is a very popular method with provable convergence
5
Under review as a conference paper at ICLR 2020
Algorithm 2 Variance-Reduced Mirror Policy Optimization (VRMPO).
1:	Initialize: Policy ∏θ(a|s) With parameter θo, mirror map ψ,step-size ak > 0, epoch Size K,m.
2:	for k = 1 to K do
3：	θk,o = θk-ι, generate Tk = {τi}N=ι 〜∏θk,0
4:	θk,1 = θk,0 - akGk,0, where Gk,0 =-qNi J(θk,0) = - N P2l g(Tilθk,O).
5:	for t = 1 to m - 1 do
6:	Generate {Tj }N21 〜∏θk,t
Gk,t = Gk,t-1 + y-Γ X(Ig(Tj lθk,t) + g(Tj lθk,t-I)),	(25)
N2 j=1
θk,t+1 = arg min{hGk,t, ωi + — Dψ (ω, θk,t)}	(26)
7:	end for
8:	θk = θk,t with t chosen uniformly randomly from {0, 1, ..., m}.
9:	end for
10:	Output: θκ.
guarantee (Reddi et al., 2016; Fang et al., 2018; Horvath & Richtarik, 2019). Inspired by the above
works, now, we present an efficiently computable policy gradient estimator. For any initial θ0, let
{τ0}N=ι 〜∏θo, we calculate the initial gradient estimate as follows,
G0
j =1
(27)
Let θ1 = θ0 - αG0, for each time t ∈ N+, let {Tjt}jN=1 be the trajectories generated by πθt, we
define the policy gradient estimate Gt and the update rule of parameter as follows,
1N
Gt = Gt-I + N ɪ2(-g(τtlθt) + g(TtlθtT)),	(28)
N j =1
Θt+1 = argmin{hGt,θi + 1 Dψ(θ, θt)},	(29)
where α > 0 is step-size. We present more details in Algorithm 2.
In (28), -g(τt∣θt) and g(τt∣θt-ι) share the same trajectory {τt}N=ι, which plays a critical role
in reducing the variance of gradient estimate (Shen et al., 2019). Besides, it is different from
(20), we admit a simple recursive formulation to conduct the policy gradient estimator Gt (28),
which captures some techniques from SARAH (Nguyen et al., 2017a;b). At each time t, the term
nN PjN=ι ( - g(τt∣θt) + g(τt∣θt-ι)) can be seen as an additional “noise” for the policy gradient es-
timate. A lot of practices show that conducting a gradient estimator with the additional noise enjoys
a lower variance and speeding up the convergence (Reddi et al., 2016; Schmidt et al., 2017; Nguyen
et al., 2017a;b; Fang et al., 2018).
Theorem 3 (Convergence Analysis of Algorithm 2). The Sequence {石k}3ι is generated accord-
ing to Algorithm 2. Under Assumption 1, let Z > 备,for any positive scalar G the batch size of
the trajectories of outer loop N = (	+ ?«-亘)(1 + 炳±))忘,the iteration times of in-
- ~ ,, ..
8L(E[J(θo)] -J(θ*))
(m - I)(Z - 352 )
〜
point θK achieves
ner loop m - 1 = N2
, and step size αk
1
2(Z-32)
the iteration times of outer loop
4L. Then, Algorithm 2 outputs the
E[%-M)”K)k] ≤ e.
(30)
K
6
Under review as a conference paper at ICLR 2020
By the result in Theorem 3, under Assumption 1, to achieve the -FOSP, Algorithm 2 (VRMPO) needs
K (NI+(m - 1)N2)=8LEJZ03-J"(1+16ζ2 )(1+q w2+2ζ-p(1+3⅛))σ)表=
O(表) random trajectories. As far as We know, our VRMPO matches the best-known sample ComPlex-
ity as the HAPG algorithm (Shen et al., 2019).
In fact, according to (Shen et al., 2019), REINFORCE needs O(4) random trajectory trajectories
to achieve the -FOSP, and no provable improvement on its complexity has been made so far. The
same order of sample complexity of REINFORCE is shown by Xu et al. (2019). With the additional
assumptions Var [ QHH=。开诙；。%：%) ] ,Var[g(τ∣θ)] < +∞, Papini et al. (2018) show that the SVRPG
achieves the sample complexity of O(-4). Later, under the same assumption as (Papini et al., 2018),
XU et al. (2019) reduce the sample complexity of SVRPG to O(E-1≡0). We provide more details of the
comparison in Table 1, from which it is easy to see that our VRMPO matches the best-known sample
complexity with least conditions.
5 Related Works
Stochastic Variance Reduced Gradient in RL Although it has achieved considerable successes in
supervised learning, stochastic variance reduced gradient optimization is rarely a matter of choice
in RL. To our best knowledge, Du et al. (2017) firstly introduce SVRG (Johnson & Zhang, 2013)
to off-policy evaluation. Du et al. (2017) transform the empirical policy evaluation problem into a
(quadratic) convex-concave saddle-point problem, then they solve the problem via SVRG straightfor-
wardly. Later, to improve sample efficiency for complex RL, Xu et al. (2017) combine SVRG with
TRPO (Schulman et al., 2015). Similarly, Yuan et al. (2019) introduce SARAH (Nguyen et al., 2017a)
to TRPO to improve sample efficiency. However, the results presented by Xu et al. (2017) and Yuan
et al. (2019) are empirical, which lacks a strong theory analysis. Metelli et al. (2018) present a sur-
rogate objective function with a Renyi divergence (Renyi et al., 1961) to reduce the variance caused
by importance sampling.
Recently, Papini et al. (2018) propose a stochastic variance reduced version of policy gradient
(SVRPG), and they define the gradient estimator via important sampling,
1N	H
Gt = Get-ι + N X (- g(τt∣θt) + Y
j=1
h=0
；g"T）），
(31)
where Gt-1 is an unbiased estimator according to the trajectory generated by πθt-1 . Although the
above algorithms are practical empirically, their gradient estimates are dependent heavily on impor-
tant sampling. This fact partially reduces the effectiveness of variance reduction. Later, Shen et al.
(2019) remove the important sampling term, and they construct the gradient estimator as follows,
1N
Gt = Get-ι + N (- g]τj ∣θt) + g(Tt∣θt-ι))∙	(32)
j=1
It is different from (Du et al., 2017; Xu et al., 2017; Papini et al., 2018; Shen et al., 2019), the
proposed VRMPO admits a stochastic recursive iteration to estimate the policy gradient, see Eq.(28).
Our VRMPO exploits the fresh information to improve convergence and reduce variance. Besides,
VRMPO reduces the storage cost significantly due to it doesn’t require to store the complete historical
information. We provide more details of the comparison in Table 1.
Baseline Methods for Variance Reduction of Policy Gradient Baseline (also also known as con-
trol variates (Cheng et al., 2019a) or reward reshaping (Ng et al., 1999; Jie & Abbeel, 2010)) is a
widely used technique to reduce the variance (Weaver & Tao, 2001; Greensmith et al., 2004). For
example, A2C (Sutton & Barto, 1998; Mnih et al., 2016) introduces the value function as baseline
function, Wu et al. (2018) consider action-dependent baseline, and Liu et al. (2018) use the Stein’s
identity (Stein, 1986) as baseline. Q-Prop (Gu et al., 2017) makes use of both the linear dependent
baseline and GAE (Schulman et al., 2016) to reduce variance. Cheng et al. (2019b) present a predictor-
corrector framework that can transform a first-order model-free algorithm into a new hybrid method
that leverages predictive models to accelerate policy learning. Mao et al. (2019) derive a bias-free,
7
Under review as a conference paper at ICLR 2020
Algorithm
Estimator Conditions
Complexity
VPG/REINFORCE	Eq.(14)
SVRPG (Papini et al., 2018)	Eq.(31)
SVRPG (Xu et al., 2019)	Eq.(31)
HAPG (Shen et al., 2019)	Eq.(32)
VRMPO (Our Works)	Eq.(28)
Assumption 1;Var[g(T∣θ)] < +∞
Assumption 1;Var[ρt],Var[g(τ∣θ)] < +∞
Assumption 1;Var[ρt],Var[g(τ∣θ)] < +∞
Assumption 1
Assumption 1
44 133
-------
ee eee
(( (((
OOOOO
Table 1: Comparison on complexity required to achieve ∣∣VJ(θ)k ≤ e. Particularly, if ψ(θ)
1 ∣∣θk2, then the result (30) of our VRMPO is measured by gradient. Beside, ρt =ef QH O πθ0 (ahjh)
2	h=0 πθt (ah |sh)
input-dependent baseline to reduce variance, and analytically show its benefits over state-dependent
baselines. Recently, Grathwohl et al. (2018); Cheng et al. (2019a) provide a standard explanation for
the benefits of such approaches with baseline function.
However, the capacity of all the above methods is limited by their choice of baseline function (Liu
et al., 2018). In practice, it is troublesome to design a proper baseline function to reduce the variance
of policy gradient estimate. Our VRMPO avoids the selection of baseline function, and it uses a current
sample trajectory to construct a novel, efficiently computable gradient estimator to reduce variance
and speed convergence.
6 Experiments
6.1 Numerical Analysis of MPO
In this section, we use an experiment to demonstrate MPO converges faster than VPG/REINFORCE.
Then, we test how the mirror map ψ effects the performance of MPO.
Performance Comparison We compare the convergence rate of MPO with REINFORCE and VPG em-
pirically on the Short Corridor with Switched Actions domain (Chapter 13, Sutton & Barto (2018);
We provide some details in Appendix B). The task is to estimate the value function of state s1,
V (s1) = G0 ≈ -11.6.
OS ztupo-dtuUo p.lroMtu-βoh-
Episode
Figure 1: Comparison the performance of MPO with
REINFORCE and VPG on the short-corridor grid
world domain.
In this experiment, we use features φ(s, right) =
[1, 0]> and φ(s, left) =	[0, 1]>, s ∈ S.
Let Lθ (s, a) =	φ>(s, a)θ, (s, a) ∈ S ×
A, where A =	{right, left}. πθ (a|s) is
a exponential soft-max distribution defined as
exp{Lθ(s, a)}
πθ(a|S)= P ∈a exp{Lθ (s,a')} . Theinitialpa-
rameter θ0 = U [-0.5, 0.5], where U is uniform
distribution.
Before we report the experimental results, it is necessary to explain why we only use VPG and
REINFORCE as the baseline to compare with our MPO. VPG/REINFORCE is one of the most basic
policy gradient methods in RL, and extensive modern policy-based algorithms are derived from
VPG/REINFORCE. Our MPO is a novel framework via mirror map to learn the parameter, see (23).
Thus, it is natural to compare with VPG and REINFORCE. The result in Figure 1 shows that MPO
converges faster significantly and achieves a better performance than both REINFORCE and MPO.
Effect of Mirror Map ψ We use ψ = `p -norm to test how the mirror map affects the performance of
MPO. Particularly, the iteration (23) reduces to gradient descent update if ψ = '2-norm. For ψ = 'p-
norm, Eq.(23) has a closed implementation. Let ψ* (y) = (Pn=1 |yi ∣q)1 be the conjugate map of ψ,
8
Under review as a conference paper at ICLR 2020

-3000
-4000
-5000
-608
2QQ
best
ι ψ = ∣2~∏orm
---- ψ = iɜ-norm
---- ψ = ∕4-n0rm
---- ψ = is-norm
2∞	300	400	500	600
Episode
(b) Acrobot-v1
300	400	500	600	700	800	800	1000
Episode
best
ψ = f2-∏0rm
----ψ = f3-n0rm
----ψ = ∕4-n0rm
----ψ = f5-n0rm
best
---- ψ = ∕2-∞∣,m
---- ψ = ∣3-norm
ψ =∕4-n0rm
---- W = £5-g*m
300	400	500
Episode
(C) CartPole-VI
(a) MountainCar-v1

Figure 2: Comparison of the empirical performance of MPO between non-Euclidean distance (p 6= 2)
and EuClidean distanCe (p = 2) on standard domains: MountainCar, ACrobot and CartPole.
where p-1 + q-1 = 1, p, q > 1. ACCording to (BeCk & Teboulle, 2003), (23) is equiValent to
θk+ι = Vψ*(Vψ(θk) + αkgk),
where Vψj (x) and Vψ*(y) are p-norm link functions (Gentile, 2003): Vψj (x)	=
Sign(Xj)P-2' —, Vψj(y) = sign(yj)q-2q -, and j is coordinate index of the vector Vψ, Vψ*.
kxkp	j	kykq
To compare fairly, we use the same random seed, and let p run in [P] =
{1.1,1.2,…，1.9,2,3,4, 5}. For the non-Euclidean distance case, We only show P = 3,4, 5,
and “best”, where p = “best” value is that case it achieves the best performance among the set [P].
For the limitation of space, we provide more details of experiments in Appendix D.1.
The result in Figure 2 shows that the best method is produced by non-Euclidean distance, not the
Euclidean distance. The traditional policy gradient methods such as REINFORCE, VPG, and DPG are
all the algorithms update parameters in Euclidean distance. This simple experiment gives us some
lights that one can create better algorithms by combining the existing approaches with non-Euclidean
distance, which is an interesting direction, and we left it as future work.
6.2 Evaluate VRMPO on Continuous Control Tasks
In this section, we compare VRMPO on the MuJoCo continuous control tasks (Todorov et al., 2012)
from OpenAI Gym (Brockman et al., 2016). We compare VRMPO with DDPG (Lillicrap et al., 2016),
PPO (Schulman et al., 2017), TRPO (Schulman et al., 2015), and TD3 (Fujimoto et al., 2018). For
fairness, all the setups mentioned above share the same network architecture that computes the
policy and state value. We run all the algorithms with ten random seeds. The results of max-average
epoch return are present in Table 2, and return curves are shown in Figure 3. For the limitation of
space, we present all the details of experiments and some practical tricks for the implementation of
VRMPO in Appendix D.2-D.5; in this section, we only offer our experimental results. We evaluate the
performance of VRMPO by the following three aspects: score performance, the stability of training,
and variance.
Score Performance Comparison From the results of Figure 3 and Table 2, overall, VRMPO outper-
forms the baseline algorithms in both final performance and learning process. Our VRMPO also learns
considerably faster with better performance than the popular TD3 on Walker2d-v2, HalfCheetah-v2,
Hopper-v2, InvDoublePendulum-v2, and Reacher-v2 domains. On the InvDoublePendulum-v2 task,
our VRMPO has only a small advantage over other algorithms. This is because the InvPendulum-v2
task is relatively easy. The advantage of our VRMPO becomes more powerful when the task is more
difficult. It is worth noticing that on the HalfCheetah-v2 domain, our VRMPO achieves a significant
max-average score 16000+, which outperforms far more than the second-best score 11781.
Stability The stability of an algorithm is also an important topic in RL. Although DDPG exploits
the off-policy sample, which promotes its efficiency in stable environments, DDPG is unstable on the
Reacher-v2 task, while our VRMPO learning faster significantly with lower variance. DDPG fails to
make any progress on InvDoublePendulum-v2 domain, and the result is corroborated by the work
(Dai et al., 2018). Although TD3 takes the minimum value between a pair of critics to limit overes-
9
Under review as a conference paper at ICLR 2020
Figure 3: Learning curves for continuous control tasks. The shaded region represents the standard
deviation of the score over the best three trials. Curves are smoothed uniformly for visual clarity.
Environment	Our VRMPO	TD3	DDPG	PPO	TRPO
Walker2d-v2	5251.83	4887.85	5795.13	3905.99	3636.59
HalfCheetah-v2	16095.51	11781.07	8616.29	3542.60	3325.23
Reacher-v2	-0.49	-1.47	-1.55	-0.44	-0.66
Hopper-v2	3751.43	3482.06	3558.69	3609.65	3578.06
InvDoublePendulum-v2	9359.82	9248.27	6958.42	9045.86	9151.56
InvPendulum-v2	1000.00	1000.00	907.81	1000.00	1000.00
Table 2: Max-average return over 500 epochs, where we run 5000 iterations for each epoch. Maxi-
mum value for each task is bolded.
timation, it learns severely fluctuating in the InvertedDoublePendulum-v2 environment. In contrast,
our VRMPO is consistently reliable and effective in different tasks.
Variance Comparison As we can see from the results in Figure 3, our VRMPO converges with a
considerably low variance in the Hopper-v2, InvDoublePendulum-v2, and Reacher-v2. Although
the asymptotic variance of VRMPO is slightly larger than other algorithms in HalfCheetah-v2, the
final performance of VRMPO outperforms all the baselines significantly. The result in Figure 3 also
implies conducting a proper gradient estimator not only reduce the variance of the score during the
learning but speed the convergence of training.
7 Conclusion
In this paper, we propose the mirror policy optimization (MPO) by estimating the policy gradient via
dynamic batch-size of historical gradient information. Results show that making use of historical
gradients to estimate policy gradient is more effective to speed convergence. We also propose a
variance reduction implementation for MPO: VRMPO, and prove the complexity of VRMPO achieves
O(-3). To our best knowledge, VRMPO matches the best-known sample complexity so far. Finally,
we evaluate the performance of VRMPO on the MuJoCo continuous control tasks, results show that
VRMPO outperforms or matches several state-of-art algorithms DDPG, TRPO, PPO, and TD3.
10
Under review as a conference paper at ICLR 2020
References
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation
with policy gradient methods in markov decision processes. arXiv preprint arXiv:1908.00261,
2019.
Navid Azizan and Babak Hassibi. Stochastic gradient/mirror descent: Minimax optimality and im-
plicit regularization. International Conference on Learning Representations, 2019.
Heinz H Bauschke, Patrick L Combettes, et al. Convex analysis and monotone operator theory in
Hilbert spaces, volume 408. Springer, 2011.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters, 31(3):167-175, 2003.
Dimitri P Bertsekas. Convex optimization theory. Athena Scientific Belmont, 2009.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Ching-An Cheng, Xinyan Yan, Nolan Wagener, and Byron Boots. Fast policy learning through
imitation and reinforcement. In Conference on Uncertainty in Artificial Intelligence, 2018.
Ching-An Cheng, Xinyan Yan, and Byron Boots. Trajectory-wise control variates for variance re-
duction in policy gradient methods. arXiv preprint arXiv:1908.03263, 2019a.
Ching-An Cheng, Xinyan Yan, Nathan Ratliff, and Byron Boots. Predictor-corrector policy opti-
mization. In Proceedings of International Conference on Machine Learning, 2019b.
Yinlam Chow, Ofir Nachum, and Mohammad Ghavamzadeh. Path consistency learning in tsallis
entropy regularized mdps. In International Conference on Machine Learning, pp. 978-987, 2018.
Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Sbeed:
Convergent reinforcement learning with nonlinear function approximation. International Confer-
ence on Machine Learning, 2018.
Damek Davis and Benjamin Grimmer. Proximally guided stochastic subgradient method for nons-
mooth, nonconvex problems. SIAM Journal on Optimization, 29(3):1908-1930, 2019.
Simon S Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance reduction
methods for policy evaluation. In International Conference on Machine Learning, 2017.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex op-
timization via stochastic path-integrated differential estimator. In Advances in Neural Information
Processing Systems, pp. 686-696, 2018.
Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft
updates. In Conference on Uncertainty in Artificial Intelligence, 2016.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. International Conference on Machine Learning, 2018.
Claudio Gentile. The robustness of the p-norm algorithms. Machine Learning, 53(3):265-299,
2003.
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation meth-
ods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):
267-305, 2016.
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey Roeder, and David Duvenaud. Backpropagation
through the void: Optimizing control variates for black-box gradient estimation. International
Conference on Learning Representations, 2018.
Evan Greensmith, Peter L Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient
estimates in reinforcement learning. Journal of Machine Learning Research, 5(Nov):1471-1530,
2004.
11
Under review as a conference paper at ICLR 2020
Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, and Sergey Levine. Q-prop:
Sample-efficient policy gradient with an off-policy critic. International Conference on Learning
Representation, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning, 2018.
Samuel Horvath and Peter Richtarik. Nonconvex variance reduced optimization with arbitrary Sam-
pling. International Conference on Machine Learning, 2019.
Tang Jie and Pieter Abbeel. On a connection between importance sampling and the likelihood ratio
Policy gradient. In Advances in Neural Information Processing Systems, pp. 1000-1008, 2010.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems, pp. 315-323, 2013.
Sham M Kakade. A natural policy gradient. In Advances in Neural Information Processing Systems,
pp. 1531-1538, 2002.
Yunwen Lei and Ke Tang. Stochastic composite mirror descent: optimal bounds with high probabil-
ities. In Advances in Neural Information Processing Systems, pp. 1519-1529, 2018.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. ICLR,
2016.
Hao Liu, Yihao Feng, Yi Mao, Dengyong Zhou, Jian Peng, and Qiang Liu. Action-dependent con-
trol variates for policy optimization via stein identity. International Conference on Learning
Representation, 2018.
Hongzi Mao, Shaileshh Bojja Venkatakrishnan, Malte Schwarzkopf, and Mohammad Alizadeh.
Variance reduction for reinforcement learning in input-driven environments. International Con-
ference on Learning Representations, 2019.
Alberto Maria Metelli, Matteo Papini, Francesco Faccio, and Marcello Restelli. Policy optimization
via importance sampling. In Advances in Neural Information Processing Systems, pp. 5442-5454,
2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning, pp. 1928-1937, 2016.
Jean-JacqUes Moreau. Proximite et dualite dans Un espace hilbertien. Bull. Soc. Math. France, 93
(2):273-299, 1965.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574-
1609, 2009.
Arkadii Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method
efficiency in optimization. 1983.
Y Nesterov. Introductory lectures on convex optimization: a basic course. Kluwer Academic Pub-
lishers, Dordrecht, 2004.
Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In International Conference on Machine Learning,
volume 99, pp. 278-287, 1999.
12
Under review as a conference paper at ICLR 2020
Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. Sarah: A novel method for machine
learning problems using stochastic recursive gradient. In International Conference on Machine
Learning, 2017a.
Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. Stochastic recursive gradient algo-
rithm for nonconvex optimization. arXiv preprint arXiv:1705.07261, 2017b.
Matteo Papini, Giuseppe Canonaco Damiano Binaghi, and Marcello Restelli Matteo Pirotta.
Stochastic variance-reduced policy gradient. In International Conference on Machine Learning,
2018.
Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing,71(7-9):1180-1190, 2008.
Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In AAAI, pp.
1607-1612, 2010.
Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In International Conference on Machine Learning, pp.
314-323, 2016.
Alfred Renyi et al. On measures of entropy and information. In Proceedings ofthe Fourth Berkeley
Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of
Statistics, 1961.
R Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM journal on
control and optimization, 14(5):877-898, 1976.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83-112, 2017.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. International Confer-
ence on Learning Representations, 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Claude Elwood Shannon. A mathematical theory of communication. Bell system technical journal,
27(3):379-423, 1948.
Zebang Shen, Alejandro Ribeiro, Hamed Hassani, Hui Qian, and Chao Mi. Hessian aided policy
gradient. In International Conference on Machine Learning, pp. 5729-5738, 2019.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International Conference on Machine Learning,
2014.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 550(7676):354, 2017.
Charles Stein. Approximate computation of expectations. Lecture Notes-Monograph Series, 7:
i-164, 1986.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 1998.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in Neural Infor-
mation Processing Systems, pp. 1057-1063, 2000.
13
Under review as a conference paper at ICLR 2020
Philip S Thomas, William C Dabney, Stephen Giguere, and Sridhar Mahadevan. Projected natural
actor-critic. In Advances in neural information processing Systems, pp. 2337-2345, 2013.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.
Lex Weaver and Nigel Tao. The optimal reward baseline for gradient-based reinforcement learning.
In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, pp. 538-
545. Morgan Kaufmann Publishers Inc., 2001.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre M Bayen, Sham Kakade,
Igor Mordatch, and Pieter Abbeel. Variance reduction for policy gradient with action-dependent
factorized baselines. International Conference on Learning Representation, 2018.
Pan Xu, Felicia Gao, and Quanquan Gu. An improved convergence analysis of stochastic variance-
reduced policy gradient. Conference on Uncertainty in Artificial Intelligence, 2019.
Tianbing Xu, Qiang Liu, and Jian Peng. Stochastic variance reduction for policy gradient estimation.
arXiv preprint arXiv:1710.06034, 2017.
Huizhuo Yuan, Chris Junchi Li, Yuhao Tang, and Yuren Zhou. Policy optimization via stochastic
recursive gradient algorithm. 2019. https://openreview.net/forum?id=rJl3S2A9t7.
Siqi Zhang and Niao He. On the convergence rate of stochastic mirror descent for nonsmooth
nonconvex optimization. arXiv preprint arXiv:1806.04781, 2018.
14
Under review as a conference paper at ICLR 2020
A Proof of Theorem 2
Let f(x) be a L-Smooth function defined on Rn, i.e ∣∣Vf(x) - Vf(y)k ≤ Lkx - y∣∣. Then, for
∀x, y ∈Rn, the following holds
kf(χ) - f(y4 - gf(y，x -yik ≤ Lkx - yk2.	(33)
The following Lemma 2 is useful for our proof.
Lemma 2 (Ghadimi et al. (2016), Lemma 1 and Proposition 1). Let X be a closed convex set in
Rd, h : X → R be a convex function, but possibly nonsmooth, and Dψ : X × X → R is Bregman
divergence. Moreover, define
x+ = arg min j (g, Ui + ^Dψ (u, x) + h(u) I
u∈X	η
Pχ (x,g,η) = 1(x - x+),	(34)
where g ∈ Rd, x ∈ X, and η > 0. Then, the following statement holds
hg,Pχ(x,g,η)i ≥ ZIIPX(x,g,η)k2 + 1[h(x+) - h(X)],	(35)
η
where ζ is a positive constant determined by ψ (i.e. ψ is a a continuously-differentiable and ζ-
strictly convex function) that satisfying hx - y, Vψ(x) - Vψ(y)i ≥ ζkx - yk2. Moreover, for any
g1 , g2 ∈ Rd, the following statement holds
kPχ(x,gι,η) - Pχ(x, g2,η)k ≤ 1 l∣gι - g2∣l∙	(36)
Theorem 2 (Convergence Rate of Algorithm 1) Under Assumption 1, and the total trajectories are
{τ^k}N=ι. Consider the sequence {θk }NN=ι generated by Algorithm 1, and the output Θn = θn follows
the distribution ofEq.(16). Let 0 < αk < L, '(g, u) =(g,u), the term L and σ are defined in Eq.(5)
and Eq.(6) correspondingly. Let gk = 1 Pk=I gi, where gi = PHO V log∏(at∣st)R(τi)∣θ=θi.
Then we have
E[kGψn,'(-gn,θn) (^kH ≤
(J W- J (θι))+σ2 pk=ι 赞
-PN=ι(Zɑfc - Lɑk)-
Proof. (Proof of Theorem 2)
Let T = {τk }kN=1 be the trjecories generated by the differentiable parametric policy πθ. At each
terminal end ofa trajectory τk = {st, at, rt+1}tH=τ0k ∈ T, let
Hτk	1 k
gk =	Vθ log∏θ(at∣st)R(τk)∣θ=θk, gk = k Egi,
t=0	i=1
according to Algorithm 1, at the terminal end of k-th episode, k = 1, 2,…，N, the following holds,
Θk+1 = arg min {-9k 阳 + ɪ Dψ (θ,θk)} = Mψk,'(-gk,θ)(θk).
To simplify expression, let J (θ) = -J (θ), then J (θ) is L-smooth, from Eq.(33), we have
J (θk+ι) ≤ J (θk) + Dvθ J (θ) ∣θ=θfc, θk+ι- θk) + 2 ∣∣θk+ι- θk Ii
Lα2	2
=J(θk) - αk(VJ(θk), Gψk,'(—gk,θk)(θk)) + -2k∣∣Gψk,'(-gk,θk)(θk)∣∣
=J (θk-k D^k,索上,'(-0%瓜)(% )〉+Lαk∣∣Gak,'(-gk,θk)(θk)∣∣∣2
+ αk (kk, Gψk ,'(-gk ,θk)(θk)),
15
Under review as a conference paper at ICLR 2020
where Ek = -gk - (-VJ(θk)) = -gk - VJ(θk). By Eq.(34) and let h(x) ≡ 0 and η = α, then
PX(θ, g, α) = Gψ'(g θ)(θ). Furthermore, by Eq.(35), let η = αk and g = -gk, then We have
22	2
J (θk+ι) ≤ J (θk) - αk ζ∣∣G"(-gk,θk)(θk)∣∣ + -≠∣∣Gψk ,'(-gk ,θk)(θk )∣∣
+αk DEk, GΨk,'(-gk,θk)(θk)E
2	Lα2	2
=J (θk) - αk ζ∣∣G"(-gk,θk)(θk)∣∣ + -≠∣∣Gψk ,'(-gk ,θk)(θk )∣∣
+ αk (Ek, GΨk ,'(-VJ (θk),θk )(θk))
+αk DEk, Gak ,'(-gk ,θk)(θk) - GΨk,'(-VJ(θk),θk)(θk)).	(37)
Rearrange Eq.(37), we have
Lα2	2
J (θk+1) ≤ J (θk)-(Zak- ɪ)||Gak,'(-gk,θk)(θk)∣∣ +αk {kk, Gak,'(-vj(θk),θk)(θk)>
+αkkEkkllGz,`(-gk ,θk)(θk)- Gψk ,'(-VJ (θk),θk)(θk )∣∣.
By Eq.(36), let X = θk,gι = -gk,g2 = -VJ(θk), h(x) ≡ 0, then the following statement holds
J(θk+1)
≤ J (θk)- Qk- LOk)11嗫'(-0%凡)(纵 )∣∣ +αk DEk, Gψk,'(-vj (θk ),θk)(θk)E+Ok kEk k2.
(38)
ζ
Summing the above Eq.(38) from k = 1 to N and with the condition Ok ≤ —, we have the following
statement
N2
X (Zak-Lak)∣∣Gψk,'(-gk,θk)(θk)∣∣
k=1
≤ X(Zak- Laak- )∣∣Gψk,'(-gk,θk)(θk )∣∣
k=1
N
≤ X [ak (Ek，Gψk,'(-VJ(θk),θk)(θk))+ ~7~ kEk 112] + J(θ1) - J(θk+1)
k=1
N
≤ X [ak (Ek，Gψk,'(-VJ(θk),θk)(θk)) + -7^ kEk 112] + J(θ1) - J*.	(39)
k=1	Z
Recall
Hτk	1 k
gk =	Vθ log ∏θ(at∣St)R(τk),^k = - V^gi,
k
t=0	i=1
by policy gradient theorem
E[-gk] = E[-0k] = -VJ (θk) = vj (θk).	(4O)
Let Fk be the σ-field generated by all random variables defined before round k, Ek = gk - VJ(θk)
then the Eq.(40) implies: for k = 1,…，N,
EhDEk,Gaψk,'(-VJ(θk),θk)(θk)EFk-1i = EhDEEk, Gaψk,'(-VJ(θk),θk)(θk)EFk-1i = 0.
Let δs = PS=ι Et, noticing that for S = 1,…，k,
E[hδs,Es+ιi∣δs]=0.	(41)
16
Under review as a conference paper at ICLR 2020
Furthermore, the following statement holds
E[kδkk2] = E[kδk-ιk2 +2〈8L1,1〉+ 恒k2] (=) E[kδk-ι∣∣2 + k≡tk2i =…=XEkM2.
t=1
(42)
By Lemma 1 and Eq.(42), we have
k2
E[k≡kk2] =庐 XEkM2 ≤ 五.	(43)
t=1
Taking Eq.(43) in to Eq.(39), and taking expections w.r.t FN, we have
N	2N
X (Zak-Lak)EhUGψk4-gk,θk)g 升[≤ J θI)-J *+ɪ X 皆.
k=1	k=1
TL T	- 1	.1	.	. X	八 C 11	. 1	/' 1 -	/< /、	1
Now, consider the output θN = θn follows the distribution of Eq.(16), we have
E[kGψn,'(-gn,θn) (^^] ≤
(J W- J (θι))+σ2 Pk=午
-PN=ι(Zak - Lak)-
Particularly, if the step-size ak is fixed to a Constant：Z/2L, then
E[kGψn,'(-gn ,θn)(θn)k2] ≤
4L(J(θ*) - J(θι))+2σ2 PN=1 1
Nζ2
Recall the following estimation
N1
ΣSτ=ln N + C +。⑴,
k
k=1
where C	is the Euler constant—a positive real number and o(1) is infinitesimal. Thus the overall
convergence rate reaches O(InNN) as
E[kGψ e( ^ Θ )(θn)k2] ≤ 4LDJ +2σPk=11 = O(⅛N).
Ukan,'(-gn,θn)' n *」—	NZ	' N
□
17
Under review as a conference paper at ICLR 2020
B Short corridor with switched actions
Consider the small corridor grid world which contains three sates S = {1, 2, 3}. The reward is -1
per step. In each of the three nonterminal states there are only two actions, right and left. These
actions have their usual consequences in the state 1 and state 3 (left causes no movement in the
first state), but in the state 2 they are reversed. so that right moves to the left and left moves to
the right.
§ 1	三 2	◄——∙—► 3	"G
(a)
Figure 4: Short corridor with switched actions (Chapter 13, (Sutton & Barto, 2018)).
An action-value method with -greedy action selection is forced to choose between just two policies:
choosing right with high probability 1 - j on all steps or choosing left with the same high
probability on all time steps. If = 0.1, then these two policies achieve a value (at the start state) of
less than -44 and -82, respectively, as shown in the following graph.
Short corridor with switched actions
(a)
Figure 5: Short corridor with switched actions (Chapter 13, (Sutton & Barto, 2018)).
A method can do significantly better if it can learn a specific probability with which to select right.
The best probability is about 0.58, which achieves a value of about -11.6.
18
Under review as a conference paper at ICLR 2020
C Proof of Theorem 3
We need the following lemmas to prove the convergence result.
Lemma 3 (Lemma1 (Fang et al., 2018)). Under Assumption 1, Gk,t is generated according to (25),
θk,t is generated according to (26), then for any 0 ≤ t ≤ m, the following holds
L2
E[kGk,t - ^J(θk,t)k ] ≤ NE[kθk,t - θk,t-1k ]+ E[kGk,t-1 - J(θk,t-1)k ].	(44)
Telescoping Eq.(3) over t from 1 to the time t, then the following holds
t L2
E[kGk,t - RJ (θk,t )k2] ≤	寸 E[kθk,i+1 - θk,ik2]+ E[kGk-1,0 - RJ (θ*k-1)k2].	(45)
i=1 N2
Lemma 4. Let Z >	32, the batch size of the trajectories of outer loop Ni	=
(8L1ζ2 + 2iζ⅛ (1 + 3⅛ ))σ2
----- --------------------------, the iteration times of inner loop m - 1 = N	=
e2________________________________________________________________________________________
+ 2(Z⅛2)(1 + 3⅛ ))σ	I
-----------------------,and step size αk = 4L. For each k and t, Gk 0 and θk 0 are
4L	,	,
generated by Algorithm 2, then the following holds,
EkRJ (θk,0) - Gk,0k2 ≤
1
2(Z - 3322 )
1+
(46)
Proof.
EkRJ (θk,0) - Gk,0k2
1 N1
=EIlRJ(θk,0) - N- Y?g(Ti%,。)『
N1 i=1
1	N1
=N2 EEkRJ(θk,0) - g(τilθk,0)k
N1 i=1
(6) σ2
≤——
≤ Ni
+ 2(C 1 5)(1 + 32Z2))	e2
2(Z - 32)	32ζ /
-一	J
^{z
def
=21
(47)
(48)
(49)
(50)
□
|
Theorem 3 (Convergence Rate of VRMPO)The sequence {京k}K=1 is generated according to
Algorithm 2. Under Assumption 1, let Z > 352, the batch size of the trajectories of outer loop
Ni = (	+ 2(z-亘)(1 + 32ζ2)) σσ2, the iteration times of inner loop m — 1 = N =
2(ζ-32) (1 +
3⅛)) 7
the iteration times of outer loop K
ɪ, and step size ak
2
8L
(m - I)(Z - 32)
1+
1	τ-7	Λ 1	■ . 1 C .	.	. 1	■	. Zi	1 -
4L. Then, Algorithm 2 outputs the point θκ achieves
E[kGψ,h7J(% ),θi
,~ ...,
(θK)k] ≤ 7
(51)
Proof. (Proof of Theorem 3)
19
Under review as a conference paper at ICLR 2020
By the definition of Bregman grdient mapping in Eq.(10) and iteration (26), let αk = α, we have
α (°k,t - θk,t+I)J) α (θk,t - arg mUin{hGk,t, Ui + O~DΨ (u, °k，t)}) J' Gψ,hGk t,u (θk,t),
(52)
where we introduce gk,t to simplify notations.
Step 1: Analyze the inner loop of Algorithm 2 Now, we analyze the inner loop of Algorithm 2. In
this step, our goal is to prove
m-1
E[J(θk)] - E[J(θk-i)] ≤ - X (ηE[kgk,tk2] - 2e2).
t=1
In fact,
(33)	L
J(θk,t+1) ≤ J(θk,t) + "J(θk,t), θk,t+1 - θk,ti + ɪ ∣∣θk,t+1 - θk,tk2
(= J(θk,t) - α (VJ(θk,t),gk,ti + Lα2∣gk,t∣2
=J(θk,t) - α hVJ(θk,t) - Gk,t, gk,ti - a hGk,t, gk,ti +----2~ ∣∣gk,t∣∣2
≤ J (θk,t) + 2 IIvJ (θk,t) - Gk,tk2 - a hGk,t,gk,ti + (—2-----+ 2) kgk,tk2	(53)
≤ J (θk,t) + 2 kvJ (θk,t) - Gk,tk2 - ζαkgk,t∣∣2 + (—2------+ 2) kgk,tk2,	(54)
Eq.(53) holds due to the Cauchy-Schwarz inequality |〈u, v)| ≤ Ilukkvk ≤ 1(∣∣u∣∣2 + ∣∣v∣∣2) for any
u, v ∈ Rn. Eq.(54) holds if h ≡ 0 by Eq.(35).
Taking expectation on both sides of Eq(54), we have
E[J(θk,t+ι)] ≤ E[J(θk,t)] + 2E[∣VJ(θk,t) - Gk,tk2] - (Za -?-2) E[kgk,t∣2]
α t L2
≤ EJr(θk,t)] + 5 X ▽Ekθk,i+1 - θk,i∣∣2
2 i=1 N2
+ 2EkGk-1,0 - VJ(θk-1,0)k2 - (Za---------2---2) Ekgk,t∣∣2	(55)
Eq.(55) holds due to Lemma 3.
By Lemma 4, Eq.(55) and Eq.(52), we have
E[J (θk,t+1)] ≤ E[J(θk,t)] +
a3L2
^2NT
Ekgk,tk2.
20
Under review as a conference paper at ICLR 2020
CIl ,ι	, X	n	∙	, ιι , ι ι , , ∙	c∙ / ι ι ∖ . ι ∙ ι
Recall the parameter θk-1 = θk-1,m is generated by the last time of (k — 1)-th episode, we now
consider the following equation
一 . ., 一 .~ ,,
E[J (θk,t+1)] — E[J (θk-1)]
≤ 第 X X Ek。/+2X
2 j=1i=1	j=1
Q3L2 G(	2 Q G
≤ -2N7∑ EEkgk,ik +2∑S
2 j=1i=1	j=1
=M XEkgk,ik2 + 2 Xe2
< α3L2(m — 1)
tt
X Ekgkik2+2 X
i=1	3 =
e2
e2
t
X Ekgkjk2
3=1
t
X Ekgkjk2
3=1
EEkgkjk2
3=1
t
XEkgkjk2
3=1
)X Ekgkj k2
L j=1
(56)
t
Q
2 r
3=1
e1 —
I
α
2
—
α3 L2(m — 1)
2N2
t
-X
i=1
∖
Eq.(56) holds due to t ≤ m — 1.
If t = m — 1, then by the last Eq.(57) implies
{^^^^^™
ζ _ 5
=η=Z	32
(57)
E[J(θk)] — E[J(θk-ι)] ≤ —
m— 1
X (ηE[kgk, t||2] - 2e2
t=1
(58)
—
/
〜
〜
Step 2: Analyze the outer loop of Algorithm 2
We now consider the output of Algorithm 2,
〜
〜
〜
〜
〜
〜
E[J (θκ)] — E[J (θ0)] = E[J (θι)] — E[J (θ0)]	+	E[J (θ2)] — E[J (θι)]
〜
〜
+ .••+ E[J (θκ)] — E[J (θκ-1
(58)	0 /
≤ - X (ηEkg1,t∣l2
t=0
—
m—1
2e2) - X (ηEkg2,t∣∣2 - 2e
m—1
一 X (ηEkgK,t∣∣2 - 2e2)
t=0
2
1
K m—1
-X X (ηEkgkmi2 - 2e1)
k=1 t=0
K m—1
-XX(ηE∣∣gk,tk2)+ Kαe2,
k=1 t=1
then we have
K m—1
X X (ηEkgk,tk2) ≤ E[J(θ0)] — J(θ*) + K( 2	) e1.
k=1 t=1
(59)
21
Under review as a conference paper at ICLR 2020
Recall the notation in Eq.(52)
gk,t = α (θk,t - arg mUn{hGk,t, Ui + αDΨ (U, θk,t)}) = Ga,hGk t,ui(θk,t),
and We introduce following g(θk,t) to simplify notations,
g(θk,t) = Gψ,h-VJ(θk,t),ui(θk,t) = gk,t
=α (θk,t - arg min{h-vJ (θk,t),Ui + αDψ(U, θk,t)}^.
Then, the following holds
E∣∣0k,t∣∣2 ≤Ekgk,tk2 + Ekgk,t - gk,tk2
(36)	1
≤ Ellgk,t『+ 2EEkVJ(θk,t) - Gk,tk2,
(60)
(61)
Eq.(61) holds due to the Eq.(36).
Let V be the number that is selected randomly from {1,…，(m — 1)K} which is the output of
Algorihtm 2,for the convenience of proof the there is no harm in hypothesis that V = k ∙ (m 一 1) +1
and we denote the output θν = θk,t.
Now, we analyze above Eq.(61) and show it is bounded as following two parts (63) and (66)
Ekg(θν)k2
K m-1
XX Ekgk,tk2
(m-1)Kk=1t=1
(≤9) E[J(θ0)]-J(θ*) + ^e2
-	(m - 1)Kη	2η 1i
which implies the following holds
Ekgk,tk2 ≤
-.~ .- ..
E[J (θ0)] -J (θ*) + 旦2
(m - 1)Kη 2η 11
For another part of Eq.(61), notice V = k(m - 1) + t, then we have
EkVJ(θk,t) - Gk,tk2 =EkVJ(θν) - Gνk2
(62)
(63)
(64)
≤ E ~ Elθ Ekθk,i+1 - θk,ik2 + E[kGk-1,0 - RJ(θθk-1 )k2]
N2 i=1
(46)	L2	t	α
≤ E 寸 X Ekθk,i+1 - θk,ik2 + ^ye1
N2 i=1	2
(=)E"ILf XEkgk，ik2# + α≡1
N2 i=1	2
t≤m	L2α2 m-1	2 α 2
≤ E N E Ekgk,ik	+ τye1
N2 i=1	2
2 2 K m-1
≤ ⅛⅛ XX Ekgk,tk2 + 2 e1
2 k=1 t=1
(59) L2α2
≤--------
≤ KN2η
(E[J (θO)I-J(3 + (LaNm-I+2 卜 2，
(65)
(66)
Eq.(65) holds due to the fact that the probability of selecting V = k ∙ (m - 1) +1 is less than 强.
Taking Eq(62) and Eq.(65) into Eq.(61), then we have the following inequity
Ekgk,tk2 ≤
((m - 1)Kη + KN2ηZ2 ) (E[J(θ0)] - J(θ*)) +
L2α3(m - 1)	α α 2
(2N2ηZ2	+2Z2 + 而尸1.
22
Under review as a conference paper at ICLR 2020
1	(而 + 丞-W (1+ 32Z2))σ2
Recall α =	工,N1	=	-----------------2----------------, N2	= m — 1
,(而 + 2⅛ (1 + 32ζ2 ))σ
-----------------------------------,then We have
EkGa,h-VJ(%),θik2 = Ekgk,tk2 ≤ κ(m -4L(ζ-备) (1 + 表)(E[J(θo) - J(θ*)) + 1 e2.
32	(67)
8L(I + 1⅛)	E[J(θ0)] -J(θ*).
Furthermore, K =	∙	, we have
(m - 1)(Z - 32)	e2
E[kGΨ,h-VJ(θκ),θi(θκ)k] ≤ J	(68)
□
D Experiments
D. 1 Experiments Details of Figure 2
For all 'p, we set P ∈ [P] = {1.1, 1.2,…，1.9,2, 3,4,5}, we set Y = 0.99. The learning rate is
chosen by grid search from the set{0.01, 0.02, 0.04, 0.08, 0.1}. For our implementation of MPO, we
use a two layer feedforward neural network of 200 and 100 hidden nodes respectively, with rectified
linear units (ReLU) between each layer.
D.2 Some Practical Tricks for the Implementation of VRMPO
We present the details of the practical tricks we apply to VRMPO in the following Algorithm 3.
(I)	For the complex real-world domains, we should tune necessitate meticulous hyper-parameter.
In order to improve sample efficiency, we draw on the technique of Double Q-learning (Van Hasselt
et al., 2016) to VRMPO.
(II)	For Algorithm 2, the update rule of policy gradient (28)/(25) is a full-return update accord-
ing to R(τ), which is the expensive Monte Carlo method and it tends to learn slowly. In prac-
tice, we use the one-step actor-critic structure. Let D be the replay memory, replacing the term
N12 PN=1(-g(τ八θk,t) + g(τj∣θk,t-1)) (in (25)) as the following δk,t
δk,t = J X(vθLθk,t (si, ai) - VθLθk,t-ι (si,ai)),	(69)
N2 i=1
where L§(s, a) = - log ∏θ(s, a)Qω (s, a) is the training loss of actor, {(si, ai)}N=1 〜D, Qω (s, a)
is an estimate of action-value that can be trained to minimize the loss of critic
1N
Lω = N ɪ2(Qωt-ι (Si, ai) - Qω (Si, ai)¥ ∙	(70)
N i=1
More details of implementation are provided in the following Algorithm 3.
In this section, we use `p as the mirror map.
23
Under review as a conference paper at ICLR 2020
Algorithm 3 On-line VRMPO
Initialize: Policy ∏θ(a|s) With parameter θo, mirror map ψ,step-size α > 0, epoch Size K,m.
Initialize: Parameter ωj ,j = 1, 2 ,0 < κ < 1 .
for k = 1 to K do
for each domain step do
at 〜π" (Ist)
st+1 〜P(Ist, at)
D = D∪ {(st,at,rt, st+1)}
end for
sample mini-batch {(si, ai)}N=ι 〜 D
j
θk,0 = θk-1, ωk,0 = ωk-ι, j = 1, 2
Lθ(s,a) = -logπθ(s,a)	(min Q j (s, a))
j=1,2 ωk-1
|
}
"^^^^^^^^^{^^^^^^^^^^"^
Double Q-Learning (Van Hasselt et al., 2016)
θk,1 = θk,0 - akGk,0, where Gk,0 = N PPi=ι ▽ θLθ (si, ai) I
θ=θk,0
for t = 1 to m - 1 do
/* Update Actor (m - 1) Epochs */
sample mini-batch {(si,ai)}N=ι 〜 D
1N	I	1N	I
δk,t = N X vθLθ(Si, ai)lθ=θk t - N X vθLθ(Si, ai)lθ=θk
i=1	,	i=1
Gk,t = δk,t + Gk,t-1
Θk,t+1 = arg min{hGk,t,ui + ɪ Dψ (u,θk,t)}
u	αk
end for
for t = 1 to m - 1 do
/* Update Critic (m - 1) Epochs */
sample mini-batch {(si, ai)}N=ι 〜D
(71)
(72)
(73)
Lj
ωk-1,t-1
1N	2
(ω) = N y^(Qωj 1 I(Si, Oi)- Qω (Si, ai)) ,j = 1, 2
N i=1	- , -
ωkj,t = arg min Lωj	(ω), j = 1,2
,	ω	k-1,t-1
(74)
(75)
end for
θk == θk m-1, ωj == ωj	1, j = 1, 2
k	k,m-1 , k	k,m-1 ,	,
/* Soft Update */
θk J κθk-i + (1 — κ)θk
ωk J κωj-ι +(1 - κ)ωk,j =1,2
end for
D.3 Test Score Comparison
We compare the VRMPO with baseline algorithm on test score. All the results are shown in the
following Figure 6.
D.4 Max-return Comparison
We compare the VRMPO with baseline algorithm on max-return. All the results are shown in the
following Figure 7.
24
Under review as a conference paper at ICLR 2020
12000
10000
VRMPO
DDPG
PPO
---TD3
---TRPO
IUliaa -BI 0aES><
100∞
80∞
(b) HalfCheetah-v2
LU4aαI-81 oaE0><
(a) Walker2d-v2
—DDPG
VRMPO
—PPO
TD3
TRPO
ER2 -BDHOOE0><
1∞	150	2∞25030C	350	4O)450	5∞
Epocti
(d) Hopper-v2
ER也 ~βoH oσsx<
2000
O
20	40	60	80	100	120	1<W
Epoch
(e) InvDoublePendulum-v2
1∞	150	200	250	3∞	350	«0	450	5∞
E∞dι
(c) Reacher-v2
30	40	50	∞	70	80
Epcxti
(f) InvPendulum-v2
Figure 6: Learning curves of test score over epoch, where we run 5000 iterations for each epoch.
The shaded region represents the standard deviation of the test score over the best 3 trials. Curves
are smoothed uniformly for visual clarity.
LUliaa S3 DraED><
15000
12500
1!XW
1∞	200	300	400
Epoch
(a) Walker2d-v2
LU4aOISs 0raED><
VRMPO
DDPG
PPO
---TD3
TRPO
(c) Reacher-v2
(b) HalfCheetah-v2
E^OK KS3 0αES><
1∞	2∞	300	400	5»
Epoch
(d) Hopper-v2
0
20	40	60	80	100	5.0
Epoch
(e) InvDoublePendulum-v2
—DDPG
---PPO
---TO3
TRPO
---VRMPO
7.5	10.0	12.5	15.0	17.5	20.0	22.5
(f) InvPendulum-v2
Figure 7: Learning curves of max-return over epoch, where we run 5000 iterations for each epoch.
The shaded region represents the standard deviation of the test score over the best 3 trials. Curves
are smoothed uniformly for visual clarity.
D.5 Details of Baseline Implementation
For all algorithms, we set γ = 0.99. For VRMPO, the learning rate is chosen by grid search from the
set {0.1, 0.01, 0.004, 0.008}, batch-size N = 100. Memory size |D| = 106. We run 5000 iterations
for each epoch.
25
Under review as a conference paper at ICLR 2020
DDPG For our implementation of DDPG, we use a two layer feedforward neural network of 400
and 300 hidden nodes respectively, with rectified linear units (ReLU) between each layer for both
the actor and critic, and a final tanh unit following the output of the actor. This implementation is
largely based on the recent work by (Fujimoto et al., 2018).
TD3 For our implementation of TD3, we refer to the work TD3 (Fujimoto et al., 2018) and https:
//github.com/sfujim/TD3.
We excerpt some necessary details about the implementation of TD3 (Fujimoto et al., 2018). TD3
maintains a pair of critics along with a single actor. For each time step, we update the pair of critics
towards the minimum target value of actions selected by the target policy:
y = r+γ min Qθ0(s0,πφ0(s0) +),
i=1,2	i
E 〜clip (N (0, σ), —c, C).
Every d iterations, the policy is updated with respect to Qθ1 following the deterministic policy
gradient algorithm. The target policy smoothing is implemented by adding E 〜 N(0,0.2) to the
actions chosen by the target actor network, clipped to (—0.5, 0.5), delayed policy updates consists
of only updating the actor and target critic network every d iterations, with d = 2. While a larger d
would result in a larger benefit with respect to accumulating errors, for fair comparison, the critics are
only trained once per time step, and training the actor for too few iterations would cripple learning.
Both target networks are updated with τ = 0.005.
TRPO and PPO For implementation of TRPO/PPO, we refer to https://github.com/openai/
baselines/tree/master/baselines and https://spinningup.openai.com/en/latest/algorithms/trpo.html.
26