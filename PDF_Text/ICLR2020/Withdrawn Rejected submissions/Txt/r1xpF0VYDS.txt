Under review as a conference paper at ICLR 2020
QUANTUM ALGORITHM FOR FINDING THE NEGATIVE
CURVATURE DIRECTION
Anonymous authors
Paper under double-blind review
AB STRACT
We present an efficient quantum algorithm aiming to find the negative curvature
direction for escaping the saddle point, which is a critical subroutine for many
second-order non-convex optimization algorithms. We prove that our algorithm
could produce the target state corresponding to the negative curvature direction
with query complexity O(Polylog(d)c 1), where d is the dimension of the opti-
mization function. The quantum negative curvature finding algorithm is exponen-
tially faster than any known classical method which takes time at least O(de-1/2).
Moreover, we propose an efficient algorithm to achieve the classical read-out of
the target state. Our classical read-out algorithm runs exponentially faster on the
degree of d than existing counterparts.
1	INTRODUCTION
Algorithms for finding the minima of functions have attracted significant attention due in part to their
prevalent applications in machine learning, deep learning and robust statistics; in particular, those
with good complexity guarantees that can converge to the local minima. Numerous algorithms have
been proposed in recent years for finding points that satisfying
Rf (x)∣∣ ≤ Eg, and λmin (V2f (x)) ≥ -EH ,
where Eg, EH ∈ (0,1). Recent proposals (Nesterov & Polyak, 2006; Conn et al., 2000; Agarwal
et al., 2017) based on second-order Newton-type and first-order methodology have been analyzed
from such a perspective. However, those methods normally deal with the situations that the iterations
may be trapped in the saddle points, since in many cases, such as deep neural networks (Dauphin
et al., 2014; Choromanska et al., 2015), existence of many saddle points is the main bottleneck.
In general non-convex optimization, many algorithms have been proposed to escape the saddle
points. These algorithms can be divided into the following two categories: the first-order gradient-
based algorithms and the second-order Hessian-based algorithms. Generally, second-order algo-
rithms have better iteration complexity than first-order algorithms (cf. Jin et al. (2017)). However,
each iteration in the second-order method involves the computation of the negative curvature di-
rection, namely, the eigenvector of a Hessian matrix H = V2f (x) with negative eigenvalue. This
computation could take time O(d2) when the Hessian matrix is given, or O(d/λ∕E) when Lanczos
method is used with Gradient information to approximate the Hessian-vector product.
Quantum algorithms have shown great potential to become faster alternatives than classical algo-
rithms for many kinds of problems in the field of linear algebra, including principal component
analysis (Lloyd et al., 2014), support-vector machine (Rebentrost et al., 2014), singular value de-
composition (Rebentrost et al., 2018). These works encourage us to develop an efficient quantum
algorithm for finding the negative curvature. To begin with, we formally define the negative curva-
ture finding problem as follows.
Negative Curvature Finding (NCF) problem: Given a function f(x) : Rd → R which has L-
Lipschitz continuous gradient and the corresponding Hessian , we aim to build a quantum algorithm
that could efficiently provide the unit vector u with the condition:
UTHu ≤ —α + E,
(1)
1
Under review as a conference paper at ICLR 2020
where 0 < α < L and 0 < E < α; or make the non-veCtor statement that with high probability there
is no unit vector u satisfying the following condition:
uTHu < -α.	(2)
1.1	RELATED WORK
Optimization methods for non-convex problems can be roughly divided into first-order and second-
order methods, depending on the order of the derivative to the objective function they used. Gen-
erally, the second-order methods (Carmon et al., 2018; Agarwal et al., 2017) are exploited to find
the effective direction to escape the saddle point. Specifically, finding the Negative Curvature is
considered as a critical subroutine to analyze the characteristic of the saddle point.
First-order algorithms: For the non-convex problem, the first-order method (Gradient-based
method) can find the stationary point, which could be a global minima, local minima or saddle
point. However, standard analysis by gradient descent cannot distinguish between saddle points and
local minima, leaving open the possibility that gradient descent may get stuck at saddle points. Re-
cently Ge et al. (2015); Jin et al. (2017; 2019) showed that by adding noise at each step, gradient
descent can escape all saddle points in a polynomial number of iterations. Lee et al. (2016) proved
that under similar conditions, gradient descent with random initialization avoids saddle points even
without adding noise. However, each step of Gradient-based methods requires O(d) operations and
their iteration complexity is higher than second-order algorithms (Jin et al., 2017).
Second-order algorithms: Traditionally, second-order Newton-based methods can converge to lo-
cal minima, which use the Hessian information to distinguish between first-order and second-order
stationary points. There are two kinds of methods that make use of Hessian information. 1) Hessian-
based: trust-region (Conn et al., 2000) and cubic regularization (Nesterov & Polyak, 2006) are two
methods, in which the sub-problem is to find the decrease direction based on the given Hessian.
The calculation of each iteration involves performing Hessian-vector production, which takes time
at least O(d2). 2) Hessian-free: The Hessian-free methods use Lanczos method to calculate the
negative curvature direction and use gradient to approximate the Hessian-vector product (Agarwal
et al., 2017; Carmon et al., 2018; Carmon & Duchi, 2016). The Hessian-free method involves
O(dE-1∕2) complexity per iteration. The advantage of the second-order algorithm is the superior
iteration complexity than the first-order algorithm. However, using Hessian information usually
increases computation time per iteration.
Quantum Algorithm for Linear Algebra: There are some proposed quantum algorithms for
problems in the related linear algebra field. For example, given copies to quantum state ρ =
id,j=1 xij |i〉〈j|, where xij is the i, j-th element ofd × d matrix X with eigen-decomposition
X = Er=nk(X) λiUiui, previous quantum PCA algorithm (Lloyd et al., 2014) could perform the
mapping Ej βj |uj)→ Ej βj |uj)∣λj〉in time O(Polylog(d)E 3). However, the quantum PCA
model use the density matrix ρ to store the information of matrix X, which implicitly assumes the
condition ∣∣XIlF = 1 and λmin(X). Another quantum SVD algorithm (Rebentrost et al., 2018)
shows an efficient method to estimate the value λj/d with error E in time O(e-3), for d × d matrix
X with eigenvalues {λj}，=i. However, this model only suits the case when λj/d is relatively large,
and it would take time O(d3E-3) to produce E-estimation on eigenvalues. Moreover, both of these
works did not study the classical read-out of the output state, which generally takes time at least
O(d) for d-dimensional state (Aaronson, 2015), and could offset the claimed quantum speed-up.
1.2	OUR CONTRIBUTION
The contribution of this work can be briefly divided into two parts: 1) an efficient quantum algorithm
to generate the required quantum state, which corresponds to the negative curvature direction, and
2) an efficient quantum algorithm to obtain the description of the target state |ut〉 = ir=1 xi |si〉,
where {si}ir=1 is an independent vector set selected from columns of Hessian H with rank r.
Negative Curvature Finding: We develop an efficient quantum algorithm to produce the target
state |ut〉 (for case (1)) or make the non-vector statement (for case (2)). We provide Proposition 1
as the main result of this part, which guarantees the time complexity of our NCF algorithm:
2
Under review as a conference paper at ICLR 2020
Proposition 1. There exists a quantum algorithm which could solve the Negative Curvature Finding
problem in time O(Polylog(d)poly(r)c-1), by providing the target state |ut〉(for case (1)), or
making the non-vector statement (for case (2)).
Classical Read-out: The classical read-out problem is one bottleneck for many quantum machine
learning algorithms whose results are quantum states. Generally, the read-out of a d-dimensional
quantum state takes time at least O(d) (Aaronson, 2015), and could offset the claimed quantum
speed-up. In order to solve this dilemma, we develop an efficient quantum algorithm for the clas-
sical read-out of the target state. We notice that the target state |ut〉 can be written as the linear
combination form |ut〉 = ir=1 xi |si〉, where {si }ir=1 is a linearly independent basis sampled from
column vectors {hj}jd=1. The algorithm suits the case when the result quantum state lies in the span
of several given states, and may give rise to independent interest.
One advantage of generating the form |ut〉 = ir=1 xi |si〉is that it provides the target eigenvector as
the linear-sum of r columns, which guides the curvature direction in general second-order methods.
Our state read-out algorithm contains two subroutines named as the Complete Basis Selection and
the State Overlap Estimation. The main results about the Complete Basis Selection and the Classical
Read-out are briefly summarized as following theorems:
Theorem 1.	There exists a quantum algorithm which takes time O(poly(r)e 2rc) to find an index
set {g(i)}r=ι, where r is the rank of H, C = 2 log 4r'H'F and {g(i)}r=ι forms a complete basis
{|hg(i)〉}ir=1 with probability at least 3/4.
Theorem 2.	The classical description of the target state |ut〉 =	ir=1 xi |si〉 could be presented in
time O(Polylog(d)poly(r)c 5) with error bounds in ∣, when the basis set {sj}r=ι is given.
The rest of this paper is organized as follows. Some preliminaries about quantum information are
introduced in Section 2. In Section 3, we develop an quantum algorithm to solve the NCF prob-
lem. In Section 4, we develop an quantum algorithm which aims to read out the target state. We
summarize our results and contributions in Section 5.
2	PRELIMINARY
In this section we present some preliminary concepts. Some basic quantum knowledge will be
introduced in Section 2.1. Some quantum technics will be introduced in Section 2.2.
2.1 NOTATIONS AND DEFINITIONS
In this section, we introduce some useful notations and definitions about quantum computing. The
dirac notation is a standard notation in quantum mechanics to describe the quantum states. The form
|x〉 is the state which corresponds to the vector x, and the form 〈y| is the state which corresponds to
the vector yτ. The notation〈y|x〉denotes the value yτx/(|IyIWxI∣). The notation |y〉〈x| denotes
the matrix yxτ/(∣IylWxI∣). Quantum state is unitary, which means |||x)||2 =〈x|x)= 1. Thus
for vector x ∈ Rd, the state |x〉is defined as £；=] Xj/IxI |j〉，where Xj is the j-th component of
vector x and {|j〉}jd=1 is the state basis which acts like {ej }jd=1 in classical case. One could obtain
information from the quantum state by performing measurement. For example, the measurement of
|x〉on the basis {|j)}f=i could randomly produce different index j with probability χ2/1x 12.
We use [n] to denote the set {1, 2,… ,n}. We denote the norm ∣∣ ∙ ∣∣ as the ∣∣ ∙ ∣∣2 norm for vector
and the spectral norm for matrix, if there is no more explanation. IlAlIF = (Em=I ∑n=ι aj)i/2
is the Frobenius norm of matrix A ∈ Rm×n . The lowercase form hi is defined as the i-th column
vector of matrix H ∈ Rd×d . Xi is defined as the i-th component of vector x. The tensor product of
two matrix A ∈ RmXn and B ∈ Rp×q is defined as C = A ㊂ B. The tensor product operation
could be performed between vectors, since vector is one special kind of matrix. The tensor product
could be defined between quantum states |x1〉 and |x2〉 which is written as |x1〉|x2〉. We present
definitions of smoothness and γ-separation here.
Definition 1. (smoothness) A function f : Rd → R is L-smooth if it has L-Lipschitz continuous
gradient, that is ^Vf (x) — Vf (y) ∣∣ ≤ Lllx — y ∣∣, ∀x, y ∈ X, where X is the domain of f (x).
3
Under review as a conference paper at ICLR 2020
Definition 2. (Y-separation) The set G = {a1,a2,…,an} is said to be Y-separated if | ai - aj | >
γ,∀i,j ∈ [n] and i ∕= j.
Based on these definitions, we assume that the Hessian H in this article has two properties:
1.	Hessian H ∈ Rd×d is a r-rank matrix;
2.	The absolute value of H's non-zero eigenvalue is E-SeParated.
The first property is directly derived from the assumption of previous classical non-convex opti-
mization method (Carmon et al., 2018), and the low-rank Hessian case has been observed in neural
networks (Gur-Ari et al., 2018). The second property is assumed such that we could distinguish
different eigenvalues by their absolute value. We further assume that the Hessian matrix H has the
eigen-decomposition H = jr=1 λjuj ujT for the convenience of following discussion.
2.2 TECHNIQUES
The motivation idea behind our approach is to perform the quantum singular value estimation model
and then generate eigen-states by the post-selection on the output state. Here we introduce two
techniques including oracle models and critical conclusions in previous work.
Quantum Oracle Models (Kerenidis & Prakash, 2016): For the whole paper, we assume the ex-
istence of following quantum oracles, and discuss the query complexity of our algorithms to these
oracles. Given Hessian H ∈ Rd×d,
we assume that H is stored in a classical data structure such
that the following quantum oracles could be implemented:
1d
UH : |i)|0〉→ |i〉|hi)=时 EhijIi)|j), ∀i ∈ [d],	⑶
j=1
1d
VH : |0)|j〉→ Ih〉|j〉= V77Γ E 也忖 j>,∀j ∈ [d],	(4)
UHiF i=1
where h stands for the d-dimensional vector whose i-th component is Ilhill/∣∣HIlF.
The required data structure has a binary tree form. The sign and square value for each entry are
stored in different leaves and the value stored in each parent node is the sum of its children’s value.
A detail description about this data structure can be referred to (Kerenidis & Prakash, 2016). Denote
TH as the time complexity of these oracles.
Quantum Singular Value Estimation (SVE): Given matrix H ∈ Rd×d which has the eigenvalue
decomposition H = jr=1 λj uj uj , previous work (Kerenidis & Prakash, 2016) provided a quan-
tum SVE algorithm, which could be used for estimating singular value or generating eigenstate.
Here we briefly introduce their conclusion about the time complexity of their algorithm:
Theorem 3. (Kerenidis & Prakash, 2016) Suppose quantum accesses to oracles (3) and (4) exist.
There is an algorithm which could perform the mapping Ej βj ∣Uj)→ Ej βj ∣Uj)||入)|)With time
complexity O(TH Polylog(d)E-1), where λj ∈ [λj — e∣H IIF, λ j + e∣H IIF ] with probability at least
1 - 1/poly(d).
3 QUANTUM NEGATIVE CURVATURE FINDING ALGORITHM
Our main contribution in this section is the quantum Negative Curvature Finding (quantum NCF)
algorithm presented in Algorithm 1. The quantum NCF algorithm solves the NCF problem by
providing the target state Iut〉 (for case (1)) or making the non-vector statement (for case (2)).
The target state Iut〉 corresponds to the eigenvector ut which satisfies the condition utT Hut ≤
—α + e/2. Here We present a tighter restrict on the target state |ut〉to keep a e/2 redundancy for the
classical read-out of the quantum state. The quantum NCF Algorithm uses the Proper Eigenvalue
Labelling (Algorithm 2) and the Target State Generating Algorithm (Algorithm 5 in Appendix) as
subroutines proposed in Section 3.2 and Section 3.3, respectively.
4
Under review as a conference paper at ICLR 2020
Algorithm 1 Quantum Negative Curvature Finding (Quantum NCF) Algorithm
Input: Quantum access to oracles UH and VH. The parameter E and α in the NCF problem.
Output: The target state |ut〉 whose corrsponding classical unit vector ut satisfies the condition
UTHUt ≤ -α + e/2; or a statement with high probability that there is no such kind of unit
vector u which satisfies the condition uTHu ≤ -α.
1:	Label the Proper(less than —α + e/2) eigenvalue of H (Proper Eigenvalue Labelling).
2:	if the least eigenvalue of H is less than —α + e/2, then
3:	generate the target state (Target State Generating Algorithm) and output the state;
4:	else,
5:	claim that there is no such kind of unit vector U which satisfies the condition utHu ≤ —α.
6:	end if
3.1	CHALLENGES TO DEVELOP QUANTUM NCF ALGORITHM
The core technical component of our quantum algorithm for the NCF problem is the quantum SVE
algorithm. However, there are three major challenges that we have to overcome.
Firstly, the positive-negative eigenvalue problem. In the negative curvature finding problem, we are
interested in obtaining eigenvectors with negative eigenvalues. Hence, we cannot directly apply the
quantum SVE algorithm since it only gives the estimation on ∣λj |. In order to overcome this critical
issue, we develop Algorithm 4 in Appendix to label negative eigenvalues.
Secondly, since the quantum SVE Algorithm presents E-estimation on singular values with time
complexity O(TH∣∣HIlFpolylog(d)e-1 )(Theorem 3), We need to provide a tight upper bound for
the Frobenius norm ∣∣H∣∣f, which is shown in Lemma 1:
Lemma 1. Suppose H ∈ Rd×d is the Hessian matrix derived from the function f : Rd → R
which has the L-Lipschitz continuous gradient. Thus the Frobenius norm of H has the upper bound
∣∣H IF ≤ “L, where r is the rank of H.
Finally, the input-state problem. For the general superposition state j βj |Uj〉, the output state of
quantum SVE algorithm has the form Ej βj ∣Uj)∣∣λj |). We could generate different pure state ∣Uj)
with probability ∣βj ∣2 by the measurement on eigenvalue register. Thus in order to guarantee a small
time complexity, we need to prepare a special input state such that the overlap between the input and
the target state is relatively large. We briefly summarize our conclusion on the time complexity of
Algorithm 1 in Proposition 2.
Proposition 2. Algorithm 1 takes time O(TH ∣∣H∣∣FPolylog(d)E-1) to solve the negative curvature
finding problem by providing the target state |Ut〉 or making the statement that there is no unit vector
satisfies the condition UT Hu ≤ —α.
3.2	POSITIVE-NEGATIVE EIGENVALUE DISCRIMINATION
In this section, we propose an algorithm aiming to label the target eigenvalue which is less than —α+
e/2. This algorithm helps verifying the existence of solution to the NCF problem and generating
the target state. Since the eigenvalue information of H is unknown to us, we need to build the
Algorithm 2 to label the proper eigenvalue, which would benefit the target state generation task in
the following section. The proper eigenvalue means the eigenvalue is less than —a + e/2. We view
this kind of eigenvalue as our target eigenvalue.
The mean idea of Algorithm 2 is to use the input state 后Ej=I λj ∣Uj〉|uj〉for the quantum SVE
model and obtain the state:
1r
∣H∣尸	λj|uj)|uj川λj|).
The measurement on the eigenvalue register would lead this entangled state collapse to different
states |Uj〉|Uj〉 for j ∈ [r]. We could obtain the state |Uj〉 by neglecting the state in any other
register. Using state |Uj〉 to apply the PNED algorithm (Algorithm 4 in appendix) could provide
a discrimination on the positive and negative of the corresponding eigenvalue λj . Thus, we could
label the proper eigenvalue, for the case that the least eigenvalue is less than —a + e/2; or make the
5
Under review as a conference paper at ICLR 2020
Algorithm 2 Proper Eigenvalue Labelling
Input: Quantum access to oracles UH and VH. The parameter E and α in the NCF problem.
Output: A proper label to the singularvalue ∣λj | such that λj ≤ —α + e/2 with probability 1 — δ,
or a non-vector statement that there is no unit vector U which satisfies utHu < —a.
1
2
3
4
5
6
7
8:
9:
10:
11:
12:
13:
for k = 1 to ≡H2A (2 4^HF log δ + 3) do
Create the state IHKF ∑r=ι λj |uj)|uj〉.
r
Apply the quantum SVE model to obtain the state 帚口F Ej=I λj ∣Uj)|uj)∣∣λj |)，where
.~~ . . _ . . ... ,
∣λj | ∈ ∣λj | 土 e/4 with probability 1 — 1∕poly(d).
Measure the eigenvalue register and mark the result.
Use the rest state in the first register as the input to apply the PNED algorithm.
end for
Count the result in step 4 and step5 to obtain the sequence {(∣λj |,nj ,mj )}r=ι∙ n，j is the number
〜
〜
of resulting Aj | in step 4, and * Imj is the number of resulting 1 in step 5 for different ∣λj |.
if mmj < 1 for all j ∈ [r], then make the non-vector statement;
else, choose the largest ∣λj | which satisfies the condition m > ∣.
if ∣λj | < α — e/4, then make the no-vector statement;
else, label eigenvalue λj as the proper eigenvalue.
end if
end if
non-vector statement, for the case that all of eigenvalues are greater than -α. We present the time
complexity of Algorithm 2 in Theorem 4.
Theorem 4. Algorithm 2 could label the proper eigenvalue of H with probability 1 - 1/poly(d),
or claim with high probability that there is no unit vector u which satisfies uTHu < -α, with time
complexity O(TH∣∣HIlFPolylog(d)e- 1).
3.3 TARGET STATE GENERATING
Suppose the result of Algorithm 2 implies the existence of the target eigenvector ut, which satisfies
UTHUt ≤ —α + e/2. Then in order to give a solution to the NCF problem, We need to obtain the
vector ut. In this section we construct a quantum procedure to generate the state |ut〉. The classical
read-out of |Ut〉, Which means to estimate vector Ut from quantum state |Ut〉, Will be discussed in the
following section. The idea is very similar to Algorithm 2. We still use state 借口 F £；=i λj ∣Uj〉|u))
as the input of quantum SVE algorithm to obtain state:
1r
K fλj |Uj)|Uj 川 λj |).
Suppose λt denotes the eigenvalue of |Ut〉 that λt ≤ —a + e/2. The measurement on the eigenvalue
λ2	2
register could generate state ∣Ut)∣Ut〉with probability Pt =	2 ≥ 4而产.Thus the probability
of generating at least one state ∣uQ in N = [41HF log 1] + 1 times of measurement is 1 —
(1 — Pt)N. There is: 1 — (1 — Pt)N ≥ 1 — e-NP ≥ 1 — e-log(I⑷=1 — δ. So state |ut)
could be generated in N times of measurement with probability at least 1 — δ . By considering the
time complexity to run the quantum SVE algorithm (O(TH∣∣H∣∣fPolylog(d)E-1)) and setting the
probability error bound δ = 1/poly(d), we could derive the time complexity of generating target
state in Theorem 5. The detail of Target State Generating algorithm is in Appendix (Algorithm 5).
Theorem 5. Suppose that Hessian H has the eigenvector Ut with eigenvalue less than —a + ∣.
Then state |ut〉could be generated in time O(TH ∣∣H∣∣FPolylog(d)E-1) with probability at least
1 — 1/Poly(d).
6
Under review as a conference paper at ICLR 2020
4 STATE READ-OUT
In this section, we propose an efficient algorithm to read-out the classical vector ut from the quantum
state |ut〉. Generally, the classical read-out of a d-dimensional quantum state takes at least O(d)
times of measurement to obtain the form |x〉= ^d=I xie” Thus the classical read-out of the
required state could offset the exponential speed-up (Aaronson, 2015) provided in many quantum
machine learning algorithms. In order to avoid this problem, we propose a quantum-classical hybrid
procedure to rewrite the target state |ut〉 as the linear combination ofr states, which is selected from
column vectors of Hessian H. The main result is stated in Proposition 3.
Proposition 3. The classical description of the target state |ut〉 = ir=1 xi |hg(i)〉1 could be pre-
Sented in time O(THpolylog(d)poly(r)e-5) with error bounds in e/2, when the complete basis set
{hg(j)}jr=1 is given.
Recall that our Hessian matrix H ∈ Rd×d has the eigendecomposition H = jr=1 λjujujT, where
λj is the eigenvalue of H with respect to eigenvector uj . Then, any eigenvector of H that cor-
responds to a non-zero eigenvalue could be represented as the linear combination of vectors in
{hj }d=ι because Huj = λjUj can be written as £d=i hiuji) = λjUj. Since H has the rank of
r, there exists a subset of complete basis {hg(i) }ir=1 of the column space, which is sampled from
the set {hj }jd=1. Thus, any eigenvector Uj could also be represented as the linear combination of
vectors in {hg(i) }ir=1.
Back to the state read-out problem, denote si as hg(i) for simplicity. Suppose the target state |Ut〉
that we generated in the previous section can be written as |Ut〉 = ir=1 xi |si〉, where {xi }ir=1
are coordinates of the state |Ut〉 under the basis {|si〉}ir=1. Thus, instead of simply reading out
components of vector Ut , we could get the classical description of |Ut〉 by calculating each xi . Note
that the complete basis {|si〉}ir=1 is not unique and we only need to identify one of them.
We also notice some recent breakthroughs about quantum-inspired algorithms (Tang, 2019; Arrazola
et al., 2019), which are based on sampling technics and FKV Algorithm (Frieze et al., 2004). These
quantum-inspired algorithms could perform approximate SVD which also outputs eigenvector as the
linear-sum on a group of column vectors. However, in order to cover the whole column space, the
2
quantum-inspired algorithm need to sample at least O(r2) number of columns as basis, while our
method generate the linear-sum form exactly on r columns.
4.1 COMPLETE BASIS SELECTION
In this section, we develop a quantum algorithm to select a subset SI = {g(1),g(2),…，g(r)} from
[d], which corresponds to the complete basis {|hg(i)〉}ir=1. The quantum complete basis selection
algorithm can be viewed as the quantum version of Gram-Schmidt orthogonalization: firstly we
choose 也〉=|hg(i)〉from the set {∣hj)}f=ι; then given state set {∖tm)}lnτ=ι, we choose |ti+i)0
|hg(l + 1)〉-Em=IItm)(tm|hg(l+1)〉fromthe StateSet {|hj〉-Em=IItm)(tm|hj)}d=「Since the
chosen |tl+1〉 is orthogonal to states {|tm〉}lm=1 for each iteration l ∈ [r-1], the state set {|tm〉}rm=1
forms an orthonormal basis. Note that the state Itm〉 is generated along with an index g(m) for
m ∈ [r], so we would obtain a complete basis index set {g(m)}rm=1 after the implementation of this
quantum algorithm. The quantum complete basis selection algorithm is provided in Algorithm 3, in
which we denote:
1
WF
l
|tm〉(tm|hj"I。〉- E |tm〉(tm|hj)|D
m=1
(5)
,
as the initial state in each iteration. The time complexity of Algorithm 3 is analyzed in Theorem 6.
The detail about Algorithm 3 is in Appendix. We also provide Lemma 2 which provides the time
complexity of confirming whether a given set {si }ir=1 are linearly independent.
Theorem 6. The Algorithm 3 takes time O(THpoly(r)E-2r2log(4r^H^F/e)) to find an index set
{g(i)}ir=1, which forms a complete basis {Ihg(i)〉}ir=1 with probability at least 3/4.
1 g(i) is the index of the i-th column vector in the complete basis.
7
Under review as a conference paper at ICLR 2020
Algorithm 3 Complete Basis Selection
Input: Quantum access to oracle UH and VH .
Output: The index set of the complete basis: SI = {g(i)}ir=1.
1:	Initialize the index set SI = 0.
2:	for l = 0 to r - 1 do
3:	Create the state ∣φf), defined in Equation (5).
4:	Measure the third register of state |。，〉multiple times to get the state ∣φ2l)〉which is pro-
POrtiOnal to p⅛F Ed=Ij)电 Il [|hj)- Em=IItm)(tm|hj)].
5:	Measure the first register and record the result as g(l + 1).
6：	Denote ∣tι+ι) as the state proportional to |hg(i+i)〉一 Em=I |tm〉(tm|hg(i+i)〉.
7:	Update the index set SI = SI ∪ {g(l + 1)}.
8:	end for
Lemma 2. It takes O(r3) time to check whether the vector set {si}ir=1 is linearly independent when
the classical access to Hessian H is given, where si is sampled from column vectors of matrix H.
4.2 COORDINATES ESTIMATION
Assume the complete basis {|si)，∣s2), …，|s『)} has been selected out in Section 4.1. Thus the
read-out problem could be viewed as solving the equation |ut〉 = ir=1 xi |si〉, where xi ∈ R are
unknown variables. The coordinate {xi}ir=1 could be obtained by solving the r-dimensional linear
equation system Cx = b, where bi = 〈ut|si〉 and cij = 〈si |sj〉 for i,j ∈ [r]. The equation Cx = b
could be solved classically in at most O(r3) time. Note that we can only get the approximation to
cij or bi instead of the exact value. Theorem 7 verifies the impact of the approximate error to cij or
bi on the read-out of the target state.
Theorem 7. Suppose Cjk is the ∈ι-approximation to Cjk =(Sj|sk) and bj is the -approximation
to bj = (ut∣Sj), ∀j,k ∈ [r], where 6ι = 6*口&-ι 产 and ∈2 = $『/-^. Denote vector X ∈ Rr as
r
the solution of Cx = b. Then xC could lead an approximate eigenvector uCt =	j=1 xCjsj, such that
IIut - UtIl ≤ 〃2.
We propose several methods in Appendix to estimate overlap bi = 〈ut |si〉 and cij = 〈si|sj〉,
which are based on the Quantum SWAP Test (Buhrman et al., 2001) and the Hadamard Test. Our
proposed quantum algorithms could present ∈ι-estimation to Cij =(Si|sj〉in time O(THe-2) and
62-estimation to b = (ut∣Si) in time O((TInput + TH)^-4), where TInput is the time to generate
state ∣ut). Since the time complexity to generate the target state is O(TH∣∣H∣∣FPolylog(d)c-1) as
proposed in Theorem 5, we could derive Proposition 3.
Considering the time complexity O(TH∣∣H∣∣FPolylog(d)c-1) to label the proper eigenvalue and
the time complexity O(THpoly(r)E-2r2log(4r^H^F/e)) to generate the complete basis set, We could
solve the NCF problem in time O(THPolylog(d)poly(r)E-2(E-3 + r2 log(4r^H^F/e))) by providing
the target vector in the form ut = Er=I Xihg(i)∕∣hg(i) ∣∣ with error bounded by E or making the
non-vector statement.
5 CONCLUSION
We propose an efficient quantum algorithm for the Negative Curvature Finding problem, which is
a critical subroutine in many second-order methods for non-convex optimization. The proposed
quantum algorithm could produce the target state in time O(THE-IPoly(r)polylog(d)) with prob-
ability 1 - 1/Poly(d), which runs exponentially faster than existing classical methods. Moreover,
we propose an efficient hybrid quantum-classical algorithm for the efficient classical read-out of the
target state with time complexity O(THpoly(r)polylog(d)E-2(E-3 + r2lοg(4r^H^F/'))), which is
exponentially faster on the degree of d than existing general quantum state read-out methods.
8
Under review as a conference paper at ICLR 2020
REFERENCES
Scott Aaronson. Quantum machine learning algorithms : Read the fine print. 2015.
Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approxi-
mate local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT
Symposium on Theory of Computing, pp. 1195-1199. ACM, 2017.
Juan Miguel Arrazola, Alain Delgado, Bhaskar Roy Bardhan, and Seth Lloyd. Quantum-inspired
algorithms in practice. arXiv preprint arXiv:1905.10415, 2019.
Harry Buhrman, Richard Cleve, John Watrous, and Ronald De Wolf. Quantum fingerprinting. Phys-
ical Review Letters, 87(16):167902, 2001.
Yair Carmon and John C Duchi. Gradient descent efficiently finds the cubic-regularized non-convex
newton step. arXiv preprint arXiv:1612.00547, 2016.
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for nonconvex
optimization. SIAM Journal on Optimization, 28(2):1751-1772, 2018.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192-204, 2015.
Andrew R Conn, Nicholas IM Gould, and Ph L Toint. Trust region methods, volume 1. Siam, 2000.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex op-
timization. In Advances in neural information processing systems, pp. 2933-2941, 2014.
Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast monte-carlo algorithms for finding low-rank
approximations. J. ACM, 51(6):1025-1041, November 2004. ISSN 0004-5411. doi: 10.1145/
1039488.1039494. URL http://doi.acm.org/10.1145/1039488.1039494.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic
gradient for tensor decomposition. In Conference on Learning Theory, pp. 797-842, 2015.
Guy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace. arXiv
preprint arXiv:1812.04754, 2018.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. In The Collected
Works of Wassily Hoeffding, pp. 409-426. Springer, 1994.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle
points efficiently. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 1724-1732. JMLR. org, 2017.
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. Stochastic gradient
descent escapes saddle points efficiently. arXiv preprint arXiv:1902.04811, 2019.
Iordanis Kerenidis and Anupam Prakash. Quantum recommendation systems. arXiv preprint
arXiv:1603.08675, 2016.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only
converges to minimizers. In Conference on learning theory, pp. 1246-1257, 2016.
Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum principal component analysis.
Nature Physics, 10(9):631, 2014.
Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global perfor-
mance. Mathematical Programming, 108(1):177-205, 2006.
Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. Quantum support vector machine for big
data classification. Physical review letters, 113(13):130503, 2014.
Patrick Rebentrost, Adrian Steffens, Iman Marvian, and Seth Lloyd. Quantum singular-value de-
composition of nonsparse low-rank matrices. Physical review A, 97(1):012327, 2018.
9
Under review as a conference paper at ICLR 2020
Alex Schwarzenberg-Czerny. On matrix factorization and efficient least squares solution. Astronomy
and Astrophysics Supplement Series, 110:405, 1995.
Changpeng Shao. From linear combination of quantum states to grover’s searching algorithm. arXiv
preprint arXiv:1807.09693, 2018.
Ewin Tang. A quantum-inspired classical algorithm for recommendation systems. In Proceedings
ofthe 51st Annual ACM SIGACT Symposium on Theory ofComputing, pp. 217-228. ACM, 2019.
A APPENDIX
A. 1 THE PROOF OF THEOREM 2
Proof. The time complexity of Algorithm 1 could be directly obtained by the time complexity of
Algorithm 2 and Algorithm 5, whose complexity analysis are presented in Theorem 4 and Theorem
5, respectively.	□
A.2 THE PROOF OF LEMMA 1
Proof. Assume λι ≤ λ2 ≤ •…≤ λd are eigenvalues of H, We have:
min v Hv ≤ λj ≤ max vT Hv, ∀j ∈ [d].
IIvIl = I	j — ∣∣v∣∣ = 1
By the definition of the Hessian matrix, for unit vector v , We have:
5	「2 r/ ∖	1∙ Vf (X + hv) — Vf (x)
Hv = V2 f (x)v = lim -----------J-----
h→0	h
From above equation, We can obtain:
VTHv ≤ IIv卜 IlHvll ≤ limh→0 m(x+hh)7f(x)1 ≤ limh→0 牛=L,
and vTHv ≥ TvIHHvI ≥ —山力—。2川+怨7〃哂1 ≥ —历卬…中=—L.
Thus, the eigenvalue λj is bounded in [—L, L] for all j ∈ [d].
We have IIHllF = Ei Ej %= Tr(H ∙ H) = Ej λj (H2) ≤ rL2 , so IIHIF ≤ √TL.	□
Lemma 3. Hoeffding’s inequality (Hoeffding, 1994)
Suppose X1,X2,… ,Xn are independent random variables with bounds Xi ∈ [ai,bi],∀i ∈ [n].
Define X =	二 n En=I Xi, then ∀e > 0, we have: 2 =	—,	,	2n2f2	. P(X — E[X] ≥	E)	≤ exp (— En	(b -	a.)2 ),	⑹
and	.—	―	2n2E2 P(X - E[X] ≤ -E) ≤ exp (- En—(b - a )2 ).	⑺
A.3 THE DETAIL ABOUT ALGORITHM 4
In Algorithm 4, P ∈ Rd2×d is the matrix whose column vector Pi = ei ㊂ ∣hi∣ for i ∈ [d] , and
Q ∈ Rd2×d is the matrix whose column vector qj = JHʒ;名 ej for j ∈ [d]. hi is the i-th column
vector of matrix H and h is a d-dimensional vector whose i-th component is IlhiI ∣. It can be directly
obtained that the matrix P and Q satisfy the decomposition H/∣∣HIlF = PTQ and have property
PTP = QTQ = I. Mappings |x〉|0〉 → |Px〉 and |0〉|x〉 → |Qx〉 can be performed by the
quantum oracle UH and VH respectively.
10
Under review as a conference paper at ICLR 2020
Algorithm 4 Positive-Negative Eigenvalue Discrimination(PNED) Algorithm
Input: Quantum access to oracles UH and VH . Eigenstate |u〉 which corresponds to eigenvalue λ.
Output: A random boolean variable with distribution P(0) = 1+，/2"UF and P(1) = 1-%/^”.
1:	Create state |u〉|0〉|0〉.
2:	Apply the Hadmard gate on the third register to obtain the state 击(|u〉|0〉|0〉+ |u〉|0〉|1)).
3:	Apply the Controlled-SWAP gate to obtain the state 我(|u)|0〉|0〉+ |0〉|u〉|1〉).
4:	Apply gate UH 名 |0〉〈0| + VH 名 |1〉〈1| on the state to obtain √(|Pu)|0〉+ |Qu〉|1〉).
2
5:	Apply the Hadmard gate on the third register to obtain the state |Pu〉:|Qu〉10〉+ |Pu〉-|Qu〉11〉.
6:	Measure the third register and output the result.
Theorem 8. Positive-negative eigenvalue discrimination. For eigenvalue λ of H with property
∣λ∣ ≥ a, one could run Algorithm 4 for n = 2[几HF log ∣ - 2] +3 times, to make a statement that
λ is positive or negative, with probability 1 - δ.
Proof. The measurement in step 6 of Algorithm 4 outputs 1 with probability:
P(I) = U|Pu>2 |Qu>『=1(〈Pu| -(Qu|)(|Pu)- |Qu〉).
(8)
Note that〈Pu|Qu)= UTPTQu = 口捻F UTHu = uh∖∖f , so P(1) = 1-'/2印”.Similarly We
have P(0) = 1»*^ .
Suppose that we need 2x + 1 times of measurement to give an 1 - δ correct statement about whether
λ > 0 or λ < 0. The problem can be viewed as the biased coin problem. Define random variables
Xi such that P(Xi = 1) = P and P(Xi = 0) = 1 — P and Sn = £N1 X” Then there has the
Hoeffding,s inequality P (Sn / n — P ≤ —e) ≤ e-2ne2 and P (Sn/n — P ≥ E) ≤ e-2ne2.
Back to the problem, suppose λ < 0, by setting n = 2x + 1, n(p — E) = X and P = - /∖—
we have:
P(S2x+1 ≤ X) ≤ exp(-2(2x + 1)[1-"H" - 2xx+1]2) < e-守岛
2x+1	a2
≤ e 2 HHHF
-2x + 1 a2
Similarly for λ > 0, there is P(S2χ+1 ≥ x) ≤ e 2 HHHF .
-2x + 1 a2	2
Let e 2 HHHF ≤ δ , we have X ≥ [ "H2∣F log ∣ — ∣] + 1.
□
A.4 THE PROOF OF THEOREM 4
Proof. The input state 后£j=1 λj ∣Uj〉|uj〉could be generated with oracles UH and VH:
d	dd
|0)|0〉—→ F Σ MiMi)|0〉—→ 大 ∑∑hij|i)办	⑼
Since H has the eigen-decomposition H = rk=1 λkukukT, we could rewrite entry hij as
hij = Ek=1 λkUf)Uj), where Uf) is the i-th component of vector Uk. Thus the state
p⅛ ∑d=1 Edj=I hijli)j)could be written as:
ddr	r
UH7 ΣΣΣ 尤 Uki)Ukj)Ii〉j〉= uH7 Σ Xk|uk)|uk〉.
i=1 j=1 k=1	k=1
Then we apply the quantum SVE model on this state. In order to give E/4-estimation on the singular
value, the time complexity to run the quantum SVE algorithm is O(THIlHIlFPolylog(d)E-1) by
Theorem 3.
11
Under review as a conference paper at ICLR 2020
Suppose there are eigenvalues λj which are less than -α + e/2. We denote the least one as λt and
label it as the proper eigenvalue. By Theorem 8, We need to generate n = 2[ɪHF log 1- 2 ] + 3
numbers of state |ut〉 in order to guarantee that λt < 0 with probability 1 - δ. Note that the
probability of generating state |ut〉in each iteration of step 2-4 in Algorithm 2 is Pt = 口裔产.
So averagely we need to perform step 2-4 in Algorithm 2 for n = ɪHF{2[ɪHF log 1- 2 ] + 3}
times. The number n can be roughly upper bounded by 4%'F (2 4%'f log ∣+3), since for negative
curvature case E < α, we have ∣λt∣ = α — e/2 > α∕2.
By considering the time complexity to run the quantum SVE algorithm
(O(TH∣∣H∣∣fPolylog(d)E-1)) and setting the probability error bound δ = 1∕poly(d), we
could derive that the time complexity of Algorithm 2 is O(TH ∣∣H∣∣FPolylog(d)E-1).	□
A.5 TARGET STATE GENERATING ALGORITHM
Algorithm 5 Target State Generating
Input: Quantum access to oracles UH and VH. Parameters α and E in the NCF problem.
Output: The target state |ut〉with property〈Ut|H|ut〉= λt ≤ —α + e/2.
1: for k = 1 to [4，H』F log 1 ] +1 do
2： Create the state IHlF ∑r=ι λj |uj〉*〉.
r
3:	Apply the quantum SVE model to obtain the state JHɪ^ ∑j=ι λj ∣Uj)|uj)∣∣λj |)，where
.~~ . . _ . . ... ,
∣λj | ∈ ∣λj | 土 e/4 with probability 1 — 1∕poly(d).
4:	Measure the eigenvalue register and mark the result.
5:	if the eigenvalue measured in in step 4 is labelled to be proper in Algorithm 2, then
6:	output the state in the first register as the target state.
7:	end if
8:	end for
A.6 THE DETAIL ABOUT ALGORITHM 3
Consider the state:
l
∏ Igj))回φ1l))三
j=1
1
ld	l
∏ Ig(j)) E Mj ∣∣j)	Ihj〉- E |tm〉(tm|hj)
j=1	j=1	m=1
l
∣0)- E |tm)(tm|hj )|1)
m=1
)
(10)
12
Under review as a conference paper at ICLR 2020
We note that the state |。，)in Step 3 of Algorithm 3 can be generated by the following procedure:
「ι
∏ ∣g(j ))
Lj=I
|0)|0)
(°)	1
WF
(b)	1
WF
(c)	1
WF
-l	Id
∏∣g(j))E Mj W〉也〉
-J=1	」j=ι
'∏ ∣g(jME Mj W)|hj)"2
ɪ	2^[	V 2
j=1	j=1
j=1
j=1
l
%)一2 E |tm)(tm|hJ)
m=1
(11)
(12)
√+同中]
√2	1 j' √2∫
(13)
(d)	1
WF
j=1
j=1
l
|hj)一 £|tm〉〈tm%)
m=1
l
1o)- E
m=1
(14)
l
d
Il|g(j)〉EMj W)
{
l
d
I!|g(j)〉EMj W)
{
|tm)(tm|hj)|1)}
d
-→ 商F E % "j)
{
ll
|hj〉- E |tm)(tm|hj)|0)- E |tm)(tm|hj )|1)
m=1	」	m=1
)
(15)
In step (a), we employ the quantum oracles UH VH on ancillas |0)|0). In step (b), an auxiliary
register |0) is appended, followed by a Hadamard gate. In step (c), we apply operation ^lm=11Rm ㊂
|0)〈0| + I ㊂ |1)(1|]. The operation Rm is defined on the register |g(1)),… |g(m))，|hj)：
Rm = I须""+1)- 2{ [口 |g(j))] |〃)} {[口S(j)|] CM
In step (d), we apply the Hadamard gate on the auxiliary register. In step (e), we trace out registers
∏j-=1 |g(j)) to generate state 就)〉.
The crucial part in Algorithm 3 is to implement the reflection Rm. For the m +1 case, there is:
|tm+1)
1m
Z	(Ism+1)-〉: |tiXtiIsm+1)),
(16)
Note that {|ti)} forms the orthogonal basis: 〈ti|tj) = 0,∀i = j, and Zm+1 = (tm+1|sm+1)
Msm+1) - £% |ti)(ti|Sm+1)11 = √1 - ∑Z1(ti |sm+1)2∙
Denote |片 ≡ Ej= 1 Xij|sj). The restriction that |tm+1) is normalized and orthogonal to states
|s1), |s2), …|sm) could yield Equation (17):
'm+1
E Xm+1,iCsj |si) = 0, ∀j ∈ [m],
i=1
m+1 m+1
ΣΣxm+1,jxm+1,i〈sj Isi) = 1∙
、j=1 i=1
(17)
Note that Xm+1,m+1 = 1∕Zm+1 by Equation (16). Define the m-dimensional vectors x, b, and the
m × m matrix Cm to be:
mm
X =): xm+1,iei , b = E⑸1sm+1)ej , Cm = {cij ≡(si|sj)}i,j
i=1	j=1
where each value Cij =〈si|sj) could be estimated by Hadamard Test. Thus, we could derive the
following linear equations about x from Equation (17):
CmX =-万一b,
Zm+1
XTb=zm+1 - 1.
Zm+1
(18)
13
Under review as a conference paper at ICLR 2020
We could obtain the coordinate {xm+1,i }im=+11 by solving Equation (18). There is:
_	1	_ I |Cm|
xm+1,m+1 =	= ∖ 77； P
Zm+1	|Cm+1 |
xm+1,i
ι cm)∣»一]
万一77^T, ∀i ∈ H,
Zm+1 |Cm |
(19)
where matrix Cm(i) denotes the matrix generated from Cm by replacing the i-th column with b.
Suppose now We have obtained the linear combination form ∣tm+ι) = £m=+1 Xm+ι,i∣s/
Thus, we could construct a unitary to generate the state
[∏m=+11 |g(j))] ∣tm+1)
from the state
[∏m=+1 ∣g(j)〉] I。〉，by the linear-Combination-of-states method (Shao, 2018).
The idea of linear combination of states was introduced in Shao (2018), which focuses on the fol-
lowing problem: given quantum states |a〉and |b〉，to prepare the state |c)= }(x|a〉+ y|b〉). The
method is based on the fact that R2θ = (I - 2|b〉〈b|)(I - 2|a〉〈a|) can be viewed as the clockwise
rotation in the plane spanned by |a〉 and |b〉 with angle 2θ, where θ = arccos〈a|b〉 is the angle
between |a〉 and |b〉. Thus any clockwise rotation in space SPAN{|a〉, |b〉} with angle φ could
be written as Rφ =
φ = arccos x+吸⑸
n case:
Rφ/2". For the case |c)
arccos
x+y〈a|b〉
= -1-(x|a)+ y|b〉)，there is |c)= Rφ∣a), where
. The linear sum of 2 states could be generalized to

Theorem 9. (Shao, 2018) Assume state ∣φi〉could be prepared by given unitary operation in time
Tin, for i ∈ [n]. Then there is a unitary which could prepare the state ∣φ) = En=I α∕φi) in time
O(TinnIo虱n/')) with error e.
Note that here we actually perform the state ∣tm+1) = |g(1)〉|g(2))…∣g(m + 1)) ∣tm+1)
from states {∣g(1))∣g(2))…∣g(m + 1))∣Si)}m+1, and each state (∏m=+t1 ∣g(j)))∣Si) could be
performed by oracle UH on state ("；=+1 ∣g(j)))∣0).	Denote Um+1 as the generates the state [口；=+1 |g(j))] ∣tm+1) from the state [∏m=ι1 ∣g(j))]∣o).	unitary which The operation
in step (c) of procedure (11): ∏m=1[Rm ㊂ ∣0)(0∣ + I ㊂ |1)(1|] could be ∏m=1 {[umι+1 (I- 2∣0)(0∣)Um+1] 0 |0){0| +10 ∣1)(1∣}.	performed by
A.7 THE PROOF OF THEOREM 6
Denote Pl as the probability of resulting 0 after the measurement in Step 4 of Algorithm 3. In order
to generate the required state, the measurement in Step 4 needs to be performed for O(1/Pl) times.
The Lemma 4 provides a lower bound on Pl .
Lemma 4. The probability of resulting 0 after the measurement in Step 4 of Algorithm 3 as the
(r-l)e2
4WF.
lower bound Pl ≥
Proof. Suppose λ2 ≥ λ2 ≥ …≥ λ2, where λi is the eigenvalue of H. Since state |tm) is the linear
sum of {|hj)}jd=1, we can assume that |tm) has the decomposition |tm) = ir=1 wmi |ui), for all
14
Under review as a conference paper at ICLR 2020
m =1,2, ∙∙∙ , l, where Er=I Wmiwni = δmn. There is:
1d	l
Pl =商百 Σ Mj『IMj)一 E |tm〉(tm|hj "2
11H 11F j = 1 L	m=1
1d
WF j=ι
j=1
l
Mj『-∑ Mj『l(tm|hj〉l2
m=1
dl r
1-F ∑ £ ⅛ wmiλiu(j)
1-
1
WF
dl
ΣΣ
j=1 m=1
rr
∑ wmi λ2 (u(j ))2 + ∑ wmi wmk λi λk u(j ) Ukj )
i=1	i∕=k
1-
1
WF
lr
∑∑ wm K
m=1 i=1
1-
1r
K Σ ci λi,
Where ci = Em=Iwm i.
Consider the r-dimensional vector wm = ir=1 wmiei . The vector set {wm}lm=1 forms an
orthogonal basis in a l-dimensional subspace. Note that we can add wl+ι, •…Wr such that
{wm }rm=1 forms an orthonormal basis in the whole r-dimensional space. Denote matrix W =
(wι, w2, •…,Wr). Since WTW = I. Since W is unitary, there is:
r
∑ wm i = 1,∀i ∈ [r].
m=1
ThUSwe have the UPPer bound: c = Em=Iwmi ≤ Em=I wmi
∑r=ι Em=Iwmi=Em=I Er=Iwmi=l, so there is:
(20)
=1. Note that Er=Ici
Pl ≥ 1 -
1
WF
l
∑ λ
i=1
wr=l+ι λ2
UHuF
(21)
〜
Note that for the case ∣λi | ≤ e/2, λi = 0 is a good estimation for the NCF problem, so We could
further assume ∣λi∣ > e/2 for the general case and bound the inequality (21) as Pl ≥
(r-l)£2
4WF.
□
The error of imPlementing |tm+1〉 comes from the imPerfect imPlementing of the linear-
combination-of-states operation and the error of calculating x. The former has been analyzed in
Theorem 9. The error of calculating x is more complex.
Define vector y = -Zm+1x. Note that all parameters cij = 〈si |sj〉, i, j ∈ [m] and bj =
〈Sm+i|sj)，j ∈ [m] are estimated by Hadamard test, which takes time O(THe,-2) with error
bounded in e' (see Appendix A.10 for more information). Thus the calculation on vector y = Cml1b
would have an error. Define matrix Cm = Cm + ∆Cm and b = b + ∆b which are estimations on
Cm and b. Suppose 归j 一 cj | ≤ ∈ι and |bi 一 bi| ≤ ∈ι are error bounds for cj and bi, respectively,
∀i,j ∈ [m]. Denote y = CmIb as the solution to the approximate linear equation and ∆y = y — y
as the error to y . We have:
JCm y = b,
(Cm + ∆Cm)(y + ∆y) = (b + ∆b).
15
Under review as a conference paper at ICLR 2020
So there is:
U∆y∣l = HCm + ∆Cm)-1(∆b - ∆Cm∙Cm1b)∣∣
≤ UCmIHH + Cm1∆Cm)T∣∣∙ (∣∣∆b∣∣ + 11 ∆Cm ∙ C^b 11 )
≤ UCmI 卜 I SIIʌ 「 U ∙ (U∆bU + ∣∣∆Cm∣∣∣∣Cm1"b∣)
1 - IlCm ∆Cmll
≤
ICmI I
ι -IICmIlImEI
∙ (√m6ι + m3∕2Eι∣∣CmII) ≤
2m3∕2∣∣C-1∣ 宣 ι
1 -IlC-I IlmEI
The norm IlCmIIl here denotes the largest norm of eigenvalues of matrix Cm1. Thus, for E2
3m3∕2∣∣Cm1 ∣∣2∈1,we have bound ∣∣∆y∣∣ ≤ ∈2.
For |tm+1)= Em+1 Xm+1,i|Si〉and Bm+1)= Em+1 Xm+1,c∣s) there is:
m+1 m+1
(tm+1 |tm+1)=ΣΣXm+1,i Cij Xm+1,j
i=1 j=1
1
r7 7
Zm+1 Zm+1
.~ .
1
7	7 一
Zm+1 Zm+1
m,m
m
[E	y，c，jyj- Eyibi
i=1j=1
i=1
m
-E bj yj+1]
j=1
Note that the form ∣tm+1) =
m+1 m+1
m
[1 - E仍仇]
i=1
Zm+1
7
Zm+1
Im+1 Xm+1,i∣Si) is not a normalized state:
I∣tm+1)I2 =E E Xm+1,iCijXm+1,j
i=1 j=1
] m,m	m
小[E	yicij yj + 1 - 2 E yibi]
Zm+1 i=1,j=1	i=1
1 m,m	m,m	m,m
=Z-[ E ∆yiCij∆yj +2 E XiCij yj + ∑ yi Cij yj + 1
Zm+1 i=1,j=1	i=1,j=1	i=1,j=1
1	m,m
=Z- [zm+1+ E △%◎-△%・] ∙
Zm+1	i=1,j=1
Lm .1	1	1	. . I I	∖	1 I ^i X ∙
Thus, the overlap between state ∣tm+1) and ∣tm+1) is:
(tm+1 |tm+1) _	Zm+1
m
m
-2£ ∆y也一2£ y bl]
i=1
i=1
There is:
Let E3/2
，，，~ . 11
IIItm+1)Il
JZm+1 + ∆yτ Cm∆y
11 . . ， ~ 、 ，，，, ~	. 11 11
IlItm+1)一Itm+1"∣∣ 1tm+1)Illl =
∖
~
~
2-2
≤
≤;
≤
≤
3m2∣∣C∕Aι
Zm + 1
(22)
Zm+1
Jzm+1 + ∆yτ Cm∆y
2-2
Zm+1
JZm+1 + I∆yI2ICmI
Z +1
2 - 2Z I IlCmllll∆y∣∣2
Zm+1 +	2zm+1-
ICmI”∣∆yI
Zm+1
m1∕2E2 _ 3m2∣∣Cm1I2E1
Zm+1
Zm+1
It is clear that to obtain the linear combination form
EmIIXm+1,i∣Si) takes time O(m3 + m21
~
(23)
(24)
(25)
(26)
(27)
∖+ ∖
∣tm+1)
THE1 2), where Mtm+1)∕∣∣∣tm+1)I - ∣tm+1)I ≤
16
Under review as a conference paper at ICLR 2020
E3/2. By Theorem 9, the implementation of state ∣tj7i+1) = [口；=+LI |g(j)》]忆m+1)takes time
O(TH(m + 1)log(2(m+1)/£3)) with error bounds E3/2. Thus, we can implement state Itm+1)=
[∏m=ι1 ∣9(j))]∣tm+1) by unitary in time O(m3 + m2T⅛e-2 + TH(m + 1)log(2(m+1)∕e3)) With error
bounds in E3.
Now we consider the influence of imperfect implementation of state 1f：) to Algorithm 3. For
simplicity, we neglect the term [∏J=1 ∣g(j))] in state ∣%) = [∏J=1 ∣g(j))]∣tm). Denote ∏ι =
∏i=1(I- 2∣ti)(ti∣) and Πι = ∏i=1(I- 2∣tl)(t4∣). There is ∣∣Πz - ΠJ∣ ≤ 2a.
Note that the state
1 d 17 l
^∑∣hj ∣∣j) I [∣hj)- ∑ ∣tm)(tm∣h1)
∣0)- ± ∣tm)(tm∣hj)∣1)l
m=1	)
in step 3 of Algorithm 3 can be written as:
看W卬⑷?电用+F电问.
J —1
The probability of generating 0 after the measurement on the last register is:
pι = W W ∣hJ 内?电)12.
SimilariyWedefine 欣I)) = PLrF Ed=Illhj Ilj)[%tI Ihj力。)+ %户电X1X
and Pι = rH\\F Ed=1 ∣∣hj ∣∣21 πl+I ∣hj) ∣2 for the approximate case.
Since the objective of step 5 is to obtain the index g(l + 1) such that the column state ∣h5(i+1))=
∣sι+1) is linearly independent from basis {∣s )}i=1, we define PfaIIie as the probability of selecting
out the state ∣sι+1) ∈ {∣Si)}i=1 in the approximate case. Note that for state ∣hj) ∈ {∣Si)}i=1,
(∏ι + I)∣hj) = 0, so there is:
P false
P ** ZJhj ∣2∣ ∏⅛2 E ,∣2
一 π 1 τ	一
E"h 力∈{∣sQ}i=1 ∣hj『I	〔hj )『
Ed=1∣hj I21 呼 Ihj )∣2
C τ-r τ~r	C
Ej：i”{⑸》}「也 I2I—∣hj )∣2
Ed=1 ∣hj ∣2(1∕2+(hj∣∏ι - ∏ιIhj)/2+ (hj∣∏ι∣hj)/2)
≤	ι%3
pι - k-3
^t
(r-ι)e2
4WF -
≤
Let e3 = 8(r-1')IlHllF ,thereis:
r—1	r—1
EPfaISe ≤ς
ι=o	ι=o
_________12J_________
2(r - 1)(r - l)e3 - le3
r —1
≤ W K
l=o
r(r — 1)	e2
2	8(r-1)∣H ∣F
re2	1
------≤ 一.
16∣H∣F ≤ 4
(28)
Thus, by choosing E3
probability at least 4.
e2
8(r—1)∣∣H∣∣F
,Algorithm 3 could select out a complete basis {∣Si)}i=1 with
17
Under review as a conference paper at ICLR 2020
Note that in Algorithm 3 We need to Peform operations R = I - 2|ti〉〈t； | for i = 1,2,… ,r — 1,
which needs the information of parameters {cij }ir=-11,,jr=-11. In order to guarantee the success prob-
ability of Algorithm 3 (Equation (28)), the estimation on each cij = 〈si |sj〉 should have error
bound EI = Iminm∈[r-2] ξ⅛⅛E3 ≥ 48：3设[阜扃F E2∙ Thus，the estimation on each Cij
takes time O(THr6∣H∣∣FE-4) and the estimation on the parameter group {cj 1：—：j=； takes time
O(Th r8∣H |FE-4). In order to obtain the description |tmj = Em=I Xm,i|si)，additional time is
required for solving (m-1)-dimensional equations for m = 2, 3, •…r — 1, which results the time
complexity O(r4) in total. With given parameters {xm,i }im=1， the implementation of operation
Rm = I - 2|%，〈%」takes time O(TH(m + 1)log(2(m+1)∕e3)^) ≤ O(TH ∙ r2log(4r|H|F/e)).
Denote Tbasis as the required time to implement Algorithm 3 and TRi as the required time to
implement operation Ri. Since in each iteration of l ∈ [r - 1], Algorithm 3 refers operation
R1,R2,… ,Ri for 1/Pi times, there is:
r-1	l
O(Tbasis) = O(Thr8∣H∣Fe-4) + E IP E O(TRi)
l=0 l m=1
≤ O(Thr8∣∣H∣∣Fe-4) + 4HUF O(Thr2log(WHlF同)E _l_
E2	r- r — l
l=0
=O(TH IlHIIF E-2(r8∣∣H∣∣F E-2 + r1+2logOrHIF√e)))
≤ O(THpoly(r)E-2r2log(4rHlF同).
A. 8 THE PROOF OF LEMMA 2
Proof. Define the index function g : [r] → [d] such that si = hg(i) , ∀i ∈ [r]. Consider the eigen-
decomposition of matrix H :
r
h = E λ UjUT.	(29)
j=1
It is natural to generate the decomposition:
r
hj = E λiUiU(j),	(30)
i=1
r
hjk = E λ Ujuikk.
i=1
(31)
Define the r × r dimensional matrix C = (hTii), hT2),…，hTir))T(hg(ι), hg(2),…，hg(r)).
There is:
{hg(i) }ir=1 is linear independent ⇔ det(C) ∕= 0.	(32)
Denote the jk-th element of C as Cjk. Since Cjk = hhk = Er=ι λ2uij)uik), there is:
∑r=ι λ2uig(I))Uig⑴)…EL λ2uig(I))Uigir))
det(C) =	...	...	...
Lr	∖2.,(g(r)).,(g(1))	Lr	∖2.,(g(r)).,(g(r))
‰=1 λi Ui	Ui	…	‰=1 λi Ui	Ui
r r r	入^：(I))Uig(I))…入"?(I))U
=EE …E
i1=1 i2 =1	ir=1	λ2 2∕(g(r)),∕(g(1))	λ2 7∕(g(r))°∕(g(r))
λil Uii	UiI	…N.".,	Uir
U(g(l))	...	U(g(l))
r r	r r	r	Uii	Uir
=EE …E (∏ λ2j )(∏ U(g(j))).....
i1=1 i2 =1	ir=1 j = 1	j=1	°,(g(r))	°,(g(r))
UiI	…Uir
(33)
(34)
(35)
18
Under review as a conference paper at ICLR 2020
On the other hand, construct the matrix H' whose jk-th element is hjk = hg(j),g(k). There is:
det(H')
∑r=1 Mg(I))Uy(I))…∑r=1 Mg(I))UyS))
..	..	..
.	.	.
∑r=1 M(g(r))Uy(I))…∑r=ι Mys))Uys))
(36)
r r	r
ΣΣ ∙∙∙ Σ
i1 = 1 i2 = l	ir = 1
λ. u(g(1))u(g(1))
λi1 ui1	uiι
.
.
.
λ∙ U(Cr))Ug(I))
λiι uiι	Uii
λir"(g(I))U(Mr))
rr	r
(37)
ς ς ∙∙∙ς (∏ λij)(∏ Ug(J)))
i1 = 1 i2 = 1	ir = 1 j=1	j = 1
λir U(g(r))U(g(r))
rr	r
Uig(I))	∙ ..	∙	w(g(1)) Uir .
. . U(g(r))	∙	.. ∙	w(g(r)) Uir
(38)
Note that the determinant in eq(35) and eq(38) is non-zero only if im = in for any different m, n ∈
[r]. Consider the summation of ij for all j ∈ [r] over {1, 2, ∙ ∙ ∙ , r}, there is:
rr
det(C)/口 λ = det(H')/" 入
i=1	i=1
(39)
Thus the problem about whether group {hg(i)}r=1 is linearly independent could be solved by cal-
culating the determinant of matrix H'. Since H' is a r × r dimensional matrix, det(H') could be
calculated in O(r3) time (Schwarzenberg-Czerny, 1995). We could claim that the group {hg(i) }r=1
is linearly independent if det(H') = 0, or {hg(i)}r=1 is linear dependent if det(H') = 0. 口
A.9 The proof of Theorem 7
Proof. For ∣∆cj | ≤ ∈1 and ∣∆bj∣ ≤ ∈2, there is:
∣∣∆C∣∣ ≤ r61 and ∣∣∆b∣∣ ≤ √r62∙
The matrix norm ∣∣ ∙ ∣∣ here denotes the largest singular value of the matrix. Note that elements of
matrix C and vector b are overlap of quantum states, which are bounded in [—1,1], so there is:
∣∣C∣ ≤ r and ∣∣b∣ ≤ √r.
We have:
∣∣∆x∣ = ∣∣(C + ∆C)-1 (∆b — ∆C ∙ CTb)Il
≤ ICTIHH + C-1∆C)-1∣ ∙ (∣∆b∣ + ∣∆C∙ C-1b∣)
≤ ∣c-1∣ ∙「J” ∙ (∣∆b∣ + ∣∆C ∣∣C-1∣∣b∣)
≤ 1-∣CTIreI ∙ (√re2 + r3/2e11C-1∣) ≤ 2√r.
Thus, for Ut = Er=I XjSj and Ut = Er=I XjSj, there is:
∣ut — UtIl = √∆xTC∆x ≤ ∣∆x∣ ∙ IlCII1/2 ≤ -√= ∙ √ = f.
2r	2
□
A.10 The estimation of Cij = 国岛)
The overlap Cij =(Si ∣Sj)=(hg(i) |hg(j))can be estimated by the Hadamard Test. We provide the
detail in Algorithm 6:
Proposition 4. Algorithm 6 present the e-estimation to the overlap Cij = (SiISj〉with probability
at least 1 — δ with running time O(THe-2 log(1∕δ)).
19
Under review as a conference paper at ICLR 2020
Algorithm 6 cij estimation
Input: Quantum access to oracle UH. The index number i and j. The precision parameter ε. The
probability error bound δ .
Output: An estimation	Cij	to the value	Cij	=	(si∣sj),	SUCh that	Cij	∈	[cij∙	- e ,	Cij	+	ε]	with
probability at least 1 - δ .
1:	for k = 1 to n =修 log( δ)] + 1 do
2:	Create state [|si)|0〉+ ∣sj)∣1>]∕√2.
3:	Apply the Hadmard gate on the second register to obtain the state lsi>+1Sj〉|0〉+ |si〉-lsj〉11).
4:	Measure the second register and record the result.
5:	end for
6:	Count the number of resulting 0 in step 4 as m. Output 2m∕n - 1 as the estimation to Cij .
State 同〉1°〉岩〉11〉=
2
|hg(i)〉|0〉+|hg(j)〉|1〉
√2
in step 2 could be generated by performing the following
procedure on state |g(i)〉|g(j)〉|0〉|0〉:
∣g(i))∣g(j))∣0)∣0)-→∣g(i))∣g(j))∣0) 10√^
2
(40)
u-≡→ ∣g(i))∣g(j))
|hg（i）〉|0〉+10〉|1〉
U----→ ∣g(i))∣g(j))
√2
|hg(i)〉|0〉+ |hg(j )〉|1〉
(41)
(42)
-tr-ac-e-o-ut-th-e -fir-st-tw-o-r-eg-is-te→rs
|hg(i)〉|0〉+ |hg(j)〉|1〉
(43)
The Hadmard gate in (40) acts on the 4-th register. The gate UH ③ |0〉〈0| in (41) acts on the 1-st,
3-rd and 4-th registers. The gate UH ③ 11)〈11 in (42) acts on the 2-nd, 3-rd and 4-th registers.
A.11 THE ESTIMATION OF bi = 〈ut|si〉
The estimation to the overlap bi = 〈ut |si〉 is more complicated. Technics like Algorithm 6 is infea-
sible, due to the post-selection method for generating target state |ut〉. Here we introduce another
standard quantum algorithm named as Quantum Swap Test (Buhrman et al., 2001), which could
estimate the square overlap between two quantum states ∣φ) and ∣ψ). The circuit of the Quantum
Swap Test is illustrated in Figure 1.
Figure 1: Circuit of the Quantum Swap Test
As shown in Figure 1, Quantum Swap Test performs the operation:
∣0)∣φ)∣ψ) → (H ③ I)(|0〉〈0| ③ I + |1〉⑴③ USWAP)(H ③ I)∣0)∣φ)∣ψ).
The final state could be written as:
11
2 ∣0)(∣φ)∣ψ) + IΨ)IΦ)) + 2 ∣1)(∣Φ)∣Ψ)-∣Ψ)∣Φ)).
(44)
(45)
The USWAP gate could be implemented in time O (polylog(d)), which performs the swap transfor-
mation ∣φ)∣ψ) → ∣ψ)∣φ) for d-dimensional state ∣φ) and ∣ψ). The measurement on the first qubit
20
Under review as a conference paper at ICLR 2020
produces 0 with probability P0 = 1 (1+ ∣(φ∣ψ)∣2). Thus, by replacing step 2-3 in Algortihm 6 with
the Quantum Swap Test operation, we could build an algorithm to estimate the square of the state
overlap.
Proposition 5. There exists a quantum algorithm which could present e-estimation to value b2 =
|〈ut|si)|2 with probability at least 1 - δ in running time O(TInputpolylog(d)e-2 log(1∕δ)), where
TInput is the time complexity to generate states |ut〉 and |si〉.
In order to estimate values bi = 〈ut |si〉for i ∈ [r], we need to discriminate the positive and negative
of bi. Note that for state |ut〉, the state | -ut〉 is also a target state which shares the same eigenvalue.
So both states |ut〉 and | - ut〉 are legal outputs and indistinguishable for our algorithm in Section 3.
Thus we analysis the value bi = sgn(ut(k))〈ut |si〉 as the overlap between states |ut〉 and |si〉, where
ut(k) is the k-th component of vector ut. Generally k could be any index such that the corresponding
component is non-zero.
Note that b = Sgn(Utk))〈ut®〉= -Sgn(〈ut|hk〉)〈ut|si〉. Define two states ∣ψ+) = 亡(|hk〉+
|hg(i)〉) and	∣ψ-〉=	Z-(∣h⅛)一	|hg(i)〉)，where	Z±	are normalized constants such that	Z±	=
2 ± 2〈hk |hg(i)〉. Then there is:
I(ut∣ψ+)∣2 =	Z2	[〈ut|hk〉2 +〈Ut|hg(i)〉2 +	2〈ut|hQ〈ut|hg(i)〉],	(46)
∣(ut∣ψ-)∣2 =	z12-	[〈ut|hQ2 +〈ut|hg(i)〉2 -	2(ut|hk〉〈ut|hg(i)〉].	(47)
States ∣ψ+) and ∣ψ-> could be generated by step 2-4 in Algorithm 6. The overlap (h⅛ |hg(i)〉could
be estimated by Algorithm 6. The square overlap ∣(ut∣ψ+)∣2 and ∣(ut∣ψ-)∣2 could be estimated by
Quantum Swap Test. Thus for |〈ut|hg(i))| > J one could discriminate the positive and negative
of〈u/hk〉(Ut|hg(i)〉by calculate the value Z+ ∣(ut∣ψ+)∣2 — Z-∣(ut∣ψ-)∣2. The estimation on
the square overlap ∣(ut∣ψ+)∣2 and ∣(ut∣ψ-)∣2 need to have the precision e'∣(ut∣h⅛)|/2, which takes
time O(TInputd-C. For |〈ut |hg(i))| ≤ e', 0 is an d estimation to〈ut |hg(i)). Since an 6’-estimation
to |bi| could be achieved by an e'2-estimation to b which takes time O(TInputd-4), we could derive
the time complexity of estimating bi in Theorem 10.
Theorem 10. There exists a quantum algorithm which could present e-estimation to value b =
Sgn(Utk))(ut|si〉with probability at least 1 — δ in running time O(TInputpolylog(d)e-4 log(1∕δ)),
where TI nput is the time complexity to generate states |ut〉 and |si〉.
A. 12 GENERATE THE HESSIAN MATRIX
Obtaining the Hessian matrix of a general objective function is computationally expensive. However,
the calculation can be simplified in special case. For example, consider the binary classification
problem with the sample set Z = {(xj , yj)|j ∈ [n]} ⊂ Rd × {0, 1}. Consider the non-convex loss
function with form: F (x, w) = [h(θ) - y]2, where h : R → R is the non-linear function such that
h(θ) = 1+；_@ and θ = WTx + b. Denote Xji) as the ith component of Xj. The total loss function
on the set S is defined as f (w) = n En=I F (xj, w) = 1 Ej=I fj(w).
There is:
∂fj
∂ w(i)
2(h(θj) - yj) ^hj
2(h(θj ) - yj )
e-θj	∂θj
(1 + e-% )2 ∂W(i)
2(h(θj ) - yj )
-θj
(1 + e-θj )2
x(ji)
e
(48)
21
Under review as a conference paper at ICLR 2020
∂wfk) = 2∂⅛) (h(θj …)U⅛ *)
∂(h(θj)-%)	e-θj
∂w⑻	(1 + e-j )2
2
∂	e-θj
+ (h(θj) - y3)	e
jj∂w(k) (1 + e-θj)
2
x(ji)
e-%	x(k)	e-%
(1 + e-θj ft (1 + e-% )2
+ (h(θj) - yj)
(-1) (e% - e-θj) e-2θj (
(1 + e-θj )4	Xj
x(ji)
-2θj
2x"Xk)“ q θ.、4 [1 - (h(θ) -yMje-θj)].
(1 + e-θj)
(49)
Define
e-2θj
αj ≡ 2 ---Γ^4 [1 - (h(θj) - yj ) (e内-e-θ) )] ,	(5O)
(1 +e-θj)
we have:
▽W fj = αj Xj Xj.	(51)
Thus we could derive the Hessian matrix for the total loss function:
nn
H = n E ▽w fj = n E jXT.	(52)
j=1	j=1
Each term in the sum above is a rank-1 matrix, which means the rank of H is bounded by n. Thus,
for sparse samples {Xj }jn=1 with at most s non-zero elements, we could derive the Hessian as the
form {(i, j, hij |hij ∕= 0)} in time O(ns2). Note that in practical case, d, the dimension of the sample
Xj , can be millions large (for example, the number of pixels on an image), while n, the number of
sample involved in one iteration of the update, is relatively small.
22