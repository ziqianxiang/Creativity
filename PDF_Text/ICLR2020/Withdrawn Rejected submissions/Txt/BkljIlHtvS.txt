Under review as a conference paper at ICLR 2020
Decoupling Adaptation from Modeling with
Meta-Optimizers for Meta Learning
Anonymous authors
Paper under double-blind review
Ab stract
Meta-learning methods, most notably Model-Agnostic Meta-Learning (Finn et al.,
2017) or MAML, have achieved great success in adapting to new tasks quickly,
after having been trained on similar tasks. The mechanism behind their success,
however, is poorly understood. We begin this work with an experimental analysis
of MAML, finding that deep models are crucial for its success, even given sets of
simple tasks where a linear model would suffice on any individual task. Further-
more, on image-recognition tasks, we find that the early layers of MAML-trained
models learn task-invariant features, while later layers are used for adaptation, pro-
viding further evidence that these models require greater capacity than is strictly
necessary for their individual tasks. Following our findings, we propose a method
which enables better use of model capacity at inference time by separating the
adaptation aspect of meta-learning into parameters that are only used for adap-
tation but are not part of the forward model. We find that our approach enables
more effective meta-learning in smaller models, which are suitably sized for the
individual tasks.
1	Introduction
Meta-learning or learning to learn is an appealing notion due to its potential in addressing important
challenges when applying machine learning to real-world problems. In particular, learning from
prior tasks but being able to to adapt quickly to new tasks improves learning efficiency, model
robustness, etc. A promising set of techiques, Model-Agnostic Meta-Learning (Finn et al., 2017)
or MAML, and its variants, have received a lot of interest (Nichol et al., 2018; Lee & Choi, 2018;
Grant et al., 2018). However, despite several efforts, understanding of how MAML works, either
theoretically or in practice, has been lacking (Finn & Levine, 2018; Fallah et al., 2019).
For a model that meta-learns, its parameters need to encode not only the common knowledge ex-
tracted from the tasks it has seen, which form a task-general inductive bias, but also the capability
to adapt to new test tasks (similar to those it has seen) with task-specific knowledge. This begs the
question: how are these two sets of capabilities represented in a single model and how do they work
together?
In the case of deep learning models, one natural hypothesis is that while knowledge is represented
distribUtedly in parameters, they can be localized - for instance, lower layers encode task-general
inductive bias and the higher layers encode adaptable task-specific inductive bias. This hypothesis is
consistent with one of deep learning’s advantages in learning representations (or featUre extractors)
Using its bottom layers.
Then we mUst ask, in order for a deep learning model to meta-learn, does it need more depth than
it needs for solving the target tasks? In other words, is having a large capacity to encode knowledge
that is Unnecessary post-adaptation the price one has to pay in order to be adaptable? Is there a way
to have a smaller (say, less deep) meta-learnable model which still adapts well? This qUestion is of
both scientific interest and practical importance - a smaller model has a smaller (memory) footprint,
faster inference and consUmes less resoUrces.
In this work, throUgh empirical stUdies on both synthetic datasets and benchmarks Used in the litera-
tUre, we investigate these qUestions by analyzing how well different learning models can meta-learn
and adapt. We choose to focUs on MAML dUe to its popUlarity.
1
Under review as a conference paper at ICLR 2020
Our observations suggest depth is indeed necessary for meta-learning, despite the tasks being solv-
able using a shallower model. Thus, applying MAML to shallower models does not result in suc-
cessful meta-learning models that can adapt well. Moreover, our studies also show that higher layers
are responsible more for adapting to new tasks while the lower layers are responsible for learning
task-general features.
Our findings prompt us to propose a new method for meta-learning. The new approach introduces
a meta-optimizer which learns to guide the (parameter) optimization process of a small model. The
small model is used for solving the tasks while the optimizer bears the burden of extracting the
knowledge of how to adapt. Empirical results show that despite using smaller models, the proposed
algorithm with small models attains similar performance to larger models which use MAML to
meta-learn and adapt.
We note that a recent and concurrent work to ours addresses questions in this line of inquiry (Raghu
et al., 2019). They reach similar conclusions through different analysis and likewise, they propose a
different approach for improving MAML. We believe our work is complementary to theirs.
2	Related Work
Meta-learning, or learning-to-learn, is a vibrant research area with a long and rich history, lying at
the intersection of psychology (Maudsley, 1980; Biggs, 1985), neuroscience (Hasselmo & Bower,
1993), and computer science(Schmidhuber, 1987; Thrun & Pratt, 1998) Of particular interest to this
manuscript is the line of work concerned with optimization-based meta-learning (OBML) algorithms
in the few-shot regime, of which MAML is a particular instance. (Finn & Levine, 2018; Finn et al.,
2017; Finn, 2018)
Since its inception, MAML has been widely applied to tackle the few-shot learning challenge, in
domains such as computer vision (Lee et al., 2019), natural language processing (Gu et al., 2018),
and robotics (Nagabandi et al., 2018). It is also the basis of extensions for continual learning (Finn
et al., 2019), single- and multi-agent reinforcement learning (Rothfuss et al., 2018; Al-Shedivat et al.,
2017), objective learning (Chebotar et al., 2019), transfer learning (Kirsch et al., 2019), and domain
adaptation (Li et al., 2018). Due to its generality, the adaptation procedure MAML introduces -
which is the focus of our analysis - has recently been branded as Generalized Inner Loop Meta-
Learning. (Grefenstette et al., 2019)
While popular in practical applications, relatively few works have analysed the convergence and
modelling properties of those algorithms. Finn & Levine (2018) showed that, when combined with
deep architectures, OBML is able to approximate arbitrary meta-learning schemes. Fallah et al.
(2019) recently provided convergence guarantees for MAML to approximate first-order stationary
points for non-convex loss surfaces, under some assumptions on the availability and distribution
of the data. Other analyses (empirical or theoretical) have attempted to explain the generalization
ability of OBML (Guiroy et al., 2019; Nichol et al., 2018), the bias induced by restricting the num-
ber of adaptation steps (Wu et al., 2018), or the effect of higher-order terms in the meta-gradient
estimation(Foerster et al., 2018; Rothfuss et al., 2019)
Closely related to our proposed methods are works attempting to improve the adaptation mecha-
nisms of OBML. Meta-SGD (Li et al., 2017) meta-learns per-parameter learning rates, while Alpha
MAML (Behl et al., 2019) adapts those learning rates during adaptation via gradient-descent. Meta-
Curvature (Park & Oliva, 2019) propose to learn a Kronecker-factored pre-conditioning matrix to
compute fast-adaptation updates. Their resulting algorithm is a special case of one of our methods,
where the linear transformation is not updated during adaptation. Another way of constructing pre-
conditioning matrices is to explicitly decompose all weight matrices of the model in two separate
components, as done in T-Nets (Lee & Choi, 2018). The first component is only updated via the
evaluation loss, while the second is also updated during fast-adaptation. Warped Gradient Descent
(Flennerhag et al., 2019) further extends T-Nets by allowing both components to be non-linear func-
tions. Instead of directly working with gradients, (Chebotar et al., 2019; Xu et al., 2018) suggest to
directly learn a loss function which is differentiated during fast-adaptation and results in faster and
better learning. Additionally, meta-optimizers have also been used for meta-descent (Sutton, 1981;
Jacobs, 1988; Sutton, 1992b;a; Schraudolph, 1999). They can be learned during a pre-training phase
2
Under review as a conference paper at ICLR 2020
(Andrychowicz et al., 2016; Li & Malik, 2017a;b; Metz et al., 2019b;a;c; Wichrowska et al., 2017)
or online (Kearney et al., 2018; Jacobsen et al., 2019; Ravi & Larochelle, 2017).
Our work differentiates from the the above by diagnosing and attempting to address the entangle-
ment between modelling and adaptation in the meta-learning regime. We uncover a failure mode of
MAML with linear and smaller models, and propose an effective solution in the form of expressive
meta-optimizers.
3	Analysis of MAML
3.1	Background
In MAML and its many variants (Lee & Choi, 2018; Nichol et al., 2018; Li et al., 2017), we have
a model whose parameters are denoted by θ. We would like to optimize θ such that the resulting
model can adapt to new and unseeen tasks fast. To this end, we are given a set of (meta)training
tasks, indexed by τ. For each such task, we associate with a loss Lτ (θ).
Distinctively, MAML minimizes the expected task loss after an adaptation phase, consisting of a
few steps of gradient descent from the model’s current parameters. Since we do not have access to
the target tasks to which we wish to adapt to, we use the expected loss over the training tasks,
LMETA =	E [Lτ (θ — αVLτ(θ))]	(1)
T 〜p(τ)
where the expectation is taken with respect to the distribution of the training tasks. α is the learning
rate for the adaptation phase. The right-hand-side uses only one step gradient descent such that the
aim is to adapt fast: in one step, we would like to reduce the loss as much as possible. In practice, a
few more steps are often used.
3.2	Analysis
Shallow models can be hard to meta-learn Many intuitive explanations for why MAML works
exist. One appealing suggestion is that the minimizer of the meta-learning loss LMETA is chosen
in such a way that it provides a very good initialization for the adaptation phase; however, if the
model is shallow such that Lτ is convex in its parameters, then any initialization that is good for
fast adapting to one subset of tasks could be bad for another subset of tasks since all the tasks have
precisely one global minimizer and those minimizers can be arbitrarily far from each other. When
the test tasks are distributed similar to the training tasks, the ideal initialization point has to be the
“mean” of the minimizers of the training tasks — the precise definition of the mean is not important,
as we will see below.
We illustrate a surprising challenge by studying MAML on a synthetic dataset and the Omniglot
task (Lake et al., 2015). Specifically, for the former study, we construct a set of binary classification
tasks by first randomly sampling datapoints w ∈ R100 from a standard Gaussian and use each of
them to define a linear decision boundary of a binary classification task. We assume the boundaries
pass through the origin and we sample training, validation and testing samples by randomly sampling
data points from both sides of the decision boundaries.
By construction, a linear model such as logistic regression is sufficient to achieve very high accuracy
on any task. But can MAML learn a logistic regression model from a subset of training tasks that is
able to adapt quickly to the test tasks?
Note that due to the random sampling of the training tasks, the average of the minimizers (ie, the
samples from the Gaussian distribution) is the origin. Likewise, for a set of test tasks randomly
sampled the same way, the origin provides the best initialization by not favoring any particular task.
Figure 1 reports the 1-step post-adaptation accuracy on the test tasks for the meta-learned logistic
regression model. Surprisingly, the model fails to perform better than chance. Despite the simplicity
of the tasks, logistic regression models are unable to find the origin as an initialization that adapts
quickly to a set of test tasks that are similar to the training tasks.
Figure 1 also reports how being deep can drastically change the behavior of MAML. There, we add
a 4-layer linear network (LinNet) to the logistic regression model (before the sigmoid activation).
3
Under review as a conference paper at ICLR 2020
Synthetic
Figure 1: Meta learning results on two datasets: Synthetic binary classification (left) and Omniglot (right).
The models to be meta-learned are shallow ones ( logistic regression models for binary classification and multi-
nomial logistic regression) and deep ones - the shallow models With linear networks (LinNet) added to them.
While the shallow models do not adapt and achieve chance level accuracies on average, the overparameterized
versions attain significantly better results after adaptation.
Omniglot
Note that while the model has the same representational capacity as a linear logistic regression, it is
overparameterized and has many local optimizers. As such, MAML can train this model such that
its 1-step adaptation accuracy reaches 92% on average.
We observe the same phenomena on meta-learning with MAML on the Omniglot dataset (details of
the dataset are given in the Section 5). The shallow logistic regression model achieves 45% accuracy
on average (for the 5-way classification tasks) after 2-step adaptation from the meta-learned initial-
ization. However, with a linear network, the adapted model achieves significantly higher accuracy -
70% on average, while having the same modeling capacity as the logistic regression.
In summary, these experiments suggest that even for tasks that are solvable with shallow models,
a model needs to have enough depth in order to be meta-learnable and to adapt. We postpone the
description of our experiments on nonlinear models to section 5, where we also show that having
sufficient depth is crucial for models to be meta-learnable, even when the tasks require fewer layers.
A natural question arises: if being deep is so important for meta-learning on even very simple tasks,
what different roles, if any, do different layers of a deep network play?
Depth enables task-general feature learning and fast adaptation We hypothesize that for deep
models meta-learned with MAML, lower layers learn task-invariant features while higher layers are
responsible for fast-adaptation.
To examine this claim, we meta-train a model consisting of four convolutional layers (C1 - C4) and
a final fully-connected layer (FC) on Omniglot (Lake et al., 2015) and CIFAR-FS (Bertinetto et al.,
2019). (Experimental setups are detailed in Section 5.) Once the model has finished meta-training,
we perform a layer-wise ablation to study each layer’s effect on adaptation. In particular, we iterate
over each layer and perform two sets of experiments. In the first, we freeze the weights of the layer
such that it does not get updated during fast-adaptation - we call it freezing only this. In the second
experiment, we freeze all layers but the layer such that this layer is updated during fast-adaptation
一 we call it adapting only this. The left two plots in Figure 2 report the average accuracy over 100
testing tasks from both datasets.
We observe that freezing only the first few lower layers (C1-C3) does not cause noticeable degra-
dation to the post-adaptation accuracy. In fact, as long as the last convolutional layer (C4) is not
frozen, post-adaptation accuracy remains unaffected. This indicate that C1-C3 provide information
that is task-invariant, while C4 is crucial for adaptation.
This does not mean FC is not important — since adapting C4 requires gradients passing through
the FC layer, it cannot be arbitrary. In fact, in the rightmost plot of the figure, C1-C3 are held fixed
during adaptation. While C4 and FC are allowed to adapt, and FC is perturbed with noise. When
4
Under review as a conference paper at ICLR 2020
&e-n:Da UO一ss-ep吓κ0d
&e-ro专 UO一ss-ep'κod
&e-ro专 UO一ss-ep'κod
Freezing C1-C3, Perturbing FC
0 2-4 2-3 2-2 2-1 20 21
Noise Scaling Factor
Figure 2: Layer-wise study of each layer’s effect on adaptation, in the left and the middle plots. The rightmost
plot shows that both C4 and FC have to work together to adapt. See texts for details.
the noise is strong, the performance degrades significantly. Thus, we conclude that both C4 and FC
play important roles in the mechanism for fast adaptation. We note that the recent work by Raghu
et al. (2019) concurently reached similar conclusions on the mini-ImageNet dataset, using feature
similarity-based analyses of the model’s representations.
These observations highlight a fundamental issue: the property of being meta-learnable entails more
model capacity than being learnable for a specific task. Thus, MAML can fail on models that lack
the capacity to encode both task-general features and adaptation information, even when the mod-
els themselves are powerful enough to perform well on each of the tasks used for the meta-learning
procedure. For example, with linear models (e.g. logistic regression), the parameters are forced to
overlap and serve both modelling and adaptation purposes. However, as soon as the models are over-
parameterized, the extra layers enable meta-learnability. In section 5, we show that this observation
also applies to nonlinear models where MAML-trained models quickly lose their performance when
the number of layers is reduced.
4 Meta-Optimizer for Meta Learning
The previous section has shown that MAML, when applied to deep learning models, meta-learns
both task-general features and task-specific adaption parameters at the same time. Since both are
represented in a single model where strong dependencies exist between the lower layers and the
higher layers, the model needs to be large enough and all its parameters have to be used to solve the
test tasks. In some cases, the model has a modelling capacity that is bigger than what the test tasks
require. For a model where linear network layers exist, the layers can be collapsed after adaptation
so that the actual model used for inference on teh test tasks is small. However, it is not clear how to
do so for typical deep models where nonlinearities prevent collapsing the model into smaller ones.
Can we have a different adaptation mechanism such that a smaller model, whose modelling ca-
pacity is still adequate for the test tasks, can be adapted to find the minimizers of its loss on the
aforementioned tasks?
In this section, we describe a new approach of learning meta-optimizers for meta-learning. The
meta-optimizer aims to separate modelling from adaptation. The meta-optimizer transforms the
parameter update process for the model so the model can converge fast to its minimizer. How to
transform is, however, learned from meta-training tasks. Because of this separation, the model for
the task does not have to know how to adapt; it only needs to have enough capacity (roughly, big
enough to represent the target task).
We note that classical tools could be used to compress a large model for a given task. However,
in the scenario where meta-learning is often used, such tools are unlikely effective. For example,
distillation requires a lot of labeled data from the target task, which is not suitable for few-shot
learning tasks. Pruning often degrades performance.
5
Under review as a conference paper at ICLR 2020
4.1	Learnable Meta-Optimizers
A meta-optimizer - or learnable optimizer - is a parameterized function Uξ defining the model's
parameter updates. For example, a linear meta-optimizer might be defined as:
Uξ(g) = Ag + b,
(2)
where ξ = (A, b) is the set of parameters of the linear transformation. The objective is to jointly
learn the model and optimizer’s parameters in the hope of accelerating the minimization of Lτ .
Concretely, we alternate between model and optimizer updates such that their values at adaptation
step t + 1 are given by:
θt+ι = θt- Uξt (VθtLT (θt))
ξt+ι= ξt- αVξtLT (θt+ι).
(3)
(4)
While this change to the classical optimization pipeline might seem innocuous at first, it bears its
own set of computational challenges. First, it is common for modern machine learning models
to have millions of parameters. In this case, simply storing the optimizer’s parameters becomes
infeasible even for the simplest of models, such as the linear one outlined above. Because of this
dimensionality issue most of the current literature considers a per-parameter independent update
function, thus greatly limiting the expressivity of the meta-optimizer. We propose to address the
issue of parameter dimensionality via factorizations of the optimizer’s parameters, which we detail
in the following subsection.
4.2	Factorized Meta-Optimizers
To tackle the issue of parameter dimensionality we propose to factorize the optimizer’s parameters.
To demonstrate this, let us revisit the linear optimizer example above: we assume g ∈ Rk to be the
vectorization of a matrix gradient G ∈ Rn×m, such that Vec(G) = g and m ∙ n = k. By expressing
A = R> 0 L as the Kronecker product of small matrices R ∈ Rm×m and L ∈ Rn×n, the linear
update above can be efficiently computed as: (Bernstein, 2018, Section 9.1)
Uξ(g) = vec (L ∙ G ∙ R) + b,
(5)
where b ∈ Rk and ξ = (L, R, b). In the best case scenario where m = n = √k, the above
factorization requires O (k) memory as opposed to O k2 for the non-factored case. Similarly, the
matrix-product takes O (k√k) time complexity, while the non-factored version takes O (k2).
Note that this linear transformation can be used as a building block to implement more expressive
and non-linear meta-optimizers. For example, a fully-connected network meta-optimizer Uξ is the
composition of linear transformations UWi interleaved with activation function σ . If the weight
matrices (W1, . . . , Wh) of the network admit a Kronecker-factorization Wi = Ri> 0 Li, the fully-
connected meta-optimizer is imlemented as:
Uξ (g) = UWh ◦.・・◦ σ ◦ Uwι ◦ g
=vec (Lhσ (...σ (Li ∙ G ∙ Ri)...) Rh),
(6)
(7)
where ξ = (Wi , . . . , Wh). Such a scheme can be used to implement arbitrary meta-optimizers -
such as convolutional or recurrent ones - so long as the architecture involves a composition of linear
maps. We refer the reader to Appendix A.3 for pseudo-code and schematics of the model-optimizer
loop.
We emphasize that the choice ofa Kronecker factorization is arbitrary; many matrix decompositions
work equivalently well and result in different modeling and computational trade-offs. For example,
using a low-rank Cholesky factorization A = LLT where L ∈ Rk×r allows to interpolate between
6
Under review as a conference paper at ICLR 2020
Table 1: Accuracies and standard deviation of Meta-Learning of Linear Models
Dataset	LR w/ MAML	LR + LINNET w/ MAML	LR W/ KLIN	LR W/ KFC
Synthetic	51.38% ± 3.91	95.77% ± 0.41	99.00% ± 0.08	98.98% ± 0.10
Omniglot	46.50% ± 2.80	71.31% ± 4.52	63.88% ± 3.93	71.56% ± 4.40
computational complexity and decomposition rank by tuning the additional hyper-parameter r. The
Cholesky decomposition might be preferable to the Kronecker one in memory-constrained applica-
tions, since r can be used to control the memory requirements of the meta-optimizer. Moreover,
such a decomposition imposes symmetry and positiveness on A, which might be desirable when
approximating the Hessian or its inverse.
In this work, we prefered the Kronecker decomposition over alternatives for three reasons: (1)
the computational and memory cost of the KroneCker-ProdUCt are acceptable, (2) R> 0 L is full-
rank whenever L, R are full-rank, and (3) the identity matrix lies in the span of Kronecker-factored
matrices. In particular, this last motivation allows meta-optimizers to recover the gradient descent
update by letting R, L be the identity.
5	Experiments
In our experiments, we complement our empirical studies and analysis from Section 3 with further
results and address the following question: can we meta-learn smaller (or shallower) models that
perform as well as larger (or deeper) ones? To this end, we apply the proposed approach of learning
meta-optimizers to the example synthetic dataset, as well as popular benchmark datasets: Omniglot
(Lake et al., 2015), mini-ImageNet (Ravi & Larochelle, 2017), and CIFAR-FS (Bertinetto et al.,
2019). All hyper-parameters and additional experimental details are available in Appendix A.1.
Unless otherwise noted, all models are trained using MAML. For models that use learnable deep
optimizers, we meta-learn both model and optimizer parameters. Similarly, both sets of parameters
are adapted during fast-adaptation.
To differentiate, we add the name of the optimizer to the model. For example, LR w/ KLin cor-
responds to a logisitic regression model with the Kronecker-factorized linear transformation from
the previous section. Similarly, W/ KFC indicates a model optimized by Kronecker-factored Fully-
Connected network in the previous section.
5.1	Meta-Learning Linear Models
We adopt the same setting as in section 3 where we study the synthetic data and the Omniglot
dataset. For the Omniglot dataset, we consider the 1-shot, 5-way setup of Finn et al. (2017) and
let the model adapt for 2 steps. Recall that the logistic regression model does not meta-learn while
overparameterized logistic regression models (ie, models with linear networks) do.
Table 1 reports the final post-adaptation testing accuracies and their standard deviations. As can
be observed, while logistic regression (LR) cannot be meta-learned, overparameterized models
(LinNet) as well as LR with our meta-optimizers (LR w/ KLin and LR w/ KFC) can all meta-
learn while our approaches lead to similar or better results compared to LinNet.
Note that since linear networks can always be collapsed into a single weight matrix and bias term,
any increase in performance is the consequence of better adaptation, rather than better modelling.
5.2	Non-linear models
We examine whether our meta-optimizers can meta-learn successfully with smaller but deep nonlin-
ear models to match bigger deep models. We focus on two datasets: the Omniglot where the setting
is 10-way classification with 5-shots and 4 adaptation steps, using the original 4-layer convolutional
network (CNN) of Finn et al. (2017), and the CIFAR-FS dataset (Bertinetto et al., 2019), doing 10-
way classification with 3-shots and 2 adaptation steps. The model is a CNN similar to the one for
the Omniglot, but only uses 32 hidden units as opposed to 64.
7
Under review as a conference paper at ICLR 2020
Figure 3: The effect of the number of convolutional layers on adaptation performance. While the model
becomes smaller, the performance drops. However, our approach degrades in a slower pace, as the adaptation
is shared by both the model and the meta-optimizer, thus leaving more freedom for the model to represent the
tasks.
Table 2: Post-adaptation accuracies by different meta-optimizers adapting a small CNN.
Meta-Learner Architecture
Task	UPPER B OUND	W/ MAML	W/ MSGD	W/ MC	W/ KFC
Omniglot	95.09% ± 2.32	60.94% ± 1.54	69.25% ± 0.81	85.70% ± 0.91	91.13% ± 0.93
CIFAR-FS	63.94% ± 3.89	38.44% ± 0.87	45.96% ± 1.73	52.89% ± 1.43	52.94% ± 1.33
mini-ImageNet	58.45% ± 5.58	18.55% ± 0.46	26.59% ± 1.43	29.74% ± 0.74	27.40% ± 0.60
We report numerical results in detail in Table A2 in the Appendix, which are graphically presented
in Figure 3. We vary the number of convolutional layers. As the number of the layers decrease, the
adaptation performance by both MAML and our approach (KFC) decrease. However, our approach
has a much slower degradation rate. In fact, our approach is generally able to adapt a smaller model
to the same level of performance as a MAML-trained model with an additional layer.
5.3	Comparison to other meta-optimizers
We also compare our approach of learning meta-optimizers to other approaches, notably Meta-SGD
(Li et al., 2017) and Meta-Curvature (Park & Oliva, 2019). Both of these methods attempt to ap-
proximate the expected task-loss landscape curvature and can be seen as ablations of our methods;
Meta-Curvature corresponds to KLin without adapting the optimizer, while Meta-SGD approxi-
mates Meta-Curvature with a diagonal matrix.
To ease the computational burden, we use a smaller CNN with 2 convolutional layers (SCNN) model
for adaptation. As an upper bound on adaptation performance, we train the model on individual
tasks to convergence and report the average final accuracy. Additionally, we include results on the
mini-ImageNet dataset for all methods, in the 10-way 1-shot with 5 adaptation steps setting (Ravi &
Larochelle, 2017).
Table 2 reports post-adaptation testing accuracies. All methods of learning optimizers are able to
adapt better than SCNN w/ MAML. In particular, our approachs perform the best and comes closest
to approaching the upper bound of performance (SCNN).
6	Conclusion
We introduce our approach by analyzing the success and failure modes of optimization-based meta-
learning methods. Namely, we find that, when successful, these methods tend to learn task-general
features in early layers and adaptable parameters/update functions in the later layers. Moreover, we
find that this learning fails when model size is reduced, indicating that optimization-based meta-
learning methods rely on the ability to encode task-general features and/or adaptable parameters,
8
Under review as a conference paper at ICLR 2020
even when the model itself is adequate for learning on the individual tasks. As such, we introduce
our method for decomposing modelling from adaptation using factored meta-optimizers. These
meta-optimizers enable the forward model to use more capacity on learning task-specific features,
while the expressiveness of their updates allows the forward model to adapt quickly to different
tasks. We find that our approach is able to enable successful meta-learning in models that do not
work with traditional optimization-based meta-learning methods.
9
Under review as a conference paper at ICLR 2020
References
Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel.
Continuous adaptation via meta-learning in nonstationary and competitive environments. arXiv
preprint arXiv:1710.03641, 2017.
Marcin Andrychowicz, Misha DeniL Sergio Gomez, Matthew W Hoffman, David Pfau, Tom
Schaul, Brendan Shillingford, and Nando de Freitas. Learning to learn by gradient de-
scent by gradient descent. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 3981—
3989. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/
6461-learning-to-learn-by-gradient-descent-by-gradient-descent.
pdf.
Harkirat Singh Behl, Atilim GUnes Baydin, and Philip H S Torr. Alpha MAML: Adaptive Model-
Agnostic Meta-Learning. arXiv preprint arXiv:1905.07435, May 2019.
Dennis S Bernstein. Scalar, Vector, and Matrix Mathematics: Theory, Facts, and Formulas - Revised
and Expanded Edition. Princeton University Press, revised, expanded edition edition, February
2018.
Luca Bertinetto, Joao F. Henriques, Philip Torr, and Andrea Vedaldi. Meta-learning with differen-
tiable closed-form solvers. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=HyxnZh0ct7.
JB Biggs. THE ROLE OF METALEARNING IN STUDY PROCESSES. Br. J. Educ. Psychol., 55
(3):185-212, November 1985.
Yevgen Chebotar, Artem Molchanov, Sarah Bechtle, Ludovic Righetti, Franziska Meier, and Gau-
rav S. Sukhatme. Meta-learning via learned loss. CoRR, abs/1906.05374, 2019. URL http:
//arxiv.org/abs/1906.05374.
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. On the convergence theory of Gradient-
Based Model-Agnostic Meta-Learning algorithms. arXiv preprint arXiv:1908.10400, August
2019.
Chelsea Finn. Learning to Learn with Gradients. PhD thesis, UC Berkeley, July 2018.
Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradi-
ent descent can approximate any learning algorithm. In International Conference on Learning
Representations, 2018. URL https://openreview.net/forum?id=HyjC5yWCW.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for fast adap-
tation of deep networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning
Research, pp. 1126-1135, International Convention Centre, Sydney, Australia, 2017. PMLR.
Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning.
In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th Interna-
tional Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Re-
search, pp. 1920-1930, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL http:
//proceedings.mlr.press/v97/finn19a.html.
Sebastian Flennerhag, Andrei A Rusu, Razvan Pascanu, Hujun Yin, and Raia Hadsell. Meta-
Learning with warped gradient descent. arXiv preprint arXiv:1909.00025, August 2019.
Jakob Foerster, Gregory Farquhar, Maruan Al-Shedivat, Tim Rocktaschel, Eric Xing, and Shimon
Whiteson. Dice: The infinitely differentiable monte carlo estimator. In International Conference
on Machine Learning, pp. 1524-1533, 2018.
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-
based meta-learning as hierarchical bayes. In International Conference on Learning Representa-
tions, 2018. URL https://openreview.net/forum?id=BJ_UL-k0b.
10
Under review as a conference paper at ICLR 2020
Edward Grefenstette, Brandon Amos, Denis Yarats, Phu Mon Htut, Artem Molchanov, Franziska
Meier, Douwe Kiela, Kyunghyun Cho, and Soumith Chintala. Generalized inner loop meta-
learning, 2019.
Jiatao Gu, Yong Wang, Yun Chen, Victor O. K. Li, and Kyunghyun Cho. Meta-learning for
low-resource neural machine translation. In Proceedings of the 2018 Conference on Empir-
ical Methods in Natural Language Processing, pp. 3622-3631, Brussels, Belgium, October-
November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1398. URL
https://www.aclweb.org/anthology/D18-1398.
Simon Guiroy, Vikas Verma, and Christopher Pal. Towards understanding generalization in
Gradient-Based Meta-Learning. arXiv preprint arXiv:1907.07287, July 2019.
M E Hasselmo and J M Bower. Acetylcholine and memory. Trends Neurosci., 16(6):218-222, June
1993.
Robert A Jacobs. Increased rates of convergence through learning rate adaptation. Neural Netw., 1
(4):295-307, January 1988.
Andrew Jacobsen, Matthew Schlegel, Cameron Linke, Thomas Degris, Adam White, and Martha
White. Meta-descent for online, continual prediction. In AAAI Conference on Artificial Intelli-
gence, 2019.
Alex Kearney, Vivek Veeriah, Jaden B Travnik, Richard S Sutton, and Patrick M Pilarski.
TIDBD: Adapting temporal-difference step-sizes through stochastic meta-descent. arXiv preprint
arXiv:1804.03334, April 2018.
Louis Kirsch, Sjoerd van Steenkiste, and Jurgen Schmidhuber. Improving generalization in meta
reinforcement learning using learned objectives. arXiv preprint arXiv:1910.04098, 2019.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332-1338, December 2015.
Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with
differentiable convex optimization. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 10657-10665, 2019.
Yoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric
and subspace. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th Interna-
tional Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Re-
search, pp. 2927-2936, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL
http://proceedings.mlr.press/v80/lee18a.html.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-
learning for domain generalization. In Thirty-Second AAAI Conference on Artificial Intelligence,
2018.
Ke Li and Jitendra Malik. Learning to optimize. In International Conference on Learning Repre-
sentations, 2017a. URL https://openreview.net/forum?id=ry4Vrt5gl.
Ke Li and Jitendra Malik. Learning to optimize neural nets. arXiv preprint arXiv:1703.00441,
March 2017b.
Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-SGD: Learning to learn quickly for
Few-Shot learning. arXiv preprint arXiv:1707.09835, July 2017.
Donald B Maudsley. A THEORY OF META-LEARNING AND PRINCIPLES OF FACILITA-
TION: AN ORGANISMIC PERSPECTIVE. 1980.
Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein. Meta-learning up-
date rules for unsupervised representation learning. In International Conference on Learning
Representations, 2019a. URL https://openreview.net/forum?id=HkNDsiC9KQ.
11
Under review as a conference paper at ICLR 2020
Luke Metz, Niru Maheswaranathan, Jeremy Nixon, Daniel Freeman, and Jascha Sohl-Dickstein.
Understanding and correcting pathologies in the training of learned optimizers. In Kamalika
Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference
on Machine Learning, volume 97 of Proceedings ofMachine Learning Research, pp. 4556-4565,
Long Beach, California, USA, 09-15 JUn 2019b. PMLR. URL http://proceedings.mlr.
press/v97/metz19a.html.
Luke Metz, Niru Maheswaranathan, Jonathon Shlens, Jascha Sohl-Dickstein, and Ekin D Cubuk.
Using learned optimizers to make models robust to input noise. arXiv preprint arXiv:1906.03367,
June 2019c.
Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine,
and Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta-
reinforcement learning. arXiv preprint arXiv:1803.11347, 2018.
Alex Nichol, Joshua Achiam, and John Schulman. On First-Order Meta-Learning algorithms. arXiv
preprint arXiv:1803.02999, March 2018.
Eunbyung Park and Junier B Oliva. Meta-Curvature. arXiv preprint arXiv:1902.03356, February
2019.
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse?
towards understanding the effectiveness of MAML. arXiv preprint arXiv:1909.09157, September
2019.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International
Conference on Learning Representations, 2017, 2017.
Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Promp: Proximal
meta-policy search. arXiv preprint arXiv:1810.06784, 2018.
Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. ProMP: Proxi-
mal meta-policy search. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=SkxXCi0qFX.
Juergen Schmidhuber. Evolutionary Principles in Self-Referential Learning. PhD thesis, 1987.
Nicol N Schraudolph. Local gain adaptation in stochastic gradient descent. 1999.
Rich Sutton. Adaptation of learning rate parameters. Goal Seeking Components for Adaptive Intel-
ligence: An Initial Assessment, 1981.
Richard S Sutton. Gain adaptation beats least squares. In Proceedings of the 7th Yale workshop on
adaptive and learning systems, volume 161168, 1992a.
Richard S. Sutton. Adapting bias by gradient descent: An incremental version of delta-bar-delta.
In AAAI, pp. 171-176, 1992b. URL http://www.aaai.org/Library/AAAI/1992/
aaai92-027.php.
Sebastian Thrun and Lorien Pratt (eds.). Learning to Learn. Kluwer Academic Publishers, Norwell,
MA, USA, 1998.
Olga Wichrowska, Niru MahesWaranathan, Matthew W. Hoffman, Sergio Gomez Colmenarejo,
Misha Denil, Nando de Freitas, and Jascha Sohl-Dickstein. Learned optimizers that scale and
generalize. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
3751-3760, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR. URL
http://proceedings.mlr.press/v70/wichrowska17a.html.
Yuhuai Wu, Mengye Ren, Renjie Liao, and Roger Grosse. Understanding short-horizon bias in
stochastic meta-optimization. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=H1MczcgR-.
12
Under review as a conference paper at ICLR 2020
Zhongwen Xu, Hado P van Hasselt, and David Silver. Meta-gradient reinforcement learn-
ing. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 2396-
2407. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
7507-meta-gradient-reinforcement-learning.pdf.
13
Under review as a conference paper at ICLR 2020
A	Appendix
A.1 Experimental Details and Hyper-parameters
The following paragraphs describe the data-generation process, model architectures, and learning
hyper-parameters for experiments in Section 5. Meta-optimizers’s parameter matrices are always
initialized to the identity.
A.1.1 Linear Model Experiments
Synthetic Binary Classification The task decision boundaries W 〜N(0.0,lιoo) are sampled
from standard Gaussian. Each one of the 1,000 data point X 〜N(0.0, Iioo) is also sampled from the
standard Gaussian, and is assigned a label from {-1, 1} based on the sign of w>x. All models are
trained for 30,000 iterations of stochastic gradient descent (without momentum) so as to minimize
the 1-step MAML loss LMAML. The task loss L(θ) is the binary cross-entropy. Each stochastic
gradient is estimated on a single task. (i.e. the meta-batch size is 1.)
For the logistic regression (LR) results, we used meta and adapatation learning rates of 0.01 and 0.5.
The linear network (LR+LinNet) consists of 3 hidden layers of 64 units, and is trained with both
meta- and fast-adaptation learning rates set to 1.9. Experiments using the Kronecker-factored linear
meta-optimizer also use the same meta- and fast-adaptation learning rate, set to 3.33. The meta-
optimizer itself consists of 10,001 parameters. (m = 100, n = 1) The Kronecker-factored fully-
connected meta-optimizer uses a single hidden layer with rectified linear-units (ReLU) activation
functions, for a total of 20,002 parameters. (m = 100, n = 1) It has both learning rates set to 3.9.
None of the meta-optimizers use a bias term.
Omniglot Our Omniglot experiments exactly replicate the setup of Finn et al. (2017). Of the
1623 classes, we designate 1,200 for meta-training, 100 for validation, and 423 for testing. We then
generate 20,000 training tasks, 600 validation/testing tasks, each consisting of five classes containing
a single character image, possibly rotated by 90, 180, or 270 degrees. All models are trained using
Adam for 60,000 iterations, an iteration consisting of32 tasks for which the model is allowed 2 steps
of fast-adaptation.
The logistic regression model uses a meta-learning rate of 0.0005 and an adaptation learning-rate
set to 1.0. The linear network uses 4 hidden layers of 256, 128, 64, and 64 units and flattens the
character images into a 784-dimensional vector before processing them. The meta-learning rate was
set to 0.0005 and the adaptation learning rate to 0.08. For the KLin experiment (LR w/ KLin), we
use a meta-learing rate of 0.003 and set the adaptation learning rate to 0.9. The KLin optimizer
consists of 614k parameters, with m = 784, n = 5. The KFC meta-optimizer (LR w/ KFC) consists
of 4 hidden layers (2.5M parameters) with ReLU activations, whose meta and adaptation learning
rates are set to 0.001 and 0.01, respectively. Again, none of the meta-optimizer use biases.
A.1.2 Nonlinear Model Experiments
For the non-linear model experiments, we learn one KFC optimizer per layer of the model. Each
KFC optimizer is based on the same architecture, but learns its own set of parameters. Moreover, we
separate processing of the magnitude and direction of the model’s gradient as follows: the gradient
is initially scaled so as to have unit norm before being fed to the KFC network. Once the normalized
gradient has been processed, it is rescaled by the initial normalizing constant times a learnable
factor. (This learnable factor is initialized to 1.) We found this architecture to help in situations
where the model is allowed several adaptation steps. For convolutional models, we flatten the height
and width weight dimensions such that the optimizer is shared across filters of a same layer. (i.e. if
the convolutional layer has shape NxCxHxW, we have m = C, n = HW .) Note that, again, none
of the meta-optimizers use a bias term.
Omniglot The dataset replicates the processing of linear model experiments, detailed above. In-
stead of 5 ways and 1 shot, we use 10 ways and 5 shots. All models are allowed 4 steps of fast
adaptation.
14
Under review as a conference paper at ICLR 2020
The 4-layer CNN network (CNN w/ MAML) uses a meta-learning rate of 0.003, and an adaptation
learning rate of 0.5. Its architecture replicates the one described in Finn et al. (2017). The 2-layer
CNN (SCNN w/ MAML) uses the same first two layers and same input dimensionality to the FC
layer as the 4-layer CNN, with meta and adapation learning rates set to 0.003 and 0.8, respectively.
The 4-layer CNN has a total of 112,586 parameters, while the 2-layer only 38,474. We used the
same learning rates for the Meta-SGD, Meta-Curvature, and KFC optimizers: a meta learning rate
of 0.003, and an adaptation learning rate of 0.5. The KFC architecture consists of 4 layers, such that
the meta-optimizers contains a total of 134,171 parameters for the 2-layer CNN model and 267,451
parameters for the 4-layer CNN.
CIFAR-FS We obtained the splits created by Bertinetto et al. (2019) and exactly reproduced their
preprocessing setting for our experiments on CIFAR-FS. We also split the 100 classes in 64 training,
16 validation, and 20 testing classes and generate 20,000 training tasks, and 600 validation and
testing tasks. We consider the 10-ways 3-shots setup with 2 fast-adaptation steps. As opposed to
Bertinetto et al. (2019), our model closely resembles the one of our Omniglot experiments. Its
main difference with the model described in prior work is the absence of max-pooling layers, and
averaging of the last two convolutional feature dimensions before the fully-connected layer. Doing
so allows us to conduct our ablation studies on the effect of our meta-optimizers with different
number of convolutional layers, while keeping the input dimensionality of the fully-connected layer
constant.
The 4- and 2-layer CNNs use meta- and fast-adaptation learning rates of 0.003 and 0.7, respectively.
The 4-layer CNN has a total of 29,226 parameters, while the 2-layer only 10,602. Meta-SGD, Meta-
Curvature, and KFC also use a 0.003 meta-learning rate, but the adaptation learning rate is decreased
to 0.5. The KFC optimizer consists 4 layers with ReLU activations, for a total of 69,325 parameters
for the 4-layer CNN model and 35,117 parameters for the 2-layer CNN.
mini-ImageNet As for the Omniglot experiment, we replicate the mini-ImageNet setting and
model from Finn et al. (2017). We consider the 10-ways, 1-shot and 5 fast-adaptation steps set-
ting.
The 4-layer CNN and 2-layer CNN both use a meta-learning rate of 0.0005 and a fast-adaptation
learning rate of 0.07. The 4-layer CNN has a total of 36,906 parameters, while the 2-layer only
18,282. The Meta-SGD, Meta-Curvature, and KFC meta-optimizer use the same meta-learning rate,
but a smaller fast-adaptation learning rate set to 0.1 for Meta-SGD, 0.09 for Meta-Curvature, and
0.005 for KFC. For the 2-layer CNN model, the 4-layer KFC optimizer consists of approximately
2.60M parameters, while for the 4-layer CNN model it consists of 2.63M parameters.
15
Under review as a conference paper at ICLR 2020
A.2 Additional Experimental Results
This section provides additional experimental evidence supporting the claims in the main text.
A.2.1 Effect of width on adaptability
Synthetic ι layer
---w=2
---w=4
---w= 8
---w= 16
---W= 32
---W= 64
——W= 128
---w=256
1	2	3
Iterations ιθ4
Synthetic 2 layers
>lo
io.8
<
U 0.6
o
⅛
⅛0∙4
ra
P
S 0.2
S
o
d
0.0
m ---- w=2
/n ---- w=4
JS - w=8
	 w=16
∣∣Λ∣*r	— w=32
---w=64
——w=128
---w=256
1	2	3
Iterations ιθ4
Synthetic 3 layers
---w=2
---W=4
---W= 8
---W= 16
---W= 32
---w=64
——W= 128
---w=256
1	2	3
Iterations ie4
Synthetic 4 layers
---w=2
---w=4
---w=8
---w=16
---w=32
---w=64
——w=128
---w=256
1	2	3
Iterations 1θ4
Figure A.1: Effect of width on adaptability on the synthetic binary classification setting. We vary
the width and depth of linear networks and report their post-adaptation accuracy. As long as the
model has one or more hidden layers, the model is able to adapt to the tasks regardless of the layers’
width.
In Section 3, we argue that depth is a curcial ingredient to enable gradient-based meta-learning.
In particular, we show that even on simple, linearly separable binary classification tasks a logistic
regression model (i.e. with no hidden layer) fails to solve the meta-learning problem. Using the
same toy experiment, we now argue that width plays a relatively less important role with respect to
adaptability.
In Figure A.2, we plot the post-adaptation accuracy for different number of hidden layers, and vary
their width w = 2, 4, . . . , 256. As can be seen, all models are able to eventually solve the meta-
learning problem regardless of width, so long as the model has one or more hidden layers.
A.2.2 Do layers C1-C3 behave similarly to FC
Scaling Cl
fe-ln8-∙UO-ss'->∙lSod
Scaling C2
Scaling C3
Scaling C4
Scaling FC
fe-ln8-∙UO-ss'->∙lSod
Pre-AdaPtatiOn
Post-Adaptation
0	20	21	22	23	24	25
Weight Scaling Factor
0	20	21	22	23	24	25
Weight Scaling Factor
0	20	21	22	23	24 25
Weight Scaling Factor
0	20	21	22	23	24	25
Weight Scaling Factor
0 8 6 4 2
.....
10000
e-ln8uoeepW-Sod
20	21	22	23	24	25
Weight Scaling Factor
Figure A.2: Weight scaling experiment for the meta-trained models on Omniglot (top) and CIFAR-
FS (bottom). We observe that the modelling layers (C1-C3) are insensitive to parameter scaling
regardless of whether it is applied pre- or post-adaptation, indicating they serve modelling purposes.
On the other hand, parameter scaling on the adaptation layers (C4 & FC) quickly degrades post-
adaptation accuracy if applied pre-adaptation, but has virtually no effect if applied post-adaptation.
In turn, this underlines the importance of these layers for adaptation.
In Section 3, we claim that early layers in deep meta-trained models are purposed for modelling,
while latter layers are responsible for fast-adaptation. This conclusion is reached through the ar-
16
Under review as a conference paper at ICLR 2020
gument that (a) layers C1-C3 can be completely omitted from the adaptation dynamics, and (b)
perturbing the last FC layer results in large post-adaptation accuracy degradation.
We now present further evidence that both layer groups (i.e. C1-C3 and C4-FC) indeed serve differ-
ent purposes. For each layer of a meta-trained model, we multiply the weights of the selected layer
by a constant factor either pre- or post-adaptation. Intuitively, layers that affect the fast-adaptation
update will incur a large post-adaptation accuracy penalty as the scaling magnitude increases, due
to the value of their weights playing an important part in the computation of the adaptation gradient.
On the other hand, a constant scaling of the weights should have little effect on the modelling ability
of the network, due to the relative magnitude of activation being maintained. (Note that this intuition
is somewhat blurred for our models, as they make use of batch normalization.)
Figure A.2 reports post-adaptation accuracy for those experiments. As predicted, scaling the weights
of C1-C3 has little impact on accuracy, whether applied pre- or post-adaptation. Clearly, those layers
do not affect the computation of the fast adaptation update. Similarly, scaling the weights of C4-FC
post-adaptation does not impact the modelling ability of the network, and post-adaptation accuracy
does not suffer. However, the same rescalings prove to be disastrous when applied pre-adaptation;
the model is unable to adapt and accuracy drops by as much as 85% for Omniglot and 40% for
CIFAR-FS. Evidently, those layers are central to successful fast-adaptation.
A.2.3 Additional Methods for the Non-linear Model Experiments
In this section, we detail additional methods and apply them to the non-linear models of Section 5.
Cholesky-factored Meta-Optimizers As alluded in Section 4, the choice of Kronecker decom-
position is arbitrary. We now describe an alternative implementation of meta-optimizers, based
on a low-rank Chholesky decomposition. The rank r Cholesky decomposition consists of a ma-
trix L ∈ Rn×r, such that a rank r symmetric positive definite matrix A ∈ Rn×n is expressed as
A = LL>. Using this decomposition, we can write the linear transformation of Section 4 as:
Ag+b= (LL>)g+b= L(L>g) +b,
(8)
where b ∈ Rn×1 is the bias term and g ∈ Rn×1 is the vectorization ofa weight’s gradient. As for the
Kronecker product, the latter expression of the linear transformation does not require to construct a
large n × n matrix. Additionally, multiple linear transformations can be chained or interleaved by
activation functions in order to obtain more expressive and adaptable meta-optimizers.
A major attribute of the Cholesky decomposition is its flexibility in terms of memory requirements.
For example, by letting r = 1 we force the meta-optimizer to have as many weights as the model.
This attribute can in turn be a disadvantage: as we show in our experiments below, r plays an
important role in the expressivity of the meta-optimizer, and should be treated as an additional
hyper-parameter to tune. A second inconvenience stems from the choice of initialization scheme of
the meta-optimizer; for the Kronecker product, we initialize the Kronecker factors to the identity,
thus recovering hypergradient descent in the first meta-training batch. However, since the identity
matrix In does not lie in the span of low-rank Cholesky factorizable matrices, the initialization
scheme becomes another decision left to the practitioner. For our subsequent experiments, We find
that letting L follow a Glorot initialization (i.e. Lij 〜N(0, gain ∙ Jn)) to work well.
Table A1: Post-adaptation accuracies by different meta-optimizers adapting a small CNN.
Meta-Learner Architecture
Task	W/ MAML	W/ PRUNE	W/ CFC1	W/ CFC10	W/ KFC
Omniglot	60.94% ± 1.54	70.06% ± 8.87	72.62% ± 1.12	79.56% ± 1.24	91.13% ± 0.93
CIFAR-FS	38.44% ± 0.87	10.00% ± 0.05	32.11% ± 0.98	47.81% ± 0.91	52.94% ± 1.33
mini-ImageNet	18.55% ± 0.46	12.20% ± 0.13	16.13% ± 1.29	21.48% ± 1.57	27.40% ± 0.60
We compare the low-rank Cholesky-factored meta-optimizers to the Kronecker ones in Table A1.
We denote by CFC1 the rank-1 Cholesky-factored fully-connected network, while CFC10 indicates
17
Under review as a conference paper at ICLR 2020
the rank-10 version. CFC10 contains approximately as many parameters as KFC. (c.f. Table ??)
While models trained with CFC do not outperform their KFC counter-parts, they offer a competitive
alternative to existing methods.
Effect of Pruning We study the effect of post-adaptation pruning of a meta-trained model. Ide-
ally, pruning provides an attractive alternative to explicitly separating adaptation from modelling for
learning lean models: practitioners could meta-train a larger model, adapt it to the desired target
task, and finally prune it. We implement this pipeline by pruning weights smaller than a predefined
threshold and report results in Figure A.3. As can be observed, pruning the 4-layer CNN models to
obtain the equivalent number of parameters to a 2-layer SCNN can be catastrophic. On Omniglot,
the pruned model reaches 70.06% post-adaptation accuracy, more than 20% lower than using meta-
optimizers. On CIFAR-FS and mini-ImageNet , pruning can barely outperform chance prediction,
reaching on average 10.00% and 12.20%, respectively. These results are also available in Table A1,
in the w/ PRUNE column.
0.8
0.6
0.4
0.2
0.0
0.0	0.15	0.25	0.35
Weight Pruning Threshold
Weight Pruning Threshold
mini-ImageNet
Figure A.3: Effect of pruning weights smaller than a given threshold on the 4-layer networks for
Omniglot (left), CIFAR-FS (middle), and mini-ImageNet (right). The respective pruning thresholds
to obtain the same number of parameters as the SCNN are 0.30 (Omniglot), 0.5 (CIFAR-FS), and
0.075 (mini-ImageNet). For those pruning values, the accuracies are much lower than the ones
obtained by separating the adaptation mechanism from the model.
18
Under review as a conference paper at ICLR 2020
A.3 Additional Visualizations
In this section, we provide visualizations of experimental results complementary to the ones pre-
sented in the main text.
e -e —%”北⑻)
(a) Model-Loss-Optimizer Loop.
(aι,ιB … altnB、
：■：
%n,l>B ∙ ∙ ∙ 5n,∏Bj
(b) Kronecker product.
Figure A.4: Schematic of the model-loss-optimizer loop and of the Kronecker product.
Synthetic
0.5	1.0	1.5
Iterations le4
Omniglot
Figure A.5: Evolution of test accuracies along training for our methods with linear models on the
synthetic data binary classification (left) and Omniglot (right).
Table A2: Accuracies and standard deviation of Meta-Learning of Non-Linear Models
Omniglot	CIFAR-FS
Num. Layers	CNN	CNN W/ KFC	CNN	CNN W/ KFC
1	29.20% ± 1.13	66.45% ± 1.42	24.86% ± 0.47	42.02% ± 0.52
2	60.94% ± 1.54	91.13% ± 0.93	38.44% ± 0.87	52.94% ± 1.33
3	91.69% ± 0.70	96.57% ± 0.86	50.91% ± 0.80	53.07% ± 0.58
4	97.56% ± 0.45	98.49% ± 0.41	55.03% ± 1.23	55.97% ± 0.59
19
Under review as a conference paper at ICLR 2020
1.0
0.8
&0.6
ro
U-
U
⅛θ4
0.2
0.0
Omniglot
---CNN w/ MAML
SCNN w/ MAML
—— SCNN w/ MSGD
——SCNN w/ MC
——SCNN w/ KFC
0	2	4	6
Iterations ie4
Omniglot
1.0
0.8
---CNN w/ MAML
SCNN w/ MAML
——SCNN w/ MSGD
——SCNN w/ MC
——SCNN w/ KFC
u
⅛θ∙4
0.2
0.0
0	2	4	6
Iterations ie4
(a)	Omniglot - Train Tasks
(b)	Omniglot - Test Tasks
Figure A.6: Evolution for the training (left) and testing (right) accuracies of the non-linear models
on Omniglot.
1.0
0.0
CIFAR-FS
0.8
0.2
A
⅛0∙6
苫0.4
0	2	4	6
Iterations ie4
1.0
0.8
A
⅛0∙6
苫0.4
0.2
CIFAR-FS
——CNN w/ MAML
SCNN w/ MAML
——SCNN w/ MSGD
——SCNN w/ MC
---SCNN w/ KFC
0	2	4	6
Iterations ie4
(a)	CIFAR-FS - Train Tasks
(b)	CIFAR-FS - Test Tasks
Figure A.7: Evolution for the training (left) and testing (right) accuracies of the non-linear models
on CIFAR-FS.
mini-lmageNet
1.0
0.8
皮0.6
10
U-
U
2 0.4
0.2
0.0
mini-lmageNet
(b) miniImageNet - Test Tasks
——CNN w/ MAML
SCNN w/ MAML
——SCNN w/ MSGD
——SCNN w/ MC
0	2	4	6
Iterations ie4
(a) miniImageNet - Train Tasks
Figure A.8: Evolution for the training (left) and testing (right) accuracies of the non-linear models
on mini-ImageNet .
20
Under review as a conference paper at ICLR 2020
Algorithm 1 Meta-Learning with Meta-Optimizers
Require: Fast learning rate α, Initial parameters θ(init) and ξ(init), Optimizer Opt
1:	while θ(init) , ξ(init) not converged do
2:	Sample task T 〜p(τ)
3:	θ1 = θ(init) ,	ξ1 = ξ(init)
4:	for step t = 1, . . . , T do
5:	Compute loss Lτ (θt)
6:	Compute gradients VθtLτ(θt) and VξtLτ(θt)
7:	Update the meta-optimizer parameters ξt+ι = ξt 一 αVξtLT(θt)
8:	Compute the model update Uξt+1 (Vθt Lτ (θt))
9:	Update the model parameters θt+1 = θt 一 Uξt+1 (VθtLτ(θt))
10:	end for
11:	Update model and meta-optimizer initializations
12:	θ(init) 一 θ(init) 一 Opt(Vθ(init)Lτ(θτ))
13:	ξ(init) J ξ(init) 一 Opt(Vξ(init)LT(θτ))
14:	end while
A.4 Computational Metrics
This section provides computational metrics for the non-linear experiments in Table A3. All metrics
are measured using a single NVIDIA Titan XP.
Table A3: Computational metrics for each of the methods. The timing metrics measure the time to
compute one meta-gradient step on an entire meta-batch. Inference time is identical across methods
for models of the same size.
Task	Method	Model Parameters	Optimizer Parameters	Time	Accuracy
Omniglot	CNN W/ MAML	112,586	0	1.09s	97.56%
	W/ KFC	112,586	267,451	10.35s	98.49%
	SCNN W/ MAML	38,474	0	0.58s	60.94%
	W/ KFC	38,474	134,171	5.44s	91.13%
	W/ CFC1	38,474	8,905	4.30s	72.62%
	W/ CFC10	38,474	66,379	4.36s	79.56%
CIFAR-FS	CNN W/ MAML	29,226	0	0.63s	55.03%
	W/ KFC	29,226	69,325	2.04s	55.97%
	SCNN W/ MAML	10,602	0	0.25s	38.44%
	W/ KFC	10,602	35,117	1.08s	52.94%
	W/ CFC1	10,602	4,185	0.83s	32.11%
	W/ CFC10	10,602	34,317	0.84s	47.81%
mini-ImageNet	CNN W/ MAML	36,906	0	0.25s	29.20%
	W/ KFC	36,906	2.63M	1.75s	30.81%
	SCNN W/ MAML	18,282	0	0.14s	18.55%
	W/ KFC	18,282	2.60M	0.94s	27.40%
	W/ CFC1	18,282	42,585	0.69s	16.13%
	W/ CFC10	18,282	349,197	0.71s	21.48%
21