Under review as a conference paper at ICLR 2020
SGD with Hardness Weighted Sampling for
Distributionally Robust Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
Distributionally Robust Optimization (DRO) has been proposed as an alternative
to Empirical Risk Minimization (ERM) in order to account for potential biases in
the training data distribution. However, its use in deep learning has been severely
restricted due to the relative inefficiency of the optimizers available for DRO in
comparison to the wide-spread Stochastic Gradient Descent (SGD) based opti-
mizers for deep learning with ERM. In this work, we propose SGD with hardness
weighted sampling, a principled and efficient optimization method for DRO in
machine learning that is particularly suited in the context of deep learning. Sim-
ilar to an online hard example mining strategy in essence and in practice, the
proposed algorithm is straightforward to implement and computationally as effi-
cient as SGD-based optimizers used for deep learning. It only requires adding a
softmax layer and maintaining a history of the loss values for each training exam-
ple to compute adaptive sampling probabilities. In contrast to typical ad hoc hard
mining approaches, and exploiting recent theoretical results in deep learning opti-
mization, we prove the convergence of our DRO algorithm for over-parameterized
deep learning networks with ReLU activation and finite number of layers and pa-
rameters. Preliminary results demonstrate the feasibility and usefulness of our
approach.
1 Introduction
In standard deep learning pipelines, a neural network h with parameters θ is trained by minimizing
the mean of a per-example loss L over a training dataset {(xi, yi)}in=1, where xi are the inputs and yi
are the labels. This corresponds to Empirical Risk Minimization (ERM), defined as the non-convex
optimization problem
1n
argmin - Y^L (h(xi； θ), Yi)	(1)
θn	i
i=1
Since the empirical risk is equal to the expectation of the per-example loss over the empirical training
data distribution, an approximate solution of (1) can be obtained efficiently by Stochastic Gradient
Descent (SGD) with a uniform sampling over the training data (Bottou et al., 2018).
This approach has led to remarkable results in terms of average performance, but may lead to outliers
with high loss values compared to the average loss. Such cases can even be observed for elements
belonging to the training data set. This is because solutions of ERM are prone to ignore a few hard
eamples in order to obtain a low mean per-example loss.
In practice, outlier results have, for example, been consistently reported in the context of deep learn-
ing for brain tumor segmentation, as illustrated in the recent annual BRATS challenges (Bakas et al.,
2018). For safety-critical systems, such as those used in healthcare, where outliers must be avoided,
this is not satisfactory.
Efficient biased sampling methods, including Online Hard Example Mining (OHEM) (Shrivastava
et al., 2016; Loshchilov & Hutter, 2015; Chang et al., 2017) and weighted sampling (Bouchard et al.,
2015; Berger et al., 2018; Gibson et al., 2018), have been proposed to mitigate this issue. However,
even though these works typically start from an ERM formulation, it is not clear how those heuristics
actually relate to ERM in theory.
1
Under review as a conference paper at ICLR 2020
Distributionally Robust Optimization (DRO) is an alternative to ERM (1) that takes into account
uncertainty in the empirical data distribution. Formally, training a deep neural network with DRO
corresponds to the min-max non-convex-concave optimization problem
arg min max
θq
X qi L (h(xi; θ), yi) -
IX1 φ (nqi)
i=1
(2)
where φ is a convex function that defines a φ-divergence (Csiszar et al., 2004), q = @)；= Cor-
responds to an arbitrary sampling distribution over the training data, and β > 0 is a robustness
parameter. Instead of minimizing the mean per-example loss on the training dataset, DRO seeks the
hardest weighted empirical training data distribution around the (uniform) empirical training data
distribution. This suggests a link between DRO and OHEM.
The parameter β allows DRO to interpolate between ERM (β → 0) and the minimization of the
maximum per-example loss (β → +∞). Motivations for using the minimization of the maximum
per-example loss for safety-critical applications have been discussed in (Shalev-Shwartz & Wexler,
2016).
DRO as a generalization of ERM for machine learning has been studied in (Duchi et al., 2016;
Rafique et al., 2018; Namkoong & Duchi, 2016; Chouzenoux et al., 2019), but still lacks optimiza-
tion methods that are computationally as efficient as SGD in the non-convex setting of deep learning.
If one could efficiently solve the inner maximization problem in (2) for a given θ, DRO could be
addressed by alternating between this maximization problem and a minimization scheme akin to
the standard ERM (1), but over an adaptively weighted empirical distribution. However, even when
a closed-form solution is available for the inner maximization problem, it requires performing a
forward pass over the entire training dataset at each iteration. This cannot be done efficiently for
large dataset.
Previously proposed optimization methods for large-scale non-convex-concave problem of the form
of (2) are based on the min-max structure of the problem, and consist in alternating between approx-
imate maximization and minimization steps (Rafique et al., 2018; Lin et al., 2019; Jin et al., 2019).
However, they differ from SGD methods for ERM by the introduction of additional hyperparame-
ters for the optimizer such as a second learning rate and a ratio between the number of minimization
and maximization steps. As a result, DRO is difficult to use as a replacement of ERM in practice.
In addition, those min-max methods do not use the link between DRO and adaptive weighted sam-
pling, therefore departing from efficient heuristics used in hard example mining. From a theoretical
perspective, they further make the assumption that the model is either smooth or weakly-convex, but
none of those properties are true for deep neural networks with ReLU activation functions that are
largely used in practice.
In this work, we propose SGD with hardness weighted sampling, a novel, principled optimization
method for training deep neural networks with DRO (2) and inspired by OHEM. Compared to SGD,
our method only requires introducing an additional softmax layer and maintaining a history of the
stale per-example loss to compute sampling probabilities over the training data. Since the loss is
already computed at each iteration for SGD, our SGD with hardness weighted sampling for DRO
is computationally as efficient as SGD for ERM. In practice, we show that our method performs
favorably to SGD in the case of class imbalance.
We also formally link DRO in our method with OHEM (Shrivastava et al., 2016). As a result, our
method can be seen as a principled OHEM approach. In this context, the robustness parameter β
controls the trade-off between exploitation and exploration in the OHEM process.
Last but not least, we generalize recent results in the convergence theory of SGD with ERM and
over-parameterized deep learning networks with ReLU activation functions (Allen-Zhu et al., 2019;
2018; Cao & Gu, 2019; Zou & Gu, 2019) to our SGD with hardness weighted sampling for DRO.
This is, to the best of our knowledge, the first convergence result for deep learning networks with
ReLU trained with DRO.
2
Under review as a conference paper at ICLR 2020
2	Related Work in DRO with a Wasserstein Distance
In this work, We focus on DRO with a φ-divergence (Csiszar et al., 2004). In this case, the data
distributions that are considered in the DRO problem (2) are restricted to sharing the support of the
empirical training distribution. In other words, the weights assigned to the training data can change,
but the training data itself remains unchanged.
Another popular formulation is DRO with a Wasserstein distance (Sinha et al., 2017; Duchi et al.,
2016; Staib & Jegelka, 2017; Chouzenoux et al., 2019). In contrast to φ-divergences, using a Wasser-
stein distance in DRO seeks to apply small data augmentation to the training data to make the deep
learning model robust to small deformation of the data, but the sampling weights of the training data
distribution typically remains unchanged. In this sense, DRO with a φ-divergence and DRO with a
Wasserstein distance can be considered as orthogonal endeavours.
While we show that DRO with φ-divergence can be seen as a principled OHEM method, it has
been shown that DRO with a Wasserstein distance can be seen as a principled adversarial training
method (Sinha et al., 2017; Staib & Jegelka, 2017).
3	MACHINE LEARNING WITH DRO AND φ-DIVERGENCE
In machine learning based on Empirical Risk Minimization (ERM), a predictor h is trained using
a training dataset {(xi, yi)}in=1 to perform well on average on a task for which the performance is
measured on a per-example basis by a smooth criteria L. Note that parameter regularization terms
can easily be embedded in L since they are independent of the example. For ease of presentation, we
focus on the supervised machine learning setting ,where h : x 7→ y, and omit explicitly mentioning
any parameter regularisation term.
Let ∆n ⊂ Rn be the set of empirical weighted training data distributions defined according to a
given training dataset
∆n =	p = (pi)in=1 ∈ [0, 1]n | Xpi = 1
(3)
and let Pdata be the uniform empirical training data distribution, i.e. for all i, Pdata,i = n
Let θ be the set of parameters of the predictor h( . ; θ) : x 7→ y we want to train, and h : θ 7→
(h(xi; θ))in=1 be the vector of inferred outputs from the training data. We assume L is a smooth and
potentially non-convex function. We also denote L(h(θ)) = (L(h(xi; θ), yi))in=1.
Definition 3.1 (Mean Loss).
1n
M(L(h(θ))) = Epdata [L (h(x; θ), y)] = - ∑L (h(Xi； θ),阴)	(4)
The ERM predictor, as used in most learning settings, is obtained by minimizing the mean loss (4).
Definition 3.2 (Empirical Risk Minimization (ERM) predictor).
θ = arg min M(L(h(θ)))	(5)
θ
However, Pdata is typically biased compared to the true data distribution. Therefore, predictors
trained with ERM are prone to fail on new examples that are not well represented in the training
dataset.
Distributionally Robust Optimization (DRO) is an alternative to ERM that mitigates this issue by
encouraging robustness to bias in the empirical training data distribution. DRO, in its simplest form,
is based on the notion of φ-divergence that we use to induce robustness with respect to the set of all
the empirical distributions of the training dataset ∆n .
Definition 3.3 (φ-divergence). Letφ : R+ → R∪{+∞} be a closed, convex, lower semi-continuous
function such that ∀z ∈ R+, φ(z) ≥ φ(-) = 0. The φ-divergence Dφ is defined as, for all P =
(Pi)in=1, q = (qi)in=1 ∈ ∆n
dφ (qkp) = XPiφ (IL)	⑹
3
Under review as a conference paper at ICLR 2020
Example 3.1. For φ : z 7→ z log(z), Dφ is the Kullback-Leibler (KL) divergence:
Dφ(qkp)
n
DKL (qkp) =	qi log
i=1
(7)
And, for φ : z 7→ (z - 1)2, Dφ is the Pearson χ2 divergence:
n
Dφ(qkp) = χ2(qkp) = X
i=1
(qj -Pi)
Pi
(8)
Definition 3.4 (Distributionally Robust Loss).
R(L(h(θ))) = max Eq [L (h(x; θ), y)] - 1 Dφ(q∣∣pdata)
q∈∆n	β
n
max	qi L (h(xi; θ), yi)-
q∈∆n
(9)
1n
nβ Hφ (Uq
where β > 0 is a hyperparameter that controls the amount of robustness.
For a given φ-divergence, we define the DRO predictor, that is obtained by minimizing the distribu-
tionally robust loss (9) instead of the mean loss (4).
Definition 3.5 (Distributionally Robust Optimization (DRO) predictor).
θ = arg min R(L(h(θ)))	(10)
θ
DRO interpolates between ERM as β -→ 0 and the minimization of the maximum loss as β -→ ∞,
and is equivalent to a mean-variance trade-off when β -→ 0 small (Gotoh et al., 2018), i.e.
maχ (Eq [L(h (X； θ) , y)] - 1 Dφ(qkp>data)} = Epdata [L(h (X； θ) , y)]
q∈∆n ∖	β	J
β _	.....................
<	+2φβ(1) Vpdata [L(h (x; θ), y)] + o(β)	(11)
max (Eq [L(h (x； θ) ,y)] — 1 Dφ(qkPdata)) —————→ max L(h X； θ) ,yj
q∈∆n	β	β-→+∞ i
where Vpdata is the empirical variance.
Furthermore, we observe that the distributionally robust loss (9) is an upper bound to the mean loss
(4) (independently to the choice of φ and β), i.e. for all φ-divergence and all β > 0
∀θ,	M(L(h(θ)))≤ R(L(h(θ)))	(12)
We now make assumptions for the φ-divergence to simplify the derivations of our optimization
method for DRO (10) in the remainder of the paper.
Assumption 3.1 (Regularity of φ). φ : R+ → R is two times continuously differentiable on [0, n],
P-strongly convex on [0, n], i.e.: ∃ρ > 0,∀z,z0 ∈ [0, n], φ(z0) ≥ φ(z) + φ0(z)(z0 — z) + P (z — z0)2
and satisfies (see D.1 for a justification): ∀z ∈ R, φ(z) ≥ φ(1)= 0, φ0 (1)= 0.
These assumptions are verified by most φ-divergences used in practice (e.g. the KL divergence).
4 SGD with Hardness Weighted S ampling
4.1	Distributionally Robust Optimization with SGD and Adaptive Sampling
Existing optimization methods for DRO with a non-convex predictor h alternate between approxi-
mate minimization and maximization steps (Rafique et al., 2018; Jin et al., 2019; Lin et al., 2019),
requiring the introduction of additional hyperparameters compared to SGD. These are difficult to
4
Under review as a conference paper at ICLR 2020
tune in practice and convergence has not been proven for deep neural networks with ReLU activa-
tion functions.
In this section, we highlight properties that allows us to link DRO with SGD combined with adap-
tive sampling. Our analysis relies on Fenchel duality (Moreau, 1965) and the notion of Fenchel
conjugate (Fenchel, 1949) that we now define.
Definition 4.1 (Fenchel Conjugate Function). Let f : Rm → R ∪ {+∞} be a proper function. The
Fenchel conjugate of f is defined as ∀v ∈ Rm, f *(v) = maXχ∈Rm(v, Xi — f (x).
Let
∀p ∈ Rn, G(P)= βDφ(Pkptrain) + δ∆n (P)	(13)
where δ∆n is the characteristic function of the closed convex set ∆n, i.e.
∀P ∈ Rn,	δ∆ (P) =	0 ifP ∈ ∆n	(14)
,	∆n	+∞ otherwise
One can remark that the distributionally robust loss R (9) can be rewritten using the Fenchel conju-
gate function of G. This allows us to obtain regularity properties for R.
Lemma 4.1 (Regularity of R). Ifφ satisfies Assumption 3.1, then G and R satisfy the following:
nρ
G is -strongly convex
∀θ,	R(L(h(θ))) = max ((L(h(θ)), q〉— G(q)) = G* (L(h(θ)))
q∈Rn
R is f ɪ ) -gradient Lipschitz continuous.
nρ
(15)
(16)
(17)
Equation (16) follows from Definition 4.1. Proofs of (15) and (17) can be found in Appendix D.3.
According to (15), the optimization problem (16) is strictly convex and admit a unique solution in
∆n . Let us denote this solution
P(L(h(θ))) = argmax ((L(h(θ)), q — G(q))
q∈Rn
(18)
The following lemma shows that the gradient, with respect to θ, of the distributionally robust loss (9)
at a given θ can be rewritten as the expectation, with respect to the weighted empirical distribution
p(L(h(θ))), of the per-example loss gradient. We further show that straightforward analytical for-
mulas exist for P when relying on classical φ-divergences. This result motivates our Algorithm 4.1
for efficient training with the distributionally robust loss.
Lemma 4.2 (Stochastic Gradient of the Distributionally Robust Loss). For all θ, we have
p(L(h(θ))) = Vv R(L(h(θ)))
Vθ(R。£。h)(θ) = Ep(L(h(θ))) [Vθ L (h(x; θ), y)]
where Vv R is the gradient ofR with respect to its input.
(19)
The proof is found in Appendix D.4. It is apparent from (19) that, given p, an estimate of Vθ(R。
L ◦h) could easily be provided by sampling a batch according to P and estimating the per-example
loss gradients in the batch as per standard practice. We now provide closed-form formulas for P
given L(h(θ)) for the KL divergence and the Pearson χ2 divergence.
Example 4.1. For the KL divergence (i.e. φ : z 7→ z log(z) — z + 1), we have (see D.2 for a proof)
P(L(h(θ))) = SoftmaX (β L(h(θ)))
And for the Pearson χ2 divergence (i.e. φ : z 7→ (z — 1)2), we have:
∀i,
Pi(L(h(θ))) = ReLU (l (l + β (L(h(θ))i — 1XX L(h(θ))) j j
In both cases, we can verify consistency with (11) as
∀i∈{i,..∙,n},物团俏⑻))-→→ 1
(20)
(21)
(22)
5
Under review as a conference paper at ICLR 2020
Algorithm 1 SGD-HWS: SGD with Hardness Weighted Sampling for Kullback-Leibler DRO
1: Input: Training data {(xi, yi)}in=1, number of epochs T > 1, robustness parameter β > 0,
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
learning rate η > 0, batch size b ∈ {1, . . . , n}.
Initialization:
Initialise θ randomly
Initialise the loss history L = -1
Warm start:
// Split the training data into batches B and run one epoch with classic SGD
for {(xi, yi)}i∈I inBdo
// Run forward pass and store losses for all the samples in the batch
for i ∈ I do
L i — L(h(χi; θ),yi)
// Run backward pass and update the parameters of the model
θ 一 θ - η 1 Pi∈ι Vθ L(h(χi; θ), yi)
SGD with dynamic hardness weighted sampling:
for epoch = 2, . . . , T do
for iteration i = 1, ..., ([.] + 1)do
// Run softmax to update the sampling probabilities of the samples
p^ = Softmax(βL)
// Draw a batch with replacement using the probability distribution P
{(xi,yi)}i∈ι such that I i好.P and |I| = b
// Run forward pass and update losses for all the samples in the batch
for i ∈ I do
~ 一 ，-， 一、 、
Li J L(h(xi; θ),yi)
// Run backward pass and update the parameters of the model
θ J θ - η1 Pi∈ι Vθ L(h(xi; θ),yi)
Output: θ
4.2	Efficient Algorithm for Distributionally Robust Deep Learning
The second equality in (19) implies that Vθ L(hi(θ), yi) is an unbiased estimator of the distribu-
tionally robust loss gradient when i is sampled with respect to p(L(h(θ))). This suggests that the
distributionally robust loss can be minimized efficiently by SGD by sampling mini-batches with re-
spect to p(L(h(θ))) at each iteration. However, even though closed-form formulas were provided
for p, evaluating exactly L(h(θ)), i.e. doing one forward pass on the whole training dataset at each
iteration, is computationally prohibitive for large training dataset.
In practice, we propose to use a stale version of L(h(θ)) by maintaining an online history of the loss
values of the training examples during training L(h(xi; θ(ti)), yi) . Where for all i, ti is the last
iteration at which the per-example loss of example i has been computed. Using the Kullback-Leibler
divergence as φ-divergence, this leads to the SGD with hardness weighted sampling algorithm pro-
posed in Algorithm 4.1. This would also apply to the Pearson χ2 divergence mutatis mutandis.
In contrast to alternate min-max optimization methods, our SGD with an adaptive sampling strategy
is similar to the SGD-based optimizers used by the vast majority of deep learning practitioners.
Compared to standard SGD-based training optimizers for the mean loss, our algorithm requires only
an additional softmax operation per iteration and to store an additional vector of size n (number of
training examples), thereby making it ideally suited for deep learning applications.
4.3	Convergence of SGD with Hardness Weighted Sampling for
OVER-PARAMETERIZED DEEP NEURAL NETWORKS WITH ReLU
Convergence results for over-parameterized deep learning has recently been proposed in (Allen-
Zhu et al., 2019). Their work gives convergence guarantees for deep neural networks h with any
activation functions (including ReLU), and with any (finite) number of layers L and parameters m,
6
Under review as a conference paper at ICLR 2020
under the assumption that m is large enough. At the time of writing, this is the most realistic setting
for which a convergence theory of deep learning exists.
In this section, we extend the convergence theory developed by (Allen-Zhu et al., 2019) for ERM and
SGD to DRO and the proposed SGD with hardness weighted sampling (as stated in Algorithm 4.1).
Theorem 4.1 (Convergence of Algorithm 4.1 for over-parameterized neural networks with ReLU).
Let L be a smooth per-sample loss function, b ∈ {1, . . . , n} be the batch size, and > 0. If m is
large enough, and the learning rate is small enough, then, with high probability over the randomness
of the initialization and the mini-batches, Algorithm 4.1 finds ∣∣Vθ (R oLoh)(θ)k ≤ E aftera finite
number of iterations.
A more detailed version of this theorem is described in B.2 and the proof can be found in D.8.
4.4	DRO as Principled Online Hard Example Mining
In this section, we discuss the relationship between DRO and Online Hard Example Mining
(OHEM) (Shrivastava et al., 2016). SGD with an ad hoc adaptive sampling strategy is already
used in practice while starting from a mean loss optimization formulation in the OHEM litera-
ture (Loshchilov & Hutter, 2015; Shrivastava et al., 2016). Similarly to our algorithm, in OHEM
heuristics, the hard examples, those training examples with relatively high values of the loss, are
sampled more often. We formalize this in the following definition for OHEM sampling.
Definition 4.2 (Online Hard Example Mining Sampling). Any adaptive sampling method such that
the probability pi of sampling example xi is an non-decreasing function of the (potentially stale)
loss value associated with xi.
Theorem 4.2. The proposed hardness weighted sampling is a hard example mining sampling for
any φ-divergence that satisfies Assumption 3.1. In addition, the probability pi of sampling example
xi is an non-increasing function of the loss value associated with xj for all j 6= i.
See Appendix D.5 for the proof. The second part of Theorem 4.2 implies that as the loss of an
example diminishes, the sampling probabilities of all the other examples increase. As a result, the
proposed SGD with hardness weighted sampling balances exploitation (i.e. sampling the identified
hard examples) and exploration (i.e. sampling any example to keep the record of hard examples up
to date).
5	Experiments
We now illustrate the properties of our SGD with hardness weighted sampling described in Algo-
rithm 4.1 for training deep neural networks with ReLU activation functions for DRO (10).
5.1	Robustness to domain gap
We create a bias between training and testing data distribution of MNIST (LeCun, 1998) by keep-
ing only 1% of the digits 3 in the training dataset, while the testing dataset remains unchanged.
Implementation details can be found in Appendix A.1.
Comparison of ERM with SGD and DRO with our SGD with hardness weighted sampling for β =
10 at testing can be found in Figure 1. More values of β and the learning curves during training can
be found in Figure 2.
Our experiment suggests that DRO and ERM lead to different optima. Indeed, DRO for β = 10
outperforms ERM by more than 10% of accuracy on the under-represented class, as illustrated in
Figure 1. This suggests that DRO leads to better generalization than ERM. Especially, it appears that
DRO is more robust than ERM to domain gaps between the training and the testing dataset. In addi-
tion, Figure 1 suggests that DRO with our SGD with hardness weighted sampling can convergence
faster than ERM with SGD.
Furthermore, the variations of learning curves with β that can be found in Figure 2 are consistent
with our theoretical insight. As β decreases to 0, the learning curve of DRO with our Algorithm 4.1
converges to the learning curve of ERM with SGD.
7
Under review as a conference paper at ICLR 2020
Figure 1: Comparison of learning curves for ERM with SGD (blue) and DRO with our SGD with
hardness weighted sampling (red). The models are trained on an imbalanced MNIST dataset (only
1% of the digits 3 kept in the training datatset) and evaluated on the original MNIST testing dataset.
This suggests that our SGD with hardness weighted sampling is more robust than SGD to a domain
gap between the training and the testing dataset.
5.2	Stability of DRO
For large values of β (here β ≥ 10), instabilities appear in the testing learning curves at the
begining of training, as illustrated in Figure 1 and in the top panels of Figure 2. For ERM this
usually suggests that the learning rate is too high.
However, we observed that reducing the learning rate does not reduce those instabilities. The bottom
left panel of Figure 2 shows that the training loss curves for β ≥ 10 were actually stable there. We
also observe that during the iterations for which instabilities appear on the testing set, the standard
deviation of the per-sample loss on the training set increases and then decreases. Following (20) the
higher the standard deviation of the per-sample loss history, the more our weighted sampler focuses
on hard examples. Therefore, instabilities on the testing set during training with DRO are due to the
sampler focusing on hard examples.
6	Conclusion and Discussion
We have shown that efficient training of deep neural networks with Distributionally Robust Opti-
mization (DRO) with a φ-divergence (10) is possible. Our Stochastic Gradient Descent (SGD) with
hardness weighted sampling is a principled Online Hard Example Mining (OHEM) method. It is
as straightforward to implement, and as computationally efficient as SGD for Empirical Risk Min-
imization (ERM). It can be used for deep neural networks with any activation function (including
ReLU), and with any per-example loss function. We have shown that the proposed approach can
formally be described as a principled online hard example mining strategy. In addition, we prove
the convergence of our method for over-parameterized deep neural networks. Thereby, extending
the convergence theory of deep learning of (Allen-Zhu et al., 2019). This is, to the best of our
knowledge, the first convergence result for training a deep neural network based on DRO.
Our experiments on an imbalanced MNIST dataset illustrate the practical usefulness and feasibility
of our methods. SGD with hardness weighted sampling is more robust to domain gaps between the
training and the testing dataset and converges faster than SGD.
However, adapting acceleration methods for ERM with SGD, like momentum updates, to DRO
remains non-trivial because of the inner maximization of DRO. Investigating accelerated extension
of our method is left for future work.
8
Under review as a conference paper at ICLR 2020
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural
networks. arXiv preprint arXiv:1810.12065, 2018.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In ICML, pp. 242-252, 2019.
Spyridon Bakas, Mauricio Reyes, Andras Jakab, Stefan Bauer, Markus Rempfler, Alessandro Crimi,
Russell Takeshi Shinohara, Christoph Berger, Sung Min Ha, Martin Rozycki, et al. Identifying
the best machine learning algorithms for brain tumor segmentation, progression assessment, and
overall survival prediction in the brats challenge. arXiv preprint arXiv:1811.02629, 2018.
Yoshua Bengio. Practical recommendations for gradient-based training of deep architectures. In
Neural networks: Tricks of the trade, pp. 437-478. Springer, 2012.
Lorenz Berger, Hyde Eoin, M Jorge Cardoso, and Sebastien Ourselin. An adaptive sampling scheme
to efficiently train fully convolutional networks for semantic segmentation. In Annual Conference
on Medical Image Understanding and Analysis, pp. 277-286. Springer, 2018.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223-311, 2018.
Guillaume Bouchard, Theo Trouillon, Julien Perez, and Adrien Gaidon. Online learning to sample.
arXiv preprint arXiv:1506.09016, 2015.
Yuan Cao and Quanquan Gu. A generalization theory of gradient descent for learning over-
parameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019.
Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. Active bias: Training more accu-
rate neural networks by emphasizing high variance samples. In Advances in Neural Information
Processing Systems, pp. 1002-1012, 2017.
Emilie Chouzenoux, Henri Gerard, and Jean-Christophe Pesquet. General risk measures for robust
machine learning. arXiv preprint arXiv:1904.11707, 2019.
Imre Csiszar, Paul C Shields, et al. Information theory and statistics: A tutorial. Foundations and
TrendsR in Communications and Information Theory, 1(4):417-528, 2004.
John Duchi, Peter Glynn, and Hongseok Namkoong. Statistics of robust optimization: A generalized
empirical likelihood approach. arXiv preprint arXiv:1610.03425, 2016.
Werner Fenchel. On conjugate convex functions. Canadian Journal of Mathematics, 1(1):73-77,
1949.
Eli Gibson, Wenqi Li, Carole Sudre, Lucas Fidon, Dzhoshkun I Shakir, Guotai Wang, Zach Eaton-
Rosen, Robert Gray, Tom Doel, Yipeng Hu, et al. Niftynet: a deep-learning platform for medical
imaging. Computer methods and programs in biomedicine, 158:113-122, 2018.
Jun-ya Gotoh, Michael Jong Kim, and Andrew EB Lim. Robust empirical optimization is almost
the same as mean-variance optimization. Operations research letters, 46(4):448-452, 2018.
Jean-Baptiste Hiriart-Urruty and Claude Lemarechal. Convex analysis and minimization algorithms
I: Fundamentals, volume 305. Springer science & business media, 2013.
Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Minmax optimization: Stable limit points of
gradient descent ascent are locally optimal. arXiv preprint arXiv:1902.00618, 2019.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Tianyi Lin, Chi Jin, and Michael I Jordan. On gradient descent ascent for nonconvex-concave
minimax problems. arXiv preprint arXiv:1906.00331, 2019.
Ilya Loshchilov and Frank Hutter. Online batch selection for faster training of neural networks.
arXiv preprint arXiv:1511.06343, 2015.
9
Under review as a conference paper at ICLR 2020
Jean-JacqUes Moreau. Proximite et dualite dans Un espace hilbertien. Bulletin de la Societe
mathematique de France, 93:273-299,1965.
Hongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust
optimization with f-divergences. In Advances in Neural Information Processing Systems, pp.
2208-2216, 2016.
Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Non-convex min-max optimization:
Provable algorithms and applications in machine learning. arXiv preprint arXiv:1810.02060,
2018.
Shai Shalev-Shwartz and Yonatan Wexler. Minimizing the maximal loss: How and why. In ICML,
pp. 793-801, 2016.
Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors
with online hard example mining. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 761-769, 2016.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with
principled adversarial training. arXiv preprint arXiv:1710.10571, 2017.
Matthew Staib and Stefanie Jegelka. Distributionally robust deep learning as a generalization of
adversarial training. In NIPS workshop on Machine Learning and Computer Security, 2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. arXiv preprint arXiv:1906.04688, 2019.
10
Under review as a conference paper at ICLR 2020
A More on our Experiments
A.1 Experiments on MNIST
100.0
Testing Accuracy for All Digits (in %)
97.5
95.0
92.5
90.0
87.5
85.0
82.5
80.0
400
500
--ERM (beta=0.0, lr=0.01)
DRO (beta=0.1, lr=0.01)
——DRO (beta=1.0, lr=0.01)
——DRO (beta=10, lr=0.01)
DRO (beta=100, lr=0.01)
200	300
epoch
Training Loss
2.0-
1.5-
1.0-
0.5-
0.0-
0
50
250
300
100	150	200
epoch
Figure 2: Comparison of learning curves at testing (top panels) and at training (bottom panels) for
ERM with SGD (blue) and DRO with our SGD with hardness weighted sampling for different values
of β on MNIST (β = 0.1, β = 1, β = 10, β = 100). The models are trained on an imbalanced
MNIST dataset (only 1% of the digits 3 kept in the training datatset) and evaluated on the original
MNIST testing dataset.
A.1.1 Implementation Details
For our experiments on MNIST, we used a Wide Residual Network (WRN) (Zagoruyko & Ko-
modakis, 2016). The familly of WRN models has proved to be very efficient and flexible, achieving
state-of-the-art accuracy on several dataset. More specifically, we used WRN-16-1 (see Zagoruyko
& Komodakis, 2016, section 2.3).
For the optimization we used a learning rate of 0.01. No momentum or weight decay were used. No
data augmentation was used.
A.1.2 Comment on Early- S topping with ERM and DRO
It is worth noting that we do not use the knowledge of the training dataset bias in our method. For
a less obvious bias, we would have access to the accuracy on the under-representated subset of the
training set. As a result, in general, convergence criteria can be based only on the global measure of
accuracy (in our case the top left panel of Figure 2).
11
Under review as a conference paper at ICLR 2020
Early-stopping is a widely used heuristic for selecting the optimal number of epochs to prevent
overfitting (Bengio, 2012). Early-stopping crucially depends on the patience parameter. In the top
left panel of Figure 2, there is no improvement of the accuracy of ERM of more than 0.04% between
epoch 20 and 30. As a result, if the patience parameter is too low, ERM might be considered to
plateau at an epoch at which it ERM achieves an accuracy of 0% on the under-represented class.
This suggests two things. First, the mean accuracy is not a good criteria to decide when to stop the
training of ERM for safety-critical systems. Second, our SGD with hardness weighted sampling
converge faster than SGD to a safe solution, and is more robust to the hyperparameters of early-
stopping.
B	Convergence of SGD with Hardnes s Weighted Sampling for
OVER-PARAMETERIZED DEEP NEURAL NETWORKS WITH ReLU
(detailed statement)
In this section, we give a detailed statement of the convergence results presented in 4.3.
Our analysis is based on the results developed in (Allen-Zhu et al., 2019) which is a simplified
version of (Allen-Zhu et al., 2018). Improving on those theoretical results would automatically
improve our results as well. We focus on providing theoretical tools that could be used to generalize
any convergence result for ERM using SGD to DRO using Algorithm 4.1. In addition, we compared
our conditions and results the one obtained for ERM and SGD in (Allen-Zhu et al., 2019) and discuss
the implication of those differences.
Let us first state our assumptions on the neural network h, and the per-example loss function L.
Assumption B.1 (Deep Neural Network). In this section, we use the following notations and as-
sumptions similar to (Allen-Zhu et al., 2019):
•	h is a fully connected neural network with L + 2 layers, ReLU activation function, and m
nodes in each hidden layers
•	For all i ∈ {1, . . . , n}, we denote hi : θ 7→ hi(xi; θ) the output d dimensional scores of h
applied to example xi of dimension d.
•	θ = (θl)lL=+01 is the set of parameters of the neural network h, where θl is the set of weights
for layer l with θ0 ∈ Rd ×m, θL+1 ∈ Rm×d, and θl ∈ Rm×m for any other l.
•	(Data separation) It exists δ > 0 such that for all i, j ∈ {1, . . . , n}, if i 6= j, kxi - xj k ≥ δ.
•	We assume m ≥ Ω(d X poly(n, L, δ-1)) for some sufficiently large polynomial poly, and
δ ≥ O (-1). We refer the reader to (Allen-Zhu et al., 2019)for details about poly.
•	The parameters θ = (θl)lL=+01 are initialized at random such that:
-	[θo]i,j 〜N (0, m) for every (i,j) ∈ {1,...,m} × {1,..., d}
-	[θι]i,j 〜N (0, m) for every (i,j) ∈ {1, ...,m}2 and l ∈ {1,...,L}
—	[θ-+ι]i,j 〜N (O, d) for every (i,j) ∈ {1,..., d} × {1,..., m}
Assumption B.2 (Regularity of L). For all i, Li is a C (V L) -gradient Lipschitz continuous, C (L)-
Lipschitz continuous, and bounded (potentially non-convex) function.
We first generalize the convergence of SGD in (Allen-Zhu et al., 2019, Theorem 2) to the minimiza-
tion of the distributionally robust loss using SGD and an exact hardness weighted sampling (19), i.e.
with an exact non-stale loss history.
Theorem B.1 (Convergence of Robust SGD with exact Loss History). Let batch size 1 ≤ b ≤
n, and > 0. Suppose there exists constants C1 , C2 , C3 > 0 such that the number of hidden
units satisfies m ≥ Cι(de-1 × poly(n, L, δ-1)), δ ≥ (C), and the learning rate be ηeχact =
C3 (min (1, βC(L)2+⅛(VL)) X poly(n3储。g2(m)) ∙ There exists constants C4, C5 > 0 such
12
Under review as a conference paper at ICLR 2020
that with probability at least 1 - exp -C4(log2(m)) over the randomness of the initialization
and the mini-batches, Robust SGD with exact loss vector finds ∣∣Vθ (R oLoh)(θ)k ≤ E after T =
C5 (ηex⅛) iterations.
where α = min® minipi(L(θ)) is lower bound on the sampling probabilities. For the KUllback-
Leibler φ-divergence, and for any φ-divergence satisfying assumption 3.1 with a robustness param-
eter β small enough, we have α > 0. We refer the reader to (Allen-Zhu et al., 2019, Theorem 2) for
the values of the constants C1, C2, C3, C4, C5 and the definitions of the polynomials. Compared
to (Allen-Zhu et al., 2019, Theorem 2) only the learning rate differs. The min(1, . ) operation in the
formula for ηexact allows us to guarantee that ηexact ≤ η0 where η0 is the learning rate of (Allen-Zhu
et al., 2019, Theorem 2). The proof can be found in Appendix D.7.3.
It is worth noting that for the KL φ-divergence, P = ɪ. In addition, in the limit β → 0, which
corresponds to ERM, We have α → 1. As a result, We recover exactly Theorem 2 of (Allen-
Zhu et al., 2019) as extended in their Appendix A for any L that satisfies assumption B.2 with
C(V L) =1.
When the amount of distributionally robustness increases the sampling differs more and more from
the uniform sampling and becomes more sensitive to changes of the loss distribution. One way to
mitigate this issue is to reduce the learning rate. The conditions of Theorem B.1 are consistent with
this observation since when β increases, α and ηexact decreases.
In practice in algorithm 4.1, we have access only to a stale loss history. We know restate the conver-
gence of Robust SGD with a stale loss history and a warm-up as in Algorithm 4.1.
Theorem B.2 (Convergence of Robust SGD with Stale Loss History and warm-up). Let batch size
1 ≤ b ≤ n, and E > 0. Under the conditions of Theorem B.1, the same notations, and with the
I -	.	C . /r	aρd3∕2δblog(ι⅛α)	ʌ	「	, , c - c
learning rate ηstaie = C& mm I 1,万勿仁那①仁江向分装为：Og2(m)) X ηeχact for a constant C& > 0.
With probability at least 1 - exp -C4 (log2 (m)) over the randomness of the initialization and
the mini-batches, Robust SGD with exact loss vector finds ∣∣Vθ(R oLoh)(θ)∣∣ ≤ E after T =
C5 (ηsL⅛) iterations.
Where C(L) > 0 is a constant such that L is C(L)-Lipschitz continuous, and A(V L) > 0 is a
constant that bound the gradient of L with respect to its input. C(L) and A(V L) are guaranteed to
exist under assumptions B.1. The proof can be found in Appendix D.8.
Compared to Theorem B.1 only the learning rate differs. Similarly to Theorem B.1, when β tends
to zero we recover Theorem 2 of (Allen-Zhu et al., 2019).
It is worth noting that when β increases,万。(£)；(^ L)Lmog(；-启3(m) decreases. This implies that
ηstale decreases faster than ηexact when β increases. This was to be expected since the error that is
made by using the stale loss history instead of the exact loss increases when β increases.
C S ummary of the notations used in the proofs
For the ease of following the proofs we first summarize our notations.
C.1 Probability Theory Notations
•	∆n = {p = (pi)in=1 ∈ [0, 1]n, Pipi = 1}
•	Let q = (qi) ∈ ∆n, and f a function, we denote Eq [f (x)] := Pin=1qif(xi).
•	Letq ∈ ∆n, andf a function, we denote Vq [f (x)] := Pin=1 qi ∣f(xi) - Eq[f(x)]∣2.
•	Pdata is the uniform training data distribution, i.e. Pdata = (ɪn=1∈ ∈ ∆n
C.2 Machine Learning notations
•	n is the number of training examples
13
Under review as a conference paper at ICLR 2020
•	d is the dimension of the output
•	d is the dimension of the input
•	training data: {(xi, yi)}in=1, where for all i ∈ {1, . . . , n}, xi ∈ Rd and yi ∈ Rd
•	h : x 7→ y is the predictor
•	θ is the set of parameters of the predictor
•	For all i, hi : θ 7→ h(xi; θ) is the output of the network for example i as a function of θ
•	L is the objective function
•	Li : hi 7→ L(hi, yi) is the objective function for example i.
•	By abuse of notation we also denote by L the function L : (hi)in=1 7→ (Li (hi))in=1
•	b ∈ {1, . . . , n} is the batch size
•	η > 0 is the learning rate
•	ERM is short for Empirical Risk Minimization
C.3 Distributionally Robust Optimisation Notations
•	Forall θ, R(L(h(θ))) = maxq∈∆n Eq [L (h(x; θ), y)] - 1 Dφ(q∣∣p^data) is the Distribu-
tionally Robust Loss evaluated at θ, where β > 0 is the parameter that adjusts the distri-
butionally robustness (see (9) for more details). For short, we also used the term Robust
Loss for R(L(h(θ)))
•	DRO is short for Distributionally Robust Optimisation
C.4 Miscellaneous
By abuse of notation, and similarly to (Allen-Zhu et al., 2019), we use the Bachmann-Landau nota-
tions to hide constants that do not depend on our main hyper-parameters. Let f and g be two scalars,
we note:
f f ≤ O(g) o
f f ≥ ω5 ^⇒
I f = Θ(g)	0
∃c > 0
∃c > 0
∃c1 > 0 and ∃c2 > c1
s.t.	f ≤ cg
s.t.	f ≥ cg
s.t.	c1g ≤ f ≤ c2g
D Proofs
D.1 ASSUMPTIONS FOR THE φ-DIVERGENCE
Let Dφ be a φ-Divergence, and c ∈ R. One can note that for φc : z 7→ φ(z) + c (1 - z), we have
for all p = (pi)in=1,q = (qi)in=1 ∈ ∆n,
n
Dφc (qkp) = piφc
i=1
Dφ(qkp) +c
(23)
Dφ (qkp)
In other words, Dφ is not uniquely defined by φ under the definition 3.3. If φ is differentiable, one
can assume without loss of generality that φ0(1) = 0.
For more detailed discussion, We refer the interested reader to (Csiszar et al., 2004).
14
Under review as a conference paper at ICLR 2020
D.2 Proof of Example 4.1: formula of the sampling probabilities for the KL
DIVERGENCE
We give here a simple proof of the formula of the sampling probabilities for the KL divergence as
φ-divergence (i.e. φ : z 7→ z log(z) - z + 1)
∀θ, p(L(h(θ))) = softmax(β L(h(θ)))
For any θ, the distributionally robust loss (9) for the KL divergence at θ is given by
qi L ◦ hi (θ) -
i
R ◦ L ◦ h(θ) = max
q∈∆n
= max
q∈∆n
i=1
qi L ◦ hi (θ) -
i
1n
β ɪ^qi log (nqi)
1 qi log (nqi))
To simplify the notations, let us denote V = (vi)n=ι = Lo h(θ) = (Li ◦ hi(θ))n=ι, and P
(Pi)i=ι= P(L(h(θ)))∙	一
Thus p(L(h(θ))) is, by definition, solution of the optimization problem
arg max X "v - 1 qi log(nqi))
(24)
First, let us remark that the function q 7→ Pin=1 qi log (nqi) is strictly convex on the non empty
closed convex set ∆n as a sum of strictly convex functions. This implies that the optimization (24)
has a unique solution and as a result p(L(h(θ))) is well defined.
We now reformulate the optimization problem (24) as a convex smooth constrained optimization
problem by writing the condition q ∈ ∆n as constraints.
n1
arg max	qivi — Nqi log (nqi)
q∈Rn+ i=1	β
n
s.t.	qi = 1
i=1
(25)
There exists a Lagrange multiplier λ ∈ R, such that the solution P of (25) is characterized by
∀i ∈ {1, . . . , n},
vi - 1 (log (nPi) + 1)+ λ = 0
β
n
X Pi = ι
i=1
(26)
Which we can rewrite as
∀i ∈ {1, . . . , n},
Pi = 1 exp (β (vi + λ) - 1)
n
1n
n E eχp (β (vi + λ) -1) = ι
i=1
(27)
The last equality gives
exp (βλ
- 1)
En=I eχp Cevi)
n
And by replacing in the formula of the Pi
∀i ∈ {1, . . . , n},
Pi =1 exp (βvi) exp (βλ - 1)
n
_	exp(βvi)
Pn=I exP (IeVj)
Which corresponds exactly to
P = SoftmaX (βv)
15
Under review as a conference paper at ICLR 2020
D.3 PROOF OF LEMMA 4.1: REGULARITY PROPERTIES OF R
For the ease of reading, let us first recall that given a φ-Divergence that satisfies assumptions 3.1,
We have defined in (9)
R: Rn →R
V → max ^X qivi —石DoSIgtraiV
q∈∆n i	β
And in (13)
G : Rn →R
P → βDφ(PkPtrain) + δ∆n (P)
Where δ∆n is the characteristic function of the closed convex set ∆n, i.e.
∀P ∈ Rn, δ∆ (P) =	0 ifP ∈ ∆n
∆n	+∞ otherWise
(28)
(29)
(30)
We now prove Lemma 4.1 on the regularity of R.
Lemma D.1 (Regularity of R - Restated from Lemma 4.1). Let φ that satisfies Assumption 3.1, G
and R satisfy
nρ
G is -strongly convex
R(L(h(θ))) = max (hL(h(θ)), q〉— G(q)) = G* (L(h(θ)))
q∈Rn
R is I ɪ ) -gradient Lipschitz continuous.
nρ
(31)
(32)
(33)
φ is ρ-strongly convex on [0, n] so
∀x,y ∈ [0,n]2, ∀λ ∈ [0, 1],φ (λx +(1 — λ)y) ≤ λφ(x) + (1 — λ)φ(y) — "(； " |y — x|2 (34)
LetP = (Pi)in=1, q = (qi)in=1 ∈ ∆n, and λ ∈ [0, 1], using (34) and the convexity of δ∆n, We obtain:
G(λp+ (1 - λ)q)
1n
β~	φ (nλpi + n(I — λ)qi) + δ∆n (λp +(1 — λ)q)
βn i=1
λG(p) + (1 - λ)G(q) -
λG(p) + (1 - λ)G(q) -
1 ρλ(1 — λ)	2	(35)
而 E —2— |nqi — nPi |	(35)
i=1
nρ λ(1 — λ)	2
ββ ~Γ- kq— Pk
This proves that G is nβP-strongly convex.
Since G is convex, R = G* is also convex, and R* = (G*)* = G (Hiriart-Urruty & Lemarechal,
2013).
We obtain (32) using Definition 4.1.
We noW shoW that R is Frechet differentiable on Rn . Let v ∈ Rn .
G is strongly-convex, so in particular G is strictly convex. This implies that the following optimiza-
tion problem has a unique solution that We denote p^(v).
arg max (hv, qi — G(q))
q∈Rn
(36)
In addition
P ∈ ∆n solution of (36) u⇒ 0 ∈
^⇒ V ∈
^⇒ P ∈
^⇒ p ∈
v — ∂G(p)
∂G(P)
∂G*(v)
∂R(v)
≤
≤
16
Under review as a conference paper at ICLR 2020
where We have used (Hiriart-Urruty & Lemarechal, 2013, Proposition 6.1.2 p.39) for the third equiv-
alence, and (32) for the last equivalence.
As a result, ∂R(v) = {p^(v)}. this implies that R admit a gradient at v, and
Vv R(V)= P(V)	(37)
Since this holds for any v ∈ Rn, we deduce that R is Frechet differentiable on Rn.
We are now ready to show that R is ^^ -gradient Lipchitz continuous by using the following lemma
(Hiriart-Urruty & Lemarechal, 2013, Theorem 6.1.2 p.280).
Lemma D.2. A necessary and sufficient condition for a convex function f : Rn → R to be c-
strongly convex on a convex set C is that for all x1, x2 ∈ C
hs2 - s1, x2 - x1i ≥ c kx2 - x1 k2 for all si ∈ ∂f(xi), i = 1, 2.
Using this lemma for f = G, C =詈,and C = ∆n, we obtain:
For all p1, p2 ∈ ∆n , for all V1 ∈ ∂G(p1), V2 ∈ ∂G(p2),
hv2 一 V1,P2 一 Pli ≥ nβp kP2 一 Plk2
In addition, for i ∈ {1, 2}, Vi ∈ ∂G(pi) ^⇒ Pi ∈ ∂R(vι) = {VvR(Vi)}.
And using Cauchy Schwarz inequality
∣∣V2 ― VIIlIlP2 ― Plk ≥ hv2 ― V1,P2 ― Pli
We conclude that
nρ ∣VvR(V2) -VvR(V1 )k ≤ ∣V2 ― Vik
β
Which implies that R is 焉-gradient Lipchitz continuous.
D.4 Proof of Lemma 4.2: Formula of the distributionally robust loss gradient
We prove Lemma 4.2 that we restate here for the ease of reading.
Lemma D.3 (Stochastic Gradient of the Distributionally Robust Loss - Restated from Lemma 4.2).
For all θ, we have
P(L(h(θ))) = Vv R(L(h(θ)))	(38)
Vθ(R。£。h)(θ) = Ep(L(h(θ))) [Vθ L (h(x; θ), y)]	(39)
where VvR is the gradient of R with respect to its input.
For a given θ, equality (38) is a special case of (37) for v = L(h(θ)).
Then using the chain rule and (38),
n ∂R
Vθ(R。£。h)(θ) = V — (L。h(θ)))Vθ(L。hi)(θ)
i=1 ∂Vi	i
n
=XPi(L(h(θ)))Vθ(L。hi)(θ)
i
i=1
=Ep(L(h(θ))) [Vθ L (h(x; θ),y)]
D.5 Proof of Theorem 4.2: Distributionally Robust Optimization as Principled
Hard Example Mining
Let Dφ an φ-divergence satisfying Assumption 3.1, and V = (Vi)in=1 ∈ Rn . V will play the role of a
generic loss vector.
17
Under review as a conference paper at ICLR 2020
φ is strongly convex, and ∆n is closed and convex, so the following optimization problem has one
and only one solution:
1n
P=(Pmn=x∈"v,pi- βn i=1 φ(npi)
(40)
Making the constraints associated with p ∈ ∆n explicit, this can be rewritten as
1n
P=(Pmax ∈Rnhv,pi- βn i=1 φ(npi)
s.t. ∀i ∈ {1, . . . , n}, pi ≥ 0	(41)
n
s.t.	pi = 1
i=1
There exists KKT multipliers λ ∈ R and ∀i, μi ≥ 0 such that the solution P = (pi)：=、satisfies:
∀i ∈ {1,..., n},		vi -石”(nPi) + λ - 2i 二 β μiPi = 0	0
∀i ∈ {1,.	..,n},		
∀i ∈ {1,.	..,n},	Pi ≥ 0	
		n X Pi = 1	
		i=1	
(42)
Since φ is continuously differentiable and strongly convex, we have (φ0)-1 = (φ*)0, where φ* is
the Fenchel conjugate of φ (see Hiriart-Urruty & Lemarechal, 2013, Proposition 6.1.2). As a result,
(42) can be rewritten has:
∀i ∈ {1,..., n}, ∀i ∈ {1,. .., n}, ∀i ∈ {1, . . . , n},	Pi = 1 (φ*)0 (β(vi + λ - μi)) n μipi = 0 Pi ≥ 0	(43)
	1n -∑(φ*)0 (β(vi + λ - μi)) = 1 n i=1
We now show that the KKT multipliers are uniquely defined.
The μis are uniquely defined by V and λ:
Since ∀i ∈ {1,..., n}, μ%pi = 0, Pi ≥ 0 and μi ≥ 0, for all ∀i ∈ {1,..., n}, either Pi = 0 or
μi = 0.
In the casePi = 0, using (43) it comes (φ*)0 (β(Vi + λ - μi)) = 0.
According to assumption 3.1, φ is strongly convex and continuously differentiable, so φ0 and (φ*)0 =
(φ0)-1 are continuous and strictly increasing functions. As a result, it exists a unique μi (dependent
to V and λ) such that:
(φ*)0 (β(vi + λ - μi)) = 0
And (43) can be rewritten as:
∀i ∈ {1,..., n}, Pi = ReLU ɑ (φ*)0 (β (Vi + λ))) = IReLU ((φ*)0 (β(vi + λ)))
1 n	(44)
装 EReLU ((φ*)0 (β(vi + λ))) = 1
n i=1
18
Under review as a conference paper at ICLR 2020
λ is uniquely defined by v and a continuous function of v :
Let λ ∈ R that satisfies (44).
We have § PZi ReLU ((Φ*)0 (β(vi + λ))) = 1. So there exists at least one index io such that
ReLU ((φ*)0 (β(vio + λ))) = (φ*)0 (β(vio + λ)) ≥ 1
Since (φ*)-1 is continuous and Striclty increasing, λ0 → ReLU ((φ*)0 (β(vi° + λ0))) is continuous
and strictly increasing on a neighborhood of λ.
In addition ReLU is continuous and increasing, so for all i ∈	{1, . . . , n}, λ0 7→
ReLU ((φ*)0 (β(Vi + λ0))) is a continuous and increasing function.
As a result, λ0 → ɪ P§=i ReLU ((φ*)0 (β(vi + λ0))) is a continuous function that is increasing on
R, and strictly increasing on a neighborhood of λ.
This implies that λ is uniquely defined by v, and that v 7→ λ(v) is continuous.
Hard Example Mining Sampling:
For any pseudo loss vector V = (vi)§=i ∈ Rn, there exists a unique λ and a unique P that satisfies
(44), so we can define the mapping:
P： Rn → ∆n
V → p(v; λ(v))
(45)
where for all V, λ(V) is the unique λ ∈ R satisfying (44).
We will now demonstrate that each pi0 (v) for io ∈ {1,..., n} is an increasing function of Vi and a
decreasing function of the Vi for i 6= i0. Without loss of generality we assume i0 = 1.
Let V = (Vi)in=i ∈ Rn, and > 0.
Let us define V0	=	(Vi0)in=i	∈	Rn, such that	Vi0	=	Vi	+	and ∀i	∈	{2,	. . .	, n},	Vi0	=	Vi.
Similarly as in the proof of the uniqueness of λ above, we can show that there exists η > 0 such that
the function
1n
F ： λ0→	ReLU ((φ*)0 (β(vi + λ0)))
is continuous and strictly increasing on [λ(V) - η, λ(V) + η], and F (λ(V)) = 1.
V 7→ λ(V) is continuous, so for small enough λ(V0) ∈ [λ(V) - η, λ(V) + η].
Let us now prove by contradiction that λ(V0) ≤ λ(V). Therefore, let us assume that λ(V0) > λ(V).
Then, as ReLU ◦ (φ*)0 is an increasing function and F is strictly increasing on [λ(v) - η, λ(v) + η],
and > 0 we obtain
1n
1 = -∑ReLU ((φ*)0 (β(vi + λ(v0))))
n i=i
1n
≥ -∑ReLU ((φ*)0 (β(vi + λ(v0))))
i=i
≥ F (λ(V0))
> F (λ(V))
>1
which is a contradiction. As a result
λ(V0) ≤ λ(V)
(46)
19
Under review as a conference paper at ICLR 2020
Using (46), (44), and the fact that ReLU ◦ (φ*)0 is an increasing function, We obtain for all i ∈
{2,...,n}
Pi(v0) = IReLU ((Φ*)0 (β(Vi + λ(v0))))
=IReLU ((φ*)0 (β(Vi + λ(v0))))
≤ IReLU ((φ*)0 (β(Vi + λ(v))))
≤ Pi(V)
(47)
In addition
nn
X Pi(VO) =1 = X Pi(V)
i=1	i=1
So necessarily
PI(V) ≥ Pi (v)	(48)
This holds for any i0 and any V , Which concludes the proof.
D.6 PROOF THAT R ◦ L IS ONE-SIDED GRADIENT LIPCHITZ
This property that R ◦ L is one-sided gradient Lipschitz is a key element for the proof of the semi-
smoothness theorem for the distributionally robust loss Theorem D.1.
Under assumption 3.1, We have shown that R* is 磊-gradient LiPchitz continuous. And under as-
sumption B.2, for all i, Li is C (L)-Lipschitz continuous and C (V L) -gradient Lipschitz continuous.
Letz = (zi)in=1,z0 = (zi0)in=1 ∈ Rdn.
We Want to shoW that R ◦ L is one-sided gradient Lipschitz, i.e. We Want to prove the existence ofa
constant C > 0, independent to z and z0, such that:
hVz(R ◦ L)(z) - Vz(R ◦ L)(z0), z - z0i ≤Ckz-z0k2
We have
NzlR ◦ L)(Z)-Vz(R ◦ L)(z0),z - z0i
n
=XhVzi(R ◦ L)(Z)- Vzi(R ◦ L)(ζ0),Zi - ζ0i
i=1
n
=XhPi(L(Z))Vzi L(Zi)- Pi(L(ZO))VziL(Z0 ),zi- ζii
i=1	i	i	(49)
n
=XIPi(L(ZY)Ezi L(Zi)-VziL(Z0\电一 A
ii
i=1
n
+ X (Pi(L(z)) - Pi(L(Z0))) hVzi L(Zi),Zi - Z0i
i
i=1
Where for all i ∈ {1, . . . , n} We have used the chain rule
ʌ ∂R*
Vzi(R OL)(Z)=X 西
(L(Z))Vzi L(Zj )= Pi(L(Z))Vzi L(Zi)
ji
Let
n
A = X Pi(L(Z))hVzi L(Zi)- VziL(Zi),z⅛ - Zii
ii
i=1
20
Under review as a conference paper at ICLR 2020
For all i, Li is C (V L)-gradient LiPchitz continuous, so using CaUchy-SchWarz inequality
n
A≤XC(VL)kzi-zi0k2=C(VL)kz-z0k2
i=1
Let
n
B =X (Pi(L(Z))- pi (L(z''y)) hvZi L(zi),zi - z0 i
i
i=1
Using the triangular inequality:
n
B ≤ X (pi(L(Z))- pi(L(z')))(L(zi) — L(Zi)
ii
i=1
n
+ X (Pi(L(Z))— 6(L(Zy)i(L(ZIi) + IhVzi Lizi), Zi-Zii- L(Zi)
i=1
≤ ∣V(R*)(L(Z)) - V(R*)(L(Z0)), L(Z) - L(Z0)i
n
+2XLi(Zi')+hVziLi(Zi'),Zi -Zi'i - Li (Zi)
i=1
≤ - kL(Z) -L(Z0)k2 +2C(V-Ly kZ-Z'k2
nρ	2
≤ (βC(L^ + C(VL)) kZ - Z0k2
nρ
Combining (49), (50) and (51) We finally obtain:
∣Vz(R oL)(Z)-Vz(R oL)(z')
Z - Z0i ≤ (βC(L2 + 2C(VL)) kZ - Z0k2
nρ
(50)
(51)
(52)
From there, We can obtain the folloWing inequality that Will be used for the Proof of the semi-
smoothness ProPerty in Theorem D.1:
R(L(z')) - R(L(Z)) -IVz(R ◦ L)(z),z' - Zi
=/ ∣Vz(R ◦L) (z + t(z' - z)) -Vz(R oL)(z),z' - Zidt
≤ 1 (βC(L^ +2C(VL)) kz-z'k2
2 nρ
(53)
D.7 Proof of the convergence of Robust SGD
In this Part, We Prove the results of Therem B.1 and B.2.
They are generalizations of the convergence result for SGD Presented in Theorem 2 of (Allen-Zhu
et al., 2019).
For the ease of reading the Proof, We remind here the chain rules for the distributionally robust loss
(9) that We are going to use intensively in the folloWing Proofs.
Chain rule for the derivative of R ◦ L with respect to the network outputs h:
Vh(R ◦ L)(h(θ)) = (Vhi (R ◦ L)(h(θ)))in=1
n ∂R
∀i ∈ {1,…n},	Vhi(R ◦ L)(h(θ)) = E 西(L(h(θ)))Vhi L(hj(θ))
=pi(L(h(θ)))VhiL(hi(θ))
i
(54)
21
Under review as a conference paper at ICLR 2020
Chain rule for the derivative of R ◦ L ◦h with respect to the network parameters θ:
n
Vθ (R °Loh)(θ) = X Vθ hi(θ)Vhi(R oL)(h(θ))
i=1
n
=XPi(L(h(θ)))Vθhi(θ)Vhi L(hi(θ))	(55)
i
i=1
n
=X Pi(L(h(θ))Vθ (Lohi)(θ))
i
i=1
where for all i ∈ {1, . . . n}, Vθhi(θ) is the transpose of the Jacobian matrix of hi as a function of
θ.
D.7.1 Semi-smoothness property for the Distributionally Robust Loss
We prove the following lemma which is a generalization of Theorem 4 in (Allen-Zhu et al., 2019)
for the distributionally robust loss (9).
Theorem D.1 (Semi-smoothness of the Distributionally Robust Loss).
Let ω ∈ [ω (m3/2L3d；/0g3/2(m)) , O (小 lθg3(m)) ], and the θ(0) being initialized randomly as de-
scribed in assumption B.1. With probability as least 1 一 exp (一Ω(mω3^L)) over the initialization,
we have for all θ, θ0 ∈ (Rm×m)L with θ - θ(0) 2 ≤ ω, and kθ - θ0k2 ≤ ω
R(L(h(θ0)) ≤ R(L(h(θ)) + hVθ(R oLoh)(θ), θ0 一 θ)
+ kVh(RoL)(h(θ))∣bι O LL2ω"3√tlOg(m)! kθ0 - θ∣h,
+o(( βCnLf+2C(VL)) nLm )kθ0-C∞
(56)
where for all layer l ∈ {1, . . . , L}, θl is the vector of parameters for layer l, and
kθ0 一 θk2,∞ = mlax kθl0 一 θlk2
kθ0 - θk2,∞ = (max kθ0 - θlk2) = max kθ0 - θlk2
n
kVh(RoL)(h(θ))∣∣2j = X kVh"RoL)(h(θ))∣∣2
n
X Mi(L(h(θ)))Vhi L(hi(θ))∣∣	(chainrule (54))
i=1	2
To compare this semi-smoothness result to the one in (Allen-Zhu et al., 2019, Theorem 4), let us
first remark that
kVh(R oL)(h(θ))∣∣2j ≤√ kVh(R oL)(h(θ))∣∣2,2
As a result, our result is analogous to (Allen-Zhu et al., 2019, Theorem 4), up to an additional
multiplicative factor CCPL + 2C (V L)) in the last term of the right-hand side. It is worth noting
that there is also implicitly an additional multiplicative factor C (V L) in Theorem 3 of (Allen-Zhu
et al., 2019) since (Allen-Zhu et al., 2019) make the assumption that C (V L) = 1 (see Allen-Zhu
et al., 2019, Appendix A).
Let θ, θ0 ∈ (Rm×m)L verifying the conditions of Theorem D.1.
Let A = R(L(h(θ0)) 一 R(L(h(θ)) 一(Ve(R ◦ L °h)(θ), θ0 一 θ) , the quantity we want to bound.
22
Under review as a conference paper at ICLR 2020
Using (53) for z = h(θ) and z0 = h(θ0), we obtain
A ≤ 1 (βCn(L)2 + 2C(VL)) kh(θ0) - h(θ)k2
+ hVh(R oL)(h(θ)),h(θ0) - h(θ)i
-hVθ(RoLoh)(θ), θ0 — θi
Then using the chain rule (55)
A ≤ 1 (βCn(L)2 + 2C(VL)) kh(θ0) - h(θ)k2
n
+ XhVhi(R ◦ L)(h(θ)), hi(θ0) - hi(θ) - (Vθhi(θ))T (θ0 - θ)i
i=1
(57)
(58)
For all i ∈ {1, . . . , n}, let us denote lossi := Vhi (R ◦ L)(h(θ)) to match the notations used in
(Allen-Zhu et al., 2019) for the derivative of the loss with respect to the output of the network for
example i of the training set.
With this notation, we obtain exactly equation (11.3) in (Allen-Zhu et al., 2019) up to the multiplica-
tive factor (βC(L)? + 2C(V L)) for the distributionally robust loss.
From there the proof of Theorem 4 in (Allen-Zhu et al., 2019) being independent to the formula for
7 J	1 1 .1	i' i'	EI	1 ʌ < / ♦ ɪ 11 τ~rΛ	. 1 CCTC A	1 ∙ * ∖
lossi, we can conclude the proof of our Theorem D.1 (as in Allen-Zhu et al., 2019, Appendix A).
D.7.2 Gradient B ounds for the Distributionally Robust Loss
We prove the following lemma which is a generalization of Theorem 3 in (Allen-Zhu et al., 2019)
for the distributionally robust loss (9).
Theorem D.2 (Gradient Bounds for the Distributionally Robust Loss).
Let ω ∈ O Qg∕2Lδ3∕0g3(m)), and θ⑼ being initialized randomly as described in assump-
tion B.1. With probability as least 1 一 exp(一Ω(mω^2L)) over the initialization, we have for
allθ ∈ (Rm×m)L with θ - θ(0)2 ≤ ω
- - - - ʌ
∀i ∈{1,...,n}, ∀l ∈{1,...,L}, ∀L ∈ Rn
b/We, (L °hi)(θ)∣∣2
≤O
(mb(L)Vhi L(hi(θ))
- - ʌ
∀l ∈ {1,...,L}, ∀L ∈ Rn
n
XPi(L)Vθι (Lohi)(θ)
i
i=1
2n
≤ OmF Xii
2	i=1
IPi(L)Vhi L(hi(θ))∣∣2
(59)
ʌ
2
n
XPi(L)VθL (L ohi)(θ)
i=1
2 ≥ Ω (黑 X Il
2	i=1
Mi(L)Vhi L(hi(θ))∣∣2
2
ʌ
It is worth noting that the loss vector L used for computing the robust probabilities P(L)
(Pi(L))	does not have to be equal to L(h(θ)).
We will use this for the proof of the Robust SGD with stale loss history.
The adaptation of the proof of Theorem 3 in (Allen-Zhu et al., 2019) is straightforward.
_	_______ T,	_ . 一	一 . .	_	_	_ 3	___
Let θ ∈ (Rm×m) satisfying the conditions of Theorem D.2, and L ∈ Rn.
23
Under review as a conference paper at ICLR 2020
Let us denote V := (Pi(L)Vhi Li (hi (θ)))	, applying the proof of Theorem 3 in (Allen-Zhu et al.,
2019) to our v gives:
∀i ∈ {1, . . . , n}, ∀l ∈ {1, . . . , L},
2
ʌ
Mi(L)Vθι(L◦hi)(θ)∣∣2 ≤ O
- - ʌ
∀l ∈ {1,...,L}, ∀L ∈ Rn
2
m
di
IPi(L)Vhi L(hi(θ))∣∣2
2
ʌ
n
X
i=1
Pi(L)Pθι(Lohi)(θ)	≤ O
n
ʌ
mδ
ma max
dn i
Pi(L)Vhi L(hi(θ))∣∣2
n
X Pi(L)VθL (L ohi)(θ)
i=1
2
2
≥ Ω
2
Ipi(L)Vhi L(hi(θ))∣∣2
ʌ
i
2
ʌ
2
In addition
max Qpi(L)Vhi Lwθ))∣∣2) ≥ n X Il
2
ʌ
IPi(L)Vhi L(hi(θ))∣∣2
This allows us to conclude the proof of our Theorem D.2.
D.7.3 Convergence of Robust SGD with Exact Loss History
We can now prove Theorem B.1.
Theorem D.3 (Convergence of Robust SGD with exact Loss History - Restated from Theorem B.1).
Suppose batch size 1 ≤ b ≤ n, number of hidden units m ≥ Ω(de-1 X poly(n, L, δ-1)), and δ ≥
O (j1). Let e > 0, and the learning rate be ηeχact = Θ
an2 P
βC(L)2+2nρC(VL)
bδd
poly(n,L)m log2(m) J,
with probability at least 1 一 exp (一Ω(log2(m))) over the randomness of the initialization and the
mini-batches, Robust SGD with exact loss vector finds ∣∣Vθ (R ◦ f ◦ h)(θ)k ≤ E after T = O (Lδ⅛)
iterations.
×
Similarly to the proof of the convergence of SGD for the mean loss (4) (Theorem 2 in (Allen-Zhu
et al., 2019)), the convergence of SGD for the distributionally robust loss (9) will mainly rely on
the semi-smoothness property (Theorem D.1) and the gradient bound (Theorem D.2) that we have
proved previously for the distributionally robust loss.
L
Let θ ∈ (Rm×m) satisfying the conditions of Theorem B.1, and L be the exact loss history at θ,
i.e.
n
L =(L (hi(θ))
i=1
(60)
τ-1 .1 1 . 1 ♦ T _ r -I	T 1 . Γ1 r ■ T /1	Λ . 1 C	1	i'	—/ ∖ ∙ .Λ .
For the batch Size b ∈ {1,..., n}, let S = {j }b=1 a batch of indices drawn from P(L) without
replacement, i.e.
∀j ∈{1,...b}, iji吧.P(L)
(61)
Let θ0 ∈ (Rm×m)L be the values of the parameters after a stochastic gradient descent step at θ for
the batch S, i.e.
θ0 = θ 一 η1X Vθ (L°hi)(θ)
bi
i∈S
where η > 0 is the learning rate.
(62)
24
Under review as a conference paper at ICLR 2020
Assuming that θ and θ0 satisfies the conditions of Theorem D.1, we obtain
R(L(h(θ0)) ≤R(L(h(θ)) - ηhVθ(R ◦ L oh)(θ), 1 X Vθ(L ◦hi)(θ)i
bi
i∈S
+ η√nkVh(R oL)(h(θ)% O L3pmog(m
IX vθ (L …
i∈S
2,∞
+η2O((βCρL^+2C(VL)) nLdm) IbXvθ(L°hi)(θ) 2
2,∞	(63)
where we refer to (55) for the form of Vθ(RoLoh)(θ) and to (54) for the form of Vh(RoL)(h(θ)).
In addition, we make the assumption that for the set of values of θ considered the hardness weighted
sampling probabilities admit an upper-bound
α = min minPi (L(θ)) > 0	(64)
θi
Which is always satisfied under assumption B.2 for Kullback-Leibler φ-divergence, and for any
φ-divergence satisfying assumption 3.1 with a robustness parameter β small enough.
Let ES be the expectation with respect to S. Applying ES to (63), we obtain
ES [R(L(h(θ0))]
≤R(L(h(θ)) - η kVθ(R ◦ Loh)(θ)k2,2
+ ηkVh(RoL)(h(θ))k2,2 O (nL2ω1/3PmIogm
+ η2O ((βCnL2 +2C(VL)) nLdm) 1 Xmax∣Pi(L)Vθι(L◦hi)(θ)∣2
i=1	(65)
where we have used the following results:
n2
X maχ ∣Pi(L)Vθι (L Ohi)⑹ I
i=1
∖
•	For any integer k ≥ 1, and all (ai)in=1 ∈ Rk n, we have (see the proof in D.7.4)
ES b X ai = Ep(L) [ai]
i∈S
•	Using (66) for 3)21 = (Vθ(Li ohi)(θ))2ι, and the Chain rule (55)
Es 1 X Vθ (Lohi)(θ)
bi
i∈S
n
X Pi(L)Vθ (Lohi)(θ) = Vθ (R oLoh)(θ)
i
i=1
(66)
(67)
•	Using the triangular inequality
1 X Vθ (Lohi)(θ)
bi
i∈S
2,∞
≤ b X Ivθ (L。闲矶 ∞
(68)
∞in=1
And using (66) for (ai)n=ι = (∣∣Vθ(Li ohi)(θ)∣∣2,,
Es	1 X Vθ (Lohi)(θ)
bi
i∈s
2,∞
n
≤ X Pi(L) ∣∣Vθ (L ohi)(θ)∣∣
i=1	2,∞
n
≤ X maxI I vΘi (Pi (L) L θhi)(θ)∣2
i=1	2
(69)
n
2
2
≤ √nt Xmax ∣∣Vθι (Pi(L) Lohi)(θ)∣∣
i=1
25
Under review as a conference paper at ICLR 2020
where we have used Cauchy-Schwarz inequality for the last inequality.
• Using (68) and the convexity of the function x 7→ x2
1 X Vθ (Lohi)(θ)
bi
i∈S
2
2,∞≤ bX M(L 血)叽 ∞
(70)
AndUsing (66) for (ai)n=1 = (∣∣Ve(Li °hi)(θ)k2,∞) 一
ES ![I b X Vθ (L ohi)(θ)
2,∞
n2
≤ XPi(L) ∣∣Vθ(Lohi)(θ)∣Loo
i=1	2,∞
n 1	I	I2
≤ X—maxl∣vθι (Pi(L) Lohi)||
M Pi(L)	l 11	i	ll2
1n I
≤ — Xmax ∣∣Vθι(pi(L) L ohi)(θ)
αll	i
i=1
(71)
2
Important Remark: It is worth noting the apparition of α (64) in (71). If we were Using a Uniform
sampling as for ERM (i.e. for DRO in the limit β → 0), We would have α = ɪ. So although our
ineqUality (71) may seem brUtal, it is consistent with eqUation (13.2) in (Allen-ZhU et al., 2019) and
the corresponding inequality in the case of ERM.
The rest of the proof of convergence will consist in proving that η ∣∣Vθ (R ◦ L oh)(θ)k2 2 dominates
the two last terms in (63). As a result, we can already state that either the robustness parameter β,
or the learning rate η will have to be small enough to control α. This is consistent with what we
observed in our experiments.
Indeed, combining (63) with the chain rule (55), and the gradient bound Theorem D.2 where we use
our L defined in (60)
ES [R(L(h(θ0))] ≤ R(L(h(θ))
,ʌ. ..
Pi(L)Vhi L(hi(θ))
i
+ ηO
Ird )X"Vhi L (hi (θ)“2
+ η2O
βCnL^+2C(V L)) nLdd) O( dmα)X Mi(L)VhiL (hi®) %
nρ
2
2
There are only two differences with equation (13.2) in (Allen-Zhu et al., 2019):
•	in the last fraction we have n/a instead of n2 (see remark D.7.3 for more details), and an
additional multiplicative term K. So in total, this term differs by a multiplicative factor On
from the analogous term in the proof of (Allen-Zhu et al., 2019).
•	we have PZi ∣∣Pi(L)Vhi Li(hi(θ))∣∣ instead of F(W(t)). In fact they are analogous
since in equation (13.2) in (Allen-Zhu et al., 2019), F(W(t)) is the squared norm of the
26
Under review as a conference paper at ICLR 2020
mean loss for the L2 loss. We don’t make such a strong assumption on the choice of L (see
assumption B.2). It is worth noting that the same analogy is used in (Allen-Zhu et al., 2019,
Appendix A) where they extend their result to the mean loss with other objective function
than the L2 loss.
Our choice of learning rate in Theorem B.2 can be rewritten as
ηexact = Θ
αn2ρ
βc (L)2 +2nρC (VL)
bδd
poly(n, L)m log2 (m)
________bδd_________
poly(n, L)m log2 (m)
αn
K
× η0
(74)
And we also have
ηexact ≤ η	(75)
where η0 is the learning rate chosen in the proof of Theorem 2 in (Allen-Zhu et al., 2019). We refer
the reader to (Allen-Zhu et al., 2019) for the details of the constant in "㊀" and the exact form of the
polynom poly(n, L).
As a result, for η = ηeχact, the term Ω (ηm∙) dominates the other term of the right-hand side of
inequality (72) as in the proof of Theorem 2 in (Allen-Zhu et al., 2019).
This implies that the conditions of Theorem D.2 are satisfied for all θ(t), and that we have for all
iteration t > 0
(L)Vhi L(hi(θ(t)))∣∣2	(76)
ESthR(L(h(θ(M))] ≤ R(L(h(θW) - Ω (ng) X
And using a result in Appendix A of (Allen-Zhu et al., 2019), since under assumption B.2 the
distributionally robust loss is non-convex and bounded, we obtain for all 0 > 0
∣∣Vh(R。尔Wah ≤ J if T=O (得)	(77)
where according to (54)
n2
∣∣Vh(R°L)(h(θ(T)))∣∣2 2 = X ∣∣Pi(L)Vhi L(hi(θ㈤))∣L	(78)
However, we are interested in a bound on ∣∣Vθ(RoLoh)(θ(T))∣∣2 2,
∣∣Vh(R oL)(h(θ(T )))∣∣2,2∙	,
Using the gradient bound of Theorem D.2 and the chain rules (55) and (54)
∣∣Vθ (R °Loh)(θ(T ))∣∣2 2 ≤ ci yLmn∣∣Vh(R °L)(h(θ(T )))∣∣22
rather than
(79)
where c1 > 0 is the constant hidden in O
SoWith W = Ci
, we finally obtain
Vθ (R °Loh)(θ(T ))∣∣2 2 ≤ ci yimn∣∣Vh(R QL)(h(θ(T )))∣∣22
rLmn 0
d E
If
(80)
which concludes the proof.
dn2 Lmn ) 。
ηδm d2
(81)
≤
×
27
Under review as a conference paper at ICLR 2020
D.7.4 Proof of technical lemma 1
For any integer k ≥ 1, and all (ai)n=ι ∈ (Rk)n, We have
ES b X
ai
i∈S
Σ
1≤i1 ,...,ib ≤n
n	1b
Ypik (L)	b X aij
k=1
1≤i1,...,ib ≤n j=1
b
XPij (L) aij
1b
b x
j=1	1≤i1 ,...,ib ≤n
E	Pij (L) aij
j=1
(n
Y Pik (L)
k=1
k6=j
(n
Y Pik (L)
b XI(X Pij (L) aj fl
j=1	ij =1	kk6==1j
(82)
1
b
Σ
ai
1b
b X
j=1
n
Epi(L) ai
i=1
Ep(L) [ai]
D.8 Convergence of Robust SGD with Stale Loss History
The proof of the convergence of Algorithm 4.1 under the conditions of Theorem B.2 folloWs the
same structure as the proof of the convergence of Robust SGD With exact loss history D.7.3. We
Will reuse the intermediate results of D.7.3 When possible and focus on the differences betWeen the
tWo proofs due to the inexactness of the loss history.
Let an iteration number t, so that the Warm-up of Algorithm 4.1 is already over at t.
Let θ(t) ∈ (Rm×m)L the parameters of the deep neural netWork at iteration t.
We define the stale loss history at iteration t as
L= (f(hi(θ氏⑴))))：]	(83)
Where for all i, ti(t) < t corresponds to the latest iteration before t at Which the loss for example i
has been updated. Or equivalently, it corresponds to the last iteration before t When example i Was
draWn to be part of a mini-batch.
Thanks to the Warm-up stage of Algorithm 4.1, it is guaranteed that the loss value of every example
has been computed at least once before We start using the adaptive sampling. As a result, for all
iteration after the Warm-up, the stale loss history L is Well defined.
We also define the exact loss history that is unknoWn in Algorithm 4.1, as
L=(L (hi (θ(t))))L	(84)
Remark on the warm-up stage of Algorithm 4.1: The iterations performed during the Warm-up
stage amounts to classic SGD to minimize the mean loss (4). As a result, the convergence results of
(Allen-Zhu et al., 2019, Theorem 2) apply during the Warm-up. This guarantees that the condition on
θ of Theorem B.2 remains satisfied during the Warm-up if it Was satisfied by the initial parameters.
28
Under review as a conference paper at ICLR 2020
Similarly to (62) we define
θ(t+1) = θ㈤一η 1X Vθ (Lohi)(θ㈤)
i∈S i
(85)
and using Theorem D.1, similarly to (63), we obtain
R(L(h(θ(t+1))) ≤R(L(h(θ㈤))-ηhVθ(R oLoh)(θ㈤),1 X Vθ(Lohi)(θ(t))i
bi
i∈S
+ η2o ((BCL +2C(VL)
L2ω1/3 W log⑺八 1 X Vθ (Lohi)(θ(t))
+η Vh(R ◦ L)(h(θ(t)))
∑Vθ (L。加)但㈤)
i∈S
i∈S
2,∞
2,∞
(86)
2
We can still define α as in (64)
α = min minPi (L(θ)) > 0
θi
(87)
where we are guaranteed that α > 0 under assumptions B.1.
Since Theorem D.2 is independent to the choice of L, taking the expectation with respect to S,
similarly to (72), we obtain
n
ES [R(L(h(θ(t+1)))] ≤ R(L(h(θ⑶))-ηhVθ(R。L oh)(θ⑴),XPi(LRe(Lohi)(θ(t)))i
i=1
where the differences with respect to (72) comes from the fact that L is not the exact loss history
here, i.e. L 6= L, which leads to
n
Vθ(R oLoh)(θ㈤)=XPi(L)Vθ(Lohi)(θ㈤))
i
i=1
n
=X Pi (L)Vθ (L ohi)(θ㈤))
i=1
(89)
And
n
IIVh(RoL)(h(θ⑴))[2 = XlPi(L)Vhi L(hi(θ⑴)))IL
,	i=1	2
n
=XIPi(L)Vhi L (hi (θ(t))))∣∣2
i=1	2
(90)
Let
K0 = C (L)A(V L) O
1⅛
Where C(L) > 0 is a constant such that L is C(L)-Lipschitz continuous, and A(V L) > 0 is a
constant that bound the gradient of L with respect to its input.
(91)
29
Under review as a conference paper at ICLR 2020
C(L) and A(V L) are guaranteed to exist under assumptions B.1.
We can prove that, with probability at least 1 一 exp (一Ω (log2(m))),
•	according to lemma D.8.1
• according to lemma D.8.2
IIp(L)-
(92)
n
hVθ (R oL°h)(θ㈤)一 X Pi(L)Ve (Lohi)(θ ⑴)),£pi(L)Ve (f°hi)(θ㈤))〉
i=1
i=1
n
≤ 哈K0X IIPi(L)Ve(fohi)(θ⑴))
i=1
(93)
2
2
•	according to lemma D.8.3
Vh(RoL)(h(θ⑴))∣∣	≤ (√n+ηκ0)∖
1,2
2
n
i=1
(94)
2
Combining those three inequalities with (88) we obtain
ES hR(L(h(θ(t+1)))i 一 R(L(h(θ(t))) ≤
η
m mδ ∖	…/ nL2mω1/3
一ω(" + O	d
n
X I限(L)Vhi L(hi(θ(t)))∣∣
i=1
2
2
η2O K
(n∕α)L2m2
d2
+(1 + md) K0) X Ipi(L)Vhi L(hi(θ(t)))
(95)
2
i
2
One can see that compared to (72), there is only the additional term(1 + m K0.
Using our choice of η,
η = ηstale ≤ O ( n2 κ0 ηexact
where ηexact is the learning rate of Theorem B.1, we have
Ω(ηmδ) ≥ O
dn2
(96)
(97)
As a result, η2(1 + 号)K0 is dominated by the term Ω ( nm
In addition, since ηstaie ≤ ηeχact, Ω (⅛mδ) still dominates also the ther terms as in the proof of
Theorem B.1.
As a consequence, we obtain as in (76) that for any iteration t > 0 (after the end of the warm-up)
n
ESt R(L(h(θ(t+1))) ≤ R(L(h(θ(t)))
dn2
i=1
i(L)Vhi L(hi(θ㈤“I；	(98)
This concludes the proof using the same arguments as in the end of the proof of Theorem B.1 starting
from (76).
30
Under review as a conference paper at ICLR 2020
D.8.1 Proof of technical lemma 2
Using Lemma 4.2 and Lemma 4.1 we obtain
IIp(L)- p3(L)∣∣2 = ∣∣Vv R(L)-Vv R(L)∣∣2
≤ nρ∣∣L-L∣∣2
Using assumptions B.2 and (Allen-Zhu et al., 2019, Claim 11.2)
(99)
2
hi(θ(t)) -L。hi(θ(ti(t)))J
≤ FC(L)C(h)t Xn ∣∣θ(t) - θ(ti(t)) ∣∣22,2
nρ	i=1
≤C(L)O(βnLm? )∖X ∣∣M- θ(ti叫 2,
(100)
Where C(L) is the constant of Lipschitz continuity of the per-example loss L (see assumptions B.2)
and C(h) is the constant of Lipschitz continuity of the deep neural network h with respect to its
parameters θ.
By developing the recurrence formula of θ(t) (85), we obtain
2
∣∣P(L)-P(L)∣∣2 ≤C(L)O(Iemr)∖XX i-
X b X Vθ(L。hj)(θ(τ))∣ - θ(ti(t))
τ=ti(t)	j∈Sτ
2,2
≤ ηC(L)O (βLm/2) , XX X 1 X Vθ(L。hj)(θ(T))
nρd1/2	b	j
nρ	i=1 ∣τ=ti(t) j∈Sτ
2
2,2
Let A(V L) a bound on the gradient of the per-example loss function. Using Theorem D.2 and the
chain rule
∀j, ∀T	Vθ(L。hj)(θ(τ))	≤ A(VL)O (md)	(101)
And using the triangular inequality
∣∣ t-1 1
E b ∑ Vθ(L。hj)(θ(τ))
∣τ =ti (t) j∈Sτ
t-1 1
≤ ∑ b ∑ vθ(L。hj)(θ(τ))
2 2	τ=ti (t) j ∈Sτ
t-1
≤ X A(VL)O (号)
τ =ti (t)
≤ A(VL)O (md) (t- ti(t))
(102)
As a result, we obtain
∣∣P(L)-P(L)∣∣2 ≤ ηC(L)A(VL)o (βLm/2) t XX(t - ti(t))2	(103)
For all i and for any τ the probability that the sample i is not in batch Sτ is lesser than (1 - α)b.
31
Under review as a conference paper at ICLR 2020
Therefore, for any k ≥ 1 and for any t,
P(t- ti(t) ≥ k) ≤ (1 - α)kb
(104)
log2(m)
log( 1⅛ )
we have (1 - a)kb ≤ exp (-Ω (log2(m)))
, and thus with probability at
least 1 - exp (-Ω (log2(m))),
For k ≥ 1Ω
-b
∀t,
t - ti(t) ≤ O
(105)
As a result, we finally obtain that with probability at least 1 - exp (-Ω (log2(m))),
< ηαK0
(106)
D.8.2 PROOF OF TECHNICAL LEMMA 3
Let us first denote
n	n
A = "e(R OL oh)(θ⑴)-XPi(L)Ve(L ohi)(θ(t))), XPi(L)Ve(L %)(θ㈤))〉
i=1	i	i=1	i
n	n
=hX (Pi(L)-Pi(L)) Ve(Lohi)(θ⑴)),XPi(L)Ve(Lohi)(θ⑴))〉
i=1	*	i=1	*
(107)
Using Cauchy-Schwarz inequality
2
nn
X hVe(L ohi)(θ⑴)),XPj(L)Ve(L叫)(θ㈤))〉
i=1 \	2	j=1	j	/
(108)
Let
n
B = hVe(L ohi)(θ(t))), XPj(L)Ve(L ohj)(*))〉	(109)
j=i	j
Using again Cauchy-Schwarz inequality
n
B ≤ ∣∣Ve(Lohi)(θ⑴)“	XPj(L)Ve(LOhj)(θ⑴))	(110)
C	2,2 j = 1	j	2,2
32
Under review as a conference paper at ICLR 2020
As a result, A becomes
n
X Pj (L)Vθ (LOhj )(θ㈤))
j=1	j
2,2
n2
XM (L Ohi)(θ(t)⅛2
i=1	,
≤ IIP(L)-P(L)IL
n
X Pj (L)Vθ (LOhj )(θ(t)))
j=1	j
un	2
tX /Mj (L)Vθ (L Ohi)(M))k
i=1	,
n
X Pj (L)Vθ (LOhj )(θ⑴))
j=1	j
2
2,2
(111)
Using the triangular inequality, Theorem D.2, and Lemma D.8.1, we finally obtain
A
n
≤ *K 0 X
j=1
I2
Pj(L)Vhj L(hj(θ㈤))
j	2,2
I2
Pj(L)Vhj L(hj(θ㈤))
j	2,2
(112)
D.8.3 Proof of technical lemma 4
We have
nI	I
Vh(R oL)(h(θ㈤))∣∣	= XPj(L) Vhj L(hj(θ㈤))
1,2 j=1	j	2,2
nI	I
=XPj(L) Vhj L(hj(θ㈤))
j=1	j	2,2
(113)
+X (¾r)! Pj(L) m ” (A))
Using Cauchy-Schwarz inequality
IIVh(R O L)(h(θ(t)))
Using Lemma D.8.1
Therefore, we finally obtain
Vh(ROL)(h(θ(t)))II
u n I	I2
≤ (√n + ηK0)、X Pj(L)Vhj L(hj(θ㈤))
j=1	j	2,2
(116)
33