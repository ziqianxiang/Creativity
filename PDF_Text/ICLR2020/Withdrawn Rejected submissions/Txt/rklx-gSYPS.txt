Under review as a conference paper at ICLR 2020
Learning to Optimize via
Dual space Preconditioning
Anonymous authors
Paper under double-blind review
Ab stract
Preconditioning an minimization algorithm improve its convergence and can lead to
a minimizer in one iteration in some extreme cases. There is currently no analytical
way for finding a suitable preconditioner. We present a general methodology for
learning the preconditioner and show that it can lead to dramatic speed-ups over
standard optimization techniques.
1	Introduction
Many problems arising in applied mathematics can be formulated as the minimization of a convex
function f : Rd → (-∞, +∞]
min f (x).
x∈Rd
The resolution of an optimization problem is usually tackled using an optimization algorithm that
produces a sequence of iterates converging to some minimizer of f (Nesterov (2018)). The gradient
descent algorithm is a standard optimization algorithm that converges linearly (i.e exponentially
fast) if f is regular enough. Preconditioned methods (Nemirovsky & Yudin (1983); Lu et al. (2016);
Maddison et al. (2019)) are powerful optimization algorithms that converges linearly under weaker
assumptions that gradient descent. The performance of these methods relies heavily on the pre-
conditioning - the task of choosing the hyperparameter called the preconditioner. In the case of
the Dual space Preconditioned Gradient Descent (DPGD) of Maddison et al. (2019), an optimal
preconditioning can lead to convergence in one iteration.
The preconditioner is a function that has to be selected properly w.r.t. f to obtain the desired linear
convergence. Although Maddison et al. (2019) gives some hints to precondition DPGD, there is
currently no analytical way for finding a suitable preconditioner.
In this paper, we propose to learn the preconditioner of DPGD using a neural network. We make the
following contribution:
•	We propose a supervised learning setting to learn the preconditioner of an optimization
algorithm (DPGD, Maddison et al. (2019))
•	We present a general methodology that allows to effectively learn the preconditioner while
avoiding issues related to this task
•	We implement this methodology in dimension one and 50 and show that this can lead to
dramatic speed-ups.
The remainder is organized as follows. The next section provides background knowledge on the
DPGD algorithm. Then, in section 3 we present our supervised learning setting. Finally, we apply
this methodology in section 4. Additional developments are provided in the appendix as well as
postponed proofs.
2	Background on dual preconditioning
The main message of this section is that a good preconditioner is a preconditioner that is "close" to
Vf ?, where f ? is the Fenchel conjugate of f.
1
Under review as a conference paper at ICLR 2020
2.1	Legendre functions
Before introducing DPGD algorithm, we need to define the class of functions f that we aim to
minimize. These functions are called Legendre convex functions and are studied in (Rockafellar,
1970, Section 26). We recall their definition and some of their properties. Given a function f : Rd →
(-∞, ∞], the domain of f is the set {x ∈ Rd, f(x) < ∞}. We denote D(f) the interior of the
domain of f .
Definition 1. The function f is Legendre if D(f) is not empty, and if
•	the function f is lower semicontinuous on Rd, and is differentiable and strictly convex on
D(f),
•	the gradient Vf is coercive on D(f): For every Sequence (xi)i∈N ∈ D(f) converges to a
Point on the boundary ofthe domain of f, kVf (xi) k → ∞.
Given a Legendre function f, one can define its Fenchel conjugate f? (y) = supx∈Rd hx, yi - f (x).
Proposition 1 (Rockafellar (1970)). The function f is Legendre if and only if its Fenchel conjugate
f? is Legendre, in which case Vf is one-to-one between D(f) and D(f ?). Moreover, Vf is
continuous in both directions, for every x ∈ D (f), Vf ? (Vf (x)) = x and for every y ∈ D(f?),
Vf(Vf?(y)) = y.
Proposition 2 (Rockafellar (1970)). If f is Legendre, then it either has no minimizer, or one
unique global minimizer x? ∈ D(f). If f admits a minimizer, then Vf(x?) = 0, 0 ∈ D(f ?) and
x? = Vf?(0).
2.2	DPGD algorithm (Maddison et al. (2019))
Consider a convex Legendre function f : Rd → (-∞, +∞]. To minimize f, the DPGD algorithm is
written
x+ = x - γ [Vp(Vf (x)) - Vp(0)] ,	(1)
where γ > 0 and p : Rd → (-∞, +∞] are parameters. DPGD is well defined over the set of
Legendre convex functions f , see Maddison et al. (2019). The map Vp is called a non-linear
Preconditioner of Vf . The preconditioning is the design of Vp.
2.3	Convergence theory of DPGD
In the sequel, we consider a Legendre function f admitting a minimizer x? . The preconditioning
can dramatically affect the convergence of DPGD. Indeed, if Vp = Vf? , then DPGD algorithm
converges in one iteration. To see this, note that in this case
x+ = x - [x - Vp(0)] = Vp(0) = Vf ? (0),
ifγ = 1 and recall that Vf?(0) is the minimizer of f (Proposition 2). Of course, this preconditioning
is unrealistic since computing Vf?(0) is as hard as minimizing f.
It is more realistic to aim to find a preconditioner p that satisfies the following conditions.
Assumption 1 (Dual relative smoothness). There exists L ≥ 0 such that
hVp(x) - Vp(y), x - yi ≤ LhVf?(x) - Vf?(y),x - yi,	∀x,y ∈ D(p).	(2)
Assumption 2 (Dual relative strong convexity). There exists μ > 0 such that
(Vp(x) - Vp(y),x — yi ≥ μhVf *(x) —Vf*(y),x — yi,	∀x,y ∈ D(p).	(3)
If p and f? are twice differentiable, these assumptions can be written as
μV2f? 5 V2p √ LV2f?,
for the ordering of nonnegative matrices. These assumptions mean that Vp must be chosen close
to Vf? in some sense. Moreover, they are sufficient for the linear convergence of DPGD.
2
Under review as a conference paper at ICLR 2020
Theorem 1 (Maddison et al. (2019), Informal). If assumption 1 holds and if p is Legendre, then
the sequence of iterates of DPGD converges to x? = arg min f. If assumption 2 holds, then the
convergence is linear.
In comparison, gradient descent (and many other first order methods, Nesterov (2018)) converges for
smooth convex functions f (i.e. functions f such that Vf is LiPschitz continuous). Moreover, the
convergence is linear if f is strongly convex. Many functions satisfy assumption 1 (and assumption 2)
for some Preconditioner p, without being smooth (or strongly convex), see Maddison et al. (2019).
If f is smooth and strongly convex, the convergence rate of gradient descent dePends on the global
condition number κ of f (defined as the LiPschitz constant of Vf divided by the strong convexity
Parameter). If κ is high, then the linear convergence of gradient descent is slow and f is said
ill-conditioned. The function f is said well-conditioned else. When f is ill-conditioned, it might be
well-conditioned locally (i.e., on small subsets of Rd) but the rate of convergence of gradient descent
only takes into account the global, high condition number. This is not the case for Preconditioned
algorithms ( Li & Malik (2016); Maddison et al. (2019)).
3	Supervised learning setting
In this section, we describe our methodology to learn Vf? .
3.1	Formulation
Our idea is to samPle Points xi ∈ D(f) according to some distribution and create the dataset
{(Vf(xi), xi), i ∈ {1, . . . , n}} where the Vf(xi) rePresent the features and xi the labels. The
feature sPace is D(f ?) and the label sPace is D(f) (ProPosition 1). Since Vf? maPs Vf(xi) to xi
(ProPosition 1), Vf? can be seen as the solution of this suPervised learning Problem. We choose a
neural net to modelize Vf? . To solve our suPervised learning Problem, we design a machine learning
algorithm. The goal of the training of the machine learning algorithm is to minimize the theoretical
risk L”(θ)
Lμ(θ) = EX〜μ ('(Models* (Vf(X),θ), X)),	(4)
where μ is the distribution of the labels X, θ represents the parameters of the neural network,
Modelvf? (Vf (X), θ) is the output of the neural network with input Vf (X) and parameter θ, and
'(Models * (Vf(X ),θ),X)
is the loss associated to one sample (Vf(X), X).
To fully specify the machine learning algorithm, we need to fully define Lμ and the way we approxi-
mate it. We proceed by
• Choosing an architecture to specify the function Modelvf* (this is problem specific, see
sections 3.2, 4.2)
• Giving a value for μ (section 3.3)
• Providing a way to approximate the expectation in (4) by a finite sum (section 3.4)
• Choosing a loss ' (this is problem specific, see section 4.2).
Once this is done, the algorithm is trained using Stochastic Gradient Descent (SGD) (Bottou et al.
(2018)).
3.2 General setup
In this section we give a general description of the map Modelvf* . According to recommendations
in Bengio (2012), we first standardize the features Vf(xi) and the labels xi. In other words, we apply
a diffeomorphism (i.e, a one-to-one map which is continuously differentiable in both directions) G :
D(f?) → [-0.5, 0.5]d to the features Vf(xi) and a diffeomorphism H-1 : D(f) → [-0.5, 0.5]d to
the labels xi . Then a neural network is used to learn a predictor from the standardized features to the
standardized labels, see figure 1. One can obtain a label from a standardized label by applying the
3
Under review as a conference paper at ICLR 2020
inverse map H of H-1, which can be easily computed (in practice H-1 is essentially an addition
and a multiplication by a positive number), see section 4.
Figure 1: General model setup
3.3	Choice of μ
We found that the choice of the distribution μ that parametrized Lμ is critical for the performance of
the machine learning algorithm. The reason is the following. Recall that We want to learn Vf ? to
precondition DPGD algorithm whose goal is to minimize nonsmooth and non-strongly convex (or
ill-conditioned) objectives f , see section 2. Let us imagine that we chose an uniform distribution for
μ. Then, the situation in dimension one is represented by the figure 2.
Over-represented
Uniform sampling
Under-represented
Figure 2: The curve of Vf is typical of objectives f that we want to minimize
The image distribution V of μ by Vf over-represents some areas of the feature space while other
areas are under-represented. When the distribution of the features is as degenerated as ν, it is obvious
that machine learning algorithms cannot generalize well (see Bengio (2012); Mesnil et al. (2011)).
Therefore, We have to choose μ carefully. The classical recommendation is to ensure that the input of
the neural network (i.e the standardized features) are uniformly distributed (Mesnil et al. (2011)). In
other words, we have to find μ such that, if Xi 〜μ, then G ◦ Vf (Xi) is uniformly distributed over
[-0.5, 0.5]d.
This can be done by using the change of variable formula.
Proposition 3. Assume that Vf and Vf ? are differentiable. If μ is the distribution with density
proportional to
| det(JGwf (x))|
with respect to Lebesgue measure (where J denotes the Jacobian matrix), then the distribution of
G ◦ Vf(X) where X 〜μ is uniform over [-0.5, 0.5]d.
4
Under review as a conference paper at ICLR 2020
In the sequel, μ is the distribution whose density is given by Proposition 3. Inspecting figure 2, this
seems reasonable since we expect X 〜μ to have a high density when Vf,s slope is large and a low
density when VfS slope is small.
Remark 1. It is worth noting that the evaluation of μ doesn't require any inversion as in the inverse
transform sampling technique. Besides, second order optimization algorithms usually require (an
approximation of) the inverse of the Hessian matrix, which we don’t need.
3.4	Estimation of the loss
At this step, μ is chosen and Lμ has to be minimized by SGD. As usual, Lμ cannot be computed
in closed form. The classical idea is to approximate Lμ by an empirical mean involving samples
from the distribution μ. Sampling from μ turns out to be difficult in our case. Indeed, the use of
rejection-based MCMC methods lead to a high rejection rate. Since we can evaluate the density of μ,
we suggest to use importance sampling instead.
Proposition 4. If Xi 〜π where the density of π is positive, then
E (l XX'(Models* (Vf(Xi),θ), Xi)等)=Lμ(θ),
where μ(x) (resp. π(x)) denotes the density of μ (resp. π) with respect to Lebesgue measure.
We call the map θ → 1 PZi '(Models* (Vf (xi), θ),Xi)∏μ(χi) the empirical risk. Proposition 4
states that if Xi 〜∏, then the empirical risk is an unbiased estimator of the theoretical risk. Since this
estimator will be minimized using SGD, we only need to know it up to a constant factor. Therefore we
can get rid ofthe 1/n factor and of constant factors defining the densities of μ and ∏. The distribution
π is called the proposal distribution.
3.5	Related works
Learning an optimization algorithm has already been proposed in the literature. In Li & Malik
(2016); Andrychowicz et al. (2016) the authors propose to learn a better optimization algorithm
by observing its execution over a feature space of objective functions. This outperforms existing
hand-engineered algorithms in terms of convergence speed, but require to know the performance of
some algorithms over some objectives. Using optimization literature knowledge, our approach allows
to reduce the problem to a supervised learning setting in Rd, that we can solve with a neural net. In
its philosophy, our approach can also be related to deep learning methods for inverse problems used
in signal processing, see e.g Lucas et al. (2018).
4	Numerical experiment
In this section, we run DPGD algorithm after learning Vf? . We first consider a one dimensional
toy problem with a ground truth involving power functions as in Maddison et al. (2019). Then
ill-conditioned logistic losses are considered. These functions cannot be minimized (efficiently) by
gradient descent. We call LDPGD our approach (for Learned DPGD).
4.1	Power functions and logistic loss
Two class of objective functions are considered. Power functions are defined by
1	( a ∈ (2, +∞)
f : x → - kAx 一 Cka , where c c ∈ Rd	(5)
IA is a matrix.
These functions are nonsmooth and non-strongly convex, therefore gradient descent is not guaranteed
to minimize them. Power function minimization is the main example of Maddison et al. (2019).
We also consider the logistic loss
1n	r 2
f ： x → - £log(1 + exp(—bi hai,xi)) + 2∣∣xk2,	(6)
n i=i
5
Under review as a conference paper at ICLR 2020
where r > 0 and for every i, bi ∈ {-1, 1}n and ai ∈ Rd. These functions are strongly-convex and
smooth. It is easy to control the condition number of f:
Lemma 1. Consider the logistic loss f and the matrix A whose lines are the ai, and denote λmax
the largest eigenvalue of the real symmetric matrix ATA. Denote L the smoothness parameter of f1
and κ the condition number of f. Then,
L ≤ L0 = max + r, and K ≤ max + 1.
4n	4nr
4.2 Experimental setup
We use a three layers neural network, with leaky ReLU activations and respectively 256 and 128
neurons in the two hidden layers. The square loss is used '(y,χ) = 2∣∣y - x∣∣2. The proposal
distribution π for the importance sampling is taken gaussian. In the case of power functions, the
features are rescaled using the map
log(1 +x) ifx > 0
log-rescaling : x 7→
- log(1 - x) else.
Justification for this choice is provided in the appendix (section D). Then the rescaled features and the
labels are normalized to have zero mean and unit variance. The general architecture is summarized in
figure 3.
Figure 3: Model with log-rescaling
The two first blocks correspond to the map G and the last block to the map H of section 3.2.
The model is trained with a dataset of 1000 samples.
4.3	Results for power functions
Consider f a power function (5) where d = 1, a = 50, c = 50 and A is randomly chosen. We train
the model during 100 epochs.
The next figure represents the performance of the machine learning algorithm we developped to learn
Vf * (note the scale of the inputs of Vf *, see section D).
1 i.e, the Lipschitz constant of Vf
6
Under review as a conference paper at ICLR 2020
Figure 4: Learning Vf *. Left: Learning curves. Right: predictions.
We see that Vf ? is accurately learned the machine learning algorithm. This approximation of Vf ?
is then used as a preconditioner of DPGD. The value of the objective function and the value of the
iterates while running LDPGD is plotted in the next figure. The stepsize is set to 1 during 10 steps
and LDPGD is initialized at x0 = 100.
Figure 5: Minimizing a one dimensional power function. Left: Objective function values. Right:
iterates values.
The iterates of gradient descent always diverge (no matter the stepsize choice). On the contrary,
LDPGD iterates quickly converge to 357.1 (with Vf (357.1) = 3.8 × 10-14) which is close to the
actual minimizer (356.4 in this example).
4.4	Results for logistic regression
Consider f a logistic loss (6) with d = 50, n = 1000, and condition number κ. We train the model
during 100 epochs. and represent the evolution of the objective function and the iterates while running
LDPGD (after learning Vf?) and gradient descent (GD).
The algorithms are initialized at (50, 50, 50)T and we use a stepsize for which GD converges linearly2.
This is done for several logistic losses with worsening condition numbers. To vary κ, we vary the
parameter r that controls the strong convexity of the objective f (see Lemma 1). Intuitively, if r is
high then f is well-conditioned and if r is low f is ill-conditioned. The left figure represents the
iterates (each curve corresponds to the evolution of one coordinate of the iterates) and the right figure
represents the objective fonction values while running both algorithms.
2GD converges linearly if the stepsize is smaller than 1/L, where L is the Lipschitz constant of Vf.
Therefore, it is enough to take a stepsize smaller than 1/L0 where L0 is defined in Lemma 1
7
Under review as a conference paper at ICLR 2020
5040302010
Iterates
4000	6000
step i
8000	10000
Figure 6: Logistic regression with r
0.5
Figure 7: Logistic regression with r = 0.1
Figure 8: Logistic regression with r = 0.05
Our careful preconditioning allows LDPGD to outperform GD especially when the objective is
ill-conditioned, as predicted by the theory, see section 2.
8
Under review as a conference paper at ICLR 2020
References
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient
descent. In Advances in neural information processing systems, pp. 3981-3989, 2016.
Yoshua Bengio. Practical recommendations for gradient-based training of deep architectures. arXiv
e-prints, art. arXiv:1206.5533, Jun 2012.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223-311, 2018.
Ke Li and Jitendra Malik. Learning to Optimize. arXiv e-prints, art. arXiv:1606.01885, Jun 2016.
Haihao Lu, Robert M. Freund, and Yurii Nesterov. Relatively-Smooth Convex Optimization by
First-Order Methods, and Applications. arXiv e-prints, art. arXiv:1610.05708, Oct 2016.
Alice Lucas, Michael Iliadis, Rafael Molina, and Aggelos K Katsaggelos. Using deep neural networks
for inverse problems in imaging: beyond analytical methods. IEEE Signal Processing Magazine,
35(1):20-36, 2018.
Chris J. Maddison, Daniel Paulin, Yee Whye Teh, and Arnaud Doucet. Dual Space Preconditioning
for Gradient Descent. arXiv e-prints, art. arXiv:1902.02257, Feb 2019.
Gregoire Mesnil, Yann Dauphin, Xavier Glorot, Salah Rifai, Yoshua Bengio, Ian Goodfellow, Erick
Lavoie, Xavier Muller, Guillaume Desjardins, David Warde-Farley, et al. Unsupervised and transfer
learning challenge: a deep learning approach. In Proceedings of the 2011 International Conference
on Unsupervised and Transfer Learning workshop-Volume 27, pp. 97-111. JMLR. org, 2011.
Arkadii Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method
efficiency in optimization. 1983.
Yurii Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.
R. Tyrrell Rockafellar. Convex analysis. Princeton Mathematical Series. Princeton University Press,
Princeton, N. J., 1970.
9
Under review as a conference paper at ICLR 2020
Appendix
Contents
1	Introduction	1
2	Background on dual preconditioning	1
2.1	Legendre functions ...................................................... 2
2.2	DPGD algorithm (Maddison et al. (2019)) ................................. 2
2.3	Convergence theory of DPGD .............................................. 2
3	Supervised learning setting	3
3.1	Formulation ............................................................. 3
3.2	General setup ........................................................... 3
3.3	Choice of μ.............................................................. 4
3.4	Estimation of the loss .................................................. 5
3.5	Related works ........................................................... 5
4	Numerical experiment	5
4.1	Power functions and logistic loss ....................................... 5
4.2	Experimental setup ...................................................... 6
4.3	Results for power functions ............................................. 6
4.4	Results for logistic regression ......................................... 7
A	Proof of Proposition 3	11
B	Proof of Proposition 4	11
C	Proof of Lemma 1	11
D	Log-rescaling	12
10
Under review as a conference paper at ICLR 2020
A Proof of Proposition 3
Recall that G is a diffeomorphism and that f is Legendre. Let U the uniform distribution over
[-0.5, 0.5]d and denote q its density with respect to Lebesgue measure. Under the assumption of
Proposition 3, Vf : D(f) → D(f?) is a diffeomorphism. Therefore, the map M = G ◦ Vf :
D(f) → [-0.5,0.5]d is also a diffeomorphism and its inverse map is M-1 = Vf? ◦ G-1 :
[-0.5,0.5]d → D(f). Our problem is to find a distribution μ supported by D(f) such that M(X)=
G ◦ Vf (X)〜U if X 〜μ. Such μ satisfies for every nonnegative measurable function φ,
∕φ(x)dμ(X)=J'φ(M-1 ◦M (X))dμ3
=EX〜μ(Φ(MT ◦ M(X)))
=EU 〜u(Φ(M T (U)))
=	φ(M -1 (u))q (u)du
=	φ(M-1 (M(X)))q(M(X))| det(JM (X))|dX
=	φ(X)q(M (X))| det(JM (X))|dX,
where JM denotes the Jacobian matrix of M and the penultimate equality follows from the change of
variable formula (which is allowed because M is a diffeomorphism). Therefore, the only possible
solution to our problem is the distribution μ with density proportional to | det( Jm(x))| if M(x) ∈
[-0.5, 0.5]d (which is always satisfied by definition of M) and 0 else. One can check that this density
solves the problem.
B Proof of Proposition 4
We apply the importance sampling principle to the measurable nonnegative function (the parameter θ
is fixed)
φ(x) = '(Models* (Vf(X),θ), x).
We have
EY〜μ(Φ(Y))= EX〜∏ (φ(X)∏(X)) = E (1 X φ(xi)μ(xj
where Xi 〜π.
C Proof of Lemma 1
Denote L the smoothness constant of f (i.e the Lipschitz constant of its gradient) and λ its strong
convexity parameter. For every X ∈ Rd ,
Vf(X)
V2f(X)
1n
-V-biai
n
i=1
1
1 + exp (bi hai,xi))
+ rX
1 X^ b2	T	exp (bi hai, Xi))
n i=1 i ɑiɑi (1+exp(bi hai,Xi)))2
+ rId,
where Id is the identity matrix. Note that for all a = 1,鼻+可 ≤ 1, and that b ≤ 1. It follows that
V2f
AT A
4n
+ rId .
Therefore, L ≤ λmx + r. Since K = L∕λ, the result follows from the fact that r ≤ λ (see Equation 6).
11
Under review as a conference paper at ICLR 2020
D Log-rescaling
For power functions, Vf (x) can range in [-10100, 1O100] if X is sampled from a gaussian. Assume
d = 1 that we use Doubles precision in the implementation. This means that all the values we
manipulate are stored with a precision of approximately 15 decimal places.
If we naively standardize (Vf(xi))i∈[1,n] to have zero mean and unit variance, we rescale numbers
in the range [-10100, 10100]d, to the range [-0.5, 0.5]d. This means that a precision of 10-15 in the
standardized scale represents a precision of 1085 in the original scale.
In practice, this loss of precision means that inputs that are different before standardization, might
be indistinguishable after the standardization. As a result, the neural network might receive many
different outputs for a single input value.
As it tries to minimize the empirical risk, the model will tend to associate to input values an average
of the corresponding outputs. As a consequence, the regressed model will be a piecewise constant
function, and will be constant on subsets of diameter 1085.
This is especially problematic for inputs close to 0. Let us denote N0 the neighborhood of 0 on which
the regressed model is constant equal to Modelvf* (0, θ). Any X ∈ D(f) for which Vf (x) ∈ N0 is
a fixed point of the DPGD alogorithm:
[Modelvf*(Vf(X),θ) -Modelvf*(0,θ)]
[0]
=X
The consequence is very undesirable: the solution offered by the DPGD alogorithm might be X even
though Vf (X) = O(1085) is far from zero.
ɪ%ɪ%
XX
==
+
X
12
Under review as a conference paper at ICLR 2020
Figure 9: Regressed model on the imprecise dataset
As mentioned above, the lack of precision due to a naive standardization of the features is especially
problematic around 0. We therefore propose to preserve the scale around small values more than
around large values. This can be achieved by applying the following transformation to the features
before normalizing them (to have zero mean and unit variance):
log(1 +x) ifx > 0
log-rescaling : x 7→
- log(1 - x) else.
Figure 10: Log-rescaling.
13