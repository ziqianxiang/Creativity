Under review as a conference paper at ICLR 2020

AN   INFORMATION   THEORETIC   APPROACH   TO   DIS-
TRIBUTED  REPRESENTATION  LEARNING

Anonymous authors

Paper under double-blind review

ABSTRACT

The problem of distributed representation learning is one in which multiple sources of
information X1, . . . , XK  are processed separately so as to extract useful information about
some statistically correlated ground truth Y . We investigate this problem from information-
theoretic grounds. For both discrete memoryless (DM) and memoryless vector Gaussian
models, we establish fundamental limits of learning in terms of optimal tradeoffs between

relevance and complexity.  We also develop a variational bound on the optimal tradeoff
that generalizes the evidence lower bound (ELBO) to the distributed setting. Furthermore,
we provide a variational inference type algorithm that allows to compute this bound and
in which the mappings are parametrized by neural networks and the bound approximated
by Markov sampling and optimized with stochastic gradient descent. Experimental results
on synthetic and real datasets are provided to support the efficiency of the approaches and
algorithms which we develop in this paper.

1    INTRODUCTION

Let a measurable variable X           and a target variable Y            with unknown joint 
distribution PX,Y   be
given.  In the classic problem of statistical learning, one wishes to infer an accurate predictor 
of the target
variable Y  ∈ Y  based on observed realizations of X ∈ X. That is, for a given class F  of 
admissible predictors

φ  :  X  → Y and an additive loss function l : Y  → Yˆ that measures discrepancies between true 
values and their

estimated fits, one aims at finding the mapping φˢ ∈ F  that minimizes the expected risk

CPX,Y  (φ, l) = EPX,Y  [l(Y, φ(X))].                                                      (1)

Because the joint distribution PX,Y   is unknown, in practice the risk equation 1 (also called 
population risk)
cannot be computed directly; and, in the standard approach, one usually resorts to choosing the 
predictor with

minimal risk on a training dataset consisting of n labeled samples {(xi, yi)}ⁿ    that are drawn 
independently

from the unknown joint distribution PX,Y . Also, it is important to restrict the set     of 
admissible predictors to a
low-complexity class to prevent overfitting. This leads to the abstract inference problem shown in 
Figure 1.

In this paper, we study a generalization of this problem in which the prediction is to be performed 
in a distributed
manner. The model is shown in Figure 2. Here, the prediction of the target variable Y          is 
to be performed
on  the basis of samples of statistically correlated random variables (X1, . . . , XK ) that are 
observed each at a
distinct predictor. We investigate this problem in the case in which the loss function l( ) is the 
logarithmic-loss
fidelity measure, given by

llog(y, yˆ) = log       1                                                                        
(2)

yˆ(y)

where yˆ( ) designates a probability distribution on     and yˆ(y) is the value of this 
distribution evaluated for
the outcome y         . The choice of a ‘good” loss function is often controversial in statistical 
learning theory,
and although a complete and rigorous justification of the usage of logarithmic loss as a fidelity 
measure in
learning theory is still awaited, partial explanations appeared in Jiao et al. (2015) and, 
especially in Painsky and
Wornell (2018) where it is shown that, for binary classification problems, by minimizing the 
logarithmic-loss
one actually minimizes an upper bound to any choice of loss function that is smooth, proper (i.e., 
unbiased and
Fisher consistent) and convex. Also, we constrain the complexity of the predictors by using mutual 
information
as   a regularizer term. This is inline with recent works Xu and Raginsky (2017); Russo and Zou 
(2015) that show
that the generalization error can be upper-bounded using the mutual information between the input 
dataset and
the output of the predictor – see also Bousquet and Elisseeff (2002); Shalev-Shwartz et al. (2010) 
where the
stability of an algorithm is controlled by constraining the mutual information between its input 
and output.

1


Under review as a conference paper at ICLR 2020


Y ∈ Y

PX|Y

X ∈ X

U = φ(X)

φ                                 ψ

Y^  ∈ Y

Figure 1: An abstract inference model for learning.


Y  ∈ Y

X1                                               U1

1

PX1 ,...,XK |Y                ·                      ψ

·

XK                                          UK

φk

Y^  ∈ Y

Figure 2: A model for distributed, e.g., multi-view, learning.

1.1    AN EXAMPLE: MULTI-VIEW LEARNING

In many data analytics problems, data is collected from various sources of information or feature 
extractors;
and is intrinsically heterogeneous.  For example, an image can be identified by its color or 
texture features;
and a document may contain text and images.  Conventional machine learning approaches concatenate 
all
available data into one big row vector (or matrix) on which a suitable algorithm is then applied.  
Treating
different observations as a single source might cause overfitting and is not physically meaningful 
because each
group of data may have different statistical properties.  Alternatively, one may partition the data 
into groups
according to samples homogeneity, and each group of data be regarded as a separate view. This 
paradigm, termed
multi-view learning Xu et al. (2013), has received growing interest; and various algorithms exist, 
sometimes
under references such as co-training Blum and Mitchell (1998); Dhillon et al. (2011); Kumar and 
Daume´ (2011);
Go¨ nen and Alpaydın (2011), multiple kernel learning Go¨ nen and Alpaydın (2011) and subspace 
learning Jia
et al. (2010). By using distinct encoder mappings to represent distinct groups of data, and jointly 
optimizing over
all mappings to remove redundancy, multiview learning offers a degree of flexibility that is not 
only desirable in
practice but is likely to result in better learning capability. Actually, as shown in Vapnik 
(2013), local learning
algorithms produce less errors than global ones. Viewing the problem as that of function 
approximation, the
intuition is that it is usually non-easy to find a unique function that holds good predictability 
properties in the
entire data space.

1.2    INFORMAL SUMMARY OF RESULTS

In this paper, first we characterize the optimal tradeoff between relevance and complexity for the 
distributed
learning model of Figure 2 for both discrete memoryless (DM) and memoryless vector Gaussian models. 
While
the result for the discrete data model (Theorem 1) is not difficult to establish using connections 
with Courtade
and Weissman (2014, Appendix B) which we explicit here, the result for the multivariate Gaussian 
data model
(Theorem 2), which provides a sharp analytic characterization of optimal tradeoffs, is new and 
non-trivial (the
proof  of the converse part is not straightforward and was missing before this work in both 
theoretic learning and
information theoretic communities including in the scalar case). Second, we develop a variational 
bound on the
optimal tradeoff that can be seen as a generalization of the ELBO and the β-VAE criteria Higgins et 
al. (2016) to
the distributed setting. Furthermore, for both DM and Gaussian models, we also provide a 
variational inference
type algorithm which is parametrized by neural networks and allows to compute the developed 
variational bound
when the data distribution is not known. Specifically, the main contributions of this paper are:

In Section 3.2, we find an explicit analytic characterization of optimal tradeoffs between relevance
and complexity for the memoryless vector Gaussian model.  The result generalizes the Gaussian
Information Bottleneck method of Globerson and Tishby (2004); Chechik et al. (Feb. 2005) to the
distributed learning scenario.

In Section 3.3, we study the problem of maximizing relevance under a constraint on the sum 
complexity
for which we establish a variational bound which generalizes the ELBO and the β-VAE criteria to the
distributed setting.

Section 3.4 is algorithmic-oriented. We develop a variational inference type algorithm which enables
to compute the bound. This algorithm is obtained by parametrizing the encoders, the decoder, and the
prior distributions via DNNs and using Monte-Carlo sampling. Also, it makes usage of Kingma et

2


Under review as a conference paper at ICLR 2020

al.’s re-parametrization trick Kingma and Welling (2013) and can be seen as a generalization of the
variational information bottleneck algorithm in Alemi et al. (2017) to the distributed setting.

Section  4  contains  some  experimental  results  on  real  datasets  which  show  the  efficiency 
 of  the
approaches and algorithms that we develop in this paper.

Most relevant to this paper is the single-encoder Information Bottleneck (IB) method of Tishby et 
al. (1999)
which readily and elegantly captures the above mentioned viewpoint of seeking the right balance 
between data
fit and generalization by using the mutual information both as a cost function and as a regularizer 
term. Thus,
the results of this paper can be seen as a generalization of those of Tishby et al. (1999) for the 
DM model
and Globerson and Tishby (2004); Chechik et al. (Feb. 2005) for the Gaussian model to the 
distributed learning
setting.

Remark: Due to space constraints, the proofs of the results of this paper are deferred to the 
appendices section,
which also contains additional experimental results.

1.3    NOTATION

Throughout,  upper case letters denote random variables,  e.g.,  X; lower case letters denote 
realizations of
random variables, e.g., x; and calligraphic letters denote sets, e.g.,     .  The cardinality of a 
set is denoted by

.  For a random variable X  with probability mass function (pmf) PX , we use PX (x)  =  p(x), x

for short.  Boldface upper case letters denote vectors or matrices, e.g., X, where context should 
make the
distinction clear.  For random variables (X1, X2, . . .) and a set of integers K  ⊆  N, XK   
denotes the set of
random variables with indices in the set K, i.e., XK  = {Xk : k ∈ K}. If K = Ø, XK  = Ø. For k ∈ K 
we let
XK/k = (X₁, . . . , Xk−₁, Xk₊₁, . . . , XK), and assume that X₀ = XK₊₁ = Ø. Also, for zero-mean 
random
vectors X and Y, the quantities Σₓ, Σₓ,y and Σₓ|y denote respectively the covariance matrix of the 
vector
X, the covariance matric of vector (X, Y) and the conditional covariance matrix of X, conditionally 
on Y.
Finally, for two probability measures PX   and QX   on the random variable X  ∈  X, the relative 
entropy or
Kullback-Leibler divergence is denoted as DKL(PX ǁQX ).

2    FORMAL  PROBLEM  FORMULATION

Let K ≥ 2 and (X1, . . . , XK, Y ) be a tuple of random variables with a given joint probability 
mass function
(pmf) PX1 ,...,XK ,Y (x₁, . . . , xK, y) for (x₁, . . . , xK) ∈ X₁ × . . . × XK and y ∈ Y, where Xk 
designates the
alphabet of Xk and Y  that of Y . Throughout, we assume that the Markov chain


Xk −− Y  −− XK/k

holds for all k ∈ K. That is, the joint pmf factorizes as                    K

(3)

PX1 ,...,XK ,Y (x₁, . . . , xK, y) = PY (y)        PXk |Y (xk|y).                                   
 (4)

k=1

The variable Y  is a target variable;  and we seek to characterize how accurate it can be predicted 
from a
measurable random vector (X1, . . . , XK ) when the components of this vector are processed 
separately, each by

a distinct encoder. More specifically, let {(X1,i, . . . , XK,i, Yi)}ⁿ    be a collection of n 
independent copies of

(X₁, . . . , XK, Y ). Encoder k ∈ K only observes the sequence Xⁿ; and generates a description Jk = 
φk(Xⁿ)


according to some mapping

k                                                                                                   
k

n                 (n)

φk : Xk  → Mk   ,                                                                    (5)

where M(n) is an arbitrary set of descriptions. The range of allowable description sets will be 
specified below.

A decoder ψ(·) collects all descriptions JK  = (J1, . . . , JK ) and returns an estimate Yˆ n  of Y 
ⁿ as

ψ : M(n) × . . . × M(n)  → Yˆn.                                                         (6)

The relevance of the estimation Yˆ n  is measured in terms of the relevance, defined here as the 
information
that the descriptions φ1(Xⁿ), . . . , φK (Xⁿ ) collectively preserve about Y ⁿ, as measured by 
Shannon mutual


1                          K

information ¹                                                        K

n                      n                               n


∆(n)(P

) =  1

Σ      P (yn) Y P (xn|yn) log   P (y  , ψ(φ₁(x₁ ), . . . , φK(xK)))  

			


:=  1 I

n

1

PXK,Y

K

(Y ⁿ; Yˆ n),                                                                                        
         (7)

¹Alternatively, the relevance could be defined in a more operational manner by the average 
logarithmic loss
distortion or error EPXK,Y  [llₒg(Y ⁿ, Yˆ n)] = H(Y ⁿ|Yˆ n).

3


Under review as a conference paper at ICLR 2020

where Yˆ n   =  ψ(φ1(Xⁿ), . . . , φK (Xⁿ )) and the subscript PX    ,Y   indicates that the mutual 
information is

1                          K                                                           K

computed under the joint distribution PXK,Y .

There are various ways to control the complexity of the encoding functions {φk}K     . In this 
paper, we do so by

restricting their ranges. This is known as minimum description length complexity measure Hinton and 
van Camp

(1993). Specifically, the mapping φk(·) at Encoder k ∈ K needs to satisfy

R   ≥      log |φ  (X   )|     for all   X    ∈ X   .                                               
 (8)

Definition 1  A tuple (∆, R1, . . . , RK ) is said to be achievable if there exists an integer n, a 
family of encoding


mappings {φk}K

and a decoder mapping ψ such that

∆ ≤     I           .Y ⁿ; ψ(φ  (Xⁿ), . . . , φ   (Xⁿ ))Σ                                   (9)

1

 				

R   ≥      log |φ  (X   )|     for all   k ∈ K.                                                     
     (10)

The relevance-complexity region IRDIB is given by the closure of all achievable tuples (∆, R1, . . 
. , RK ).

In some cases, for given RK   =  (R1, . . . , RK ), for the ease of the exposition we will be 
content with the
relevance-complexity function ∆(RK, PXK,Y ) defined as


∆(RK, PXK

,Y ) =     max

{φk }K

∆(n)(PX

,ψ                        K

,Y )                                                (11)

where the maximization is subjected to equation 8.

3    MAIN  RESULTS

3.1    DISCRETE MEMORYLESS DATA MODEL

The following theorem (the proof of which can be found in the appendices section) provides a 
computable
characterization of the relevance-complexity region       DIB. The result can be seen as a 
generalization of Tishby
et                al. Tishby et al. (1999) single encoder IB to the distributed learning model with 
K encoders.

Theorem 1  The relevance-complexity region IRDIB of the distributed learning problem with PXK,Y   
for which
the Markov chain equation 3 holds is given by the union of all tuples (∆, R1, . . . , RK ) ∈ RK⁺¹ 
that satisfy for

all S  ⊆ K,

∆ ≤        [Rk −I(Xk; Uk|Y, T )] + I(Y ; USc |T ),                                          (12)

k∈S

for some set of pmfs P := {PUk |Xk ,T , . . . , PUk |Xk ,T , PT } with joint distribution of the 
form

K                                        K

PT (t)PY (y) Y PXk |Y (xk|y) Y PUk |Xk ,T (uk|xk, t).                                    (13)

	

Remark 1  In  Theorem  1,  the  random  variable  T  stands  for  a  convexification  of  the  
region,  i.e.,  convex
combination of achievable relevance-complexity tuples is itself achievable.  For given T  =  t, the 
result of
Theorem1 comprises the optimization over K conditional distributions {PUK |Xk ,t}. For k ∈ K, the 
conditional
distribution PUK |Xk ,t  represents a stochastic encoding of the feature Xk into a latent variable 
Uk. Intuitively,
the latent variable Uk should capture all relevant information about Y  that is contained in Xk and 
non redundant

with those carried out by {Ui}i  k. The requirement of non-redundancy is mandated by the need to 
operate at the

minimum possible complexity at which a desired relevance level is achievable (recall that minimum 
complexity,
as expressed by algorithm’s input-output mutual information, translates directly into a better 
generalization
capability). Collectively, however, the set of all latent variables (U1, . . . , UK ) should be 
expressive enough to
reproduce the target variable Y  to within the desired relevance level.

Remark 2  Like for the single-encoder IB problem of Tishby et al. (1999) and an increasing number 
of works
that followed, including Courtade and Weissman (2014, Section III-F), our approach here is 
asymptotic.  In
addition to that it leads to an exact characterization, the result also readily provides a lower 
bound on the

performance in the non-asymptotic (e.g., one shot) set4ting. For the latter setting known 
approaches (e.g., the
functional representation lemma of Li and El Gamal (2018)) would lead to only non-matching inner 
and outer

bounds on the region of optimal tradeoff pairs, as this is the case even for the single encoder 
case Li et al. (2018).


Under review as a conference paper at ICLR 2020

3.2    MEMORYLESS VECTOR GAUSSIAN DATA MODEL

We now turn to a continuous-alphabet setting. Here, (X1, . . . , XK, Y) is a zero-mean Gaussian 
random vector
such that

Xk = HkY + Nk    for all    k ∈ K,                                                   (14)
where Hk      Cⁿk ×ny   models the linear model connecting the target variable Y      Cⁿʸ to the 
observation
at encoder k, and Nk      Cⁿk , k = 1, . . . , K, is the noise vector at encoder k, assumed to be 
Gaussian with

zero-mean and covariance matrix Σk, and independent from all other noises and the target variable 
Y.  We

denote by Σy  the covariance matrix of of the target vector Y ∈ Cⁿʸ .

For this model, we find an explicit analytic characterization of optimal tradeoffs between 
relevance and complex-
ity. The proof relies on deriving an outer bound on the region described by equation 12, and 
showing that it is
achievable with Gaussian distribution, with no time-sharing. In doing so, we use techniques that 
rely on the de
Bruijn identity and the properties of Fisher information and minimum mean square error (MMSE).


Theorem 2  The relevance-complexity region IRG
tuples (∆, R1, . . . , RL) that satisfy for all S  ⊆ K

for the vector Gaussian model is given by the union of all


for some 0 ≤ Ωk ≤ Σ−¹.

k                  k                                               y          k                     
 y

k∈Sc

Proof: The proof of the direct part follows by evaluating the region of Theorem 1, which can be 
extended to the
case of continuous alphabets using standard discretization (quantization) arguments, with the 
choices T  = Ø and
p(uk|xk, t) = CN(xk, Σ¹/²(Ωk − I)Σ¹/²). The main contribution in the proof is that of the converse 
part.

This proof is technical and rather lengthy and, for this reason, is deferred to the appendices 
section.

In the special case in which K = 1, the result of Theorem 2 recovers that by Globerson and Tishby 
(2004) (see
also Chechik et al. (Feb. 2005)) which establishes the optimal relevance-complexity tradeoff of the 
single-encoder
Gaussian IB problem.

3.3    A VARIATIONAL BOUND

In this section, we consider the problem of learning encoders- and decoder mappings that maximize 
the relevance
level for a given (fixed) complexity level, i.e., those that perform at the vicinity of the 
boundary of the region
DIB. First, we derive a parametrization of the relevance-complexity region; and, then, we develop a 
variational
bound which expresses the optimal encoders’ and decoder mappings as the solution to an optimization 
problem –

(an algorithm for solving this problem in the case of unknown distributions is given in the next 
section).


Let Rsum  := ΣK

Rk. Also, let IRsum  denote the region of achievable (relevance, sum-complexity) pairs,


DIB

+                                            +

(∆, R1, . . . , RK ) ∈ IRDIB  and

kΣ=1

Rk = Rsum,.

Proposition 1  The relevance-complexity region under sum-complexity constraint RIsum is given by 
the convex-


hull of all tuples (∆, Rsum) ∈ R²  satisfying ∆ ≤ ∆(Rsum, PX

.

,Y ) where

Σ                   Σ

 

and where the maximization is over the set of pmfs P  :=  {PU1 |X1 , . . . , PUK |XK }  such that 
the joint pmf

factorizes as pY (y) QK      pX  |Y (xk|y) QK      pU  |X  (uk|xk).

The next proposition provides a characterization of the pairs (∆, Rsum) that lie on the boundary of 
RIˢᵘᵐ in

terms of a nonnegative parameter s ≥ 0.

Proposition 2  For every pair (∆, Rsum) ∈ R²  that lies on the boundary of the relevance-complexity 
region

RIDIB there exist s ≥ 0 such that (∆, Rsum) = (∆s, Rs), where

∆   =       1      Σ(1 + sK)H(Y ) + sR   + max L  (P)Σ ,                                   (16)

s

5


Under review as a conference paper at ICLR 2020

Rs = I(Y ; U ∗ ) + Σ[I(Xk; U ∗) − I(Y ; U ∗)],                                                 (17)

and P∗ is the set of conditional pmfs P that maximize the cost function

K

Ls(P) := −H(Y |UK) − s       [H(Y |Uk) + I(Xk; Uk)].                                   (18)

k=1

Using Proposition 2 it is clear that the encoders   PUk |Xk    k∈K   that achieve the 
relevance-complexity pair
(∆s, Rs) can be computed by maximizing the regularized cost equation 18 for the corresponding value 
of
s     0. The corresponding optimal decoder PY |U    for these encoders can be found as in equation 
??. Different
relevance-complexity pairs (∆s, Rs) on the boundary of       ˢᵘᵐ and encoders- and decoder mappings 
that
achieve it can be found by solving equation 18 for different values of s      0 and then evaluating 
equation 16
and equation 17 for the obtained solution.

The optimization of equation 18 generally requires to compute marginal distributions involving the 
descriptions
U1, . . . , UK , an aspect which can be non-easy computationally costly.  To overcome this 
limitation, in the
following we derive a tight variational bound on Ls(P) which lower bounds the DIB cost function 
with respect
to  some arbitrary distributions. Let us consider the arbitrary decoder QY |U1 ,...,UK (y|u₁, . . . 
, uK) for y ∈ Y,
u₁ ∈  U₁, . . . , uK  ∈  UK, the K decoders QY |Uk (y|uk) for k  ∈  K for y  ∈  Y, uk  ∈  Uk, and 
latent variable
priors QUk (uk), k ∈ K, uk ∈ Uk. For short, we denote

Q := {QY |U1 ,...,UK , QY |U1 , . . . , QY |UK , QU1 , . . . , QUK }.

Let us define the variational DIB cost function LVB(P, Q) as

                                                .                                                   
          Σ

 	 


`     av. logar˛it¸hmic-loss         x

`                             regu˛la¸rizer                                                     x

The following lemma states that LVB(P, Q) is a lower bound to Ls(P) for all distributions Q.

Lemma 1  For fixed pmfs P, we have

Ls(P) ≥ Ls   (P, Q),         for all pmfs Q.                                              (20)

In addition, there exists a unique Q that achieves the maximum maxQ LVB(P, Q) = Ls(P), and is given 
by

Q∗Uk  = PUk ,        Q∗Y |Uk  = PY |Uk ,     k = 1, . . . , K,                                      
(21)

Q∗Y |U1 ,...,Uk  = PY |U1 ,...,UK ,                                                                 
   (22)

where PUk , PY |Uk  and PY |U1 ,...,UK  are computed from the pmfs P.                               
                                 Q

Using the above, the optimization in equation 16 can be written in terms of the variational DIB 
cost function as

max Ls(P) = max max LVB(P, Q).                                                  (23)

P                                 P        Q

We close this section by noting that the cost function equation 19 can be seen as a generalization 
of the evidence
lower bound (ELBO) as given in Rezende et al. (2014); Kingma and Welling (2013) for the 
single-encoder
learning to the distributed setting. Also, in the specific case in which Y  = (X1, . . . , XK ) the 
bound generalizes
the ELBO used for VAEs to the case of an arbitrary number of encoders.

3.4    CASE OF UNKNOWN DISTRIBUTIONS: VARIATIONAL DISTRIBUTED IB ALGORITHM

In practice only a set of training samples {(X1,i, . . . , XK,i, Yi)}ⁿ    are available. In this 
section, we provide

a method to optimize equation 23 in this case by parametrizing the encoding and decoding 
distributions that
are to optimize using a family of distributions whose parameters are determined by Deep Neural 
networks
(DNNs).  This allows us to formulate equation 23 in terms of the DNN parameters and optimize it by 
using
the reparametrization trick Kingma and Welling (2013), Monte Carlo sampling, as well as stochastic 
gradient
descent (SGD) type algorithms.


e
NN,k

denote the parametric family of encoding probability distributions PUk |Xk

over Uk for each element

e         e           le

on Xk. Each member of this collection, PUk |Xk ;γe , is described by a parameter vector γk  ∈ Γk  ⊆ 
R k , where

6


Under review as a conference paper at ICLR 2020

e            le                                                                                     
                                                                   e

Γk   ⊆  R k  denotes the set of allowable parameter vectors.  The parameter vector γk  is the 
output of a DNN

e                                                                              de

fθk  : Xk → Γk , with network parameters θk ∈ Θk ⊆ R  k , e.g., the weights of the network at all 
layers. The

e

DNN fθk  takes Xk as input and outputs the parameter vector γ  , determining one of the probability 
members

PUk |Xk ;γe . We have


k

FNN,k  =

,PUk |Xk ;γe (uk|xk),  for uk ∈ Uk, xk ∈ Xk :     γk = fθk (xk), θk ∈ Θk

,.            (24)

For example, the family of multivariate Gaussian distributions is parametrized by the mean µθ and 
covariance
matrix Σθ , i.e., γk := (µθ , Σθ ). Therefore, given an observation Xk, γk := (µθ , Σθ ) is 
determined by the

k                                      k        k                                                   
                                                                     k        k


output of the DNN fθ

and Fe

is given by PU  |X  ;γ  (uk|xk) = N(uk; µθ , Σθ ).

d

Similarly, for decoders Q         over Y, define the family of distributions parametrized by a 
vector in Γᵈ ⊆ Rˡ


Y |Uk

determined by the output of a DNN fφk

: Uk

→ Γk , with parameters φk

∈ Φk

k               k

⊆ R  k , as

F          = ,Q           d (y|uk),  for y ∈ Y, uk ∈ Uk :     γᵈ = fφ  (uk), φk ∈ Φk,,              
 (25)

and for the distribution QY |UK  over Y  for each element in U₁ × · · · × UK, define the family of 
distributions

d                                          dd                d           dd

parameterized by the output of the DNN fφK  : U₁ × · · · × UK  → ΓK, with φK  ∈ ΦK  ⊆ R  K , and ΓK 
 ⊆ R  K

F       K  = ,Q                     d (y|u₁, . . . , uK), y ∈ Y, uk ∈ Uk : γᵈ = fφ   (u₁, . . . , 
uK), φK  ∈ ΦK,.   (26)

Finally, for the distributions Qϕk (uk) we define the family of distributions with parameter ϕk ∈ 
Ψk ⊆ R k

F          = ,QU  ;ϕ  (uk),  for uk ∈ Uk : ϕk ∈ Ψk,.

In the following, for brevity we use Pθk (uk|xk), Qψk (y|uk), QψK (y|uK) and Qϕk (uk) to denote the 
distribu-
tions parametrized by the DNNs fθk , fψk , fψK  and ϕk, respectively.

By restricting the optimization of the variational DIB cost in equation 23 to the encoder, decoder 
and priors


within the families of distributions Fe

d
NN,k

d
NN,K

p
NN,k

we get

max max LVB(P, Q) ≥  max LNN(θ, φ, ϕ),                                            (27)

where we use the notation θ  := [θ1, . . . , θK ], φ  := [φ1, . . . , φK, φK] and ϕ  := [ϕ1, . . . 
, ϕK ] to denote the
DNN and prior parameters and, the cost in equation 27 is given by


Ls   (θ, φ, ϕ) := EP

Y,X

E

{Pθk (Uk

|Xk )}Σ log QφK

(Y |UK)

Σ .                                                                   ΣΣ

   

Next, we train the DNNs to maximize a Monte Carlo approximation of equation 27 over θ, φ, ϕ using 
SGD.
We use the reparameterization trick Kingma and Welling (2013), to sample from Pθk (Uk|Xk). In 
particular,

we consider Fe        to consist of a parametric family of distributions that can be sampled by 
first sampling

a random variable Zk  with distribution PZk (zk), zk  ∈  Zk  and then transforming the samples 
using some
function gθk    :  Xk × Zk  →  Uk  parameterized by θk, such that Uk  =  gθk (xk, Zk)  ∼  Pθk 
(Uk|xk).  The
reparametrization trick reduces the original optimization to estimating θk  of the deterministic 
function gθk
and allows to compute estimates of the gradient using backpropagation Kingma and Welling (2013).  
The

variational DIB cost in equation 27 can be approximated, by sampling m independent samples 
{uk,i,j}m      ∼

Pθk (uk|xk,i) for each training sample (x₁,i, . . . , xK,i, yi), i  =  1, . . . , n.  Sampling is 
performed by using

uk,i,j  = gφ  (xk,i, zk,j) with {zk,j}m      i.i.d. sampled from PZ  . We then have

Lemp(θ, φ, ϕ) :=   1  Σlog Q     (y |u       , . . . , u        )


s    m      K

+

m

j=1 k=1

log Qφk (yi|uk,i,j)−DKL(Pθk (Uk,i|xk,i)ǁQϕk (Uk,i))Σ

.        (29)

4    EXPERIMENTS: RESILIENCE  TO  NOISE, ROTATION  AND  OCCLUSION

In this experiment, we test the robustness of our method against noise, rotation and random 
occlusion on the
MNIST dataset. Specifically, we combine two types of random occlusions: the first encoder observes 
a digit
from the MNIST that is occluded by a square which is rotated randomly (rotation angle uniformly 
distributed
over [−45ᵒ, 45ᵒ]); and the second encoder observes a noisy version of the same digit corrupted by 
additive noise

7


Under review as a conference paper at ICLR 2020

(noise level uniform between 0 and 3). The noisy pixels are clipped between 0 and 1, with more than 
60% of the
pixels occluded. These occlusions make the problem significantly more involved than the standard 
MNIST (for
which application of our algorithm leads to an relevance of about 99.9%).

We considered a CNN deterministic networks with dropout which achieves a 99.8% for test data on the 
clean
MNIST data. Then, we have trained the same CNN architecture for each of the noisy inputs to the 
encoders,
resulting in a relevance of 92.1% from the input to encoder 1 (randomly rotated occlusion) and 
79.68% from the
input to encoder 2 (noisy clipped image).


Original Y

Figure 3: View 1: occluded. View 2: noisy.

CNN Layers
Encoder k            conv. ker. [5,5,32]-ReLu

maxpool [2,2,2]
conv. ker. [5,5,64]-ReLu

maxpool [2,2,2]
dense [1024]-ReLu
dropout 0.4

dense [256]-relu
Latent space k           dense [256]-ReLu
Decoder 12                dense [256]-ReLu

Decoder k                 dense [256]-ReLu

Table 1: Used CNN architecture.

2.0


1.5

1.0

0.5

0.0

10¹

10²

           C-IB with Rsum  → ∞
D-VIB train n=50000
D-VIB test n=50000

10³

relevance (%)

1 shot                  avg.

D-VIB                                96.16                 97.24

D-VIB-noReg                    96.04                 96.72

C-VIB                                96.01                 96.68

Deterministic CNN            93.18                 93.18


Sum-Complexity  Rsum

Figure 4: relevance v.s. sum-complexity for

n = 50.000 and s ∈ [10−¹⁰, 1].

Independent CNNs       92.1 / 79.68       93.1 / 82.01

Table 2: Achieved relevance levels.

We applied our D-VIB algorithm of Section 3.4 to this model with the CNN architecture of Table 1, 
in which

Encoder k = 1, 2 is parametrized by an nu     = 256 dimensional multivariate Gaussian distribution 
N(µᵉ , Σᵉ )

determined by the output of a DNN fθk  consisting of the concatenation of convolution, dense and 
maxpool
layers with ReLu activations and dropout.  The output of the last layer is followed by a dense 
layer without

activation that generate µᵉ and Σᵉ . The prior is chosen as Qϕ   (u) = N(0, I). Each decoder takes 
the samples

from Pθk (Uk|Xk) and processes its inputs with a dense layer DNN (fφK  and fφk ) each with 256 
neurons and
ReLu activation, which outputs a vector yˆi  of size |Y|  = 10 normalized with a softmax, 
corresponding to a
distribution over the one-hot encoding of the digit labels {0, . . . , 9} from the K observations,

Qφk (yˆk|uk) = Softmax(fφk (Uk)),     k = 1, 2,  and                                       (30)


where Softmax(p)

for

QφK (yˆ|uK) = Softmax(fφK (U₁, U₂))),

p ∈ Rᵈ is a vector with i-th entry as [Softmax(p)]i  = exp(pi)/

d
j=1

exp(pj ).

(31)

Figure 4 shows the relevance-complexity tradeoffs obtained using our D-VIB algorithm of Section 
3.4, with

n  =  50.000 and 15 distinct s-values randomly chosen in the range [10−¹⁰, 1].   For comparison,  
we also
present the performance obtained using three methods among state-of the-art multiview learning 
approaches:

(i) applying a deterministic CNN on the two views concatenated (deterministic CNN), (ii) applying 
the single-
encoder variational IB method of Alemi et al. on the two views concatenated (C-VIB), and (iii) 
learning one
function for each view via a distinct CNNs and optimize all CNNs independently (independent CNNs). 
The
achieved relevance is reported in Table 2. For other experimental results, see the appendices 
section.

We also mention that at a high level our algorithm D-VIB can be considered as performing some form 
of co-
regularization (for instance its Gaussian version is similar to the CCA of Hardoon et al. (2004)). 
Comparatively,
the single-view algorithm C-VIB can be viewed as belonging to the family of co-training style 
algorithms (such
as the co-EM of Nigam and Ghani (2000)) which, as mentioned in the recent survey Zhao et al. 
(2017), override
on single-view algorithms. The performance of D-VIB dominates that of C-VIB, which itself dominates 
co-EM.

8


Under review as a conference paper at ICLR 2020

REFERENCES

A. Alemi, I. Fischer, J. Dillon, and K. Murphy. Deep variational information bottleneck. In ICLR, 
2017. URL

https://arxiv.org/abs/1612.00410.

R. Blahut. Computation of channel capacity and rate-distortion functions. IEEE Transactions on 
Information
Theory, 18(4):460–473, Jul 1972. ISSN 0018-9448.

A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of 
the eleventh
annual conference on Computational learning theory, pages 92–100. ACM, 1998.

O. Bousquet and A. Elisseeff.  Stability and generalization.  Journal of Machine Learning Research, 
2(Mar):
499–526, 2002.

G. Chechik, A. Globerson, N. Tishby, and Y. Weiss. Information bottleneck for Gaussian variables. 
Journal of
Machine Learning Research, 6:165–188, Feb. 2005.

T. A. Courtade and T. Weissman. Multiterminal source coding under logarithmic loss. IEEE 
Transactions on
Information Theory, 60(1):740–761, Jan. 2014. ISSN 0018-9448. doi: 10.1109/TIT.2013.2288257.

T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley-Interscience, 1991.

A. Dembo, T. M. Cover, and J. A. Thomas. Information theoretic inequalities. IEEE Trans. on Inf. 
Theory, 37
(6):1501–1518, Nov 1991. ISSN 0018-9448. doi: 10.1109/18.104312.

A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em 
algorithm.

Journal of the Royal Statistical Society, Series B, 39(1):1–38, 1977.

P. Dhillon, D. P. Foster, and L. H. Ungar. Multi-view learning of word embeddings via CCA. In 
Advances in
neural information processing systems, pages 199–207, 2011.

R. Dobrushin and B. Tsybakov. Information transmission with additional noise. IRE Transactions on 
Information
Theory, 8(5):293–304, Sep. 1962. ISSN 0096-1000. doi: 10.1109/TIT.1962.1057738.

E. Ekrem and S. Ulukus.   An outer bound for the vector Gaussian CEO problem.   IEEE Transactions on
Information Theory, 60(11), Nov. 2014.

A. El Gamal and Y.-H. Kim. Network information theory. Cambridge University Press, 2011.

A. Globerson and N. Tishby. On the optimality of the Gaussian information bottleneck curve. Hebrew 
University
Tech. Report, 2004.

M. Go¨ nen and E. Alpaydın. Multiple kernel learning algorithms. Journal of Machine Learning 
Research, 12
(Jul):2211–2268, 2011.

D.-R. Hardoon, S. Szedmak, and J. Shawe-Taylor. Canonical correlation analysis: an overview with 
application
to learning methods. Neur. Comput., 16:2639–2664, 2004.

P. Harremoes and N. Tishby. The information bottleneck revisited or how to choose a good distortion 
measure.
In Proc. IEEE Int. Symp. Information Theory, pages 566–570, Jun. 2007.

I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner.  
β-vae:
Learning basic visual concepts with a constrained variational framework. 2016.

G. E. Hinton and D. van Camp. Keeping the neural networks simple by minimizing the description 
length of
the weights. In Proceedings of the Sixth Annual Conference on Computational Learning Theory, COLT 
’93,
pages 5–13, New York, NY, USA, 1993. ACM. ISBN 0-89791-611-5. doi: 10.1145/168304.168306. URL
http://doi.acm.org/10.1145/168304.168306.

Y. Jia, M. Salzmann, and T. Darrell. Factorized latent spaces with structured sparsity. In Advances 
in Neural
Information Processing Systems, pages 982–990, 2010.

J. Jiao, T. A. Courtade, K. Venkat, and T. Weissman.  Justification of logarithmic loss via the 
benefit of side
information. IEEE Transactions on Information Theory, 61(10):5357–5365, 2015.

9


Under review as a conference paper at ICLR 2020

D. P. Kingma and M. Welling.  Auto-encoding variational bayes.  CoRR, abs/1312.6114, 2013.  URL 
http:

//dblp.uni-trier.de/db/journals/corr/corr1312.html#KingmaW13.

D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. ArXiv e-prints, Dec. 2013.

A. Kumar and H. Daume´. A co-training approach for multi-view spectral clustering. In Proceedings 
of the 28th
International Conference on Machine Learning (ICML-11), pages 393–400, 2011.

C.-T. Li and A. El Gamal. Strong functional representation lemma and applications to coding 
theorems. IEEE
Transactions on Information Theory, 64(11):6967–6978, 2018.

C.-T.   Li,   X.   Wu,   A.   Ozgur,   and   A.   El   Gamal.      Minimax   learning   for   
remote   prediction.      In

https://arxiv.org/abs/1806.00071, May. 2018.

K. Nigam and R. Ghani.   Analyzing the effectiveness and applicability of co-training.   In Proc. 
of the 9th
International Conference of Information and Knowledge Management, pages 86–93, 2000.

A. Painsky and G. W. Wornell. On the universality of the logistic loss function. arXiv preprint 
arXiv:1805.03804,
2018.

M. Razaviyayn, M. Hong, and Z.-Q. Luo.  A unified convergence analysis of block successive 
minimization
methods for nonsmooth optimization. SIAM J. on Opt., 23:1126–1153, Jun. 2013.

D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in 
deep
generative models.  In E. P. Xing and T. Jebara, editors, Proceedings of the 31st International 
Conference
on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 1278–1286, Bejing,
China, 22–24 Jun 2014. PMLR.

D. Russo and J. Zou.  How much does your data exploration overfit?  controlling bias via 
information usage.

arXiv preprint arXiv:1511.05219, 2015.

S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan.  Learnability, stability and uniform 
convergence.

Journal of Machine Learning Research, 11(Oct):2635–2670, 2010.

N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. In Proc. 37th Annual 
Allerton Conf.
on Comm., Control, and Computing, pages 368–377, 1999.

V. Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.

A. Xu and M. Raginsky. Information-theoretic analysis of generalization capability of learning 
algorithms. In

Advances in Neural Information Processing Systems, pages 2521–2530, 2017.

C. Xu, D. Tao, and C. Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634, 2013.

J. Zhao,  X. Xie,  X. Xu,  and S. Sun.   Multi-view learning overview:  recent progress and new 
challenges.

Information Fusion, 38:43–54, 2017.

10


Under review as a conference paper at ICLR 2020

APPENDICES

PROOFS OF MAIN THEOREMS, PROPOSITIONS AND LEMMAS.

                     ADDITIONAL EXPERIMENTAL RESULTS                          

5    PROOFS  OF  MAIN  THEOREMS, PROPOSITIONS  AND  LEMMAS

5.1    AUXILIARY LEMMAS

Lemma 2  Dembo et al. (1991); Ekrem and Ulukus (2014) Let (X, Y) be a pair of random vectors with 
pmf

p(x, y). We have

log |(πe)J−¹(X|Y)| ≤ h(X|Y) ≤ log |(πe)mmse(X|Y)|,

where the conditional Fischer information matrix is defined as

J(X|Y) := E[∇ log p(X|Y)∇ log p(X|Y)†],

and the minimum mean squared error (MMSE) matrix is

mmse(X|Y) := E[(X − E[X|Y])(X − E[X|Y])†].

Lemma 3  Ekrem  and  Ulukus  (2014)  Let  (V1, V2)  be  a  random  vector  with  finite  second  
moments  and

N ∼CN(0, ΣN ) independent of (V1, V2). Then

mmse(V2|V1, V2 + N) = ΣN  − ΣN J(V2 + N|V1)ΣN .

5.2    PROOF OF THEOREM 1

If K = 1 the distributed learning problem that we study boils down to the well known Information 
Bottleneck
(IB) problem of Tishby et al. (1999).  The single-encoder IB problem is essentially a remote 
point-to-point
source coding problem Dobrushin and Tsybakov (1962) in which distortion is measured under the 
logarithm
loss fidelity criterion Harremoes and Tishby (2007). In accordance with this analogy, for K      2 
consider the
multiterminal source coding problem under logarithmic loss in which the sequence Y ⁿ models a 
remote source
that            is observed by K spatially distributed agents; the agents observe noisy versions of 
the remote source and
communicate independently with a decoder or Chief Executive Officer (CEO) over rate-constrained 
noise-free
links.        For instance, agent k, k         , observes Xⁿ and uses Rk bits per sample to 
describe it to the decoder.
The decoder wants to reconstruct the remote source Y ⁿ to within a prescribed fidelity level, where 
incurred
distortion is measured using the logarithmic loss criterion, i.e.,


l    (yⁿ, yˆⁿ) =  1 log                            1

,                                   (32)


log

n         ˆ            n              n                               n


where J  = (φ1(Xⁿ), . . . , φK (Xⁿ )).

PY n|J (y  |φ₁(x1 ), . . . , φK(xK ))

1                          K

Here, (Xⁿ, . . . , Xⁿ , Y ⁿ) is assumed to be distributed i.i.d. according to the n-product of the 
pmf PX   ,...,X    ,Y ,

1                K                                                                                  
                                                                                                    
         1          K

i.e., the Markov chain equation 3 holds.

Definition 2  A rate-distortion code (of blocklength n) for the CEO problem consists of K encoding 
functions

φ˜k   :  X n  → {1, . . . , M ⁽ⁿ⁾},     for  k = 1, . . . , K,                                      
  (33)

11


Under review as a conference paper at ICLR 2020


and a decoding function

ψ˜  :  {1, . . . , M ⁽ⁿ⁾} × . . . × {1, . . . , M ⁽ⁿ⁾} → Yˆn.Q                              (34)

A distortion-rate tuple (D, R1, . . . , RK ) is achievable for the DM CEO source coding problem 
with side


information if there exist a blocklength n, encoding functions {φ˜k }K

and a decoding function ψ˜ such that

1            (n)

R   ≥      log M     ,           k = 1, . . . , K,

D ≥ EΣllₒg.Y ⁿ, ψ˜(φ˜1(Xⁿ), . . . , φ˜K (Xⁿ ))ΣΣ.

The distortion-rate region        CEO  of the CEO model is defined as the closure of all 
non-negative tuples

(D, R1, . . . , RK ) that are achievable.                                                           
                                                     Q

Key to the proof of Theorem 1 is the following proposition which states that       DIB and        
CEO can be inferred
from each other.

Proposition 3  (∆, R1, . . . , RK ) ∈ IRDIB if and only if   H(Y ) − ∆, R1, . . . , RK     ∈ DRCEO.

Proof: Let, for k = 1, . . . , K, Jk = φk(Xⁿ) and J  = (J₁, . . . , JK). Then,


E[l

log

(Y ⁿ, Yˆ n)|J  = j] =

yn∈Yn

P (yⁿ j) log           1                                                           (35)

Pˆ(yn|j)


=

yn∈Yn

P (yⁿ

|j) log

P (yⁿ|j)

Pˆ(yn|j)

+ H(Y

|J  = j)                   (36)

= DKL(P (yn|j)ǁPˆ(yn|j)) + H(Y n|J  = j)                                (37)

≥ H(Y   |J  = j),                                                                            (38)

where equation 38 is due to the non-negativity of the Kullback-Leibler divergence and the equality 
holds if and
only if for Pˆ(yⁿ|j) = P (yⁿ|j) where P (yⁿ|j) = Pr{Y ⁿ = yⁿ|J  = j} for all j and yⁿ ∈ Yn.

Let an achievable tuple (∆, R₁, . . . , RK)  ∈  IRDIB be given.  Then, there must exist functions 
{φk}K

such that equation 9 and equation 10 hold. Using equation 38 that by letting the decoding function 
ψ˜(JK) =


{PY n|JK

(yⁿ|JK)}, we have E[llₒg(Y ⁿ, Yˆ n)|JK] = H(Y ⁿ|JK), which implies (H(Y ) − ∆, R₁, . . . , RK) ∈

DRCEO.

The result of Theorem 1 follows easily by combining (Courtade and Weissman, 2014, Theorem 10), 
which

provides a single-letter characterization of the rate distortion region DRs          of the CEO 
problem, and Proposi-

tion 3.

5.3    PROOF OF THEOREM 2

The proof of the direct part of Theorem 2 follows by evaluating the region of Theorem 1 with the 
choice T  = Ø

and p(uk|xk, t) = CN(xk, Σ     (Ωk − I)Σ     ).

The proof of the converse part is as follows. Fix t ∈ T , S  ⊆ K and a family of distributions 
{p(uk|xk, t)}K

such that the joint distribution factorizes as equation 13. Also, let 0 ≤ Ωk,t ≤ Σ−¹ and

mmse(Xk|Y, Uk,t, t) = Σk − ΣkΩk,tΣk.                                             (39)


Such Ωk,t always exists since

Then, we have

0 ≤ mmse(Xk|Y, Uk,t, t) ≤ Σ−¹.                                                    (40)

I(Xk; Uk|Y, t) ≥ log |Σk| − log |mmse(Xk|Y, Uk,t, t)|

= − log |I − Σ¹/²Ωk,tΣ¹/²|,                                                    (41)

where the inequality is due to Lemma 2; and equation 41 is due to equation 39.
Also, we have

I(Y; USc,t|t) ≤ log |Σy| − log |J−1(Y|USc,t, t)|                                                 
(42)

12


Under review as a conference paper at ICLR 2020

		

 k∈Sc

where equation 42 follows by using Lemma 2; and equation 43 holds by using the following equality

J(Y|USc,t, t) =  Σ H† Ωk,tHk + Σ−y ¹.                                              (44)

k∈Sc

the proof of which uses a connection between MMSE and Fisher information as shown next.

For the proof of equation 44, first note that from the MMSE estimation of Gaussian random vectors 
El Gamal
and Kim (2011), we have

Y = E[Y|XSc ] + ZSc  =          GkXk + ZSc ,                                           (45)

k∈Sc


where Gk = Σy|ₓ

H† Σ−¹ and ZSc  ∼ CN(0, Σy|ₓ    ), with

Sc       k      k                                                                 Sc

Σ−¹     = Σ−y ¹ +  Σ H† Σ−¹Hk.                                                   (46)

Note  that  ZSc   is  independent  of  YSc   due  to  the  orthogonality  principle  of  the  MMSE  
and  its  Gaussian
distribution. Hence, it is also independent of USc,q . We have


k∈Sc

k∈Sc

= Σy|x

Σ H† .Σ−¹ − ΩkΣ HkΣy|ₓ

,               (48)

where equation 47 follows since the cross terms are zero due to the Markov chain (Uk,t, Xk) −− Y −−

(UK/k,t, XK/k); and equation 48 follows due to equation 39 and Gk. Finally,


J(Y U      , t) = Σ−¹

y|xSc

Σ−1

y|xSc

mmse             G  X   Y, U      , t    Σ−¹

y|xSc

k∈Sc

(49)


=Σ−1

−  Σ H† .Σ−¹ − Ωk,tΣ Hk                                                           (50)

=Σ−y ¹ +  Σ H† Ωk,tHk,                                                                              
   (51)

k∈Sc

where equation 49 is due to Lemma 3; equation 50 is due to equation 48; and equation 51 follows due 
to
equation 46.

Now, let Ω¯ k  :=      t         p(t)Ωk,t.  The rest of the converse proof follows by averaging 
over the time sharing
random variable to get

I(Xk; Uk|Y, T ) ≥ − Σ p(t) log |I − Σ¹/²Ωk,tΣ¹/²|

≥ − log |I − Σ¹/²Ω¯ k Σ¹/²|,                                                   (52)

where equation 52 follows from the concavity of the log-det function and Jensen’s inequality.  
Similarly to
equation 52, from equation 43 and Jensen’s Inequality we have

		

 k∈Sc

Finally, using equation 52 and equation 53 in equation ??, noting that Ωk  = Σ      p(t)Ωk,t  ≤  
Σ−¹ since

0      Ωk,t       Σk    , and taking the union over Ωk  satisfying 0      Ωk       Σk    , 
completes the proof of the

converse part; and, hence, that of Theorem 2.

13


Under review as a conference paper at ICLR 2020

5.4    PROOF OF PROPOSITION 1

For simplicity of exposition, the proof is given for the case K  = 2 encoders.  The proof for K  > 
2 follows
similarly. By the definition of IRˢᵘᵐ, the accuracy complexity tuple (∆, Rsum) ∈ R²  is achievable 
for some

random variables Y, X1, X2, U1, U2 with joint pmf satisfying equation 13, if it holds that

∆ ≤ I(Y ; U1, U2)                                                                                   
    (54)

∆ ≤ R1 − I(X1; U1|Y ) + I(Y ; U2)                                                        (55)

∆ ≤ R2 − I(X2; U2|Y ) + I(Y ; U1)                                                        (56)

∆ ≤ R1 + R2 − I(X1; U1|Y ) − I(X2; U2|Y )                                       (57)

R1 + R2  ≤ Rsum.                                                                                    
           (58)

The application of the Fourier-Motzkin elimination to project out R1 and R2 reduces the system on 
inequalities
equation 54-equation 58 to the following system of inequalities

∆ ≤ I(Y ; U1, U2)                                                                                   
                          (59)

∆ ≤ Rsum − I(X1; U1|Y ) − I(X2; U2|Y )                                                              
     (60)

2∆ ≤ Rsum − I(X1; U1|Y ) − I(X2; U2|Y ) + I(Y ; U1) + I(Y ; U2)                          (61)

It follows due to the Markov chain U1      X1      Y     X2      U2 that we have I(Y ; U1, U2)      
I(Y ; U1)+I(Y ; U2).
Therefore, inequality equation 61 is redundant as it is implied by equation 59 and equation 60. 
This completes
the proof of Proposition 1.

5.5    PROOF OF PROPOSITION 2

Suppose that P∗ yields the maximum in equation 16. Then,

(1 + s)∆s  = (1 + sK)H(Y ) + sRs  + Ls(P∗)                                                          
                     (62)

= (1 + sK)H(Y ) + sRs + .−H(Y |U ∗ ) − s Σ[H(Y |U ∗) + I(Xk; U ∗)]Σ       (63)

K                                                 k                               k

k=1

= (1 + sK)H(Y ) + sRs  + (−H(Y |U ∗ ) − s(Rs  − I(Y ; U ∗ ) + KH(Y )))            (64)

= (1 + s)I(Y ; U ∗ )                                                                                
                            (65)

≤ (1 + s)∆(Rs, PXK,Y ),                                                                             
                 (66)

where  equation  63  is  due  to  the  definition  of  Ls(P)  in  equation  18;  equation  64  
follows  since  we  have

ΣK     [I(Xk; U ∗) + H(Y |U ∗)] = Rs − I(Y ; U ∗ ) + KH(Y ) from the definition of Rs in equation 
17; and

equation 66 follows from the definition in equation ??.

Conversely, if P∗ is the solution to the maximization in the function ∆(Rsum, PXK,Y ) in equation 
?? such that

∆(Rsum, PX   ,Y ) = ∆s, then ∆s ≤ I(Y ; U ∗ ) and ∆s ≤ Rsum − ΣK      I(Xk; U ∗|Y ) and we have, 
for any


∆(Rsum, PXK,Y ) = ∆s

≤ ∆s  − (∆s  − I(Y ; U ∗ )) − s

.∆s  − Rsum +

K

kΣ=1

I(Xk; Uk∗|Y )Σ

K                                                                                                  
k

k=1

K


K                                                   k

k=1

k

(67)

≤ H(Y ) − s∆s + sRsum + L∗s  + sKH(Y )                                                              
   (68)

= H(Y ) − s∆s  + sRsum + sKH(Y ) − ((1 + sK)H(Y ) + sRs  − (1 + s)∆s)    (69)

= ∆s  + s(Rsum − Rs),                                                                               
                 (70)

where in equation 67 we have ΣK      I(Xk; Uk|Y ) = −KH(Y ) + ΣK      I(Xk; Uk) + H(Y |Uk) due to 
the

Markov chain Uk − Xk − Y  − (XK\k, UK\k); equation 68 follows since L∗s  is the maximum over all 
possible
distributions P (not necessarily P∗ maximizing ∆(Rsum, PXK,Y )); and equation 69 is due to equation 
16.

14


Under review as a conference paper at ICLR 2020

Finally, equation 70 is valid for any Rsum  ≥  0 and s ≥  0.  Given s, and hence (∆s, Rs), choosing 
R = Rs

yields ∆(Rs, PXK,Y ) ≤ ∆s. Together with equation 66, this completes the proof of Proposition 2.

5.6    PROOF OF LEMMA 1

The proof follows by deriving the following bounds. For any conditional pmf QY |Z(y|z), y ∈ Y  and 
z ∈ Z,
e.g., Z  = UK or Z  = Uk, proceeding similarly to equation 38 and averaging over Z, we have

H(Y |Z) = E[− log QY |Z(Y |Z)] − DKL(PY |ZǁQY |Z).                                   (71)


Similarly, we have

Thus, we get

I(Xk; Uk) = H(Uk) − H(Uk|Xk)                                                                        
          (72)

= E[− log QUk (Uk)] − DKL(PUk ǁQUk ) − H(Xk|UK)                           (73)

= DKL(PY |Uk ǁQUk ) − DKL(PUk ǁQUk )                                                   (74)


Ls(P) = L      (P, Q) + DKL(PY |U   ||QY |U

) + s Σ(DKL(PY |U  ||QY |U  ) + DKL(PU  ||QU  ))


s                                                         K             K

VB

k                  k                                k            k

k=1

≥ Ls   (P, Q),                                                                                      
                                        (75)

where equation 75 holds by the non-negativity of relative entropy: and the equality is met if and 
only if Q∗ is as
given by equation 21 and equation 22.

6    OTHER EXPERIMENTAL RESULTS (REGRESSION FOR UNKNOWN GAUSSIAN
MODEL)

6.1    D-VIB ALGORITHM FOR VECTOR GAUSSIAN MODEL

For the vector Gaussian data model equation 14 the optimal distributions P and Q in equation 23 lie 
within
the family of multivariate Gaussian distributions.  Motivated by this observation, we consider the 
following
parameterization for k ∈ K:

Pθ  (uk|xk) = N(uk; µᵉ , Σᵉ )                                                          (76)

Qφ   (yˆ|uK) = N(yˆ; µᵈ , Σᵈ )                                                          (77)

Qφ  (yˆ|uk) = N(yˆ; µᵈ, Σᵈ)                                                            (78)

Qϕk (uk) = N(0, I).                                                                   (79)

where µᵉ , Σᵉ are the output of a DNN fθ   with input Xk that encodes the observations in a nu  
-dimensional

k        k                                                                   k                      
                                                                                                    
     k

Gaussian  distribution,  µᵈ , Σᵈ  are  the  outputs  of  a  DNN  fφ    with  inputs  U₁, . . . , 
UK,  sampled  from


P    (u  |x  ), and µᵈ, Σᵉ K         K

f     with inpuKt

, k = 1, . . . , K.

With the above choice of parametric encoders and decoders, and using a single sample m = 1, the 
empirical
DIB cost in equation 29 is given for the sample (x1,i, . . . , xK,i, yi) by


Lemp(θ, φ, ϕ) := −  1 .(y   − µ

)T Σᵈ,−¹(y   − µ

) + log det(Σᵈ   )Σ


s,i

2        i

Σ 1 .

12,i

12,i         i

12,i

12,i

Σ


2

k=1

Σ 1 .

 

k,i

k,i

k,i

k,i

Σ

	  

ny (1 + sK) log(2π),
2


where (µᵈ

d
12,i

) denote the output of the DNN fφK

for the i-th sample (x1,i, . . . , xK,i, yi), and similarly

for  the  other  mean  and  covariance  terms;  and  where  we  have  used  that  each  term  in  
the  empirical  DIB
cost equation 29 can be computed noting that for d-dimensional Gaussian pmfs N(y; µ, Σ) we have

log N(y; µ, Σ) = − 1 .(y − µ)T Σ−¹(y − µ) + d log(2π) + log det(Σ)Σ ,

15


Under review as a conference paper at ICLR 2020

and the KL divergence between two multivariate Gaussian pmfs P1            (µ1, Σ1) and P2          
  (µ2, Σ2) in

Rᵈ, is


D     (P  ǁP  ) = 1 .(µ

− µ  )T Σ−¹(µ

− µ  ) + log |Σ  Σ−¹| − d + tr{Σ−¹Σ  }Σ .           (80)

The multivariate Gaussian parametrization of the encoders, decoders and prior distribution as given 
by equa-
tion 76-equation 79 can be used for other data models that are not necessary Gaussian.  For 
example, it is
particularly suitable for regression problems in which Y  lies on a continuous space. Also, it is 
very often used in
conjunction with VAE generative problems Rezende et al. (2014); Kingma and Welling (2013).

6.2    REGRESSION FOR VECTOR GAUSSIAN DATA MODEL

Consider a distributed learning model with K = 2 encoders, each observing a noisy version of an ny 
-dimensional
Gaussian vector Y ∼ N(y; 0, I), as Xk = HkY + Nk, where Hk ∈ Rⁿk ×ny  and the noises are distributed
as    Nk ∼ N(0, I) for k = 1, 2.

For this model, the optimal accuracy-complexity region can be computed using Theorem 2. In what 
follows,
we evaluate the performance of our D-VIB of the previous section for regression.  The algorithm is 
trained

using a dataset of n i.i.d. samples {(X1,i, X2,i, Yi)}ⁿ    form the described vector Gaussian data 
model. We

train the DNNs for various values of the parameter s.  We use the multivariate Gaussian 
parameterization in
equation 76-equation 79 for the DNNs architecture shown in Table 6.2.  Specifically, Encoder k, k  
=  1, 2,

consists of three dense layers of 512 neurons each followed by rectified linear unit (ReLu) 
activations.  The

output of encoder k is processed by a dense layer without nonlinear activation to generate µᵉ and 
Σᵉ of size 512

k                 k

and 512     512, respectively. Each decoder consists of two dense layers of 512 neurons with ReLu 
activations.
The output of decoder 1, 2 and 12 is processed, each, by a fully connected layer without activation 
to generate


µᵈ and Σᵈ and µᵈ

and Σᵈ , of size 2 and 2 × 2.


0.6

0.5

0.4

0.3

C-IB with R

sum  → ∞

1.0

0.9

0.8

0.7

Min mmse

D-IB Theorem 2
Centralized IB

V-DIB train n=30000

V-DIB train n=30000
BA-DIB Algorithm 3


0.2

0.1

0.0

0

D-IB Theorem 2

C-IB Upper Bound

D-VIB train n =30000
D-VIB test n=30000
BA-DIB Algorithm 3

2                              4                              6                              8      
                       10                            12

Sum-Complexity  Rsum

0.6

0.5

0.4

0.3

0

2                              4                              6                              8      
                       10                            12

Sum-Complexity Rsum


Figure  5:   Accuracy  vs.   sum-complexity
tradeoffs for the Gaussian data model.  Pa-
rameters:  K  =  2 encoders, ny   =  1, n1  =
n2           = 3 and n = 30.000.

Figure  6:   Mean  square  error  vs.    sum-
complexity tradeoffs for the Gaussian data
model. Parameters: K = 2 encoders, ny  =
1, n1  = n2  = 3 and n = 30.000.

Figure 5 shows the optimal relevance-complexity region of tuples (∆, Rsum) obtained from Theorem 2 
for a
vector Gaussian model with K = 2 encoders, target variable dimension ny  = 1, and observations 
dimension
n1  = n2  = 3. A set of 40.000 samples split among training (30.000 samples) and test (10.000 
samples). The
figure depicts all accuracy-complexity pairs obtained by application of our algorithm D-VIB to this 
setting. The
results are compared to the case of inference with known joint distribution (referred to as D-IB, 
see next section)
as well as the case of centralized inference (C-IB). For the D-VIB algorithm, the the DNN 
architecture for the

coders is shown in Table 6.2. Figure 6 shows the evolution of the associated mean squared error 
(MSE) in the
estimation of the label Y  using our D-VIB algorithm. As it can bee seen from both figures the 
performance of
our D-VIB algorithm (which does not require knowledge of the joint label-feature distribution) is 
very close to
that predicted by the theory, i.e., our Theorem 2.

Figure 7 shows similar curves for ny  = 2, n1  = n2  = 3 dimensions, for various sizes of the 
training datset.
As expected large training sets allow a more accurate prediction. Noteworthy, that the performance 
during the
training phase might be better than that of the centralized learning scenario is an indicator can 
be caused by
overfitting. Related to this aspect, recall that although the D-VIB algorithm does not estimate the 
underlying
distribution explicitly, intuitively it does for the computation of the cost function. This is 
related to that universal
compressors also learn the actual distribution of the data that is being compressed. Recall that 
since the plug-in
estimator of entropy is biased downward, estimations of the mutual information terms that are 
involved in the
cost function are then biased upward, which is an alternate explanation to the observed overfitting 
during the
training phase.

16


Under review as a conference paper at ICLR 2020


1.4

1.3

1.2

1.1

1.0

0.9

0.8

C-IB with Rsum  → ∞

D-IB Theorem 2

D-VIB train n=5000
D-VIB test n=5000

D-VIB train n=10000
D-VIB test n=10000
D-VIB train n=50000
D-VIB test n=50000

DNN Layers
Encoder k         dense [512]-ReLu

dense [512]-ReLu
dense [512]-ReLu

Lat. space k     dense [256]-ReLu

Decoder 12       dense [256]-ReLu


0                 50               100              150              200              250           
   300              350

Sum-Complexity  Rsum

Figure 7: Effect of varying training set
size n  =    5.000, 10.000, 50.000   on

Gaussian inference for D-VIB.

Decoder k        dense [256]-ReLu

Table 3: Used DNN architecture.

7    DISTRIBUTED  BLAHUT-ARIMOTO  TYPE  ALGORITHMS

7.1    DISCRETE-ALPHABET SETTING

In this section, we derive an iterative method to optimize the variational DIB cost function in 
equation 23 when

the data model is discrete and the joint distribution PXK,Y   is either known, or a good estimation 
of it can be
obtained from the training samples. In these cases, the maximizing distributions P, Q of the 
variational DIB

cost in equation 23 can be efficiently found by an alternating optimization procedure over P and Q 
similar
to the expectation-maximization (EM) algorithm Dempster et al. (1977) and the standard 
Blahut-Arimoto
(BA) methodBlahut (1972). An extension to the vector Gaussian data model, which involves random 
variable
with continuous alphabets, is also provided. The main idea of the algorithm is that at iteration t, 
the optimal
distributions P⁽ᵗ⁾ that maximize the variational D-IB bound    VB(P, Q⁽ᵗ⁾) for fixed Q⁽ᵗ⁾ can be 
optimized in
closed form and, next, the maximizing pmfs Q⁽ᵗ⁾ for given P⁽ᵗ⁾ can be also found analytically.  So, 
starting
from an initialization P⁽⁰⁾ and Q⁽⁰⁾ the algorithms performs the following computations 
successively and in
this order, until convergence,

P⁽⁰⁾ → Q⁽⁰⁾ → P⁽¹⁾ → . . . → P⁽ᵗ⁾ → Q⁽ᵗ⁾ → . . .                                      (81)

We refer to such algorithm as “Blahut-Arimoto Distributed Information Bottleneck Algorithm 
(BA-DIB)”.
Algorithm 1 describes the steps taken by BA-DIB to successively maximize    VB(P, Q) by solving a 
concave
optimization problem over P and over Q at each iteration. We have the following lemma whose proof 
follows
essentially  by  using  the  log-sum  inequality  Cover  and  Thomas  (1991)  and  the  convexity  
of  the  mapping
x         ›→ x log x.

Lemma 4  The function LVB(P, Q) is concave in P and in Q.

For fixed P⁽ᵗ⁾, the optimal Q⁽ᵗ⁾ maximizing the variational D-IB bound in equation 19 follows from 
Lemma 1
as given by equation 21-equation 22. For fixed Q⁽ᵗ⁾, the optimal P⁽ᵗ⁾ can be found using the 
following lemma.


Lemma 5  For fixed Q, there exists a P that achieves the maximum maxP LVB(P, Q), where PU  |X

is given


s

by

∗                                             exp (−ψs(uk, xk))             

k      k

(82)


p  (uk|xk) = q(uk) Σ

,

q(u  ) exp(−ψ  (u  , x  ))


uk ∈Uk           k

for uk ∈ Uk and xk ∈ Xk, k ∈ K, and where we define

s      k       k


ψ  (u  , x  ) := D     (P

||Q

) + 1 E

[D     (P

||Q

))].          (83)

Proof:  Due to its concavity, to maximize    VB          with respect to      for given     , we 
add the Lagrange
multipliers λₓk   ≥  0 for each constraint      u              p(uk|xk)  =  1 with xk  ∈  Xk.  For 
each s, λₓk   ≥  0 and
p(uk|xk) can be explicitly found by solving the KKT conditions, e.g.,

        ∂         LVB(P, Q) +   Σ  λ       Σ  p(u  |x  ) − 1 = 0.


k      k

This completes the proof.

xk ∈Xk

uk ∈Uk

17


Under review as a conference paper at ICLR 2020

Algorithm 1 BA-DIB training algorithm for discrete data

1:  inputs:

discrete pmf PX1 ,...,Xk,Y , parameter s ≥ 0.

2:  output: optimal PU∗ |X  , pair (∆s, Rs).

k      k

3:  initialization

Set t = 0 and set P⁽⁰⁾ with p(uk|xk) =   ¹   for uk ∈ Uk, xk ∈ Xk, k = 1, . . . , K.

4:  repeat

5:  Compute Q⁽ᵗ⁺¹⁾ using equation 21 and  equation 22.

6:  Compute P⁽ᵗ⁺¹⁾ using equation 82.

7:  t     t + 1

8:  until    convergence.

7.1.1    CONVERGENCE

Algorithm 1 essentially falls into the class of the Successive Upper-Bound Minimization (SUM) 
algorithms
Razaviyayn et al. (2013) in which    VB(P, Q) acts as a globally tight lower bound on    s(P).  
Algorithm
1 provides a sequence P⁽ᵗ⁾ for each iteration t, which converges to a stationary point of the 
optimization
problem equation 23.

Proposition 4  Every limit point of the sequence P⁽ᵗ⁾ generated by Algorithm 1 converges to a 
stationary point
of equation 23.

Proof: Let Q∗(P) = arg maxQ LVB(P, Q). Using Lemma 1, for every P′  /= P, it holds that

LVB(P, Q∗(P′)) ≤ LVB(P, Q∗(P))

s                                                 s

= Ls(P).                                                                  (84)

Since    s(P) and    VB(P, Q∗(P′)) satisfy the assumptions of (Razaviyayn et al., 2013, Proposition 
1), then

VB(P, Q∗(P′)) satisfies A1-A4 in Razaviyayn et al. (2013). Convergence to a stationary point of 
equation 23

follows from (Razaviyayn et al., 2013, Theorem 1).

The self consistent equations equation 21, equation 22 and equation 83 satisfied by any stationary 
point of
the D-IB problem extend those of the standard point-to-point IB problem Globerson and Tishby (2004) 
to the
distributed IB problem with K ≥ 2 encoders. In particular, note the additional divergence term in 
equation 83.

7.2    GAUSSIAN SETTING

Recall Algorithm 1.  For finite alphabet sources the updating rules of Q⁽ᵗ⁺¹⁾ and P⁽ᵗ⁺¹⁾ in 
Algorithm 1 are
relatively easy, but they become unfeasible for continuous alphabet sources. We leverage on the 
optimality of
Gaussian test channels, shown in Theorem 2, to restrict the optimization of P to Gaussian 
distributions, which
are easily represented by a finite set of parameters, namely mean and covariance.  We show that if 
P⁽ᵗ⁾ are
Gaussian distributions, then P⁽ᵗ⁺¹⁾ are also Gaussian distributions, which can be computed with an 
efficient

update algorithm of its representing parameters. In particular, if at time t the k-th distributions 
P ⁽ᵗ⁾      is given

by                                                                                                  
                                                  Uk |Xk

Uᵗ  = Aᵗ Xk + Zᵗ ,                                                                 (85)

k              k                     k


where Zk  ∼ CN(0, Σzt ), we show that at t + 1, for P

(t+1)

updated as in equation 82, the encoder P ⁽ᵗ⁺¹⁾

Uk |Xk

corresponds to Uᵗ⁺¹ = Aᵗ⁺¹Xk + Zᵗ⁺¹, where Zᵗ⁺¹ ∼ CN(0, Σ  t₊₁ ) and Σ  t₊₁ , Aᵗ⁺¹ are updated as


k                   k                          k

∫∫      1 ,              1

			

k

,−1

 

zk                         zk              k


At+1  = Σ

∫∫1 + 1 , Σ−1

	

Aᵗ (I − Σ

Σ−1) −  1 Σ−1

Aᵗ (I − Σ

t       Σ−¹), .    (87)

The detailed update procedure is given in Algorithm 2 (see the following section for the details of 
the derivations).

18


Under review as a conference paper at ICLR 2020

Algorithm 2 BA-DIB algorithm for the Gaussin Vector D-IB

1:  inputs:

covariance Σy,ₓ1 ,...,xk , parameter s    0.

2:  output: optimal pairs (Ak, Σz∗ ), k = 1, . . . , K.

3:  initialization

Set randomly A⁰ and Σz0  ≤ 0, k ∈ K.

k               k

4:  repeat


5:  Compute Σx    ut

K\k

and update for k ∈ K

Σut |y  = At Σx  |yAt,† + Σzt

(88)


Σuᵗ |uᵗ

t,†

= A  Σx  |ut       A     + Σzt ,                                (89)


k     K\k

k       k     K\k      k              k

6:  Compute Σzt+1  as in equation 86 for k ∈ K.

7:  Compute Aᵗ⁺¹ as equation 87, k ∈ K.

8:  t     t + 1.

9:  until    convergence.

7.2.1    DERIVATION OF ALGORITHM 2

We derive the update rules of Algorithm 2 and show that the Gaussian distribution is invariant to 
the update rules
in Algorithm 1, in line with Theorem 2. First, we recall that if (X1, X2) are jointly Gaussian, 
then

PX2 |X1 =x1  = CN(µₓ2 |x1 , Σₓ2 |x1 ),                                                   (90)


where µₓ  |ₓ

:= Kₓ  |ₓ  x₁, with Kₓ  |ₓ

:= Σx   ,x   Σ−x 1 .

Then, for Q⁽ᵗ⁺¹⁾ computed as in equation 21 and equation 22 from P⁽ᵗ⁾, which is a set of Gaussian 
distributions,
we have

QY|uk   = CN(µy|ut , Σy|ut ),

k                k

(t+1)

QY|uK  = CN(µy|ut  , Σy|ut  ).

K           K

Next, we look at the update P⁽ᵗ⁺¹⁾ as in equation 82 from given Q⁽ᵗ⁺¹⁾.  First, we have that p(uᵗ ) 
is the

t                                t                                                                  
                  t                  t,H

marginal of Uk, given by Uk ∼ CN(0, Σut ) where Σut   = AkΣxk Ak     + Σzt .

k                             k                                                         k

Then, to compute ψs(uᵗ , xk), first, we note that

EUK\k |xk [DKL(PY |UK\k ,xk ||QY |UK\k ,uk )] = DKL(PY,UK\k |xk ||QY,UK\k |uk )−DKL(PUK\k |xk 
||QUK\k |uk )

(91)

and that for two generic multivariate Gaussian distributions P1              (µ1, Σ1) and P2        
      (µ2, Σ2) in CN ,
the KL divergence is computed as in equation 80 below.

Applying  equation  91  and  equation  80  in  equation  83  and  noting  that  all  involved  
distributions  are
Gaussian,  it  follows  that  ψs(uᵗ , xk)  is  a  quadratic  form.    Then,  since  p(uᵗ )  is  
Gaussian,  the  product

k                                                                                                   
              k

log(p(uᵗ ) exp(−ψs(uᵗ , xk))) is also a quadratic form, and identifying constant, first and second 
order terms,


k

we can write

k

log p⁽ᵗ⁺¹⁾(uk|xk) = Z(xk) + (uk − µ  t₊₁

)H Σ−1

k

(uk  − µut+1|xk

),                    (92)

where Z(xk) is a normalization term independent of uk,


Σ−1

= Σ−¹ + KH  t Σ−¹  K     t


t+1

k

t               y|uk

+ 1 KH

y|ut

y|uk

Σ−1

1    H

K    t            t   −     K

	

Σ−1

K  t            t ,        (93)


and

µ  t+1

= Σ  t+1 .KH

yut

K\k

t Σ−¹  µ

|ut

yut

K\k

|ut

yuK\k |uk

t

s      K\k

|ut

ut

K\k

|ut

uK\k |uk

19


Under review as a conference paper at ICLR 2020


1          t            t Σ−¹

1

µ      t                −     K  t

t Σ−¹       µ  t               , .


+ s Ky,uK\k |uk

y,ut

K\k

|ut

y,uK\k |xk

s    uK\k |uk

ut

K\k

|ut

uK\k |xk

(94)

This shows that p⁽ᵗ⁺¹⁾(uk|xk) is a multivariate Gaussian distribution and that Uᵗ⁺¹|{Xk  =  xk} is 
also a

multivariate Gaussian distributed as CN(µut+1|xk , Σzt+1 ).

Next, we simplify equation 93 and equation 94 to obtain the update rules equation 86 and equation 
87. From the
matrix inversion lemma, similarly to Chechik et al. (Feb. 2005), for (X1, X2) jointly Gaussian we 
have


Σ−1      = Σ−x 1 + KH  |

Σ−¹   Kₓ  |ₓ  .                                               (95)


x2 |x1                    2

Applying equation 95, in equation 93 we have

x1  x2

x1 |x2        1    2


−1

zt+1

= Σ−1

ut |y

+ 1 Σ−1

ut |yut

1    −1

ut |ut

,                                          (96)


k                      k              s

k        K\k

s      k     K\k


= ∫1 + 1 , Σ−¹

1    −1

ut |ut

,                                                    (97)

s          k              s      k     K\k

where equation 97 is due to the Markov chain Uk −− Y −− UK\k.

Then, also from the matrix inversion lemma, we have for jointly Gaussian (X1, X2),


Σ−1     Σx   ,x   Σ−x 1  = Σ−x 1Σx   ,x   Σ−1

.                                              (98)


x2 |x1       2    1          1

2        2    1

x1 |x2

Applying equation 98 to equation 94, for the first term in equation 94, we have


KH  t Σ−¹  µ

= Σ−¹  Σ

t Σ−¹µ

(99)


y|uk      y|ut

y|xk

ut |y

y,uk      y

y|xk


= Σ−1

At Σx   ,yΣ−1Σy,x   Σ−1xk


ut |y      k        k          y

k      xk


= Σ−1

ut |y

t

Aᵗ (I − Σₓ

|yΣ−ₓ ¹)xk,                                       (100)

where Σy,ut   = Ak Σₓk ,y; and equation 100 is due to the definition of Σₓk |y.
Similarly, for the second term in equation 94, we have


K    t            t Σ−¹

µ      t                = Σ−¹         Aᵗ (I − Σ

t       Σ−¹)xk,                (101)


yuK\k |uk      yut

|ut

y,uK\k |xk              ut |yut             k

xk |yuK\k      xk


K\k     k

t

k

= Σ−1

ut |y

K\k

Aᵗ (I − Σₓ

|yΣ−ₓ ¹)xk,                               (102)


where we use Σut ,yut

= Ak Σxk ,yut

; and equation 102 is due to the Markov chain Uk −b− Y −b− UK\k.


K  t            t Σ−¹       µ  t

= Σ−1

Aᵗ (I − Σ

t       Σ−¹)xk.                     (103)


uK\k |uk      ut       |ut      uK\k |xk              ut |ut             k

xk |uK\k      xk

K\k     k                                         k     K\k

Equation equation 87 follows by noting that µ  t₊₁     =  Aᵗ⁺¹xk, and that from equation 94 Aᵗ⁺¹ 
can be


identified as in equation 87.

uk     |xk               k                                                                          
             k


Finally, we note that due to equation 85, Σut |y  and Σut |ut

are given as in equation 88 and equation 89,

k                          k     K\k


where Σₓk |y  = Σk and Σxk |ut

can be computed from its definition. This completes the proof.

20


Under review as a conference paper at ICLR 2020

REFERENCES

A. Alemi, I. Fischer, J. Dillon, and K. Murphy. Deep variational information bottleneck. In ICLR, 
2017. URL

https://arxiv.org/abs/1612.00410.

R. Blahut. Computation of channel capacity and rate-distortion functions. IEEE Transactions on 
Information
Theory, 18(4):460–473, Jul 1972. ISSN 0018-9448.

A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of 
the eleventh
annual conference on Computational learning theory, pages 92–100. ACM, 1998.

O. Bousquet and A. Elisseeff.  Stability and generalization.  Journal of Machine Learning Research, 
2(Mar):
499–526, 2002.

G. Chechik, A. Globerson, N. Tishby, and Y. Weiss. Information bottleneck for Gaussian variables. 
Journal of
Machine Learning Research, 6:165–188, Feb. 2005.

T. A. Courtade and T. Weissman. Multiterminal source coding under logarithmic loss. IEEE 
Transactions on
Information Theory, 60(1):740–761, Jan. 2014. ISSN 0018-9448. doi: 10.1109/TIT.2013.2288257.

T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley-Interscience, 1991.

A. Dembo, T. M. Cover, and J. A. Thomas. Information theoretic inequalities. IEEE Trans. on Inf. 
Theory, 37
(6):1501–1518, Nov 1991. ISSN 0018-9448. doi: 10.1109/18.104312.

A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em 
algorithm.

Journal of the Royal Statistical Society, Series B, 39(1):1–38, 1977.

P. Dhillon, D. P. Foster, and L. H. Ungar. Multi-view learning of word embeddings via CCA. In 
Advances in
neural information processing systems, pages 199–207, 2011.

R. Dobrushin and B. Tsybakov. Information transmission with additional noise. IRE Transactions on 
Information
Theory, 8(5):293–304, Sep. 1962. ISSN 0096-1000. doi: 10.1109/TIT.1962.1057738.

E. Ekrem and S. Ulukus.   An outer bound for the vector Gaussian CEO problem.   IEEE Transactions on
Information Theory, 60(11), Nov. 2014.

A. El Gamal and Y.-H. Kim. Network information theory. Cambridge University Press, 2011.

A. Globerson and N. Tishby. On the optimality of the Gaussian information bottleneck curve. Hebrew 
University
Tech. Report, 2004.

M. Go¨ nen and E. Alpaydın. Multiple kernel learning algorithms. Journal of Machine Learning 
Research, 12
(Jul):2211–2268, 2011.

D.-R. Hardoon, S. Szedmak, and J. Shawe-Taylor. Canonical correlation analysis: an overview with 
application
to learning methods. Neur. Comput., 16:2639–2664, 2004.

P. Harremoes and N. Tishby. The information bottleneck revisited or how to choose a good distortion 
measure.
In Proc. IEEE Int. Symp. Information Theory, pages 566–570, Jun. 2007.

I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner.  
β-vae:
Learning basic visual concepts with a constrained variational framework. 2016.

G. E. Hinton and D. van Camp. Keeping the neural networks simple by minimizing the description 
length of
the weights. In Proceedings of the Sixth Annual Conference on Computational Learning Theory, COLT 
’93,
pages 5–13, New York, NY, USA, 1993. ACM. ISBN 0-89791-611-5. doi: 10.1145/168304.168306. URL
http://doi.acm.org/10.1145/168304.168306.

Y. Jia, M. Salzmann, and T. Darrell. Factorized latent spaces with structured sparsity. In Advances 
in Neural
Information Processing Systems, pages 982–990, 2010.

J. Jiao, T. A. Courtade, K. Venkat, and T. Weissman.  Justification of logarithmic loss via the 
benefit of side
information. IEEE Transactions on Information Theory, 61(10):5357–5365, 2015.

21


Under review as a conference paper at ICLR 2020

D. P. Kingma and M. Welling.  Auto-encoding variational bayes.  CoRR, abs/1312.6114, 2013.  URL 
http:

//dblp.uni-trier.de/db/journals/corr/corr1312.html#KingmaW13.

D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. ArXiv e-prints, Dec. 2013.

A. Kumar and H. Daume´. A co-training approach for multi-view spectral clustering. In Proceedings 
of the 28th
International Conference on Machine Learning (ICML-11), pages 393–400, 2011.

C.-T. Li and A. El Gamal. Strong functional representation lemma and applications to coding 
theorems. IEEE
Transactions on Information Theory, 64(11):6967–6978, 2018.

C.-T.   Li,   X.   Wu,   A.   Ozgur,   and   A.   El   Gamal.      Minimax   learning   for   
remote   prediction.      In

https://arxiv.org/abs/1806.00071, May. 2018.

K. Nigam and R. Ghani.   Analyzing the effectiveness and applicability of co-training.   In Proc. 
of the 9th
International Conference of Information and Knowledge Management, pages 86–93, 2000.

A. Painsky and G. W. Wornell. On the universality of the logistic loss function. arXiv preprint 
arXiv:1805.03804,
2018.

M. Razaviyayn, M. Hong, and Z.-Q. Luo.  A unified convergence analysis of block successive 
minimization
methods for nonsmooth optimization. SIAM J. on Opt., 23:1126–1153, Jun. 2013.

D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in 
deep
generative models.  In E. P. Xing and T. Jebara, editors, Proceedings of the 31st International 
Conference
on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 1278–1286, Bejing,
China, 22–24 Jun 2014. PMLR.

D. Russo and J. Zou.  How much does your data exploration overfit?  controlling bias via 
information usage.

arXiv preprint arXiv:1511.05219, 2015.

S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan.  Learnability, stability and uniform 
convergence.

Journal of Machine Learning Research, 11(Oct):2635–2670, 2010.

N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. In Proc. 37th Annual 
Allerton Conf.
on Comm., Control, and Computing, pages 368–377, 1999.

V. Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.

A. Xu and M. Raginsky. Information-theoretic analysis of generalization capability of learning 
algorithms. In

Advances in Neural Information Processing Systems, pages 2521–2530, 2017.

C. Xu, D. Tao, and C. Xu. A survey on multi-view learning. arXiv preprint arXiv:1304.5634, 2013.

J. Zhao,  X. Xie,  X. Xu,  and S. Sun.   Multi-view learning overview:  recent progress and new 
challenges.

Information Fusion, 38:43–54, 2017.

22

