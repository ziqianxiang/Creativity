Natural- to formal-language generation
using Tensor Product Representations
Anonymous authors
Paper under double-blind review
Ab stract
Generating formal-language represented by relational tuples, such as Lisp pro-
grams or mathematical operations, from natural-language input is a challenging
task because it requires explicitly capturing discrete symbolic structural infor-
mation implicit in the input. Most state-of-the-art neural sequence models do
not explicitly capture such structural information, limiting their performance on
these tasks. In this paper we propose a new encoder-decoder model based on
Tensor Product Representations (TPRs) for Natural- to Formal-language genera-
tion, called TP-N2F. The encoder of TP-N2F employs TPR ‘binding’ to encode
natural-language symbolic structure in vector space and the decoder uses TPR
‘unbinding’ to generate, in symbolic space, a sequence of relational tuples, each
consisting of a relation (or operation) and a number of arguments. On two bench-
marks, TP-N2F considerably outperforms LSTM-based seq2seq models, creating
new state-of-the-art results: the MathQA dataset for math problem solving, and
the AlgoLisp dataset for program synthesis. Ablation studies show that improve-
ments can be attributed to the use of TPRs in both the encoder and decoder to
explicitly capture relational structure to support reasoning.
1 INTRODUCTION
When people perform explicit reasoning, they can typically describe the way to the conclusion
step by step via relational descriptions. There is ample evidence that relational representations are
important for human cognition (e.g., (Goldin-Meadow & Gentner, 2003; Forbus et al., 2017; Crouse
et al., 2018; Chen & Forbus, 2018; Chen et al., 2019)). Although a rapidly growing number of
researchers use deep learning to solve complex symbolic reasoning and language tasks (a recent
review is (Gao et al., 2019)), most existing deep learning models, including sequence models such
as LSTMs, do not explicitly capture human-like relational structure information.
In this paper we propose a novel neural architecture, TP-N2F, to solve natural- to formal-language
generation tasks (N2F). In the tasks we study, math or programming problems are stated in natural-
language, and answers are given as programs, sequences of relational representations, to solve the
problem. TP-N2F encodes the natural-language symbolic structure of the problem in an input vector
space, maps this to a vector in an intermediate space, and uses that vector to produce a sequence
of output vectors that are decoded as relational structures. Both input and output structures are
modelled as Tensor Product Representations (TPRs) (Smolensky, 1990). During encoding, NL-input
symbolic structures are encoded as vector space embeddings using TPR ‘binding’ (following Palangi
et al. (2018)); during decoding, symbolic constituents are extracted from structure-embedding output
vectors using TPR ‘unbinding’ (following Huang et al. (2018; 2019)).
Our contributions in this work are as follows. (i) We propose a role-level analysis of N2F tasks. (ii)
We present a new TP-N2F model which gives a neural-network-level implementation of a model
solving the N2F task under the role-level description proposed in (i). To our knowledge, this is the
first model to be proposed which combines both the binding and unbinding operations of TPRs to
achieve generation tasks through deep learning. (iii) State-of-the-art performance on two recently
developed N2F tasks shows that the TP-N2F model has significant structure learning ability on tasks
requiring symbolic reasoning through program synthesis.
1
2	Background: Review of Tensor-Product Representation
The TPR mechanism is a method to create a vector space embedding of complex symbolic structures.
The type of a symbol structure is defined by a set of structural positions or roles, such as the left-
child-of-root position in a tree, or the second-argument-of-R position of a given relation R. In a
particular instance of a structural type, each of these roles may be occupied by a particular filler,
which can be an atomic symbol or a substructure (e.g., the entire left sub-tree of a binary tree can
serve as the filler of the role left-child-of-root). For now, we assume the fillers tobe atomic symbols.1
The TPR embedding of a symbol structure is the sum of the embeddings of all its constituents, each
constituent comprising a role together with its filler. The embedding of a constituent is constructed
from the embedding of a role and the embedding of the filler of that role: these are joined together
by the TPR 'binding' operation, the tensor (or generalized outer) product @
Formally, suppose a symbolic type is defined by the roles {ri }, and suppose that in a particular
instance of that type, S, role ri is bound by filler fi . The TPR embedding of S is the order-2 tensor
T = X fi ㊈ ri = X fir>	(1)
ii
where {fi } are vector embeddings of the fillers and {ri } are vector embeddings of the roles. In
Eq. 1, and below, for notational simplicity we conflate order-2 tensors and matrices.
As a simple example, consider the symbolic type string, and choose roles to be ri = first_eIement,
r = SeCOnd.element, etc. Then in the specific string S = cba, the first role ri is filled by c, and
r and r3 by b and a, respectively. The TPR for S is C 0 ri + b 0 r2 + a 0 r3, where a, b, C are
the vector embeddings of the symbols a, b, c, and ri is the vector embedding of role ri.
A TPR scheme for embedding a set of symbol structures is defined by a decomposition of those
structures into roles bound to fillers, an embedding of each role as a role vector, and an embedding of
each filler as a filler vector. Let the total number of roles and fillers available be nR, nF, respectively.
Define the matrix of all possible role vectors to be R ∈ RdR×nR, with column i, [R]:i = ri ∈ RdR,
comprising the embedding of ri. Similarly let F ∈ RdF×nF be the matrix of all possible filler
vectors. The TPR T ∈ RdF×dR. Below, dR, nR, dF, nF will be hyper-parameters, while R, F will
be learned parameter matrices.
Using summation in Eq.1 to combine the vectors embedding the constituents of a structure risks
non-recoverability of those constituents given the embedding T of the the structure as a whole. The
tensor product is chosen as the binding operation in order to enable recovery of the filler of any role
in a structure S given its TPR T. This can be done with perfect precision if the embeddings of the
roles are linearly independent. In that case the role matrix R has a left inverse U: UR = I. Now
define the unbinding (or dual) vector for role rj, uj, to be the jth column of U>: U:>j . Then,
since [I]ji = [UR]ji = Uj:R:i = [U:>j ]>R:i = uj>ri = ri>uj, we have ri>uj = δji. This means
that, to recover the filler of rj in the structure with TPR T, we can take its tensor inner product (or
matrix-vector product) with uj :2
Tuj = Xfiri> uj = X fiδij =fj	(2)
ii
In the architecture proposed here, we will make use of both TPR binding using the tensor product
with role vectors ri and TPR unbinding using the tensor inner product with unbinding vectors uj .
Binding will be used to produce the order-2 tensor TS embedding of the NL problem statement.
Unbinding will be used to generate output relational tuples from an order-3 tensor H. Because they
pertain to different representations (of different orders in fact), the binding and unbinding vectors
we will use are not related to one another.
1 When fillers are structures themselves, binding can be used recursively, giving tensors of order higher than
2. In general, binding is done with the tensor product, since conflation with matrix algebra is only possible for
order-2 tensors. Our unbinding of relational tuples involves the order-3 TPRs defined in Sec. 3.1.2.
2When the role vectors are not linearly independent, this operation performs unbinding approximately,
taking U to be the left pseudo-inverse of R. Because randomly chosen vectors on the unit sphere in a high-
dimensional space are approximately orthogonal, the approximation is often excellent (Anonymous, in prep.).
2
3 TP-N2F MODEL
We propose a general TP-N2F neural network architecture operating over TPRs to solve N2F tasks
under a proposed role-level description of those tasks. In this description, natural-language input is
represented as a straightforward order-2 role structure, and formal-language relational representa-
tions of outputs are represented with a new order-3 recursive role structure proposed here. Figure 1
shows an overview diagram of the TP-N2F model. It depicts the following high-level description.
r--► Attentio
Tuple Vector，_______
re I0 argl0 arg201
∙∙L∙a Attention2 一
Tuple Vector
rer, argl
Figure 1: Overview diagram of TP-N2F.
As shown in Figure 1, while the natural-language input is a sequence of words, the output is a
sequence of multi-argument relational tuples such as (R A1 A2), a 3-tuple consisting of a binary re-
lation (or operation) R with its two arguments. The “TP-N2F encoder” uses two LSTMs to produce
a pair consisting of a filler vector and a role vector, which are bound together with the tensor prod-
uct. These tensor products, concatenated, comprise the “context” over which attention will operate
in the decoder. The sum of the word-level TPRs, flattened to a vector, is treated as a representation
of the entire problem statement; it is fed to the “Reasoning MLP”, which transforms this encoding
of the problem into a vector encoding the solution. This is the initial state of the “TP-N2F decoder”
attentional LSTM, which outputs at each time step an order-3 tensor representing a relational tuple.
To generate a correct tuple from decoder operations, the model must learn to give the order-3 ten-
sor the form of a TPR for a (R A1 A2) tuple (detailed explanation in Sec. 3.1.2). In the following
sections, we first introduce the details of our proposed role-level description for N2F tasks, and then
present how our proposed TP-N2F model uses TPR binding and unbinding operations to create a
neural network implementation of this description of N2F tasks.
3.1	Role-level description of N2F tasks
In this section, we propose a role-level description of N2F tasks, which specifies the filler/role struc-
tures of the input natural-language symbolic expressions and the output relational representations.
3.1.1	Role-level description for natural-language input
Instead of encoding each token of a sentence with a non-compositional embedding vector looked up
in a learned dictionary, we use a learned role-filler decomposition to compose a tensor representation
for each token. Given a sentence S with n word tokens {w0, w1, ..., wn-1}, each word token wt is
assigned a learned role vector rt , soft-selected from the learned dictionary R, and a learned filler
vector ft, soft-selected from the learned dictionary F (Sec. 2). The mechanism closely follows that
of Palangi et al. (2018), and we hypothesize similar results: the role and filler approximately encode
the grammatical role of the token and its lexical semantics, respectively.3 Then each word token wt
is represented by the tensor product of the role vector and the filler vector: Tt = f10 rt. In addition
3Although the TPR formalism treats fillers and roles symmetrically, in use, hyperparameters are selected so
that the number of available fillers is greater than that of roles. Thus, on average, each role is assigned to more
words, encouraging it to take on a more general function, such as a grammatical role.
3
to the set of all its token embeddings {T0, . . . , Tn-1}, the sentence S as a whole is assigned a TPR
equal to the sum of the TPR embeddings of all its word tokens: TS = Ptn=-01 Tt .
Using TPRs to encode natural language has several advantages. First, natural language TPRs can
be interpreted by exploring the distribution of tokens grouped by the role and filler vectors they are
assigned by a trained model (as in Palangi et al. (2018)). Second, TPRs avoid the Bag of Word
(BoW) confusion (Huang et al., 2018): the BoW encoding of Jay saw Kay is the same as the BoW
encoding of Kay saw Jay but the encodings are different with TPR embedding, because the role
filled by a symbol changes with its context.
3.1.2	Role-level description for relational representations
In this section, we propose a novel recursive role-level description for representing symbolic rela-
tional tuples. Each relational tuple contains a relation token and multiple argument tokens. Given
a binary relation R, a relational tuple can be written as (rel arg1 arg2) where arg1, arg2 indicate
two arguments of relation rel. Let us adopt the two positional roles, pirel = argi -of-rel for i = 1, 2.
The filler of role pirel is argi . Now let us use role decomposition recursively, noting that the role
Prel can itself be decomposed into a sub-role Pi = argi-of-_ which has a sub-filler rel. Suppose
that argi, rel,Pi are embedded as vectors a%, r, p%. Then the TPR encoding of Prel is r 0 pi, so the
TPR encoding of filler argi bound to role PreliS a% 0 (r 0 Pi). The tensor product is associative, so
we can omit parentheses and write the TPR for the formal-language expression, the relational tuple
(rel arg1 arg2), as:
H = a1 0 r 0 p1 + a2 0 r 0 p2 .	(3)
Given the unbinding vectors p0i for positional role vectors pi and the unbinding vector r0 for the
vector r that embeds relation rel, each argument can be unbound in two steps as shown in Eqs. 4-5.
H ∙ Pi = [aι 0 r 0 pi + a2 0 r 0 p?] ∙ Pi = ai 0 r	(4)
[ai 0 r] ∙ r0 = ai	(5)
Here ∙ denotes the tensor inner product, which for the order-3 H and order-1 Pi in Eq. 4 can be
defined as [H ∙ pi]jk = Pl[H]jkl [pi]l; in Eq. 5, ∙ is equivalent to the matrix-vector product.
Our proposed scheme can be contrasted with the TPR scheme in which (rel arg1 arg2) is embedded
as r 0 a1 0 a2 (e.g., Smolensky et al. (2016); Schlag & Schmidhuber (2018)). In that scheme, an
n-ary-relation tuple is embedded as an order-(n + 1) tensor, and unbinding an argument requires
knowing all the other arguments (to use their unbinding vectors). In the scheme proposed here, an
n-ary-relation tuple is still embedded as an order-3 tensor: there are just n terms in the sum in Eq. 3,
using n position vectors p1, . . . , pn; unbinding simply requires knowing the unbinding vectors for
these fixed position vectors.
In the model, the order-3 tensor H of Eq. 3 has a different status than the order-2 tensor TS of
Sec. 3.1.1. TS is a TPR by construction, whereas H is a TPR as a result of successful learning.
To generate the output relational tuples, the decoder assumes each tuple has the form of Eq. 3, and
performs the unbinding operations which that structure calls for. In Appendix Sec. A.3, it is shown
that, if unbinding each ofa set of roles from some unknown tensor T gives a target set of fillers, then
T must equal the TPR generated by those role/filler pairs, plus some tensor that is irrelevant because
unbinding from it produces the zero vector. In other words, if the decoder succeeds in producing
filler vectors that correspond to output relational tuples that match the target, then, as far as what the
decoder can see, the tensor that it operates on is the TPR of Eq. 3.
3.1.3	The TP-N2F Scheme for Learning the input-output mapping
To generate formal relational tuples from natural-language descriptions, a learning strategy for the
mapping between the two structures is particularly important. As shown in (6), we formalize the
learning scheme as learning a mapping function f mapping (∙), which, given a structural representation
of the natural-language input, TS, outputs a tensor TF from which the structural representation of
the output can be generated. At the role level of description, there’s nothing more to be said about
this mapping; how it is modeled at the neural network level is discussed in Sec. 3.2.1.
TF = fmapping (TS)	(6)
4
3.2	The TP-N2F Model for Natural- to Formal-Language Generation
As shown in Figure 1, the TP-N2F model is implemented with three steps: encoding, mapping, and
decoding. The encoding step is implemented by the TP-N2F natural-language encoder (TP-N2F
Encoder), which takes the sequence of word tokens as inputs, and encodes them via TPR binding
according to the TP-N2F role scheme for natural-language input given in Sec. 3.1.1. The mapping
step is implemented by an MLP called the Reasoning Module, which takes the encoding produced
by the TP-N2F Encoder as input. It learns to map the natural-language-structure encoding of the
input to a representation that will be processed under the assumption that it follows the role scheme
for output relational-tuples specified in Sec. 3.1.2: the model needs to learn to produce TPRs such
that this processing generates correct output programs. The decoding step is implemented by the
TP-N2F relational tuples decoder (TP-N2F Decoder), which takes the output from the Reasoning
Module (Sec. 3.1.3) and decodes the target sequence of relational tuples via TPR unbinding. The
TP-N2F Decoder utilizes an attention mechanism over the individual-word TPRs Tt produced by
the TP-N2F Encoder. The detailed implementations are introduced below.
3.2.1	The TP-N2F natural-language Encoder
The TP-N2F encoder follows the role scheme in Sec. 3.1.1 to encode each word token wt by soft-
selecting one of nF fillers and one of nR roles. The fillers and roles are embedded as vectors. These
embedding vectors, and the functions for selecting fillers and roles, are learned by two LSTMs,
the Filler-LSTM and the Role-LSTM. (See Figure 2.) At each time-step t, the Filler-LSTM and
the Role-LSTM take a learned word-token embedding wt as input. The hidden state of the Filler-
LSTM, htF, is used to compute softmax scores ukF over nF filler slots, and a filler vector ft = FuF
is computed from the softmax scores (recall from Sec. 2 that F is the learned matrix of filler vectors).
Similarly, a role vector is computed from the hidden state of the Role-LSTM, htR . fF and fR denote
the functions that generate ft and rt from the hidden states of the two LSTMs. The token wt is
encoded as Tt, the tensor product of ft and rt. Tt replaces the hidden vector in each LSTM and is
passed to the next time step, together with the LSTM cell-state vector Ct: see (7)-(8). After encoding
the whole sequence, the TP-N2F encoder outputs the sum of all tensor products Pt Tt to the next
module. We use an MLP, called the Reasoning MLP, for TPR mapping; it takes an order-2 TPR from
the encoder and maps it to the initial state of the decoder. Detailed equations and implementation
are provided in Sec. A.2.1 of the Appendix.
htF = fFiller-LSTM(wt, Tt-1, ctF-1)	htR = fRole-LSTM(wt, Tt-1, ctR-1)	(7)
Tt = ft ③ rt = fF(hF)③ fR(hR)	(8)
Figure 2: Implementation of the TP-N2F encoder.
Y
TP-N2F
Mapping
Model
=φo -φpoouφLLZN—dl
COnteXf
vector
5
3.2.2	The TP-N2F Relational-Tuple Decoder
The TP-N2F Decoder is an RNN that takes the output from the reasoning MLP as its initial hidden
state for generating a sequence of relational tuples (Figure 3). This decoder contains an attentional
LSTM called the Tuple-LSTM which feeds an unbinding module: attention operates on the context
vector of the encoder, consisting of all individual encoder outputs {Tt }. The hidden-state H of the
Tuple-LSTM is treated as a TPR of a relational tuple and is unbound to a relation and arguments.
During training, the Tuple-LSTM needs to learn a way to make H suitably approximate a TPR.
At each time step t, the hidden state Ht of the Tuple-LSTM with attention (The version in Luong
et al. (2015)) (9) is fed as input to the unbinding module, which regards Ht as if it were the TPR
of a relational tuple with m arguments possessing the role structure described in Sec. 3.1.2: Ht ≈
Pm=I at 0 rt 0 Pi. (In Figure 3, the assumed hypothetical form of Ht, as Wen as that of Bt below,
is shown in a bubble with dashed border.) To decode a binary relational tuple, the unbinding module
decodes it from Ht using the two steps of TPR unbinding given in (4)-(5). The positional unbinding
vectors p0i are learned during training and shared across all time steps. After the first unbinding step
(4), i.e., the inner product of Ht with p0i, we get tensors Bit (10). These are treated as the TPRs of
two arguments ait bound to a relation rt . A relational unbinding vector r0t is computed by a linear
function from the sum of the Bit and used to compute the inner product with each Bit to yield ait,
which are treated as the embedding of argument vectors (11). Based on the TPR theory, r0t is passed
to a linear function to get r t as the embedding of a relation vector. Finally, the softmax probability
distribution over symbolic outputs is computed for relations and arguments separately. In generation,
the most probable symbol is selected. (More detailed equations are in Appendix Sec. A.2.3)
Ht = Atten(fTuple-LSTM(relt,arg1t,arg2t, Ht-1,ct-1), [T0, ...,Tn-1])	(9)
Bl = Ht ∙ pl	B2 = Ht ∙ P	(10)
r0t =	flinear(B1 +	B；)	a：	=	B： ∙	r0t	a = B； ∙	r0t	(11)
Tuple VectortlA___
® Tensor inner product
® Tensor product
φ Sum
Figure 3: Implementation of the TP-N2F decoder.
TP-N2F
Mapping
3.3	Inference and The Learning Strategy of the TP-N2F Model
During inference time, natural language questions are encoded via the encoder and the Reasoning
MLP maps the output of the encoder to the input of the decoder. We use greedy decoding (selecting
the most likely class) to decode one relation and its arguments. The relation and argument vectors
are concatenated to construct a new vector as the input for the Tuple-LSTM in the next step.
TP-N2F is trained using back-propagation (Rumelhart et al., 1986) with the Adam optimizer
(Kingma & Ba, 2017) and teacher-forcing. At each time step, the ground-truth relational tuple
is provided as the input for the next time step. As the TP-N2F decoder decodes a relational tuple at
each time step, the relation token is selected only from the relation vocabulary and the argument to-
kens from the argument vocabulary. For an input I that generates N output relational tuples, the loss
6
is the sum of the cross entropy loss L between the true labels L and predicted tokens for relations
and arguments as shown in (12).
N-1	N-1 2
LI = X L(reli, Lreli) + X X L(argji, Largji)	(12)
i=0	i=0 j=1
4	EXPERIMENTS
The proposed TP-N2F model is evaluated on two N2F tasks, generating operation sequences to
solve math problems and generating Lisp programs. In both tasks, TP-N2F achieves state-of-the-art
performance. We further analyze the behavior of the unbinding relation vectors in the proposed
model. Results of each task and the analysis of the unbinding relation vectors are introduced in turn.
Details of experiments and datasets are described in Sec. A.1 in the Appendix.
4.1	Generating operation sequences to solve math problems
Given a natural-language math problem, we need to generate a sequence of operations (operators and
corresponding arguments) from a set of operators and arguments to solve the given problem. Each
operation is regarded as a relational tuple by viewing the operator as relation, e.g., (add, n1, n2).
We test TP-N2F for this task on the MathQA dataset (Amini et al., 2019). The MathQA dataset
consists of about 37k math word problems, each with a corresponding list of multi-choice options
and the corresponding operation sequence. In this task, TP-N2F is deployed to generate the opera-
tion sequence given the question. The generated operations are executed with the execution script
from Amini et al. (2019) to select a multi-choice answer. As there are about 30% noisy data (where
the execution script returns the wrong answer when given the ground-truth program; see Sec. A.1
of the Appendix), we report both execution accuracy (of the final multi-choice answer after running
the execution engine) and operation sequence accuracy (where the generated operation sequence
must match the ground truth sequence exactly). TP-N2F is compared to a baseline provided by the
seq2prog model in Amini et al. (2019), an LSTM-based seq2seq model with attention. Our model
outperforms both the original seq2prog, designated SEQ2PROG-orig, and the best reimplemented
seq2prog after an extensive hyperparameter search, designated SEQ2PROG-best. Table 1 presents
the results. To verify the importance of the TP-N2F encoder and decoder, we conducted experiments
to replace either the encoder with a standard LSTM (denoted LSTM2TP) or the decoder with a stan-
dard attentional LSTM (denoted TP2LSTM). We observe that both the TPR components of TP-N2F
are important for achieving the observed performance gain relative to the baseline.
Table 1: Results on MathQA dataset testing set
MODEL	Operation Accuracy(%)	Execution Accuracy(%)
SEQ2PROG-orig	59.4	519
SEQ2PROG-best	66.97	54.0
TP2LSTM (ours)	68.84	54.61
LSTM2TP (ours)	68.21	54.61
TP-N2F (ours)	71.89	55.95
4.2	Generating program trees from natural-language descriptions
Generating Lisp programs requires sensitivity to structural information because Lisp code can be
regarded as tree-structured. Given a natural-language query, we need to generate code containing
function calls with parameters. Each function call is a relational tuple, which has a function as the
relation and parameters as arguments. We evaluate our model on the AlgoLisp dataset for this task
and achieve state-of-the-art performance. The AlgoLisp dataset (Polosukhin & Skidanov, 2018) is a
program synthesis dataset. Each sample contains a problem description, a corresponding Lisp pro-
gram tree, and 10 input-output testing pairs. We parse the program tree into a straight-line sequence
of tuples (same style as in MathQA). AlgoLisp provides an execution script to run the generated
program and has three evaluation metrics: the accuracy of passing all test cases (Acc), the accuracy
of passing 50% of test cases (50p-Acc), and the accuracy of generating an exactly matching program
(M-Acc). AlgoLisp has about 10% noisy data (details in the Appendix), so we report results both on
the full test set and the cleaned test set (in which all noisy testing samples are removed). TP-N2F is
7
Table 2: Results of AlgoLisp dataset
	Full Testing Set			Cleaned Testing Set		
MODEL (%)	Acc	50p-Acc	M-Acc	Acc	50p-Acc	M-Acc
Seq2Tree	61.0					
LSTM2LSTM+atten	67.54	70.89	75.12	76.83	78.86	75.42
TP2LSTM (ours)	72.28	77.62	79.92	77.67	80.51	76.75
LSTM2TPR (ours)	75.31	79.26	83.05	84.44	86.13	83.43
SAPSpre-VH-Att-256	83.80	87.45		92.98	94.15	
TP-N2F (ours)	84.02	88.01	93.06	93.48	94.64	92.78
compared with an LSTM seq2seq with attention model, the Seq2Tree model in Polosukhin & Ski-
danov (2018), and a seq2seq model with a pre-trained tree decoder from the Tree2Tree autoencoder
(SAPS) reported in Bednarek et al. (2019). As shown in Table 2, TP-N2F outperforms all existing
models on both the full test set and the cleaned test set. Ablation experiments with TP2LSTM and
LSTM2TP show that, for this task, the TP-N2F Decoder is more helpful than TP-N2F Encoder. This
may be because lisp codes rely more heavily on structure representations.
4.3	Interpretation of learned structure
To interpret the structure learned by the model, we extract the trained unbinding relation vectors from
the TP-N2F Decoder and reduce the dimension of vectors via Principal Component Analysis. K-
means clustering results on the average vectors are presented in Figure 4 and Figure 5 (in Appendix
A.6). Results show that unbinding vectors for operators or functions with similar semantics tend to
be close to each other. For example, with 5 clusters in the MathQA dataset, arithmetic operators such
as add, subtract, multiply, divide are clustered together, and operators related to square or volume of
geometry are clustered together. With 4 clusters in the AlgoLisp dataset, partial/lambda functions
and sort functions are in one cluster, and string processing functions are clustered together. Note that
there is no direct supervision to inform the model about the nature of the operations, and the TP-
N2F decoder has induced this role structure using weak supervision signals from question/operation-
sequence-answer pairs. More clustering results are presented in the Appendix A.6.
5	Related work
N2F tasks include many different subtasks such as symbolic reasoning or semantic parsing (Kamath
& Das, 2019; Cai & Lam, 2019; Liao et al., 2018; Amini et al., 2019; Polosukhin & Skidanov,
2018; Bednarek et al., 2019). These tasks require models with strong structure-learning ability.
TPR is a promising technique for encoding symbolic structural information and modeling symbolic
reasoning in vector space. TPR binding has been used for encoding and exploring grammatical
structural information of natural language (Palangi et al., 2018; Huang et al., 2019). TPR unbinding
has also been used to generate natural language captions from images (Huang et al., 2018). Some
researchers use TPRs for modeling deductive reasoning processes both on a rule-based model and
deep learning models in vector space (Lee et al., 2016; Smolensky et al., 2016; Schlag & Schmid-
huber, 2018). However, none of these previous models takes advantage of combining TPR binding
and TPR unbinding to learn structure representation mappings explicitly, as done in our model. Al-
though researchers are paying increasing attention to N2F tasks, most of the proposed models either
do not encode structural information explicitly or are specialized to particular tasks. Our proposed
TP-N2F neural model can be applied to many tasks.
6	CONCLUSION AND FUTURE WORK
In this paper we propose a new scheme for neural-symbolic relational representations and a new
architecture, TP-N2F, for formal-language generation from natural-language descriptions. To our
knowledge, TP-N2F is the first model that combines TPR binding and TPR unbinding in the encoder-
decoder fashion. TP-N2F achieves the state-of-the-art on two instances of N2F tasks, showing
significant structure learning ability. The results show that both the TP-N2F encoder and the TP-
N2F decoder are important for improving natural- to formal-language generation. We believe that
the interpretation and symbolic structure encoding of TPRs are a promising direction for future
work. We also plan to combine large-scale deep learning models such as BERT with TP-N2F to
take advantage of structure learning for other generation tasks.
8
References
Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel Kedziorski, Yejin Choi, and Hannaneh Ha-
jishirzi. Mathqa: Towards interpretable math word problem solving with operation-based for-
malisms. In NACCL, 2019.
Anonymous. Unbinding compressed tensor product representations, in prep.
Jakub Bednarek, Karol Piaskowski, and Krzysztof Krawiec. Ain’t nobody got time for coding:
Structure-aware program synthesis from natural language. In arXiv.org, 2019.
Deng Cai and Wai Lam. Core semantic first: A top-down approach for amr parsing. In
arXiv:1909.04303, 2019.
Kezhen Chen and Kenneth D. Forbus. Action recognition from skeleton data via analogical gener-
alization over qualitative representations. In Thirty-Second AAAI Conference, 2018.
Kezhen Chen, Irina Rabkina, Matthew D. McLure, and Kenneth D. Forbus. Human-like sketch
object recognition via analogical learning. In Thirty-Third AAAI Conference, volume 33, pp.
1336-1343, 2019.
Maxwell Crouse, Clifton McFate, and Kenneth D. Forbus. Learning from unannotated qa pairs to
analogically disanbiguate and answer questions. In Thirty-Second AAAI Conference, 2018.
Kenneth.D. Forbus, Chen Liang, and Irina Rabkina. Representation and computation in cognitive
models. In Top Cognitive System, 2017.
Jianfeng Gao, Michel Galley, and Lihong Li. Neural approaches to conversational ai. Foundations
and TrendsR in Information Retrieval, 13(2-3):127-298, 2019.
Susan Goldin-Meadow and Dedre Gentner. Language in mind: Advances in the study of language
and thought. MIT Press, 2003.
Qiuyuan Huang, Paul Smolensky, Xiaodong He, Oliver Wu, and Li Deng. Tensor product generation
networks for deep nlp modeling. In NAACL, 2018.
Qiuyuan Huang, Li Deng, Dapeng Wu, chang Liu, and Xiaodong He. Attentive tensor product
learning. In Thirty-Third AAAI Conference, volume 33, 2019.
Aishwarya Kamath and Rajarshi Das. A survey on semantic parsing. In AKBC, 2019.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2017.
Moontae Lee, Xiaodong He, Wen-tau Yih, Jianfeng Gao, Li Deng, and Paul Smolensky. Reasoning
in vector space: An exploratory study of question answering. In ICLR, 2016.
Yi Liao, Lidong Bing, Piji Li, Shuming Shi, Wai Lam, and Tong Zhang. Core semantic first: A
top-down approach for amr parsing. In EMNLP, pp. 3855-3864, 2018.
Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-
based neural machine translation. EMNLP, pp. 533-536, 2015.
Hamid Palangi, Paul Smolensky, Xiaodong He, and Li Deng. Question-answering with
grammatically-interpretable representations. In AAAI, 2018.
Illia Polosukhin and Alex Skidanov. Neural program search: Solving programming tasks from
description and examples. In ICLR workshop, 2018.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning internal representations
by error propagation. In David E. Rumelhart, James L. McClelland, and the PDP Group (eds.),
Parallel distributed processing: Explorations in the microstructure of cognition, volume 1, pp.
318-362. MIT press, Cambridge, MA, 1986.
Imanol Schlag and Jurgen Schmidhuber. Learning to reason with third order tensor products. In
Neural Information Processing Systems, 2018.
9
Paul Smolensky. Tensor product variable binding and the representation of symbolic structures in
Connectionist networks. In Artificial Intelligence, volume 46, pp. 159-216, 1990.
Paul Smolensky, Moontae Lee, Xiaodong He, Wen-tau Yih, Jianfeng Gao, and Li Deng. Basic
reasoning with tensor product representations. arXiv preprint arXiv:1601.02745, 2016.
A Appendix
A.1 Implementations of TP-N2F for experiments
In this section, we present details of the experiments of TP-N2F on the two datasets. We present the
implementation of TP-N2F on each dataset.
The MathQA dataset consists of about 37k math word problems ((80/12/8)% training/dev/testing
problems), each with a corresponding list of multi-choice options and an straight-line operation
sequence program to solve the problem. An example from the dataset is presented in the Appendix
A.4. In this task, TP-N2F is deployed to generate the operation sequence given the question. The
generated operations are executed to generate the solution for the given math problem. We use the
execution script from Amini et al. (2019) to execute the generated operation sequence and compute
the multi-choice accuracy for each problem. During our experiments we observed that there are
about 30% noisy examples (on which the execution script fails to get the correct answer on the
ground truth program). Therefore, we report both execution accuracy (the final multi-choice answer
after running the execution engine) and operation sequence accuracy (where the generated operation
sequence must match the ground truth sequence exactly).
The AlgoLisp dataset (Polosukhin & Skidanov, 2018) is a program synthesis dataset, which has
79k/9k/10k training/dev/testing samples. Each sample contains a problem description, a corre-
sponding Lisp program tree, and 10 input-output testing pairs. We parse the program tree into a
straight-line sequence of commands from leaves to root and (as in MathQA) use the symbol #i to
indicate the result of the ith command (generated previously by the model). A dataset sample with
our parsed command sequence is presented in the Appendix A.4. AlgoLisp provides an execution
script to run the generated program and has three evaluation metrics: accuracy of passing all test
cases (Acc), accuracy of passing 50% of test cases (50p-Acc), and accuracy of generating an exactly
matched program (M-Acc). AlgoLisp has about 10% noise data (where the execution script fails to
pass all test cases on the ground truth program), so we report results both on the full test set and the
cleaned test set (in which all noisy testing samples are removed).
We use dR, nR, dF, nF to indicate the TP-N2F encoder hyperparameters, the dimension of role vec-
tors, the number of roles, the dimension of filler vectors and the number of fillers. dRel , dArg , dPos
indicate the TP-N2F decoder hyper-parameters, the dimension of relation vectors, the dimension of
argument vectors, and the dimension of position vectors.
In the experiment on the MathQA dataset, we use nF = 150, nR = 50, dF = 30, dR = 20,
dRel = 20, dArg = 10, dPos = 5 and we train the model for 60 epochs with learning rate 0.00115.
The reasoning module only contains one layer. As most of the math operators in this dataset are
binary, we replace all operators taking three arguments with a set of binary operators based on
hand-encoded rules, and for all operators taking one argument, a padding symbol is appended. For
the baseline SEQ2PROG-orig, TP2LSTM and LSTM2TP, we use hidden size 100, single-direction,
one-layer LSTM. For the SEQ2PROG-best, we performed a hyperparameter search on the hidden
size for both encoder and decoder; the best score is reported.
In the experiment on the AlgoLisp dataset, we use nF = 150, nR = 50, dF = 30, dR = 30,
dRel = 30, dArg = 20, dPos = 5 and we train the model for 50 epochs with learning rate 0.00115.
We also use one-layer in the reasoning module like in MathQA. For this dataset, most function calls
take three arguments so we simply add padding symbols for those functions with fewer than three
arguments.
10
A.2 Detailed equations of TP-N2F
A.2.1 TP-N2F encoder
Filler-LSTM in TP-N2F encoder
This is a standard LSTM, governed by the equations:
fft =以Uff wt + Vff [(Tt-1) + bff)	(13)
gft = tanh(Ufg wt + Vfg [(Tt-1) + bfg)	(14)
if =以Ufi wt + Vfi [(Tt-1) + bfi)	(15)
of =以Ufo wt + Vo [(Tt-1) + bfo)	(16)
cft = fft	cft-1 + ift gft	(17)
hft = oft	tanh(ctf)	(18)
夕,tanh are the logistic sigmoid and tanh functions applied elementwise. [ flattens (reshapes) a
matrix in RdF ×dR into a vector in RdT, where dT = dFdR. is elementwise multiplication. The
variables have the following dimensions:
fft, gft, ift, oft, cft, hft, bff, bfg, bfi, bfo, [(Tt-1) ∈ RdT
wt ∈ Rd
Uff,Ufg,Ufi,Ufo ∈ RdT×d
Vff,Vfg,Vfi,Vfo ∈ RdT×dT
Filler vector
The filler vector for input token wt is ft, defined through an attention vector over possible fillers,
aft:
aft = softmax((Wfa hft)/T)	(19)
ft = Wf aft	(20)
(Wf is the same as F of Sec. 2.) The variables’ dimensions are:
Wfa ∈ RnF×dT
aft ∈ RnF
Wf ∈ RdF ×nF
ft ∈ RdF
T is the temperature factor, which is fixed at 0.1.
Role-LSTM in TP-N2F encoder
Similar to the Filler-LSTM, the Role-LSTM is also a standard LSTM, governed by the equations:
f =以Urf wt + VTf [(Tt-1) + brf)	(21)
grt = tanh(Urg wt + Vrg[(Tt-1) +brg)	(22)
itr =以Uri wt + Vri [(Tt-1) + bri)	(23)
oT =中(Uro wt + Vro [(Tt-1) + bro)	(24)
crt = frt	ctr-1 + itr	grt	(25)
htr = otr	tanh(ctr)	(26)
The variable dimensions are:
frt,grt,itr,ort,crt,hrt,brf,brg,bri,bro,[(Tt-1) ∈RdT
wt ∈ Rd
Urf,Urg,Uri,Uro ∈ RdT×d
Vrf,Vrg,Vri,Vro ∈ RdT×dT
11
Role vector
The role vector for input token wt is determined analogously to its filler vector:
art = softmax((Wra htr)/T)	(27)
rt = Wr atr	(28)
The dimensions are:
Wra ∈ RnR×dT
atr ∈ RnR
Wr ∈ RdR×nR
rt ∈ RdR
Binding
The TPR for the filler/role binding for token wt is then:
Tt = rt 乳 ft
(29)
where
Tt ∈ RdR ×dF
A.2.2 Structure Mapping
H0 = fmapping (Tt)	(30)
H0 ∈ RdH, where dH = dA, dO, dP are dimension of argument vector, operator vector and position
vector. fmapping is implemented with a MLP (linear layer followed by a tanh) for mapping the
Tt ∈ RdT to the initial state of decoder H0.
A.2.3 TP-N2F decoder
Tuple-LSTM
The output tuples are also generated via a standard LSTM:
wdt = γ(wRt-e1l, wAt-r1g1, wAt-r1g2)	(31)
ft =以Uf Wd + Vf [(Ht-1)+ bf)	(32)
gt = tanh(Ug wdt +Vg[(Ht-1)+bg)	(33)
it	=以 Ui Wd	+ Vi [(Ht-1) + bi)	(34)
ot	=以Uo Wd	+ Vo [(Ht-1)+ bo)	(35)
ct = ft	ct-1 + it gt	(36)
hitnput =	ot tanh(ct)	(37)
Ht = Atten(hitnput, [T0, ...,Tn-1])	(38)
Here, γ is the concatenation function. WRt-e 1l is the trained embedding vector for the Relation of
the input binary tuple, WAt-r1g1 is the embedding vector for the first argument and WAt-r1g2 is the
embedding vector for the second argument. Then the input for the Tuple LSTM is the concatenation
of the embedding vectors of relation and arguments, with dimension ddec .
f t, gt, it, ot, ct, hitnput, bf, bg, bi, bo, [(Ht-1) ∈RdH
Wdt ∈ Rddec
Uf, Ug, Ui, Uo ∈ RdH×ddec
Vf, Vg, Vi, Vo ∈ RdH×dH
Ht ∈ RdH
12
Atten is the attention mechanism used in Luong et al. (2015), which computes the dot product
between hitnput and each Tt0 . Then a linear function is used on the concatenation of hitnput and
the softmax scores on all dot products to generate Ht . The following equations show the attention
mechanism:
dt = score(hitnput, CT)	(39)
st = CT softmax(dt)	(40)
Ht = Kγ(hitnput,st)	(41)
score is the score function of the attention. In this paper, the score function is dot product.
CT ∈ RdH×n
dt ∈ Rn
st ∈ RdH
K ∈ RdH ×(dT+n)
Unbinding
At each timestep t, the 2-step unbinding process described in Sec. 3.1.2 operates first on an en-
coding of the triple as a whole, H, using two unbinding vectors p0i that are learned but fixed for
all tuples. This first unbinding gives an encoding of the two operator-argument bindings, Bi . The
second unbinding operates on the Bi , using a generated unbinding vector for the operator, r0, giving
encodings of the arguments, ai . The generated unbinding vector for the operator, r0, and the gener-
ated encodings of the arguments, ai, each produce a probability distribution over symbolic operator
outputs Rel and symbolic argument outputs Argi ; these probabilities are used in the cross-entropy
loss function. For generating a single symbolic output, the most-probable symbols are selected.
	Bt1	Ht p01	(42)
	Bt2	Ht p02	(43)
r0t	= Wdual (B1t +B2t)		(44)
	at1 =	Bt1 r0t	(45)
	at2 =	Bt2 r0t	(46)
	lt = lr =	Ltr r0t	(47)
	lt la1	Lta at1	(48)
	lt la2	Lta at2	(49)
Relt =	argmax(softmax(lrt ))		(50)
Arg1t =	argmax(softmax(lta ))		(51)
Arg2t =	argmax(softmax(lta ))		(52)
r0t ∈ RdO
The dimensions are:
at1 , at2 ∈ RdA
p01,p02 ∈RdP
Bt1,Bt2 ∈ RdA×dO
Wdual ∈ RdH
Ltr ∈ RnO×dO
Lta ∈ RnA×dA
ltr ∈ RnR
lt , lt ∈ RnA
a1 , a2
13
A.3 The tensor that is input to the decoder’ s Unbinding Module is a TPR
Here we show that, if learning is successful, the order-3 tensor H that each iteration of the decoder’s
Tuple LSTM feeds to the decoder’s Unbinding Module (Figure 3) will be aTPR of the form assumed
in Eq. 3, repeated here:
H = E aj 0 r 0pj.	(53)
j
The operations performed by the decoder are given in Eqs. 4-5, and Eqs. 10-11, rewritten here:
H ∙ Pi = qi	(54)
qi ∙ r0 = ai	(55)
This is the standard TPR unbinding operation, used recursively: first with the unbinding vectors for
positions, p0i , then with the unbinding vector for the operator, r0 . It therefore suffices to analyze a
single unbinding; the result can then be used recursively. This in effect reduces the problem to the
order-2 case. What we will show is: given a set of unbinding vectors {ri0} which are dual to a set of
role vectors {ri}, with i ranging over some index set I, if H is an order-2 tensor such that
H ∙ r0 = fi,∀i ∈ I	(56)
then
H = X fi ri> + Z ≡ HTPR + Z	(57)
i∈I
for some tensor Z that annihilates all the unbinding vectors:
Z ∙ ri = 0, ∀i ∈ I.	(58)
If learning is successful, the processing in the decoder will generate the target relational tuple
(R, A1, A2) by obeying Eq. 54 in the first unbinding, where we have ri0 = p0i, fi = qi, I = {1, 2},
and obeying Eq. 55 in the second unbinding, where we have ri0 = r0, fi0 = ai , with I = the set
containing only the null index.
Treat rank-2 tensors as matrices; then unbinding is simply matrix-vector multiplication. Assume
the set of unbinding vectors is linearly independent (otherwise there would in general be no way
to satisfy Eq. 56 exactly, contrary to assumption). Then expand the set of unbinding vectors, if
necessary, into a basis {rk0 }k∈K⊇I. Find the dual basis, with rk dual to rk0 (so that rl>rj0 = δlj).
Because {rk0 }k∈K is a basis, so is {rk}k∈K, so any matrix H can be expanded as H = Pk∈K vkrk>.
Since Hri0 = fi , ∀i ∈ I are the unbinding conditions (Eq. 56), we must have vi = fi , i ∈ I. Let
HTPR ≡ Pi∈I fi ri> . This is the desired TPR, with fillers fi bound to the role vectors ri which
are the duals of the unbinding vectors ri0 (i ∈ I). Then we have H = HTPR + Z (Eq. 57) where
Z ≡ Pj∈K,j6∈I vjrj>; so Zri0 = 0, i ∈ I (Eq. 58). Thus, if training is successful, the model must
have learned how to feed the decoder with order-3 TPRs with the structure posited in Eq. 53.
The argument so far addresses the case where the unbinding vectors are linearly independent, mak-
ing it possible to satisfy Eq. 56 exactly. In relatively high-dimensional vector spaces, it will often
happen that even when the number of unbinding vectors exceeds the dimension of their space by a
factor of 2 or 3 (which applies to the TP-N2F models presented here), there is a set of role vectors
{rk}k∈K approximately dual to {rk0 }k∈K, such that rl>rj0 = δlj ∀l, j ∈ K holds to a good approx-
imation. (If the distribution of normalized unbinding vectors is approximately uniform on the unit
sphere, then choosing the approximate dual vectors to equal the unbinding vectors themselves will
do, since they will be nearly orthonormal (Anonymous, in prep.). If the {rk0 }k∈K are not normal-
ized, we just rescale the role vectors, choosing rk = rk0 /krk0 k2.) When the number of such role
vectors exceeds the dimension of the embedding space, they will be overcomplete, so while it is
still true that any matrix H can be expanded as above (H = Pk∈K vkrk>), this expansion will no
longer be unique. So while it remains true that H a TPR, it is no longer uniquely decomposable into
filler/role pairs. The claim above does not claim uniqueness in this sense, and remains true.)
14
A.4 Dataset samples
A.4. 1 Data sample from MathQA dataset
Problem: The present polulation of a town is 3888. Population increase rate is 20%. Find the
population of town after 1 year?
Options: a) 2500, b) 2100, c) 3500, d) 3600, e) 2700
Operations: multiply(n0,n1), divide(#0,const-100), add(n0,#1)
A.4.2 Data sample from AlgoLisp dataset
Problem: Consider an array of numbers and a number, decrements each element in the given array
by the given number, what is the given array?
Program Nested List: (map a (PartiaH b -))
Command-Sequence: (partial1 b -), (map a #0)
A.5 Generated programs comparison
In this section, we display some generated samples from the two datasets, where the TP-N2F model
generates correct programs but LSTM-Seq2Seq does not.
Question: A train running at the speed of 50 km per hour crosses a post in 4 seconds. What is the
length of the train?
TP-N2F(correct):
(multiply,n0,const1000) (divide,#0,const3600) (multiply,n1,#1)
LSTM(wrong):
(multiply,n0,const0.2778) (multiply,n1,#0)
Question: 20 is subtracted from 60 percent of a number, the result is 88. Find the number?
TP-N2F(correct):
(add,n0,n2) (divide,n1,const100) (divide,#0,#1)
LSTM(wrong):
(add,n0,n2) (divide,n1,const100) (divide,#0,#1) (multiply,#2,n3) (subtract,#3,n0)
Question: The population of a village is 14300. It increases annually at the rate of 15 percent.
What will be its population after 2 years?
TP-N2F(correct):
(divide,n1,const100) (add,#0,const1) (power,#1,n2) (multiply,n0,#2)
LSTM(wrong):
(multiply,const4,const100) (sqrt,#0)
Question: There are two groups of students in the sixth grade. There are 45 students in group a,
and 55 students in group b. If, on a particular day, 20 percent of the students in group a forget their
homework, and 40 percent of the students in group b forget their homework, then what percentage
of the sixth graders forgot their homework?
TP-N2F(correct):
(add,n0,n1) (multiply,n0,n2) (multiply,n1,n3) (divide,#1,const100) (divide,#2,const100) (add,#3,#4)
(divide,#5,#0) (multiply,#6,const100)
LSTM(wrong):
(multiply,n0,n1) (subtract,n0,n1) (divide,#0,#1)
Question: 1 divided by 0.05 is equal to
TP-N2F(correct):
(divide,n0,n1)
LSTM(wrong):
15
(divide,n0,n1) (multiply,n2,#0)
Question: Consider a number a, compute factorial of a
TP-N2F(correct):
(j=,arg1,1)(-,arg1,1) (self,#1)( *,#2,arg1 )(if,#0,1,#3 ) (lambda1,#4 ) (invoke1,#5,a)
LSTM(wrong):
(j=,arg1,1) (-,arg1,1) (self,#1) ( *,#2,arg1) (if,#0,1,#3 ) (lambda1,#4 )(len,a )(invoke1,#5,#6
)
Question: Given an array of numbers and numbers b and c, add c to elements of the product of
elements of the given array and b, what is the product of elements of the given array and b?
TP-N2F(correct):
( partial, b,* ) ( partial1,c,+ ) ( map,a,#0 ) ( map,#2,#1 )
LSTM(wrong):
( partial1,b,+ ) ( partial1,c,+ ) ( map,a,#0 ) ( map,#2,#1 )
Question: You are given an array of numbers a and numbers b , c and d , let how many times you
can replace the median in a with sum of its digits before it becomes a single digit number and b be
the coordinates of one end and c and d be the coordinates of another end of segment e , your task is
to find the length of segment e rounded down
TP-N2F(correct):
( digits arg1 ) ( len #0 ) ( == #1 1 ) ( digits arg1 ) ( reduce #3 0 + ) ( self #4 ) ( + 1 #5 ) ( if #2 0 #6
) ( lambda1 #7 ) ( sort a ) ( len a ) ( / #10 2 ) ( deref #9 #11 ) ( invoke1 #8 #12 ) ( - #13 c ) ( digits
arg1 ) (len#15) (==#16 1 ) ( digits arg1 ) ( reduce #18 0+) ( self #19) (+ 1 #20) (if#170#21
) ( lambda1 #22 ) ( sorta) (len a) (/#25 2 ) ( deref #24 #26 ) ( invoke1 #23 #27 ) ( - #28 c ) ( *
#14 #29) ( -bd) ( -bd) ( * #31 #32) (+#30#33 ) (sqrt#34) (floor#35)
LSTM(wrong): ( digits arg1 ) ( len #0 ) ( == #1 1 ) ( digits arg1 ) ( reduce #3 0 + ) ( self #4 ) ( + 1
#5) (if#20#6) ( lambda1 #7) (sorta) (lena) (/#102) ( deref #9 #11 ) ( invoke1 #8#12c) (-
#13 ) (-bd) (-bd) ( *#15#16) ( *#14#17) (+#18 ) ( sqrt #19 ) (floor#20)
Question: Given numbers a , b , c and e , let d be c , reverse digits in d , let a and the number
in the range from 1 to b inclusive that has the maximum value when its digits are reversed be the
coordinates of one end and d and e be the coordinates of another end of segment f , find the length
of segment f squared
TP-N2F(correct):
( digits c ) ( reverse #0 ) ( * arg1 10 ) ( + #2 arg2 ) ( lambda2 #3 ) ( reduce #1 0 #4 ) ( - a #5 ) ( digits
c ) ( reverse #7 ) ( * arg1 10 ) ( + #9 arg2 ) ( lambda2 #10 ) ( reduce #8 0 #11 ) ( - a #12 ) ( * #6 #13
) ( + b 1 ) ( range 0 #15 ) ( digits arg1 ) ( reverse #17 ) ( * arg1 10 ) ( + #19 arg2 ) ( lambda2 #20
) ( reduce #18 0 #21 ) ( digits arg2 ) ( reverse #23 ) ( * arg1 10 ) ( + #25 arg2 ) ( lambda2 #26 ) (
reduce #24 0 #27) (Z #22 #28)(if #29 arg1 arg2 ) (lambda2 #30 ) (reduce #16 0#31)(- #32 e
) ( + b 1 ) ( range 0 #34 ) ( digits arg1 ) ( reverse #36 ) ( * arg1 10 ) ( + #38 arg2 ) ( lambda2 #39
) ( reduce #37 0 #40 ) ( digits arg2 ) ( reverse #42 ) ( * arg1 10 ) ( + #44 arg2 ) ( lambda2 #45 ) (
reduce #43 0#46) (Z#41 #47) (if#48 arg1 arg2) ( lambda2 #49) (reduce#350#50) (-#51 e)
( *#33#52) (+#14#53 )
LSTM(wrong):
( - a d ) ( - a d ) ( * #0 #1 ) ( digits c ) ( reverse #3 ) ( * arg1 10 ) ( + #5 arg2 ) ( lambda2 #6 ) (
reduce #4 0 #7 ) ( - #8 e ) ( + b 1 ) ( range 0 #10 ) ( digits arg1 ) ( reverse #12 ) ( * arg1 10 ) ( + #14
arg2 ) ( lambda2 #15 ) ( reduce #13 0 #16 ) ( digits arg2 ) ( reverse #18 ) ( * arg1 10 ) ( + #20 arg2 )
( lambda2 #21 ) ( reduce #19 0 #22 ) ( Z #17 #23 ) ( if #24 arg1 arg2 ) ( lambda2 #25 ) ( reduce #11
0 #26 ) ( - #27 e ) ( * #9 #28 ) ( + #2 #29 )
16
A.6 Unbinding relation vector clustering
We run K-means clustering on both datasets with k = 3, 4, 5, 6 clusters and the results are displayed
in Figure 4 and Figure 5. As described before, unbinding-vectors for operators or functions with
similar semantics tend to be closer to each other. For example, in the MathQA dataset, arithmetic
operators such as add, subtract, multiply, divide are clustered together at middle, and operators re-
lated to geometry such as square or volume are clustered together at bottom left. In AlgoLisp dataset,
basic arithmetic functions are clustered at middle, and string processing functions are clustered at
right.
MathQA Clustering Result with 5 Clusters
• surface_cylinder
• SUrfaCe.cylinder
Figure 4: MathQA clustering results
17
Figure 5: AlgoLisp clustering results
18