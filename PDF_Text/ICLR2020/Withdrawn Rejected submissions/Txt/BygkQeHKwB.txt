Under review as a conference paper at ICLR 2020
Walking on the Edge: Fast, Low-Distortion
Adversarial Examples
Anonymous authors
Paper under double-blind review
Ab stract
Adversarial examples of deep neural networks are receiving ever increasing at-
tention because they help in understanding and reducing the sensitivity to their
input. This is natural given the increasing applications of deep neural networks
in our everyday lives. When white-box attacks are almost always successful, it is
typically only the distortion of the perturbations that matters in their evaluation.
In this work, we argue that speed is important as well, especially when considering
that fast attacks are required by adversarial training. Given more time, iterative
methods can always find better solutions. We investigate this speed-distortion
trade-off in some depth and introduce a new attack called boundary projection
(BP) that improves upon existing methods by a large margin. Our key idea is that
the classification boundary is a manifold in the image space: we therefore quickly
reach the boundary and then optimize distortion on this manifold.
1 Introduction
Adversarial examples (Szegedy et al., 2013) are small, usually imperceptible perturbations of images
or other data (Carlini & Wagner, 2018) that can arbitrarily modify a classifier’s prediction. They have
been extended to other tasks like object detection or semantic segmentation (Xie et al., 2017), and
image retrieval (Li et al., 2018; Tolias et al., 2019). They are typically generated in a white-box
setting, where the attacker has full access to the classifier model and uses gradient signals through
the model to optimize for the perturbation. They are becoming increasingly important because they
reveal the sensitivity of neural networks to their input (Simon-Gabriel et al., 2018; Fawzi et al., 2016;
Amsaleg et al., 2017) including trivial cases (Azulay & Weiss, 2018; Engstrom et al., 2017) and they
easily transfer between different models (Moosavi-Dezfooli et al., 2016a; Tramer et al., 2017b).
Adversarial examples are typically evaluated by probability of success and distortion. In many cases,
white-box attacks have probability of success near one, then only distortion matters, as a (weak)
measure of imperceptibility and also of the ease with which adversarial samples can be detected.
The speed of an attack is less frequently discussed. The fast single-step FGSM attack (Goodfellow
et al., 2014) produces high-distortion examples where adversarial patterns can easily be recognized.
At the other extreme, the Carlini & Wagner (C&W) attack (Carlini & Wagner, 2017), considered
state of the art, is notoriously expensive. Decoupling direction and norm (DDN) (Rony et al., 2018)
has recently shown impressive progress in distortion but mostly in speed.
Speed becomes more important when considering adversarial training (Goodfellow et al., 2014).
This defense, where adversarial examples are used for training, was in fact introduced in the same
work as FGSM. The latter remains the most common choice for generating those examples because
of its speed. However, unless a powerful iterative attack is used (Madry et al., 2017), adversarial
training is easily broken (Tramer et al., 2017a).
In this work, we investigate in more depth the speed-distortion trade-off in the regime of probability
of success near one. We observe that iterative attacks often oscillate across the classification bound-
ary, taking long time to stabilize. We introduce a new attack that rather walks along the boundary.
As a result, we improve the state of the art in distortion while keeping iterations at a minimum.
Illustrating the attacks. To better understand how our attack works, we illustrate it qualitatively
against a number of existing attacks in Fig. 1. On this toy 2d classification problem, the class
boundary and the path followed by the optimizer starting at input X can be easily visualized.
1
Under review as a conference paper at ICLR 2020
(c) DDN	(d) BP (this work)
Figure 1: Adversarial attacks on a binary classifier in two dimensions. The two class regions are
shown in red and blue. Contours indicate class probabilities. The objective is to find a point in the
red (adversarial) region that is at the minimal distance to input x. Gray (black) paths correspond
to low (high) distortion target e for PGD2 (Kurakin et al., 2016) (a, in green) or parameter λ for
C&W (Carlini & Wagner, 2017) (b). The simulation is only meant to illustrate basic properties of
the methods. In particular, it does not include Adam optimizer (Kingma & Ba, 2015) for C&W.
PGD2, an b version OfI-FGSM (Kurakin et al., 2016), a.k.a. PGD (Madry et al., 2017), is controlled
by a distortion target e and eventually follows a path on a ball of radius c centered at X (cf. Fig. 1(a)).
Section 4 shows that varying e is an effective yet expensive strategy. It can only be done for a limited
set of values so that the optimal distortion target per image may only be found by luck.
C&W (Carlini & Wagner, 2017) depends on a parameter λ that controls the balance between distor-
tion and classification loss. A low value may lead to failure. A higher value may indeed reach the
optimal perturbation, but with oscillations across the class boundary (cf. Fig. 1(b)). Therefore, an
expensive line search over λ is performed internally.
DDN (Rony et al., 2018) (cf. Fig. 1(c)) increases or decreases distortion on the fly depending on
success and at the same time pointing towards the gradient direction. It arrives quickly near the
optimal perturbation but still suffers from oscillations across the boundary.
On the contrary, boundary projection (BP), introduced in this work (cf. Fig. 1(d)), cares more about
quickly reaching the boundary, not necessarily near the optimal solution, and then walks along the
boundary, staying mostly in the adversarial (red) region. It therefore makes steady progress towards
the solution rather than going back and forth.
Our attack. Our key idea is that, once we reach the adversarial region near the boundary, the
problem becomes optimization on a manifold (Absil et al., 2009): in particular, minimization of the
% distortion on a level set of the classification loss. When in the adversarial region, We project the
distortion gradient on the tangent space of this manifold. We do this simply by targeting a particular
reduction of the distortion while moving orthogonally to the gradient of the classification loss.
Our benchmark. Quantization is another major issue in this literature. Most papers implicitly
assume that the output of a white-box attack is a matrix where pixel values are real numbers in
[0,1]. Rony et al. (2018) is one of the rare works where the output is a quantized. We agree with
this definition of the problem. Indeed, an adversarial image is above all an image. The goal of an
attacker is to publish images deluding the classifier (for instance on the web), and publishing implies
compliance with pixels encoded in bytes.
Contributions. We make the following contributions. To our knowledge, we are the first to
1.	Study optimization on the manifold of the classification boundary for an adversarial attack,
providing an analysis under a number of constraints, such as staying on the tangent space of
the manifold and reaching a distortion target.
2.	Investigate theoretically and experimentally the quantization impact on the perturbation.
3.	Achieve at the same speed as I-FGSM (Kurakin et al., 2016) (20 iterations) and under the
constraint of a quantization, less distortion than state-of-the-art attacks including DDN, which
needs 100 iterations on ImageNet.
2
Under review as a conference paper at ICLR 2020
2	Prob lem, background and related work
2.1	Problem formulation
Preliminaries. Let 匕={0, △,…，1 — l}n with Δ ：= 1∕(L — 1) denote the set of grayscale
images of n pixels quantized to L levels, and let X ：= [0, l]n denote the corresponding real-valued
images. An image of more than one color channels is treated independently per channel; in this
case n stands for the product of pixels and channels. A classifier f ： X T 触 maps an image X
to a vector /(x) ∈ representing probabilities per class over c given classes. The parameters of
the classifier are not shown here because they remain fixed in this work. The classifier prediction
π ： Λ, → [c] ：= {1,... ,c} maps X to the class label having the maximum probability:
π(x) := arg max∕(x)⅛.	(1)
⅛∈[c]
If a true label t ∈ [c] is known, the prediction is correct if π(x)=..
Problem. Let x ∈ Λ, be a given image with known true label .. An adversarial example y ∈ Λ,
is an image such that the distortion ∣∣x — y∣∣ is small and the probability /(y)⅛ is also small. This
problem takes two forms:
1.	Target distortion, minimal probability:
min /(y)t	(2)
y∈Ar
subject to ∣∣x-y∣∣ ≤ e,	(3)
where CiSa given distortion target. The performance is then measured by the probability
ofsuccess Auc ：= P(π(y) ≠ f) as a function of e.
2.	Target success, minimal distortion:
min ∣∣x-y∣∣	(4)
y∈%
subject to π(y)* t.	(5)
The performance is then measured by the expected distortion D ：= E(∣∣x — y∣∣).
This work focuses on the second form, but we present example attacks of both forms in section 2.2.
Untargeted attack. The constraint π(y)丰 t in (5) is referred to as an untargeted attack, meaning
that y is misclassified regardless of the actual prediction. As an alternative, a targeted attack requires
that the prediction π(y) = f isa target label tl 丰 t. We focus on the former.
Loss function. We focus on a white-box attack in this work. Such an attack is specific to /, which is
public. In this setting, attacks typically rely on exploiting the gradient of some loss function, using
variants of gradient descent. A classification loss is defined on the probability vector p = /(y) with
respect to the true label.. For an untargeted attack, this is typically the negative of cross-entropy
g(p,t):= IOgj¾.	(6)
We should warn that, while the cross-entropy is appropriate for bringing examples into the region
of class t during classifier training, its negative (6) is in general not appropriate for pulling them out
during an attack. This is because this function is mostly flat in the class region. A common solution
is to normalize the gradient of q (Goodfellow et al., 2014; Rony et al., 2018), assuming it is nonzero.
We consider more options in this work. A targeted attack on the other hand may use — logp⅛∕, which
works fine because it brings examples into class tl region.
Distortion. This work focuses on the 2-norm IHl as a measure of distortion. Alternatives like
1-norm and ∞-norm are also common (Goodfellow et al., 2014; Carlini & Wagner, 2017). It is
known that none is appropriate for measuring the imperceptibility of adversarial attacks, while more
sophisticated measures like structural similarity (SSIM) (Wang et al., 2004) are limited too (Sharif
et al., 2018). Measuring imperceptibility is arguably as difficult as classification itself.
Integral constraint. The constraint y ∈ Λ, in (3) and (5) is typically relaxed to y ∈ < during
optimization. Some works conclude the attack by loosely quantizing the optimal solution onto Λ,,
3
Under review as a conference paper at ICLR 2020
typically by truncation towards zero. To our knowledge, DDN (Rony et al., 2018) is the only work to
do rounding instead, and at the end of each iteration. Quantization is becoming an important issue in
adversarial examples because the distortions achieved in recent papers are so small that quantization
impacts a lot the perturbations. Appendix A provides a more in-depth study of the impact of the
quantization.
2.2	Attacks
Target Distortion. Given a distortion target G the fast gradient sign method (FGSM) (Goodfellow
et al., 2014) performs a single step in the opposite direction of the (element-wise) sign of the loss
gradient with ∞-norm G
y := x-esignVx-^(∕(x),f).	(7)
This is the fastest method for problem (2)-(3). In the same work adversarial training was introduced,
this method quickly generates adversarial examples for training. However, the perturbations are
usually high-distortion and visible. The iterative-FGSM (I-FGSM) (Kurakin et al., 2016) initializes
y0 ：= X and then iterates
yi+ι := projBoo[x;e](yi -asignVx£(∕(yi),f)),	(8)
where projection1 is element-wise to the closed ∞-norm ball B00 [x； o of radius a and center x, and
also to X (element-wise clipping to interval [0,1]). This method is also known as basic iterative
method (BIM) (Papernot et al., 2018) and as projected gradient descent (PGD) (Madry et al., 2017).
We refer to as PGD2 a 2-norm version replacing (8) with
%+ι = P9⅛[χ河(力 - ^(Vx^(∕(yi),f))),	⑼
where ?)(x) ：= x/ ∣∣x∣∣ denotes 2-normalization, and projection is to the closed 2-norm ball B2[x; e]
of radius a and center x, followed again by element-wise clipping to [0,1]. Although this method is
part of Cleverhans library (Papernot et al., 2018), it is not published according to our knowledge.
Target Success. This family of attacks is typically more expensive. Szegedy et al. (2013) propose a
Lagrangian formulation of problem (4)-(5), minimizing the cost function
J(y,c) := ∣∣x-y∣∣1 2 + λ£(∕(y),f),	(10)
where variable XiSa Lagrange multiplier for (5). They carry out this optimization by box-
constrained L-BFGS.
The attack of Carlini & Wagner (2017), denoted by C&W in the sequel, pertains to this approach.
A change of variable eliminates the box constraint, replacing y ∈ Λ, by σ(w), where w ∈ Mn and
σ is the element-wise sigmoid function. The classification loss encourages the logit logp⅛ to be less
than any other logp⅛ for ⅛ ≠ f by at least margin m ≥ 0,
An(P")：=[logp⅛ - maxlogp⅛ + m]+,	(11)
⅛≠⅛
where [∙]+ denotes the positive part. This function is similar to the multi-class SVM loss by Crammer
and Singer (Crammer & Singer, 2001), where Tn = L and, apart from the margin, it is a hard version
of negative cross-entropy w where softmax is producing the classifier probabilities. It does not have
the problem of being flat in the region of class.. The C&W attack uses the Adam optimizer (Kingma
& Ba, 2015) to minimize the cost function
J(w,入):=∣∣σ(w) -x∣∣2 + λ£ro(∕(σ(w)),f).	(12)
for W ∈ Mn. When the margin is reached, loss tm vanishes and the distortion term pulls σ(w) back
towards x, causing oscillations around the margin. This is repeated for different A2 by line search,
which is expensive.
Decoupling direction and norm (DDN) (Rony et al., 2018) is iterating similarly to PGD2 (9),
%+ι := ProjS[χg] (% -助(Vχg(∕(%), t))),	(13)
1We define PMQjyl(U) := arg minv∈A ∣∣u — v∣∣.
2Referred to as C in Carlini & Wagner (2017).
4
Under review as a conference paper at ICLR 2020
but projection is to the sphere S[x; of of radius Pi and center x, and the radius is adapted to the
current distortion: It is set to pi = (1 — 7)IM -XHif y? is adversarial and to (1 + 7)||y?—
x∣∣ otherwise, where 7 ∈ (0,1) is a parameter. Another major difference is that each iteration is
concluded by a projection onto X (rather than 无)by element-wise clipping to [0,1] and rounding.
Discussion. Optimizing around the class boundary is not a new idea. All of the above attacks do
so in order to minimize distortion; implicitly, even attacks targeting distortion like PGD2 do so,
if the minimum parameter e is sought (cf. Figure 1(a) and Section 4.2). Even black-box attacks do
so (Brendel et al., 2018), without having access to the gradient function. The difference of our attack
is that our updates are along the class boundary, i.e., in a direction normal to the gradient. Deep-
Fool (Moosavi-Dezfooli et al., 2016b) is a popular attack targeting success, that is not optimizing
for distortion and not following a path around the class boundary.
2.3	Other related work
Optimization on manifolds. In the context of deep learning, stochastic gradient descent on Rie-
manian manifolds has been studied, e.g. RSGD (Bonnabel, 2013) and RSVRG (Zhang et al., 2016).
It is usually applied to manifolds whose geometry is known in analytic form, for instance Grass-
mann manifolds (Bonnabel, 2013), optimizing orthogonal matrices on Stiefel manifolds (Harandi &
Fernando, 2016) or embedding trees on the POinCare ball (Nickel & Kiela, 2017).
In most cases, the motivation is to optimize a very complex function (e.g. a classification loss) on a
well-studied manifold, e.g. matrix manifold (Absil et al., 2009). On the contrary, we are optimizing
a very simple quadratic function (the distortion) on a complex manifold not known in analytic form,
i.e. a level set of the classification loss.
3	Method
Our attack is an iterative process with a fixed number K of iterations. Stage 1 aims at quickly
producing an adversarial image, whereas Stage 2 is a refinement phase decreasing distortion. The
key property of our method is that while in the adversarial region during refinement, it tries to walk
along the classification boundary by projecting the distortion gradient onto the tangent hyperplane
of the boundary. Hence we call it boundary projection (BP).
3.1	Stage 1
This stage begins at y0 = x and iteratively updates in the direction of the gradient of the loss function
as summarized in Algorithm 1. The gradient is normalized and then scaled by two parameters:
a fixed parameter a that is large s.t., with high probability, Stage 1 returns an adversarial image
y? ∈ 力；and a parameter that is increasing linearly with iteration a as follows
…一 .* i /.. ..、
7i •— 7min + τ, , 1 ∏max - Tmin),
A + 1
(14)
such that updates are slow at the beginning to keep distortion low, then faster until the attack suc-
ceeds, where 7mi11 ∈ (O,7max) and7max = 1. Clipping is element-wise.
Algorithm 1 Stage 1
Input: x: original image to be attacked
Input: :： true label (untargeted)
Output: y with π(y) ≠ o or failure, iteration i
1: Initialize y0 — χ, e — O
2: while (π(yi) = f) Λ (i < K) do
3:	g一〃(Vχg(f(%),t))
4:	y⅛+ι  clip[0jl] (yi - «7ig)
5:	5 — " 1
6: end while
5
Under review as a conference paper at ICLR 2020
3.2	STAGE2
Once Stage 1 has succeeded, Stage 2 continues by considering two cases: if y? is adversarial, case
out aims at minimizing distortion while staying in the adversarial region. Otherwise, case in aims
at decreasing the loss while controlling the distortion. Both work with a first order approximation
of the loss around %:
e(∕(% +u),t) ≈ 2(J(yt),t') + Uτg,	(15)
where g = Vx£(∕(yi),f). The perturbation at iteration i is 尻：=y? — x. Stage 2 is summarized in
Algorithm 2. Cases out and in illustrated in Fig. 2 are explained below.
Algorithm 2 Stage 2
Input: : true label (untargeted), current iteration number
Input: yi： current adversarial image, e: target distortion
Output: y^κ
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
while i < K do
δi — % — x
g ― ^(Vχf(∕(y⅛),f))
r .伽,④
if 7τ(y? )* t then
e - 7i ∣∣^i∣l
v* x ÷ rg
z - v* + η(yi — v⅛)ʌ/[e2 — r2]+
Ori+1 - QOUT(Z, y)
else
e ll^ll hi _________________________
Z 1 —一 卜 + ʌ/e2 - ∣∣^∣∣2 g
y≈+ι - QIN(Z, %)
end if
分一分+ 1
> perturbation
> direction
> OUT
> target distortion
> IN
> target distortion
16: end while
Case out takes as input y? outside class r region, i.e. τr(y?) 丰 t We set a target distortion
e = % ll&ll < ll&ll (14) such that updates decelerate to convergence once the attack has already
succeeded. We then solve the following problem:
Z := arg min IlV- y? ||	(16)
v∈Vz
V := argmin ∣∣∣v — x∣∣ — e∣,	(17)
v∈F
where P ：= {v ∈ Mn ： (v — yi, g) = 0} is the tangent hyperplane of the level set of the loss at y?,
normal to g. The constraint v ∈ 尸 aims at maintaining the value of the loss, up to the first order.
On this hyperplane, V is the set of points having distortion close to e.
Consider the projection v* ：= χ + rg of X onto that hyperplane, where r ：= (ð^,g). If r ≥ e, then
V = {v*}, and the solution of (16) is trivially Z = v*. Note that v* = y? if 尻，g are collinear. If
/Ce, there is an infinity of solutions to (17). We pick the one closest to y]:
z = v* + W* — 7*N & —2.	(18)
This case is illustrated in Fig. 2(a), where V is a circle that is the intersection of sphere S[x; e] and
尸；then Z is the intersection of V and the line through y? and v*.
Directly quantizing vector Z onto Λ, by Q(∙), the component-wise rounding, modifies its norm (see
App. A). This pulls down our effort to control the distortion. Instead, the process QOUT(Z, %) in
line 9 looks for the scale β of the perturbation to be applied s.t. ∣∣Q(y? + 0(z — y?))|| = ∣∣z∣∣. This
is done with a simple line search over β.
Case IN takes as input y? inside class t region, i.e. τr(y?) = t. We set a target distortion e =
∣∣⅛i∣∣ ∕7i>∣∣⅛i∣∣ (14) such that updates decelerate as in Case out. We then solve the problem:
Z := arg min (v,g),	(19)
v∈S[x56]
6
Under review as a conference paper at ICLR 2020
Figure 2: Refinement stage ofBP. Case OUT when ∖V∖>1 (a); case IN (b). See text for details.
i.e., find the point Z at the intersection of sphere S[x;e] and the ray through y? in the direction
opposite of g as shown in Fig. 2(b). The solution is simple:
z = %一 卜 + ʌ/e2 - ∣∣⅛d∣2 S,
(20)
Vector Z moves away from y? along direction —g by a step size so to reach S[x, e]. Case IN is not
guaranteed to succeed, but invoking it means that Stage 1 has succeeded.
Again a direct rounding jeopardizes the norm of the update z — %. Especially, quantization likely
resultsinQ(Z) = Q(yi) if ∣∣z-yi∣∣ C ∕3min = 0.1 (see App. A). Instead of a line search as in method
out, line 13 just makes sure that this event will not happen: QIN(z, y?) = Q(y? + 0(Z — y?)) with
β = maX(I,⅛nm∕∣∣z - yi∣∣).
4 Experiments
In this section we compare our method boundary projection (BP) to the attacks presented in Sect. 2,
namely: FGSM (Goodfellow et al., 2014), I-FGSM (Kurakin et al., 2016), PGD2 (9), C&W (Carlini
& Wagner, 2017), and DDN (Rony et al., 2018). This benchmark is carried out on three well-known
datasets, with a different neural network for each.
4.1	Datasets, networks, and parameters
For the target distortion attacks i.e. FGSM, I-FGSM and PGD2, we test a set of e and calculate RUC
and D according to our evaluation protocol (cf. section 4.2). For C&W, we test several parameter
settings and pick up the optimum setting as specified below. For DDN, the parameter settings are
the default (Rony et al., 2018), i.e. S = 1.0 and T = 0.05. Below we specify different networks and
parameters for each dataset.
MNIST (LeCun et al., 2010). We use is a simple network with three convolutional layers and one
fully connected layer achieving accuracy 0.99, referred to as C4. The first convolutional layer has
64 features, kernel of size 8 and stride 2; the second has 128 features, kernel 6 and stride 2; the third
has also 128 features, but kernel 5 and stride L It uses LeakyRelu activation (Maas et al., 2013).
Parameters. We set α = 0.08 for I-FGSM and α = e/2 for PGD2. For C&W: for 5x20 iterations3,
learning rate η = 0.5 and initial constant λ = 1.0; for 1 X 100 iterations, η = 0.1 and λ = 10.0.
CIFAR10 (Krizhevsky & Hinton, 2009). We use a simple CNN network with nine convolutional
layers, two max-pooling layers, ending in global average pooling and a fully connected layer. Its
accuracy is 0.927. Batch normalization (Ioffe & Szegedy, 2015) is applied after every convolutional
layer. It also uses LeakyRelu.
Parameters. We set α = 0.08 for I-FGSM and a = e/2 for PGD2. For C&W: for 5 x 20 iterations,
learning rate η = 0.1 and initial constant λ = 0.1; for 1 X 100 iterations, η = 0.01, and λ = 1.0.
3C&W performs line search on Λ: “5 X 20” means 5 values of Λ, 20 iterations for each.
7
Under review as a conference paper at ICLR 2020
	# Grads	Auc	D
Rounding in the end	20	1.00	1.44
	100	1.00	1.43
Rounding at each iteration	20 100	1.00 1.00	0.41 0.32
Rounding with QIN, QOUT	20 100	1.00 1.00	0.35 0.28
Table 1: Success probability AUC and average distortion D of our method BP on ImageNet with
different quantization strategies.
ImageNet (Kurakin et al., 2018) comprises 1,000 images from ImageNet (Deng et al., 2009). We
use InCePtionV3 (Szegedy et al., 2016) whose accuracy is 0.96.
Parameters. We set α = 0.08 for I-FGSM and α = 3 for PGD2. For C&W: for 5 x 20 iterations,
learning rate η = 0.01 and initial constant λ = 20; for 1 X 100 iterations, η = 0.01 and λ = 1.0.
4.2	Evaluation protocol
We evaluate an attack by its runtime, two global statistics AUC and D, and by an operating charac-
teristic curve D T P(D) measuring distortion vs. probability of success as described below.
Since we focus on the speed-distortion trade-off, we measure the required time for all attacks. For
the iterative attacks, the complexity of one iteration is largely dominated by the computation of the
gradient, which requires one forward and one backward pass through the network. It is thus fair
to gauge their complexity by this number, referred to as iterations or ‘# Grads’. Indeed, the actual
timings of 100 iterations for I-FGSM, PGD2, C&W, DDN and BP are 1.08, 1.36, 1.53, 1.46 and
1.17 s/image on average respectively on ImageNet, using Tensorflow, Cleverhans implementation
for I-FGSM and C&W, and authors implementation for DDN.
We measure distortion when the adversarial images are quantized by rounding each element to
the nearest element in Λ,. This makes sense since adversarial images are meant to be stored or
communicated as images rather than real-valued matrices. DDN and BP adversarial images are
already quantized. For reference, we report distortion without quantization in Appendix B.3.
Given a test set of N images, We only consider its subset XOfN images that are classified correctly
without attack. The accuracy of the classifier is NIN二 Let XSUC be the subset of X with NSUC ：=
IXSUCl where the attack succeeds and let D(x) ：= ∣∣x — y∣∣ be the distortion for image X ∈ Xsuc∙
The global statistics are the success probability Auc and conditional average distortion D
PSUC = M，方= WEaX)•
suc X∈Xsuc
(21)
Here, D is conditioned on success. Indeed, distortion makes no sense for a failure.
We define the operating CharacteriStic of a given attack over the set X as the function P :
[0, Pmax] T [0,1], where Pmax := maXX°x)∙ Given D ∈ [0,Pmax], P(O is the prob-
ability of success subject to distortion being upper bounded by D,
PeD) ：= g∣{x ∈ XSUC : O(X) ≤ Q}∣∙	(22)
This function increases from P(O) = 0 to P(QmaX) = Auc∙ We sample one intermediate point:
EPP := P(QUPP) is the success rate within a distortion upper bounded by I)UPP ∈ (0, DmaX).
It is difficult to define a fair comparison of distortion targeting attacks to success targeting attacks
(see section 2.2). For the first family, we run a given attack several times over the test set with
different target distortion J The attack succeeds on image x ∈ X if it succeeds on at least one of
the runs, and the distortion D(x) is the minimum distortion over all successful runs. All statistics
are then evaluated as above.
8
Under review as a conference paper at ICLR 2020
			MNIST				CIFAR10			ImageNet		
Attack	# Grads	FSUC	万	BPP	Auc	万	BPP	Auc	万	BpP
FGSM	1	0.99	5.80	0.00	0.95	5.65	0.00	0.88	9.18	0.00
I-FGSM	20	1.00	3.29	0.17	1.00	3.54	0.00	1.00	4.90	0.00
	100	1.00	3.23	0.18	1.00	3.53	0.00	1.00	4.90	0.00
PGD2	20	1.00	1.80	0.63	1.00	0.66	0.76	0.63	3.63	0.00
	100	1.00	1.74	0.66	1.00	0.60	0.84	1.00	1.85	0.00
C&W	5×20	1.00	1.94	0.56	0.99	0.56	0.81	1.00	1.70	0.00
	1X100	0.98	1.90	0.57	0.87	0.38	0.76	0.97	2.57	0.00
DDN	20	0.82	1.40	0.70	1.00	0.63	0.74	0.99	1.18	0.05
	100	1.00	1.41	0.87	1.00	0.21	0.98	1.00	0.43	0.97
BP (this work)	20	1.00	1.45	0.86	0.97	0.49	0.87	1.00	0.35	0.96
	100	1.00	1.37	0.91	0.97	0.30	0.97	1.00	0.28	1.00
Table 2: Success probability AUC and average distortion D with quantization. EPP is the success
rate under distortion budget I)UPP = 2 for MNIST, 0.7 for CIFAR10, and 1 for ImageNet.
ι
0.8
⅛ 0.6
窜0.4
0.2
0
0	2	4	6
D
(a) MNIST
(c) ImageNet
Figure 3: OPerating characteristics on MNIST, CIFAR10 and ImageNet. The number of iterations
is 5 x 20 for C&W and 100 for I-FGSM, PGD2, DDN and our BP.
4.3	Quantization
Before addressing the benchmark, Table 1 shows the critical role of quantization in our method
BP. Since this attack is iterative and works with continuous vectors, one may quantize only at the
end of the Process, or at the end of each iteration. Another oPtion is to anticiPate the detrimental
action of quantizing by adapting the length of each step accordingly, as done by Qin(∙) and QOUT(∙)
in Algorithm 2. The exPerimental results show that the key is to quantize often so to let the next
iterations comPensate. AnticiPating and adaPting gives a substantial extra imProvement.
4.4	Attack evaluation
Table 2 summarizes the global statistics of the benchmark. Fig. 3 offers a more detailed view Per
dataset with oPerating characteristic Plots.
In terms of average distortion, all iterative attacks Perform much better than the single-steP FGSM.
The Performances of C&W are on Par with those of I-FGSM, which is unexPected for this more
elaborated attack design. The reason is that C&W is Put under stress in our benchmark. It usually
requires a bigger number of iterations to deliver high quality images. Note that it is Possible to avoid
the line search on parameter λ as shown in row 1 X 100. However, it requires a fine tuning so that
this single value works over all the images of the dataset. This is not Possible for ImageNet.
DDN and our method BP are clearly ahead of the benchmark. DDN yields lower distortion on
MNIST at fewer iterations, but its probability of success is not satisfying. DDN is indeed better than
BP only on CIFAR10 at 100 iterations. Fig. 3 reveals that the two attacks have similar operating
characteristic on all datasets but this is because it refers to 100 iterations.
In terms of success rate, FGSM fails on MNIST; on CIFAR10, I-FGSM and PGD2 fail as well;
finally on ImageNet, C&W fails too. DDN also fails on ImageNet at 20 iterations.
Increasing the number of iterations helps but not at the same rate for all the attacks. For instance,
going from 20 to 100 iterations is waste of time for I-FGSM while it is essential for decreasing
the distortion of DDN or making PGD2 efficient on ImageNet. Most importantly, our attack BP
brings a dramatic improvement in the speed vs. distortion trade-off. Just within 20 iterations, the
9
Under review as a conference paper at ICLR 2020
			MNIST								CIFAR10						
Attack →		PGD2		DDN		BP		PGD2		DDN		BP	
D Defense		20	100	20	100	20	100	20	100	20	100	20	100
	FsUC	1.00	1.00	0.82	1.00	T.00	1.00	1.00	1.00	1.00	1.00	-097"	-~097-
baseline	D	1.80	1.74	1.40	1.41	1.45	1.37	0.66	0.59	0.63	0.21	0.49	0.30
	EJPP	0.63	0.66	0.70	0.87	0.86	0.91	0.76	0.84	0.74	0.98	0.87	0.97
	FsUC	1.00	1.00	0.51	1.00	""0.89	1.00	1.00	1.00	1.00	1.00	-099	~_1.00-
FGSM	D	1.92	1.85	1.28	1.60	1.92	1.58	0.68	0.62	0.59	0.24	0.67	0.24
	EJPP	0.48	0.53	0.44	0.72	0.53	0.73	0.72	0.79	0.80	0.98	0.72	0.99
	FsUC	0.99	1.00	0.29	1.00	""0.99	1.00	1.00	1.00	0.98	1.00	T.00	~_1.00-
DDN	D	3.03	2.89	1.68	2.38	2.69	2.27	0.95	0.94	0.77	0.71	0.75	0.68
	EJPP	0.12	0.14	0.20	0.32	0.28	0.34	0.52	0.52	0.54	0.55	0.56	0.58
	FsUC	0.94	0.96	0.36	1.00	-095"	1.00	1.00	1.00	0.97	1.00	T.00	~_1.00-
BP	D	3.14	3.12	1.65	2.81	2.98	2.73	0.96	0.94	0.75	0.70	0.76	0.69
	EJPP	0.15	0.15	0.24	0.27	0.25	0.26	0.55	0.55	0.57	0.59	0.56	0.59
Table 3: Success probability Auc, average distortion D, and success rate EPP under adversarial
training defense with I-FGSM, DDN, or BP (with 20 iterations) as the reference attack. For the
MNIST, the model is trained from scratch. For CIFAR10, it is fine-tuned for 30 extra ePochs as
suggested by Rony et al. (2018) with DDN and BP, and trained from scratch for 200 ePochs with
FGSM. T⅛p measured at distortion DUPP = 2 for MNIST, and 0.7 for CIFAR10.
distortion achieved on ImageNet is very low comPared to the others. APPendix B.2 shows the sPeed
vs. distortion trade-off in more detail.
Statistics of BP stages are as follows: On CIFAR-10 and MNIST, Stage 1 takes 7 iterations on
average. On ImageNet, Stage 1 takes on average 3 iterations out of 20, or 8 iterations out of 100.
APPendix C shows examPles of images along with corresPonding adversarial examPles and Pertur-
bations for different methods.
4.5 Defense evaluation with adversarial training
We also test under adversarial training (Goodfellow et al., 2014). The network is re-trained with a
dataset comPosed of the original training set and the corresPonding adversarial images. This training
is sPecial: at the end of each ePoch, the network is uPdated and fixed, then the adversarial images
for this new uPdate are forged by some reference attack, and the next ePoch starts with this new set.
This is tractable only if the reference attack is fast. We use it with FGSM as the reference attack.
It is more interesting to study DDN and BP as alternatives to FGSM: at 20 iterations, they are fast
enough to Play the role of the reference attack in adversarial training. In this case, we follow the
training Process suggested by Rony et al. (2018): the model is first trained on clean examPles, then
fine-tuned for 30 iterations with adversarial examPles. As shown in Table 3, DDN and BP Perform
equally better than FGSM on CIFAR10, in terms of either average distortion or success rate. Among
the reliable attacks (i.e. whose AUC is close to 1), the worst attack now requires a distortion three
times larger than the distortion of the worst attack without defense. In the same way, on MNIST, the
distortion of the worst case attack doubles going from 1.37 (baseline) to 2.73 (BP defense). In most
cases, BP is a better defense than DDN, forcing the attacker to have 20% more distortion. Note that
for a given defense, the strongest attack is almost always BP.
5 Discussion
The main idea of BP is to travel on the manifold defined by the class boundary while seeking to
minimize distortion. This travel is oPerated by the refinement stage, which alternates on both sides
of the boundary, but attemPts to stay mostly in the adversarial region. Referring to section 2.1, BP is
in effect doing for the target success problem what PGD2 is doing for the target distortion problem:
BP minimizes distortion on the class boundary manifold (a level set of the classification loss), while
PGD2 minimizes the classification loss on a sphere (a level set of the distortion).
BP also takes into account the detrimental effect of quantization. By doing so, the amplitude of
the perturbation is controlled from one iteration to another. The main advantage of our attack is the
small number of iterations required to achieve both reliability (probability of success close to one)
and high quality (low average distortion).
10
Under review as a conference paper at ICLR 2020
References
P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds. Princeton
University Press, 2009.
Laurent Amsaleg, James E. Bailey, Dominique Barbe, Sarah Erfani, Michael E Houle, Vinh Nguyen, and
Milos Radovanovic. The Vulnerability of Learning to Adversarial Perturbation Increases With Intrinsic
Dimensionality. In Proc. of WIFS 2017, Rennes, France, December 2017.
Aharon Azulay and Yair Weiss. Why do deep convolutional netWorks generalize so poorly to small image
transformations? Technical report, 2018.
Silvere Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on Automatic
Control, 58(9):2217-2229, 2013.
Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks
against black-box machine learning models. In ICLR, 2018.
Nicholas Carlini and David Wagner. ToWards evaluating the robustness of neural netWorks. In IEEE Symp. on
Security and Privacy, 2017.
Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on speech-to-text. Technical
report, 2018.
Koby Crammer and Yoram Singer. On the algorithmic implementation of multiclass kernel-based vector ma-
chines. Journal of Machine Learning Research, 2(Dec), 2001.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In CVPR, pp. 248-255. Ieee, 2009.
Logan Engstrom, Brandon Tran, Dimitris Tsipras, LudWig Schmidt, and Aleksander Madry. A rotation and a
translation suffice: Fooling cnns With simple transformations. Technical report, 2017.
Alhussein FaWzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classifiers: From
adversarial to random noise. Technical report, 2016.
A. Gersho and R.M. Gray. Vector Quantization and Signal Compression. The Springer International Series in
Engineering and Computer Science. Springer US, 1991. ISBN 9780792391814.
Ian J GoodfelloW, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.
arXiv:1412.6572, 2014.
Mehrtash Harandi and Basura Fernando. Generalized backpropagation, etude de cas: Orthogonality. Technical
report, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep netWork training by reducing
internal covariate shift. In International Conference on Machine Learning, 2015.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report,
2009.
Alexey Kurakin, Ian GoodfelloW, and Samy Bengio. Adversarial examples in the physical World.
arXiv:1607.02533, 2016.
Alexey Kurakin, Ian GoodfelloW, Samy Bengio, Yinpeng Dong, Fangzhou Liao, Ming Liang, Tianyu Pang, Jun
Zhu, Xiaolin Hu, Cihang Xie, et al. Adversarial attacks and defences competition. arXiv:1804.00097, 2018.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handWritten digit database. AT&T Labs [Online]. Avail-
able: http://yann. lecun. com/exdb/mnist, 2, 2010.
Jie Li, Rongrong Ji, Hong Liu, Xiaopeng Hong, Yue Gao, and Qi Tian. Universal perturbation attack against
image retrieval. Technical report, 2018.
AndreW L Maas, AWni Y Hannun, and AndreW Y Ng. Rectifier nonlinearities improve neural netWork acoustic
models. In ICML, volume 30, 2013.
Aleksander Madry, Aleksandar Makelov, LudWig Schmidt, Dimitris Tsipras, and Adrian Vladu. ToWards deep
learning models resistant to adversarial attacks. arXiv:1706.06083, 2017.
11
Under review as a conference paper at ICLR 2020
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial
perturbations. arXiv:1610.08401, 2016a.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate
method to fool deep neural networks. In CVPR, 2016b.
Maximilian Nickel and DouWe Kiela. PoinCare embeddings for learning hierarchical representations. Technical
report, 2017.
Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian GoodfelloW, Reuben Feinman, Alexey Kurakin, Cihang
Xie, Yash Sharma, Tom BroWn, Aurko Roy, Alexander Matyasko, Vahid Behzadan, Karen Hambardzumyan,
Zhishuai Zhang, Yi-Lin Juang, Zhi Li, Ryan Sheatsley, Abhibhav Garg, Jonathan Uesato, Willi Gierke,
Yinpeng Dong, David Berthelot, Paul Hendricks, Jonas Rauber, and Rujun Long. Technical report on the
cleverhans v2.1.0 adversarial examples library. arXiv:1610.00768, 2018.
Jerome Rony, Luiz G. Hafemann, Luis S. Oliveira, Ismail Ben Ayed, Robert Sabourin, and Eric Granger.
Decoupling direction and norm for efficient gradient-based l2 adversarial attacks and defenses. Technical
report, 2018.
Mahmood Sharif, Lujo Bauer, and Michael K. Reiter. On the suitability of Zp-norms for creating and preventing
adversarial examples. arXiv:1802.09653, 2018.
Carl-Johann Simon-Gabriel, Yann Ollivier, Bernhard Scholkopf, Leon Bottou, and David Lopez-Paz. Adver-
sarial vulnerability of neural netWorks increases With input dimension. Technical report, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian GoodfelloW, and Rob
Fergus. Intriguing properties of neural netWorks. arXiv:1312.6199, 2013.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and ZbignieW Wojna. Rethinking the incep-
tion architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 2818-2826, 2016.
G. Tolias, F. Radenovic, and O. Chum. Targeted mismatch adversarial attack: Query With a floWer to retrieve
the toWer. In Proc. of ICCV, 2019.
Florian Tramer, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick McDaniel. Ensemble adversarial
training: Attacks and defenses. arXiv:1705.07204, 2017a.
Florian Tramer, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. The space of transfer-
able adversarial examples. arXiv:1704.03453, 2017b.
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error
visibility to structural similarity. IEEE Trans. on image processing, 13(4), 2004.
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. Adversarial examples
for semantic segmentation and object detection. arXiv:1703.08603, 2017.
Hongyi Zhang, Sashank J Reddi, and Suvrit Sra. Riemannian svrg: Fast stochastic optimization on riemannian
manifolds. In NIPS, pp. 4592-4600, 2016.
12
Under review as a conference paper at ICLR 2020
A Predicting distortion after quantization
This appendix aims at predicting the norm of the update after quantization, assuming that it is inde-
pendent from the computation of the perturbation. Iteration s starts with a quantized image y? ∈ Λ,,
adds update U ∈ Mn, and then quantizes s.t. y?+i = Q(y? + u). Quantization is done by rounding
with Δ := 1∕(L — 1) the quantization step. Pixel j is quantized to
%+l,3. = yi,j + eJ	(23)
for some ej ∈ ΔZ such that ¾ ∈ © — ∆∕2,βj + Δ∕2]. Border effects where %,j + 3 g ；Vare
neglected.
We now take a statistical point of view where the update is modelled by a random vector U uniformly
distributed over the hypersphere of radius p. That parameter P is the norm of the perturbation
before quantization. This yields random quantization values, denoted by Ej ∈ ΔZ for pixel 九 The
distortion between the two images is
D2 =
n
∑2 (%+1L %,“2
(24)
n
=E玲
A common approach in source coding theory is the additive noise model for quantization error in
the high resolution regime (Gersho & Gray, 1991). It states that Ej=IJj + Qj where Qj ∈
(—Δ∕2,Δ∕2] is the quantization error. When p》△, then Qj becomes uniformly distributed (s.t.
E(Q) = O and E(Q∣) = ∆2∕12) and independent of Uj (s.t. E(UjQj) = E(Uj)E(Qj) = O).
Under these assumptions, Eq. 24 simplifies in expectation to:
/ n	∖	Λ2
E(D2) = E I £l7j +Qf+ 2l7j.Qd =p2+∏-∙
(25)
This shows that quantization increases the distortion on expectation.
Yet, this simple analysis is wrong outside the high resolution regime, and we need to be more careful.
The expectation of a sum is always the sum of the expectations, whatever the dependence between
the summands: E(D2) = I E(Ej) = ∏E(E,J) with
L-I
E(Ef) =Δ252 ^2P(∣¾∣ = £△).	(26)
z=o
We need the distribution of Ej to compute the expected distortion after quantizarion. This ran-
dom variable Ej takes a value depending on the scalar product Sj ：= UTcj, where Cj is the j-th
canonical vector. This scalar product lies in [—p, p], so that P(场 ≥ £△) = O if — Δ∕2 > ∣∣p∣∣.
Otherwise, Ej ≥ £△ when ISjl ≥ M —Δ∕2, which happens when U lies inside the dual hypercone
of axis Cj and semi-angle。⑶ =arccos(⅛(£)) with 水)：=(2£ — l)∆∕2∣∣p∣∣. The probability of
this event is equal to the ratio of the solid angles of this dual hypercone and the full space Mn. This
quantity can be expressed via the incomplete regularized beta function ʃ, and approximately equals
2Φ(√n⅛(-^)∕2) for large n. In the end, V£ ∈ {0,... ,L — 1),
T, if£ = O
P(IEil ≥ £△) = ( 1 —，⑷ 2(1/2,⑺ - 1)/2), if O ≤ $(£) ≤ 1
i0, otherwise
Computing E(D2) is now possible because P(∖Ej∖ = £△) = P(∖Ej∖ > £A) - P(∖Ej∖ ≥ (£+1)∆).
This expected distortion after quantization depends on Δ, n, and P the norm of the perturbation be-
fore quantization. Figure 4 shows that quantization reduces the distortion outside the high resolution
regime. Indeed, ʌ∕E(D2) is close to 0 for p < 0.1 when n = 3 * 2992 (i.e. ImageNet). When the
update has a small norm p, quantization is likely to kill it, y?+i = y?, and we waste one iteration. On
the contrary, ʌ∕E(D2) converges to ʌ/p2 + n∆2∕12 for large p (i.e. in the high resolution regime).
Note that the ratio of the distortions before and after quantization p∕ʌ∕E(D2) quickly converges to
1 for large p.
13
Under review as a conference paper at ICLR 2020
Figure 4: >∕E(D2) as a function of 0 for = 3 * 2992 and △ = 1/255.
B Additional experiments
B.1 PARAMETER STUDY
There are two parameters in BP: a and ‰lin∙ Both determine the step size of stage 1, while ‰lin
also determines the step size of stage 2. We consider 4 values for α, i.e. 1,2,3,4 and 9 values for
Tmin, i.e. 0.1,0.2,…,0.9. For each pair of values, we evaluate BP with 20 iterations on a validation
set, which we define as a random subset sampled of the training set: 10000 images for MNIST and
CIFAR10, and 1000 images for ImageNet. As shown in Fig. 5, success probability is close to one
in all cases, while average distortion is in general stable up to 7mi11 = 0.8. We choose α = 2 and
7min = 0.7 for all experiments.

1
8
7
1
IQ
1
6
1
5
Tm i ■
4
0.2 0.4 0.6 0.8
cfmin
0.99
0.98
0.97
(a) MNIST
1
Tm i ∙
0.96
0.2 0.4 0.6 0.8
(d) MNIST
(b) CIFAR10
1
0.99
□
窜 0.98
0.97
0.96
0.2 0.4 0.6 0.8
Iymirl
(e) CIFAR10
(c) ImageNet
Tmin
(f) ImageNet
1
Figure 5:	Success probability RUC and average distortion D for different values of parameters a and
7min of BP with 20 iterations.
B.2 Speed VS. distortion trade-off
Figure 6(a) is a graphical view of some results reported in Table 2 with more choices of number
of iterations between 20 and 100, and only for ImageNet where our performance gain is the most
significant. Just within 20 iterations, its distortion D is already so much lower than that of other
attacks, that its decrease (-20% at 100 iterations) is not visible in Fig. 6. on the contrary, more
iterations are useless for I-FGSM, and PGD2 can achieve low distortion only with a number of
iterations bigger than 50. Figure 6(b) confirms that the probability of success is close to 1 for both
DDN and BP for the numbers of iterations considered.
14
Under review as a conference paper at ICLR 2020
(a)	(b)
Figure 6:	(a) Average distortion VS. number of iterations for I-FGSM, PGD2, C&W, DDN and our
method BP on ImageNet. I-FGSM is not improving with iterations because it is constrained by e.
(b) Corresponding probability of success VS. number of iterations for PGD2 and BP.
B.3	Attack evaluation without quantization
Table 4 is the equivalent of Table 2 but without the integral constraint: the attack is free to output any
real matrix provided that the pixel values all belong to [0,1]. When the distortion is large, there is
almost no difference. The model of App. A explains this: We are in the high resolution regime and
the extra term in Eq. 25 is negligible compared to the perturbation distortion before quantization.
This is especially true when the number of samples n is small (i.e. MNIST, and to some extend,
CIFAR-10).
When an attack delivers low distortion on average with real matrices, the quantization may lower the
probability of success. This is especially true with the iterative attacks finding adversarial examples
just nearby the border between the two classes. Quantization jeopardizes this point and sometimes
brings it back in the true class region. More importantly, the impact of the quantization on the
distortion is no longer negligible. This is clearly visible when comparing Table 4 and Table 2 for
DDN and BP over ImageNet.
Similarly, Fig. 7 is the equivalent of Fig. 3 without the integral constraint. By comparing the two
figures, it can be seen that PGD2 and C&W, but also DDN and BP, are improving on ImageNet
by having significantly lower distortion. This agrees with measurements of success rate in Table 4,
where pGd2 and C&W are not failing as they do in Table 2 with quantization. Our BP is still the
strongest attack over all datasets.
			MNIST				CIFAR10			ImageNet		
Attack	# Grads	FSUC	万	EJPP	FSUC	万	EJPP	FSUC	万	EJPP
FGSM	1	0.99	5.81	0.00	0.97	4.78	0.00	0.85	3.02	0.00
I-FGSM	20	1.00	3.22	0.27	1.00	3.54	0.00	1.00	4.47	0.00
	100	1.00	3.16	0.29	1.00	3.53	0.00	1.00	4.47	0.00
PGD2	20	1.00	1.76	0.63	1.00	0.51	0.77	0.64	3.94	0.36
	100	1.00	1.70	0.66	1.00	0.43	0.85	0.95	1.11	0.61
C&W	5×20	1.00	1.93	0.56	1.00	0.56	0.81	1.00	1.37	0.23
	1x100	1.00	1.89	0.57	0.97	0.38	0.84	1.00	1.87	0.06
DDN	20	0.82	1.39	0.70	1.00	0.62	0.74	1.00	0.76	0.95
	100	1.00	1.41	0.87	1.00	0.20	0.98	1.00	0.28	0.99
BP (this work)	20	1.00	1.41	0.86	0.97	0.33	0.87	1.00	0.20	1.00
	100	1.00	1.35	0.91	0.97	0.18	0.97	1.00	0.16	1.00
Table 4: Success probability AUC and average distortion D without quantization. EPP measured at
Dupp = 2 for MNIST, 0.7 for CIFAR10, and 1 for ImageNet.
15
Under review as a conference paper at ICLR 2020
1
0.8
0.6
0.4
0.2
0
0	2	4	6
D
(b) CIFAR10
(c) ImageNet
Figure 7: Operating characteristics on MNIST, CIFAR10 and ImageNet without quantization. The
number of iterations is 5 x 20 for C&W and 100 for I-FGSM, PGD2, DDN and our BP.
B.4	Attack evaluation on robust models
Table 5 is similar to Table 2 but is evaluating attacks on robust models. In particular, on MNIST
and CIFAR10, we use the same models as described in Section 4.1, which we adversarially train
according to Madry et al. (2017). On ImageNet, we use off-the shelf4 InceptionV3 obtained by
ensemble adversarial training on four models Tramer et al. (2017a).
In general, DDN and BP outperform all other attacks in terms of either average distortion D or
success rate J⅛p. On ImageNet in particular, all other attacks have significantly higher distortion
and fail in terms of success rate. DDN and BP have similar performance on CIFAR10. On MNIST,
DDN fails in terms of probability of success at 20 iterations, while at 100 iterations BP is superior.
On ImageNet, DDN has significantly greater distortion than BP and fails in terms of success rate at
20 iterations, while at 100 iterations BP still has lower distortion.
Fig. 8	is showing a more detailed view of operating characteristics, similarly to Fig. 3 for models
trained on natural images. We can see that BP is still ahead of the competition. It is close to DDN,
but this is because Fig. 8 refers to 100 iterations. The two attacks outperform all others by a large
margin.
		MNIST (Madry et al., 2017)			CIFAR10 (Madry et al., 2017)			ImageNet (Tramer et al., 2017a)		
Attack	# Grads	Auc	方	EJPP	Auc	方	EJPP	Auc	万	EJPP
FGSM 一	1	0.48	5.69	0.05	-098^	6.21	0.00	~0A4~	2.98	0.00
I-FGSM	-20-	1.00	4.99	0.08	T.00"	4.53	0.00	~00Q~	4.92	0.00
	100	1.00	4.99	0.08	1.00	4.56	0.00	1.00	4.93	0.00
PGD2	-20-	0.99	2.76	0.19	T.00"	1.03	0.41	-076-	2.14	0.00
	100	1.00	2.68	0.20	1.00	1.02	0.41	0.98	1.59	0.00
C&W	5x20	0.99	2.75	0.27	-098^	1.41	0.22	-098^	2.85	0.00
	1X100	0.94	2.22	0.34	0.60	0.77	0.27	0.97	2.41	0.00
DDN	-20-	0.43	1.61	0.32	-097"	0.92	0.41	-099^	1.10	0.23
	100	1.00	2.12	0.48	1.00	0.87	0.42	1.00	0.34	0.98
BP (this work)	-20-	1.00	2.17	0.46	T.00"	0.94	0.41	T.00"	0.35	0.94
	100	1.00	2.00	0.51	1.00	0.88	0.43	1.00	0.23	0.99
Table 5: Success probability AUc, average distortion D, and success rate EPP under adversarial
training with PGD as the reference attack, following (Madry et al., 2017) for MNIST and CIFAR10;
and ensemble adversarial training (Tramer et al., 2017a) for ImageNet. Epp measured at I?UPP = 2
for MNIST, 0.7 for CIFAR10, and 1 for ImageNet.
4https://github.com/tensorflow/models/tree/master/research/adv
imagenet_models
16
Under review as a conference paper at ICLR 2020
1
0.8
M 0.6
M 0.4
0.2
0
0	2	4	6
D
(b) CIFAR10
(c) ImageNet ens4
Figure 8: Operating characteristics of attacks against robust models: adversarial training with PGD
as the reference attack (Madry et al., 2017) for MNIST and CIFAR10, and ensemble adversarial
training (Tramer et al., 2017a) for ImageNet. The number of iterations is 5 x 20 for C&W and 100
for I-FGSM, PGD2, DDN and our BP.
(a) MNIST
C Adversarial image examples
Fig. 9	shows the worst-case ImageNet example for BP along with the adversarial examples generated
by all methods and the corresponding normalized perturbations. FGSM has the highest distortion
over all methods in this example and BP the lowest. DDN has the highest ∞-norm distortion.
Observe that for no method is the perturbation visible, although this is a worst-case example.
17
Under review as a conference paper at ICLR 2020
original image FGSM: '	'	D=6.08 ■播		I-FGSM:	PGD2: D=5.05	D= 3.23 RfRI ■■	:		C&W: D=1.74 ⅛⅜		DDN: 0=2.11 S		BP: 0=1.00 3	
	D=6.08 ■■		D= 4.97	0=3.23 ■■			D=1.84 ■		D=2.02 ■		D=0.86 ■ 上■:	
	I									
	 D=6.07 ⅛s品能 C	D= 5.01	=3 3.24 33 二 ⅛⅛∣F 簟丁IU呼E		0 8	D=2.04 -	I u 	=,« ∣≡BS∣∣S BI。 ρ⅞⅞u			D=1.45 1 I M 		 _ GS ∣≡BS∣≡ —τr⅛3 HlH . 弃清嘲1	D=0.82		一
D=1.75	D=0.82
D=6.09
D= 4.98	D=3.24	D=2.45
Figure 9: Original (left), adversarial (top row) and scaled perturbation (below) images against In-
ceptionV3 on ImageNet. The five images are the worst 5 images for BP requiring the strongest
distortions, yet these are smaller than the distortions necessary with all other methods (The red color
means that the forged image is not adversarial). Perturbations are inverted (low is white; high is
colored, per channel) and scaled in the same way for a fair comparison.
18