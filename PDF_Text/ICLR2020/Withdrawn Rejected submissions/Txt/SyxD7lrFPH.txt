Under review as a conference paper at ICLR 2020
Frequency Pooling:	Shift-equivalent and
Anti-Aliasing Down Sampling
Anonymous authors
Paper under double-blind review
Ab stract
Convolutional layer utilizes the shift-equivalent prior of images which makes it
a great success for image processing. However, commonly used down sampling
methods in convolutional neural networks (CNNs), such as max-pooling, average-
pooling, and stride convolution, are not shift-equivalent. This destroys the shift-
equivalent property of CNNs and degrades their performance. In this paper, we
provide a formal definition of shift-equivalent when down sampling involved.
We propose a strict shift equivalent and anti-aliasing pooling method. This is
achieved by (inverse) Discrete Fourier Transform and we call our method fre-
quency pooling. Experiments on image classifications show that frequency pool-
ing improves accuracy and robustness w.r.t shifts of CNNs.
1 Introduction
Convolutional neural networks (CNNs) have achieved great success on image processing Goodfel-
low et al. (2016), natural language processing Yin et al. (2017), game playing Mnih et al. (2013) and
so on. One of the reasons is that convolutions utilize the shift-equivalent prior of signals. Modern
CNNs include not only convolutional layers but also down sampling or pooling layers. As an im-
portant part of CNNs, pooling layers are used to reduce spatial resolution of feature maps, aggregate
spatial information, and reduce computational and memory cost.
Based on classical Nyquist sampling theorem Nyquist (1928), the sampling rate must be at least
as twice as the highest frequency of a signal. Otherwise, frequency aliasing will appear, i.e. high-
frequencies of the signal alias into low-frequencies. This leads sub-optimal when reconstruction the
signals and misleads the following processing because orthogonal components are mixed again. To
anti-alias, a traditional solution is that applying low-pass filter to the signal before down sampling
it. Following the spirit of blurred down sampling, early CNNs use average pooling to achieve down
sampling Lecun et al. (1989). Later, empirical evidence suggests max-pooling Scherer et al. (2010)
and stride convolutions Long et al. (2015) provide better performance. They are widely used in
CNNs but they don’t consider about anti-aliasing.
Shift-equivalent is another expected property of pooling. Otherwise it will destroy the shift-
equivalent of CNNs and thus the shift-equivalent prior of signals is not fully utilized. Unfortunately,
most commonly used poolings are not shift-equivalent. Worse, small shifts in the input can drasti-
cally change the output when stacking multiple max-pooling or stride convolutions Engstrom et al.
(2017); Azulay & Weiss (2018); Zhang (2019).
Shift-equivalent is believed to be a fundamental property of CNNs. However, the fact that CNNs
with poolings are not shift-equivalent has been ignored by the community. Until recently, Zhang
(2019) propose anti-aliasing pooling (AA-pooling) by low-pass filtering before down sampling.
They observe increased accuracy and better generalization on image classification when low-pass
filtering is integrated correctly. Specifically, they decompose a pooling operator with down sampling
factor s into two parts: a pooling operator with factor 1 and a blur filter with factor s. Although AA-
pooling reduces the aliasing effects and makes the outputs more stable w.r.t input shifts, it is neither
strict shift-equivalent nor anti-aliasing in theory.
In this paper, we propose a strict shift equivalent and anti-aliasing pooling in theory. We first
transform a signal or image into frequency domain via Discrete Fourier Transform (DFT). Then
we only retain its low frequencies, i.e. the frequencies which are smaller than Nyquist sampling
1
Under review as a conference paper at ICLR 2020
(a) Max-pooling
(b) Avg-pooling
(c) F-pooling
Figure 1: Simple tests of shift-equivalent. Blue lines are obtained by pooling, up sampling, and
shift in order. Orange lines are obtained by shift, pooling, and up sampling in order. All Pooling
operations down sample original signals by a factor 4. The shift operation shifts original signals by
2 pixels. The up sampling operation is set to equation 8. Best viewed on screen.
rate. Finally, we transform the low frequencies back into time domain via inverse DFT. We call our
method frequency pooling (F-pooling). Note that a similar method is proposed in Ryu et al. (2018).
However, they only focus on classification accuracy. Shift equivalent of their method is not evaluated
in both theory and practice. Compared with previous works, the novelties and contributions of F-
pooling are summarized as follows:
•	We formally define shift-equivalent of functions which contain down sampling operations.
A suitable up sampling operation U must be involved in the definition. Without it, the
definition for discrete signal is ill-posed. We believe a formal mathematical treatment has
great value for further research.
•	We prove that F-pooling is the optimal anti-aliasing down sampling operation from the
perspective of reconstruction. We also prove that F-pooling is shift equivalent. The up
sampling operation U plays an important rule in our proofs. To best of our knowledge,
F-pooling is the first pooling method which has those properties.
•	Experiments on CIFAR-100 and a subset of ImageNet demonstrate that F-pooling remark-
ably increases accuracy and robustness w.r.t shifts of commonly-used network architec-
tures. Moreover, the shift consistency of F-pooling is improved more when we replace
zero padding of convolutions with circular padding.
2	Method
In this section, we first define shift-equivalent for CNNs formally. Then we describe F-pooling in
detail and prove its properties. Finally we discuss our implementation and some practical issues.
2.1	Definition of shift-equivalent
Denote X as an h0 × w0 × c tensor where h0 , w0 , and c are the height, width, and number of
channels of X respectively. Denote Y as an h1 × w1 × c tensor. We suppose h0 > h1 and w0 > w1.
M : X → Y is a pooling operation and U : Y → X is a up sampling operation. We say M is
shift-equivalent if and only if
S hif t4h,4w (U MX) =U MS hif t4h,4w (X)	(1)
∀(4h, 4w), ∃U
That is, if there is a suitable up sampling operation U which makes UM and S hif t4h,4w com-
mutable, then M is shift-equivalent. The shift operation is required to be circular or periodic. When
a shifted element hits the edge, it rolls to other side.
Shift4h,4w (Xa,b) = X(a+4h)%h0,(b+4w)%w0	(2)
where % is the modulus function. Our definition of shift-equivalent is similar with the one in Zhang
(2019). The difference is that we introduce an up sampling U to make this definition strict. Without
2
Under review as a conference paper at ICLR 2020
Figure 2: An illustration of the forward process of F-pooling. We assume the lowest frequencies are
at center.
introducing U, the definition of shift-equivalent for pooling is ill-posed. Suppose M down samples
signals by a factor s. Without introducing U , one may define shift-equivalent as follows:
Shift4h∕s,4w∕s ∙ M(X) = M ∙ Shift4h,4w(X),	∀(4h, 4w)	(3)
However, Shif t4h/s,4w/s is not operable for discrete signals when 4h%s 6= 0 or 4h%s 6= 0.
To make the shifted element which is not on lattice operable, one must interpolate or up sample the
down sampled signals. This leads to the definition in equation 1.
As in equation 1, to make M shift-equivalent, one must find the corresponding up sampling opera-
tion U . For a given M, the first line in equation 1 may hold for some up sampling operations. But
it may not hold for other up sampling operations.
2.2	F-pooling
The basic idea of F-pooling is removing the high frequencies of signals and reconstructing the sig-
nals only using the low frequencies. In this paper, high frequencies mean the frequencies which are
beyond Nyquist frequency, i.e. half of signal resolution. And low frequencies mean the frequencies
which are not higher than Nyquist frequency. To remove high frequencies, we first transform signals
into frequency domain via DFT. Then only the low frequencies are retained. Finally, we transform
the low frequencies back to time domain via inverse DFT (IDFT). Figure 2 gives an illustration of
the forward process of F-pooling.
Since 2D (inverse) DFT can be decomposed into two 1D (inverse) DFT, we provide the formal
representation of F-pooling in 1D case. Denote x ∈ RN as time domain signal and y ∈ CM as
frequency domain signal. Without loss of generality, we suppose M is even. FN ∈ CN ×N is the
so-called DFT-matrix:
FN
,0∙0
N
,ι∙0
N
1,0∙1
ωN
,,1∙1
ωN
0∙(N-1)
N
1∙(N-1)
N
(4)
(N-1)∙0	(N-1)∙1
ωN	ωN
(N-1)∙(N-1)
ωN
ω
ω
ω
ω
where ωN = e-2ni/N. FN is the inverse DFT-matrix. By definition, NNFNFN = I where I is
identity matrix. We denote Tμ as a operation which selects the first μ rows and the last μ rows of a
matrix.
Tμ(x) = [Xi：*； XN-μ+1:N]	(5)
That is we select the basis of frequencies ranged in [-μ, μ), due to periodicity of DFT. When ap-
plying Tμ to a signal, we obtain its lowest μ frequencies. Then the function of F-pooling for 1D
signals is represented as:
y =-1 FMT M FN X d=f Px
N2
where P ∈ CM×N is the transform matrix of F-pooling.
(6)
Now we formalize F-pooling for CNNs. Recall that X is an h0 ×w0 ×c tensor and Y is an h1 ×w1 ×c
tensor. We apply F-pooling to each channel of X.
Y：,：,i = PhX：,：,iPW,	i ∈ [1,c]
(7)
3
Under review as a conference paper at ICLR 2020
where Ph ∈ Ch1×h0 and Pw ∈ Cw1×w0 are the transform matrices of F-pooling along vertical
direction and horizontal direction respectively. Since F-pooling can be represented as two times
matrix-matrix multiplications, its back propagation rule is easily derived.
As mentioned in section 2.1, the choice of up sampling operation U is important. In this paper, U is
set to the inverse process of F-pooling. Specifically, we transform a signal into frequency domain.
Then we zero pad the transformed signal to match the resolution of output. Finally, we transform
it back to time domain. This process can also be represented by matrix multiplications. For 1D
signals, we have:
X = M FNZ M FMy	⑻
where is Z is the zero padding operation.
Zμ = [Xi：*； 0； XM-μ+1:M]	(9)
The formulations of F-pooling and U are easily extended to 2D case.
2.3	Optimal anti-aliasing down sampling
In this section, we prove that F-pooling is the optimal anti-aliasing down sampling operation from
the perspective of reconstruction given U . We focus on 1D case here. Given U , the optimal anti-
aliasing down sampling is obtained by solving the following problem:
min ||U MX - X||22, s.t. M ∈ A	(10)
M
where A is a set of all possible anti-aliasing down sampling operations. That is, we hope to find an
anti-aliasing down sampling which minimizes the reconstruction error. Based on Nyquist sampling
theory, M must remove high frequencies of signals and this holds for F-pooling. We focus on the
optimality of F-pooling. We decompose X into low frequencies part Xl and high frequencies part
Xh, i.e. xι = ~NFND M FNX and Xh = X - xι. D* is a diagonal matrix whose the first μ and the
last μ diagonal elements equal to 1 while others equal to 0.
||U MX - X||22 = ||UMX - Xι - Xh||22	(11)
=||UMx - Xι∣∣2 + ∣∣Xh||2 + hUMX, xh - g Xh〉	(12)
=∣∣UMχ - xi||2 + ∣∣Xh||2	(13)
equation 13 holds because the third term and the forth term of equation 12 equal to 0. M removes
high frequencies and U doesn’t introduce new frequencies. Thus UMX only contains low frequen-
cies. Due to the orthogonality of frequency spectrum, hU MX, Xhi = 0. Similarly, hXι, Xhi = 0.
When M is F-pooling, we have
UMX =	FNZ m FMFMTm FNX	(14)
MN 2	2
=NFN (Z MM TMM )FNX	(15)
=NFNDMFNX d=f Xl	(16)
equation 16 holds due to the definition of operations T, Z, and D. See appendix A for proof. Since
the fist term of equation 13 equals to 0, equation 10 is minimized. Thus, we have proved that F-
pooling is the optimal anti-aliasing down sampling operation from the perspective of reconstruction.
We choose reconstruction optimality because: 1) we believe reconstruction optimality relates to the
final performance, e.g. classification optimality for image classification. Ifwe accept that the feature
extracted by previous convolution layers is useful, then it is best to keep it as much as possible for
the current pooling layer. Prior works have shown that using self reconstruction loss as an auxiliary
is helpful for classification Rasmus et al. (2015); 2) It is difficult to directly define classification
optimality for an intermediate layer.
4
Under review as a conference paper at ICLR 2020
Baseline
MaxPooling
Stride S
Conv
Stride S
AvgPooling
Stride S
Max Pooling
Strided Conv
Average Pooling
Figure 3: An illustration of how to replace max-pooling, average pooling, and stride convolution
with F-pooling. We follow the settings in Zhang (2019).
2.4	Shift-equivalent
Based on shift theorem of Fourier transform, we have
FNShift4h(x) =FNx S4h	(17)
Shift4h (FNX)=FNX Θ S4h	(18)
where S4h ∈ CN whose kth element is e-2Nk 4h and Θ is element-wise multiplication. Combining
with equation 16, we have
IUMShift4h(x) = ɪ FND M FN Shift 4h(x)	(19)
N2
=NFN (D M FNx) Θ S4h	(20)
=Shift4h (NFNDM FNX)	QI)
= Shift4h (UMX)	(22)
We have proved that F-pooling is shift-equivalent. Note that equation 22 is not hold if other up
sampling operations are used, such as linear interpolation.
The proofs in section 2.3 and 2.4 can be easily extended for 2D signals.
2.5	Transitivity of shift-equivalent
We study the transitivity of shift-equivalent for F-pooling. Denote f as a shift-equivalent function
without down sampling or up sampling. It is straightforward to shown
UMShift4h(f(X)) = Shift4h(UMf(X))	(23)
That is shift-equivalent of F-pooling is transitive for any shift-equivalent function f without down
sampling or up sampling. And it is transitive for F-pooling itself. For example, a stack of two
F-pooling layers is still a shift-equivalent function.
Unfortunately, this transitivity of F-pooling is not hold for a shift-equivalent function with down
sampling. For example, a function of F pooling → conv → relu → F pooling is not shift-
equivalent. This prevents us to obtain strict shift-equivalent deep CNNs.
2.6	Practical issues
Dealing with imaginary part: generally, the output of F-pooling, i.e. MX contains both real part
and imaginary part. However, for commonly used CNNs, the feature maps must be real. Thus one
must ignore the imaginary part of F-pooling. On the other hand, MX is treated as complex in the
proofs in section 2.3 and 2.4. If we ignore the imaginary part, F-pooling is no longer the optimal
anti-aliasing down sampling (but still anti-aliasing) and F-pooling is no longer strict shift-equivalent.
Fortunately, this issue is easy to overcome. When the resolution of down sampled signals is odd, the
imaginary part of Mx is zero. Suppose the resolution is 2μ + 1.
^-μ,..., X-1, X0, Xl ..., Xμ	(24)
5
Under review as a conference paper at ICLR 2020
Table 1: Accuracy and consistency on SUb-ImageNet
accuracy/consistency	densenet-121	resnet-50	mobilenet-v2
Origin	77.47/82.74=	74.44/80.17	73.01/78.08 =
AA-pooling	77.14/83.09	76.12/82.26	74.03/79.12
F-pooling	77.56/84.10	76.05/82.63	74.72/80.34
Due to its symmetry, it only contains real part when transforms back into time domain. If the
resolution is μ + 2
x-μ, . . . , x-1, x0, x1 . . . , xμ, xμ+1	(25)
In this case, it contains imaginary part. But we can recover the symmetry and eliminate the imag-
inary part by setting ^μ+ι to zero. We call this trick odd padding. We show the effects of odd
padding in appendix B. A drawback of odd padding is that more information is lost during down
sampling which may reduces accuracy. We don’t use it in this paper.
Compared with equation 24 and equation 25, the imaginary part is introduced by Xμ+ι. Thus, the
error of ignoring the imaginary part is smaller than ∣∣Xμ+ι ||2. In practice, We find this effect is small.
Dealing with zero padding: F-pooling is designed to be circular shift equivalence. However, zero
padding is commonly used in convolution layers. Convolutional layers with zero padding destroy
circular shift-equivalence. Thus, one should expect that using circular padding in convolutional
layers will greatly increase the shift consistency of F-pooling. In this paper, we evaluate the per-
formance of F-pooling with both zero-padding and circular padding. We suggest one replaces zero
padding with circular padding for better shift-equivalence.
2.7	Implementation
We implement F-pooling in PyTorch Paszke et al. (2017). Theoretically, it is best to implement
F-pooling with fast Fourier transform (FFT) and inverse FFT. Suppose F-pooling down samples
an h0 × w0 × c tensor to a h1 × w1 × c tensor. Then its time complexity is ch0w0 log(w0h0)).
Unfortunately, we find such a implementation in PyTorch is comparatively slow.
Instead, we implement F-pooling via 1 × 1 convolutions. As in equation 7, 2D F-pooling can
be represented as two times matrix-matrix multiplications along vertical direction and horizontal
direction respectively. This is equivalent to 1 × 1 convolutions along vertical direction and direction
horizontal. Its time complexity is O(h0w0 (h1 + w1)) which is higher than the optimal complexity.
F-pooling requires much more computational costs than average pooling or max-pooling. F-pooling
is faster than a convolutional layer when the resolution of feature maps is smaller than the number
of channels, as the situations of image classifications. Moreover, the number of pooling layers is
limited compared with the number of convolutional layers. Thus, introducing F-pooling will not
introduce too many computational costs into CNNs.
Zhang (2019) claims that it is important to integrate anti-aliasing pooling into CNNs in a correct
way. In this paper, we follow their settings. Specifically, a max-pooling with stride s is replaced
with a max-pooling with stride 1 and an F-pooling with stride s. A convolution with stride s is
replaced with a convolution with stride 1 and an F-pooling with stride s. An average pooling with
stride s is replaced with an F-pooling with stride s. See the illustration in figure 3.
3	Experiments
3.1	1D signals
We test the shift-equivalent of F-pooling on 1D signals. In figure 1, the original signal is a randomly
selected row of a 512 × 512 image. We apply max-pooling, average pooling, and F-pooling with
stride 4 to those signals. As shown in figure 1, F-pooling is perfectly shift-equivalent. And average
pooling is better than max-pooling from shift-equivalent perspective. odd padding is used here.
6
Under review as a conference paper at ICLR 2020
Table 2: Accuracy and consistency on CIFAR-100
accuracy/COnSiStenCy	shift argument	densenet-41	resnet-18	resnet-34
Origin	with	74.90/71.55	75.52/70.21	76.56/72.21 ：
AA-pooling	with	75.55/71.71	77.43/73.08	76.95/73.38
F-pooling	with	75.45/71.91	77.36/73.43	77.68/73.54
Origin	without	71.81/57.27	67.60/45.00	68.11/46.56 ：
AA-pooling	without	73.81/60.29	74.49/58.24	74.00/57.72
F-pooling	without	73.19/60.51	73.93/57.48	73.51/56.54
3.2	Image classification
CIFAR-1001: we test classification of low-resolution 32 × 32 color images. This dataset contains
50k images for training and 10k images for test. Images are classified into one of 100 categories.
sub-ImageNet: we then test classification of high-resolution color images. Original ImageNet
dataset Russakovsky et al. (2015) contains 1.28M training and 50k validation images, classified into
one of 1,000 categories. To reduce the computational resources for training, we use a subset of
ImageNet (sub-ImageNet) in this work. We randomly select 200 categories from 1,000 categories.
For each category, we randomly select 500 images. Thus, we collect 100k images for training.
And we select corresponding 10k validation images. All models are trained on a single GPU with
batchsize 64 and 100 epochs. We decrease the initial learning rate by a factor 10 every 40 epochs.
For other hyper-parameters, we follow PyTorch’s official training script.2
We train CIFAR-100 using resnet He et al. (2016) and densenet Huang et al. (2017). We train sub-
ImageNet using resnet, densenet, and mobilenet-v2 Sandler et al. (2018). Those models are widely
used as benchmarks. Max-pooling, average pooling, and stride convolution are covered in those
models. We also compare F-pooling with AA-pooling Zhang (2019). For a model trained on sub-
ImageNet, its first down sampling operation is kept to reduce computational costs. This setting is
also used in Zhang (2019). The results on CIFAR-100 are averaged by 3 runs.
We study not only accuracy but also consistency. We follow the metric of consistency used in Zhang
(2019): we check how often the network outputs the same classification, given the same image with
two different shifts.
Eh0,h1,w0,w11 {arg maxp(Shifth0,w0) = argmaxp(Shifth1,w1)}	(26)
We only evaluate diagonal shifts in this paper. For CIFAR-100, the number of shifted pixels ranges
from -7 to 7. For sub-ImageNet, it ranges from -63 to 63.
Zero padding: we keep the padding method of convolutions as zero padding. We measure accuracy
and consistency of different pooling methods. Results on sub-ImageNet is shown in table 1 and
results on CIFAR-100 are shown in table 2 respectively.
Circular padding: as mentioned earlier, circular padding may improve the consistency of F-
pooling. Thus we replace the zero padding of convolutions with circular padding. To further evaluate
the robustness w.r.t shifts, we use standard deviation (std) of probabilities of the correct label. Std
and consistency on sub-ImageNet an CIFAR-100 are shown in table 3 and table 4 respectively.
Based on those results, we conclude that
•	F-pooling is significantly and consistently better than original pooling methods in terms of
accuracy and shift consistency.
•	F-pooling is comparable with AA-pooling in term of accuracy and shift consistency with
zero padding. With circular padding, F-pooling is significantly better than AA-pooling.
•	Shift consistency of F-pooling is improved more when replacing zero padding with circular
padding, especially on CIFAR-100.
1https://www.cs.toronto.edu/ kriz/cifar.html
2https://github.com/pytorch/examples/tree/master/imagenet
7
Under review as a conference paper at ICLR 2020
Table 3: Std and consistency on SUb-ImageNet with circular padding.
std/consistency	densenet-121	resnet-50	mobilenet-v2
Origin	0.043/90.44 =	0.051/87.97	0.051/87.71
AA-pooling	0.055/88.21	0.056/87.77	0.059/86.19
F-pooling	0.035/91.88	0.037/91.01	0.041/90.32
Table 4: Std and consistency on CIFAR-100 with circular padding without shift argument.
std/consistency	densenet-41	resnet-18	resnet-34
Origin	0.111/77.62	0.176/59.93	0.183/58.94 ：
AA-pooling	0.166/66.97	0.164/65.67	0.180/60.53
F-pooling	0.088/82.99	0.073/84.34	0.088/80.90
4	Related Works
Pooling which reduces the resolution of feature maps is an important part of CNNs. Early CNNs
Lecun et al. (1989) use average pooling which is good for anti-aliasing. Later empirical evidence
suggests max-pooling Scherer et al. (2010) and strided-convolutions Long et al. (2015) provide bet-
ter performance. However, small shifts in the input can drastically change the output when stacking
multiple max-pooling or strided-convolutions Engstrom et al. (2017); Azulay & Weiss (2018). Other
variants such as Graham (2014); He et al. (2015); Lee et al. (2016) (we just list a few of them), focus
on extending the functionality of pooling Lee et al. (2016) or making pooling adjusted to variable
input size Graham (2014); He et al. (2015).
Recently, Zhang (2019) shows that CNNs will have better shift-equivalent and anti-aliasing prop-
erties when low-pass filtering is integrated correctly. However, their method is not strict shift-
equivalent and anti-aliasing in theory. Williams & Li (2018) propose Wavelet-pooling. They decom-
pose a signal via wavelet transform and retain the lowest sub-band. This process is repeated until
the designed down sampling factor is met. The spirit of Wavelet-pooling is similar to F-pooling.
However, they claim that Wavelet-pooling is better than others because it is a global transform in-
stead of a local transform. They neither focus on shift-equivalent nor prove Wavelet-pooling is
shift-equivalent or not.
F-pooling is a complex transformation because DFT and IDFT are involved in it. Many works in-
tegrate complex transformations or complex values into neural networks. Amin & Murase (2009)
study single-layered complex-valued neural networks for real-valued classification problems. Com-
plex numbers represented in polar coordinates are more suitable to deal with rotations. Based on
this, Cohen et al. (2018) propose spherical CNNs which are rotation-equivalent to deal with signals
projected from spherical surface. Trabelsi et al. (2018) propose general deep complex CNNs. They
adjust batch normalization and non-linear activations for complex CNNs. F-pooling can used in
their method without worry about the imaginary part. F-pooling utilizes shift theorem of DFT and
achieves shift-equivalent. This is anew success for the combination of complex transformations and
neural networks.
5	Conclusions
In this paper, we have proposed frequency pooling (F-pooling) for CNNs. F-pooling reduces the
dimension of signals in frequency domain. We have defined shift-equivalent of functions which
contain down sampling operations by introducing an up sampling operation. Under this definition,
we have proved that F-pooling is the optimal anti-aliasing down sampling operation and F-pooling
is shift-equivalent. We have integrated F-pooling into modern CNNs. We have verified that F-
pooling remarkably increases accuracy and robustness w.r.t shifts of modern CNNs. We believe that
F-pooling plays a more important role in applications where shift-equivalent is more serious, such
as object detection and semantic segmentation.
8
Under review as a conference paper at ICLR 2020
References
Md Faijul Amin and Kazuyuki Murase. Single-layered complex-valued neural network for real-
valued classification problems. NeurocomPuting, 72(4-6):945-955, 2009.
Aharon Azulay and Yair Weiss. Why do deep convolutional networks generalize so poorly to small
image transformations? arXiv PrePrint arXiv:1805.12177, 2018.
Taco S. Cohen, Mario Geiger, Jonas Khler, and Max Welling. Spherical CNNs. In International
Conference on Learning RePresentations, 2018. URL https://openreview.net/forum?
id=Hkbd5xZRb.
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A
rotation and a translation suffice: Fooling cnns with simple transformations. arXiv PrePrint
arXiv:1712.02779, 2017.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. DeeP learning, volume 1.
MIT Press, 2016.
Benjamin Graham. Fractional max-pooling. arXiv PrePrint arXiv:1412.6071, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid pooling in deep con-
volutional networks for visual recognition. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 37(9):1904-1916, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on comPuter vision and Pattern recognition, pp.
770-778, 2016.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on comPuter vision and Pattern
recognition, pp. 4700-4708, 2017.
Yann Lecun, Bernhard E Boser, John S Denker, D Henderson, R E Howard, W Hubbard, and L D
Jackel. Handwritten digit recognition with a back-propagation network. pp. 396-404, 1989.
Chenyu Lee, Patrick W Gallagher, and Zhuowen Tu. Generalizing pooling functions in convolu-
tional neural networks: Mixed, gated, and tree. pp. 464-472, 2016.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic
segmentation. pp. 3431-3440, 2015.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. ComPuter Science,
2013.
Harry Nyquist. Certain topics in telegraph transmission theory. Transactions of The American
Institute of Electrical Engineers, 47(2):617-644, 1928.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
Antti Rasmus, Harri Valpola, Mikko Honkala, Mathias Berglund, and Tapani Raiko. Semi-
supervised learning with ladder networks. pp. 3546-3554, 2015.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of ComPuter Vision
(IJCV), 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
Jongbin Ryu, Minghsuan Yang, and Jongwoo Lim. Dft-based transformation invariant pooling layer
for visual classification. pp. 89-104, 2018.
9
Under review as a conference paper at ICLR 2020
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 4510-4520, 2018.
Dominik Scherer, Andreas Muller, and Sven Behnke. Evaluation of pooling operations in convolu-
tional architectures for object recognition. pp. 92-101, 2010.
Chiheb Trabelsi, Olexa Bilaniuk, Ying Zhang, Dmitriy Serdyuk, Sandeep Subramanian, Joao Fe-
lipe Santos, Soroush Mehri, Negar Rostamzadeh, Yoshua Bengio, and Christopher J Pal. Deep
complex networks. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=H1T2hmZAb.
Travis Williams and Robert Li. Wavelet pooling for convolutional neural networks. In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=rkhlb8lCZ.
Wenpeng Yin, Katharina Kann, Mo Yu, and Hinrich Schutze. Comparative study of cnn and rnn for
natural language processing. arXiv: Computation and Language, 2017.
Richard Zhang. Making convolutional networks shift-invariant again. arXiv preprint
arXiv:1904.11486, 2019.
A Proof of operation
We prove D M = Z M T M .By definition, T can be represented as an M X N matrix and Z can be
represented as a N X M matrix.
M_2 m_2
0I
m2 m2
00
MM
--
n-2 n-2
00
MM
--
N一2 N一2
00
m2 m2
00
m2 m_2
I0
=
m-2
T
27
where IM is an M X M identity matrix and 0m is an M X M zero matrix. Z M is equal to the
transpose of T M by definition. Then
I M
2
Z M T M =	∙
2	2
0N-M
(28)
I M
2 -
which is the same as D M by definition.
B EFFECT OF odd padding
C Loss curves on sub-ImageNet
10
Under review as a conference paper at ICLR 2020
(a) with odd padding
Figure 4: With odd padding, F-pooling is strict shift-equivalent. Without it, F-pooling is not strict
shift-equivalent. But the error is acceptable. Best viewed on screen.
(b) without odd padding
11
Under review as a conference paper at ICLR 2020
(a) densenet-train
(b) densenet-test
Epoch
(c) resnet-train
---Origin
AA-pooliπg
——F-pooliπg
Ado-uə ss。」》6~u∙;
20	30	40	50	60	70	80	90	100
Epoch
(d) resnet-test
20	30	40	50	60	70	80	90	100	20	30	40	50	60	70	80	90	100
Epoch	Epoch
(e) mobile-train	(f) mobile-test
Figure 5: Loss curves on sub-ImageNet. Best viewed on screen.
12