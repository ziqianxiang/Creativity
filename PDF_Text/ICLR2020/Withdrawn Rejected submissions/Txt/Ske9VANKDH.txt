Under review as a conference paper at ICLR 2020
An Optimization Principle of Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
Training deep neural networks (DNNs) has achieved great success in recent years.
Modern DNN trainings utilize various types of training techniques that are devel-
oped in different aspects, e.g., activation functions for neurons, batch normalization
for hidden layers, skip connections for network architecture and stochastic algo-
rithms for optimization. Despite the effectiveness of these techniques, it is still
mysterious how they help accelerate DNN trainings in practice. In this paper,
we propose an optimization principle that is parameterized by γ > 0 for stochas-
tic algorithms in nonconvex and over-parameterized optimization. The principle
guarantees the convergence of stochastic algorithms to a global minimum with
a monotonically diminishing parameter distance to the minimizer and leads to
a O(1∕γK) sub-linear convergence rate, where K is the number of iterations.
Through extensive experiments, we show that DNN trainings consistently obey the
γ-optimization principle and its theoretical implications. In particular, we observe
that the trainings that apply the training techniques achieve accelerated convergence
and obey the principle with a large Y, which is consistent with the O(1∕γK) con-
vergence rate result under the optimization principle. We think the γ-optimization
principle captures and quantifies the impacts of various DNN training techniques
and can be of independent interest from a theoretical perspective.
1	Introduction
Deep learning has been successfully applied to various domains such as computer vision, natural
language processing, etc, and has achieved state-of-art performance in solving challenging tasks.
Although deep neural networks (DNNs) have been well-known for decades to have great expressive
power Cybenko (1989), the empirical success of training DNNs postponed to recent years when
sufficient computation power is accessible and effective DNN training techniques are developed.
The milestone developments of DNN training techniques can be divided into two categories. First,
various techniques have been developed at different levels of neural network design. At the neuron
level, various functions have been applied to activate the neurons, e.g., sigmoid function, hyperbolic
tangent (tanh) function and the more popular rectified linear unit (ReLU) function Nair & Hinton
(2010). At the layer level, batch normalization (BN) has been widely applied to the hidden layers
of DNNs to stabilize the training Ioffe & Szegedy (2015). Moreover, at the architecture level, skip
connections have been introduced to enable successful training of deep networks He et al. (2015);
Szegedy et al. (2015); Srivastava et al. (2015); Huang et al. (2016). Second, various efficient stochastic
optimization algorithms have been developed for DNN training, e.g., stochastic gradient descent
(SGD) Robbins & Monro (1951); Rumelhart et al. (1986), SGD with momentum Qian (1999);
Nesterov (2014) and Adam Kingma & Ba (2015), etc. Table 1 provides a summary of these important
DNN training techniques.
Table 1: Summary of DNN training techniques
Neuron activation	Layer normalization	Network architecture	Optimization algorithm
sigmoid, tanh, ReLU	batch normalization	skip connection	SGD, SGD-momemtum, Adam
1
Under review as a conference paper at ICLR 2020
While these training techniques have been widely applied in practical DNN training, there is limited
understanding on how they help facilitate the training and achieve the global minimum of the network.
In the existing literature, it is known that the sigmoid and tanh activation functions can cause the
vanishing gradient problem, and the ReLU activation function is a popular replacement that avoids
this issue Nair & Hinton (2010); Glorot et al. (2011). On the other hand, the batch normalization
is originally proposed to reduce the internal covariance shift Ioffe & Szegedy (2015), and more
recent studies show that it allows to use a large learning rate Bjorck et al. (2018) and improves
the loss landscape Santurkar et al. (2018). The skip connection has been shown to help eliminate
singularities and degeneracies Orhan & Pitkow (2018) and improve the loss landscape Hardt & Ma
(2016). Moreover, regarding the optimization algorithm, the momentum scheme has been well-known
to accelerate convex optimization Nesterov (2014) and is also widely applied to accelerate nonconvex
optimization Ghadimi & Lan (2016), whereas the Adam algorithm normalizes the update in each
dimension to accelerate deep learning Kingma & Ba (2015). While these studies provide partial
explanations to the effectiveness of various DNN training techniques, their reasonings are from
very different perspectives and it is unclear whether a general principle can exist that guides these
training techniques to facilitate the training process. In particular, these studies do not explain why
DNN trainings with different techniques can achieve the global minimum in practice. Moreover,
the existing explanations cannot quantify the impacts of the training techniques on training deep
networks, and a principled quantification metric is still far from clear.
This paper attempts to propose an optimization principle that characterizes and quantifies the impacts
of various training techniques on DNN training. The proposed principle is applicable to general
stochastic algorithms in the nonconvex and over-parameterized regime and leads to guaranteed
convergence to a global minimum. We summarize our contributions as follows.
1.1	Our Contributions
Theory: We propose an optimization principle that is parameterized by γ > 0 for stochastic
algorithms in nonconvex and over-parameterized optimization. The γ-optimization principle (see
Definition 1) requires the current update points toward a global minimizer, and the inner product
between both directions is lower-bounded by the update norm and the loss gap that is scaled by the
factor γ . We show that the γ-optimization principle guarantees the optimization path generated by
stochastic algorithms to approach a global minimizer with a monotonically diminishing distance.
Moreover, it leads to convergence of the optimization path to the global minimum with a O(1∕γK)
sub-linear convergence rate .
Experiments: We conduct extensive experiments to examine the validity of the γ-optimization
principle in DNN training. In specific, we find that all the tested DNN trainings obey the γ-
optimization principle throughout the training process, and the generated optimization paths satisfy
the theoretical properties of the principle that are mentioned above. Moreover, we explore the impacts
of the training techniques that are listed in Table 1 on the parameterization γ of the optimization
principle. We find that DNN trainings that apply the training techniques achieve fast convergence and
obey the optimization principle with a large γ . As a comparison, DNN trainings without the training
techniques converge slowly and obey the optimization principle with a small γ . These observations
are consistent with the convergence rate characterization of the γ-optimization principle that a larger
γ leads to faster convergence. Therefore, the γ-optimization principle captures and quantifies the
impacts of these training techniques on DNN training in a unified way through the parameter γ , and
sheds light on the underlying mechanism that leads to successful DNN training.
1.2	Related Work
DNN training techniques: Various training techniques have been developed for DNN training.
Examples include piece-wise linear activation functions, e.g., ReLU Nair & Hinton (2010), ELU
Clevert et al. (2015), leaky ReLU Maas et al. (2013), batch normalization Ioffe & Szegedy (2015),
skip connection He et al. (2015); Szegedy et al. (2015); Srivastava et al. (2015) and advanced
optimizers such as SGD with momentum Qian (1999), Adagrad Duchi et al. (2011), Adam Kingma
& Ba (2015), AMSgrad Reddi et al. (2018). The ReLU activation function and skip connection have
been shown to help avoid the vanishing gradient problem and improve the loss landscape Hardt &
Ma (2016); Zhou & Liang (2017); Zhang et al. (2019); Zou et al. (2018). The batch normalization
has been shown to help avoid the internal covariance shift problem Ioffe & Szegedy (2015). More
recent studies show that batch normalization allows to adopt a large learning rate Bjorck et al. (2018)
2
Under review as a conference paper at ICLR 2020
and improves the loss landscape in the training Santurkar et al. (2018). The convergence properties
of the advanced optimizers have been studied in nonconvex optimization Chen et al. (2019).
Optimization properties of nonconvex ML: Many nonconvex ML models have amenable properties
for accelerating the optimization. For example, nonconvex problems such as phase retrieval Zhang
et al. (2017b), low-rank matrix recovery Tu et al. (2016), blind deconvolution Li et al. (2018b) and
neural network sensing Zhong et al. (2017) satisfy the local regularity geometry around the global
minimum Zhou et al. (2016); Tu et al. (2016); Li et al. (2018b); Zhong et al. (2017); Zhou & Liang
(2017), which guarantees the linear convergence of gradient-based algorithms. More recently, DNN
trainings that use SGD have been shown to follow a star-convex optimization path Zhou et al. (2019).
2	An Optimization Principle for Over-parameterized ML
In this section, we introduce the γ-optimization principle that provides convergence guarantees
for over-parameterized nonconvex machine learning (ML). We show empirically in the subsequent
sections that practical DNN trainings obey this principle.
2.1	Global minimum of over-parameterized models
The goal of a machine learning task is to search for a good ML model θ that minimizes the total loss
f on a set of training data samples Z : {zi}in=1. The problem is formally written as
1n
min f(θ; Z) := — E'(θ; Zi),	(P)
θ∈Rd	n
i=1
where '(∙; Zi) : Rd → R corresponds to the loss on the i-th data sample zi. For many nonconvex ML
models, e.g., deep neural networks, a prominent feature is the over-parameterization of the model,
i.e., the model capacity is sufficient to over-fit all the training data samples. In another word, the
global minimizers of the total loss f under an over-parameterized model are common minimizers of
all the individual loss functions '(∙; zj, i = 1,..., n. We summarize this fact formally as follows.
Fact 1. Consider the problem (P) with an over-parameterized model θ. Every global minimizer θ* of
the total loss f is also a global minimizer ofeach individual loss '(∙; Zi) for all i = 1,..., n.
To elaborate, note that we typically use non-negative loss in DNN training. Therefore, if the total
loss f is trained to achieve zero (i.e., the global minimum), then each individual loss '(∙; Zi) must
also achieve zero and the obtained model over-fits the training data. Of course, in practice, due to
the termination of the optimization process within finite number of iterations, the total loss can only
achieve an approximate global minimum that is very close to zero. Throughout the rest of this section,
we assume that Fact 1 holds for the problem (P).
2.2	THE γ-OPTIMIZATION PRINCIPLE
Consider a generic stochastic algorithm (SA) that is initialized with certain model θ0. In each iteration
k, the SA samples a data sample Zξk ∈ Z, where ξk ∈ {1, ..., n} is obtained via cyclic sampling with
reshuffle. Based on the current model θk and the sampled data Zξk , the SA generates a stochastic
update U(θk; Zξk) and applies it to update the model with a learning rate η > 0 according to
(SA) :	θk+1 = θk - ηU(θk; Zξk),	k = 0, 1, 2, ...	(1)
Equation (1) covers the update rule of many existing optimizers for DNN training. For example,
the stochastic gradient descent (SGD) algorithm chooses the update U(θk; Zξk) to be the stochastic
gradient Vθ'(θk; Zξk). In comparison, the SGD with momentum algorithm generates the update
using an extra momentum step, and the Adam algorithm generates the update as a moving average of
the stochastic gradients normalized by the moving average of their second moments 1.
We next introduce the γ-optimization principle for the SA.
Definition 1 (γ -optimization principle for SA). Apply SA to solve the over-parameterized problem
(P). For a certain global minimizer θ* of the problem (P), the optimization path {θk }k generated by
the SA satisfies: for certain γ > 0 and all k = 0, 1, 2, ...,,
hθk-θ,U(θk； Zξk)i ≥ 2kU(θk； Zξk)k2 + Ykθ0-θ*k2('(θk； Zξk)-'(θ*; Zξk)).	(2)
1The Adam updates depend on the past stochastic samples.
3
Under review as a conference paper at ICLR 2020
To elaborate, the left hand side of eq. (2) measures the coherence between θk - θ* and the current
model update U(θk; Zξk ). Intuitively, a positive coherence implies that the current model update
points toward the global minimizer θ* from the current model θk. On the other hand, the right hand
side of eq. (2) regularizes the coherence by two non-negative terms: the square norm of the model
update scaled by the learning rate η and the optimality gap of the loss on the sampled data scaled
by the constant γ∣θo 一 θ*∣2. Hence, a larger value of Y implies that the update U(θk; Zξk) is more
coherent with the direction towards the minimizer θk - θ* and therefore facilitates the convergence.
We show in Theorem 1 later that the Y plays a central role in characterizing the convergence rate of
the SA.
Discussion: The γ-optimization principle is related to other existing ones in nonconvex ML. In
specific, nonconvex problems such as phase retrieval Zhang et al. (2017b) and low-rank matrix
recovery Tu et al. (2016) have been shown to satisfy the regularity condition, which replaces the last
term in eq. (2) by Y∣∣θk 一 θ*∣2 and guarantees the linear convergence of {θk}k to θ*. However, such
a fast convergence rate does not hold in practical DNN training. On the other hand, DNN trainings
have been shown in Zhou et al. (2019) to follow a star-convex optimization path, which corresponds
to eq. (2) with Y = ∣∣θo ― θ[∣-2,U ®; Zξk) = V'(θk; z±Q and absence of the term η ∣∣U ®; Zξk )∣2.
However, such a principle is applicable to SGD updates only and an additional global Lipschitz
assumption on the loss function is required in order to have theoretical convergence guarantee. In
comparison, the Y-optimization principle is applicable to the more general SA and does not rely on
additional assumption to have convergence guarantee as we elaborate below.
We obtain the following convergence results of SA under the Y-optimization principle.
Theorem 1 (Convergence of SA). Apply SA to solve the over-parameterized problem (P). If the
optimization process satisfies the principle in Definition 1, then the following statements hold.
1.
2.
3.
The optimization path {θk }k approaches the globalminimizer θ* with a monotonically diminishing
distance, i.e., kθk+ι 一 θ*k ≤ ∣∣θk 一 θ*k for all k = 0,1, 2,...,.
For all i, each '(∙; Zi) converges to its global minimum along the optimization path, i.e., denote
{i(T)}t∈n as the sequence Ofiterations that sample Zi, then, limτ→∞。(9μT); Zi) = '(θ*; Zi).
For any K = nB , B ∈ N, the accumulated loss converges to the global minimum at the rate
1 K-1	1
天 X '(Ok； zξk)-以。*； Z) ≤ 干
k=0	ηY
(3)
To elaborate, item 1 of Theorem 1 shows that the optimization path of SA approaches the global
minimizer with a monotonically diminishing distance under the Y-optimization principle, and item 2
further establishes the convergence of each individual loss to its global minimum. More importantly,
item 3 shows that the average of the accumulated loss convergences to the global minimum of the
total loss at a sub-linear convergence rate, which is inverse proportionally to the parameterization Y of
the optimization principle in eq. (2). Intuitively, a larger Y implies a more coherent update U(θk; Zξk )
with the desired direction θk 一 θ* and therefore leads to a faster convergence.
Remark: Under the Y-optimization principle, the sub-linear convergence rate in item 3 depends on the
learning rate η and the parameter Y only, which are problem-independent parameters. Therefore, given
a fixed learning rate, the parameter Y of the optimization principle provides a universal quantification
of the optimization quality. Inspired by this idea, in the subsequent sections, we conduct extensive
experiments to examine the validity of the Y-optimization principle in DNN training. In particular, we
empirically quantify the impacts of different training techniques on the DNN training by evaluating
the parameter Y of the optimization principle in each optimization process.
3 Experiments on Network-level Training Techniques
In this section, we examine the validity of the Y-optimization principle in training DNNs with
different neural network-level training techniques, i.e., activation function, batch normalization and
skip connection. We outline the exploration plan below and provide the details of the experiment
setup in the corresponding subsections.
4
Under review as a conference paper at ICLR 2020
Exploration plan: In all DNN trainings, we train the network for a sufficient number of epochs to
achieve an approximate global minimum. We store the network parameters {θk}k,loss {'(θk; zξk )}k
and update {U(θk; zξk)}k that are generated in each DNN training. Then, we compute the upper
bound for Y in each iteration according to eq. (2), where we set θ* to be the network parameters
produced in the last training iteration. We use mini-batch sampling and all the stored loss and updates
correspond to their mini-batch versions.
3.1	IMPACT OF ACTIVATION FUNCTION ON γ-OPTIMIZATION PRINCIPLE
Experiment setup: We train a variant of the Alexnet Zhang et al. (2017a) and the Resnet-18 He et al.
(2015) with different choices of activation functions for all the nonlinear neurons. The activation
functions that we explore include sigmoid, tanh, ReLU and leaky ReLU (with slope 10-2). We apply
the standard SGD optimizer with a fixed initialization point, a mini-batch size 128 and a constant
learning rate η = 0.05 to train these networks for 150 epochs on the Cifar10 and Cifar100 datasets
Krizhevsky (2009), respectively.
In the three rows of Figure 1, we present the training results of the normalized distance-to-minimizer,
training loss and parameter γ of the optimization principle along the optimization path, respectively.
From the figures in the third row, one can see that these trainings with different activation functions
obey the γ-optimization principle with γ > 0 in the training process. This demonstrates the validity
of the principle in these DNN trainings. Also, the figures in the first and second rows respectively
show that the distance-to-minimizer diminishes monotonically and the training loss converges to the
global minimum in these trainings. These observations are consistent with the theoretical implications
of the γ-optimization principle in items 1 and 2 of Theorem 1.
Moreover, regarding the trainings of the Alexnet (first two columns), we observe that the trainings
with the sigmoid activation function converge much slower than those with the other activation
functions, and the parameter γ under the sigmoid activation has a smaller value than those under the
other activation functions. On the other hand, in the trainings of the Resnet-18 (last two columns),
we observe that the convergence speed of the trainings with ReLU types of activation functions
is the fastest, and is followed by that of the trainings with tanh activation function and sigmoid
activation function, respectively. Moreover, one can see that the γ in the trainings with ReLU types of
activation functions has the largest value, and is followed by that in the trainings with tanh activation
function and sigmoid activation function, respectively. These observations are consistent with item 3
of Theorem 1, where a larger γ implies faster convergence.
Number of Epochs
0	10 20 30 40 50	60
NUmber of Epochs
Figure 1: Training Alexnet and Resnet-18 with different activation functions.
5
Under review as a conference paper at ICLR 2020
The experiments in this subsection show that DNN trainings with different activation functions obey
the γ-optimization principle, which captures and quantifies the impact of choice of activation function
on the training process via the parameter γ .
3.2	IMPACT OF BATCH NORMALIZATION ON γ-OPTIMIZATION PRINCIPLE
Experiment setup: We train the Resnet-18, 34 and the Vgg-11, 16 networks with the settings: 1)
keep all the BN layers; 2) keep the first BN layer in each block; and 3) remove all the BN layers. We
remove all the dropout layers in the Vgg networks. We apply SGD with a fixed initialization point, a
constant learning rate (η = 0.05 for Resnet, 0.01 for Vgg) and batch size 128 to train these networks
on the Cifar10 and Cifar100 datasets, respectively.
Figure 2 shows the training results on the Cifar10 dataset. Due to space limitation, we present the
distance-to-minimizer results in Appendix B, where one can see that the distances are monotonically
diminishing and therefore are consistent with item 1 of Theorem 1. In all the trainings shown in
Figure 2, removing all BN layers significantly slows down the convergence of loss. In particular,
vanishing gradient occurs after around 25 epochs in the trainings of resnets without any BN layer and
we plot the corresponding curves up to the epoch when gradient vanishes. On the other hand, the
trainings that keep the first BN layer in each block converge as fast as those with all BN layers, and
they all converge to the global minimum.
Regarding the γ values, they are all positive throughout these trainings, which demonstrates the
validity of the γ-optimization principle. Moreover, the γ in the trainings without any BN layer is
considerably smaller than that in the trainings with either one BN layer or all BN layers. Such an
observation is consistent with the convergence rate result in item 3 of Theorem 1, where a larger γ
implies faster convergence.
0.06
L 0.04
S3
I
且 0.02
ResNet-34, CIFAR-10
-Θ-Keep all BN	^
-■-Keep 1st BN in all blocks
∙⅜-Remove all BN_
z≤⅛i
0	10	20	30	40	50
Number of Epochs
0
0	10	20	30	40
Number of Epochs
0	10	20	30	40	50
Number of Epochs
Figure 2:	Training Resnets and Vggs with and without BN on Cifar10.
We also report in Figure 3 the results of training these networks on the Cifar100 dataset, where
one can see the positiveness of the γ throughout the training that demonstrates the validity of the
γ-optimization principle. Particularly in the trainings of the resnets (see top row), the convergence
of the trainings with all BN layers is the fastest, and is followed by that of the trainings with one
BN layer in each block and without any BN layer, respectively. Moreover, in the first 20 epochs
where the training loss is close to the global minimum, one can see that the γ in the trainings with
all BN layers is larger than that in the trainings with one BN layer in each block, and the γ in the
trainings without any BN layer has the smallest value. Similar observation is made in the training
of the Vgg-16 network. Moreover, in the training of the Vgg-11 network, the training with one BN
layer in each block is sufficient to achieve comparable performance to that of the training with all BN
layers, and their γ have similar values throughout the training. In comparison, the training without
any BN layer is significantly slower and have much smaller γ values.
The experiments in this subsection demonstrate that the γ-optimization principle is able to capture
and quantify the impact of batch normalization on DNN training in terms of the parameter γ .
6
Under review as a conference paper at ICLR 2020
X10FteSNet44, CIFAR-100
Figure 3:	Training Resnets and Vggs with and without BN on Cifar100.
3.3 IMPACT OF SKIP CONNECTION ON γ-OPTIMIZATION PRINCIPLE
Experiment setup: We train the Resnet-18 with the settings: 1) keep all the skip connections; and 2)
keep the first skip connection in each block. For the Resnet-34, we consider an additional setting
where we keep the first two skip connections in each block. We apply SGD with a fixed initialization
point, a constant learning rate η = 0.05 and batch size 128 to train these networks for 150 epochs on
the Cifar10 and Cifar100 datasets, respectively.
Figure 4 shows the training results and we present the distance-to-minimizer results in Appendix C,
where one can see that they are all monotonically diminishing in the trainings. One can see from the γ
curves that all these trainings satisfy the γ-optimization principle. In specific, regarding the trainings
of the Resnet-18 on Cifar10, the training with one skip connection in each block has comparable
performance to that of the training with all skip connections, and both trainings have comparable γ
values. In the trainings of the Resnet-18 on the more complex Cifar100 dataset, the training with
more skip connections achieves a slightly faster convergence, and the corresponding γ value is larger
than that in the training with one skip connection in each block. Regarding the trainings of the deeper
Resnet-34, it can be seen that skip connections significantly accelerate the training on both datasets.
In particular, trainings are faster with more number of skip connections in each block, and the γ
values are larger in the trainings with more skip connections in each block. All these observations are
consistent with the theoretical implications of the γ-optimization principle in Theorem 1. Therefore,
the γ-optimization principle is able to characterize and quantify the impact of skip connections on
DNN training. Our training results imply that skip connections are more effective in deeper resnets.
ResNet-34, CIFAR-100
ReSNet-18, CIFAR-100
0	10	20
Number of Epochs
Figure 4: Training Resnets with and without skip connections.
Number of Epochs
10 XioRtesNeQ, CIFAR-100
8
o
⅝ 6
1
4
I
2
0
0	10	20	30
Number of Epochs
4 Experiments on Optimization-level Training Techniques
In this section, we explore the impacts of stochastic optimizers on the γ-optimization principle.
7
Under review as a conference paper at ICLR 2020
Experiment setup: We train the Resnet-18, 34 and the Vgg-11, 16 networks using SGD, SGD with
momentum and Adam, respectively. We remove all the dropout layers in the Vgg networks. We apply
a fixed initialization point, a constant learning rate η = 0.001 and batch size 128 to all the optimizers
and train these networks on the Cifar10 and Cifar100 datasets, respectively. We set the momentum to
be 0.5 for the SGD with momentum and set β1 = 0.9, β2 = 0.999, = 10-2 for the Adam.
Figure 5 presents the training results on the Cifar10 dataset, and we present the distance-to-minimizer
results in Appendix D.1 where one can see that they are all monotonically diminishing. In all these
trainings, the γ curves are above zero and therefore demonstrates the validity of the γ-optimization
principle. Also, it can be seen that the SGD with momentum trainings achieve significantly faster
convergence speed than that achieved by the SGD trainings, and the Adam trainings converge the
fastest. Regarding the γ, it can be seen that the SGD trainings obey the optimization principle with
the smallest γ, which is further enlarged in the SGD with momentum trainings. Moreover, the Adam
trainings obey the optimization principle with the largest γ. These observations are consistent with
item 3 of Theorem 1, where a larger γ implies faster convergence of the training.
Number of Epochs
Figure 5:	Training Resnets and Vggs with different optimizers on Cifar10.
The training results on the Cifar100 dataset are shown in Figure 6, where one can make very similar
observations as those on Cifar10. In particular, for the Adam training of VGG-16 on Cifar100, the
sign of γ fluctuates after 50 epochs, which is possibly caused by the stochastic pre-conditioner in the
Adam update. In addition to these experiments, we also visualize the optimization paths of different
optimizers in Appendix D.2, where one can see that the directions of SGD updates are very different
from those of the updates generated by the SGD with momentum and Adam. These experiments
show that the γ-optimization principle characterizes the impact of optimizers on DNN training.
VGG-16, CIFAR-100
1
0.8
0.6
0.4
0.2
0
-0.2
0 10 20 30 40 50 60 70 80
Number of Epochs
Figure 6:	Training Resnets and Vggs with different optimizers on Cifar100.
8
Under review as a conference paper at ICLR 2020
5 Conclusion
In this paper, we propose a γ-optimization principle for general stochastic algorithms in nonconvex
and over-parameterized optimization. The γ-optimization principle provides solid convergence
guarantees to stochastic optimization and achieves a sub-linear convergence rate that scales inverse
proportionally to γ and is independent of the problem parameters. Through extensive DNN training
experiments, we show that practical DNN trainings obey the principle reasonably well, and the
parameter γ provides a unified metric that measures the impacts of different training techniques on
DNN training. In the future work, we expect that such an optimization principle can be exploited to
develop improved training techniques for deep learning.
References
N. Bjorck, C. P. Gomes, B. Selman, and K. Q. Weinberger. Understanding batch normalization. In
Proc. Advances in Neural Information Processing Systems (NeurIPS), pp. 7694-7705, 2018.
X. Chen, S. Liu, R. Sun, and M. Hong. On the convergence of a class of Adam-type algorithms for
non-convex optimization. In Proc. International Conference on Learning Representations (ICLR),
2019.
D. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by exponential
linear units (elus). In Proc. International Conference on Learning Representations (ICLR), 2015.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,
Signals and Systems, 2(4):303-314, Dec 1989.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12:2121-2159, July 2011.
S. Ghadimi and G. Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic
programming. Mathematical Programming, 156(1):59-99, Mar 2016.
X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In Proc. International
Conference on Artificial Intelligence and Statistics (AISTATS), pp. 315-323, 11-13 Apr 2011.
M. Hardt and T. Ma. Identity matters in deep learning. arXiv:1611.04231, 2016.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proc. IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, 2015.
G. Huang, Z. Liu, and K. Weinberger. Densely connected convolutional networks. In Proc. IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2261-2269, 2016.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In Proc. International Conference on Machine Learning (ICML), pp.
448-456, 2015.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proc. International
Conference on Learning Representations (ICLR), 2015.
A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss
landscape of neural nets. In Proc. Advances in Neural Information Processing Systems (NeurIPS),
pp. 6389-6399, 2018a.
X. Li, S. Ling, T. Strohmer, and K. Wei. Rapid, robust, and reliable blind deconvolution via nonconvex
optimization. Applied and Computational Harmonic Analysis, 2018b.
A. Maas, Y. Hannun, and A. Ng. Rectifier nonlinearities improve neural network acoustic models. In
Proc. ICML Workshop on Deep Learning for Audio, Speech and Language Processing, 2013.
V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proc.
International Conference on Machine Learning (ICML), pp. 807-814, 2010.
9
Under review as a conference paper at ICLR 2020
Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer, 2014.
E. Orhan and X. Pitkow. Skip connections eliminate singularities. In Proc. International Conference
on Learning Representations (ICLR), 2018.
N. Qian. On the momentum term in gradient descent learning algorithms. Neural Networks, 12(1):
145-151,JanUary 1999.
S. Reddi, S. Kale, and S. Kumar. On the convergence of Adam and beyond. In Proc. International
Conference on Learning Representations (ICLR), 2018.
H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics,
22(3):400-407, 09 1951.
D. E. RUmelhart, G. E. Hinton, and R. J. Williams. Learning Representations by Back-propagating
Errors. Nature, 323(6088):533-536, 1986.
S. SantUrkar, D. Tsipras, A. Ilyas, and A. Madry. How does batch normalization help optimization?
In Proc. Advances in Neural Information Processing Systems (NeurIPS), 2018.
R.	Srivastava, K. Greff, and J. SchmidhUber. Highway networks. ArXiv 1505.00387, 2015.
C. Szegedy, W. LiU, Y. Jia, P. Sermanet, S. Reed, D. AngUelov, D. Erhan, V. VanhoUcke, and
A. Rabinovich. Going deeper with convolUtions. In Proc. IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2015.
S.	TU, R. Boczar, M. Simchowitz, M. Soltanolkotabi, and B. Recht. Low-rank solUtions of linear
matrix eqUations via ProcrUstes flow. In Proc. 33rd International Conference on Machine Learning
(ICML), pp. 964-973, 2016.
C.	Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning reqUires
rethinking generalization. In Proc. International Conference on Learning Representations (ICLR),
2017a.
H. Zhang, Y. ZhoU, Y. Liang, and Y. Chi. A nonconvex approach for phase retrieval: reshaped
Wirtinger flow and incremental algorithms. Journal of Machine Learning Research (JMLR), 18
(141):1-35, 2017b.
X. Zhang, Y. YU, L. Wang, and Q. GU. Learning one-hidden-layer relU networks via gradient descent.
In Proc. International Conference on Artificial Intelligence and Statistics (AISTATS), volUme 89,
pp. 1524-1534, 16-18 Apr 2019.
K. Zhong, Z. Song, P. Jain, P. L. Bartlett, and I. S. Dhillon. Recovery gUarantees for one-hidden-layer
neUral networks. In Proc. 34th International Conference on Machine Learning (ICML), volUme 70,
pp. 4140-4149, AUg 2017.
Y. ZhoU and Y. Liang. Characterization of gradient dominance and regUlarity conditions for neUral
networks. ArXiv:1710.06910v2, Oct 2017.
Y. ZhoU, H. Zhang, and Y. Liang. Geometrical properties and accelerated gradient solvers of non-
convex phase retrieval. In Proc. 54th Annual Allerton Conference on Communication, Control,
and Computing (Allerton), pp. 331-335, 2016.
Y. ZhoU, J. Yang, H. Zhang, Y. Liang, and V. Tarokh. SGD converges to global minimUm in deep learn-
ing via star-convex path. In Proc. International Conference on Learning Representations(ICLR),
2019.
D.	ZoU, Y. Cao, D. ZhoU, and Q. GU. Stochastic gradient descent optimizes over-parameterized deep
relU networks. arXiv:1811.08888, 2018.
10
Under review as a conference paper at ICLR 2020
Supplementary Materials
A Proof of Theorem 1
To prove item 1, note that the SA update rule implies that
kθk+1 - θ*k2 = ∣∣θk - ηU (θk; zξk ) - θ*∣∣2
=I% - θ*k2 + η2ku(θk;zξk)k2 - 2η(θk-θ*, U(θk； zξk)i
≤kθk-θ*k2 + η2ku (θk; zξk )k2-η2ku (θk; zξk )k2
— 2ηγkθ0 - θ*k2('(θk ； Zξk )-'(θ*; Zξk)),	(4)
where the last inequality follows from the γ-optimization principle in Definition 1. Note that θ*
is a common global minimizer for the loss on all indivisual data samples. Therefore, `(θk; zξk ) -
'(θ* ； zξk) > 0 for all k and the above inequality further implies that
kθk + 1-θ*k2 ≤ kθk-θ*k2,
which completes the proof of item 1.
To prove item 2, note that the SA algorithm adopts the cyclic sampling with reshuffle scheme.
Summing eq. (4) over the B-th epoch (i.e., k = nB, nB + 1, ..., n(B + 1) - 1) gives that
n(B+1)-1
kθn(B+1)-θ*k2 ≤ kθnB —。*『— 2〃7|四—。*『 X	('® ； Zξk ) —，(。*； Zξk )).
k=nB
Further telescoping over the epoch index, we obtain that for all B
B-1 n(P +1)-1
kθnB -θ*k2 ≤kθ0-θ*k2- 2ηγkθo-θ*k2 X X	('(θk； zξk)-'(θ*; Zξk)).	(5)
P=0 k=nP
Note that the left hand side is nonegative for all B and '(θk; z>：) 一 '(θ*; zξk) ≥ 0 for all k. Due
to the cyclic sampling with reshuffle scheme, every data sample is visited once in each epoch. Fix
an i ∈ {1, ..., n} and denote the sequence of iterations in which the data sample zi is sampled as
i(0), i(1), ..., i(B - 1). We can rewrite the above inequality as
n B-1
kθnB — θ*k2 Wk。。— θ*k2 — 2ηγkθ0 —叫2 XX ('(θi(P)； Zi)- '(θ*; Zi)).
i=1 P=0
Suppose for certain i the sequence {'(θi(p); Zi)}p does not converge to the global minimum '(θ*; Zi).
Then, for any e > 0, '(θ%(p); Zi) — '(θ*; Zi) > e for infinitely many P. This implies that the above
double summation diverges to +∞ and the right hand side of the inequality eventually becomes
negative, contradicting with the non-negativity of the left hand side. Therefore, '(θ%(p); Zi) →→
'(θ*; Zi) for all i. This completes the proof of item 2.
Item 3 follows from eq. (5) after rearranging the terms and noting that θ* is a common minimizer for
all the indivisual loss functions.
11
Under review as a conference paper at ICLR 2020
B Distance-to-Minimizer Results on Batch normalization
All distances-to-minimizer diminishes monotonically and are consistent with the theoretical implica-
tions of the γ-optimization principle in items 1 of Theorem 1.
Number of Epochs
Number of Epochs	Number of Epochs
Figure 7:	Training Resnets and Vggs with and without BN on Cifar10.
Figure 8:	Training Resnets and Vggs with and without BN on Cifar100.
C Distance-to-Minimizer Results on Skip Connection
All distances-to-minimizer diminishes monotonically and are consistent with the theoretical implica-
tions of the γ-optimization principle in items 1 of Theorem 1.
18 6 4 2
■e1Oe=7_01忐『
Figure 9: Training Resnets with and without skip connections.
ReSNet-18, CIFAR-100
1∙8∙64 2 O
-O14-'-
12
Under review as a conference paper at ICLR 2020
D More results on Optimizers
D. 1 Distance-to-Minimizer Results
All distances-to-minimizer diminishes monotonically and are consistent with the theoretical implica-
tions of the γ-optimization principle in items 1 of Theorem 1.
ResNet-IB1 CIFAR-10	ResNet-34, CIFAR-10	VGG-11, CIFAR-10	VGG-16, CIFAR-10
18 6 4 2
∙elo=7=0l√τ
sgd
-÷-SGD (m=0.5)
-∙-Adam'(e=0.01)
1∙8∙64 2
∙6lo0=7=5≈lo^iτ
sgd
-e-SGD (m=0.5)
-*"Adam' (e=0.01)
10	20	30
Number of Epochs
1∙8∙64 2
=felo=7=0l^iτ
SGD
-0-SGD (m=0.5)
f-Adam (e=0.01)
0	10	20	30	40
Number of Epochs
0	10	20	30	40
Number of Epochs
0	10	20	30	40
Number of Epochs
Figure 10:	Training Resnets and Vggs with different optimizers on Cifar10.
Number of Epochs
Figure 11:	Training Resnets and Vggs with different optimizers on Cifar100.
D.2 Visualizing Optimization Path of Different Optimizers
By exploiting the visualization method proposed in Li et al. (2018a), we plot the 2D visualization of
the optimization paths of different optimizers on the loss landscape contours of the Resnet-18, 34,
Vgg-11, 16 networks in Figure 12. In specific, we store the optimization paths generated by both
the SGD with momentum and Adam ( = 10-2). For the parameters in each optimization path, we
compute their first and second principle components as the 2D axes. Moreover, we also calculate the
SGD updates evaluated at the parameters along each optimization path and project their directions
onto the two principle components for visualization.
From the visualization results shown in Figure 12, it can be seen that the directions of SGD updates
are very different from those of the updates generated by the SGD with momentum and Adam. This
is consistent with the experiments in the previous subsection where we observe that the γ in the
SGD trainings are very different from those in both the SGD with momentum trainings and Adam
trainings.
ResNet-18. CIFAR-IO ResNet-18r CIFAR-IO
ResNet-34r CIFAR-IO ResNet-34. CIFAR-IO
s90∙ZI Od PUZ
s66.ZI Odsz
——SGD with momentum
・ SGD
VGG-Il. CIFAR-IO
isz∙,"Iod PUZ
O 2«	4« M eo
1st PC: 66.47 %
VGG-16. CIFAR-IO
VGG-Il. CIFAR-IO
s6∙0I Od PUZ
is9∙ZI Od PUZ
«	1» 20	S« 8	8
1st PC: 62.23 %
VGG-16r CIFAR-IO
s~∙UOd PUZ
« lβ2ΦSΦ*ΦSΦMMM
1st PC: 71.61 %
OSlelS 20 2ssOsS-	6	⅛	« e« M
1st PC: 72.78 %	1st PC: 65.87 %
Figure 12:	Visualization of optimization paths of different optimizers and SGD updates
13