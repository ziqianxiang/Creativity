Under review as a conference paper at ICLR 2020
Limitations for Learning from Point Clouds
Anonymous authors
Paper under double-blind review
Ab stract
In this paper we prove new universal approximation theorems for deep learning
on point clouds that do not assume fixed cardinality. We do this by first general-
izing the classical universal approximation theorem to general compact Hausdorff
spaces and then applying this to the permutation-invariant architectures presented
in PointNet (Qi et al) and Deep Sets (Zaheer et al). Moreover, though both archi-
tectures operate on the same domain, we show that the constant functions are the
only functions they can mutually uniformly approximate. In particular, DeepSets
architectures cannot uniformly approximate the diameter function but can uni-
formly approximate the center-of-mass function but it is the other way around for
PointNet. Additionally, even when the point clouds are limited to at most k points,
PointNet cannot uniformly approximate center-of-mass and we obtain explicit er-
ror bounds and a method to produce geometrically derived adversarial examples.
1	Introduction
Recently, architectures proposed in PointNet (Qi et al., 2017) and Deep Sets (Zaheer et al., 2017)
have allowed for the direct processing of point clouds within a deep learning framework. These
methods produce outputs that are permutation-invariant with respect to the member points and work
for point clouds of arbitrarily large cardinality. A common source of such data is LIDAR mea-
surements from autonomous vehicles. Zaheer et al. (2017) also presents a permutation-equivariant
architecture which we do not discuss here.
Each of of these works provide their own universal approximation theorem (UAT) to support the
empirical success of their architectures. However, both results assume the cardinality of the point
cloud is fixed to some size n. In this work we refine these results, remove the cardinality limitation,
use weaker architecture assumptions, and arrive at three main results which can be summarized
roughly as follows (assuming unrestricted finite cardinality for the input point clouds):
1)	PointNet (DeepSets) architectures can uniformly approximate real-valued functions that
are uniformly continuous with respect to the Hausdorff (Wasserstein) metric and nothing
else (Theorem 3.4).
2)	Only the constant functions can be uniformly approximated by both architectures. In partic-
ular, PointNet architectures can uniformly approximate the diameter function but DeepSets
architectures cannot. Conversely, DeepSets architectures can uniformly approximate the
center-of-mass function but PointNet architectures cannot (Theorem 4.1).
3)	We prove explicit error lower bounds and produce adversarial examples to show that even
when limited to point clouds of size k, PointNet cannot uniformly approximate center-of-
mass (Theorem 4.2).
To do this we extend the many universal approximation results for feed-forward networks (Cybenko,
1989; Hornik et al., 1989; Leshno et al., 1993; Stinchcombe, 1999) to the abstract setting of general
compact Hausdorff spaces. We then find appropriate compact metric spaces over which PointNet
and DeepSets architectures can be easily analyzed and then finally we observe the resulting conse-
quences in the original setting of interest, i.e. point clouds.
1
Under review as a conference paper at ICLR 2020
2	Preliminaries
2.1	PointNet and DeepSets Architectures
In practice, the implementations of the architectures presented in PointNet and Deep Sets can in-
volve many additional tricks, but the essential ideas are quite simple. We do however make a small
modification to the Deep Sets model. For A ⊆ Rn of cardinality |A| < ∞, we have Qi et al. (2017)
and Zaheer et al. (2017) suggesting scalar-output neural networks of the form
FPN(A)= P (mmaX8(a)),	and FDS(A)= P (b + j X 8(a)),
respectively. Here φ : Rn → Rm creates features for each point in A, then a symmetric operation
is applied, and then P : Rm → R combines these features into a scalar output (here max is the
component-wise maximum). In practice, We need both P and φ to be neural networks. Note that
because we use a symmetric operation before P, the output will not depend on the ordering of points
in the point cloud, and because the max and sum operations scale to arbitrary finite cardinalities the
size of the point cloud is not an issue. The original model in Deep Sets did not have a bias term b
and used a sum instead of the averaging we use here. This change will help us later in our theoretical
analysis.
It will help to introduce some simplifying notation. Let F(Ω) denote the set of all nonempty finite
subsets of a set Ω (i.e. point clouds in Ω), F≤k(Ω) the set of nonepmty subsets of size ≤ k, and
Fk(Ω) the set of k-point subsets. Now consider Ω ⊆ RN and define maxf, avef,b : F(Ω) → R
which are given by maxf (A) = maxa∈A f (a) and avef,b(A) = b + 吉 Pa∈A f (a) respectively.
We make sense of this in the natural way if we use vector-valued f and b by operating component-
wise. We call these operations max neurons and biased-averaging neurons respectively.
Once again letting P and φ be neural networks, FPN = P ◦ max^ and FDS = P ◦ ave>b will be
the general form of what we call the PointNet and DeepSets architectures (resp.) in this paper.
Some natural questions are 1)is there a topology for F(Ω) that makes these architectures continuous,
2) how expressive are these approaches, and 3) how deep is deep enough for function approximation?
2.2	Function Spaces and Uniform Approximation
From now on, we only consider R-valued functions unless otherwise stated. Let B(A) be the set of
bounded functions on a set A, let C(X) and Cb(X) be the set of continuous and bounded continu-
ous functions on a topological space X (respectively), and let U(M) and Ub(M) be the uniformly
continuous and bounded uniformly continuous functions on a metric space (M, d) (respectively).
We equip all of these with the uniform norm i.e. |||f ∣∣∣a = suPa∈A |f (a)| - we reserve k*k for the
Euclidean norm. This makes them all normed spaces, with B(A), Cb(X) and Ub(M) additionally
being Banach spaces. Moreover, if X is compact and (M, d) has compact metric completion, then
C(X) = Cb(X) and U(M) = Ub(M) and hence are also Banach spaces. For background see Rudin
(2006).
If given an injective map i : A → X, then we say that 夕：A → R (uniquely) continuously extends
to X if there is a (unique) 0 ∈ C(X) such that 0 ◦ i =夕.We say a family of funcntions N on A
(uniquely) continuously extends to X if every 夕 ∈ N (uniquely) continuously extends to X.
We will make use of the following lemma which is proved in the appendix.
Lemma 2.1. LetN ⊆ B(D) where D is a dense subset ofa compact metric space (X, d). Suppose
N has a continuous extension to X denoted by N0 ⊆ C(X) which is dense. Then the uniform
closure ofN in B(D) is r(C (X)) = U(D) where r : C(X) → Cb(D) is the domain restriction map.
Letting D = F(Ω), this lemma suggest the following plan of attack: find a compact metric space
(X, d) in which we can realize F(Ω) as a dense subset and hope that our class of neural networks
N continuously extends to a dense subset of C(X). If we can do that, then we know the uniform
closure of our class of neural networks are precisely the uniformly continuous functions on F(Ω)
with respect to the metric inherited from X . This motivates the next subsection.
2
Under review as a conference paper at ICLR 2020
2.3	Metrics on the Space of Point Clouds
From now on We will assume (Ω, d) is a compact metric space and when Ω ⊆ Rn it will be compact
and equipped with the Euclidean metric. Let K(Ω) denote the set of all compact subsets of Ω and
P(Ω) denote the set of all Borel probability measures on Ω. The HaUSdorff metric d，H (Munkres,
2000) is a natural metric for K(Ω) and 1-Wasserstein metric dw (Villani, 2009) (also called the
Earth-mover distance) is a natural metric for P(Ω). With these metrics, K(Ω) and P(Ω) become
compact metric spaces of their own. From now on we will assume these two spaces are always
equipped with the aforementioned metrics.
We also briefly mention M(Ω) the Banach space of finite signed regular Borel measures on Ω.
By the Riesz-Markov theorem it is the topological dual space of C(Ω). Of interest to us is that
P(Ω) ⊆ M(Ω) and that the weak-* topology on P(Ω) coincides with the topology induced by dw.
This means that dw(μn μ) → 0 iff j f dμn → Jf dμ for all f ∈ C(Ω).
Next, note that F(Ω) ⊆ K(Ω) and let iκ denote the natural inclusion map. We can also define
an injective map ip : F(Ω) → P(Ω) by mapping A ∈ F(Ω) to its associated empirical measure
ip(A) = μA = 由 Pa∈A δa ∈ P(Ω) where δ° is the Dirac delta measure supported at a. The
injective maps iκ and ip allow us to induce the d，H and dw metrics on F(Ω). We will denote the
metrized versions by FH(Ω) and FW(Ω) respectively and use the same convention for FH(Ω) and
FW(Ω). Another important fact to know is that iκ and ip embed F(Ω) as dense subset of K(Ω)
and P(Ω). The former follows from compactness of the members of K(Ω) and to see why the latter
is true see Fournier & Guillin (2015); Villani (2009)
For f ∈ C(Ω) and b ∈ R, define Maxf : K(Ω) → R and Avef,b : P(Ω) → R as the functions
Maxf(K) = maxχ∈κ f(x) and Avef,b(μ) = b + Ω f dμ.
Lemma 2.2. Let (Ω, d) be compact, f ∈ C(Ω), and b ∈ R. Then Maxf ∈ C(K(Ω)) and Avef,b ∈
C(P(Ω)) and Maxf oiκ = maxf and Avef,b ◦ip = avef,b. As a consequence, PointNet and
DeePSets are uniformly continuous on FH(Ω) and FW(Ω) respectively.
This lemma (proved in the appendix) tells us that the max neurons and biased-averaging neurons
continuously extend to K(Ω) and P(Ω) and hence so do PointNet and DeepSets architectures (since
we merely compose with the continuous ρ after). Thus, we wil be able analyze such architectures
as continuous functions on compact metric spaces, which is mathematically a much nicer problem
than studying them as set-theoretic functions on an un-metrized F(Ω).
2.4	Generalized Neural Network Notation
For A a collection of functions from X to Y and B a collection of functions from Y to Z, we denote
the set of all compositions by B ◦ A = {f ◦ g | f ∈ B, g ∈ A}. In the case of a single function
σ : Y → Z we let σ ◦ A = {σ ◦ f | f ∈ A} and similarly for right-composition.
Next, let Aff denote the set of all affine functionals on RN, i.e. any function of the form f(x) =
w∙x+b. Let Nσ := span {σ ◦ Aff} denote the set of single hidden-layerneural networks with linear
output-layer and activation function σ, and then denote an H-layered network by Nσ where σ =
(σι,... ,σH) is a list of H-many activation functions where N (σ,τ) := Nσ,τ := SPan(T ◦ Nσ).
Next we define various classes of PointNet networks whose weight functions are themselves neural
networks. Let NPN := NPN := span {maxf | f ∈ Nσ} then define NPN := span {τ ◦Npn}.
Like before, we can inductively define deeper networks, but we can also use deeper weight networks
as well to create NPN - thus we have two distinct notions of depth.
Next, we do the same for DeepSets. Let NDS := NDS := SPan {avef,b | f ∈ Nσ, b ∈ R}. Note
that avef,b + aveg,c = avef +g,b+c and α avef,b = aveαf,αb. Thus since Nσ is a linear space,
taking the span has no effect andNDσS = {avef,b | f ∈ Nσ, b ∈ R}. Going one layer deeper yields
NDS := span {τ ◦NDs} which gets us new functions. Like with PointNet, we can inductively
develop deeper families in two ways.
By Lemma 2.2 we can extend all the operations of our neural networks to K(Ω) and P(Ω) in a
natural way. This let,s us talk about about PointNet networks on K(Ω) and DeepSets networks on
3
Under review as a conference paper at ICLR 2020
P(Ω) which we'll define analogously by replacing maxf with Maxf and avef,b with Avef,b. Thus
MPN = span {Maxf | f ∈ Nσ} ,	MPN = Span {τ ◦NPn},
MDS = span {Avef,b | f ∈ Nσ, b ∈ R} ,	MDS = Span {τ ◦ MDS} ∙
As before, the linear structure of Nσ makes MσDS = {Avef,b | f ∈Nσ}
3	Main Results
3.1	Topological UAT
Leshno et al. (1993) prove that Nσ with σ ∈ C(R) has universal approximation property iff σ is
not a polynomial. For this reason, we will say a σ ∈ C(R) is ‘universal’ if it is non-polynomial
and denote the set of all such such functions by U(R). Using this theorem and Stone-Weierstrass
we prove a UAT for certain kinds of two-hidden-layer ‘neural networks’ on an abstract compact
Hausdorff space.
Recall that a family of functions S on Ω separates points if for any X = y there is an f ∈ S so that
f(x) 6= f(y).
Theorem 3.1 (Topological-UAT). Let X be a compact Hausdorff space and σ ∈ U(R). If S ⊆
C(X) separates points and contains a nonzero constant, then span(σ ◦ span S) is dense in C(X).
Additionally, if S also happens to be a linear subspace, then span(σ ◦ S) is dense in C(X).
Proof. Let S and σ satisfy the above and let V = span S. Let Alg(V ) denote the algebra generated
by V , i.e. all possible finite products, sums and scalar multiples of the elements of V . Then Alg(V )
is unital subalgebra of C(X) that seperates points. By the Stone-Weierstrass theorem Alg(V ) is
dense in C(X). Now let F ∈ C(X) and > 0 be arbitrary. By density there is a G ∈ Alg(V ) such
that |F (a) - G(a)| < /2 for all a ∈ X. Since G ∈ Alg(V ) there is an N -variable polynomial p
and s = (s1, . . . , sN) where si ∈ S, so that G = p ◦ s. Since all si ∈ C(X) and X is compact, the
image s(X) ⊆ RN is compact. By the classical UAT (Leshno et al., 1993), there exists an η ∈ Nσ
such that |p(x) - η(x)∣ < 〃2 for all X ∈ RN. Thus,
|F(a) - (η ◦ s)(a)l ≤ |F(a) - p(s(a))l + ∣p(s(a)) - η(s(a))l < "2 + e/2 = E
for every a ∈ X. Finally note that η(s(a)) = Pm=I aiσ(wi ∙ s(a) + bi) for some ai, b ∈ R and
Wi ∈ RN. Since S constains a nonzero constant, span S contains every constant and so Wi ∙ S + b ∈
span S. Thus η ◦ s ∈ span(σ ◦ span S) as desired.
Lastly, if S is also linear subspace, then S = span S and so span(σ ◦ S) is dense in C(X).	□
3.2	Point Cloud UAT
We have met almost all the conditions required to use the topological-UAT on K(Ω) and P(Ω). We
just need to show that Maxf and Avef,b yield nonzero constants and can separate points even when
we limit ourselves to f ∈ Nσ .
Lemma 3.2 (Separation Lemma). Let Ω ⊆ RN be compact and σ ∈ U(R). Then SPN =
{Maxf | f ∈ Nσ} and SDS = {Avef,b | f ∈ Nσ, b ∈ R} separate points and contain constants.
Proof. Let d denote the Euclidean distance. First note that the constant function h = σ(c) ∈ Nσ
for some c ∈ R. Since σ is not a polynomial, there is a choice of c for which σ(c) 6= 0. This means
Maxh ∈ SPN and Aveh,0 ∈ SDS are both constant. Now we just need to show that SPN and SDS
separate points.
(SpN separates points): Let A, B ∈ K(Ω) with A = B. Without loss of generality, A \ B = 0 so
choose a ∈ A \ B. Let f(x) = min {1, d(x, B)/d(a, B)} and note that f(a) = 1, f(B) = {0}
and f (Ω) = [0,1]. By the classical UAT (Leshno et al., 1993) Nσ is dense in C(ω), so there is a
g ∈ Nσ so that |f (x) - g(x)| < 1/2 for all X ∈ Ω. Note Maxg ∈ SPN and that Maxg(A) > 1/2
and Maxg(B) < 1/2. Since A and B were arbitrary, this shows SPN separates point in K(X).
4
Under review as a conference paper at ICLR 2020
(SDS separates points): Given μ1,μ2 ∈ P(Ω) with μι = μ2, by the Hahn-Banach separation
theorem there exists a weak-* continuous linear functional L : M(Ω) → R that separates them.
Let δ = ∣L(μι) - L(μ2)∣. The topological dual of M(Ω) with the weak-* topology is equivalent to
C(Ω) and so there is an f ∈ C(Ω) so that L(η) = R f dη for all η ∈ M(Ω). Since Nσ is dense in
c(ω) there is a g ∈ Nσ so the that |f (x) - g(x)∣ < δ∕2 for all X ∈ Ω. Define J(η) = R gdη. Then
for all η ∈ P(Ω) We have ∣L(η) - J(η)∣ ≤ ʃ |f - g| dη < 2 ʃ dη = δ∕2. Applying the triangle
inequality we obtain,
δ = IL(MI) - L(μ2)1 ≤ IL(MI) - J(μι)| + | j(μι) - J(μ2)∖ + IJ(μ2) - L(μ2)1
x------------------------------{-------}	X-------{-----}
<δ∕2	<δ∕2
Thus 0 < ∖ J(μι) - J(μ2)∖ and so J = Aveg,0 ∈ SDS separates μι and μ2. Since μι and μ2 were
arbitrary, it follows that SDS seperates points in P(Ω).	□
The following theorems show that one hidden layer in the weight networks and one hidden layer
of the of the other kind suffice to prove the universal approximation theorems for PointNet and
DeepSets.
Theorem 3.3. Let Ω ⊆ RN be compact and σ,τ ∈ U(R). Then MPN and MDS are dense in C(A)
and C(B) respectively, where A ⊆ K(Ω) and B ⊆ P(Ω) are closed subsets.
Proof. Recall MPN = span {τ ◦ span SPN} and MDS = span {τ ◦ SDS}. Since K(Ω) and
P(Ω) are compact metric spaces, A and B are compact Hausdorff. By Lemma 3.2 we know SPN
and SDS separate points and contain nonzero constants and so the topological-UAT (Theorem 3.1)
yields the desired result.	□
Theorem 3.4 (Point-Cloud-UAT). Let Ω ⊆ RN be compact. If σ,τ ∈ U(R), then the uniform
closure of NPN andNDS within B(F(Ω)) is U(FH(Ω)) andU(FW(Ω)) respectively.
Proof. FH(Ω) and FW(Ω) are isometrically isomorphic to iκ(F(Ω)) and ip(F(Ω)) which are in
turn dense in (K(Ω), dH) and (P(Ω), dw). By Lemma 2.2 we have that NPN and NDS continu-
ously extend to K(Ω) and P(Ω) as MPN and MDS. By Theorem 3.3 we know MPN and MDS
are dense in C (K(Ω)) and C (P (Ω)). Finally, by Lemma 2.1 we have the desired result.	□
It’s worth noting that we could have used Stinchcombe’s generalization of the UAT to the case of
neural networks on compact subsets of locally convex spaces (Stinchcombe, 1999) to prove that
MDS is dense in C(P(Ω)) but we chose the above route for consistency of technique and to be
self-contained.
We now prove as a corollary a refinement of the universal approximation theorems ofQi et al. (2017)
and Zaheer et al. (2017), both of which applied to the the case of k-point point clouds. In this version
of the theorem we are able to restrict the depth of the neural network to just two hidden layers. The
proof is essentially the same as Theorem 3.4.
Corollary 3.5. Let Ω ⊆ RN be compact. If σ,τ ∈ U(R), then the uniform closure of NPN and
NDS within B(Fk(Ω)) are U(Fk(Ω)h) andU(Fk(Ω)w) respectively.
Proof. FH(Ω) and FW(Ω) are isometrically isomorphic to iκ(Fk(Ω)) and ip(Fk(Ω)) which are
in turn dense in their respective closures which we denote GH(Ω) ⊆ K(Ω) and GW(Ω) ⊆ P(Ω).
Thus by Lemma 2.2 and Theorem 3.3 we have that MPN and MDS are dense in C(GH(Ω)) and
C(GW(Ω)). Finally, by Lemma 2.1 we have the desired result.	□
4	Limitations of PointNets and DeepSets
Note that unlike the classical universal approximation theorem we should not expect to be able
uniformly approximate C(FH(Ω)) or C(FW(Ω)) since their elements might not even be bounded
functions. For example, ακ(A) = d，H(A, K)-1 is unbounded but continuous on FH(Ω) whenever
K ∈ K(Ω) but K ∈ F(Ω). Subtler still, we do not even obtain all elements of Cb(FH(Ω)) and
5
Under review as a conference paper at ICLR 2020
(a) dH -continuous closed loop.
(b) Illustration of Lemma 4.3.
Figure 1: In (a) we see a dH -continuous loop in FH≤4 where ‘x’ marks the center-of-
mass. Though the point cloud continuously changes from a 4-point set to a 2-point
set, the center of mass discontinuously changes at the moment of convergence. In (b)
we see the triangle formed by Cp, Cq and F ({p, q}) in Lemma 4.3 and how the ball
of radius less than kCp - Cqk /4 ensures the predicted error for |||F - Cent|||.
Cb(FW (Ω)). As an example of this, observe that βK = Sin ◦ ακ is bounded and is also continuous
on FH (Ω) because ακ is.
We'll now compare the representation power of these two architectures. Let Ω ⊆ RN be compact.
We define the point cloud diameter function Diam : F(Ω) → R and point cloud center-of-mass
function Cent : F(Ω) → RN by Diam(A) = maxx,y∈A d(x, y) and Cent(A)= 在 Ex∈A x.
Theorem 4.1.	Let (Ω, d) be an infinite compact metric space with no isolated points. Then a func-
tion f : F(Ω) → R is continuous with respect to both dH and dw iff it is constant. As a corollary,
Diam is uniformly approximable by PointNet networks but not DeepSets networks and Cent is uni-
formly approximable by DeepSets networks but not PointNet networks.
Proof. Assume f : F(Ω) → R is both dH-continuous and dw-continuous. Let A ∈ F(Ω) and let
P ∈ A. For each n = 1, 2,... choose An ∈ F(Ω) to be an n-point set contained within the 1 /n-ball
around p. We can do this because Ω is infinite without isolated points. Now let An = An ∪ (A∖{p}).
Observe that An d→H A and An d→W {p}. Thus,
f(A) = f
= lim f(An) = f
n→∞
dW
lim
n→∞
= f({p})
Note that A was arbitrary so f must always give a set and any of its singleton subsets the same
value. Now let B, C ∈ F(Ω) such that B = C. Without loss of generality assume q ∈ B \ C and
q 6= r ∈ C. Thus by the above,
f(B)=f({q})=f({q,r})=f({r})=f(C)
thus f must be constant. Conversely, constant maps are always continuous.
Finally, it’s known that the Diam satisfies |Diam(A) - Diam(B)| ≤ 2dH (A, B) and hence is dH-
continuous on K(Ω) and Cent is dw-continuous on P(Ω) because Aveni,0 is dw-continuous (here
πi is the projection onto the i-th component map). This means they are uniformly continuous on
FH(Ω) and FW(Ω) respectively and so the result follows from the above and Theorem 3.4.	□
While it is interesting to know that these neural networks describe fundamentally different kinds of
functions when we allow for unbounded cloud cardinality, in practice there is always a bound due
to computational resource limitations. The next result shows that even when we bound the cloud
cardinality, PointNet still cannot uniformly approximate the center-of-mass function.
6
Under review as a conference paper at ICLR 2020
Theorem 4.2.	Let Ω ⊆ Rd be an infinite compact set with no isolated points. Then for ^very
dH-continuous F : F≤k (Ω) → Rd there exists A ∈ Fk (Ω) such that
kF(A)- Cent(A)k ≥ k-2 Diam(Ω).
2k
In particular, since PointNet architectures are dH -continuous they cannot uniformly approximate
center-of-mass when k ≥ 3 and suffer from the above uniform-norm error lower-bound.
It is possible to see that PointNet cannot uniformly approximate Cent by considering Figure 1a.
Moving k -2 points in a k-element cloud to either of the remaining two points produce the same 2-
element cloud in a dH -continuous way. By dH -continuity, this means PointNet must output similar
centers for the cloud in the top-right and bottom-right - this introduces error. To obtain the explicit
error lower bound in Theorem 4.2, we need a slightly more detailed result.
Lemma 4.3. Let Ω ⊆ Rd be infinite with no isolated points and F : F≤k (Ω) → Rd an arbitrary
d，H-continuous map. Thenfor every distinct p, q ∈ Ω and 0 < τ < 1 there exists an A ∈ Fk (Ω)
such that p, q ∈ A and
kF(A)- Cent(A)k > (1 - τ) (k2k2) kp - qk
In particular, since PointNet architectures are dH -continuous they must satisfy this error bound.
Proof. Assume k ≥ 3 since the inequality is trivial for k = 1, 2. Let CP := p++q + (k-2 )p and
Cq = p++q + ( k-2 )q and observe that kCp - Cq k = k-2 kp - qk. NoW let E = T kCp - Cq ∣∣ /4
and choose AP and Aq to be k-point supersets of {p, q} such that AP \ {q} ⊆ B(p) and Aq \ {p} ⊆
B(q). We can do this because p and q are not isolated points. Since F is dH -continuous and
Ap, Aq d→H {p, q} as E → 0, We can additionally demand that F(Ap) - F ({p, q}) < E and
F (Aq) - F ({p, q}) < E. Next observe that
Cent(Ap)=中 + (1/k) X a,	Cent(Aq)=+ + (1/k) X a.
a∈Ap∖{p,q}	a∈Al ∖{P,q}
By the triangle inequality it folloWs that Cent(Ap) - Cp < E and Cent(Aq) - Cq < E. NoW
We can consider the triangle in Rd formed by Cp, Cq, and F ({p, q}) and realize by basic geometry
that one of kF ({p, q}) - Cpk or kF ({p, q}) - Cqk must greater than or equal to kCp - Cqk /2 (see
Figure 1b). Without loss of generality, let kF ({p, q}) - Cqk ≥ kCp - Cqk /2. Then,
kCp - Cqk ≤ IlF({p,q}) - F(Aq)∣∣ + ∣∣F(Aq) - Cent(Aq)∣∣ + IICent(Aq) - Cqll
2	'{}	'{}
<	<
Thus,
IlF(Aq) - Cent(Aq)∣∣ ≥ kCP - Cqk - 2e = (1 - τ) (k--2) kp - qk .
2	2k
Thus Aq is the promised set which achieves the desired uniform-norm error.	□
Theorem 4.2 noW folloWs easily.
proofofTheorem 4.2. Choose p, q ∈ Ω so ∣∣p 一 qk = Diam(Ω) and take T → 0 in Lemma4.3. □
5	Experiments
The proof of Lemma 4.3 not only establishes the error bound but also suggest an algorithmic ap-
proach to finding point clouds that exhibit the failure of uniform approximation. This let’s us produce
adversarial examples to the centor-of-mass problem for PointNet. When Ω is additionally convex -
e.g. the unit disk D in R2 - it becomes fairly easy to construct many examples of Ap and Aq ex-
plicitly for a given PointNet model, allowing us to empirically verify the uniform-norm error lower
7
Under review as a conference paper at ICLR 2020
(a) Training curve of a PointNet model learning Cent.
Figure 2: In (a) we see the training curve for the PointNet model learning the center-
of-mass of random 10-element point clouds. In (b) we see that we are always able to
find adversarial examples to this task producing errors at least as large as the theoret-
ical gaurantee.
(b) Experimental test of error lower bound.
bound. In the following experiment, we train a simple PointNet architecture to learn the center-of-
mass for 10-element point clouds in D . We train on a synthetic data set of 1 million point clouds
(each element uniformly sampled from D) labeled with their center-of-mass. The PointNet architec-
ture has 500K trainable parameters. The network has the form F(A) = ρ(maxa∈A φ(a)) where φ
has 2-D input layer, 500-D hidden layer, and 500-D linear output layer, and ρ has 500-D input layer,
500-D hidden layer and 2-D linear output layer (in accordance with the Point-Cloud-UAT). The hid-
den layers of φ and P are ReLU. Since it is not possible to train with respect to the uniform-norm,
we opt for the traditional L2 loss. In Figure 2a we can see the training curve settles after about 50
epochs. The loss at the 50th epoch however is 3-orders-of-magnitude greater than the predict worst
case.
To form our adversarial examples, we pick a nonzero τ = 0.01 and two distinct points p, q ∈ D at
random. We set = τ kp - q k /4. We then sample D another 8 times. We then linearly pull those
8 points towards p and towards q, sufficiently close so that the criteria in the proof of Lemma 4.3
are satisfied. This {p, q} adjoined with the 8 points pulled towards p and q form Ap and Aq. We are
theoretically ensured one of these two will have error larger than our bound. In Figure 2b we plot
the the produced adversarial error vs the distance between p and q that were used to make Ap and
Aq . As predicted, all the adversarial errors lie above the line representing the uniform-norm error
lower bound.
6	Conclusions and Future Work
The failure of the perceptron model to learn the XOR function was a blow that motivated the search
for new models. When classical feed-forward neural networks began to be successful, the spectre
of limited representation power loomed over the field until the universal approximation theorem
Cybenko (1989)Hornik et al. (1989)Leshno et al. (1993) resolved the question. In this work we
resolved the same question for the case of two current deep learning models for point clouds, and
laid out a program that could work for other models as well. However, unlike the case of classical
neural networks on compact domains ofRN, the same question for point clouds has a less definitive
answer even after having determined all of the uniformly approximable functions. Each method
explored here has their strengths and limitations but the presence of useful functions out-of-reach of
PointNet and DeepSets - even when considering bounded cardinality - opens the door for further
research.
There are still many questions. Will merging PointNet and DeepSets in a way to obtain greater
approximation power? Are there other useful topologies on F(Ω) for which new kinds of contin-
uous neural networks can be constructed? How much do these limitations matter in practice? And
finally, are there ways of developing architectures from the ground up with desirable yet different
approximation capabilities on point clouds and what are they like?
8
Under review as a conference paper at ICLR 2020
References
George Cybenko. Approximation by superpositions ofa sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314,1989.
Nicolas Fournier and Arnaud Guillin. On the rate of convergence in wasserstein distance of the
empirical measure. Probability Theory and Related Fields, 162(3-4):707-738, 2015.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-
versal approximators. Neural networks, 2(5):359-366, 1989.
Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward net-
works with a nonpolynomial activation function can approximate any function. Neural networks,
6(6):861-867, 1993.
James Raymond. Munkres. Topology. Prentice Hall, 2000.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point
sets for 3d classification and segmentation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 652-660, 2017.
Walter Rudin. Real and complex analysis. Tata McGraw-hill education, 2006.
M.b. Stinchcombe. Neural network approximation of continuous functionals and continuous func-
tions on compactifications. Neural Networks, 12(3):467477, 1999. doi: 10.1016/s0893-6080(98)
00108-7.
Cedric Villani. Optimal transport: old and new. Springer, 2009.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov,
and Alexander J Smola. Deep sets. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30,
pp. 3391-3401. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
6931-deep-sets.pdf.
9
Under review as a conference paper at ICLR 2020
A Appendix
Proof of Lemma 2.1. First we show that the r is a linear isometry. Since X is compact for f ∈ C(X)
there is a p ∈ X so that |f (p)| = |||f |||X. By density of D there is a sequence pn ∈ D that limits to
p. So
|||f|||X =	|f(p)|	= lim	|f(pn)|	≤ sup |f(x)|	= |||r(f)|||D	≤ sup	|f(x)|	= |||f|||X
X	n→∞	x∈D	D x∈X	X
Next, since C(X) is complete, so is its isometric image r(C (X)) and because Cb(X) is complete
that means r(C(X)) is closed. Thus,
r(C(X)) = r(N0) ⊆ r(N0) ⊆ r(C(X)) = r(C(X))
where the first subset results from continuity. Thus N = r(N0) = r(C(X)).
Finally, to show r(C(X)) = U(D) note that every uniformly continuous function g on D con-
tinuously extends to a function on X (because D is dense in X) placing this extension in C(X)
and so g ∈ r(C (X)). The reverse inclusion follows as well because restriction preserves uniform
□
continuity.
ProofofLemma 2.2. First we show that Maxf is d，H-continuous. Let e > 0. Since Ω is compact, f
is uniformly continuous and so there is a δ > 0 so that |f(x) - f(y)| < /2 whenever d(x, y) < 2δ.
Now let A, B ∈ K(Ω) and suppose d，H(A, B) < δ. By definition this means A ⊆ Bδ and B ⊆ Aδ.
By the triangle inequality we have
∣Maxf(A) — Maxf(B)| ≤ ∣Maxf(A) — Maxf(Aδ)| + ∣Maxf(Aδ) — Maxf(B)|
Since Aδ is compact there is a p ∈ Aδ so that Maxf (Aδ ) = f (p). Observe that if q ∈ K ⊆ Aδ
with d(p, q) < 2δ then |f (p) - f(q)| < /2 and f(p) = Maxf (Aδ) ≥ Maxf (K) ≥ f (q). This
implies |Maxf (Aδ) - Maxf (K)| < /2. In particular, since p ∈ Aδ there is an a ∈ A such that
d(p, a) < δ, and since B is compact there is a b ∈ B closest to p and so,
d(p, b) = d(p,B) ≤ dH(Aδ,B) ≤ dH(Aδ,A) +dH(A,B) < 2δ.
Thus |Maxf (Aδ) - Maxf (A)| and |Maxf (Aδ) - Maxf (B)| are less than	/2 and so
|Maxf (A) - Maxf (B)| < as desired.
To see why Avef,b is dW -continuous recall that the topology of dW is the same as the weak-*
topology for measures and so the map μ → Jf dμ is by definition continuous whenever f ∈ C(Ω).
Avef,b(ip(A)) = Avef,b (1 X δ°) = b + 1 X / f dδC = b + 1 X f (a) = avef,b(A)
nn	n
a∈A	a∈A	a∈A
It’s clear that Maxf ◦iK = maxf. The other identity follows from the linearity of integration.
Lastly, by composition of continuous functions it follows that PointNet and DeepSets continuously
extend to K(Ω) and P(Ω). Since (Ω, d) compact implies both K(Ω) and P(Ω) are compact, we can
□
deduce that PointNet and DeePSets are uniformly continuous on FH(Ω) and FW(Ω).
10