Under review as a conference paper at ICLR 2020

A   UNIVERSAL    SELF-ATTENTION    GRAPH    NEURAL

NETWORK

Anonymous authors

Paper under double-blind review

ABSTRACT

Existing  graph  embedding  models  often  have  weaknesses  in  exploiting  graph
structure  similarities,  potential  dependencies  among  nodes  and  global  network
properties. To this end, we present U2GNN, a novel embedding model leveraging
on    the strength of the recently introduced universal self-attention network (De-
hghani et al., 2019), to learn low-dimensional embeddings of graphs which can be
used for graph classification. In particular, given an input graph, U2GNN first ap-
plies a self-attention computation, which is then followed by a recurrent transition
to iteratively memorize its attention on vector representations of each node and
its neighbors across each iteration. Thus, U2GNN can address the weaknesses in
the existing models in order to produce plausible node embeddings whose sum is
the final embedding of the whole graph.  Experimental results in both supervised
and unsupervised fashions show that our U2GNN produces new state-of-the-art
performances on a range of well-known benchmark datasets for the graph classi-
fication task.

1    INTRODUCTION

Many real-world and scientific data are represented in forms of graphs, e.g.  data from knowledge
graphs, recommender systems, social and citation networks as well as telecommunication and bi-
ological networks (Battaglia et al., 2018; Zhang et al., 2018c).  In general, a graph can be viewed
as     a network of nodes and edges, where nodes correspond to individual objects and edges encode
relationships among those objects. For example, in online forum, each discussion thread can be con-
structed  as a graph where nodes represent users and edges represent commenting activities between
users (Yanardag & Vishwanathan, 2015).

Early approaches focus on computing the similarities among graphs to build a graph kernel for graph
classification (Ga¨rtner et al., 2003; Kashima et al., 2003; Borgwardt & Kriegel, 2005; Shervashidze
et al., 2009; Vishwanathan et al., 2010; Shervashidze et al., 2011; Yanardag & Vishwanathan, 2015;
Narayanan et al., 2017; Ivanov & Burnaev, 2018). These graph kernel-based approaches treat each
atomic substructure (e.g., subtree structure, random walk or shortest path) as an individual 
feature,
and count their frequencies to construct a numerical vector to represent the entire graph, hence 
they
ignore node attributes, substructure similarities and global network properties.

One recent notable strand is to learn low-dimensional continuous embeddings of the whole graphs
(Hamilton et al., 2017b; Zhang et al., 2018a; Zhou et al., 2018), and then use these learned embed-
dings to train a classifier to predict graph labels (Wu et al., 2019).  Advanced approaches in this
direction have attempted to exploit graph neural network (Scarselli et al., 2009), capsule network
(Sabour et al., 2017) or graph convolutional neural network (Kipf & Welling, 2017; Hamilton et al.,
2017a) for supervised learning objectives (Li et al., 2016; Niepert et al., 2016; Zhang et al., 
2018b;
Ying et al., 2018; Verma & Zhang, 2018; Xu et al., 2019; Xinyi & Chen, 2019; Maron et al., 2019b;
Chen et al., 2019).  These graph neural network (GNN)-based approaches usually consist of two
common phases: the propagating phase and the readout phase. The former phase aims to iteratively
update vector representation of each node by recursively aggregating representations of its neigh-
bors, and then the latter phase applies a pooling function (e.g., mean, max or sum pooling) on 
output
node representations to produce an embedding of each entire graph; and this graph embedding is
used to predict the graph label.  We find that these approaches are currently showing very promis-
ing performances, nonetheless the dependency aspect among nodes, which often exhibit strongly

1


Under review as a conference paper at ICLR 2020

in many kinds of real-world networks, has not been exploited effectively due to lack of advanced
computations within the propagating phase.

Very recently,  the universal self-attention network (Dehghani et al., 2019) has been shown to be
very powerful in NLP tasks such as question answering, machine translation and language model-
ing.  Inspired by this new attention network, we propose U2GNN – a novel universal self-attention
graph neural network to learn plausible node and graph embeddings.  Our intuition comes from an
observation that the recurrent attention process in the universal self-attention network can 
memorize
implicit dependencies between each node and its neighbors from previous iterations, which can be
then aggregated to further capture the dependencies among substructures into latent representations
in subsequent iterations; this process, hence, can capture both local and global graph structures. 
Al-
gorithmically, at each timestep, our proposed U2GNN iteratively exchanges a node representation
with its neighborhood representations using a self-attention mechanism (Vaswani et al., 2017) fol-
lowed by a recurrent transition to infer node embeddings. We finally take the sum of all learned 
node
embeddings to obtain the embedding of the whole graph. Our main contributions are as follows:

In our proposed U2GNN, the novelty of memorizing the dependencies among nodes implies that
U2GNN can explore the graph structure similarities locally and globally – an important feature
that most of existing approaches are unable to do.

U2GNN can be seen as a general framework where we prove the powerfulness of our model in
both supervised or unsupervised fashions. The experimental results on 9 well-known benchmark
datasets for the graph classification task show that both our supervised and unsupervised U2GNN
models produce new state-of-the-art (SOTA) accuracies in most of benchmark cases.

To our best of knowledge, our work is the first to show that a unsupervised model can noticeably
outperform up-to-date supervised approaches by a large margin. Therefore, we suggest that future
GNN  works  should  pay  more  attention  to  the  unsupervised  fashion  as  well  as  not  
comparing
supervised models with unsupervised models together.   This is important in both industry and
academic applications in reality where expanding unsupervised GNN models is more suitable
due    to the limited availability of class labels.

2    RELATED  WORK

Early popular approaches are based on “graph kernel” which aims to recursively decompose each
graph  into  “atomic  substructures”  (e.g.,  graphlets,  subtree  structures,  random  walks  or  
shortest
paths) in order to measure the similarity between two graphs (Ga¨rtner et al., 2003). For this 
reason,
we can view each atomic substructure as a word token and each graph as a text document, hence
we represent a collection of graphs as a document-term matrix which describes the normalized fre-
quency of terms in documents.  Then, we can use a dot product to compute the similarities among
graphs to derive a kernel matrix used to measure the classification performance using a kernel-based
learning algorithm such as Support Vector Machines (SVM) (Hofmann et al., 2008). We refer to an
overview of the graph kernel-based approaches in (Nikolentzos et al., 2019; Kriege et al., 2019).

Since  the  introduction  of  word  embedding  models  i.e.,  Word2Vec  (Mikolov  et  al.,  2013)  
and
Doc2Vec (Le & Mikolov, 2014), there have been several efforts attempted to apply them for the
graph classification task.   Deep Graph Kernel (DGK) (Yanardag & Vishwanathan, 2015) applies
Word2Vec  to  learn  embeddings  of  atomic  substructures  to  create  the  kernel  matrix.   
Graph2Vec
(Narayanan et al., 2017) employs Doc2Vec to obtain embeddings of entire graphs in order to train a
SVM classifier to perform classification. Anonymous Walk Embedding (AWE) (Ivanov & Burnaev,
2018) maps random walks into “anonymous walks” which are considered as word tokens, and then
utilizes Doc2Vec to achieve the graph embeddings to produce the kernel matrix.

In parallel, another recent line of work has focused on using deep neural networks to perform the
graph classification in a supervised manner. PATCHY-SAN (Niepert et al., 2016) adapts a graph la-
beling procedure to generate a fixed-length sequence of nodes from an input graph, and orders k-hop
neighbors for each node in the generated sequence according to their graph labelings; PATCHY-SAN
then selects a fixed number of ordered neighbors for each node and applies a convolutional neural
network to classify the input graph.  MPNN (Gilmer et al., 2017), DGCNN (Zhang et al., 2018b)
and DIFFPOOL (Ying et al., 2018) are end-to-end supervised models which share similar two-phase
process by (i) using stacked multiple graph convolutional layers (e.g., GCN layer (Kipf & Welling,

2


Under review as a conference paper at ICLR 2020

2017) or GraphSAGE layer (Hamilton et al., 2017a)) to aggregate node feature vectors, and (ii) ap-
plying a graph-level pooling layer (e.g., mean, max or sum pooling, sort pooling or differentiable
pooling) to obtain the graph embeddings which are then fed to a fully-connected layer followed by
a softmax layer to predict the graph labels.

Graph neural networks (GNNs) (Scarselli et al., 2009) aim to iteratively update the vector represen-
tation of each node by recursively propagating the representations of its neighbors using a 
recurrent
function until convergence. The recurrent function can be a neural network e.g., gated recurrent 
unit
(GRU) (Li et al., 2016), or multi-layer perceptron (MLP) (Xu et al., 2019).  Note that both stacked
GCN and GraphSAGE multiple layers can be seen as variants of the recurrent function in GNNs.
Other graph embedding models are briefly summarized in (Zhou et al., 2018; Zhang et al., 2018c;
Wu et al., 2019).

3    THE  PROPOSED  U2GNN

In this section, we detail how to construct our U2GNN and then present how U2GNN learns model
parameters to produce node and graph embeddings.

Figure 1: Illustration of our U2GNN learning process, e.g., for node 3 with d = 2 and N = 3.

Graph classification.  Given a set of graphs {G₁, G₂, ..., GN } ⊆ G and their corresponding class
labels {y₁, y₂, ..., yN } ⊆ Y, our U2GNN aims to learn a plausible embedding oG of each entire
graph G in order to predict its label y.

Each graph G is defined as G  =  (V, E , X),  where V is a set of nodes,  E is a set of edges,  and
X      R|V|×d  represents feature vectors of nodes.  In U2GNN, as illustrated in Figure 1, we use a
universal self-attention network (Dehghani et al., 2019) to learn a node embedding ov of each node
v       ∈ V, and then oG is simply returned by summing all learned node embeddings as follows:¹

oG = SUM ({ov, ∀v ∈ V})                                                           (1)

Constructing U2GNN. Formally, given an input graph   , we uniformly sample a set of N neighbors
for each v           ,  and then use node v  and its neighbors for the U2GNN learning process.²   
For
example, as illustrated in Figure 1, we generate a set of N = 3 neighbors   1, 2, 4   for node 3, 
and
then consider   3, 1, 2, 4   as an input to U2GNN where we leverage on the universal self-attention
network (Dehghani et al., 2019) to learn an effective embedding of node 3.

Intuitively,  the universal self-attention network can help to better aggregate feature vectors from
neighbors  of  a  given  node  to  produce  its  plausible  embedding.   In  particular,  each  
node  and  its
neighbors is transformed into a sequence of feature vectors which are then iteratively refined at 
each
timestep – using a self-attention mechanism (Vaswani et al., 2017) followed by a recurrent 
transition
(TRANS) with adding residual connection (He et al., 2016) and layer normalization (LNORM) (Ba
et al., 2016).

Given a node v ∈ V, we consider a sequence of (N + 1) nodes {vi}N +1  where v₁ = v and {vi}N +1

are N sampled neighbors of v. We obtain an input sequence of feature vectors {h⁰ }N +1  for which

¹The experimental results in (Xu et al., 2019) show that the sum pooling performs better than the 
mean and
max poolings.

²We sample a different set of neighbors at each training step.

3


Under review as a conference paper at ICLR 2020

h⁰   = Xv  ∈ Rd. In U2GNN, at each step t, we feed {hᵗ−¹}N +1  as an input sequence and produce

an output sequence {hᵗ }N +1  for which hᵗ  ∈ Rd as follows:


vi    i=1

vi

t       =    LNORM .xᵗ

+ TRANS .xᵗ ΣΣ                                   (2)

with  xᵗ     =    LNORM .hᵗ−¹ + ATT .hᵗ−¹ΣΣ                                 (3)

where TRANS(.) and ATT(.) denote a feed-forward network and a self-attention network respec-
tively as follows:

TRANS .xᵗ Σ = W₂ReLU .W₁xᵗ  + b₁Σ + b₂                                           (4)

where W₁ ∈ Rk×d and W₂ ∈ Rd×k  are weight matrices, and b₁ and b₂ are bias parameters, and:


ATT .

N +1

t   1

vi

j=1

αi,j

.V ht−1Σ

(5)

where V      Rd×d is a value-projection weight matrix; αi,j is an attention weight which is computed
using the softmax function over scaled dot products between the i-th and j-th input nodes:


αi,j = softmax 

vi                            vj

√d

                                        (6)

where Q ∈ Rd×d and K ∈ Rd×d are query-projection and key-projection matrices, respectively.³


Algorithm 1: The unsupervised learning process.

1   Input: G = (V, E , X)

2   for v ∈ V do

Algorithm 2: The supervised learning process.    

1   Input: G = (V, E , X) with its label y

2   for v ∈ V do


N +1

N +1

3              SAMPLE {vi}N +1  where {vi}N +1  are N


3              SAMPLE    vi  i=1    where   vi  i=2    are N

sampled neighbors of v₁ = v

4              for t = 1, 2, ..., T do

i=1

sampled neighbors of v₁ = v

4              for t = 1, 2, ..., T do

i=2


5                        ∀i ∈ {1, 2, ..., (N + 1)}

	

		

.       ΣΣ

5                        ∀i ∈ {1, 2, ..., (N + 1)}

	  

.       ΣΣ


6                        xᵗ  ← LNORM

ht−1  + ATT     t   1

6                            vi

← LNORM

hᵗ−¹ + ATT

.

ht−1

.    ΣΣ


8   ov ← hT , ∀v ∈ V

9   oG = SUM ({ov, ∀v ∈ V})                                      

T

v

9   y ← WoG + b                                                                

Learning parameters of U2GNN in “unsupervised” fashion:  After T  steps, we use the vector

representation  hT  to  infer  node  embeddings  ov  .   For  example,  as  shown  in  Figure  1,  
we  have

1

v₁  =  3,  v₂  =  1,  v₃  =  2  and v₄  =  4,  and then consider hT  to infer o₃.   We learn our 
model

parameters (including the weight matrices and biases as well as node embeddings ov) by minimizing

the sampled softmax loss function (Jean et al., 2015) applied to node v ∈ V as follows:

       exp(ov · hT )


LU₂GNN (v) = − log

where V′ is a subset sampled from V.

v'∈V'  exp(ov'  · hv )

(7)

We briefly describe the general learning process of our proposed U2GNN model in Algorithm 1.

Here, the learned node embeddings ov are used as the final representations of nodes v         .  
After

that, we obtain the plausible embedding o    of the graph    by summing all learned node embeddings

as mentioned in Equation 1.  Finally, we employ a logistic regression classifier from LIBLINEAR

(Fan et al., 2008) to predict the graph labels.

³It is important to note that GAT (Velicˇkovic´  et al., 2018) borrows the standard attention 
technique from

(Bahdanau et al., 2015) in using a single-layer feedforward neural network parametrized by a weight 
vector and

then applying the LeakyReLU non-linearity function followed by the softmax function to compute 
importance

weights of neighbors of a given node.  Therefore, GAT is much different with the self-attention 
mechanism.

More information about GCN, GraphSAGE and GAT can be found in Appendix A.

4


Under review as a conference paper at ICLR 2020

Learning parameters of U2GNN in “supervised” fashion:  We do not need to learn the node

embeddings separately,  hence after T  steps,  we use hT  as the final embedding of node v₁  ∈  V

(i.e., ov  =  hT ).  We then feed o    to a fully connected layer to predict the graph labels as 
briefly
presented in Algorithm 2.  Finally, we learn the model parameters by minimizing the cross entropy
loss function.

Proof: In general, on the node level, each node and its neighbors are iteratively attended in the 
recur-
rent process with weight matrices shared across timesteps and iterations, thus U2GNN can memorize
the potential dependencies among nodes within substructures.  On the graph level, U2GNN views
the shared weight matrices as memories to access the updated node-level information from previ-
ous iterations to further aggregate broader dependencies among substructures into implicit graph
representations in subsequent iterations. Therefore, U2GNN is advantageous to capture both global
and        local graph structures to learn effective node and graph embeddings, leading to 
state-of-the-art
performances for the graph classification task.

4    EXPERIMENTAL  SETUP

We prove the effectiveness of our U2GNN on the graph classification task using a range of well-
known benchmark datasets for both supervised and unsupervised fashions.

4.1    DATASETS

We use 9 well-known datasets consisting of 5 social network datasets (COLLAB, IMDB-B, IMDB-
M, RDT-B and RDT-M5K) (Yanardag & Vishwanathan, 2015) and 4 bioinformatics datasets (DD,
MUTAG, PROTEINS and PTC). We follow (Niepert et al., 2016; Zhang et al., 2018b) to use node
degrees as features on all social network datasets as these datasets do not have available node fea-
tures. Table 1 reports the statistics of these datasets.


Dataset           #G         #Cls     A.NG     A.NN     d

COLLAB        5,000        3         74.5      65.9        –

IMDB-B         1,000        2         19.8      9.8          –

IMDB-M         1,500        3         13.0      10.1        –

RDT-B            2,000        2        429.6     2.3          –

RDT-M5K      5,000        5        508.5     2.3          –

DD                  1,178        2        284.3     5.0          82

MUTAG          188           2         17.9      2.2          7

PROTEINS     1,113        2         39.1      3.7          3

PTC                 344           2         25.6      2.0          19

Table  1:  Statistics  of  the  experimental  benchmark
datasets.  #G denotes the numbers of graphs.  #Cls
denotes the number of graph classes. A.NG denotes
the average number of nodes per graph.  A.NN de-
notes the average number of neighbors per node. d is
the dimension of node feature vectors (i.e. the num-
ber of node labels).

thread belongs to.

Social  networks  datasets.   COLLAB  is  a
scientific  dataset  where  each  graph  repre-
sents  a  collaboration  network  of  a  corre-
sponding  researcher  with  other  researchers
from each of 3 physics fields; and each graph
is labeled to a physics field the researcher be-
long to.   IMDB-B and IMDB-M are movie
collaboration  datasets  where  each  graph  is
derived  from  actor/actress  and  genre  infor-
mation  of  different  movies  on  IMDB,  in
which nodes correspond to actors/actresses,
and each edge represents a co-appearance of
two actors/actresses in the same movie; and
each  graph  is  assigned  to  a  genre.    RDT-
B and RDT-M5K are datasets derived from
Reddit community, in which each online dis-
cussion  thread  is  viewed  as  a  graph  where
nodes  correspond  to  users,  two  users  are
linked  if  at  least  one  of  them  replied  to
another’s  comment;  and  each  graph  is  la-
beled to a sub-community the corresponding

Bioinformatics datasets. DD (Dobson & Doig, 2003) is a collection of 1,178 protein network struc-
tures with 82 discrete node labels, where each graph is classified into enzyme or non-enzyme class.
PROTEINS comprises 1,113 graphs obtained from (Borgwardt et al., 2005) to present secondary
structure elements (SSEs).  MUTAG (Debnath et al., 1991) is a collection of 188 nitro compound
networks with 7 discrete node labels, where classes indicate a mutagenic effect on a bacterium. PTC
(Toivonen et al., 2003) consists of 344 chemical compound networks with 19 discrete node labels
where classes show carcinogenicity for male and female rats.

5


Under review as a conference paper at ICLR 2020

4.2    TRAINING PROTOCOL TO LEARN GRAPH EMBEDDINGS

Coordinate embedding:  The relative coordination among nodes might provide meaningful infor-
mation about graph structure. We follow Dehghani et al. (2019) to associate each position i at step
t     a pre-defined coordinate embedding pᵗ using the sinusoidal functions (Vaswani et al., 2017), 
thus
we can change Equation 3 in Section 3 to:

xᵗ     =    LNORM .hᵗ−¹ + ATT .hᵗ−¹ + pᵗΣΣ                             (8)


with  pᵗ

t
i,2j+1

=    sin(i/10000²ʲ/ᵈ) + sin(t/10000²ʲ/ᵈ)

=    cos(i/10000²ʲ/ᵈ) + cos(t/10000²ʲ/ᵈ)

From the preliminary experiments, adding coordinate embeddings enhances classification results on
MUTAG and PROTEINS in the unsupervised fashion, hence we use the coordinate embeddings
only for these two datasets.

Hyper-parameter setting to learn our model parameters for all experimental datasets: (i) Re-
garding the unsupervised fashion, we fix the hidden size of the feed-forward network in Equation
4 to 1024 (k  =  1024),  and the number of samples in the sampled loss function    U₂GNN to 512
(    ′  = 512) in Equation 7.  We set the batch size to 512 for COLLAB, DD and RDT-B; 1024 for
RDT-M5K; and 128 for remaining datasets.  (ii) Regarding the supervised fashion, we vary the the

hidden size k in   32, 128, 512, 1024   and the batch size in   1, 2, 4  . (iii) For both the 
unsupervised
and supervised fashions, we use the number N of neighbors sampled for each node from   4, 8, 16
and the number T of steps from   1, 2, 3, 4, 5, 6  .  We apply the Adam optimizer (Kingma & Ba,
2014) to train our U2GNN model and apply a grid search to select the Adam initial learning rate
lr ∈ {5e−⁵, 1e−⁴, 5e−⁴, 1e−³}. We run up to 50 epochs and evaluate the model as in what follows.

4.3    EVALUATION PROTOCOL

For each dataset, after obtaining the graph embeddings, we perform the same evaluation process
from (Yanardag & Vishwanathan, 2015; Niepert et al., 2016; Zhang et al., 2018b; Xu et al., 2019;
Xinyi & Chen, 2019), which is using 10-fold cross-validation scheme to calculate the classification
performance for a fair comparison.⁴

Baseline models:    We compare our U2GNN with up-to-date strong baselines as follows:

Unsupervised  approaches:   Graphlet  Kernel  (GK)  (Shervashidze  et  al.,  2009),  Weisfeiler-
Lehman kernel (WL) (Shervashidze et al., 2011), Deep Graph Kernel (DGK) (Yanardag & Vish-
wanathan, 2015) and Anonymous Walk Embedding (AWE) (Ivanov & Burnaev, 2018).

Supervised  approaches:  PATCHY-SAN  (PSCN)  (Niepert  et  al.,  2016),  Graph  Convolutional
Network  (GCN)  (Kipf  &  Welling,  2017),  GraphSAGE  (Hamilton  et  al.,  2017a),  Deep  Graph
CNN (DGCNN) (Zhang et al., 2018b), Graph Capsule Convolution Neural Network (GCAPS)
(Verma  &  Zhang,  2018),  Capsule  Graph  Neural  Network  (CapsGNN)  (Xinyi  &  Chen,  2019),
Graph Isomorphism Network (GIN) (Xu et al., 2019), Graph Feature Network (GFN) (Chen et al.,
2019), Invariant-Equivariant Graph Network (IEGN) (Maron et al., 2019b), Provably Powerful
Graph Network (PPGN) (Maron et al., 2019a) and Discriminative Structural Graph Classification
(DSGC) (Seo et al., 2019).

We report the baseline results taken from the original papers or published in (Ivanov & Burnaev,
2018; Verma & Zhang, 2018; Xinyi & Chen, 2019; Chen et al., 2019; Seo et al., 2019).

5    EXPERIMENTAL  RESULTS

Table 2 presents the experimental results on the 9 benchmark datasets.  We use “Unsupervised” to
denote the unsupervised graph embedding models that can access all nodes from the whole dataset,
but not the graph labels;  and we use “Supervised” to denote the supervised models that use the
graph labels of training graphs during training.

⁴Regarding the unsupervised fashion, we use the logistic regression classifier from LIBLINEAR (Fan 
et al.,
2008) with setting the termination criterion to 0.001.

6


Under review as a conference paper at ICLR 2020

In general, our unsupervised U2GNN achieves state-of-the-art performances on a range of bench-
marks for the graph classification task, hence this demonstrates a high impact of the unsupervised
U2GNN in inferring the plausible node and graph embeddings.  It is to note that several existing
works (Xinyi & Chen, 2019; Xu et al., 2019; Chen et al., 2019; Maron et al., 2019b; Seo et al.,
2019) compared the supervised models with the unsupervised models together due to the use of
the same 10-fold cross-validation scheme.  However, our unsupervised U2GNN significantly out-
performs the supervised methods in most of benchmark cases by a large margin of 18+%, thus we
suggest that future GNN works should separate two training fashions.


Model

GK (2009)

WL (2011)

DGK (2015)

AWE (2018)
U2GNN
DSGC (2019)

GFN (2019)
PPGN (2019a)
GIN (2019)
IEGN (2019b)
CapsGNN (2019)

GCAPS (2018)
DGCNN (2018b)
GCN (2017)

GraphSAGE (2017a)
PSCN (2016)
U2GNN

COLLAB

72.84 ± 0.28

79.02 ± 1.77

73.09 ± 0.25

73.93 ± 1.94

95.62 ± 0.92

79.20 ± 1.60

81.50 ± 2.42

81.38 ± 1.42

80.20 ± 1.90

77.92 ± 1.70

79.62 ± 0.91

77.71 ± 2.51

73.76 ± 0.49

81.72 ± 1.64

79.70 ± 1.70

72.60 ± 2.15

77.84 ± 1.48

IMDB-B

65.87 ± 0.98

73.40 ± 4.63

66.96 ± 0.56

74.45 ± 5.83

93.50 ± 2.27

73.20 ± 4.90

73.00 ± 4.35

73.00 ± 5.77

75.10 ± 5.10

71.27 ± 4.50

73.10 ± 4.83

71.69 ± 3.40

70.03 ± 0.86

73.30 ± 5.29

72.40 ± 3.60

71.00 ± 2.29

79.40 ± 4.35

IMDB-M

43.89 ± 0.38

49.33 ± 4.75

44.55 ± 0.52

51.54 ± 3.61

74.80 ± 4.11

48.50 ± 4.80

51.80 ± 5.16

50.46 ± 3.59

52.30 ± 2.80

48.55 ± 3.90

50.27 ± 2.65

48.50 ± 4.10

47.83 ± 0.85

51.20 ± 5.13

49.90 ± 5.00

45.23 ± 2.84

56.20 ± 3.35

RDT-B

77.34 ± 0.18

81.10 ± 1.90

78.04 ± 0.39

87.89 ± 2.53

84.80 ± 1.53

92.20 ± 2.40

–

–

92.40 ± 2.50

–

–
87.61    ± 2.51

76.02 ± 1.73

–
89.10    ± 1.90

86.30 ± 1.58

80.25 ± 2.38

RDT-M5K

41.01 ± 0.17

49.44 ± 2.36

41.27 ± 0.18

50.46 ± 1.91

77.25 ± 1.46

–

57.59 ± 2.40

–
57.50    ± 1.50

–

50.46 ± 1.91

50.10 ± 1.72

48.70 ± 4.54

56.81 ± 2.37

–
49.10    ± 0.70

50.90 ± 2.31


Model

GK (2009)

WL (2011)

DGK (2015)

AWE (2018)
U2GNN
DSGC (2019)

GFN (2019)
PPGN (2019a)
GIN (2019)
IEGN (2019b)
CapsGNN (2019)

GCAPS (2018)
DGCNN (2018b)
GCN (2017)

GraphSAGE (2017a)
PSCN (2016)
U2GNN

DD

78.45 ± 0.26

79.78 ± 0.36

73.50 ± 1.01

71.51 ± 4.02

95.67 ± 1.89

77.40 ± 6.40

78.78 ± 3.49

–

–

–
75.38    ± 4.17

77.62 ± 4.99

79.37 ± 0.94

79.12 ± 3.07

65.80 ± 4.90

77.12 ± 2.41

81.24 ± 1.84

PROTEINS

71.67 ± 0.55

74.68 ± 0.49

75.68 ± 0.54

–

78.07 ± 3.36

74.20 ± 3.80

76.46 ± 4.06

77.20 ± 4.73

76.20 ± 2.80

75.19 ± 4.30

76.28 ± 3.63

76.40 ± 4.17

75.54 ± 0.94

75.65 ± 3.24

65.90 ± 2.70

75.89 ± 2.76

78.53 ± 4.07

MUTAG

81.58 ± 2.11

82.05 ± 0.36

87.44 ± 2.72

87.87 ± 9.76

81.34 ± 6.56

86.70 ± 7.60

90.84 ± 7.22

90.55 ± 8.70

89.40 ± 5.60

84.61 ± 10.0

86.67 ± 6.88

–
85.83    ± 1.66

87.20 ± 5.11

79.80 ± 13.90

92.63 ± 4.21

89.97 ± 3.65

PTC

57.26 ± 1.41

57.97 ± 0.49

60.08 ± 2.55

–

84.59 ± 5.12

–

–
66.17    ± 6.54

64.60 ± 7.00

59.47 ± 7.30

–
66.01    ± 5.91

58.59 ± 2.47

–

–
62.29    ± 5.68

79.36 ± 4.06

Table 2: Graph classification results (% accuracy). The best scores are in bold.

Regarding our supervised U2GNN, on the social network datasets, our model produces new state-
of-the-art performances on IMDB-B and IMDB-M, especially U2GNN significantly achieves 4+%
absolute higher accuracies than all supervised baselines.  In addition, U2GNN obtains promising
scores on COLLAB, RDT-B and RDT-M5K. On the bioinformatics datasets, our U2GNN obtains
new highest accuracies on DD, PROTEINS and PTC. Moreover, U2GNN produces a competitive
accuracy compared with those of baseline models on MUTAG, for which there are no significant
differences between our supervised U2GNN and some supervised baselines (e.g., GFN, GIN and

7


Under review as a conference paper at ICLR 2020

PPGN). Note that there are only 188 graphs in the MUTAG dataset, which explains the high variance
in the results.

COLLAB                                                                                              
                         IMDB-M                                                                     
                                          DD


95.5

95.0

94.5

1                                                      1

2                                  74             2

3                                                      3

4                                                      4

5                                                      5

6                                  72             6

1

2

95                                                                                                  
        3

4

5

6

94

94.0                                                                                                
                                                          70

93

93.5                                                                                                
                                                          68


93.0

92.5

10          15          20          25          30          35          40          45          50

Epoch

66

15          20          25          30          35          40          45          50

Epoch

92

91

10                  20                  30                  40                  50

Epoch

COLLAB                                                                                              
                         IMDB-M                                                                     
                                          DD


95.5

95.0

94.5

4                                                        4

8                                     74             8

16                                                      16

72

70

4

95.5                                                                                                
                   8

16

95.0

94.5

94.0


94.0

93.5

68                                                                                                  
                                      93.5

93.0

66

92.5


93.0

10          15          20          25          30          35          40          45          50

Epoch

64

15          20          25          30          35          40          45          50

Epoch

92.0

91.5

10                    20                    30                    40                    50

Epoch

Figure 2:  Effects of the number of steps (T ) (in the 3 top figures), and the number of neighbors
(N ) sampled for each node (in the 3 bottom figures) in the unsupervised fashion.  Regarding each
dataset, for all 10 folds, we vary the value of either T  or N while using the same fixed values of
other hyper-parameters.

Next we investigate the effects of hyper-parameters on the experimental datasets in the unsupervised
fashion in Figure 2.⁵  In general, our unsupervised U2GNN could consistently obtain better results
than those of baselines with any value of T and N , as long as the training process is stopped 
precisely
for all datasets.  In particular, we find that higher T  helps on most of the datasets, and 
especially
boosts the performance on bioinformatics data. A possible reason is that the bioinformatics datasets
comprise sparse networks where the average number of neighbors per node is below 5 as shown
in Table 1, hence we need to use more steps to learn graph properties.  In addition, using small N
generally produces higher performances on the bioinformatics datasets, while using higher values
of  N  is  more  suitable  for  the  social  network  datasets.   Besides,  the  social  network  
datasets  are
much denser than the bioinformatics datasets, thus this is reason why we should use more sampled
neighbors on the social networks rather than the bioinformatics ones. Note that the similar findings
also occur in the supervised fashion.


150

Node embedding

150

Graph embedding

100                                                                                                 
                                    100

50                                                                                                  
                                     50

0                                                                                                   
                                      0

50                                                                                                  
                                     50


100

100


150

100              50              0                50              100             150

150

100           0             100           200           300           400

Figure 3: A visualization of the node and graph embeddings learned by our unsupervised U2GNN
on the DD dataset. We do not include the embedding visualization of other baselines because those
methods are significantly different from our unsupervised U2GNN.

To qualitatively demonstrate the effectiveness of capturing the local and global graph properties,
we use t-SNE (Maaten & Hinton, 2008) to visualize the learned node and graph embeddings in the

⁵More figures can be found in Appendix B.

8


Under review as a conference paper at ICLR 2020

unsupervised fashion on the DD dataset where the node labels are available.  It can be seen from
Figure 3 that our unsupervised U2GNN can effectively capture the local structure wherein the nodes
are clustered according to the node labels, and the global structure wherein the graph embeddings 
are
well-separated from each other; verifying the plausibility of the learned node and graph 
embeddings.

6    CONCLUSION

In this paper, we introduce a novel graph neural network model U2GNN for the graph classification
task.  Given an input graph,  U2GNN applies a self-attention mechanism followed by a recurrent
transition to learn the node embeddings and then sums all the learned node embeddings to obtain
the embedding of the entire graph.   Our U2GNN achieves new highest accuracies on most of 9
well-known benchmark datasets in both the supervised and unsupervised fashions, using the same
10-fold cross-validation scheme, against the up-to-date unsupervised and supervised baselines.  In
future works, we plan to investigate the effectiveness of U2GNN on other important tasks such as
node classification and link prediction. Our code is available at: https://anonymous-url/.

REFERENCES

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv:1607.06450,
2016.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.   Neural machine translation by jointly
learning to align and translate. ICLR, 2015.

Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz  Malinowski,  Andrea  Tacchetti,  David  Raposo,  Adam  Santoro,  Ryan  Faulkner,  et  al.
Relational inductive biases, deep learning, and graph networks. arXiv:1806.01261, 2018.

Karsten M. Borgwardt and Hans-Peter Kriegel.   Shortest-Path Kernels on Graphs.   In ICDM, pp.
74–81, 2005.

Karsten M Borgwardt, Cheng Soon Ong, Stefan Scho¨nauer, SVN Vishwanathan, Alex J Smola, and
Hans-Peter Kriegel.  Protein function prediction via graph kernels.  Bioinformatics, 21(suppl 1):
i47–i56, 2005.

Ting Chen, Song Bian, and Yizhou Sun. Are powerful graph neural nets necessary? a dissection on
graph classification. arXiv:1905.04579, 2019.

Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Cor-
win Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro com-
pounds. Correlation with molecular orbital energies and hydrophobicity.   Journal of medicinal
chemistry, 34(2):786–797, 1991.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser.  Universal
Transformers. ICLR, 2019.

Paul D Dobson and Andrew J Doig.  Distinguishing enzyme structures from non-enzymes without
alignments. Journal of molecular biology, 330(4):771–783, 2003.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.  LIBLINEAR:
A Library for Large Linear Classification. Journal of Machine Learning Research, 9:1871–1874,
2008.

Thomas Ga¨rtner, Peter Flach, and Stefan Wrobel.  On graph kernels: Hardness results and efficient
alternatives. In Learning Theory and Kernel Machines, pp. 129–143. 2003.

Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl.  Neural
Message Passing for Quantum Chemistry. In ICML, pp. 1263–1272, 2017.

William  L.  Hamilton,  Rex  Ying,  and  Jure  Leskovec.   Inductive  representation  learning  on  
large
graphs. In NIPS, pp. 1024–1034, 2017a.

9


Under review as a conference paper at ICLR 2020

William L. Hamilton, Rex Ying, and Jure Leskovec.  Representation learning on graphs:  Methods
and applications. arXiv:1709.05584, 2017b.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, pp. 770–778, 2016.

Thomas Hofmann, Bernhard Scho¨lkopf, and Alexander J Smola. Kernel methods in machine learn-
ing. The annals of statistics, pp. 1171–1220, 2008.

Sergey Ivanov and Evgeny Burnaev. Anonymous walk embeddings. In ICML, pp. 2191–2200, 2018.

Se´bastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target
vocabulary for neural machine translation. In ACL, pp. 1–10, 2015.

Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi. Marginalized kernels between labeled graphs.
In ICML, pp. 321–328, 2003.

Diederik Kingma and Jimmy Ba.  Adam:  A method for stochastic optimization.  arXiv:1412.6980,
2014.

Thomas N. Kipf and Max Welling.   Semi-supervised classification with graph convolutional net-
works. In ICLR, 2017.

Nils  M  Kriege,  Fredrik  D  Johansson,  and  Christopher  Morris.     A  survey  on  graph  
kernels.

arXiv:1903.11835, 2019.

Quoc Le and Tomas Mikolov.  Distributed representations of sentences and documents.  In ICML,
pp. 1188–1196, 2014.

Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel.  Gated Graph Sequence Neural
Networks. ICLR, 2016.

Laurens van der Maaten and Geoffrey Hinton.   Visualizing data using t-sne.   Journal of machine
learning research, 9:2579–2605, 2008.

Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman.  Provably powerful graph
networks. arXiv:1905.11136, 2019a.

Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. ICLR, 2019b.

Tomas  Mikolov,  Ilya  Sutskever,  Kai  Chen,  Gregory  S.  Corrado,  and  Jeffrey  Dean.   
Distributed
representations of words and phrases and their compositionality. In NIPS, pp. 3111–3119, 2013.

Annamalai  Narayanan,   Mahinthan  Chandramohan,   Rajasekar  Venkatesan,   Lihui  Chen,   Yang
Liu,   and  Shantanu  Jaiswal.      graph2vec:    Learning  distributed  representations  of  
graphs.
arXiv:1707.05005, 2017.

Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov.  Learning Convolutional Neural Net-
works for Graphs. In ICML, pp. 2014–2023, 2016.

Giannis  Nikolentzos,  Giannis  Siglidis,  and  Michalis  Vazirgiannis.    Graph  kernels:   A  
survey.

arXiv:1904.12218, 2019.

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. DeepWalk: Online Learning of Social Represen-
tations. In SIGKDD, pp. 701–710, 2014.

Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. In NIPS,
pp. 3859–3869, 2017.

Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2009.

Younjoo Seo, Andreas Loukas, and Nathanael Peraudin.  Discriminative structural graph classifica-
tion. arXiv:1905.13422, 2019.

10


Under review as a conference paper at ICLR 2020

Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten Borgwardt.  Ef-
ficient Graphlet Kernels for Large Graph Comparison. In AISTATS, pp. 488–495, 2009.

Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M Borg-
wardt.  Weisfeiler-lehman graph kernels.  Journal of Machine Learning Research, 12(Sep):2539–
2561, 2011.

Hannu Toivonen, Ashwin Srinivasan, Ross D King, Stefan Kramer, and Christoph Helma. Statistical
evaluation of the predictive toxicology challenge 2000–2001. Bioinformatics, 19(10):1183–1193,
2003.

Ashish  Vaswani,  Noam  Shazeer,  Niki  Parmar,  Jakob  Uszkoreit,  Llion  Jones,  Aidan  N  Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 5998–6008, 2017.

Petar Velicˇkovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio`, and Yoshua
Bengio. Graph Attention Networks. ICLR, 2018.

Saurabh Verma and Zhi-Li Zhang. Graph capsule convolutional neural networks. arXiv:1805.08090,
2018.

S Vichy N Vishwanathan, Nicol N Schraudolph, Risi Kondor, and Karsten M Borgwardt.  Graph
kernels. Journal of Machine Learning Research, 11(Apr):1201–1242, 2010.

Zonghan Wu,  Shirui Pan,  Fengwen Chen,  Guodong Long,  Chengqi Zhang,  and Philip S Yu.   A
comprehensive survey on graph neural networks. arXiv:1901.00596, 2019.

Zhang Xinyi and Lihui Chen. Capsule Graph Neural Network. ICLR, 2019.

Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.   How Powerful Are Graph Neural
Networks?  ICLR, 2019.

Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In SIGKDD, pp. 1365–1374, 2015.
Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, and Jure Leskovec.

Hierarchical graph representation learning with differentiable pooling.   In NeurIPS, pp. 4805–
4815, 2018.

Daokun Zhang, Jie Yin, Xingquan Zhu, and Chengqi Zhang.  Network representation learning:  A
survey. IEEE Transactions on Big Data, to appear, 2018a.

Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen.  An End-to-End Deep Learning
Architecture for Graph Classification. In AAAI, 2018b.

Ziwei Zhang, Peng Cui, and Wenwu Zhu.  Deep learning on graphs: A survey.  arXiv:1812.04202,
2018c.

Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Graph neural
networks: A review of methods and applications. arXiv:1812.08434, 2018.

A    GNN VARIANTS

Regarding GCN (Kipf & Welling, 2017), each node v ∈ V has a feature vector xv ∈ Rd1 , and GCN
infers a new vector representation hv ∈ Rd2  for node v from its neighbors as follows:


hv = g 

u∈Nv∪{v}

(W xu + b) , ∀v ∈ V                                (9)

where W       Rd2 ×d1   and b      Rd2   are a weight matrix and a bias respectively, g  is a 
non-linear
activation function, and    v is the set of neighbors of node v.  GCN can indirectly capture nodes 
at
many hops away by using multiple GCN layers stacked on top of each other as follows:


(k+1)

v

u∈Nv∪{v}

.W ⁽ᵏ⁾h⁽ᵏ⁾ + b⁽ᵏ⁾Σ , ∀v ∈ V                       (10)

11


Under review as a conference paper at ICLR 2020

where k is the layer index, and h⁽⁰⁾ = x  . The vector outputs of the last GCN layer are used as 
the

u              u

final vector representations of nodes.  GCN can be jointly trained with a classification classifier 
by
minimizing the cross-entropy loss function.

GraphSAGE (Hamilton et al., 2017a) is an extension of GCN, in which the representation h⁽ᵏ⁺¹⁾ of

node v after k-th GraphSAGE layer is produced as follows:


h(k+1)  = g .W (k)

Σh⁽ᵏ⁾ ⊕ h⁽ᵏ⁺¹⁾ΣΣ , ∀v ∈ V                          (11)

where [⊕] denotes a vector concatenation operation, and h⁽ᵏ⁺¹⁾ can be computed in several ways


such as using the GCN computation as follows:

h(k+1)  = g . Σ .W (k)

Nv

h⁽ᵏ⁾ + b⁽ᵏ⁾ΣΣ , ∀v ∈ V                         (12)

or applying an element-wise max-pooling operation as follows:

h⁽ᵏ⁺¹⁾ = max .{g .Wpₒₒlh⁽ᵏ⁾ + bΣ , ∀u ∈ Nv}Σ                             (13)

In addition, h⁽ᵏ⁺¹⁾ can be computed by taking the mean (or sum) of all vectors in {h⁽ᵏ⁾, ∀u ∈ N }.

v is defined as a fixed-size, uniformly drawn from the set of all neighbor nodes of v, and uniformly
sampled differently through each layer.  The vector outputs of the last GraphSAGE layer are used
to infer node embeddings.  GraphSAGE employs the random walk algorithm (Perozzi et al., 2014)
to  generate  context  nodes  for  each  target  node  and  then  adapts  the  negative  sampling  
technique
(Mikolov et al., 2013) to learn the model parameters.

GAT (Velicˇkovic´ et al., 2018) extends GCN in assigning importance weights to neighbors of a given
node by exploring the standard attention technique (Bahdanau et al., 2015).  GAT induces a new
vector representation hv of node v from its neighbors as follows:


hv = g 

u∈Nv∪{v}

τv,uW xu , ∀v ∈ V                                (14)


where τv,u is an importance weight which is computed as follows:

exp .LeakyReLU .aT [W xv ⊕ W xu]ΣΣ

(15)


τv,u  =  Σ

u'∈Nv

exp (LeakyReLU (aT [W xv

⊕ W xu' ]))

B    EFFECTS  OF  HYPER-PARAMETERS

12


Under review as a conference paper at ICLR 2020


1

93              2

3

4

92              5

6

91

IMDB-B

1

84              2

3

4

5

82

6

REDDIT-B

78              1

2

3

4

76              5

6

REDDIT-M5K

80                                                                                                  
                                          74

90

89                                                                                                  
                                             78

72

88

76

87                                                                                                  
                                                                                                    
                                                                                         70


15           20           25           30           35           40           45           50

Epoch

10         15         20         25         30         35         40         45         50

Epoch

10         15         20         25         30         35         40         45         50

Epoch


77.5

75.0

72.5

70.0

PROTEINS

82

1                                                       1

2                                                       2

80

3                                                       3

4                                                       4

5                                   78              5

6                                                       6

76

MUTAG

85.0

82.5

80.0

77.5

PTC

1

2

3

4

5

6


67.5

75.0

74


65.0

62.5

60.0

57.5

0                    10                   20                   30                   40              
     50

Epoch

72

70

68

10         15         20         25         30         35         40         45         50

Epoch

72.5

70.0

67.5

65.0

0                    10                   20                   30                   40              
     50

Epoch

Figure 4:  Effects of the number of steps (T ) in the unsupervised fashion.  Regarding each dataset,
for all 10 folds, we vary the value of T while using the same fixed values of other 
hyper-parameters.


4

93              8

16

92

91

IMDB-B

4

84              8

16

82

REDDIT-B

78              4

8

77              16

76

75

REDDIT-M5K

90                                                                                                  
                                             80                                                     
                                                                                          74

73

89

78

72

88

71

76

87

70


15           20           25           30           35           40           45           50

Epoch

10         15         20         25         30         35         40         45         50

Epoch

10         15         20         25         30         35         40         45         50

Epoch


PROTEINS

75

4                                                         4

8                                                         8

16                                   80              16

MUTAG

85.0

82.5

PTC

4

8

16


70                                                                                                  
                                                             78

80.0

77.5


65                                                                                                  
                                                             76

75.0


60                                                                                                  
                                                             74

72.5

70.0


55                                                                                                  
                                                             72

67.5


0                    10                   20                   30                   40              
     50

Epoch

15           20           25           30           35           40           45           50

Epoch

65.0

0                    10                   20                   30                   40              
     50

Epoch

Figure 5: Effects of the number of neighbors (N ) sampled for each node in the unsupervised fashion.
Regarding each dataset, for all 10 folds, we vary the value of N while using the same fixed values
of   other hyper-parameters.

13

