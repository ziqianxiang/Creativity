Under review as a conference paper at ICLR 2020
The Convex Information B ottleneck La-
GRANGIAN
Anonymous authors
Paper under double-blind review
Ab stract
The information bottleneck (IB) problem tackles the issue of obtaining relevant
compressed representations T of some random variable X for the task of predict-
ing Y . It is defined as a constrained optimization problem which maximizes the
information the representation has about the task, I(T; Y ), while ensuring that
a minimum level of compression r is achieved (i.e., I(X; T) ≤ r). For practi-
cal reasons the problem is usually solved by maximizing the IB Lagrangian (i.e.,
LIβB (T) = I(T; Y ) - βI(X; T)) for many values of β ∈ [0, 1], therefore draw-
ing the IB curve (i.e., the curve of maximal I(T; Y ) for a given I(X; Y )) and
selecting the representation of desired predictability and compression. It is known
when Y is a deterministic function of X, the IB curve cannot be explored and
other Lagrangians have been proposed to tackle this problem (e.g., the squared IB
Lagrangian: Lsβqs-qIB(T) = I(T; Y ) - βsqI(X; T)2)). In this paper we (i) present
a general family of Lagrangians which allow for the exploration of the IB curve
in all scenarios; (ii) prove that if these Lagrangians are used, there is a one-to-one
mapping between the Lagrange multiplier and the desired compression rate r for
known IB curve shapes, hence, freeing from the burden of solving the optimiza-
tion problem for many values of the Lagrange multiplier.
1	Introduction
Let X and Y be two statistically dependent random variables with joint distribution p(x, y). The
information bottleneck (IB) (Tishby et al., 2000) investigates the problem of extracting the relevant
information from X for the task of predicting Y .
For this purpose, the IB defines a bottleneck variable T obeying the Markov chain Y 什 X 什 T
so that T acts as a representation of X . Tishby et al. (2000) define the relevant information as the
information the representation keeps from Y after the compression of X (i.e., I(T; Y)), provided a
minimum level of compression (i.e, I(X ; T) ≤ r). Therefore, we select the representation which
yields the value of the IB curve that best fits our requirements.
Definition 1 (IB functional). Let X and Y be statistically dependent variables. Let ∆ be the set of
random variables T obeying the Markov condition Y 什 X 什 T. Then the IBfunctional is
FIB,max(r) = max{I(T;Y)} s.t. I(X; T) ≤ r, ∀r ∈ [0, ∞).
(1)
Definition 2 (IB curve). The IB curve is the set of points defined by the solutions of FIB,max (r) for
varying values ofr ∈ [0, ∞).
Definition 3 (Information plane). The plane is defined by the axes I(T; Y) and I(X; T).
In practice, solving a constrained optimization problem such as the IB functional is difficult. Thus,
in order to avoid the non-linear constraints from the IB functional the IB Lagrangian is defined.
Definition 4 (IB Lagrangian). Let X and Y be statistically dependent variables. Let ∆ be the
set of random variables T obeying the Markov condition Y 什 X 什 T. Then we define the IB
Lagrangian as
LIB(T) = I(T; Y) - βI(X; T).
(2)
1
Under review as a conference paper at ICLR 2020
Here β ∈ [0, 1] is the Lagrange multiplier which controls the trade-off between the information of
Y retained and the compression of X. Note we consider β ∈ [0, 1] because (i) for β ≤ 0 many
uncompressed solutions such as T = X maximizes LIβB, and (ii) for β ≥ 1 the IB Lagrangian
is non-positive due to the data processing inequality (DPI) (Theorem 2.8.1 from Cover & Thomas
(2012)) and trivial solutions like T = const are maximizers with LIβB = 0 (Kolchinsky et al., 2019).
We know the solutions of the IB Lagrangian optimization (if existent) are solutions of the IB func-
tional by the Lagrange’s sufficiency theorem (Theorem 5 in Appendix A of Courcoubetis (2003)).
Moreover, since the IB functional is concave (Lemma 5 of Gilad-Bachrach et al. (2003)) we know
they exist (Theorem 6 in Appendix A of Courcoubetis (2003)).
Therefore, the problem is usually solved by maximizing the IB Lagrangian with adaptations of
the Blahut-Arimoto algorithm (Tishby et al., 2000), deterministic annealing approaches (Tishby &
Slonim, 2001) or a bottom-up greedy agglomerative clustering (Slonim & Tishby, 2000) or its im-
proved sequential counterpart (Slonim et al., 2002). However, when provided with high-dimensional
random variables X such as images, these algorithms do not scale well and deep learning based
techniques, where the IB Lagrangian is used as the objective function, prevailed (Alemi et al., 2017;
Chalk et al., 2016; Kolchinsky et al., 2017).
Note the IB Lagrangian optimization yields a representation T with a given performance
(I(X; T), I(T; Y )) for a given β. However there is no one-to-one mapping between β and I(X; T).
Hence, we cannot directly optimize for a desired compression level r but we need to perform several
optimizations for different values of β and select the representation with the desired performance
(e.g., Alemi et al. (2017)). The Lagrange multiplier selection is important since (i) sometimes even
choices of β < 1 lead to trivial representations such that pT |X (t|x) = pT (t), and (ii) there exist
some discontinuities on the performance level w.r.t. the values of β (Wu et al., 2019).
Moreover, recently Kolchinsky et al. (2019) showed how in deterministic scenarios (such as many
classification problems where an input xi belongs to a single particluar class yi) the IB Lagrangian
could not explore the IB curve. Particularly, they showed that multiple β yielded the same perfor-
mance level and that a single value of β could result in different performance levels. To solve this
issue, they introduced the squared IB Lagrangian, Lsβqs-qIB = I(T; Y ) - βsqI(X; T)2, which is able to
explore the IB curve in any scenario by optimizing for different values of βsq . However, even though
they realized a one-to-one mapping between βsq existed, they did not find such mapping. Hence,
multiple optimizations of the Lagrangian were still required to fing the best traded-off solution.
The main contributions of this article are:
1.	We introduce a general family of Lagrangians (the convex IB Lagrangians) which are able
to explore the IB curve in any scenario for which the squared IB Lagrangian (Kolchinsky
et al., 2019) is a particular case of. More importantly, the analysis made for deriving this
family of Lagrangians can serve as inspiration for obtaining new Lagrangian families which
solve other objective functions with intrinsic trade-off such as the IB Lagrangian.
2.	We show that in deterministic scenarios (and other scenarios where the IB curve shape
is known) one can use the convex IB Lagrangian to obtain a desired level of perfor-
mance with a single optimization. That is, there is a one-to-one mapping between the
Lagrange multiplier used for the optmization and the level of compression and informa-
tiveness obtained, and we know such mapping. Therefore, eliminating the need of multiple
optimizations to select a suitable representation.
Furthermore, we provide some insight for explaining why there are discontinuities in the perfor-
mance levels w.r.t. the values of the Lagrange multipliers. In a classification setting, we connect
those discontinuities with the intrinsic clusterization of the representations when optimizing the IB
bottleneck objective.
The structure of the article is the following: in Section 2 we motivate the usage of the IB in super-
vised learning settings. Then, in Section 3 we outline the important results used about the IB curve
in deterministic scenarios. Later, in Section 4 we introduce the convex IB Lagrangian and explain
some of its properties. After that, we support our (proved) claims with some empirical evidence on
the MNIST dataset (LeCun et al., 1998) in Section 5. The reader can download the PyTorch (Paszke
et al., 2017) implementation at https://gofile.io/?c=G9Dl1L.
2
Under review as a conference paper at ICLR 2020
2	The IB in supervised learning
In this section we will first give an overview of supervised learning in order to later motivate the
usage of the information bottleneck in this setting.
2.1	Supervised learning overview
In supervised learning we are given a dataset Dn = {(xi, yi)}in=1 ofn pairs of input features and task
outputs. In this case, X and Y are the random variables of the input features and the task outputs.
We assume xi and yi are sampled i.i.d. from the true distribution pXY (x, y) = pY |X(y|x)pX(x).
The usual aim of supervised learning is to use the dataset Dn to learn a particular conditional distri-
bution qγ∣χ θ(y|x) of the task outputs given the input features, parametrized by θ, which is a good
approximation of pγ∣χ(y|x). We use Y and y to indicate the predicted task output random variable
and its outcome. We call a supervised learning task regression when Y is continuous-valued and
classification when it is discrete.
Usually supervised learning methods employ intermediate representations of the inputs before mak-
ing predictions about the outputs; e.g., hidden layers in neural networks (Chapter 5 from Bishop
(2006)) or transformations in a feature space through the kernel trick in kernel machines like SVMs
or RVMs (Sections 7.1 and 7.2 from Bishop (2006)). Let T be a possibly stochastic function of the
input features X with a parametrized conditional distribution qτ∣χ,θ (t|x), then, T obeys the Markov
condition Y 什 X 什 T. The mapping from the representation to the predicted task outputs is de-
fined by the parametrized conditional distribution qγ∣τθ(y|t). Therefore, in representation-based
machine learning methods the full Markov Chain is Y 什 X 什 T 什 Y. Hence, the overall estima-
tion of the conditional probabilitypY|X(y|x) is given by the marginalization of the representations,
qY ∣X,θ
⑻X) = I
∀t
qγ∣τ,θ (y∣t)qτ ∣x,θ (t∣χ)dt.
(3)
In order to achieve the goal of having a good estimation of the conditional probability distribution
pY|X(y|x), we usually define an instantaneous cost function jθ (x, y) : X × Y → R. This serves as
a heuristic to measure the loss our algorithm (parametrized by θ) obtains when trying to predict the
realization of the task output y with the input realization x.
Clearly, we are interested in minimizing the expectation of the instantaneous cost function over all
the possible input features and task outputs, which we call the cost function. However, since we only
have a finite dataset Dn we have instead to minimize the empirical cost function.
Definition 5 (Cost function and empirical cost function). Let X and Y be the input features and
task output random variables and x ∈ X and y ∈ Y their realizations. Let also jθ (x, y) be the
instantaneous cost function, θ the parametrization of our learning algorithm, and Dn = {xi, yi}in=1
the given dataset. Then we define:
1.	The cost function:
2.	The emprical cost function:
J(θ) = EpXY [jθ (x, y)]	(4)
1n
J(θ, Dn) = n	jθ (xi,yi)	(5)
The discrepancy between the normal and empirical cost functions is called the generalization gap
or generalization error (see Section 1 of Xu & Raginsky (2017), for instance) and intuitevely, the
smaller this gap is, the better our model generalizes (i.e., the better it will perform to new, unseen
samples in terms of our cost function).
Definition 6 (Generalization gap). Let J(θ) and J(θ, Dn) be the cost and the empirical cost func-
tions as defined in Definition 5. Then, the generalization gap is defined as
gen(θ, Dn) = J(θ) - J(θ, Dn),
(6)
and it represents the error incurred when the selected distribution is the one parametrized by θ when
the rule J(θ, Dn) is used instead of J(θ) as the function to minimize.
3
Under review as a conference paper at ICLR 2020
Ideally, we would want to minimize the cost function. Hence, we usually try to minimize the em-
pirical cost function and the generalization gap simultaneously. The modifications to our learning
algorithm which intend to reduce the generalization gap but not hurt the performance on the empir-
ical cost function are known as regularization.
2.2	Why do we use the IB?
Definition 7 (Representation cross-entropy cost function). Let X and Y be two statistically de-
pendent variables with joint distribution pXY (x, y) = pY |X (y|x)pX (x). Let also T be a random
variable obeying the Markov condition Y 什 X 什 T and qτ∣χ,θ(t|x) and qγ戈6(y|t) be the en-
coding and decoding distributions of our model, parametrized by θ. Finally, let C(p(z)||q(z)) =
-Ep(Z) [log(q(z))] be the cross entropy between two probability distributions p and q. Then, the
cross-entropy cost function is
JCE(θ)
=EqT ∣x,θ PX ICgY|T,e(yIt)IIqY∣t,θ(y|t))i = EpXY jcE,θ(x,y)],	⑺
where jcE,θ(x,y) = C(qτ∣χ,θ(t∣x)∣∣qγ∣tθ(y∣t)) is the instantaneous representation cross-entropy
cost function and qγ ∣τ,θ(y∣t) = J∀x pγ ∣χ (y∣χ)qτ ∣x,θ (t∣χ)px (x)∕qτ,θ (t)dx ∙
The cross-entropy is a widely used cost function in classification tasks (e.g., Krizhevsky et al. (2012);
Shore & Gray (1982); Teahan (2000)) which has many interesting properties (Shore & Johnson,
1981). Moreover, itis known that minimizing the JcE(θ) maximizes the mutual information I(T; Y)
(see Section 2 of Kolchinsky et al. (2019) or Section II A. of Vera et al. (2018)).
Definition 8 (Nuisance). A nuisance is any random variable which affects the observed data X but
is not informative to the task we are trying to solve∙ That is, Ξ is a nuisance for Y if Y ⊥ Ξ or
I(Ξ,Y)=0∙
Similarly, we know that minimizing I(X; T) minimizes the generalization gap for restricted classes
when using the cross-entropy cost function (Theorem 1 of Vera et al. (2018)), and when using
I(T; Y) directly as an objective to maximize (Theorem 4 of Shamir et al. (2010)). Furthermore,
Achille & Soatto (2018) in Proposition 3.1 upper bound the information of the input representations,
T, with nuisances that affect the observed data, Ξ, with I(X; T). Therefore minimizing I(X; T)
helps generalization by not keeping useless information of Ξ in our representations.
Thus, jointly maximizing I(T; Y) and minimizing I(X; T) is a good choice both in terms of per-
formance in the available dataset and in new, unseen data, which motivates studies on the IB.
3	The Information B ottleneck in deterministic s cenarios
Kolchinsky et al. (2019) showed that when Y is a deterministic function of X (i.e., Y = f(X)), the
IB curve is piecewise linear. More precisely, it is shaped as stated in Proposition 1.
Proposition 1 (The IB curve is piecewise linear in deterministic scenarios). Let X be a random
variable and Y = f (X) be a deterministic function of X∙ Let also T be the bottleneck variable
that solves the IB functional∙ Then the IB curve in the information plane is defined by the following
equation:
I	I(T;Y)= I(X;T)	if	I(X;T)	∈	[0,I(X;Y)]
I	(T; Y) = H(Y)	ifI (X;T)	>I (X; Y)
(8)
Furthermore, they showed that the IB curve could not be explored by optimizing the IB Lagrangian
for multiple β because the curve was not strictly concave. That is, there was not a one-to-one
relationship between β and the performance level.
Theorem 1 (In deterministic scenarios, the IB curve cannot be explored using the IB La-
grangian). Let X be a random variable and Y = f(X) be a deterministic function of X∙ Let
also T be the bottleneck variable that solves arg maxT ∈∆ {LIβB } with ∆ the set of r∙v∙ obeying the
Markov condition Y 什 X 什 T∙ Then:
4
Under review as a conference paper at ICLR 2020
1.	Any solution T ∈ ∆ s.t. I(X;T) ∈ [0, I(X; Y)) and I(T;Y) = I(X; T) solves
argmaxT∈∆{LIβB}forβ = 1.
2.	Any solution T ∈ ∆ s.t. I(X; T) > I(X;Y) and I(T;Y) = I(X;Y) solves
argmaxT∈∆{LIβB}forβ = 0.
3.	The solution of I(X; T) = I(T; Y ) = I(X; Y ) is achieved ∀β ∈ (0, 1). Furthermore, this
is the only solution β ∈ (0, 1) yields.
4	The Convex IB Lagrangian
4.1	Exploring the IB curve
Clearly, a situation like the one depicted in Theorem 1 is not desirable, since we cannot aim for
different levels of compression or performance. For this reason, we generalize the effort from
Kolchinsky et al. (2019) and look for families of Lagrangians which are able to explore the IB
curve. Inspired by the squared IB Lagrangian, Lsβqs-qIB(T) = I(T; Y ) - βsqI(X; T)2, we look at the
conditions a function of I(X; T) requires in order to be able to explore the IB curve. In this way,
we realize that any monotonically increasing and strictly convex function will be able to do so, and
we call the family of Lagrangians with these characteristics the convex IB Lagrangians, due to the
nature of the introduced function.
Theorem 2 (Convex IB Lagrangians). Let ∆ be the set of r.v. T obeying the Markov condition
Y 什 X 什 T. Then, if h is a monotonically increasing and strictly convex function, the IB curve
can always be recovered by the solutions of arg maxT ∈∆ {LIβBh,h (T)}, with
LIβBh,h(T) = I(T; Y) - βhh(I(X; T)).	(9)
That is, for each point (I(X; T), I(T; Y)) s.t. dI(T; Y)/dI(X; T) > 0 there is a unique βh for
which maximizing LIβBh,h(T) achieves this solution. Furthermore, βh is strictly decreasing w.r.t.
I(X; T). We call LIβBh,h(T) the convex IB Lagrangian.
The proof of this theorem can be found on Appendix A. Furthermore, by exploiting the IB curve
duality (Lemma 10 of Gilad-Bachrach et al. (2003)) we were able to derive other families of La-
grangians which allow for the exploration of the IB curve (Appendix E).
Remark 1. Clearly, we can see how if h is the identity function (i.e., h(I(X; T)) = I(X; T)) then
we end up with the normal IB Lagrangian. However, since the identity function is not strictly convex,
it cannot ensure the exploration of the IB curve.
4.2	Aiming for a specific compression level
Let Bh denote the domain of Lagrange multipliers βh for which we can find solutions in the IB curve
with the convex IB Lagrangian. Then the convex IB Lagrangians do not only allow us to explore
the IB curve with different βh . They also allow us to identify the specific βh that obtains a given
point (I(X; T), I(T; Y)), provided we know the IB curve in the information plane. Conversely,
the convex IB Lagrangian allows to find the specific point (I(X; T), I(T; Y)) that is obtained by a
given βh.
Proposition 2 (Bijective mapping between IB curve point and convex IB Lagrange multiplier).
Let the IB curve in the information plane be known; i.e., I(T; Y) = fIB (I(X; T)) is known. Then
there isa bijective mapping from Lagrange multipliers βh ∈ Bh\{0}from the convex IB Lagrangian
to points in the IB curve (I(X; T), fIB(I(X; T)). Furthermore, these mappings are:
β — d⅛(i(x;T))	ι d, I(XT)_(ho)-1 (d⅞(i(X;T)) n	(10)
βh	dI(X; T) h0(I(X; T))	( ; ) ( ) V dI(X； T) βh 卜	()
where h0 is the derivative ofh and (h0)-1 is the inverse ofh0.
5
Under review as a conference paper at ICLR 2020
It is interesting since in deterministic scenarios we know the shape of the IB curve (Theorem 1) and
since the convex IB Lagrangians allow for the exploration of the IB curve (Theorem 2). A proof for
Proposition 2 can be found in Appendix B.
Remark 2. The inclusion of the function h is what allows us to find the bijection between βh and
I(X; T). The previous definition from Tishby et al. (2000) of β as d(I(T; Y ))/dI (X; T) did not.
A direct result derived from this proposition is that we know the domain of Lagrange multipliers, Bh,
which allow for the exploration of the IB curve if the shape of the IB curve is known. Furthermore,
if the shape is not known we can at least bound that range.
Corollary 1 (Domain of convex IB Lagrange multiplier with known IB curve shape). Let
the IB curve in the information plane be I(T; Y ) = fIB(I(X; T)) and let Imax = I(X; Y ).
Let also I(X; T ) = rmax be the minimum mutual information s.t. fIB (rmax) = Imax (i.e.,
rmax = minr {fIB (r) = Imax}). Then, the range of Lagrange multipliers that allow the exploration
of the IB curve with the convex IB Lagrangian is Bh = [βh,min, βh,max], with
and
βh,max = lim
r→0+
(11)
where fI0B(r) and h0 (r) are the derivatives of fIB(I(X; T)) and h(I(X; T)) w.r.t. I(X; T) evaluated
at r respectively.
Corollary 2 (Domain of convex IB Lagrange multiplier bound). The range of the Lagrange mul-
tipliers that allow the exploration of the IB curve is contained by [0, βh,top] which is also contained
by [0, βh+,top], where
βh,top
(infΩχ⊂x{加电)})-1 and β+	=	1
limr_0+ {h0(r)}	, and βh,top . limr_o+ {h0(r)},
(12)
h0(r) is the derivative of h(I(X; T)) w.r.t. I(X; T) evaluated at r, X is the set of possible realiza-
tions of X and βo1 and Ωχ are defined as in (Wu et al., 2019). That is, Bh ⊆ [0, βh,top] ⊆ [0, β+top].
Corollaries 1 and 2 allow us to reduce the range search for β when we want to explore the IB
curve. Practically, infωx⊂x{βo(Ωχ)} might be difficult to calculate so WU et al. (2019) derived an
algorithm to approximate it. However, we still recommend 1 for simplicity. The proofs for both
corollaries are found in Appendices C and D.
5	Experimental support
In order to showcase our claims we use the MNIST dataset (LeCun et al., 1998). We simply modify
the nonlinear-IB method (Kolchinsky et al., 2017), which is a neural network that minimizes the
cross-entropy while also minimizing a differentiable kernel-based estimate of I(X; T ) (Kolchin-
sky & Tracey, 2017). Then we use this technique to maximize a lower bound on the convex IB
Lagrangians by applying the functions h to the I(X; T ) estimate.
For a fair comparison, we use the same network architecture as that in (Kolchinsky et al., 2017):
First, a stochastic encoder 2 3 T = fθ,enC(X) + W with W 〜N(0,I2) such that T ∈ R2. Here
fθ,enc is a three fully-conected layer encoder with 800 ReLU units on the first two layers and 2 linear
units on the last layer. Second, a deterministic decoder qγ戈6(y|t) = fθ,dec(t). Here, fθ,dec is a
fully-conected 800 ReLU unit layers followed by an output layer with 10 softmax units. For further
details about the experiment setup and additional results for different values of α and η please refer
to Appendix F.
In Figure 1 we show our results for two particularizations of the convex IB Lagrangians:
1 Note in (Wu et al., 2019) they consider the dual problem (see Appendix E) so when they refer to β-1 it
translates to β in this article.
2The encoder needs to be stochastic to (i) ensure a finite and well-defined mutual information (Kolchinsky
et al., 2019; Amjad & Geiger, 2019) and (ii) make gradient-based optimization methods over the IB Lagrangian
useful (Amjad & Geiger, 2019).
3The clusters were obtained using the DBSCAN algorithm (Ester et al., 1996; Schubert et al., 2017).
6
Under review as a conference paper at ICLR 2020
Figure 1: The top row shows the results for the power IB Lagrangian with α = 1, and the bottom
row for the exponential IB Lagrangian with η = 1. In each row, from left to right it is shown
(i) the information plane, where the region of possible solutions of the IB problem is shadowed
in light orange and the information-theoretic limits are the dashed orange line; (ii) I(T; Y ) as a
function of βh; and (iii) the compression I(X; T) as a function of βh. In all plots the red crosses
joined by a dotted line represent the values computed with the training set, the blue dots the values
computed with the validation set and the green stars the theoretical values computed as dictated
by Proposition2. Moreover, in all plots it is indicated I(X; Y ) = H(Y ) = log2 (10) in a dashed,
orange line. All values are shown in bits.
10
9
S
7
6
5
4
3
2
(a) Number of clusters for different βpow.
(b) Example of clusters for different βpow.
Figure 2: Depiction of the clusterization behavior3of the bottleneck variable for the power IB La-
grangian with α = 1.
1.	the power IB Lagrangians4: LIβBp,opwow(T, α) = I(T; Y ) - βpowI(X; T)(1+α), α > 0 .
2.	the exponential IB Lagrangians: LIβBex,epxp(T, η) = I(T; Y ) - βexp exp(ηI(X; T)), η > 0.
We can clearly see how both Lagrangians are able to explore the IB curve (first column from Figure
1) and how the theoretical performance trend of the Lagrangians matches the experimental results
(second and third columns from Figure 1). There are small mismatches between the theoretical and
experimental performance. This is because using the nonlinear-IB, as stated by Kolchinsky et al.
(2019), does not guarantee that we find optimal representations due to factors like: (i) innacurate
estimation of I(X; T ), (ii) restrictions on the structure of T , (iii) use of an estimation of the de-
coder instead of the real one and (iv) the typical non-convex optimization issues that arise with
gradient-based methods. The main difference comes from the discontinuities in performance for in-
4 Note when α = 1 we have the squared IB functional from Kolchinsky et al. (2019).
7
Under review as a conference paper at ICLR 2020
creasing β, which cause is still unknown (cf. Wu et al. (2019)). It has been observed, however, that
the bottleneck variable performs an intrinsic clusterization in classification tasks (see, for instance
(Kolchinsky et al., 2017; 2019; Alemi et al., 2018) or Figure 2b). We realized how this clusteri-
zation matches with the quantized performance levels observed (e.g., compare Figure 2a with the
top center graph in Figure 1); with maximum performance when the number of clusters is equal to
the cardinality of Y and reducing performance with a reduction of the number of clusters. We do
not have a mathematical proof for the exact relationship between these two phenomena; however,
we agree with Wu et al. (2019) that it is an interesting matter and hope this realization serves as
motivation to derive new theory.
To sum up, in order to achieve a desired level of performance with the convex IB Lagrangian as an
objective one should:
1.	In a deterministic or close to deterministic setting (see -deterministic definition in Kolchin-
sky et al. (2019)): Use the adequate βh for that performance using Proposition 2. Then if
the perfomance is lower than desired (i.e., we are placed in the wrong performance plateau),
gradually reduce the value of βh until reaching the previous performance plateau.
2.	In a stochastic setting: Draw the IB curve with multiple values of βh on the range defined
by Corollary 2 and select the representations that best fit their interests.
In practice, there are different criterions for choos-
ing the function h. For instance, the exponential
IB Lagrangian could be more desirable than the
power IB Lagrangian when we want to draw the
IB curve since it has a finite range of βh . This is
Bh = [(η exp(ηHmax))-1, η-1] for the exponen-
tial IB Lagrangian vs. Bh = [((1+α)Hmαax)-1, ∞)
for the power IB Lagrangian. Furthermore, there is
a trade-off between (i) how much the selected h
function ressembles the identity (e.g., with α or η
close to zero), since it will suffer from similar prob-
lems as the original IB Lagrangian; and (ii) how
fast it grows (e.g., higher values of α or η), since it
will suffer from value convergence; i.e., optimizing
for separate values of βh will achieve similar lev-
els of performance (Figure 3). Please, refer to Ap-
pendix G for a more thorough explanation of this
phenomenon.
Figure 3: Example of value convergence with
the exponential IB Lagrangian with η = 3. We
show the intersection of the isolines of LIβBex,epxp
for different βexp ∈ Bexp5with the IB curve.
6 Conclusion
The information bottleneck is a widely used and studied technique. However, it is known that the
IB Lagrangian cannot be used to achieve varying levels of performance in deterministic scenarios.
Moreover, in order to achieve a particular level of performance multiple optimizations with different
Lagrange multipliers must be done to draw the IB curve and select the best traded-off representation.
In this article we introduced a general family of Lagrangians which allow to (i) achieve varying levels
of performance in any scenario, and (ii) pinpoint a specific Lagrange multiplier βh to optimize
for a specific performance level in known IB curve scenarios (e.g., deterministic). Furthermore,
we showed the βh domain when the IB curve is known and a βh domain bound for exploring the
IB curve when it is unkown. This way we can reduce and/or avoid multiple optimizations and,
hence, reduce the computational effort for finding well traded-off representations. Finally, (iii) we
provided some insight to the discontinuities on the performance levels w.r.t. the Lagange multipliers
by connecting those with the intrinsic clusterization of the bottleneck variable.
5Bh ≈ [1.56 ∙ 10-5,3-1 ] using Corollary 1.
8
Under review as a conference paper at ICLR 2020
References
Alessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep rep-
resentations. The Journal ofMachine Learning Research, 19(1):1947-1980, 2018.
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. 2017.
Alexander A Alemi, Ian Fischer, and Joshua V Dillon. Uncertainty in the variational information
bottleneck. arXiv preprint arXiv:1807.00906, 2018.
Rana Ali Amjad and Bernhard Claus Geiger. Learning representations for neural network-based
classification using the information bottleneck principle. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 2019.
Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.
Matthew Chalk, Olivier Marre, and Gasper Tkacik. Relevant sparse codes with variational informa-
tion bottleneck. In Advances in Neural Information Processing Systems, pp. 1957-1965, 2016.
Costas Courcoubetis. Pricing Communication Networks Economics, Technology and Modelling.
Wiley Online Library, 2003.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.
Martin Ester, Hans-Peter Kriegel, Jorg Sander, XiaoWei Xu, et al. A density-based algorithm for
discovering clusters in large spatial databases with noise. In Kdd, volume 96, pp. 226-231, 1996.
Ran Gilad-Bachrach, Amir Navot, and Naftali Tishby. An information theoretic tradeoff betWeen
complexity and accuracy. In Learning Theory and Kernel Machines, pp. 595-609. Springer, 2003.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforWard neural
netWorks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Artemy Kolchinsky and Brendan Tracey. Estimating mixture entropy With pairWise distances. En-
tropy, 19(7):361, 2017.
Artemy Kolchinsky, Brendan D Tracey, and David H Wolpert. Nonlinear information bottleneck.
arXiv preprint arXiv:1705.02436, 2017.
Artemy Kolchinsky, Brendan D Tracey, and Steven Van Kuyk. Caveats for information bottleneck
in deterministic scenarios. In ICLR, 2019.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification With deep convo-
lutional neural netWorks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, EdWard Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn:
Machine learning in python. Journal of machine learning research, 12(Oct):2825-2830, 2011.
Erich Schubert, Jorg Sander, Martin Ester, Hans Peter Kriegel, and Xiaowei Xu. Dbscan revisited,
revisited: Why and hoW you should (still) use dbscan. ACM Transactions on Database Systems
(TODS), 42(3):19, 2017.
9
Under review as a conference paper at ICLR 2020
Ohad Shamir, Sivan Sabato, and Naftali Tishby. Learning and generalization with the information
bottleneck. Theoretical Computer Science, 411(29-30):2696-2711, 2010.
John Shore and Rodney Johnson. Properties of cross-entropy minimization. IEEE Transactions on
Information Theory, 27(4):472-482, 1981.
John E Shore and Robert M Gray. Minimum cross-entropy pattern classification and cluster analysis.
IEEE Transactions on Pattern Analysis and Machine Intelligence, (1):11-17, 1982.
Noam Slonim and Naftali Tishby. Agglomerative information bottleneck. In Advances in neural
information processing systems, pp. 617-623, 2000.
Noam Slonim, Nir Friedman, and Naftali Tishby. Unsupervised document classification using se-
quential information maximization. In Proceedings of the 25th annual international ACM SIGIR
conference on Research and development in information retrieval, pp. 129-136. ACM, 2002.
William John Teahan. Text classification and segmentation using minimum cross-entropy. In
Content-Based Multimedia Information Access-Volume 2, pp. 943-961. LE CENTRE DE
HAUTES ETUDES INTERNATIONALES D’INFORMATIQUE DOCUMENTAIRE, 2000.
Naftali Tishby and Noam Slonim. Data clustering by markovian relaxation and the information
bottleneck method. In Advances in neural information processing systems, pp. 640-646, 2001.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.
Matias Vera, Pablo Piantanida, and Leonardo Rey Vega. The role of the information bottleneck in
representation learning. In 2018 IEEE International Symposium on Information Theory (ISIT),
pp. 1580-1584. IEEE, 2018.
Tailin Wu, Ian Fischer, Isaac Chuang, and Max Tegmark. Learnability for the information bottle-
neck. In ICLR, 2019.
Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learn-
ing algorithms. In Advances in Neural Information Processing Systems, pp. 2524-2533, 2017.
A Proof of Theorem 2
Proof. We start the proof by remembering the optimization problem at hand (Definition 1):
FIB,max(r) =max{I(T;Y)}s.t.I(X;T) ≤r	(13)
We can modify the optimization problem by
max{I(T;Y)} s.t. h(I(X; T)) ≤ h(r)
(14)
iff h is a monotonically non-decreasing function since otherwise h(I(X; T )) ≤ h(r) would not
hold necessarily. Now, let Us assume ∃T* ∈ ∆ and βh s.t. T* maximizes LeBlh(T) over all T ∈ ∆,
and I(X; T*) ≤ r. Then, We can operate as follows:
mT ∈a∆x	{I(T; Y)}
h(I(X;T ))≤h(r)
=	m∈ax	{I (T ； Y) — βh(h(I (X ； T)) — h(r) + ξ)}
h(I(X;T ))≤h(r)
≤ m∈ax{I(T； Y) — β*(h(I(X； T))- h(r)+ ξ)}
=I(T*； Y) — βh(h(I(X； T*) — h(r) + ξ)= I(T*； Y).
(15)
(16)
(17)
10
Under review as a conference paper at ICLR 2020
Here, the equality from equation (15) comes from the fact that since I(X; T ) ≤ r, then ∃ξ ≥ 0 s.t.
h(I(X; T)) - h(r) + ξ = 0. Then, the inequality from equation (16) holds since we have expanded
the optimization search space. Finally, in equation (17) We use that T* maximizes Lehh(T) and that
I(X； T*) ≤ r.	,
NoW, We can exploit that h(r) and ξ do not depend on T and drop them in the maximization in
equation (16). We can then realize We are maximizing over LIβBh,h (T); i.e.,
arg max	{I(T; Y )} ≤ arg max{I (T; Y ) - βh* (h(I (X; T)) - h(r) + ξ)}	(18)
T∈∆	T∈∆
h(I(X;T ))≤h(r)
=argmax{I(T; Y) - β*h(I(X;T))} = argmax{Lβhh(T)}.	(19)
T ∈∆	T∈∆	,
Therefore, since I(T*; Y) satisfies both the maximization With T* ∈ ∆ and the constraint
I(X;T*) ≤ r, maximizing LIβBh,h(T) obtains FIB,max(r).
NoW, We knoW if such βh* exists, then the solution of the Lagrangian Will be a solution for FIB,max(r).
Then, if We consider Theorem 6 from the Appendix of Courcoubetis (2003) and consider the maxi-
mization problem instead of the minimization problem, We knoW if both I(T; Y) and -h(I (X; T))
are concave functions, then a set of Lagrange multipliers Sh* exists With these conditions. We can
make this consideration because f is concave if -f is convex and max{f} = min{-f}. We knoW
I(T; Y) is a concave function of T for T ∈ ∆ (Lemma 5 of Gilad-Bachrach et al. (2003)) and
I(X; T) is convex W.r.t. T given pX (x) is fixed (Theorem 2.7.4 of Cover & Thomas (2012)). Thus,
if We Want -h(I(X; T)) to be concave we need h to be a convex function.
Finally, We Will look at the conditions of h so that for every point (I(X; T), I(T; Y)) in the IB
curve, there exists a unique βh* s.t. LIβBh,h(T) is maximized. That is, the conditions of h s.t. |Sh* | = 1.
For this purpose We Will look at the solutions of the Lagrangian optimization:
dLMT) = d(I (T ； Y)-βhh(I (X ； T))) = dI (T ； Y) _	dh(I (X； T)) dI (X ；T)
—dT — =	dT	= —dT	βh di(X； T)	dT一
NoW, if We integrate both sides of equation (20) over all T ∈ ∆ We obtain
β _ di(T； Y) ( dh(I(X； T))「= β
βh = di(X; T) V di(X； T) )	= h0(i(X； T)),
(21)
Where β is the Lagrange multiplier from the IB Lagrangian (Tishby et al., 2000) and h0(i(X； T)) is
dhr；XT)). Also, if we want to avoid indeterminations of βh we need h0(i(X; T)) not to be 0. Since
We already imposed h to be monotonically non-decreasing, We can solve this issue by strengthening
this condition. That is, we will require h to be monotonically increasing.
We would like βh to be continuous, this way there would be a unique βh for each value of i(X； T).
We know β is a non-increasing function of i(X； T) (Lemma 6 of Gilad-Bachrach et al. (2003)).
Hence, if we want βh to be a strictly decreasing function of i(X； T), we will require h0 to be
an strictly increasing function of i(X； T). Therefore, we will require h to be a strictly convex
function.
Thus, if h is an strictly convex and monotonically increasing function, for each point
(i(X； T), i(T； Y)) in the IB curve s.t. di(T； Y)/di(X； T) > 0 there is a unique βh for which
maximizing LIβBh,h(T) achieves this solution.
□
11
Under review as a conference paper at ICLR 2020
B	Proof of Proposition 2
Proof. In Theorem 2 we showed how each point of the IB curve (I(X; T), I(T; Y )) can be found
with a unique βh maximizing LIβBh,h. Therefore since we also proved LIβBh,h is strictly concave w.r.t. T
we can find the values of βh that maximize the Lagrangian for fixed I(X; T ).
First, we look at the solutions of the Lagrangian maximization:
dLB,h(T) = d(fiB (I (X ； T))- βhh(I (X ； T))) = dfiB(I(X ； T)) _e dh(I(X ； T)) dI (X； T)=(
—dT — =	dT	= dT	βh dI(X; T)	dT —=
(22)
Then as before we can integrate at both sides for all T ∈ ∆ and solve for βh :
β _ dfiB(I(X； T))	1
βh =	dI(X; T)	h0(I(X; T)).
(23)
Moreover, since h is a strictly convex function its derivative h0 is strictly decreasing. Hence, h0 is an
invertible function (since a strictly decreasing function is bijective and a function is invertible iff it
is bijective by definition). Now, if we consider βh > 0 to be known and I(X； T) to be the unknown
we can solve for I(X； T) and get:
I(X；T)=(h0)-1
f (I (X; T)) ɪʌ
I dI(X；T)	βh).
(24)
Note we require βh not to be 0 so the mapping is defined.
□
C Proof of Corollary 1
Proof.
Lemma 1. Let LIβBh,h(T) be a convex IB Lagrangian, then maxT∈∆{LI0B,h(T)} = I(X； Y ).
Proof. If we write LI0B,h (T) = I(T； Y ), we see that maximizing this Lagrangian is directly max-
imizing I(T； Y ). We know I(T； Y ) is a concave function of T for T ∈ ∆ (Theorem 2.7.4 from
Cover & Thomas (2012)); hence it has a maximum. We also know I(T； Y ) ≤ I(X； Y ). Moreover,
we know I (X ； Y ) can be achieved if, for example, Y is a deterministic function of T (since then the
Markov Chain X 什 T 什 Y is formed). Thus, maxτ∈∆{Lj0B,h(T)} = I(X; Y).	□
For βh = 0 we know maximizing LIB,h (T) can obtain the point in the IB curve (rmax, Imax) (Lemma
1).
Moreover, we know that for every point (I(X； T), fIB (I(X； T))), ∃!βh s.t. max{LIβBh,h(T)} achieves
that point (Theorem 2). Thus, ∃!βh min s.t. lim → - (r, fIB(r)) is achieved. From Proposition 2 we
max
know this βh,min is given by
βh,min
(25)
Since we know fIB(I(X； T)) is a concave non-decreasing function in (0, rmax) (Lemma 5 of Gilad-
Bachrach et al. (2003)) we know it is continuous in this interval. In addition we know βh is strictly
decreasing w.r.t. I(X； T) (Theorem 2). Furthermore, by definition of rmax and knowing I(T； Y) ≤
I(X； Y) we know fI0B(r) = 0, ∀r > rmax. Therefore, we cannot ensure the exploration of the IB
curve for βh0 s.t. 0 < βh0 < βh,min.
12
Under review as a conference paper at ICLR 2020
Then, since h is a strictly increasing function in (0, rmax), h0 is positive in that interval. Hence,
taking into account βh is strictly decreasing we can find a maximum βh when I(X; T) approaches
to 0. That is,
βh,maχ=r→o+ {fIBr
(26)
□
D	Proof of Corollary 2
Proof. If we use Corollary 1, it is straightforward to see that βh ⊆ [L-, L+] if βh,min ≥ L- and
βh,max ≤ L+ for all IB curves fIB and functions h. Therefore, we look at a domain bound dependent
on the function choice. That is, ifwe can find βmin ≤ fI0B(r) and βmax ≥ fI0B(r) for all IB curves and
all values of r, then
Bh ⊆
βmin
βmax
limr→r- {h0(r)}, limr→0+{h0(r)} .
max
(27)
The region for all possible IB curves regardless of the relationship between X and Y is depicted in
Figure 4. The hard limits are imposed by the DPI (Theorem 2.8.1 from Cover & Thomas (2012)) and
the fact that the mutual information is non-negative (Corollary 2.90 for discrete and first Corollary of
Theorem 8.6.1 for continuous random variables from Cover & Thomas (2012)). Hence, a minimum
and maximum values of fI0B are given by the minimum and maximum values of the slope of the
Pareto frontier. Which means
Bh ⊆
0, limr→0+ {h0(r)}_ .
(28)
Note 0∕(limr→r-{h0(r)}) = 0 since h is monotonically increasing and, thus, h0 will never be 0.
Figure 4: Graphical representation of the IB curve in the information plane. Dashed lines in orange
represent tight bounds confining the region (in light orange) of possible IB curves (delimited by
the red line, also known as the Pareto frontier). Black dotted lines are informative values. In blue
we show an example of a possible IB curve confining a region (in darker orange) of an IB curve
which does not achieve the Pareto frontier. Finally, the yellow star represents the point where the
representation keeps the same information about the input and the output.
13
Under review as a conference paper at ICLR 2020
Finally, we can tighten the bound using the results from Wu et al. (2019), where, in The-
orem 2, they showed the slope of the Pareto frontier could be bounded in the origin by
fιB ≤ (infωx⊂x{βo(Ωχ)})-1. Finally, We know that in deterministic classification tasks
infωx⊂x{βo(Ωχ)} = 1, which aligns with KoIchinSky et al. (2019) and what We can observe from
Figure 4. Therefore,
Bh ⊆
(inf Ω%⊂x {βo(Ωχ)})T
limr→0+ {h0(r)}
0,limr_0+{h/(r)}_ .
(29)
□
E Other Lagrangian Families
We can use the same ideas we used for the convex IB Lagrangian to formulate new families of
Lagrangians that allow the exploration of the IB curve. For that we will use the duality of the IB
curve (Lemma 10 of (Gilad-Bachrach et al., 2003)). That is:
Definition 9 (IB dual functional). Let X and Y be statistically dependent variables. Let also ∆
be the Set of random variables T obeying the Markov condition Y 什 X 什 T. Then the IB dual
functional is
FIB,min(i) = min{I(X;T)} s.t. I(T; Y) ≥i, ∀i ∈ [0, I(X; Y)].
(30)
Theorem 3 (IB curve duality). Let the IB curve be defined by the solutions of FIB,max(r) for varying
r ∈ [0, ∞). Then,
∀r∃i s.t. (r, FIB,max(r)) = (FIB,min(i),i)	(31)
and
∀i∃r s.t. (FIB,min(i), i) = (r, FIB,max(r)).	(32)
From this definition it follows that minimizing the dual IB Lagrangian,LIβBd,udalual(T) = I(X;T)-
βdualI(T; Y), for βdual = β-1 is equivalent to maximizing the IB Lagrangian. In fact, the original
Lagrangian for solving the problem was defined this way (Tishby et al., 2000). We decided to use
the maximization version because the domain of useful β is bounded while it is not for βdual.
Following the same reasoning as we did in the proof of Theorem 2, we can ensure the IB curve can
be explored if:
1.	We minimize Leg,g(T) = I(X; T) - βgg(I(T; Y)).
2.	We maximize L⅛‰(T) = g(I(T； Y))- βg,dualI(X； T).
3.	Weminimize £解孔(7) = h(I(X; T))- βh,dualI(T; Y).
Here, h is a monotonically increasing strictly convex function, g is a monotonically increasing
strictly concave function, and βg, βg,dual, βh,dual are the Lagrange multipliers of the families of La-
grangians defined above.
In a similar manner, one could obtain relationships between the Lagrange multipliers of the IB
Lagrangian and the convex IB Lagrangian with these Lagrangian families. Also, one could find a
range of values for these Lagrangians to allow for the IB curve exploration and define a bijective
mapping between their Lagrange multipliers and the IB curve. However, (i) as mentioned in Section
2.2, I(T; Y) is particularly interesting to maximize without transformations because of its meaning.
Moreover, (ii) like βdual, the domain of useful βg and βh,dual is not upper bounded. These two
reasons make these other Lagrangians less preferable. We only include them here for completeness.
Nonetheless, we encourage the curiours reader to explore these families of Lagrangians too.
14
Under review as a conference paper at ICLR 2020
F	Experimental setup details
In order to generate the empirical support results from Section 5 we used the nonlinear IB (Kolchin-
sky et al., 2017) on the MNIST dataset (LeCun et al., 1998). This dataset contains 60,000 training
samples and 10,000 testing samples of hand-written digits. The samples are 28x28 pixels and are
labeled from 0 to 9; i.e., X = R784 and Y = {0, 1, ..., 9}.
As in (Kolchinsky et al., 2019) we trained the neural network with the Adam optimization algorithm
(Kingma & Ba, 2014) with a learning rate of 10-4 but we introduced a 0.6 decay rate every 10
iterations. After talking with the authors of the nonlinear IB (Kolchinsky et al., 2017), we decided
to estimate the gradients of both Iθ(X; T) and the cross entropy with the same mini-batch of 128
samples. Moreover, we did not learn the covariance of the mixture of Gaussians used for the kernel
density estimation of Iθ(X; T) and we set it to (exp(-1))2. We trained for 100 epochs6. All
the weights were initialized according to the method described by Glorot & Bengio (2010) using
a Gaussian distribution. The reader can find the PyTorch (Paszke et al., 2017) implementation at
https://gofile.io/?c=G9Dl1L.
Then, we used the DBSCAN algorithm (Ester et al., 1996; Schubert et al., 2017) for clustering.
Particularly, we used the scikit-learn (Pedregosa et al., 2011) implementation with = 0.3 and
min_samples = 50.
In Figure 5 we show how the IB curve can be explored with different values of α for the power IB
Lagrangian and in Figure 6 for different values of η and the exponential IB Lagrangian.
Finally, in Figure 7 we show the clusterization for the same values of α and η as in Figures 5 and 6.
In this way the connection between the performance discontinuities and the clusterization is more
evident. Furthermore, we can also observe how the exponential IB Lagrangian maintains better the
theoretical performance than the power IB Lagrangian (see Appendix G for an explanation of why).
G Guidelines for selecting a proper function in the Convex IB
Lagrangian
When chossing the right h function, it is important to find the right balance between avoiding value
convergence and aiming for strong convexity. Practically, this balance is found by looking at how
much faster h grows w.r.t. the identity function.
G. 1 Avoiding value convergence
In order to explain this issue we are going to use the example of classification on MNIST (Le-
Cun et al., 1998), where I(X; Y ) = H(Y ) = log2 (10), and again the power and exponential IB
Lagrangians.
If we use Proposition 2 on both Lagrangians we obtain the bijective mapping between their Lagrange
multipliers and a certain level of compression in the classification setting:
_-|	_1
1.	Power IB Lagrangian: βpow = ((1 + α)I(X; T)α) 1 and I(X; T) = ((1 + α)βpow) α.
2.	Exponential IB Lagrangian: βeχp = (η exp(ηI(X; T)))-1 and I(X; T) = - log(ηβeχp)∕η.
Hence, we can simply plot the curves of I(X; T) vs. βh for different hyperparameters α and η (see
Figure 8). In this way we can observe how increasing the growth of the function (e.g., increasing α
or η in this case) too much provokes that many different values ofβh converge to very similar values
of I(X; T). This is an issue both for drawing the curve (for obvious reasons) and for aiming for a
specific performance level. Due to the nature of the estimation of the IB Lagrangian, the theoretical
and practical value of βh that yield a specific I(X; T) may vary slightly (see Figure 1). Then if
we select a function with too high growth, a small change in βh can result in a big change in the
performance obtained.
6 Note in the last version of the Nonlinear IB article in arxiv (v8) they eχplain many of this issues.
15
Under review as a conference paper at ICLR 2020
Figure 5: Results for the power IB Lagrangian with α = {0.5, 1, 2}, from top to bottom. In each row,
from left to right it is shown (i) the information plane, where the region of possible solutions of the IB
problem is shadowed in light orange and the information-theoretic limits are the dashed orange line;
(ii) I(T; Y ) as a function of βh; and (iii) the compression I(X; T) as a function of βh. In all plots
the red crosses joined by a dotted line represent the values computed with the training set, the blue
dots the values computed with the validation set and the green stars the theoretical values computed
as dictated by Proposition 2. Moreover, in all plots it is indicated I(X; Y ) = H(Y ) = log2(10) in
a dashed, orange line. All values are shown in bits.
G.2 Aiming for strong convexity
Definition 10 (μ-Strong convexity). If a function f (r) is twice continuous differentiable and its
domain is confined in the real line, then it is μ-strong convex if f 00(r) ≥ μ ≥ 0 Nr.
Experimentally, we observed when the growth of our function h(r) is small in the domain of interest
r > 0 the convex IB Lagrangian does not perform well. Later we realized that this was closely
related with the strength of the convexity of our function.
In Theorem 2 we imposed the function h to be strictly convex to enforce having a unique βh for
each value of I(X; T). Hence, since in practice we are not exactly computing the Lagrangian but an
estimation of it (e.g., with the nonlinear IB (Kolchinsky et al., 2017)) we require strong convexity in
order to be able to explore the IB curve.
We now look at the second derivative of the power and exponential function: h00 (r) = (1 + α)αrα-1
and h00(r) = η2 exp(ηr) respectivelly. Here we see how both functions are inherently 0-strong
convex for r > 0 and α,η > 0. However, values of a < 1 and η < 1 could lead to low μ-strong
convexity in certain domains of r. Particularly, the case of α < 1 is dangerous because the function
approaches 0-strong convexity as r increases, so the power IB Lagrangian performs poorly when
low α are used to find high performances.
16
Under review as a conference paper at ICLR 2020
Figure 6: Results for the exponential IB Lagrangian with η = {log(2), 1, 1.5}, from top to bottom.
In each row, from left to right it is shown (i) the information plane, where the region of possible
solutions of the IB problem is shadowed in light orange and the information-theoretic limits are
the dashed orange line; (ii) I(T; Y ) as a function of βh; and (iii) the compression I(X; T) as a
function of βh . In all plots the red crosses joined by a dotted line represent the values computed
with the training set, the blue dots the values computed with the validation set and the gren stars
the theoretical values computed as dictated by Proposition 2. Moreover, in all plots it is indicated
I(X; Y ) = H(Y ) = log2 (10) in a dashed, orange line. All values are shown in bits.
0.0 g 0.4 0.6 «.a	1.0	1.2	1.4
0.0	0.2	0.4	0.6	0.8	1.0
心卷n∙"
0.0	0.1	0.2	03	0.4	0.5	0.6	0.7
Figure 7: Depiction of the clusterization behavior of the bottleneck variable. In the first row, from
left to right, the power IB Lagrangian with different values of α = {0.5, 1, 2}. In the second row,
from left to right, the exponential IB Lagrangian with different values of η = {log(2), 1, 1.5}.
17
Under review as a conference paper at ICLR 2020
Figure 8: Theoretical bijection between I(X; T) and different α from βh,min to 1.5 in the power IB
Lagrangian (top), and different η in the domain Bh in the exponential IB Lagrangian (bottom).
18