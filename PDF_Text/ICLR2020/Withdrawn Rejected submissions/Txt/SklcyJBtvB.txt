Under review as a conference paper at ICLR 2020
Off-policy Bandits with Deficient Support
Anonymous authors
Paper under double-blind review
Ab stract
Off-policy training of contextual-bandit policies is attractive in online systems
(e.g. search, recommendation, ad placement), since it enables the reuse of large
amounts of log data. State-of-the-art methods for off-policy learning, however, are
based on inverse propensity score (IPS) weighting, which requires that the logging
policy chooses all actions with non-zero probability for any context (i.e., full sup-
port). In real-world systems, this condition is often violated, and we show that
existing off-policy learning methods based on IPS weighting can fail catastroph-
ically. We therefore develop new off-policy contextual-bandit methods that can
controllably and robustly learn even when the logging policy has deficient sup-
port. To this effect, we explore three approaches that provide various guarantees
for safe learning despite the inherent limitations of support deficient data: restrict-
ing the action space, reward extrapolation, and restricting the policy space. We
analyze the statistical and computational properties of these three approaches, and
empirically evaluate their effectiveness in a series of experiments. We find that
controlling the policy space is both computationally efficient and that it robustly
leads to accurate policies.
1	Introduction
Many interactive systems (e.g., voice assistants, recommender systems, ad placement) can be mod-
eled as contextual bandit problems (Langford & Zhang, 2008). In particular, each user request
provides a context (e.g., user profile, query) for which the system selects an action (e.g., recom-
mended product, presented ad) and receives a reward (e.g., purchase, click). Such contextual-bandit
data is logged in large quantities as a by-product of normal system operation (Li et al., 2011; 2015;
Joachims et al., 2017), making it an attractive and low-cost source of training data. With terabytes of
such log data readily available in many online systems, a range of algorithms have been proposed for
batch learning from such logged contextual-bandit feedback (Strehl et al., 2011; Dudlk et al., 2011;
Swaminathan & Joachims, 2015a; Thomas & Brunskill, 2016; Farajtabar et al., 2018; Su et al., 2019;
London & Sandler, 2019). However, as we will argue below, these algorithms require an assumption
about the log data that makes them unsuitable for many real-world applications.
This assumption is typically referred to as the positivity or support assumption, and it is required
by the Empirical Risk Minimization (ERM) objective that these algorithms optimize. Specifically,
unlike in online learning for contextual bandits (Williams, 1992; Agarwal et al., 2014), batch learn-
ing from bandit feedback (BLBF) operates in the off-policy setting. During off-policy learning, the
algorithm has to address the counterfactual question of how much reward each policy in the policy
space would have received, if it had been used instead of the logging policy. To this effect, virtually
all state-of-the-art off-policy learning methods for contextual-bandit problems rely on counterfac-
tual estimators (Bottou et al., 2013; Dudlk et al., 2011; Swaminathan & Joachims, 2015a; Thomas
& Brunskill, 2016; Farajtabar et al., 2018; Su et al., 2019) that employ inverse propensity score
(IPS) weighting to get an unbiased ERM objective. Unlike regression-based direct-modeling (DM)
approaches that are often hampered by bias from model-misspecification, IPS allows a controllable
bias-variance trade-off through clipping and other variance-regularization techniques (Strehl et al.,
2011; Swaminathan & Joachims, 2015a; London & Sandler, 2019).
Unfortunately, IPS and its variance-control mechanisms break down when the logging policy does
not have full support - meaning that some actions have zero probability of being selected under the
logging policy. In this case IPS can be highly biased. Full support is an unreasonable assumption
in many real-world systems, especially when the action space is large and many actions have poor
1
Under review as a conference paper at ICLR 2020
rewards. For example, in a recommender system with a large catalog (e.g. movies, music), it may be
that less than 10% of the actions have support under the logging policy. We will show that existing
learning algorithms can fail catastrophically on such support deficient data.
In this paper, we develop new off-policy contextual-bandit algorithms that are specifically designed
to deal with support deficient log data. Since support deficiency translates into blind spots where we
do not have any knowledge about the rewards, accounting for these blind spots as part of learning
is crucial for robust learning. We approach this problem from three perspectives. First, we explore
restricting the action space to those actions that have support under the logging policy. Second,
we explore imputation methods that extrapolate estimated rewards to those blind spots. And, third,
we restrict the policy space to only those policies that have limited exposure to the blind spots. To
make the latter approach computationally tractable, we define a new measure of Support Divergence
between policies, show how it can be estimated efficiently without closed-form knowledge of the
logging policy, and how it can be used as a constraint on the policy space. We analyze the statistical
and computational properties of all three approaches and perform an extensive empirical evaluation.
We find that restricting the policy space is particularly effective, since it is computationally efficient,
empirically effective at learning good policies, and convenient to use in practice.
2	Related Work
Most prior works on BLBF can be classified into two different approaches. The first - called Direct
Model (DM) - is based on a reduction to supervised learning, where a regression estimate is trained
to predict rewards (Beygelzimer & Langford, 2009). To derive a policy, the action with the highest
predicted reward is chosen. A drawback of this simple approach is the bias that results from mis-
specification of the regression model. Since regression models are often substantially misspecified
for real-world data, the DM approach often does not work well empirically.
The second approach is based on policy learning via ERM with a counterfactual risk estimator. In-
verse propensity score (IPS) weighting is one of the most popular estimators to be used as empirical
risk. However, policy learning algorithms based on IPS and related estimators (Strehl et al., 2011;
Swaminathan & Joachims, 2015a;b; Thomas & Brunskill, 2016; London & Sandler, 2019) require
the assumption that the logging policy has full support for every policy in the policy space. One
exception is the work of Liu et al. (2019). They relax the assumption to the existence of an optimal
policy such that the logging policy covers the support of this optimal policy. However, this is an
untestable assumption that does not provide guarantees for real-world applications.
Our work proposes three approaches to addressing off-policy learning with support deficiency. First,
our conservative extrapolation method is related to the method proposed by Liu et al. (2019). They
focus on the correction of the state distribution by defining an augmented MDP, and pessimistic
imputation is used to get an estimate for policy-gradient learning. Second, our method of restricting
the policy space uses a surrogate for the support divergence of two policies that was previously used
as control variate in the SNIPS estimator (Swaminathan & Joachims, 2015b). It also appeared in
the Lagrangian formulation of the BanditNet objective (Joachims et al., 2018) and in the gradient
update in REINFORCE algorithm (Williams, 1992). This connection gives interesting new insight
that the baselines used in policy-gradient algorithms not only help to reduce variance in gradients
(Greensmith et al., 2004), but that they also connect to the problem of support deficiency in the
off-policy setting.
3	Off-policy Learning with Deficient Support
We start by formally defining the problem of learning a contextual-bandit policy in the BLBF setting.
Input to the policy are contexts x ∈ X drawn i.i.d from a fixed but unknown distribution P (X).
Given context x, the system executes a possibly stochastic policy ∏(Y∣χ) that selects an action
y ∈ Y. For this context and action pair, the system observes a reward r ∈ [rmin , rmax] from
P (r|x, y). Given a space of policies Π, the reward of any policy π ∈ Π is defined as
R(π) = E	E E [r].	(1)
X y〜π(y∣x) r〜P(r∣x,y)
In the BLBF setting, the learning algorithm is given a dataset
D = {χi,yi ,r ,∏o(yi∣χi)}n=ι
2
Under review as a conference paper at ICLR 2020
of past system interactions which consists of context-action-reward-propensity tuples. The propen-
Sity ∏o(yi∣Xi) is the probability of selecting action yi for context Xi under the policy ∏o that was
used to log the data. We call π0 the logging policy, and we will discuss desired conditions on the
stochasticity of π0 in the following. The goal of off-policy learning is to exploit the information in
the logged data D to find a policy ∏ ∈ Π that has high reward R(Π).
Analogous to the ERM principle in supervised learning, off-policy learning algorithms typically
.∙	. Γ∙	1	. ∙	. -f∖ / ∖ Cc ∕∖	. 1 .	[♦ . ∙	ZT ∙ . 1 ΠΓ∖Λ Λ O r∖ι I-
optimize a counterfactual estimate R(π) of R(π) as the training objective (Li et al., 2011; 2015;
Bottou et al., 2013; Swaminathan & Joachims, 2015a).
ʌ	Γ ^r∖ / M
π = arg max[R(π)]
π∈Π
(2)
For conciseness, we ignore additional regularization terms in the objective (Swaminathan &
Joachims, 2015a), since they are irrelevant to the main point of this paper. As counterfactual es-
tιmator R(π), most algorithms rely on some form of IPS weighting (Strehl et al., 2011; Dudlk et al.,
2011; Swaminathan & Joachims, 2015a;b; Wang et al., 2017; Su et al., 2019) to correct the distribu-
tion mismatch between the logging policy π0 and each target policy π ∈ Π.
1n
RIPS (π) = n 工
∏(y∕χi)
∏o(yi∣χi) i
(3)
A crucial condition for the effectiveness of the IPS estimator (and similar estimators) is that the
logging policy π0 assigns non-zero probability to all actions that have non-zero probability under
the target policy π we aim to evaluate. This condition is known as positivity or full support, and it is
defined as follows.
Definition 1 (Full support). The logging policy ∏o is said to havefull Supportfor π when ∏o(y∣x) >
0 for all actions y ∈ Y and contexts X ∈ X for which π(y∣x) > 0.
It is known that the IPS estimator is unbiased, ED[RIPS(π)] = R(π), if the logging policy π0 has
full support for π (Li et al., 2011). To ensure unbiased ERM, algorithms that use the IPS estimator
require that the logging policy π0 has full support for all policies π ∈ Π in the policy space. For
sufficiently rich policy spaces, like deep-networks fw(X, y) with softmax outputs of the form
∏w (y|x)
exp(fw (x,y))
Pyo∈γ exp(fw(X,yO)),
(4)
this means that the logging policy π0 needs to assign non-zero probability to every action y in every
context X. This is a strong condition that is not feasible in many real-world systems, especially if
the action space is large and many actions have poor reward.
If the support requirement is violated, ERM learning can fail catastrophically. We will show below
that the underlying reason is bias, not excessive variance that could be remedied through clipping
or variance regularization (Strehl et al., 2011; Swaminathan & Joachims, 2015a). To quantify how
support deficient a logging policy is, we denote the set of unsupported actions for context X under
π0 as
U(χ, ∏o) ：= {y ∈ Y∣∏o(y∣χ) = 0}.
The bias of the IPS estimator is then characterized by the expected reward on the unsupported
actions.
Proposition 1. Given contexts X 〜P (X) and logging policy ∏o(Y∣x), the bias of RIPS for target
policy π(Y∣x) is equal to the expected reward on the unsupported action sets, i.e., bias(π∣∏o)=
Ex [- Py∈U(x,∏o) π(ylx)δ(x,y)].
The proof is in Appendix A.1. From Proposition 1, it is clear that support deficient log data can
drastically mislead ERM learning. To quantify the effect of support deficiency on ERM, we define
the support divergence between a logging policy π0 and a target policy π as follows.
Definition 2 (Support Divergence). For contexts X 〜P(X) and any corresponding pair of target
policy π and logging policy π0, the Support Divergence is defined as
DX (n|no) = Xj:(X)
E	n(y|X) .
y∈U(x,π0)
(5)
3
Under review as a conference paper at ICLR 2020
With this definition in hand, we can quantify the effect of support deficiency on ERM learning for a
policy space Π under logging policy π0 .
Theorem 1. For any given hypothesis space Π with logging policy π0 ∈ Π, there exists a
reward distribution Pr with support in [rmin , rmax] such that in the limit of infinite training
data, ERM using IPS over the logged data D 〜P(X) X ∏o(∙∣X) X Pr can select a policy
π ∈ argmax∏∈∏ ED[Rips(∏)] that Isat least (rmax 一 rmin) max∏∈∏ DX(∏∣∏o) SuboPtImaL
The proof is in Appendix A.2. To illustrate the theorem, consider a problem with rewards r ∈
[一1, 0]. Furthermore, consider a policy space Π that contains a good policy πg with R(πg) = 一0.1
and a bad policy ∏b with R(∏b) = -0.7. If policy ∏ has support divergence DX(∏b∣∏o) = 0.6 or
larger, then ERM may return the bad πb instead of πg even with infinite amounts of training data.
Note that it is sufficient to merely have one policy in Π that has large support deficiency to achieve
this suboptimality. It is therefore crucial to control the support divergence DX (∏∣∏o) uniformly
over all π ∈ Π, or to account for the suboptimality it can induce. To this effect, we explore three
approaches in the following.
3.1	Safe learning by restricting the action space
The first and arguably most direct approach to reducing DX (∏∣∏o) is to disallow any action that
has zero support under the logging policy. For the remaining action set, the logging policy has full
support by definition. This restriction of the action set can be achieved by transforming each policy
π ∈ Π into a new policy that sets the probability of the unsupported actions to zero.
∏(y∣χ) -→ ∏(y∣χ):
n(yIx) H{y∈U(x,∏o)}
1 - Py0∈U(x,∏o) π(y0|x)
(6)
This results in a new policy space Π. All ∏ ∈ Π have support divergence of zero DX (∏∣∏0) = 0 and
ERM via IPS is guaranteed to be unbiased.
While this transformation of the policy space from Π to Π is conceptually straightforward, it has two
potential drawbacks. First, restricting the action space without any exceptions may overly constrain
the policies in Π. In particular, if the optimal action y* for a specific context x does not have
support under the logging policy, no ∏ ∈ Π can ever choose y* even if there are many observations
of similar y’s on similar context x0. The second drawback is computational. For every context x
during training and testing, the system needs to evaluate the logging policy ∏o(y∣χ) to compute the
transformation from ∏ to ∏. This can be prohibitively expensive especially at test time, where - after
multiple rounds of off-policy learning with data from previously learned policies - we would need
to evaluate the whole sequence of previous logging policies to execute the learned policy.
3.2	Safe learning through reward extrapolation
As illustrated above, support deficiency is a problem of blind spots where we lack information about
the rewards of some actions in some contexts. Instead of disallowing the unsupported actions like in
the previous section, an alternative is to extrapolate the observed rewards to fill in the blind spots. To
this effect, we propose the following augmented IPS estimator that imputes an extrapolated reward
δ(x, y) for each unsupported action y ∈ U(x, π0).
1n
RIPS(n) = n X
i=1
⅛⅛⅛ri+ X	n(y|xi应xi,y)
y∈U(xi,π0)
(7)
In the following proposition, we characterize the bias of the augmented IPS estimator for any given
reward extrapolation δ(x, y). We denote the mean of the reward r for context x and action y with
δ(x, y) = Er~p(r∣χ,y) [r]. Furthermore, ∆(x, y) := δ(x, y) - δ(x, y) denotes the error of the reward
extrapolation for each x and y.
Proposition 2. Given contexts x1, x2, . . . , xn drawn i.i.d from the unknown distribution P(X), for
action yi drawn independently from logging policy ∏o with probability ∏o(Y∣xi), the bias of the
empirical risk defined in Equation (7) is Ex[£y∈u∏o ∏(y∣x)∆(x, y)].
4
Under review as a conference paper at ICLR 2020
In this way we can learn in the original action
and policy space, but mitigate the effect of the
support deficiency by explicitly incorporating
the extrapolated reward δ(x, y). We explore
two choices for δ(x, y) in the following, which
provide different types of guarantees.
Conservative Extrapolation. To minimize the
user impact of randomization in the logging
policy, it is generally desirable to put zero prob-
ability on actions the are very likely to have
low (or even catastrophic reward). This means
that precisely those bad actions are likely to
not be supported in the logging policy. A key
danger of blind spots regarding those actions is
that naive IPS training will inadvertently learn
Algorithm 1: Data Augmentation
input: original logged dataset D, replaycount k,
reward estimate δ(x, y); output: D0;
initialization: D0 = 0 ;
for j = 1, . . . , k do
for i = 1, . . . , n do
Define Uxi to be the uniform distribution
over U(xi, π0);
Draw y 〜Uxi;
D= D S{χi,y,δ(χi,y), ∣u(xi,∏o)∣ };
end
end
a policy that selects those actions. This can be avoided by being maximally conservative about
unsupported actions and imputing the lowest possible reward ∀x, y ∈ U (x, π0) : δ(x, y) = rmin.
Intuitively, by imposing the worst possible reward for the unsupported actions, the learning algo-
rithm will aim to avoid these low-reward areas. However, unlike for the ∏ policies resulting from
the restricted action space, the learned policy is not strictly prohibited from choosing unsupported
actions - it is merely made aware of the maximum loss that the action may incur. Note that for
problems where rmin = 0, the naive IPS estimator is identical to conservative extrapolation since
the second term in Equation (7) is zero.
Regression Extrapolation. Instead of extrapolating with the worst-case reward, we may have addi-
tional prior knowledge in the form of a model-based estimate that reduces the bias. In particular, we
explore using a regression estimate δ = argmin§e § P2ι(δθ(xi,yi) 一 ri)2 that extrapolates from
the observed data D. Typically, δθ comes from a parameterized class of regression functions. Other
regression objectives could also be used, such as weighted linear regression that itself uses impor-
tance sampling as weights (Farajtabar et al., 2018). But, fundamentally, all regression approaches
assume that the regression model is not misspecified and that it can thus extrapolate well. Note that
the IPS part of Equation (7) can be changed to any estimators (with action set restricted on U(x, π0)c
for all x), and it turns out that doubly robust (Dudik et al., 2011) and CAB (SU et al., 2019) are special
extensions of regression extrapolation that substitute the IPS part with their corresponding estimator.
Efficient Approximation. Evaluating the augmented IPS estimator from Equation (7) can be com-
putationally expensive if the number of unsupported actions U(x, π0) is large. To overcome this
problem, we propose to use sampling to estimate the expected reward on the unsupported action,
which can be thought of as augmenting the dataset D with additional observations where the log-
ging policy has zero support. In particular, we propose the data-augmentation procedure detailed in
Algorithm 1. With the additional bandit data D0 = {xj,yj, δ(xj ,yj ),pj }m=1 from Algorithm 1, the
new objective is
arg min J 1 XX
∏∈π I n i=1
n(yi|Xi) r, + ɪ XX n(yj|xj) δ(χo, y0) ∖
∏o(yi∣Xi) "	m j=1	Pj j, j
(8)
In Appendix A.5, we show that the empirical risk in Equation (8) has the same expectation (over
randomness in D and D0) as RIδPS(D) and can thus serve as an approximation for Equation (7).
3.3	Safe Learning by Restricting the Policy Space
As motivated by Theorem 1, the risk of learning from support deficient data scales with the maxi-
mum support divergence DX(∏∣∏o) among the policies in the policy space Π. Therefore, our third
approach restricts the policy space to the subset Πκ ⊂ Π that contains the policies π ∈ Π with an
acceptably low support divergence DX (∏∣∏o) ≤ κ.
Πκ = {π∣π ∈ Π ∧ Dχ(π∣∏o) ≤ κ}	(9)
The parameter κ has an intuitive meaning. It specifies the maximum probability mass that a learned
policy can place on unsupported actions. By limiting this to κ, we limit the maximum bias of
5
Under review as a conference paper at ICLR 2020
the ERM procedure according to Proposition 2 while not explicitly torquing the rewards like in
conservative reward imputation.
A key challenge, however, is implementing this restriction of the hypothesis space, such that the
ERM learner ∏ = arg max∏∈∏κ [Rips(∏)] only considers the subset Πκ ⊂ Π. In particular, We do
not have access to the context distribution P(X) for calculating DX (∏∣∏o), nor would it be possible
to enumerate all ∏ ∈ Π to check the condition DX (∏∣∏o) ≤ κ, which itself requires a possibly
infeasible iteration over all actions. The following theorem (with proof in Appendix A.3) gives us
an efficient way of estimating and controlling DX(∏∣∏o) without explicit knowledge of P(X) or
access to the logging policy π0 beyond the logged propensities.
Theorem 2. For contexts xi drawn i.i.d from P(X), action yi drawn from logging policy π0, we
define SD (∏∣∏o) = 1 pn=1 ∏(yi | Xi)). For any policy π it holds that
E E	[Sd (∏∣∏o)] + Dχ(∏∣∏o) = 1	(10)
X〜P(X) y〜∏o(∙∣x)
Using this theorem, the following proposition (proof in Appendix A.4, empirically verified in Ap-
pendix B) gives us an efficient way of implementing the constraint DX (∏∣∏o) ≤ K via 1 - SD (∏∣∏o).
Proposition 3. For any given K ∈ (0,1), 0 < e < κ∕2, let Pmin denote the minimum propen-
sity under SuPPorted set Pmin = maxχ,y∈u(χ,∏0)c∏o(y∣x), then with probability larger than
1-2 exp(-2ne2pmmin), the constraint 1 —κ+e ≤ SD (π∣∏o) ≤ 1—e will ensure 0 ≤ DX (∏∣∏o) ≤ K.
We can thus use 1 - SD(∏∣∏o) as a surrogate for DX(∏∣∏o) in the training objective:
arg min 1 X πw⅛⅛ri. subject to 1 - K + e ≤ 1 X π(yi叫 ≤ 】-
∏w∈π n M ∏o(yi∣Xi)	n M ∏o(yi∣Xi)
Using Lagrange multipliers, an equivalent dual form of Equation (11) is:
max min ɪ	πw(yiJxi)(% + 〃1 — 〃？) — 〃[(1 — ^十 〃？(1 — K + E)
u1 ,u2 ≥0 πw ∈Π n	π0 (yi |xi )
(11)
(12)
For each fixed (u1, u2 ) pair, the inner minimization objective is ERM with IPS under a shift of
the reward. Instead of maximizing over (u1, u2) in the outer objective, we treat (u1 - u2) as a
hyperparameter that we select on a validation set. We explore various estimators for this model-
selection problem in Section 4.
Note that, among the methods we proposed for dealing with support deficiency, this approach is the
most efficient to implement, and it does not require access to the logging policy during training or
testing. Furthermore, the form of the inner objective coincides with that of BanditNet (Joachims
et al., 2018), which is known to work well for deep network training by controlling propensity
overfitting (Swaminathan & Joachims, 2015a).
4	Empirical Evaluation
We empirically evaluate the effectiveness and robustness of the three proposed approaches: restrict-
ing the action space, conservative and regression extrapolation, as well as restricting the policy
space. The semi-synthetic experiments are based on two real-world datasets: one is the popular im-
age classification dataset CIFAR10 (Krizhevsky et al.) and the other is the credit-card fraud dataset
of Dal Pozzolo et al. (2015). We use the naive IPS estimator and the regression-based Direct Method
(DM) as baselines.
The experiments are set up as follows. We first create a train-validation-test split for both datasets.
The training set is used to generate bandit datasets for learning, the validation set is used to gener-
ate bandit datasets for model selection, and the full-information test set serves as ground truth for
evaluating the learned policies. To simulate bandit feedback for the CIFAR10 dataset, our experi-
ment setup follows traditional supervised → bandit conversion for multi-class classification datasets
(Beygelzimer & Langford, 2009). To not only have bandit data with binary multi-class rewards, we
6
Under review as a conference paper at ICLR 2020
choose a different methodology for the credit-card dataset by designating some features as corre-
sponding to actions and rewards. More details are given in Appendix B.
To get logging policies for generating bandit feedback, we start by training a softmax-policy as in
Equation (4) on a subset of the full-information data. We then introduce a temperature parameter τ
into the learned policy via τfw (x, y) to be able to control its stochasticity and support deficiency.
In particular, we enforce zero support for some actions by clipping the propensities to 0 if they are
below a threshold of = 0.01. The larger τ, the higher the support deficiency. Note that making
the threshold at = 0.01 allows us to control support while the variance of IPS stays bounded. This
allows us to study support deficiency without having to worry about variance control.
For both logging and target policies, we train softmax policies where fw (x, y) is a neural network.
We use the ResNet20 architecture (He et al., 2016) for CIFAR10, and a fully-connected 2-layer
network for the credit-card dataset.
ClFAR	Credit Card	Credit Card - Translated
φ 0.25
O
Φ 0.20
P
SJ 0.15
g 0.10
昌 0.05
<Λ 0.00
0	45	60 70 80
% Unsupported actions
DM Hardmax
---Naive IPS
0	20	45	60 70 80
% Unsupported actions
0	20	45	60 70 80
% Unsupported actions
—∙= Conservative Extrapolation —Regression Extrapolation →∙- DR
Action Restriction	—Policy Restriction
Figure 1:	Learning results with varying support deficiency in the logging policy.
How do the methods perform at different level of support deficiency? Results are shown in
Figure 1. First, as expected, learning using naive IPS degrades on both datasets as we make the
logging policy more peaked and the number of unsupported actions increases. Note that naive IPS
coincides with Conservative Extrapolation, since both datasets are scaled to have a minimum reward
of zero. In the rightmost column, however, we translated the rewards to [-1, 0]. This has a strong
detrimental effect on naive IPS, as it is now overly optimistic about unsupported actions. Second,
the approach of dealing with support deficiency by restricting the action space also performs poorly.
The second row of plot sheds some light on this, as it shows the support divergence DX (∏∣∏o) of
the learned policy. It is zero for Action Restriction as expected, which means that bias is not the
problem. Instead, as the number of unsupported actions increases, the best actions are more likely to
be pruned and unavailable in the restricted policy space Π. Third, Regression Extrapolation performs
better than Conservative Extrapolation on both datasets. In both cases, the DM model is quite good
which also benefits Regression Extrapolation. However, on the credit-card dataset the regression
seems better at ranking than at predicting the true reward, which explains why DM performs better
than Regression Extrapolation. Fourth, the method that performs well most consistently is Policy
Restriction. Unlike all the other IPS-based methods, it performs well even under the translated
rewards in the third column of Figure 1. This is because the objective of Policy Restriction coincides
with that of BanditNet (Joachims et al., 2018), which is known to remedy propensity overfitting due
to the lack of equivariance of the IPS estimator (Swaminathan & Joachims, 2015b).
How does the learning performance change with more training data? Results are shown in
Figure 2. As the number of bandit examples increases, Policy Restriction, Regression Extrapolation
and DM dominate over most of the range especially when the percentage of unsupported actions is
large. Among the other methods, Action Restriction can take the least advantage of more data. This
is plausible, since its maximum performance is limited by the available actions. For similar reasons,
Conservative Extrapolation (and equivalently IPS) also flattens out, since it also tightly restricts the
action space by imputing the minimum reward.
7
Under review as a conference paper at ICLR 2020
43% Unsupported Actions
CIFAR
81% Unsupported Actions
Credit Card
46% Unsupported Actions 80% Unsupported Actions
0.01 0.05 0.10	0.25	0.01 0.05 0.10	0.25	0.01 0.10	0.23	0.46	0.01 0.10	0.23	0.46
# Bandit Data (Million)	# Bandit Data (Million)	# Bandit Data (Million)	# Bandit Data (Million)
DM Hardmax —Conservative Extrapolation —Regression Extrapolation
---Naive IPS I Action Restriction	∙ Policy Restriction
Figure 2:	Learning results with varying amounts of bandit data on CIFAR10 and credit-card dataset.
% Unsupp.	Oracle	Regr. Extrap.	DM	Cons. Extrap.	SNIPS
45	0.878	0.878	0.878	0.878	0.876
60	0.871	0.871	0.871	0.871	0.871
70	0.858	0.858	0.856	0.858	0.858
77	0.856	0.854	0.854	0.856	0.856
80	0.855	0.855	0.855	0.838	0.849
Figure 3:	Model selection performance on CIFAR10.
How effective are the estimators for model selection? Most learning algorithms have hyperpa-
rameters, and we now evaluate how the estimators perform for this secondary learning problem. We
specifically focus on the parameter k := u1 - u2 in Policy Restriction, since it controls how much
the learned policies can step outside the region of support. The table on the left of Figure 3 shows the
reward of the learned policy when performing model selection with the respective estimator. Oracle
is the estimator that has access to the full-information validation set, and can thus be considered as a
skyline. We also included the SNIPS estimator (Swaminathan & Joachims, 2015b), which imputes
the average reward on the supported action for the unsupported actions (Gilotte et al., 2018). All
estimators perform quite well for model selection on CIFAR, and the results are analogous for the
credit-card data (see Appendix B.2). However, the plot to the right of Figure 3 reveals that SNIPS
does not accurately reflect the shape of the Oracle curve. Both Regression Extrapolation and DM,
however, are found to be sufficiently accurate for reliable model selection.
5 Discussion and Conclusions
We identified and analyzed how off-policy learning based on IPS weighting can suffer severely de-
graded learning performance when the logging policy is support deficient. To remedy this problem,
we explored approaches that limit the impact of missing support through three different means: re-
stricting the action space, reward extrapolation and restricting the policy space. We find that the
most natural approach of restricting the action space is neither computationally efficient, nor does
it learn accurate policies. Reward extrapolation through regression and restricting the policy space,
however, both perform well and robustly even at high levels of support deficiency. Among those two
methods, reward extrapolation has the potential drawback that we need to compute (and/or sample
from) the complement of the logging policy, which can be computationally challenging. Further-
more, having to store all old logging policies is inconvenient in practice. This makes the approach
of restricting the policy space particularly attractive, since it is computationally efficient and it does
not require access to the logging policy beyond the logged propensity values.
8
Under review as a conference paper at ICLR 2020
References
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming
the monster: A fast and simple algorithm for contextual bandits. In International Conference on
Machine Learning (ICML), 2014.
Alina Beygelzimer and John Langford. The offset tree for learning with partial labels. In Pro-
ceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data
mining,pp.129-138. ACM, 2009.
Leon Bottou, Jonas Peters, JoaqUin QUinonero-Cande山,Denis X Charles, D Max Chickering, Elon
Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning
systems: The example of computational advertising. The Journal of Machine Learning Research,
14(1):3207-3260, 2013.
Andrea Dal Pozzolo, Olivier Caelen, Reid A Johnson, and Gianluca Bontempi. Calibrating prob-
ability with undersampling for unbalanced classification. In 2015 IEEE Symposium Series on
Computational Intelligence, pp. 159-166. IEEE, 2015.
Miroslav Dudk John Langford, and Lihong Li. Doubly robust policy evaluation and learning. In
International Conference on Machine Learning (ICML), 2011.
Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust
off-policy evaluation. In International Conference on Machine Learning, pp. 1446-1455, 2018.
Alexandre Gilotte, Clement Calauzenes, Thomas Nedelec, Alexandre Abraham, and Simon Dolle.
Offline a/b testing for recommender systems. In Proceedings of the Eleventh ACM International
Conference on Web Search and Data Mining, pp. 198-206. ACM, 2018.
Evan Greensmith, Peter L Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient
estimates in reinforcement learning. Journal of Machine Learning Research, 5(Nov):1471-1530,
2004.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
T. Joachims, A. Swaminathan, and T. Schnabel. Unbiased learning-to-rank with biased feedback. In
ACM Conference on Web Search and Data Mining (WSDM), 2017.
T. Joachims, A. Swaminathan, and M. de Rijke. Deep learning with logged bandit feedback. In
International Conference on Learning Representations (ICLR), 2018.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-
search). URL http://www.cs.toronto.edu/~kriz/cifar.html.
John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side
information. In Advances in neural information processing systems, pp. 817-824, 2008.
Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. Unbiased offline evaluation of contextual-
bandit-based news article recommendation algorithms. In Proceedings of the fourth ACM inter-
national conference on Web search and data mining, pp. 297-306. ACM, 2011.
Lihong Li, Shunbao Chen, Jim Kleban, and Ankur Gupta. Counterfactual estimation and optimiza-
tion of click metrics in search engines: A case study. In Proceedings of the 24th International
Conference on World Wide Web, pp. 929-934. ACM, 2015.
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Off-policy policy gradient with
state distribution correction. arXiv preprint arXiv:1904.08473, 2019.
Ben London and Ted Sandler. Bayesian counterfactual risk minimization. In International Confer-
ence on Machine Learning, pp. 4125-4133, 2019.
Alex Strehl, John Langford, Lihong Li, and Sham M Kakade. Learning from logged implicit explo-
ration data. In Advances in Neural Information Processing Systems (NIPS), 2011.
9
Under review as a conference paper at ICLR 2020
Yi Su, Lequn Wang, Michele Santacatterina, and Thorsten Joachims. Cab: Continuous adaptive
blending for policy evaluation and learning. In International Conference on Machine Learning,
pp. 6005-6014, 2019.
A. Swaminathan and T. Joachims. Batch learning from logged bandit feedback through counter-
factual risk minimization. Journal of Machine Learning Research (JMLR), 16:1731-1755, Sep
2015a. Special Issue in Memory of Alexey Chervonenkis.
A. Swaminathan and T. Joachims. The self-normalized estimator for counterfactual learning. In
Neural Information Processing Systems (NIPS), 2015b.
Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement
learning. In International Conference on Machine Learning (ICML), 2016.
Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dudik. Optimal and adaptive off-policy evaluation
in contextual bandits. In International Conference on Machine Learning (ICML), 2017.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
10
Under review as a conference paper at ICLR 2020
A Appendix: Proofs
In this appendix, we provide proofs of the main theorems and propositions.
A.1 Proof of Proposition 1
Proposition 1. Given contexts X 〜P (X) and logging policy ∏o(Y∣x), the bias of RIPS for target
policy π(Y∣x) is equal to the expected reward on the unsupported action sets, i.e., bias(π∣∏o)=
Ex [- Py∈U(x,∏0) n(y|x)S(X, y)].
Proof. Recall δ(x,y) = Er [r(x, y)|x, y], and logged data D 〜PX X ∏o(∙∣X) XPr.
bias(π∣∏o) = E[RRIps (π)] — R(π)
=E[ X	no(y|x) π(ylx)δ(x,y) - X n(y|x)s(X,y)]
x	π0 (y |X)
y∈(U(x,π0))c	y∈Y
=E[—	X	n(y|x)s(X,y)]
x
y∈U (x,π0)
(13)
□
A.2 Proof of Theorem 1
Theorem 1.	For any given hypothesis space Π with logging policy π0 ∈ Π, there exists a
reward distribution Pr with support in [rmin, rmax] such that in the limit of infinite training
data, ERM using IPS over the logged data D 〜P(X) X ∏o(∙∣X) X Pr can select a policy
π ∈ arg max∏∈∏ ED [RIps(∏)] that is at least (rmax — rmin) max∏∈∏ DX(∏∣∏o) SUbOPtlmal.
Proof. For any given hypothesis space Π and logging policy π0, define a deterministic reward dis-
tribution Pr as the following: for all context X, r(X, y) = δ(X, y) = rmin for y ∈ U(X, π0)c
and r(x,y) = δ(x,y) = rmgχ for y ∈ U(x,∏o). Let ∏ ∈ argmaxπ∈∏ Dχ(∏∣∏o) and
∏* ∈ argmax∏∈∏ R(∏), then We have the following lower bound for R(∏*):
R(∏*) ≥ R(∏)
= Ex [	rmax +	rmin]
y∈U(x,π0)	y∈U(x,π0)c
=rmax max DX (n|nO) + rmin (1 — max DX (n|nO))
(14)
where the first inequality follows from the definition of ∏*, the first and second equality is based on
the specific reward distribution Pr and the definition of ∏.
In the following we will show that for any ∏ learned by the expectation of ERM (or in the limit of
infinite amount data), i.e., ∏ ∈ arg max ED [RDPs(∏)], ∏ have the same support as ∏o.
E[RIPS(∏)] = E[	E	∏(y∣x)rmin] = rmin E[	E	∏(y∣x)] ≤ Kn (15)
y∈U(x,π0)c	y∈U (x,π0)c
for all ∏ ∈ Π, then it is easy to see ∏o ∈ Π is one of the solution of ERM. Actually for any ∏ ∈
arg max ED [RDPs (∏)], Ex [Py∈u (X ∏0)c ∏(y∣x)] = 1 and it gives us that any solution of the ERM
has exactly the same support as ∏o, then we have R(Π) = rm^n for ∏ ∈ arg max ED [RDPs(∏)].
Combining the lower bound for R(∏*) and R(Π) = rmin, We have
R(∏*) — R(∏) ≥ rmax max Dx (∏∣∏0 ) + rmin (1 — max Dx(∏∣∏o)) — rmin
π∈Π	π∈Π
= (rmax — rmin)max Dχ (∏∣∏o)
π∈Π
(16)
□
11
Under review as a conference paper at ICLR 2020
A.3 Proof of Theorem 2
Theorem 2.	For contexts xi drawn i.i.d from P (X), action yi drawn from logging policy π0, we
define SD (∏∣∏o) = 1 pn=1 ∏(yi | Xi)). For any policy π it holds that
E E	[Sd (∏∣∏o)] + Dχ(∏∣∏o) = 1	(10)
X〜P(X) y〜∏o(∙∣x)
Proof.
E	[Sd(∏∣∏o)] + DX(∏∣∏o) = E[ X	∏o(y∣x) π(ylx)] + DX(∏∣∏o)
x,y~π0	X y∈U(X∏0)c	π0⑶x)
=E[ X	n(y|x)] + E[ X n(y|x)]
y∈U (X,π0 )c
E X n(y|x)] = 1
X y∈Y
(17)
y∈U(X,π0)
The first equality is based on definition of SD(∏∣∏o) and the second equality is based on definition
of support divergence.	□
A.4 Proof of Proposition 3
Proposition 3. For any given K ∈ (0,1), 0 < e < κ∕2, let Pmin denote the minimum propen-
sity under supported set Pmin = maxχ,y∈u(χ,∏0)c∏o(y∣x), then with probability larger than
1-2 exp(-2ne2pmbin), the constraint 1-κ+e ≤ SD (π∣∏o) ≤ 1—e will ensure 0 ≤ DX (∏∣∏o) ≤ K.
Proof. Recall SD(∏∣∏o) = 1 Pn=1 ^盥渭 With (xi, yi) draw i.i.d from P(X) X ∏o(Y∣χ). From
Appendix A.3, it is easy to see Eχ,y〜∏0(∙∣χ) [π∏0yXX)] = 1 一 DX(∏∣∏o). LetPmin denote the smallest
propensity under supported action set, Pmin := minχ,y∈u(χ,∏0)c ∏o(y∣χ) > 0, then the random
variable /(需)is strictly bounded between [0, p-ɪ-]. Applying Hoeffding,s bound gives:
P(Dx(π∣∏o) < 1 一 SD(π∣∏o) — e)= P(SD(π∣∏o) — (1 — Dx(∏∣∏o)) < T) ≤ exp(-2ne2pmiιn)
(18)
Since Sd(∏∣∏0) ≤ 1 — C gives 1 — SD(∏∣∏o) — C ≥ 0, then we have
P(DX(π∣∏o) < 0) ≤ exp(-2nc2pmmin)	(19)
Similar for the other direction, Hoeffding’s bound gives:
P(DX(π∣∏o) > 1 — SD(∏∣∏o) + C)= P(Sd(π∣∏o) — (1 — Dχ(∏∣∏o)) > c) ≤ exp(—2nCpmin)
(20)
Since Sd(∏∣∏0) ≥ 1 + C — K gives 1 — SD(∏∣∏o) + C ≤ κ, then we have
P(DX(π∣∏o) ≥ κ) ≤ exp(-2nC2pmmin)
Combining the above, we have
P(0 ≤ Dχ(∏∣∏o) ≤ κ) = 1 — P(Dχ(π∣∏o) < 0) — P(Dχ(π∣∏o) > κ)
≥ 1 — 2exp(—2nC2p2min)
(21)
(22)
□
A.5 Proof for efficient approximation
Claim 1. The empirical risk defined by in Equation (8) has the same expectation (over randomness
in D and sampling) as RδIPS (D).
12
Under review as a conference paper at ICLR 2020
Proof. Taking the expectation of empirical risk defined in Equation (8):
E
n(yi|Xi) r +
πo(y∕Xi) i
m
-1 X
m
j=1
j) δ(xj M)]
E [ y∈B …Π0yx) i" E [ y.X∏o)	^i
E [ E	∏(y∣χ)δ(χ,y)] + E [ E	∏(y∣χ)δ(χ,y)]
y∈U (x,π0 )c	y∈U (x,π0 )
TL T	∙11 1	∙ . 1	.1	.	∙ . 1 T^∖Λ / ∖
Now we will show it has the same expectation with RIδP S (π)
E[∏π((⅛⅛ri+ X	π(ylxi)δ(xi，y)]
0 yi i	y∈U(xi,π0 )
=E [ E [ π(ylχ) δ(x,y)] + X	π(y0∣x)δ(x,y0)]
X y~π0 no(y|x)	yθ∈U(X,∏o)
=E[ X	no(y|x) π(ylx)δ(x,y) + X	π(y1χ)δ(χ,y0)]
X	∏θ(y∣x)
y∈U (x,π0 )c	y0∈U(x,π0 )
=E [ X	π(ylx)δ(x,y)] + E [ X n(y|x)i(x,y)]
y∈U (X,π0 )c	y∈U(X,π0)
The proof is done by comparing Equation (23) and Equation (24).
(23)
(24)
□
A.6 Proof of Proposition 2
Proposition 2. Given contexts x1, x2, . . . , xn drawn i.i.d from the unknown distribution P(X), for
action yi drawn independently from logging policy ∏o with probability ∏o(Y∣Xi), the bias of the
empirical risk defined in Equation (7) is Ex[£y∈u∏o ∏(y∣x)∆(x, y)].
Proof. From Appendix A.5, we are given the expectation of RIδPS(π), and the bias is:
bias(RIPS(n)) = E[R，PS(n)] - R(π)
=E [ X	π(ylx)δ(x,y) + X	π(ylx)δ(x,y)] - R(π)
y∈U (X,π0 )c	y∈U(X,π0)
=E [ X	∏(y∣χ)(δ(χ,y) - δ(χ,y))]
X
y∈U (X,π0 )
=E [ X	∏(y|χ)∆(χ,y)]
X
y∈U (X,π0 )
(25)
The second equality is from Appendix A.5, the second equality is based on R(π) =
Ex [ Py∈u(X,∏0)c ∏(y∣x)δ(x, y) + Py∈u(4尸。)∏(y∣x)δ(x, y)], and the last one is based on the def-
inition of ∆(x, y) := δ(x, y) — δ(x, y) for all X ∈ X, y ∈ Y.	□
13
Under review as a conference paper at ICLR 2020
B Appendix: Experiments
In this section, we provide the experiment details and additional results to help promote reproducibil-
ity of this work.
B.1	Experiment setup details
Datasets and baseline. We follow a 75:10:15 train-validation-test split for credit card fraud detec-
tion dataset, while for CIFAR10 already coming with a train-test split, we keep 10% of the training
set as validation set. Baseline estimators are IPS and DM, the hyperparameters (learning rate, L2
regularization) are optimized for all the methods based on the validation set.
Bandit data generation. For CIFAR10, given supervised data {xi, y*}n=ι where Xi denotes the
3072 features and y* denotes the correct label of data (ranging from 0 to 9), under logging policy
∏o, the logged bandit data is generated by drawing y% 〜∏o(Y∣Xi), then a deterministic reward is
defined as 1[{yi=y*}. For the credit card fraud detection dataset, We throw away the class label and
only use the features for each sample to generate bandit data. To be specific, for each sample with a
28-dimensional feature vector, we define the first 20 features as the contextual information, and use
the remaining 8 features as the underlying true reward for 8 different actions (with normalization).
Logging policy. For CIFAR, we learn the softmax logging policy on 35K full-information data
points as a multi-class classification problem with cross-entropy loss. Similar as the experiments
on BanditNet (Joachims et al., 2018), we adopt the conventional ResNet20 architecture but restrict
training after a mere two epochs to derive a relative stochastic policy, since it will be easier to add
temperature later to control its stochasticity and support deficiency. Similarly, for the credit card
fraud detection dataset, the softmax logging policy is learned on 8K full-information data points
by treating it as a multi-class classification problem using cross-entropy loss and the label being the
action with the highest reward on this specific context. For CIFAR, the logging policy we trained has
a 57.43% accuracy on the test-set; whereas for the credit card fraud detection dataset, the logging
policy has an expected true reward of 0.71.
Reward estimator. For each experiment, we train a different regression function using the full
bandit dataset. We use the same architecture as the one used for off-policy learning - where the final
layer is the size of the actions, specifying the reward for each action given a particular context. The
regression function is trained using the MSE objective.
ClFAR
Credit Card
1.002
1.000
0.998
0.996
0.994
0.992
# Bandit Data (Million)
# Bandit Data (Million) # Bandit Data (Million) # Bandit Data (Million)
% Unsupported Actions
—60%	—70%	—75%	-→≈ 80%
Figure 4:	Behaviour of SD(π∣∏o) + Dχ(∏∣∏o)
B.2 Supplementary results
Reliability of approximating support divergence using control variate. In this experiment, we
empirically verify the reliability of estimating support divergence using control variate. The tar-
get policy is the uniform policy while the logging policies are varying in their support deficiency.
Results are averaged over 10 runs and is shown in Figure 4. We investigate the behaviour of
SD(∏∣∏o) + DX(∏∣∏o) under different number of training data and different support deficiency
of the corresponding logging policy. As we can see, the sum converges to 1 as the training data
14
Under review as a conference paper at ICLR 2020
% Unsupp.	Oracle	Reg. Extrap.	DM	Cons. Extrap.	SNIPS
0	0.7934	0.793	0.793	0.793	0.793
20	0.803	0.791	0.803	0.791	0.803
45	0.799	0.798	0.799	0.795	0.798
60	0.797	0.797	0.796	0.797	0.757
70	0.787	0.787	0.780	0.771	0.754
75	0.778	0.778	0.778	0.766	0.774
80	0.774	0.759	0.759	0.751	0.759
Figure 5:	Model selection result for credit card fraud detection
increases. Meanwhile, the variance decreases as shown in the right figure. For different support
deficiency curves, the curve converges in a similar fashion and we conjecture it is due to the effect
of clipping the propensity at the same threshold = 0.01, which makes pmin = 0.01 in the bound
shown in Proposition 3.
Model selection comparison for the credit card dataset. The model selection comparison over
the credit card fraud detection dataset is demonstrated in Figure 5. Similar as the trend in Figure 3,
SNIPS and Conservative Extrapolation exhibit a large bias, also SNIPS even can not reflect the shape
of the Oracle curve. DM and Regression Extrapolation closely track the Oracle line, and they have
the best performance when used in model selection, as seen in the left table of Figure 5.
15