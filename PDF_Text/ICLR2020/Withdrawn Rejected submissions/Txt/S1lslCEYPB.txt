Under review as a conference paper at ICLR 2020
Improved Mutual Information Estimation
Anonymous authors
Paper under double-blind review
Ab stract
We propose a new variational lower bound on the KL divergence and show that
the Mutual Information (MI) can be estimated by maximizing this bound using a
witness function on a hypothesis function class and an auxiliary scalar variable. If
the function class is in a Reproducing Kernel Hilbert Space (RKHS), this leads to
a jointly convex problem. We analyze the bound by deriving its dual formulation
and show its connection to a likelihood ratio estimation problem. We show that the
auxiliary variable introduced in our variational form plays the role of a Lagrange
multiplier that enforces a normalization constraint on the likelihood ratio. By
extending the function space to neural networks, we propose an efficient neural MI
estimator, and validate its performance on synthetic examples, showing advantage
over the existing baselines. We then demonstrate the strength of our estimator in
large-scale self-supervised representation learning through MI maximization.
1 Introduction
Mutual information (MI) is an ubiquitous measure of dependency between a pair of random variables,
and is one of the corner stones of information theory. In machine learning, the information maxi-
mization principle for learning representation from unlabeled data through self-supervision (Bell &
Sejnowski, 1995) motivated the development of many MI estimators and applications (Hjelm et al.,
2019; Noroozi & Favaro, 2016; Kolesnikov et al., 2019; Doersch et al., 2015; van den Oord et al.,
2018b; Hu et al., 2017). The information bottleneck (Tishby et al., 1999; Kolchinsky et al., 2017) is
another principle that triggered recent interest in mutual information estimation. MI is also used to
understand the information flow in neural networks, in learning clusters (Krause et al., 2010) and in
regularizing the training of Generative Adversarial Networks (GANs) (Chen et al., 2016).
In many of those machine learning applications and other scientific fields, one has to estimate MI
given samples from the joint distribution of high dimensional random variables. This is a challenging
problem and many methods have been devised to address it. Since MI is defined as the Kullback-
Leibler (KL) divergence between the joint distribution and the product of marginals, one can leverage
non parametric estimators of f-divergences (Nguyen et al., 2008; Nowozin et al., 2016; Sriperumbudur
et al., 2009). Specifically of interest to us is the Donsker-Varadhan (DV) representation of the KL
divergence (Donsker & Varadhan, 1976) that was used recently with neural networks estimators
(Belghazi et al., 2018; Poole et al., 2019). Other approaches to estimating the MI are through finding
lower bounds using variational Bayesian methods (Alemi et al., 2016; 2017; Barber & Agakov, 2003;
Blei et al., 2017), as well as through geometric methods like binning (Kraskov et al., 2004).
In this paper we propose a new estimator of MI that can be used in direct MI maximization or as
a regularizer, thanks to its unbiased gradients. Our starting point is the DV lower bound of the KL
divergence that we represent equivalently via a joint optimization that we call η-DV on a witness
function f and an auxiliary variable η in Section 2. In Section 3, we show that when the witness
function f is learned in a Reproducing Kernel Hilbert Space (RKHS) the η-DV problem is jointly
convex in both f and η. The dual of this problem sheds the light on this estimator as a constrained
ratio estimation where η plays the role of a Lagrange multiplier that ensures proper normalization of
the likelihood ratio. We also show how the witness function can be estimated as a neural network akin
to Belghazi et al. (2018). We specify our estimator for MI in Section 4, and show how it compares
to alternatives in the literature (Nguyen et al., 2008; Belghazi et al., 2018; Poole et al., 2019). The
experiments are presented in Section 5. On synthetic data, we validate our estimators by estimating
MI on Gaussian variables and by regularizing GAN training as in Chen et al. (2016). On real data,
1
Under review as a conference paper at ICLR 2020
Lower Bounds on
KL(P, Q)
Estimators of
I(X; Y)
Figure 1: Overview of the paper. Top row shows several KL divergence lower bounds for two probability
distributions P and Q. By substituting P = Pxy, Q = Pxpy, and defining f as a neural network, we obtain
corresponding MI estimators. n-DV and n-MINE are the proposed bound and its derived estimator. Further
details are provided in the text.
we explore our estimator in deep MI maximization for learning representation from unlabeled data.
Figure 1 shows an overview of all the bounds and related MI estimators discussed in this paper.
2 Lower Bounds on KL Divergences and Mutual Information
Consider two probability distributions P and Q, where P is absolutely continuous w.r.t. Q. Let P and
q be their respective densities defined on X ⊂ Rd. Their KL divergence is defined as
KL(P, Q)
Ex 〜Plog
p(x) log
We are interested in the MI between two random variables X, Y where X is defined on X ⊂ Rdx ,
and Y on Y ⊂ Rdy. Let pxy be their joint densities andpx,py the marginals of X and Y respectively.
The MI is defined as follows:
I(X; Y ) = KL(pxy,pxpy),	(1)
which is the KL divergence between the joint density and the product of marginals. Non-parametric
estimation of MI from samples is an important problem in science and machine learning. In what
follows, we review variational lower bounds on KL to enable such estimation.
Variational Characterization of KL divergence. Let H be any function space mapping X to R.
The first variational characterization of the KL divergence goes back to Donsker & Varadhan (1976):
KL(P, Q) ≥ DH(P, Q) = sup{Ex〜Pf(X)- log(Ex〜Qef(x)): f ∈ H},	(2)
where the equality holds if and only if (iif) f * = log(p∕q) ∈ H. We refer to this bound as the DV
bound.
The second variational representation was introduced in Nguyen et al. (2008); Nowozin et al. (2016)
and derived through convex duality to be finally stated as follows:
KL(P, Q) ≥ DH(P, Q) = 1 + sup{Ex〜Pf(X)- Ex〜Qef(X) : f ∈ H},	(3)
with equality iif f* = log(p/q) ∈ H. We call this bound the f -div bound (as in f -divergence).
From Eq. 2 and Eq. 3 we see that the variational bounds are attempting to estimate the log-likelihood
ratio f* = log(p/q), and the tightness of the bound depends on the representation power of H.
In order to compare these two lower bounds, observe that log(t) ≤ t - 1, t > 0. Therefore
log (Ex〜Qef(X)) ≤ Ex〜Qef(X) - 1 which means that for any function space H we have:
KL(P, Q) ≥DDHV(P,Q) ≥ DfH (P, Q),	(4)
from which we conclude that the DV bound is tighter than the f -div bound.
Now, given samples {χi,i =1 ...N,Xi 〜P}, {yi,i =1 ...N,yi 〜Q}, estimating the KL diver-
gence can be done by computing the variational bound from Monte-Carlo simulation. specifically,
2
Under review as a conference paper at ICLR 2020
for the DV bound we have the following estimator:
:f ∈ H }.
(5)
Note that the Mutual Information Neural Estimator (MINE) (Belghazi et al., 2018) considered the
above formulation with the hypothesis class H being a neural network. For the f -div bound we
have similarly the following estimator:
1N	1N
Db H (P, Q) = 1+sup∣ NN y=2 f(xi)- NN y=2 ef (yi) : f ∈ H ʃ,	(6)
for which Nguyen et al. (2008) introduced and studied a convex estimator with H being an RKHS.
While DV bound is tighter than the f -div bound, in order to learn the function f using stochastic
gradient optimization, f -div is a better fit because the cost function is linear in the expectation,
whereas in the DV bound we have a log non-linearity being applied to the expectation. This non-
linearity introduces biases in the mini-batch estimation of the cost function as noted in Belghazi et al.
(2018). In the following, we show how to alleviate this problem and remove the non-linearity at the
price of an additional auxiliary variable that will enable better optimization for the DV bound.
An η-trick for the DV Bound. We start with the following elementary Lemma, that gives a
variational characterization of the log. All proofs are given in the Appendix A.
Lemma 1. Let x > 0, we have: log(x) = minη e-ηx + η - 1.
Using Lemma 1 we can now linearize the log in the DV bound.
Lemma 2 (η-Donsker-Varadhan). Let H be any function space mapping X to R:
KL(P, Q) ≥ DHV(P, Q) = - inf{L(f, η) ： f ∈ H,η ∈ R}	⑺
L(f, η) = e-ηEx〜Qef(X)- Ex〜Pf(X) + η - 1,	(8)
We refer to this bound as η-DV bound.
Note that for η=0 we recover the f -div bound. Using Lemma 2, we can now rewrite the estimator
for the DV bound in Eq. 5 as follows:
NN
DH(P,Q) = — fnf L(f,η), where L(f,η)= e-ηN X ef(yi) - X f (xi) + η - 1, (9)
enabling unbiased stochastic gradient optimization of the function f . We note that similar variational
tricks of non-linearities have been devised for g(η) = √η in Argyriou et al. (2008); Bach et al. (2011).
3 η-DV BOUND, CONVEXITY, MMD RATIO ESTIMATES AND LAGRANGIANS
We established that the DV bound is tighter than the f -div and derived η-DV as an alternative to
DV, enabling unbiased stochastic optimization. However, the role of η in improving the lower bound
remains unclear. To highlight its role, we consider the η-DV bound from Eq. 7, and restrict the
hypothesis function class to an RKHS, similarly to Nguyen et al. (2008). Working in the dual, we
can compare η-DV estimator to the f -div estimator and further understand this hierarchy of lower
bounds. See Figure 2 for an illustration.
For simplicity, and without loss of generality, we let RKHS be a finite dimensional feature map, i.e.,
H = {f |f (x) = hw, Φ(x)i , Φ : X → Rm, w ∈ Rm}. Now for f ∈ H, the loss given in Eq. 8 for
η-DV can be rewritten as follows:
L(f, η) δ L(w, η) = e-ηEx〜Qehw,φ(X)i - hw, Ex〜pΦ(x)i + η - 1.	(10)
Following Nguyen et al. (2008), we consider the following regularized loss: L (w, η) = L(w, η) +
Ω(w), and the corresponding sample-based formulation L(w, η) = L(w, η) + Ω(w), where Ω(w)
is a convex regularizer, e.g., Ω(w) = 2 ∣∣wk2. The n—DV primal problem can now be defined as:
n-DV-P : min L(w, η) + Ω(w).	(11)
w,η
3
Under review as a conference paper at ICLR 2020
Primal
f-div
sup {Ep [f ] - Eqef + 1 }
f ∈H
η-DV
sup {Ep [f ] - e-ηEq [ef ] - η + 1 }
f ∈H,η∈R
Dual in RKHS min {Eq [r log(r)] + EQm - 1 + λMMD2(rg,^)}嚼口 俚@ lr l°g(r)] + AMMD2(rq,p)}
Figure 2: Comparison of dual formations of η-DV and f -div in RKHS. Here r is an estimator for density ratio
p/q and MMDφ(P, Q) = ∣∣Eχ〜pΦ(x) - Ey〜Qφ(y)∣∣. As can be seen, both duals have a relative entropy term,
but the f -div bound does not impose the normalization constraint on the ratio, which biases the estimate, while
η-DV uses η as the Lagrangian multiplier to impose the constraint EQ [r] = 1 and ensure density normalization.
In the following, we show that L(w, η) is jointly convex in (w, η) and derive its dual, which will
shed light on the nature of the likelihood ratio estimate as well as the role of n.
Convex Estimate in RKHS. In Lemma 3 we first establish the convexity of the η-DV loss function.
ʌ
Lemma 3. L and L are jointly convex in W and η.
Lε(u,η) = L(u,η)+Ω(u)+εe-η. Our KL estimate is therefore given as Db H (P, Q) = -L(w*,η*),
ʌ
where (w* ,η*) =argmin(u,n)Lε(u,η).
η-DV Dual is a Constrained Likelihood Ratio Estimation. Another angle for estimating the KL
divergence is through likelihood ratio estimation (Mohamed & Lakshminarayanan, 2016). Given
a ratio estimate r of p/q, the KL divergence can be computed as EQr log(r), which can be easily
estimated using samples from Q.
In Theorem 1, proven in Appendix A, we show that the dual problem (denoted D), corresponding
to the primal minimization problem (denoted P) of η-DV, reduces to a constrained likelihood ratio
estimation problem (denoted C).
Theorem 1. Let Ω*(.) be the Fenchel conjugate of Ω(.). The η-DV bound restricted to an RKHS,
amounts to the following regularized convex minimization: P = — (minw,η L(w, η) + Ω(w)), that
has the following dual form:
D = min max
r>0 η
r(y) log (r(y)) q(y)dy + (η - 1)
r(y)q(y)dy - 1
X
+ Ω*(∆(r)),
X
X
where ∆(r) = X Φ(x)p(x)dx - X r(y)q(y)Φ(y)dy.
Noticing that η - 1 plays the role of a Lagrangian multiplier, this is equivalent to the following
likelihood ratio estimation problem:
C = min / r(y) log (r(y)) q(y)dy + Ω* (/ Φ(x)p(x)dx — / r(y)q(y)Φ(y)dy)
subject to:	r(y)q(y)dy = 1.
X
Therefore, we have P = D = C. Let (w* , η*) be an optimizer ofP. Let r* be an optimizer ofD ,
then the KL estimate is given by: DDHV (P, Q) = RX r* (y) log (r* (y)) q(y)dy = L(w*, η*).
The regularizer Ω(w) = 2 ∣∣wk2 can now be given the following interpretation based on the results
of Theorem 1. Recall the definition of the MMD (Gretton et al., 2012) between distributions using an
RKHS: MMDφ(P, Q) = ∣Ex〜PΦ(x) — Ey〜Qφ(y)∣. Replacing the Fenchel conjugate Ω*(.) by its
expression in Theorem 1, we see that η-DV is equivalent to the following dual (η-DV-D)
η-DV-D : min maX r r(y) log (r(y)) q(y)dy + (η —1)( r r(y)q(y)dy — 1)+ EMMDφ(rq,p)
r>0 η X	X	2λ
(12)
which can be written as a constrained ratio estimation problem (η-DV-C)
η-DV-C : min J r(y) log (r(y)) q(y)dy + 2λMMDφ(rq,p), subject to: J r(y)q(y)dy = 1.
(13)
4
Under review as a conference paper at ICLR 2020
Hence, it is clear now that the η-DV optimization problem is equivalent to the constrained likelihood
ratio estimation problem r given in Eq. 13, where the ratio is estimated using the MMD distance in the
RKHS between rq and p. It is also easy to see that p/q is a feasible point and for the universal feature
map MMDΦ(rq, p) = 0 iif r = p/q, therefore, for a universal kernel, p/q is optimal and we recover
the KL divergence for r = p/q. When comparing n-DV-D from Eq. 12 and η-DV-C from Eq. 13, We
see that η-1 plays the role ofa Lagrangian that ensures that rq is indeed a normalized distribution. In
practice, We solve the primal problem (P), While the dual problem (D) and its equivalent constrained
form (C) explain Why this formulation estimates the KL divergence and the role ofη as a Lagrangian
multiplier that enforces a normalization constraint. Let r* be the solution, then we have:
DDHV(P,Q) =	r* (y) log (r*(y)) q(y)dy = -L(w*, η*).
X
(14)
For comparison, the f -div bound, restricted to an RKHS, is equivalent to the following ratio esti-
mation (follows from the proof of Theorem 1 by eliminating max on η, and setting η= 0), and is
consistent with the results of Nguyen et al. (2008) (see Eq. 51 therein):
f-div :嗯n / r(y) log (r(y)) q(y)dy + 1 - 1 r(y)q(y)dy + ^mmdΦ(rq,p).	(15)
Let r* be the optimum, then the KL divergence can be estimated as follows Nguyen et al. (2008):
DfH (P, Q) = X
r*(y) log (r*(y)) q(y)dy +
1- X
r* (y)q(y)dy
(16)
Comparing DDHV (P, Q) and DfH (P, Q) we see that they both have a relative entropy term but the
f -div bound does not impose the normalization constraint on the ratio, which biases the estimate. We
ʌ
end with the following remark: In Theorem 1, if we replace the loss L by its empirical counterpart L
from Eq. 9, the equivalent Dual takes the following form:
1N	1N	1N	1N
mn>n NEri	log (Ti) + ω* ( N	E φ(Xi)- NEriφ(yi)	, SUbjeCt to: NEri	= 1,
ri N i=1	N	i=1	N i=1	N i=1
and the KL estimate is given by: D H (P, Q) = N PN=I r* log (ri*) = -L(w*,η*).
From RKHS to Neural Estimation. One shortcoming of the RKHS approach is that the method
depends on the choice of feature map Φ. We propose to learn Φ as a deep neural network as in MINE
(Belghazi et al., 2018). Given samples xi from P, yi from Q, the KL estimation problem becomes:
1N N
DNN(P, Q) = - (nφnnηιin e-η N N ehw,φM)i - i^(w, Φ(x∕ + η - 1∖ ,	(17)
which can be solved using BCD on (w, η, Φ). Note that if Φ(∙) is known and fixed, then optimization
problem in Eq. 17 becomes convex in η and w. We refer to this bound as η -DV-convex.
What Do Neural Estimators of KL or MI Learn? We conclude with the following observations:
1.	Ratio estimation via Feature Matching. KL divergence estimation with variational bounds boils
down to a ratio estimation using a form of MMD matching.
2.	Choice of Feature/Architecture. The choice of the feature space or the architecture of the neural
network introduces a bias in the ratio estimation; also observed in Tschannen et al. (2019).
3.	Ratio Normalization. η-DV bound introduces a normalization constraint on the ratio, that ensures
a better consistent estimate.
4.	Impact of Regularizers. The choice of the regularizer dictates the choice of the metric. For
instance, Ozair et al. (2019) used gradient penalties as a regularizer, and this corresponds to a ratio
matching in the Sobolev discrepancy sense. Please see Appendix A.1 for more details.
4 η-DV MUTUAL INFORMATION ESTIMATION
We now specify the KL estimators given in Section 3 for the MI estimation problem. Given a function
space H defined on X × Y,
I (X ； Y) ≥ IH (X ； Y ) = f∈up Epxyf(X,y)-log (EpxEpy ef (x,y)) ≥ IHiV(X； Y),	(18)
5
Under review as a conference paper at ICLR 2020
where IfH-div(X; Y) = supf∈H Epxyf(x,y) - EpxEpyef(x,y).
Equivalently, with the η-trick we have:
IDHV(X;Y)

inf e-ηEpxpyef(x,y) -Epxyf(x,y)+η-1.
f∈H,η
(19)
Now, given iid samples from marginals Xi 〜Px and yi 〜Py, i = 1 ...N, and samples from thejoint
(xi, yi)〜pxy, i = 1 ...N, we can estimate the MI as follows:
1N	1N
IH(X,Y) = DH(px,y,PxPy) = - inf e-η- Eef(Xi,yi) - - Ef(Xi,yj + η - 1, (20)
f ∈H ,η	N	N
i=1	i=1
when H is an RKHS, η can be seen as a Lagrangian ensuring that the ratio estimation r(X, y) of
pxy- is normalized when integrated on PxPy, i.e., η is a Lagrangian associated with the constraint
pxpy
X×Y r(X, y)Px(X)Py (y)dXdy = 1. In Table 1 we review other variational bounds for MI based on
f -div, DV, and η-DV bounds (a-MINE from Poole et al. (2019) is discussed next).
MI Estimator (bound)	τ	,	∙ ∙	∙	τ Loss to minimize L	Constraints
DV-MINE (DV)	log (n1 PLef(xi,yi)) - NP= f(Xi,yi)	f is a DNN
Unbiased DV-MINE	飞P⅛f"i) -寺PLf(Xi,%)	一	f is a DNN, m running avg. of NPi=Ief(xi,yi)
f-MINE (f -div)	N PL ef(xi,yi)- N PLf(Xi,yi) - 1	f is in RKHS (convex) or f is a DNN
a-MINE (DV)	n1 PL (⅞y1 + iog(a(yi))) - N PLf(Xi,yi) -1	a is a DNN, a > 0 f is a DNN
InfoNCE (-)	N PLI (IOg N P皂 ef(xi,yj) - f (Xi,yi))	f is a DNN
η-MINE (ours:n-DV)	e-ηN PL ef(xi,yi) - PN=If(XMyi) + η - 1	f is in RKHS or f is a DNN; η ∈ R
η-MINE-convex (ours:n-DV)	e-ηN Pi=ι ehw,φ(xi,yi)i - Pi=ι hw, Φ(Xi,%)i + η - 1	Φ(∙) is fixed w ∈ Rdim(Φ),η ∈ R
Table 1: Given iid samples from marginals Xi 〜Px and yi 〜Py and samples from thejoint (χi,yi)〜Pxy, we
list some MI estimators, corresponding variational bounds, associated losses, and constraints on the function
space. MI estimators include biased and unbiased DV-MINE from DV (Belghazi et al., 2018), f-MINE from
f -div (Nguyen et al., 2008; Nowozin et al., 2016), InfoNCE (van den Oord et al., 2018a), a-MINE from DV
(Poole et al., 2019) and η-MINE, η-MINE-convex from η-DV(ours).
Improved Donsker-Varadhan for Mutual Information and a-DV Bound Discussion. In the
following Lemma we show that for the particular case of MI estimation, the DV bound can be made
tighter:
Lemma 4. For any function g that maps X × Y to R+ we have:
KL(Px,y,PxPy) + Epx log(Epy g(X, y)) ≥ Epx,y log(g(X, y))
Therefore: I(X; Y) = supf ∈G (X ×Y) Epx,y f(X, y) - Epy log(Epxef(x,y)) where G (X × Y) is the
entire space of functions defined on X × Y. For any function space H map X × Y to R we have:
I(X； Y) ≥ IHV(X； Y) := sup Epχ,yf(x,y) - Epy log(Epχef3y)).
f∈H
Using Jensen’s inequality, we can easily derive the following hierarchy of lower bounds:
I(X; Y) ≥ IHV(X; Y) ≥ IHV(X; Y) ≥ IHiV(X; Y).	(21)
In Theorem 2, we apply the η-trick of Lemma 1 to IaH-DV(X; Y), where we replace each
log Epx ef (x,y) using the variational form of log which leads to η(y). The interchangeability
between minimization and expectation and going from scalar η for each y to a function η(.) needs
some care and we prove it in the Appendix B.1, appealing to the interchangeability principle (Lemma 1
in Dai et al. (2016)).
6
Under review as a conference paper at ICLR 2020
Theorem 2 (η-trick for Improved DV Bound). Let H be a space of bounded functions defined on
X × Y we have: I(X;Y) ≥ - inff∈H,η∈G(Y) Epx.pyef(x,y)-η(y) + Epy η(y) - 1 - Epx,y (f (x, y)),
where G (Y) is the entire space of functions defined on Y. Let Hη be a function space subset of
G (Y) we obtain:
I(X； Y) ≥ IHV(X; Y) = sup	Epx,y f (x,y) - Epx.py ef(x,y)-η(y) - Epyη(y) + 1. (22)
f∈H ,η(.)∈Hη
Hence, we recover the lower bound given in Poole et al. (2019) (given in Table 1), that was derived
through other variational principles. In the following, we make a few remarks about IaH-DV(X; Y):
1.	The IaH-DV(X; Y) bound, as given in Poole et al. (2019), can be also obtained by setting η(y) =
log(a(y)), a(y) > 0. While this seems equivalent, the restriction of a to R+ (η is in R) changes
the landscape of the optimization problem and the interchangeability principle does not apply in a
straightforward way.
2.	Poole et al. (2019) refers to a(y) as a “critic” (as in the reinforcement learning literature). As
shown in our analysis in Section 3, when we restrict H to an RKHS, η(y) plays a role of a
Lagrangian multiplier or a dual potential in a ratio estimation problem. In this case it is enforcing
multiple constraints X r(x, y)p(x)dx = 1 for each y, instead of imposing a single constraint
X Y r(x, y)p(x)p(y)dxdy = 1 as in the IηH-DV (X; Y) bound.
3.	No Free Lunch: while IaH-DV(X; Y) is tighter than our IηH-DV(X; Y) bound, it comes at the
price of estimating a function η(y) together with the witness function f. This triggers a higher
computational cost and larger estimation errors.
4.	We show in Appendix C how IaH-DV(X; Y) can be extended to conditional MI estimation.
5.	Generality: η-DV bound is applicable for general probability distributions P and Q: KL(P, Q) ≥
DηH-DV(P,Q), whileIaH-DV(X;Y) only works forP = pxy andQ = pxpy.
Finally, in Algorithm 1 we outline the steps of η-MINE for MI estimation.
Algorithm 1 η-MINE (Stochastic BCD )
Inputs: X, Y dataset X ∈ RN×dχ,Y ∈ RN×dy, such that (Xi = Xm = Yi,.)〜Pxy
Hyperparameters: αη , αθ (learning rates), nc (number of critic updates)
Initialize η , θ parameter of the neural network fθ
for i = 1 . . . Maxiter do
for j = 1 . . . nc do
Fetch a minibatch of size N (xi, yi) 〜 pxy
Fetch a minibatch of size N (xi, yi)〜Pxpy {yi obtained by permuting rows of Y}
Evaluate L(fθ,η) = e-ηN PN=I efθ(XSyi)- N PN=I fθ(χi,yi) + η — 1
Stochastic Gradient step on θ:
θ 一 θ - α dL(f ,η) {We use ADAM}
end for
Update η:
η 一 η 5吗展
end for
OUtPUt： fθ, η, IHDV(X, Y) = N PlN=I fθ(Xi, yi) - e-ηN PN=I efθ(XSyi)- η + 1
5	Experiments
In this Section, we conduct a set of experiments using synthetic and real data on a number of
applications to compare the proposed η-MINE MI estimator to the existing baselines.
MI estimation. We compared different MI estimators on three synthetically generated Gaussian
datasets [5K training and 1K testing samples]. Each evaluated MI estimator was run 10 times and
its average performance (together with the standard deviation) is shown in Fig. 3.a. As can be seen,
7
Under review as a conference paper at ICLR 2020
0.5
NqD ∙puo□u∩ HN+ N<D ∙PUOD Hzτ≡≈+ NqD ∙PUOD
0.0
Figure 3: Performance of different MI estimators on synthetic GauSSian datasets. Top: MI estimation. Data WaS
sampled from 2-, 10- and 20-dimensional Gaussian distributions with randomly generated means and covariance
matrices. As we increase the data complexity, the difference between estimators decreases, although we observed
that η-MINE (or its convex extension) on average performed better than the baseline methods, converging to the
true MI [red line] faster. Bottom: MIfor GAN regularization. Top row: unconditional GAN baseline fails at
capturing all 25 modes in the Gaussian mixture. Middle row: MI-regularized conditional GAN using DV-MINE
(Belghazi et al., 2018) converges after 140K steps of the generator. We found this estimator to be sensitive to the
hyper-parameters and unstable during training. Bottom row: MI-regularized conditional GAN using η-MINE;
the model converges in 50K steps of the generator.
MI estimation in high dimensions is a challenging task, where the estimation error for all methods
increases as the data dimensionality grows [red line shows true value for MI]. Nevertheless, the
proposed η-MINE is able to achieve on average more accurate results compared to the existing
baselines. We also see that the convex formulation of η-MINE has overall a better performance and
fast convergence rate. This estimator has a linear witness function that is defined as f (∙) =〈w, Φ(∙)i
using pre-trained fixed feature map Φ(∙) from the regular η-MINE. In the experiments that follow we
compare the proposed estimator η-MINE to DV-MINE, as a main baseline approach.
MI-regularized GAN. We investigate GAN training improvements with MI, and especially dimin-
ishing mode collapse, as addressed in Belghazi et al. (2018) in Section 5.1. Belghazi et al. (2018)
uses a 25-Gaussian dataset to show improvements on GAN Clustering by using MI objective for
regularization. As in InfoGAN (Chen et al., 2016), the conditional generator G is supplied with
random codes c along with noise z, we maximize the mutual information between I(G(z, c), c) using
η-MINE estimators. In Fig. 3.b, we repeat this task and establish that η-MINE can recover all the
modes within a fewer steps than DV-MINE (50K vs. 140K) and with a stable training.
Self-supervised: Deep InfoMax (Hjelm et al., 2018). In unsupervised or self-supervised learning,
the objective is to build a model without relying on labeled data but using an auxiliary task to learn
informative features that can be useful for various downstream tasks. In this experiment we evaluate
the effectiveness of the proposed η-MINE estimator for unsupervised feature learning using the
recently proposed Deep InfoMax method from Hjelm et al. (2018). For feature representation we
used an encoder similar to DCGAN (Radford et al., 2015), shown in Fig. 4.a and evaluated results on
CIFAR10 and STL10 datasets (STL10 images were scaled down to match CIFAR10 resolution).
8
Under review as a conference paper at ICLR 2020
Figure 4: (a) Encoder architecture and classification accuracy (topi) on CIFAR-10 dataset for different pre-
trained encoders. Each number represents test accuracy of the system trained by maximizing MI between
features from layers pointed by the corresponding arrows. Interestingly, the highest accuracy was obtained by
pre-training encoder composed ofjust the first two convolutional layers (see (b) and (c) for details of this process).
(b) Model pre-training by maximizing MI between features from different layers [additionally transformed by
a neural net, aimed at constructing witness function f (•)]. (c) After pre-training, We fix encoder and attach a
trainable linear layer to perform traditional supervised training on the same or different datasets.
The encoder is trained by maximizing MI
I(χ0, y0) between features from one of the shal-
low layers l (χ0 = El(X)) and a deeper layer
k (y0 = Ek(y)). We examined different layer
combinations and found that the encoder com-
posed of only the first two convolutional layers
give the best performance on the downstream
tasks. As shown in Fig. 4.b the encoder features
are passed through additional trainable neural
network layers, whose job is to build a classi-
fier f (x0, y0), discriminating cases when x0 are
y0 are coming from the same image and cases
when x0 are y0 are unrelated. Finally, we at-
tach a linear layer to the pre-trained and now
fixed encoder (see Fig. 4.c) to perform super-
vised training. In Tab. 2 we present the results
Table 2: Classification accuracy (topi) results on CI-
FAR10 (C10) and STL10 (S10) for unsupervised pre-
training task with DV-MINE and η-MINE using encoder
in Fig. 4.b. For reference we also list results for su-
pervised (CE) training using full encoder in Fig. 4.a.
η-MINE-based pre-training achieves overall better re-
sults when evaluated on same or different data, even
outperforming the supervised model on S10.
Train	Test					
	C10			S10		
	DV	η	Sup.	DV	η	Sup.
C10	77.5	74.8	84.2	551	56.4	一
S10	67.5	68.3	-	61.8	63.3	61.3
for two MI estimators: η-MINE and DV-MINE, whose loss functions are listed in Tab. 1. As can be
seen, η-MINE-based pre-training performs competitively with DV-MINE, achieving overall better
results on both datasets, showing practical benefits of the proposed approach.
Self-Supervised: Jigsaws with MI. The self-supervision Jigsaw pretext task (Noroozi & Favaro,
2016; Kolesnikov et al., 2019) aims at solving an image jigsaw puzzle by predicting the scrambling
permutation π. From image X with x = {x1 . . . x9} 3×3 jigsaw patches, and permutation y = πi :
[1, 9] → [1, 9], scrambled patches xπi = {xπi(1) . . . xπi(9)} generate the puzzle. Each patch is passed
through an encoder E, which needs to learn meaningful representations such that a classifier CJ can
solve the puzzle by predicting the scrambling order πi (Noroozi & Favaro, 2016) (see Fig. 5.a). While
this standard formulation relies on a CE-based classification of the permutation, we propose here to
use MI Jigsaw, where we train the encoder E to maximize I(E(xπi); πi) by using MI estimators DV-
and η-MINE, as seen in Fig. 5.b. A patch preprocessing similar to Kolesnikov et al. (2019) avoids
shortcuts based on color aberration, patch edge matching, etc. (Noroozi & Favaro, 2016); for details
of our implementation, see Appendix D. All our models are built on a 10% subset of ImageNet (128K
train., 5K val., 1K classes) as proposed by Kolesnikov et al. (2019). This is a larger set than Tiny
ImageNet (200 classes) used in many publications. E is a ResNet50 for all our experiments. In our
ImageNet target classification task, E from CE and MI Jigsaw trainings are frozen (at 200 epochs)
and followed by linear classifier C (Fig. 5.c); an adequate setup for comparing encoders as argued by
Kolesnikov et al. (2019).
All models are compared on target task in Tab. 3. Best accuracy numbers are reported for Cs trained
for exactly 200 epochs. For all results, E is trained from Jigsaw task (CE or MI) and frozen, with
only C trained as in Kolesnikov et al. (2019). DV- and η-MINE share the same f architecture.
η-MINE gives better accuracy performance compared to DV-MINE. CE model trained from a Jigsaw-
9
Under review as a conference paper at ICLR 2020
supervised encoder E provides an upper-bound for supervised performance for E from the Jigsaw
task. Despite CE being better than η-MINE and DV-MINE, η-MINE does a respectable job at learning
a useful representation space for the target task, better than DV-MINE. More details about the Jigsaw
experiments setup can be found in Appendix D.
Table 3: ImageNet (10% subset) classification ac-
curacy results (in %). DV-MINE and η-MINE are
using fixed Encoder from MI training. CE is for
using a CE Jigsaw Encoder. Numbers reported
are average and standard deviations over 8 models
from different initialization seeds.
	DV-MINE	η-MINE	CE
top1	8.5 ± 1.3	11.0 ± 1.1	12.9 ± 0.3
top5	20.0 ± 2.5	24.1 ± 2.2	28.1 ± 0.6
top10	27.8 ± 3.1	32.7 ± 2.2	37.7 ± 0.6
X -* ∏ IxnEnCoder Fxn) Classfier ∣→ ∏ — CE Loss (a)
y—ɪ-----------------------------------------------------ɪ
Jigsaw permutation label
X-* ∏ IxnEnCoder^E(Xn) f (E(Xn),y)---------► MI Loss (b)
y-Γ-______________________⅛
Jigsaw permutation label
X---------，Encoder|~E(X)>|~CIassfier ∣→ C - CE Loss (C)
Im ImageNet ClaSs lab el	[
Figure 5: (a) Jigsaw CE training. (b) Jigsaw MI train-
ing. (c) ImageNet Classification CE training.
6	Conclusion
In this paper we introduced a new lower bound on the KL divergence and showed how it can be used
to improve the estimation of mutual information using neural networks. Theoretically we proved that
the dual of our η-DV formulation reduces to a constrained likelihood ratio estimation. In practice, the
stability of η-MINE is due to its unbiased gradient. We tested our estimator η-MINE on synthetic data
and applied it on various real-world tasks where MI can be used as a regularizer, or as an objective in
self-supervised learning. We used η-MINE in unsupervised learning of representations through MI
maximization and by solving Jigsaw puzzles. We leave for a future work the analysis and various
possible applications of conditional mutual information neural estimators.
References
Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information
bottleneck, 2016.
Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, and Kevin Murphy.
Fixing a broken elbo, 2017.
Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Convex multi-task feature learning.
Mach. Learn., 2008.
Francis Bach, Rodolphe Jenatton, and Julien Mairal. Optimization with Sparsity-Inducing Penalties
(Foundations and Trends(R) in Machine Learning). Now Publishers Inc., Hanover, MA, USA,
2011. ISBN 160198510X, 9781601985101.
David Barber and Felix V. Agakov. The im algorithm: A variational approach to information
maximization. In NIPS, 2003.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and R Devon Hjelm. Mine: Mutual information neural estimation, 2018.
Anthony J. Bell and Terrence J. Sejnowski. An information-maximization approach to blind separation
and blind deconvolution. Neural Comput., 1995.
David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians.
Journal of the American Statistical Association, abs/1601.00670, 2017.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
NIPS, 2016.
10
Under review as a conference paper at ICLR 2020
Bo Dai, Niao He, Yunpeng Pan, Byron Boots, and Le Song. Learning from conditional distributions
via dual kernel embeddings. CoRR, abs/1607.04579, 2016.
Carl Doersch, Abhinav Gupta, and Alexei A. Efros. Unsupervised visual representation learning by
context prediction. 2015 IEEE International Conference on Computer Vision (ICCV), Dec 2015.
doi: 10.1109/iccv.2015.167. URL http://dx.doi.org/10.1109/ICCV.2015.167.
M. D. Donsker and Srinivasa Varadhan. Asymptotic evaluation of certain markov process expectations
for large time, i. Communications on Pure and Applied Mathematics, 1976.
I. Ekeland and T. Turnbull. Infinite-dimensional Optimization and Convexity. The University of
Chicago Press, 1983.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard SchOlkopf, and Alexander Smola.
A kernel two-sample test. JMLR, 2012.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved
training of wasserstein gans. arXiv:1704.00028, 2017.
Zaid Harchaoui, Francis R Bach, and Eric Moulines. Testing for homogeneity with kernel fisher
discriminant analysis. In NIPS, 2008.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Adam Trischler, and
Yoshua Bengio. Learning deep representations by mutual information estimation and maximization.
In ICLR, 2018.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=Bklr3j0cKX.
Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning
discrete representations via information maximizing self-augmented training. In Proceedings of
the 34th International Conference on Machine Learning, 2017.
Artemy Kolchinsky, Brendan D Tracey, and David H Wolpert. Nonlinear information bottleneck.
arXiv preprint arXiv:1705.02436, 2017.
Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representa-
tion learning, 2019.
Alexander Kraskov, Harald Stoi gbauer, and Peter Grassberger. Estimating mutual information.
Physical review. E, Statistical, nonlinear, and soft matter physics, 2004.
Andreas Krause, Pietro Perona, and Ryan G. Gomes. Discriminative clustering by regularized
information maximization. In Advances in Neural Information Processing Systems 23. 2010.
Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models.
arXiv:1610.03483, 2016.
Youssef Mroueh and Tom Sercu. Fisher gan. arXiv:1705.09675 NIPS, 2017.
Youssef Mroueh, Chun-Liang Li, Tom Sercu, Anant Raj, and Yu Cheng. Sobolev gan. ICLR, 2018.
Youssef Mroueh, Tom Sercu, and Anant Raj. Sobolev descent. In AISTATS, 2019.
XuanLong Nguyen, Martin J Wainwright, and Michael I. Jordan. Estimating divergence functionals
and the likelihood ratio by penalized convex risk minimization. In NIPS. 2008.
Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw
puzzles. Lecture Notes in Computer Science, pp. 69-84, 2016.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In NIPS, 2016.
11
Under review as a conference paper at ICLR 2020
Sherjil Ozair, Corey Lynch, Yoshua Bengio, Aaron van den Oord, Sergey Levine, and Pierre Sermanet.
Wasserstein dependency measure for representation learning. CoRR, 2019.
Ben Poole, Alexander A Alemi, Jascha Sohl-Dickstein, and Anelia Angelova. Improved generator
objectives for gans. arXiv preprint arXiv:1612.02780, 2016.
Ben Poole, Sherjil Ozair, Aaron van den Oord, Alexander A. Alemi, and George Tucker. On
variational bounds of mutual information, 2019.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv:1511.06434, 2015.
Bharath K. Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Scholkopf, and Gert R. G.
Lanckriet. On integral probability metrics, φ-divergences and binary classification. 2009.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. pp.
368-377,1999.
Michael Tschannen, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual
information maximization for representation learning, 2019.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. ArXiv, 2018a.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding, 2018b.
12
Under review as a conference paper at ICLR 2020
Supplementary Material for Improved Mutual Information Estimation
The supplementary material is organized as follows:
•	In Appendix A we layout proofs of Lemmas 1, 2 and 3. Those lemmas provide the η-DV
formulation. We also give in this Appendix the proof of the main Theorem 1. This theorem
gives the dual formulation of this variational bound in RKHS with Tikhonov Regularization
and sheds the light on the ratio estimation interpretation of η-DV.
•	We give in Appendix A.1 the impact of other regularizers on the estimation ofKL divergence
in RKHS.
•	In Appendix B, we show that further improved bounds can be obtained for the particular
case of mutual information leading to an improved Donsker-Varadhan. We show that by
using the interchangeability principle, we can obtain an equivalent functional η-DV bound
that is related to a-MINE introduced in Poole et al. (2016).
•	In Appendix C, we show how those improved variational bounds can be extended to
conditional mutual information.
•	In Appendix D, we give more details on the experimental setup of self-supervised learning
with the jigsaw task.
A Technical Proofs
Proof of Lemma:1. Let g(η) = e-ηx + η - 1. We have g0 (η) = -e-η x + 1 and g00 (η) = e-ηx > 0.
Hence g is convex and admits a unique minimum. First order optimality gives us: g0(η*) =
—e-η*X +1 = 0, hence e-η*X = 1 and n =log(x). Finally, We get
g(η*) = e- log(x) x + log(x) — 1 = elog(1 )x + log(x) — 1 = 1 + log(x) — 1 = log(x).
□
Proof of Lemma 2. By Donsker-Varadhan representation We have:
DH (P,Q) = SUp Ex〜Pf(X)- log (Ex〜q，
f∈H
-fiflog (Ex~Q
—f吸{inf e-ηEx〜Qef(X) + η — 1}- Ex〜Pf(X)

f ∈J⅛∈R{e-ηEx 〜Qef(X)- Ex 〜Pf(X) + η -1}∙
□
ProofofLemma 3. L is a sum of a linear function in w, η and Ω(w), that are both convex. We need
to prove that fb(w, η) = e-ηehw,bi is jointly convex, since L can be Written as a sum of fb and other
convex functions:
Vw fb(w, η) = e-ηehw,bib, ▽： fb(w, η) = e-ηehw,bib 0 b PSD (Positive Semi-Definite)
∂∂Wf∂η = —e-η ehw,bib
∂2fb
∂η2
e-ηehw,bi
The Hessian of fb(w, η), Hfb, has the folloWing form:
Hfb(w, η) =	'	∂2l	∂2L - ∂w∂η	=	'e-ηehw,bib 0 b	—e-ηehw,bib -
	∂w%∂w				
	-∂2L - _ ∂η∂w	∂2L ∂η2	_		—e-η ehw,bib>	e-η ehw,bi
Let us prove that for all (w, η): (w0, η0)>Hfb(w, η)(w0, η0) ≥ 0, ∀(w0, η0). We have:
(w0, η0)>Hfb(w, η)(w0, η0) = e-ηehw,bi(hw0, bi)2 — 2η0e-ηehw,bi hw0, bi + η02e-ηehw,bi
=e-ηehw,bi ((hw0, bi)2 — 2η0 hw0, b〉+ η02)
= e-ηehw,bi (hw0, bi — η0)2 ≥ 0∙
13
Under review as a conference paper at ICLR 2020
Hence, fb is jointly convex in η and w, and so is L. Now adding the perturbation εe-η to the loss, we
get
w0, η0)>Hfb(w, η)(w0, η0) = e-η ehw,bi (hw0, bi - η0)2 + εe-η > 0,
for finite η and hence We have a strict convexity.	□
Proof of Theorem 1. Let’s consider:
f(x) = hw, Φ(x)i, and 夕(t) = et.
Recall that the FenChel conjugate of f is f *(p) = SuPx hp, Xi - f (x). It follows that 夕*(t)=
t log(t) - t ift > 0, 0 for t = 0, and ∞ otherWise.
mine-ηEy〜Qq((w, Φ(y)i) - Ex〜P (w, Φ(x)i + η - 1 + Ω(w)
w,η
min e-η
w,η
min e-η
w,η,z
/ P((w,Φ(y)i)q(y)dy
X
'/ P(z(y))q(y)dy
X
Φ(x)p(x)dx + + η — 1 + Ω(w)
Φ(x)p(x)dx〉+ η — 1 + Ω(w) s.t. z(y) =(w, Φ(y)i
Introducing Lagrangian’s α we have
min max e-η
w,η,z α
max min e-η
α w,η,z
I 2(Z(U))q(y)dy
X
I 2(Z(J))q(y)dy
X
Φ(x)p(x)dx) + η - 1 + Ω(w) - / α(y)((w, Φ(y)i - z(y))dy
Φ(x)p(x)dx) + η - 1 + Ω(w) - / α(y)((w, Φ(y)i - z(y))dy
using strong duality (convex function and lower semi-continuous) to swap min, max (Ekeland & Turnbull, 1983)
=max min - (max - (/ W (z(y)) q(y)e-η + α(y)z(y))dy))
max
w

Φ(x)p(x)dx + / α(y)Φ(y)dy) — Ω(w) — η + 1
max min -
αη
max min -
α<0 η
max min -
α>0 η
- min max
α>0 η
X
ZX
ZX
ZX
q(y)e-η max -φ(z)——,a(y) Z Z) - Ω* ( Φ Φ(x)p(x)dx + / α(y)Φ(y)dy ) + η - 1
z	q(y)e-η	X	X
q(y)e-ηΨf(--α⅛^)[ — Ω* ( [ Φ(x)p(x)dx + / α(y)Φ(y)dy∖ + η - 1
q(y)e-η	X	X
q(y)e-η"
q(y)e-η"
α(y)
q(y)e-η
α(y)
q(y)e-η
))—Ω* (/ Φ(x)p(x)dx — / α(y)Φ(y)dy) + η — 1
))+Ω* (/ Φ(x)p(x)dx - X α(y)Φ
-η+1
Hence, we have for α > 0, and finite η:
JXi/ (谷)珈
Define
X	q(y)e-η	( a(y) log( a(y)) <q(y)e-η g( q(y)e-η )	-α(y)、 q(y)e-η√	dy
X	q(y)e-η	(α(y log( α⑹) <q(y)e-η g(q(y)e-η)	-α(y) ' q(y)e-η√	dy
[α(y) log X	(H)- α(y)]dy		
X	α(y) log	— 1)/	α(y))dy.	
α(y) r(y) =	>0		
14
Under review as a conference paper at ICLR 2020
then we have
(A): [ q(y)e-ηψ* ( /。，-“ Idy = [ r(y) log (r(y)) q(y)dy + (n — 1) [ r(y)q(y)dy,
X	q(y)e-η	X	X
and
(B) : Ω	Φ(x)p(x)dx — / α(y)Φ(y)dy) = Ω* (/ Φ(x)p(x)dx — / r(y)q(y)Φ(y)dy).
Hence, by replacing (A) and (B) by their expressions, we get:
min L (w, η)
w,η
—minmax ( / q(y)e-η4*( @^1 ) ∣ +Ω* ∣ / Φ(x)p(x)dx — / α(y)Φ
α>0 η	X	q(y)e-η	X	X
-η+1
= — min max	r(y) log (r(y)) q(y)dy + (η
r(.)>0 η X
+ Ω* (/ Φ(x)p(x)dx — / r(y)q(y)Φ
r(y)q(y)dy - 1
Now consider again the η-DV Bound:
- minL (w, η)
w,η
= min max	r(y) log (r(y)) q(y)dy + (η — 1)(	r(y)q(y)dy — 1)
r>0 η X	X
+ Ω* (/ Φ(x)p(x)dx — / r(y)q(y)Φ
= m>n/ r(y)log(r(y)) q(y)dy + Ω* (/ Φ(x)p(x)dx — / r(y)q(y)Φ
s.t.	r(y)q(y)dy = 1.
X
We See that n-DV amounts to a likelihood ratio r estimation, where We match with the dual norm Ω*
the difference of kernel mean embedding of p, and rq. And η plays a role of a Lagrange multiplier that
enforces the constraint that rq is a normalized distribution. In particular, consider Ω(u) = 2∣∣u∣H, so
that Ω*(v) = 2λ ∣∣v∣∣2, and hence we have the following equivalent problem of the regularized n-DV
bound:
mr>in0 X r(y) log (r(y)) q(y)dy
|
Φ(x)p(x)dx -	r(y)q(y)Φ(y)dy
X
2
2
MMD matching of rq and p
s,t.	r(y)q(y)dy = 1.
X
|------------{-------------}
Normalization Constraint
Another way of coming informally to the regularized η-DV objective is to start from expression of
KL using r:
mr>in0 X r(y) log(r(y))q(y)dx
MMDΦ(rq, p) < ε
r(x)q (x) = 1.
X
We see that r in the dual function in estimating the ratio, η is the Lagrangian of the inequality
constraint, and the MMD is added to the cost as a penality.
□
15
Under review as a conference paper at ICLR 2020
A. 1 Impact of regularizers on KL Estimation
As discussed before, the choice of the regularize] Ω translates to a particular type of metric used in
estimating the likelihood ratio: Ω(.) = k .∣∣2 translates in a ratio estimation in the MMD sense. Here,
we consider other choices of regularization used in Fisher discriminant analysis (Harchaoui et al.,
2008; Mroueh & Sercu, 2017), specifically:
η-DV-Fisher : Imin力(于，n + 2Ex〜¥产3，	(23)
or gradient penalty regularizer (Gulrajani et al., 2017; Mroueh et al., 2018; 2019):
η-DV-Sobolev : minL(f,η) + λEx〜p+q ∣∣Vxf(χ)k2.	(24)
f,η	2	2
It is easy to see that when We restrict f to H, η-DV-Fisher has the same objective as in Eq. 11
with Ω(w) = λ hw, (C(P, Q))wi, where C (P, Q) = E p+q Φ(x) 0 Φ(x) is a covariance matrix.
Similarly, n—DV-Sobolev corresponds to Eq. 11 with Ω(w) = λ(w, (D(P, Q))w〉，where D(P, Q)=
E(p+q)/2 pd=ι ∂jΦ(x) 0 ∂jΦ(x) is a Gramian of derivatives. The convex conjugate Ω*(w) is
therefore 4〈w，(C*(P, Q))w〉and 4〈w，(D1(P, Q))w〉，for n—DV-Fisher and n—DV-Sobolev
respectively (where f denotes the pseudo-inverse).
Let δP,Q = EPΦ(x) - EQΦ(y). Recall from Harchaoui et al. (2008) that the Fisher discrepancy is
Fφ(P, Q) = <δp,Q, C(P, Q),δp,Q), and from Mroueh et al. (2018; 2019) the Sobolev discrepancy is
Sφ(P, Q) = <δp,Q, D(P, Q),δp,Q). Now replacing Ω* by its expression in Theorem 1 we see that the
choice of the regularizer dictates the discrepancy used for the ratio estimation:
n—DV-Fisher : rɪiin JX r(y)log (r(y)) q(y)dy + 21λFφ(rq,p), subject to: JX r(y)q(y)dy = 1,
n—DV-Sobolev : Imin / r(y) log (r(y)) q(y)dy + 2λSφ(rq,p), subject to: / r(y)q(y)dy = 1.
B	FURTHER BOUNDING MUTUAL INFORMATION: a-MINE FORMULATION
from Interchangeability Principle
We start by the classical result of Donsker and Varadhan:
Lemma 5 (Donsker-Varadhan). For any Distribution P and any distribution Q and any non-negative
function g for which Ex 〜P log g(x) is finite, we have:
KL(P,Q) +log(Ex〜Qg(X)) ≥ Ex〜plog(g(x)).
This gives us a representation ofKL divergence:
KL(P, Q) = supEx〜plog(g(x)) - log(Ex〜Qg(X)) = SuPEx〜Pf(X)- log(Ex〜Qef(x)).
g>0	f
The following Lemma shows that a finer application of the Donsker-Varadhan Lemma gives an
improved variational representation of MI :
Lemma 6. Let g > 0
KL(px,y, pxpy) + Epx log(Epy g(X, y)) ≥ Epx,y log(g(X, y)).
Hence, we get
I(X;Y) = sup Epx,y log(g(X, y)) -Epx log(Epy g(X, y))
g>0
= sup	Epx,y f (X, y) -Epx log(Epy ef (x,y))
f ∈G (X ×Y)
= sup	Epx,y f (X, y) -Epy log(Epxef (x,y))
f ∈G (X ×Y)
where G (X × Y) is the entire space of functions defined on X × Y.
16
Under review as a conference paper at ICLR 2020
Proof.
p
KL(Pχ,y,PxPy) + EpxlOg(Epyg(x, y)) = Epxy log(--) + EpxlOg(Epyg(x, y))
pxpy
=Epx(EpyIx log( pyx ) + log(Epy g(X, y^)
Py
px KL(Py|x, Py) + lOg(Epy g(x, y)) .
Now applying Donsker-Varadhan Lemma, we have:
KL(Py|x, Py) + lOg(Epy g(x, y)) ≥ Epy|x lOg(g(x, y)).
We conclude therefore that
KL(Pχ,y,PxPy) + Epx log(Epyg(x, y)) ≥ Epx(EpyIx log(g(x, y)))
= Epx,y lOg(g(x, y)).
□
B.1 η-TRICK AND INTERCHANGEABILITY PRINCIPLE FOR IMPROVED DV
We just showed that an improved Donsker-Varadhan representation of Mutual Information can be be
derived as follows:
I(X;Y)=	sup	Epx,yf(x, y) - Epy lOg(Epxef(x,y)).
f∈G(X×Y)
We show in this section that, by using interchangeability principle, we can replace the ”lOg” non
linearity preceding the expectation with an ”functional” η-trick.
We start first by stating the interchangeability principle between expectation and minimum operations:
Lemma 7 (Interchangeability Principle see Lemma 1 in (Dai et al., 2016)). Let ξ be a random
variable on Ξ and assume for any ξ ∈ Ξ, the function g(., ξ) : R → (-∞, +∞) is a proper and
lower semi-continuous convex function. Then
Eξ min g(u, ξ) = min Eξg(u(ξ), ξ).
u∈R	u(.)∈G (Ξ)
Applying the interchangeability principle and the variational representation of lOg we obtain the
following Theorem:
Theorem 3 (Theorem 2 restated: η-trick for improved DV Bound). Let H be a space of bounded
functions defined on X × Y, we have:
I(X; Y) ≥ - f∈∕n∈G(Y) Epx.pyef(x,y)-η(y) + Epyη(y) -1 - Epx,y (f(χ,y)),
where G (Y) is the entire space of functions defined on Y. Let Hη be a function space subset of
G (Y), we obtain:
I(X； Y) ≥ IHV(X； Y) = f∈∕U(P)∈老 Epx,yf (X,y) - Epx.pyef"-Ky)- Epyη(y) + 1.
Proof. We know from Lemma 6 that for any function space H
I(X; Y) ≥ - infEpylOg(Epxef(x,y))-Epx,y(f(x,y)).
f∈H
Applying the η-trick for each lOg in the expectation:
Epy lOg(Epxef(x,y)) =Epymη∈iRne-ηEpxef(x,y) +η- 1.
(25)
(26)
We assume here that the function space f is bounded. Before applying the interchangeability Lemma
7, let us verify that all assumptions hold for
17
Under review as a conference paper at ICLR 2020
g(η, y) = e-η Epx ef (x,y) + η - 1.
Convex. g(., y) is convex. Considering derivatives wrt to η: g0(η, y) = -e-ηEpx (ef(x,y)) + 1, and
g00 (η, y) = e-ηEpx (ef(x,y)) > 0.
Proper Function. g(η, .) is a proper function, since f is bounded (-M ≤ f(x, y) ≤ M):
g(0, y) = Epx ef (x,y) - 1 ≤ eM - 1 < ∞.
and
g(η, y) > -∞, ∀η,
since
g(η,y) ≥ g(η*,y) = iog(Ep°ef(x,y)) ≥ -M,∀η.
Lower semi-continuity. g(η, .) is lower semi-continuous since it is continuous (sum of continuous
functions).
Now back to Eq. 26, we are ready to apply the interchangeability principle given in Lemma 7:
Epylog(Epxef(x,y)) =Epymη∈iRne-ηEpxef(x,y) +η-1
=Epy min g(η, y)
η∈R
= η(.)m∈iGn(Y)Epyg(η(y),y)
=η(.)m∈iGn(Y)Epy e-η(y)Epxef(x,y) + η(y) -1 .
Using this equivalent form, we have:
fi∈nf Epy log(Epxef(x,y)) -Epx,yf(x,y)
inf min E
f∈H η(.)∈G (Y)
inf E
f∈H,η∈G(Y)
py
px .py
e-η(y)Epx ef(x,y) + η(y) - 1 -Epx,y(f(x,y)
ef(x,y)-η(y) + Epyη(y) - 1 - Epx,y(f(x, y).
It follows that
I(X; Y) ≥ -
inf
f∈H,η∈G(Y)
ef (x,y)-η(y)
.pye
+ Epy η(y) - 1 - Epx,y (f (x, y)
f∈Hs,uηp∈G(Y)Epx,yf(x,y)-Epx.pyef(x,y)-η(y)-Epyη(y)+1.
Consider now a function space Hη from Y → R we have:
I(X; Y) ≥	sup	Epx,yf(x,y)-Epx.pyef(x,y)-η(y)-Epyη(y)+1,
f∈H ,η∈Hη
The MI lower bound is looser than regular η-MINE, since we have two lower bounds passing from
all function spaces G(X X Y) on to H, then going from G(Y) to H.	□
No free lunch: tighter bound but we need to estimate a function η(x) which makes bigger estimation
errors. To get a good estimate of η(x), to obtain its optimal value log(Epy ef(x,y)), we need for each
x many samples from py (which means many epochs of cycling through the same data in random
order). The η-trick with only a scalar variable is better computationally, since it enjoys convexity and
has better optimization properties. But as it is clear from this discussion, η function of a modality is
good for conditional MI estimation which we will discuss next.
18
Under review as a conference paper at ICLR 2020
C	Conditional Mutual Information
To illustrate the power of improved Donsker-Varadhan and its variational representation with the η-
trick and interchangeability principle, we show how to extend this to Conditional Mutual Information
estimation.
We have a (X, Y ) pair of random variables that may depend on another variable Z. This conditional
dependence is of high importance in all applications of joint embedding or in the information
bottleneck principle, etc.
I(X; Y |Z) = KL(pxyz, px|z py|z p(z)),
meaning that conditioned on Z, how much X and Y are dependent.
Therefore we have:
I (X ； Y |Z )= Ez~Pz KL(Px,y∣z ∣∣Px∣zPy∖z )∙
Now applying Donsker-Varadhan Lemma, we have for all z and for any function f
KL(Px,y∖z ||Px|z Py∖z ) + log EpxIz Py∣z f (x,y,z) ≥ EPχ,y∣z log(f (x,y,z)).
Now marginalizing on Z , we get
Ez~pz KL(Px,y∖z ||px|z py∖z ) + Ez~pz log EpxIz py∣zf (x,y,z) ≥ Eζ~pz Epx,y∣z log(f (χ,y,z))∙
Hence,
I(X; Y|Z) ≥ Ez~pzEpx,y∣z log(f(x,y, z)) - Ez~pz logEpxIzpy∣zf (x,y,ζ),
and
I(X； Y|Z) =	Sup Ez~pzEpxy∣z log(f(χ,y,z)) - Ez~pz logEpx∣zpy∣zf (χ,y,z).	(27)
f∈F,f>0	,
Now using the η-trick for each z , followed by the interchangeability principle, we have the final
variational form:
I(X; Y|Z) = sup Ez~pzEpx,y∣zf (χ,y,z) - Ez~pz (e-η(Z)Epx∣zpy∣z…⑶ + η(z) -1).
f∈F,η
(28)
D	Jigsaw self-supervision and Mutual Information
Jigsaw Task Preprocessing For Jigsaw self-supervision task, the encoder needs to learn features
from images randomly cropped and tiled into 3×3 jigsaw puzzles. In order to avoid shortcuts based
on matching edges of patches or picking up on color aberration as discussed in Noroozi & Favaro
(2016), images are resized to 292×292, randomly cropped into 255×255 and jigsawed into 3×3
or 85×85 per patch wherein a 64×64 patch is randomly cropped, avoiding potentially easy edge
matching. 255 X 255 crops are randomly assigned to gray scales with a probability of % prior to patch
extraction. Each patch RGB channel is normalized to zero mean, unit variance. At test time, the
255×255 crops are obtained from images center crops.
ImageNet 10% For the Jigsaw experiments, we decided to subsample ImageNet to 10% within
each class for both training and validation sets (128K training and 5K validation images). The
reason is that running on all of ImageNet 1.3 Million images was not practical for reasonable training
turnaround on our infrastructure given what we had to train one set of encoder models from the
pretext task and one set of classifier models for the target task. Even though we only train classifiers
for the target task (as explained later in this section), we still need to forward input features through
the Encoder which can still be computationally costly despite only backpropagating through the
classifier. This 10% subset of ImageNet was proposed by (Kolesnikov et al., 2019) and can be used
for faster training turnaround while still being larger than the Tiny ImageNet dataset (200 classes,
500 training, 50 vaildation, 50 test images per class) often used in publications.
19
Under review as a conference paper at ICLR 2020
Jigsaw CE Training For self-supervision Jigsaw pretext task (Noroozi & Favaro, 2016; Kolesnikov
et al., 2019) we aim at solving an image jigsaw puzzle by predicting the scrambling permutation π.
Given an image X with x= {x1, . . . , x9} 3×3 jigsaw patches, and a permutation y = πi : [1, 9] →
[1, 9], scrambled patches xπi = {xπi(1), . . . , xπi(9)} generates the puzzle. The Encoder E is based on
a ResNet50 and outputs 2048 features for each patch. The classifier C (or ”descrambler” in this case)
is a simple Neural Network with a linear projection upfront from 2048 to 512 for each patch features
that are then stacked (1×(9*512)) and linearly projected to a 512 dimensional subspace, followed
by batch normalization, ReLU, dropout, and another linear projection from 512 to 100 (number of
our subset of permutations from Noroozi & Favaro (2016)). E and C area trained with minibatch
made from 16 permutations of 10 images (160 effective minibatch size). The 10 images and 16
permutations are randomized for each minibatches. For both E and C updates, we use ADAM with
(β1 = 0.9, β2 = 0.999) with a learning rate of 10-4 that stays fixed for all epochs and no weight
decay. We let the model build for a total of 200 epochs with a random seed fixed (for reproducibility).
We perform 4 independent builds with 4 different random seeds. We let the model build and just
survey the Jigsaw solving accuracy assess the health of training only, not to select and encoder model
as performance in pretext task is not a good predictor of performance in the target task (as observed
in Kolesnikov et al. (2019). We run for 200 epochs so to be able to turnaround results fast on our
infrastructure, not for the goal of early stopping as no overfitting was observed.
Jigsaw MI training For MI Jigsaw training, we train E so to maximize MI such that
maxI E(xπ(1)), . . . , (xπ(9)); π , which means minimizing the following losses derived from Eq. 8:
E
LDV(f) = logEχ,y~PχPy [exp(f(x,y))] - Eχ,y~p°y [f(x,y)]	(DV-MINE algo.)
Ln-DV(f, η) = e-ηEχ,y~PχPy [exp (f (x, y))] — Eχ,y~P°y [f (x, y)] + η — 1,	(η-MINE algo.)
where X are scrambled images patches, and y are scrambling permutations. (x, y)〜Pxpy are sampled
from marginals px and py (X was not scrambled by permutation y) while (x, y)〜Pxy are sampled
from the joint distribution (y permutation scrambled x). For MI training, we need to define the witness
function f(x, y). x is the set of 9 patches feature vectors from E and y is a one-hot embedding
of our 100 permutations from Noroozi & Favaro (2016). x and y are first linearly projected in a
common dimensionality of 512. Then x goes through a shallow network made of 2 sequential stages
each composed of CNN, batch normalization, ReLU, and max pooling. After these 2 stages, we
have about a combined 9216 features that we project to 512 dimensional space (same as dimension
used for the projection of y). f(x, y) is then just the dot product of 512 feature vectors from x and
y once normalized, i.e. a cosine distance that can give scores between -1 and 1. We train using
minibatches of 10 images and 16 permutations (160 effective minibatch size) as sampling from the
joint distribution pxy, allowing for 16 unused permutations for sampling from marginal distribution
pxpy . Therefore expectation terms in the losses above for joint and marginal distribution samples
are using the same number of samples (160) for each minibatch. Again, we use ADAM for E and
f(x, y) with (β1 = 0.9, β2 = 0.999) with a learning rate of 10-4 that stays fixed for all epochs and
no weight decay. For η-MINE, we use ADAM for η with the same parameters as for E and f(x, y)
but with a learning rate of 10-2 to allow for faster updates of η. η is initialized to 0 for η-MINE. We
run models for 200 epochs of training. A fixed random seed is given to ensure reproducibility. We use
4 seeds to ensure variability of our model builds (same seeds as used for Jigsaw CE). Once again, we
observe the training and validation loss as indicators of a training’s health and observe no overfitting.
ImageNet 10% Target Task For our target classification task, Encoders E from 200 epochs of
Jigsaw Task training are frozen and used to train a classifier C for ImageNet 10% with 1000 output
classes. C is composed only of a linear projection from 2048 to 1000 classes. For each input image,
we resize the images to 232×232 and take a 192×192 center crop before turning it into a 3×3 cells
grid where each individual patch (now 64×64) is forwarded through E to get a 2048-dimensional
vector for each of the 9 patches. Then, we take the mean of these 9 vectors and pass this 2048-dim
mean vector to the classifier C as done in Kolesnikov et al. (2019). Only the classifier C is trained
using ADAM with (β1 = 0.9, β2 = 0.999) with a learning rate of 10-4 that stays fixed for all
epochs and no weight decay. We let C build for a total of 200 epochs with a random seed fixed (for
reproducibility); we used 2 seeds for each classification build, for a total of 8 final model configuration
(4 seeds for CE and MI Jigsaw training and 2 seeds for Target Task task classification). The reason
for 4 and 2 seeds is just so that our infrastructure can handle all the required builds (200 epochs of
training each time). We then report average and standard deviation over a set of 8 accuracies (top1,
20
Under review as a conference paper at ICLR 2020
top5 and top10 in %) on the validation set (5K) in Tab. 3. We used ADAM with these parameters for
all classifier builds. Here again random seeds were fixed.
Model CE We built a ”baseline” we call mode CE in Tab. 3. The CE model is an attempt at
providing a meaningful upper-bound on the performance on our MI trained Encoders by using a
Jigsaw CE trained encoder, freezing it and just learning C . 200 epochs of training is used, 2 random
seeds as well. Despite being better than η-MINE (and DV-MINE), the difference is only 1.9%
absolute for top1 (for η - MINE). This means that our MI training, and particularly η-MINE, does
a respectable job at learning a useful representation space for the target task.
21