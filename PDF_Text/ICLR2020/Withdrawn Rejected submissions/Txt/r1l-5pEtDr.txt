Under review as a conference paper at ICLR 2020
AdaX: Adaptive Gradient Descent with Expo-
nential Long Term Memory
Anonymous authors
Paper under double-blind review
Ab stract
Adaptive optimization algorithms such as RMSProp and Adam have fast con-
vergence and smooth learning process. Despite their successes, they are proven
to have non-convergence issue even in convex optimization problems as well as
weak performance compared with the first order gradient methods such as stochas-
tic gradient descent (SGD). Several other algorithms, for example AMSGrad and
AdaShift, have been proposed to alleviate these issues but only minor effect has
been observed. This paper further analyzes the performance of such algorithms in
a non-convex setting by extending their non-convergence issue into a simple non-
convex case and show that Adam’s design of update steps would possibly lead
the algorithm to local minimums. To address the above problems, we propose a
novel adaptive gradient descent algorithm, named AdaX, which accumulates the
long-term past gradient information exponentially. We prove the convergence of
AdaX in both convex and non-convex settings. Extensive experiments show that
AdaX outperforms Adam in various tasks of computer vision and natural language
processing and can catch up with SGD.
1	Introduction
In the era of deep learning, Stochastic Gradient Descent (SGD), though proposed in the last century,
remains the most effective algorithm in training deep neural networks (Robbins & Monro, 1951).
Many methods have been created to accelerate the training process and boost the performance of
SGD, such as momentum (Polyak, 1964) and Nesterov’s acceleration (Nesterov, 1983). Recently,
adaptive optimization methods have become popular as they adjust parameters’ learning rates in
different scales, instead of directly controlling the overall step size. These algorithms schedule the
learning rates using a weighted average of the past gradients. For example, AdaGrad (Duchi et al.,
2011) chooses the square root of the global average of the past gradients as the denominator of
the adaptive learning rates. It is shown that when the gradients are sparse or small, AdaGrad can
converge faster than vanilla SGD. However, its performance is quite limited in non-sparse settings.
Other adaptive algorithms have been proposed to replace the global average in AdaGrad by us-
ing the exponential moving average of past gradients, such as RMSProp (Tieleman & Hinton,
2012), AdaDelta (Zeiler, 2012), and Adam (Kingma & Ba, 2015). Among all these variants, Adam
(Kingma & Ba, 2015) is the most popular yet controversial optimization algorithm since it has faster
convergence rate than the others. However, Adam has worse performance (i.e. generalization ability
in testing stage) compared with SGD. Recent theories (Wilson et al., 2017; Reddi et al., 2018) have
also shown that Adam suffers from non-convergence issue and weak generalization ability. For ex-
ample, Reddi et al. (2018) proposed that Adam’s non-convergence problem originate from a mistake
in their proof of convergence. They constructed a counterexample for Adam and thoroughly proved
that Adam did not guarantee convergence even in a simple convex setting.
In the meantime, Zhou et al. (2019) theoretically proved that Adam could make a large update
when gradients are small, and a small update when gradients are large, which probably lead the
optimization process to wrong directions. Shazeer & Stern (2018) also empirically showed that
Adam’s parameter updates are not stable and its second moment could be out of date. Luo et al.
(2019) examined the effective learning rate of Adam in training and found that Adam would produce
too large or too small extreme learning rates. All the above analyses have suggested that Adam’s
unstable design of adaptive learning rate may impair the optimization process.
1
Under review as a conference paper at ICLR 2020
To address the above issues, we propose a new adaptive optimization method, termed AdaX, which
guarantees convergence in both convex and non-convex settings. This is done by memorizing the
long-term square of gradients exponentially as the second moment. To achieve this, we first extend
the convex counterexample in Reddi et al. (2018) to a non-convex setting. We then analyze how the
second moment instability in Adam and RMSProp could cause the optimization process to converge
to a local minimum even without noisy gradients, revealing that Adam produces update steps that
are too large in optimization. We also show how AMSGrad (Reddi et al., 2018) is unable to solve
Adam’s problem completely, because its effectiveness relies heavily on the magnitude of the maxi-
mal second moment, for instance, too large second moment would lead to insufficient training and
thus sub-optimal solutions. To address the above problems, we introduce a novel AdaX algorithm
and theoretically prove that it converges with a similar speed to Adam, but gets rid of the second
moment instability. Extensive experiments show that AdaX outperforms Adam and is comparable
to SGD with momentum in many tasks of computer vision and natural language processing.
2	Background and Notation
Overview of Adaptive Methods. To compare AdaX with the previous optimization methods, we
follow (Reddi et al., 2018) to present a generic framework of adaptive algorithms as shown in Algo-
rithm 1. Let Sd+ be a set of positive definite matrices in Rd×d, and F be the parameter domain. In
line 1, we take φt : F → Rd and ψt : F → S+d as input, which are unspecified moment functions
that vary among different optimization algorithms. After obtaining the gradient at time t in line 3,
We can calculate the corresponding first and second moment mt, Vt. In line 5, at = α∕√t is chosen
as the step size in each iteration for the convergence analysis of adaptive algorithms. The projection
operation in line 6, ∏f,m(y) is defined as argminχ∈F∣∣√M(X - y) ∣∣, where M ∈ S+ and y ∈ Rd.
Algorithm 1 Generic Adaptive Optimization Algorithm
1:	Input: x ∈ F, step size α, sequence of functions {φt , ψt }tT=1
2:	for t = 1 to T do
3:	gt = ▽%(Xt)
4:	mt = Φt(g1,g2, ...,gt) and Vt = Ψt(g1,g2, ...,gt)
5:	at = a/ʌ/t
6:	Xt+1 = ∏f ,√vt (xt — αtmt/√¼)
7:	end for
The main differences between the adaptive methods and the conventional SGD are in line 4 and 6,
where the matrix Vt scales the overall step size at element-wisely by 1/√Vt, known as the adaptive
learning rate. Using the general framework in Algorithm 1, we are able to summarize many adaptive
optimization algorithms proposed recently. For example, AdaGrad (Duchi et al., 2011) designed
its Vt as the global average of past gradients, while Adam (Kingma & Ba, 2015) and RMSProp
(Tieleman & Hinton, 2012) chose the exponential moving average as follows instead.
Vt「)diag(X β2-ig2),
(Adam)
where β2 is the fixed second moment coefficient and gi2 denotes the element-wise square of the
gradients. The diagonal operation diag() perform the dimension transformation from Rd to S+d . In
order to improve the performance of Adam, Reddi et al. (2018) proposed AMSGrad, which took
max operation on the second moment. Zhou et al. (2019) argued that we could replace gt2 in Vt
with some past gradient squares gt2-n to temporarily remove the correlation between the first and
second moment. Huang et al. (2019) constructed NosAdam, where a sequence of β2t ’s gave higher
weights to past gradients. We provide a summary of different designs of adaptive learning rate in
Table 1. It’s noticeable that these algorithms, due to their exponential moving average design, still
assign relatively high weights on recent gradients and past information is not emphasized. Besides,
Loshchilov & Hutter (2019) noticed the difference between L2 regularization and weight decay in
adaptive algorithms and improved Adam with standard weight decay by proposing AdamW.
Optimization Framework. A commonly used framework for analyzing convex optimization al-
gorithms was constructed by Zinkevich (2003), named the online optimization problem. In this
2
Under review as a conference paper at ICLR 2020
Table 1: Comparisons of different designs of the second moment
	SGDM	AdaGrad	RMSProp
ψt	I	diag(Pi=I g2 埒	(1 - β2)diag(Pit=1β2t-igi2)
	Adam	AMSGrad	AdaShift
ψt	(1-βt)diag(Pi=ι β2-ig2)	diag (max (Vt-ι,vt))	diag(β2vt-1 + (1 - β2)gt2-n)
	NosAdam	...	AdaX (ours)
ψt	diag(β2tvt-1 + (1 - β2t)gt2-n)	...	a+β¾t-ι diag(Pt=ι(1 + β2)t-ig2)
framework setting, the optimization algorithm chooses a parameter set θt ∈ F and an unknown
cost function ft (θ) evaluates its performance at θt in each iteration. Suppose that there exists a best
parameter ft(θ*) such that θ* = argmi□θ∈F (PT=I ft(θ)), then a metric used to show the algo-
rithm's performance is the regret function RT = PT=I ft(θt) - ft(θ*) and we want to ensure that
RT = o(T) so that the algorithm will always converge to the optimal solution. (Zinkevich, 2003).
Non-convergence of Adam. Reddi et al. (2018) proposed that the matrix Γt defined as follows, was
mistakenly assumed to be positive semi-definite in the original convergence proof of Adam.
(1)
where Vt and αt are defined as in Algorithm 1. Based on such an observation, they constructed
the following online convex optimization problem, in which Adam failed to converge to the optimal
solution. Let C > 2 be a fixed constant and {ft} be the sequence of cost functions whose sum is to
be minimized. Define ft as follows
ft(x)
Cx, for t mod 3 = 1
-x, otherwise
(2)
where X ∈ F = [-1,1]. In this problem, Adam could not distinguish between the true large gradient
direction (C) and the noisy gradient directions (-1) because its √Vt scales the gradients to be of
similar sizes, which forces the algorithm to reach a highly suboptimal solution x = 1 every three
iterations. However, SGD and AdaGrad are both able to counteract the noisy gradients and converge
to the optimum, which reveals the fact that Adam’s design of adaptive learning rate is very unstable.
3 The Nonconvergence of Adam in a Non-convex Setting
In this section, we extend the non-convergence problem in (2) to the non-convex setting and explain
why the fast convergence of Adam impairs its performance in the long term. Let C ∈ (1, +∞), λ ∈
(0, 1) be constants in R, consider the following simple sequence of non-convex functions ft.
Cλt-1x, forx ≥ 0
ft(X)=Ig, forx< 0 —
∀t ≥ 1
(3)
It can be easily observed that for the domain F = [-2, C/(1 - λ)], the minimum of each ft is
obtained at X = 0. Suppose we start from X0 > 0, then this problem simulates a situation where the
gradient decreases exponentially as time increases, implying that the algorithm is approaching the
global minimum and smaller step sizes are needed. Compared with the non-convergence problem
proposed by Reddi et al. (2018), no high-frequency noise exist in our gradients. However, next to
the optimal solution, there is a local minimum trap where no gradients exist and thus no algorithm
could escape. Let ɑι = α be the initial step size, We are able to show that SGD is capable of
avoiding the trap, even without the learning rate decrease at = α/ʌ/t, and will converge to X = 0
if initialized well. However, Adam ignores the gradient decrease information and always enters the
trap regardless of initialization. We summarize the above results in the following lemma
3
Under review as a conference paper at ICLR 2020
Lemma 3.1 In problem (3), with βι 二 0, β2 ∈ (0, λ2) and α ≥ α∕t, Adam will always reach the
local minimum, i.e. PT=I f (t)/T → ιC-λ, ∀x1,α1 > 0.
We provide all the proofs in the Appendix. This parameter setting of Adam is the same as RMSProP
except for the bias correction term, and the condition βι < √β2 mentioned by Kingma & Ba (2015)
is automatically satisfied. αt ≥ a/t is a weak requirement for the step sizes and can be ensured with
constant step sizes or at = a∕√t as in the convergence analysis. Intuitively, Adam would scale
the decreasing gradient by 1/√Vt, which approximately increases with the same speed. Therefore,
its update steps would be larger than a fixed constant at any time step and would ultimately lead
the parameters to the trap regardless of initialization. People may wonder whether the first moment
design helps Adam in such a situation, but we can show that as long as the condition βι < √β2 is
satisfied, Adam would always reach the local minimum. Hence, Adam converges faster than SGD
because of its large updates, but it cannot slow down when approaching the global minimum.
AMSGrad is constructed to address the problem of Adam’s large steps during optimization. How-
ever, it suffers from two major issues in a non-convex situation. 1) The never decreasing Vt in
AMSGrad could lead to early stops and therefore insufficient training during optimization, as re-
vealed by Huang et al. (2019); 2) The time for achieving the maximum of Vt is uncontrollable. We
show that for certain cases in our problem, AMSGrad is unable to help Adam.
Lemma 3.2 In problem (3), with βι = 0 and α ≥ α∕t, ∀β2 ∈ (0,1), ∃λ ∈ (√β^, 1), such that
AMSGrad will always reach the local minimum, i.e. PT=Ife)/T → ιC-λ,∀x1,α1 > 0.
The lemma essentially states for any fixed β2, we can find a λ such that AMSGrad cannot help Adam
because Vt keeps increasing before stepping into the trap. Therefore, we still need an algorithm that
can generate stable learning rates and control the update steps effectively. We emphasize that al-
though the functions in (3) are not smooth, the problem does successfully provide some intuition on
why Adam variants trains much faster than SGD, but cannot have comparable testing performance.
4 Algorithm and Convergence
Next, we introduce our novel optimization algorithm and present its special way of adjusting the
adaptive learning rate. Based upon the above discussions that current gradients lead to unstable
second moment and that long-term memory algorithms are preferred, we design our algorithm AdaX
by weighting exponentially more on the history gradients and less on the current gradients, as shown
in Algorithm 2. The most important differences between AdaX and Adam are in line 6 and 7, where
instead of an exponential moving average, we change β2 to 1+β2 and accumulate the past gradients.
Such a design guarantees that noisy and extreme gradients cannot greatly influence the update steps,
and Vt would gradually become stable. Similar to Kingma & Ba (2015)'s derivation, in order to
Algorithm 2 AdaX Algorithm
1:	Input: x ∈ F, step size {at}tT=1, β1, β2, β3
2:	Initialize m0 = 0, v0 = 0
3:	for t = 1 to T do
4:	gt = Vft(xt)
5:	mt = β1mt-1 + (1 - β3)gt
6:	vt	=	(1 + β2 )vt-1	+ β2gt2
7:	Vt	=	vt/[(l + β2)t	- l] and Vt	= diag(Vt)
8:	xt+l = ∏f ,√vt (Xt — αtmt/λ∕^ )
9:	end for
achieve an unbiased estimate of second moment, we obtain our bias correction term as follows. Let
gt be the gradient at timestep t and further suppose gt ’s are drawn from a stationary distribution
4
Under review as a conference paper at ICLR 2020
gt 〜p(gt). Take expectation on both sides of line 6 in Algorithm 2, We get
E(vt ) = E((1 + β2 )vt-1 + β2gt2 )
t
= X(1 + β2)t-iβ2E(gt2)	(4)
i=1
= [(1 + β2)t - 1]E(gt2)
To maintain an accurate second moment, We Would naturally divide vt by (1 + β2)t - 1 in line 7.
HoWever, it’s Worth mentioning that We do not include a first moment correction term (1 - β1t ) as
Kingma & Ba (2015) did for the folloWing reason. Consider the Stochastic Gradient Descent With
momentum(SGDM) algorithm and Adam’s first moment,
t
SGDM:	mt = γmt-1 + gt =	γt-igi
i=1
t
Adam:	mt = β1mt-1 + (1 - β1)gt = (1 - β1) X β1t-igi
i=1
It can be observed that they have the same form except for the scaling constant 1 - β1, and therefore
the first order bias correction term is counter intuitive. We change the scaling constant from 1 - β1
to 1 - β3 in line 5 in our algorithm to obtain a more general form of the moment expression,
and When β3 6= β1 , it helps to scale the hyper-parameters (such as step size, Weight decay) of
adaptive algorithms to the same size as SGD. Next We shoW that our algorithm ensures the positive
semi-definiteness of Γt and hence does not have the non-convergence issue of Adam. Consider the
folloWing lemma Which leads to the conclusion that Γt is positive semi-definite in our algortihm,
Lemma 4.1 Algorithm 2 ensures that the matrix V - -V2-1 占 0
Finally, We provide the convergence analysis of our algorithm in both convex and non-convex set-
tings. Using the analysis framework by Zinkevich (2003), the following theorem states that we are
able to obtain a regret bound of O(√T), which is the same as the results of Reddi et al. (2018). A
domain F is said to have bounded diameter if kx - yk∞ ≤ D∞, ∀x, y ∈ F for some D∞ ∈ R.
Theorem 4.1 Let {xt} and {vt} be the Sequences obtained from Algorithm 2, αt = α∕√t, βι,ι =
βι,βι,t ≤ βι,forall t ∈ [T ] and β2t = β2∕t, β3t = 1 —1∕√t∙ Assume that F has bounded diameter
D∞ and ∣∣Vft(x)k ≤ G∞ for all t ∈ [T ] and X ∈ F. Thenfor Xt generated using Algorithm 2, we
have the following bound on the regret.
RT ≤
D∞ XX ^1∕2 +	d∞	XX XX βltvt,i
2ατ(I — βI) = T,i	2(I — βI) t=1 =	αt
αC √Γ+Tog-T
(I- βI)3√β^
d
X l|gl：T,ik2
i=1
(5)
+
The following corollary follows naturally from the above theorem.
Corollary 4.1 Suppose β1t = β1λt-1 in Theorem 4.1, then we have
D∞ √t SL "/2	dβ1D∞ G∞	αC√1 + lθg T SL H H
≤ 2α(1-βι) ⅛% + 2α(1-βι)(1- λ)2 + "-FW ⅛ ""'"2
(6)
Analyzing optimization algorithms in a non-convex setting is slightly different from the convex
case, where instead of the average regret, stationarity in gradient is utilized to show convergence in
time. Following Chen et al. (2019), suppose we are minimizing a cost function f that satisfies the
following three assumptions
A1. f is differentiable and has L-Lipschitz gradient, i.e. ∀X, y, "Vf (X) -
Vf (y)" ≤ LkX — y". and f (x*) > ∞ where x* is the optimal solution.
5
Under review as a conference paper at ICLR 2020
λκ⅝ou4 I∙d⅞LUM.
1
V*Iaa¥
ιooαα Mow
3∞∞ 4O∞0 SMM ∞α∞
——A<⅛ mW, wd-5&4
—AdamWf Wd-Ie*4
—SGD
X⅛X-W, wd-5β-4
——⅛⅛*W, wd∙lc∙3
IOOOO
x>oaa 3∞∞ 4ooaa saooa ∞a∞
I I
AMlV*I-
IOOOO
20aoa 3θooa
4ooao 5«oa βa∞a
I HraHcns	IttradQns	ItmtiQnS
(a) Training Accuracy (b) Testing Accuracy for
for L2 Regularization	L2 Regularization
(c) Training Accuracy (d) Testing Accuracy for
for Weight Decay	Weight Decay
Figure 1: Training and Testing Accuracy on CIFAR-10
Method	Top 1 Acc	Method	Top 1 Acc
Adam	90.96 ± 0.03	AdamW, Wd=Ie-4	91.86± 0.04
AdaGrad	86.56 ± 0.07	AdamW, wd=5e-4	92.12 ± 0.05
AMSGrad	90.98 ± 0.04	SGDM	92.30 ± 0.09
RMSProp	89.64 ± 0.05	AdaX-W, wd=5e-4(ours)	92.32 ± 0.04
AdaX(ours)	91.60 ± 0.10	AdaX-W, Wd=Ie-3(ours)	92.51 ± 0.07
Table 2: Validation accuracy on CIFAR-10.
A2. Suppose that the true gradients and the noisy gradients are bounded i.e.
kVf (xt)k ≤ G∞, kgtk ≤ G∞,∀t ≥ 1. Also, kαt m k ≤ G forsome G> 0
A3. The noisy gradient is unbiased and the noise is independent, i.e. gt =
Vf(xt) + ηt, E[ηt] = 0 and ηi is independent ofηj ifi 6= j.
then we would obtain the following theorem and corollary, which prove that AdaX converges with
a speed close to AMSGrad as mentioned by Chen et al. (2019).
Theorem 4.2 Let {xt} and {vt} be the sequences obtained from Algorithm 2, αt = α∕√t, βι,ι =
β1, β1,t ≤ β1, for all t ∈ [T] and β2t = β2 /t, β3t = β1. Assume that kVft (x)k ≤ G∞ for all
t ∈ [T] and x ∈ F. Then for xt generated using Algorithm 2, we have the following bound.
minE kVf(xt)k2
< G∞	(CιG∞α2 + C2da 十 C3dα2
(7)
where C1 , C2 , C3 , C4 are constants independent ofT
Corollary 4.2 Suppose β2t = β2, with the other assumptions same as in Theorem 4.2, we have
min E [kVf (χt)k2] ≤ -G∞( C⅛^+C2dα+乎+C4)	⑻
t∈[T]	α T c	c	c
5 Experiments
In this section, we evaluate the performance of AdaX on various tasks in comparison with SGD with
Nesterov momentum (SGDM), Adam(W), and many other common optimizers. The implementa-
tion of AdaX consists of two parts, AdaX and AdaX-W, representing using L2 regularization and
standard weight decay in the algorithm respectively as discussed in Loshchilov & Hutter (2019). We
relegate the detailed implementation of AdaX to the Appendix. We show that AdaX, combined with
a proper weight decay, is capable of performing better than Adam and SGDM in many tasks.
5.1	Convolutional Neural Network on CIFAR- 1 0
Using ResNet-20 proposed by He et al. (2016), we verified the performance of AdaX on CIFAR-10
(Krizhevsky et al., 2009) image classification task. In our experiments, we utilized a learning rate
6
Under review as a conference paper at ICLR 2020
>UE3UW<
—AdamW
—AdaX-W
—SGD
60555°
10∞00
200∞0
300000	400000
Iterations
500000
45
(a) Training Top-1 Accuracy on ImageNet
70
65
Q 5 O
6 5 5
A3enMV,dp3s⅝L
10∞00
200∞0
300000	400000	50∞00
Iterations
(b) Testing Top-1 Accuracy on ImageNet
Figure 2:	Training and Testing Results on ImageNet.
(a) Validation accuracy on ImageNet and IoU on
VOC2012 Segmentation
(b) Validation perplexity on One Billion
Word for language modeling.
Method	ImageNet Top 1	Method	VOC2012 IoU	Method	Validation PPL
SGDM	69:90	-SGDM-	76.28	Adam	36:90
AdamW	66.92	AdamW	74.62	AdaX(ours)	35.22
AdaX-W(ours)	69.87	RMSProP	58.70		
		AMSGrad	73.62		
		AdaX-W	76.53		
Table 3: Performance of AdaX on ImageNet, VOC2012 Segmentation and One Billion Word
schedule that the initial step size was scaled down by 0.1 and 0.01 at the 100-th and the 150-th
epoch. The experimental results in Table 2 corresponded to our theoretical finding that Adam was
actually taking steps that were ”too large” and would potentially converge to local minimum at the
end.
L2 Regularization. We used L2 regularization of strength 5e-4 for all the optimizers. As shown
in Figure 1a and 1b, although AdaX was relatively slow at the beginning compared with other
optimizers, its testing accuracy quickly caught up with the others after the first learning rate decrease
and became the highest (91.6) at last.
Weight Decay. The baseline was trained with SGDM with weight decay 5e-4. Adam with weight
decay, named AdamW (Loshchilov & Hutter, 2019) was also trained for comparisons. Although
AdamW with 1e-4 weight decay converged much faster than SGDM and AdaX-W, its final accuracy
could not catch up with the other two (see Figure 1c & 1d).A higher weight decay could potentially
help AdamW achieve a better performance (92.1), but it was as slow as SGDM. On the other hand,
AdaX-W with step size rate 0.5 and 5e-4 weight decay converged fast and yielded the same perfor-
mance as SGDM(92.32). The best result we obtained was AdaX-W with step size 0.25 and 1e-3
weight decay, which resulted in 92.51 Top-1 accuracy, even slightly higher than SGDM.
5.2 Convolutional Neural Network on ImageNet
We also conducted experiments to examine the performance of AdaX-W on ImageNet (Deng et al.,
2009). SGDM, AdaX-W, and AdamW were used to train a ResNet-18 model on ImageNet, with
a standard 1e-4 weight decay and 0.1, 0.5, 1e-3 step sizes as in CIFAR-10 respectively. A warm
up scheme was applied in the initial 25k iterations (Goyal et al., 2017), and then the step size was
multiplied by 0.1 at the 150k, 300k and 450k-th iteration steps. As observed from Figure 2, although
AdamW was fast at the beginning of training, its test accuracy stagnated after the second learning
rate decrease. AdaX-W, on the other hand, converged faster than SGDM without loss of testing
accuracy (69.87), as shown in Table 3a.
7
Under review as a conference paper at ICLR 2020
(a) Training Dynamics on One
Billion Word.
(b) Training Loss on VOC2012
Segmentation
(c) Testing IoU
Figure 3:	(a) Traning loss curves for Adam and AdaX on One Billion Word. (b, c) Training Loss
and Testing Results on VOC2012 Segmentation task. In (c), dashed lines are mean accuracy values
and solid lines are Intersection over Union (IoU) values
5.3	Recurrent Neural Network on Language Modeling
AdaX has also been validated on One Billion Word (Chelba et al., 2013) dataset of language mod-
eling task. For the One Billion Word, we used a two-layer LSTMs with 2048 hidden states and sam-
pled softmax. The global experiment settings in the released public code Rdspring1 was adopted
in this study. For both vanilla Adam and AdaX, the LSTMs were trained using for 5 epochs, with
learning rate decaying to 1e-8 linearly. Similarly, the weight decay for AdaX was set to 0. Note that
the regular SGD is not suitable in this task, so it is not included in the comparison.
The training loss and the validation perplexity is shown in Figure 3a and Table 3b. We can see that
the AdaX outperforms the Adam baseline by a significant margin (35.22 vs. 36.90). Moreover,
similar to the effect on image classification tasks described above, AdaX starts a little slower at the
early stage, but it soon surpasses Adam on both training and validation performance.
5.4	Transfer Learning
To further examine the robustness of AdaX in transfer learnings such as semantic segmentation,
we evaluated its performance on the PASCAL VOC2012 augmented dataset (Everingham et al.,
2014) (Hariharan et al., 2011). The classic Deeplab-ASPP model (Chen et al., 2016) was adopted
with a ResNet-101 backbone pretrained on MS-COCO dataset(Lin et al., 2014). Adaptive methods
are seldom used in semantic segmentation tasks because of their bad performances, but AdaX can
surprisingly be applied to train these models as well. The initial step sizes for different algortithms
was recorded in section A.7 and all other parameters stuck to the default setting in Chen et al.
(2016). We evaluated the algorithms’ performances at the 5k, 10k, 15k and 20k iterations using
intersection over union (IoU). As can be observed in Figure 3c, AdaX-W trained faster than SGDM
and obtained a higher IoU (76.5) at the same time. On the other hand, AdamW was not capable of
obtaining comparable results.
5.5	Comparison of Second moment Design
Finally, we compared the second moment design of Adam and AdaX empirically and showed the
instability of Adam’s second moment could have a large impact on its performance while our design
was stable and robust. We first evaluated the performance of different algorithms in our synthetic
example (3). The problem parameters were set to be C = 1e - 3, λ = 0.9999, x0 = 1. To
ensure fair comparisons, default hyperparameters were chosen for all the algorithms, specifically
α0 = 0.1, γ = 0.9 for SGDM, α0 = 1e - 3, β1 = 0.9, β2 = 0.999 for Adam, and α0 = 0.15, β1 =
0.9, β2 = 1e - 4, β3 = 0.999 for AdaX. As shown in Figure 4a and 4b, SGD and AdaX quickly
converged under the strong gradient decrease information and could potentially reach the global
minimum if initialized well. However, the update steps of Adam decreased with a much slower rate,
which resulted in substantial changes in x and ultimately lead the algorithm to the local minimum.
Zaheer et al. (2018) found that Adam’s performance could be affected by different values of, which
was originally designed to avoid zeros in the denominator. Such a finding revealed the instability
8
Under review as a conference paper at ICLR 2020

0Λαθ∣ια
0Λ0αα0β
θΛoααoβ
04Wo(U
sβooα
IMMO UMM MOOM 2M0M 3M
IttradQns
oλoooooL
0
50000
ιooα oα	ιsooαα aoooαα zsmm sao«
∣nraa0n5
λκ⅝ou4 I∙d⅞LUM.
βe
AdaX-W e-ie-β
Mamw e-iβ-β
AdaX-We-Ie-S
Adamw e-ie-5
AdaX-We-Ie-*
AdamW e-iβ-3
IMM 2M00	3000« 44M0 MOM «M8
►(radons
I I
AMlV*I-
ιooaa aoaoa soooa <o∞o sβooa ca∞a
IWadcn5
OM叫

(a) Updates over Itera- (b) x-Position over Iter-	(c) Training Accuracy	(d) Testing Accuracy for
tions	ations	for Different values of	Different values of
Figure 4:	Second momentum stability comparisons. (a),(b). Training updates and x-positions on
Problem (3). (c),(d) Training and testing results of AdamW and AdaX-W with different ’s
of Adam’s second moment. We verified their claim by testing AdamW with different values of
∈ {1e - 8, 1e - 5, 1e - 3} on the CIFAR-10 dataset. We found that larger values of , since it
helped to stabilize very small second moment, did improve AdamW’s performance by around 0.35
percent accuracy on CIFAR-10. However, AdaX-W’s performance was not affected by different
choices of as shown in Figure 4d, which again proved our claim that a long term memory design
was more stable than the exponential moving average in Adam.
The experiments shown above verify the effectiveness of AdaX, showing that the accumulated long-
term past gradient information can enhance the model performance, by getting rid of the second
moment instability in vanilla Adam. It is also worth noticing that the computational cost for each
step of AdaX and Adam are approximately the same, as they both memorize the first and second
momentum in the past. Therefore AdaX enables one to get higher performance than Adam in various
tasks using the same training budget.
6 Conclusion
In this paper, we present a novel optimization algorithm named AdaX in order to improve the per-
formance of traditional adaptive methods. We first extend the non-convergence issue of Adam to
a non-convex case, and show that Adam’s fast convergence impairs its convergence. We then pro-
pose our variant of Adam, analyze its convergence, and evaluate its performance on various learning
tasks. Our theoretical analysis and experimental results both show that AdaX is more stable and
performs better than Adam in various tasks. In the future, more experiments still need to be per-
formed to evaluate the overall performance of AdaX and AdaX-W. Moreover, our paper is a first
step into designing adaptive learning rates in ways different from simple and exponential average
methods. Other new and interesting designs should also be examined. We believe that new adap-
tive algorithms that outperform AdaX in convergence rate and performance still exist and remain to
explore.
References
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony
Robinson. One billion word benchmark for measuring progress in statistical language modeling.
arXiv preprint arXiv:1312.3005, 2013.
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille.
Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and
fully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40:834-
848, 2016.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence ofa class of adam-type
algorithm for non-convex optimization. Proceedings of 7th International Conference on Learning
Representations(ICLR), 2019.
9
Under review as a conference paper at ICLR 2020
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale
hierarchical image database. in 2009 ieee conference on computer vision and pattern recognition.
IEEE, 40:248255, 2009.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning Research (JMLR), pp. 12:2121-2159,
2011.
Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John Winn, and
Andrew Zisserman. The pascal visual object classes challenge: A retrospective. International
Journal of Computer Vision(IJCV), 2014.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Se-
mantic contours from inverse detectors. International Conference of Computer Vision(ICCV),
2011.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2016.
Haiwen Huang, Chang Wang, and Bin Dong. Nostalgic adam: Weighting more of the past gradients
when designing the adaptive learning rate. arXiv preprint arXiv: 1805.07557, 2019.
Diederik P Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. Proceedings
of the 3rd International Conference on Learning Representations (ICLR), 2015.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-
search). 2009.
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. LaWrence Zitnick. Microsoft COCO:
common objects in context. CoRR, abs/1405.0312, 2014.
Ilya Loshchilov and Frank Hutter. Decoupled Weight decay regularization. Proceedings of 7th
International Conference on Learning Representations (ICLR), 2019.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods With dynamic
bound of learning rate. Proceedings of 7th International Conference on Learning Representations,
2019.
H Brendan Mcmahan and MattheW Streeter. Adaptive bound optimization for online convex opti-
mization. Proceedings ofthe 23rd Annual Conference On Learning Theory (COLT), pp. 244-256,
2010.
Y. Nesterov. A method for unconstrained convex minimization problem With the rate of convergence
o(1/k2). Doklady AN USSR, pp. (269), 543-547, 1983.
Boris Polyak. Some methods of speeding up the convergence of iteration methods. USSR Compu-
tational Mathematics and Mathematical Physics, pp. 4(5):1-17, 1964.
Rdspring1. Pytorch gbW lm. https://github.com/rdspring1/PyTorch_GBW_LM.
Sashank J. Reddi, Stayen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. Pro-
ceedings of the 6th International Conference on Learning Representations (ICLR), 2018.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, pp. 22(3):400-407, 1951.
Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates With sublinear memory cost.
arXiv preprint arXiv: 1804.04235), 2018.
10
Under review as a conference paper at ICLR 2020
Tijmen Tieleman and Geoffrey Hinton. Rmsprop: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural networksfor machine learning,pp. 4(2):26-31, 2012.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. Advances in Neural Information Pro-
cessing Systems 30, pp. 4148-4158, 2017.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive meth-
ods for nonconvex optimization. Advances in Neural Information Processing Systems 31, 2018.
Matthew D. Zeiler. Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Zhiming Zhou, Qingru Zhang, Guansong Lu, Hongwei Wang, Weinan Zhang, and Yong Yu.
Adashift: Decorrelation and convergence of adaptive learning rate methods. Proceedings of 7th
International Conference on Learning Representations (ICLR), 2019.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In-
ternational Conference on Machine Learning (ICML), 2003.
11
Under review as a conference paper at ICLR 2020
A Appendix
A.1 Proofs of Lemma 3.1, Lemma 3.2
We consider a one dimensional non-convex case where {ft} are a sequence of linear functions that
have decreasing gradients in the long term. We want to show that because Adam trusts its current
gradient as the second moment, its step sizes are too large and the algorithm would converge to a
suboptimal solution. Let constant C be the initial gradient, define cost function ft as follows:
ft(x)
Cλt-1x, forx ≥ 0
C2
-----,for x < 0
1-λ
∀t ≥ 1
(9)
where λ is the decreasing factor of gradient. Consider F = [-2, C/(1 - λ)], then it’s obvious that
the minimum regret is obtained at x = 0. Let the initial step size be α1 = α, we then consider the
performances of different algorithms in this setting.
(SGD). We first show that without the momentum, vanilla SGD is able to converge to the optimum
solution and avoid the trap. Take derivative with respect to x, we obtain that
Vft(X) = Cλt-1, for X ≥ 0
∞∞
XVft(X) = XCλt-1
(10)
Therefore, even if We set at = α, ∀t ≥ 1, as long as the initial point xo ≥ -ααCλ, SGD is able to
avoid the trap. If at = α/√t, then the condition can be even less strict: xo ≥ P∞=1 αC√t-1. SGD
is able to converge to the optimum if the equal signs are true.
(Adam). We consider the Adam algorithm With the folloWing parameter setting:
/	a
βι = 0,0 < ββ2 < λ < 1, and at = √=
(11)
Note that this parameter setting of Adam is the same as RMSProp, but We can further shoW that
even if β1 6= 0, We still obtain similar results. Consider hoW vt changes in time, before it reaches
the negative region, the gradients are positive and
vt = β2vt-1 + (1 - β2)(C λt-1)2
t
= X β2t-i(1 -β2)(Cλi-1)2
i=1
(1- β2)c 2(λ2t- β2)
λ2 - β2
(12)
Note that λ > √β2, therefore the update rule is:
_	gt _	√λ2-12λt-1	_	√λ2-12λ-1
xt+1 = xt - at-= = xt - at —∕/	C、/、C, 京=Xt - at -/	=
√vt	p(1 - β2)(λ2 - β2)	q(1 - β2)(1 - (β2)t)
I_________ V	(13)
/ a / λ2 - β2
≤ Xt -√^Vλ2(1 — β2)
C
1 — λ
12
Under review as a conference paper at ICLR 2020
Note that the series P∞=ι √1t diverges, hence Adam would always reach the negative region. Same
argument applies as long as at ≥ a∕t. We would emphasize here that the bias correction term in
Adam does not change the final result as 1 - β2 ≥ 1 - β2 and therefore the update steps are still
bounded. We could further show that when βι = 0,βι < √β2, Adam will still go to the negative
region. Since mt = β1mt-1 + (1 - β1)gt = Pit=1(1 - β1)β1t-igi, therefore
mt	(1 - βι)(λt- βt)	√λ2-^2
---=----------------- —
√vt	λ - β	√(1 - β2 )(λ2t - β2)
(1 - βι)√2-β2	λt- βt
=--------- =•—	=
(λ - βI) √(I - β2 )	√λ2t - β2
(I- βI) √λ2- β 1 - (β”λy
= ------..—，	, •—,
(λ - βI)√1 - β	√1 - (β2∕λ2)t
≥ (I - βI) √λ2 - 82 (1 - βι)
一(λ - βι)√Γ-W Y FJ
(14)
Since the update steps are lower bounded, the algorithm would still go to the negative region.
(AMSGrad). We now evaluate the performance of AMSGrad in our formulated problem. Note that
Vt in AMSGrad would take the same form as Adam, and Vt = max{vt}t=1. We suppose that the
maximum is obtained at v1 as an example, then
ʌ	(I- β2)C2(λ2 - β2) 门 R、「2
vt=V1 = —λ2-β— = (1 - β2)C
gt	gt	λt-1
—=-----=-
√Vt	√vT	√1 - β2
(15)
As we can see, AMSGrad partially solves the problem of Adam and restores the gradient decrease
information as its Vt is lower bounded. If the maximum of Vt is obtained before the parameters
enter the trap, AMSGrad could possibly have a better performance in this problem as it prevents
the update steps from being too large. However, one important determining factor is the time when
the maximum value is obtained. If Vt in fact keeps increasing before a very large number T , then
AMSGrad would have the same performance as Adam. We explain the above intuition as follows.
Let h(t) = λ2t - β2t, then
等=ln λ2 ∙λ2t-ln β2 ∙ β2
(16)
If Cdlat ≥ 0, We have
dt
ln λ2 ∙ λ2t - ln β2 ∙ β2 ≥ 0
(λ! )t ≤曲鱼
(β2) ≤ ln λ2
When the equal sign is true, we have
ln β2
t =log β2 m不
1
β⅛t=β⅛，
lim t = ∞
λ→1-
(17)
1
ln(λ2)
13
Under review as a conference paper at ICLR 2020
The first equal sign in the first limit is due to L’Hospital’s rule. Therefore, the value of T where
vT = max{vt } depends on the difference between β2 and λ2, and the value of λ. If β2 is close to
λ2 or λ is close to 1, then vt needs a large number of steps to obtain the maximum. In such cases,
AMSGrad may not able to help Adam. Specifically, for a fixed β2, since limλ→1- t = ∞. and
gj√vt=s λλα →2)=√τ⅛ rι-ι	(18)
we know that larger λ2 will lead to both larger update steps and larger T when the maximum is
obtained, hence ∃λ∈ (0, 1), such that AMSGrad cannot avoid the trap.
(AdaX). We provide the performance of AdaX in this problem for completeness. We only show for
the case when β1 = β3 = 0, but the same results hold when the first order momentum is used.
1t
vt =(1+ β2)t- 1 X β2(I + β 尸(CλiT)2
(1+ β2)tβ2C2 λ-2 'X( λ
(1+ β2)t- 1	士 (1 + β2
(1 + β2)tβ2C2	1 - (1+β2)t
-:-----:~:--- -------------
(1 + β2)t- 1	1 + β2 - λ2
β2C2	(1 + β2)t -λ2t
-----------•—:------------
1 + β2 - λ2	(1 + β2)t - 1
(19)
gt
/1 + β2 - λ2
V -β-
λt-1 √(1 + β2)t - 1
√(1+ β2)t - λ2t
≤尸厂2尸
(20)
As we can see, AdaX successfully restores the gradient decrease information and controls the de-
crease speed by an almost fixed parameter instead of trusting the gradient at a certain moment, and is
therefore expected to perform better than AMSGrad since its step sizes are not affected by extreme
gradients. With a suitable initial step size and starting point, AdaX is able to avoid the designed trap.
A.2 Proof of Lemma 4.1
Proof.
Vt - t Pt=l(1+ β2 )t-iβ2g2
-
α 02	(1 + β2)t - 1
A t-1 Pt=1(1 + β2)t-iβ2g2
—α2	(1+ β2)t-(1+ β2)
A t-1 Pt=ι(1 + β2y-iβ2g2 - β2g2
—α2	(1 + β2)t-(1 + β2)
t - 1 Pt-1(1 + β2)t-1-iβ2g2 Vt-I
---7----：------：--：-------- ----7^-
α	(1+ β2)t-1 - 1	α2-ι
(21)
where in the first inequality We utilize the fact that (1 + β2)t ≥ 1 + tβ2 and hence ([十.：)t-i ≥
(i+β2tt-l(i+β2). Intuitively, it is easier to see this inequality if we simply let β2 to be a small number
such as 1e-4 in our implementation, then the denominator doesn’t change much while the numerator
decreases.
A.3 Auxillary Lemmas for Convergence Analysis
LemmaA.1 Assume that β21 = β2,β2t = β2∕t, With β2 ∈ (0,1) and Vt = [(1 + β2t)vt-1 +
β2tg2]∕[(1 + β2t)t - 1],Vt = diag(Vt), then we have V^ A V-I
14
Under review as a conference paper at ICLR 2020
Proof: Similar to Lemma 4.1 in the algorithm section, we have
Vt	t Pit=1 β2iΠtk-=i1(1 + β2(t-k+1))gi2
-
α2 α2	(1 + β2t)t - 1
=2Pt=I β∏k=iι(1 + t — k+1 )gi2
—α2	(1 + β2)t - 1
A t_ Pi-1 β ∏k=iι(1 + F 雇
― α2	(1+ β2)t - 1
=2 Pt-1 β ∏k=1-i(1 + t — k +1)9ιi
― α2	(1 + β)t-1 -(1 + β)-1
「-1 Pt-1 β ∏k=1-i(1 + t⅛ )。2 - vt-ι
A
—α2	(1 + t-1 尸-1	α2-ι
(22)
The first inequality comes from deleting the last term βt2 g2 and second one comes from the following
fact:
(1 + β )t-1-(1 + β )-1
(1 + β )t-1
t
t + β2
≤ 1 + — β2 +
β2 )2+•••+(t-1)( β
三)2+•••+(t-1)(
-1+
β2
t + β
1 + β2 +
≤ 1 + β2 +
t-1 - 1 + (	β2
t + β2
β2
t
三)T- 1
—
(1 + 7β⅛ )T- 1
t- 1
(23)
Therefore the positive semi-definiteness is satisfied.
Lemma A.2 For the parameter settings and conditions assumed in Theorem 4.1, we have
T
X
t=1
Bιtat
2(1 - βιt)
kVt-1/4mt-1k2 ≤
αC √Γ+Tog-T
(I- βι)2√β^
d
kg1:T,ik2
i=1
(24)
Proof: We first analyze with the following process directly from the update rules, note that
mT,i =	(1 - β3j)ΠkT=-1jβ1(T-k+1)gj,i
j=1
1T
VT,i = (1+ β2T)T - 1 Σβ2jπk = 1(1 + β2(T-k+1))gj,i
(25)
15
Under review as a conference paper at ICLR 2020
T
X αt∣L4mt-ik2
t=1
T-1	d m2
X αt(I-+ αT X PT
=X αt(1 - βιt)kVt-1imt-ik2 + α,(1+ β^)T - 1 X 平-3(T…⑼"产
t=1 i=1 T Pj=1 β2j Πk=1 (1 + β2(T -k+1))gj2
T-1	______________
≤ X^ αt(1 — βιt)kVt 1/4mt-ik2 + α J(1+ β2τ)t - 1
t=1
CX (PZI ∏T-3βi(τ-k+i))(PT=ι(1- β33)2∏T-3βi(τ-k+ιj
i=1	qT PZI β23 πT=1(I + β2(T-k+2g3,i
≤ X αt(1 — βιt)kV-1∕4mt-ik2 + α√(1+ β2τ)t - 1 X (PT=I，')(P](1-%产βT-3j
t=1	i=1	TPjT=1β2jΠkT=-1j(1 +β2(T-k+1))gj2
T-1
≤ X αt(1-βιt)kV-1∕4mt-ik2 +
t=1
T-1
≤ Xαt(1 -β1t)kVt-1∕4mt-1k2+
t=1
aP(1 + β2τ)t - 1 X	PT=I(I - β3)eT 3g2,i
(I-β1)√β2T	i=ι qj 1 ∏τ=ιa+β2cT-k+Qg3,i
αP(1+ β2τ)τ - 1 X X	√βTj2,i
(I- βl)√β2T	i=1 3 = 1 q3∏T-3 (1+ β2(τ-k + ι))g2,i
T-1	d T
≤ X αt(1-βιt)kV-1∕4mt-ik2 +(1- ；；E XXβTj3,i∣
t=1	i=1 3=1	(26)
where the first inequality is due to an application of Cauchy-Schwarz inequality. The second inequal-
ity is due to the fact that β1t ≤ β1, ∀t. The third inequality follows from P3T=1 β1T -3 ≤ 1/(1 - β1)
and the fact that 1 - β33 ≤ 1. The fourth one comes from only keeping one of the positive terms
in the denominator. The final one is from the fact that，(1 + β2τ)τ 一 1 ≤ C for a constant
C = √eβ2 - 1. By using induction on all the terms in the equation, We are able to further bound it.
T
XαtkVt-1∕4mt-1k2
t=1
T	dt	T	dt
≤ X【X X βj3,il=X【X X √tβt-3j
dT	T	dT	T
(1 - βI)√β2X X|gt，i1X Wj ≤: X X1gt，i|X √j
i=1 t=	3 =t	i=1 t=	3 =t
≤
≤
αC
(1 - βI)2√β^
dT
X Xlgt,il√t ≤
d uT	uT
™ X tXlgtRX t
αC
(1 - βI)2√β^
dT
X Xlgt,il√t ≤
αC λ∕1 + log T
(1 - β1 )2√β2
dt
Xtt
i=1
T
X|gt,i|2
t=1
(27)
The second equality folloWs from a re-arrange of sum order. The third inequality comes from the fact
that PT=t β3-t ≤ 1∕(1-βι) ≤ 1 /(1 -βι). The fourth inequality is again from the Cauchy-Schwarz
inequality and the final one is from the harmonic sum PtT=1 ≤ (1 + logT). By denoting g1:t,i to be
16
Under review as a conference paper at ICLR 2020
the vector of the past graidents from time 1 to t in the i-th dimension, i.e. g1:t,i = [g1,i, g2,i, ..., gt,i],
we complete the proof of the lemma.
Lemma A.3 For the parameter settings and conditions assumed in Theorem 4.1, we have
T	2d
X J [∣k”(χt -χ*)k2 -kVt1/4(xt+i -χ*)k2i ≤ 2α∞ X vT/2	(28)
t=1 αt	2αT i=1
1/2	1/2
vv
Proof: Using the definition of L2 norm, by Lemma A.1, since -Oi∙ ≥ ；-1；
T
X ɪ hkVt1/4(xt - χ*)k2 -∣U1∕4(χt+ι - χ*)k2i
t=1 αt
T
≤ —k%1∕4(xi- x*)k2 + X
α1
1	t=2
kVt1∕4(xt-x*)k2	kVt1∕4(xt+ι-x* )k2
αt	αt-1
d	Td
01 X V:/:" F2+XX
i=1	t=2 i=1
d	Td
αι X v1∕i2(χi,i-W)2 + XX
i=1	t=2 i=1
1/2
vt,i
αt
*、2
,i - xi)
1/2
vt-1,i (	*、2
------(xt,i - Xi)
αt-1	i
(29)
1/2	1/2
vt,i	vt-1,i
-------	
αt---αt-1
(xt,i - xi*)2
—
≤ DTX vT/i2
i=1
where the first inequality is from separating the first term and getting rid of the last negative term
in the summation. The last inequality is from a telescopic summation and the diameter bound that
kx - x*k ≤ D∞
A.4 Proof of Regret Bound
A.4. 1 Proof of Theorem 4.1
Proof. Following the proof given by Reddi et al. (2018), we provide the proof of regret bound
in Theorem 4.1. Beginning with the definition of the projection operation ∏f,√v, We have the
observation
xt+1 =∏f√^(xt - atVtτ∕2mt) = min IlVtI/4(x -(Xt- atVΓ"2mt))∣∣	(30)
,t
Using Lemma 4 in Reddi et al. (2018) proved by Mcmahan & Streeter (2010) with a direct substitute
of z： = (xt — αtV-"2mt), Q = V1/2 and z2 = X* for X* ∈ F, the following inequality holds:
∣K1∕4(uι - u2)k2 = k匕1∕4(xt+ι - X*)k2 ≤ IK”(xt - αt匕-1/2mt - x*)k2
=kVt1/4(Xt- x*)k2 + α2kVtτ∕4mtk2 - 2αthmt, (xt - x*)i
= IVt / (Xt - X*)I2 + αt2 IVt- / mtI2 - 2αthβ1tmt-1 + (1 - β1t)gt, (Xt - X*)i
(31)
where the first equality is due to the fact that ∏f &(x*) = x*. Rearrange the last inequality, we
obtain
17
Under review as a conference paper at ICLR 2020
(I-%hgt,(Xt-"≤ W hkVt1/4(Xt-x*肝-kVt1/4(Xt+ι-*k2i +αtkVt-Ftk2
-β1thmt-1, (Xt - x*)i
≤ 2α [kV1/4(xt - x*)k2 -kV1/4(xt+i - x*)k2i + OaIkT/4mtk2
+ H MT4mt-ik2 + 2⅛ kV1∕4(xt-x*)k2
t	(32)
The second inequality comes from applications of Cauchy-Schwarz and Young’s inequality. We
now make use of the approach of bounding the regret using convexify of ft as in Kingma & Ba
(2015). Following Lemma A.2 and Lemma A.3, we have
T
T
Eft(Xt)- ft(X) ≤ Ehgt, (Xt- x*)i
t=1
t=1
T
≤X
t=1
2Ot(1⅛) UVv4(Xt-X*)k2 -kVt1∕4(Xt+ι -X*)k2i + 2(T⅛) k%τ4mtk2
D∞2
Bιtαt
2(1 - βIt)
d
kVtT4mt-ιk2 + 2θd‰ i(Xt-X*)k2
2αT(1 - β1)
X vT∕i2 + X
β1t
D∞2
i=1
d
t=1
T
2αt (1 - β1 )
∣∣τz1∕4/	*、∣∣2 , OC √1 + log T VIl H
M (Xt-X)k + (i - β1)3√β2 入 kg1"2
2αT(1 - β1)
X vT∕i2 + X
D∞2
2αT(1 - β1)
D∞2
2αT(1 - β1)
i=1
d
X vT∕i2 +
i=1
d
X vT∕i2 +
i=1
t=1
1 X o (	_	* ^2^1∕2 , OC √1 + log T
20t(1- βι)工e1t(Xt,i - Xi) vt,i + (1 - βι)3√β2
d
kg1:T,ik2
i=1
D∞2
2(1 - β1)
D∞2
2(1 - β1)
Td
XX
t=1 i=1
Td
XX
t=1 i=1
1∕2
βιtvt,i	+
αt
βιtV1∕2 +
oC ʌ/l + log T
(I- βI)3√β2
αt
oC √Γ+TogT
(I- β1)3√β2
d
X kg1:T,ik2
i=1
d
kg1:T,ik2
i=1
(33)
A.4.2 Proof of Corollary 4.1
Proof. We first take a look at the size of V1,,2, note that ∣Vft(θ) ∣∣∞ ≤ G∞
^t,i
(1+β2t)t- 1
t
X β2j Πtk-=j1(1 + β2(t-k+1))gj2,i
j=1
≤ W X β W(1 + 3)
t1	1
≤ g∞ X /=ji(1 + EI)
=G∞ X j≡=tG∞
(34)
The first inequality is due to the fact that (1 + β2t)t ≥ (1 + β2) and the gradient bound. The second
inequality follows from β2 < 1. The last inequality is from the telescopic sum. Then we have the
following inequality,
≤
≤
≤
+
T
1
18
Under review as a conference paper at ICLR 2020
XXX X βltv1/2 ≤ dG∞ β χχχ λt-1t ≤ 4G⅛2	(35)
t=1 i=1	αt	α t=1	α(1 - λ)2
The second inequality is due to the arithmetic geometric series sum PT=I λt-1t < (I-λ)2, the
reason is as follows
(S = λ0 + 2λ1 + …+ tλt-1
∖λS = λ1 +2λ2 + …+ tλt
(36)
(1 - λ)S = λ0 + λ1 + …+ λt-1 - tλt ≤ λ0 + λ1 + …+ λt-1 ≤ -ɪ-	(37)
1-λ
Therefore we have the following regret bound
D∞2 √T	„1/2	dβ1D∞G∞	αC√1 + log T	Ii
≤ 2α(1-βι) ⅛% + 2α(1-βι)(1- λ)2 + LIK ⅛ 的中k2
(38)
A.5 Proof of Non-Convex Convergence
A.5.1 Proof of Theorem 4.2
Proof. We first directly refer to the original paper and obtain the following bound (Chen et al., 2019).
■ T
E X athVf (xt), Vf(Xt)/pti
t=1
≤E
TT
Ci X kat gt/Vvtk2 + C2 X ll√V=-
T-1
修 k1 + C3 X kj› - 1 k2
∕vt-1	M Vvt	√vt-1
+ C4
(39)
where C1,C2, C3 are constants independent of d and T, C4 is a constant independent of T. For the
first term, assume that min7-∈[d] (Vv1')j ≥ c > 0, we have
T
E C1 X kɑtgt/√Vtk2
t=1
T
≤ E C1 X kagt∕ck2
t=1
E 卜IX 鼻 kgtk2l ≤ C1G∞α2
t=1
(40)
where the first inequality follows from Lemma A.1 as V ≥ vt-21. The second inequality is from
αt	αt
the gradient bound kVf(xt)k ≤ G∞. For the second term with C2, similarly by the positive semi-
definiteness in Lemma A.1, we have
E
≤ E C2 X	≤ Cda
≤	2 j=1 (√v1)j ≤ C
dT
C2Xj=1Xt=2
The second equality is from the telescope sum and for the third term
(41)
19
Under review as a conference paper at ICLR 2020
E
T-1
CC X k√α∣ …
T-1
I2 ≤ E C3
—
αt-1
Ili
V C3d02
c2
(42)
where the first inequality is because | √α= 一 ：；-1 | ≤ √V-1 ≤ αc and the last one is due to the
previous inequality with second term. Hence in summary, we have
E [χx αthVf (Xj Vf(Xt)/pvti] ≤ C1 G∞α2 + C2dα + C3dα2 + C4	(43)
c2	c	c2
t=1
Now by the proof of Corollary 4.1, We have an upper bound (Vtj ≤ tG∞, therefore We can bound
the effective step sizes,
αt	、 α
(√Vt)j ≥ tG∞
(44)
And thus we have
一 T
E X athVf (xt), Vf(Xt)/pti
t=1
T
≥ E X TGj kVf(χt)k2
tG∞
α	T1
≥ G- min E[kVf(Xt)k2] X ~t
∞t∈[T]	t=1 t
≥ α- min E [kVf (xt)k2] log(1 + T)
G∞t∈[T]
(45)
The last inequality is due to the fact that PtT=1 1/t ≥ RtT=+11 1/tdt = log(1 + T), therefore we have
min E [∣Vf (xt)k2] ≤	；：	(CG^2 + C2dα + (Cda + C4)	(46)
t∈[T]	α log(1 + T)	c2	c	c2
We would emphasize that the assumption Ilatmt/√tk ≤ G in the theorem is automatically satis-
fied as √0‰ ≤ √1 = a. Hence IIatmt/√tk ≤ αG∞.
A.5.2 Proof of Corollary 4.2
Similar to the proof of Theorem 4.2, we still have
E [χ athVf (xt), Vf(Xt)/PVti] ≤ CIG泮 + Cda + Cda2 + C4	(47)
c2	c	c2
t=1
Note that (Vt)j has the following upper bound as ∣∣Vft(θ)k∞ ≤ G∞,
1t
vt,i =(1+ β2)t- 1 Σβ2(1 + β2)t-j gj,i
G2	t
≤ (W∞-1 X β2(1 + β2)t-j = G∞
And thus we have
(48)
20
Under review as a conference paper at ICLR 2020
一 T
E X at<Vf(xt), Vf(Xt)/pt)
t=1
TT
≥ E X √O-kvf(χt)k2 ≥ * * minE [kvf(χt)k2]X √
t=1 tG∞	G∞ t∈[T]	t=1 t
≥ 启 min E [kVf (xt)k2] √T
G∞ t∈[T]
(49)
where the last inequality is by the fact that PT=I √1t ≥ √T, therefore We have
min E [kvf (χt)k2] ≤ g∞( C⅛α2+胃+CdQ+。，)	⑤)
t∈[T]	α T c	c	c
A.6 Implementation of AdaX and AdaX-W
The detailed implementations of AdaX and AdaX-W are as follows. The performance of AdaX is
robust with respect to the value of β2, but we recommend smaller values such as 1e - 4, 1e - 5
to reduce computational cost. The value of β3 can be set as 0.9 to match the first moment design
of Adam, consequently we need step sizes. The size of weight decay should also be adjusted
respectively.
Algorithm 3 AdaX Algorithm with L2 Regularization and Weight Decay
1:	Input: x ∈ F, step size {αt}tT=1, (β1, β2, β3) = (0.9, 1e - 4, 0.999), weight decay λ = 5e - 4,
=1e-12
2:	Initialize m0 = 0, v0 = 0
3:	for t = 1 to T do
4:	gt = Vft(Xt) ]+λxt
5:	mt = βιmt-i + (1 — βs)gt
6:	vt =	(1 + β2 )vt-1 + β2gt2
7:	Vt =	vt∕[(1 + β2)t — l] and Vt	= diag(Vt)
8:	Xt+1	= ∏f,√Vt(Xt — αt(mt/(√Vt +	e)	+ λxt ))
9:	end for
A.7 Hyper-parameter Tuning in the Experiments
The hyperparameters in different algorithms have a huge impact on their performances in the ex-
periments. To efficiently find the best step sizes, we follow Wilson et al. (2017) to perform a
logarithmically-spaced grid search of the optimal step sizes and we list the step sizes we tried in
the following tables, where the step sizes in bold are the ones with best performances and used in
the experiments section.
Step size: Image Classification
• SGD(M) {10, 1, 1e-1, 1e-2, 1e-3}
• Adam/AdamW {1e-2, 3e-3, 1e-3, 3e-4, 1e-4}
• AdaGrad {5e-2, 1e-2, 5e-3, 1e-3, 5e-4}
• AMSGrad {1e-2, 3e-3, 1e-3, 3e-4, 1e-4}
• RMSProp {1e-2, 5e-3,1e-3, 5e-4, 1e-4}
• AdaX(ours) {5e-1, 2.5e-1, 2e-1, 1.5e-1, 1e-1, 5e-2 1e-2, 5e-3, 1e-3}
• AdaX-W(ours) {1,5e-1, 4e-1, 3e-1, 2.5e-1(CIFAR), 1e-1, 1e-2, 5e-3, 1e-3}
21
Under review as a conference paper at ICLR 2020
Step size: VOC2012 Segmentation
•	SGD(M) {1e-3, 5e-4, 2.5e-4, 1e-4, 5e-5}
•	AdamW {5e-4, 1e-5, 5e-5, 1e-6, 5e-7}
•	AMSGrad {5e-4, 1e-5, 5e-5, 1e-6, 5e-7}
•	RMSProp {1e-4, 5e-5, 1e-5, 5e-6, 1e-6}
•	AdaX-W(ours) {1e-2, 5e-3, 1e-3, 5e-4, 1e-4}
Step size: Billionwords
•	SGD {30, 20, 10, 5, 1, 1e-1, 1e-2}
•	Adam {5e-3, 2e-3, 1e-3, 5e-4, 1e-4 }
•	AdaX(ours) {5e-3, 2e-3, 1e-3, 5e-4, 1e-4} × {0.5, 1,5, 15, 25, 50, 100}
Momentum parameters. For the momentum parameters of Adam, RMSProp and AMSGrad,
we tuned over (β1, β2) = {(0.9, 0.999), (0.99, 0.999), (0.99, 0.9999)} and found that the default
values (0.9, 0.999) as in Kingma & Ba (2015) yield the best result. For the momentum param-
eters (β1, β2, β3) in AdaX, we directly applied β1 = 0.9 as in Adam and we tuned β2 over
{1e - 3, 1e - 4, 1e - 5}. We found that the value of β2 didn’t affect the general performance
of AdaX. Tuning β3 is the same as tunning the step sizes, as we will show in the next subsection.
For the other hyperparameters such as weight decay and batch size, we directly apply the same
settings as in the baselines (He et al., 2016)(Chen et al., 2016)(Rdspring1) (Loshchilov & Hutter,
2019).
A.8 THE EFFECT OF β3
As we mentioned in section 4 and 5, adding a new hyper-parameter β3 does not influence the first
momentum sum of AdaX as it is just a constant common factor that can be extracted out as follows.
t
mt = β1 mt-1 + (1 -β3)gt = (1 -β3)Xβ1t-igi	(51)
i=1
Therefore, tuning the hyperparameter β3 will have exactly the same effect as tuning the step size α.
As long as (1 - β3)α remains the same, the performance of AdaX doesn’t change. We also show
this fact empirically using different combinations of (α, β3) on the CIFAR-10 dataset.
1叫 ，	，一
—j⅝ =0.999,α=0.5
——βi =0.99,a=0.05
—j⅝=0∙9la =0.005
95
A3ejny τ4pu-£
β5
80O	l∞00	20000	30000	40000	50000	60∞0
Iterations
5 0 5
8∙7
A9e-nv3v,dpl⅞L
l∞00	20000	30000	4∞00	50000	6。OOO
Iterations
(a) Training Top-1 Accuracy	(b) Testing Top-1 Accuracy
Figure 5: Training and Testing Results on CIFAR-10 with Different combinations of (α, β3).
As shown above, we trained the ResNet-20 model (He et al., 2016) on the CIFAR-10 dataset with
different combinations of α and β3 from {(0.5, 0.999), (0.05, 0.99), (0.005, 0.9)}. However, the
22
Under review as a conference paper at ICLR 2020
value of (1 - β3)α is kept to be 5e-4. It can be easily observed that they have approximately the
same training and testing curves, as well as similar final performances, which proves our claim.
23