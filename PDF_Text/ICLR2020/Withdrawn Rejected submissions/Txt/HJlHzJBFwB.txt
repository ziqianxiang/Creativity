Under review as a conference paper at ICLR 2020
Accelerating Monte Carlo Bayesian Predic-
tion via Approximating Predictive Uncer-
tainty over the Simplex
Anonymous authors
Paper under double-blind review
Ab stract
Estimating the predictive uncertainty of a Bayesian learning model is critical in var-
ious decision-making problems, e.g., reinforcement learning, detecting adversarial
attack, self-driving car. As the model posterior is almost always intractable, most
efforts were made on finding an accurate approximation the true posterior. Even
though a decent estimation of the model posterior is obtained, another approxima-
tion is required to compute the predictive distribution over the desired output. A
common accurate solution is to use Monte Carlo (MC) integration. However, it
needs to maintain a large number of samples, evaluate the model repeatedly and
average multiple model outputs. In many real-world cases, this is computationally
prohibitive. In this work, assuming that the exact posterior or a decent approxi-
mation is obtained, we propose a generic framework to approximate the output
probability distribution induced by model posterior with a parameterized model
and in an amortized fashion. The aim is to approximate the true uncertainty of a
specific Bayesian model, meanwhile alleviating the heavy workload of MC inte-
gration at testing time. The proposed method is universally applicable to Bayesian
classification models that allow for posterior sampling. Theoretically, we show that
the idea of amortization incurs no additional costs on approximation performance.
Empirical results validate the strong practical performance of our approach.
1	Introduction
Bayesian inference is a principled method to estimate the uncertainty of probabilistic models. In most
applications, especially in deep learning, the likelihood model and model prior are not conjugate
hence marginalizing over model prior or posterior cannot be performed analytically, which hinders
the practical applicability. For tractability, a simple point estimate such as maximum a posteriori
(MAP) estimate could be used to approximate the full model posterior. The price paid is the loss of
model uncertainty due to incomplete characterization of the model posterior. Approximate inference
methods, such as Markov chain Monte Carlo and variational inference, enhance the approximate
posterior by a better probability distribution while keeping inference tractability. However, even
though a decent approximation of posterior can be obtained, computation of predictive distribution is
usually intractable due to loss of conjugacy, and is of high cost if tractable.
To introduce the problem, we consider a Bayesian classification model trained on dataset D =
{(yn, xn)}nN=1, where xn ∈ X and yn ∈ Y are the nth input and output, respectively. Let the model
posterior p(θ∣D) be approximated by MC estimate 1 P1 * * * S=I δ(θ - θs), and the predictive distribution
(a categorical distribution parameterized by the predicted class probabilities) is thus approximated by
1S
p(y∣χ, D)= p p(y∣χ, θ)p(θ∣D) dθ ≈ S £p(y|x, θs).
S s=1
The predictive distribution can be accurately estimated as S → ∞. However, to perform the
computation, we need to maintain a large number of samples, repeatedly evaluate the model for S
times and finally average the model outputs. This problem is critical in many real-world cases. For
example, assisted-driving car system requires an accurate measure of uncertainty to avoid making
mistakes with a high confidence (Kendall & Gal, 2017; Sunderhauf et al., 2018). Due to the limited
computational resources and storage in such system, it’s hard to maintain a large number of samples
and perform S times evaluation of the Bayes model for the real-time image data.
1
Under review as a conference paper at ICLR 2020
In this work, aiming at boosting the prediction speed while maintaining a rich characterization of the
prediction, we propose to approximate the distribution of class probabilities over the simplex induced
by the model posterior p(θ∣D), in an amortized fashion. This naturally diverts the heavy-load MC
integration process from testing period to approximation period. Different from the previous work in
Bayesian knowledge distillation (Balan et al., 2015; Bulo et al., 2016) that only focuses on the output
categorical distribution (a point on simplex), the induced distribution over simplex provides: 1) rich
knowledge includes prediction confidence for identifying out-of-domain (OOD) data (see empirical
examples in Fig. 2); 2) the possibility to use more expressive distributions as the approximate model.
We term the Bayes classifier as “Bayes teacher” and the approximate distribution as “student”, due to
the analogy with teacher-student learning. A Dirichlet distribution is used as the student due to its
expressiveness, conjugacy to categorical distribution and its efficient reparameterization for training.
We propose to explicitly disentangle the parameters of the student into a prediction model (PM) and
concentration model (CM), which capture class probability and sharpness of Dirichlet respectively.
The CM output can directly be used as a measure for detecting OOD data. We term our approximation
method as One-Pass Uncertainty (OPU) as it simplifies real-world evaluation of Bayesian models
by computing the predictive distribution with only one model evaluation. Note that, OPU allows
choosing various types of student model (e.g., compressed neural network (Lin et al., 2017; Hubara
et al., 2016; Han et al., 2015)) for further speedup on specific platforms with no extra design efforts.
As the amortized approximation of induced distributions is unexplored in the literature, we consider
and compare several choices of probability distance measure: forward KL, earth-mover’s distance
(EMD) and maximum mean discrepancy (MMD). We theoretically analyze the performance gap
incurred by the amortized approximation and show that, under MMD, besides model loss due to
restriction of student distribution, the amortized approximation does not introduce additional loss.
Empirical evaluations show a significant speedup (〜500 ×) ofBayes models. The results on Bayes
NN show that OPU performs better in misclassification detection and OOD detection than state-of-
the-art works in Bayesian knowledge distillation. It can also be observed that explicit disentangling
of mean and concentration helps improve the performance. The comparisons of different probability
measures validate the theoretical analysis. We also conduct empirical evaluations and comparisons
on Bayes logistic regression and Gaussian process, to show OPU is universally applicable to all
Bayesian classification models.
2	One-Pass Uncertainty Framework
2.1	Induced Distribution over S implex
In this section we present our OPU framework for a generic Bayesian parametric classifier, e.g.
Bayesian logistic regression (BLR) or Bayesian neural networks (NN). Let the Bayesian classifier be
specified with categorical likelihood and a parametric function, i.e.,
p(y|x, θ) = Cat(y|T (x; θ))
(1)
and a prior distribution p(θ) be specified over the parameter space Θ, where T : X × Θ → SK-1 is
a parametric function from input space to simplex, e.g., a neural network with softmax output layer,
and K is the number of classes. In this paper, we assume the posterior p(θ∣D) is obtained and focus
on the computation of the predictive distribution. In what follows, let p(θ∣D) be approximate or
exact (if available) model posterior, from which samples could be obtained.
From a dual perspective, with a fixed input x, the mapping Tx = T(x; ∙) is also understood as a
data-dependent mapping Tx : Θ → S K-1, which could be used to transform the posterior p(θ∣D)
to a conditional distribution (on x) over the simplex. That is, for each x, we can define a random
variable π = Tx(θ) whose distribution is induced by θ 〜p(θ∣D) and Tx. This is effectively a well-
defined push-forward measure TX#p(0|D) over the simplex (see Fig. 1). We term the conditional
distribution of π∣x, D as an “induced distribution” and define px(∏) = p(π∣x, D) to keep the
notation uncluttered.
The induced distribution isolates the dependence between input and output and simplifies the compu-
tation of predictive distribution due to the change-of-variable formula. Specifically, given a test point
x*, the predictive distribution can be alternatively written as
p(y∣χ*,D)
Cat(y∣TX*(θ))p(θ∣D)dθ
/
SK-1
Cat(y∣π)px* (π) dπ.
(2)
Θ
2
Under review as a conference paper at ICLR 2020
Student----------------
Prediction
X→→	，
Concentration
Approximation ∣
-►q(n|x)
Categorical h
Class probability
Scalar g
Confidence
Figure 2: An empirical example of MC estimate of
the induced distribution over simplex. The classi-
fier (MCDP) is trained on real-world digits images
of {0,1, 2} (corresponding to left, right, top cor-
ners of simplex). The 1st and 3rd rows indicate
the input while the 2nd and 4th rows indicate MC
estimate of π over a 2-simplex.
Figure 1: An intuitive example of the proposed framework.
Only one input X is consider in this example. “Margin.”
indicates marginalization over π.
Our key insight is that px contains all information we need for prediction and uncertainty measurement.
Hence π is sufficient in the sense that, given π, y is independent of both θ and x. The isolation
combines the complexities in both the likelihood and posterior into a single object px , and keeps a
simple dependence structure between y and π . Also, the isolation renders the last probabilistic layer
y∣π nuisance, which can thus be peeled off for prediction. To validate the idea, one can show that the
predictive distribution is simply p(y|x, D) = Cat(y|Epx π). The difference between probabilistic
structures of original Bayesian model and the isolated version is showed in Fig. 3, Appendix.
Although the density function px is sometimes hard to compute, especially for complicated Tx (like
NN), its samples can be obtained via 1) sampling the posterior, i.e. θs 〜p(θ∣D); 2) “push-forward”
the samples by Tx, i.e. πs = Tx(θs). The behavior ofπ can be empirically observed via the particles
{πs}sS=1. As shown in Fig. 2, π is able to distinguish different types of input x based on the behavior
of its samples. In Fig. 2(a), the inputs are similar to the training data. The particles gather around
a corner of the simplex, indicating a confident prediction. In Fig. 2(b), the inputs are digit images
but are relatively hard to predict. Therefore, the particles gathers around the center, indicating the
model is certain that the input is on decision boundary. In Fig. 2(c), the inputs are images outside the
domain of training data, the particles spread over the simplex, which means the model has a high
uncertainty about input, indicating out-of-domain (OOD). In Fig. 2(d), the particles spread along a
line between two corners, indicating the model is confident that the result is not at the other corner.
However, using particles for inference is time-consuming as each particle requires one evaluation of
the model. This motivates us to use a tractable conditional distribution q(∏∣x) to approximate pχ.
2.2	Amortized Approximation
The view of Px enables flexible choices of q(∏∣x), as any distribution defined on RK can be
transformed to S K-1 via logistic transformation (Aitchison, 1982). However, modeling q(∏∣x)
locally for every input is not practical, as the design efforts and number of parameters grows linearly
with the number of data points. Therefore, we propose to approximate px in two aspects: 1)
use a single family of distribution q; 2) to generalize to unseen examples, let the parameter of
q, αx = α(x; φ), be a function depending on x and parameterized by a set of global adaptive
parameters φ, and thus qx = q(∏∣x) = q(π∣αx). The computational cost is amortized by casting
the problem of learning a series of conditional distributions to a regression problem.
We term px as “teacher distribution” and qx as “student”. In our method, as seen in graphical
representation in Fig. 3, Appendix, by proper approximation, the stochasticity and knowledge in
node θ of the teacher is transferred into node π of the student model, such that the full predictive
uncertainty is maintained. In terms of computation, the approximation requires sampling only in the
training stage. While in the testing stage, to obtain the predictive distribution, only one evaluation is
required. Thus, we denote the framework as the One-Pass Uncertainty (OPU) model.
The above benefits do not introduce any concession on generalizability. Since OPU is based on
approximating the distribution of the output class probabilities which is common for all classifiers,
3
Under review as a conference paper at ICLR 2020
the amortized approximation can be applied to any Bayesian classifier. Note that the approximation
framework can be extended to non-parametric model like Gaussian process, where the computational
cost of inference is high 1.
In this work, We choose the student model q to be a Dirichlet distribution, q(π∣ɑχ) = Dir(π∣ɑχ),
where αx is a function mapping input x to a Dirichlet parameter. The reasons for choosing Dirichlet
is the tractability: the Dirichlet is the conjugate prior to the Categorical, and thus enables tractable
integration of (2) given the parameters. To better disentangle the uncertainty measures, we use the
design αχ = hχ ∙ egx, where hχ = h(x; φι) and gχ = g(x; φ2) are two neural networks, and the
vector output h sums to 1. Vector output h determines the mean of the Dirichlet (i.e., the predicted
class probabilities), andg determines the concentration of the Dirichlet (i.e., the prediction confidence).
To see this, the posterior of the class labels is the Dirichlet mean, p(n` = 1|x, φ) = / Cat(yg =
1∣π)Dir(π∣ɑχ)dπ = Pax,,'	= hχ,' where y? and αχ,' are the '-th coordinate of y and αχ
c αx,c
respectively. Therefore, we call hx as the “prediction model” (PM). Similarly, the precision parameter
α0 (determines sharpness) of the Dirichlet solely depends on gx, α0 = Pc αx,c = Pc hx,cegx = egx.
Therefore, we term gx as the “concentration model” (CM). Based on this property, whether the
Dirichlet is flat or not, can be fully characterized by CM. It can be expected that, when approximating
particles in Fig. 2(a) and (b), the output value of CM is high, as the samples are concentrated, which
means high confidence. CM outputs a low value for particles in Fig. 2(c), yielding a flat distribution
and low confidence.
2.3	Learning
With a probability distance or divergence, we define the approximation loss L(x, α) = ρ(px, qx)
The student model is trained to minimize the aggregated objective
αmin Ep(x)L(x, α)	(3)
where F is some hypothesis space and p(x) is some distribution over X. In practice we take p(x) =
pD0 (x) with D0 = {xm}mM=1 a dataset containing features only. As the amortized approximation of
induced distributions in Bayesian classifiers is unexplored in the literature, we consider and compare
several choices of ρ including KL divergence, earth mover’s distance (EMD) and maximum mean
discrepancy (MMD). The corresponding derivation of the training objectives and training algorithms
are in the appendix.
Forward KL divergence Minimizing aggregated reverse KL divergence is not tractable in our
scenario as the density function px is not available in general. This difficulty is avoided by using
forward KL, in which the intractable density function is only involved in the irrelevant entropy
term. It is equivalent to using cross-entropy as a local loss, i.e. L(x, α) = -Epx ln qx. By
plugging in a particle estimation Px = S PS=I δ(π -Tx(θs)), the training objective becomes
mina -EpD0(x)S1 Ps lnqx(TX(θs)), which is equivalent to an “amortized” MLE problem with
particles providing the estimation of sufficient statistics of qx . Due to the zero-avoiding nature,
forward KL tends to over-estimate the support of px . This leads to under-confidence approximation
(“flat” approximate distribution) and hence might deteriorate the quality of uncertainty measurements.
This is expected to be more serious when px is multi-modal.
EMD It is known that EMD provides much weaker topology than other probability distance
measures Peyre et al. (2019). In the application where data is supported on strictly lower-dimensional
manifolds, EMD provides more stable gradient than KL divergence (Arjovsky et al., 2017; Tolstikhin
et al., 2017). An example of particles on low-dimensional manifold is shown in Fig. 2(d).
Specifically, the KR dual representation of EMD (Villani, 2008) in our problem is given by
Wι(px,qx) = suPkψkL≤ι Eθ〜p(θ∣D)[Ψ(TX(θ))] - En〜qx[ψ(∏)], where ∣∣ ∙ ∣∣l denotes the Lips-
chitz semi-norm and ψ is known as the critic (or the discriminator (Arjovsky et al., 2017)). As the
induced distribution is conditioned on x, a local critic ψx should be defined for each x. Following
Arjovsky et al. (2017), intractable supremum is solved by parameterizing ψx = ψ(∙; wx). To avoid
training |D0 | local critics, we propose to let w be the global weight, and let ψ depends on x, i.e.,
ψx = ψ(π; w, x) (see Fig. 4, Appendix). The final aggregated training objective becomes,
nun max EpD,(χ) [Ep(θ∣D) [Ψ(Tx(θ); w, x)] - Eqx[Ψ(∏; w, x)]] + λR(w),	(4)
1The details of extracting samples from various Bayes parametric models and non-parametric model (GP
classifier) are summarized in the appendix.
4
Under review as a conference paper at ICLR 2020
where w is the introduced global parameter for ψ, R(w) is the imposed gradient penalty (Gulrajani
et al., 2017) over w to enforce the Lipschitz constraint. Solving the minimax problem requires the
supremum to be attained for each x under the Lipschitz constraint. Specifically, in every optimization
step, ψ is trained to generate a critic for each x that matches the exact EMD. Practically, this needs a
high-capacity critic and the required capacity increases with the number of classes K .
MMD Let Hk be a reproducing kernel Hilbert space (RKHS) defined by a positive-definite kernel
k, the MMD between px and qx can be written as
MMDk(p,q)=	SUp	Ep(θ∣D)[Ψ(TX(θ))]- Eqχ[ψ(∏)]	(5)
ψ∈Hk,kψkH≤1
Compared with EMD, the advantage of MMD is that there is no need to train an NN as the critic that
maximizes Eq. 5. With kernel trick, MMD can be readily estimated in closed-form with its empirical
version under finite sample (Sec. C).
Compared with KL divergence, MMD is a valid statistical metric. Due to the symmetry property, the
approximation is expected to be neither mean-seeking nor mode-seeking. Therefore, MMD is not
expected to have an under-confidence issue.
Reparameterization Note that optimization under both EMD and MMD requires gradient of the
expectation of critic via sampling from q, which contains parameters. To obtain efficient gradient
estimator and reduce variance, we use the reparameterization trick (specifically, implicit reparameter-
ization trick). For details, see Sec. C in Appendix.
2.4	Amortization Gap
To better understand nature of the proposed approximation, we consider the “unamortized” version
of the approximation as an intermediate stage, which involves fitting separate approximations to each
px . To demonstrate the idea, we leverage to MMD due to its nice property. For fixed px , the optimal
point-wise approximation within family Q is defined as
qX = argminMMDk (q,Px).	(6)
q∈Q
Then we have the following lemma:
Lemma 1. Let P(SK-1) be the space of probability measures over the simplex, equipped with MMD
metric defined by a universal kernel. If T satisfies Assumption 1, then the map x 7→ px is continuous.
Further, if Q ⊆ P (SK-I) is a closed convex model space, the projection qχ is unique and the map
x → qX is also continuous.
Further, if We assume the model space is parameterized and identifiable in MMD, i.e. Q = {q(π∣α):
α ∈ A} and MMDk(q(∏∣ɑ), q(π∣ɑ0)) = 0 if and only if ∣∣α - ɑ0∣∣ = 0, we may obtain continuity
in parameter space. The continuity of optimal parameters implies there exists a continuous function
α* : X ∈ X → α*(x) ∈ A, which serves as the essential target of our amortized goal.
To analyze how amortization affects the approximation, we define the local amortization gap as
∆(x) = MMDk(qx,Px) - MMDk(qX,Px).	(7)
Then it holds that
0 ≤ ∆(x) ≤ MMDk(qx,qx),	(8)
where the lower bound is because qx is the projection and the upper bound is due to triangle inequality.
Then our goal, minimizing aggregated MMD loss Ep(x)MMDk(qx,px), is essentially equivalent to
minimizing aggregated amortization gap Ep(x)∆(x), up to an irrelevant additive constant.
One can see that α* is the global minimizer of aggregated amortization gap and hence the global
minimizer of the aggregated MMD loss. Intuitively, if F is of enough capacity, then the infimum (over
F) of aggregated amortization gap could be 0, i.e. we can push the amortization gap arbitrarily small.
If the optimal solution is covered by hypothesis space, no additional cost is introduced by amortizing
the approximation. In other words, model loss due to using restrictive Q will dominate. Further, if
the global optimum is reached, OPU approximation exactly matches the point-wise minimizer, i.e.
MMDk (qx, qx) = 0, due to uniqueness of projection.
5
Under review as a conference paper at ICLR 2020
3	Related Work
In this section, related works are reviewed and compared with OPU in terms of methodology. In
CompactApprox (Snelson & Ghahramani, 2005), a parametric model composed of a small subset
of “best samples” selected from the original MC samples is used to approximate the full predictive
Categorical distribution. Extending the approximation to Bayes NN, BDK (Balan et al., 2015)
proposes to use NN to approximate the predictive Categorical distribution of a Bayes NN trained
by stochastic gradient Langevin dynamics (SGLD). Specifically, the teacher network generates
samples via SGLD and KL between the two distributions is minimized in an online fashion. However,
the disadvantage is that data uncertainty, model uncertainty and distributional uncertainty are all
entangled in the class probabilities because categorical distributions are of limited expressiveness.
Different from these previous works that only approximate the class probabilities, OPU approximates
the induced distribution of class probabilities, which contains richer information including both class
probabilities and prediction confidence (e.g., the three types of uncertainty observable via the samples
in Fig. 2.) We choose a Dirichlet distribution as the student model, and explicitly disentangle the mean
and concentration to fully capture the thee types of uncertainty. We also explore other probability
distance measures (EMD and MMD), showing that KL yields degenerate prediction performance.
Using a Dirichlet to estimate uncertainty has also been explored by Deep Prior Network (DPN)
(Malinin & Gales, 2018), where a parameterized Dirichlet is used in a Bayesian model to characterize
the “distributional uncertainty”, i.e., to tell if the data is in the training domain or not. However,
DPN adds a stochastic layer in the Bayes model, rather than approximating a well-trained Bayes
teacher. Due to the intractable inference, DPN uses an MAP estimate of the model posterior which
incurs a loss of uncertainty. To compensate for the lost characterization of uncertainty, DPN uses a
hand-crafted training goal that explicitly requires OOD examples (which are typically unavailable in
real-world applications). In contrast to DPN, our OPU is able to: 1) extract predictive uncertainty
from any Bayesian classification model according to the practical requirements; 2) choose various
types for student model (e.g., quantized neural network) to enable fast prediction; 3) use only in-
domain data in training to get a good uncertainty measure (see Sec. 4). Note that none of these
properties can be achieved by DPN.
4	Experiments
4.1	Experimental Setup
Models and Tasks In this section, we present the experimental results on using Bayesian NN
(BNN).2 For each model, we choose a few Bayesian methods as teachers, and approximate them
with our OPU. We also compare with state-of-the-art approximations that are proposed for the
specific types of methods. For BNN, we use MCDP (Gal & Ghahramani, 2016) and SGLD (Welling
& Teh, 2011) as the teacher, and BDK and DPN for comparisons. The methods for obtaining
posterior samples from the Bayes teachers are in Appendix B. For each type of model, in-domain
misclassification (MisC) detection, out-of-domain (OOD) input detection, prediction performance
and prediction time are presented.
Baselines and Uncertainty Measures For the uncertainty measures of teachers, we take MCDP
and KL as a example. For each testing data x, a Dirichlet is fit on particles under KL, to get an optimal
qχ, whose differential entropy (D) is an uncertainty measure. For the uncertainty in prediction, to
avoid the model loss, entropy (E) and maximum probability (P) of the averaged particle are directly
adopted as uncertainty measures. This gives MCDP-KL model as a baseline. The other baselines
MCDP-EMD and MCDP-MMD are also obtained similarly. Note that they share the same E and P as
the sample mean is the same. The D of qX is expected to be the best uncertainty measure that OPU
can approach theoretically (when the amortization loss is zero, see Sec. 2.4). For fairness, the same
set of posterior particles is used for obtaining D. The baselines for SGLD are obtained in a similar
way. Students use categorical entropy (for BDK) or D (for DPN) and P of the output distributions.
For OPU, we consider E and P of the prediction model, and the scalar output of the concentration
model (C) as the measures.
Data and Evaluation Metrics For a fair comparison, we let D0 = D in the approximation of OPU.
The in-domain dataset is split to training data and testing data, i.e., Din = D0 = {Dtr , Dte}, which is
used for training models, evaluating prediction and MisC detection. The OOD dataset Dood and Dte
are used for OOD detection. To assess the performance, we use accuracy, time, Area under the ROC
2The setup and results on BLR and GP are available in the appendix.
6
Under review as a conference paper at ICLR 2020
Table 1: Results on BNN - MNIST. The doubt quote means “same as above”.
Model	MisC detection		Omniglot		SEMEION		Acc. (%)	Test time(s)
	AUROC	AUPR	AUROC	AUPR	AUROC	AUPR		
MCDP-KL	97.3(E)	43.0 (E)	99.4 (D)	99.7 (D)	86.8(E)	53.8 (P)	97.9	210.6
MCDP-EMD	97.3 (E)	43.0 (E)	99.6 (D)	99.9 (D)	86.8(E)	53.8 (P)	97.9	"
MCDP-MMD	97.3 (E)	43.0 (E)	99.7 (D)	99.9 (D)	90.1 (D)	71.2 (D)	97.9	"
OPU-MCDP-KL	94.2 (E)	37.7 (E)	100 (C)	77.0 (C)	91.4 (C)	67.3 (C)	96.2	0.443
OPU-MCDP-EMD	95.3 (P)	43.7 (P)	100 (C)	100 (C)	93.3 (C)	82.5 (C)	96.1	"
OPU-MCDP-MMD	97.2 (P)	40.9 (P)	100 (C)	100 (C)	99.8 (C)	98.6 (C)	97.9	"
SGLD-KL	97.9 (P)	46.2 (E)	99.2 (E)	99.6 (E)	89.3(E)	46.8 (E)	^984^	233.5
SGLD-EMD	97.9 (E)	46.2 (E)	99.4 (D)	99.7 (D)	89.9 (D)	47.1 (D)	98.4	"
SGLD-MMD	97.9 (E)	46.2 (E)	99.2 (E)	99.6 (E)	89.3 (E)	46.8 (E)	98.4	"
OPU-SGLD-KL	94.2 (E)	46.7 (E)	100 (C)	100 (C)	99.5 (C)	98.4 (C)	98.2	0.443
OPU-SGLD-EMD	93.7 (P)	44.4 (E)	100 (C)	100 (C)	98.9 (C)	96.2 (C)	98.0	"
OPU-SGLD-MMD	97.2 (P)	44.6 (E)	100 (C)	100 (C)	99.1 (C)	98.0 (C)	98.1	"
BDK-SGLD	85.9 (E)	46.6 (E)	46.1(E)	41.7 (E)	35.3(P)	46.5 (P)	^92ΓΓ	0.441
BDK-MCDP	86.9 (E)	41.1 (E)	47.5 (P)	44.1 (P)	43.3 (P)	47.2 (P)	92.4	"
BDK-DIR-SGLD	89.9 (E)	40.0 (E)	95.4 (E)	96.4 (E)	74.7 (E)	38.3 (E)	94.1	"
DPN	99.0 (E)	43.6 (E)	100 (E)	100 (E)	99.7 (E)	98.6 (E)	~994~	-
Table 2: Results on BNN - Balanced EMNIST.
Model	MisC detection		Omniglot		Acc. (%)
	AUROC	AUPR	AUROC	AUPR	
MCDP-KL	89.7 (P)	46.8 (P)	99.7 (E)	99.7 (E)	88.8
MCDP-MMD	89.7 (P)	46.8 (P)	99.9 (D)	99.9 (D)	88.8
OPU-MCDP-KL	84.8(P)	40.6 (P)	96.2 (E)/67.5(C)	96.5 (E) /63.7 (C)	87.9
OPU-MCDP-MMD	89.8 (P)	49.6 (P)	100.0 (C)	100 (C)	88.4
(AUROC) and PR (AUPR), following the baseline in (Hendrycks & Gimpel, 2017) (The detailed
calculation process is shown appendix D.3). Time is evaluated on the whole Dte. To save space, we
only present the best performing uncertainty measure (E, P or C) for each task and method. (Full
results on undistilled MCDP/SGLD are shown in appendix D.3.) We use the MXNet implementation
of BDK and GPflow implementation of GP, and the remaining models are implemented in Pytorch.
All experiments run on a desktop with an i7-8700 CPU and an RTX-2080 Ti GPU. The experiments
for Bayesian logistic regression follows the same setup as Gaussian process.
4.2	Bayesian Neural Network
The experiments for Bayesian neural network use MNIST and balanced EMNIST datasets as Din,
and use Omniglot and SEMEION dataset as Dood, as in Malinin & Gales (2018). Given MCDP
and SGLD as teacher models, the baselines are obtained as illustrated in Sec. 4.1. The rest models
are: OPU approximating MCDP (OPU-MCDP), OPU-SGLD, BDK-SGLD, BDK-Dir-SGLD and
DPN. For BDK-Dir-SGLD, we replace the Categorical distribution in BDK by a Dirichlet without
disentangling the mean and concentration, then train it with the same MC ensemble as OPU-SGLD.
This is to show the benefits of explicit disentanglement.
The NN architecture used by these models is an MLP with size 784-400-400-10, ReLU activations,
and softmax outputs, following Balan et al. (2015). For CM, we use an MLP with size 784-400-400-
1. MCDP is trained by SGD with hyper-parameters: dropout-rate of 0.5, learning rate 5 × 10-4,
mini-batch size of 256, number of iterations 103. The parameters of critic are shown in Appendix.
For MMD, we use a summation of RBF kernel and polynomial kernel. OPU-MCDP is trained
by Adam with hyper-parameters: number of iterations 100, learning rate for student 10-3. The
training of SGLD and BDK follows Balan et al. (2015). Then OPU-SGLD is trained with the same
hyperparameters as OPU-MCDP. Results of DPN are from Malinin & Gales (2018). The results are
presented in Table 1.
Computation time.OPU offers a 〜500x speedup compared to the original MCDP/SGLD, as OPU
only evaluate the model twice (PM and CM in the student network) while MCDP/SGLD evaluates
for S times. This confirms our idea of accelerating Bayesian prediction by diverting the sampling
process from the test period to the approximation period. Note that the time cost of MCDP/SGLD
increases with more posterior samples involved. BDK is slightly faster than OPU because it runs
one network while OPU runs both PM and CM. Experimental results on original MCDP/SGLD with
fewer samples are given in appendix D.3 to show performance degradation with fewer samples.
7
Under review as a conference paper at ICLR 2020
OPU vs BDK. In some tasks especially OOD detection, the measure of concentration outperforms
the baseline. This is because the explicit disentanglement of mean and concentration helps “targeted”
knowledge distillation, as shown in Sec. 2.2. By comparing OPU-SGLD-KL and BDK (trained by
forward KL), we observe that OPU-SGLD-KL is significantly better in OOD detection tasks and
AUROC in MisC detection. BDK shows a slight advantage in AUPR in MisC detection task. This is
because the knowledge distillation only happens between two categorical variables in BDK, which
only helps capturing prediction information. In contrast, OPU framework first extracts all information
in a BNN with the induced distribution, then transfers the knowledge to a more expressive distribution
with a small loss guaranteed (Sec. 2.4). Adding a Dirichlet distribution to BDK (BDK-DIR-SGLD)
helps improving the performance in OOD detection. However, on SEMEION, which is expected to
be harder as it is more similar to MNIST, there is a large performance difference from OPU. This
further validates the necessity of explicit disentanglement of mean and concentration.
OPU vs DPN. Our OPU model (without OOD data in training) has comparable performance to DPN
(which uses a hand-crafted goal and OOD data in training). Another reason that DPN performs
slightly better is that DPN uses VGG-6 (4 Convolutional layer and 1 FC layer), which is a much
stronger model than the 2-layer MLP model that other models use.
KL vs EMD vs MMD. With MCDP, MCDP-MMD gives the best performance of differential entropy
baseline (D). This is because the samples of MCDP are relatively spread out over the simplex and
might be multi-modal. The probability distance is fully captured by MMD under such case. EMD is
expected to perform well as there are a lot of samples πs residing on a low-dimensional manifold.
However, the performance seems to be degenerated due to the limited capacity of the hyper-network
and the difficulty to train the minimax problem. KL presents the worst performance as expected
because it is likely to be under confident with samples of MCDP. With SGLD, the performance of KL
is the best except for AUROC of MisC detection. This is because the samples πs of SGLD over the
simplex are much denser and are typically unimodal.
KL vs MMD (EMNIST). The experiments are conducted on EMNIST whose number of classes is
large. We choose Omniglot as the OOD detection dataset as it is also contains handwritten characters,
which are harder than SEMEION for a model trained on EMNIST. A CNN with structure similar
with LeNet is used for MCDP and OPU (20 and 50 output channels in two convolutional layers). The
baseline approach achieves a classification accuracy of 88.8%. The samples of MCDP Bayes NN are
expected to be even more dispersive and multi-modal than MCDP trained with MNIST. OPU trained
with EMD failed to converge, possibly because the capacity of the hypernetwork was not enough.
Therefore, we do not recommend to use EMD for amortized approximation of predictive uncertainty,
unless a more scalable estimator of EMD can be provided. As shown in Table 2, the performance gap
between OPU-MCDP-KL and the baseline is larger because the multi-modality is severe - the entropy
of sample mean is a better measure for OOD detection than the concentration. This further validates
that OPU trained by KL suffers from an under-confidence issue. Specifically, KL forces OPU to
cover the support of all samples, making the student distribution more dispersive. This inaccurate
estimate of concentration affects the estimation of prediction results (mean) in turn, thus the accuracy
is lower and MisC detection performance is degenerated. By contrast, MMD consistently provides an
approximation that has similar performance with the teacher, that echoes the analysis in Sec. 2.4.
Amortization Gap. The details of experiments on the approximation error/gap can be found in
appendix D.3. It is shown that the amortization gap of using OPU is small, indicating the function
space is enough to cover the essential target. It also shows the approximation error is mainly from the
model error of using Dirichlet distribution, which is small as well, see appendix D.3 for details.
5	Discussion
The idea of “transferring” the randomness from model posterior to a simple-structure distribution at
the output can be generalized to other problems where a real-time evaluation of uncertainty is critical,
e.g., object segmentation. This allows interesting designs of structured output distributions.
The choice of student distribution for OPU is flexible. Although we verify that Dirichlet is suitable for
the tested classification tasks, there are still chances that Dirichlet incurs great model error. Note that,
in our framework, using a Dirichlet for the student is modeling choice, similar to assuming Gaussian
posteriors for variational approximations. The OPU framework is general and any student distribution
can be adopted, e.g., generalized Dirichlet, mixture of Dirichlets. To do this requires: 1) a suitable
parametrization of the model that can capture uncertainty; 2) deriving/computing the approximation
loss (KL, MMD, EMD); 3) reparameterization trick of expectations for efficient gradient estimation.
The algorithms of KL, EMD and MMD as well as the analysis still apply.
8
Under review as a conference paper at ICLR 2020
References
John Aitchison. The statistical analysis of compositional data. Journal of the Royal Statistical Society:
SeriesB(MethodoIogical), 44(2):139-160,1982.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. arXiv preprint
arXiv:1701.07875, 2017.
Anoop Korattikara Balan, Vivek Rathod, Kevin P Murphy, and Max Welling. Bayesian dark
knowledge. In Advances in Neural Information Processing Systems, pp. 3438-3446, 2015.
Samuel Rota Bulb, Lorenzo Porzi, and Peter Kontschieder. Dropout distillation. In International
Conference on Machine Learning, pp. 99-107, 2016.
Mikhail Figurnov, Shakir Mohamed, and Andriy Mnih. Implicit reparameterization gradients. In
Advances in Neural Information Processing Systems, pp. 441-452, 2018.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model
uncertainty in deep learning. In International conference on machine learning, pp. 1050-1059,
2016.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of Wasserstein GANs. In Advances in Neural Information Processing Systems,
pp. 5767-5777, 2017.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution ex-
amples in neural networks. Proceedings of International Conference on Learning Representations,
2017.
James Hensman, Alexander G Matthews, Maurizio Filippone, and Zoubin Ghahramani. MCMC for
variationally sparse Gaussian processes. In Advances in Neural Information Processing Systems,
pp. 1648-1656, 2015a.
James Hensman, Alexander G de G Matthews, and Zoubin Ghahramani. Scalable variational gaussian
process classification. In Proceedings of the Eighteenth International Conference on Artificial
Intelligence and Statistics, 2015b.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks. In Advances in neural information processing systems, pp. 4107-4115, 2016.
Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer
vision? In Advances in neural information processing systems, pp. 5574-5584, 2017.
Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network. In
Advances in Neural Information Processing Systems, pp. 345-353, 2017.
Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. In S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in
Neural Information Processing Systems 31, pp. 7047-7058. Curran Associates, Inc., 2018.
Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport. Foundations and TrendsR in
Machine Learning, 11(5-6):355-607, 2019.
Nicholas G Polson, James G Scott, and Jesse Windle. Bayesian inference for logistic models
using Pdlya-Gamma latent variables. Journal ofthe American statistical Association, 108(504):
1339-1349, 2013.
Edward Snelson and Zoubin Ghahramani. Compact approximations to Bayesian predictive distri-
butions. In Proceedings of the 22nd international conference on Machine learning, pp. 840-847.
ACM, 2005.
9
Under review as a conference paper at ICLR 2020
Niko Sunderhauf, Oliver Brock, Walter Scheirer, Raia HadselL Dieter Fox, Jurgen Leitner, Ben
Upcroft, Pieter Abbeel, Wolfram Burgard, Michael Milford, et al. The limits and potentials of deep
learning for robotics. The International Journal ofRobotics Research, 37(4-5):405-420, 2018.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders.
arXiv preprint arXiv:1711.01558, 2017.
CedriC Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient Langevin dynamics. In
Proceedings of the 28nd international conference on Machine learning, pp. 681-688. ACM, 2011.
A	Appendix: Proof
Assumption 1. Let T : X × Θ → Y be a map between finite dimensional vector spaces. We say T
satisfies Assumption1for distribution P if T(∙; θ) is Lipschitz and the Lipschitz constant L§ satisfies
Eθ〜pLθ < +∞.
Proof. We simply show both maps are Lipschitz continuous with MMD metric on P(SK-1). Let
x, x0 ∈ X and ψ ∈ Hk such that kψkHk ≤ 1. For x 7→ px , the “push-forward” definition of px leads
to
MMDk(Px,Pχ0) = sup Ep(θ∣D)Ψ(TX(θ)) - Ep(θ∣D)Ψ(TXo(θ))
kψk≤1
≤ sup Ep(θ∣D)∣Ψ(TX(θ)) - Ψ(Tχ0 (θ))∣
kψk≤1
≤ sup ∣∣Ψ∣∣Ep(θ∣D)dk(兀(θ), Tx (θ))
kψk≤1
≤ C Ep(θ∣D)kTX (θ) -Tχ0 (θ)k
≤ CEp(θ∣D)Lθkx - x0k,
where dk = is a kernel-based distance. The constant C emits when bounding dk with Euclidean
norm and the existence of such a constant is due to topology equivalence in finite-dimensional space.
For X → 葭 we notice that qχ is the projection of Px onto Q (effectively the projection of kernel
mean embeddings in Hk). By projection theorem in Hilbert space, the projection map Px → qx is
non-expansive, i.e.
MMDk(qx,qxo) ≤ MMDk(px,Pxo),
which leads to Lipschitz property of x → qx.	□
B Appendix: Plots.
The graph representations of original view , isolated view of Bayes teacher and the student are shown
in Fig. 3.
C Appendix: algorithms.
For presenting the algorithms, we slightly change the notation. We let α(x) = α(x, φ), h(x) =
h(x, φ1), g(x) = g(x, φ2), where φ = {φ1, φ2}, φ1 and φ2 are the parameters of prediction model
h and concentration model g, respectively.
KL. With the training objective
mφn	-EpDo(x)S Xlnq(T(x; θs)∣x, α),	(9)
s
10
Under review as a conference paper at ICLR 2020
Figure 3: Graphical representation of probabilistic structure of the Bayes teacher (left and middle) and the
student (right). Dashed edges denote deterministic dependence, box nodes are deterministic and circle nodes are
stochastic. The left and middle graphs correspond to the LHS and RHS of (2), respectively.
(C)
The student model is updated by doing φ1+1 := φ1 - γt(-1 P,θs Vφι ln q(T(x*; θs)∣α(x*; φι)))
and φ2+1 := φ2 - Yt( — S Pa, Vψ2 ln q(T(x*; θs)∣α(x*; φ2))) alternately, where Yt is the learning
rate at iteration t and x* is the input at this iteration.
EMD. With the following training objective,
min max EPD0(x) [Ep(θ∣D)[ψ(Tx⑻;w, X)] - Eqx [ψ(π; w, X)]] + λR(W),
φw	D
Algorithm 1:OPU Training Algorithm with EMD
Input: Posterior samples: {θs}sS=1; OPU training data D0; Gradient penalty coefficient λ; Number
of training iterations: Tstu and Twit .
while φ not converge do
Sample x(i)〜pd，(x)
/* Update Approximation	*/
for iter in 1 . . . Tstu do
Sample {∏s0}S=ι 〜q(∏∣x(i), Φ)
L(i)(φ1) = -PsS0=1 ψ(πs0; w, x(i))
φι . Adam(Vφι L(i))
Sample {∏s0}S=ι 〜q(∏∣x(i), Φ)
L(i)(φ2) = -PsS0=1 ψ(πs0; w, x(i))
φ2 . Adam(Vφ2 L(i))
end
/* Update Critic	*/
for iter in 1 . . . Twit do
Sample {πg=J 〜q(π∣x(i), φ)
Compute R(v)
L(i)(v) = PsS=1 ψ(T(x; θs); w, x(i)) - PsS0=1 ψ(πs0; w, x(i)) + λR(w)
V 一 Adam(VvL⑺)
end
end
MMD. The kernel mean embedding of Px and qx are given by μp = Epx [k(π, •)] and μq =
Eqx [k(π, •)]. The training objective is then,
min kμp - μqkHk,	(IO)
φ
11
Under review as a conference paper at ICLR 2020
NN layers
Output
Figure 4: A brief example of the critic in EMD.
Algorithm 2:OPU Training Algorithm with MMD
Input: Posterior samples: {θs}sS=1; OPU training data D0; Gradient penalty coefficient λ.
while φ not converge do
Sample x(i)〜pd，(x)
Sample {∏q,sθ}S=I 〜q(∏∣x⑴,φ), get ʤ}
L(i)(φ1)
2
S0(S0-I)
S0(S0-I) Pm=n k(πq,m, πq,m) +
Pm,,n=1 k(πq,m, πp,n)
φι — Adam( Vφι L⑶)
Sample {πs0 }sS00
〜q(∏∣χ⑶,φ)
S0
s0=1
1
S0(S0-1)
{TSx∣θs0)}S0=1
Pm6=n k(πp,m, πp,m) -
L(i)(φ1)
2
S0(S0-I)
S0(S0-1)
Pm,,n=1 k(πq,m, πp,n)
Pm=n k(πq,m, πq,m) + S0(S0-I)
end
φ — Adam(Vφ2 L(i))
0
1
Reparameterization: To obtain efficient gradient estimator and reduce variance, we reparame-
terize the Dirichlet by an equivalent product of K independent Gamma distributions. If ∏ 〜
PG(Π∣α) = QK=I Gam(∏k ∖αk), then π = (Pk πk)-1 ∏ 〜Dir(π∣α). By Thm. 3 in (Arjovsky
et al., 2017), in each L(φ∖x), the supremum is attained at ψ* ∈ Li and the gradient is VφL(φ∖x)=
-VφEq(∏∣χ,φ)[ψχ(π)]. Then as noted by (Figurnov et al., 2018), the gradient VφL(φ∖x) can be
implicitly computed without knowing the inverse of standardization function (e.g., CDF). Specifically,
by Eq∙ 5 in (Figurnov et al., 2018), VφEq(∏*φ) [ψχ(π)] = Epg(∏mφ) [V∏ψχ(π)Vφ∏], where the
first term V∏ψχ(π) is computed via the chain rule and the second term Vφ∏ is obtained by solving
a local diagonal linear system. Refer to (Figurnov et al., 2018) and references therein for details.
D Appendix: experiments
For the critic in EMD used in MNIST experiments, NN1 is a 784-256 MLP, NN2 is a 10-256 MLP
and NN3 is a 512-256-1 MLP (see Fig. 4). For the critic in EMD used in MNIST experiments, NN1
is the same as convolutional layers used in Ba, NN2 is a 10-256 MLP and NN3 is a 512-256-1 MLP
(see Fig. 4).
D. 1 Bayesian logistic regression
We test two models in this experiment: Polya Gamma (PG), CompactApprox approximating PG
(CA-PG) and OPU approximating PG (OPU-PG). We draw 500 posterior samples from PG and train
OPU with the following hyperparameters: number of epochs 100, learning rate for student. CA-PG is
trained by first drawing 5000 samples from PG then evaluating the model with 50 randomly selected
samples from them (same setup as CompactApprox). The random selection is repeated for 105 times
12
Under review as a conference paper at ICLR 2020
Table 3: Results on Bayesian logistic regression models.
Data	Model	MiSC detection		OOD detection		Acc. (%)	Time (sec.)
		AUROC	AUPR	AUROC	AUPR		
	PG	60.0 (Ent)	24.2 (Ent)	87.0 (Ent)	76.7 (Ent)	64.4	1.01 ± 0.001
Pima	CA-PG	58.2 (Ent)	24.2 (Ent)	80.1 (Ent)	74.5 (Ent)	62.3	0.18 ± 0.001
	OPU-PG	59.7 (Ent)	25.6 (Ent)	100.0 (CM)	100.0 (CM)	64.4	0.01±0.002
	PG	83.9 (Ent)	24.3 (Ent)	54.6 (Ent)	53.5 (Ent)	92.4	5.94 ± 0.001
Spam	CA-PG	64.1 (Ent)	24.2 (Ent)	71.5 (Ent)	67.5 (Ent)	85.4	0.36 ± 0.001
	OPU-PG	83.9 (Ent)	23.8 (Ent)	99.7 (CM)	99.3 (CM)	92.4	0.01±0.002
Table 4: Results on Gaussian process classification models.
Data	Model	MisC detection		OOD detection		Acc. (%)	Time (second)
		AUROC	AUPR	AUROC	AUPR		
	SGPMC	65.3(E)	46.4 (E)	96.7 (E)	94.9 (E)	79.3	0.003-
Pima	SVGP	64.3 (E)	43.2 (E)	96.0 (E)	91.1 (E)	77.1	0.004
	OPU-SGPMc	65.7 (E)	44.4 (E)	100.0 (C)	100.0 (C)	79.2	0.010
	SGPMC	86.7 (E)	37.8(E)	98.6 (E)	97.6 (E)	92.4	-0.056-
Spam	SVGP	86.2 (E)	33.3 (E)	99.2 (E)	98.5 (E)	92.1	0.032
	OPU-SGPMC	86.5(E)	39.5 (E)	100.0 (C)	100.0 (C)	92.0	0.011
and we pick the best group of samples. As the results for the three metrics are similar, we only show
the results trained with KL divergence.
The results are shown in Table 3. OPU-PG maintains similar performance with the original PG
on prediction accuracy and MisC detection. Meanwhile, OPU outperforms CA-PG on prediction
accuracy, MisC and OOD detection. For OPU, CM outperforms other uncertainty measures at OOD
detection, which indicates it captures the distributional uncertainty well. OPU performs better than
the PG Bayes teacher. The reason might be that a parametric model is learned to approximate the
ensemble of discrete samples, which could produce a smoother output distribution (regularization),
leading to better performance.OPU also achieves a ~100-600x speedup from the original PG.
D.2 Gaussian Process
For GP, we use SGPMC (Hensman et al., 2015a) as the teacher, and SVGP (Hensman et al., 2015b)
for comparison.
This experiment uses Pima and Spambase datasets as Din . Pima is a medical dataset with 769 data
points and 9 dimensions. Spambase is a text dataset with 4601 data points and 57 dimensions for
identifying spam email. We generate the same number of data points from a zero-mean multivariate
Gaussian distribution for Dood. For each dataset, 10% of data points are uniformly selected into
the testing set Dte . We normalize the data by features with L2 norm. where K is the number of
classes. There are 3 models tested: SGPMC, OPU approximating SGPMC (OPU-SGPMC) and
SVGP. SGPMC and SVGP are trained with 吉 |D叫 data points randomly selected from Din as
inducing points. Then 500 samples over functions of Din are generated from SGPMC. As the results
for the three metrics are similar, we only show the results trained with KL divergence.
The results are presented in Table 4. On MisC detection and prediction accuracy, OPU has similar
performance to SGPMC, which indicates the effectiveness of approximating prediction results with
ensemble of samples in the nonparametric family. With the same number of inducing points, SVGP
performs slightly worse than SGPMC, because it incurs a two-fold approximation. Measuring
uncertainty with CM in OPU outperforms other measures and models, which indicates the sharpness
of the logistic-normal distribution can be captured via the CM the designed Dirichlet.
SGPMC and SVGP are faster than OPU on Pima, but are slower than OPU on the larger Spambase.
This is due to the static latency for setting up the GPU for OPU, which becomes the main time cost
when the dataset is small (as in Pima). For the two GP methods, the computation time depends on the
number of inducing points and the number of dimensions. Therefore, as the dataset becomes larger
(Spambase), the computation time increases.
13
Under review as a conference paper at ICLR 2020
Table 5: Results on error/gap on EMNIST.
Avg.Approx.Err.	Avg.Model.Err.	Avg.Amt.Gap	Avg.Amt.Err.
0.0650	0.0601	00049 —	00053 —
D.3 More experiments on Bayesian Neural Network
In this section, we present more experimental results/details on Bayesian NN.
Computation of AUROC/AUPR: First, we show how AUROC/AUPR is calculated. We use entropy
(E) and max probability (P) of the particle mean, and differential entropy (D) of locally fitted student
distribution as the measures for the teacher. We use entropy (E) and max probability (P) of the particle
mean, and the scalar output of student concentration model (C) as the measures for the OPU student.
For OOD detection, say qx1 has higher E (or lower P or lower C) than qx2, then qx1 is more likely to
be an OOD data. The value of E, P and C (D for the teacher) are computed for all testing data points
(both in domain data and out-of-domain data), which rank the data points based on the numerical
magnitude. The ROC curve is plotted by setting threshold on each magnitude and compute the True
Positive Rate and False Positive Rate at each threshold. The PR curve is plotted similar by computing
the Precision and Recall. Then the area under two curves (AUROC and AUPR) can be obtained. For
misclassification detection, the similar calculation process is applied.
Error/gap: Second, we use the four types of error/gap under MMD defined in Sec. 2.4 to show the
effectiveness of amortized approximation and the suitableness of using Dirichlet family. Specifically,
the following measures are used:
•	Averaged total approximation error (Avg.Approx.Err.): The averaged MMD be-
tween teacher’s particles (for each x) and the predicted Dirichlet by OPU, i.e.,
NN PL MMD(qχi —J
•	Averaged total approximation error (Avg.Model.Err.): The averaged MMD between
teacher's particles (for each x) and locally fitted Dirichlet, i.e., -N PN=1 MMD(Pxi, qXj.
•	Averaged local amortization gap (Avg.Amt.Gap): The difference between Avg.Approx.Err
and Avg.Model.Err., i.e., ∆(x), defined in Eq. 7.
•	Averaged local amortization error (Avg.Amt.Err.): We define this type of error to be the
averaged MMD between locally fitted Dirichlet (for each x) and the predicted Dirichlet by
OPU, i.e., N PN=I MMD(qxi, qxj. The relation between Avg.Amt.Gap and Avg.Amt.Err.
is given by Eq. 8.
We show the numerical results on EMNIST dataset by Table 5. It can be observed that the Avg.Amt.Err.
0.0053 is low and it bounds the Avg.Amt.Gap ∆(x) =Avg.Approx.Err. — Avg.Model.Err=0.0049,
which is consistent with Eq. 8. The approximation error is mainly determined by the model error,
which turns out to be acceptably small. This is consistent with the analysis and also shows the
effectiveness and suitableness of using Dirichlet family.
Cifar10 results: Third, we show the experimental results of OPU approximating Bayesian NN on
Cifar10 dataset in Table 6. The NN model for teacher and student is standard VGG19 with the
output dimension of concentration model to be 1. The MCDP teacher model is trained with SGD
optimizer and a cyclical learning rate policy. The base learning rate is initialized to be 0.01 and
linearly increased to 10x of learning rate, then decreases back to the base learning rate. The base
learning rate is then scaled by half. The number of MCDP particles is 700. The OPU students are
trained with KL/EMD/MMD using the corresponding algorithm for 200 epochs.
As the task is harder than MNIST, particles tends to be more spread over the corners. Under such
case, it can be observed that the performance of EMD and MMD is much better than that of KL.
Teachers with fewer samples: Fourth, the performance of original MCDP/SGLD with fewer samples
is shown in Table 7. We note that using few particles would affect the performance of Bayesian
classifier, especially the performance on OOD detection. In real-world cases like automatic driving
car where the robustness is critical, it is not worth to trade safety for speed. Therefore, OPU solves
this issue by providing accurate estimation of predictive uncertainty with short evaluation time.
Besides the speedup, OPU approximation provides a full distribution to characterize the predictive
14
Under review as a conference paper at ICLR 2020
Table 6: Results on BNN - Cifar10.
Model	MisC detection		OOD detection LSUN		Acc. (%)
	AUROC	AUPR	AUROC	AUPR	
MCDP-KL	92.2 (P)	47.0 (P)	90.5(E)	88.7 (E)	92.4
MCDP-EMD	92.2 (P)	47.0 (P)	91.4 (D)	89.1 (D)	92.4
MCDP-MMD	92.2 (P)	47.0 (P)	91.0 (D)	89.3 (D)	92.4
-OPU-MCDP-KL-	87.2 (P)	45.9 (P)	86.1(E)	85.5 (E)	89.9
OPU-MCDP-EMD	91.8 (E)	46.9 (P)	93.5 (C)	92.0 (C)	91.8
OPU-MCDP-MMD	91.3 (E)	46.6 (P)	92.9 (C)	91.7 (C)	91.8
Table 7: Results on MNIST - MCDP/SGLD with fewer samples.
Model	MisC detection AUROC	Omniglot AUROC	SEMEION AUROC	Acc. (%)	Time S
MCDP	-96.1 (-1.2)-	98.5(-0.9)	82.7 (-4.1)	94.9 (-3.0)	132.1
SGLD	97.1(-0.9)~~	91.2 (-8.0)	82.5 (-6.8)-	98.0 (-04)	141.9
distribution, which is not available with particle approximation. This allows for better uncertainty
measures such as differential entropy.
Un-distilled teachers: Finally, we show the detailed un-distilled MCDP/SGLD, whose elements
might be covered by differential entropy of locally fitted Dirichlet in Table 1. The detailed results are
shown in Table 8
E Appendix: sampling
In this section, we illustrate the details for extracting samples from Bayesian logistic regression,
Bayesian neural network and Gaussian process. Under some contexts, we use X and Y to collectively
denote the inputs and outputs respectively for previous D .
E.1 Bayesian Logistic Regression
The Polya-Gamma (PG) scheme (Polson et al., 2013) is a data augmentation strategy that allows for a
closed-form Gibbs sampler. In binary classification, i.e., yn ∈ Y = {1, 0}, let θ be the regression
coefficients with a Gaussian conditional conjugate prior p(θ)〜N(0, Λ-1). The PG Gibbs sampler
is composed of the following two conditionals,
ωn∣θ, Xn 〜PG(1, x1θ)	(11)
θ∣ω,D 〜N(mω, Vω),	(12)
where ωn is the augmenting data corresponding to the nth data point. The posterior conditional
variance and mean are given by Vω = (XtΩX+Λ-1)-1 and mω = V-I(XT(y- 2)),respeCtiVely.
Given this formulation we will be able to collect samples from the posterior after a burn-in period.
The posterior samples {θ}sS=1, together with dataset D0, are used to train our approximation with
goal defined in Eq. 10.
As an MCMC method, the PG augmentation scheme offers accurate samples. Other alternative
methods such as local variational approximation can be employed, which are much faster but sacrifice
accuracy.
E.2 Monte Carlo Dropout
A neural network with dropout applied before every weight layer was shown to be an approximation to
probabilistic deep Gaussian process (GP) (Gal & Ghahramani, 2016). Let q(θ) to be the approximate
distribution to the GP posterior. Here, θ = {Θi}iL=1 and Θi is a parameter matrix of dimensions
Ki × Ki=1 for NN layer i. In this approximation, q(θ) can be defined through direct modification:
Θi = Midiag([zi,j]iK=i1),	(13)
15
Under review as a conference paper at ICLR 2020
Table 8: Results on MNIST - undistilled MCDP/SGLD.
Model	MisC detection AUROC AUPR	Omniglot AUROC AUPR	SEMEION AUROC AUPR	Acc. (%)
MCDP SGLD	97.3(E) 43.0(E) 97.9 (E) 46.2 (E)	99.2 (E) 98.8 (P) 99.2 (E) 99.6 (E)	86.8(E) 53.8(P) 89.3(E) 46.8(E)	97.9 98.4
where Zij 〜Bern(Pi) for i ∈ [L] and j ∈ [Ki-ι], given some prior dropout probabilities Pi and
matrices Θi are treated as variational parameters. The binary variable zi,j = 0 indicates that unit j in
layer i - 1 being dropped out as an input to layer i. The predictive mean of this approximation is
given by E[y∣x] ≈ 1 PS=I f (x, Zι,s,..., Zl,s), which hence referred to as MC dropout (MCDP).
We use OPU to approximate the uncertainty induced by q(θ). A sample in the MC ensemble is
given by θs = {Θi,s }iL=1 = Midiag([zi,j]iK=i1). We set φ1 = {Φi}iL=1Z to the mean of θs , i.e.,
Φi = S PS=1 Θi,s = Mil PS=1 diag([Zi,j,s]Kiι).
MCDP provides a simple way of approximating Bayesian inference through dropout sampling.
However, it still introduces a variational approximation to exact Bayesian posterior. Therefore,
we further include a more accurate way to generate MC ensemble - stochastic gradient Langevin
dynamics (SGLD).
E.3 VANILLA SGLD
SGLD enables mini-batch MC sampling from the posterior via adding a noise step to SGD (Welling
& Teh, 2011). We choose the “vanilla” version of SGLD in our approximation. Specifically, we start
training of fθ(∙) from θ(0). In each epoch with mini-batch size B,
θ(t+1) = θ⑴ + QV logp(θ(t) |D) + ηt	(14)
B
= θ(t) + tV(log P(θ(t)) + X logP(yb |xb, θ(t))) + ηt,	(15)
b=1
where B is the size of a mini-batch and η 〜N(0,2jI). After SGLD converges at step T, the
samples θs = θ(T +s) is collected by running the training process for another S iterations. We
use averaged samples as parameter of h, i.e., φι = S PS=I θs and train OPU by Eq. 10 with this
ensemble.
E.4 Monte Carlo Gaussian Process
We apply the OPU framework to the GP framework, which demonstrate its use on a non-parametric
classifier. Let data D be split into as input matrix X and output matrix Y. We consider GP prior
over the space of functions, i.e., μ 〜GP(0, K). where K is a positive definite kernel controlling
the prior belief on smoothness. Existing techniques allow us to compute q(μ) which approximates
p(μ∣D) and is comparable to previous q(θ) under a parametric model. In a classification task, the
posterior p(μ∣D) can be sampled via MCMC (Hensman et al., 2015a) or approximated, e.g., via
variational approximation (Hensman et al., 2015b). Let μ* be a shorthand for μχ and π is then
defined as S(μ*). If Gaussian variational approximation is used, the marginal posteriorp(μ*∣x, D)
at X induces a logistic-normal distribution for p(π∣x, D). In our approximation, we obtain samples
{μ>3 fromp(μ*∣x, D). The optimization goal is the same as that in the NN case with a different
target distribution defined as
1S
次n|x, D) = sJ2δ(π -s (μ≡))∙	(16)
16