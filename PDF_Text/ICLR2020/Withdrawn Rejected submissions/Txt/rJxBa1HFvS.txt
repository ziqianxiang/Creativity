Under review as a conference paper at ICLR 2020
Value-driven hindsight modelling
Anonymous authors
Paper under double-blind review
Ab stract
Value estimation is a critical component of the reinforcement learning (RL)
paradigm. The question of how to effectively learn predictors for value from
data is one of the major problems studied by the RL community, and different
approaches exploit structure in the problem domain in different ways. Model
learning can make use of the rich transition structure present in sequences of ob-
servations, but this approach is usually not sensitive to the reward function. In
contrast, model-free methods directly leverage the quantity of interest from the
future but have to compose with a potentially weak scalar signal (an estimate of
the return). In this paper we develop an approach for representation learning in
RL that sits in between these two extremes: we propose to learn what to model in
a way that can directly help value prediction. To this end we determine which fea-
tures of the future trajectory provide useful information to predict the associated
return. This provides us with tractable prediction targets that are directly relevant
for a task, and can thus accelerate learning of the value function. The idea can be
understood as reasoning, in hindsight, about which aspects of the future observa-
tions could help past value prediction. We show how this can help dramatically
even in simple policy evaluation settings. We then test our approach at scale in
challenging domains, including on 57 Atari 2600 games.
1	Introduction
Consider a baseball player trying to perfect their pitch. The player performs an arm motion and
releases the ball towards the batter, but suppose that instead of observing where the ball lands and
the reaction of the batter, the player only gets told the result of the play in terms of points or,
worse, only gets told the final result of the game. Improving their pitch from this experience appears
hard and inefficient, yet this is essentially the paradigm we employ when optimizing policies in
model-free reinforcement learning. The scalar feedback that estimates the return from a state (and
action), encoding how well things went, drives the learning while the accompanying observations
that may explain that result (e.g. flight path of the ball or the way the batter anticipated and struck
the incoming baseball) are ignored. To intuitively understand how such information could help
value prediction, consider a simple discrete Markov chain X → Y → Z, where Z is the scalar
return and X is the observation from which we are trying to predict Z. If the space of possible
values of Y is smaller than X, then it may be more efficient to estimate both P(Y |X) and P(Z|Y )
rather than directly estimating P(Z|X).1 In other words observing and then predicting Y can be
advantageous to directly estimating the signal of interest Z. Model-based RL approaches would duly
exploit the observed Y (by modeling the transition Y |X), but Y would, in general scenarios, contain
information that is irrelevant to Z and hard to predict. Building a full high-dimensional predictive
model to indiscriminately estimate all possible future observations, including potentially chaotic
details of the ball trajectory and the spectators’ response, is a challenge that may not pay off if the
task-relevant predictions (e.g., was the throw accepted, was the batter surprised) are error-ridden.
Model-free RL methods directly consider the relation X to Z, and focus solely upon predicting
and optimising this goal, rather than attempting to learn the full dynamics. These methods have
recently dominated the literature, and have attained the best performance in a wide array of complex
problems with high-dimensional observations (Mnih et al., 2015; Schulman et al., 2017; Haarnoja
et al., 2018; Guez et al., 2019).
1In the discrete case, this follows from a counting argument from the size of the probability tables involved.
1
Under review as a conference paper at ICLR 2020
In this paper, we propose to augment model-free methods with a lightweight model of future quan-
tities of interest. The motivation is to model only those parts of the future observations (Y ) that
are needed to obtain better value predictions. The major research challenge is to learn, from obser-
vational data, which aspects of the future are important to model (i.e. what Y should be). To this
end, we propose to learn a special value function in hindsight that receives future observations as an
additional input. This learning process reveals features of the future observations that would be most
useful for value prediction (e.g. flight path of the ball or the reaction of the batter), if provided by an
oracle. These important features are then predicted, in advance, using only information available at
test time (at the time of releasing the baseball, we knew the identity of the batter, the type of throw
and spin given to the ball). Learning these value-relevant features can help representation learning
for an agent and provide an additional useful input to its value and policy. Experimentally, hindsight
value functions surpassed the performance of model-free RL methods in a challenging association
task (Portal Choice). When hindsight value functions were added to the prior state-of-the-art RL
method for Atari games, they significantly increased median performance from 833% to 965%.
2	Background and Notation
We consider a reinforcement learning setting whereby an agent learns from interaction in a sequen-
tial decision-making environment (Sutton & Barto, 2011). An agent’s policy π, mapping states to an
action distribution, is executed to obtain a sequence of rewards and observations as follows. At each
step t, after observing state St, the policy outputs an action at, sampled from ∏(A∣st), and obtains
a scalar reward rt and the next-state st+1 from the environment. The sum of discounted rewards
from state s is the return denoted by G = Pt∞=0 γtRt, with γ < 1 denoting the discount factor. Its
expectation, as a function of the starting state, is called the value function, vπ (S) = En [G∣So = s].
An important related quantity is the action-value, or Q-value, which corresponds to the same ex-
pectation with a particular action executed first: qπ(S, a) = Eπ [G|S0 = S, A0 = a]. The learning
problem consists in adapting the policy π in order to achieve a higher value vπ. This usually entails
learning an estimate of vπ for the current policy π, this is the problem we focus on in this paper.
Note that in practice we are interested in partially-observed environments where the state of the
world is not directly accessible. For this case, we can think of replacing the observed state S in the
case of the fully-observed case by a learned function that depends on past observations.
3	Value Learning
3.1	Direct Learning
A common approach to estimate v (or q) is to represent it as a parametric function vθ (or qθ ) and
directly update its parameters based on sample returns of the policy of interest. Value-based RL
algorithms vary in how they construct a value target Y from a single trajectory. They may regress
vθ towards the Monte-Carlo return (Yt = Gt), or exploit sequentiality in the reward process by
relying on a form of temporal-difference learning to reduce variance (e.g. the TD(0) target Yt =
Rt + γvθ(St+1)). For a given target definition Y , the value loss Lv to derive an update for θ is:
Lv(θ) = 1 Es [(vθ(S) - Y)2]. In constructing a target 匕 based on a trajectory of observations
and rewards from time t, the observations are either unused (for a Monte Carlo return) or only
indirectly exploited (when bootstrapping to obtain a value estimate). In all cases, the trajectory
is distilled into a scalar signal that estimates the return of a policy, and other relevant aspects of
future observations are discarded. In particular in partially observed domains or domains with high-
dimensional observation spaces it can be difficult to discover correlations with this noisy signal.
3.2	Model-based approach
An indirect way of estimating the value is to first learn a model of the dynamics. For example a
1-step observation model mθ learns to predict the conditional distribution St+1, rt|St, at. Then a
value estimate v(S) for state S can be obtained by autoregressively rolling out the model (until the
end of the episode or to a fixed depth with a parametric value bootstrap).
2
Under review as a conference paper at ICLR 2020
The model is trained on potentially much richer data than the return signal since it exploits all
information in the trajectory. Indeed, the observed transitions between states can reveal the structure
behind a sparse reward signal. A drawback of classic model-based approaches is that they predict a
high-dimensional signal, a task which may be costly and harder than directly predicting the scalar
value . As a result, the approximation of the dynamics mθ may contain errors where it matters
most for predicting the value (Talvitie, 2014). Although the observations carry all the data from
the environment, most of it is not essential to the task (Gelada et al., 2019). The concern that
modeling all observations is expensive also applies when the model is not used for actual rollouts
but merely for representation learning. So while classic model-based methods fully use this high-
dimensional signal at some cost, model-free methods take the other extreme to focus only on the
most relevant low-dimensional signal (the scalar return). Below we propose a method that strikes a
balance between these paradigms.
3.3	Hindsight value and model
We introduce a new value function estimate that can only be computed at training time, the hindsight
value function v+ . This value still estimates the expected return from a state st but it is further
conditioned on k additional observations τt+ = st+1 , st+2 , . . . st+k occurring after time t:2
v+(st, τt+) ≈ E[G|S0 = st, . . . , Sk = st+k].	(1)
Furthermore, we require v + to follow this particular parametric structure:
v+(st,τt+; θ) = ψθ1(f(st),φθ2(τt+)),	(2)
where θ = (θ1, θ2), which forces information about the future trajectory through some vector-valued
function φ ∈ Rd . Intuitively, v+ is estimating the expected return from a past time point using
privileged access to future observations. Note that if k is large enough, then v+ simply estimates the
empirical return from time t given access to the state trajectory. However, if k is small and φ is low-
dimensional, then φ becomes a bottleneck representation of the future trajectory τt+ . By learning in
hindsight, we identify features that are maximally useful to predict the return on the trajectory from
time t. The hindsight value function is not a useful quantity by itself, since - because of its use of
privileged future observations - We cannot readily use it at test time. Furthermore, it cannot be used
as a baseline either, as when computing the policy gradient it will yield a biased gradient estimator.
. Instead, the idea is to learn a model φ of φ, that can be used at test time. We conjecture that if
privileged features φ are useful for estimating the value, then the model of those features Will also
be useful for estimating the value function. We propose to learn the approximate expectation model
φη2 (s) conditioned on the current state s and parametrized by η2, minimizing the folloWing squared
loss:
LmOdel(η2) = Es,τ + [kΦθ2(s,T+) - Φη (s)k2]	⑶
Where the expectation is taken over the distribution of states and partial trajectories τ+ resulting
from that state.
The approximate model φ can then be leveraged to obtain a better model-based value estimate
vm(s; η) = ψη1 (f (s), φη2 (s)). Although φ(s) cannot contain more information than included
already in the state s, it can still benefit from having being trained using a richer signal before the
value converges. Figure 3 summarizes the relation betWeen the different quantities.
3.4	Illustrative example
We consider the folloWing example to illustrate hoW the approaches to estimating the value function
can differ. There are no actions in this example3 and each episode consists of a single transition
from initial state s to terminal state s0, With a reWard r(s, s0) on the Way.
Each instance of this example is parametrized by a square matrix W and a vector b sampled from a
unit normal distribution, Which determine the uncontrolled MDP. Initial states s are of dimension D
and sampled from a multivariate unit normal distribution (Si 〜N(0,1) for all state dimension i).
2In general τt+ can be defined to include full future transitions, actions and reWards.
3This can be understood as a Markov ReWard Process or a policy evaluation setting
3
Under review as a conference paper at ICLR 2020
Given s = ( ss21 ), where s1 and s2 are of dimension D1 and D2 (D = D1 + D2), the next state
s0 = ss01 is determined according to the transition function: s01 = MLP(s) + and s02 = σ(Ws2 +
b) where σ is the HeaViside function. s'1 acts as a distractor here, with additive noise e 〜N(0,1).
The reward obtained is r(s, s0) = Pi SI) Pi s2(i/√D. The true value in the start state is also
v(s) = r(s, s0).
The key aspect of this domain is that S0 reveals structure that helps predict the value function in
the start state S. This is made visually obvious in the trajectories sampled in this domain shown in
Figure 1.
Figure 1: Visualization of episodes in the illustrative example of Section 3.4. Model-free value prediction see
the start state on the left and must predict the corresponding color-coded reward on the right. Hindsight value
prediction can leverage the observed structure in the intermediate state to obtain a better value prediction. In
more detail, this plot shows the second half s2 of initial state s on the left. In the middle, superimposed is
the observed reward-relevant quantity Pi s02(i) that has been color-coded on the s2 vectors. On the right is the
color-coded reward for each trajectory. The dimension of states is D = 4 in this example.
Let us consider how the different approaches to learning values presented above fare in this problem.
For direct learning, the value from v(S0) is 0 since S0 is terminal, so any n-step return is identical to
the Monte-Carlo return, that is, the information present in observation S0 is not leveraged. Results
from learning v from S given the return is presented in Figure 2 (blue curve). A model-based
approach first predicts S0 from S, then attempts to predict the value given S and the estimated next
state. When increasing the input dimension, given a fixed capacity, the model does not focus its
attention on the reward-relevant structure in S0 and makes error where it matters most. As a result, it
can struggle to learn v faster than a model-free estimate (cf. red curve in Figure 2). When learning
in hindsight, v+ can directly exploit the revealed structure in the observation of τ+ , and as a result
the hindsight value learns faster than the regular causal model-free estimate (cf. dotted yellow curve
in Figure 2). This drives the learning of φ and its model φ, which directly gets trained to predict
these useful features for the value. As a result, vm also benefits and learns faster than the regular v
estimate on this problem (cf. green curve in Figure 2).
3.5	When is it advantageous to model in hindsight?
To understand the circumstances in which hindsight modelling provides a better value estimate, we
first consider an analysis that relies on the following assumptions. Suppose that vθm is sharing the
same function ψ as	v+	(i.e., θ1 =	η1),	and let ψ be linear. Ifwe write ψθ1 (f,	φ)	=	( ωω21 )>	φf	+ b,
where θ1 = (ω1, ω2), then we have for fixed values of the parameters:
E[(vm(s; η) - v+ (s,τ+; θ)2] = E[kω>(φ(τ+; θ2) - φ(s; η2)) ∣∣2]	(4)
≤ E[kω2k2kφ(τ+) - φ(s)k2]	⑸
= kω2k2Lmodel(η2),	(6)
4
Under review as a conference paper at ICLR 2020
Figure 2: Learning the value of the initial state in the example of Section 3.4. The dimension of the data is
D = 32 for this experiment, with the dimension of the useful data in the next state D2 = 4. The results
are averaged over 4 different instances, each repeated twice. Note that v+ (dotted line) is using privileged
information (the next state).
using the Cauchy-Schwarz inequality. Let L define the value error for a particular value function v:
L(v) = E[(v(s) - G)2] and L(v+) = E[(v+ (s, τ+) - G)2]. Then we have:
L(vm) = E[(vm(s) - v+(s, τ+) + v+(s, τ+) -G)2]	(7)
≤ 2(kω2k2Lmodel(η2)+L(v+)),	(8)
using the fact that E[(X + Y )2] ≤ 2(E[X2] + E[Y2]) for random variables X and Y . Ifwe assume
L(v+) = CL(v) with C < 0.5 (i.e., estimating the value in hindsight with more information is an
easier learning problem), then the following holds:
Lmodel(η2) < (1 -12C)L(V) =⇒ L(Vm) < L(v).	(9)
2kω2k2
In other words, this relates how small the modeling error needs to be to guarantee that the value
error for Vm is smaller than the value error for the direct estimate V. The modeling error can be large
for different reasons. If the environment or the policy is stochastic, then there is some irreducible
modeling error for the deterministic model. Even in these cases, a small C can make hindsight
modeling advantageous. The modeling error could also be high because predicting φ is hard. For
example, it could be that φ essentially encodes the empirical return, which means predicting φ is
at least as hard as predicting the value function (Lmodel(η2) ≥ L(V)). Or it could be that φ is high-
dimensional, this could cause both a hard prediction problem but also would cause the acceptable
threshold for Lmodel to decrease (since kθ2 k2 will grow). We address some of these concerns with
specific architectural choices like V + having a limited view on future observations and having low
dimensional φ (see next section). Note that the analysis above ignores any advantage that could
be obtained from representation learning when training φ (if the state encoding function f shares
parameters with φ).
4	Architecture
The architecture for Hindsight Modelling (HiMo) we found to work at scale and tested in the ex-
perimental section of the paper is described here. To deal with partial observability, we employ a
recurrent neural network, the state-RNN, which replaces the state st with a learned internal state ht ,
a function of the current observation ot and past observations through ht-1: ht = f(ot, ht-1; η3),
where we have extended the parameter description ofVm as η = (η2, η1, η3). The model-based value
function Vm and the hindsight value function V+ share the same internal state representation h, but
the learning of V+ assumes h is fixed (we do not backpropagate through the state-RNN in hindsight).
In addition, we force φ to only be learned through Lmodel, so that Vm uses it as an additional input.
5
Under review as a conference paper at ICLR 2020
To summarize:
v+(ht, ht+k; θ) = ψθι (ht, φθ2 (ht+k)),	(10)
V (ht； η) = ψηι (ht, φη2(htY),	(II)
with the bar notation denoting quantities treated as non-differentiable (i.e. where the gradient is
stopped). The different losses in the HiMo architecture are combined in the following way:
L(θ, η) = Lv(η) + αLv+(θ) + βLmodel(η).	(12)
A diagram of the architecture is presented in Figure 3, and further implementation details can be
found in the appendix.
ot	ot+k
Figure 3: Network architecture for HiMo. Double blue arrows denote losses on different outputs of the network.
Red denote quantities which are only computed in hindsight at train time (using parameters θ). The Z symbol
on an arrow means its input is assumed to be non-differentiable (also sometimes called a stop gradient).
This architecture can be straightforwardly generalized to cases where we also output a policy πη for
an actor-critic setup, providing h and φ as inputs to a policy network.4 For a Q-value based algorithm
like Q-learning, we predict a vector of values qm and q+ instead of vm and v+ . Computing v+ and
training φ can be done in an online fashion by simply delaying the updates by k steps (just like the
computation of an n-step return).
5	Experiments
The illustrative example in Section 3.4 demonstrated the positive effect of hindsight modeling in
a simple policy evaluation setting. In this section, we now explore these benefits in the context
of policy optimization in challenging domains, a custom navigation task called Portal Choice, and
Atari 2600. To demonstrate the generality and scalability of our approach we test hindsight value
functions in the context of two high-performance RL algorithms, IMPALA (Espeholt et al., 2018)
and R2D2 (Kapturowski et al., 2019).
5.1	Portal Choice task
The Portal Choice (Fig. 4) is a two-phase navigation task where, in phase one an agent is presented
with a contextual choice between two portals, whose positions vary between episodes. The position
of the portal determines its destination in phase two, one of two different goal rooms (green and red
rooms). Critically, the reward when terminating the episode in the goal room depends on both the
color of the goal room in phase two and a visually indicated combinatorial context shown in the first
phase. If the context matches the goal room color, then a reward of 2 is given, otherwise the reward
is 0 when terminating the episode (see appendix for the detailed description).
An easy suboptimal solution is to select the portal at random and finish the episode in the resulting
goal room by reaching the goal pixel, which will result in a positive reward of 1 on average. A more
4In this case, the total loss also contains an actor loss to update πη and a negative entropy loss.
6
Under review as a conference paper at ICLR 2020
Figure 4: Portal Choice task. Left: an observation in the starting room of the Portal Choice task. Two portals
(cyan squares) are available to the agent (orange), each of them leading to a different room deterministically
based on their position. Right: The two possible goal rooms are identified by a green and red pixel. The reward
upon reaching the goal (blue square) is a function of the room and the initial context.
difficult strategy is to be selective about which portal to take depending on the context, in order
to get the reward of 2 at each every episode. A model-free agent has to learn the joint mapping
from contexts and portal positions to rewards. Even if the task is not visually complex, the context is
combinatorial in nature (the agent needs to count randomly placed pixels) and the joint configuration
space of context and portal is fairly large (around 250M). Since the mapping from portal position to
rooms does not depend on context, learning the portal-room mapping independently is more efficient
in this scenario.
For this domain, we implemented the HiMo architecture within a distributed actor-critic agent,
named IMPALA proposed by Espeholt et al. (2018). In this case, the target Yt to train vm (used
as a critic in this context) and v+ is the V-trace target (Espeholt et al., 2018) to account for off-
policy corrections between the behavior policy and the learner policy. The actor shares the same
network as the critic and receives h and φ as inputs.
(a)
(b)	(c)
Figure 5: Results in the Portal Choice task. (a) shows the median performance as a function of environment
steps out of 4 seeds. (b) shows the value error averaged across states on the same x-axis scale for different value
function estimate. (c) is an analysis that shows the cross-entropy loss of a classifier that takes as input φ (solid
line) or φ (dotted line) and predicts the identity of the goal room (red or green) as a binary classification task.
The HiMo curves (blue) show that information about the room identity becomes present first in φ and then gets
captured in its model φ. For the baseline (where we set α = β = 0), φ is not trained based on φ and only
achieves to classify the room identity at chance level.
We found that HiMo+IMPALA learned reliably faster to reach the optimal behavior, compared to
the vanilla IMPALA baseline that shared the same network capacity (see Figure 5a). The hindsight
value v+ rapidly learns to predict whether the portal-context association is rewarding based on
seeing the goal room color in the future. Then φ learns to predict the new information from the
future that helps that prediction: the identity of the room (see analysis Fig 5c). The prediction of
φ becomes effectively a model of the mapping from portal to room identity (since the context does
not correlate with the room identity). Having access to such mapping through φ helped the value
prediction (Fig 5b), which led to better action selection. Note that if the two rooms were visually
indistinguishable, for example with no red/green rooms separation, HiMo would not be able to offer
any advantage over its model-free counterpart.
5.2	Atari
We tested our approach in Atari 2600 videogames using the Arcade Learning Environment (Belle-
mare et al., 2013). We added HiMo on top of Recurrent Replay Distributed DQN (R2D2), a DQN-
7
Under review as a conference paper at ICLR 2020
based distributed architecture introduced by Kapturowski et al. (2019) which achieved state-of-the-
art scores in Atari games.
In this value-based setting, HiMo trains qm(∙, ∙; η) and q+(∙, ∙; θ) based on n-step return targets:
Yt = g (X γmRt+m + YngT (qm(St+n, A*; η-))],	(13)
m=0
where g is an invertible function, η- are the periodically updated target network parameters (as in
DQNbyMnihetaL (2015)), and A* = argmax。qm (St+n, a; η) (the Double DQN update proposed
by Van Hasselt et al. (2016)). The details of the architecture and hyperparameters are described in
the appendix.
Figure 6: Difference in human normalized score per game in Atari, HiMo versus the improved R2D2 after
200k learning steps, alongside learning curves for a selection of HiMo worst and top performing games. Note
that the high variance of the curves in Atari between seeds can usually be explained by the variable timestep at
which different seeds jump from one performance plateau to the next.
We ran our approach on 57 Atari games for 200k gra-
dient steps (around 1 day of training), with 3 seeds for
each game. The evaluation averages the score between
200 episodes across seeds, each lasting a maximum of
30 minutes each and starting a random number (up to
30) of no-op actions. In order to compare scores be-
tween different games and aggregated results, we com-
puted normalized scores for each game based on ran-
dom and human performance so that 0% corresponds
to random performance and 100% corresponds to hu-
man. We observed an increase of 132.5% in the me-
Table 1: Median and mean human normalized
scores across 57 Atari2600 games for HiMo
versus the R2D2 baseline after a day of train-
ing.
I R2D2 I R2D2 + HiMo
Median	832.5%	965%
Mean	2818.5%	2980%
dian human normalized score compared to the R2D2 baseline with the same network capacity, ag-
gregate results are reported in Table 1. Figure 6 details the difference in normalized score between
HiMo and our R2D2 baseline for all games individually. We note that the original R2D2 results
reported by Kapturowski et al. (2019), which used a similar hardware configuration but a different
network architecture, were around 750% median human normalized score after a day of training.
In our experimental evaluation we observed that HiMo typically either offers improved data ef-
ficiency or has no overwhelming adverse effects in training performance. In Figure 6 we show
training curves for a selection of representative Atari environments where at evaluation time HiMo
both under-performed (left) and out-performed R2D2 (right); these seem to indicate that in the worst
case scenario HiMo’s training performance reduces to R2D2’s.5
Bowling is one of the Atari games where rewards are delayed with relevant information being com-
municated through intermediate observations (the ball hitting the pins), just like the baseball example
5We will include a performance analysis over longer training regimes in a future version of the paper.
8
Under review as a conference paper at ICLR 2020
we have used in the introduction. We found HiMo to perform better than the R2D2 baseline in this
particular game. We also ran HiMo with the actor-critic setup (IMPALA) described in the previous
section, finding similar performance gain with respect to the model-free baseline. Learning curves
for these experiments are presented in Figure 7.
Parameter updates	Parameter updates
(a)	(b)
(c)
Figure 7: (a) The bowling game in Atari, where a delayed reward can be predicted by the intermediate event of
the ball hitting the pins. (b-c) Learning curves for HiMo in the bowling game using two different RL methods:
a value-based method (R2D2) in (b) and a policy-gradient method (IMPALA) in (c).
6	Related Work
Recent work have used auxiliary predictions successfully in RL as a mean to obtain a richer signal
for representation learning (Jaderberg et al., 2016; Sutton et al., 2011). However these additional
prediction tasks are hard-coded and so they cannot adapt to the task demand when needed. We see
them as a complementary approach to more efficient learning in RL.
Buesing et al. (2018) have considered using observations in an episode trajectory in hindsight to
infer variables in a structural causal model of the dynamics, allowing to reason more efficiently in a
model-based way about counterfactual actions. However this approach requires learning an accurate
generative model of the environment.
In supervised learning, the learning using privileged information (LUPI) framework introduced by
(Vapnik & Izmailov, 2015) considers ways of leveraging privileged information at train time. Al-
though the techniques developed in that work do not apply directly in the RL setting, some of our
approach can be understood in that setting as considering the future trajectory as the privileged
information for a value prediction problem.
Privileged information coming from full state observation has been leveraged in RL to learn better
critic in asymmetric actor-critic architectures (Pinto et al., 2017; Zhu et al., 2018). However this
does not use future information and only applies to settings where special side-information (full
state) is available at train time.
7	Conclusion
High-dimensional observations in the intermediate future often contain task-relevant features that
can facilitate the prediction of an RL agent’s final return. We introduced a reinforcement learning
algorithm, HiMo, that leverages this insight by the following two-stage approach. First, by reason-
ing in hindsight, the algorithm learns to extract relevant features of future observations that would
be been most helpful for estimating the final value. Then, a forward model is learned to predict
these features, which in turn is used as input to an improved value function, yielding better policy
evaluation and training at test time. We demonstrated that this approach can help tame complexity
in environments with rich dynamics at scale, yielding increased data efficiency and improving the
performance of state-of-the-art model-free architectures.
9
Under review as a conference paper at ICLR 2020
References
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:
253-279, 2013.
Lars Buesing, Theophane Weber, Yori Zwols, Sebastien Racaniere, Arthur Guez, Jean-Baptiste
Lespiau, and Nicolas Heess. Woulda, coulda, shoulda: Counterfactually-guided policy search.
arXiv preprint arXiv:1811.06272, 2018.
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam
Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with
importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018.
Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G Bellemare. Deep-
mdp: Learning continuous latent space models for representation learning. arXiv preprint
arXiv:1906.02736, 2019.
Arthur Guez, Mehdi Mirza, Karol Gregor, Rishabh Kabra, Sebastien Racaniere, Theophane Weber,
David Raposo, Adam Santoro, Laurent Orseau, Tom Eccles, et al. An investigation of model-free
planning. In International Conference on Machine Learning, pp. 2464-2473, 2019.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018.
Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David
Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv
preprint arXiv:1611.05397, 2016.
Steven Kapturowski, Georg Ostrovski, Will Dabney, John Quan, and Remi Munos. Recurrent ex-
perience replay in distributed reinforcement learning. In International Conference on Learning
Representations, 2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel. Asym-
metric actor critic for image-based robot learning. arXiv preprint arXiv:1710.06542, 2017.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 2011.
Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White,
and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsu-
pervised sensorimotor interaction. In The 10th International Conference on Autonomous Agents
and Multiagent Systems-Volume 2, pp. 761-768, 2011.
Erik Talvitie. Model regularization for stable sample rollouts. In Proceedings of the Thirtieth
Conference on Uncertainty in Artificial Intelligence, pp. 780-789. AUAI Press, 2014.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Thirtieth AAAI conference on artificial intelligence, 2016.
Vladimir Vapnik and Rauf Izmailov. Learning using privileged information: similarity control and
knowledge transfer. Journal of machine learning research, 16(2023-2049):2, 2015.
Yuke Zhu, Ziyu Wang, Josh Merel, Andrei A. Rusu, Tom Erez, Serkan Cabi, Saran Tunyasuvu-
nakool, Janos Kramar, Raia HadselL Nando de Freitas, and Nicolas Heess. Reinforcement and
imitation learning for diverse visuomotor skills. CoRR, abs/1802.09564, 2018.
10
Under review as a conference paper at ICLR 2020
A	Appendix
A. 1 General Architecture Details
To compute v+ and train φ in an online fashion, we process fixed-length unrolls of the state-RNN
and compute the hindsight value and corresponding updates at time t ift+k is also within that same
unroll. Also, we update v+ at a slower rate (i.e., α < β) to give enough time for the model φ to adapt
to the changing hindsight features φ. In our experiments we found that even a low-dimensional φ
(in the order of d = 3) and a relatively short hindsight horizon k (in the order of 5) are sufficient to
yield significant performance boosts, whilst keeping the extra model computational costs modest.
A.2 Portal Choice
Environment The observation is a 7 × 23 RGB frame (see Figure 4). There are 3 possible spawn-
ing points for the agent in the center and 42 possible portal positions (half of which lead to the
green room, the other half leading to the red room). At the start of an episode, two portals, each
leading to a different room, are chosen are random. They are both displayed as cyan pixels. In-
cluded in the observation in the first phase is the context, a random permutation in a 5 × 5 grid of
N pixels, where is uniformly sampled at the start of each episode: N 〜 U{1,10}. A fixed map
f : {1, . . . , 10} → {0, 1} determines which contexts are rewarding with the green room, the rest
being rewarding with the red room. The reward when reaching the goal is determined according to:
R = 2(f(N)G + (1 - f(N))(1 - G)),	(14)
where G ∈ {0, 1} is whether the reached room is green.
Network architecture The policy and value network takes in the observation and passes it to a
ConvNet encoder (with filter channels [32, 32, 32], kernel shapes [4, 3, 3] applied with strides [2,
1, 1]) before being passed to a ConvLSTM network with 32 channels and 3x3 filters. The output
~ ∙	„	_______ .	■	.	一	■ 一 个	一. „ 一	. .	__ 一 一 一一一
of the ConvLSTM is the internal state h. The φ network is a ConvNet with [32, 32, 32, 1] filter
channels with kernels of size 3 except for a final 1x1 filter, whose output is flatten and passed to an
MLP with 256 hidden units with ReLu activation, before a linear layer with dimension d = 3. The
φ network is a similarly configured network with one less convolution layer and 128 hidden units in
the MLP. The ψη network is an MLP with 256 hidden units followed by a linear layer that takes h
and φ as input and outputs the policy πm and the value vm . v+ is obtained similarly with a similar
MLP that has a single scalar output. We used a future observation window of k = 5 steps in this
domain and loss weights α = 0.25, β = 0.5. Unroll length was 20, and γ = 0.99. Optimization
was done with the Adam optimizer (learning rate of 5e - 4), with batch size 32. The model-free
baseline is obtained by using the same code and network architecture, and setting the modeling loss
and hindsight value loss to 0 (α = β = 0).
A.3 Atari
Hyper-parameters and infrastructure are the same as reported in Kapturowski et al. (2019), with
deviations as listed in table 2. For our value target, we also average different n-step returns with
exponential averaging as in Q(λ) (with the return being truncated at the end of unrolls). The Q
network is composed of a convolution network (cf. Vision ConvNet in table) which is followed by
an LSTM with 512 hidden units. What we refer to in the main text as the internal state h is the output
of the LSTM. The φ and φ networks are MLPs with a single hidden layer of 256 units and ReLu
activation function, followed by a linear which outputs a vector of dimension d. The ψθ1 function
concatenates h and φ as inputs to an MLP with 256 hidden units with ReLu activation function,
followed by a linear which outputs q+ (a vector of dimension 18, the size of the Atari action set).
qm is obtained by passing h and φ to a dueling network as described by Kapturowski et al. (2019).
Other HiMo parameters are described in table 3. The R2D2 baseline with the same capacity is
obtained by running the same architecture with α = β = 0.
11
Under review as a conference paper at ICLR 2020
Table 2: Hyper-parameter values used for our R2D2 implementation.
Number of actors	320
Sequence length	80 (+ prefix ofl = 20 in burn-in experiments)
Learning rate	2e-4
Adam optimizer β1	0.9
Adam optimizer β2	0.999
λ	0.7
Target update interval	400 / 	 ∖
Value function rescaling	g(x) = Sign(X) (，kXk + 1 — 1)+ ex, C = 10-3
Frame pre-processing	None (full res. including no frame stacking)
Vision ConvNet filters sizes	[7, 5, 5, 3]
Vision ConvNet filters strides	[4, 2, 2, 1]
Vision ConvNet filters channels	[32, 64,128,128]
Table 3: Hindsight modeling parameters for Atari
α	0.01
β	1.0
k	5
d	3
12