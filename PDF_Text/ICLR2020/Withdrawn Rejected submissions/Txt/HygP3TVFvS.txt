Under review as a conference paper at ICLR 2020
Non-Gaussian processes and
NEURAL NETWORKS AT FINITE WIDTHS
Anonymous authors
Paper under double-blind review
Ab stract
Gaussian processes are ubiquitous in nature and engineering. A case in point is
a class of neural networks in the infinite-width limit, whose priors correspond to
Gaussian processes. Here we perturbatively extend this correspondence to finite-
width neural networks, yielding non-Gaussian processes as priors. The method-
ology developed herein allows us to track the flow of preactivation distributions
by progressively integrating out random variables from lower to higher layers,
reminiscent of renormalization-group flow. We further develop a perturbative pre-
scription to perform Bayesian inference with weakly non-Gaussian priors.
1 Inception
Gaussian processes model many phenomena in the physical world. A prime example is Brownian
motion (Brown, 1828), modeled as the integral of Gaussian-distributed bumps exerted on a point-
like solute (Einstein, 1905). The theory of elementary particles (Weinberg, 1995) also becomes a
Gaussian process in the free limit where interactions between particles are turned off, and many-
body systems as complex as glasses come to be Gaussian in the infinite-dimensional, mean-field,
limit (Parisi & Zamponi, 2010). In the context of machine learning, Neal (1996) pointed out that
a class of neural networks give rise to Gaussian processes in the infinite-width limit, which can
perform exact Bayesian inference from training to test data (Williams, 1997). They occupy a corner
of theoretical playground wherein the karakuri of neural networks is scrutinized (Lee et al., 2018;
Matthews et al., 2018; Jacot et al., 2018; Chizat et al., 2018; Lee et al., 2019; Geiger et al., 2019).
In reality, Gaussian processes are but mere idealizations. Brownian particles have finite-size struc-
ture, elementary particles interact, and many-body systems respond nonlinearly. In order to under-
stand rich phenomena exhibited by these real systems, Gaussian processes rather serve as start-
ing points to be perturbed around. Indeed many edifices in theoretical physics are build upon
the successful treatment of non-Gaussianity, with a notable example being renormalization-group
flow (Kadanoff, 1966; Wilson, 1971; Weinberg, 1996; Goldenfeld, 2018). In the quest to elucidate
behaviors of real neural networks away from the infinite-width limit, it is thus natural to wonder if
the similar treatment of non-Gaussianity yields equally elegant and powerful machinery.
Here we set out on this program, perturbatively treating finite-width corrections to neural networks.
Prior distributions of outputs are obtained through progressively integrating out preactivation of neu-
rons layer by layer, yielding non-Gaussian priors. Intriguingly, intermediate recursion relations and
their derivation resemble renormalization-group flow (Goldenfeld, 2018; Mehta & Schwab, 2014).
Such a recursive approach further enables us to treat finite-width corrections on Bayesian inference
and their regularization effects, with arbitrary activation functions.
The rest of the paper is structured as follows. In Section 2 we review and set up basic concepts.
Our master recursive formulae (R1,R2,R3) are derived in Section 3, which control the flow of pre-
activation distributions from lower to higher layers. After an interlude with concrete examples in
Section 4, we extend the Gaussian-process Bayesian inference method to non-Gaussian priors in
Section 5 and use the resulting scheme to study inference of neural networks at finite widths. We
conclude in Section 6 with dreams.
1
Under review as a conference paper at ICLR 2020
2 To infinity and beyond
In this paper we study real finite-width neural networks in the regime where the number of neurons
in hidden layers is asymptotically large whereas input and output dimensions are kept constant.
2.1	Gaussian processes and neural networks at infinite widths
Let us focus on a class of neural networks termed multilayer perceptrons, with model parameters,
θ = { bi'), W('j)}, and an activation function, σ. For each input, X ∈ Rn0, a neural network outputs
a vector, z(x; θ) = z(L) ∈ RnL, recursively defined as sequences of preactivations through
zi(1)(x)
n0
bi(1) + X Wi(,1j)xj for i = 1, . . . , n1 ,
(1)
Zy)(X)
j=1
n`-1
b(') + X Wg)σ [z尸)
for i = 1, . . . , n` ; ` = 2, . . . , L .
(2)
j=1
Following Neal (1996), we assume priors for biases and weights given by independent and identi-
cally distributed Gaussian distributions with zero means, E [b(')] = E [W(?] = 0, and variances
E [b(')b(')]
E [WSιWy
δi1,i2 喈,
C (')
δ- - δ- - CW
°i1,i2 CjIj2	.
n`-1
(3)
(4)
Higher moments are then obtained by Wick’s contractions (Wick, 1950; Zee, 2010). For instance,
E
bi')b(')i	=	hCb')i	×	(δiι,i2	δi3,i4	+	δi1,i3 δi2,i4	+	δi1,i4δi2,i3 ) .
(5)
For those unfamiliar with Wick’s contractions and connected correlation functions (a.k.a. cumu-
lants), a pedagogical review is provided in Appendix A as our formalism heavily relies on them.
In the infinite-width limit where n1 , n2 , . . . , nL-1 → ∞ (but finite n0 and nL), it has been argued
-with varying degrees of rigor (Neal, 1996; Lee et al., 2018; Matthews et al., 2018) - that the prior
distribution of outputs is governed by the Gaussian process with a kernel
Kiα11,i,2α2 ≡ E zi(1L)(Xα1)zi(2L)(Xα2)
(6)
and all the higher moments given by Wick’s contractions. In particular, there exists a recursive
formula that lets us evaluate this kernel for any pair of inputs (Lee et al., 2018) [c.f. Equation (R1)].
Importantly, once the values of the kernel are evaluated for all the pairs of ND = NR + NE input
data, {Xα}α=1,...,N , consisting of NR training inputs with target outputs and NE test inputs with
unknown targets, we can perform exact Bayesian inference to yield mean outputs as predictions for
NE test data (Williams, 1997; Williams & Rasmussen, 2006) [c.f. Equation (GPM)]. This should be
contrasted with stochastic gradient descent (SGD) optimization (Robbins & Monro, 1951), through
which typically a single estimate for the optimal model parameters of the posterior, θ? , is obtained
and used to predict outputs for test inputs; Bayesian inference instead marginalizes over all model
parameters, performing an ensemble average over the posterior distribution (MacKay, 1995).
2.2	Beyond infinity
We shall now study real finite-width neural networks in the regime nι,..., nL-ι 〜n>1.1 At
finite widths, there are corrections to Gaussian-process priors. In other words, a whole tower of
1 Note that input and output dimensions, n0 and nL, are arbitrary. To be precise, defining n1, . . . , nL-1 ≡
μιn,..., μL-ιn, We send n》1 while keeping {c1'), C(W)}	, μι,..., μL-ι, no, and nL constants,
and compute the leading 1/n corrections. In particular it is crucial to keep the number of outputs nL constant
in order to consistently perform Bayesian inference within our approach.
2
Under review as a conference paper at ICLR 2020
nontrivial preactivation correlation functions beyond the kernel,
G (')αι,…Qm 一 E h ('))(^α∩	(% am ʌi	G
G il,…,im ≡ E [zil (x ) …Zim (X	)∖ ,	(7)
collectively dictate the distribution of preactivations. Our aim is to trace the flow of these distribu-
tions progressively and cumulatively all the way up to the last layer whereat Bayesian inference is
executed. More specifically, we shall inductively and self-consistently show that two-point preacti-
vation correlation functions take the form2
G (')a1，a2 — δ--
i1,i2	=	i1,i2
α1,α2
W)
1
+n-ι Sar+O
and connected four-point preactivation correlation functions
(KS)
(V)
—
a1,a2
i1 ,i2
a3,a4
i3,i4
—
a1,a3
i1 ,i3
a2,a4
i2,i4
G (')aι ,a4
G	i1,i4
a2,a3
i2 ,i3
G (')a1,a2 ,a3,a4
i1,i2,i3,i4
connected
G (')a1,a2 ,a3,a4
i1,i2,i3,i4
,a2),(a3,a4)
+ M,i3 δi2,i4 V^,a3),(a2,a4)

—
+ δi1,i4 Um%。4",叫 + O
and higher cumulants are all suppressed by O (% ).3 Here the Gaussian-process core kernel Ka) ,α2
aa
and the self-energy correction S(j 2 are symmetric under the exchange of sample indices α1 什 a2
and the four-point vertex 唬1。2),®。4)is symmetric under α1 什 α2, α3 什 α4, and (α1,a2)分
(α3, α4). At the first layer the preactivation distribution is exactly Gaussian for any finite widths and
hence Equations (KS) and (V) are trivially satisfied, with
a1 a2
KaI),a2 = cbi) + CW) ∙ x——∖ , Sal),a2 =0, and ,小。2)，⑺3。4)=0 . (ro)
Obtained in Section 3 are the recursive formulae that link these core kernel, self-energy, and four-
point vertex at the `-th layer to those at the (` + 1)-th layer while in Section 5 these tensors at the
last layer ` = L are used to yield the leading 1/n correction for Bayesian inference at finite widths.
2.3	Related work
Our Schwinger operator approach is orthogonal to the replica approach by Cohen et al. (2019) and,
unlike the planar diagrammatic approach by Dyer & Gur-Ari (2019), applies to general activation
functions, made possible by accumulating corrections layer by layer rather than dealing with them
all at once. More substantially, in contrast to these previous approaches, we here study finite-width
effects on Bayesian inference and find that the renormalization-group picture naturally emerges.
3 Distributional flow
As auxiliary objects in recursive steps, let us introduce activation correlation functions
H(')aι,...,am ≡ E [σ hzi(') (xaι)i …σ hzi(') (xam)]].	⑻
il ,...,im	il	im
Our basic strategy is to establish relations
{g(1)} → {H(1)} → {g(2)} → ——> {h(LT)O → {g(l)} ,	(ZIGZAG)
zigzagging between sets of preactivation correlation functions and sets of activation correlation func-
tions, keeping track of leading finite-width corrections. Below, relations G(') → H(') are obtained
by integrating out preactivations while relations H(') → G('+1) are obtained by integrating out
biases and weights. At first glance the algebra in this paper may look horrifying but repeated appli-
cations of Wick’s contractions are all there is to it. The results are summarized in Section 3.2.
2In the main text We place tildes on objects that depend only on sample indices a's in order to distinguish
them from those that depend both on sample indices a's and neuron indices i's.
3Given that the means of biases and weights are zero, G⑷彳 ’ .；丁 =。for all odd m.
3
Under review as a conference paper at ICLR 2020
3.1	Zigzag relations for preactivation and activation correlation functions
Integrating over the Gaussian biases and weights at ''s connections yield the relations that link
activation correlations H(') to preactivation correlations G('+1) at the next layer. Recalling EqUa-
tions (KS) and (V), trivial Wick’s contractions yield
eα1,α2
K('+1)
1ea1,α2
+ 不' S('+1)
e (α1,α2),(α3,α4)
Y ('+1)
hcW+1)i2
+O
n`
Cb	) + CW) — X H (')ɑj,α2	and (9)
n`
j=1
n`
1 ^X H(')α1,α2,α3,α4 - H(')α1,α2H(')α3,α4] (10)
n` j,k=1	, , ,	,
The remaining task is to relate preactivation correlations G(') to activation correlations H(') within
the same layer, which will complete the zigzag relation (ZIGZAG) for these correlation functions.4
Once the sorcery of Wick’s contractions and connected correlation functions is mastered, it is
simple to derive the following combinatorial hack (Appendix A.4): viewing prior preactivations
z(`)
ziα ≡ zi(`) (xα)
I	J i=1,…,n'；a=1,…,Nd
as a random (h'Nd)-dimensional vector and defining
the Gaussian integral with the kernel 卜才Za }K(')= K('々1；；2 ≡ δi1,i2Ka),α2, the prior average
E nF[z(')]} = DF[z(')]〉K⑹ + n-1 hDF[z(')]OS[z(')]+F[z(')]Ov[z"κ/ + O
G)
(HACK)
for any function F. Here the operators OS [z(`)] and OV [z(`)] capture 1/n corrections due to self-
energy and four-point vertex, respectively, and are defined as
OS[z(')] ≡
2 X灌a
α1 ,α2
α1 α2
zi zi
-n'K0T2
and
OV [z(`)] ≡
Y(α1,α2),(α3,α4)
α1,α2,α3,α4
(OS)
(OV)
×
-4
α1 α3
zi	zi
α1 α2
zi
n`
Xzjα3zjα4
j=1
κ α2,α4+nκ(
—2n'
α1 α2	eα3,α4
Zi Zi J K(')
α1,α2	α3,α4	α1,α3 α2,α4
(')K(')	+2n'K(')	K(')

1
8



where the sample indices are lowered by using the inverse core kernel as a metric, meaning
,α2	≡
,α2),(α3,α4)	≡
Σ
α1,...,α4
α1 ,α01
α4,α04
,α02 ),(α03 ,α04 )
(11)
(12)
Using the above hack, we can evaluate the activation correlations by straightforward algebra with
一 you guessed it - Wick,s contractions. As the Gaussian integral is diagonal in the neuron index
i, we just need to disentangle cases with repeated and unrepeated neuron indices. The solution for
this exercise is in Appendix B: this is the most cumbersome algebra in the paper and the ability to
perform it certifies the graduation from the magical school of Wick’s crafts and wiZardly cumulants.
4The nontrivial parts of the inductive proof for Equations (KS) and (V) are to show (i) that the right-hand
side of Equation (10) is finite as n → ∞, (ii) that the leading contribution of Equation (9) is the Gaussian-
process kernel, and (iii) that higher-point connected preactivation correlation functions are all suppressed by
O (n⅛∙), all of which are verified in obtaining the recursive equations. See Appendix B for a full proof.
4
Under review as a conference paper at ICLR 2020
3.2	Master recursive flow equations
α α	αα
Denoting the GaUssian integral With the core kernel〈z 1z 2)&)= Kd 2 for a single-neuron
random vector z(') ≡ {za}a=ι ND, and plugging in results of Appendix B into Equations (9) and
(10), We arrive at our master recursion relations
KK('+T) = Cb'+1) + CW+1) hσ(zɑ1 )σ(z.2 )i” ,	(R1)
e('+l) 2),(α3,α4) = [CW+1T hσ(Zɑ1 )σ(Z.2 )σ(Z.3 )σ(Z.4(R2)
— hσ(Zα1 )σ(Z°2睚⑹ hσ(Zα3)σ(Za4)〉跖)
+ 4 (n'-1)	^X	V('α1,α2),(α3,α4)Dσ(ZaI )σ(zɑ2)(zα1 zɑ2- KK⅛α2 )Eκ(')
α01 ,α02 ,α03 ,α04
×
(σ(Za3)σ(Za4)(Za3Za4 - K^")〉太
and
eaι,a2 — n n' ʌ c('+1) 1	e(')	Dc(≈aι)π√*2)(*1 ya2 _ K«1,«2)E
S('+1) = (n-J CW	[2 2-Ja1,a2 V(Z )σ(z )(Z Z - K	)/K(')
1, 2	(R3)
+8 x	瑞晨),(a3,a4)Dσ(ZaI)σ(za2)
a01 ,a02 ,a03 ,a04
×
Za1 Za2Za3Za4 - 2Za1 Za2Ka3,a4 - 4Za1 Za3
0
0
+ Kea1
00	0
+ 2Ka1,a3Ka2
For ' = 1,a special note about the ratio nn`γ is in order: even though no stays constant while nι》
1, the terms proportional to that ratio are identically Zero due to the complete Gaussianity (R0).
The preactivation distribution in the first layer (R0) sets the initial condition for the flow from lower
to higher layers dictated by these recursive equations. Once recursed up to the last layer ` = L, the
resulting distribution of outputs z = z(L) can be succinctly encoded by the probability distribution
e-H[z]
P[Z] = RdzOe-HZ
with the potential H[z] = Ho[z] + eHι[z] + O(e2) where E ≡	《1,
H0[z]
2 X (K (-Dais (XX za1 za2)，and
a1,a2	i=1
H1[z]
-1 X J1,a2 X za1 za2)
a1,a2	i=1
X	Ve (L)
(a1,a2),(a3,a4)
a1,a2,a3,a4
a1 a2
Zi Zi
nL
X Zja3Zja4
j=1
with
≡se(L)	- X	ka3,a4 V(L)	+	nLV(L)	1
- aι,a2	/」	(L)	( (a1,a3),(a2,a4) 丁	2	(a1,a2),(a3,a4)J	.
a3,a4
(D0)
(D1)
(D2)
(13)
—
1
8
By now, the reader should be deriving this through Wick’s contractions without solicitation. It is
important to note that nL is constant and thus EH1[z] can consistently be treated perturbatively.5
5If nL were of order n 1, the potential H would become a large-n vector model, for which we would
have to sum the infinite series of bubble diagrams (Moshe & Zinn-Justin, 2003).
5
Under review as a conference paper at ICLR 2020
4 Interlude: examples
The recursive relations obtained above can be evaluated numerically (Lee et al., 2018) [or sometimes
analytically for ReLU (Cho & Saul, 2009)], which is a perfectly adequate approach: at the leading
order it involves four-dimensional Gaussian integrals at most. Here, continuing the theme of wearing
out Wick’s contractions, we develop an alternative analytic method that works for any polynomial
activations (Liao & Poggio, 2017), providing another perfectly cromulent approach.
For a general polynomial activation of degree p, σ(z) = Pkp=0 akzk, the nontrivial term in Equa-
tion (R1) can be expanded as
p
hσ(Zα1 )σ(Zα2)iκf(') = X a®】＜(Zα1 )k1 (Zα2)k2)跖J	(14)
Each term can then be evaluated by Wick’s contractions and the same goes for all the terms in Equa-
tions (R2) and (R3). Below and in Appendix C, we illustrate this procedure with simple examples.
4.1	Deep linear networks
When the activation function is linear, σ(z ) = z, multilayer perceptrons go under the awe-
inspiring name of deep linear networks (Saxe et al., 2013). Setting C?
αα	αα
1 for simplicity, our recursion relations reduce to K('+i2 = K(j ,
Γ e α1,α3 e α2,α4	I	e α1,α4	e α2 ,α3	I (	n`	∖e (α1,α2),(α3,α4)^l	a ea	ea1 ,α2
[κ(') k(') + κ(') k(') + (n-lj V(`)	卜 and s('+i)
Solving them yields the layer-independent core kernel and zero self-energy
0 and C(^)
eα1 ,α2
κ(')
eα1 ,α2
K(1)
xα1 ∙ xα2
n0
and So1,α2 = 0	(15)
(`)
and the linearly layer-dependent four-point vertex
α2)(α3α4)
eα1 ,α4 eα2,α3
+ K(1)	K(1)	.
(16)
It succinctly reproduces the result that can be obtained through planar diagrams in this special
setup (Dyer & Gur-Ari, 2019). Quadratic activation (Li et al., 2018) is worked out in Appendix C.1.
4.2	ReLU with single input
The recursion relations simplify drastically for the case ofa single input, ND = 1, as worked out in
detail in Appendix C.2. For instance, for rectified linear unit (ReLU) activation with C(') = 0 and
CW) = 2, We obtain the layer-independent core kernel, zero self-energy, and the four-point vertex
(17)
Interestingly, as for deep linear networks, the factor £金(l/n`o) appears again. This factor has also
been found by Hanin & Rolnick (2018), which provides guidance for network architectural design
through its minimization. We generalize this factor for monomial activations in Appendix C.2.1
4.3	Experimental verification: output distributions for a single input
Here we put our theory to the test. For concreteness, take a single black-white image of hand-
written digits with 28-by-28 pixels (i.e. n0 = 784) from the MNIST dataset (LeCun et al., 1998)
without preprocessing, set depth L = 3, bias variance C? = 0, weight variance CW) = CW, and
widths (n0, n1, n2, n3) = (784, n, 2n, 1), and use activations σ(z) = z (linear) with CW = 1 and
max(0, z ) (ReLU) with CW = 2. In Figure 1, for each width-parameter n of the hidden layers
we record the prior distribution of outputs over 106 instances of Gaussian weights and compare
6
Under review as a conference paper at ICLR 2020
it with the theoretical prediction - obtained by cranking the knob from the initial condition (R0)
through the recursion relations (R1-R3) to the distribution (D0-D2). The prior distribution becomes
increasingly non-Gaussian as networks narrow and the deviation from the Gaussian-process prior is
correctly captured by our theory. Higher-order perturbative calculations are expected to systemat-
ically improve the quality - and extend the range - of the agreement. Additional experiments are
performed in Appendix C.3, which further corroborates our theory.
Figure 1: Comparison between theory and experiments for prior distributions of outputs for a single
input. The agreement between our theoretical predictions (smooth thick lines) and experimental data
(rugged thin lines) is superb, correctly capturing the initial deviations from Gaussian processes at
n = ∞ (black), all the way down to n 〜10 for linear activation and to n 〜30 for ReLU activation.
5 Bayesian inference
Let us take off from the terminal point of Section 3: we have obtained the recursive equa-
tions (R0-R3) for the Gaussian-process kernel and the leading finite-width corrections and
codified them in the weakly non-Gaussian prior distributions p[z] (D0-D2) of outputs z ≡
nziα ≡	zi(L)	(xα)o	, dictated by the potential H[z]	=	H0[z]	+	H1 [z]	+ O(2)
i=1,...,nL ;a=1,...,ND
with e ≡ n^∙《1. Examples in Section 4 illustrate that finite-width corrections stay perturba-
tive typically when widh《1. Let us now divide ND inputs into NR training and NE test inputs
as {xα}α=1,...,ND = {xβR}β=1,...,NR ∪ {xγE}γ=1,...,NE; the training inputs come with target outputs
yR ≡ {(yR)iβ}β=1,...,NR;i=1,...,nL . We shall develop a formalism to infer outputs for test inputs a la´
Bayes, perturbatively extending the textbook by Williams & Rasmussen (2006). For field theorists,
our calculation is just a tree-level background field calculation (Weinberg, 1996) in disguise.
Taking the liberty of notations, we let the number of input-data arguments dictate the summation
over sample indices α inside the potential H, and denote the joint probabilities
p[zR] =
e-H[zR]
R dzR e-H[zR ]
and p[zR, zE] =
e-H[zR,zE]
R dzR dzE e-H[zR,zE ]
(18)
Given the training targets yR, the posterior distribution of test outputs are given by Bayes’ rule:
p[zE|yR]
p[yR]
R dzR e-H[zR ]
R dzR dzEe-H[zR,zE ]
e-(H[yR,zE]-H[yR])
(Bayes)
The leading Gaussian-process contributions can be segregated out through the textbook manipula-
tion (Williams & Rasmussen, 2006) [c.f. Appendix D.1]: denoting the full Gaussian-process kernel
in the last layer as
Ke β1 ,β2 Ke β1 ,γ2
Kα1,α2	δ	KRR	KRE
Ki1,i2 = δi1,i2 Keγ1,β2 Keγ1,γ2	,
and the Gaussian-process posterior mean prediction as
(yGP)Y ≡ X(KerκK)[%)β,
ββ
(19)
(GPM)
7
Under review as a conference paper at ICLR 2020
and defining a fluctuation (ZE)Y ≡ (yGP)： + (δzE)γ and a matrix KK∆ ≡ KEE - KERKRRLKre,
Ho [yR, ze] - Ho [zr] = 1 XX (δzE)Y1 (κ¾1)	也)：2 .
i Y1,Y2	Y1,Y2
For any function F , its expectation over the Bayesian posterior (Bayes) then turns into
Z dzEF[zE]p[zE|yR] = Ne De-H1[yR,yEGP+δzE]F[yEGP + δzE]E
(GP∆)
(20)
where the deviation kernel ((δzE)iY1 (δzE)iY2 K ≡ δi1,i2Ke∆Y1,Y2 and the normalization factor
e-H1 [yR,yEGP+δzE] E
K∆
-1
= 1 + O() .
(21)
In particular the mean posterior output is given by
(yE)γ ≡ / dzE (ZE)Y P [zE|yR]
(yGP) γ + Ne <(δzE)Y e-eH1 [yRyGP+^E])
K∆
(22)
=(yGP)： - e(3ZE)Y HI [yR, yGP + SzeDkδ + OD.
Stringing together φα ≡ [(yR)β , (yGP)：], recalling Equation (D2) for Hi, and using Wick's Con-
tractions for one last time, the mean prediction becomes
(yGP)：
(NGPM)
+ e E瓦厂φα1
α1 ,γ1
S
γ1 ,α1
X
Ve(Y1,α2),(α1,α3)Keα2,α3 +	Ve(Y1,Y2),(α1,Y3)Ke∆Y2,Y3
α2 ,α3	Y2 ,Y3
+ ~γ X V>1,γ1),(γ2,γ3)κ∆2 ,γ3 + X V⅛ɑ1),(α2,α3)(一? Ke KaQa + j X φj φα3 ^ # .
Y2 ,Y3	α2 ,α3	j
With additional manipulations in Appendix D, this expression is simplified into the actionable form
that is amenable to use in practice. For illustration, there, we also show a simple preliminary exper-
iment, which indicates the 1/n regularization effect for sufficiently large width n and small amount
of training data NR. This is in line with expectations that finite widths ameliorate overfitting and
that non-Gaussian priors increase the expressivity of neural functions, but additional large-scale
extensive experiments would be desirable in the future.
6 Dreams
In this paper, we have developed the perturbative formalism that captures the flow of preactivation
distributions from lower to higher layers. The spiritual resemblance between our recursive equations
and renormalization-group flow equations in high-energy and statistical physics is highly appealing.
It would be exciting to investigate the structure of fixed points away from the Gaussian asymp-
topia (Schoenholz et al., 2016) and fully realize the dream articulated by Mehta & Schwab (2014)
beyond their limited example of a mapping between two antiquated techniques - the audacious
hypothesis that neural networks wash away microscopic irrelevancies and extract relevant features.
In addition we have developed the perturbative Bayesian inference scheme universally applicable
whenever prior distributions are weakly non-Gaussian, and have applied it to the specific cases of
neural networks at finite widths. In light of finite-width regularization effects, it would be pru-
dent to revisit the empirical comparison between SGD optimization and Bayesian inference at finite
widths (Lee et al., 2018; Novak et al., 2019), especially for convolutional neural networks.
Finally, given surging interests in SGD dynamics within the large-width regime (Jacot et al., 2018;
Chizat et al., 2018; Lee et al., 2019; Cohen et al., 2019; Dyer & Gur-Ari, 2019), it would be natural
to adapt our formalism for investigating corrections to neural tangent kernels, and even nonpertur-
batively aspire to capture a phase transition out of lazy-learning into feature-learning regimes.
8
Under review as a conference paper at ICLR 2020
References
Robert Brown. XXVII. A brief account of microscopical observations made in the months of June,
July and August 1827, on the particles contained in the pollen of plants; and on the general
existence of active molecules in organic and inorganic bodies. The Philosophical Magazine, 4
(21):161-173,1828.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
arXiv preprint arXiv:1812.07956, 2018.
Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Advances in Neural
Information Processing Systems, pp. 342-350, 2009.
Omry Cohen, Or Malka, and Zohar Ringel. Learning curves for deep neural networks: a Gaussian
field theory perspective. arXiv preprint arXiv:1906.05301, 2019.
Ethan Dyer and Guy Gur-Ari. Feynman diagrams for large width networks. In International Con-
ference on Machine Learning workshop: Theoretical Physics for Deep Learning, 2019.
Albert Einstein. Uber die Von der molekularkmetiSchen Theone der Warme geforderte BeWegUng
Von in ruhenden FlUssigkeiten SUsPendierten Teilchen. Annalen der Physik, 322(8):549-560,
1905.
Mario Geiger, Stefano SPigler, ArthUr Jacot, and MatthieU Wyart. Disentangling featUre and lazy
learning in deeP neUral netWorks: an emPirical stUdy. arXiv preprint arXiv:1906.08034, 2019.
Nigel Goldenfeld. Lectures on phase transitions and the renormalization group. CRC Press, 2018.
Boris Hanin and DaVid Rolnick. HoW to start training: the effect of initialization and architectUre.
In Advances in Neural Information Processing Systems, PP. 571-581, 2018.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: convergence and gener-
alization in neUral netWorks. In Advances in Neural Information Processing Systems, PP. 8571-
8580, 2018.
Leo P. Kadanoff. Scaling laWs for Ising models near Tc. Physics Physique Fizika, 2(6):263-272,
1966.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
docUment recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Jaehoon Lee, Yasaman Bahri, Roman NoVak, Samuel S. Schoenholz, Jeffrey Pennington, and
Jascha Sohl-Dickstein. Deep neural netWorks as Gaussian processes. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
B1EA-M-0Z.
Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and
Jeffrey Pennington. Wide neural netWorks of any depth eVolVe as linear models under gradient
descent. arXiv preprint arXiv:1902.06720, 2019.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in oVer-parameterized
matrix sensing and neural netWorks With quadratic actiVations. In Conference On Learning The-
ory, pp. 2-47, 2018.
Qianli Liao and Tomaso Poggio. Theory II: landscape of the empirical risk in deep learning. arXiv
preprint arXiv:1703.09833, 2017.
DaVid J. C. MacKay. Probable netWorks and plausible predictions - a reVieW of practical Bayesian
methods for superVised neural netWorks. Network: Computation in Neural Systems, 6(3):469-
505, 1995.
Alexander G. de G. MattheWs, Mark RoWland, Jiri Hron, Richard E. Turner, and Zoubin Ghahra-
mani. Gaussian process behaViour in Wide deep neural netWorks. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
H1-nGgWC-.
9
Under review as a conference paper at ICLR 2020
Pankaj Mehta and David J. Schwab. An exact mapping between the variational renormalization
group and deep learning. arXiv preprint arXiv:1410.3831, 2014.
Moshe Moshe and Jean Zinn-Justin. Quantum field theory in the large N limit: a review. Physics
Reports, 385(3-6):69-228, 2003.
Radford M. Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks, pp.
29-53. Springer, 1996.
Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Jiri Hron, Daniel A. Abo-
lafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with
many channels are Gaussian processes. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=B1g30j0qF7.
Giorgio Parisi and Francesco Zamponi. Mean-field theory of hard sphere glasses and jamming.
Reviews of Modern Physics, 82(1):789-845, 2010.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, 22(3):400-407, 1951.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. In International Conference on Learning Representations, 2016. URL https:
//openreview.net/forum?id=H1W1UN9gg.
Steven Weinberg. The quantum theory of fields: Volume I: Foundations. Cambridge University
Press, 1995.
Steven Weinberg. The quantum theory of fields: Volume II: Modern Applications. Cambridge
University Press, 1996.
Gian-Carlo Wick. The evaluation of the collision matrix. Physical Review, 80(2):268-272, 1950.
Christopher K. I. Williams. Computing with infinite networks. In Advances in Neural Information
Processing Systems, pp. 295-301, 1997.
Christopher K. I. Williams and Carl Edward Rasmussen. Gaussian processes for machine learning.
MIT Press, 2006.
Kenneth G. Wilson. Renormalization group and critical phenomena. I. Renormalization group and
the Kadanoff scaling picture. Physical Review B, 4(9):3174-3183, 1971.
Anthony Zee. Quantum field theory in a nutshell. Princeton University Press, 2010.
10
Under review as a conference paper at ICLR 2020
A Wicked tricks
Welcome to the magical school of Wick’s crafts and wizardly cumulants. Here is all you need to
know in order to follow the calculations in the paper. In the main text, Wick’s contractions are
used trivially for integrating out biases and weights as straightforward applications of Appendix A.1
while they are used more nontrivially for integrating out preactivations, with concepts of cumulants
reviewed in Appendix A.2 and A.3, culminating in a hack derived in Appendix A.4. For most parts,
we shall forget about the neuron index i for pedagogy and put them back in at the very end.
A.1 Wick’s contractions
For Gaussian-distributed variables z = {zα}α=1,...,N with a kernel Kα,α0, moments
R dze-H0 [z] zα1 zα2	z
hza1 za2 …Zam i	≡ d_____________________
h	iK ≡	RdZe-H0[z]
αN
αm	1	0
-With H0[z] ≡ 2 E Za (K-1)a,a0 Za
α,α0 =1
(S1)
For any odd m such moments identically vanish. For even m, Isserlis-Wick’s theorem states that
hza1 Za2 …Zam iK = E Kak1 ,ak2 …Kakm-1 "	(S2)
all pairing
where the sum is over all the possible pairings of m variables, (k1, k2), . . . , (km-1, km). In general,
there are (m - 1)!! = (m -1) ∙ (m - 3) … 1 SUch pairings. For a proof, see for example Zee (2010).
In order to understand and use the theorem, it is instructive to look at a few examples:
hZa1 Za2 iK = Ka1,a2 ;	(S3)
hZa1 Za2 Za3 Za4 i	= Ka1,a2Ka3,a4 + Ka1,a3Ka2,a4 + Ka1,a4Ka2,a3 ;	(S4)
and
hZa1 Za2 Za3 Za4 Za5 Za6 iK							
=	Ka1,a2	Ka3,a4Ka5,a6	+ Ka1,a3 Ka2,a4 Ka5,a6 + Ka1,a4Ka2,a3Ka5,a6				
+	Ka1,a2	Ka3,a5Ka4,a6	+ Ka1,a3	Ka2,a5Ka4,a6	+ Ka1,a5	Ka2,a3	Ka4,a6
+	Ka1,a2	Ka5,a4Ka3,a6	+ Ka1,a5	Ka2,a4Ka3,a6	+ Ka1,a4	Ka2,a5	Ka3,a6
+	Ka1,a5	Ka3,a4Ka2,a6	+ Ka1,a3	Ka5,a4 Ka2,a6	+ Ka1,a4	Ka5,a3	Ka2,a6
+	Ka5,a2	Ka3,a4Ka1,a6	+ Ka5,a3	Ka2,a4Ka1,a6	+ Ka5,a4	Ka2,a3	Ka1,a6
(S5)
A.2 Connected correlations
Given general (not necessarily GaUssian) random variables, connected correlation fUnctions are de-
fined indUctively throUgh
E[zα1 za2 ∙∙∙ zam]	(S6)
≡ E [za1 Za? •- Zam ] I
≡ E [Z Z	] I connected
+ X E hzak11] …Zak叫 ∣	…E hzak1s] …zakVsSi∣
connected	connected
all sUbdivisions
where the sUm is over all the possible sUbdivisions of m variables into s > 1 clUsters of siZes
(ν1, . . . , νs) as (k1[1] , . . . , kν[11] ), . . . , (k1[s] , . . . , kν[ss]). In order to Understand the definition, it is again
instrUctive to look at a few examples. AssUming that all the odd moments vanish,
E [Za1Za2]	= E[Za1Za2] ∣∣connected and	(S7)
E[Za1Za2Za3Za4]	= E[Za1Za2Za3Za4] ∣∣connected	(S8)
+E [Za1Za2] ∣∣connectedE [Za3Za4] ∣∣connected
+E [Za1Za3] ∣∣connectedE [Za2Za4] ∣∣connected
+E [Za1Za4] ∣∣connectedE [Za2Za3] ∣∣connected .
11
Under review as a conference paper at ICLR 2020
Rearranging them in particular yields
E[zα1zα2zα3zα4] connected	(S9)
= E [zα1zα2zα3zα4]
-E[zα1zα2]E[zα3zα4] -E[zα1zα3]E[zα2zα4] - E [zα1zα4] E [zα2zα3] .
If these examples do not suffice, here is yet another example to chew on:
E [zα1 zα2 zα3 zα4 zα5 zα6]
E [zα1zα2zα3zα4zα5zα6] connected
(S10)
+E[zα1zα2] connectedE[zα3zα4] connectedE[zα5zα6] connected
+ [14 other (2, 2, 2) subdivisions]
+E[zα1zα2zα3zα4] connectedE[zα5zα6] connected
+ [14 other (4, 2) subdivisions]
and hence
E [zα1 zα2 zα3 zα4 zα5 zα6] connected	(S11)
= E [zα1 zα2 zα3 zα4 zα5 zα6]
-{E[zα1zα2zα3zα4]E[zα5zα6] + [14 other (4, 2) subdivisions]}
+2{E[zα1zα2]E[zα3zα4]E[zα5zα6] + [14 other (2, 2, 2) subdivisions]} .
We emphasize that these are just renderings of the definition (S6). The power of this definition will
be illustrated in the next two subsections.
A.3 Hierarchical clustering
We often encounter situations with the hierarchy
E [zα1 …zɑm] Lnected = O(≡ W )	(S12)
where 1 is a small perturbative parameter and here again odd moments are assumed to vanish.
Often comes with the hierarchical structure is the asymptotic limit → 0 where
E[zα1zα2] =Kα1,α2 +Sα1,α2 +O(2)	(S13)
with the Gaussian kernel Kα1,α2 at zero and the leading self-energy correction Sα1,α2 . Let us also
denote the leading four-point vertex
E[zα1zα2zα3zα4] connected = Vα1,α2,α3,α4 + O(2) .	(S14)
For instance this hierarchy holds for weakly-coupled field theories - from which We are importing
names such as self-energy, vertex, and metric - and, in this paper, such hierarchical structure is
inductively shown to hold for prior preactivations z(') with E = in the regime nι,..., nL-ι 〜
n`-1
n》1. Note that, by definition, Kα1,α2 and Sα1,α2 are symmetric under αι 什 α2 and Vα1,α2,α3,α4
is symmetric under permutations of (α1, α2, α3, α4).6
6In the main text the connected four-point preactivation correlation functions are symmetric under the per-
mutations of four (sample, neuron) indices, {(α1, i1), (α2, i2), (α3, i3), (α4, i4)}.
12
Under review as a conference paper at ICLR 2020
With the review of connected correlation functions passed us, we can now readily see that
E [zα1 ∙ ∙ ∙ zαm]	(CLUSTER)
=E [zα1 zα2]…E [zαm-1 zαm] + {[(m - 1)!! - 1] other pairings}
+ E [zɑ1 zα2zα3zα4] IcOnnectedE [zα5zα6]…E [zαm-1 zam]
+ m × (m - 5)!! - 1 other (4, 2, 2, . . . , 2) clusterings + O(2) .
=Kα1,α2 ∙ ∙ ∙ Kαm-1,αm + {[(m - 1)!! - 1] other pairings}
+ ES a1,α2 K a3,α4 ... K am-ι,αm
m × (m-3)!!-1
other such clusterings
+ e Va1,α2,α3,α4Ka5,α6 ... Kam-ι,αm
m4	× (m - 5)!! - 1
other (4, 2, 2, . . . , 2) clusterings + O(E2) .
=hza1 ∙∙∙ Zam iκ
+ ESa1,a2 hza3 …Zam iκ +
m2	-1
+ C Va1,α2,α3,α4 {工仪5 ... Zam〉K +
other self-energy contractions
- 1 other vertex contractions + O(E2) .
m
4
where in the last equality Wick’s theorem was used backward.
A.4 Combinatorial hack
So far we have reviewed the standard technology of Wick’s contractions, connected correlation func-
tions, and all that. Here is one sorcery, which lets Us magically evaluate E [za1 …Zam ] by mindless
repetitions of Wick’s contractions. Throughout, we shall assume the hierarchical structure (S12),7
which is the inductive assumption in the main text, and start from its consequence (CLUSTER).
Below, let us use the inverse kernel K-1 a1,a2 as a metric to lower indices:
First note that
E (KT)aι,a1 (KT)a2,a2 S^1,a2 and
a01 ,a02
a1 ,a01
Va01 ,a02 ,a03 ,a04
(S15)
(S16)
(za1 …Zam
Σ
a1 ,...,a4


X Sa1,a2 (za1 Za2 Khz hza1 …ZamiK
a01,a02
+2 (Zalza% (za2za2,K hza3 …Zamiz +
m2	-1
other (α1, α2)
E Sa1,a2 Ka1,a2	hza1 …Z,
a01,a02
+2Sa1,a2hza3 …Zam iz + ∖	[,
amiz
m
2
- 1 other (α1, α2)
7More precisely, We shall only use the weaker assumption that E [zα1 •…Zam] Connected = O(e2) for m ≥ 6
along with Equations (S13) and (S14).
13
Under review as a conference paper at ICLR 2020
where the symmetry aι 什 α2 of Sa1,a2 Was used. Hence, defining
OS [z] ≡ 2 X Sa1,a2 (za1 Za2 - K4 M
2 a01,a02
(OS’)
we obtain, for one term in Equation (CLUSTER),
m
2
eSa1,a2 hza3 …Zamiκ +
- 1 other (α1 , α2 )
(S17)
E hzαι …Zam OS [z]iκ .
(S18)
The similar algebraic exercise renders the other term in Equation (CLUSTER) to be
with
E Vaι ,a2,a3,a4 (zθ5 …Zam)	+
- 1 other (α1 , α2 , α3 , α4 )
(S19)
e hzα1 …ZamOv[z]iK
(S20)
1	0	0	0	0	0	0	00	00	00
OV[z] ≡ — E	Va0 ,a0 ,a0 ,a0 (za1 Za2 Za3 Za4 - 6za1 Za2 K a3,a4 + 3K a1,a2 K a3,a4
a1,...,a4
(OV’)
In summary, for any function F [z] of random variables Zα
E{F[z]}= hF[z]iK+hF[z]OS[z]iK+hF[z]OV[z]iK+O(2).
(HACK’)
In order to get the expressions used in the main text at the `-th layer, we need only to put back neuron
indices i by replacing α → (α, i), identify E = η1^, and use the inductive assumptions (KS)
E ziα11ziα22	= δi1,i2
ara-a ,a2 I	Da1,a2 _LC
K(')	+ n— S(')	+ O
and (V)
E Zia11Zia22Zia33Zia44connected
n`-1
δi1 ,i2 δi3,i4
,α2),(α3,α4)
i2,i4
,α4),(α2,α3)
+O
,α3),(α2,α4)
m
4
1
The operators in Equations (OS’) and (OV’) then become
OS [z]
2 X Sa?,a2
α1,α2
(X ZIa za) -n'K0T2
and
OV [z]
8	^X	V( a1,a2),(a3,a4)
α1,α2,α3,α4
×
n`	n`	n`
(X zθ1 zθ] (X VZM4 ∖ - 2n' (Xzθ1 za]K⅞,a4
堂 za3	Kor4 + n2K



α1,α2 α3,α4	α1,α3 α2,α4
(')K(')	+2n'κ(')	K(')
i.e., the operators in Equations (OS) and (OV) in the main text.
14
Under review as a conference paper at ICLR 2020
B Full condensed proof
In this Appendix, we provide a full inductive proof for one of the main claims in the paper, stream-
lined in the main text. Namely, we assume at the `-th layer that Equations (KS) and (V) hold and
that all the higher-point connected preactivation correlation functions are of order O (n⅛) - which
are trivially true at' = 1 - and prove the same for the (' +1)-th layer. We assume the full mastery
of Appendix A or, conversely, this section can be used to test the mastery of wicked tricks.
First, trivial Wick’s contractions yield
广》('+I)ɑ1,...,α2m	/oɔ i ∖
i1 ,...,i2m
m
M,i2 …Mm-i£[0^^
k=0
XJ 1	X	H (')ɑι,α2 ,...,α2k-ι,α2k	,	ΓAmA 1 Cthe rll
× n n	匚	H jι,jι,...,jk,jk	+ IkJ-1 others	j
` j1 ,...,jk =1
+ [(2m -	1)!! -	1 other pairings] .
Studiously disentangling cases with different numbers of repetitions in neuron indices (j1 , . . . , jk),
We notice that at order O (n), terms without repetition or with only one repetition contribute, finding
n`
1	H (')a1,α2,...,α2k-1,a2k
k k ×	j1,j1,...,jk ,jk
` j1,...,jk=1
[(σ(Zαι)σ(Zα2)i% …hσ(zα2k-1 )σ(Zα2k))和]
(S22)
+* { [hσ(Zα1 )σ(Zα2)σ(Zα3)σ(Zα4睚⑹-{σ(Zα1 )σ(Z"2)〉及© (σ(Zα3)σ(Zα4)〉&/
× [hσ(Zα5)σ(Zα6)iκ(') …hσ(Zα2k-1 )σ(Zα2k)》&/
+
+六件建)σ(zα2>”冲一)σ(zα2k)] {OS[z]+Ov[叫〉K(')
+O
1
2
where we used the inductive hierarchical assumption at the `-th layer, i.e., its consequence (HACK)
and denoted a single-neuron random vector z(') = {Zα}α=ι	ND and the Gaussian integral with
一	一	_     二〜_C，C-	. , , .	_ . _	「 r
the core kernel〈z 1Z 2)及名)=K(j 2. Plugging in expressions (OS,OV) for operators Os,v [z],
([σW1 )σ(zα2)…σ(zα2k-1 )σ(zα2k)] OS [z])Kg
(S23)
1 X S.1,α2
α01,α02
σ(Zα1 )σ(Zα2) (zα1Zα2 - K"、&\京
× hσ(Zα3)σ(Zα4鹿⑼…hσ(Zα2k-1 )σ(Zα2k睚⑹ + [(k - 1) others] |
15
Under review as a conference paper at ICLR 2020
and
<[σ(zα1 )σ(zα2)…σ(zα2k-1 )σ(zα2k)] Ov[z])心)
(S24)
1
8
α03 ,α04
×
0
0
α
α
1,
2,
zα1 )σ(zα2)

z
。1 Za2 Za3 Z,
0
0
0
α04
-2Za1

z
α02

0
-4Za1 Zα3

0

0
+Kα01,
0
α02

0
1
×
hσ(Zα3 )σ(Z"4 )〉氏')∙•.
+ 4
α
0
1,
α
0
2,
α
0
3,
α
0
4
×
σ(Zα1 )σ(Zα2)

z
0
α01
~(
z
0
α02
×
0
0
0
+ 2Kα01 ,α03 Kα02


hσ(Zα2k-1 )σ(Zα2k)〉及') + [(k - 1) others]
)
—
hσ(Zα5 )σ(Z"6 鹿⑼…
KMW)〉公 (σ(Zα3)σ(Zα4) (zα3Zα4
hσ(Zα2k-1 )σ(Zα2k )〉及') +
）〉k（`）
0
0
- Kα03 ,α04

1 others
)
As special cases, we obtain expressions advertised in the main text to be contained in this Appendix:

n`
and
Aα1,α2
1
≡ n1; EHaj
j
1
,α2
(S25)
+——
n`-1
Be (α1,α2),(α3,α4)
ɪʃ
n`
1 X sαi),α2 Dσ(Zɑ1 )σ(zɑ2)铲
α
0
1,
α
0
2
0
1
za2 - K名尾))

K(')
1
+ 8
×
α
z


0
1,
X	ee(a1,a2),(a3,a4)"闭浮2 )
α
0
2,
α
0
3,
α
0
4
0
α01
z

000
产2 Za3Za4
-2Za1 Za2Ka3，a4
-4Za1 Za3Ka2,a4
0
+Kα01,
α02

1
n
n`
1
）〉
0
0
Kα03 ,α04

0
0
0
0
+ 2Kα01 ,α03Kα02,α04



+O
n2
n`
ΣΓ H (')a1,a2,a3,a4
H j1 ,j1 ,j2 ,j2
j1 ,j2 =1
_ H(')a1,a2H(2)a3,a4^∣
- H j1 ,j1 H	j2 ,j2
(S26)
hσ(Zα1 )σ(Zα2 )σ(Zα3 )σ(Zα4 )〉

K(')
-hσ(Za1 )σ(Za2 )iκ(')
hσ(Zα3 )σ(Zα4 )〉

K(')
1
+4
n`
n`-1
α
0
1,
α
0
2,
α03 ,α04
(σ(Za1 )σ(za2 )(za
0

×
<σ(Zα3 )σ(zα4 )(zα3 zα4
) k
0
0
- Kα01 ,α02

)EK(J+O a).
0
0
- Kα03 ,α04

16
Under review as a conference paper at ICLR 2020
Assembling everything,
G ('+1)ɑι,…,α2m
i1 ,...,i2m
m
δil,i2 …δi2m-i,i2m Y [或+1) + CW^ A"2%— 1	]
k=1
+ [(2m - 1)!! - 1 other pairings]
m
+ δi1,i2 …δi2m-1,i2m B("1，。2 )，("3，。4 ) Y ^('^+^ + C^ 加注-…]
k=3
(2m - 5)!! - 1 other (4, 2, 2, . . . , 2) clusterings
(S27)
In particular,
C('+1)a1，ɑ2
G	i1，i2
G ('+1)ɑ1，"2，"3，"4
i1 ，i2 ，i3 ，i4
connected
Si* [或+1) + cW+1')Aeα^α2i + O (J) ,	(S28)
Si1，i2 Si3，i4 B(a1，a2)，(a3，a4) + Si1，i3 δi2，i4 B ("1，"3 )，("2，"4)
+ δi1"4 Si2，i3 B(a1，a4)，(a2，a3) + O
G ('+1)ɑ1，"2，…Sm-Igm I
i1，i2，...，i2m-1，i2m connected
for 2m ≥ 6 .
and
(S29)
completing our inductive proof. Note that B(a1，a2)，(a3，a4) = O 信).
Nowhere in our derivation had we assumed anything about the form of activation functions. The
only potential exceptions to our formalism are exponentially growing activation functions - which
We have never seen in practice - that would make the Gaussian integrals unintegrable.
17
Under review as a conference paper at ICLR 2020
C Bestiary of concrete examples
C.1 Quadratic activation
Let us take multilayer perceptrons with quadratic activation, σ(z) = z2, and study the distributions
of preactivations in the second layer as another illustration of our technology. From the master
recursion relations (R1-R3) with the initial condition (R0), Wickology yields
—_ _
eα1,α2
K(2)
Cb(2) + CW(2)
eα1,α1 eα2,α2	eα1,α2 eα1,α2
K(1)	K(1)	+ 2K(1)	K(1)
,α2),(α3,α4)
[cW)i2
'~ ' ~ _ _
α1,α1	α3,α3
=2 K(1)	K(1)
α4	2
,
'~	' ~ _ _
α1,α1	α4,α4
+ K(1)	K(1)
α3 2
(S30)
(S31)
α4	2
α3	2
eα2,α2 eα3,α3
+ K(1)	K(1)
eα2,α2 eα4,α4
+ K(1)	K(1)
+8
_ _ ~ _ _ ~ _ _ ~ _ _ ~ _ _ ~ _ _ ~ _ _ ~ _ _
α1,α1	α2,α3	α3,α4	α4,α2	α2,α2	α3,α4	α4,α1	α1,α3
(1)	K(1)	K(1)	K(1)	+ K(1) K(1) K(1) K(1)












~
~
~
~
~
~
~
~
ʃ _ _ ' - _ _ ' ~ _ _ ' ~ _ _ ' ~ _ _ ' ~ _ _ ' ~ _ _ ' ~ _ _
α3,α3	α4,α1	α1,α2	α2,α4	α4,α4	α1,α2	α2,α3	α3,α1
+ K(1) K(1) K(1) K(1)	+ K(1) K(1) K(1) K(1)
+ 16 Ke (
~
~
~
~
~
~
~
]
_ _ ʃ _ _ ' ~ _ _ ' ~ _ _ ' ~ _ _ ' ~ _ _ ' ~ _ _ ' ~ _ _
α1,α2	α1,α3	α2,α4	α3,α4	α1,α2	α1,α4	α2,α3	α3,α4
(1)	K(1)	K(1)	K(1)	+ K(1) K(1) K(1) K(1)
~
~
~
ʃ _ _ ' - _ _ ' ~ _ _ ' ~ _ _
α1,α3 α1,α4	α2,α3 α2,α4
+ 16K(1) K(1)	K(1)	K(1)
and
—_ _
eα1 ,α2
S(2)
=0 .
(S32)
where Ka),α2 = CbD + CW) ∙ (Xanxa2). These expressions are used in Appendix D.2 for the
experimental study of finite-width corrections on Bayesian inference.
C.2 Details for single-input cases
The recursive relations simplify drastically for the case of a single input, ND = 1. Setting C(') = 0
for simplicity and dropping α index, our recursive equations reduce to
K（'+i）=cW+1）Q（z）］2〉双 J
(S33)
⅛1 fD≡‰ J+“ 三 d—K 八
K")【(吟]2〉" ) 4 Q-J[S ⑶]2E Kj' J
V(')	.
tz:t— , and
Ke 2 ,
K(')
(S34)
S('+1)	1
-----=—
Ke ('+1)	2
(S(Z)]2 z%	ʌ
κ(')	- 1 I
3σ(Z)I2〉」」
S(')
Ke(')
(S35)
1
+ 8
I G(Z)]2 Z4EK2 D[σ(Z)]2 Z2Ee \
；------L⅛ - 6 ;----------+ 3
D[σ(Z)]2E e	K'	D[σ(Z)]2E e	K`
Ke (`)	Ke (`)
Ve(')
K2
K(')
18
Under review as a conference paper at ICLR 2020
C.2.1 Monomials with single input
For monomial activations, σ(z) = zp, such as in deep linear networks (Saxe et al., 2013) and
quadratic activations (Li et al., 2018),
力 κ('+i)-	=[(2p - 1)!!-W+1		i Ke p K k('),		
Yk'+1) _ 	- k('+1)	((4p - 1)!! - [[(2p - 1)!!]2		1 + p2	(U ) n`-1	V(') K 2 k(')
~ s('+1) 	= k('+1)	;(念)	Γ ~ s(') p—^— .κ('	+ P(P - + —2^	D VL # %」	.
and
In particular the four-point vertex solution is given by
1	V(')
ne-ip2('T) Ke2
(`)
(S36)
(S37)
(S38)
(S39)
The factor (p`, nɪ`rj generalizes the factor (p`, n^) for linear and ReLU activations. Fol-
lowing Hanin & Rolnick (2018), this factor guides us to narrow hidden layers as we pass through
nonlinear activations.
C.2.2 ReLU with single input
ReLU activation, σ(z) = max(0, z), can also be worked out for a single input through Wick’s
contractions, noting that the Gaussian integral is halved, yielding
〜	-C('+1)"〜
K('+1)=	K(`),
⅛ = 5 +
κ2'+i)
and
s('+1)
力
k('+1)
Setting -W) = 2 for simplicity, these equations can be solved, leading to
(S40)
(S41)
(S42)
(S43)
(S44)
(S45)
C.3 More experiments on output distributions
Here is an extended version of experiments in Section 4.3. As in the main text, take a single black-
white image of hand-written digits from the MNIST dataset as an n0 = 784-dimensional input,
without preprocessing. Set bias variance —b') = 0, weight variance -W) = -W, and use activations
σ(z) = Z (linear) with -W = 1, σ(z) = z2 (quadratic) with -W = 1 and max(0, Z) (ReLU) with
-W = 2. For all three cases, we consider both depth L = 2 with widths (n0, n1, n2) = (784, n, 1)
and depth L = 3 with widths (n0, n1, n2, n3 ) = (784, n, 2n, 1). As in Figure 1, in Figure S1,
for each width-parameter n of the hidden layers we record the prior distribution of outputs over
106 instances of Gaussian weights and compare it with the theoretical prediction. Results again
corroborate our theory.
19
Under review as a conference paper at ICLR 2020
Figure S1: Comparison between theory and experiments for prior distributions of outputs for a
single input. Our theoretical predictions (smooth thick lines) and experimental data (rugged thin
lines) agree, correctly capturing the initial deviations from the Gaussian processes (black, n = ∞),
at least down to n = n? with n?〜10 for linear cases, n?〜30 for ReLU cases and depth L = 2
quadratic case, and n?〜100 for depth L = 3 quadratic case. This also illustrates that nonlinear
activations quickly amplify non-Gaussianity.
20
Under review as a conference paper at ICLR 2020
D Finite-width corrections on Bayesian inference
D.1 Actionable expression
In order to massage Equation (NGPM) into an actionable form, first playing with the metric inver-
Sions and defining。g ≡ Pa，(K-1)ɑι,α0Φ , the mean prediction becomes
e (α0,α2),(α1,α3)
V(L)
(S46)
×
nL e(α0,α1),(α2,α3)
+ T Y(L)	J
1	e (α0,α1),(α2,α3)
+ 2	V(L
α2,α3
This expression simplifies drastically through the identity
KRE
KeEE
+ KeRR KeRE Ke∆ KeER KeRR
-Ke∆-1KeERKeR-R1
-KeR-R1KeREKe∆-1
Ke∆-1
(S47)
which can be checked explicitly, recalling K∆ ≡ KEE - KER KR-R1 KRE. Incidentally, this iden-
tity can also be used to prove Equation (GP∆). Now equipped with this identity, recalling
φa ≡ [(yR)β,	(yGP)γ],	WenotiCethat &,产1	= PeO	(KRR1L	(yR)β0	and &,外=0.	Similarly
β1 ,β
(Ke-1β2,β3-Pγ2,γ3	(Ke-1β2,γ2Ke∆γ2,γ3	(Ke-1γ3,β3	=	(KeR-R1β2,β3	and other components
[i.e. With one or both of training components (β2, β3) replaced by test components γ] vanish. Equa-
tion (S46) thus simplifies to
(yGP)： + e X KkYΦ3 (KT)	a "Sθ0)βl + 1 X V(L0,βl),(β2,β3) (X电商万力
β1,γ1 ,α0	1, 0	β2,β3	j
(S48)
-X (V(a0,β2),(β1,β3) + nLγ(a0,βι),(β2,β3)) (K-I)
β2,β3	2	β2,β3
Finally, denoting the matrix inside the parenthesis to be
Aa0,β1 ≡S00)βl + 1 X	V(L0,β1),(β2,β3) (X φj,β2 φj,β3 )	(NGPM’)
β2,β3,β20 ,β30	j
-X (V(L0,β2),(β1,β3) + nLV(L0,β1),(β2,β3)) (KrV),
β2,β3	2	β2,β3
and noticing Pγ1 Ke∆γ,γ1 (Ke-1)	= - (KeERKeR-R1)γ and Pγ1 Ke∆γ,γ1 (Ke-1)	= δγγ0,
(NGPM”)
is the mean prediction. Equations (NGPM′)and (NGPM”)with 6①出、
are actionable, i.e., easy to program.
21
Under review as a conference paper at ICLR 2020
D.2 Small-scale experiments
It turns out that for deep linear networks the leading finite-width correction given above vanishes,
and the first correction is likely to show up at higher order in 1/n asymptotic expansion, which
is not carried out in this paper. Here we instead use the L = 2 multilayer perceptron with the
quadratic activation for illustration, plugging Equations (S30,S31,S32) into Equations (NGPM’)
and (NGPM"). Set C、' = 0 and CWW) = 1/3 and Vary e ≡ n^1γ =看.Results in Figure S2
indicate the regularization effects of finite widths, at least when the number of training samples, NR,
is small, resulting in peak performance at finite widths.
Figure S2: Test accuracy for NE = 10000 MNIST test data as a function of the inverse width
= 1/nL-1 of the hidden layer with quadratic activation. For each number NR of subsampled
training data, the result is averaged over 10 distinct choices of such subsamplings. For small numbers
of training data, finite widths result in regularization effects, improving the test accuracy.
22