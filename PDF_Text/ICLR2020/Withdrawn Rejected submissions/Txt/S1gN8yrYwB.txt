Under review as a conference paper at ICLR 2020
Augmented Policy Gradient Methods for
Efficient Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
We propose a new mixture of model-based and model-free reinforcement learning
(RL) algorithms that combines the strengths of both RL methods. Our goal is to re-
duce the sample complexity of model-free approaches utilizing fictitious trajectory
rollouts performed on a learned dynamics model to improve the data efficiency of
policy gradient methods while maintaining the same asymptotic behaviour. We
suggest to use a special type of uncertainty quantification by a stochastic dynamics
model in which the next state prediction is randomly drawn from the distribution
predicted by the dynamics model. As a result, the negative effect of exploiting
erroneously optimistic regions in the dynamics model is addressed by next state
predictions based on an uncertainty aware ensemble of dynamics models. The
influence of the ensemble of dynamics models on the policy update is controlled
by adjusting the number of virtually performed rollouts in the next iteration ac-
cording to the ratio of the real and virtual total reward. Our approach, which we
call Model-Based Policy Gradient Enrichment (MBPGE), is tested on a collec-
tion of benchmark tests including simulated robotic locomotion. We compare our
approach to plain model-free algorithms and a model-based one. Our evaluation
shows that MBPGE leads to higher learning rates in an early training stage and an
improved asymptotic behaviour.
1 Introduction
Reinforcement Learning (RL) can be broadly classified in two categories: model-free RL, which
uses experience from interacting with the environment to learn a value function, and model-based
RL, which uses experience to approximate the environment by a model (Deisenroth & Rasmussen
(2011)). Recently, impressive results were achieved applying model-free RL aproaches to challeng-
ing tasks. A RL agent with a neural network (NN) based Q-value representation was introduced
in Mnih et al. (2015) which was able to reach an equivalent or higher performance on many Atari
games compared to humans by learning a state function based on raw pixels. Further modifications
addressed the problem of Q-value overestimation (van Hasselt et al. (2015)), dropping significant
experience (Schaul et al. (2015)) and extended the approach to asynchronous methods (Mnih et al.
(2016)) to improve the overall performance. Beside learning a value function, it is possible to use
the gradient of the RL objective to update the policy (Peters & Schaal (2006), Sutton & Barto
(2010), Silver et al. (2014), Lillicrap et al. (2015)). Actor-critic methods aim at combining the
strong points of policy gradients and an approximated value function, in which the critic learns
an approximated value function that is used to update the policy of the actor in the direction of its
gradient (Konda & Tsitsiklis (2000), Schulman et al. (2015a), Mnih et al. (2016), Gu et al. (2016)).
In contrast, model-based RL can be significantly more data efficient. It is notable that a dynamics
model is not necessarily globally valid. In guided policy search (GPS), time-varying linear
dynamics models are learned from multiple rollouts with diverse start and goal conditions (Levine
& Koltun (2013a), Levine & Koltun (2013b), Levine & Koltun (2014)). These trajectory samples
are then used to fit a time-varying Gaussian dynamics model leading to a Gaussian distribution over
trajectories, even for unknown dynamics (Levine & Abbeel (2014)). The neural network policy is
trained in a supervised fashion such that local policies are improved minimizing a cost function
bounded by the trust region of the KL-divergence, and the weights of the NN policy are adjusted
to mimick the trajectory distribution. Further improvements extended GPS to a broader robust
1
Under review as a conference paper at ICLR 2020
state space by introducing a new policy representation which is called general motor reflex (GMR)
(Ennen et al. (2018)). In a first step, the state space is encoded to a latent state representation by
a variational autoencoder. Then, a translation model predicts motor reflex parameters in order to
adjust a Gaussian controller resulting in robust trajectories even outside the distribution of training
samples.
However, modelling errors can reduce the efficiency and benefit of model-based RL algorithms
since model imperfections are exploited (Deisenroth & Rasmussen (2011), Schneider (1996),
Atkeson & Santamaria (1997)) known as model-bias. With model-bias being a result of overfitting,
one approach to solve this problem is to incorporate explicit uncertainty estimates by using a
probabilistic dynamics model. Probabilistic inference for learning control (PILCO) (Deisenroth &
Rasmussen (2011)) uses a Gaussian process (GP) to represent the dynamics of the environment.
Such a model provides predictions with true Bayesian uncertainty estimates but suffer from the curse
of dimensionality (Calandra et al. (2014)). Recently, Bayesian neural networks (BNN) received
much attention. Approximate Bayesian techniques, e.g. variational inference (Graves (2011)) or
Monte-Carlo dropout (Gal & Ghahramani (2015)), come at the drawback that correlations between
parameters are not maintained, while scaling abilities of the Bayesian ’gold standard’ inference
method Markov-Chain-Monte-Carlo (MCMC) are limited (Hinton & Neal (1995). Adding a
regularization term to the loss function of a NN, e.g. Tikhonov regularization, permits maximum
a posterior (MAP) estimates of the parameters since the solution of the regularized loss function
is equivalent to the maximum of the posterior density function (Bardsley (2012)). However, the
regularized solution leads to a point estimator of the true posterior density. This insufficiency can
be addressed when sampling from the true posterior distribution using a MCMC method providing
additional information about the shape of the posterior but is computationally more demanding.
Our approach is inspired by recent advances, Model-Ensemble Trust-Region Policy Optimization
(ME-TRPO) (Kurutach et al. (2018)), Model Based Meta Policy Optimization (MB-MPO) (Clavera
et al. (2018)), Model-assisted Boostrapped DDPG (MA-BDDPG) (Kalweit & Boedecker (2017)),
and Model-Based Value Expansion (MVE) (Feinberg et al. (2018)). In ME-TRPO, a set of samples
from the real environment is used to train an ensemble of NNs. In a second iteration, fictitious tra-
jectory rollouts are generated to update the policy using Trust Region Policy Optimization (TRPO)
(Schulman et al. (2015a)), i.e., the policy update is only based on samples from the approximated
dynamics model of the environment. In MB-MPO, the meta policy learning framework ’model
agnostic meta learning’ (Finn et al. (2017)) is used to train a policy on an ensemble of learned
dynamics models. The notion is that the meta-learning frame of the dynamics models is used
to adapt quickly to any new dynamics model within one update step. The overall similarities of
all dynamics models are incorporated in the meta-model that is then adjusted to create the fitted
dynamics model. MA-BDDPG employs the same idea to augment the dataset to achieve a better
data efficiency. In contrast to our approach, MA-BDDPG makes use of an actor-critic method in
which the policy is represented deterministically. Updates in the actor and critic network follow the
advances of Deep Q-Networks by refering to time-delayed target networks. A further difference is
in the procedure for estimating uncertainty, i.e. Kalweit & Boedecker (2017) applies bootstrapping
(Efron & Tibshirani (1998)) to get a distribution over Q-functions while we use an anchored
ensemble of NNs. MVE improves value estimates assuming that an approximated dynamics model
can be used in training a critic up to a depth H in which we are certain about the model accuracy.
Contrary to our approach, MVE defines its dynamics model as a deterministic neural network.
Our new algorithm MBPGE is a model-free RL algorithm that is augmented by a true Bayesian
ensemble of dynamics models to generate fictitious trajectory rollouts to decrease the sample com-
plexity. In contrast to ME-TRPO, we take advantage of a very recent inference advance based on
randomised anchored MAP sampling that allows for a true Bayesian uncertainty estimate of the true
posterior (Pearce et al. (2018)). As a result, our stochastic dynamics model reduces the effect of
policy overfitting as model inaccuracies will be addressed by highly random next state predictions.
We have seen that ill-distributed datasets, i.e., data points of some regions in the state space are
overrepresented while others are underrepresented, might lead to enormous fictitious total rewards
that diverge heavily from those of the real environment samples due to overfitting dynamics mod-
els. We tackle this issue by introducing a ratio of fictitious and real rewards so that our algorithm
2
Under review as a conference paper at ICLR 2020
adjusts naturally to become more model-free if the dynamics model is inaccurate and acts in a more
model-based way if the approximation of the environment is accurate. To summarize, our main
contributions are:
1.	A novel model-free RL algorithm that uses a true Bayesian representation for the stochastic
dynamics model and
2.	a routine that adjusts the composition of the dataset for a policy gradient estimate by
weighting the trajectory samples based on the ratio of the total rewards of the real envi-
ronment and the approximated model.
We derive our approach by reviewing the background in constrained policy gradient methods and
randomized maximum a posterior sampling. Then, we present our augmented model-free reinforce-
ment learning algorithm in detail. Finally, we evaluate our approach on challenging simulated tasks
in a continuous control domain.
2 Preliminaries
We formulate the underlying problem of our tasks as a discrete-time finite-horizon Markov decision
process (MDP), defined by the tuple (S, A, P, r, ρ0 , γ, T ). Here, we assume that our agent acts
in a stochastic environment with continuous state space S ⊆ Rn and continuous action space
A ⊆ Rm. Additionally, we assume that an unknown dynamics model P (st+1 |st, a) is defined
for our environment and we are given a reward function r(st, at). Let ρ0 denote the initial state
distribution, Y is the discount factor, T is the finite horizon and ∏(a|s) denotes our neural network
policy, parameterized by θ, distributed over actions a, and conditioned to states s. The overall aim
is to learn an optimal policy ∏ such that the expected total reward J(∏) = ET〜∏
is maximized, where T = s。,。。,…,aτ-ι, ST denotes a trajectory with s0 〜
st+1 = P (st, at).
PtT=0 rs(st, at)
Po, at 〜∏θ and
2.1	Policy gradient methods
The maximization problem of the RL objective can be solved by performing gradient ascent com-
puting the derivative with respect to the policy parameters θ. The major problem of policy gradient
methods is to find a good estimator for the gradient since the variance of the gradient estimator
increases with the time horizon. Fortunately, a variance reduction scheme for policy gradients, gen-
eralized advantage estimator (GAE) (Schulman et al. (2015b)), reduces the variance significantly
while only introducing a small bias. It is possible to trade-off between bias and variance by using
a λ-return (Sutton & Barto (2010)). Extending the idea of an n-step return, one can weight each
n-step update by λn-1 and normalize the sum by (1 - λ) resulting in
∞
Rtλ = (1 - λ) X λn-1Rtn .	(1)
n=1
The finite horizon version assumes that all rewards after time step T equal 0, leading to
T -t-1
Rtλ = (1 - λ) X λn-1Rtn + λT-t-1RtT-t .	(2)
n=1
Setting λ = 0 results in the biased single step return Rt(1) with low variance, while λ = 1 corre-
sponds to the unbiased Monte-Carlo return Rt(∞) with high variance. GAE can be derived when
estimating the advantage Aπ(st, at) = Qπ(st, at) - Vπ(st) with the λ-return. The gradient of the
objective now turns to
T
g ：= VθE X r
t=。
T
EAtVθ log∏θ (at∣st)
t=。
(3)
E
Usually, we intend to update our parameters only by small changes. However, instead of perform-
ing update steps in parameter space using the Euclidean distance, it is much more convenient to
3
Under review as a conference paper at ICLR 2020
measure closeness between the current policy and the updated policy in terms of distances between
distributions. The intuition behind is that distances between distributions, e.g. Kullback-Leibler
divergence (Kullback & Leibler (1951)) or Hellinger distance (Hellinger (1909)), are invariant to
scaling or shifting. This type of approach is known as natural policy gradients (Kakade (2001),
Sutton et al. (2000), Peters & Schaal (2006)). Trust region policy optimization (TRPO) utilizes this
idea and maximizes its ’surrogate’ objective function by constraining the allowed policy update step
(Schulman et al. (2015a))
max Et	""a"：' At	s.t. Et [KL [∏θ0id (*∣st),∏θ (*|st)]] 5 δ.	(4)
θ	Lπθold (at|st)	」
TRPO builds on the concept of guaranteed monotonic improvement. The notion is that we can
compute an upper bound for the error of diverging policies. Therefore, we can guarantee a policy
improvement as long as we optimize the local approximation within a trusted region. The proof
is given in Schulman et al. (2015a). PPO combines the appealing guarantee of improvement with
a much simpler implementation (Schulman et al. (2017)). Instead of maximizing the ’surrogate’
function in eq. 4, it uses the following clipped objective
J(θ) = Et [min(rt(θ)At, clip(rt(θ), 1 - e, 1 + E)Atl ∙	⑸
Clipping the probability ensures that the probability ratio rt(θ) =	πθ(Fst))is in the interval
πθold (at |st)
[1 - E, 1 + E], while taking the minimum of the clipped and unclipped objective leads to a lower
bound of the objective with the same convenient effect as constraining the ’surrogate’ objective in
TRPO.
2.2	Model-Based Reinforcement Learning with an anchored ensemble of deep
NEURAL NETWORKS
Contrary to model-free RL, model-based RL utilizes the interactions with the environment to learn
an approximated model. Model-based approaches benefit from a significantly lower sample com-
plexity. However, learning an accurate model of the environment can be very challenging for certain
domains. As a consequence, model-based algorithms often show a worse asymptotic behaviour. To
date, there are several diverse approaches to represent the dynamics model and a proper choice is
often crucial. Dynamics model in RL must reliably fit to datasets with a low and high number of
data points: during the first few iterations only a limited number of data points is available, causing
expressive general function approximators, e.g. deep neural networks, to overfit. With progressing
iterations simple function approximators tend to underfit complex system dynamics.
Bayesian regression aims to find the parameters θ of a function y* = fθ(x*), which are likely to
have generated the output Y given input X . This can be achieved by updating a prior distribution
over the parameters of the neural network θo 〜p(θ) by using Bayes Theorem. If it is possible to
compute the posterior distribution
p(θ∣x,y )
p(Y∣X,θ)p(θ)
p(Y |X)
(6)
the posterior distribution of new data points y * can be inferred by marginalizing over θ
p(y * |x*,
X,Y)=
(Y |X, θ)p(θ)dθ .
(7)
Previous work revealed that exploiting uncertainty estimation in RL can increase the sample effi-
ciency dramatically (Deisenroth & Rasmussen (2011)). Gaussian Processes (Rasmussen & Williams
(2008)) perform well in low data regimes but do not scale to complex tasks with high dimensionality
(Calandra et al. (2014)). On the other hand, probabilistic NNs can approximate arbitrarily complex
dynamics but come at the cost that a true Bayesian uncertainty estimate cannot be provided (Gal &
Ghahramani (2015)). The computation of the integral in eq. 7 can in general only be approximated
numerically under a trade-off between accuracy and performance.
A Baysian neural network (BNN) can be approximated by using a diverse ensemble F =
{f1 , ..., fK } of NNs. Ensembling provides an uncertainty estimate in such way that the variance
4
Under review as a conference paper at ICLR 2020
of the ensemble’s prediction is considered to be its uncertainty. Technically, this approach is not
Bayesian (Gal (2016)). This drawback was addressed in Pearce et al. (2018) by regularizing the
parameters about values drawn from a prior distribution which is called randomized anchored max-
imum a posterior sampling. This approach uses Bayes’ rule to compute a multivariate posterior
by
N(μpost, ςPost) H N(μp
rior,
prior
)N(μiike,
Σlike)
(8)
Σ
with N(μprior, Σprior) as multivariate prior and N(μiike, ∑iike) as multivariate likelihood. When
following the MAP approach Mmap = μpost, We can use a parameterized mean of the prior μprior,θ0
so that the true posterior distribution is matched with
μMAP(°O) = (Uike + £pr：or ) ^UikeP^ike + EprLrμprior,θο ) .	⑼
According to Pearce et al. (2018) suitable parameters for θ0 can be found by setting
E [μMAP(θο)] = μpost and V [μMAp(θο)] = Σpost resulting in θο 〜N(μο, ∑ο) with
μ0 = μprior
(10)
Σ0
Σprior
or Σl-ike ≈ Σprior .
(11)
When using NNs in an anchored ensemble, the loss function needs to be modified. The typical loss
function in a neural network is defined as
L = N l∣y - y∣l2 + N I∣r1∕2θ∣∣2
(12)
where y denotes the predictions of the neural net, y is the vector of the corresponding labels, θ is
the flattened vector of NN parameters, and Γ is the diagonal square matrix of a L2 regularization.
Minimizing the loss in eq. 12 results in parameters that are equal to MAP estimates with a zero
mean normal prior. Eq. 12 can be modified in such way that the minimized parameters can be seen
as MAP estimates with non-zero centered priors
Lanchored = N IIy - y∣∣2 + N ||「1/2(。- θO)II2 .	(13)
The proof is provided in Pearce et al. (2018). As a result, randomised anchored MAP sampling with
NN leads to a good approximation of the true posterior. The algorithm for training an anchored
ensemble with deep neural networks is given in Algorithm 1. In the initialization, an ensemble
of K neural networks with different parameters θj,O is created. The parameters are drawn from a
distribution over the network weights θj,ο 〜N(μο, ∑o) with μo according to eq. 10 and ∑o as
presented in eq. 11 (see Alg. 1, 3-8). During training, every individual neural network is trained with
the training data X and the corresponding labels Y. The loss is computed according to eq. 13 (see
Alg. 1, 9-11). Mean and variance of the true posterior distribution are estimated for the ensemble by
1K
y(χ*) = Kffj(X*)
j=1
(14)
1
σ2(χ*) = Iσ2 + κ Efj(χ*) - y)T(fj(χ*) - y),	(15)
j=1
where fj- denotes an individual NN within the set of K NNS of the ensemble, σ2 is an estimate of the
noise of the training data, y(x*) is the predicted mean of the ensemble and σ2 denotes the predicted
variance of the ensemble at test point x*.
We make use of the uncertainty estimate in eq. 15 by sampling the next state prediction from the dis-
tribution predicted by the approximated BNN, i.e. st+ι 〜N(y^(st, at), σ2(st, at)). Consequently,
the rollouts sampled from the learned dynamics model will diverge much in regions of high uncer-
tainty which prevents the policy to overfit to optimistic model inaccuracies. The efficiency of this
approach was empirically proven by Kurutach et al. (2018).
5
Under review as a conference paper at ICLR 2020
Algorithm 1 Randomized MAP sampling with anchored ensembles
1:	input: Training data X & Y, vector test data point x*, prior mean μprior and prior covariance
Σprior, ensemble size K, data noise variance estimate σ2
2:	output: estimate of mean y^ and variance σ^y
# Ensemble Initialization:
3:	γ = 62£Prior
4:	for j = 1 to K do
5:	μ0 = μprior (eq. 10), ς0 = Σprior (eq. 11)
6:	sample θj,o from N(μο, ∑o)
7:	create neural network fj with Γ, θj,0
8:	end for
# Ensemble Training:
9:	for j = 1 to K do
10:	train fj with X, Y, loss in eq. 13
11:	end for
# Ensemble Prediction:
12:	y(X*) = -1 PK=I fj(X*) (eq. 14)
13:	σy(X*) = Iσ2 + 11 Pj=I(fj(X*)- y)T(fj(X*)- y) (eq.15) 3
3 Augmented Model-Free Reinforcement Learning
In this section, we describe our gradient based model-free RL algorithm that uses BNNs with ran-
domized MAP sampling as explained in the previous section. Our motivation is to address complex
and high-dimensional real robotic tasks in continuous action and state spaces. This aim requires
model-free RL algorithms to become more data efficient while maintaining the same asymptotic
behaviour. Our approach, model-based policy gradient enrichment, achieves this goal by amending
model-free RL with an ensemble of deep NN based dynamics models. Using randomized anchored
MAP sampling our approach does not only compute a point estimate of the true posterior distribu-
tion but also accounts for the shape of the posterior distribution (Pearce et al. (2018)). This does not
only provide a true Bayesian uncertainty estimate, but also prevents overfitting in the approximated
stochastic dynamics model.
3.1 The Model-Free Reinforcement learning frame
The first step of our proposed algorithm follows the commonly accepted procedure of model-free
RL algorithms in which we iteratively perform trajectory rollouts to collect data and estimate the
gradient (see Alg. 2, 6-7). Policy improvement is performed following the clipped ’surrogate’
objective shown in eq. 5 (see Alg. 2, 12). Additionally, we use an advantage estimator (Schulman
et al. (2015b)) to reduce variance when estimating the policy gradient. However, we do not discard
the collected samples of the real environment when performing a policy update. Instead, we add
the samples to our dataset D in order to train our stochastic dynamics model. In MBPGE, the
batch of trajectories UPPO needed for each unique policy update is sampled partially from the real
environment (see Alg. 2, 7) and partially from the approximated dynamics model (see Alg. 2, 9).
First, the trajectories τ will be collected from the real environment and added to UPPO and D. In
a second step, the anchored ensemble of NNs is trained based on all trajectory samples of the real
environment (see Alg. 2, 8). The complete algorithm is given in Algorithm 1. This ensemble is used
to collect fictitious trajectories T which are added to UPPO.Afterwards, a standard policy update
according to proximal policy optimization (PPO) (Schulman et al. (2017)) based on the batch UPPO
will be performed. Depending on the composition of sampled trajectories in the sample batch UPPO,
we can effect the influence of the stochastic dynamics model on the policy update. If the learned
dynamics model is inaccurate, i.e. it is corrupted by a high model bias, we want our policy update
to rely more on trajectories from the real environment. Contrary, we can use an accurate dynamics
model to generate the major part of the update samples from the approximated environment to
reduce the sample complexity of PPO. In order to control the amount of trajectories sampled from
the real and approximated environment, we utilize a simple indicator for model inaccuracies. If the
agent achieves on both the real- and approximated environment about the same total reward (i.e.
6
Under review as a conference paper at ICLR 2020
R(τ) = PtT=1 r(st, at)), it indicates that the dynamics model is accurate in the states which are
likely to be explored by the policy. Hence, it is sufficient to sample the major part from the learned
dynamics model. We use this notion to adjust the composition of trajectory samples drawn from the
real and approximated environment by the following heuristic: The number nk of trajectories from
the learned dynamics model is set to
nk = max
min
(IIi
IRi
(16)
n	nk-1	n	nk-1
R(T)=——X R(Ti)	R(T) = L X R(Ti)
nk-1 Aγ	nk-1 个
i=1	i=1
with k being the index of the current PPO iteration. The hyperparameter α ∈ [0, 1] controls the
maximum of trajectories which can be collected from the learned dynamics model. Consequently,
the number of trajectories sampled from the real environment is set to
nk = maχ(Nr - nk , nmin)	(17)
where Nr encodes a fixed number of trajectories utilized for a PPO update and nmin is the minimal
amount of real environment trajectories per PPO update. Algorithm 2 summarizes our approach.
4 Results
In this section, we present the evaluation results of our proposed augmented model-free RL algo-
rithms. The experiments aim to compare the performance of our new method shown in Algorithm 2
to state-of-the-art model-free and model-based algorithms. We used the following algorithms:
•	Trust Region Policy Optimization (Schulman et al. (2015a)): a model-free RL algorithm
with constraint policy gradient update
•	Proximal Policy Optimization (Schulman et al. (2017)): a model-free RL algorithm with
clipped ’surrogate’ objective
•	Model-Based Meta-Policy-Optimization (Clavera et al. (2018)): a model-based RL algo-
rithm that uses an ensemble of dynamics model to quickly adapt to new dynamcis model
with one policy gradient step
The evaluation process is performed on four simulated OpenAI Gym Benchmarking tasks (Brock-
man et al. (2016)), i.e., Hopper, Half-cheetah, Swimmer, and Walker, which are based on the the
MuJoCo physics simulator (Todorov et al. (2012)). A detailed description of the environments
is given in Appendix A.1 and the hyperparameter settings are listed in Appendix A.2. In
Algorithm 2 Model-based Policy Gradient Enrichment
1:	input: Batch size Nr; nmin a; PPO hyperparameters; dynamics model hyperparameters
2:	initialize D as an empty dataset
3:	n0 = Nr
4:	n0 = 1
5:	for k = 0 to Niter do
6:	initialize UPPO as an empty dataset
7:	collect nk trajectories with the current policy π on the real environment and add them to
UPPO and D
8:	train anchored ensemble of NNs on D (Algorithm 1)
9:	collect n trajectories with the current policy ∏ on the approximated environment and add
them to UPPO
10:	compute nk+ι with equation 16
11:	compute nk+1 with equation 17
12:	run a PPO update step with the data in UPPO
13:	end for
7
Under review as a conference paper at ICLR 2020
time step ×ιθ5
(a) Swimmer
time step ×ιθ5
(c) Walker
(b) Hopper
-200
time step ×ιos
(d) Half-Cheetah
samples from real env MIOS
(e) real vs fictitious samples
---MBPGE ------- PPO	TRPO ----- MB-MPO ------ MBPGE-Deteministic
Figure 1: Comparison of learning curves of our approach to other state-of-the-art RL algorithms.
The horizontal axis denotes the number of samples, the vertical axis is the average return. The maxi-
mum trajectory length is set to 200 time steps. The bottom right figure compares the number samples
from the real environment to the total number of used samples for optimization by MBPGE. The
result of MBPGE-Deterministic refers to our algorithm in which a deterministic dynamics model
was in use, i.e. the next state was not randomly sampled from a predicted distribution but instead
chosen to be the point estimate given by the NN.
0	12	3	4
time step × ιθ5
(a) Walker2D: maximum trajectory length = 200
E=I əj BbJO区 DAY
40OT
3000
2000
o 2	4	6	8 io
time step × ιθ5
(b) Walker2D: maximum trajectory length = 1000
Figure 2: Design study on hyperparameter α in eq.16 to manually restrict the maximum possible
number of fictitious data samples. The performance curves are shown for three values, α = 1,
α = 0.8, and α = 0.6. The results show that finding the optimal composition of the dataset for a
policy gradient estimate is non-trivial.
8
Under review as a conference paper at ICLR 2020
our experiments we would like to evaluate how our approach compares to state-of-the-art RL
algorithms in terms of sample complexity and final performance. The results are performed for
a fixed trajectory length of 200 time steps. In total, all algorithms perform 4e5 steps on the
environment. Figure 1 depicts the comparison of our MBPGE algorithm to prior work on pure
policy gradient based RL and MB-MPO that exceeds the performance of ME-TRPO as shown
in Wang et al. (2019). Furthermore, Figure 1 illustrates the total number of samples (the sam-
ples from the real environment plus fictitious samples) our algorithm has processed for optimization.
The results clearly show that MBPGE can outperform the available alternatives. Incorporating fic-
titious samples from a learned stochastic dynamics can dramatically improve the raw performance
of MF-RL. The necessary amount of training samples from the real environment is greatly reduced
while the asymptotic performance of MF-RL algorithms is maintained. The positive effect of the
stochastic dynamics model becomes apparent by the increased learning rate. As a matter of fact, the
averaged return rises significantly faster for MBPGE than for the pure model-free algorithm. We
believe that the increased performance is caused by two reasons. First, having no prior on the actual
task, the agent needs to explore how to distinguish between high and low-rewarded actions. Since
the dataset of the observed experience in an early training stage is simply not accurate enough,
the Baysian uncertainty estimate of the randomized anchored MAP in the anchored ensemble
will have high variance. Consequently, the expected reward under high variance of the fictitious
trajectory samples is very low, even though the mean would be exactly the same as in a deterministic
dynamics model. This permits a policy update in the direction of the highest gradient we are certain
about while avoiding catastrophic failures due to erroneously optimistic predictions. Second, with
progressing data samples the dataset of collected experiences becomes more expressive resulting
in a potentially more accurate dynamics model. Figure 1(e) illustrates that the more certain the
agent is about its dynamics model the more samples it generates in the fictitious environment. The
less challenging the task, the more confident is the agent about its dynamics model and the higher
the factor by which the total number of samples used for policy gradient estimates is upscaled
(Hopper: 10.62, Swimmer: 4.62, Half-Cheetah: 2.44, Walker: 1.94). Even for the most challenging
environment our approach can double the amount of data samples based on the learned dynamics
model. As a result, the raw performance of MBPGE is significantly higher.
Designing a suitable heuristic for estimating the best amount of fictitious data samples is very chal-
lenging. We tested diverse heuristics with different trajectory lengths and found that the heuristic in
eq. 16 performs best (cf. 2). Additionally, we find that the number of fictitious data samples used for
a policy update step is, up to a certain degree, of minor importance for learning speed but effects the
asymptotic behaviour. As a consequence, we introduced a hyperparameter α in eq. 16 to manually
reduce the size of fictitious data samples. Improvement to this inconvenient solution is part of our
future research.
5 Conclusions
In this paper, we introduce model-based policy gradient enrichment, an algorithm that incorporates
a stochastic dynamics model to augment the dataset for estimating the current policy gradient. Our
method takes advantage of randomized anchored MAP that results in an ensemble of neural networks
providing a true Baysian uncertainty estimate. We presented that our approach leads to significantly
faster learning while maintaining the final asymptotic performance of plain model-free RL. We
further show that the stochastic dynamics model is a suitable approach to reduce model bias leading
to an increased learning rate in an early training stage.
Acknowledgments
We thank Maren Bennewitz for helpful discussions and feeback, and Christian Lagemann for com-
ments that greatly improved the manuscript.
9
Under review as a conference paper at ICLR 2020
References
Christopher G. Atkeson and Juan Carlos Santamaria. A comparison of direct and model-based re-
inforcement learning. In IN INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMA-
TION,pp. 3557-3564. IEEE Press,1997.
Johnathan M. Bardsley. Mcmc-based image reconstruction with uncertainty quantification. SIAM
Journal on Scientific Computing, 34(3):A1316-A1332, 2012. ISSN 1064-8275. doi: 10.1137/
11085760x.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Roberto Calandra, Jan Peters, Carl Edward Rasmussen, and Marc Peter Deisenroth. Manifold gaus-
sian processes for regression, 2014.
Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel.
Model-based reinforcement learning via meta-policy optimization. CoRR, abs/1809.05214, 2018.
URL http://arxiv.org/abs/1809.05214.
Marc Deisenroth and Carl Rasmussen. Pilco: A model-based and data-efficient approach to policy
search. In ICML, pp. 465-472, 01 2011.
Bradley Efron and Robert Tibshirani. An introduction to the bootstrap, volume 57 of Monographs
on statistics and applied probability. Chapman & Hall, Boca Raton, Fla., [nachdr.] edition, 1998.
ISBN 978-0412042317.
Philipp Ennen, Pia Bresenitz, Rene Vossen, and Frank Hees. Learning robust manipulation skills
with guided policy search via generative motor reflexes. CoRR, abs/1809.05714, 2018. URL
http://arxiv.org/abs/1809.05714.
Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I. Jordan, Joseph E. Gonzalez, and Sergey
Levine. Model-based value estimation for efficient model-free reinforcement learning. CoRR,
abs/1803.00101, 2018. URL http://arxiv.org/abs/1803.00101.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. CoRR, abs/1703.03400, 2017. URL http://arxiv.org/abs/1703.
03400.
Justin Fu, Sergey Levine, and Pieter Abbeel. One-shot learning of manipulation skills with online
dynamics adaptation and neural network priors. CoRR, abs/1509.06841, 2015. URL http:
//arxiv.org/abs/1509.06841.
Yarin Gal. Uncertainty in deep learning. 2016.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning, 2015.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the Thirteenth Interna-
tional Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine
Learning Research, pp. 249-256, Chia Laguna Resort, Sardinia, Italy, 13-15 May 2010. PMLR.
URL http://proceedings.mlr.press/v9/glorot10a.html.
Alex Graves. Practical variational inference for neural networks. In Proceedings of the 24th In-
ternational Conference on Neural Information Processing Systems, NIPS’11, pp. 2348-2356,
USA, 2011. Curran Associates Inc. ISBN 978-1-61839-599-3. URL http://dl.acm.org/
citation.cfm?id=2986459.2986721.
Shixiang Gu, Timothy P. Lillicrap, Zoubin Ghahramani, Richard E. Turner, and Sergey Levine. Q-
prop: Sample-efficient policy gradient with an off-policy critic. CoRR, abs/1611.02247, 2016.
URL http://arxiv.org/abs/1611.02247.
10
Under review as a conference paper at ICLR 2020
E. Hellinger. NeUe begrUndung der theorie quadratischer formen Von Unendlichvielen
veranderlichen. Journalfur die reine Und angewandte Mathematik(CreUes Journal), 1909(136):
210-271,1909. ISSN0075-4102. doi: 10.1515/crll.1909.136.210.
Geoffrey E. Hinton and Radford M. Neal. Bayesian learning for neural networks. 1995.
Sham Kakade. A natural policy gradient. volume 14, pp. 1531-1538, 01 2001.
Gabriel Kalweit and Joschka Boedecker. Uncertainty-driven imagination for continuous deep rein-
forcement learning. In Sergey Levine, Vincent Vanhoucke, and Ken Goldberg (eds.), Proceedings
of the 1st Annual Conference on Robot Learning, volume 78 of Proceedings of Machine Learn-
ing Research, pp. 195-206. PMLR, 13-15 Nov 2017. URL http://proceedings.mlr.
press/v78/kalweit17a.html.
Vijay R. Konda and John N. Tsitsiklis. Actor-critic algorithms. In S. A. Solla,
T. K. Leen, and K. MUller (eds.), Advances in Neural Information Processing Systems
12, pp. 1008-1014. MIT Press, 2000. URL http://papers.nips.cc/paper/
1786-actor-critic-algorithms.pdf.
S. KUllback and R. A. Leibler. On information and sUfficiency. The Annals of Mathematical Statis-
tics, 22(1):79-86, 1951. ISSN 0003-4851. doi: 10.1214/aoms/1177729694.
Thanard KUrUtach, Ignasi Clavera, Yan DUan, Aviv Tamar, and Pieter Abbeel. Model-ensemble
trUst-region policy optimization. CoRR, abs/1802.10592, 2018. URL http://arxiv.org/
abs/1802.10592.
Sergey Levine and Pieter Abbeel. Learning neUral network policies with gUided policy search
Under Unknown dynamics. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence,
and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27,
pp. 1071-1079. CUrran Associates, Inc., 2014. URL http://papers.nips.cc/paper/
5444-learning-neural-network-policies-with-guided-policy-search-under-unknown-dyna
pdf.
Sergey Levine and Vladlen KoltUn. GUided policy search. In Sanjoy DasgUpta and David McAllester
(eds.), Proceedings of the 30th International Conference on Machine Learning, volUme 28 of
Proceedings of Machine Learning Research, pp. 1-9, Atlanta, Georgia, USA, 17-19 JUn 2013a.
PMLR. URL http://proceedings.mlr.press/v28/levine13.html.
Sergey Levine and Vladlen KoltUn. Variational policy search via trajectory optimiza-
tion. In C. J. C. BUrges, L. BottoU, M. Welling, Z. Ghahramani, and K. Q.
Weinberger (eds.), Advances in Neural Information Processing Systems 26, pp. 207-
215. CUrran Associates, Inc., 2013b. URL http://papers.nips.cc/paper/
5178-variational-policy-search-via-trajectory-optimization.pdf.
Sergey Levine and Vladlen KoltUn. Learning complex neUral network policies with trajectory opti-
mization. volUme 3, 06 2014.
Timothy Lillicrap, Jonathan HUnt, Alexander Pritzel, Nicolas Heess, Tom Erez, YUval Tassa, David
Silver, and Daan Wierstra. ContinUoUs control with deep reinforcement learning. CoRR, 09 2015.
Volodymyr Mnih, Koray KavUkcUoglU, David Silver, Andrei A. RUsU, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis AntonogloU, Helen King, Dharshan KUmaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. HUman-level control throUgh deep reinforcement learn-
ing. Nature, 518:529 EP -, 2015. doi: 10.1038/natUre14236. URL https://doi.org/10.
1038/nature14236.
Volodymyr Mnih, Adria PUigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray KavUkcUoglU. AsynchronoUs methods for deep reinforcement
learning. CoRR, abs/1602.01783, 2016. URL http://arxiv.org/abs/1602.01783.
Tim Pearce, Mohamed Zaki, Alexandra BrintrUp, Nicolas Anastassacos, and Andy Neely. Uncer-
tainty in neUral networks: Bayesian ensembling, 2018.
11
Under review as a conference paper at ICLR 2020
J. Peters and S. Schaal. Policy gradient methods for robotics. In 2006 IEEE/RSJ International
Conference on Intelligent Robots and Systems, pp. 2219-2225, Oct 2006. doi: 10.1109/IROS.
2006.282564.
Lutz Prechelt. Early Stopping — But When?, pp. 53-67. Springer Berlin Heidelberg, Berlin,
Heidelberg, 2012. ISBN 978-3-642-35289-8. doi: 10.1007/978-3-642-35289-8_5. URL
https://doi.org/10.1007/978-3-642-35289-8_5.
Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning.
Adaptive computation and machine learning. MIT Press, Cambridge, Mass., 3. print edition, 2008.
ISBN 0-262-18253-X.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay.
CoRR, abs/1511.05952, 2015.
Jeff G. Schneider. Exploiting model uncertainty estimates for safe dynamic control learning. In
Proceedings of the 9th International Conference on Neural Information Processing Systems,
NIPS’96, pp. 1047-1053, Cambridge, MA, USA, 1996. MIT Press. URL http://dl.acm.
org/citation.cfm?id=2998981.2999128.
John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region
policy optimization. CoRR, abs/1502.05477, 2015a. URL http://arxiv.org/abs/1502.
05477.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. abs/1506.02438, 06
2015b.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/
1707.06347.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In Proceedings of the 31st International Confer-
ence on International Conference on Machine Learning - Volume 32, ICML’14, pp. I-387-
I-395. JMLR.org, 2014. URL http://dl.acm.org/citation.cfm?id=3044805.
3044850.
Richard Sutton, David Mcallester, Satinder Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. Adv. Neural Inf. Process. Syst, 12, 02
2000.
Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. A Bradford
book. MIT Press, Cambridge, Mass., [nachdr.] edition, 2010. ISBN 9780262193986.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based con-
trol. 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033,
2012.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. CoRR, abs/1509.06461, 2015. URL http://arxiv.org/abs/1509.06461.
Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shun-
shi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based rein-
forcement learning. CoRR, abs/1907.02057, 2019. URL http://arxiv.org/abs/1907.
02057.
12
Under review as a conference paper at ICLR 2020
Appendices
A	Experimental Description
A.1 Environments
Our method requires the reward of a time step to be a known function of the form r(st, st-1, at) or
r(st , at). Otherwise, it would be necessary to introduce another regression model for learning the
reward function. Hence, we had to manipulate the default environments slightly as described in the
following.
Hopper:
The basis of our Hopper was the Hopper-v2 from OpenAI Gym. Its observation vector consists
of all angular joint positions and velocities and our only modification was that we extended the
observation vector by the x position in space (pseudo code: x = sim.data.qpos[0]). We utilized the
default reward function r(st, st-ι,at) = xt^∆ttT + 1 - 0.0011%||2 and early termination criterion.
Walker:
The basis of our Walker was the Walker2d-v2 from OpenAI Gym. Its observation vector consists
of all angular joint positions and velocities and our only modification was that we extended the
observation vector by the x position in space (pseudo code: x = sim.data.qpos[0]). We utilized the
default reward function r(st, st-ι,at) = xt^∆ttT + 1 — 0.0011%||2 and early termination criterion.
HalfCheetah:
The basis of our HalfCheetah was the HalfCheetah-v2 from OpenAI Gym. Its observation vector
consists of all angular joint positions and velocities and our only modification was that we extended
the observation vector by the x position in space (pseudo code: x = sim.data.qpos[0]). We utilized
the default reward function r(st, st-ι, at) = Xt-X；-1 — 0.1||atk2 and no kind of early termination
criterion.
Swimmer:
The basis of our Swimmer was the Swimmer-v2 from OpenAI Gym. Its observation vector consists
of all angular joint positions and velocities and our only modification was that we extended the
observation vector by the x position in space (pseudo code: x = sim.data.qpos[0]). We utilized the
default reward function r(st, st-ι, at) = Xt-XtT - 0.0001||。右∣∣2 and no kind of early termination
criterion.
Ant:
The basis of our Ant was the Ant-v2 from OpenAI Gym. Its observation vector consists of all angular
joint positions and velocities as well as some contact related states. Our modification was to remove
the contact states from the observation vector and to add the x position in space (pseudo code: x =
sim.data.qpos[0]) to it. We utilized the reward function r(st, st-ι,at) = xt-XtT + 1 - 0.5|%||2.
The only difference of it to the default reward function is that it has no contact penalty. We
additionally utilized the default early termination criterion of Ant-v2.
A.2 Hyperparameters
If not specified differently, we used the hyperparameters as listed in Table 1 (MBPGE), Table 2
(PPO), Table 2 (TRPO) and Table 3 (MB-MPO). For our plain PPO and TRPO experiments we
have not defined a fixed number of trajectories to collect for a policy update. Instead, we collected
trajectories until the batch contained a certain number of time steps Tmin . The Anchored Ensemble
requires an assumption about the prior distribution. We have set the mean μprior of the Gaussian
prior distribution to zero and its variance Σprior according to the Xavier initialization (Glorot &
Bengio (2010)) (ηin =b fan-in, ηout =b fan-out)
∑ ∙ ∙ ∙ =________2______
Jprior,i,j
ηin,i,j + ηout,i,j
13
Under review as a conference paper at ICLR 2020
within this work. The index i denotes the NN layer and j the unit inside that layer.
PPO	Anchored Ensemble	MBPGE	
λGAE = 0.95 γ = 0.99 M = 64 = 0.2 Adam stepsize: 0.0003	σ2 = 10-5 K=5	Nr = 15 nmin = 1 α=1 walker: α = 0.8
Table 1: MBPGE hyper parameter settings.
PPO
λGAE = 0.95
γ = 0.99
M = 64
=0.2
Tmin = 2048
Adam stepsize: 0.0003
Trpo
λGAE = 0.95
γ = 0.99
max KL: 0.01
Tmin = 2048
Table 2: PPO and TRPO hyper parameter settings.
TRPO	MB-MPO	
λGAE = 0.95 γ=0.99 max KL: 0.01	inner adaption step size α = 0.001 number of meta-optimization steps: 5 number of collected trajectories per algorithm iteration: 20 (real en- vironment) 40 trajectories were collected from the learned dynamics model for an inner adaption step and for the meta-policy optimization. Table 3: MB-MPO hyper parameter settings.
Dynamics Model:
The NNs used for representing the dynamics model in MBPGE and in MB-MPO consisted of 4
hidden ReLU layers with 1000, 900, 800 and 700 units (2244000 parameters). (Fu et al., 2015, p. 5)
described the positive effect of the second order Markov assumption (i.e. the next state depends
on the current- and previous state and action) with regard to the ability of a NN in learning strong
nonlinearities, e.g. contacts. Most challenging robotic manipulation or locomotion problems which
are typically solved by model free RL include contacts, so in the experiments we used NNs of the
form f(st, at, st-1, at-1). A problem with this NN/BNN configuration is the very first prediction
of a roll out in which st-1 and at-1 are not defined. We handled this problem by using a second
NN/BNN f0 which was trained only on the very first roll out transitions {s1, s0, a0} (i.e. a dynamic
model only for predicting s1 = f0 (s0, a0)). f0 consisted of 3 hidden ReLU layers with 50 units
each (7268 parameters).
Furthermore, we trained our dynamics models to predict ∆st = st+1 - st instead of directly pre-
dicting st+1. Additionally, we used the following regularization techniques:
•	Early stopping wit GL criterion as specified in (Prechelt, 2012, p. 58)
•	Normalization of the training dataset so that it has a zero mean and a unit variance
•	Shuffling of the training data
•	Weight decay with λ = 0.0001 (only for MB-MPO)
14
Under review as a conference paper at ICLR 2020
Policy:
For each tested algorithm We utilized a stochastic policy of the form at 〜N(μ(st),σ), in which
μ(st) was the output of a NN (2 tanh layers with 128 units each) and σ was a vector containing the
standard deviation of each action. The network’s parameters were initialized randomly according to
the Xavier initialization (Glorot & Bengio (2010)) and the initial standard deviation vector was set
to σi = 1 ∀ i ∈ {1, . . . , Da} with Da being the dimension of the action vector.
15