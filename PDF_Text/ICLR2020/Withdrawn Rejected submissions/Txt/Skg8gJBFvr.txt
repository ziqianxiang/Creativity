Under review as a conference paper at ICLR 2020
Filling the Soap Bubbles: Efficient Black-Box
Adversarial Certification with Non-Gaussian
Smoothing
Anonymous authors
Paper under double-blind review
Ab stract
Randomized classifiers have been shown to provide a promising approach for
achieving certified robustness against adversarial attacks in deep learning. How-
ever, most existing methods only leverage Gaussian smoothing noise and only
work for `2 perturbation. We propose a general framework of adversarial cer-
tification with non-Gaussian noise and for more general types of attacks, from
a unified variational optimization perspective. Our new framework allows us to
identify a key trade-off between accuracy and robustness via designing smooth-
ing distributions, helping to design two new families of non-Gaussian smoothing
distributions that work more efficiently for '2 and '∞ attacks, respectively. Our
proposed methods achieve better results than previous works and provide a new
perspective on randomized smoothing certification.
1	Introduction
Deep neural networks have achieved state-of-the-art performance on many tasks such as image clas-
sification (He et al., 2016; Lu et al., 2018) and language modeling (Devlin et al., 2019). Nonethe-
less, modern deep learning models have been shown to be highly sensitive to small and adversarially
crafted perturbations on the inputs (Goodfellow et al., 2015), which means a human-imperceptible
changes on inputs could cause the model to make dramatically different predictions. Although many
robust training algorithms have been developed to overcome adversarial attacking, most heuristically
developed methods can be shown to be broken by more powerful adversaries eventually, (e.g., Atha-
lye et al., 2018; Madry et al., 2018; Zhang et al., 2019; Wang et al., 2019). This casts an urgent
demand for developing robust classifiers with provable worst case guarantees.
One promising approach for certifiably robustness is the recent randomized smoothing method (e.g.,
Cohen et al., 2019; Salman et al., 2019; Lee et al., 2019; Li et al., 2019; Lecuyer et al., 2018),
which constructs smoothed classifiers with certifiable robustness by introducing noise on the inputs.
Compared with the other more traditional verification approaches (e.g. Wong & Kolter, 2017; Jordan
et al., 2019; Dvijotham et al., 2018) that exploits special structures of the neural networks (such as
the properties of ReLU), the randomized smoothing methods work more flexibly on general black-
box classifiers and is shown to be more scalable and provide tighter bounds on challenging datasets
such as ImageNet (Deng et al., 2009).
However, the existing randomized smoothing methods can only work against `2 attack, in which the
perturbations are allowed within an `2 ball of certain radius. A stronger type of attack, such as the
'∞ attacks, is much more challenging to defense and verify due to the larger set of perturbations,
but is more relevant in practice.
In addition, all the existing randomized smoothing methods use Gaussian noise for smoothing. Al-
though appearing to be a natural choice, one of our key observations is that Gaussian distributions
is in fact a rather sub-optimal choice in high dimensional spaces, even for `2 attack. This is due
to a counter-intuitive phenomenon in high dimensional spaces (Vershynin, 2018) that almost all of
the probability mass of standard Gaussian distribution concentrates around the sphere of radius one
(and hence “soap bubble” in the title), instead of the center point (which corresponds to the original
input). As a result, the variance of the Gaussian noise needs to be sufficiently small to yield good
approximation to the original classifiers (by squeezing the “soap bubble” towards the center point),
1
Under review as a conference paper at ICLR 2020
which, however, makes it difficult to verify due to the small noise. Further, for the more challenging
'∞ attack, Gaussian smoothing Provably degenerates in high dimensions.
Our contribution We propose a general framework of adversarial certification using non-
Gaussian smoothing noises, based on a new PersPective from variational oPtimization. Our frame-
work re-derives the method of Cohen et al. (2019) as a sPecial case, and is aPPlicable to more general
families of non-Gaussian smoothing distributions and more tyPes of attacks beyond `2 norm. Im-
Portantly, our new framework reveals a fundamental trade-off between accuracy and robustness for
guiding better choices of smoothing distributions. Leveraging our insight, we develoP two new fam-
ilies of distributions for better certification results on '2 and '∞ attacks, respectively. Efficient com-
Putational aPProaches are develoPed to enable our method in Practice. EmPirical results show that
our new framework and smoothing distributions significantly outperform the existing approaches
for both '2 and '∞ attacking, on challenging datasets such as CIFAR-10 and ImageNet.
2	Related works
Empirical Defenses Since Szegedy et al. (2013) and Goodfellow et al. (2015), many previous
works have focused on utilizing small perturbation δ under certain constraint, e.g. in a 'p norm ball,
to attack a neural network. Adversarial training (Madry et al., 2018) and its variants (Kannan et al.,
2018; Zhang & Wang, 2019; Zhai et al., 2019) are the most successful defense methods to date,
in which the network is forced to solve a mini-max game between the defender and attacker with
adversarial examples as data augmentation. However, these empirical defense methods are still easy
to be broken and cannot provide provable defense.
Certified Defenses Unlike the empirical defense methods, once a classifier can guarantee a con-
stant classification within a local region, it is called a robust certificate. Exact certification methods
provide the minimal perturbation condition which leads to a different classification result. This
line of work focus on deep neural networks with ReLU-like activation that makes the classifier a
piece-wise linear function. This enables researchers to introduce satisfiability modulo theories (Car-
lini et al., 2017; Ehlers, 2017) or mix integer linear programming (Cheng et al., 2017; Dutta et al.,
2018). Sufficient certification methods take a conservative way and try to bound the Lipschitz con-
stant or other information of the network (Jordan et al., 2019; Wong & Kolter, 2017; Raghunathan
et al., 2018; Zhang et al., 2018). However, these certification strategies share a drawback that they
are not feasible on large-scale scenarios, e.g. large enough practical networks, large enough datasets.
Randomized Smoothing To mitigate this limitation of previous certifiable defenses, improving
network robustness via randomness has been recently discussed (Xie et al., 2018; Liu et al., 2018).
In certification community, Lecuyer et al. (2018) first introduced randomization with technique in
differential privacy. Li et al. (2019) improved their work with a bound given by Renyi divergence.
In succession, Cohen et al. (2019) firstly provided a tight bound for arbitrary Gaussian smoothed
classifiers based on previous theorems found by Li & Kuelbs (1998). Salman et al. (2019) combined
the empirical and certification robustness, by applying adversarial training on randomized smoothed
classifiers to achieve a higher certified accuracy. Lee et al. (2019) focused on '0 norm perturbation
setting, and proposed a discrete smoothing distribution to beat the Gaussian distribution baseline.
Similar to (Lee et al., 2019), we also focus on finding a suitable distribution to trade-off accuracy
and robustness for different types of adversarial attacks, such as '2 and '∞.
3	Black-box Certification with variational Optimization
We start with introducing the background of adversarial certification problem and and randomized
smoothing method. We then introduce in Section 3.1 our general framework of adversarial cer-
tification using non-Gaussian smoothing noises, from a new variational optimization perspective.
Our framework includes the method of Cohen et al. (2019) as a special case, and reveals a critical
trade-off between accuracy and robustness that provides important guidance for better choices of
smoothing distributions in Section 4.
2
Under review as a conference paper at ICLR 2020
Adversarial Certification We consider binary classification of predicting binary labels y ∈ {0, 1}
from feature vectors x ∈ Rd for simplicity. The extension to multi-class cases is straightforward,
and is discussed in Appendix D. Assume f] : Rd → [0, 1] is a pre-trained binary classifier (] means
the classifier is given), which maps from the feature space Rd to either the class probability in
interval [0, 1] or the binary labels in {0, 1}. In the robustness certification problem, a testing data
point x0 ∈ Rd is given, and one is asked to verify if the classifier outputs the same prediction when
we perturb the input x0 arbitrarily in B, a given neighborhood of x0 . Specifically, let B be a set
of possible perturbation vectors, e.g., B = {δ ∈ Rd : kδkp ≤ r} for `p norm with a radius r. If
the classifier predicts y = 1 on x0, i.e. f] (x0) > 1/2, we want to verify if f] (x0 + δ) > 1/2
holds for any δ ∈ B. In this paper, we consider two types of attacks, including the `2 attack
B'2,r def {δ ： ∣∣δ∣∣2 ≤ r}, and the '∞ attack B'∞,r def {δ : kδ∣∣∞ ≤ r}. More general 'p attack can
also be handled by our framework but is left as future works.
Black-box Certification with Randomness Directly verifying f ] heavily relies on the smooth-
ness of f], which has been explored in a series of recent works (Lecuyer et al., 2018; Wong &
Kolter, 2017). These methods typically depend on the special structure property (e.g., the use of
ReLU units) of f], and thus can not serve as general purpose algorithms. We are instead interested
in black-box verification methods that could work for arbitrary classifiers. One approach to en-
able this, as explored in recent works (Cohen et al., 2019; Lee et al., 2019), is to replace f] with a
smoothed classifier by convovling with Gaussian noise, and verify the smoothed classifier.
Specifically, assume π0 is a smoothing distribution with zero mean and bounded variance, e.g.,
π0 = N(0, σ2). The randomized smoothed classifier is defined by
f∏0 (xo) ：= Ez〜∏0 f](xo + z)],
which returns the averaged probability of x° + Z under the perturbation of Z 〜 ∏o. Assume
we replace the original classifier with fπ] , then the goal becomes verifying fπ] using its inherent
smoothness. Specifically, if fπ]0 (x0) > 1/2, we want to verify that fπ]0 (x0 + δ) > 1/2 for every
δ ∈ B, that is,
mδ∈iBn fπ]0 (x0 + δ) = mδ∈iBn Ez
~∏0[f](x0+z+δ)] > 1/2.
(1)
In this case, it is sufficient to obtain a guaranteed lower bound of minδ∈B fπ] (x0 + δ) and check if
it is larger than 1/2. When π0 is Gaussian N(0, σ2) and for `2 attack, this problem was studied in
Cohen et al. (2019), which shows that a lower bound of
m∈in Ez 〜∏0 [f](XO + Z)] ≥ φ(φ-1(f∏o (XO ))-r/6,	⑵
where Φ(∙) is the cumulative density function (CDF) of standard Gaussian distribution, and Φ-1(∙)
represents its inverse function. The proof of this result in Cohen et al. (2019) uses Neyman-
Pearson lemma (Li & Kuelbs, 1998), while in the following section another derivation using
variational optimization is provided.
Note that the bound in Equation (2) is tractable since it only requires to evaluate the smoothed clas-
sifier fπ]0 (XO) at the original image XO, instead of solving the difficult adversarial optimization over
perturbation Z in Equation (1). In practice, fπ]0(XO) is approximated by Monte Carlo approximation
with a non-asymptotic confidence bound.
3.1 Variational Adversarial Certification
We propose a variational adversarial certification (AVC) framework, which yields a guaranteed
lower bound for Equation (1) based on variational optimization. The main idea is simple: assume F
is a function class which is known to include f], then the following optimization immediately yields
a guaranteed lower bound,
mδ∈iBn fπ]0 (XO + δ) ≥ fm∈iFn mδ∈iBn fπ0(XO + δ) s.t. fπ0(XO) = fπ]0(XO) ,	(3)
where We define f∏° (xo) = Ez〜∏0 [f (xo + z)] for any f, and search for the minimum value of
fπ0 (XO + δ) for all classifiers in F and satisfies fπ0 (XO) = fπ] (XO). This obviously yields a lower
3
Under review as a conference paper at ICLR 2020
bound once f] ∈ F. If F includes only f], then the bound is exact, but is computationally pro-
hibitive due to the difficulty of optimizing δ . The idea is then to choose F properly to incorporate
rich information of f], while allowing us to calculate the lower bound in Equation (3) computation-
ally tractably. In this paper, we consider the set of all functions bounded in [0, 1], that is,
F[0,1] = f : f(z) ∈ [0,1],∀z∈Rd
(4)
which guarantees to include f] by definition. There are other F that also yields computationally
tractable bounds, including the Lp space F = {f : kfkLp ≤ v}, which we leave for future work.
Denote by Vπ0 (F, B) the lower bound in Equation (3). We can rewrite it into the following minimax
form using the Lagrangian function,
Vπ0(F,B)
min min max L(f, δ, λ)
f∈F δ∈B λ∈R
d=effπ0(x0 + δ) - λ(fπ0(x0) - fπ]0(x0)) ,
where λ is the Lagrangian multiplier. Exchanging the min and max yields the following dual form.
Theorem 1. I) (Dual Form) Denote by ∏δ the distribution of Z + δ when Z 〜∏o. Assume F and
B are compact set. We have the following dual form of Vπ0 (F, B) via strong duality:
F (λπ0 k πδ) ,	(5)
Vπ0(F,B) ≥ mλ≥a0x fm∈iFn mδ∈iBn L(f, δ, λ) = mλ≥a0x λfπ]0(x0) - mδ∈aBx D
where we define
DF (λ∏o k ∏δ) = max {λEz〜∏° [f(x° + z)] - Ez〜∏δ[f(xo + z)]},
f∈F
which measures the difference of λπ0 and πδ by seeking the maximum discrepancy of the expectation
for f ∈ F. As we show later, the bound in (5) is computationally tractable with proper (F, B, π0).
II)	When F = F[0,1] := {f: f(x) ∈ [0, 1], x ∈ Rd}, we have in particular
DF[0,1] (λπ0	k	πδ)	=	(λπ0(Z) -	πδ(Z))+	dZ,	where	(t)+	= max(0, t).
In addition, we have 0 ≤ DF[0,1] (λπ0 k πδ) ≤ λ for any π0, πδ and λ > 0. Note that
DF[0,1] (λπ0 k πδ) coincides with the total variation distance between π0 and πδ when λ = 1.
III)	Suppose F = F[0,1] and suppose that for any λ ≥ 0, minδ∈B minf ∈F[0,1] L (f, δ, λ) =
minf∈F[o,i] L (f, δ*,λ),forsome δ* ∈ B, we have
Vπ0 (F, B) = maxminminL(f,δ,λ) .
λ≥0 δ∈B f∈F
Remark We will show later that the proposed methods and the cases we study satisfy the condition
in part III of the theorem and thus all the lower bounds of the proposed method are tight.
Proof. First, observe that the constraint in Equation (3) can be equivalently replaced by an inequality
constraint fπ0(x0) ≥ fπ]0 (x0). Therefore, the Lagrangian multiplier can be restricted to be λ ≥ 0.
We have
V(F, B) =minminmαχ E∏δ[f(x0 + z)] + λ (f]0 (XO)- E∏0 [f(x0 + z)])
δ∈B f ∈F λ≥0
≥ max min min Eπδ [f (x0 + Z)] + λ f] (x0) - Eπ [f (x0 + Z)]	//exchange min and max
λ≥0 δ∈B f∈F	π0
max min
λ≥0 δ∈B
(x0) + min Eπδ [f (x0 + Z)] - λEπ0 [f (x0 + Z)])
f∈F
max min {λf∏0 (xo) - DF(λπ0 k πδ)}
λ≥0 δ∈B
The proof of the strong duality is in Appendix A.1. II) follows a straightforward calculation. □
4
Under review as a conference paper at ICLR 2020
Although the lower bound in Equation (5) still involves an optimization on δ and λ, both of them are
much easier than the original adversarial optimization in Equation (1). With proper choices of F, B
and π0, the optimization of δ can be shown to provide simple closed form solutions by exploiting
the symmetry of B, and the optimization ofλ is a very simple one-dimensional searching problem.
As a corollary of Theorem 1, we can exactly recover the bound derived by Cohen et al. (2019).
Corollary 1. With isotropic Gaussian noise π0 = N(0, σ2Id×d), `2 attack B = {δ : kδk2 ≤ r}
and F = F[0,1], the lower bound in Equation (5) equals the bound in Equation (2) by Cohen et al.
(2019), that is,
max λ λf∏0 (XO)-IImax DF[0,1] (λπ0kπδ)	= φ(φ-1(f∏0 (χ0)) - r/σ).
λ≥0	kδk2 ≤r	,
See Appendix A.2 for more details. A key step of the proof is to show
that maxkδk2≤r DF[0,1] (λπ0kπδ) is achieved when δ is on the bound-
ary of the `2 ball B = {kδ k2 ≤ r}, which can be, for example,
δ = [r, 0, . . . , 0]>, due to symmetry of B (see figure on the right).
Trade-off between Accuracy and Robustness The lower bound in
Equation (5) reflects an intuitive trade-off between the robustness and
accuracy,
(6)
max
λ≥0
λfπ]0 (x0 ) -
'----V----}
Accuracy
max DF (λπ0 k πδ )
Robustness
(7)
where the first term reflects the accuracy of the smoothed classifier (assuming the true label is y = 1),
while the second term maxδ∈B DF (λπ0 k πδ) measures the robustness of the smoothed classifier,
via the maximum difference between the original smoothing distribution π0 and perturbed distribu-
tion πδ for δ ∈ B. The scalar λ can be viewed as looking for a best balance between these two terms
to achieve the largest lower bound.
More critically, different choices of smoothing distributions also yields a fundamental trade-off be-
tween accuracy and robustness in Equation (7). If π0 has large variance or high tail probability, the
distance DF (λπ0 k πδ ) will tend to be large and the model is robust. However, if the variance of
π0 is too large, we may obtain a low value of fπ] (x0) and hence less accurate model. The opti-
mal choice of the smoothing distribution should optimally balance the accuracy and robustness, by
distribute its mass properly to yield small maxδ∈B DF (λπ0 k πδ) and large fπ]0 (x0) simultaneously.
4 Filling the Soap Bubbles: New Families of Non-Gaussian
Smoothing Distributions
In this section, we identify a key problem of using Gaussian smoothing noise in high dimensional
space, due to the “thin shell” phenomenon that the probability mass of Gaussian distributions con-
centrates on a sphere far away from the center points in high dimensional spaces. Motivated by
this observation, we propose in Section 4.1 a new family of non-Gaussian smoothing distributions
that alleviate this problem for `2 attack, and also in Section 4.2 another mixed norm smoothing
distribution designed specifically for '∞ attack.
4.1	`2 REGION CERTIFICATION
Although isotropic Gaussian distributions appears to be a natural choice of smoothing distributions,
they are in fact sub-optimal for trading-off accuracy and robustness in Equation (7), especially in
high dimensions. The key problem is that, in high dimensional spaces, the probability mass of
Gaussian distributions concentrates on a thin shell away from the center, and hence looks like “soap
bubbles”, instead of “solid balls” as what it appears in low dimension spaces.
Lemma 1 (Vershynin (2018), Section 3.1). Let Z 〜N(0, Id×d) be a d-dimensional standard
Gaussian random variable. Then there exists a constant c, such that for δ ∈ (0, 1),
Prob (√d - Pclog(2∕δ) ≤ ∣∣zk2 ≤ √d + Pclog(2∕δ)) ≥ 1 - δ.
5
Under review as a conference paper at ICLR 2020
This suggests that with high probability (e.g, 1 一 δ = 0.99), Z takes values very close to the sphere of
radius √d, within a constant distanCefrom that sphere! See Vershynin (2018)for more discussion.
This phenomenon makes it problematic to use standard Gaussian distribution for adversarial certi-
fication, because one would expect that the smoothing distribution should concentrate around the
center (the original image) in order to make the smoothed classifier close to the original classifier
(and hence accurate). To illustrate the problem, consider a simple example When the true classifier
is f ](x) = I(kx — xok2 ≤ e√d) for a constant e < 1, where I is the indicator function. Then when
the dimension d is large, We Would have f](x0) = 1 While fπ] (x0) ≈ 0 When π0 = N(0, Id×d). It
is of course possible to decrease the variance of π0 to improve the accuracy of the smoothed classi-
fier fπ] 0 . However, this would significantly improve the distance term in Equation (7) and does not
yield an optimal trade-off on accuracy and robustness.
In this work, we introduce anew family of non-Gaussian distributions to address this curse of dimen-
sionality. To motivate our method, it is useful to examine the density function of the distributions of
the radius of spherical distributions in general.
Lemma 2. Assume z is a spherically symmetric random variable on Rd with a probability density
function (PDF) ofform ∏o(z) H φ(∣∣z∣∣2), where φ: [0, ∞) → [0, ∞) is a univariate function, then
the PDF of the norm of Z is Pkzk? (r) H rd-1φ(r). The term rd-1 arises due to the integration on
the sphere of radius r in Rd.
In particular, when Z 〜 ∏o = N(0,σ2Id×d), we have φ(r) = exp(-r2∕(2σ2)) and hence
Pkzk2(r) H rd-1 exp(-r2∕(2σ2)), which is a scaled Chi distribution, also known as Nakagami
distribution. Examining this P.D.F., we can see that the concentration of the norm is caused by the
rd-1 term, which makes the density to be highly peaked when d is large. To alleviate the concen-
tration phenomenon, we need to have a way to cancel out the effect of rd-1. This motivates the
following family of smoothing distributions:
π0(Z) H
and hence	Pkzk2 (r) H rd-k-1exp (一3
(8)
where we introduce a kZk2-k term in π0, with k a positive parameter, to make the radius distribution
less concentrated when k is large.
The radius distribution in Equation (8) is controlled by two pa-
rameters (σ, k), where σ controls the scale of the distribution
(and is hence the scale parameter), while k controls the shape
of the distribution (and hence the shape parameter). The key
idea is that adjusting k allows us to trade-off the accuracy and
robustness much more optimally. As shown in Figure 1, ad-
justing σ enables us to move the mean close to zero (hence
yielding higher accuracy), but at cost of decreasing the vari-
ance quadratically (hence more less robust). In contrast, ad-
justing k allows us to decrease the mean without significantly
impacting the variance, and hence yield a much better trade-off
on the accuracy and robustness.
Computational Method With the more general non-
Gaussian smoothing distribution, weno longer have the closed
form solution of the bound like Equation (6). However,
efficient computational methods can be still developed for
calculating the bound in Equation (5) with π0 in Equa-
tion (8). The key is that the maximum of the distance term
Figure 1: Starting from radius distri-
bution in Equation (8) with d = 100
σ = 1 and k = 0 (black start), increas-
ing k (green curve) allows us to move
the mean towards zero without signifi-
cantly reducing the variance. Decreas-
ing σ (red curve) can also decrease the
mean, but with a cost of decreasing the
variance quadratically.
DF[0,1] (λπ0 || πδ) over δ ∈ B is always achieved on the boundary of B as we show in the sequel,
while the optimization on λ ≥ 0 is one-dimensional and can be solved numerically efficiently.
Theorem 2. Consider the `2 attack with B = {δ : kδk2 ≤ r} and smoothing distribution π0(Z) H
∣∣ζk-k exp (—k2σ⅛) with k ≥ 0 and σ > 0. Define δ* = [r, 0,..., 0]>, we have
DF[0,i] (λπ0 k πδ* ) = max DF[o,i] (λπ0 k πδ) .
6
Under review as a conference paper at ICLR 2020
With Theorem 2, We can compute Equation (5) with δ = δ*. We then calculate Df^h (λ∏o ∣∣ ∏δ*)
using Monte Carlo approximation. Note that
DF[0,i] (λ∏0 k ∏δ*) = Z (λ∏0(z) - ∏δ*(z))+ dz = Ez〜∏0 "-π⅛) )」
which can be approximated with Monte Carlo method with Hoeffding concentration bound.
Let {zi}in=1 be an i.i.d. sample from π0, then we can approximate DF[0,1] (λπ0 ∣ πδ* ) with
D := n-1 Pn=ι (λ — ∏δ*(zi)∕∏o(Ni)* Because 0 ≤ (λ — ∏δ* (zi)∕∏o(zi))+ ≤ λ, we have
DF[o i] (λ∏o k ∏δ*) ∈ [DD — λpiog(2∕δ)∕(2n), DD + λpiog(2∕δ)∕(2n)] with probability 1 — δ for
δ ∈ (0, 1). Drawing sufficiently large number of samples allows us to achieve approximation with
arbitrary accuracy.
4.2	'∞ Region Certification
Going beyond the '2 attack, we consider the '∞ attack, whose attacking region is b` 丁 =
{δ: kδk∞ ≤ r}. This is a far more difficult problem, because the '∞ ball is substantially larger
than the `2 ball of the same radius in high dimensional space. This makes the Gaussian smooth-
ing distribution, as well as our '2-based smoothing distribution in Equation (8), unsuitable for '∞
attack. in fact, as shown in the following negative result, if we use Equation (8) as the smoothing
distribution for '∞ attack, the bound we obtain is effectively the bound we would get for verifying
a '2 ball with radius √dr, which is too large to give meaningful results when the dimension is high.
Theorem 3. With the smoothing distribution π0 in Equation (8) for k ≥ 0, σ > 0, and F = F[0,1]
shown in Equation (4), the bound we get for certifying the '∞ attack on B'∞,r = {δ : ∣δ∣∞ ≤ r}
is equivalent to thatfor certifying the '2 attack on B'2 3={δ : ∣δ∣2 ≤ √dr}, that is,
V∏0 (F[0,1]，B'∞,r ) = V∏0 (F[0,1]，B'2,√dr ).
The key reason of this negative result is that the furthest points to
the origin (vertexes) in B'∞,r have an '2 radius of √dr, illustrated
as the “pointy” points in Figure 2. Thus, the maximum distance
maxδ∈B'g,r DF(λ∏0 ∣∣ ∏δ) is achieved at one of these pointy points, mak-
ing it equivalent to optimizing in the '2 ball with radius √dr.
In order to address this problem, we propose the following new mixed norm FigUre 2: '∞ and '2
family of smoothing distribution that uses a mix of'2 and '∞ norms:	balls in high dimension.
π0(Z) H llzk∞ exP
(9)
in which we replace the ∣z ∣2-k term in Equation (8) with ∣z ∣-∞k . The motivation is that this allows
us to allocate more probability mass along the “pointy” directions with larger '∞ norm, and hence
decrease the maximum distance term maxδ∈B'co,r DF(λ∏0 ∣∣ ∏δ). See Figure 2 for an illustration.
in practice, we find that this mixed norm smoothing distribution in Equation (9) work much more
efficiently than the '2 norm-based family in Equation (8).
Given the difference of '2 and '∞ norms, it is also natural to consider the following pure '∞ norm
distributions, which uses '∞ norm in both of the terms of the distribution,
π0(z) H
(10)
Unfortunately, this seemingly natural choice does not work efficiently for '∞ attacks (even worse
than the '2 family Equation (8)). This is because the volume of the '∞ ball is in some sense “too
large” (e.g., compared with the volume of '2 ball). As a result, in order to make the probability
mass of Equation (10) in a reasonable scale, one has to choose a very small value ofσ, which makes
maximum distance term too large to be practically useful.
7
Under review as a conference paper at ICLR 2020
Figure 3: For '∞ attacking, the mixed
norm distribution (right) yields smaller
TV distances (larger overlap areas), and
hence higher robustness.
Figure 4: The Pareto frontier of accuracy and robustness
(in the sense of Equation (7)) of the three smoothing fam-
ilies in Equation (8), Equation (9), and Equation (10) for
'∞ attacking, when We search for the best parameters (k, σ)
for each of them. The mixed norm family Equation (9)
yields the best trade-off than the other two. We assume
f](x) = I(kxk2 ≤ r) and dimension d = 5. The case
when f](x) = I(kxk∞ ≤ r) has similar result (not shown).
Theorem 4.	Consider the adversarial attacks on the '∞ ball B'∞,r = {δ : kδ∣∣∞ ≤ r}. Suppose
we use the smoothing distribution π0 in Equation (10) and choose the parameters (k, σ) such that
1)	∣∣zk∞ is stochastic bounded when Z 〜∏o, in thatfor any e > 0, there exists a finite M > 0 such
that Pπ0 (|z | > M) ≤ ;
2)	the mode of ∣z ∣∞ under π0 equals Cr, where C is some fixed positive constant,
then for any e ∈ (0, 1) and sufficiently large dimension d, there exists a constant t > 1, such that ,
we have
max DDf[o,i] (λ∏o ∣ ∏δ) ] ≥ (1 - e) (λ -OSd)).
δ∈B'∞,r
This shows that, in very high dimensions, the maximum distance term is arbitrarily close to λ
which is the maximum possible value of DF[0,1] (λπ0 ∣ πδ) (see Theorem 1). In particular, this
implies that in high dimensional scenario, once fπ]0 (x0) ≤ (1 - e) for some small e, we have
V∏o (F[0,i], B'∞,r) = O(t-d) and thus fail to certify.
Remark The condition 1) and 2) in Theorem 4 are used to ensure that the magnitude of the random
perturbations generated by π0 is within a reasonable range such that the value of fπ]0 (x0) is not too
small, in order to have a high accuracy in the trade-off in Equation (7). Note that the natural images
are often contained in cube [0, 1]d. If ∣z∣∞ is too large to exceed the region of natural images,
the accuracy will be obviously rather poor. Note that if we use variants of Gaussian distribution,
we only need ∣∣z∣∣2∕√d to be not too large. Theorem 4 says that once ∣∣z∣∣∞ is in a reasonably
small scale, the maximum distance term must be unreasonably large in high dimensions, yielding a
vacuous lower bound.
Empirical Justification We construct a simple toy example to verify the advantages of the mixed
norm family Equation (9) overall the '2 family in Equation (8) and the '∞ family in Equation (10).
We assume that the true classifier is f] (x) = I(∣x∣2 ≤ r) in r = 0.65, d = 5 case and plot
in Figure 4 the Pareto frontier of the accuracy and robustness terms in Equation (7) for the three
families of smoothing distributions, as we search for the best combinations of parameters (k, σ).
We can see that the mixed norm smoothing distribution clearly obtain the best trade-off on accuracy
and robustness, and hence guarantees a tighter lower bound for certification.
Computational Method In order to compute the lower bound when using the mixed norm fam-
ily Equation (9), we need to establish the closed form solution of the maximum distance term
maxδ∈BDF[0,1] (λπ0 || πδ) similar to Theorem 2. The following result shows that the optimal δ
is achieved at one vertex (the pointy points) of the '∞ ball.
Theorem 5.	Consider the '∞ attack with B'∞,r = {δ : ∣∣δ∣∞ ≤ r} and the mixed norm Smoothing
distribution in Equation (9) with k ≥ 0 and σ > 0. Define δ* = [r, r,..., r]>. We have for any
8
Under review as a conference paper at ICLR 2020
`2 RADIUS (CIFAR-10)	0.25	0.5	0.75	1.0	1.25	1.5	1.75	2.0	2.25
Cohen et al. (2019) (%)	60	43	34	23	17	14	12	10	8
OURS (%)	61	46	37	25	19	16	14	11	9
Table 1: Certified top-1 accuracy of the best classifiers with various `2 radius on CIFAR-10.
`2 RADIUS (ImageNet)	0.5	1.0	1.5	1.0	2.0	2.5	3.0
Cohen et al. (2019) (%)	^49^^	^3T^	29	19	15	12	9
OURS (%)	50	39	31	21	17	13	10
Table 2: Certified top-1 accuracy of the best classifiers with various `2 radius on ImageNet.
λ>0,
DF[0,i] (λπ0 Il πδ* ) = lδ∈Bχ DF[o,i] (λπ0 Il πδ) .
The proof of Theorem 2 and 5 is non-trivial, thus we defer the details to Appendix A.3. With
the optimal δ* found above, We can calculate the bound with similar Monte Carlo approximation
outlined in Section 4.1.
5	Experiments
We evaluate our new bound and smoothing distributions for both '2 and '∞ attacks. We compare
with the randomized smoothing method of Cohen et al. (2019) with Gaussian smoothing distribu-
tion. For fair comparisons, we use the same model architecture and pre-trained models provided by
Cohen et al. (2019) and Salman et al. (2019), which are ResNet110 on CIFAR10 and ResNet50 on
ImageNet.
Settings and Hyperparameters The details of our method are shown in Algorithm 2 in Appendix.
Since our method requires Monte Carlo approximation, we draw 0.1M samples from π0 and con-
struct α = 99.9% confidence lower bounds of that in Equation (7). The optimization on λ is solved
using grid search. For `2 attacks, we set k = 500 for CIFAR10 and k = 50000 for ImageNet in
our non-Gaussian smoothing distribution Equation (8). If the used model was trained with a Gaus-
sian perturbation noise of N(0, σ0), then the σ parameter of our smoothing distribution is set to be
ʌ/(d - 1)∕(d — 1 — k)σo, such that the expectation of the norm ∣∣z∣2 under our non-Gaussian dis-
tribution Equation (8) matches with the norm of N(0, σ0). For '∞ situation, we set k = 250 and σ
also equals to ʌ/(d — 1)∕(d — 1 — k)σo for the mixed norm smoothing distribution Equation (9). In
both cases, the baseline algorithm uses a Gaussian smoothing distribution N(0, σ02 ). More ablation
study about k is deferred to Appendix C.
Evaluation Metrics The methods are evaluated using the certified accuracy defined in Cohen
et al. (2019). Given an input image x and a perturbation region B, the smoothed classifier is called
certified correct if its prediction is correct and has a guaranteed lower bound larger than 1/2 for
δ ∈ B. The certified accuracy is the percentage of images that are certified correct. Following
Salman et al. (2019), we calculate the certified accuracy of all the classifiers in Cohen et al. (2019)
or Salman et al. (2019) for various radius, and report the best results over all of classifiers.
5.1	`2 CERTIFICATION
We test our method on CIFAR10 and ImageNet for `2 certification. For fair comparison, we use the
same pre-trained models as Cohen et al. (2019), which is trained with Gaussian noise on both CI-
FAR10 and ImageNet dataset. The readers are referred to Appendix C for detailed ablation studies.
Table 1 and Table 2 report the certified accuracy of our method with the non-Gaussian smoothing
distribution in Equation (8) and the baseline on CIFAR10 and ImageNet, respectively. We find that
our method consistently outperforms the baseline.
9
Under review as a conference paper at ICLR 2020
l∞ RADiUS (CiFAR-10)	2/255	4/255	6/255	8/255	10/255	12/255
Salman et al. (2019) (%)	-58^^	-^42^^	31	25	18	13
OURS (%)	60	47	38	32	23	17
Table 3: Certified top-1 accuracy of the best classifiers with various l∞ radius on CIFAR-10.
σ0 = 1.00
σ0 = 0.50
σ0 = 0.25
Gaussian
Ours
Figure 5: Results of '∞ verification on CIFAR10, on models trained with Gaussian noise data
augmentation with different variances σ0 . Our method obtains consistently better results.
])%( ycaruccA[ go
5.2	'∞ Certification
We test our lower bound based on the mixed norm family in Equation (9) for verifying '∞ attacking
on CiFAR10, using the models trained by Salman et al. (2019). The certified accuracy of our method
and the baseline using Gaussian smoothing distribution are shown in Table 3. We can see that
our method consistently outperforms the Gaussian distribution baseline by a large margin, which
empirically shows our distribution is a more suitable distribution for '∞ perturbation.
To further confirm the advantage of our method, we plot in Figure 5 the certified accuracy of our
method and Gaussian baseline using models trained with Gaussian perturbation of different vari-
ances σ0, under different '∞ radius. We again find that our approach outperforms the baseline
consistently, especially when the '∞ radius is large. We also experimented our method and baseline
on ImageNet, but did not obtain non-trivial results. This is because '∞ verification is extremely hard
with very large dimensions. Future work will investigate how to obtain non-trivial bounds for '∞
attacking at imageNet scales with smoothing classifiers.
6	Conclusions
We propose a general variational optimization based framework of adversarial certification with
non-Gaussian smoothing distributions. Based on the insights from our new framework and high di-
mensional geometry, we propose two new families of non-Gaussian smoothing distributions, which
significantly outperform the Gaussian-based smoothing for '2 and '∞ attacking, respectively. Our
work provides basis for a variety of future directions, including improved methods for `p attacks,
and tighter bounds based on adding additional constraints to our optimization framework.
References
Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. pp. 274-283, 2018.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, New
York, NY, USA, 2004. iSBN 0521833787.
Nicholas Carlini, Guy Katz, Clark Barrett, and David L. Dill. Provably minimally-distorted adver-
sarial examples, 2017.
10
Under review as a conference paper at ICLR 2020
Chih-Hong Cheng, Georg Nuhrenberg, and Harald Ruess. Maximum resilience of artificial neural
networks. In Automated Technology for Verification and Analysis - 15th International Symposium,
ATVA 2017, Pune, India, October 3-6,2017, Proceedings,pp. 251-268, 2017.
Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certified adversarial robustness via randomized
smoothing. arXiv preprint arXiv:1902.02918, 2019.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
and Short Papers), pp. 4171-4186, 2019. URL https://www.aclweb.org/anthology/
N19-1423/.
Souradeep Dutta, Susmit Jha, Sriram Sankaranarayanan, and Ashish Tiwari. Output range analysis
for deep feedforward neural networks, 2018.
Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy A. Mann, and Pushmeet Kohli.
A dual approach to scalable verification of deep networks. In Proceedings of the Thirty-Fourth
Conference on Uncertainty in Artificial Intelligence, UAI 2018, Monterey, California, USA, Au-
gust 6-10, 2018, pp. 550-559, 2018. URL http://auai.org/uai2018/proceedings/
papers/204.pdf.
Ruediger Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In Interna-
tional Symposium on Automated Technology for Verification and Analysis, pp. 269-286. Springer,
2017.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Matt Jordan, Justin Lewis, and Alexandros G Dimakis. Provable certificates for adversarial exam-
ples: Fitting a ball in the union of polytopes. arXiv preprint arXiv:1903.08778, 2019.
Harini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arXiv preprint
arXiv:1803.06373, 2018.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. arXiv preprint arXiv:1802.03471,
2018.
Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi S Jaakkola. A stratified approach to robustness
for randomly smoothed classifiers. Advances in neural information processing systems (NeurIPS),
2019.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Second-order adversarial attack and
certifiable robustness. Advances in neural information processing systems (NeurIPS), 2019.
Wenbo V Li and James Kuelbs. Some shift inequalities for gaussian measures. In High dimensional
probability, pp. 233-243. Springer, 1998.
Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. Towards robust neural networks via
random self-ensemble. In Proceedings of the European Conference on Computer Vision (ECCV),
pp. 369-385, 2018.
Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond finite layer neural networks:
Bridging deep architectures and numerical differential equations. 2018.
11
Under review as a conference paper at ICLR 2020
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. 2018.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Semidefinite relaxations for certifying ro-
bustness to adversarial examples, 2018.
Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien
Bubeck. Provably robust deep learning via adversarially trained smoothed classifiers. arXiv
preprint arXiv:1906.04584, 2019.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge University Press, 2018.
Dilin Wang, Chengyue Gong, and Qiang Liu. Improving neural language modeling via adversarial
training. pp. 6555-6565, 2019.
Eric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial
effects through randomization, 2018.
Runtian Zhai, Tianle Cai, Di He, Chen Dan, Kun He, John Hopcroft, and Liwei Wang. Adversarially
robust generalization just requires more unlabeled data. arXiv preprint arXiv:1906.00555, 2019.
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate
once: Accelerating adversarial training via maximal principle. Advances in neural information
processing systems (NeurIPS), 2019.
Haichao Zhang and Jianyu Wang. Defense against adversarial attacks using feature scattering-based
adversarial training, 2019.
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural net-
work robustness certification with general activation functions. In Advances in neural information
processing systems, pp. 4939-4948, 2018.
12
Under review as a conference paper at ICLR 2020
A	Proofs
A.1 Proof for the strong duality in theorem 1
We first introduce the following lemma, which is a straight forward generalization of the strong
Lagrange duality to functional optimization case.
Lemma 3. Given some δ*, we have
max . min E∏δ* [f (XO + Z)] + λ (f∏0 (XO)- E∏0 [f (x0 + Z)D
λ∈R f ∈F[0,1]
=max ，min E∏δ* [f (XO + Z)] + λ (fl (XO)- E∏0 [f (x0 + Z)D .
λ∈R f ∈F[0,1]
The proof of Lemma 3 is standard. However, for completeness, we include it here.
Proof. Without loss of generality, we assume fπ]0 (XO) ∈ (0, 1), otherwise the feasible set is trivial.
Let α* be the value of the optimal solution of the primal problem. We define f∏o (xo)-
E∏0 [f (xo + z)] = h[f] and g[f] = E∏δ* [f (xo + z)]. We define the following two sets:
A = {(v,t) ∈ R X R : ∃f ∈ F[O,1], h[f] = v,g[f] ≤ t}
B = {(0,s) ∈ R × R : s<α*}.
Notice that both sets Aand B are convex. This is obvious for B. For any (v1, t1) ∈ Aand (v2, t2) ∈
A, we define f1 ∈ F[O,1] such that h[f1] = v1, g[f1] ≤ t1 (and similarly we define f2). Notice that
for any γ ∈ [0, 1], we have
γf1 + (1 - γ)f2 ∈ F[O,1]
γh[f1] + (1 - γ)h[f2] = γv1 + (1-γ)v2
γg[f1] + (1 - γ)g[f2] ≤ γt1 + (1 - γ)t2,
which implies that γ(v1, t1) + (1 - γ)(v2, t2) ∈ A and thus A is convex. Also notice that by
definition, A∩ B = 0. Using separating hyperplane theorem, there exists a point (qι, q2) = (0,0)
and a value α such that for any (v, t) ∈ A, q1v + q2t ≥ α and for any (0, s) ∈ B, q2s ≤ α. Notice
that we must have q2 ≥ 0, otherwise, for sufficient s, we will have q2s > α. We thus have, for any
f ∈ F[O,1], we have
qιh[f] + q2g[f] ≥ α* ≥ q2α*.
If q2 > 0, we have
max fmin g[f] + λhf] ≥ fmin g[f] + "h[f] ≥ α*,
λ∈R f ∈F[0,1]	f ∈F[0,1]	q2
which gives the strong duality. If q2 = 0, we have for any f ∈ F[O,1] , q1h[f] ≥ 0 and by the
separating hyperplane theorem, q1 6= 0. However, this case is impossible: If q1 > 0, choosing
f ≡ 1 gives qιh[f ] = qι (f∏0(xo) - 1) < 0; If qι < 0, by choosing f ≡ 0, we have qιh[f ]=
q1 fπ]0 (XO ) - 0 < 0. Both cases give contradiction.
□
Based on Lemma 3, we have the proof of the strong duality as follows.
Notice that by Lagrange multiplier method, our primal problem can be rewritten as follows:
min min max E∏δ [f(x0 + z)] + λ (f∏0 (XO)- E∏0 [f (x0 + Z)D ,
δ∈B f ∈F[0,1] λ∈R
and the dual problem is
max min ,fmin E∏δ [f (xo + z)] + λ (f∏0 (XO) - e∏0 [f (xo + Z)D
λ∈R δ∈B f ∈F[0,1]
= maxmin ,miη E∏δ [f (xo + z)] + λ (f∏0 (XO) - e∏0 [f (xo + Z)D .
λ≥O δ∈B f ∈F[0,1]
13
Under review as a conference paper at ICLR 2020
By the assumption that for any λ ≥ 0, we have
maxmin .min E∏δ [f (x0 + Z)] + λ f∏0 (XO) - E∏0 [f (XO + Z)D
λ≥0 δ∈B f ∈F[0,1]
=max ，min E∏δ* [f (XO + z)] + λ f∏0 (XO)- E∏0 [f (xo + z)]),
λ≥O f ∈F[0,1]
for some δ* ∈ B. We have
max min min E∏δ [f (xo + z)] + λ (f∏0 (xo) - E∏o [f (xo + z)])
λ∈R δ∈B f ∈F[0,1]
=max min	E∏δ*	[f (xo +	z)]	+ λ	f∏o (XO)-	Eno	[f (xo +	Z)D
λ≥o f ∈F[0,1]
=max . min	E∏δ*	[f (XO +	z)]	+ λ	f∏o (XO)-	Eno	[f (XO +	Z)D
λ∈R f ∈F[0,1]
=fmin max E∏δ* [f(XO + z)] + λ f∏o (XO)- Eno [f (Xo + Z)D
f ∈F[0,1] λ∈R
≥ min , min maxE∏δ* [f(XO + z)] + λ (fΠo (XO) - Eno [f (xO + Z)D ,
δ∈B f ∈F[0,1] λ∈R
where the second equality (*) is by Lemma 3.
A.2 Proof for corollary 1
Proof. Given our confidence lower bound
min max
kδk2≤r λ≥O
(λ∏o(z) - ∏μ(z))+ dz
define C = {z : λ∏o(z) ≥ ∏δ(z)} = {z : δ>z ≤ kδ2k——+ σ2 lnλ} and Φ(∙) to be the Cdf of
standard gaussian distribution, then
(λπ0(z) - πδ(z))+ dz
=	(λπ0(z) - πδ(z)) dz
Cλ
=λ ∙ P (N(z; 0, σ2I) ∈ Cλ) - P (N(z; δ, σ2I) ∈ Cλ)
=λ ∙ φ (kδk2 + σln2、- φ (-kδk2 + σln3、
I 2σ + ∣∣δ∣∣2 J I 2σ +MgJ .
Define
F(δ,λ) := λPo- Z (λπ0(z) - πδ(z))+ dz = λPo-λ∙φ (kδk2 + σlnλ)+φ ( kδk2 + σlnλ).
+	2σ	kδk2	2σ	kδk2
For ∀δ, F is a concave function w.r.t. λ, as F is actually a summation of many concave piece wise
linear function. See Boyd & Vandenberghe (2004) for more discussions of properties of concave
functions.
Define λδ = exp (2σkδk2φ2σ2p0)-kδk2), simple calculation can show dF∂λ,λ) 卜=尤 = 0, which
means
min max F(δ, λ)
kδk2≤r λ≥O
min F(δ, λδ )
kδk2≤r
*r{0+φ(寸+σ⅛
min Φ
kδk2≤r
14
Under review as a conference paper at ICLR 2020
This tells us
min maxF(δ, λ) > 1/2 ⇔ Φ
kδk2≤r λ≥0
φ-1(po) - r) > 1/2 ⇔ r <σ ∙ Φ-1(po)
,i.e. the certification radius is σ ∙ Φ-1(p0). This is exactly the core theoretical contribution of Cohen
et al. (2019). This bound has a straight forward expansion for multi-class classification situations,
We refer interesting readers to Appendix D.	□
A.3 Proof For Theorem 2 and 5
In this subsection, We give the proof of theorem 2 and 5. Here We consider a more general smooth
distribution ∏o(z) H ∣∣zk∞k1 kzk-k2 exp (-k2⅛), for Some kι, k2 ≥ 0 and σ > 0. We first gives
the following key theorem shows that D/。斗(λ∏o k ∏) increases as ∣δ∕ becomes larger for every
dimension i.	,
Theorem 6. Suppose π0(z) H kzk-∞k1 kzk2-k2 exp (—k2Zσk2^ ,for some k1,k2 ≥ 0 and σ > 0 ,for
any λ ≥ 0 we have
for any i ∈ {1, 2, ..., d}.
∂
Sgn(Si)∂δDF[0,1] (λπ0 k πδ) ≥ 0,
Theorem 2 and 5 directly follows the above theorem. Notice that in Theorem 2, as our distribution is
spherical symmetry, it is equivalent to set B = δ : δ = [a, 0, ..., 0]>, a ≤ r by rotating the axis.
Proof. Given λ, kι and k2, we define φι(s) = s-k1, φ2(s) = s-k2e-σ2. Notice that φι and φ2 are
monotone decreasing for non-negative s. By the symmetry, without loss of generality, we assume
δ = [δ1, ..., δd]> for δi ≥ 0, i ∈ [d]. Notice that
∂ ..	... 一..	...	. .一 ∂ Γ, "7
kx0 kx0 - δk∞= I{kx0 - δk∞ = |xi - δ|}而 V (Xi - δi)
∂δi	∂δi
=I{kx0 - δk∞ =|Xi-Si|}-0x-δδ∞.
And also
∂Si kx0 - μk2=∂Si SX (xi-μi)2
=-(Xi - μi)
kxo - μk2.
We thus have
(λπ0 (x0) - πδ (x0))+ dx0
∂
—I I{λ∏0(x0) ≥ ∏δ(xo)} k∏δ(x0)dx0
∂δ1
I{λπ0(x0) ≥ πδ(x0)} F1 (kx0 - δk∞ , kx0 - δk2) dx0
+
I{λπ0(x0)
I{λπ0(x0)
≥ ∏δ(x0),x1 > μι} Fl (kxo — δk∞
≥ ∏δ(x0),x1 < μι} Fl (kxo — δk∞
k0 — δk2) dx0
k0 — δk2) dx0,
where we define
Fi (kxo — δk∞ , kxo — δk2)
=φ1 (kx0 - δk∞)φ2 (kx0 - δk2)I{kx0 - δk∞ = E- δil}k≡⅛
+φi (kxo—μk∞)φ2 (kxo-μk2) k⅛≡⅛.
15
Under review as a conference paper at ICLR 2020
Notice that as φ01 ≤ 0 and φ02 ≤ 0 and we have
/
Z
I{λπ0(x0) ≥ πδ(x0),x1 > δ1}F1 (kx0 - δk∞ , kx0 - δk2) dx0 ≤ 0
I{λπ0(x0) ≥ πδ(x0),x1 < δ1}F1 (kx0 - δk∞ , kx0 - δk2) dx0 ≥ 0.
Our target is to prove that 品((λ∏o(xo) - ∏δ(xo))+ dxo ≥ 0. Now define the set
Hi = {xo : λ∏0(x0) ≥ ∏δ(xo), xι > μι}
H2 = [2δ1 - x1, x2, ..., xd]> : x0 = [x1, ..., xd]> ∈ H1 .
Here the set H2 is defined as a image of a bijection
Proj(XO) = [2δ1 - x1, χ2,…,xd]	= χ0,
that is constrained on the set H1. Notice that under our definition,
I {λπ0 (x0) ≥ πδ(x0),x1 > δ1} F1 (kx0 - δk∞ , kx0 - δk2) dx0
=	F1 (kx0 - δk∞ , kx0 - δk2)dx0.
H1
Now we prove that
I {λπ0 (x0) ≥ πδ(x0), x1 < δ1} F1 (kx0 - δk∞ , kx0 - δk2) dx0
(1)
≥	F1 (kx0 - δ k∞ , kx0 - δ k2 ) dx0
H2
F1 (kx0 - δk∞ , kx0 - δk2)dx0 .
Property of the projection Before we prove the (1) and (2), we give the following property of the
defined projection function. For any Xo = Proj(x0), xo ∈ Hi, we have
kxo- δk∞ = IlXO- δk∞
kxO - δk2 = kxO - δk2
llx0k2 ≥ kx0k2
kxO∣∣∞ ≥ kxo∣∣∞.
This is because
xi = xi, i ∈ [d] - {1}
xi = 2δι — xi,
and by the fact that xi ≥ δι ≥ 0, we have |xi| ≤ |xi| and |xi - δι∣ ≤ |xi - δι∣.
Proof of Equality (2) By the fact that Proj is bijective constrained on the set Hi and the property
of Proj, we have
F Fi (kXo- δk∞
H2
kXo - δk2) dXo
ri(kx。
-δk∞) Φ2 (kXo - δk2) I{kXo - δk∞ = Ixi - δi∣}kxi-dXo
+ 小i (kxo-δk∞) φ2 (kxo-δk2) f-⅛ dxO
=/ Φi (kxo
H1
- δk∞) φ2 (kXo
δk2) I{kxo - δk∞ = lxi-'"hB-it Idet(J)|dxo
—
Uιφi (kxo - δk∞)φ2 (kxo - δk2) k⅛o≡⅛dxo
- Fi (kXo - δk∞
H1
kXo - δk2) dXo,
16
Under review as a conference paper at ICLR 2020
where (*) is by change of variable x⅞ = Proj(X0) and J is the Jacobian matrix J
--1	0	…0 -
0	1	…	0
.	.	.	.	and	here	we	have	the	fact	that	Xi - δι = (2δι - xι) - δι = -(χι - δι).
.............
0	0	…	1
Proof of Inequality (1) This can be done by verifying that H2	⊆
{x0 : λπ0(x0) ≥ πδ(x0), x1 < δ1}. By the property of the projection, for any x0 ∈ H1, let
Xo = Proj(X0), then λ∏0(X0) ≥ λ∏0(x0) ≥ ∏δ(xo) = ∏δ(Xo) (by the fact that t φι and φ2 are
monotone decreasing). It implies that for any Xo ∈ H2, We have λ∏0(X0) ≥ ∏δ(Xo) and thus
H2 ⊆ {X0 : π0(X0) ≥ πδ(X0), x1 < δ1}.
Final statement By the above result, we have
∂
福 J (λ∏o(xo) - ∏μ(xo))+ dxo ≥ 0,
and the same result holds for any 祭 / (λ∏0(xo) - ∏μ(xo))+ dxo, i ∈ [d], which implies our
result.	□
A.3.1 Proof for Theorem 4
First notice that the distribution of z can be factorized by the following hierarchical scheme:
a2
a 〜∏R(a) H ad-1-ke-2σ2 I{a ≥ 0}
S 〜Uni产d(-1, 1)
s
Z -M∞ a.
Without loss of generality, we assume δ* = [r,…,r]>. (see Theorem 6)
Notice that as the distribution is symmetry,
p∏o (Iz + δ*k∞ = a + r
DF[0,i] (λ∏0 Il ∏δ*) = Ez〜∏0
| IzI∞ = a)
1
2
Define |z|(i) is the i-th order statistics of |zj |, j = 1, ..., d conditioning on IzI∞ = a. By the
factorization above and some algebra, we have, for any ∈ (0, 1),
P (lzl(" > (1 - e) I kzk∞ = a) ≥ 1 - (1 - e)d-i.
And |[|(d))⊥ |z|(d). Now we estimate Df^i1(λ∏0 ∣∣ ∏δ*).
=EaEz 〜∏0
=1 EaEz 〜∏0 ] (λ - ∏δ (Z)) 1 kzk∞ = a,kz + δ*k∞ = a + r
+ 1 EaEz 〜∏0 ](λ - ∏δ (z)J ∣ 同∞ = a,∣z + δ*∣∣∞ = a + r
17
Under review as a conference paper at ICLR 2020
Conditioning on ∣∣n∣∣∞ = a, ∣∣n + δ*∣∣∞ = a + r, we have
π3、	1 1
πo (Z)=(T+r
=(ɪ
υ + r
k
e-
k
e-⅛k (2a+1)
Here the second equality is because we choose mode(∣∣N∣∣∞) = Cr, which implies that
√d - 1 - kσ = Cr. And thus we have
for some t > 1. Here the last equality is by the assumption that ∣∣n∣∣∞ = Op(1).
Next we bound the second term EaEZ〜∏o [(
the property of uniform distribution, we have
λ - ∏0 (Z)) +∣kzk∞ = α,kz + δ* k∞ = a + r
.By
P
> (1 - E)I kzk∞ = a, kz + δ*k∞ = a + r
=P (¾dΓ > (1 -') ∣kzk∞ = a
≥1 - (1 - e)d-1.
And thus, for any E ∈ [0,1),
P (kz + δ*k∞ ≥ ((1-e)a + r)2∣∣∣z∣∣∞
It implies that
EZ〜∏o ](λ - ∏δ (Z)) I kzk∞ =
≥2 (1 - (1 - e)d-1) (λ - (1 - E +
=2 (1 - (1 - E)d-1) (λ - (1 - E +
=a,kz + δ*∣∣∞ = a + r ≥ 1 (1-(1-e)/1
a, kz + δ*k∞ = a + r
r) k e-212 (e(e-2)a2+2r(1-e)a+r2))
For any E ∈ (0,1), by choosing E = lo%2f), for large enough d, we have
EZ〜∏o	卜-∏δ(Z))	∣kzk∞=a,kz+δ*k∞=a+r
≥ 1 (1-(1-E)d-1)
λ -
r∖ —k
-E + -
a
d — 1 — k
e 2C2
(2(1-e)a∕r+1)
a2 log(2∕e0)
e	C272
≥2(1 - EZ) (λ -
1 八 a	log(2∕E0) √-1
211 - (1 - --rr)
log(2∕E0)
d — 1
d — 1 — k
e 2c2
(2(1-e)a∕r+1)
a2 log(2∕e0)
e	C272
r -k
-e+a)
-d-1—k (2(1-e)a∕r+1) a '修
e 2C2 V k 一 ' , e	C2 r2
1
+
r
a
+
18
Under review as a conference paper at ICLR 2020
Thus we have
2 EaEz 〜∏o ](λ - ∏δ (Z))Jkzk∞ = a,kz + δ*∣∣∞ = a + r
= 1(1-e0)(λ -O(t-d)).
Combine the bounds, for large d, we have
DF[o,i] (λπ0 Il πδ*) = (I- e0) (λ - O(t-d)).
B Practical Algorithm
In this section, we give our algorithm for certification. Our target is to give a high probability bound
for the solution of
V∏0 (F[0,1], B'∞,r ) = m≥x {λf∏0 - DF[0,1] (λπ0 k πδ)}
given some classifier f] . Following Cohen et al. (2019), the given classifier here has a binary
output {0, 1}. Computing the above quantity requires us to evaluate both fπ] and DF[0,1] (λπ0 k πδ).
A lower bound po of the former term is obtained through binominal test as Cohen et al. (2019)
do, while the second term can be estimated with arbitrary accuracy using Monte Carlo samples.
We perform grid search to optimize λ and given λ, we draw N i.i.d. samples from the proposed
smoothing distribution π0 to estimate λfπ]0 - DF[0,1] (λπ0 k πδ). This can be achieved by the
following importance sampling manner:
λfπ]0 - DF[0,1]
≥ λpo 一
(λπ0 k πδ )
πδ(Z)) ∏o(z)dz
π0	+
≥ λp0 - Nn X(λ - π0(Zi))「
i=1	+
And we use reject sampling to obtain samples from π0 . Notice that, we restrict the search space of
λ to a finite compact set so the importance samples is bounded. Since the Monte Carlo estimation
is not exact with an error , we give a high probability concentration lower bound of the estimator.
Algorithm 1 summarized our algorithm.
Algorithm 1 Certification algorithm
Input: input image x0 ; original classifier: f]; smoothing distribution π0 ; radius r; search interval
[λstart, λend] of λ; search precision h for optimizing λ; number of samples N1 for testing p0;
pre-defined error threshold ; significant level α;
compute search space for λ : Λ =range(λstart, λend, h)
compute N2: number of Monte Carlo estimation given , α and Λ
compute optimal disturb: δ depends on specific setting
for λ in Λ do
sample zι,…, znl 〜∏o
compute n = N P= f ](x0 + Zi)
computepo =LoWerConfBound(nι, Ni, 1 一 α)
sample zi, ∙∙∙ , zn 〜∏o
COmPUte DF[0,1] (λπ0 k πδ) = N2 PN21 (λ - ∏δ0(Zi))十
.	i' 1	1	F	1 1	ʌ ʌ	TT⅛,	/、	I I	∖
compute confidence lower bound bλ = λpo 一 Df^ 1(λ∏o ∣∣ ∏δ) 一 E
end
if maxλ∈Λ bλ ≥ 1/2 then
I xo can be certified
else
I xo cannot be certified
end
19
Under review as a conference paper at ICLR 2020
(竽).
亡.
The LowerConfBound function performs a binominal test as described in Cohen et al. (2019). The
in Algorithm 1 is given by concentration inequality.
Theorem 7. Let h(zι,…，zn) = N PN=I (λ - ∏0(Zi)) “ we yield
Pr{∣h(zι,…，Zn) - / (λ∏o(z) - ∏δ(z))+ dz| ≥ ε}≤ exp
Proof. Given McDiarmid’s Inequality, which says
sup	|h (xι, x2,..., Xn) — h (xι, x2,..., Xi-ι, Xi, Xi+ι,..., Xn)∣ ≤ Ci for 1 ≤ i ≤ n,
Xl,X2,...,Xn,Xi
We have Ci = N, and then obtain
Pr{∣h(zι,…，zN) - / (λ∏o(z) - ∏δ(z))+ dz| ≥ ε}≤ exp
□
The above theorem tells us that, once ,λ, N is given, We can yield a bound With high-probability
1 - α. One can also get N When ,λ, α is provided. Note that this is the same as the Hoeffding
bound mentioned in Section 4.1 as Micdiarmid bound is a generalization of Hoeffding bound.
HoWever, in practice We can use a small trick as beloW to certify With much less comupation:
Algorithm 2 Practical certification algorithm
Input: input image x0 ; original classifier: f] ; smoothing distribution π0 ; radius r; search interval
for λ: [λstart,λend]; search precision h for optimizing λ; number of Monte Carlo for first
estimation: N10 , N20 ; number of samples N1 for a second test of p0 ; pre-defined error thresh-
old ; significant level α; optimal perturbation δ (δ = [r, 0, . . . , 0]> for `2 attacking and
δ = [r,..., r]> for '∞ attacking).
forλ in Λ do
sample zι,…,ZNo 〜∏o
compute n0 =点 PN=I f ](xo + Zi)
compute p^o =LoWerConfBound(n1, N0,1 - a)
sample zι, ∙∙∙ , ZN0 〜∏o
ComPUte DF[0,1] (λπ0 k πδ) = N PN2I (λ - ∏δ(Zi))十
.	f' 1	1	F	1 1	ʌ ʌ	π⅛,	/、	I I	∖
compute confidence lower bound bλ = λpo - Df^ ^(λ∏0 ∣∣ ∏δ)
end
compute λ = arg maxλ∈Λ bλ
compute N2 : number of Monte Carlo estimation given , α andλ
sample zι, ∙∙∙ , znl 〜∏o
compute n1 = N PN=I f](χ0 + Zi)
compute po =LOWerConfBound(nι, Nl , 1 - α)
sample zι, ∙∙∙ , zn 〜∏o
COmPUte DF[0,1] (λπ0 k πδ) = N2 PN21 (λ - ∏δ(zi)) +
compute b = λpo -⑪々。# (λ∏o k ∏δ) - E
if b ≥ 1/2 then
I xo can be certified
else
I xo cannot be certified
end
Algorithm 2 allow one to begin with small N0, N0 to obtain the first estimation and choose a λ. Then
a rigorous lower bound can be achieved with λ with enough (i.e. N1, N2) Monte Carlo samples.
20
Under review as a conference paper at ICLR 2020
C Abalation study
On CIFAR10, we also do ablation study to show the influence of different k for the `2 certification
case as shown in Table 4.
`2 RADIUS (CIFAR-10)	0.25	0.5	0.75	1.0	1.25	1.5	1.75	2.0	2.25
Cohen et al. (2019) (%)	60	43	34	23	17	14	12	10	8
k= 100 (%)	60	43	34	23	18	15	12	10	8
k =200 (%)	60	44	36	24	18	15	13	10	8
k= 500 (%)	61	46	37	25	19	16	14	11	9
k= 1000 (%)	59	44	36	25	19	16	14	11	9
k = 2000 (%)	56	41	35	24	19	16	15	12	9
Table 4: Certified top-1 accuracy of the best classifiers at various `2 radius. We use the same model
as Cohen et al. (2019) and do not train any new models.
D Illumination about bilateral condition
The results in the main context is obtained under binary classfication setting. Here we show it has a
natural generalization to multi-class classification setting. Suppose the given classifier f ] classifies
an input x0 correctly to class A, i.e.,
fA] (x0) > max fB] (x0)
B6=A
(11)
where fB] (x0) denotes the prediction confidence of any class B different from ground truth label
A. Notice that fA] (x0) + PB6=A fB] (x0) = 1, so the necessary and sufficient condition for correct
binary classification fA] (x0) > 1/2 becomes a sufficient condition for multi-class prediction.
Similarly, the necessary and sufficient condition for correct classification of the smoothed classifier
is
min
f∈F
Ez 〜∏o
[fA (x0 + δ + z)]
max
f∈F
Ez 〜∏o
[fB(x0 +δ+z)]
s.t.	Eπ0 [fA(x0)] = fπ]0 ,A(x0)	>
s.t.	Eπ0 [fB(x0)] = fπ]0 ,B(x0)
for ∀B 6= A and any perturbation δ ∈ B. Writing out their Langragian forms makes things clear:
maxλfπ]0,A(x0) - DF[0,1] (λπ0 k πδ) > min max λfπ]0,B (x0) + DF[0,1] (πδ k λπ0)
λ	λ B 6=A
Thus the overall necessary and sufficient condition is
min mmax (λf∏0 ,a(XO) - DF[0,1] (λπ0 k πδ)) - maxmiη (λf∏0 ,B (XO) + DF[0,1] (πδ k λπO))J > 0
δ∈B λ	B6=A λ
Optimizing this bilateral object will theoretically give a better certification result than our method
in main context, especially when the number of classes is large. But we do not use this bilateral
formulation as reasons stated below.
When both π0 and πδ are gaussian, which is Cohen et al. (2019)’s setting, this condition is equivalent
to:
min {φ (φT(f∏0 ,A(XO))-%)- B=xφ (φTf∏0 ,B (XO)) + %)} > 0
⇔ Φ-1(f∏o,A(χo)) - r > Φ-1(f∏o,b(χo))+r，∀b=a
⇔ r < 2 (Φ-1 (f∏0,A(XO)) - Φ-1(f∏0,B (XO))), ∀B = A
21
Under review as a conference paper at ICLR 2020
with a similar derivation process like Appendix A.2. This is exactly the same bound in the (restated)
theorem 1 of Cohen et al. (2019).
Cohen et al. (2019) use 1 - PA as a naive estimate of the upper bound of 圾,b (xo), where PA is
a lower bound of fπ]0 ,A (x0). This leads the confidence bound decay to the bound one can get in
binary case, i.e., r ≤ σΦ-1(fπ]0,A(x0)).
As the two important baselines (Cohen et al., 2019; Salman et al., 2019) do not take the bilateral
form, we also do not use this form in experiments for fairness.
22