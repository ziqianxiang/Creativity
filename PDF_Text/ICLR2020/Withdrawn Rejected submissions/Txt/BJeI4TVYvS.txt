Under review as a conference paper at ICLR 2020
Classification Logit Two-sample Testing by
Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
The recent success of generative adversarial networks and variational learning
suggests training a classifier network may work well in addressing the classical
two-sample problem. Network-based tests have the computational advantage that
the algorithm scales to large samples. This paper proposes to use the difference of
the logit of a trained neural network classifier evaluated on the two finite samples
as the test statistic. Theoretically, we prove the testing power to differentiate two
smooth densities given that the network is sufficiently parametrized, by comparing
the learned logit function to the log ratio of the densities, the latter maximizing
the population training objective. When the two densities lie on or near to low-
dimensional manifolds embedded in possibly high-dimensional space, the needed
network complexity is reduced to only depending on the intrinsic manifold geom-
etry. In experiments, the method demonstrates better performance than previous
network-based tests which use the classification accuracy as the test statistic, and
compares favorably to certain kernel maximum mean discrepancy (MMD) tests
on synthetic and hand-written digits datasets.
1	Introduction
The two-sample test problem concerns the comparison of two unknown distributions p and q from
finitely observed data samples (Lehmann & Romano, 2006). As a central problem in statistics, it is
widely encountered in general data analysis of biomedical data, audio and imaging data, etc. (Borg-
wardt et al., 2006; Chwialkowski et al., 2015; Jitkrittum et al., 2016; Lopez-Paz & Oquab, 2016;
Cheng et al., 2017), and particularly, in the machine learning application of training and evaluating
generative models (Li et al., 2015; 2017; Goodfellow et al., 2014; Arjovsky et al., 2017; Nowozin
et al., 2016; Lloyd & Ghahramani, 2015; Sutherland et al., 2016; Chwialkowski et al., 2016; Liu
et al., 2016; Jitkrittum et al., 2017). Many existing tests are based on certain estimators of a dis-
tance or divergence between p and q. Important examples include Maximum Mean Discrepancy
(MMD), especially kernel-based MMD (Anderson et al., 1994; Gretton et al., 2012a) and distance
of Reproducing Kernel Hilbert Space Mean Embedding (Chwialkowski et al., 2015; Jitkrittum et al.,
2016), divergence based methods which may involve non-parametric estimation of density differ-
ence or density ratio (Sriperumbudur et al., 2009; Wornowizki & Fried, 2016; Sugiyama et al., 2013;
Kanamori et al., 2012). While these methods have been intensively studied and theoretically well-
understood, the application is often restricted to data of small dimensionality and/or small sample
size, or certain specific classes of densities p and q, due to model and computational limitations.
The powerful expressiveness of neural networks and the recent progress in optimization of deep
networks suggest the natural idea of using a network for the two-sample problem, as revisited in
(Lopez-Paz & Oquab, 2016). In the training of generative adversarial networks (GAN) (Goodfellow
et al., 2014; Arjovsky et al., 2017; Nowozin et al., 2016), in each iteration a discriminative network
(D-net) is trained to distinguish the model density q produced by a generative network from the data
density p which is only accessible via observed samples, that is, a two-sample problem. Strictly
speaking, the task of D-net is a goodness-of-fit problem as the model density q is analytically given
(Chwialkowski et al., 2016; Liu et al., 2016; Jitkrittum et al., 2017). Since batch sampling is com-
monly used in training GAN and other generative networks, the ability of a trained network, such as
the D-net in GAN, to detect the difference between two densities from finite samples is crucial for
such applications. We review these connections in more detail in Section 1.1.
The current paper studies the method of training a network for the two-sample problem, where the
test statistic is the log ratio of the class probabilities averaged over samples, which can be computed
once a classifier network is trained. Our contributions include: (1) We introduce a network-based
1
Under review as a conference paper at ICLR 2020
two-sample test statistic based on classification logit, and the algorithm inherits the scalable com-
putational efficiency of neural networks; (2) Theoretical guarantee of testing power is proved for
smooth densities p and q in RD, and the needed network complexity is reduced to be intrinsic when
p and q lie on or near to low-dimensional manifolds embedded in possibly high dimensional ambient
space. (3) Numerical experiments show that the proposed test compares favorably to kernel MMD
tests and earlier neural network classifier test based on classification accuracy, on both synthetic
manifold data and hand-written digits datasets.
The proposed statistic belongs to a general class of f -divergence between p and q, which means that
the method may extend to a broad class of classification networks as suggested by f -GAN (Nowozin
et al., 2016) beyond maximizing softmax loss. Since KL divergence (corresponding to softmax loss)
is a prototypical case of f -divergence, we focus on softmax classifier network in this paper.
1.1	Related works
Classification and two-sample testing. The relations between two-sample testing, divergence esti-
mation and binary classification has been pointed out in earlier statistical literature Friedman (2004);
Sriperumbudur et al. (2009); Reid & Williamson (2011). (Ramdas et al., 2016) studied Fisher LDA
classifier used for testing mean shift of Gaussian distributions. Discriminative approach has also
been used to detect and correct covariant shifts (Bickel et al., 2007; 2009). Training classifier pro-
vides an estimator of density ratio, as has been pointed out in (Menon & Ong, 2016) and in the
formulation of learning generative models (Mohamed & Lakshminarayanan, 2016). While distri-
bution divergence estimation has been studied and used for two-sample problems (Kanamori et al.,
2011; 2012; Sugiyama et al., 2013; Wornowizki & Fried, 2016), the use of neural network as a di-
vergence estimator for two-sample testing was less investigated. In terms of theoretical guarantee of
test power, the analysis in (Lopez-Paz & Oquab, 2016) assumes a non-zero population test statistic
under H1 which is not specified, along with other approximations. Theoretical analysis of neural
network two-sample testing power remains limited.
MMD and kernel MMD tests. MMD encloses a wide class of two-sample statistics such as
Kolmogorov-Smirnov statistic, Wasserstein metric, and general integral probability metrics. Par-
ticularly, kernel-based MMD (Anderson et al., 1994; Gretton et al., 2012a) has been widely applied
due to its non-parametric form, and recently in training moment matching networks (MMN) (Li
et al., 2015; 2017) and evaluating generative models (Sutherland et al., 2016). To optimize kernel
parameters, (Gretton et al., 2012a) considers the selection of kernel bandwidth from data, (Jitkrittum
et al., 2016) studies the optimization of reference locations in the Mean Embedding test, (Gretton
et al., 2012b) optimizes the kernel via a convex combination of multiple kernels, (Cheng et al., 2017)
choses anisotropic kernels. The combination of neural network feature learning and kernel MMD
has been studied in (Li et al., 2017), where the training is typically more costly than that of a classi-
fier network. Compared to kernel methods, neural networks are algorithmically more efficient and
scalable, and the current paper investigates if it also has advantage in testing power.
Relation to goodness-of-fit test and GAN. The goodness-of-fit problem differs from the two-
sample problem in that one of the two densities is analytically accessible. Using the explicit formula
of q, methods based on kernel Stein discrepancy have been developed in (Chwialkowski et al., 2016;
Liu et al., 2016; Jitkrittum et al., 2017) and applied to generative model evaluation. However, the
computation of the score function V log q may be difficult for certain generative models, including
many generative networks. Meanwhile, in many generative models including MMN and GAN the
goodness-of-fit is evaluated by batch sampling, i.e. the two-sample setting: Kernel MMD is used in
MMN, and GAN, Wasserstein GAN (Arjovsky et al., 2017) and f-GAN (Nowozin et al., 2016) esti-
mate density divergence by a trained network (the D-net). Since the success of GAN training relies
on the discriminative power of the D-net, the efficiency of using a neural network for the two-sample
test is important for the training and evaluating of such models.
2	Log-ratio test by network clas s ifier
2.1	Two-sample problem
Formally, the two-sample problem asks to test the null hypothesis H0 : p = q given datasets X =
{xi}n=Xι and Y = {yj }；Y 1 where Xi 〜P i.i.d., yj 〜q i.i.d., and X is independent from Y.
The test method is usually based upon a statistic T = T (X, Y), which is computed from the two
1	.	.	1 . . .ι i Ii	1 .ι	ill	. i . c t .	♦ . 1 ∙ r∙ rr∖ -	E	. ι . ι r∙ ι
datasets, and a test threshold τ, and the null hypothesis H0 is rejected ifT > τ. To control the false
2
Under review as a conference paper at ICLR 2020
discovery under the null, the threshold T is usually set to the smallest value s.t. Pr[T > T|Ho] ≤ α,
where α ∈ (0, 1) is a pre-specified number called the significance level of the test (typically α =
0.05). Algorithm-wise, T is given either by theory (the probabilistic distribution of T under H0) or
computed from data (Higgins, 2003; Gretton et al., 2012a).
2.2 Test statistic and density log ratio estimation
The proposed test statistic is computed in the following way: given X and Y as above, without loss
of generality suppose n = nX + nY is even integer. Same as in (Lopez-Paz & Oquab, 2016), we
split the dataset D = {(xi, 0)}in=X1 ∪ {(yj , 0)}jn=Y 1 = {(zi, li)}in=1, li ∈ {0, 1}, into two halves,
i.e. D = Dtr ∪ Dte, |Dtr| = ∣Dte∣ = 2, Dte = Xte ∪ Yte and similarly for the training set. A
binary classification neural network is trained on Dtr using softmax loss, which gives estimated
class probabilities as Pr[l = 0∣z] = euθ(eu[z)θ(Z), Pr[l = 1∣z]
evθ (Z)
euθ(Z)+evθ(Z)
, where uθ and vθ
are activations in the last hidden layer of the network, θ denoting the network parametrization. We
define fθ := uθ - vθ, which is the logit, and let the test statistic be
T=	ιXe∣ XXt fθ (x) -∣Y∣ Xj (y).
x∈Xte	y∈Yte
(1)
which can be written as T= R fθ(X)(Pte(X) - qte(x))dx where Pte and qte stand for the empirical
density of Xte and Yte respectively.
As has been pointed out in (Menon & Ong, 2016), the training of the classifier can be interpreted
as minimizing a Bregman divergence between the estimated logit fθ and the true log ratio f* =
log P. Thus the proposed statistic T can be viewed as estimating the symmetric KL divergence
J(P — q) log P = KL(PIIq) + KL(q∣∣p), which is an f-divergence with f (u) = (u - 1) logU
(Kanamori et al., 2011; Nowozin et al., 2016). The testing power of (1) will be theoretically analyzed
in Section 4, particularly when P and q lie on or near to low-dimensional manifolds.
2.3	Algorithm in practice
Threshold τ. In practice, the test threshold τ can be computed by a permutation test (Higgins,
2003): randomly permute the |Dte| many labels li on the test set, and recompute the test statistics for
mperm times, typically a few tens. This gives an empirical distribution of T under the null hypothesis
where both densities equal a mixture of the original p and q. Then τ is set to be the (1-α)-quantile
of the empirical distribution so as to control the type-I error to be at most α.
Density difference indicator. Many two-sample methods also provide an indication of where q
differs from p, which is often of more application interest, e.g., via the witness function in kernel
MMD (Gretton et al., 2012a). For the proposed test, such a differential indicator is provided by the
logit fθ of the trained classifier network, which can be viewed as an estimator of log density ratio
f * as discussed above. Following kernel MMD, We call fθ (and f *) the empirical (and population)
witness function of the proposed classification logit test.
Network training and computational complexity. Our training of network is conducted via Adam
(Kingma & Ba, 2014), the convergence of which has been analyzed in many places such as (Reddi
et al., 2019). We use fixed learning rate over a fixed number of epochs, and it is entirely possible
that our training procedure is over simplified and better usage of stochastic gradient descent method
as studied in (Bottou, 2010; Zeiler, 2012; Sutskever et al., 2013; Shamir & Zhang, 2013; Du et al.,
2018) may lead to improved performance. Given n data samples, evaluating the network output
on each sample takes a fixed amount of flops, and thus computing the test statistic takes O(n)
operations. The permutation test to determine τ adds negligible cost since fθ has been evaluated at
each test sample, and permuting the class labels only reorders these computed values. The training
phase is certainly more expensive, though theoretically the overall complexity is O(n) assuming that
training is terminated after a fixed number of epochs. Note that the computation can be conducted
by batch sampling so the algorithm scales to large sample size and also to multiple sample problems.
3	A One-dimensional Example
Setting-up. We compare the proposed test based on log ratio (net-logit) to (1) gaussian kernel MMD
(gmmd) and (2) test based on classification accuracy (net-acc) (Lopez-Paz & Oquab, 2016), on 1D
example where the distributions are
Xi~ N((V),	yj 〜(I- δ)N(0,1)+ δN(3, B，
(2)
3
Under review as a conference paper at ICLR 2020
gmmd net-aCC net-logit
^mean	19T4	19.98	78.09
Std	1.95	10.43	20.56
median	19.63	17.63	84.13
Test Power
Figure 1: Plots: Top-left: Two densities P and q as in (2); Right three columns: The test statistic T on |Xte| =
|Yte| = 100 samples i.e. under Hi (red cross) and the histogram of T under 1000 permutation tests i.e. Ho
(blue curve). The population witness function (black curve) and the empirical one evaluated on test data (red
cross for Xte, blue crosses for Yte) of the three methods are shown in the bottom row. Table: The mean, standard
deviation (“std”) and median of the test power computed over nrep = 20 replicas, as described in Sec. 3.
the number δ ∈ [0, 1] controls the difference of the densities. We set δ = 0.08, and p and q are
illustrated in Fig. 1. For both network tests, 200 training and 200 testing samples are used, and same
samples in X and Y . Testing power is computed by the frequency of rejecting H0 in nrun = 400 test
runs, and nrep = 20 replicas of whole experiment are used to compute mean and standard deviation
of the estimated power. The kernel bandwidth σ in gmmd is set to be the median of the pairwise
distances among all samples (Gretton et al., 2012a). gmmd can make use of the training set for a
more fair comparison, see below. More experimental details in Appendix A.
Test power. The table in Fig. 1 lists the power for the three methods, where net-logit gives signif-
icantly better average power 〜80%, and the power of net-acc and gmmd are similar, both 〜20%.
The variation of the power is much larger for the two net-based tests though (c.f. Fig. A.2). We note
that such large variation is due to the instability of network training, possibly due to small training
size, and is a limitation of the current net-based methods.
One may observe that the above comparison to kernel MMD is not fair: First, kernel MMD with
median-distance σ does not use the training set, thus it would be a more fair comparison if gmmd
can use all the data samples without training-test splitting. Second, the median setting of σ may not
be optimal and can be improved by existing methods in literature. We thus repeat the experiment
of gmmd with median-distance σ which uses all the 400 samples (“gmmd+”), and also test over a
range of values of σ which are {2-3, 2-2, •…23}, and select the best test power (“gmmd++”). Other
experimental setting being the same as above. The results are reported in Table A.1, where gmmd+
achieves a test power of 47% and gmmd++ a power of 57%, remaining inferior to net-logit, while
both with small variation (std . 2) and thus are more stable than net-based tests. Results with other
values of δ and sample sizes are reported in Sec. 5.1, Fig. 2.
Witness function. The three tests all provide witness functions to indicate where q differs from p:
The population witness functions for gmmd is	wσ(x)	:=	gσ(x	-	y)(p(y)	-	q(y))dy,	gσ(z)	:=
eTz|2/(2b2), and its empirical counterpart is by replacing P and q with the empirical densities of
Xte and Yte respectively in the integral. Recall that the population and empirical witness function
for net-logit test are f *(x) = log P(X) and fθ respectively. For net-acc, when |Xte| = |Ke|, it is
equivalent to using the sign (taking value of ±1) of fθ instead of fθ in computing the test statistic in
(1), as shown in (A.1). Thus we call Sign(fθ(x)) the empirical witness function for the net-acc test,
and Sign(f*(x)) its population version.
The population and empirical witness functions (in one test run) are plotted in Fig. 1. Comparing
to gmmd, the witness function of net-logit, i.e., the log density ratio, weighs larger at the differential
region which is at the tail of the density p. This is also reflected in the empirical witness functions.
Taking the sign offθ as done in classification accuracy test introduces discontinuity of at the decision
boundary neat x = 2, which leads to comparatively larger variance in view of the mean of the
statistic under H1. This intuitively explains why the net-logit test is more powerful here, and a
quantitative comparison of mean vs. standard deviation of the three tests is given in Appendix A.2.
4	Analysis of test power
In this section we prove the power of the network logit test assuming that fθ is identified by min-
imizing the population classification loss. The proof is based on network approximation analysis,
4
Under review as a conference paper at ICLR 2020
and a key question is the needed network complexity, particularly how it scales with the data dimen-
sionality. We first analyze the general case of smooth densities p and q in RD, and then consider the
important case where p and q lie on or near a smooth low-dimensional manifold, where we reduce
the needed network complexity to depend on the intrinsic manifold geometry only.
4.1	IDENTIFICATION OF fθ BY POPULATION LOSS
Training with the population loss of the classification network can be expressed as (samples from X
and Y are of same number)
mFΘ Lf] = 1( ∕p logι2ef+/q logi+ef),
(3)
where FΘ denotes the class of functions that can be expressed as the difference of the outputs
in the last hidden layer of the classification network. A direct verification shows that f * =
f
arg maxf L[f ] = log P (see e.g. (Goodfellow et al., 2014) where it is proved in terms of D = 1+^),
which characterizes the solution of (3) when the function class is arbitrarily large or something large
enough to contains f*. Then L[f*] = 1 Rf Plog p+pq + / qlog p+q) = JSD(p, q), which is the
Jensen-Shannon divergence. For certain classification network of finite complexity, f* may not be
contained in FΘ, and instead the optimization (3) finds some fθ s.t.
L[fθ] = max L[f].
f ∈FΘ
(4)
We analyze the exact optimization of the population loss only. Since we do not address training
from finite samples in this section, we abuse the notation ofn (which used to denote |Dtr| + |Dte|)
to be n = |Xte| = |Yte|. Then after f = fθ is identified by (4), the test statistic Tn = T can be
computed as in (1), and we denote its expectation as T[fθ] = fθ(p - q).
4.2	TESTING POWER FOR DISTINGUISHING GENERAL p AND q IN RD
We assume that P and q are smooth densities in RD compactly supported on a bounded region Ω,
and without loss of generality Ω = {x ∈ RD, |x| < 1}. The analysis proceeds in the following three
steps, all proofs in Appendix B:
Step 1. Use neural network approximation result to construct a network function fcon ∈ FΘ
that uniformly approximates f*, which bounds |L[fcon] - L[f*]| proportionally. This implies
that L[fcon] > C for some C ≈ L[f*] = JSD(P, q) > 0 (c.f. Proposition B.1). The relation
L[fθ] ≥ L[fcon] then gives thatL[fθ] > C > 0.
Step 2. The lower bound of L[fθ] serves to show that T[fθ] > 4C under H1 via a relation between
the two (c.f. Lemma B.2).
Step 3. We compute and bound the variance of Tn to be at order O(n-1), under both H0 and H1
(c.f. Proposition B.4). This together with the proved O(1) mean (bias) of Tn under H1 will prove
the test power to be asymptotically 1.
Combining these steps gives the following theorem, where we set r =2 in Proposition B.1:
Theorem 4.1. Let the densities P = eu, q = ev be C2 and supported on Ω ,the unit ball in RD, and
u,v ∈ C2(Ω), f * = u-v. IfP 6= q such that JSD(P, q) > 0, then for any small 0 < ε1 < JSD(P, q),
there is a neural network architecture Θ with O(ε-D/2) many trainable parameters, where the
constant depends on the regularity of the second derivative of f*, such that fθ identified by (3)
satisfies that
(1)	ETn = T[fθ] > 4C, where C = JSD(P, q) - ε1 > 0.
(2)Forall n, Var(Tn) = 1 (Var X 〜p(fθ (x)) + Var X〜q (fθ (x))) ≤ n Bθ, Bθ is a constant depending
on the network function family Θ, as defined in (A.3).
(3) Under Ho, √nTn → N(0,σHo) in distribution; Under H1, √n(Tn - T[fθ]) → N(0,σHɪ) in
distribution; σH° = 2VarX〜p(fθ(x)), σH1=VarX〜p(fθ(x)) + VarX〜q(fθ(x)), both ≤ 2BΘ.
We discuss the extension to less regular f* after Proposition B.1, including the case where P and q
nearly non-overlap. In particular, the more relevant situation for two-sample problem is where P and
q weakly differ. The above Theorem directly gives the asymptotic test consistency:
5
Under review as a conference paper at ICLR 2020
Corollary 4.2. Notations and settings as in Theorem 4.1, and for given p, q, the network architecture
Θ has been fixed to satisfy C = JSD(p, q) - ε1 > 0. Let 0 < α < 1 be the two-sample test level,
typically α = 0.05, and the test threshold be Tn = σH0Ψ-1(α), where Ψ(x) := f∞ √√2∏e-y /2dy,
then, as n → ∞, Pr[Tn > τn∣Ho] → ɑ and
Prpn > Tn∣Hι] → 1 - Ψ (√nT[C[- Tn) ≥ 1 — Ψ (√n(4C- Tn))
which ≥ 1 - c0e-c1n for positive constants c0 and c1.
The analysis reveals that the critical regime for two-sample test is when the divergence between p and
q is 〜O(n-1/2). Actually, one may obtain a lower bound of testing power based on the bound of
ETn and Var(Tn ) in Theorem 4.1 and Chebyshev inequality to control the large deviation. This will
prove a finite-sample testing power which is positive and approaching 1 as long as JSD(p, q) exceeds
a constant multiple ofn-1/2, where the constant depends on the choice of Θ which guarantees both
small ε1 and BΘ. We omit the details here.
4.3 Reducing Network Complexity to Be Intrinsic
The above analysis is for general p and q in RD, and when D is large the needed network com-
plexity grows exponentially. We now show that when p and q are on/near manifold M as in many
applications, the network complexity can be reduced to be intrinsic, i.e., depending on the manifold
only, and replacing D by the intrinsic dimension d in Theorem 4.1.
On-manifold densities. When both p and q are degenerate and support on the manifold M, all
the integrals above, L[f ] and T [f ], are carried out on the manifold only. Specifically, assume that
M is compact smooth manifold without boundary, and p and q are smooth on M with respect to
the Riemannian geometry, then the log ratio f * is also smooth on M. This allows to apply the
manifold function approximation result in (Shaham et al., 2018) to obtain ∣∣fco∩ 一 f * ∣∣l∞(m) < ει
with needed network complexity of O(1-d/2) (c.f. Theorem B.6). The rest of the proof remains the
same by replacing all the integrals in Ω ⊂ RD to be on M.
Near-manifold densities. The analysis extends when p and q decay sufficiently fast away from
the manifold, as proved in Proposition B.5. This is because the manifold is locally near Euclidean
and there exists differentiable one-to-one mapping between the manifold chart and the local tangent
plane on local neighborhoods in RD of radius δ > 0, which is an absolute constant determined by
the manifold M. This allows to approximate integrals L[fcon] and L[f *] in RD by their counterparts
on M, using the off-manifold decay of the densities, and the two integrals on M are close due to
uniform approximation off * on the manifold. Proofs in Appendix B.
5	Experiments
This section conducts numerical experiments of the proposed two-sample test and compares with
alternatives, on synthetic 1D and manifold densities and evaluating hand-written digits generating
models. Codes to produce all experiments will be publicly online.
5.1	Synthetic data
1D normal density departure. The following three examples all have p = N(0, 1), and Eg.1.
Mean shift, q = N(δ, 1); Eg.2. Dilation of variance, q = N(0, (1 + δ)2). Eg.3. Mixture with
bump at tail, q as in (2). We examine the tests: (1) net-acc, (2) net-logit, (3) gmmd which sets σ
to be median distance, (4) gmmd-ad which selects σ by maximizing kernel MMD discrepancy on
the training set, and (5) gmmd+, (6) gmmd++ as described in Sec. 3. (1)-(4) only use the test set
when measuring the power, while (5)-(6) access both the training and test sets. More details about
experimental setting-up in Appendix A.
The test powers of all the methods are plotted in Fig. 2 for the three examples. For Eg.1 and Eg.2,
gmmd+, gmmd++ are performing consistently better than the other four which only access the test
data set, particularly in Eg.1. Eg.3 has been discussed in Sec. 3, and net-logit gives stronger power
than gmmd+, gmmd++ when nall > 200, where net-acc remains inferior to the two. Note that
gmmd-ad does worse than gmmd, that is, the median-distance choice of kernel bandwidth σ works
better than the adaptive choice here. Among the four methods (1)-(4), the performances on Eg.1 are
comparable, and net-logit gives better power on Eg.2 and Eg. 3. This is especially the case of Eg. 3,
where net-logit shows the most significant advantage.
6
Under review as a conference paper at ICLR 2020
Figure 2: Three examples of 1D data in Sec. 5.1. Test power of: gmmd (blue), gmmd-ad (green), net-aCC
(pink), net-logit (red), error bar standing for the standard deviation of the estimated power over 20 replicas, and
gmmd+ (blue dash), gmmd++ (green dash). nall = |X| + |Y | including half-half training-testing split.
Densities on a 2D manifold. The example consists of p and q which lie on the sphere S2 , a 2-
dimensional manifold embedded in R3 . A realization of samples X and Y is shown in the left
of Fig. 3. Fig. 3 plots the test power of the 5 methods over increasing density departure δ and
sample size. It can been seen that net-logit gives the fastest growth of power as δ increases and
the strongest average power for all nall, but the variation can be large if the power is not close to
1. Unlike some of the 1D cases, gmmd with median σ does not do better than the trivial power for
all cases (blue solid), even with access to the full data samples gmmd+ the power is only 0.2 with
the largest nall (blue dash). The adaptive choice by maximizing MMD discrepancy on training set
improves the power significantly (gmmd-ad, green solid), but does not do as well as net-aCC, which
again performs inferior to net-logit. The optimal choice of σ (gmmd++, green dash) achieves better
power than net-aCC at nall = 200 and comparable performance with larger nall. net-logit performs
better than gmmd++ and the advantage is more evident when nall > 200. This indicates that larger
sample size can be particularly in favor of network-based tests, which rely on the search in the
network parameter space optimized on a separated training set.
5.2	Generated vs authentic MNIST data
As a real-world data example, we study the task of distinguishing “faked” MNIST samples produced
by a pre-trained generative network from authentic ones. The MNIST dataset consists of gray-scale
hand-written digits of size 28 × 28 falling into 10 classes, which is relatively simple and thus is
viewed to lie near to low-dimensional manifolds in the ambient space of R784. The classifier network
used in net-logit test is a convolutional neural network (CNN) with 2 convolutional layers. More
details about the generative and classification networks in Appendix A.
We compare (1) net-aCC (2) net-logit (3) gmmd (4) gmmd-ad on two samples X and Y , half of
D = X ∪ Y used for training. X consists of authentic MNIST samples, and Y of a mixture of
authentic and faked ones, i.e. p = pdata and q = (1 - δ)pdata + δpmodel, δ ∈ [0, 1]. The test power is
evaluated on 400 test runs and the training is repeated for 20 replicas. The results for increasing δ
and sample size nall = |D| up to 500 is shown in Fig. 5, where net-logit gives the strongest power
throughout all cases, and the two network-based tests significantly outperforms the other two when
nall ≥ 300. The adaptive choice of kernel bandwidth also improves the power over the median-
distance choice, shown by the better power of gmmd-ad than gmmd. The standard deviation of the
net-aCC and net-logit power is less than that of gmmd-ad power when nall = 300 and δ ≥ 0.4, when
the former two give near to 1 power. We also observe that the training of the CNN classifier in this
experiment is more stable than that of the previous fully-connected network on low-dimensional
synthetic data, as revealed in the training error evolution plots, c.f. Fig. A.1 Fig. A.5. With another
pre-trained model which generates faked images that are closer to authentic ones, net-logit again
Figure 3: Test power of the different tests on data on sphere in R3 in Sec. 5.1. Markers same as in Fig. 2.
7
Under review as a conference paper at ICLR 2020
ɪ	ɪ	ɪ	ɪ	3	ɪ	ɪ	JL
H	5	ɪ	5	Q	ɪ	ɪ	q
ɪ	ɪ	ɪ	q	ɪ	Q	ɪ	3
2		ɪ	ɪ	ɪ		3	7
ɪ	S	q	ɪ	JL	ɪ	q	ɪ
ɪ	ʌ	ɪ	1_	jγ	q	ɪ	ɪ
3		q	Sl	2	Q	ɪ	ɪ
z	N	6	ɪ	ɪ	ɪ		ɪ
ɪ	ɪ	2		q	ɪ	q	2
jγ	ɪ	ɪ	ɪ	ɪ	ɪ		ɪ
ɪ	7	3	ɪ	ɪ	3	ɪ	ɪ
8	y	O	g	ð	3	g	7
Detected by gmmd-ad	Detected by net-logit
Figure 4: Two-sample problem of differentiating p, the density of authentic MNIST digits, and q which contains
a δ = 0.4 fraction of digits “faked” by a generative model. |X| = |Y | = 500. The gmmd-ad and net-logit
tests use half as training set, and test on the other |Dte | = 500 samples. Left and middle: the most likely fake
digits identified by the empirical witness functions of the two tests, red box indicates authentic digits incorrectly
identified. Right: The test statistic T (HI) and the histogram of its value under 1000 permutation tests (Ho).
Figure 5: Test power of gmmd (blue) gmmd-ad (green) net-acc (pink) net-logit (red) on differentiating authentic
vs synthesized MNIST digits produced by a generative model, where sample X has all authentic ones, and δ
stands for the fraction of synthesized ones in Y , nall = |X| + |Y | including half-half training-testing split.
shows the best discriminative power, net-acc gives comparable performance starting nall = 300,
while gmmd and gmmd-ad gives trivial power up to nall = 500, c.f. Fig. A.4.
Setting nall = 1000, δ = 0.4, the results of gmmd-ad and net-logit in one test run is shown in
Fig. 4. Based on the nall = 500 plot in Fig. 5, both tests shall have non-trivial power, and that of
net-logit shall be close to 1. In this test, both methods correctly rejects H0, yet the net-logit statistic
deviates from the distribution of T∣H0 more significantly, indicating stronger power (shown in the
histogram plots). To compare the detecting ability of the empirical witness function W of gmmd-ad
and net-logit, for each method, we sort the 250 samples in Yte (among which 100 are faked ones)
in ascending order of the value of W and select the first 100 samples. These are samples which the
model views as most likely to be faked ones. The success rate of identifying faked samples is ~ 60
by gmmd-ad W, and ~ 90 by net-logit W. The first 48 most likely faked digits identified with both
witness functions are plotted in Fig. 4, where gmmd-ad W incorrectly includes 5 authentic samples,
and none by net-logit W.
6 Conclusion
The paper proposes to use estimated log ratio to compute a two-sample statistic once a classification
network has been trained on a split training set. The proposed statistic empirically demonstrates
stronger testing power than previously studied neural network classifier tests based on classification
accuracy. It also compares favorable to gaussian kernel-based MMD in certain settings, especially
for higher dimensional data, including distinguishing generated MNIST digits from authentic ones.
The proposed test has more advantage with large samples, due to that larger training set makes
the training more stable, as well as its linear computational complexity and scalable algorithm.
Theoretically, we prove the power of the proposed test when the network is sufficiently parametrized,
and reduce the needed network complexity to be intrinsic when p and q lie on or sufficiently near to
low-dimensional manifolds in possibly high-dimensional space,
The analysis in this paper gives a positive result towards justifying the power of two-sample tests
based on training a classification network. However, the proof is based on a network approxima-
tion analysis and optimization of population loss, which means that the derived testing power only
applied when the training perfectly identifies the global optimizer, a situation not necessarily hap-
pening in practice. In experiments we observe that the performance of the network-based tests has
larger variance than traditional methods like kernel MMD, due to the instability of the training, par-
ticularly with small training size. Thus, more understanding of the network optimization, which is
not addressed in the current paper, is needed so as to better understand network classification two-
sample tests and to develop better methods. At last, we have not systematically explored the effects
of different network architecture on the performance, which surely has an influence.
8
Under review as a conference paper at ICLR 2020
References
Niall H Anderson, Peter Hall, and D Michael Titterington. Two-sample test statistics for measuring discrepan-
cies between two multivariate probability density functions using kernel-based density estimates. Journal of
Multivariate Analysis, 50(1):41-54, 1994.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks. In Inter-
national Conference on Machine Learning, pp. 214-223, 2017.
Steffen Bickel, Michael Bruckner, and Tobias Scheffer. Discriminative learning for differing training and test
distributions. In Proceedings of the 24th international conference on Machine learning, pp. 81-88. ACM,
2007.
Steffen Bickel, Michael Bruckner, and Tobias Scheffer. Discriminative learning under covariate shift. Journal
of Machine Learning Research, 10(Sep):2137-2155, 2009.
Karsten M Borgwardt, Arthur Gretton, Malte J Rasch, Hans-Peter Kriegel, Bernhard Scholkopf, and Alex J
Smola. Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics, 22
(14):e49-e57, 2006.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMP-
STAT’2010, pp. 177-186. Springer, 2010.
Xiuyuan Cheng, Alexander Cloninger, and Ronald R Coifman. Two-sample statistics based on anisotropic
kernels. arXiv preprint arXiv:1709.05006, 2017.
Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton. A kernel test of goodness of fit. JMLR: Work-
shop and Conference Proceedings, 2016.
Kacper P Chwialkowski, Aaditya Ramdas, Dino Sejdinovic, and Arthur Gretton. Fast two-sample testing with
analytic representations of probability measures. In Advances in Neural Information Processing Systems,
pp. 1981-1989, 2015.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-
parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.
Jerome Friedman. On multivariate goodness-of-fit and two-sample testing. Technical report, Stanford Linear
Accelerator Center, Menlo Park, CA (US), 2004.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information process-
ing systems, pp. 2672-2680, 2014.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A kernel
two-sample test. Journal of Machine Learning Research, 13(Mar):723-773, 2012a.
Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano Pontil, Kenji
Fukumizu, and Bharath K Sriperumbudur. Optimal kernel choice for large-scale two-sample tests. In Ad-
vances in neural information processing systems, pp. 1205-1213, 2012b.
James J Higgins. Introduction to modern nonparametric statistics. 2003.
Wittawat Jitkrittum, Zoltan Szabo, Kacper P Chwialkowski, and Arthur Gretton. Interpretable distribution
features with maximum testing power. In Advances in Neural Information Processing Systems, pp. 181-
189, 2016.
Wittawat Jitkrittum, Wenkai Xu, Zoltan Szabo, Kenji Fukumizu, and Arthur Gretton. A linear-time kernel
goodness-of-fit test. In Advances in Neural Information Processing Systems, pp. 262-271, 2017.
Takafumi Kanamori, Taiji Suzuki, and Masashi Sugiyama. f -divergence estimation and two-sample homo-
geneity test under semiparametric density-ratio models. IEEE Transactions on Information Theory, 58(2):
708-720, 2011.
Takafumi Kanamori, Taiji Suzuki, and Masashi Sugiyama. f -divergence estimation and two-sample homo-
geneity test under semiparametric density-ratio models. IEEE Transactions on Information Theory, 58(2):
708-720, 2012.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Erich L Lehmann and Joseph P Romano. Testing statistical hypotheses. Springer Science & Business Media,
2006.
9
Under review as a conference paper at ICLR 2020
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas PoCzos. Mmd gan: Towards deeper
understanding of moment matching network. In Advances in Neural Information Processing Systems, pp.
2203-2213,2017.
Yujia Li, Kevin Swersky, and RiCh Zemel. Generative moment matChing networks. In International Conference
on Machine Learning, pp. 1718-1727, 2015.
Qiang Liu, Jason Lee, and MiChael Jordan. A kernelized stein disCrepanCy for goodness-of-fit tests. In Inter-
national Conference on Machine Learning, pp. 276-284, 2016.
James R Lloyd and Zoubin Ghahramani. StatistiCal model CritiCism using kernel two sample tests. In Advances
in Neural Information Processing Systems, pp. 829-837, 2015.
David Lopez-Paz and Maxime Oquab. Revisiting Classifier two-sample tests. arXiv preprint arXiv:1610.06545,
2016.
Aditya Menon and Cheng Soon Ong. Linking losses for density ratio and Class-probability estimation. In
International Conference on Machine Learning, pp. 304-313, 2016.
Shakir Mohamed and Balaji Lakshminarayanan. Learning in impliCit generative models. arXiv preprint
arXiv:1610.03483, 2016.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using
variational divergenCe minimization. In Advances in neural information processing systems, pp. 271-279,
2016.
Aaditya Ramdas, Aarti Singh, and Larry Wasserman. ClassifiCation aCCuraCy as a proxy for two sample testing.
arXiv preprint arXiv:1602.02210, 2016.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the ConvergenCe of adam and beyond. arXiv preprint
arXiv:1904.09237, 2019.
Mark D Reid and Robert C Williamson. Information, divergenCe and risk for binary experiments. Journal of
Machine Learning Research, 12(Mar):731-817, 2011.
Robert J Serfling. Approximation theorems of mathematiCal statistiCs, 1981.
Uri Shaham, Alexander Cloninger, and Ronald R Coifman. Provable approximation properties for deep neural
networks. Applied and Computational Harmonic Analysis, 44(3):537-557, 2018.
Ohad Shamir and Tong Zhang. StoChastiC gradient desCent for non-smooth optimization: ConvergenCe results
and optimal averaging sChemes. In International Conference on Machine Learning, pp. 71-79, 2013.
Bharath K SriPerumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Scholkopf, and Gert RG Lanckriet.
On integral probability metriCs,\phi-divergenCes and binary ClassifiCation. arXiv preprint arXiv:0901.2698,
2009.
Masashi Sugiyama, Takafumi Kanamori, Taiji Suzuki, Marthinus Christoffel du Plessis, Song Liu, and Ichiro
Takeuchi. Density-difference estimation. Neural Computation, 25(10):2734-2775, 2013.
Dougal J Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alex Smola, and
Arthur Gretton. Generative models and model criticism via optimized maximum mean discrepancy. arXiv
preprint arXiv:1611.04488, 2016.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and
momentum in deep learning. In International conference on machine learning, pp. 1139-1147, 2013.
Max Wornowizki and Roland Fried. Two-sample homogeneity tests based on divergence measures. Computa-
tional Statistics, 31(1):291-313, 2016.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:103-114,
2017.
Dmitry Yarotsky. Optimal approximation of continuous functions by very deep relu networks. arXiv preprint
arXiv:1802.03620, 2018.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.
10
Under review as a conference paper at ICLR 2020
Figure A.1: Left: MMD discrepancy on trained set used by gmmd-ad to select kernel bandwidth σ. Middle and
right: training of the classification network used in net-based tests. On the synthetic 1D dataset.
Figure A.2: Histogram of estimated test power from 400 test runs of gmmd, net-acc and net-logit over 20
replicas of training (no training for gmmd), on the example in Sec. 3, Fig. 1.
A Details of Experiments
A.1 Training in Sec. 3 and 5.1
Training of networks. In all the experiments, the classifier network used by net-acc and net-logit
is a two-layer fully-connected neural network with 32 hidden nodes in each hidden layer, and the
bottom layer has the same dimension as the input data. The training of the network is conducted via
100 epochs of Adam with learning rate 10-3, and batch size 100 when the size of training set > 100.
A typical plot of evolution of training loss and training error is given in Fig. A.1. Training via SGD
with momentum 0.9 produces similar result. The result is qualitatively the same when the number
of hidden units varies from 16 to 1024. We have not investigated the optimal choice of network
architecture hyperparameters for the two-sample problem.
Adaptive choice ofσ in gmmd-ad. In the training phase, the algorithm computes the gaussian kernel
MMD discrepancy
11	2
TMMD (X, Y)	=	∣χ∣2	£	kσ (X,x	) +	∣γ∣2	£	kσ (y，y ) -	∣χ ∣∣γ |	£	k (X，y)
|X| x,x0∈X	|Y| y,y0∈Y	|X||Y| x∈X, y∈Y
on the training set X = Xtr, Y = Ytr, for a range of values of the kernel bandwidth σ, i.e.
σ = {2-3, ∙ ∙ ∙ , 23}. kσ(x, y) = exp{- |x-y2| } is the isotropic gaussian kernel. A plot of MMD
discrepancy as a function of varying σ is given in Fig. A.1. The σ which maximizes the MMD dis-
crepancy on the training set is then chosen to compute the test statistic on the test set. The method is
equivalent to the training process in (Li et al., 2017) with only one trainable parameter which is the
kernel bandwidth σ. The MMD test statistic also takes the form as TMMD in (Gretton et al., 2012a;
Li et al., 2017).
A.2 More details about results in Sec. 3
Test power. The way of computing the test power is empirical and has randomness: for kernel mmd
the variation is due to the finite number of runs (nrun times), and for network based tests there is
extra variation due to the stochastic optimization of the network. Thus we use experiment replicas
to recored the variations of the test power.
The empirical distribution of the test power of the three methods over training replicas is given in
A.2, corresponding to the experiment in Fig. 1. The plots of the two net-based methods indicate
large variation of the power given by each trained network, that is, the “quality” of the trained net
to discriminate the two densities varies. This instability is due to limited training samples as well
as the randomness in the optimization algorithm. We observe decreased power variation with larger
11
Under review as a conference paper at ICLR 2020
	gmmd	gmmd+	gmmd++	net-acc	net-logit
mean	19.14	46.63	-^57.29	19.98	78.09
std	1.95	2.49	1.578	10.43	20.56
median	19.63	47.13	57.38	17.63	84.13
Table A.1: The mean, standard deviation (“std”) and median of the test power of the various methods computed
from nrun = 400 test runs over nrep = 20 replicas on the 1D example in Sec. 3. The gmmd, net-acc, net-logit
tests are computed on |Xte| = |Yte | = 100 samples, where net-acc and net-logit train a classification network
on another training set of size |Xtr| = |Ytr| = 100. gmmd only uses the test set and sets the kernel bandwidth
σ to be the median distance. gmmd+ and gmmd++ accesses both the training and test sets, where gmmd+ uses
the median distance as σ, and gmmd++ reports the best power over varying range of choices of σ, as described
in Sec. 3. The results of gmmd, net-acc, net-logit are also reported in Fig. 1.
training set, where the stochastic optimization converges to solutions of lower error and the resulted
net gives better two-sample test power. However, the two-sample problem itself is expected to be
easier with larger n too.
Table A.1 gives the full table of test power including that of the methods gmmd+ and gmmd++.
Equivalent form of net-acc test. Here we show that the net-acc test studied in (Lopez-Paz & Oquab,
2016) is equivalent to using Sign(fθ) instead of fθ in (1) when nX = nY , up to multiplying and
adding constants. Specifically, by the definition of test statistic in (Lopez-Paz & Oquab, 2016), and
recall that |Xte| = ∣Y‰∣ = 2 |Dte|, Sign(Z) = 1 if Z ≥ 0 and -1 if z < 0,
Tnet-acc = 2 ( jχ-1 X 1{fθ(x)≥0} + jγ^-∣ X 1{fθ(y)<0}
e x∈Xte	e y∈Yte
2 (* XXte 2(l + M (X))) + 击 yXe 1(I-SigW' (X)))
(A.1)
11
1
^Xtel
Sign(fθ (X)) -
x∈Xte
iY1-r X sign(fθ(X))).
te y∈Yte
2 + 4
Quantitative comparison of mean and std of test statistics. Let w be the population witness
function of the three methods respectively, and define
Mean := Ex〜p,γ〜q(W(X) - W(Y)),
Std := JVarx〜P(W(X)) + VarY〜q(W(Y)).
For tests using T = R W(P - q) as the statistic, such as in net-logit and net-acc, by independence
of the samples the mean and variance of T are Mean and Std∕√n respectively. For kernel MMD,
the actually test statistic is computed via quadratic sums, however the mean remains the same, and
Std/√n will be a lower bound of the standard deviation of the MMD statistic (Serfling, 1981).
Strictly speaking, the test statistic is computed from empirical rather than the population witness
function. For net-logit and net-acc, considering population witness function is as if the training
is able to identify the exact optimizer which lies inside the representable function family of the
network, an idealized scenario. With this idealization, the relation of Mean and Std to the test power
can be made rigorous making use of the asymptotic normality of the test statistic (as independent
sums), as done in Section 4. The conclusion gives that the larger the Mean, and the smaller the Std,
the more powerful the test is going to be. For kernel MMD, these two quantities similarly indicate
the testing power, see e.g. (Serfling, 1981; Cheng et al., 2017). Thus we will use Mean and Std for
all three methods for comparison.
To remove the scaling equivalence of test statistics (a test statistic multiplied by a positive constant
gives the same test power), we will use the ratio of Mean and Std as an indicator of test power.
For the 1D example in Section 3, due to the explicit formula of p and q the values of Mean and
Std can be analytically computed, which are shown in Table A.2. The net-logit gives the largest
ratio in this comparison. The normalized witness functions are plotted in Fig. A.3, where a constant
12
Under review as a conference paper at ICLR 2020
	Mean	Std	Mean/Std
gmmd	0.0087	0.0421	0.2075
net-acc	0.1579	0.6087	0.2594
net-logit	0.2445	0.9011	0.2714
Table A.2: The values of Mean, Std, and their ratio of the three tests, where p and q are as in the 1D example
in Sec. 3, c.f. Fig. 1.
Figure A.3: Plots of the population witness functions normalized to have Std = 1, of the three tests in Sec. 3,
c.f. Fig. 1, Table A.2.
is multiplied to each w respectively to enforce Std = 1. It can be seen that the net-logit witness
function gives the largest weights to the differential region of p and q in this example.
A.3 Experimental details in Section 5.1
1D normal density departure experiment. The experiments with (1)-(4) use 400 test runs to
estimate the power, and are repeated for 20 replicas. The test with (5) and (6) uses 200 test runs
to estimate the power, since these gmmd methods demonstrate less variation in estimated power.
Training and testing split is half-and-half in all cases.
Densities P and q on the 2D manifold. The construction of Xi 〜P and yj∙〜 q are as below:
xi = T(ui), yj = T(vj) where T : R2 → R3 is a smooth mapping from unit square to the spherical
surface given by
T(x1, x2) =
1
R
R=1.5,
and ui, vj are i.i.d. copies of random variables u and v in R2 distributed as
U = tu + ηu ,	V = tv + rηv ,	ηu , ηv 〜N(O,aI2) ,	6 = 0.05,
tu 〜 uniformly on a quater circle in [0,1] × [0,1],
tv 〜 the distribution of tu rotated around (我,我)by angle δ,
where the 4 random variables are all independent.
A.4 Experiment on MNIST data in Sec. 5.2
The pre-trained generated model is based on a convolutional auto-encoder:
c5x5x1x16 - re - ap 2x2 - c5x5x16x32 - re - ap 2x2 - fc128 - re
-fc10-re — code space R10
- fc128 - re - ct 5x5x128x32 - re
- ct5x5x32x16 (upsample 2x2) - re - ct5x5x16x1 (upsample 2x2) - Euclidean loss
where “c” stands for convolutional layer, “ct” for transposed convolutional layers, “re” for Relu
activation, and “ap” for average pooling. The auto-encoder is trained on 50000 MNIST dataset for
20 epochs using Adam with learning rate decreasing from 10-3 to 10-6 and batch size 100.
13
Under review as a conference paper at ICLR 2020
Figure A.4: Same plot as Fig. 5 with another pre-trained generative model which produces faked images that
are closer to the authentic ones.
Figure A.5: Same plot as Fig. A.1 on MNIST data.
The sampling of generative model is conducted by adding a small isotropic gaussian noise (“gig-
gering”) to the 10-dimensional codes of authentic MNIST digits computed by an encoder, and then
mapping through the decoder to R784 .
We also prepare another generative model by removing the bottleneck layer in the above auto-
encoder architecture and retrain the model, which gives smaller reconstruction error and a higher-
dimensional code space of R128 . The generative model is conducted in the same way by sampling
in the code space using gaussian noise of smaller variance per coordinate. This produces faked
images that are closer to the authentic ones in Euclidean distance in R784 , however less explore the
“manifold” of pdata. The test power of the four methods is shown in Fig. A.4.
The classification network used in net-logit is the following CNN
c5x5x1x16 - re - ap 2x2
-	c5x5x16x32 - re - ap 2x2
-	fc128 - re - fc2 - softmax loss
where dropout is used between the last 2 fully-connected layers. The classification CNN is trained
for 100 epochs using Adam with learning rate 10-3 and batch size 100. A typical plot of evolution
of training loss and training error is given in Fig. A.5.
The procedure of adaptive selection of σ by gmmd-ad is same as in Sec. 5.1, where the bandwidth
search range is σ = {2-1,…，26}.
B Proofs and Details of the Analysis in Section 4
B.1	Proofs and details for Theorem 4.1
B.1.1	CONSTRUCTION OF fCON BY NETWORK
Suppose that P and q are non-vanishing inside Ω, and let P = eu and q = ev for smooth potential
functions U and v, then f * = U - V is also smooth. We first assume that f * is has properly bounded
derivatives, then standard network approximation theory, e.g. that in (Yarotsky, 2017), guarantees
that one can approximate f * by a network output function fcon of a multi-layer fully-connected
network with
kfcon - f*kL∞(Ω)= O(N-r/D)
where N is the number of parameters in the network, and the constant depends on the regularity of
r-th derivative of f*. This leads to the following guaranteed positive lower bound of L[fcon]:
Proposition B.1. Let the densities P = eu, q = ev be C r and supported on Ω ,the unit ball in RD,
and u, v ∈ Cr (Ω), f * = U — V. If P = q such that JSD(p, q) > 0, then for any 0 < ε1 < JSD(p, q),
14
Under review as a conference paper at ICLR 2020
there is a neural network architecture Θ with O(ε- /r) many trainable parameters and 于/ ∈ FΘ
such that
L[fcon] > JSD(p, q) - ε1 := C > 0.
Note that C can be made arbitrarily close to largest possible value of L[f] which is JSD(p, q) given
sufficient network complexity. More recent approximation result which improves the approximation
rate, such as (Yarotsky, 2018), will improve the complexity needed accordingly.
We now consider the case that f * is less regular in terms of having derivatives of larger magnitude.
While mathematically the previous argument is still valid, the worse constant in the approximation
bound indicates difficulty for the network to approximate such f*. This can be improved by observ-
ing that in the previous proof it suffices to make kf * - fcon kL1 (with the measure p and q) small
rather than in the L∞ norm. This allows, e.g., using the network to approximate f*, which is a
regularized version of f*, and thus can be more efficiently parametrized by the network, as long as
kf* - f* kL1 < ε2, and it will give L[fcon] > JSD - ε2 - ε1. This argument will extend to the case
where f* = u - v has places of discontinuity or other singularity as long as such places are of small
measure under (p + q).
A specific type of singularity of f* is when its magnitude diverges to ∞ or very large. This happens
when p almost vanishes on a region where q takes significantly large value and vice versa. The
nearly divergent value of f * surely creates a difficulty for network approximation, however, in this
case f* again can be replaced by a bounded and regularized version f*, at least locally, which will
produce an a comparably large L[f*]. We then approximate f* by the network function fcon and
obtains L[fcon] > C > 0 for some sufficiently large C.
When the regions where p and q nearly non-overlap are large, the above argument may lead to C
much less than JSD(p, q). However, note that such situation is actually a trivial case for two-sample
testing: ifp and q already differ significantly then many test statistics will give strong testing power.
Because that two-sample test is trying to distinguish differential densities reliably with as small
number of samples as possible, the the more interesting situation is the “weak-separation” regime of
p and q, that is, the departure of q from p is small and then f* is not far from zero.
Due to above reasons, in what follows we focus on f * which is regular, bounded and has properly
bounded derivatives. We will see that the critical regime for two-sample detection is when the
magnitude of (P - q) and thus that of f * 〜O(n-1/2) which is asymptotically 0 as n increases.
B.1.2	BOUNDINGT[f] BYL[f]
Lemma B.2. For any f so that the integrals are defined, T [f] ≥ 4L[f].
The relaxation in Lemma B.2 may not be sharp, particularly, when p and q nearly non-overlap on
their supports, T[f*] = 2SKL(p, q) diverges to infinity, while L[f*] remains bounded (by 2 log 2).
When f is close to zero, which as discussed above is the more relevant scenario for two-sample test,
the following lemma quantifies the tightness of the relaxation
Lemma B.3. For any f s.t. f2 is integrable w.r.t p and q,
1	f2
0 ≤ 2T[f] - 2L[f] ≤ J(P + q)-2.
B.1.3 B OUNDING THE VARIANCE OF Tn
By the definition of Tn and the independence of Xi ’s and Yi ’s, the random variable Tn is asymptot-
ically normal by Central Limit Theorem, and
Var(Tn) = 1(VarX 〜p(f(X)) + VarY 〜q (f (Y))).	(A.2)
The following Proposition proves that both of the variances of f(X) and f(Y) are bounded by O(1)
constants depending on the network function family.
Proposition B.4. Given network function family FΘ, suppose that
bΘ0 = SUp kfkL∞(Ω), Bg)= SUp LiP(f),
f∈FΘ	f∈FΘ
15
Under review as a conference paper at ICLR 2020
are both finite, Ω is the unit sphere in RD, where densities P and q are supported on. Thenfor any
f ∈FΘ,
VarX〜p(f (x)), VarX〜q(f (x)) ≤ min{2B((0), B(^)}2 := Bl).	(A.3)
The boundedness of BΘ(0) and BΘ(1) relies on the regularization of the network function family FΘ .
For a given Θ, B(^) and By) may be very large compared to IlfkL∞(ω) and Lip(f) of the trained
fθ, which leads to a loose upper bound of the variance. In practice, regularization techniques may
lead to smaller values of |f| and Lip(f) while at a price of smaller L[f], revealing the trade-off
between stability of the test statistic and the sensitivity to detect differential density departure at
large samples. We do not further pursue this problem in the current paper.
B.1.4 Proofs
Proofof Proposition B.1. Under the assumptions, f * can by approximated by fcon ∈ Fi such that
kfcon - f *kL∞(Ω) ≤ ε1.
Writing fcon as f, we then have that
|L[f ] — L[f"
≤ ε1,
2ef	,	2ef
Ef - logE
q (log1⅛ - logi⅛
P(X)If (X)- f "Lq|f (X)- f *(x)1)，
where the second inequality uses that log ɪ+^ and log(1 + eξ) are LiP 1. This implies the claim.
ɛ □
Proof of Lemma B.2. By definition,
2
2L[f] =	P log
1 + e-f
+ / q log1⅛
=-
≤-Z
P log『-Z q log
p log e-
/ 2(p-q)
f f	f
2 — q log e 2
=2 T [f].
1 + ef
2
“	1	片 1+ eξ、 ξ∖
(for any real number ξ, --- ≥ e2)
□
Proof of Lemma B.3.
/
Z
Z
/
Z
1T[f] - 2L[f]
plog
Jqf
1 + ef
^2ef∕2
plog
q log
2ef	f 1	2
E-Jq logE
1 + ef
^2ef∕2
f
P 2 -
—
+
e-f/2 + ef/2
(p + q) log-----2------
and by that ex+e X ≤ ex2/2,
2T[f] - 2L[f ] ≤ /(p + q) log ef2/2 = / f2-(p + q).
□
16
Under review as a conference paper at ICLR 2020
Proof of Proposition B.4. Let L = Lip(f),
Varx〜P(f (x)) ≤ Ex|f (x) - f (0)|2
|f(x)
Jω
- f(0)|2p(x)dx
≤ L2
|x|2p ≤ L2	p
L2.
This proves that Varx〜p(f(χ)) ≤ (B(1))2 as L ≤ B(1). Meanwhile,
Zf(X)- f (0)∣2p(x)dx ≤ (2kf kL∞(Ω))2 ≤ (2B(0))2,
Jω
which proves the other upper bound. Same proof for q.
□
Proof of Theorem 4.1. Using Proposition B.1 with r = 2, there exists fcon ∈ FΘ such that L[fcon] >
C > 0. The optimization (4) then gives that L[fθ] ≥ L[fcon] > C. Lemma B.2 then gives that
T[fθ] ≥ 4L[fθ] > 4C. This proves (1). (2) and (3) follow from (A.2) and Proposition B.4, and
Central Limit Theorem.	□
B.2 Extension to near-manifold densities
Like before, suppose fθ is the minimizer of population training loss, and fcon ∈ FΘ is to be con-
structed to approximate the log density ratio f *. The Step 2 and 3 of proving Theorem 4.1 remain
the same, thus it suffices to establish the “manifold intrinsic complexity” version of Proposition B.1,
which is the following
Proposition B.5. Let the densities P = eu, q = ev be C2 and supported on Ω ,the unit ball in RD,
andu,v ∈ C2(Ω), f* =U — V. Let M ⊂ Ω be a compact Smooth manifold ofdimension d, and P,
q decay exponentially fast away from M, that is, p, q ∈ Pσ defined to be
Pσ = {p smooth density SuPPorted on Ω, s.t. PrX〜p[d(X, M) > t] ≤ cιe-c2三},	(A.4)
where d(x, M) := inf y∈M kx - yk2 for any x ∈ RD, and c1, c2 are absolute positive constants.
We will need σ to be a small constant. Suppose P 6= q, JSD(P, q) > 0, then for any ε1 > 0, there
is a neural network architecture Θ with O(ε-d/2) many trainable parameters and fcon ∈ Fθ such
that
L[fcon] > JSD(P, q) -(10ε1+c(f*,Θ,M)σ) :=C>0,
where we need ε1 and σ to be small enough to guarantee that
c4(M)σ < 9,	10ει + c(f *, Θ, M)σ < JSD(p, q)
where c4 is a constant determined by the manifold and atlas, c(f *, Θ, M) a constant determined by
f*, Θ, and manifold atlas.
Again we only consider sufficiently regular f * which has properly bounded 2nd derivative, by the
comments below Proposition B.1.
The main elements of proving Proposition B.5 are
(1)	Replacing the integrals in RD by a counterpart on M, using the exponential away-manifold
decay of the densities P and q .
(2)	The uniform approximation of f* by fcon on M.
The second argument is also used to prove the “on-manifold” case in Section 4.3.
To proceed, we reproduce the needed result in Shaham et al. (2018), including the construction of
atlas, the δ-wide neighborhood around manifold, and other notations, for completeness.
B.2.1	Result and set-up from Shaham et al. (2018)
We first establish some notation for the manifold and atlas cover. Recall that M be a smooth,
compact manifold embedded in Ω ⊂ RD. We cover M with an atlas {(Ui, Φi)}K=∖, where Ui =
B(xi, δ) ∩ M is an open set on M and φi : Ui → Rd is the map that takes Ui to the local tangent
17
Under review as a conference paper at ICLR 2020
space around xi ∈ Ui. We also define the map ψi : φi(Ui) → Ui, which is the inverse of φi due to
the one-to-one correspondence between Ui and φi (Ui).
We can choose δ small enough such that for any x, x0 ∈ Ui , there exist positive αi and βi s.t.
αikφi(x) - φi(x0)k2 ≤ dM(x,x0) ≤ βikφi(x) - φi(x0)k2,	(A.5)
and for all i, αi ≥ αM, βi ≤ βM, and αM, βM are absolute constants. In particular, if the manifold
is locally near Euclidean, then αi, βi are close to 1. For each neighborhood, this is possible for some
δi > 0 due to manifold smoothness, and the constants in that neighborhood will depend on the local
curvature of the manifold. There exist global δ, αM, βM due to compactness of the manifold.
Using the covering atlas, there exists a partition of unity {ηi }iK=1 such that supp(ηi) ⊂ Ui, ηi ∈
C∞ (M), and PiK=1 ηi(x) = 1 for all x ∈ M. The following theorem has been established under
this setting:
Theorem B.6 (Shaham et al. (2018)). Notations and assumptions as above, let h ∈ C2 (M) and
have a bounded Hessian. Then there exists a four layer feed network hN with rectified linear unit
activations, DK nodes in the first layer, 8dN nodes in the second layer, and 2N nodes in the third
layer, such that
kh - hN∣∣L∞(M) ≤ N2/d,
where Ch depends on ∣∣h∣2, ∣∣V2h∣2 and the manifold and atlas. The total number of trainable
parameters in the network is O(N).
The proof of Theorem B.6 also constructs for each Ui a rectangle neighborhood Ni in RD which is
φ(Ui) × (-δ, δ)D-d, thus φ(Ni) = φ(Ui). Then the partition of unity function ηi is extended to Ni,
given by ηi(χ) = ηi(ψi ◦ φi(χ)), for any X ∈ N%. The union N := ∪K=ιM forms a neighborhood
of M in Ω. For any P ∈ P, We consider σ sufficiently small such that Ως∖Nςδ P is exponentially
small and negligible - when ci, c2 in (A.4) are 1, then σ < ɪθδ suffices, and generally, we need
c4(M)σ < 1 Where c4(M) is a constant depending on manifold and atlas (the δ) and c1, c2. By this
truncation argument, in the following analysis we assume that P and q are supported on Nδ.
B.2.2 Technical lemmas
This implies that the extended partition of unity function η is LiP in RD, that is
Lemma B.7. For i = 1,…，K, Lip(电)≤ L*m which is an absolute Constant
Proof of Lemma B.7. For a fixed i, Since ηi is smooth on M and compactly supported on Ui , we
assume that
lηi(yι) 一 η(y2)∣ ≤ cdM(y1,y2), ∀y1,y2 ∈ U
Now for x1, x2 ∈ Ni, let y1 = ψi ◦ φi(x1), y2 = ψi ◦ φi(x2), thus
Ini(XI) 一 ηi(χ2)| = lηi(yι) 一 nW) ≤ cdM(yι,y2)
≤ cβi∣φi(y1) 一 φi(y2)∣2 (by (A.5))
= cβi∣φi(X1) 一 φi(X2)∣2 ≤ cβi∣X1 一 X2∣2,
this proves that Lip(ni) ≤ cβi, where C is the Lip(ni) w.r.t. manifold geometry. Taking maxi-
mum over i gives L%m which is absolute content determined by the atlas and partition of unity
construction.	□
Lemma B.8. There is c3 an absolute constant s.t. for any P ∈ Pσ, σ > 0,
d(X, M)P(X)dX
< c3σ.
ProofofLemma B.8. Let X 〜p, then d(X, M) is a non-negative random variable, and
d(X, M)P(X)dX = Ed(X, M) = ∞ Pr[d(X, M) > t]dt
RD	0
∞
≤	C1e-c2 σ dt = σ —,
0	c2
which proves the claim with c3 = c1 /c2 .
□
18
Under review as a conference paper at ICLR 2020
This immediately gives the following lemma
Lemma B.9. For any ξ : Ω → R which is Lip continuous, and ξ∣M = 0,thenfor any P ∈ Pσ,
I ∣ξ(χ)∣p(χ)
Jω
dx < Lip(ξ)c3 σ.
ProofofLemma B.9. By compactness and smoothness of M, for any X ∈ Ω, there exists ψ(χ) ∈ M
s.t. kψ(x) - xk2 = d(x, M). thus
lξ(X)I = lξ(X)- ξ3(X))I ≤ LiP⑹kx - ψ(x)k2 = LiP⑹d(X, M).
Then
L Iξ (X)Ip(X)dX ≤ L
Lip(ξ)d(X, M)p(X)dX < Lip(ξ) ∙ c3σ,
where the last < is by Lemma B.8.
The following Lemma fulfills element (1) in the proof.
Lemma B.10. Let g : Ω → R hasfinte Lip(g) and kgkL∞(Ω), P ∈ Pσ, and define
K
P(X) = Eni(X)Pi(X),
i=1
where Pi is an atlas dependent projection of the density P to Ui, the explicit formula to be given
below, then
/ g(X)P(X)dX — / g(X)P(X)d，M(X)
Ω	JM
≤ K(IIgkL∞(Ω)Lη,M + LiP(g)(I + βM))c3σ.
ProofofLemma B.10. Let Hi := φi(Ui) = φi(Ni) for each i = 1,…，K,
I P(X)g(X)dX ≈ I p(x)9(x) ɪ2 ni (x)
JQl	JQl	i=1
(error 1)
K
=Xi=1ZNi
≈XZ
i Ni
=Xi ZHi
=:Xi ZUi
g(X)ni(X)P(X)dX (P supported on ∪iNi, c.f. comment after Theorem B.6)
g(ψi ◦ φi(X))ni(X)P(X)dX (error 2)
(g ∙ ni)(ψi(u)) /	P(u,v)dudv
g(z)ni(z)Pi(z)dM(z) (definition ofPi)
/ g(Z)	Eni(Z)Pi(Z)	dM (Z) = /	g(Z)P(Z)dM(Z),
Mi	M
where dM(Z) stands for the Reimannian volume measure on M. Thus it suffices to bound the error
in (error 1) and (error 2) and show that the sum ≤ the right hand side of the claim in the Lemma.
Bound of (error 1):
P(X)g (X)dX -
K
/ P(X)g(X) Eni(X)
JQ	i=1
≤ kgkL∞(Ω)
P(X)
Jω
K
1 — ɪ2 ni (x) dX,
i=1
□
and the Lip constant of the function ξ := (1 — PK=I ni) is upper bounded by PK=I Lip(ni) ≤
KLη,M by Lemma B.7. Also ξ vanishes on M. Applying Lemma B.9 gives that
(error 1) ≤ IlgkL∞(Ω)KLηMc3σ.
19
Under review as a conference paper at ICLR 2020
K
Xi=1ZN
EJ (g(x) - g(ψi ◦ φi(x)))η∣i(x)p(x)dx ≤
Bound of (error 2):
|g(x) - g(ψi ◦ φi(x))∣p(x)dx, (A.6)
using ηi(x) ≤ 1. For each i, consider ξ(x) := g(x) - g(ψi ◦ φi(x)), one can verify that
Lip(ξ) ≤ Lip(g) + Lip(g)βi,
by (A.5), thus each term in the summation of the r.h.s of (A.6) ≤ Lip(g)(1 + βM)c3σ. This proves
that
(error 2) ≤ KLip(g)(1 + βM)c3σ.
Combining the two bounds of (error 1) and (error 2) proves the claim.	口
B.2.3 Proof of Proposition B.5
We are now ready to prove the main result in this section.
Proofof Proposition B.5. Since JSD(p, q) = L[f *], it suffices to control ∣L[fcon] - L[f *]| as stated
in the claim.
We first consider f = f * which has a finite Lip(f *), and we have that
Lf = KPlogτ+e∕+ Ωqlogτ+e∕) := 2(Lp+ Lq).
Define g := log ι2ef, then
Lp
L
Jω
Pg,
and by that log 1+ξξ as a function of ξ ∈ R is Lip 1, we have that
Lip(g) ≤ Lip(f*).
To bound ∣g(χ)∣, note that there is at least one point xo ∈ Ω s.t. f *(χo) = 0, thus g(χo) = 0. Then
∀x ∈ Ω,
|g(x)| = |g(x) - g(x0)| ≤ Lip(g)kx - x0k ≤ 2Lip(g).
Applying Lemma B.10, we have that
Lp = j Pg + r1 = p PlogI 2e f + r1,	m
M	M 1 + ef	(A.7)
|r1| ≤ Lip(g)K(2Lη,M + 1 +βM)c3σ ≤ cMLip(f*)σ,
where
cM := K (2Lη,M + 1 + βM )c3
is an absolute constant only depending on manifold and atlas.
Similarly, we can show that
(A.8)
LL IM qlogUf +
|r2| ≤ cM Lip(f*)σ.
This gives that
Lf*] = 2 UMPlogT+eF + ZM汕gi⅛)+ r1，2，|ri，2| ≤ CMLip(f*)σ∙
(A.9)
We then consider f = fcon, where fcon is constructed by Theorem B.6 to uniformly approximate
f* on M up to ε1 . Following Proposition B.4, suPx∈ω lfcon(x)∣ ≤ Bθ0), andLip(fcon) ≤ B*.
Similar as before, g := log 12f or log ^f both have
Lip(g) ≤ Lip(f) ≤ BΘ(1) .
20
Under review as a conference paper at ICLR 2020
Also observe the relation that |F(ξ)∣ ≤ ∣ξ∣ for ξ ∈ R, where F(ξ) = log 1^ or log 1+^, which
gives that ∣g(χ)∣ ≤ |f (x)|, for all X ∈ Ω. This gives that kgkL∞(Ω) ≤ B(0). Itisgenerallyvalidthat
fcon(x0) = 0 for some X0 ∈ M ⊂ Ω, and then g(χ0) = 0, thus We also have that ∣g(χ)∣ ≤ 2Lip(g).
Thus
kgkL∞(Ω) ≤ min{B(0), 2B^} := Be,.	(A.10)
Putting together, we then have
L[fcon]=2 UM Plog f+ZM qlog τ⅛) +r3，
|r3 | ≤ K(BΘLη,M + BΘ() (1 + βM))c3σ.
Comparing (A.9), (A.11) gives that
|L[f *] - L[fcon] l≤ 5εl [ (P + q) + |r1,2| + 忙3|,
2M
where we used that |f *(x) - fcon(x)∣ ≤ ε1, ∀x ∈ M. Observe that
P ≤ / P + KLη,Mc3σ = 1 + KLη,MC3σ,
JM Jω
by applying Lemma B.10 with g(x) = 1, and same for JM q. This proves that
|L[f *] - L[fcon]1 ≤ (1 + KLη,Mc3σ )ε1 + Lip(f *)K (2Lη,M + 1 + Bm )c3σ
+ K(BΘLη,M + BΘ() (1 + βM))c3σ
< 10ε1 + c(f*, Θ, M)σ,
(A.11)
(A.12)
where we assume that σ is small enough to make KLη,MC3σ < 9, and c(f *, Θ, M) is a positive
constant the formula of which is given in the previous term. At last, the comment after Theorem
B.6) needs that c4(M)σ < 1, and we let c4 be the maximum of 9c4(M) and KL%MC3.	口
21