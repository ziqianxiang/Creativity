Under review as a conference paper at ICLR 2020
TransINT: Embedding Implication Rules in
Knowledge Graphs with Isomorphic Intersec-
tions of Linear Subspaces
Anonymous authors
Paper under double-blind review
Ab stract
Knowledge Graphs (KG), composed of entities and relations, provide a structured
representation of knowledge. For easy access to statistical approaches on relational
data, multiple methods to embed a KG into f (KG) ∈ Rd have been introduced. We
propose TransINT, a novel and interpretable KG embedding method that isomorphi-
cally preserves the implication ordering among relations in the embedding space.
TransINT maps set of entities (tied by a relation) to continuous sets of vectors that
are inclusion-ordered isomorphically to relation implications. With a novel param-
eter sharing scheme, TransINT enables automatic training on missing but implied
facts without rule grounding. We achieve new state-of-the-art performances with
signficant margins in Link Prediction and Triple Classification on FB122 dataset,
with boosted performance even on test instances that cannot be inferred by logical
rules. The angles between the continuous sets embedded by TransINT provide
an interpretable way to mine semantic relatedness and implication rules among
relations.
1 Introduction
Recently, learning distributed vector representations of multi-relational knowledge has become an
active area of research (Bordes et al.; Nickel et al.; Kazemi & Poole; Wang et al.; Bordes et al.). These
methods map components of a KG (entities and relations) to elements of Rdand capture statistical
patterns, regarding vectors close in distance as representing similar concepts. However, they lack
common sense knowledge which are essential for reasoning (Wang et al.; Guo et al.; Nickel & Kiela).
For example, “parent" and “father" would be deemed similar by KG embeddings, but by common
sense, "parent ⇒ father" yet not the other way around. Thus, one focus of current research is to bring
common sense rules to KG embeddings (Guo et al.; Wang et al.; Wei et al.(. Some methods impose
hard geometric constraints and embed asymmetric orderings of knowledge (Nickel & Kiela; Vendrov
et al.; Vilnis et al.(. However, they only embed hierarchy (unary Is_a relations), and cannot embed
n-ary relations in KG’s. Moreover, their hierarchy learning is largely incompatible with conventional
relational learning, because they put hard constraints on distance to represent partial ordering, which
is a common metric of similarity/ relatedness in relational learning.
We propose TransINT, a new KG embedding method that isomorphically preserves the implication
ordering among relations in the embedding space. TransINT restrict entities tied by a relation to be
embedded to vectors in a particular region of Rd included isomorphically to the order of relation
implication. For example, we map any entities tied by is_father_of to vectors in a region that is part
of the region for is_parent_of; thus, we can automatically know that if John is a father of Tom, he is
also his parent even if such a fact is missing in the KG. Such embeddings are constructed by sharing
and rank-ordering the basis of the linear subspaces where the vectors are required to belong.
Mathematically, a relation can be viewed as sets of entities tied by a constraint (Stoll). We take such
a view on KG’s, since it gives consistancy and interpretability to model behavior. Furthermore, for
the first time in KG embedding, we map sets of entitites under relation constraint to a continuous set
of points (whose elements are entity vectors) - which learns relationships among not only individual
entity vectors but also sets of entities. We show that angles between embedded relation sets can
identify semantic patterns and implication rules - an extension of the line of thought as in word/
image embedding methods such as Mikolov et al., Frome et al. to relational embedding. Such mining
is both limited and less interpretable if embedded sets are discrete (Vilnis et al.; Vendrov et al.) or
each entitity itself is embedded to a region, not a member vector of it (Vilnis et al.).1 TransINT’s
1Vilnis et al. can be interpreted in both ways.
1
Under review as a conference paper at ICLR 2020
such interpretable meta-learning opens up possibilities for explainable reasoning in applications such
as recommender systems (Ma et al.) and question answering (Hamilton et al.).
The main contributions of our work are: (1) A novel KG embedding such that implication rules
in the original KG are guaranteed to unconditionally, not approximately, hold. (2) We introduce a
novel parameter sharing regularization and negative example construction methods. (3) Our model
suggests possibilities of learning semantic relatedness between groups of objects. (4) We achieve
new state-of-the-art performances with large margins in Link Prediction and Triple Classification on
FB122 datasets.
(b)
JohnJ)
is_parent_of T ∖
∖ is<father-of
is_mother_of ∖	∖
Sue
is_parent_of
is fatherI/；S-Parenjof
“ Harry
Set for
ls_Parent_Of
Set for
ls_Father_Of
(Harry, Tom)
(Tom, John)
(c)
is_extended_fami ly_of
Set for
ls_Mother_Of
is_family_of
is_grandparent_of
(Sue, Tom)
is_parent_of
is_child_of
is_mother_of is_father_of is_daughter_of is_son_of is_grandfather_of
Figure 1: Two equivalent ways of expressing relations. (a): relations defined in a hypothetical KG. (b): relations
defined in a set-theoretic perspective (Definition 1). Because is_father_of ⇒ is_parent_of, the set for is_father_of
is a subset of that for is_parent_of (Definition 2). (c): Hierarchical depiction of familial relations.
2 TRANSINT
In this section, we describe the intuition and justification of our method. We first define relation as
sets, and revisit TransH as mapping relations to sets in Rd . Finally, we propose TransINT, which
connects the ordering of the two aforementioned sets. We put * next to definitions and theorems We
propose/ introduce. Otherwise, we use existing definitions and cite them.
2.1	Sets as Relations
We define relations as sets and implication as inclusion of sets, as in set-theoretic logic.
Definition (Relation Set): Let ri be a binary relation x, y entities. Then, ri(x, y) iff there exists some
set Ri such that the pair (x, y) ∈ Ri. Ri is called the relation set of ri.(Stoll)
For example, consider the distinct relations in Figure 1a, and their corresponding sets in Figure 1b;
Is_Father_Of(Tom, Harry) is equivalent to (Tom, Harry) ∈ RIs_Father_Of .
Definition (Logical Implication): For two relations, r1 implies r2 (or r1 ⇒ r2) iff ∀x, y,
(x, y) ∈ R1 ⇒ (x, y) ∈ R2 or equivalently, R2 ⊂ R1.(Stoll)
For example, Is_Father_Of ⇒ Is_Parent_Of. (In Figure 1b, RIs_Father_Of ⊂ RIs_Parent_Of).
Tbm * John
f∖	HiS _Parent-Pf
Tbm - Harry
rent_pf
Γ rt_parent_of
Γ rtjarent_of
Harry - John
Γκ-parβnt-of
Figure 2: Two perspectives of viweing TransH in R3; the orange dot is the origin, to emphasize that a vector
is really a point from the origin but can be translated and considered equivalently. (a): first projecting h and
#» onto His_famii»_of, and then requiring h⊥ + j ≈ t⊥ (b): first substracting #» from h, and then projecting
the distance (t - h) to His_f amily_of and requiring (t - h)⊥ ≈ rj. The red line is unique because it is when
r# is _ famiiy _ o (f is translated to the origin.
2.2	Background: TransE and TransH
Given a fact triple (h, r, t) in a given KG (i.e. (Harry, is_father_of, Tom)), TransE wants #h» + #r» ≈ #t»
where h , #r», #t» are embeddings of h, r, t. In other words, the distance between two entity vectors
is equal to a fixed relation vector. TransE applies well to 1-to-1 relations but has issues for N-to-1,
1-to-N and N-to-N relations, because the distance between two vectors are unique and thus two
entities can only be tied with one relation.
To address this, TransH constraints the distance of entities in a multi-relational way, by decomposing
distance with projection (Figure 2a). TransH first projects an entity vector into a hyperplane unique
to each relation, and then requires their difference is some constant value. Like TransE, it embeds
2
Under review as a conference paper at ICLR 2020
an entity to a vector. However, for each relation rj , it assigns two components: a relation-specific
hyperplane Hj and a fixed vector r#»j on Hj . For each fact triple (h, rj , t), TransH wants (Figure 2)
J⊥ + r» ≈ t⊥........ (Eq. 1)
where h⊥,t⊥ are projections on h, 7» onto Hj (Figure 2a).
Revisiting TransH We interpret TransH in a novel perspective. An equivalent way to put Eq.1 is
to change the order of subtraction and projection:
Projection of (t# - h») onto Hj ≈ r#»j .
This means that all entity vectors ( #h», #t») such that their distance t# - h» belongs to the red line are
considered to be tied by relation rj (Figure 2b) i.e. Rj ≈ the red line. For example,
#	»
(Tom, SUe) ∈ Rj = (Sue 一 TOm) ∈ the red line
The red line is the set of all vectors whose projection onto Hj is the fixed vector r#»j . Thus, upon a
deeper look, TransH actually embeds a relation set in KG (figure 1b) to a particular set in Rd .
We call such sets relation space for now; in other words, a relation space of some relation ri is the
space where each (h, ri, t)’s t 一 h can exist. We formally visit it later in Section 3.1.
Thus, in TransH,
ri(x, y) ≡ (x, y) ∈ Ri	(relation in KG)
=(X 一 y) ∈ relation space of r	(relation in Rd)
Figure 3: (a): Vieweing TransINT as intersection of H,s and projection of ?»'s. The dotted orange lines are the'
projection constraint. (b): Viewing TransINT in the relation space (Figure 2b) perspective. The blue line, red
line, and the green plane is respectively is_father_of, is_mother_of and is_parent_of ’s relation space - where
t - h’s of h, t tied by these relations can exist. The blue and the red line lie on the green plane - is_parent_of ’s
relation space includes the other two’s.
2.3 TransINT
Like TransH, TransINT embeds a relation rj to a (subspace, vector) pair (Hj, r#»j). However, TransINT
modifies the relation embeddings (Hj, r#»j) so that the relation spaces (i.e. red line of Figure 2b) are
ordered by implication; we do so by intersecting the Hj ’s and projecting the r#»j ’s (Figure 3a). We
explain with familial relations as a running example.
Intersecting the Hj ’s TransINT assigns distinct hyperplanes His_father_of and His_mother_of to
is_father_of and is_mother_of. However, because is_parent_of is implied by the aforementioned
relations, we assign
His_parent_of = His_f ather_of ∩ His_mother_of .
TrainsINT’s His_parent_of is not a hyperplane but a line (Figure 3a), unlike in TransH where all Hj ’s
are hyperplanes.
Projecting the r#»j ’s TransH constrains the r#»j ’s with projections (Figure 3a’s dotted orange lines).
First, r#is _ father _ o »f and r#is _ mother _ o »f are required to have the same projection onto His_parent_of .
Second, r#is _ parent _ o »f is that same projection onto His_parent_of.
Connection to Relation Spaces We connect the two above constraints to ordering re-
lation spaces. Figure 3b graphically illustrates that is_parent_of ’s relation space (green
hyperplane) includes those of is_father_of (blue line) and is_mother_of (red line).
3
Under review as a conference paper at ICLR 2020
More generally, TransINT requires two hard geometric constraints on (Hj,j)’s that
For distinct relations ri , rj , require the following if and only if ri ⇒ rj :
Intersection Constraint: Hj = Hi ∩ Hj .
Projection Constraint: Projection of r#»1 to Hj is r#»j .
where H# »i , H# »j and r#»i , r#»j are distinct.
We prove that these two constraints guarantee that an ordering isomorphic to implication holds in the
embedding space: (r ⇒ rj) iff (ri's rel. space ⊂ r7-'s rel. space) or equivalently,
(Ri ⊂ Rj) iff (ri’s rel. space ⊂ rj ’s rel. space)
The orderings are isomorphic, because for example, if is_parent_of subsumes is_father_of, the first
relation space also subsumes the latter's (Figure 3). At first sight, it may look paradoxical that
the Hj 's and the relation spaces are inversely ordered; however, it is a natural consequence of the
rank-based geometry in Rd .
3	TransINT’ s Isomorphic Guarantee
In this section, we formally state TransINT's isomorphic guarantee and its grounds. We also discuss
the intuitive meaning of our method. We denote all d × d matrices with capital letters (ex) A) and
vectors with arrows on top (ex) b ).
3.1	Projection and Relation Space
In Rd, points are projected to linear subspaces by projection matrices; each linear subspace Hi has
a projection matrix Pi such that ∀x» ∈ Rd, Px ∈ H (Strang). For example, in Figure 4, a random
point #a» ∈ Rd is projected onto H1 when multiplied by P1; i.e. P1a = b ∈ H1. In the rest of the
paper, denote Pi as the projection matrix onto subspace Hi .
Now, we algebraically define a general concept that subsumes relation space(Figure 3b).
Definition * (Sol(P, k)) : Let H be a linear subspace and P its projection matrix. Then, given k on
H, the set of vectors that become k when projected on to H, or the solution space of P#x» = k , is
denoted as Sol(P, k ).
With this definition, relation space (Figure 3b) is (Sol(Pi, r#»i)), where Pi is the projection matrix of
Hi (subspace for relation ri); it is the set of points t - h such that Pi (t - h) = r#»i.
3.2	Isomorphic Guarantees
Main Theorem 1 (Isomorphism): Let {(Hi, r#»i )}n be the (subspace, vector) embeddings assigned
to relations {Ri }n by the Intersection Constraint and the Projection Constraint; Pi the projection
matrix of Hi. Then, ({Sol(Pi, r#»i)}n, ⊂) is isomorphic to ({Ri}n, ⊂).
#	»	»
In actual implementation and training, TransINT requires something less strict than Pi(t - h) = r#»i:
Pi (t - h) - r#»i ≈ 0 ≡ ||Pi (t - h - r#»i )||2 < ,
#	»	»
for some non-negative and small . This bounds t - h - r#»i to regions with thickness 2, centered
around Sol(Pi, r#»i) (Figure 5). We prove that isomorphism still holds with this weaker requirement.
Definition* (Sol (P, k)) : Given a projection matrix P, the solution space of ||P #x» - #k»||2 < is
denoted as Sol(P, k ).
Main Theorem 2 (Margin-aware Isomorphism): For all non-negative scalar , ({Sol(Pi, r#»i)}n, ⊂)
is isomorphic to ({Ri}n, ⊂).
3.3	Intuitive Meaning of the Isomorphism
At a first glance, it may look paradoxical that a relation whose Hi is the intersection of other relations'
Hj 's (i.e. is_parent_of of Figure 3a) actually subsumes all the relations that were intersected (i.e.
is_fahter_of, is_mohter_of). This inverse ordering of Hj 's and Sol(Pj , r#»j) arise from the fact that the
two are orthocomplements (Strang).
Geometrically, projection is decomposition into independent directions; #x» = P#x» + (I - P) #x»
holds for all #x». In Fig. 4a, one can see that P and I - P are orthogonal. Algebraically, a vector
#x» ∈ Rd bound by P #x» = b, composed of k independent constraints (rank k), #x» is free in all other
d - k directions of I - P (Fig. 4b). Thus, the lesser constraint the space to be projected onto, the
4
Under review as a conference paper at ICLR 2020
Figure 4: Orthogonality of P and I — p. (a) X is be decomposed into two orthogonal directions, when projection
is applied. (b) The constraint Px = b does not impose anything to the orthogonal direction; x can have any
magnitude in (I — P)’s direction.
more freedom a vector is given; which is isomorphic to that, for example, is_family_of puts more
freedom on who can be tied by it than is_f ather_of. (Fig. 1b).
Thus, the intuitive meaning of the above proof is that we can map degree of freedom in the logical
space to that in Rd .
Figure 5: Fig. 3(b)'s relation spaces When Pi(t — h) — r» ≈ 0 ≡ ∣∣Pi(t — h — r»)∣∣2 < e is required. (a):
Each relation space now becomes regions with thickness , centered around figure 3(b)’s relation space. (b):
Relationship of the angle and area of overlap betWeen tWo relation spaces. With respect to the green region, the
nearly perpendicular cylinder overlaps much less With it than the other cylinder With much closer angle.
4	Initialization and Training
The intersection and projection constraints can be imposed With parameter sharing. We describe hoW
shared parameters are initialized and trained.
4.1	Parameter Sharing Initializaion
From initialization, We bind parameters so that they satisfy the tWo constraints. For each entity ej ,
We assign a d-dimensional vector e#»j. To each Ri, We assign (Hi, r#»i) (or (Ai, r#»i)) With parameter
sharing. We first construct the H’s.
Intersection constraint We define the H’s top-doWn, first defining the intersections and then the
subspaces that go through it. To the head Rh, assign ah linearly independent roWs for the basis of
Hh . Then, to each Ri that is not a head, additionally assign ai roWs linearly independent to the bases
of all of its parents, and construct Hi With its bases and the bases of all of its parents. Projection
matrices can be uniquely constructed given the bases (Strang).
NoW, We initlialize the r#»i ’s.
Projection Constraint To the head Rh, pick any random xh ∈ Rd and assign r#»h = Phx. To each
non-head Ri Whose parent is Rp, assign r#»i = r#»p + (I - Pp)(Pi)xi for some random xi. This results
in
Pp r#»i = Pp r#»p + Pp(I - Pp)(Pi)x#»i = r#»p + #0» = r#»p
for any parent, child pair.
Parameters to be trained Such initialization leaves the folloWing parameters given a KG With
entities ej’s and relations ri’s: (1) Ah for the head relation, (2) ci for each non-head relation, (3) x#»i
for each head and non-head relation, (4) e#»j for each entity ej .
4.1.1	Training
We construct negative examples (Wrong fact triplets) and train With a margin-based loss, folloWing
the same protocols as in TransE and TransH.
Training Objective We adopt the same loss function as in TransH. For each fact triplet (h, ri , t),
We define the score function	f(h,ri,t) = ||Pi(t# - h») -r#»i||2
and train a margin-based loss L Which is aggregates f’s and discriminates betWeen correct and
negative examples. L= X max(0,f(h,ri,t)2+γ-f(h0,ri0,t0)2)
(h,ri,t)∈G
5
Under review as a conference paper at ICLR 2020
where G is the set of all triples in the KG and (h0 , ri0 , t0) is a negative triple made from corrupting
(h, ri , t). We minimize this objective with stochastic gradient descent.
Automatic Grounding of Positive Triples The parameter sharing scheme guarantees two advan-
tages during all steps of training. First, the intersection and projection constraint are met not only at
initialization but always.
Second, traversing through a particular (h, ri , t) also automatically executes training with (h, rp , t)
for any ri ⇒ rp . For example, by traversing (Tom, is_father_of, Harry) in the KG, the model
automatically also traverses (Tom, is_parent_of, Harry), (Tom, is_family_of, Harry), even if the two
triples are missing in the KG. This is because PpPi = Pp with the given initialization (section 4.1.1)
2
and thus,	f(h,rp,t) = ||Pp(t# - h») - r#»p||2
2
≤ ||(Pp + (I - Pp))Pi((t# - h»)-r#»i))||22
||Pp(Pi((t# - h»)-r#»i))||22
||(Pi((t# - h»)-r#»i))||22=f(h,ri,t)
In other words, training f(h, ri, t) towards less than automatically guarantees, or has the effect of
training f(h, rp, t) towards less than . This enables the model to be automatically trained with what
exists in the KG, eliminating the need to manually create missing triples that are true by implication
rule.
5	Experiments
We evaluate TransINT on Freebase 122 (respectively created by Vendrov et al. and Guo et al.) against
the current state-of-the-art method.
5.1	Link Prediction
The task is to predict the gold entity given a fact triple with missing head or tail - if (h, r, t ) is a fact
triple in the test set, predict h given (r, t) or predict t given (h, r). We follow TransE and KALE’s
protocol. For each test triple (h, r, t ), we rank the similarity score (f (e, r, t) when h is replaced with
e for every entity e in the KG, and identify the rank of the gold head entity h; we do the same for the
tail entity t. Aggregated over all test triples, we report: (i) the mean reciprocal rank (MRR), (ii) the
median of the ranks (MED), and (iii) the proportion of ranks no larger than n (HITS@N), which are
the same metrics reported by KALE. A lower MED and higher MRR and Hits HITS@N are better.
TransH and KALE adopt a "filtered" setting that addresses when entities that are correct, albeit not
gold, are ranked before the gold entity. For example, if the gold entity is (Tom, is_parent_of, John)
and we rank every entity e for being the head of (?, is_parent_of, John), it is possible that Sue, John’s
mother, gets ranked before Tom. To avoid this, the "filtered setting" ignore corrupted triplets that
exist in the KG when counting the rank of the gold entity. (The setting without this is called the "raw
setting").
We compare our performance with that of KALE and previous methods (TransE, TransH, TransR)
that were compared against it, using the same dataset (FB122). FB122 is a subset of FB15K (Bordes
et al.) accompanied by 47 implication and transitive rules; it consists of 122 Freebase relations on
“people”, “location”, and “sports” topics. Since we use the same train/ test/ validation sets, we directly
copy from Guo et al. for reporting on these baselines.
5.1.1	Details of Training
TransINT’s hyperparameters are: learning rate (η), margin (γ), embedding dimension (d), and
learning rate decay (α), applied every 10 epochs to the learning rate. We find optimal configurations
among the following candidates: η ∈ {0.003, 0.005, 0.01}, γ ∈ {1, 2, 5, 10}, d ∈ {50, 100}, α, ∈
{1.0, 0.98, 0.95}. We create 100 mini-batches of the training set and train for maximum of 1000
epochs with early stopping based on the best median rank. Furthermore, we try training with and
without normalizing each of entity vectors, relation vectors, and relation subspace bases after every
batch of training.
5.1.2	Experiment Settings
Out of the 47 rules in FB122, 9 are transitive rules (such as person/nationality(x,y) ∧
country/official_language(y,z) ⇒ person/languages(x,z)) to be used for KALE.
However, since TransINT only deals with implication rules, we do not take advantage of them,
unlike KALE.
We also put us on some intentional disadvantages against KALE to assess TransINT’s robustness
to absence of negative example grounding. In constructing negative examples for the margin-based
loss L, KALE both uses rules (by grounding) and their own scoring scheme to avoid false negatives.
While grounding with FB122 is not a burdensome task, it known to be very inefficient and difficult
for extremely large datasets (Ding et al.). Thus, it is a great advantage for a KG model to perform
well without grounding of training/ test data. We evaluate TransINT on two settings for avoiding
false negative examples; using rule grounding and only avoiding ones that exist in the KG. We call
them respectively TransINTG (grounding), TransINTNG (no grounding).
6
Under review as a conference paper at ICLR 2020
Table 1: Results on Link Prediction on FB122. *: For KALE, We report the best performance by any of
KALE-PRE, KALE-Joint, KALE-TRIP (three variants of KALE proposed by Guo et al.).
		Raw						Filtered				
	MRR	MED	Hits N%			MRR	MED	Hits N%		
			3	5	10			3	5	10
TransE	0.262	10.0	^336^	^425^	=50O=	0.480	2.0	^389^	^64T"	^70T^
TransH	0.249	12.0	31.9	40.7	48.6	0.460	3.0	53.7	59.1	66.0
TransR	0.261	15.0	28.9	37.4	45.9	0.523	2.0	59.9	65.2	71.8
KALE*	0.294	9.0	36.9	44.8	51.9	0.523	2.0	61.7	66.4	72.8
TransINTG	0.339	6.0	40.1	49.1	54.6	0.655	1.0	70.4	75.1	78.7
TranSINTNG	0.323	8.0	38.3	46.6	53.8	0.620	1.0	70.1	74.1	78.3
Table 2: Results on Triple Classification on FB122, in Mean Average Precision (MAP).
TransE	TransH	TransR	KALE*	TransINT1	TransINT2
0.634	0.641	0.619	0.677	0.781 (0.839/0.752)	0.743(0.709/0.761)
5.1.3 Results
We report link prediction results in Table 1. While the filtered setting gives better performance (as
expected), the trend is generally similar betWeen raw and filtered. TransINT outperforms all other
models by large margins in all metrics, even Without grounding; especially in the filtered setting, the
Hits@N gap between TransINTG and KALE is around 4〜6 times that between KALE and the best
performing Trans Baseline (TransR).
Also, while TransINTG performs higher than TransINTN G in all settings/metrics, the gap between
them is much smaller than the that between TransINTN G and KALE, showing that TransINT robustly
brings state-of-the-art performance even without grounding. The results suggest two possibilities in a
more general sense. First, the emphasis of true positives could be as important as/ more important than
avoiding false negatives. Even without manual grounding, TransINTN G has automatic grounding of
positive training instances enabled (Section 4.1.1.) due to model properties, and this could be one
of its success factors. Second, hard constraint on parameter structures can bring performance boost
uncomparable to that by regularization or joint learning, which are softer constraints. We also note
that norm regularization of any parameter did not help in training TransINT, unlike stated in TransE,
TransH, and KALE. Instead, it was important to use a large margin (either γ = 5 or γ = 10).
5.2	Triple Classification
The task is to classify whether an unobserved instance (h, r, t) is correct or not, where the test set
consists of positive and negative instances. We use the same protocol and test set provided by KALE;
for each test instance, we evaluate its similarity score f(h, r, t) and classify it as "correct" if f(h, r, t)
is below a certain threshold (σ), a hyperparameter to be additionally tuned for this task. We report on
mean average precision (MAP), the mean of classification precision over all distinct relations (r’s) of
the test instances. We use the same experiment settings/ training details as in Link Prediction other
than additionally finding optimal σ.
5.2.1	Results
Triple Classification results are shown in Table 2. Again, TransINTG and TransINTN G both signifi-
cantly outperform all other baselines. We also separately analyze MAP for relations that are/ are not
affected by the implication rules (those that appear/ do not appear in the rules), shown in parentheses
of Table 2 with the order of (influenced relations/ uninfluenced relations). We can see that both
TransINT’s have MAP higher than the overall MAP of KALE, even when the TransINT’s have the
penalty of being evaluated only on uninfluenced relations; this shows that TransINT generates better
embeddings even for those not affected by rules. Furthermore, we comment on the role of negative
example grounding; we can see that grounding does not help performance on unaffected relations
(i.e. 0.752 vs 0.761), but greatly boosts performance on those affected by rules (0.839 vs 0.709).
While TransINT does not necessitate negative example grounding, it does improve the quality of
embeddings for those affected by rules.
6	Semantics Mining with Overlap B etween Embedded Regions
Traditional embedding methods that map an object (i.e. words, images) to a singleton vector learn
soft tendencies between embedded vectors, such as semantic similarity (Mikolov et al., Frome et al.).
Table 3: Examples of relations, angles and imb with respect to /people/person/place_of_birth
		RelatiOn	Anlge	imb
Not Disjoint	RelatedneSS	/people/person/nationality	22.7	1ΓT8^
	ImPliCatiOn	/people/person/place_lived/location*	46.7	3VΓΓ
Disjoint		/people/cause_of_death/people	76.6	n/a
		/sports/sports_team/colors	83.5	n/a
7
Under review as a conference paper at ICLR 2020
A common metric for such tendency is cosine similarity, or angle between two embddings. TransINT
extends such line of thought to semantic relatedness between groups of objects, with angles between
relation spaces. In Fig. 5b, one can observe that the closer the angle between two embedded regions,
the larger the overlap in area. For entities h and t to be tied by both relations r1, r2, t - h has to
belong to the intersection of their relation spaces. Thus, we hypothesize the following over any two
relations r1 , r2 that are n»ot explicitly tied by the pre-determined rules:
Let V1 be the set of t - h’s in r1’s relation space (denoted as Rel1) and V2 that of r2’s. Then,
(1)	Angle between Rel1 and Rel2 represents semantic "disjointness" of r1, r2; the more disjoint two
relations, the closer their angle to 90°.
When the angle between Rel1 and Rel2 is small,
(2)	if majority of V1 belongs to the overlap of V1 and V2 but not vice versa, r1 implies r2 .
(3)	if majority of V1 and V2 both belong to their overlap, r1 and r2 are semantically related.
Hypotheses (2) and (3) consider the imbalance of membership in overlapped regions. Exact
calculation of this involves specifying an appropriate (Fig. 3). As a proxy for deciding whether an
element of V1 (denote v1 ) belongs in the overlapped region, we can consider the distance between
v1 to and its projection to Rel2 ; the further away v1 is from the overlapped region, the larger the
projected distance (visualization available in our code repository). We call the mean of such distances
from V1 to Rel2 d12 and the reverse d21. We quantify the imbalance in d12, d21 with 1 (照 + 念),
which is minimized to 1 when d21 = d12 and increases as d12, d21 are more imbalanced. We call this
factor imb.
For hypothesis (1), we verified that the vast majority of relation pairs have angles near to 90°,
with the mean and median respectively 83.0° and 85.4°; only 1% of all relation pairs had angles
less than 50° . We observed that relation pairs with angle less than 20° were those that can be
inferred by transitively applying the pre-determined implication rules. Relation pairs with angles
within the range of [20°, 60°] had strong tendencies of semantic relatedness or implication; such
tendency drastically weakened past 70°. Table 3 shows the angle and imb of relations with respect
to /people/person/place_of_birth, whose trend agrees with our hypotheses. While we only
show a subset of the complete list, we note that almost all relation pairs generally follow such a
tendency; the complete list can be accessed in our code repository. Finally, we note that such analysis
could be possible with TransH as well, since their method too maps t - h’s to lines (Fig. 2b).
Throughout target tasks (Link Prediction, Triple Classification) and semantics mining, TransINT’s
theme of optimal regions to bound entity sets is unified and consistent. Furthermore, the integration of
rules into embedding space geometrically coherent with KG embeddings alone. These two qualities
were missing in existing works such as TransE or KALE, and TransINT opens up new possibilities
for applying KG embeddings to explainable reasoning in applications such as recommender systems
(Ma et al.) and question answering (Hamilton et al.).
7	Related Work
Our work is related to three strands of work. The first strand is Order Embeddings (Vendrov et al.)
and their extensions (Vilnis et al.; Athiwaratkun & Wilson), whose limitation we discussed in the
introduction. While Nickel & Kiela also approximately embed unary partial ordering, their focus
is on achieving reasonably competent result with unsupervised learning of rules in low dimensions,
while ours is achieving state-of-the-art in a supervised setting. The second strand is those that enforce
the satisfaction of common sense logical rules in the embedded KG. Wang et al. explicitly constraints
the resulting embedding to satisfy logical implications and type constraints via linear programming,
but it only requires to do so during inference, not learning. On the other hand,Guo et al. induces
that embeddings follow a set of logical rules during learning, but their approach is soft induction not
hardly constrain. Our work combines the advantages of both works.
8	Conclusion
We presented TransINT, a new KG embedding method that embed sets of entities (tied by relations)
to continuous sets in Rd that are inclusion-ordered isomorphically to relation implications. Our
method achieved new state-of-the-art performances with signficant margins in Link Prediction and
Triple Classification on the FB122 dataset, with boosted performance even on test instances that are
not affected by rules. We further propose and interpretable criterion for mining semantic similairty
among sets of entities with TransINT.
8
Under review as a conference paper at ICLR 2020
References
Ben Athiwaratkun and Andrew Gordon Wilson. Hierarchical density order embeddings, 2018.
Antoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. Learning structured embed-
dings of knowledge bases. In AAAI, 2011.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Durdn, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Proceedings of the 26th International
Conference on Neural Information Processing Systems - Volume 2, NIPS'13,pp. 2787-2795, USA,
2013. Curran Associates Inc. URL http://dl.acm.org/citation.cfm?id=2999792.
2999923.
Boyang Ding, Quan Wang, Bin Wang, and Li Guo. Improving knowledge graph embedding using
simple constraints. In ACL, 2018.
Andrea Frome, Gregory S. Corrado, Jonathon Shlens, Samy Bengio, Jeffrey Dean, Marc’Aurelio
Ranzato, and Tomas Mikolov. Devise: A deep visual-semantic embedding model. In NIPS, 2013.
Shu Guo, Quan Wang, Lihong Wang, Bin Wang, and Li Guo. Jointly embedding knowledge graphs
and logical rules. In EMNLP, 2016.
William L. Hamilton, Payal Bajaj, Marinka Zitnik, Daniel Jurafsky, and Jure Leskovec. Embedding
logical queries on knowledge graphs. In NeurIPS, 2018.
Seyed Mehran Kazemi and David Poole. Simple embedding for link prediction in knowl-
edge graphs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 4284-
4295. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
7682-simple-embedding-for-link-prediction-in-knowledge-graphs.
pdf.
Weizhi Ma, Min Zhang, Yue Cao, Woojeong Jin, Chenyang Wang, Yiqun Liu, Shaoping Ma, and
Xiang Ren. Jointly learning explainable rules for recommendation with knowledge graph. In
WWW, 2019.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed
representations of words and phrases and their compositionality. In NIPS, 2013.
Maximilian Nickel and Douwe Kiela. POinCare embeddings for learning hierarchical representations.
In NIPS, 2017.
Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning
on multi-relational data. In ICML, 2011.
Robert Roth Stoll. Set theory and logic. Courier Corporation, 1979.
Gilbert Strang. Linear algebra and its applications. Thomson, Brooks/Cole, Belmont, CA, 2006.
ISBN 0030105676 9780030105678 0534422004 9780534422004. URL http://www.amazon.
com/Linear- Algebra- Its- Applications- Edition/dp/0030105676.
Ivan Vendrov, Jamie Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images
and language. CoRR, abs/1511.06361, 2015.
Luke Vilnis, Xiang Li, Shikhar Murty, and Andrew McCallum. Probabilistic embedding of knowledge
graphs with box lattice measures. arXiv preprint arXiv:1805.06627, 2018.
Quan Wang, Bin Wang, and Li Guo. Knowledge base completion using embeddings and rules. In
IJCAI, 2015.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by
translating on hyperplanes. In AAAI, 2014.
Zhuoyu Wei, Jun Zhao, Kang Liu, Zhenyu Qi, Zhengya Sun, and Guanhua Tian. Large-scale
knowledge base completion: Inferring via grounding network sampling over selected instances. In
CIKM, 2015.
9
Under review as a conference paper at ICLR 2020
Figure 6: Projection matrices of subspaces that include each other.
A	Appendix
Proof For TransINT’s Isomorphic Guarantee
Here, we provide the proofs for Main Theorems 1 and 2. We also explain some concepts necessary in
explaining the proofs. We put * next to definitions and theorems We propose/ introduce. Otherwise,
we use existing definitions and cite them.
A.1 Linear Subspace and Projection
We explain in detail elements of Rd that were intuitively discussed. In this and later sections, we mark
all lemmas and definitions that we newly introduce with * ; those not marked with * are accompanied
by reference for proof. We denote all d × d matrices with capital letters (ex) A) and vectors with
arrows on top (ex) b ).
A.1.1 Linear Subspace and Rank
The linear subspace given by A(x - #b») = 0 (A is d × d matrix and b ∈ Rd) is the set of x ∈ Rd
that are solutions to the equation; its rank is the number of constraints A(x - b ) = 0 imposes. For
example, in R3, a hyperplane is a set of #x» = [ x1, x2, x3] ∈ R3 such that ax1 + bx2 + cx3 - d = 0
for some scalars a, b, c, d; because vectors are bound by one equation (or its "A" only really contains
one effective equation), a hyperplane’s rank is 1 (equivalently rank(A) = 1). On the other hand, a
line in R3 imposes to 2 constraints, and its rank is 2 (equivalently rank(A) = 2).
Consider two linear subspaces H1, H2, each given by A1(#x» - b#»1) = 0, A2(#x» - b#»2) = 0. Then,
#»	#»
(Hi ⊂ H) ⇔ (Ai (X - b#i )=0 ⇒ A2 (X - b#2) = 0)
by definition. In the rest of the paper, denote Hi as the linear subspace given by some Ai (#x»- b#»i ) = 0.
A.1.2 Properties of Projection
Invariance For all #X» on H, projecting #X» onto H is still #X»; the converse is also true.
Lemma 1 P #X» = #X» ⇔ #X» ∈ H (Strang).
Orthogonality Projection decomposes any vector #X» to two orthogonal components - P #X» and
(I - P) #X» (Figure 4). Thus, for any projection matrix P, I - P is also a projection matrix that is
orthogonal to P (i.e. P (I - P) = 0) (Strang).
Lemma 2 Let P be a projection matrix. Then I-P is also a projection matrix such that P(I-P) = 0
(Strang).
The following lemma also follows.
Lemma 3 ||P #X»|| ≤ ||P #X» + (I - P) #X»|| = || #X»|| (Strang).
10
Under review as a conference paper at ICLR 2020
Projection onto an included space If one subspace H1 includes H2, the order of projecting a
point onto them does not matter. For example, in Figure 3, a random point #a» in R3 can be first
projected onto H1 at b , and then onto H3 at d . On the other hand, it can be first projected onto H3
at d, and then onto H1 at still d. Thus, the order of applying projections onto spaces that includes
one another does not matter.
If we generalize, we obtain the following two lemmas (Figure 6):
Lemma 4* Every two subspaces Hi ⊂ H ifand only if P1P2 = P2P1 = Pi.
proof) By Lemma 1, if Hi ⊂ H, then P2 X = X ∀X ∈ Hi. On the other hand, if Hi ⊂ H, then
there is some #x» ∈ Hi, #x» 6∈ H2 such that P2 #x» 6= #x». Thus,
Hi ⊂ H2 ⇔∀#» ∈ Hi,	P2#» = #»
⇔∀歹，P2(Pi») = Piy ⇔ P2Pi = Pi.
Because projection matrices are symmetric (Strang),
P2Pi =Pi =PiT =PiTP2T =PiP2.
Lemma 5* For two subspaces Hi , H2 and vector #k» ∈ H2,
Hi ⊂ H2 ⇔ Sol(P2, #k») ⊂ Sol(Pi, Pi #k»).
proof) Sol(P2, k) ⊂ Sol(Pi, Pi k) is equivlaent to ∀X ∈ Rd, P2X = k ⇒ PiX = Pi k.
By Lemma 4, if Hi ⊂	H2 ⇔ PiP2 = P1. Since #k» ∈	P2, P2 #X»	= #k» ⇔	P2(X -	#k»)	= #0»	⇔
Pi (P2 #X» - k ) = 0 ⇔	PiP2 #X» = Pi k ⇔ Pi #X» = Pi k .
Partial ordering If two subspaces strictly include one another, projection is uniquely defined from
lower rank subspace to higher rank subspace, but not the other way around. For example, in Figure 3,
a point #a» in R3 (rank 0) is always projected onto Hi (rank 1) at point b . Similarly, point b on Hi
(rank 1) is always projected onto similarly, onto H»3 (order 2) at point d. However, "inv»erse projection"
from H3 t»o Hi is not defined, because not on»ly b but other points on Hi (»such as b0) pro»ject to H3
at point d; these points belong to Sol(P3, d ). In other words, Sol(Pi, b ) ⊂ Sol(P3, d ). This is
the key intuition for isomorphism , which we prove in the next chapter.
A.2 Proof for Isomorphism
Now, we prove that TransINT’s two constraints (section 2.3) guarantee isomorphic ordering in the
embedding space.
Two posets are isomorphic if their sizes are the same and there exists an order-preseving mapping
between them. Thus, any two posets ({Ai}n, ⊂), ({Bi}n, ⊂) are isomorphic if |{Ai}n| = |{Bi}n|
and
∀i, j Ai ⊂ Aj ⇔ Bi ⊂ Bj
Main Theorem 1 (Isomorphism): Let {(Hi, r#»i )}n be the (subspace, vector) embeddings assigned
to relations {Ri}n by the Intersection Constraint and the Projection Constraint; Pi the projection
matrix ofHi. Then, ({Sol(Pi, r#»i)}n, ⊂) is isomorphic to ({Ri}n, ⊂).
proof) Since each Sol(Pi, r#»i ) is distinct and each Ri is assigned exactly one Sol(Pi, r#»i ),
∣{Sol(Pi,n )}n| = ∣{Ii}n∣ .①
Now, let’s show
∀i, j,	Ri ⊂ Rj ⇔ Sol(Pi, r#»i) ⊂ S ol(Pj, r#»j ).
Because the ∀i, j, intersection and projection constraints are true iff	Ri ⊂ Rj, enough to show
that the two constraints hold iffSol(Pi, r#»i ) ⊂ Sol(Pj , r#»j.
First, let’s show Ri ⊂ Ri ⇒ Sol(Pi, r#»i) ⊂ Sol(Pj , r#»j ). From the Intersection Constraint, Ri ⊂
Ri ⇒ Hj ⊂ Hi. By Lemma 5, Sol(Pi, r#»i ) ⊂ S ol(Pj , Pj r#»i ). From the Projection Constraint,
r=Pj r. Thus, Sol(P%, r)⊂ Sol(Pj,Pj r)= Sol(Pj,r)...............②
11
Under review as a conference paper at ICLR 2020
Now, let’s show the converse; enough to show that if S ol(Pi, r#»i) ⊂ S ol(Pj, r#»j), then the intersection
and projection constraints hold true.
Sol(Pi, r#»i) ⊂ Sol(Pj,r#»j)
⇔ ∀#», Pi #» = r» ⇒ Pj #» = r»)
If Pi#x» = r#»i,
∀x», Pj Pi ~x = Pj ri
∀ χ	Pj x = r
both have to be true. For any #x» ∈ Hi, or equivalently, if #x» = Pi #y» for some #y», then the second
equation becomes ∀#», PjPi #» = r», which can be only compatible with the first equation if
r#»j = Pj r#»i, since any vector’s projection onto a subspace is unique. (Projection Constraint)
Now that we know ri = Pj r», by Lemma 5, Hi ⊂ Hj (intersection constraint). ∙ ∙ ∙③ From ①,②,
③,the two posets are isomorphic..
In actual implementation and training, TransINT requires something less strict than Pi(t# - h») = r#»i:
Pi (t - h) - r#»i ≈ 0 ≡ ||Pi (t - h - r#»i )||2 < ,
#	»	»
for some non-negative and small . This bounds t - h - r#»i to regions with thickness 2, centered
around Sol(Pi, r#»i) (Figure 5). We prove that isomorphism still holds with this weaker requirement.
Definition * (Sole(P, k)) : Given a projection matrix P, the solution space of ||P x» — k ∣∣2 < e is
denoted as Sol(P, k ).
Main Theorem 2 (Margin-aware Isomorphism): For all non-negative scalar , ({Sole(Pi, r#»i)}n, ⊂)
is isomorphic to ({Ri}n, ⊂).
proof) Enough to show that ({Sole(Pi, r#»i)}n, ⊂) and ({Sol(Pi, r#»i)}n, ⊂) are isomorphic for all .
First, let’s show
Sol(Pi,正)⊂ Sol(Pj ,r») ⇒ Sole(Pi 不)⊂ Sole(Pj ,r»).
By Main Theorem 1 and Lemma 4,
Sol(Pi, r#»i) ⊂ Sol(Pj, r#»j) ⇔ r#»j = Pj r#»i, Pj = PjPi.
Thus, for all vector #b»,
Pi(x—r#»i) = #b»
⇔ Pj Pi( #» — r) = Pj b
⇔ Pj (#» — r») = Pj b (「Lemma 4)
⇔ Pj(#» — ri) = Pj 了(「Pjr = j = Pjr)
Thus, if||Pi(#x»—r#»i)|| < , then ||Pj(#x»—r#»j)|| = ||Pj(Pi(#x»—r#»i))|| < ||Pj(Pi(#x»—r#»i))+(I—
P)(Pi(#» - r))|| = ||Pi(#» — r)|| <e.…1
Now, let’s show the converse. Assume ||Pi(#xi — r#ii)|| < for some i. Then,
Thus, for ||Pi (#xi — r#ii)|| < to bound ||Pj (#xi — r#ij)|| at all for all #xi,
Pj(I — Pi) = 0, Pj (r#ii — r#ij) = 0
need to hold. By Lemma 4 and 5,
Pj = Pj Pi ⇔ Hj ⊂ Hi
⇔ Sol(Pi,r) ⊂ Sol(Pj,Pjr») = Soi(Pj,r») 一2
|{S ole (Pi, r#ii)}n | = |{Sol(Pi, r#ii)}n | holds obviously; each S ol(Pi, r#ii) has a distinct Sole(Pi, r#ii)
and each Sole(Pi, r) also has a distinct"Center” (Sol(Pi, r)) ∙ ∙③
From D，2), ③,the two sets are isomorphic..
12