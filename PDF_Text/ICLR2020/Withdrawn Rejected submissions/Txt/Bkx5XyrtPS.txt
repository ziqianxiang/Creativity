Under review as a conference paper at ICLR 2020
Depth creates no more spurious local minima
IN LINEAR NETWORKS
Anonymous authors
Paper under double-blind review
Ab stract
We show that for any convex differentiable loss, a deep linear network has no
spurious local minima as long as it is true for the two layer case. This reduction
greatly simplifies the study on the existence of spurious local minima in deep lin-
ear networks. When applied to the quadratic loss, our result immediately implies
the powerful result by Kawaguchi (2016). Further, with the recent work by Zhou
& Liang (2018), we can remove all the assumptions in (Kawaguchi, 2016). This
property holds for more general “multi-tower” linear networks too. Our proof
builds on the work in (Laurent & von Brecht, 2018) and develops a new pertur-
bation argument to show that any spurious local minimum must have full rank, a
structural property which can be useful more generally.
1	Introduction
One major mystery in deep learning is that deep neural networks do not seem to suffer from spurious
local minima. Understanding this mystery has become one of the most important topics in the
machine learning theory. This problem turns to be quite challenging. Even for the subcase of the
deep linear network, much still remains unknown despite a long line of studies on this subject.
In this paper, we prove a useful structural property about the deep linear networks. We show that
for any convex differentiable loss function, any spurious local minima in a deep linear network
should already be present in a two layer linear network. Hence, depth does not create more spurious
local minima in linear networks. This reduction greatly simplifies the study about the existence of
spurious local minima in deep linear networks. When applied to the quadratic loss, it leads to the
first unconditional proof that there is no local minima in deep linear networks.
Baldi & Hornik (1989) started the investigation on the existence of spurious local minima in linear
networks. They showed that, under mild assumptions, for quadratic loss, two layer linear networks
do not have spurious local minima. They also conjectured it is true for deep linear networks. This
conjecture is only proved recently by Kawaguchi (2016). For the special case of linear residual
networks, Hardt & Ma (2017) showed that there are no spurious local minima through a simpler
argument.
While most existing work have been on quadratic loss functions, Laurent & von Brecht (2018)
showed a surprisingly general result for any convex differentiable loss. In (Laurent & von Brecht,
2018), the authors consider the special linear networks which have no bottlenecks, i.e. when the
narrowest layer is on the either end. They showed that for any convex differential loss function, there
is no spurious local minima in such networks. In addition to its generality, the proof in (Laurent &
von Brecht, 2018) is quite intuitive through a novel perturbation argument. However, their special
cases excludes networks with bottleneck layers, commonly used in the practice and studied in the
literature (Baldi & Hornik, 1989; Kawaguchi, 2016).
We build on the work in (Laurent & von Brecht, 2018) and further develop the technique to show
that for general deep linear networks, whether there are spurious local minima is reduced to the two
layer case.
Theorem 1.	Given any convex differentiable function f : Rm×n → R. For any k ≥ 2, let
Lk (Mι,...,Mk) = f (Mk …Mi) where Mi ∈ Rdi ×di-1 for 1 ≤ i ≤ k with dk = m and
d0 = n. Let d = min0≤i≤k di. Define L2(A, B) = f(AB) for A ∈ Rm×d, B ∈ Rd×n. Then Lk
has no spurious local minima iff L2 has no spurious local minima.
1
Under review as a conference paper at ICLR 2020
We emphasize that in the above theorem, f depends on both the data and the loss function. Hence
the reduction is instance specific and does not depend on the global property of a family of loss
functions.
To prove Theorem 1, we show a non-degeneracy property of local minima, namely, any spurious
local minimum must have rank same as the width of the bottleneck. This property can be useful
for the two layer case too - it implies that each layer in a spurious local minimum cannot be rank-
deficient, hence covering subcases which may otherwise require onerous analysis (Baldi & Hornik,
1989; Zhou & Liang, 2018).
Theorem 2.	With the same notation as in Theorem 1, if A, B is a spurious local minimum of L2,
then both A, B must have full rank.
As an application of Theorem 1, for quadratic loss, Theorem 1, together with (Baldi & Hornik,
1989), immediately implies the main result in (Kawaguchi, 2016). We can further remove all the
assumptions needed in (Baldi & Hornik, 1989; Kawaguchi, 2016), using the recent result by Zhou
& Liang (2018) (Theorem 2(1)), hence providing the first unconditional proof of the non-existence
of spurious local minima in deep linear networks for quadratic loss functions. Below ∣∣ ∙ ∣∣ denotes
the Frobenius norm.
Corollary 1. Forany X ∈ Rd0×n,Y ∈ Rdk×n, let L(Mι,...,Mk) = IlMk …MiX — Y ∣∣2. Then
L has no spurious local minima.
Theorem 1 can be further generalized to “multi-tower” linear networks. Define a multi-tower lin-
ear network as the sum of multiple deep linear networks (towers), i.e. Mi^ •…Mii + ... +
Msks …Msi), where Mij ∈ Rdi,j×di,j-1 with 蠹,弧=m and ʤo = n. For 1 ≤ i ≤ s, let
bi = minj di,j denote the bottleneck size of each tower i. Write b = Pi bi.
Corollary 2. For any differentiable convex loss f, a multi-tower linear network has no spurious
local minima iff the linear network AB, where A ∈ Rm×b, B ∈ Rb×n, has no spurious local
minima. Moreover, if b ≥ m, n, there is no spurious local minima in this network.
There are two main technical ideas in our proof. First, we show a reduction of any local minimum of
the multi-layer network to a critical point of a two-layer network. Secondly, we generalize the local
perturbation argument in (Laurent & von Brecht, 2018) to weaker critical conditions. Besides the
conceptual importance of our results, these technical contributions might be useful for understanding
deep networks too.
1.1	Related work
In Baldi & Hornik (1989), itis shown that, under mild assumptions on data, two layer linear network
with quadratic loss has no spurious local minima. This is probably the first positive result on this
long line of investigation. Kawaguchi (2016) showed that it holds for deep linear network too. The
tour de force proof in (Kawaguchi, 2016) works by examining the Hessian using powerful tools
from the matrix theory. There have been much subsequent work to simplify and generalize the
result. For example, Lu & Kawaguchi (2017) came up with a different argument. Yun et al. (2018;
2019) showed simpler arguments for special cases and considered more general non-linear networks.
Hardt & Ma (2017) showed that under certain assumptions, there might not even be stationary point
in the deep linear residual network. Venturi et al. (2018) defined a notion of spurious valleys and
showed that for quadratic losses, there is no spurious valley in deep linear networks. Venturi et al.
(2018) was able to remove all the assumption in (Baldi & Hornik, 1989) under this weaker notion.
The mild assumption in Baldi & Hornik (1989), which was also needed in Kawaguchi’s proof, was
removed by Zhou & Liang (2018), which leads to our Corollary 1.
Laurent & von Brecht (2018) considers the special case of linear networks with the narrowest layer
on the either end. It uses a novel perturbation argument to show that for any convex differentiable
loss, there is no spurious local minimum in such network. However, the special case considered
in (Laurent & von Brecht, 2018) excludes networks through low rank approximation such as auto-
encoders. But it is really the intuitive yet powerful result in (Laurent & von Brecht, 2018) that
motivated this work.
There have been recent studies on the gradient descent convergence on the deep linear net-
works (Arora et al., 2018; Bartlett et al., 2018; Arora et al., 2019). It has been shown by Arora
2
Under review as a conference paper at ICLR 2020
et al. (2018) that, under certain conditions, increasing the depth of linear networks can speed up the
convergence, which is another positive property of deep linear networks.
There have been much work (Soltanolkotabi et al., 2018; Li & Liang, 2018; Allen-Zhu et al.,
2018a;b; Du et al., 2018a; Zou et al., 2018; Du et al., 2018b) recently on studying the optimiza-
tion landscape and convergence of non-linear networks. They focus mostly on shallow networks
with over-parameterized wide layers.
2	Preliminaries
We define notations used through the paper. We state some simple facts and the main theorem
from Laurent & von Brecht (2018) which we need in our proof.
Deep linear networks. Denote by Rm×n all the matrices with m rows and n columns. For 1 ≤
i ≤ k, let Mi ∈ Rdi×di-1 . For i ≥ j, denote by Mi …Mj the matrix product of Mi ∙ Mi-ι... ∙ Mj.
A (deep) linear network with parameters Mi,…，Mk is defined as Φ(χ) = Mk …Mix. We call
k the depth of the network and d0 , dk the input and the output dimensions, respectively. Define
d = min0≤i≤k di be the narrowest width. We say a network has a bottleneck if both d0 > d and
dk > d. A multi-tower linear network is defined as the sum of multiple linear networks (towers)
with the same input and output dimensions.
Empirical loss. Given training data D which consist of examples of pairs of x, y where the input
feature vector x ∈ Rd0, and the label y in some arbitrary set, we wish to minimize the total loss:
L(Mi,…，Mk； D)= X fy(Φ(x))= X fy(Mk …Mix).
(x,y)∈D	(x,y)∈D
Define f(A) = P(x,y)∈D fy(Ax). Then L(Mi,…，Mk; D) = f (Mk …Mi). If fy,s are all
convex differentiable functions1, then clearly f is convex differentiable too. Below we omit D
and consider the loss function L : Rd1×d0 X ... X Rdk×dk-1 → R where L(Mi,…，Mk)=
f (Mk …Mi) for some f : Rdk×d0 → R.
Derivative. Denote by ∂∂M the matrix form of the partial derivative of L with respect to M. If L
has only one variable M, We write L = ∂∂L. For simplicity, We sometimes abuse the notation by
using the same symbol for the variable and the value and omit the value. If L(X， Y ) = f(XY ), by
the chain rule,煞=f(XY)YT and ∂L = XTf0(XY).
Local minimum. For any loss function L, Mi， . . . ， Mk is a local minimum of L if there ex-
ists an open ball B, in Frobenius norm, centered at Mi， . . . ， Mk such that L(Mi， . . . ， Mk) ≤
L(Mi,…，Mk) for any (M0,…，Mk) ∈ B. A local minimum is called spurious if it is
not a global minimum. If L is differentiable, then any local minimum is a critical point of
L.	In particular, if	L(Mi,	∙∙∙ , Mk)	=	f (Mk .…Mi),	then Mi,	∙∙∙ , Mk	satisfy that ∂∂L-	=
(Mk …Mi+i)T f 0(Mk …MI)(Mi-i …Mi)T = 0	"
We need the following theorem from Laurent & von Brecht (2018):
Theorem 3. Let Lk(Mi,..., Mk) = f (Mk …Mi) where f : Rdk×d0 → R is a convex differen-
tiable function. If there is no bottleneck, i.e. d0 or dk = min0≤i≤k di, then any local minimum of
Lk is a global minimum of f.
3	Proofs
With the above preparation, we will now prove Theorem 1. If the network we consider has no
bottleneck, i.e. d = dk or d = d0 , then Theorem 3 immediately implies that all the local minima
1In practice, fy ’s are typically convex. They are usually differentiable, and if not, can be smoothly approx-
imated. For example the hinge loss can be approximated by the modified Huber loss (Zhang, 2004).
3
Under review as a conference paper at ICLR 2020
for Lk are global minima of f so the statement is vacuously true. Below we consider the case when
d = dj for some 0 < j <k. Let A = Mk ∙…Mj+ι and B = Mj …Mi. The following is the main
technical claim of the paper.
Lemma 1. If M1, . . . , Mk is a local minimum, then either f0 (AB) = 0 or A, B both have rank d.
We first show that the above lemma implies Theorem 1.
Proof. (Theorem 1) IfL2 has spurious local minima, then Lk has too. Since dj = min0≤i≤k di, for
any A ∈ Rdk ×dj and B ∈ Rdj ×d0, we can easily construct matrices Mi ∈ Rdi×di-1 for 1 ≤ i ≤ k
such that Mk …Mj+i = A, and Mj …Mi = B. If A, B is a spurious local minimum of L?, then
clearly Mi,…，Mk is a spurious local minimum of Lk.
The other direction is implied by Lemma 1. This implication has been used before multiple times
by Kawaguchi (2016); Lu & Kawaguchi (2017); Yun et al. (2018). Here we include the easy proof
for completeness. Suppose that Mi,…，Mk is a local minimum of Lk. Then by Lemma 1, either
f(AB) = 0 or A, B both have rank d. If f 0(AB) = 0, then AB = Mk .…Mi is a global minimum
of f because f is convex. Hence Mi , . . . , Mk is a global minimum of Lk too.
In the other case, A and B both have full rank d. We show that any local perturbation to A (resp.
B) can be performed by local perturbation to Mk (resp. Mi). If A = Mk … Mj+i has rank d, then
Ai = Mk-i …Mj+i ∈ Rdk-1×d has rank d too because d ≤ dk-i. Then for any D ∈ Rdk×d,
there exists Di ∈ Rdk×dk-1 such that DiAi = D. Hence (Mk + Di)Ai = MkAi + DiAi =
A+D. This implies any local perturbation to A can be done through local perturbation to Mi. More
precisely, there exists a constant c > 0, such that for any D ∈ Rdk×d, there exists Di ∈ Rdk×dk-1
with kDi k ≤ ckDk and DiAi = D. Same is true for B. This implies that if Lk(Mi, . . . , Mk)
is minimum in an open ball of radius δ centered at Mi, . . . , Mk, then L2 (A, B) is minimum in an
open ball of radius δ∕c centered at A, B. Hence if Mi,..., Mk is a local minimum of Lk, then A, B
is a local minimum ofL2. IfL2 has no spurious local minima, A, B, hence Mi, . . . , Mk, is a global
minimum.	□
In the above proof, we actually showed that ifMi, . . . , Mk is a spurious local minimum of Lk, then
A = Mk •…Mj+i, B = Mj …Mi is a spurious local minimum of L2. That is, every spurious
local minima of Lk can be directly mapped to a spurious local minimum ofL2, hence the title of the
paper.
Lemma 1 directly implies Theorem 2.
Proof. (Theorem 2) Consider the case of k = 2 and j = 1. Then we have A = M2 ∈ Rd2×d1
and B = Mi ∈ Rd1 ×d0 with di < min(d0, d2). If A, B is a spurious local minimum of L2, then
f0(AB) 6= 0 because otherwise they would have been a global minimum of f. By Lemma 1, we
have that both A, B are of rank di, i.e. they both have full rank because di < do,d2.	□
To prove Lemma 1, we first observe that
Lemma 2. If Mi,..., Mk is a local minimum of Lk ,then d∂A (A, B) = 0, d∂B (A, B) = 0.
Proof. Define gB(X) = L2 (X, B) = f(XB). Clearly gB is convex and differentiable too.
Let LB (Mj+i,..., Mk) = gB (Mk •…Mj+i). If Mi,..., Mk is a local minimum of Lk, then
Mj+i , . . . , Mk must be a local minimum of LB . In addition, dj = min0≤i≤k di = minj≤i≤k di,
so there is no bottleneck in Mj+i,..., Mk. We apply Theorem 3 to get that A = Mk •…Mj+i is a
global minimum of gB, hence d∂A (A, B) = 0. Similarly ∂L2 (A, B) = 0.	□
Now we prove the key technical claim of Lemma 1.
Proof. (Lemma 1)
We just need to show that if f0 (AB) 6= 0, then A, B must be of rank d. Below we assume f0(AB) 6=
0. We will show that A has rank d. For B, we can apply the same argument to g(X) = f(XT ).
4
Under review as a conference paper at ICLR 2020
Let r denote the rank of A. We will derive contradiction by assuming r < d. We first use
an argument by Laurent & von Brecht (2018) to construct a family of local minima. Since
A = Mk •…Mj+ι is of rank r < d, for any 2 ≤ i ≤ j + 1, Mk •…Mi has rank at most r.
Since r < d ≤ di-ι, there exists nonzero Wi-ι ∈ RdiT such that Mk •…MiWi-I = 0. Then for
any vi-1 ∈ Rdi-2, we have
Mk …Mi(Mi-I + Wi-ιvi-ι) = Mk …Mi-ι.
Now for any v1, v2,…，vj∙ where Vi ∈ Rdi-1, we claim that
Mk …Mj+1 (Mj + WjvT)…(M1 + w1vT) = Mk …M1 .	(1)
This can be shown inductively for i = j,…,1.
Mk …Mj+ι(Mj + WjVT)…(Mi + WivT) = Mk …Mi
Since M = (M1, . . . , Mk) is a local minimum, it is the minimum in an open neighborhood of M.
TT
If We SetkVik S small enough so that M = (Mi + wιvT,…，Mj + WjVT, Mj+ι,…,Mk) is in
a smaller neighborhood, then M is a local minimum too since Lk(M) = Lk(M). See Claim 1
in (Laurent & von Brecht, 2018) for a rigorous proof.
Let Be = Mj …f1. Then by Lemma 2, ¾2(A, B) = 0, i.e f AB) (A, B) = f0(ABe)BT = 0.
Since AB = AB , we have that for any M1, . . . , Mj constructed above,
f0(AB )BeT =0.	(2)
For any matrix M ∈ Rm1 ×m2, denote by M' ∈ Rm2 the '-th row vector of M, and by R(M) all
the row vectors of M. Consider the linear subspace
V = {V ∈ Rd0 | f0(AB )V = 0}.
Then (2) implies that R(B ) ⊆ V . We now show that we can choose Vi’s for 1 ≤ i ≤ j, such that
B = Mj … Mi contains a row vector which is not in V to reach a contradiction.
Let i* = min{i | R(Mi …Mi) ⊆ V}. If i* = 1, we choose a sufficiently small non-zero vector
Vi ∈/ V. This can be done by our assumption that f0(AB) 6= 0. Set Mi = Mi + WiViT. Since
Wi = 0, there exists ' such that wi` = 0. Then M' = M' + wi`vi. By M' ∈ V,vi ∈ V, wi` = 0,
`
we have M' ∈ V since V is a linear subspace. Now assuming i* > 1. Suppose that we have
constructed Mi,..., Mi, for some i ≥ i* - 1 ≥ 1, such that R(Mi …Mi) * V. We show the
construction for i + 1. If R(Mi+iMi ∙ ∙ ∙ Mi) * V, then we can simply set Mi+i = Mi+i. Assume
below that R(Mi+iMi ∙ ∙ ∙ Mi) ⊆ V. By inductive hypothesis R(Mi •…Mi) * V, thus there exists
d
say the '-th row vector V of	Mi …Mi not	in V.	Set Vi+i as	the '-th basis vector	in Rdi	so that
E ɔr ɔr rτι _ _ 一	ɔr	_	E 一
Vi+iMi …Mi = VT. Now let Mi+i = Mi+i + Wi+iVT_[. Then
Mi+iMi …Mi = Mi+iMi …Mi + Wi+iV^iMi …Mi
=Mi+i Mi …Mi + Wi+iVT .
Since R(Mi+iMi …Mi) ⊆ V but v ∈ V and Wi+i = 0, by the same argument as for i* = 1,
there must exist a row vector in Mi+i …Mi which is not in V. We have inductively constructed
B = Mj …Mi such that R(B) * V, contradicting to (2). Hence A must have rank d. This
concludes the proof.	□
Corollary 1 immediately follows from Theorem 1 and Theorem 2(1) in (Zhou & Liang, 2018). In
the following, we prove Corollary 2.
5
Under review as a conference paper at ICLR 2020
Proof. (Corollary 2) If some tower has no bottleneck, we can fix all the parameters but this
tower, we can then apply Theorem 1 to show that any local minimum is a global minimum. Hence
the statement holds. Below we assume that each tower has a bottleneck with width bi .
Suppose that We have a spurious local minimum M = (Mn,…,M^,…,M§i,…,Msks).
Similar to the proof of Theorem 1, we can break each tower i as Ai ∈ Rm×bi, Bi ∈ Rbi×n at
the bottleneck layer. Write M = P AiBi. Similarly We can shoW that either f0(M) = 0 or all the
Ai, Bi’s have full rank. Since M is a spurious local minimum, it must be the second case, i.e. all the
(Bι∖
Ai,Bi have full rank. Now let A = (A1, ∙∙∙,As) and B =	: I. Then A ∈ Rm×b,B ∈ Rb×n
s	B.s
where b = bi + …+ b§. By the same argument in the proof of Theorem 1, any perturbation of
A, B can be done through perturbation ofM, hence A, B is a spurious local minimum for the single
tower two layer network AB . If b ≥ m, n, then Theorem 3 implies that any local minimum is a
global minimum of f.	口
4 Conclusion
We have shown a non-degeneracy property of local minima in deep linear networks for general con-
vex differentiable loss. This property allows us to reduce the existence of spurious local minima in
a deep (with depth ≥ 3) linear network to the two layer linear network, and, for two layer networks,
to simplify analysis by removing the rank-deficient case. We show the application to quadratic
loss functions and the generalization to multi-tower deep linear networks. Our proof uses a novel
perturbation argument and does not require any heavy mathematical machinery.
It would be interesting to study when there is no spurious local minima beyond the quadratic loss.
By our result, we only need to consider the two layer case. Another interesting question is whether
similar phenomenon exists for non-linear networks.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018b.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. arXiv, 2018.
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient de-
scent for deep linear neural networks. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=SkMQg3C5K7.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural networks, 2(1):53-58, l989.
Peter Bartlett, Dave Helmbold, and Phil Long. Gradient descent with identity initialization effi-
ciently learns positive definite linear transformations. In ICML, 2018.
Simon S Du, Jason D Lee, Liwei Wang Haochuan Li and, and Xiyu Zhai. Gradient descent finds
global minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018a.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018b.
Mortiz Hardt and Tengyu Ma. Identiy matters in deep learning. In International Conference on
Learning Representations, 2017.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information
Processing Systems, pp. 586-594, 2016.
6
Under review as a conference paper at ICLR 2020
Thomas Laurent and James von Brecht. Deep linear networks with arbitrary loss: All local minima
are global. In International Conference on Machine Learning, pp. 2908-2913, 2018.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. NeurIPS, 2018.
Haihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. arXiv preprint
arXiv:1702.08580, 2017.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 2018.
Luca Venturi, Afonso S Bandeira, and Joan Bruna. Spurious valleys in two-layer neural network
optimization landscapes. arXiv preprint arXiv:1802.06384, 2018.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Global optimality conditions for deep neural networks.
In International Conference on Learning Representations, 2018.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small nonlinearities in activation functions create bad
local minima in neural networks. In International Conference on Learning Representations, 2019.
Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent algo-
rithms. In ICML, 2004.
Yi Zhou and Yingbin Liang. Critical points of linear neural networks: Analytical forms and land-
scape properties. In International Conference on Learning Representations, 2018.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.
7