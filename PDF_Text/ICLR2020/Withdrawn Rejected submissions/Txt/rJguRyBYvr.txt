Under review as a conference paper at ICLR 2020
Improved Detection of Adversarial Attacks
via Penetration Distortion Maximization
Anonymous authors
Paper under double-blind review
Ab stract
This paper is concerned with the defense of deep models against adversarial at-
tacks. We develop an adversarial detection method, which is inspired by the cer-
tificate defense approach, and captures the idea of separating class clusters in the
embedding space to increase the margin. The resulting defense is intuitive, ef-
fective, scalable, and can be integrated into any given neural classification model.
Our method demonstrates state-of-the-art (detection) performance under all threat
models.
1	Introduction
Defending machine learning models from adversarial attacks has become an increasingly pressing
issue as deep neural networks become associated with more critical aspects of society. Adversarial
attacks can effectively fool deep models and force them to misclassify, using a slight but maliciously-
designed distortion that is typically invisible to the human eye (Carlini & Wagner, 2017c; Athalye
et al., 2018; Szegedy et al., 2013; Goodfellow et al., 2014; Kurakin et al., 2016). Despite numerous
developments, defense mechanisms are still wanting.
Many interesting ideas have been proposed to construct defense mechanisms for adversarial exam-
ples. Among these are adversarial training (ZUo et al., 2020; Yan et al., 2018; Tramer et al., 2017;
Madry et al., 2017), ensemble methods (Strauss et al., 2017), and randomization (Dhillon et al.,
2018) to name a few. These works consider both detection and resiliency. However, many of these
defense ideas were foUnd to be inadeqUate (Athalye et al., 2018; Carlini et al., 2019; Carlini & Wag-
ner, 2017b; He et al., 2017). For example, adversarial training critically depends on the specific
choice of adversarial attacks Used to generate the adversarial training instances. As a resUlt, often
this method cannot withstand attacks based on different strategies. (Engstrom et al., 2018).
A more formal approach to adversarial defense is the certification approach (Hein & An-
driUshchenko, 2017), which is designed to provide a lower boUnd for the penetration distortion
reqUired to fool a given network. Certified defense methods are referred to as being either “exact”
or “conservative”. In exact methods no distortion smaller than the certification boUnd can penetrate
the deep neUral network (DNN) (Hein & AndriUshchenko, 2017; Wong & Kolter, 2017; Wong et al.,
2018; Cohen et al., 2019). In “conservative” methods, the boUnd is merely a relative metric for
comparing DNN robUstness to adversarial examples (TsUzUkU et al., 2018; Zhang et al., 2019; Ding
et al., 2018). Both exact and conservative methods have been criticized for being compUtationally
expensive and Unscalable (Tjeng et al., 2018; Cohen et al., 2019).
It is interesting to view adversarial attacks throUgh activation geometry in embedding layers. A
trained deep classification model tends to organize instances into clUsters in the embedding space,
according to class labels. Classes with clUsters in close proximity to one another, provide excellent
opportUnities for attackers to fool the model. This geometry explains the tendency of Untargeted
attacks to alter the label of a given image to that of an adjacent class in the embedding space as
demonstrated in FigUre 1a. ThUs, if we can modify the model to increase the margin between
clUsters, while lowering (or not increasing) the activation sensitivity in the embedding space to inpUt
changes, we can make the network more immUne to attacks. This embedding sensitivity can be
qUantified throUgh a Lipschitz constant or directly via the Jacobian.
Inspired by certificate defense methods, we developed an adversarial detection method, that captUres
the above separation in embedding space intUition. Ideally, we woUld like to lower boUnd the distor-
1
Under review as a conference paper at ICLR 2020
tion, , required by the adversary to force a DNN F to misclassify x + , where x is an input image.
We propose an approximation to such a bound which, while not formal, motivates a useful strategy
for creating defense methods. The bound, E & n/|| Jf (x)||, which is similar to other known bounds,
is given in terms of η, where η quantifies the “embedding margin” of the network, and JF (x), the
Jacobian of F with respect to x (see details in Section 2). The embedding margin, for a given inter-
mediate layer, is the minimal distance (under any p-norm) between two instances belonging to two
different classes. This approximate relation motivates a strategy of penetration distortion maximiza-
tion (PDM) whereby, we implicitly or explicitly maximize this lower bound without attempting to
calculate it.
The notion of increasing a DNN classification margin has been discussed in several contexts Liu
et al. (2016); Hoffer & Ailon (2015); Wen et al. (2016). To apply the PDM approach we propose
two procedures to increase the embedding margin. These two methods are complementary in the
sense that we can benefit by applying them together. In conjunction, we use the reverse cross-
entropy method of Pang et al. (2018), which tends to smooth the Jacobian. Our adversarial detection
mechanism is constructed by training a resilient classifier using the above three procedures; we then
apply standard kernel density estimation (KDE) on the embedding layer (Feinman et al., 2017). We
present an extensive empirical study focusing on the detection of adversarial examples under all
threat models, in which we consider the FGSM, BIM, C&W and JSMA attacks. Our experimental
procedure strictly adheres to the comprehensive evaluation desiderata proposed by Carlini et al.
(2019). The results we obtain indicate that the proposed defense achieves state-of-the-art detection.
2	Penetration Distortion Maximization
In this section, we explain the PDM strategy. Let F be a neural classifier and let x ∈ Rh×w be an
image assumed to have class label c = c(x). Let E ∈ Rh×w be a vector representing an adversarial
distortion for image x such that the (successful) adversarial instance is xadv =M x + E whose label
is different from c; namely, cadv =M F (xadv) 6= c. The attacker’s goal is to find the smallest
perturbation E such that F misclassifies x,
min ||E||
s.t. F (x + E) 6= c(x) .
For a successful adversarial attack whose distortion is required to be small, in the spirit of (Ding
et al., 2018; Tsuzuku et al., 2018; Hein & Andriushchenko, 2017; Zhang et al., 2019), we approxi-
mate a prediction for xadv using the first-order Taylor approximation
||1
F (xadv) = F(x + E) ≈ F(x) + JF (x)E,	(1)
for vector-valued functions with JF (x) being the Jacobian of F. The same approximation applies
to the output of any intermediate layer '. Denoting by F'(x) the output of layer ' we thus have,
F'(xadv) ≈ F'(x) + J'(x)e.
For layer `, we define its embedding margin,
n` =	arg min	l∣F'(xι) - Fi(x2)||.
x1,x2,c(x1)6=c(x2)
Thus,
||J£(x)e|| ≈ l∣F'(χ) - F'(χadv)|| ≥ n`	⑵
The Frobenius norm used here is sub-multiplicative (proof can be found in Appendix A); namely,
||J'(x)||||d| ≥ ||J'(X)小	⑶
Combining (2) and (3) (and ignoring the approximation error) we lower bound the norm of the
distortion E in terms of the embedding margin and the norm of the Jacobian,
1⑹& ≡∣Γ.	⑷
2
Under review as a conference paper at ICLR 2020
(a) CIFAR-10: t-SNE of embedding Layer
(b) Adversarial confusion histogram
Figure 1: Histogram of origin and target classes from C&W untargeted adversarial attack compared
to embedding layer t-SNE
While the attacker’s goal is to find a small distortion that “penetrates” another class, our goal as the
defender is to create a resilient model that forces a larger distortion. The lower bound (4) motivates
our penetration distortion maximization (PDM) method whereby the goal is to explicitly maximize
the right side of (4) with respect to the embedding layer of the model F . To successfully apply this
technique we must increase the embedding margin ηl (while not increasing the norm of the Jacobian)
and/or smooth the network to decrease the norm of the Jacobian || J'(χ)∣∣. We note that similar and
stronger, formal bounds, in terms of the Lipschitz constant, have been introduced by Tsuzuku et al.
(2018), Zhang et al. (2019), Hein & Andriushchenko (2017) and Ding et al. (2018).
3 Increasing Resiliency Using PDM
In this section we show how we use PDM, which is applied to the final layer that captures the full
embedding of the network (often referred to as the “pre-logits”). We note that technically we can
also apply PDM to any other layer in the model but defer such explorations to future work. The
proposed approach consists of three components, which are described in this section. Two novel
components are used to increase the margin, and the third is a known technique that is responsible
for reducing the norm of the Jacobian.
Our approach for increasing the embedding margin relies on the observation that at higher embed-
ding layers of a trained model, the embedding vectors (tensors) of instances tend to be structured
in clusters according to class labels. This can be seen, for example, in Figure 1a where we observe
the t-SNE visualization (Maaten & Hinton, 2008) of the embedding layer of a network trained for
CIFAR-10. Moreover, we observe that an adversarial example created by an untargeted attack of-
ten obtains a class label whose cluster is in close proximity with the cluster of the original class.
In Figure 1b we present a color matrix showing the adversarial label distribution obtained by the
C&W attack (Carlini & Wagner, 2017a). For example, the color for the Cat and Dog entry is bright
red indicating a frequent label change from cats (original) to dogs (adversarial), whose clusters are
the closest. By increasing the margin between these clusters without increasing the norm of this
layer’s Jacobian ,we make it harder for an adversary to alter the label using distortion of the same
magnitude.
Adopting ideas from cluster analysis, the increase in the embedding margin can be achieved by
either increasing the distance between clusters or reducing the variance of each cluster. Let μc =
N1- PNcI Zc be the mean of each cluster, where Nc is the number of samples from class C and Zc is
3
Under review as a conference paper at ICLR 2020
the embedding of sample i from class c , and let M be the number of classes. We thus have,
M 1 Nc
Cluster Variance = E	E ∖∖zf —μc∖∣2
c=1 Nc	i=1
M1M	1	M
ClUsterDistance = MZ M-1 £ ∖∖μi - μc∖∖2
To increase the margin, we would like to maximize the cluster distance and minimize the cluster
variance, hence
Margin Maximization Objective = Cluster Variance — Cluster Distance.
A straightforward maximization of the cluster distance is problematic because the distance is poten-
tially unbounded. However, we can proxy the distance using the angular distance between clusters.
To this end, we use the cosine similarity. We now introduce two methods to optimize these compo-
nents. We use a Siamese training procedure to maximize the cluster distance. The cluster variance
is minimized by including a variance in the loss function.
3.1	Siamese Training
To explicitly increase the embedding margin, we propose using Siamese training. We create a
Siamese network (Bromley et al., 1994), where each sub-network is our classifier. The Siamese
network has two input images denoted by xf, Xj and three outputs: two classification outputs and an
auxiliary output for the cosine similarity between each sub-network’s embedding. We introduce an
additional loss term to force embeddings from different classes samples to have a cosine similarity
of 0 or 1 otherwise. Formally,
SiameseLoss
zj ∙ Z ɪ [
∖∖zj ∖∖∖∖zj∖∖ = I
1
0
if C = C
else
3.2	Reduce Variance Loss
Inspired by Szegedy et al. (2016), we include an additional loss term that penalizes large variance
for each class’ cluster individually. We refer to this component as the “reduce variance loss” (RVL).
Formally,
Nc	Nclasses
σc = N X ∖∖zj - μc∖∖2,	RVL = N X σc	⑸
Nc Nc
i=1	c=1
The variance is estimated per class on each mini-batch, then averaged and minimized as part of the
learning process.
3.3	Reverse Cross Entropy
We use the reverse cross-entropy loss introduced by Pang et al. (2018) to minimize the norm of the
Jacobian. By labeling a sample with a “reverse” one-hot vector, we obtain
(	0 , if i = c
Ric=
[(N,s-1) ,	else,
and using a reverse cross entropy loss
LRCE = —Rc log F (x).
Similar to label smoothing (Szegedy et al., 2016), this method smooths the classifier’s gradients and
prevents the network from becoming over-confident (Muller et al., 2019). Intuitively, the differen-
tiation between two samples has a tighter upper bound, given the reverse labels Rc , than a one-hot
labeling. We tested the gradient L2 norm value on different layers. Comparing to the baseline model,
the gradients were five to ten times smaller when using the RCE training process.
4
Under review as a conference paper at ICLR 2020
3.4	PDM TRAINING
A simultaneous application of the three components described above, which can robustify a clas-
sification model, is obtained by training the model using an appropriate loss function as well as a
specialized mini-batch construction procedure. A pseudo-code of the training procedure including
the loss function appears in Algorithm 1 under Appendix F. The code is self-explanatory for the
most part. We note that an epoch begins by creating a Siamese counterpart for each image-label pair
in a given batch. With probability Q, the Siamese sample is selected from the same class, and its
cosine similarity label is set to 1. Otherwise (probability 1 - Q), the Siamese sample is selected
from a different class, and its cosine similarity label is set to 0. Notice that the Siamese and RVL
(Equation 5) components of the loss function are computed from the embedding vectors of each
mini-batch. The RCE component is calculated using the logits.
3.5	PDM Visualization
Using t-SNE to visualize the embedding space activation, Figure 2 illustrates the effect of each of the
components of our defense method. Figure 2c demonstrates how well the RVL reduces the variance,
while the Siamese training process made a more profound impact on the between-class distance as
shown in Figure 2d.
While t-SNE is useful for visualization purposes, the aggressive dimensionality reduction may lead
to misleading conclusions. To obtain quantitative evidence, We calculated the Davies-BoUldin index
(DBI) (Davies & Bouldin, 1979), which scores clustering quality according to the distance between
cluster centroids divided by the Euclidean distance betWeen points Within a cluster (loWer score
means better clustering). The DBI of the combined method is indeed the loWest at 0.23. (See the
other DBIs in the figure.)
4 Experiments
FolloWing (Pang et al., 2018; Meng & Chen, 2017; Madry et al., 2017; Song et al., 2017; Dhillon
et al., 2018; Samangouei et al., 2018) We evaluated our defense technique on the MNIST (LeCun
et al., 1998) and CIFAR-10 (Krizhevsky & Hinton, 2009) datasets.
We adopted the detection method presented by Feinman et al. (2017), using a univariate Gaussian-
based kernel density estimation (KDE), Where the density Was estimated using 1000 training sample
embeddings per class. An input image is deemed adversarial if the distance to the predicted class’
manifold exceeds a predefined threshold. As the decision is threshold-dependent, We report our
results as the area under the ROC curve (AUC). We use ResNet 56 (He et al., 2016) as our classifier,
and compare our results to tWo baselines: standard training of ResNet-56 (i.e., Without any defense
mechanism) and ResNet-56 equipped With the RCE defense, the current state-of-the-art model. The
hyper-parameters used are listed in Appendix B.
In our study We used several attacks, Which are described in Appendix E. For the bounded adversarial
attack algorithms, We used tWo versions of FGSM and BIM, one With a small perturbation = 0.05,
and another With a large perturbation = 0.1. For the unbounded attacks, We used JSMA and
tWo versions of C&W: a lean version With zero confidence, and an extensive version With a higher
confidence value, denoted by C&W-hc. We used the Cleverhans implementation (Papernot et al.,
2018) for the attacks and applied them in an untargeted manner. A detailed description of the
parameters used in the adversarial attacks appears in Appendix C. A description of the threat models
We consider in this paper appears in Appendix D.
4.1	Performance on Normal Samples
We began by evaluating the performance of our model on normal samples, as shoWn in Table 1.
While the RCE method of Pang et al. (2018) loWered the classifier’s accuracy on CIFAR-10, using
the Siamese training scheme and applying the reduce variance loss increased the accuracy for both
CIFAR-10 and MNIST. These results indicate that these margin-increasing procedures may be of
independent value in training standard classifiers, regardless of the need for adversarial robustness.
5
Under review as a conference paper at ICLR 2020
(c) Reduce Variance ; DBI:0.28
(d) Siamese Training ; DBI:0.32
(e) Combined Methods ; DBI:0.23
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck
(f) Legend
Figure 2: CIFAR-10 t-SNE visualization of the two margin increasing components of PDM . Com-
pared to the baseline, each method contributes to the increase of the margin; the combined method
display the best clustering according to the Davies-BoUldin Index.
CE RCE Siamese + RVL PDM (ours)
OFAR-10^^93.62^^933	94.37	93.81^^
MNIST 99.33 99.32	99.37	99.52
Table 1: Performance on normal samples
4.2	Gray-Box Model
We follow a strict definition of a gray-box threat model as in (Pang et al., 2018), where the attacker
has full access to the trained model, but is unaware of the detection mechanism. See Appendix D for
precise definitions of the threat-models we consider in this paper. We evaluated the performance un-
der the gray-box threat model by creating equally-sized groups of adversarial and normal examples.
6
Under review as a conference paper at ICLR 2020
We scored each example using our (KDE-based) detection mechanism. The results are shown in Ta-
ble 2. The PDM detection AUC results over MNIST are outstanding, showing that all the unbounded
attacks (such as C&W) were detected perfectly. On the CIFAR-10 dataset PDM performance lagged
behind in defending the two BIM attacks. However, it achieved excellent AUC results for the other
attacks, including a perfect score for the strong C&W high confidence attack. Figure 3 presents
the resiliency of PDM and the baselines when attacked by FGSM and BIM. Consider, for example,
Figure 3(a) depicting the resiliency achieved against FGSM over the CIFAR-10 dataset. The x-axis
corresponds to the distortion step size () used by the adversary, which alters each pixel by ±. The
y-axis measures the resiliency, namely how many perturbed instances were predicted correctly by
the model. While the resiliency monotonically decreases as a function of the step size, as expected,
the resiliency exhibited by PDM (blue) is consistently higher than the baselines. Similar behavior
was observed for the BIM attack on this dataset. Over the MNIST dataset, PDM was more resilient
than most methods, but not all step sizes.
Further investigation of the mediocre results obtained for the BIM attack revealed that the norm
of the embedding layer gradients increased significantly for embedding vectors located in-between
clusters. Since BIM makes a sequence of small gradient steps starting inside clusters, it is able to
move further away into the center of a different class where it can no longer be detected using KDE.
This observation was made by measuring the mean gradient norm after each BIM step. After several
such steps we observed that the mean norm increased by an order of magnitude. This phenomenon
does not occur when using solely RCE-trained model.
	MNIST		CIFAR-10				
	Baseline	RCE	PDM	Baseline	RCE	PDM
FGSM-0.05	0.981	0.983	0.988	0.958	0.898	0.967
FGSM-0.1	0.988	0.99	0.995	0.971	0.926	0.983
BIM-0.05	0.983	0.967	0.987	1	0.99	0.95
BIM-0.1	0.945	0.92	0.99	1	0.996	0.962
C&W	0.994	1	1	0.874	0.918	0.933
C&W-hc	0.88	0.98	1	0.637	0.94	1
JSMA	0.995	1	1	0.952	0.96	0.973
Table 2: Detection AUC under the gray-box threat model.
4.3	White-B ox Model
The white-box threat model (see Appendix D) is perhaps the most interesting from the defender’s
viewpoint because no limitations are made regarding the information known to the attacker (Carlini
et al., 2019). For the white-box threat model, we applied the C&W modified attack (hereafter
referred to as C&W-wb) (Carlini & Wagner, 2017a), which has been shown to penetrate density
estimation-based detection. To the best of our knowledge, this is the only known attack with this
property. Following the evaluation procedure used by Pang et al. (2018), we set the parameters of
the C&W-wb attack such that all attacks would succeed in fooling the model, and detection is fully
breached (i.e., their AUC score ≤ 0.5). Then, we measured the average (minimal) required distortion
that was able meet these criteria. Thus, a stronger defense should yield larger distortion. We note
that the distortion is quantified using the L2 norm, namely, d = M P= "Ji g, where M is
the number of adversarial instances, and n is the number of pixels per image.
	Baseline	RCE	PDM (ours)
MNIST	0.087^^	0.104	0.162
CIFAR10	0.008	0.019	0.026
Table 3: Distortion under the white-box threat model, scaled to [0,1], Our defense method requires
30% higher distortion on CIFAR-10 and 60% higher on MNIST
The white-box results are presented in Table 3. PDM, clearly outperforms the baselines by a wide
margin by forcing a 30% higher distortion than RCE on CIFAR-10, and 60% on MNIST.
7
Under review as a conference paper at ICLR 2020
Baseline
----RCE
----PDM
0.025	0.050	0.075	0.100	0.125	0.150	0.175	0.200
Perturbation
(a) FGSM-CIFAR-10
1.0
0.40
0.35
0.30
Baseline
----RCE
----PDM
0.025	0.050	0.075	0.100	0.125	0.150	0.175	0.200
Perturbation
(b) BIM-CIFAR-10
0.025	0.050	0.075	0.100	0.125	0.150	0.175	0.200
Perturbation
(c) FGSM-MNIST
Figure 3: Resiliency under the FGSM and BIM adversarial attacks. Our method displays signifi-
cantly higher resiliency
0.025	0.050	0.075	0.100	0.125	0.150	0.175	0.200
Perturbation
(d) BIM-MNIST
4.4	Black-Box Model
To evaluate our model in the black-box setting, we follow (Papernot et al., 2017; Carlini et al., 2019)
and create a proxy model, which is trained using input-output pairs probed from the defender’s
(target) model (i.e., the proxy model is trained via teacher-student distillation of the target model).
The proxy model is then used by the attacker to generate adversarial examples under the white-box
threat model (this black-box model variant is the most difficult, and was referred to by Pang et al. as
“A white, black-box attack”). In our case, where detection is based on KDE, we could only use the
C&W-wb because it is the only available attack known to penetrate KDE.
Following Pang et al. in this setting we used ResNet32 He et al. (2016) for the proxy model. Here
again, we measured detection AUC rates as in the gray-box setting. The results appear in Table 4
and show that PDM is consistently and significantly better than the baselines.
Baseline RCE PDM (ours)
MNIST	088	094	099
Cifar10	093	0933	0952
Table 4: Detection AUC under the black-box threat model
5 Concluding Remarks
We introduced a powerful approach for the defense of deep models against adversarial attacks
by building on procedures for margin maximization within a penetration distortion maximization
framework and the RCE loss technique. Our empirical evaluation demonstrated state-of-the-art re-
sults in defense against all threat models (with mixed results for the BIM attack). In addition, we
provide some geometric intuition on attacks and defenses using t-SNE visualizations.
8
Under review as a conference paper at ICLR 2020
This work raises several interesting questions. First, it would be valuable to examine other meth-
ods for margin maximization and Jacobian reduction. For example, recently Elsayed et al. (2018)
proposed a sophisticated loss function that tends to maximize the embedding margin. Similarly, a
recent work by Zhang et al. (2019) proposed an iterative technique to reduce the norm of the Jaco-
bian. Finally, it would be very interesting to explore ways to increase the margin (and reduce the
Jacobian) on shallower embedding layers where lower-level features are formed.
Acknowledgment: The authors would like to express their gratitude to Tianyu Pang for his invalu-
able help in the implementation and understanding of his work on the reverse cross-entropy method.
References
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420,
2018.
Jane Bromley,Isabelle Guyon, Yann LeCun, Eduard Sackinger, and RooPak Shah. Signature Verifi-
cation using a” siamese” time delay neural network. In Advances in neural information processing
Systems, pp. 737-744,1994.
Nicholas Carlini and David Wagner. Adversarial examPles are not easily detected: ByPassing ten
detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and
Security, pp. 3-14. ACM, 2017a.
Nicholas Carlini and David Wagner. Magnet and” efficient defenses against adversarial attacks” are
not robust to adversarial examples. arXiv preprint arXiv:1711.08478, 2017b.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39-57. IEEE, 2017c.
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris
Tsipras, Ian Goodfellow, and Aleksander Madry. On evaluating adversarial robustness. arXiv
preprint arXiv:1902.06705, 2019.
Jeremy M Cohen, Elan Rosenfeld, andJ Zico Kolter. Certified adversarial robustness via randomized
smoothing. arXiv preprint arXiv:1902.02918, 2019.
David L Davies and Donald W Bouldin. A cluster separation measure. IEEE transactions on pattern
analysis and machine intelligence, (2):224-227, 1979.
Guneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein, Jean Kossaifi,
Aran Khanna, and Anima Anandkumar. Stochastic activation pruning for robust adversarial de-
fense. arXiv preprint arXiv:1803.01442, 2018.
Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. Max-margin adversar-
ial (mma) training: Direct input space margin maximization through adversarial training. arXiv
preprint arXiv:1812.02637, 2018.
Gamaleldin Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large
margin deep networks for classification. In Advances in neural information processing systems,
pp. 842-852, 2018.
Logan Engstrom, Andrew Ilyas, and Anish Athalye. Evaluating and understanding the robustness
of adversarial logit pairing. arXiv preprint arXiv:1807.10272, 2018.
Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. Detecting adversarial
samples from artifacts. arXiv preprint arXiv:1703.00410, 2017.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
9
Under review as a conference paper at ICLR 2020
Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. Adversarial example
defense: Ensembles of weak defenses are not strong. In 11th {USENIX} Workshop on Offensive
Technologies ({WOOT} 17), 2017.
Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier
against adversarial manipulation. In Advances in Neural Information Processing Systems, pp.
2266-2276, 2017.
Elad Hoffer and Nir Ailon. Deep metric learning using triplet network. In International Workshop
on Similarity-Based Pattern Recognition, pp. 84-92. Springer, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533, 2016.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax loss for convolu-
tional neural networks. In ICML, volume 2, pp. 7, 2016.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(Nov):2579-2605, 2008.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In
Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security,
pp. 135-147. ACM, 2017.
Rafael Muller, Simon Komblith, and Geoffrey Hinton. When does label smoothing help? arXiv
preprint arXiv:1906.02629, 2019.
Tianyu Pang, Chao Du, Yinpeng Dong, and Jun Zhu. Towards robust detection of adversarial exam-
ples. In Advances in Neural Information Processing Systems, pp. 4584-4594, 2018.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. In 2016 IEEE European Sympo-
sium on Security and Privacy (EuroS&P), pp. 372-387. IEEE, 2016a.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on
Security and Privacy (SP), pp. 582-597. IEEE, 2016b.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM
on Asia conference on computer and communications security, pp. 506-519. ACM, 2017.
Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Alexey Ku-
rakin, Cihang Xie, Yash Sharma, Tom Brown, Aurko Roy, Alexander Matyasko, Vahid Behzadan,
Karen Hambardzumyan, Zhishuai Zhang, Yi-Lin Juang, Zhi Li, Ryan Sheatsley, Abhibhav Garg,
Jonathan Uesato, Willi Gierke, Yinpeng Dong, David Berthelot, Paul Hendricks, Jonas Rauber,
and Rujun Long. Technical report on the cleverhans v2.1.0 adversarial examples library. arXiv
preprint arXiv:1610.00768, 2018.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against
adversarial attacks using generative models. arXiv preprint arXiv:1805.06605, 2018.
Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend:
Leveraging generative models to understand and defend against adversarial examples. arXiv
preprint arXiv:1710.10766, 2017.
10
Under review as a conference paper at ICLR 2020
Thilo Strauss, Markus Hanselmann, Andrej Junginger, and Holger Ulmer. Ensemble meth-
ods as a defense to adversarial perturbations against deep neural networks. arXiv preprint
arXiv:1709.03423, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 2818-2826, 2016.
Vincent Tjeng, Kai Y Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. 2018.
Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204,
2017.
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certifi-
cation of perturbation invariance for deep neural networks. In Advances in Neural Information
Processing Systems, pp. 6541-6550, 2018.
Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning approach
for deep face recognition. In European conference on computer vision, pp. 499-515. Springer,
2016.
Eric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.
Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial
defenses. In Advances in Neural Information Processing Systems, pp. 8400-8409, 2018.
Ziang Yan, Yiwen Guo, and Changshui Zhang. Deep defense: Training dnns with improved adver-
sarial robustness. In Advances in Neural Information Processing Systems, pp. 417-426, 2018.
Huan Zhang, Pengchuan Zhang, and Cho-Jui Hsieh. Recurjac: An efficient recursive algorithm for
bounding jacobian matrix of neural networks and its applications. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 33, pp. 5757-5764, 2019.
Fei Zuo, Bokai Yang, Xiaopeng Li, and Qiang Zeng. Exploiting the inherent limitation of l0 ad-
versarial examples. In 22nd International Symposium on Research in Attacks, Intrusions and
Defenses ({RAID} 2019), 2020.
11
Under review as a conference paper at ICLR 2020
Appendix
A L2 NORM SUB -MULTIPLICATIVITY
Given A ∈ RNxMxK and B ∈ RKxJxL, we claim that the Frobenius norm of the multiplication of
A and B is less than or equal to the multiplication of each tensor’s Frobenius norm. Proof:
NMJL K
kABk2F=XXXXXan,m,kbk,j,l
n=1 m=1 j=1 l=1 k=1
NMJLK	K
6 X X XXX |an,m,k|2X |bk,j,l|2
n=1 m=1 j=1 l=1 k=1	k=1
NMJL K
=XXXX(X |an,m,k|2 |bk,j,l|2)
n=1 m=1 j=1 l=1 k,s=1
NMK	JLK
=XXX|an,m,k|2XXX|bs,j,l|2
n=1 m=1 k=1	j=1 l=1 s=1
= kAk2FkBk2F
(Cauchy-Schwarz)
(6)
B Classifier Hyper-Parameters
Parameter	Value
Optimizer	SGD
ResNet Depth	56
Weight Regularization	L2 (0.002)
Batch Size	128
Initial Learning Rate	0.1
Epochs-CIFAR-10	400
Epochs-MNIST	80
Activation	Leaky-Relu (0.1)
C Adversarial Attack Parameters
Attack	Parameter	Value
BIM	Iterations	10
JSMA	Max Iterations	100
CW	Max Iterations	10
	Binary Search Steps	9
	Confidence	0
CW-hc	Max Iterations	1000
	Binary Search Steps	9
	Confidence	10
CW-wb	Max Iterations	5000/10000
	Confidence	0/0
	Binary Search Steps	3/3
Table 5: Adversarial attacks parameters
12
Under review as a conference paper at ICLR 2020
D	Threat Models
Rigorous analyses of adversarial defense systems requires precise specification of what the adversary
knows and can do. Conducting this type of specification is called threat modeling (Kurakin et al.,
2016). In this paper we consider the three core models, which are differentiated by their knowledge
of the classifier (target network) and the defense mechanism being used.
•	Black-box: In this weakest scenario, the adversary has no knowledge of our system. It
does not know the target network architecture, cannot access its gradients and does not
know the defense methods. The adversary can, however, sample the targeted network for
input-output pairs and has access to the dataset used to train the target network.
•	Gray-box (a.k.a. oblivious): Here the adversary has full knowledge of the target network
and can access its gradients and parameters, including the data being used in the training
process. However it has no information on the defense mechanism.
•	White-box: In this most challenging scenario, we assume the adversary has full knowledge
of the entire system to be attacked, which includes the target network, the defense method,
their parameters and the data used for training.
To ensure a clear distinction between threat models, Carlini et al. (2019) recently proposed a compre-
hensive methodology for evaluating adversarial attacks and defenses, thus emphasizing the differ-
ence between gray-box and white-box attacks. We believe that following the Carlini et al. guideline
is crucial for advancing this line of research and therefore we adhere to them strictly.
E Attack Algorithms
Adversarial attack algorithms aim to fool DNNs such that a given image is misclassified with min-
imal perturbations. In untargeted attacks, the adversary aims to minimize the true class activation
so that another arbitrary class will be predicted. In this paper, we focus on targeted attacks where
the adversary aims to fool the classification in a controlled manner such that a specific class will be
predicted instead of the correct one. Thus, targeted attacks are considered far more dangerous. For
example, fooling an autonomous driving system to interpret a stop sign as a speed limit sign is far
more dangerous than interpreting it as a yield sign.
In this section, we discuss the various attack algorithms we use to evaluate our defense method. We
denote x,x0 as the input and adversarial images, respectively, ` as the target label, F as the target
model with loss function LF (x, `) and = ||x - x0|| as the pixel-wise perturbation between the
adversarial and normal images. The general formulation, therefore, becomes,
minimize ||x - x0 ||2 s.t. F (x0) = `	(7)
x0
FGSM
Goodfellow et al. (2014) introduced the fast gradient sign method (FGSM), which optimizes the
adversarial image by back-propagating the input through the attacked DNN, in accordance with the
desired target. Formally, letting be a fixed parameter, the adversarial example is,
X = X + E sign(VLF (x,')	(8)
While not as effective as other attack algorithms, this method has the advantage of being one of the
fastest ones.
BIM
Kurakin et al. (2016) introduced the Basic Iterative Method (BIM), which performs the FGSM
method iteratively, clipping the perturbation if needed. Formally,
x0N +1 = x0N + E sign(VLF (x0N, l)),
where E is a fixed parameter.
C&W
13
Under review as a conference paper at ICLR 2020
Carlini & Wagner (2017c) introduced the C&W method, which operates by modifying the L-BFGS
method as follows,
minimize ||x - x0||2 + cf (x0),
x0
where c is a hyperparameter and the loss function, f, is chosen such that f(x0) <= 0 if x0 is
classified as the target class; namely,
f (x0) = max(Z(x0)` — Z(x0)t, K)
where Z is the target DNN logits, t is the correct label and κ is a hyperparameter referring to the
confidence of the attack. The higher the confidence, the higher the activation for the target class is,
and therefore, the larger the perturbation.
Three different Euclidean norms are considered with this algorithm, L0 , L2 , L∞ . Following Pang
et al. (2018), we conduct our evaluation using L2 . This attack method is considered very ef-
fective and has had great success in overcoming various defense methods Carlini & Wagner
(2017c),Papernot et al. (2016b).
JSMA
Papernot et al. (2016a) introduced the Jacobian-based Saliency Map Attack (JSMA) which alters a
single pixel of x at each iteration to maximize the saliency map. This method is known to enforce
large perturbations but on fewer pixels than other methods.
F PDM Training Algorithm
Algorithm 1 PDM Training		
1	: procedure PDM	
2	:	for batch = 1, . . . ,#batches do	
3	X, Y — get_batch()	
4	:	initialize Xsiamese = [], Ysiamese = [], S = []	
5	for b = 1,..., batch_size do	
6	q 〜Bernoulli(Q)	
7	:	if q == 1 then	
8	y — Y [b]	
9	S — 1	
10	:	else	
11	y J random class = yι	
12	s J 0	
13	:	x J random sample from class Y[b]	
14	:	Append x, y, s to Xsiamese , Ysiamese , S	
15	z1,z2 J F'(X),F'Xsiamese)	. sample’s embedding
16	:	p1,p2 J F(X),F(Xsi amese)	
17	SL = batcisize P 1 ∣∣zz1zZ2∣∣ - SI	. model logits
18	RVL J 0	
19	:	for c = 0, . . . ,classes do	
20	μc = N1c PNcIzi	
21	σc = Nr PNc1 llzc - Mc||2	
22	:	RVL = RVL + σc	
23	:	minimize -RY log (p1)-RYsiamese log (p2) + RVL + SL	
14