Under review as a conference paper at ICLR 2020
GAN-based Gaussian Mixture Model Responsi-
bility Learning
Anonymous authors
Paper under double-blind review
Ab stract
Mixture Model (MM) is a probabilistic framework which allows us to define a
dataset containing K different modes. When each of the modes is associated with
a Gaussian distribution, we refer it as Gaussian MM, or GMM. Given a data point
x, GMM may assume the existence of a random index k ∈ {1, . . . , K} identify-
ing which Gaussian the particular data is associated with. In a traditional GMM
paradigm, it is straightforward to compute in closed-form, the conditional like-
lihood p(x|k, θ), as well as responsibility probability p(k|x, θ) which describes
the distribution index corresponds to the data. Computing the responsibility al-
lows us to retrieve many important statistics of the overall dataset, including the
weights of each of the modes. Modern large datasets often contain multiple unla-
belled modes, such as paintings dataset containing several styles; fashion images
containing several unlabelled categories. In its raw representation, the Euclidean
distances between the data do not allow them to form mixtures naturally, nor it’s
feasible to compute responsibility distribution, making GMM unable to apply.
To this paper, we utilize the Generative Adversarial Network (GAN) framework
to achieve an alternative plausible method to compute these probabilities at the
data’s latent space z instead of x. Instead of defining p(x|k, θ) explicitly, we de-
vised a modified GAN to allow us to define the distribution using p(z|k, θ), where
z is the corresponding latent representation of x, as well as p(k|x, θ) through an
additional classification network which is trained with the GAN in an “end-to-
end” fashion. These techniques allow us to discover interesting properties of an
unsupervised dataset, including dataset segments as well as generating new “out-
distribution” data by smooth linear interpolation across any combinations of the
modes in a completely unsupervised manner.
1	Introduction
Gaussian Mixture Model (GMM) is one of the commonly used probabilistic models for dataset enjoy
multiple modes. It assumes that all data points come from a mixture of a finite number of Gaussian
distributions. The density function of the GMM is defined below:
K
PZ(Z) = EakN(z; μk, ∑k)
k=1
(1)
where K is the total number of Gaussians in the mixture, and kth component is characterized by a
Gaussian distribution with weight ak, mean μk and covariance matrix ∑k.
Given x is the data and k is the (latent) index of the mixture density, GMM allows us to compute
the conditional likelihood p(x|k, θ) as well as responsibility probability p(k|x, θ). In the Bayesian
paradigm, one may refer p(k|x, θ) as the posterior density where the prior is p(k) ≡ U ({1, . . . , K}).
This will further allow us to retrieve many important statistics and properties about the dataset,
for example the segment membership of the data, the overall weights of the modes, we may even
synthesizing meaningful “out-distribution” data using a convex combination of any two modes.
However, the conditional likelihood is hard to compute when dealing with high dimensional data,
such as images, as data in its raw form do not form mixtures naturally.
1
Under review as a conference paper at ICLR 2020
At the same time, Generative Adversarial Network (GAN) gives us a way to compute the latent
representation z associated with the data x: GAN (Goodfellow et al. (2014)), introduces a 2-player
non-cooperative game by a generator G and a discriminator D. The generator produces samples
from the random noise vector z. The Discriminator differentiates between true samples and fake
samples. The objective function of the game is given as follows:
minmaxV(D, G) = Ex〜Pdata(X) [logD(x)] + Ez〜Pz(z)[log(1 - D(G(Z)))]	⑵
The problem of the original GAN methodology is that the association between the latent vector z
from a low-dimension latent space to a data sample x (in high-dimension data space) is one way, i.e.,
one must generate z before generating x, making the backward association from x → z infeasible.
Since the generation of z is independent of x, any arbitrary distribution should theoretically achieve
its take, making a uniform distribution or a standard Gaussian distribution a popular choice.
In our proposed work, we are to devise methods in which we are able to compute p(z|k, θ) and
p(z|x, θ) to replace p(x|k, θ) and p(k|x, θ) respectively, through the use of GAN training. p(z|x, θ)
is achieved by upgrading prior distribution p(z) to a GMM instead of a simple Gaussian. At the
same time, the responsibility probability p(k|x, θ) is learned through a classification network which
is trained from end-to-end with the GAN. The learned classifier can subsequently be used to classify
new data points or perform segmentation on the test set. We also observe through experiments that a
smooth linear interpolation can be performed across multiple distribution modes to create the desired
effects of “out-distribution” data.
Below we introduce some previous research that are related to our work.
1.1	Gaussian Mixture Model in GAN
Eghbal-zadeh & Widmer (2017) integrate a GMM into the GAN framework. Both of the means and
covariance matrices are trainable through the generator loss. Besides, instead of applying the classic
adversarial loss as in Equation 1, the authors proposed to use GMM likelihood.
Ben-Yosef & Weinshall (2018) also proposed a method named GM-GAN which used Gaussian
Mixture to model the distribution over the latent space, in addition to its variant for the conditional
generation. The supervised GM-GAN modifies the Discriminator, so that instead of a single scalar,
it returns a vector o ∈ RN. Each element of o represents the probability of the given sample being
in each class. This will allow that images generated from the generator will be classified by the
discriminator as a certain class.
In addition, in the absence of prior knowledge, the GMM is assumed to be uniform for both works
above, i.e., ∀k ∈ {1,...,K} αk = K. On the contrary, our work makes no such assumption, the
segmentation of data can be inferred from the trained model.
1.2	Linear Interpolation in GAN
Previously researchers have studied linear interpolation in the trained GAN models, where linear
interpolations in the noise space lead to semantic interpolations in the generated images. Bojanowski
et al. (2017) models the latent space as a unit sphere and learns the correspondence from the sphere
space to the data space without the adversarial loss. Such a trained model is able to achieve smooth
linear interpolation output between any two random vectors on the unit sphere.
Chen et al. (2016) proposed InfoGAN which learns to maximize the mutual information between
a subset of latent variables and the observation, i.e. I(c; G(z, c)), where c is the class of the real
sample. A learned InfoGAN model can disentangle discrete and continuous latent factors. Thus,
linear interpolation can be performed using the continuous latent code, for example, from a thin
digit to a wide digit; but not possible on the discrete codes, such as across categories of images.
In the next section, we explain in details how each part of the proposed algorithm works.
2
Under review as a conference paper at ICLR 2020
1.3	Paper Organization
In Section 2, we describe how each component of the proposed mechanism works. In Section 3,
we demonstrate the performance of the proposed method by comparing it against several baseline
models. Section 4 concludes the paper.
2	Architecture
The proposed architecture consists of three networks. First, a classifier C which outputs the possi-
bility that a given image belonging to each Gaussian, and the possibility is used to construct a GMM
distribution. Second, a Generator G which produces synthetic samples from GMM random vectors.
Third, a Discriminator D which encodes samples to feature vectors and discriminates between real
and synthetic samples. The overall architecture design is shown below in Figure 1. In the following
sections, we explain the details of the three networks.
Figure 1: The overall architecture. The feed-forward logic of the Classifier C, the Generator G and
the Discriminator D are marked with different colours.
2.1	The Classifier
For an image xi, the classifier C outputs the probability αi = {αi1, . . . , αik, . . . , αiK} of the image
belonging to each Gaussian indexed from 1 to K . These probability values are considered to be
the mixture weights of the GMM which corresponds to the specific image, i.e, the density function
K
of the GMM model which corresponds to image Xi is P akN(μk, Σk). μk and Σk for k =
k=1
{1, 2, . . . , K} are trainable parameters. During test time, the learned Classifier can easily output
p(k |x, θ) for any test image x.
The design of the classifier has two alternatives, the first is to share the feature encoding layers with
the Discriminator. The shared feature encoding network encodes each image to a feature vector, and
the Classifier will be a simple standard linear Soft-max classifier built on top of the features. The
second is to build a stand-alone network which contains multiple CNN layers to classify images.
Details of both network designs can be found below in Table 1. In Table 1, we use acronyms for
operations in the table: “Conv” is the convolution operation, of which the kernel and stride size are
in the bracket; “Batch norm” is short for batch normalization; “Flatten” refers to the operation that
flattens a tensor to 1D array. The same acronyms are also used in Table 2 and Table 3. Dimg, Dh and
Dz are related to the datasets we use, the exact value of each are reported in Section 3.
In Section 3.3, we report the results of both classifier designs in terms of generation performance
and computation costs.
3
Under review as a conference paper at ICLR 2020
Stage	Sub-stage	Name	Input Tensors	Output Tensors
C	Encoding Network (if shared)	Conv (kernel=5, stride=2) + LeakyReLU	Dimg × Dimg × Dh	Dimg/2 × Dimg/2 × 64
		Conv (kernel=4, stride=2) + Batch norm + LeakyReLU + Flatten	Dimg/2 × Dimg/2 × 64	Dimg/4 × Dimg/4 × 128
	Encoding Network (if not shared)	Conv (kernel=5, stride=1) + ReLU	Dimg × Dimg × Dh	Dimg × Dimg × 32
		MaxPool (pooLsize=2, stride=2)	Dimg × Dimg × Dh	Dimg/2 × Dimg/2 × 32
		Conv (kernel=5, stride=2) + ReLU + Flatten	Dimg/2 × Dimg/2 × 32	Dimg/4 × Dimg/4 × 16
		Linear + ReLU	Dimg/4 × Dimg/4 × 16	1024
	Classification Network (if shared)	Linear	Dimg/4 × Dimg/4 × 128	K
	Classification network (if not shared)	Linear	1024	K
Table 1: The network structure of the Classifier C . Each column from left to right reports the
network, sub-stage name, name of operations and shapes of input and output tensors.
Parameters of the classifier are optimized by both the adversarial loss and the mutual information
loss, the details of both loss functions are discussed in Section 2.2, and how the update is performed
is introduced in Section 2.4.
The Classifier is not required for generating new images after the model is fully trained. During
testing, a random vector is sampled directly from the GMM model for the generation. The Classifier
can then used to assign an unseen image to Gaussians and perform segmentation on the testing set.
2.2	The Generator
Given the classification result αi for the input image xi, ideally, multinomial sampling is performed
K
to select one Gaussian out of K. The density function of this sampling is P af * N(μk, Σk), and a
k=1
random vector ziis sampled from the chosen Gaussian distribution to perform the image generation.
However, our design expects the training to be performed in an “end-to-end” fashion, and the sam-
pling process would break the chain of the back-propagation from the adversarial loss to the classi-
fier parameters. Therefore, we instead sample one random vector from each Gaussian represented
as Zi = [z1,..., ZK]. These vectors are used to generate K synthetic images [X1,..., XK]. αi is
used to weigh the adversarial loss calculated from each pair of a generated sample Xk and the real
image xi.
In addition, the reparameterization trick is applied to the Zi sampling process so that the back-
propagation can be used to update the parameters μk and ∑k of each Gaussian. Instead of sampling
Zi 〜N(μk, ∑k), We define Zi = AkE + μk ∀k ∈ [1, K], where E is sampled as E 〜N(0, I), and
Σk = Ak Ak> .
The adversarial loss for the proposed framework is thus calculated over K pairs of real and synthetic
samples as:
LadVerSanaI = Eχi∈Pdata (KXαk × (logD(Xi) + log(1 - D(Xk))))	⑶
i=1
In addition to the adversarial loss, we also encourage the classifier to restore the input classi-
fication output from the generated image. Therefore, we also employ mutual information loss
between the classification result from the real and the generated image. Following InfoGAN
by Chen et al. (2016) where the mutual information loss is formulated as I(c, G(Z, c)) =
Ec〜P(c),x〜G(z,c) [logQ(c∣x)] + H(c). where C is the class of real sample. We define the mutual
information loss in our work as:
1
L = Exi∈Pdata κ X [αi × "Xli))
k=1
(4)
The Generator structure used in our experiments is as below in Table 2. In the table, “LeakyReLU”
which is short for “leaky rectified linear unit” and “Tanh” are activation functions.
4
Under review as a conference paper at ICLR 2020
Stage	Name	Input Tensors		Output Tensors	
G	Linear + Batch norm + LeakyReLU + Reshape	1 ×Dz		Dimg/4 × Dimg/4 × 256	
	Transposed Conv (kernel=5, stride=1) + Batch norm + LeakyReLU	Dimg/4 × Dimg/4	× 256	Dimg/4 × Dimg/4 ×	128
	Transposed Conv (kernel=5, stride=2) + Batch norm + LeakyReLU	Dimg/4 × Dimg/4	× 128	Dimg/2 × Dimg/2 ×	64
	Transposed Conv (kernel=5, stride=2) + Tanh	Dimg/2 × Dimg/2	× 64	Dimg × Dimg × Dh	
Table 2: The network structure of the Generator G.
2.3	The Discriminator
As we mentioned in Section 2.1, the design of the Discriminator has two options: whether or not
the image encoding layers are shared with the classifier. The image encoding network is followed
by a standard linear logistic regression to identify the given image to be real or fake. Details of the
network are shown below in Table 3.
Stage	Sub-stage	Name	Input Tensors	Output Tensors
D	Encoding Network Discriminator Network	Conv (kernel=5, stride=2) + LeakyReLU Conv (kernel=4, stride=2) + Batch norm + LeakyReLU + Flatten Linear	Dimg × Dimg × Dh Dimg/2 × Dimg/2 × 64 Dimg/4 × Dimg/4 × 128	Dimg/2 × Dimg/2 × 64 Dimg/4 × Dimg/4 × 128 1
Table 3: The network structure of the Discriminator D.
2.4	Training parameters for the prior distribution
In our setting, the prior distribution is a GMM model, the two trainable variables are means for K
Gaussians μ ∈ RK×Dz and standard deviations for K Gaussians A ∈ Rκ×Dz×Dz. Both variables
are updated by the adversarial loss in addition to the mutual information loss as the training is
performed “end-to-end”.
Below we give the pseudocode about how the updates are performed on each network in one itera-
tion.
Algorithm 1 Training the proposed model for 1 iteration
Require: X = [x1, x2, . . . , xM] - M training images in one batch
1:	for i = 1 . . . M do
2:	Classify xi into αi = [αi0, αi1, . . . , αik, . . . , αiK]
3:	for k = 1 . . . K do
4:	E 〜N(0, I)
5:	Zk = AkE + μk
6：	Xk J G(Zk)
7:	Classify Xk into c^i = [α0,ɑ1,... ,αk,..., aK ]
8:	end for
9:	Calculate LadVerSariaI from Xi and [χi... XK] as in Equation 3
10:	Calculate LI from αi and Ci as in Equation 4
11:	Update the Discriminator with Ladversarial
12:	Update both the Generator and the Classifier with Ladversarial
13:	Update the Classifier with LI
14:	end for
3	Experiments
In this section, we evaluate the performance of the proposed method by comparing it with several
baselines.
3.1	Experiment setup
The datasets we use are the MNIST (LeCun et al. (2010)), Fashion-MNIST (Xiao et al. (2017))
and Oxford-102 Flower (Krizhevsky (2009)) datasets. The details are listed below in Table 4. In
5
Under review as a conference paper at ICLR 2020
particular, we only select a subset of Oxford-102 to perform the training, which is the images which
belong to the first 10 classes. For experiments performed on each dataset, we used different hyper-
parameters, the details are listed in Table 5.
Dataset	Number of Classes	Data Dimension	Train Samples	Validation Samples	Test Samples
MNIST	10	28 × 28 × 1	60,000	-	10,000
Fashion-MNIST	10	28 × 28 × 1	60,000	-	10,000
Oxford-102 Flower	102	64 × 64 × 3	1,020	1,020	6,149
Table 4: Statistics of the different datasets used in the empirical evaluation.
Dataset	Number of Epochs	Learning Rate γ	Dimg	Dh
MNIST	200	0.0002	28	1
Fashion-MNIST	200	0.0002	28	1
Oxford-102	10,000	0.0002	64	3
Table 5: Hyper-parameters for the training performed on each dataset.
3.2	Linear Interpolation across Gaussian
Below in Figure 2, we show samples generated by the proposed model trained on several datasets
in the completely unsupervised manner. We set the number of Gaussians equal to the total number
of classes of the dataset in all experiments. When we are performing the linear interpolation as in
the right panels, the random vector Z for each image generation is calculated as Z = AkE + μk
∀k ∈ {1,..., K}, where E is sampled as E 〜 N(0, I). E is kept the same for all images for each
dataset.
We can draw two conclusions from the results in Figure 2. First, a fully trained proposed method
can learn to “allocate” each class of image to a Gaussian. Second, the trained model can be used to
perform smooth linear interpolation between Gaussians and even among more than two Gaussians.
In Figure 3, we demonstrate the linear interpolation performed over 3 categories. The proportion of
Gaussians of the synthetic images can be set manually.
3.3	Image Generation Quality
The generation performance is measured with two commonly used metrics: Inception score (Sali-
mans et al. (2016)) and Frchet Inception Distance (FID) score (Heusel et al. (2017)).
Inception score is calculated as I = exp(ExDKL(p(y|x)||p(y))), and a higher value generally indi-
cates a better performance.
where x is a generated image and y is the label predicted by the Inception model (Szegedy et al.
(2015)).
FID score is another metrics that measures the image generation quality. A lower value shows a
better image quality and diversity. It calculates the difference between real images x and generated
images g as FID(x, g) = ∣∣μχ - μg||2 + Tr(Σχ + Σg - 2(∑χ∑g)2).
Note that limitations of both Inception and FID score have been pointed out in several previous
literature (Barratt & Sharma (2018), Lucic et al. (2018)), and there is currently no “perfect” metrics
at this moment. These two metrics are used as an indication rather than a hard measure.
This evaluation is performed on the Oxford-102 dataset. As the Inception score is suggested to be
evaluated on a large enough number of samples, we generate 5K synthetic samples to calculate both
values. Below in Figure 4, we show the plot of Inception scores and FID scores calculated over the
training epochs. In Table 6 we report the number of trainable parameters, the best Inception and FID
score.
While the four algorithms use the same network settings for the Generator and Discriminator, it is
clear to see that the proposed method, whether or not the encoding layers are shared between the
6
Under review as a conference paper at ICLR 2020
Figure 2: Samples generated by the proposed models trained on the MNIST (top panels), Fashion-
MNIST (middle panels) and CIFAR-10 (bottom panels) datasets. In the left panels, the Gaussian
mixture contains K = 10 Gaussians, and each row contains images sampled from a different Gaus-
sian. In the right panels, each row shows the linear interpolation result from one Gaussian to another.
Note that the index of Gaussian does not necessarily correspond to the actual digit, generated images
are reordered as the order of the digit only for demonstration purpose.
7
Under review as a conference paper at ICLR 2020
Figure 3: Linear interpolation over 3 Gaussians on the MNIST, Fashion-MNIST and Oxford-102
dataset. Each row and each column contains linear interpolation performed between 2 Gaussians.
Images along the diagonal shows the interpolation across 3 Gaussians.
classifier and the Discriminator, is able to out-perform previous baseline models in terms of the im-
age generation quality. The shared feature encoding layers would further improve the performance
and reduce the size of the network.
	number of parameters	Inception Score ↑	FID score J
Proposed (encoding not shared)	13,005,411	2.9664 ± 0.2188	231.0577 ± 7.5371
Proposed (encoding shared)	8, 794, 835	3.1368 ± 0.1596	205.9776 ± 7.8587
GM-GAN	8, 467, 145	2.6770 ± 0.1079	239.3936 ± 6.7672
Vanilla GAN	8,366,145	2.4882 ± 0.1065	247.0610 ± 7.2361
Table 6: Number of parameters, Inception scores and FID scores of the proposed method and the
baselines.
Figure 4: Inception score and FID score over training epochs.
4	Conclusion
In this paper we propose a novel framework which we incorporate the GMM to model the latent
prior distribution. We also use a Classifier that learns to categorize an image to Gaussians. The
Classifier is trained with the GAN model in an “end-to-end” fashion. We demonstrate through
experiments that the proposed method surpass previous baselines in terms of the image generation
performance with only minor growth on the size of the network. A trained model is also able to
perform smooth linear interpolation across Gaussians, i.e. generate images with mixed styles from
multiple categories.
8
Under review as a conference paper at ICLR 2020
References
Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973,
2018.
Matan Ben-Yosef and Daphna Weinshall. Gaussian mixture generative adversarial networks for
diverse datasets, and the unsupervised clustering of images. CoRR, abs/1808.10356, 2018. URL
http://arxiv.org/abs/1808.10356.
Piotr Bojanowski, Armand Joulin, David Lopez-Paz, and Arthur Szlam. Optimizing the latent space
of generative networks. arXiv preprint arXiv:1707.05776, 2017.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 29, pp. 2l72-2180. Curran Associates, Inc., 2016.
Hamid Eghbal-zadeh and Gerhard Widmer. Probabilistic generative adversarial networks. CoRR,
abs/1708.01886, 2017. URL http://arxiv.org/abs/1708.01886.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Pro-
cessing SyStemS 27, pp. 2672-2680. Curran Associates, Inc., 2014. URL http://papers.
nips.cc/paper/5423-generative-adversarial-nets.pdf.
Martin HeUSeL HUbert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gunter Klambauer, and
Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a nash equilibrium.
CoRR, abs/1706.08500, 2017. URL http://arxiv.org/abs/1706.08500.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann. lecun. com/exdb/mnist, 2, 2010.
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans
created equal? a large-scale study. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems
31, pp. 700-709. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
7350-are-gans-created-equal-a-large-scale-study.pdf.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and
Xi Chen. Improved techniques for training gans. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp.
2234-2242. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/
6125-improved-techniques-for-training-gans.pdf.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Re-
thinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015. URL
http://arxiv.org/abs/1512.00567.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. CoRR, abs/1708.07747, 2017. URL http://arxiv.org/
abs/1708.07747.
9