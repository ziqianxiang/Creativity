Under review as a conference paper at ICLR 2020
On the decision boundaries of deep neural
networks: A tropical geometry perspective
Anonymous authors
Paper under double-blind review
Ab stract
This work tackles the problem of characterizing and understanding the decision
boundaries of neural networks with piecewise linear non-linearity activations. We
use tropical geometry, a new development in the area of algebraic geometry, to
provide a characterization of the decision boundaries of a simple neural network
of the form (Affine, ReLU, Affine). Specifically, we show that the decision bound-
aries are a subset of a tropical hypersurface, which is intimately related to a poly-
tope formed by the convex hull of two zonotopes. The generators of the zonotopes
are precise functions of the neural network parameters. We utilize this geometric
characterization to shed lights on new perspectives of three tasks. In doing so, we
propose a new tropical perspective for the lottery ticket hypothesis, where we see
the effect of different initializations on the tropical geometric representation of the
decision boundaries. Also, we leverage this characterization as a new set of trop-
ical regularizers, which deal directly with the decision boundaries of a network.
We investigate the use of these regularizers in neural network pruning (removing
network parameters that do not contribute to the tropical geometric representation
of the decision boundaries) and in generating adversarial input attacks (with input
perturbations explicitly perturbing the decision boundaries geometry to change
the network prediction of the input).
1	Introduction
Deep Neural Networks (DNNs) have demonstrated outstanding performance across several research
domains, including computer vision (Krizhevsky et al., 2012), speech recognition (Hinton et al.,
2012), natural language processing (Bahdanau et al., 2015; Devlin et al., 2018), quantum chemistry
(Schutt et al., 2017), and healthcare (ArdiIa et al., 2019; ZhoU et al., 2019) to name a few (Le-
Cun et al., 2015). Nevertheless, a rigorous interpretation of their success remains evasive (Shalev-
Shwartz & Ben-David, 2014). For instance, and in an attempt to uncover the expressive power
of DNNs, Montufar et al. (2014) studied the complexity of functions computable by DNNs that
have piecewise linear activations. They derived a lower bound on the maximum number of linear
regions. Several other works have followed to improve such estimates under certain assumptions
(Arora et al., 2018). In addition, and in attempt to understand some of the subtle behaviours DNNs
exhibit, e.g. the sensitive reaction of DNNs to small input perturbations, several works directly
investigated the decision boundaries induced by a DNN used for classification. The work of Seyed-
Mohsen Moosavi-Dezfooli (2019) showed that the smoothness of these decision boundaries and
their curvature can play a vital role in network robustness. Moreover, He et al. (2018a) studied the
expressiveness of these decision boundaries at perturbed inputs and showed that these boundaries
do not resemble the boundaries around benign inputs. Li et al. (2018) showed that under certain
assumptions, the decision boundaries of the last fully connected layer of DNNs will converge to a
linear SVM. Also, Beise et al. (2018) showed that the decision regions of DNNs with width smaller
than the input dimension are unbounded.
More recently, and due to the popularity of the piecewise linear ReLU as an activation function,
there has been a surge in the number of works that study this class of DNNs in particular. As a
result, this has incited significant interest in new mathematical tools that help analyze piecewise
linear functions, such as tropical geometry. While tropical geometry has shown its potential in
many applications such as dynamic programming (JosWig & Schroter, 2019), linear programming
(Allamigeon et al., 2015), multi-objective discrete optimization (Joswig & Loho, 2019), enumerative
geometry (Mikhalkin, 2004), economics (Akian et al., 2009; Mai Tran & Yu, 2015), it has only been
1
Under review as a conference paper at ICLR 2020
recently used to analyze DNNs. For instance, Zhang et al. (2018) showed an equivalency between
the family of DNNs with piecewise linear activations and integer weight matrices and the family
of tropical rational maps, i.e. ratio between two multi-variate polynomials in tropical algebra. The
work of Zhang et al. (2018) was mostly concerned about characterizing the complexity of a DNN
and specifically counting the number of linear regions, into which the function represented by the
DNN can divide the input space, by counting the number of vertices of some polytope representation.
This novel approach recovered the results of Montufar et al. (2014) with a much simpler analysis.
Contributions. In this paper, we take the results of Zhang et al. (2018) some steps further and
present a novel perspective on the decision boundaries of DNNs using tropical geometry. To that
end, our contributions are three-fold. (i) We derive a geometric representation (convex hull between
two zonotopes) for a super set to the decision boundaries of a DNN in the form (Affine, ReLU,
Affine). (ii) We demonstrate support for the lottery ticket hypothesis (Frankle & Carbin, 2019) using
a geometric perspective. (iii) We leverage the geometrical representation of the decision boundaries
(the decision boundaries polytope) in two interesting applications: network purning and adversarial
attacks. In regards to tropical pruning, we provide a new geometric perspective in which one can
directly compress the decision boundaries polytope efficiently resulting in only minor perturbations
to the decision boundaries. We conduct extensive experiments on AlexNet (Krizhevsky et al., 2012)
and VGG16 (Simonyan & Zisserman, 2014) on SVHN (Netzer et al., 2011), CIFAR10, and CI-
FAR 100 (Krizhevsky & Hinton, 2009) datasets, in which 90% pruning rate can be achieved with
a marginal drop in testing accuracy. As for tropical adversarial attack, we show that one can con-
struct input adversaries that can change network predictions by perturbing the decision boundaries
polytope. We conduct extensive experiments on MNIST (LeCun, 1998).
2	Preliminaries to Tropical Geometry
We provide here some preliminaries to tropical geometry. For a thorough detailed review, we refer
the reader to the work of Itenberg et al. (2009); Maclagan & Sturmfels (2015).
Definition 1. (Tropical Semiring1) The tropical semiring T is the triplet {R ∪{-∞},㊉，Θ}, where
㊉ and Θ define tropical addition and tropical multiplication, respectively. They are denoted as:
X ㊉ y = max{x, y},	X Θ y = X + y,	∀x, y ∈ T.
It can be readily shown that -∞ is the additive identity and 0 is the multiplicative identity.
Given the previous definition, a tropical power can be formulated as xθa = X Θ X …Θ X = a.x, for
X ∈ T, a ∈ N, where a.X is standard multiplication. Moreover, the tropical quotient can be defined
as: X y = X - y where X - y is the standard subtraction. For ease of notation, we write Xa as Xa.
Now, we are in a position to define tropical polynomials, their solution sets and tropical rationals.
Definition 2. (Tropical Polynomials) For x ∈ Td, ci ∈ R and ai ∈ Nd, a d-variable tropical
polynomial with n monomials. f : Td → Td can be expressed as:
f (x) = (ci Θ xa1)㊉(C2 Θ xa2)㊉•…㊉(Cn Θ Xan), ∀ a% = a, when i = j.
We use the more compact vector notation Xa = x^ Θ x2>2 …Θ Xad where x, a ∈ Rd. Moreover
and for ease of notation, we will denote ci Θ Xai as ciXai throughout the paper.
Definition 3. (Tropical Rational Functions) A tropical rational function is a standard difference or
equivalently, a tropical quotient of two tropical polynomials: f(X) - g(X) = f(X)	g(X).
Algebraic curves or hypersurfaces in algebraic geometry, which are the solution sets to polynomials,
can be analogously extended to tropical polynomials too.
Definition 4. (Tropical Hypersurfaces) A tropical hypersurface of a tropical polynomial f(X) =
cιxa1 ㊉•一㊉ CnXan is the set ofpoints X where f is attained by two or more monomials in f, i.e.
T(f) := {X ∈ Rd : ciXai = cj Xaj = f (X), for some ai 6= aj}.
Tropical hypersurfaces divide the domain of f into convex regions, where f is linear in each region.
Moreover, every tropical polynomial can be associated with a Newton polytope.
1A semiring is a ring that lacks an additive inverse.
2
Under review as a conference paper at ICLR 2020
Definition 5. (Newton Polytopes) The Newton polytope of a tropical polynomial f (x) = cιxa1 ㊉
•一㊉ CnXan is the convex hull Ofthe exponents ai ∈ Nd regarded as points in Rd, i.e.
∆(f) := ConvHull{ai ∈ Rd : i = 1, 2, . . . , n and ci 6= -∞}.
A tropical polynomial determines a dual subdivision, which can thus be constructed by projecting the
collection of upper faces (UF) in P(f) := ConvHull{(ai, ci) ∈ Rd × R : i = 1, . . . , n} to Rd. That
is to say, the dual subdivision determined by f is given as δ(f) := {π(p) ⊂ Rd : p ∈ UF(P (f))}
where π : Rd × R → Rd is the projection that drops the last coordinate. It has been shown
by Maclagan & Sturmfels (2015) that the tropical hypersurface T(f) is the (d-1)-skeleton of the
polyhedral complex dual to δ(f). So, each vertex of δ(f) corresponds to one region in Rd where f
is linear. Zhang et al. (2018) showed an equivalency between tropical rational maps and any neural
network f : Rn → Rk with piecewise linear activations and integer weights through the following
theorem.
Theorem 1.	(Tropical Characterization of Neural Networks, Zhang et al. (2018)). A feedforward
neural network with integer weights and real biases with piecewise linear activation functions is a
function f : Rn → Rk, whose coordinates are tropical rational functions of the input, i.e.,
f(X) = H(X)	Q(X) = H(X) - Q(X),
where H and Q are tropical polynomials.
While this result is new in the context of tropical geometry, it is not surprising, since any piecewise
linear function can be represented as a difference of two max functions over a set of hyperplanes
Melzer (1986). Mathematically, that is to say if f is a piecewise linear function, it can be written
as f(X) = maxi∈[m] {ai>X} - maxj∈[n] {bj>X}, where [m] = {1, . . . , m} and [n] = {1, . . . , n}.
Thus, it is clear that each of the two maxima above is a tropical polynomial recovering Theorem 1.
3	Decision B oundaries of Deep Neural Networks as Polytopes
In this section, we analyze the decision boundaries of a network in the form (Affine, ReLU,
Affine) using tropical geometry. For ease, we use ReLUs as the non-linear activation, but any
other piecewise linear function can also be used. The functional form of this network is: f(X) =
Bmax (AX + c1, 0) + c2, where max(.) is an element-wise operator. The outputs of the network f
are the logit scores. Throughout this section, we assume2 that A ∈ Zp×n , B ∈ Z2×p , c1 ∈ Rp and
c2 ∈ R2. For ease of notation, we only consider networks with two outputs, i.e. B2×p, where the
extension to a multi-class output follows naturally and it is discussed in the appendix. Now, since f
is a piecewise linear function, each output can be expressed as a tropical rational as per Theorem 1.
If f1 and f2 refer to the first and second outputs respectively, we have f1(X) = H1(X) Q1(X) and
f2(X) = H2(X) Q2(X), where H1, H2, Q1 and Q2 are tropical polynomials. In what follows and
for ease of presentation, we present our main results where the network f has no biases, i.e. c1 = 0
and c2 = 0, and we leave the generalization to the appendix.
Theorem 2.	For a bias-free neural network in the form off(X) : Rn → R2 where A ∈ Zp×n and
B ∈ Z2 ×p, let R(X) = HI(X) Θ Q2(x)㊉ H2(x) Θ Qι(x) be a tropical polynomial. Then:
•	Let B = {X ∈ Rn : f1(X) = f2(X)} defines the decision boundaries off, then B ⊆ T (R(X)).
•	δ (R(X))	= ConvHull (ZG1 , ZG2). ZG1 is a zonotope in Rn with line segments
{(B+(1,j)+B-(2,j))[A+(j,:),A-(j,:)]}jp=1 and shift (B-(1,:) + B+(2,:))A-. ZG2 is
a zonotope in Rn with line segments {(B- (1, j) + B+(2,j))[A+(j, :), A- (j, :)]}jp=1 and shift
(B+(1, :) + B-(2, :))A-. Note that A+ = max(A, 0) and A- = max(-A, 0). The line seg-
ment (B+(1, j) + B- (2, j))[A+ (j, :), A-(j, :)] has end points A+(j, :) and A-(j, :) in Rn and
scaled by (B+(1,j) + B-(2,j)).
The proof for Theorem 2 is left for the appendix.
Digesting Theorem 2. Theorem 2 can be broken into two major results. The first, which is on the
algebra side, i.e. finding the solution set to tropical polynomials, states that the decision boundaries
2Without loss of generality, as one can very well approximate real weights as fractions by multiplying by
least common multiple of the denominators as discussed in Zhang et al. (2018).
3
Under review as a conference paper at ICLR 2020
Figure 1: Decision Boundaries as Geometric Structures. The decision boundaries B (in red) comprise two
linear pieces separating classes C1 and C2. As per Theorem 2, the dual subdivision of this single hidden neural
network is the convex hull between the zonotopes ZG1 and ZG2. The normals to the dual subdivison δ(R(x))
are in one-to-one correspondence to the tropical hypersurface T (R(x)), which is a superset to the decision
boundaries B. Note that some of the normals to δ(R(x)) (in red) are parallel to the decision boundaries.
B is a subset of the tropical hypersurface of the tropical polynomial R(x), i.e. T (R(x)). The
second result, which is on the geometry side, of Theorem 2 relates the tropical polynomial R(x)
to the geometric representation of the solution set to R(x), i.e. T (R(x)), referred to as the dual
subdivision, i.e. δ(R(x)). In particular, Theorem 2 states that the dual subdivision for a network f
is the convex hull of two zonotopes denoted as ZG1 and ZG2 . Note that this dual subdivision is a
function of only the network parameters A and B.
Theorem 2 bridges the gap between the behaviour of the decision boundaries B, through the super-set
T (R(x)), and the polytope δ (R(x)), which is the convex hull of two zonotopes. It is worthwhile
to mention that Zhang et al. (2018) discussed a special case of the first part of Theorem 2 for a
neural network with a single output and a score function s(x) to classify the output. To the best
of our knowledge, this work is the first to propose a tropical geometric formulation of a super-set
containing the decision boundaries of a multi-class classification neural network. In particular, the
first result of Theorem 2 states that one can alter the network, e.g. by pruning network parameters,
while preserving the decision boundaries B, if one preserves the tropical hypersurface of R(x)
or T (R(x)). While preserving the tropical hypersurfaces can be equally difficult to preserving
the decision boundaries directly, the second result of Theorem 2 comes in handy. For a bias free
network, π becomes an identity mapping with δ(R(x)) = ∆(R(x)), and thus the dual subdivision
δ(R(x)), which is the Newton polytope ∆(R(x)) in this case, becomes a well structured geometric
object that can be exploited to preserve decision boundaries. Since Maclagan & Sturmfels (2015)
(Proposition 3.1.6) showed that the tropical hypersurface is the skeleton of the dual to δ(R(x)), the
normal lines to the edges of the polytope δ(R(x)) are in one-to-one correspondence with the tropical
hypersurface T (R(x)). Figure 1 details this intimate relation between the decision boundaries,
tropical hypersurface T (R(x)), and normals to δ (R(x)). Before any further discussion, we recap
the definition of zonotopes.
Definition 6. Let u1, . . . , up ∈ Rn. The zonotope formed by u1, . . . , up is defined as
Z(u1 , . . . , up) := { ip=1 xiui : 0 ≤ xi ≤ 1}. Equivalently, the zonotope can be expressed with
respect to the generator matrix U ∈ Rp×n, where U(i, :) = ui> as ZU := {U>x : ∀x ∈ [0, 1]p}.
Another common definition for zonotopes is the Minkowski sum (refer to appendix A for the def-
inition of the Minkowski sum) of a set of line segments that start from the origin with end points
u1 , . . . , up ∈ Rn . It is also well known that the number of vertices of a zonotope is polynomial in
the number of line segments. That is to say, | vert (ZU) | ≤ 2 Pn-I (P-1) = O (pn-1) (Gritzmann
& Sturmfels, 1993).
While Theorem 2 presents a strong relation between a polytope (convex hull of two zonotopes)
and the decision boundaries, it remains unclear how such a polytope can be efficiently constructed.
Although the number of vertices of a zonotope is polynomial in the number of its generating line
segments, fast algorithms for enumerating these vertices are still restricted to zonotopes with line
segments starting at the origin (Stinson et al., 2016). Since the line segments generating the zono-
topes in Theorem 2 have arbitrary end points, we present the next result that transforms these line
segments into a generator matrix of line segments starting from the origin, as prescribed in Definition
6. This result is essential for the efficient computation of the zonotopes in Theorem 2.
Proposition 1. Consider p line segments in Rn with two arbitrary end points as follows
{[ui1, ui2]}ip=1. The zonotope formed by these line segments is equivalent to the zonotope formed
by the line segments {[ui1 - ui2 , 0]}ip=1 with a shift of ip=1 ui2.
4
Under review as a conference paper at ICLR 2020
Figure 2: Effect of Different Initializations on the Decision Boundaries Polytope. From left to right:
training dataset, decision boundaries polytope of original network followed by the decision boundaries polytope
during several iterations of pruning with different initializations.
The proof is left for the appendix. As per Proposition 1, the generator matrices of zonotopes
ZG1 , ZG2 in Theorem 2 can be defined as G1 = Diag[(B+(1, :)) + (B- (2, :))]A and G2 =
Diag[(B+(2,:)) + (B-(1,:))]A, both with shift (B-(1,:) + B+(2,:) +B+(1,:) +B-(2,:))A-.
In what follows, we show several applications for Theorem 2. We begin by leveraging the geometric
structure to help in reaffirming the behaviour of the lottery ticket hypothesis.
4	Tropical View to the Lottery Ticket Hypothesis
The lottery ticket hypothesis was recently proposed by Frankle & Carbin (2019), in which the au-
thors surmise the existence of sparse trainable sub-networks of dense, randomly-initialized, feed-
forward networks that—when trained in isolation—perform as well as the original network in a
similar number of iterations. To find such sub-networks, Frankle & Carbin (2019) propose the fol-
lowing simple algorithm: perform standard network pruning, initialize the pruned network with the
same initialization that was used in the original training setting, and train with the same number of
epochs. They hypothesize that this should result in a smaller network with a similar accuracy to
the larger dense network. In other words, a subnetwork can have similar decision boundaries to the
original network. While in this section we do not provide a theoretical reason for why this proposed
pruning algorithm performs favorably, we utilize the geometric structure that arises from Theorem
2 to reaffirm such behaviour. In particular, we show that the orientation of the decision boundaries
polytope δ(R(x)), known to be a superset to the decision boundaries T (R(x)), is preserved after
pruning with the proposed initialization algorithm of Frankle & Carbin (2019). On the other hand,
pruning routines with a different initialization at each pruning iteration will result in a severe vari-
ation in the orientation of the decision boundaries polytope. This leads to a large change in the
orientation of the decision boundaries, which tends to hinder accuracy.
To this end, we train a neural network with 2 inputs (n = 2), 2 outputs, and a single hidden layer
with 40 nodes (p = 40). We then prune the network by removing the smallest x% of the weights.
The pruned network is then trained using different initializations: (i) the same initialization as the
original network (Frankle & Carbin, 2019), (ii) Xavier (Glorot & Bengio, 2010), (iii) standard Gaus-
sian and (iv) zero mean Gaussian with variance of 0.1. Figure 2 shows the evolution of the decision
boundaries polytope, i.e. δ(R(x)), as we perform more pruning (increasing the x%) with different
initializations. It is to be observed that the orientation of the polytopes δ(R(x)) vary much more for
all different initialization schemes as compared to the lottery ticket initialization. This gives an indi-
cation that lottery ticket initialization indeed preserves the decision boundaries throughout the evo-
lution of pruning. Another approach to investigate the lottery ticket could be by observing the poly-
topes representing the functional form of the network directly, i.e. δ(H{1,2} (x)) and δ(Q{1,2} (x)),
in lieu of the decision boundaries polytopes. However, this does not provide conclusive answers
to the lottery ticket, since there can exist multiple functional forms, and correspondingly multiple
polytopes δ(H{1,2}(x)) and δ(Q{1,2}(x)), for networks with the same decision boundaries. This is
why we explicitly focus our analysis on δ(R(x)), which is directly related to the decision boundaries
of the network. Further discussions and experiments are left for the appendix.
5	Tropical Network Pruning
Network pruning has been identified as an effective approach for reducing the computational cost
and memory usage during network inference time. While pruning dates back to the work of LeCun
et al. (1990) and Hassibi & Stork (1993), it has recently gained more attention. This is due to the fact
that most neural networks over-parameterize commonly used datasets. In network pruning, the task
is to find a smaller subset of the network parameters, such that the resulting smaller network has sim-
ilar decision boundaries (and thus supposedly similar accuracy) to the original over-parameterized
network. In this section, we show a new geometric approach towards network pruning. In particu-
5
Under review as a conference paper at ICLR 2020
8(JR(X)) = ConVeXHUu(2g-2g2)6(A(X)) = ConVeXHU11(22，2品)
Figure 3: Tropical Pruning Pipeline. Pruning the 4th node, or equivalently removing the two yellow vertices
of zonotope ZG2 does not affect the decision boundaries polytope which will not lead to any change in accuracy.
lar, as indicated by Theorem 2, preserving the polytope δ(R(x)) preserves a superset to the decision
boundaries T (R(x)), and thus supposedly the decision boundaries themselves.
Motivational Insight. Fora single hidden layer neural network, the dual subdivision to the decision
boundaries is the polytope that is the convex hull of two zonotopes, where each is formed by taking
the Minkowski sum of line segments (Theorem 2). Figure 3 shows an example where pruning a
neuron in the neural network has no effect on the dual subdivision polytope and equivalently no
effect on the accuracy, since the decision boundaries of both networks remain the same.
Problem Formulation. Given the motivational insight, a natural question arises: Given an over-
parameterized binary neural network f(x) = B max (Ax, 0), can one construct a new neural
network, parameterized by some sparser weight matrices A and B, such that this smaller network
has a dual subdivision δ(R(x)) that preserves the decision boundaries of the original network?
In order to address this question, we propose the following general optimization problem
mm in d(δ (R(X)),δ (R(X))) = mm in d (ConVHun (ZG ι, ZG 2) , ConVHun (Zgi , ZG2)
(1)
The function d(.) defines a distance between two geometric objects. Since the generators G 1 and
G2 are functions of A and B (as per Theorem 2), this optimization problem can be challenging to
solve. However, for pruning purposes, one can observe from Theorem 2 that if the generators G1
and GG2 had fewer number of line segments (rows), this corresponds to a fewer number of rows in
the weight matrix A (sparser weights). To this end, we observe that if G1 ≈ G1 and G2 ≈ G2 ,
then δ(R(X)) ≈ δ(R(X)), and thus the decision boundaries tend to be preserved as a consequence.
Therefore, we propose the following optimization problem as a surrogate to Problem (1)
minA ,B1 (份1-G1ii+a2 -G2Ii)+λ1 a$2,1+λ2 2 I』.
(2)
The matrix mixed norm for C ∈ Rn×k is defined as kCk2,1 = Pin=1 kC(i, :)k2, which encourages
the matrix C to be row sparse, i.e. complete rows of C are zero. Note that G1 = Diag[ReLU(B(1, :
、、	，二，	、、r ' 士	L	，—	，、、	， U ，	、、r Y	，、
))+ ReLU(-B(2, :))]A, G2 = Diag[ReLU(B(2,:))+ ReLU(-B(1,:))]A, andDiag(v) rearranges
the elements of vector v in a diagonal matrix. We solve the aforementioned problem with alternating
optimization over the variables A and B, where each sub-problem is solved in closed form. Details
of the optimization and the extension to multi-class case are left for the appendix.
Extension to Deeper Networks. For deeper networks, one can still apply the aforementioned op-
timization for consecutive blocks. In particular, we prune each consecutive block of the form
(Affine,ReLU,Affine) starting from the input and ending at the output of the network.
Experiments on Tropical Pruning. Here, we evaluate the performance of the proposed pruning
approach as compared to several classical approaches on several architectures and datasets. In par-
ticular, we compare our tropical pruning approach against Class Blind (CB), Class Uniform (CU)
and Class Distribution (CD) Han et al. (2015); See et al. (2016). In Class Blind, all the parameters
across all nodes of a layer are sorted by magnitude where x% with smallest magnitudes are pruned.
Similar to Class Blind, Class Uniform prunes the parameters with smallest x% magnitudes per node
in a layer as opposed to sorting all parameters in all nodes as in Class Blind. Lastly, Class Distri-
bution performs pruning of all parameters for each node in the layer, just as in Class Uniform, but
the parameters are pruned based on the standard deviation σc of the magnitude of the parameters
per node. Since fully connected layers in deep neural networks tend to have much higher memory
complexity than convolutional layers, we restrict our focus to pruning fully connected layers. We
6
Under review as a conference paper at ICLR 2020
CD - AUC = 0.843
——CB - AUC = 0.837
40	— CU- AUC = 0.852
20
Pruning Rate %
Pruning Rate %
Pruning Rate %
CB - AUC = 0.63
CU - AUC = 0.61
CD - AUC = 0.606
CB
CU
Ours - AUC = 0.885
Ours - AUC = 0.668
10 ——CD - AUC = 0.338
——Ours - AUC = 0.375
Figure 4: Results of Tropical Pruning. Pruning-accuracy plots for AlexNet (top) and VGG16 (bottom)
trained on SVHN, CIFAR10, and CIFAR100, pruned with our tropical method and three other pruning methods.
train AlexNet and VGG16 on SVHN , CIFAR10, and CIFAR 100 datasets. We observe that we can
prune more than 90% of the classifier parameters for both networks without affecting the accuracy.
Moreover, we can boost the pruning ratio using our method without affecting the accuracy by simply
retraining the network biases only.
Setup. We adapt the architectures of AlexNet and VGG16, since they were originally trained on Ima-
geNet (Deng et al., 2009), to account for the discrepancy in the input resolution. The fully connected
layers of AlexNet and VGG16 have sizes of (256,512,10) and (512,512,10), respectively on SVHN
and CIFAR100 with the last layer replaced to 100 for CIFAR100. All networks were trained to
baseline test accuracy of (92%,74%,43%) for AlexNet on SVHN, CIFAR10 and CIFAR100, respec-
tively and (92%,92%,70%) for VGG16. To evaluate the performance of pruning, following previous
works (Han et al., 2015), we report the area under the curve (AUC) of the pruning-accuracy plot.
The higher the AUC is, the better the trade-off is between pruning rate and accuracy. For efficiency
purposes, we run the optimization in Problem (2) for a single alternating iteration to identify the
rows in A and elements of B that will be pruned, since an exact pruning solution might not be
necessary. The algorithm and the parameters setup to solving (2) is left for the appendix.
Results. Figure 4 shows the pruning comparison between our tropical approach and the three afore-
mentioned popular pruning schemes on both AlexNet and VGG16 over the different datasets. Our
proposed approach can indeed prune out as much as 90% of the parameters of the classifier without
sacrificing much of the accuracy. For AlexNet, we achieve much better performance in pruning as
compared to other methods. In particular, we are better in AUC by 3%, 3%, and 2% over other
pruning methods on SVHN, CIFAR10 and CIFAR100, respectively. This indicates that the decision
boundaries can indeed be preserved by preserving the dual subdivision polytope. For VGG16, we
perform similarly well on both SVHN and CIFAR10 and slightly worse on CIFAR100. While the
performance achieved here is comparable to the other pruning schemes, if not better, we emphasize
that our contribution does not lie in outperforming state-of-the-art pruning methods, but rather in
giving a new geometry based perspective to network pruning. We conduct more experiments, where
only the biases of the network or the biases of the classifier are fine tuned after pruning . Retrain-
ing biases can be sufficient as they do not contribute to the orientation of the decision boundaries
polytope, thereafter the decision boundaries, but only a translation. Discussion on biases and more
results are left for the appendix.
6	Tropical Adversarial Attacks
DNNs are notoriously known to be susceptible to adversarial attacks. In fact, adding small im-
perceptible noise, referred to as adversarial attacks, at the input of these networks can hinder their
performance. Several works investigated the decision boundaries of neural networks in the presence
of adversarial attacks. For instance, Khoury & Hadfield-Menell (2018) analyzed high dimensional
geometry of adversarial examples by the means of manifold reconstruction. Also, He et al. (2018b)
crafted adversarial attacks by estimating the distance to the decision boundaries using random search
directions. In this work, we provide a tropical geometric view to this problem. where we show how
Theorem 2 can be leveraged to construct a tropical geometric based targeted adversarial attack.
7
Under review as a conference paper at ICLR 2020
Figure 5: Dual View of Tropical Adversarial Attacks. We show the effects of tropical adversarial attacks on
a synthetic binary dataset at two different input points (in black). From left to right: the decision regions of the
original and perturbed models, and decision boundaries polytopes (green for original and blue for perturbed).
Dual View to Adversarial Attacks. For a classifier f : Rn → Rk and input x0 that is classified as
c, a standard formulation for targeted adversarial attacks flips the classifier prediction to a particular
class t and it is usually defined as follows
minη D(η) s.t. arg maxi fi(x0 + η) = t 6= c.
(3)
This objective aims at computing the lowest energy input noise η (measured by D) such that the the
new sample (x0 + η) crosses the decision boundaries of f to a new classification region. Here, we
present a dual view to adversarial attacks. Instead of designing a sample noise η such that (x0 + η)
belongs to anew decision region, one can instead fix x0 and perturb the network parameters to move
the decision boundaries in a way that x0 appears in a new classification region. In particular, let A1
be the first linear layer of f, such that f(x0) = g(A1x0). One can now perturb A1 to alter the
decision boundaries and relate the perturbation to the input perturbation as follows
g((A1 +ξA1)x0)	= g (A1x0 +ξA1x0)	= g(A1x0	+A1η) =	f(x0	+η).	(4)
From this dual view, we observe that traditional adversarial attacks are intimately related to per-
turbing the parameters of the first linear layer through the linear system: A1η = ξA1x0. To this
end, Theorem 2 provides explicit means to geometrically construct adversarial attacks by means of
perturbing decision boundaries. In particular, since the normals to the dual subdivision polytope
δ(R(x)) of a given neural network represent the tropical hypersurface set T (R(x)) which is, as per
Theorem 2, a superset to the decision boundaries set B, ξA1 can be designed to result in a minimal
perturbation to the dual subdivision that is sufficient to change the network prediction of x0 to the
targeted class t. Based on this observation, we formulate the problem as follows
min	D1 (η) +D2(ξA1)
η,ξA1
s.t. - loss(g(A1(x0 + η)), t) ≤ -1;	-loss(g(A1 +ξA1)x0,t) ≤ -1;	(5)
(x0 +η)	∈	[0, 1]n,	kηk∞ ≤ 1;	kξA1	k∞,∞	≤ 2;	A1η	- ξA1 x0	= 0.
The loss is the standard cross-entropy loss. The first row of constraints ensures that the network
prediction is the desired target class t when the input x0 is perturbed by η, and equivalently by
perturbing the first linear layer A1 by ξA1 . This is identical to f1 as proposed by Carlini & Wagner
(2016). Moreover, the third and fourth constraints guarantee that the perturbed input is feasible
and that the perturbation is bounded, respectively. The fifth constraint is to limit the maximum
perturbation on the first linear layer, while the last constraint enforces the dual equivalence between
input perturbation and parameter perturbation. The function D2 captures the perturbation of the
dual subdivision polytope upon perturbing the first linear layer by ξA1 . For a single hidden layer
neural network parameterized as (A1 + ξA1 ) ∈ Rp×n and B ∈ R2×p for the 1st and 2nd layers
respectively, D2 can capture the perturbations in each of the two zonotopes discussed in Theorem 2.
D2(ξA1) = 1 P2=1 ∣∣Diag(B+ (-/IF + IlDiag(B-(j"))ξAι ∣∣F .	⑹
The derivation, discussion, and extension of (6) to multi-class neural networks is left for the ap-
pendix. We solve Problem (5) with a penalty method on the linear equality constraints, A1 η =
ξA1x0, where each penalty step is solved with ADMM (Boyd et al., 2011) in a similar fashion to the
work of Xu et al. (2018). The details of the algorithm are left for the appendix.
Motivational Insight to the Dual View. This intuition is presented in Figure 5. We train a single
hidden layer neural network where the size of the input is 2 with 50 hidden nodes and 2 outputs
on a simple dataset as shown in Figure 5. We then solve Problem 5 for a given x0 shown in black.
We show the decision boundaries for the network with and without the perturbation at the first lin-
ear layer ξA1 . Figure 5 shows that indeed perturbing an edge of the dual subdivision polytope, by
perturbing the first linear layer, corresponds to perturbing the decision boundaries and results in
miss-classifying x0 . Interestingly and as expected, perturbing different decision boundaries corre-
sponds to perturbing different edges of the dual subdivision. In particular, one can see from Figure 5
8
Under review as a conference paper at ICLR 2020
Figure 6: Effect of Tropical Adversarial Attacks on MNIST Dataset. We show qualitative examples of
adversarial attacks, produced by solving Problem (5), on two digits (8,9) from MNIST. From left to right,
images are classified as [8,7,5,4] and [9,7,5,4] respectively.
that altering the decision boundaries, by altering the dual subdivision polytope through perturbations
in the first linear layer, can result in miss-classifying a previously correctly classified input x0 .
MNIST Experiment. Here, we design perturbations to misclassify MNIST images. Figure 7 shows
several adversarial examples that change the network prediction for digits 8 and 9 to digits 7, 5, and
4, respectively. In some cases, the perturbation η is as small as = 0.1, where x0 ∈ [0, 1]n. Several
other adversarial results are left for the appendix. We again emphasize that our approach is not
meant to be compared with (or beat) state of the art adversarial attacks, but rather to provide a novel
geometrically inspired perspective that can shed new light in this field.
7 Conclusion
In this paper, we leverage tropical geometry to characterize the decision boundaries of neural net-
works in the form (Affine, ReLU, Affine) and relate it to well-studied geometric objects such as
zonotopes and polytopes. We leaverage this representation in providing a tropical perspective to
support the lottery ticket hypothesis, network pruning and designing adversarial attacks. One natu-
ral extension for this work is a compact derivation for the characterization of the decision boundaries
of convolutional neural networks (CNNs) and graphical convolutional networks (GCNs).
References
Marianne Akian, Stphane Gaubert, and Alexander Guterman. Tropical polyhedra are equivalent to
mean payoff games. International Journal of Algebra and Computation, 2009.
Xavier Allamigeon, Pascal Benchimol, Stphane Gaubert, and Michael Joswig. Tropicalizing the
simplex algorithm. SIAM J. Discrete Math. 29:2, 2015.
Diego Ardila, Atilla P. Kiraly, Sujeeth Bharadwaj, Bokyung Choi, Joshua J. Reicher, Lily Peng,
Daniel Tse, Mozziyar Etemadi, Wenxing Ye, Greg Corrado, David P. Naidich, and Shravya Shetty.
End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest com-
puted tomography. Nature Medicine, 2019.
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural
networks with rectified linear units. In International Conference on Learning Representations,
2018.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In International Conference on Learning Representations, 2015.
Hans-Peter Beise, Steve Dias Da Cruz, and Udo Schroder. On decision regions of narrow deep
neural networks. arXiv preprint arXiv:1807.01194, 2018.
Leonard Berrada, Andrew Zisserman, and M Pawan Kumar. Trusting svm for piecewise linear cnns.
arXiv preprint arXiv:1611.02185, 2016.
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed optimiza-
tion and statistical learning via the alternating direction method of multipliers. Foundations and
TrendsR in Machine learning, 2011.
Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks.
CoRR, 2016.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Computer Vision and Patter Recognition Conference (CVPR),
2009.
9
Under review as a conference paper at ICLR 2020
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In ICLR. OpenReview.net, 2019.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of 13th International Conference on Artificial Intelligence and Statis-
tics, 2010.
Peter Gritzmann and Bernd Sturmfels. Minkowski addition of polytopes: computational complexity
and applications to grθbner bases. SIAM Journal on Discrete Mathematics, 1993.
Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for
efficient neural networks. CoRR, 2015.
Babak Hassibi and David G Stork. Second order derivatives for network pruning: Optimal brain
surgeon. In Advances in neural information processing systems, 1993.
Warren He, Bo Li, and Dawn Song. Decision boundary analysis of adversarial examples. In Inter-
national Conference on Learning Representations (ICLR), 2018a.
Warren He, Bo Li, and Dawn Song. Decision boundary analysis of adversarial examples. In In-
ternational Conference on Learning Representations, 2018b. URL https://openreview.
net/forum?id=BkpiPMbA-.
G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. N. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recogni-
tion: The shared views of four research groups. IEEE Signal Processing Magazine, 2012.
Ilia Itenberg, Grigory Mikhalkin, and Eugenii I Shustin. Tropical algebraic geometry. Springer
Science & Business Media, 2009.
Michael Joswig and Georg Loho. Monomial tropical cones for multicriteria optimization. AIP
Conference Proceedings, 2019.
Michael JosWig and Benjamin Schroter. The tropical geometry of shortest paths. arXiv preprint
arXiv:1904.01082, 2019.
Marc Khoury and Dylan Hadfield-Menell. On the geometry of adversarial examples. arXiv preprint
arXiv:1811.00525, 2018.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification With deep con-
volutional neural netWorks. In Advances in Neural Information Processing Systems (NeurIPS),
2012.
Yann LeCun. The mnist database of handWritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural
information processing systems, 1990.
Yann LeCun, Y Bengio, and Geoffrey Hinton. Deep learning. Nature, 2015.
Yu Li, Peter Richtarik, Lizhong Ding, and Xin Gao. On the decision boundary of deep neural
netWorks. arXiv preprint arXiv:1808.05385, 2018.
Qinghua Liu, Xinyue Shen, and Yuantao Gu. Linearized admm for nonconvex nonsmooth optimiza-
tion With convergence analysis. IEEE Access, 2019.
D. Maclagan and B. Sturmfels. Introduction to Tropical Geometry. Graduate Studies in Mathemat-
ics. American Mathematical Society, 2015.
Ngoc Mai Tran and Josephine Yu. Product-mix auctions and tropical geometry. Mathematics of
Operations Research, 2015.
10
Under review as a conference paper at ICLR 2020
D Melzer. On the expressibility of piecewise-linear continuous functions as the difference of two
piecewise-linear convex functions. In Quasidifferential Calculus. Springer, 1986.
Grigory Mikhalkin. Enumerative tropical algebraic geometry in r2. Journal of the American Math-
ematical Society, 2004.
Guido Montufar, Razvan Pascanu, Kyunghyun Cho, and Y Bengio. On the number of linear regions
of deep neural networks. Advances in Neural Information Processing Systems (NeurIPS), 2014.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. Advances in Neural Information
Processing Systems (NeurIPS), 2011.
Kristof Schutt, Farhad Arbabzadah, Stefan Chmiela, Klaus-Robert Muller, and Alexandre
Tkatchenko. Quantum-chemical insights from deep tensor neural networks. Nature Commu-
nications, 2017.
Abigail See, Minh-Thang Luong, and Christopher D. Manning. Compression of neural machine
translation models via pruning. CoRR, 2016.
Jonathan Uesato Pascal Frossard Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi. Robustness
via curvature regularization, and vice versa. CVPR, 2019.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Kerrek Stinson, David F Gleich, and Paul G Constantine. A randomized algorithm for enumerating
zonotope vertices. arXiv preprint arXiv:1602.06620, 2016.
Kaidi Xu, Sijia Liu, Pu Zhao, Pin-Yu Chen, Huan Zhang, Deniz Erdogmus, Yanzhi Wang, and Xue
Lin. Structured adversarial attack: Towards general implementation and better interpretability.
arXiv preprint arXiv:1808.01664, 2018.
Liwen Zhang, Gregory Naitzat, and Lek-Heng Lim. Tropical geometry of deep neural networks. In
International Conference on Machine Learning, ICML 2018, 2018.
Jian Zhou, Christopher Y. Park, Chandra L. Theesfeld, Aaron Wong, Yuan Yuan, Claudia Scheckel,
John Fak, Julien Funk, Kevin Yao, Yoko Tajima, Alan Packer, Robert Darnell, and Olga G. Troy-
anskaya. Whole-genome deep-learning analysis identifies contribution of noncoding mutations to
autism risk. Nature Genetics, 2019.
11
Under review as a conference paper at ICLR 2020
A Preliminaries and Definitions.
Fact 1. P+Q = {p + q, ∀p ∈ P and q ∈ Q} is the Minkowski sum between two sets P and Q.
Fact 2. Let f be a tropical polynomial and let a ∈ N. Then
P(fa) = aP(f).
Let both f and g be tropical polynomials, Then
Fact 3.
P (f Q g) = P (f)+P (g).	(7)
Fact 4.
P(f ㊉ g) = ConvexHull(V (P(g)) ∪ V (P(g))).	(8)
Note thatV (P(f)) is the set of vertices of the polytope P(f).
12
Under review as a conference paper at ICLR 2020
B Proof Of Theorem 2
Theorem 3. For a bias-free neural network in the form of f(x) : Rn → R2 where A ∈ Zp×n and
B ∈ Z2 ×p, and let R(X) = Hι(x) Θ Q2(x)㊉ H2(x) Θ Qι(x) be a tropical polynomial, then
•	If the decision boundaries of f is given by the set B = {x ∈ Rn : f1(x) = f2(x)}, then
we have B ⊆ T (R(X)).
•	δ (R(X))	= ConvHull (ZG1 , ZG2) where ZG1 is a zonotope in Rn with
line segments {(B(1,j)+ + B(2,j)-)[A(j, :)+, A(j, :)-]jp=1} with shift
(B(1, :)- + B(2, :)+ )A- while ZG2 is a zonotope in Rn with line segments
{(B(1, j)- + B(2, j)+)[A(j,:)+, A(j,:)-]jp=1} with shift (B(1,:)+ + B(2,:)-)A-.
Note that A+ = max(A, 0) and A- = max(-A, 0) where the max(.) is element-wise. The
line segment (B(1, j)+ + B(2,j)-)[A(j, :)+, A(j, :)-] is one that has the end points A(j, :)+ and
A(j, :)- in Rn and scaled by the constant B(1, j)+ + B(2,j)-.
Proof. For the first part, recall from Theorem1 that both f1 and f2 are tropical rationals and hence,
f1(X) = H1(X) - Q1(X)	f2(X) = H2(X) - Q2 (X)
Thus
B = {x ∈ Rn	:	f1(X) = f2(X)}	=	{x ∈ Rn : H1(X)	-	Q1(X) =	H2(X)	- Q2(X)}
= {x ∈ Rn	:	H1(X) + Q2(X)	=	H2(X) + Q1(X)}
= {x ∈ Rn	:	H1(X) Θ Q2(X)	=	H2(X) Θ Q1(X)}
Recall that the tropical hypersurface is defined as the set of X where the maximum is attained by
two or more monomials. Therefore, the tropical hypersurface of R(X) is the set of X where the
maximum is attained by two or more monomials in (H1 (X) Θ Q2 (X)), or attained by two or more
monomials in (H2(X) Θ Q1(X)), or attained by monomials in both of them in the same time, which
is the decision boundaries. Hence, we can rewrite that as
T (R(X)) = T (H1(X) Θ Q2(X)) ∪ T (H2(X) Θ Q1(X)) ∪ B.
Therefore B ⊆ T (R(x)). For the second part of the Theorem, we first use the decomposi-
tion proposed by Zhang et al. (2018); Berrada et al. (2016) to show that for a network f(X) =
B max (AX, 0), it can be decomposed as tropical rational as follows
f(x) = (B+ — B-) (max(A+x, A-X) — A-X)
= B+ max(A+X, A-X) + B-A-X - B- max(A+X, A-X) + B+A-X .
Therefore, we have that
H1(X) + Q2(X)	=	B+(1, :) + B-(2, :)	max(A+X, A-X)	+ B-(1,	:) + B+(2, :) A-X,
H2(X) + Q1(X)	=	(B-(1, :) + B+(2, :))	max(A+X, A-X)	+ (B+(1,	:) + B-(2, :))A-X.
Therefore note that
δ(R(X))
㊉
H2(X) Θ Q1(X)
(=8) ConvexHull δ(H1(X) Θ Q2(X)), δ(H2(X) Θ Q1(X))
=ConvexHull ( δ(Hi (x)) + δ(Q2 (x)), δ(H (x)) + δ (QI(X))
13
Under review as a conference paper at ICLR 2020
Now observe that H1 (x) = Pjp=1
B+(1,j)+B-(2,j) max A+(j, :),A-(j, :)x
tropically is
given as follows Hι(x) = Θp=ι hxA+(j,:) ㊉ XA (j,[	(，")’(")
thus we have that
δ(Hι(x)) =(B+ (1,1) + B-(2,1)) δ (XA+(1，:)㊉ XA-(I,：)) + ...
+ (b+(1,p) + B-(2,p))(δ(XA+(Pe ㊉ XA-(P，:)))
=(B+ (1,1) + B-(2,1)) ConvexHull (a+(1, :), A-(1,:))+ ...
+(B+(1,p) + B-(2,p))ConvexHUll(A+(p,:), A-(p,:)).
The operator + indicates a Minkowski sum between sets. Note that ConvexHull(A+(i,:), A-(i,:
) is the convexhull between two points which is a line segment in Zn with end points that are
{A+(i, :), A+(i, :)} scaled with B+(1, i) + B-(2, i). Observe that δ(F1(X)) is a Minkowski sum
of line segments which is is a zonotope. Moreover, note that Q2(X) = (B- (1, :) + B+ (2, :))A-X
tropically is given as follows Qz(x) = Θp=ιXA (j,:)(B (1,j)0B (2,j)). ThUS it is easy to see that
δ(Q2(X)) is theMinkowski sum of the points {(B-(1,j) 一 B+(2,j))A-(j, :)}Vjin Rn (which is a
standard sum) resulting in a point. Lastly, it is easy to see that 6(Hi(x))+δ(Q2(X)) is a Minkowski
sum between a zonotope and a single point which corresponds to a shifted zonotope. A similar
symmetric argument can be applied for the second part δ(H2 (x))+ 6(Qi(x)).	□
It is also worthy to mention that the extension to network with multi class output is trivial. In that
case all of the analysis can be exactly applied studying the decision boundary between any two
classes (i, j) where B = {x ∈ Rn : fi(X) = fj (X)} and the rest of the proof will be exactly the
same.
14
Under review as a conference paper at ICLR 2020
C Derivation with Biases
In this section, we derive the statement of Theorem 2 for the neural network in the form of (Affine,
ReLU, Affine) with the consideration of non-zero biases. We show that the presence of biases does
not affect the obtained results as they only increase the dimension of the space, where the polytopes
live, without affecting their shape or edge-orientation. Starting with the first linear layer for x ∈ Rn,
we have
z1 = Ax + c1 = A+x + c1 - A-x = H1	Q1,
with coordinates
z1i =	A+(i,	:)x+	c1i	- A-(i, :)x = (c1i	xA+(i,:))	xA-(i,:)	=	H1i	Q1i.
Thus, ∆(H1i) is a point in (n+1) dimensions at (A+(i, :), c1i), and ∆(Q1i) is a point in (n + 1)
dimensions at (A-(i, :), 0), while under π projection, δ(H1i) is a point in n dimensions at (A+(i, :
)), and δ(Q1i) is a point in n dimensions at (A-(i, :)) . It can be seen that under projection π, the
geometrical representation of the output of the first linear layer does not change after adding biases.
Looking to the output after adding the ReLU layer, we get
Z2 = max(zι, 0) = max(A+x + ci, A-X) — A-X = (Hi ㊉ Q1) - Q1 = H2	Q2.
Hence, ∆(H2i) is the line segment [(A+(i, :), cii) , (A-(i, :), 0)], and ∆(Qii) is the point (A-(i, :
), 0). Thus, δ(H2i) is the line segment [(A+(i, :)) , (A-(i, :))], and δ(Qii) is the point (A-(i, :)).
Again, the biases does not affect the geometry of the output after the ReLU layer, since the line
segments now are connecting points in (n + 1) dimensions, but after projecting them using π, they
will be identical to the line segments of the network with zero biases.
Finally, looking to the output of the second linear layer, we obtain
z3 =Bz2+c2 = (B+ —B-)(H2 —Q2)+c2
=(B+H2+B-Q2+c2)—(B-H2+B+Q2)
= H3	Q3
Therefore
∆(H3i) = + j(∆(B(i,j)H2(j, ：)))+△(£B-(i,j)Q2(j,:)5)
j
δ(H3i) = + j (δ(B(i,j)H2(j, :)))+δ(X B-(i,j)Q2(j,:))
j
Similar arguments can be given for △(Q3i ) and δ(Q3i ). It can be seen that the first part in both
expressions is a Minkowski sum of line segments, which will give a zonotope in (n + 1), and n
dimensions in the first and second expressions respectively. While the second part in both expres-
sions is a Minkowski sum of bunch of points which gives a single point in (n+ 1) and n dimensions
for the first and second expression respectively. Note that the last dimension of the aforementioned
point in n + 1 dimensions is exactly the ith coordinate of the bias of the second linear layer which
is dropped under the π projection. Therefore, the shape of the geometrical representation of the
decision boundaries with non-zero biases will not be affected under the projection π, and hence the
presence of the biases will not affect any of the results of the paper.
15
Under review as a conference paper at ICLR 2020
D Proof of Proposition 1
Proposition 1. Consider p line segments in Rn with two arbitrary end points as follows
{[ui1, ui2]}ip=1. The zonotope formed by these line segments is equivalent to the zonotope formed
be the line segments {[ui1 - ui2 , 0]}ip=1 with a shift of ip=1 ui2.
Proof. Let Uj be a matrix with Uj(:, i) = uij , i = 1, . . . , p, w be a column-vector with w(i) =
wi, i = 1, . . . ,p and 1p is a column-vector of ones of length p. Then, the zonotope Z formed by the
Minkowski sum of line segments with arbitrary end points can be defined as
p
Z =	Xwiui1 + (1 - wi)ui2; wi ∈ [0, 1], ∀ i
i=1
= nU1w-U2w+U21p, w∈ [0,1]po
= n(U1-U2)w+U21p, w∈ [0,1]po
=n (Ui- U2) w, W ∈ [0,1]p} + {U2lp}.
Note that the Minkowski sum of any polytope with a point is a translation; thus, the result follows
directly from Definition 6.
□
16
Under review as a conference paper at ICLR 2020
D. 1 Optimization of Objective equation 2 of the Binary Classifier
2
min1∣∣G i — Gι∣∣ +
A,B 2 Il	IIf
1
2G2 - G2
+λ1∣∣G i∣∣2,ι+λ2
2,1
(9)
F
Note that Gi = Diag ReLU(B(1,:)) + ReLU(-B(2,:)) A, G2 = Diag ReLU(B(2,:)) 十
ReLU(-B(1,:))] A. Note that Gi = Diag ∣ReLU(B(1,:)) + ReLU(-B(2,:))] A and G2 =
Diag∣ReLU(B(2,:)) + ReLU(-B(1,:))] A. For ease of notation We refer to ReLU(B(i,:)) and
〜
〜
〜
ReLU(-B(i, :)) as B+(i, :) and B- (i, :), respectively. We solve the problem with co-rodinate
descent an alternate over variables.
Update A.
2
2
〜
〜
A J arg min ɪ ∣∣Diag (ci) A 一 G
1F
+ 2 ∣∣DiagQ)A - G
2∣∣F + λ1 ∣∣Diag(CI)AI∣2,1 + λ2 ∣∣Diag(C2)A∣∣2,1
where c1 = ReLU(B(1, :)) + ReLU(-B(2, :)) and c2 = ReLU(B(2, :)) + ReLU(-B(1, :)). Note
that the problem is separable per-row of A. Therefore, the problem reduces to updating rows of A
independently and the problem exhibits a closed form solution.
~ , .
A(i,:)
1
arg min 一
A(i,：)	2
∣∣c1jA(i, :) 一 GI(i, :)H2 + 2 ∣∣c2jA(i, :) 一 G2 (i, :)H2 + (λ1 q∕c1 + λ2 qc2) ∣∣jA(i, :)H2
1
arg min -
A(i,:)	2
~ ,
A(i,:) —
clGι(i,:) + c2G2(i,:)
2 (c1 + c2)
+ 1 λi √Cf + λ √C2
2	2 (c1 + c2)
1 λiPCi + λ2PC2____1______ʌ (CiGi(i,:) + c2G2(i,:)
2	2(ci+c2)	FGT⅞¾G式(C ∣∣2八-―
__	~ I ,.	.
Update B +(1,:).
+1
B+ (1,:) = arg min
B+(i,:) 2
∣∣Diag (B +(1,:)) A — Cι∣∣2 + λι ∣∣Diag (B +(1,:)) A + C2( ɪ
s.t. B+(1,:) ≥ 0.
Note that Ci = Gi — Diag (B-(2,:)) A and where Diag (B-(2,:)) A. Note the problem is
separable in the coordinates of B+(1, :) and a projected gradient descent can be used to solve the
problem in such a way as
B+ (1,j) = argmin 1 ∣∣B +(1,j)A(j,:) — Ci(j, :)^ + λι ∣∣B +(1,j )A(j,:) + C2(j,:)( , s.t. B +(1,j) ≥ 0.
A similar symmetric argument can be used to update the variables B+(2, :), B+(1, :) and B- (2, :).
17
Under review as a conference paper at ICLR 2020
D.2 Adapting Optimization equation 2 for Multi-Class Classifier
Note that Theorem 2 describes a superset to the decision boundaries of a binary classifier through
the dual subdivision R(x), i.e. δ(R(x)). For a neural network f with k classes, a natural extension
for it is to analyze the pair-wise decision boundaries of of all k-classes. Thus, let T(Rij(x)) be the
superset to the decision boundaries separating classes i and j . Therefore, a natural extension to the
geometric loss in equation 1 is to preserve the polytopes among all pairwise follows
m,in ∀X∈sd( ConvexHUll (ZG(i+,j-),ZG "), ConvexHull (ZG(i+j-), ZG(j+，i-)))
(10)
The set S is all possible pairwise combinations of the k classes sUch that S = {[i, j], ∀i 6=
j, i = 1, . . . , k, j = 1, . . . , k}. The generator Z (G(i,j)) is the zonotope with the generator matrix
G(i+,j-) = Diag ReLU(B(i, :)) + ReLU(-B(j, :)) A. However, sUch an approach is generally
compUtationally expensive, particUlarly, when k is very large. To this end, we make the following
observation that G(i+, j-) can be eqUivalently written as a Minkowski sUm between two sets zono-
topes With the generators G(i+) = Diag ∣ReLU(B(i,:)] A and Gj- = Diag ∣ReLU(Bj-)] A.
That is to say, ZG _ = ZG +Zg _ ∙ This follows from the associative property ofMmkowskι
G(i+,j-)	Gi+	Gj-
sUms given as follows:
Fact 5. Let {Si}in=1 be the set of n line segments. Then we have that
〜〜 〜
S = Sl+ ... + Sn = P+V
一	~	Ly
〜

〜
i	. i	. i t 7	C	» τ λ T	Cl	广r	ι ʌ*	ι	.	. ■
where the sets P = +j∈C1 Sj and V = +j ∈C2 Sj where C1 and C2 are any complementary parti-
tions of the set {Si}in=1.
Hence, G(i+,j-) can be seen a concatenation between G(i+) and G(j-). ThUs, the objective in 10
can be expanded as follows
mB X d(ConvexHUll (ZGa+,-), ZGj+,i-))，ConveXHUll (ZG(i+,j-), ZGj+,i-)))
A,B ∀[i,j]∈S
〜
〜
〜
〜
min d
A,B ∀[i,j]∈S
ConvexHUll (ZG ++ Z^. _, ZG + + Z(g _ , , ConvexHUll (ZG什 + ZGj-, Zg+ + ZG-
2
F
2
2
min
A ,B
X 2IIGi+ - Gi+IIf+2IIGi- - Gi- (尸+1 BG^+ - Gj+
min
A ,B
∀[i,j]∈S
k
X 2(k - 1)(IIG i+ - Gi+
i=1
+2∣IG j-- Gj-
F + IIGi- - Gi- IIF + IIGj+ - Gj+ IIF + IIGj- - Gj
F
F
2
〜
2
〜
2
〜
The approximation follows in a similar argUment to the binary classifier case where approximating
the generators. The last eqUality follows from a coUnting argUment. We solve the objective for
all mUlti-class networks in the experiments with alternating optimization in a similar fashion to the
binary classifier case. Similarly to the binary classification approach, we introdUce the kk2,1 to
enforce sparsity constraints for prUning pUrposes. Therefore the overall objective has the form
2,1 + iGjT[i
+ IIGj+ - Gj+IIf +
ii Gj-
For completion, we derive the Updates for A and B.
18
Under review as a conference paper at ICLR 2020
Update A.
k r /	C	C
A = argminX2 ɑDiag (B +(i,:)) A - Gi+∣ ∣ + IDiag (B-(i,:)) A - Gi-L
+ ∣∣Diag(B +(j,:)) A - Gj+1∣2 + ∣∣Diag(B-(j,:)) A - Gj
+ λ (∣ ∣ Diag (B +(i,:)) A∣∣21 + ∣∣Diag (B飞:))A∣∣? J ∣∣Diag (B +(j,:)) A∣∣? ɪ
+ ∣∣Diag(B P:)) Ah).
〜
Similar to the binary classification, the problem is seprable in the rows of A. and a closed form
≈
solution in terms of the PrOxImal operator of '2 norm follows naturally for each A(i,:).
__	~ I ,.
Update B +(i,:).
〜
〜
B+ (i,:) = arg min ɪ ∣∣Diag (B +(i,:)) A — G
∙i+∣∣2 + λ ∣∣Diag (B +(i,:))
A∣L J s.t. B+(i,:) ≥ 0.
Note that the problem is separable per coordinates of B+(i,:) and each subproblem is updated as:
2
B+ (i,j) = arg min 1 ∣∣B +(i,j)A(j,:)
B + (i,j) 2
-Gi+(j, :)||2 + λ
∣∣B +(i,j)A(j,:)(, s.t. B +(i,j) ≥0

arg min 2 ∣∣B +(i, j)A(j,:) - G i+(j,:)(+ λ ∣B(i, j )∣ ∣∣A(j, :)^, s.t. B +(i,j) ≥ 0
max 0,
~	,	. -I-	~	. .	~	,	...
A(j, :)>Gi+ (j,:) - λ∣∣A(j, :)||2
kA(j, :)k2
~
~ ~
A similar argument can be used to update B- (i,:) ∀i. Finally, the parameters of the pruned network
will be constructed A — A and B - B+ - B .
19
Under review as a conference paper at ICLR 2020
Algorithm 1: Solving Problem (5)
Input : Ai ∈ Rp×n, B ∈ Rk×p, xo ∈ Rn,t,λ > 0,γ > 1,K > 0,ξA1 = 0p×n,η1 = z1 =
w1 = z1 = u1 = w1 = 0n.
Output: η, ξA1
Initialize: ρ = ρ0
while not converged do
for k ≤ K do
η update: ηk+1 = (2λA1>A1 + (2 + ρ)I)-1(2λA1>ξAk x0 + ρzk - uk)
{min(1 — xo, 6ι) : Zk _ 1/pvk > min(1 — x0, 6ι)
max(-xo, —€1) : Zk _ 1/pvk < max(-xo,一"
zk - 1/pvk	: otherwise
z update: zk+1 =n忆+1+2。(ηk+1zk + ρ(ηk+1 + 1/puk + Wk + 1/pvk) - VL(zk + xo))
ξA1 update:
ξA+ 1 = argmingA kξAιkF + 入|归人产0 - A1ηk+1k2 + L(A1) s.t. ∣∣ξAj∞,∞ ≤ €
u update: uk+1 = uk + ρ(ηk+1 - Zk+1)
v update: vk+1 = vk + ρ(Wk+1 - Zk+1))
P — YP
end
λ 一 γλ
P — Po
end
E Algorithm for S olving 5.
In this section, we are going to derive an algorithm for solving the following problem.
min	D1 (η) +D2(ξA1)
η,ξA1
s.t.	- loss(g(A1(xo + η)),t) ≤ -1,	-loss(g(A1 + ξA1)xo,t) ≤ -1,	(11)
(xo +η) ∈ [0, 1]n,	kηk∞ ≤ €1,	kξA1 k∞,∞ ≤ €2,	A1η - ξA1 xo = 0.
The function D2 (ξA) captures the perturbdation in the dual subdivision polytope such that the
dual subdivion of the network with the first linear layer A1 is similar to the dual subdivion of
the network with the first linear layer A1 + ξA1 . This can be generally formulated as an ap-
Proximation to the following distance function d(ConvHUll (ZGɪ, ZG2), ConvHUll (Zgi ,ZG2 ) ,
where G 1 = Diag [ReLU(B(1,:)) + ReLU(-B(2,:))](A + §aJ, G2 = Diag ReLU(B(2,:
))+ ReLU(-B(1, ：))] (A + ξAJ, G1 = Diag ∣ReLU(B(1,:)) + ReLU(-B(2,:))] A and G2 =
[,ɪ ,. .. ，二，. ..≈
ReLU(B(2,:)) + ReLU(-B(1,:)) A.
In ParticUlar, to aPProximate the fUnction d, one can
Use a similar argUment as in Used in network PrUning 5 sUch that D2 aPProximates the generators of
the zonotoPes directly as follows
33 )=11G 1-G1lF+IBg 2 -MF
=2 IlDiag(B+(1, j)ξAι∣∣F + 1 IIDiag(B-(I, j)ξA1∣∣F
+2 IlDiag(B+⑵：))£a』F+111Diag(B-⑵：))1』：.
This can thereafter be extended to mUlti-class network with k classes as follows D2(ξA1 ) =
2 Pk=1 IlDiag(B+(j, ：))€ai Il尸 + IlDiag(B-(j, ：))€ai 卜.Following XU et al. (2018), We take
D1(η) = 1 kηk2. Therefore, We can write 11as follows
20
Under review as a conference paper at ICLR 2020
min
η,ξA
Dι(η)+x Mag(B+(j,:DW2+Mag(B-j, j)ξA∣∣2.
j=1	F	F
s.t. - loss(g(A1(x0 + η)), t) ≤ -1,	-loss(g((A1 +ξA1)x0),t) ≤ -1,
(x0 +η)	∈	[0,	1]n,	kηk∞	≤	1,	kξA1 k∞,∞ ≤	2,	A1η -	ξA1 x0	=	0.
To enforce the linear equality constraints A1η - ξA1 x0 = 0, we use a penalty method, where each
iteration of the penalty method we solve the sub-problem with ADMM updates. That is, we solve
the following optimization problem with ADMM with increasing λ such that λ → ∞. For ease
of notation, lets denote L(x0 + η) = -loss(g(A1(x0 + η)),t), and L(AI) = -loss(g((Aι +
ξA1 )x0), t).
min
η,z,w,ξA1
s.t.
kηk2 + X IlDiag(ReLU(B(j,：)粒/2 + IlDiag(ReLU(-B(j,：))居』2
j=1	F	F
+ L(XO + Z) + h1(W) + h2(ξAι ) + λk AIn - ξA1 x0k2 + L(AI).
η = z z = w.
where
h1(η)
0,
∞,
if (X0 +η) ∈ [0, 1]n, kηk∞ ≤ 1
else
h2(ξA1)=	0∞,
if kξA1 k∞,∞ ≤ 2
else
The augmented Lagrangian is thus given as follows
k
L(n,w, z, ξA1, u, V)= link2 + L(XO + z) + hi(W) + X ∣lDiag(B+(j,：))匕 ∣∣F + ||Diag(B-(j,：))出 ∣∣F
j=1
+ L(AI) + h2(ξA1) + λk Ain - ξA1 χok2 + u>(n - z) + v>(w - z)
+ P(In - zk2 + kw - zk2)∙
Thereafter, ADMM updates are given as follows
{nk+i, wk+i} = argminL(n,w,zk,ξAk1,uk,vk),
η,w
zk+i = arg min L(nk+i, wk+i, z, ξAk , uk, vk),
ξAk+1i = arg min L(nk+i, wk+i, zk+i, ξA1, uk, vk).
ξA1
uk+i = uk + ρ(nk+i - zk+i), vk+i = vk + ρ(wk+i - zk+i).
Updating n :
nk+1 = arg min knk2 + λkAιn - 0 Xok2 + u>n + P kn - zk2
η2
= (2λAi>Ai + (2 + P)I-i(2λAi>ξAk 1 XO + Pzk -uk.
Updating w:
21
Under review as a conference paper at ICLR 2020
wk+1 = arg min vk> W + h4(w) + P ∣∣w - zk∣∣2
w2
1
arg min 一
w2
w-
—
+ 1 hι(w).
ρ
It is easy to show that the update w is separable in coordinates as follows
{min(1 — xo,"
max(—x0, —1)
Zk — 1∕ρvk
:Zk — 1∕ρvk > min(1 — x0,"
:Zk — 1∕ρvk < max(-xo,一⑴
: otherwise
Updating Z:
Zk+1 = arg min L(x0 + Z) — uk>Z — v
z
k>z + 2 (kηk+1 - z∣2 + l∣wk+1 - Zk2).
Liu et al. (2019) showed that the linearized ADMM converges for some non-convex problems.
Therefore, by linearizing L and adding Bergman divergence term ηk∕2∣Z -Zk∣22, we can then update
z as follows
zk+1 = ɪ— (ηkzk + ρ(ηk+1 + 1 Uk + wk+1 + 1 Vk) - VL(Zk + X0)).
η k + 2ρ	ρ	ρ
It is worthy to mention that the analysis until this step is inspired by Xu et al. (2018) with modifica-
tions to adapt our new formulation.
Updating ξA :
ξA+1 = argmin ∣ξAιkF + λ∣ξAιxo - Aιη∣2 + L(AI) s.t. ∣ξAιk∞,∞ ≤ 3
ξA
The previous problem can be solved with proximal gradient method.
22
Under review as a conference paper at ICLR 2020
F Experimental Details and More Results
In this section, we are going to describe the settings and the values of the hyper-parameters that we
used in the experiments. Moreover, we will show more results since we have limited space in the
main paper.
F.1 Tropical View to the Lottery Ticket Hypothesis.
We begin by throwing the following question. Why investigating the tropical geometrical perspec-
tive of the decision boundaries is more important than investigating the tropical geometrical repre-
sentation of the functional form of the network ? In this section, we show one more experiment that
differentiate between these two views. In the following, we can see that variations can happen to the
tropical geometrical representation of the functional form (zonotopes in case of single hidden layer
neural network), but the shape of the polytope of the decision boundaries is still unchanged and con-
sequently, the decision boundaries. For this purpose, we trained a single hidden layer neural network
on a simple dataset like the one in Figure 2, then we do several iteration of pruning, and visualise
at each iteration both the polytope of the decision boundaries and the zonotopes of the functional
representation of the neural network. It can be easily seen that changes in the zonotopes may not
change the shape of the decision boundaries polytope and consequently the decision boundaries of
the neural network.
Figure 7: Changes in Functional Zonotopes and Decision Boundaries Polytope. First column:
decision boundaries polytope, rest of the columns are the geometrical representation of the func-
tional form of the network. Under different pruning iterations using class blind, we can spot the
changes that affected the tropical geometric representation of the functional form of the network
(zonotopes) while the shape of the decision boundaries polytope is unaffected.
And thus it can be clearly seen that our formulation, which is looking at the decision boundaries
polytope is more general, precise and indeed more meaningful.
Moreover, we conducted the same experiment explained in the main paper of this section on another
dataset to have further demonstration on the favour that the lottery ticket initialization has over other
23
Under review as a conference paper at ICLR 2020
Figure 8: Effect of Different Initializations on the Decision Boundaries Polytope. From left to right:
training dataset, decision boundaries polytope of original network followed by the decision boundaries polytope
during several iterations of pruning with different initializations.
Input Space
Original Polytope
IOttery InitialiZatiOn
×avier Initialization
(0,0.1) gaussian InitialiZation (O,1) gaussian InitiaIiZation
Figure 9: Effect of Different Initializations on the Decision Boundaries Polytope. From left to right:
training dataset, decision boundaries polytope of original network followed by the decision boundaries polytope
during several iterations of pruning with different initializations.
initialization when pruning and retraining the pruned model. It is clear that the lottery initializations
is the one that preserves the shape of the decision boundaries polytope the most.
F.2 Tropical pruning
In the tropical pruning, we have control on two hyper-parameters only, namely the number of iter-
ations and the regularizer coefficient λ which controls the pruning rate. In all of the experiments,
we ran the algorithm for 1 iteration only and we increase λ starting from 0.02 linearly with a factor
of 0.01 to reach 100% pruning. It is also worthy to mention that the output of the algorithm will
be new sparse matrices A, B , but the new network parameters will be the elements in the original
matrices A, B that have indices correspond to the indices of non-zero elements in A, B . By that,
the algorithm removes the non-effective line segments that do not contribute to the decision bound-
aries polytope, without changing the non-deleted segments. Above all, more results of pruning of
AlexNet and VGG16 on various datasets are shown below.
AIexNet on SVHN	AIexNet on CIFAR10	AIexNet on CIFAR100
——CB - AUC = 0.808
—CU - AUC = 0.831
—CD - AUC = 0.823
——Our - AUC = 0.838
CB - A∪C = 0.672
CU - AUC = 0.659
CD - AUC = 0.655
Our - AUC = 0.706
20
O
8
6040
% AUB.IrmV
CD - AUC = 0.914
Pruning Rate %
Figure 10: Results of Tropical Pruning with Fine Tuning the Biases of the Classifier. Tropical
pruning applied on AlexNet and VGG16 trained on SVHN, CIFAR10, CIFAR100 against different
pruning methods with fine tuning the biases of the classifier only.
Oooo
8 6 4 2
CB -AUC = 0.918
CU - AUC = 0.914
Our-AUC = 0.913
24
Under review as a conference paper at ICLR 2020
AleXNet On SVHN
50	60	70	80	90	100
Pruning Rate %
——CB - AUC = 0.854
-CU-AUC = 0.87
CD-AUC = 0.859
——Our-AUC = 0.886
60
40
20
AleXNet On CIFARlO
AIeXNetOn CIFARIOO
0	20	40	60	80	100
Pruning Rate %
——CB - AUC = 0.894
CU - AUC = 0.882
—CD - AUC = 0.883
——Our-AUC = 0.897
VGG16On SVHN
Pruning Rate %
Figure 11: Results of Tropical Pruning with Fine Tuning the Biases of the Network. Tropical
pruning applied on AlexNet and VGG16 trained on SVHN, CIFAR10, CIFAR100 against different
pruning methods with fine tuning the biases of the network.
VGG16 On ClFARlO
Figure 12: Dual View of Tropical Adversarial Attacks. Effect of tropical adversarial attack on
a synthetic dataset with two classes in two different scenarios for the black input point. From left
to right: decision boundaries of Original model, perturbed model and decision boundaries poly-
topes(green for original model and blue for perturbed model).
F.3 Tropical Adversarial Attack
For the tropical adversarial attack, we control five different hyper parameters which are
1	: The upper bound for the infinite norm of δ.
2	: The upper bound for thek.k∞,∞of the perturbation on the first linear layer.
λ : Regularizer to enforce the equality between input perturbation and first layer perturbation
η : Bergman divergence constant.
ρ : ADMM constant.
For all of the experiments, {2, λ, η, ρ} had the values {1, 10-3, 2.5, 1} respectively. the value of 1
was 0.1 when attacking the -fours- images, and 0.2 for the rest of the images. Finally, we show extra
results of attacking the decision boundaries of synthetic data in R2 and MNIST images by tropical
adversarial attacks.
25
Under review as a conference paper at ICLR 2020
Figure 13: Effect of Tropical Adversarial Attacks on MNIST Images. First row from the left:
Clean image, perturbed images classified as [7,3,2,1,0] respectively. Second row from left: Clean
image, perturbed images classified as [9,8,7,3,2] respectively. Third row from left: Clean image,
perturbed images classified as [9,8,7,5,3] respectively. Fourth row from left: Clean image, perturbed
images classified as [9,4,3,2,1] respectively. Fifth row from left: Clean image, perturbed images
classified as [8,4,3,2,1] respectively.
G	Rebuttal
G.1 Reviewer 3
We thank R3 for the time spent reviewing the paper. It is though not clear to the authors the main
reason behind the initial score of weak reject as R3 seems to have very generic questions about our
work but not a particular criticism of the novelty/contribution that we can address in our rebuttal. We
hope that the following response addresses and clarifies some key elements. Moreover, we want to
bring to the attention of R3 that we have addressed the comments/typos/suggestions of all reviewers
in the revised version and marked them in blue.
Q1: What benefit does introducing tropical geometry brings in terms of theoretical analysis?
Does using tropical geometry give us the theoretical results that traditional analysis can not
give us? If so, what is it? I am trying to understand why the authors use this tool. The authors
should be explicit in their motivation so that the readers are clear about the contribution of
this paper. More specifically, from my perspective, tropical semiring, tropical polynomials and
tropical rational functions all can be represented with the standard mathematical tools. Here
they are just redefining several concepts.
As discussed thoroughly in the introduction (last paragraph of page 1), tropical geometry is the
younger twin to algebraic geometry on a particular semiring defined in a way to align with the study
26
Under review as a conference paper at ICLR 2020
of piecewise linear functions. The early definitions stated in the paper (1 to 5) are well known in the
TG literature and were restated for the completion of this paper. While it is true that the definitions
can be represented with standard mathematical tools; however, this misses the fundamental powerful
element TG promises. TG transforms algebraic problems of piecewise linear nature to a combina-
toric problem on general polytopes. To that end, Zhang et. al. 2018 (to the best of our knowledge
the only work at the intersection between TG and DNNs) rederived classical results (upper bound on
the number of linear pieces of DNNs) in a much simpler analysis by counting vertices on polytopes.
In this work, instead of studying the functional representation of piecewise linear DNNs, we study
their decision boundaries using the lens of TG. To wit, the geometric characterization of the deci-
sion boundaries of DNNs developed in Theorem 2 cannot be attained using standard mathematical
tools. More specifically, Theorem 2 represented a superset to the decision boundaries (the tropical
hypersurface T (R(x)), with a geometric structure that is the convex hull between two zonotopes.
While this by itself opens doors for a family of new geometrically motivated regualrizers for train-
ing DNNs that are in direct correspondence with the behaviour of the decision boundaries, we do
not dwell on training beyond this point and leave that for future work. However, this new result
allowed for re-affirmation to the lottery ticket hypothesis in a new fresh perspective. Moreover, we
propose new optimization problems that are geometrically motivated (based on Theorem 2) for sev-
eral classical problems, i.e. network pruning and adversarial attacks that were not possible before
and have provided several new insights and directions. That is we show an intimate relation between
network perturbations (through decision boundary polytope perturbations) and the construction of
adversarial attacks (input perturbations).
Q2: In Experiments on Tropical Pruning, the authors mentioned we compare our tropical
pruning approach against Class Blind (CB), Class Uniform (CU), and Class Distribution (CD)
methods Han et al. (2015). What is Class Blind, Class Uniform and Class Distribution? There
seems to be an error here Figure 5 shows the pruning comparison between our tropical ap-
proach ..., i think Figure 5 should be Figure 4.
We have added the definition of the pruning methods of Han et al. (2015) in the revised version of
the paper for completion, and corrected the typo in the Figure reference.
Q3: In the adversarial attack part, is the authors proposing a new attack method? If so,
then the authors should report the test accuracy under attack. Also, the experimental results
should not be restricted to MNIST dataset. I am also not sure about the attack settings here,
the authors said Instead of designing a sample noise such that (x0 + η) belongs to a new deci-
sion region, one can instead fix x0 and perturb the network parameters to move the decision
boundaries in a way that x0 appears in a new classification region.. Why use this setting? Are
there any intuitions? Since this is different from traditional adversarial attack terminology,
the authors should stop using adversarial attacks as in tropical adversarial attacks because it
is really misleading.
As highlighted in the last sentence in section 6, we are not competing against other attacks, but we
rather show how this new geometric view to the decision boundaries provided by the TG analysis in
Theorem 2 can be leveraged for the construction of adversarial attacks. We want to emphasize to R3
that the polytope representing the decision boundaries (convex hull of two zonotopes as per Theorem
2) is a function of the network parameters and not of the input space. Thus, it is not initially clear
how one can frame the adversarial attacks problem in this new fresh tropical setting since adversarial
attacks is the task of perturbing the input space as opposed to the parameters space of the network
resulting in a flip in the prediction. In the tropical adversarial attacks section, we show that the
problem of designing an adversarial attack x0 +η that flips the network prediction is closely related to
the problem of flipping the network prediction by perturbing the network parameters in the first layer
A1 + ζA1 where both problems are related through a linear system. That is to say, if one finds ζA1
that perturbs the geometric structure (convex hull between two zonotopes, i.e. decision boundaries)
sufficiently enough to flip the network prediction, one can find an equivalent pixel adversarial attack
η by solving the linear system A1η = ζA1 x0 that flips the prediction of the original unperturbed
network (see the end of page 7). We thereafter propose Problem (5) incorporating the geometric
information from Theorem 2 where the linear system is accounted for in the constraints set. We
propose an algorithm to solve the problem (a mix of penalty and ADMM) detailed in Algorithm 1
in the appendix. The solution to problem 5 by applying Algorithm 1 results in the construction of
adversarial attacks (η) that indeed flip the network prediction over all tested examples on the MNIST
dataset.
27
Under review as a conference paper at ICLR 2020
G.2 Reviewer 2
We thank R2 for the time spent reviewing the paper. We also thank R2 for acknowledg-
ing our technical and theoretical contributions. Please note that we have addressed the com-
ments/typos/suggestions of all reviewers in the revised version and marked them in blue. Follows
our response.
Q1: This paper needs to be placed properly among several important missing references on
the decision boundary of deep neural networks [1][2]. In particular, using introduced tropical
geometry perspective, how we can obtain the complexity of the decision boundary of a deep
neural network?
The two works referenced by R2 are not directly related to the body of our work. Below, we sum-
marize both works and state how our work is vastly different from both. The authors of [1] show
that under certain assumptions, the decision boundaries of the last fully connected layer converges
to an SVM classifier. That is to say, the features learnt in deep neural networks are linearly separable
with max margin type linear classifier. On the other hand, the authors of [2] showed that the deci-
sion regions of neural networks with width smaller than the input dimension are unbounded. In our
work, we use a new type of analysis (tropical geometry) to represent the set of decision boundaries
B through its superset T (R(x)) that is the solution set to the tropical polynomial R(x). We then
show that this solution set is related to a geometric structure referred to as the decision boundaries
polytope (convex hull between two zonotopes), this is analogous to constructing newton polytopes
for the solution sets to classical polynomials in algebraic geoemtry. The normals to the edges of
this polytope are parallel to the superset of the decision boundaries T (R(x)). That is to say, if one
processes the polytope in an way that preserves the direction of the normals, the decision boundaries
of the network are preserved. This is the base idea behind all later experiments. In general, this
new representation presents a new fresh revisit to the lottery ticket hypothesis and an utterly new
view to network pruning and adversarial attacks. We do believe this new representation can be of
benefit to other applications and can open doors for a family of new geometrically inspired network
regularizers as well.
Q2: Regarding the complexity of the decision boundaries of neural networks.
The only work at the intersection between TG and DNNs that we are aware of is the work of Zhang
et. al. 2018. They re-derived a classical upper bound to the number of linear regions of feed forward
neural networks with ReLU activations by counting vertices of polytopes (Theorem 6.3). The work
was limited to the complexity (number of linear regions) of the functional representation of the
network and not to the decision boundary.
Q3: The second part of Theorem 2 should be explained straightforwardly and clearly as it
plays an important role in the subsequent results and applications.
We have added a ”Digesting Theorem 2” paragraph in the revised version and rearranged the struc-
ture a bit around Theorem 2.
Q4: Pruning convolutional layers.
Most of the parameters (memory complexity) are the in fully connected layers. For example,
the convolutional part of VGG16 has 14,714,688 parameters, whereas the fully connected layers
have 262,000,400 parameters in total which is 17 times larger. Similarly, the convolutional part of
AlexNet has 3,747,200 parameters while only the first fully connected layer has 37,752,832 param-
eters [5]. However, efficiently extending the tropical pruning to convolutional layers is a nontrivial
interesting direction. Generally speaking, convolutional layers fit our framework naturally since
a convolutional kernel can be represented with a structured topelitz/circulant matrix. However, a
question of efficiency still remains as one still needs to construct the underlying structured matrix
representing the convolutional kernel. Thereafter, a direction of interest is the tropical formulation
of the network pruning problem as a function of the convolutional kernels surpassing the need for
the construction of the dense representation of the kernel. We keep this for future work.
Q5: For the similarity measure.
Comparing the exact decision boundaries between two different architectures can be very difficult
in the sense where decision boundaries for a two-class output network f are defined as {x ∈ Rn :
f1(x) = f2(x)}. Another approach to compare decision boundaries, which is proposed by our work,
is by computing the distance between the the dual subdivision polytope (δ(R(x))) of the tropical
28
Under review as a conference paper at ICLR 2020
polynomials R(x) representing two different architectures. This is since the normals to the edges
of the polytope (δ(R(x))) are parallel to a superset of the decision boundaries (see Figure 1). This
is exactly the proposed objective (1) where d(.) is a distance function to compare the orientation
between two general polytopes. Since finding a good choice for d(.) is generally difficult we instead
approximate it by comparing the generators constructing the (δ(R(x))) in Euclidean distance for
ease (objective (2)). Experimentally and following prior art in the pruning literature (Han et. al.
2015), to compare the effectiveness of the pruning scheme, we compare the test accuracies across
architectures as a function of the pruning ratio. Regardless, the reviewer is right about that similar
test accuracies does not imply similar decision boundaries but rather only an indication.
Q6: In adversarial examples generation, typically for a pre-trained deep neural network model
one is interested in generating examples that are misclassified by the model while they resemble
real instances. In this setting, we keep the model and thus its decision boundary intact. In
this paper, nevertheless, aiming at generating adversarial examples, the decision boundary
and thus the (pre-trained) model is altered. By chaining the decision boundary, however, the
model’s decisions for original real samples might change as well. Therefore, it is not clear
to the reviewer how the introduced method is comparable to the well-established adversarial
example generation setting.
The new approach is definitely comparable to the well-establisehd adversarial example generation
setting. Let us explain. The new analysis provided by Theorem 2, allows to present the decision
boundaries geometrically as a convex hull between two zonotopes which is a function of only the
network parameters and not the input space. Thus, it not clear how one can frame the adversarial
attacks problem in this new fresh tropical setting since adversarial attacks is the task of perturbing the
input space as opposed to the parameters space of the network resulting in a flip in the prediction. In
the tropical adversarial attacks section, we show that the problem of designing an adversarial attack
x0 + η that flips the network prediction is closely related to the problem of flipping the network
prediction by perturbing the network parameters in the first layer A1 + ζA1 where both problems are
related through a linear system. That is to say, if one finds ζA1 (perturbations in the first linear layer)
that perturbs the geometric structure (convex hull between two zonotopes, i.e. decision boundaries)
sufficiently enough to flip the network prediction, one can find an equivalent pixel adversarial attack
η by solving the linear system A1η = ζA1 x0 that flips the prediction of the original unperturbed
network (see the end of page 7). We incorporate this in an overall objective (5) where the linear
system is incorporated as a constraint. Upon solving (5) with the proposed Algorithm 1, we attack
the original network (unperturbed) with the adversarial attack η . Therefore, this is comparable to
the classical adversarial attacks framework. This approach indeed resulted into flipping the network
prediction over all tested examples on the MNIST dataset. As highlighted at the end of section 6,
we do not aim in our approach to outperform the state of the art adversarial attacks, but rather to
provide a novel geometrically inspired perspective that can shed new light in this field.
Q7: Two previous papers investigated the decision boundary of the deep neural networks in
the presence of adversarial examples [3][4]. Please discuss how the introduced method in this
paper is placed among these methods.
The work of [3] analyzed the geometry of adversarial examples by means of manifold reconstruc-
tion to study the trade off between robustness under different norms. On the other hand, [4] crafted
adversarial attacks by estimating the distance to the decision boundaries using random search direc-
tions. Both of the papers made a local estimation of the decision boundary around the attacked point
to construct the adversarial attack to the input image. In our work, we geometrically characterized
the decision boundaries in Theorem 2 where the polytope (convex hull of two zonotopes) is only a
function of the network parameters and NOT the input space. We presented a dual view to adver-
sarial attacks in which one can construct adversarial examples by investigating network parameters
perturbations that results in the largest perturbation to this polytope representing the decision bound-
aries. The scope of our work, and unlike prior art [3,4], is focused towards a new geometric polytope
representation of the decision boundary in the network parameter space (not the input space) through
a new novel analysis. We have added a discussion of both papers in the adversarial attacks section
(Section 6).
Minor comments.
We have addressed the concerns of R2 and left the changes in blue in the revised version.
[1]	”On the decision boundary of deep neural networks”. SLi, Yu and Richtarik, Peter and Ding,
Lizhong and Gao, Xin.
29
Under review as a conference paper at ICLR 2020
[2]	”On decision regions of narrow deep neural networks”. Beise, Hans-Peter and Da Cruz, Steve
Dias and Schroder, Udo.
[3]	”On the geometry of adversarial examples”. Marc Khoury and and Dylan Hadfield-Menell.
[4]	”Decision Boundary Analysis of Adversarial Examples”. Warren He, Bo Li and Dawn Song.
[5]	”Learning both weights and connections for efficient neural networks”. Song Han, Jeff Pool,
John Tran, and William J. Dally.
G.3 Reviewer 1
We thank R1 for the constructive detailed thorough review of the paper and for acknowledging our
contributions and the new insights. Follows our response to R1’s concerns.
In regards to clarity, exposition and focus.
To improve the clarity and exposition, we have added several paragraphs in the revised version of
the paper. The revised edits are marked in blue. As in regards to the focus, we found that it is
challenging within a reasonable time to carry out this major change in the paper. To that end, we
have done our best to further elaborate on several key results in the paper. For instance, we have
merged the paragraph above the contributions with the contributions paragraph. We have added
another paragraph dissecting Theorem 2. We have added some few relevant references that are
essential for the context of motivating tropical adversarial attacks.
In regards to the suggestions.
•	Adding the information that the semiring lacks the additive inverse.
This has been addressed in the revised version.
•	Adding tropical quotient to definition 1.
This has been addressed in the revised version.
•	Definition of π and the upper faces.
Indeed, π is a projection operator that drops the last coordinate. As for upper faces, the formal
definition is given as follows, for a polytope P, F is an upper face of P if x + te ∈/ P for any
x ∈ F, t > 0 where e is a canonical vector. That is the faces that can be seen from ”above”. A good
graphical example can be found in Figure 2 from Zhang et. al. 2018. We dropped this definition
from the paper as it may add some confusion while it does not play an important role in the later
analysis.
•	Theorem 2 lacks intuitive formulation.
This has been addressed in the revised version and we have rearranged the structure of text below
Theorem 2.
•	Regarding the issue of the tropical hypersurface.
Indeed, the superset is in terms of set theory. That is to say, the set of decision boundaries B is a
supset of the tropical hypersurface set T (R(x)).
•	Regarding Figure 2.
The color map represents the compression percentage. We have added a legend to Figure 2. Note
that the second figure in Figure 2 tilted ”original polytope” represents the polytope of the dual
subdivision (convex hull between two zonotopes). While the polytope seems to have only 4 vertices,
there are in fact many other overlapping vertices and thereafter many small edges between all the
seemingly overlapping vertices. It is to observe that the normals to only the 4 major edges in the
”original polytope” are indeed parallel to the decision boundaries plotted by performing forward
passes through the network in the first figure titled ”Input Space”. Note that despite the fact that
more compression is performed, the orientation of the overall polytope is preserved for the lottery
ticket initialization and thereafter preserving the main orientation of the decision boundaries. This
is unlike the the other types of initialization where the orientation of the polytope is vastly different
with different compression ratios resulting into a larger change in the orientation of the decision
boundaries.
30
Under review as a conference paper at ICLR 2020
•	I would suggest to place Figure 1 after stating Theorem 2, since it is only referenced later
on. Furthermore, the red structures are somewhat confusing. According to Theorem 2, the
decision boundary is a subset of the hypersurface, right? What is the relation of the red
structures in the convex hull visualisation? The caption states that they are normals, but as
far as I can tell, this has not been formalised anywhere in the paper (it is used later on, though).
Correct. The decision boundaries are subsets of the tropical hypersurfaces. However, it has been
shown by Maclagan & Sturmfels (2015) (Propositon 3.1.6) that the tropical hypersurface T to any
d variate tropical polynomial is the (d-1)-skeleton of the polyhedral complex dual to the dual subdi-
vision δ(). This implies that the normals to the edges (faces in higher dimensions) are parallel to the
tropical hypersurface. If R1 is interested in learning more about this, an excellent starting point to
build up this intuition is the work of Erwan Brugall and Kristin Shaw ”A bit of tropical geometry”.
Please refer to section 2.2 ”Dual subdivisions” pages 6 and 7.
•	The discussion about the functional form.
We elaborate on this in section F in the appendix. Instead of investigating the decision boundaries
polytope of the tropical polynomial representing the decision boundaries R(x), we analyze the 4
different tropical polynomials representing the 2 different classes. Recall each output function of the
network is a tropical rational of two tropical polynomials giving rise to 4 different polytopes. Figure
7 shows that the decision boundary polytope δ(R(x)) with the lottery ticket is hardly changed while
pruning the network (first column in Figure 7). On the contrary, the zonotopes (dual subdivisions
of the 4 different tropical polynomials) vary much more significantly (coloumns 2,3,4,5). This
demonstrates that there can exist different tropical polynomials representing the functional form of
the network (i.e. H1,2 and Q1,2) while having the same structure for the decision boundary polytope
the corresponding first figure in the same row of Figure 7. This is a mere observation and worth
investigating in future work.
•	In Section 4, how many experiments of the sort were performed? I find this a highly in-
structive view so I would love to see more experiments of this sort. Do these claims hold over
multiple repetitions and for (slightly) larger architectures as well?
We already had one extra experiment (Figure 8) in the appendix for another data. We have also
based on the suggestion of R1 added two more experiments (Figure 9) on two other datasets. Note
that extracting the decision boundaries polytope for larger architectures is much more difficult for
two different reasons. First, for deeper networks beyond the structure Affine-ReLU-Affine, the
decision boundaries polytope is generic and no longer enjoys the nice properties zonotopes exhibit.
Enumerating their vertices rapidly turns to a computationally intractable problem. Secondly, one
can perhaps visualize the polytope for networks with 3 dimensional input but it gets trickier beyond
that.
•	Regarding that the claim that orientations are preserved should be formalised.
That is an excellent question. We have investigated several metrics that try to capture information
about the orientation of a given polytope. For instance, we have investigated the feature that is the
histogram of the oriented normals. That is a histogram of angles for all the normals to edges given a
polytope. We have also investigated the Hausdorff distance as a metric between polytopes. However,
we have decided to keep this for a future direction as this is by itself a entirely new line of work.
That is designing the distance functions d(.) between polytopes that captures orientation information
that can be used for tropical pruning (objective 8) or perhaps other applications. Another interesting
direction is whether such a distance function can be learnt in a meta-learning fashion. For now, we
restrict the experiments to the approximation used in objective (2) which for now only captures the
distance between the sets of generators of the zonotopes in Eucledian sense.
•	Adding link to the definition of minkowski sum in the appendix.
This has been addressed in the revised version.
•	Description to other pruning methods.
This has been addressed in the revised version where we have added the definition to all pruning
competitors.
•	About the plots in Figure 4.
31
Under review as a conference paper at ICLR 2020
In the experiments of Figure 4, there is no stochasticity. All base networks (AlexNet and VGG16)
are trained before hand to achieve the state-of-art baseline results on the respective datasets (SVHN,
CIFAR10 and CIFAR100). The networks are then fixed and several pruning schemes, including the
tropical approach, are applied with a varying level of pruning ratio. We do not re-train the networks
after each pruning step. Thus there exists no source of randomness. However, this is still an excellent
observation. This is since we conduct experiments in the appendix where after each pruning ratio
we fine tune only the biases of the classifier (Figure 10) or we fine tune the biases of the complete
network (Figure 11). In such experiments, due to the fine tuning step, we will consider in the final
version to report the results averaged over multiple runs.
•	In Section 6, I find the comment on normals generating a superset to the decision boundaries
hard to understand.
Indeed, the wording of the sentence was sub optimal. The statement was to reassure what has been
established in earlier sections that the normals to the decision boundary polytope δ(R(x)) represent
the tropical hypersurface set T (R(x)) which is a super set to the decision boundaries set B as per
Theorem 2. We have rephrased the sentence.
•	Take-away message regarding perturbing the decision boundaries.
The proposed approach of perturbing the decision boundaries is a mere dual view for adversarial
attacks. That is to say, to flip a network prediction for a sample x0 , one can either adversarially
perturb the sample (add noise) to cross the decision boundary to a new classification region or
one can perturb the decision boundaries to move closer to the sample x0 to appear as if it is in a
new classification region. Figure 5, demonstrates the later visually through perturbing the decision
boundary polytope. One can observe that perturbing the correct edge in the polytope in Figure 5,
corresponds to altering a specific decision boundary. Figure 5, indeed as correctly pointed out of
R1, is a feasibility study showing that one can perturb decision boundary by perturbing the dual
subdiviosn polytope. However, the real take away message is that these two views (perturbing the
input or the parameter space) are intimately related through a linear system as discussed in the
subsection titled ”Dual View to Adversarial Attacks”. We propose an objective function (Problem
5) and an algorithm (Algorithm 1) to tackle this problem. The solution to problem (5) provides an
INPUT perturbation that results in altering the network prediction. That is to say, the new framework
allows for incorporating geometrically motivated objective function towards constructing classical
adversarial attacks.
•	Regarding the future extension on CNNs and GCNs.
We are currently investigating efficient extension of the current results to convolutional layers. Note
that while convolutional layers can be represented with a large structured topelitz/circulant matrix,
we are interested in extensions that allow for similar analysis but as a function of the convolutional
kernel surpassing the need to constructing convolutional matrix. The GCNs is definitely an exciting
excellent future direction that we have not yet entertained.
•	Minor style issues. We have addressed all the style issues in the revised version.
32