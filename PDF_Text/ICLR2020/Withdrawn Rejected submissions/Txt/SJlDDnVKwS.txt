Under review as a conference paper at ICLR 2020
Improving Evolutionary S trategies with Gen-
erative Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Evolutionary Strategies (ES) are a popular family of black-box zeroth-order op-
timization algorithms which rely on search distributions to efficiently optimize a
large variety of objective functions. This paper investigates the potential bene-
fits of using highly flexible search distributions in ES algorithms, in contrast to
standard ones (typically Gaussians). We model such distributions with Generative
Neural Networks (GNNs) and introduce a new ES algorithm that leverages their
expressiveness to accelerate the stochastic search. Because it acts as a plug-in,
our approach allows to augment virtually any standard ES algorithm with flexible
search distributions. We demonstrate the empirical advantages of this method on
a diversity of objective functions.
1	Introduction
We are interested in the global minimization of a black-box objective function, only accessible
through a zeroth-order oracle. In many instances of this problem the objective is expensive to
evaluate, which excludes brute force methods as a reasonable mean of optimization. Also, as the
objective is potentially non-convex and multi-modal, its global optimization cannot be done greedily
but requires a careful balance between exploitation and exploration of the optimization landscape
(the surface defined by the objective).
The family of algorithms used to tackle such a problem is usually dictated by the cost of one evalua-
tion of the objective function (or equivalently, by the maximum number of function evaluations that
are reasonable to make) and by a precision requirement. For instance, Bayesian Optimization (Jones
et al., 1998; Shahriari et al., 2016) targets problems of very high evaluation cost, where the global
minimum must be approximately discovered after a few hundreds of function evaluations. When
aiming for a higher precision and hence having a larger budget (e.g. thousands of function eval-
uations), a popular algorithm class is the one of Evolutionary Strategies (ES) (Rechenberg, 1978;
Schwefel, 1977), a family of heuristic search procedures.
ES algorithms rely on a search distribution, which role is to propose queries of potentially small
value of the objective function. This search distribution is almost always chosen to be a multivariate
Gaussian. Itis namely the case of the Covariance Matrix Adaptation Evolution Strategies (CMA-ES)
(Hansen & Ostermeier, 2001), a state-of-the-art ES algorithm made popular in the machine learn-
ing community by its good results on hyper-parameter tuning (Friedrichs & Igel, 2005; Loshchilov
& Hutter, 2016). It is also the case for Natural Evolution Strategies (NES) (Wierstra et al., 2008)
algorithms, which were recently used for direct policy search in Reinforcement Learning (RL) and
shown to compete with state-of-the-art MDP-based RL techniques (Salimans et al., 2017). Occa-
sionally, other distributions have been used; e.g. fat-tails distributions like the Cauchy were shown
to outperform the Gaussian for highly multi-modal objectives (Schaul et al., 2011).
We argue in this paper that in ES algorithms, the choice of a standard parametric search distribu-
tion (Gaussian, Cauchy, ..) constitutes a potentially harmful implicit constraint for the stochastic
search of a global minimum. To overcome the limitations of classical parametric search distribu-
tions, we propose using flexible distributions generated by bijective Generative Neural Networks
(GNNs), with computable and differentiable log-probabilities. We discuss why common existing
optimization methods in ES algorithms cannot be directly used to train such models and design a
tailored algorithm that efficiently train GNNs for an ES objective. We show how this new algo-
rithm can readily incorporate existing ES algorithms that operates on simple search distributions,
1
Under review as a conference paper at ICLR 2020
Algorithm 1: Generic ES procedure
input: zeroth-order oracle on f, distribution π0, population size λ
repeat
(Sampling) Sample xι,..., xλ i^d ∏t
(Evaluation) Evaluate f(x1), . . . , f(xn).
(Update) Update πt to produce x of potentially smaller objective values.
until convergence;
like the Gaussian. On a variety of objective functions, we show that this extension can significantly
accelerate ES algorithms.
We formally introduce the problem and provide background on Evolutionary Strategies in Section 2.
We discuss the role of GNNs in generating flexible search distributions in Section 3. We explain why
usual algorithms fail to train GNNs for an ES objective and introduce a new algorithm in Section 4.
Finally we report experimental results in Section 5.
2	Preliminaries
In what follows, the real-valued objective function f is defined over a compact X and π will generi-
cally denote a probability density function over X. We consider the global optimization of f:
x* ∈ argmin f (x)	(1)
x∈X
2.1	Evolutionary S trategies
The generic procedure followed by ES algorithms is presented in Algorithm 1. To make the update
step tractable, the search distribution is tied to a family of distributions and parametrized by a real-
valued parameter vector θ (e.g. the mean and covariance matrix of a Gaussian), and is referred to as
πθ . This update step constitutes the main difference between ES algorithms.
Natural Evolution Strategies One principled way to perform that update is to minimize the ex-
pected objective value over samples x drawn from πθ . Indeed, when the search distribution is
parametric and tied to a parameter θ, this objective can be differentiated with respect to θ thanks to
the log-trick:
J(θ)，E∏θ [f (x)]	and J = E∏θ f (x)学产 1	(2)
∂ θ	∂ θ
This quantity can be approximated from samples - it is known as the score-function or REINFORCE
(Williams, 1992) estimator, and provides a direction of update for θ. Unfortunately, naively follow-
ing a stochastic version of the gradient (2) - a procedure called Plain Gradient Evolutionary Strate-
gies (PGES) - is known to be highly ineffective. PGES main limitation resides in its instability when
the search distribution is concentrating, making it unable to precisely locate any local minimum. To
improve over the PGES algorithm the authors of Wierstra et al. (2008) proposed to descend J(θ)
along its natural gradient (Amari, 1998). More precisely, they introduce a trust-region optimiza-
tion scheme to limit the instability of PGES, and minimize a linear approximation of J(θ) under a
Kullback-Leibler (KL) divergence constraint:
argmin J(θ + δθ) ` J(θ) + δθTVθJ(θ) s.t KL(∏θ+δθ∣∣∏θ) ≤ C	⑶
δθ
To avoid solving analytically the trust region problem (3), Wierstra et al. (2008) shows that its
solution can be approximated by:
δθ* H —F-1VθJ(θ) where Fθ = E∏θ [Vθ log π(x)Vθ log π(X)T]	(4)
is the Fischer Information Matrix (FIM) of πθ . The parameter θ is therefore not updated along
the negative gradient of J but rather along Fθ-1VθJ(θ), a quantity known as the natural gradient.
The FIM Fθ is known analytically when πθ is a multivariate Gaussian and the resulting algorithm,
Exponential Natural Evolutionary Strategies (xNES) (Glasmachers et al., 2010) has been shown to
reach state-of-the-art performances on a large ES benchmark.
2
Under review as a conference paper at ICLR 2020
ES iterations
Figure 2: Example of an undesirable behavior of a Gaussian search distribution. The dashed lines
represent density level lines of the search distribution. Because the latter cannot have a curved
profile, it is forced to drastically reduce its entropy until it reaches the straight part of the valley.
CMA-ES Naturally, there exist other strategies to update the search distribution πθ . For instance,
CMA-ES relies on a variety of heuristic mechanisms like covariance matrix adaptation and evolution
paths, but is only defined when πθ is a multivariate Gaussian. Explaining such mechanisms would
be out of the scope of this paper, but the interested reader is referred to the work of Hansen (2016)
for a detailed tutorial on CMA-ES.
2.2	Limitations of classical search distributions
ES implicitly balance the need for exploration and exploitation of the optimization landscape. The
exploitation phase consists in updating the search distribution, and exploration happens when sam-
ples are drawn from the search distribution’s tails. The key role of the search distribution is therefore
to produce a support adapted to the landscape’s structure, so that new points are likely to improve
over previous samples.
We argue here that the choice of a given parametric distribution (the multivariate Gaussian distri-
bution being overwhelmingly represented in state-of-the-art ES algorithms) constitutes a potentially
harmful implicit constraint for the stochastic search of a global minimum. For instance, a Gaussian
distribution is not adapted to navigate a curved valley because of its inability to continuously curve
its density. This lack of flexibility will lead it to drastically reduce its entropy, until the curved val-
ley looks locally straight. At this point, the ES algorithm resembles a hill-climber and barely takes
advantage of the exploration abilities of the search distribution. An illustration of this phenomenon
is presented in Figure 2 on the Rosenbrock function. Another limitation of classical search distri-
bution is their inability to follow multiple hypothesis, that is to explore at the same time different
local minima. Even if mixture models can show such flexibility, hyper-parameters like the number
of mixtures have optimal values that are impossible to guess a priori.
We want to introduce flexible search distributions to overcome these limitations. Such distributions
should, despite their expressiveness, be easily trainable. We should also be concerned when de-
signing them with their role in the exploration/exploitation trade off: a search distribution with too
much capacity could over-fit some seemingly good samples, leading to premature convergence. To
sum-up, we want to design search-distributions that are:
•	more flexible than classical distributions
•	yet easily trainable
•	while keeping control over the exploration / exploitation trade-off
In the following section, we carefully investigate the class of Generative Neural Networks (GNNs)
to find a parametric class of distributions satisfying such properties.
3	Flexible search distributions with GNNs
Generative Neural Networks (MacKay, 1995) have been studied in the context of density estimation
and shown to be able to model complex and highly multimodal distributions (Srivastava et al., 2017).
We propose here to leverage their expressiveness for ES, and train them in a principled way thanks
to the ES objective:
J(π) =Eπ[f(x)]
3
Under review as a conference paper at ICLR 2020
As discussed in Section 2, optimizing J (π) with gradient-based methods is possible through the
score-function estimator, which requires to be able to compute and efficiently differentiate the log-
probabilities of π.
3.1	GNN background
The core idea behind a GNN is to map a latent variable z ∈ Z drawn from a known distribution νω
to an output variable x = gη(z) where gη is the forward-pass of a neural network. The parameter η
represents the weights of this neural network while ω describe the degrees of freedom of the latent
space distribution νω. We denote θ= (ω, η) and πθ(x) the density of the output variable x.
For general neural network architectures, it is impossible to compute πθ (x) for samples x drawn
from the GNN. This is namely why their are often trained with adversarial methods (Goodfellow
et al., 2014) for sample generation purposes, bypassing the need of computing densities, but at the
expense of a good density estimation (mode-dropping). An alternative to adversarial methods was
proposed with variational auto-encoders (Kingma & Welling, 2013) however at the cost of learning
two neural networks (an encoder and a decoder). A less computationally expensive method consists
in restricting the possible architectures to build bijective GNNs, also known as Normalizing Flows
(NF) (Rezende & Mohamed, 2015; Papamakarios et al., 2017), which allows the exact computation
of the distribution’s density. Indeed, if gη is a bijection from Z to X with inverse hη , gη-1, the
change of variable formula provides a way to compute πθ (x):
∏θ(x) = Vω(hη(x)) J dh∂X[	(5)
To have a tractable density one therefore needs to ensure that the determinant of the Jacobian
∣∂hη(x)∕∂x∣ is easily computable. Several models satisfying these two properties (i.e bijectivity
and computable Jacobian) have been proposed for density estimation (Rippel & Adams, 2013; Dinh
et al., 2014; 2016), and proved their expressiveness despite their relatively simple structure.
NFs therefore answer two of our needs when building our new search distribution: flexibility and
easiness to train. In this work, we will focus on one NF model: the Non-Linear Independent Compo-
nent Estimation (Dinh et al., 2014) (NICE) model, for its numerical stability and volume preserving
properties.
3.2	NICE model
The authors of NICE proposed to build complex yet invertible transformations through the use of
additive coupling layers. An additive coupling layer leaves half of its input unchanged, and adds a
non-linear transformation of the first half to the second half. More formally, by noting v = [v1 , v2]
the output of a coupling layer and u = [u1, u2] its input, one has:
v1 = u1 and v2 = u2 + t(u1 )	(6)
where t is an arbitrarily complex transformation - modelled by a Multi-Layer Perceptron (MLP)
with learnable weights and biases. This transformation has unit Jacobian determinant and is easily
invertible:
u1 = v1 and u2 = v2 - t(v1 )	(7)
and only requires a feed-forward pass on the MLP t. The choice of the decomposition u = [u1, u2]
can be arbitrary, and is performed by applying a binary filter to the input. By stacking additive
coupling layers, one can create complex distributions, and the inversion of the resulting mapping is
independent of the complexity of the neural networks t. The density of the resulting distribution is
readily computable thanks to the inverse transform theorem (5).
3.3	Volume preserving properties
The transformation induced by NICE is volume preserving (it has a unitary Jacobian determinant).
This is quite desirable in a ES context, as the role of concentrating the distribution on a minimum
can be left to the latent space distribution νω . The role of the additive coupling layers is therefore
only to introduce non-linearities in the inverse transform hη so that the distribution is better adapted
4
Under review as a conference paper at ICLR 2020
to the optimization landscape. The fact that this fit is volume-preserving (every subset of the latent
space has an image in the data space with the same probability mass) encourages the search distri-
bution to align its tails with regions of small value of the optimization landscape, which is likely to
improve the quality of future exploration steps. The NICE model therefore fits perfectly our needs
for a flexible search distribution that is easy to train, and that provides enough control on the explo-
ration / exploitation trade-off. Other bijective GNN models like the Real-NVP (Dinh et al., 2016)
introduce non-volume preserving transformations, which cannot provide such a control. In prac-
tice, we observed that using such transformations for ES led to early concentration and premature
convergence.
4	An efficient training algorithm
We are now equipped with enough tools to use GNNs for ES: an adapted model (NICE) for our
search distribution πθ, and an objective to train it with:
J(θ) = Eπθ [f(x)]	(8)
Here, θ describes jointly the free parameters of the latent distribution νω and η, the weights and
biases of the MLPs forming the additive coupling layers.
We start this section by explaining why existing training strategies based on the objective (8) are
not sufficient to truly leverage the flexibility of GNNs for ES, before introducing a new algorithm
tailored for this task.
4.1	Limitations of existing training strategies
We found that the PGES algorithm (naive stochastic gradient descent of (8) with the score-function
estimator) applied to the NICE distribution suffers from the same limitations as when applied to the
Gaussian; it is inable to precisely locate any local minimum. As for the Gaussian, training the NICE
distribution for ES requires employing more sophisticated algorithms - such as NES.
However, using the natural gradient for the GNNs distributions is not trivial. First the Fischer In-
formation Matrix Fθ is not known analytically and must be estimated via Monte-Carlo sampling,
thereby introducing approximation errors. Also, we found that the approximations justifying to
follow the descent direction provided by the natural gradient are not adapted to the NICE distribu-
tion. Indeed, the assumption behind the NES update (4) is that the loss J (θ) can be (locally) well
approximated by the quadratic objective:
J (θ + δθ) = J (θ) + δθT Vθ J (θ) + γδθT Fθ δθ	(9)
where γ is a given non-negative Lagrange multiplier. For NICE, given the highly non-linear nature of
πθ this approximation is bound to fail even close to the current parameter θ and will lead to spurious
updates. A classical technique (Martens, 2010) to avoid such updates is to artificially increase the
curvature of the quadratic term, and is known as damping. Practically, this implies using Fθ + βI
instead of Fθ as the local curvature metric, with β a non-negative damping parameter.
We found that to ensure continuous decrease of J (θ), and because of its highly non-linear nature
when using the GNNs, the damping parameter β has to be set to such high values that the modifi-
cations of the search distribution are too small to quickly make progress and by no means reaches
state-of-the-art performances. We observed that even if the training of the additive coupling layers
is performed correctly (i.e the distribution has the correct shape), high damping of the latent space
parameters prevents the distribution from quickly concentrating when a minimum is found.
It is unclear how the damping parameter should be adapted to avoid spurious update, while still
allowing the distribution to make large step in the latent space and ensure fast concentration when
needed. In the following, we present an alternated minimization scheme to bypass the issues raised
by natural gradient training for GNN distributions in a ES context.
4.2	Alternating minimization
So far, we used the parameter θ to describe both ω and η (respectively, the free parameters of the
latent space distribution νω and the degrees of freedom of the non-linear mapping gη), and the
5
Under review as a conference paper at ICLR 2020
optimization over all these parameters was performed jointly. Separating the roles of ω and η, the
initial objective (2) can be rewritten as follows:
J (θ)= Ez~νω[f(gη (z))] = J (ω,η)	(10)
Therefore, the initial objective can be rewritten as the expected value of samples drawn from the
latent distribution, under the objective f ◦ gη - that is, the representation of the objective function f
in the latent space. If νω is a standard distribution (i.e efficiently trainable with the natural gradient)
and f ◦ gη is a well structured function (i.e one for which νω is an efficient search distribution),
then the single optimization of ω by classical methods (such as the natural gradient) should avoid
the limitations discussed in 2.2. This new representation motivates the design of a new training
algorithm that optimizes the parameters ω and η separately.
Alternating Minimization In the following, we will replace the notation πθ with πω,η to refer
to the NICE distribution with parameter θ = (ω, η). We want to optimize ω and η in an alternate
fashion, which means performing the following updates at every step of the ES procedure:
ωt+1 = argmin J(ω, ηt)
ω
ηt+1 = argmin J(ωt+1, η)
η
(11a)
(11b)
This means that at iteration t, samples are drawn from πωt ,ηt and serve to first optimize the la-
tent space distribution parameters ω, and then the additive coupling layers parameters η. For the
following iteration, the population is sampled under πωt+1,ηt+1 .
The update (11a) of the latent space parameters is naturally derived from the new representation (10)
of the initial objective. Indeed, ω can be updated via natural gradient ascent of J(ω, ηt) - that is with
keeping η = ηt fixed. Practically, this therefore reduces to applying a NES algorithm to the latent
distribution νω on the modified objective function f ◦ gηt .
Once the latent space parameters updated, the coupling layers parameters should be optimized with
respect to:
J(ωt+1,η) = Eπωt+1,η [f(x)]	(12)
At this stage, the only available samples are drawn under πωt,ηt. To estimate, based on these sam-
ples, expectations under πωt+1,ηt one must use importance propensity scores:
J(ωt+ι,η) = E∏ωt,ηt f(x) ∏ωt+1,η(X)) 1	(13)
πωt,ηt (x)
The straightforward minimization of this off-line objective is known to lead to degeneracies (Swami-
nathan & Joachims, 2015, Section 4), and must therefore be regularized. For our application, it is
also desirable to make sure that the update η does not undo the progress made in the latent space
- in other words, we want to regularize the change in f ◦ gη . To that extent, we adopt a technique
proposed in Schulman et al. (2017) and minimize a modification on the initial objective with clipped
propensity weights:
ηt+1 = argmin E∏ωt+ι,ηt f (X)CliPε (((X)) )]	(14)
η	πωt+1 ,ηt (x)
CliPε (x) CliPs the value of x between 1 - and 1 + . The Parameter ε is an hyPer-Parameter that
Controls the Change in distribution, and the Program (14) Can be effiCiently solved via a gradient
desCent tyPe algorithm, suCh as Adam (Kingma & Ba, 2014).
To sum uP, we ProPose oPtimizing the latent distribution and the CouPling layers seParately. The
latent sPaCe is oPtimized by natural gradient desCent, and the CouPling layers via an off-PoliCy ob-
jeCtive with CliPPed ProPensity weights. We Call this algorithm GNN-ES for Generative Neural
Networks Evolutionary Strategies.
6
Under review as a conference paper at ICLR 2020
Latent space optimization It turns out the GNN-ES can be readily modified to incorporate virtu-
ally any existing ES algorithms that operates on the simple distribution νω . For instance, if νω is set
to be a multivariate Gaussian with learnable mean and covariance matrix, the latent space optimiza-
tion (11a) can be performed by either xNES or CMA-ES. This holds for any standard distribution
νω and any ES algorithm operating on that distribution. This remark allows us to place GNN-ES
in a more general framework and to understand it as a way to improve existing ES algorithm, by
providing a principled way to learn complex, non-linear transformations on top of rather standard
search distributions (like the Gaussian). In what follows, we will use the GNN prefix in front of
existing ES algorithm to describe its augmented version with our algorithm, working as a plug-in.
Pseudo-code for this general algorithm can be found in Appendix B.
4.3	Additional tools
Using historic data ES algorithms typically use small populations of samples to estimate expec-
tations. Such small sample sizes don’t allow for enough data exposure for the GNN to build a mean-
ingful transformation gη. To circumvent this problem, we augment the off-line program (14) with
samples for past generations thanks to the fused importance sampling estimator (Peshkin & Shelton,
2002). This technique is classical in similar settings like MDP-based reinforcement learning and
counterfactual reasoning (Nedelec et al., 2017; Agarwal et al., 2017) and proves to be essential for
our problem. Formally, for a given horizon T that controls how far we look in the past, this amounts
to storing the samples x drawn from πθt-T +1 , . . . , πθt (as well as their respective scores) in a buffer
HT. The objective (13) can then be rewritten as:
πωt+1 ,η (x)	πωt+1 ,η (x)
πωt,ηt f( ) ∏ωt,ηt(x) J =	∙ x,f(x)∈HT f( )Πθt-T+1 (x) + …+ ∏θt(x)J	( )
This technique allows to increase the data exposure of the GNN by using past samples (and therefore
does not require additional function evaluations) and to reduce the variance of the off-line estimator
of the original expectation (12) (Nedelec et al., 2017). To control the change in distribution, the
fused propensity weights can then be clipped in a similar fashion than in the program (14).
Mode preserving properties To achieve improved exploration, the search distribution should
align its tails with the level sets of the objective function. This is not guaranteed when performing
the update step (14) since the GNN’s update could simply move the mean of the search distribution
without shaping the tails. One way to encourage the GNN’s capacity to be allocated to the tails is
to impose a mode-preserving property. If μ denotes the location of a mode of the latent distribu-
tion, then the mode of the distribution ∏θ generated by the NICE model is located in gη (μ) (see
Appendix A for the proof). It is therefore easy to build a map fη based on the initial gη that is
mode-preserving:
fη (Z) , gη (Z)- gη (〃) + fηt (〃)	(16)
where μt denotes the mode of the latent distribution Vω at iteration t. Defined as such, f preserves
the mode of the previous search distribution (since 为士+、(μ) = %t(μ)), is trivially still a bijection
and remains volume preserving. Using the push-forward map fη instead of gη , we explicitly push
the flexibility brought by the GNN to impact only the tails of the search distribution. As detailed in
an ablation study presented in Appendix F, this additional tool turns out to be essential in order to
use GNNs for ES.
5	Experimental results
In all that follows, we build the NICE model with three coupling layers. Each coupling layer’s non-
linear mapping t is built with a one hidden layer MLP, with 128 neurons and leaky ReLU (Maas
et al., 2013) activation functions. This architecture is kept constant in all our experiments.
5.1	Visualization
We present here two-dimensional visualizations of the behavior of a GNN distribution trained with
GNN-xNES - the latent distribution is therefore Gaussian. Figure 3a displays the density level lines
7
Under review as a conference paper at ICLR 2020
(b) Latent space
global minimum
Density level curves (dotted lines) in the data space and in the latent space
of the resulting search distribution on the Rosenbrock function. Figure 3b displays the density level
lines of the latent distribution, as well as the learned representation of the objective in the latent
space. The search distribution is able to have curved density isolines, enabling better exploration. In
the latent space, the global minimum can be reached without navigating a curved valley. Figures 4a
and 4b provide similar visualizations on the Rastrigin function, a highly multimodal but symmetric
objective. The GNN lowers the barriers between local minima, making it easier to escape a local
minimum to the global minimum.
5.2	Synthetic objectives
Experimental set-up We present experiments on both unimodal and multimodal objectives for
xNES and GNN-xNES. We use the official implementation of xNES1 with default hyper-parameters
(such as the population size λ), both as a baseline and as an inner optimization method for GNN-
xNES. All experiments are run on the COmparing Continous Optimizers (COCO) (Hansen et al.,
2016) platform, a popular framework for comparing black-box optimization algorithms. It namely
allows to benchmark different algorithms on translated and rotated versions of the same objectives,
in order to evaluate multiple configurations with different global minimum positions. We compare
xNES and GNN-xNES on functions from the 2018 Black-Box Optimization Benchmark (BBOB)
(Hansen et al., 2010) suite. When comparing these two algorithms, we impose that their initial
search distributions are close in order to ensure fair comparison. We insist on the fact that the xNES
algorithm has the exact same configuration whether it is used by itself or as an inner-optimization al-
gorithm for GNN-xNES. Further experimental details, including additional hyper-parameters value
for GNN-xNES are provided in Appendix C.
Unimodal landscapes We run the different algorithms on two unimodal landscapes where we
expect GNN search distributions to bring a significant improvement over the Gaussian - as discussed
in 2.2. These objectives functions are the Rotated Rosenbrock function (a curved valley with high
conditioning) and the Bent Cigar (an asymmetric and curved Cigar function). Extensive details on
these objective functions can be found in the BBOB documentation (Hansen et al., 2010). Results
on additional unimodal functions can be found in Appendix E.
Performance is measured through Empirical Cumulative Distribution Functions (ECDFs) of the run-
time, also known as data profiles (More & Wild, 2009). Such curves report the fraction of problems
solved as a function of the number of objective evaluations. For a given precision ∆, a problem is
said to be solved if the best function evaluation made so far is smaller than f (x*) + ∆. We create
200 problems, equally spaced on a log-scale from ∆ = 102 to ∆ = 10-5 and, as in the COCO
framework, aggregate them over 15 function instances. Results are presented in Figure 5 for the two
benchmark functions and in dimensions d = 2, 5, 10.
Multimodal landscapes We now compare the performances of the different algorithms on a col-
lection of three multimodal objectives: the Rastrigin function, the Griewank-Rosenbrock function
and the Schwefel function. Extensive details about these objectives can be found in Hansen et al.
(2010).
1available in the PyBrain library (SChaUl et al., 2010)
8
Under review as a conference paper at ICLR 2020
(a) Rotated Rosenbrock, d=2
(b) Rotated Rosenbrock, d=5
(c) Rotated Rosenbrock, d=10
(d) Bent Cigar, d=2
(e) Bent Cigar, d=5
(f) Bent Cigar, d=10
Figure 5:	ECDFs curves comparing GNN-xNES and xNES on the Rotated Rosenbrock and Bent
Cigar functions, in dimensions d=2,5,10.
7000
6000
5000 ∙
4000 ∙
30∞
2000 ∙
10∞
5
dimension
⅞ 30∞
$
P
B2≡∞
s
g 2000
,s
g 1500
5 1000
I 500
O
2
5
dimension
GPAdFel Ow *uo!-en-Λ∂aCmUE
6000
5000 ∙
4000 ∙
30∞
2000 ∙
10∞
5
dimension
(a) Rastrigin	(b) Griewank-Rosenbrock	(c) Schwefel
Figure 6:	Scaling comparison of GNN-xNES and xNES on the Rastrigin, Griewank-Rosenbrock
and Schwefel functions, d=2,5,10.
When using ES algorithms to optimize multimodal functions, itis usual to augment them with restart
strategies (Hansen, 2016). When convergence is detected, the search distribution is re-initialized in
order to search another part of the landscape, and often the population size is increased. This allows
to fairly compared algorithms that converge fast to potentially bad local minima, and algorithms that
converges slower to better minima. Their exist a large variety of restart strategies (Loshchilov et al.,
2012; Auger & Hansen, 2005); as the official implementation of xNES is not equipped with a default
one, we trigger a restart whenever the algorithm makes no progress for more than 30 × d iterations.
The standard deviation of the search distribution is set back to 1, and its mean sampled uniformly
within the compact X of interest (defined by the COCO framework). At each restart, the population
size of the algorithm is multiplied by 2, as in Auger & Hansen (2005). This restart strategy is used
for both xNES and GNN-xNES.
We measure performance as the number of functions evaluations to find an objective value smaller
than f (x*) + 10-5 within a budget of d X 105 function evaluations, averaged over 15 function
instances. When an algorithm is not able to discover the global minimum within the given budget,
we use the maximum number of evaluations as its performance. For visualization purposes, this
measure of performance is divided by d2 . Results are reported in Figure 6. On all objectives,
and for all dimensions, GNN-xNES discovers (in average) the global minimum faster than xNES.
Additional results on others multimodal functions are presented in Appendix E.
9
Under review as a conference paper at ICLR 2020
p」eMaJ pω6e,Jω>e
PJ,°M9J P36l°-I3>,°
(a) Swimmer-v1	(b) InvertedDoublePendulum-v1	(c) HalfCheetah-v1
Figure 7:	Direct Policy Search experiments
5.3 Reinforcement Learning experiments
The goal of this section is to present additional comparison between xNES and GNN-xNES on
RL-based objective functions - less synthetic than the previously considered BBOB functions. ES
algorithms have recently been used for direct policy search in Reinforcement Learning (RL) and
shown to reach performances comparable with state-of-the-art MDP-based techniques (Liu et al.,
2019; Salimans et al., 2017). Direct Policy Search ignores the MDP structure of the RL environment
and rather considers it as a black-box. The search for the optimal policy is performed directly in
parameter space to maximize the average reward per trajectory:
f (X)= ET〜Px X rj	(17)
j∈τ
where px is the distribution of trajectories induced by the policy (the state-conditional distribution
over actions) parametrized by x, and r the rewards generated by the environment. The objective
(17) can readily be approximated from samples by simply rolling out M trajectories, and optimized
using ES. In our experiments2, we set M = 10 and optimize deterministic linear policies (as in
Rajeswaran et al. (2017)).
In Figures 7a and 7b we report results of the GNN-xNES algorithm compared to xNES, when run on
the Mujoco locomotion tasks Swimmer and InvertedDoublePendulum, both from the OpenAI Gym
(Brockman et al., 2016). Performance is measured by the average reward per trajectory as a function
of the number of evaluations of the objective f . Results are averaged over 5 random seeds (ruling
the initialization of the environment and the initial distribution over the policy parameters x). In all
three environments, GNN-xNES discovers behaviors of high rewards faster than xNES.
6 Conclusion
In this work, we motivate the use of GNNs for improving Evolutionary Strategies by pinpointing the
limitations of classical search distributions, commonly used by standard ES algorithms. We propose
a new algorithm that leverages the high flexibility of distributions generated by bijective GNNs with
an ES objective. We highlight that this algorithm can be seen as a plug-in extension to existing ES
algorithms, and therefore can virtually incorporate any of them. Finally, we show its empirical ad-
vantages across a diversity of synthetic objective functions, as well as from objectives coming from
Reinforcement Learning. Beyond the proposal of this algorithm, we believe that our work highlights
the role of expressiveness in exploration for optimization tasks. This idea could be leverage in other
settings where exploration is crucial, such a MDP-based policy search methods. An interesting line
of future work could focus on optimizing GNN-based conditional distribution for RL tasks - an idea
already developed in Ward et al. (2019); Mazoure et al. (2019). Other possible extensions to our
work could focus on investigating first-order and mixed oracles, such as in Grathwohl et al. (2017);
Faury et al. (2018).
2We used the rllab library (Duan et al., 2016) for our experiments.
10
Under review as a conference paper at ICLR 2020
References
Aman Agarwal, Soumya Basu, Tobias Schnabel, and Thorsten Joachims. Effective evaluation us-
ing logged bandit feedback from multiple loggers. In Proceedings of the 23rd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pp. 687-696. ACM, 2017.
ShUn-Ichi Amari. Natural gradient works efficiently in learning. Neural Computation, 10(2):251-
276, 1998.
Anne Auger and Nikolaus Hansen. A restart cma evolution strategy with increasing population size.
In 2005 IEEE congress on evolutionary computation, volume 2, pp. 1769-1776. IEEE, 2005.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAi Gym. arXiv preprint arXiv:1606.01540, 2016.
Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-Linear Independent Components
Estimation. arXiv preprint arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density Estimation using Real NVP. arXiv
preprint arXiv:1605.08803, 2016.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking Deep Rein-
forcement Learning for Continuous Control. In International Conference on Machine Learning,
pp. 1329-1338, 2016.
Louis Faury, Flavian Vasile, Clement Calauzenes, and Oliver Fercoq. Neural Generative Models for
Global Optimization with Gradients. arXiv preprint arXiv:1805.08594, 2018.
Frauke Friedrichs and Christian Igel. Evolutionary tuning of multiple SVM parameters. Neurocom-
puting, 64:107-117, 2005.
Tobias Glasmachers, Tom Schaul, Sun Yi, Daan Wierstra, and Jurgen Schmidhuber. Exponential
natural evolution strategies. In Proceedings of the 12th annual conference on Genetic and evolu-
tionary computation, pp. 393-400. ACM, 2010.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In Advances in Neural Infor-
mation Processing Systems, pp. 2672-2680, 2014.
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation
through the void: Optimizing control variates for black-box gradient estimation. arXiv preprint
arXiv:1711.00123, 2017.
Nikolaus Hansen. The CMA Evolution Strategy: a tutorial. arXiv preprint arXiv:1604.00772, 2016.
Nikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in Evolution
Strategies. Evolutionary Computation, 9(2):159-195, 2001.
Nikolaus Hansen, Anne Auger, Steffen Finck, and Raymond Ros. Real-parameter black-box opti-
mization benchmarking 2010: Experimental setup. PhD thesis, INRIA, 2010.
Nikolaus Hansen, Anne Auger, Olaf Mersmann, Tea Tusar, and Dimo Brockhoff. Coco: A platform
for comparing continuous optimizers in a black-box setting. arXiv preprint arXiv:1603.08785,
2016.
Nikolaus Hansen, Youhei Akimoto, and Petr Baudis. CMA-ES/pycma on Github. Zen-
odo, DOI:10.5281/zenodo.2559634, February 2019. URL https://doi.org/10.5281/
zenodo.2559634.
Donald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of expensive
black-box functions. Journal of Global optimization, 13(4):455-492, 1998.
Diederik P Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding Variational Bayes. arXiv preprint
arXiv:1312.6114, 2013.
11
Under review as a conference paper at ICLR 2020
Guoqing Liu, Li Zhao, Feidiao Yang, Jiang Bian, Tao Qin, Nenghai Yu, and Tie-Yan Liu. Trust
Region Evolution Strategies. Association for the Advancement of Artificial Intelligence, 2019.
Ilya Loshchilov and Frank Hutter. CMA-ES for hyperparameter optimization of deep neural net-
works. arXiv preprint arXiv:1604.07269, 2016.
Ilya Loshchilov, Marc Schoenauer, and Michele Sebag. Alternative restart strategies for cma-es. In
International Conference on Parallel Problem Solvingfrom Nature, pp. 296-305. Springer, 2012.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural net-
work acoustic models. In ICML Workshop on Deep Learning for Audio, Speech and Language
Processing. Citeseer, 2013.
David JC MacKay. Bayesian neural networks and density networks. Nuclear Instruments and
Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated
Equipment, 354(1):73-80, 1995.
James Martens. Deep Learning via Hessian-free Optimization. In International Conference of
Machine Learning, volume 27, pp. 735-742, 2010.
Bogdan Mazoure, Thang Doan, Audrey Durand, R Devon Hjelm, and Joelle Pineau. Leveraging
exploration in off-policy algorithms via normalizing flows. arXiv preprint arXiv:1905.06893,
2019.
Jorge J More and Stefan M Wild. Benchmarking derivative-free optimization algorithms. SIAM
Journal on Optimization, 20(1):172-191, 2009.
Thomas Nedelec, Nicolas Le Roux, and Vianney Perchet. A Comparative Study of Counterfactual
Estimators. arXiv preprint arXiv:1704.00773, 2017.
George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density
estimation. In Advances in Neural Information Processing Systems, pp. 2338-2347, 2017.
Leonid Peshkin and Christian R Shelton. Learning from scarce experience. In Proceedings of
the Nineteenth International Conference on Machine Learning, pp. 498-505. Morgan Kaufmann
Publishers Inc., 2002.
Aravind Rajeswaran, Kendall Lowrey, Emanuel V Todorov, and Sham M Kakade. Towards gen-
eralization and simplicity in continuous control. In Advances in Neural Information Processing
Systems, pp. 6550-6561, 2017.
Ingo Rechenberg. Evolutionsstrategien. In Simulationsmethoden in der Medizin und Biologie, pp.
83-114. Springer, 1978.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Interna-
tional Conference on Machine Learning, pp. 1530-1538, 2015.
Oren Rippel and Ryan Prescott Adams. High-dimensional Probability Estimation with Deep Density
Models. arXiv preprint arXiv:1302.5125, 2013.
Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution Strategies as a
scalable alternative to Reinforcement Learning. arXiv preprint arXiv:1703.03864, 2017.
Tom SchaUL Justin Bayer, Daan Wierstra, Yi Sun, Martin Felder, Frank Sehnke, Thomas Ruckstieβ,
and Jurgen Schmidhuber. PyBrain. Journal ofMachine Learning Research, 11:743-746, 2010.
Tom Schaul, Tobias Glasmachers, and JUrgen Schmidhuber. High dimensions and heavy tails for
Natural Evolution Strategies. In Proceedings of the 13th annual conference on Genetic and Evo-
lutionary Computation, pp. 845-852. ACM, 2011.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy
Optimization Algorithms. arXiv preprint arXiv:1707.06347, 2017.
Hans-Paul Schwefel. Numerische Optimierung von Computer-Modellen mittels der Evolution-
Sstrategie: mit einer Vergleichenden Einfuhrung in die Hill-Climbing-Und Zufallsstrategie.
Birkhauser, 1977.
12
Under review as a conference paper at ICLR 2020
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the
human out of the loop: A review of Bayesian Optimization. Proceedings of the IEEE, 104(1):
148-175, 2016.
Akash Srivastava, Lazar Valkoz, Chris Russell, Michael U Gutmann, and Charles Sutton. Veegan:
Reducing mode collapse in GANs using Implicit Variational Learning. In Advances in Neural
Information Processing Systems, pp. 3308-3318, 2017.
Adith Swaminathan and Thorsten Joachims. Counterfactual Risk Minimization: Learning from
Logged Bandit Feedback. In International Conference on Machine Learning, pp. 814-823, 2015.
Patrick Nadeem Ward, Ariella Smofsky, and Avishek Joey Bose. Improving exploration in soft-
actor-critic with normalizing flows policies. arXiv preprint arXiv:1906.02771, 2019.
Daan Wierstra, Tom Schaul, Jan Peters, and Juergen Schmidhuber. Natural Evolution Strategies.
In Evolutionary omputation, 2008. CEC 2008.(IEEE World Congress on Computational Intelli-
gence), pp. 3381-3387. IEEE, 2008.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist Reinforcement
Learning. Machine Learning, 8(3-4):229-256, 1992.
13
Under review as a conference paper at ICLR 2020
A Computing the mode of the search distribution
We prove here the fact that if μ denotes the location of the mode of the latent distribution νω, then
gη(μ) is a mode for ∏ω,η. Indeed, under reasonable smoothness assumptions, one has that y is a
mode for πω,η if and only if:
∂πω,η(x)
∂x
=0
x=y
(18)
Since πω,η(x) = νω(hη(x)), this is therefore equivalent to:
∂hη(x)
∂x
x=y
∂Vω (Z) I
dz Iz=hη(y)
(19)
0
In the NICE model, We have that ∣ dhgXX) ∣ = 1 for all X hence the matrix
and its kernel is reduced to the null vector. Therefore:
∂νω (Z) I
dz Iz=hη (y)
dh∂XX) I	is invertible
x=y
(20)
0
and therefore μ = hη (y) by definition of μ (the only critical point of Vω). Hence since h-1 = gη,
we have that y = gη (μ) which concludes the proof.
B Algorithm pseudo-code
We provide below the pseudo-code for the generic algorithm GNN-A-ES, where A is a generic
ES algorithm operating on a parametric distribution νω . The additional hyper-parameters are the
horizon T as well as the clipping constant ε. The function clip(x, lb, ub) clips the input x between a
lower-bound lb and an upper-bound ub.
Algorithm 2: GNN-A-ES (ex: GNN-xNES, GNN-CMA-ES)
inputs	: objective function f, distribution Vω and its related ES algorithm A
hyper-parameters: clipping constant ε, NICE model architecture, initial parameters ω0, initial
weights η0, horizon T , population size λ
(Initialization)
Initialize NICE MLPS weights and biases with η0.
Let H be a circular buffer of length T X λ
while not terminate do
(Sampling)
Sample Z = {zι,... Zλ} i ' νωt
Apply fηt to Z, obtain X = {xι,...xλ} i^d ∏ωt,ηt
Evaluate F = {f (x1), . . . , f(xλ)}.
Let HJH + {X, F, πωt,ηt }
(ES update)
//One step ES-based optimization of the latent space
Apply A to the latent distribution
ωt+1 J A (νωt, (Z, F))
(GNN update) *
//Many-steps gradient based optimization of the GNN
"	■,〜0nP	/ Zin	( πωt+ι,η(X)	πωt+ι,.t(X) 门 — ʌ	πωt+ι,.t(X) 门 I Λ0
ηt+1 ' η tXf∈H∈H	f ∙ clip	(p∏∈h ∏(x) ,	P∏∈H ∏(x)	∙ (1 - ε),	P∏∈H ∏(x)	• (1 + ε)) ʃ
end
* The (GNN iteration) step can be performed with virtually any gradient descent solver. In all our
experiments, we used Adam (Kingma & Ba, 2014) with learning rate 1e-4 for 500 epochs.
14
Under review as a conference paper at ICLR 2020
Algorithm 2 does not detail the mode-preserving addition for the sake of readability and clarity. We
provide additional details on this procedure here. Let μt be the mode of the latent distribution Vωt. At
the (Initialization) step, set ɑ0 = g砒 (μo) where gη (∙) is the push-forward map on the NICE model
described in Section 3.2. For all round t ≥ 1, let f (Z) = gη(Z) - gη (μt) + αh The variable αt
represent the push forward mapping of the latent distribution’s mean under the current model. Every
time the latent space is updated - the (ES update) step, let ɑt+ι = f^ (μt+ι). Then, for the (GNN
update), optimize the forward-map f (Z) = gη(Z) - gη(μt+ι) + αt+ι. After this update, We have
fηt+ι (μt+ι) = αt+ι = f% (μt+ι), which means that the mode of the search distribution (which is
the image of the latent distribution mode) has not been impacted by the GNN update.
C Experimental details
C.1 Hyper-parameters
Baselines We use xNES with its default (adapted) hyper-parameters (described in Wierstra et al.
(2008)) for both its baselines versions and its inner optimization parts in GNN-xNES. The popu-
lation size λ is one such hyper-parameters, and is therefore set to λ = 4 + b3 log(d)c. Also, as
it is classically done in ES algorithms, we use a rank-based fitness shaping, designed to make the
algorithm invariant with respect to order-preserving cost transformations. We use the same fitness-
shaping function as in Wierstra et al. (2008).
GNN-ES Across all experiments, we use the same hyper-parameters for GNN-xNES without fine
tuning for each tasks. We use three coupling layers, each with a single hidden layer MLP with 128
hidden neurons and Leaky ReLU activations. The MLPs are initialized via Glorot initialization, and
the clipping constant is set to ε = 0.05. The history size T was determined experimentally, and set
to T = [3 * (1 + log(d))C. When restarts are used, this history size is divided by the numbers of
restart so far (as the population size grows larger).
C.2 Synthetic objectives
Every synthetic objective we used in this work was taken from the BBOB2019 benchmark dataset.
Their expression as well as additional details on the framework can be found in Hansen et al. (2010;
2016). At the beginning of each experiment, we set the Gaussian search distribution (for xNES)
and the Gaussian latent distribution (for GNN-xNES) to a standard normal, with a mean uniformly
sampled within the compact X of interest (defined by the COCO framework).
C.3 RL environments
Table 1 provides details on the RL environment used to compare GNN-xNES and xNES, like the
dimensions of the state space S and action space A, the number d of the policy’s degrees of freedom
and the maximum number of steps m per trajectory. At the beginning of each experiment, we set the
Gaussian search distribution (for xNES) and the Gaussian latent distribution (for GNN-xNES) to a
standard normal with zero mean. In this particular case, where the function evaluations are noisy,
we kept the default population size of the xNES algorithm.
Name	∣s∣	IAI	d	m
SWimmer-VI	13	2	~^8~	1000
InvertedDoublePendulum-v1	11	1	12	1000
HalfCheetah-v1	20	6	126	1000
Table 1:	Reinforcement Learning environments
D Two-dimensional visualizations
We provide in Figure 8 additional two-dimensional visualizations of the behavior of GNN-xNES, on
the Rosenbrock, Rastrigin, Beale and Bent-Cigar functions. We see that the NICE distributions can
15
Under review as a conference paper at ICLR 2020
(c) Beale, global optimum
at(3,0.5)
(d) (Doubly asymmetric)
Bent Cigar, global opti-
mum at (0, 0)
(a) Rosenbrock, global op- (b) Rastrigin, global opti-
timum at (1, 1)	mum at (0, 0)
Figure 8: Two-dimensional visualizations. The black dotted lines represent the isolines of the level
curves of a NICE search distribution trained with GNN-xNES.
Algorithm
xNES
GNN-xNES
mean(# restarts), d=2 mean(# restarts), d=5 mean(# restarts), d=10
2.3
1.3
2.7
2.5
3.4
2.9
Table 2:	Mean number of restarts needed to discover the global minimum on the Rastrigin function.
efficiently fit each optimization landscapes, without having to reduce its entropy like a multivariate
normal would.
E	Additional results
We present here some additional results on some unimodal and multimodal synthetic functions.
Figure 9 present ECDFs curve obtained from the Attractive Sector function, a highly asymmetrical
function around its global minimum. On such a function, GNN-xNES seems to accelerate xNES
in small dimensions, however this speed-up disappears in higher dimensions. Figure 10 presents
results on the Rosenbrock function (without random rotations). Again, GNN-xNES accelerates the
xNES algorithm. Figure 11 present results on the multimodal functions Gallagher’s Gaussian 101
Peaks and Gallagher’s Gaussian 21 Peaks. Again, GNN-xNES discovers the global minimum faster
(on average) than xNES.
In our multimodal experiments, we used simulated restarts as a fair mean of comparing different
algorithm (this is common practice in order to fairly compare algorithms that converge fast to poten-
tially bad local minima to algorithms that converge slowly to the global minimum). If the empirical
results prove that GNN-xNES accelerate xNES in the discovery of the global minimum, it does not
prove that GNN-xNES leverages the flexibility of the GNN to detect the global minimum when
xNES misses it. In an attempt to prove that it is indeed the case, we report in Table 2 the number
of restarts needed by both GNN-xNES and xNES to discover the global minimum on the Rastrigin
function (averaged over the 15 randomly initialized run). For this instance, GNN-xNES consistently
discovers the global minimum with less restarts than xNES.
As detailed in Section 4, one can apply Algorithm 2 as a plug-in to any ES method. So far, we
empirically evaluated the benefits of our approach by comparing xNES against its GNN extension
(GNN-xNES). We present in Figure 12 additional evaluations obtained by comparing CMA-ES and
its GNN extension (denoted GNN-CMA-ES) on the Rosenbrock function in dimension 2,5 and 10.
CMA-ES is considered to be the state-of-the-art ES algorithm, and improving its performances is
a non-trivial task. On the considered example GNN-CMA-ES improves CMA-ES, highlighting the
empirical benefit of our approach for a large class of ES algorithm. One can however observe that
the performance boost brought by the GNN extension is milder for GNN-CMA-ES then for GNN-
xNES. We suspect that this is due to the use of cumulation via an evolution path in CMA-ES3,
which basically introduces a momentum-like update when optimizing the latent distribution. While
using an evolution path makes a lot of sense when optimizing a stationary objective, it can be quite
harmful for non-stationary ones. We therefore believe that the cumulation step in CMA-ES (for the
latent distribution) and the GNN optimization (making the objective optimized by CMA-ES in the
3We used the PyCMA library (Hansen et al., 2019) for these experiments.
16
Under review as a conference paper at ICLR 2020
(a) Attractive Sector, d=2
(b) Attractive Sector, d=5
Figure 9: ECDFs curve for the Attractive Sector function, d=2,5,10
(c) Attractive Sector, d=10
(a) Rosenbrock, d=2
(b) Rosenbrock, d=5	(c) Rosenbrock, d=10
Figure 10: ECDFs curve for the Rosenbrock function, d=2,5,10
latent space non-stationary) can lead to conflicting updates and might hinder the benefits brought
by the GNN’s additional flexibility. Designing a GNN update strategy complying with the use of
evolution paths could therefore be a way of further improving GNN-CMA-ES, and is left for future
work.
F	Ablation study
We present here an ablation study for two additional tools that we introduced after the alternating
optimization view: the mode preserving (16) extension as well as the history augmentation (15).
Figure 13 presents ECDFs curves on the Rosenbrock, Rotated Rosenbrock and Bent Cigar functions
in 2D, for a version of GNN-xNES that doesn’t use history but only the current population. Using
history and therefore exposing the GNN to larger datasets improves the procedure. Figure 14 present
similar results on a version of GNN-xNES without the mode preserving property (16). Again, one
can notice that ensuring that the GNN training is mode-preserving is crucial to improve experimental
results.
Gps3sOw SUORen-aə*) CmVE
xπes
gnn-xnes
2	5	10
dimension
(t∕p6Je-jo-jsuo=en"ΛaiCnVE
xnes
gnn-xnes
5
dimension
(a)	Gallagher’s Gaussian 101 Peaks
(b)	Gallagher’s Gaussian 21 Peaks
Figure 11: Scaling comparison of GNN-xNES and xNES on the Gallagher’s Gaussian 101 Peaks
and Gallagher’s Gaussian 21 Peaks functions, d=2,5,10.
17
Under review as a conference paper at ICLR 2020
(a) Rosenbrock, d=2
(b) Rosenbrock, d=5
(c) Rosenbrock, d=10
Figure 12: ECDFs curves for the Rosenbrock function, (d=2,5,10) comparing the CMA-ES and
GNN-CMA-ES
(a) Rosenbrock, d=2
(b) Rotated Rosenbrock, d=2
(c) Bent Cigar, d=2
Figure 13:	ECDFs curves for xNES, GNN-xNES and GNN-xNES-no-history, for which the history
size T = 1. Using past populations to estimate expectations improves the optimization.
(a) Rosenbrock, d=2
(b) Rotated Rosenbrock, d=2
(c) Bent Cigar, d=2
Figure 14:	ECDFs curves for xNES, GNN-xNES and GNN-xNES-nmp, which is not mode pre-
serving. Ensuring that the training of the GNN doesn’t impact the mode of the search distribution
improves the optimization.
18