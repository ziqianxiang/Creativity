Under review as a conference paper at ICLR 2020
Learning to Discretize: Solving 1D Scalar
Conservation Laws via Deep Reinforcement
Learning
Anonymous authors
Paper under double-blind review
Ab stract
Conservation laws are considered to be fundamental laws of nature. It has broad
application in many fields including physics, chemistry, biology, geology, and
engineering. Solving the differential equations associated with conservation laws is
a major branch in computational mathematics. Recent success of machine learning,
especially deep learning, in areas such as computer vision and natural language
processing, has attracted a lot of attention from the community of computational
mathematics and inspired many intriguing works in combining machine learning
with traditional methods. In this paper, we are the first to view numerical PDE
solvers as a MDP and to use (deep) RL to learn new solvers. As a proof of concept,
we focus on 1-dimensional scalar conservation laws. We deploy the machinery of
deep reinforcement learning to train a policy network that can decide on how the
numerical solutions should be approximated in a sequential and spatial-temporal
adaptive manner. We will show that the problem of solving conservation laws can
be naturally viewed as a sequential decision making process and the numerical
schemes learned in such a way can easily enforce long-term accuracy. Furthermore,
the learned policy network is carefully designed to determine a good local discrete
approximation based on the current state of the solution, which essentially makes
the proposed method a meta-learning approach. In other words, the proposed
method is capable of learning how to discretize for a given situation mimicking
human experts. Finally, we will provide details on how the policy network is trained,
how well it performs compared with some state-of-the-art numerical solvers such
as WENO schemes, and how well it generalizes. Our code is released anomynously
at https://github.com/qwerlanksdf/L2D.
1	Introduction
Conservation laws are considered to be one of the fundamental laws of nature, and has broad
applications in multiple fields such as physics, chemistry, biology, geology, and engineering. For
example, Burger’s equation, a very classic partial differential equation (PDE) in conservation laws,
has important applications in fluid mechanics, nonlinear acoustics, gas dynamics, and traffic flow.
Solving the differential equations associated with conservation laws has been a major branch of
computational mathematics (LeVeque, 1992; 2002), and a lot of effective methods have been proposed,
from classic methods such as the upwind scheme, the Lax-Friedrichs scheme, to the advanced
ones such as the ENO/WENO schemes (Liu et al., 1994; Shu, 1998), the flux-limiter methods
(Jerez Galiano & Uh Zapata, 2010), and etc. In the past few decades, these traditional methods
have been proven successful in solving conservation laws. Nonetheless, the design of some of the
high-end methods heavily relies on expert knowledge and the coding of these methods can be a
laborious process. To ease the usage and potentially improve these traditional algorithms, machine
learning, especially deep learning, has been recently incorporated into this field. For example, the
ENO scheme requires lots of ‘if/else’ logical judgments when used to solve complicated system
of equations or high-dimensional equations. This very much resembles the old-fashioned expert
systems. The recent trend in artificial intelligence (AI) is to replace the expert systems by the so-called
‘connectionism’, e.g., deep neural networks, which leads to the recent bloom of AI. Therefore, it
1
Under review as a conference paper at ICLR 2020
is natural and potentially beneficial to introduce deep learning in traditional numerical solvers of
conservation laws.
1.1	Related works
In the last few years, neural networks (NNs) have been applied to solving ODEs/PDEs or the
associated inverse problems. These works can be roughly classified into three categories according to
the way that the NN is used.
The first type of works propose to harness the representation power of NNs, and are irrelevant to the
numerical discretization based methods. For example, Raissi et al. (2017a;b); Yohai Bar-Sinai (2018)
treated the NNs as new ansatz to approximate solutions of PDEs. It was later generalized by Wei
et al. (2019) to allow randomness in the solution which is trained using policy gradient. More recent
works along this line include (Magiera et al., 2019; Michoski et al., 2019; Both et al., 2019). Besides,
several works have focused on using NNs to establish direct mappings between the parameters of the
PDEs (e.g. the coefficient field or the ground state energy) and their associated solutions (Khoo et al.,
2017; Khoo & Ying, 2018; Li et al., 2019; Fan et al., 2018b). Furthermore, Han et al. (2018); Beck
et al. (2017) proposed a method to solve very high-dimensional PDEs by converting the PDE to a
stochastic control problem and use NNs to approximate the gradient of the solution.
The second type of works focus on the connection between deep neural networks (DNNs) and
dynamic systems (Weinan, 2017; Chang et al., 2017; Lu et al., 2018; Long et al., 2018b; Chen et al.,
2018). These works observed that there are connections between DNNs and dynamic systems (e.g.
differential equations or unrolled optimization algorithms) so that we can combine deep learning
with traditional tools from applied and computational mathematics to handle challenging tasks in
inverse problems (Long et al., 2018b;a; Qin et al., 2018).The main focus of these works, however,
is to solve inverse problems, instead of learning numerical discretizations of differential equations.
Nonetheless, these methods are closely related to numerical differential equations since learning a
proper discretization is often an important auxiliary task for these methods to accurately recover the
form of the differential equations.
The third type of works, which target at using NNs to learn new numerical schemes, are closely related
to our work. However, we note that these works mainly fall in the setting of supervised learning (SL).
For example, Discacciati et al. (2019) proposed to integrate NNs into high-order numerical solvers
to predict artificial viscosity; Ray & Hesthaven (2018) trained a multilayer perceptron to replace
traditional indicators for identifying troubled-cells in high-resolution schemes for conservation laws.
These works greatly advanced the development in machine learning based design of numerical
schemes for conservation laws. Note that in Discacciati et al. (2019), the authors only utilized
the one-step error to train the artificial viscosity networks without taking into account the long-
term accuracy of the learned numerical scheme. Ray & Hesthaven (2018) first constructed several
functions with known regularities and then used them to train a neural network to predict the location
of discontinuity, which was later used to choose a proper slope limiter. Therefore, the training of
the NNs is separated from the numerical scheme. Then, a natural question is whether we can learn
discretization of differential equations in an end-to-end fashion and the learned discrete scheme also
takes long-term accuracy into account. This motivates us to employ reinforcement learning to learn
good solvers for conservation laws.
1.2	Our Approach
The main objective of this paper is to design new numerical schemes in an autonomous way. We
propose to use reinforcement learning (RL) to aid the process of solving the conservation laws. To
our best knowledge, we are the first to regard numerical PDE solvers as a MDP and to use
(deep) RL to learn new solvers. We carefully design the proposed RL-based method so that the
learned policy can generate high accuracy numerical schemes and can well generalize in varied
situations. Details will be given in section 3.
Here, we first provide a brief discussion on the benefits of using RL to solve conservation laws (the
arguments apply to general evolution PDEs as well):
•	Most of the numerical solvers of conservation law can be interpreted naturally as a sequential
decision making process (e.g., the approximated grid values at the current time instance definitely
2
Under review as a conference paper at ICLR 2020
affects all the future approximations). Thus, it can be easily formulated as a Markov Decision
Process (MDP) and solved by RL.
•	In almost all the RL algorithms, the policy π (which is the AI agent who decides on how the
solution should be approximated locally) is optimized with regards to the values Qπ (s0 , a0) =
r(s0, a0) + Pt∞=1 γtr(st, at), which by definition considers the long-term accumulated reward (or,
error of the learned numerical scheme), thus could naturally guarantee the long-term accuracy of
the learned schemes, instead of greedily deciding the local approximation which is the case for
most numerical PDEs solvers. Furthermore, it can gracefully handle the cases when the action
space is discrete, which is in fact one of the major strength of RL.
•	By optimizing towards long-term accuracy and effective exploration, we believe that RL has a
good potential in improving traditional numerical schemes, especially in parts where no clear
design principles exist. For example, although the WENO-5 scheme achieves optimal order of
accuracy at smooth regions of the solution (Shu, 1998), the best way of choosing templates near
singularities remains unknown. Our belief that RL could shed lights on such parts is later verified
in the experiments: the trained RL policy demonstrated new behaviours and is able to select better
templates than WENO and hence approximate the solution better than WENO near singularities.
•	Non-smooth norms such as the infinity norm of the error is often used to evaluate the performance
of the learned numerical schemes. As the norm of the error serves as the loss function for the
learning algorithms, computing the gradient of the infinity norm can be problematic for supervised
learning, while RL does not have such problem since it does not explicitly take gradients of the
loss function (i.e. the reward function for RL).
•	Learning the policy π within the RL framework makes the algorithm meta-learning-like (Schmid-
huber, 1987; Bengio et al., 1992; Andrychowicz et al., 2016; Li & Malik, 2016; Finn et al., 2017).
The learned policy π can decide on which local numerical approximation to use by judging from
the current state of the solution (e.g. local smoothness, oscillatory patterns, dissipation, etc). This is
vastly different from regular (non-meta-) learning where the algorithms directly make inference on
the numerical schemes without the aid of an additional network such as π. As subtle the difference
as it may seem, meta-learning-like methods have been proven effective in various applications such
as in image restoration (Jin et al., 2017; Fan et al., 2018a; Zhang et al., 2019). See (Vanschoren,
2018) for a comprehensive survey on meta-learning.
•	Another purpose of this paper is to raise an awareness of the connection between MDP and
numerical PDE solvers, and the general idea of how to use RL to improve PDE solvers or even
finding brand new ones. Furthermore, in computational mathematics, a lot of numerical algorithms
are sequential, and the computation at each step is expert-designed and usually greedy, e.g., the
conjugate gradient method, the fast sweeping method (Zhao, 2005), matching pursuit (Mallat
& Zhang, 1993), etc. We hope our work could motivate more researches in combining RL and
computational mathematics, and stimulate more exploration on using RL as a tool to tackle the
bottleneck problems in computational mathematics.
Our paper is organized as follows. In section 2 we briefly review 1-dimensional conservation laws and
the WENO schemes. In section 3, we discuss how to formulate the process of numerically solving
conservation laws into a Markov Decision Process. Then, we present details on how to train a policy
network to mimic human expert in choosing discrete schemes in a spatial-temporary adaptive manner
by learning upon WENO. In section 4, we conduct numerical experiments on 1-D conservation
laws to demonstrate the performance of our trained policy network. Our experimental results show
that the trained policy network indeed learned to adaptively choose good discrete schemes that
offer better results than the state-of-the-art WENO scheme which is 5th order accurate in space
and 4th order accurate in time. This serves as an evidence that the proposed RL framework has
the potential to design high-performance numerical schemes for conservation laws in a data-driven
fashion. Furthermore, the learned policy network generalizes well to other situations such as different
initial conditions, mesh sizes, temporal discrete schemes, etc. The paper ends with a conclusion in
section 5, where possible future research directions are also discussed.
3
Under review as a conference paper at ICLR 2020
2	Preliminaries
2.1	Notations
in this paper, we consider solving the following 1-D conservation laws:
ut(x, t) + fx(u(x, t)) = 0, a ≤ x ≤ b, t ∈ [0, T], u(x, 0) = u0(x).
(1)
For example, f = u2 is the famous Burger,s Equation. We discretize the (x, t)-plane by choosing a
mesh with spatial size ∆X and temporal step size ∆t, and define the discrete mesh points (Xj , tn) by
Xj = a + j∆x, tn = n∆t with j = 0, 1,…,J = -ʌ—, n = 0, 1,…,N =	.
We denote Xj+1 = Xj + ∆x∕2 = a + (j + ɪ)∆x. The finite difference methods will produce
approximations Ujn to the solution U(xj, tn) on the given discrete mesh points. We denote point-
wise values of the true solution to be Ujn = U(xj, tn), and the true point-wise flux values to be
fjn = f(U(xj,tn)).
2.2	WENO-Weighted Essentially NON-OSCILLATORY Schemes
WENO (Weighted Essentially Non-Oscillatory) (Liu et al., 1994) is a family of high order accurate
finite difference schemes for solving hyperbolic conservation laws, and has been successful for many
practical problems. The key idea of WENO is a nonlinear adaptive procedure that automatically
chooses the smoothest local stencil to reconstruct the numerical flux. Generally, a finite difference
method solves Eq.1 by using a conservative approximation to the spatial derivative of the flux:
dUj (t)
dt
∆1x fj+2 - fj-2
(2)
—
where Uj (t) is the numerical approximation to the point value U(Xj, t) and ji is the numerical
flux generated by a numerical flux policy
0
fj+2
π (ujr, ..., uj+s),
which is manually designed. Note that the term “numerical flux policy" is a new terminology that we
introduce in this paper, which is exactly the policy we shall learn using RL. in WENO, πf works
as follows. Using the physical flux values {fj-2, fj-1, fj}, we could obtain a 3th order accurate
2
polynomial interpolation f-+ι, where the indices {j - 2,j - 1,j} is called a Stencil. We could also
use the stencil {j-1,j, j+1}, {j, j+1, j+2} or {j+1, j+2,j+3} to obtain another three interpolants
1 i∙1	rɪ-il 1 ♦ 1	∕' -rʃ TT-ITL T z ʌ ♦ .	Z ∙ .1	1 1	1 ♦ 1 , 、 1 1
i and f ɪ i. The key idea of WENO is to average (with properly designed weights) all
2	j+ 2
11
these interpolants to obtain the final reconstruction: ji = Er=-2 Wrfj+1/2， Σr=-2 Wr = 1.
The weight wi depends on the smoothness of the stencil. A general principal is: the smoother is the
stencil, the more accurate is the interpolant and hence the larger is the weight. To ensure convergence,
we need the numerical scheme to be consistent and stable (LeVeque, 1992). it is known that WENO
schemes as described above are consistent. For stability, upwinding is required in constructing the
flux. The most easy way is to use the sign of the Roe speed αj+1 = (j i
1”(Uj+1 - Uj-2)
- fj
2
to determine the upwind direction: if aj+1 ≥ 0, we only average among the three Interpolants f—i,
f-12 and f0+ 2; if j 1 < 0, We use fj+12
i and f1+1.
2	j+ 2
Some further thoughts. WENO achieves optimal order of accuracy (up to 5) at the smooth region
of the solutions (Shu, 1998), while lower order of accuracy at singularities. The key of the WENO
method lies in how to compute the weight vector (w1 , w2 , w3 , w4), which primarily depends on
the smoothness of the solution at local stencils. in WENO, such smoothness is characterized by
handcrafted formula, and was proven to be successful in many practical problems when coupled with
high-order temporal discretization. However, it remains unknown whether there are better ways to
combine the stencils so that optimal order of accuracy in smooth regions can be reserved while, at the
4
Under review as a conference paper at ICLR 2020
1
2
3
4
5
6
7
8
9
same time, higher accuracy can be achieved near singularities. Furthermore, estimating the upwind
directions is another key component of WENO, which can get quite complicated in high-dimensional
situations and requires lots of logical judgments (i.e. “if/else"). Can we ease the (some time painful)
coding and improve the estimation at the aid of machine learning?
3	Methods
In this section we present how to employ reinforcement learning to solve the conservation laws
given by Eq.1. To better illustrate our idea, we first show in general how to formulate the process of
numerically solving a conservation law into an MDP. We then discuss how to incorporate a policy
network with the WENO scheme. Our policy network targets at the following two key aspects of
WENO: (1) Can we learn to choose better weights to combine the constructed fluxes? (2) Can
we learn to automatically judge the upwind direction, without complicated logical judgments?
3.1	MDP Formulation
Algorithm 1: A Conservation LaW Solving Procedure
Input: initial values u00, u10, ..., u0J, flux f (u), ∆x, ∆t, evolve time N, left shift r and right shift s.
Output: {Ujn | j = 0, ..., J, n = 1, ..., N}
Uj0 = uj0 , j = 0, ..., J
for n = 1 to N do
for j = 0 to J do
Cnmniitp thp nιιmprir∩1 flnγ ^n 一 Ff(TTn-1 J^Γn-1 Un-I ∖ Cnrl ^n 一 Ff(TTn-1
ComPUte Ihe numerical flux f- ι 一 π (Uj-r-ι, Uj-r ,…，uj+s-ι) and f+1 一 ∏ (Uj-r ,
Ujn--r1+1, ..., Ujn+-s1), e.g., using the WENO scheme
Γ''r∖mv4∏tα du j (t)	1 ^ ^n	^n ∖
COmPUte -j = - ∆X (fj+2 - fj-2)
Compute Ujn = πt(Un-1, dudy)), e.g., using the Euler scheme Ujn = UnT + ∆tdud?)
Return {Un | j = 0,…，J, n = 1,…,N}
As shoWn in Algorithm 1, the Procedure of numerically solving a conservation laW is naturally a
sequential decision making Problem. The key of the Procedure is the numerical flux Policy πf and
the temPoral scheme πt as shoWn in line 6 and 8 in Algorithm 1. Both Policies could be learned using
RL. HoWever, in this PaPer, We mainly focus on using RL to learn the numerical flux Policy πf , While
leaving the temPoral scheme πt With traditional numerical schemes such as the Euler scheme or the
Runge-Kutta methods. A quick review of RL is given in the appendix.
NoW, We shoW hoW to formulate the above Procedure as an MDP and the construction of the state S,
action A, reward r and transition dynamics P. Algorithm 2 shows in general how RL is incorporated
into the procedure. In Algorithm 2, we use a single RL agent. Specifically, when computing Ujn :
•	The state for the RL agent is sjn = gs(Ujn--r1-1, ..., Ujn+-s1), where gs is the state function.
•	In general, the action of the agent IS used to determine how the numerical fluxes f 晨 ι and f n_ ι IS
j+2	j-2
computed. In the next subsection, we detail how we incorporate ajn to be the linear weights of the
fluxes computed using different stencils in the WENO scheme.
•	The reward should encourage the agent to generate a scheme that minimizes the error between
its approximated value and the true value. Therefore, we define the reward function as rjn =
gr (Ujn-r-1 - Un-r-1, …，Un+s -un+s), e.g., a simplest choice is gr = -∣∣∙ ∣∣2.
•	The transition dynamics P is fully deterministic, and depends on the choice of the temporal
scheme at line 10 in Algorithm 2. Note that the next state can only be constructed when we have
obtained all the point values in the next time step, i.e., sjn+1 = gs(Ujn-r-1, ..., Ujn+s) does not
only depends on action ajn, but also on actions ajn-r-1, ..., ajn+s (action ajn can only determine
the value Ujn). This subtlety can be resolved by viewing the process under the framework of
multi-agent RL, in which at each mesh point j we use a distinct agent AjRL, and the next state
sjn+1 = gs(Ujn-r-1, ..., Ujn+s) depends on these agents’ joint action ajn = (ajn-r-1, ..., ajn+s).
5
Under review as a conference paper at ICLR 2020
However, it is impractical to train J different agents as J is usually very large, therefore we enforce
the agents at different mesh point j to share the same weight, which reduces to case of using just a
single agent. The single agent can be viewed as a counterpart of a human designer who decides on
the choice of a local scheme based on the current state in traditional numerical methods.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Algorithm 2: General RL Running Procedure
Input: initial values u00 , ..., u0J, flux f (u), ∆x, ∆t, evolve time N, left shift r, right shift s and RL policy πRL
Output: {Ujn| j = 0,..., J, n = 1, ..., N}
Uj0 = uj0 , j = 0, ..., J
for Many iterations do
Construct initial states sj0 = gs(Uj0-r-1, ..., Uj0+s) for j = 0, ..., J
for n = 1 to N do
for j = 0 to J do
n RL n	n	n
Compute the action a； = π	(Sn) that determines how fn+1 and fjn-ι is computed
Cnmmitp duj(t) ——	1 (Fn ^n ∖
ComPUte -^r- = - ∆X (fj+ 2 - fj- 2 )
Compute Uj = πt(Un-1, djt)), e.g., the EUler scheme Uj = UnT + ∆t djt)
Compute the reward rj = gr(Un-r-ι — Un-『_、,…，Un+§ — uj+s).
Construct the next states sjn+1 = gs (ujn-r-1, ..., ujn+s) for j = 0, ..., J
Use any RL algorithm to train the RL policy πRL with the transitions {(sjj,aj ,rn sn+1)}J=0.
Return the well-trained RL policy πRL .
3.2 RL Empowered WENO
We now present how to transfer the actions of the RL policy to the weights of WENO fluxes. Instead
of directly using πRL to generate the numerical flux, we use it to produce the weights of numerical
fluxes computed using different stencils in WENO. Since the weights are part of the configurations of
the WENO scheme, our design of action essentially makes the RL policy a meta-learner, and enables
more stable learning and better generalization power than directly generating the fluxes.
Specifically, at point xj (here we drop the time superscript n for simplicity), to compute the numerical
flux fj-1 and j 1, we first construct four fluxes {f i_ ɪ }1=-2 and {fi, 1 }1=-2 using four different
2	2	2	j - 2	j + 2
stencils just as in WENO, and then use the RL policy πRL to generate the weights of these fluxes:
πRL(sj ) = (W-2 * 4 2 ,w-- 2 ,wj-1 ,wj-1 ,w-+2 ,w-+ 2 ,wj+1 ,w1+1).
1
The numerical flux is then constructed by averaging these fluxes: f∙-1 = Ei=-2 wi-1 fj_1, and
O	.一. 1	.-	0.-
fj+2 = Pi=-2 wj+2 fj+1.
Note that the determination of upwind direction is automatically embedded in the RL policy since
it generates four weights at once. For instance, when the roe speed aj+1 ≥ 0, we expect the 4th
weight w1 1 ≈ 0 and when ©.+1 < 0, we expect w-21 ≈ 0. Note that the upwind direction can
j+2	2' 2	j+2
be very complicated in a system of equations or in the high-dimensional situations, and using the
policy network to automatically embed such a process could save lots of efforts in algorithm design
and implementation. Our numerical experiments show that π RL can indeed automatically determine
upwind directions for 1D scalar cases. Although this does not mean that it works for systems and/or
in high-dimensions, it shows the potential of the proposed framework and value for further studies.
4	Experiments
In this section, we describe training and testing of the proposed RL conservation law solver and
compare it with WENO. More comparisons and discussions can be found in the appendix.
6
Under review as a conference paper at ICLR 2020
4.1	Setup
In this subsection, we explain the general training setup. We train the RL policy network on the
Burger's equation, whose flux is computed as f (U) = 2u2. In all the experiments, We set the left-shift
r = 2 and the right shift s = 3. The state function gs(sj) = gs(Uj-r-1, ..., Uj+s) will generate two
vectors: Sl = (fj-r-ι, ...,fj+s-ι, aj-1), and Sr = (fj-,…，fj+s,%+ ι) for computing fj— ι and
fj+1 respectively. Sl and Sr will be passed into the same policy neural network ∏RL to produce the
desired actions, as described in section 3.2. The reward function gr simply computes the infinity
norm, i.e., gr ( Uj—r — 1 - uj—r — 1,…，Uj+s - Uj+s ) = - || (Uj—r — 1 - uj—r — 1,…，Uj+s - Uj+s ) U∞ .
The policy network πθRL is a feed-forward Multi-layer Perceptron with 6 hidden layers, each has
64 neurons and use Relu (Goodfellow et al., 2016) as the activation function. We use the Deep
Deterministic Policy Gradient Algorithm (Lillicrap et al., 2015) to train the RL policy.
To guarantee the generalization power of the trained RL agent, we randomly sampled 20 initial
conditions in the form u°(x) = a + b ∙ func(cπx), where |a| + |b| ≤ 3.5, func ∈ {sin, cos} and
c ∈ {2, 4, 6}. The goal of generating such kind of initial conditions is to ensure they have similar
degree of smoothness and thus similar level of difficulty in learning. The computation domain is
-1 ≤ x ≤ 1 and 0 ≤ t ≤ 0.8 with ∆x = 0.02, ∆t = 0.004, and evolve steps N = 200 (which
ensures the appearance of shocks). When training the RL agent, we use the Euler scheme for temporal
discretization. The true solution needed for reward computing is generated using WENO on the same
computation domain with ∆x = 0.001, ∆t = 0.0002 and the 4th order Runge-Kutta (RK4).
In the following, we denote the policy network that generates the weights of the WENO fluxes (as
described in section 3.2) as RL-WENO. We randomly generated another different 10 initial conditions
in the same form as training for testing.
∆x △L^^		0.02				0.04				0.05		
	RL-WENO	WENO	RL-WENO	WENO	RL-WENO	WENO
0.002	5.66 (1.59)	5.89 (1.74)	8.76 (2.50)	9.09 (2.62)	9.71 (2.42)	10.24 (2.84)
-0.003-	5.64(1.54)	5.86 (1.67)	8.73 (2.46)	9.06 (2.58)	9.75 (2.41)	10.28(2.81)
-0.004-	5.63 (1.55)	5.81 (1.66)	8.72 (2.46)	9.05 (2.55)	9.61 (2.42)	10.13 (2.84)
-0.005-	5.08 (1.46) —	5.19 (1.58)	8.29 (2.34)	8.58 (2.47)	9.30 (2.26)	9.78 (2.69)
-0.006-	-	-	8.71(2.49)	9.02 (2.61)	9.72 (2.38)	10.24 (2.80)
-0.007-	-	-	8.56 (2.49)	8.84 (2.62)	9.59 (2.41)	10.12(2.80)
0.008	-	-	8.68 (2.55) —	8.93 (2.66)-	9.57 (2.49)-	10.06 (2.92「
Table 1: Comparison of relative errors (× 10—2) of RL-WENO and WENO with standard deviations
of the errors among 10 trials in the parenthesis. Temporal discretization: RK4; flux function: ɪu2.
∆x △t^^		0.02				0.04				0.05		
	RL-WENO	WENO	RL-WENO	WENO	RL-WENO	WENO
0.002	4.85 (1.15) —	5.17(1.26)-	7.77 (1.95)	8.05 (2.02)	8.16(1.93)	8.56(2.19)
-0.003-	-	-	7.79 (1.96)	8.06 (2.03)	8.18(1.92)	8.59 (2.18)
-0.004-	-	-	7.72(1.93) —	7.98(2.01)-	8.15 (1.95)	8.54 (2.20)
0.005	-	-	-	-	8.18(1.94) —	8.55(2.15)-
Table 2: Comparison of relative errors (× 10—2) of RL-WENO and WENO with standard deviations
of the errors among 10 trials in the parenthesis. Temporal discretization: RK4; flux function:看u4.
4.2	Results
We compare the performance of RL-WENO and WENO. We also test whether the trained RL policy
can generalize to different temporal discretization schemes, mesh sizes and flux functions that are not
included in training. Table 1 and Table 2 present the comparison results, where the number shows the
relative error (computed as ||U-U||2 with the 2-norm taking over all x) between the approximated
solution U and the true solution U, averaged over 250 evolving steps (T = 1.0) and 10 random initial
values. Numbers in the bracket shows the standard deviation over the 10 initial conditions. Several
entries in the table are marked as ‘-’ because the corresponding CFL number is not small enough
7
Under review as a conference paper at ICLR 2020
to guarantee convergence. Recall that training of the RL-WENO was conducted with Euler time
discretization, (∆x, ∆t) = (0.02, 0.004), T = 0.8 and f(u) = 2u2.
Our experimental results show that, compared with the high order accurate WENO (5th order accurate
in space and 4th order accurate in time), the linear weights learned by RL not only achieves smaller
errors, but also generalizes well to: 1) longer evolving time (T = 0.8 for training and T = 1.0 for
testing); 2) new time discretization schemes (trained on Euler, tested on RK4); 3) new mesh sizes
(see Table 1 and Table 2 for results of varied ∆x and ∆t); and 4) a new flux function (trained on
f (U) = 1 u2 shown in Table 1, tested on Ju4 Table 2).
Figure 1 shows some examples of the solutions. As one can see, the solutions generated by RL-WENO
not only achieve the same accuracy as WENO at smooth regions, but also have clear advantage over
WENO near singularities which is particularly challenging for numerical PDE solvers and important
in applications. Figure 2 shows that the learned numerical flux policy can indeed correctly determine
upwind directions and generate local numerical schemes in an adaptive fashion. More interestingly,
Figure 2 further shows that comparing to WENO, RL-WENO seems to be able to select stencils
in a different way from it, and eventually leads to a more accurate solution. This shows that the
proposed RL framework has the potential to surpass human experts in designing numerical schemes
for conservation laws.
(C) f (U) = Ju4
dx: 0.02 dt: 0.002 T 0.06
(d) f (U) = 11 u4
dx: 0.02 dt: 0.002 T 0.40
Figure 1: First row: solutions of RL-WENO (red), WENO (blue) and exact solutions (green). Second
row: zoom-in views corresponding to the first row.
(g)f (U) = 16 u4
(h) f (U) = 1⅛ u4
Figure 2: This figure compares the weights generated by the learned numerical flux policy π RL and
those of WENO. The weights shown in (a) are {wjr- 1 }1=-2; while those in (b) are {wjr+1 }1=-2∙
In each of the two plots, the 4 numbers in the upper bracket of each location are the weights of RL-
WENO and those in the lower bracket are the weights of WENO. The relative errors of RL-WENO
and WENO are 8.0 × 10-3 and 2.5 × 10-2 respectively.
8
Under review as a conference paper at ICLR 2020
5	Conclusion
In this paper, we proposed a general framework to learn how to solve 1-dimensional conservation
laws via deep reinforcement learning. We first discussed how the procedure of numerically solving
conservation laws can be naturally cast in the form of Markov Decision Process. We then elaborated
how to relate notions in numerical schemes of PDEs with those of reinforcement learning. In
particular, we introduced a numerical flux policy which was able to decide on how numerical flux
should be designed locally based on the current state of the solution. We carefully design the action
of our RL policy to make it a meta-learner. Our numerical experiments showed that the proposed RL
based solver was able to outperform high order WENO and was well generalized in various cases.
As part of the future works, we would like to consider using the numerical flux policy to inference
more complicated numerical fluxes with guaranteed consistency and stability. Furthermore, we can
use the proposed framework to learn a policy that can generate adaptive grids and the associated
numerical schemes. Lastly, we would like consider system of conservation laws in 2nd and 3rd
dimensional space.
9
Under review as a conference paper at ICLR 2020
References
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient
descent. In Advances in NeUral Information Processing Systems, pp. 3981-3989, 2016.
Christian Beck, E Weinan, and Arnulf Jentzen. Machine learning approximation algorithms for high-
dimensional fUlly nonlinear partial differential eqUations and second-order backward stochastic
differential equations. JoUrnal OfNonlinear Science, pp. 1-57, 2017.
Samy Bengio, YoshUa Bengio, Jocelyn CloUtier, and Jan Gecsei. On the optimization of a synaptic
learning rule. In PrePrintS Conf. OPtimaIity in ArtifiCial and BioIogiCal NeUral Networks, pp. 6-8.
Univ. ofTexas, 1992.
Gert-Jan Both, Subham Choudhury, Pierre Sens, and Remy Kusters. Deepmod: Deep learning for
model discovery in noisy data. arXiv PrePrint arXiv:1904.09406, 2019.
Bo Chang, Lili Meng, Eldad Haber, Frederick Tung, and David Begert. Multi-level residual networks
from dynamical systems view. arXiv PrePrint arXiv:1710.10348, 2017.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differen-
tial equations. In AdVanCeS in NeUral Information ProCeSSing Systems, pp. 6571-6583, 2018.
Niccolo’ Discacciati, Jan S Hesthaven, and Deep Ray. Controlling oscillations in high-order discon-
tinuous galerkin schemes using artificial viscosity tuned by neural networks. Technical report,
2019.
Qingnan Fan, Dongdong Chen, Lu Yuan, Gang Hua, Nenghai Yu, and Baoquan Chen. Decouple
learning for parameterized image operators. In ProCeedingS of the EUroPean Conference on
ComPUter ViSion (ECCV), pp. 442T58, 2018a.
Yuwei Fan, Lin Lin, Lexing Ying, and Leonardo Zepeda-Nunez. A mUltiscale neUral network based
on hierarchical matrices. arXiv PrePrint arXiv:1807.01883, 2018b.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In ProCeedingS of the 34th InternationaI ConferenCe on MaChine Learning-Volume
70,pp. 1126-1135.JMLR. org, 2017.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. DeeP Iearning. MIT press, 2016.
Jiequn Han, Arnulf Jentzen, and E Weinan. Solving high-dimensional partial differential equations
using deep learning. ProceedingS of the National ACademy of Sciences, 115(34):8505-8510,
2018.
Silvia Jerez Galiano and Miguel Uh Zapata. A new tvd flux-limiter method for solving nonlinear
hyperbolic equations. JoUrnal of ComPUtationaI and APPIied Mathematics, 234(5):1395-1403,
2010.
Meiguang Jin, Stefan Roth, and Paolo Favaro. Noise-blind image deblurring. In ProceedingS of the
IEEE ConferenCe on ComPUter ViSion and Pattern Recognition, pp. 3510-3518, 2017.
Yuehaw Khoo and Lexing Ying. Switchnet: a neural network model for forward and inverse scattering
problems. arXiv PrePrint arXiv:1810.09675, 2018.
Yuehaw Khoo, Jianfeng Lu, and Lexing Ying. Solving parametric pde problems with artificial neural
networks. arXiv PrePrint arXiv:1707.03351, 2017.
Randall J LeVeque. NUmeriCaI methods for ConSerVation laws, volume 132. Springer, 1992.
Randall J LeVeque. Finite VoIUme methods for hyperbolic problems, volume 31. Cambridge univer-
sity press, 2002.
Ke Li and Jitendra Malik. Learning to optimize. arXiv PrePrint arXiv:1606.01885, 2016.
10
Under review as a conference paper at ICLR 2020
Yingzhou Li, Jianfeng Lu, and Anqi Mao. Variational training of neural network approximations of
solution maps for physical models. arXiv Preprint arXiv:1905.02789, 2019.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
PrePrint arXiv:1509.02971, 2015.
Xu-Dong Liu, Stanley Osher, and Tony Chan. Weighted essentially non-oscillatory schemes. JoUrnaI
of computational physics, 115(1):200-212, 1994.
Zichao Long, Yiping Lu, and Bin Dong. Pde-net 2.0: Learning pdes from data with a numeric-
symbolic hybrid deep network. arXiv PrePrint arXiv:1812.04426, 2018a.
Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net: Learning pdes from data. In ICML,
2018b.
Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond finite layer neural networks:
Bridging deep architectures and numerical differential equations. In ICML, 2018.
Jim Magiera, Deep Ray, Jan S Hesthaven, and Christian Rohde. Constraint-aware neural networks
for riemann problems. arXiv PrePrint arXiv:1904.12794, 2019.
StePhane G Mallat and Zhifeng Zhang. Matching pursuits with time-frequency dictionaries. IEEE
TranSaCtiOnS on signal PrOCeSsing, 41(12):3397-3415, 1993.
Craig Michoski, Milos Milosavljevic, Todd Oliver, and David Hatch. Solving irregular and data-
enriched differential equations using deep neural networks. arXiv PrePrint arXiv:1905.04351,
2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Tong Qin, Kailiang Wu, and Dongbin Xiu. Data driven governing equations approximation using
deep neural networks. arXiv PrePrint arXiv:1811.05537, 2018.
Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics informed deep learning (part i):
Data-driven solutions of nonlinear partial differential equations. arXiv PrePrint arXiv:1711.10561,
2017a.
Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics informed deep learning (part ii):
Data-driven discovery of nonlinear partial differential equations. arXiv PrePrint arXiv:1711.10566,
2017b.
Deep Ray and Jan S Hesthaven. An artificial neural network as a troubled-cell indicator. JOUmaI of
Computational Physics, 367:166-191, 2018.
Jurgen Schmidhuber. Evolutionary principles in self-referential learning. On Iearning how to learn:
The meta-meta-... hook.) Diploma thesis, InStitUt f.Informatik, Tech. Univ. Munich, 1987.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In PrOCeedingS of the 32nd International COnferenCe on MaChine Learning
(ICML-15), pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv PrePrint arXiv:1707.06347, 2017.
Chi-Wang Shu. Essentially non-oscillatory and weighted essentially non-oscillatory schemes for
hyperbolic conservation laws. In AdVanCed numerical approximation of nonlinear hyperbolic
equations, pp. 325T32. Springer, 1998.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484^89, 2016.
11
Under review as a conference paper at ICLR 2020
JoaqUin Vanschoren. Meta-Iearning: A survey. arXiv Preprint arXiv:1810.03548, 2018.
Shiyin Wei, Xiaowei Jin, and Hui Li. General solutions for nonlinear differential equations: a
rule-based self-learning approach using deep reinforcement learning. Computational Mechanics,
ρρ.1-14, 2019.
E Weinan. A proposal on machine learning via dynamical systems. CommUniCationS in MathematiCS
and StatiStics, 5(1):1-11, 2017.
Jason Hickey Michael P. Brenner Yohai Bar-Sinai, Stephan Hoyer. Data-driven discretization:
a method for systematic coarse graining of partial differential equations. arXiv preprint
arXiv:1808.04930, 2018.
Xiaoshuai Zhang, Yiping Lu, Jiaying Liu, and Bin Dong. Dynamically unfolding recurrent restorer:
A moving endpoint control method for image restoration. ICLR, 2019.
Hongkai Zhao. A fast sweeping method for eikonal equations. MathematiCS of computation, 74
(250):603-627, 2005.
12
Under review as a conference paper at ICLR 2020
A Complementary Experiments
A. 1 Comparison with supervised learning (SL) based methods
We first note that most of the neural network based numerical PDE solvers cited in the introduction
requires retraining when the initialization, terminal time, or the form of the PDE is changed; while the
proposed RL solver is much less restricted as shown in our numerical experiments. This makes proper
comparisons between existing NN-based solvers and our proposed solver very difficult. Therefore,
to demonstrate the advantage of our proposed RL PDE solver, we would like to propose a new SL
method that does not require retraining when the test setting (e.g. initialization, flux function, etc.) is
different from the training.
However, as far as we are concerned, it is challenging to design such SL methods without formulating
the problem into an MDP. One may think that we can use WENO to generate the weights for the
stencil at a particular grid point on a dense grid, and use the weights of WENO generated from the
dense grid as the label to train a neural network in the coarse grid. But such setting has a fatal flaw
in that the stencils computed in the dense grids are very different from those in the coarse grids,
especially near singularities. Therefore, good weights on dense grids might perform very poorly on
coarse grids. In other words, simple imitation of WENO on dense grids is not a good idea. One
might also argue that instead of learning the weights of the stencils, we could instead generate the
discrete operators, such as the spatial discretization of d∂Uχj, or the temporal discretization of 唔,
the numerical fluxes fj+1 (u), fj-1 (u), etc., on a dense grid, and then use them as labels to train a
neural network in the supervised fashion on a coarse grid. However, the major problem with such
design is that there is no guarantee that the learned discrete operators obey the conservation property
of the equations, and thus they may also generalize very poorly.
After formulating the problem into a MDP, there is indeed one way that we can use back-propagation
(BP) instead of RL algorithms to optimize the policy network. Because all the computations on using
the stencils to calculate the next-step approximations are differentiable, we can indeed use SL to train
the weights. One possible way is to minimize the error (e.g. 2 norm) between the approximated and
the true values, where the true value is pre-computed using a more accurate discretization on a fine
mesh. The framework to train the SL network is described in Algorithm 3. Note that the framework
to train the SL network is essentially the same as that of the proposed RL-WENO (Algorithm 2). The
only difference is that we train the SL network using BP and the RL network using DDPG.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Algorithm 3: Using BP instead of RL algorithm to train the policy
Input: initial values u00, ..., u0J, flux f (u), ∆x, ∆t, evolve time N, left shift r, right shift s and a neural network
πθ
Output: {Ujn | j = 0, ..., J, n = 1, ..., N}
Uj0 = uj0 , j = 0, ..., J
for Many iterations do
Construct initial states sj0 = gs(Uj0-r-1, ..., Uj0+s) for j = 0, ..., J
for n = 1 to N do
for j = 0 to J do
Cnmniitp thp wpiσhtc ^nn，—2 n”n，—1 n,0,0 ”，n，1 n,-,~2 n,-,11 n”n，0 n,1jl 、_ 7rθ<cnλ
Compute the WeightS(Wj- ι , Wj- i , Wj- i , Wj- i , wj+1 , wj +1 ，wj+1，wj+1 ) 一 π (Sj )
Compute the fluxes fj- 1 = P；=. wn-Ifn-4, j 1 = P；=- wi+1 fn^, where fj±i1 are
the fluxes computed by WENO
Cnmniitp duj(t) ——	1 ^ ^n ^n ʌ
Compute ~^Γ~ 一 - ∆X (fj+1 - fj- 1 )
Compute Un = πt(Un-1, djt)), e.g., the Euler scheme Un = Un-I + ∆t djt
Compute the loss for θ:
Ln(θ) = ||(Un-r-1 - un-r-1,…，Un+s - un+s) - (Ujn-r-1 - Un-r-1,…，Un+s - Un+s)”2∙
Perform a gradient descent on θ w.r.t Ln (θ)
Construct the next states sn+1 = gs(un-r-ι,..., un+s) for j = 0,...,J
Return the BP optimized policy πθ .
13
Under review as a conference paper at ICLR 2020
However, we argue that the main drawback of using SL (BP) to optimize the stencils in such a way is
that it cannot enforce long-term accuracy and thus cannot outperform the proposed RL-WENO. To
support such claims, we have added experiments using SL to train the weights of the stencils, and
the results are shown in table 3 and 4. The SL policy is trained till it achieves very low loss (i.e.,
converges) in the training setting. However, as shown in the table, the SL-trained policy does not
perform well overall. To improve longer time stability, one may argue that we could design the loss
of SL to be the accumulated loss over multiple prediction steps, but in practice as the dynamics of
our problem (computations for obtaining multiple step approximations) is highly non-linear, thus the
gradient flow through multiple steps can be highly numerically unstable, making it difficult to obtain
a decent result.
∆χ ∆t^∖^		0.02					0.04					0,05			
	RL-WENO	SL	WENO	RL-WENO	SL	WENO	RL-WENO	SL	WENO
0.002	5.66(1.59)	7.86(1.23)	5.89 (1.74)	8.76 (2.50)	12.48(0.78)	9.09 (2.62)	9.71 (2.42)	12.14 (0.44)	10.24 (2.84)
0003	5.64 (1.54)	7.77 (1.26)	5.86(1.67)	8.73 (2.46)	12.44 (0.78)	9.06 (2.58)	9.75 (2.41)	12.13 (0.41)	10.28 (2.81)
0004	5.63 (1.55)	7.72 (1.14)	5.81 (1.66)	8.72 (2.46)	12.44 (0.64)	9.05 (2.55)	9.61 (2.42)	12.14 (0.45)	10.13 (2.84)
0005	5.08 (1.46)一	7.14 (1.37) 一	5.19(1.58) 一	8.29 (2.34)	12.06 (0.86)	8.58 (2.47)	9.30 (2.26)	11.86 (0.38)	9.78 (2.69)-
0006	-	-	-	8.71 (2.49)	12.33(0.73)	9.02 (2.61)	9.72 (2.38)	12.14 (0.41)	10.24 (2.80)
0007	-	-	-	8.56 (2.49)	12.29(0.83)	8.84 (2.62)	9.59 (2.41)	12.06 (0.45)	10.12 (2.80)
0.008 —	-	-	-	8.68 (2.55)一	12.22 (0.70)-	8.93 (2.66) 一	9.57 (2.49)	12.08 (0.46)-	10.06 (2.92~
Table 3: Comparison of relative errors (×10-2) of RL-WENO, WENO, and SL-trained policy with
standard deviations of the errors among 10 trials in the parenthesis. Temporal discretization: RK4;
flux function: 1 u2. RL-Weno consistently outperforms WENO and SL-trained policy in all test cases.
∆x ∆t"∖^		 0.02						 0.04						 0.05				
	RL-WENO	SL	WENO	RL-WENO	SL	WENO	RL-WENO	SL	WENO
0.002	4.85(1.15) 一	5.84 (0.79) 一	5.17 (1.26)一	7.77 (1.95)	8.60(1.12)	8.05 (2.02)	8.16(1.93)	8.42 (1.00)	8.56(2.19)
0.003	-	-	-	7.79 (1.96)	8.62 (1.12)	8.06 (2.03)	7.70(1.96)	8.42 (0.98)	8.59(2.18)
0004	-	-	-	7.72 (1.93)一	8.55(1.15) 一	7.98 (2.01)	8.15(1.95)	8.41 (1.02)	8.54 (2.20)
0.005 —	-	-	-	-	-	-	8.18 (1.94)一	8.40(1.03) 一	8.55(2.15) 一
Table 4: Comparison of relative errors (×10-2) of RL-WENO, WENO, and SL-trained policy with
standard deviations of the errors among 10 trials in the parenthesis. Temporal discretization: RK4;
flux function: 1 u4. RL-weno consistently outperforms WENO and SL-trained policy in all test cases.
A.2 RL-weno’ s performance on smooth and singular regions
As mentioned in section 2.2, WENO itself already achieves an optimal order of accuracy in the
smooth regions. Since RL-WENO can further improve upon WENO, it must have obtained higher
accuracy especially near singularities. Here we provide additional demonstrations on how RL-WENO
performs in the smooth/singular regions. We run RL-WENO and WENO on a set of initial conditions,
and record the approximation errors at every locations and then separate the errors in the smooth
and singular regions for every time step. We then compute the distribution of the errors on the entire
spatial-temporal grids with multiple initial conditions. The results are shown in figure 3. In figure 3,
the x-axis is the logarithmic (base 10) value of the error and the y-axis is the number of grid points
whose error is less than the corresponding value on the x-axis, i.e., the accumulated distribution of
the errors. The results show that RL-WENO indeed performs better than WENO near singularities.
RL-WENO even achieves better accuracy than WENO in the smooth region when the flux function is
⅛ u4∙
A.3 Inference Time of RL-WENO and WENO
In this subsection we report the inference time of RL-WENO and WENO. Although the computation
complexity of the trained RL policy (a MLP) is higher than that of WENO, we could parallel and
accelerate the computations using GPU.
Our test is conducted in the following way: for each grid size ∆x, we fix the initial condition as
u0(x) = 1 + cos(6πx), the evolving time T = 0.8 and the flux function f = u2. We then use
RL-WENO and WENO to solve the problem 20 times, and report the average running time. For
completeness, we also report the relative error of RL-WENO and WENO in each of these grid sizes
in table 6. Note that the relative error is computed on average of several initial functions, and our
RL-WENO policy is only trained on grid (∆x, ∆t) = (0.02, 0.004).
14
Under review as a conference paper at ICLR 2020
Figure 3: These figures show the total number of grids whose error is under a specific value (i.e. the
accumulated distribution function). The x-axis is the error in logarithmic (base 10) scale. (a) and (c)
show the distribution in smooth regions, (b) and (d) are near singularities.
For RL-WENO, we test it on both CPU and on GPU; For WENO, we test it purely on CPU, with a
well-optimized version (e.g., good numpy vectorization in python), and a poor-implemented version
(e.g., no vectorization, lots of loops). The CPU used for the tests is a custom Intel CORE i7, and the
GPU is a custom NVIDIA GTX 1080. The results are shown in table 5.
(∆x, ∆t)	RL-WENO(CPU)	RL-WENO(GPU)	WENO-optimized	WENO-Poor
(0.02, 0.004)	2.490	1650	0.148	2739
(0.01, 0.002)	7.720	1.700	0.349	10.778
(0.005, 0.001)	26.70	1.628	0.921	44.23
(0.002, 0.0004)	110.92	1.611	1.961	277.88
Table 5: Average inference time (in seconds) for RL-WENO and WENO. Bold numbers are the
smallest ones.
(∆x, ∆t)	RL-WENO error	WENO error
(0.02, 0.004)	3.73(0.40)	4.08(0.23)
(0.01, 0.002)	1.86(0.17)	1.99(0.12)
(0.005, 0.001)	1.00(0.05)	0.93(0.01)
(0.002, 0.0004)	0.48(0.03)	0.39(0.02)
Table 6: Relative error of RL-WENO and WENO (×10-2) on grid sizes tested in table 5. Note
RL-WENO is only trained on grid (∆x, ∆t) = (0.02, 0.004)
From the table we can tell that as ∆x decreases, i.e., as the grid becomes denser, all methods, except
for the RL-WENO (GPU), requires significant more time to finish the computation. The reason that
the time cost of the GPU-version of RL-WENO does not grow is that on GPU, we can compute
15
Under review as a conference paper at ICLR 2020
all approximations in the next step (i.e., to compute (U0t+1, U1t+1, ..., UJt+1) given (U0t, U1t, ..., UJt ),
which dominates the computation cost of the algorithm) together in parallel. Thus, the increase
of grids does not affect much of the computation time. Therefore, for coarse grid, well-optimized
WENO indeed has clear speed advantage over RL-WENO (even on GPU), but on a much denser grid,
RL-WENO (GPU) can be faster than well-optimized WENO by leveraging the paralleling nature of
the algorithm.
B Review of Reinforcement Learning
B.1 Reinforcement Learning
Reinforcement Learning (RL) is a general framework for solving sequential decision making problems.
Recently, combined with deep neural networks, RL has achieved great success in various tasks such
as playing video games from raw screen inputs (Mnih et al., 2015), playing Go (Silver et al., 2016),
and robotics control (Schulman et al., 2017). The sequential decision making problem RL tackles is
usually formulated as a Markov Decision Process (MDP), which comprises five elements: the state
space S, the action space A, the reward r : S × A → R, the transition probability of the environment
P : S × A × S → [0, 1], and the discounting factor γ. The interactions between an RL agent and the
environment forms a trajectory τ = (s0, a0, r0, ..., sT, aT, rT, ...). The return of τ is the discounted
sum of all its future rewards:
∞
G(τ) = Xγtrt
t=0
Similarly, the return of a state-action pair (st, at) is:
∞
G(st, at) = X γl-trl
l=t
A policy π in RL is a probability distribution on the action A given a state S: π : S × A → [0, 1]. We
say a trajectory τ is generated under policy π if all the actions along the trajectory is chosen following
π, i.e., T 〜π means at 〜∏(∙∣st) and st+ι 〜P(∙∣st, at). Given a policy π, the value of a state S is
defined as the expected return of all the trajectories when the agent starts at s and then follows π:
Vn(S) = ET[G(τ)∣τ(so) = s,τ 〜π]
Similarly, the value of a state-action pair is defined as the expected return of all trajectories when the
agent starts at S, takes action a, and then follows π:
Qπ(s,a) = Eτ[G(τ)∣τ(so) = s,τ(a0) = a,τ 〜π]
As aforementioned in introduction, in most RL algorithms the policy π is optimized with regards to
the values Qπ (s, a), thus naturally guarantees the long-term accumulated rewards (in our setting, the
long-term accuracy of the learned schemes). Bellman Equation, one of the most important equations
in RL, connects the value of a state and the value of its successor state:
Q (s,a)= r(s,a)+ YEsO〜P(∙∣s,a),a0〜π(∙∣s0)[Q (s , a )]
Vn (S) = Ea 〜π(∙∣s),s0 〜P (∙∣s0,a)[r(S,a) + YV"(SO)]
The goal of RL is to find a policy π to maximize the expected discounted sum of rewards starting
from the initial state s0, J(π) = Es。〜ρ[Vπ(s0)], where P is the initial state distribution. If We
parameterize π using θ, then we can optimize it using the famous policy gradient theorem:
-=E = Es 〜ρπθ ,a 〜∏θU logπθ (als)Qπθ (s,a)]
dθ
where ρπθ is the state distribution deduced by the policy πθ. In this paper we focus on the case where
the action space A is continuous, and a lot of mature algorithms has been proposed for such a case,
e.g., the Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al., 2015), the Trust Region Policy
Optimization algorithm (Schulman et al., 2015), and etc.
16