Under review as a conference paper at ICLR 2020
Learning Reusable Options for Multi-Task
Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Reinforcement learning (RL) has become an increasingly active area of research
in recent years. Although there are many algorithms that allow an agent to solve
tasks efficiently, they often ignore the possibility that prior experience related to
the task at hand might be available. For many practical applications, it might
be unfeasible for an agent to learn how to solve a task from scratch, given that
it is generally a computationally expensive process; however, prior experience
could be leveraged to make these problems tractable in practice. In this paper,
we propose a framework for exploiting existing experience by learning reusable
options. We show that after an agent learns policies for solving a small number
of problems, we are able to use the trajectories generated from those policies to
learn reusable options that allow an agent to quickly learn how to solve novel and
related problems.
1	Introduction
Reinforcement learning (RL) techniques have experienced much of their success in simulated en-
vironments, such as video games (Mnih et al., 2015) or board games (Silver et al., 2016; Tesauro,
1995). One of the main reasons why RL has worked so well in these applications is that we are able
simulate millions of interactions with the environment in a relatively short period of time. In many
real world applications, however, where the agent interacts with the physical world, it might not be
easy to generate such a large number of interactions. The time and cost associated with training such
systems could render RL an unfeasible approach for training in large scale.
As a concrete example, consider training a large number of humanoid robots (agents) to move
quickly, as in the Robocup competition (Farchy et al., 2013). Although the agents have similar
dynamics, subtle variations mean that a single policy shared across all agents would not be an ef-
fective solution. Furthermore, learning a policy from scratch for each agent is too data-inefficient to
be practical. As shown by Farchy et al. (2013), this type of problem can be addressed by leveraging
the experience obtained from solving a related task (e.g., walking) to quickly learn a policy for each
individual agent that is tailored to anew task (e.g., running). These situations also occurs in industry,
such as robots tasked with sorting items in fulfillment centers. A simple approach, like using PD
controllers, would fail to adapt to the forces generated from picking up objects with different weight
distributions, causing the arm to drop the objects. RL is able to mitigate this problem by learning a
policy for each arm that is able to make corrections quickly, which is tailored to the robot’s dynam-
ics. However, training a new policy for each agent would be far too costly to be a practical solution.
In these scenarios, it is possible to use a small number of policies learned a subset of the agents, and
then leverage the experience obtained from learning those policies to allow the remaining agents to
quickly learn their corresponding policies. This approach can turn problems that are prohibitively
expensive to solve into relatively simple problems.
To make use of prior experience and improve learning on new related problems in RL, several lines
of work, which are complementary to each other, have been proposed and are actively being studied.
Transfer learning (Taylor & Stone, 2009) refers to the problem of adapting information acquired
while solving one task to another. One might consider learning a mapping function that allows for
a policy learned in one task to be used in a different task (Ammar et al., 2015) or simply learn a
mapping of the value function learned in one task to another (Taylor et al., 2007). These techniques
can be quite effective, but are also limited in that they consider mapping information from one source
task to another target task. Another approach to reusing prior knowledge is through meta learning
1
Under review as a conference paper at ICLR 2020
or learning to learn (Schmidhuber, 1995; Schmidhuber et al., 1998). In the context of RL, the goal
under this framework for an agent to be exposed to a number of tasks where it can learn some general
behavior that generalizes to new tasks (Finn et al., 2017).
One last technique to leverage prior experience, and the one this paper focuses on, is through tem-
porally extended actions or temporal abstractions (McGovern & Sutton, 1998; Sutton et al., 1999).
While in the standard RL framework the agent has access to a set of primitive actions (i.e., actions
that last for one time step), temporally extended actions allow an agent to execute actions that last
for several time-steps. They introduce a bias in the behavior of the agent which, if appropriate for the
problem at hand, results in dramatic improvements in how quickly the agent learns to solve a new
task. A popular representation for temporally extended actions is the options framework (Sutton &
Precup, 1998; Sutton et al., 1999) (formally introduced in the next section), which is the focus of
this work. It has been shown that options learned in a specific task or set of tasks, can be reused
to improve learning on new tasks (Machado et al., 2017; Bacon et al., 2017); however, this often
requires knowledge from the user about which options or how many options are appropriate for the
type of problems the agent will face.
In this paper, we propose learning reusable options for a set of related tasks with minimal infor-
mation provided by the user. Throughout this paper, we refer as (near)-optimal policies to those
policies that were learned to solve a particular task, but are not strictly speaking optimal. We con-
sider the scenario where the agent must solve a large numbers of tasks and show that after learning
a (near)-optimal policy for a small number of problems, we can learn an appropriate number of
options that facilitates learning in a remaining set of tasks. To do so, we propose learning a set of
options that minimize the expected number of decisions needed to represent trajectories generated
from the (near)-optimal policies learned by the agent, while also maximizing the probability of gen-
erating those trajectories. Unlike techniques that learn options to rach bottleneck states (McGovern
& Barto, 2001) or states deemed of high value (Machado et al., 2017), our method seeks to learn
options that are able to generate trajectories known to perform well. This does not necessarily lead
to learn options that reach states one might consider “interesting”.
2	Background and Notation
A Markov decision process (MDP) is a tuple, M = (S, A, P, R, γ, d0), where S is the set of possible
states of the environment, A is the set of possible actions that the agent can take, P (s, a, s0) is
the probability that the environment will transition to state s0 ∈ S if the agent executes action
a ∈ A in state s ∈ S, R(s, a, s0) is the expected reward received after taking action a in state s
and transitioning to state s0, d0 is the initial state distribution, and γ ∈ [0, 1] is a discount factor for
rewards received in the future. We use t to index the time-step and write St , At , and Rt to denote the
state, action, and reward at time t. A policy, π : S × A → [0, 1], provides a conditional distribution
over actions given each possible state: π(s, a) = Pr(At = a|St = s). We denote a trajectory of
length t as ht = (s0, a0, r0, . . . , st-1, at-1, rt-1, st), that is, ht is defined as a sequence of states,
actions and rewards observed after following some policy for t time-steps. This work focuses on
learning options that can be used for a set of related tasks. We consider the setting where an agent
must solve a set of related tasks, where each task is an MDP, M = (S, A, PM , RM , γ, d0M); that is,
each task is an MDP with its own transition function, reward function and initial state distribution,
with shared state and action sets.
An option, o = (Zo,μ0,βo), isa tuple in which Zo ⊆ S is the set of states in which option o can be
executed (the initiation set), μo is a policy that governs the behavior of the agent while executing o,
and βo : S → [0, 1] is a termination function that determines the probability that o terminates in a
given state. We assume that Zo = S for all options o; that is, the options are available at every state.
The options framework does not dictate how an agent should choose between available options or
how options should be discovered. A common approach to selecting between options is to a learn a
policy over options, which is defined by the probability of choosing an option in a particular state.
Two recent popular approaches to option discovery are eigenoptions (Machado et al., 2017) and the
option-critic architecture (Bacon et al., 2017).
The eigenoptions (Machado et al., 2017) of an MDP are the optimal policies for a set of implicitly
defined reward functions called eigenpurposes. Eigenpurposes are defined in terms of proto-value
functions (Mahadevan, 2005), which are in turn derived from the eigenvectors of a modified adja-
cency matrix over states for the MDP. The intuition is that no matter the true reward function, the
2
Under review as a conference paper at ICLR 2020
eigenoptions allow an agent to quickly traverse the transition graph, resulting in better exploration
of the state space and faster learning. However, there are two major downsides: 1) the adjacency
matrix is often not known a priori, and may be difficult to construct for large MDPs, and 2) for
each eigenpurpose, constructing the corresponding eigenoption requires solving a new MDP. The
option-critic architecture (Bacon et al., 2017) is a more direct approach to learn options and a policy
over options simultaneously using policy gradient methods. One issue that often arises within this
framework is that the termination functions of the learned options tend to collapse to “always termi-
nate”. In a later publication, the authors built on this work to consider the case where there is a cost
associated with switching options (Harb et al., 2018). This method resulted in the agent learning to
use a single option while it was appropriate and terminate when an option switch was needed, allow-
ing it to discover improved policies for a particular task. The authors argue that minimizing the use
of the policy over options may be desirable, as the cost of choosing an option may be greater than
the cost of choosing a primitive action when using an option. Recent work by Harutyunyan et al.
(2019) approaches the aforementioned termination problem by explicitly optimizing the termination
function of options to focus on small regions of the state space. However, in contrast to the work
presented in these paper, these methods do not explicitly take into consideration that the agent might
face many related tasks in the future.
We build on the idea that minimizing the number of decisions made by an agent leads to the discov-
ery of general reusable options, and propose an offline method where they are learned by solving a
small number of tasks. The options are then leveraged to quickly solve new problems the agent will
face in the future. We use the trajectories generated while learning (near)-optimal policies, and learn
an appropriate set of options by directly minimizing the expected number of decisions the agent
makes while simultaneously maximizing the probability of generating the observed trajectories.
3	Learning Reusable Options from Experience
In this section, we introduce the objective for learning a set of reusable options for a set of related
tasks. Our algorithm introduces one option at a time until introducing a new option does not improve
the objective further. This procedure results in a natural way of learning an adequate number of
options without having to predefine it; a new option is included if it is able to improve the probability
of generating optimal behavior while minimizing the number of decisions made by the agent. Our
method assumes that the agent has learn a policy for a small number of tasks, and sample trajectories
are obtained from these (near)-optimal policies. Notice that the propose algorithm is only concerned
with being able to recreate the demonstrated trajectories, so if these were sample from a poorly
performing policy the options learned are unlikely to provide any benefits.
3.1	Problem Formulation
In the options framework, at each time-step, t, the agent chooses an action, At, based on the current
option, Ot. Let Tt be a Bernoulli random variable, where Tt = 1 if the previous option, Ot-1,
terminated at time t, and Tt = 0 otherwise. IfTt = 1, Ot is chosen using the policy over options, π.
If Tt = 0, then the previous option continues, that is, Ot = Ot-1. To ensure we can represent any
trajectory, we consider primitive actions to be options which always select one specific action and
then terminate; that is, for an option, o, corresponding to a primitive, a, for all s ∈ S , the termination
function would be given by βo(s) = 1, and the policy by μ(s, a0) = 1 if a0 = a and 0 otherwise.
Let O = OA ∪ OO denote a set of options, {o1, . . . , on}, where OA refers to the set of options
corresponding to primitive actions and OO to the set corresponding to temporal abstractions. Fur-
thermore, let H be a random variable denoting a trajectory of length |H| generated by a near-optimal
policy, and let Ht be a random variable denoting the sub-trajectory of H up to the state encountered
at time-step t. We seek to find a set, O* = {o1,..., o∕}, that maximizes the following objective:
|H|
J(∏, O) = E [ X Pr(Tt = 0, Ht ∣∏, O) + λ1g(H, Oo)],	(1)
t=1
where g(h, OO ) is a regularizer that encourages a diverse set of options, and λ1 is a scalar hyper-
parameter. If we are also free to learn the parameters of π, then O* ∈ arg max max J(π, O).
Oπ
One choice for g is the average KL divergence on a given trajectory over the set of m options
3
Under review as a conference paper at ICLR 2020
being learned: g(h, OO) = m(m-i)Po,o0∈oo Pt=-I DKL ( μo(St)|| μo0 (St)). 1 Intuitively, we
seek to find options that are capable of generating near-optimal trajectories with a small number
of terminations. Notice that minimizing the number of terminations is the same as minimizing the
number of decisions made by the policy over options, as each termination requires the policy to
choose a new option. Given a set of options, a policy over options, and a near-optimal sample
trajectory, we can calculate the joint probability for a trajectory exactly, and estimate equation 1 by
averaging over a set of near-optimal trajectories.
3.2 Optimization Objective for Learning Options
Given that the agent must solve a set of tasks, we can use the experienced gathered on a subset of
tasks to obtain trajectories demonstrating optimal behavior. Given a set, H, of trajectories generated
from an initial subset of tasks, we can now estimate the expectation in equation 1 to learn options that
can be leveraged in the remaining problems. Because the probability of generating any trajectory
approaches 0 as the length of the trajectory increases, we make modify the original objective for
better numerical stability, and arrive to the objective J that we optimize in practice.
^ , .
J(∏,O, H)
ɪ X(λ2 Pr(H = h∣∏,O) - Pt=I 呐=严=hi。+	λιg(h,Oo))
H	|h|
h∈H  ------------{z------------- -	Il	,	-------{------'
probability of generating h '	{z	} encourage diverse options
expected number of terminations
(2)
A more detailed discussion on how we arrived to this objective is provided in Appendix A. We can
express equation 2 entirely in terms of the policy over options π, options O = {o1, . . . , on} and
the transition function, P (which we estimate from samples). The following theorems show how to
calculate the first two terms in equation 2, allowing us to maximize the proposed objective.
Theorem 1.	Given a set of options, O, and a policy, π, over options, the expected number of
terminations for a trajectory h is given by:
|h|
X E Tt = 1Ht = ht , π, O
t=1
|h|
=XXβo(st)
t=1 o∈O
μo (st-1 ,at-1)Pr(Ot-1 = o∣Ht-ι = ht-ι,∏, O)
Po0∈o μo(st-ι ,at-1)Pr(Ot-1 = o0∣Ht-1 = ht-ι,∏,O)
(3)
Pr(Ot = o|Ht = ht, π, O)
π(st , o)βo (st)
1
at-1, st)αt-1(o)(1 - βo(st-1))
and Pr(O0 = o|H0 = h0, π, O) = π(s0 , o).
Proof. See Appendix B.
□
Theorem 2.	Given a set of options O and a policy π over options, the probability of generating a
trajectory h of length |h| is given by:
|h|-1
Pr(HIhI= h∖h∖l∏, O) =do(so)[ En(S0,o)μo(s0,a0)f(h®,o, 1)] ɪɪ P(Sk,ak,Sk+ι),
o∈O	k=0
where f is a recursive function defined as:
f(ht, o, i)
1,
(βo(si) Po0∈o∏(si+ι,o0)μo0(si+ι,ai+ι)f(ht,o0,i+1)
+ ( (1 - βo(si))μo(si+1,ai+1)f(ht,o,i + 1))
ifi=t
otherwise
Proof. See Appendix C.
□
1This term is only defined when we consider more than one option. Otherwise, we set this term to 0.
4
Under review as a conference paper at ICLR 2020
Given a parametric representation of the op-
tion policies and termination functions for each
o ∈ O and for the policy π over options, we use
Theorems 1 and 2 to differentiate the objective
in equation 2 with respect to their parameters
and optimize with any numerical optimization
technique.
3.3 Learning Options Incrementally
One common issue in option discovery is iden-
tifying how many options are needed for a
given problem. Oftentimes this number is pre-
defined by the user based on intuition. In such
a scenario, one could learn options by simply
randomly initializing the parameters of a num-
ber of options and optimizing the proposed ob-
jective in equation 2. Instead, we propose not
only learning options, but also the number of
options needed, by the procedure shown in Al-
gorithm 1. This algorithm introduces one op-
tion at a time and optimizes the objective J with
respect to the policy over options πθ , with pa-
rameters θ, and the newly introduced option,
o0 = (μφ,βψ), with parameters φ and ψ, for
N epochs. Optimizing both o0 and πθ allows us
to estimate how much we can improve J given
Algorithm 1 Option Learning Framework - Pseu-
docode
1:
2:
3:
4:
5:
6:
7
8
9
10
11
12:
13:
14:
15:
16:
17:
18:
Collect set of trajectories H
Initialize option set O with primitive options
done = false
^
Jprev = -∞
while done == false do
Initialize new option o0 = (μφ, βψ), ini-
tializing parameters for φ and ψ.
O0 = O ∪ o0
Initialize parameters θ of policy πθ
for k=1,. . . ,N do
ʌ ʌ, .
Jk = J(∏θ, O, H)
θ = θ + α dJθk
φ = φ + α dJk
ψ = ψ + α 端
if JN - Jprev < ∆ then
done = true
else
O = O0
^	^
Jprev = JN
19:	Return new option set O
that we keep any previously introduced option fixed. After the new option is trained, we measure
how much J has improved; if it fails to improve above some threshold, ∆, the procedure terminates.
This results in a natural way of obtaining an appropriate number of options, as options stop being
added once a new option no longer improves the ability to represent the demonstrated behavior.
4	Experimental Results
This section describes experiments used to evaluate the proposed technique approach. We show
results in the “four rooms” domain to allow us to visualize and understand the options produced by
our method, and to show empirically that these options produce a clear improvement in learning.
We use this domain to show that options generated by our method are able to generalize to tasks
where the option-critic architecture (Bacon et al., 2017) and eigenoptions (Machado et al., 2017)
would fail to do so. We then extend our experiments to evaluate our technique in a few selected
problems from the Atari 2600 emulator provided by OpenAI Gym (Brockman et al., 2016). These
experiments demonstrate that by using the trajectories obtained from solving a small subset of tasks,
our approach is able to discover options that significantly improve the learning ability of the agent
in the tasks it has yet to solve. For the four room experiment, we assume the transition function was
known in advance. In all ATARI experiments, we estimated the transition functions by fitting the
parameters of a linear Gaussian model to all the transitions experienced during training.
4.1	Experiments on Four Rooms Environment
We tested our approach in the four rooms domain: a gridworld of size 40 × 40, in which the agent
is placed in a start state and needs to reach a goal state. At each time-step, the agent executes one of
four available actions: moving left, right, up or down, and receives a reward of -1. Upon reaching
the goal state, the agent receives a reward of +10. We generated 30 different task variations by
changing the goal and start locations, and collected six sample trajectories from optimal policies
learned in six tasks. We evaluated our method on the remaining 24 tasks.
Figure 1a shows the change in the average expected number of terminations and average probability
of generating the observed trajectories while learning options, as new options are introduced and
adapted to the sampled trajectories. Options were learned over the six sampled optimal trajectories
and every 50 epochs anew option was introduced. For every new option, the change in probability of
5
Under review as a conference paper at ICLR 2020
Training Progression in Four Room Environpient
750	1000	1250	1500	1750
Episodes
1 Option 2 Options 3 OPtiOnS 4 Options
Average Performance on Four-Room Testing Tasks
Primitives
Option Initial
Option Learm
Option LeArfii
Option Critic
100	125
Training"Epochs
(a) Visualization of loss over 200 training epochs
for the four rooms domain. The decreasing aver-
age number of decisions made by π is shown in
blue and the increasing probability of generating
the sampled trajectories is shown in red.
(b) Performance comparison on four rooms domain.
Six tasks were used for training and 24 different for
testing. The plot shows the average return (and stan-
dard error) on the y-axis as a function of the episode
number on the test tasks.
Figure 1: Results on four-room domain. Six tasks were used for training and 24 for testing.
generating the observed trajectories as well as the change in expected number of decisions reaches
a plateau after 30 or 40 training epochs. When a new option is introduced, there is a large jump
in the loss because a new policy, π, is initialized arbitrarily to account for the new option set being
evaluated. However, after training the new candidate option, the overall loss improves beyond what
it was possible before introducing the new option.
In Figure 1b, we compare the performance of Q-learning on 24 novel test tasks using options dis-
covered by our method (with and without regularization using KL divergence), eigenoptions, and
option critic. We allowed each competing method to learn options from the same six training tasks
and, to ensure a fair comparison, we used the original code provided by the authors. As baselines,
we also compare against primitive actions and randomly initialized options. It might seem surpris-
ing that both eigenoptions and the option-critic failed to reach an optimal policy when they were
shown to work well in this type of problem; for that we offer the following explanation. Our imple-
mentation of four rooms is defined in a much larger state space than the ones where these methods
were originally tested, making each individual room much larger. Since the options identified by
these methods tend to lead the agent from room to room, it is possible that, once in the correct room,
the agent executes an option leading to a different room before it had the opportunity to find the
goal. When testing our approach in the smaller version of the four room problem, we found no clear
difference in performance of the competing methods. In this experiment, we set the threshold ∆ for
introducing a new option to 10% of J at the previous iteration and the hyperparameter λ2 = 100.0.
When adding KL regularization, we set λ1 = 0.001.
Figure 2 shows a visualization of the policy learned by the agent for a specific task. The policy leads
the agent to navigate from a specific location in the bottom-left room to a location in the top-right
room in a small “four-room” domain of size 10 × 15. 2 The new task to solve is shown in the top-left
figure, while the solution found is shown in the top-right figure. The remaining rows of images
depict the learned option policies, termination functions, and how they were used in the new task.
The first row shows the learned option policies after training, the center row depict the termination
functions and the bottom row shows a heat-map depicting where each option is likely to be called.
The figure shows that while the options are defined over the entire state space, they are only useful in
specific regions—that is, they are specialized. These options, when used in combination in specific
regions, allow the agent to learn how to solve new problems more efficiently.
4.2	Experiments using Atari 2600 Games
We evaluated the quality of the options learned by our framework in two different Atari 2600 games:
Breakout and Amidar. We trained the policy over options using A3C (Mnih et al., 2016) with
grayscale pixel input. Options were represented by a two layer convolutional neural network, and
2We show a smaller domain than used in the experiments for ease of visualization
6
Under review as a conference paper at ICLR 2020
Figure 2: Visualization of our framework in four rooms domain. A novel task is seen in the top left,
where the agent (red) has to navigate to a goal (green). On the top right, we show the solution found
by the agent. The three rows below show how the options were learned and exploited in the new
task. The highlighted area in the top row shows a sample trajectory and the color corresponds to
the probability that the option would take the demonstrated action. Notice that this trajectory was
obtained on a previous tasks, so it does not correspond to the new task on top. The arrows show the
action that is most likely at each state. After training (first row) each option specializes in a specific
skill (a navigation pattern). In this case, the demonstrated trajectory can be generated by using
option 3 and 2. The middle rows shows a heat-map indicating where an option is likely to terminate
(close to walls), and the last row shows where each options is likely to be used by the policy learned
in the new task. The agent learns to use each option in specific situations; for example, option 1 is
likely to be called to make the agent move up, if it is located in one of the bottom rooms.
were given the previous two frames as input. In both experiments the task variations consisted
in changing the number of frames skipped after taking an action (randomly selected between 2
and 10), the reward function by scaling the reward with a real number between 0.1 and 10.0, and
initial state distribution by letting the agent execute between 0 and 20 actions before start it starts
learning. The full implementation details for these experiments are given in Appendix E. Figures 3a
and 3b show the performance of the agent as a function of training time in Breakout and Amidar,
respectively. The plots show that given good choices of hyperparameters, the learned options led to
a clear improvement in performance during training. For both domains, we found that λ2 = 5, 000
led to a reasonable trade-off between the first two term in J, and report results with three different
regularization values: λ1 = 0.0,, λ1 = 0.01 and λ1 = 0.1.
Note that our results do not necessarily show that the options result in a better final policy, but
they improve exploration early in training and enable the agent to learn more effectively. Figure 4a
depicts the behavior for one of the learned options on Breakout. The option efficiently catches the
ball after it bounces off the left wall, and then terminates with high probability before the ball has
to be caught again. Bear in mind that the option remains active for many time-steps, significantly
reducing the number of decisions made by the policy over options. However, it does not maintain
control for so long that the agent is unable to respond to changing circumstances. Note that the
option is only useful in specific case; for example, it was not helpful in returning a ball bounced off
the right wall. That is to say, the option specialized in a specific sub-task within the larger problem:
7
Under review as a conference paper at ICLR 2020
(a) Average returns on novel tasks for Breakout.
(b) Average returns on novel tasks for Amidar.
Figure 3: Comparison on Atari domains for primitives (blue), options before training (orange) and
learned options for different values of λ1 and λ2 . Shaded regions indicate standard error.
a highly desirable property for generally useful options. Figure 4b shows the selection of two of the
options learned for Amidar when starting a new game. At the beginning of the game, option 1 is
selected, which takes the agent to a specific intersection before terminating. The agent then selects
option 2, which chooses a direction at the intersection, follows the resulting path, and terminates at
the next intersection. Note that the agent does not need to repeatedly select primitive actions in order
to simply follow a previously chosen path. Having access to these types of options enables an agent
to easily replicate known good behaviors, allowing for faster and more meaningful exploration of
the state space.
(a) Visualization of a learned option executed until termination on Breakout. The option learned to
catch the ball bouncing off the left wall and terminates with high probability before the ball bounces
a wall again (ball size increased for visualization).
Option 1	Option 2
(b) Visualization of two learned options on Amidar. The agent is shown in yellow and
enemies in pink. Option 1 learned to move up, at the beginning of the game, and turn left
until reaching an intersection. Option 2 learned to turn in that intersection and move up
until reaching the next one.
5	Conclusion and Future Work
In this work we presented an optimization objective for learning options offline from demonstrations
of near-optimal behavior on a set of tasks. Optimizing the objective results in a set of options that
allows an agent to reproduce the behavior while minimizing the number of decisions made by the
policy over options, which are able to improve the learning ability of the agent on new tasks. We
provided results showing how options adapt to the trajectories provided and showed, through several
experiments, that the identified options are capable of significantly improving the learning ability of
an agent. The resulting options encode meaningful abstractions that help the agent interact with and
learn from its environment more efficiently.
8
Under review as a conference paper at ICLR 2020
References
Haitham Bou Ammar, Eric Eaton, Paul Ruvolo, and Matthew E. Taylor. Unsupervised cross-domain
transfer in policy gradient reinforcement learning via manifold alignment. In Proceedings of the
Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAr15,pp. 2504-2510. AAAI Press,
2015. ISBN 0-262-51129-0. URL http://dl.acm.org/citation.cfm?id=2886521.2886669.
Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In AAAI, 2017.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. CoRR, 2016.
Alon Farchy, Samuel Barrett, Patrick MacAlpine, and Peter Stone. Humanoid robots learning to
walk faster: From the real world to simulation and back. In Proc. of 12th Int. Conf. on Autonomous
Agents and Multiagent Systems (AAMAS), May 2013.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
1126-1135, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR. URL
http://proceedings.mlr.press/v70/finn17a.html.
Jean Harb, Pierre-Luc Bacon, Martin Klissarov, and Doina Precup. When waiting is not an option:
Learning options with a deliberation cost. In AAAI, 2018.
Anna Harutyunyan, Will Dabney, Diana Borsa, Nicolas Heess, Remi Munos, and Doina Precup.
The termination critic. In AISTAT, 2019.
Marlos C. Machado, Marc G. Bellemare, and Michael Bowling. A Laplacian Framework for Option
Discovery in Reinforcement Learning. CoRR, 2017.
Sridhar Mahadevan. Proto-value functions: Developmental reinforcement learning. In Proceedings
of the 22nd International Conference on Machine Learning (ICML-2005), pp. 553-560. ACM,
2005.
A. McGovern and R. Sutton. Macro actions in reinforcement learning: An empirical analysis.
Technical report, University of Massachusetts - Amherst, Massachusetts, USA, 1998.
Amy McGovern and Andrew G. Barto. Automatic discovery of subgoals in reinforcement learning
using diverse density. In Proceedings of the Eighteenth International Conference on Machine
Learning, ICML ’01, pp. 361-368, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers
Inc. ISBN 1-55860-778-1. URL http://dl.acm.org/citation.cfm?id=645530.655681.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.
Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe-
tersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,
Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep rein-
forcement learning. Nature, 518(7540):529-533, February 2015. ISSN 00280836. URL
http://dx.doi.org/10.1038/nature14236.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd
International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning
Research, pp. 1928-1937, New York, New York, USA, 20-22 Jun 2016. PMLR. URL http:
//proceedings.mlr.press/v48/mniha16.html.
Jurgen Schmidhuber, JieyU Zhao, and Nicol N. Schraudolph. Learning to learn. chapter Reinforce-
ment Learning with Self-modifying Policies, pp. 293-309. Kluwer Academic Publishers, Nor-
well, MA, USA, 1998. ISBN 0-7923-8047-9. URL http://dl.acm.org/citation.cfm?id=296635.
296658.
JUrgen Schmidhuber. On learning how to learn learning strategies. Technical report, 1995.
9
Under review as a conference paper at ICLR 2020
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game ofGo with
deep neural networks and tree search. Nature, 529(7587):484-489, 2016. ISSN 0028-0836. doi:
10.1038/nature16961.
Richard S. Sutton and Doina Precup. Intra-option learning about temporally abstract actions. In In
Proceedings of the 15th International Conference on Machine Learning (ICML-1998), 1998.
Richard S. Sutton, Doina Precup, and Satinder P. Singh. Between mdps and semi-mdps: A frame-
work for temporal abstraction in reinforcement learning. Artificial Intelligence, 1999.
Matthew E. Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey.
J. Mach. Learn. Res., 10:1633-1685, December 2009. ISSN 1532-4435. URL http://dl.acm.org/
citation.cfm?id=1577069.1755839.
Matthew E. Taylor, Peter Stone, and Yaxin Liu. Transfer learning via inter-task mappings for tem-
poral difference learning. J. Mach. Learn. Res., 8:2125-2167, December 2007. ISSN 1532-4435.
URL http://dl.acm.org/citation.cfm?id=1314498.1314569.
Gerald Tesauro. Temporal difference learning and td-gammon. Commun. ACM, 38(3):58-68, March
1995. ISSN 0001-0782. doi: 10.1145/203330.203343. URL http://doi.acm.org/10.1145/203330.
203343.
10
Under review as a conference paper at ICLR 2020
A Appendix
The following list defines the notation used in all derivations:
1.	At : random variable denoting action taken at step t.
2.	St : random variable denoting state at step t.
3.	Ht: random variable denoting history up to step t. Ht = (S0, A0, S1, A1, . . . , St).
4.	Tt : random variable denoting the event that the option used at step t - 1 terminates at state
St.
5.	π : policy over options.
6.	P : transition function. P (s, a, s0) denotes the probability of transitioning to state s0 by
taking action a in state s
7.	Ot : random variable denoting the option selected for execution at state St .
8.	o: option defined as o = (μ0,βo), where μ° is the option policy for option and βo is the
termination function.
9.	Assume primitives are options that perform only 1 action and last for 1 time-step.
10.	O: set of available options.
We can compute the probability of an option terminating at state st and generating a trajectory ht
as:
Pr(Tt = 1,Ht = ht∣∏, O) = Pr(Tt = 1|Ht = ht, ∏, O) Pr(Ht = ht∣∏, O)	(4)
To compute the proposed objective J we need to find an expression for Pr(Tt = 1|Ht = ht, π, O)
and Pr(Ht = ht∣∏, O) in terms of known quantities.
A.1 APPENDIX A - DERIVATION OF J
Recall J(π,O,H) = E [pt=|1 Pr(Tt= 0, Ht ∣∏, O)]
, ignoring the regularization term. Assuming
access to a set H of sample trajectories, we start by estimating J from sample averages and derive
the objective J as follows:
1	|h|
J(π, O, H) ≈H XXPr(Tt = 0,Ht = ht∖∏, O)
|H| h∈H t=1
1	|h|
=∏7∣- X X (1 - Pr(Tt = 1∖Ht = ht,π,O)) Pr(Ht = ht∖π, O)
∖H∖ h∈H t=1
1	|h|
由 E E 1 - E[Tt∖Ht = ht,∏,O] Pr(Ht = ht∖∏,O)
∖H∖ h∈H t=1
It can easily be seen that to maximize the above expression E Tt∖Ht = ht, π, O should be mini-
mized while Pr(H = h∖π, O) should be maximized. Given that for long trajectories the expected
number of terminations increases while the probability of generating the trajectories goes to 0, we
normalize the number of terminations by the lenght of the trajectory, ∖h∖, and adjust a hyperpa-
rameter, λ2 , to prevent one term from dominating the other during optimization. Based on this
observation we propose optimizing the following objective:
1
J(∏,O, H) = H E λ Pr(H = h∖∏,O)-
h|h| ∈H
Pt=ι E [Tt∖Ht = ht,∏,O]
∖h∖
This objective allow us to control a trade-off, through λ2, of how much we care about the options
reproducing the demonstrated trajectories vs. how much we want the agent to minimize the number
of decisions.
11
Under review as a conference paper at ICLR 2020
A.2 Appendix B - Proof of Theorem 1
Theorem 1 Given a set of options O and a policy π over options, the expected number oftermina-
tionsfor a trajectory h of length ∖h∖ is given by:
|h|	|h|
X E [Tt = 1∖Ht = ht,π,O]= X
t=1	t=1
Eβo(st)
o∈O
μo(st-1, at -1) Pr(θt-1 = o∖Ht-I = ht-1, π, θ)
PPo0∈O μo0 (st-1, at-1) Pr(θt-1 = OIHt-I = ht-1, π, O)
where,
Pr(Ot-I = o∖Ht-1 = ht-1, π, O)
π(st-1,o)βo(st-1)
,at-2,St-1)μo(st-2,at-2)
× Pr(Ot-2 = o∖Ht-2 = ht-2,π, O)(1 - βo(st-1))
and Pr(O0 = o∖H0 = h0, π, O) = π(s0, o).
Proof. Notice that Et=I E [Tt = 1∖Ht = ht, π, O] = Et=IPr(Tt = 1∖Ht = ht, π, O) 1, so if we
find an expression forRr(Tt = 1∖Ht = ht, π, O), we can calculate the expectation exactly. We
define Pr(TO = 1∖H1 = h1,π, O) = 1 for ease of derivation even though there is no option to
terminate at T0 .
Pr(Tt = IlHt = ht, π, O)=E Pr(Tt = 1∖Ot-1 = o, Ht = ht,π, O) Pr(Ot-I = OlHt = ht,π, O)
o∈O
=X βo(st) Pr(Ot-1 = o∖Ht = ht, π, O)
o∈O
=E βo(st) Pr(Ot-1 = o∖Ht-1 = ht-1,At-1 = at-1,St = st,∏, O)
o∈O
Σβo(st)
Pr(St = StlHt-I = ht-1,At-1 = at-1, Ot-1 = o,π, O)
o∈O
× Pr(Ot-1
Pr(St = st∖Ht-1 = ht-1, At-1 = at-1,∏, O)
o∖Ht-I = ht-1, At-I = at-1,π, O)
v^ B P )Pr(St = st∖Ht-I = ht-1, At-I = at-1, π, O)
£ βo(St)Pr(St = st∖Ht-1 = ht-1,At-1 = at-1,π,O)
× Pr(Ot-I = o∖Ht-I = ht-1, At-1 = at-1, π, O)
X βo(st) Pr(Ot-1 = o∖Ht-1 = ht-1,At-1 = at-1,π, O)
o∈O
X βo(st)
o∈O
Σβo(st)
o∈O
Pr(At-I = at-1∖Ht-1 = ht-1,Ot-1 = o,π, O)Pr(Ot-1 = 0∖Ht-1 = ht-1,π, O)
Pr(At-I = at-1∖Ht-1 = ht-1,π, O)
μo(st-1,at-1)Pr(Ot-1 = 0∖Ht-1 = ht-1,π, O)
Pr(At-1 = at-1∖Ht-1 = ht-1,π, O)
12
Under review as a conference paper at ICLR 2020
μo(st-1, at -1) Pr(θt-1 = o∣Ht-1 = ht-1, π, O)
Po0∈o Pr(At-I = at-1,Οt-1 = o0∣Ht-ι = ht-ι,∏, O)
Eβo(st)
o∈O
Eβo(st)μo(st-i,at-i)Pr(Οt-i = o∣Ht-1 = ht-1,∏, O)
o∈O
× ( E Pr(At-1 = at-1∣Οt-1 = o , Ht-1 = ht-1, ∏, O)
o0∈Ο
× Pr(Ot-I =。/∣Ht-1 = ht-1,∏,O))-1
μo(St-1, at-1) Pr(Ot-I = o∣Ht-I = ht-1, π, O)
PPoz∈Ο μo0 (st-1, at-1) Pr(Ot-I = o0∣Ht-I = ht-1, π, O)
Σβo(st)
o∈O
We are left with finding an expression in terms of known probabilities for Pr(Ot-I = o∣Ht-1
ht-1,π, O).
Pr(Ot-1 = 0∣Ht-1 = ht-1,∏,O) =[ Pr(Ot-1 = o,Tt-1 = 1∣Ht-1 = ht-1,∏,O)
+ Pr(Ot-1 = o, Tt-1 = 0∣Ht-1 = ht-1,∏, O)]
=(Pr(Ot-1 = 0∣Ht-1 = ht-1,Tt-1 = 1,π, O)
× Pr(Tt-1 = 1∣Ht-1 = ht-1,π,O))
+ (Pr(Ot-1 = 0∣Ht-1 = ht-1,Tt-1 = 0,π, O)
× (1 - Pr(Tt-1 = 1∣Ht-1 = ht-1,π, O)))
=k∏(st-1, o) Pr(Tt-1 = 1∣Ht-1 = ht-1,π, O))
+ (Pr(Ot-1 = 0∣Ht-1 = ht-1,Tt-1 = 0,π, O)
× (1 - Pr(Tt-1 = 1∣Ht-1 = ht-1,π, O)))
=](π(st-1, 0)βo(st-1)) +
× ( Pr(Ot-I = 0∣Ht-I = ht-1, Tt-1 = 0,π, O)(1 - βo(st-I)))
Given that by convention, Pr(T0 = 1∣H0 = h0,π, O) = 1.0, we are now left with figuring out how
to calculate Pr(Ot-I = 0∣Ht-1 = ht-1,Tt-1 = 0,π, O)
Pr(Ot-1 = 0∣Ht-1 = ht-1, Tt-1 = 0,π, O) =Pr(Ot-2 = 0,At-2 = at-2,St-1 = St-1∣Ht-1 = ht-1,π, O)
= Pr(At-2 = at-2,St-1 = st-1∣Ot-2 = 0, Ht-1 = ht-1,π, O)
× Pr(Ot-2 = 0∣Ht-1 = ht-1,π, O)
=Pr(St-I = st-11At-2 = at-2, Ot-2 = 0, Ht-1 = ht-1, π, O)
× Pr(At-2 = at-2 ∣Ot-2 = 0, Ht-1 = ht-1, π, O)
× Pr(Ot-2 = 0∣Ht-1 = ht-1,π, O)
=P(st-2,at-2, St-1)μo(st-2, at-2) Pr(Ot-2 = 0∣Ht-1 = ht-1, π, O)
=P(st-2,at-2, St-1)μo(st-2, at-2) Pr(Ot-2 = 0∣Ht-2 = ht-2, π, O)
where Pr(O0 = 0∣H0 = h0, π, O) = π(s0,0)
13
Under review as a conference paper at ICLR 2020
Using the recursive function Pr(Ot-1 = o0|Ht-1 = ht-1, π, O), the expected number of termina-
tions for a given trajectory is given by:
一口	μo	11 μ μ	-1	G μ μθ(	Q / ʌ	μo(St-1,at-1) Pr(Ot-I =	olHt-1	=	ht-1,π, O)
占 E [Tt = 1lHt = ht,π, O] = ∑ (工βo(st) Po0∈o μoθ(st-i,at-i)Pr(Ot-i = o0Ht-i = ht-i,∏, O)
□
A.3 Appendix C - Proof of Theorem 2
Theorem 2
Given a set of options O and a policy π over options, the probability of generating a trajectory h of
length |h| is given by:
Pr(H∣h∣ = h∣h∣∣π, O) = d0(s0) Po∈o π(s0,o)μo(so,ao)f(h∣h∣,o,1) “露1 P(sk,a®,Sk+ι),
where f is a recursive function defined as:
1,
if i = t
f(ht, o, i) =
βo(Si) ∑ζo∈o ∏(si+ι,o0)μoo (Si+1, ai+ι)f (ht, O, i + 1)
+ (I - β (Si))μo(si+1, ai+1)f (ht, o,i + 1)
otherwise
Proof. We define Hi,t to be the history from time i to time t, that is, Hi,t
(Si, Ai, Si+1, Ai+1, . . . , St), where i < t. If i = t, the history would contain a single state.
Pr(Ht = ht∣∏, O) =Pr(S0 = so∣∏, O)Pr(Hι,t = h1,t,A0 = αo∣So = so,∏,O)
=d0(S0) Pr(H1,t = h1,t, A0 = a0|S0 = S0, π, O)
=d0 (S0)	Pr(H1,t = h1,t , A0 = a0 , Oo = o|S0 = S0 , π, O)
o∈O
=d0(S0)	Pr(O0 = o|S0 = S0, π, O) Pr(H1,t = h1,t, A0 = a0|S0 = S0, O0 = o, π, O)
o∈O
=d0 (S0)	π( S0, o) Pr(H1,t = h1,t, A0 = a0 |S0 = S0, O0 = o, π, O)
o∈O
=d0 (S0)	π( S0, o) Pr(A0 = ao |S0 = S0 , O0 = o, π, O)
o∈O
× Pr(H1,t = h1,t|S0 = S0,O0 =o,A0 =a0,π,O)
=d0(SO)): π(s0, o)μo(s0, ao) Pr(H1,t = h1,t |S0 = s0, O0 = o, AO = a0, π, O).
o∈O
We now need to find an expression to calculate Pr(H1,t = h1,t|SO = SO, OO = o, AO = aO, π, O).
Consider the probability of seeing history hi,t given the previous state, S, the previous option, o, and
the previous action, a:
14
Under review as a conference paper at ICLR 2020
Pr(Hi,t = hi,t |Si-1 = s, Oi-1 = o, Ai-I = a)
=Pr(Si = SilSi-1 = S, Oi-1 = O, Ai-I = a) Pr(Hi+i,t = hi+1,t, Ai = ai I Si - 1 = s, Oi-1 = o, Ai-I = a, Si
=P(S, a, Si) Pr(Hi+1,t = hi+1,t, Ai = ai lSi-1 = s, Oi-1 = o, Ai-1 = a, Si = Si)
=P(S, a, Si) Pr(Hi+1,t = hi+1,t, Ai = ai lOi-1 = o, Ai-1 = a, Si = Si)
=P(S,a, Si) [ Pr(Ti = 1∣Oi-1 = 0,Ai-1 = a, Si = Si)
× Pr(Hi+1,t = hi+1,t, Ai = ai|Oi-1 = 0, Ai-I = a, Si = Si, Ti = 1)
+ Pr(3=0∣Oi-1 = 0, Ai-1 = a, Si = Si)
× Pr(Hi+1,t = hi+1,t, Ai = ai∣Oi-1 = 0, Ai-1 = a,Si = Si, Ti = 0)]
=P(S, a, Si) [βo(Si)
× Pr(Hi+1,t = hi+1,t, Ai = ai∣Oi-1 = 0, Ai-1 = a, Si = Si, Ti = 1)
+ (1 - βo(Si))
× Pr(Hi+1,t = hi+1,t, Ai = ai∣Oi-1 = 0, Ai-1 = a, Si = Si, Ti = 0)] ∙
si)
Even though the equation above might seem complicated, there are only two cases we need to
consider: either the current option terminates and a new one must be selected (the first term), or the
current option does not terminate (the second term). Let,s consider each of them separately.
Case 1 - option terminates: If we terminate, we sum over new options:
Pr(Hi+1,t = hi+1,t, Ai = αi ∣Oi-1 = 0, Ai-1 = a, Si = Si, 3=1)
X Pr(Oi = 00∣Oi-1 = 0,Ai-1 = a,Si = Si,3=1)
o0∈Ο
× Pr(Hi+1,t = hi+1,t, Ai = ai ∣Oi-1 = 0, Ai-1 = a, Si = Si, 3=1, Oi = 0 )
):n(Si, 0 ) Pr(Hi+1,t = hi+1,t, Ai = ai ∣Oi-1 = 0, Ai-1 = a, Si = Si, 3=1, Oi = 0 )
o0∈Ο
):n(Si, 0 ) Pr(Hi+1,t = hi+1,t, Ai = ai ∣Si = Si, Oi = 0 )
o0∈Ο
E π(Si, 00) Pr(Ai = a/Si = Si, Oi = 00) Pr(Hi+1,t = hi+1,t∣Si = Si, Oi = 00, Ai = ai)
o0∈Ο
〉:n(Si, 0 )μo0 (Si, ai) Pr(Hi+1,t = hi+1,t ISi = Si, Oi = 0 , Ai = ai)∙
o0∈Ο
Note that the expanded probability has the same form as Pr(Hi,t = hi,t∣Si-1 = S,Oi-1 =
0, Ai-1 = a).
Case 2 - option does not terminate: This tells us that Oi = 0, so we may drop the dependency on
the i — 1 terms:
Pr(Hi+1,t = hi+1,t, Ai = ai ∣Si-1 = S, Oi-1 = 0, Ai-1 = a, Si = Si, 3=0)
=Pr(Hi+1,t = hi+1,t, Ai = ai ∣Si = Si, Oi = O)
=Pr(Ai = ai∣Si = Si, Oi = 0) Pr(Hi+1,t = hi+1,t ISi = Si, Oi = 0, Ai = ai)
=Mo(Si, ai) Pr(Hi+1,t = hi+1,t ISi = Si, Oi = 0, Ai = ai)∙
Plugging these two cases back into our earlier equation yields:
15
Under review as a conference paper at ICLR 2020
Pr(Hi,t = hi,t |Si-1 = s, Oi-1 = o, Ai-1 = a)
=P(S, a, Si)[e0(Si)): π(si, O)μo0 (si, ai) Pr(Hi+1,t = hi+1,t|Si = si, Oi = o , Ai = ai)
o0∈O
十 (I - βo(Si))μo(Si, ai) Pr(Hi+1,t = hi+1 ,t ISi = si,Oi = o, Ai = ai)].
Note that each term contains an expression of the same form, Pr(Hi,t = hi,t|Si-1 = S, Oi-1 =
o, Ai-1 = a). We can therefore compute the probability recursively. Our recursion will terminate
when we consider i = t, as Ht,t contains a single state, and we adopt the convention of its probability
to be 1. Notice that for every recursive step, both inner terms will produce a P(S, a, Si) term.
Consider the result when we factor every recursive P(S, a, Si) term to the front of the equation. We
define the following recursive function:
1,	ifi =t
βo(Si) Po0∈o ∏(Si+ι,o0)μoθ (Si+1, ai+ι)f (ht, o0, i + 1)
+(1 - βo(Si))μo(Si+1,ai+1)f(ht,o,i + 1) ,	otherwise
Notice that this is the recursive probability described above, but with the P(S, a, S0) terms factored
out. We now see that:
t-1
Pr(Hi,t =	hi,t |Si-1 =	Si-1 , Oi-1	= o, Ai-1	= ai-1 )	= f(ht ,	o, i)	P(Sk ,	ak ,	Sk+1 ).
k=i-1
Plugging this all the back into our original equation for Pr(Ht = ht∣∏, O) gives Us the desired
result:
Pr(H∣h∣ = h∣h∣l∏, O) = d0(S0)
π(So,o)μo(S0,ao )f (h∣h∣,o, 1)
t-1
P(Sk, ak, Sk+1 ).
k=0
□
A.4 Appendix D - Empirical Validation of Derived Equations
To double check the derivation of the proposed objective and make sure the implementation was
correct, we conducted a simple empirical test to compared the calculated expected number of deci-
sions in a trajectory and the probability of generating each trajectory for a set of 10 trajectories on
10 MDPs. The MDPs are simple chains of 7 states with different transition functions. We randomly
initialized four options and a policy over options, and estimated the probability of generating each
trajectory and the expected number of terminations, for each sampled trajectory, by Montecarlo
sampling 10, 000 trials. Table 1 presents results for the 10 trajectories verifying empirically that
the equations were correctly derived and implemented. The table compares the empirical and true
probability of generating a given trajectory, Pr(H∣∙) and Pr(H∣∙), respectively, and the empirical
and true sum of expected number of decisions an agent has to make to generate those trajectories,
Pt=I E [Tt∣∙] and PtHI E [Tt ",respectively.
16
Under review as a conference paper at ICLR 2020
H	ʌ Pr(H∣π, O)	Pr(H∣π, O)	Pt=1 E[T∕Ht,∏H	PtHI E[Tt∣Ht,∏H
hi	-0.0932-	-0.0957-	3.060	3.178
h2	-0.0158-	-0.0173-	4Γ39	4Γ54
h3	-0.2149-	-0.2122-	1.965	2T7S
h4	-0.0995-	-0.0957-	2.979	3.178
h5	-0.0962-	-0.0957-	3.024	3.178
h6	-0.1354-	-0.1384-	2.9579	3.1596
h7	-0.00040-	-0.00038-	9.750	8.794
h8	-0.1854-	-OW-	2.820	3.072
h9	-0.0379-	-0.0368-	4.2612	4.4790
h10	0.1864 —	0.1881 —	2.8404	3.0723
Table 1: Validation of equations and implementation.
Note that the cases with largest discrepancy between the estimated and calculated number of termi-
nations occur when the probability of generating a trajectory is low. This happens because, since
the trajectory is unlikely to be generated, the Monte Carlo sampling is not able to produce enough
samples of the trajectory.
A.5 Appendix E - Implementation Details for Atari Experiments
For these experiments we first learned a good performing policy with A3C for each game and sam-
pled 12 trajectories for training. Each trajectory lasted until a life was lost, not for the entire duration
of the episode. Each option was represented as a two-layer neural network, with 32 neurons in each
layer, and two output layers: a Softmax output layer over the four possible actions representing μ,
and a separate sigmoid layer representing β. We implemented our objective using PyTorch which
simplifies gradient calculations. The options were represented by a two-layer neural network, where
the input was represented by gray scale images of the last two frames. We ran 32 training agents in
parallel on CPUs, the learning rate was set to 0.0001 and the discount factor γ was set to 0.99.
Because the options can only learn from the states observed in the trajectories, it is possible that
when using them, they will be executed in previously unseen states. When this happens, the ter-
mination function may decide to never terminate, as it has not seen that region of the state space
before. To address this issue, we add a value of 0.05 to the predicted probability of termination per
time-step that the option has been running since executed. Therefore, in our experiments an option
cannot run for more than 20 time-steps in total.
17