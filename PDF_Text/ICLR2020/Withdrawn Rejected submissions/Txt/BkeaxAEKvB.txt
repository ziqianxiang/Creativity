Under review as a conference paper at ICLR 2020
New Loss Functions for Fast Maximum Inner
Product Search
Anonymous authors
Paper under double-blind review
Ab stract
Quantization based methods are popular for solving large scale maximum inner
product search problems. However, in most traditional quantization works, the ob-
jective is to minimize the reconstruction error for datapoints to be searched. In this
work, we focus directly on minimizing error in inner product approximation and
derive a new class of quantization loss functions. One key aspect of the new loss
functions is that we weight the error term based on the value of the inner product,
giving more importance to pairs of queries and datapoints whose inner products
are high. We provide theoretical grounding to the new quantization loss function,
which is simple, intuitive and able to work with a variety of quantization techniques,
including binary quantization and product quantization. We conduct experiments
on public benchmarking datasets http://ann-benchmarks.com to demon-
strate that our method using the new objective outperforms other state-of-the-art
methods. We are committed to release our source code.
1	Introduction
Maximum inner product search (MIPS) has become a popular paradigm for solving large scale classi-
fication and retrieval tasks. For example, in recommendation systems, user queries and documents
are embedded into dense vector space of the same dimensionality and MIPS is used to find the most
relevant documents given a user query (Cremonesi et al., 2010). Similarly, in extreme classification
tasks (Dean et al., 2013), MIPS is used to predict the class label when a large number of classes,
often on the order of millions or even billions are involved. Lately, MIPS has also been applied to
training tasks such as scalable gradient computation in large output spaces (Yen et al., 2018), efficient
sampling for speeding up softmax computation (Mussmann and Ermon, 2016) and sparse updates in
end-to-end trainable memory systems (Pritzel et al., 2017).
To formally define Maximum Inner Product Search (MIPS) problem, consider a database X =
{xi}i=1,2,...,N with N datapoints, where each datapoint xi ∈ Rd in a d-dimensional vector space. In
the MIPS setup, given a query q ∈ Rd, we would like to find the datapoint x ∈ X that has the highest
inner product with q, i.e., we would like to identify
x* := argmaxhq,xi).
xi∈X
Exhaustively computing the exact inner product between q and N datapoints is often very expensive
and sometimes infeasible. Several techniques have been proposed in the literature based on hashing
and quantization to solve the approximate maximum inner product search problem efficiently, and
the quantization based techniques have shown strong performance (Ge et al., 2014; Babenko and
LemPitsky, 2014; Johnson et al., 2017). Quantizing each datapoint Xi to xci not only reduces storage
costs and memory bandwidth bottlenecks, but also permits efficient computation of distances. It avoids
memory bandwidth intensive floating Point oPerations through Hamming distance comPutation and
look uP table oPerations (Norouzi et al., 2014; Jegou et al., 2011; Wu et al., 2017). In most traditional
quantization works, the objective in the quantization Procedures is to minimize the reconstruction
error for the dataPoints to be searched.
In this PaPer, we ProPose a new class of loss functions in quantization to imProve the Performance of
MIPS. Our contribution is threefold:
•	We derive a novel class of loss functions for quantization, which deParts from regular
reconstruction loss by weighting each Pair of q and x based on its inner Product value. We
Prove that such weighting leads to an effective loss function, which can be used by a wide
class of quantization algorithms.
1
Under review as a conference paper at ICLR 2020
•	We devise algorithms for learning the codebook, as well as quantizing new datapoints,
using the new loss functions. In particular, we give details for two families of quantization
algorithms, product quantization and binary quantization.
•	We show that on large scale standard benchmark datasets, such as Glove100, the change of
objective yields a significant gain on the approximation of true inner product, as well as the
retrieval performance.
This paper is organized as follows. We first briefly review previous literature on quantization for
Maximum Inner Product Search, as well as its links to `2 nearest neighbor search in Section 2. Next,
we give our main result, which is the derivation of our objective in Section 3. Applications of the new
loss functions to binary quantization and product quantization are given in Section 4. Finally, we
present the experimental results in Section 5.
2	Related Works
There is a large body of similarity search literature on inner product and nearest neighbor search.
We refer readers to (Wang et al., 2014; 2016) for a comprehensive survey. Some methods also
transform MIPS problem into its equivalent form of `2 nearest neighbor using transformation such
as (Shrivastava and Li, 2014; Neyshabur and Srebro, 2014), but in general are less successful than the
ones that directly work in the original space. In general, these bodies of works can be divided into two
families: (1) representing the data as quantized codes so that similarity computation becomes more
efficient (2) pruning the dataset during the search so that only a subset of data points is considered.
Typical works in the first family include binary quantization (or binary hashing) techniques (Indyk and
Motwani, 1998; Shrivastava and Li, 2014) and product quantization techniques (Jegou et al., 2011;
Guo et al., 2016), although other families such as additive quantization (Babenko and Lempitsky,
2014; Martinez et al., 2016; 2018) and trenary quantization Zhu et al. (2016) also apply. There are
many subsequent papers that extend these base approaches to more sophisticated codebook learning
strategies, such as (He et al., 2013; Erin Liong et al., 2015; Dai et al., 2017) for binary quantization
and Zhang et al. (2014); Wu et al. (2017) for product quantization. There are also lines of work that
focus on learning transformations before quantization (Gong et al., 2013; Ge et al., 2014). Different
from these methods which essentially minimize reconstruction error of the database points, we argue
in Section 3 that reconstruction loss is suboptimal in the MIPS context, and any quantization method
can potentially benefit from our proposed objective.
The second family includes non-exhaustive search techniques such as tree search (Muja and Lowe,
2014; Dasgupta and Freund, 2008), graph search (Malkov and Yashunin, 2016; Harwood and
Drummond, 2016), or hash bucketing (Andoni et al., 2015) in nearest neighbor search literature.
There also exist variants of these for MIPS problem (Ram and Gray, 2012; Shrivastava and Li, 2014).
Some of these approaches lead to larger memory requirement, or random access patterns due to the
cost of constructing index structures in addition to storing original vectors. Thus they are usually
used in combination with linear search quantization methods, in ways similar to inverted index (Jegou
et al., 2011; Babenko and Lempitsky, 2012; Matsui et al., 2015).
In addition, many researchers have devoted to high quality implementation of such libraries, including
SPTAG Chen et al. (2018), FAISS Johnson et al. (2017), hnswlib Malkov and Yashunin (2016) etc..
We compared with ones available on ann-benchmarks in Section. 5.
3	Problem Formulation
Common quantization techniques focus on minimizing the reconstruction error (sum of squared error)
when X is quantized to x. It can be shown that minimizing the reconstruction errors is equivalent
to minimizing the expected inner product quantization error under a mild condition on the query
distribution. Indeed, consider the quantization objective of minimizing the expected total inner
product quantization errors over the query distribution:
NN
EqXkhq,χii - hq,χiik2 = EqX llhq,χi - Xiik2.	(I)
i=1	i=1
2
Under review as a conference paper at ICLR 2020
□
% /
(a)
Figure 1: (a) Not all pairs of q and x are equally important: for x, it is more important to accurately
quantize the inner product of hq1, xi than hq2, xi or hq3, xi, because hq1, xi has a higher inner product
and thus is more likely to be the maximum; (b) Quantization error of x given one of its quantizer c2
can be decomposed to a parallel component rk and an orthogonal component r⊥. Notice that c3 incur
more parallel loss (rk ), while c2 incur more orthogonal loss (r⊥). (c) Graphical illustration of the
intuition behind Equation (7). Even if c3 is closer to x in terms of Euclidean distance, c2 is a better
quantizer than c3 in terms of inner product approximation error of hq1, x - ci.
(b)	(c)
Under the assumption that q is isotropic, i.e., E[qqT] = cI, where I is the identity matrix and c ∈ R+,
the objective function becomes
NN	N
XEqkhq,χi - Xiik2 = XEq(Xi - Xi)TqqT(Xi - Xi) = CX Ilxi- xik2
Therefore, the objective becomes minimizing the reconstruction errors of the database points
PN=ι kxi - Xi∣2, and this has been considered extensively in the literature.
One key observation about the above objective function (1) is that it takes expectation over all possible
combinations of datapoints X and queries q. However, it is easy to see that not all pairs of (X, q) are
equally important. The approximation error on the pairs which have a high inner product is far more
important since they are likely to be among the top ranked pairs and can greatly affect the search
result, while for the the pairs whose inner product is low the approximation error matters much less.
In other words, for a given datapoint X, we should quantize it with a bigger focus on its error with
those queries which have high inner product with X.
Following this key observation, we propose a new loss function by weighting the approximation
error of the inner product based on the value of true inner product. More precisely, let w(t) ≥ 0
be a monotonically non-decreasing function, and consider the following inner-product weighted
quantization error
N
Eq [w(hq, Xii)hq, Xi -
i=1
Xii2] = XZ W⑴Eq [hq,
i=1
Xi- Xii2∣hq,Xii = t]dP(hq,Xii ≤ t).
(2)
One common choice can be w(t) = I(t ≥ T ), in which case we care about all pairs whose inner
product is greater or equal to certain threshold T, and disregard the rest of the pairs.
3.1	New Loss Function
We decompose the inner-product weighted quantization errors based on the direction of the datapoints.
We show that the new loss function (2) can be expressed as a weighted sum of the parallel and
orthogonal components of the residual errors with respect to the raw datapoints. Formally, let
r(X, X) := X - X denote the quantization residual function. Given the datapoint x with ∣∣x∣ > 0 and
its quantizer X, We can decompose the residual error into two parts, the one parallel to x and the one
orthogonal to X:
X
rk(X,X) := hX - X,Xi ∙ i∣X∣∣,	(3)
r⊥(X, X) := (x — X) — r∣∣ (x, X),	(4)
It,s easy to see that r(X, X) = r∣∣ (x, X) + r⊥(X, X). Next we develop our new loss function. Without
loss of generality, we assume:
3
Under review as a conference paper at ICLR 2020
1.	kqk = 1. The norm of q does not matter to the ranking result;
2.	kxk ≤ b. The norm of x is finite and bounded.
Theorem 3.1. Assuming the query q is uniformly distributed in d-dimensional unit sphere. Given the
datapoint X and its quantizer x, conditioned on the inner product hq, Xi = t for some t > 0, we have
Eq [hq,X - xi2lhq,Xi = t] = k⅛llrk(X,X) ||2+ ⅛F llr⊥(X,X) ||2.	⑸
Proof. First, We can decompose q := q∣∣ + q⊥ with q∣∣ := hq, Xi ∙ ∣∣χ∣∣ and q⊥ := q - q∣∣ where q∣∣ is
parallel to X and q⊥ is orthogonal to X. Then, we have
Eq[hq,X - X)2|hq,X〉= t] = Eq[hq∣∣ + q⊥,r∣∣(X,X) + r⊥(X,X)i2∣hq,X) = t]
=Eq [(hqk,rk(X,X)i + hq⊥,r⊥(X,X)))2|hq,Xi =t]
=EqKqk,rk(X,X)i2|hq,X)=t] + Eq[hq⊥,r⊥(X,X)i2|hq,X)=t],⑹
The last step uses the fact that Eq[hqk, rk (x, X)ihq⊥,r⊥ (x, X)i∣hq, x) = t] = 0 due to symmetry. The
first term of (6), Eq筋卜口(x,X)i2∣hq, xi = t] = ∣∣r∣∣ (x,X)∣∣2Eq[∣∣q∣∣ ∣∣2|hq,x〉= t] = krχ2 . For
the second term, since q⊥ is uniformly distributed in the (d- 1) dimensional subspace orthogonal
11- t
to x with the norm Jl — ^^, we have Eq[hq⊥,r⊥(x,X)i2∣hq,x> = t] = d-xf ∣∣r⊥(x,X)∣∣2.
Therefore,
2
C	t2	CI - τπ^	C
Eq[hq,r(χ,χ)i2lhq,χi =t] = ∣∣-∣τ2||rk(x,X)||2 +	, a1 llr⊥(χ,χ)ll2.
kXk2	d - 1
□
Now we compute the inner-product weighted quantization error (2) for the case w(t) = I(t ≥ T ).
One can do similar derivations for any reasonable w(t). Given T > 0 and a datapoint X:
Eq[I(hq,xi ≥ T)hq,x - Xi2] = f (T, ∣∣x∣∣)忻(x,X) ||2 +。"口) ∣∣r⊥(x,X)∣∣2.	⑺
where f(T,kXk) and g(T,kXk) are defined as
kxk	t2
f (T,同)：=	EdP(hq,xi≤ t),
t=min(T,∣∣x∣∣) kXk
kxk	t2
g(τ, ∣∣χ∣∣) ：= Wmcn向)(1 - kxk2)dP(hq,χi ≤ t).
It is easy to see that f(T, kXk) and g(T, kXk) are decreasing functions of T and increasing functions
of kXk.
XXEq[I(hq,Xii ≥ T)hq,Xi- xii2] = Xf (T,归||)忻(xi,xi)∣∣2 + g(T-xk)∣∣r⊥(xi,xi)∣∣2.
i=	i=	d - 1
≤ XXf(T,b)∣∣rk(Xi,Xi)∣∣2 + g(T4)∣∣r⊥(Xi,Xi)∣∣2
d - 1
i=
NN
Y (d - 1)λ(T, b)X llrk(xi,xi)ll2+ X llr⊥(xi, Xi)『⑻
i=1 i=1
where λ(T, b) = f(τb).
4
Under review as a conference paper at ICLR 2020
3.2 COMPUTING λ(T, b)
The new loss functions (2) we proposed is upper bounded by (8) and the equality is achieved when
all of kxi k = b for all xi . (8) can be viewed as the weighted sum of the parallel quantization errors
and the orthogonal quantization errors, with respect to the original data points. Note that when
λ(T, b) = 1, (8) reduces to the traditional reconstruction loss.
We can also characterize the asymptotic behavior of λ(T, b) and show that (1) λ(T, b) can be
analytically computed, and (2) λ(T, b) → I-TTb)2 as the dimension d → ∞. In practice, We can
choose λ(T, b) empirically or through cross-validation. However, we found that λ(T, b) when d → ∞
offers a very good estimate. We discuss the sensitivity λ(T, b) in Section. 7.5 of the Appendix.
R0arccos(T/b) sind-2 θdθ
Theorem 3.2.	For b > 0, T < b, we have λ(T, b) = -.....------------------1.
∕0arccoscT/6) sind θdθ
Proof. See the Section. 7.1 of Appendix.	□
We furthermore prove that the limit of λ(T, b) exists and that it equals(d-%//)? as d → ∞. In
Figure 2a, We plot λ With (T /b) = 0.2 and We can see it approaches its limit quickly as d groWs.
Theorem 3.3.	When T ≥ 0, we have limd→∞ λ(T, b) = I-TT/)b)2.
Proof. See the Section. 7.2 of Appendix.	□
4	Application to Quantization Techniques
In this section, We derive algorithms for applying neW loss functions in (8) to common quantization
techniques, including vector quantization, product quantization. Discussion on binary quantization
can be found in Section 7.8 of Appendix.
4.1	Vector Quantization
Recall that in vector quantization, given a set of N datapoints, We Want to find a codebook of size
k and quantize each datapoint as one of the k codes. The goal is to minimize the total squared
quantization error. Formally, the traditional vector quantization solves
N
min d X ∣∣xi - Xik2,
c1,c2,...,ck∈Rd	i=1
Xi∈{c1,c2,…,Ck },1≤i≤N 一
One of the most popular quantization algorithms is the k-Means algorithm, Where We iteratively
partition the datapoints into k quantizers Where the centroid of each partition is set to be the mean of
the datapoints assigned in the partition.
Motivated by minimizing the inner product quantization error for cases When the inner product
betWeen queries and datapoints is high, our proposed objective solves:
N
min d	X(μ∣∣r∣∣(xi,Xi)||2 + l∣r⊥(xi,Xi)l∣2),	⑼
c1 ,c2 ,...,ck ∈R	i=1
Xi∈{cι ,C2,…,Ck},1≤i≤N 一
where μ = (d 一 1)λ(T, b) is a hyperparameter as a function of d and T following (8).
We solve (9) through a k-Means style Lloyd’s algorithm, Which iteratively minimizes the neW loss
functions by assigning datapoints to partitions and updating the partition quantizer in each iteration.
The assignment step is computed by enumerating each quantizer and finding the quantizer that
minimizes (9). The update step finds the new quantizer X* ∈ Rd for a partition of datapoints
x1, x2 , . . . , xm ∈ Rd, i.e.,
m
X* = mind X (μ∣rk(xi,X)k2 + ∣r⊥(xi,X)k2) ,	(10)
5
Under review as a conference paper at ICLR 2020
Because of the changed objective, the best quantizer is no longer the center of the partition. Since (10)
is a convex function of X, there exists an optimal solution. The update rule given a fixed partitioning
can be found by setting the partial derivative of (10) with respect to each codebook entry to zero.
This algorithm provably converges in a finite number of steps. See Algorithm 1 in Appendix for a
complete outline of the algorithm. Note that, in the special case that μ = 1, it reduces to regular
k-Means algorithm.
Theorem 4.1.	The optimal solution of (10) is
X* := μ + + T X x⅞ !-1 J
m i=1 kxi k2	m
(11)
Proof. See Section. 7.3 of the Appendix.
□
Theorem 4.2.	Algorithm 1 converges in finite number of steps.
Proof. This immediately follows from the fact that the loss defined in (9) is always non-increasing
during both assignment and averaging steps under the changed objective.	□
4.2	Product Quantization
A natural extension of vector quantization is product quantization, which works better in high
dimensional spaces. In product quantization, the original vector space ∈ Rd is decomposed as the
Cartesian product of m distinct subspaces of dimension a,and vector quantizations are applied in
each subspace separately 1. For example, let X ∈ Rdbe written as
X = (X(1) , X(2) , . . . , X(m)) ∈ Rd,
where x(j) ∈ Rm is denoted as the sub-vector for the j-th subspace. We can quantize each of the
x(j) to X(j) with its vector quantizer in subspace j, for 1 ≤ j ≤ m. With product quantization, X is
quantized as (X(1),..., X(m)) ∈ Rd and can be represented compactly using the assigned codes.
Using our proposed loss objective (8), we minimize the following loss function instead of the usual
objective of reconstruction error:
N
L min,	£ (μkrk(Xi,Xi)k2 + llr⊥(χi ,xi )k2),
C1 ,C2 ,...,Cm ,
A∈{1,2,...,k}N×m i=1
where Xi denotes the product quantization of Xi, i.e.,
Xi ：= (C1,Ai,1 ,C2,Ai,2 ,…，Cm,Ai,m ).
(12)
To optimize (12), we apply the vector quantization of Section 4.1 over all subspaces, except that the
subspace assignment is chosen to minimize the global objective over all subspaces (12), instead of
using the objective in each subspace independently. Similarly, the update rule is found by setting the
derivative of loss in (12) with respect to each codebook entry to zero. The complete algorithm box
of Algorithm (1) is found in Section 7.6 of the Appendix.
5	Experiments
In this section, we show our proposed quantization objective leads to improved performance on maxi-
mum inner product search. First, we compare using productization mechanism with reconstruction
loss and our proposed loss to show that the new loss leads to better retrieval performance and more
accurate estimation of maximum inner product values. Secondly, we compare in fixed-bit-rate settings
with QUIPS, which achieves state-of-the-art in multiple MIPS tasks. Finally, we analyze the end-
to-end MIPS retrieval performance of our algorithm in terms of speed-recall trade-off in controlled
hardware environment and timing. We follow the benchmark setting from ann-benchmarks,
which provides 11 competitive baselines with pre-tuned parameters. We plot benchmarks speed-recall
curve and show our algorithm achieves the state-of-the-art.
1Random rotation or permutation of the original vectors can be done before doing the Cartisean product.
6
Under review as a conference paper at ICLR 2020
(P)Y
Figure 2: (a) λ(T = 0.2, b = 1) in Theorem 3.2 computed analytically as function of d using
recursion of (13), quickly approaches its limit. Therefore we use its limit for computing λ in our
experiments. (b) The retrieval Recall1@10 for different T when b = 1. (c) The relative error of inner
product estimation for true Top-1 on Glove1.2M dataset, across multiple number of bits settings.
5.1	Analysis of the proposed loss function
We compare the result of same quantization mechanism with different loss functions (reconstruction
loss and the proposed loss (8)). We use Glove1.2M which is a collection of 1.2 million 100-
dimensional word embeddings trained with the method described in Pennington et al. (2014), and we
provide rationale on using Glove1.2M for evaluation in Section 7.9 of the Appendix. We set λ as
limd→∞ λ(T = 0.2, b = 1) for all our experiments.
Figure. 2b illustrates the Recall1@10 of product quantization on Glove1.2M, with reconstruction
loss and proposed loss. We can see that our algorithm leads to large improvement over the one
with reconstruction loss. In addition to retrieval, many application scenarios also require estimating
the value of the inner product hq, xi. For example, in softmax functions, inner product values are
often logits and are later used to compute the probability. One direct consequence of (8) is that the
objective weighs pairs by their importance and thus leads to lower estimation error on top-ranking
pairs. We measure | ’”］；"哈 | as the relative error on true inner product. New objective clearly
produces smaller relative error over all bitrate settings (Figure. 2c).
5.2	Maximum inner product search retrieval
Next, we show our MIPS retrieval performance with fixed number of bits. We compare to that of
QUIPS Guo et al. (2016) which achieves the state-of-the-art on MIPS tasks. QUIPS describes three
variants, QUIPS-Cov(x), QUIPS-Cov(q) and QUIPS-Opt which uses covariance of database
vectors, covariance of query vectors and sample queries respectively. In Figure 3. We measure the
performance at fixed bitrate by Recall 1@N, which corresponds to the fraction that Top-1 ground
truth result is recalled in N retrieved results. Clearly the results using our proposed loss function
out-performs all of the variants in QUIPS.
Other quantization methods may benefit from new loss function by switching from reconstruction.
For example, binary quantization such as Dai et al. (2017) uses reconstruction loss in its original
paper, which can be easily swapped for the proposed loss by one line change of the loss objective.
We show the results which illustrated the improvement of loss function in Section 7.8 of Appendix. It
is possible that other quantization methods also see a moderate improvement. We will discuss our
attempt with Local Search Quantization (LSQ) Martinez et al. (2018) in Section 7.7 of the Appendix.
5.3	Extreme classification inference
Extreme classification with large number of classes requires evaluating the last layer (classification
layer) with all possible classes. When there are O(M) classes, this becomes a major computation
bottleneck as it involves huge matrix multiplication followed by Top-K. Thus this is often solved
using Maximum Inner Product Search to speed up the inference. We evaluate our methods on extreme
classification using the Amazon-670k dataset Bhatia et al. (2015). An MLP classifier is trained over
670,091 classes, where the last layer has a dimensionality of 1,024. We evaluate retrieval performance
on the classification layer and show the results in Figure. 3c, by comparing it against brute force
matrix multiplication.
7
Under review as a conference paper at ICLR 2020
N©I -8φκ
Recall of Glove-1.2M -100 bits
QUIPS-Cov(X)
QUIPS-tov(q)
QUIPS⅛…
Ou rs「一
20	40	60	80	100
M
Recall of Glove-1.2M - 200 bits
0.9
≡ 0.8
©
=0.7
/ 0.6
0.5
QUIPS-Cov(X)
QUIPS-Cov(q)
QUlPS-Opt
Ours
0.4∣--------ɪ-----------i-------ɪ---------ɪ----------- _
0	20	40	60	80	100
105
104
)s/1( dnoces rep seireu
103
102
101
100
Recall-Query per second (1/s) tradeoff - up and to the right is better
■
0.5	0.6	0.7	0.8	0.9
annoy —I—
BallTree (nmslib) —X-
faiss-ivf ―来一
hnsw (faiss) -B—
rpforest —■-
NGT-Panng	Θ“…
mrpt —•—
Recall (k=10)
hnswlib —Δ—
SW-graph (nmslib) -A—
ours -V-
ours (batch = 1000) ―▼—
kd 9…“
NGT-onng .
(c) Speed-recall trade-off on Glove1.2M Recall@10
(a) MIPS recall on Glove1.2M.
Bitrate	1@1	1@10	1@100	Bitrate	1@1	1@10	1@100
256 bits, PQ=	0.652	0.995	0.999	512bits, PQ=	0.737	0.998	1.000
256 bits, OurS-	0.656	0.996	1.000	512 bits, OUrS-	0.744	0.997	1.000
1024 bits, PQ=	0.778	1.000	1.000	2048 bits, PQ=	0.782	1.000	1.000
1024 bits, OurT	0.812	1.000	1.000	2048 bits, OurT	0.875	1.000	1.000
(b) Extreme classification on Amazon670k.
Figure 3: (a) Recall 1@N curve on Glove1.2M comparing with variants of QUIPS Guo et al.
(2016) on MIPS tasks. (b) Extreme classification on Amazon670k performed through MIPS,
compared to baseline product quantization that uses reconstruction loss. (c) Recall-Speed benchmark
with 11 baselines from Aumuller et al. (2019) on Glove1.2M. The parameters of each baselines are
pre-tuned and released on: http://ann-benchmarks.com/. Our approach compare favorably
on speed-recall trade-off over popular state-of-the-art, production ready methods.
5.4	Recall-Speed benchmark
Fixed-bit-rate experiments mostly compare asymptotic behavior, and often overlook preprocessing
overhead such as learned rotation or lookup table computation, which can be substantial. To
evaluate effectiveness of MIPS algorithms in realistic setting, it is important to perform end-to-end
benchmarks and compare the speed-recall curve. We adopted the methodology of public benchmark
ANN-benchmarks Aumuller et al. (2019), which plots a comprehensive set of 11 algorithms for
comparison, including faiss Johnson et al. (2017) and hnswlib Malkov and Yashunin (2016).
Our benchmarks are conducted on same platform of Intel Xeon W-2135 with one CPU single thread,
and followed the benchmark’s protocol. Our implementation builds on product quantization with the
proposed quantization and SIMD based ADC Guo et al. (2016) for distance computation. This is
further combined with a vector quantization based tree Wu et al. (2017), and our curve is plotted by
varying the number of leaves to search in the tree. Figure 3c shows our performance on Glove1.2M
significantly outperforms the competing methods, especially in high recall region, where Recall of 10
is over 80%. We are committed our open source our implementation and parameter tunings.
6	Conclusion
In this paper, we propose a new quantization loss function for inner product search, which replaces
traditional reconstruction error. The new loss function is weighted based on the inner product values,
giving more weight to the pairs of query and database points with higher inner product values. The
proposed loss function is theoretically proven and can be applied to a wide range of quantization
methods, for example product and binary quantization. Our experiments show superior performance
on retrieval recall and inner product value estimation, compared to methods that use reconstruction
error. The speed-recall benchmark on public datasets further indicates that the proposed method
outperform state-of-arts baselines which are known to be hard to beat.
8
Under review as a conference paper at ICLR 2020
References
Paolo Cremonesi, Yehuda Koren, and Roberto Turrin. Performance of recommender algorithms on
top-n recommendation tasks. In Proceedings of the Fourth ACM Conference on Recommender
Systems, pages 39-46, 2010.
Thomas Dean, Mark Ruzon, Mark Segal, Jonathon Shlens, Sudheendra Vijayanarasimhan, and
Jay Yagnik. Fast, accurate detection of 100,000 object classes on a single machine: Technical
supplement. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,
2013.
Ian En-Hsu Yen, Satyen Kale, Felix Yu, Daniel Holtmann-Rice, Sanjiv Kumar, and Pradeep Raviku-
mar. Loss decomposition for fast learning in large output spaces. In Proceedings of the 35th
International Conference on Machine Learning, volume 80, pages 5640-5649, 2018.
Stephen Mussmann and Stefano Ermon. Learning and inference via maximum inner product search.
In Proceedings of The 33rd International Conference on Machine Learning, volume 48, pages
2587-2596, 2016.
Alexander PritzeL Benigno Uria, Sriram Srinivasan, Adri鱼 PUigdomenech Badia, Oriol Vinyals,
Demis Hassabis, Daan Wierstra, and Charles Blundell. Neural episodic control. In Proceedings of
the 34th International Conference on Machine Learning, volUme 70, pages 2827-2836, 2017.
T. Ge, K. He, Q. Ke, and J. SUn. Optimized prodUct qUantization. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 36(4):744-755, April 2014. ISSN 0162-8828. doi: 10.1109/
TPAMI.2013.240.
Artem Babenko and Victor Lempitsky. Additive qUantization for extreme vector compression. In
Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 931-938.
IEEE, 2014.
Jeff Johnson, Matthijs Douze, and Herve J6gou. Billion-scale similarity search with gpus. arXiv
preprint arXiv:1702.08734, 2017.
Mohammad Norouzi, Ali Punjani, and David J Fleet. Fast exact search in hamming space with
multi-index hashing. IEEE transactions on pattern analysis and machine intelligence, 36(6):
1107-1119, 2014.
Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search.
IEEE transactions on pattern analysis and machine intelligence, 33(1):117-128, 2011.
Xiang Wu, Ruiqi Guo, Ananda Theertha Suresh, Sanjiv Kumar, Daniel N Holtmann-Rice, David
Simcha, and Felix Yu. Multiscale quantization for fast similarity search. In Advances in Neural
Information Processing Systems 30, pages 5745-5755. 2017.
Jingdong Wang, Heng Tao Shen, Jingkuan Song, and Jianqiu Ji. Hashing for similarity search: A
survey. arXiv preprint arXiv:1408.2927, 2014.
Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. Learning to hash for indexing big data survey.
Proceedings of the IEEE, 104(1):34-57, 2016.
Anshumali Shrivastava and Ping Li. Asymmetric lsh (alsh) for sublinear time maximum inner product
search (mips). In Advances in Neural Information Processing Systems, pages 2321-2329, 2014.
Behnam Neyshabur and Nathan Srebro. On symmetric and asymmetric lshs for inner product search.
arXiv preprint arXiv:1410.5518, 2014.
Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of
dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing,
pages 604-613. ACM, 1998.
Ruiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, and David Simcha. Quantization based fast inner
product search. In Proceedings of the 19th International Conference on Artificial Intelligence
and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016, pages 482-490, 2016. URL
http://jmlr.org/proceedings/papers/v51/guo16a.html.
9
Under review as a conference paper at ICLR 2020
Julieta Martinez, Joris Clement, Holger H Hoos, and James J Little. Revisiting additive quantization.
In European Conference on Computer Vision, pages 137-153. Springer, 2016.
Julieta Martinez, Shobhit Zakhmi, Holger H Hoos, and James J Little. Lsq++: Lower running time
and higher recall in multi-codebook quantization. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 491-506, 2018.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv
preprint arXiv:1612.01064, 2016.
Kaiming He, Fang Wen, and Jian Sun. K-means hashing: An affinity-preserving quantization method
for learning binary compact codes. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 2938-2945, 2013.
Venice Erin Liong, Jiwen Lu, Gang Wang, Pierre Moulin, and Jie Zhou. Deep hashing for compact
binary codes learning. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2015.
Bo Dai, Ruiqi Guo, Sanjiv Kumar, Niao He, and Le Song. Stochastic generative hashing. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 913-
922. JMLR. org, 2017.
Ting Zhang, Chao Du, and Jingdong Wang. Composite quantization for approximate nearest neighbor
search. In ICML, volume 2, page 3, 2014.
Yunchao Gong, Svetlana Lazebnik, Albert Gordo, and Florent Perronnin. Iterative quantization: A
procrustean approach to learning binary codes for large-scale image retrieval. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 35(12):2916-2929, 2013.
Marius Muja and David G Lowe. Scalable nearest neighbor algorithms for high dimensional data.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(11):2227-2240, 2014.
Sanjoy Dasgupta and Yoav Freund. Random projection trees and low dimensional manifolds. In
Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 537-546.
ACM, 2008.
Yury A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search
using hierarchical navigable small world graphs. CoRR, abs/1603.09320, 2016. URL http:
//arxiv.org/abs/1603.09320.
Ben Harwood and Tom Drummond. FANNG: Fast approximate nearest neighbour graphs. In
Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on, pages 5713-5722.
IEEE, 2016.
Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Practical
and optimal lsh for angular distance. In Advances in Neural Information Processing Systems, pages
1225-1233, 2015.
Parikshit Ram and Alexander G Gray. Maximum inner-product search using cone trees. In Proceed-
ings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,
pages 931-939. ACM, 2012.
Artem Babenko and Victor Lempitsky. The inverted multi-index. In Computer Vision and Pattern
Recognition (CVPR), 2012 IEEE Conference on, pages 3069-3076. IEEE, 2012.
Yusuke Matsui, Toshihiko Yamasaki, and Kiyoharu Aizawa. Pqtable: Fast exact asymmetric dis-
tance neighbor search for product quantization using hash tables. In Proceedings of the IEEE
International Conference on Computer Vision, pages 1940-1948, 2015.
Qi Chen, Haidong Wang, Mingqin Li, Gang Ren, Scarlett Li, Jeffery Zhu, Jason Li, Chuanjie Liu,
Lintao Zhang, and Jingdong Wang. SPTAG: A library for fast approximate nearest neighbor
search, 2018. URL https://github.com/Microsoft/SPTAG.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for
word representation. In Empirical Methods in Natural Language Processing (EMNLP), pages
1532-1543, 2014.
10
Under review as a conference paper at ICLR 2020
Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik Varma, and Prateek Jain. Sparse local
embeddings for extreme multi-label classification. In Advances in neural information processing
systems, pages 730-738, 2015.
Martin Aumuller, Erik Bernhardsson, and Alexander Faithfull. Ann-benchmarks: A benchmarking
tool for approximate nearest neighbor algorithms. Information Systems, 2019.
Miguel A Carreira-Perpindn and Ramin Raziperchikolaei. Hashing with binary autoencoders. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 557-566,
2015.
11
Under review as a conference paper at ICLR 2020
7 Appendix
7.1	Proof of Theorem 3.2
Proof of Theorem 3.2. Let θ := arccos t, and α := arccos(T∕b). Note that dP '(hidx"t is propor-
tional to the surface area of (d - 1)-dimensional hypersphere with a radius of sin θ. Thus we have
dP(hd'Xi=t) 8 Sd-ι sind-2 θ, Where Sd-ι is the surface area of (d - 1)-sphere with unit radius.
Thus, λ(T, b) can be re-written as:
Rα cos2 θSd-1 sind-2 θdθ	Rα sind-2 θdθ
MT b' = R'α sin2 θSd-1 sind-2 θdθ = Rla Sind θdθ	L
Denote Id = R0α sind θdθ,
Id = - cos α sind-1 α +	cos2 θ(d - 1) sind-2 θdθ
0
= - cos α sind-1 α + (d - 1)	sind-2 θdθ - (d - 1)	sind θdθ
00
= - cos α sin α + (d - 1)Id-2 - (d - 1)Id .
This gives us a recursive formula to compute Id when d is a positive integer :
-cos a Sind-1 α	d - 1
d	d	d	d-2
(13)
With the base case of I0 = α, and I1 = 1 - cos α, the exact value of λ
explicitly in O(d) time.
Id-2 - 1 can be computed
d	□
7.2	Proof of Theorem 3.3
ProofofTheorem 3.3. First, it is easy to see that because Sind-2 α < sind α, II-2 < 1. Next, from
Cauchy-Schwarz inequality for integrals, we have
Sind+2 x Sind-2 xdx ≤	Sind+2xdx2	Sind-2 xdx2
Rearranging this we have IId^ ≤ Id-2, which proves that Id-2 is monotonically non-increasing.
Given that it has a lower bound and is monotonically non-increasing, the limit of IId^ exists.
Dividing both sides of Equation. 13 by Id, we have
-cos a sind-1 α	(d - 1)Id-2
=	did	did
Thus limd→∞
(d-1)Id-2
did
exists. Furthermore,
limd→∞ Id-2 > 1 exists. And therefore lim
Id	d→∞
coS α Sind-1 α
dId
> 0 also
lim
d→∞
CoS a Sind-I α
did
CoS a Sind-3 a
(d —2)Id-2
ɪ - 1 =
Sin α
1 ⇒ lim
d→∞
(d - 2)Id-2
dId
1
Sin2 α
Finally we have lim
d→∞
T/b
1 - (T /b)2
, and this proves Theorem 3.3.
λ
□
12
Under review as a conference paper at ICLR 2020
7.3 PROOF OF THEOREM 4.1
ProofofTheorem 4.1. Indeed,
g(xi,X) := μkr∣(xi,X)k2 + ∣∣r⊥(xi,X)k2
=μ((X -Xc)J Xi)2 + k(x - χi)-
Xi
llχik	llχik
l2
(X - Xi)T(X - Xi) + (μ - 1)
((X - Xi)TXi)2
(Xt X — 2xt X + xt Xi) + (μ — 1)
(Xt X — 2xt X + xt Xi) + (μ — 1)
lXi l2
C 不TT - _ TT T 八2
X Xi - Xi Xi
FrP
(XtXi)2 + IlXilI4 - 2XTXikXik2
(Xt X — 2xt X + xτ Xi) + (μ — 1)
lXi l2
XT x)2	C E
画春 + IIXik2 - 2XtXi
=∣∣X∣2 + (μ - I)(X χi2——2μXTXi + μ∣∣Xik2
kxik2
TT
F	ʃr σ∙√ ʃrf ʃr	m	C
=XTX + (μ - 1) —M-∣⅛-----2μXTXi + μ∣Xi∣2∙
kχik2
Therefore, the derivative of g(χi, X) with respect to X is
当/= 2X + 2 舟 XiXT X - 2μXi∙
Setting PNl %2=0, we have
X dg(xi,X) = 0
⅛ dx -
m /	]	、
T2X + 2μ-r-yXiXTX — 2μXi j = 0
M ∖	kχik2	)
0 fl + 口 X χ⅝! X = μ j
∖ m M l∣Xik2/	m
jχ=∕ι+g X 隹!-1 J
∖ m M kxik2 J	m
□
13
Under review as a conference paper at ICLR 2020
7.4 Algorithm for Vector Quantization with the modified objective
Algorithm 1 Proposed Vector Quantization Algorithm For Minimizing Weighted Quantization Errors
Input:
•	A set of N datapoints x1 , x2 , . . . , xN ∈ Rd.
•	A scalar μ > 0, the weight for quantization error components tradeoff (selection guided by
desired inner product threshold T in Sec 3).
•	A positive integer k, the size of the codebook.
Output:
•	A set of codebook c1, c2, . . . , ck ∈ Rd
•	Partition assignment a1, a2, . . . , aN ∈ {1, 2, . . . , k} for the N datapoints such that xi is
quantized as cai .
Algorithm:
Initialize the centroids c1, c2, . . . , ck by choosing k random datapoints.
Set new_error = +∞.
do
old_error J new_error.
[Partition Assignment]
for each i ∈ {1, 2, . . . , N} do
a，i J arg min (μ∣∣r∣∣(xi,Cj )k2 + ∣∣r⊥ (xi,cj )∣∣2)
j
end for
[Centroid Update]
for each i ∈ {1, 2, . . . , k} do
S J {xj |aj = i,1 ≤ j ≤ N}
, (飞上 μ - 1 V XxT ∖	Pχ∈S X
Cij”(1 + ISrX∈SPd ɪ
where |S | denote the cardinality of the set S.
end for
Computenew_error J PN=I (||n(x” CaJk2 + λ∣r⊥(xi, CaJk2)
while old_error < new_error
Output the codebook c1, c2, . . . , ck and the assignment a1, a2, . . . , aN.
7.5	SENSITIVITY TO CHOICE OFT
Another interesting question is how the retrieval performance relates to T, given λ(T, b). Intuitively,
if T is set too low, then the objective takes almost all pairs of query and database points into
consideration, and becomes similar to the standard reconstruction loss. If T is too high, then very few
pairs will be considered and the quantization may be inaccurate for low value inner product pairs.
Figure 2b shows the retrieval evaluation of Glove1.2M dataset under different T. We use T = 0.2
for all of our retrieval experiments.
14
Under review as a conference paper at ICLR 2020
7.6	Codebook Optimization in Product Quantization
For example, consider the first vector C1,1 in the codebook C1 for the first subspace, and let xi be
一一一 ・一一 一	.................. 一.	71)	—	—
one of the datapoints the first subspace of which is encoded as C1,1, i.e., xi	= C1,1. Write xi as
,rn (-λ V 一一 ..	， ~ (二 1一 一
xi = (xi , xi ), and x7i as x7i = (C1,1 , xi	). Then
μ∣∣r∣∣(xi,Xi)k2 + ∣∣r⊥(xi ,Xi )k2
XiTXi + (μ - 1)
x Tx xTx
xi xixi xi
∣Xik2
-2μχi Xi + μkxik
T
= (C1T,1 C1,1 + Xi(- )
CT X(1)X(1)
+ (μ — 1) C1,1i i
T
X(T))- 2μ(CTιx(1) + X(T) X(T)) + μkxik2
TT
1 C1,1 +2X(T) X(T)x(1) C1,1 + X(T) X(T)X(T) X(T)
Fif
Let S denote the set of indices S := {i|Ai,1 = 1}. Therefore, the partial derivative of (12) with
respect to C1,1 is
Σ
i∈S
'	•	.	∙.丁	.	.	.丁	.
2X(1)X(1) C + 2X(1)X(-1) X(-7 1)
2Xi Xi C1,1 + 2Xi Xi Xi
2C1J + (μ — 1)--------------Pik----------------
(14)
Set (14) to be zero, and we have
μ — 1 VX(I)X(I) ∖	Pi∈s(X(D -
1+讨 X” J —
(1-1 )x(1)X(T)
kXik2
|S I
_ ~
X(- 1)
xi )
(15)
〜
7.7	Notes on applying new loss function on LSQ
Local Search Quantization (LSQ) has shown strong results on achieving state-of-the-art performance
on SIFT1M with fixed-bit-rates. Since LSQ also uses reconstruction loss, it is possible that the new
loss would provide additional improvement over it. The author released the code on Github built on
CUDA and Julia. However we ran into multiple issues with julia-0.7.0 and GeForce GTX 1080 (8G
of GPU RAM), which seems to be related internals of Julia’s CuArray library that causes OOM. Even
though we tried our best to fix the issues, we weren’t able to apply this on LSQ. We refer readers
to the open issues of LSQ’s code on https://github.com/una-dinosauria/Rayuela.
jl/issues/ for more context. We would love to update and release the results if the issues are
resolved.
15
Under review as a conference paper at ICLR 2020
7.8	Results on B inary Quantization
Another popular family of quantization function is binary quantization. In such a setting, a function
h(x) := Rd → {0, 1}h is learned to quantize datapoints into binary codes, which saves storage
space and can speed up distance computation. There are many possible ways to design such a binary
quantization function, and some Carreira-Perpindn and Raziperchikolaei (2015); Dai et al. (2017)
uses reconstruction loss.
It is straight-forward to apply our proposed loss function there and the details of algorithm. We follow
the setting of Stochastic Generative Hashing (SGH) Dai et al. (2017), which explicitly minimizes
reconstruction loss and has been shown to outperform earlier baselines. In their paper, a binary
auto-encoder is learned to quantize and dequantize binary codes:
X = g(h(x)); where h(x) ∈ {0,1}h
where h(∙) is the “encoder” part which binarizes original datapoint into binary space and g(∙) is the
“decoder” part which reconstructs the datapoints given the binary codes. The authors of the paper
uses h(x) = sign(WhT x + bh) as the encoder function and g(h) = WgTh as the decoder functions,
respectively. The learning objective is to minimize the reconstruction error of ||x - X||2, and the
weights in the encoder and decoder are optimized end-to-end using standard stochastic gradient
descent. Following our discussion in Section. 3, this is an suboptimal objective in the MIPS settings,
and the learning objective given in (8) should be preferred. This implies minimal changes to the
algorithm of Dai et al. (2017) except the loss function part, while holding everything else unchanged.
We show below the results of SGH Dai et al. (2017) and SGH with the new loss function to illustrate
the proposed loss function can be easily applied to binary quantization case.
SIFTIM	1@1	1@10	10@10	10@100
64 bits, SGH	0.028	0.096	-0.053-	-0.220-
64 bits, SGH-newloss-	0.071	0.185	0.093	0.327
128 bits, SGH	0.073	0.195	0.105	0.376
128 bits, SGH-newloss	0.196	0.406	0.209	0.574
256 bits, SGH	0.142	0.331	0.172	0.539
256 bits, SGH-newloss	0.362	0.662	0.363	0.820
16
Under review as a conference paper at ICLR 2020
7.9 Idiosyncrasy of datasets for MIPS / NNS evaluation
With the increasing interest in Maximum Inner Product Search problem (MIPS) and its practical value
in embedding based retrieval systems, researchers compete intensively on evaluation for publications.
The widely used datasets for evaluating these include SIFT1M/1B, GIST1M, Glove1.2M for
NNS, as well as Movielens and Netflix for MIPS. However, we argue many datasets have
data distribution issues and can lead to misinformed conclusions about the relative performance of
algorithms. Some of these datasets suffer from (1) serve representation redundancy; (2) heterogeneous
importance across dimensions. Since algorithms were often developed based on the empirical
observation of these datasets, some have adapted to exploit the idiosyncrasies of particular datasets.
Our suggestion is to evaluate embedding vectors which show less data distribution issues, and thus
methods can be compared on the merit of their effectiveness against well-formed vectors, and revisit
some of comparisons.
SIFT1M, SIFT1B and GIST1M are introduced by Jegou et al. (2011) to illustrate the use of
product quantization. SIFT is a keypoint descriptor while GIST is image-level descriptor, which have
been hand-crafted for image retrieval. These vectors have a high correlation between dimensions and
has a high degree of redundantly. The intrinsic dimension of SIFT1M and GIST are much lower than
its dimensionality.
Movielens and Netflix dataset are formed from the SVD of the rating matrix of Movielens
and Netflix websites, respectively. This is introduced by Shrivastava and Li (2014) for MIPS
retrieval evaluation. Following SVD of X = (U A1/2T )(A1/2 V), the dimension of these two datasets
correspond to the eigenvalues of X . Thus the variance of dimensions are sorted by eigenvalues, and
the first few dimensions are much more important than later ones. Moreover, the datasets are O(10k)
in size and can hardly be called large-scale for evaluation purposes.
Glove1.2M is a word embeddings dataset similar to word2vec, which use neural-network style
training with a bottleneck layer. Similarly we also created Amazon670k which comes from a
feedforward network. These datasets exhibit less data distribution problem as illustrated in In fact, it
is our general observation that bottleneck layer lead to independent dimensions with similar entropy,
making them good datasets for benchmarking.
Below we plot the correlation and variance by dimensions of SIFT1M, GIST1M, MovielensSVD,
NetflixSVD, as well as Glove1.2M and Amazon670k. Itis obvious that SIFT1M and GIST1M
has strong correlations between dimensions and the intrinsic dimensions are significantly lower than
its original dimension. On the hand MovielensSVD and NetflixSVD suffers from problem of
variance of numeric scales across dimensions, as shown in the graph below. With huge variance in
the numeric scale, a few dimensions are dominating the results and the rest of dimension play much
smaller roles and can almost be discarded.Thus we argue, despite the popular of first four datasets,
has strong structure in data distribution and are not effective representation. On such datasets, it is
difficult to judge the performance of a MIPS / NNS algorithm, because the dataset idiosyncrasy may
be exploited to provide much gain to the extend it may overwhelm the effect of searching strategies.
We argue that the role of a MIPS / NNS algorithm should be efficient search against well-formed
data, not fixing representational deficiency of features in the dataset. On the other hand, dataset
such as Glove1.2M, Amazon670k, or empirically many neural network embedding has less
distribution structures. This is perhaps because bottleneck layers encourage each dimension to encode
independent information and carry similar amount of entropy. We suggest that these datasets should
be used for benchmarking, under reproducible settings such as http://www.ann-benchmarks.
17
Under review as a conference paper at ICLR 2020
Dataset	Size	Correlation	Variance by dimension
SIFT1M	(1000000, 128)	l Correlation rηatrixlof Sy=TI. ≡ lllllll 0	20	40	60	80	100 120 Dimension	30QQ -I	YarianCq per dirηensiθQ on SIFyiM l ⅛jj⅜⅛j⅛; H 1 1 1 1 1 1-- 0 20 40 60 80 1□□ 120 Dimension
GIST1M	(1000000, 960)	Correlation matrix .of GISTlM ‰ ×.;	.. i” 200	'>,∖ ''∖ - I- 0 75 U	4 ' I" 0'6° o 400-久入∙ ,S⅛ J⅛⅜√⅜ ∙. λ>- H I ■' . 1i-Λ√∙∙ ∙-.,i'∙, Γ45 a 6oo -*、，浅.、_ ^I- 030 ,噜，，;∖ 0	200	⅛J0	600 SOO Dimension	Varjance per pimenSioq on GlSTIM ClQ7 τ	i	''>	i	[	- I 二 ιl⅛l⅜ 003 -J	i		U	i	 0	200	400	600	800 Dimension
MovielensSVD	(10681, 150)	C9rrelρti09 matrix Pf MPVielenSPVD 1∖ - -090 xχ	- 0.75 40 -	∖	- ∖	- oβα § ∞- ∖ - «	X	- 0 45 ® 80 -	∖	- =m- ∖ .卜” ∖	■- 0.15 I2”	∖.	- -0.00 140 -	∖-	--0 15 0	20 40 60 80 100 120 140 Dimension	16 VaqanCC per dimension on MHvitιle∕ιsSVD ⊂ -4I	:	:	，	- j ；|tEzE：Z Z ZZZ： »-, ――	—一 0	20	40	60	30	100	120	140 Dimension
NetflixSVD	(17770, 300)	porre∣atioη matrix OfINetflilXSVD_ 二∖	一” 100 -	∖κ	-	- 0.69 I 150-	∖	- -° 45 £	ʌv	- α,30 c 200-	∖,	- I ∖	0.15 250 -	x∖	-	- 0.00 ∖	--0.15 Illlll 0	50	100 150	200 250 Dimension	20	Variance p^r dimension onl NetfIix^VD L ”1—I— lη……I……；…；……；……；… QJ i i i i i r 0 50 100 150 200 250 300 Dimension
Glove1.2M	(1183514, 100)	° lCorrelation rpatrix lof Gloγel00 - 0.90 20 -	›⅛	- - 0.75 λ∖	- 0.60 I40-	∖	- -0-45 .∣ 60-	； -	- 0∙30 - CK 5 80 -	-	- 0.00 ∖	- -α.ιs Illll 0	20	40	60	80 Dimension	θ 020	Variance per fiimensioŋ on GIOVqlOO u .5 0.005 4	i-	；	；	i-	- 「	i	；	Ii. 0	20	40	60	80	100 Dimension
Amazon670k	(670091, 1024)	UGOrreIalion matrix qf Amafon67pk ∖	- U J∙: 2∞ -	、、\	- - o,75 ∖	- 0 60 G ⅛)0 -	∖	- ,K	∖	- 0.45 E 600 -	∖	1I-	0.30 ∖	-	0,L5 800 -	∖	- ∖	,	-	0.00 1000 -	∖-	- -0.15 0	200	400	600	B00	1000 Dimension	0O4O-∣ Variance per d∣mension lon Amaz9n67θk l U 0-OS5 J	i	-i	⅛	i	i— I 003。帆W**Il阳MIMH⅛v≡⅛- ∙≡ 0020 - ζ ɔois J e OSU 4	  - > 3Λ6S -I	：	- ° CCC ，	I	 0	200	400	600	800	1000 Dimension
18