Under review as a conference paper at ICLR 2020
A Functional Characterization of Randomly
Initialized Gradient Descent in Deep ReLU
Networks
Anonymous authors
Paper under double-blind review
Ab stract
Despite their popularity and successes, deep neural networks are poorly under-
stood theoretically and treated as ’black box’ systems. Using a functional view of
these networks gives us a useful new lens with which to understand them. Theoret-
ical (in shallow) and experimentally probing properties of these networks reveals
insights into the effect of standard initializations, the value of depth, the underly-
ing loss surface, and the origins of generalization. One key result is that general-
ization results from smoothness of the functional approximation, combined with
a flat initial approximation. This smoothness increases with number of units, ex-
plaining why massively overparamaterized networks continue to generalize well.
1 Introduction
Deep neural networks, trained via gradient descent, have revolutionized the field of machine learn-
ing. Despite their widespread adoption, theoretical understanding of fundamental properties of deep
learning - the true value of depth, the root cause of implicit regularization, and the seemingly ‘un-
reasonable' generalization achieved by overparameterized networks - remains mysterious.
Empirically, it is known that depth is critical to the success of deep learning. Theoretically, it has
been proven that maximum expressivity grows exponentially with depth, with a smaller number of
trainable parameters (Raghu et al., 2017; Poole et al., 2016). This theoretical capacity may not be
used, as recently shown explicitly by (Hanin & Rolnick, 2019). Instead, the number of regions within
a trained network is proportional to the total number of hidden units, regardless of depth. Clearly
deep networks perform better, but what is the value of depth if not in increasing expressivity?
Another major factor leading to the success and widespread adoption of deep learning has been
its surprisingly high generalization performance (Zhang et al., 2016). In contrast to other machine
learning techniques, continuing to add parameters to a deep network (beyond zero training loss)
tends to improve generalization performance. This is even for networks that are massively overpa-
rameterized, wherein according to traditional ML theory they should (over)fit all the training data
(Neyshabur et al., 2015). How does training deep networks with excess capacity lead to generaliza-
tion? And how can it be that this generalization error decreases with overparameterization?
We believe that taking a functional view allows us a new, useful lens with which to explore and
understand these issues. In particular, we focus on shallow and deep fully connected univariate
ReLU networks, whose parameters will always result in a Continuous Piecewise Linear (CPWL)
approximation to the target function. We provide theoretical results for shallow networks, with
experiments showing that these qualitative results hold in deeper nets.
Our approach is related to previous work from (Savarese et al., 2019; Arora et al., 2019; Frankle &
Carbin, 2018) in that we wish to characterize parameterization and generalization. We differ from
these other works by using small widths, rather than massively overparamaterized or infinite, and
by using a functional parameterization to measure properties such as smoothness. Other prior works
such as (Serra et al., 2017; Arora et al., 2016; Montufar et al., 2014) attempt to provide theoretical
upper or lower bounds to the number of induced pieces in ReLU networks, whereas we are more
interested in the empirical number of pieces in example tasks. Interestingly, (Serra et al., 2017) also
takes a functional view, but is not interested in training and generalization as we are. Previous work
1
Under review as a conference paper at ICLR 2020
(Advani & Saxe, 2017) has hinted at the importance of small norm initialization, but the functional
perspective allows us to prove generalization properties in shallow networks.
Main Contributions The main contribution of this work are as follows:
-	Functional Perspective of Initialization: Increasingly Flat with Depth. In the functional perspec-
tive, neural network parameters determine the locations of breakpoints and their delta-slopes (de-
fined in Section 2.1) in the CPWL reparameterization. We prove that, for common initializations,
these distributions are mean 0 with low standard deviation. The delta-slope distribution becomes
increasingly concentrated as the depth of the network increases, leading to flatter approximations.
In contrast, the breakpoint distribution grows wider, allowing deeper network to better approxi-
mate over a broader range of inputs.
-	Value of Depth: Optimization, not Expressivity. Theoretically, depth adds an exponential amount
of expressivity. Empirically, this is not true in trained deep networks. We find that expressivity
scales with the number of total units, and weakly if at all with depth. However, we find that depth
makes it easier for GD to optimize the resulting network, allowing for a greater flexibility in the
movement of breakpoints, as well as the number of breakpoints induced during training.
-	Generalization is due to Flat Initialization in the Overparameterized Regime. We find that gener-
alization in overparametrized FC ReLu nets is due to three factors: (i) the very flat initialization,
(ii) the curvature-based parametrization of the approximating function (breakpoints and delta-
slopes) and (iii) the role of gradient descent (GD) in preserving (i) and regularizing via (ii). In
particular, the global, rather than local, impact of breakpoints and delta-slopes helps regularize
the approximating function in the large gaps between training data, resulting in their smoothness.
Due to these nonlocal effects, more overparameterization leads to smoother approximations (all
else equal), and thus typically better generalization (Neyshabur et al., 2018; 2015).
2 Theoretical Results
2.1	ReLU Nets in Function Space: From Weights to Breakpoints & Slopes
Consider a fully connected ReLU neural net fθ (x) with a single hidden layer of width H, scalar
input X ∈ R and scalar output y ∈ R. f (∙; θ) is continuous piecewise linear function (CPWL) since
the ReLU nonlinearity is CPWL. We want to understand the function implemented by this neural
net, and so we ask: How do the CPWL parameters relate to the NN parameters? We answer this by
transforming from the NN parametrization (weights and biases) to two CPWL parametrizations:
H
f(x; Θnn ),	vi(wix + bi)+	(1)
i=1
=X μi(x - βi) ( Jx> eiK， Sil	, f(x; θBDSO)	⑵
i=1	Jx < βiK, si = -1
P
=EJep ≤ x < βp+ιK (mpX + Yp)	，f(x; Θpwl)	(3)
p=1
where the Iversen bracket JbK is 1 when the condition b is true, and 0 otherwise. Here the NN
parameters θNN , {(wi, bi, vi)}iH=1 denote the input weight, bias, and output weight of neuron i,
and (•)+，max{0, ∙} denotes the ReLU function. The first CPWL parametrization is Θbdso，
{(βi,μi, si)}H=ι, where βi ，-W is (the x-coordinate of) the breakpoint (or knot) induced by
neuron i, μi，Wivi is the delta-slope contribution of neuron i, and si，Sgn Wi ∈ {±1} is the
orientation of βi (left for si = -1, right for si = +1). Intuitively, in a good fit the breakpoints
βi will congregate in areas of high curvature in the ground truth function |f 00 (x)| ≥ 0, while delta-
slopes μi will actually implement the needed curvature by changing the slope by μi from one piece
p(i) to the next p(i) + 1. As the number of pieces grows, the approximation will improve, and the
delta-slopes (scaled by the piece lengths) approach the true curvature of f: limp→∞ μp(i)∕(βp -
βp-1) → f00 (x = βi).
2
Under review as a conference paper at ICLR 2020
We note that the BDSO parametrization of a ReLU NN is closely related to but different than a tra-
ditional roughness-minimizing m-th order spline parametrization fφiine(x)，PK=I μi(x - βi)m +
Pjm=0 cjxj: BDSO (i) lacks the base polynomial, and (ii) it has two possible breakpoint orientations
si ∈ {±1} whereas the spline only has one. We note in passing that adding in the base polynomial
(for linear case m = 1) into the BDSO ReLU parametrization yields a ReLU ResNet parametriza-
tion. We believe this is a novel viewpoint that may shed more light on the origin of the effectiveness
of ResNets, but we leave it for future work.
The second parametrization is the canonical one for PWL functions: θPWL , {(βp, mp, γp)}pP=1,
where βo < βι < ... < βp，- Wbp(i) < ... < βp is the sorted list of (the X-Coordinates of) the
P , H + 1 breakpoints (or knots), mp , γp are the slope and y-intercept of piece p.
Computing the analogous reparametrization to function space for deep networks is more involved,
so we present a basic overview here, and a more detailed treatment in Appendix B. For L ≥ 2 layers
with widths H('), the neural network,s activations are defined as: z(') = PH=ι w(')χj'-1) +
b('),χ(') = (z('))+,gθ(x) = z(L+1) for all hidden layers ' ∈ {1, 2,...,L} and for all neurons
i ∈ {1,2,..., H(`)}. Then β(') is a breakpoint induced by neuron i in layer ' if it is a zero-crossing
of the net input i.e. z(') (β(')) = 0.
Considering these parameterizations (especially the BDSO parameterization) provides a new, useful
lens with which to analyze neural nets, enabling us to reason more easily and transparently about
the initialization, loss surface, and training dynamics. The benefits of this approach derive from two
main properties: (1) that we have ‘modded out’ the degeneracies in the NN parameterization and (2)
the loss depends on the NN parameters θNN only through the BDSO parameters (the approximating
function) Θbdso i.e. '(Θnn ) = '(Θb DSO (Θnn )), analogous to the concept of a minimum sufficient
statistic in exponential family models. Much recent related work has also veered in this direction,
analyzing function space (Hanin & Rolnick, 2019; Balestriero et al., 2018).
2.2	Random Initialization in Function Space
We now study the random initializations commonly used in deep learning in function space.
These include the independent Gaussian initialization, with b 〜 N(0,σfc), Wi 〜 N(0, σw),
Vi 〜 N(0, σv), and independent Uniform initialization, with b 〜 U[-afc, ab∖, Wi 〜 U[-aw, aw],
Vi 〜U[-av, av∖. We find that common initializations result in flat functions, becoming flatter with
increasing depth.
Theorem 1. Consider a fully connected ReLU neural net with scalar input and output, and a sin-
gle hidden layer of width H. Let the weights and biases be initialized randomly according to a
zero-mean Gaussian or Uniform distribution. Then the induced distributions of the function space
parameters (breakpoints β, delta-slopes μ) are asfollows:
(a)	Under an independent Gaussian initialization,
Pβ,μ(βi,μi) = ----------/ 2 ,	2 a exp
2πσv σb2 + σw2 βi2
|〃i| ,σ2 + σW β2
σbσvσw
(b)	Under an independent Uniform initialization,
pβ,μ(βi, μi)
JM| ≤ min{ 臂V , aw , av }K
4abaw av
ab
min{百,
Using this result, we can immediately derive marginal and conditional distributions for the break-
points and delta-slopes.
Corollary 1. Consider the same setting as Theorem 1.
(a) In the case of an independent Gaussian initialization,
Pe⑼i = CauChy S；°, σw^) = π (σWβ +σ2)
3
Under review as a conference paper at ICLR 2020
pμ(μi) = 2∏σ1vσw G0,0 (4σvσw
0, 0
pμ∣β (μi lβi) — LaPIace I μi ； 0,	/2 QCf)
V	√στ+σwβ2
-ɪ Ko (以)
πσv σw	σv σw
—Pσ22 + σ22 β
—	exp
2σbσvσw
—
|〃i| ,σ2 + σW β
σbσvσw
where Gnm(∙∣∙) is the Meijer G-function and KV(∙) is the modifiedBesselfunction ofthe Second
kind.
(b) In the case of an indePendent Uniform initialization,
pβ (βi)—京
J - αw αv ≤ μi ≤ αw αv K
2aw av
log
Pμ (μi )
aw av
%|
Pμ∣β(μi∣βi) = Tri(μi; a min{o√∣βi∣,aw})
Jlμi | ≤ αv min{ab/|ei|, aw }K
av min{ ab/ lβi |, aw }
-
_________lμi∣_______
av min{ ab / lβi |, αw }
where Tri(∙; a) is the symmetric triangular distribution with base [-a, a] and mode 0.
Implications. Corollary 1 implies that the breakpoint density drops quickly away from the origin
for common initializations. If f has significant curvature far from the origin, then it may be far
more difficult to fit. We show that this is indeed the case by training a shallow ReLU NN with an
initialization that does not match the underlying curvature, with training becoming easier if the initial
breakpoint distribution better matches the function curvature. We also show that during training,
breakpoint distributions move to better match the underlying function curvature, and that this effect
increases with depth (see Section 3, Table 1, and Appendix A.6). This implies that a data-dependent
initialization, with a breakpoint distribution near areas of high curvature, could potentially be faster
and easier to train.
Next, We consider the typical Gaussian He (He et al., 2015) or Glorot (Glorot & Bengio) initializa-
tions. In the He initialization, we have σw — √2, σv —，2/H. In the Glorot initalization, we have
σw — σv —，2/(H + 1). We wish to consider their effect on the smoothness of the initial function
approximation. From here on, we measure the smoothness using a roughness metric, defined as
P，Pi μ2, where lower roughness indicates a smoother approximation.
Theorem 2. Consider the initial roughness ρ0 under a Gaussian initialization. In the He initializa-
tion, we have that the tail Probability is given by
IP[ρo - E[ρo] ≥ λ] ≤  ------λ2H,
r η	1 + 128
where E[ρo] — 4. In the Glorot initialization, we have that the tail probability is given by
!P[ρo - E[ρo] ≥ λ] ≤ ——ɪ
4H	— O (ɪ)	1 +	128H
(H+1)2 — O HH ).
where E[ρo]
λ2(H+1)4 ,
Thus, as the width H increases, the distribution of the roughness of the initial function fo gets
tighter around its mean. In the case of the He initialization, this mean is constant; in the Glo-
rot initialization, it decreases with H. In either case, for reasonable widths, the initial rough-
ness is small with high probability. This smoothness has implications for the implicit regulariza-
tion/generalization phenomenon observed in recent work (Neyshabur et al., 2018) (see Section 3 for
generalization/smoothness analysis during training).
Related Work. Several recent works analyze the random initialization in deep networks. However,
there are two main differences, First, they focus on the infinite width case (Savarese et al., 2019;
Jacot et al., 2018; Lee et al., 2017) and can thus use the Central Limit Theorem (CLT), whereas we
focus on finite width case and cannot use the CLT, thus requiring nontrivial mathematical machinery
(see Supplement for detailed proofs). Second, they focus on the activations as a function of input
whereas we also compute the joint densities of the BDSO parameters i.e. breakpoints and delta-
slopes. The latter is particularly important for understanding the non-uniform density of breakpoints
away from the origin as noted above.
4
Under review as a conference paper at ICLR 2020
2.3 Loss Surface in the Function Space
We now consider the mean squared error (MSE) loss as a function of either the NN parameters
'(Θnn) , PN=I 2(f(xn； θ) - yn)2 or the BDSO parameters '(θBDSO ). Now consider some
Θbdso. Then f(∙; ΘbDSO)induces a partition Π = (∏ι,..., ∏h+i) of the data {χn}N=ι SUch that
the restriction of ∕bdso to any piece of this partition, denoted f (∙; Θbdso )∣∏p, is a linear function.
Theorem 3.	Suppose that for all P ∈ [P ], f (∙; Θbdso )∣∏p is an Ordinary Least Squares fit of the
data in piece P. Then, Θb DSO is a critical point of '(Θb DSO).
An open question is how many such critical points exist. A starting point is to consider that there are
C(N +H, H) , (N+H)!/N!H! possible partitions of the data. Not every such partition will admit
a piecewise-OLS solution which is also continuous, and it is difficult to analytically characterize
such solutions, so we resort to simulation and find a lower bound that suggests the number of critical
points grows at least polynomially in N and H (Figure 7).
Using Theorem 3, we can characterize growth of global minima in the overparameterized case.
Call a partition Π lonely if each piece πp contains at most one datapoint. Then, we can prove the
following results:
Theorem 4.	For any lonely partition Π, there are infinitely many parameter settings θBDSO that
induce Π and are global minima With '(ΘbDSO)=0.
Proof. Note that each linear piece P has two degrees of freedom (slope and intercept). By way of
induction, start at (say) the left-most piece. If there is a datapoint in this piece, choose an arbitrary
slope and intercept that goes through it; otherwise, choose an arbitrary slope and intercept. At each
subsequent piece, we can use one degree of freedom to ensure continuity with the previous piece,
and use one degree of freedom to match the data (if there is any).	□
Remark 1. Suppose that the H breakpoints are uniformly spaced and that the N data points
are uniformly distributed within the region of breakpoints. Then in the overparametrized regime
H ≥ αN2 for some constant α > 1, the induced partition Π is lonely with high probabilility
1 一 e-N2/(H+1) = 1 一 e-1/a. Furthermore, the total number oflonely partitions, and thus a lower
bound on the total number of global minima of I is (HN+1)= O(N αN)
Thus, with only order N2 units, we can almost guarantee lonely partitions, where the piecewise OLS
solution on these lonely paratitions will be the global optimum. Note how simple and transparent the
function space explanation is for why overparametrization makes optimization easy, as compared to
the weight space explanation (Arora et al., 2019), requiring order N7 units.
2.4 Generalization: Implicit Regularization via Delta-Slope Parametrization
The above sections argue that overparameterization leads to a flatter initial function approximation,
and an easier time reaching a global minima over the training data. However, neural networks also
exhibit unreasonably high generalization performance, which must be due to implicit regularization,
since the effect is independent of loss function. Here we provide an argument that overparameteri-
zation directly leads to this implicit regularization, due to the increasing flatness of the initialization
and the non-locality of the delta-slope parameters.
Consider a dataset like that shown in Figure 8 with a data gap between regions of two con-
tinuous functions fL , fR and consider a breakpoint i with orientation si in the gap. Starting
with a flat initialization, the dynamics of the i-th delta-slope are “i(t) = 一 h^(t) Θ ai(t), Xi +
βi(t) h^(t) Θ ai(t), 1〉，r2,si(t) + r3,si(t)βi(t) where r2,s(t), r3,s(t) are the (negative) net corre-
lation and residual on the active side of i, in this case including data from the function fsi but not
f-si. Note that the both terms of the gradient ai have a weak dependence on i through the orienta-
tion si, and the second term additionally depends on i through βi(t). Thus the vector of delta-slopes
with orientation S evolves according to μS = r2,s(t)l + r3,s(t)βs. Now consider the regime of
overparametrization H N . It will turn out to be identical to taking a continuum limit H → ∞
yielding μi∕(βi-βi-ι) → μ(x,t)，f (x, t), the curvature of the approximation (the discrete index
5
Under review as a conference paper at ICLR 2020
■ 1 F	. ∙	♦∙>	∖	1 A∕>∖	r∖∕∕'11	♦ r∙	EI	r∙	1 . ∙ 1	A / > ∖
i has become a continuous index x) and βi (t) → 0 (following from Theorem 5, multiplying βi (t)
by vi(t)/wi(t) and factoring out μi(t) → 0). Integrating the dynamics μs(x,t) = r2,s(t) + r3,s(t)x
over all time yields μs(x, t = ∞) = μs(x, t = 0)+ R21,s + R3,sX, where the curvature μs(x, t =
0) ≈ 0 (Section 3) and R；S，f∞ dt0rj,s(t0) < ∞ (convergence of residuals %(t) and immobility
of breakpoints βi(t) = 0 implies convergence of rj,s(t)). Integrating over space twice(from x0= ξs
to X0 = x) yields a cubic spline f(x,t) = c0,s + cι,s(x - ξs) + c2,s(x - ξs)2∕2! + c3,s(x - ξs)3∕3!,
where c0,s , c1,s are integration constants determined by the per-piece boundary conditions (PBCs)
/(X = ξs,t = ∞) , Ps fS(X = ξs) = f (X = ξs) and f0(X = ξs,t = ∞) , Ps fS(X = ξs) =
f 0 (x = ξs ), thus matching the 0-th and 1st derivatives at the gap endpoints. The other two coeffi-
cients ck,s，Rk s,k ∈{2,3} and serve to match the 2nd and 3rd derivatives at the gap endpoints.
Clearly, matching the training data only requires the two parameters c0,s, c1,s; and yet, surprisingly,
two unexpected parameters c2,s , c3,s emerge that endow f with smoothness in the data gap, despite
the loss function not possessing any explicit regularization term. Tracing back to find the origin of
these smoothness-inducing terms, we see that they emerge as a consequence of (i) the smoothness
of the initial function and (ii) the active half space structure, which in turn arises due to the discrete
curvature-based (delta-slope) parameterization. Stepping back, the ReLU net parameterization is a
discretization of this underlying continuous 2nd-order ordinary differential equation. In Section 3
we conduct experiments to test this theory.
3 Experiments
Breaking Bad: Breakpoint densities that are mismatched to function curvature makes opti-
mization difficult We first test our initialization theory against real networks. We initialize fully-
connected ReLU networks of varying depths, according to the popular He initializations (He et al.,
2015). Figure 1 shows experimentally measured densities of breakpoints and delta-slopes. Our
theory matches the experiments well. The main points to note are that: (i) breakpoints are indeed
more highly concentrated around the origin, and that (ii) as depth increases, delta-slopes have lower
variance and thus lead to even flatter initial functions. We next ask whether the standard initial-
izations will experience difficulty fitting functions that have significant curvature away from the
origin (e.g. learning the energy function of a protein molecule). We train ReLU networks to fit
a periodic function (sin(X)), which has high curvature both at and far from the origin. We find
that the standard initializations do quite poorly away from the origin, consistent with our theory
that breakpoints are essential for modeling curvature. Probing further, we observe empirically that
breakpoints cannot migrate very far from their initial location, even if there are plenty of break-
points overall, leading to highly suboptimal fits. We additionally show (see Appendix A.6 for de-
tails) that breakpoint distributions change throughout training to more accurately match the ground
truth curvature. In order to prove that it is indeed the breakpoint density that is causally responsible,
we attempt to rescue the poor fitting by using a simple data-dependent initialization that samples
breakpoints uniformly over the training data range [Xmin , Xmax], achieved by exploiting Eq. (2).
We train shallow ReLU networks on training data
sampled from a sine and a quadratic function,
two extremes on the spectrum of curvature. The
data shows that uniform breakpoint density res-
cues bad fits in cases with significant curvature
far from the origin, with less effect on other cases,
confirming the theory. We note that this could be
Init	Sine
Standard 4.096 ± 2.25
Uniform 2.280 ± .457
Quadratic
.1032 ± 0404
.1118 ± .0248
Table 1: Test loss for standard vs uniform break-
2
point initialization, on sine and quadratic 号
a potentially useful data-dependent initialization strategy, one that can scale to high dimensions, but
we leave this for future work.
Explaining and Quantifying the Suboptimality of Gradient Descent. The suboptimality seen
above begs a larger question: under what conditions will GD be successful? Empirically, it has been
observed that neural nets must be massively overparameterized (relative to the number of parameters
needed to express the underlying function), in order to ensure good training performance. Our
theory provides a possible explanation for this phenomenon: if GD cannot move breakpoints too
far from their starting point, then one natural strategy is to sample as many breakpoints as possible
everywhere, allowing us to fit an arbitrary f. The downside of this strategy is that many breakpoints
will add little value. In order to test this explanation and, more generally, understand the root causes
6
Under review as a conference paper at ICLR 2020
Figure 1: Left: Training loss Vs number of pieces (α number of parameters) for various algorithms
fitting a CPWL function to a quadratic. Middle: Breakpoint distribution for a He initialization across
a 3 layer network. Right: Delta-slope distribution for a He initialization across a 3 layer network.
L	Sine	5 piece Poly	Sawtooth	Arctan	Exponential	Quadratic
1	40 ± 0	40TO	40±0	40Γ0	40Γ0	40±0
2	55.5 ± 2.9	52 ± 1.414	5O±.7	49.25 ± 3.3	51.25 ± 6.1	49.25 ± 4.5
4	68 ± 3.1	57.25 ± 6.8	48.5 ± 2.5	42.5 ± 4.8	40.25 ± 3.9	40.25 ± 3.3
5	62.25 ± 15.1	49 ± 3.5	44.5 ± 5.1	38 ± 5.1	33.75 ± 1.1	31.5 ± 1.7
Table 2: Comparison of the number of pieces induced in a network of up to depth 5, with 40 units
evenly distributed across layers, trained to fit varying target functions.
of the GD’s difficulty, we focus on the case ofa fully connected shallow ReLU network. A univariate
input (i) enables us to use our theory, (ii) allows for visualization of the entire learning trajectory,
and (iii) enables direct comparison with existing globally (near-)optimal algorithms for fitting PWL
functions. The latter include the Dynamic Programming algorithm (DP, (Bai & Perron, 1998)), and
a very fast greedy approximation known as Greedy Merge (GM, (Acharya et al., 2016)). How do
these algorithms compare to GD, across different target function classes, in terms of training loss,
and the number of pieces/hidden units? We use this metric for the neural network as well, rather
than the total number of trainable parameters.
Taking the functional approximation view allows us to directly compare neural network performance
to these PWL approximation algorithms. For a quadratic function (e.g. with high curvature, requir-
ing many pieces), we find that the globally optimal DP algorithm can quickly reduce training error to
near 0 with order 100 pieces. The GM algorithm, a relaxation of the DP algorithm, requires slightly
higher pieces, but requires significantly less computational power. On the other hand all variants
of GD (vanilla, Adam, SGD w/ BatchNorm) all require far more pieces to reduce error below a
target threshold, and may not even monotonically decrease error with number of pieces. Interest-
ingly, we observe a strict ordering of optimization quality with Adam outperforming BatchNorm
SGD outperforming Vanilla GD. These results (Figure 1) show how inefficient GD is with respect
to (functional) parameters, requiring orders of magnitude more for similar performance to exact or
approximate PWL fitting algorithms.
Learned Expressivity is not Exponential in Depth. In the previous experiment, we counted the
number of linear pieces in the CPWL approximation as the number of parameters, rather than the
number of weights. Empirically, we know that the greatest successes have come from deep learning.
This raises the question: how does the depth of a network affect its expressivity (as measured in
the number of pieces)? Theoretically, it is well known that maximum expressivity increases expo-
nentially with depth, which, in a deep ReLU neural network, means an exponential increase in the
number of linear pieces in the CPWL approximation. Thus, theoretically the main power of depth
is that it allows for more powerful function approximation relative to a fixed budget of parameters
compared to a shallow network. However, recent work (Hanin & Rolnick, 2019) has called this
into question, finding that in realistic networks expressivity does not scale exponentially with depth.
We perform a similar experiment here, asking how the number of pieces in the CPWL function
approximation of a deep ReLU network varies with depth.
The results in Table 2 clearly show that the number of pieces does not exponentially scale with
depth. In fact, we find that depth only has a weak effect overall, although more study is needed to
determine exactly what effect depth has on the number and variability of pieces. These results lend
more support to the recent findings of (Hanin & Rolnick, 2019), and of taking a functional view
7
Under review as a conference paper at ICLR 2020
Function	Shallow	Spiky Shallow	DeeP	Spiky Deep
Sine	42.95 ± 6.406	157.5 ± 60.27	31.48 ± 7.078	122.0 ± 128.2
Arctan	.01252 ± .07650	2.499 ± 1.257	0.9795 ± 0.9355	32.57 ± 26.10
Sawtooth	156.9 ± 12.45	150.1 ± 61.48	148.1 ± 8.755	198.0 ± 170.9
Cubic	3.608 ± 1.683	136.7 ± 124.1	56.77 ± 98.91	191.6 ± 114.1
Quadratic	3.559 ± 4.553	150.6 ± 49.00	1.741 ± 1.296	46.02 ± 19.42
Exp	.6509 ± .5928	181.1 ± 75.36	1.339 ± 1.292	54.50 ± 37.77
Table 3: Comparison of testing loss (generalization ability) of various network shallow and deep
networks with a standard vs ‘spiky’ initialization
of measuring parameterization. Intriguingly, variability in the number of pieces appears to increase
with depth. From the functional approximation, we know that a unit induces breakpoints only when
the ReLU function applied to the unit’s input has zero crossings. In layer one, this happens exactly
once per unit as the input to each ReLU is just a line over the input space. In deeper layers, the
function approximation is learned, allowing for a varying number of new breakpoints. Given our
previous results on the flatness of the standard initializations, this will generally only happen once
per unit, implying that the number of pieces will strongly correlate with the number of units at
initialization.
Depth helps with Optimization by enabling the Creation, Annihilation and Mobility of Break-
points. If depth does not strongly increase expressivity, then it is natural to ask whether its value
lies with optimization. In order to test this, we examine how the CPWL function approximation de-
velops in each layer during learning, and how it depends on the target function. A good fit requires
that breakpoints accumulate at areas of higher curvature in the training data, as these regions require
more pieces. We argue that the deeper layers of a network help with this optimization procedure,
allowing the breakpoints more mobility as well as the power to create and annihilate breakpoints.
One key difference between the deeper layers of a network and the first layer is the ability for a single
unit to induce multiple breakpoints. As these units’ inputs change during learning, the number of
breakpoints induced by deeper units in a network can vary, allowing for another degree of freedom
for the network to optimize. Through the functional parameterization of the hidden layers, these
”births and deaths” of breakpoints can be tracked as changes in the number of breakpoints induced
per layer. Another possible explanation for the value added of depth is breakpoint mobility, or
that breakpoints in deeper layers can move more than those in shallow layers. We run experiments
comparing how the velocity and number of induced breakpoints varies between layers of a deeper
network.
relative to the first layer in each layer of a five layer ReLU network
Figure 2 shows the results. The number of breakpoints in deeper layers changes more often than in
shallow layers. The breakpoint velocity in deeper layers is also higher than the first layer, although
not monotonically increasing. Both of these results provide support for the idea that later layers help
significantly with optimization and breakpoint placement, even if they do not help as strongly with
expressivity.
Note that breakpoints induced by a layer of the network are present in the basis functions of all
deeper layers. Their functional approximations thus become more complex with depth. However
the roughness of the basis functions at initialization in the deeper layers is lower than that of the
shallow layers. But, as the network learns, for complex functions most of the roughness is in the
later layers as seen in Figure 3 (right).
Generalization: Implicit Regularization emerges from Flat Init and Curvature-based
Parametrization. The experiments above show that the functional view can give us a new per-
8
Under review as a conference paper at ICLR 2020
Figure 3: Roughness (summed by layer) during training for a 5 layer ReLU network with 8 units per
hidden layer, learning the quadratic function x2/2 (left) and the periodic function sin(x) (right)
spective on how depth and parameterization affect the training of neural networks. One of the most
useful and perplexing properties of deep neural networks has been that, in contrast to other high
capacity function approximators, overparameterizing a neural network does not tend to lead to ex-
cessive overfitting (Savarese et al., 2019). Where does this generalization power come from? Much
recent work (Neyshabur et al., 2018; 2015) has argued that it comes from an implicit regularization
inherent in the optimization algorithm itself (i.e. SGD). In contrast, for the case of shallow and
deep univariate fully connected ReLU nets, we provide causal evidence that it is due to the specific,
very flat CPWL initialization induced by common initialization methods. In order to test this in
both shallow and deep ReLU networks, we compare training with the standard flat initialization to a
‘spiky’ initialization.
For a shallow ReLU network, we can test a ‘spiky’ initialization by exactly solving for network
parameters to generate a given arbitrary CPWL function. This network initialization is then com-
pared against a standard initialization, and trained against a smooth function with a small number of
training data points. Note that in a 1D input space we need a small number of training data points
to create a situation similar to that of the sparsity caused by high dimensional input, and to allow
for testing generalization between data points. We find that both networks fit the training data near
perfectly, reaching a global minima of the training loss, but that the ‘spiky’ initialization has much
worse generalization error (Table 3). Visually, we find that the initial ‘spiky’ features of the starting
point CPWL representation are preserved in the final approximation of the smooth target function
(Figures 4 and 6). For a deep ReLU network, it is more difficult to exactly solve for a ‘spiky’
initialization. Instead, we train a network to approximate an arbitrary CPWL function, and call
those trained network parameters the ‘spiky’ initialization. Once again, the ‘spiky’ initialization has
near identical training performance, hitting all data points, but has noticeably worse generalization
performance.
Figure 4: ’Spiky’ (orange) and standard initialization (blue), compared before (left) and after (right)
training. Note both cases had similar, very low training set error.
It appears that generalization performance it not automatically guaranteed by GD, but instead due to
the flat initializations which are then preserved by GD. ‘Spiky’ initializations also have their (higher)
curvature preserved by GD. This idea makes sense, as generalization depends on our target function
smoothly varying, and a smooth approximation is promoted by a smooth initialization.
9
Under review as a conference paper at ICLR 2020
Smoothness in Data Gaps increases with Hidden Units and Decreases with Initial Weight Vari-
ance. Our last experiment examines how smoothness (roughness) depends on the number of units,
particularly in the case where there are large gaps in the training data. We use a continuous and
discontinuous target function (shown in Figure 8). We trained shallow ReLU networks with varying
width H and initial weight variance σw on these training data until convergence, and measured the
total roughness of resulting CPWL approximation in the data gaps.
Figure 5: Roughness vs. Width (left) and the variance of the initialization (right) for both data
gap cases shown in Figure 8. Each data point is the result of averaging over 4 trials trained to
convergence.
Figure 5 shows that roughness in the data gaps decreases with width and increases with initial weight
variance, confirming our theory. A spiky (and thus rougher) initialization leads to increased rough-
ness at convergence as well, lending support to the idea that roughness in data gaps can be ‘remem-
bered’ from initialization. On the other hand, higher number of pieces spreads out the curvature work
over more units, leading to smaller overall roughness. Taken together, our experiments indicate that
smooth, flat initialization is partly (if not wholly) responsible for the phenomenon of implicit reg-
ularization in univariate fully connected ReLU nets, and that increasing overparameterization leads
to even better generalization.
Conclusions. We show in this paper that examining deep networks through the lens of function
space can enabled new theoretical and practical insights. We have several interesting findings: the
value of depth in deep nets seems to be less about expressivity and more about learnability, enabling
GD to finding better quality solutions. The functional view also highlights the importance initial-
ization: a smooth initial approximation seems to encourage a smoother final solution, improving
generalization. Fortunately, existing initializations used in practice start with smooth initial approx-
imations, with smoothness increasing with depth. Analyzing the loss surface for a ReLU net in
function space gives us a surprisingly simple and transparent view of the phenomenon of overpa-
rameterization: it makes clear that increasing width relative to training data size leads w.h.p. to
lonely partitions of the data which are global minima. Function space shows us that the mysterious
phenomenon of implicit regularization may arise due to a hidden 2nd order differential equation that
underlies the ReLU parameterization. In addition, this functional lens suggests new tools, architec-
tures and algorithms. Can we develop tools to help understand how these CPWL functions change
across layers or during training? Finally, our analysis shows that bad local minima are often due to
breakpoints getting trapped in bad local minima: Can we design new learning algorithms that make
global moves in the BDSO parameterization in order to avoid these local minima?
10
Under review as a conference paper at ICLR 2020
References
Jayadev Acharya, Ilias Diakonikolas, Jerry Li, and Ludwig Schmidt. Fast algorithms for segmented
regression. arXiv preprint arXiv:1607.03990, 2016.
Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017.
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural
networks with rectified linear units. arXiv preprint arXiv:1611.01491, 2016.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019.
Jushan Bai and Pierre Perron. Estimating and testing linear models with multiple structural changes.
Econometrica ,pp.47-78,1998.
Randall Balestriero et al. A spline theory of deep networks. In International Conference on Machine
Learning, pp. 383-392, 2018.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635, 2018.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256.
I.S. Gradshteyn and I.M. Ryzhik. Tables of Integrals, Series, and Products. 8 edition, 2015. ISBN
978-0-12-373637-6.
Boris Hanin and David Rolnick. Deep relu networks have surprisingly few activation patterns. arXiv
preprint arXiv:1906.00904, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165,
2017.
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear
regions of deep neural networks. In Advances in neural information processing systems, pp.
2924-2932, 2014.
Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized opti-
mization in deep neural networks. In Advances in Neural Information Processing Systems, pp.
2422-2430, 2015.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The role
of over-parametrization in generalization of neural networks. 2018.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponen-
tial expressivity in deep neural networks through transient chaos. In Advances in neural informa-
tion processing systems, pp. 3360-3368, 2016.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl Dickstein. On the
expressive power of deep neural networks. In Proceedings of the 34th International Conference
on Machine Learning-Volume 70, pp. 2847-2854. JMLR. org, 2017.
11
Under review as a conference paper at ICLR 2020
Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded
norm networks look in function space? arXiv preprint arXiv:1902.05040, 2019.
Thiago Serra, Christian Tjandraatmadja, and Srikumar Ramalingam. Bounding and counting linear
regions of deep neural networks. arXiv preprint arXiv:1711.02114, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
12
Under review as a conference paper at ICLR 2020
A Experimental Details
Figure 6: ’Spiky’ (orange) and standard initialization (blue), compared before training (left) and
post-training (right) using a deep network
Figure 7: Growth in the (minimum) amount of local minima, as a function of the number of break-
points and data points. Right plot is identical, but with log scaling
A.1 Uniform Initialization
Trained on a shallow, 21 unit FC ReLU network. Trained on function over the interval [-2,2].
Learning rate = 5e-5, trained via GD over 10000 epochs. Compared against pytorch default of He
initialization. Training data sampled uniformly every .01 of the target interval. Each experiment was
run 5 times, with results reported as mean ± standard deviation. Breakpoints y values were taken
from the original standard initialization for the uniform initialization plus a small random noise term
N(0,.01), making initial condition within the target interval nearly identical.
A.2 Roughness by Layer Plots
Trained on a deep, 5 layer network, with 4 hidden layers of width 8. Trained on function over the
interval [-2,2]. Learning rate = 1e-4, trained via GD over 10000 epochs, with roughness measured
every 50 epochs. Roughness per layer was summed over all units within that layer.
A.3 Spiky Initialization Plots
Shallow version trained on a 21 unit FC ReLU Network. Deep version trained on a deep, 5-layer
network with 4 hidden layers of width 8. In both cases, the ’spiky’ initialization was a 20 - breakpoint
CPWL function, with yn 〜Uniform([—2, 2]). Inthe deep case, the spiky model was initialized with
the same weights as the non-spiky model, and then pre-trained for 10,000 epochs to fit the CPWL.
After that, gradient descent training proceeded on both models for 20,000 epochs, with all training
having learning rate 1e-4. Training data was 20 random points in the range [-2,2], while the testing
13
Under review as a conference paper at ICLR 2020
data (used to measure generalization) was spaced uniformly at every ∆x = .01 of the target interval
of the target function.
In the shallow case, there was no pre-training, as the ’spiky’ model was directly set to be equal to
the CPWL. In the shallow model, training occurred for 20,000 epochs. All experiment were run
over 5 trials, and values in table are reported as mean ± standard deviation. Base shallow learning
rate was 1e-4 using gradient descent method, with learning rate divided by 5 for the spiky case due
to the initialization method generating larger weights. Despite differing learning rates, both models
had similar training loss curves and similar final training loss values, e.g. for sine, final training
loss was .94 for spiky and 1.02 for standard. Functions used were sin(x), arctan(x), a sawtooth
function from [-2,2] with minimum value of -1 at the endpoints, and 4 peaks of maximum value 1,
cubic χ43 + χ22 - 2, quadratic X2, and exp(.5x) Note GD was chosen due to the strong theoretical
focus of this paper - similar results were obtained using ADAM optimizer, in which case no differing
learning rates were necessary.
A.4 Breakpoints Induced by Deep Networks
We used networks with a total ofH = 40 hidden units, spread over L ∈ {1, 2, 3, 4, 5} hidden layers.
Training data consiste of uniform samples of function over the interval x ∈ [-3, 3]. Learning rate
=5 ∙ 10-5, trained via GD over 25,000 epochs. The target functions tested were sin(πx), a 5-piece
polynomial with maximum value of2 in the domain [-3, 3], a sawtooth with period 3 and amplitude
1, arctan(x), exp(x), and 1 x2. Each value in the table was the average of 5 trials.
A.5 Breakpoint Mobility in Deep Networks
We use a deep, 6-layer network, with 5 hidden layers of width 8. Training data consists of the
‘smooth’ and ‘sharp’ functions over the interval x ∈ [-3, 3]. Learning rate = 5e-5, trained via GD
until convergence, where convergence was defined as when the loss between two epochs changed
Figure 8: Training data sampled from two ground truth functions, one smoothly (left) and the other
sharply (right) discontinuous, each with a data gap at [-0.5, 0.5].
A.6 Breakpoint Distributions
Various function classes were trained until convergence on a depth 1 or 4 ReLU network, with 500
total units distributed evenly across layers. Initial and final breakpoint distributions were measured
using a kernel density estimate, and compared with the underlying curvature (absolute value of 2nd
derivative) of the ground truth function. The cubic spline was a cubic spline fit to a small number of
arbitrary data points.
Table 4 shows that the breakpoint densities moved over training to become more correlated with
the underlying curvature of the ground truth function. This effect was more pronounced with depth.
In certain very simple functions (e.g. x2 or exp(x), not shown), a failure case emerged where
there was no real change in correlation over training. Diagnostics appeared to show this was due to
the function being so simple as to train almost instantaneously in our overparameterized network,
meaning breakpoints had no time to move. Figure 9 shows what happens to the breakpoint densities
14
Under review as a conference paper at ICLR 2020
Depth/Type	Sine	3 X3	Sin( x2)	Cubic Spline
1 - Initial	-0.0770	-0.904	-0.754	-0.164
1 - Final	0.324	-0.891	-0.592	0.289
4 - Initial	-0.171	-0.900	-0.752	-0.133
4 - Final	0.494	0.688	-0.212	0.798
1-∆	0.401	0.0130	0.162	0.452
4-∆	0.665	1.59	0.540	0.931
Table 4: Top: Correlation of the BP distribution before and after training for depth 1 and 4 networks
across function classes. Bottom : Change in correlation over training
over training - in the shallow case, they are more constrained by the initial condition, and continue
to have a higher density near the origin even when not necessary or appropriate.
B More details on Breakpoints for Deep ReLU Nets
Each neuron of the second hidden layer receives as input the result of a CPWL function zi(2) (x) as
defined above. The output of this function is then fed through a ReLU, which has two implications:
first, every zero crossing of zi(2) is a breakpoint of xi(2); second, any breakpoints βj(1) of zi(2) such
that zi(2)(βj(1)) < 0 will not be breakpoints of xi(2). Importantly, the number of breakpoints in gθ(x)
is now a function of the parameters θ, rather than equal to fixed H as in the L = 1 case; in other
words, breakpoints can be dynamically created and annihilated throughout training. This fact will
have dramatic implications when we explore how gradient descent optimizes breakpoints in order to
model curvature in the training data (see Section 3). But first, due to complexities of depth, we must
carefully formalize the notion of a breakpoint for a deep network.
Definition 1. β(') is a breakpoint induced by neuron i in layer ' if z(') (β(')) = 0. Since IhefUnction
z(')(∙) is nonlinear, neuron i may induce multiple breakpoints, which we denote β(?. A breakpoint
βik is active ifthere exists some path π through neuron i such that for all other neurons j = i ∈ π,
zj(`(j)) > 0, i.e. abj (x) = 1. If two neurons i andj in layers ` and `0 induce the same breakpoint(s),
β,k = βj'k0, then both are referred to as degenerate breakpoints.
Let b∏ (x) = Qi∈∏ bi. Then, β(') is active iff there exists some path π such that b∏ is discontinuous
at X = βi'). Thus, gθ(x) is non-differentiable at X if X = β(') for some (', i). If no degenerate
breakpoints exist, then the converse also holds. (If there do exist degenerate breakpoints β(') and
(`0 )	(`)	(`0 )
βj，, then it is possible that μi，= -μj，, i.e. the changes in slope cancel out and gθ(x) remains
linear and differentiable.)
C Proofs of Theoretical Results
C.1 Reparametrization from ReLu Network to Piecewise Linear Function
Proof of Equations (2) and (3):
H
fθ,H (x) = fviφ(wix + bi)
i=1
H
=	vi(wiX + bi)JwiX + bi > 0K
JX>βiK wi< 0 where 民 Vi
i=1
H
viwi(X - βi)
i=1
H
£ μi(χ - βi) 彳
i=1
JX > βiK wi > 0
Jx<βiK Wi< 0	where μi , Viwi
15
Under review as a conference paper at ICLR 2020
Figure 9: Shallow (left) and deep (right) plots of initial and final breakpoint distributions, along
with the underlying true curvature of the functions (top to bottom) sin(x), x3, sin(X), and a cubic
spline of a few arbitrary data points
This gives us Equation (2), as desired. Let the subscripts p, q denote the parameters sorted by βp
value. In this setting, let β0 , -∞, and βH+1 , ∞. Then,
16
Under review as a conference paper at ICLR 2020
/
H
X
p=0
H
X
p=0
H
X
p=0
H
X
p=0
H
X
p=0
∖
pH
E μq (X — β ) + E μq (X — β )
Jβp ≤ X < βpK
q=p+1
wq <0
pH
E (μqX — μqβ ) + E (μqX
q=1	q=p+1
wq >0	wq <0
(
pp
\
-μqBq )
/
HH
Jβp ≤ X < βp+1K
∑ μq X — Σ μq Bq + Σ μq X —Σ>q βql Je
q=1
wq>0
心μq
wq>0
q=1
wq >0
、
q=p+1
wq <0
p
X - £〃qβ +
q=1
wq >0
\
H
q=p+1
wq <0
/	\
H
∑μq
q=p+1
wq<0
p
p ≤ X < βp+1K
\
H
X — E μqβ	Je
q=p+1
wq<0
W
H
p ≤ X < βp+1K
I ∑μq + ∑ μq
q=1	q=p+1
wq>0	wq<0
1--------{---------
∑μq βq +Σ μq βq
q=1	q=p+1
wq>0	wq<0
---------{--------}
Jβp ≤ X < βp+1K
mp
,γp

H
p


X
} I
(mpX — γp) Jep ≤ X < ep+1K
p=0
This gives us Equation (3), as desired.
□
C.2 Random Initialization in Function Space
Lemma 1. Suppose (bi, wi, vi) are initialized independently with densities fB (bi), fW (wi), and
fv(Vi). Then, the density of (βi,μi) isgiVenby
Z∞
∞
/b (βiu)fw (u)fv (μ) du .
Proof. Suppose (bi, wi, vi) are initialized i.i.d. from a distribution with density fB,W,V (bi, wi, vi).
Then, We can derive the density of (βi, μi) by considering the invertable continuous transformation
given by (βi,μi,u) = g(bi,Wi,Vi) = (bi∕wi,Vi∣Wi∣,Wi), where g-1(βi,μi,u) = (βiu,u,μi∕∣u∣).
The density of (βi, μi, u) is given by ∕b,m,v (βiu, u, μi∕∖u∖)∖J |, where J is the Jacobian determi-
nant of g-1. Then, we have J = — Sgn Wi and ∖ J∖ = 1. The density of (βi, μi) is then derived by
integrating out the dummy variable u: fβ,μ(βi, μi) = R-∞∞ ∕b,w,v(βiu, u,野) du. If (bi, Wi, Vi)
are independent, this expands to f∞∞ /b (βiu)fw (U) fv (μ) du.	□
C.3 Gaussian Initialization in Function
Theorem 1(a). Consider a fully connected ReLU neural net with scalar input and output, and a
single hidden layer of width H. Let the weights and biases be initialized randomly according to a
zero-mean Gaussian or Uniform distribution. Then, under an independent Gaussian initialization,
pβ,μ(βi, μi)
2πσv √σ2 + σW βi2	P
∖μi∖ ,σ2 + σW β
σbσv σw
1
—
Proof. Starting with Lemma 1,
fβ,μ(β,μ)
Z∞
∞
fB (βiu)fw (u)fv ( μ ) du
17
Under review as a conference paper at ICLR 2020
(Sympy)
Z∞
∞
1	- Ieu))	1	- 上 1	- (μ∕u)2
, Ce 2σ2	, e 2σW	/ e	2σV du
√2πσ2	√2πσW	√2πσV
exp -
μ√σ2+σW(β)2
σb σv σw
2πσv Λ∕σ2+σW ⑶2
unknown
μ > 0
otherwise
but the integrand is even in μ, giving
-- √σ22+σW (β)
σbσv σ w
2πσv Pσ2 + σW W
□
Corollary 1(a). Consider the same setting as Theorem 1. In the case of an independent Gaussian
initialization,
Pe⑹=Cauchy C0, σθ = "bσ⅛
1	g2,0 (忌
2πσvσw 0,2 14σ.σw 0,0
Pμ∣β (μi∖βi) = Laplace μi,0
σbσv σw
√σ2 + σW β2
-ɪ Ko ( -⅛"j
πσvσw	σvσw
=√σ2 + σWβ2 exp
2σbσvσw
皿 v7σ2 + σW β2
σbσvσw
where Gnm(∙∣∙) is the Meijer G-function and KV (∙) is the modified Bessel function of the second
kind.
Proof. Marginalizing out μ from the joint density in SymPy returns the desired fβ (β) from above.
Sympy cannot compute the other marginal, so we verify it by hand:
|*| √σJ+σW β
fμ(μ)
Z∞
∞
Xn	|*| √σ2+σW β2
Xp--------------
σbσvσw
2∏σv √σ2 + σW β2
1
2πσv
Z∞
∞
σbσvσw
√σ2 + σW β2
dx
φ(β=σw)
1
2πσv σW
|{z}
φ0(β)
Z∞
∞
T二| ∙√σ2+β2
σbσvσw
√ σ+β2
dx
from Gradshteyn & Ryzhik (2015), Eq. 3.462.20, we have
∞
K0 (ab) =
0
exp (—a √β2 + b2
√β2+^2
dβ
1 ∞ exp(—a,β2 + b2)
(integrand is even in β) = 2 J	——'√/2 + 炉^^L dβ
[Re a > 0, Re b > 0]
[Rea > 0,Reb > 0]
applying this with a = zτ % and b = σb,
σbσv σw
1
2πσv σW
Z∞
∞
T*| √σ2 + β2
σbσv σw
PrWr-
dβ
-ɪ Ko
πσvσW
as desired. We can then use these densities to derive the conditional:
√σ2+ σW (β )2 eχp
fμ(μ∣β)
, Λ∕σ2+σW(e)
σbσvσw
2σbσvσW
18
Under review as a conference paper at ICLR 2020
Laplace μ; 0.
σbσvσw
，√σ22 + σW(β)2
□
C.4 Uniform Initialization in Function Space
Theorem 1(b). Consider a fully connected ReLU neural net with scalar input and output, and a
single hidden layer of width H. Let the weights and biases be initialized randomly according to a
zero-mean Gaussian or Uniform distribution. Then, under an independent Uniform initialization,
pβ,μ(βi, μi)
J∣μiI ≤ min{餐
4abaw av
,aw , av }K ( a a ab 】IμiI
--------minV∏ΓT ,aw }	
)-------∖	lβ"	av
Proof. Starting with Lemma 1,
fβ,μ(β,μ)
f	fB (βu)fw (u)fv (μ∕u) du
-aw
J —ab ≤ βu ≤ abK 2a J —aw ≤ U ≤ aw K 2a- J —av ≤ μ/u ≤ av K du
J—ab/|e| ≤ U ≤ ab/|e |K -- J—aw ≤ U ≤ aw K -— Ju ≤ TμDav ∨ U ≥ ∣μ^av Kdu
2aw	2av
Zaw 1
--J— min{ab∕∣β∣,aw} ≤ u ≤ Tμ∣∕av ∨ ∣μ∣∕av ≤ U ≤ min{ab∕∣β∣,aw}K
aw 8abawav
×J∣μ∣ ≤ abav∕∣βIK du
”μ∣ ≤ abav八βlK
8ab aw av
J|〃| ≤ abav^βlK
4ab aw av
J|〃| ≤ abav^βlK
4ab aw av
I	J- min{ab∕∣β∣,aw} ≤ U ≤ Tμ∣∕av ∨ ∣μ∣∕av ≤ U ≤ min{ab∕∣βI, aw}K du
-aw
/ J∣μ∣∕av ≤ u ≤ min{ab∕∣β∣,aw}K du
0
(min{ab∕lβ |, aw} — lμl∕av ) J— aw av ≤ μ ≤ aw av K
as desired.
□
Corollary 1(b). Consider the same setting as Theorem 1. In the case of an independent Uniform
initialization,
pβ(民) =*(min {奇,aw})
Pμ(μi)
J-awav ≤ μi ≤ awav K
2aw av
log
aw av
Pμ∣β(μi∣βi) = Tri(μi; a，v min{ab∕∣βi∣,aw})
%|
=J∣μi∣ ≤ av min{ab∕∣βi∣,aw}K <1 —
av min{abAβi1, aw}	∖
M|
av min{ab/ |ei |, aw } J
where Tri(∙; a) is the Symmetric triangular distribution with base [—a, a] and mode 0.
Proof. Beginning with the marginal of βi ,
fβ (β) = / fβ,μ(β,μ) dμ
Z∞
∞
J|M| ≤ abav/|e|K
4ab aw av
(min{ab∕lβl, aw } — lμ[∕av ) J— aw av ≤ μ ≤ aw av K dμ
awav
-awa
1
v
J∣μ∣ ≤ abav/|e|K
2ab aw av
f
0
4ab aw av
awav
(min{ab∕lβ|, aw} — lμ[∕av) dμ
Jμ ≤ abav∕∣β∣K (min{ab∕∣βl,aw} — μ∕av)dμ
19
Under review as a conference paper at ICLR 2020
1
2ab@w av
1
2ab@w av
1
2ab aw av
1
2ab aw av
1
2ab aw av
1
2ab aw av
(/	Jμ ≤ ab&v∕∣βIK min{ab∕∣β∣,aw} dμ - /
Jμ ≤ abav∕∣β∣Kμ∕av
min{ab∕∣β∣, aw} [	Jμ ≤ abav∕∣β∣K dμ - Lj	Jμ ≤ abav∕∣β∣Kμdμ
0	av 0
mi∩{ab/∣βI, aw} min{awav, abav∕∣β∣} -	Z
av √0
min{αwαv ,a^av/∖β∖}
12
min{ab∕∣β∣, aw} min{awav, abav/∣β∣} - 7λ	(min{awav, abav/∣β∣})
2av
av (min{ab∕∣β∣, aw})2 - ɪ S min{aw, ab∕∣β∣})2 )
2av
av (min{ab∕∣β∣, aw})2 -?(min{aw, ab∕∣β∣})2)
— (min{a∕β∣, aw})2
4abaw
as desired. Then,
fμ(μ)
/ fβ,μ(β,μ)dβ
[ J∣μ 产 abav"β∣K (min{ab ∕∣β∣,aw } - ∣μ∣∕av) J-aw a ≤ μ ≤ aw a K dβ
J-∞	4abaw av
J-aw av ≤ μ ≤ aw aK
4ab aw av
J-aw av ≤ μ ≤ aw aK
/ J∣μ∣ ≤ abav∕∣β∣K (min{ab∕∣β∣, aw}-∣μ∣∕av)dβ
J -∞
2
4ab aw av
J-awav ≤ μ ≤ awavK )
4ab aw av
J - aw av ≤ μ ≤ aw av K 2
4ab aw av
J - aw av ≤ μ ≤ aw av K 2
4ab@w av
/ J∣μ∣ ≤ abav∕βK (min{ab∕β, aw} -∣μ∣∕av)dβ
0
I Jβ ≤ abav∕∣μ∣K (min{ab∕β, aw} -∣μ∣∕av)dβ
0
(abav ∕∖μ∖
/	min{ab∕β, aw} - ∣μ∣∕av dβ
0
/ ∕min{abav∕∖μ∖,ab∕aw}
a	aw -∣μ∣∕av dβ
0
Lbav ∕∖μ∖
+ /	ab/β - ∣μ^avdβ
√ min{abav ∕∖μ∖,ab∕aw }
J - aw av ≤ μ ≤ aw av K 2
,ab∕aw
4ab@w av
/	aw - ∣μ^avdβ +
gav∕∖μ∖
ab∕β -∣μ∣∕av dβ
f
aa∕∕∙
aw
J - aw av ≤ μ ≤ aw av K
4ab aw av
2 ( ( ab/aw )( aw - ∣μ ∣ /av ) +
'ɑbɑv ∕∖μ∖
ab∕β dβ -
f
Jab ∕ `
aw
(abav∕∖μ∖
∣	∣μ∣∕av dβ
ab∕
aw
J-aw av ≤ μ ≤ aw aK
4ab aw av
2 ( ab/aw )(aw - ∣μ ∣ /av ) +
”bav ∕∖μ∖
ab∕β dβ
Jab∕aw
-(∣μDav)(abav^μ∣ - ab/aw)]
J-awav ≤ μ ≤ awavK n ∕/	/	7	I I / X1 1 _ abav Aμ∣
^j	2 I (ab/aw)(aw - ∣μDav) + ab log ~ι
4ab aw av	ab∕aw
-(∣μDav)(abav^μ∣ - ab/aw)
J-awav ≤ μ ≤ awavK
4ab aw av
2ab log
abav∕∣μ∣
ab ∕aw
J - aw av ≤ μ ≤ aw av K
2awav
log
abav∕∣μ∣
ab ∕aw
J-awav ≤ μ ≤ awavK
2awav
log
aw av
∣μ∣
20
Under review as a conference paper at ICLR 2020
as desired. We can then use these densities to derive the conditional:
“"i4αaaWv!βilK (min{ab∕∣βi∣,aw }-∣μi∣∕°v) J-°w a” ≤ μi ≤ aw a” K
μ(μil i)=	H	ɪ (min{ab∕∣βi∣,αw})2
MiKabav∕lβiικa-awav≤μi≤awavK (min{ab∕∣βi * |, 0w} - 〃|/&”)
(min{ab∕∣βi∣,aw })2
J∣μi∣ ≤ a” min{ab∕∣βi∣,aw}K 八___________∣μi∣______)
av min{ab/ ∣βi ∣, aw }	∖ av min{ab∕ ∣βi ∣, aw } J
□
Remarks. Note that the marginal distribution on μ% is the distribution of a product of two inde-
pendent random variables, and the marginal distribution on βi is the distribution of the ratio of two
random variables. For the Gaussian case, the marginal distribution on μi is a symmetric distribution
with variance σ22σW and excess Kurtosis of 6. For the Uniform case, the marginal distribution of βi
is a symmetric distribution with no finite higher moments. The marginal distribution of μ% is a sym-
metric distribution with bounded support and variance 2awav and excess Kurtosis of 50*a 3.
The conditional distribution of μi given βi is a symmetric distribution with bounded support and
variance (av mm{ab'βil,aw}产 and excess Kurtosis of -3.
C.5 Roughness of Random Initialization
Theorem 2. Consider the initial roughness ρ0 under a Gaussian initialization. In the He initializa-
tion, we have that the tail probability is given by
F[ρoE[ρo] ≥ λ] ≤ ] 'H ,
1 + 128
where E[ρ0] = 2. In the Glorot initialization, we have that the tail probability is given by
P[ρoE[ρo] ≥ λ] ≤ ] + 入2：H+1)4 ,
Where 5E[ρo] = (HHL)2 = O (H).
Proof. Using the moments of the delta-slope distribution computed in Theorem 1, Corollary 1, and
above, we can compute:
HH
叫 ρo] = X 叫 μ2] = X Var[μi] +E[μi]2 = HσVσW
i=1	i=1
H
Var[ρ0] = XVar[μ2] = HVar[μ2] = H(叫μ4] -叫〃2]2)
i=1
=H(9(σvσw )4 - σ4σW) = 8Hσ4σW
Applying the two initializations, we have
E[ρHe] = H 12=4
H1
Var[ρHe] = 8HΗ2 4 = 1H8
叫 pF = HΗ⅜ii⅛ = (H‰ = O(H)
Var谭。rot] = 8H (H⅛ WHF = (H+⅜ = O (H)
By applying Cantelli,s theorem, we get the stated tail probabilities.	□
21
Under review as a conference paper at ICLR 2020
C.6 Dynamics in Function Space (Breakpoints and Delta-Slopes)
Theorem 5. For a one hidden layer univariate ReLU network trained with gradient descent with
respect to the neural network parameters θNN = {(wi, bi, vi)}iH=1, the gradient flow dynamics of
thefunCtion space parameters Θb DSO = {(βi, μi)}H=I are governed by thefollowing laws:
dβi _	∂'(Θnn)
-：—一------二~~：-
dt	∂βi
dμi(t) _	∂'(Θnn)
--:---------二-----
dt	∂μi
v(t) [(e(t) Θ ai(t), 1)+βi(t)〈e(t) Cl ai(t), xi]	(4)
Wi ⑻ ×------------------}	×-------{z-------}
net relevant residual	correlation
一(v2(t) + w2(t)) he(t) Θ ai(t), x) - Wi(t)bi(t) he(t) Θ ai(t), 1) (5)
Proof. Computing the time derivatives of the BDSO parameters and using the loss gradients of the
loss with respect to the NN parameters gives us:
∂'(Θnn )
∂Wi
d'(θNN )
∂Vi
∂'(Θnn )
∂bi
dβi(t)
dt
Vi h^ Θ ai, x)
h^,σ(wix + bi1)) 一 h^ Θ ai,WiX + bi1) 一 Wih^ Θ ai, x) + bh^ Θ ai, 1)
Vi h^ Θ ai, 1)
d 一_ bi(t))
dt V Wi(t) J
Wi(t)吟 一 bi(t)*
-------------------------
Wi(t)2
Wi(t)(-d∂⅛F) - bi(t)(-∂WNN)
------------------------------------
Wi(t)2
W (t) d'(θNN ) _ b (t) d'(θNN )
wi(t) ∂bi(t)	bi(t) ∂Wi(t)
Wi(t)2
Wi(t)vi(t)h^(t) Θ ai(t), 1) — bi(t)vi(t)h^(t) Θ ai(t), x)
Wi(t)2
Vi(t) (2(t) Θ ai(t),Wi(t)1 - bi(t)x)
Wi(t)2
Vi(t)	bi(t)
-77? ( e(t) θ ai(t),1------/ )
wi(t)	wi(t)
v(t) /e(t) Θ ai(t), 1 + βi(t)x∖
Wi ⑻ \-------{-----}
relevant residuals
dμi(t)
dt
v(t) [(e(t) Θ ai(t), 1) +βi(t) (e(t) Θ ai(t), x)]
wi(t) |----------{---------}	|--------{---------}
d	net relevant residual	correlation
dtwivi
dWi	dVi
—■—Vi + Wi--
dt i i dt
—
^NN Vi
∂Wi
-Wi d'fNN)
∂Vi
-V2 h^ Θ ai, x) - W2 h^ Θ ai, x) - Wibih^ Θ ai, 1)
-(v2 + W2)(e Θ ai, x) — Wibih卷 Θ ai, 1)
This completes the proof.
□
22
Under review as a conference paper at ICLR 2020
C.7 Loss Surface in Function Space
EI_ . _	_ _ ∙~t C	.ι . r U - Γ 7-»1 P/ r∖	∖ I -	z ∖ ι ■	τ ,c	t`. r .ι
Theorem 3. Suppose that for all P ∈ [P ], f (∙; Θbdso )∣∏p is an Ordinary Least Squares fit of the
data in piece P. Then, Θb DSO is a critical point of '(Θb DSO).
Proof. If, for all p, f (∙; ΘbDSO)∣∏p is an OLS fit of the data ∏p, then We must have h^∏p, ∏P = 0,
where ^∏p is the residual for ∏p. Similarly, we must have that the net residual h^∏p, 1)= 0.
Next, consider, for any neuron j, the vector aj. If j is right-facing, aj = (0,..., 0,1,..., 1), where
the transition from 0s to 1s corresponds to the data index n where xn > βj ; if j is left-facing, a
1-to-0 transition occurs at n. Thus, aj∙ is constant for n ∈ ∏p, as the boundaries of ∏p correspond to
breakpoints βi and βi+1 . Noting that these inner products are just sums of products, we have that,
for any neuron j, h^ Θ aj, Xi can be decomposed into a sum PnP h^∏p, ∏p) = 0, where the sum is
over the pieces on the active side of j. Similarly, h^ Θ aj, 1) = PnP h^∏p, 1) = 0.
Applying Theorem 5, we see that dj = dj = 0 for all j, and so Θbdso is a critical point of
~, _ 、 ________________________________________________________________________________________
'(Θbdso).	□
23