Under review as a conference paper at ICLR 2020
Dimensional Reweighting Graph Convolu-
tional Networks
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose a method named Dimensional reweighting Graph
Convolutional Networks (DrGCNs), to tackle the problem of variance between
dimensional information in the node representations of GCNs. We prove that
DrGCNs have the effect of stabilizing the training process by connecting our
problem to the theory of the mean field. However, practically, we find that the
degrees DrGCNs help vary severely on different datasets. We revisit the prob-
lem and develop a new measure K to quantify the effect. This measure guides
when we should use dimensional reweighting in GCNs and how much it can help.
Moreover, it offers insights to explain the improvement obtained by the proposed
DrGCNs. The dimensional reweighting block is light-weighted and highly flex-
ible to be built on most of the GCN variants. Carefully designed experiments,
including several fixes on duplicates, information leaks, and wrong labels of the
well-known node classification benchmark datasets, demonstrate the superior per-
formances of DrGCNs over the existing state-of-the-art approaches. Significant
improvements can also be observed on a large scale industrial dataset.
1	Introduction
Deep neural networks (DNNs) have been widely applied in various fields, including computer vi-
sion (He et al., 2016; Hu et al., 2018), natural language processing (Devlin et al., 2019), and speech
recognition (Abdel-Hamid et al., 2014), among many others. Graph neural networks (GNNs) is
proposed for learning node presentations of networked data (Scarselli et al., 2009), and later be ex-
tended to graph convolutional network (GCN) that achieves better performance by capturing topo-
logical information of linked graphs (Kipf & Welling, 2017). Since then, GCNs begin to attract
board interests. Starting from GraphSAGE (Hamilton et al., 2017) defining the convolutional neu-
ral network based graph learning framework as sampling and aggregation, many follow-up efforts
attempt to enhance the sampling or aggregation process via various techniques, such as attention
mechanism (Velickovic et al., 2018), mix-hop connection (AbU-El-Haija et al., 2019) and adaptive
sampling (Huang et al., 2018).
In this paper, we stUdy the node representations in GCNs from the perspective of covariance between
dimensions. SUprisingly, applying a dimensional reweighting process to the node representations
may be very UsefUl for the improvement of GCNs. As an instance, Under oUr proposed reweighting
scheme, the inpUt covariance between dimensions can be redUced by 68% on the Reddit dataset,
which is extremely UsefUl since we also find that the nUmber of misclassified cases redUced by 40%,
compared with the previoUs SOTA method.
We propose Dimensional reweighting Graph ConvolUtional Networks (DrGCNs), in which the inpUt
of each layer of the GCN is reweighted by global node representation information. OUr discovery is
that the experimental performance of GCNs can be greatly improved Under this simple reweighting
scheme. On the other hand, with the help of mean field theory (Kadanoff, 2009; Yang et al., 2019),
this reweighting scheme is also proved to improve the stability of fUlly connected networks, provding
insight to GCNs. To deepen the Understanding to which extent the proposed reweighting scheme can
help GCNs, we develop a new measUre to qUantify its effectiveness Under different contexts (GCN
variants and datasets).
Experimental resUlts verify oUr theoretical findings ideally that we can achieve predictable improve-
ments on pUblic datasets adopted in the literatUre over the state-of-the-art GCNs. While stUdying on
1
Under review as a conference paper at ICLR 2020
these well-known benchmarks, we notice that two of them (Cora, Citeseer) suffer from duplicates
and feature-label information leaks. We fix these problems and offer refined datasets for fair com-
parisons. To further validate the effectiveness, we deploy the proposed DrGCNs on A* 1 company’s
recommendation system and clearly demonstrate performance improvements via offline evaluations.
2	DrGCNs: Dimensional reweighting Graph Convolutional
Networks
2.1	Preliminaries
Notations. We focus on undirected graphs G = (V, E, X), where V = {vi} represents the node set,
E = {(vi, vj)} indicates the edge set, and X stands for the node features. For a specific GCN layer,
we use Rin = (ri1n, ..., rinn) to denote the input node representations and Rout = (ro1ut, ..., ronut)
to symbolize the output representations.2 For the whole layer-stacked GCN structure, we use H0
to denote the input node representation of the first layer, and Hl (l > 0) to signify the output node
representation of the lth layer, which is also the output representation of the (l - 1)th layer. Let A
be the adjacency matrix with aij = 1 when (vi , vj ) ∈ E and aij = 0 otherwise.
Graph Convolutional Networks (GCNs). Given the input node set V, the adjacency matrix A, and
the input representations Rin, a GCN layer uses such information to generate output representations
Rout:
Rout = σ(aggregator(Rin, A)),	(1)
where σ is the activation function. Although there exist non-linear aggregators like the LSTM
aggregator (Hamilton et al., 2017), in most GCN variants the aggregator is a linear function which
can be viewed as a weighted sum of node representations among the neighborhood (Kipf & Welling,
2017; Huang et al., 2018), followed by a matrix multiplication on a refined adjacency matrix A, with
a bias added. The procedure can be formulated as follows:
Rout = σ (WRinA + b),	(2)
where W is the projection matrix and b denotes the bias vector. Development on GCNs mainly lies
in different ways to generate A. GCN proposed some variants including simply taking A = AD-1,
which is uniform average among neighbors with D being the diagonal matrix of the degrees, or
C_1	一1	1	1	1	1
weighted by degree of each node A = D-2 AD-2, or including self-loops A = I + D-2 AD-2.
Other methods include attention (Velickovic et al., 2018), or gated attention (Zhang et al., 2018),
or even neural architecture search methods (Gao et al., 2019) to generate A. To improve scalability,
some GCN variants contain a sampling procedure, which samples a subset of the neighborhood for
aggregation (Chen et al., 2018; Huang et al., 2018). We can set all unsampled edges to 0 in A in
sampling-based GCNs, in this case A even has some randomness.
2.2	Model Formulation for DrGCNs
Given input node representations of a GCN layer Rin , the proposed DrGCN tries to learn a dimen-
sional reweighting vector s = (s1, ..., sd), where si is an adaptive scalar for each dimension i. This
reweighting vector s then helps reweighting each dimension of the node representation rivn to rrve,
v ∈ V, where rrve = rivn ◦ s. Here we use ◦ to denote component-wise multiplication, i.e.,
rvr,ej =sjrvin,j,∀1 ≤j ≤d,∀v∈V.	(3)
We define S as the diagonal matrix with diagonal entries corresponding to the components of s.
Then a DrGCN layer can be formulated as:
Rout = σ(WSRinA + b).	(4)
Inspired by SENet (Hu et al., 2018), we formulate the learning of the shared dimensional reweighting
vector s in two stages. First we generate a global representation rin , whose value is the expectation
of rivn on the whole graph. Then we feed rin into a two-layer neural network structure to generate
1To preserve anonymity we use A* for the company and dataset name.
2For the convenience of our analysis, we use columns of R, H, X instead of rows to represent node repre-
sentations.
2
Under review as a conference paper at ICLR 2020
Figure 1: Our proposed dimensional reweighting block (Dr Block) in DrGCNs.
s of the same dimension size. Equation (5) denotes the procedure to generate s given node weight
{wv|v ∈ V, Pv∈V wv = 1} and node representations {rivn|v ∈ V}:
rin =E[rivn|v ∈ V] =	wvrivn
v∈V
g=σg(Wgrin+bg),
(5)
s = σs(Wsg + bs),
where g is output of the first layer; Wg , bg , Ws , and bs are parameters to be learnt. Figure 1
summarizes the dimensional reweighting block (Dr Block) in DrGCNs.
Combining With Existing GCN variants. The proposed Dr Block can be implemented as an
independent functional process and easily combined with GCNs. As shown in equation (4), Dr
Block only applies on Rin and does not involve in the calculation of A, W and b. Hence, the
proposed Dr Block can easily be combined with existing sampling or aggregation methods without
causing any contradictions. In § 4, we will experimentally test the combination of our Dr Block
with different types of sample-and-aggregation GCN methods. Suppose that the input features are
X, DrGCNs can be viewed as follows:
Hl = σι(WlSlHlTAl + bl),∀ι ≤ ι ≤ k,	(6)
where H0 = X and Hk being the output representation for a k-layer DrGCN:
Complexity of Dr Block. Consider a GCN layer with a input and b output channels, n nodes and e
edges in a sampled batch, the complexity ofa GCN layer is O(abn + be). The proposed Dr block has
a complexity of O(ag), where g is the dimension of g. In most cases, we have g < b and n >> 1,
so we could have O(ag) = o(abn + be), which indicates that Dr block introduces negligible extra
computational cost.
3	Theoretical Analyses
In this section, we connect our study to mean field theory (Yang et al., 2019). We theoretically prove
that the proposed Dr Block is capable of reducing the learning variance brought by perturbations on
the input, making the update more stable in the long run. To deepen the understanding, we further
develop a measure to quantify the stability gained by Dr block.
3.1	Mean Field Approximation
(Lee et al., 2018; Yang et al., 2019) employ mean field approximation to analyze fully-connected
networks. Following their ideas, we provide theoretical analyses for Dr Blocks on fully-connected
networks. GCNs are different from fully-connected networks only in A, and degrade to fully-
connected networks when A = I, our idea is to provide insight to GCNs from the analysis of
fully-connected networks. We assume the average of the data is 0 in the following discussions as
transformation does not affect the covariance structure. For simplicity, we only consider neural
networks with constant width and assume all layers use the same activation function φ. We follow
the pre-activation recurrence relation hli = Wlσl (Sl ◦ hli-1) + bl in (Yang et al., 2019) to facilitate
the problem. When S being a diagonal matrix with positive entries, and φ is ReLU activation,
φ(SH) = Sφ(H) holds for all H. We can take the pre-activation step as the post-activation step of
the previous layer and generalize our analysis to post-activation. So the recursive relation is:
Hl = Wlφ(SlHl-1) + bl.	(7)
3
Under review as a conference paper at ICLR 2020
We apply the mean field approximation to replace the input data by a normal random variable with
the same mean and variance and define an estimator Vφ to characterize the fluctuation of the random
variable φ(SlH).
Define:	Vφ(Sl, Cl-1), E[φ(SlH)φ(SlH)T].	(8)
In this equation H 〜 N(0, Cl-1), φ is the ReLU activation, and Cl represents the covariance
matrix of Hl, i.e. Cl = Cov(Hl). Note that Vφ does not completely coincide with variance, but
can reflect the covariance structure. We call it covariance matrix for convenience. With mean field
approximation, the covariance matrix can be updated by (note that bl is independent with H):
Cl = WlVφ(Sl, Cl-1)(Wl)T + Cov(bl).	(9)
We assume this dynamical system has a BSB1(Block Symmetry Breaking 1 (Yang et al., 2019))
fixed point (where Cl = Cl-1), i.e. a solution having the form C* = q*((1 - c*)I + c* ~~T) of the
following equation with respect to C:
C = WlVφ(Sl, C)(Wl)T + Cov(bl).	(10)
Next we make a reduction ofVφ so that only the second slot matters. Take Slh as a whole, it follows
N(0, SlCl-1Sl) distribution. Note that S is diagonal so S = ST. Thus equation (9) can be written
as:
Cl = Vφ(I,SlCl-1Sl) + Cov(bl).	(11)
The derivative of Cl measures the fluctuation of our updating algorithm when input perturbations
exist, hence it characterizes the sensitivity of the algorithm to the data structures and its robustness.
We will turn to show that Dr can relieve this sensitivity. We fix the point Cl where we are taking
derivative at. For most common types of activation functions, this recursive map has a fixed point,
at which this linearization is most useful. Recall that such a derivative will be a linear map from
symmetric matrices to symmetric matrices.
Define:	Jφ(Cι)，dVφ(1, C) ∣ci (Ci).	(12)
dC
Here C1 could be intuitively understood as the increment near Cl . We denote by Hd the space of
symmetric matrices of size d × d. Using these notations, we prove that:
Theorem 1. There exist diagonal matrices Sl ,constants 0 < γl < 1 such that, ∣∣Jφ(SlCSl )||f ≤
γlI∣Jφ(c)IIf u⅛k for any fixed general C. By general, we mean there exists a Haar measure on
the collection of symmetric matrices HB with respect to which the statement fails has measure zero
Detailed proofs and explanations are included in the Appendix G,H. For symmetric matrices, with
λi denoting eigenvalues of A, we have:	d
l∣A∣∣F = X λ2.
(13)
i=1
This norm measures the overall magnitude of eigenvalues of the operator. This result demonstrates
that our method brings variance reduction and improves the stability of the updating algorithms. To
summarize, for any input data, there exists a vector s that improves the stability of the updating
algorithms.
3.2	Stability Measure for DrGCNs
In this section we turn to define a quantified measurement of the improvement of the stability of
DrGCNs.
Define:
K,
Pi Ciis2i - d Pij CjSlisj
(Pi Cii - d Pij Cj) X "d li
(14)
where cij is the (i, j)th element of the convariance matrix C. Theorem 2 suggests that K measures
the instability of the update. The measure is a relative covariance measure that when S = I (without
Dr), K = 1. This quantity only involves entries ofC, and it is homogeneous of degree 0 with respect
to these entries and invariant under scalar multiplication on these entries. Being the covariance
3|| ∙ ||f is the Frobenius norm, i.e. for a matrix A = (Aij), ∣∣A∣∣F = Pi j Aj.
4
Under review as a conference paper at ICLR 2020
matrix of H, C does not change under the mean zero case of H. Consequently, we could proceed
our analyses under the dimensional normalized assumption without loss of generality. We turn
to consider the dimensional normalized version of Vφ by replacing φ with dφ , which is φ with
normalization:
dφ : Rd → Rd,dφ(H) = Φ( v∣dGH'H),	(15)
where G I — d117 t i.e. Gx X — μ1, μ d Pi Xi.
Theorem 2.	Near the fixed point C* of Vdφ, the exponential growth rate ofthe deviation of Cl from
C* is proportional to K.
Here C* is used to denote the BSB1 fixed points of Vdφ (I, C) * 5. Since SlH has covariance matrix
SlCSl, our scaling effect is that Vdφ (S, C) = VBφ (I, SlCSl). We use the following definitions to
simplify notations.
Define:	CG,GCGT,K(S,C) , SCS.	(16)
Theorem 3.6 of (Yang et al., 2019) tells us the derivative of Vdφ (I, C) (as a linear map) has a very
explicit eigenspace decomposition, we describe it in Theorem 3. A simple reflection suggests that
our linear operators still satisfy the DOS condition and the ultrasymmetry condition needed in the
proof of this theorem, so this decomposition still holds.
Theorem 3.	Jd" := ddVCφ at C* has eigenspaces and eigenvalues:
1.	V0 = {C0 : CG = 0}, with eigenvalue 0. 6.
2.	VG = RG, with eigenvalue λG.
3.	A (d - 1) dimensional eigenspace VL. = {DG : D diagonal, trD = 0}.
4.	A d(d—3) -dimensional eigenspace VM = {C : CG = C, diag C = 0} with eigenvalue Xm.
λL, λM < 1, whereas λG > 1.
So an appropriately chosen S can reduce the proportion that lies in VG. We prove that the Frobenius
norm of the component in VG is proportional to K in Appendix H. Thus, it is natural to consider
the orthogonal (in terms of Frobenius norm and corresponding inner product) eigendecomposition
(with subindices indicating the corresponding eigenspaces we listed above):
K(S,C)=C0+CG+CL+CM.	(17)
The effect of Dr is to reduce the RG-component at each step to make the dynamic system more
stable. Since the decomposition is orthogonal, this is equivalent to reducing
Gl :=< SlCSl,G>,	(18)
recall that G = I - 11~t, i.e. GX = X - μl, μ = d Pi xi,. Since We take the normalization
assumption, only the relative magnitude of sli matters, and we can put any homogeneous restriction.
In order to include the case sli = l, we consider the restriction Pid=1 sl2i = d. By definition
Cij = E[hi hj], hence we have
< SlCSl, G > = Tr(SlCSl(I - 1(11T)T)
=Tr(SlCSl) - 1 Tr(SlCSlnT)
=X ciisli - d X Cij slislj∙
Finally we come to the effectiveness measure of the proposed Dr Block.
Pi CiiS2i - d Pij Cij SliSlj
-K	PP	2^
(Pi Cii - d Pi,j Cij) × 7
(19)
(20)
4~1 is the d-dimensional vector with all component 1.
5All results involve the BSB1 fixed point (Yang et al., 2019) require permutation, diagonal and off-diagonal
symmetry, and hold for dimensional normalization, too.
6 G, S above are symmetric, so the transpose is only introduced for the sake of notational balance
5
Under review as a conference paper at ICLR 2020
Table 1: Dataset statistics.
	CoraR	CiteseerR	PUbmed	PPI	Reddit	A*
Nodes/Users	2,680	3,191	19,717	56,944	232,965	35,246,808
Edges	5,148	4,172	44,324	818,716	11,606,919	129,834,116
Classes/Items	7	6	3	121	41	6,338,428
FeatUres	302	768	500	50	602	27
Training Nodes	1,180	1,691	18,217	44,906	152,410	35,246,808
Validation Nodes	500	500	500	6,514	23,699	-
Test Nodes	1,000	1,000	1,000	5,524	55,334	35,246,808
The denominator is chosen to display the ratio of variance reduction for the proposed Dr Block.
Without Dr, sli = 1 for all i, and we have K = 1. From our calculation on the inner product, it can
be discovered that this quantity is proportional to the part in VG in the orthogonal decomposition, this
proves Theorem 2. Since this is the only part for Jdφ with eigenvalue larger than 1, the exponential
growth rate is proportional to this quantity. Therefore, this quantity measures the magnitude of
improvement Dr Blocks make to the stability of the learning process under perturbation.
4 Experiments
In this section, we evaluate the proposed DrGCNs on a variety of datasets compared to several SOTA
methods. Detailed descriptions of the experiments and datasets are included in Appendix C,D.
4.1	Experimental Settings
Datasets We present the performance of DrGCNs on several public benchmark node classifica-
tion datasets, including Pubmed (Yang et al., 2016), Reddit, PPI (Hamilton et al., 2017). We also
conduct experiments on a large-scale real-world commercial recommendation A* dataset. Table 1
summarizes statistics of the datasets.
There are also two widely adopted Cora and Citeseer datasets (Yang et al., 2016) for citation net-
works. We investigate the originality of these datasets not only from the public data provided by
(Yang et al., 2016) but also from much earlier versions (McCallum et al., 2000; Lu & Getoor, 2003)
and find problems in those datasets. 32(1.2%) samples in Cora and 161(4.8%) samples in Citeseer
are duplicated, while 1,145(42.3%) samples in Cora and 2,055(61.8%) samples in Citeseer have in-
formation leak that includes their labels directly as a dimension of their features. To address such
problems, we remove duplicated samples, modify their features using word and text embeddings
to reduce information leak, and construct two refined datasets, CoraR and CiteseerR. For details of
these refined datasets and A* dataset, please refer to Appendix A,B,E.
Competitors We compare the results of our DrGCN with GCN (Kipf & Welling, 2017),
GAT (Velickovic et al., 2018), MixHoP (AbU-El-Haija et al., 2019), GraPhSAGE (Hamilton et al.,
2017), FastGCN (Chen et al., 2018) and ASGCN (Huang et al., 2018) on citation networks including
CoraR, CiteseerR and PUbmed. We also Provide resUlts on the original Cora and Citeseer dataset
in APPendix C. OUr DrGCN also works for indUctive datasets that we evalUate the Dr-GAT with
several state-of-the-art methods on the PPI dataset, inclUding GraPhSAGE (Hamilton et al., 2017),
LGCN (Gao et al., 2018), and GeniePath (LiU et al., 2019). As for A* dataset, we comPare oUr
method with the comPany’s PrevioUs best GraPhSAGE model, see APPendix E.
DrGCNs We combine Dr block with five most rePresentative GCN methods and comPare with
them on PUblic datasets. Two fUll GCN methods inclUde the vanilla GCN (KiPf & Welling, 2017)
and a variant that exploits an attention aggregator GAT (Velickovic et al., 2θl8). Sampling GCN
methods contain FastGCN (Chen et al., 2018), AdaPtive SamPling GCN (HUang et al., 2018), and
A* company’s heterogeneoUs GraphSAGE model on A* dataset. Every GCN layer is replaced by a
DrGCN layer as in EqUation (4). FUrther implementation details are covered in Appendix C.
4.2	Results and Analyses
Table 2 illUstrates the performance of DrGCNs on foUr transdUctive datasets when combined with
foUr different variations of GCN models. OUr resUlts are averaged among 20 rUns with different
random seeds. OUr DrGCNs achieve sUperior performances on all of the datasets and demonstrate
6
Under review as a conference paper at ICLR 2020
Table 2: Summary of classification accuracy on public transductive datasets(%).
Category	Method	CoraR	CiteseerR	Pubmed	Reddit
	GCN	85.9±0.5	74.9± 0.7	88.0± 0.3	-
Full GCNs	GAT	86.9± 0.4	76.5± 0.4	85.0± 0.2	-
	Mix-Hop	85.9± 0.5	75.3± 0.3	88.1± 0.2	-
SamPling-based GCNs	GraphSage	84.9± 0.5	70.6± 0.8	84.2± 0.3	94.8± 0.1
	FastGCN	81.7± 0.4	74.7± 0.6	88.3± 0.4	92.5± 0.2
	ASGCN	84.3± 0.7	75.1± 0.9	89.8± 0.3	96.4± 0.3
DrGCNs	Dr-GCN	86.8± 0.5	77.5± 0.6	88.4± 0.3	-
	Dr-GAT	87.3± 0.2	77.0± 0.3	84.7± 0.4	-
DrGCNs(samPling based)	Dr-FastGCN	81.6± 0.5	75.5± 0.5	88.2± 0.3	94.0± 0.1
	Dr-ASGCN	84.3± 0.5	75.4± 0.4	90.3± 0.4	97.9± 0.1
Table 3: Summary of performance (micro F1) on inductive PPI dataset.
Method I GraPhSAGE LGCN GeniePath GAT Dr-GAT
microF1 ∣ 61.2	77.2	97.9	97.3 98.8± 0.1
relatively significant imProvement on the Reddit dataset. Dr-ASGCN even reduces the error rates
by more than 40% (3.6% → 2.1%), comPared with Previous state-of-the-art method ASGCN.
The Performance imProvements can be exPlained by our stability measure ProPosed in Equation
(20). Theoretically, when K ≈ 1, we exPect Dr block to have limited ability in refining the reP-
resentation, while when K 1 we exPect the vector to strengthen the stability of the model by
reducing the magnitude of the derivatives of the covariance matrix and imProve the Performance.
To verify the theoretical analyses, we collect the average K-value of the learnt reweighting vectors
for different layers in the Dr-ASGCN model, see Table 5. The K-value in the second layer is around
1 on all datasets. However, the K-value for the first layer is around 1 for citation datasets, but 0.32
on the Reddit dataset, which emPhatically exPlains why the DR-ASGCN achieves such a massive
imProvement on the Reddit dataset.
On the inductive PPI dataset (Table 3), Dr Block increases the micro f1-score of GAT by 1.5%
and outPerforms all Previous methods. Table 4 suggests that the Dr method can also accomPlish
substantial imProvements on the real-world, large-scale recommendation dataset. It demonstrates
imProvement on industrial measure recall@50, which is the rate of users clicking the toP 50 Pre-
dicted items among 6 million different items within the next day of the training set, from 5.19%
(Previous best model) to 5.26% (Dr Block added).
4.3 Batch-norm and Layer-norm
We also comPare DrGCNs with other feature refining methods, including Batch-Norm (Ioffe &
Szegedy, 2015) and Layer-Norm (Lei Ba et al., 2016). These methods use variance information on
every single dimension to refine rePresentations, while DrGCN joins information on each dimension
and learns a reweighting vector S adaPtively. We Provide results of DrGCN and these methods on the
Reddit dataset for ASGCN( Table 6). Batch-Norm and Layer-Norm also imProve the Performance
of the ASGCN model on the Reddit dataset. Combining Dr and Layer-norm yields an even better
result for ASGCN on Reddit. More detailed results are in APPendix F.
5	Related Works
The idea of using neural networks to model graPh-based data can be traced back to GraPh Neural
Network (Scarselli et al., 2009), which adoPts a neural network structure on graPh structure learning.
GCN (KiPf & Welling, 2017) ProPoses a deeP-learning-based method to learn node rePresentations
on a graPh using gathered information from the neighborhood of a node. GraPhSAGE (Hamilton
et al., 2017) formulated a samPle and aggregation framework of inductive node embedding. The idea
of the samPle and aggregation framework is to incorPorate information from the neighborhood to
generate node embeddings. DesPite being uniform when first being ProPosed, both samPling and ag-
gregation can be weighted. These methods, including FastGCN (Chen et al., 2018), GAT (Velickovic
7
Under review as a conference paper at ICLR 2020
Table 4: Performance (Recall@50) on A* online recommendation system. GraphSAGE refers to
A* company’s best heterogeneous GraphSAGE model.
Method I GraPhSAGE ∣ DrGraPhSAGE
Recan@50(%) ∣	5.19	∣	5.26
Table 5: Classification accuracy of ASGCN and Dr-ASGCN model, with average K value learnt for
each layer.
Method	Cora	Citeseer	CoraE	CiteseerE	Pubmed	Reddit
ASGCN	87.23	78.95	84.30	75.12	89.82	96.37
Dr-ASGCN	87.07	79.06	84.34	75.44	90.34	97.95
Improvement Rate(%)	-0.16	0.11	0.04	0.32	0.52	1.58
Learnt K-value(Layer 1)	1.04	1.01	0.90	0.95	0.98	0.32
Learnt K-value(Layer 2)	1.00	1.00	0.99	1.04	0.98	1.14
Table 6: Accuracy and average training time Per ePoch for Plain, batchnorm, layernorm and DrAS-
GCN methods on Reddit dataset.
Method	ASGCN	Batch-norm	Layer-norm	Dr	Dr+LN
Accuracy(%)	96.37± 0.22	96.99± 0.15	97.68± 0.15	97.95± 0.13	98.02± 0.12
Time(s/epoch)	17.99	17.90	18.74	17.46	17.93
et al., 2018), LGCN (Gao et al., 2018), ASGCN (Huang et al., 2018), GaAN (Zhang et al., 2018),
and Mix-HoP (Abu-El-Haija et al., 2019), treat all nodes in the graPh unequally and try to figure out
more imPortant nodes and assign them higher weights in samPling and aggregation Procedure.
Feature imbalance Phenomena have long been aware of. (Blum & Langley, 1997) Different dimen-
sions of the hidden rePresentation generated by neural networks may also share such imbalance
behavior. The idea of refining hidden rePresentations in neural networks can be traced back to Net-
work in Network (Lin et al., 2014), whom ProPoses a fully-connected neural network to refine the
pixel-wise hidden representation before each convolutional layer-known as the 1 X 1 convolution
layer which is widely adoPted in modern convolutional neural networks. Squeeze and Excitation
Networks (Hu et al., 2018) proposes a dimensional reweighting method called Squeeze and Excita-
tion block, which involves the techniques of global average pooling and encoder-decoder structure.
It works well in computer vision CNNs and wins the image classification task of Imagenet 2017.
The success attracts our concern that such dimensional reweighting methods might also be useful in
node representation learning on graphs.
Another natural idea to refine representations of neural networks is normalization. Batch normaliza-
tion (Ioffe & Szegedy, 2015) is a useful technique in neural networks to normalize and reduce the
variance of input representations. Layer normalization (Lei Ba et al., 2016) is an improved version
of BatchNorm, for it also works on a single sample.
Many also try to give theoretical analyses to such normalization techniques. (Kohler et al., 2018)
explains the efficiency of batch normalization in terms of convergence rate. (Bjorck et al., 2018)
shows that batch normalization enables larger learning rates. (Yang et al., 2019) demonstrates the
gradient explosion behaviors of batch normalization on fully-connected networks via mean field
theory (Kadanoff, 2009). In our approach, we adopt some of these methods and apply them to the
analysis of DrGCNs.
6	Conclusion
We propose Dimensional reweighting Graph Convolutional Networks (DrGCNs) and prove that
DrGCNs can improve the stability of GCN models via mean field theory. Further explorations lead to
the proposal of a new measure K to evaluate the effectiveness of DrGCNs. We conduct experiments
on various benchmark datasets and compare DrGCNs with several GCN variations. Experimental
results not only prove the efficiency of DrGCNs, but also support the theoretical analysis on measure
K. DrGCNs’ usefulness is likewise verified on large-scale industrial dataset A*.
8
Under review as a conference paper at ICLR 2020
References
Ossama Abdel-Hamid, Abdel-rahman Mohamed, Hui Jiang, Li Deng, Gerald Penn, and Dong Yu.
Convolutional neural networks for speech recognition. IEEE/ACM Transactions on audio, speech,
and language processing, 22(10):1533-1545, 2014.
Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Hrayr Harutyunyan, Nazanin Alipourfard,
Kristina Lerman, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolu-
tion architectures via sparsified neighborhood mixing. In ICML, 2019.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples. Journal of machine learning research, 7
(Nov):2399-2434, 2006.
Nils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger. Understanding batch normal-
ization. In Advances in Neural Information Processing Systems, pp. 7694-7705, 2018.
Avrim L Blum and Pat Langley. Selection of relevant features and examples in machine learning.
Artificial intelligence, 97(1-2):245-271, 1997.
Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: fast learning with graph convolutional networks via
importance sampling. In ICLR, 2018.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Advances in neural information processing systems,
pp. 3844-3852, 2016.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, 2019.
Ming Ding, Jie Tang, and Jie Zhang. Semi-supervised learning on graphs with generative adversarial
nets. In Proceedings of the 27th ACM International Conference on Information and Knowledge
Management, pp. 913-922. ACM, 2018.
Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional
networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pp. 1416-1424. ACM, 2018.
Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu. Graphnas: Graph neural architecture
search with reinforcement learning. arXiv preprint arXiv:1904.09981, 2019.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in Neural Information Processing Systems, pp. 1024-1034, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 7132-7141, 2018.
Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph
representation learning. In Advances in Neural Information Processing Systems, pp. 4558-4567,
2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448-456,
2015.
Leo P Kadanoff. More is the same; phase transitions and mean field theories. Journal of Statistical
Physics, 137(5-6):777, 2009.
9
Under review as a conference paper at ICLR 2020
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In ICLR, 2017.
Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Ming Zhou, Klaus Neymeyr, and Thomas Hof-
mann. Towards a theoretical understanding of batch normalization. stat, 1050:27, 2018.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. In ICLR, 2018.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. NIPS 2016 Deep
Learning Symposium, 2016.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. In ICLR, 2014.
Ziqi Liu, Chaochao Chen, Longfei Li, Jun Zhou, Xiaolong Li, Le Song, and Yuan Qi. Geniepath:
Graph neural networks with adaptive receptive paths. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 33,pp. 4424-443l, 2019.
Qing Lu and Lise Getoor. Link-based classification. In Proceedings of the 20th International
Conference on Machine Learning (ICML-03), pp. 496-503, 2003.
Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the
construction of internet portals with machine learning. Information Retrieval, 3(2):127-163,
2000.
Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M
Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5115-
5124, 2017.
Federico Monti, Oleksandr Shchur, Aleksandar Bojchevski, Or Litany, Stephan Gunnemann,
and Michael M Bronstein. Dual-primal graph convolutional networks. arXiv preprint
arXiv:1806.00770, 2018.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532-1543, 2014.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre-
sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining, pp. 701-710. ACM, 2014.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2009.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93-93, 2008.
Aravind Subramanian, Pablo Tamayo, Vamsi K Mootha, Sayan Mukherjee, Benjamin L Ebert,
Michael A Gillette, Amanda Paulovich, Scott L Pomeroy, Todd R Golub, Eric S Lander, et al.
Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expres-
sion profiles. Proceedings of the National Academy of Sciences, 102(43):15545-15550, 2005.
Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. Arnetminer: extraction and
mining of academic social networks. In Proceedings of the 14th ACM SIGKDD international
conference on Knowledge discovery and data mining, pp. 990-998. ACM, 2008.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In ICLR, 2018.
Jason Weston, Frederic Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi-
supervised embedding. In Neural Networks: Tricks of the Trade, pp. 639-655. Springer, 2012.
Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S Schoenholz. A
mean field theory of batch normalization. In ICLR, 2019.
10
Under review as a conference paper at ICLR 2020
Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning
with graph embeddings. In Proceedings of the 33rd International Conference on International
Conference on Machine Learning-Volume 48, pp. 40-48. JMLR. org, 2016.
Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: Gated
attention networks for learning on large and spatiotemporal graphs. In UAI, 2018.
Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian
fields and harmonic functions. In Proceedings of the 20th International conference on Machine
learning (ICML-03), pp. 912-919, 2003.
11
Under review as a conference paper at ICLR 2020
Appendices
A Problems of Cora and Citeseer dataset
We investigate the originality of the Cora and CiteSeer dataset. The two datasets are widely used for
being light-weighted and easy to handle. The most popular version is provided by Planetoid (Yang
et al., 2016). The two datasets are both citation networks where each node represents a research
paper, and each edge represents a citation relationship between two papers. Edges are directed but
are usually handled undirectedly by GCN methods. Each paper belongs to a sub-field in computer
science and is marked as its label. Papers have features of bag-of-word(BOW) vectors that each
dimension represents whether the document of the paper contains a particular word in the dictionary
or not. Cora has 2,708 papers with a dictionary size of 1,433, while Citeseer has 3,327 papers with
a dictionary size of 3,703.
A.1 Cora
Cora originates in (McCallum et al., 2000)7 with extracted information(including titles, authors,
abstracts, references, download links etc.) in plain-text form. Those download links are mostly
unavailable now. Before they become unavailable, (Lu & Getoor, 2003)8 extracts a subset of 2,708
papers and assigns labels and BOW feature vectors to the papers. The dictionary is chosen from
words(after stemming)9 that occur 10 or more times in all papers and result in a dictionary size of
1,433. Planetoid (Yang et al., 2016) reordered each node to form the benchmark Cora dataset (Yang
et al., 2016).
There exist a lot of duplicated papers (one paper appears as multiple identical papers in the dataset)
in the original Cora of (McCallum et al., 2000), and (Lu & Getoor, 2003) inherits the problem of
duplicated papers. In Cora, we find 32 duplicated papers among the 2,708 papers. Another problem
is the information leak. The generation process of the dictionary chooses words that occur more than
10 times, and does not exclude the label contexts of papers. Therefore, some papers may be clas-
sified easily only by looking at their labels. For instance, 61.8% of papers labeled ”reinforcement
learning” contain exactly the word ”reinforcement” ”learning” in their title and abstract(after stem-
ming). Altogether 1,145(42.3%) of these papers contain their label as one or some of the dimensions
of their features. 10
A.2 CiteSeer
CiteSeer is a digital library maintained by PSU(Currently named as CiteSeerX11), which provides a
URL based web API for each paper according to their doc-ids. (Lu & Getoor, 2003)12 extracts 3,327
nodes from the library to form a connected citation network in which 3,312 nodes are associated with
a BOW vector of dictionary size 3,703. The rest 15 nodes’ features are padded with zero vectors. It
is also reordered and adopted by Planetoid (Yang et al., 2016).
Although the original version of Citeseer only consists of links that are unavailable now, a back-up
version of Citeseer contains the title and abstract information of 3,186 of the papers. 13 We also find
another 81 by author and year information using Google Scholar. Unfortunately, among the papers
we collected from the Citeseer dataset, 161(4.8%) of them are actually duplicated.
Since the Citeseer dataset shares a similar embedding generation method with Cora, there also exists
the problem of information leak. For the data we collected, at least 2, 055(61.8%) of the papers in
Citeseer contain their labels in title or abstract, which are sure to become some of the dimensions of
their feature representations.
7https://People.cs.umass.edu/~mccallum/data/cora.tar.gz
8https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz
9Stemming basically transforms a word into its original form.
10If we include references of the paper the rate becomes 86.1%, see what GCN learns!
11http://citeseerx.ist.psu.edu
12https://linqs-data.soe.ucsc.edu/public/lbc/citeseer.tgz
13https://www.cs.uic.edu/~cornelia/russir14/lectures/citeseer.txt
12
Under review as a conference paper at ICLR 2020
Table 7: Label terms appearing in Cora’s title and abstract.
	Case Based	Theory	Probabilistic Methods	Rule Learning	Neural Networks	Reinforcement Learning	Genetic Algorithms	Total
Total	298	351	426	180	818	217	418	2708
Appeared	151	84	103	38	410	134	225	1145
Rate	50.7%	23.9%	24.2%	21.1%	50.1%	61.8%	53.8%	42.3%
Table 8: Label terms appearing in CiteSeer’s title and abstract.
	Artificial Intelligence	Machine Learning	Information Retrieval	Database	Agents	Human Computer Interface	Total
Total	264	590	668	701	596	508	3327
Appeared	54	334	434	403	521	309	2055
Rate	20.5%	56.6%	65.0%	57.5%	87.4%	60.8%	61.8%
B	The Refined CoraR and CiteseerR dataset
Due to the problems in Cora and Citeseer, we produce new datasets CoraR and CiteSeerR to address
these issues. We remove duplicated nodes and generate indirect pre-trained word/text embeddings
as node features to reduce the information leak.
B.1	CORAR
We double-check the articles belong to Cora (Lu & Getoor, 2003) dataset in the original Cora (Mc-
Callum et al., 2000). Among the 2,708 papers, 474 of them have a wrong title that can hardly be
found on any academic search engines such as google scholar or aminer (Tang et al., 2008). We
manually searched for these papers based on other information like author, abstract, references and
feature vectors in Cora. Finally, we figure out that 32 of the total 2,708 papers are duplicated papers
that actually belong to 13 identical ones. 9 papers are absolutely missing and not able to trace. We
recover the actual title of the rest 2,680 papers, and use their titles to generate their features. We
apply averaged GLOVE-300 (Pennington et al., 2014) word vector for their titles(with stop words
removed) and we add two dimensions expressing the length of the title, and the length of the title
with stop words removed. This leads to a 302 dimension feature representation for each node in
CoraR. The average and word embedding process can better reduce the effect of label information
leak than using simple BOW vectors. As in (Hamilton et al., 2017; Chen et al., 2018), we split the
2,680 nodes into a training set of 1,180 nodes, a validation set of 500 nodes and a test set of 1,000
nodes.
B.2	CiteSeerR
We use Cornelia’s back-up extractions of CiteSeer and manually find some other documents using
author and year information from academic search engines. For the rest we only have a numerical
ID and an invalid download link, so we are not able to trace them. We check the titles, abstracts and
feature vectors and find 161 papers are actually 80 identical papers. We combine duplicated papers
together and also remove 55 papers that we are not able to trace. Our refined CiteSeerR has 3,191
nodes and 4,172 edges. We average over the last hidden state of BERT (Devlin et al., 2019) for each
sentence from the title and abstract and produce a 768-dimensional feature for each paper. For pairs
of duplicated papers with distinct labels, we manually assign a label for it. As in (Hamilton et al.,
2017; Chen et al., 2018), we split the 3,191 nodes into a training set of 1,691 nodes, a validation set
of 500 nodes and a test set of 1,000 nodes.
Both CoraR and CiteseerR are included in the link of code.
13
Under review as a conference paper at ICLR 2020
C Implementation Details
C.1 Dr Block configuration
For the dimensional reweighting part of each layer, the dimension of the encoder g is set to the
closest integer of the square root of the dimension of the input node representation ri . The pooling
weight Wv is set to uniform weight Wv =薪,where |V| is the number of nodes in the graph for
full GCNs, and the batch size for batch-wise sampling GCNs. We use ELU activation for σg and
sigmoid activation for σs . We do not apply dropout, but apply weight regularization with the same
coefficient as the original GCNs in the Dr Blocks of DrGCNs.
We keep all other settings, including learning rate, early stop criteria, loss function, hidden rep-
resentation dimension, batch size, weight decay, and regularization loss the same as the original
models.
C.2 Evaluation Details
All of our methods and compared baseline methods are run 20 times, and we report the average
accuracy and variation for methods that we run. We evaluate their performance mainly based on
their released code and paper. For methods without codes, we use their reported performance if they
share the same evaluation settings as ours.
C.3 Method Details
We describe our method implementation details here.
GCN (Kipf & Welling, 2017) We use the GCN code provided in AS-GCN14, and use a 2-layer GCN
model with 64 hidden units in layer 1. We run it 20 times and report the average and variance. For
each running time, we use the model that performs best within 800 training epochs on the validation
set for testing.
GAT (Velickovic et al., 2018)
We use the GAT code provided by the authors 15. We use the dataset-specific structures described
in (Velickovic et al., 2018) and early stopping strategy mentioned in the code repo. The original
paper uses a high dropout rate of 0.6 on semi-supervised citation datasets test setting. We find that
for CoraR, CiteseerR and the fully-supervised training set setting on Pubmed, such a high dropout
may have a chance to lead the original GAT model not to converge, so we adjusted the dropout rate
to 0.35(which gives the best performance among all dropout rates from 0 to 0.6) for both the original
and our Dr-GAT. On PPI we simply follow their instructions and use their suggested structure and
hyperparameters.
GAT forms a 2 layer structure for citation datasets. For Cora and Citeseer, GAT has 8 hidden units
in every attention head in the first layer, and 8 attention heads in the first layer and 1 in the second
layer, which has the number of hidden units equal to node classes. We adopt the structure for CoraR
and CiteseerR. We also discover that for the fully-supervised training set setting on Pubmed, the
structure for Pubmed in GAT paper(which has 8 heads in the second layer) does not perform as
good as the GAT structure for Cora and Citeseer (this setting achieves 82.5 ± 0.3% under the best
dropout rate), so we also adopt the Cora/Citeseer structure to Pubmed.
For PPI, GAT has a three layer structure, with 256 hidden units in every attention head in the first
two layers. It has 4 attention heads in the first layer and 4 in the second layer, and 6 attention heads
in the third layer, each third layer attention head has a number of hidden units equal to node classes.
It also sets dropout equals to 0 and uses residual connection (He et al., 2016).
14https://github.com/huangwb/AS-GCN
15https://github.com/PetarV-/GAT
14
Under review as a conference paper at ICLR 2020
FastGCN (Chen et al., 2018) We run the code provided by the authors. 16 We use the weighted
sampling method described in the paper, and use a neighborhood size of 400 for CoraR, CiteseerR,
Pubmed and 512 for Reddit. We run 20 times and generate an average performance and variations.
ASGCN (Huang et al., 2018) We use the code provided by the authors 17. We use a neighborhood
size of 128 for CoraR, CiteseerR, 256 for Pubmed and 512 for Reddit as the paper suggested. (Huang
et al., 2018) Their original code seems to have abug causing unnecessary test set information leaking
during the validation process. We modified their code to avoid such problems. We choose the best
model on validation set within 800 epochs for citation datasets and 200 epochs for Reddit and use
that for testing.
Mix-Hop (Abu-El-Haija et al., 2019) We use the code provided by the authors 18. We use the best
author provided script for Pubmed. Unfortunately the parameters the authors provided for Cora and
Citeseer does not work well for CoraR and CiteseerR, so we fix the network structure and tune the
hyperparameter set for CoraR and CiteseerR ourselves.
GraphSAGE (Hamilton et al., 2017) We use the code provided by the authors19 on CoraR, Cite-
seerR and Pubmed. For Reddit this code does not work so we use another code also provided by the
author 20
Other Methods Many of them do not have their codes released, so we use their reported perfor-
mance in their papers, or reported performance of their method by other papers, if we share a similar
evaluation setting.
Specifically, the performances of all baseline methods in appendix table 11 are from their original
papers (some non-GCN baseline results are from (Kipf & Welling, 2017)). The performance of
GraphSAGE, GeniePath, LGCN, GAT on the PPI dataset are from their original papers.
D Datasets
The details of our datasets are listed in this section. We generally use 6 datasets, including 3 citation
datasets, 1 Reddit dataset, 1 inductive PPI dataset, and 1 sizeable online recommendation A* dataset.
Citation Networks We evaluate the performance of our DR models on the three citation network
datasets, CoraR, CiteseerR, and Pubmed (Sen et al., 2008; Yang et al., 2016) There are two types of
experimental settings, the semi-supervised setting for full GCNs (KiPf & Welling, 2017; VeliCkovic
et al., 2018), which uses only a little fraction of the node labels on the graph and all link information
for training. Another fully-supervised setting (Chen et al., 2018; Huang et al., 2018) uses node labels
of the full graph except the validation and test set for training. The dataset statistics of original Cora,
Citeseer, and Pubmed dataset under these two settings are shown in table 9. We double-check the
adjacency matrices, remove self-loops, and correct the number of edges in these datasets, which is
mistaken in various papers, including GCN (Kipf & Welling, 2017). Besides evaluation on CoraR,
CiteseerR, and Pubmed in the main article, we provide the result of the fully-supervised setting for
Cora and Citeseer in table 10. We also provide our results on the semi-supervised setting on Cora,
Citeseer, and Pubmed dataset in table 11.
PPI The protein-protein interaction dataset is collected by SNAP (Hamilton et al., 2017) from
the Molecular Signatures Database (Subramanian et al., 2005), which is an inductive multi-label
node classification task. The training set contains 20 protein graphs, while the validation and test set
contains two graphs each. We evaluate the performance of different models by micro F1-score.
Reddit The Reddit dataset is collected by SNAP (Hamilton et al., 2017) from Reddit posts. It is a
node classification dataset for classifying different communities of each user by his/her posts.
16https://github.com/matenure/FastGCN
17https://github.com/huangwb/AS-GCN
18https://github.com/samihaija/mixhop
19https://github.com/williamleif/graphsage-simple
20https://github.com/williamleif/GraphSAGE
15
Under review as a conference paper at ICLR 2020
Table 9: Supplement of Dataset statistics for citation datasets.
	Cora	Citeseer	Pubmed
Nodes	2,708	3,327	19,717
Edges	5,278	4,552	44,324
Classes	7	6	3
Features	1,433	3,703	500
Training Nodes(Semi)	140	120	60
Training Nodes(Full)	1,208	1,827	18,217
Validation Nodes	500	500	500
Test Nodes	1,000	1,000	1,000
Table 10: Summary of classification accuracy on fully supervised Cora and Citeseer(%).
Category	Method	Cora	Citeseer
Full GCNs	GCN KiPf & Welling (2017)	86.4±0.3	77.4± 0.2
	GAT Velickovic et al. (2018)	87.2± 0.4	77.8± 0.2
Sampling-based	FastGCN Chen et al. (2018)	83.9± 0.4	78.6± 0.4
GCNs	ASGCN HUang et al. (2018)	87.2± 0.2	79.0± 0.4
Dr GCNs(ours)	Dr-GCN	86.8± 0.2	77.5± 0.3
	Dr-GAT	87.4±0.2	77.8± 0.2
Dr Sampling-based	Dr-FastGCN	84.0± 0.4	78.3± 0.3
GCNs(ours)	Dr-ASGCN	87.1± 0.2	79.1± 0.4
E A* dataset: Dataset, Baseline and Evaluation
As for the industrial A* dataset we use. It is an item recommendation dataset, with the training
set has about 35 million users and 6.3 million items with 120 million edges. Although the target
is node-classification like(to find the most likely items that each user may click), instead of simply
taking each item as a class, A* uses a graph embedding model to generate embedding for both users
and items. There are 27 user attributes and 33 item attributes. For every user, we use K nearest
neighbor (KNN) with Euclidean distance to calculate the top-N items that the user is most likely
to click, and the customer will see these recommended items in A* company’s APP. We use the
recall@N to evaluate the model:
recall@N = mean(X MuL∩JIu1)	(21)
V	|Iu1
Mu represents the top-N items recommended by the model and Iu indicates the items clicked by the
customer. The baseline model is the A* online heterogeneous GraphSAGE, and we add Dr block in
it to compare Recall@N with the online model.
Recall@50 is the most commonly used metric in A* company. Experimental results show that we
reach 5.264% on Recall@50, improving from the original best model’s 5.188%. It is quite a good
result, considering random guess will only give less than 0.001% (50/6,300,000).
F Batch-Norm and Layer-norm GCNs on citation networks
In Table 12 we also provide the batch-norm and layer-norm GCN results on publication datasets.
The results are averaged among 20 runs.
G Proof of Theorem 1
We also provide proof for theorem 1 in the main article.
16
Under review as a conference paper at ICLR 2020
Table 11: Summary of classification accuracy (%) of semi-supervised labels on citation datasets.
Category I	Method
Cora Citeseer Pubmed
MLP	55.1	46.5	71.4
ManiReg (Belkin et al., 2006)	59.5	60.1	70.7
SemiEmb (Weston et al., 2012)	59.0	59.6	71.1
LP (Zhu et al., 2003)	68.0	45.3	63.0
DeepWalk (Perozzi et al., 2014)	67.2	43.2	65.3
ICA (Lu & Getoor, 2003)	75.1	69.1	73.9
Planetoid (Yang et al., 2016)	75.7	64.7	77.2
GraphSGAN (Ding et al., 2018)	83.0	73.1	—
noitulovnoC hpar
Chebyshev (Defferrard et al., 2016)	81.2	69.8	74.4
GCN (Kipf & Welling, 2017)	81.5	70.3	79.0
MoNet (Monti et al., 2017)	81.7	—	78.8
DPFCNN (Monti et al., 2018)	83.3	72.6	—
LGCN (Gao et al., 2018)	83.3	73.0	79.5
GAT (VeIickovic et al., 2018)	83.0	72.5	79.0
Mix-Hop (Abu-El-Haija et al., 2019)	81.9	71.4	80.8
	DR-GCN	81.6± 0.1	71.0±0.6	79.2±0.4
DR-GCNs(ours)	DR-GAT	83.6± 0.5	72.8± 0.8	79.1±0.3
Table 12: GCN, BatchNorm , LayerNorm and DrGCN methods accuracy(%) on citation datasets.
Method	CoraR	CiteseerR	Pubmed
GCN	85.9± 0.5	74.9± 0.7	88.0± 0.3
Batch-norm	86.2± 0.5	77.5±0.4	88.6 ± 0.3
Layer-norm	85.6± 0.7	76.4 ± 0.9	89.9± 0.4
Dr-GCN	86.8± 0.5	77.5± 0.6	88.4± 0.3
Now we turn to prove theorem 1. We say a linear operator T : Hd → Hd is diagonal-off-diagonal
semidirect (DOS) if and only if:
∀C ∈ Hd, T (C)ii = ucii,
T (C)ij = vcii + vcjj + wcij .
Here u, v, w are constants, and we will call the set of operators with these parameters DOS(u, v, w).
By the definition of Vφ, the (i, j) component of its output will only involve the i - th and j - th
components of the input and symmetric with respect to them, hence itself and its derivatives Jφ
will also involve them only and being symmetric with respect to them. Thus it is determined by
cii , cij , cjj . Furthermore, since Jφ is a linear map, so it will have this form. The result in Theorem
1 should hold in general for DOS operators and do not require information about the fixed point
structure.
Now Jφ is a DOS operator, hence it will belong to DOS(u, v, w) for some u, v, w. Then we know:
d
I∣Jφ(c)I∣F = X (Jφ(c)ij )2
i,j=1
d
=	(ucii)2 +	(vcii + vcjj + wcij)2.
i=1	i6=j
17
Under review as a conference paper at ICLR 2020
Correspondingly we have:
d
∣∣jφ(scs)∣∣F = χ (jφ(scs)ij )2
i,j=1
d
=	(usi2cii)2 +	(vsi2cii + vsj2cjj + wsisj cij)2.
i=1	i6=j
Since the inequality we want to prove is homogeneous of degree 2 with respect to si on both sides,
hence without loss of generality we can assume Pid=1 si2 = d (this choice of gauge is intended to
include the case in which all si = 1). Consider the function of (nonzero) c ∈ Hd:
K(c) :
minS
I∣Jφ(scs)I∣F
I∣Jφ(c)I∣F
Here minS is minimizing over diagonal S with Pid=1 si2 = d, which is a compact set, hence the
minimum is achieved at some point for any fixed c. And notice that at S = Id, K = 1, so
K(c) ≤ 1 and the equality will hold if and only if S = Id is the minimun point of:
κK(c, S) ： = ∣∣Jφ(scs)∣∣F
d
=	(usi2cii)2 +	(vsi2cii + vsj2cjj + wsisjcij)2 ,
with this fixed c. In particular, we know that at s1 = s2 = ... = sd = 1 this function satisfties
Karush-Kuhn-Tucker (KKT) conditions (which could also be derived from the method of Lagrange
multiplier in this case). Next we derive the KKT condition for each component. Define:
dd
L(s1 , ..., sd, λ) ,	(usi cii ) +	(vsi cii + vsj cjj + wsisj cij ) - λ( si - d),
then the KKT conditions (i.e., the extreme value condition for restricted optimization) are:
4u2si3cii +	2(vsi2cii + vsj2 cjj + wsisjcij)(2vsicii + wsj cij) - 2λsi = 0.
j6=i
Now evaluate at s1 = ... = sd = 1, we have:
4u2cii +	2(vcii + vcjj + wcij)(2vcii + wcij) - 2λ = 0.
j6=i
When v 6= 0, the coefficient of c2 is nonzero. Thus this gives a quadratic defining function for those
C where our statement may fail. Denote the left hand side of the equation by Fi, when VcFi = 0,
it defines a smooth codimension 1 submanifold of Hd. When VCFi = 0, it gives rise to a linear
equation, in which cii has coefficient 4(d - 1)v2, hence still gives rise to a codimension one smooth
submanifold of Hd . In particular, the union of them is a codimension 1 object (not necessarily
smooth after We take union). Therefore, those C where K(c, S) could reach 1 have measure zero
(this can be proved rigorously by outer regularity of the Haar measure μ on Hd). Thus, for μ-almost
every matrix, we could choose an S with Pd=ι s2 = d, such that || Jφ(ScS)∣∣F < I∣Jφ(c)I∣f.
That is, the scaling increases the stability of updates.
H	Characterization of The Operator in Theorem 1
For T ∈ DOS(u, v, w), its eigenspace has clear characterization, which is useful in the study of
the behavior of T. Let Md be the subspace of Hd spanned by those matrices only has off-diagonal
18
Under review as a conference paper at ICLR 2020
entries, which have dimension d(d-I) and a basis Mij = Eij + Eji, where Eij is the matrix with 1
on (i, j) position and 0 anywhere else. And Ld is the span of Li, which is defined as:
•	…	一V	… ∖
•	∙ ∙	一V	∙ ∙ ∙
T Δ	…	…	λ .
Li，	—V —V ♦-W - U —V ♦一 ,
•	• •	一V	• • •
\	…	—V	…	)
where those non-zero entries are on i—th row and column.
Theorem 4. For T ∈ DOS (u, V, w), w 6= u, Md, Ld are its eigenspaces, with eigenvalues w, u
respectively.
Proof. Here the condition w 6= u ensures that Ld is linearly independent with elements in Md since
it spans the diagonal part of Hd . The results TMij = wMij , TLi = uLi could be calculated using
the definition equations of DOS (u, V, w) and consider them on component level. Since T is a linear
operator, verifying these eigen properties on the basis is enough for the result. Furthermore, the
space We have specified spans a d(d-I) + d = d(d+I) = dimHd dimensional space, hence it is the
whole Hd. Thus We have completely characterized the eigenspaces of such T.	口
I Diagram of DrGCN
Figure 2: Diagram of a DrGCN layer. The ”Sampling Neighbors” procudure is only applied in
sampling-based DrGCNs.
J	Pre-activation GCNs
As discussed in section 3, Our theoretical analysis is based on the pre-activation setting, while
common GCN methods use post activation. Although they are basically the same if we consider the
activation in the pre-activation setting as the activation of the previous layer in the post-activation
setting, there is still a little difference between pre and post activation that pre-activation activates
the input feature. So we also experiment pre-activation on GCN, see table 13. Results are averaged
among 20 runs.
19
Under review as a conference paper at ICLR 2020
Table 13: Results of GCN with pre and post activation on citation datasets.
Method	CoraR	CiteseerR	Pubmed
GCN(pre activation)	84.7± 0.7	74.1± 0.4	87.9± 0.3
Dr-GCN(pre activation)	85.1± 0.6	74.8±0.8	88.2 ± 0.2
GCN	85.9± 0.7	74.9 ± 0.7	88.0± 0.3
Dr-GCN	86.8± 0.5	77.5± 0.6	88.4± 0.3
20