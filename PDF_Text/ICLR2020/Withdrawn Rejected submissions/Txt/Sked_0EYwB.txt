Under review as a conference paper at ICLR 2020
Objective Mismatch in
Model-based Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Model-based reinforcement learning (MBRL) has been shown to be a powerful
framework for data-efficiently learning control of continuous tasks. Recent work
in MBRL has mostly focused on using more advanced function approximators and
planning schemes, leaving the general framework virtually unchanged since its
conception. In this paper, we identify a fundamental issue of the standard MBRL
framework - What We call the objective mismatch issue. Objective mismatch arises
when one objective is optimized in the hope that a second, often uncorrelated, met-
ric Will also be optimized. In the context of MBRL, We characterize the objective
mismatch betWeen training the forWard dynamics model W.r.t. the likelihood of
the one-step ahead prediction, and the overall goal of improving performance on a
doWnstream control task. For example, this issue can emerge With the realization
that dynamics models effective for a specific task do not necessarily need to be
globally accurate, and vice versa globally accurate models might not be sufficiently
accurate locally to obtain good control performance on a specific task. In our
experiments, We study this objective mismatch issue and demonstrate that the like-
lihood of the one-step ahead prediction is not alWays correlated With doWnstream
control performance. This observation highlights a critical flaW in the current
MBRL frameWork Which Will require further research to be fully understood and
addressed. We propose an initial method to mitigate the mismatch issue by re-
Weighting dynamics model training. Building on it, We conclude With a discussion
about other potential directions of future research for addressing this issue.
1 Introduction
Model-based reinforcement learning (MBRL) is a popular approach for learning to control nonlinear
systems that cannot be expressed analytically (Bertsekas, 1995; Sutton and Barto, 2018; Deisen-
roth and Rasmussen, 2011; Williams et al., 2017). MBRL techniques achieve the state of the art
performance for continuous-control problems With access to a limited number of trials (e.g. Chua
et al. (2018); Wang and Ba (2019)) and in controlling systems given only visual observations With no
observations of the original system’s state (e.g. Hafner et al. (2018); Zhang et al. (2018)). MBRL
approaches typically learn a forward dynamics model that predicts hoW the dynamical system Will
evolve When a set of control signals are applied. This model is classically fit With respect to the
maximum likelihood of a set of trajectories collected on the real system, and then used as part of a
control algorithm to be executed on the system (e.g., model-predictive control).
In this paper, We highlight a fundamental problem in the standard MBRL learning scheme: the
objective mismatch issue. The learning of the forWard dynamics model is decoupled from the
subsequent controller and policy that it induces through the optimization of tWo different objective
functions - negative log-likelihood (or its equivalent for deterministic models, e.g., the RMSE) of
the single- or multi-step look-ahead prediction for the dynamics model, and task performance (i.e.,
reWard) for the policy optimization. While the use of negative log-likelihood (NLL) for system
identification is an historically accepted objective, it results in optimizing an objective that does
not necessarily correlate to the actual performance. The contributions of this paper are to: 1)
identify and formalize the problem of objective mismatch in MBRL; 2) examine the signs of and the
effects of objective mismatch on simulated control tasks; 3) propose a initial mechanism to mitigate
objective mismatch; 4) discuss the impact of objective mismatch on existing MBRL and outline
future directions to address this problem.
1
Under review as a conference paper at ICLR 2020
Figure 1: Objective mismatch in MBRL arises when a dynamics model is trained to maximize the
likelihood but then used for the policy to maximize a reward signal that is not used during training.
2 Model-based REINFORCEMENT Learning
We now outline the MBRL formulation used in the
paper. At time t, we denote the state St ∈ Rds, the
actions at ∈ Rda, and the reward r(st,at). We say
that the MBRL agent acts in an environment gov-
erned by a state transition distribution p(st+1 |st, at).
We denote a parametric model to approximate this
distribution with pθ(st+1 |st, at). MBRL follows the
general approach of an agent acting in its environ-
ment, learning a model of said environment, and then
Algorithm 1: Model-based RL
Data: Initialize D from random actions
while Improving do
Train model pθ(st+1 |st, at) on D
Collect data D0 with controller using pθ
in real environment
Aggregate dataset D = D ∪ D0
leveraging the model to act more effectively, summarized in Alg. 1. While iterating over para-
metric control policies, the agent collects measurements of state, action, next-state and forms a
dataset D = {(sn, an, s0n)}nN=1, where N is the sum of the length of all episodes. With the dy-
namics data D, the agent learns the environment in the form of a neural network forward dynamics
model, learning an approximate dynamics pθ . This dynamics model is leveraged by a controller that
takes in the current state st and returns an action sequence at:t+T maximizing the expected reward
Eπθ(st) Pit=+tT r(si, ai), where T is the predictive horizon and πθ(st) is the set of state transitions
induced by the model pθ . In our paper, we primarily use probabilistic networks designed to minimize
the NLL of the predicted parametric distribution pθ, denoted as P, or ensembles of probabilistic
networks denoted P E, and compare to deterministic networks minimizing the mean squared er-
ror (MSE), denoted D or DE. Unless otherwise stated we use the models as in PETS (Chua et al.,
2018) with an expectation-based trajectory planner and a cross-entropy-method (CEM) optimizer.
3 Objective Mismatch and its Consequences
The Origin of Objective Mismatch: The Sub-
tle Differences between MBRL and System
Identification Many ideas and concepts in
model-based RL are rooted in the field of opti-
mal control and system identification (Bertsekas,
1995; Zhou et al., 1996; Kirk, 2012; Bryson,
2018; Sutton and Barto, 2018). In system identifi-
cation, the main idea is to use a two-step process
where we first generate on the robot (optimal)
elicitation trajectories τ to fit a dynamics model
(typically analytical), and subsequently we ap-
ply this model to a specific task. This particular
scheme has multiple assumptions: 1) the data col-
lected cover the entire state-action space (done by
appropriate elicitation trajectories); 2) the pres-
ence of large (virtually infinite) amount of data;
3) the global nature of the model resulting from
the system identification process (which should
be generalizable to a large range of tasks). With
these assumptions, the general idea of system
Figure 2: Sketch of the state-action space for
system identification and MBRL. (Left) In sys-
tem identification, the elicitation trajectories are
designed off-line to cover the entire state-action
space, without considering a specific task. (Right)
In MBRL instead, the data collected during the
learning is often concentrated in trajectories to-
wards the goal, with other parts of the state-action
space being completely unexplored (grey area).
identification is effectively to collect large amount of data covering the whole state-space to create
2
Under review as a conference paper at ICLR 2020
once a global model that is sufficiently accurate that we can at deployment time specify any desired
task, and still obtain good performance.
When adopting the idea of learning the dynamics model used in optimal control for MBRL, it is
important to consider if these assumptions still hold. The assumption of virtually infinite data is
visibly in tension with the explicit goal of MBRL which is to reduce the number of interactions with
the environment by being “smart” about the sampling of new trajectories. In fact, in MBRL the
offline data collection performed via elicitation trajectories is largely replaced by on-policy sampling
(with the exception of some initial motor babbling (Chua et al., 2018)) in order to explicitly reduce
the need to collect large amount of data. Moreover - in practice - in the MBRL setting the data
will not usually cover the entire state-action space, since they are generated by a policy trying to
optimize one specific task. In conjunction with the use of non-parametric models (i.e., not analytical
model), this results in learned models that are strongly biased towards capturing the distribution of
the data, and that are only locally accurate. Nonetheless, this is at first sight not an issue since the
common MBRL setting consider the learning aimed at solving only one specific task, and rarely test
for generalization capabilities of the learned dynamics.
In practice, we can now see how the assumptions and goals of system identification are in contrast
with the ones of MBRL. Understanding these differences and the downstream effects on algorithmic
approach is crucial to design new families of MBRL algorithms.
Objective Mismatch During the MBRL process of iteratively learning a controller, the reward
signal from the environment is diluted by the training of a forward dynamics model with a independent
metric, as showing in Fig. 1. In our experiments, we highlight that the minimization of some network
training cost does not hold a strong correlation to maximization of episode reward. As dynamic
environments becoming increasingly complex in dimensionality, the assumptions of collected data
distributions become weaker and over-fitting to different data poses an increased risk.
Formally, the problem of objective mismatch appears as two de-coupled optimization problems
repeated over many cycles of Alg. 1, shown in Eq. (1a,b), which could be at the cost of minimizing
the final reward. This loop becomes increasingly difficult to analyze as the dataset used for model
training changes with each experimental trial - a step that is needed to include new data from
previously unexplored states. In this paper we characterize the problems introduced by the interaction
of these two optimization problems, but avoid to consider the interactions added by the changes in
data distribution during the learning process, as this would significantly increase the complexity of
the analysis. In addition, we discuss potential solutions, but do not make claims about the best way to
do so, which is left for future work.
N	t+T
Training: arg min ɪ2 log Pθ (si∣si,ai), Control: arg max E∏θ(st) £^r(si,a，i)	(1a,b)
θ i=1	at:t+T	i=t
4	Identifying Objective Mismatch
In this section, we experimentally study the issue of objective mismatch in MBRL to answer the
following questions: 1) Does the distribution of models obtained from running a MBRL algorithm
show a strong correlation between NLL and reward? 2) Are there signs of sub-optimality in the
dynamics models training process that could be limiting performance? 3) What model differences are
reflected in reward but not in NLL?
Experimental Setting In our experiments, we use two popular RL benchmark tasks: the cartpole
and half cheetah. For more details on these tasks see Chua et al. (2018). For our cartpole experiments,
we aggregate dynamics models at each episode from 50 runs of PETS (20 trials per run) paired
with the on-policy episode reward achieved, giving the number of models Mcp = 1000. Our half
cheetah experiments consist of 8 runs (300 trials per run) of PETS, tabulating the reward with the
dynamics models again, yielding Mhc = 2400. All dynamics models are of depth 3, hidden width
500, and use the tanh activation function. With a large set of dynamics models in the feasible
space of on-policy models, we then used a series of control datasets to evaluate the NLL of the
dynamics models versus performance. The datasets we use are designed to investigate how different
assumptions made in MBRL trickle down into final performance. We start with expert datasets to
3
Under review as a conference paper at ICLR 2020
P.IBΛ∖3H əposɪdM
180
160
140
120
100
80
60
40
20
-10
-8	-6	-4	-2	0	2	4
Log Likelihood
(b) CartPole On-PoliCy (ρ = 0.34)
(C) CartPole Grid (ρ = -0.06)
-8	-6	-4-2	0	2
Log Likelihood
(a) Cartpole Expert (ρ = 0.59)
4≡^
.*Λ*,.∙*∙ V ・%・$ ,,
30004 ∙ ∙	'	∙
・ 2000扑 2,三 JY "
1000
(VQ 法f.
-10	-8	-6	-4-2	0	2
Log Likelihood
入八・・,∙
(d)	Half Cheetah Expert
(ρ = 0.07)
-10	-8	-6	-4	-2	0	2	4
Log Likelihood
(e)	Half Cheetah On-PoliCy
(ρ = 0.46)
-10	-8	-6	-4	-2	0	2	4
Log Likelihood
(f)	Half Cheetah SamPled
(ρ = 0.19)

Figure 3: The distribution of dynamiCs models from our PETS exPeriments Plotting in the LL-Reward
sPaCe on three datasets, with Correlation CoeffiCients ρ. (Notes: we are using log likelihood (LL) here
rather than NLL for easier interpretability and the reward at each point is the mean of 10 trials with
the CEM optimizer, to disentangle the stochasticity of MPC.) The numbers of models evaluated are
Mcp = 1000 and Mhc = 2400. For the datasets shown in (C,d,f) many of the LL’s are extremely high
and outside of the range of the figure, but the trend lines and Correlation CoeffiCients are CalCulated on
all Points. There is a trend of high reward to ‘good’ LL that breaks down as the datasets Contain more
of the state-sPaCe than only exPert trajeCtories. Trend lines are regressed from total least squares, and
inCluded when the sum of squares solution imPlemented Converges.
test if on-PoliCy PerformanCe is only linked to having adequately exPlored the environment. As a
baseline, we ComPare the exPert data to datasets ColleCted by standard data aggregation ProCedures
when learning. Finally, a dataset rePresenting the entire state sPaCe is generated as a grid over the
state-sPaCe or by samPling from reasonable values and measuring the transitions in order to test how
ComPlete dynamiCs models relates to model aCCuraCy and final PerformanCe. SPeCifiCally, the CartPole
datasets are as follows: an exPert seleCtion of ePisodes data with r > 179 (2, 400 Points), a standard
on-PoliCy dataset aggregated from the end of a PETS run (3, 780 Points), and a grid of dataPoints to
aPProximate the full system dynamiCs (16, 807 Points). For half Cheetah we ComPared three similar
datasets: an exPert dataset from a PETS run of half Cheetah using the true dynamiCs for Planning
(3, 000 Points), an on-PoliCy dataset aggregated from PETS (90, 900 Points), and an uniform samPled
dataset attemPting to aPProximate the full system dynamiCs (200, 000 Points).
4.1	Exploration of Model Loss vs Episode Reward Space
In the standard MBRL framework, it is assumed that there is a Clear ProPortional relationshiP between
model testing loss and on-PoliCy PerformanCe. We here show that this is a strong assumPtion even
in simPle domains, and on sPeCifiC on-PoliCy data. The relationshiPs between model aCCuraCy on
data rePresenting the full environment sPaCe and reward show no ConCrete trend in Fig. 3C,f. The
simPliCity of the CartPole environment results in quiCk learning and a ConCentration of networks
around Peak reward. The distribution of rewards versus log-likelihood (LL) is shown in Fig. 3a-C,
where there is a trend of inCreased reward as LL deCreases, but there is substantial varianCe and
Points of disagreement. This bi-model distribution on the half Cheetah exPert data set, shown in
most Clearly Fig. 3d, relates to a failure mode in early half Cheetah trials where the agent enters a
unreCoverable state. The Contrast between Fig. 3e and Fig. 3d,f shows a substantial differenCe in the
transitions rePresented within the datasets. The multiPle modes by whiCh half Cheetah runs during
different stages of learning warrants more investigation into how new data should be inCorPorated
into a training dataset while maintaining the knowledge of older data.
4
Under review as a conference paper at ICLR 2020
(a) D model.
(b) DE model (E = 5).
Figure 4: The reward versus epoch when re-evaluating the controller leveraging a dynamics model at
each training epoch for different types of dynamics models. Even for the simple cartpole environment,
networks of width 500 and depth 3 cannot learn the entire grid dataset: simple models (D, DE) fail
to achieve full performance, while more advanced models reach higher performance but eventually
over-fit to available data (P, PE). The over-fitting of the P model is further evaluated in Fig. 5a.
■ Validation Error ■ Episode Reward (X)
10
8
6
4
! 2
0
-2
-4
-6
(c) P model.
10
8
6
4
ɪ 2
0
-2
-4
-6
(d) PE model (E = 5).
These results show that objective mismatch is not preventing MBRL from functioning, which would
appear as model loss being fully dissociated with performance and the plots shown lacking any
coherent shape. Rather, the mismatch we show likely is a ceiling on the performance of current
MBRL algorithms by reducing the correlation between model accuracy and evaluation reward. The
distributions in this section of on-policy data in Fig. 3b,e show a noisy trend of higher reward with
better model loss and not the guarantee of improvement that is expected when training a better model.
4.2	MODEL LOSS VS EPISODE REWARD DURING TRAINING
This section explores the relationship between how
model training impacts performance at the per-epoch
level. These experiments also shed light onto the im-
pact of the strong dataset assumptions outlined in Sec. 3.
Figures 4 and 5 are generated by re-evaluating the con-
troller leveraging a dynamics model at each training
epoch. As a dynamics model is trained, there are two
key inflection points - the first is the training epoch
where episode reward is maximized, and the second is
when error on the validation set is minimized. When
training on data from the entire state-space, our experi-
ments show that the reward is maximized at a drastically
different time than when validation loss is minimized.
These experiments are focused on showing the discon-
nect between three standard practices in MBRL a) the
assumption that the dynamics data can express large
portions of the state-space when collected on policy,
b) the idea that simple neural networks can satisfacto-
rily capture complex dynamics, c) and the practice that
model training is a simple optimization problem discon-
nected from reward. For the cartpole environment with
the grid dataset, Fig. 4 shows that the performance of
the controller degrades for P , PE models and fails to
adequately learn with the D, DE models. For cartpole,
training on on-policy data replicates results of the tri-
als (> 175 reward), shown in Fig. 5b while the expert
dataset fails to achieve any learned performance (< 20
reward) because it lacks a robust enough dataset for
stable prediction generation during stochastic action
sampling. Fig. 5 highlights how the trained models are
able to represent other datasets that they are trained on,
■ Grid Data (4) ■ Policy Data (O)
■ Expert Data (◊) ■ Episode Reward (X)
10
8
6
4
H
月2
0
-2
-4
-6
0	100	200	300	400	500
Training Epoch
180
160
140
120
100
80
60
40
20
0
(b) Trained: on-policy Tested: expert, grid
Figure 5: The effect of the dataset choice on
model (P) training and accuracy in differ-
ent regions of the state-space. (Top) when
training on the complete dataset, the model
begin over-fitting to the on-policy data even
before the performance drops in the con-
troller. (Bottom) A model trained only on
policy data does not accurately model the
entire state-space.
which points to the need for fine-tuning of that assumption on datasets made at the outset between
the state space and that collected on-policy. There is no information indicating the on-policy dataset
will lead to a complete dynamics understanding because on-policy and grid show little correlation in
evaluation loss. When training the gird on cartpole, the fact that the on-policy data diverges before
5
Under review as a conference paper at ICLR 2020
(a) Initial Model:	(b) Adversarially Generated Model:
NLL:-4.83, Reward:176	NLL:-4.85, Reward:98
Figure 6: Planned trajectories along the expert trajectory for the initial model and the adversarially
generated model trained to lower the reward. It can be seen how the planned trajectories are
qualitatively similar except for the peak at t = 25. There, the adversarially generated model learned
that applying a small nudge to the dynamics model at the right place/moment yield to significantly
influencing the control outcome with minimal change in terms of NLL.
the reward decreasing is encouraging as objective mismatch may be preventable in simple tasks, but
the problem becomes increasingly complex as the dynamics data distribution becomes large and
multi-modal, as in half cheetah. Similar experiments on half cheetah data failed to achieve a matching
reward to the full trials because the training process in PETS involves incremental network with
dataset aggregation. When training on a single set of dynamics data for many epochs the controller
did not achieve comparable reward to networks incrementally trained on new data after each episode.
4.3	Decoupling Model Loss from Controller Performance
When two dynamics models evaluated on a control dataset have similar validation loss, there can be a
large variance in the reflected control policy. In this section, we explore how differences in dynamics
models are qualitatively reflected in control policies to show the reader that a accurate dynamics
model does not guarantee performance.
Adversarial attack on model performance We performed
an adversarial attack (Goodfellow et al., 2015; Szegedy et al.,
2013) on a neural network dynamics model so that it attains a
good likelihood but poor reward to continue illustrating objec-
tive mismatch. We start with a dynamics model that achieves
high likelihood and high reward and tweak the parameters
so that it continues achieving high likelihood but has a low
reward. We fine-tune the network’s last layer to minimize the
episode reward, with a large penalty if the model validation
likelihood drops below the original value. We use a zeroth-
order optimizer (CMA-ES) because the reward is generally
non-differentiable. As a starting point for this experiment we
sampled a P dynamics model from the last trial of a PETS run
180
160
P
140
120
100
80
-4.4
Start
-4.6	-4.8	-5	-5.2
NLL
-5.4
Figure 7: Convergence of the CMA-
ES population’s best member over
iterations to the minimum reward
achieved with comparable NLL.
on cartpole. This model achieves reward of 176 and has a NLL of -4.827 on it’s on-policy training
dataset. Using CMA-ES, We reduced the on-policy reward of the model to 98 - a 45% decrease in
performance compared to the original model - while slightly improving the NLL to —4.845; the
CMA-ES convergence over population iterations is shown in Fig. 7 and the difference between the
two models is visualized in Fig. 6. Fine tuning of all model parameters would be even more likely
to find sub-optimal performing controllers with low model loss because the output layer consists
of about 1% of the total model parameters. This experiment shows that the model parameters that
achieve a low model loss inhabit a broader space than the subset that also achieves high reward.
Sampling models with similar NLLs, different rewards To better understand the objective mis-
match, we also compared how a difference of model loss can impact a control policy. We sampled
models with similar NLL’s and extremely different rewards from Fig. 3d-e and visualized the chosen
optimal action sequences along an expert trajectory. The control policies and dynamics models appear
to be converging to different regions of state spaces. In these visualizations, there is not a emphatic
6
Under review as a conference paper at ICLR 2020
Standard DataSet Size
Weighted DatSet Size
Figure 8: Mean reward of PETS trials, with and without model re-weighting, on a log-grid of
dynamics model training sets with number of points S ∈ [10, 10000] and sampling expert-distance
bounds ∈ [.28, 15.66]. The re-weighting shows an ability to learn moderate performance at
substantially lower number of datapoints, but suffers from increased variance in larger set sizes. The
performance of PETS declines when the dynamics model is trained on points too near to the expert
dataset because the model lacks robustness when running online with the stochastic MPC.
reason why the models achieved different reward, so further study is needed to quantify the impact
of model differences. The interpretability of the difference between models and controllers will be
important to solving the objective-mismatch issue.
5	Addressing Objective Mismatch During Model Training
Tweaking dynamics model training can partially mitigate the problem of objective mismatch. While
keeping the NLL minimization standard in MBRL, the trainer can prioritize state transitions associated
with an expert trajectory over tuples further from expert, and dynamics model will use its capacity to
learn relevant tasks more quickly. We show improved sample efficiency on Cartpole via re-weighting
network training by a measure of Euclidean distance in the state-action space.
Given a element of a state space (si , ai), we can quantify the distance of any two tuples
{(si, ai, s0i), i = 1, 2} as d = ||[s1, a1] - [s2, a2]||. With this distance, we re-weight the loss,
l(y), of points further from an expert policy to be lower, so that points in the expert trajectory get
a weight ω(y) = 1, and points at the edge of the grid dataset used in Sec. 4 get a weight ω(y) = 0
(in our experiments, multiple monotonic functions from 0 to 1 had similar results). With this notion
of distance and weighting, we used the expert dataset discussed in Sec. 4 as a distance baseline.
We generated a base dataset consisting of 25, 000, 000 tuples of (s, a, s0) by uniformly sampling
across the state and action space of cartpole. We taxonomized this dataset by taking the minimum
orthogonal distance, d*, from each of the points to the 200 element dataset from one expert trajectory
that achieved a reward of 180. To create different datasets that range from near-expert to nearly
uniform across the state space, we vary the distance bound, , and number of points, S, trained on. For
each sampling bound, G We sample 5 different datasets such that d； < e from our distance-tabulated
random dataset, and for each dataset (S, ) we trained 5 P models, giving 25 episodes from PETS
algorithm on the cartpole task for each point in the heatmap shoWn in Fig. 8. This simple form of
re-Weighting the neural netWork loss, shoWn in Eq. (2a,b,c), demonstrated an improvement in sample
efficiency to learn the cartpole task, as seen in the left halves of each plot in Fig. 8. Developing
an iterative method to re-Weight samples in an online training method has the potential to further
improve the sample efficiency of MBRL baselines.
ed*(y)
Weighting ω(y) =------- Standard l(y, y)	Re-weight l(y, y) ∙ ω(y)	(2a,b,c)
6	Discussion, Related Work, and Future Work
Objective mismatch seriously impacts the performance of MBRL. Our experiments have gone deeper
into this fragility in the context of the state-of-the-art MBRL algorithms from an analysis perspective
rather than from a solution perspective. Beyond the re-Weighting of the NLL presented in Sec. 5, here
We summarize and discuss the key pieces in the community starting to address this issue.
7
Under review as a conference paper at ICLR 2020
Learning the dynamics model to optimize the task performance Most relevant to our work
are research directions on controllers that directly connect the reward signal back to terms of the
controller. In theory, this exactly solves the model mismatch problem we discuss in this paper but
in practice the current approaches are borderline intractable and have proven difficult to scale to
complex systems. One popular way to do this is by designing systems that are fully differentiable so
that the task reward can be backpropagated all the way to the dynamics. This has been investigated
with differentiable MPC (Amos et al., 2018), Path Integral control (Okada et al., 2017; Pereira et al.,
2018), and stochastic optimization (Donti et al., 2017). Universal Planning Networks (Srinivas et al.,
2018) propose a differentiable planner that unrolls gradient descent steps over the action space of a
planning network. An alternative explored in Bansal et al. (2017) is to use a zero-order optimizer
instead (Bayesian optimization specifically) which can be used to learn locally linear dynamics to
maximize the controller’s performance without having to compute gradients explicitly.
Shaping the cost or reward Closely related are methods that, instead of learning a dynamics
model optimizing task performance, fix the dynamics model and fine-tune the cost function of a
controller to optimize, e.g., imitation loss Trimpe et al. (2014); Talvitie (2018); Tamar et al. (2017).
Add heuristics to the dynamics model structure or training process to make control easier If
it is infeasible or intractable to shape the cost or dynamics components of a controller, adding
heuristics to the structure or training process of the dynamics model is reasonable and can give
state-of-the-art results in many settings. One challenge in these heuristics is that they may be unstable
and difficult to fix or improve when they do not work in new environments. These heuristics can
manifest in the form of learning a latent space that is locally linear, e.g., in Embed to Control and
related methods (Watter et al., 2015; Banijamali et al., 2017), by enforcing that the model makes
long-horizon predictions (Ke et al., 2019), ignoring parts of the state space that actions can not control
(Ghosh et al., 2018), detecting and correcting when a predictive model steps off the manifold of
reasonable states (Talvitie, 2017), adding reward signal prediction on top of the latent space Gelada
et al. (2019), adding noise when training transitions Mankowitz et al. (2019). among other approaches
JonschkoWski et al. (2018); Ke et al. (2019); Miladinovic et al. (2019); Ichter and Pavone (2019);
Singh et al. (2019).
Using incorrect models Instead of fixing the issues directly in the model or cost parameters, one
can assume that the approximations are alWays incorrect but to use them anyWay and potentially try
to correct them Abbeel et al. (2006); Sorg et al. (2010); Joseph et al. (2013); Nagabandi et al. (2018).
Add inductive biases to the controller Prior knoWledge and inductive biases can be added to the
controller in the form of hyper-parameters such as the horizon length, or by penalizing unreasonable
control sequences by using, e.g., a sleW rate penalty. These heuristics can significantly improve the
performance if done correctly but can be difficult to tune. Jiang et al. (2015) use complexity theory to
justify using a short planning horizon With an approximate model to reduce the the class of induced
policies. Whitney and Fergus (2018) empirically shoW that long horizons are better for some tasks.
Continuing Experiments Our experiments represent an initial exploration into the challenges of
objective mismatch in MBRL, but more analysis is needed. Our exploration in Sec. 4.2 is limited to
cartpole due to computational challenges of training With large dynamics datasets and Sec. 4.3 could
be strengthened by defining quantitative comparisons in controller performance other than episode
reWard. Additionally, the effects of model mismatch should be quantified in other state of the art
algorithms in MBRL such as MBPO (Janner et al., 2019) and POPLIN (Wang and Ba, 2019).
7	Conclusion
This paper identifies, formalizes and analyzes the issue of objective mismatch in MBRL. This
fundamental disconnect in objectives betWeen the training of the model W.r.t. the likelihood, and the
overall task reWard emerges from the subtle differences at the origins of MBRL With assumptions
that have not been fully adapted. Experimental results highlighted the negative effects that objective
mismatch have on the performance of a current state of the art MBRL algorithm. Our results are an
exposition of current limitations and an encouragement to investigate nuance in current practices. In
providing a first insight on the issue of objective mismatch in MBRL, We hope future Work Will more
deeply examine these underlying assumptions. With our re-Weighting method to mitigate mismatch,
Which also shoWs improvements in sample efficiency, We share our more thoughts about promising
directions of future research to address this issue. We believe that fundamentally understanding and
addressing the objective mismatch issue Will contribute to improving the performance of MBRL.
8
Under review as a conference paper at ICLR 2020
References
P. Abbeel, M. Quigley, and A. Y. Ng. Using inaccurate models in reinforcement learning. In
International Conference on Machine Learning (ICML), pages 1-8, 2006. ISBN 1-59593-383-2.
doi: 10.1145/1143844.1143845.
B. Amos, I. Jimenez, J. Sacks, B. Boots, and J. Z. Kolter. Differentiable MPC for end-to-end planning
and control. In Neural Information Processing Systems, pages 8289-8300, 2018.
E. Banijamali, R. Shu, M. Ghavamzadeh, H. Bui, and A. Ghodsi. Robust locally-linear controllable
embedding. arXiv preprint arXiv:1710.05373, 2017.
S. Bansal, R. Calandra, T. Xiao, S. Levine, and C. J. Tomlin. Goal-driven dynamics learning via
Bayesian optimization. In IEEE Conference on Decision and Control (CDC), pages 5168-5173,
2017. doi: 10.1109/CDC.2017.8264425.
D. P. Bertsekas. Dynamic programming and optimal control, volume 1. Athena scientific Belmont,
MA, 1995.
A. E. Bryson. Applied optimal control: optimization, estimation and control. Routledge, 2018.
K. Chua, R. Calandra, R. McAllister, and S. Levine. Deep reinforcement learning in a handful of
trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems,
pages 4754-4765, 2018.
M. Deisenroth and C. E. Rasmussen. Pilco: A model-based and data-efficient approach to policy
search. In Proceedings of the 28th International Conference on machine learning (ICML-11),
pages 465-472, 2011.
P. Donti, B. Amos, and J. Z. Kolter. Task-based end-to-end model learning in stochastic optimization.
In Advances in Neural Information Processing Systems, pages 5484-5494, 2017.
C.	Gelada, S. Kumar, J. Buckman, O. Nachum, and M. G. Bellemare. Deepmdp: Learning continuous
latent space models for representation learning. arXiv preprint arXiv:1906.02736, 2019.
D.	Ghosh, A. Gupta, and S. Levine. Learning actionable representations with goal-conditioned
policies. arXiv preprint arXiv:1811.07819, 2018.
I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In
International Conference on Learning Representations (ICLR), 2015.
D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson. Learning latent
dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.
B. Ichter and M. Pavone. Robot motion planning in learned latent spaces. IEEE Robotics and
Automation Letters, 2019.
M.	Janner, J. Fu, M. Zhang, and S. Levine. When to trust your model: Model-based policy optimiza-
tion. arXiv preprint arXiv:1906.08253, 2019.
N.	Jiang, A. Kulesza, S. Singh, and R. Lewis. The dependence of effective planning horizon on
model accuracy. In Proceedings of the 2015 International Conference on Autonomous Agents
and Multiagent Systems, pages 1181-1189. International Foundation for Autonomous Agents and
Multiagent Systems, 2015.
R.	Jonschkowski, D. Rastogi, and O. Brock. Differentiable particle filters: End-to-end learning with
algorithmic priors. arXiv preprint arXiv:1805.11122, 2018.
J.	Joseph, A. Geramifard, J. W. Roberts, J. P. How, and N. Roy. Reinforcement learning with
misspecified model classes. In 2013 IEEE International Conference on Robotics and Automation,
pages 939-946. IEEE, 2013.
N.	R. Ke, A. Singh, A. Touati, A. Goyal, Y. Bengio, D. Parikh, and D. Batra. Learning dynamics model
in reinforcement learning by incorporating the long term future. arXiv preprint arXiv:1903.01599,
2019.
9
Under review as a conference paper at ICLR 2020
D. E. Kirk. Optimal control theory: an introduction. Courier Corporation, 2012.
D. J. Mankowitz, N. Levine, R. Jeong, A. Abdolmaleki, J. T. Springenberg, T. Mann, T. Hester, and
M. Riedmiller. Robust reinforcement learning for continuous control with model misspecification.
arXiv preprint arXiv:1906.07516, 2019.
D. MiladinoviC, M. W. Gondal, B. Scholkopf, J. M. Buhmann, and S. Bauer. Disentangled state space
representations. arXiv preprint arXiv:1906.03255, 2019.
A. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn. Learning to
adapt in dynamic, real-world environments through meta-reinforcement learning. arXiv preprint
arXiv:1803.11347, 2018.
M. Okada, L. Rigazio, and T. Aoshima. Path integral networks: End-to-end differentiable optimal
control. arXiv preprint arXiv:1706.09597, 2017.
M. Pereira, D. D. Fan, G. N. An, and E. Theodorou. Mpc-inspired neural network policies for
sequential decision making. arXiv preprint arXiv:1802.05803, 2018.
S. Singh, S. M. Richards, V. Sindhwani, J.-J. E. Slotine, and M. Pavone. Learning stabilizable
nonlinear dynamics with contraction-based regularization, 2019.
J. Sorg, R. L. Lewis, and S. P. Singh. Reward design via online gradient ascent. In Advances in
Neural Information Processing Systems, pages 2190-2198, 2010.
A. Srinivas, A. Jabri, P. Abbeel, S. Levine, and C. Finn. Universal planning networks. arXiv preprint
arXiv:1804.00645, 2018.
R.	S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
E. Talvitie. Self-correcting models for model-based reinforcement learning. In Thirty-First AAAI
Conference on Artificial Intelligence, 2017.
E. Talvitie. Learning the reward function for a misspecified model. arXiv preprint arXiv:1801.09624,
2018.
A. Tamar, G. Thomas, T. Zhang, S. Levine, and P. Abbeel. Learning from the hindsight plan -
episodic MPC improvement. In IEEE International Conference on Robotics and Automation
(ICRA), pages 336-343, May 2017. doi: 10.1109/ICRA.2017.7989043.
S.	Trimpe, A. Millane, S. Doessegger, and R. D’Andrea. A self-tuning lqr approach demonstrated on
an inverted pendulum. IFAC Proceedings Volumes, 47(3):11281-11287, 2014.
T.	Wang and J. Ba. Exploring model-based planning with policy networks. arXiv preprint
arXiv:1906.08649, 2019.
M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller. Embed to control: A locally linear
latent dynamics model for control from raw images. In Advances in neural information processing
systems, pages 2746-2754, 2015.
W. Whitney and R. Fergus. Understanding the asymptotic performance of model-based rl methods.
2018.
G. Williams, N. Wagener, B. Goldfain, P. Drews, J. M. Rehg, B. Boots, and E. A. Theodorou.
Information theoretic MPC for model-based reinforcement learning. In International Conference
on Robotics and Automation (ICRA), pages 1714-1721, 2017.
M. Zhang, S. Vikram, L. Smith, P. Abbeel, M. J. Johnson, and S. Levine. Solar: Deep structured
latent representations for model-based reinforcement learning. arXiv preprint arXiv:1808.09105,
2018.
K. Zhou, J. C. Doyle, K. Glover, et al. Robust and optimal control, volume 40. Prentice hall New
Jersey, 1996.
10
Under review as a conference paper at ICLR 2020
Appendix
A Effect of Dataset Distribution when Learning
Learning speed can be slowed by many fac-
tors in dataset distribution, such as adding
additional irrelevant transitions. When ex-
tra transitions from a specific area of the
state space are included in the training set,
the dynamics model will spend increased
expression on these transitions. NLL of the
model will be biased down as it learns this
data, but it will reduce the learning speed
as new, more relevant transitions are added
to the training set.
Running cartpole random data collection
with a short horizon of 10 steps (while forc-
ing initial babbling state to always be 0),
for 20, 200,400 and 2000 babbling roll-
outs (that sums up to 200, 2000, 4000
and 20000 transitions in the dataset fi-
nally shows some regression in the learning
speed for runs with more useless data in the
motor babbling. This data highlights the
■ Random Transitions: 200	■ Random Transitions: 2000
■ Random Transitions: 4000	■ Random Transitions: 20000
Figure 9: Cartpole (Mujoco simulations) learning effi-
ciency is suppressed when additional data not relevant
to the task is added to the dynamics model training set.
This effect is related to the issue of objective mismatch
because model training needs to account for potential
off-task data.
importance of careful exploration vs exploitation tradeoffs, or changing how models are trained to be
selective with data.
B Task Generalization in Simple Environments
In this section, we compare the perfor-
mance of a model trained on data for the
standard cartpole task (x position goal at 0)
to policies attempting to move the cart to
different positions in the x-axis. Fig. 10 is
a learning curve of PETS with a PE model
using the CEM optimizer. Even though
performance levels out, the NLL contin-
ues to decrease as the dynamics models
accrue more data. With more complicated
systems, such as halfcheetah, the reward
of different tasks verses global likelihood
of the model would likely be more inter-
esting (especially with incremental model
training) - we will investigate this in future
work. Below, we show that the dynamics
model generalizes well to tasks close to
zero (both positive in (Fig. 11b) and nega-
tive positions (Fig. 11a), but performance
■ Validation Error ■ Episode Reward
Figure 10: Learning curve for the standard Cartpole
task used in this paper (Xgoal = 0). The median reward
from 10 trials is plotted with the mean NLL of the
dynamics models at each iteration. The reward reaches
maximum (180) well before the NLL is at it’s minimum.
drops off in areas the training set does not cover as well.
Below the learning curves in Fig. 12, we include snapshots of the distributions of training data
used for these models at different trials, showing how coverage relates to reward in cartpole. It is
worth investigating how many points can be removed from the training set while maintaining peak
performance on each task.
11
Under review as a conference paper at ICLR 2020
Figure 11: MPC control with different reward functions with the same dynamics models loaded
from trials shown in Fig. 10. The cartpole solves tasks further from 0 proportional to the state
space coverage (Goal further from zero causes reduced performance). The distribution of x data
encountered is shown in Fig. 12.
300
ssouam。。O
-2	T	0	1	2
X Position
(a) x distribution after trial 1.
ssouam。。O
3 200
20
X Position
100--------------
50-------------
------------2
(b) x distribution after trial 5.
X Position
(c) x distribution after trial 20.
Figure 12: Distribution of x position encountered during the trials shown in Fig. 10. The distribution
converges to a high concentration around 0, making it difficult for MPC to control outside of the area
close to 0.
C Ways model mismatch can harm the performance of a
CONTROLLER
Model mismatch between fitting the likelihood and optimizing the task’s reward manifests itself in
many ways. Here we highlight two of them and in Sec. 6 we discuss how related work connects in
with these issues.
Long-horizon rollouts of the model may be unstable and inaccurate. Time-series or dynamics
models that are unrolled for long periods of time easily diverge from the true prediction and can
easily step into predicting future states that are not on the manifold of reasonable trajectories. Taking
these faulty dynamics models and using them as a smaller part of a controller that optimizes some
cost function under a poor approximation to the dynamics. Issues can especially manifest if, e.g., the
approximate dynamics do not properly capture stationarity properties necessary for the optimality of
the true physical system being modeled.
Non-convex and non-smooth models may make the control optimization problem challenging
The approximate dynamics might have bad properties that make the control optimization problem
much more difficult than on the true system, even when the true optimal action sequence is optimal
under the approximate model. This is especially true when using neural network as they introduce
non-linearities and non-smoothness that make many classical control approaches difficult.
12