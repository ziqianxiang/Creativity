Under review as a conference paper at ICLR 2020
Storage Efficient and Dynamic Flexible Run-
time Channel Pruning via Deep Reinforcement
Learning
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose a deep reinforcement learning (DRL) based framework
to efficiently perform runtime channel pruning on convolutional neural networks
(CNNs). Our DRL-based framework aims to learn a pruning strategy to determine
how many and which channels to be pruned in each convolutional layer, depend-
ing on each specific input instance at runtime. The learned policy optimizes the
performance of the network by restricting the computational resource on layers
under an overall computation budget. Furthermore, unlike other runtime pruning
methods which require to store all channels parameters for inference, our frame-
work can reduce parameters storage consumption for deployment by introducing
a static pruning component. Comparison experimental results with existing run-
time and static pruning methods on state-of-the-art CNNs demonstrate that our
proposed framework is able to provide a tradeoff between dynamic flexibility and
storage efficiency in runtime channel pruning.
1	Introduction
In recent years, convolutional neural networks (CNNs) have been proven to be effective in a wide
range of computer vision tasks, such as image classification (Krizhevsky et al., 2012; Simonyan &
Zisserman, 2015; He et al., 2016), objection detection (He et al., 2017; Zhou et al., 2019; Law &
Deng, 2018), segmentation (He et al., 2017; Zhu et al., 2019). Therefore, nowadays, many computer-
vision-based systems, such as automatic-driving cars, security surveillance cameras, and robotics,
are built on the power of CNNs. However, since most state-of-the-art CNNs require expensive
computation power for inference and huge storage space to store large amount of parameters, the
limitation of energy, computation and storage on mobile or edge devices has become the major bot-
tleneck on real-world deployments of CNNs. Existing studies have been focused on speeding up
the execution of CNNs for inference on edge devices by model compression using matrix decom-
position (Denil et al., 2013; Masana et al., 2017), network quantization (Courbariaux et al., 2016),
network pruning (Dong et al., 2017), etc. Among these approaches, channel pruning has shown
promising performance (He et al., 2017; Luo et al., 2017; Zhuang et al., 2018; Peng et al., 2019).
Specifically, channel pruning discards an entire input or output channel and keep the rest of the
model with structures.
Most channel pruning approaches can be categorized into two types: runtime approaches and static
approaches. Static channel pruning approaches aim to design a measurement to evaluate the im-
portance of each channel over the whole training dataset and remove the least important channels
to minimize the loss of performance after pruning. By permanently pruning a number of chan-
nels, the computation and storage cost of CNNs can be dramatically reduced when being deployed,
and the inference execution can be accelerated consequently. Runtime channel pruning approaches
have been recently proposed to achieve dynamic channel pruning on each specific instance (Gao
et al., 2019; Luo & Wu, 2018). To be specific, the goal of runtime approaches aims to evaluate the
channel importance at runtime, which is assumed to be different on different input instances. By
pruning channels dynamically, different pruned structures can be considered as different routing of
data stream inside CNNs. This kind of approaches is able to significantly improve the representation
capability of CNNs, and thus achieve better performance in terms of prediction accuracy compared
with static approaches. However, previous runtime approaches trade storage cost off dynamic flex-
1
Under review as a conference paper at ICLR 2020
ibility of pruning. To achieve dynamic pruning on different specific instances, all parameters of
kernels are required to be stored (or even more parameters are introduced). This makes runtime
approaches not applicable on resource-limited edge devices. Moreover, most of previous runtime
approaches only evaluate the importance among channels in each single layer independently, with-
out considering the difference in efficiency among layers.
In this paper, to address the aforementioned issues of runtime channel pruning approaches, we pro-
pose a deep reinforcement learning (DRL) based pruning framework. Basically, we aim to apply
DRL to prune CNNs by maximizing received rewards, which are designed to satisfy the overall
budget constraints along side with networkâ€™s training accuracy. Note that automatic channel pruning
by DRL is a difficult task because the action space is usually very huge. Specifically, the discrete
action space for the DRL agent is as large as the number of channels at each layer, and the action
spaces may vary among layers since there are different numbers of channels in different layers. To
facilitate pruning CNNs by DRL, for each layer, we first design a novel prediction component to
estimate the importance of channels, and then develop a DRL-based component to learn the sparsity
ratio of the layer, i.e., how many channels should be pruned.
More specifically, different from previous runtime channel pruning approaches, which only learn
runtime importance of each channel, we propose to learn both runtime importance and additionally
static importance for each channel. While runtime importance maintains the saliency of specific
channels for each given specific input, the static importance captures the overall saliency of the
corresponding channel among the whole dataset. According to each type of the channel importance,
we further design different DRL agents (i.e., a runtime agent and a static agent) to learn a sparsity
ratio in a layer-wise manner. The sparsity ratio learned by the runtime agent together with the
estimated runtime importance of channels are used to generate runtime pruning structures, while the
sparsity ratio learned by the static agent together with the estimated static importance of channels are
used to generate static (permanent) pruning structures. By considering both the pruning structures,
our framework is able to provide a trade-off between storage efficiency and dynamic flexibility for
runtime channel pruning.
In summary, our contributions are 2-fold. First, we propose to prune channels by taking both runtime
and static information of the environment into consideration. Runtime information endows pruning
with flexibility based on different input instances while static information reduces the number of
parameters in deployment, leading to storage reduction, which cannot be achieved by conventional
runtime pruning approaches. Second, we propose to use DRL to determine sparsity ratios, which
is different from the previous pruning approaches which manually set sparsity ratios. Extensive
experiments demonstrate the effectiveness of our method.
2	Related Work and Preliminary
2.1	Structure Pruning
Wen et al. (2016) pioneered structure pruning in deep neural network by imposing the L2,1 norm
in training. Under the same framework, Liu et al. (2017) regarded parameters in batch normaliza-
tion as channel selection signal, which is minimized to achieve pruning during training. He et al.
(2017) formulated channel pruning into a two-step iterative process including LASSO regression
based channel selection and least square reconstruction. Luo et al. (2017) formulated channel prun-
ing as minimization of difference of output features, which is solved by greedy selection. Zhuang
et al. (2018) further considered early prediction, reconstruction loss and final loss to select impor-
tance channels. Overall, structure pruning methods accelerate inference by producing regular and
compact model. However, this brought regularness requires preserving more parameters to ensure
performance.
2.2	Dynamic Pruning
Dynamic pruning provides different pruning strategies according to input data. Wang et al. (2018)
proposed to reduce computation by skipping layers or channels based on the analysis of input fea-
tures. Luo & Wu (2018) proposed to use layer input to learn channel importance, which is then
binarized for pruning. Gao et al. (2019) applied the same framework while extended features selec-
tion in both input and output features. Similarly, Liu & Deng (2018) introduced multiple branches
2
Under review as a conference paper at ICLR 2020
Figure 1: Illustration of our proposed DRL-based runtime pruning framework.
for runtime inference according to inputs. A gating module is learnt to guide the flow of feature
maps. Bolukbasi et al. (2017) learned to choose the components of a deep network to be evalu-
ated for each input adaptively. Early exit is introduced to accelerate computation. Dynamic pruning
adaptively takes different actions for different inputs, which is able to accelerate the overall inference
time. However, the original high-precision model needs to be stored, together with extra parameters
for making specified pruning actions. Rosenbaum et al. (2018) proposed to learn routers to route
layers output to different next layers, in order to adjust a network to multi-task learning.
2.3	Deep reinforcement learning in Pruning
Channel selection is on trial using deep reinforcement learning. Lin et al. (2017) trained a LSTM
model to remember and provide channel pruning strategy for backbone CNN model, which is con-
ducted using reinforcement learning techniques. He et al. (2018) proposed to determine the com-
pression ratio in each layer by training an agent regarding the pruning-retraining process as an envi-
ronment.
2.4	Preliminary
Reinforcement Learning We consider a standard setup of reinforcement learning: an agent se-
quentially takes actions over a sequence of time steps in an environment, in order to maximize the
cumulative reward (Sutton & Barto, 1998). This problem can be formulated as a Markov Deci-
sion Process (MDP) of a tuple (S, A, P, R, Î³), where S is the state space, A is the action space,
P : S Ã— A Ã— S â†’ [0, 1] is transition probabilities, R : S Ã— A â†’ R is the reward function, and
Y âˆˆ [0,1) is the discount factor. The goal of reinforcement learning is to learn a policy Ï€(aâˆ£s) that
maximizes the objective of cumulative rewards over finite time steps,
T
max
Ï€ t=0
R(st, at),
where st âˆˆ S and at âˆˆ A are state and taken action at time step t respectively.
3 DRL-based Runtime Pruning framework
The overview of our proposed framework is presented in Fig. 1. To prune convolutional layer t,
we learn two types of learnable channel importance: runtime channel importance ur âˆˆ RC Ã—1 and
static channel importance us âˆˆ RC Ã—1 , where C is the number of channels in layer t. The runtime
channel importance Ur is generated by a subnetwork importance predictor f (âˆ™), which takes the
input feature map Fin as input, while the static channel importance us is randomly initialized and
updated during training. Both ur and us indicate the channel importance of the full precision output
feature map Fout through a convolution layer. Channels are selected to be pruned according to the
values of each element in ur and us, and how many channels tobe selected is decided by the sparsity
ratios dr and ds, respectively. To learn the sparsity ratios dr and ds, two DRL agents, the runtime
agent and the static agent, are introduced, where actions atr and at3 * s are defined to set values of dr
and ds, respectively. The detail of the two DRL agents are described in Sec. 3.3. Consequently,
3
Under review as a conference paper at ICLR 2020
a trade-off pruner g(âˆ™) is performed to balance the runtime and static pruning results, and output
a decision mask M of binary values (1/0) to indicate which channels to be pruned (1: pruned, 0:
preserved), as well as a unified channel importance vector u âˆˆ RC Ã—1 as follows,
[M, u] = g(ur, us, dr, ds).	(1)
The final output after pruning is constructed by multiplying the full precision output feature map
Fout, by 1 - M and u as,
F out = Fout æ°§(1 - M)ä¹³ u,	(2)
where 0 is the broadcast element-wise multiplier, and 1 is the matrix of the same size as M with all
the elements being 1. In the following, we introduce how to learn the runtime channel importance
vector ur and the static channel importance vector us in Sec. 3.1, how to construct the trade-off
pruner g(âˆ™) in Sec. 3.2, and how to design the two DRL agents in Sec. 3.3.
3.1	Learnable Channel Importance
We consider that a convolutional layer takes input of feature map Fin âˆˆ RCinÃ—HinÃ—Win and gen-
erates an output feature map Fout âˆˆ RCout Ã—Hout Ã—Wout, where C*, H and W* are the number of
channels, width and height of the feature map F*, respectively. Each element of the channel impor-
tance vectors ur âˆˆ RCout and us âˆˆ RCout represents the importance value of the corresponding
channel, respectively. In the following, we drop the subscript out for simplicity in presentation.
3.1.1	Runtime Channel Importance
As mentioned above, the runtime channel importance ur of output feature Fout is predicted by a
importance predictor f (âˆ™), which takes Fin as input. Therefore, Ur can be considered as a function
of Fin , whose values vary over different input instances. In this paper, we design a subnetwork to
approximate f (âˆ™), which is expected to be of a small size and computationally efficient. Similar to
many existing dynamic network pruning methods (Gao et al., 2019; Hu et al., 2018; Luo & Wu,
2018), we use global pooling layer as the first layer in f (âˆ™), because global pooling is computation-
ally efficient and it can reduce the dimension of Fin dramatically. We then feed the output of global
pooling into a fully-connected layer without any activation function. The output of fully-connected
layer is the runtime channel importance vector ur .
Which channels to be preserved / pruned at runtime are determined according to the values of ur .
We denote by Mr âˆˆ {0, 1}C a mask for pruning, where if the value is 0, then the corresponding
channel is preserved, otherwise pruned. For now, suppose a sparsity ratio dr for runtime pruning has
already been generated via the dynamic DRL agent, which will be introduced in Sec. 3.3. We then
prune (C - ddrCe) channels with the smallest importance values in ur. Accordingly, the value of
an element in Mr is set to be 1 if the corresponding channel is pruned, otherwise 0.
3.1.2	Static Channel Importance
The static channel importance vector us is to capture the global information for pruning, and thus
is learned from the whole dataset. It is randomly generated and learned through backpropagation.
Similar to runtime channel pruning, given a sparsity ratio ds learned by the static DRL agent, (C -
ddsCe) channels with smallest importance values in us are pruned, and a mask Ms âˆˆ {0, 1}C is
generated to indicate the static pruning results.
3.2	Trade-off Pruner
With the runtime and the static pruning decisions, Mr and Ms, we now propose a trade-off pruner
to generate a unified channel pruning decision. The main idea behind the trade-off pruner is to 1)
prune those channels which are agreed to be pruned by both decisions, and 2) prune a portion of the
rest channels by weighted votes from both decisions.
To be specific, we define the mask representing channels pruned by both decisions as
Mo = Ms âˆ§ Mr,	(3)
where âˆ§ is element-wise logical AND and 1/0 in mask represents logical true or false. The channels
indicated to be pruned by Mo (i.e., the corresponding values are 1) are pruned in final. The channels
4
Under review as a conference paper at ICLR 2020
which are determined to be pruned by Mr but not by Ms can be represented by a new mask Mr =
Mr - Mo. Similarly, the channels which are determined to be pruned by Mr but not by Ms can be
represented by another new mask Ms = Ms - Mo.
To control the trade-off between Mr and Ms, we define a rate Rr denoting how much we trust the
pruning decision made by Mr, while 1 - Rr is for Ms. That means the channels selected by Mr
will be finally pruned with the rate Rr. Specifically, the number of channels which are selected by
Mr and finally will be pruned is
Cr = bRr (1>Mr )C,	(4)
where 1>Mr returns the number of channels selected by Mr. We then select the first Cr-smallest
important channels which are recommended to be pruned by Mr to form a mask Cr. Similarly,
for static pruning, we select the first Cs0 -smallest important channels which are recommended to be
pruned by Ms to form another mask cs, where Cs = [(1 - Rr )(1>Ms)C.
The final trade-off pruning mask is defined as
M = Mo + Mcr + Mcs .	(5)
Moreover, in this work, the unified channel importance is simply defined as follows,
U = Ur 0 Us.	(6)
With the trade-off pruning mask M and the unified channel importance u, the pruned output feature
Fout can be generated by Eq. 2.
3.3	DRL based pruning
In this section, we present how to formulate the problems of learning the ratios ds and dt for static
pruning and runtime pruning, as a MDP, and solve it via DRL, respectively.
3.3.1	DRL for runtime pruning
In the MDP for runtime pruning, we consider the t-th layer of the network as the t-th timestamp.
The details of the MDP are listed as follows.
State Given an input feature map Fin of layer t, we pass it to a global pooling layer to reduce its
dimension to RCin, where Cin is the number of input channel of layer t. Since Cin varies among
layers, we feed the output of global pooling to a layer-dependent encoder to project it to a fix-length
vector str, which is considered as as a state representation of DRL in the context of runtime pruning.
Action The action atr is defined as the sparsity ratio at layer t, alternating dr in runtime pruning
mentioned in Section 3.1.1. Existing DRL-base pruning method RNP (Lin et al., 2017) uses a unified
discrete actions space with k actions which are too coarse to achieve high accuracy. However, fine-
grained discrete action space as large as number of channels suffers from exploration difficulty.
Therefore, instead of using discrete action spaces, we propose a continuous action space with action
atr âˆˆ (0, 1]. To avoid over-pruning the filters and crashing in training, we set a minimum sparsity
ratio +Î± such that atr âˆˆ (+Î±, 1].
Reward The reward function is proposed to consider both network accuracy and computation
budget. We define the accuracy relative reward based on the loss of pruned backbone network,
Rracc = -LCNN,	(7)
where LCNN is the loss in CNN, and it may vary in scale among different training stage, i.e. large
at beginning of training and small near convergence. To avoid the instability brought by the reward
scale, Rarcc is normalized by a moving average,
Racc= Raccâˆ•Î²b,	(8)
Î²b = Î»Î²b-1 + (1 - Î»)Rracc,	(9)
5
Under review as a conference paper at ICLR 2020
where Î²b is the moving average at the b-th training batch and Î» is the moving weight.
To force computation of the pruned network under a given computation budget, we define a expo-
nential reward function of budget regarding reward Rbrud :
r
Rbud
JeXP(Î±Î¹ (Bcom - Bcom))-	1,
0,
Bcom > Bcom ,
otherwise,
(10)
where Bcom is the computation consumption, which is calculated based on the current of pruned
strategy, and Bcom is the given computation budget constraint. Finally We sum up the two rewards
to form sparse rewards, with being non-zero at terminated step T and zeros at other time step t < T ,
Rra0cc + Rbrud , t = T,
0,	t < T.
(11)
Actor-Critic Agent To solve the continuous action space problem, we choose a commonly used
actor-critic agent with a Gaussian policy. Actor-critic agent consists of two components: 1) actor
outputs the mean and variance to form a Gaussian policy where the continuous action are sampled
from; 2) critic outputs a scalar predicting the future discounted accumulated reward and assists the
policy training. Actor network and Critic network share one-layer RNN which takes state str as
input. The output of RNN is fed into actor specific network constructed by two branches of fully-
connected layers, leading to the mean and variance of the Gaussian policy. The action is sampled
for the Gaussian distribution outputed by the actor:
ar ã€œN(Î¼(srï¼› Î¸r),Ïƒ(sr; Î¸r)),	(12)
where Î¼(sâˆ–; Î¸r) and Ïƒ(s[; Î¸r) is the mean and variance outputed from actor network. The Critic
specific network has one fully-connected layer after the shared RNN, and outputs the predictive
value V (str ; Î¸r).
To optimize the actor-critic agent, ProXimal Policy Optimization (PPO) (Schulman et al., 2017) is
used. Note that we relaX the action atr to (-âˆž, +âˆž) in PPO, and use truncate function to clip atr in
(+Î±, 1] when perform pruning.
Besides, an additional regularizer is introduced to restrict the relaXed atr staying in range (+Î±, 1],
La = 1 ||ar - max(min(ar, 1), +Î±)âˆ£âˆ£2.	(13)
3.3.2	DRL for static pruning
Similar to runtime pruning, the MDP in static pruning is also formulated layer-by-layer. The differ-
ence against runtime pruning is the definition of state and reward.
State The state sts in static pruning is defined as the full shape of Fout, and does not depend on
Fout and the current input data.
Action Action ats is sampled from actorâ€™s outputed Gaussian policy, and it is to alternate the spar-
sity ds in static pruning mentioned in Section 3.1.2.
Reward The reward function takes both network accuracy and parameters budget into considera-
tion. The accuracy relative is defined as the same as that in runtime pruning:
0
Rsacc = Rarcc .
(14)
To reduce the number of parameters of network to satisfy the parameters storage budget, the param-
eters relative reward is defined in an exponential form as,
Rs
param
exp(Î±2(Bp
aram
0,
- Bparam )) - 1, Bparam > Bparam ,
otherwise,
(15)
where Bparam is the number of preserved parameters after static pruning and Bparam is the param-
eters storage budget.
6
Under review as a conference paper at ICLR 2020
Method	I Baseline acc.	Acc.	âˆ†acc.	Speed-up	#Params
FBS (Gao et al., 2019)	91.37	89.88	-1.49	3.93Ã—	1.11Ã—
RNP (Lin et al., 2017)	92.07	84.93	-7.14	3.56Ã—	1.00Ã—
ours (runtime only Rr =	äºŒ 1) 92.07	91.333	-0.737	3.92Ã—	1.31Ã—
ours (Rr = 0.5)	92.07	91.066	-1.004	3.92Ã—	0.78Ã—
Table 1: Comparison to state-of-the-art runtime pruning methods on CIFAR-10 at sparsity 0.5.
Speed-up is calculated on MACs.
Method	I Baseline acc.		Acc.	âˆ†acc.	Speed-up	#Params
FBS (Gao et al., 2019)		91.37	91.23	-0.14	2Ã—	1.11Ã—
ours (Rr	1.0)	92.07	93.178	+1.108	1.99Ã—	1.31Ã—
ours (Rr	0.5)	92.07	92.502	+0.432	1.99Ã—	0.97Ã—
Table 2: Comparison to state-of-the-art runtime pruning methods on CIFAR-10 at sparsity 0.7.
Speed-up is calculated on MACs.
Actor-Critic Agent This agent is similar to the one in runtime pruning. It has the same archi-
tecture as runtime pruning but differs in introducing a fully-connected layer as the encoder before
RNN. This agent is also optimized by PPO.
3.4	Inference
In inference, the static agent is not required any more because the static pruning strategy does not
depend on individual input data points but the full shape of Fin . Therefore, the output action ats is
fixed to each layer t. With the action ats and the rate Rr, we can decide which filters can be pruned
permanently. Specifically, channels with ((1 - ats)(1 - Rr))-smallest static importance values are
pruned permanently.
4 Experiment
We evaluate our DRL pruning framework on two popular datasets: CIFAR-10 (Krizhevsky, 2009)
and ImageNet ILSVRC2012 (Russakovsky et al., 2015), to show the advantage over other chan-
nel pruning methods. We analyze the effect of hyper-parameters and different sparsity settings on
CIFAR-10. For CIFAR-10, we use M-CifarNet (Zhao et al., 2018) as the backbone CNN. On Ima-
geNet ILSVRC2012, ResNet-18 is used as the backbone CNN.
4.1	Implementation details
We start with a pretrained backbone CNN. Firstly we finetune the backbone CNN and train runtime
importance predictor jointly, with sparsity dr = 1 and fixed all static pruning importance us to 1.
Then we remove the restriction on the static pruning importance us, and train static pruning impor-
tance, the backbone CNN and the runtime importance predictor, with sparsity ds = 1 and runtime
pruning sparsity fixed as dr = 1. After finetuning, we use the DRL agents to predict the sparsity
given computation and storage constraints. The DRL agents and the CNN with runtime/static im-
portance are trained in alternating manner: We first fix the CNN as well as runtime/static importance
and train two DRL agents, regarding the CNN as environments. Then we fix two agents and finetune
the CNN and runtime/static importance. We repeat these two steps until convergence is achieved.
We use Adam optimizer for both DRL agent and CNN, and set learning rate at 10-6 for the DRL
agents. For CNN finetuning and runtime/static importance training, the learning rate is set to 10-3
on CIFAR-10. On ImageNet ILSVRC2012, the learning rate starts from 10-3 and is divided by 10
after 15 millions iterations.
7
Under review as a conference paper at ICLR 2020
Figure 2: Trade-off between runtime pruning
and static pruning at sparsity 0.45. X-axis is
the rate Rr
Figure 3: Comparison accuracy drop for M-
CifarNet on CIFAR-10 with computational
budget.
Method	Baseline top-1 acc.	Top-1 acc.	âˆ† top-1 acc.	Baseline top-5 acc.	Top-5 acc.	âˆ† top-5 acc.	Speed-up
DCP (Zhuang et al., 2018)	69.64	67.35	-2.29	88.98	88.86	-0.12	1.71Ã—
FPGM (He et al., 2019)	70.28	68.41	-1.87	89.63	88.48	-1.15	1.71Ã—
Dynamic Sparse Graph (Liu et al., 2019)	69.48	64.8	-4.68	-	-	-	1.4Ã—
CGNN (Hua et al., 2018)	69.02	67.95	-1.07	88.84	88.21	-0.63	1.63Ã—
FBS (Gao et al., 2019)	70.71	68.17	-2.54	89.68	88.22	-1.46	1.98Ã—
Ours (Rr = 0.5)	69.758	68.79	-0.968	89.078	88.534	-0.544	1.94Ã—
Table 3: Comparison with the state-of-the-art channel pruning ResNet-18 on ImageNet. Speed-up
is calculated on MACs.
4.2 Experimental results on CIFAR- 1 0
We compare our proposed method with the following state-of-the-art runtime pruning methods:
FBS (Gao et al., 2019), RNP (Lin et al., 2017) on CIFAR-10. The comparison results at sparsity
0.5 and 0.7 are shown in Table 1 and Table 2 respectively. Note that for fair comparison with other
methods, the computation and storage budget constraints in our method is calculated according to
the sparsity of other methods. Under these constraints, our method does not necessarily lead to
the same sparsity as other methods in each layer. RNP cannot set exact sparsity ratio. Instead, its
average sparsity ratio is accessible only during testing, which is 0.537 in Table 1. The result of FBS
is reproduced using the released code1. The column #Params represents the number of parameters
compared to the backbone CNN.
Table 1 shows that our method outperforms other state-of-the-art methods, achieving highest accu-
racy at an overall sparsity ratio of 0.5. Our method has very close computation speed-up compared
to FBS, but outperforms FBS around 0.48% to 0.76%. When the runtime pruning strategy is solely
considered by setting Rr = 1, our method surpasses other comparison methods, indicating that our
DRL-based framework improves the performance of channel runtime pruning. By balancing run-
time and static pruning via setting Rr = 0.5, our method reduces the number of the overall stored
parameters and achieves lower accuracy drop than other methods. Table 2 shows that our method
outperforms FBS at sparsity of 0.7. When Rr = 0.5, our method achieves better performance than
the baseline CNN with 2Ã— speed-up and contains less parameters.
We also study the relation between Rr and network compactness in our framework. Fig. 2 demon-
strates the impact of Rr when sparsity is 0.45. The hyper-parameter Rr determines how much we
trust about runtime pruning. With Rr close to 1, the accuracy becomes higher due to the more
dynamic network flexibility but the space of the parameters storage also increases. When Rr dimin-
ishes, the network accuracy decreases but the parameter storage is reduced.
Fig. 3 shows the performance of various sparsity ratios in our method. Again, our method does not
prune with one single sparsity ratio for all layers, but uses the sparsity ratio to calculate computation
and storage constraints, with which the sparsity ratio is learned for each layer. Fig. 3 demonstrates
1https://github.com/deep-fry/mayo
8
Under review as a conference paper at ICLR 2020
that our method holds the accuracy when sparsity is larger than about 0.5, which corresponds to
about 4Ã— computational acceleration.
4.3 Experimental results on ImageNet ILSVRC20 1 2
We compare our method with state-of-the-art channel pruning methods on ImageNet ILSVRC2012
as shown in Table 3. In this experiment, we use ResNet-18 as the backbone CNN. Among the state-
of-the-art pruning methods for comparison, FBS (Gao et al., 2019) and CGNN (Hua et al., 2018)
are runtime pruning methods. The overall sparsity ratio of our method is 0.7, which is under the
same setting of FBS. Our method with Rr = 0.5 achieves the smallest top-1 accuracy drop com-
pared with other methods, and also achieves the highest top-1 accuracy after pruning. Overall, our
proposed method achieves comparable or better performance compared to other methods with more
acceleration. Our method has very close MACs to FBS, while the number of preserved parameters
is reduced to 81.2% of the baseline.
5 Conclusion
In this paper, we present a deep reinforcement learning based framework for deep neural network
channel pruning in both runtime and static sheme. Specially, channels are pruned according to input
feature as runtime pruning, and based on entire training dataset as static pruning, with 2 reinforce-
ment agents to determine the corresponding sparsity. Our method combines the merits of runtime
and static pruning, and provides trade-off between storage and dynamic flexibility. Extensive exper-
iments demonstrate the effectiveness of our proposed method.
References
Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama. Adaptive neural networks for
efficient inference. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70,pp. 527-536. JMLR. org, 2017.
Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks: Training deep neural networks with weights and activations constrained to+ 1
or-1. arXiv preprint arXiv:1602.02830, 2016.
Misha Denil, Babak Shakibi, Laurent Dinh, Marcâ€™Aurelio Ranzato, and Nando de Freitas. Predicting
parameters in deep learning. In Advances in Neural Information Processing Systems 26, pp. 2148-
2156. Curran Associates, Inc., 2013.
Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise
optimal brain surgeon. In Advances in Neural Information Processing Systems, pp. 4857-4867,
2017.
Xitong Gao, Yiren Zhao, ukasz Dudziak, Robert Mullins, and Cheng zhong Xu. Dynamic channel
pruning: Feature boosting and suppression. In International Conference on Learning Represen-
tations, 2019. URL https://openreview.net/forum?id=BJxh2j0qYm.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, June 2016. doi:
10.1109/CVPR.2016.90.
K. He, G. Gkioxari, P. Dollr, and R. Girshick. Mask r-cnn. In 2017 IEEE International Conference
on Computer Vision (ICCV), pp. 2980-2988, Oct 2017. doi: 10.1109/ICCV.2017.322.
Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median
for deep convolutional neural networks acceleration. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 4340-4349, 2019.
Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural net-
works. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1389-1397,
2017.
9
Under review as a conference paper at ICLR 2020
Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model
compression and acceleration on mobile devices. In Proceedings of the European Conference on
Computer Vision (ECCV),pp. 784-800, 2018.
Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Weizhe Hua, Christopher De Sa, Zhiru Zhang, and G Edward Suh. Channel gating neural networks.
2018.
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep
convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q.
Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp. 1097-
1105. Curran Associates, Inc., 2012. URL http://papers.nips.cc/paper/
4824-imagenet-classification-with-deep-convolutional-neural-networks.
pdf.
Hei Law and Jia Deng. Cornernet: Detecting objects as paired keypoints. In The European Confer-
ence on Computer Vision (ECCV), September 2018.
Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In I. Guyon, U. V.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in
Neural Information Processing Systems 30, pp. 2181-2191. Curran Associates, Inc., 2017. URL
http://papers.nips.cc/paper/6813- runtime-neural-pruning.pdf.
Lanlan Liu and Jia Deng. Dynamic deep neural networks: Optimizing accuracy-efficiency trade-offs
by selective execution. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Liu Liu, Lei Deng, Xing Hu, Maohua Zhu, Guoqi Li, Yufei Ding, and Yuan Xie. Dynamic sparse
graph for efficient deep learning. In International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=H1goBoR9F7.
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learn-
ing efficient convolutional networks through network slimming. In Proceedings of the IEEE
International Conference on Computer Vision, pp. 2736-2744, 2017.
Jian-Hao Luo and Jianxin Wu. Autopruner: An end-to-end trainable filter pruning method for
efficient deep model inference. CoRR, abs/1805.08941, 2018. URL http://arxiv.org/
abs/1805.08941.
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural
network compression. In The IEEE International Conference on Computer Vision (ICCV), Oct
2017.
Marc Masana, Joost van de Weijer, Luis Herranz, Andrew D. Bagdanov, and Jose M. Alvarez.
Domain-adaptive deep network compression. In The IEEE International Conference on Computer
Vision (ICCV), Oct 2017.
Hanyu Peng, Jiaxiang Wu, Shifeng Chen, and Junzhou Huang. Collaborative channel pruning for
deep networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pp. 5113-5122, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection of
non-linear functions for multi-task learning. In International Conference on Learning Represen-
tations, 2018. URL https://openreview.net/forum?id=ry8dvM-R-.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-
Fei. Imagenet large scale visual recognition challenge. International Journal of Computer Vi-
sion, 115(3):211-252, Dec 2015. ISSN 1573-1405. doi: 10.1007/s11263-015-0816-y. URL
https://doi.org/10.1007/s11263-015-0816-y.
10
Under review as a conference paper at ICLR 2020
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/
1707.06347.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In International Conference on Learning Representations (ICLR), 2015.
Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press,
Cambridge, MA, USA, 1st edition, 1998. ISBN 0262193981.
Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learning dy-
namic routing in convolutional networks. In Proceedings of the European Conference on Com-
Puter Vision (ECCV),pp. 409-424, 2018.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in
deep neural networks. In Advances in neural information Processing systems, pp. 2074-2082,
2016.
Yiren Zhao, Xitong Gao, Robert Mullins, and Chengzhong Xu. Mayo: A framework for auto-
generating hardware friendly deep neural networks. In Proceedings of the 2Nd International
WorkshoP on Embedded and Mobile DeeP Learning, EMDLâ€™18, pp. 25-30, New York, NY, USA,
2018. ACM. ISBN 978-1-4503-5844-6. doi: 10.1145/3212725.3212726. URL http://doi.
acm.org/10.1145/3212725.3212726.
Xingyi Zhou, Dequan Wang, and PhiliPP Krahenbuhl. Objects as points. In arXiv preprint
arXiv:1904.07850, 2019.
Yi Zhu, Karan Sapra, Fitsum A. Reda, Kevin J. Shih, Shawn Newsam, Andrew Tao, and Bryan
Catanzaro. Improving semantic segmentation via video propagation and label relaxation. In
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.
Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou
Huang, and Jinhui Zhu. Discrimination-aware channel pruning for deep neural networks. In
Advances in Neural Information Processing Systems, pp. 875-886, 2018.
A Additional Experimental Results
A. 1 Comparison to S eparately Static and Runtime Pruning
In this section, we compare our method with two additional baseline methods. One is a variation of
our method by separately training static pruning and the training runtime pruning. In this method,
We start from a pretrained backbone CnN, f (âˆ™) and uÂ§. Then We add the static DRL agent to prune
channels statically by learning the static policy and us . Finally, we add the runtime DRL agent
to prune channels dynamically, by fixing the static DLR agent and us, and updating the runtime
DLR agent and f (âˆ™) only. Another method is to combine state-of-the-art static and runtime pruning
methods. We start from a pretrain backbone CNN, and then prune channels With the static pruning
method FPGM (He et al., 2019), and finally prune channels With the runtime method FBS (Gao
et al., 2019). The experimental results are shoWn in Table4.
Method	Baseline acc.	Acc.	âˆ†acc.
ours (runtime only Rr = 1)	92.07	91.333	-0.737
ours (Rr = 0.5)	92.07	91.066	-1.004
ours (separate static and runtime)	92.07	90.965	-1.0105
FPGM+FBS		92.07	90.456	-1.614
Table 4: Comparison to methods With separately static and runtime pruning on Cifar-10 at sparsity
0.5.
11
Under review as a conference paper at ICLR 2020
A.2 Storage/accuracy Trade-off
Besides Fig. 2, to further illustrate the trade-off between storage and accuracy, we show additional
results in Fig. 4.
Figure 4: Trade-off between runtime pruning and static pruning at sparsity 0.45. X-axis is stor-
age(number of parameters), Y-axis is accuracy.
B	Pseudocode of Training Process
Algorithm 1: Training process
INPUT: pretrained backbone CNN, computation budget Bcom, storage budget Bparam
OUTPUT: backbone CNN, importance predictor f (âˆ™), static pruning importance us, runtime and
static DRL agents
Add runtime importance predictor f (âˆ™) and static pruning importance uÂ§.;
Us â€” 1ï¼›
ds â€” 1;
dr iâ€” 1;
while not converge do
I fix Us, update f(âˆ™) and backbone CNN;
end
while not converge do
I update Us, f(âˆ™) and backbone CNN;
end
add runtime DRL agent and static DRL agent to predict actions atr and ats, and alternate dr and ds
at each layer t ;
while not converge do
for i â€” 1 to N do
Forward entired model;
Compute rewards Rr and Rt using budget Bcom and Bparam ;
Fix Us, f (âˆ™) and backbone CNN, update runtime and static DRL agents by PPO loss;
end
for i â€” 1 to N do
I Fix runtime and static DRL agents, update us, f (âˆ™) and backbone CNN by cross entropy
I loss
end
end
12
Under review as a conference paper at ICLR 2020
Figure 5: Training curve of our method on CIFAR-10 at sparsity 0.5 with Rr = 0.5.
C Training curve
We show the training curve of our method in Fig. 5. We train our method on CIFAR-10 at sparsity
0.5 with Rr = 0.5. The figure includes the accuracy curves evaluated on training set and test set,
showing that our methods is stable during training.
13