Under review as a conference paper at ICLR 2020
Model Architecture Controls Gradient De-
scent Dynamics: A Combinatorial Path-Based
Formula
Anonymous authors
Paper under double-blind review
Ab stract
Recently, there has been a growing interest in automatically exploring neural
network architecture design space with the goal of finding an architecture that
improves performance (characterized as improved accuracy, speed of training, or
resource requirements). However, our theoretical understanding of how model
architecture affects performance or accuracy is limited. In this paper, we study the
impact of model architecture on the speed of training in the context of gradient
descent optimization. We use the ideas from prior work that shows gradient
descent can be modeled as a first-order ODE and use ODE’s coefficient matrix H
to characterize the convergence rate. We introduce a simple analysis technique
that enumerates H in terms of all possible “paths” in the network. We show that
changes in model architecture parameters reflect as changes in the number of paths
and the properties of each path, which jointly control the speed of convergence. We
believe our analysis technique is useful in reasoning about more complex model
architecture modifications.
1	Introduction
Gradient descent and its variants are the cornerstones of deep learning. The theoretical properties
of gradient descent have been widely studied in the literature; study of the convergence bounds and
guarantees [6, 12, 15, 13], the characterization of the local geometry of stationary points [21, 11, 10,
19, 25, 30, 24, 23, 13], exploration of better algorithms to optimize the descent process [28, 16, 26,
33, 7] are just a few example of active research areas in this domain. Another major research area
is the exploration of network architecture and its impact on performance. In recent years, network
architecture search and design have shown major performance boosts in various aspects of deep
learning [36, 35, 18, 8, 34].
Recent theoretical results are trying to relate model architecture and gradient descent properties. It has
been shown that over-parametrization in width guarantees convergence in deep neural network [15].
It has also been shown that increasing depth has an impact similar to adding momentum optimization
and adaptive learning rate to the objective function [4].
The gradient descent process can be described by a system of first-order differential equations in
the continuous-time limit, in the form of X(t) = H(t)x(t). In this paper, We study the properties
of the coefficient matrix H(t), which governs the dynamics of the gradient descent. Specifically,
We formulate H(t) in terms of all possible paths in the netWork. This representation is poWerful
as it enables us to analyze the gradient behavior symbolically and abstracts aWay the unnecessary
bookkeeping of derivatives and Weights. Specifically, We use this representation to explore the
folloWings. (1) We study the impact of Width on convergence. Using a simple path counting
argument, We shoW that H(t) is a sum of m i.i.d. terms Where m is dictated by the Width of the
netWork. This has been implied in the Work of Simon Du, et al. [12] for the special case of a
2-layer RELU-activated netWork. We contrast the ease of arriving at such conclusions using our
representation against methods used in previous Work [12, 15, 13, 6]. (2) We study the impact of
depth on convergence. While prior Work [3] established the relationship betWeen momentum and
depth, We shoW a direct relationship betWeen depth-induced momentum and the scale of the output
value. We also discuss hoW depth-induced momentum is different than explicit momentum.
1
Under review as a conference paper at ICLR 2020
(3)	We argue why the number of paths is more important than the number of nodes in a network.
2	Related Work
Related work can be categorized into three groups which we will explain in detail below. In general,
the common theme across previous works is to show that if some conditions, such as bound on
models’ width or depth or assumptions on geometrical properties of the loss landscape are met,
gradient descent or its variance converges to a global minimum.
Landscape Analysis Researchers have extensively studied the properties of error surface in linear
neural network ([10, 21]) and have proved convergence to a global minimum for non-convex opti-
mizations if error surface shows certain properties, such as all saddle points to be strict (i.e. there
exists a negative curvature)([11, 19, 25, 30, 24, 23, 13]). However, it has been shown that the strict
saddle property is not guaranteed for deep neural networks.
Width Analysis A number of recent work have characterized minimum width that guarantees
convergence in terms of model architecture parameters and input data. We on the other hand study
how convergence rate varies with width. Examples of finding width bounds include [15] where
it has been shown that for a 2-layer ReLU-activated neural network with squared loss, as long as
every hidden layer is wide enough, gradient descent converges to a global minimum at a linear rate.
In [14, 1], it is extended to fully connected linear networks and ResNets. Du et. al. [12] has also
generalized this work to an L-layer fully connected linear neural network and shows the convergence
rate as a function of depth, output dimension and least eigenvalue of the Gram matrix for large enough
hidden layers. Ardalani et. al. [2] have shown empirically that for a wide range of RNN applications,
increasing width will reduce the number of steps to minimum validation loss. Neural tangent kernel
[22] studies convergence in the infinite width limit.
Depth Analysis Bartlett et al. [6] have proven for a deep linear neural network with isotropic input
and identity initialization that the number of steps to -proximity of the best answer is polynomial in
the number of layers. Neural ODE studies the convergence in the infinite depth limit [27, 9, 20, 17].
Work that motivated our depth analysis are [3, 4]. They have shown that over-parametrization
accelerate training with these extra restrictions: (1) linear neural network with (2) fully-connected
layers and (3) "balanced" initialization where (4) dimensions of hidden layers are at least the minimum
of the input and output dimensions. We contribute in the following ways. We characterize depth-
induced momentum in value space with an inequality and compare with explicit gradient descent
momentum. We observe that momentum is directly impacted by value of the sub-network. We lift
the structure and activation restrictions.
3	Notations and Conventions
Matrix entries are denoted with row and column indices, for instance H = (Hij ) is a matrix with
entries Hij. When H is diagonalizable, we denote by λmin(H) its smallest eigenvalue. For a
matrix W , W (i, :) and W (:, j) denote the row vector in the i-th row and the column vector in
the j-th column, respectively. Write Pw∈W for summing over all elements of W . We use pairs
{Xi, yi}, 1 ≤ i ≤ N to represent labeled data where Xi is input and yi is the output. We write xi,k
for the k-th component of Xi . We denote loss by L. We work with feed-forward neural networks,
which we sometimes denote abstractly by a function f, or f (w, x) as a function of input x and
weights w. We denote network weights individually by wi. Predictions are denoted by ui = f(w, Xi)
1 ≤ i ≤ N. The prediction on all inputs is denoted by the vector u. lp loss is defined as P(y - ui)p
for even integers p ≥ 2. We use angle brackets to denote inner products, i.e. h-→v , -→w i = Pi viwi
represents inner product of two vectors. i.i.d. random variables stands for independent and identically
distributed random variables. ODE stands for ordinary differential equations. NN stands for neural
networks. GD stands for gradient descent.
4	Background
We build upon prior work [15, 12, 14, 3, 31] which take gradient descent to a continuous time limit
and derive the Gram-matrix H governing gradient descent dynamics. In this section, we give a quick
derivation for Equation (4) for completeness.
2
Under review as a conference paper at ICLR 2020
Convergence rate can be characterized as the rate of change in loss during the training process. If
steps are infinitesimally small, the rate of change in loss can be characterized as:
dL
dt
XN ∂L dui
∂u dt
i=1
(1)
Where ui(t) = f (w(t), Xi) is the prediction on input Xi at time t. For simplicity we focus on the
dynamics of 鬻,which can be expanded as follows:
dui
dt
m ∂ui dwr
∂w ∂Wr dt
r=1
Nm
∂ui ∂uj
-Q"砧 ∂Wr
j=1 r=1 r r
mN
∂ui	∂L ∂uj
∂w ∂Wr	n@Uj ∂Wr
=1	j=1
∂L) XX , XX ∂ui ∂uj) ∂L
∂uj	η ɪʧ ∂w ∂wr ∂wr ∂uj
(2)
We use the definition of gradient descent and chain rule to derive the equation above. Gradient
descent at infinitesimal small step is defined by:
dWi	∂L	N ∂L ∂uj
Ir = -η∂Wi = -% 西∙ ∂Wi
(3)
-∂→L
Observe that ∂Lisa COlUmn vector. Hence,if We define Hijas:
H
Hij
m
X
r=1
∂Wr ∂Wr
(4)
Then we can simplify eqUation (2) in a vector form as:
d duι ∖
dt
（熟、
-ηH ∙
(5)
du N
'"ɪ )
∂L
∖∂un /
AssUming L is l2 loss, eqUation (5) simplifies to the following. This resUlt will hold Up to a constant
for other lp losses (see Appendix A).
dt = ηH(y - U)
(6)
d(y - U)
dt
du = -ηH Yy - U)
(7)
—
Notice that this pUts y - u in a system of differential eqUations whose single variable analogUe is:
df (t)
dt
hf (t)
(8)
The above eqUation’s solUtion is:
f(t) = c0eht = c0(1 - ξ)t
(9)
when h < 0, eh < 1 and let eh = 1 - ξ. ξ governs the convergence rate in EqUation (9). In the
actUal system of eqUations (7), ξ will be determined by the minimUm eigenvalUe of H(t), denoted
λmin (H (t)).
5	Main Idea: Characterizing H in terms of Network Paths
In order to state the path decomposition formUla, we first establish some notations.
• We denote the neUral network abstractly with a fUnction f(X).
3
Under review as a conference paper at ICLR 2020
•	For any neural network, we view the underlying network as a directed graph where edges are in
the backward propagation direction.
•	We define path p, as a series of connected nodes and directed edges. We define an output path as
a path that starts with the output node. We denote the length of p by l(p).
•	We index the nodes as follows. Index the output node by 0, the one after by 1, and so on. We
denote the j -th node in the i-th path by pij .
•	We allow different activation functions at nodes. We denote the activation function at ps by σs .
•	An example path that ends in an input node may look like this:
Θ^Θ 一
Figure 1: Illustration of a multilayer perceptron
•	The post-subnetwork at ws is defined as the network formed by the subgraph rooted at the right
vertex of ws. The pre-subnetwork at ws is defined as the union of all output paths ending in ws,
denoted by pre(ws).
•	For every input Xi, we denote the activation value at the node after ws with f [ws](Xi). Note
that this is equivalent to the value at the right vertex of ws once Xi propagated through the
post-subnetwork at ws .
•	We denote the local gradient wrt. input at node s along the path p with τpi .
•	Given the pre-subnetwork at Ws, We write the explicit formula for ∂dUi:
∂u
∂Ws
f [ws](Xi) ∙	E	path_gradientp(ws,Xi)
p∈pre(ws )
(10)
where path_gradientp(ws, Xi) is the gradient value along the path p at the left vertex of ws, and
can be expanded as follows using the chain rule:
path_gradientp(ws,Xi) = (τ0iw0τpi1w1 . . . τpil(p)-1 wl(p)-1)(p)	(11)
we refer to path_gradientp(ws, Xi) as PGp(ws, Xi) for brevity from now on.
Theorem 5.1. For a network f,
Hij = X (f [w](Xi) ∙ f [w](Xj) ∙ ( X PGp(w,Xi))∙ ( X PGp(W,Xj)))	(12)
w	p∈pre(w)	p∈pre(w)
which easily follows by substituting Equation (10) in Equation (4). In the rest of the paper, we will
make many arguments most of which rely on the number of terms in this equation. Number of terms
in Equation 12 is itself controlled by the number of paths in the pre-subnetworks at each weight.
Following examples provide some intuition.
Example 5.2. In this example, our network has a structure as illustrated in Figure 2a. The output
node is denoted with value one, indicating that the activation function at output layer is an identity
function. As shown in Figure 2b, there are only one path in each weight’s pre-subnetwork. Therefore,
there are only three terms in Equation 12, where each summand is of form PGp (w, Xi)PGp(w, Xj).
We refer to these terms as symmetrical pairs since both multipliers are representing the same path.
Example 5.3. A more complicated application of the formula can be found in Figure 2d. There is
one path in each of the following weights’ presubnetwork: w0,w1,w2,w3, and w4, and three paths
at w5 ’s presubnetwork. Therefore, Equation 12 would have 5 + 32 = 14 summands, 8 of which are
terms with symmetrical pairs and 6 are cross-terms induced by the paths in w5 ’s pre-subnetwork. In
general, cross-terms will appear only if there are two paths that share a final edge which can only
happen if there are three or more number of layers in the network.
4
Under review as a conference paper at ICLR 2020
(a) Example 5.2
pre(w0)
pre(w1)
pre(w2)
(c) Example 5.3
pre(w4)
(b) Paths Example 5.2
(d) Paths Example 5.3
Figure 2: Path Decomposition Examples.
6 Architecture Implications
6.1	Convergence Rate Scales Linearly with Width
In this Section, we show that for sufficiently wide neural networks, convergence rate has a linear
relationship with the network’s width. Results are drawn theoretically at time 0 assuming random i.i.d
initialization. For these results to generalize across time, we assume that H stays close to its initial
value. In the Appendices D and E, we show empirical results that suggest this assumption is true.
For the extreme case of infinitely-wide networks, it has been shown that H is constant throughout
training [22]. Others [5, 15] have also shown that for sufficiently-wide neural networks weights
remain close to initialization, and so our argument holds true across training.
Proposition 6.1. For a 2-layer neural network with hidden dimension m, the expected convergence
rate scales linearly with m.
To demonstrate the simplicity and expressiveness of our approach, next we will discuss proofs with
and without our approach.
Brute-force approach A 2-layer fully-connected linear neural network can be modeled as:
w11	... w1d	x1
U(X) = V X W X X = (vι …Vm) X .	. I ×	. I	(13)
wm. 1 ... wm.d	x.d
where V ∈ R1,m and W ∈ Rm,d are the weights of the second and first layer, respectively and the
input X is a d-dimensional column vector. We begin the analysis with the simple case of a 2-layer
linear network where the network's width is one, i.e. m = 1 and u(X) = vo ∙(W(:, 1), X).
Hij can be decomposed into two terms, partial derivatives with respect to weights of the first layer
and partial derivatives with respect to weights of the second layer.
Hij =黑duj + X ⅛⅛ = hWc,1),χii ∙hw(:，1)，Xji + voXχi,k ∙ χj,k	(14)
0	0	w∈W	k=1
For a more general case where the width is m, we have:
md
Hij = X hW(:, l), Xii ∙hW(:, l), Xji + v2 X Xi,k ∙ Xj,k	(15)
l=1	k=1
5
Under review as a conference paper at ICLR 2020
Adding non-linearity would make this equation even more complex, which is the approach used in
previous literature together with induction:
md
Hij = X σ(hWC,l),Xii)∙σ(hW(：,l),Xji)+v22 X Xi,k∙σ0(hW(:, l), Xii)∙Xj,k∙σ0(hW(:, l), Xj〉)
l=1	k=1
(16)
See Appendix B for details of how we derive these equations. Since we assume all weights (wlk, v0)
are independent and identically distributed, the summands within Equations (15) and (16) are also
independent and identically distributed. This indicates that Hij is composed of m i.i.d components,
hence convergence rate scales linearly with m.
Our approach: Using Theorem 5.1 More generally, by applying Theorem 5.1, it is immediate
to see every pre-subnetwork will be a path in a 2-layer fully connected network. There are two
types of paths; m length 1 paths and m length 2 paths. The weights are independent and follow the
same distribution, hence, a 2-layer network’s H matrix always decomposes as m i.i.d sums. Hence
convergence rate scales linearly with m. This proves Proposition 6.1.
See Section 7 for empirical analysis that motivated this study. We show for a 2-layer linear and
non-linear networks that convergence rate has linear relationship with width.
6.2	Depth-induced First-Order Momentum
In this section, we use our formula to explain how depth has a momentum-like effect. Take a network
f. Construct a new network g by adding a multiplication layer on top, i.e. g(X) = μf (X) with an
extra trainable parameter μ. We first present an inequality (with the proof left to Appendix F), then
explain why the inequality implies momentum-like behavior and finally how they are different.
Proposition 6.2. With Ug = μu f,
s( ^Ug |u , U ) ≥ cos
' dt lug, g 一
|uf , uf )
(17)
where cos denotes cosine between two vectors.
Remark 6.3 (Momentum interpreted as directional preference). In physics, momentum is defined
as the tendency of an object to follow its current velocity of motion. In gradient based optimization,
momentum can be defined as in Equation (1) of [32],
vt+1 = m0Vt - €▽/(Wt)	(18)
where v’s are the updates, is the learning rate and m0 is the momentum coefficient. Quantitatively,
cos(vt+1, vt) increases with m0. A bigger m0 implies a stronger preference in vt+1 for the direction
of vt. For network depth, Proposition 6.2 describes the stronger directional preference of the deeper
network g in the direction of the current value vector. In the depth-induced momentum experiment
below, we show that this directional preferences can have similar effects on convergence rates as
momentum in gradient based optimization.
Remark 6.4 (Difference to momentum). The key difference between depth-induced momentum and
gradient descent momentum is the following. The depth-induced momentum is a condition between
U0 and U, whereas gradient descent momentum is a second order condition between U00 and U0 (see
[29] Eq. (4) and (10)).
Remark 6.5 (Sensitivity to output scale). Since hu0,u)= 2S〈u,u)= 2£∣∣uk2, depth-induced
momentum will be more pronounced when kUk can increase. In Appendix G, we present an experiment
that shows that depth-induced momentum is more pronounced when output scale is larger.
6.3	On the Importance of Number of Paths for Convergence
Here, we will provide some heuristic arguments, based on our formula, on why over-parametrization
through increasing the number of paths can be more effective than increasing the number of nodes (pa-
rameters) and conclude that increasing depth is more effective than increasing width for accelerating
the training process. We first introduce an important terminology which we will use below.
Definition 6.6. Given a collection of vectors {gi}, the Gram matrix G is defined as a matrix whose
ij -th entry is hgi, gji under the Euclidean inner product.
6
Under review as a conference paper at ICLR 2020
Remark 6.7. Based on the definition above it is easy to see that Gram matrix is symmetric and
positive-semidefinite and its rank is exactly the dimension of the subspace spanned by {gi}. Suppose
H = H + •…+ Hn are all Gram matrices. Then H is positive definite ifany ofthe Hi is positive
definite. This is because of supperadditivity of the minimum eigenvalue function, i.e.
≥
λmin (HI) + …+ λmin(Hn).
λmin(H)
Strongly-Gram vs. Weakly-Gram Equation(12) in Theorem 5.1 can be expanded further as follows
and the terms can be classified into two groups. The first group will include all the symmetrical pairs
and the second group will include all the cross-terms.
Hij(f) = X X X PGp(w,Xi)f[w](Xi))∙( X	PGp(w,'Xj)fH(Xj))
w	p∈pre(w)	p∈pre(ws )
=XX PGp(w,Xi )f [w](Xi) ∙ PGp(w,Xj )f [w](Xj)+	(19)
w p∈pre(w)
X X	PGpi (w,Xi)f [w](Xi) ∙ PGp2(w,Xj)f [w](Xj)
w p1 ,p2 ∈pre(w)
p1 6=p2
=X PGp(w,Xi)f [w](Xi) ∙ PGp(w,Xj)f [w](Xj)+
p
X X	PGpi (w,Xi)f [w](Xi) ∙ PGp2(w,Xj)f [w](Xj)
w p1 ,p2 ∈pre(w)
pi 6=p2
=HiSj(f)+HiWj (f)	(20)
We refer to the first group as strongly Gram, and denote it by HiSj (f), and refer to the second group
as weakly Gram and denote it by HiWj (f), i.e. we have:
Remark 6.8 (Gramness of the strongly-Gram component). Suppose there are q paths in the network,
p1,p2, . . . , pq. Then Equation (19) implies that the strongly Gram component is Gramian with vectors
gi = PGpi(w, Xi)f[w](Xi),	...	, PGpq (w, Xi)f [w](Xi)	(21)
Remark 6.9 (Insignificance of the weakly Gram component). We claim that the weakly Gram
component is insignificant, see Appendix C for heuristics and empirical evidence.
Remark 6.10 (Number paths vs. number of nodes). The Gram matrix HS should ideally be of full
rank for good convergence. So we want the subspace spanned by {gi } to have dimension at least
N . N is the dimension of H and also the number of sample points. On the other hand, as shown in
Equation 21 gi ’s are embedded as vectors in a q-dimensional vector space where q is the number of
paths.Therefore, to ensure H is full-rank, q should satisfy q ≥ N. The bigger q is, the more likely
they are linearly independent and make the matrix full rank. In deep learning, N is usually large,
therefore, we need to have networks whose number of paths is comparable to N for best convergence.
Remark 6.11 (Depth vs. width). Adding a new parameter at a new layer can increase the number
of paths more effectively than adding it at an existing layer. In other words, making models deeper
increases the number of paths faster than making models wider.
7 Experiments
In this section, we provide experimental results that demonstrate the impact of model structure on
convergence.
Setup We employ a teacher-student framework to put our ideas to test. Our teacher model is a
two-layer neural network with hidden dimension 100, input dimension 50 and output dimension 1.
We assume a Gaussian distribution both for the inputs and ground-truth weights. The student models
are also two-layer neural networks with variable hidden dimensions, and l2 loss. The convergence
rate at step t is defined as 1 -
Losst+1
Losst
7
Under review as a conference paper at ICLR 2020
#Steps (#Gradient Updates)
(a) Training Curve
g~ J J J J
(3-eus 6OD sso-∣ 6U-U-EU
≡-BH ①ɔu跖」①>uo□
1.0025E+00
1.0020E+00
1.0015E+00
1.0010E+00
1.0005E+00
1.0000E+00
9.9950E-01
0	500	1000	1500	2000	2500	3000
Model's Width
(b) Convergence Rate
Figure 3: Training Curve Characterization for Two-layer Linear Neural Network.
zJ/y
(3-eus 6OD sso-∣ 6u_u_e」
“点S	∙⅛	©	夕产
#StePS (#Gradient Updates)
(b) Convergence Rate
Figure 4: Training Curve Characterization for Two-layer Non-Linear Neural Network.
(a) Training Curve
Two-Layer Linear Neural Network As shown in Figure 3a, training curve in log scales look like
straight lines. As shown in Figure 3b, convergence rate (λ) grows linearly with width.
Two-Layer Non-Linear Neural Network As shown in Figure 4a, training curve for models with
the sigmoid non-linearity begins in a steep region where loss drops very quickly for a short period of
time and ends in a steady region where loss drops very slowly. We are interested in characterizing the
curve in the steady region. As shown in Figure 4b, convergence rate (λ) grows linearly with width.
Depth induced momentum Figure 5 compares the convergence rate curves for 2-layer, 3-layer and
4-layer NN using GD, against a 2-layer network with a momentum optimizer. The deeper networks
are constructed from 2-layer networks with additional scalar multiplication layers. As discusses
in Section 4.5, one can observe depth-induced momentum in convergence rates, similar to using
momentum optimizer with a 2-layer NN.
8	Conclusion
In this paper, we provide a path-based combinatorial formula for the coefficient matrix H which
governs the gradient descent dynamics. We advocate analyzing H with this path-based approach by
showing the insights we could derive from this.
OleH 8UΘ6J∂AUOO
hidden dim IOOO
hidden dim 2000
hidden dim 1000
momentum 0.2
hidden dim 1000
momentum 0.5
hidden dim 1000
momentum 0.8
hidden dims 1000,1
hidden dims 1000,1,1
St叩S
Figure 5: Depth induced momentum.
8
Under review as a conference paper at ICLR 2020
References
[1]	Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via
over-parameterization. arXiv preprint arXiv:1811.03962, 2018.
[2]	Joel Hestness Ardalani, Newsha and Gregory Diamos. Have a larger
cake and eat it faster too: A guideline to train larger models faster.
http://research.baidu.com/Public/uploads/5abdde96f0ca3.pdf, 2018.
[3]	Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018.
[4]	Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018.
[5]	Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis
of optimization and generalization for overparameterized two-layer neural networks. arXiv
preprint arXiv:1901.08584, 2019.
[6]	Peter L Bartlett, David P Helmbold, and Philip M Long. Gradient descent with identity
initialization efficiently learns positive-definite linear transformations by deep residual networks.
Neural computation, 3l(3):477-502, 2019.
[7]	Raef Bassily, Mikhail Belkin, and Siyuan Ma. On exponential convergence of sgd in non-convex
over-parametrized learning. arXiv preprint arXiv:1811.02564, 2018.
[8]	Lingjiao Chen, Hongyi Wang, Jinman Zhao, Dimitris Papailiopoulos, and Paraschos Koutris.
The effect of network width on the performance of large-batch training. In Advances in Neural
Information Processing Systems, pages 9302-9309, 2018.
[9]	Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In Advances in neural information processing systems, pages 6571-6583,
2018.
[10]	Anna Choromanska, Mikael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun.
The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pages 192-204,
2015.
[11]	Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and
Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-
convex optimization. In Advances in neural information processing systems, pages 2933-2941,
2014.
[12]	Simon S Du and Wei Hu. Width provably matters in optimization for deep linear neural
networks. arXiv preprint arXiv:1901.08572, 2019.
[13]	Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos.
Gradient descent can take exponential time to escape saddle points. In Advances in neural
information processing systems, pages 1067-1077, 2017.
[14]	Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds
global minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018.
[15]	Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.
[16]	John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
[17]	Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. arXiv preprint
arXiv:1904.01681, 2019.
[18]	Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey.
arXiv preprint arXiv:1808.05377, 2018.
9
Under review as a conference paper at ICLR 2020
[19]	Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online
stochastic gradient for tensor decomposition. In Conference on Learning Theory, pages 797-
842, 2015.
[20]	Amir Gholami, Kurt Keutzer, and George Biros. Anode: Unconditionally accurate memory-
efficient gradients for neural odes. arXiv preprint arXiv:1902.10298, 2019.
[21]	Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural
network optimization problems. arXiv preprint arXiv:1412.6544, 2014.
[22]	Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pages
8571-8580, 2018.
[23]	Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape
saddle points efficiently. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pages 1724-1732. JMLR. org, 2017.
[24]	Kenji Kawaguchi. Deep learning without poor local minima. In Advances in neural information
processing systems, pages 586-594, 2016.
[25]	Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping
Tak Peter Tang. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp
Minima. arXiv preprint arXiv:1609.04836, 2016.
[26]	Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[27]	Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond finite layer neural net-
works: Bridging deep architectures and numerical differential equations. arXiv preprint
arXiv:1710.10121, 2017.
[28]	Yu Nesterov. Introductory lectures on convex programming. 1998.
[29]	Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks,
12(1):145-151, 1999.
[30]	Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error
guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
[31]	Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nes-
terov’s accelerated gradient method: Theory and insights. In Advances in Neural Information
Processing Systems, pages 2510-2518, 2014.
[32]	Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of
initialization and momentum in deep learning. In International conference on machine learning,
pages 1139-1147, 2013.
[33]	Yazhen Wang. Asymptotic analysis via stochastic differential equations of gradient descent
algorithms in statistical and computational paradigms. arXiv preprint arXiv:1711.09514, 2017.
[34]	Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
[35]	Yanqi Zhou, Siavash Ebrahimi, Sercan O Arik, Haonan Yu, Hairong Liu, and Greg Diamos.
Resource-efficient neural architect. arXiv preprint arXiv:1806.07912, 2018.
[36]	Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable
architectures for scalable image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 8697-8710, 2018.
10
Under review as a conference paper at ICLR 2020
A lp LOSS
We argue very briefly that without loss of generality, we can assume that we work with l2 loss instead
of other Lp norms. For lp loss,
∂L =P ∙(yj-% )p-1
So for l2 loss, Equation 5 becomes
du = ηH …
(22)
(23)
Recall the argument for decay rate under Equation 8. The single variable analogue of the system is:
	ft) = hf(t)	(24)
the solution looks like:	f(t) = c0eht	(25)
and decays like:	f(t) = c0(1 - ξ)t	(26)
with	ξ=1-eh	(27)
When h is small, by Taylor expansion		
	h2 ξ ≈ 1 一 (1 + h + ɪ...) ≈ —h	(28)
Instead of using the closed form solution, this ODE could equivalently be solved using the series
method and we would be getting the Taylor expansion of equation (25) on the right and equation (28)
remains the same. Note that the series method (more fundamentally, Taylor expansion involving only
one matrix) works for both ODEs and systems with convergent assumptions on the coefficient matrix
H. ηH is small by assumption, so we have convergent Taylor expansions, so we continue to argue
for lp loss as if we are in the single variable case.
For general lp loss, we have:
f0 = hfp-1	(29)
then
f = coe P t	(30)
and the decay rate would be P, scaled down by p, so all the convergence rate statements We make for
the rest of the paper for l2 remains true if scaled by P.
B 2 layer linear network
The detailed brute-force calculation of Hij for a 2-layer fully connected linear network with m = 1
is as follows. Recall m is the width of the hidden layer.
H
Hij
X∂ui∂uj
∂Vs ∂Vs
v∈V
∂ui∂uj
∂w ∂w
+
d
hW(:,1), Xii ∙ hW(:,1), Xji + X(V0 ∙ Wik ∙ Xi,k) ∙(V0 ∙ wik ∙ Xj,k)
k=1
d
hW(：,1), Xii ∙ hW(：,1), Xji + v0 X(wik ∙ Xi,k) ∙ (wik ∙ Xj,k)
k=i
(31)
11
Under review as a conference paper at ICLR 2020
(Yω+JraQ≤φu⊂ωwl-ω>⊂ou
4.5000E-05
4.0000E-05
3.5000E-05
3.0000E-05
2.5000E-05
2.0000E-05
1.5000E-05
1.0000E-05
5.0000E-06
0.0000E+00
0	500
1000	1500	2000	2500	3000
Model's Width
Figure 6: Convergence Rate for a 3-layer RELU-activated Network.
For a general m, we have:
Hij
X∂ui ∂uj
∂vs ∂vs
v∈V s s
∂ui ∂uj
∂w ∂w
+
m	md
XhW (：,I),xii ∙ hW (:, l),Xj i + XX(Vl ∙ Xi,k ) ∙ (vl ∙ Xj,k)
md
X ( hW (：,l),Xii ∙ hW (：,1),Xj) + X(Vl ∙ Xi,k) ∙ (vl ∙ xj,k) )
l=1	k=1
md
X 卜W(：,l),Xii ∙hW(:,l), Xji + Vl2 X Xi,k ∙ Xj,k)
(32)
C Weakly Gram matrix is empirically insignificant
In terms of network architecture, the weakly Gram term would only appear when two paths share
a final edge. That means the network needs to have at least 3 layers. The number of pairs could
grow quadratically with width. Heuristically, each term E[PGpι (w, Xi) ∙ PGp2 (w, Xj-)] tend to be
0 because of independent weights in PG and the weights having expectation 0. Hence, we focus
on the strongly Gram part for the rest of the section. As in Equation (19), the number of terms in
weakly Gram component grows quadratically in the width of the hidden layer. One might expect
this results in a quadratic growth in convergence, given all the terms are i.i.d. random variables and
strictly positive. In order to test this hypothesis, we exploit a teacher-student framework as outlined
in Section 7, however the student models are 3-layer RELU-activated networks to ensure the number
of terms in weakly Gram component is non-zero. We choose RELU activation function to ensure that
the expected value of the activation to be greater than zero. Our experimental results shows that the
convergence rate relationship with width is linear (see Figure 6) and not quadratic. This implies that
weakly Gram components have expectation zero.
D	JUSTIFY H STAYS CLOSE TO INITIALIZATION
Weights and H are indeed time-dependent and follow their respective stochastic process. However,
theoretical results in the overparametrized regime suggest that every weight vector remains close to
initialization (Lemma 5.3 [5] and Lemma 3.3 [15]). We also observe that the mean and variance stay
very close to initialization, N(0, 0.1) (Figure 7). In Figure 8, we ran the Kolmogorov-Smirnov test to
compare the weight distribution against the initialization distribution. As shown, the p-values are
overall stable and far from significance levels to invalidate the null hypothesis (H0 : trained weights
follow N (0, 0.1)).
12
Under review as a conference paper at ICLR 2020
Mean of Weights, Linear Network
- width 2000 layer 0	- width 2000 layer 1
0
-0.0002
-0.0004
-0.0006
Std of Weights, Linear Network
Std of Weights, Sigmoid
width 2000 layer 0 - width 2000 layer 1
5000
10000 20000 30000 40000 50000 60000
Figure 7: Mean and Std across the training process. (X-axis is the number of steps)
2000	3000
Mean of Weights, Sigmoid
・ width 2000 layer 0 . width 2000 layer 1
-0.0001
-0.0002
-0.0003
Figure 8: KS test p-values across the training process.
-layer 0. width:2000, sigmoid
Figure 9: Turning point test p-values
layer 1r
width:2000, sigmoid
10000	20000	30000	40000	50000
Steps
E Justify iid assumption for weights
The Turning Point Test is commonly used to check iid-ness. Figure 9 shows the p-value through the
training process. As shown, p-values do not exhibit any major regime shifts and the null hypothesis
(the weights are iid samples) stays far from rejectable with any commonly used significance levels
(0.1, 0.05 . . . ). iid-ness can also be seen as a consequence of closeness to initialization in response 1.
By staying close to initialization, statistical properties are preserved and the weights remain an iid
sample of the original distribution.
F Proof of Proposition 6.2
Proof. By definition of f and g, in g, there's a new path of length 1 whose edge is μ and all paths in
f are annexed by μ at the front. Hence Hij for the new network g can be computed in terms of Hij
13
Under review as a conference paper at ICLR 2020
for the network f as follows:
Hij(g)= μ2Hij(f) + f (Xi) ∙ f (Xj)	(33)
This can be written in the matrix form as:	
H(g) = μ2H(f) + uf ∙UT	(34)
We first calculate an inner product,	
hH(g)ug, Ugi = h(μH(f) + uf ∙ Uf)ug, Ugi	(35)
=h(μ2H(f) + Uf ∙ UT)μUf, μUfi	(36)
= hμ2H(f) ∙ μUf, μUfi +hUf ∙ UT ∙ μUf, μUfi	(37)
=μ4(H(f )Uf, Ufi + μ2(Uf ∙ UT ∙ Uf, Uf)	(38)
=μ4 hH (f )Uf, Uf) + μ h ||Uf Il Uf ,Uf i	(39)
=μ4hH(f )Uf ,Uf i + μ2bf l∣4	(40)
Now we calculate cosines,	
Cos(H I* … ) = ∣H11⅛	(41)
=μ4hH(f )Uf ,Uf i + 〃2||Uf『 μ2∣∣H (g)Uf||||Uf U	(42)
=μ2hH(f )Uf ,Uf i +||Uf『 ||H (g)Uf||||Uf||	(43)
Using equation (34) to expand the denominator,	
皿、	、μ2hH(f )Uf ,Uf i +||Uf||4 (H (g)Ug, Ugi = ∏	—— M2H (f)+UfUT )叼 ||||Uf||	(44)
≥ μ2hH(f)Uf,Ufi +||Uf||4 一||〃2H(f )Uf||||Uf|| 十||Uf『	(45)
It suffices to check	
μ2hH(f)Uf,Ufi +||UfU4 ≥	hH(f)叼,Ufi	= RHS ||〃2H (f )Uf||||Uf||+||Uf||4 一 ||H (f )叼||||叼|| 一	(46)
which after cross-multiplication, reduces to:	
||H (f)Uf ||||Uf || ≥ hH(f)Uf,Ufi	(47)
This follows from Cauchy-SchWarz inequality. Equality holds precisely when H(f )uf is parallel to
Uf (Equation (47), (44)).	□
Remark F.1. One further remark that could be made after Remark 6.3 and 6.4 is the following.
The proof above tells us that depth-induced momentum comes mainly from the extra term uf in
Equation (34). Note that uf s are the values of the networks at sub-network f. Comparing this with
our path-based expansion formula at Equation (12) where all sub-networks’ values appear with
different scaling factors proportional to their path gradients sum, we can conclude qualitatively that
this phenomenon is present across all sub-networks and each sub-network contributes to overall
momentum proportional to its path gradients sums.
14
Under review as a conference paper at ICLR 2020
9Bd 8ua)5,lωAU0□
Figure 10: Momentum’s dependence on output scale.
G	The Impact of Output Value Distribution on Depth-Induced
Momentum
Figure 10 shows the impact of output distribution on momentum. The underscore separated strings
represent hidden dimensions. "2000" represents hidden dimension of width 2000 and "2000_1"
represents hidden layers of width 2000 and 1. The number after "std" represents the output’s standard
deviation in the teacher network, equivalently scale as the mean is 0. As we can observe from the
graph, momentum acceleration is not present for small standard deviation (0.06 and 0.12) and the
behavior is more similar to a shallow 2-layer network of the same width. When standard deviation is
raised to 0.24, then We observe acceleration of the convergence rate. In all of these experiments, μ is
initialized to 1 and empirically stays close to 1 to counteract the effect of μ2 in equation (34).
15