Under review as a conference paper at ICLR 2020
Neural Subgraph Isomorphism Counting
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we study a new graph learning problem: learning to count sub-
graph isomorphisms. Although the learning based approach is inexact, we are
able to generalize to count large patterns and data graphs in polynomial time com-
pared to the exponential time of the original NP-complete problem. Different
from other traditional graph learning problems such as node classification and
link prediction, subgraph isomorphism counting requires more global inference to
oversee the whole graph. To tackle this problem, we propose a dynamic interme-
dium attention memory network (DIAMNet) which augments different represent-
ation learning architectures and iteratively attends pattern and target data graphs to
memorize different subgraph isomorphisms for the global counting. We develop
both small graphs (≤ 1,024 subgraph isomorphisms in each) and large graphs (≤
4,096 subgraph isomorphisms in each) sets to evaluate different models. Experi-
mental results show that learning based subgraph isomorphism counting can help
reduce the time complexity with acceptable accuracy. Our DIAMNet can further
improve existing representation learning models for this more global problem.
1 Introduction
Graphs are general data structures widely used in many applications, including social network ana-
lysis, molecular structure analysis, natural language processing and knowledge graph modeling, etc.
Learning with graphs has recently drawn much attention as neural network approaches to repres-
entation learning have been proven to be effective for complex data structures (Niepert et al., 2016;
Kipf & Welling, 2017; Hamilton et al., 2017b; Schlichtkrull et al., 2018; Velickovic et al., 2018; Xu
et al., 2019). Most of existing graph representation learning algorithms focus on problems such as
node classification, linking prediction, community detection, etc. (Hamilton et al., 2017a). These
applications are of more local decisions for which a learning algorithm can usually make infer-
ences by inspecting the local structure of a graph. For example, for the node classification problem,
after several levels of neighborhood aggregation, the node representation may be able to incorporate
sufficient higher-order neighborhood information to discriminate different classes (Xu et al., 2019).
In this paper, we study a more global learning problem: learning to count subgraph isomorphisms
(counting examples are shown as Figure 1). Although subgraph isomorphism is the key to solve
graph representation learning based applications (Xu et al., 2019), tasks of identifying or counting
subgraph isomorphisms themselves are also significant and may support broad applications, such
as bioinformatics (Milo et al., 2002; Alon et al., 2008), chemoinformatics (Huan et al., 2003), and
online social network analysis (Kuramochi & Karypis, 2004). For example, in a social network, we
can solve search queries like “groups of people who like X and visited Y-city/state.” In a knowledge
graph, we can answer questions like “how many languages are there in Africa speaking by people
living near the banks of the Nile River?” Many pattern mining algorithms or graph database indexing
based approaches have been proposed to tackle subgraph isomorphism problems (Ullmann, 1976;
Cordella et al., 2004; He & Singh, 2008; Han et al., 2013; Carletti et al., 2018). However, these
approaches cannot be applied to large-scale graphs because of the exponential time complexity.
Thanks to the powerful graph representation learning models which can effectively capture local
structural information, we can use a learning algorithm to learn how to count subgraph isomorphisms
from a lot of examples. Then the algorithm can scan a large graph and memorize all necessary
local information based on a query pattern graph. In this case, although learning based approaches
can be inexact, we can roughly estimate the range of the number of subgraph isomorphism. This
can already help many applications that do not require exact match or need a more efficient pre-
1
Under review as a conference paper at ICLR 2020
Homogeneous			Heterogenous					
Pattern	Graph	Count	Pattern	Graph	Count	Pattern	Graph	Count
-ɪ	-□^^	0	ɪ	ɪ	0	ɪ	ɪ	0
Δ	S	12	A	H	4	A	S	1
A	因	24	Λ	ɪ	6	A	ɪ	2
Figure 1: Subgraph isomorphism counting examples in different settings. The first is the homogen-
eous graph counting problem. The second is a heterogeneous vertex graph counting problem where
there are two types of nodes. The third is a heterogeneous vertex and heterogeneous edge graph
counting problem where there are two types of nodes and two types of edges.
processing step. To this end, in addition to trying different representation learning architectures,
we develop a dynamic intermedium attention memory network (DIAMNet) to iteratively attend
the query pattern and the target data graph to memorize different local subgraph isomorphisms for
global counting. To evaluate the learning effectiveness and efficiency, we develop a small (≤ 1,024
subgraph isomorphisms in each graph) and a large (≤ 4,096 subgraph isomorphisms in each graph)
dataset and evaluate different neural network architectures.
Our main contributions are as follows.
•	To our best knowledge, this is the first work to model the subgraph isomorphism counting problem
as a learning problem, for which both the training and prediction time complexities are polynomial.
•	We exploit the representation power of different deep neural network architectures in an end-to-
end learning framework. In particular, we provide universal encoding methods for both sequence
models and graph models, and upon them we introduce a dynamic intermedium attention memory
network to address the more global inference problem for counting.
•	We conduct extensive experiments on developed datasets which demonstrate that our framework
can achieve good results on both relatively large graphs and large patterns compared to existing
studies.
2	Related Work
Subgraph Isomophism Problems. Given a pattern graph and a data graph, the subgraph isomorph-
ism search aims to find all occurrences of the pattern in the data graph with bijection mapping func-
tions. Subgraph isomorphism is an NP-complete problem among different types of graph matching
problems (monomorphism, isomorphism, and subgraph isomorphism). Most subgraph isomorphism
algorithms are based on backtracking. They first obtain a series of candidate vertices and update a
mapping table, then recursively revoke their own subgraph searching functions to match one ver-
tex or one edge at a time. Ullmann’s algorithm (Ullmann, 1976), VF2 (Cordella et al., 2004), and
GraphQL (He & Singh, 2008) belong to this type of algorithms. However, it is still hard to perform
search when either the pattern or the data graph grows since the search space grows exponentially as
well. Some other algorithms are designed based on graph-index, such as gIndex (Yan et al., 2004),
which can be used as filters to prune out many unnecessary graphs. However, graph-index based
algorithms have a problem that the time and space in indexing also increase exponentially with the
growth of the graphs (Sun et al., 2012). TurboISO (Han et al., 2013) and VF3 (Carletti et al., 2018)
add some weak rules to find candidate subregions and then call the recursive match procedure on
subregions. These weak rules can significantly reduce the searching space in most cases.
Graph Representation Learning. Graph (or network) representation learning can be directly learn-
ing an embedding vector of each graph node (Perozzi et al., 2014; Tang et al., 2015; Grover &
Leskovec, 2016). This approach is not easy to generalize to unseen nodes. On the other hand, graph
neural networks (GNNs) (Battaglia et al., 2018) provide a solution to representation learning for
nodes which can be generalized to new graphs and unseen nodes. Many graph neural networks have
been proposed since 2005 (Gori et al., 2005; Scarselli et al., 2005) but rapidly developed in recent
years. Most of them focus on generalizing the idea of convolutional neural networks for general
graph data structures (Niepert et al., 2016; Kipf & Welling, 2017; Hamilton et al., 2017b; Velickovic
et al., 2018) or relational graph structures with multiple types of relations (Schlichtkrull et al., 2018).
2
Under review as a conference paper at ICLR 2020
More recently, Xu et al. (2019) propose a graph isomorphism network (GIN) and show its discrimin-
ative power. Others use the idea of recurrent neural networks (RNNs) which are originally proposed
to deal with sequence data to work with graph data (Li et al., 2016; You et al., 2018). Interestingly,
with external memory, sequence models can work well on complicated tasks such as language mod-
eling (Sukhbaatar et al., 2015; Kumar et al., 2016) and shortest path finding on graphs (Graves et al.,
2016). There is another branch of research called graph kernels (Vishwanathan et al., 2010; Sher-
vashidze et al., 2011; Yanardag & Vishwanathan, 2015; Togninalli et al., 2019; Chen et al., 2019)
which also convert graph isomorphism to a similarity learning problem. However, they usually work
on small graphs and do not focus on subgraph isomorphism identification or counting problems.
3	Preliminaries
We begin by introducing the subgraph isomophism problems and then provide the general idea of
our work by analyzing the time complexities of the problems.
3.1	Problem Definition
Traditionally, the subgraph isomorphism problem is defined between two simple graphs or two dir-
ected simple graphs, which is an NP-complete problem. We generalize the problem to a counting
problem over directed heterogeneous multigraphs, whose decision problem is still NP-complete.
A graph or a pattern is defined as G = (V, E, X , Y) where V is the set of vertices. E ⊆ V × V is
the set of edges, X is a label function that maps a vertex to a vertex label, and Y is a label function
that maps an edge to a set of edge labels. We use an edge with a set of edge labels to represent
multiedges with the same source and the same target for clarity. That is, there are no two edges in
a graph such that they have the same source, same target, and the same edge label. To simplify the
statement, we assume Y((u, v)) = φ if (u, v) 6∈ E.
In this paper, we discuss isomorphic mappings that preserve graph topology, vertex labels and edge
labels, but not vertex ids. More precisely, a pattern GP = (VP , EP , XP , YP) is isomorphic to a
graph GG = (VG, EG, XG, YG) if there is a bijection f : VG → VP such that:
•	∀v ∈ VG, XG(v) = XP (f(v)),
•	∀v∈VP,XP(v) = XG(f-1(v)),
•	∀(u, v) ∈ EG, YG((u, v)) = YP ((f (u), f (v))),
•	∀(u, v) ∈ EP,YP((u,v)) = YG((f-1(u), f -1(v))).
Furthermore, GP being isomorphic to a graph GG is denoted as GP ' GG and the function f is
named as an isomorphism.
A pattern GP = (VP, EP, XP, YP) is isomorphic to a subgraph GG0 = (VG0 , EG0 , XG, YG) of a graph
GG = (VG, EG, XG,YG): VG0 ⊆ VG, EG0 ⊆ EG ∩ (VG0 × VG0 ) if GP ' GG0 . The bijection function
f : VG0 → VP is named as a subgraph isomorphism.
The subgraph isomorphism counting problem is defined as to find the number of all different sub-
graph isomorphisms between a pattern graph GP and a graph GG. Examples are shown in Figure 1.
3.2	General Idea
Intuitively, We need to compute O(Perm(|Vg|, |Vp|) ∙ d|Vp|) to solve the subgraph isomorphism
counting problem by enumeration, where Perm(n, k)= ⑺-")!, |Vg | is the number of graph nodes,
|VP | is the number of pattern nodes, d is the maximum degree. The first subgraph isomorphism
algorithm, Ullmann,s algorithm (Ullmann, 1976), reduces the seaching time to O(∣Vp||Vg| ∙ |Vg|2).
If the pattern and the graph are both small, the time is acceptable because both of two factors are
not horrendously large. However, since the computational cost grows exponentially, it is impossible
to count as either the graph size or the pattern size increases. If we use neural networks to learn
distributed representations for VG and VP or EG and EP, we can reduce the complexity to O(|Vp | ∙
|Vg| + |Vg|2) or Ο(∣Ep∣∙∣Eg∣ + |Eg|2) via source attention and self-attention. Assuming that we
can further learn a much higher level abstraction without loss of representation power for GP, then
3
Under review as a conference paper at ICLR 2020
Pattern
Graph
Figure 2: General framework of neural subgraph isomorphism counting models.
1
5
7
4
Graph
11122	66
23545	78
Sequence view
1 o∙
Graph view
8
Figure 3: Two views of different encoding methods. There are three labels of vertices (marked as
orange, blue, and yellow) and two labels of edges (marked as solid and dashed lines). The numbers
affixed show one possible set of ids for the vertices.
the computational cost can be further reduced to O(|VG|2) or O(|EG|2). However, the complexity of
the latter framework is still not acceptable when querying over large graphs. If we do not consider
self-attention, the computational cost will be O(|VG|) or O(|EG|), but missing the self-attention
will hurt the performance. In this work, we hope to use attention mechanism and additional memory
networks to further reduce the complexity compared with O(|VG|2) or O(|EG|2) while keeping the
performance acceptable on the counting problem.
4	Methodologies
A graph (or a pattern) can be represented as a sequence of edges or a series of adjacent matrices and
vertex features. For sequence inputs we can use CNNs (Kim, 2014), RNNs such as GRU (Cho et al.,
2014), or Transformer-XL (Dai et al., 2019) to extract high-level features. While if the inputs are
modeled as series of adjacent matrices and vertex features, we can use RGCN (Schlichtkrull et al.,
2018) to learn vertex representations with message passing from neighborhoods. After obtaining
the pattern representation and the graph representation, we feed them into an interaction module to
extract the correlated features from each side. Then we feed the output context of the interaction
module into a fully-connected layer to make predictions. A general framework is shown in Figure 2
and the difference between sequence encoding and graph encoding is shown in Figure 3.
4.1	Sequence Models
4.1.1	Sequence Encoding
In sequence models, the minimal element of a graph (or a pattern) is an edge. By definition, at
least three attributes are required to identify an edge e, which are the source vertex id u, the target
vertex id v, and its edge label y ∈ Y (e). We further add two attributes of vertices’ labels to form
a 5-tuple (u, v, X (u), y, X (v)) to represent an edge e, where X (u) is the source vertex label and
X (v) is the target vertex label. A list of 5-tuple is referred as a code. We follow the order defined
in gSpan (Yan & Han, 2002) to compare pairs of code lexicographically; the detailed definition is
given in Appendix A. The minimum code is the code with the minimum lexicographic order with
the same elements. Finally, each graph can be represented by the corresponding minimum code, and
vice versa.
Given that a graph is represented as a minimum code, or a list of 5-tuples, the next encoding step
is to encode each 5-tuple into a vector. Assuming that we know the max values of |V|, |X |, |Y|
4
Under review as a conference paper at ICLR 2020
in a dataset in advance, we can encode each vertex id v , vertex label x, and edge label y into B-
nary digits, where B is the base and each digit d ∈ {0,1,…，B - 1}. It is easy to replace each
digit with a one-hot vector so that each 5-tuple can be vectorized as a multi-hot vector which is
the concatenation of one-hot vectors. The length of the multi-hot vector of a 5-tuple is B X (2 ∙
dlogB(Max(∣V∣))e + 2 ∙ dlogB(Max(∣X∣))e +「logB(Max(∣Y∣))]). Then minimum is achieved
when B = 2 and de = 2 × (2 ∙ dl0g2(Max(∣V∣))e + 2 ∙ dl0g2(Max(∣X∣))e + dlog2(Max(|Y|))]).
Then we can easily calculate the graph dimension dg and the pattern dimension dp . Furthermore,
the minimum code can be encoded into a multi-hot matrix, G ∈ RlEGl×dg for a graph GG or P ∈
R1EP l×dp fora pattern G P according to this encoding method.
This encoding method can be extended when we have larger values of ∣V ∣, ∣X ∣, ∣Y ∣. A larger value,
e.g., ∣V∣, only increases the length of one-hot vectors corresponding to its field. Therefore, we can
regard new digits as the same number of zeros in previous data. As long as we process previous
one-hot vectors carefully to keep these new dimensions from modifying the original distributed
representations, we can also extend these multi-hot vectors without affecting previous models. A
simple but effective way is to initialize additional new weights related to new dimensions as zeros.
4.1.2	Sequence Neural Networks
Given the encoding method in Section 4.1.1, we can simply embed graphs as multi-hot matrices.
Then we can use general strategies of sequence modeling to learn dependencies among edges in
graphs.
Convolutional Neural Networks (CNNs) have been proved to be effective in sequence modeling
(Kim, 2014). In our experiments, we apply multiple layers of the convolution operation to obtain a
sequence of high-level features.
Recurrent Neural Networks (RNNs), such as GRU (Cho et al., 2014), are widely used in many
sequence modeling tasks.
Transformer-XL (TXL) (Dai et al., 2019) is a variant of the Transformer architecture (Vaswani
et al., 2017) and enables learning long dependencies beyond a fixed length without disrupting tem-
poral coherence. Unlike the original autoregressive settings, in our model the Transformer-XL en-
coder works as a feature extractor, in which the attention mechanism has a full, unmasked scope over
the whole sequence. However, its computational cost grows quadratically with the size of inputs, so
the tradeoff between performance and efficiency would be considered.
4.2	Graph Models
4.2.1	Graph Encoding
In graph models, each vertex has a feature vector and edges are used to pass information from its
source to its sink. GNNs do not need vertex ids and edge ids explicitly because the adjacency
information is included in a adjacent matrix. As explained in Section 4.1.1, we can vectorize vertex
labels into multi-hot vectors as vertex features. In a simple graph or a simple directed graph, the
adjacent information can be stored in a sparse matrix to reduce the memory usage and improve the
computation speed. As for heterogeneous graphs, behaviors of edges should depend on edge labels.
RGCNs have relation-specific transformations so that each edge label and topological information
are mixed into the message to the sink. We follow this method and use basis-decomposition for
parameter sharing (Schlichtkrull et al., 2018).
4.2.2	Graph Neural Networks
Relational Graph Convolutional Networks (RGCNs) (Schlichtkrull et al., 2018) are developed
specifically to handle multi-relational data in realistic knowledge bases. Each relation corresponds
to a transformation matrix to transform relation-specific information from a neighbor to the center
vertex. Two decomposition methods are proposed to address the rapid growth in the number of para-
meters with the number of relations: basis-decomposition and block-diagonal-decomposition. We
use the first method, which is equivalent to the MLPs in GIN (Xu et al., 2019). The original RGCN
uses the mean aggregator, but Xu et al. (2019) find that the sum-based GNNs can capture graph
structures better. We implement both and named them as RGCN and RGCN-SUM respectively.
5
Under review as a conference paper at ICLR 2020
Figure 4: Illustration of dynamic intermedium attention memory network (DIAMNet). Φ1 repres-
ents Eqs. (1) and (2), Φ2 represents Eqs. (4) and (5), and two types of gates are Eqs. (3) and
(6).
4.3	Dynamic Intermedium Attention Memory Network
After obtaining a graph representation G and a pattern representation P from a sequence model or
a graph model where their column vectors are d-dimensional, we feed them as inputs of interaction
layers to extract the correlated context between the pattern and the graph. A naive idea is to use
attention modules (Bahdanau et al., 2015) to model interactions between these two representations
and interactions over the graph itself. However, this method is not practical due to its complexity,
UP to O(∣Ep∣∙∣Eg∣ + |Eg|2) for sequence modeling and O(∣Vp∣∙∣Vg∣ + |Vg|2) for graph modeling.
To address the problem of high computational cost in the attention mechanism, we propose the
Dynamic Intermedium Attention Memory Network (DIAMNet), using an external memory as an
intermedium to attend both the pattern and the graph in order. To make sure that the memory has
the knowledge of the pattern while attending the graph and vice-versa, this dynamic memory is
designed as a gated recurrent network as shown in Figure 4. Assuming that the memory size is M
and We have T recurrent steps, the time complexity is decreased into O(T ∙ M ∙ (∣Ep∣ + ∣Eg∣)) or
O(T ∙ M ∙ (∣Vp∣ + ∣Vg∣)), which means the method can be easily applied to large-scale graphs.
The external memory is divided into M blocks {m1, ..., mM}, Where mj ∈ Rd. At each time step t,
{mj } is updated by the pattern and the graph in order via multi-head attention mechanism (Vaswani
et al., 2017). Specifically, the update equations of our DIAMNet are given by:
s(jt)	二 MultiHead(m*, P, P)	(1)
zj(t)	σ (UP m(jt) + VPsj(t))	(2)
Sjt)=	(t)	(t)	(t)	(t) zj	mj + (1 - zj )	sj	(3)
se(jt)	二 MuIltiHead司),G, G)	(4)
zej(t)	二 σ(UGsjt) + VG寸))	(5)
m(jt+1)	二 Zjt) Θ s(t) + (1- Zjtt) Θ ejt)	(6)
Here M ultiH ead is the attention method described in (Vaswani et al., 2017), σ represents the
logistic sigmoid function, Sj is the intermediate state of the j th block of memory that summarizes
information from the pattern, and se for information from both the pattern and the graph. zj and zej
are two gates designed to control the updates on the states in the jth block. UP, VP , UG, VG ∈
Rd×d are trainable parameters.
5	Experiments
In this section, we report our major experimental results. More results can be found in the Appendix.
6
Under review as a conference paper at ICLR 2020
5.1	Datasets
In order to train and evaluate our neural models for the subgraph isomorphism counting problem,
we need to generate enough graph-pattern data. As there’s no special constraint on the pattern,
the pattern generator may produce any connected multigraph without identical edges, i.e., parallel
edges with identical label. In contrast, the ground truth number of subgraph isomorphisms must be
tractable in our synthetic graph data. Therefore, our graph generator first generates multiple discon-
nected components, possibly with some subgraph isomorphisms. We use the idea of neighborhood
equivalence class (NEC) in TurboISO (Han et al., 2013) to control the necessary conditions of a
subgraph isomorphism in the graph generation process. The detailed algorithms are shown in Ap-
pendix B. Then the generator merges these components into a larger graph and ensures that there is
no more subgraph isomorphism generated in the merge process. The subgraph isomorphism search
can be done during these components subgraphs in parallel.
Using the pattern generator and the graph generator above, we can generate many patterns and
graphs for neural models. We are interested in follow research questions: whether sequence models
and graph convolutional networks can perform well given limited data, whether their running time
is acceptable, and whether memory can help models make better predictions even faced with a NP-
complete problem. To evaluate different neural architectures and different prediction networks, we
generate two datasets in different graph scales and the statistics are reported in Table 1. There are 187
unique patterns in whole pairs, where 75 patterns belong to the small dataset, 122 patterns belong to
the large dataset. Target data graphs are not required similar so they are generated randomly. The
generation details are reported in Appendix C.
Table 1: Statistics of the datasets.
I #Training		#Dev	#Test	Mean(IVG |)	Mean(IEG ∣) ∣		Counts
Small	398,088	49,761	49,761	32.6	76.3	{c	∈ NIc ≤ 1, 024}
Large	316,224	39,528	39,528	240.0	560.0	{c	∈ NIc ≤ 4, 096}
5.2	Implementation Details
Instead of directly feeding multi-hot encoding vectors into representation modules, we use two
simple linear layers separately to transform graph multi-hot vectors and pattern multi-hot vectors
to lower-dimensional, distributed ones. To improve the efficiency, we also add a filtering layer to
filter out irrelevant parts before all representation modules. The details of this filter layer is shown
in Section D.1.
5.2.1	Representation Models
In our experiments, we implemented five different representation models: (1) CNN is a 3-layer con-
volutional layers followed by max-pooling layers. The convolutional kernels are 2,3,4 respectively
and strides are 1. The pooling kernels are 2,3,4 and strides are 1. (2) RNN is a simple 3-layer GRU
model. (3) TXL is a 6-layer Transformer encoder with additional memory. (4) RGCN is a 3-layer
RGCN with the basis decomposition. We follow the same setting in that ordinal paper to use mean-
pooling in the message propagation part. (5) RGCN-SUM is a modification of RGCN to replace
the mean-pooling with sum-pooling.
5.2.2	Interaction Networks
After getting a graph representation G and a pattern representation P from the representation learn-
ing modules, we feed them into the following different types of interaction layers for comparison.
SumPool: A simple sum-pooling is applied for G and P to obtain g and p, and the model sends
Concate(g, p, g 一 p, g Θ P) with the graph size and the pattern size information into the next fully
connected (FC) layers.
MeanPool: Similar settings as SumPool, but to replace the pooling method with mean-pooling.
MaxPool: Similar settings as SumPool, but to replace the pooling method with max-pooling.
7
Under review as a conference paper at ICLR 2020
AttnPool: We want to use attention modules without much computational cost so the self-attention
is not acceptable. We simplify the attention by first applying a pooling mechanism for the pattern
graph and then use the pooled vector to perform attention over the data graph rather than simply
perform pooling over it. Other settings are similar with pooling methods. The detailed information
is provided in Appendix D.2. We only report results of mean-pooling based attention, because it is
the best of the three variants.
DIAMNet: We compare the performance and efficiency of our DIAMNet proposed in Section 4.3
with above interaction networks. The initialization strategy we used is shown in Appendix D.3. And
we feed the whole memory with size information into the next FC layers.
5.3	Experiment Settings
For fair comparison, we set embedding dimensions, dimensions of all representation models, and
the numbers of filters all 64. The segment size and memory size in TXL are also 64 due to the
computation complexity. The length of memory is fixed to 4 and the number of recurrent steps
is fixed to 3 in the DIAMNet for both small and large datasets. We use the mean squared error
(MSE) to train models and evaluate the validation set to choose best models. The optimizer is
Adam with learning rate 0.001. L2 penalty is added and the coefficient is set as 0.001. To avoid
gradient explosion and overfitting, we add gradient clipping and dropout with a dropout rate 0.2.
We use Leaky _ReLU as activation functions in all modules. Due to the limited number of patterns,
the representation module for patterns are easy to overfit. Therefore, we use the same module
with shared parameters to produce representation for both the pattern and the graph. We also find
that using curriculum learning (Bengio et al., 2009) can help models to converge better. Hence, all
models in Table 3 are fine-tuned based on the best models in small in the same settings. Training and
evaluating were finished on one single NVIDIA GTX 1080 Ti GPU under the PyTorch framework.
5.4	Evaluation Metrics
As we model this subgraph isomorphism counting problem as a regression problem, we use common
metrics in regression tasks, including the root mean square error (RMSE) and the mean absolute
error (MAE). In this task, negative predictions are meaningless, so we only evaluate ReLU(Yi) as
final prediction results. Considering that about 75% of countings are 0’s in our dataset, we also use
evaluation metrics for the binary classification to analyze behaviors of different models. We report
F1 scores for both zero data (F1zero) and nonzero data (F1nonzero). Two trivial baselines, Zero that
always predicts 0 and Avg that always predicts the average counting of training data, are also used
in comparison.
5.5	Results and Analysis
We first report results for small dataset in Table 2 and results for large dataset in Table 3. In addition
to the trivial all-zero and average baselines and other neural network learning based baselines, we
are also curious about to what extent our neural models can be faster than traditional searching
algorithms. Therefore, we also compare the running time. Considering the graph generation strategy
we used, we decide to compare with VF2 algorithm (Cordella et al., 2004) to avoid unnecessary
interference from the similar searching strategy. From the experiments, we can draw following
observations and conclusions.
Comparison of different representation architectures. As shown in Table 2, in general, graph
models outperform most of the sequence models but cost more time to do inference. CNN is the
worst model for the graph isomorphism counting problem. The most possible reason is that the se-
quence encoding method is not suitable for CNN. The code order does not consider the connectivity
of adjacent vertices and relevant label information. Hence, convolutional operations and pooling
operations cannot extract useful local information but may introduce much noise. From results of
the large dataset, we can see that F1nonzero=0.180 is even worse than others. In fact, we find that
CNN always predicts 0 for large graphs. RNN and TXL are widely used in modeling sequences
in many applications. The two models with simple pooling can perform well. We note that RNN
with sum-pooling is better than TXL with memory. RNN itself holds a memory but TXL also has
much longer memory. However, the memory in RNN can somehow memorize all information that
8
Under review as a conference paper at ICLR 2020
Table 2: Results of different models on the small dataset. Time is evaluated on the whole test set.
			TpQt		
Models					
	RMSE	MAE	FIzero	F1nonzero	Time (sec)
SumPool	55.429	11.057	0.807	0.397	0.29
MeanPool	57.298	10.517	0.821	0.475	0.27
CNN	MaxPool	47.353	11.212	0.832	0.539	0.27
AttnPool	55.963	12.717	0.796	0.340	0.54
	DIAMNet	34.448	6.953	0.884	0.753	0.90
SumPool	29.955	5.740	0.908	0.819	0.55
MeanPool	31.010	6.447	0.912	0.833	0.54
RNN	MaxPool	30.824	6.236	0.869	0.690	0.58
AttnPool	31.857	6.025	0.899	0.793	0.79
	DIAMNet	29.743	5.547	0.927	0.877	1.15
SumPool	34.391	7.042	0.899	0.798	1.98
MeanPool	32.569	6.656	0.882	0.754	1.98
TXL	MaxPool	65.152	30.289	0.572	0.669	1.99
AttnPool	37.721	7.426	0.869	0.719	2.22
	DIAMNet	31.649	6.680	0.900	0.811	2.48
SumPool	32.414	6.578	0.900	0.796	6.05
MeanPool	33.829	7.152	0.878	0.735	5.99
RGCN	MaxPool	50.851	9.707	0.869	0.704	6.04
AttnPool	32.526	6.523	0.870	0.697	6.27
	DIAMNet	28.712	5.782	0.918	0.868	6.69
SumPool	22.379	3.958	0.920	0.845	5.78
MeanPool	22.483	4.254	0.903	0.803	5.81
RGCN-SUM	MaxPool	42.434	7.900	0.874	0.709	6.08
AttnPool	24.875	5.131	0.840	0.569	6.49
	DIAMNet	21.734	3.853	0.924	0.864	6.55
Zero	67.195	13.716	0.761	-00-	-
	Avg		65.780	21.986	0.0	0.557	-
VF2	0.0	-00-	1.0	-10-	^^T20
Table 3: Results of different models on the large dataset. Time is evaluated on the whole test set.
Model	RMSE	Test			Time (sec)
		MAE	F1zero	F1nonzero	
CNN	MaxPool	234.215	33.859	0.787	0.180	201
CNN DIAMNet	193.410	29.726	0.831	0.523	5.82
RNN	SumPool	136.031	20.234	0.905	0.799	16.40
RNN DIAMNet	134.786	21.595	0.911	0.844	19.98
TXL	MeanPool	182.637	35.749	0.842	0.587	-61.83
TXL	DIAMNet	167.480	26.954	0.881	0.749	67.95
SumPool	149.060	23.051	0.927	0.864	-25.74
RGCN DIAMNet	147.511	24.398	0.926	0.883	29.63
SumPool	125.022	16.551	0.903	0.788	-25.14
RGCN-SUM DIAMNet	118.433	16.152	0.930	0.879	30.72
Zero	237.904	35.445	0.769	-0.0-	-
Avg	235.253	60.260	0.0	0.545	-
VF2	-00-	-00-	1.0	-10-	〜5 X 103
has been seen previously but the memory of TXL is the representation of the previous segment. In
our experiments, the segment size is 64 so that TXL can not learn the global information at a time.
A part of the structure information misleads TXL, which is consistent with CNN. A longer segment
set for TXL may lead to better results, but it will require much more GPU memory and much longer
time for training. RGCN-SUM is much better than RGCN and other sequence models, which shows
that sum aggregator is good at modeling vertex representation in this task. The mean aggregator can
model the distribution of neighbor but the distribution can also misguide models.
Effectiveness of the memory. Table 2 shows the effectiveness of our dynamic attention memory
network as the prediction layer. It outperforms the other three pooling methods as well as the simple
attention mechanism for all representation architectures. Sum, Mean, and Attention pooling are all
9
Under review as a conference paper at ICLR 2020
UOD.apo.ld&.swnoj
UOD.apo.ld&.slunoj
O IOOOO 20000 30000 40000 50000	O IOOOO 20000 30000 40000 50000	O IOOOO 20000 30000 40000 50000
example id	example id	example id
(a) CNN + MaxPool	(b) CNN + DIAMNet (c) RGCN-SUM + DIAMNet
Figure 5: Model behaviors of three models in small dataset. The x-axis is the example id and the
y-axis is the count value. We mark the ground truth value as orange + and the predictions as blue ×.
We use two green dashed lines to separate patterns into three blocks based on numbers of vertices
(3,4, and 8). In each block, the examples are sorted based on the data graphs’ sizes.
comparable with each other, because they all gather the global information of the pattern and graph
representations. Prediction layer based on max pooling, however, performs the worst, and even
worse when the representation layer is CNN or Transformer-XL. This observation indicates that
every context of the pattern representation should be counted and we need a better way to compute
the weights between each context. The dynamic attention memory with global information of both
the pattern and the graph achieves the best results in most of the cases. One of the most interesting
observations is that it can even help extract the context of pattern and graph while the representation
layer (such as CNN) does not perform very well, which proves the power of our proposed method
of DIAMNet.
Performance on larger graphs. Table 3 shows our models can be applied to larger-scale graphs.
For the large dataset, we only choose the best pooling method for each of the baselines to report.
We can find most of the results are consistent to the small dataset, which means RGCN is the best
representation method in our task and the dynamic memory is effective. In terms of the running
time, all learning based models are much faster than the traditional VF2 algorithm for subgraph
isomorphism counting.
Model behaviors. As shown in Figure 5, we compare the model behaviors of the best model
(RGCN+SUM) and the worst model (CNN), as well as the great improvement of CNN when
memory is added. We can find that CNN+SumPool tends to predict the count value below 400
and has the same behavior between three patterns. This results may come from the fact that CNN
can only extract local information of a sequence and Sum pooling is not a good way to aggregate
it. However, the memory can memorize local information to each memory cell so it can improve
the representation power of CNN and can gain a better performance. RGCN, on the other hand, can
better represent the graph structure, so it achieves a better result, especially on the largest pattern
(the third block of each figure) compared with CNN. More results can be found in Appendix F.
6	Conclusions
In this paper, we study the challenging subgraph isomorphism counting problem. With the help of
deep graph representation learning, we are able to convert the NP-complete problem to a learning
based problem. Then we can use the learned model to predict the subgraph isomorphism counts
in polynomial time. Counting problem is more related to a global inference rather than only learn-
ing node or edge representations. Therefore, we have developed a dynamic intermedium attention
memory network to memorize local information and summarize for the global output. We build two
datasets to evaluate different representation learning models and global inference models. Results
show that learning based method is a promising direction for subgraph isomorphism detection and
counting and memory networks indeed help the global inference. We also performed detailed ana-
lysis of model behaviors for different pattern and graph sizes and labels. Results show that there is
much space to improve when the vertex label size is large. Moreover, we have seen the potential
real-world applications of subgraph isomorphism counting problems such as question answering
and information retrieval. It would be very interesting to see the domain adaptation power of our
developed pretrained models on more real-world applications.
10
Under review as a conference paper at ICLR 2020
References
Noga Alon, PhUong Dao, Iman Hajirasouliha, FereydoUn Hormozdiari, and Suleyman Cenk Sahin-
alp. Biomolecular network motif counting and discovery by color coding. In ISMB, pp. 241-249,
2008.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In ICLR, 2015.
Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinlcius Flores
Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner,
Caglar GUICehre, H. Francis Song, Andrew J. Ballard, Justin Gilmer, George E. Dahl, Ashish
Vaswani, Kelsey R. Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan
Wierstra, Pushmeet Kohli, Matthew Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu.
Relational inductive biases, deep learning, and graph networks. CoRR, abs/1806.01261, 2018.
Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
ICML, pp. 41-48, 2009.
Vincenzo Carletti, Pasquale Foggia, Alessia Saggese, and Mario Vento. Challenging the time com-
plexity of exact subgraph isomorphism for huge and dense graphs with VF3. IEEE Trans. Pattern
Anal. Mach. Intell., 40(4):804-818, 2018.
Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph
isomorphism testing and function approximation with gnns. In NeurIPS, 2019.
Kyunghyun Cho, Bart van Merrienboer, CagIar GUlCehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder
for statistical machine translation. In EMNLP, pp. 1724-1734, 2014.
Luigi P. Cordella, Pasquale Foggia, Carlo Sansone, and Mario Vento. A (sub)graph isomorphism
algorithm for matching large graphs. IEEE Trans. Pattern Anal. Mach. Intell., 26(10):1367-1372,
2004.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan
Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In
ACL, pp. 2978-2988, 2019.
Michele Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph do-
mains. IJCNN, 2:729-734, 2005.
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou,
Adria Puigdomenech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain,
Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis.
Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):
471-476, 2016.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In SIGKDD,
pp. 855-864, 2016.
William L. Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods
and applications. IEEE Data Eng. Bull., 40(3):52-74, 2017a.
William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In NIPS, pp. 1024-1034, 2017b.
Wook-Shin Han, Jinsoo Lee, and Jeong-Hoon Lee. Turboiso : towards ultrafast and robust subgraph
isomorphism search in large graph databases. In SIGMOD, pp. 337-348, 2013.
Huahai He and Ambuj K. Singh. Graphs-at-a-time: query language and access methods for graph
databases. In SIGMOD, pp. 405-418, 2008.
11
Under review as a conference paper at ICLR 2020
Jun Huan, Wei Wang, and Jan F. Prins. Efficient mining of frequent subgraphs in the presence of
isomorphism. In ICDM, pp. 549-552, 2003.
YoonKim. Convolutional neural networks for sentence classification. In EMNLP, pp. 1746-1751,
2014.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In ICLR, 2017.
Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor
Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for
natural language processing. In ICML, pp. 1378-1387, 2016.
Michihiro Kuramochi and George Karypis. GREW-A scalable frequent subgraph discovery al-
gorithm. In ICDM, pp. 439-442, 2004.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. Gated graph sequence neural
networks. In ICLR, 2016.
R.	Milo, S. Shen-Orr, S. Itzkovitz, N. Kashtan, D. Chklovskii, and U. Alon. Network motifs: simple
building blocks of complex networks. Science, 298(5594):824-827, October 2002.
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural net-
works for graphs. In ICML, pp. 2014-2023, 2016.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representa-
tions. In SIGKDD, pp. 701-710, 2014.
Franco Scarselli, Sweah Liang Yong, Marco Gori, Markus Hagenbuchner, Ah Chung Tsoi, and
Marco Maggini. Graph neural networks for ranking web pages. In Web Intelligence, pp. 666-
672. IEEE Computer Society, 2005.
Michael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and
Max Welling. Modeling relational data with graph convolutional networks. In The Semantic Web
- International Conference, ESWC, pp. 593-607, 2018.
Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M.
Borgwardt. Weisfeiler-lehman graph kernels. J. Mach. Learn. Res., 12:2539-2561, 2011.
Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks.
In NIPS, pp. 2440-2448, 2015.
Zhao Sun, Hongzhi Wang, Haixun Wang, Bin Shao, and Jianzhong Li. Efficient subgraph matching
on billion node graphs. PVLDB, 5(9):788-799, 2012.
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. LINE: large-scale
information network embedding. In WWW, pp. 1067-1077, 2015.
Matteo Togninalli, M. Elisabetta Ghisu, Felipe Llinares-Lopez, Bastian Rieck, and Karsten M.
Borgwardt. Wasserstein weisfeiler-lehman graph kernels. In NeurIPS, 2019.
Julian R. Ullmann. An algorithm for subgraph isomorphism. J. ACM, 23(1):31-42, 1976.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 5998-6008, 2017.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In ICLR, 2018.
S.	V. N. Vishwanathan, Nicol N. Schraudolph, Risi Kondor, and Karsten M. Borgwardt. Graph
kernels. J. Mach. Learn. Res., 11:1201-1242, 2010.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In ICLR, 2019.
12
Under review as a conference paper at ICLR 2020
Xifeng Yan and JiaWei Han. gspan: Graph-based substructure pattern mining. In ICDM, pp. 721-
724, 2002.
Xifeng Yan, Philip S. Yu, and JiaWei Han. Graph indexing: A frequent structure-based approach. In
SIGMOD, pp. 335-346, 2004.
Pinar Yanardag and S. V. N. VishWanathan. Deep graph kernels. In KDD, pp. 1365-1374. ACM,
2015.
Jiaxuan You, Rex Ying, Xiang Ren, William L. Hamilton, and Jure Leskovec. Graphrnn: Generating
realistic graphs With deep auto-regressive models. In ICML, pp. 5694-5703, 2018.
13
Under review as a conference paper at ICLR 2020
A	Lexicographic Order of Codes
The lexicographic order is a linear order defined as follows:
If A = (ao, aι,…，am) and B = (bo, bi,…，bn) are the codes, then A ≤ B iff either of the
following is true:
1.	∃t, 0 ≤ t ≤ min(m, n), ∀k < t,ak = bk, at Ye bt,
2.	∀0 ≤ k ≤ m, ak = bk , and n ≥ m.
In our setting, ai = (uai, vai, X (uai), yai, X (vai)) Ye bj = (ubj, vbj, X (ubj), ybj, X (vbj)) iff one
of the following is true:
1.	uai < ubj,
2.	uai = ubj , vai < vbj ,
3.	uai = ubj , vai = vbj , yai < ybj .
B	Pattern Generator and Graph Generator
As proposed in Section 5.1, two generators are required to generate datasets. The algorithm about
the pattern generator is shown in Algorithm 1. The algorithm first uniformly generates a directed
tree. Then it adds the remaining edges with random labels. Vertex labels and edge labels are also
uniformly generated but each label is required to appear at least once. Algorithm 2 shows the process
of graph generation. Two hyperparameters control the density of subisomorphisms: (1) α ∈ [0, 1]
decides the probability of adding subisomorphisms rather than random edges; (2) β ∈ N+ is the
parameter of Dirichlet distribution to sample sizes of components. After generating several directed
trees and satisfying the vertex number requirement, the algorithm starts to add remaining edges. It
can add edges in one component and try to add subgraph isomorphisms, or it can randomly add edges
between two components or in one component. The following merge subroutine aims to merge these
components into a large graph. Shuffling is also required to make datasets hard to be hacked. The
search of subisomorphisms in the whole graph is equivalent to the search in components respectively
because edges between any two components do not satisfy the necessary conditions.
Algorithm 1 Pattern Generator.
Input: the number of vertices Nv , the number of edges Ne, the number of vertex labels Lv, the
number of edge labels Le .
1:	P := GenerateDirectedTree(Nv)
2:	AssignNodesLabels(P, Lv )
3:	AddRandomEdges(P, P, null, Ne - Nv + 1)
4:	AssignEdgesLabels(P, Le)
Output: the generated pattern P
In Algorithm 1 and Algorithm 2, the function AddRandomEdges adds required edges from one
component to the other without generating new subgraph isomorphisms. The two component can
also be the same one, which means to add in one component. The NEC tree is utilized in Tur-
boISO (Han et al., 2013) to explore the candidate region for further matching. It takes O(|Vp|2)
time but can significant reduce the searching space in the data graph. It records the equivalence
classes and necessary conditions of the pattern. We make sure edges between two components dis-
satisfy necessary conditions in the NEC tree when adding random edges between them. This data
structure and this idea help us to generate more data and search subisomorphisms compared with
random generation and traditional subgraph isomorphism searching.
C Details of Datasets
We can generate as many examples as possible using two graph generators. However, we limit
the numbers of training, dev, and test examples whether learning based models can generalize to
14
Under review as a conference paper at ICLR 2020
Algorithm 2 Graph Generator.
Input: a pattern P, the number of vertices Nv, the number of edges Ne, the number of vertex labels
Lv , the number of edge labels Le, hyperparameters α and β.
1:	N s := DirichletSampling(Nv , β)
2:	Gs := {}
3:	de := Ne
4:	for each n in Ns do
5:	g := GenerateDirectedTree(n)
6:	AssignNodesLabels(g, n)
7:	AssignEdgesLabels(g, n - 1)
8:	Gs := Gs + {g}
9:	de := de - n + 1
10:	end for
11:	Tp := RewriteNECTree(P)
12:	Ne,P := EdgeCount(P)
13:	while de > 0 do
14:	g1 , g2 = RandomPick(Gs, 2)	. Pick two components randomly
15:	r = RandomNum(0,	1)
16:	if de < Ne,P then
17:	de := de - AddRandomEdges(g1, g2, Tp, de)	. Add de edges between g1 and g2
18:	else
19:	if r < α then
20:	de := de - AddPatterns(g1, P)	. Add necessary edges in g1 to add patterns
21:	else
22:	de := de - AddRandomEdges(g1, g2, Tp, Ne,P)
23:	end if
24:	end if
25:	end while
26:	G, f := MergeComponents(Gs)	. f is the graph id mapping
27:	G, f0 := ShuffleGraph(G)	. f0 is the shuffled graph id mapping
28:	Is := {}
29:	for each g in Gs do
30:	Is := SearchSubIsomorphisms(g, P)
31:	for each I in Is do
32:	I :=UpdateID(I, f, f0)
33:	Is := Is + {I}
34:	end for
35:	end for
Output: the generated graph G, the subgraph isomorphisms Is
new unseen examples. Pattern and data graphs in different scales have been generated. Parameters
for two generators and constraints are listed in Table 4. Two constraints are added to make this
task easier. The average degree constraint (NNe ≤ 4) is used to keep graphs not so dense, and the
subisomorphism counting constraints ({c ∈ N|c ≤ 1024} for small, {c ∈ N|c ≤ 4096} for large)
ensure the difficulty of two datasets are different.
We use a server with an 8-core Intel E5-2620v4 CPU (16 threads) and 512GB RAM to generate and
record running time in parallel. The distributions of countings of two datasets are shown in Figure 6.
From the figure we can see that both datasets follow the long tail distributions.
D More Implementation Details
In this section, we provide more implementation details other than the representation architectures
and the DIAMNet modules.
15
Under review as a conference paper at ICLR 2020
Table 4: Parameters and constraints for two datasets
I Hyperparameters	small	large
"ζ	N	{3, 4, 8}	{3,4, 8,16}
S	Ne	{2, 4, 8}	{2, 4, 8, 16}
营	Lv	{2, 4, 8}	{2, 4, 8, 16}
L	Le	{2, 4, 8}	{2, 4, 8, 16}
Nv	{8, 16, 32, 64}	{64, 128, 256, 512}
Ne	{8, 16,…，256}	{64, 128,…，2,048}
Lv	Lv	{4, 8, 16}	{16, 32, 64}
J	Le	{4, 8, 16}	{16, 32,64}
ɑ	{0.1,0.2,…，0.8}	{0.05, 0.1,0.15}
β	{512}	{512}
(a) Test data distribution of the small dataset.
Figure 6: Subgraph isomorphism counting distributions of two datasets.
(b) Test data distribution of the large dataset.
D. 1 Filter Network
Intuitively, not all vertices and edges in graphs can match certain subisomorphisms so we simply
add a FilterNet to adjust graph encoding as follows:
P = MaxPool(P),
Gb = GWG> ,
fi = σ(WF(Gi,： Θ p)),
Gi,: =fiGi,:,
(7)
(8)
(9)
(10)
where Gb is the graph representation in the pattern space, σ is the sigmoid function, fi is the gate
that decides to filter the jth vertex or the jth edge, WG ∈ Rdp ×dg and WF ∈ R1×dp are trainable.
Thanks to the multi-hot encoding, We can simply use Eq. (7) to accumulate label information of pat-
terns. After this filter layer, only relevant parts of the graphs will be passed to the next representation
layer.
D.2 AttnPool
The computation process of the AttnPool (Mean) is:
P = MeanPool(P),	(11)
g = MultiHead(p, G, G).	(12)
Combing with the pooling strategy to make P as P ∈ R1×d, the time complexity is decreased to
O(|VP | + |VG|) or O(|EP | + |EG|). We also implement three variants with sum-pooling, mean-
16
Under review as a conference paper at ICLR 2020
pooling, and max-pooling respectively. Results of AttnPool (Sum) and AttnPool (Max) are shown
in the Appendix E.
D.3 DIAMNet Initialization
There are many ways to initialize the memory {m10), ∙∙∙ , m^)}. In experiments, We simply ini-
tialize mi(0) by
S = b%,
k = |Vg| — (M — 1) ∙ s,
m(0) = MeanPool({G %.§
户 .ɪiʌ
Gi∙s+k-1} ),
(13)
(14)
(15)
where s is the stride and k is the kernel size. In Table 5, we compared two additional pooling
methods with MeanPool in Eq. (15).
Table 5: Detailed results of different models with different predict networks on the small dataset.
Models					Test			
	RMSE	MAE	PzerO	RZerO	F 1zero	Pnonzero	Rnonzero	F 1nonzero
SumPool	55.429	11.057	0.997	0.678	0.807	0.249	0.980	0.397
MeanPool	57.298	10.517	0.999	0.697	0.821	0.312	0.995	0.475
MaxPool	47.353	11.212	0.993	0.715	0.832	0.372	0.973	0.539
AttnPool (Sum)	57.800	15.642	0.753	0.825	0.787	0.747	0.655	0.698
CNN	AttnPool (Mean)	55.963	12.717	0.991	0.665	0.796	0.207	0.937	0.340
AttnPool (Max)	57.102	11.196	0.989	0.682	0.807	0.267	0.936	0.416
DIAMNet (SumInit)	43.312	10.284	0.900	0.871	0.885	0.789	0.833	0.810
DIAMNet (MeanInit)	34.448	6.953	0.980	0.805	0.884	0.623	0.952	0.753
DIAMNet (MaxInit)	35.055	7.188	0.980	0.802	0.882	0.617	0.951	0.748
SumPool	29.955	5.740	0.979	0.846	0.908	0.717	0.956	-0.819
MeanPool	31.010	6.447	0.971	0.859	0.912	0.746	0.942	0.833
MaxPool	30.824	6.236	0.994	0.771	0.869	0.532	0.982	0.690
AttnPool (Sum)	32.481	6.262	0.977	0.853	0.911	0.733	0.953	0.829
RNN	AttnPool (Mean)	31.857	6.025	0.985	0.827	0.899	0.672	0.967	0.793
AttnPool (Max)	41.186	11.011	0.948	0.888	0.917	0.811	0.908	0.857
DIAMNet (SumInit)	29.682	5.632	0.925	0.949	0.937	0.921	0.885	0.903
DIAMNet (MeanInit)	29.743	5.547	0.948	0.907	0.927	0.846	0.911	0.877
DIAMNet (MaxInit)	29.778	5.827	0.951	0.926	0.938	0.879	0.919	0.899
SumPool	34.391	7.042	0.976	0.833	0.899	0.690	0.947	-0.798
MeanPool	32.569	6.656	0.972	0.808	0.882	0.632	0.935	0.754
MaxPool	65.152	30.289	0.406	0.965	0.572	0.977	0.509	0.669
AttnPool (Sum)	55.166	12.748	0.870	0.832	0.851	0.721	0.778	0.748
TXL	AttnPool (Mean)	37.721	7.426	0.965	0.790	0.869	0.592	0.914	0.719
AttnPool (Max)	60.196	18.916	0.529	0.892	0.664	0.898	0.545	0.679
DIAMNet (SumInit)	32.659	6.904	0.907	0.940	0.923	0.908	0.861	0.884
DIAMNet (MeanInit)	31.649	6.680	0.961	0.847	0.900	0.725	0.921	0.811
DIAMNet (MaxInit)	37.877	8.317	0.914	0.884	0.899	0.810	0.856	0.832
SumPool	32.414	6.578	0.984	0.829	0.900	0.677	0.965	-0.796
MeanPool	33.829	7.152	0.981	0.795	0.878	0.599	0.951	0.735
MaxPool	50.851	9.707	0.982	0.780	0.869	0.559	0.952	0.704
AttnPool (Sum)	33.816	6.834	0.991	0.809	0.890	0.628	0.977	0.764
RGCN	AttnPool (Mean)	32.526	6.523	0.994	0.774	0.870	0.540	0.982	0.697
AttnPool (Max)	42.970	8.850	0.964	0.847	0.902	0.724	0.928	0.813
DIAMNet (SumInit)	29.459	5.688	0.964	0.871	0.915	0.772	0.931	0.844
DIAMNet (MeanInit)	28.712	5.782	0.924	0.912	0.918	0.859	0.877	0.868
DIAMNet (MaxInit)	29.345	5.501	0.937	0.918	0.927	0.866	0.896	0.881
SumPool	22.379	3.958	0.988	0.860	0.920	0.746	0.974	-0.845
MeanPool	22.483	4.254	0.987	0.833	0.903	0.685	0.971	0.803
MaxPool	42.434	7.900	0.994	0.780	0.874	0.554	0.984	0.709
AttnPool (Sum)	23.214	4.149	0.992	0.830	0.904	0.679	0.981	0.802
RGCN-SUM AttnPool (Mean)	24.875	5.131	0.998	0.725	0.840	0.399	0.991	0.569
AttnPool (Max)	35.037	6.845	0.987	0.794	0.880	0.593	0.967	0.735
DIAMNet (SumInit)	21.385	3.811	0.962	0.886	0.922	0.803	0.930	0.862
DIAMNet (MeanInit)	21.734	3.853	0.964	0.887	0.924	0.804	0.934	0.864
DIAMNet (MaxInit)	23.940	4.156	0.966	0.882	0.922	0.794	0.937	0.860
Zero	67.195	13.716	-10-	0.614	0.761	-00-	0.0	00~~
Avg	65.780	21.986	0.0	0.0	0.0	1.0	0.386	0.557
VF2	-00-	-00-	-10-	-10-	-10-	-10-	1.0	10-
17
Under review as a conference paper at ICLR 2020
E	Detailed Results of Different PredictNets
Simple pooling, attention based pooing, and attention with memory can be used to fuse pattern and
graph representations in our framework. AttnPool (Sum) and AttnPool (Max) can be regarded as
two variants of AttnPool (Mean) by replacing pooling methods. Table 5 shows results of different
representation models with different interaction networks in the small dataset. AttnPool (Mean) and
DIAMNet (MeanInit) usually perform better compared with other pooling methods.
F	Further Discussions
As shown in Figure 7, different interaction modules perform differently in different views. We can
find MaxPool always predicts higher counting values when the pattern is small and the graph is
large, while AttnPool always predicts very small numbers except when the pattern vertex size is 8,
and the graph vertex size is 64. The same result appears when we use edge sizes as the x-axis. This
observation shows that AttnPool has difficulties predicting counting values when either of the pattern
and the graph is small. It shows that attention focuses more on the zero vector we added rather than
the pattern pooling result. Our DIAMNet, however, performs the best in all pattern/graph sizes.
When the bins are ordered by vertex label sizes or edge label sizes, the performance of all the three
interaction modules among the distribution are similar. When bins are ordered by vertex label sizes,
we have the same discovery that AttnPool prefers to predict zeros when then patterns are small.
MaxPool fails when facing complex patterns with more vertex labels. DIAMNet also performs not
so good over these patterns. As for edge labels, results look good for MaxPool and DIAMNet but
AttnPool is not satisfactory.
As shown in Figure 8, different representation modules perform differently in different views. CNN
performs badly when the graph size is large (shown in Figure 8a and 8d) and patterns become
complicated (show in Figure 8g and 8j), which further indicates that CNN can only extract the local
information and suffers from issues when global information is need in larger graphs. RNN, on
the other hand, performs worse when the graph are large, especially when patterns are small (show
in Figure 8e), which is consistent with its nature, intuitively. On the contrary, RGCN-SUM with
DIAMNet is not affected by the edge sizes because it directly learns vertex representations rather
than edge representations.
18
Under review as a conference paper at ICLR 2020
(a) OV, MaxPool
(b) OV, AttnPool
(c) OV, DIAMNet
255075
2 1
UOw≡xld∕6≡luno3 篦』3AE
Uo=ɔ1pə.Id⅛DUAUn03 əmujəAE
50
pattern/graph edge size
R9rkB989Γ4k8989rk89
CIE 9 Z S ClE9ZS CI E 9 Z 5
ΓM 6 6 Csl 6 C 6 Csl QO C 6 C
JZZZ G 6^z寸寸寸 C 07888
Z S∙zsβz^∙✓寸 Tr 7‰b√s∙z
pattern/graph edge size
(d) OE , MaxPool	(e) OE , AttnPool	(f) OE , DIAMNet
16
(g)OX,MaxPool	(h)OX,AttnPool	(i) OX, DIAMNet
IlovpdjdZMUqlInOJ
H∣a≡
12
^∞Gs^∞Gs∞Ss
77 CTr — C oo 6
pattern/graph edge label size
令	S?	号	令	S?	g	S?	号
rΓ	rΓ C	Tr	V C	Q© G
pattern/graph edge label size
令 S? 号 令	号
7 Cr C Tr Tr C ∞ C
pattern/graph edge label size
(j) OY, MaxPool
(k)	OY , AttnPool
(l)	OY, DIAMNet

Figure 7: Model behaviors of three CNN based models on the small dataset. We compare MaxPool,
AttnPool, and DIAMNet in four different view settings. Here OV means that on the x-axis the bins
are ordered by the size of pattern/graph vertex, OE by the size of edge, OX by the size of vertex
label, and OY by the size of edge label. The orange color refers to the ground truth and the blue
color refers to the predictions.
19
Under review as a conference paper at ICLR 2020
々I Se91)
(9SZ C91)
W91)
(寸9 C91)
(ZIs∞)
(9SZOc)
(8Zl00)
Q900)
(ZISS
(9szS
(8ZlS
09S
CIS范
(9SZ M)
∞ZL 范
Q9或
pattern/graph vertex size
(c) OV, RGCN-SUM
(f) OE, RGCN-SUM
(b) OV, RNN
pattern/graph edge size
pattern/graph vertex size
(a) OV, CNN
pattern/graph edge size
(d) OE, CNN
(e) OE, RNN
(g)OX,CNN	(h)OX,RNN	(i)OX,RGCN-SUM
pattern/graph edge label size	pattern/graph edge label size	pattern/graph edge label size
(j) OY, CNN
(k) OY , RNN
(l) OY, RGCN-SUM
Figure 8: Model behaviors of three models on the large dataset. We compare CNN, RNN, and
RGCN-SUM with our DIAMNet in four different view settings. Here OV means that on the x-axis
the bins are ordered by the size of pattern/graph vertex, OE by the size of edge, OX by the size of
vertex label, and OY by the size of edge label. The orange color refers to the ground truth and the
blue color refers to the predictions.
20