Under review as a conference paper at ICLR 2020
MANAS: Multi-Agent Neural Architecture
Search
Anonymous authors
Paper under double-blind review
Ab stract
The Neural Architecture Search (NAS) problem is typically formulated as a graph
search problem where the goal is to learn the optimal operations over edges in
order to maximize a graph-level global objective. Due to the large architecture
parameter space, efficiency is a key bottleneck preventing NAS from its practical
use. In this paper, we address the issue by framing NAS as a multi-agent problem
where agents control a subset of the network and coordinate to reach optimal
architectures. We provide two distinct lightweight implementations, with reduced
memory requirements (1/8th of state-of-the-art), and performances above those of
much more computationally expensive methods. Theoretically, We demonstrate
vanishing regrets of the form O(√T), with T being the total number of rounds. Fi-
nally, aWare that random search is an (often ignored) effective baseline We perform
additional experiments on 3 alternative datasets and 2 network configurations, and
achieve favorable results in comparison with this baseline and other methods.
1	Introduction
Determining an optimal architecture is key to accurate deep neural networks (DNNs) with good
generalisation properties (Szegedy et al., 2017; Huang et al., 2017; He et al., 2016; Han et al.,
2017; Conneau et al., 2017; Merity et al., 2018). Neural architecture search (NAS), which has been
formulated as a graph search problem, can potentially reduce the need for application-specific expert
designers allowing for a wide-adoption of sophisticated networks in various industries. Zoph and Le
(2017) presented the first modern algorithm automating structure design, and showed that resulting
architectures can indeed outperform human-designed state-of-the-art convolutional networks (Ko,
2019; Liu et al., 2019). However, even in the current settings where flexibility is limited by expertly-
designed search spaces, NAS problems are computationally very intensive with early methods
requiring hundreds or thousands of GPU-days to discover state-of-the-art architectures (Zoph and Le,
2017; Real et al., 2017; Liu et al., 2018a;b).
Researchers have used a wealth of techniques ranging from reinforcement learning, where a controller
network is trained to sample promising architectures (Zoph and Le, 2017; Zoph et al., 2018; Pham
et al., 2018), to evolutionary algorithms that evolve a population of networks for optimal DNN
design (Real et al., 2018; Liu et al., 2018b). Alas, these approaches are inefficient and can be
extremely computationally and/or memory intensive as some require all tested architectures to be
trained from scratch. Weight-sharing, introduced in ENAS (Pham et al., 2018), can alleviate this
problem. Even so, these techniques cannot easily scale to large datasets, e.g., ImageNet. More recently,
gradient-based frameworks enabled efficient solutions by introducing a continuous relaxation of the
search space. For example, DARTS (Liu et al., 2019) uses this relaxation to optimise architecture
parameters using gradient descent in a bi-level optimisation problem, while SNAS (Xie et al., 2019)
updates architecture parameters and network weights under one generic loss. Still, due to memory
constraints the search has to be performed on 8 cells, which are then stacked 20 times for the final
architecture. This solution is a coarse approximation to the original problem as show in Section 6
of this work and in Doe (2019). In fact, we show that searching directly over 20 cells leads to a
reduction in test error (0.24 p.p.; 8% relative to Liu et al., 2019). ProxylessNAS (Cai et al., 2019) is
one exception, as it can search for the final models directly; nonetheless they still require twice the
amount of memory used by our proposed algorithm.
1
Under review as a conference paper at ICLR 2020
To enable the possibility of large-scale joint optimisation of deep architectures we contribute MANAS,
the first multi-agent learning algorithm for neural architecture search. Our algorithm combines the
memory and computational efficiency of multi-agent systems, which is achieved through action
coordination with the theoretical rigour of online machine learning, allowing us to balance exploration
versus exploitation optimally. Due to its distributed nature, MANAS enables large-scale optimisation
of deeper networks while learning different operations per cell. Theoretically, we demonstrate that
MANAS implicitly coordinates learners to recover vanishing regrets, guaranteeing convergence.
Empirically, we show that our method achieves state-of-the-art accuracy results among methods using
the same evaluation protocol but with significant reductions in memory (1/8th of Liu et al., 2019) and
search time (70% of Liu et al., 2019).
The multi-agent (MA) framework is inherently scalable and allows us to tackle an optimization
problem that would be extremely challenging to solve efficiently otherwise: the search space of a
single cell is 814 and there is no fast way of learning the joint distribution, as needed by a single
controller. More cells to learn exacerbates the problem, and this is why MA is required, as for each
agent the size of the search space is always constant.
In short, our contributions can be summarised as: (1) framing NAS as a multi-agent learning problem
(MANAS) where each agent supervises a subset of the network; agents coordinate through a credit
assignment technique which infers the quality of each operation in the network, without suffering
from the combinatorial explosion of potential solutions. (2) Proposing two lightweight implemen-
tations of our framework that are theoretically grounded. The algorithms are computationally and
memory efficient, and achieve state-of-the-art results on Cifar-10 and ImageNet when compared with
competing methods. Furthermore, MANAS allows search directly on large datasets (e.g. ImageNet).
(3) Presenting 3 news datasets for NAS evaluation to minimise algorithmic overfitting; and offering a
fair comparison with a random baseline.
2	Related work
MANAS derives its search space from DARTS (Liu et al., 2019) and is therefore most related to other
gradient-based NAS methods that use the same search space. SNAS (Xie et al., 2019) appears similar
at a high level, but has important differences: 1) it uses GD to learn the architecture parameters.
This requires a differentiable objective (MANAS does not) and leads to 2) having to forward all
operations (see their Eqs.5,6), thus negating any memory advantages (which MANAS has), and
effectively requiring repeated cells and preventing search on ImageNet. ENAS (Pham et al., 2018)
is also very different: its use of RL implies dependence on past states (the previous operations
in the cell). It explores not only the stochastic reward function but also the relationship between
states, which is where most of the complexity lies. Furthermore, RL has to balance exploration and
exploitation by relying on sub-optimal heuristics, while MANAS, due to its theoretically optimal
approach from online learning, is more sample efficient. Finally, ENAS uses a single LSTM (which
adds complexity and problems such as exploding/vanishing gradients) to control the entire process,
and is thus following a monolithic approach. Indeed, at a high level, our multi-agent framework
can be seen as a way of decomposing the monolithic controller into a set of simpler, independent
sub-policies. This provides a more scalable and memory efficient approach that leads to higher
accuracy, as confirmed by our experiments.
3	Preliminary: Neural Architecture Search
We consider the NAS problem as formalised in DARTS (Liu et al., 2019). At a higher level, the
architecture is composed of a computation cell that is a building block to be learned and stacked
in the network. The cell is represented by a directed acyclic graph with V nodes and N edges;
edges connect all nodes i, j from i to j where i < j . Each vertex x(i) is a latent representation
for i ∈ {1, . . . , V }. Each directed edge (i, j) (with i < j) is associated with an operation o(i,j)
that transforms x(i) . Intermediate node values are computed based on all of its predecessors as
x(j) = Pi<j o(i,j)(x(i)). For each edge, an architect needs to intelligently select one operation o(i,j)
from a finite set of K operations, O = {θk(∙)}3ι, where operations represents some function to be
applied to x(i) to compute x(j), e.g., convolutions or pooling layers. To each oki,j)(∙) is associated a
2
Under review as a conference paper at ICLR 2020
set of operational weights wk(i,j) that needs to be learned (e.g. the weights of a convolution filter).
Additionally, a parameter α(ki,j) ∈ R characterises the importance of operation k within the pool O
for edge (i, j). The sets of all the operational weights {wk(i,j)} and architecture parameters {α(ki,j)}
are denoted by W and α, respectively. DARTS defined the operation 0(i,j) (x) as
K	α(i,j)
o(i,j)(χ) = X PK” a(i,j) ∙ oki,j)(X)	⑴
k=1	k0=1 e k0
in which α encodes the network architecture. The optimal choice of architecture is defined by
α? = min L(Val)(α, w*(α)) s.t. w?(a) = arg min L(train)(α, w).	(2)
αw
The final objective is to obtain a sparse architecture Z? = {Z(i,j)}, ∀i, j where Z(i,j) =
[z1(i,j), . . . , zK(i,j)] with zk(i,j) = 1 for k corresponding to the best operation and 0 otherwise. That is,
for each pair (i, j ) a single operation is selected.
4 Online Multi-agent Learning for AutoML
NAS suffers from a combinatorial explosion
in its search space. A recently proposed ap-
proach to tackle this problem is to approxi-
mate the discrete optimisation variables (i.e.,
edges in our case) with continuous counterparts
and then use gradient-based optimisation meth-
ods. DARTS (Liu et al., 2019) introduced this
method for NAS, though it suffers from two im-
portant drawbacks. First, the algorithm is mem-
ory and computationally intensive (O(NK)
with K being total number of operations be-
tween a pair of nodes and N the number of
nodes) as they require loading all operation pa-
rameters into GPU memory. As a result, DARTS
only optimises over a small subset of 8 cells,
which are then stacked together to form a deep
network of 20. Naturally, such an approxima-
tion is bound to be sub-optimal. Second, evalu-
ating an architecture amounts to a prediction on
a validation set using the optimal set of network
parameters. Learning these, unfortunately, is
Agents choose successor operations
agent
Figure 1: MANAS with single cell. Between
each pair of nodes, an agent Ai selects action a(i)
according to ∏(i). Feedback from the validation
loss is used to update the policy.
highly demanding since for an architecture Zt , one
would like to compute Lt(val) (Zt, wt?) where
wt? = arg minw L(ttrain)(w, Zt). DARTS, uses weight sharing that updates wt once per architecture,
with the hope of tracking wt? over learning rounds. Although this technique leads to significant speed
up in computation, it is not clear how this approximation affects the validation loss function.
Next, we detail a novel methodology based on a combination of multi-agent and online learning
to tackle the above two problems (Figure 1). Multi-agent learning scales our algorithm, reducing
memory consumption by an order of magnitude from O(NK) to O(N); and online learning enables
rigorous understanding of the effect of tracking wt? over rounds.
4.1	NAS as a multi-agent problem
To address the computational complexity we use the weight sharing technique of DARTS. However,
we try to handle in a more theoretically grounded way the effect of approximation of Lt(val) (Zt, wt?)
by Lt(val) (Zt, wt). indeed, such an approximation can lead to arbitrary bad solutions due to the
uncontrollable weight component. To analyse the learning problem with no stochastic assumptions
on the process generating ν = {L1, . . . , LT} we adopt an adversarial online learning framework.
3
Under review as a conference paper at ICLR 2020
Algorithm 1 GENERAL FRAMEWORK: [steps with asterisks (*) are specified in section 5]
1:	Initialize: π1i is uniform random over all j ∈ {1, . . . N}. And random w1 weights.
2:	Fort = 1, . . . ,T
3:	* Agent Ai samples at 〜∏i(at) for all i ∈ {1,...,N}, forming architecture Zt.
4:	Compute the training loss Lt(train) (at) = Lt(train) (Zt, wt)
5:	Update wt+1 for all operation ait in Zt from wt using back-propagation.
6:	Compute the validation loss L(tval) (at) = Lt(val) (Zt, wt+1)
7:	* Update πti+1 for all i ∈ {1, . . . N} using Z1, . . . , Zt and L(1val), . . . , Lt(val).
8:	Recommend ZT +1, after round T, where aT+1〜∏T +ι(aT +1) for all i ∈ {1,..., N}.
NAS as Multi-Agent Combinatorial Online Learning. In Section 3, we defined a NAS problem
where one out of K operations needs to be recommended for each pair of nodes (i, j) in a DAG. In
this section, we associate each pair of nodes with an agent in charge of exploring and quantifying the
quality of these K operations, to ultimately recommend one. However, the only feedback for each
agent is the loss that is associated with a global architecture Z, which depends on all agents’ choices.
We introduce N decision makers, A1, . . . , AN (see Figure 1 and Algorithm 1). At training round
t, each agent chooses an operation (e.g., convolution or pooling filter) according to its local action-
distribution (or policy) aj 〜πj, for all j ∈ {1,...,N} with aj ∈ {1,...,K}. These operations
have corresponding operational weights wt that are learned in parallel. Altogether, these choices at =
at1 , . . . , atN define a sparse graph/architecture Zt ≡ at for which a validation loss Lt(val) (Zt, wt)
is computed and used by the agents to update their policies. After T rounds, an architecture is
recommended by sampling a!T+]〜 ∏j+「for all j ∈ {1,...,N}. These dynamics resemble bandit
algorithms where the actions for an agent Aj are viewed as separate arms. This framework leaves
open the design of 1) the sampling strategy πj and 2) how πj is updated from the observed loss.
Minimization of worst-case regret under any loss. The following two notions of regret motivate
our proposed NAS method. Given a policy π the cumulative regret RT ∏ and the simple regret rTT ∏
after T rounds and under the worst possible environment ν, are:
T
T
T
RTn = SuPEXLt(at)-
ν t=1
min	Lt(a), rT? π = supE	Lt(aT+1) -
aν
t=1	ν t=1
T
min X Lt(a) (3)
a
where the expectation is taken over both the losses and policy distributions and a
t=1
= {a(Aj) }jN=1
denotes a joint action profile. The simple regret leads to minimising the loss of the recommended
architecture aT+1 , while minimising the cumulative regret adds the extra requirement of having to
sample, at any time t, architectures with close-to-optimal losses. We discuss in the appendix E how
this requirement could improve in practice the tracking of wt? by wt. We let Lt (at) be potentially
adversarilly designed to account for the difference between wt? and wt and make no assumption on
its convergence. Our models and solutions in Section 5 are designed to be robust to arbitrary Lt (at).
5 S olution Methods
This section elaborates our solution methods for NAS when considering adversarial losses. We
propose two algorithms, MANAS and MANAS-LS, that implement two different credit assignment
techniques specifying the update rule in line 7 of Algorithm 1. The first approximates the validation
loss as a linear combination of edge weights, while the second handles non-linear loss. We propose
two associated sampling techniques that specify line 3 of Algorithm 1, one minimising the simple
regret rT,∏ and one targeting the cumulative regret RT,∏, (3).
Agent coordination, combinatorial explosion and approximate credit assignment. Our set-up
introduces multiple agents in need of coordination. Centralised critics use explicit coordination and
learn the value of coordinated actions across all agents (Rashid et al., 2018), but the complexity of
the problem grows exponentially with the number of possible architectures Z, which equals KN . We
argue instead for an implicit approach where coordination is achieved through a joint loss function
depending on the actions of all agents. This approach is scalable as each agent searches its local
4
Under review as a conference paper at ICLR 2020
action space—small and finite—for optimal action-selection rules. Both credit assignment methods
below learn, for each operation k belonging to an agent Ai, a quantity Bti [k] (similar to α in Section
3) that quantifies the contribution of the operation to the observed losses.
5.1 MANAS-LS
Linear Decomposition of the Loss. A simple credit assignment strategy is to approximate edge-
importance (or edge-weight) by a vector βs ∈ RKN representing the importance of all K operations
for each of the N agents. βs is an arbitrary, potentially adversarially-chosen vector and varies
with time s to account for the fact that the operational weights ws are learned online and to avoid
any restrictive assumption on their convergence. The relation between the observed loss L(sval) and
the architecture selected at each sampling stage s is modeled through a linear combination of the
architecture’s edges (agents’ actions) as
L(sval) = βsT Zs
(4)
where Zs ∈ {0, 1}KN is a vectorised version of the architecture Zs containing all action choices.
After evaluating S architectures, at round t we estimate β by solving the following via least-squares:
S2
Credit assignment: Bet = min X L(sval) - βT Zs	.
β s=1
(5)
Though simple, the solution gives an efficient way for agents to update their corresponding action-
selection rules which they implicitly coordinate. Indeed, in Appendix C we demonstrate that the
worst-case regret R?T (3) can actually be decomposed into an agent-specific form RiT πi , νi
defined in the appendix: RT = SuPV RT(∏, V) ^⇒ SuPVi RT (∏i, Vi), i = 1,...,N. This
decomposition allows us to significantly reduce the search space and apply upcoming sampling
techniques for each agent Ai in a completely parallel fashion.
Zipf Sampling for rTT,∏. Ai samples an operation k proportionally to the inverse of its estimated
i
i
〜
rank hkit, where hkit is computed by sorting the operations of agent Ai w.r.t Bti[k], as
一.	一.	√ 「-r	i 77^f i7   一 7   .	. ，一	.， 
Sampling policy: ∏ii+Jk] = 1 / WogK where IogK = 1 + 1/2 + ... + 1/K.
Zipf explores efficiently as, up to log factors, for 1 ≤ m ≤ K, the m estimated best operations are
picked uniformly ignoring the remaining K - m operations: All operations are explored almost as in
uniform exploration while the estimated best is picked almost all the time. The Zipf law is anytime,
parameter free, minimises optimally the simple regret in multi-armed bandits when the losses are
adversarially designed and adapts optimally to stationary losses (Abbasi-Yadkori et al., 2018).
5.2 MANAS
Coordinated Descent for Non-Linear Losses. As the linear approximation is likely to be crude,
an alternative is to make no assumption on the loss function and have each agent directly associate
the quality of their action with the loss Lt(val)(at). This results in obtaining all the agents performing
a coordinated descent approach to the problem. Each agent updates for operation k its Bti [k] as


一	一.	二；「一 r	二；	「一 r
Credit assignment: Beti [k] = Beti-1 [k] +
L(Val) lat = k∕∏i[k].
(6)
Softmax Sampling for RT ∏. Based on EXP3 (Auer et al., 2002), samples are from a Softmax
distribution (with temperature η) w.r.t. Bti[k] and the aim is to always pull the best operation as
K
Sampling policy: ∏i+ι[k] = exp (η⅛i[k]) / Xexp (ηBi[j]) for k =1,...,K.
j=1
Comments on credit assignment. Our MA formulation provides a gradient-free, credit assignment
strategy. Gradient methods are more susceptible to bad initialisation and can get trapped in local
5
Under review as a conference paper at ICLR 2020
minima more easily than our approach, which, not only explores more widely the search space,
but makes this search in an optimal way, given by the multi-armed bandit/multi-agent framework.
Concretely, MANAS can easily escape from local minima as the reward is scaled by the probability
of selecting an action (Eq. 6). Thus, the algorithm has a higher chance of revising its estimate of the
quality of a solution based on new evidence. This is important as one-shot methods (such as MANAS
and DARTS) change the network—and thus the environment—throughout the search process. Put
differently, MANAS’ optimal exploration-exploitation allows the algorithm to move away from
’good’ solutions towards ’very good’ solutions that do not live in the former’s proximity; in contrast,
gradient methods will tend to stay in the vicinity of a ’good’ discovered solution.
5.3 Theoretical guarantees
MANAS. This algorithms runs EXP3 (Auer et al., 2002) for each agent in parallel. If the re-
gret of each agent is computed by considering the rest of the agent as fixed, then each agent has
regret O (√TK log K) which sums over agents to O (N√TK log K). The proof in given in
Appendix D.2.
MANAS-LS. We prove for this new algorithm an exponentially decreasing simple regret rT? =
O (e-T/H), where H is a measure of the complexity for discriminating sub-optimal solutions
as H = N(minj=k?,vwN} BT[j] - BT[k?]), where k? = minι≤j≤κ BT[j]) and BT[j]=
PtT=1 βt(Ai) [j]. The proof in given in Appendix D.1.
6	Experiments Results
This section, we (1) compare MANAS against existing NAS methods on the well established Cifar-10
dataset. (2) evaluate MANAS on ImageNet. (3) compare MANAS, DARTS and random sampling on
3 new datasets. Descriptions of the datasets and details of the search are provided in the Appendix.
We report the performance of two algorithms, MANAS and MANAS-LS, described in Section 5.
Search Spaces: we use the same CNN search space as Liu et al. (2019). Since MANAS is memory
efficient, it can search for the final architecture without needing to stack a posteriori repeated
cells, and so our cells are unique. For fair comparison, we use 20 cells on Cifar-10 and 14 on
ImageNet. Experiments on Sport-8, Caltech-101 and MIT-67 in Section 6.3 use both 8 and 14 cell
networks. Search Protocols: for datasets other than ImageNet, we use 500 epochs during the search
phase for architectures with 20 cells, 400 epochs for 14 cells, and 50 epochs for 8 cells. All other
hyperparameters are as in Liu et al. (2019). For ImageNet, we use 14 cells and 100 epochs during
search. In our experiments on the three new datasets we rerun the DARTS code to optimise an 8 cell
architecture; for 14 cells we simply stacked the best cells for the appropriate number of times.
Synthetic experiment. To illustrate the theoretical properties of MANAS we apply it to the
Gaussian Squeeze Domain experiment, a problem where agents must coordinate their actions in order
to optimize a Gaussian objective function (Colby et al., 2015). MANAS progresses steadily towards
zero regret while the Random Search baseline struggles to move beyond the initial starting point.
Details and results are provided in Appendix F.
6.1	Results on Cifar- 1 0
Evaluation. To evaluate our NAS algorithm, we follow DARTS’s protocol: we run MANAS 4 times
with different random seeds and pick the best architecture based on its validation performance. We
then randomly reinitialize the weights and retrain for 600 epochs. During search we use half of the
training set as validation. To fairly compare with more recent methods, we also re-train the best
searched architecture using AutoAugment and Extended Training (Cubuk et al., 2018).
Results. Both MANAS implementations perform well on this dataset (Table 1). Our algorithm is
designed to perform comparably to Liu et al. (2019) but with an order of magnitude less memory.
However, MANAS actually achieves higher accuracy. The reason for this is that DARTS is forced
to search for an 8 cell architecture and subsequently stack the same cells 20 times; MANAS, on
the other hand, can directly search on the final number of cells leading to better results. We also
report our results when using only 8 cells: even though the network is much smaller, it still performs
6
Under review as a conference paper at ICLR 2020
Table 1: Comparison with state-of-the-art image classifiers on Cifar-10. The four row blocks represent: human-designed, NAS, MANAS search with DARTS training protocol and best searched MANAS retrained with extended protocol (AutoAugment + 1500 Epochs + 50 Channels).				
Architecture	Test Error (%)	Params (M)	Search Cost (GPU days)	Search Method
DenseNet-BC (Huang et al., 2017)	3.46	25.6	—	manual
NASNet-A (Zoph et al., 2018)	2.65	3.3	1800	RL
AmoebaNet-B (Real et al., 2018)	2.55	2.8	3150	evolution
PNAS (Liu et al., 2018a)	3.41	3.2	225	SMBO
ENAS (Pham et al., 2018)	2.89	4.6	0.5	RL
SNAS (Xie et al., 2019)	2.85	2.8	1.5	gradient
DARTS, 1st order (Liu et al., 2019)	3.00	3.3	1.5*	gradient
DARTS, 2nd order (Liu et al., 2019)	2.76	3.3	4*	gradient
Random + cutout (Liu et al., 2019)	3.29	3.2	—	—
MANAS (8 cells)	3.05	1.6	0.8*	MA
MANAS (20 cells)	2.63	3.4	2.8*	MA
MANAS-LS (20 cells)	2.52	3.4	4*	MA
MANAS (20 cells) + AutoAugment	1.97	3.4	—	MA
MANAS-LS (20 cells) + AutoAugment	1.85	3.4	—	MA
* Search cost is for 4 runs and test error is for the best result (for a fair comparison With other methods).
Table 2: Comparison with state-of-the-art image classifiers on ImageNet (mobile setting). The four
roW blocks represent: human-designed, NAS, MANAS search With DARTS training protocol and best
searched MANAS retrained with extended protocol (AutoAugment + 600 Epochs + 60 Channels).
Architecture	Test Error (%)	Params (M)	Search Cost (GPU days)	Search Method
ShuffleNet 2x (v2) (Zhang et al., 2018)	26.3	5	—	manual
NASNet-A (Zoph et al., 2018)	26.0	5.3	1800	RL
AmoebatNet-C (Real et al., 2018)	24.3	6.4	3150	evolution
PNAS (Liu et al., 2018a)	25.8	5.1	225	SMBO
SNAS (Xie et al., 2019)	27.3	4.3	1.5	gradient
DARTS (Liu et al., 2019)	26.7	4.7	4	gradient
Random sampling	27.75	2.5	—	—
MANAS (search on C10)	26.47	2.6	2.8	MA
MANAS (search on IN)	26.15	2.6	110	MA
MANAS (search on C10) + AutoAugment	26.81	2.6	—	MA
MANAS (search on IN) + AutoAugment	25.26	2.6	—	MA
competitively With 1st-order 20-cell DARTS. This is explored in more depth in Section 6.3. Cai et al.
(2019) is another method designed as an efficient alternative to DARTS; unfortunately the authors
decided to a) use a different search space (PyramidNet backbone; Han et al., 2017) and b) offer no
comparison to random sampling in the given search space. For these reasons we feel a numerical
comparison to be unfair. Furthermore our algorithm uses half the GPU memory (they sample 2 paths
at a time) and does not require the reward to be differentiable. Lastly, we observe similar gains when
training the best MANAS/MANAS-LS architectures with an extended protocol (AutoAugment +
1500 Epochs + 50 Channels, in addition to the DARTS protocol).
6.2	Results on ImageNet
Evaluation. To evaluate the results on ImageNet we train the final architecture for 250 epochs. We
report the result of the best architecture out of 4, as chosen on the validation set for a fair comparison
with competing methods. As search and augmentation are very expensive we use only MANAS and
not MANAS-LS, as the former is computationally cheaper and performs slightly better on average.
7
Under review as a conference paper at ICLR 2020
Figure 2: Comparing MANAS, random sampling and DARTS (Liu et al., 2019) on 8 and 14 cells.
Average results of 8 runs. Note that DARTS was only optimised for 8 cells due to memory constraints.
Results. We provide results for networks searched both on Cifar-10 and directly on ImageNet, which
is made possible by the computational efficiency of MANAS (Table 2). When compared to SNAS
and DARTS—currently the most efficient methods, using the same search space, available—MANAS
achieves state-of-the-art results both with architectures searched directly on ImageNet (0.85 p.p.
improvement) and also with architectures transferred from Cifar-10 (0.55 p.p. improvement). We
observe similar gains when training the best MANAS architecture with an extended training protocol
(AutoAugment + 600 Epochs + 60 Channels, in addition to the DARTS protocol).
6.3	Results on new datasets: Sport-8, Caltech-101, MIT-67
Evaluation. The idea behind NAS is that of finding the optimal architecture, given any sets of data
and labels. Limiting the evaluation of current methods to Cifar-10 and ImageNet could potentially
lead to algorithmic overfitting. Indeed, recent results suggest that the search space was engineered
in a way that makes it very hard to find a a bad architecture (Li and Talwalkar, 2019; Sciuto et al.,
2019). To mitigate this, we propose testing NAS algorithms on 3 datasets (composed of regular sized
images) that were never before used in this setting, but have been historically used in the CV field:
Sport-8, Caltech-101 and MIT-67, described briefly in the Appendix. For these set of experiments we
run the algorithm 8 times and report mean and std. We perform this both for 8 and 14 cells; we do the
same with DARTS (which, due to memory constraints can only search for 8 cells). For our random
baseline we sample uniformly 8 architectures from the search space. Each proposed architecture is
then trained from scratch for 600 epochs as in the previous section.
Results. For these experiments can be found in Figure 2. MANAS manages to outperform the
random baseline and significantly outperform DARTS, especially on 14 cells. It can be clearly seen
from our experiments, that the optimal cell architecture for 8 cells is not the optimal one for 14 cells.
Discussion on Random Search. Clearly, in specific settings, random sampling performs very
competitively. On one hand, since the search space is very large (between 8112 and 8280 architectures
exist in the DARTS experiments; Liu et al., 2019), finding the global optimum is practically impossible.
Why is it then that the randomly sampled architectures are able to deliver nearly state-of-the-art
results? Previous experiments (Sciuto et al., 2019; Li and Talwalkar, 2019) together with the results
presented here seem to indicate that the available operations and meta-structure have been carefully
chosen and, as a consequence, most architectures in this space generate meaningful results. This
suggests that human effort has simply transitioned from finding a good architecture to finding a good
search space - a problem that needs careful consideration in future work.
7	Conclusions
We presented MANAS, a theoretically grounded multi-agent online learning framework for NAS.
We then proposed two extremely lightweight implementations that, within the same search space,
outperform state-of-the-art while reducing memory consumption by an order of magnitude compared
to Liu et al. (2019). We provide vanishing regret proofs for our algorithms. Furthermore, we evaluate
MANAS on 3 new datasets, empirically showing its effectiveness in a variety of settings. Finally,
we confirm concerns raised in recent works (Sciuto et al., 2019; Li and Talwalkar, 2019; Doe, 2019)
claiming that NAS algorithms often achieve minor gains over random architectures. We however
demonstrate, that MANAS still produces competitive results with limited computational budgets.
8
Under review as a conference paper at ICLR 2020
References
Yasin Abbasi-Yadkori, Peter Bartlett, Victor Gabillon, Alan Malek, and Michal Valko. Best of
both worlds: Stochastic & adversarial best-arm identification. In Conference on Learning Theory
(COLT), 2018.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E SchaPire. The nonstochastic multiarmed
bandit problem. SIAM journal on computing, 32(1):48-77, 2002.
Sebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic
multi-armed bandit problems. Foundations and TrendsR in Machine Learning, 5(1):1-122, 2012.
Han Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct neural architecture search on target task
and hardware. In International Conference on Learning Representations (ICLR), 2019.
Nicolo Cesa-Bianchi and Gabor Lugosi. Combinatorial bandits. Journalof Computer and System
Sciences, 78(5):1404-1422, 2012.
Mitchell K Colby, Sepideh Kharaghani, Chris HolmesParker, and Kagan Tumer. Counterfactual
exploration for improving multiagent learning. In Autonomous Agents and Multiagent Systems
(AAMAS 2015), pages 171-179. International Foundation for Autonomous Agents and Multiagent
Systems, 2015.
Alexis Conneau, Holger Schwenk, Loic Barrault, and Yann Lecun. Very deep convolutional networks
for text classification. In European Chapter of the Association for Computational Linguistics:
Volume 1, Long Papers, pages 1107-1116, 2017.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation policies from data. arXiv:1805.09501, 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale
hierarchical image database. In Computer Vision and Pattern Recognition (CVPR), pages 248-255,
2009.
J Doe. NAS evaluation is frustratingly hard. To appear, 2019.
Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training
examples: An incremental bayesian approach tested on 101 object categories. Computer Vision
and Image Understanding, 106(1):59-70, 2007.
David A. Freedman. On tail probabilities for martingales. The Annals of Probability, pages 100-118,
1975.
Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyramidal residual networks. In Computer
Vision and Pattern Recognition (CVPR), pages 5927-5935, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Computer Vision and Pattern Recognition (CVPR), pages 770-778, 2016.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Computer Vision and Pattern Recognition (CVPR), pages 4700-4708,
2017.
ByungSoo Ko. Imagenet classification leaderboard. https://kobiso.github.io/
Computer-Vision-Leaderboard/imagenet, 2019.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University
of Toronto, 2009.
Li-Jia Li and Li Fei-Fei. What, where and who? classifying events by scene and object recognition.
In International Conference on Computer Vision (ICCV), pages 1-8, 2007.
Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search.
arXiv:1902.07638, 2019.
9
Under review as a conference paper at ICLR 2020
Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan
Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In European
Conference on Computer Vision (ECCV), pages 19-34, 2018a.
Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. Hierar-
chical representations for efficient architecture search. In International Conference on Learning
Representations (ICLR), 2018b.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In
International Conference on Learning Representations (ICLR), 2019.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM
language models. In International Conference on Learning Representations (ICLR), 2018.
Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture
search via parameter sharing. In International Conference on Machine Learning (ICML), pages
4092-4101, 2018.
Ariadna Quattoni and Antonio Torralba. Recognizing indoor scenes. In Computer Vision and Pattern
Recognition (CVPR), pages 413-420, 2009.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder Witt, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. QMIX: Monotonic value function factorisation for deep multi-agent
reinforcement learning. In International Conference on Machine Learning (ICML), pages 4292-
4301, 2018.
Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V
Le, and Alexey Kurakin. Large-scale evolution of image classifiers. In International Conference
on Machine Learning (ICML), pages 2902-2911, 2017.
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image
classifier architecture search. arXiv:1802.01548, 2018.
Christian Sciuto, Kaicheng Yu, Martin Jaggi, Claudiu Musat, and Mathieu Salzmann. Evaluating the
search phase of neural architecture search. arXiv:1902.08142, 2019.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, Inception-
ResNet and the impact of residual connections on learning. In AAAI Conference on Artificial
Intelligence, 2017.
Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: Stochastic neural architecture search.
In International Conference on Learning Representations (ICLR), 2019.
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. ShuffleNet: An extremely efficient
convolutional neural network for mobile devices. In Conference on Computer Vision and Pattern
Recognition (CVPR), pages 6848-6856, 2018.
Barret Zoph and Quoc Le. Neural architecture search with reinforcement learning. In International
Conference on Learning Representations (ICLR), 2017.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures
for scalable image recognition. In Computer Vision and Pattern Recognition (CVPR), pages
8697-8710, 2018.
10
Under review as a conference paper at ICLR 2020
A Datasets
Cifar-10. The CIFAR-10 dataset (Krizhevsky, 2009) is a dataset of 10 classes and consists of
50, 000 training images and 10, 000 test images of size 32×32. We use standard data pre-processing
and augmentation techniques, i.e. subtracting the channel mean and dividing the channel standard
deviation; centrally padding the training images to 40×40 and randomly cropping them back to
32×32; and randomly flipping them horizontally.
ImageNet. The ImageNet dataset (Deng et al., 2009) is a dataset of 1000 classes and consists
of 1, 281, 167 training images and 50, 000 test images of different sizes. We use standard data
pre-processing and augmentation techniques, i.e. subtracting the channel mean and dividing the
channel standard deviation, cropping the training images to random size and aspect ratio, resizing
them to 224×224, and randomly changing their brightness, contrast, and saturation, while resizing
test images to 256×256 and cropping them at the center.
Sport-8. This is an action recognition dataset containing 8 sport event categories and a total of 1579
images (Li and Fei-Fei, 2007). The tiny size of this dataset stresses the generalization capabilities of
any NAS method applied to it.
Caltech-101. This dataset contains 101 categories, each with 40 to 800 images of size roughly
300×200 (Fei-Fei et al., 2007).
MIT-67. This is a dataset of 67 classes representing different indoor scenes and consists of 15, 620
images of different sizes (Quattoni and Torralba, 2009).
In experiments on Sport-8, Caltech-101 and MIT-67, we split each dataset into a training set containing
80% of the data and a test set containing 20% of the data. For each of them, we use the same data
pre-processing techniques as for ImageNet.
B Implementation details
B.1	Methods
MANAS. Our code is based on a modified variant of Liu et al. (2019). To set the temperature
and gamma, We used as starting estimates the values suggested by BUbeck et al. (2012): t = 1
with η = 0.95 VzlnKK) (K number of actions, n number of architectures seen in the whole training).
Y = 1.05 K In(K). We then tuned them to increase validation accuracy during the search.
MANAS-LS. For our Least-Squares solution, we alternate between one epoch of training (in
which all β are frozen and the ω are updated) and one or more epochs in which we build the Z matrix
from Section 4 (in which both β and ω are frozen). The exact number of iterations we perform
in this latter step is dependant on the size of both the dataset and the searched architecture: our
goal is simply to have a number of rows greater than the number of columns for Z. We then solve
Bt = (ZZt) * ZL, and repeat the whole procedure until the end of training. This method requires
no additional meta-parameters.
Number of agents. In both MANAS variants, the number of agents is defined by the search space
and thus is not tuned. Specifically, for the image datasets, there exists one agent for each pair of
nodes, tasked with selecting the optimal operation. As there are 14 pairs in each cell, the total number
of agents is 14 × C, with C being the number of cells (8, 14 or 20, depending on the experiment).
B.2	Computational resources
ImageNet experiments were performed on multi-GPU machines loaded with 8× Nvidia Tesla V100
16GB GPUs (used in parallel). All other experiments were performed on single-GPU machines
loaded with 1× GeForce GTX 1080 8GB GPU.
11
Under review as a conference paper at ICLR 2020
C Factorizing the Regret
Factorizing the Regret: Let us firstly formulate the multi-agent combinatorial online learning
in a more formal way. Recall, at each round, agent Ai samples an action from a fixed discrete
collection {a(jAi)}jK=1. Therefore, after each agent makes a choice of its action at round t, the
resulting network architecture Zt is described by joint action profile ~at
(A1),[t]	(AN),[t]
aj1	, . . . , ajN
and thus, we will use Zt and ~at interchangeably. Due to the discrete nature of the joint action space,
the validation loss vector at round t is given by L~ (tval) = Lt(val) Zt(1) , . . . , Lt(val) Zt(KN)	and
(val)	(val)
for the environment one can write ν = L1 , . . . , LT . The interconnection between joint policy
π and an environment V works in a sequential manner as follows: at round t, the architecture Zt 〜
∏t(∙∣Zι, L，al),..., Zt-1, Lt-1) is sampled and validation loss LtvaI) = L(VaI)(Zt) is observed1. As
we mentioned previously, assuming linear contribution of each individual actions to the validating
loss, one goal is to find a policy π that keeps the regret:
RT (π, ν) =E
T
XβtTZt-
t=1
Zm∈iFn XT βtT Z
(7)
small with respect to all possible forms of environment ν. We reason here with the cumulative regret
the reasoning applies as well to the simple regret. Here, βt ∈ R+KN is a contribution vector of all
actions and Zt is binary representation of architecture Zt and F ⊂ [0, 1]KN is set of all feasible
architectures2. In other words, the quality of the policy is defined with respect to worst-case regret:
RT = sup RT(π, ν)
ν
(8)
Notice, that linear decomposition of the validation loss allows to rewrite the total regret (7) as a sum
of agent-specific regret expressions RTAi) (∏(Ai), V(Ai)) for i = 1,...,N:
TN
RT(π,ν) =E
X Xβt(Ai),TZt(Ai) -X
t=1	i=1
i=1
min
Z(Ai) ∈B(K)0,ι(0)
T
Xβt(Ai),TZ(Ai)
t=1
N
N
XE
i=1
T
Xβt(Ai),TZt(Ai) -	min
t 1 t	t	Z(Ai) ∈B(K )	(0)
t=1	|H|o,1
T
Xβt(Ai),TZ(Ai)
t=1

N
= XR(TAi) π(Ai), V(Ai)
i=1
where βt = hβtA1,T,.. .,βtAN,TiT and Zt = hZt(A1),T, ...,Zt(AN),TiT, Z =
Z(A1),T, . . . , Z(AN),TT are decomposition of the corresponding vectors on agent-specific parts,
joint policy ∏(∙) = QN=I ∏(Ai) (∙), and joint environment V = QN=I v(Ai), and b(") i(0) is unit
ball with respect to ∣∣∙∣∣o norm centered at 0 in [0,1]K. Moreover, the worst-case regret (8) also can
be decomposed into agent-specific form:
RT = SuPRt(π,v)	^⇒ sup RTAi) (π(Ai),v(Ai)) , i = 1,...,N.
ν	ν(Ai)
This decomposition allows us to significantly reduce the search space and apply the two following
algorithms for each agent Ai in a completely parallel fashion.
D Theoretical Guarantees
D.1	MANAS -LS
First, we need to be more specific on the way to obtain the estimates β(Ai) [k].
1Please notice, the observed reward is actually a random variable
2We assume that architecture is feasible if and only if each agent chooses exactly one action.
12
Under review as a conference paper at ICLR 2020
In order to obtain theoretical guaranties we considered the least-square estimates as in Cesa-Bianchi
and Lugosi (2012) as
N
βt = L(VaI)PtZt where P = E [ZZT] With Z has law ∏t(∙) = Y ∏(Ai)(∙)	(9)
i=1
Our analysis is under the assumption that each βt ∈ RKN belongs to the linear space spanned by
the space of sparse architecture Z. This is not a strong assumption as the only condition on a sparse
architecture comes with the sole restriction that one operation for each agent is actiVe.
Theorem 1.	Let us consider neural architecture search problem in a multi-agent combinatorial online
learning form with N agents such that each agent has K actions. Then after T rounds, MANAS-LS
achieves joint policy {πt}tT=1 with expected simple regret (Equation 3) bounded by O e-T /H in
any adversarial environment with complexity bounded by H = N(min7∙=k?,i∈{i,…,n} BTAi) [j] 一
BT(Ai) [ki?]), where ki? = minj∈{1,...,K} BT(Ai) [j].
Proof. In Equation 9 we use the same constructions of estimates βt as in ComBand. Using Corol-
lary 14 in Cesa-Bianchi and Lugosi (2012) we then haVe that Bt is an unbiased estimates of Bt.
GiVen the adVersary losses, the random Variables βt can be dependent of each other and t ∈ [T] as πt
depends on preVious obserVations at preVious rounds. Therefore, we use the Azuma inequality for
martingale differences by Freedman (1975).
Without loss of generality we assume that the loss Lt(Val) are bounded such that Lt(Val) ∈ [0, 1] for all t.
Therefore we can bound the simple regret of each agent by the probability of misidentifying of the
best operation P (ki? 6= aTA+i 1).
We consider a fixed adVersary of complexity bounded by H. For simplicity, and without loss of
generality, we order the operations from such that BT(Ai)[1] < BT(Ai)[2] ≤ . . . ≤ BT(Ai)[K] for all
agents.
We denote for k > 1, ∆k = BT(Ai) [k] 一 BT(Ai) [ki?] and ∆1 = ∆2.
We also haVe λmin as the smallest nonzero eigenValue of M where M is M = E [ZZT ] where Z is
a random Vector representing a sparse architecture distributed according to the uniform distribution.
P (ki? 6= aTA+i 1) = P ∃k ∈ {1,...,K} : BeT(Ai)[1] ≥ BeT(Ai)[k]
≤ P (∃k ∈ {1,...,K} : BTAi)[k] — BTAi)[k] ≥ T∆k or BTAi)[1] 一 BTAi)[1] ≥ T∆λ
≤P
K
(≤a) X exp 一
k=1
≤ K exp 一
(∆k )2T
2Nlθg(K )∕λmin
(∆1)2T
2Nlθg(K )∕λmin
where (a) is using Azuma’s inequality for martingales applied to the sum of the random Variables
with mean zero that are βk,t 一 βk,t for which we have the following bounds on the range. The range
of βk,t is [0, NIog(K)∕λmin]. Indeed our sampling policy is uniform with probability 1∕log(K)
therefore one can bound βk,t as in (Cesa-Bianchi and Lugosi, 2012, Theorem 1) Therefore we have
∣βk,t — βk,t∣ ≤ Nlog(K)∕λmin.
We recover the result with a union bound on all agents.	□
13
Under review as a conference paper at ICLR 2020
D.2 MANAS
We consider a simplified notion of regret that is a regret per agent where each agent is considering the
rest of the agents as part of the adversarial environment. Let us fix our new objective as to minimise
NN
i (π(Ai)) = X sup E
i=1	i=1 a-i,ν
T
XLt(val)(at(Ai)
t=1
a-i) -
min
a∈{1,...,K}
X Lt(val) (a, a-i)
where a-i is a fixed set of actions played by all agents to the exception of agent Ai for the T rounds
of the game and ν contains all the losses as ν = {L(tval) (a)}t∈{1,...,T},a∈{1,...,K N}.
We then can prove the following bound for that new notion of regret.
Theorem 2.	Let us consider neural architecture search problem in a multi-agent combinatorial
online learning form with N agents such that each agent has K actions. Then after T rounds, MANAS
achieves joint policy {∏t}T=ι with expected cumulative regret bounded by O (N√TK log K).
Proof. First we look at the problem for each given agent Ai and we define and look at
R?T,i (π(Ai)
T
a-i) = sup E X Lt(val) (a(tAi)
a-i) -
min
a∈{1,...,K}
X Lt(val) (a, a-i)
We want to relate that the game that agent i plays against an adversary when the actions of all the
other agents are fixed to a-i to the vanilla EXP3 setting. To be more precise on why this is the EXP3
setting, first we have that L(tval)(at) is a function of at that can take KN arbitrary values. When we
fix a-i, Lt(val) (at(Ai), a-i) is a function of at(Ai) that can only take K arbitrary values.
One can redefine Lt@,(val)(a(tAi)) = L(tval)(at(Ai), a-i) and then the game boils down to the vanilla ad-
versarial multi-arm bandit where each time the learner plays at(Ai) ∈ {1, . . . , K} and observes/incur
the loss Lt@,(val)(at(Ai)). Said differently this defines a game where the new ν0 contains all the losses
as ν0 = {Lt ,(va ) (a(Ai))}t∈{1,...,T},a(Ai)∈{1,...,K}.
For all a-i
Ri(EXP3, a-i) ≤ 2,TK log(K)
Then we have
Ri(EXP3) ≤ sup2√TK log(K)
a-i
= 2,TK log(K)
Then we have
N
X Ri(EXP3) ≤ 2NPTK log(K)
i=1
□
E	Relation between weight sharing and cumulative regret
Ideally we would like to obtain for any given architecture Z the value Lval(Z, w? (Z)). However
obtaining w? (Z) =arg minw Ltrain (w, Z) for any given fixed Z would already require heavy
computations. In our approach the wt that we compute and update is actually common to all Zt as
wt replaces w? (Zt). This is a simplification that leads to learning a weight wt that tend to minimise
the loss EZ〜∏t [Lvai(Z, W(Z)] instead of minimising Lval(Zt, w(Zt). If ∏t is concentrated on a
fixed Z then these two previous expressions would be close. Moreover when πt is concentrated on Z
then wt will approximate accurately w? (Z) after a few steps. Note that this gives an argument for
using sampling algorithm that minimise the cumulative regret as they naturally tend to play almost
all the time one specific architecture. However there is a potential pitfall of converging to a local
minimal solution as wt might not have learned well enough to compute accurately the loss of other
and potentially better architectures.
14
Under review as a conference paper at ICLR 2020
F Gaussian S queeze Domain experiment
Figure 3: Regret for the Gaussian Squeeze Domain experiment with 100 agents, 10 actions, μ = 1,
σ= 10.
s<0τx)3"①」① ≥4-nlurD
Figure 4: Theoretical bound for the MANAS cumulative regret (2N T K log K; see Appendix D.2)
and the observed counterpart for the Gaussian Squeeze Domain experiment with 100 agents, 10
actions, μ = 1, σ = 10.
15