Under review as a conference paper at ICLR 2020
Asymptotic learning curves of kernel meth-
ODS: EMPIRICAL DATA v.s.	TEACHER-STUDENT
PARADIGM
Anonymous authors
Paper under double-blind review
Ab stract
How many training data are needed to learn a supervised task? It is often observed
that the generalization error decreases as n-β where n is the number of training
examples and β an exponent that depends on both data and algorithm. In this work
we measure β when applying kernel methods to real datasets. For MNIST we find
β ≈ 0.4 and for CIFAR10 β ≈ 0.1. Remarkably, β is the same for regression
and classification tasks, and for Gaussian or Laplace kernels. To rationalize the
existence of non-trivial exponents that can be independent of the specific kernel
used, we introduce the Teacher-Student framework for kernels. In this scheme, a
Teacher generates data according to a Gaussian random field, and a Student learns
them via kernel regression. With a simplifying assumption — namely that the
data are sampled from a regular lattice — we derive analytically β for translation
invariant kernels, using previous results from the kriging literature. Provided that
the Student is not too sensitive to high frequencies, β depends only on the training
data and their dimension. We confirm numerically that these predictions hold when
the training points are sampled at random on a hypersphere. Overall, our results
quantify how smooth Gaussian data should be to avoid the curse of dimensionality,
and indicate that for kernel learning the relevant dimension of the data should
be defined in terms of how the distance between nearest data points depends on
n. With this definition one obtains reasonable effective smoothness estimates for
MNIST and CIFAR10.
1	Introduction
In supervised learning machines learn from a finite collection of n training data, and their generaliza-
tion error is then evaluated on unseen data drawn from the same distribution. How many data are
needed to learn a task is characterized by the learning curve relating generalization error to n. In
various cases, the generalization error decays as a power law n-β, with an exponent β that depends
on both the data and the algorithm. In (Hestness et al., 2017) β is reported for state-of-the-art (SOTA)
deep neural networks for various tasks: in for neural-machine translation β ≈ 0.3-0.36 (for fixed
model size) or β ≈ 0.13 (for best-fit models at any n); language modeling shows β ≈ 0.06-0.09; in
speech recognition β ≈ 0.3; SOTA models for image classification (on ImageNet) have exponents
β ≈ 0.3-0.5. Currently there is no available theory of deep learning to rationalize these observations.
Recently it was shown that for a proper initialization of the weights, deep learning in the infinite-width
limit (Jacot et al., 2018) converges to kernel learning. Moreover, it is nowadays part of the lore that
there exist kernels whose performance is nearly comparable to deep networks (Bruna and Mallat,
2013; Arora et al., 2019), at least for some tasks. It is thus of great interest to understand the learning
curves of kernels. For regression, if the target function being learned is simply assumed to be Lips-
chitz, then the best guarantee is β = 1/d (Luxburg and Bousquet, 2004; Bach, 2017) where d is the
data dimension. Thus for large d, β is very small: learning is completely inefficient, a phenomenon
referred to as the curse of dimensionality. As a result, various works on kernel regression make the
much stronger assumption that the training points are sampled from a target function that belongs to
the reproducing kernel Hilbert space (RKHS) of the kernel (see for example (Smola et al., 1998)).
With this assumption β does not depend on d (for instance in (Rudi and Rosasco, 2017) β = 1/2
is guaranteed). Yet, RKHS is a very strong assumption which requires the smoothness of the target
1
Under review as a conference paper at ICLR 2020
function to increase with d (Bach, 2017) (see more on this point below), which may not be realistic
in large dimensions.
In this work we compute β empirically for kernel methods applied on MNIST and CIFAR10 datasets.
We find βMNIST ≈ 0.4 and βCIFAR10 ≈ 0.1 respectively. Quite remarkably, we observe essentially
the same exponents for regression and classification tasks, using either a Gaussian or a Laplace kernel.
Thus the exponents are not as small as 1/d (d = 784 for MNIST, d = 3072 for CIFAR10), but neither
are they 1/2 as one would expect under the RKHS assumption. These facts call for frameworks in
which assumptions on the smoothness of the data can be intermediary between Lipschitz and RKHS.
Here we propose such a framework for regression, in which the target function is assumed to be a
Gaussian random field of zero mean with translation-invariant isotropic covariance KT(x). The data
can equivalently be thought as being synthesized by a “Teacher" kernel KT (χ). Learning is performed
with a “Student" kernel KS(χ) that minimizes the mean-square error. In general KT(x) = KS(χ).
In this set-up learning is very similar to a technique referred to as kriging, or Gaussian process
regression, originally developed in the geostatistics community (Matheron, 1963; Stein, 1999b). To
quantify learning, we first perform numerical experiments for data points distributed uniformly at
random on a hypersphere of varying dimension d, focusing on a Laplace kernel for the Student, and
considering a Laplace or Gaussian kernel for the Teacher. We observe that in both cases β(d) is a
decreasing function.
To derive β(d) we consider the simplified situation where the Gaussian random field is sampled
at training points lying on a regular lattice. Building on the kriging literature (Stein, 1999b), we
show that β is controlled by the high-frequency scaling of both the Teacher and Student kernels:
assuming that the Fourier transforms of the kernels decay as KT(W) = cτ∣∣w∣∣-αT + o (∣∣w∣∣-αT)
and Ks(W) = cs∣∣w∣∣-αS + o (|w| S), we obtain
β = m min(αT — d, 2αs).	(1)
Importantly (i) Eq. (1) leads to a prediction for β(d) that accurately matches our numerical study for
random training data points, leading to the conjecture that Eq. (1) holds in that case as well. We offer
the following interpretation: ultimately, kernel methods are performing a local interpolation whose
quality depends on the distance δ(n) between adjacent data points. δ(n) is asymptotically similar for
random data or data sitting on a lattice. (ii) If the kernel KS is not too sensitive to high-frequencies,
then learning is optimal as far as scaling is concerned and β = (αT - d)/d. We will argue that the
smoothness index s ≡ [(αT - d)/2] characterizes the number of derivatives of the target function
that are continuous. We thus recover the curse of dimensionality: s needs to be of order d to have
non-vanishing β in large dimensions. Point (ii) leads to an apparent paradox: β is significant for
MNIST and CIFAR10, for which d is a priori very large, leading to a smoothness value s in the
hundreds in both cases, which appears unrealistic. The paradox is resolved by considering that real
datasets actually live on lower-dimensional manifolds. As far as kernel learning is concerned, our
findings support that the correct definition of dimension should be based on how the nearest-neighbors
distance δ(n) scales with n: δ(n)〜n-1/deff. Direct measurements of δ(n) support that MNIST and
CIFAR10 live on manifolds of lower dimensions deMffNIST ≈ 15 and deCffIFAR10 ≈ 35. Considering
the effective dimensions that we find, the observed values for β would be obtained for Gaussian
fields of smoothness sMNIST ≈ 3 and sCIFAR10 ≈ 1, values that appear intuitively more reasonable.
More generally this analogy with Gaussian fields allows one to associate a smoothness index s to
any dataset once β and deff are measured, which may turn out to be a useful characterization of data
complexity in the future.
2	Related works
Our set-up of Teacher-Student learning with kernels is also referred to as kriging, or Gaussian process
regression, and it was originally developed in the geostatistics community (Matheron, 1963). In
Section 5 we present a theorem that allows one to know the rate at which the test error decreases
as we increase the number of training points, assumed to lie on a high-dimensional regular lattice.
Similar results have been previously derived in the kriging literature (Stein, 1999b) when sampling
occurs on the regular lattice with the exception of the origin, where the inference is made. Here
we propose an alternative derivation that some readers might find simpler. We also study a slightly
different problem: instead of computing the test error when the inference is carried on at the origin,
2
Under review as a conference paper at ICLR 2020
we compute the average error for a test point that lie at an arbitrary point, sampled uniformly at
random and not necessarily on the lattice.
In what follows we show, via extensive numerical simulations, that such predictions are accurate
even when the training points do not lie on a regular lattice, but are taken at random on a hypersphere.
An exact proof of our result in such a general setting is difficult and cannot be found even in the
kriging literature. To our knowledge the results that get closer to the point are those discussed
in (Stein, 1999a), where the author studies one-dimensional processes where the training data are not
necessarily evenly spaced.
In this work the effective dimension of the data plays an import role, as it controls how the distance
between nearest neighbors scales with the dataset size. Of course, there exists a vast literature
(Grassberger and Procaccia, 1983; Costa and Hero, 2004; Hein and Audibert, 2005; Levina and
Bickel, 2005; Rozza et al., 2012; Facco et al., 2017; Allegra et al., 2019) devoted to the study of
effective dimensions, where other definitions are analyzed. The effective dimensions that we find are
compatible with those obtained with more refined methods.
3	Learning curve for kernel methods applied to real data
In what follows we apply kernel methods to the MNIST and CIFAR10 datasets, each consisting
of a set of images (Xμ)n=1. We simplify the problem by considering only two classes whose label
Z(χμ) = ±1 correspond to odd and even numbers for MNIST, and to two groups of 5 classes in
CIFAR10. The goal is to infer the value of the label ZS (x) of an image X that does not belong to the
dataset. The S subscript reminds us that inference is performed using a positive definite kernel KS .
We perform inference in both a regression and a classification setting. The following algorithms and
associated results can be found in (Scholkopf and Smola, 2001).
Regression. Learning corresponds to minimizing a mean-square error:
n2
min X [ZS(Xμ) - Z(Xμ)].
μ=1
(2)
For algorithms seeking solutions of the form ZS (χ) = £* a*Ks (χ*, x) ≡ a ∙ ks (x) by minimizing
the man-square loss over the vector a, one obtains:
ZS(x)= ks (x) ∙ K-1Z,	(3)
where the vector Z contains all the labels in the training set, Z ≡ (Z(χ*))n=ι, and Ks,μν ≡
KS (xμ, Xν) is the Gram matrix. The Gram matrix is always invertible if the kernel KS is positive
definite. The generalization error is then evaluated as the expected mean-square error on unseen data,
estimated by averaging over a test set composed of ntest unseen data points:
MSE
nt1est XhZS(xμ)- Z(xμ)i	.
μ=ι
(4)
Classification. We perform kernel classification via the algorithm soft-margin SVM. The details can
be found in Appendix A. After learning from the training data with a student kernel KS , performance
is evaluated via the generalization error. It is estimated as the fraction of correctly predicted labels for
data points belonging to a test set with ntest elements.
In Fig. 1 we present the learning curves for (binary) MNIST and CIFAR10, for regression and
classification. Learning is performed both with a Gaussian kernel K(x) a exp(TlxF∕(2σ2)) and
a Laplace one K(x) a exp(-∣∣x∣∣∕σ). Remarkably, the power laws in the two tasks are essentially
identical (although the estimated exponent appears to be slightly larger, in absolute value, for
classification). Moreover, the two kernels display a very similar behavior, compatible with the same
exponent: about -0.4 for MNIST and -0.1 for CIFAR10. The presented data are for σ = 1000; in
Appendix B we show that the same behaviour is observed for different values.
3
Under review as a conference paper at ICLR 2020
Figure 1: Learning curves for regression on MNIST and CIFAR10 (a-b); and for classification
on MNIST and CIFAR10 (c-d). Curves are averaged over 400 runs. A power law is plotted to
estimate the asymptotic behavior at large n: the exponent is fitted on the last decade on the average
of the two curves, since it does not seem to depend significantly on the specific kernel or on the
task. In each setting We use both a Gaussian kernel K(x) 8 exp(-∣∣χ∣∣2∕(2σ2)) and a Laplace one
K(x) H exp(-∣∣x∣∣∕σ), with σ = 1000.
4	Generalization scaling in kernel Teacher-Student problems
We study β in a simplified setting where the data is assumed to follow a Gaussian distribution with
known covariance. It falls into the class of teacher-Student problems, which are characterized by a
machine (the Teacher) that generates the data, and another machine (the Student) that tries to learn
from them. The Teacher-Student paradigm has been broadly used to study supervised learning (Saad
and Solla, 1995; Monasson and Zecchina, 1995; Opper and Saad, 2001; Engel and Van den Broeck,
2001; ZdeboroVg and Krzakala, 2016; Barbier et 1 2019; Gabrie et al., 2018; AUbin et al., 2018;
Franz et al., 2018). He we restrict our attention to kernel methods: we assume that a target function is
distributed according to a Gaussian random field Z 〜N(0, KT)——the Teacher ——characterized by
a translation-invariant isotropic covariance function KT(x, x) = KT(∣x - x0∣∣), and that the training
dataset consists the finite set of n observations Z = (Z (χ*))μ=ι∙ This is equivalent to saying that
the vector of training points follows a centered Gaussian distribution with a covariance matrix that
depends on KT and on the location of the points (x")n=i：
Z 〜N (0, KT),	where KT = (KT(X“W ))μ,ν=ι∙	⑸
Once the Teacher has generated the dataset, the rest follows as in the kernel regression described in
the previous section. We use another translation-invariant isotropic kernel KS(χ, χ0) 一 the Student
——to infer the value of the field at another point, ZS (χ), with a regression task, i.e. minimizing the
mean-square error in Eq. (2). The solution is therefore given again by Eq. (3).
Fig. 2 (a-b) shows the mean-square error obtained numerically. In the examples the Student is always
taken to be a Laplace kernel, and the Teacher is either a Laplace kernel or a Gaussian kernel. The
4
Under review as a conference paper at ICLR 2020
points (χμ)μ=ι are taken uniformly at random on the unit d-dimensional hypersphere for several
dimensions d and for several dataset sizes n. We take σS = σT = d as we observed that with this
choice smaller datasets were enough to approach a limiting curve — in Appendix C we show the plots
for the case σS = σT = 10, which appears to converge to the same limit curve with increasing n, but
at a smaller pace. The figure shows that when n is large enough, the mean-square error behaves as a
power law (dashed lines) with an exponent that depends on the spatial dimension of the data, as well
as on the kernels. The fitted exponents are plotted in Fig. 2 (c-d) as a function of the spatial dimension
d for different dataset sizes n. In the next section we will discuss the theoretical prediction, that in
the figure is plotted a thick black line. The figure shows that as the dataset gets bigger, the asymptotic
exponent tends to our prediction. In Appendix D we present the learning curves of Gaussian Students
with both a Laplace and a Gaussian kernel. When both kernels are Gaussian the test error decays
exponentially fast, a result that matches our theoretical prediction. In Appendix E we also provide
further numerical results for the case where the Teacher kernel is a Matern kernel (as defined therein).
5 Analytic asymptotics for the kernel Teacher-Student problem
ON A LATTICE
In this section we compute analytically the exponent that describe the asymptotic decay of the
generalization error when the number n of training data increases. In order to derive the result we
assume that both the Teacher Gaussian random field lives on a bounded hypercube, X ∈ V ≡ [0, L]d,
where L is a constant and d is the spatial dimension. The fields and the kernels can then be thought of
.0.5.0.5.0
2.2.3.3.4.
- - - - -
c6θ/山 sw⅛6°
n=16
n=128
—π=1024
—π=4096
——n=8192
Laplace-Laplace
-石 MSE ~0-Mdτ)
5	10	15	20	25	30	5	10	15	20	25	30
d	d
Figure 2: Results for the Teacher-Student kernel regression problem, where the Student is always a
Laplace kernel. Data points are sampled uniformly at random on a d-dimensional hypersphere. (a-b)
Mean-square error versus the size of the training dataset, for Gaussian and Laplace Teachers and for
multiple spatial dimensions. Dotted lines are the fitted power laws — we fit starting from n = 700.
(c-d) Fitted exponent -β = log E MSE/ log n against the spatial dimension, for several dataset sizes.
We fit from n = 0 to a varying n (written in the legends). The thick black lines are the theoretical
predictions.
5
Under review as a conference paper at ICLR 2020
as L-periodic along each dimension. Furthermore, to make the problem tractable we assume that the
points (Xμ)μ=ι live on a regular lattice, covering all the hypercube V. Therefore, the linear spacing
between neighboring points is δ = Ln-1/d. This is of course a different setting than the one used in
the numerical simulations presented in the previous section, yet our results below support that these
differences do not matter.
Generalization error is then evaluated via the typical mean-square error
E MSE = E [Z(χ) - ZS(x)] 2,	(6)
where the expectation is taken over both the Teacher process and the point X at which We estimate the
field, assumed to be uniformly distributed in the hypercube V. In Appendix F we prove the following:
Theorem 1. Let KT (w) = cτ∣∣w∣∣ T + o (∣∣w∣∣-αT) and KS(W) = cs∣∣w∣∣ S + o (∣∣w∣∣-αS)
as ∣∣w∣∣ → ∞, where KT (w) and KS (w) are the Fourier transforms of the kernels KT (x),
KS (x) respectively, assumed to be positive definite. We assume KT (W) and KS (W) has a finite
limit as ∣∣w∣∣ → 0 and that K(0) < ∞. Then,
E MSE = n-β + o (n-β) with β = ɪ min(αT — d, 2a5).	(7)
Moreover, in the case ofa Gaussian kernel the result holds valid if we take the corresponding
exponent to be α = ∞.
Apart from the specific value of the exponent in Eq. (7), Theorem 1 implies that if the Student kernel
decays fast enough in the frequency domain, then β depends only on the data through the behaviour
of the Teacher kernel at high frequencies. One then recovers β = (αT - d)/d, also found for the
Bayes-optimal setting where the Student is identical to the Teacher.
Consider the predictions of Theorem 1 in the cases presented in Fig. 2 (a-b) of Gaussian and Laplace
kernels. If both kernels are Laplace kernels then αγ = a5 = d + 1 and E MSE 〜n-1/d, which
scales very slowly with the dataset size in large dimensions. If the Teacher is a Gaussian kernel
(αT = ∞) and the Student is a Laplace kernel then β = 2(1 + 1/d), leading to β → 2 as d → ∞.
In Fig. 2 (c-d) we compare these predictions with the exponents extracted from Fig. 2 (a-b). We
plot logE MSE/ log n ≡ -β, against the dimension d of the data, varying the dataset size n. The
exponents extracted numerically tend to our analytical predictions when n is large enough.
Notice that, although the theory and the experiments do not assume the same distribution for the
sampling points (χ*)μ=ι, this does not seem to yield any difference in the asymptotic behavior of the
generalization error, leading to the conjecture that our predictions are exact even when the training
set is random, and does not correspond to a lattice. The conjecture can be proven in one dimension
following results of the kriging literature (Stein, 1999a), but generalization to higher d is a much
harder problem. Intuitively, for kernel learning performs an expansion, whose quality is governed by
the target function smoothness and the typical distance δmin between a point and its nearest neighbors
in the training set. Both for random points or on a lattice, one has δmin 〜n-1/d when n is large
enough, thus both situations lead to the same β .
Theorem 1 underlines that kernel methods are subjected to the curse of dimensionality. Indeed
for appropriate students, one obtains β = (αT - d)/d. Let us define the smoothness index s ≡
[(aT — d)/2] = βd∕2, which must be O(d) to avoid β → 0 for large d. The two Lemmas below,
derived in Appendix, indicate that the target function is s time differentiable (in a mean-square sense).
Thus learning with kernels in very large dimension can only occur if the target function is O(d) times
differentiable, a condition that appears very restrictive in large d.
Lemma 1. Let K (χ, χ0) be a translation-invariant isotropic kernel such that K (w) = c∣∣w∣∣ α +
o (∣∣w∣∣	α)	as ∣w∣ → ∞ and	∣∣w∣∣dK(w)	→ 0 as ∣∣w∣∣	→	0. If α	> d + n for some n ∈	Z+,	then
K(χ) ∈ Cn, that is, it is at least n-times differentiable. (ProofinAppendix G).
6
Under review as a conference paper at ICLR 2020
Lemma 2. Let Z 〜N(0, K) be a d-dimensional Gaussian random field, with K ∈ C2n being
a 2n-times differentiable kernel. Then Z is n-times mean-square differentiable in the sense that
•	derivatives of Z (x) are a Gaussian random fields;
•	EdnI …∂ndz(x)= 0;
•	EdnI …an”(x) ∙ ∂n …an z(χo)= ∂nι+n1 …∂n+nK(X - x0)< ∞ if the
derivatives of K exist.
In particular, EdmZ(x) ∙ dmZ(x0) = ∂XmK(X 一 x0) < ∞∀m ≤ n. (ProofinAppendix G).
6 Effective dimension of data sets
(P3Z=EEJoU) AU-Wg V
10°
._-1
6*10
._-1
4*10
101	102	103	104	105	101	102	103	104	W5
n	n
Figure 3: Average distance from one point to its nearest neighbor as a function of the dataset size n.
(a) For random points on d-dimensional hypersphere, <δmini 〜n-1/d. Colored solid curves are found
numerically, dashed lines are the theoretical asymptotic prediction and the gray lines are numerical fit
(we fitted only starting from n ≈ 6000 to reduce finite size effects, and the fit have been rescaled to
match the data at n = 10). The larger d, the stronger the preasymptotic effects (a larger n is needed
to observe the predicted scaling). (b) Comparison between random data on 15- and 35-dimensional
hyperspheres and the MNIST, CIFAR10 datasets. According to this definition of effective dimension,
MNIST live on a 15-dimensional manifold and CIFAR10 on a 35-dimensional one. Data have been
rescaled along the y-axis for ease of comparison.
If we approximate the high-dimensional MNIST and CIFAR10 datasets with Gaussian random fields,
to obtain the curves shown in Fig. 1 and to find the values the we report for β these fields would
have to be hundreds of times differentiable, which seems unrealistic. A possible resolution of this
paradox lies in the fact that the data live in a much smaller manifold than the number of pixels of
these pictures would suggest. As argued above, a key determinant of kernel performance is the typical
distance δmin between a point in the training set and its nearest neighbor. We define the effective
dimension deff accordingly from the asymptotic relationship between δmin and n:
δmin 〜n"".	⑻
For random points on a d-dimensional hypersphere δmin displays fluctuations and the scaling is valid
only on average and only asymptotically, that is for n larger than some characteristic scale n? (d)
that depends on the spatial dimension. In Fig. 3 (a) we show how the typical δmin scales with the
dataset size n for random points on hyperspheres of dimension d = 15 and d = 35. Notice that while
for d = 15 the asymptotic regime is reached when n ' 104, for d = 35 a larger dataset is needed,
with n > 105 points (that is about the maximum size of the dataset that we can use to apply kernel
methods in our simulations, due to memory constraints). One can naturally wonder whether real
data are also subjected to a scaling relation like in Eq. (8), from which an effective dimension can be
defined. Consider for instance the MNIST dataset, and sample from it a subset ofn pictures. For each
7
Under review as a conference paper at ICLR 2020
point we could compute the distance from its nearest neighbor and average such quantities. As the
number of data points increases, we expect that this measure characterizes the geometry of the local
manifold where the data live in, since nearest neighbors are going to be closer and closer. In Fig. 3 (b)
we present how δmin scales with n for the MNIST and CIFAR10 datasets. Both display a power-law
decay, but the exponent is not compatible with 1/d with d the spatial dimension, namely d = 784 for
MNIST and d = 3072 for CIFAR10. MNIST actually seems to scale pretty much like random data
on a hypersphere with d = 15, and CIFAR10 scales approximately as random data on a hypersphere
with d = 35. For this reason, the effective dimensions of these datasets are consistent with:
dMeffNIST ≈ 15
dCeffIFAR10 ≈ 35.
Obviously, the intrinsic dimension of the data could vary in data space, as has been reported for
MNIST (Costa and Hero, 2004; Hein and Audibert, 2005; Rozza et al., 2012; Facco et al., 2017). In
this qualitative discussion we neglect such subtle effects. Interestingly, our effective dimensions leads
to reasonable values for the effective smoothness:
Seff = Bdeff/2.
In particular we find s ≈ 3 for MNIST and s ≈ 1 for CIFAR10.
7 Conclusion
In this work we have shown for CIFAR10 and MNIST respectively that kernel regression and
classification display a power-law decay in the learning curves, quite remarkably with essentially
the same exponent β, found to be larger for MNIST. These exponents are much larger than β = 1/d
expected for Lipschitz target functions and smaller than β = 1/2 expected for RKHS target functions.
This observation led us to introduce a framework in which data are modeled as Gaussian random
fields of varying smoothness, in which intermediary values of β are obtained.
It is important to note the high degree of smoothness underlying the RKHS hypothesis. Consider
realizations Z(χ) of a Teacher Gaussian process with covariance KT and assume that they lie in the
RKHS of the Student kernel KS, namely
E∣∣Z∣∣Ks = e/dxdyZ(x)K-1(x - y)Z(y) = /dwKT(W)K-1(w) < ∞.	(9)
If the Teacher and Student kernels decay in the frequency domain with exponents αT and αS
respectively, convergence requires ατ > as + d, and KS(0) a / dwKs(W) < ∞ (true for many
commonly used kernels) implies αS > d. Then using Lemma 1 and Lemma 2 we can conclude that
the realizations Z(x) must be at least ［也［-times mean-square differentiable to be RKHS.
From this perspective, the RKHS assumption appears to be very strong, and thus may not provide an
accurate description of various empirical learning curves. Our assumption that data are generated
by Gaussian random processes is milder, and may thus have broader applications. Yet, we view this
approximation as a first step on which to build on, to later include other effects such as noise in the
data and deviations from Gaussianity.
Acknowledgments
Anonymized for double-blind review.
References
M. Allegra, E. Facco, A. Laio, and A. Mira. Clustering by the local intrinsic dimension: the hidden
structure of real-world data. arXiv preprint arXiv:1902.10459, 2019.
S. Arora, S. S. Du, W. Hu, Z. Li, R. Salakhutdinov, and R. Wang. On exact computation with an
infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019.
B. Aubin, A. Maillard, F. Krzakala, N. Macris, L. Zdeborovd, et al. The committee machine:
Computational to statistical gaps in learning a two-layers neural network. In Advances in Neural
Information Processing Systems, pages 3223-3234, 2018.
8
Under review as a conference paper at ICLR 2020
F. Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of Machine
Learning Research,18(1):629-681, 2017.
J. Barbier, F. Krzakala, N. Macris, L. Miolane, and L. Zdeborova. Optimal errors and phase transitions
in high-dimensional generalized linear models. Proceedings of the National Academy of Sciences,
116:201802705, 03 2019. doi: 10.1073/pnas.1802705116.
J. Bruna and S. Mallat. Invariant scattering convolution networks. IEEE transactions on pattern
analysis and machine intelligence, 35(8):1872-1886, 2013.
J. A. Costa and A. O. Hero. Learning intrinsic dimension and intrinsic entropy of high-dimensional
datasets. In 2004 12th European Signal Processing Conference, pages 369-372. IEEE, 2004.
A. Engel and C. Van den Broeck. Statistical mechanics of learning. Cambridge University Press,
2001.
E. Facco, M. d’Errico, A. Rodriguez, and A. Laio. Estimating the intrinsic dimension of datasets by a
minimal neighborhood information. Scientific reports, 7(1):12140, 2017.
S. Franz, S. Hwang, and P. Urbani. Jamming in multilayer supervised learning models. arXiv preprint
arXiv:1809.09945, 2018.
M. Gabria A. Manoel, C. Luneau, N. Macris, F. Krzakala, L. Zdeborov直 et al. Entropy and mutual
information in models of deep neural networks. In Advances in Neural Information Processing
Systems, pages 1821-1831, 2018.
P. Grassberger and I. Procaccia. Measuring the strangeness of strange attractors. Physica D: Nonlinear
Phenomena, 9(1-2):189-208, 1983.
M. Hein and J.-Y. Audibert. Intrinsic dimensionality estimation of submanifolds inrd. In Proceedings
of the 22nd international conference on Machine learning, pages 289-296. ACM, 2005.
J. Hestness, S. Narang, N. Ardalani, G. F. Diamos, H. Jun, H. Kianinejad, M. M. A. Patwary, Y. Yang,
and Y. Zhou. Deep learning scaling is predictable, empirically. CoRR, abs/1712.00409, 2017.
A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural
networks. In Advances in Neural Information Processing Systems 31, pages 8580-8589. 2018.
E. Levina and P. J. Bickel. Maximum likelihood estimation of intrinsic dimension. In Advances in
neural information processing systems, pages 777-784, 2005.
U. v. Luxburg and O. Bousquet. Distance-based classification with lipschitz functions. Journal of
Machine Learning Research, 5(Jun):669-695, 2004.
G. Matheron. Principles of geostatistics. Economic geology, 58(8):1246-1266, 1963.
R. Monasson and R. Zecchina. Weight space structure and internal representations: a direct approach
to learning and generalization in multilayer neural networks. Physical review letters, 75(12):2432,
1995.
M. Opper and D. Saad. Advanced mean field methods: Theory and practice. MIT press, 2001.
A. Rozza, G. Lombardi, C. Ceruti, E. Casiraghi, and P. Campadelli. Novel high intrinsic dimensional-
ity estimators. Machine learning, 89(1-2):37-65, 2012.
A.	Rudi and L. Rosasco. Generalization properties of learning with random features. In Advances in
Neural Information Processing Systems, pages 3215-3225, 2017.
D. Saad and S. A. Solla. On-line learning in soft committee machines. Physical Review E, 52(4):
4225, 1995.
B.	Scholkopf and A. J. Smola. Learning with kernels: support vector machines, regularization,
optimization, and beyond. MIT press, 2001.
9
Under review as a conference paper at ICLR 2020
A. J. Smola, B. Scholkopf, and K.-R. Muller. The connection between regularization operators and
support vector kernels. Neural networks,11(4):637-649,1998.
M. L. Stein. Predicting random fields with increasing dense observations. The Annals of Applied
Probability, 9(1):242-273, 1999a.
M. L. Stein. Interpolation of spatial data: some theory for kriging. Springer Science & Business
Media, 1999b.
C.	K. Williams and C. E. Rasmussen. Gaussian processes for machine learning, volume 2. MIT
Press Cambridge, MA, 2006.
L. Zdeborova and F. Krzakala. Statistical physics of inference: Thresholds and algorithms. Advances
in Physics, 65(5):453-552, 2016.
10
Under review as a conference paper at ICLR 2020
A S oft- margin Support Vector Machines
The kernel classification task is performed via the algorithm known as soft-margin Support Vector
Machine.
We want to find a function ZS(x) such that its Sign correctly predicts the label of the data. In this
context we model such a function as a linear prediction after projecting the data on a feature space
via x → φ(χ):
△，、 . ....
ZS (X)= W ∙ φs (Xμ) + b,	(IO)
where w, b are parameters to be learned from the training data. The kernel is related to the feature
space via KS(χ,χ0) = φ,(x) ∙ φ,(X). We require that Z(Xμ)Zs(χ*) > 1 - ξμ for all training
points. Ideally We want to have some large margins 1 - ξ* = 1, but We allow some of them to
be smaller by introducing the slack variables ξμ and penalizing large values. To achieve this the
following constrained minimization is performed:
min1∣wF + C X ξμ
w,b,ξ 2
μ
subjected to
∀μ Z (Xμ) [w ∙ φs (Xμ) + b ≥ 1 - ξμ, ξμ ≥ 0∙
(11)
This problem can be expressed in a dual formulation as
1
min -a ∙
a 2 一
n
Qs a — £ aμ
μ=1
subjected to Z ∙ a = 0, 0 ≤ a* ≤ C,
(12)
where Qs,μ,ν = Z(x*)Z(XVIKS(Xμ,Xν) and Z is the vector of the labels of the training points.
Here C (= 104 in our simulations) controls the trade-off between minimizing the training error and
maximizing the margins 1 - ξμ. For the details we refer to (Scholkopf and Smola, 2001). If a? is the
solution to the minimization problem, than
w? = X αμφs (X“)，	(I3)
μ
b? = Z(Xμ) — ɪ2 aνyνKS(X*,Xν) for any μ such that α* < C.	(14)
ν
The predicted label for unseen data points is then
Sign(ZS (x)) = Sign(E Z (x“)a“K$ (x“, x) + b?)	(15)
μ
The generalization error is now defined as the probability that an unseen image has a predicted label
different from the true one, and such a probability is again estimated as an average over a test set
with ntest elements:
Error = -ɪ X θ [-sign (^Zs(x“)) Z(x“)] .	(16)
ntest μ=1
B Different kernel variances
In Fig. 4 we show the learning curves for kernel regression on the MNIST (parity) dataset — the same
setting as in Fig. 1 (a). Several Laplace kernels of varying variance σ are used. The variance ranges
several orders of magnitude and the learning curves all decay with the same exponent, although for
σ = 10 the algorithm achieves suboptimal performance and the test errors are increased by some
factor.
11
Under review as a conference paper at ICLR 2020
Figure 4: Learning curves for kernel regression on the MNIST dataset. Regression is performed with
several Laplace kernels of varying variance σ ranging from σ = 10 to σ = 10000.
C Different choice of kernel variances
In Fig. 5 we show the learning curves for the Teacher-Student kernel regression problem, with a
Student kernel that is always Laplace and a Teacher that can be either Gaussian or Laplace. We
show how the test error decays with the size of the training dataset and how the asymptotic exponent
depends on the spatial dimension. Every experiment is run with two different choices of the kernel
variances: in one case σT = σS = d and in the other σT = σS = 10. We observed that scaling the
variances with the spatial dimension leads faster to the results that we predicted in this paper, but
overall the choice has little effect on the exponents (both tend towards the prediction as the dataset
size is increased).
Gaussian-Lap la∞ σ=d
d = 4
---------d = 8
---------d = 16
d = 32
2 3 3
- - -
U6O≥SW⅛6°
Gaussian-Lapla∞ σ = 10
d = 4
---------d= 8
—d= 16
—d = 32
Laplace-Lapla∞ σ = d
10
10	10
n
10	10
10	10	10
n
Figure 5: In these plots we show the results for the Teacher-Student kernel regression. The Student is
always a Laplace kernel, the Teacher is either Gaussian or Laplace. The four plots on the left depict
the mean-square error against the size of the dataset for different spatial dimensions of the data, those
on the right show the fitted asymptotic exponent against the spatial dimension for different dataset
sizes. For every case we show both the the results for σT = σS = d and σT = σS = 10.
d = 4
d = 8
d = 16
d = 32
5	10	15	20	25	30
d
5	10	15	20	25	30
d

12
Under review as a conference paper at ICLR 2020
Figure 6: Left: The test error of a Laplace Teacher (αT = d + 1) with a Gaussian Student (αS = ∞)
decays as a power law with the predicted exponent β = d min(1, ∞) = 1 in d = 6 dimensions.
Center: When both the Teacher and the Student are Gaussian the test error decays faster than any
power law as the number n of data is increased. This plot confirm this by showing that the logarithm
of the test error decays linearly as a function of n 1. Right: Comparison between the learning curves
for the cases where both kernels are either Laplace (top blue line) or Gaussian (bottom orange line).
While the former decays algebraically with the predicted exponent, the latter decays exponentially, in
agreement with the prediction β = ∞ found within our framework. In all these plots we have taken
the variances of both the Teacher and Student kernels to be equal to the dimension d = 6.
D	Gaussian Students
In this appendix we present the learning curves of Gaussian Students: the Fourier transform of
these kernels decays faster than any power law and one can effectively consider αS = ∞. If
the Teacher is Laplace (αT = d + 1) then the predicted exponent is finite and takes the values
β = d min(αT - d, 2ɑs) = d min(1, ∞) = d. Such a case is displayed in Fig. 6 (left) in dimension
d = 6. However, if we consider the Teacher to be Gaussian as well, then the predicted exponent
would be β = d min(∞, ∞) = ∞. This case corresponds to Fig. 6 (center): the test errors decays
faster than a power law. In Fig. 6 (right) we compare the case where both kernels are Gaussian to the
case where both kernels are Laplace: while the latter decays as a power law, the former decays much
faster.
E Matern Teachers
To further test the applicability of our theory, we show here some numerical simulations for a Teacher
kernel that is a Matern covariance function and a Laplace kernel as student. We ran the simulations in
1d: the data points are sampled uniformly on a 1-dimensional circle embedded in R2. Matern kernels
are parametrized by a parameter ν > 0:
1-ν
KT(X) = Γ(V)ZVKV ⑶,
(17)
where Z = √2ν粤(σ being the kernel variance), Γ is the gamma function and KV is the Bessel
function of the second kind with parameter ν. Interestingly we recover the Laplace kernel for ν = 1/2
and the Gaussian kernel for ν = ∞. As one can find in e.g. (Williams and Rasmussen, 2006), the
exponent αT that governs the decay at high frequency of this kernels is αT = d + 2ν. Varying ν we
can change the smoothness of the target function.
For d = 1 our prediction for the learning curve exponent β is
β
m min(αT - d, 2αs) = min(2ν, 4).
d
(18)
In Fig. 7 we verify that our prediction matches the numerical results.
13
Under review as a conference paper at ICLR 2020
Figure 7: Mean-squared error for Matern Teacher kernels and Laplace students. The variance of the
kernels is equal to 2 for all the curves.
F	Proof of theorem
We prove here Theorem 1:
Theorem 1 Let KT(W) = cτ∣w∣ T + o (|w| T) and KS(W) = cs∣∣w∣∣ S + o (|WII S) as
∣w∣ → ∞, where KT(w) and KS(w) are the Fourier transforms of the kernels KT(x), KS(x)
respectively, assumed to be positive definite. We assume KT (W) and KS (w) has a finite limit as
∣w∣ → 0 and that K(0) < ∞. Then,
E MSE = n-β + o (n-β)	with β = ɪ min(αT — d, 2αs).	(19)
Moreover, in the case ofa Gaussian kernel the result holds valid ifwe take the corresponding exponent
to be α = ∞.
Proof. Our strategy is to compute how the mean-square test error scales with distance δ between two
nearest neighbors on the d-dimensional regular lattice. At the end, We will use the fact that δ H n-1/d,
where n is the number of sampled points on the lattice.
We denote by F(w) the Fourier transform of a function F : V → R:
2π
F(W) = L /2	dχe iwxF(χ),	where W ∈ L ≡ -j~Zd,	(20)
—	Jv —	—	—	L
F(χ) = L-d/2 X eiw,xF(W).	(21)
w∈L
If Z 〜N(0, K) is a Gaussian field with translation-invariant covariance K then by definition
EZ (X)Z (χ0) = K (X — χ0).	(22)
14
Under review as a conference paper at ICLR 2020
Properties of the Fourier transform of a Gaussian field:
~ . ~ . .
K(W) = K(-w) ∈ R,
一 ~ , .
EZ(W) = 0,
EZ(W)Z(w0) = L/2Sww，K(W).
(23)
(24)
(25)
Eq. (23) comes from the fact that K(x) is an even, real-valued function. The real and imaginary
parts of Z(W) are Gaussian random variables. They are all independent except that Z(-w) = Z(W).
Eq. (25) follows from the fact that Z(x) and K(x) are L-periodic functions, and therefore eiw XK(W)
is the Fourier transform of K(∙ + x) if W ∈ 2πZd.	#
The solution Eq. (3) for kernel regression has two interpretations. In Section 4 we introduced it as the
quantity that minimizes a quadratic error, but it can also be seen as the maximum-a-posteriori (MAP)
estimation of another formulation of the problem (Williams and Rasmussen, 2006). The field Z(x) is
assumed to be drawn from a Gaussian distribution with covariance function KS(x): KS therefore
plays a role in the prior distribution of the data Z = (Z (xμ)n=ι). Inference about the value of the
field ZS (x) at another location is then performed by maximizing its posterior distribution,
ZS(x) ≡ arg max P (Z(x)|Z).
(26)
Such a posterior distribution is Gaussian, and its mean — and therefore also the value that maximizes
the probability — is exactly Eq. (3):
ZS(x)= kS (x) ∙ K-1Z,
(27)
where where Z = (Z(x*))；=、are the training data, k5(x) = (KS(x*,x))；=、and KS =
(KS(x*,xν))；v=i is the Gram matrix, that is invertible since the kernel KS is assumed to be
positive definite. By Fourier transforming this relation we find
~ ,.
ZS (w)
~
z*(w)
KS (w)
,
KS(W)
(28)
where We have defined F? (W) ≡ ^∈uef F
for a generic function F .
Another way to reach Eq. (28) is to consider that we are observing the quantities
Z*(w) ≡ δdL-d/2	E	e-iw∙χZ(x) ≡ EZ (W +
χ∈ lattice	n∈Zd	'
2πn∖
(29)
Given that we know the prior distribution of the Fourier components on the right-hand side in Eq. (29),
We can infer their posterior distribution once their sums are constrained by the value of Z?(w), and it
is straightforward to see that we recover Eq. (28).
The mean-square error can then we written using the Parseval-Plancherel identity,
2
EMSE
L-dE
dx [Z (x) — ZS (x)]2
L-dE
w∈L
~ . . ~ .. .
Z(W)- Z (w)
KS (w)
~ ..
KS (W)
(30)
V
15
Under review as a conference paper at ICLR 2020
By taking the expectation value with respect to the Teacher and using Eq.(23)-Eq.(25) We can write
the mean-square error as
E MSE = L-dEf
w∈L
~ , , ~ , . ~ , . ~ .
Z(W)Z(W) — 2Z(w)Z*(w)
KS (W)
~ ..
KS (W)
+ Z*(w)Z*(w) KM
KS (w).
L-dEf Z(w)Z(w)—
w∈L
2 K S (W)
K S (W)
Σ~ ,	~
Z(W)Z
n∈Zd
W +
+
≡ A Z (W +竽)Z (W +竽
2
L-也 X KT(W)- 2K-=-KT(w) +
w∈L	KS(W)
K(W) X KT(W +蜉
KS (w) n⅛d	v δ
L-d∕2 X K.T(w) — 2[KTKS] (W)
WwB	KS(w)
~	. 、一 ~ C -	.	.
l KT (w)[kSr (W)
-1~	~
KS (W) 2
(31)
where B = [— ∏, ∏]d is the Brillouin zone.
,.~	..	，，，，，-、	-	~	Z	K	..	，，，，，-、
At high frequencies,	KT(w)	= CT|| WIl	T + o ( 11 W|I	T) and	KS(w)	=	CS|| WIl	S + o	( || W|I	S).
Therefore:
KT(w) = KT(w) + δαT CT| wδ + 2πn | ∣ -αT + o ( | W|-0T) ≡
n∈Zd∖{0}
≡ KT (w) + δαT CT Ψt (wδ) + o ( | I w | I αT) . (32)
This equation defines the function Ψt, and a similar equation holds for the Student as well. The
hypothesis KT(0) 8 / dwK(w) < ∞ implies αγ > d and therefore En∈^d || n|| ατ < ∞ (and
likewise for the Student). Then, ψaT (0), ΨαS (0) are finite; furthermore, the w's in the sum Eq. (31)
are at most of order O (δ-1), therefore the terms ψα(wδ) are O(δ0) and do not influence how
Eq. (31) scales with δ. Applying Eq. (32), expanding for δ《1 and keeping only the leading orders,
we find
E MSE =
:L-d/2	X 2ct ψατ(wδ)δατ + cS ψ2αs (wδ) ^^ δ2°S +。(||WraT) +。(国-叱)=
w∈L∩B	KS (w)
:L-d/2	X 2ct ψaτ(wδ)δaτ + cS ψ2as (wδ) Kl^ δ2aS +。( | WraT-d)+。(国-叱-CI).
w∈L∩B	KS(w)
(33)
We have neglected terms proportional to, for instance, δɑT+as, since they are subleading with respect
to δaT , but we must keep both δaT and δaS since we do not know a priori which one is dominant.
The additional term δ-d in the subleading terms comes from the fact that |L ∩ B| = O (δ-d).
The first term in Eq. (33) is the simplest to deal with: since ∣∣wδH is smaller than some constant for all
W ∈ L ∩ B and the function ψaT (wδ) has a finite limit, we have
δαT E 2ctψaT (wδ) = O (δaT |L ∩BD = O (δaT-d).
w∈L∩B
(34)
We then split the second term in Eq. (33) in two contributions:
16
Under review as a conference paper at ICLR 2020
Small ||w|| We consider “small” all the terms W ∈ L ∩ B such that |w| < Γ, where Γ》1 is O(δ0)
but large. As δ → 0, ψ2as (wδ) → ψ2as (0) which is finite because K(0) < ∞. Therefore
δ2αs X CSψ2αs (Wδ) KT(W) → δ2αs CSψ2αs (0) X KT(W).	(35)
w∈L∩B	一KS (W)
w∈L∩B KS (W)
Ilwll <Γ	Ilwll <Γ
The summand is real and strictly positive because the positive definiteness of the kernels implies that
their Fourier transforms are strictly positive. Moreover, as δ → 0, L ∩B∩{ Ilwll < Γ} → L ∩{ Ilwll <
Γ}, which contains a finite number of elements, independent of δ. Therefore
δ2αS
E CSψ2αs (Wδ)
w∈L∩B
⅛I<γ
KT (W)
KS (W)
O (δ2αS).
(36)
Large IlwIl “Large” W are those with ∣w∣ > Γ: we recall that Γ》1 is O(δ0) but large. This
allows us to approximate KT, KS in the sum with their asymptotic behavior:
δ2αS
wXιBcS ψ2αS (幽 KTg	Y
llwll>r
δ2αS E l∣Wl∣-αT+2αS + o (l∣Wl∣-αT+2αS) ≈
w∈L∩B
⅛I>γ
1/δ
≈ δ2αS J	dWWd-1-aT+2αS + o(l∣Wl∣-αT+2αS) = O ^δmin(αT-d,2αSζ) .	(37)
Finally, putting Eq. (34), Eq. (36) and Eq. (37) together,
E MSE = O δmin(αT -d,2αS)
(38)
The proof is concluded by considering that δ = O n-1/d .
In the case of a Gaussian kernel K(x) y exp(-∣∣χ∣∣2∕(2σ2)) — and therefore K(W) y
exp(一σ2∣∣W∣∣2∕2) — one has to redo the calculations starting from Eq. (31), but the final result
can be easily recovered by taking the limit α → +∞ (Gaussian kernels decay faster than any power
law).
□
G Proofs of lemmas
Lemma 1 Let K(x, x0) be a translation-invariant isotropic kernel such that K(w) = c∣W∣ α +
o (∣∣W∣∣ α) as ∣W∣ → ∞ and IlwIIdK (w) → 0 as IlWIl → 0. If α > d + n for some n ∈ Z+ ,then
K(x) ∈ Cn, that is, it is at least n-times differentiable.
Proof. The kernel is rotational invariant in real space (K(x) = K(∣∣χ∣∣)) and therefore also in the
frequency domain. Then, calling ^ι = (1,0,...) the unitary vector along the first dimension xι,
K(x) Y
dweiwaχK(∣∣w∣∣).
(39)
It follows that
r
K(IIwI) < J dw Iw ∙ ^1 ImIK(IIwII)IY
∣∂mK(x)∣ Y I	dw(w ∙ ^ι)meiw∙gιx
γ ∞ dwwd-1+m∣KK(w)∣ ∏ dφι∣ cos(φι)∣m y ∞ dw wdτ+m∣K(w)∣. (40)
0	00
We want to claim that this quantity is finite if m ≤ n. Convergence at infinity requires m < α 一 d,
that is always smaller than or equal to n because of the hypothesis of the lemma. Convergence in
zero requires that Wd+m ∣K(W)∣ → 0, and we want this to hold for all 0 ≤ m < α 一 d, the most
constraining one being the condition with m = 0.	□
17
Under review as a conference paper at ICLR 2020
Lemma 2 Let Z 〜 N(0, K) be a d-dimensional Gaussian random field, with K ∈ C2n being a
2n-times differentiable kernel. Then Z is n-times differentiable in the sense that
•	derivatives of Z (χ) are a Gaussian random fields;
•	Edni …∂χdZ(x) = 0;
•	EdnI …∂nd Z(x) ∙ ∂ni …∂nd Z(χ0) = ∂n1+n1 …dnd+nd K(x - x0) < ∞ if the deriva-
tives of K exist.
Inparticular, EdmZ(x) ∙ dmZ(x0) = ∂XmK(x — x0) < ∞ ∀m ≤ n.
Proof. Derivatives of Z(x) are defined as limits of sums and differences of the field Z evaluated at
different points, therefore they are Gaussian random fields too, and furthermore it is straightforward
to see that their expected value is always 0 if the field itself is zero centered.
The correlation can be computed via induction. Assume that EdnI •…dnd Z(x) ∙ dni …dndZ(x0)=
dn1+n1 •…dXd+ndK(x — x0) holds true. Then, if We increment nι:
Edn1 + 1 …dndZ(x) ∙ dn1 …dndZ(x0) =
=h→0 h-1 E [dni …dndZ(x + h^ι) - dni …dXdZ(x)] ∙ dn1 …dndZ(x0)=
=iim h-1 [dnι+n1 …dnd+nd K (x - xo + h^ι) - dnι+n1 …dnd+nd K (x - x)i =
=dnι+1+n1 …dnd+ndK(x - x0).	(41)
Of course by symmetry the same can be said about the increase of any other exponent. To conclude
the induction proof we simply recall that by definition EZ(x)Z(x0) = K(x — x0).	□
18