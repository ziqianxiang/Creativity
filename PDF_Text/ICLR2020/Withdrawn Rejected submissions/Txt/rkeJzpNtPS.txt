Under review as a conference paper at ICLR 2020
LOCALLY ADAPTIVE ACTIVATION FUNCTIONS WITH
slope recovery term for deep and physics-
INFORMED NEURAL NETWORKS
Anonymous authors
Paper under double-blind review
Ab stract
We propose two approaches of locally adaptive activation functions namely, layer-
wise and neuron-wise locally adaptive activation functions, which improve the
performance of deep and physics-informed neural networks. The local adapta-
tion of activation function is achieved by introducing scalable hyper-parameters
in each layer (layer-wise) and for every neuron separately (neuron-wise), and
then optimizing it using the stochastic gradient descent algorithm. Introduction of
neuron-wise activation function acts like a vector activation function as opposed to
the traditional scalar activation function given by fixed, global and layer-wise ac-
tivations. In order to further increase the training speed, an activation slope based
slope recovery term is added in the loss function, which further accelerate conver-
gence, thereby reducing the training cost. For numerical experiments, a nonlinear
discontinuous function is approximated using a deep neural network with layer-
wise and neuron-wise locally adaptive activation functions with and without the
slope recovery term and compared with its global counterpart. Moreover, solution
of the nonlinear Burgers equation, which exhibits steep gradients, is also obtained
using the proposed methods. On the theoretical side, we prove that in the proposed
method the gradient descent algorithms are not attracted to sub-optimal critical
points or local minima under practical conditions on the initialization and learn-
ing rate. Furthermore, the proposed adaptive activation functions with the slope
recovery are shown to accelerate the training process in standard deep learning
benchmarks using CIFAR-10, CIFAR-100, SVHN, MNIST, KMNIST, Fashion-
MNIST, and Semeion data sets with and without data augmentation.
1	Introduction
In recent years, research on neural networks (NNs) has intensified around the world due to their suc-
cessful applications in many diverse fields such as speech recognition (Hinton et al., 2012), computer
vision (Krizhevsky et al., 2012), natural language translation (Wu et al., 2016), etc. Training of NN
is performed on data sets before using it in the actual applications. Various data sets are available for
applications like image classification, which is a subset of computer vision. MNIST (LeCun et al.,
1998) and their variants like, Fashion-MNIST (Xiao et al., 2017), and KMNIST (Clanuwat et al.,
2018) are the data sets for handwritten digits, images of clothing and accessories, and Japanese
letters, respectively. Apart from MNIST, Semeion (Brescia, 1994) is a handwritten digit data set
that contains 1593 digits collected from 80 persons. SVHN (Netzer et al., 2011) is another data set
for street view house numbers obtained from house numbers in Google Street View images. CI-
FAR (Krizhevsky et al., 2009) is the popular data set containing color images commonly used to
train machine learning algorithms. In particular, the CIFAR-10 data set contains 50000 training and
10000 testing images in 10 classes with image resolution of 32x32. CIFAR-100 is similar to the
CIFAR-10, except it has 100 classes with 600 images in each class, which is more challenging than
the CIFAR-10 data set.
Recently, NNs have also been used to solve partial differential equations (PDEs) due to their ability
to effectively approximate complex functions arising in diverse scientific disciplines (cf., Physics-
informed Neural Networks (PINNs), (Raissi et al., 2019)). PINNs can accurately solve both forward
1
Under review as a conference paper at ICLR 2020
problems, where the approximate solutions of governing equations are obtained, as well as inverse
problems, where parameters involved in the governing equation are inferred from the training data.
Highly efficient and adaptable algorithms are important to design the most effective NN which not
only increases the accuracy of the solution but also reduces the training cost. Various architec-
tures of NN like Dropout NN (Srivastava et al., 2014) are proposed in the literature, which can
improve the efficiency of the algorithm for specific applications. Activation function plays an im-
portant role in the training process of NN. In this work, we are particularly focusing on adaptive
activation functions, which adapt automatically such that the network can be trained faster. Various
methods are proposed in the literature for adaptive activation function, like the adaptive sigmoidal
activation function proposed by (Yu et al., 2002) for multilayer feedforward NNs, while (Qian et al.,
2018) focuses on learning activation functions in convolutional NNs by combining basic activation
functions in a data-driven way. Multiple activation functions per neuron are proposed (Dushkoff
& Ptucha, 2016), where individual neurons select between a multitude of activation functions. (Li
et al., 2013) proposed a tunable activation function, where only a single hidden layer is used and the
activation function is tuned. (Shen et al., 2004), used a similar idea of tunable activation function
but with multiple outputs. Recently, KUnc and Klema proposed a transformative adaptive activation
functions for gene expression inference, see (KUnc & Klema, 2019). One such adaptive activation
function is proposed (Jagtap & Karniadakis, 2019) by introducing scalable hyper-parameter in the
activation function, which can be optimized. Mathematically, it changes the slope of activation
function thereby increasing the learning process, especially during the initial training period. Due
to single scalar hyper-parameter, we call such adaptive activation functions globally adaptive activa-
tions, meaning that it gives an optimized slope for the entire network. One can think of doing such
optimization at the local level, where the scalable hyper-parameter are introduced hidden layer-wise
or even for each neuron in the network. Such local adaptation can further improve the performance
of the network. Figure 1 shows a sketch of a neuron-wise locally adaptive activation function based
Figure 1: Schematic of LAAF-PINN for the Burgers equation. The left NN is the uninformed
network while the right one induced by the governing equation is the informed network. The two
NNs share hyper-parameters and they both contribute to the loss function.
physics-informed neural network (LAAF-PINN), where both the NN part along with the physics-
informed part can be seen. In this architecture, along with the output of NN and the residual term
from the governing equation, the activation slopes from every neuron are also contributing to the
loss function in the form of slope recovery term.
The rest of the paper is organized as follows. Section 2 presents the methodology of the proposed
layer-wise and neuron-wise locally adaptive activations in detail. This also includes a discussion on
the slope recovery term, expansion of parametric space due to layer-wise and neuron-wise introduc-
tion of hyper-parameters, its effect on the overall training cost, and a theoretical result for gradient
decent algorithms. Section 3 gives numerical experiments, where we approximate a nonlinear dis-
continuous function using deep NN by the proposed approaches. We also solve the Burgers equation
using the proposed algorithm and present various comparisons in appendix B. Section 4 presents
numerical results with various standard deep learning benchmarks using CIFAR-10, CIFAR-100,
SVHN, MNIST, KMNIST, Fashion-MNIST, and Semeion data sets. Finally, in section 5, we sum-
marize the conclusions of our work.
2
Under review as a conference paper at ICLR 2020
2	Methodology
We use a NN of depth D corresponding to a network with an input layer, D - 1 hidden-layers and an
output layer. In the kth hidden-layer, Nk number of neurons are present. Each hidden-layer of the
network receives an output zk-1 ∈ RNk-1 from the previous layer where an affine transformation
of the form
Lk(zk-1) , wkzk-1 + bk	(1)
is performed. The network weights wk ∈ RNk×Nk-1 and bias term bk ∈ RNk associated with
the kth layer are chosen from independent and identically distributed sampling. The nonlinear-
activation function σ(∙) is applied to each component of the transformed vector before sending it as
an input to the next layer. The activation function is an identity function after an output layer. Thus,
the final neural network representation is given by the composition
uΘ(z) = (LD ◦ σ ◦ LD-1 ◦ . . . ◦ σ ◦ L1)(z),
where the operator ◦ is the composition operator, Θ = {wk , bk}kD=1 represents the trainable pa-
rameters in the network, u is the output and z0 = z is the input.
In supervised learning of solution of PDEs, the training data is important to train the neural network,
which can be obtained from the exact solution (if available) or from high-resolution numerical so-
lution given by efficient numerical schemes and it can be even obtained from carefully performed
experiments, which may yield both high- and low-fidelity data sets.
We aim to find the optimal weights for which the suitably defined loss function is minimized. In
PINN the loss function is defined as
J(Θ) = MSEF + MSEu	(2)
where the mean squared error (MSE) is given by
1 Nf	1 Nu
MSEF = Nr X IF(Xif ,yf ,tf )12, MSEu = Nr X ∣ui -u(xU,yU,tu)∣2.
Nf i=1	Nu i=1
Here {xif, yfi , tif}iN=f1 represents the residual training points in space-time domain, while
{xiu , yui , tiu }iN=u1 represents the boundary/initial training data. The neural network solution must sat-
isfy the governing equation at randomly chosen points in the domain, which constitutes the physics-
informed part of neural network given by first term, whereas the second term includes the known
boundary/initial conditions, which must be satisfied by the neural network solution. The resulting
optimization problem leads to finding the minimum of a loss function by optimizing the parame-
ters like, weights and biases, i.e., We seek to find w*, b* = argmi□wb∈θ (J(w, b)). One can
approximate the solutions to this minimization problem iteratively by one of the forms of gradi-
ent descent algorithm. The stochastic gradient descent (SGD) algorithm is widely used in machine
learning community see, Ruder (2016) for a complete survey. In SGD the weights are updated as
wm+1 = wm - mVwm Jm(w), where ηι > 0 is the learning rate. SGD methods can be initialized
with some starting value w0. In this work, the ADAM optimizer Kingma & Ba (2014), which is a
variant of the SGD method is used.
A deep network is required to solve complex problems, which on the other hand is difficult to train.
In most cases, a suitable architecture is selected based on the researcher’s experience. One can also
think of tuning the network to get the best performance out of it. In this regard, we propose the
following two approaches to optimize the adaptive activation function.
1. Layer-wise locally adaptive activation functions (L-LAAF)
Instead of globally defining the hyper-parameter a for the adaptive activation function, let us define
this parameter hidden layer-wise as
σ(ak Lk(zk-1)).
This gives additional D - 1 hyper-parameters to be optimized along with weights and biases. Here,
every hidden-layer has its own slope for the activation function.
2. Neuron-wise locally adaptive activation functions (N-LAAF)
3
Under review as a conference paper at ICLR 2020
One can also define such activation function at the neuron level as
σ(ak Lk(ZkT)), ∀i = 1, 2, ∙∙∙ , Nk
This gives additional PkD=-11 Nk hyper-parameters to be optimized. Neuron-wise activation function
acts as a vector activation function as opposed to scalar activation function given by L-LAAF and
global adaptive activation function (GAAF) approaches, where every neuron has its own slope for
the activation function.
The resulting optimization problem leads to finding the minimum of a loss function by optimizing
ak along with weights and biases, i.e., we seek to find (ak )* = argmin(αk)∈R+∖{o} (J (ak)), ∀i =
1,2,…，N. The process of training NN can be further accelerated by multiplying a with scaling
factor n > 1. The final form of the activation function is given by σ(naik Lk(zk-1)). It is important
to note that the introduction of the scalable hyper-parameter does not change the structure of the
loss function defined previously. Then, the final adaptive activation function based neural network
representation of the solution is given by
U(Θ(Z) = (LD ◦ σ ◦ naD-1 Ld-i ◦ σ ◦ naD-2Ld-2 ◦ ... ◦ σ ◦ na1 Lι)(z).
In this case, the set of trainable parameters Θ consists of {wk, bk}D=r and {aik}kD=-11, ∀i =
1,2,…，N. In all the proposed methods, the initialization of scalable hyper-parameters is done
such that naik = 1, ∀n.
2.1 Loss function with slope recovery term
The main motivation of adaptive activation function is to increase the slope of activation function,
resulting in non-vanishing gradients and fast training of the network. It is clear that one should
quickly increase the slope of activation in order to improve the performance of NN. Thus, instead
of only depending on the optimization methods, another way to achieve this is to include the slope
recovery term based on the activation slope in the loss function as
J(Θ) = MSEF + MSEu + S (a),
(3)
where the slope recovery term S(a) is given by
f D—1 PD=II N(QJk)
S (a) =	------------1 / Nkk
I 1 PD-1 N P Pi=1 ai
I D-1 2^k=ι N I Nk
for L-LAAF
for N-LAAF
where N is a linear/nonlinear operator. Although, there can be several choices of this operator,
including the linear identity operator, in this work we use the exponential operator. The main reason
behind this is that such term contributes to the gradient of the loss function without vanishing. The
overall effect of inclusion of this term is that it forces the network to increase the value of activation
slope quickly thereby increasing the training speed.
We now provide a theoretical result regarding the proposed methods. The following theorem states
that a gradient descent algorithm minimizing our objective function J(Θ) in equation 3 does not
converge to a sub-optimal critical point or a sub-optimal local minimum, for neither L-LAAF nor
N-LAAF, given appropriate initialization and learning rates. In the following theorem, We treat Θ as
a real-valued vector. Let Jc(0) = MSEF + MSEu with the constant network uΘ(z) = uΘ(z0) =
c ∈ RND for all z, z0 where c is a constant.
Theorem 2.1. Let (Θm)m∈N be a sequence generated by a gradient descent algorithm as Θm+1 =
Θm - ηmVJ(Θ). Assume that J(Θo) < Jc(0) + S(0) for any c ∈ RND, J is differentiable,
and that for each i ∈ {1,...,Nf}, there exist differentiable function φb and input P such that
|F(Xf, yf, tf )|2 = 2i(uθ(Pi)). Assume that at least one ofthe following three conditions holds.
(i) (constant learning rate) VJ is Lipschitz continuous with Lipschitz constant C (i.e., ∣∣VJ(Θ) 一
VJ(Θ0)k2 ≤ C∣∣Θ — Θ0k2 for all Θ, Θ0 in its domain), and E ≤ ηm, ≤ (2 — e)/C, where
is a fixed positive number.
4
Under review as a conference paper at ICLR 2020
(ii)	(diminishing learning rate) VJ is LiPschitz continuous, ηm → 0 and E∞=0 ηm = ∞.
(iii)	(adaptive learning rate) the learning rate ηm is chosen by the minimization rule, the limited
minimization rule, the Armjio rule, or the Goldstein rule (Bertsekas, 1997).
ʌ
Then, for both L-LAAF and N-LAAF, no limit Point of (Θm)m∈N is a sub-oPtimal critical Point or a
sub-oPtimal local minimum.
ʌ ʌ
The initial condition J(Θ0) < Jc(0) + S(0) means that the initial value J(Θ0) needs to be less
than that of a constant network plus the highest value of the slope recovery term. Here, note that
S(1) < S(0). The proof of Theorem 2.1 is included in appendix A.
3 Neural network approximation of nonlinear discontinuous
FUNCTION
In this section, we shall solve a regression problem of a nonlinear function approximation using
deep neural network. The Burgers equation using physics-informed neural network is solved in
appendix B. In this test case, a standard neural network (without physics-informed part) is used to
approximate a discontinuous function. In this case, the loss function consists of the data mismatch
and the slope recovery term as
1 Nu
J(S) = N X |ui - u(Xu,yU,tu)l2 + S⑷.
Nu
i=1
The following discontinuous function with discontinuity at x = 0 location is approximated by a
deep neural network.
( )	0.2 sin(6x)	If x ≤ 0
u(x) = 1 + 0.1 x cos(18x) Otherwise.
(4)
Here, the domain is [-3, 3] and the number of training points used is 300. The activation function is
tanh, learning rate is 2.0e-4 and the number of hidden layers are four with 50 neurons in each layer.
Figure 2 shows the solution (first column), solution in frequency domain (second column) and point-
wise absolute error in log scale (third column). The solution by standard fixed activation function
is given in the first row, GAAF solution is given in second row, whereas the third row shows the
solution given by L-LAAF without and with (fourth row) slope recovery term. The solution given
by N-LAAF without slope recovery term is shown in the fifth row and with slope recovery term in the
sixth row. We see that the NN training speed increases for the locally adaptive activation functions
compared to fixed and globally adaptive activations. Moreover, both L-LAAF and N-LAAF with
slope recovery term accelerate training and yield the least error as compared to other methods.
Figure 3 (top) shows the variation of na for GAAF, whereas the second row, left and right shows
the layer-wise variation of nak for L-LAAF without and with the slope recovery term respectively.
The third row, left and right shows the variation of scaled hyper-parameters for N-LAAF without
and with the slope recovery term respectively, where the mean value of naik along with its standard
deviation (Std) are plotted for each hidden-layer. We see that the value of na is quite large with the
slope recovery term which shows the rapid increase in the activation slopes. Finally, the comparison
of the loss function is shown in figure 4 for standard fixed activation, GAAF, L-LAAF and N-LAAF
without the slope recovery (left) and for L-LAAF and N-LAAF with the slope recovery (right) using
a scaling factor of 10. The Loss function for both L-LAAF and N-LAAF without the slope recovery
term decreases faster, especially during the initial training period compared to the fixed and global
activation function based algorithms.
4 Numerical results for deep learning problems
The previous sections demonstrated the advantages of adaptive activation functions with PINN for
physics related problems. One of the remaining questions is whether or not the advantage of adaptive
activations remains with standard deep neural networks for other types of deep learning applications.
To explore the question, this section presents numerical results with various standard benchmark
problems in deep learning. Figures 5 and 6 shows the mean values and the uncertainty intervals
5
Under review as a conference paper at ICLR 2020
1.2
1.0
Exact
Predicted at Iter ：
Predicted at Iter ：
Predicted at Iter ：
0.8
0.6
0.2
0.0
-0.2
1.2
,=
,=.
1.0
0.8
0.6
10
0.2
0.0
-0.2
1.2
1.0
0.8
0.6
0.2
0.0
-0.2
1.2
1.0
0.8
0.6
0.2
0.0
-0.2
1.2
1.0
0.8
0.6
：15000
:8000
■ = 2000
=8000
=8000
2®AM
-----Exact
X Predicted at Iter
-----Predicted at Iter
-----Exact
X Predicted at Iter
-----Predicted at Iter
---Predicted at Iter = 2000
=15000
=15000
=15000
Predicted at Iter = 2000
飞0.4
0.4
a
0.4
a
0.4
20
15
,=.
20
15
10
Exact
,=
,=.
20
15
10
Frequency index
==
,=.
20
15
10
Predicted at Iter ：
Predicted at Iter ：
：15000
:8000
：15000
:8000
：15000
:8000
■ = 2000
■ = 2000
Predicted at Iter = 2000
----Exact
-X- Predicted at Iter =
----Exact
-K	Predicted	at	Iter	=	15000
----Predicted	at	Iter	=	8000
Wn°SqV
Exact
X Predicted at Iter
----Predicted at Iter
Wv
=8000
Predicted at Iter = 2000
Predicted at Iter = 15000
Predicted at Iter = 8000
Predicted at Iter = 2000
λ∕Vv
X
a
0.4
0.2
0.0
-0.2
WJxJ
-3
-2
-1
Wn°SqV
Frequency index
Frequency index
Exact
Predicted at Iter ：
Predicted at Iter ：
—■ PnsriiadH a.t. T⅛r :
■ = 2000
----Exact
-M- Predicted at Iter ：
Predicted at Iter ：
Predicted at Iter ：


0
1
2
3
X


Figure 2:	Discontinuous function: Neural network solution using standard fixed activation (first
row), GAAF (second row), L-LAAF without (third row) and with (fourth row) slope recovery term,
and N-LAAF without (fifth row) and with (sixth row) slope recovery term using the tanh activation.
First column shows the solution which is also plotted in frequency domain (zoomed-view) as shown
by the corresponding second column. Third column gives the point-wise absolute error in the log
scale for all the cases.
6
Under review as a conference paper at ICLR 2020
1.0
0
Epochs
10
5 ■
Layer 1
Layer 2
Layer 3
Layer 4
1.5
0
3
2
2
1
1
Std	2	Std
5000	10000	15000
Epochs
Layer 3
5000	10000	15000	0
Epochs
Layer 2
3
2
1
5000	10000	15000	0
Epochs
Layer 1
0	5000	10000	15000
Epochs
0	5000	10000	15000
Epochs
0	5000	10000	15000
Epochs
5000	10000	15000
Epochs
Layer 4
0	5000	10000	15000
Epochs
Layer 1
Layer 2
Layer 4
0	5000	10000	15000	0	5000	10000	15000
0	5000	10000	15000
20
10
0
Epochs
Layer 1
0	5000	10000	15000
Epochs
Epochs
Layer 2
0	5000	10000	15000
Epochs
Layer 3
0	5000	10000	15000
Epochs
0	5000	10000	15000
Epochs
Layer 4
20
10
0
0	5000	10000	15000
Epochs
Figure 3:	DiScontinuouS function: compariSon of variation of na for GAAF (top row), L-LAAF
without (Second row, left) and with (Second row, right) Slope recovery term, and N-LAAF without
(third row, left) and with (third row, right) Slope recovery term uSing tanh activation. In caSe of
N-LAAF, the mean value of naik along with itS Standard deviation (Std) are plotted for each hidden-
layer.
Figure 4: CompariSon of loSS function for Standard fixed activation, GAAF, L-LAAF and N-LAAF
without (left) and with (right) Slope recovery term uSing Scaling factor n = 10.
2000	4000	6000	8000	10000	12000	14000
Epochs
of the training loSSeS for fixed activation function (Standard), GAAF, L-LAAF, and N-LAAF, by
uSing the Standard deep learning benchmarkS. The Solid and daShed lineS are the mean valueS over
three random trialS with random SeedS. The Shaded regionS repreSent the intervalS of 2×(the Sample
Standard deviationS) for each method. FigureS 5 and 6 conSiStently Show that adaptive activation
SSO- uω2
10	20	30	40	50
epoch
(a) Semeion
IO 20	30	40	50
epoch
(b) KMNIST
10	20	30	40	50
epoch
(c) MNIST
10	20	30	40	50
epoch
(d) FaShion-MNIST
10	20	30	40	50
epoch
(e) CIFAR-10
10	20	30	40	50
epoch
⑴ CIFAR-100
10	20	30	40	50
epoch
(g) SVHN
Figure 5:	Training loss in log scale versus epoch without data augmentation.
7
Under review as a conference paper at ICLR 2020
(f) CIFAR-100
50	100	150	200
epoch
0	50	100	150	200
epoch
50	100	150	200
epoch
(e) CIFAR-10
(d) FaShion-MNIST
50 IOO 150	200
epoch
(g) SVHN
Figure 6:	Training loss in log scale versus epoch with data augmentation.
accelerates the minimization process of the training loss values. Here, all of GAAF, L-LAAF and
N-LAAF use the slope recovery term, which improved the methods without the recovery term.
Accordingly, the results of GAAF are also new contributions of this paper. In general, L-LAAF
improved against GAAF as expected. The standard cross entropy loss was used for training and
plots. We used pre-activation ResNet with 18 layers (He et al., 2016) for CIFAR-10, CIFAR-100,
and SVHN data sets, whereas we used a standard variant of LeNet (LeCun et al., 1998) with ReLU
for other data sets; i.e., the architecture of the variant of LeNet consists of the following five layers
(with the three hidden layers): (1) input layer, (2) convolutional layer with 64 5 × 5 filters, followed
by max pooling of size of2 by 2 and ReLU, (3) convolutional layer with 64 5 × 5 filters, followed by
max pooling of size of 2 by 2 and ReLU, (4) fully connected layer with 1014 output units, followed
by ReLU, and (5) Fully connected layer with the number of output units being equal to the number of
target classes. All hyper-parameters were fixed a priori across all different data sets and models. We
fixed the mini-batch size s to be 64, the initial learning rate to be 0.01, the momentum coefficient to
be 0.9 and we use scaling factor n = 1 and 2. The learning rate was divided by 10 at the beginning
of 10th epoch for all experiments (with and without data augmentation), and of 100th epoch for
those with data augmentation.
5 Conclusions
In this paper, we present two versions of locally adaptive activation functions namely, layer-wise and
neuron-wise locally adaptive activation functions. Such local activation functions further improve
the training speed of the neural network compared to its global predecessor. To further accelerate the
training process, an activation slope based slope recovery term is added in the loss function for both
layer-wise and neuron-wise activation functions, which is shown to enhance the performance of the
neural network. Various NN and PINN test cases like nonlinear discontinuous function approxima-
tion and Burgers equation respectively, and benchmark deep learning problems like MNIST, CIFAR,
SVHN etc are solved to verify our claim. Moreover, we theoretically prove that no sub-optimal crit-
ical point or local minimum attracts gradient descent algorithms in the proposed methods (L-LAAF
and N-LAAF) with the slope recovery term under only mild assumptions.
8
Under review as a conference paper at ICLR 2020
References
Cea Basdevant, M Deville, P Haldenwang, JM Lacroix, J Ouazzani, R Peyret, Paolo Orlandi, and
AT Patera. Spectral and finite difference solutions of the burgers equation. Computers & fluids,
14(1):23-41,1986.
Harry Bateman. Some recent researches on the motion of fluids. Monthly Weather Review, 43(4):
163-170, 1915.
Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.
Automatic differentiation in machine learning: a survey. Journal of machine learning research,
18(153), 2018.
Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48(3):
334-334, 1997.
M Brescia. Semeion handwritten digit data set. Semeion Research Center of Sciences of Communi-
cation, 1994.
Johannes Martinus Burgers. A mathematical model illustrating the theory of turbulence. In Advances
in applied mechanics, volume 1, pp. 171-199. Elsevier, 1948.
Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David
Ha. Deep learning for classical japanese literature. arXiv preprint arXiv:1812.01718, 2018.
Michael Dushkoff and Raymond Ptucha. Adaptive activation functions for deep networks. Elec-
tronic Imaging, 2016(19):1-5, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016.
Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, An-
drew Senior, Vincent Vanhoucke, Patrick Nguyen, Brian Kingsbury, et al. Deep neural networks
for acoustic modeling in speech recognition. IEEE Signal processing magazine, 29, 2012.
Ameya D Jagtap and George Em Karniadakis. Adaptive activation functions accelerate convergence
in deep and physics-informed neural networks. arXiv preprint arXiv:1906.01170, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
viadim´r KUnc and jir´ Klema. On transformative adaptive activation functions in neural networks
for gene expression inference. bioRxiv, pp. 587287, 2019.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Bin Li, Yibin Li, and Xuewen Rong. The extreme learning machine learning algorithm with tunable
activation function. Neural Computing and Applications, 22(3-4):531-539, 2013.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.
Sheng Qian, Hua Liu, Cheng Liu, Si Wu, and Hau San Wong. Adaptive activation functions in
convolutional neural networks. Neurocomputing, 272:204-212, 2018.
Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A
deep learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of Computational Physics, 378:686-707, 2019.
9
Under review as a conference paper at ICLR 2020
Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint
arXiv:1609.04747, 2016.
Yanjun Shen, Bingwen Wang, Fangxin Chen, and Liang Cheng. A new multi-output neural model
with tunable activation function and its applications. Neural processing letters, 20(2):85-104,
2004.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Gerald Beresford Whitham. Linear and nonlinear waves, volume 42. John Wiley & Sons, 2011.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-
lation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144, 2016.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Chien-Cheng Yu, Yun-Ching Tang, and Bin-Da Liu. An adaptive activation function for multilayer
feedforward neural networks. In 2002 IEEE Region 10 Conference on Computers, Communi-
cations, Control and Power Engineering. TENCOM’02. Proceedings., volume 1, pp. 645-650.
IEEE, 2002.
A Proof of Theorem 2.1
We first prove the statement by contradiction for L-LAAF. Suppose that the parameter vector Θ
consisting of {wk, bk}D=ι and {ak}D=-1 is a limit point of (Θm)m∈N and a sub-optimal critical
point or a sub-optimal local minimum.
Let 'f := d(uθ^ (Pi)) and 'U := |ui - u© (Xu ,yU ,tU)∣2. Let Zfk and ZiUk be the outputs of the k-th
layer for ρi and (xiu, yui , tiu), respectively. Define
hif,k,j := nak(wk,jZfi,k-1 +bk,j) ∈ R,
and
hiu,k,j := nak(wk,jZui,k-1 +bk,j) ∈ R,
for all j ∈ {1, . . . , Nk}, where wk,j ∈ R1×Nk-1 and bk,j ∈ R.
ʌ
Following the proofs in (Bertsekas, 1997, Propositions 1.2.1-1.2.4), We have that VJ(Θ) = 0 and
J(Θ) < Jc(0) + S(0), for all three cases of the conditions corresponding the different rules of the
learning rate. Therefore, we have that for all k ∈ {1, . . . , D - 1},
∂ J(Θ)
∂ak
…Nf Nk ∂'i
=Nnf XX ∂f --
i=1 j=1 f
Nk …Nf ∂'i
=X N X ∂hfj --
j=1 i=1 f
= 0.
(5)
+ bk,j + 生 X X " (WkL + bk,j + ∂S(a)
Nuw j=1 ∂hukj	u	/	∂ak
+ bkj + 叁 X 乌h (WkjZi,k-1 + bkj + dS⅛)
Nu W ∂hukj	j ∂ak
10
Under review as a conference paper at ICLR 2020
Furthermore, we have that for all k ∈ {1, . . . , D - 1} and all j ∈ {1, . . . , Nk},
ʌ
∂J(Θ)
∂wk,
=nak Xf d'f i i,k-1x > . nak Xu d'U 7 i,k-1 ]>
=Nf 2^∂hkj(Zf ) + Nu 2-∂hUF(zu ) ,
i=1 f	i=1	u
=0
(6)
and
∂J(Θ) _ nak Xf ∂'f	nak Xf ∂'U _ 0
∂bk,j =	∂hf,kj +	∂hU,kj =.
By combining equation 5-equation 7, for all k ∈ {1,...,D - 1},
ʌ
0 = ak ∂J≡
∂ak
=£ Nk XX ∂f - bkj) + Nk XX ∂l⅛ (WykT + bkj) +
=Xwr JΘ !> + j JΘ ! + ak ”
♦Ik 端1.
(7)
k∂S(a)
a C ]
∂ak
Therefore,
0 = ak dSt(kα) = -ak(D - 1) (X exp(ak))	exp(ak),
which implies that for all ak = 0 since (D - 1) PkD=-11 exp(ak) exp(ak) 6= 0. This implies
that J(Θ) = Jc(0) + S(0), which contradicts with J(Θ) < Jc(0) + S(0). This proves the desired
statement for L-LAAF.
For N-LAAF, We prove the statement by contradiction. Suppose that the parameter vector Θ con-
sisting of {wk, bk}kD=ι and {aj}.d=l1 ∀j = 1, 2,…，Nk is a limit point of (Θm)m∈N and a SUb-
optimal critical point or a sub-optimal local minimum. Redefine
hif,k,j := najk(wk,jzfi,k-1 +bk,j) ∈ R,
and
hiu,k,j := najk(wk,jzui,k-1 +bk,j) ∈ R,
for all j ∈ {1, . . . , Nk}, where wk,j ∈ R1×Nk-1 and bk,j ∈ R. Then, by the same proof steps, we
have that VJ(Θ) = 0 and J(Θ) < Jc(0) + S(0), for all three cases of the conditions corresponding
the different rules of the learning rate. Therefore, we have that for all k ∈ {1, . . . , D - 1} and all
j ∈ {1, . . . ,Nk},
ʌ
∂ J(Θ)
F
(8)
—X -d'f-(wykτ + bk,j) + 土 XX	(wkykT + bk,j) + dS(a)
Nf = ∂hfkj	f	；	Nu = ∂hUkj	∂aj
0.
11
Under review as a conference paper at ICLR 2020
By combining equation 6-equation 8, for all k ∈ {1,..., D - 1} and all j ∈ {1,..., Nk },
ʌ
0=ak ∂J(Θ)
j ∂aj
najk
i=1
Nf
∂'f	i . ,k_ 1	, n naj NU ∂'i	- “ 1	-	LdS(a)
f(W zf- + b )+Nj X 命 (w zu	+ b )+aj 亮
>+bkj d 黑!+ak dSa
∂bk,j	j ∂ajk
i=1
wk,j
k ∂S (a)
=j "∂Of.
Therefore,
0 = aj ∂afe0^l = -2ak (D - I)
exp
PiN=k1 aik
!!-2exp
PiN=k1 aik
which implies that for all aj = 0 since (D — 1) (PD=II exp ( PNIai )) exp ( PiNI ai ) = 0.
This implies that J(Θ) = Jc(0) + S(0), which contradicts with J(Θ) < Jc(0) + S(0). This proves
the desired statement for N-LAAF.
□
B Appendix: Burgers equation
The Burgers equation is one of the fundamental partial differential equation arising in various fields
such as nonlinear acoustics, gas dynamics, fluid mechanics etc, see Whitham (2011) for more details.
The Burgers equation was first introduced by H. Bateman, Bateman (1915) and later studied by J.M.
Burgers, Burgers (1948) in the context of theory of turbulence. Here, we consider the viscous
Burgers equation given by equation equation 9 along with its initial and boundary conditions. The
non-linearity in the convection term develops very steep solution due to small e value. We consider
the Burgers equation given by
Ut + Uux = euχχ, X ∈ [-1, 1], t > 0	(9)
with initial condition u(x, 0) = — sin(πx), boundary conditions u(-1, t) = u(1,t) = 0 and e =
0.01/n. The analytical solution can be obtained using the Hopf-Cole transformation, see Basdevant
et al. (1986) for more details. The number of boundary and initial training points is 400, whereas the
number of residual training points is 10000. The activation function is tanh, learning rate is 0.0008
and the number of hidden layers are 6 with 20 neurons in each layer.
Figure 7 shows the evolution of frequency plots of the solution at three different times using the
standard fixed activation function (first row), global adaptive activation function (second row), L-
LAAF without (third row) and with (fourth row) slope recovery term, N-LAAF without (fifth row)
and with (sixth row) slope recovery term using scaling factor n = 10. Again, for both L-LAAF
and N-LAAF the frequencies are converging faster towards the exact solution (shown by black line)
with and without slope recovery term as compared to the fixed and global activation function based
algorithms.
Figure 8 shows the comparison of solution of the Burgers equation using the standard fixed activation
(first row), global adaptive activation function (second row), L-LAAF with slope regularization
(third row), and N-LAAF with slope regularization (fourth row) using n = 10. Columns from
left to right represent the solution t = 0.25, 0.5 and 0.75, respectively. As discussed in Jagtap
& Karniadakis (2019), t = 0.25 case needs large number of iterations for convergence, whereas
both t = 0.5 and 0.75 converges faster. Both L-LAAF and N-LAAF gives more accurate solution
compared to GAAF. Figure 9 shows the loss function for standard fixed, GAAF, L-LAAF and N-
LAAF without slope recovery term (top left) and with slope recovery term (top right). Loss function
12
Under review as a conference paper at ICLR 2020
101
10-1
10-1
10-3
ɪ
10-5
10-7
10-9
101
10-1
10-7
10-3
10-5
10-2
40
Frequency index
Frequency index
Frequency index
10-2
Frequency index
10-3
10-5
10-9
Frequency index
Frequency index
10-1
10-3
ɪ
10-5
10-7
10-9
Frequency index
Frequency index
40
Frequency index
40
Frequency index
40	60
Frequency index
40
Frequency index
Figure 7: Comparison of solution of the Burgers equation in frequency domain using standard fixed
activation (first row), GAAF (second row), L-LAAF without (third row) and with (fourth row) slope
recovery term, N-LAAF without (fifth row) and with (sixth row) slope recovery term using n =
10. Columns (left to right) represent the solution in frequency domain at t = 0.25, 0.5 and 0.75,
respectively.
10-7
10-9
Frequency index
Frequency index
13
Under review as a conference paper at ICLR 2020
t = 0.75
-1	0	1
X
t = 0.75
-1	0	1
X
(a-≡(a-≡
Figure 8: Comparison of solution of the Burgers equation after 2000 iteration using standard fixed
activation (first row), GAAF (second row), L-LAAF with slope recovery term (third row), and N-
LAAF with slope recovery term (fourth row) using n = 10. Columns (left to right) represent the
solution t = 0.25, 0.5 and 0.75, respectively.
Figure 9: Comparison of loss function for standard fixed, GAAF, L-LAAF and N-LAAF without
slope recovery term (left) and L-LAAF and N-LAAF with slope recovery term (right).
14
Under review as a conference paper at ICLR 2020
decreases faster for all adaptive activations, in particular GAAF. Even though it is difficult to see
from the actual solution plots given by figure 8, one can see from the table 1 that both L-LAAF and
N-LAAF gives smaller relative L2 error using slope recovery term.
	Standard	GAAF	L-LAAF WIo SRT	N-LAAF w/o SRT	L-LAAF with SRT	N-LAAF with SRT
ReL L2 error	1.9139e-01	9.5171e-02-	8.8663e-02-	9.1294e-02-	7.6934e-02-	8.3784e-02-
Table 1: Burgers equation: Relative L2 error after 2000 iterations for fixed and all cases of adaptive
activation functions. Slope recovery term is abbreviated as SRT.
Layer 1
1.2 -
-■
1.0 -Ll_____I_______r
0	1000	2000
Layer 2	Layer 3
厂厂
ι.0
0	1000	2000	0	1000	2000
Layer 4	Layer 5	Layer 6
1.75
1.50
1.25
1.00
0	1000	2000	0	1000	2000	0	1000	2000
Epochs	Epochs	Epochs
Layer 1	Layer 2	Layer 3
1.75 -	20
1.50 - ，	/
J	/
,	,	,11.00 -I,	,	, I ι.0 -I,	,	,
0	1000	2000	0	1000	2000	0	1000	2000
0	1000	2000	0	1000	2000	0	1000	2000
Epochs
Epochs
Epochs
Figure 10: Burgers equation: comparison of nak for L-LAAF for all six hidden-layers. First three
columns represent results for L-LAAF without slope recovery term whereas the last three columns
are with slope recovery term. In all simulations, the scaling factor n is 10.
Figure 10 shows the comparison of layer-wise variation of nak for L-LAAF with and without slope
recovery term. It can be seen that, the presence of slope recovery term further increases the slope of
activation function thereby increasing the training speed. Similarly, figure 11 shows the mean and
standard deviation of naik for N-LAAF with and without slope recovery term, which again validates
that with slope recovery term network training speed increases.
Layer 1
Mean
Std
2.0 -
1.5 -
1.0 -
1000	2000
Layer 2
2.0 -
1.5 -
1.0 -
0	1000	2000
Layer 4
Layer 5
2.0 -
1.5 -
1.0 -
0	1000	2000	0	1000	2000
Epochs	Epochs
Epochs
Epochs
Epochs
Epochs
Figure 11: Burgers equation: comparison of mean and standard deviation of naik for N-LAAF for
all six hidden-layers. First three columns represent resuls for N-LAAF without the slope recovery
term whereas the last three columns are with slope recovery term. In all simulations, the scaling
factor n is 10.
15