Under review as a conference paper at ICLR 2020
Single Deep Counterfactual Regret Minimiza-
TION
Anonymous authors
Paper under double-blind review
Ab stract
Counterfactual Regret Minimization (CFR) is the most successful algorithm for
finding approximate Nash equilibria in imperfect information games. However,
CFR’s reliance on full game-tree traversals limits its scalability and generality.
Therefore, the game’s state- and action-space is often abstracted (i.e. simplified)
for CFR, and the resulting strategy is then mapped back to the full game. This
requires extensive expert-knowledge, is not practical in many games outside of
poker, and often converges to highly exploitable policies. A recently proposed
method, Deep CFR, applies deep learning directly to CFR, allowing the agent to
intrinsically abstract and generalize over the state-space from samples, without
requiring expert knowledge. In this paper, we introduce Single Deep CFR (SD-
CFR), a variant of Deep CFR that has a lower overall approximation error by
avoiding the training of an average strategy network. We show that SD-CFR is
more attractive from a theoretical perspective and empirically outperforms Deep
CFR with respect to exploitability and one-on-one play in poker.
1	Introduction
In perfect information games, players usually seek to play an optimal deterministic strategy. In
contrast, sound policy optimization algorithms for imperfect information games converge towards a
Nash equilibrium, a distributional strategy characterized by minimizing the losses against a worst-case
opponent. The most popular family of algorithms for finding such equilibria is Counterfactual Regret
Minimization (CFR) (Zinkevich et al., 2008). Conventional CFR methods iteratively traverse the
game-tree to improve the strategy played in each state. For instance, CFR+ (Tammelin, 2014), a
fast variant of CFR, was used to solve two-player Limit Texas Hold’em Poker (Bowling et al., 2015;
Tammelin et al., 2015), a variant of poker frequently played by humans.
However, the scalability of such tabular CFR methods is limited since they need to visit a given state
to update the policy played in it. In games too large to fully traverse, practitioners hence often employ
domain-specific abstraction schemes (Ganzfried & Sandholm, 2014; Brown et al., 2015) that can be
mapped back to the full game after training has finished. Unfortunately, these techniques have been
shown to lead to highly exploitable policies in the large benchmark game Heads-Up No-Limit Texas
Hold’em Poker (HUNL) (Lisy & Bowling, 2016) and typically require extensive expert knowledge.
To address these two problems, researchers started to augment CFR with neural network function
approximation, first resulting in DeePStack (MoravcIk et al., 2017). Concurrently with LibratUs
(Brown & Sandholm, 2018a), DeepStack was one of the first algorithms to defeat professional poker
players in HUNL, a game consisting of 10160 states and thus being far too large to fully traverse.
While tabular CFR has to visit a state of the game to update its policy in it, a parameterized policy
may be able to play an educated strategy in states it has never seen before. Purely parameterized (i.e.
non-tabular) policies have led to great breakthroughs in AI for perfect information games (Mnih et al.,
2015; Schulman et al., 2017; Silver et al., 2017) and were recently also applied to large imperfect
information games by Deep CFR (Brown et al., 2018a) to mimic a variant of tabular CFR from
samples.
Deep CFR’s strategy relies on a series of two independent neural approximations. In this paper, we
introduce Single Deep CFR (SD-CFR), a simplified variant of Deep CFR that obtains its final strategy
after just one neural approximation by using what Deep CFR calls value networks directly instead of
training an additional network to approximate the weighted average strategy. This reduces the overall
1
Under review as a conference paper at ICLR 2020
sampling- and approximation error and makes training more efficient. We show experimentally that
SD-CFR improves upon the convergence of Deep CFR in poker games and outperforms Deep CFR
in one-one-one matches.
2	Extensive-form games
This section introduces extensive-form games and the notation we will use throughout this work.
Formally, a finite two-player extensive-form game with imperfect information is a set of histories
H, where each history is a path from the root φ ∈ H to any particular state. The subset Z ⊂ H
contains all terminal histories. A(h) is the set of actions available to the acting player at history
h, who is chosen from the set {1, 2, chance} by the player function P (h). In any h ∈ H where
P(h) = chance, the action is chosen by the dynamics of the game itself. Let N = {1, 2} be the set
of both players. When referring to a player i ∈ N, we refer to his opponent by -i. All nodes z ∈ Z
have an associated utility u(z) for each player. This work focuses on zero-sum games, defined by
the property ui(z) = -u-i(z) for all z ∈ Z.
Imperfect information is represented by partitioning H into information sets. An information set
Ii is a subset of H, where histories h, h0 ∈ H are in the same information set if and only if player i
cannot distinguish between h and h0 given his private and all available public information. For each
player i ∈ N, an information partition Ii is a set of all such information sets. Let A(I) = A(h)
and P (I) = P (h) for all h ∈ I and each I ∈ Ii.
Each player i chooses actions according to a behavioural strategy σi, with σi (I, a) being the
probability of choosing action a when in I. We refer to a tuple (σ1, σ2) as a strategy profile σ.
Let πσ (h) be the probability of reaching history h if both players follow σ and let πiσ (h) be the
probability of reaching h if player i acts according to σi and player -i always acts deterministically
to get to h. It follows that the probability of reaching an information set I if both players follow σ is
πσ(I) = Ph∈I πσ (h) andisπiσ(I) = Ph∈Iπiσ(h) if-iplaystogettoI.
Player i’s expected utility from any history h assuming both players follow strategy profile σ from h
onward is denoted by uiσ(h). Thus, their expected utility over the whole game given a strategy profile
σ can be written as uiσ (φ) = Pz∈Z πσ (z)ui (z).
Finally, a strategy profile σ = (σ1 , σ2) is a Nash equilibrium if no player i could increase their
expected utility by deviating from σi while -i plays according to σ-i. We measure the exploitability
e(σ) of a strategy profile by how much its optimal counter strategy profile (also called best response)
can beat it by. Let us denote a function that returns the best response to σi by BR(σi). Formally,
e(σ) = -	(ui(σi, BR(σi))
i∈N
3	Counterfactual Regret Minimization (CFR)
Counterfactual Regret Minimization (CFR) (Zinkevich et al., 2008) is an iterative algorithm. It
can run either simultaneous or alternating updates. If the former is chosen, CFR produces a new
iteration-strategy σit for all players i ∈ N on each iteration t. In contrast, alternating updates produce
a new strategy for only one player per iteration, with player t mod 2 updating his on iteration t.
To understand how CFR converges to a Nash equilibrium, let us first define the instantaneous regret
for player i of action a ∈ A(I) in any I ∈ Ii as
rit(I,a) = π-σti(I)(viσt(I, a) - viσt(I))	(1)
where vσ(I) = Ph∈ι 匚∏σu(ι)"h and vσ(I, a) = Ph∈ι π-i(h)u5(h)-→). Intuitively, rt(I, a)
quantifies how much more player i would have won (in expectation), had he always chosen a
in I and played to get to I but according to σt thereafter. The overall regret on iteration T is
RiT (I, a) = PtT=1 rit(I, a). Now, the iteration-strategy for player i can be derived by
σit+1(I,a)
R Rt(i,α)+
J Pα∈A(I) Rt(I,G) +
[w∏
ifPg∈A(I) Rt(I, a)+ > 0
otherwise
(2)
2
Under review as a conference paper at ICLR 2020
where x+ = max(x, 0). Note that σi0 (I, a)
1
W)
The iteration-strategy profile σt does not converge to an equilibrium as t → ∞ in most variants of
CFR 1. The policy that has been shown to converge to an equilibrium profile is the average strategy
σT. For all I ∈I and each a ∈ A(I) it is defined as
T (T ʌ	PT=1 ∏σt (I )σt(I,a)
i (, )=	PL ∏σt (I)
(3)
3.1	Variations of CFR
Aiming to solve ever bigger games, researchers have proposed many improvements upon vanilla CFR
over the years (Tammelin et al., 2015; Brown & Sandholm, 2018a; Moravclk et al., 2017). These
improvements include alternative methods for regret updates (Tammelin, 2014; Brown & Sandholm,
2018b), automated schemes for abstraction design (Ganzfried & Sandholm, 2014), and sampling
variants of CFR (Lanctot et al., 2009). Many of the most successful algorithms of the recent past
also employ real-time solving or re-solving (Brown et al., 2018b; MoravClk et al., 2017).
Discounted CFR (DCFR) (Brown & Sandholm, 2018b) slightly modifies the equations for RiT(I, a)
and σTT. A special case OfDCFR is linear CFR (LCFR), where the contribution of the instantaneous
regret of iteration t as well as the contribution of σt to στ is weighted by t. This change alone
suffices to let LCFR converge up to two orders of magnitude faster than vanilla CFR does in some
large games.
Monte-Carlo CFR (MC-CFR) (Lanctot et al., 2009) proposes a family of tabular methods that visit
only a subset of information sets on each iteration. Different variants of MC-CFR can be constructed
by choosing different sampling policies. One such variant is External Sampling (ES), which executes
all actions for player i, the traverser, in every I ∈ Ii but draws only one sample for actions not
controlled by i (i.e. those of -i and chance). In games with many player-actions Average Strategy
Sampling (Burch et al., 2012), Robust Sampling (Hui et al., 2018) are very useful. They, in different
ways, sample only a sub-set of actions for i. Both LCFR and a similarly fast variant called CFR+
(Tammelin, 2014) are compatible with forms of MC sampling, although CFR+ was regarded as to
sensitive to variance until recently (Schmid et al., 2018).
4	DEEP CFR
CFR methods either need to run on the full game tree or employ domain-specific abstractions. The
former is infeasible in large games and the latter not easily possible in all domains. Deep CFR
(Brown et al., 2018a) computes an approximation of linear CFR (Brown & Sandholm, 2018b) with
alternating player updates. It is sample-based and does not need to store regret tables, making it
generally applicable to any two-player zero-sum game.
On each iteration, Deep CFR fits a value network Di for one player i to approximate what we call ad-
Vamage, which is defined as Di (I, a) = PT：$?(I)), where RTlinear (I, a) = PT=1(trt(I, a)).
In large games, reach-probabilities naturally are (on average) very small after many tree-branchings.
Considering that it is hard for neural networks to learn values across many orders of magnitude (van
Hasselt et al., 2016), Deep CFR divides RiT,linear(I,a) by the total linear reach PtT=1(tπ-σti(I)) and
thereby avoids this problem. This still yields correct results because PtT=1(tπ-σti(I)) is identical for
all a ∈ A(I ).
We can derive the iteration-strategy for t + 1 from Dt similarly to CFR in equation 2 by
小	Dt(Iia)+rr	ifP-u4GDt(I,&)+ > 0
σt+1(I,a) = < Wa∈A(i) Dt(I，a)+	乙a∈A(I)八 , ' +
''	[∣A⅛	otherwise
(4)
1In CFR-BR (Johanson et al., 2012) σt does converge probabilistically as t → inf and in CFR+ (Tammelin,
2014) it often does so empirically (but without guarantees); in vanilla CFR and linear CFR (Brown & Sandholm,
2018b) σt typically does not converge.
3
Under review as a conference paper at ICLR 2020
However, Deep CFR modifies this to heuristically choose the action with the highest advantage
whenever Pa∈ a(i)Dt (I, a)+ ≤ 0. Deep CFR obtains the training data for D via batched external
sampling (Lanctot et al., 2009; Brown et al., 2018a). All instantaneous regret values collected over
the N traversals are stored in a memory buffer Biv . After its maximum capacity is reached, Biv is
updated via reservoir sampling (Vitter, 1985). To mimic the behaviour of linear CFR, we need to
weight the training losses between the predictions D makes and the sampled regret vectors in Bv
with the iteration-number on which a given datapoint was added to the buffer.
At the end of its training procedure (i.e. after the last iteration), Deep CFR fits another neural network
Si(I, a) to approximate the linear average strategy
O" =
(5)
Data to train Si is collected in a separate reservoir buffer Bs during the same traversals that data for
Biv is being collected on. Recall that external sampling always samples all actions for the traverser,
let us call him i, and plays according to σ-t i for the opponent. Thus, when i is the traverser, -i is
the one who adds his strategy vector σ-t i(I) to B-s i in every I ∈ I-i visited during this traversal.
This elegantly assures that the likelihood of σ-t i(I) being added to B-s i on any given traversal is
proportional to π-σti(I). Like before, we also need to weight the training loss for each datapoint by
the iteration-number on which the datapoint was created.
Notice that tabular CFR achieves importance weighting between iterations through multiplying with
some form of the reach probability (see equations 1 and 3). In contrast, Deep CFR does so by
controlling the expected frequency of datapoints from different iterations occurring in its buffers and
by weighting the neural network losses differently for data from each iteration.
5	Single Deep Counterfactual Regret Minimization (SD-CFR)
Notice that storing all iteration-strategies would allow one to compute the average strategy on the
fly during play both in tabular and approximate CFR variants. In tabular methods, the gain of not
needing to keep σ in memory during training would come at the cost of storing t equally large tables
(though potentially on disk) during training and during play. However, this is very different with
Deep CFR. Not aggregating into S removes the sampling- and approximation error that Bs and S
introduce, respectively. Moreover, the computational work needed to train S is no longer required.
Like in the tabular case, we do need to keep all iteration strategies, but this is much cheaper with
Deep CFR as strategies are compressed within small neural networks.
We will now look at two methods for querying σ from a buffer of past value networks BM.
5.1	Acting on freely playable trajectories
Often (e.g. in one-one-one evaluations and during rollouts), a trajectory is played from the root of the
game-tree and the agent is only required to return action-samples of the average strategy on each step
forward. In this case, SD-CFR chooses a value network Dt ∈ BM at the start of the game, where
each Dt is assigned sampling weight t. The policy σ%, which this network gives by equation 4, is
now going to be used for the whole game trajectory. We call this method trajectory-sampling.
By applying the sampling weights when selecting a Di ∈ BM, we satisfy the linear averaging
constraint of equation 5, and by using the same σi for the whole trajectory starting at the root, we
ensure that the iteration-strategies are also weighted proportionally to each of their reach-probabilities
in any given state along that trajectory. The latter happens naturally, since Dit of any t produces σit ,
which reaches each information set I with a likelihood directly proportional to πiσt(I) when playing
from the root.
The query cost of this method is constant with the number of iterations (and equal to the cost of
querying Deep CFR).
4
Under review as a conference paper at ICLR 2020
5.2	Querying a complete action distribution in any information set
Let Us now consider querying the complete action probability distribution σf (I) in some information
set I ∈ Ii. Given BM, we can compute σf (I) exactly through equation 5, where we compute
πiσt(I) =	Y	σit(I0,a0)	(6)
I0∈I,P (I0)=i,a0∕→I
Here, I0 ∈ I means that I0 is on the trajectory leading to I and a0 : I0 → I is the action selected in I0
leading to I .
This computation can be done with at most2 * as many feedforward passes through each network in
BiM as player i had decisions along the trajectory to I, typically taking a few seconds in poker when
done on a CPU.
5.3	Querying a complete action distribution on a trajectory
If a trajectory is played forward from the root, as is the case in e.g. exploitability evaluation,
we can cache the step-wise reach-probabilities on each step Ik along the trajectory and compute
πiσt (Ik+1) = σit (Ik+1, a0)πiσt (Ik), where a0 is the action that leads from Ik to Ik+1. This reduces
the number of queries per step to at most |BiM |.
5.4	Theoretical and practical properties
SD-CFR always mimics σf correctly from the iteration-strategies it is given. Thus, if these iteration-
strategies were perfect approximations of the real iteration-strategies, SD-CFR is equivalent to linear
CFR (see Theorem 2), which is not necessarily true for Deep CFR (see Theorem 1).
As we later show in an experiment, SD-CFR’s performance degrades if reservoir sampling is per-
formed on BM after the number of iterations trained exceeds the buffer’s capacity. Thankfully, the
neural network proposed to be used for Deep CFR in large poker games has under 100,000 parameters
(Brown et al., 2018a) and thus requires under 400KB of disk space. Deep CFR is usually trained for
just a few hundred iterations (Brown et al., 2018a), but storing even 25,000 such networks on disk
would need only 10GB of disk space. At no point during any computation do we need all networks in
memory. Thus, keeping all value networks will not represent a problem in practise.
Observing that Deep CFR and SD-CFR depend upon the accuracy of the value networks in exactly
the same way, we can conclude that SD-CFR is a better or equally good approximation of linear CFR
as long as all value networks are stored. Though this shows that SD-CFR is largely superior in theory,
it is not implicit that SD-CFR will always produce stronger strategies empirically. We will investigate
this next.
Theorem 1.	If the capacity of strategy buffer Bis is finite or if only a finite number K of traversals
is executed per iteration, Bs is not guaranteed to reflect the true average Strategy στ (I) for every
I ∈ Ii even if all value networks are perfect approximators of the true advantage after any number
of training iterations T > 2. Hence, even a perfect function approximator for S is not guaranteed to
model στ without error.
Theorem 2.	Assume that for all i ∈ N, all I ∈ Ii, all a ∈ A(I), and all t up to the number of
iterations trained T, Dt(I, a) = Dtt(I, a) (i.e. that all value networks perfectly model the true
advantages). Now, SD-CFR represents στ without error. This holds for both trajectory-sampling
SD-CFR andfor when SD-CFR computes στ (I) explicitly. Furthermore, an opponent has no way of
distinguishing which ofthe two proposed methods ofsampling from σ is used solelyfrom gameplay.
Proofs for both Theorem 1 and 2 can be found in the supplementary material.
2This number can further be reduced by omitting queries for any σt as soon as it assigns probability 0 to the
action played on the trajectory.
5
Under review as a conference paper at ICLR 2020
6 Experiments
We empirically evaluate SD-CFR by comparing to Deep CFR and by analyzing the effect of sampling
on BM. Recall that Deep CFR and SD-CFR are equivalent in how they train their value networks.
This allows both algorithms to share the same value networks in our experiments, which makes
comparisons far less susceptible to variance over algorithm runs and conveniently guarantees that
both algorithms tend to the same Nash equilibrium.
Where not otherwise noted, we use hyperparamters as Brown et al. (2018a). Our environment
observations include additional features such as the size of the pot and represent cards as concatenated
one-hot vectors without any higher level features, but are otherwise as Brown et al. (2018a).
6.1	Exploitability in Leduc Poker
32
00
11
g/Am ni ytilibatiolpx
101	102	103
Algorithm Iterations
(a) Comparing SD-CFR and Deep CFR
20
101	102	103
Algorithm Iterations
(b) Sampling on BM with finite capacity
Figure 1: Empirical analysis of SD-CFR in Leduc Hold’em Poker. Results in Figure 1a and 1b are
averaged over five and three runs, respectively.
Figure 1a shows the exploitability (i.e. loss against a worst-case opponent) of SD-CFR and Deep
CFR in Leduc Poker (Southey et al., 2005) measured in milli-antes per game (mA/g).
In Leduc Poker, players start with an infinite number of chips. The deck has six cards of two suits
{a, b} and three ranks {J, Q, K}. There are two betting rounds: preflop and flop. After the preflop, a
card is publicly revealed. At the start of the game, each player adds 1 chip, called the ante, to the
pot and is dealt one private card. There are at most two bets/raises per round, where the bet-size is
fixed at 2 chips in the preflop, and 4 chips after the flop is revealed. If no player folded, the winner
is determined via hand strength. If a player made a pair with the public card, they win. Otherwise
K > Q > J. If both players hold a card of the same rank, the pot is split.
Hyperparameters are chosen to favour Deep CFR as the neural networks and buffers are very large in
relation to the size of the game. Yet, we find that SD-CFR minimizes exploitability better than Deep
CFR. Exact hyperparameters can be found in the supplementary material.
Although we concluded that storing all value networks is feasible, we analyze the effect of reservoir
sampling on BM in Figure 1b and find it leads to plateauing and oscillation, at least up to |BM | =
1000.
6.2	One-on-One matches in 5-Flop Hold’ em Poker (5-FHP) against Deep CFR
Figure 2 shows the results of one-one-one matches between SD-CFR and Deep CFR in 5-Flop
Hold’em Poker (5-FHP). 5-FHP is a large poker game similar to regular FHP (Brown et al., 2018a),
which was used to evaluate Deep CFR (Brown et al., 2018a). The only difference is that 5-FHP
uses five instead of three flop cards, forcing agents to abstract and generalize more. For details on
6
Under review as a conference paper at ICLR 2020
Depth	Round	Dif Mean	DIF STD	N
0	PF	0.012± 0.0001	0.017	200K
1	PF	0.013± 0.0001	0.018	100K
2	FL	0.052± 0.0003	0.048	80K
3	FL	0.083± 0.0005	0.075	83K
4	FL	0.113± 0.0011	0.109	37K
5	FL	0.175± 0.0057	0.206	5k
Table 1: Disagreement between SD-CFR’s and Deep CFR’s average strategies. ”DEPTH”:
number of player actions up until the measurement, ”ROUND”: PF=Preflop, FL=Flop, ”DIF MEAN”:
mean and 95% confidence interval of the absolute differences betWeen the strategies over the ”N”
occurrences. ”DIF STD”: approximate standard deviation of agreement across information sets.
FHP, please refer to (BroWn et al., 2018a). The neural architecture is as BroWn et al. (2018a). Both
algorithms again share the same value networks during each training run. Like BroWn et al. (2018a),
Bv and Bs have a capacity of 40 million per player. On each iteration, We run a batch of 300,000
external sampling traversals and train a value netWork from scratch using a batch size of 10,240
for 4,000 updates. Average strategy netWorks are trained With a batch size of 20,480 for 20,000
updates. SD-CFR’s BM stores all value netWorks, requiring 120MB of disk space, While each Bs
needs around 25GB of memory during training.
The y-axis plots SD-CFR’s average winnings
against Deep CFR in milli-big blinds per game
(mbb/g) measured every 30 iterations. For ref-
erence, 10 mbb/g is considered a good margin
between humans in Heads-Up Limit Hold’em
(HULH), a game with longer action sequences,
but similar minimum and maximum winnings
per game as 5-FHP. Measuring the performance
on iteration t compares how well the SD-CFR
averaging procedure would do against the one
of Deep CFR if the algorithm stopped training
after t iterations
0000
321
g/bbm ni sgninniW s’RFC-D
-∙- Run 1
-	∙- Run 2
-	∙- Run 3
-	∙- Run 4
-	∙- Run 5
---Mean
30 60 90 120 150 180 210 240 270 300
Algorithm Iterations
Bs reached its maximum capacity of 40 million
for both players by iteration 120 in all runs. Be-
fore this point, SD-CFR defeats Deep CFR by
a sizable margin, but even after that, SD-CFR
clearly defeats Deep CFR.
-10
6.2.1 Comparing strategies
We analyze hoW far the average strategies of SD-
CFR and Deep CFR are apart at different depths
of the tree of 5-FHP. In particular, We measure
Figure 2: One-on-One performance of Single
Deep CFR vs. Deep CFR. Dashed lines are inde-
pendent algorithm runs. All evaluations have 95%
confidence intervals betWeen ±5.4 and ±6.51 and
are the average result of 3M poker hands each.

1 X (EIy X ∣σT,SD(I,a) - σT,S(I,a)∣)
i∈1,2	a∈A(I)
We ran 200,000 trajectory rollouts for each player, where player i plays according to SD-CFR’s
T SD
average strategy σT and -i plays uniformly random. Hence, We only evaluate on trajectories on
which the agent should feel comfortable. The two agents again share the same value networks and
thus approximate the same equilibrium. We trained for 180 iterations, a little more than it takes for
Bs and Bv to be full for both players. Table 1 shoWs that Deep CFR’s approximation is good on early
levels of the tree but has a larger error in information sets reached only after multiple decision points.
7
Under review as a conference paper at ICLR 2020
7	Related Work
Regression CFR (R-CFR) (Waugh et al., 2015) applies regression trees to estimate regret values in
CFR and CFR+ . Unfortunately, despite promising expectations, recent work failed to apply R-CFR
in combination with sampling (Srinivasan et al., 2018). Advantage Regret Minimization (ARM) (Jin
et al., 2017) is similar to R-CFR but was only applied to single-player environments. Nevertheless,
ARM did show that regret-based methods can be of interest in imperfect information games much
bigger, less structured, and more chaotic than poker.
DeePStack (MoravcIk et al., 2017) was the first algorithm to defeat professional poker players in
one-on-one gameplay of Heads-Up No-Limit Hold’em Poker (HUNL) requiring just a single GPU and
CPU for real-time play. It accomplished this through combining real-time solving with counterfactual
value approximation with deep networks. Unfortunately, DeepStack relies on tabular CFR methods
without card abstraction to generate data for its counterfactual value networks, which could make
applications to domains with many more private information states than HUNL has difficult.
Neural Fictitious Self-Play (NFSP) (Heinrich & Silver, 2016) was the first algorithm to soundly apply
deep reinforcement learning from single trajectory samples to large extensive-form games. While
not showing record-breaking results in terms of exploitability, NFSP was able to learn a competitive
strategy in Limit Texas Hold’em Poker over just 14 GPU/days. Recent literature elaborates on
the convergence properties of multi-agent deep reinforcement learning (Lanctot et al., 2017) and
introduces novel actor-critic algorithms (Srinivasan et al., 2018) that have similar convergence
properties as NFSP and SD-CFR.
8	Future Work
So far, Deep CFR was only evaluated in games with three player actions. Since external sampling
would likely be intractable in games with tens or more actions, one could employ outcome sampling
(Lanctot et al., 2009), robust sampling (Hui et al., 2018), Targeted CFR (Jackson, 2017), or
average-strategy-sampling (Burch et al., 2012) in such settings.
To avoid action translation after training in an action-abstracted game, continuous approximations
of large discrete action-spaces where actions are closely related (e.g. bet-size selection in No-Limit
Poker games, auctions, settlements, etc.) could be of interest. This might be achieved by having the
value networks predict parameters to a continuous function whose integral can be evaluated efficiently.
The iteration-strategy could be derived by normalizing the advantage clipped below 0. The probability
of action a could be calculated as the integral of the strategy on the interval corresponding to a in the
discrete space.
Given a few modifications to its neural architecture and sampling procedure, SD-CFR could poten-
tially be applied to much less structured domains than poker such as those that deep reinforcement
learning methods like PPO (Schulman et al., 2017) are usually applied to. A first step on this line
of research could be to evaluate whether SD-CFR is preferred over approaches such as (Srinivasan
et al., 2018) in these settings.
9	Conclusions
We introduced Single DeeP CFR (SD-CFR), a new variant of CFR that uses function approximation
and partial tree traversals to generalize over the game’s state space. In contrast to previous work,
SD-CFR extracts the average strategy directly from a buffer of value networks from past iterations.
We show that SD-CFR is more attractive in theory and performs much better in practise than Deep
CFR.
Acknowledgments
References
Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin. Heads-up limit hold’em
poker is solved. Science, 347(6218):145-149, 2015.
8
Under review as a conference paper at ICLR 2020
Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top
professionals. Science, 359(6374):418-424, 2018a.
Noam Brown and Tuomas Sandholm. Solving imperfect-information games via discounted regret
minimization. arXiv preprint arXiv:1809.04040, 2018b.
Noam Brown, Sam Ganzfried, and Tuomas Sandholm. Hierarchical abstraction, distributed equilib-
rium computation, and post-processing, with application to a champion no-limit texas hold’em
agent. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent
Systems, pp. 7-15. International Foundation for Autonomous Agents and Multiagent Systems,
2015.
Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret mini-
mization. arXiv preprint arXiv:1811.00164, 2018a.
Noam Brown, Tuomas Sandholm, and Brandon Amos. Depth-limited solving for imperfect-
information games. arXiv preprint arXiv:1805.08195, 2018b.
Neil Burch, Marc Lanctot, Duane Szafron, and Richard G Gibson. Efficient monte carlo counterfactual
regret minimization in games with many player actions. In Advances in Neural Information
Processing Systems, pp. 1880-1888, 2012.
Sam Ganzfried and Tuomas Sandholm. Potential-aware imperfect-recall abstraction with earth
mover’s distance in imperfect-information games. In AAAI, pp. 682-690, 2014.
Johannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfect-
information games. arXiv preprint arXiv:1603.01121, 2016.
Li Hui, Hu Kailiang, Ge Zhibang, Jiang Tao, Qi Yuan, and Song Le. Double neural counterfactual
regret minimization. https://openreview.net/pdf?id=Bkeuz20cYm, 2018, 2018.
Eric Griffin Jackson. Targeted cfr. In Workshops at the Thirty-First AAAI Conference on Artificial
Intelligence, 2017.
Peter H Jin, Sergey Levine, and Kurt Keutzer. Regret minimization for partially observable deep
reinforcement learning. arXiv preprint arXiv:1710.11424, 2017.
Michael Johanson, Nolan Bard, Neil Burch, and Michael Bowling. Finding optimal abstract strategies
in extensive-form games. In Twenty-Sixth AAAI Conference on Artificial Intelligence, 2012.
Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael Bowling. Monte carlo sampling for
regret minimization in extensive games. In Advances in neural information processing systems, pp.
1078-1086, 2009.
Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat,
David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement
learning. In Advances in Neural Information Processing Systems, pp. 4190-4203, 2017.
Viliam Lisy and Michael Bowling. Equilibrium approximation quality of current no-limit poker bots.
arXiv preprint arXiv:1612.07547, 2016.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529, 2015.
Matej Moravclk, Martin Schmid, Neil Burch, Viliam Lisy, Dustin Morrill, Nolan Bard, Trevor
Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial
intelligence in heads-up no-limit poker. Science, 356(6337):508-513, 2017.
Martin Schmid, Neil Burch, Marc Lanctot, Matej Moravcik, Rudolf Kadlec, and Michael Bowling.
Variance reduction in monte carlo counterfactual regret minimization (vr-mccfr) for extensive form
games using baselines. arXiv preprint arXiv:1809.03057, 2018.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
9
Under review as a conference paper at ICLR 2020
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. Nature, 550(7676):354, 2017.
Finnegan Southey, Michael P Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse Billings,
and Chris Rayner. Bayes’ bluff: Opponent modelling in poker. Proceedings of the 21st Conference
in Uncertainty in Artificial Intelligence, pp. 550—-558, 2005.
Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien Perolat, Karl Tuyls, Remi Munos, and
Michael Bowling. Actor-critic policy optimization in partially observable multiagent environments.
In Advances in Neural Information Processing Systems, pp. 3426-3439, 2018.
Oskari Tammelin. Solving large imperfect information games using cfr+. arXiv preprint
arXiv:1407.5042, 2014.
Oskari Tammelin, Neil Burch, Michael Johanson, and Michael Bowling. Solving heads-up limit
texas hold’em. In International Joint Conference on Artificial Intelligence, pp. 645-652, 2015.
Hado P van Hasselt, Arthur Guez, Matteo Hessel, Volodymyr Mnih, and David Silver. Learning
values across many orders of magnitude. In Advances in Neural Information Processing Systems,
pp. 4287-4295, 2016.
Jeffrey S Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical Software
(TOMS), 11(1):37-57, 1985.
Kevin Waugh, Dustin Morrill, James Andrew Bagnell, and Michael Bowling. Solving games
with functional regret estimation. In Association for the Advancement of Artificial Intelligence,
volume 15, pp. 2138-2144, 2015.
Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization
in games with incomplete information. In Advances in neural information processing systems, pp.
1729-1736, 2008.
A Hyperparameters of experiments performed in Leduc Hold’ em
Poker
Bv and Bs have a capacity of 1 million for each player. On each iteration, data is collected over 1,500
external sampling traversals and a new value network is trained to convergence (750 updates of batch
size 2048), initialized randomly at t < 2 and with the weights of the value net from iteration t - 2
afterwards. Average-strategy networks are trained to convergence (5000 updates of batch size 2048)
always from a random initialization. All networks used for this evaluation have 3 fully-connected
layers of 64 units each, which adds up to more parameters than Leduc Hold’em has states. All other
hyperparameters were chosen as in (Brown et al., 2018a).
B Rules of Leduc Hold’ em Poker
Leduc Hold’em Poker is a two-player game, were players alternate seats after each round. At the start
of the game, both players add 1 chip, the ante, to the pot and are dealt a private card (unknown to the
opponent) from a deck consisting of 6 cards: {A, A, B, B, C, C}. There are two rounds: pre-flop
and flop. The game starts at the pre-flop and transitions to the flop after both players have acted and
wagered the same number of chips. At each decision point, players can choose an action from a
subset of {fold,call, raise}. When a player folds, the game ends and all chips in the pot are awarded
to the opponent. Calling means matching the opponent’s raise. The first player to act in a round has
the option of checking, which is essentially a call of zero chips. Their opponent can then bet or also
check. When a player raises, he adds more chips to the pot than his opponent wagered so far. In
Leduc Hold’em, the number of raises per round is capped at 2. Each raise adds 2 additional chips in
the pre-flop round and 4 in the flop round. On the transition from pre-flop to flop, one card from the
remaining deck is revealed publicly. If no player folded and the game ends with a player calling, they
show their hands and determine the winner by the rule that if a player’s private card matches the flop
card, they win. Otherwise the player with the higher card according to A B C wins.
10
Under review as a conference paper at ICLR 2020
C Proof of Theorem 1
Proof. Let I be any information set in Ii. Assuming that 0 < πiσt (I) < 1. Recall that external
sampling samples only one action for player i and chance at any decision point, when -i is the
traverser. Since (1 - πiσt (I))K > 0 for any finite number of traversals K per iteration, we cannot
guarantee that I will be visited. If I is not visited despite ∏ (I) > 0, the contribution of σ1t to σT(I)
is not represented in Bis .
For the second argument, we assume that K = ∞. Let I again be any information set in Iiin
tj
which |A(I)| > 1. Assume that πiσ (I) is irrational and that πiσ (I) is rational. Clearly, because its
tj
capacity is finite, Bis could not reflect the ratio between πiσ (I) and πiσ (I) correctly through the
frequency of the appearance of samples from iterations t and j , regardless of the number of traversals.
Furthermore, in games where the number of members in the set
{I∈Ii: |A(I)| > 1,πiσt(I) >0}
is bigger than the capacity of Bis, not every I ∈ I can fit into Bis on iteration t, also making Bis an
incomplete representation of στ (I).	□
D Proof of Theorem 2
Proof. Let BiM be a buffer of all value networks up to iteration T belonging to player i.
Since DD t(I, a) = Dtt(I, a) for all I ∈I and all a ∈ A(I) by assumption,
Dt(I,a)+
Pa∈A(i) Dt(I,a) +
1
Wn
can be restated in terms of Dit(I, a).
if EaDXI, a)+ > 0
otherwise
(7)
By definition,
πiσt(I) =	Y	σit(I0,a0)
I0∈I,P (I0)=i,a0∕→I
(8)
Since all σtt have no error by assumption, SD-CFR's recomputation of πσ (I) and hence also σT (I, a)
are exact for any I ∈ Ii and all a ∈ A(I).
To show this for trajectory-sampling SD-CFR, consider a trajectory starting at the tree’s root φ leading
to an information set in I ∈ Ii. Since σ1t can be deduced from Dt as before, BM can be seen as a
buffer of iteration-strategies. Let f : I → a be a function that first chooses a σit ∈ BiM , where each
σit is assigned a sampling weight of tπiσ (I). f then returns an action sampled from the distribution
σit(I). Since f weights strategies like the numerator of the definition
-a) = ⅛Sr
(9)
executing f (I) is equivalent to sampling directly from σT.
Note that πiσ(φ) = 1 for all σ. Thus, f(φ) would choose a given σit ∈ BiM with sampling weight t.
This is what trajectory-sampling SD-CFR does at φ. For each information set I0 from φ until the end
of the trajectory, SD-CFR plays using the same iteration-strategy selected at φ. Thus, SD-CFR will
reach each information set I with a probability proportional to πiσt (I) conditional on knowing which
iteration-strategy was selected. Combining these facts, we see that the assigned weight of σit in any I
is tπiσt (I) for any t up to T. It follows that the probability of σit being the acting policy in any I is
t∏σt (I)
pT=ι(t0∏σt0 (I))
Since this is equivalent to the weighting scheme between iteration-strategies in the definition of σT,
trajectory-sampling SD-CFR samples correctly from σT.
11
Under review as a conference paper at ICLR 2020
Moreover, because the opponent does not know which σit is the acting policy, this result also shows
that an opponent cannot tell whether the agent is using this sampling method or following an explicitly
computed σf	□
E Deep CFR performs well on early iterations in some games
We conducted experiments searching to investigate the harm caused by the function approximation
of S. We found that in variants of Leduc Hold’em (Southey et al., 2005) with more that 3 ranks
and multiple bets, the performance between Deep CFR and SD-CFR was closer. Below we plot the
exploitability curves of the early iterations in a variant of Leduc that uses a deck of 12 ranks and
allows a maximum of 6 instead of 2 bets per round.
We believe the smaller difference in performance is due to the equilibrium in this game being less
sensitive to small differences in action probabilities, while the game is still small enough to see every
state often during training. In vanilla Leduc, slight deviations from optimal play give away a lot
about one’s private information as there are just three distinguishable cards. In contrast, this variant
of Leduc, despite having more states, might be less susceptible to approximation error as it has 12
distinguishable cards but similarly simple rules.
For the plot below, we ran Deep CFR and SD-CFR with shared value networks, where all buffers have
a capacity of 4 million. On each iteration, data is collected over 8,800 external sampling traversals and
a new value network is trained to convergence (1200 updates of batch size 2816), initialized randomly
at t < 2 and with the weights of the value net from iteration t - 2 afterwards. Average-strategy
networks are trained to convergence (10000 updates of batch size 5632) from a random initialization.
The network architecture used is as Brown et al. (2018a), differing only by the card-branch having
64 units per layer instead of 192.
g/Am ni ytilibatiolpx
Figure 3: Exploitability of Single Deep CFR and Deep CFR averaged over five runs.
12