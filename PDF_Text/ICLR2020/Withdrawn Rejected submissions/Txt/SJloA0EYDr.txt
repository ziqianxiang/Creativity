Under review as a conference paper at ICLR 2020
A?MCTS: SEARCH WITH THEORETICAL GUARANTEE
using Policy and Value Functions
Anonymous authors
Paper under double-blind review
Ab stract
Combined with policy and value neural networks, Monte Carlos Tree Search
(MCTS) is a critical component of the recent success of AI agents in learning to
play board games like Chess and Go (Silver et al., 2017). However, the theoretical
foundations of MCTS with policy and value networks remains open. Inspired by
MCTS, we propose A?MCTS, a novel search algorithm that uses both the policy
and value predictors to guide search and enjoys theoretical guarantees. Specifically,
assuming that value and policy networks give reasonably accurate signals of the
values of each state and action, the sample complexity (number of calls to the value
network) to estimate the value of the current state, as well as the optimal one-step
action to take from the current state, can be bounded. We apply our theoretical
framework to different models for the noise distribution of the policy and value
network as well as the distribution of rewards, and show that for these general
models, the sample complexity is polynomial in D, where D is the depth of the
search tree. Empirically, our method outperforms MCTS in these models.
1	Introduction
Monte Carlo Tree Search (MCTS) is an online heuristic search algorithm commonly used to find op-
timal policies for problems in reinforcement learning. It is a vital component of recent breakthroughs
for AI in game play, including AlphaGo and AlphaZero (Silver et al., 2016; 2017).
An interesting difference between the MCTS used in AlphaGo/AlphaZero and traditional MCTS
is the adoption of neural networks to predict the value of the current state (value network), and to
prioritize the next action to search (policy network). The value network replaces random rollouts used
to evaluate a non-terminal state, which saves both online computation time and substantially reduces
the variance of the estimate. The policy network (coupled with PUCT (Rosin, 2011)) prioritizes
promising actions for more fruitful exploration. Both networks are trained on pre-existing datasets
(either from human replays or self-play) to incorporate prior knowledge. This strategy has been
applied in other domains including Neural Architecture Search (Wang et al., 2018; Wistuba, 2017;
Negrinho & Gordon, 2017; Wang et al., 2019b).
Despite the empirical success and recent popularity, theoretical foundations are lacking. (Kocsis &
Szepesvari, 2006; Coquelin & Munos, 2007) analyze MCTS with rollouts using a multi-armed bandit
framework and gives asymptotic results without finite sample complexity bounds. Powering MCTS
with value/policy neural networks instead of rollouts is even more poorly understood.
To better understand how to efficiently learn optimal policies in decision trees using value and policy
networks, we propose a novel algorithm, A?MCTS, which is a combination of A? (Delling et al.,
2009; Kanoulas et al., 2006) and MCTS. Like A?, it uses a priority queue to store all leaves currently
being explored and picks the most optimistic one to expand, according to an upper confidence bound
heuristic function. Like MCTS, it uses policy and value networks to prioritize the next state to be
explored.
To facilitate the analysis, the policy and value networks are treated as black-box functions and a sta-
tistical model is built the accuracy of the predictions. In particular, we assume the standard deviation
of the noisy estimates of intermediate state values decays (either polynomially or exponentially) as a
function of the depth of the tree, so that the values of near-terminal states are estimated to a greater
degree of accuracy. With this statistical model, we provide theoretical guarantees for the sample
1
Under review as a conference paper at ICLR 2020
complexity (expected number of state expansions / rollouts) to determine optimal actions. We apply
our theoretical framework to simple models for rewards and show that our algorithm enjoys a sample
complexity that is polynomial in depth D.
Our experiments validate the theoretical analysis and demonstrate the effectiveness of A?MCTS over
benchmark MCTS algorithms with value and policy networks. To our knowledge, our work is the first
that studies tree search for optimal actions in the presence of pre-trained value and policy networks,
and we hope that it inspires further progress on this important problem.
2	Background and Related Work
Monte Carlo Tree Search (Browne et al., 2012) is an algorithm that tries to find optimal decisions in
Markov Decision Processes by taking random samples in the decision space and building a tree based
on the results. In each iteration, the agent traverses the tree by acting according to the tree policy
(which usually involves some exploration) for the set of known states and then determines the value
of an unexpanded state by performing random rollouts.
MCTS is commonly used to search for a one-step optimal actions from the root. The agent then
takes this action, observes the result, transitions to a new state, and repeats MCTS starting from this
new state, with the benefit of the additional observation. The main theoretical question of interest
in MCTS is how quickly and efficiently one can find this optimal one-step action, in terms of the
number of random rollouts, or the number of nodes in the tree that should be expanded. On the
theoretical side, there is a small body of literature that addresses the theoretical foundations of MCTS
with random rollouts (Kocsis & Szepesvari (2006), Veness et al. (2011)). The algorithm, called
UCT (UCB applied to trees) models the search as a complex multi-armed bandit problem where the
rewards are allowed to drift over time. The analysis shows that after a period of time T , only the
optimal branch will be followed. Veness et al. (2011) gives various ways of improving UCT with
variance reduction techniques.
Later followup work in Coquelin & Munos (2007) highlights some hard instances of UCT and offers
some alternatives, including a modified UCT (with a different exploration bonus), Flat UCB, and
BAST (smooth tree models).
On the experimental side, MCTS was popularized by the story of AlphaGo (Silver et al. (2016)),
which used MCTS using a weighted combination of the value network’s output and random rollouts
to defeat the leading champion in Go. Follow-up work in AlphaZero (Silver et al. (2017)) used MCTS
with only policy and value neural networks, without random rollouts and reports faster training
with better performance. However, there is no theoretical foundation for the search algorithm that
AlphaZero uses.
A? (Nosrati et al., 2012) uses a priority queue to keep track of lowest cost actions. A? combined with
hand-designed heuristics, has been used in many path finding and routing problems (AlShawi et al.,
2012; Rana & Zaveri, 2011; Ghaffari, 2014). Recent work (Wang et al., 2019a; Chen & Wei, 2011;
Sigurdson & Bulitko, 2017) trains neural-based heuristic functions for A? for specific problems.
However, to the best of our knowledge, we are not aware of any prior work that performs theoretical
analysis of such heuristic based A? algorithms.
3	Preliminaries and Summary of Contributions
In this work we consider a deterministic, finite D-horizon Markov decision process (MDP) with no
intermediate rewards. With the set of states denoted by S, there is one deterministic root initial state
s0 and the process terminates at horizon D with a termination reward that depends on the end state.
We assume that there are exactly K possible actions at every intermediate state s and for simplicity
that the process does not backtrack, i.e., each action leads to a new and unique state. This is a Markov
decision tree.
The set of leaf states of the Markov decision tree are denoted as L. We shall denote an arbitrary
state at depth d as sd and also a leaf state as l ∈ L. There is a value function V : S → R that takes
as input a state s and returns the value of s, defined in the following way. If s ∈ L, then Vs is the
termination reward of that particular leaf. For an intermediate node s, Vs = maxl∈L(s) Vl, where
2
Under review as a conference paper at ICLR 2020
Summary of Notations Used	
K	Branching factor
Kd	True value of state Sd at depth d
∆sd	Gap to optimal value (V* - Vsd)
Usd	Predicted value of state Sd by value network (deterministic)
F	0-mean, standard deviation σd random variable for the noise of the value prediction Usd at depth d. Vsd - Usd 〜Xd.
σd	standard deviation of Xd, fixed and known as part of the problem model. We assume that ◎& decays with depth.
Cd		exploration bonus, determined based on σd. |Xd | ≤ cd with high probability
Usd + cd	priority value (value used to determine the ordering of which to nodes expand in the priority queue).
Ur	Predicted value of child i at some depth d by policy network (deterministic)
Xn	0-mean, standard deviation ◎& random variable for the noise of the value PrediCtiOn of child i at depth d, Ur. Vri - Ur 〜Xd.
Table 1: Summary of main notations used in the paper.
L(s) is the set of leaf nodes that share s as an ancestor, i.e., the set of leaf nodes in the subtree rooted
at s. We assume that the highest value V * ≡ maxι∈L Vi = maxs∈s Vs is achieved at a unique leaf
node l* = arg maxι∈L ^½.
In this work, we focus on the problem of efficiently computing V * exactly or approximately with a
controlled accuracy without direct access to the value function V : S → R, as a stepping stone to
finding the optimal one-step action from s0 . Instead, there is a pre-trained black-box resource, value
network U : S → R that takes as input a state and returns a noisy value estimate, i.e., for a state sd at
depth d, Usd ≡ Vsd + Xsd, where Xsd are i.i.d. copies of a random variable Xd that depends only
on the depth d. We assume that at depth D, which is the level with all the leaf nodes, Ui = Vi (i.e.,
the value network returns the ground truth values without any noise). The efficiency is defined in
terms of sample complexity, which is the number of queries made to the value network for the value
of a state. In the worst case, all leaves need to be queried and the sample complexity is KD .
Another type of available pre-trained black box resource is a policy network. Let C(s) = {r1, . . . rK}
denote the set of K children of state s, each corresponding to the transition state of one of the K
actions {a1, . . . aK}. The policy network outputs a probability estimate of the form pri
eUπi
PK=I^,
where each Urπi is a perturbed estimate of Vri . Here the superscript π indicates that this is the
approximation from the policy network. We assume that if the child ri is at depth d then the estimate
Urπ = Vri + Xrπ , where Xrπ are i.i.d. copies of a random variable Xdπ that depends only on
the depth d. We assume that the distribution of Xd and Xdπ are known a priori for each depth d.
Further, the variances of Xd and Xdπ decrease sufficiently rapidly as the depth d increases (i.e., as
one traverses deeper in the tree, the noise level is smaller). This assumption is motivated by basic
approximate dynamic programming principles, which is that near-terminal states require a smaller
sample complexity to estimate well, since the backwards induction will propagate this noise to earlier
states, making it harder to estimate the values of initial states well.
3.1	Main Contributions
We develop and analyze (giving specific sample complexity bounds) algorithms to estimate V*
exactly and approximately using value and policy networks. Since our value and policy networks are
trained functions that give fixed, deterministic noisy outputs for each state (no matter how many times
one calls the function on a particular state), where the noise variance is smaller at deeper levels of the
search tree, we use our value and policy network estimates as signals for which are the promising
states to expand. We rank the states based on our optimistic estimates Usd + cd in a priority queue,
and we expand (query the value network for the value estimate) the children of the top nodes of
the priority queue to get increasingly more accurate value estimates and determine the value of the
optimal leaf V * .
The main contributions are organized in the following sections:
3
Under review as a conference paper at ICLR 2020
•	Section 4 introduces and analyzes A? MCTS-V * and A? MCTS-π * V * that are our core
techniques for estimating V * exactly. A? MCTS-V * uses only a value network to determine
V* exactly and outputs the optimal one-step from s0 that leads to l* . We give an expected
sample complexity in Theorem 1. A? MCTS-π * V * builds on A? MCTS-V * by using a policy
network to further reduce the sample complexity, which we bound in Theorem 2.
•	Section 5 extends A? MCTS-V * and A? MCTS-π * V * to the case when We tolerate δ-additive
approximations of V* .
•	Section 6 introduces some sample distribution models for the value functions and the noise
and applies our results to those models. We show that for some reasonable models, our
algorithms can achieve sample complexity that is polynomial in depth D . We also provide
some experimental comparisons against benchmark MCTS implementations.
4	Finding V * Exactly using Value and Policy Networks
This section presents our main techniques, A? MCTS-V * and A? MCTS-π * V *, which find V * exactly
using a value network only in the former and a value network and a policy network in the latter.
A?MCTS-V*, Algorithm 1, uses only a value network to determine V* exactly and outputs the
optimal one-step action from s0 that takes the agent to an ancestor of l* . The idea behind the
algorithm is very simple and similar to A? search. We first expand (i.e., query the value network)
the children of the root node s0 , compute their value network estimate plus the exploration bonus,
and then insert these nodes to a priority queue Q. The exploration bonus, denoted by cd , is chosen
such that P(∣½d - Usd | ≤ Cd) ≥ 1 - DKd. In each subsequent iteration, the algorithm picks the
top element in the priority queue Q, which is the most optimistic state, and expands all the children
of that state, and adds those children to Q. We terminate at the first time when the most optimistic
element is a leaf node. Since the value network gives the values of leaf nodes exactly, this leaf node
must be l*.
Figure 1: illustration of our overall method. We prioritize nodes to expand using the value network
estimates. Algorithm 1 queries the value network for all children of the first node in the priority
queue. in our next algorithm, Algorithm 2, we show how to use a policy network to choose which
children nodes to add to the priority queue.
We present the complexity bound in Theorem 1. The complexity bound is based on one very simple
observation, which is that we end up choosing a sub-optimal node if the value network estimate of
that node plus the upper bound on the possible error (optimism) is higher than the true optimal value.
otherwise we will not choose to expand that node, since we are sure that it will not be the optimal
node. Therefore a sub-optimal internal node sd is chosen if V* ≤ Usd + cd = Vsd + Xsd + cd , where
Usd is the value network estimate for state sd, and cd is the upper bound on the possible error for the
value estimate. We should also account for the ancestors of sd , if we are lucky enough to be able to
rule out one of the ancestors of sd , we would never expand beyond that ancestor to reach sd , and sd
would not be in our priority queue. Theorem 1 states this complexity bound.
Theorem 1. With probability 1 - β, Algorithm 1, A?MCTS-V*, returns V*. The expected sample
complexity (number of calls to the value network) is
E[N] = KD +	X	K∙P I ∆sd - Xsd - Cd ≤ 0	\	∆sdo- Xsdo- Cd，≤ 0
Sd∈S,sd∈L,sd∈A(l*)	∖	sdo ∈A(sd)
4
Under review as a conference paper at ICLR 2020
Algorithm 1 A?MCTS-V *
Require: Value Network with error distribution for Xd at each depth d ≤ D
1:	For each d < D, compute Cd so that P(∣Xd) | ≤ Cd) ≥ 1 - DKd.
2:	Q J (S0, Uso + cd)
3:	while the depth of Q.f ront() is < D do
4:	(SL) = Q.pop()
5:	for r ∈ C(s) do
6:	d J depth of r
7:	Q.enqueue(r, Ur + Cd )
8:	end for
9:	end while
10:	(s, _) J Q.front()
11:	Return Us and the action at S0 that leads to S
where A(l*) is the set of ancestor states of the optimal leaf, l*, and ∆sd = V* - Vsd.
Before we give the proof, we briefly note that getting an exact expression for the expected sample
complexity depends on the problem model (the distribution for the gaps ∆sd and the errors Xsd). We
give two examples in Section 6 of different models and we apply this theorem to those models to
derive the sample complexity, which we show is polynomial in depth D. Please check Appendix for
the proof.
4.1 Combining Value Network with Policy Network
Suppose that in addition to the value network U, one has access to a policy network Uπ that
outputs, for each child of S, r1, . . . rK (each corresponding to the transition state of a different action
a1 , . . . aK ) a probability estimate of the form:
eu∏
Pri = PKI^ ,
where each Urπi = Vri + Xrπi is a perturbed estimate of Vri . If ri is at depth d, the noise Xrπi
is an i.i.d. copy of the random variable Xdπ that depends only on the depth d. Since all the
probabilities are known, we assume without loss of generality that states r1, . . . rK are ordered so
that Urπ1 ≥ Urπ2 ≥ Urπ3 . . . ≥ UrπK , or equivalently pr1 ≥ pr2 ≥ pr3 . . . ≥ prK .
The following algorithm A? MCTS-∏* V * uses both the value and the policy networks to determine
V* exactly. When we expand a given state Sd, instead of querying for the value estimate for every
child of Sd and adding each child to the priority queue, we add the top 2 children (ordered by the
probabilities given by the policy network) to the queue by default. For the k-th child node, where
k > 2, we add this child if there is a small gap between the policy network probability of the k - 1-th
child and the probability of the top child, where the threshold for the gap is given by the noise model.
Intuitively, we are saying that if this gap is sufficiently big, even after accounting for the noise in the
policy network, there is no way that this k-th child is part of the optimal policy, so we do not have to
add it to the queue.
The key insight behind this algorithm is that, if there is a large gap between the probabilities of the
children as given by the policy network, one can infer that the children states associated with the
smaller probabilities are not worth expanding because they have small values. This is made rigorous
in the statement and proof of Theorem 2.
Theorem 2.	With probability 1 — β, Algorithm 2, A? MCTS-π*V *, returns V *. The expected sample
complexity (number of calls to the value network) is
E[N] ≤ X(2 + Id P (% - %k + X∏d+I),ι - X(d+I),k ≤ 2c∏+ι)) ∙
Sd∈L ∖	k=2	)
P I δSd - Xsd - cd ≤ 0	\	∆Sd0 - XsdO- cd0 ≤ 0 ) ,
sd0 ∈A(sd)
5
Under review as a conference paper at ICLR 2020
Algorithm 2 A?MCTS-π* V *
Require: Value Network, Policy Network, failure tolerance β
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
For each d < D, compute Cd so that P(∣Xd∣ ≤ Cd) ≥ 1 - 2∩lζd.
For each d < D, compute Cn so that P(∣X∏ | ≤ Cn) ≥ 1 - 2DK.
Q J (S0, UsO )
while the depth of Q.f ront() is < D do
(SL) = Q.pop()
for i ∈ [1, . . . , K], ordered according to the probabilities of the policy network do
d J depth of ri
if i < 3 or ln (Jr ) ≤ 2c∏ then
Q.enqueue(ri, Uri + Cd)
else
break;
end if
end for
end while
(SL) J Qfronta
Return Us and the action at S0 that leads to S
where ∆sd = V* - Vsd, and X(nd+1),k is the random variable associated with rk for the policy
network, i.e., Urnk = Vrk + X(nd+1),k.
As with Theorem 1, getting an exact expression for the expected sample complexity depends on
the problem model (the distribution for the gaps ∆sd and the errors Xsd). We give two examples
in Section 6 of different models and we apply this theorem to those models to derive the sample
complexity, which we show is polynomial in depth D. Please check Appendix for the proof.
5	Approximating V * Using Value and Policy Networks
This section extends the algorithms to the problem setting where we are willing to tolerate an additive
error δ in the estimate for V*
We first consider the extension of A?MCTS-V*, Algorithm 1. The key idea is that it suffices to stop
searching at a depth D whenever CDe ≤ δ. As long as the most optimistic state at depth D is identified,
the value of this state approximates V * up to an additive error δ . The main result is stated in Theorem
3 and the algorithm is Algorithm A?MCTS-V, presented in Appendix 8.3.
Theorem 3.	With probability 1 — β, Algorithm 3, A? MCTS- V, returns V * UP to additive error δ.
The expected sample complexity (number of calls to the value network) is
E[N] ≤ KD +〉： K∙P I	∆sd	- Xsd	- cd - CD ≤ 0	\	∆sd0	- XsdO- Cd-	CD	≤ 0
sd∈S	sd0 ∈A(sd)
d<De
sd∈A(l*)
i	Λ / ι^∖ ∙ .ι	. r	.	.	r . ι	. ∙ ι ι r 7 ⅛	ι Λ	Tτ* TT	ι tλ ∙ .ι
where A(l*) is the set of ancestor states of the optimal leaf, l*, and ∆sd = V* - Vsd, and D is the
minimum depth at which CDe ≤ δ
T τ ∙	.1	∙	. 1	,'	Λ -4^τι X X-1ΓT-T C J7^	.	1 ZIATl X X-1ΓT-T C ± T7^ 士 . . 1 CIl	♦
Using the same reasoning as the one for A?MCTS-V, We can extend A?MCTS-∏* V * to the following
result.
6
Under review as a conference paper at ICLR 2020
Theorem 4.	With probability 1 - β, one can find V * up to additive approximation error δ using
expected sample complexity (number of calls to the value network):
E[N] ≤ X (2 + X P(Ki - Kk + X∏d+i),ι - X∏d+i),k ≤ 2c∏+ι))∙
sd,d<De	k=2
P ( δSd-	XSd-Cd-CD	≤ 0	\	δS"0	-	Xsd0	- cd0	-	cD	≤ 0 ),
sd0 ∈A(sd)
where D is the minimum depth at which CDe ≥ δ.
6 Models and Experiments
In this section, we discuss two simple yet representative models for the Markov decision tree and
show that the sample complexity is polynomially bounded under reasonable noise models.
6.1	CHOOSING Cd FOR GAUSSIAN NOISE
The noises Xd, Xdπ are taken to be Gaussian N (0, σd2) random variables. Two noise models are tested.
The first is a GaUSSian with an exponentially decaying standard deviation, i.e., Xd, Xd ~ N(0, α2d)
for some α > 1. The second is a Gaussian with a polynomially decaying standard deviation, i.e.,
Xd, Xd ~ N(0,表),for some γ > 1. The experiments are carried out for α = 1.3 and 1.5 for the
exponential case and γ = 1.3 and 1.5 for the polynomial case.
The assumption of decaying standard deviation as a function of depth is motivated by basic approxi-
mate dynamic programming principles, which we elaborate on in Section 3.
The choice of Cd is essential to the performance of the algorithms. Standard concentration inequalities
for sub-Gaussian random variables state that P(∣Xd| ≥ cd) ≤ 2e 2σ2 , and this probability expression
is required to be less than DKd. After substituting it in, we arrive at cd = O(Vdσd).
6.2	Constant Gap Model
In the constant gap model, V * — Vl* = η for a fixed constant η > 0 and Vl = 0 for any other leaf
l 6= l*. Therefore, the gap ∆(l) = η for any l 6= l*. The noise variables Xd, Xdd are mean-zero
Gaussian random variables.
Figure 2: The constant gap model. The leaves have value 0 except for the optimal leaf, which has
value η . A blue path is a sub-optimal path, the red path is the optimal path.
Using value network. The expected sample complexity for Algorithm 1 is
D-1 d-1	i
E[N] = KD + XX 卜 K-1) Y K P(η- X(d-j) - C(d-j) ≤ 0)
7
Under review as a conference paper at ICLR 2020
This translates to a sample complexity of E[N] ≤ KD + D2 (K - 1)Kc, for the smallest constant C
that satisfies ση - √c ≥ √2 log K.
The analysis for this result, as well as similar analyses and sample complexity results for Algorithm 2
and the approximate counterparts are in Appendix 8.4.
Experiments. We compare the performance of A?MCTS-V* with the MCTS algorithm that uses the
UCB estimate Qs,a
+ 2	尸PWl
Ns,a
and queries the value network Us instead of expanding state
s with random rollouts, where c is a constant, and Ns,b are visitation counts for choosing action a
from state s. We also compare A*MCTS-∏* V * with the PUCT algorithm described in Silver et al.
(2017) that uses the UCB estimate Qs,a + C ∙ ps,a
the policy network Uπ .
/Pb Ns,b
N	Ns,a
, where ps,a is the probability given by
The experiments are performed on a tree of depth D = 10 with K = 5 children per state. As defined
in the constant gap model, the optimal leaf has reward η and all other leaves have reward 0, and
We experiment with η = 1 and η = 0.5. In our experiments, We use C = 1 in the the bonus UCB
expression for benchmark MCTS and PUCT algorithms. Our algorithms set Cd = 5 ∙ √dσd, where
σd is the variance of the Gaussian noise. This choice is justified earlier in Section 6.1.
Since the main use case for these algorithms is to find the optimal one-step action from s0, we
collect data on the one-step action that the algorithm would output at every 1, 000 expansions (or
queries to the value network U). MCTS algorithms would output the one-step action from s0 with
the highest visitation count. Our algorithms would output the action from s0 that leads to s, where s
is the highest-valued element in the current priority queue Q based on the value network estimate Us .
We give an overall budget of 20,000 expansions and note whether or not (so the data is 1 or 0) the
algorithm chooses the right one-step action. Each of our experiments average over 200 trials. Tables
1 and 2 give the proportion of trials that return the correct optimal one-step action at the 20,000-th
expansion. To give an idea about the performance gain, Figures 3 and 4 show the success proportion
over time, at successive 1000 expansion intervals for Algorithm 2 and PUCT for η = 0.5 in both
noise models (polynomially and exponentially decaying).
	Polynomially Decaying Noise				Exponentially Decaying Noise			
	Y = 1.3		γ = 1.5		α = 1.3		α = 1.5	
	Alg1	MCTS	Alg1	MCTS	Alg1	MCTS	Alg1	MCTS
η = 1	T	^05I-	T	0.695	T	0.355	T	0.605
η = 0.5	1	0.38	1	0.435	0.65	0.265	1	0.4
Table 2: Value Network Only Results for the Constant Gap Model
	Polynomially Decaying Noise				Exponentially Decaying Noise			
	Y = 1.3		γ = 1.5		α = 1.3		α = 1.5	
	Alg2	PUCT	Alg2	PUCT	Alg2	PUCT	Alg2	PUCT
η = 1	1	T	1	T	1	T	1	T
η = 0.5	1	0.885	1	0.92	0.685	.705	1	0.875
Table 3: Value and Policy Network Results for the Constant Gap Model
6.3	Generative Model
The model generates the value function of the nodes in the Markov decision tree in a recursive way.
This model first chooses the optimal value Vs0 = V* at the root s0 . Given the value function of a
parent node p, the value functions for the children r1, . . . , rK are generated as follows: one of the
children (without loss of generality, say r1) has the same value function as the parent Vp; each of the
remaining children r has a value function Vp - Y with Y sampled independently from 〜 U[0,η],
the uniform distribution over the interval [0, η]. At level d, there are exactly dt (K - 1)t states with
value function distributed according to V* - (Y1 + . . . + Yt). When t is non-negligible, Y1 + . . . +Yt
concentrates at constant value tn/2 by the concentration of measure phenomenon.
8
Under review as a conference paper at ICLR 2020
Value and Policy in Constant Gap Model with Gap = 0.5 and polynomially decaying noise Value and Policy in Constant Gap Model with Gap = 0.5 and exponentially decaying noise
25	5.0	7.5 IOJ 125	15.0	17.5	20.0
Number of Samples (scale = 1000)
Figure 3: Using Value and Policy Networks
for search in the Constant Gap Model with
polynomially decaying noise and η = 0.5.
---PUCT with gamma = U
---- A*MCTS-pi*V* with gamma = 1.3
---PUCT Witb gamɪna = 15
——A*MCTS-pi*V* with gamma = 1.5
Figure 4: Using Value and Policy Networks
for search in the Constant Gap Model with
exponentially decaying noise and η = 0.5.
Number of Samples (scale = 1000)
Figure 5: The generative model. A blue path is a sub-optimal path, the red path is the optimal path.
The optimal leaf has value V *. Sub-optimal nodes inherit the gaps of their parent as well as an extra
gap Y , so that the sub-optimal leaf values get smaller and smaller.
Using value networks. Following the estimate in Theorem 1, the overall complexity for Algorithm 1
can be bounded up to an extra K D term by
XX XX C) (κ - i)t P (χd 泻-Cd) = XX XX C 卜 K - Dtp (z ≥ 1
d=1 t=0	d=1 t=0	σd
—
where Z is a standard normal distribution. The goal is to bound this by a quantity polynomial
(rather than exponential) in D when η, K , and α are considered fixed. To proceed, we introduce
T(d)
2(√2 log K+1)√dσd
η
. For any t ≥ T (d),
e- log Kd = K-d
Therefore, for a fixed d,
Xd	dt(K-1)tP Z ≥
t=T (d) t
P(Z ≥ 2σtd
≤ Xd	dt(K-1)tK-d≤Xd	dt(K-1)tK-d=1.
t=T (d)	t=0
Therefore, up to a term linear in D , the sample complexity can be bounded by
D T (d)
X X dt
(K - 1)tP Z ≥
d=1 t=1
(1)
Notice first that P(Z ≥ 舞—Vzd) is always bounded by 1. Second, since in both the exponential
and polynomial noise models σd decays faster than 1 / Vd asymptotically, T(d)
2(√2 log K +1)√dσd
η
9
Under review as a conference paper at ICLR 2020
is uniformly bounded by a constant C(K, η, α) due to the exponential decay in d. Therefore, the term
(4) can be bounded by (DK)C(K,η,α), which is polynomial (rather than exponential) in the depth D.
Similar analysis and sample complexity statements can be made for the performance of Algorithm 2
and the approximate counterparts. We defer this treatment to Appendix 8.5.
Experiments. Our experimental results for the Generative Model follow the same setup and parame-
ters as the experiments for the Constant Gap Model in Section 6.2. Tables 3 and 4 give the proportion
of trials that return the correct optimal one-step action at the 20,000-th expansion. Figures 6 and 7
show the success proportion over time, at successive 1000 expansion intervals for Algorithm 2 and
PUCT for η = 1 in both noise models (polynomially and exponentially decaying).
	Polynomially Decaying Noise				Exponentially Decaying Noise			
	Y = 1.3		γ = 1.5		α = 1.3		α = 1.5	
	Algl	MCTS	Alg1	MCTS	Alg1	MCTS	Alg1	MCTS
η = 1	T	0.895	T	0.895	T	^09	T	0.895
η = 0.5	1	0.865	1	0.865	0.825	0.865	0.995	0.87
Table 4: Value Network Only Results for the Generative Model
	Polynomially Decaying Noise				Exponentially Decaying Noise			
	Y = 1.3		γ = 1.5		α = 1.3		α = 1.5	
	Alg2	PUCT	Alg2	PUCT	Alg2	PUCT	Alg2	PUCT
η = 1	T	^094-	T	^094-	^099-	^092-	T	0.935
η = 0.5	1	0.935	1	0.92	0.82	0.9	1	0.92
Table 5: Value and Policy Network Results for the Generative Model
Figure 7: Using Value and Policy Networks
for search in the Generative Model with
exponentially decaying noise and η = 0.5.
Figure 6: Using Value and Policy Networks
for search in the Generative Model with
polynomially decaying noise and η = 0.5.
7	Conclusion
We introduce A?MCTS, a search algorithm that uses pre-trained value/policy networks to guide
exploration and learn to make optimal decisions. We provide expected sample complexity bounds
and demonstrate theoretically and experimentally that our algorithms work efficiently for reasonable
models of reward distributions and noise (in value/policy network estimates) distribution. Future
work includes more rigorous experimental analysis, improved search algorithms that automatically
adapts to the noise level of the value/policy networks, and applications to real scenarios.
10
Under review as a conference paper at ICLR 2020
References
Imad S AlShawi, Lianshan Yan, Wei Pan, and Bin Luo. Lifetime enhancement in wireless sensor
networks using fuzzy approach and a-star algorithm. 2012.
Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp
Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A survey
of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in
games, 4(1):1-43,2012.
Hung-Che Chen and Jyh-Da Wei. Using neural networks for evaluation in heuristic search algorithm.
In Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011.
Pierre-AmaUd CoqUelin and Remi Munos. Bandit algorithms for tree search. arXiv preprint
cs/0703062, 2007.
Daniel Delling, Peter Sanders, Dominik SchUltes, and Dorothea Wagner. Engineering roUte planning
algorithms. In Algorithmics of large and complex networks, pp. 117-139. Springer, 2009.
Ali Ghaffari. An energy efficient roUting protocol for wireless sensor networks Using a-star algorithm.
Journal of applied research and technology, 12(4):815-822, 2014.
Evangelos KanoUlas, Yang DU, Tian Xia, and DonghUi Zhang. Finding fastest paths on a road network
with speed patterns. In 22nd International Conference on Data Engineering (ICDE’06), pp. 10-10.
IEEE, 2006.
Levente Kocsis and Csaba Szepesvari. Bandit based monte-carlo planning. In European conference
on machine learning, pp. 282-293. Springer, 2006.
Renato Negrinho and Geoff Gordon. Deeparchitect: AUtomatically designing and training deep
architectUres. arXiv preprint arXiv:1704.08792, 2017.
MasoUd Nosrati, Ronak Karimi, and Hojat Allah Hasanvand. Investigation of the*(star) search
algorithms: Characteristics, methods and approaches. World Applied Programming, 2(4):251-256,
2012.
KeyUr Rana and MUkesh Zaveri. A-star algorithm for energy efficient roUting in wireless sensor
network. In Trends in Network and Communications, pp. 232-241. Springer, 2011.
Christopher D Rosin. MUlti-armed bandits with episode context. Annals of Mathematics and Artificial
Intelligence, 61(3):203-230, 2011.
Devon SigUrdson and Vadim BUlitko. Deep learning for real-time heUristic search algorithm selection.
In Thirteenth Artificial Intelligence and Interactive Digital Entertainment Conference, 2017.
David Silver, Aja HUang, Chris J Maddison, ArthUr GUez, LaUrent Sifre, George Van Den Driessche,
JUlian Schrittwieser, Ioannis AntonogloU, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neUral networks and tree search. nature, 529(7587):484, 2016.
David Silver, JUlian Schrittwieser, Karen Simonyan, Ioannis AntonogloU, Aja HUang, ArthUr GUez,
Thomas HUbert, LUcas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go withoUt
hUman knowledge. Nature, 550(7676):354, 2017.
Joel Veness, Marc Lanctot, and Michael Bowling. Variance redUction in monte-carlo tree search. In
Advances in Neural Information Processing Systems, pp. 1836-1844, 2011.
JingyUan Wang, Ning WU, Wayne Xin Zhao, Fanzhang Peng, and Xin Lin. Empowering a* search
algorithms with neUral networks for personalized roUte recommendation. In Proceedings of
the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
539-547. ACM, 2019a.
Linnan Wang, Yiyang Zhao, YUU Jinnai, and Rodrigo Fonseca. Alphax: exploring neUral architectUres
with deep neUral networks and monte carlo tree search. arXiv preprint arXiv:1805.07440, 2018.
11
Under review as a conference paper at ICLR 2020
Linnan Wang, Saining Xie, Teng Li, Rodrigo Fonseca, and Yuandong Tian. Sample-efficient neural
architecture search by learning action space. arXiv preprint arXiv:1906.06832, 2019b.
Martin Wistuba. Finding competitive network architectures within a day using uct. arXiv preprint
arXiv:1712.07420, 2017.
12
Under review as a conference paper at ICLR 2020
8	Appendix
8.1	Proof of Theorem 1
Proof. Firstly, the event E that all states sd satisfy |Xsd | = |Vsd - Usd | ≤ cd happens with probability
1 - β, where Xsd are i.i.d. copies of Xd. This is because that cd is chosen so that P(|Vsd - Usd | ≤
Cd) ≥ 1 - DeKd for each sd. Therefore, taking union bound over Kd states at depth d and then over
all D levels ensures that the event E happens with probability 1 - β .
A*MCTS-V* chooses iteratively the state Sd with the highest UCB estimate, Usd + Cd and queries
the value network for the values of each of the K children of sd. Under event E, the UCB estimate
Usd + Cd is always an over-estimate of Vsd. Since the algorithm always take from the queue the state
sd with the highest UCB estimate Usd + Cd and it only stops when the queue Q of possible candidates
has only one leaf element, the algorithm is guaranteed to find l*. Suppose otherwise, i.e., |Q| = 1
and Q contains a leaf state l where Vl < V*. Then for an intermediate state sd that is an ancestor
to l*, Vl < Vsd = V* ≤ Usd + Cd under the event E. Therefore, as Q is sorted in descending order
according to the UCB estimates, sd must be ahead of l in Q, which is a contradiction.
To estimate the complexity, one needs to think about how many non-optimal internal nodes are
queried using the value network. Notice that a sub-optimal internal node sd is chosen if V* ≤
Usd + Cd = Vsd + Xsd + Cd. Then it is necessary to understand the probability of the event that
∆sd - Xsd - Cd ≥ 0, because under that event the algorithm does not need to query the value network
for sd. Therefore the sample complexity N (i,e. the number of states for which the value network is
queried) is the given by
N =	^X	K ∙ 1 I ∆sd	-	Xsd	- cd ≤ 0	\	∆sd0	— XsdO-	cd0	≤ 0 ),
sd∈S,sd∈L	∖	sdO ∈A(Sd)	)
where K is the number of children of each node, and A(sd) is the set of ancestors of state sd.
Since Vsd = V*, ∆sd = 0 whenever sd ∈ A(l*). Moreover, because |Xsd | ≤ Cd under event E, it
follows that the expected sample complexity is
E[N] = KD +	X	K ∙ P I∆sd	-	Xsd	—	Cd	≤ 0	\	∆sd0-	XsdO-	CdO	≤ 0 )
sd∈S,sd∈L,sd∈A(l*)	∖	sdO ∈A(sd)	)
□
8.2	Proof of Theorem 2
Proof. Firstly, the event E that all states sd satisfy |Vsd - Usd | ≤ Cd and |Vsd - Usπ | ≤ Cdπ happens
with probability 1 - β, following the same union bound argument as in Theorem 1.
The correctness of A*MCTS-∏* V * follows largely from the proof of correctness for A?MCTS-V *.
The main difference in the two algorithms is in the new sampling condition for ri for i > 2, which is
that ln (ppr1 ) ≤ 2c∏+ι. To show why this is relevant, let us elaborate on the case of i = 3. Under
the event E, Vr3 ≤ Urπ + Cdπ+1, where Cdπ+1 depends on the noise distribution Xdπ+1. Moreover,
Vr1 ≥ Urπ1 -Cdπ+1. Hence, one should sample r3 if Urπ2 + Cdπ+1 ≥ Urπ1 - Cdπ+1, i.e. Urπ1 - Urπ2 ≤ 2Cdπ+1.
This is because if Urπ2 + Cdπ+1 < Urπ1 - Cdπ+1 then it is impossible for Vr3 > Vr1 .
Since the probabilities are given in a softmax form, the condition Urπ1 - Urπ2 ≤ 2Cdπ+1 is equivalent to
1 (ln H ≤ 2cd+1).
When combined with the result from Theorem 1, it gives the following sample complexity bound:
S (2+X11 (ln( Prt )≤ 2cπ+1
13
Under review as a conference paper at ICLR 2020
•1 I	δSd- Xsd-Cd-CD ≤ 0 PI	∆Sd0	-	XsdO-Cd-CD	≤ 0 I .
sd0 ∈A(sd)
This is equivalent to:
X 2 + X 1	Vr1	-	Vrk	+	X(πd+1),1	-	X(πd+1),k ≤	2Cdπ+1	•
sd ∈/ L	k=2
1 I∆sd	- Xsd	- Cd - CDe	≤ 0	\	∆sd0	- Xsd0	-	Cd0 -	CDe	≤ 0I ,
sd0 ∈A(sd)
where X(πd+1),k is the noise random variable associated with rk for the policy network,
i.e., Urπk = Vrk +	X(πd+1),k.	The	events 1	Vr1 - Vrk + X(πd+1),1 - X(πd+1),k ≤ 2Cdπ+1	and
1 ∆sd - Xsd - Cd	- CDe ≤ 0 T ∆sd0 -	Xsd0 - Cd0 - CDe ≤ 0 are independent, since	∆sd
sd0 ∈A(sd)
is the gap between V * and Vsd,彳-Vrk is the difference in value between two children of sd, and
these two quantities are uncorrelated.
Therefore our total expected sample complexity is:
E[N] ≤ X 2+KX-1PVr1 - Vrk + X(πd+1),1 - X(πd+1),k ≤ 2Cdπ+1	•
sd∈L ∖	k=2	)
P I∆sd	-	Xsd	-	Cd	≤ 0	\	∆sd0	-	Xsd0	- Cd0	≤ 0I ,
sd0 ∈A(sd)
□
8.3	APPROXIMATING V*
Algorithm 3 A?MCTS-V
Require: Value Network, additive approximation tolerance δ, failure tolerance β
1:	For each d < D, compute Cd so that P(∣VSd - Usd I ≤ Cd) ≥ 1 - DeKd for each Sd.
2:	Calculate D such that CDe ≤ δ.
3:	Q — (S0, Uso + CO)
4:	while the depth of Q.f ront() is < D do
5:	(SL) = Q.pop()
6:	for r ∈ C(S) do
7:	d J depth of r
8:	Q.enqueue(r, Ur + Cd )
9:	end for
10:	end while
11:	(s, _) J Q.front()
12:	Return Us and the action at S0 that leads to S
8.3.1 Proof of Theorem 3 for Algorithm 3
Proof. Under event E, the correctness for Algorithm 3, A?MCTS-V largely follows the proof in
Theorem 1, with the exception that the algorithm stops when the most optimistic state in the queue Q
is at a special intermediate depth D. Let l* be the state at depth D that achieves arg max Usd + Cd.
sd∈Q
14
Under review as a conference paper at ICLR 2020
For every node sDe at depth D, Q contains an ancestor of sDe . Under the event E, every estimate
Usd + cd is a valid upper bound of Vsd and the true value of sd’s descendants. It follows that
U* - CD ≤ V * ≤ Um + CD. Therefore, UR approximates V * UP to additive error δ when CD ≤ δ.
In order to reason about the sample complexity, notice that we query the value network U for a state
Sd if U* < Usd + Cd ≤ Vsd + Xsd + Cd with the additional requirement that d ≤ D. Therefore,
it is sufficient to consider the event where U/* - Kd ≤ Xsd + c4. Since Uj* is lower-bounded by
V* - CDe, it suffices to consider the event of V* - Vsd ≤ Xsd + Cd + CDe.
Therefore the sample complexity N of states queried using the value network is upper-bounded by
N ≤ X K ∙ 1 I	∆sd	- Xsd	-Cd	-CD ≤ 0	\	ZsdO- XsdO-Cd0	- cD ≤ 0 ).
sd∈S,d<De	sd0 ∈A(sd)
Since ∆sd = 0 whenever sd ∈ A(l*) and |Xsd | ≤ Cd under event E, it follows that the expected
sample complexity is
E[N] ≤ KD + X K∙P I Zsd	-	Xsd	- cd - cD	≤ 0	\	ZsdO	- XsdO- cd0	-	cD	≤ 0 )
sd∈S	sdO ∈A(sd)
d<De
sd∈A(l*)
□
8.4	Constant Gap Model
The first example we will consider is the constant gap model, where ∆(l) = η for any l 6= l*, and the
noise variables Xd , Xdπ are mean-0 Gaussian random variables.
8.4.1	Using Value Network
Lemma 1. In the Constant Gap Model, the expected sample complexity for Algorithm 1 is:
D-1 d-1	i
E[N] =KD+ XX (K-1)YKP(η-X(d-j) -C(d-j) ≤0) .
d=1 i=0	j=0
This translates to a sample complexity of E[Ν] ≤ KD + D2 (K — 1)KC, for some Constant C that
satisfies ση——√C ≥ ʌ/2 log K.
Proof. For A?MCTS-V*, Algorithm 1, Theorem 1, the sample complexity
E[N ] = KD+	X	K ∙P I ∆sd - Xsd - Cd ≤ 0	\	∆sdo - Xsdo - CdO ≤ 0 )：
sd∈S,sd∈L,sd∈{A(l*)	∖	sdO ∈A(sd)	)
is easy to evaluate, because the individual events ∆sd - Xsd - Cd ≤ 0 and ∆sdO - XsdO - CdO ≤ 0
are independent. We claim that
X ：	K ∙ P I	Zsd	-	Xsd	- cd ≤ 0	\	ZsdO	- XsdO-	cd0	≤ 0 1
sd∈S,sd∈L,sd∈{A(l*)	∖	sdO ∈A(sd)	)
is equivalent to
D-1 d-1	i
XX (K -1) Y KP(η -X(d—j) - c(d-j) ≤ 0) I .	⑵
To see why this is the case, we can reason about the first few levels. At depth d = 1, we choose to
expand a state with probability P(η - X1 - C1 ≤ 0). There are K - 1 states s1 where s1 ∈/ A(l*).
15
Under review as a conference paper at ICLR 2020
If we expand a particular node, the sample complexity is K. Therefore the expression would be
(K - 1)(K) ∙ P(η - Xi- ci ≤ 0).
At depth d = 2, there are (K - 1)(K) states whose depth 1 ancestor is not in A(l*). For these states,
we choose to expand each with probability P(η - X2 - c2 ≤ 0)P(η - Xi - ci ≤ 0). The sample
complexity for each expansion is K, so in expectation, we query (K - 1)(K2)P(η - X2 - c2 ≤
0)P(η - Xi - ci ≤ 0) states. There are also K - 1 states who are not in A(l*) but their parent in
depth 1 are in A(l*), and We expand them with probability P(η - X2 - c? ≤ 0). For these states, We
query (K - 1)(K)P(η - X2 - c2 ≤ 0) states in expectation. Continuing this reasoning through for
depths 3, . . . D - 1 yields the final expression in Equation 2.
Therefore the expected sample complexity is:
D-i d-i	i
E[N] = KD + XX kK - 1) Y KP(η - x(d-j) - S) ≤ 0)
We can rewrite the above as:
D-i d-i	i
E[N] = KD + XX (K - 1) Y KP(X(d-j) ≥ η - C(d-j))
which is equivalent to:
D-i d-i	i
E[N] = KD + XX (K-1)YKP
where Z is the standard normal distribution.
Z ≥ —η— P(d- j)
i i	_ t2	_ t2
Standard tail inequalities gives US that P(Z ≥ t) ≤ t √2∏e-3 ≤ e-百,where the last inequality
holds for t > 1.
t2
We are interested in a value for t such that e-ɪ ≤ e- log K. This is achieved when t = √2 log K.
For exponentially decaying standard deviation as a function of depth, i.e., σd =表 for α > 1, when
η ∙ α(d-j) - √d-j >t, P(Z ≥ σ(d-j) - √pf) ≤ K.
Since α, and K are constants, there exists a constant c such that
η ∙ a(d-j) - √(d - j) ≥ √2log K
whenever d - j ≥ c.
Therefore, the sample complexity is bounded by:
E[N] ≤ KD+D2(K- 1)Kc	(3)
which is polynomial in the depth D.
For polynomially decaying standard deviation as a function of depth, i.e., ◎& = / for α > 1, when
η ∙ (d - j)α - √d - j > t, P (Z ≥ σ(d-j) - P(d - j)) ≤ KK.
Since α, and K are constants, there exists a constant c0 such that
η ∙ (d - j)α - √(d - j) ≥ √2 log K
whenever d - j ≥ c, and a similar sample complexity to Equation 3 holds.	□
8.4.2	δ-APPROXIMATE VALUE ESTIMATION
The above reasoning generalizes naturally to a δ-approximate value estimation using A?MCTS-V ,
Algorithm 3, and the expected sample complexity is:
D-i d-i
E[N] = KDe + XX
d=i i=0
i
(K-1)YKP(η-X(d-j)-c(d-j)-cDe ≤0)
j=0
16
Under review as a conference paper at ICLR 2020
8.4.3	Using Value and Policy Network
Lemma 2. In the Constant Gap Model, the expected sample complexity for Algorithm 2 is:
D-1	K-1
E[N] = X 2 + X P (η + X(d+I),ι + X(d+I),k ≤ 2c∏+ι)
d=0	k=2
D-1 d-1	K-1	i
+XX 2 +	^X	P	(η	+ X(d-i),1	+ X(d-i),2	≤	2cπ-i)	Y KP(η	-	Xd-j	- cd-j	≤ O)
d=1 i=0	k=2	j=0
Proof. The policy network for this constant gap problem model is not extremely useful, because
for suboptimal states, the immediate children have the same optimality gap, and therefore, the
probabilities given by the policy network will be fairly evenly distributed among the immediate
children nodes. The policy network can give more differentiated probabilities for the immediate
children of a parent state that is an ancestor of the optimal leaf.
First, the sample complexity KD for the optimal path can be improved to
D-1	K-1
X ( 2+ X P (η + x(d+1),1 + x(d+1),k ≤ 2cπ+l)).
d=0	k=2
For the ancestor leaf at level d, the number of child leaves that we would expand in expectation using
K-1
the information from the policy network is 2 + P η + X(d+1),1 + X(d+1),k ≤ 2cdπ+1	.
k=2
The second expression can be improved to:
D-1 d-1
dX=1Xi=0
K-1
2 + X P (η + X(d-i),1 + X(d-i),2 ≤ 2cπ-i)
k=2
i
Y KP(η - Xd-j - cd-j ≤0) .
j=0
To see why this is the case, we can reason about the first few levels. At depth d = 1, we choose to
expand a state with probability P(η - Xi - ci ≤ 0). There are K - 1 states si where si ∈ A(l*),
K-1
but in expectation we only consider 2 + P P η + X(i),i + X(i),2 ≤ 2ciπ of these states based on
k=2
the information from the policy network. If we expand a particular state, the sample complexity is K,
since all K children nodes have the same value in this constant gap model, so the sampling criteria
using probabilities from the policy network will always be met. Therefore the expression would be
K-i
2+ P P(η + X⑴,i + X(i),2 ≤ 2cπ) (K) ∙P(η - Xi - ci ≤ 0).
k=2
K-i
At depth d = 2, there are in expectation 2 + P P η + X(i),i + X(i),2 ≤ 2ciπ	(K) states
k=2
that We would consider expanding from whose depth 1 ancestor is not in A(l*). For these
states, we choose to expand each with probability	P(η	-	X2	-	c2	≤	0)P(η	-	Xi	-
ci ≤ 0). The sample complexity for each expansion is K, so in expectation, we query
K-i
2 + P P	(η + X⑴,i + X⑴,2 ≤	2cπ)	(K2)P(η	-	X2	-	c2	≤ O)P(n	- Xi	-	ci	≤ 0) states.
k=2
There are also K - 1 states who are not in A(l*) but their parent in depth 1 are in A(l*), and
we expand them with probability P(η - X2 - c2 ≤ 0). For these K - 1 states, in expectation,
K-i
we have expanded only 2 + P P η + X(2),i + X(2),2 ≤ 2ciπ	of them, therefore, we expect
k=2
K-i
to query 2 + P P η + X(i),i + X(i),2 ≤ 2ciπ	(K)P(η - X2 - c2 ≤ 0) states in expectation.
k=2
Continuing this reasoning through for depths 3, . . . D - 1 yields the final expression:
D-i	K-i
E[N] = X 2+ X P (η + X(d+i),i + X(d+i),k ≤ 2c∏+i)
d=0	k=2
17
Under review as a conference paper at ICLR 2020
D-1 d-1 K-1
+ XX 2+ X P (η + x(d-i),1 + X(d-i),2 ≤ 2cπ-i)
i
Y KP(η - Xd-j - cd-j ≤ 0)
j=0
□
For a δ-approximate value estimate, similar arguments apply to give a sample complexity of:
d0-1	K-1
E[N] = X 2+ X P (η + X(d+I),ι + X(d+I),k ≤ 2c∏+ι)
d=0	k=2
d0-1 d-1	K-1	i
+XX 2 +	^X	P (η + x(d-i),1	+ x(d-i),2	≤ 2cπ-i)	Y KP(η - xd-j	- cd-j	- cD	≤	O).
d=1 i=0	k=2	j=0
8.5 Generative Model
The model generates the value function of the nodes in the Markov decision tree in a recursive way.
This model first chooses the optimal value VS0 = V * at the root s°. Given the value function of a
parent node p, the value functions for the children r1 , . . . , rK are generated as follows: one of the
children (without loss of generality, say r1) has the same value function as the parent Vp; each of the
remaining children r has a value function Vp - Y with Y sampled independently from 〜 U[0,η],
the uniform distribution over the interval [0, η]. At level d, there are exactly dt (K - 1)t states with
value function distributed according to V * - (Y1 + . . . + Yt). When t is non-negligible, Y1 + . . . +Yt
concentrates at constant value tn/2 by the concentration of measure phenomenon.
Using value networks. Let us consider the expected complexity of Algorithm 1 for this model. In
the algorithms, cd = C dσd for some constant C. Following the estimate in Theorem 1, the overall
complexity can be bounded up to an extra KD term by
XX (d)(K - 1)tP (Xd ≥ $-cd) = XX C)(K - 1)tP (z ≥ M - Ce√d)
d=1 t=0 t	2	d=1 t=0 t	2σd
where Z is a standard normal distribution. The goal is to bound this by a quantity polynomial
(rather than exponential) in D when η, K, and α are considered fixed. To proceed, we introduce
2(√2 log K+1)√dσd
T(d)
. For any t ≥ T (d),
η
P(Z ≥ 2σd
e- log Kd = K-d
Therefore, for a fixed d,
X (d)(K-i)tP (Z ≥ 畀-c√d)≤ X (d)(KTyK-d ≤ X (d)(KTyK-d=ι.
Therefore, up to a term linear in D, the sample complexity can be bounded by
D T(d)
X X ("K - IyP(Z ≥ 翁
—
(4)
Notice first that P(Z ≥ 吃-√d) is always bounded by 1. Second, since in both the exponential
and polynomial noise models σd decays faster than 1 /√d asymptotically, T(d)
2(√2 log Κ+1)√dσd
is uniformly bounded by a constant C(K, η, α) due to the exponential decay in d. Therefore, the term
(4) can be bounded by (DK)C(K,η,α), which is polynomial (rather than exponential) in the depth D.
Using value and policy networks. Let us consider now the expected complexity of Algorithm 2.
Following the argument of the constant gap model, it can be bounded by
D T(d)
XX dt (K - 1)t-1
d=1 t=1
K-1
2+ X P(η + x∏ι + x∏2 ≤ 2c∏)
k=2
—
P Z ≥ ɪ
V — 2σd
η
18
Under review as a conference paper at ICLR 2020
which is also polynomial in D as it improves over the complexity of using just the value network.
Approximating V * using value and policy networks. For a δ-approximate value estimate using a
value network, one can stop early at level De instead and the complexity analysis gives a bound
XX Tlo- 1)tP (Z ≥ 2σd
O((DeK)C(K,η,α))
For a δ-approximate value estimate using both policy and value networks, one can stop early at level
T-∖ ∙ .	1	1 . 1	1	1	F	1
D instead and the complexity analysis gives a bound
De T(d)
X X	dt (K - 1)t-1
d=1 t=1
K-1
2+ XP(η + X∏1 + X∏2 ≤2c∏)
k=2
—
P Z ≥ ɪ
k — 2σd
19