Under review as a conference paper at ICLR 2020

l₁ ADVERSARIAL  ROBUSTNESS  CERTIFICATES:
A  RANDOMIZED  SMOOTHING  APPROACH

Anonymous authors

Paper under double-blind review

ABSTRACT

Robustness is an important property to guarantee the security of machine learning
models.  It has recently been demonstrated that strong robustness certificates can
be obtained on ensemble classifiers generated by input randomization.  However,
tight robustness certificates are only known for symmetric norms including l₀ and
l₂, while for asymmetric norms like l₁, the existing techniques do not apply.  By
converting the likelihood ratio into a one dimensional mixed random variable, we
derive the first tight l₁ robustness certificate under isotropic Laplace distributions
in binary case. Empirically, the deep networks smoothed by Laplace distributions
yield the state-of-the-art certified robustness in l₁  norm on CIFAR-10 and Ima-
geNet.

1    INTRODUCTION

have done a series of nice works in practical sights or theoretical sights (Zheng et al., 2016; Gouk
et al., 2018). Among them, certifiably robustness is valuable, since it can withstand all attacks 
within
a norm ball and has a nice theoretical and practical outcome. However, most work cannot deal with
the case for general neural networks.

Deep networks are flexible models that are widely adopted in various applications. However, it has
been shown that such models are vulnerable against adversary (Szegedy et al., 2014). Concretely, an
unnoticeable small perturbation on the input can cause a typical deep model to change predictions
arbitrarily.   The phenomenon raises the concerns of the security of deep models,  and hinders its
deployment in decision-critical applications. Indeed, the certification of robustness is a 
pre-requisite
when AI-generated decisions may have important consequences.

Certifying  the  robustness  of  a  machine  learning  model  is  challenging,  especially  for  
modern
deep  learning  models  that  are  over-parameterized  and  effectively  black-box.   Hence,  the  
exist-
ing  approaches  mainly  rely  on  empirical  demonstration  against  specific  adversarial  attack 
 algo-
rithms (Goodfellow et al., 2015; Madry et al., 2018; Finlay et al., 2019). However, this line of 
works
can give a false sense of security.  Indeed, successful defense against the existing attack 
algorithms
does not guarantee actual robustness against any adversaries that may appear in the future.

Recently, the adversarial robustness community has shifted the focus towards establishing certifi-
cates  that  prove  the  robustness  of  deep  learning  models.   The  certificate  can  be  
either  exact  or
conservative,  so long as the certified region cannot exhibit any adversarial examples.   Given the
over-parameterized deep models and modern high-dimensional datasets, scalability becomes a key
property           for the certification algorithms, as many methods are computationally 
intractable.

Our work is based on the novel modeling scheme that generates ensembles of a fixed black-box
classifier based on input randomization (Cohen et al., 2019). Under this framework, tight robustness
certificates can be obtained with only the ensemble prediction values and randomization parameters.
Given  appropriate  choices  of  distributions,  the  robustness  guarantee  can  be  derived  for  
l₂  or  l₀
norms (Cohen et al., 2019; Lee et al., 2019). The tightness simply implies that any point outside 
the
certified region is an adversarial example in the worst case. However, the derivations of the 
previous
results heavily relies on the fact that the target norm (l₂ or l₀) is symmetric, therefore 
analyzing any
perturbation direction for attacking the model gives the same certification guarantee.

In contrast, l₁ norm is asymmetric.  That is, for a given l₁ ball centered at the origin, if we move
another l₁ ball also from the origin by a distance δ, where ǁδǁ₁ is fixed, then the overlapped 
region

1


Under review as a conference paper at ICLR 2020

Figure 1: In l₁ case, if we perturb the input with δ such that   δ  ₁ is fixed, we may get very 
different
overlapped regions with different size. Notice that this is different from l₂ (or l₀, not shown), 
where
the overlapped regions are always symmetric and of the same size.

between the two l₁ balls may have different shapes and sizes (See Figure 1).  The characterization
of this overlapped region is the key step for proving tight certificates, hence the existing 
techniques
do not apply for l₁ norm.

In  this  work,  we  derive  a  tight  l₁  robustness  guarantee  under  isotropic  Laplace  
distributions.
The  Laplace  distribution  can  be  interpreted  as  an  infinite  mixture  of  uniform  
distributions  over
l₁-norm  balls,  which  is  a  natural  “conjugate”  distribution  for  l₁  norm.   Due  to  
asymmetry,  we
first identified the tight robustness certificate for attacking the model in one particular 
direction,
δ  = (  δ  ₁, 0,       , 0).  To show that other perturbation directions cannot lead to worse 
results, we
convert the d dimensional likelihood function into an one dimensional function, where we apply
relaxation for various δ  and show that the worst case result is bounded by the specific direction

(ǁδǁ₁, 0, · · · , 0).

Theoretically, our certificate is tight in the binary classification setting.  In the multi-class 
classi-
fication setting, our certificate is always tighter than the previous certificate proposed by 
Lecuyer
et   al. (2019).  The theoretical improvement always leads to superior empirical results on 
certifying
the same model, where we demonstrate the result on CIFAR-10 and ImageNet with ResNet models.
Moreover,  the proposed robustness certificate on models smoothed by Laplace distributions also
outperforms the same models trained and certified using Gaussian distributions (Cohen et al., 2019)
in           l₁ certified robustness, where the Gaussian-based robustness certificate is adapted 
from l₂ norm.

2    RELATED  WORK

Robustness of a model can be defined in various aspects.  For example, Feynman-Kac Formalism
can be used to improve robustness (Wang et al., 2018). In this paper, we focus on the classification
setting, where the goal is to provide guarantee of a constant prediction among a small region spec-
ified via some metric.  The robustness certificate can be either exact or conservative, so long as a
constant prediction is guaranteed in the certified region.  Note that the certification of a 
completely
black-box model requires checking the prediction values at every point around the point of interest,
which is clearly infeasible. A practical certification algorithm inevitably has to specify and 
leverage
the functional structure of the classifier in use to reduce the required computation.

Exact  certificates.   The  exact  certificate  of  deep  networks  is  typically  derived  for  
the  networks
with a piecewise linear activation function such as ReLU. Such networks have an equivalent mixed
integer linear representation (Cheng et al., 2017; Lomuscio & Maganti, 2017; Dutta et al., 2017;
Bunel et al., 2018). Hence, one may apply mixed integer linear programming to find the worst case
adversary within any convex polyhedron such as an l₁-ball or l   -ball. Despite the elegant 
solution,
the complexity is, in general, NP-hard and the algorithms are not scalable to large problems(Tjeng
et al., 2017).

2


Under review as a conference paper at ICLR 2020

Conservative certificates.  A conservative certificate can be more scalable than the exact methods,
since one can trade-off the accuracy of certification with efficiency (Gouk et al., 2018; Tsuzuku
et al., 2018; Cisse et al., 2017; Anil et al., 2018; Hein & Andriushchenko, 2017). For example, one
can relax the search of the worst case adversary as a simpler optimization problem that only bounds
the effect of such adversary. Alternatively, people also consider the robustness problem in a 
modular
way, where the robustness guarantee can be derived iteratively for each layer in the deep networks 
by
considering the feasible values for each hidden layer (Gowal et al., 2018; Weng et al., 2018; Zhang
et         al., 2018; Mirman et al., 2018; Singh et al., 2018). However, this line of works have 
not yet been
demonstrated to be feasible to realistic networks in high dimensional problems like ImageNet.

Randomized smoothing.  Randomized smoothing has been proved to be closely related to robust-
ness.  Although similar techniques have been tried by (Liu et al., 2018; Cao & Gong, 2017),  no
corresponding proofs have been given; Li et al. (2018) and Cohen et al. (2019) have proved certified
robustness of l₂ norm under isotropic Gaussian noise, and Lee et al. (2019) proved robustness for
l₀  form. Lecuyer et al. (2019) use techniques from differential privacy to prove l₁ robustness 
under
Gaussian and Laplace noise respectively, but the bounds are not tight.  Li et al. (2018); Pinot et 
al.
(2019) use Re´nyi divergence framework without tightness proof. Our results synthesize the ideas in
(Cohen et al., 2019; Lee et al., 2019; Lecuyer et al., 2019; Li et al., 2018; Pinot et al., 2019) 
and
prove the tight robustness radius under the binary classification setting.

3    PRELIMINARIES

Definition 1 (Laplace distribution)  Given λ  ∈ R⁺, d  ∈ Z⁺, we use L(λ) to denote the Laplace
distribution  in  dimension  d  with  parameter  λ.    The  p.d.f.  of  L(λ)  is  denoted  as  L(x; 
λ)   ,

    1      exp(−ǁxǁ1 ).

As we will see in Lemma 3.1, in smoothing analysis, we are interested in the likelihood ratio of two
random variables X = s and Y  = δ + s (here s ∼ L(λ) and δ ∈ Rᵈ is a fixed vector). Specifically,

µY (x)  = exp .− 1 (ǁx − δǁ  − ǁxǁ )Σ

Therefore, the likelihood ratio between two d dimensional random variables is controlled by a one
dimensional random variable T (x) ,   x     δ  ₁       x  ₁, where x        (λ).  This 
transformation is
crucial in our analysis, and it is easy to see that T (x) is a mixed random variable, since Pₓ(T 
(x) =
ǁδǁ₁) > 0.

In our analysis, we need to calculate the inverse of c.d.f. of T (x).  However, since T (x) is a 
mixed
random  variable,  sometimes  the  inverse  may  not  exist.   See  Figure  3  for  illustration,  
where  the
inverse of the probability 0.85 does not exist. To deal with this case, we have the following 
modified
version of Neyman-Pearson Lemma, with the proof in Appendix A.

Lemma 3.1  (Neyman-Pearson Lemma for mixed random variables).  Let X  ∼ L (λ) and Y   ∼
L (λ).+ δ.  Let h : Rᵈ → {0, 1} be anΣy deterministic or random function.  Given any β  ∈ R, and

1. If S =   z ∈ Rᵈ : ǁz − δǁ₁ − ǁzǁ₁ > β   ∪ S′, and P(h(X) = 1) ≥ P(X ∈ S) then P(h(Y ) =

1) ≥ P(Y  ∈ S)

2. If S =   z ∈ Rᵈ : ǁz − δǁ₁ − ǁzǁ₁ < β   ∪ S′, and P(h(X) = 1) ≤ P(X ∈ S), then P(h(Y ) =

1) ≤ P(Y  ∈ S)

4    MAIN  RESULTS

In  this  paper,  we  apply  the  randomized  smoothing  technique  (Cohen  et  al.,  2019)  for  
getting
robustness  certificates,  which  works  as  follows.    Given  an  input  x,  we  perturb  it  
with  s,  s.t.
s             ∼ L(λ).  Then instead of evaluating the robustness of the original function f (x), we 
evaluate
g(x) , arg maxc Pc(f (x + s) = c), which is effectively the smoothed version of f (x).

3


Under review as a conference paper at ICLR 2020

1

0.5


0

0           1           2           3           4           5

x

Figure 3:   For mixed random variables, some-
times the inverse of the probability does not ex-
ist. E.g., see the solid blue line.

Figure 4:   Comparison for Eqn. (1).  Green re-
gion shows that baseline is better, while red re-
gion shows our new bound is better.

4.1    ROBUSTNESS CERTIFICATES FOR GENERAL CASES

Our first theorem proves that for the smoothed classifier g, and a given input x, there always 
exists
a robust radius R, such that any perturbation δ s.t. ǁδǁ₁ ≤ R, does not alter the prediction of 
g(x).

Theorem 1  Let f  : Rᵈ → Y  be deterministic or random function, and let s ∼ L(λ).  Let g(x) =
arg maxc Pc(f (x + s) = c). Suppose PA, PB ∈ [0, 1] are such that

P (f (x + s) = cA) ≥ PA ≥ PB ≥ max P(f (x + s) = c)


Then g(x + δ) = g(x), ∀ǁδǁ₁ ≤ R, where

c   cA


R = max . λ log(P   /P   ), −λ log(1 − P    + P

)Σ                          (1)

2           A      B                               A         B

Some Remarks:

1.  When PA  →  1 or PB  →  0,  we can get R  →  ∞.   It is reasonable since the Laplace
distribution is supported over Rᵈ, PA → 1 is equivalent to f  = cA almost everywhere.

2.  Compared with (Lecuyer et al., 2019) where they have R  =  λ log(PA/PB), our bound

                 √                                                       2          √               
       


is better if  1−2PA(1−PA)−    1−4PA(1−PA)

A

≤  PB

≤  1−2PA(1−PA)+     1−4PA(1−PA) .   See

Figure 4 for illustration, where we use baseline to denote the bound R =  λ log(PA/PB).

Proof sketch: (The full proof is in Appendix B) For arbitrarily classifier f , we can transform it 
into
a random smoothing classifier g using random smoothing technique, where g returns class cA with
probability no less than PA, and class cB with probability no more than PB. Below we list the three
main ideas we used in our proof:

1. How to deal with an arbitrary f with PA and PB?

Following Cohen et al. (2019), we use Neyman-Pearson Lemma to transform the relation between
P(f (X)  =  cA) and P(f (Y )  =  cA) into the relation between P(X      A) and P(Y       A).  From
Lemma 3.1, Neyman-Pearson Lemma still holds for mixed random variables.

2. How to deal with the relation between X = s and Y  = s + δ?

Inspired by Lecuyer et al. (2019), we use the DP-form inequality (P (Y  ∈ A) ≤ eˢP (X  ∈ A)) to
deal with the relation between P (X ∈ A) and P (Y  ∈ A). In Laplace distribution, ϵ =  ǁδǁ1 .

3. Take complements to get tighter bound.

When PA ₒr B < 1/2, the above DP-form inequality gets tighter.  Therefore, we analyze Aᶜ when

PA ≥ 1/2 to get a new bound, and compare it with the baseline expression.

We derive this bound by Neyman-Pearson Lemma in this work, but an alternative approach is using
Re´nyi Divergence (Li et al., 2018).

4


Under review as a conference paper at ICLR 2020

4.2    TIGHT ROBUSTNESS CERTIFICATES FOR BINARY CASE

Although we get improved result over Lecuyer et al. (2019), the bound in Theorem 1 is not tight
since it considers the general case with multiple categories. In this section, we first present our 
result
for binary classification (Theorem 2), which further improves over Theorem 1.

Theorem 2  (binary case) Let f  : Rᵈ      Y  be deterministic or random function, and let s        
(λ).
Let g(x)  =  arg maxc Ps(f (x + s)  =  c).   Suppose there are only two classes cA and cB,  and

PA ∈ [ ¹ , 1] s.t.

P (f (x + s) = cA) ≥ PA

Then g(x + δ) = g(x), ∀ǁδǁ₁ ≤ R, for

R = −λ log[2(1 − PA)]                                                       (2)

Scretch  of  the  proof:  (The  full  proof  is  in  Appendix  C)  Theorem  2  is  a  special  
binary  case  of
Theorem 1.  We can use a method similar to Theorem 1 to get the results.  However, it is worth
noting that in binary cases, our new improved bound in Theorem 1 always dominates the bound by
Lecuyer et al. (2019). Moreover, our bound in Eqn. (2) is tight, as shown below.

Theorem 3  (tight bound in binary case) In the same setting as Theorem 2, assume PA + PB ≤ 1
and PA ≥  1 .  ∀R′ > −λ log[2(1 − PA)], ∃ base classifier f ∗ and perturbation δ∗ with g∗(x) =
arg maxc Ps(f ∗(x + s) = c) and ǁδǁ₁ = R′, s.t. g∗(x) /= g∗(x + δ∗).

Scretch of the proof:(The full proof is in Appendix C) For Theorem 3, we prove that the bound in
Theorem 2 is tight by calculating the results in one-dimensional case, where δ = (ǁδǁ₁, 0, . . . , 
0).

By calculating, we show that when δ = (ǁδǁ₁, 0, . . . , 0)


∫ ǁδǁ1 +λ log[2PB ]   1 

|x|

.exp( ǁδǁ1 )PB              when ǁδǁ₁ ≤ −λ log[2PB]

1 −    1     exp(−           1 )    o.w.

Therefore, when   δ  ₁         λ log[2PB], the DP-inequality is tight.  The worst-case δ appears in 
the
one-dimension case.


1

0.5

x

P(X     B)

P(Y     B)

4

Our

Lecuyer et al.'s

3           Cohen et al.'s

0                                                                                                   
                   2

-0.5                                                                                                
                      1


-1

-1        0         1         2         3         4         5

x

0

0.5         0.6         0.7         0.8         0.9          1

PA


Figure 5:  When δ is small, we will take the red
part to construct P(X      B), and blue part to
construct P(Y       B).  The difference between
them meets the condition that T (x) =      δ  ₁,
which leads to a tight bound.

Figure  6:    Comparing  different  methods  un-
der different PA.  Our model always gives the
largest radius compared with the other models,
because our bound is tight.

Figure 5 shows the reason why the inequality is tight. When δ is small, for P(X     B), the set B we
selected satisfies   x     B, T (x) =      δ  ₁ (red part).  When P(Y      B) is considered, it 
moves set
S towards left by step δ. However, due to the small δ, S after moving still satisfies the 
requirement
of ∀x ∈ S, T (x) = −ǁδǁ₁ (blue part). Therefore, the inequality is tight.

5


Under review as a conference paper at ICLR 2020

4.3    METHOD COMPARISON

We compared our method with Cohen et al.’s and Lecuyer et al.’s in binary case, see Table 1.  We
plot the curves in Figure 6.  As we can see, under the same variance of each noise, our method can
reach better robustness radius. Below we show simple derivations of the bounds in Table 1.

Table 1: Robustness Radius Comparison
Method                     Noise                         Radius

Our                Laplace L(0, λ)       −λ log[2(1 − PA)]

Lecuyer et al.’s       Laplace L(0, λ)        λ log(PA/1 − PA)


Cohen et al.’s

Gaussian N (0, σ²)

2

σΦ−¹(PA)

Robustness radius of Lecuyer et al. (2019)

Using the basic inequality from differential privacy, we have:

P (f (X) = cA) ≤ exp(β)P (f (Y ) = cA)

P (f (Y ) = cB) ≤ exp(β)P (f (X) = cB)

where  β   =     δ  ₁/λ.    The  above  two  inequalities  show  that  to  guarantee  P (f (Y ) = 
cA)   >

P (f (Y ) = cB), it suffices to show that:

P (f (X) = cA) > exp(2β)P (f (X) = cB)

So plug in β = ǁδǁ₁/λ, we have ǁδǁ₁ ≤ λ log(PA/PB). Furthermore, in binary case, we can plug
in PB = 1 − PA, and get the robustness radius: R =  λ log(PA/1 − PA).

Robustness radius of Cohen et al. (2019)

Denote    p,r(c)  =    x  :    x     c  p     r  .  Since we know that    ₁,r(c)         ₂,r(c), so 
the radius in
(Cohen et al., 2019) can be directly used in l₁ form, which is σΦ−¹(PA).

Besides, since    ₁,r₊s(c)        ₂,r(c) whatever ϵ > 0 is. And (Cohen et al., 2019) is an exact 
robust-
ness guarantee, so we have that the best l₁  form that isotropic Gaussian noise random smoothing
can get is σΦ−¹(PA).

Finally we will prove that −λ log[2(1 − PA)] ≥ λ log(PA/1 − PA).  For simple denotion, we just
set PA = p ≥ 0.5. So it is sufficient to show that −λ log[2(1 − p)] ≥ λ log(p/(1 − p)). By applying

                                     2

 	 

5    EXPERIMENTS

5.1    IMPLEMENTATION DETAILS

Monte Carlo.  Since we cannot get the exact value of PA, we have to use Monte Carlo method
to get the approximate value of PA.  More specifically, we take multiple random samples from the
Laplace distribution to estimate PA.  One way to do it is grouping the samples and get PA using
non-parametric estimation.

In our experiments, we applied two different types of training, as described below.

Type1-Training:  The  first  method  is  intuitive,  and  was  applied  in  (Cohen  et  al.,  
2019).   In  the
training process, we add into inputs:

inputs = inputs + noise

where the noise is sampled from isotropic Laplace distribution.

Type2-Training: The second method was recently proposed by Salman et al. (2019). The idea is to
use adversarial noise samples instead of the raw noise samples in a neighborhood to train the base
classifier. Each training sample can be decomposed to

inputs = inputs + noise + perturbation

6


Under review as a conference paper at ICLR 2020

where the noise comes from an isotropic Laplace distribution, and the perturbation is found approx-
imately by the gradient of loss with respect to the input. Concretely, if we denote the loss as L 
and
the input as x, the perturbation ∆ can be calculated by ∆ = a    sign(   ₓL(θ, x, y)), where a is a
constant.

Evaluation Index. In this paper, we choose certified accuracy as our evaluation index. Robustness
certified accuracy at radius r refers to the proportion of correctly classified samples with at 
least
robustness radius r.  Specifically, if a group of samples with capacity n is   xi  , i = 1, 2, . . 
. , n, its
corresponding certified robustness radius is Ri.  An index xi represent if the sample is classified
correctly.   If  the  sample  is  correctly  classified,  xi  =  1,  otherwise  xi  =  0.   For  a  
given  r,  the

corresponding robustness certified accuracy is defined as α = Σn     xi1(Ri ≥ r)/n, where 1(·) is

an indicator function.

However, from Section 5.1 we know that we cannot calculate the exact robustness radius R, so we
use its Rˆ to approximate R, which leads to a “approximate robustness certified accuracy”(αˆ), which
is calculated by

n

αˆ =        xiI(Rˆi  ≥ r)/n                                                       (3)

i=1

Cohen et al. (2019) demonstrates that when significance level of Rˆ is small, the difference between
these two quantities is negligible. In practice, we plot approximate certified accuracy αˆ as a 
function
of radius r. From Eqn. (3), we know that αˆ is non-increasing w.r.t. r. And when r → ∞, αˆ → 0.

Hyperparameters.   In our paper,  we set all our hyperparameters following Cohen et al. (2019).
Specifically, we set significance level to 0.001.  n₀  =  100 in Monte Carlo simulation (used to get
bound for αˆ) and n  =  100, 000 in estimation part (used to estimate αˆ).  Moreover, we test three
parameters in CIFAR-10 dataset and ImageNet dataset (σ = 0.25, 0.50, 1.00).  Since (Cohen et al.,

2019) use Gaussian noise and we use Lapla√ce noise, they should have the same standard deviation

during comparison. This requires that σ =    2λ.

5.2    EXPERIMENTAL RESULTS

Results on ImageNet and CIFAR-10.  We applied random smoothing on CIFAR-10 (Krizhevsky
(2009)) and ImageNet (Deng et al. (2009)) respectively. On each data set, we trained several random
smoothing models with differential standard deviation σ for Laplace noise. In order to keep in line

with Cohen et al.’s method and make a comparis√on, we select σ  = 0.25, 0.50, 1.00 on CIFAR-10,

and ImageNet, corresponding parameter λ = σ/   2.

Figure 6 draws the certified accuracy achieved by smoothing with each sigma.  For the ImageNet
dataset, we only use the most basic training method (Type1 Training). For the CIFAR-10 data set, we
use two training methods (Type 1 and Type 2 Training). We can see that the smaller sigma performs
better when the radius is smaller.  As the noise gets bigger, the accuracy becomes lower, but the
robustness guarantee becomes higher. The dashed black line shows the empirical robust accuracy of
an undefended classifier from Cohen et al. (2019).


1.0

0.8

0.6

= 0.12

= 0.25

= 0.50

= 1.00

undefended

1.0

0.8

0.6

= 0.12

= 0.25

= 0.50

= 1.00

         undefended

1.0

0.8

0.6

= 0.25

= 0.50

= 1.00

         undefended


0.4

0.4

0.4


0.2

0.2

0.2


0.0

0                1                2                3                4                5

radius

(a) Cifar(Type1)

0.0

0                1                2                3                4                5

radius

(b) Cifar(Type2)

0.0

0           1           2           3           4           5           6           7

radius

(c) ImageNet

Figure 6:  Approximate certified accuracy on CIFAR-10 and ImageNet.

Comparison with baseline.

7


Under review as a conference paper at ICLR 2020

We will show our comparison results in the following. Based on Table. 1, we will test our method on
CIFAR-10 with the ResNet110 architecture as well as Type1 and Type2 training, and ImageNet with
ResNet50 architecture as well as Type1 training.  We will compare our results with (Cohen et al.,
2019) and (Lecuyer et al., 2019) under the same standard deviation σ. For base classifiers, ours and
Lecuyer et al.’s share the same base classifier with Laplace training noise, and Cohen et al.’s uses
the base classifier with Gaussian training noise.


1.0

0.8

0.6

Ours, type1
Ours, type2

Lecuyer et al, type1
Lecuyer et al, type2
Cohen et al

1.0

0.8

0.6

Ours, type1
Ours, type2

Lecuyer et al, type1
Lecuyer et al, type2
Cohen et al

1.0

0.8

0.6

Ours, type1
Ours, type2

Lecuyer et al, type1
Lecuyer et al, type2
Cohen et al


0.4

0.4

0.4


0.2

0.2

0.2


0.0

0                1                2                3                4                5

radius

(a) σ = 0.25

0.0

0                1                2                3                4                5

radius

(b) σ = 0.50

0.0

0                1                2                3                4                5

radius

(c) σ = 1.00

Figure 7:  Approximate certified accuracy attained by randomized smoothing on CIFAR-10.


1.0

0.8

Ours
Cohen et al

Lecuyer et al

1.0

0.8

Ours
Cohen et al

Lecuyer et al

1.0

0.8

Ours
Cohen et al

Lecuyer et al


0.6

0.6

0.6


0.4

0.4

0.4


0.2

0.2

0.2


0.0

0           1           2           3           4           5           6           7

radius

(a) σ = 0.25

0.0

0           1           2           3           4           5           6           7

radius

(b) σ = 0.50

0.0

0           1           2           3           4           5           6           7

radius

(c) σ = 1.00

Figure 8:  Approximate certified accuracy attained by randomized smoothing on ImageNet.

6    CONCLUSION

In this paper, we combine the inequality from differential privacy and the classic Neyman-Pearson
Lemma to resolve the challenging asymmetry of l₁ metric and the mixed discrete-continuous prop-
erty of the likelihood ratios under isotropic Laplace distributions.  In addition, by comparing the
high-dimensional case with a special edge case, we prove the tight l₁ robustness guarantee for 
binary
classification problems, and obtain the state-of-the-art certified accuracy in large scale 
experiments.

The establishment of l₁  certificate via Laplace distributions and the prior result of l₂  
certificate
via Gaussian distributions may be extended to a generic theorem for a general lp norm robustness
certificate via the associated realization of the generalized Gaussian distribution, where the 
afore-
mentioned results are special cases of the general scheme.  The introduction of the mixed random
variable  analysis  and  lp geometry  analysis  may  serve  as  a  valuable  extension  of  
existing  works
towards such general goal.

REFERENCES

Cem Anil, James Lucas, and Roger B Grosse.  Sorting out lipschitz function approximation.  arXiv:
Learning, 2018.

Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo`  Cesa-Bianchi, and
Roman Garnett (eds.).   Advances in Neural Information Processing Systems 31:  Annual Con-
ference on Neural Information Processing Systems 2018,  NeurIPS 2018,  3-8 December 2018,

8


Under review as a conference paper at ICLR 2020

Montre´al, Canada, 2018.  URL   http://papers.nips.cc/book/advances-in-neu
ral-information-processing-systems-31-2018.

Rudy Bunel, Ilker Turkaslan, Philip H. S. Torr, Pushmeet Kohli, and Pawan Kumar Mudigonda.  A
unified view of piecewise linear neural network verification.  In Bengio et al. (2018), pp. 4795–
4804.   URL    http://papers.nips.cc/paper/7728-a-unified-view-of-pie
cewise-linear-neural-network-verification.

Xiaoyu Cao and Neil Zhenqiang Gong.   Mitigating evasion attacks to deep neural networks via
region-based classification.  In Proceedings of the 33rd Annual Computer Security Applications
Conference,  Orlando,  FL, USA, December 4-8,  2017,  pp. 278–287. ACM, 2017.   ISBN 978-
1-4503-5345-8.   doi:    10.1145/3134600.3134606.   URL    https://doi.org/10.1145/

3134600.3134606.

Chih-Hong Cheng, Georg Nu¨hrenberg, and Harald Ruess.   Maximum resilience of artificial neu-
ral  networks.   In  Deepak  D’Souza  and  K.  Narayan  Kumar  (eds.),  Automated  Technology  for
Verification  and  Analysis  -  15th  International  Symposium,  ATVA  2017,  Pune,  India,  October
3-6,  2017,  Proceedings,  volume  10482  of  Lecture  Notes  in  Computer  Science,  pp.  251–268.

Springer,  2017.    ISBN  978-3-319-68166-5.    doi:     10.1007/978-3-319-68167-2   18.    URL

https://doi.org/10.1007/978-3-319-68167-2_18.

Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann N Dauphin, and Nicolas Usunier. Parse-
val networks: Improving robustness to adversarial examples. arXiv: Machine Learning, 2017.

Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certified adversarial robustness via randomized
smoothing. international conference on machine learning, 2019.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li.  Imagenet:  A large-scale
hierarchical image database.  In 2009 IEEE Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR 2009),  20-25 June 2009,  Miami,  Florida,  USA, pp. 248–255.
IEEE Computer Society, 2009. ISBN 978-1-4244-3992-8. doi:  10.1109/CVPRW.2009.5206848.

URL   https://doi.org/10.1109/CVPRW.2009.5206848.

Souradeep Dutta, Susmit Jha, Sriram Sankaranarayanan, and Ashish Tiwari.  Output range analysis
for deep neural networks. arXiv: Learning, 2017.

Chris Finlay, Aram-Alexandre Pooladian, and Adam M Oberman. The logbarrier adversarial attack:
making effective use of decision boundary information. arXiv preprint arXiv:1903.10396, 2019.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.  Explaining and harnessing adversarial
examples. international conference on learning representations, 2015.

Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J Cree.  Regularisation of neural net-
works by enforcing lipschitz continuity. arXiv: Machine Learning, 2018.

Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy A Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training verifiably robust models. arXiv: Learning, 2018.

Matthias Hein and Maksym Andriushchenko.  Formal guarantees on the robustness of a classifier
against adversarial manipulation. arXiv: Learning, 2017.

Alex Krizhevsky. Learning multiple layers of features from tiny images. In Technical report, 2009.

Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel J Hsu, and Suman Jana.  Certified
robustness  to  adversarial  examples  with  differential  privacy.   ieee  symposium  on  security 
 and
privacy, 2019.

Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi S. Jaakkola.  A stratified approach to robust-
ness for randomly smoothed classifiers.  In Advances in neural information processing systems,
2019.

Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin.  Second-order adversarial attack and
certifiable robustness. arXiv: Learning, 2018.

9


Under review as a conference paper at ICLR 2020

Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. Towards robust neural networks via
random self-ensemble. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss
(eds.), Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September
8-14, 2018, Proceedings, Part VII, volume 11211 of Lecture Notes in Computer Science, pp. 381–
397. Springer, 2018.  ISBN 978-3-030-01233-5.  doi:   10.1007/978-3-030-01234-2   23.  URL

https://doi.org/10.1007/978-3-030-01234-2_23.

Alessio Lomuscio and Lalit Maganti.   An approach to reachability analysis for feed-forward relu
neural networks. arXiv: Artificial Intelligence, 2017.

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. international conference on learning
representations, 2018.

Matthew  Mirman,  Timon  Gehr,  and  Martin  T.  Vechev.   Differentiable  abstract  interpretation 
 for
provably robust neural networks. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the
35th International Conference on Machine Learning, ICML 2018, Stockholmsma¨ssan, Stockholm,
Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 3575–
3583. PMLR, 2018. URL   http://proceedings.mlr.press/v80/mirman18b.htm
l.

Rafael Pinot, Laurent Meunier, Alexandre Araujo, Hisashi Kashima, Florian Yger, Ce´dric Gouy-
Pailler, and Jamal Atif.  Theoretical evidence for adversarial robustness through randomization:
the case of the exponential family.  CoRR, abs/1902.01148, 2019.  URL    http://arxiv.or
g/abs/1902.01148.

Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien
Bubeck.   Provably  robust  deep  learning  via  adversarially  trained  smoothed  classifiers.   
arXiv
preprint arXiv:1906.04584, 2019.

Gagandeep  Singh,  Timon  Gehr,  Matthew  Mirman,  Markus  Pu¨schel,  and  Martin  T.  Vechev.
Fast  and  effective  robustness  certification.      In  Bengio  et  al.  (2018),   pp.  
10825–10836.
URL     http://papers.nips.cc/paper/8278-fast-and-effective-robustn
ess-certification.

Christian Szegedy,  Wojciech Zaremba,  Ilya Sutskever,  Joan Bruna,  Dumitru Erhan,  Ian J Good-
fellow, and Rob Fergus.  Intriguing properties of neural networks.  international conference on
learning representations, 2014.

Vincent Tjeng, Kai Y Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. arXiv: Learning, 2017.

Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama.  Lipschitz-margin training: Scalable certifica-
tion of perturbation invariance for deep neural networks. In Bengio et al. (2018), pp. 6542–6551.
URL          http://papers.nips.cc/paper/7889-lipschitz-margin-trainin
g-scalable-certification-of-perturbation-invariance-for-deep-n
eural-networks.

Bao Wang, Binjie Yuan, Zuoqiang Shi, and Stanley J. Osher.  Enresnet:  Resnet ensemble via the
feynman-kac formalism.  CoRR, abs/1811.10745, 2018.  URL    http://arxiv.org/abs/
1811.10745.

Tsuiwei Weng, Huan Zhang, Hongge Chen, Zhao Song, Chojui Hsieh, Duane S Boning, Inderjit S
Dhillon, and Luca Daniel.   Towards fast computation of certified robustness for relu networks.
arXiv: Machine Learning, 2018.

Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel.  Efficient neural net-
work robustness certification with general activation functions. In Bengio et al. (2018), pp. 4944–
4953.  URL    http://papers.nips.cc/paper/7742-efficient-neural-netwo
rk-robustness-certification-with-general-activation-functions.

Stephan Zheng, Yang Song, Thomas Leung, and Ian J Goodfellow. Improving the robustness of deep
neural networks via stability training.  computer vision and pattern recognition, pp. 4480–4488,
2016.

10


Under review as a conference paper at ICLR 2020

A    PROOF  OF  LEMMA  1

In this section, we will prove that Neyman-Pearson Lemma holds with mixed random variable.

WLOG, x = 0, X         (λ) and Y          (λ) + δ. We will firstly introduce Neyman-Pearson Lemma,
which plays an important role in our proof.

Lemma 3.1 (restated).Let X ∼ L (λ) and Y  ∼ L.(λ)+δ. Let h : Rᵈ → {0, 1} be aΣny deterministic

1. If S =   z ∈ Rᵈ : ǁz − δǁ₁ − ǁzǁ₁ > β   ∪ S′, and P(h(X) = 1) ≥ P(X ∈ S) then P(h(Y ) =

1) ≥ P(Y  ∈ S)

2. If S =   z ∈ Rᵈ : ǁz − δǁ₁ − ǁzǁ₁ < β   ∪ S′, and P(h(X) = 1) ≤ P(X ∈ S), then P(h(Y ) =

1) ≤ P(Y  ∈ S)

Proof of Lemma 3.1    First, notice that P(X       S) can be regarded as a mixed random variable.
We want to prove that as long as we can choose a S′ that satisfies P(X      S)      P(h(X)  =  1),
Neyman-Pearson Lemma can always hold.

Let’s first see what happens in the proof of Neyman-Pearson Lemma.   Notice that X  and Y  are
continuous variables, but X      S and Y       S can be regarded as mixed continuous-discrete event.
Then we can choose a reasonable S′ for X and Y .  We will prove case 1 and the other one can be
proved with similar method.

P(h(Y ) = 1) − P(Y  ∈ S)

=       h(1|z)µY (z)dz − ∫  µY(z)dz


=[∫

= ∫

h(1|z)µY (z)dz + ∫

h(1|z)µY (z)dz − ∫

h(1|z)µY(z)dz] − [∫

h(0|z)µY(z)dz

h(1|z)µY(z)dz + ∫

h(0|z)µY(z)dz]


Sc

t(       h(1 z)µX (z)dz

Sc                                                       S

h(0|z)µX(z)dz)

(4)


=t([

Sc

h(1|z)µX (z)dz + ∫

h(1|z)µX(z)dz] − [∫

h(0 z)µX(z)dz +

Sc

h(1|z)µX(z)dz])


=t(P(h(X) = 1) − P(X ∈ S))

≥0

The first inequality holds due to the construction of mixed definition S.  If z  ∈ S,  µY (z)

≥ t.  If

z ∈ Sᶜ,  µY (z)  ≤ t. Compared with continuous set, the only difference appears in the equal sign.

It should be noted that P(X      S) and P(Y       S) should keep consistent, which means that they
should have the same S′. In this derivation, we can find that P (X     S) and P (Y      S) use the 
same
set S′ in Eqn. (4).

Next, we will plug in the condition that X and Y are isotropic Laplaces.

Then we just need to prove that


.z ∈ Rᵈ :  µY (z)  ≤ tΣ ⇐⇒ .z ∈ Rᵈ : ǁz − δǁ

− ǁzǁ

≥ βΣ

When X and Y are isotropic Laplaces, the likelihood ratio turns out to be:


µY (z)

µX (z)

exp(− 1 ǁz − δǁ₁)
exp(− 1 ǁzǁ₁)

1

= exp(− λ (ǁz − δǁ₁ − ǁzǁ₁))

11


Under review as a conference paper at ICLR 2020

By choosing β = −λ log(t), we can derive that

µY (z)


ǁz − δǁ₁ − ǁzǁ₁ ≥ β ⇐⇒ µ

(z)  ≤ t

µY (z)


ǁz − δǁ₁ − ǁzǁ₁ ≤ β ⇐⇒ µ

B    PROOF  OF  THEOREM  1

(z)  ≥ t

Theorem 1(restated) Let f  : Rᵈ → Y  be deterministic or random function, and let s ∼ L(λ). Let

g(x) = arg maxc Ps(f (x + s) = c). Suppose PA, PB ∈ [0, 1] are such that

P (f (x + s) = cA) ≥ PA ≥ PB ≥ max P(f (x + s) = c)


Then g(x + δ) = g(x), ∀ǁδǁ₁ ≤ R, where

c   cA


R = max . λ log(P   /P

), −λ log(1 − P    + P

)Σ                          (5)

Proof of Theorem 1    Denote T (x) =   x     δ  ₁        x  ₁.  Use Triangle Inequality we can 
derive a
bound for T (x):

− ǁδǁ₁ ≤ T (x) ≤ ǁδǁ₁                                                         (6)

Pick β₁, β₂ such that there exists A′ ⊆ {z : T (z) = β₁}, B′ ⊆ {z : T (z) = β₂}, and

P(X ∈ {z : T (z) > β₁} ∪ A′) = PA ≤ P(f (X) = cA))

P(X ∈ {z : T (z) < β₂} ∪ B′) = PB ≥ P(f (X) = cB)


Define

Thus, apply Lemma 3.1, we have

A := {z : T (z) > β₁} ∪ A′

B := {z : T (z) < β₂} ∪ B′

P(Y  ∈ A) ≤ P(f (Y ) = cA)

P(Y  ∈ B) ≥ P(f (Y ) = cB)

(7)

Then consider P(Y  ∈ A) and P(Y  ∈ B)


P(Y  ∈ A) = ∫

= ∫

[2λ]−ᵈ exp(    ǁx − δǁ1 )dx

λ

[2λ]−ᵈ exp(−ǁxǁ1 ) exp(− T (x))dx


≥ exp(−ǁδǁ1 ) ∫

[2λ]−ᵈ exp(    ǁxǁ1 )dx

λ

= exp(−ǁδǁ1 )P   

The inequality is derived by Eqn.( 6). Similarly, we can get


P(Y  ∈ B) = ∫

= ∫

[2λ]−ᵈ exp(    ǁx − δǁ1 )dx

λ

[2λ]−ᵈ exp(−ǁxǁ1 ) exp(− T (x))dx


≤ exp( ǁδǁ1 ) ∫

= exp( ǁδǁ1 )P

[2λ]−ᵈ exp(    ǁxǁ1 )dx

λ

λ       B

12


Under review as a conference paper at ICLR 2020

First, we would like to show that robustness can be guaranteed when R ≤ λ log(PA/PB).

If ǁδǁ₁ ≤ λ log(PA/PB), by Eqn. (7, 8, 9), we have

P(f (Y ) = cA) ≥ P(Y  ∈ A) ≥ P(Y  ∈ B) ≥ P(f (Y ) = cB)

Then, we would like to show that robustness can be guaranteed when R ≤ −λ log(1 − PA + PB).

From Eqn. (9), we know that P(Y   ∈ B)  ≤ exp( ǁδǁ1 )PB.  Besides, by applying Eqn. (9) in set

Aᶜ, we can get that P(Y   ∈ A)  ≥ 1 − exp( ǁδǁ1 )(1 − PA).  So we can calculate that if ǁδǁ₁  ≤

−λ log(1 − PA + PB), we have

P(f (Y ) = cA) ≥ P(Y  ∈ A) ≥ P(Y  ∈ B) ≥ P(f (Y ) = cB)

Moreover, by simple algebraic operation, we can derive that−λ log(1− PA +PB) ≥ λ log(PA/PB)

                 √                                                            √                     
         2


requires 1−2PA(1−PA)−    1−4PA(1−PA)  ≤ P

The proof for Theorem 1 is finished.

1−2PA(1−PA)+     1−4PA(1−PA)

2PA

C    PROOF  OF  THEOREM  2 AND  THEOREM  3

Theorem 2(restated) (binary case) Let f  : Rᵈ → Y  be deterministic or random function, and let
s ∼ L(λ). Let g(x) = arg maxc Ps(f (x + s) = c). Suppose there are only two classes cA and cB,
and PA ∈ [ ¹ , 1] s.t.

P (f (x + s) = cA) ≥ PA

Then g(x + δ) = g(x), ∀ǁδǁ₁ ≤ R, for

R = −λ log[2(1 − PA)]                                                     (10)

Proof of Theorem 2:

It is similar to the proof of Theorem 1. Pick β₃ such that there exists B′ ⊆ {z : T (z) = β₃}, and

P(X ∈ {z : T (z) < β₃} ∪ B′) = PB = P(f (X) = cB)


Define

S := {z : T (z) < β₃} ∪ B′

So we also have P(X /∈ S) = PA = P(f (X) = cA). Plug into Lemma 3.1, we can get

P(Y  /∈ S) ≤ P(f (Y ) = cA)

P(Y  ∈ S) ≥ P(f (Y ) = cB)

Using a similar method as Eqn. (9), we can get that

P(Y  ∈ S) ≤ exp( ǁδǁ1 )P

λ       B

Since we have

PB = P(f (X) = cB) = 1 − PA ≤ 1 − PA

Thus, if ǁδǁ₁ ≤ R = −λ log[2(1 − PA)], it holds that

P(Y  ∈ S) ≤ exp( ǁδǁ1 )P                

≤ exp( −λ log[2(1 − PA)])(1 − P   )

λ                           A

1

=

2

13


Under review as a conference paper at ICLR 2020

That is to say, P(f (Y ) = cA) ≥ P(Y  /∈ S) ≥ 1  ≥ P(Y  ∈ S) ≥ P(f (Y ) = cB).

The proof for Theorem 2 is finished.

Theorem  3(restated)  (tight  bound  in  binary  case)  In  the  same  setting  as  Theorem  2,  
assume
PA + PB  ≤ 1 and PA ≥  1 .  ∀R′ >  −λ log[2(1 − PA)], ∃ base classifier f ∗ and perturbation δ∗
with g∗(x) = arg maxc Ps(f ∗(x + s) = c) and ǁδǁ₁ = R′, s.t. g∗(x) /= g∗(x + δ∗).

Proof of Theorem 3:    Here, we first set δ  =  (ǁδǁ₁, 0, . . . , 0).  For simplification, we 
denote δ  =

ǁδǁ₁.And define

A := .z : |z − δ| − |z| ≥ max{δ + 2λ log Σ2 .1 − PAΣΣ , −δ}Σ

Then, we can calculate that

P(X ∈ A) = Pₓ(|x − δ| − |x| ≥ max{δ + 2λ log[2(1 − PA)], −δ})


∫ −λ log[2(1−PA)]   1 

|x|


=                              2λ exp (− λ  )dx

−∞

(11)


∞                    

= 1 −

 1 

exp (

x

)dx


= PA

−λ log[2(1−PA)]  2λ          λ

where x ∼  1   exp (−|x| ), δ = ǁδǁ₁ .  Notice that if δ + 2λ log[2(1 − PA)] ≤ −δ, we will get the

integral equation by choosing S′. With Eqn. (11), we have

P(X ∈ A) = PA ≤ P(f (X) = cA)                                            (12)

Thus, plug Eqn. (12) into the results of Lem. 3.1, we have

P(Y  ∈ A) ≤ P(f (Y ) = cA)                                                  (13)

Also, since Y  = X + δ, it can be derived that


∫ −λ log[2(1−PA)]−δ   1 

|x|

Here we use the consistency of X     A and Y      A. Since Y  can be regarded as an offset of X, the
integral limit should be translated into the same length.  So, if   δ  ₁  = δ         λ log[2(1     
PA)], by
Eqn. (7) and Eqn. (14), we have

1

P(f (Y ) = cA) ≥ P(Y  ∈ A) ≥ 2

This m  eans that the results we get in binary case is a tight bound, and the worst-case δ appears 
when
δ = (δ, 0, . . . , 0). Furthermore, if we slightly enlarge δ, there would be a case that the 
robustness is
destroyed.

The proof for Theorem 3 is finished.

D    WHY  LAPLACE  NOISE  INSTEAD  OF  GAUSSIAN

In this section, we theoretically analyze the certification capabilities of Gaussian and Laplace 
noises.
We will show that, given the same base classifier f  the parameter of Laplace distributions λ is 
less
sensitive than the parameter of Gaussian distributions σ. Given a base classifier f , where

f (x) =    cA    |x| ≤ 1

cB    o.w.

14


Under review as a conference paper at ICLR 2020

and two random smoothing functions

g₁(x) = arg max P(f (x + ϵ) = c), ϵ ∼ L(0, λ),

g₂(x) = arg max P(f (x + ϵ) = c), ϵ ∼ N (0, σ²),

we aim to prove that Laplace noises will better protect the original prediction than Gaussian 
noises.

Formally, we compare their Rectified Optional Parameter Space (ROPS), defi√ned as Λ = {√2λ :

g₁(x; λ) =√f (x)} and Σ = {σ : g₂(x; σ) = f (x)}. Note that the rectified term    2 is due to the 
fact

that σ  =     2λ yield the same variance.  Essentially, ROPS indicates the feasible region where 
the

smoothing distribution does not negatively impact the base classifier, thus measuring the 
sensitivity
of the smoothing distribution (the larger the better).

First, we would like to compare its prediction on a given point (x, f (x)) = (0, cA). We have

1                                                  1        1                     1

g₁(0) = cA ⇐⇒ P(f (0 + ϵ) = cA) ≥ 2  ⇐⇒ P(|ϵ| ≤ 1) = 1 − exp(− λ ) ≥ 2  ⇐⇒ λ ≤ log 2 ,

1                                       1               1                         1

g₂(0) = cA ⇐⇒ P(f (0 + ϵ) = cA) ≥ 2  ⇐⇒ P(|ϵ| ≤ 1) = 2Φ( σ ) − 1 ≥ 2  ⇐⇒ σ ≤ Φ−1(3/4) .


Since  √2

>        ¹       , Laplace noises have a larger ROPS than Gaussian noises at the point x = 0.

The analysis can be further extended in two cases.

First, if we have x  =  0, what is the corresponding ROPS that leads to the desired result (g(x)  =

f (x))? We show in Fig. 10 that we will have a larger ROPS under Laplace noises.

Second, if we have a fixed x but fixed a desired certified radius, what is the corresponding ROPS?
We show in Fig. 11 that Laplace noises again have a larger ROPS.

We empirically validate this finding with ResNet110 on CIFAR-10. The resulting smoothed model
has 24.8% clean accuracy under a Laplace noise, and 23.7% clean accuracy under a Gaussian noise
(with the same variance as the Laplace noise).  Here the accuracy is computed with respect to pre-
dictions  of  the  base  classifier  instead  of  the  labels  (to  illustrate  how  the  smoothing 
 impacts  the
predictions).


sup ROPS implies     /     = [0, sup ROPS)

2.5

Laplace

sup ROPS implies     /     = [0, sup ROPS)

2

Laplace


2

1.5

1

0.5

Gaussian

1.5

1

0.5

Gaussian


0

0            0.2          0.4          0.6          0.8            1

Location of x

0

0            0.2          0.4          0.6          0.8            1

Certified Robustness Radius


Figure 10:   The ROPS under various x.  Here
Σ   =   [0, sup  ROPS),  and  similarly  for  Λ.
Laplace noises are less sensitive than Gaussian
noises in terms of ROPS.

Figure 11:   The ROPS under various certified
robustness  radii  with  a  fixed  x  =  0.3  (other
x    yields  similar  results).   Laplace  noises  are
less sensitive than Gaussian noises in terms of
ROPS.

15

