Under review as a conference paper at ICLR 2020
Step Size Optimization
Anonymous authors
Paper under double-blind review
Ab stract
This paper proposes a new approach for step size adaptation in gradient methods.
The proposed method called step size optimization (SSO) formulates the step size
adaptation as an optimization problem which minimizes the loss function with re-
spect to the step size for the given model parameters and gradients. Then, the step
size is optimized based on alternating direction method of multipliers (ADMM).
SSO does not require the second-order information or any probabilistic models
for adapting the step size, so it is efficient and easy to implement. Furthermore,
we also introduce stochastic SSO for stochastic learning environments. In the ex-
periments, we integrated SSO to vanilla SGD and Adam, and they outperformed
state-of-the-art adaptive gradient methods including RMSProp, Adam, L4-Adam,
and AdaBound on extensive benchmark datasets.
1	Introduction
First-order gradient methods (simply gradient methods) have been widely used to fit model param-
eters in machine learning and data mining, such as training deep neural networks. In the gradient
methods, step size (or learning rate) is one of the most important hyperparameters that determines
the overall optimization performance. For this reason, step size adaptation has been extensively
studied from various perspectives such as second-order information (Byrd et al., 2016; Schaul et al.,
2013), Bayesian approach (Mahsereci & Henning, 2015), learning to learn paradigm (Andrychow-
icz et al., 2016), and reinforcement learning (Li & Malik, 2017). However, they are hardly used in
practice due to lack of solid empirical evidence for the step size adaptation performance, hard im-
plementation, or huge computation. For these reasons, some heuristically-motivated methods such
as AdaGrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton, 2012), and Adam (Kingma & Ba,
2015) are mainly used in practice to solve the large-scale optimization problems such as training
deep neural networks.
Recently, two impressive methods, called L4 (Rolinek & Martius, 2018) and AdaBdound (Luo et al.,
2019), were proposed to efficiently adapt the step size in training of models, and showed some
improvement over existing methods without huge computation. However, performance comparisons
to them were conducted only on relatively simple datasets such as MNIST and CIFAR-10, even
though L4 has several newly-introduced hyperparameters, and AdaBound needs manually-desgined
bound functions. Moreover, L4 still requires about 30% more execution time, and AdaBound lacks
the time complexity analysis or empirical results on training performance against actual execution
time.
This paper proposes a new optimization-based approach for the step size adaptation, called step size
optimization (SSO). In SSO, the step size adaptation is formulated as a sub-optimization problem of
the gradient methods. Specifically, the step size is adapted to minimize a linearized loss function for
the current model parameter values and gradient. The motivation of SSO and the justification for the
performance improvement by SSO is clear because it directly optimizes the step size to minimize
the loss function. We also present a simple and efficient algorithm to solve this step size optimiza-
tion problem based on the alternating direction method of multipliers (ADMM) (Gabay & Mercier,
1976). Furthermore, we provide a practical implementation of SSO on the loss function with L2
regularization (Krogh & Hertz, 1992) and stochastic SSO for the stochastic learning environments.
SSO does not require the second-order information (Byrd et al., 2016; Schaul et al., 2013) and any
probabilistic models (Mahsereci & Henning, 2015) to adapt the step size, so it is efficient and easy
to implement. We analytically and empirically show that the additional time complexity of SSO in
1
Under review as a conference paper at ICLR 2020
the gradient methods is negligible in the training of the model. To validate the practical usefulness
of SSO, we made two gradient methods, SSO-SGD and SSO-Adam, by integrating SSO to vanilla
SGD and Adam. In the experiments, we compared the training performance of SSO-SGD and SSO-
Adam with two state-of-the-art step size adaptation methods (L4 and AdaBdound) as well as the
most commonly used gradient methods (RMSProp and Adam) on extensive benchmark datasets.
2	Step Size Optimization
2.1	Problem formulation
The goal of step size optimization (SSO) is to find the optimal step size that minimizes the loss
function with respect to the step size η as:
η* = arg min J(θ 一 ηv) + Ω(θ 一 ηv),	(1)
η
where J is the loss function; Ω is a regularization term; θ is the model parameter; and V is the
gradient for updating θ. Note that v is an optimizer-dependent gradient such as the moving average
of the gradients in Adam. As gradient methods update the model by moving to the opposite direction
of the gradient (θ J θ 一 ηv), the loss function J(θ) and the regularization term Ω(θ) can be
expressed as J(θ 一 ηv) and Ω(θ 一 ηv), respectively. In real-world problems, however, directly
solving the optimization problem in Eq. (1) is infeasible due to the severe nonlinearity of J. To
handle this difficulty, first, we linearize J around θ as:
J(θ - ηv) ≈ J(θ) + (Ve J)T(θ - ηv 一 θ) = J(θ) - ηgTv,	⑵
where g = Ve J is the true gradient. Note that V is the same as g in vanilla gradient method.
However, in order for the linearization of Eq. (2) to be valid, η should be sufficiently small. To this
end, we introduce an inequality constraint for the upper bound ofη. Thus, the optimization problem
of SSO is given by:
η* = arg min J(θ) 一 ηgTV + Ω(θ - ηv),	(3)
0≤η≤
where is a positive hyperparameter that defines the upper bound of the step size. That is, SSO
adapts the step size by solving the constrained optimization problem in Eq. (3).
2.2	Augmented Lagrangian for Step Size Optimization Problem
The augmented Lagrangian is a widely used optimization technique to handle a constrained opti-
mization problem by transforming it into an unconstrained problem. The objective function in the
augmented Lagrangian is defined based on equality constraints. For this reason, we need to trans-
form the optimization problem with the inequality constraints in Eq. (3) to the problem with the
equality constraints by introducing slack variables s1 and s2 as:
η* = arg min J(θ) 一 ηgTV + Ω(θ - ηv)	(4)
η
s. t. η 一 s1 = 0, 一 η 一 s2 = 0
s1 ≥ 0, s2 ≥ 0.
Finally, the augmented Lagrangian for the problem of Eq. (4) is given by:
Lμ(η, λι, λ2, Si, S2)= J(θ) ― ηgTV + Ω(θ ― ηv) ― λι(η ― si) 一 λz(e ― η ― S2)
+ 2(η - SI)2 + 2(E - η - s2)2,	(5)
where λι ≥ 0 and λ2 ≥ 0 are dual variables, and μ is a balancing parameter between the objective
function and the penalty term for the equality constraints. In general, μ is simply set to be gradually
increased in optimization process to guarantee the feasibility for the equality constraints (Gabay &
Mercier, 1976; Ouyan et al., 2013).
2
Under review as a conference paper at ICLR 2020
2.3 Optimization
In this section, we describe the optimization algorithm to find the optimal step size that minimizes
the augmented Lagrangian in Eq. (5) using ADMM. In mathematical optimization and machine
learning, ADMM has been widely used to solve the optimization problem containing different types
of primal variables x, z with the equality constraints such as:
x, z = arg min f(x) + g(z)	(6)
x,z
s. t. Ax + Bz = c,
where A and B are coefficient matrices of the equality constraints, and c is a constant. ADMM
iteratively finds the optimal variables by minimizing the augmented Lagrangian for the problem in
Eq. (6), denoted by Lμ (x, z, λ), because directly solving the problem can be nontrivial. Specifically,
ADMM (Algorithm 1) optimizes primal and dual variables in a one-sweep Gauss-Seidel manner:
Lμ(x, z, λ) is minimized with respect to the primal variables X and Z alternatively for the fixed dual
variable λ. Then, Lμ(x, z, λ) is minimized over the dual variable λ for the fixed primal variables X
and z.
Algorithm 1: ADMM
Output: Optimized primal variables X and z
ι t = 0, λ = 0, μ =1
2	repeat
3	χ(t+1) — argminX Lμ(x, Ztt, λ⑶)
4	z(t+I) - argmi□z Lμ(x(t+1), z, λ(t))
5	λ(t+1) - λ(t) 一 μ(Ax(t+1) + Bz(t+1) 一 C)
6	Increase μ
7	t — t + 1
8	until X, z, λ converged;
The step size optimization problem of Eq. (4) is an equality constrained problem, and also has
two kinds of primal variables, the step size η and the slack variables s1, s2. That is, for the primal
variables η and s1, s2, the step size optimization problem of Eq. (4) has the same structure as
the problem in Eq. (6), where f (η) = J(θ) 一 ηgTV + Ω(θ 一 ηv), g(sι, s2) = 0, and the equality
constraints are η 一 s ι = 0 and E - η 一 s2 =0. Thus, the primal variables of the step size optimization
problem η, s1, s2, and the dual variables λ1, λ2 can be optimized by ADMM as:
η(t+1) - argminL”(η, sit), s2t), λIt),λ2t)),	(7)
η
sit+1) - max ∣0, arg min L”(η(t+1), si, s2t), λljt), λ2t))} ,	(8)
s2t+1) - max ∣0, arg min Lμ(η(t+i), sit+1), s2, λit), λ2t)) j- ,	(9)
λ(it+i)
= max n0, 入『一小"+1)一£1+))},	(10)
λ2t+1) = max {0, Wt- μ(c - η(t+i) - s2t+1))} .	(11)
Note that the max operation with zero is applied to Eq. (8)〜(11) for satisfying the nonnegative
constrains of the slack and the dual variables.
Algorithm 2 shows the overall process of the gradient descent method with SSO to optimize θ. In
line 3, the true gradient gand the adaptive gradient v are computed, and the augmented Lagrangian
Lμ(η, si, s2, λι, λ2) for optimizing the step size is determined based on the current gradients. Then,
the step size is optimized for currently given model parameters and gradients by iteratively mini-
mizing Lμ(η, si, s2, λι, λ2) in line 5〜13. Finally, θ is updated with the optimized step size and the
adaptive gradient in line 14.
3
Under review as a conference paper at ICLR 2020
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Algorithm 2: Gradient descent method with SSO
Input : Upper bound of the step size: ∈ (0, 1]
Output: θ
k=0
repeat
Compute g = Vθ J and V to determine Lμ(η, si, s2, λι, λ2)
t = 0, si = 0, s2 = 0, λι = 0, λ2 = 0, μ = 1
repeat
η(t+1) - argmi□η Lμ(η, sf), s2t), λf), λ2t))
sit+1) - max {0, argmi□sι Lμ(η(t+i), si, s2t), λf), λ2t))}
s2t+1) - max {0, argm% Lμ(η(t+i), s1t+1), s2,λf), λ2t))}
λιt+i) - max {0, λ1t) — μ(η(t+i) — slt+1))}
λ2t+i) J max {0, λ2t) — μ(e — η(t+i) — s2t+1))}
Increment μ
tJt+1
until η converged;
θ(k+i) J θ(k) — ηv
kJk+1
until θ converged;
2.4	Upper B ound Decay
In this section, we additionally introduce the upper bound decay for SSO. Since the shape of the
augmented Lagrangian for SSO can be changed by the regularization term Ω, the convergence ProP-
erty of the gradient methods with SSO may be different for each different regularization term. To
overcome this Problem, we devise the uPPer bound decay method and integrate it into SSO. One
Possible imPlementation of the uPPer bound decay is to use the exPonential decay as follows.
(k+i) J γ(k),	(12)
where γ ∈ (0, 1) is decay factor. That is, is exPonentially decreased over the training.
It is similar to the steP size decay, but there is a big difference. The uPPer bound decay indirectly
reduces the steP size by decreasing the uPPer bound of the steP size instead of reducing it directly.
That is, SSO with the uPPer bound decay automatically Provides an oPtimal steP size that will
be gradually reduced over the training. Furthermore, SSO with the uPPer bound decay always
guarantees that the steP size converges to zero regardless of the shaPe of the augmented Lagrangian
for the valid decay factors such as γ ∈ (0, 1) in Eq (12). Thus, the uPPer bound decay is more
flexible than the steP size decay and can be regarded as a generalized method of the steP size decay.
One main advantage of SSO over the existing methods is that it can exPloit such uPPer bound decay.
In SSO with the uPPer bound decay, the initial uPPer bound (0) is a hyPerParameter.
2.5	PRACTICAL IMPLEMENTATION: SSO WITH L2 REGULARIZATION
In this section, we derive SSO with the L2 regularization that is the most widely used regularization
technique and also Provide a Practical imPlementation of the gradient method with SSO.
2.5.1	Update rule
With the L2 regularization term, the augmented Lagrangian of the steP size oPtimization Problem is
given by:
β
Lμ(η, λ1,λ2,s1, s2) = J(θ) ― ηgτV + 2llθ ― nv||2 ― λι(η ― SI) 一 λ2(e ― η ― s2)
+ 2(η — SI)2 + 2(e — η — s2)2,	(13)
4
Under review as a conference paper at ICLR 2020
where β is a positive hyperparameter of the L2 regularization for balancing the loss function and the
regularization term. By applying ADMM in Algorithm 1, we can optimize the step size using the
following update rules:
η(t+1)
gτV + βVTθ + λ1t) - λ2t) + μ(s1t) - s2t) + E)
βvτV + 2μ
(14)
(15)
(16)
Note that the update rule for the dual variables λ1 and λ2 are the same as Eq. (10) and (11) because
the update rules of the dual variables are independent of the loss function and the regularization term
in SSO. More precisely, the update rules of the dual variables depend only on the equality constraints
for the step size.
2.5.2 Convergence
If the optimal step size exists within an range [0, E), the slack variables converge as s1 → η and
s2 → E - η When ADMM in SSO is sufficiently iterated (μ → ∞). Thus, the step size η converges
to some value as:
η(t+1) →
gτv + βvτθ + λ1t) - λ2t) + 2μη(t)
βvτv + 2μ
(17)
≈货=产
In contrast, if the optimal step size exists over the upper bound, the step size may converge near the
upper bound (η ≈ E) in ADMM. Thus, the slack variables converge as s1 → E and s2 → 0, and the
step size consequently converge to the upper bound as folloWs.
η(t+1) →
gτv + βvτθ + λ1t) — λt + 2μE 〜2μE
βvτv + 2μ	〜2μ	J
(18)
Thus, the step size alWays converges in ADMM of SSO.
Unfortunately, the second case (η → E) is not the desired result in Which the model is sufficiently
trained because a relatively small step size is required in this situation to make the gradient methods
converge. HoWever, it is not a problem in SSO With the upper bound decay because the upper bound
E must be reduced by the decay method over the training.
3 Stochastic SSO
In this section, We describe SSO for the stochastic learning environments and also provide stochas-
tic SSO With L2 regularization. The optimization problem in the stochastic environments can be
formulated on a mini-batch With respect to the step size as folloWs.
1N
η(k+1) = arg min — T Ji(θ(k - ηv(k)) + Ω(θ(k) - ηv(k)),	(19)
0≤η≤ Ni=1
Where Ji is the loss function for the ith sample in the mini-batch. This problem can be reWritten as
an optimization problem With a conservative penalty term as (Ouyan et al., 2013; Li et al., 2014):
η(k+1) =argmin-1 XX Ji(e(k) - ηv(W+Ω(θ^ - nv(k))+ llθ(k)-θ(k+1)112 .	(20)
0≤η≤ N i=1	η
Note that the conservative penalty term is introduced to prevent undesired large change in the model
parameters as mini-batch changes. The conservative penalty term can be reWritten With respect to
the step size as:
M(k)- e(k+1)ll2 = ||e(k)-(e(k)- nv(k))||2 = nv(k)T v(k).
(21)
η
η
5
Under review as a conference paper at ICLR 2020
Thus, we can derive the optimization algorithm for η(k+1) in stochastic environments by applying
the linearization and ADMM to the problem in Eq. (20) as described in Section 2.1 〜2.3.
In addition, we provide the update rule of stochastic SSO with L2 regularization for the practical
implementation of it. Similar to the update rule of the deterministic SSO with L2 regularization in
Section 2.5.1, the update rule for η of stochastic SSO with L2 regularization is given by:
(k+i) = N PN=I g(k)Tv(k) - VKkTv(k) + βvkkτe(k) + λ1t) - λ2t) + μ(sIt) - s2t) + e)
βv(k)τ v(k) + 2μ
(22)
Note that the update rule of the slack variables are the same as Eq. (15) and (16) because they
depend only on the constraints of the problem.
4	Time Complexity Analysis
Due to the sub-optimization process for the step size adaptation of SSO, a gradient method with
SSO inevitably requires additional complexity. In this section, we analyze the time complexity of
the gradient method with SSO and show that the additional time complexity of the gradient method
with SSO is almost the same as the vanilla gradient method in large-scale optimization problems
such as training deep neural networks. The empirical time complexity analysis over the actual
execution time will be conducted in the experiment section.
The time complexity ofa gradient method with SSO is O(Tψ+Td+TI), where T is the number of
epochs in the gradient method; ψ is the total computation for model updates such as the forward and
the backward steps in neural networks; d is the number of model parameters; and I is the number
of iterations for updating η (line 5〜13 in Algorithm 2). The additional time complexity from SSO
is O(Td + TI). The time complexity O(T d) comes from the dot product operations to compute
gTv, vTθ, and vTv. Another time complexity O(TI) comes from the iterative optimization process
of ADMM for adapting η. Practically, however, O(Tψ + Td + TI) ≈ O(T ψ) because the dot
product operation can be accelerated significantly by GPU, and O(Td+TI) is negligible compared
to O(T ψ) when we use a sophisticated model for high representation ability, such as a deep neural
network. Thus, in real-world applications, the time complexity of the gradient method with SSO is
approximately the same as O(T ψ), which is the time complexity of the vanilla gradient method. In
the experiment section, this time complexity analysis for SSO will be also demonstrated empirically
on several real-world datasets.
5	Experiments
In the experiments, we validated the effectiveness of SSO in gradient-based optimization. To this
end, We generated two gradient methods with stochastic SSO using the upper bound decay 一 (1)
vanilla SGD with SSO (SSO-SGD) and (2) Adam with SSO (SSO-Adam), and then their optimiza-
tion performance was compared with the state-of-the-art gradient methods: 1) RMSProp; 2) Adam;
3) L4-Adam; 4) AdaBound. We specified the optimization problem as training deep neural net-
works because it is the most appealing and challenging problem using gradient methods. To train
deep neural networks, cross-entropy loss with L2 regularization is used as a loss function.
We conducted the experiments on four well-known benchmark datasets: MNIST, SVHN, CIFAR-
10, and CIFAR-100 datasets. For MNIST dataset, convolutional neural network (CNN) was used.
For CIFAR and SVHN datasets, ResNet (He et al., 2016) was used. For all datasets, we measured
the training loss and the highest test accuracy during the training. We reported the mean and the
standard deviation of the highest accuracies by repeating the training several times. Specifically,
we repeated the training 10 times for MNIST dataset and 5 times for the other datasets. Table 1
summarizes the experiment results, and detailed explanations of the results for each dataset will be
provided in the following sections.
For each gradient method, we selected the best initial learning rate using a grid search in a set of
{0.1, 0.05, 0.01, 0.005, 0.001} for all datasets. For the other hyperparameters of Adam, such as the
exponential decay rate of Adam, we followed the settings of the original paper of Adam (Kingma &
Ba, 2015). To set the additional hyperpameters ofL4-Adam and the bound functions of AdaBound,
6
Under review as a conference paper at ICLR 2020
Table 1: Test accuracy in percent of the neural networks trained by each gradient method. The
highest test accuracy measured during the training is reported.
Gradient method	MNIST	SVHN	CIFAR-10	CIFAR-100
RMSProp	98.82 ± 0.04	94.51 ± 0.14	89.13 ± 0.17	64.61 ± 0.09
Adam	98.89 ± 0.04	94.96 ± 0.13	89.95 ± 0.34	67.35 ± 0.18
L4-Adam	99.31 ± 0.05	95.68 ± 0.09	90.17 ± 0.25	66.79 ± 0.81
AdaBound	99.28 ± 0.03	95.48 ± 0.12	91.46 ± 0.21	69.71 ± 0.16
SSO-SGD	99.29 ± 0.05	96.41 ± 0.08	94.43 ± 0.15	74.96 ± 0.36
SSO-Adam	99.33 ± 0.04	95.75 ± 0.06	92.53 ± 0.12	70.82 ± 0.41
we used the recommended setting in their original papers (Rolinek & Martius, 2018; Luo et al.,
2019). For MNIST dataset, the initial upper bound of SSO, (0), was set to 0.5. For the other
datasets, (0) was fixed to 1. The decay factor in the upper bound decay, γ, was fixed to 0.95 for
all datasets. The number of iterations for optimizing the step size in SSO was fixed to 20 (I = 20).
We selected the best regularization coefficient (β) using a grid search for each gradient method on
for all datasets. The batch size was fixed to 128 for all datasets. For MNIST dataset, we exploited
a commonly used architecture of CNN with two convolution layer and one fully-connected output
layer. For SVHN and CIFAR datasets, we used the ResNet with three residual blocks and one
fully-connected output layer (ResNet-18). All experiments were conducted on NVIDIA GeForce
RTX 2080 Ti. We used PyTorch to implement SSO and the author’s source code for L4-Adam1 and
AdaBound2. The source code of SSO and the experiment scripts are available at GitHubURL (open
after the review).
5.1	Digit Recognition
MNIST dataset is a widely used benchmark dataset for digit recognition. It contains 60,000 training
instances and 10,000 test instances of 28×28×1 size from 10 classes. As shown in Fig. 1-(a), L4-
Adam, AdaBound, SSO-SGD, and SSO-Adam rapidly reduced the training loss to zero on MNIST
dataset. Although SSO-Adam showed the highest test accuracy, other gradient methods also showed
similar accuracies because MNIST dataset is too simple and easy to fit the model (easy to achieve
99% test accuracy). For this reason, to accurately evaluate the effectiveness of each method, we
compared the performance on SVHN dataset, which is also a widely used benchmark dataset for
digit recognition and more realistic.
(b) Test accuracy over epoch
(a) Training loss over epoch
Figure 1: Training progress of CNNs on MNIST dataset.
1https://github.com/martius-lab/l4-optimizer
2https://github.com/Luolc/AdaBound
7
Under review as a conference paper at ICLR 2020
(a) Training loss over epoch
(b) Test accuracy over epoch
KMbHrop
Adam
L4-Adam
AdaBound
Figure 2: Training progress of ResNets on SVHN dataset.
SSO-SGD
SSO-Adam
SVHN dataset contains 73,257 training instances and 26,032 test instances of 32×32×3 size from
10 classes. As shown in Fig. 2-(a), L4-Adam, SSO-SGD, and SSO-Adam rapidly reduced the
training loss to zero on SVHN dataset. Furthermore, both SSO-SGD and SSO-Adam outperformed
all state-of-the-art competitors in the test accuracy. In the experiments, although L4-Adam, SSO-
SGD, and SSO-Adam all rapidly reduced the training loss, SSO-SGD and SSO-Adam showed better
generalization performance than L4-Adam.
5.2	Object Classification
CIFAR datasets contain 50,000 training instances and 10,000 test instances of 32×32×3 size.
CIFAR-10 and CIFAR-100 contain 10 and 100 categories (classes), respectively. We also used
ResNet with three residual blocks and one fully-connected layer for CIFAR datasets.
(a) Training loss over epoch
(b) Test accuracy over epoch
Figure 3: Training progress of ResNets on CIFAR-10 dataset.
On CIFAR-10 dataset, L4, SSO-SGD, and SSO-Adam also rapidly reduced the training loss to
zero (Fig. 3-a). Furthermore, SSO-SGD and SSO-Adam outperformed all competitors in the test
accuracy (Fig. 3-a). Especially, SSO-SGD achieved about 3% improvement on the test accuracy
compared to AdaBound that showed the highest test accuracy among the competitors.
As shown in Fig. 4-(b), both SSO-SGD and SSO-Adam outperformed all state-of-the-tart com-
petitors in the test accuracy again. In particular, SSO-SGD achieved 5% improved test accuracy
compared to L4-Adam that showed the highest test accuracy among the competitors. In this experi-
8
Under review as a conference paper at ICLR 2020
3.0
2.5
u 2.0
ɪe
y 1.5
ra
ω
R 1.0
0.5
0.0
RMSProp
Adam
L4-Adam
AdaBound
SSO-SGD
SSO-Adam -
Aup-Jnuu(πEI
50	100	150	200
Epoch
(a) Training loss over epoch
Epoch
(b) Test accuracy over epoch
Figure 4: Training progress of ResNets on CIFAR-100 dataset.
ment, SSO-SGD and SSO-Adam showed better generalization performance than L4-Adam because
stochastic SSO is designed to the stochastic learning environments.
5.3	Training Performance over Execution Time
In this experiment, we measured the test accuracy over actual execution time to evaluate the useful-
ness of each method. Fig. 5 shows the results of the experiment on SVHN and CIFAR100 datasets.
(a) Test accuracy on SVHN dataset
(b) Test accuracy on CIFAR100 dataset
AUQ.JnUOP〕S①一
Figure 5: Test accuracy over the execution time.
The experiment results are similar to the results in Fig. 2-(b) and 4. It shows that SSO is as efficient
as existing step size adaptation methods. Furthermore, quantitatively, SSO-SGD and SSO-Adam
required about 5,500 seconds execution time for 100 epochs like RMSProp and Adam, but L4-
Adam required about 9,000 seconds execution time that is 30% higher than the execution time of
SSO-SGD and SSO-Adam. On CIFAR-100 dataset, SSO-SGD and SSO-Adam also required about
8,500 seconds execution time for 200 epochs like RMSProp and Adam, but L4-Adam spent about
13,000 seconds.
5.4	Initial Upper B ounds and Training Performance
We checked the training performance on MNIST dataset by change the initial upper bound (0) to
measure the sensitivity of SSO for the hyperparameter. Fig. 6 shows the experiment results.
9
Under review as a conference paper at ICLR 2020
Figure 6: Changes on the test accuracy with the different initial upper bounds on MNIST dataset.
As shown in the result, SSO-Adam showed consistent test accuracy in 99.2%〜99.35% for all initial
upper bounds in {0.5, 0.6, 0.7, 0.8, 0.9, 1.0}.
References
Marcin Andrychowicz, Misha Denil, Sergio Gomez Colmenarejo, Mathew W. Hoffman, David Pfau,
Tom Schaul, Brendan Shillingford, and Nando de Freitas. Learning to learn by gradient descent
by gradient descent. In Advances in Neural Information Processing Systems (NIPS), 2016.
R. H. Byrd, S. L. Hansen, Jorge Nocedal, and Y. Singer. A stochastic quasi-newton method for
large-scale optimization. SIAM Journal on Optimization, 26:1008-1031, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research (JMLR), 12:2121-2159, 2011.
Daniel Gabay and Bertrand Mercier. A dual algorithm for the solution of nonlinear variational
problems via finite element approximation. Computes and Mathematics with Applications, 2:
17-40, 1976.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
Diederik P. Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations (ICLR), 2015.
Anders Krogh and John A. Hertz. A simple weight decay can improve generalization. Conference
on Neural Information Processing Systems (NIPS), 1992.
Ke Li and Jitendra Malik. Learning to optimize. International Conference on Learning Representa-
tions (ICLR)), 2017.
Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J. Smola. Efficient mini-batch training for
stochastic optimization. ACM SIGKDD Conference on Knowledge Discovery and Data Mining,
2014.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. International Conference on Learning Representations (ICLR), 2019.
Maren Mahsereci and Philipp Henning. Probabilistic line searches for stochastic optimization. In
Advances in Neural Information Processing Systems (NIPS), 2015.
Hua Ouyan, Niao He, Long Q. Tran, and Alexander Gray. Stochastic alternating direction method
of multipliers. International Conference on Machine Learning, 2013.
Michal Rolinek and Georg Martius. L4: Practical loss-based stepsize adaptation for deep learning.
In Advances in Neural Information Processing Systems (NeurIPS), 2018.
10
Under review as a conference paper at ICLR 2020
Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. Proceedings of Machine
LearningResearch (PMLR), 28:343-351, 2013.
T. Tieleman and G. Hinton. Rmsprop: Divide the gradient by a running average of its recent magni-
tude. Coursera: Neural Networks for Machine Learning, 2012.
A Appendix: Optimized Step Size on MNIST dataset.
In this experiment, we measured the optimized step size of SSO rate for each epoch. We used SSO-
Adam with the upper bound decay. Since the step size adaptation is executed by the number of
mini-batches for each epoch, we presented maximum, mean, and minimum of the optimized step
size for each epoch. Fig. 7 shows the optimized step sizes for each epoch. As shown in the result,
the learning rate is strictly optimized within [0, ] and gradually reduced over the epochs. Note that
the current upper bounds are not shown because the maximums of the optimized step sizes overlap
them in Fig. 7-(a).
0.0
10	20	30	40
Epoch
(a) Optimized step size over the epochs
(b) Optimized step size over the epochs without
presenting the maximum
Figure 7: Maximum, mean, and minimum of the optimized step sizes for each epoch on MNIST
dataset.
11