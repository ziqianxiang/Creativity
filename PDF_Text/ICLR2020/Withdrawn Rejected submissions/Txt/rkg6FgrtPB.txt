Under review as a conference paper at ICLR 2020
Biologically Plausible Neural Networks via
Evolutionary Dynamics and Dopaminergic
Plasticity
Anonymous authors
Paper under double-blind review
Ab stract
Artificial neural networks (ANNs) lack in biological plausibility, chiefly because
backpropagation requires a variant of plasticity (precise changes of the synap-
tic weights informed by neural events that occur downstream in the neural circuit)
that is profoundly incompatible with the current understanding of the animal brain.
Here we propose that backpropagation can happen in evolutionary time, instead
of lifetime, in what we call neural net evolution (NNE). In NNE the weights of the
links of the neural net are sparse linear functions of the animal’s genes, where each
gene has two alleles, 0 and 1. In each generation, a population is generated at ran-
dom based on current allele frequencies, and it is tested in the learning task. The
relative performance of the two alleles of each gene over the whole population is
determined, and the allele frequencies are updated via the standard population ge-
netics equations for the weak selection regime. We prove that, under assumptions,
NNE succeeds in learning simple labeling functions with high probability, and
with polynomially many generations and individuals per generation. We test the
NNE concept, with only one hidden layer, on MNIST with encouraging results.
Finally, we explore a further version of biologically plausible ANNs inspired by
the recent discovery in animals of dopaminergic plasticity: the increase of the
strength of a synapse that fired if dopamine was released soon after the firing.
1	Introduction
In his Turing award lecture, neural networks pioneer Geoff Hinton opined that “evolution can’t get
gradients because a lot of what determines the relationship between the genotype and the phenotype
is outside your control” (Hinton, 2019). We beg to differ. Evolution does have what amounts to an
effective oracle access to the (indeed, complex and intractable) mapping from genotype to pheno-
type. The well-established equations of population genetics governing evolution under recombina-
tion (Burger, 2000; Chastain et al., 20l4) describe the way whereby the distribution of genotypes
in the population is updated from one generation to the next, informed by the empirical fitness of
the phenotypes during lifetime; and these equations do bear a similarity to gradient descent and,
even closer, to no-regret learning (Chastain et al., 2014). In this paper, we show that, in fact, quite
effective training of neural nets can be carried out, without backpropagation, in evolutionary time.
The towering empirical success of ANNs has brought into focus their profound incongruity with
what we know about the brain: backpropagation requires that and synaptic weights and plasticity
be informed by downstream events. Clever versions of ANNs have been proposed recently that
avoid this criticism: ANNs whose backward weights are random and fixed (Lillicrap et al., 2016)
and a variant that also uses random feedback weights but with zero initial conditions (N0 kland,
2016), a backpropagation interpretation of STDP (a widely accepted theory of plasticity) (Bengio
et al., 2015), unsupervised learning using STDP (Diehl & Cook, 2015), ANNs driven by neural
competition (Krotov & Hopfield, 2019), or ANNs with target value propagation at each layer rather
than the loss gradient (Lee et al., 2015).
Here we take a very different approach. We believe that, while forward neural computation is
coterminous with life, backpropagation (i.e., feedback to individual neurons and synapses about their
contribution to the overall performance of the circuit) can be effectively carried out over evolutionary
1
Under review as a conference paper at ICLR 2020
time. Suppose that the brain circuitry for a particular classification task, such as “food/not food”,
is encoded in the animal’s genes, assuming each gene to have two alleles 0 and 1. A (haploid)
genotype is a bit string. Crucially, we assume that the weight of each link of the neural network is
a fixed sparse linear function of the genes. Evolution proceeds in generations. At each generation,
a gene is an independent binary variable with fixed probability of 1. A population is sampled from
this distribution of genotypes, and it experiences a sequence of inputs to the brain circuit. Fitness of
each genotype depends, to some small degree, on the animal’s success over its lifetime in the specific
classification task. In the next generation, the allele frequencies will change slightly, depending on
how each allele of each gene fared cumulatively (over both all inputs and all genotypes containing
it) in the classification task. These changes follow the standard population genetics equations for
the weak selection regime, see Burger (2000); Chastain et al. (2014); weak selection means that the
classification task is only one of the many biological functions (digestion, locomotion, other brain
circuits, etc.) that affect the animal’s fitness.
The question is, can competent brain circuits evolve this way? We offer theoretical evidence that this
is indeed the case1. In Section 2 we prove that, if the classifier to be learned is linear, then evolution
does indeed succeed in creating a neural network that classifies well, almost certainly and within
polynomial (in the dimension of the problem) number of generations, individuals per generation,
and neurons per individual. We also validate our theorem through experiments on the MNIST data
set. Our experiments are not meant to compete with what is happening in the ANN world. We
want to make the point that competent learning can happen in life: NNE with a single hidden layer
already gives surprisingly good accuracy rates (more than 90% accuracy for classifying the MNIST
digits 0 to 4).
There is a different way of looking at, and motivating, our results, namely from the point of view
of the study of the brain in connection to evolution. “Nothing in biology makes sense except in the
light of evolution,” Theodosius Dobzhansky famously proclaimed. Neuroscientists have espoused
this point of view, and evolutionary arguments come up often in the study of the brain, see for
example Bosman & Aboitiz (2015). However, we are not aware of a technical discussion in the
literature of the obvious existential question: Is the architecture of the brain susceptible to evolution
through natural selection? Can brain circuits evolve? Our mathematical and empirical results in
this paper on NNE strongly suggest that, indeed, effective brain circuits specializing in classification
tasks could have evolved.
We also propose a second biologically plausible ANN-like mechanism, based on dopaminergic plas-
ticity. It was recently established experimentally (Yagishita et al., 2014) that weights in certain
synapses (in this case from the cortex to the striatum in the mouse brain, but not only) are increased
if dopamine was released within 0.5-2 seconds after the synapse’s firing. Intuitively, this is a rein-
forcement plasticity mechanism that “rewards” synapses whose firing led to a favorable outcome.
Inspired by this experiment, we define dopaminergic neural nets (DNN), in which the weight of a
link that fired (that is, both nodes fired during the current training example) is modified by a multiple
of (4 一 err2), where err is the error of the current training example. That is, links that fired are
rewarded if the result was good, and actually also punished ifit was not. Our experiments show that
such DNNs can also learn to classify quite well, comparable to SGD.
Our Contributions. In Section 2, we give a rigorous proof that NNE with a single hidden layer
succeeds in learning arbitrary linear target functions. In Section 3, we discuss experiments with
NNE and DNN on MNIST.
2	Analysis of NNE
A genotype can be viewed as a vector x ∈ {0, 1}n. A probability distribution over the genotypes
is given by a vector p ∈ [0, 1]n; a genotype x is sampled by setting x(i) = 1 with probability
p(i), independently for each i. The neural network corresponding to a genotype x is a feed-forward
neural network (FFNN) whose weights are computed as follows. For a prediction network having
m links, the weights of the links are given by Wx, where W is an m × n sparse weight generation
1We note incidentally that NNE is of course very much distinct from neuroevolution (see the recent sur-
vey Stanley et al. (2019)), which optimizes ANN architecture and hyperparameters through genetic algorithms.
2
Under review as a conference paper at ICLR 2020
matrix. We choose the entries of W to be random and i.i.d.: with probability β, W (i, j) is chosen
uniformly at random from [-1, 1], and is 0 with probability 1 - β.
The input to the network is a vector y drawn from a distribution D and has a label (possibly real-
valued) '(y). The output of the network on an input y is NNEx (y). In the simplest linear case
(Section 2.1), y ∈ Rm and NNEx (y) = xTWTy. In our experiments (Section 3.1), we study the
case when D is the uniform distribution over MNIST, and NNEx (∙) is a 1-layer neural network with
a ReLU output gate (see Section 3.1 for formal definition).
For each genotype x, we measure its performance by computing the loss L(NNEx (y) , l(y)) (this
could be squared loss, cross-entropy loss, etc.). For a probability distribution over genotypes p, we
define the loss as
L(P)= Ex~pEy~0L(NNEx (y) ,'(y)).
We calculate the rewards ft (i) and ft (i) as the expected negative loss whenever the allele is present
and absent respectively.
ft⑴=Ex~pt [Ey~D [-L(NNEx (y),'(J))] I X⑴=1] .	(I)
and
ft(i) = Ex~pt [Ey~D [-L(NNEx (y),'(y))] | X⑴=0] .	⑵
For the next generation we calculate,
P = Pt(i)(1 + ft(i))	and q =(1 — Pt(i))(1 + ft(i))
We normalize P and q to make it a probability distribution. Thus the allele probabilities for the next
generation will be,
P
p + q
Pt+1(i)
pt(i)(1 + ft(i))
1 + ft(i) + EPt(i)(ft(i)- ft(i))
(3)
This is the standard update rule in population genetics under the weak selection assumption. The
multiplier captures the small degree to which the performance of this task by the animal confers
an evolutionary advantage leading to larger progeny.
Our first observation is that perfomance per allele is in fact a function of the gradient of the loss
function.
Lemma 1
∂
L(Pt) = -ft。)— Pt(i)(f t(i) — ft(i)) and d-^ (L(Pt)) = —(ft ⑶ — ft(i))∙
∂P (i)
Proof.
L(Pt) = Ex~pEy~D [L(NNEx (y) ,'(y))]
=Pt(i)Ex~pt [Ey~D [L(NNEx (y) ,'(y))] ∣x(i) = 1]
+ (1 — Pt(i))Ex~pt [Ey~D [L(NNEx (y), '(y))] ∣ x(i) = 0]
=Pt(i)(-ft(i)) + (1 — Pt(i)) (—ft(i)) = -ft(i) — Pt(i)(ft(i) — ft(i)).
Here, the last line follows from equations 1 and 2. Now, taking the derivative w.r.t. Pt (i) we get
∂
-wτrL (L(Pt)) = —(ft(i) — ft(i))∙
∂Pt (i)
□
We use this to prove the following theorem.
Theorem 1 Fix δ > 0. Suppose V2L(z) W H ∙ I ∀z ∈ [0, 1]n. Let U := supp∈[0,1]n L(P) and
St := {i ∈ [n]Iδ ≤ Pt(i) ≤ 1 — δ}. For ≤ min{1/ (max{2U, 1}) , 2/H, 1}, there is an η > 0 s.t.
E(L(Pt+1)) ≤ L(Pt) — η X (ViL(Pt))2.
i∈St
3
Under review as a conference paper at ICLR 2020
Proof. Using Equation 3 and Lemma 1 we get
pt+1(i)-pt(i)
e ∙pt(i)(1 -pt(i))(ft(i)-产(i)) - e ∙pt(i)(1 -pt(i))
1 + ft(i)+ EPt(i)(ft(i)- ft(i))
-Yi ∙ ViL(pt)
1 - L(pt)
ViL(Pt)
—
where γ is as defined above. For our choice of , we have
1 - eL(pt) ≥ 1 - eU ≥ 1 and e ∙ pt(i)(1 - pt(i)) ≤ ∣ ≤ ɪ.
2	4	2H
Therefore, γi ≤ 1/H. Using Taylor’s theorem, there exists a z ∈ [pt,pt+1] s.t.
L(pt+1) = L(Pt)- (pt+1 - Pt)TVL(Pt) + 2(pt+1 - Pt)TV2L(z)(pt+1 - Pt)
≤ L(Pt)- X Yi(ViL(Pt))2 + HH X γ2(ViL(Pt))2	(Using V2L(z) √ HI)
ii
=L(Pt) - X(Yi- HYB(ViL(Pt))2.
Since, Yi ≤ 1/H, we have Yi - Hγ2 ≥ Yi/2. Therefore,
L(Pt+1) ≤ L(Pt) - X Yi(ViL(Pt))2 = L(Pt) - 2「:L“t、、X理)(1 - Pt(i)) (ViL(Pt))2
i 2	2(1 - L(P )) i
≤ L(Pt) - 2δ⅛B)) X(ViL(Pt))2,
i∈St
where B := infp∈[0,1]n L(P). Therefore, the conclusion of the theorem holds for η = δ(1 -
δ)/ (2(1 - eB)).
2.1	Learning linear functions
In this section, we show that in the case of a linear target functions, with high probability, NNE
converges to an allele distribution P which is arbitrarily close to the correct linear labeling. Our
NNE has m input gates connected to one output gate (i.e., no hidden layers). For a genotype x, the
weights of the connections are given by Wx. On input y, the NNE outputs xTWTy.
Theorem 2 Let D be the uniform distribution over vectors in an n-dimensional unit ball. Let a be
a fixed vector with ∣∣ak ≤ 1, such that the label of y is '(y) := aτ y. Let W have i.i.d. entries with
Wij = ±ʌ/m/d with probability d/m and 0 with probability 1 — (d/m). Then, for any δ ∈ (0,1],
with n = O(m + log(1/6)), with probability at least 1 一 δ, there exists an allele distribution P s.t.
WP = a. Moreover, with probability at least 3/4, for any E ∈ (0,1], with n = C(m(log(1/E)/E2),
there is an X ∈ {0,1}n s.t. 八弋^小：口 ≥ 1 —匕
We remark that the above guarantee works for every linear target function in Rm . To learn, with
high confidence, the target function from among d unknown (arbitrary) linear functions, m above
can be replaced by log d.
Proof. To have WP = a, with P(i) ∈ [0, 1], it suffices that the vector a lies in the convex hull of
the columns of W . This follows if the unit ball around the origin is contained in the convex hull
of the vectors W(i) . By duality, it suffices that every halfspace tangent to the unit ball (and not
containing it) has at least one of the W(i) in it. For any single halfspace tangent to the unit ball,
the probability that a random W(i) falls in it is at least a constant factor — each W(i) has squared
length m in expectation and concentrated near it. Thus, the halfspace it defines carves out a cap of
constant measure. Next, by the VC theorem, if n = Ω(m + log(1/δ)), with probability 1 - δ every
such halfpsace will contain a column of W. This establishes E(W x) = a.
4
Under review as a conference paper at ICLR 2020
Generation t
Generation t + 1
√K networks (genotypes)
Rewards to
alleles
Figure 1: Trainng NNE across generations
To bound the error, We consider the subset of columns of W that have a nontrivial inner product
with a and take their sum, i.e., let J = {i : wi ∙ a ≥ √mIlakkwik} and d = | J|, and consider the
random variables:
Y = 1 X wi and Z = Y ∙ a.
d
i∈J
Then by the symmetry of the distribution of W, E(Y) points in the same direction as a (in all
other directions, the truncated distribution remains symmetric and therefore has mean zero). For
convenience . Then,
Var(Z) = X ^X Var(wi ∙ a) ≤ Ckak∙
i∈J
On the other hand,
E(kYk2) = dp X E(wi ∙ wj) ≤ dp (dE(kwik2) + d2E(wi ∙ a)2) ≤ 亨 + C2∣∣ak∙
i,j∈J
where c, ci, c2 are absolute constants. So if d = Ω(m∕e), then with large probability,
Y ∙ a
Wm ≥1 - O(e).
(4)
However, we need this for every possible a. So we take an (e/2)-net of the unit ball in Rm (which
has size at most (3∕e)m). For any fixed a, by taking d = Ω (mlog(1/0), the Hoeffding bound
tells us that the probability that (4) is violated is at most e-2 d/4 ≤ (e/4)m. Then, by a union
bound, this bound on d suffices for all a. Finally, with n = Ω(mlog(1∕e)∕e2) whp, every cap
{y : a ∙ y ≥ 1/√m} has at least d columns of W in it.	□
3	Experiments
3.1	NNE on MNIST
We study the effectiveness of NNE by evaluating its classification performance on the MNIST
dataset.
To train an NNE via evolution of T generations of genotypes, we fix a sufficiently large population
size N. Each generation t ∈ [T] consists of a sample ofN independently sampled genotypes from
the allele distribution pt , we denote this sample by Pt . This distribution is updated based on the
average performance f t(i) and ft(i) of all the genotypes on a task, in our case, MNIST handwirtten
digit recognition task. We let the allele distribution pt evolve over T generations in this manner (see
fig. 1).
5
Under review as a conference paper at ICLR 2020
Experimental setup. We use 200 training samples for each of the digits, drawn uniformly at
random from MNIST; we denote this set of training examples by S. p1, the allele distribution for
the first generation, is sampled uniformly at random from [0, 1]n. We evaluate the performance of
the alleles over N = 1000 genotypes.
Our network has 784 input units, one hidden layer of |h1 | = 1000 units with ReLU activation and
an output layer of 10 units with softmax activation. We add a sparse random graph between the
input layer and the hidden layer: between a neuron in the input layer and a neuron in the hidden
layer, we independently add an edge with probability 0.1. The hidden layer is fully connected to the
output layer. We choose β = 0.0025 for our experiments, i.e., each edge weight is a sparse random
function of only β fraction of the alleles. For the input sample y, '(y) is now a one-hot encoding
of the label, and NNEx (y) is the soft-max output of the network. We use the cross-entropy loss
function, L(NNEx (y) ,'(y)) = - £.3 '(y)Clog(NNEχ (y)c).
If a classifier were to randomly guess the label of an input intance, its loss function value would
be α := - log (1/10). We use the relative performance of the genotype w.r.t. to a random
guess for our updates. To this end, we define for a genotype x, δx :=看 Ps∈[s] max{0, α2 -
L(NNEx (y), '(y))2}. For each allele, we calculate the rewards f t(i) and ft(i) whenever the allele
is present and absent respectively.
ft(i) = Σ2x∈Pt δxx(Z)
f()= Px∈P t x(i)
P t δ (1 - x(i))
and	ft(i)= ¾PPtxi(-x(i()).
The allele distribution for the next generation is updated using equation 3.
NNE as described above achieves 78.8% test accuracy on the full MNIST test set. While this is
somewhat far from the state of art in classification of MNIST images, our results demonstrate that
very basic NNEs can perform reasonably well in this task.
Convergence of allele distributions. We repeat for many (hundreds of thousands) generations.
As our theoretical results predict (see also Mehta et al. (2015)), the vast majority of genes have
allele probabilities that are very close to 0 and 1. Figure 2 shows the fraction of allele probabilities
that are at a distance [x, 1 - x] from 0 or 1, i.e., y is calculated as y = 1 - |{i:mm{p (i1-p (i)}」x}|.
----t = 10K-------t = 100K..... t = 200K
s<υW=qnqαjd<υ-<υ-ra¾UO-tie」LL.
0.0	0,1	0.2	0.3	0.4	0.5
Distance to {0,l}
Figure 2: Convergence of allele distribution after t generations: x-axis shows the distance of the allele prob-
abilities from 0 or 1 and the y-axis shows the fraction of n allele probabilities that are at a distance [x, 1 — x]
from 0or1.
NNE with output layer training. The biological implausibility objection of using stochastic
gradient based updates is less acute for the output layer, since in animal brains synaptic changes due
to plasticity happen at the post-synaptic neuron, and for the output layer this is the output neuron.
Even then, computing exact (or approximate) gradients is a nontrivial computational task; instead we
consider using just the sign of the gradient for only the output layer as a lifetime training mechanism.
For the same network described as above, we randomly initialize the network weights using allele
distribution learned using the NNE. We then calculate the sign of the gradient of the output layer
weights and update the weights in the opposite direction (SignSGD), using a sufficiently small
learning rate 0, similar to stochastic gradient descent. For i in the hidden layer and j in the output
6
Under review as a conference paper at ICLR 2020
1.0
n = 300
= 400
=500
=500
=1000
n = 2000
generations
Figure 3: Number of genes (n) Vs performance of NNE: (a) Accuracy rates of NNE on MNIST 0 — 4 showing
the effect of number of genes on performance. (b) Similarly, We also plot the accuracy rates of NNE on MNIST
0 — 9 dataset while varying the number of genes. The accuracy trends show that more the number of genes,
better the performance of NNE, but at the cost of more training time.
generations
model	MNIST 0to4	MNIST 0to9
NNE	92.1	788
NNE + SignSGD	91.6	85.6
SGD	96 ± 0.4	88.2 ± 0.75~~
Table 1: Accuracy rates of NNE on MNIST test digits.
layer, the update is
Wij ：= Wij - e0 ∙ sign ((Zj - '(yj)hi)	(5)
where hi is output of the neuron i, and zj softmax output of neuron j (see appendix for the proof).
SignSGD has been shown to be effective for traning large deep neural networks (for e.g., see Bern-
stein et al. (2018)).
We perform a few hundred iterations of this training using batch size 50. In this experiment (NNE
+ SignSGD), we obtain 86.3% accuracy on full MNIST test set. This further demonstrates that
biologically plausible neural networks can perform reasonably well in this task.
Number of genes. A crucial choice for an NNE is the number of genes. In our experiments, we
use a few thousand genes; this is not unreasonable as it is estimated that about 5, 000 genes are
expressed in the cells of the mammalian brain. To investigate further, we compare the performance
of our algorithm with increasing values of n (the number of genes). Figure 3 presents the validation
accuracy trends on the same network described above for five class [0 - 4] classification and for
full MNIST dataset. We observe that the accuracy rate of the network improves significantly with
increase in the number of genes. However, it requires much longer training time to achieve a desired
accuracy rate.
Table 1 compares the results of all the models along with the baseline, stochastic gradient descent
trained on the same subset of MNIST.
3.2	Dopaminergic Neural Nets (DNNs)
DNNs are biologically plausible ANNs based on dopaminergic plasticity. They learn by a weak
form of immediate reinforcement - “rewarding” synapses whose firing led to a favourable outcome.
If a connection between two neurons has fired during a training step, then its weight is increased
if the square error was low (less than 4). In this section, we demonstrate that simple DNNs can
perform reasonably well for tasks like classifying the images in the MNIST dataset.
Experimental setup. For our experiments we use a network consisting of an input layer, a single
hidden layer, and an output layer consisting of 784, h, and 10 neurons respectively. Each neuron
in the input layer has a link to each neuron in the hidden layer, and its weight is initialised by the
popularly used Kaiming Uniform (more commonly called He initialisation (He et al., 2015)). These
weights are unchanged through out the learning process. Recent theoretical results suggest that a
7
Under review as a conference paper at ICLR 2020
h	SGD	SignSGD	DNN
1000	90.34	-^86.84^^	84.84
100, 000	92.56	90.21	90.76
Table 2: Test accuracies of different models for different h.
large enough random layer is sufficiently rich and efficiently trainable (Vempala & Wilmes, 2019)
(see also (Rahimi & Recht, 2008)).
Each neuron in the hidden layer has a link to each neuron in the output layer. The output layer
outputs the softmax score. The weights of this layer are learned using plasticity based updates. On
seeing an input y, the DNN tries to predict the label of y; let us denote this by DNNW (y). If the
DNN got the prediction correct, i.e. the loss L(DNNW (y), '(y)) is at most ∈o, then weight Wij get
increased by a small amount, provided the output neuron j has low error (i.e. |zj - l(y)j |1 2 3 4 5 ≤ 1/4)
where zj is the jth coordinate of DNNW (s).
Formally, the update rule is as follows for i in the hidden layer and j in the output layer.
_	max {0, 4 - |zj - '(y)j∣2} ∙ max {0, L(DNNw (y), '(y)) - e0}
Wij = Wij	£1	(4 -∣zj-'(y)j∣2)∙(L(DNNw (y) ,'(y)i)
Experimental results. To study the effectiveness of our DNN in the 10-class MNIST digit classi-
fication, we compare its peformance with some other standard baselines.
1.	SGD: In this we use the standard stochastic gradient descent (with the Adam optimiser
Kingma & Ba (2015)) based updates to train our network.
2.	SignSGD: As before, we use the sign of the gradient for updates (equation 5).
Table 2 shows the results for different h values. All results are after 500 epochs of training. As
with NNE we use the cross-entopy loss for all the models. We found that 0 = 0.75 and 1 = 1 for
the DNN gives reasonable performance. Our DNN gives encouraging results and is comparable to
SignSGD in performance.
4	Discussion and Further Work
We have presented two biologically plausible mechanisms for the evolution of neural networks,
motivated by the brain, and based on evolution. One feature of these mechanisms is that they process
one input instance at a time (i.e., unit batch size) and do not require the computation of gradients.
Our preliminary experiments with bio-plausible mechanisms suggest that they are promising alter-
natives to backpropagation and the explicit use of gradients. The results raise several interesting
possibilities:
1. In our current set-up, network weights are sparse linear functions of alleles. What if we
used nonlinear functions (e.g., sigmoids or ReLUs) to define the weights?
2. In our experiments, allele distributions approach 0/1 values for most coordinates. We could
take advantage of this by fixing alleles that are sufficiently close to 0 or 1 and continue only
on the rest.
3. Can this approach be used, with greater success, for multi-layer networks?
4. Does NNE or DNN implicitly optimize the underlying architecture?
5. A recent model of memory creation and association in the mammalian brain is based on
plasticity and inhibition (Papadimitriou & Vempala, 2019). In this model, inhibition is
implemented as a cap, where the top k highest weighted input neurons of an entire layer
are the ones that fire; the rest of the neurons in the layer are suppressed. Our experiments
with DNN indicate that using a k-cap for the hidden layer with k as small as 256 when
h = 100, 000 does not degrade performance (and even enhances it slightly), while reducing
computation. Can k-cap help with learning?
8
Under review as a conference paper at ICLR 2020
References
Yoshua Bengio, Dong-Hyun Lee, Jorg Bornschein, and Zhouhan Lin. Towards biologically plausible
deep learning. arXiv preprint arXiv:1502.04156, 2015.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar.
SIGNSGD: compressed optimisation for non-convex problems. In Jennifer G. Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML
2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of
Machine Learning Research,pp. 559-568. PMLR, 2018. URL http://proceedings.mlr.
press/v80/bernstein18a.html.
Conrado Arturo Bosman and Francisco Aboitiz. Functional constraints in the evolution of brain
circuits. In Front. Neurosci., 2015.
Reinhard Burger. The mathematical theory of selection, recombination, and mutation, volume 228.
Wiley Chichester, 2000.
Erick Chastain, Adi Livnat, Christos Papadimitriou, and Umesh Vazirani. Algorithms, games, and
evolution. Proceedings of the National Academy of Sciences, 111(29):10620-10623, 2014.
Peter Diehl and Matthew Cook. Unsupervised learning of digit recognition using spike-timing-
dependent plasticity. Frontiers in Computational Neuroscience, 9:99, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In 2015 IEEE International Conference
on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pp. 1026-1034. IEEE
Computer Society, 2015. ISBN 978-1-4673-8391-2. doi: 10.1109/ICCV.2015.123. URL https:
//doi.org/10.1109/ICCV.2015.123.
Geoffrey E. Hinton. Turing award lecture ”the deep learning revolution”, 1:27:43. 2019.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015.
Dmitry Krotov and John J. Hopfield. Unsupervised learning by competing hidden units. Proc. Natl.
Acad. Sci. U.S.A., 116(16):7723-7731, 2019.
Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation.
In Machine Learning and Knowledge Discovery in Databases, 2015.
Timothy P. Lillicrap, Daniel Cownden, Douglas Blair Tweed, and Colin J. Akerman. Random synap-
tic feedback weights support error backpropagation for deep learning. In Nature communications,
2016.
Ruta Mehta, Ioannis Panageas, and Georgios Piliouras. Natural selection as an inhibitor of genetic
diversity: Multiplicative weights updates algorithm and a conjecture of haploid genetics. In Pro-
ceedings of the 2015 Conference on Innovations in Theoretical Computer Science, 2015.
Arild N0 kland. Direct feedback alignment provides learning in deep neural networks. In Advances
in Neural Information Processing Systems 29. 2016.
Christos H. Papadimitriou and Santosh S. Vempala. Random projection in the brain and computation
with assemblies of neurons. In 10th Innovations in Theoretical Computer Science Conference,
ITCS 2019, January 10-12, 2019, San Diego, California, USA, pp. 57:1-57:19, 2019.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. C. Platt,
D. Koller, Y. Singer, and S. T. Roweis (eds.), Advances in Neural Information Processing Systems
20, pp. 1177-1184. Curran Associates, Inc., 2008.
Kenneth O. Stanley, Jeff Clune, Joel Lehman, and Risto Miikkulainen. Designing neural networks
through neuroevolution. Nature Machine Intelligence, 1:24-35, 2019.
9
Under review as a conference paper at ICLR 2020
Santosh Vempala and John Wilmes. Gradient descent for one-hidden-layer neural networks: Poly-
nomial convergence and sq lower bounds. In Alina Beygelzimer and Daniel Hsu (eds.), Proceed-
ings of the Thirty-Second Conference on Learning Theory, volume 99 of Proceedings of Machine
LearningResearch,pp. 3115-3117, Phoenix, USA, 25-28 JUn 2019. PMLR.
Sho Yagishita, Akiko Hayashi-Takagi, Graham CR Ellis-Davies, Hidetoshi Urakubo, Shin Ishii, and
HarUo Kasai. A critical time window for dopamine actions on the strUctUral plasticity of dendritic
spines. Science, 345(6204):1616-1620, 2014.
5	Appendix
Lemma 2 For neuron i in the hidden layer and neuron j in the output layer, the gradient of the
cross entropy loss with respect to the weights wij in the final layer is
∂L
∂Wij = (Zj-'(y)j )hi.
where hi is output of the neuron i, and zj softmax output of neuron k.
Proof. For the hidden layer oUtpUt h and the final layer weights W, we compUte the soft-max oUtpUt
as follows,
o=WTh.
z = sof tmax(o).
TrUe oUtpUt l(y) is a one-hot encoded vector and the oUtpUt of the model z is the softmax of the final
layer inpUt o. The derivative of loss fUnction L w.r.t. o is given by
∂L = - XX ∂'(y)k log(zk)
d0j^^⅛ —doj一
C -X '(y)k k=1	C ∂log(zk)	V W、1 ∂zk -Λ	 = - 7,'(y)k — Tj- ∂oj	Zk ∂oj k=1	
'⑹j dzj Zj ∂oj	C L '(y)k ∂zk __ '⑹j C - ⅛j 丁西=-Fzj(I	-Zj) - X '(yk (-zkZj) Zk k6=j
CC	C
=-'(y)j + '(y)j zj + ^X '(y)kzj = -'(y)j + ^X '(y)k Zj = -'(y)j + Zj ^X '(y)k = Zj- '(y)j.
k6=j	k=1	k=1
We know that oj = Pi wij hi . Hence,
∂oj
W-- = hi.
∂wij
Then the gradient for the final layer weights wik is calcUlated as:
∂L- = ∂L ∙篝=(zj - '(yj)hi.
∂wij	∂oj ∂wij
□
10