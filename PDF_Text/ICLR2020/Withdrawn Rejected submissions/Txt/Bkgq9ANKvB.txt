Under review as a conference paper at ICLR 2020
Peer Loss Functions: Learning from Noisy La-
bels without Knowing Noise Rates
Anonymous authors
Paper under double-blind review
Ab stract
Learning with noisy labels is a common problem in supervised learning. Existing
approaches require practitioners to specify noise rates, i.e., a set of parameters
controlling the severity of label noises in the problem. The specifications are ei-
ther assumed to be given or estimated using additional approaches. In this work,
we introduce a technique to learn from noisy labels that does not require a priori
specification of the noise rates. In particular, we introduce a new family of loss
functions that we name as peer loss functions. Our approach then uses a standard
empirical risk minimization (ERM) framework with peer loss functions. Peer loss
functions associate each training sample with a certain form of “peer” samples,
which evaluate a classifier’ predictions jointly. We show that, under mild condi-
tions, performing ERM with peer loss functions on the noisy dataset leads to the
optimal or a near optimal classifier as if performing ERM over the clean training
data, which we do not have access to. To our best knowledge, this is the first re-
sult on “learning with noisy labels without knowing noise rates” with theoretical
guarantees. We pair our results with an extensive set of experiments, where we
compare with state-of-the-art techniques of learning with noisy labels. Our results
show that peer loss functions based method consistently outperforms the baseline
benchmarks, as well as some recent new results. Peer loss provides a way to sim-
plify model development when facing potentially noisy training labels, and can be
promoted as a robust candidate loss function in such situations.
1	Introduction
The quality of supervised learning models depends on the training data {(xn, yn)}nN=1. In practice,
label noise can arise due to a host of reasons. For instance, the observed labels ynS may represent
human observations of a ground truth label. In this case, human annotators may observe the label
imperfectly due to differing degrees of expertise or measurement error, see e.g., medical examples
such as labeling MRI images from patients. Many prior approaches to this problem in the machine
learning literature aim to develop algorithms to learn models that are robust to label noise (Bylander,
1994; Cesa-Bianchi et al., 1999; 2011; Ben-David et al.; Scott et al., 2013; Natarajan et al., 2013;
Scott, 2015). Typical approaches require a priori knowledge of noise rates, i.e., a set of parameters
that control the severity of label noise. Working with unknown noise rates is difficult in practice:
Often, one must estimate the noise rates from data, which may require additional data collection
(Natarajan et al., 2013; Scott, 2015; Van Rooyen et al., 2015) (e.g., be a redundant set of noisy
labels for each sample point, or a set of ground truth labels for tuning these parameters) and may
introduce estimation error that can affect the final model in less predictable ways. Our main goal is
to provide an alternative that does not require the specification of the noise rates, nor an additional
estimation step for the noises. This target solution might help when the practitioner does not have
access to reliable estimates of the noise rates (e.g., when the training data has limited size for the
estimation tasks, or when the training data is already collected in a form that makes the estimation
hard to perform).
In this paper, we introduce a new family of loss functions, peer loss functions, to empirical risk
minimization (ERM), for a broad class of learning with noisy labels problems. Peer loss functions
operate under different noise rates without requiring either a priori knowledge of the embedded
noise rates, oran estimation procedure. This family of loss functions builds on approaches developed
in the peer prediction literature (Miller et al., 2005; Dasgupta & Ghosh, 2013; Shnayder et al., 2016),
1
Under review as a conference paper at ICLR 2020
which studies how to elicit information from self-interested agents without verification. Typical
approaches in the peer prediction literature design scoring functions to score each reported data
using another noisy reference answer, without accessing ground truth information. We borrow this
idea and the associated scoring functions via making a connection through treating each classifier’s
prediction as an agent’s private information to be elicited and evaluated, and the noisy label as an
imperfect reference from a “noisy label agent”. The peer loss takes a form of evaluating classifiers’
prediction using noisy labels on both the targeted samples and a particular form of constructed “peer”
samples. The evaluation on the constructed peer sample encodes implicitly the information about
the noises as well as the underlying true labels, which helps us offset the effects of label noises. The
peer sample evaluation returns us a favorable property that expected risk of peer loss turns to be
an affine transformation of the true risk of the classifier defined on the clean distribution. In other
words, peer loss is invariant to label noises when optimizing with it. This effect helps us get rid of
the estimation of noise rates.
The main contributions of this work are:
1.	We propose a new family of loss functions that can easily adapt to existing ERM framework that
i) is robust to asymmetric label noises with formal theoretical guarantees and ii) requires no prior
knowledge or estimation of the noise rates (no need for specifying noise rates). We believe having
the second feature above is a non-trivial progress, and it features a promising solution to deploy
in an unknown noisy training environment.
2.	We present formal results showing that performing ERM with a peer loss function can recover an
optimal, or a near optimal classifier f * as if performing ERM on the clean data (Theorem 2, 3,4).
We also provide analysis for peer loss functions’ risk guarantees (Theorem 5 and 7).
3.	We present extensive experimental results to validate the usefulness of peer loss (Section 5 and
Appendix). This result is encouraging as it is able to remove the long-standing requirement of
learning error rates of noises (or estimating transition matrices as used in many relevant papers)
before many of the existing methods can be applied. We also provide preliminary results on how
peer loss generalizes to multi-class classification problems.
4.	We will contribute to the community by publishing our codes and implementations.
1.1	Related Work
Learning from Noisy Labels Our work fits within a stream of research on learning with noisy
labels. A large stream of research on this topic works with the random classification noise (RCN)
model, where observed labels are flipped independently with probability e ∈ [0, 2 ] (Bylander, 1994;
Cesa-Bianchi et al., 1999; 2011; Ben-David et al.). Recently, learning with asymmetric noisy data
(or also referred as class-conditional random classification noise (CCN)) for binary classification
problems has been rigorously studied in (Stempfel & Ralaivola, 2009; Scott et al., 2013; Natarajan
et al., 2013; Scott, 2015; Van Rooyen et al., 2015; Menon et al., 2015). For a more thorough survey
of classical results on learning with noisy data, please refer to (Frenay & Verleysen, 2014).
Symmetric loss For RCN, where the noise parameters are symmetric, there exists works that show
symmetric loss functions (Manwani & Sastry, 2013; Ghosh et al., 2015; 2017; Van Rooyen et al.,
2015) are robust to the underlying noises, without specifying the noise rates. It was also shown that
under certain conditions, the proposed loss functions are able to handle asymmetric noises. Our
focus departs from this line of works, and we will exclusively focus on asymmetric noise setting,
and study the possibility ofan approach that can ignore the knowledge of noise rates.
Follow-up works (Du Plessis et al., 2013; van Rooyen et al., 2015; Menon et al., 2015; Charoen-
phakdee et al., 2019) have looked into leveraging symmetric conditions and 0-1 loss with asymmet-
ric noises, and with more evaluation metrics, such as balanced error rate and AUROC. In particular,
experimental evidences are reported in (Charoenphakdee et al., 2019) on the importance of sym-
metricity when learning with noisy labels.
More recent works More recent developments include an importance re-weighting algorithm (Liu
& Tao, 2016), a noisy deep neural network learning setting (Sukhbaatar & Fergus, 2014; Han et al.,
2018; Song et al., 2019), and learning from massive noisy data for image classification (Xiao et al.,
2015), robust cross entropy loss for neural network (Zhang & Sabuncu, 2018), loss correction (Pa-
2
Under review as a conference paper at ICLR 2020
trini et al., 2017), among many others. Loss or sample correction has also been studied in the context
of learning with unlabeled data with weak supervisions (Lu et al., 2018). Most of above works either
lacks theoretical guarantee of the proposed method against asymmetric noise rates ((Sukhbaatar &
Fergus, 2014; Zhang & Sabuncu, 2018)), or requires estimating the noise rate (or transition matrix
between noisy and true labels, (Liu & Tao, 2016; Xiao et al., 2015; Patrini et al., 2017; Lu et al.,
2018)). A good number of the recent works can be viewed as derivatives or extention of the unbiased
surrogate loss function idea introduced in (Natarajan et al., 2013), therefore they would naturally re-
quire the knowledge of the noise rates or transition matrix. We do provide thorough comparisons
between peer loss and the unbiased surrogate loss methods.
Mostly relevant to us is a recent work (Xu et al., 2019) that proposes an information theoretical loss
(an idea adapted from an earlier theoretical contribution (Kong & Schoenebeck, 2018)) that is also
robust to asymmetric noises rate. We aimed for a simple-to-optimize loss function that can easily
adapt to existing ERM solutions. (Xu et al., 2019) involves estimating a joint distribution matrix
between classifiers and noisy labels, and then invokes computing a certain information theoretical
measure based on this matrix. Therefore, its sample complexity requirement and the sensitivity to
noises in this estimation are not entirely clear to us (not provided in the paper either). We do provide
calibration guarantees and generalization bounds. We provide conditions when the loss functions
are convex. In general, we do think computationally peer loss functions are easy to optimize with,
in comparing to information theoretical measures. Experiments comparing with (Xu et al., 2019)
are also given in Section 5.
Peer Prediction Our work also builds on the literature for peer prediction (Prelec, 2004; Miller
et al., 2005; Witkowski & Parkes, 2012; Radanovic & Faltings, 2013; Witkowski et al., 2013; Das-
gupta & Ghosh, 2013; Shnayder et al., 2016; Liu & Chen, 2017). (Miller et al., 2005) established
that strictly proper scoring rule (Gneiting & Raftery, 2007) could be adopted to elicit truthful reports
from self-interested agents. Follow-up works that have been done to relax the assumptions imposed
(Witkowski & Parkes, 2012; Radanovic & Faltings, 2013; Witkowski et al., 2013; Radanovic et al.,
2016; Liu & Chen, 2017). Most relevant to us is (Dasgupta & Ghosh, 2013; Shnayder et al., 2016)
where a correlated agreement (CA) type of mechanism was proposed. CA evaluates a report’s cor-
relations with another reference agent - its specific form inspired our peer loss.
2	Preliminaries
Notations and preliminaries: For positive integer n, denote by [n] := {1, 2, ..., n}. Suppose
(X, Y ) ∈ X × Y are drawn from a joint distribution D, with their marginal distributions denoted
as PX, PY respectively. We assume X ⊆ Rd, and Y = {-1, +1}, that is we consider a binary
classification problem. Denote by p := P(Y = +1) ∈ (0, 1). There are N training samples
(x1, y1), ..., (xN, yN) drawn i.i.d. from D.
Instead of observing ynS, the learner can only collect a noisy set of training labels yn s, generated
according to yn s and a certain error rate model, that is we observe a dataset {(Xn,yn)}N=1. We
assume a uniform error model for all the training samples We collect, in that errors in yns follow the
same error rate model: denoting the random variable for noisy labels as Y and we denote e+ι :=
P(Y = —1|Y = +1), e-ι := P(Y = +1∣Y = —1) such that 0 ≤ e+ι + e-ι < 1. e-ι + e+ι < 1
is not unlike the condition imposed in the existing learning literature (Natarajan et al., 2013), and it
simply implies that the noisy labels are positively correlating with the true labels (informative about
the true labels). Label noises are conditional independent from the features, that is the error rate is
uniform across xns: P(Y = y0∣Y = y) = P(Y = y0∣X,Y = y),∀y,y0 ∈ {-1, +1}. Denote the
distribution of the noisy data (X, Y) as D.
f : X → R is a real-valued decision function, and its risk w.r.t. the 0-1 loss is defined as
E(x,y)~d[l(f (X),Y)]. The Bayes optimal classifier f * is the one that minimizes the 0-1 risk:
f * = argminf E(χ,γ)~d[l(f (X),Y)]. Denote this optimal risk as R*. Instead of minimizing the
above 0-1 risk, the learner often uses a surrogate loss function ` : R × {—1, +1} → R+, and find a
f ∈ F that minimizes the following error: E(χ,γ)~d ['(f (X), Y)]. Denote the following measures:
RD(f)= E(x,y)~d[l(f(X),Y)], R',d(f) = E(x,y)~d['(f(X),Y)].
3
Under review as a conference paper at ICLR 2020
When there is no confusion, We will also short-hand E(χ,γ)〜D['(f (X),Y)] as Ed['(f (X),Y)].
Using D to denote a dataset collected from distribution D (correspondingly D := {(xn, yn)}N=ι
for D), the empirical risk measure for f is defined as r`,d(f)=奇 E(X 期)三0 '(f (x), y).
2.1	Learning with noisy labels
Typical methods for learning with noisy labels include developing bias removal surrogates loss func-
tion methods to learn with noisy data (Natarajan et al., 2013). For instance, Natarajan et al. (2013)
tackle this problem by defining an “un-biased” surrogate loss functions over ` to help “remove”
noise, when e-ι + e+ι < 1: '(t, y) := (1-e-y)-et,y--ey：(t「y, ∀t, y. 1 is identified such that when
a prediction is evaluated against a noisy label using this surrogate loss function, the prediction is as
if evaluated against the ground-truth label using ` in expectation. Hence the loss of the prediction is
unbiased , that is ∀ prediction t, Eγ∣y ['(t, Y)] = '(t, y) [Lemma 1, (Natarajan et al., 2013)].
One important note to make is most, if not all, existing solutions require the knowledge of error
rates e-1, e+1. Previous works either assumed the knowledge of it, or needed additional clean
labels or redundant noisy labels to estimate them. This becomes the bottleneck of applying these
great techniques in practice. Our work is also motivated by the desire to remove this limitation.
2.2	Peer Prediction: Information Elicitation without Verification
Peer prediction is a technique developed to truthfully elicit information when there is no ground truth
verification. Suppose we are interested in eliciting private observations about a binary event y ∈
{-1, +1} generated according to a random variable Y. There are K agents indexed by [K]. Each
of them holds a noisy observation of y, denoted as y(i) ∈ {-1, +1}, i ∈ [K]. We would like to elicit
the y(i)s, but they are completely private and we won’t observe y to evaluate agents’ reports. Denote
by r(i) the reported data from each agent i. It is completely possible that r(i) 6= y(i) if agents are
not compensated properly for their information. Results in peer prediction have proposed scoring or
reward functions that evaluate an agent’s report using the reports of other peer agents. For example,
a peer prediction mechanism may reward agent i for her report r(i) using S(r(i), r(j)) where r(j)
is the report of a randomly selected reference agent j ∈ [K]\{i}. The scoring function S is designed
so that truth-telling is a strict Bayesian Nash Equilibrium (implying other agents truthfully report
their y(j)), that is, ∀i Ey(j)[S(y(i), y(j))|y(i)] > Ey(j)[S(r(i), y(j))|y(i)], ∀r(i) 6= y(i).
Correlated Agreement (Shnayder et al., 2016; Dasgupta & Ghosh, 2013) (CA) is a recently estab-
lished peer prediction mechanism for a multi-task setting 1. CA is also the core and the focus of our
subsequent sections on developing peer prediction based loss functions. This mechanism builds on
a ∆ matrix that captures the stochastic correlation between the two sources of predictions y(i) and
y(j). Denote the following mapping function: g(1) = -1, g(2) = +1, ∆ ∈ R2×2 is then defined as
a squared matrix with its entries defined as follows:
∆(k,l)= P(y(i) = g(k),y(j) = g(l)) - P(y(i) = g(k))P(y(j) = g(l)), k,l = 1, 2
The intuition of above ∆ matrix is that each (i, j ) entry of ∆ captures the marginal correla-
tion between the two predictions. M ∈ R2×2 is defined as the sign matrix of ∆: M :=
Sgn(∆), where Sgn(x) = 1, x > 0; Sgn(x) = 0, o.w. Define the following score matrix
MS ： {-i,+i} × {-1, +1} → {0,1} ： MS(y,y0) =： M(g-1(y),g-1(y0)),	⑴
where g-1 is the inverse function of g. CA requires each agent i to perform multiple tasks: denote
agent i's observations for the N tasks as yι(i), ...,yN(i). Ultimately the scoring function S(∙) for
each task k that is shared between i, j is defined as follows: randomly draw two other tasks k1p, k2p,
S(yk(i),yk(j)) ：=MS(yk(D,y(j)) - MS(ykp(i),ykp(j)), k1 = kp = k
Note a key difference between the first and second MS terms is that the second term is defined for
two independent peer tasks k1p, k2p (as the reference answers). It was established in (Shnayder et al.,
1We provide other examples of peer prediction functions in the Appendix.
4
Under review as a conference paper at ICLR 2020
2016) that CA is truthful and proper (Theorem 5.2, Shnayder et al. (2016).) 2; in particular, if y(j)
is categorical w.r.t. y(i): P(y(j) = y0∣y(i) = y) < P(y(j) = y0), ∀i,j ∈ [K], y0 = y then S(∙) is
strictly truthful (Theorem 4.4, Shnayder et al. (2016)).
3	Learning with noisy data: a peer prediction approach
In this section, we show that peer prediction scoring functions, when specified properly, will adopt
Bayes optimal classifier as their maximizers (or minimizers for the corresponding loss form).
3.1	Learning with noisy data as an elicitation problem
We first state our problem of learning with noisy labels as a peer prediction problem. The connec-
tion is made by firstly rephrasing the two data sources, the classifiers and the noisy labels, from
agents’ perspective. For a task y ∈ {-1, +1}, say +1 for example, denote the noisy labels Y
as r(X),X 〜 Pχ∣γ=ι. In general, r(X) can be interpreted as the agent that observes yι, ...,yN
for a set of randomly drawn feature vectors xι,..., XN: yn 〜r(X). Suppose the agent,s obser-
vations are defined as follows (similar to the definition of e+1, e-): P(r(X) = -1|Y = +1) =
e+1, P(r(X) = +1|Y = -1) = e-1. Denote another agent whose observations “mimic” the Bayes
optimal classifier f *. Again denote this optimal classifier agent as r*(X):= f *(X):
PX(r*(X) = -1|Y = +1) = e+ι, PX(r*(X) = +1∣Y = -1) = e-ι
Suppose we would like to elicit predictions
from the optimal classifier agent r*, while
the reports from the noisy label agent r will
serve as the reference reports. Both r and
r* are randomly assigned a task x, and each
of them observes a signal r(x) and r*(x) re-
spectively. Denote the report from agent r * as
r*. A scoring function S : R X R → R is
called to induce strictly truthfulness if the fol-
lowing fact holds: EX [S(r*(X),r(X))] >
EX [S(r*,r(X))], ∀r* = r*(X). Taking the
negative of S(∙) (changing a reward score one
aims to maximize to a loss to minimize) we also
Reference report as the noisy label
S(y(i),y(j))	琦 eer(K∕(X))
MaX reward = MiII IOSS	[
Elicited report as the classifier prediction
Figure 1: Illustration of our idea. S is the peer pre-
diction function; our `peer is to “evaluate” a classi-
fier’s prediction using a noisy reference.
have EX [-S(r*(X),r(X))] < EX [-S(r*,r(X))], ∀r* = r*(X), implying when taking -S(∙)
as the loss function, minimizing -S(∙) w.r.t. R will return US the Bayes optimal classifier f *. Our
idea can be summarized easily using Fig. 1.
3.2	“Proper” peer prediction function induced Bayes optimal classifier
When there is no ambiguity, we will shorthand r(X), r*(X) as r, r*, with keeping in mind that r, r*
encode the randomness in X. Suppose S(∙) is able to elicit the Bayes optimal classifier f * (agent
r* ) using r, we have the following theorem formally:
Theorem 1. f* = argminf E(X,Y)〜D [-Sf(X), r)].
This proof can be done via showing that any non-optimal Bayes classifier corresponds to a mis-
reporting strategy, thus establishing its non-optimality. We emphasize that it is not super restrictive
to have a strictly truthful peer prediction scoring function S. We provide discussions in Appendix.
Theorem 1 provides a conceptual connection and can serve as an anchor point when connecting
a peer prediction score function to the problem of learning with noisy labels. So far we have not
discussed about a specific form of how we construct a loss function using ideas from peer prediction,
and have not mentioned the requirement of knowing the noise rates. We will provide the detail about
a particular peer loss in next section, and explain its independence of noise rates.
2To be precise, it is an informed truthfulness. We refer interested readers to (Shnayder et al., 2016) for the
detailed differences.
5
Under review as a conference paper at ICLR 2020
4	Peer Loss Function
We now present peer loss, a family of loss functions inspired by a particular peer prediction mecha-
nism, the correlated agreement (CA), as presented in Section 2.2. We are going to show that peer loss
is able to induce the minimizer of a concept class F, under a broad set of non-restrictive conditions.
In this Section, we do not restrict to Bayes optimal classifiers, nor do we impose any restrictions on
the loss functions’ elicitation power.
4.1	Preparation: explaining CA in our classification problem
To give a gentle start, we repeat the setting of CA for our classification problem.
∆ and scoring matrix First recall that ∆ ∈ R2×2 is a squared matrix with entries defined between
r* (the f *) and r (i.e., the noisy labels Y):
δ^ I) = P(f*(X) = g(k),Y = g(I)) - P(f *(X)= g(k))P(Y = g(I)), k,l = 1, 2
Recall g(∙) is simply a mapping function: g(1) = -1, g(2) = +1. ∆ characterizes the “marginal”
correlations between the optimal classifier, prediction and the noisy label Y. Then the following
scoring matrix M ∈ Rn×n, sign matrix of ∆, M := Sgn(∆) is computed.
Example 1. Consider a binary class label case: P(Y = -1) = 0.4, P(Y = +1) = 0.6,
the noises in the labels are e-1 = 0.3, e+1
have ∆(1, 1) = 0.036,
∆	0.036 -0.036
∆ = -0.036 0.036
∆(1, 2) = -0.036,
⇒ M = Sgn(∆) =
= 0.4 and e*-1 = 0.2, e*+1 = 0.3. Then we
∆(2, 1) = -0.036, ∆(2, 2) = 0.036. And:
10
01.
Peer samples For each sample (xi, yi), randomly draw another two samples (Xip,yip), (Xip, y说)
such that ip = ip and ip, ip = i. We will name (Xip,yip), (Xip,yip) as i,s peer samples. After
pairing Xip with yip (two independent tasks), the scoring function S(∙) for each sample point Xi is
defined as follows: S(f(Xi),yi)) = MS(f (Xi),yi) - MSU(Xip),y∣ip). Recall MS(∙) is a sign
score matrix defined for ∆ (Eqn. (1)). Define loss function '(∙) as the negative of S(∙):
(Generic Peer Loss) '(f (Xi),yn) := (1 - MS(f (Xi),yi)) -(1 - MS(f (Xip)加)).(2)
The first term above evaluates the classifier,s prediction on Xi using noisy label yi, and the second
“peer” term defined on two independent tasks ip1, ip2 “punishes” the classifier from overly agreeing
with the noisy labels. We will see this effect more clearly. According to Theorem 1, minimizing '(∙)
is going to find the Bayes optimal classifier, ifY and f* are categorical, which is easily satisfied:
Lemma 1. When e-ι + e+ι < 1 and e-1 + e+1 < 1, r and r* (Y and f *) are categorical.
e*-1 + e*+1 < 1 means that the optimal classifier is at least informative ((Liu & Chen, 2017)) - if
otherwise, we can flip the classifier,s output to obtain one.
4.2	Peer Loss
We need to know Sgn(∆) in order to specify MS and ', which requires certain information about
f* and Y. We show that for the cases that the literature is broadly interested in, Sgn(∆) is simply
the identify matrix (same condition as stated in Lemma 1):
Lemma 2. If e-1 + e+1 < 1, e*-1 + e*+1 < 1, then Sgn(∆) = I2×2, i.e., the identity matrix.
This is basically stating that for ∆(k, k), k = 1, 2, f* and Y are positively correlating, so the
marginal correlation is positive; while for off-diagonal entries, they are negatively correlating.
TΓ⅛ 1	-c-r Tl C / Λ ∖ T	7l∕,q/	/ ∖	1 ∙ i'	/	1 /ʌ ,1	∙	7∕∖ FC 1 ∙ Tn
Peer loss When Sgn(∆) = I2×2, MS (y, y ) = 1 if y = y , and 0 otherwise. '(∙) defined in Eqn.
(2) reduces to the following form:
lpeer(f(Xi),yi) = l(f(g),*) - l(f (Xip )百说)	(3)
6
Under review as a conference paper at ICLR 2020
To see this, for instance 1 - MS(f (xi) = +1, yi = +1)=1 - M(2,2) = 1 - 1 = 0 = l(f (xi)=
-1,yi = +1). Replacing 1(∙) with any generic loss '(∙) We define:
(PeerLoss) : 'peer(f(Xi),yi) = '(f(χi),yi) -'(f(χip),yip)	(4)
We name above loss as peer loss. This strikingly simple form of 'peer(f (χi),yi) implies that knowing
e-ι + e+ι < 1, e-ι + ejι < 1 hold is all we need to specify 'peer.
Why do We not need the knowledge of noise rates explicitly? Both of the terms l(f (xi), y%)
and l(f (Xip), yip) encoded the knowledge of noise rates implicitly. The carefully constructed form
as presented in Eqn. 3 allows peer loss to be invariant against noises (Lemma 3, a property we will
explain later). For a preview, for example if we take expectation of IPeer (f (Xi) = +1, yi = +1) We
一 ... ., . ~ . . . . ~
will have E [1 peer(f (Xi) = +1, yi = +1)] = P(f (X) = +1, Y = +1) - P(f (X) = +1) ∙ P(Y =
+1), the marginal correlation between f and Y , which is exactly capturing the entries of ∆ defined
between f and Y! The second term above is a product of marginals because of the independence
of peer samples ip1, i2p. Using the sign of ∆ is all we need to recover this information measure
in expectation. In other words, both the joint and marginal distribution terms encode the noise
rate information in an implicit way. Later we will show this measure is invariant under label noises,
which gives us the property of peer loss being invariant to label noises and the ability of dropping the
requirement of knowing noise rates. We will instantiate this argument formally with Lemma 3 and
establish a link between the above measure and the true risk ofa classifier on the clean distribution.
The rest of presentation focuses on 'peer (Eqn. (4)), but 'peer recovers IPeer Via replacing ' with 1.
ERM With Peer loss fpeer = argminf∈F 瓦Peer,D(f) = argminf∈F NN Pn=I 'peer(f (Xn),yn).
Note again that the definition of 'peer does not require the knowledge of either e+1,e-1 or e*ι, e-：
4.3	Property of Peer Loss
We now present a key property of peer loss, which shows that its risk over the noisy labels is simply
an affine transformation of its true risk on clean data. We denote by ED ['peer (f (X), Y)] the expected
peer loss of f when (X, Y), as well as its peer samples, are drawn i.i.d. from distribution D.
.	一	-.	,____ r	,	.	一	-	.	,	.	__ ___
Lemma 3. EDD ['Peer(f (X),Y)] = (1 -	e-1 - e+l) ∙ ED ['Peer(f (X),Y)].
The above Lemma states	that peer loss is invariant to label noises in expectation.	We have also
empirically observed this	effect in our	experiment. Therefore minimizing	it	over noisy labels	is
equivalent to minimizing over the true distribution. The Theorems below establish the connection
between ED ['peer(f(X), Y)], the expected peer loss over clean data, with the true risk: Denote
fipeer = arg minf ∈τ RIPeer DD(f). With Lemma 3, we can easily prove the following:
Theorem 2. [Optimality guarantee With equal prior] When P = 0.5, f ^eeF ∈ arg minf ∈f RD (f).
The above theorem states that for a class-balanced dataset with p = 0.5, peer loss induces the same
minimizer as the one that minimizes the 0-1 loss on the clean data. Removing the constraint of
F, i.e., fɪpeer = arg minf RlPeer D(f) ⇒ fReer = f *. In practice we can balance the dataset s.t.
p → 0.5. When p 6= 0.5, denote ∆p = P(Y = +1) - P(Y = -1), we have the following theorem:
Theorem 3. [Approximate optimality guarantee with unequal prior] When p 6= 0.5, suppose the
following conditions hold: (1) e-1,e+1 < 0.5 ;(2) (1-e)∙e-1+e∙e+1 > e ;(3) (1-e)∙e+1+e∙e-1 >
e, where e ：= 1 -卤.Then |Rd(fPeer) - minf∈FRD(f )| ≤ 2e(2 - '), Ve ≤ ∣∆p∣∕2, if ' is
bounded with ' ' denoting its max and min.
Condition (1) is a well-adopted assumption in the literature of learning with noisy labels. When
e+1, e-ι > e, we have conditions (2) and (3) hold: (1 - e) ∙ e-ι + e ∙ e+ι > (1 - e) ∙ e + e ∙ e =
e, (1 - e) ∙ e+ι + e ∙ e-ι > (1 - e) ∙ e + e ∙ e = e. When ∣∆p∣ is small, i.e., P is closer to 0.5, this
condition becomes weaker, as we will afford to have a small e but also a small e.
Multi-class extension Our results in this section are largely generalizable to multi-class setting.
Suppose we have K classes of labels, denoting as {1, 2, ..., K}. We denote by Q a transition matrix
7
Under review as a conference paper at ICLR 2020
.i . i	,	.ι i	i ∙ i	∙ ii i Cr ι .ι . IK ι，z^ El / ∙ ∙ ∖ , c /ʌ
that characterizes the relationships between noisy label Y and the true label Y . The (i, j) entry of Q
is defined as Qij = P(Y = j|Y = i). We write Qij = qij . For many classes of noise matrices, the
M(∙) matrix is simply a diagonal matrix. Consider the following case: suppose the noisy labels have
uniform probability of flipping to a wrong class, that is, we pose the following conditions: qij = qik,
for all j 6= k 6= i. This condition allows us to define K new quantities ei = qij for all i 6= j , and
qii = 1 - Pj=分 ej. We show that M(∙) is a diagonal matrix when PK=I ej < 1, a similar condition
as e-1 + e+1 < 1. Adapting from our proof for Lemma 3, we also have (derivation provided in
APPendiX)EDD [lpeer(f (X), Y)] = (1 - Pj=I ej) ∙ ED [1 peer(f (X ),Y)]. The above again will help
us reach the conclusion that minimizing peer loss leads to the same minimizer on the clean data. We
provide experiment results for peer loss with multi-class labels in Section 5.
4.4	α-WEIGHTED PEER LOSS
We take a further look at the case with p 6= 0.5. Denote by R+1(f) = P(f(X) = -1|y =
+1), R-1(f) = P(f(X) = +1|y = -1). It is easy to prove:
Lemma 4. Minimizing E[Ipeer(f (X),Y)] is equivalent to minimizing R-ι(f) + R+ι(f)∙
However, minimizing the true risk RD (f) is equivalent to minimizing p∙ R+ι(f ) + (1-p)∙ R- ι(f), a
weighted sumofR+1(f) and R-1(f). The above observation and the failure to reproduce the strong
theoretical guarantee when p 6= 0.5 motivated us to study a α-weighted version of peer loss, to make
it robust to the case p 6= 0.5. We propose the following α-weighted peer loss via adding a weight
α ≥ 0 to the second term, the peer term:
(α-Peer Loss) : 'α-peer(f(xi), yi) = '(f(xi),yi) - α ∙ '(f (xip),yip)	(5)
Denote Ia-Peer as 'α-peer When replacing ' with 1, ffα Peer = arg mmf∈τ RIa 喇 D (f) as the optimal
classifier under Ia-Peer, and ∆p = P(Y = +1) - P(Y = -1). Then We have:
∆
Theorem 4. Let α = 1 - (1 - e-i - e+ι) ∙忘.Then fL-peer ∈ argmm,∈f RD(f).
Denote αf := 1 - (1 - e-ι - e+ι) ∙ ∆p. Several remarks follow: (1) When P = 0.5, We have
αf = 1, we recover the earlier definition of`peer. (2) When e-1 = e+1, αf = 0, we recover ` for the
clean learning setting. (3) When the signs of P(Y = 1) - P(Y = -1) and P(Y = 1) - P(Y = -1)
are the same, αf < 1. Otherwise, αf > 1. In other words, when the noise changes the relative
quantitative relationship of P(Y = 1) and P(Y = -1), αf > 1 and vice versa. (4) Knowing αf
requires certain knowledge ofe+1, e-1 when p 6= 0.5. Though we do not claim this knowledge, this
result implies tuning αf (using validation data) may improve the performance.
Theorem 2 and 4 imply that performing ERM with la*-peeι∙: ff * Cer = arg mmf Rl * D (f) will
lead to a classifier converging to ff:
Theorem 5. With probability at least 1 一 δ, RD (ff *	) 一 Rf ≤ j(1+-/	.
α -peer	1-e-1 -e+1	2N
4.5	Calibration and Generalization
So far our results focused on minimizing 0-1 losses, which is hard in practice. We provide evi-
dences of 'peer's, and 'a-peer's in general, calibration and convexity with a generic and differentiable
calibrated loss. We consider a ` that is classification calibrated, convex and L-Liptchitz.
Classification calibration describes the property that the convergence to optimality using a loss
function ' would also guarantee the convergence to optimality with 0-1 loss:
Definition 1. ' is classification calibrated if there ∃ a convex, invertible, nondecreasing transfor-
mation Ψ' with Ψ'(0) = 0 s.t. Ψ'(RD(f) - R ) ≤ R',d(f) - mmf R',d(f).
Denote f' ∈ arg minf r`,d (f). Below we provide sufficient conditions for 'α-peer to be calibrated.
Theorem 6. 'a-peer is classification calibrated when either of the following two conditions holds:
(1) α = 1 (i.e., 'α-peer = 'peer), P = 0.5, and f' satisfies the following: E['(ff(X), -Y)] ≥
E['(f (X), -Y)], ∀f. (2) α < 1, max{e+1, e-1} < 0.5, and '00(t, y) = '00(t, -y).
8
Under review as a conference paper at ICLR 2020
(1) states that f' not only achieves the smallest risk over (X, Y) but also performs the worst on the
“opposite” distribution with flipped labels (X, -Y). (2) '00(t, y) = '00(t, -y) is satisfied by some
common loss function, such as square losses and logistic losses, as noted in (Natarajan et al., 2013),
Under the calibration condition, and denote the corresponding calibration function for 'α-peer as
ψ'a-peer. DenOteby fa-peer = argminf∈F 瓦α-peer,D(f):= N PN=I 'ɑ-peer(f (Xn),yn). WehaVethe
following generalization bound:
Theorem 7. The following generaIizatiOn bound holdsfor 'α* -Peer With probability at least 1 一 δ:
RD (f` *	) — R ≤ --------------∙ Ψ-1*	min r`	D (f) — min R?	D (f)
' %* -Peer '	一 ] — e-1 — 6+]	`a* -peer ∖f ∈F 'α* -Peer, D ' / f 'a* -Peer, D ' /
+ 2(1 + α*)L ∙ <(F) + 2jl2gNδ O + (1 +。*)促—')))
where <(F) is Rademacher comPlexity ofF.
Convexity In experiments, we use neural networks which are more robust to non-convex loss
functions. We provide sufficient conditions for R% P r D(f) to be convex in Appendix (Lemma 8).
5	Experiments
We implemented a two-layer ReLU Multi-Layer Perceptron (MLP) for classification tasks on 10
UCI Benchmarks and applied our peer loss to update their parameters. We show the robustness of
peer loss with increasing rates of label noises on 10 real-world datasets. We compare the perfor-
mance of our peer loss based method with surrogate loss method (Natarajan et al., 2013) (unbiased
loss correction with known error rates), symmetric loss method (Ghosh et al., 2015), DMI (Xu et al.,
2019), C-SVM (Liu et al., 2003) and PAM (Khardon & Wachman, 2007), which are state-of-the-art
methods for dealing with random binary-classification noises, as well as a neural network solution
with binary cross entropy loss (NN). We use a cross-validation set to tune the parameters specific to
the algorithms. For surrogate loss, we use the true error rates e-1 and e+1 instead of learning them
on the validation set. Thus, surrogate loss could be considered a favored and advantaged baseline
method. Accuracy of a classification algorithm is defined as the fraction of examples in the test set
classified correctly with respect to the clean and true label. For given noise rates e+1 and e-1, labels
of the training data are flipped accordingly.
episodes (x20)
(a) Splice (e-1 = 0.4, e+1 = 0.4)
Accuracy During Testing
(b) Twonorm (e-1 = 0.4, e+1 = 0.4)
Figure 2: Accuracy on test set during training
A subset of the experiment results are shown in Table 1. A full table with all details can be found
in Appendix. Equalized Prior means that we pre-sample the dataset to guarantee p = 0.5. For this
case We used 'peer without a (or rather α = 1 as in 'α-peer). For P = 0.5, We use validation dataset
(using noisy labels) to tune α. Our method is competitive across all datasets and is even able to
outperform the surrogate loss method with access to the true error rates in a number of datasets, as
well as symmetric loss functions (which does not require the knowledge of noise rates when error
rates are symmetric) and the recently proposed information theoretical loss (Xu et al., 2019). Fig. 2
shows that our method can prevent over-fitting when facing noisy labels.
9
Under review as a conference paper at ICLR 2020
	Task			With Prior Equalization P =				0.5	Without Prior Equalization p = 0.5				
(d,N+,N-)	e-i,e+ι	Peer	Surr	Symm	DMI	NN	Peer	Surr	Symm	DMI	NN
	0.1,0.3	0.977	0.968	0.969	0.974	0.964	0.977	0.968	0.969	0.974	0.964
Twonorm	0.2, 0.4	0.976	0.919	0.959	0.966	0.911	0.976	0.919	0.959	0.966	0.911
(20,3700,3700)	0.4, 0.4	0.973	0.934	0.958	0.936	0.883	0.973	0.934	0.958	0.936	0.883
	0.1,0.3	0.919	0.878	0.851	0.875	0.811	0.925	0.885	0.868	0.889	0.809
Splice	0.2, 0.4	0.901	0.832	0.757	0.801	0.714	0.912	0.84	0.782	0.81	0.725
(60,1527,1648)	0.4, 0.4	0.819	0.754	0.657	0.66	0.626	0.822	0.755	0.674	0.647	0.601
	0.1,0.3	0.833	0.78	0.777	0.797	0.756	0.856	0.802	0.803	0.83	0.75
Heart	0.2, 0.4	0.812	0.768	0.717	0.788	0.679	0.856	0.758	0.725	0.797	0.693
(13,165,138)	0.4, 0.4	0.75	0.729	0.654	0.69	0.595	0.785	0.728	0.686	0.711	0.554
	0.1,0.3	0.745	0.707	0.674	0.72	0.667	0.778	0.75	0.738	0.729	0.727
Diabetes	0.2, 0.4	0.755	0.681	0.634	0.682	0.596	0.739	0.705	0.695	0.707	0.672
(8,268,500)	0.4, 0.4	0.719	0.645	0.619	0.637	0.551	0.651	0.685	0.68	0.633	0.583
	0.1,0.3	0.639	0.563	0.507	0.529	0.519	0.727	0.645	0.709	0.666	0.648
Breast	0.2, 0.4	0.63	0.534	0.482	0.496	0.538	0.73	0.674	0.666	0.58	0.672
(9,85,201)	0.4, 0.4	0.596	0.519	0.504	0.526	0.471	0.677	0.628	0.545	0.537	0.529
	0.1,0.3	0.928	0.922	0.924	0.934	0.873	0.956	0.949	0.943	0.954	0.92
Breast	0.2, 0.4	0.93	0.885	0.844	0.89	0.844	0.933	0.898	0.898	0.918	0.831
(30,212,357)	0.4, 0.4	0.928	0.867	0.819	0.746	0.824	0.908	0.839	0.817	0.795	0.673
	0.1,0.3	0.701	0.624	0.614	0.637	0.581	0.68	0.693	0.603	0.605	0.6
German	0.2, 0.4	0.664	0.59	0.6	0.618	0.572	0.676	0.681	0.537	0.573	0.535
(23,300,700)	0.4, 0.4	0.606	0.55	0.573	0.573	0.556	0.654	0.632	0.549	0.611	0.553
	0.1,0.3	0.89	0.895	0.892	0.856	0.868	0.893	0.898	0.883	0.785	0.863
Waveform	0.2, 0.4	0.881	0.89	0.828	0.835	0.81	0.884	0.884	0.745	0.761	0.837
(21,1647,3353)	0.4, 0.4	0.87	0.866	0.867	0.773	0.835	0.853	0.852	0.852	0.672	0.828
	0.1,0.3	0.906	0.9	0.89	0.87	0.909	0.943	0.909	0.897	0.811	0.93
Thyroid	0.2, 0.4	0.863	0.862	0.85	0.784	0.822	0.905	0.898	0.865	0.759	0.881
(5,65,150)	0.4, 0.4	0.762	0.738	0.859	0.788	0.764	0.769	0.818	0.876	0.738	0.738
	0.1,0.3	0.856	0.875	0.843	0.896	0.866	0.796	0.835	0.903	0.896	0.878
Image	0.2, 0.4	0.836	0.862	0.719	0.845	0.832	0.672	0.755	0.722	0.86	0.599
(18,1320,990)	0.4, 0.4	0.741	0.72	0.788	0.763	0.732	0.806	0.803	0.823	0.762	0.8
Table 1: Experiment results on 10 UCI Benchmarks (N+ , N- are the numbers of positive and negative sam-
ples). Surr: surrogate loss method (Natarajan et al., 2013); DMI: (Xu et al., 2019); Symm: symmetric loss
method (Ghosh et al., 2015). Entries within 2% from the best in each row are highlighted in bold. All results
are averaged across 8 random seeds. Neural-network-based methods (Peer, Surrogate, NN, Symmetric, DMI)
use the same hyper-parameters. Full table with complete set of comparisons is in Appendix.
Model	Error Rate e = 0.2	Error Rate e = 0.4
Cross Entropy	86.67	82.09
DMI(XU et al.,2019)	85711	81767
Peer Loss	87.72	83.81
Table 2: Accuracy on CIFAR-10.
Preliminary results on multi-class classification We now provide some preliminary results on
CIFAR-10 in Table 2. We followed the setup in (Xu et al., 2019) and used ResNet (He et al., 2016)
as the underlying optimization solution. However, different from (Xu et al., 2019) whose noise only
exists between specific class pairs, our noise is universal. For each class, we flip the label to any
other label with a probability of /9, where is the error rate and 9 is the number of other classes.
We do show peer loss is competitive against Cross Entropy and DMI (Xu et al., 2019). More results
and complete details are available in the Appendix.
Conclusion This paper introduces peer loss, a family of loss functions that enables training a
classifier over noisy labels, but without using explicit knowledge of the noise rates of labels. We
provide both theoretical justifications and extensive experimental evidences.
10
Under review as a conference paper at ICLR 2020
References
Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds.
Journal of the American Statistical Association,101(473):138-156, 2006.
Shai Ben-David, David Pal, and Shai Shalev-Shwartz. Agnostic online learning. In COLT2009.
Tom Bylander. Learning linear threshold functions in the presence of classification noise. In Pro-
ceedings of the seventh annual conference on Computational learning theory, pp. 340-347. ACM,
1994.
Nicolo Cesa-Bianchi, Eli Dichterman, Paul Fischer, Eli Shamir, and Hans Ulrich Simon. Sample-
efficient strategies for learning in the presence of noise. Journal of the ACM (JACM), 46(5):
684-719, 1999.
Nicolo Cesa-Bianchi, Shai Shalev-Shwartz, and Ohad Shamir. Online learning of noisy data. IEEE
Transactions on Information Theory, 57(12):7907-7931, 2011.
Nontawat Charoenphakdee, Jongyeong Lee, and Masashi Sugiyama. On symmetric losses for learn-
ing from corrupted labels. In International Conference on Machine Learning, pp. 961-970, 2019.
Anirban Dasgupta and Arpita Ghosh. Crowdsourced judgement elicitation with endogenous profi-
ciency. In Proceedings of the 22nd international conference on World Wide Web, pp. 319-330.
International World Wide Web Conferences Steering Committee, 2013.
Marthinus Christoffel Du Plessis, Gang Niu, and Masashi Sugiyama. Clustering unclustered data:
Unsupervised binary labeling of two datasets having different class balances. In 2013 Conference
on Technologies and Applications of Artificial Intelligence, pp. 1-6. IEEE, 2013.
Beno^t Frenay and Michel Verleysen. Classification in the presence of label noise: a survey. IEEE
transactions on neural networks and learning systems, 25(5):845-869, 2014.
Aritra Ghosh, Naresh Manwani, and PS Sastry. Making risk minimization tolerant to label noise.
Neurocomputing, 160:93-107, 2015.
Aritra Ghosh, Himanshu Kumar, and PS Sastry. Robust loss functions under label noise for deep
neural networks. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.
Tilmann Gneiting and Adrian E. Raftery. Strictly proper scoring rules, prediction, and estimation.
Journal of the American Statistical Association, 102(477):359-378, 2007.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In
Advances in neural information processing systems, pp. 8527-8537, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Roni Khardon and Gabriel Wachman. Noise tolerant variants of the perceptron algorithm. J.
Mach. Learn. Res., 8:227-248, May 2007. ISSN 1532-4435. URL http://dl.acm.org/
citation.cfm?id=1248659.1248667.
Yuqing Kong and Grant Schoenebeck. Water from two rocks: Maximizing the mutual informa-
tion. In Proceedings of the 2018 ACM Conference on Economics and Computation, pp. 177-194.
ACM, 2018.
Yann LeCun, Y. Bengio, and Geoffrey Hinton. Deep learning. Nature, 521:436-44, 05 2015. doi:
10.1038/nature14539.
Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee, and Philip S. Yu. Building text classifiers using
positive and unlabeled examples. In Proceedings of the Third IEEE International Conference on
Data Mining, ICDM ’03, pp. 179-, Washington, DC, USA, 2003. IEEE Computer Society. ISBN
0-7695-1978-4. URL http://dl.acm.org/citation.cfm?id=951949.952139.
11
Under review as a conference paper at ICLR 2020
Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE
Transactions on pattern analysis and machine intelligence, 38(3):447-461, 2016.
Yang Liu and Yiling Chen. Machine Learning aided Peer Prediction. ACM EC, June 2017.
Nan Lu, Gang Niu, Aditya K Menon, and Masashi Sugiyama. On the minimal supervision for
training any binary classifier from only unlabeled data. arXiv preprint arXiv:1808.10585, 2018.
Naresh Manwani and PS Sastry. Noise tolerance under risk minimization. IEEE transactions on
cybernetics, 43(3):1146-1151, 2013.
Aditya Menon, Brendan Van Rooyen, Cheng Soon Ong, and Bob Williamson. Learning from cor-
rupted binary labels via class-probability estimation. In International Conference on Machine
Learning, pp. 125-134, 2015.
Nolan Miller, Paul Resnick, and Richard Zeckhauser. Eliciting informative feedback: The peer-
prediction method. Management Science, 51(9):1359 -1373, 2005.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In Advances in neural information processing systems, pp. 1196-1204, 2013.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1944-1952, 2017.
D.	Prelec. A bayesian truth serum for subjective data. Science, 306(5695):462-466, 2004.
G.	Radanovic and B. Faltings. A robust bayesian truth serum for non-binary signals. In Proceedings
of the 27th AAAI Conference on Artificial Intelligence, AAAI ’13, 2013.
Goran Radanovic, Boi Faltings, and Radu Jurca. Incentives for effort in crowdsourcing using the
peer truth serum. ACM Transactions on Intelligent Systems and Technology (TIST), 7(4):48, 2016.
Clayton Scott. A rate of convergence for mixture proportion estimation, with application to learning
from noisy labels. In AISTATS, 2015.
Clayton Scott, Gilles Blanchard, Gregory Handy, Sara Pozzi, and Marek Flaska. Classification with
asymmetric label noise: Consistency and maximal denoising. In COLT, pp. 489-511, 2013.
V.	Shnayder, A. Agarwal, R. Frongillo, and D. C. Parkes. Informed Truthfulness in Multi-Task Peer
Prediction. ACM EC, March 2016.
Victor Shnayder, Arpit Agarwal, Rafael Frongillo, and David C Parkes. Informed truthfulness in
multi-task peer prediction. In Proceedings of the 2016 ACM Conference on Economics and Com-
putation, pp. 179-196. ACM, 2016.
Hwanjun Song, Minseok Kim, and Jae-Gil Lee. Selfie: Refurbishing unclean samples for robust
deep learning. In International Conference on Machine Learning, pp. 5907-5915, 2019.
Guillaume Stempfel and Liva Ralaivola. Learning svms from sloppily labeled data. In International
Conference on Artificial Neural Networks, pp. 884-893. Springer, 2009.
Sainbayar Sukhbaatar and Rob Fergus. Learning from noisy labels with deep neural networks. arXiv
preprint arXiv:1406.2080, 2(3):4, 2014.
Brendan Van Rooyen, Aditya Menon, and Robert C Williamson. Learning with symmetric label
noise: The importance of being unhinged. In Advances in Neural Information Processing Systems,
pp. 10-18, 2015.
Brendan van Rooyen, Aditya Krishna Menon, and Robert C Williamson. An average classification
algorithm. arXiv preprint arXiv:1506.01520, 2015.
J.	Witkowski and D. Parkes. A robust bayesian truth serum for small populations. In Proceedings
of the 26th AAAI Conference on Artificial Intelligence, AAAI ’12, 2012.
12
Under review as a conference paper at ICLR 2020
Jens Witkowski, Yoram Bachrach, Peter Key, and David C. Parkes. Dwelling on the Negative:
Incentivizing Effort in Peer Prediction. In Proceedings of the 1st AAAI Conference on Human
Computation and Crowdsourcing (HCOMP’13), 2013.
Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy
labeled data for image classification. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 2691-2699, 2015.
YilUn Xu, Peng Cao, YUqing Kong, and YizhoU Wang. L_dmi: An information-theoretic noise-robust
loss function. NeurIPS, arXiv:1909.03388, 2019.
ZhilU Zhang and Mert R. SabUncU. Generalized cross entropy loss for training deep neUral networks
with noisy labels, 2018.
13
Under review as a conference paper at ICLR 2020
Illustration of our implementation of peer loss
Figure 3: Illustration of peer loss method.
Other peer prediction functions
Other notable examples include quadratic and logarithmic scoring function, defined as follows:
Example 2.	Quadratic scoring function:
S(r(i),r(j)) =2P(y(j)= r(j)∣y(i)=/⑺)- X	P(y(j) = s∣y(i) = r(i))2,
s∈{-1,+1}
Example 3.	Logarithmic scoring function:
S(r(i),r(j)) ：= logP(y(j) = r(j)∣y(i) = r(i)).
We know the following is true:
Lemma 5 (Miller et al. (2005)). S defined in Example 1 & 2 induce strict truthfulness when y(i)
and y(j ) are stochastically relevant.
with defining stochastic relevance as follows:
Definition 2. y(i) and y(j) are stochastically relevant if ∃ s ∈ {-1, +1} s.t.
P(y(j) = s∣y(i) = +1) = P(y(j) = s∣y(i) = -1).
Similarly We conclude that when r and r* are stochastic relevant, the correlated agreement scoring
rule, quadratic scoring rule and logarithmic scoring rule are strictly truthful. This stochastic rele-
vance condition essentially states that the optimal classifier is statistically different from the noisy
data source r on some signals. Stochastic relevance is further satisfied in the binary classification
setting when e-ι + ejι = 1, under the assumption that e-ι + e+ι < 1, as similarly imposed in
learning with noisy labels literature (Scott et al., 2013; Natarajan et al., 2013; Scott, 2015).
Lemma 6. r and r* are StOChaStiCauy relevant ifand only if e-ι + ejι = L
Proof. Since r* can be written as a function of X and Y, due to conditional independence between
r and X (conditional on Y), by chain rule
P(r* = -1,r = +1) = P(Y = +1)(1 - e+Mι + P(Y = -1)e-i ∙ (1 - e-ι)
Since
P(r = +1) = P(Y = +1)(1 — e+ι) + P(Y = —1) ∙ e-ι
P(r* = +1) = P(Y = +1)(1 — e+ι) + P(Y = —1) ∙ e-ι
14
Under review as a conference paper at ICLR 2020
We have
P(r* =+1, r = -1) - P(r* = +1)P(r = -1)
=-P(Y = +1)P(Y = -1)(1 - e+1 - e-i)(1 - e+1 - e-1)	(6)
For the binary signal case, the condition for stochastic relevance writes as follows:
P(r = +1∣r* = +1) = P(r = +1∣r* = -1)
P(r = +1,r* = +1)
⇔	P(r* = +1)	=
P(r = +1,r* = -1)
P(r* = -1)
⇔P(r = +1, r* =	+1)P(r* = -1) = P(r = +1, r* = -1)P(r* = +1)
⇔P(r = +1, r* =	-1)	=	P(r = +1) ∙ P(r*	=	-1)
⇔P(r = +1, r* =	-1)	=	P(r = +1) ∙ P(r*	=	-1)
⇔e-1 + e+ι = 1,
where the last step is a consequence of Eqn.(6).
□
Proof for Theorem 1
Proof. It is equivalent to prove f * = argmaxf E(X P)〜D [S(f (X), r)]. First S(∙) is able to elicit
the Bayes optimal classifier f * (r*) using r implies that:
ED|y=+ι[S(r*,r)] > ED∣y=+ι[S(r*,r)], ∀r* = r*
EDIy=-1 [S(r*, r)] > E25∣κ =-1[S(r*,r)], ∀r* = r*
First note that the expected score of a classifier over the data distribution further writes as follows:
ED [S(f (X), r)] = p ∙ ED∣y『[S(f (X), r)] +(1 - p) ∙ EDIy=-1 [S(f(X), r])
Denote by f a sub-optimal classifier that disagrees with f * on set XdS = {χ∣Y = +1 : f0(χ)=
f *(χ)}. By sub-optimality of f we know that E := PX(X ∈ XdiS) > 0, as a zero measure X+s
does not affect its optimality. Construct the following reporting strategy that
r* = J r*, w.p.1 - E
—I -r*, w.p. E
Not hard to check that
EDIy=+ι [S(f Z(X), r)] = ED∣y=+ι [S(r*, r)]
Yet we have the following fact that
EDIy=+1 [S (r*,r)]
=(1 - e) ∙ ED∣y=+ι[S(f *(X),r)]
+ e ∙ ED∣y=+ι[S(-f *(X),r)]
<ED∣y=+1 [S (f *(X ),r)]	⑺
where the inequality is due to strict truthfulness of S and the fact that e > 0. We similarly conclude
that
ED∣y=-1 [S(r*, r)] < E25∣y =-1[S(f*(X), r)]	(8)
Combine Eqn. (7) and (8) we conclude the proof.	□
15
Under review as a conference paper at ICLR 2020
Proof for Lemma 1
Proof. Being categorical means
P(r = -y|r* = y) < P(r = -y), y ∈ {-1, +1}
which further implies
P(r = -y, r* = y) < P(r = -y)P(r* = y), y ∈ {-1, +1}
and
P(r = y,r* = y) > P(r = y)P(r* = y), y ∈ {-1, +1}.
Consider the following fact
P(r = +1,r* = +1)
=P(Y = +1)P(r = +1,r* = +1∣Y = +1)
+ P(Y = -1)P(r = +1,r* = +1∣Y = -1)
=P(Y = +1)P(r = +1∣r* = +1,Y = +1)
. P(r* = +1∣Y = +1)
+P(Y = -1)P(r = +1∣r* = +1,Y = -1)
. P(r* = +1∣Y = -1)
Since r* can be written as a function of X and Y, due to conditional independence between r and
X (conditional on Y) we have
P(r = +1∣r* = +1, Y = +1) = P(r = +1∣Y = +1) = 1 — e+ι,
P(r = +1∣r* = +1, Y = -1) = P(r = +1∣Y = -1) = e-ι
Therefore
P(r = +1, r* = +1) = P(Y = +1)(1 - e+1)(1 - e+1) + P(Y = -1) ∙ e-i ∙ e-1
We also have
P(r = +1) = P(Y = +1)(1 - e+ι) + P(Y = -1) ∙ e-i
P(r* = +1) = P(Y = +1)(1 - e+i) + P(Y = -1) ∙ e-i
Then we have
P(r = +1,r* = +1) - P(r = +1)P(r* = +1)
=P(Y = +1)P(Y = -1)(1 - e+i - e-ι)(1 - e+i - e-i)
>0
when 1 > eji + e-1.	□
Proof for Lemma 2
Proof. Again recall that
P(r* =+1,r = +1) = P(Y = +1)(1 - e+i)(1 - eji) + P(Y = -1)e-i ∙ e-i
P(r = +1) = P(Y = +1)(1 — e+i) + P(Y = -1) ∙ e-i
P(r* = +1) = P(Y = +1)(1 - e+i) + P(Y = -1) ∙ e-i
Then we have
P(r* = +1,r = +1) - P(r* = +1)P(r = +1)
=P(Y = +1)P(Y = -1)(1 - e+i - e-i)(1 - e+i - e-i)
>0
when 1 - e+i - e-i > 0, 1 - e+i - e-i > 0. Interestingly this coincides with the condition
imposed in (Natarajan et al., 2013). Similarly we can prove that
P(r* = +1, r = -1) — P(r* = +1)P(r = -1)
=-P(Y = +1)P(Y = -1)(1 - e+i - e-i)(1 - e+i - e-i)
<0
The other entries for P(r* = -1, r = -1) — P(r* = -1)P(r = -1) and P(r* = -1, r =
+1) — P(r* = -1)P(r = +1) are symmetric. Therefore the sign matrix of above score matrix is
exactly the diagonal matrix.	□
16
Under review as a conference paper at ICLR 2020
Proof for Lemma 3
Proof. We denote by Xip, Yip the random variable corresponding to the peer samples Xip, yip.
First we have
E['peer(f(X ),Y)]= E['(f(X ),Y)] - E['(f(Xip ),Yp )]
Consider the two terms on the RHS separately.
一 . . . ~ . 一
E['(f(X ),Y)]
=Eχ,y=τ [P(Y = -1|Y = -1) ∙ '(f (X),-1) + P(Y = +1∣Y = -1) ∙'(f(X),+1)]
+ EX,Y= + 1[P(Y = +1∣Y = +1) ∙'(f(X), +1) + P(Y = -1∣Y = +1) ∙0(f(X),-1)]
=Ex,y=-1 [(1 - e-1) ∙'(f(X),-1) + e-i∙'(f(X), +1)]
+ EX,Y=+i [(1 - e+1) ∙ 0 (J(X), +1) + e+1 ∙ ` (J(X), -1)]
=Ex,y=-1 [(1 - e-1 - e+1) ∙'(f(X),-1) + e+1 ∙ '(f (X),-1) + e7∙0(f(X), +1)]
+ Ex,y=+1 [(1 - e-1 - e+1) ∙0(f(X), +1) + e-1 ∙ 0(f(X), +1) + e+1 ∙ `(f(X),-1)]
=(1 - e-1 - e+1) ∙ Eχ,γ ['(f (X), y)] + Eχ [e+1 ∙ '(f(X),-1) + e7∙'(f(X),+1)]
And consider the second term:
一 . . . ~ . 一
E['(f(Xip ),Yip)]
- . . . ., .~ . , . , . ~ .
=Eχ[4(f(X), -1)] ∙ P(Y = -1) + Eχ['(f (X), +1)] ∙ P(Y = +1)
=Eχ [(e+1 p +(1 - e-1)(1 -p)) ∙'(f(X),-1) + ((1 - e+1)p + e-1(1 - p)) ∙ '(f (X), +1)]
=Eχ [(1 - e-1 - e+1)(1 -p) ∙ '(f(X),-1) + (1 - e-1 - e+1)p ∙0(f(X),+1)]
+ Eχ [(e+1p + e+1(1 - p)) ∙ '(f (X), -1) + (e-1(1 - p) + e-1p) ∙ '(f (X), +1)]
= (I- e-1 - e+1) ∙ Eχ ['(f (Xj ), Yfc)] + EX [e+1 ∙ '(f (X), -1) + e-1 ∙ 4(f (X), +1)]
Thus,
- ...~., ~., ~ ., . . . . .,
E['peer(f (X),Y)]= E['(f(X),Y)] - E['(f(Xj),K)] = (1 - e-1 - e+1)∙ E['Peer(f(X),Y)]
Multi-class extension Notice the following facts:
-...~., , ~ ., . ~.. . . ~
E[l(f(X), Y)] - E[l(f(Xip),Yip)] = P(l(f(X) = Y))- P(f(Xip) = Yip)
and
K
XP(Y = k)qjk = P(Y = j)(1 - X efc) + (1 - P(Y = j))ej = (1 - X efc)P(Y = j) + ej-
k=1	k=j	k
. . . . ~ ..
P(i(f (X ) = Y))
KK
=X P(Y = k) X P(f (X )= j ∣Y = k)qjk
k=1	j=1
KK
=XXP(f (X )= j∣Y = k)P(Y = k)qjk
KK
=XP(f(X) = j∣Y = j)P(Y = j)(1 - Xek) + XXP(f (X)= j∣Y = k)P(Y = k)ej
KK
=XP(f(X) = j∣Y = j)P(Y = j)(1 - Xek) + Xej (P(f (X)= j) - P(f (X)= j∣Y = j)P(Y = j))
KK
=(1 - X ek) X P(f (X)= j ∣Y = j)P(Y = j) + X ejP(f (X)= j)
k	j=1	j=1
17
Under review as a conference paper at ICLR 2020
Now consider the following
.. . ~ .
p(f(Xip ) = Kp)
K
=X P(f(x )=j)P(Y = j)
j=i
KK
=X P(f(x )=j) X P(Y = k)qjk
j=1	k=1
=X P(f (χ) = j) ((i - X eQP(y = j) + βj)
j=1	∖	k	)
Therefore
E[l(f(X),Y)]- E[l(f(Xip),Yip)]
. . . . ~ . . . . ~
=P(l(f(X )= Y)) - P(f(Xip )= Y)
K
=(1 - Xek) X (P(f(X) = j|Y = j)P(Y = j) - P(f(X)= j)P(Y = j))
k	j = 1
For clean labels we have
K
E[l(f (X), Y)] = X P(f (X) = j|Y = j )P(Y = j)
j=i
For the second term we have
K
E[l(f(Xip ),Yip)] = X P(f(X )= j)P(Y = j)
j=i
Therefore
E[l(f(X ),Y)] - E[l(f(Xip ),Yip)]
K
=XP(Y = k)(P(f(X) = j|Y = j)P(Y = j) - P(f(X) = j)P(Y = j))
j=i
We finish the proof.	□
Proof for Theorem 2
Proof. From Lemma 3 we know
一 ...~. 一
E['peer(f(X ),Y)]
= (1-e-1-e+1) ∙E['peer(f(X),Y)]
=(1 -e-1 -e+1) ∙(E[4(f(X ),Y)] - E['(f(Xip ),Yip)])
=(1 - e-1 - e+ι) ∙ E '(f(X), Y)] - 0.5 ∙ Eχ ['(f( X), -1)] - 0.5 ∙ EX ['(f( X), +1)fj
When ' is the 0-1 loss we have '(f (X), -1) + '(f (X), +1) = 1, Vx, and therefore
E['peer(f (X), Y)] = (1 - e—1 - e+ι) ∙(E[，(f (X), Y)] - 1)
With above we proved f £er ∈ arg minʃ∈τ RD (f).	□
18
Under review as a conference paper at ICLR 2020
Proof for Theorem 3
Proof. Our proof is inspired by our argument for P = 0.5. We ask the following question: if it is
possible to show that Y corresponds an error-flipped distribution of another distribution Y whose
marginals PY is close to or equal to 0.5. Observe the following: randomly flipping Y with probability
e uniformly, we will have a new distribution of labels Y that satisfies:
PY := P(Y = +1) = P(Y =	+1)	∙ (1 — e) + P(Y = —1) ∙ e = p(1 —	2e)	+ e.
Denote by E the tolerance of PY: E =	∣pγ	— 0.5|. When e sets to be: 1 — 2e	=	ɪ^r7,	we have
∖pγ — 0.5∣ = e. The next question we ask: is it possible to find parameters e_1, e+1:
,~ . ʌ , ~ . ʌ
P(Y = +1∣Y = —1) = ^-1, P(Y = —1∣Y = +1) = e+1
Note that
P(Y = —1∣Y = +1)
,~ . ʌ , ʌ
=P(Y = —1∣Y = +1) ∙ P(Y = +1∣Y = +1)
,~ . ʌ , ʌ
+ P(Y = —1∣Y = —1) ∙ P(Y = —1∣Y = +1)
=(1 — e) ∙ e+1 + e ∙ (1 — e_i)
Similarly P(Y = +1∣Y = —1) = (1 — e) ∙ e_1 + e ∙ (1 — e+1). Jointly we need the following
equations to hold:
(1 — e) ∙ e+1 + e ∙ (1 — e_i) = e+1
(1 — e) ∙ e-1 + e ∙ (1 — e+1) = e-1
Solving above equations we have
八 _ (1 — e) ∙ e_i + e ∙ e+ι	e
e-1 =	1 — 2e	1 — 2e
For a feasible solution to e_i, e+1, the conditions need to satisfy that (1) e_i, e+1 ≥ 0 and (2)
e_i + ^+1 < 1. First of all, from (2) we have
e ∙ (1 — (e-1 + e+1)) = e_1 — e_1
Then a necessary condition for e_1 + e+1 < 1 is
1	e_1
e_1 — e_1 > 0 ⇔ e_1 < 2 + 2(1 — 2e)
This condition holds as long as e_1, e+1 < 0.5. From e_1, e+1 ≥ 0 we have
(1 — e) ∙ e_1 + e ∙ e+1 > e, (1 — e) ∙ e+1 + e ∙ e_1 > e	(9)
This above jointly proves that R£a P r D (f) is equivalent to a peer loss defined over the noisy distri-
bution of y with error parameters e_1, e+1.
Denote by fF ∈ arg minʃ∈τ RD (f). From the optimality of fɪpeer we have
,~. , ~, .- , ~,	.-
RD(fipj — PY ∙ EX['(f'(X), +1)] — (1 — PY) ∙ EX[4(f∖(X), +1)]
≤ RD(疔)—Py ∙ EX['(疔(X), +1)] — (1 — PY) ∙ EX ['(疔(X), +1)]	(10)
Note ∀f:
忸Y ∙ EX['(f (X), +1)] + (1 — PY) ∙ EX['(f (X), +1)]	(11)
—0.5 ∙ EX['(f (X), +1)] — 0.5 ∙ EX['(f (X), —1)]∣
= ∣PY — 0.5∣∙∣ Ex ['(f (X), +1)] — EX ['(f (X), —1)] ∣
≤e 伍—')	(12)
19
Under review as a conference paper at ICLR 2020
Notice that
,~. , ~, , .- . . , ~,, .-
RD (启 Peer)- M EX 做居eer (X )，+i)] — (1 - PY ) - EX ['(启 Peer (X )，+i)]
≤ RD(fF) - Pγ ∙ EX['(fF(X), +1)] -(I- PY) ∙ EX['(fF(X), +1)]
≤ RD(疗)-0.5 ∙ EX ['(f⅛(X), +1)] - 0.5 ∙ EX ['(f⅛(X)，+1)] + E(C-')	(13)
Combining Eqn. (10, 12, 13) we have
Rd(几eer) - RD(f⅛) ≤Py - EX['(f (X), +1)] + (1 - PY) ∙ EX['(f(X), +1)]
-0.5 ∙ EX ['(f(X), +1)] - 0.5 ∙ EX ['(f (X),-1)]+ e(' - ')
≤2e(C - ')
□
Proof for Lemma 4
Proof.
-	~.-
E[lpeer(f(X ),Y)]
=(1 - e-1 - e+1) ∙ (P(f (X) = -1, Y = +1) + P(f (X) = +1, Y = -1)
-	P(f (X) = -1)P(Y = +1) - P(f (X) = +1)P(Y = -1))
=(1 - e-1 - e+1) ∙ (pR+1 + (1 - p)R-i
-	P ∙ P(f (X) = 1) - (1 - p) ∙ P(f (X) = -1))
=(1 - e-1 - e+1) ∙ (pR+1 + (1 - p)R-i
-P ∙ (PR+1 + (1 -P)(1 - R-I)) - (1 -P) ∙ (p(1 - R+i) + (1 - P)R-I))
=2(1 - e-i - e+i) ∙ p(1 - p) ∙ (R-i + R+i - 1)
□
Proof for Theorem 4
Proof.
- ~.-
E[l α-peer(f(X ),Y)]
- , , . ~ . . . . ~ .-
=E[l(f (X ),Y)] - α ∙ E[l(f (Xip ),Yip)]
=E[1 peer(f (X), Y)] + (1 - «) ∙ E[l(f (Xip ), Y )] - 1
=E[1 Peer(f(X), Y)] + (1 - «) ∙ (P(f (X) = -1) ∙ P(Y = -1) + P(f (X) = +1) ∙ P(Y = +1)) - 1
=E[1 Peer(f(X), Y)] + (1 - «) ∙ ((p ∙ (1 - R+i) + (1 -p) ∙ R-i) ∙ P(Y = -1)
+ (pR+i + (1 - p)(1 - R-i)) ∙ P(Y = +1)) - 1
- , , . ~.-. , ~ ~ ... . . .
=E[1 peer(f (X), Y)] + (1 -。)∙(P(Y = +1) - P(Y = -1)) ∙ (pR+i - (1 - P)R-I) + C
=2(1 - e-i - e+i) ∙ p(1 -p) ∙ (R-i + R+i - 1)
+ (1 - α) ∙ (P(Y = +1) - P(Y = -1)) ∙ (pR+i - (1 - p)R-i) + C
=R+i ∙(2(1 - e-i - e+i) ∙ p(1 - p) + (1 - α)p ∙ (P(Y = +1) - P(Y = -1)))
+ R-i ∙ (2(1 - e-i - e+i) ∙ p(1 - P)-(I- α)(1 - P) ∙ (P(Y = +1) - P(Y = -I))) + C0,
where C, C0 are constants:
C = (1 - α) ∙ ((1 - p) ∙ P(Y = +1) + P ∙ P(Y = -1)) - 1
C0 = C — 2(1 — e-i — e+i) ∙ p(1 — p)
20
Under review as a conference paper at ICLR 2020
Let
P _	2(1 - e-i - e+1) ∙ p(1 - p) + (1 - α) ∙ p ∙ (P(Y = +1) - P(Y = -1))
1 - P 2(1 - e-1 - e+ι) ∙ p(1 - p) - (1 - α) ∙ (1 - p) ∙ (P(Y = +1) - P(Y = -1))
that	ʌ
α = 1 - (1 - e-i - e+ι) ∙ —ɪ.
△p
we obtain that
E[l a-peer(f (X), Y)] = (1 - e-i - e+ι)E[l(f(X ),Y)]+ C 0,	(14)
concluding our proof. The last equation Eqn.(14) also implies the following proposition:
Proposition 8. For any f, f, we have
% [1 α -Peer (f(X ),丫)]一% [ɪ α -Peer (f KX ), Y)] = (1-e-1-£+1 )(E[l (f (X ),Y )]-E[l(f 0 (X ),Y )]).
□
Proof for Theorem 5
Proof. ∀f, using Hoeffding,s inequality with probability at least 1 - δ
IRIa-peer,D f )- RIFrQ f )1
≤(1 + α)
og 2/
2N
Note we also have the following:
τ*	^ ^ ^*	、 z? _ * *	∖
ɪ α-peer, D ʃɪ α-peer	Ba-Peer,D(f1α-peer )
/6	~∕^*	^	* *	_/£*	^	~∕^*
—	Ba-Pee口。( ʃɪɑ-peer	Ia-Pee口。( ʃɪɑ-peer	(	Ba-Peer,D( ∕lα-peeJ	Ia-Pee口。( ∕]Lα-peer))
I ^ ^	~ / f*	、 E> ~ 7* n
+ (RIα-peer,D (fα-peer)-RIα-peer,D (f1α-peer))
≤0 + 2max ∣ 冗
_	f 1'
a-peer,D (f )- 吗 a-peer,D (f )I
Now we show
,ʌ ,
RD (f"er)-
R*
_	, O ,	、	_	,.,	、
=RD (f1a*-peer)- RD (K/peer)	(TheOrem 4)
1
1 - e-i - e+ι
(Ri * D (f* *
' ɪa* -peer, j-y a
-peer
)-
R1 * D(f* *	))
ɪ a * -peer, j-y a -peer ，
(Proposition 8)
2
≤-------------
1 - e-1 - e+ι
max IRi * D(f)- ri * D(f )1
f	ɪa*-peer, j-y	ɪa* -peer,2 *-x
≤	2(1 + α*)
—1 - e-1 - e+ι
/lθg2∕δ
V 2N
We conclude the proof.
□
Proof for Theorem 6
Proof. We start with condition (1). From Lemma 3,
E['peer(f (X), Y)] =(1 - e-i - e+ι) ∙(E['(f (X ),Y)] - 0.5 ∙ E['(f (X),-1)] - 0.5 ∙ E['(f (X ),+1)]
21
Under review as a conference paper at ICLR 2020
The above further derives as
- ,,.~.-
E['peer(f(X ),Y)]
=(1 - e-1 - e+1) ∙(E[4(f (X), Y)] - 0.5 ∙ E['(f(X), Y)] - 0.5 ∙ E['(f(X),-Y)])
= 1-e-1-e+1 ∙ (E[,(f(X),Y)] - E['(f(X), -Y)])
Denote by C := I---2---we have
J	1-e—1-e+1
- , .- - ~ . , .-
E['(f(X ), Y )] = C ∙ E['peer(f (X ),Y)]+ E['(f (X ), -Y )]
Then
E['(f(X), Y)] - E[，(f；(X), Y)] - (E['(f(X),-Y)] - E['(f (Y), -Y))]
, - , , . ~.- - , . . ~.-.
=c ∙ (E[`peer(f (X),Y)] - E['pe∕f (X),Y)])
≤C ∙ (E['peer(f (X),Y)] - E['peer(/^(X),Y)])
Further by our conditions we know
E['(∕(X ),Y )]-E['(/(X ),Y)] - (E['(/(X), -Y)] - E['(/(Y),-Y))]
≥ E['(∕(X ),Y)] - E['(/(X ),Y)].
Therefore we have proved
E['peer(∕(X ),Y)] - E['peer(尼eer (X ),Y)] ≥ C (E['(∕(X ),Y )] - E['(/(X ),Y)]).
Since '(∙) is calibrated, and according to Proposition 8 and Theorem 2:
- ,,.~. - - , . . ~.,
ED[1 α-peer(/(X),Y)] - E^ [1 α-peer(/(X),Y)]
=(1 - e-1 - e+1 )(E[1(/(X ),Y)] - E[l(/(X ),Y)])
≤(1 - e-1 - e+1) ∙ Ψ-1 (E['(∕(X), Y)] - E['(/(X), Y)])
≤(1 - e-1 - e+1) ∙ Ψ-1 (C ∙ (E['peer(∕(X),Y)] - E['peer(&r(X)»])).
Therefore Wqeer(x) = Cψ( 1_e_；_e十]).It,s straight-forward to verify that Wqeer(x) satisfies the
conditions in Definition 1. We conclude the proof.
Now we check condition (2). Again, from previously, we know the following holds for a certain
Py = Py (1 - ey) + (1 - Py )e-y where p+1 = p,P-1 = 1 - p:
- ~.-
E['α-peer (/(X ),Y)]
- , , . ~ . ~ .,
=E['(∕(X ),Y)- α ∙ '(/(X ),Yk)]
=E (1 - ey)'(/(X), Y) + ey'(/(X), -Y) - α ∙ pγ'(/(X),Y) - α ∙ (1 - pγ)'(/(X), -Y)
=E [(1 - eγ - αpγ)'(/(X ),Y) + (eγ - α ∙ (1 - pγ))'(/(X), -Y)
Let φ(/(X) ∙ Y) := '(/(X), Y), we have
- ~ ..
E['a-peer(/(X ),Y))
=e (1 - eγ - αpγ)φ(∕(X) ∙ Y) + (eγ - α ∙ (1 - PY))φ(-/(X) ∙ Y)
:=E[p(/(X) ∙ Y)]
We first introduce a Theorem:
Theorem 9 (Theorem 6, (Bartlett et al., 2006)). Let 夕 be convex. Then 夕 is classification-calibrated
ifand only ifit is differentiable at 0 and 夕0 < 0.
22
Under review as a conference paper at ICLR 2020
We now show that φ is convex:
夕00(β) =(1 - eγ - αpγ) ∙ φ00(β) + (eγ - α ∙ (1 - PY))φ00(-β)
=(1 - eγ - αpγ) ∙ φ00(β) + (eγ - α • (1 - PY))φ00(β)
=(1 - eγ - apγ + eγ - a • (1 - PY))φ00(β)
=(1 - α)φ00(β) > 0
when α < 1. The last inequality is due to the fact that ` is convex.
Secondly We show the first derivative of 夕 is negative at 0:夕0(0) < 0:
夕0(0) =(1 — eγ — αpγ) ∙ φo(0) — (eγ — α • (1 — PY))φo(0)
=(1 - 2ey + α(1 - 2py))φ0(0)	(15)
Note that
Py = Py(I - ey) + (I -Py)e-y
Plug back to Eqn. (15) we have
夕0(0) =(1 — eY — αPY) ∙ φo(0) — (ey — a • (1 — PY))φo(0)
=(1 - 2eY + α(1 - 2^y))φ0(0)
= (1 - αPy)(1 - 2ey) + α(1 - Py)(1 - e-y) φ0(0)	(16)
Since (1 - αPy)(1 - 2ey) + α(1 - Py)(1 - e-y) > 0 and φ0(0) < 0 (due to calibration property
of ', Theorem 6 of Bartlett et al. (20O6)), we proved that 夕0(0) < 0. Then based on Theorem 6 of
Bartlett et al. (2006), we know ''α-peer is classification calibrated.	口
Proof for Theorem 7
Proof. We first prove the following Rademacher complexity bound
Lemma 7. Let <(F) denote the Rademacher complexity ofF. L denote the Lipschitz constant of`.
Then with probability at least 1 — δ, maxf∈f |左Q口eer,D (f) - 4。-PeerD (f )| ≤ (1 + L ∙ <(F) +
、/lθg4∕δ (1 + '	- '	)
V 2N V ɪ + Oa-peer	Oa-peer).
Note we also have the following ∀α:
R'α-peer ,D(fLpeJ- R`α -peer, D ( ,' α -peer )
≤R' D (f`	) — R'	D (f`	)
—	'α-peer,D '" Za-Peer，	'α-peer,D '" Za-Peer，
, ʌ, ʌ , ʌ. ..
+ (R'	D(f'	) - r`	d(f'	))
∖	'a-peer,D∖' 'a-peer >	'a-peer,D 'j 'a-peer > >
.ʌ , . , 、 _ ,., 一
+ (R'a-peer,D (f;a-peer ) - R'a-peer,D fa-peer))
≤0 + 2max |R'	D(f) - R'	D(f)|
f ∈F	`a-peer ,D	`a-peer ,D
23
Under review as a conference paper at ICLR 2020
Then apply the calibration condition we have
RD(J *
'"七α*-Peer
1
1 - e-1 - e+1
_	1
1 - e-1 - e+ι
1
≤-------------
1 - e-1 - e+ι
(Rl α*-peer,Df*-Peer)- RI
(Proposition 8)
(Rl JPeer,Df *-Peer)- Rl JpeerDfL-Peej)
ψ-1*-peer f F R"eer,D(f ) -	Re.*
D(f)
-peer,
(Theorem 3)
(Calibration of lα*-peer)
)-R*
α * -peer,
_	, O ,	_	,.,	、
+ RA-Peer,D(∙C*-Peer)- ReJPeer,D ( f.* -Peer)
≤----------------Ψ-1 I minr`	d(J) — minr`
1 — e-1 — e+1 eα*-peer ∖f ∈f 'α*-peer,D	f 'α
.*
D(f)
-peer,
a*-peer，D(f' - R3eer，D(f)|
+ 2 max |R
f ∈F '
1
≤-------------
1 - e-1 - e+1
ψ-1*-peer fF RjXD(f ) -	Rea
D(f)
-peer,
(Lemma 7)
+ 2(1 + α*)L ∙<(F)+2J
log4∕δ
2N
(1 + 'ɑ* -peer
：*-Peer) I ,
—
*
with probability at least 1 - δ.
□
Proof for Lemma 7
Proof. Due to the random sampling, via Hoeffding inequality we first have there exists some Pyn ∈
(0,1), with probability at least 1 - δ,
1 N	1 N
N E'α-peer(f(Xn),yn) - N £('(f (Xn),^n)
n=1
n=1
-« ∙讥n队于(Xn),yn) - α ∙ (1 - Pyn )队于(Xn) -yn))
≤ J
log2∕δ
2N
∙ ('a—peer - 'a-Peer)
Define the following loss function:
'(Xn ,Un) ：= '(f (Xn),yn) - α ∙ Pyn '(f (Xn),^n) - α ∙ 1 - PyC。(J(Xn), -Vn)
Via Rademacher bound on the maximal deviation we have with probability at least 1 - δ
maχ∣R',D(J) - R',D(f )∣ ≤ 2 ∙<(^OF) +J
log 1∕δ
2N
(17)
Since ' is L-Lipschitz, due to the linear combination, ' is (1 + α)L-Lipschitz. Based on the Lipschitz
composition of Rademacher averages, we have
,~ ,
员(。OF) ≤ (1 + α)L ∙<(F)
24
Under review as a conference paper at ICLR 2020
Therefore, via union bound, we know with probability at least 1 - 2δ:
1N
N Σ 'α-Peer (f (Xn),yn ) - R'a-peer,D ⑺
1N
⅛ X
n=1
ʌ
ʌ
'α-peer (f(xn),击)- R',D (f ) + R',D (f ) - R/D (f )
1N
4 X
n=1
'α-peer (f (Xn),yn ) - R',D (f ) + RKD (f ) - R'a-peer,D (f)
≤
≤r
log2/6	------
-2N- ∙ ('a-peer
log2/6	------
-2N- ∙ ('a-peer
-Peer) + lRl, D (f ) - R',D (f )|
-peer) + (1 + α)L ∙<(F) + /0^
≤
—
—
ʌ
ʌ
≤(1 + α)L ∙ <(F) + ↑j 2N1 ∙(I + 'a-peer
—
In above R£a Pr D (f) = RWD (f) because 'α-peer and ' share the same expected risk by construction.
Plug in the fact that 'a-peer is linear in ' and an easy consequence that
'α-peer - 'a-peer ≤ (1 + α)(' - '),
let δ := δ∕2,we conclude the proof.
□
Proof for Lemma 8
Nonetheless, despite the fact that 'α-peer(∙) is not convex in general, [Lemma 5, (Natarajan et al.,
2013)] informs US that as long as 直品漕 D (f) is close to some convex function, mirror gradient
type of algorithms will converge to a small neighborhood of the optimal point when performing
ERM with 'α-peer. A natural candidate for this convex function is the expectation of R' Peer,D(f) as
R'a-peer,D (f) → Rea-Peer,D (f) When N → ∞.
Lemma 8. When α < 1, max{e+1,e-1} < 0.5 ,and '00(t, y) = '00(t, —y), R"e" (f) is convex.
Proof. This was proved in the proof for Theorem 6, when proving the classification calibration
property of 'α-peer under condition (2).	□
Experiment
Implementation Details
We implemented neural networks (LeCun et al., 2015) for classification on 10 UCI Benchmarks and
applied our peer loss to update their parameters. For surrogate loss, we use the true error rates e-1
and e+1 instead of learning them on the validation set. Thus, surrogate loss could be considered a
favored and advantaged baseline method. On each benchmark, we use the same hyper-parameters
for all neural network based methods. For C-SVM, we fix one of the weights to 1, and tune the
other. For PAM, we tune the margin.
Results
The full experiment results are shown in Table.??. Equalized Prior indicates that in the correspond-
ing experiments, we resample to make sure P(Y = +1) = P(Y = -1) and we fix α = 1 in these
experiments. Our method is competitive in all the datasets and even able to outperform the surrogate
25
Under review as a conference paper at ICLR 2020
(a) Twonorm (e-1 = 0.2, e+1 = 0.4)
(b) Splice (e-1 = 0.1, e+1 = 0.3)
(c) Heart (e-1 = 0.2, e+1 = 0.4)
(d) Breast (e-1 = 0.4, e+1 = 0.4)
Figure 4: Accuracy on test set during training
loss method with access to the true error rates in most of them. C-SVM is also robust when error
rates are symmetric, and is competitive in 8 datasets.
From Figure.4, we can see our peer loss can prevent over-fitting, which is also part of the reason of
its achieved high robustness across different datasets and error rates.
26
Under review as a conference paper at ICLR 2020
	Task			With Prior Equalization P = 0.5						Without Prior Equalization p = 0.5					
(d,N+,N-)	e-ι, e+1	Peer	Surr	Symm	DMI	NN	C-SVM	Peer	Surr	Symm	DMI	NN	C-SVM
	0.1,0.3	0.977	0.968	0.969	0.974	0.964	0.966	0.977	0.968	0.969	0.974	0.964	0.966
	0.2, 0.2	0.977	0.969	0.974	0.976	0.972	0.969	0.977	0.969	0.974	0.976	0.972	0.969
Twonorm	0.1,0.4	0.976	0.964	0.956	0.974	0.911	0.95	0.976	0.964	0.956	0.974	0.911	0.95
(20,3700,3700)	0.2, 0.4	0.976	0.919	0.959	0.966	0.911	0.935	0.976	0.919	0.959	0.966	0.911	0.935
	0.4, 0.4	0.973	0.934	0.958	0.936	0.883	0.875	0.973	0.934	0.958	0.936	0.883	0.875
	0.1,0.3	0.919	0.878	0.851	0.875	0.811	0.928	0.925	0.885	0.868	0.889	0.809	0.933
	0.2, 0.2	0.918	0.874	0.879	0.888	0.819	0.931	0.927	0.876	0.906	0.885	0.812	0.941
Splice	0.1,0.4	0.914	0.86	0.757	0.842	0.743	0.891	0.925	0.862	0.777	0.852	0.754	0.898
(60,1527,1648)	0.2, 0.4	0.901	0.832	0.757	0.801	0.714	0.807	0.912	0.84	0.782	0.81	0.725	0.824
	0.4, 0.4	0.819	0.754	0.657	0.66	0.626	0.767	0.822	0.755	0.674	0.647	0.601	0.76
	0.1,0.3	0.833	0.78	0.777	0.797	0.756	0.753	0.856	0.802	0.803	0.83	0.75	0.788
	0.2, 0.2	0.821	0.762	0.795	0.801	0.75	0.717	0.856	0.813	0.793	0.826	0.769	0.796
Heart	0.1,0.4	0.827	0.777	0.714	0.779	0.717	0.744	0.859	0.815	0.725	0.814	0.723	0.677
(13,165,138)	0.2, 0.4	0.812	0.768	0.717	0.788	0.679	0.714	0.856	0.758	0.725	0.797	0.693	0.704
	0.4, 0.4	0.75	0.729	0.654	0.69	0.595	0.688	0.785	0.728	0.686	0.711	0.554	0.698
	0.1,0.3	0.745	0.707	0.674	0.72	0.667	0.67-	0.778	0.75	0.738	0.729	0.727	0.726
	0.2, 0.2	0.755	0.708	0.72	0.729	0.671	0.745	0.759	0.736	0.753	0.743	0.706	0.759
Diabetes	0.1,0.4	0.745	0.682	0.612	0.701	0.627	0.568	0.777	0.724	0.694	0.713	0.71	0.688
(8,268,500)	0.2, 0.4	0.755	0.681	0.634	0.682	0.596	0.59	0.739	0.705	0.695	0.707	0.672	0.7
	0.4, 0.4	0.719	0.645	0.619	0.637	0.551	0.654	0.651	0.685	0.68	0.633	0.583	0.702
	0.1,0.3	0.639	0.563	0.507	0.529	0.519	0.529	0.727	0.645	0.709	0.666	0.648	0.698
	0.2, 0.2	0.659	0.606	0.537	0.548	0.534	0.615	0.698	0.661	0.655	0.627	0.623	0.695
Breast	0.1,0.4	0.587	0.577	0.504	0.504	0.519	0.553	0.735	0.654	0.685	0.621	0.66	0.698
(9,85,201)	0.2, 0.4	0.63	0.534	0.482	0.496	0.538	0.538	0.73	0.674	0.666	0.58	0.672	0.698
	0.4, 0.4	0.596	0.519	0.504	0.526	0.471	0.51	0.677	0.628	0.545	0.537	0.529	0.698
	0.1,0.3	0.928	0.922	0.924	0.934	0.873	0.924	0.956	0.949	0.943	0.954	0.92	0.943
	0.1,0.4	0.932	0.938	0.937	0.944	0.83	0.85	0.951	0.929	0.946	0.941	0.898	0.929
Breast	0.2, 0.2	0.928	0.904	0.835	0.897	0.887	0.961	0.952	0.952	0.897	0.942	0.955	0.946
(30,212,357)	0.2, 0.4	0.93	0.885	0.844	0.89	0.844	0.865	0.933	0.898	0.898	0.918	0.831	0.862
	0.4, 0.4	0.928	0.867	0.819	0.746	0.824	0.855	0.908	0.839	0.817	0.795	0.673	0.866
	0.1,0.3	0.701	0.624	0.614	0.637	0.581	0.611	0.68	0.693	0.603	0.605	0.6	0.671
	0.2, 0.2	0.689	0.65	0.647	0.623	0.611	0.664	0.702	0.693	0.704	0.62	0.6	0.738
German	0.1,0.4	0.696	0.642	0.587	0.63	0.562	0.55	0.667	0.693	0.54	0.594	0.54	0.553
(23,300,700)	0.2, 0.4	0.664	0.59	0.6	0.618	0.572	0.469	0.676	0.681	0.537	0.573	0.535	0.581
	0.4, 0.4	0.606	0.55	0.573	0.573	0.556	0.572	0.654	0.632	0.549	0.611	0.553	0.696
	0.1,0.3	0.89	0.895	0.892	0.856	0.868	0.862	0.893	0.898	0.883	0.785	0.863	0.878
	0.2, 0.2	0.883	0.899	0.9	0.861	0.894	0.886	0.901	0.899	0.894	0.792	0.898	0.897
Waveform	0.1,0.4	0.884	0.893	0.762	0.856	0.771	0.804	0.888	0.894	0.703	0.778	0.821	0.821
(21,1647,3353)	0.2, 0.4	0.881	0.89	0.828	0.835	0.81	0.795	0.884	0.884	0.745	0.761	0.837	0.837
	0.4, 0.4	0.87	0.866	0.867	0.773	0.835	0.776	0.853	0.852	0.852	0.672	0.828	0.848
	0.1,0.3	0.906	0.9	0.89	0.87	0.909	0.881	0.943	0.909	0.897	0.811	0.93	0.924
	0.2, 0.2	0.913	0.894	0.907	0.897	0.899	0.918	0.905	0.905	0.905	0.91	0.936	0.936
Thyroid	0.1,0.4	0.875	0.862	0.834	0.784	0.88	0.869	0.902	0.924	0.856	0.75	0.919	0.917
(5,65,150)	0.2, 0.4	0.863	0.862	0.85	0.784	0.822	0.781	0.905	0.898	0.865	0.759	0.881	0.92
	0.4, 0.4	0.762	0.738	0.859	0.788	0.764	0.781	0.769	0.818	0.876	0.738	0.738	0.837
	0.1,0.3	0.856	0.875	0.843	0.896	0.866	0.892	0.796	0.835	0.903	0.896	0.878	0.892
	0.2, 0.2	0.9	0.835	0.911	0.894	0.908	0.912	0.931	0.896	0.917	0.883	0.934	0.908
Image	0.1,0.4	0.723	0.841	0.705	0.881	0.799	0.785	0.717	0.806	0.679	0.888	0.825	0.808
(18,1320,990)	0.2, 0.4	0.836	0.862	0.719	0.845	0.832	0.802	0.672	0.755	0.722	0.86	0.599	0.825
	04, 0.4	0.741	0.72	0.788	0.763	0.732	0.834	0.806	0.803	0.823	0.762	0.8	0.86
Table 3: Experiment Results on 10 UCI Benchmarks. Entries within 2% from the best in each row
are in bold. Surr: surrogate loss method (Natarajan et al., 2013); DMI: (Xu et al., 2019); Symm:
symmetric loss method (Ghosh et al., 2015). All method-specific parameters are estimated through
cross-validation. The pro- posed method (Peer) are competitive across all the datasets. Neural-
network-based methods (Peer, Surrogate, NN, Symmetric, DMI) use the same hyper-parameters.
All the results are averaged across 8 random seeds.
27