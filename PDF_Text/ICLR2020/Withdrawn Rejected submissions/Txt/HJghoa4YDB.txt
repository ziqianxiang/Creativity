Under review as a conference paper at ICLR 2020

TEMPORAL-DIFFERENCE  LEARNING  FOR  NONLINEAR
VALUE    FUNCTION    APPROXIMATION    IN    THE    LAZY
TRAINING  REGIME

Anonymous authors

Paper under double-blind review

ABSTRACT

We  discuss  the  approximation  of  the  value  function  for  infinite-horizon  dis-
counted Markov Reward Processes (MRP) with nonlinear functions trained with
the Temporal-Difference (TD) learning algorithm. We consider this problem under
a certain scaling of the approximating function, leading to a regime called lazy
training. In this regime the parameters of the model vary only slightly during the
learning process, a feature that has recently been observed in the training of neural
networks, where the scaling we study arises naturally, implicit in the initialization
of their parameters.  Both in the under- and over-parametrized frameworks, we
prove exponential convergence to local, respectively global minimizers of the above
algorithm in the lazy training regime. We then give examples of such convergence
results in the case of models that diverge if trained with non-lazy TD  learning, and
in the case of neural networks.

1    INTRODUCTION

In recent years, deep reinforcement learning has pushed the boundaries of Artificial Intelligence 
to an
unprecedented level, achieving what was expected to be possible only in a decade and outperforming
human intelligence in a number of highly complex tasks. Paramount examples of this potential have
appeared over the past few years, with such algorithms mastering games and tasks of increasing
complexity, from playing Atari to learning to walk and beating world grandmasters at the game
of Go (Haarnoja et al., 2018; Mnih et al.; 2013; Silver et al., 2016; 2017; 2018). Such impressive
success would be impossible without using neural networks to approximate value functions and / or
policy functions in reinforcement learning algorithms.  While neural networks, in particular deep
neural networks, provide a powerful and versatile tool to approximate high dimensional functions
(Barron, 1993; Cybenko, 1989; Hornik, 1991), their intrinsic nonlinearity might also lead to trouble
in training, in particular in the context of reinforcement learning. For example, it is well known 
that
nonlinear approximation of the value function might cause divergence in classical 
temporal-difference
learning due to instability (Tsitsiklis & Van Roy, 1997).  Several algorithms have been proposed
in the literature to address the issue of non-convergence (Bhatnagar et al., 2009; Maei & Sutton,
2010; Riedmiller, 2005; Sutton et al., 2009a;b; Szepesva´ri, 2010), while practical deep 
reinforcement
learning often employs and prefers basic algorithms such as temporal-difference (Sutton, 1988) and
Q-learning (Watkins, 1989) due to their simplicity. It is thus crucial to understand the 
convergence of
such algorithms and to bridge the gap between theory and practice.

The theoretical understanding of deep reinforcement learning is of course rather challenging, as 
even
for supervised learning, which can be viewed as a special case of reinforcement learning, deep 
neural
networks are still far from being understood despite the huge amount of research focus in recent 
years.
On   the other hand, recent progress has led to an emerging theory for neural network learning at 
least
in the regime of over-parametrization, including recent works on mean-field point of view of 
training
dynamics (Chizat & Bach, 2018a; Mei et al., 2018; Rotskoff et al., 2019; Rotskoff & Vanden-Eijnden,
2018; Wei et al., 2018) and also the linearized training dynamics in the over-parametrized regime
(Allen-Zhu et al., 2018a;b; Chizat & Bach, 2018b; Du et al., 2018a;b; Ghorbani et al., 2019a; Jacot
et         al., 2018; Lee et al., 2019; Oymak & Soltanolkotabi, 2019; Zou et al., 2018).

The main goal of this work is to analyze the dynamics of a prototypical reinforcement learning
algorithm – temporal/difference (TD) learning – based on the recent progress in deep supervised
learning. In particular, we will focus on the lazy training regime, inspired by the recent work 
Chizat

1


Under review as a conference paper at ICLR 2020

& Bach (2018b), and analyze TD  learning in both over-parametrized and under-parametrized regimes
with scaled value function approximations.

Related Works.    This work is closely related to the recent paper Chizat & Bach (2018b), addressing
the problem of lazy training in the supervised learning framework when models are trained through
(stochastic) gradient descent.  In particular, that paper introduced the scaling that we consider in
this  work as an explanation, e.g., of the small relative displacement of the weights of over- and
under-parametrized neural networks for supervised learning.  That work, however, leverages the
gradient structure of the underlying vector field, which we lack in the present framework when the
underlying policy is not reversible (Ollivier, 2018). The linear stability analysis is also 
considered
in the recent work Achiam et al. (2019) based on the neural tangent kernel (Jacot et al., 2018) for
off-policy deep Q-learning.

The groundbreaking paper Tsitsiklis & Van Roy (1997) proves convergence of TD  learning for linear
value function approximation, unifying the manifold interpretations of this convergence phenomenon
that preceded it by highlighting that convergence of the algorithm is to be understood in the norm
induced by the invariant measure of the underlying Markov process. Furthermore, the paper gives an
illuminating counterexample for the extension of the linear result to the general, nonlinear 
setting.
Our   result shows that divergence does not occur in the lazy training regime.

Concurrent work (Brandfonbrener & Bruna, 2019) has shown convergence and non-divergence of
TD learning in the over-parameterized, respectively the under-parametrized regime, provided that
the environment is sufficiently reversible. We note that working in the lazy training regime allows 
to
ensure convergence independently on the reversibility of the environment and quantify the error of
the fitted model in the under-parametrized regime. Finally, another concurrent work (Cai et al., 
2019)
analyzes global convergence of a modified TD algorithm for two-layer neural networks with ReLu
nonlinearity when the width of the hidden layer diverges. In contrast, in the present paper we focus
on the original TD(λ) learning algorithm for general approximators.

Contributions.    This paper proves that on-policy TD learning for policy evaluation (on-policy 
policy-
evaluation for short), a widely used algorithm for value function approximation in reinforcement
learning, is convergent (asymptotically with probability one), in the lazy training regime, when the
model is a nonlinear function of its parameters.  More specifically, we prove convergence of this
algorithm in both the under- and over-parametrized regime to local and global minima, respectively, 
of
a natural, weighted error function (the projected TD  error), and illustrate such convergence 
properties
through numerical examples.

To obtain the result summarized above, we adapt the contraction conditions developed in the frame-
work of linear function approximations to a nonlinear, differential geometric setting. Furthermore,
we extend some existing results on the convergence in the lazy training regime of nonlinear models
trained by gradient descent in the supervised learning framework to the world of reinforcement
learning. This requires a generalization of the techniques developed in the gradient flow setting to
non-gradient (i.e., rotational) vector fields such as the ones encountered in the TD  learning 
framework.

2    MARKOV  DECISION  PROCESSES

We denote a Markov Reward Process (MRP) by the 4-tuple (S, P, r, γ), where S  is the state space,
P  = P (s, s′)s,s'∈S  a transition kernel, r(s, s′)s,s'∈S  is the real-valued, bounded immediate 
reward
function and γ     (0, 1) is a discount factor. In this context, the value function V   :           
R₊ maps
each state to the infinite-horizon, expected discounted reward obtained by following the Markov
process defined by P . We assume that this Markov process satisfies the following assumption:

Assumption 1.  The Markov process with transition kernel P  is ergodic and its stationary measure µ

has full support in S. Furthermore we assume that S is compact.

In this note we are interested in learning the value (or cost-to-go) function V ∗(x) of a given MRP

(S, P, r, γ), which is given by


V ∗(s) := Es

∞     t

t

t=0

t+1

)Σ ,                                               (1)

where Es [ · ] denotes the expectation of the stochastic process st starting at s₀ = s. More 
specifically
we would like to estimate this function through a set of predictors Vw(s) in a Hilbert space F
parametrized by a vector w ∈ W  := Rp. We make the following assumption on such predictors:

2


Under review as a conference paper at ICLR 2020

Assumption 2.  The parametric model V   :  Rp                mapping w       Vw(  ) is 
differentiable with
Lipschitz continuous derivative DV   :  w      DVw (where DVw is a linear map from Rp              
) with
Lipschitz constant LDV  defined WRT  the operator norm.

A popular algorithm to solve this problem is given by value function approximation with TD(λ)
updates (Sutton & Barto, 2018). Starting from an initial condition w(0)         , for any λ     [0, 
1), this
learning algorithm updates the parameters w of the predictor by the following rule:

w(t + 1) := w(t) + βtδ(t)zλ(t) ,                                                (2)
for a fixed sequence of time steps   βt   to be specified later, where the temporal-difference 
error δ(t)

and eligibility vector zλ(t) are given by

t

δ(t) := r(st, st₊₁) + γVw₍t₎(st₊₁) − Vw₍t₎(st)         zλ(t) :=       (γλ)ᵗ−τ ∇wVw₍t₎(sτ ) .      
(3)

τ =0

This work focuses on the asymptotic regime of small constant step-sizes βt      0. In this adiabatic
limit, the stochastic component of the dynamics is averaged out before the parameters of the model
can undergo a significant change. This allows to consider the TD  update as a deterministic 
dynamical
system emerging from the averaging of the underlying stochastic algorithm. We focus on analysis of
this deterministic system to highlight the aspect of nonlinear function approximation. The averaged,
deterministic dynamics is given by the set of ODEs


 d

dtw(t) = Eµ

Σ.r(s, s′) + γVw₍t₎

(s′) − V

w(t)

(s)Σzλ

(t)Σ ,                            (4)

where Eµ denotes the expectation with respect to the invariant measure of the underlying dynamics.

In the case of finite state space (      = d) we can represent Vw as a vector in Rd, while in 
general it is
a function          R, which we will restrict to the space L²(   , µ), namely square integrable 
function
with respect to the measure µ.

To streamline our analysis of the TD algorithm, we define the TD operator Tλ  :  L²(S, µ) → L²(S, 
µ):


TλV (s) := (1 − λ)

mΣ=0

λᵐEs

m

t=0

γᵗr(st, st₊₁) + γᵐ⁺¹V (sm₊₁)Σ .

Note that when λ = 0 the above operator acquires the simple form T ⁰V  := r¯ + γPV  for r¯(s) :=
Es [r(s, s′)]. Then, denoting throughout by DVw the Fre´chet derivative of V  at w, it can be shown
(Tsitsiklis & Van Roy, 1997, Lemma 8) (and is immediately verified in the special case λ = 0) that
the continuous dynamics (4) for general λ < 1 can be written as


 d w(t) =   TλV

dt

w(t)

− Vw(t)

, DV

w(t)⟩µ

,                                          (5)

where we define throughout the inner product induced by the invariant measure µ (acting component-
wise in expressions such as the one above) as


and denote by ǁ  ·  ǁ

⟨a, b⟩µ :=

S

a(s)b(s)µ(ds) ,                                                   (6)

|S|  =  d, denoting by Γ the

µ the corresponding norm.  Note that in the case

d-dimensional diagonal matrix whose entries are the (positive) values of the invariant measure µ(s),
one has  a, b  µ  = aTΓb.  The extension of convergence results for the limiting, average dynamics 
we
consider in this paper to convergence with probability one of the underlying, stochastic algorithm 
can
be obtained through standard stochastic approximation arguments (Borkar & Meyn, 2000; Borkar,
2009). More details on this straightforward extension are given in Remark 3.4 in Section 3 and in 
the

appendix.

In  this  work,  we  are  interested  in  a  certain  scaling  of  the  TD  learning  algorithm  
with  function
approximation. More specifically, we consider the rescaled update


 d w(t) =  1 ⟨Tλ(αV

) − αV

, DV      ⟩

(7)

for large values of the scaling parameter α > 1 . One of the reasons why this scaling of the model
is of practical interest is because it arises naturally when training neural networks, implicit in 
some
widely applied choices of initial conditions, as we explain in Section 4.2. Furthermore, as we 
shall see
below, under some mild assumptions for large values of α the parameters w of the model vary only
slightly during training, inducing what is called the “lazy training” regime. A visual 
representation of
the geometric effect of this scaling in the case where p < d < ∞ is given in Fig. 1.

3


Under review as a conference paper at ICLR 2020


W

w(0)

TV (w(0))Fw

V (w(0))

αV (W)

Figure 1: Schematic representation of the effect of the linear scaling of the approximating function
(e.g., in (11)) in the under-parametrized setting.  The space of parameters (left) is mapped to the
space of predictors (right) by the parametric model V . The scaling V       αV  changes the manifold
w that the parameter space is mapped to (different surfaces on the right). In particular, this 
scaling
“widens” the reach in the space of functions of the predictors within a ball of small radius in     
, but
at     the same time it “flattens” that space (locally in     ) bringing it closer to the 
tangential plane to
the initial model Vw₍₀₎.  Choosing Vw₍₀₎  = 0 as in the picture above leaves the initial point of 
the

dynamics (in predictor space) invariant under such transformation.

3    MAIN  RESULTS

3.1    OVER-PARAMETRIZED REGIME

In the over-parametrized setting we assume that DVw₍₀₎ is surjective, i.e., its singular values are
uniformly  bounded  away  from  0.   This  is  only  possible  in  the  finite  state  space  
setting  and  is
automatically the case if the number of parameters p is larger than the size of the state space    .
Admittedly, in applications such as AlphaGo (Silver et al., 2016; 2017), it is unrealistic to over-
parametrize, but we start with this regime as it parallels the study of over-parametrized supervised
learning for global convergence of the training loss. Analysis of the under-parametrized regime will
be discussed in the next subsection. In order to state our first result, we introduce the scalar 
product
in     defined by   a, b  0  =   a, gw₍₀₎b   where gw := (DVw   DVwT)−¹, and denote by         ₀ 
the norm

it induces.  Note that gw is the metric tensor associated to the pushforward metric induced by the

parametric model V    :   Rp  →  F.  We note that if DVw₍₀₎ has singular values that are uniformly
bounded away from 0, the norms ǁ  ·  ǁµ, ǁ  ·  ǁ₀ are equivalent, i.e., there exists κ  >  0 such 
that
κ−¹ǁf ǁ₀ < ǁf ǁµ < κǁf ǁ₀ for all f  ∈ F .

Theorem  3.1  (Over-parametrized  case).   Assume  that  σmin    >   0,  where  σmin  is  the  
small-
est  singular  value  of  DVw₍₀₎.    Assume  further  that  w(0)  is  such  that  ǁVw₍₀₎ǁ₀   <   M  
 :=

(1 − γ)²σ²   /(192κ²LDV ǁDVw₍₀₎ǁ), then for α > α₀ := ǁV ∗ǁ₀/M we have for all t ≥ 0 that

1−γ

V ∗ − αV      ǁ2  ≤ ǁV ∗ − αV       ǁ2e−     ᵗ .                                      (8)

Recall that V ∗ is the exact value function given by (1). Moreover, if ǁVw₍₀₎ǁ₀ ≤ Cα−¹ for a 
constant

C > 0, then supt>0 ǁw(t) − w(0)ǁ = O(α−¹).

Similarly to the proof in Chizat & Bach (2018b), we first show that DVw and Vw do not change much
assuming that w stays in a small ball of radius Q.  Then, combining this result with the Lipschitz
continuous character of DV  in w, one shows that w does indeed stay in the desired ball of radius

Q. A similar computation can be done in our case. To bypass the absence of a strongly convex cost
functional in our framework, which was crucial in the analysis of Chizat & Bach (2018b), we adopt a
strategy based on the use of a local Lyapunov function

U (f ) = ǁf − V ∗ǁ2 ,                                                          (9)

where V ∗ is the sought for value function (1). The theorem is based on some preparatory lemmas,
proofs of which can be found in appendix.  The first one states that for large values of the scaling
parameter α the pushforward metric gw varies in a negligible way during training. Throughout, we

denote by 1 the identity map in the corresponding space and by Bµ(v), B0(v) and Bq(v) the balls

q               q

with radius Q around v in ǁ  ·  ǁµ, ǁ  ·  ǁ₀ and ǁ  ·  ǁ₂ respectively.

Lemma 3.2 (Perturbation of the metric).  Let G₀ be a compact subset of a linear space G. For v(0) ∈
G₀, let gv be a continuous, self-adjoint linear operator that is positive definite in a 
neighborhood of
v(0) when restricted on G. Then for all ε > 0 there exists δ > 0 such that, for all v ∈ Bδ(v(0)) ⊆ 
G₀

gv₍₀₎ = (1 + g˜v)gv ,                                                        (10)

4


Under review as a conference paper at ICLR 2020

for a linear operator g˜v with ǁg˜vǁ < ε . More specifically, let σmin be the smallest singular 
value of

DVw₍₀₎. Then if Q ≤ (1 − γ)σ²   /(48LDV ), (10) holds with ǁg˜V ₍w₎ǁ <  ¹−γ  for all w ∈ Bq(w(0)).

We also recall from Tsitsiklis & Van Roy (1997) the following contraction property of the TD 
operator
in the ǁ  ·  ǁµ norm. For the convenience of readers, we recall the proof in the appendix.

Lemma 3.3.  (Tsitsiklis & Van Roy, 1997, Lemmas 1, 3, 7) Under Assumption 1, for any V, V˜  ∈ F

we have that ǁTλV  − TλV˜ǁµ  ≤  γλǁV  − V˜ǁµ   for γλ  := γ  ¹−λ   ≤  γ  < 1 . In particular there

exists a unique fixed point of Tλ, V ∗ ∈ F  given by (1).

The proof of Theorem 3.1 relies on the above lemma to establish decay of the local Lyapunov function
U as long as w stays within a ball. The nonlinear effects become negligible when α is sufficiently
large. The control of U in turn gives the bound of the change of w, which closes the argument. The
details are given in the supplementary materials.

Remark  3.4.  Our  results  can  be  extended  to  show  stability  and  convergence  in  the  
stochastic
approximation setting, similarly to Bhatnagar et al. (2009); Tsitsiklis & Van Roy (1997), under the
additional assumption that the step size   βt   satisfies the Robbins-Monro condition (Robbins &
Monro, 1951).  For example, one can apply (Borkar & Meyn, 2000, Thms.  2.2, 2.4) guaranteeing
almost sure convergence and exponential contraction of the expected error with probability one over
the initial condition provided that the limiting vector field (in our case (7)) has a unique fixed 
point
and is Lipschitz continuous.  Lipschitz continuity is an immediate consequence of the linearity of
Tλ and the boundedness of closed balls in     together with the Lipschitz continuity of the models
Assumption 2. The existence of a fixed point (1) in     of the limiting vector field is trivial 
while its
uniqueness is shown in the proof of Theorem 3.1 in the appendix.

3.2    UNDER-PARAMETRIZED REGIME

We now proceed to state and prove a convergence theorem in the under-parametrized case.  The
underlying assumption in this section is that the size of state space is larger than the number of
parameters, which in turn bounds the rank r of DVw₍₀₎ from above:  r  < p < d (where possibly
d =     ). In this regime, in general, there is no hope that TD  will converge to the true value 
function
V ∗.  In fact, the image of the operator Tλ might not even lie in the space    w  of approximating

functions.  However, the derivative DVwT(t)  in the TD  update acts as a projection (WRT  the 
product

⟨ · , · ⟩µ) onto the tangent space of Fw at Vw₍t₎ (more specifically, DVwT(t)  projects the image 
of Tλ
onto W, which is then mapped back to TV ₍w₍t₎₎Fw by DVw₍t₎). We denote throughout by Π and Π₀
the projection operator under (6) onto TV ₍w₍t₎₎Fw and TV ₍w₍₀₎₎Fw respectively. What one can hope
for is that the TD algorithm converges to a locally “optimal” approximation V˜∗ of V ∗ on the 
manifold

Fw, which is close to the best approximator Π₀V ∗ of V ∗ on the linear tangent space TV ₍w₍₀₎₎Fw.

Theorem 3.5 (Under-parametrized case).  Assume that r := rank(DVw) is constant in a neighbor-
hood of w(0) and Vw₍₀₎ = 0. Then there exists α₀ > 0 such that for any α > α₀ the dynamics (7) (and
the corresponding approximation Vw) converge exponentially fast to a locally (in W) attractive fixed
point V˜∗, for which ǁΠ(TλV˜∗ − V˜∗)ǁµ = 0 and ǁV˜∗ − V ∗ǁµ <  ¹−λγ ǁΠ₀V ∗ − V ∗ǁµ + O(α−¹).

Note that for random initialization the constant rank assumption is generically satisfied. Indeed, 
the
maximal rank property holds generically in      and thus WP1 at w(0) when the model parameters are
initialized randomly. Furthermore, by the lower semicontinuity of the rank function the Jacobian DV
will have maximal rank in an open subset of     . The main difference of the proof of the above 
result

WRT  the one in the over-parametrized regime is that DVw   DVwT does not have full rank anymore.
This implies on one hand that the norms          µ and          ₀ are not equivalent in    , even 
though
we still have         ₀      κ        µ for a κ > 0, provided that Assumption 1 holds. On the other 
hand,
as mentioned above, this implies that the model Vw evolves on a submanifold    w of    , and that
Tλ does not, in general, map onto the tangential plane TV ₍w₎   w of    w at Vw. The action of Tλ is
then projected back onto TV ₍w₎   w by the operator DVw₍t₎. The nonlinear structure of the space    
w

slightly complicates the proof WRT  the over-parametrized case, and we apply standard differential

geometric tools to map the problem back to a linear space.                                   W0     
   V         F0

Proof.  We apply the rank theorem (Boutaib, 2015; Lee, 2003) ((Abraham et al.,        φ             
    ψ

2012) for the ∞-dimensional setting) to show that there exist sets W₀, W₀ ⊆                πr

Rp, F₀, F₀  ⊆  F  and diffeomorphic maps φ  :  W₀  →  W₀, ψ   :  F₀  →  F₀     W₀              F₀

where ψ◦V ◦φ−¹ = πr, φ(w(0)) = 0, ψ(Vw₍₀₎) = 0 and, for an appropriate choice of bases, πr maps

5


Under review as a conference paper at ICLR 2020

the coordinates of     ₀ to the first r coordinates of    ₀, i.e., (x₁, . . . , xp)      (x₁, . . . 
, xr, 0, 0, . . . ),
where r is the rank of the operator DVw₍₀₎. We denote by Πr the hyperplane in     spanned by the
first r vectors of the basis. We recall that by Abraham et al. (2012); Boutaib (2015); Lee (2003) 
the
maps, ψ, φ, πr are continuous with Lipschitz derivatives Dψ, Dφ, Dπr respectively.

We consider the trajectory of V w₍t₎  :=  πr ◦ φ(w(t))  =  ψ(Vw₍t₎).  Denoting by D·  the Fre´chet
derivative at the corresponding point of the dynamics and noting that DV  = Dψ−¹DπrDφ we have


 d

dt V w(t)

=     1  DψDV DV T, Tλαψ−¹(V

α

w(t)

) − αψ−¹(V

w(t)

)⟩π


=     1  Dπ  DφDφTDπT(Dψ−¹)T, Tλαψ−¹(V

) − αψ−¹(V

)⟩   ,     (11)

so V  remains in Πr. As a consequence of the above we can naturally define a metric (the pushforward
metric) on F₀ by the tensor g¯v¯ = (DπrDφDφTDπrT)−¹. In fact, by choosing the metric tensor to
be constant on F₀, i.e., equal to g¯₀ for all v ∈ F₀, we equip the linear space F₀ with a scalar 
product

⟨ · , · ⟩₀. This, in turn, directly induces a norm ǁ  ·  ǁ₀ on the same space. We now proceed to 
use such

simple metric structure to establish the existence and uniqueness of a fixed point of (11) in    ₀ 
for α

large enough.

The result of our theorem follows from (Simpson-Porco & Bullo, 2014, Proposition 4.1), which
establishes uniqueness and exponential contraction at rate l > 0 of a dynamical system evolving
under the flow of a vector field X given by the RHS  of (11) in a forward invariant set    ₀ 
provided
that for every geodesic γ(s) in    ₀ (12) holds. Therefore, the proof of convergence is concluded by
applying Lemma 3.6 and Lemma 3.7, whose proofs can be found in supplementary materials. The
proof of the optimality of the fixed point is postponed as Lemma A.1 in the appendix.

Lemma 3.6.  There exists δ > 0 and α₀ > 0 such that the ball    ⁰(0)         ₀ is forward invariant 
and
forward complete with respect to the dynamics of (7) for all α > α₀.

Lemma 3.7.  There exists l > 0, δ > 0 and α₀ > 0 such that for all α > α₀ and all geodesics γ(s)

contained in the ball B0(0) ⊆ F₀, the function

⟨γ′(s), X(γ(s))⟩₀ − ls⟨γ′(0), γ′(0)⟩₀ ,                                         (12)

is strictly decreasing in s.

Remark 3.8.  The proof of Theorem 3.5 can be straightforwardly generalized to the case where the

initial condition V₀ is not identically 0 but within Bµ      (0) for Q(α) going to 0 with α → ∞. 
This

generalization, however, requires the map V  to be uniformly Lipschitz smooth for w         ₀. Among
other things, this extension allows to explicitly cover the training of randomly initialized, 
single layer
neural networks.

4    NUMERICAL  EXAMPLES

4.1    A DIVERGENT NONLINEAR APPROXIMATOR

We illustrate the convergence properties of TD  learning in the lazy training regime in the under-
parametrized case by applying it to the classical framework of (Tsitsiklis & Van Roy, 1997, Section
X). This reference gives an example of a family of nonlinear function approximators that diverge
when trained with the TD  method. The intuition behind this counterexample is that one can construct
a manifold of approximating functions    w in the form of a spiral, with the same orientation as the
rotation of the vector field induced by the TD  update in the space of functions.  By choosing the
windings of the spiral to be dense enough, the projection of the TD  vector field follows the 
spiral in
the outward direction, leading to a divergence of the algorithm, as displayed schematically in Fig. 
2a.
More specifically, consistently with Tsitsiklis & Van Roy (1997), we parametrize the manifold Fw
as Vϑ  := eεˆϑ(a cos(λˆϑ) − b sin(λˆϑ)) − V ∗ for a = (10, −7, −3), b = (2.3094, −9.815, 7.5056),

εˆ = 0.01, λˆ = 0.866. We choose the discount γ = 0.9 and a step-size of βt      2     10−³, while 
the
underlying Markov chain is defined by the transition matrix Pij = (δj,mₒd₍i,₃₎₊₁ +δi,j)/2, where 
δi,j

is the Kronecker delta function and equals 1 if i = j and 0 else. We note that the step-size does 
not
affect the convergence properties of the algorithm, as argued in Tsitsiklis & Van Roy (1997), where
the immediate reward was set to r¯ = (0, 0, 0). Note that, as realizing the conditions of Theorem 
3.5
would start the simulation at the solution V ∗ = (0, 0, 0), we shift both the solution and the 
manifold

6


Under review as a conference paper at ICLR 2020

(a) α = 1                                                        (b) α = 10²

Figure 2:  Schematic representation of the manifold    w for the example in Section 4.1 before (a)
and after (b) scaling of α. The underlying vector field represents the TD  error δ(V ) from (3), 
whose
projection on Tϑ   w gives the dynamics of the TD update in    w. In (a) this projection points 
“outwards”
along the spiral, while (b) it has a fixed point close to 0. The scaling yields an effective 
“linearization”
of the manifold around 0. The red point marks the global fixed point of the vector field.

of approximating functions by the same vector in the embedding space, leaving the new solution
V ∗ =    V₀ =    a at the center of the spiral, i.e., realized at ϑ =        . This corresponds to 
choosing
an average reward r¯ = (   6.85, 8.35,    1.5). We note that by the affine nature of the TD  
update, this
change in r¯ results in a global shift of the TD  vector field in     and does not affect the 
update of ϑ. In
particular, this means that the TD  update remains divergent for every initial condition different 
than
the solution V ∗.

We run the TD  update in the off-centered situation both for values of α = 1 (the classical, 
divergent
regime) and α  =  100.  As explained in the previous sections, this scaling of the approximating
function makes the TD  update convergent, as displayed in Fig. 3a.  Indeed, under this scaling the
solution converges to a local minimum of the dynamics. The intuition behind the convergence of
the algorithm is outlined in Fig. 2: when α is large we are in an almost linear regime where the TD
update converges.

4.2    SINGLE LAYER NEURAL NETWORKS

We show that the regime of study arises naturally in one hidden layer neural networks for a certain
family of initialization. We consider the example of ReLu activation, i.e., when the model is given 
by

N

Vw(s) =        ai max(0, bi · s − ci) ,                                            (13)

i=1

for s ∈  Rm  and N  distinct (m + 2)-dimensional vectors wi  = (ai, (bi)₁, . . . , (bi)m, 
ci)i∈₍₁,...,N₎.

Typical initialization of the weights of the above model is of the form ai  ∼  N(0, 1/√N ), (b )

iid                                                iid

N(0, 1/√m) for all j and ci  ∼  N(0, 1). However, by the linearity of (13) in ai, by the rescaling

property of normal distribution this is equivalent to writing

N

αV   (s) = α  1  Σ a  max(0, b  · s − c ) ,                                       (14)

for an N -dependent α(N )  =  √N  (diverging in N ),  ai  ∼   N(0, 1),  (bi)j   ∼   N(0, 1/√d) and

iid

c  i∼ⁱᵈ N(0, 1)¹. Therefore, this common choice of initial conditions implicitly starts the 
training of

the above model in the lazy regime (Ghorbani et al., 2019b). We train the model (14) by TD  
learning

(7) with fixed step-size βt ≡ 10−³ both in the over- and under-parametrized regime. To do so, we
draw an objective function V ∗ randomly with distribution V ∗(s) i∼ⁱᵈ N(0, 1) for all s ∈ S on a 
grid
1A heuristic justification that the scaling the parameters of the neural network by α(N )/N = 1/√N 
leads

to lazy training while the scaling N −¹ is natural for the model Vw and does not lead to the lazy 
regime can be
found in Chizat & Bach (2018b). This natural scaling is studied in depth in Chizat & Bach (2018a); 
Mei et al.
(2018); Rotskoff & Vanden-Eijnden (2018)

7


Under review as a conference paper at ICLR 2020

(a) Example from Tsitsiklis & Van Roy (1997)                          (b) Neural networks 
simulation

Figure 3: Results of the training of nonlinear value function approximation with TD  learning for 
the
examples described in Section 4.1 (a) and Section 4.2 (b). In (a), we plot the µ-norm of the 
projected
TD error Π(TλV     V ). This quantity measures the increments of the model parameters during 
training
and vanishes at a local minimum of the TD  dynamics. We see that the algorithm diverges for α = 1
(blue curve), but converges to a local minimum for α = 100. In (b, above) we plot the MSE  of single
layer neural network during training in the over-parametrized regime (N  = 100, d = 30, α = 500 )
for different choices of γ (0.8, 0.83, 0.85, 0.87, 0.9), showing exponential convergence (at 
different
rates) to the global minimum claimed in Theorem 3.1. In (b, below) we again plot the norm of the the
projected TD error for a neural network in the under-parametrized regime (N  = 10, d = 50, α = 100)
for different initial conditions, showing that the dynamics converge to a local fixed point.

of d equally spaced points on the interval [   1, 1]. We then compute the corresponding average 
reward
by solving the TD  equation: r¯ = (1    γP )V ∗, and train the model (7) for λ = 0, γ = 0.9 (when 
not
specified otherwise) with transition matrix Pij = (δj,mₒd₍i,d₎₊₁ + δi,j)/2. To respect the 
conditions
of Theorem 3.5, we initialize half of the parameters of the neural network as explained above, while
the other half is obtained by replicating the values of bi, ci and inverting the one of ai         
ai. This
“doubling trick” introduced in Chizat & Bach (2018b) produces a neural network with Vw₍₀₎      0 
and

randomly initialized weights with the desired distribution. We consider situations where N  = 10,
d  =  50 (under-parametrized, taking α  =  100) and N  =  100, d  =  30 (over-parametrized, with
α = 500), and plot the convergence to local, respectively global minima in Fig. 3b.

5    DISCUSSION  AND  CONCLUSION

In this work we have proven the convergence properties of the TD  learning algorithm with nonlinear
value function approximation in the lazy training regime.  In this regime, the algorithm behaves
essentially like a linear approximator spanning the tangential space of the approximating manifold 
(in
function space) at initialization. As such, the training converges exponentially fast with 
probability
one   to the global minimum or a local fixed point depending on the codimension of the approximating
manifold in the search space. This guarantees convergence with little parametric displacement. This
phenomenon can be intuitively understood as an effect of the linearized regime in which the neural
networks are trained which reduces them, in the limit, to a randomized kernel method (more precisely
a Neural Tangent Kernel (Jacot et al., 2018)). In this sense, convergence of lazy models may come
at the expense of their expressivity. Recent works (Chizat & Bach, 2018b; Ghorbani et al., 2019b)
discuss the approximating power of lazy neural networks in the supervised setting, highlighting 
their
limits WRT  their non-lazy counterparts and naturally comparing them with random feature models
(Yehudai & Shamir, 2019), but an exhaustive study of the expressivity of these models, in particular
in     the context of reinforcement learning is still lacking. Nonetheless, the results proven in 
this work
emphasize the interest of this regime in the framework of deep reinforcement learning, where models
often suffer from divergent behavior especially during early stages of training.

Future directions of research include the extension of these results to more complex, nonlinear 
rein-
forcement learning algorithms such as Q-learning, and the development of more refined, nonasymp-
totic versions of the above theorems. Furthermore, a more thorough exploration of the relationship
between the limiting results in Chizat & Bach (2018a) and the ones presented here and in Chizat &
Bach (2018b) while transposing those to the framework of reinforcement learning would be important
for the understanding of the limiting dynamics of neural networks in this domain.

8


Under review as a conference paper at ICLR 2020

REFERENCES

Ralph Abraham, Jerrold E Marsden, and Tudor Ratiu. Manifolds, tensor analysis, and applications,
volume 75. Springer Science & Business Media, 2012.

Joshua Achiam, Ethan Knight, and Pieter Abbeel.  Towards Characterizing Divergence in Deep
Q-Learning, 2019. preprint, arXiv:1903.08894.

Z. Allen-Zhu, Y. Li, and Y. Liang. Learning and generalization in overparametrized neural newtorks,
going beyond two layers, 2018a. preprint, arXiv:1811.04918.

Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization,
2018b. preprint, arXiv:1811.03962.

A. R. Barron.  Universal approximation bounds for superpositions of a sigmoidal function.  IEEE
Transactions on Information Theory, 39(3):930–945, 1993.

Shalabh Bhatnagar, Doina Precup, David Silver, Richard S Sutton, Hamid R. Maei, and Csaba
Szepesva´ri. Convergent temporal-difference learning with arbitrary smooth function approximation.
In Advances in Neural Information Processing Systems 22, pp. 1204–1212. 2009.

V. Borkar and S. Meyn.   The O.D.E. method for convergence of stochastic approximation and
reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447–469, 2000. doi:
10.1137/S0363012997331639.

Vivek S Borkar.  Stochastic approximation: a dynamical systems viewpoint, volume 48.  Springer,
2009.

Youness Boutaib. On Lipschitz maps and their flows, 2015. preprint, arXiv:1510.07614.

D. Brandfonbrener and J. Bruna. On the expected dynamics of nonlinear TD learning, 2019. preprint,
arXiv:1905.12185.

Qi Cai, Zhuoran Yang, Jason D. Lee, and Zhaoran Wang.   Neural temporal-difference learning
converges to global optima, 2019. preprint, arXiv:1905.10027.

Le´na¨ıc Chizat and Francis Bach. On the global convergence of gradient descent for 
over-parameterized
models using optimal transport. In Proceedings of the 32Nd International Conference on Neural
Information Processing Systems, NIPS’18, pp. 3040–3050, 2018a.

Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming.
2018b. preprint, arXiv:1812.07956.

G. Cybenko.  Approximation by superpositions of a sigmoidal function.  Mathematics of Control,
Signals and Systems, 2(4):303–314, Dec 1989.

S. S. Du, J. D. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima of deep 
neural
networks, 2018a. preprint, arXiv:1811.03804.

S. S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient descent provably optimizes over-parameterized
neural networks, 2018b. preprint, arXiv:1810.02054.

Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers
neural networks in high dimension, 2019a. arXiv preprint arXiv:1904.12191.

Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy
training of two-layers neural networks, 2019b. preprint, arXiv:1906.08899.

T. Haarnoja, S. Ha, A. Zhou, J. Tan, G. Tucker, and S. Levine. Learning to walk via deep 
reinforcement
learning, 2018. preprint, arXiv:1812.11103.

Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks, 4(2):
251 – 257, 1991. ISSN 0893-6080.

9


Under review as a conference paper at ICLR 2020

Arthur Jacot,  Franck Gabriel,  and Cle´ment Hongler.   Neural tangent kernel:  Convergence and
generalization in neural networks.  In Advances in neural information processing systems, pp.
8571–8580, 2018.

Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and
Jeffrey Pennington. Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient
Descent, 2019. preprint, arXiv:1902.06720.

J.M. Lee. Introduction to Smooth Manifolds. Graduate Texts in Mathematics. Springer, 2003. ISBN
9780387954486.

Hamid Reza Maei and Richard S. Sutton. GQ(lambda): A general gradient algorithm for temporal-
difference prediction learning with eligibility traces.   In 3d Conference on Artificial General
Intelligence (AGI-2010). Atlantis Press, 2010. ISBN 978-90-78677-36-9.

Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665–E7671,
2018. doi: 10.1073/pnas.1806579115.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis.  Human-level control through deep reinforcement learning.
Nature, (7540):529–533, 02 .

Volodymyr  Mnih,  Koray  Kavukcuoglu,  David  Silver,  Alex  Graves,  Ioannis  Antonoglou,  Daan
Wierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. In NIPS Deep
Learning Workshop. 2013.

Yann Ollivier. Approximate temporal difference learning is a gradient descent for reversible 
policies,
2018. preprint, arXiv:1805.00869.

S. Oymak and M. Soltanolkotabi.   Towards moderate overparametrization:  global convergence
guarantees for training shallow neural networks, 2019. preprint, arXiv:1902.04674.

Martin Riedmiller.  Neural fitted Q iteration – first experiences with a data efficient neural rein-
forcement learning method. In Joa˜o Gama, Rui Camacho, Pavel B. Brazdil, Al´ıpio Ma´rio Jorge,
and Lu´ıs Torgo (eds.), Machine Learning: ECML 2005, pp. 317–328, Berlin, Heidelberg, 2005.
Springer Berlin Heidelberg.

H. Robbins and S. Monro.  A stochastic approximation method.  Ann. Math. Statist., 22:400–407,
1951.

G. Rotskoff, S. Jelassi, J. Bruna, and E. Vanden-Eijnden. Global convergence of neuron birth-death
dynamics, 2019. preprint, arXiv:1902.01843.

Grant Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time convergence
and  asymptotic  error  scaling  of  neural  networks.   In  S.  Bengio,  H.  Wallach,  H.  
Larochelle,

K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing
Systems 31, pp. 7146–7155. Curran Associates, Inc., 2018.

David Silver, Aja Huang, Christopher J. Maddison, Arthur Guez, Laurent Sifre, George van den
Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game
of Go with deep neural networks and tree search. 2016.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan
Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the
game of Go without human knowledge. Nature, 550:354, October 2017.

10


Under review as a conference paper at ICLR 2020

David  Silver,  Thomas  Hubert,  Julian  Schrittwieser,  Ioannis  Antonoglou,  Matthew  Lai,  Arthur
Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen
Simonyan, and Demis Hassabis. A general reinforcement learning algorithm that masters chess,
shogi, and Go through self-play. Science, 362(6419):1140–1144, 2018.

John W Simpson-Porco and Francesco Bullo. Contraction theory on Riemannian manifolds. Systems
& Control Letters, 65:74–80, 2014.

R. S. Sutton and A. G. Barto.  Reinforcement Learning: An Introduction.  MIT Press, Cambridge,
MA, second edition, 2018.

Richard S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3
(1):9–44, Aug 1988.

Richard S Sutton, Hamid R Maei, and Csaba Szepesva´ri.  A convergent o(n) temporal-difference
algorithm for off-policy learning with linear function approximation.   In Advances in neural
information processing systems, pp. 1609–1616, 2009a.

Richard  S  Sutton,  Hamid  Reza  Maei,  Doina  Precup,  Shalabh  Bhatnagar,  David  Silver,  Csaba
Szepesva´ri, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning
with linear function approximation. In Proceedings of the 26th Annual International Conference
on Machine Learning, pp. 993–1000. ACM, 2009b.

Csaba Szepesva´ri. Algorithms for reinforcement learning. Synthesis Lectures on Artificial 
Intelligence
and Machine Learning, 4(1):1–103, 2010.

John N Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function
approximation. In Advances in neural information processing systems, pp. 1075–1081, 1997.

C. J. C. H. Watkins. Learning from Delayed Rewards. PhD thesis, King’s College, Oxford, 1989.

C. Wei, J. D. Lee, Q. Liu, and T. Ma. On the margin theory of feedforward neural networks, 2018.
preprint, arXiv:1810.05369.

Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding
neural networks, 2019. preprint, arXiv:1904.00687.

D. Zou, Y. Cao, D. Zhou, and Q. Gu.  Stochastic gradient descent optimizes over-parametererized
deep ReLU networks, 2018. preprint, arXiv:1811.08888.

11


Under review as a conference paper at ICLR 2020

A    SUPPLEMENTARY  PROOFS

To simplify the notation in the forthcoming analysis, we slightly abuse the notation used when the
state space is finite-dimensional extending it, when necessary, to the infinite-dimensional setting.
This naturally generalizes matrix multiplication to the action of linear operators. In particular 
the
action of Γ, which we recall in the finite-dimensional setting is a diagonal matrix with entries 
µ(s), is
to be intended as

(aTΓb)ij =      ai(s)bj(s) µ(ds) .

S

Furthermore, we introduce the following decomposition of the TD  operator:

TλV  = r¯λ + γPλV  ,


where

r¯λ(s) := (1−λ)

mΣ=0

λᵐEs

m
t=0

γᵗr(st, st₊₁)Σ

,         PλV (s) := (1−λ)

mΣ=0

(λγ)ᵐEs [V (sm₊₁)] ,

or, in vector notation

∞           m                                                                                  ∞

r¯λ := (1 − λ) Σ λᵐΣ γᵗPᵗr ,         PλV (s) := (1 − λ) Σ(λγ)ᵐPᵐ⁺¹V  .

		

In the proofs below, we will use the above, simplified notation to obtain contraction estimates on 
the
dynamical system (4). These estimates will leverage the fact that Pλ is nonexpansive and γ < 1, and
from this notation contraction rates in terms of γ will arise naturally. However, by Lemma 3.3, we
know that the contraction rate of Tλ is γλ. Rewriting the proofs with γ      γλ will show the 
stronger
contraction.

A.1    OVER-PARAMETRIZED REGIME

Lemma 3.2 (Perturbation of the metric).  Let G₀ be a compact subset of a linear space G. For v(0) ∈
G₀, let gv be a continuous, self-adjoint linear operator that is positive definite in a 
neighborhood of
v(0) when restricted on G. Then for all ε > 0 there exists δ > 0 such that, for all v ∈ Bδ(v(0)) ⊆ 
G₀

gv₍₀₎ = (1 + g˜v)gv ,

for a linear operator g˜v   :   F  →  F  with ǁg˜vǁ  <  ε . More specifically, let σmin  be the 
smallest

singular value of DVw₍₀₎. Then if Q ≤ (1 − γ)σ²   /(48LDV ), (10) holds with ǁg˜V ₍w₎ǁ <  ¹−γ  for

all w ∈ Bq(w(0)).

Proof of Lemma 3.2.  Let Bw = DVw₍₀₎DVwT(0)  − DVwDVwT. We carry out the proof for the case

σmin < 1 (else the result holds with σmin = 1 in Q), in which case we have for all w ∈ Bq(w(0)) 
that

2

ǁBwǁ ≤ 2LDV ǁw(0) − wǁ + (LDV ǁw(0) − wǁ)   ≤ 3LDV ǁw(0) − wǁ .

Then we can write

gw(0)  = (DVw(0)DVwT(0))−1  = (DVw DVwT + Bw )−1

= (gw−1(1 + gwBw))−¹ = (1 + gwBw)−¹gw

∞                                                      ∞

= Σ(−1)ⁿ(gwBw)ⁿgw = gw + Σ(−1)ⁿ(gwBw)ⁿgw .

	

Furthermore, by the assumptions on the regularity of V  and on the initial condition w(0) we have

that gwΣ≤ 4/σ²   1, provided that w ∈ Bq(w(0)) for Q as in Lemma 3.2. Therefore, the perturbation

 

		


w           n₌₁  −1)

Σ

(gwBw)

n                   n           Σ

n      Σ . 3LDV

Σn         1 − γ


ǁg˜wǁ = ǁ

n=1

(−1)

(gwBw)

ǁ ≤

n=1

ǁgwBwǁ

≤

n=1

2

min

/4 ǁw(0) − wǁ                4     .

The same proof applies in the general case with different, implicit constants.

12


Under review as a conference paper at ICLR 2020

Lemma 3.3.  (Tsitsiklis & Van Roy, 1997, Lemmas 1, 3, 7) Under Assumption 1, for any V, V˜

we have that


ǁTλV  − TλV˜ǁµ

≤ γλ

ǁV  − V˜ǁµ

for     γλ

:= γ  1 − λ

1 − γλ

≤ γ < 1 .              (A.1)

In particular there exists a unique fixed point of Tλ, V ∗ ∈ F  given by (1).

Proof of Lemma 3.3.  We first prove that   PV   µ        V   µ. This follows by Jensen inequality 
and by
the invariance of µ:


ǁPV ǁ2  = V TP TΓPV  =

S

µ(ds)(

S

P (s, ds′)V (s′))²


Then, writing

≤ ∫S2

µ(ds)P (s, ds′)V (s′)² =

S

µ(ds)V (s)² = ǁV ǁ2  .                 (A.2)


TλV (s) = (1 − λ)

mΣ=0

λᵐEs

m

t=0

γᵗr(st, st₊₁) + γᵐ⁺¹V (sm₊₁)Σ


= (1 − λ)

λm

m=0

Σ

.Σm

.Σm

γᵗEs [r¯(st)] + γᵐ⁺¹Es [V (sm₊₁)]Σ

Σ

	

where st is the process on S  induced by P  with initial condition s₀, we have contraction of the
operator Tλ in L²(S, µ) by

ǁTλ(V  − V˜)ǁµ = ¨(1 − λ) Σ λᵐ(γP )ᵐ⁺¹ .V (s) − V˜(s)Σ¨

¨           m=0                                                                       ¨µ


∞

≤ (1 − λ)

m=0

λᵐγᵐ⁺¹ ¨V (s) − V˜(s)¨

=  γ(1 − λ)   V (s)     V˜(s)      ,

1 − γλ                            µ

where in the inequality above we have used (A.2).  This proves that Tλ is a contraction in    , and
as such it must have a unique fixed point.  That this fixed point corresponds to (1) is immediately
checked by direct computation.

Theorem  3.1  (Over-parametrized  case).   Assume  that  σmin    >   0,  where  σmin  is  the  
small-
est  singular  value  of  DVw₍₀₎.    Assume  further  that  w(0)  is  such  that  ǁVw₍₀₎ǁ₀   <   M  
 :=

(1 − γ)²σ²   /(192κ²LDV ǁDVw₍₀₎ǁ), then for α > α₀ := ǁV ∗ǁ₀/M we have for all t ≥ 0 that


V ∗ − αV

ǁ2  ≤ ǁV ∗ − αV

ǁ2e− 1−γ t .                                  (A.3)

Recall that V ∗ is the exact value function given by (1). Moreover, if ǁVw₍₀₎ǁ₀ ≤ Cα−¹ for a 
constant

C > 0, then supt>0 ǁw(t) − w(0)ǁ = O(α−¹).


Proof of Theorem 3.1.  By setting Q := (1 − γ)σ²

/(48LDV ) and by the assumed Lipschitz smooth-


ness of V ,  DVw · DVwT

2

min

/4 as long as w  ∈  Bq(w(0)).   We would like to check a local

exponential contraction condition, i.e., that for all w(t) ∈ Bq(w(0)) we have


 d

U (αV

d t

w(t)

)      γ − 1 U (αV

2κ2

w(t)

) ,         for t > 0 .                              (A.4)

To obtain the above result we apply the chain rule:

d                                                    d

d t U (αVw₍t₎) = ⟨∂f U (αVw₍t₎) ,  d tαVw₍t₎⟩₀

13


Under review as a conference paper at ICLR 2020


= α⟨αV

w(t)

− V ∗ , DV

w(t)

 d

·  d tw(t)⟩₀

= ⟨αVw₍t₎ − V ∗ , DVw₍t₎ · DVwT(t)Γ(T λαVw₍t₎ − αVw₍t₎)⟩   .         (A.5)

Throughout, we define τq := inf{t < 0  :  w(t) /∈ Bq(w(0))}, gw  := (DVw · DVwT)−¹ (recalling
that the DVw · DVwT has full rank in Bq(w(0))) and write g₀ = (1 + g˜w)gw, where g˜w is defined in
Lemma 3.2. Then, as long as t < τq we have, for every a, b ∈ F

⟨a, DVw₍t₎ · DVwT(t)Γb⟩₀ = ⟨a, (1 + g˜w₍t₎)Γb⟩ ≤ ⟨a, b⟩µ + ǁg˜w₍t₎ǁǁaǁµǁbǁµ .

By the above result we can bound from above the RHS  of (A.5) by


 d

U (αV

) ≤ ⟨αV

−V ∗, TλαV

−αV

⟩  +ǁg˜

ǁǁαV

−V ∗ǁ

ǁT   αV

−αV      ǁ   .


d t          w(t)

w(t)

w(t)

w(t)   µ

w(t)

w(t)               µ

w(t)

w(t)   µ

(A.6)

Recalling that by Lemma 3.3 we have

ǁTλαVw₍t₎ − αVw₍t₎ǁµ = ǁTλαVw₍t₎ − V ∗ǁµ + ǁαVw₍t₎ − V ∗ǁµ ≤ 2ǁαVw₍t₎ − V ∗ǁµ ,   (A.7)
and applying Lemma 3.2, we can bound the second term of (A.6) from above as


ǁg˜

ǁǁαV

− V ∗ǁ

ǁT   αV

− αV

ǁ    ≤  1 − γ ǁαV

− V ∗ǁ2  .          (A.8)

On the other hand, for the first term we have by Cauchy-Schwartz inequality and (A.1) that

⟨αVw₍t₎ − V ∗, TλαVw₍t₎ − αVw₍t₎⟩    = ⟨αVw₍t₎ − V ∗ , (TλαVw₍t₎ − V ∗) − (αVw₍t₎ − V ∗)⟩   ,

≤ ǁαVw₍t₎ − V ∗ǁµǁTλαVw₍t₎ − V ∗ǁµ − ǁαVw₍t₎ − V ∗ǁ2

≤ (γ − 1)ǁαVw₍t₎ − V ∗ǁ2  ,                                          (A.9)

where γ is the contraction rate of the TD  difference in    , see (A.1). Finally, combining (A.8) 
and
(A.9) we obtain


 d

U (αV

) ≤  γ − 1 ǁαV

− V ∗ǁ2  ≤  γ − 1 ǁαV

− V ∗ǁ2 ,              (A.10)

and the last inequality results from the equivalence of norms         ₀ and         µ (both have 
full support
on a finite set). The desired result (A.3) follows directly from the above by Gro¨nwall’s 
inequality for
all t < τq.

It now only remains to show that under the given choice of α, we have τq =     . By the contraction
of Tλ Lemma 3.3 and our choice of Q < σmin/(2LDV ) we write


¨    w(t)¨

1

DV      ǁǁT   αV

− αV

2

DV       ǁǁαV

− V ∗ǁ   .

Integrating the above and combining with the result from (A.10) in the previous paragraph we have


2

ǁw(t) − w(0)ǁ₂ ≤  α ǁDVw₍₀₎ǁǁαVw₍₀₎ − V

4κ²

∗ǁ0

t

exp

0

∗

γ − 1 s   ds
2κ2


≤  α(1 − γ) ǁDVw₍₀₎ǁǁαVw₍₀₎ − V

ǁ₀ .                                 (A.11)

Given that   αVw₍₀₎     V ∗  ₀      2αM , the above quantity is bounded by Q and therefore τq =     
, as
desired.


Finally, from (A.11) we see that if ǁV
Mα₀) = O(α−¹) for all t > 0.

w(0)ǁ0

≤ Cα−¹ then ǁw(t) − w(0)ǁ₂

4κ²

α(1−γ)

ǁDV

w(0)

ǁ(C +

A.2    UNDER-PARAMETRIZED REGIME

Lemma 3.6.  There exists δ > 0 and α₀ > 0 such that the ball    ⁰(0)         ₀ is forward invariant 
and
forward complete with respect to the dynamics of (7) for all α > α₀.

14


Under review as a conference paper at ICLR 2020

Proof of Lemma 3.6.  We define the Lyapunov function U¯ (f )  :=  ¹ ǁf ǁ2, whose sublevel sets are

B0(0).  We prove forward invariance of such sets by showing that, on their boundary (i.e., on the
sphere Sʳ−¹ ⊂ F¯0  of radius δ), U¯ (f ) decreases along trajectories of (7) for α large enough.

Noting that Sʳ−¹ ⊂ F₀ upon taking δ small enough, we differentiate U¯ (V w₍t₎) WRT  time for w(t)

obeying (7) at points V  := V w₍t₎ ∈ Sʳ−¹:

 d U¯ (V ) =  1 ⟨V , g¯−¹  Dψ−¹Γ(Tλαψ−¹(V ) − αψ−¹(V ))⟩

=  1 ⟨V , (Dψ−¹)TΓ(Tλαψ−¹(V ) − αψ−¹(V ))⟩ + R  (V )


=  1 ⟨Dψ−¹V , r¯λ + α(γPλ − 1)ψ−¹(V )⟩

+ R  (V )


≤ ⟨Dψ−¹V , (γPλ − 1)ψ−¹(V )⟩

+  1 ǁDψ−¹V ǁ

ǁr¯  ǁ

+ |R  (V )| .    (A.12)

where we have defined Rg(V ) :=  ¹ ⟨V , g˜w₍t₎(Dψ−¹)TΓ(Tλαψ−¹(V ) − αψ−¹(V ))⟩ for g˜w from

Lemma 3.2. We now proceed to bound the last two terms on the RHS  from above. The second term
is of order α−¹  and therefore goes to 0 for α  →  ∞ while for the last one we have that, by the
equivalence of the norms ǁ  ·  ǁµ and ǁ  ·  ǁ₂,

1                                           Σ  λ                 λ                    −1          Σ

|R  (V )            V ǁ  ǁg˜      ǁǁ(Dψ−¹)TΓ  r¯   + (γP    − 1)αψ    (V )  ǁ


g       | ≤  α ǁ      ₂

1       

w(t)

V

−1   T     λ

2

−1   T           λ                 −1


≤  α ǁV ǁ₂ǁg˜w₍t₎ǁǁ(DψV    )

Γr¯  ǁ + ǁV ǁ₂ǁg˜w₍t₎ǁǁ(DψV    )

Γ(γP

− 1)ψ

(V )ǁ₂

≤ α−¹C + εR(δ)ǁV ǁ2  .                                                                              
            (A.13)

for a constant C bounded by the norm of all operators and, by Lemma 3.2 a positive function εR(δ)
with limδ→₀ εR(δ)  =  0.  By the bounds established above and the fact that ǁV ǁµ  ≥  κ−¹δ  for
V  ∈ Sʳ−¹ ⊂ F₀ it is sufficient to show that the first term in (A.12) satisfies

⟨Dψ−¹V , (γPλ − 1)ψ−¹(V )⟩    ≤ −εǁV ǁµ ,                                (A.14)

for δ small enough and a constant ε > 0 independent of δ. We Taylor-expand ψ−¹ around the origin,
denoting the second order remainder of that expansion by R₂(  ,   ), and since ψ−¹(V ₀) = 0 we
have,

⟨Dψ−¹V , (γPλ − 1)ψ−¹(V )⟩    = ⟨Dψ−¹V , (γPλ − 1)Dψ0−1V ⟩                  


V                                                         µ                   V                     
                                     µ

+ ⟨Dψ−¹V , (γPλ − 1)R₂(V , V )⟩

,    (A.15)

where we have introduced the short hand notation Dψ−¹ = Dψ−¹. By the Lipschitz smoothness of

V 0

ψ−¹( · ) (Lee, 2003) we can bound the norm of the second term from above as

⟨Dψ−¹V , (γPλ − 1)R₂(V , V )⟩    ≤ 2ǁDψ−¹V ǁµǁR₂(V , V )ǁµ ≤ 2LDψ−1 ǁDψ−¹ǁǁV ǁ3  .


V                                                             µ                      V

V                 µ

(A.16)

For the first term in (A.15) we can also expand Dψ−¹  = Dψ0−1  + R˜2(V , · ) , and by applying a

similar bound as (A.16) we obtain that


⟨Dψ−¹V , (γPλ − 1)Dψ−¹V ⟩

≤ ⟨Dψ−¹V , (γPλ − 1)Dψ−¹V ⟩

+ 2LDψ−1 ǁDψ−¹ǁǁV ǁ3  .


V                                             0           µ                   0

0           µ                                      0

µ

(A.17)

The second term of the above equation being O(ǁV ǁ3), we now consider the first one.  By the
nonexpansion of P  in ǁ  ·  ǁµ proven in Lemma 3.3 we have

⟨Dψ−¹V , (γPλ − 1)Dψ−¹V ⟩    ≤ γǁDψ−¹V ǁµǁPλDψ−¹V ǁµ − ǁDψ−¹V ǁ2

≤ (γ − 1)ǁDψ−¹V ǁ2  ≤ (γ − 1)(σDψ−1 )²ǁV ǁ2  ,  (A.18)

where σDψ−1  denotes the smallest singular value of Dψ−¹ in B0(0). Combining (A.16), (A.17) and


min

(A.18) we finally obtain

⟨Dψ−¹V , (γPλ − 1)ψ−¹(V )⟩

δ

≤ ǁV ǁ2 ((γ − 1)(σDψ−1 )² + C′κ−¹ǁV ǁ  ) ,            (A.19)
15


Under review as a conference paper at ICLR 2020

for C′ = 2LDψ−1 (ǁDψ0−1ǁ + ǁDψ−¹ǁ) and recalling that κ is the equivalence constant between the
norms         µ and         ₀ in    ₀. ² Therefore, choosing δ small enough we obtain (A.14) and 
conclude
the proof of forward invariance.

By boundedness of B0(0) in F₀, forward completeness follows directly from forward invariance.

Lemma 3.7.  There exists l > 0, δ > 0 and α₀ > 0 such that for all α > α₀ and all geodesics γ(s)

contained in the ball B0(0) ⊆ F₀, the function


is strictly decreasing in s.

⟨γ′(s), X(γ(s))⟩₀ − ls⟨γ′(0), γ′(0)⟩₀ ,

Proof of Lemma 3.7.  To simplify the notation and the forthcoming computation, we prove the differ-
ential version of the desired result, i.e., we show that there exists l > 0 such that


 d  [  γ′(s), X(γ(s))
d s

₀ − ls⟨γ′(0), γ′(0)⟩₀] < 0 .                             (A.20)

The above expression exists almost everywhere by Lipschitz continuity of the terms to be differenti-
ated. When this is not the case, we must interpret this derivative in the sense of distributions. 
We will
highlight the steps where this could be necessary as we go along the proof.

In our case, X is the RHS  of (11) mapped through ψ onto F₀, i.e.,

X(γ(s)) = − 1 g¯−¹  (Dψ−¹  )TΓ(Tλαψ−¹(γ(s)) − αψ−¹(γ(s))) .

We are going to consider the ”flattened” manifold obtained by the maps φ and ψ equipped with
the metric g¯₀.  In this space, geodesics have the form γ(s) = v₁ + s∆v where ∆v := v₂ − v₁ for
v₁, v₂ ∈ F₀ and their derivative is γ′(s) = ∆v. Consequently (A.20) reads

d                                   2


where defining g˜γ₍s₎ := g¯₀g¯−¹

⟨∆v, ds X(γ(s))⟩₀ < lǁ∆vǁ₀ ,                                            (A.21)

− 1 as in Lemma 3.2 we have

 d X(γ(s)) =   d g¯  g¯−¹  (Dψ−¹  )TΓ(Tλ(αψ−¹(γ(s))) − αψ−¹(γ(s)))


=   d X¯ (γ(s)) + g˜       d X¯ (γ(s)) + Dg˜

(X¯ (γ(s)), γ′(s)) .         (A.22)


for

ds                     γ(s) ds

γ(s)

X¯ (γ(s)) := (Dψ−¹  )TΓ(Tλ(αψ−¹(γ(s))) − αψ−¹(γ(s))) .

We proceed by analyzing the first term in the above equation and leave the task of bounding the 
last

two for later. Using ∂sαψ−¹(γ(s)) = αDψ−¹  γ′(s) = αDψ−¹  ∆v we have that

γ(s)                                 γ(s)

 d X¯ (γ(s)) =  1 (D²ψ−¹  )T(Γ(Tλαψ−¹(γ(s)) − αψ−¹(γ(s))), ∆v)                   (A.23)


ds                    α

γ(s)

+ (Dψ−¹  )TΓΣDTλDψ−¹  ∆v − Dψ−¹  ∆vΣ ,

where (D²ψ−¹  )T denotes the inversion of the last two indices of the Hessian.  We now proceed
to consider the two terms in the sum above separately (multiplied by the scalar product of (A.21)),
defining throughout (TD)s := Γ(Tλαψ−¹(γ(s)) − αψ−¹(γ(s))). For the first term we have:

1 ⟨∆v, D²ψ−¹  (TD  , ∆v)⟩   ≤ ǁ∆vǁ2ǁD²ψ−¹   .α−¹r¯λ + (γPλ − 1)ψ−¹γ(s)Σ ǁ ≤ ε′ǁ∆vǁ2 ,

²We recall that by the construction of the mappsings ψ, φ, πr  and by our assumption in Theorem 3.5 
the
metric tensor g¯t has full rank on    0  and being the latter set compact its eigenvalues are 
uniformly bounded
from below.  At the same time, we can equip     0  with the metric induced by Γ by restricting it 
to its first r
elements, which are uniformly bounded from below. Hence, the two metrics are equivalent on this 
space for
some equivalence constant κ.

16


Under review as a conference paper at ICLR 2020

for any ε′ >  0 by using the linearity of the Hessian and bounding its operator norm of ψ−¹  on
a compact space in    ₀ while choosing α large enough and δ small enough, since γ(s)         ⁰(0).
Note that if Dψ−¹ is not differentiable, the above computation is to be understood in the sense of
distributions.

We now focus on the second term of (A.23). In this case we incorporate the operator Γ in the inner
product and write this term as

⟨Dψ−¹  ∆v, DTλDψ−¹  ∆v⟩   − ǁDψ−¹  ∆vǁ2  .

Now, by the contraction property of Tλ onto the tangential space Tψ−1   F  in the norm ǁ  ·  ǁµ we 
can

write

⟨Dψ−¹  ∆v, DTλDψ−¹  ∆v⟩    ≤ ǁDψ−¹  ∆vǁµǁPλDψ−¹  ∆vǁµ ≤ γǁDψ−¹  ∆vǁ2  ,


so that

γ(s)

γ(s)           µ

γ(s)

γ(s)

γ(s)           µ

⟨Dψ−¹  ∆v, DTλDψ−¹  ∆v⟩   − ǁDψ−¹  ∆vǁ2  ≤ (γ − 1)ǁDψ−¹  ∆vǁ2  .             (A.25)

Denoting by σDψ−1 , σDψ−1  the largest and smallest, respectively, singular values of the map Dψ−¹


max

min

in B0(0) (which are bounded away from 0 upon possibly making this set smaller), by nondegeneracy
of Dψ−¹ and by the equivalence of the ǁ  ·  ǁµ and ǁ  ·  ǁ₀ norms on F₀ we have that


κ−¹σDψ−1 ǁ∆vǁ

≤ ǁ∆vǁ

σDψ−1  ≤ ǁDψ−¹  ∆vǁ

≤ ǁ∆vǁ

σDψ−1  ≤ κǁ∆vǁ  σDψ−1  .


min                    0

Thus we have

µ    min

γ(s)           µ

µ    max

0    max

ǁDψ−¹  ∆vǁ2  ≥ κ−².σDψ−1 Σ2ǁ∆vǁ2 .                                    (A.26)

Getting back to the last two terms in (A.12), we immediately see from Lemma 3.2 that g˜γ₍s₎ is a
small, Lipschitz continuous perturbation. Hence, the product

⟨γ′(s), g˜γ₍s₎X¯ ′(γ(s))⟩

can be bounded from above similarly to (A.13), while the second order derivative in the third term 
of

(A.22) can be dealt with analogously to what is done in (A.24), giving terms ε′′ǁ∆vǁ2  and 
ε⁽³⁾ǁ∆vǁ2

0                                  0

respectively, both going to 0 as δ → 0.

Therefore, combining the above with (A.24), (A.25) and (A.26) we have


⟨∆v,

 

dt X¯ (γ(s))⟩₀ ≤

γ − 1

κ2

Dψ

min

−1 Σ2

ǁ∆vǁ₀ +

.Σ3

ε⁽ⁱ⁾(δ)Σ

ǁ∆vǁ₀

≤  γ − 1 .σDψ−1 Σ2ǁ∆vǁ2 .


This directly gives (A.21) by choosing l large enough.

2κ2

min                          0

The next lemma estimates the distance between the fixed point V˜∗ of the dynamics (7) and V ∗ given
by (1), showing that it is close, for large values of α to the best linear model in the tangent 
space of
w at Vw₍₀₎, given by Π₀V ∗. We recall that the projection operator Π₀ onto the linear space spanned
by the columns of DV  is given by (Tsitsiklis & Van Roy, 1997, Eq. (1))


Π₀W  :=           arg min

{DVw₍₀₎∆w  : ∆w∈Rp}

ǁDVw(0)∆w − W ǁµ  = DVw(0)(DVwT(0)ΓDVw(0))−1DVwT(0)ΓW ,

for all W  ∈ F  where, if necessary, we interpret (DVwT(0)ΓDVw₍₀₎)−¹ as a pseudo-inverse.

Lemma A.1.  Let V˜∗ be the fixed point of (7) and V ∗ be the global fixed point of the TD  operator,
given by (1). Then under the assumptions of Theorem 3.5 there exists constants α₀ > 0 and C∗ > 0
(independent of α₀), such that


ǁV˜∗ − V ∗ǁ

<  1 − λγ ǁΠ  V ∗ − V ∗ǁ

+ C∗α−¹ ,                           (A.27)

1 − γ

where Π₀ is the projection operator onto TV ₍w₍₀₎₎Fw.

17


Under review as a conference paper at ICLR 2020

To prove the above result we compare the dynamics (7) to the dynamics of the model V  when

linearized at w(0). In this case, the dynamics of the parameters is given by


 d  w¯(t) = DV T

Γ(TλV

− V       ) ,                                    (A.28)

where V  ∈ F  is the linear, tangent model of V  at w(0) defined as

Vw := Vw₍₀₎ + DVw₍₀₎(w − w(0)) .                                       (A.29)

We can also write the dynamics of the linear model as


 d

d t V

w¯(t)

:= DV

w(0)

· DVwT(0)

Γ(TλV

w¯(t)

− Vw¯(t)

) .                           (A.30)

Scaling the model as V  → αV  and t → α−¹t we obtain the analogue of (7):

 d  w¯(t) :=  1 DV T   Γ(TλαV        − αV       ) .                               (A.31)


which in F  reads

d t               α

 d

αV        := DV

w(0)

· DV T

w¯(t)

Γ(TλαV

w¯(t)

− αV       ) .

Proof of Lemma A.1.  Recall from (Tsitsiklis & Van Roy, 1997, Lemma 6) that for the linear value
function approximation one has


ǁV∗ − V ∗ǁ

<  1 − λγ ǁΠ  V ∗ − V ∗ǁ

,                                    (A.32)

1 − γ

where Π₀ is the projection on TV ₍w₍₀₎₎   w and    ∗ is the unique fixed point of the dynamics 
(A.30)
on that space. In light of this result, our task reduces to bounding the distance between the 
trajectories
of the original (i.e., dynamics (7)) and the linearized model (i.e., dynamics (A.31)) by Cα−¹ for C
large enough. We do so in 3 main steps. First of all, we bound the maximal excursion of the models
V  and V . Mapping both dynamics onto a common coordinate space, we then bound from above the
distance between the two trajectories in this space by O(α−¹). Finally, we map the dynamics back to

the embedding space and show that the correction is again of the same order O(α−¹).

Bounding the maximal excursion.    To compare the dynamics of αVw₍t₎ and α  w¯₍t₎ we map them
to a common space. Recalling the definition of the maps φ, πr, ψ from the proof of  Theorem 3.5 we
note that the first order expansion of ψ, maps TV ₍w₍₀₎₎Fw to F₀.  Explicitly, for V  ∈  F₀ and for

∆V  ∈ TV ₍w₍₀₎₎Fw with ǁ∆Vǁ₀ small enough we have

ψ¯(Vw₍₀₎ + ∆V) := Dψ₀∆V         and        ψ¯−1(V ) = Vw₍₀₎ + Dψ0−1V  ∈ TV ₍w₍₀₎₎Fw .    (A.33)

Now, we proceed to show that the dynamics of (7) and (A.31), mapped to    ₀, do not exit a ball    
⁰(0),
when choosing δ = C/α for C large enough. We show this with the same strategy used for the proof
of Lemma 3.6, i.e., we show that U¯ (f ) :=  ¹ ǁf ǁ2  decreases on Sʳ−¹(0) along the trajectories 
of

interest (note that δ is now much smaller than that used in Lemma 3.6). We will start with the 
curved

dynamics (7) and will then show that the same result follows, in a simpler setting, for (A.31). For

V  := V w₍t₎ ∈ Sʳ−¹(0) we start by bounding, as in (A.12), the derivative


 d  U¯ (V ) ≤ ⟨Dψ−¹V , (γPλ − 1)ψ−¹(V )⟩

+  1 ǁDψ−¹V ǁ

ǁr¯  ǁ

+ |R  (V )| .         (A.34)

Before bounding the above terms we recall that by Lipschitz smoothness of ψ we have that

ǁψ−¹(V )ǁ < ǁVw₍₀₎ǁ + ǁDψ0−1V ǁ + LDψ−1 ǁV ǁ2 .                          (A.35)

Then, since Vw₍₀₎ = 0, similarly to (A.12) we have for the last term in (A.34) that, for α large 
enough,

|Rg(V )| ≤ ǁg˜wǁǁV ǁ₂.ǁV ǁ₂ǁ(Dψ−¹)TΓ(γPλ − 1)ǁ(ǁDψ0−1ǁ + LDV ǁV ǁ₂)

+  1 ǁ(Dψ−¹)TΓr¯λǁ  Σ .

18


Under review as a conference paper at ICLR 2020

By the equivalence of the norms ǁ  ·  ǁµ, ǁ  ·  ǁ₂ and ǁ  ·  ǁ₀ on Πr and since δ = C/α we have 
that

|Rg(V )| ≤ ǁg˜wǁǁV ǁ2(K + 1) + O(α−³) ,                                  (A.36)
upon increasing C if necessary and defining K  = κ²ǁ(Dψ−¹)TΓ(γPλ − 1)ǁǁDψ−¹ǁ for κ₂ the

equivalence constant between         ₂ and         ₀ on Πr. The second term in (A.34) can be bounded
similarly to the above by the equivalence of norms:


1                                          

κ²ǁDψ−¹ǁǁr¯λǁµ

ǁDψ−¹V ǁµǁr¯λǁµ ≤ ǁV ǁ2                  V                       .                             
(A.37)

The first term in (A.34) can be treated identically to the proof of Lemma 3.6 to obtain (A.19).
Changing the norm in (A.19) and combining it with (A.36) and (A.37) gives


 d  ¯                   2

U (V ) ≤ ǁV ǁ

. γ − 1

(σDψ−1

)² +

κ²ǁDψ−¹ǁǁr¯λǁµ

+ ǁg˜wǁ(K + 1)Σ

+ O(α−³) .

Since γ − 1 < 0, we can choose C large enough to make the second term in brackets smaller than
(γ     1)/12κ²(σDψ−1 )². The same holds for the third term in brackets by (10), and for the higher
order term by taking α large enough, showing that

 d  U¯ (V ) ≤  γ − 1 (σDψ−1 )²ǁV ǁ2  < 0 ,

as desired. We note that the same reasoning with LDV  = 0 and Dψ−¹ ≡ Dψ0−1  yields an identical
conclusion for the dynamics of     in a ball of radius δ = C/α for C, α large enough. Also, we note
that combining the above computation with (A.16) yields


ǁDψ−¹Γ(Tλαψ−¹(V ) − αψ−¹(V ))ǁ ≤ ǁDψ−¹Γǁ(ǁr¯λǁ + α(γ + 1)ǁDψ0−1V ǁ + αL

−1 ǁV ǁ  )

≤ (γ + 1)ǁDψ−¹Γǁ(ǁDψ0−1ǁC + ǁr¯λǁ + O(α−¹))

≤ C₀ ,                                                                           (A.38)

for C₀ large enough, where Dψ−¹Γ is considered as an operator mapping F₀ → F¯0.

Bounding the distance of trajectories.    The distance between two trajectories with the same 
initial
condition can be bounded by    (α−²) using a similar argument as in (Chizat & Bach, 2018b, Lemma
B2)   for the present framework. We include the proof of this lemma here as the assumptions are not
identical and to make the paper self-contained, while we do not claim any improvement on that 
result.
To enounce this result, we recall that σDψ−1  denotes the smallest singular eigenvalue of Dψ−¹ in a

ball B0(0), which is bounded away from 0 for δ small enough. Similarly, we recall that g¯−¹ ≤ σᵍ   
1


δ

for σᵍ

> 0 in B0(0) for δ small enough.

t               min

Lemma A.2.  Let V t, Vt in F₀ be solutions of


 d  V   = g¯−¹(Dψ−¹)TΓ(Tλαψ−¹(V

) − αψ−¹(V

)) ,

V   = g¯−¹(Dψ−¹)TΓ(Tλαψ¯−1(V  ) − αψ¯−1(V  )) .

Then  defining  K  :=  supt>0 ǁ(g¯t−1  − g¯0−1)(Dψ−¹)TΓ(Tλαψ−¹(V t) − αψ−¹(V t))ǁ  and  β  :=

¹−γ (σDψ−1 )² we have that


κ2        min

1 2K

sup ǁV t − Vtǁ₀ ≤  α   β   .

Proof of Lemma A.2.  We define the function h(t) :=  ¹ ǁV t − Vtǁ2, take its time derivative


and defining

2                           0

h′(t) = ⟨V ′  − V′ , V t − Vt⟩   ,

(TD)t := Tλαψ−¹(V t) − αψ−¹(V t) ,

19


Under review as a conference paper at ICLR 2020

(T D)t := Tλαψ¯−1(Vt) − αψ¯−1(Vt) ,

we evaluate (for simplicity of notation, we introduce the short hand Dψ−¹ := Dψ−¹ for the rest of

V t

the proof)

V ′  − V′ =  1 g¯−¹(Dψ −1)TΓ(TD)  −  1 g¯−¹(Dψ−¹)TΓ(T D)

≤  1 Σg¯−¹(Dψ−¹)TΓ(TD)  − g¯−¹(Dψ−¹)TΓ(T D) Σ                  (A.39)

+  1 Σg¯−¹(Dψ−¹)TΓ(TD)  − g¯−¹(Dψ−¹)TΓ(TD) Σ .            (A.40)

We look at the two terms on the RHS  separately and obtain, for (A.39)


1 ⟨g¯−¹(Dψ−¹)TΓ(TD)

− g¯−¹(Dψ−¹)TΓ(T D) , V

− V  ⟩

(A.41)


=  1 ⟨(Dψ−¹)TΓ(TD)

− (Dψ−¹)TΓ(T D) , V

− V  ⟩


1

=     ⟨(TD)

− (T D) , Dψ−¹(V

− V  )⟩

(A.42)


+  1 ⟨(Dψ−¹ − Dψ−¹)TΓ(TD) , V

− V  ⟩ .                           (A.43)

We immediately see that by Lipschitz smoothness of ψ−¹ and the equivalence of         ₂ and         
₀

norms on Πr and (A.38), for (A.43) we have


1          −1

−1   T

1                                              

C1 √

by choosing C₁ large enough. For (A.42) by the definition of ψ we have

(TD)t − (T D)t = Tλαψ−¹(V t) − Tλαψ¯−1(Vt) − α(ψ−¹(V t) − ψ¯−1(Vt))

= α(Pλ − 1)(ψ−¹(V t) − ψ¯−1(Vt)) ,

and hence, by (A.35) we have


1

⟨(TD)

− (T D) , Dψ−¹(V

− V  )⟩

≤ ⟨(Pλ − 1)(ψ−¹(V

) − ψ¯−1(V  )), Dψ−¹(V

− V  )⟩

≤ ⟨(Pλ − 1)Dψ0−1(V t − Vt), Dψ0−1(V t − Vt)⟩

+ LDψ−1 ǁV tǁ2 ǁDψ−¹ǁǁV t − Vtǁµ .

Defining β :=  ¹−γ (σDψ−1 )², the first term from above can be bounded as in (A.18) to obtain

κ2        min

⟨(Pλ − 1)Dψ0−1(V t − Vt), Dψ0−1(V t − Vt)⟩    ≤ −βh(t) ,                     (A.45)

while for the second by our choice of δ = C/α we have


2             −1

C2                                  −1   √


LDψ−1 ǁV tǁµǁDψ0    ǁǁV t − Vtǁµ ≤  α2 κLDψ−1 ǁDψ0    ǁ

Finally, combining (A.44), (A.45) and (A.46) we have

2h(t) .             (A.46)

(A.41) ≤ −βh(t) + C2 √2h(t) ,                                          (A.47)

where C₂ := C₁ + C²κLDψ−1 ǁDψ0−1ǁ.

We now consider (A.40). Here by the definition of K we have


1 ⟨(g¯−¹ − g¯−¹)Dψ−¹Γ(TD) , V

K

V   − V  ǁ

=  K √2h(t) .

Combining the above with (A.47) we finally obtain

h′(t) ≤ −βh(t) + K √2h(t) + C2 √2h(t) ≤ −βh(t) + 2K √h(t) ,

20


Under review as a conference paper at ICLR 2020

for α large enough.  The above expression is negative as soon as h(t) > 4K²/(αβ)².  Therefore,
because h(0) = 0, we must have that h(t) ≤ 4K²/(αβ)² for all t > 0, i.e.,

1 2K


as claimed.

ǁV t − Vtǁ₀ <  α

for all t > 0 ,

β

To achieve the claimed     (α−²) bound, we observe that K  in the above Lemma can be chosen

(α−¹) by the Lipschitz continuity of g¯t−1.  Indeed, since we chose   V   ₀  = C/α, by (A.38) we
have that


−1                                               Dψ−¹

C       β K′

K ≤ sup ǁΓ(TD)tǁǁDψV    ǁLg¯−1 ǁV ǁ₀ ≤ C₀σmax      Lg¯−1  α  ≤  2  α  ,

t>0                                           ᵗ       ⁰                                             
                             ⁰

for K′ large enough, and therefore

K′

ǁV t − Vtǁ₀ <  α2             for all t > 0 .                                       (A.48)

Mapping to the embedding space.    We conclude the proof by mapping back to the original space,
where we have

sup ǁVt − Vtǁµ = sup ǁαψ−¹(V t) − αψ¯−1(Vt)ǁµ


t>0

t

≤ sup α .ǁDψ−¹(V t − Vt)ǁµ + LDψ−1 ǁV tǁ2 Σ

≤ α .κǁDψ−¹ǁ sup ǁV t − Vtǁ₀ + κ²LDψ−1  sup ǁV tǁ2Σ .

Then, letting    ∗ be the fixed point of (A.30) (unique and attracting by Tsitsiklis & Van Roy 
(1997)),
by our choice of δ = C/α, (A.32) and (A.48) we have that

ǁV˜∗ − V ∗ǁµ ≤ ǁV∗ − V ∗ǁµ + sup ǁVt − Vtǁµ


1 − γλ

∗         ∗           1

−1         ′        2                      2


as claimed.

≤   1 − γ  ǁΠ₀V

− V   ǁµ + α (κǁDψ0    ǁK

+ κ  LDψ−1 C  ) ,

21

