Under review as a conference paper at ICLR 2020

DISTANCE-BASED   COMPOSABLE   REPRESENTATIONS
WITH  NEURAL  NETWORKS

Anonymous authors

Paper under double-blind review

ABSTRACT

We introduce a new deep learning technique that builds individual and class rep-
resentations  based  on  distance  estimates  to  contextual  dimensions  for  different
modalities.  Recent works have demonstrated advantages to creating representa-
tions from probability distributions over their contexts rather than single points in
a low-dimensional Euclidean vector space. These methods, however, rely on pre-
existing features and are limited to textual information.  In this work, we obtain
generic template representations that are vectors containing the average distance
between the distribution of a class and that of contextual information, the latter of
which consists of a selection of a thresholded number of classes from the power
set of classes.  These representations have the benefit of being both interpretable
and composable. They are initially learned by estimating the Wasserstein distance
for different data subsets with deep neural networks.  Individual samples or in-
stances can then be compared to the generic class representations, which we call
templates, to determine their similarity and thus class membership. We show that
this technique, which we call WDVec, delivers good results for multi-label im-
age classification.   Additionally,  we illustrate the benefit of templates and their
composability by performing retrieval with complex queries where we modify the
information content in the representations. Our method can be used in conjunction
with any existing neural network and create theoretically infinitely large feature
maps.

1    INTRODUCTION

We introduce a new deep learning technique to create interpretable and composable representations
both for generic classes as well as for individual samples based on distance estimates with respect 
to
contextual information. The generic class representations, which we refer to as ‘templates’, express
how samples from a class relate to other classes on average and can be used to efficiently determine
class membership.

Most neural network-based approaches to representation learning,  focus on learning locations of
entities in a low dimensional Euclidean vector space.  Word2Vec,  for example,  extracts meaning
from the learned location of words in a vector space (Mikolov et al., 2013a;b).  Other text-based
approaches use the hidden state vector of LSTM networks (Hochreiter & Schmidhuber, 1997) that
learn useful information while performing sequence-to-sequence learning tasks (Sutskever et al.,
2014), such as machine translation (Wu et al., 2016), or surrounding sentence reconstruction (Kiros
et al., 2015).  For images, relevant features are typically extracted from convolutional neural net-
works (CNNs) (LeCun et al., 1998). When they are pre-trained on a classification task, they are con-
sidered to contain abstract knowledge that can be used as an input to perform further tasks (Vinyals
et al., 2015; Karpathy & Fei-Fei, 2015; Xu et al., 2015).

Such point-based representations have achieved great results for many tasks,  but lack flexibility.
Word2Vec representations don’t change depending on the context.  CNN-based representations fail
to accurately relay all relevant features for interacting objects in scenes.  Recent works in 
Natural
Language Processing (NLP) have attempted to make point estimations dependent on the context, for
example ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), yet such representations still
lack interpretability.  Other approaches have started looking at representations that are 
essentially
probability distributions that are built from the co-occurrence of particular contexts.  Singh et 
al.

1


Under review as a conference paper at ICLR 2020

(2019) create sentence embeddings by defining a Context Mover’s Distance over words occurring
in different contexts and Wu et al. (2018) create text document embeddings with feature maps by
estimating the distance of a document to a range of arbitrarily created documents.  In this work we


Estimated distance
to environment "bat"

Football
Baseball

Template for
Football

Template for
Baseball


Representations are created
for input samples.

Representations for individual samples
are interpretable along contextual
dimensions and ...

... can be compared to generic
class representations.

Figure 1:  Our method creates representations for individual samples that can be interpreted along
contextual dimensions,  which we call environments.   Those subvectors can then be compared to
generic class representations, which we refer to as ‘templates’.

will build on the latter approaches by creating a method that is not restricted by modality type. 
Our
approach relies on neural networks to estimate Wasserstein distances between the sample spaces for
different classes and the sample spaces for events that we call environments.  These environments
are defined by the occurrence of any element in a subset of attributes where each subset, limited to
a certain size, is selected uniformly from the power set of all attributes. While in this work we 
limit
the definition of attributes to simply refer to ‘class labels’, future work might build on a broader
interpretation. In the simplest case, when the size of the subsets is limited to 1, each 
environment is
given by the occurrence of a singular class label. The goal is to capture some common sense knowl-
edge about how classes relate to a range of differentiating contexts. Such environments are bound to
have distinctive features in relation to any given class as they are made up of a combination of 
arbi-
trarily chosen factors. This mechanism loosely resembles the associative nature of human memory.
Long-term memory storage is believed to rely on semantic encoding that performs better if it can be
associated with existing contextual knowledge (Lepage et al., 2000; Murdock, 1982). Additionally,
relating classes to arbitrarily chosen environments increases the probability of encountering 
distinc-
tive diagnostic features, which should help retrieval, as might be the case with human long-term
memory (Nairne, 2002).

We also propose a method to generate template representations of classes such that individual input
samples can be interpreted in reference to the template representations. A template representation 
is
thus a vector containing the average distance estimates between the sample distribution of a 
particu-
lar class and those of the available environments. This approach, illustrated in figure 1, 
addresses the
limitation of Euclidean point based representations where no distinction is made between generic
class representations and specific instance representations.  With the use of such templates, class
membership of individual samples can then easily be determined by computing the similarity be-
tween the representation of an individual sample and all template representations.  We will show
that, for the same neural network, this method performs well on multi-label classification tasks for
images, where any image can be assigned several classes (for example, ‘baseball’ and ‘bat’).

Both  the  template  and  individual  sample  representations  are  interpretable  in  the  sense  
that  they
contain distance estimates along contextual dimensions.  Samples that belong to a particular class
can be analyzed as to how and to what extent they differ from the generic template representation.
A sample that is classified as a ‘cat’ might have a smaller distance to the ‘bear’ class, suggesting
that that particular cat looks more like a bear than the average cat. Another advantage is that we 
can
alter or combine representations in order to modify the information content. The usefulness of these
characteristics becomes apparent in retrieval applications. We will demonstrate that it allows one 
to
retrieve an image similar to a particular image but with altered information content.

2


Under review as a conference paper at ICLR 2020

CONTRIBUTIONS

We introduce a technique to build representations based on probability distribution esti-
mates that can be used in downstream tasks. The method, which we call WDVec (Wasser-
stein Distance Vector), is general in that it can be combined with any neural network.

The obtained representations are interpretable as distance estimates between class and con-
textual information. They are composable in the sense that they can be modified to reflect
different class membership or contextual information. Similar inputs lay near each other in
the representation space.

We define template representations for concepts to which individual samples can be com-
pared.  This allows efficient class-membership computation and easy manipulation in the
representation space.

2    BACKGROUND

We will shortly explain some useful concepts, mostly in relation to distance estimates and how they
can be found with neural networks. Such distance estimates will form the basis for our 
representation
learning method further on. We will also refer to recent work that uses similar tactics.

Useful concepts    Central to our approach is the concept of the ‘Earth Mover’s Distance’ (EMD)
(Rubner et al., 2000), also known as the Wasserstein distance.  It can be understood as the minimal
amount  of  effort  that  is  required  to  move  the  mass  from  one  probability  distribution  
to  another.
Finding the EMD between two distributions is traditionally done by solving the ‘optimal transport
problem’ (Hitchcock, 1941; Altschuler et al., 2017; Singh et al., 2019).

While some approaches Kusner et al. (2015); Wu et al. (2018) use some variation of Sinkhorn itera-
tions (Sinkhorn, 1964; Altschuler et al., 2017) to solve the transport problem, neural network-based
approximations of the EMD have also been developed in recent years. This is usually in connection
to  Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) where two neural networks,
a generator and a discriminator, are staged against each other. Most recent GAN formulations make
use of an Integral Probability Metric (IPM) that essentially defines the loss function for a neural
network (referred to as the critic f  instead of the discriminator) that allows it to maximally dis-
criminate between two distributions. Most famously, in the Wasserstein GAN (WGAN) formulation
(Arjovsky et al., 2017), the IPM of the critic attempts to estimate the Wasserstein distance between
the distribution of real samples versus the distribution of generated samples.  As the generator im-
proves, the distance estimated by the critic should approach zero. The loss function for the critic 
of
the Wasserstein GAN can be defined as follows:

dF (R, G) = sup ,xE  f (x) − xE  f (x),.                                  (1)

where R represents the distribution of real samples in a dataset and G represents the distribution 
of
fake (or generated) samples. F is a set of measurable, symmetric and bounded real valued functions.
If F is unbounded, equation 1 will scale without bounds.

In order for this approximation to work,      thus needs to be bounded.  Usually, the critic network
is restricted to be a 1-Lipschitz function.  Several methods have been proposed to enforce this con-
straint, such as weight-clipping (Arjovsky et al., 2017), gradient penalties (Gulrajani et al., 
2017) or
spectral normalization (Miyato et al., 2018). The Fisher GAN formulation takes a different approach
and bounds      by construction, that is, by defining a data dependent constraint on its second 
order
moments (Mroueh & Sercu, 2017). The IPM becomes:

xE  [f (x)] − xE  [f (x)]


dFisherF (

R, G) = sup

∼R

1/ E

∼G

f 2(x) + 1/ E

f 2(x)

(2)

Equation 2 can be interpreted as the search for a critic function f that maximizes the average 
discrep-
ancy between two distributions R and G (thus maximizing inter-class variance) whilst minimizing
the second order discrepancy (i.e., limiting intra-class variance) (Mroueh & Sercu, 2017).  Upon
completing training, the numerator thus gives a good Wasserstein distance estimate when inter-class

3


Under review as a conference paper at ICLR 2020

variance is small in comparison to intra-class variance. When this is not the case, the numerator 
will
be large, yet not an exact approximation of the Wasserstein distance.  In practice, the Fisher GAN
IPM can be estimated with neural network training where the numerator in equation 2 is maximized
while the denominator is expressed as a constraint that is enforced with a Lagrange multiplier.

Recent work    The EMD has been successfully applied to NLP problems.  Kusner et al. (2015),
for example, define the ‘Word Mover’s Distance’ (WMD), which measures the minimal amount of
effort to move Word2Vec based word embeddings from one document to another.  Their method is
interpretable and outperforms other document distance metrics in text-based classification tasks. Wu
et          al. (2018) improve upon their solution by defining a Word Mover’s Embedding, an 
unsupervised
feature representation for documents, created by concatenating Word Mover’s Distance estimates
to arbitrarily chosen feature maps.  They then calculate an approximation of the distance between
a pair of documents with the use of a kernel over the feature map.  The building blocks of the fea-
ture maps are thus documents built from an arbitrary combination of words.  This idea is based on
the Random Features approximation (Rahimi & Recht, 2008) that suggests that randomized feature
maps are useful for approximating shift-invariant kernels.  Singh et al. (2019) build on this idea 
to
create unsupervised sentence representations where each entity is a probability distribution based 
on
co-occurrence of words. They embed the distributions in a low-dimensional representation space for
text         and demonstrate state-of-the-art performance on tasks such as sentence similarity and 
word en-
tailment. They note that their approach captures uncertainty and allows to interpret the outcome 
over
different contexts. Also relevant is the concept of ‘classemes’ in computer vision, where rich 
features
are created out of a combination of visual concepts (Torresani et al., 2010).  Finally, ‘Wasserstein
Discriminant Analysis’ (Flamary et al., 2018) obtains features through dimensionality reduction by
maximizing Wasserstein distances between classes while minimizing intra-class discrepancy.

Our method borrows some concepts from these works,  such as the creation of possibly infinite-
dimensional feature maps and representations that are built on (randomized) contextual information.
WDVec can be used with any modality though and features can be found with neural network-based
EMD estimations between different sample spaces.  Additionally, we will create generic template
representations  as  well  as  individual  sample  representations,  which  allows  efficient  
membership
tests          and complex retrieval queries. Our representations will be both interpretable and 
composable.

3    WDVEC: METHOD

We first define some notions that are of use to understand the WDVec method.

3.1    NOTATIONS

Given a dataset with distribution P, we denote PE as the sample space for the event E. In this 
paper,
we compare the sample spaces Pci  of classes ci with the sample spaces Pₑj  of environments ej. Let
nc denote the amount of classes and nₑ the amount of environments for the representation spaces
that are built in this work. Each environment ej is composed of the union of rj attributes ak, where
rj is an integer uniformly chosen from the range [1, R] with R the maximum amount of attributes
per environment.  In turn, each attribute ak is uniformly selected without replacement from the set
of all possible attributes. As a concrete example, given R = 5, one might obtain r₁ = 2 and r₂ = 3.
Environments might then become e₁ = (a₅     a₁₃), e₂ = (a₄     a₉     a₅₄) and so on. Pₑ1  would 
thus
refer to the sample space for which attribute a₅ or a₁₃ occurs. Note that ‘attribute’ in principle 
can
be interpreted in a general manner, as it can refer to any attribute, class, feature, and so on. We 
limit
the scope in the experiments in section 4, where attributes will simply be classes, i.e., 
environments
will be built from combinations of classes. We leave it to future work to explore other 
possibilities.

3.2    CONTEXTUAL DISTANCE

We propose to represent each sample as a feature map that is constructed with distance estimates be-
tween the distributions of classes and environments. Therefore, we calculate the estimated Wasser-
stein distance Wij(Pci , Pₑj ) between the sample spaces of all classes ci in C and of all 
environments
ej.  Intuitively, one can understand why such co-occurence contains useful information.  A subset
of image samples containing ‘cats’, for example, will have a relatively small distance to the 
subset

4


Under review as a conference paper at ICLR 2020

containing ‘dogs’, and a larger distance to the subset containing ‘fork’. If one of the 
environments is
defined by the occurrence of ‘cats’ as well, the estimated distance should be closer to 0. For a 
con-
crete example, see table A5 in the appendix. In cases where the estimated distance is not small, the
IPM optimization essentially splits samples from the classes and the environments by maximizing
the outputs relating to distinctive features of both groups.  In the next section we explain how 
such
class representations can be built with neural networks, as well as how representations for 
individual
samples can be built.

3.3    IMPLEMENTATION WITH NEURAL NETWORKS

To get estimates of the EMD, we will train a critic f to maximize the Fisher IPM in equation 2. The
advantage of the Fisher IPM over other Wasserstein distance estimation methods, is that any neural
network can be used as f as long as the last layer is a linear, dense layer. Empirically, the 
Fisher IPM
also leads to more stable and accurate distance estimates.  The distance Wij(Pci , Pₑj ) can then be
found by maximizing equation 2 over the distributions Pci  and Pₑj . As there are nc classes and nₑ
environments, this would require training nc*nₑ critics which is not feasible in practice. Therefore
we pass samples through a common neural network N for which the output layer has a dimension of
nf .  These nf features are then passed to nc*nₑ single layer neural networks, the outputs of which
constitute the estimates for all Wij(Pci , Pₑj ).

During training,  any given mini-batch will contain samples for many different c and e.   For any
sample s, backpropagation is then performed efficiently by multiplying all nc*nₑ outputs fij with

a mask that is 1 if s ∈ Pci   or s ∈ Pₑj , 0 otherwise.  The average for a critic, x  E   [fij(x)], 
can

be  calculated  over  all  samples  (weighted),  or  only  over  the  samples  where  the  mask  is 
 1  (non-
weighted). Depending on the application, the first can improve performance for unbalanced datasets
by implicitly including an estimate of the probability of occurrence of a particular class. 
Algorithm
A1 in appendix A.4 explains the algorithm in detail.

For typical GAN training, the loss function receives separate batches of real and fake samples. In 
our
case, to improve efficiency the same batch is used for both Pci  and Pₑj  as the multiplication 
with the
mask guides backpropagation over different contexts. While the last layers impose a slightly larger
memory footprint,  computing time is barely impacted compared to similar neural network-based
methods. Also, for applications where only the compact output representation needs to be stored in
a database, such as image retrieval, our method is very efficient. The general approach is 
illustrated
in figure 2. As we will discuss in the next section, to determine class membership, one only needs 
to
perform a similarity calculation between a compact representation and the template representations.

3.4    TEMPLATE REPRESENTATIONS


Let’s denote oij  =     E

x∼Pej

[f  (x)]        E

∼Pci

[fij(x)].  This is effectively an estimate of the distance

between the distributions of ci and ej.  This value can be found by saving the average value of this
output as all training batches pass through the network during the last training epoch. Thus, for 
each
class ci, the class template representation T (ci) is then defined as:

T (ci) = [oi₁oi₂...oine ]                                                        (3)

i.e., a vector containing distance estimates between the sample spaces of class ci and all nₑ 
environ-
ments ej.  Additionally, the representation for any individual sample s can be defined as the matrix
D(s) with nc rows and nₑ columns containing the distance estimates dij(s):


dij(s) =    E

x∼Pej

[fij(x)] − fij(s)                                                 (4)

The result is that for an input s with class label ci, Di,:(s) is correlated to Tci  as its 
distance estimates
with respect to all different environments should be similar, whereas this is not the case for a 
sample
without class label ci. Therefore, the cosine similarity between vector Di,:(s) and the template T 
(ci)
will be large for input samples from class i.

Such  templates  can  then  be  used  in  a  variety  of  ways,  for  example  in  multi-label  
classification
tasks (see section 4.2).   Finding the classes to which a sample belongs can simply be calculated

5


Under review as a conference paper at ICLR 2020

nf output

features                           f1R


Input batch

Neural
Network

f12

11

f21 f22

f2R

nc

ne

Figure 2:  Overview of the WDVec approach.  A neural network obtains nf  features from inputs.
These features serve as the common input for nc   nₑ critic functions fij. nc is the amount of 
classes
and nₑ is the amount of environments. These critics are trained to estimate the EMD between sample
spaces defined by classes ci and environments ej.  nc template representations Tci   are created by


computing    E

x∼Pci

[fij(x)] over all training samples of class ci. Each template has a dimension nₑ.

by computing the cosine similarity between Di,:(s) and T (ci) over all classes ci.  If they are more
similar than a threshold (the level of which is determined during training), the sample is assumed
to belong to class ci. Being able to check class membership for all classes is particularly handy 
for
tasks where multiple labels are connected to each sample. Similar to this method, one can also 
define
templates for each column (i.e., environment) dimension and infer which attributes are relevant for
the current sample. The template representations thus offer information about how different classes
and attributes relate to each other on average. This is shown in table A5.

Another application can be found in image retrieval (see section 4.3).  As the representations are
interpretable over contextual environments, complex queries can be formulated such as: Retrieve an
image that is similar to a particular sample and that contains a particular class ci yet doesn’t 
contain
a class cj.  Taking this one step further, we will discuss the ability to compose new 
representations
from existing ones in order to modify the content.

3.5    COMPOSING NEW REPRESENTATIONS

Given a sample s    Pc+  and s    Pc   with representation D(s) (see section 3.4). The goal is now 
to
modify D(s) such that it represents a sample s˜ for which s˜     Pc+  and s˜     Pc   while 
preserving the
contextual information of s, which in this case reflects how the sample relates to all other 
classes.

To achieve this, a Singular Value Decomposition of D(s): D(s) = USV is performed, such that the
rows of U and the columns of V can be interpreted as the factors leading to the distance estimates 
as
contributed by the corresponding classes ci and environments ej respectively. By making relatively
straightforward changes to U and V  and subsequently reconstructing the information, one is able to
compose a new representation D(s˜).  To control the extent to which we modify the representation
we introduce a factor 0 < q <= 1, where q = 1 is a fully modified representation and q < 1 is a
weighted combination of the original and modified representation.

Concretely, by increasing the value of Uc+,:, one can increase the distance estimate of a sample s
with respect to class c₊.  The opposite can be done for class c  .  As we know that s     Pc+  and
s     Pc  , it is thus sufficient to simply swap the values of Uc+,:  and Uc  ,: in order to 
reverse class
membership.

In this study, the attributes (from which the environments are constructed) are simply object 
classes.
In  order  to  change  class  membership  for  the  sample  s we  also  need  to  modify  all  the  
columns
V:,₊  and V:,   that correspond to environments that are constructed from c₊  and c   respectively.
Intuitively, for any sample s, the value of the j-th column V:,j that corresponds to an environment 
ej
is influenced by two factors. The first factor is the amount of classes r in ej. This is easily 
understood
as a larger r means more dissimilar samples and thus a larger distance estimate. The second factor 
is

6


Under review as a conference paper at ICLR 2020

whether the current sample s belongs to any of the classes from which ej is constructed, for 
example,
e₁  = (c₅     c₇) and s    Pc5 .  If that is the case, the values of D:,j(s) are mainly determined 
by the
values of U rather than V:,j.

A simple strategy is then to set each element of V:,₊  proportional to the amount of attributes r₊
its  environment  has:  V:,₊  =  (1      q)V:,₊  + q(V:,min + χ₊r₊(V:,mₐₓ     V:,min)/rmₐₓ),  where
rmₐₓ is the maximum amount of attributes in any environment.  As mentioned before, q is a factor
that allows intermediate solutions (thus not fully changing class membership if q < 1).  Next, we
set the value of each element of V:,   to be close to the mean value of each row of V  as follows:
V:,     = (1     q)V:,    + q(V:,mean + χ  r  (V:,max      V:,min)/rmax). V:,mean, V:,min and 
V:,max are
the mean, minimum and maximum as calculated over the rows of V  respectively.  χ₊ and χ   are
independent parameters between 0 and 1 and can be tuned on the validation set. By having χ₊ < 1,
one avoids mapping to outliers in V .  By setting χ   relatively small, V:,   will be close to 
V:,mₑₐn
and thus have less importance for the reconstruction of the representation than the values of U .

A valid representation can then be reconstructed by calculating the outer product Ds˜ = ΣσkU:,k ⊗


T

k,:

where σk are the eigenvalues of D(s).

Interestingly,  the spectrum of the singular values appears to be non-flat and the eigenvectors be-

longing to the largest eigenvalue can be visually inspected to determine to which classes the 
current
sample has the smallest distance. In section 4.3 we illustrate this methodology by modifying the in-
formation in existing representations and retrieving similar images to the modified 
representations.

4    EXPERIMENTS

To illustrate the approach we perform two types of experiments. First, in section 4.2, we show how
it compares to a (binary) cross-entropy baseline for multi-label image classification. In section 
4.3,
the unique benefits of the representations and their composability are illustrated in a retrieval 
setting.

4.1    SETUP

The experiments are performed on the COCO dataset (Lin et al., 2014) which contains multiple
labels for each image.  We use all available 91 class labels (which includes 11 supercategories that
contain other labels, e.g. ‘animal’ is the supercategory for ‘zebra’ and ‘cat’). One image can 
contain
more than one class label.  We use the 2014 train/val splits where we split the validation set into
two equal, arbitrary parts to have a validation and test set for the classification task ¹. Unless 
noted
otherwise, the model is a ResNet-18 with weighted calculation of averages.  nₑ is 300 and R is 40.
nf     is chosen to be 1024.  All images are randomly cropped and rescaled to 224     224 pixels.  
For
all runs, an Adam optimizer was used with learning rate 5.e    3. ρ for the Fisher GAN loss was set

to 1e−⁶. In all experiments in this work, the attributes from which the environments are constructed
are class labels, e.g.  e₁  = cpₑrsₒn    ccₐr.  Parameters are found empirically based on 
performance

on the validation set.

4.2    CLASSIFICATION

In this experiment, we compare WDVec to an approach that uses cross-entropy for multi-label image
classification.  The multi-label image classification task is of interest as multiple labels per 
image
need to be identified.  The typical approach uses a binary cross-entropy loss to determine whether
each label independently should be applied or not.   With WDVec,  classification is performed by
comparing classes to similar contextual environments, that in aggregate contain information about
all classes.  This is done efficiently through the use of the templates:  the cosine similarity is 
com-
puted between sample representation and template representation and compared to a threshold that
is determined during training.

We  use  some  recent  state-of-the-art  classification  models  to  compare  performance:   
ResNet-18,
ResNet-101 (He et al., 2016) and Inception-v3 (Szegedy et al., 2016).   For each experiment ex-
actly     the same neural networks are used in both approaches where only the last layers are 
modified.

¹Dataset splits will be published upon acceptance

7


Under review as a conference paper at ICLR 2020

Table  1:  F1  scores,  precision  (PREC)  and  recall  (REC)  for  different  models  for  the  
multi-label
classification task.  σ is the standard deviation of the F1 score over three runs.  BXENT refers to
binary cross-entropy loss. All results are the average of three runs.

     MODEL       METHOD       F1       PREC      REC           σ       

ResNet-18        BXENT          0.517     0.677       0.418       6.3e−³

ResNet-18        WDVec          0.529     0.600       0.473       3.5e−³

ResNet-101      BXENT          0.505     0.663       0.409       1.39e−²

ResNet-101      WDVec          0.538     0.595       0.494       2.6e−³

Inception-v3     BXENT          0.562     0.707       0.4667     9.4e−³

Inception-v3     WDVec          0.554     0.550       0.559       1.0e−³

In table 1, it is shown that WDVec performs better in terms of the F1 score when combined with the
ResNet models, and yields similar results with the Inception-v3 model. The performance of WDVec
does depend on the choice of the parameters nₑ and R. Increasing nₑ, the amount of environments,
leads in general to better performance, although it tends to plateau after a certain level.  For R, 
the
maximum amount of concepts per environment, a value of roughly nc/2 leads to relatively good
results.  This can be understood in the sense that combining a large amount of attributes creates a
unique subset to compare samples with. When R is too large, however, subsets with unique features
are no longer created and performance deteriorates.  The influence of nₑ and R is illustrated in the
appendix in figure A1. We also find that, even when nₑ and R are small, the outcome is not particu-
larly sensitive with regard to the choice of environments. This suggests that the amount and 
diversity
of environments is more important than the composition of the environments.  This is illustrated in
the appendix in tables A1 and A2.

4.3    RETRIEVAL

This experiment is designed to show the interpretability and composability of the representations.
We formulate some retrieval queries that seek to retrieve samples with modified class membership
while retaining contextual information as follows:  “Given a sample s that belongs to class c₊ but
not c  , retrieve the sample in the dataset that is most similar to s that belongs to c   and not 
c₊”.
c₊ and c   are uniformly chosen in each case from the remaining class labels.  For this experiment,
we uniformly select 100 samples from the validation set as reference images.  Let cos(x, y) be the
cosine similarity between two flattened representations x and y and let mean cos(x, y) be the mean
cosine similarity between x and y with the mean calculated over all class dimensions.  We denote
the representation for sample s as D(s) as defined in section 3.4.  For each reference sample sr we
retrieve from the remaining samples the nearest sample s according to the following methods:

1.  Nearest Neighbor (NN): cos(D(sr), D(s))

2.  SIM: cos(D(sr), D(s)) subject to cos(D₊,:(s), T₊) < t₊ and cos(D−,:(s), T−) > t−
where T₊, t₊, T− and t− are the template representations and thresholds for classes c₊ and
c− respectively.

3.  COMP slight:  mean cos(D(s¯r), D(s)) over all classes c for which cos(Dc,:(s), Tc)  >

0.9 × tc where D(s¯r) is a modified version of D(sr) with a factor q < 1 (here: q = 0.6).

4.  COMP heavy:  mean cos(D(s¯r), D(s)) over all classes c for which cos(Dc,:(s), Tc)  >

0.9 × tc where D(s¯r) is a modified version of D(sr) with a factor q = 1.

Remark that for methods 3 and 4, the similarity is calculated over class dimensions where classes
with  low  relevance,  i.e.,  those  that  have  a  low  similarity  with  the  templates,  are  
not  taken  into
account.   The  templates  are  thus  essential  to  methods  2,3  and  4.   Note  also  that  
methods  3  and
4    rely on the methodology of section 3.5 to compose new representations.  For these experiments
we set χ₊ and χ   to 0.2 and 0.07 respectively.  The distinction between methods 3 and 4 reflects
how the representations have acquired some type of common-sense knowledge: it is not necessarily
reasonable to retrieve an image that replaces a train with the category ‘orange’, as such a request
could  be interpreted in many ways. Moving to more dissimilar images is thus a reasonable outcome

8


Under review as a conference paper at ICLR 2020

Table 2: Precision and similarity scores for retrieved images. The baseline consists of CNN features
from  the  last  pooling  layer  of  a  ResNet-18  architecture.   For  comparison,  the  ‘NN’  
method  for
unaltered queries is added.  ‘COMP heavy’ achieves the highest precision for altered class queries.
Our method outperforms CNN features on all accounts in terms of precision. Note: the value of the
similarity between CNN and WDVec should not be compared directly as the features differ in size.

                                                 NN     SIM     COMP slight     COMP heavy  

CNN features     Precision     0.80     0.14             0.06                     0.23

Avg sim      0.85     0.78             0.74                     0.70

WDVec              Precision     0.85     0.31             0.47                     0.62

Avg sim      0.97     0.81             0.64                     0.60

in such cases. As a method to illustrate the sensitivity of the factor q, we measure the average q 
that
is needed to modify the class label from the retrieved sample in the COMP method. It turns out that
the expected q is 0.505 with a σ of 0.026, illustrating both that the composition method works as
expected and that class membership isn’t sensitive to small perturbations.

As a baseline we retain CNN features of size 512 from the last average pooling layer of the ResNet-
18 model.  To compare them to WDVec, we define templates for the CNN as the average feature
vector for a particular class.  The ‘SIM’ method can be directly applied.  For the ‘COMP’ methods,
we modify the features of a sample s by subtracting the template of c₊ and adding the one of c−.

The advantages of the composability of the representations become obvious in table 2. The precision
is shown, which is determined by whether the retrieved sample belongs to the desired class(es), as
well as the average similarity between retrieved image and queried image.  The ‘SIM’ method was
often able to find very similar samples that were misclassified however, thus leading to a 
relatively
low precision score.  With the ‘COMP’ methods a better balance between similarity and precision
can be found.   Some examples of retrieval results are presented in appendix A.3.   The obtained
representations can thus be interpreted, composed, and capture useful information such that similar
instances   are  near  each  other  in  the  representation  space.   The  templates  are  useful  
to  interpret
class membership efficiently and manipulate the instance representations as demonstrated here for
a retrieval task.  We see potential additional uses for the templates in future work, for example 
as a
reference representation that retains knowledge in a continual learning setting. Additionally, 
further
research might explore how the method can be applied to other tasks and modalities with alternative
building blocks for the environments.

5    CONCLUSION

Our main contributions are firstly the introduction of a technique to build representations that 
rely
on distance estimates that can be combined with any neural network.  This is demonstrated by per-
forming multi-label image classification with different state-of-the-art models and achieving good
results. Secondly, the representations are interpretable and composable in the sense that class mem-
bership  and  contextual  information  can  be  observed  and  modified  by  means  of  a  singular 
 value
decomposition. This is shown to be useful in a retrieval task where the class membership of samples
is    modified while contextual information is maintained.  Samples that are altered achieve a 
better
trade-off between precision and similarity than unaltered samples.  Finally, we introduce the con-
cept  of template representations which are generic class representations. We show how they lead to
efficient and accurate class membership calculation in a multi-label classification experiment.  Ad-
ditionally, they help achieve good precision in the retrieval task when representations are modified
along class dimensions.  The distance estimates in the templates also provide an overview of how
different classes relate to each other.

9


Under review as a conference paper at ICLR 2020

REFERENCES

Jason Altschuler, Jonathan Weed, and Philippe Rigollet. Near-linear time approximation algorithms
for optimal transport via sinkhorn iteration.  In Advances in Neural Information Processing Sys-
tems, pp. 1964–1974, 2017.

Martin  Arjovsky,   Soumith  Chintala,   and  Le´on  Bottou.      Wasserstein  gan.      arXiv  
preprint
arXiv:1701.07875, 2017.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.  Bert:  Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Re´mi Flamary, Marco Cuturi, Nicolas Courty, and Alain Rakotomamonjy. Wasserstein discriminant
analysis. Machine Learning, 107(12):1923–1945, 2018.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio.  Generative adversarial nets.  In Advances in Neural Infor-
mation Processing Systems, pp. 2672–2680, 2014.

Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.  Im-
proved training of wasserstein gans.  In Advances in Neural Information Processing Systems, pp.
5769–5779, 2017.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770–778, 2016.

Frank L Hitchcock. The distribution of a product from several sources to numerous localities. Jour-
nal of Mathematics and Physics, 20(1-4):224–230, 1941.

Sepp Hochreiter and Ju¨rgen Schmidhuber.  Long short-term memory.  Neural Computation, 9(8):
1735–1780, 1997.

Andrej Karpathy and Li Fei-Fei.  Deep visual-semantic alignments for generating image descrip-
tions.  In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
3128–3137, 2015.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Tor-
ralba, and Sanja Fidler.   Skip-thought vectors.   In Advances in Neural Information Processing
Systems, pp. 3294–3302, 2015.

Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to document
distances. In International Conference on Machine Learning, pp. 957–966, 2015.

Yann LeCun, Le´on Bottou, Yoshua Bengio, Patrick Haffner, et al.  Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Martin Lepage, Reza Habib, Holly Cormier, Sylvain Houle, and Anthony Randal McIntosh. Neural
correlates of semantic associative encoding in episodic memory. Cognitive Brain Research, 9(3):
271–280, 2000.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dolla´r,  and C Lawrence Zitnick.   Microsoft coco:  Common objects in context.   In European
Conference on Computer Vision, pp. 740–755. Springer, 2014.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.  Efficient estimation of word represen-
tations in vector space. arXiv preprint arXiv:1301.3781, 2013a.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean.  Distributed represen-
tations  of  words  and  phrases  and  their  compositionality.   In  Advances  in  Neural  
Information
Processing Systems, pp. 3111–3119, 2013b.

Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.  Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.

10


Under review as a conference paper at ICLR 2020

Youssef Mroueh and Tom Sercu. Fisher gan. In Advances in Neural Information Processing Systems,
pp. 2513–2523, 2017.

Bennet  B  Murdock.   A  theory  for  the  storage  and  retrieval  of  item  and  associative  
information.

Psychological Review, 89(6):609, 1982.

James S Nairne. The myth of the encoding-retrieval match. Memory, 10(5-6):389–395, 2002.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and

Luke Zettlemoyer.  Deep contextualized word representations.  In Proceedings of the 2018 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers), pp. 2227–2237, 2018.

Ali Rahimi and Benjamin Recht.  Random features for large-scale kernel machines.  In Advances in
neural information processing systems, pp. 1177–1184, 2008.

Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas.   The earth mover’s distance as a metric for
image retrieval. International Journal of Computer Vision, 40(2):99–121, 2000.

Sidak Pal Singh, Andreas Hug, Aymeric Dieuleveut, and Martin Jaggi. Context mover’s distance &
barycenters: Optimal transport of contexts for building representations.  In Proceedings of ICLR
Workshop on Deep Generative Models, 2019.

Richard Sinkhorn.  A relationship between arbitrary positive matrices and doubly stochastic matri-
ces. The Annals of Mathematical Statistics, 35(2):876–879, 1964.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Systems, pp. 3104–3112, 2014.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.  Rethink-
ing the inception architecture for computer vision.   In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2818–2826, 2016.

Lorenzo Torresani, Martin Szummer, and Andrew Fitzgibbon. Efficient object category recognition
using classemes. In European Conference on Computer Vision, pp. 776–789. Springer, 2010.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image
caption generator.  In Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference
on, pp. 3156–3164. IEEE, 2015.

Lingfei Wu, Ian EH Yen, Kun Xu, Fangli Xu, Avinash Balakrishnan, Pin-Yu Chen, Pradeep Raviku-
mar, and Michael J Witbrock.  Word mover’s embedding:  From word2vec to document embed-
ding. arXiv preprint arXiv:1811.01713, 2018.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim  Krikun,  Yuan  Cao,  Qin  Gao,  Klaus  Macherey,  et  al.   Google’s  neural  machine  trans-
lation  system:   Bridging  the  gap  between  human  and  machine  translation.     arXiv  preprint
arXiv:1609.08144, 2016.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio.  Show, attend and tell:  Neural image caption generation with visual
attention. In International conference on machine learning, pp. 2048–2057, 2015.

A    APPENDIX

A.1    HYPERPARAMETER SELECTION FOR ENVIRONMENTS

In image A1, the influence of R and nₑ on performance is shown for multi-label image classification.
In table A1 and A2, the standard deviations of the F1 scores are given for different values of nₑ 
and
R respectively.  The standard deviation is smaller than or equal to 1% indicating low sensitivity to
different choices of environments.  Also shown in tables A3 and A4 are the average values of the
spectral norms over 100 arbitrarily selected representations, for varying hyperparameters nₑ and R.

11


Under review as a conference paper at ICLR 2020

ne (log)

1                                                      10                                           
         100

0.55

0.5

0.45

F1 for diﬀerent ne (top)
F1 for diﬀerent R (bottom)

0.4

0.35

1                                                      10                                           
         100

R (log)

Figure A1:  Influence of R and nₑ on the F1 score for multi-label image classification using the
WDVec approach with ResNet-18. When modifying R, nₑ is fixed to 300. When modifying nₑ, R
is fixed to 40. All datapoints are the average of three runs.

Table A1:  Standard deviations of F1 scores of the classification experiment for different values 
of

nₑ. Each value is computed on the basis of three runs.

nₑ = 2     nₑ = 4     nₑ = 8     nₑ = 16

R = 1     3.6e−³     9.1e−³     9.5e−³     7.5e−³

A.2    DISTANCES BETWEEN DIFFERENT CLASSES

Table  A5  shows  some  rows  and  columns  of  a  template  representation  where  R is  1,  i.e., 
 every
environment  consists  of  1  particular  class.   The  results  show  that  related  concepts  
have  smaller
estimated distances than those that are less related.   Note that the class ‘mouse’ only appears in
the dataset in relation to computers, rather than animals, which is reflected in the estimates.  
Some
interesting things happen as well, as presumably some of the bananas in the dataset appear in images
with computers or similar contexts, thus leading to a smaller estimated distance between the classes
‘keyboard’ and ‘banana’. The distance between the class ‘vehicle’ and itself is accurately estimated
as exactly 0.

A.3    RETRIEVAL EXAMPLES

In figure A2, some results of retrieval of images are shown.  The goal of the ‘SIM’ and ‘COMP’
methods is to retrieve an image where the class of the retrieved image reflects the content change,
while maintaining the context of the original sample. The original classes in this case were respec-
tively: sheep, zebra, toilet, train.  They were modified into the following classes respectively: 
bear,
giraffe, airplane and orange. For the SIM method, similar images are often obtained as only results
are returned when the class membership, as determined by the cosine similarity with the templates,
allows for it. The method performs badly when the classification fails, as for example in the second
row of figure A2.  It often leads to very good results though, as in the third row where the ‘SIM’
method retrieves an image of a bathroom stall from an airplane or in the fourth row where an orange
truck is retrieved that is not a train.  The ‘COMP slight’ method reflects an intermediate trade-off
between similarity and modified class membership. ‘COMP heavy’ modifies class membership cor-
rectly more often, at the cost of similarity.

A.4    ALGORITHM

In algorithm A1 we shortly explain the exact algorithm to calculate the backpropagation during the
training phase.

12


Under review as a conference paper at ICLR 2020

Table A2:  Standard deviations of F1 scores of the classification experiment for different values 
of

R. Each value is computed on the basis of three runs.

R = 1      R = 2      R = 4      R = 8      R = 16

nₑ = 1     3.6e−³     4.2e−³     5.1e−³     1.3e−³     1.0e−²

Table A3: The average of the spectral norm was taken over 100 arbitrarily selected representations
for different values of nₑ.


R = 40

nₑ = 4     nₑ = 16     nₑ = 64     nₑ = 256
9.1e³        2.8e⁴          1.5e⁵          6.7e⁵

Table A4: The average of the spectral norm was taken over 100 arbitrarily selected representations
for different values of R.


nₑ = 300

R = 1     R = 4     R = 16     R = 32
2.6e⁵       6.6e⁵       7.8e⁵         5.3e⁵

Table A5: Example of a template representation with R = 1.  Classes that are related have smaller
estimated distances.  Rows are classes whereas columns are environments that are made up of 1
attribute (in this case an attribute is a class).


PERSON
VEHICLE
ANIMAL
BASEBALL  BAT
KEYBOARD

VEHICLE     SHEEP     BANANA     TRAIN     MOUSE     SPORTS BALL

5.09               5.11          4.99              4.60          5.06            4.32

0.0                 5.27          5.23              5.24          5.30            5.26

5.21               3.92          5.23              5.12          5.15            5.15

5.28               5.26          5.19              5.18          5.18            3.80

5.27               5.20          3.86              5.17          2.99            5.13

13


Under review as a conference paper at ICLR 2020


reference
image

nearest
neighbor

content
change

SIM

COMP

slight

COMP

heavy

not: sheep
but: bear

not: zebra
but: giraffe

not: toilet
but:
airplane

not: train
but: orange

Figure A2: Some results for image retrieval with the different methods. The column ‘nearest neigh-
bor‘ shows the most similar image without modification of the content. The column ‘content change’
refers to how the class of the representations was altered.  The nearest neighbors to the altered 
rep-
resentations are then retrieved with the ‘SIM’, ‘COMP slight’ and ‘COMP heavy’ methods respec-
tively.

14


Under review as a conference paper at ICLR 2020

Algorithm A1 Algorithm of the training process.   Note that in this work the attributes are class
labels.   For matrices and tensors,      refers to matrix multiplication and     refers to 
element-wise
multiplication.

For each environment uniformly sample r from the range [1, R] and uniformly select r attributes
from the set of all attributes.  Create a random feature vector v with shape [nc, nₑ] which has a
value of 1 for each uniformly selected attribute.  Each column thus has maximum R non-zero
entries.

Initialize λ as a tensor of zeros with shape [nc, nₑ].

while Training do

Sample a mini-batch b, with batch size nb, containing samples s and one-hot labels l, with
shape [nb, nc].

Create masks

Expand l to create a mask mc with shape [nb, nc, nₑ], such that mck,i,: = 1 if the k-th sample,

sk, belongs to class i, 0 otherwise.

Multiply l and v, then expand the result, to create a mask mr with shape [nb, nc, nₑ], such
that mrk,:,j  =  a where a is the sum of all the attributes of the k-th sample that are present in
environment j.

Calculate the FISHER GAN loss

Propagate b through the neural network to obtain outlₒgits with shape [nc, nₑ].
Calculate outP = outlₒgits × mr and outQ = outlₒgits × mc.

EP f = sum(outP , dim = 0)/sum(mr, dim = 0)

EP fs = sum(outP    outP , dim = 0)/sum(mr, dim = 0)

EQf = sum(outQ, dim = 0)/sum(mc, dim = 0)

EQfs = sum(outQ ∗ outQ, dim = 0)/sum(mc, dim = 0)

constraint = 1 − (0.5 ∗ EP fs + 0.5 ∗ EQfs)

Minimize the loss loss =    torch.sum(EP f    EQf + λ   constraint    ρ/2   constraint²)

end while

15

