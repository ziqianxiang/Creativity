Under review as a conference paper at ICLR 2020
Building Hierarchical Interpretations in Nat-
ural Language via Feature Interaction De-
TECTION
Anonymous authors
Paper under double-blind review
Ab stract
The interpretability of neural networks has become crucial for their applications
in real-world with respect to reliability and trustworthiness. Existing explanation
generation methods usually provide important features by scoring their individual
contributions to the model prediction and ignore the interactions between features,
which eventually provide a bag-of-words representation as an explanation. In nat-
ural language processing, this type of explanations is challenging for human users
to understand the meaning of an explanation and draw the connection between
explanation and model prediction, especially for long texts. In this work, we fo-
cus on detecting the interactions between features and propose a novel approach
to build a hierarchy of explanations based on feature interactions. The proposed
method is evaluated with three neural classifiers, LSTM, CNN, and BERT, on two
benchmark text classification datasets. The generated explanations are assessed
by both automatic evaluation measurements and human evaluators. Experiments
show the effectiveness of the proposed method in providing explanations that are
both faithful to models, and understandable to humans.
1	Introduction
Deep neural networks have become a significant component in natural language processing (NLP),
achieving state-of-the-art performance in various NLP tasks, such as text classification (Kim, 2014),
question answering (Rajpurkar et al., 2016), and machine translation (Bahdanau et al., 2014). How-
ever, the lack of understanding on their decision making leads them to be characterized as black-box
models and increases the risk of applying them in real-world applications (Lipton, 2016). Produc-
ing interpretable decisions has been a critical factor on whether people will trust and use the neural
network models (Ribeiro et al., 2016).
Most of existing work on local explanation generation for NLP focuses on producing word-level
explanations (Ribeiro et al., 2016; Lei et al., 2016; Plumb et al., 2018), where a local explanation
consists of a set of words extracted from the original text. Figure 1 presents an example sentence
with its sentiment prediction and corresponding word-level explanation generated by LIME (Ribeiro
et al., 2016). Although the LIME explanation captures a negative sentiment word waste, it presents
the explanation in a bag-of-words format. Without resorting to the original text, it is difficult for us
to understand the contribution of word a and of, as both of them have no sentiment polarity. The
situation will become even more serious when this type of explanations are extracted from longer
texts.
In this work, we present a novel method to construct hierarchical explanations of a model prediction
by capturing the interaction between features. Ultimately, our method is able to produce a hierarchi-
cal structure as illustrated in Figure 1. Produced by the proposed method, this example provides a
comprehensive picture of how different granularity of features interacting with each other for model
prediction. With the hierarchical structure, this example tells us how the words and phrases are
combined and what are the contributions of words and phrases to the final prediction. For example,
the contribution of the phrase of good is dominated by the word waste, which eventually leads
to the right prediction.
1
Under review as a conference paper at ICLR 2020
Figure 1: A negative movie review a waste of good performance with a LIME expla-
nation and a hierarchical explanation, where the color of each block represents the importance of
the corresponding word/phrase with respect to the model prediction.
To capture feature interactions, we adopt the interacted Shapley value (Lundberg et al., 2018), an
extension of Shapley value (Shapley, 1953) from cooperative game theory, to measure the inter-
actions between features. Based on the interaction scores, we propose a top-down method, called
InterShapley, to segment a text recursively into phrases and then words eventually. The pro-
posed method is evaluated on text classification tasks with three typical neural network models:
long short term memory networks (Hochreiter & Schmidhuber, 1997, LSTM) and convolutional
neural networks (Kim, 2014, CNN), and a state-of-the-art model BERT (Devlin et al., 2018) on
some benchmark datasets. The comparison of our method is against several competitive baselines
from prior work on explanation generation, including Leave-one-out (Li et al., 2016), Contextual
Decomposition (CD) (Murdoch et al., 2018) and its hierarchical extension (ACD) (Singh et al.,
2019), L- and C-Shapley (Chen et al., 2018), and LIME (Ribeiro et al., 2016).
Our contribution of this work is three-fold: (1) we propose an effective method to calculate feature
importance and extend the Shapley value to measure feature interactions; (2) we design a top-down
segmentation algorithm to build hierarchical interpretations based on feature interactions; (3) we
compare the proposed method with several competitive baselines via both automatic and human
evaluations, and show the InterShapley method outperforms the existing methods on both word-
and phrase-level explanations.
2	Related Work
Over the past years, many different research directions have been explored to build interpretable
models or improve the interpretability of neural networks: (1) tracking the inner-workings of neural
networks to understand their decision-making, such as contextual decomposition (CD) (Murdoch
et al., 2018; Godin et al., 2018); (2) decoding interpretable knowledge (e.g., contextual information,
latent representations) from networks to explain model predictions, such as gradient-based interpre-
tation methods (Hechtlinger, 2016; Sundararajan et al., 2017) and attention-based methods (Ghaeini
et al., 2018; Xie et al., 2017); (3) modifying neural network architectures to make their output more
explainable, such as retreating to simpler models (Alvarez-Melis & Jaakkola, 2018) or incorporating
strong regularizers (Yuan et al., 2019); and (4) developing explainable model-agnostic methods to
learn the behaviors of neural network models and explain them, such as LIME (Ribeiro et al., 2016)
and KernelSHAP (Lundberg & Lee, 2017). The first three directions have limited capacity in real-
world applications, as they require deep understanding of neural network architectures (Murdoch
et al., 2018) or only work with specific models (Alvarez-Melis & Jaakkola, 2018). On the other
hand, model-agnostic methods (Ribeiro et al., 2016; Lundberg & Lee, 2017) generate explanations
solely based on model predictions and can be used even without much expertise on machine learning
and deep learning. In this work, we also focus on model-agnostic interpretations.
2
Under review as a conference paper at ICLR 2020
Model-agnostic interpretations. Model-agnostic methods are applicable for any black-box mod-
els, generating explanations solely based on model predictions. Li et al. (2016) proposed a method,
called Leave-one-out, to probe the black-box model by erasing a given word and observing the
change in the probability on the predicted class. Another work, LIME (Ribeiro et al., 2016), esti-
mated individual word contributions locally by linear approximation based on perturbed examples.
A line of most related works are Shapley-based methods, such as SHAP (Lundberg & Lee, 2017), L-
Shapley and C-Shapley (Chen et al., 2018), where the variants of Shapley value (Shapley, 1953) are
used to evaluate feature importance. They mainly focus on the challenge of reducing computational
complexity of Shapley value by approximations (Kononenko et al., 2010; Datta et al., 2016).
Hierarchical interpretations. Previous work on interpreting neural networks mainly focuses on
word-level interpretations, where the top-ranked words are selected based on their contributions
to the prediction. There are a few works on building hierarchical interpretations. For example,
agglomerative contextual decomposition (ACD) (Singh et al., 2019) uses the word-level CD scores
as the joining metric determining which features are clustered together. Unlike our method, this work
is model-dependent, and proposes a bottom-up clustering method to aggregate features with respect
to their interactions. Besides, Lundberg et al. (2018) extented SHAP values to SHAP interaction
values to calculate the interactions between features along a given tree structure. Similarly, Chen
& Jordan (2019) suggests to utilize a linguistic tree structure to capture the contributions beyond
individual features for text classification. The difference with our work is that both methods from
(Lundberg et al., 2018; Chen & Jordan, 2019) require hierarchical structures given, while our method
constructs the structures without resorting external resources.
3	Method
In this section, we first introduce a new definition of feature importance score in subsection 3.1,
which forms the basis of the feature interaction detection method discussed in subsection 3.2. Then,
subsection 3.3 presents a novel method of building a hierarchy of explanations by recursively opti-
mizing the feature interaction score on a given text.
3.1	Feature Importance Score
In this paper, the feature importance score is usually understood as the contribution of a subset of
features to each model prediction. It is essential in building a hierarchy of explanations where inter-
action scores built on top of it are extensively used. Considering a set of features x = (x1 , . . . , xn),
and a subset of it written as xS, the feature importance score of xS is defined as follows,
f(xs ) = gy(xs) — ,ma?、,gy' (XS),	(1)
y0 6=y,y0 ∈Y
where gy (x) is the model output with respect to label y, Y is the possible label set, and y is the
predicted label obtained by the model, i.e., y = argmaxy∈γ gy (x); S is an integer set used to select
features with subscript values matching those in S, i.e. xS = {xi|i ∈ S}. More concretely, in this
work, x consists of a sequence of words (sentence or text), xi is the i-th word in x.
The feature importance score defined in Equation 1 measures the difference between the probability
on the predicted class and the probability of the second most probable class. It measures how far the
prediction is to the prediction boundary, hence the confidence of classifying xS into the predicted
label y. Particularly in text classification, it can be interpreted as the contribution to a specific class y.
For a given text x, xS is usually considered as a consecutive subsequence of words for interpretable
explanations. To compute gy(χs)in Equation 1, it is a common practice that words outside S are
replaced with the special token hpadi (Chen et al., 2018; Shrikumar et al., 2017; Lundberg & Lee,
2017) in order to keep the same length between x and xS . The effectiveness of Equation 1 as a
feature importance score is shown in subsection 4.1.
3.2	Feature Interaction Score
By considering the prediction process as a coalition game (Lundberg et al., 2018), where each player
(feature) contributes fairly, the feature interactions can be measured consistently, meaning the com-
puted scores are in line with human intuition and are robust when model changes (Lundberg et al.,
3
Under review as a conference paper at ICLR 2020
2018). Such scores often have the form of Shapley values. Motivated by the Shapley interaction
score (Fujimoto et al., 2006; Lundberg et al., 2018), and combined with our model-agnostic feature
importance score in subsection 3.1, the feature interaction between the p-th and q-th features in z is
computed as follows,
/	、—	ISl!(n -ISl- 1)!	(	6
ψ(p,q,z) = T	2(n - 1)!	γp,q(z, S)，
S⊆N∖{p,q}	, 卜
(2)
where z is a set of features, and a feature in z can be a word or a set of words; N is the index set
of features from z; ISI is the size of the set S. Additionally, γp,q(z, S) is the interaction between
feature zp and zq considering the contributions from other features in zS , which is defined as
γp,q(z, S) = f(zS∪{p,q}) -f(zS∪{p}) - f(zS∪{q}) + f(zS).	(3)
The total interaction between zp and zq is the sum of ψ(p, q, z) and ψ(q, p, z) (Lundberg et al.,
2018). Since ψ(p, q, z) is symmetric with respect to p and q, the total interaction between the p-th
element and q-th element in z is φ(p, q, z) = 2ψ(p, q, z).
3.3 Building Hierarchical Interpretation
To build a hierarchical interpretation, we adopt a top-down segmentation approach recursively split-
ting a feature set into two subsets with the minimum interaction score. In other words, a feature
set is partitioned into two subsets with strong intra-interactions and weak inter-interactions in each
step. The constructed hierarchy of explanations consists of multiple levels and each level is a set of
feature subsets.
Formally, considering the feature set at the '-th level of the hierarchy, z' = (z`,...,z^),
We can obtain various candidate sets z'+1 for the next layer by splitting one element z', ∀i ∈
{1,..., ng} at a time, where n` is the size of z'. For example, one z'+1 could be Z'+1 =
(z',..., Z'j1, Z'+1,..., Zn `), where z' is partitioned into Z'j1 and Z'j1, and the size of it is
n` + 1. Of course, such partition is not unique and the number of potential partitions depends on the
size of z' . The best feature set at level ` + 1 satisfies
z'+1 = argmin φ(i1,i2, Z'+1),	(4)
z'+1
s.t. z'+1 ∪ z'+1 = z'.	(5)
Equation 4 finds the best partition between j^'j1 and j^'j1 whose interaction score is minimum. At
level-zero, there is only one feature set with all input words included, z0 = x. By splitting z0 into
two subsets z11 and z21, we can obtain the feature set at level-one, z1 = [z11, z21]. This process stops
when all elements in a layer are word-level features. The top-down segmentation method is named
as InterShapley.
Note that in generally, the solution to Equation 4 and Equation 5 has exponential computational
complexity. In NLP, features (i.e. words) are usually arranged in order, meaning that i1 and i2
in Equation 5 are adjacent. With the aid of Shapley value approximation (Chen et al., 2018), where
only k neighbors are involved in computation, the computational complexity measured in number of
model evaluations will be O(4kn log n). So the method scales well with input text size n. Also note
that opposite to the top-down method, we can also combine words with the strongest interaction
into phrases, then phrases into larger phrases, resulting in the bottom-up method. In some cases, the
bottom-up method cannot capture the phrase having opposite sentiment polarity until the last step.
Detailed comparisons will be shown in Appendix A.
4 Experiments
The proposed method is evaluated on text classification tasks with three typical neural network
models, a long short-term memories (LSTM) (Hochreiter & Schmidhuber, 1997), a convolutional
neural networks (CNN) (Kim, 2014), and BERT (Devlin et al., 2018), on SST (Socher et al., 2013)
and IMDB (Maas et al., 2011) datasets. The detailed experimental setup is in Appendix B. We
4
Under review as a conference paper at ICLR 2020
employ both automatic and human evaluations on word-level and phrase-level explanations, to show
the ability of InterShapley in interpreting neural network models.
Table 1 shows the best performance of the models on both datasets in our experiments.
4.1	Word-level Evaluation
We adopt two evaluation metrics from prior work on evalu-
ating word-level explanations: the area over the perturbation
curve (AOPC) (Nguyen, 2018; Samek et al., 2016) and log-odds
scores (Chen et al., 2018). These two metrics measure local fidelity
by deleting or masking words in the decreasing order of their impor-
tance scores and comparing the probability change on the predicted
label.
Models	Dataset	
	SST	IMDB
LSTM	0.842	0.870
CNN	0.850	0.901
BERT	0.924	0.930
Table 1: The classification
accuracy of different mod-
els on the SST and IMDB
datasets.
AOPC. Word-level explanations provide a sequence of words ac-
cording to their importance scores. AOPC is calculated as the aver-
age change in prediction probability on the predicted class over all
of the test data by deleting top k% words. Higher AOPCs are better,
which means that the deleted words are important for model predic-
tion.
1N
AOPC(k) = N f{p(y | Xi)- p(y | X(k))},	(6)
N i=1
where y is the predicted label, N is the number of samples, p(y | ∙) is the probability on the predicted
class, and X(k) is constructed by dropping the top k% word features from Xi.
Log-odds. The log-odds score is calculated by averaging the difference of negative logarithmic
probabilities on the predicted class over all of the test data before and after masking the top r%
features with zero paddings. Under this metric, lower log-odds scores are better.
Log-odds(r) = ɪ X log p(yJxi J,	(7)
N i=1	p(y I Xi)
(r)
The notations are the same as in Equation 6 With the only difference that Xi ' is constructed by
replacing the top r% word features with the special token hpadi in Xi.
4.1.1	Results
Figure 2 shows the results of AOPCs and log-odds scores of LSTM model on the SST dataset. In-
terShapley achieves the best performance on both evaluation metrics. Together with the other
results in Appendix C, this evaluation demonstrates the effectiveness of our method in extracting
keywords via feature importance scores, which lays a solid foundation for feature interaction detec-
tion.
The L- and C-Shapley perform worse on the IMDB dataset than on the SST dataset, because a small
window size (2 as recommended) may cause relatively large error in approximating Shapley values
for long sentences. The performance of LIME drops significantly on BERT comparing to the other
two models, as Figure 9 and 10 show, which suggests that this locally linear approximation may
not be capable of deep neural network models. We also observed an interesting phenomenon that
the simplest baseline Leave-one-out can achieve relatively good performance, even better than L-
and C-Shapley, and CD. And we suspect that is because the criteria of Leave-one-out for picking
keywords matches the evaluation metrics.
4.2	Phrase-level Evaluation
This section presents some evaluation on phrase-level explanations extracted with the proposed
methods. Take the hierarchy in Figure 1 as an example, the set of phrases extracted from this hier-
archical structure include {waste of good performance, waste of good, of good}.
5
Under review as a conference paper at ICLR 2020
(a) AOPCs of LSTM on the SST dataset.
Figure 2: The AOPC and log-odds scores of LSTM on the SST dataset.
(b) Log-odds of LSTM on the SST dataset.
On phrase level, we can use the same way as in Equation 1 to compute importance scores. The quan-
titative evaluation in section 4.2.1 shows our method successfully captures the strongest interactions
using a contrastive comparison, and the qualitative analysis in section 4.2.2 illustrates how different
levels of features interact with each other while contributing to final predictions.
4.2.	1 Quantitative Evaluation
We evaluate the interaction effect of an important phrase by extracting the words within this phrase
and then randomly insert them back to the original sentence. For example, in a sentence with n
words x = (x1, . . . , xi, . . . , xj, . . . , xn), if (xi, . . . , xj) is the most important phrase, then for every
word in (xi, . . . , xj), we randomly pick a position in (x1, . . . , xi-1, xj+1, . . . , xn) and insert that
word back. Let X be the newly constructed word sequence in this way. The quantitative evaluation
is to compare the difference between p(y | x) and p(y | X). Intuitively, by breaking the strong
interaction within (xi, . . . , xj), it will lead to a significant drop on the probability of predicted label
y. To obtain a robust evaluation, for each sentence xi, we construct K different word sequences
{X(k) }K==1 and compute the average as
1N1K
COheSion-ioss = NEK E(P(U | χi)- p(y | χi )),	⑻
i=1	k=1
where X(k) is the kth disturbed sample of xi, and K = 100 is the number of perturbations for each
sentence. Note that when we pick the most important phrase, we consider the one whose length is no
more than 5 as the explanation because a very long clause is uninterpretable. Then we calculate the
average length of the most important phrases over all test data as the Average Explanation Length
(Ave. Exp. length).
Table 2 shows the results of cohesion-loss of InterShapley and ACD with different models on the
SST and IMDB datasets. For the IMDB dataset, we tested on a subset with 2000 randomly selected
samples due to computation costs. InterShapley outperforms ACD with higher cohesion-loss
and smaller average length of generated explanations on both datasets with LSTM, which indicates
that InterShapley can capture more important and concise phrases. Comparing the results of
InterS hapley for different models, the cohesion-loss of LSTM model improves on the IMDB
dataset, while BERT performs the opposite. We reason that it is the length constraint on selected
phrases that limits BERTs performance, since BERT often utilizes a large range of context for pre-
dictions.
4.2.2 Qualitative Analysis
Figure 3 visualizes the hierarchical interpretations of InterShapley and ACD for LSTM on a
negative movie review from the SST dataset, where red and green colors represent the negative and
6
Under review as a conference paper at ICLR 2020
Methods	Models	Cohes SST	ion-loss IMDB	Ave. SST	Exp. length IMDB
InterShapley	CNN	0.025	0.021	1.63	1.31
	BERT	0.055	0.020	2.7	3.35
	LSTM	0.021	0.039	2.07	2.07
ACD	LSTM	0.007	0.030	3.19	4.83
Table 2: Cohesion-loss and Average Explanation Length (Ave. Exp. length) of InterShapley
and ACD with different models on SST and IMDB dataset. As ACD is model-dependent, we only
compare it on the LSTM model.
(a) InterShapley for LSTM on SST.
Figure 3: InterShapley and ACD for LSTM on a negative movie review, where the importance
scores of InterShapley and CD scores are normalized for comparing. LSTM makes wrong pre-
diction (positive). Red and green colors represent the negative and positive sentiments respectively.
positive sentiments respectively. In this case, LSTM makes a wrong prediction as positive. Figure
3(a) shows that INTERSHAPLEY gives correct positive and negative sentiments for bravura and
emptiness respectively, and captures the interaction between them that bravura exercise
flips the polarity of in emptiness to positive, which explains why the model makes the wrong
prediction. However, ACD incorrectly marks the two keywords with opposite polarities, and misses
the feature interaction, as Figure 3(b) shows.
Figure 4 visualizes InterShapley for LSTM and BERT on a positive movie review, on which
BERT makes a correct prediction, while LSTM makes a wrong prediction. The comparison between
Figure 4(a) and Figure 4(b) shows the difference of feature interactions within the two models and
explains why a model makes the correct/wrong prediction. For example, Figure 4(b) illustrates that
BERT captures the key phrase not a bad in level 1, and thus makes the positive prediction, while
LSTM (as shown in 4(a)) misses the interaction between not and bad, and the phrases not a and
bad journey both push the model making the negative prediction. Due to the page limitation,
more examples are presented in Appendix D.
(b) ACD for LSTM on SST.
4.3 Human Evaluation
We employ 10 human annotators to do human evaluation on Amazon Mechanical Turk (AMT).
The most important feature (with the highest importance score) is picked from the hierarchy as
the explanation, with its length being limited to no more than five words for the ease of human
understanding. We evaluate the explanation by asking human annotators to guess the output of
a model based on the explanation and the input text (Nguyen, 2018). For each example, human
annotators choose a label from ”Negative”, ”Positive”, ”N/A” based on their understanding of the
explanation, where ”N/A” means that the explanation may be some irrelevant words/phrase, and
annotators cannot figure out the model’s prediction. An example interface is shown in Appendix E.
We measure the number of human annotations that are coherent with the actual model predictions,
7
Under review as a conference paper at ICLR 2020
(a) InterShapley for LSTM on SST.
(b) InterShapley for BERT on SST.
Figure 4:	InterShapley for LSTM and BERT on a positive movie review from the SST dataset, on
which BERT makes correct prediction (positive), while LSTM makes wrong prediction (negative).
Red and green colors represent the negative and positive sentiments respectively.
and define the coherence score as the ratio between the coherent annotations and the total number
of examples.
We randomly pick 60 movie reviews from IMDB dataset, half of which are used in one of the
following experiments. First, we compare the explanations obtained from InterShapley and
ACD with LSTM model. For the same model, higher coherence score means that the explanations
are more human understandable. Second, we apply InterShapley to the three neural models, and
compare the corresponding explanations. For the same interpretation method, a more interpretable
mode usually can achieve higher coherence score.
4.3.1 Results
Table 3 shows the coherence scores of the two interpretation methods. InterShapley
outperforms ACD with much higher coherence score, which means that the important fea-
tures captured by InterShapley are highly consistent with human comprehension in ex-
plaining model predictions. Table 4 shows the accuracy and coherence scores of differ-
ent models. InterShapley achieves relatively high coherence scores on all of the three
neural networks, which validates its ability in interpreting black-box models. Although
BERT can achieve higher prediction accuracy than other two models, its coherence score
is lower, which illustrates that the interpretability of more complex model is even worse.
Methods	Coherence Score
InterShapley	0.88
ACD	0.53
Table 3: Human evaluation of InterShap-
ley and ACD with LSTM model on the
IMDB dataset.
Models	Accuracy	Coherence scores
LSTM	0. 87	0.90
CNN	0.90	0.91
BERT	0.97	0.83
Table 4: Human evaluation of InterShapley with
different models on the IMDB dataset.
5 Conclusion
In this paper, we proposed an effective method, InterShapley, building model-agnostic hierar-
chical interpretations via detecting feature interactions. In this work, we mainly focus on sentiment
classification task. We test InterShapley with three different neural network models on two
benchmark datasets, and compare it with several competitive baseline methods. The superiority of
InterShapley is approved by both automatic and human evaluations.
8
Under review as a conference paper at ICLR 2020
References
David Alvarez-Melis and Tommi S Jaakkola. Towards robust interpretability with self-explaining
neural networks. In NeurIPS, 2018.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Jianbo Chen and Michael I Jordan. Ls-tree: Model interpretation when the data are linguistic. arXiv
preprint arXiv:1902.04187, 2019.
Jianbo Chen, Le Song, Martin J Wainwright, and Michael I Jordan. L-shapley and c-shapley: Effi-
cient model interpretation for structured data. arXiv preprint arXiv:1808.02610, 2018.
Anupam Datta, Shayak Sen, and Yair Zick. Algorithmic transparency via quantitative input influ-
ence: Theory and experiments with learning systems. In 2016 IEEE symposium on security and
privacy (SP),pp. 598-617. IEEE, 2016.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Katsushige Fujimoto, Ivan Kojadinovic, and Jean-Luc Marichal. Axiomatic characterizations of
probabilistic and cardinal-probabilistic interaction indices. Games and Economic Behavior, 55
(1):72-99, 2006.
Reza Ghaeini, Xiaoli Z Fern, and Prasad Tadepalli. Interpreting recurrent and attention-based neural
models: a case study on natural language inference. arXiv preprint arXiv:1808.03894, 2018.
Frederic Godin, Kris Demuynck, Joni Dambre, Wesley De Neve, and Thomas Demeester. ExPlain-
ing character-aware neural networks for word-level prediction: Do they discover linguistic rules?
arXiv preprint arXiv:1808.09551, 2018.
Yotam Hechtlinger. Interpretation of prediction models using the input gradient. arXiv preprint
arXiv:1611.07634, 2016.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Yoon Kim. Convolutional neural networks for sentence classification. arXiv preprint
arXiv:1408.5882, 2014.
Igor Kononenko et al. An efficient explanation of individual classifications using game theory.
Journal of Machine Learning Research, 11(Jan):1-18, 2010.
Tao Lei, Regina Barzilay, and Tommi Jaakkola. Rationalizing neural predictions. In Proceedings of
the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 107-117, 2016.
Jiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation
erasure. arXiv preprint arXiv:1612.08220, 2016.
Zachary C Lipton. The mythos of model interpretability. arXiv preprint arXiv:1606.03490, 2016.
Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances
in Neural Information Processing Systems, pp. 4765-4774, 2017.
Scott M Lundberg, Gabriel G Erion, and Su-In Lee. Consistent individualized feature attribution for
tree ensembles. arXiv preprint arXiv:1802.03888, 2018.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.
Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the
association for computational linguistics: Human language technologies-volume 1, pp. 142-150.
Association for Computational Linguistics, 2011.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-
tations of words and phrases and their compositionality. In Advances in neural information pro-
cessing systems, pp. 3111-3119, 2013.
9
Under review as a conference paper at ICLR 2020
W James Murdoch, Peter J Liu, and Bin Yu. Beyond word importance: Contextual decomposition
to extract interactions from lstms. arXiv preprint arXiv:1801.05453, 2018.
Dong Nguyen. Comparing automatic and human evaluation of local explanations for text classifi-
cation. In Proceedings of the 2018 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp.
1069-1078, 2018.
Gregory Plumb, Denali Molitor, and Ameet S Talwalkar. Model agnostic supervised local explana-
tions. In Advances in Neural Information Processing Systems, pp. 2515-2524, 2018.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the
predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference
on knowledge discovery and data mining, pp. 1135-1144. ACM, 2016.
Wojciech Samek, Alexander Binder, Gregoire Montavon, Sebastian Lapuschkin, and Klaus-Robert
Muller. Evaluating the visualization of what a deep neural network has learned. IEEE transactions
on neural networks and learning systems, 28(11):2660-2673, 2016.
Lloyd S Shapley. A value for n-person games. Contributions to the Theory of Games, 2(28), 1953.
Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through
propagating activation differences. In Proceedings of the 34th International Conference on Ma-
chine Learning-Volume 70, pp. 3145-3153. JMLR. org, 2017.
Chandan Singh, W. James Murdoch, and Bin Yu. Hierarchical interpretations for neural network
predictions. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=SkEqro0ctQ.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 conference on empirical methods in natural language pro-
cessing, pp. 1631-1642, 2013.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3319-
3328. JMLR. org, 2017.
Qizhe Xie, Xuezhe Ma, Zihang Dai, and Eduard Hovy. An interpretable knowledge transfer model
for knowledge base completion. arXiv preprint arXiv:1704.05908, 2017.
Hao Yuan, Yongjun Chen, Xia Hu, and Shuiwang Ji. Interpreting deep models for text analysis via
optimization and regularization methods. In AAAI, 2019.
10
Under review as a conference paper at ICLR 2020
Dataset	Classes	Average length	Train	Dev.	Test
SST-2	2	19	6920	872	1821
IMDB	2	268	22500	2500	25000
Table 5: Summary statistics for the datasets.
A Comparison between Top-down and Bottom-up Approaches
Given the sentence a waste of good performance for example, Figure 5 shows the hier-
archical interpretations for the LSTM model using the bottom-up and top-down approaches respec-
tively. Figure 5(a) shows that the interaction between waste and good can not be captured until
the last (top) layer, while the important phrase waste of good can be extracted in the intermedi-
ate layer by top-down algorithm. We can see that waste flips the polarity of of good to negative,
causing the model predicting negative as well.
(b) Top-down segmentation
Figure 5:	Hierarchical interpretations for the LSTM model using the bottom-up and top-down ap-
proaches respectively. Red and green colors represent the negative and positive sentiments respec-
tively.
B	Experimental Setup
Models. In this work, we use a recurrent neural network (RNN) with uni-directional one-layer
LSTM Hochreiter & Schmidhuber (1997), a CNN model with single convolutional layer Kim
(2014), and BERT Devlin et al. (2018) which has achieved remarkable performance on a variety
of NLP tasks including text classification. The CNN model includes a single convolutional layer
with filters of the window sizes ranging from 3 to 5. The LSTM is also single layer with 300 hid-
den states. Both models are initialized with 300-dimensional pretrained word embeddings Mikolov
et al. (2013). We use the pre-trained BERT model1 with 12 transformer layers, 12 self-attention
heads, and the hidden size of 768, and fine-tune it in different downstream tasks to achieve the best
performance.
Datasets. We use two benchmark datasets on text classification: the SST-2 corpus (Socher et al.,
2013) and the IMDB corpus (Maas et al., 2011). Table 5 shows the summary statistics of the datasets.
SST-2 is the binary-class version of the Stanford sentiment treebank and IMDB is a balanced dataset
with 25000 training examples and 25000 test examples. We split 90% of the training examples for
training, and the rest as the development set.
Competitive Baselines. We compare our method with some competitive baselines, including
Leave-one-out (Li et al., 2016), CD (Murdoch et al., 2018), L- and C-Shapley (Chen et al., 2018),
and LIME (Ribeiro et al., 2016) for word-level explanation generation, and ACD (Singh et al., 2019)
for phrase-level explanation generation.
1https://github.com/huggingface/pytorch-transformers
11
Under review as a conference paper at ICLR 2020
C Other results of AOPCs and log-odds
(a) AOPCs of LSTM on IMDB dataset.
Figure 6: The AOPC and log-odds scores of LSTM on IMDB dataset.
(b) Log-odds of LSTM on IMDB dataset.
—Leave-one-out
f LIME
—L-Shapley
0.15	0.2	0.25	0.3	0.35	0.4	0.45
Mask ratio
0.15
0.25	0.3
Drop ratio
0.4	0.45	0.5
(a) AOPCs of CNN on SST dataset.
(b) Log-odds of CNN on SST dataset.
Figure 7: The AOPC and log-odds scores of CNN on SST dataset.
—e— Leave-one-out
→-LIME
—L-Shapley
C-Shapley
→-INTERSHAPLEY
-→-C-Shapley
+ INTERSHAPLEY
12
Under review as a conference paper at ICLR 2020
Odo<
—e- Leave-one-out
→-LIME
—L-Shapley
-→-C-Shapley
→-INTERSHAPLEY
(a) AOPCs of CNN on IMDB dataset.
Figure 8: The AOPC and log-odds scores of CNN on IMDB dataset.
(b) Log-odds of CNN on IMDB dataset.
Leave-one-out
LIME
L-Shapley
—θ— C-Shapley
→-INTERSHAPLEY

-χ-∣
一 INTERSHAPLEY
L-Shapley
- C-Shapley
→- Leave-one-out
-H-LIME
0.05	0.1	0.15	0.2	0.25	0.3	0.35	0.4	0.45	0.5
Drop ratio
(b) Log-odds of BERT on SST dataset.
Figure 9: The AOPC and log-odds scores of BERT on SST dataset.
(a) AOPCs of BERT on SST dataset.

(a) AOPCs of BERT on IMDB dataset.
(b) Log-odds of BERT on IMDB dataset.
Figure 10: The AOPC and log-odds scores of BERT on IMDB dataset.
13
Under review as a conference paper at ICLR 2020
D Visualization of Hierarchical Interpretations
Figure 11: InterShapley for BERT on a positive movie review from SST dataset, on which
BERT makes correct prediction. Red and green colors represent the negative and positive sentiments
respectively.
Level O
it
never
Level 1
it
Level 2
it
Level 3
it
Level 4
it
Level 5
it
never
never
never
never
never
fails
to
Figure 12:	InterShapley for LSTM on a positive movie review from SST dataset, on which
LSTM makes wrong prediction. Red and green colors represent the negative and positive sentiments
respectively.
never fails
Level 3
to	engage us
H-1.00
H-0.75
0.50
0.25
-0.00
-0.25
-0.50
U--0.75
■--1.00
Figure 13:	ACD for LSTM on a positive movie review from SST dataset, on which LSTM makes
wrong prediction. Red and green colors represent the negative and positive sentiments respectively.
14
Under review as a conference paper at ICLR 2020
.L00
fl-0.75
tθ,5θ
0.25
-0.00
-0.25
-0.50
.—0.75
■-LOO
Figure 14: InterShapley for BERT on a positive movie review from SST dataset, on which
BERT makes correct prediction. Red and green colors represent the negative and positive sentiments
respectively.
Level O
harmless and mildly amusing family comedy
Level 1
Level 2
Level 3
Level 4
Level 5
Level 6
harmless and
harmless and
harmless
harmless
harmless
harmless
and
and
and
and
mildly amusing family comedy
mildly amusing family comedy
mildly amusing family comedy
mildly amusing family H
mildly amusing
mildly
amusing family
comedy
comedy
.L00
Hθ,75
∩-0.50
0.25
0.00
-0.25
-0.50
B--0.75
■-LOO
Figure 15: InterShapley for LSTM on a positive movie review from SST dataset, on which
LSTM makes wrong prediction. Red and green colors represent the negative and positive sentiments
respectively.
Figure 16: ACD for LSTM on a positive movie review from SST dataset, on which LSTM makes
wrong prediction. Red and green colors represent the negative and positive sentiments respectively.
15
Under review as a conference paper at ICLR 2020
E Human Evaluation Interface
[Example] Not a bad movie at all!
	Positive	Negative	N/A
A. bad movie	O	©	O
B. not a bad	©	O	O
Figure 17: Interfaces of Amazon Mechanical Turk where annotators are asked to guess the model’s
prediction based on different explanations.
16