Under review as a conference paper at ICLR 2020
P-BN: TOWARDS EFFECTIVE BATCH NORMALIZA-
tion in the Path Space
Anonymous authors
Paper under double-blind review
Ab stract
Neural networks with ReLU activation functions have demonstrated their success
in many applications. Recently, researchers noticed a potential issue with the op-
timization of ReLU networks: the ReLU activation functions are positively scale-
invariant (PSI), while the weights are not. This mismatch may lead to undesirable
behaviors in the optimization process. Hence, some new algorithms that conduct
optimizations directly in the path space (the path space is proven to be PSI) were
developed, such as Stochastic Gradient Descent (SGD) in the path space, and it
was shown that, SGD in the path space is superior to that in the weight space.
However, it is still unknown that whether other deep learning techniques beyond
SGD, such as batch normalization (BN), could also have their counterparts in the
path space. In this paper, we conduct a formal study on the design of BN in the
path space. According to our study, the key challenge is how to ensure the forward
propagation in the path space, because BN is utilized during the forward process.
To tackle such challenge, we propose a novel re-parameterization of ReLU net-
works, with which we replace each weight in the original neural network, with
a new value calculated from one or several paths, while keeping the outputs of
the network unchanged for any input. Then we show that BN in the path space,
namely P-BN, is just a slightly modified conventional BN on the re-parameterized
ReLU networks. Finally, our experiments on two benchmark datasets, CIFAR and
ImageNet, show that the proposed P-BN can significantly outperform the conven-
tional BN in the weight space.
1	Introduction
In recent years, neural networks with rectified linear unit (ReLU) activation functions (abbrev. ReLU
networks) , have been successfully applied to many domains, such as image classification (He et al.,
2016), game playing (Mnih et al., 2015), and text processing (Kim, 2014). Recently, it has been
noticed that the feedforward neural networks with ReLU, namely ReLU networks, are positively
scale-invariant (PSI) (Neyshabur et al., 2015a; 2016), which means when the incoming and out-
going weights of a hidden node, are respectively multiplied and divided by a positive constant, the
outputs of ReLU networks will keep unchanged for any input. However, the vector space, composed
of weights, in which the conventional optimization algorithms, e.g., Stochastic Gradient Descent (S-
GD), optimize the neural networks, is not PSI, and such mismatch may lead to undesirable behaviors
in the optimization process (Neyshabur et al., 2015a).
To tackle such challenge, some recent studies have shown that, one ReLU network can be opti-
mized in a completely new PSI parameter space, i.e., the path space, instead of its original weight
space (Meng et al., 2019). Detailedly, while regarding ReLU networks as directed acyclic graphs
(DAGs), we can calculate the outputs of ReLU networks by using path-values, i.e., the multiplication
of weights along each input-output path in the DAG. Here, path-values are invariant to the positive
rescaling of weights, which exactly matches the PSI property of ReLU networks, and the optimiza-
tion algorithms (e.g., SGD) in the path space (i.e., the vector space composed of path-values), are
shown to be superior to those in the weight space (Neyshabur et al., 2015a; 2016; Meng et al., 2019).
There are already some optimization algorithms developed in the path space, however, to the best
of our knowledge, it is still unknown whether other deep learning techniques beyond SGD, can also
have their counterparts in the path space. For example, BN is one of the most widely used normal-
1
Under review as a conference paper at ICLR 2020
ization approaches (Ioffe & Szegedy, 2015; Santurkar et al., 2018), and is crucial for facilitating the
training process of neural networks. With the help of BN, the training process of neural networks
can be accelerated, by both decoupling the scale and direction of weight vectors (Arora et al., 2018)
and smoothing the optimization landscape (Santurkar et al., 2018).
In this paper, we conduct a formal study on the de-
sign of BN in the path space. First, it is known that,
BN is successfully utilized during the forward pro-
cess in the weight space, but how to ensure the for-
ward propagation still remains unclear in the path
space, i.e., the way to calculate the outputs of hid-
den nodes via path-values layer by layer is unknown.
For example, in Fig. 1, we show a network and its
outputs calculated by both weights and path-values.
Here, the outputs of this network can be simply cal-
culated by using path-values, because a path starts
from an input node and ends up with an output n-
ode, and the path-value is just the multiplication of
weights along this path. Nevertheless, the outputs
of hidden nodes cannot be calculated by path-values
in this obvious way. Thus, to solve this problem, we
propose a re-parameterization for the weights in Re-
LU networks, which can replace each weight in the
[Calculate With Weights:)	Jτ√Outputsfidden:NodesL
fι= ReL=(XlWI +Λ^W3)w5 + ReLU(XI w? + ʌ^w/)w,
=(α⅛W1W5 +。2卬2卬7)化 1 + (。1卬3卬5 +。2卬4卬7)化 2
% % % 以
ICalcUlate With path-valueS：
fl= (。1 . 1 +。2 . 2)化 1 + (。1 . 3 +。2 .4)龙 2
Figure 1: An example of an MLP network.
a1 and a2 are 1 when corresponding ReLU is
activated, otherwise they are 0.
original network, with a new value calculated by one or several path-values, and the outputs of the
re-parameterized network will keep unchanged for any input. Therefore, the forward propagation in
the path space can be easily conducted after re-parameterizing the network.
Then, we show that BN in the path space is just a slightly modified conventional BN on the re-
parameterized network. Considering that BN in the weight space, can make the gradient propagation
more effective, we study BN in the path space from the perspective of effective propagation. In
details, we apply the conventional BN to the re-parameterized network, and find that the radients will
explode during the backward propagation in the path space. Thus, we propose a novel BN in the path
space, namely P -BN, which slightly modifies the conventional BN, via excluding the term whose
coefficient is fixed by re-parameterization from normalization. Particularly, we theoretically prove
and experimentally show that compared with the conventional BN, P-BN can propagate gradients
in the path space more effectively.
Finally, we conduct extensive experiments on multiple datasets, including CIFAR, ImageNet, with
multiple network structures, including ResNet and PlainNet. The results demonstrate that the P-BN
based optimization algorithms can significantly outperform the ones with the conventional BN.
2	Background
2.1	Related Work
In recent years, some researchers have conducted many theoretical studies on the path of ReLU net-
works. On the one hand, in terms of optimization, for example, Neyshabur et al. (2015a) proposed
a new regularization term, i.e., path-norm, and the Path-SGD algorithm for minimizing the regular-
ized loss. Meng et al. (2019) proposed G-SGD algorithm to directly optimize the model in the path
space, by utilizing the gradients w.r.t. path-values. On the other hand, in terms of generalization,
for example, Neyshabur et al. (2015b) and Zheng et al. (2018) gave the relationship between gen-
eralization error and path-norm or basis path-norm, respectively. Besides, E et al. (2018) leveraged
the path representation of ReLU networks, to analyze the generalization error of them.
However, the aforementioned studies have not involved normalization approaches (Ba et al., 2016;
Wu & He, 2018), such as BN (Ioffe & Szegedy, 2015), which are proved to be much crucial for
training neural networks. In particular, as for ReLU networks, BN is especially widely used in
computer vision domain (He et al., 2016; Li et al., 2016; Huang et al., 2017). Furthermore, there are
also many theoretical studies on BN (Santurkar et al., 2018; Arora et al., 2018; Kohler et al., 2019).
2
Under review as a conference paper at ICLR 2020
Accordingly, based on the studies on optimizing ReLU networks in the path space, and the crucial
normalization technique, i.e., BN, in this paper, we will give a formal study on designing BN in the
path space. We will next briefly introduce the optimization of ReLU networks without BN in the
path space.
2.2	Optimizing ReLU networks in the Path Space
In this subsection, we introduce the feedforward ReLU network and
the optimization algorithm in the path space.
Firstly, as shown in Fig. 2, consider an L-layer feedforward ReLU
network without BN1 whose weight matrices are {wl; l = 1, ∙∙∙ ,L},
We use OIj,l = 0,…，L to denote the j-th node in the l-th layer.
Based on these above, we further use wjlk to represent the weight on
the edge which connects Okj-1 and Ojj. As for the outputs of hidden
nodes, suppose that given a d-dimensional input X = (χι, ∙∙∙ ,xd),
then the output of Ojj and the hidden vector in the l-th layer, can
be denoted as ojj (x) and oj(x), respectively. Thus, the outputs can
be propagated as oj(χ) = g(wjojT(X)), where g(∙) = maχ(∙, 0) is a
ReLU activation function.
Input	Hidden	Output
Figure 2: An example of a
feedforward ReLU network.
Secondly, we will show the concept of a path in ReLU networks, and briefly introduce the optimiza-
tion algorithm in the path space. In details, regarding the network structure as a DAG consisting of
nodes and edges, a path can be defined as: a list of nodes or edges, starting from an input node, suc-
cessively passing by several hidden nodes along the edges, and finally ending up with an output node.
For example, in Fig. 2, path p starts from O20, passes O21 and O12, and ends up with O33. And then, the
path-value of path p can be defined as a multiplication of the weights along p, i.e., vp = w222w122w331.
Therefore, output o1L (x) can be calculated by using path-values as o1L (x) = Pk=ι Pτ)∈pk Vp ∙ ap ∙ Xk.
Here, P is a set which contains the paths staring from Ok0 and ending up with OjL , and ap, the ReLU
activation status of path p, can be calculated as ap
that O1jpp is passed by p .
QOlp∈pI(oj1pp(X) > 0), where O1jpp
∈ p represents
As for the optimization algorithm, we introduce two typical ones, i.e., Path-SGD (Neyshabur et al.,
2015a) and G-SGD (Meng et al., 2019). Path-SGD is to add a new regularizer, i.e., path-norm, into
the loss function, and then heuristically utilizes a coordinate-wised solution for minimizing the path-
norm regularized loss. It is shown to be superior to SGD for MLP, but with huge computational costs
when applied to Convolutional Neural Networks (CNNs), due to the complicate correlations among
path-values. Besides, Path-SGD only utilizes the PSI property to propose the regularizer, but cannot
optimize the network in the path space. Comparatively, G-SGD can solve the computation problem,
by decoupling correlations between path-values. Besides, G-SGD can optimize the network via
updating path-values by using gradients w.r.t. them instead of weights. Thus, G-SGD can serve as
the SGD in the path space, and the detailed on G-SGD are listed in Appendix C.
3	To Design BN in the Path Space
In this section, we first introduce a re-parameterization process, to ensure the forward propagation in
the path space, and calculate the outputs of hidden nodes by using path-values. Then, we will show
that the conventional BN cannot effectively propagate the error signals in the path space, which calls
us to design an effective BN in the path space.
3.1	Re-parameterization
In this subsection, we introduce a re-parameterization for the parameters of ReLU networks, which
can replace each weight with a path-value or a ratio of path-values, and the outputs of the re-
1For ease of presentation, we only discuss the MLP with equal width, and in this paper, the analyses and
algorithms can be applied to the MLP and CNN with unequal width (cf. Appendix D.1).
3
Under review as a conference paper at ICLR 2020
Output 0r(xi)
02(xi)
国
W31
②
w121
W32
W22
w212
w11
WILI
W22
w⅛
w2 r w121W131
0
1
3
12
w22w32
w22w22
W1>W11W31
w11w11w31∕∖
JWIW22032
ɪi	6
Step 3.
—1
W21,2,W 22W32	1	%t1
υ22
Step 4.
W31
W31
遑
W32
,
喧片呢一片
-- --
322一 311吟福
二 L - 2
W1 * * *ιW2ιW3ι = V11,
W1ιW22W∣2 = V11,
W12W21W31 =评2,
w22w22w32 = r22
ReSc Rescaling Operation



I I Weight Layer Index	Node Layer Index
Related Rescaling Scalar	Related Rescaling Hidden Node
Figure 3: An example to demonstrate the re-parameterization process for an MLP.
parameterized network will keep unchanged for any input. First of all, we present the process of
re-parameterization in Theorem 3.1.
Theorem 3.1 Consider an L-layer ReLU network with weight matrices {wl ； l = 1, ∙ ∙ ∙ ,L}, if the
diagonal elements in matrix wl , l > 2 are positive, the hidden outputs of the ReLU network can be
calculated by using path-values as follows:
Oj (x) = g (X Vjk ∙ Xk) for l = 1,	⑴
oj (X)= g(ojT(X) + X j ∙ ok-1 (X)) for l ≥ 2.	Q)
k=1,k6=j kk
Here, g(∙) denotes the ReLU activation function, and Vkk is the path-value of the path which only
contains the k-th diagonal elements in the weight matrices, i.e., vk1k = QlL=1 wkl k, and vjlk, k 6= j is
the path-value of the path which contains one non-diagonal weight, i.e., Vjk = (Qs<l Wsk) ∙ Wjk ∙
(Qs>lwjlj).
Proof: Based on the PSI property, we will prove Theorem 3.1 by designing a re-parameterization
process which consists several weight rescaling steps. After these steps, the ReLU network can be
equivalently re-parameterized, which means that the outputs will keep unchanged for any input after
re-parameterization. Here, we will next demonstrate the re-parameterization process by showing
that it can be conducted for a 3-layer MLP, as well as generalized to a multi-layer setting.
As for a 3-layer MLP, the process is shown in Fig. 3, and described in the following two steps:
(1) The weights connected with nodes Oj2 , j = 1, 2 are rescaled, i.e., the incoming and outgoing
weights of node Oj2 are multiplied and divided by wj3j, respectively. To be specific, w131 , w231 and
w121 , w122 are divided and multiplied by w131 , and similarly, w232 , w132 and w222 , w221 are also rescaled
3
by using w32. According to the definition of a path value, We have 谭=
w3i w2i WII = v3i w32
W31 w21 WII	v11，w32
32i
w32 w22 w12
-3	2	1-
W232W222W212
v3
V12, and hence, each weight in the 3rd layer can be replaced by a constant or a ratio of
path-values. (2) The Weights connected With nodes Oj1 , j = 1, 2 are rescaled, i.e., the incoming and
outgoing weights of node Oj1 are multiplied and divided by the corresponding outgoing diagonal
weight (i.e., wj2j wj3j). Afterwards, each weight in the middle layer can be replaced by a constant or
a ratio of path-values, and each weight in the 1st layer can be replaced by a path-value. Therefore,
each weight in this network are replaced by a path-value or a ratio of path-values.
As for a multi-layer setting, the re-parameterization process is similar to that of a 3-layer MLP. In
particular, this process can be proceeded by rescaling weights orderly, from which is connected with
the nodes in the last hidden node layer to the first hidden node layer, and the scalar of the rescaling
operation at each node, is the corresponding outgoing diagonal weight. After such rescaling steps,
the weights can be re-parameterized as follows: (1) Wkk → Vkk; (2) Wkk → 1; (3) Wjk → Vjik, l =
vkk
1, j 6= k .
4
Under review as a conference paper at ICLR 2020
In summary, a network with weight at each edge (e.g., Step 1 in Fig. 3), can be equivalently re-
parameterized into another network with a path-value or a ratio of path-values at each edge (e.g.,
Step 4 in Fig. 3), by rescaling its weights layer by layer. After the re-parameterization, the outputs
of hidden nodes can be calculated by using path-values, in the same way as they are calculated by
weights, and hence, Theorem 3.1 is established.
Remark: The positive constraint is not essential for proving Theorem 3.1, because when the di-
agonal weights are negative, the re-parameterization can still be conducted correctly by using the
absolute value of the corresponding weight as the rescaling scalar. Considering the robustness and
computational efficiency, this constrain exists in the optimization algorithm (Meng et al., 2019), and
hence, we follow it. Besides, this constrain will not bring much influence on the model expressive-
ness, because the number of the constrained weights is tiny, compared with the total weights. More-
over, we discuss some other choices on the re-parameterization and the influence in Appendix D.
3.2	Analyzing the BN in the Path Space
With the re-parameterization in Section 3.1, the forward propagation can be ensured in the path
space, and the outputs of hidden nodes can be calculated by using path-values. Hence, in this
subsection, we will design BN in the path space, via applying and analyzing the conventional BN to
the re-parameterized network.
Firstly, We briefly introduce the conventional BN. Given a mini-batch of inputs {χi; i = 1,…，m},
BN can normalize each output of the hidden node as BN(oj (Xi)) = γj∙ oj(x )-μj + βj, where μj =
__________________________________________________ σj
ml Pm=I oj(Xi) and σj = VZ：! Pm=I(Oj(Xi) - μj)2 + e represent the mean and standard deviation
of the minibatch outputs, and γjl , βjl are the scale and shift term 2 * which are independent with w,
respectively. Then, we can denote the output after BN and ReLU asZj (Xi)= g (BN(oj (Xi))).
Secondly, we analyze the gradient propagation in the re-parameterized network with the the conven-
tional BN. Here, we use L and Vzι(Xi)L = QjLi)，•…，∂JLxi)), to denote the cross-entropy loss
and the gradient vector w.r.t hidden output in layer l, respectively. Then, we can give an estimation
of the norm of Nzl(Xi)L in Theorem 3.2.
Theorem 3.2 We use Wl0 to denote the parameter matrix in layer l after the re-parameterization.
Suppose zjl (Xi) < O(1), ∀i, j, l, and m > 64. The norm of the gradient w.r.t zl (xi) can be upper
bounded as
kJi(χi)Lk ≤θ( Y kDs(Xi) ∙ Ys ∙ (σs)-1 ∙ Wsk),
s=l+1
where σl = diag(σ∖,…，σd), YI = diag(γ1,…，Yd), and Dl(Xi) = diag(Dl (Xi),…，Dd(Xi)). Here,
diag(aι, ∙∙∙ ,ad) represents a diagonal matrix whose diagonal elements are aɪ, ∙∙∙ , ad, and Dj (Xi)=
1 if zjl (Xi) > 0, otherwise Djl (Xi) = 0.
The proof of Theorem 3.2 can be found in Appendix B.
Discussion: According to Theorem 3.2, the gradient norm exponentially depends on depth L. If
the spectral norm of matrix DS(Xi) ∙ YS ∙ (σs)-1 ∙ Ws roughly equals to 1, the error signals can be
effectively propagated in the backward process (i.e., not vanished or exploded). On the one hand, if
there is non-zero elements in Ds (Xi), its spectral norm will equal to 1. On the other hand, in practice,
Yjs is initialized to be 1, and will keep its value around 1 during the training process, so the spectral
norm ofYs will be around 1. Hence, the magnitudes of Ws0 and σs-1 is crucial for the gradient norm.
For matrix Ws0, according to Theorem 3.1, the diagonal elements of Ws0(s ≥ 2) are degenerated
to be constant 1, and these constant terms will not be trained. Besides, the trained parameters are
initialized to be much smaller than 1 (cf. Appendix D.3), so matrix Ws0 will approach to an identity
matrix. For matrix σs-1, if each σjs is smaller than 1, it can cause gradient exploding during the back
propagation process. We will conduct observational experiments in Section 5.2, and it shows that
kσs k is less than 1 and gradients becomes larger during the back propagation process.
2In the following analysis, referring to Santurkar et al. (2018), we assume that both of the scale and shift
term are constant.
5
Under review as a conference paper at ICLR 2020
Therefore, such discussion result motivates us to design BN in the path space by modifying the
conventional BN, in order to propagate the error signals effectively. We will next detailedly introduce
the proposed BN in the path space, i.e., P-BN.
4	P-BN: EFFECTIVE BN IN THE PATH SPACE
In this section, we design the BN in the path space, namely P -BN, via slightly modifying the forward
process of the conventional BN. Motivated by the calculation for the outputs of hidden node in
Theorem 3.1, as well as the discussion of Theorem 3.2, we propose to normalize the terms related
to the trained coefficients (i.e., Eq. 1 and the 2nd term in Eq. 2), and exclude the term related to the
constant coefficient (i.e.,the 1st term in Eq. 2). Specifically, We use Zj (Xi) to denote the output after
P-BN and ReLU, and detailedly describe the forward process of P-BN in the following two steps.
l . l	l	l
(1)	For the 1st layer, the operation of P-BN is the same as the conventional BN, i.e., Zj (χi) = Zj (χi).
(2)	For the l-th layer (l ≥ 2), as shoWn in Fig. 4, there are three sub-steps for P-BN as folloWs.
First, calculate the partial Weighted summation. Here, the in-
put related to the diagonal constant element in the parameter
matrix is excluded, i.e.,
d
oj,∕(χi)= X j ∙ ZkT(X)	(3)
k=1,k6=j kk
Second, normalize the partial Weighted summation above, i.e.,
P-BN
BN
Figure 4: An example of the 2nd
step in P -BN.
Third, add the excluded term zj-1(χi) into the normalized par-
tial Weighted summation in Eq. 4, i.e.,
Zj(xi) = g(BN(oj,/(xi)) + ZjT(Xi))	(5)
Consequently, P-BN can be easily combined With the opti-
mization algorithm in the path space for ReLU netWorks, and We Will shoW it in Algorithm 1 in
Appendix E.
Then, We Will shoW that the gradients can be effectively propagated by using P-BN in Theorem 4.1.
Theorem 4.1 Suppose Zj (xi) < O(1), ∀i,j,l, and m > 64, the norm of gradient w.r.t Zl(Xi) Can be
upper bounded as
kJ(Xi)Lk ≤ O ( YY kDs(xi) ∙ (I + γs,/ ∙(σs,∕)-1∙ WS )k),
s=+1
where	Ws	=	W0	—	I,	σj,/	=	diag(σlJ,…，σ)"),	Ylj	= diag(γ1,/,…，γd,/) and D	(xi)=
diag(DI(Xi),…，Dd(xi)) where Dj((xi) = 1 if Zj (xi) > 0, otherwise Dj (xi) = 0.
The proof of Theorem 4.1 can be found in Appendix B. Then, We give Corollary 4.2, via summariz-
ing from Theorem 3.2 and Theorem 4.1.
Corollary 4.2 Using the notations in Theorem 3.2 and Theorem 4.1 and initializing γl and γl,/ to
be identity matrix, we have the following upper bound for gradient norm at initialization point
Mi(Xi)Lk ≤θ( YY kDs(Xi) ∙ (σs)-1 ∙ (I + WS)kj ,
s=l+1
INzi(Xi)Lk ≤ O ( YY kDs(Xi) ∙(I +(σs,∕)-1∙ WS) k).
s=l+1
6
Under review as a conference paper at ICLR 2020
Dataset	Method	PlainNet		ResNet		
		18	34	18	34	50
	SGD+BN	6.93% (±0.12)	7.76% (±0.22)	6.76% (±0.10)	6.40% (±0.09)	6.55% (±0.24)
	G-SGD+BN	6.66% (±0.12)	6.74% (±0.13)	6.84% (±0.30)	6.54% (±0.06)	6.62% (±0.19)
CIFAR-10	G-SGD+BN(wnorm)	6.26% (±0.17)	6.67% (±0.26)	6.31% (±0.14)	6.33% (±0.15)	6.31% (±0.14)
	G-SGD+P-BNours	5.99% (±0.13)	6.04% (±0.16)	6.04% (±0.14)	5.66% (±0.10)	5.99% (±0.12)
	SGD+BN	28.10% (±0.19)	33.37% (±0.41)	26.97% (±0.12)	26.42% (±0.23)	25.62% (±0.18)
	G-SGD+BN	27.08% (±0.35)	28.19% (±0.35)	27.13% (±0.39)	26.61% (±0.10)	25.99% (±0.40)
CIFAR-100	G-SGD+BN(wnorm)	26.76% (±0.05)	27.61% (±0.24)	26.60% (±0.17)	26.72% (±0.27)	25.65% (±0.22)
	G-SGD+P-BNours	26.70% (±0.10)	27.04% (±0.12)	26.39% (±0.10)	26.24% (±0.29)	25.29% (±0.19)
Table 1: Test error rate on CIFAR-10 and CIFAR-100.
Epoch
Figure 5: Curves for training ImageNet.
Figure 6: Experimental observations.
Suppose λ = ∣∣IW0k. A sufficient condition for that kSι(Xi)Lk and ∣∣Vzi(Xi)Lk Can be upper
bounded by constant (i.e., will not diverge as L → ∞) is σj ∈ [ LL+1λ), LL+J ] and σj ∈ [λL, ∞)
∀j, respectively. 3
s,/
Discussion: We can conclude from Corollary 4.2 that the range for σj , is much larger than that
for σj. Furthermore, if λ ≤ L, we have [ LLJJ), LL+λ) ] ⊂ [λL, ∞) (i.e., the range for σj is fully
s,/
contained in the range σj , ), which indicates the backward stability of P -BN. We will observe the
variance in each layer in Section 5.2, and the results demonstrate that the variance becomes larger
with P-BN. Thus, to some extent, the gradient propagated more stable for NN with P-BN.
5	Experiments
In this section, we conduct experiments by first comparing the performance of P-BN with the base-
lines on various datasets with some network structures, and then showing some experimental obser-
vations to support theoretical analyses.
5.1	Performance Experiments
In this subsection, we evaluate the performance of P-BN on training deep neural networks by con-
ducting experiments on three datasets, CIFAR-10, CIFAR-100, and ImageNet. We will next intro-
duce the network structures and compared methods first.
5.1.1	Network Structures and Compared Methods
In this subsection, we describe the network structures and compared methods. First, as for the
network structures, we apply P-BN to train ResNet and PlainNet (He et al., 2016). Second, we
show the setting details of four compared methods as follows.
(1)	SGD+BN: We use SGD to train the network with the conventional BN.
(2)	G-SGD+BN: We use G-SGD to train the network with the conventional BN.
(3)	G-SGD+BN(wnorm): We use a method which was intuitively proposed to handle the conven-
tional BN without any theoretical analysis (Meng et al., 2019). Here, the gradient of the path-value,
3Please note that the conditions are tight in the sense that if we change L in the ranges to be L1-, the upper
bound of both the gradient norm for BN and P-BN will diverge.
7
Under review as a conference paper at ICLR 2020
is normalized by the L2-norm of the weights pointing to the same hidden unit.
(4)	G-SGD+P-BN: Our proposed P-BN targets on BN in the path space, and the network with
P-BN should be optimized by the algorithms in the path space. Thus, we use G-SGD to train the
network with P-BN.
5.1.2	CIFAR
We conduct experiments on CIFAR-10 dataset and
CIFAR-100 dataset (Krizhevsky et al., 2009). Here,
we train ResNet of 18, 34, and 50 layers, and
PlainNet of 18 and 34 layers, respectively. As
for the hyper-parameters of methods mentioned
in their corresponding papers, i.e., SGD+BN (He
et al., 2016) and G-SGD+BN(wnorm) (Meng et al.,
2019), we use the same settings as their original
ones. Besides, we tune the hyper-parameters for
other methods, i.e., G-SGD+BN, and the proposed
G-SGD+P-BN. More details about the experimen-
tal setting can refer to Appendix F.1. In Table 1, we
show the performance results on the test error rate, and in Fig. 8, we plot the training curve and test
accuracy of 50-layer ResNet. Such results demonstrate that: (1) G-SGD+P-BN outperforms others
on all tested datasets and network structures, which shows the superiority of P-BN; (2) Combining
the path space and the conventional BN directly hurts performance, which empirically motivates us
to propose P -BN; (3) G-SGD+P-BN outperforms G-SGD+BN(wnorm), which shows the benefit
and significance of our proposed formal analyses.
5.1.3	ImageNet
We conduct experiments on ImageNet dataset (Russakovsky et al., 2015). Here, we train ResNet of
50, 101 and 152 layers, to demonstrate that the proposed P-BN can help to train very deep networks
on a large dataset, and we compare the performance results between SGD+BN and G-SGD+P-BN.
The method for tuning the hyper-parameters is similar to that in Section 5.1.2, and more details can
refer to Appendix F.1. In Table 2, we list top-1 and top-5 test error rates, and in Fig. 5, plot the
training curve and test accuracy during training ResNet of 50, 101, and 152 layers. These results
well demonstrate the superiority of G-SGD+P-BN, and show that for the large dataset and very deep
networks, P-BN can still outperform the conventional BN.
5.2	Observations on S tandard Deviations and Gradients
After analyzing the advantage of P-BN over the conventional BN theoretically in Section 3.2 and 4,
we now provide some experimental observations to support our analyses. Here, we will show the
magnitude of standard deviations in BN, and the gradients of ReLU networks with BN and P-BN.
In this experiments, we use a stacked CNN of 50 layers with 128 channels in each layer, and use
CIFAR-10 as training data, with the batch size of 128. Then, we add the conventional BN or P-BN
at the end of each hidden layer in the stacked CNN, which is denoted as BN and P-BN, respective-
ly. For the model with the conventional BN, it is randomly initialized. For the model with P-BN,
the diagonal elements in parameter matrices (expect for the 1st layer) are initialized to 1 (cf. Sec-
tion 3.1), and other elements are initialized with small random values (cf. Appendix D.3). Because
the theoretical analyses are not related to the training process, we will just observe some quantities
at initialization as follows.
First, we show some observations on standard deviation σjl in Theorem 3.2. Here, we log σjl for
each hidden node during the forward process of the 1st mini-batch, and then calculate the average
standard deviation among the nodes in each layer. In Fig. 6, we plot the curves regarding standard
deviations, and we can observe that: (1) For the conventional BN, the standard deviations are smaller
than 1, which matches the claim in Theorem 3.2, and is one of the reasons of the gradient exploding.
(2) For P-BN, the standard deviation grows when the layer index becomes larger, and most of them
are larger than 1. Thus, in Theorem 4.1, ∣∣σ-1 ∙ Wsk are smaller than ∣∣I∣∣, and ∣∣VzιLk can be upper
bounded, which is one of the reasons for that the gradient exploding can be weaken by P-BN.
Metric	ResNet	Method	
		SGD+BN	G-SGD+P-BN
Top 1 Test Error Rate	-50-	27.34% (±0.10)	25.92% (±0.18)
	101	25.94% (±0.09)	24.36% (±0.10)
	152	25.65% (±0.15)	23.91% (±0.23)
Top 5 Test Error Rate	50	9.10% (±0.06)	8.26% (±0.16)
	101	8.33% (±0.07)	7.43% (±0.05)
	152	8.31% (±0.09)	7.25% (±0.09)
Table 2: Test error rate on ImageNet.
8
Under review as a conference paper at ICLR 2020
Second, we show some observations on gradients, to investigate that whether the gradient exploding
can be weaken by P-BN in practice. Here, we log the L2-norm of gradients of loss w.r.t. the outputs
in each layer, and plot the log of the gradient norm in Fig. 6. We can observe that, the gradients
indeed explode in the ReLU network with both the conventional BN and P-BN, and the exploding
speed is much slower with P-BN, which demonstrates that P-BN can help to weaken the gradient
exploding.
6	Conclusion
In this paper, we conduct a formal study on the design of BN in the path space. First, considering
that BN is adopted during the forward process, we ensure the forward propagation in the path space,
via proposing a re-parameterization for weights in ReLU networks. Here, the re-parameterization
can equivalently replace each weight in the original network with a new value calculated by one or
several path-values, and hence, the outputs of hidden nodes can be calculated by using path-values.
Then, we design BN in the path space, by applying and analyzing the conventional BN to the re-
parameterized network. Here, we analyze the gradient propagation after BN is applied, and then
notice that the gradient will explode in such networks. Thus, we propose a novel BN in the path
space, i.e., P-BN, which excludes the term whose coefficient is fixed by re-parameterization from
normalization. Finally, we conduct experiments to verify the proposed method, and results on both
CIFAR and ImageNet dataset, demonstrate that P-BN can enhance the performance significantly.
References
Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batch
normalization. arXiv preprint arXiv:1812.03981, 2018.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Weinan E, Chao Ma, and Lei Wu. A priori estimates for two-layer neural networks. arXiv preprint
arXiv:1810.06397, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, pp. 770-778, 2016.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In CVPR, volume 1, pp. 3, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In ICML, pp. 448U56, 2015.
Yoon Kim. Convolutional neural networks for sentence classification. arXiv preprint arX-
iv:1408.5882, 2014.
Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Thomas Hofmann, Ming Zhou, and Klaus
Neymeyr. Exponential convergence rates for batch normalization: The power of length-direction
decoupling in non-convex optimization. In AISTATS, pp. 806-815, 20l9.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normaliza-
tion for practical domain adaptation. arXiv preprint arXiv:1603.04779, 2016.
Qi Meng, Shuxin Zheng, Huishuai Zhang, Wei Chen, Zhi-Ming Ma, and Tie-Yan Liu. G-SGD:
Optimizing ReLU neural networks in its positively scale-invariant space. In ICLR, 2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
9
Under review as a conference paper at ICLR 2020
Behnam Neyshabur, Ruslan Salakhutdinov, and Nathan Srebro. Path-SGD: Path-normalized opti-
mization in deep neural networks. arxiv preprint arXiv:1506.02617, 2015a.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In COLT,pp.1376-1401, 2015b.
Behnam Neyshabur, Yuhuai Wu, Ruslan R Salakhutdinov, and Nati Srebro. Path-normalized opti-
mization of recurrent neural networks with ReLU activations. In NeurIPS, pp. 3477-3485, 2016.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
PyTorch. In NeurIPS-W.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visu-
al recognition challenge. IJCV, 115(3):211-252, 2015.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal-
ization help optimization? In NeurIPS, pp. 2483-2493, 2018.
Yuxin Wu and Kaiming He. Group normalization. In ECCV, pp. 3-19, 2018.
Shuxin Zheng, Qi Meng, Huishuai Zhang, Wei Chen, Nenghai Yu, and Tie-Yan Liu. Capacity
control of ReLU neural networks by basis-path norm. arXiv preprint arXiv:1809.07122, 2018.
10
Under review as a conference paper at ICLR 2020
A Notations
Notation	Object
X	d-dimensional input, i.e., X = (χι, ∙∙∙ ,xd)
m	Minibatch size
wl	Weight matrix in the l-th layer
olj(X)	Output of the j-th hidden node in the l-th layer
ol(X)	Hidden vector in the l-h layer
wjlk	Weight on the edge which connects the k-th node in the (l - 1)-th layer and the j-th node in the l-th layer
vk1k	Path-value of the path which only contains the k-th diagonal elements in weight matrices, i.e., vk1k = QlL=1 wkl k
vjl k , k 6= j	Path-value of the path which contains one non-diagonal weight, i.e., Vjk = (Qs<i wkk) ∙ Wjk ∙ (Qs>i Wjj)
zjl (X)	Output after BN and ReLU of the j-th hidden node in the l-th layer
Zj(X)	Output after P-BN and ReLU of the j-th hidden node in the l-th layer
Wl0	Parameter matrix in layer l after the re-parameterization.
μj	Mean of the minibatch outputs of the j-th hidden node in the l-th layer	
σj	Standard deviation of the minibatch outputs of the j-th hidden node in the l-th layer
Table 3: Main notations in this paper.
B S upplementary Proofs
Theorem B.1 We use Wl0 to denote the parameter matrix in layer l after the re-parameterization.
Suppose zjl (xi) < O(1), ∀i, j, l, and the batch size m > 64. The norm of gradient w.r.t zl(xi) can
be upper bounded as
Wzi(Xi)Lk ≤o( YY kDs(xi) ∙ Ys ∙ (σs)-1 ∙ Wsk),
s=l+1
where σl = diag(σ1,…，σd), YI = diag(γ1,…，γd) and Dl(Xi)= diag(D∖(xi),…，Dd(Xi)). Here,
diag(aι, ∙∙∙ ,ad) represents a diagonal matrix whose diagonal elements are aι,… ,a4, and Dj (Xi)=
1 if zjl (Xi) > 0, otherwise Djl (Xi) = 0.
Proof: We concat the gradient vector of different instances in a minibatch into one (d × m)-
dimensional vector as
VziL =(Si(χi)L,…Si(Xm)L),	(6)
and hence, according to chain rule, we have
∂zL ∂oL ∂ol+1
VzlL = VzL L∙ ∂ol ∙ ∂zl-i …B
(7)
Here, on the one hand, as for matrix ∂os, it donates
	「∂zs(x1)	∂z1s(x1)	∂z1s(X1)	
	∂os(x1)	-∙ - - . do2(XI)	∂od (Xm)	
∂zs	dz2 (x1)	dz2(XI)	.	dz2(XI)	
——= ∂os	∂of(x1)	do2(XI)	∙ • ∙ ∙	∙	∂osd (Xm ) •	.	(8)
	∂zS(χm)	∂zS(Xm).	∂zS(Xm)	
	, doS(XI)	do2(XI)	•	T7		T ∂osd (Xm )	
11
Under review as a conference paper at ICLR 2020
So we have
∂zjs (xi )	γjs	1	1
j) = j) ∙ σs(I- m - m .⑸(X)) )，ifj1=j2=j and i1 = i2 =i;
(9)
∂zjs(xi1)
∂os(xi)
Ds (xi1) ∙
1
——Zs(XiI) ∙ Zs(Xi2)),
m J	J
ifj1 = j2 = j and i1 6= i2 ;
(10)
∂zs1 (xi1)
∂oj (xi2)
ifj1 6= j2.
(11)
j
—
1
m
Obviously, most of the elements in the matrix equals to zero. Under the assumption that Zjs(xi) <
O(1) for all i,j, l and m > 64, the above matrix approaches to the diagonal matrix DS ∙ YS ∙ (σs)-1.
On the other hand, as for matrix d∂θ- 1, it donates
-WS	o …o -
∂os =	0	W0 …0
∂ZS — 1	∙ ∙ ∙
|_ 0	0	…Wl
(12)
Thus, using the fact that kAB k ≤ kAkkB k, we can get the result in the theorem.
Theorem B.2 Suppose Zj (Xi) < O(1), ∀i,j,l, and m > 64. The norm ofgradient w.r.t Zl(Xi) can be
upper bounded as
kJ(Xi)Lk ≤ O ( YY kDs(xi) ∙(I + γs,/ ∙ (σs,/)-1 ∙ WS )k),
s=l+1
where WS	=	Ws	—	I,	σl,/ =	diag(σlJ,…，。了),	YJ =	diag(γ?, ∙∙∙	, γ∣l,/)	and D	(Xi)
diag(DI(Xi)…，Dd(xi)) where Dj(Xi) = 1 if Zj(Xi) > 0, otherwise Dj(Xi) = 0.
Proof: According to the chain rule, we have
V=ι L = V=L L∙
Z	Z
∂Zl
∂oL,/
∂ ∂z+ ∂ol+1∕	∂Zl+1
∖dol+1,/	∂Zl 十 ∂Zl
(13)
∂	∂Zs 1	.
where k denotes
		-∂zS(x1)	∂z1(x1)
		∂o1,/(x1)	do2"(XI)
	∂Zs	dz2 (XI) do；" (x1)	dz2(XI) do2,/(XI) • ∙ ∙
	∂os,/ =		
		∂Zd (Xm)	∂Zd(Xm)
		-∂o1"(x1)	do2,(X1)
So we have			
∂zS(χ1)-
∂od"(xm)
dzs (XI)
∂od"(xm)
∂Zd(χm)
∂od,/(xm)_
(14)
—j-- =	Dj (Xi) ∙	Yjy (I - ------- ∙ (BN(os,/ (χi))2),	if j1 =	j2	= j and i1	=	i2	= i；
∂ojS,/ (xi)	j σjS,/	m m j
(15)
∂oj,∕(xi2)
DS(XiI) ∙ ⅛7(-ɪ - ɪ ∙ BN(oj,∕(xi1)) ∙ BNjxi))),
σS,/ m m j	j
σj
ifj1 = j2 = j and i1 6= i2 ;
azj1 (χi1)
∂oj,∕(xii)
0,
ifj1 6= j2.
(16)
(17)
12
Under review as a conference paper at ICLR 2020
Obviously, most of the elements in the matrix equals to zero. Under the assumption that Zj (Xi) is
bounded for all i,j, l, the above matrix tend to be the diagonal matrix Dl ∙ γl ∙ (σl )-1 if m approaches
∞.
s,/
On the one hand, as for matrix dos-ι, We have
∂zs
∂os,/
∂Zs-1 = L
(18)
On the other hand, as for matrix ?£-/, we have
∂z
	-I WS	o …o	
∂os,/ _	o W0 …o	
∂Zs-1 =	USs	U • ∙ ∙	(19)
	o	o …Ws	
Thus, using the fact that kAB k ≤ kAkkB k, We can get the result in the theorem.
C Details on Optimization Algorithms in the Path Space
We provide the detailed update rules on G-SGD (Meng et al., 2019) in this section. Here, we use wj
and wi to denote the diagonal weight and non-diagonal weight at layer 1, respectively, and use wj0
and wi0 to denote the diagonal weight and non-diagonal weight at layer l, l ≥ 2, respectively. The
gradient of w w.r.t loss l, can be represented by δw . Then we have
wj+1 = Wj - ηt ∙
δW j ∙ Wj - ∙ Ewj →wi0 δwt0 ∙ W
wjt
wjt+0 1
wjt0;
wit+1 = Wt - ηt ∙ δw。,
,t+1 = w--ηt∙ δw/ (Wj) 2
i0	I(Wj → Wi，) ∙ wj+1∕wj
(20)
(21)
where event Wj → Wi0, means Wj and Wi0 are contained in one of the paths, containing one non-
diagonal weights at most.
D Discussion about Re-parameterization
D.1 UNEQUAL WIDTH MLP AND CNN
In this section, we first show the method for finding weights
used for rescaling in Theorem 3.1, which is denoted as “red input Hidden Output
weights” here, for the network with unequal width of the input 、C C C
andoutputlayers.	i	⅛β<
As shown in Fig. 7, for weight matrix {wl； l = 2,…，L - 1},	∖
the diagonal elements are selected as red weights. For weight
matrix w1 and WL with size (D,hι) and (hL-ι,K), where	<√	<√
D and K are number of input and output nodes, respective-
ly, and hi is the number of nodes in the i-th hidden layer, Figure 7: An example of feedfor-
then, elements {w1(i mod D,i);i = 1,…，hι} and {wL(i,i WardReLUnetWork.
mod K); i = 1,…,hL-ι} are selected as red weights, respec-
tively.
Besides, all methods can be easily applied to CNN. As we all know, a filter in CNN transforms a
input feature map to the output feature map. Thus, each feature map can be regarded as a hidden
node in MLP, and then the filter connects two feature maps in CNN corresponds to the weight
connects two hidden nodes in MLP. Besides, the method to identify red weights is similar with that
for MLP unless the diagonal elements of the 4-triple of the weight matrices are selected. Suppose
the 4-triple of weight matrices is [64, 64, 3, 3] where the first two elements denote the number of
13
Under review as a conference paper at ICLR 2020
input channels and output channels, the last two elements denote the size of filter. The element
[0,0,1,1], [1,1,1,1], [2,2,1,1],…，[63, 63,1,1] are selected to be red weights. In this way, the
re-parameterization method in Theorem 3.1 is easy to extend to CNN by rescaling the weights using
the selected red weights.
D.2 Other Choices on Re-parameterization
In this section, we discuss other choices on re-parameterization and the influence. The re-
parameterization method using path-values showed in the proof of Theorem 3.1 is not unique. For
example, we can obtain another kind of re-parameterization, via multiplying and dividing the incom-
ing and outgoing weights of O11 by v111 in step 4 of Figure.3. This will not influence analyses much in
this paper for the following two reasons. First, whatever re-parameterization method used, each re-
parameterized network can serve as a sufficient representation for ReLU networks in the path space,
because the outputs of the network will keep unchanged for any input after re-parameterization. Sec-
ond, our studies are based on Theorem 3.1, which can be generalized to other re-parameterization
methods.
D.3 Relationship between Re-parameterization and Optimization Process
In this section, we show how the network should be optimized after it has been re-parameterized.
Specifically, after re-parameterization, we can optimize the loss function according to gradients
with respect to the path-values using the optimization algorithm such as SGD as follows. First, we
denote the parameter matrix of the re-parameterized network at the l-th layer as Wl0, whose elements
are constant, path-value, and ration of path-values. Next, we can use back propagation to obtain
the gradient of Wl0 . According to the chain rule, for the path-values that will not appear in the
denominator, its gradient can be calculated as:
Vwi L
VvlL = -ɪ,	(22)
jk	vk1k
and for the path-values that will appear in the denominator, its gradient can be calculated as:
Pj=1 k=1 Vwl L
Vvι L = Vwi L ——j~π——jk-
vkk	wkk	1
vkk
(23)
Then, we can derive the update rule of SGD in path space mentioned in Section C. Here, please note
that the update rules derived from re-parameterization is consistent with that provided in (Meng
et al., 2019), and re-parameterization provides a much simpler and straightforward way to explain
the complex update rules.
Based on the process above, we now provide some analyses on the magnitude of parameter matrices.
Specifically, for the 2nd term in Eq. 23, the numerator contains many terms, which tends to be larger
than the gradient of other paths, and thus, the denominator, i.e., v1k, (k = 1,…，d) is initialized to be
large in practice to match the magnitude of the numerator. Therefore, for the parameter matrix Wl0,
Vkk, (k = 1,…，d) appears in the denominator for most of its elements, and hence, these elements
are initialized to be much smaller than 1, which supports analyses in Theorem 3.2.
E FULL ALGORITHM FOR OPTIMIZING RELU NETWORKS WITH P-BN
In Algorithm 1, we show the full algorithm for optimizing ReLU networks with P-BN in the path
space. Besides, please check more details of P athOptimizer in Section 2.2 and Appendix C.
F Details on Experiments
F.1 Experimental Setting Details
In particular, following the settings in Meng et al. (2019), we apply G-SGD or P-BN with residual
blocks, because there is no positive scaling invariance across residual blocks. In addition, we intro-
duce a coefficient λ(λ < 1) for Zj-I(Xi) in Eq. 5 for experiments of ResNet, to prevent the output
exploding during the forward process, and λ for all layers in ResNet is set to be 0.1.
14
Under review as a conference paper at ICLR 2020
Algorithm 1 Optimization Algorithm in the Path Space of ReLU networks with P -BN.
Require: A mini-batch of inputs: B ={x1,…，xB }, initialization w(0), and P athOptimizer.
for t = 0,1,…，T do
Forward Process
1.	Normalize the output in the 1st layer in the same way as the conventional BN.
for l = 2,…，L do
2.	Normalize the partial weighted summation by using the conventional BN according to
Eq. 3.
3.	Calculate the mean and standard deviation of the partial weighted summation according
to Eq. 4.
4.	Add the excluded term into the normalized partial weighted summation according to Eq. 5.
end for
5.	Calculate the value of loss function l(w(t)) by using the final output.
Backward Process
6.	Calculate the gradient of w(t): Nwl(w(t)) J Back Propagation (l(w(t)); W⑴)
Update Rule of Path Optimizer
7.	w(t+1) = P athOptimizer(w(t), Nwl(w(t))).
end for
Ensure: w(T).
Here we describe pre-processing steps, which is same as He et al. (2016). For CIFAR-10 and CIFAR-
100, we randomly crop the input image by size of 32 with padding size of 4, and normalize every
pixel value to [0, 1]. Then the random horizontal flipping to the image is applied. For ImageNet, we
randomly crop the input image by size of 224, and normalize every pixel value to [0, 1]. Then the
random horizontal flipping to the image is also applied.
We use 1 NVIDIA Tesla P100 or P40 GPU to run the experiments on CIFAR, and use 4 NVIDIA
Tesla P100 or P40 GPUs in one machine to run the experiments on ImageNet. All experiments
are averaged over 5 runs with different random seeds, and PyTorch (Paszke et al.) is used for
implementation.
Here we list hyper-parameters used for CIFAR-10 and CIFAR-100. As for the hyper-parameters
of methods mentioned in their corresponding papers, i.e., SGD+BN (He et al., 2016) and (G-
SGD)+BN(wnorm) (Meng et al., 2019), we use the same settings as their original ones. Specifi-
cally, for SGD+BN, the initial learning rate is set to 0.1, and for G-SGD+BN (wnorm), the initial
learning rate is set to 1.0. Besides, we tune hyper-parameters for (G-SGD)+BN and the proposed
(G-SGD)+(P-BN). Specifically, the initial learning rate is searched from {0.1, 0.2, 0.5, 1.0}. Then,
we use the method proposed in (Zheng et al., 2018) as the weight decay in the path space, and set the
coefficient of the weight decay to 1 × 10-4, for these two methods. Besides, we use the SGD without
momentum in our experiments, because the way to utilize momentum in the path space remains
unclear now. Moreover, for all models and algorithms, the mini-batch size is set to be 128 and the
training process is conducted for 64k iterations. The learning rates are multiplied by 0.1 after 32k
and 48k iterations in all experiments, and the coefficient of weight decay for methods is set to be
0.0001.
Here we list hyper-parameters used for ImageNet. For all experiments here, the mini-batch size is
set to be 256 and the hyper-parameter for weight decay is set to be 0.0001 He et al. (2016). For
SGD+BN, the initial learning rate is set to 0.1 He et al. (2016). For G-SGD+P-BN, we only tune
the initial learning rate from {0.1, 0.2, 0.5}. The training process is conducted for 90 epochs, and
the learning rate is multiplied by 0.1 after 30 and 60 epochs.
As for the stacked CNN used for observations in Section 5.2, we use the following settings: kernel
size 3, stride 1, padding 1, and no bias. For CNN, the network width is the the number of channels,
which is set to 128 for all hidden layers.
F.2 Supplementary Results
In Fig. 8, plot the training curve and test accuracy of 50-layer ResNet. Here, we add other baseline,
i.e., SGD+P-BN, in which we use SGD to train the network with P -BN, and in Table 4, we show
15
Under review as a conference paper at ICLR 2020
Dataset Method
PlainNet
18	34
ResNet
18
34
50
CIFAR-10	SGD+P-BN	7.00% (±0.08)	7.68% (±0.21)	6.78% (±0.15)	6.57% (±0.06)	6.47% (±0.23)
CIFAR-100 SGD+P-BN	28.57% (±0.46)	32.98% (±0.81)	26.71% (±0.22)	26.80% (±0.51)	25.68% (±0.28)
Table 4: Test error rate on CIFAR-10 and CIFAR-100 for SGD+P-BN.
-SGD+BN
- SGD+P-BN
-G-SGD+BN
S-SGD+BN(wnomι)
——0∙SGD+P-BNours
10-3：
50	100	150 2QQ
Epoch
 
100	150
Epoch
50
(a) CIFAR-10.
50	100	150 2QQ	50	100	150
Epoch	Epoch
(b) CIFAR-100.
Figure 8: Training curves and test accuracy for training ResNet of 50 layers on CIFAR.
the test error for SGD+P -BN. We can observe that G-SGD+P-BN outperforms SGD+P-BN, which
demonstrate the benefits of optimizing in the path space.
F.3 Experiments for Path-SGD
As mentioned in Section 2.2, Path-SGD (Neyshabur et al., 2015a) is another PSI-related optimiza-
tion algorithm, but not the optimizer in the path space.
First, we introduce the network structure and some
basic settings. Here, because Path-SGD is compu-
tationally cost for CNN, we follow the experimen-
tal settings in (Neyshabur et al., 2015a), and use an
MLP of 5 layers, whose number hidden nodes in
each layer is 4000, and the conventional BN or P-
BN is added at the end of every hidden layer. Be-
sides, we use CIFAR-10 as the training data, and the
batch size is set to 64. Moreover, all experiments
are run for 150 epochs, and results are obtained by
averaging over 5 runs. The only hyper-parameter
needs to be tuned is learning rate, which is searched
from {1, 5}-{1,2,3}.
Method	Validation Error
SGD+BN	35.30% (±0.54)
Path-SGD	41.40% (±0.25)
Path-SGD+BN	35.00% (±0.71)
Path-SGD+P-BN	34.45% (±0.29)
Table 5: Validation error for Path-SGD exper-
iments.
Then, we show the setting details of 4 compared methods as follow:
•	SGD+BN: We use SGD to train the network with the conventional BN.
•	Path-SGD: We use Path-SGD to train the network without BN. 4
•	Path-SGD+BN: We use Path-SGD to train the network with the conventional BN.
•	Path-SGD+P-BN: We use Path-SGD to train the network with P-BN.
In Table 5, we list the validation error of these 4 methods. Results show that: (1) Path-SGD+P -BN
outperforms other baselines, which well demonstrate the superiority of P-BN. (2) Path-SGD+P -BN
and Path-SGD+BN perform better than SGD+BN, which show the benefits of optimizing in the path
space.
4We do not apply this setting for ResNet and PlainNet, because the performance will be badly hurt, and
training will fail when BN is removed.
16
Under review as a conference paper at ICLR 2020
F.4 Analysis of Computational Costs
Different from the conventional BN, P-BN is to identify the diagonal elements of weight matrices.
For convolutional networks, the outputs of hidden nodes in MLP always correspond to the feature
maps. Hence, on the one hand, if the sizes of feature maps in different layers are the same, and the
stride length is 1, we can then set the diagonal elements in weight matrices to 0 and add skip connec-
tions for all layers (except the 1st layer), which will not bring extra computational cost. On the other
hand, if the stride length is larger than 1, we can also set the diagonal elements in weight matrices
to 0, and additionally, use a fixed-weight convolutional layer, with only diagonal elements fixed to
1 and others fixed to 0. Thus, with the skip connection or the fixed-weight convolutional layer, the
back propagation process can be easily implemented. Therefore, according to the above implemen-
tation, P-BN will not bring much extra computational cost compared with the conventional BN.
Statistically, in our experiments, the ratio of run time between G-SGD+Path-BN and SGD+BN, is
less than 4 : 3, and the extra computational cost is mainly brought by the update of G-SGD instead
of P-BN.
17