Under review as a conference paper at ICLR 2020
Visual Explanation for Deep Metric Learning
Anonymous authors
Paper under double-blind review
Ab stract
This work explores the visual explanation for deep metric learning and its appli-
cations. As an important problem for learning representation, metric learning has
attracted much attention recently, while the interpretation of such model is not as
well studied as classification. To this end, we propose an intuitive idea to show
where contributes the most to the overall similarity of two input images by decom-
posing the final activation. Instead of only providing the overall activation map
of each image, we propose to generate point-to-point activation intensity between
two images so that the relationship between different regions is uncovered. We
show that the proposed framework can be directly deployed to a large range of
metric learning applications and provides valuable information for understanding
the model. Furthermore, our experiments show its effectiveness on two potential
applications, i.e. cross-view pattern discovery and interactive retrieval.
1	Introduction
Learning the similarity metrics between arbitrary images is a fundamental problem for a variety of
tasks, such as image retrieval (Oh Song et al. (2016)), verification (Schroff et al. (2015); Luo et al.
(2019)), localization (Hu et al. (2018)), video tracking (Bertinetto et al. (2016)), etc. Recently the
deep Siamese network (Chopra et al. (2005)) based framework has become a standard architecture
for metric learning and achieves exciting results on a wide range of applications (Wang et al. (2019)).
However, there are surprisingly few papers conducting visual analyses to explain why the learned
similarity of a given image pair is high or low. Specifically, which part contributes the most to the
similarity is a straightforward question and the answer can reveal important hidden information
about the model as well as the data.
Previous visual explanation works mainly focus on the interpretation of deep neural network for clas-
sification (Springenberg et al. (2014); Zhou et al. (2016; 2018); Fong & Vedaldi (2017)). Guided
back propagation (Guided BP) (Springenberg et al. (2014)) has been used for explanation by gen-
erating the gradient from prediction to input, which shows how much the output will change with a
little change in each dimension of the input. Another representative visual explanation, class activa-
tion map (CAM) Zhou et al. (2016), generates the heatmap of discriminative regions corresponding
to a specific class based on the linearity of global average pooling (GAP) and fully connected (FC)
layer. However, the original method only works on this specific architecture configuration and needs
retraining for visualizing other applications. Based on the gradient of the last convolutional layer
instead of the input, Grad-CAM (Selvaraju et al. (2017)) is proposed to generate activation maps
for all convolutional neural network (CNN) architectures. Besides, other existing methods explore
network ablation (Zhou et al. (2018)), the winner-take-all strategy (Zhang et al. (2018)), inversion
(Mahendran & Vedaldi (2015)), and perturbation (Fong & Vedaldi (2017)) for visual explanation.
Since the verification applications like person re-identification (re-ID) (Luo et al. (2019)) usually
train metric learning models along with classification, recent work (Yang et al. (2019)) starts to
leverage the classification activation map to help improve the overall performance, but the activation
map of metric learning is still not well explored. For two given images, a variant of Grad-CAM has
been used for visualization of image retrieval (Gordo & Larlus (2017)) by computing the gradient
from the cosine similarity of the embedding features to the last convolutional layers of both images.
However, Grad-CAM only provides the overall highlighted regions of two input images, the rela-
tionship between each activated region of two images is yet to be uncovered. Since the similarity
is calculated from two images and possibly based on several similar patterns between them, the
relationship between these patterns is critical for understanding the model.
1
Under review as a conference paper at ICLR 2020
In this paper, we propose an activation decom-
position framework for visual explanation of
deep metric learning and explore the relation-
ship between each activated region by point-to-
point activation response between two images.
As shown in Fig. 1, the overall activation map
of the proposed method is generated by decom-
posing the similarity along each image. In this
example, the query image (A) has a high acti-
vation on both the eyes and mouth areas, but
the overall map of the retrieved image (B) only
highlights the eyes. It is actually hard to under-
stand how the model works only based on the
overall maps. For image B, the mouth region
(green point) has a low activation which means
the activation between the mouth and the whole
image A is low compared to the overall activa-
tion (similarity). However, by further decom-
A
Overall	Partial
Similarity
Figure 1: An overview of activation decomposi-
tion. The overall activation map on each image
highlights the regions contributing the most to the
similarity. The partial activation map highlights
the regions in one image that have large activa-
tion responses on a specific position in the other
image, e.g. mouth or eye.
posing this activation (green point) along image A, that is to give the activation between the mouth
region of image B and each position in image A, the resulting partial (or point-specific) activation
map (green box) reveals that the mouth region of image B still has a high response on the mouth
region of image A. This partial activation map can be generated for each pixel, which renders the
point-to-point activation intensity representing the relationship between regions in both images, e.g.
eye-to-nose or mouth-to-mouth. The partial activation map provides much more refined information
about the model which is crucial for explanation. The contributions are summarized as follows.
•	We propose a novel explanation framework for deep metric learning architectures and it may serve
as an analysis tool for a host of applications, e.g. face recognition, person re-ID.
•	The proposed method uncovers the point-to-point activation intensity which is not explored by
existing methods. Our experiments further show the importance of the partial activation map on
several new applications, i.e. cross-view pattern discovery and interactive retrieval.
•	Our analysis suggests that two widely believed arguments (Section 2.1, 4) about CAM and Grad-
CAM are inaccurate.
2	Related Work
2.1	Interpretation for Classification
As a default setting, most existing interpretation methods (Zhou et al. (2016); Springenberg et al.
(2014); Zhang et al. (2018); Fong & Vedaldi (2017)) are designed in classification context where
the output score is generated from one input image. Among the approaches that do not require
architecture change, one intuitive idea (Zhou et al. (2016); Zhang et al. (2018)) is to look into the
model and see where contributes the most to the final prediction. CAM and its variant Grad-CAM
are among the most widely used approaches, but recent works simply consider CAM as a heuristic
linear combination of convolutional feature maps which is limited to the original architecture. It is
also believed (Zhou et al. (2016; 2018); Selvaraju et al. (2017)) that CAM only applies to the specific
architecture configuration of GAP and one FC layer (Argument 1). On the contrary, we argue that
CAM is only a special case of activation decomposition on the specific architecture, and the idea
applies to much more architectures, even beyond the classification problem. It is also believed that
Grad-CAM is a generalization of CAM for arbitrary CNN architecture (Argument 2), but the original
paper only provides the proof on CAM’s architecture. We show that Grad-CAM is not equivalent to
activation decomposition for some architectures (Section 4.3). Another direction of explanation is
to check the black box model by modifying the input and observing the response in the prediction.
A representative method Fong & Vedaldi (2017) aims to optimize the blurred region and see which
region has the strongest response on the output signal when it is blurred. Perturbation optimization
is applicable for any black box model, but the optimization can be computational expensive.
2
Under review as a conference paper at ICLR 2020
2.2	Interpretation for Metric Learning
Guided BP (Springenberg et al. (2014)) can be easily adopted to metric learning, but recent works
(Adebayo et al. (2018); Fong & Vedaldi (2017)) claim that gradient can be irrelevant to the model
and guided-BP fails on the sanity check (Adebayo et al. (2018)). Among methods that pass the san-
ity check, Grad-CAM has been used for visualization of image retrieval (Gordo & Larlus (2017)),
but no quantitative result is provided. Instead of adding more gradient heuristics like Grad-CAM++
(Chattopadhay et al. (2018)), we propose to explain the result of Grad-CAM under the decomposi-
tion framework. As shown in Section 4.3, by removing the heuristic step in Grad-CAM, the meaning
of the generated activation map can be explained as the first two terms of the proposed framework.
The perturbation optimization needs reformulation for metric learning and can be more computa-
tional expensive if the point-to-point activation map is desired.
3	Activation Decomposition on A Simple Architecture
Retrieved
Figure 2: Activation decomposition on a simple architecture for deep metric learning.
Partial map
for (i,j)
P Partial map
for (x,y)
To better introduce our method, we first review the formulation of CAM and illustrate our idea on
a simple architecture (CNN+GAP as in Fig. 2) for metric learning. As shown in Eq. 1, CAM is
actually a spatial decomposition of the prediction score of each class, and the original method only
applies for CNN with GAP and one FC layer without bias. The decomposition clearly shows how
much each part of the input image contributes to the overall prediction of one class and provides
valuable information about how the decision is made inside the classification model. The idea is
based on the linearity of GAP:
Sc = X ωk,c(Z X Ai,j,k) = Z X(X ωk,cAi,j,k)	⑴
Here Sc denotes the overall score (before softmax) of class c and ωk,c is the FC layer parameter for
the k-th channel of class c. Ai,j,k denotes the feature map of the last convolutional layer at position
(i, j), and Z is the normalization term of GAP. In fact, the result k ωk,cAi,j,k can be considered
as a decomposition of Sc along (i, j). For a two-stream architecture (see Fig. 2) with GAP and the
cosine similarity metric (S) for metric learning application (e.g. image retrieval), we propose to do
decomposition along (i, j, x, y) so that the relationship between different parts of two images are
uncovered:
S = XGAP(Ak)GAP(Ak) = ⅛ X(XAq,j,k XAx,y,k) = ⅛ X (XAq,j,kAx,y,k).
k	k i,j	x,y	i,j,x,y k
(2)
Here the normalization terms of the cosine similarity (L2 norm) and GAP are included in Z. We
use (i, j), (x, y) for different streams because cross-view applications (Hu et al. (2018)) may have
different image sizes for two streams. A denotes the feature map of the last convolutional layer.
The superscripts q and r respectively denote the query and the retrieved image in this paper. For
each query point (i, j ) in the query image, the corresponding activation map in the retrieved image
is given by Pk Aiq,j,kArx,y,k which is the contribution of features at (i, j, x, y) to the overall cosine
similarity. Like CAM and Grad-CAM, bilinear interpolation is implemented to generate the con-
tribution of each pixel pair and the full resolution map. The overall activation maps of two images
3
Under review as a conference paper at ICLR 2020
are generated by a simple summation along (i, j) or (x, y). Although we only show the positive
activation in the map, the negative value is still available for counterfactual explanation (Selvaraju
et al. (2017)).
Since recent works (Liu et al. (2017); Wang et al. (2018)) have highlighted the superiority of L2
normalization, we use the cosine similarity S as the default metric. With L2 normalization, the
squared Euclidean distance D equals to 2 - 2S so that S and D are equivalent as metrics. Although
there are still a number of existing works (Luo et al. (2019); Ristani & Tomasi (2018)) training with
the Euclidean distance without L2 normalization, Luo et al. (2019) shows that the cosine similarity
still performs well as the evaluation metric. We further empirically show that the cosine similarity
works well for this case (Section 5.4).
4	Extension for Complex Architectures
Recent metric learning approaches usually leverage more complex architectures to improve perfor-
mance, e.g. adding several FC layers after the flattened feature or global pooling. Although different
metric learning applications have different head architectures (e.g. GAP+FC) on CNN, the basic
components are highly similar. We introduce a unified extension to make our method applicable to
most existing state-of-the-art architectures for various applications, including image retrieval (Wang
et al. (2019)), face recognition (Deng et al. (2019)), re-ID (Luo et al. (2019)) and geo-localization
(Hu et al. (2018)). Note that the extension also applies to classification architectures. In Section
4.1, we address linear components by considering them together as one linear transformation. Then
Section 4.2 focuses on transforming nonlinear components to linear in the validation phase.
4.1	Linear Component
The GAP can be considered as a special case of flattened layer (directly reshape the last convolu-
tional layer A ∈ Rm×n×p as one vector A ∈ Rmnp without pooling) multiplied by a transformation
matrix TGAP ∈ Rl×mnp where l denotes the length of the feature embedding vector (see Appendix
A.3 for details). Without loss of generality, we consider a two-stream framework with flattened
layer followed by one FC layer with weights W ∈ Rl×mnp and biases B ∈ Rl. Since the Vi-
sual explanation is generated at the test phase when typical components such as FC layer and batch
normalization (BN) are linear, all the linear components together are formulated as one linear trans-
formation g(A) = W A + B = Eij Wij Ai,j + B in the FC layer. Here Wij ∈ Rl×p and Aij ∈ Rp
denote the weights matrix and feature vector corresponding to position (i, j). Although B is ignored
in CAM, we keep it as a residual term in the decomposition. Then Eq. 2 is re-formulated as:
SZ = gq (Aq) ∙ gr (Ar)
=(X WiqjAq,j + Bq) ∙(X Wr,yAX,y + Br)
i,j	x,y	(3)
=X (WiqjAqj) ∙ (WX,yAr,y) + X(WiqjA% ∙ Br) + X(Wr,yAX,y ∙ Bq) + Bq ∙ Br.
i,j,x,y	i,j	x,y
Here Z denotes the normalization term for cosine similarity (L2 norm), and ∙ is the inner product.
The decomposition of S has four terms, and the last three terms contain the bias B. The first term
clearly shows the activation response for location pair (i, j, x, y) and is considered as the point-to-
point activation in this paper. The second and third terms correspond to the activation between one
image and the bias of the other image. Although they can be considered as negligible bias term,
they actually contribute to the overall activation map. For the overall activation map of the query
image, the second term can be included, because WqjAq j ∙ Br varies at different (i,j) positions
while the third and last terms stay unchanged. Similarly, the first and third terms are considered
when calculating the overall map for the retrieved image. We investigate both settings about the
bias term (i.e. w or w/o) on the overall activation map and they are referred to as “Decomposition”:
(WiqjAqj) ∙ (Px,y Wr,yAX,y), and “DeCOmPOSition+Bias” : (WiqjAqj) ∙ (Px,y Wx；yAX,y + Br) in
Section 5. The last term is pure bias which is the same for every input image. Therefore, we ignore
this term if not mentioned.
4.2	Non-linear Component
4
Under review as a conference paper at ICLR 2020
Non-linear transformation increases the difficulty of ac-
tivation decomposition, fortunately, the most widely
used non-linear components - global maximum pooling
(GMP) and rectified linear unit (ReLU) - can be trans-
formed as linear operations in the validation phase by
multiplying a mask1. We only focus on GMP and ReLU
in this paper, and all the mentioned components together
have covered all the popular architectures for deep metric
learning applications. In the validation phase, the GMP
can be considered as a combination of a maximum mask
M and GAP or flattened layer as shown in Fig. 3. The
result of Hadamard product (M A) is considered as
the new feature map which can be directly applied to
Eq. 3. Similarly, by adding a mask for ReLU, the FC
layer with ReLU can be included in W and B in the vali-
dation phase.
Gm GMP
EqUiv Equivalent
A M
Maximum Mask
Figure 3: Maximum mask for GMP. Z
is the constant normalization term for
GAP.
GAP
2
3
1
1
5
2
0
2
1
×
4.3	Relationship with Grad-CAM
Grad-CAM is another way for generating overall activation map on more complex architecture.
When we calculate the activation map based on Grad-CAM, what do we get? Selvaraju et al. (2017)
has shown that Grad-CAM is equivalent to CAM on GAP based architecture. However, for a clas-
sification architecture with flattened feature and one FC layer, the prediction score for class c may
be formulated as Sc = Pi,j,k Wi,j,k,cAi,j,k where W is the reshaped weights ofFC layer following
the shape of the last convolutional feature (A). From our activation decomposition perspective, the
contribution of each position (i, j) is clearly given by Pk Wi,j,k,cAi,j,k, while the Grad-CAM map
for class c at (i, j) position is given by:
GTadCAMij,c = X(Aij,k GAP (藐))=X(Aij,k GAP(Wk))	(4)
In this case, the result of Grad-CAM is different from activation decomposition because of the GAP
in Eq. 4. For architectures using GMP, they are also different, because only the maximal value of
each channel contributes to the overall prediction score, while Grad-CAM puts the same weight for
features at all positions (see Appendix A.4 for details). Although Selvaraju et al. (2017) empirically
shows that the heuristic GAP step can help improve the performance of localization, this step makes
the meaning of the generated map unclear. By removing this step, which means combining the
gradient and feature directly as Aij,k∂ASc., the modified version would generate the same result
as activation decomposition for flattened and GMP based architectures. As for the metric learning
architecture discussed in Section 4.1, the Grad-CAM map of query image is written as (the derivation
is included in Appendix A.5):
GTadCAMij = 1(队EdEq" GAP(Wq')Aij)∙ (X W；yArχ,y + Br)	⑸
x,y
Eq denotes the embedding vector of query image before L2 normalization and Z is the normalization
term. The gradient term 队EdEEcl |), which comes from the L2 normalization, would put less weights
on dominant channels so that the generated activation map becomes more scattering (Appendix A.5).
It can be removed by calculating gradient from Eq ∙ Er without L2 normalization. If We remove
the gradient term as well as the GAP term, the result is actually equivalent to the overall map of
“Decomposition+Bias” given by the first two terms ofEq. 3 as (Wqj Alij) ∙ (Pxy Wr,yAX,y + BT).
5	Experiment
5.1	Weakly Supervised Localization
Weakly supervised localization has been adopted as evaluation metric in most existing works, we
thus conduct localization experiment in the context of metric learning to evaluate the overall ac-
1The mask is computed once for each input image.
5
Under review as a conference paper at ICLR 2020
tivation map. We follow the state-of-the-art image retrieval method Wang et al. (2019) on CUB
(Welinder et al. (2010)), which contains over 10,000 images from 200 bird spices. CUB is a chal-
lenging dataset for metric learning and the bounding box is available for localization evaluation.
We first train a model on CUB with the loss function and architecture of Wang et al. (2019), and
then conduct the weakly supervised localization to show the differences between the variants of
proposed methods. As CAM and Grad-CAM generate the mask with different threshold settings
(0.2 and 0.15), we use multiple thresholds and find the result is sensitive to threshold (see Appendix
A.1). We report the best result of each method in Table1. The “Grad-CAM (no norm)” means the
Grad-CAM with gradient computed from the product of two embedding vectors without normaliza-
tion and the “Decomposition+Bias” denotes the first two terms of Eq. 3. The proposed framework
outperforms Grad-CAM for metric learning as well as CAM based on classification. As justified in
Section 4.3, computing the gradients before normalization does improve the performance of Grad-
CAM by a large margin. Since the architecture of Wang et al. (2019) is based on GMP, “Grad-CAM
(no norm)” is not equivalent to “Decomposition+Bias” as shown in Section 4.3.
Table 1: Localization accuracy for 0.5 IOU on CUB val-
idation set.
	Method	Accuracy
Classification	CAM (Zhou et al. (2016))-	-410%-
Metric Learning	Grad-CAM	-167%-
	Grad-CAM (no norm) Decomposition+Bias (ours) Decomposition (ours)	-48.3%- 44.9% 50.6%
Input
GradCAM
GradCAM	Decomposition Decomposition
(no norm)	+Bias
Figure 4: Qualitative results on CUB.
5.2	Model Diagnosis
A large number of loss functions have been proposed for metric learning (Schroff et al. (2015);
Oh Song et al. (2016); Wang et al. (2019)), while only the performance and embedding distribution
are evaluated. Here we show that the overall activation map can also help evaluate the generalization
ability of different metric learning methods. We follow the setting of Wang et al. (2019) and train
Table 2: Top-1 recall accuracy on Table 3: Localization accuracy (0.5 IOU) on CUB training set.
Figure 5: Qualitative results for model diagnosis.
two metric learning models with multiple similarity (MS) loss Wang et al. (2019) and triplet loss
Schroff et al. (2015), respectively. As shown in Table 2, although they have almost the same accuracy
on training set, there is a big gap between their generalization ability on the validation set. Before
checking the result on the validation set, is there any clue in the training set for such gap? Despite the
similar training accuracies, the predictions of two models are actually based on different regions (see
Fig. 5). The quantitative results in Table 3 also support the fact that “Triplet” model is more likely to
focus on the background region rather than the object as we witness the localization accuracy drop
6
Under review as a conference paper at ICLR 2020
for Triplet loss. Although “Decomposition+Bias” can be more sensitive to different loss functions,
implying that the bias term does provide valuable information on overall map, both settings of
the proposed method have the same trend on accuracy. Therefore, our activation decomposition
framework can help shed light on the generalization ability of loss functions for metric learning.
5.3 Application I: Cross-view Pattern Discovery
0°	90°	180°	270°	360°
Aligned Image Pair
180°
90°
0°
270°
Ground Truth
Rotation Angle
Overall Estimated: Partial Estimated:
147.9°	31.9°
Figure 6: Example of cross-view pattern discovery, e.g. image orientation estimation. (Best viewed in color)
When the metric learning is applied to cross-view applications, e.g. image retrieval (Zhai et al.
(2017); Hu et al. (2018); Tian et al. (2017)), the model is capable of learning similar patterns in
different views which may provide geometric information of the two views, e.g. the camera pose
or orientation information. We conduct orientation estimation experiment to show the advantage of
partial activation map compared with the overall activation map on providing geometric information
based on cross-view patterns. In our experiment, we take street-to-aerial image geo-localization as
an example and train a Siamese network following the loss function of Hu et al. (2018). The ob-
jective of geo-localization is to find the best matched aerial-view image in a reference dataset for
a query street-view image. We conduct the experiment on CVUSA (Zhai et al. (2017)), which is
the most popular benchmark for this problem containing 35,532 training pairs and 8,884 test pairs.
Each pair consists of a query street image and the corresponding orientation-aligned aerial image
at the same GPS location. The orientation example is illustrated as angle degree in Fig. 6 (the
yellow lines on both images correspond to 0°). We train the Siamese image matching model With
randomly rotated aerial images so that the activation map is approximately rotation invariant for
aerial image. As shown in Fig. 6, the overall activation map may contain multiple highlighted re-
gions contributing to the overall matching/similarity score. Our method can further provide detailed
point-to-point relationship which is critical for orientation estimation. Here we utilize the maximum
activated region for orientation estimation to show the superiority of the partial map compared with
the overall map. We first generate the overall activation map as well as the partial map correspond-
ing to the maximum activation position of the query street-view image. For both activation maps,
the activation peaks from different views are selected for orientation estimation as shown in Fig. 6.
In this example, the overall activation maps contain multiple
highlighted regions and the relationship between them are un-
clear. The activation peaks in the two views (the cyan boxes)
actually correspond to different objects which leads to a fail-
ure estimation. However, the partial activation map clearly
shows the relationship between these regions and provides
more accurate estimation (the red line). We also present the
quantitative results in Fig. 7. As can be seen from this fig-
ure, the partial activation map significantly outperforms the
overall map for cross-view orientation estimation. Partial ac-
tivation based orientation estimation has over 16% samples
with angle error less than ±3.5° (the red bar at 0°), while
overall map based method only has less than 12%.
Angle error
Figure 7: Estimation error distri-
bution of overall and partial map.
(Better viewed with zoom in)
5.4 Application II: Interactive Retrieval
Verification applications like face recognition and re-ID usually retrieve images with a complete
query image. However, the complete query image may be not available in some scenarios. For
example, only part of a person is captured in surveillance image/video due to occlusion or view-
point. Interactive retrieval, which retrieves images with interactively selected region of the query
7
Under review as a conference paper at ICLR 2020
Beard
Nose
Clothes
Shoes
Figure 8: Top retrieved images by interactive retrieval on face and re-ID datasets. The red box on
the left column indicates the query part.
image, can provide critical information for such circumstance. Since our framework provides point-
to-point activation map, it can be served as a reasonable tool for measuring partial similarity. With
the partial similarity, we are able to interactively retrieve images having similar parts with the query
image instead of the overall most similar image, which may contain irrelevant or noisy informa-
tion. As shown in Fig. 8, the partial activation generated by the proposed method works well on
retrieving people with similar clothes and faces with similar beard. We follow the pipeline of re-
cent approaches on face recognition (Deng et al. (2019)) and person re-identification (Luo et al.
(2019)). For re-ID, the model is trained and evaluated on Market- 1501 (Zheng et al. (2015)) where
some validation images only contain a small part of a person. Although the model is trained with Eu-
clidean distance without L2 normalization, cosine similarity still works well as the evaluation metric.
For face recognition, We take the trained model on CASIA-
WebFace (Yi et al. (2014)) and evaluate the model on FIW (Robin-
son et al. (2018)) Where the face identification and kinship rela-
tionship are available. The interactive search is adopted by simply
matching the equivalent partial feature in Eq. 3 (Wiq,j Aiq,j) With
the reference embedding features. Fig. 8 shoWs that different im-
ages are retrieved When searching With different partial features on
the same query image. HoWever, We do find some failure cases as
shoWn in the last roW of Fig. 8 With the red box. There is no shoes in
the failure image, but Why is this image retrieved With a top rank? In
the activation map of Fig. 9, three regions of the query image have
a high activation on the retrieved image. And there is a high activa-
tion on the purse in the retrieved image corresponding to the shoes
in the query image. This might be because of their similar red color Figure 9: Explanation for the
and the arm of the retrieved person may appears like a leg from a failure case by decomposi-
specific vieWpoint. This example also validates the importance of tion.
partial activation map generated by our frameWork for explanation.
6 Conclusion
We propose a simple yet effective frameWork for visual explanation of deep metric learning based
on the idea of activation decomposition. The frameWork is applicable to a host of applications,
e.g. image retrieval, face recognition, person re-ID, geo-localization, etc. Experiments shoW the
importance of visual explanation for metric learning as Well as the superiority of both the overall
and partial activation map generated by the proposed method. Furthermore, We introduce tWo po-
tential applications, i.e. cross-vieW pattern discovery and interactive retrieval, Which highlight the
importance of partial activation map.
8
Under review as a conference paper at ICLR 2020
References
Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim.
Sanity checks for saliency maps. In Advances in Neural Information Processing Systems, pp.
9505-9515, 2018.
Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and Philip HS Torr. Fully-
convolutional siamese networks for object tracking. In European conference on computer vision,
pp. 850-865. Springer, 2016.
Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth N Balasubramanian. Grad-
cam++: Generalized gradient-based visual explanations for deep convolutional networks. In 2018
IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 839-847. IEEE, 2018.
Sumit Chopra, Raia Hadsell, Yann LeCun, et al. Learning a similarity metric discriminatively, with
application to face verification. In CVPR (1), pp. 539-546, 2005.
Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular mar-
gin loss for deep face recognition. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturba-
tion. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3429-3437,
2017.
Albert Gordo and Diane Larlus. Beyond instance-level image retrieval: Leveraging captions to learn
a global visual representation for semantic retrieval. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 6589-6598, 2017.
Sixing Hu, Mengdan Feng, Rang MH Nguyen, and Gim Hee Lee. Cvm-net: Cross-view matching
network for image-based ground-to-aerial geo-localization. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, pp. 7258-7267, 2018.
Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep
hypersphere embedding for face recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 212-220, 2017.
Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, and Wei Jiang. Bag of tricks and a strong baseline
for deep person re-identification. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops, pp. 0-0, 2019.
Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting
them. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
5188-5196, 2015.
Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted
structured feature embedding. In The IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), June 2016.
Ergys Ristani and Carlo Tomasi. Features for multi-target multi-camera tracking and re-
identification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
J P Robinson, M Shao, Y Wu, H Liu, T Gillis, and Y Fu. Visual kinship recognition of families in
the wild. In IEEE Transactions on pattern analysis and machine intelligence, 2018.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face
recognition and clustering. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2015.
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 618-626,
2017.
9
Under review as a conference paper at ICLR 2020
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for
simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.
Yicong Tian, Chen Chen, and Mubarak Shah. Cross-view image matching for geo-localization in
urban environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition,pp. 3608-3616, 2017.
Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei
Liu. Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 5265-5274, 2018.
Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R Scott. Multi-similarity loss
with general pair weighting for deep metric learning. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5022-5030, 2019.
P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD
Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.
Wenjie Yang, Houjing Huang, Zhang Zhang, Xiaotang Chen, Kaiqi Huang, and Shu Zhang. Towards
rich feature discovery with class activation maps augmentation for person re-identification. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1389-
1398, 2019.
Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning face representation from scratch. arXiv
preprint arXiv:1411.7923, 2014.
Menghua Zhai, Zachary Bessinger, Scott Workman, and Nathan Jacobs. Predicting ground-level
scene layout from aerial imagery. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 867-875, 2017.
Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff.
Top-down neural attention by excitation backprop. International Journal of Computer Vision, 126
(10):1084-1102, 2018.
Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian. Scalable person
re-identification: A benchmark. In Proceedings of the IEEE international conference on computer
vision, pp. 1116-1124, 2015.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep
features for discriminative localization. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
Bolei Zhou, David Bau, Aude Oliva, and Antonio Torralba. Interpreting deep visual representations
via network dissection. IEEE transactions on pattern analysis and machine intelligence, 2018.
10
Under review as a conference paper at ICLR 2020
A Appendix
A. 1 Weakly Supervised Localization Details
As discussed in Appendix A.4, the activation map of Grad-CAM on GMP architecture is more scat-
tering, as larger threshold is needed for Grad-CAM to generate comparable result in our experiment.
Table 4: Localization accuracy (IOU 0.5) with different thresholds on CUB validation set.
Threshold	Grad-CAM	Grad-CAM(no norm)	Decomposition+Bias	Decomposition
0.15	-16.71%-	16.83%	23.49%	17.38%
0.2	16.26%	17.06%	32.01%	19.77%
0.3	14.48%	21.59%	44.94%	34.92%
0.4	9.43%	35.01%	37.85%	50.64%
0.5	4.43%	47.00%	21.99%	45.78%
0.6	1.54%	48.27%	9.35%	27.98%
0.7	0.27%	20.90%	2.25%	9.11%
A.2 Model Diagnosis Details
Table 5: Localization accuracy (IOU 0.5) on CUB training set with MS (multiple similarity) loss.
Threshold	Decomposition+Bias	Decomposition
0.2	41.05%	26.16%
0.3	52.10%	44.41%
0.4	42.77%	58.99%
0.5	24.97%	51.17%
0.6	9.35%	27.98%
Table 6: Localization accuracy (IOU 0.5) on CUB training set with Triplet loss.
Threshold	Decomposition+Bias	Decomposition
0.2	36.60%	24.00%
0.3	39.93%	38.57%
0.4	29.03%	55.60%
0.5	14.22%	53.49%
0.6	4.93%	32.49%
A.3 Transformation Matrix for GAP and GMP
For the feature A ∈ Rm×n×p , the GAP (global average pooling) is equivalent to the flattened feature
A ∈ Rmnp followed by transformation matrix TGAP ∈ Rp×mnp:
GAP(A) = ----^X Aij= TGAPA
mn
i,j
(6)
Here (i, j) denotes the spatial coordinates. By reshaping the TGAP to P X m X n X P as TGAP, the
matrix is given by:
…/	,、 f ɪ C = k
TGAP(c,i,j,k) = m mn	⑺
(0 c = k
The matrix TGAP is SimPIy calculated by reshaping TGAP to P X mnp. As shown in Section 4.2,
GMP is equivalent to GAP with a maximum mask in validation phase. The transformation matrix
of GMP (global maximum pooling) is given by:
TGMP = mn(TGAP ® M)
(8)
11
Under review as a conference paper at ICLR 2020
Here M ∈ Rm×n×p is the maximum matrix of A where only the maximum position of each channel
has nonzero value as 1. TGMP is computed by reshaping TGMP to P X mnp.
A.4 GRAD-CAM ON GMP
Considering a classification architecture with GMP and FC (W ∈ Rp×l and no bias), the prediction
score for class c is given by Sc = Pk maxi,j (A:,:,k)Wk,c = Pi,j,k Ai,j,kMi,j,kWk,c, where M is
the maximum matrix in Appendix A.3. The decomposition of position (i, j) is Pk Ai,j,kMi,j,kWk,c
and only the maximum position of each channel has a nonzero value. However, for Grad-CAM, the
activation map is given by:
GradCAMij = Ai,j ∙ GAP(
∂Sc
∂Aij
)=m XAi* Wk,c,
k
(9)
where all (i,j) positions have a nonzero weight resulting in a more scattering activation map.
A.5 Grad-CAM for Metric Learning
For metric learning architecture like Section 4.1, the similarity is formulated as S = 高尚：|, where
Eq ∈ Rl and Er ∈ Rl are the embedding vector of query and retrieved image. |x| denotes the L2
norm and a ∙ b is the inner product of a and b. The Grad-CAM map of query image is given by:
∂S	∂S
GradCAMIL Aq,j ∙ GAP(dAq) = (GAP(荻))TAqj
With the gradient chain rule, the gradient is written as:
(10)
(Er )TEq
∂S = d ⅛⅛ =( Er_ )τ ∂(E q∕∣Eq |) ∂Eq	Er_ γr ∂(E q∕∣Eq |) W q
∂Aq	∂Aq	(IEr |)	∂Eq	∂Aq	(IEr |)	∂Eq	( )
By expanding Er as Px WxryArxy + Br (Section 4.1) and merging Eq. 10 with Eq. 11, the
,y ,	,
Grad-CAM map is reformulated as:
GradCAMij = (GAP((ɪ)T d(EqEEq|) Wq))TAqj
=Z(X (Er )τ d(EqEEq|) Wiqj )Aj
i* j*
=Z(E r )τ d(EqEEq|) (X Wi* j* )Aj	(i2)
i* ,j*
=Z(X W；yAx,y + Br) ∙ (d(EqEEq |) GAP(Wq)Aq,j)
x,y
=1(d(EqEE q |) GAP(Wq )Aj) ∙ (X W" Ax,y+Br)
x,y
Here the Z is the normalization term for simplicity. dEEE1 is the l × l Jacobian matrix given by:
∂(E∕∣E∣)
∂E
(∂ (Ei∕∣E∣) ʌ
V ∂Ej 儿
i=j
i 6= j
(13)
pɪ is the normalization term. For dominant channel i, the weight (1 一 E2) is small resulting in a
more scattering activation map.
12
Under review as a conference paper at ICLR 2020
A.6 Qualitative Results
We provide more qualitative results of using our activation decomposition framework for the two
new applications, i.e., cross-view pattern discovery and interactive retrieval.
A.6. 1 Cross-view Pattern Discovery
0°	90°	180°	270°	360°
0°
360°
90°
180°
270°
Ground Truth Angle=173°
Point-specific
∖ Map	/
180°
90
0°
270°
Overall
Estimated：57.7°
Partial
Es Estimated:
199.7°
Figure 10: Examples of cross-view pattern discovery, i.e., image orientation estimation. (Best viewed in color)
13
Under review as a conference paper at ICLR 2020
A.6.3 Interactive Retrieval on Person Re-ID
A.7 Cross-view Pattern Discovery Details
A.7.1 Explanation of Figure 6
90°
360°
0°
180°
270°
Partial
∖.	90.2°
0°
Ground Truth
Rotation Angle
Aligned Image Pair
180°
302.1°
122.1°
Overall Estimated:
147.9°
90.2°
Partial Estimated：
31.9°
Figure 13: Example of cross-view pattern discovery, e.g. image orientation estimation. (Best viewed in color)
Here we explain Fig. 6 again (Fig. 13) with more details. We first reiterate the application and
experiment setting. The dataset (CVUSA) we used in the experiment is for image-based cross-
view geo-localization (Zhai et al. (2017); Hu et al. (2018)) and contains image pairs from street and
aerial views. Each matched image pair corresponds to the same GPS location. The objective of
geo-localization is to find the best matching GPS-tagged aerial-view image in a reference dataset
for a query street-view image. In the CVUSA dataset (Zhai et al. (2017)), the image pairs are
aligned in terms of orientation. For example, as shown in Fig. 13, the left two images (street-
view panorama and aerial-view images) are the original image pair and the yellow line denotes the
0。which corresponds to the south direction (180。corresponds to the North direction). We use
[0,360] ◦ to denote different angles as marked on these two images.
For the cross-view image matching/retrieval task, if the image pairs are not aligned (e.g., randomly
rotate the aerial view images), we discovered that the activation map can be used for orientation
estimation. Specifically, we train the Siamese image matching model with randomly rotated aerial
images so that the model is rotation-invariant and so is the overall activation map for aerial image.
In the example of Fig. 13, the overall activation maps for the original aligned pair are first generated
to show the highlighted regions, and both views focus on the road area in this example. The most
14
Under review as a conference paper at ICLR 2020
activated regions are highly relevant in both views and most of them are similar patterns. When we
randomly rotate the aerial image (30° in this example), the aerial view activation map still focuses
on the road area which is relevant to the street view. In this paper, we simply use the pixel with the
maximum activation value for orientation estimation, because the most activated areas (highlighted
in cyan boxes in Fig. 13) are likely to be relevant from two views. For the street view image, the
selected pixel lies in the angle of 90.2° (the cyan line) based on the angle marks on the left aligned
image. For the rotated aerial view, the selected pixel lies in the angle of 302.1° (the cyan line). Since
the overall map contains multiple activated regions, the selected pixels in both views actually do not
correspond to the same object, which reveals one disadvantage by only using the overall activation
map. The estimated angle is 302.1° - 90.2° = 147.9°, which is not correct. However, for the
selected pixel (the one with the maximum activation value) in the street view image, if we generate
its corresponding point-specific (partial) activation map on the aerial view image (as shown on the
very right of Fig. 13), the new selected pixel on this point-specific activation map lies in the angle of
122.1° (the red line). Then, the estimated angle is calculated as 122.1° - 90.2° = 31.9°, which is
very close to the ground truth (30°). This demonstrates the advantage of the point-specific activation
decomposition for finding more fine-grained information in this application.
A.7.2 How to compute the angle error
The angle error is computed by error = ground truth - estimated angle. We then add or subtract
360° to this term to make it in the range of [-180, 180]° if the absolute value of the error is greater
than 180°. For example, when the error is 359°, we will subtract it by 360° and get -1° degree as
the error. This is a reasonable setting adopted from the previous work (Zhai et al. (2017)).
A.7.3 Sensitivity of the point-specific activation map
We present a demo to show how the point-specific map changes according to the query pixel:
https://github.com/Jeff-Zilence/anonymous. In this example, the resolution of the
query image is 224× 1232. As shown in the demo, the point-specific activation map changes dramat-
ically when the query pixel moves from the left to the right in the query image because the object
changes. As in this example, when the query pixel moves by 10 pixels, the object would change
and the corresponding point-specific activation map on the retrieved aerial image will be quite dif-
ferent. By generating the point-specific activation map, we can obtain fine-grained information or
interpretation for deep metric learning.
A.8 Interactive Retrieval Details
A.8.1 How to compute the similarity given a point of interest
Following Eq. 3, we first compute Wiq,j Aiq,j as the feature of each position on the last convolutional
layer, and a bilinear interpolation is adopted to generate the feature for every pixel of the original
image. For a point of interest (i, j), we compute the cosine similarity between the calculated feature
on (i, j) and the embedding feature of the reference images as the point-specific similarity, so the
embedding features of the reference dataset do not need to be recomputed. This similarity can be
also considered as the summation of the values in the point-specific map corresponding to (i, j). In
the case of Eq. 2 (CNN+GAP), the similarity can be simplified as Px,y (Pk Aiq,j,kArx,y,k).
15