Under review as a conference paper at ICLR 2020
Granger Causal Structure Reconstruction
from Heterogeneous Multivariate Time Series
Anonymous authors
Paper under double-blind review
Ab stract
Granger causal structure reconstruction is an emerging topic that can uncover
causal relationship behind multivariate time series data. In many real-world sys-
tems, it is common to encounter a large amount of multivariate time series data
collected from heterogeneous individuals with sharing commonalities, however
there are ongoing concerns regarding its applicability in such large scale com-
plex scenarios, presenting both challenges and opportunities for Granger causal
reconstruction. To bridge this gap, we propose a Granger cAusal StructurE
Reconstruction (GASER) framework for inductive Granger causality learning and
common causal structure detection on heterogeneous multivariate time series. In
particular, we address the problem through a novel attention mechanism, called
prototypical Granger causal attention. Extensive experiments, as well as an on-
line A/B test on an E-commercial advertising platform, demonstrate the superior
performances of GASER.
1	Introduction
Broadly, machine learning tasks are either predictive or descriptive in nature, often addressed by
black-box methods (Guo et al., 2018). With the power of uncovering relationship behind the data
and providing explanatory analyses, causality inference has drawn increasing attention in many
fields, e.g. marketing, economics, and neuroscience (Pearl, 2000; Peters et al., 2017). Since the
cause generally precedes its effects, known as temporal precedence (Eichler, 2013), recently, an
increasing number of studies have focused on causal discovery from time series data. They are
commonly based on the concept of Granger causality (Granger, 1969; 1980) to investigate the causal
relationship with quantification measures.
In many real-world systems, it is common to encounter a large amount of multivariate time series
(MTS) data collected from different individuals with shared commonalities, which we define as het-
erogeneous multivariate time series. The underlying causal structures of such data often vary (Zhang
et al., 2017; Huang et al., 2019). For example, in the financial market, the underlying causal drivers
of stock prices are often heterogeneous across stocks of different plates. Similar phenomenons are
also observed in the sales of different products in E-commerce. To this situation, most existing
methods have to train separate models for MTS of each individual, which suffer from over-fitting
especially given limited training samples. Although some works have been proposed to solve such
problem (Zhang et al., 2017; Huang et al., 2019), they lack the inductive capability to do inferences
for unseen samples and fall short of fully exploiting shared causal information among the heteroge-
neous data which often exist in practice. For instance, the causal structures of the products belonging
to the same categories are usually similar. Such shared information presents opportunities for causal
reconstruction to alleviate overfitting and to do inductive reasoning. However, it is also challenging
to detect common and specific causal structures simultaneously.
In this paper, we propose a Granger cAusal StructurE Reconstruction (GASER) framework for
inductive Granger causality learning and common causal structure detection on heterogeneous mul-
tivariate time series data. Our approach builds on the idea of quantifying the contributions of each
variable series into the prediction of target variable via a novel designed prototypical Granger causal
attention mechanism. In order to ensure that the attention capturing Granger causality, we first de-
sign an attention mechanism based on Granger causal attribution of the target series and then perform
prototype learning that generates both shared and specific prototypes to improve the model’s robust-
1
Under review as a conference paper at ICLR 2020
ness. Extensive experiments demonstrate the superior causal structure reconstruction and prediction
performances of GASER. In summary, our specific contributions are as follows:
•	A novel framework that inductively reconstructs Granger causal structures and uncovers common
structures among heterogeneous multivariate time series.
•	A prototypical Granger causal attention mechanism that summarizes variable-wise contributions
towards prediction and generates prototypes representing common causal structures.
•	Relative extensive experiments on real-world, benchmark and synthetic datasets as well as an on-
line A/B test on an E-commercial advertising platform that demonstrate the superior performance
on the causal discovery and the prediction performance comparable to state-of-the-art methods.
2	GASER
In this section, we formally define the problem, introduce the architecture of GASER, present the
prototypical Granger causal attention with the final objective function.
2.1	Problem Definition
Assuming we have a set of heterogeneous multivariate time series from N individuals, i.e., X =
{Xi}iN=1, with each consisting of S time series of length T, denoted as Xi = (xi1, xi2, . . . , xiS)T ∈
RS×T, where xis = (xis,1, xis,2, . . . , xis,T)T ∈ RT represents the s-th time series of individual i, and
one of them is taken as the target series yi. We aim to train a model that (1) reconstructs Granger
causal structures among variables for each individual; (2) generates K common structures among
all the N individuals, each structure represented by a prototype pk ∈ RS, k = 1, ..., K; and (3)
learns a nonlinear mapping to predict the next value of the target variable series for each individual,
i.e., yi,τ+ι = F (Xi).
2.2	Network Architecture
Our GASER framework consists of two parts: a set of parallel encoders, each predicting the target
given the past observations, and an attention mechanism that generates prototypical Granger causal
attention vectors to quantify variable-wise contributions towards prediction. Figure 1 illustrates the
overall framework of GASER. As illustrated in Figure 1(a), for an input multivariate time series
Xi , the encoder specific to s-th variable projects the time series xis into a sequence of hidden state,
denoted as his,t = Hs(xi,t, his,t-1). The encoder could be any RNN models, such as LSTM (Hochre-
iter & Schmidhuber, 1997) and GRU (Cho et al., 2014). The last hidden states, {his,T}sS=1, are used
as the hidden embeddings of each variable. Then the predicted next value of the target variable con-
ditioned on historical data of variable s, denoted as yi,τ+)can be computed by yf,τ十1=fs (hf,τ),
where fs(∙) denotes the MLP network specific to variable s. Then We obtain the prediction yi,τ +1
by aggregating the predicted values specific to variables through the prototypical Granger causal
attention described below.
2.3	Prototypical Granger Causal Attention
We propose a novel attention mechanism in GASER, namely prototypical Granger causal attention,
to reconstruct Granger causal relationships for each individual and uncover common causal struc-
tures among heterogeneous individuals. The goal is to learn attentions that can reflect the Granger
causal strength between variables for each individual, and generate prototypes among heteroge-
neous individuals. As illustrated in Figure 1(b), the idea of the prototypical Granger causal attention
mechanism is as follows. The Granger causal attribution corresponding to each individual is first
computed according to the concept of Granger causality, followed by prototype learning that sum-
marizes common causal structures for heterogeneous individuals in the training set, and produces
the attention vector specific to each individual. The details of these two parts are described below.
2.3.1	Granger Causal Attribution
Granger causality (Granger, 1969; 1980) is a concept of causality based on prediction, which de-
clares that if a time series x Granger-causes a time series y, then y can be better predicted using
all available information than if the information apart from x had been used. Thus, we obtain the
Granger causal attributions by comparing the prediction error when using all available information
2
Under review as a conference paper at ICLR 2020
(a) The architecture of the proposed GASER.
Prototypical Granger Causal Attention
Diversity Loss
Prototype Learning
ri
Prototypical
attention
(b) Prototypical Granger causal attention.
MSE
Loss
GUmbe- SoftmaX
Output
prototype
1
2
a
a
aS
ai
Prototypical Granger
causal attention
Figure 1: The overview of the GASER framework.
with the error when using the information excluding one variable series. In particular, given all the
hidden embeddings {his,T}sS=1 of individual i, we obtain the embedding that encodes all available
information and the one that encodes all available information excluding one variable s, denoted as
hiall and hiall\s respectively, by concatenating the embeddings of corresponding variables:
hiall = [hij,T]jS=1, hia \s = [hij,T]jS=1,j6=s,
(1)
where [∙] represents the concatenation operation. Then We feed them into respective predictors,
denoted as gaiι(∙) and gs (∙), to get the predicted value of target and compute the squared errors:
畔+ι = galι(hall), ya,T+sι = gs(hal'∖),
all	all	2	all\s	all\s	2
εi = (yi,T +1 ― yi,T +1) , εi = (yi,T +1 ― yi,T +1) ,
(2)
(3)
where the predictor gaiι(∙) and gs(∙) can be MLP networks. Inspired by Schwab et al. (2019), We
define the Granger causal attribution of the target variable corresponding to variable s as the decrease
in error when adding s-th series to the set of available information, computed as:
∆εS = ReLUg队，-εTl)
(4)
where ReLU(∙) is the rectified linear unit. For each individual i, by normalising the Granger causal
attribution, we obtain an attention vector that reflects Granger causality, namely Granger causal
attention, denoted as qi . The attention factor for variable s can be computed as:
∆εs
qi = PS=1∆εj
(5)
2.3.2 Prototype Learning
The Granger causal attention above is not robust enough to reconstruct Granger causal structure,
given limited data (e.g., very short time series) of each individual in training. We address the problem
3
Under review as a conference paper at ICLR 2020
by generating Granger causal prototypes from all the individuals, under the assumption that there
should be several common causal structures among heterogeneous individuals.
In particular, we assume there exist K Granger causal prototypes, denoted as {pk}kK=1, and compute
the similarity between the Granger causal attention vector qi of individual i and each prototype
vectors pk . Since the attention can be seen as a distribution, we use the cosine similarity:
d _	Pk ∙ qi
k,i = kP⅛k,
(6)
Then we output a prototype most similar to qi by sampling from the similarity distribution di using
Gumbel-Softmax (Maddison et al., 2017; Jang et al., 2016), which samples from a reparameterized
continuous distribution approximation to the categorical one-hot distribution:
e = GUmbelSoftmax(di) = Softmax((log(di) + g)∕τ),	(7)
where GumbelSoftmax(∙) denotes the Gumbel-Softmax function, e ∈ RK is the sample vector
which approaches one-hot, and g is a vector of i.i.d. samples drawn from GUmbel(0, 1) distribUtion.
τ is the softmax temperature, and the distribution becomes discrete when τ goes to 0. With the
sample vector e, the output prototype P can be obtained as:
P = [pi,P2,...,PK] ∙ e.
(8)
After normalizing the sampled prototype, we obtain an attention vector for individual i, denoted as
ri , namely prototypical attention.
The Granger causal attention reflects the Granger causal structure specific to each individual, while
the prototypical attention reflects one common Granger causal structure most similar to the Granger
causal structure of each individual. To detect the specific and common causal structures simulta-
neously, we summarize them together and generate the prototypical Granger causal attention ai as
follows:
ai = αqi + (1 - α)ri,	(9)
where α ∈ [0, 1] is a hyperparameter that controls the ratio of the two attention mechanism.
Finally, the prediction of the target variable’s next value can be computed as the weighted sum of
the predicted values from all variables:
S
yi,T +1 =	aSyS,T +1∙
s=1
(10)
2.4	Learning Objective
In order to obtain accurate prediction and Granger causality structure, and generate diverse common
causality structures, the objectives of GASER consist of three parts. The first two objective functions
are to encourage accurate predictors, including the predictors f (∙) to perform final prediction and
the auxiliary predictors g(∙) to compute Granger attribution, and We adopt the the mean squared
error (MSE) as the prediction loss function:
N	Ns
Lpred = N ^X(yi,T +1 - Vi,T +1)2, LaUX = N X(εall + Xεall∖s).	(11)
i=1	i=1	s=1
The last objective function is to avoid duplicate prototypes by a diversity regularization term that
penalizes on prototypes that are similar to each other (Ming et al., 2019):
KK
LdlV = XjXI max(γ, kPkPjk),	(12)
where γ controls the closeness to a tolerable degree.
To summarize, the loss function, denoted by L, is given by:
L=Lpred+λ1Laux+λ2Ldiv,	(13)
where λ1 and λ2 are hyperparameters that adjust the ratios between the losses.
4
Under review as a conference paper at ICLR 2020
3	Experiments
In this section, we evaluate the causal structure reconstruction performance on multivariate time
series from both single individual and multiple individuals, as well as the prediction performance of
GASER. We also conduct an online A/B test on an E-commerce advertising platform to further test
GASER in more practical situations.
3.1	Experimental Setup
We first evaluate the causal structure reconstruction performances on two causal benchmark datasets.
Finance (Kleinberg, 2009) consists of simulated financial market time series with known underlying
causal structures. Each dataset includes 25 variables of length 4,000. For each dataset, we choose
variables that are related to the most causes as the target variables to test model abilities in the
relatively most challenging scenarios.
FMRI (Smith et al., 2011) contains 28 different Blood-oxygen-level dependent time series datasets
with the ground-truth causal structures. In the experiments, we evaluate on the first 5 datasets and
take the first variable as the target as causal variables distribute relatively evenly in this dataset.
Then, we evaluate the causal structure reconstruction performance on heterogeneous individuals on
synthetic data:
Synthetic data: We first obtain the S exogenous time series through the following Non-linear Au-
toregressive Moving Average (NARMA) (Atiya & Parlos, 2000) generators:
d
xis,t = αsxis,t-1 + βsxis,t-1	xis,t-j + γsεi,t-dεi,t-1 +εi,t,	(14)
j=1
where εt are zero-mean noise terms of 0.01 variance, d is the order of non-linear interactions, and
αs, βs and γs are parameters specific to variable s, generated from N (0, 0.1). Then, we generate
the target series from the generated exogenous series via the formula:
S
yi,t =	ωis(ηis)T tanh (xis,t-p:t-1) +εi,t,	(15)
s=1
where ωis ∈ {0, 1} with 0.6 probability of being zero that controls the underlying causal relationship
from the s-th variable to the target variable, ηis ∈ Rp controls the causal strength sampling from
Unif{-1, 1}, and xis,t-p:t-1 = (xis,t-p,xis,t-p+1,...,xis,t-1)T ∈ Rp represents the last p historical
values of variable s of sample i. The 0-1 indicator vector ωi = (ωi1 , ωi2, . . . , ωiS)T ∈ RS is the
ground-truth causal structure of i-th individual.
For the causal structure reconstruction task, we compare our method with previous causal discovery
methods including linear Granger causality (Granger, 1969; LutkePohL 2005) and TCDF (NaUta
et al., 2019), as well as the interpretable neural network based prediction method, i.e., IMV-
LSTM (Guo et al., 2019), using the standard metrics of Area Under the Precision-Recall Curve
(PR-AUC), and Area Under the ROC Curve (ROC-AUC) (Fawcett, 2006).
Since a byProduct of GASER is the time series Prediction, we also evaluate the Prediction Perfor-
mance on the real-world datasets, i.e., PM2.5 and SML:
PM2.5 contains the hourly PM2.5 and meteorological data in Beijing during Jan 2010 to Dec 2014,
includes 7 variables (such as PM2.5 concentration, temPerature, Pressure and wind sPeed), and
forms a multivariate time series of length 43,824. The PM2.5 concentration is the target series. the
dataset is sPlit into training (60%), validation (20%) and testing sets (20%).
SML is a monitoring dataset for the temPerature forecasting, collected from a monitor system in a
domotic house for aPProximately 40 days. The data are samPled every minute and smoothed with
the mean of every 15 minutes, forming the MTS of length 4,137. We Predict the dinning-room
temPerature with 17 relevant variable series. The first 3,200, the following 400 and the last 537 data
Points are resPectively used for training, validation, and test.
We comPare with linear Granger causality (VAR) and the state-of-the-art Prediction models includ-
ing DUAL (Qin et al., 2017) and IMV-LSTM (Guo et al., 2019), and adoPt Root Mean Squared
Error (RMSE) and Mean Absolute Error (MAE) as metrics.
5
Under review as a conference paper at ICLR 2020
Table 1: Causal StrUctUre reconstruction results on Finance and FMRI data.
Methods	Finance (9 datasets)		FMRI (5 datasets)	
	PR-AUC	ROC-AUC	PR-AUC	ROC-AUC
IMV-LSTM	0.778±0.222	0.862±0.172	0.593±0.239	0.620±0.136
linear Granger	0.187±0.036	0.652±0.084	0.492±0.310	0.654±0.126
TCDF	0.478±0.263	0.766±0.145	0.540±0.250	0.664±0.099
GASER	L000±0.000**	L000±0.000*	0.641±0.327	0.740±0.122
** denotes the P-ValUe is less than 1%, and * denotes the P-ValUe is less than 5%.
3.2	Experimental Results
3.2.1	Causal Structure Reconstruction Performance on Homogeneous
Multivariate Time Series
To eValuate the causal discoVery Performance on homogeneous multiVariate time series, we train
indiVidual models for each dataset with the hyPer-Parameter α equaling 0.5. We rePort PR-AUC
and ROC-AUC aVeraged across all datasets, with the standard deViation rePorted in Table 1. As can
be seen, the ProPosed method greatly surPasses other methods. EsPecially, GASER recoVers the
ground-truth causal structure with high score on the Finance data.
3.2.2	Causal Structure Reconstruction Performance on Heterogeneous
Multivariate Time Series
In this Part, we eValuate the causal discoVery Performance on heterogeneous multiVariate time series.
We denote the number of common causal structures as C, the number of Variables as S and the series
length as T, and generate 100 multiVariate time series for each common causal structure according
to Equation (14) and Equation (15), forming 100C datasets. For the inductiVe methods GASER
and IMV-LSTM, we train one model using all the datasets, while for other methods, we train sePa-
rate models for each dataset. We rePort PR-AUC and ROC-AUC results w.r.t the Variable number,
the series length and the common structure number in Table 2 to 4, resPectiVely. We obserVe that
GASER outPerforms other methods significantly in all cases, and GASER (α = 0.5) (with the Pro-
totyPical Granger causal attention) Performs better than GASER (α = 1) (only with Granger causal
attention). The obserVations demonstrate the suPerior causal discoVery Performance of GASER, the
effectiVeness of the PrototyPical Granger causal attention in GASER, and the adVantages of utilizing
shared commonalities among heterogeneous MTS. Regarding the other comPetitors, linear Granger
Performs the best followed by TCDF and IMV-LSTM at most cases. The Possible reason is that
linear Granger can detect Granger causal relations to some extent, though it utilizes linear model,
i.e., Vector autoregression (VAR). TCDF utilizes attention-based CNN to inference Potential causals
followed by a causal Validation steP, but the attention it ProPosed cannot reflect Granger causality,
thus achieVes unsatisfactory Performance. ComPare to the Performance on homogeneous multiVari-
ate time series, the Performance of IMV-LSTM droPs dramatically, which indicates that the attention
mechanism in IMV-LSTM fails giVen heterogeneous multiVariate time series.
In Table 2, we Vary the number of Variables to generate datasets of different comPlexity, and we can
see that GASER outPerforms other comPetitors consistently across different S, and achieVes good
Performance when S is as large as 20, demonstrating our method’s caPability to infer comPlex causal
structures. Since in Practice, the size of collected data is often limited, which Poses challenges to
recoVer causal structure, thus we also Vary the length of time series to see the model robustness to
data of small sizes. As can be seen in Table 3, GASER outPerforms other methods across all cases,
eVen when T is as small as 20, which demonstrates that adVantage of using shared information. We
also obserVe that GASER (α = 0.5) surPasses GASER (α = 1) by a large margin, which demon-
Table 2: CaUSal StrUCtUre reconstruction results w.r.t the Variable number (C=3, T =1000).
Methods	S=5		S=10		S=20	
	PR-AUC	ROC-AUC	PR-AUC	ROC-AUC	PR-AUC	ROC-AUC
IMV-LSTM	0.511±0.102	0.500±0.236	0.536±0.056	0.514±0.019	0.599±0.087	0.619±0.087
linear Granger	0.666±0.107	0.822±0.075	0.765±0.109	0.889±0.063	0.826±0.106	0.854±0.080
TCDF	0.523±0.103	0.523±0.214	0.548±0.165	0.587±0.180	0.584±0.162	0.642±0.152
GASER (α = 1)	0.886±0.177**	0.906±0.143**	0.974±0.038**	0.975±0.037**	0.830±0.108	0.883±0.069**
GASER (α = 0.5)	0.911±0.147**	0.922±0.122**	0.998±0.009**	0.999±0.008**	0∙858±0.103*	0.939±0.050**
** denotes the p-value is less than 1%, and * denotes the p-value is less than 5%.
6
Under review as a conference paper at ICLR 2020
Table 3: Causal structure reconstruction results w.r.t the series length (C=3, S=10).
Methods	T=20		T =100		T =1000	
	PR-AUC	ROC-AUC	PR-AUC	ROC-AUC	PR-AUC	ROC-AUC
IMV-LSTM	0.467±0.025	0.541±0.035	0.503±0.081	0.511±0.018	0.536±0.056	0.514±0.019
linear Granger	0.400±0.000	0.500±0.000	0.889±0.152	0.943±0.085	0.765±0.109	0.889±0.063
TCDF	0.518±0.131	0.513±0.112	0.517±0.120	0.544±0.166	0.548±0.165	0.587±0.180
GASER (α = 1)	0.790±0.142**	0.793±0.150**	0.973±0.038**	0.974±0.038*	0.974±0.038**	0.975±0.037**
GASER (α = 0.5)	0∙824±0.123**	0∙833±0.117**	0.973±0.040**	0.976±0.036**	0.998±0.009**	0.999±0.008**
JT	denotes the p-value is less than 1%, and * denotes the p-value is less than 5%.					
Table 4: Causal structure reconstruction results w.r.t. the common structure number C (S=10,
T =1000). We set the hyper-parameter of prototype number K in the model as the same as the
ground-truth common structure number C.
Methods		C=3		C=5		C=7	
		PR-AUC	ROC-AUC	PR-AUC	ROC-AUC	PR-AUC	ROC-AUC
GASER(α =	1)	0.974±0.038	0.975±0.037	0.891±0.118	0.883±0.128	0.838±0.118	0.850±0.113
GASER(α =	0.5)	0.998±0.009	0.999±0.008	0.924±0.091	0.913±0.105	0.851±0.113	0.855±0.099
strates that learning prototypical attention can alleviate the over-fitting problem. In Table 4, we
control the causal heterogeneity by varing the number of common causal structures C = {3, 5, 7}.
We observe that the performance of GASER decreases with increasing C. In Figure 2, we map the
learned causal attention vectors to a 2D space by the visualization tool t-SNE (Maaten & Hinton,
2008). Individuals of different causal structures are labeled by different colors. From the results,
we observe that nodes belonging to the same causal structures are clustered together, which also
demonstrates the effectiveness of our method.
3.2.3	Prediction Performance
We evaluate the prediction performance on the real-world datasets, i.e., PM2.5 and SML. To eval-
uate the robustness of prediction and the accuracy of Granger causal attribution, we also build an-
other datasets that only contain top 50% important variables towards prediction detected from each
method. We report the prediction results in Table 5. As can be seen, GASER achieves the best
performance on PM2.5 data with all features, demonstrating its superior prediction performances.
We also observe that GASER achieves comparable or even better performance using selected vari-
ables, while the others’ performances decrease, which indicates that effective variable selection of
GASER. Linear Granger achieves the best performance on SML data, because the time series length
of SML is short, thus providing limited training samples for neural-network-based methods.
3.3	Online A/B Tests
In order to further evaluate the effectiveness of GASER in practice, an online A/B test is conducted
on an E-commercial platform, and the process is designed as follows:
•	We first train GASER on the historical MTS of 30,665 items. Each MTS includes 26 variables
related to searching, recommending and advertising, such as Page View (PV), Gross Merchandise
Volume (GMV) and Impression Position In-Page, etc. Here, we take the item popularity as the
target series, and generate the underlying causal structure for each item.
•	We randomly sample 100 items whose impression position in-page Granger-causes the item pop-
ularity with high confidence, and divide them into two buckets. For Bucket A, we adjust impres-
sion positions in-page of each item by one grid since 2019/08/19 till 2019/08/29, and ensure the
intervention has little impact on other variables. For Bucket B, we do nothing.
•	We compare the improvement rate of item popularity week-on-week on the two bucket in Fig 3.
Table 5: Predition results under all variables and top 50% important variables on the PM2.5 and
SML datasets.________________________________________________________________________________
Methods	PM2.5 (all)		PM2.5 (top 50%)		SML (all)		SML (top 50%)	
	RMSE	MAE	RMSE	MAE	RMSE	MAE	RMSE	MAE
linear Granger	21.38	11.82	21.39	11.83	0.0187	0.0144	0.0190	0.0151
DUAL	20.93	11.70	20.90	11.90	0.0778	0.0676	0.0887	0.0752
IMV-LSTM	21.60	11.77	21.62	11.80	0.0747	0.0561	0.1180	0.0857
GASER	18.88	11.03	18.22	10.62	0.0670	0.0540	0.0621	0.0492
7
Under review as a conference paper at ICLR 2020
Figure 2: 2D t-SNE projections of
attention vectors from 500 individ-
uals. Color of a node indicates the
underlying causal structures. The
causal groundtruth ωi is shown in
the legend (S=10,T=100).
-*-Bucket A .Bucket B
8/18 8/19 8/20 8/21 8/22 8/23 8/24 8/25 8/26 8/27 8/28 8/29
Time
Figure 3: The result of online A/B test. The interven-
tion starts from 08/19, and results in the item popularity
improvement rate of Bucket A consistently outperforming
Bucket B after 8/22, and the gap between the two buckets
increases significantly since 08/25.
As shown in Figure 3, four days after the beginning of the intervention, the item popularity im-
provement rate of Bucket A consistently outperforms that of Bucket B, and the gap between the two
buckets increases significantly since 2019/08/25, which shows that the intervention, i.e., adjusting
the impression positions in-page, causes the improvement on item popularities, thus demonstrates
the right causal relationships detected by GASER.
4	Related Work
Recently a considerable amount of work has been proposed for causal inference. Classical meth-
ods, such as constraint-based methods (Pearl, 2000; Spirtes et al., 2000; Peters et al., 2013; Runge
et al., 2017; Zhang et al., 2017), score-based methods (Chickering, 2002) and functional causal
models (FCM) based methods (Shimizu et al., 2006), mainly focus on i.i.d data. Under the scope of
time series, causal inference is commonly based on the notion of Granger causality (Granger, 1969;
1980), and a classical way is to estimate linear Granger causality under the framework of VAR
models (LutkePohL 2005). However, existing classicial methods fail to uncover causal structures
inductively. Neural network based methods that infer causal relationships or relations that approach
causality have gained increasing PoPularity. LoPez-Paz et al. (2015) learns a Probability distribution
classifier to unveil causal relations. KiPf et al. (2018) ProPoses a neural relation inference model
to infer interactions while simultaneously learning the dynamics. Yu et al. (2019) develoPs a deeP
generative model to recover the underlying DAG from comPlex data. Attention mechanism has
often been adoPted to discover relations between variables. For examPle, Dang et al. (2018) discov-
ers dynamic dePendencies with multi-level attention. Nauta et al. (2019) studies causal discovery
through attention-based neural networks with a causal validation steP. Guo et al. (2019) ProPoses an
interPretable multi-variable LSTM with mixture attention to extract variable imPortance knowledge.
However, these attention mechanisms Provide no incentive to yield accurate attributions (Sundarara-
jan et al., 2017; Schwab et al., 2019).
Since our method utilizes the concePt of PrototyPe to detect common causal structures, another line
of related research is about PrototyPe learning. PrototyPe learning is a form of cased-based reason-
ing (Slade, 1991), which solves Problems for new inPuts based on similarity to PrototyPical cases.
Recently PrototyPe learning has been leveraged in interPretable classification (Bien et al., 2011; Kim
et al., 2014; Snell et al., 2017; Li et al., 2018; Chen et al., 2018) and sequence learning (Ming et al.,
2019). We incorPorate the concePt for Granger causal structure reconstruction on time series data
for the first time.
5	Conclusion
We formalize the Problem of Granger causal structure reconstruction from heterogeneous MTS data
and ProPose an inductive framework GASER to solve it. In Particular, we ProPose a novel at-
tention mechanism, namely PrototyPical Granger causal attention, which comPutes Granger causal
attribution combined with PrototyPe learning, to reconstruct Granger causal structures and uncover
8
Under review as a conference paper at ICLR 2020
common causal structures. The approach has been successfully evaluated by offline experiments on
real-world and synthetic datasets compared to previous methods, also confirmed by an online A/B
test on an E-commercial platform.
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine
learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
Amir F Atiya and Alexander G Parlos. New results on recurrent network training: unifying the
algorithms and accelerating convergence. IEEE transactions on neural networks, 11(3):697-709,
2000.
Jacob Bien, Robert Tibshirani, et al. Prototype selection for interpretable classification. The Annals
of Applied Statistics, 5(4):2403-2424, 2011.
Chaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su, and Cynthia Rudin.
This looks like that: deep learning for interpretable image recognition. arXiv preprint
arXiv:1806.10574, 2018.
David Maxwell Chickering. Optimal structure identification with greedy search. Journal of machine
learning research, 3(Nov):507-554, 2002.
KyUnghyUn Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
Xuan-Hong Dang, Syed Yousaf Shah, and Petros Zerfos. seq2graph: Discovering dynamic depen-
dencies from multivariate time series with multi-level attention. arXiv preprint arXiv:1812.04448,
2018.
Michael Eichler. Causal inference with multiple time series: principles and problems. Philosophi-
cal Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 371
(1997):20110613, 2013.
Tom Fawcett. An introduction to roc analysis. Pattern recognition letters, 27(8):861-874, 2006.
Clive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods.
Econometrica: Journal of the Econometric Society, pp. 424-438, 1969.
Clive WJ Granger. Testing for causality: a personal viewpoint. Journal of Economic Dynamics and
control, 2:329-352, 1980.
Ruocheng Guo, Lu Cheng, Jundong Li, P Richard Hahn, and Huan Liu. A survey of learning
causality with data: Problems and methods. arXiv preprint arXiv:1809.09337, 2018.
Tian Guo, Tao Lin, and Nino Antulov-Fantulin. Exploring interpretable lstm neural networks over
multi-variable data. In International Conference on Machine Learning, pp. 2494-2504, 2019.
SePP Hochreiter and JUrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Biwei Huang, Kun Zhang, Jiji Zhang, Joseph Ramsey, Bernhard Scholkopf, and Clark Glymour.
Causal discovery and hidden driving force estimation from nonstationary/heterogeneous data.
arXiv preprint arXiv:1903.01672, 2019.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016.
Been Kim, Cynthia Rudin, and Julie Shah. The bayesian case model: A generative approach for
case-based reasoning and prototype classification. In Proceedings of Neural Information Process-
ing Systems (NIPS), 2014.
9
Under review as a conference paper at ICLR 2020
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational
inference for interacting systems. In International Conference on Machine Learning, pp. 2693-
2702, 2018.
Samantha Kleinberg. Causality, probability, and time. In Causality, probability, and time. Cam-
bridge University Press, 2009.
Oscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin. Deep learning for case-based reasoning
through prototypes: A neural network that explains its predictions. In Proceedings of AAAI,
2018.
David Lopez-Paz, Krikamol Muandet, Bernhard Scholkopf, and Iliya Tolstikhin. Towards a learning
theory of cause-effect inference. In International Conference on Machine Learning, pp. 1452-
1461, 2015.
Helmut LutkePohL New introduction to multiple time Series analysis. Springer Science & Business
Media, 2005.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(Nov):2579-2605, 2008.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. In International Conference on Learning Representations
(ICLR), 2017.
Yao Ming, Panpan Xu, Huamin Qu, and Liu Ren. Interpretable and steerable sequence learning via
prototypes. In KDD, 2019.
Meike Nauta, Doina Bucur, and Christin Seifert. Causal discovery with attention-based convolu-
tional neural networks. Machine Learning and Knowledge Extraction, 1(1):312-340, 2019.
Judea Pearl. Causality: models, reasoning and inference, volume 29. Springer, 2000.
Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Causal inference on time series using
restricted structural equation models. In Advances in Neural Information Processing Systems, pp.
154-162, 2013.
Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Elements ofcausal inference: foundations
and learning algorithms. MIT press, 2017.
Yao Qin, Dongjin Song, Haifeng Cheng, Wei Cheng, Guofei Jiang, and Garrison W Cottrell. A
dual-stage attention-based recurrent neural network for time series prediction. In Proceedings of
the 26th International Joint Conference on Artificial Intelligence, pp. 2627-2633. AAAI Press,
2017.
Jakob Runge, Peer Nowack, Marlene Kretschmer, Seth Flaxman, and Dino Sejdinovic. Detecting
causal associations in large nonlinear time series datasets. arXiv preprint arXiv:1702.07007,
2017.
Patrick Schwab, Djordje Miladinovic, and Walter Karlen. Granger-causal attentive mixtures of ex-
perts: Learning important features with neural networks. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 33, pp. 4846-4853, 2019.
Skipper Seabold and Josef Perktold. Statsmodels: Econometric and statistical modeling with python.
In 9th Python in Science Conference, 2010.
Shohei Shimizu, Patrik O Hoyer, Aapo Hyvarinen, and Antti Kerminen. A linear non-gaussian
acyclic model for causal discovery. Journal of Machine Learning Research, 7(Oct):2003-2030,
2006.
Stephen Slade. Case-based reasoning: A research paradigm. AI magazine, 12(1):42-42, 1991.
10
Under review as a conference paper at ICLR 2020
Stephen M Smith, Karla L Miller, Gholamreza Salimi-Khorshidi, Matthew Webster, Christian F
Beckmann, Thomas E Nichols, Joseph D Ramsey, and Mark W Woolrich. Network modelling
methods for fmri. Neuroimage, 54(2):875-891, 2011.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems, pp. 4077-4087, 2017.
Peter Spirtes, Clark N Glymour, Richard Scheines, David Heckerman, Christopher Meek, Gregory
Cooper, and Thomas Richardson. Causation, prediction, and search. MIT press, 2000.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3319-
3328. JMLR. org, 2017.
Yue Yu, Jie Chen, Tian Gao, and Mo Yu. Dag-gnn: Dag structure learning with graph neural
networks. In International Conference on Machine Learning, pp. 7154-7163, 2019.
KUn Zhang, BiWei Huang, Jiji Zhang, Clark Glymour, and Bernhard SchOlkopf. Causal discovery
from nonstationary/heterogeneous data: Skeleton estimation and orientation determination. In
IJCAI: Proceedings of the Conference, volume 2017, pp. 1347. NIH Public Access, 2017.
11
Under review as a conference paper at ICLR 2020
A Table of Notations
Table 6: TermS and NotationS
Symbol	Definition
N	the number of individuals
S	the number of variables
T	the length of time series
K	the number of prototypes
C	the number of underlying common causal structures
X	heterogenous multivariate time series
Xi	multivariate time series of individual i
xis	the s-th time series of individual i
yi	the target variable series of individual i
yi,T +1	the T + 1-step value of the target variable
yi,T +1	the predicted T + 1-step value of the target variable by the main predictor
s yi,T +1	the predicted T + 1-step value of the target variable conditioned on variable S
pk	the k-th prototype
ai	the prototypical Granger causal attention vector of individual i
qi	the Granger causal attention vector of individual i
ri	the prototypical attention vector of individual i
α	the hyperparameter to control the ratio of different attentions
λ1 , λ2	the hyperparameter to control the ratio of loss functions
τ	the softmax temperature
B Algorithm Pseudocode
The full algorithm iS preSented in Algorithm 1. The network parameter Set Θ includeS the param-
eterS of Sequence encoderS and MLPS. We adopt StochaStic gradient deScent (SGD) to optimize
the network parameterS and the prototype parameterS. To initialize the prototypeS, we firSt pretrain
GASER for Several epochS, and then employ k-meanS with coSine Similarity on the Granger cauSal
attentionS {q}iN=1, and finally we take the cluSter centerS aS the initial prototypeS. Note that the
Gumbel-Softmax function iS only adopted in the training phaSe to backpropagate, and replaced by
the argmax function in inference.
Algorithm 1: The algorithm of GASER.
Input:
Input data X = {Xi}iN=1; Number of prototypeS K; Maximum iterationS MaxIter;
HyperparameterS α, γ, λ1 and λ2 .
Output:
Network parameterS Θ; Attention vectorS {ai}iN=1; PrototypeS {p}jK=1; Prediction reSultS
{yi,T +1}N=1.
1:	Pretrain the model by optimizing Lpred + λ1 Laux;
2:	Employ k-meanS on all Granger cauSal attention vectorS {qi}iN to get initial prototypeS
{pj}jK=1
3:	for iter — 1 to MaxIter do
4:	Update network parameterS Θ and prototypeS {p}jK=1 by optimizing
L = Lpred + λ1Laux + λ2Ldiv;
5:	end for
6:	Generate prototypical Granger cauSal attentionS {ai}iN=1 by Equation (9) uSing the argmax
function inStead of the Gumbel-Softmax function.
12
Under review as a conference paper at ICLR 2020
C Additional Details on the Experimental Setup
C.1 Datasets
In this section, we provide some additional dataset details. Finance data are available at http:
//www.skleinberg.org/data.html. We use the processed FMRI data provide by (Nauta
et al., 2019). The source and details of PM2.5 and SML are at https://archive.ics.uci.
edu/ml/datasets/Beijing+PM2.5+Data and https://archive.ics.uci.edu/
ml/datasets/SML2010, respectively.
C.2 Implementation Details
We implement GASER in Tensorflow (Abadi et al., 2016) by the Adam optimizor (Kingma & Ba,
2014) with the learning rate set to 0.001. We adopt LSTMs as the sequence encoders with the hidden
states size set to 128 and the window size set to 5. In all experiments, we first pretrain GASER with
only the Granger causal attention for 40 epochs. The hyperparameters λ1 and λ2 are both set to 1,
and the softmax temperature in Gumbel-softmax is set to 0.1.
C.3 Compared Methods
Linear Granger (Granger, 1969; 1980): We conduct a Granger causality test in the context of Vec-
tor Autoregression (VAR) as described in chapter 7.6.3 in (LutkePohL 2005) and implemented by
the Statsmodels package (Seabold & Perktold, 2010). In detail, we perform F-test at 5% significance
level. The maximum number of lags to check for order selection is set to 5, which is larger than the
causal order in the ground-truth.
TCDF (Nauta et al., 2019): TCDF learns causal structure on multivariate time series by attention-
based convolutional neural networks combined with a causal validation step. The codes are available
at https://github.com/M-Nauta/TCDF. In all experiments, we follow the default settings
as described in (Nauta et al., 2019), i.e., the significance number (stating when an increase in loss
is significant enough to label a potential cause as true) as 0.8, the size of kernels as 4, dilation
coefficient as 4, the learning rate as 0.01, and adopting Adam optimizator.
DUAL (Qin et al., 2017): It is an encoder-decoder RNN with an input attention mechanism, which
forces the model pay more attention on certain driving series rather than treating all the input driving
series equally. In the experiment, we use the input-attention factors to detect important variables as
(Guo et al., 2019) did. We set the the size of hidden states for encoder and decoder to 64 and the
window size to 10 as stated in the paper.
IMV-LSTM (Guo et al., 2019): It is a multi-variable attention-based LSTM capable of both pre-
diction and variable importance interpretation, with the attention factors reflecting importance of
variables in prediction. Thus, we take the learnt attention vectors as the Granger causal weights
in the experiment. The codes are available at https://github.com/KurochkinAlexey/
IMV_LSTM. In all experiments, IMV-LSTM is implemented by Adam optimizer with the mini-
batch size 64, hidden layer size 128 and learning rate 0.001.
D Supplementary Experimental Results
D. 1 Visualization of Learned Prototypes
We visualize the the learned prototypes and the ground-truth causal structures in Fig. 4. In this
experiments, we set the hyper-parameter of prototype number K equal to the ground-truth common
structure number C . From the results, we can see that the learned prototypes are similar to the
ground-truth causal structures, which demonstrates the learned prototypes are interpretable.
D.2 Visualization of Granger Causal Attention
In this part, we visualize the Granger causal attention vectors over epochs during the training phase
and the ground-truth causal structures in Fig. 5. From the results, we can see that for the later epochs,
13
Under review as a conference paper at ICLR 2020
xθ
xl
x2
x3
x4
x5
x6
x7
xβ
x9
1.0
0.8
0.6
0.4
0.2
0.0
(a) C=3	(b) C=5
Figure 4: The visualization of prototypes and ground-truth causal structures. Plots are shown for
various number of common causal structures C (S=10, T =1000). Each column of the heat map
visualizes one structure.
Figure 5: The visualization of Granger causal attention vectors over epochs during the training phase
and the ground-truth causal structures. Each column of the heat map visualizes one structure (C=5,
S=10,T=1000).
shown on the right of the figure, the Granger causal attention vectors are similar to the ground-
truth structures. It demonstrates that the proposed Granger causal attention mechanism forces the
attention values to correlate with Granger causality.
D.3 Running Time
We compare the running time including the training time and the inference time with linear Granger
method, i.e. VAR. The experiments were carried out on a server with 64 Intel(R) Xeon(R) E5-2682
v4 2.50GHz processors and 512 GB RAM. Since the linear Granger method is not inductive and
has to retrain for new individuals, we set the inference time equal to the training time. As shown in
Table 7, although GASER takes more time on the training phase, about 6.4 times slower than linear
Granger, it is about 44.5 times faster on the inference phase.
Table 7: The comparison of running time on heterogeneous multivariate time series of 1,000 indi-
viduals (S=10, T =100, C =10).____________________________________
Training (s) Inference (s)
linear Granger	19.59	19.59
GASER	124.53	0.44
14
Under review as a conference paper at ICLR 2020
0.84
0.82
0.80
0.78
0.76
——PR-AUC
ROC-AUC
10
15
20
(b)
α
0.84
0.82
0.80
0.78
0.76
(c) γ
PR-AUC
ROC-AUC
(a) K
(d) λ1
(e) λ2
3	5

Figure 6: Performance results w.r.t. different numbers of prototypes K and different values of the
hyper-parameter α, λ1, λ2 and γ.
D.4 Parameter Sensitivity
We investigate the parameter sensitivity in this section. Specifically, we evaluate the sensitivity of
GASER to different numbers of prototypes K and different values of hyper-parameter α,γ, λ1 and
λ2. The results on heterogenous MTS (C=5, S=10, T =100) are shown in Fig. 6.
We first show how the number of prototypes affects the performance in Fig. 6(a). We can see that
the performance raises when K increases and achieves the best performance when the number of
prototypes K is equal to the number of underlying common structures C. Then, the performance
starts to drop slowly. Overall, the proposed method is not very sensitive to the parameter K .
Then we evaluate how the value of α affects the performance in Fig. 6(b). The parameter α balances
the weight of the Granger causal attention and the prototypical attention in our model. We can see
that when α = 0.5, the model reaches the best performance, which demonstrates both the Granger
causal attention and the prototypical attention are essential in our model, and it is important to find
a good balanced point between them.
The parameter γ controls the closeness of different prototypes. The smaller the γ, the prototypes are
more diverse. The result is shown in Fig. 6(c). We can see that when γ is too small, the result is not
good. This is intuitive as the ground-truth common structures among heterogeneous MTS are not
totally different. Too large γ also deteriorates the performance, because it will result in a number of
similar or even duplicate prototypes. Thus, it is important to determine the parameter γ carefully.
Figure 6(d) shows the the performance of GASER w.r.t. the parameter λ1. The parameter λ1 is
the weight of the auxiliary prediction loss. From the result, we can see that the performance raises
then λ1 increases initially, and then keeps stable. It demonstrates that the auxiliary predictors are
essential to detect the Granger causality.
Finally, we show how the value of λ2 affects the performance in Fig. 6(e). The parameter λ1 controls
the weight of the prototype diversity regularization. From the result, we can see that the prototype
diversity loss is essential in our mobel, but we should concentrate more on choosing an appropriate
weight.
15