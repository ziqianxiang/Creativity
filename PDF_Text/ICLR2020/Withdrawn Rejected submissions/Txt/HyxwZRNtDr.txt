Under review as a conference paper at ICLR 2020
Wasserstein Robust Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Reinforcement learning algorithms, though successful, tend to over-fit to train-
ing environments, thereby hampering their application to the real-world. This
paper proposes WR2L - a robust reinforcement learning algorithm with Signifi-
cant robust performance on low and high-dimensional control tasks. Our method
formalises robust reinforcement learning as a max-min game with a constraint
derived from Wasserstein distance, and we propose an efficient and scalable al-
gorithm to solve it. Our algorithm applies zero-order optimisation method that
we believe can be useful to numerical optimisation in general. Apart from the
formulation, we also propose an efficient and scalable solver following a novel
zero-order optimisation method that we believe can be useful to numerical opti-
misation in general. We empirically demonstrate significant gains compared to
standard and robust state-of-the-art algorithms on high-dimensional MuJuCo en-
vironments.
1	Introduction
Reinforcement learning (RL) has become a standard tool for solving decision-making problems
with feedback, and though significant progress has been made, algorithms often over-fit to train-
ing environments and fail to generalise across even slight variations of transition dynamics (Packer
et al., 2018; Zhao et al., 2019). Robustness to changes in transition dynamics is a crucial component
for adaptive and safe RL in real-world environments. Motivated by real-world applications, recent
literature has focused on the above problems, proposing a plethora of algorithms for robust decision-
making (Morimoto & Doya, 2005; Pinto et al., 2017; Tessler et al., 2019). Most of these techniques
borrow from game theory to analyse, typically in a discrete state and actions spaces, worst-case de-
viations of agents’ policies and/or environments, see Sargent & Hansen (2001); Nilim & El Ghaoui
(2005); Iyengar (2005); Namkoong & Duchi (2016) and references therein. These methods have also
been extended to linear function approximators (Chow et al., 2015), and deep neural networks (Peng
et al., 2017) showing (modest) improvements in performance gain across a variety of disturbances,
e.g., action uncertainties, or dynamical model variations. In this paper, we propose a generic frame-
work for robust reinforcement learning that can cope with both discrete and continuous state and
actions spaces. Our algorithm, termed Wasserstein Robust Reinforcement Learning (WR2L), aims
to find the best policy, where any given policy is judged by the worst-case dynamics amongst all
candidate dynamics in a certain set. This set is essentially the average Wasserstein ball around a ref-
erence dynamics P0. The constraints makes the problem well-defined, as searching over arbitrary
dynamics can only result in non-performing system. The measure of performance is the standard
RL objective, the expected return. Both the policy and the dynamics are parameterised; the policy
parameters θk may be the weights of a deep neural network, and the dynamics parameters φj the
parameters of a simulator or differential equation solver. The algorithm performs estimated descent
steps in φ space and - after (almost) convergence - performs an update of policy parameters, i.e., in
θ space. Since φj may be high-dimensional, we adapt a zero’th order sampling method based ex-
tending Salimans et al. (2017) to make estimations of gradients, and in order to define the constraint
set which φj is bounded by, we generalise the technique to estimate Hessians (Proposition 2). We
emphasise that although access to a simulator with parameterisable dynamics is required, the actual
reference dynamics P0 need not be known explicitly nor learnt by our algorithm. Put another way,
we are in the “RL setting”, not the “MDP setting” where the transition probability matrix is known
a priori. The difference is made obvious, for example, in the fact that we cannot perform dynamic
programming, and the determination ofa particular probability transition can only be estimated from
sampling, not retrieved explicitly. Hence, our algorithm is not model-based in the traditional sense
of learning a model to perform planning.
1
Under review as a conference paper at ICLR 2020
We believe our contribution is useful and novel for two main reasons. Firstly, our framing of the
robust learning problem is in terms of dynamics uncertainty sets defined by Wasserstein distance.
Whilst we are not the first to introduce the Wasserstein distance into the context of MDPs (see, e.g.,
Yang (2017) or Lecarpentier & Rachelson (2019)), we believe our formulation is amongst the first
suitable for application to the demanding application-space we desire, that being, high-dimensional,
continuous state and action spaces. Secondly, we believe our solution approach is both novel and
effective (as evidenced by experiments below, see Section 5), and does not place a great demand
on model or domain knowledge, merely access to a simulator or differentiable equation solver that
allows for the parameterisation of dynamics. Furthermore, it is not computationally demanding, in
particular, because it does not attempt to build a model of the dynamics, and operations involving
matrices are efficiently executable using the Jacobian-vector product facility of automatic differen-
tiation engines.
2	Background
A Markov decision process (MDP)1 is denoted by M = hS, A, P, R, γi, where S ⊆ Rd denotes
the state space, A ⊆ Rn the action space, P : S × A × S → [0, 1] is a state transition probability
describing the system’s dynamics, R : S × A → R is the reward function measuring the agent’s
performance, and γ ∈ [0, 1) specifies the degree to which rewards are discounted over time.
At each time step t, the agent is in state st ∈ S and must choose an action at ∈ A, transitioning it to
anew state st+ι 〜 P (st+ι∣st, aj and yielding a reward R(st, at). A policy ∏ : SX A → [0,1] is
defined as a probability distribution over state-action pairs, where ∏(at |st) represents the density of
selecting action at in state st. Upon subsequent interactions with the environment, the agent collects
a trajectory τ of state-action pairs. The goal is to determine an optimal policy π? by solving:
∏? = arg max ET〜pπ (T) [RTotal(τ)],	(1)
wherepπ(τ) denotes the trajectory density function, and RTotal(τ) the return, that is, the total accu-
mulated reward:
T-1	T-1
p∏(τ) = μo(so)π(a0∣S0) ɪɪ P(st+ι∣St, at)∏(at∣St) and RTOtaI(T) = E YtR(st, at), (2)
t=1	t=0
with μo(∙) denoting the initial state distribution.
We make use of the Wasserstein distance to quantify variations from a reference transition density
Po(∙). The latter being a probability distribution, one may consider other divergences, such as
Kullback-Leibler (KL) or total variation (TV). Our main intuition for choosing Wasserstein distance
is explained below, but we note that it has a number of desirable properties: Firstly, it is symmetric
(Wp(μ, ν) = Wp(ν,μ), a property K-L lacks). Secondly, it is well-defined for measures with
different supports (which K-L also lacks). Indeed, the Wasserstein distance is flexible in the forms
of the measures that can be compared - discrete, continuous or a mixture. Finally, it takes into
account the underlying geometry of the space the distributions are defined on, which can encode
valuable information. It is defined as follows: Let X be a metric space with metric d(∙, ∙). Let C(X)
be the space of continuous functions on X and let M(X) be the set of probability measures on X.
Let μ,ν ∈ M(X). Let K(μ, V) be the set of couplings between μ, V:
K(μ, V) := {κ ∈ M(X × X); ∀(A, B) ⊂X ×X, K(A × X) = μ(A), K(X × B) = V(B)} (3)
That is, the set of joint distributions K ∈ M(X × X) whose marginals agree with μ and V re-
spectively. Given a metric (serving as a cost function) d(∙, ∙) for X, the p,th Wasserstein distance
Wp(μ, v) for P ≥ 1 between μ and V is defined as:
%(〃,"):= Qmin,"× d(x, y)pdK(x, y)	(4)
(in this paper, and mostly for computational convenience, we use p = 2, though other values of p
are applicable).
1Please note that we present reinforcement learning with continuous states and actions. This allows us
to easily draw similarities to optimal control as detailed later. Extending these notions to discrete settings is
relatively straight-forward.
2
Under review as a conference paper at ICLR 2020
3	Wasserstein Robust Reinforcement Learning
The desirable properties of Wasserstein distance aside, our main intuition for choosing itis described
thus: Per the definition, constraining the possible dynamics to be within an -Wasserstein ball of a
reference dynamics Po(∙) means constraining it in a certain way. Wasserstein distance has the form
mass × distance. If this quantity is constrained to be less than a constant , then if the mass is large,
the distance is small, and if the distance is large, the mass is small. Intuitively, when modelling
the dynamics of a system, it may be reasonable to concede that there could be a systemic error -
or bias - in the model, but that bias should not be too large. It is also reasonable to suppose that
occasionally, the behaviour of the system may be wildly different to the model, but that this should
be a low-probability event. If the model is frequently wrong by a large amount, then there is no use
in it. In a sense, the Wasserstein ball formalises these assumptions.
3.1	Problem Definition: Robust Objectives and Constraints
Due to the continuous nature of the state and action spaces considered in this work, we resort to
deep neural networks to parameterise policies, which we write as ∏θ(a∕st), where θ ∈ Rd1 is a
set of tunable hyper-parameters to optimise. For instance, these policies can correspond to multi-
layer perceptrons for MuJoCo environments, or to convolutional neural networks in case of high-
dimensional states depicted as images. Exact policy details are ultimately application dependent
and, consequently, provided in the relevant experiment sections.
In principle, one can similarly parameterise dynamics models using deep networks (e.g., LSTM-
type models) to provide one or more action-conditioned future state predictions. Though appealing,
going down this path led us to agents that discover worst-case transition models which minimise
training data but lack any valid physical meaning. For instance, original experiments we conducted
on CartPole ended up involving transitions that alter angles without any change in angular veloci-
ties. More importantly, these effects became more apparent in high-dimensional settings where the
number of potential minimisers increases significantly. It is worth noting that we are not the first to
realise such an artifact when attempting to model physics-based dynamics using deep networks. Au-
thors in (Lutter et al., 2019) remedy these problems by introducing Lagrangian mechanics to deep
networks, while others (Koller et al., 2018; Chen et al., 2018) argue the need to model dynamics
given by differential equation structures directly.
Though incorporating physics-based priors to deep networks is an important and challenging task
that holds the promise of scaling model-based reinforcement learning for efficient solvers, in this
paper we rather study an alternative direction focusing on perturbing differential equation solvers
and/or simulators with respect to the dynamic specification parameters φ ∈ Rd2 . Not only would
such a consideration reduce the dimension of parameter spaces representing transition models, but
would also guarantee valid dynamics due to the nature of the simulator. Though tackling some of the
above problems, such a direction arrives with a new set of challenges related to computing gradients
and Hessians of black-box solvers. In Section 4, we develop an efficient and scalable zero-order
method for valid and accurate model updates.
Unconstrained Loss Function: Having equipped agents with the capability of representing poli-
cies and perturbing transition models, we are now ready to present an unconstrained version of
WR2L’s loss function. Borrowing from robust optimal control, we define robust reinforcement
learning as an algorithm that learns best-case policies under worst-case transitions:
max mφn ET〜pφ(τ) [Rtotal(T)] ,	(5)
where pθφ(τ) is a trajectory density function parameterised by both policies and transition models,
i.e., θ and φ, respectively:
T-1
Pφ(τ) = μ0(s0)π(a0∣s0) TT	Pφ(st+ι∣st, at)	∏θ(at∣st).
t=ι 、 {z ' 、 {z '
specs vector and diff. solver deep network
3
Under review as a conference paper at ICLR 2020
At this stage, it should be clear that our formulation, though inspired from robust optimal control,
is, truthfully, more generic as it allows for parameterised classes of transition models without incor-
porating additional restrictions on the structure or the scope by which variations are executed2.
Constraints & Complete Problem Definition: Clearly, the problem in Equation 5 is ill-defined
due to the arbitrary class of parameterised transitions. To ensure well-behaved optimisation ob-
jectives, we next introduce constraints to bound search spaces and ensure convergence to feasible
transition models. For a valid constraint set, our method assumes access to samples from a reference
dynamics model Po(∙∣s, a), and bounds learnt transitions in an E-Wasserstein ball around P0(∙∣s, a),
i.e., the set defined as:
We (Pφ(∙),P0(∙)) = {Pφ(∙): w2 (Pφ(∙∣s, a), Po (∙∣s, a)) ≤ e, ∀(s, a) ∈ S ×A} ,	(6)
where E ∈ R+ is a hyperparameter used to specify the “degree of robustness” in a similar spirit
to maximum norm bounds in robust optimal control. It is worth noting, that though we have ac-
cess to samples from a reference simulator, our setting is by no means restricted to model-based
reinforcement learning in an MDP setting. That is, our algorithm operates successfully given only
traces from Po accompanied with its specification parameters, e.g., pole lengths, torso masses, etc.
一 a more flexible framework that does not require full model learners.
Though defining a better behaved optimisation objective, the set in Equation 6 introduces infinite
number of constraints when considering continuous state and/or actions spaces. To remedy this
problem, we target a relaxed version that considers a constraint of average Wasserstein distance
bounded by a hyperparameter E:
W(average)	(Pφ(∙),	Po(∙)) = {pφ(∙) : /	V(s, a)W2	(Pφ(∙∣s,	a),	Po(∙∣s,	a)) d(s,	a)	≤ J
,	(7)
{Pφ(∙) ： E(s,a) [W2 (Pφ(∙∣s, a), Po(∙∣s, a))] ≤ e}
The sampling (s, a) in the expectation is done as follows: We sample trajectories using reference
dynamics Po and a policy π that chooses actions uniformly at random (uar). Then (s, a) pairs are
sampled Uar from those collected trajectories. For a given pair (s, a), W2 (Pφ(∙∣s, a), Po(∙∣s, a))
is approximated through the empirical distribution: we use the state that followed (s, a) in the col-
lected trajectories as a data point. Estimating Wasserstein distance using empirical data is standard,
see, e.g., Peyre et al. (2019). One approach which worked well in our experiments, was to assume
that the dynamics are given by deterministic functions plus Gaussian noise with diagonal convari-
ance matrices. This makes estimation easier in high dimensions since sampling in each dimension is
independent of others, and the total samples needed is a constant factor of the number of dimensions.
Gaussian distributions also have closed-form expressions for Wasserstein distance, given in terms of
mean and covariance.
We thus arrive at WR2L’s optimisation problem allowing for best policies under worst-case yet
bounded transition models:
Wasserstein Robust Reinforcement Learning Objective:
max	mφnEτ^pφ(τ)[Rtotal(τ)]	s.t.	E(s,a)[W2	(Pφ(∙∣s,	a),	Po(∙∣s, a))]	≤ E (8)
3.2	Solution Methodology
Our solution alternates between updates of θ and φ, keeping one fixed when updating the
other. Fixing dynamics parameters φ, policy parameters θ can be updated by solving
maxθ ET〜pΦ(T)[Rtotal(T)], which is the formulation of a standard RL problem. Consequently, one
can easily adapt any policy search method for updating policies under fixed dynamical models. As
2Of course, allowed perturbations are ultimately constrained by the hypothesis space. Even then, our model
is more general compared to robust optimal control that assumes additive, multiplicative, or other forms of
disturbances.
4
Under review as a conference paper at ICLR 2020
described later in Section 4, we make use of proximal policy optimisation (Schulman et al., 2017).
When updating φ given a set of fixed policy parameters θ, the Wasserstein constraints must be re-
spected. Unfortunately, even with the simplification introduced in Section 3.1 the constraint is still
difficult to compute.
To alleviate this problem, we propose to approximate the constraint in (8) by its Taylor expansion
UP to second order. That is, defining W(φ) := E(s,a)[W2 (Pφ(∙∣s, a), Po(∙∣s, a))], the above can
be approximated around φ0 by a second-order Taylor as:
W(φ) ≈ W(Φ0) + VφW(Φo)τ (φ - Φ0) + 1 (φ - φo)τ VφW(φo)(φ - φo).
Recognising that W(φ0) = 0 (the distance between the same Probability densities), and
VφW(φ0) = 0 since φ0 minimises W (φ), we can simPlify the Hessian aPProximation by writ-
ing: W(φ) ≈ 1 (φ 一 φo)TVφW(φo)(φ — φo). Substituting our approximation back in the original
Problem in EqUation 8, we reach the following oPtimisation Problem for determining model Param-
eter given fixed policies:
mφnEτ^pφ(τ) [Rtotal(τ)] s.t. 2(φ - φo)τHo(φ - φo) ≤ e,	(9)
where Ho = Vφ E(s,a)[W2 (Pφ(∙∣s, a), Po(∙∣s, a))]	is the Hessian of the expected squared
φ=φ0
2-Wasserstein distance evaluated at φ0 . Optimisation problems with quadratic constraints can be
efficiently solved using interior-point methods. To do so, one typically approximates the loss with
a first-order expansion and determines a closed-form solution. Consider a pair of parameters θ [k]
and φ[j] (which will correspond to parameters of the j’th inner loop of the k’th outer loop in the
algorithm we present). To find φ[j+1] , we solve:
T
min VφEτ^pφ(τ) [Rtotal(τ)]	(φ — φ[j]) s.t. 2(φ — φo)THo(φ — φo) ≤ e.
φ	θ[k],φ[j]
It is easy to show that a minimiser to the above equation can derived in a closed-form as:
φ[j+1]=φ0- SjH^ H-1g[k,j],	(IO)
with	g[k,j]	denoting the	gradient3	evaluated at	θ[k]	and	φ[j] ,	i.e.,	g[k,j]	=
vΦet〜pφ(τ)E [Rtotal(τ)] lθ[k],φ[j].
Generic Algorithm: Having described the two main steps needed for updating policies and mod-
els, we now summarise these findings in the pseudo-code in Algorithm 1. As the Hessian4 of the
Wasserstein distance is evaluated based on reference dynamics and any policy π, we pass it, along
with and φ0 as inputs. Then Algorithms 1 operates in a descent-ascent fashion in two main phases.
In the first, lines 5 to 10 in Algorithm 1, dynamics parameters are updated using the closed-form
solution in Equation 10, while ensuring that learning rates abide by a step size condition (we used
the Wolfe conditions (Wolfe, 1969), though it can be some other method). With this, the second
phase (line 11) utilises any state-of-the-art reinforcement learning method to adapt policy parame-
ters generating θ[k+1] . Regarding the termination condition for the inner loop, we leave this as a
decision for the user. It could be, for example, a large finite time-out, or the norm of the gradient
g[k,j] being below a threshold, or whichever happens first.
3Remark: Although this looks superficially similar to an approximation made in TRPO (Schulman et al.,
2015a), the latter aims to optimise the policy parameter rather than dynamics. Furthermore, the constraint is
based on the Kullback-Leibler divergence rather than the Wasserstein distance
4Please note our algorithm does not need to store the Hessian matrix. In line 7 of the algorithm, it is clear
that we require Hessian-vector products. These can be easily computed using computational graphs without
having access to the full Hessian matrix.
5
Under review as a conference paper at ICLR 2020
Algorithm 1: Wasserstein Robust Reinforcement Learning
1:	Inputs: Wasserstein distance Hessian, H0 evaluated at φ0 under any policy π, radius of the
Wasserstein ball , and the reference simulator specification parameters φ0
2:	Initialise φ[0] with φ0 and policy parameters θ[0] arbitrarily
3:	for k = 0, 1, . . . do
4:	x[0] J φ[0], and j J 0
5:	Phase I: Update model parameter while fixing the policy:
6:	while termination condition not met do
7:	Compute descent direction for the model parameters as given by Equation 10:
p[j] j φ0 - Sg[k,j]TH-Ig[k,j] H-1g[k,j] - x[j]
8:	Update candidate solution, while satisfying step size conditions (see discussion below) on
the learning rate α: x[j+1] J x[j] + αp[j] andj J j + 1
9:	end while
10:	Perform model update setting φ[k+1] J x[j]
11:	Phase II: Update policy given new model parameters:
12:	Use any standard reinforcement learning algorithm for ascending in the gradient direction,
e.g., θ[k+1] J θ[k] + β[k]VθEτ〜pφ(T) [Rtotal(τ)] ∣θ[k],φ[k+i], with β[k] a learning rate.
13:	end for
4 Zero’ th Order Wasserstein Robust Reinforcement Learning
Consider a simulator (or differential equation solver) Sφ for which the dynamics are parameterised
by a real vector φ, and for which we can execute steps of a trajectory (i.e., the simulator takes as input
an action a and gives back a successor state and reward). For generating novel physics-grounded
transitions, one can simply alter φ and execute the instruction in Sφ from some a state s ∈ S,
while applying an action a ∈ A. Not only does this ensure valid (under mechanics) transitions, but
also promises scalability as specification parameters typically reside in lower dimensional spaces
compared to the number of tuneable weights when using deep networks as transition models.
Gradient Estimation: Recalling the update rule in Phase I of Algorithm 1, we realise the need
for, estimating the gradient of the loss function with respect to the vector specifying the dynamics of
the environment, i.e., g[k,j] = VφEτ〜？#(丁) Rtotal(τ)]	at each iteration of the inner-loop j.
θ	θ[k],φ[j]
Handling simulators as black-box models, we estimate the gradients by sampling from a Gaussian
distribution with mean 0 and σ2I co-variance matrix. Our choice for such estimates is not arbitrary
but rather theoretically grounded as one can easily prove the following proposition:
Proposition 1 (Zero-Order Gradient Estimate). For a fixed θ and φ, the gradient can be computed
as:
vφET〜pφ(T) [Rtotal(T)] = σEξ〜N(0,σ2I)
Hessian Estimation: Having derived a zero-order gradient estimator, we now generalise these
notions to a form allowing us to estimate the Hessian. It is also worth reminding the reader that
such a Hessian estimator needs to be performed one time only before executing the instructions in
Algorithm 1 (i.e., H0 is passed as an input). Precisely, we prove the following proposition:
Proposition 2 (Zero-Order Hessian Estimate). The hessian of the Wasserstein dis-
tance around φ0 can be estimated based on function evaluations. Recalling that
HO = * * * * vφ E(Sa)〜π(∙)ρΦ0 (∙) [W2 (Pφds, a), PO(Is, a))]	, and defining W(s,a) (O) =
π	φ=φ0
ξ	pθφ+ξ (τ)Rtotal(τ)dτ	.
6
Under review as a conference paper at ICLR 2020
W2 (Pφ(∙∣s, a), Po(∙∣s, a)), we prove：
HO = ~2Eξ~N(0,σ2I) σ2ξ (E(s,Q)〜∏(∙)ρφ0 S [W(s,α) (φ0 + ξ)]) ξT
- E(s,a)〜π(∙)ρφ0 (∙) [W(s,α)(φ0 + ξ)] I .
Proofs of these propositions are given in Appendix A. They allow for a procedure where gradi-
ent and Hessian estimates can be simply based on simulator value evaluations while perturbing φ
and φ0 . It is important to note that in order to apply the above, we are required to be able to
evaluate E(Sa)〜∏(∙)ρφ0(∙) [W(s,a)(Φo)] under random ξ perturbations sampled from N(0,σ2I).
An empirical estimate of the P-Wasserstein distance between two measures μ and V can be Per-
formed by computing the p-Wasserstein distance between the empirical distributions evaluated at
sampled data. That is, one can approximation μ by μn = 1 pn=1 δχi where Xi are identically
and independently distributed according to μ. Approximating Vn similarly, we then realise that5
W2(μ, ν) ≈ W2(μn,Vn).
5	Experiments & Results
We evaluate WR2 L on a variety of continuous control benchmarks from the MuJoCo environment.
Dynamics in our benchmarks were parameterised by variables defining physical behaviour, e.g.,
density of the robot’s torso, friction of the ground, and so on. We consider both low and high dimen-
sional dynamics and demonstrate that our algorithm outperforms state-of-the-art from both standard
and robust reinforcement learning. We are chiefly interested in policy generalisation across environ-
ments with varying dynamics, which we measure using average test returns on novel systems. The
comparison against standard reinforcement learning algorithms allows us to understand whether lack
of robustness is a critical challenge for sequential decision making, while comparisons against robust
algorithms test ifwe outperform state-of-the-art that considered a similar setting to ours. From stan-
dard algorithms, we compare against proximal policy optimisation (PPO) (Schulman et al., 2017),
and trust region policy optimisation (TRPO) (Schulman et al., 2015b); an algorithm based on natural
actor-crtic (Peters & Schaal, 2008; Pajarinen et al., 2019). From robust algorithms, we demonstrate
how WR2L favours against robust adversarial reinforcement learning (RARL) (Pinto et al., 2017),
and action-perturbed Markov decision processes (PR-MDP) proposed in (Tessler et al., 2019). Due
to space constraints, the results are presented in Appendix B.2. It is worth noting that we attempted
to include deep deterministic policy gradients (DDPG) (Silver et al., 2014) in our comparisons.
Results including DDPG were, however, omitted as it failed to show any significant robustness per-
formance even on relatively simple systems, such as the inverted pendulum; see results reported in
Appendix B.3. During initial trials, we also performed experiments parameterising models using
deep neural networks. Results demonstrated that these models, though minimising training data er-
ror, fail to provide valid physics-grounded dynamics. For instance, we arrived at inverted pendula
models that vary pole angles without exerting any angular speeds. This problem became even more
apparent in high-dimensional systems, e.g., Hopper, Walker, etc due to the increased number of pos-
sible minima. As such, results presented in this section make use of our zero-order method that can
be regarded as a scalable alternative for robust solutions.
5.1	MuJoCo benchmarks
We evaluate our method both in low and high-dimensional MuJuCo tasks (Brockman et al., 2016).
We consider a variety of systems including CartPole, Hopper, and Walker2D; all of which require
direct joint-torque control. Keeping with the generality of our method, we utilise these frameworks
as-is with no additional alterations, that is, we use the exact setting of these benchmarks as that
shipped with OpenAI gym without any reward shaping, state-space augmentation, feature extraction,
or any other modifications of-that-sort. Details are given in section B.
5In case the dynamics are assumed to be Gaussian, a similar procedure can be followed or a closed form
can be used, see Takatsu (2008).
7
Under review as a conference paper at ICLR 2020
G-AllsUgOSJOl G-AllsUgOSJOla-us54E0ae
2：5
(a)	= 0 (PPO)
2500-
2«»-	■■■■■
1500-
10∞-
■■■■■
■■■
■■■
500-
ι'o 1'4 1'7 2'1 2'4 a'β
left IbOt Mctlonl μi
(e) = 0 (PPO)
∣∙35<n
-3000
2500
-2000
1500
-IOOO
-500
14800
40∞
3200
2400
-16∞
-800
I6∞0
4500
3∞0
-1500
■ S
0∙5-,	, 口■—
500 IOOO 1500 2000 2500 3000
head density, pl
£ -s"u8
3000
258-
■■■■■■■匚
2。OO-
158-
ιo∞-B
13500
3000
258
2000
1500
-IOOO
-500
200。一 ：■■■■■■■
■■■■■■■■■
1'.5 1'.7 1.9 2Λ 23 25
ground friction, μβ
(b)	= 0.003
3000-
2000-
15∞-
1000-
14800
-40∞
-3200
-2400
-1600
-800
G-AllsUgOSJOl & -s"u8
13500
13000
12500
-2000
1500
-IOOO
-500
∣∙3500
3000
2500
-2000
1500
IOOO
-500
5∞-
i：o 1'.4 1'.7 2'1 2'4 a'β
left foot frtctlcn, μt
(f) = 0.1
500 IOOO 1500 2000 2500
head density, pl
I6∞0
4500
3∞0
-1500
3000
ground friction, μβ
500 IOOO 1500 2000 2500
head density, pl
(g) € = 1.0
I6∞0
4500
3∞0
-1500
ground frtcttonl μβ
(d) PPO mean
14800
«00
-3200
-2400
-1600
-800
1.4 1.7 2.1 2.4 2.8
left r∞t friction, u1
(h) PPO mean
0.5-,
500 IOOO 1500 2000 2500 3000
head density, pl
I6∞0
4500
3∞0
-1500
£ -s"u8 0SJ2
£ -s"u8 0SJ2
(i)	€ = 0.0 (PPO)	(j) € = 0.005	(k) € = 0.03	(l) PPO mean
Figure 1: Dynamics variations in two dimensions and with different €, for Hopper (top row), Walker (middle
row), and HalfCheetah (bottom row). Fourth column is PPO trained with dynamics sampled uar from the
Wasserstein ball. Darker is better, with the numbers in the graded scale being the range of scores that the tasks
can generate.
Due to space constraints, results for one-dimensional parameter variations are given in Appendix
B.2, where it can be seen that WR2L outperforms both robust and non-robust algorithms when one-
dimensional simulator variations are considered. Figure 1 shows results for dynamics variations
along two dimensions. Here again, our methods demonstrates considerable robustness. The fourth
column, “PPO mean”, refers to experiments where PPO is trained on a dynamics sampled uniformly
at random from the Wasserstein constraint set. It displayes more robustness than when trained on
just the reference dynamics, however, as can be seen from Fig. 2, our method performs noticably
better in high dimensions, which is the main strength of our algorithm.
Results with High-Dimensional Model Variation: Though results above demonstrate robust-
ness, an argument against a min-max objective can be made especially when only considering low-
dimensional changes in the simulator. Namely, one can argue the need for such an objective as
opposed to simply sampling a set of systems and determining policies performing-well on average
similar to the approach proposed in (Rajeswaran et al., 2017).
A counter-argument to the above is that a gradient-based optimisation scheme is more efficient than
a sampling-based one when high-dimensional changes are considered. In other words, a sampling
procedure is hardly applicable when more than a few parameters are altered, while WR2L can re-
main suitable. To assess these claims, we conducted two additional experiments on the Hopper and
HalfCheetah benchmarks. In the first, we trained robustly while changing friction and torso densi-
ties, and tested on 1,000 systems generated by varying all 11 dimensions of the Hopper dynamics,
and 21 dimensions of the HalfCheetah system. The histogram Figures 2(b) and (f) demonstrate that
the empirical densities of the average test returns are mostly centered around 3,000 for the Hopper,
and around 4,500 for the Cheetah, which improves that of PPO trained on reference (Figures 2(a)
and (e)) with return masses mostly accumulated at around 1,000 in the case of the Hopper and almost
equally distributed when considering HalfCheetah. Such improvements, however, can be an artifact
of the careful choice of the low-dimensional degrees of freedom allowed to be modified during Phase
I of Algorithm 1. To get further insights, Figures 2(c) and (g) demonstrate the effectiveness of our
method trained and tested while allowing to tune all 11 dimensional parameters of the Hopper sim-
8
Under review as a conference paper at ICLR 2020
(a) PPO Test High
1.0^=3-------------------
(b) Train Low Test High
ι.o^=3-----------------------------
(C)Train HighTeStHigh
ι.o^=3---------------
2su∙d-SC-AEw
Λllsu∙p -SC-AEW
(e) PPO Test High
(d) PPO mean
1…3
(h) PPO mean
(f) Train Low Test High (g) Train High Test High
Figure 2: Results evaluating performance when considering high-dimensional variations on the Hopper (top
row) and HalfCheetah (bottom row) environment. All figures show the empirical distribution of returns on
1,000 testing systems. The first column demonstrates the robustness of PPO when trained on the reference
dynamics and trained with high dimensional variations. The second column reports empirical test returns of
WR2L’s policy trained on only two parameter changes (e.g., friction and density) of the environment but tested
on systems with all high-dimensional dynamical parameters modified. The third column trains and tests WR2L
altering all dimensional parameters of the simulator. The fourth column reports results when PPO is trained on
dynamics chosen uar and tested with high dimensional variation.
ulator, and the 21 dimensions of the HalfCheetah. Indeed, our results are in accordance with these
of the previous experiment depicting that most of the test returns’ mass remains around 3,000 for
the Hopper, and improves to accumulate around 4,500 for the HalfCheetah. Interestingly, however,
our algorithm is now capable of acquiring higher returns on all systems6 since it is allowed to alter
all parameters defining the simulator. As such, we conclude that WR2L outperforms others when
high-dimensional simulator variations are considered. In Figures 2(d) and (h), we see the results for
PPO trained with dynamics sampled uar from the Wasserstein constraint set. We see that although
in the two-dimensional variation case this training method worked well (see Figures 1(d), (h), (l)), it
does not scale well to high dimensions, and our method does better.
6	Related work
Previous work on robust MDPs, e.g., Iyengar (2005); Nilim & El Ghaoui (2005); Wiesemann et al.
(2013); Tirinzoni et al. (2018); Petrik & Russell (2019), whilst valuable in its own right, is not
sufficient for the RL setting due to the need in the latter case to give efficient solutions for large state
and action spaces, and the fact that the dynamics are not known a priori.
Closer to our own work, Rajeswaran et al. (2017) approaches the robustness problem by training on
an ensemble of dynamics in order to be deployed on a target environment. The algorithm introduced,
Ensemble Policy Optimisation (EPOpt), alternates between two phases: (i) given a distribution over
dynamics for which simulators (or models) are available (the source domain), train a policy that
performs well for the whole distribution; (ii) gather data from the deployed environment (target
domain) to adapt the distribution. The objective is not max-min, but a softer variation defined by
conditional value-at-risk (CVaR). The algorithm samples a set of dynamics {φk} from a distribution
over dynamics Pψ, and for each dynamics φk, it samples a trajectory using the current policy
parameter θi . It then selects the worst performing -fraction of the trajectories to use to update
the policy parameter. Clearly this process bears some resemblance to our algorithm, but there is
a crucial difference: our algorithm takes descent steps in the φ space. The difference if important
when the dynamics parameters sit in a high-dimensional space, since in that case, optimisation-
from-sampling could demand a considerable number of samples. In any case, our experiments
demonstrate our algorithm performs well even in these high dimensions. We note that we were
6Please note that we attempted to compare against Rajeswaran et al. (2017). Due to the lack of open-source
code, we were not able to regenerate their results.
9
Under review as a conference paper at ICLR 2020
were unable to find the code for this paper, and did not attempt to implement it ourselves. The
CVaR criterion is also adopted in Pinto et al. (2017), in which, rather than sampling trajectories and
finding a quantile in terms of performance, two policies are trained simultaneously: a “protagonist”
which aims to optimise performance, and an adversary which aims to disrupt the protagonist. The
protagonist and adversary train alternatively, with one being fixed whilst the other adapts. We made
comparisons against this algorithm in our experiments. More recently, Tessler et al. (2019) studies
robustness with respect to action perturbations. There are two forms of perturbation addressed: (i)
Probabilistic Action Robust MDP (PR-MDP), and (ii) Noisy Action Robust MDP (NR-MDP). In
PR-MDP, when an action is taken by an agent, with probability α, a different, possibly adversarial
action is taken instead. In NR-MDP, when an action is taken, a perturbation is added to the action
itself. Like Rajeswaran et al. (2017) and Pinto et al. (2017), the algorithm is suitable for applying
deep neural networks, and the paper reports experiments on InvertedPendulum, Hopper, Walker2d
and Humanoid. We tested against PR-MDP in some of our experiments, and found it to be lacking in
robustness (see Section 5). In Lecarpentier & Rachelson (2019) a non-stationary Markov Decision
Process model is considered, where the dynamics can change from one time step to another. The
constraint is based on Wasserstein distance, specifically, the Wasserstein distance between dynamics
at time t and t0 is bounded by L|t - t0|, i.e., is L-Lipschitz with respect to time, for some constant L.
They approach the problem by treating nature as an adversary and implement a Minimax algorithm.
The basis of their algorithm is that due to the fact that the dynamics changes slowly (due to the
Lipschitz constraint), a planning algorithm can project into the future the scope of possible future
dynamics and plan for the worst. The resulting algorithm, known as Risk Averse Tree Search, is -
as the name implies - a tree search algorithm. It operates on a sequence “snapshots” of the evolving
MDP, which are instances of the MDP at points in time. The algorithm is tested on small grid
world, and does not appear to be readily extendible to the continuous state and action scenarios
our algorithm addresses. To summarise, our paper uses the Wasserstein distance for quantifying
variations in possible dynamics, in common with Lecarpentier & Rachelson (2019), but is suited
to applying deep neural networks for continuous state and action spaces. Our algorithm does not
require a full dynamics available to it, merely a parameterisable dynamics. It competes well with the
above papers, and operates well for high dimensional problems, as evidenced by the experiments.
7	Conclusion & Future Work
In this paper, we proposed a robust reinforcement learning algorithm capable of outperforming oth-
ers in terms of test returns on unseen dynamics. The algorithm makes use of Wasserstein constraints
for policies generalising across varying domains, and considers a zero-order method for scalable
solutions. Empirically, we demonstrated superior performance against state-of-the-art from both
standard and robust reinforcement learning on low and high-dimensional MuJuCo environments. In
future work, we aim to consider robustness in terms of other components of MDPs, e.g., state repre-
sentations, reward functions, and others. Furthermore, we will implement WR2L on real hardware,
considering sim-to-real experiments.
References
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp.
6571-6583. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
7892- neural- ordinary- differential- equations.pdf.
Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-sensitive and robust
decision-making: a cvar optimization approach. In C. Cortes, N. D. Lawrence, D. D. Lee,
M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems 28,
pp. 1522-1530. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/
6014- risk- sensitive- and- robust- decision- making- a- cvar- optimization- approach.
pdf.
10
Under review as a conference paper at ICLR 2020
Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2):
257-280, 2005.
Torsten Koller, Felix Berkenkamp, Matteo Turchetta, and Andreas Krause. Learning-based model
predictive control for safe exploration and reinforcement learning. CoRR, abs/1803.08287, 2018.
URL http://arxiv.org/abs/1803.08287.
Erwan Lecarpentier and Emmanuel Rachelson. Non-stationary markov decision processes a worst-
case approach using model-based reinforcement learning. arXiv preprint arXiv:1904.10090,
2019.
Michael Lutter, Christian Ritter, and Jan Peters. Deep lagrangian networks: Using physics as model
prior for deep learning, 07 2019.
Jun Morimoto and Kenji Doya. Robust reinforcement learning. Neural Comput., 17(2):335-359,
February 2005. ISSN 0899-7667. doi: 10.1162/0899766053011528. URL http://dx.doi.
org/10.1162/0899766053011528.
Hongseok Namkoong and John C. Duchi. Stochastic gradient methods for distributionally robust
optimization with f-divergences. In Proceedings of the 30th International Conference on Neural
Information Processing Systems, NIPS’16, pp. 2216-2224, USA, 2016. Curran Associates Inc.
ISBN 978-1-5108-3881-9. URL http://dl.acm.org/citation.cfm?id=3157096.
3157344.
Yurii Nesterov. Random gradient-free minimization of convex functions. CORE Discussion Pa-
Pers 2011001, Universite CatholiqUe de Louvain, Center for Operations Research and Economet-
rics (CORE), 2011. URL https://EconPapers.repec.org/RePEc:cor:louvco:
2011001.
Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain
transition matrices. Operations Research, 53(5):780-798, 2005.
Charles Packer, Katelyn Gao, Jernej Kos, Philipp KrahenbUhL Vladlen Koltun, and DaWn Song.
Assessing generalization in deep reinforcement learning. CoRR, abs/1810.12282, 2018. URL
http://arxiv.org/abs/1810.12282.
Joni Pajarinen, Hong Linh Thai, Riad Akrour, Jan Peters, and Gerhard Neumann. Compatible
natural gradient policy search. CoRR, abs/1902.02823, 2019. URL http://arxiv.org/
abs/1902.02823.
Xue Bin Peng, Marcin AndrychoWicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer
of robotic control With dynamics randomization. CoRR, abs/1710.06537, 2017. URL http:
//arxiv.org/abs/1710.06537.
Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomput., 71(7-9):1180-1190, March 2008.
ISSN 0925-2312. doi: 10.1016/j.neucom.2007.11.026. URL http://dx.doi.org/10.
1016/j.neucom.2007.11.026.
Marek Petrik and Reazul Hasan Russell. Beyond confidence regions: Tight bayesian ambiguity sets
for robust mdps. arXiv preprint arXiv:1902.07605, 2019.
Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport. Foundations and Trends® in
Machine Learning, 11(5-6):355-607, 2019.
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforce-
ment learning. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
2817-2826, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR. URL
http://proceedings.mlr.press/v70/pinto17a.html.
Aravind RajesWaran, Sarvjeet Ghotra, Balaraman Ravindran, and Sergey Levine. Epopt: Learning
robust neural netWork policies using model ensembles. International Conference on Learning
Representations (ICLR) 2017, arXiv preprint arXiv:1610.01283, 2017.
11
Under review as a conference paper at ICLR 2020
Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a
scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
Thomas Sargent and Lars Hansen. Robust control and model uncertainty. American Economic
Review, 91(2):60-66, 2001. URL https://Ec0nPapers.repec.0rg/RePEc:aea:
aecrev:v:91:y:2001:i:2:p:60-66.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research,
pp. 1889-1897, Lille, France, 07-09 Jul 2015a. PMLR. URL http://pr0ceedings.mlr.
press/v37/schulman15.html.
John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region
policy optimization. CoRR, abs/1502.05477, 2015b. URL http://arxiv.0rg/abs/1502.
05477.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In Proceedings of the 31st International Confer-
ence on International Conference on Machine Learning - Volume 32, ICML’14, pp. I-387-
I-395. JMLR.org, 2014. URL http://dl.acm.0rg/citati0n.cfm?id=3044805.
3044850.
Asuka Takatsu. Wasserstein geometry of gaussian measures, 2008.
Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and applica-
tions in continuous control. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings
of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 6215-6224, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL
http://pr0ceedings.mlr.press/v97/tessler19a.html.
Andrea Tirinzoni, Marek Petrik, Xiangli Chen, and Brian Ziebart. Policy-conditioned uncertainty
sets for robust markov decision processes. In Advances in Neural Information Processing Systems,
pp. 8939-8949, 2018.
Wolfram Wiesemann, Daniel Kuhn, and Berc Rustem. Robust markov decision processes. Mathe-
matics of Operations Research, 38(1):153-183, 2013.
Philip Wolfe. Convergence conditions for ascent methods. SIAM review, 11(2):226-235, 1969.
Insoon Yang. A convex optimization approach to distributionally robust markov decision processes
with wasserstein distance. IEEE control systems letters, 1(1):164-169, 2017.
Chenyang Zhao, Olivier Sigaud, Freek Stulp, and Timothy M. Hospedales. Investigating gen-
eralisation in continuous deep reinforcement learning. CoRR, abs/1902.07015, 2019. URL
http://arxiv.0rg/abs/1902.07015.
A Proofs
Proof of Proposition 1. The proof of the above proposition can easily be derived by combining the
lines of reasoning in (Salimans et al., 2017; Nesterov, 2011), while extending to the parameterisation
of dynamical models. To commence, begin by defining J(φ) = ET3 [Rtotai(τ)] for fixed policy
parameters θ. Given any perturbation vector, ξ 〜N(0,σ2I), We can derive (through a Taylor
expansion) the following:
Jθ(φ + ξ) = Jθ(φ) + ξTVφJθ(φ) + 2ξτVφjθ(φ)ξ + O (higher-order terms).
12
Under review as a conference paper at ICLR 2020
Multiplying by ξ, and taking the expectation on both sides of the above equation, we get:
Eξ 〜N (0,σ2I) [ξJθ (φ + ξ)] = Eξ 〜N (0,σ2I) ξJθ (φ) + ξξTVφJθ (φ) + 2 ξξTVφJθ (φ)ξ
=σ2VφJθ (φ).
Dividing by σ2, we derive the statement of the proposition as:
vφJθ (O) = σEξ〜N(0,σ2I) [ξJθ (φ + ξ)]
□
Proof of Proposition 2. Commencing with the right-hand-side of the above equation, we perform
second-order Taylor expansions for each of the two terms under under the expectation of ξ. Namely,
we write:
H0 = J2 Eξ〜N(0,σ2I) σ12ξ (E(s,a)〜π(∙)ρφ0(∙) [W(s,a) (。0 + ξ)]) ξT	(11)
- E(s,a)〜π(∙)ρφ0 (∙) [w(s,a)(φ0 + ξ)] I
≈ 3唳〜N(0，。21) E(s,a)〜Mg [W(s,a)6M ξξT + ξξ”ΦE(s,a)〜π(∙)ρφ0 [W(s,a)2C] ξT
+ 2ξTvΦE(s,a)〜π(∙)ρφ0 [W(s,a)(Φθ)]号
- ~σEξ~N(0,σ2I) E(s,a)〜π(∙)ρ∏0 [W(s,a) (OO)] I + ξTv6E(s,a)〜∏(∙)ρφ0 [W(s,a) (OO)] I
+ 2ξTvΦE(s,a)〜π(∙)ρφ0 [W(s,a)(Φθ)] ξ1
Now, we analyse each of the above terms separately. For ease of notation, we define the following
variables:
g = V6E(s,a)~nG)p^ [W(s,a)(Φθ)]	H = vΦE(s,a)〜π(∙)ρφ0 [W(s,a)(Φθ)]
A = ξξTgξT	B = ξξTHξξT
c = ξTHξ.
Starting with A, we can easily see that any (i, j) component can be written as A = Pdn2=1 ξiξjξngn.
Therefore, the expectation under ξ 〜N(0, σ2I) can be derived as:
Eξ〜N(0,σ2i) [ξiξjξngn] = gnEξi^N(0,σ2) [ξ3] = 0 if i = j = n and 0 otherwise.
Thus, We conclude that Eξ〜N(0,σ21) [A] = 0d2 ×d2.
Continuing with the second term, i.e., B, we realise that any (i, j) component can be written as
Bi,j = Pdn2=1 Pdm2=1 ξiξjξnξmHm,n. Now, we consider two cases:
• Diagonal Elements (i.e., when i = j): The expectation under ξ 〜N(0, σ2I) can be fur-
ther split in three sub-cases
-SUb-CaSe I When i = j = m = n:	We have Eξ〜N(o^i) [ξiξjξnξmHm,n]	=
Hi,iEξi 〜N (0,σ2) = 3σ4Hi,i.
-SUb-CaSe H When i = j = m = n: We have Eξ〜N(o^i) [ξiξjξnξmHm,n]	=
Hm,mEξ 〜N (0,σ2I) ξi2ξm2 = σ4Hm,m.
13
Under review as a conference paper at ICLR 2020
-SUb-Case In When indices are all distinct: We have Eξ〜N(o σ2i) [ξiξjξnξmHm n]
0.	'	'
Diagonal Elements Conclusion: Using the above results we conclude that
Eξ 〜N (0,σ2i )[Bi,i] = 2σ4Hi,i + σ4 trace (H).
• Off-Diagonal Elements (i.e., when i = j): The above analysis is now repeated for comput * 1
ing the expectation of the off-diagonal elements of matrix B. Similarly, this can also be
split into three sub-cases depending on indices:
-SUb-CaSe I When i = m = j = n We have Eξ〜N(0,σ2I) [ξiξjξnξmHm,η]
Hi,jEξ〜N(0,σ2I) [ξ2ξ2] = σ4Hi,j.
-SUb-CaSe II When i = n = j = m: We have Eξ〜N(0,σ2i) [ξiξjξnξmHm,n]
Hj,iEξ〜N(0,σ2I) [ξ2ξ2] = σ4Hj,i .
-SUb-CaSe In When indices are all distinct: We have Eξ〜N(o σ21) [ξiξjξnξmHm,n]
0.	'	'
Off-Diagonal Elements Conclusion: Using the above results and due to the sym-
metric properties of H, we conclude that Eξ〜N(。b2i) Bi,j] = 2σ4Hij
Finally, analysing c, one can realise that Eξ〜N(0,σ2i) [c] = Eξ〜N(0,σ2i) [Pd= 1 Pjd=2 1 ξiξjHi,j
σ2trace(H).
Substituting the above conclusions back in the original approximation in Equation 11, and using the
linearity of the expectation we can easily achieve the statement of the proposition.	□
B	Further details on experiments
For clarity, we summarise variables parameterising dynamics in Table 1, and detail specifics next.
CartPole: The goal of this classic control benchmark is to balance a pole by driving a cart along
a rail. The state space is composed of the position X and velocity X of the cart, as well as the angle
θ and angular velocities of the pole θ. We consider two termination conditions in our experiments:
1) pole deviates from the upright position beyond a pre-specified threshold, or 2) cart deviates from
its zeroth initial position beyond a certain threshold. To conduct robustness experiments, we param-
eterise the dynamics of the CartPole by the pole length lp, and test by varying lp ∈ [0.3, 3].
Hopper: In this benchmark, the agent is required to control a hopper robot to move forward without
falling. The state of the hopper is represented by positions, {x, y, z}, and linear velocities, {X, y, Z},
of the torso in global coordinate, as well as angles, {θi}2=0, and angular speeds, {θi}2=0, of the three
joints. During training, we exploit an early-stopping scheme if “unhealthy” states of the robot were
visited. Parameters characterising dynamics included densities {ρi}i3=0 of the four links, armature
{ai}i2=0 and damping {Zi}2=° of threejoints, and the friction coefficient μg. To test for robustness,
we varied both frictions and torso densities leading to significant variations in dynamics. We further
conducted additional experiments while varying all 11 dimensional specification parameters.
Walker2D: This benchmark is similar to Hopper except that the controlled system is a biped robot
with seven bodies and six joints. Dimensions for its dynamics are extended accordingly as reported
in Table 1. Here, we again varied the torso density for performing robustness experiments in the
range ρ0 ∈ [500, 3000].
Halfcheetah: This benchmark is similar to the above except that the controlled system is a two-
dimensional slice of a three-dimensional cheetah robot. Parameters specifying the simulator consist
of 21 dimensions, with 7 representing densities. In our two-dimensional experiments we varied the
14
Under review as a conference paper at ICLR 2020
	1D experiment	2D experiment	High-dimensional experiment
Inverted Pendulum	lp	None	None
Hopper	ρ0	{ρo,μg}	{Pi}3=o ∪ {ai}2=o ∪ {ζi}2=o ∪ μg
Walker2D	ρ0	{ρo,μg}	{ρi}6=0 ∪ {ai}5=0 ∪ {Zi}5=0 ∪ μg
HalfCheetah	μ	{ρo,μ}	None
Table 1: Parameterisation of dynamics. See section 5.1 for the physical meaning of these parameters.
torso-density and floor friction, while in high-dimensional ones, we allowed the algorithm to control
all 21 variables.
B.1	Experimental protocol
Our experiments included training and a testing phases. During the training phase we applied Algo-
rithm 1 for determining robust policies while updating transition model parameters according to the
min-max formulation. Training was performed independently for each of the algorithms on the rel-
evant benchmarks while ensuring best operating conditions using hyper-parameter values reported
elsewhere (Schulman et al., 2017; Pinto et al., 2017; Tessler et al., 2019).
For all benchmarks, policies were represented using parametrised Gaussian distributions with their
means given by a neural network and standard derivations by a group of free parameters. The neural
network consisted of two hidden layers with 64 units and hyperbolic tangent activations in each of
the layers. The final layer exploited linear activation so as to output a real number. Following the
actor-critic framework, we also trained a standalone critic network having the same structure as that
of the policy.
For each policy update, we rolled-out in the current worst-case dynamics to collect a number of
transitions. The number associated to these transitions was application-dependent and varied be-
tween benchmarks in the range of 5,000 to 10,000. The policy was then optimised (i.e., Phase II of
Algorithm 1) using proximal policy optimization with a generalised advantage estimation. To solve
the minimisation problem in the inner loop of Algorithm 1, we sampled a number of dynamics from
a diagonal Gaussian distribution that is centered at the current worst-case dynamics model. The
number of sampled dynamics and the variance of the sampled distributions depended on both the
benchmark itself, and well as the dimensions of the dynamics. Gradients needed for model updates
were estimated using the results in Propositions 7 and 8. Finally, we terminated training when the
policy entropy dropped below an application-dependent threshold.
When testing, we evaluated policies on unseen dynamics that exhibited simulator variations as de-
scribed earlier. We measured performance using returns averaged over 20 episodes with a maximum
length of 1,000 time steps on testing environments. We note that we used non-discounted mean
episode rewards to compute such averages.
B.2	Results with One-Dimensional Model Variation
Figure 3 shows the robustness of policies on various taks. For a fair comparison, we trained two
standard policy gradient methods (TRPO (Schulman et al., 2015b) and PPO (Schulman et al., 2017)),
and two robust RL algorithms (RARL (Pinto et al., 2017), PR-MDP (Tessler et al., 2019)) with the
reference dynamics preset by our algorithm. The range of evaluation parameters was intentionally
designed to include dynamics outside of the -Wasserstein ball. Clearly, WR2L outperforms all
baselines in this benchmark.
B.3	Deep Deterministic Policy Gradients Results
As mentioned in the experiments section of the main paper, we refrained from presenting results
involving deep deterministic policy gradients (DDPG) due to its lack in robustness even on simple
systems, such as the CartPole.
15
Under review as a conference paper at ICLR 2020
PJeMaJ V-XKHV Csz
(b) Hopper. Reference is ρ0 = 1750; PPO2
uses the same implementation as PPO but
trained with ρ0 = 3000.
-10∞
-500
PJeMaJ∙,p0sa∙,cδz
(a) Inverted pendulum, reference is lp = 1.65.
(c) Walker. Reference is ρ0 = 1750.
Figure 3: Robustness tests for various tasks; dynamics varied along one dimension.
(d) HalfCheetah. Reference is μ = 1.5.
Figure 4: Robustness results on the inverted pendulum demonstrating that our method outperforms
state-of-the-art in terms of average test returns and that DDPG lacks in robustness performance.
pjrŋMə' ueəz
Figure 4 depicts these results showing that DDPG lacks robustness even when minor variations
in the pole length are introduced. TRPO and PPO, on the other hand, demonstrate an acceptable
performance retaining a test return of 1,000 across a broad range of pole lengths variations.
C Derivation of the Closed Form Solution
In Section 3.2 we presented a closed form solution to the following optimisation problem:
T
min VφE	φ( ) [Rtotai(τ)]	(φ - φ[j]) s.t. ∣(φ - φo)τHo(φ - φo) ≤ e,
φ	"p'τ	θ[k],φj]	2
which took the form of:
φ[j+1]=φ0- SjH^ H-1g[k,j].
16
Under review as a conference paper at ICLR 2020
In this section of the appendix, we derive such an update rule from first principles. We commence
transforming the constraint optimisation problem into an unconstrained one using the Lagrangian:
L(φ,λ)= g[k,j],T (φ - φ[j]) + λ 2(φ - φo)τHo(φ - Φo)-e ,
T
where λ isa Lagrange multiplier, and g[k,j] = VφEτ〜pφ(τ)[Rtotai(τ)]	.
θ[k],φ[j]
Deriving the Lagrangian with respect to the primal parameters φ, we write:
VφL(φ, λ) = g[k,j]T + λ(φ - φ0)TH0.	(12)
Setting Equation 12 to zero and solving for primal parameters, we attain:
Φ = φ0- 1 H-1g[k,j].
λ
Plugging φ back into the equation representing the constraints, we derive:
(φo - 1 H-1g[k,j] - Co), Ho (φo - JH-1g[k,j] — Φo) = 2e =⇒ λ2 = 21-g^i^jτH-1g^k^jj.
It is easy to see that with the positive solution for λ, the KarUsh-Kuhn-Tucker (KKT) conditions
are satisfied. Since the objective and constraint are both convex, the KKT conditions are sufficient
and necessary for optimality, thus finalising our derivation.
17