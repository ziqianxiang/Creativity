Under review as a conference paper at ICLR 2020
Stochastic Latent Actor-Critic:
Deep Reinforcement Learning
with a Latent Variable Model
Anonymous authors
Paper under double-blind review
Ab stract
Deep reinforcement learning (RL) algorithms can use high-capacity deep networks
to learn directly from image observations. However, these kinds of observation
spaces present a number of challenges in practice, since the policy must now solve
two problems: a representation learning problem, and a task learning problem.
In this paper, we aim to explicitly learn representations that can accelerate re-
inforcement learning from images. We propose the stochastic latent actor-critic
(SLAC) algorithm: a sample-efficient and high-performing RL algorithm for learn-
ing policies for complex continuous control tasks directly from high-dimensional
image inputs. SLAC learns a compact latent representation space using a stochas-
tic sequential latent variable model, and then learns a critic model within this
latent space. By learning a critic within a compact state space, SLAC can learn
much more efficiently than standard RL methods. The proposed model improves
performance substantially over alternative representations as well, such as vari-
ational autoencoders. In fact, our experimental evaluation demonstrates that the
sample efficiency of our resulting method is comparable to that of model-based
RL methods that directly use a similar type of model for control. Furthermore, our
method outperforms both model-free and model-based alternatives in terms of final
performance and sample efficiency, on a range of difficult image-based control
tasks. Our code and videos of our results are available at our website.1
1	Introduction
Deep reinforcement learning (RL) algorithms can automatically learn to solve certain tasks from raw,
low-level observations such as images. However, these kinds of observation spaces present a number
of challenges in practice: on one hand, it is difficult to directly learn from these high-dimensional
inputs, but on the other hand, it is also difficult to tease out a compact representation of the underlying
task-relevant information from which to learn instead. For these reasons, deep RL directly from
low-level observations such as images remains a challenging problem. Particularly in continuous
domains governed by complex dynamics, such as robotic control (Tassa et al., 2018; Brockman
et al., 2016), standard approaches still require separate sensor setups to monitor details of interest
in the environment, such as the joint positions of a robot or specific pose information of objects
of interest. To instead be able to learn directly from the more general and rich modality of vision
would greatly advance the current state of our learning systems, so we aim to study precisely this.
Standard model-free deep RL aims to use direct end-to-end training to explicitly unify these tasks of
representation learning and task learning. However, solving both problems together is difficult, since
an effective policy requires an effective representation, but in order for an effective representation to
emerge, the policy or value function must provide meaningful gradient information using only the
model-free supervision signal (i.e., the reward function). In practice, learning directly from images
with standard RL algorithms can be slow, sensitive to hyperparameters, and inefficient. In contrast to
end-to-end learning with RL, predictive learning can benefit from a rich and informative supervision
signal before the agent has even made progress on the task or received any rewards. This leads us to
ask: can we explicitly learn a latent representation from raw low-level observations that makes deep
RL easier, through learning a predictive latent variable model?
1https://rl-slac.github.io/slac/
1
Under review as a conference paper at ICLR 2020
Predictive models are commonly used in model-based RL for the purpose of planning (Deisenroth &
Rasmussen, 2011; Finn & Levine, 2017; Nagabandi et al., 2018; Chua et al., 2018; Zhang et al., 2019)
or generating cheap synthetic experience for RL to reduce the required amount of interaction with the
real environment (Sutton, 1991; Gu et al., 2016). However, in this work, we are primarily concerned
with their potential to alleviate the representation learning challenge in RL. We devise a stochastic
predictive model by modeling the high-dimensional observations as the consequence of a latent
process, with a Gaussian prior and latent dynamics, as illustrated in Figure 1. A model with an entirely
stochastic latent state has the appealing interpretation of being able to properly represent uncertainty
about any of the state variables, given its past observations. We demonstrate in our work that fully
stochastic state space models can in fact be learned effectively: With a well-designed stochastic
network, such models outperform fully deterministic models, and contrary to the observations in
prior work (Hafner et al., 2019; Buesing et al., 2018), are actually comparable to partially stochastic
models. Finally, we note that this explicit representation learning, even on low-reward data, allows an
agent with such a model to make progress on representation learning even before it makes progress
on task learning.
Equipped with this model, we can then perform RL in the learned latent space of the predictive
model. We posit—and confirm experimentally—that our latent variable model provides a useful
representation for RL. Our model represents a partially observed Markov decision process (POMDP),
and solving such a POMDP exactly would be computationally intractable (Astrom, 1965; Kaelbling
et al., 1998; Igl et al., 2018). We instead propose a simple approximation that trains a Markovian
critic on the (stochastic) latent state and trains an actor on a history of observations and actions. The
resulting stochastic latent actor-critic (SLAC) algorithm loses some of the benefits of full POMDP
solvers, but it is easy and stable to train. It also produces good results, in practice, on a range of
challenging problems, making it an appealing alternative to more complex POMDP solution methods.
The main contributions of our SLAC algorithm are useful representations learned from our stochastic
sequential latent variable model, as well as effective RL in this learned latent space. We show
experimentally that our approach substantially improves on both model-free and model-based RL
algorithms on a range of image-based continuous control benchmark tasks, attaining better final
performance and learning more quickly than algorithms based on (a) end-to-end deep RL from
images, (b) learning in a latent space produced by various alternative latent variable models, such as
a variational autoencoder (VAE) (Kingma & Welling, 2014), and (c) model-based RL based on latent
state-space models with partially stochastic variables (Hafner et al., 2019).
2	Related Work
Representation learning in RL. End-to-end deep RL can in principle learn representations directly
as part of the RL process (Mnih et al., 2013). However, prior work has observed that RL has a
“representation learning bottleneck”: a considerable portion of the learning period must be spent
acquiring good representations of the observation space (Shelhamer et al., 2016). This motivates
the use of a distinct representation learning procedure to acquire these representations before the
agent has even learned to solve the task. The use of auxiliary supervision in RL to learn such
representations has been explored in a number of prior works (Lange & Riedmiller, 2010; Finn et al.,
2016; Jaderberg et al., 2017; Higgins et al., 2017; Ha & Schmidhuber, 2018; Nair et al., 2018; Oord
et al., 2018; Gelada et al., 2019; Dadashi et al., 2019). In contrast to this class of representation
learning algorithms, we explicitly learn a latent variable model of the POMDP, in which the latent
representation and latent-space dynamics are jointly learned. By modeling covariances between
consecutive latent states, we make it feasible for our proposed algorithm to perform Bellman backups
directly in the latent space of the learned model.
Partial observability in RL. Our work is also related to prior research on RL under partial observ-
ability. Prior work has studied exact and approximate solutions to POMDPs, but they require explicit
models of the POMDP and are only practical for simpler domains (Kaelbling et al., 1998). Recent
work has proposed end-to-end RL methods that use recurrent neural networks to process histories of
observations and (sometimes) actions, but without constructing a model of the POMDP (Hausknecht
& Stone, 2015; Foerster et al., 2016; Zhu et al., 2018). Other works, however, learn latent-space
dynamical system models and then use them to solve the POMDP with model-based RL (Watter
et al., 2015; Wahlstrom et al., 2015; Karl et al., 2017; Zhang et al., 2019; Hafner et al., 2019).
2
Under review as a conference paper at ICLR 2020
Although some of these works learn latent variable models that are similar to ours, these model-based
methods are often limited by compounding model errors and finite horizon optimization. In contrast
to these works, our approach does not use the model for prediction and performs infinite horizon
policy optimization. Our approach benefits from the good asymptotic performance of model-free
RL, while at the same time leveraging the improved latent space representation for sample efficiency.
Other works have also trained latent variable models and used their representations as the inputs to
model-free RL algorithms. They use representations encoded from latent states sampled from the
forward model (Buesing et al., 2018), belief representations obtained from particle filtering (Igl et al.,
2018), or belief representations obtained directly from a learned belief-space forward model (Gregor
et al., 2019). Our approach is closely related to these prior methods, in that we also use model-free
RL with a latent state representation that is learned via prediction. However, instead of using belief
representations, our method learns a critic directly on latent states samples.
Sequential latent variable models. Several previous works have explored various modeling choices
to learn stochastic sequential models (Krishnan et al., 2015; Archer et al., 2015; Karl et al., 2016;
Fraccaro et al., 2016; 2017; Doerr et al., 2018a). In the context of using sequential models for RL,
previous works have typically observed that partially stochastic state space models are more effective
than fully stochastic ones (Buesing et al., 2018; Igl et al., 2018; Hafner et al., 2019). In these models,
the state of the underlying MDP is modeled with the deterministic state of a recurrent network
(e.g., LSTM (Hochreiter & Schmidhuber, 1997) or GRU (Cho et al., 2014)), and optionally with
some stochastic random variables. As mentioned earlier, a model with a latent state that is entirely
stochastic has the appealing interpretation of learning a representation that can properly represent
uncertainty about any of the state variables, given past observations. We demonstrate in our work
that fully stochastic state space models can in fact be learned effectively and, with a well-designed
stochastic network, such models perform on par to partially stochastic models and outperform fully
deterministic models.
3	Reinforcement Learning and Modeling
This work addresses the problem of learning maximum entropy policies from high-dimensional
observations in POMDPs, by simultaneously learning a latent representation of the underlying MDP
state using variational inference and learning the policy in a maximum entropy RL framework. In
this section, we describe maximum entropy RL (Ziebart, 2010; Haarnoja et al., 2018a; Levine, 2018)
in fully observable MDPs, as well as variational methods for training latent state space models for
POMDPs.
3.1	Maximum Entropy RL in Fully Observable MDPs
In a Markov decision process (MDP), an agent at time t takes an action at ∈ A from state st ∈ S
and reaches the next state st+1 ∈ S according to some stochastic transition dynamics p(st+1|st, at).
The initial state s1 comes from a distribution p(s1), and the agent receives a reward rt on each of
the transitions. Standard RL aims to learn the parameters φ of some policy ∏φ(at |st) such that the
expected sum of rewards is maximized under the induced trajectory distribution ρπ . This objective
can be modified to incorporate an entropy term, such that the policy also aims to maximize the
expected entropy H(∏φ(∙∣st)) under the induced trajectory distribution ρ∏. This formulation has a
close connection to variational inference (Ziebart, 2010; Haarnoja et al., 2018a; Levine, 2018), and
we build on this in our work. The resulting maximum entropy objective is
φ* = arg max T E	[r(st, at) + αH(∏φ(∙∣St))],	(1)
φ t=ι (St,a∕~p∏
where r is the reward function, and α is a temperature parameter that controls the trade-off between
optimizing for the reward and for the entropy (i.e., stochasticity) of the policy. Soft actor-critic
(SAC) (Haarnoja et al., 2018a) uses this maximum entropy RL framework to derive soft policy
iteration, which alternates between policy evaluation and policy improvement within the described
maximum entropy framework. SAC then extends this soft policy iteration to handle continuous
action spaces by using parameterized function approximators to represent both the Q-function Qθ
(critic) and the policy πφ (actor). The soft Q-function parameters θ are optimized to minimize the
3
Under review as a conference paper at ICLR 2020
soft Bellman residual,
Jq(Θ) =	E	1(Qθ(st, at) - (rt + γV¾(st+ι)))2 ,	(2)
(St,at,rt,st+1)〜D L2	.
V5(st+1) =	E	[Qe(st+1, at+ι) - αlog∏φ(at+ι∣St+ι)],	(3)
at+1 〜∏Φ
where D is the replay buffer, Y is the discount factor, and θ are delayed parameters. The policy
parameters φ are optimized to update the policy towards the exponential of the soft Q-function,
J∏(φ)= E E [αlog(∏φ(at|st)) - Qθ(st,at)] .	(4)
St 〜D Iat 〜∏φ
Results of this stochastic, entropy maximizing RL framework demonstrate improved robustness
and stability. SAC also shows the sample efficiency benefits of an off-policy learning algorithm, in
conjunction with the high performance benefits of a long-horizon planning algorithm. Precisely for
these reasons, we choose to extend the SAC algorithm in this work to formulate our SLAC algorithm.
3.2 Sequential Latent Variable Models and Amortized Variational Inference
IN POMDPS
To learn representations for RL, we use latent variable models trained with amortized variational
inference. The learned model must be able to process a large number of pixels that are present in the
entangled image x, and it must tease out the relevant information into a compact and disentangled
representation z. To learn such a model, we can consider maximizing the probability of each observed
datapoint x from some training set D under the entire generative process p(x) = p(x|z)p(z) dz.
This objective is intractable to compute in general due to the marginalization of the latent variables z .
In amortized variational inference, we utilize the following bound on the log-likelihood (Kingma &
Welling, 2014),
Ex〜D [logp(x)] ≥ Ex〜D [Ez〜q [logp(x∣z)] - DKL (q(z∣x) k p(z))].	(5)
We can maximize the probability of the observed datapoints (i.e., the left hand side of Equation (5))
by learning an encoder q(z |x) and a decoderp(x|z), and then directly performing gradient ascent on
the right hand side of the equation. In this setup, the distributions of interest are the prior p(z ), the
observation modelp(x|z), and the posterior q(z|x).
Although such generative models have been shown to successfully model various types of complex
distributions (Kingma & Welling, 2014) by embedding knowledge of the distribution into an informa-
tive latent space, they do not have a built-in mechanism for the use of temporal information when
performing inference. In the case of partially observable environments, as we discuss below, the
representative latent state zt corresponding to a given non-Markovian observation xt needs to be
informed by past observations.
Consider a partially observable MDP (POMDP), where an action at ∈ A from latent state zt ∈ Z
results in latent state zt+1 ∈ Z and emits a corresponding observation xt+1 ∈ X . We make an
explicit distinction between an observation xt and the underlying latent state zt, to emphasize that
the latter is unobserved and the distribution is not known a priori. Analogous to the fully observable
MDP, the initial state distribution is p(z1), the transition probability distribution is p(zt+1 |zt, at),
and the reward is rt. In addition, the observation model is given by p(xt |zt).
As in the case for VAEs, a generative model of these observations xt can be learned by maximizing the
log-likelihood. In the POMDP setting, however, we note that xt alone does not provide all necessary
information to infer zt , and thus, prior temporal information must be taken into account. This brings
us to the discussion of sequential latent variable models. The distributions of interest are the priors
p(z1) and p(zt+1 |zt, at), the observation model p(xt|zt), and the approximate posteriors q(z1 |x1)
and q(zt+1 |xt+1, zt, at). The log-likehood of the observations can then be bounded, similarly to the
VAE bound in Equation (5), as
logp(xi:T+ι∣ai:T) ≥	E
zi：T+ι 〜q
T+1
logp(xt|zt) - DKL (q(z1 |x1) k p(z1))
t=1
T
-	DKL (q(zt+1 |xt+1, zt, at) k p(zt+1 |zt, at)) . (6)
t=1
4
Under review as a conference paper at ICLR 2020
Prior work (Hafner et al., 2019; Buesing et al., 2018; Doerr et al., 2018b) has explored modeling
such non-Markovian observation sequences, using methods such as recurrent neural networks with
deterministic hidden state, as well as probabilistic state-space models. In this work, we enable
the effective training of a fully stochastic sequential latent variable model, and bring it together
with a maximum entropy actor-critic RL algorithm to create SLAC: a sample-efficient and high-
performing RL algorithm for learning policies for complex continuous control tasks directly from
high-dimensional image inputs.
4 Joint Modeling and Control as Inference
Our method aims to learn maximum entropy policies
from high-dimensional, non-Markovian observations in
a POMDP, while also learning a model of that POMDP.
The model alleviates the representation learning problem,
which in turn helps with the policy learning problem. We
formulate the control problem as inference in a proba-
bilistic graphical model with latent variables, as shown in
Figure 1.
For a fully observable MDP, the control problem can be
embedded into a graphical model by introducing a bi- Figure 1: Graphical model of POMDP
nary random variable Ot, which indicates if time step with optimality variables for t ≥ τ + 1.
t is optimal. When its distribution is chosen to be
p(Ot = 1|st, at) = exp(r(st, at)), then maximization of p(O1:T) via approximate inference in
that model yields the optimal policy for the maximum entropy objective (Levine, 2018).
In a POMDP setting, the distribution can analogously be given by p(Ot = 1|zt, at) = exp(r(zt, at)).
Instead of maximizing the likelihood of the optimality variables alone, we jointly model the observa-
tions (including the observed rewards of the past time steps) and learn maximum entropy policies
by maximizing the marginal likelihoodp(xi：T+1, OT +1：T|ai：T). This objective represents both the
likelihood of the observed data from the past τ steps, as well as the optimality of the agent’s actions
for future steps. We factorize our variational distribution into a product of recognition terms q(z1 |x1)
and q(zt+1∣xt+1, zt, at), dynamics terms p(zt+ι∣zt, aj and policy terms π(ajzt):
q(z1:T, aT+1:T|x1:T+1, a1:T)
T	T-1	T
= q(z1|x1)	q(zt+1|xt+1, zt, at)	p(zt+1 |zt, at)	π(at |zt). (7)
t=1	t=T +1	t=T +1
The variational distribution uses the dynamics for future time steps to prevent the agent from
controlling the transitions and from choosing optimistic actions (Levine, 2018). The posterior over
the actions represents the agent’s policy π. Although this derivation uses a policy that is conditioned
on the latent state, our algorithm, which will be described in the next section, learns a parametric
policy that is directly conditioned on observations and actions. This approximation allows us to
directly execute the policy without having to perform inference on the latent state at run time.
We use the posterior from Equation (7) to obtain the evidence lower bound (ELBO) of the marginal
likelihood,
T+1
logp(x1:T+1, OT+1:T|a1:T) ≥	E	logp(xt|zt)
(ZLT ,aτ+i:T)〜q t=ι
T
- DKL (q(z1 |x1) k p(z1)) - DKL (q(zt+1 |xt+1, zt, at) k p(zt+1 |zt, at))
t=1
T
+ X (r(zt, at) + logp(at) - log∏(at∣Zt)) , (8)
t=T +1
where p(at ) is the action prior. The full derivation of the ELBO is given in Appendix A. This
derivation assumes that the reward function, which determines p(Ot|zt, at), is known. However, in
many RL problems, this is not the case. In that situation, we can simply append the reward to the
5
Under review as a conference paper at ICLR 2020
observation, and learn the reward along with p(xt|zt). This requires no modification to our method
other than changing the observation space, and we use this approach in all of our experiments. We do
this to learn latent representations that are more relevant to the task, but we do not use predictions
from it. Instead, the RL objective uses rewards from the agent’s experience, as in model-free RL.
5 Stochastic Latent Actor Critic
We now describe our stochastic latent actor critic (SLAC) algorithm, which approximately maximizes
the ELBO using function approximators to model the prior and posterior distributions. The ELBO
objective in Equation (8) can be split into a model objective and a maximum entropy RL objective.
The model objective can directly be optimized, while the maximum entropy RL objective can be
solved via message passing. We can learn Q-functions for the messages, and then we can rewrite
the RL objective to express it in terms of these messages. Additional details of the derivation of the
SLAC objectives are given in Appendix A.
Latent Variable Model: The first part of the ELBO corresponds to training the latent variable model
to maximize the likelihood of the observations, analogous to the ELBO in Equation (6) for the
sequential latent variable model. The distributions of the latent variable model are diagonal Gaussian
distributions, where the means and variances are outputs of neural networks. The distribution
parameters ψ of this model are optimized to maximize the first part of the ELBO. The model loss is
JM (ψ) = E	E
(xi：T+i,ai：T ,ri：T)〜D	zLτ+ι~qψ
τ+1
Elog pψ (xt∣zt)
t=1
B
τ
-DKL (qψ (z1 |x1) k pψ (z1)) -	DKL (qψ (zt+1 |xt+1, zt, at) k pψ (zt+1 |zt, at))
t=1
(9)
We use the reparameterization trick to sample from the filtering distribution qψ (z±T+1 |xi：T +1, ai：T).
Critic and Actor: The second part of the ELBO corresponds to the maximum entropy RL objective.
As in the fully observable case from Section 3.1 and as described by Levine (2018), this optimization
can be solved via message passing of soft Q-values, except that we use the latent states z rather than
the true states s . For continuous state and action spaces, this message passing is approximated by
minimizing the soft Bellman residual, which we use to train our soft Q-function parameters θ,
JQ(θ) = E	E
(XI：T + i,ai：T,rτ)〜D |_zi：T + 1 〜qψ
	
rτ + Y E	[Qa(ZT +1, aτ +1) - α log πφ(aτ +1 |x1:T +1, a1;T )]))
,	aτ + 1 〜πφ	))
(10)
where θ are delayed parameters, obtained as exponential moving averages of θ. Notice that the
latents zT and zT+1 , which are used in the Bellman backup, are sampled from the same joint, i.e.
ZT+1 〜 qψ (ZT+1 ∣Xt +1, ZT, a「). The RL objective, which corresponds to the second part of the
ELBO, can be rewritten in terms of the soft Q-function. The policy parameters φ are optimized to
maximize this objective, analogously to soft actor-critic (Haarnoja et al., 2018a). The policy loss is
then
Jπ(φ) =	E	E	E	[α log πφ(aT+1 |x1:T+1, a1:T) - Qθ (ZT+1,aT+1)]	.
(xi：T+i,ai：T)〜D |_zi：t+i〜qψ LaT + 1 〜πφ	」」
(11)
We assume a uniform action prior, so p(at) is a constant term that we omit from the policy loss. We
use the reparameterization trick to sample from the policy, and the policy loss only uses the last
sample ZT+1 of the sequence for the critic. Although the policy used in our derivation is conditioned
in the latent state, our learned parametric policy is conditioned directly on the past observations and
actions, so that the learned policy can be executed at run time without requiring inference of the
latent state. Finally, we note that for the expectation over latent states in the Bellman residual in
Equation (10), rather than sampling latent states Z 〜Z, we sample latent states from the filtering
distribution qψ(z±T +1 |x1：T+1, a1：T). This design choice allows us to minimize the critic loss for
samples that are most relevant for Q, while also allowing the critic loss to use the Q-function in the
same way as implied by the policy loss in Equation (11).
6
Under review as a conference paper at ICLR 2020
SLAC is outlined in Algorithm 1. The actor-critic component follows prior work, with automatic
tuning of the temperature α and two Q-functions to mitigate underestimation (Fujimoto et al., 2018;
Haarnoja et al., 2018a;b). SLAC can be viewed as a variant of SAC (Haarnoja et al., 2018a) where the
critic is trained on the stochastic latent state of our sequential latent variable model. The backup for the
critic is performed on a tuple (z「, a「rT, ZT+1), sampled from the posterior q(zτ+1, z"xi：T+1, ai：T).
The critic can, in principle, take advantage of the perfect knowledge of the state zt , which makes
learning easier. However, the parametric policy does not have access to zt , and must make decisions
based on a history of observations and actions. SLAC is not a model-based algorithm, in that in does
not use the model for prediction, but we see in our experiments that SLAC can achieve similar sample
efficiency as a model-based algorithm.
Algorithm 1 Stochastic Latent Actor-Critic (SLAC)
Require: E, ψ, θ1 , θ2, φ	. Environment and initial parameters for model, actor, and critic
xi 〜 EreSet ()	. SamPIe initial observation from the environment
DJ (xi)	. Initialize replay buffer with initial observation
for each iteration do
for each environment step do
at 〜∏φ(a∕xLt, ai:t-i)	. Sample action from the policy
rt, xt+i 〜Estep(at)	. Sample transition from the environment
D J D ∪ (at, rt, xt+1)	. Store the transition in the replay buffer
for each gradient step do
ψ J ψ 一 λMVψ JM(ψ)	. Update model weights
θi J θi 一 λqVθi Jq(ΘJ for i ∈ {1,2}	. Update the Q-function weights
φ J φ 一 λπVφJπ(φ)	. Update policy weights
θi J νθi + (1 — V)θi for i ∈ {1,2}	. Update target critic network weights
6 Latent Variable Model
We briefly summarize our full model architecture here, with full details in Appendix B. Motivated
by the recent success of autoregressive latent variables in VAEs (Razavi et al., 2019; Maaloe et al.,
2019), we factorize the latent variable zt into two stochastic layers, zti and zt2 , as shown in Figure 2.
This factorization results in latent distributions that are more expressive, and it allows for some parts
of the prior and posterior distributions to be shared. We found this design to produce high quality
reconstructions and samples, and utilize it in all of our experiments. The generative model p and the
inference model q are given by
pψ (zi) = pψ (z2∣z1)p(z1),
pψ (zt+i |zt, at) = pψ (zt+i |zt+i, zt , at)pψ (zt+i |zt , at),
qψ (zι∣χι) = pψ (z2∣z1)qψ (z1∣χι),
qψ(zt+1∣xt+1,zt, at) = pψ(z2+ι∣z1+ι,z2, at)qψ(z1+i|xt+i, z2, at).
Note that we choose the variational distribution q
over zt2 to be the same as the model p. Thus, the
KL divergence in JM simplifies to the divergence be-
tween q and p over zti . We use a multivariate standard
normal distribution for p(zii), since it is not condi-
tioned on any variables, i.e. z1 〜 N(0, I). The
conditional distributions of our model are diagonal
Gaussian, with means and variances given by neural
networks. Unlike models from prior work (Hafner
et al., 2019; Buesing et al., 2018; Doerr et al., 2018b),
which have deterministic and stochastic paths and use
recurrent neural networks, ours is fully stochastic, i.e.
our latent state is a Markovian latent random variable
formed by the concatenation of zti and zt2 . Further
details are discussed in Appendix B.
Figure 2: Diagram of our full model. Solid arrows
show the generative model, dashed arrows show
the inference model. Rewards are not shown for
clarity.
7
Under review as a conference paper at ICLR 2020
7	Experimental Evaluation
We evaluate SLAC on numerous image-based continuous control tasks from both the DeepMind
Control Suite (Tassa et al., 2018) and OpenAI Gym (Brockman et al., 2016), as illustrated in Figure 3.
Full details of SLAC’s network architecture are described in Appendix B. Aside from the value of
action repeats (i.e. control frequency) for the tasks, we kept all of SLAC’s hyperparameters constant
across all tasks in all domains. Training and evaluation details are given in Appendix C, and image
samples from our model for all tasks are shown in Appendix D. Additionally, visualizations of our
results and code are available on the project website.2
7.1	Comparative Evaluation on Continuous Control Benchmark Tasks
To provide a comparative evaluation against prior methods, we evaluate SLAC on four tasks (cheetah
run, walker walk, ball-in-cup catch, finger spin) from the DeepMind Control Suite (Tassa et al., 2018),
and four tasks (cheetah, walker, ant, hopper) from OpenAI Gym (Brockman et al., 2016). Note that
the Gym tasks are typically used with low-dimensional state observations, while we evaluate on them
with raw image observations. We compare our method to the following state-of-the-art model-based
and model-free algorithms:
SAC (Haarnoja et al., 2018a): This is an off-policy actor-critic algorithm, which represents a
comparison to state-of-the-art model-free learning. We include experiments showing the performance
of SAC based on true state (as an upper bound on performance) as well as directly from raw images.
MPO (Abdolmaleki et al., 2018b;a): This is an off-policy actor-critic algorithm that performs an
expectation maximization form of policy iteration, learning directly from raw images.
D4PG (Barth-Maron et al., 2018): This is also an off-policy actor-critic algorithm, learning directly
from raw images. The results reported in the plots below are the performance after 108 training steps,
as stated in the benchmarks from (Tassa et al., 2018).
PlaNet (Hafner et al., 2019): This is a model-based RL method for learning from images, which uses
a partially stochastic sequential latent variable model, but without explicit policy learning. Instead,
the model is used for planning with model predictive control (MPC), where each plan is optimized
with the cross entropy method (CEM).
DVRL (Igl et al., 2018): This is an on-policy model-free RL algorithm that also trains a partially
stochastic latent-variable POMDP model. DVRL uses the full belief over the latent state as input into
both the actor and critic, as opposed to our method, which trains the critic with the latent state and
the actor with a history of actions and observations.
Our experiments on the DeepMind Control Suite in Figure 4 show that the sample efficiency of
SLAC is comparable or better than both model-based and model-free alternatives. This indicates that
overcoming the representation learning bottleneck, coupled with efficient off-policy RL, provides
for fast learning similar to model-based methods, while attaining final performance comparable to
fully model-free techniques that learn from state. SLAC also substantially outperforms DVRL. This
difference can be explained in part by the use of an efficient off-policy RL algorithm, which can
better take advantage of the learned representation.
We also evaluate SLAC on continuous control benchmark tasks from OpenAI Gym in Figure 5. We
notice that these tasks are much more challenging than the DeepMind Control Suite tasks, because
the rewards are not as shaped and not bounded between 0 and 1, the dynamics are different, and the
episodes terminate on failure (e.g., when the hopper or walker falls over). PlaNet is unable to solve
Figure 3: Example image observations for our continuous control benchmark tasks: DeepMind Control’s
cheetah run, walker walk, ball-in-cup catch, and finger spin, and OpenAI Gym’s half cheetah, walker, hopper,
and ant (left to right). Images are rendered at a resolution of 64 × 64 pixels.
2https://rl-slac.github.io/slac/
8
Under review as a conference paper at ICLR 2020
Cheetah, run	Walker, walk
Environment Steps (Millions)
Ball in cup, catch
Finger, Spm
Environment Steps (Millions)
——SAC (state)	D4PG (108 steps)	MPO (107 steps)	MPO	SAC	PlaNet	DVRL	SLAC (ours)
Figure 4: Experiments on the DeepMind Control Suite from images (unless otherwise labeled as "state"). SLAC
(ours) converges to similar or better final performance than the other methods, while almost always achieving
reward as high as the upper bound SAC baseline that learns from true state. Note that for these experiments,
1000 environments steps corresponds to 1 episode.
I MPO (107 steps)	MPO	SAC	PlaNet	SLAC (ours)
Figure 5: Experiments on the OpenAI Gym benchmark tasks from images. SLAC (ours) converges to higher
performance than both PlaNet and SAC on all four of these tasks. The number of environments steps in each
episode is variable, depending on the termination.
the last three tasks, while for the cheetah task, it learns a suboptimal policy that involves flipping
the cheetah over and pushing forward while on its back. To better understand the performance of
fixed-horizon MPC on these tasks, we also evaluated with the ground truth dynamics (i.e., the true
simulator), and found that even in this case, MPC did not achieve good final performance, suggesting
that infinite horizon policy optimization, of the sort performed by SLAC and model-free algorithms,
is important to attain good results on these tasks.
Our experiments show that SLAC successfully learns complex continuous control benchmark tasks
from raw image inputs. On the DeepMind Control Suite, SLAC exceeds the performance of PlaNet
on three of the tasks, and matches its performance on the walker task. However, on the harder
image-based OpenAI Gym tasks, SLAC outperforms PlaNet by a large margin. In both domains,
SLAC substantially outperforms all prior model-free methods. We note that the prior methods that
we tested generally performed poorly on the image-based OpenAI Gym tasks, despite considerable
hyperparameter tuning.
7.2	Evaluating the Latent Variable Model
We next study the tradeoffs between different design choices
for the latent variable model. We compare our fully stochastic
model, as described in Section 6, to a standard non-sequential VAE
model (Kingma & Welling, 2014), which has been used in multi-
ple prior works for representation learning in RL (Higgins et al.,
2017; Ha & Schmidhuber, 2018; Nair et al., 2018), the partially
stochastic model used by PlaNet (Hafner et al., 2019), as well as
three variants of our model: a simple filtering model that does not
factorize the latent variable into two layers of stochastic units, a
fully deterministic model that removes all stochasticity from the
hidden state dynamics, and a partially stochastic model that has
both deterministic and stochastic transitions, similar to the PlaNet
model, but with our architecture. Both the fully deterministic and
partially stochastic models use the same architecture as our fully stochastic model, including the same
two-level factorization of the latent variable. In all cases, we use the RL framework of SLAC and
only vary the choice of model for representation learning. As shown in the comparison in Figure 6,
our fully stochastic model outperforms prior models as well as the deterministic and simple variants
PlaNet model	----- fully deterministic model
VAE model	Partially stochastic model
simple filtering model	fully stochastic model (ours)
Figure 6: Comparison of differ-
ent design choices for the latent
variable model.

9
Under review as a conference paper at ICLR 2020
UnJ qπsωqu
Figure 7: Example image sequence seen for the cheetah task (first row), corresponding posterior sample
(reconstruction) from our model (second row), and generated prediction from the generative model (last two
rows). The second to last row is conditioned on the first frame (i.e., the posterior model is used for the first
time step while the prior model is used for all subsequent steps), whereas the last row is not conditioned on any
ground truth images. Note that all of these sampled sequences are conditioned on the same action sequence, and
that our model produces highly realistic samples, even when predicting via the generative model.
of our own model. The partially stochastic variant of our model matches the performance of our fully
stochastic model but, contrary to the conclusions in prior work (Hafner et al., 2019; Buesing et al.,
2018), the fully stochastic model performs on par, while retaining the appealing interpretation of a
stochastic state space model. We hypothesize that these prior works benefit from the deterministic
paths (realized as an LSTM or GRU) because they use multi-step samples from the prior. In contrast,
our method uses samples from the posterior, which are conditioned on same-step observations, and
thus these latent samples are less sensitive to the propagation of the latent states through time.
7.3	Qualitative Predictions from the Latent Variable Model
We show example image samples from our learned sequential latent variable model for the cheetah
task in Figure 7, and we include the other tasks in Appendix D. Samples from the posterior show the
images Xt as constructed by the decoder pψ (xt∣zt), using a sequence of latents Zt that are encoded
and sampled from the posteriors, qψ(z1 |x1) and qψ(zt+1 |xt+1, zt, at). Samples from the prior, on
the other hand, use a sequence of latents where z1 is sampled fromp(z1) and all remaining latents
zt are from the propagation of the previous latent state through the latent dynamics pψ (zt+1 |zt, at).
Note that these prior samples do not use any image frames as inputs, and thus they do not correspond
to any ground truth sequence. We also show samples from the conditional prior, which is conditioned
on the first image from the true sequence: for this, the sampling procedure is the same as the prior,
except that z1 is encoded and sampled from the posterior qψ(z1 |X1), rather than being sampled from
p(z1 ). We notice that the generated images samples can be sharper and more realistic by using a
smaller variance for pψ (Xt |zj when training the model, but at the expense of a representation that
leads to lower returns. Finally, note that we do not actually use the samples from the prior for training.
8	Discussion
We presented SLAC, an efficient RL algorithm for learning from high-dimensional image inputs that
combines efficient off-policy model-free RL with representation learning via a sequential stochastic
state space model. Through representation learning in conjunction with effective task learning in the
learned latent space, our method achieves improved sample efficiency and final task performance as
compared to both prior model-based and model-free RL methods.
While our current SLAC algorithm is fully model-free, in that predictions from the model are
not utilized to speed up training, a natural extension of our approach would be to use the model
predictions themselves to generate synthetic samples. Incorporating this additional synthetic model-
based data into a mixed model-based/model-free method could further improve sample efficiency
and performance. More broadly, the use of explicit representation learning with RL has the potential
to not only accelerate training time and increase the complexity of achievable tasks, but also enable
reuse and transfer of our learned representation across tasks.
10
Under review as a conference paper at ICLR 2020
References
Abbas Abdolmaleki, Jost Tobias Springenberg, Jonas Degrave, Steven Bohez, Yuval Tassa, Dan
Belov, Nicolas Heess, and Martin A. Riedmiller. Relative entropy regularized policy iteration.
arXiv preprint arXiv:1812.02256, 2018a.
Abbas Abdolmaleki, Jost Tobias SPringenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and
Martin A. Riedmiller. Maximum a posteriori policy optimisation. In International Conference on
Learning Representations (ICLR), 2018b.
Evan Archer, Il Memming Park, Lars Buesing, John Cunningham, and Liam Paninski. Black box
variational inference for state sPace models. arXiv preprint arXiv:1511.07367, 2015.
Karl J Astrom. OPtimal control of markov Processes with incomPlete state information. Journal of
mathematical analysis and applications, 1965.
Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Alistair
Muldal, Nicolas Heess, and Timothy LillicraP. Distributed distributional deterministic Policy
gradients. In International Conference on Learning Representations (ICLR), 2018.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OPenAI Gym. arXiv preprint arXiv:1606.01540, 2016.
Lars Buesing, Theophane Weber, SebaStien Racaniere, S. M. Ali Eslami, Danilo Jimenez Rezende,
David P. Reichert, Fabio Viola, Frederic Besse, Karol Gregor, Demis Hassabis, and Daan Wier-
stra. Learning and querying fast generative models for reinforcement learning. arXiv preprint
arXiv:1802.03006, 2018.
KyUnghyUn Cho, Bart van Merrienboer, Caglar GUlgehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. In Conference on Empirical Methods in Natural Language
Processing (EMNLP), 2014.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In Neural Information
Processing Systems (NeurIPS), 2018.
Robert Dadashi, Adrien Ali Taiga, Nicolas Le Roux, Dale Schuurmans, and Marc G Bellemare.
The value function polytope in reinforcement learning. In International Conference on Machine
Learning (ICML), 2019.
Marc Deisenroth and Carl E Rasmussen. PILCO: A model-based and data-efficient approach to
policy search. In International Conference on Machine Learning (ICML), 2011.
Andreas Doerr, Christian Daniel, Martin Schiegg, Nguyen-Tuong Duy, Stefan Schaal, Marc Toussaint,
and Trimpe Sebastian. Probabilistic recurrent state-space models. In International Conference on
Machine Learning (ICML), 2018a.
Andreas Doerr, Christian Daniel, Martin Schiegg, Duy Nguyen-Tuong, Stefan Schaal, Marc Toussaint,
and Sebastian Trimpe. Probabilistic recurrent state-space models. In International Conference on
Machine Learning (ICML), 2018b.
Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In International
Conference on Robotics and Automation (ICRA), 2017.
Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter Abbeel. Deep spatial
autoencoders for visuomotor learning. In International Conference on Robotics and Automation
(ICRA), 2016.
Jakob Foerster, Ioannis Alexandros Assael, Nando de Freitas, and Shimon Whiteson. Learning to
communicate with deep multi-agent reinforcement learning. In Neural Information Processing
Systems (NIPS), 2016.
11
Under review as a conference paper at ICLR 2020
Marco Fraccaro, Soren Kaae Sonderby, Ulrich Paquet, and Ole Winther. Sequential neural models
with stochastic layers. In Neural Information Processing Systems (NIPS), 2016.
Marco Fraccaro, Simon Kamronn, Ulrich Paquet, and Ole Winther. A disentangled recognition and
nonlinear dynamics model for unsupervised learning. In Neural Information Processing Systems
(NIPS), 2017.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning (ICML), 2018.
Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G Bellemare. Deepmdp:
Learning continuous latent space models for representation learning. In International Conference
on Machine Learning (ICML), 2019.
Karol Gregor, Danilo Jimenez Rezende, Frederic Besse, Yan Wu, Hamza Merzic, and Aaron van den
Oord. Shaping belief states with generative environment models for rl. In Neural Information
Processing Systems (NeurIPS), 2019.
Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning with
model-based acceleration. In International Conference on Machine Learning (ICML), 2016.
David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Conference
on Machine Learning (ICML), 2018a.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algorithms
and applications. arXiv preprint arXiv:1812.05905, 2018b.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning (ICML), 2019.
Matthew Hausknecht and Peter Stone. Deep recurrent Q-learning for partially observable MDPs. In
AAAI Fall Symposium on Sequential Decision Making for Intelligent Agents, 2015.
Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew
Botvinick, Charles Blundell, and Alexander Lerchner. DARLA: Improving zero-shot transfer in
reinforcement learning. In International Conference on Machine Learning (ICML), 2017.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 1997.
Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and Shimon Whiteson. Deep variational
reinforcement learning for POMDPs. In International Conference on Machine Learning (ICML),
2018.
Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David
Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. In
International Conference on Learning Representations (ICLR), 2017.
Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially
observable stochastic domains. Artificial intelligence, 101(1-2):99-134, l998.
Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick van der Smagt. Deep variational bayes
filters: Unsupervised learning of state space models from raw data. In International Conference on
Learning Representations (ICLR), 2016.
Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick van der Smagt. Deep variational bayes
filters: Unsupervised learning of state space models from raw data. In International Conference on
Learning Representations (ICLR), 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015.
12
Under review as a conference paper at ICLR 2020
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In International Conference
on Learning Representations (ICLR), 2014.
Rahul G. Krishnan, Uri Shalit, and David Sontag. Deep kalman filters. arXiv preprint
arXiv:1511.05121, 2015.
Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement learning.
In International Joint Conference on Neural Networks (IJCNN), 2010.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review.
arXiv preprint arXiv:1805.00909, 2018.
Lars Maaloe, Marco Fraccaro, Valentin Lievin, and Ole Winther. Biva: A very deep hierarchy of
latent variables for generative modeling. In Neural Information Processing Systems (NeurIPS),
2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing Atari with deep reinforcement learning. In NIPS Deep
Learning Workshop, 2013.
Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynam-
ics for model-based deep reinforcement learning with model-free fine-tuning. In International
Conference on Robotics and Automation (ICRA), 2018.
Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual
reinforcement learning with imagined goals. In Neural Information Processing Systems (NeurIPS),
2018.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018.
Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with
vq-vae-2. 2019.
Evan Shelhamer, Parsa Mahmoudieh, Max Argus, and Trevor Darrell. Loss is its own reward:
Self-supervision for reinforcement learning. arXiv preprint arXiv:1612.07307, 2016.
Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM SIGART
Bulletin, 2(4):160-163,1991.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin Riedmiller.
DeepMind control suite. arXiv preprint arXiv:1801.00690, 2018.
Niklas Wahlstrom, Thomas B Schon, and Marc Peter Deisenroth. From pixels to torques: Policy
learning with deep dynamical models. arXiv preprint arXiv:1502.02251, 2015.
Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control:
A locally linear latent dynamics model for control from raw images. In Neural Information
Processing Systems (NIPS), 2015.
Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew J Johnson, and Sergey Levine.
SOLAR: Deep structured latent representations for model-based reinforcement learning. In
International Conference on Machine Learning (ICML), 2019.
Pengfei Zhu, Xin Li, Pascal Poupart, and Guanghui Miao. On improving deep reinforcement learning
for POMDPs. arXiv preprint arXiv:1804.06309, 2018.
Brian D Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal
entropy. 2010.
13
Under review as a conference paper at ICLR 2020
A Derivation of the Evidence Lower B ound and SLAC Objectives
In this appendix, we discuss how the SLAC objectives can be derived from applying a variational
inference scheme to the control as inference framework for reinforcement learning (Levine, 2018) . In
this framework, the problem of finding the optimal policy is cast as an inference problem, conditioned
on the evidence that the agent is behaving optimally. While Levine (2018) derives this in the fully
observed case, we present a derivation in the POMDP setting.
We aim to maximize the marginal likelihood p(xi：T+1, OT+1：T |ai：T), Where T is the number of steps
that the agent has already taken. This likelihood reflects that the agent cannot modify the past τ
actions and they might have not been optimal, but it can choose the future actions up to the end of the
episode, such that the chosen future actions are optimal. Notice that unlike the standard control as
inference frameWork, in this Work We not only maximize the likelihood of the optimality variables
but also the likelihood of the observations, Which provides additional supervision for the latent
representation. This does not come up in the MDP setting since the state representation is fixed and
learning a dynamics model of the state Would not change the model-free equations derived from the
maximum entropy RL objective.
For reference, We restate the factorization of our variational distribution:
q(z1:T, aT+1:T|x1:T+1, a1:T)
T	T-1	T
=q(zι∣xι) Uq(zt+1∣xt+1,zt, at) ɪɪ p(zt+ι∣zt, at) ɪɪ ∏(at∣Zt). (12)
t=1	t=T +1	t=T +1
As discussed by Levine (2018), the agent does not have control over the stochastic dynamics, so We
use the dynamics p(zt+1 |zt, at) for t ≥ τ + 1 in the variational distribution in order to prevent the
agent from choosing optimistic actions.
The joint likelihood is
p(x1:T+1, OT +1:T, z1:T, aT +1:T |a1:T)
T-1	T+1	T	T
= p(z1)	p(zt+1 |zt, at)	p(xt|zt)	p(Ot|zt, at)	p(at). (13)
t=1	t=1	t=T +1	t=T +1
We use the posterior from Equation (12) and Jensen’s inequality to obtain the ELBO of the marginal
likelihood,
logp(x1:T+1, OT +1:T |a1:T)
= log	p(x1:T+1, OT+1:T, z1:T, aT +1:T |a1:T) dz1:T daT+1:T
zi：T aτ+i:T
≥E
(zi：T ,aτ + i:T)〜q
T+1
logp(xt |zt)
t=1
T
- DKL (q(z1|x1) k p(z1)) -	DKL (q(zt+1|xt+1, zt, at) k p(zt+1|zt,at))
t=1
T
+ X (r(zt, at) + logp(at) - log∏(at∣zt)) .	(14)
t=T +1
Notice that the dynamics terms logp(zt+1|zt, at) for t ≥ τ + 1 from the posterior and the prior
cancel each other out in the ELBO.
The first part of the ELBO corresponds to the model objective. When using the parametric function
approximators, the negative of it corresponds directly to the model loss in Equation (9).
The second part of the ELBO corresponds to the maximum entropy RL objective. We assume a
uniform action prior, so the logp(at) term is a constant term that can be omitted When optimizing
14
Under review as a conference paper at ICLR 2020
this objective. We use message passing to optimize this objective, with messages defined as
Q(Zt, at) = r(Zt, at) +	E	[V(Zt+1)]
Zt+1 〜q(∙∣Xt+ι,Zt,at)
V (zt) = log	exp(Q(zt, at)) dat.
at
Then, the maximum entropy RL objective can be expressed in terms of the messages as
E
(zτ+i:T ,aτ + 1：T)〜q
T
X (r(zt, at) - log ∏(at∣Zt))
t=τ +1
τ,
E	Q(ZT+1, aτ+ι) - log∏(aτ+ι∣Zτ +ι)
,aτ) LaT + ι〜π(∙∣Zτ + ι) L	」
(15)
(16)
(17)
τ,
-DKL ∏(aτ +1∣Zτ +1)
,aτ)
exP (Q(ZT +1, aτ +1))、, ʊz 、-
exp(V(zτ+ι))尸 V(Zt+叩
(18)
where the first equality is obtained from dynamic programming (see Levine (2018) for details), the
second equality holds from the definition of KL divergence, and exp (V(Zt)) is the normalization
factor for exp (Q(Zt, at)) with respect to at. Since the KL divergence term is minimized when its
two arguments represent the same distribution, the optimal policy is given by
∏(at∣Zt) = exp(Q(Zt, aj - V(zJ).	(19)
Noting that the KL divergence term is zero for the optimal action, the equality from Equation (18)
can be used in Equation (15) to obtain
Q(Zt, at) = r(Zt, at) +	E
Zt+1 〜q(∙∣Xt+ι,Zt,at)
E	Q(Zt+1, at+1) - log π(at+1 |Zt+1)	.
_a，+i〜π(TZτ+1) L	」_
(20)
This equation corresponds to the standard Bellman backup with a soft maximization for the value
function.
As mentioned in Section 5, our algorithm conditions the parametric policy in the history of observa-
tions and actions, which allows us to directly execute the policy without having to perform inference
on the latent state at run time. When using the parametric function approximators, the negative of
the maximum entropy RL objective, written as in Equation (17), corresponds to the policy loss in
Equation (11). Lastly, the Bellman backup of Equation (20) corresponds to the Bellman residual in
Equation (10) when approximated by a regression objective.
We showed that the SLAC objectives can be derived from applying variational inference in the
control as inference framework in the POMDP setting. This leads to the joint likelihood of the past
observations and future optimality variables, which we aim to optimize by maximizing the ELBO of
the log-likelihood. We decompose the ELBO into the model objective and the maximum entropy RL
objective. We express the latter in terms of messages of Q-functions, which in turn are learned by
minimizing the Bellman residual. These objectives lead to the model, policy, and critic losses.
B Network Architectures
Recall that our full sequential latent variable model
has two layers of latent variables, which we denote
Zt1 and Zt2 . We found this design to provide a good
balance between ease of training and expressivity,
producing good reconstructions and generations and,
crucially, providing good representations for rein-
forcement learning. For reference, we reproduce the
model diagram from the main paper in Figure 8. Note
that this diagram represents the Bayes net correspond-
ing to our full model. However, since all of the la-
tent variables are stochastic, this visualization also
Figure 8: Diagram of our full model, reproduced
from the main paper. Solid arrows show the gen-
erative model, dashed arrows show the inference
model. Rewards are not shown for clarity.
15
Under review as a conference paper at ICLR 2020
presents the design of the computation graph. Inference over the latent variables is performed using
amortized variational inference, with all training done via reparameterization. Hence, the computation
graph can be deduced from the diagram by treating all solid arrows as part of the generative model
and all dashed arrows as part of approximate posterior. The generative model consists of the following
probability distributions, as described in the main paper:
z1 〜P(ZI)
z1 〜Pψ(Z2|ZI)
zt+1 〜pψ (Zt+1|zt, at)
Z2+1 〜pψ(Z2+ιlz1+ι, z2, at)
Xt 〜pψ(Xt |Z1, Z2)
rt 〜pψ (rt|Zl, Z2, at, Z1+1, Z2+1).
The initial distribution p(Z11) is a multivariate standard normal distribution N(0, I). All of the other
distributions are conditional and parameterized by neural networks with parameters ψ . The networks
for Pψ(z2∣z1), Pψ(Z1+ι∣Z2, at), Pψ(Z2+ι∣Z1+ι, Z2, at), and Pψ(rt∣Z1, *, at,Z1+ι, Z2+ι) consist of
two fully connected layers, each with 256 hidden units, and a Gaussian output layer. The Gaussian
layer is defined such that it outputs a multivariate normal distribution with diagonal variance, where
the mean is the output of a linear layer and the diagonal standard deviation is the output of a fully
connected layer with SoftPlUs non-linearity. The observation model pψ(xt∣Z1, Z2) consists of 5
transposed convolutional layers (256 4 × 4, 128 3 × 3, 64 3 × 3, 32 3 × 3, and 3 5 × 5 filters,
resPectively, stride 2 each, excePt for the first layer). The oUtPUt variance for each image Pixel is
fixed to 0.1.
The variational distribUtion q, also referred to as the inference model or the Posterior, is rePresented
by the following factorization:
z1 〜qψ(z1∣xi)
zi 〜pψ(Z2|ZI)
Zt+1 〜qψ (Zt+1|xt+1, Zt, at)
Z2+1 〜pψ (Z2+1|Z1+1, Z2, at)∙
Note that the variational distribUtion over Z21 and Zt2+1 is intentionally chosen to exactly match the
generative model p, sUch that this term does not aPPear in the KL-divergence within the ELBO,
and a seParate variational distribUtion is only learned over Z11 and Zt1+1 . This intentional design
decision simPlifies the inference Process. The networks rePresenting the distribUtions qψ(Z11 |X1) and
qψ (Zt1+1 |Xt+1, Zt2, at) both consist of5 convolUtional layers (32 5 × 5, 64 3 × 3, 128 3 × 3, 256 3 × 3,
and 256 4 × 4 filters, resPectively, stride 2 each, excePt for the last layer), 2 fUlly connected layers
(256 Units each), and a GaUssian oUtPUt layer. The Parameters of the convolUtion layers are shared
among both distribUtions.
The latent variables have 32 and 256 dimensions, resPectively, i.e. Zt1 ∈ R32 and Zt2 ∈ R256. For
the image observations, Xt ∈ [0, 1]64×64×3. All the layers, excePt for the oUtPUt layers, Use leaky
ReLU non-linearities. Note that there are no deterministic recUrrent connections in the network—all
networks are feedforward, and the temPoral dePendencies all flow throUgh the stochastic Units Zt1 and
Zt2
For the reinforcement learning Process, we Use a critic network Qθ consisting of 2 fUlly connected
layers (256 Units each) and a linear oUtPUt layer. The actor network πφ consists of 5 convolUtional
layers, 2 fUlly connected layers (256 Units each), a GaUssian layer, and a tanh bijector, which
constrains the actions to be in the boUnded action sPace of [-1, 1]. The convolUtional layers are the
same as the ones from the latent variable model, bUt the Parameters of these layers are not UPdated by
the actor objective. The same exact network architectUre is Used for every one of the exPeriments in
the PaPer.
16
Under review as a conference paper at ICLR 2020
				
Benchmark	Task	Action repeat	Original control time step	Effective control time step
	cheetah run	4	0.01	0.04
DeepMind Control Suite	walker walk ball-in-cup catch	2 4	0.025 0.02	0.05 0.08
	finger spin	2	0.02	0.04
	HalfCheetah-v2	-1	-0.05	- 0.05
OpenAI Gym	Walker2d-v2	4	0.008	0.032
	Hopper-v2	2	0.008	0.016
	Ant-v2	4	0.05	0.2
Table 1: Action repeats and the corresponding agent’s control time step used in our experiments.
C Training and Evaluation Details
The control portion of our algorithm uses the same hyperparameters as SAC (Haarnoja et al., 2018a),
except for a smaller replay buffer size of 100000 environment steps (instead of a million) due to
the high memory usage of image observations. All of the parameters are trained with the Adam
optimizer (Kingma & Ba, 2015), and we perform one gradient step per environment step. The
Q-function and policy parameters are trained with a learning rate of 0.0003 and a batch size of 256.
The model parameters are trained with a learning rate of 0.0001 and a batch size of 32. We use
sequences of length τ = 8 for all the tasks. Note that the sequence length can be less than τ for the
first t steps (t < τ) of each episode.
We use action repeats for all the methods, except for D4PG for which we use the reported results from
prior work (Tassa et al., 2018). The number of environment steps reported in our plots correspond to
the unmodified steps of the benchmarks. Note that the methods that use action repeats only use a
fraction of the environment steps reported in our plots. For example, 3 million environment steps of
the cheetah task correspond to 750000 samples when using an action repeat of 4. The action repeats
used in our experiments are given in Table 1.
Unlike in prior work (Haarnoja et al., 2018a;b), we use the same stochastic policy as both the
behavioral and evaluation policy since we found the deterministic greedy policy to be comparable or
worse than the stochastic policy.
D Additional Predictions from the Latent Variable Model
We show additional samples from our model in Figure 9 and Figure 10. Samples from the posterior
show the images Xt as constructed by the decoder pψ (xt∣zt), using a sequence of latents Zt that
are encoded and sampled from the posteriors, qψ(z1 |x1) and qψ(zt+1 |xt+1, zt, at). Samples from
the prior, on the other hand, use a sequence of latents where z1 is sampled from p(z1) and all
remaining latents zt are from the propagation of the previous latent state through the latent dynamics
pψ(zt+1 |zt, at). These samples do not use any image frames as inputs, and thus they do not
correspond to any ground truth sequence. We also show samples from the conditional prior, which is
conditioned on the first image from the true sequence: for this, the sampling procedure is the same
as the prior, except that zι is encoded and sampled from the posterior qψ (zι∣xι), rather than being
sampled from p(z1).
17
Under review as a conference paper at ICLR 2020
-BF Ja-
hctac puc-ni-llaB
nips regniF
Figure 9: Example image sequences, along with generated image samples, for three of the DM Control tasks
that we used in our experiments. See Figure 7 for more details and for image samples from the cheetah task.
18
Under review as a conference paper at ICLR 2020
ZA,qsωωqυJIπH
ZA,PZJω-πAV
2v-reppoH
Zzuv
Figure 10: Example image sequences, along with generated image samples, for the four OpenAI Gym tasks
that we used in our experiments.
19