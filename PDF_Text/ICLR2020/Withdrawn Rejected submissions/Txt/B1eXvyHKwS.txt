Under review as a conference paper at ICLR 2020
The Effect of Adversarial Training: A Theo-
retical Characterization
Anonymous authors
Paper under double-blind review
Ab stract
It has widely shown that adversarial training (Madry et al., 2018) is effective in
defending adversarial attack empirically. However, the theoretical understanding
of the difference between the solution of adversarial training and that of stan-
dard training is limited. In this paper, we characterize the solution of adversarial
training for linear classification problem for a full range of adversarial radius ε.
Specifically, We show that if the data themselves are ''ε-strongly linearly SePara-
ble”, adversarial training with radius smaller than ε converges to the hard margin
solution of SVM with a faster rate than standard training. If the data themselves
are not ''ε-strongly linearly separable”, we show that adversarial training with ra-
dius ε is stable to outliers while standard training is not. Moreover, we prove that
the classifier returned by adversarial training with a large radius ε has low confi-
dence in each data point. Experiments corroborate our theoretical finding well.
1 Introduction
Despite the impressive performance of deep neural networks on various learning tasks, the widely
existing adversarial examples (Goodfellow et al., 2014; Szegedy et al., 2017) has thwarted its ap-
plication in the safety-sensitive scenarios (Kurakin et al., 2016; Chen et al., 2015). A well trained
neural network can be vulnerable to certain small adversarial perturbation added to the original data,
despite the perturbations is almost imperceptible to human.
Adversarial training (Madry et al., 2018) is an effective method to train a robust deep neural net-
work that can resist adversarial samples to some extent. However, the theoretical understanding of
adversarial training is quite limited. In this paper, we try to unveil the mystery of adversarial train-
ing. Specifically, in this paper for adversarial training, we consider the attack in l2 norm, i.e. the
following objective
1N
min — > max '(x, θ).
θ N	kx-xik2≤ε
i=1
(1)
This is in contrast with the standard training objective mine N PN=I '(x, θ). To obtain a clear
theoretical characterization, we focus on the linear classifier.
In fact, for linearly separable data, Soudry et al. (2017); Ji & Telgarsky (2018) have proven that
the linear classifier trained by gradient descent (GD) with logistic loss converges to the hard margin
solution of SVM with a rate of O((log t)-1). However, we find things become much different for
adversarial training. We first prove that for "ε-strongly linearly separable” data (Definition 1), GD
can find the hard margin classifier with a faster rate of O((log t)-(1+ε*)), with the same exponential
tail loss used in Soudry et al. (2017). Here ε* = (∣S∣ε(ε1 - ε))∕N, where ε1 is the distance between
the support vectors and the hard margin solution of SVM, and |S| is the number of support vectors
corresponding to hard margin classifier. This result shows that we can find a robust solution much
faster by adversarial training if the data themselves are more separable than the adversarial radius.
When the data are not ε-strongly linearly separable, adversarial training gives significantly different
solutions from the standard training. To further illustration, we consider the following case, most
of the data are ε-strongly linearly separable while there several outliers are not or even not linearly
separable. Then the classifier returned by standard training is heavily affected by these outliers,
since the classifier returned by standard training converges to the hard margin classifier while it can
1
Under review as a conference paper at ICLR 2020
be sensitive to outliers. However, we can show the stability of adversarial training to outliers, i.e.
the classifier returned by adversarial training is slightly affected by outliers. Next, we also show that
adversarial training leads to a classifier with relatively lower confidence in each data point than that
of standard training. We then give a formal characterization for this phenomenon under the case of
a large ε. The low confidence in each training data naturally induces a high training loss. A simple
generalization error bound informs the high loss on test set, which interprets the widely observed
poor test performance of adversarial training.
1.1	Related work
Plenty of work trying to obtain a large margin solution to promote model robustness. Cisse et al.
(2017); Hein & Andriushchenko (2017) regularize the training with the Lipschitz constant of the
model to enhance robustness. Another line of work (Elsayed et al., 2018; Lee et al., 2019a; Serra
et al., 2018; Croce et al., 2019) uses a first order approximation to compute the margin of deep neural
network, and then find a large margin solution by setting the approximation to be the optimization
objective. However, none of these works come up with a theoretical guarantee.
Our paper is also related with Soudry et al. (2017); Ji & Telgarsky (2018) who prove that gradient
descent with logistic loss converge to the hard margin classifier for linearly separable data but with
a rate of O((log t)-1) or on non-linearly separable data with rate of O(log log t/ log t). Nacson
et al. (2018) extend this result to stochastic gradient descent. Gunasekar et al. (2018) study the
convergence under other optimization methods. Lyu & Li (2019) provides a similar result for the
homogeneous neural networks. However, they all target the standard optimization problem rather
than the adversarial training.
Ilyas et al. (2019) studies adversarial training via the lens of robust/non-robust features. They claim
that adversarial attacks are attributed to the presence of non-robust features and adversarial training
can be viewed as explicitly preventing the classifier from learning useful but non-robust features.
Their theory is presented under the parameter estimation framework, which is different from the
conventional adversarial training. Moreover, they do not discuss the convergence direction of adver-
sarial training.
2	Notations and Assumptions
In this section, we introduce the notations and assumptions we used in this paper. We consider a
binary classification problem.
Notations. Dataset is represented as {xi, yi}iN=1, where xi ∈ Rd and yi ∈ {-1, 1}. The loss
function is represented by '(∙). ∣∣∙ k means l2 norm in this paper. The objective of standard training
can be represented as
1N T
L0(w) = Nfo(W yiXi),
i=1
where w ∈ Rd is a linear classifier. Adversarial training has the objective of
1N
L(W) = w∑ll max-'(WTyix),
N i=1 kx-xik≤ε
(2)
(3)
where ε is the adversarial radius. An intuitive explanation of adversarial training is that it requires
the classifier perform well in the ε-ball centered at each data xi . In this paper, wt is the iterate at
step t of gradient descent (GD) under adversarial training. We use W to represent the solution of
SVM (8). Now, we give the definition of ε-strongly linearly separable data.
Definition 1 (ε-strongly linearly separable). 1 The dataset is ε-strongly linearly separable, if there
exists w* such that ∀Xi, w*TyiXi > 0 and w*TyiXi > ε∣∣w*k∙
The ε-strongly linearly separable means there exists a linear classifier that does not only give a right
prediction to each data point but also ensures all data are away from the linear classifier larger than
1Please notice that ε-strongly linearly separable is a stronger condition compared with linearly separable.
It distinguishes with the separability under soft margin which allows some w*tyiXi < 0 but w*TyiXi >
εkw*k∙
2
Under review as a conference paper at ICLR 2020
ε. In addition, linearly separable refers to choosing ε = 0 in Definition 1. We now define confidence
of classifier.
Definition 2 (Confidence). For a given linear classifier w of binary classification problem, the
logits is 1/(1 + e-wT xi )and e-wT xi /(1 + e-wTxi)for category 1 and -1. Then max{1/(1 +
e-wT xi), e-wT xi /(1 + e-wT xi)} is the confidence of w on data xi.
The confidence measures how confident the classifier is about the prediction. In the sequel, we use
xi to represent yixi for simplify the notations. We introduce the assumptions used in this paper.
Assumption 1. The loss function '(∙)2 satisfies that ∀u,'(u) > 0,'0(u) < 0, l00(u) ≥
0, limu→∞ '(u) = limu→∞ '0(u) = 0, and the '(u) has Lipschitz gradient.3
Assumption 2. The l(∙) and l0(∙) have exponential tail, which means there exists some ConStant uo
and C1, C2 satisfies that ∀u > u0, C1e-u ≤ l(u) ≤ C2e-u as well as l0(u).4
Assumption 3. The wT xi-εkwk has the range of [c1, ∞) for each xi, where c1 > -∞. kwk ≥ c2
for some constant c2.5.
Due to the properties of loss function, the adversarial training objective (3) with radius ε has an
explicit formula
1N
L(W) = N X' (wTxi- εkwk).
i=1
The adversarial training objective equation 4 is optimized by the gradient descent, i.e.,
Nw
Wt+1 = Wt - 〃▽£(Wt) = Wt - η X ' (WTXi - ε∣∣wtk) (Xi - ε 口：\ ),
(4)
(5)
where η is the learning rate.
3	Adversarial Training Converges Faster to Hard Margin for
ε-STRONGLY LINEARLY SEPARABLE DATA
In this section, we theoretically characterize where adversarial training converges when the data
themselves are ε-strongly linearly separable. We first have the following key lemma which ensures
the loss of adversarial training with radius ε can converge to zero on ε-strongly linearly separable
data. The proof of this lemma is delegated to Appendix A.
Lemma 1. The adversarial training objective equation 4 is convex and L-smooth for some positive
constant L. Then if the data are ε-strongly linearly separable (1), the iterates {wt} returned by
GD under adversarial training satisfy 1): limt→∞ L(wt) = 0, 2): limt→∞ kwtk = ∞. and 3):
limt→∞ wtT xi - εkwtk = ∞
The distance of xi away from linear classifier wt is |wtTxi|/kwtk. Then, the convergence result
in the Lemma 1 does not inform us the robustness of trained classifier. Fortunately, the results in
Soudry et al. (2017); Ji & Telgarsky (2018) claim that the iterates of standard training updated by
GD can converge to hard margin classifier with the rate of O((log t)-1). Now, we give the main
result of this section. The conclusion is similar to Theorem 3 in Soudry et al. (2017) while the
convergence is much sharper.
Theorem 1. For any ε-strongly linearly separable data (Definition 1), and loss function '(∙) satisfies
Assumption 1 and 2. If the Assumption 3 holds, then the gradient flow iterates w(t),
dd(t) = -RL(W(t)	(6)
satisfies
W(t) = W ∙ O (log t) + h(t),	(7)
2The loss is set to be positive, differentiable, monotonically decreasing convex function. Lots of general
used loss functions are satisfied, such as e-u, log (1 + e-u) etc.
3It means that ∣'0(u) — '0(v)∣ ≤ L|u — v| for some positive constant L.
4Lots of loss functions we used such as log (1 + e-u), e-u are satisfied with this assumption.
5This assumption can be attained by re-scale the norm of w if necessary.
3
Under review as a conference paper at ICLR 2020
for a large t. Here W is the hard margin solution ofSVM:
W = argmin ∣∣wk	s.t. wτXi ≥ 1.
w∈Rd
(8)
kh(t)k is in the order of o(log t). Then
lim
t→∞
w(t) W
MtI-W
≤ O((logt)-(1+ε*)),
(9)
where ε* = 罔 EN1-ε), {εi} is the sorted distance of data away from the hard margin solution of
SVM, and S = {i : W T Xi = εJ∣W ∣∣}; |S| is the number of elements in set S. Finally, iterates {wt}
of adversarial training updated by GD with step size η satisfies
∣Wk - W(kη)∣ ≤ O(η),
for k ∈ N+. Then we can conclude
,. Wt W 一，. Il w(tη)	W 〜、
lim	----—	— ~~~7-iτ	≤	lim	11 Ti—ʒ—— —	~~~i+	+ O(η)
t→∞	kWtk kWIl	-	t→∞	IIkWdn)k	∣wIl
(10)
(11)
We give a brief discussion to hard margin solution of SVM. Since the dataset is ε-strongly linearly
separable, by KKT condition, We have ε < 1∕∣W∣∣ which informs Us all the distance between Xi
and W are larger than ε. Then, we present that perturbations smaller than ε can be defended by
W. Hence, the linear classifier Wt becomes robust for a large t, since it has the same direction
with W. We then conclude adversarial training on ε-strongly linearly separable data helps iterates
{Wt } converge to a robust solution with a fast rate. A detailed proof of this Theorem is delegated to
Appendix C.
For the theorem, we have several remarks about the adversarial training on ε-strongly linearly sepa-
rable data.
•	Adversarial training converges to a robust solution, the same as standard training, with a
faster rate. The acceleration is determined by the constant ε* = (∣S∣ε(ει — ε))∕N which
related with the true margin ε1 and the number of support vectors |S|.
•	Large proportion of support vectors |S|/N, and an appropriate choice of ε (ε 1 /2 is the best)
increase the convergence rate.
We next study where adversarial training converges when the data are not ε-strongly linearly sepa-
rable.
4	ADVERSARIAL TRAINING WHEN DATA ARE NOT ε-STRONGLY LINEARLY
SEPARABLE
It’s hard to obtain the exact separability of data without verifying the hard margin solution of SVM
before adversarial training. Hence, understanding adversarial training on non-ε-strongly linearly
separable data is also crucial.
4.1	The Influence of Outliers
We consider the case that clean data are ε-strongly linearly separable but the whole data set is not
ε-strongly linearly separable because of outliers. We investigate how the outliers affect standard
training and adversarial training.
4.1.1	S tandard Training Is Unstable to Outliers
We now illustrate how standard training can be easily altered by outliers for both linearly and non-
linearly separable data.
As shown in Soudry et al. (2017); Ji & Telgarsky (2018), gradient descent converges to the hard
margin solution for the exponential loss and linear separable data. Although the hard margin solution
is believed to be a reasonably good classifier, it has potential to be non-robust, if one can intentionally
insert outliers to be support vectors. The following proposition presents that hard margin classifier
somehow can be non-robust.
4
Under review as a conference paper at ICLR 2020
Proposition 1. For two linearly separable datasets {x1}n=11 and {x2}n=21 , let w^ι and w^2 be
their hard margin classifier respectively. Then, the hard margin Classfier W of union dataset
{x1}N=11 s{x2}N=2ι SatiSfieS kwk ≥ maχ{kwιk, kw2k}.
This proposition can be derived from the definition of hard margin classifier, which suggests that
adding some extra outliers to dataset can easily effect the robustness of hard margin classifier, since
∣∣ιwk ≥ max{∣∣Wιk, ∣∣W2∣∣} and one of Wi can be extremely large. Hence, the non-robustness of
hard margin classifier can be attributed to some outliers. We use the next example to illustrate it.
Example 1. Suppose a dataset {(x1,0)}N=II is Well linearly separable with large margin, which
means the hard margin classifier W1 has a small norm. We insert some outliers which have theform
{(0, x2,)}NN2ι and Ni》N. Ifthe outlier is linearly separable with relatively small margin, the
corresponding hard margin solution is W2, then we have ∣Wιk《 ∣W2∣.
We can show that hard margin classifier on the union dataset {(x1,0)}NII U {(0, x2)}N=2ι is
(W 1, W2). Then for each (x1,0), We have
∣(w1,W2) (x1,0)I	IWTχ1∣	IWTχ11	IWTχ1∣
k (W 1,WG k = PkWik2 + kW2k2 ≤ kw2k《 kwιk
(12)
This shows the distance between each {(χi, 0)}N=II and new hard margin classifier (W 1, W2) be-
come extremely small due to a few outliers {(0, x2)}N=2ι∙
This indicates the solution returned by standard training is somewhat very sensitive to a few outliers
because outliers can easily alter the hard margin classifier.
We now give a more general description of outliers based on the understanding of Example 1. Given
two datasets {xi1}iN=11 and {xi2}iN=21 with N1, N2 data respectively, where N1 N2. {xi1}iN=11 them-
selves are ε-strongly linearly separable , but {xi2}iN=21 are not. Then, dataset {xi2}iN=21 is considered
as outliers. Outliers themselves can even be not linearly separable. We use the next example to
illustrate another behavior of standard training to non-linearly separable outliers.
Example 2. Dataset {(xi11, xi12)}iN=11 ∈ Rd1+d2 is ε-strongly linearly separable but
{(0, x22)}N=2ι ∈ Rd1+d2 is not linearly separable. Let S = span{x12,…，xN2ι}=
span{x22,…,xN2 }. Without loss ofgenerality, we assume {x11}N=II themselves are linearly sep-
arable but the hard margin classifier of them W11 has relatively small margin compared to the
{x12}N=11, i.e., ∣ιW 11 k》k W12 k, where W12 is the hard margin classifier on {x12}N=II6 *.
The union datasets {(xi11, xi12)}iN=11 U{(0, xi22)}iN=21 is not linearly separable because of the outliers
set {(0, xi22)}iN=21. Even worse, we show in this case, the classifier returned by standard training
can make the prediction most based on the useful but non-robust information {xi11}iN=11 (Ilyas et al.,
2019).
The reason is as follows. The outliers {(0, xi22 )}iN=21 happens to be the ”Strongly Convex Part”
as in Theorem 4.1 of Ji & Telgarsky (2018), then the iterates {Wt } updated by gradient de-
scent of standard training converge to a direction of the hard margin classifier W11 of dataset
{∏s⊥ (χ11, x12)}N=11 = {(x11,0)}n=11, where ∏s⊥ (∙) is the projection operator of space S⊥. Then,
the presence of outliers {(0, xi22)}iN=21 renders the classifier returned by standard training on union
datasets with poor robustness while the true hard margin classifier on dataset {(xi11, xi12)}iN=11 is
robust.
The two examples that standard training converges to non-robust solution at the presence of outliers
also demonstrates that standard training can easily capture the ”non-robust but useful information”.
The first is to classify outliers {(0, x22)}N=21 into correct category renders the classifier (W1, W2) to
be non-robust. The second describes that the outliers mute the information from {(0, xi12 )}iN=11 for
dataset {(xi11, xi12)}iN=11. Then, the classifier turns into a non-robust but useful classifiers.
6The hard margin classifier of {(x11, x12)}N=II W1 satisfies ∣∣W ik ≤ min{kW11k, ∣∣Wι2k} which leads to
a even more robust solution.
5
Under review as a conference paper at ICLR 2020
4.1.2	Adversarial Training is Stable to Outliers
Now, we turn to the behavior of adversarial training on the above two examples. Lemma 1 shows
that the iterates {wt} of adversarial training on ε-strongly linearly separable data updated by GD
can satisfy limt→∞ kwt k = ∞. But the non-ε-strongly linearly separability of data ensures there
exists some xik such that wtT xik - εkwtk ≤ 0 for each specific t. Hence, kwtk can not go to
infinity, otherwise we end up with an infinite loss. This is the essential distinction caused by non-ε-
strongly linearly separability. Now, we give a formal analysis to the stability of adversarial training
to outliers.
Due to Lemma 1, iterates of adversarial training updated by GD converge to a minimum w*. Then,
let l0 (w*TXk - ε∣∣w*k) = pk, We have
	2	.2、Nk	/	*、 VL(w*) = -	-- ^X xχPk (Xk — ε-		 ) = 0.	(13) I )Ni + N k=1 士Pt L	kw*k；
It gives	*	2 Nk	2 Nk	2 ε 岛 XX Pk = XX Pkxk = X Xk Pk,	(14) k=i i=i	k=i i=i	k=i
where Xk = (Xk,…,XNJ and Pk = (Pk, .…PNk). This implies that the direction of vector w*
is decided by a linear combination of data points. Xkpk represents the contribution brought by
{Xik}iN=k1.
When data are ε-strongly linearly separable, by Assumption 2, we see Pik /Pjk0 = o(1) (kwtk goes to
infinity) for any support vector Xik and non-support vector Xjk0 of w*/kw* k. Hence, the direction
of w* is mostly decided by support vectors. This is consistent with the conclusion that hard margin
solution of SVM W satisfies (KKT condition) W = Pk=ι PNkI akXk, where ak = 0 for non-
support vectors. Then, hard margin classifier is only decided by the linear combination of support
vectors, which could be potential instability of standard training to outliers, since outliers can easily
alter support vectors.
However, things become different for non-ε-strongly linearly separable data. Since kwt k can not go
to infinity, we have the relationPik/Pjk0 = O(1). Then the following inequality
λmin (XTX2) kP2k ≤ kX2P2k ≤ λmax (XTX2) kp2k
λmax (XTXi) kpιk - kXlPlk -》min (XTXi) kpιk
(15)
informs us the stability of adversarial training to outliers, where λmin (A), λmax(A) are respectively
the smallest and largest eigenvalue of matrix A. From equation 13, we see the direction of w* can
be decided by both dataset {Xi1}iN=11 and {Xi2}iN=21. Since Pi1 has the same scale with Pj2 for every i, j
and N1 N2, we see kp1 k	kp2k, then kX1p1 k	kX2p2k. Hence, equation 15 implies that
X2p2 has little contribution to the direction of w*. Thus, w* can not be the hard margin classifier
on union dataset. In addition, w* is mostly decided by data {Xi1}iN=11 which tells the stability of
adversarial training to outliers.
We now provide a discussion to the stability of adversarial training via the lens of robust/non-robust
but useful features proposed by Ilyas et al. (2019). First, we quote the definition of non-robust but
useful feature here.
Definition 3 (non-robust but useful feature (Ilyas et al., 2019)). For a given distribution (X, y), if
E(χ,y)[y∙ f(x)] ≥ P for some ρ > 0,but E(x,y) [inf ∣∣δk≤ε y ∙ f(x + δ)] ≤ 0, then f(∙) is non-robust
but useful feature.
The definition of robust feature can be generalized for f (∙) with E(χ,y) [inf ∣δ∣≤ε y ∙ f (X + δ)] ≥
0. We see the feature is defined by classifier f (∙) which is W for linear case. Then, the hard
margin solution of SVM Wι, W? for {x1}N=I and {x2}n2i in Example 1 and 2 are respectively
robust feature and non-robust feature. Further more, let W* be the classifier (feature) captured by
adversarial training. We note that W * being stable to outliers means adversarial training can prevent
the classifier capture information from non-robust but useful features in sharp contrast with standard
training. This interprets the conclusion in Ilyas et al. (2019) while they only empirically verify it.
6
Under review as a conference paper at ICLR 2020
We can also understand other scenarios from equation 15 that are not limited to N1 N2. If
N1 ≈ N2 , we see adversarial training can preserve a part of information from robust features.
Moreover, N2 N1 means that most features are non-robust, and then adversarial training is also
helpless.
4.2	Confidence of Adversarial Training
In this subsection, we give a characterization to the confidence of classifier returned by adversarial
training where data is not ε-strongly linearly separable. As shown in Section 4.1.2, kwtk converges
to some constant when data is not ε-strongly linearly separable. According to Definition 2, the
confidence of classifier is decided by |wtTxi| on each xi. Then, a bounded kwtk corresponds to a
relatively low confidence in each xi .
To give a formal description to the confidence of wt , we discuss an extremely large ε ≥
1 maxi,j ∣∣Xi 一 Xj∣∣, which means each ε-ball centered at Xi has overlap with each other. For a
Xi, larger |w*T Xi| corresponds with higher prediction confidence. For a given dataset {xi}N=ι, let
N1, N2 respectively be the number of data in each category and Xk,i represents the data from the
k-th category. Then we have following theorem descriping the confidence of classifier.
Theorem 2. Let l(u) = e-u, for classifier w* returned by adversarial training on non-linearly
SeparabIedata, ∖w*τ Xk,i∣ is either smaller than ε∣w*∣ or satisfies
Nk0
e(Iw*txk,il-εkw*k) ≤ ɪ X maχ	e(-w*Tx+εkw*k),	if w*τxk,i ≥ ε∣∣w*∣∣,
Nk0 j=1 x:kx-xk0,j k≤ε
Nk0
e(Iw*Txk,iI) ≤ NN- X e(-w*Txk0,j-εkw*k), if w*τxk,i ≤ -ε∣∣w*k, w*TXk，j ≥ 0,
k0 j =1
(16)
for k0 6= k.
Our results also applies to l(∙) discussed in this paper. A more explicit upper bound for those
w*T Xk,i ≥ ε∣w* ∣ is referred to equation 56. We notice that ∣w* ∣ is bounded on non-ε-strongly
linearly separable data. Then, this theorem implies that |w*T Xi | on each Xi can not be extremely
large. Hence, a relatively low confidence in each Xi. A detailed proof of this Theorem is delegated
to Appendix D. Here We use ε ≥ 1 maxij ∣Xi — Xj ∣∣ is out of simplicity, a large ε for adversarial
training can also correspond to a similar result.
Theorem 2 informs the training loss of classifier w* is at a relatively high level. Then, a simple
generalization error bound (Vapnik, 1995) can tell that w* can also end up with a high error on test
data. This is an interpretation to the model trained by adversarial training always face a poor test
accuracy (Madry et al., 2018).
5	Empirically S tudy
In this section, we compare adversarial training with standard training via linear classifier for two
scenarios, i.e., ε-strongly linearly separable data and non-ε-strongly linearly separable data. All
experiments are conducted with loss function `(u) = log (1 + e-u), and the update rule is GD with
learning rate 0.01.
5.1	EXPERIMENTS ON ε-STRONGLY LINEARLY SEPARABLE DATA
We first conduct a series of simulations to verify our conclusion on ε-strongly linearly separable data.
The dataset includes support and non-support vectors. We generate non-support vectors {xi}N=ι 〜
N(yi ∙ 2 ∙ I2,0.3 ∙ I2) and support vectors on the ITXi = yi ∙ 2. If there exists at least one support
vector in each category, the hard margin solution of SVM W is I2. See Figure 1 for an example. The
distance of support vectors away from the hard margin classifier is √2. Hence, the data are at most
√2-linearly separable. We respectively verify our conclusions in Theorem 1 of the convergence rate,
support vector number |S| and adversarial radius ε.
7
Under review as a conference paper at ICLR 2020
(a) Dataset with 5000 support vec- (b) Dataset with 2500 support vec- (c) Dataset with 1 support vector
tors in each category.	tors and 2500 non-support vectors and 4999 non-support vectors in
in each category.	each category.
Figure 1: Three types of datasets.
(a) Adversarial training with differ- (b) Adversarial training with differ- (c) Standard training with different
ent ε.	ent number of support vectors. number of support vectors
Figure 2: Gap between the direction of iterates and direction of hard margin classifier.
We conduct adversarial training with different ε and standard training on three datasets: number of
support vectors and non-support vectors in each category are respectively (5000, 0); (2500, 2500)
and (1, 4999) (see Figure 1). The results can be referred to Figure 2. Some extra experiments are
delegated to Appendix E. We should highlight the direction of {wt} here can converge with rate of
O((log0.01 ∙ t)-1) and O((log0.01 ∙ t)-(1+ε*)), respectively for standard training and adversarial
training due to equation 11.
The ”margin gap” in Figure 2 means distance between the direction of iterates wt /kwt k and hard
margin solution of SVM I2/√2. In Figure 2, the label, for example "Adv: 2500 support 2500 non
support: 0.1 varepsilon” means adversarial training with ε = 0.1 on dataset with 2500 support vec-
tors and 2500 non-support vectors in each category. From the results, we see that adversarial training
indeed accelerates convergence rate of wt /kwt k. On the other hand, both adversarial training and
standard training can benefit from the number of support vectors. Finally, as We claimed in Theorem
1, the best adversarial radius ε is ει∕2 which is V/2/2 here.
5.2	EXPERIMENTS ON NON-ε-STRONGLY LINEARLY SEPARABLE DATA
In this subsection, we conduct experiments on linearly separable (but non-ε-strongly linearly sepa-
rable) and non-linearly separable data. More extra experiments can be referred to Appendix E.
We first discuss the stability of adversarial training to outliers on linearly separable data correspond-
ing to Example 1. We generate {(x1,02)} 1==100 〜(N(yi ∙ 2 ∙ I2, 0.3 ∙ I2), 02) and {(02, x2)}121 〜
(02, N(yi ∙ 0.5 ∙ I2, 0.1 ∙ I2)). The number of the two datasets in each category are 5000 and 50.
The dataset {(02, xi2)}i1=001 is linearly separable while non-ε-strongly linearly separable for ε = 1.5.
{(xi1 , 02)}i1=00100 is ε-strongly linearly separable for ε = 1.5. {(02, xi2)}i1=001 can be viewed as out-
liers. Then, we compare the stability of adversarial training and standard training conducted on the
union dataset to outliers. The adversarial radius ε is 1.5. Detailed results can be referred to Figure
3a.
8
Under review as a conference paper at ICLR 2020
(a) Non-ε-strongly linearly separable but (b) Non-linearly separable data.
linearly separable data.
Figure 3: Adversarial and standard training on non-ε-strongly linearly separable but linearly sepa-
rable data and non-linearly separable data.
(a) Frequency of confidence for data point (b) Frequency of confidence for data point
under adversarial training	under standard learning
Figure 4: The confidence distributions of adversarial training and standard training when data are
linearly separable but ε ≥ 1 max。∣∣xi — Xj ∣∣. The y-axis is number of data.
For linearly separable data, the solid lines in Figure 3a are the norm of iterates returned by adversarial
training and standard training. The dash lines in Figure 3a, for example, ”Adv-sep/whole” means
the ratio between the norm of classifier decided by the ε-strongly linearly separable part that is
the first two dimensions of iterates and whole norm of iterates (kw1 k/kwk, where w ∈ R4 =
(w1, w2) ∈ R2+2). The first and the last two dimensions of iterates are with respect to robust and
non-robust features. We have two observations for adversarial training, first, the norm of iterates
can converge to some constant rather than infinity. Second, The direction of iterates is mostly
decided by {(xi1, 02)}i1=00100, since the”Adv-sep/whole” can converge to 1 as the iterate steps growing
up. However, we have an opposite observation for standard training, which means the direction of
classifier obtained by standard training is easily effected by some outliers {(02 , xi2)}i1=001.
Then, we present the results on non-linearly separable data with respect to Example 2, we
generate 5000 {(x11, x12)}1=0100 〜(N(yi ∙ 0.5 ∙ I2,0.1 ∙ I2),N(yi ∙ 2 ∙ I2,0.3 ∙ I2)) and 50
{(02, x2)}1=01 〜(02, N(—yi ∙ 0.5 ∙ I2,0.5 ∙ I2)) in each category. Here {(02, x2)}1=01 is the source
of non-linearly separability. {xi12}i1=00100 corresponds with a robust hard margin classifier while
{xi11}i1=00100 is not. {(02, xi2)}i1=001 can be viewed as outliers. We conduct adversarial training with
ε = 0.7. The results can be referred to Figure 3.
Figure 3b refers to non-linearly separable data, where the solid lines in it are also the norm of it-
erates. The first and the last two dimensions of iterates are respectively decided by the ”useful but
non-robust” part and ”robust” part. The non-linearly separability is from the last two dimensions of
data brought by outliers {(02, xi2)}i1=001. The dash lines in Figure 3b, for example, ”Adv:sep-nrob”
and ”Adv-non_sep-rob” are respectively the proportion in norm for the norm of the first and the last
two dimensions of iterates (kw1k/kwk and kw2k/kwk, where w ∈ R4 = (w1, w2) ∈ R2+2).
9
Under review as a conference paper at ICLR 2020
We see standard training can easily capture the information from ”useful but non-robust” (first two
dimensions) part while the information from the ”robust” part (last two dimensions) is muted at-
tributes to outliers {(02, xi2)}i1=001. But adversarial training can oppositely capture the information
from ”mostly robust” part.
Finally, we see the confidence of the classifier obtained by adversarial training with ε ≥
1 maxi,j kxi - Xj∣∣. We generate 5000 samples {xi}1==100 〜N(±0.5 ∙ I2,0.1 ∙ I2) in each Cate-
gory. The data are linearly separable but non-ε-strongly linearly separable for ε = 1, besides that,
1 ≥ 1 maxi,j ∣Xi - Xj∣. We compare the confidence in each data point for classifiers returned by
adversarial training with ε = 1 and standard training. The distributions of confidence via classifiers
in data can be referred to Figure 4. We see the classifier obtained by adversarial training corresponds
with nearly 50% confidence in all data. However, the confidence of classifier returned by standard
training is almost 100% in all data. Hence, a large ε for adversarial training can hurt the prediction
confidence of classifier.
6 Conclusion
In this paper, we give a theoretical characterization to adversarial training for linear classifiers under
various settings. We conclude that on ε-strongly linearly separable data, adversarial training helps
iterates converge to the hard margin classifier with a rapid rate compared with standard training. It
means iterates of adversarial training can be robust with less update steps. Furthermore, we charac-
terize the adversarial training on non-ε-strongly linearly separable data. We show both theoretically
and empirically that adversarial training can be more stable to the outliers of the dataset but standard
training is not. Finally, we discuss the confidence of the classifier obtained by adversarial training.
We prove that under the condition of ε ≥ 1 maxij ∣Xi - Xj∣, the confidence of classifier obtained
by adversarial training keeps in a low level. This reveals that a large ε for adversarial training is not
a wise choice.
References
Sanjeev Arora, Simon Hu Wei Du, S, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. Deepdriving: Learning affordance
for direct perception in autonomous driving. 2015 IEEE International Conference on Computer
Vision (ICCV), 2015.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. arXiv preprint arXiv:1704.08847, 2017.
Francesco Croce, Maksym Andriushchenko, and Matthias Hein. Provable robustness of relu net-
works via maximization of linear regions. Proceedings of the 22nd International Conference on
Artificial Intelligence and Statistics (AISTATS), 2019.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. Proceedings of the 36th International Conference on Machine
Learning (ICML), 2019.
F. Gamaleldin Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large
margin deep networks for classification. arXiv preprint arXiv: arXiv:1803.05598, 2018.
J. Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. Proceedings of the 35th International Conference on Machine
Learning (ICML), 2018.
Kaiming He, Xiangyu Zhang, Ren Shaoqing, and Jian Sun. Deep residual learning for image recog-
nition. arXiv preprint arXiv:1512.03385, 2015.
10
Under review as a conference paper at ICLR 2020
Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier
against adversarial manipulation. arXiv preprint arXiv:1705.08475, 2017.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. arXiv preprint arXiv:1905.02175,
2019.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gener-
alization in neural networks. Advances in Neural Information Processing Systems 31 (NeurlPS),
2018.
Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint
arXiv:1803.07300, 2018.
A Krizhevsky, Sutskever I., and E.G. Hinton. Imagenet classification with deep convolutional neural
networks. Advances in Neural Information Processing Systems 25 (NeurlPS), 2012.
Alexey Kurakin, J. Ian Goodfellow, and Samy Bengio. dversarial examples in the physical world.
arXiv preprint arXiv:1607.02533, 2016.
Guang-He Lee, David Alvarez-Melis, and Tommi Jaakkola, S. Towards robust, locally linear deep
networks. arXiv preprint arXiv:1907.03207, 2019a.
Jaehoon Lee, S Samuel Xiao, Lechao Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jef-
frey Pennington. Wide neural networks of any depth evolve as linear models under gradient
descent. arXiv preprint arXiv:1902.06720, 2019, 2019b.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
arXiv preprint arXiv:1906.05890, 2019.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. Proceedings of the International
Conference on Representation Learning (ICLR), 2018.
Mor Nacson, Shpigel, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable
data: Exact convergence with a fixed learning rate. arXiv preprint arXiv:1806.01796, 2018.
Thiago Serra, Christian Tjandraatmadja, and Srikumar Ramalingam. Bounding and counting linear
regions of deep neural networks. In Proceedings of the 35th International Conference on Machine
Learning (ICML), 2018.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. arXiv preprint arXiv:1710.10345, 2017.
Weijie Su, Stephen Boyd, and Candes J. Emmanuel. A differential equation for modeling nesterov’s
accelerated gradient method: Theory and insights. arXiv preprint arXiv:1503.01243, 2015.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2017.
Vladimir Vapnik. Convex Optimization. Springer, Data mining and knowledge discovery, 1995.
11
Under review as a conference paper at ICLR 2020
A Proof of Lemma 1
Proof. We first give convexity to the adversarial training objective. For any two w1 , w2, and 0 ≤
λ ≤ 1, by the convexity and monotone decreasing property of '(∙), We have
1N
N X' (λwTXi + (1 - λ)wTXi - ε∣∣λwι + (1 - λ)w2∣∣
≤
i=1
1N
N X' (λwTXi + (1 - λ)wTXi - ελ∣∣wιk- (1 - λ)∣∣w2∣∣
(17)
≤
i=1
1N
N X λ' (w1 Xi- ε∣∣wιk) +(1 - λ)' (w2Xi- ε∣∣w2∣∣),
i=1
which implies the convexity of adversarial training objective. On the other hand, if UTV2L(w)u
can be bounded by Lkuk2 for some L, then L(w) is L-smooth. We see that
▽2L(W) = ⅛ X'00(WTXi-εkWk) (Xi - ε⅛)(Xi - ε高)T	(18)
-扁3 ClWT Xi- εkWk)(kwk2I- WwT).
Then, uT V2L(w)u can be divided into two parts, we respectively compute them
1N
N Xl00(WTXi - ckWk)
i=1
N
2
)2 - 2 M UTXiwTU + 时(UTW)2
(19)
≤ N kuk2 (λmaχ (XTX)+2ε X kXik +2ε2).
Here X is matrix (XT, ∙∙∙ , XN)T and λmθχ (XTX)is the largest eigenvalue of XTX. On the other
hand, according to Assumption 3,
uT H [/(WTXi - ε∣∣Wk)(kWk2I - WwT)u ≤ 2εC1 ∣∣uk2.
kW k3	c2
(20)
Combining the two above equations, we conclude that largest eigenvalue of V2L(w) is bounded for
some constant, which result in the L-smoothness of adversarial training objective. It’s a well known
(Boyd & Vandenberghe, 2004) that for a convex and L-smooth function f (∙), GD with step size
η = L will ensure the iterates {χt} satisfies f (Xt) — f (x*) ≤ O (t). Since the adversarial training
loss goes to zero only if wtTXi - εkwtk goes to infinity for each Xi. It reveals our two conclusions
limt→∞ ∣∣wtk = ∞ and limt→∞ WTXi — ε∣∣wtk = ∞.	口
B Order of Norm
In this section we give the order of ∣w(t)∣ on ε-strongly linearly separable data, which is highly
related to the convergence rate of ∣∣ 高知 — 启^ ∣∣. Our proof is based on the gradient flow, we first
bound the difference between flow iterates w(t) and wt.
Lemma 2. Let Wt be the iterates updated by GD equation 5, then for dwt = -VL(w(t)), we
have
kW(kη) - Wk k ≤ O(η).
Proof. Let
W(kη) = W((k — 1)η) — ηVL(W((k — 1)η)).
(21)
(22)
12
Under review as a conference paper at ICLR 2020
Then by LiPschitz gradient of L(∙), We see
kη
kw(kη) — w(kη)k = Il /	VL(w(u)) — VL(w((k — 1)η))du
(k-1)η
Zkη
L ∣w(u) - w((k - 1)η)∣ du
(23)
kη
(k-1)η
O(η2).
VL(w(s))ds du
(k-1)η
u
Then We have
∣∣w(kη) - Wkk ≤ Ilwk - w(kη)k + ∣∣w(kη) - w(kη)k
≤ kw((k - ι)η) - wk-ιk + η ∣∣VL(w((k - ι)η)) - vL(wk-ι)∣ + O(η2)
≤ (1 + ηL) ∣w((k - 1)η) - wk-1 ∣ + O(η2)
≤ …≤ (i + ηL)k kw(0) - wok + O(η)
= O(η),
(24)
for w(0) = w0, Which results in the conclusion.
□
Before Proving the Theorem 1, We give a lemma to illustrate that w(t) Will converge to the direction
of W.
Lemma 3. There exists a t0, for t > t0, we have
w(t) = ρ(t)w + h(t),	(25)
for some ρ(t) and h(t). Here ρ(t) goes to infinity and kh(t)k is in the order of o(ρ(t)).
Proof. Let
r(t) = w(t) - kwt)k w,	(26)
∣∣r(t)k = o (kw(t)k) will implies the conclusion. Since '0(∙) has exponential tail Assumption 2,
there exists a t0, for t > t0, We have
d kr(t)k
IrT(t)r(t)
kr(t)k
1-
w(t)τw ∖
kw(t)kkw k)
VL(w(t))T r(t)
1
≤
Then, we see
N ∣C(t)k (I - Cos(W⑴，W)) X exp (-w(t)Txi + ε∣w(t)k) (Xi-ε ∣w∣^) r⑴.
i=1	(27)
N
X exp -w(t)T xi + εkw(t)k
i=1
w(t) T
-ε MtlJ r(t)
=X exp (-r(t)Txi - kw(t)wwτxi + εkw⑴k) (xi-ε*) r(t)
≤X eχp (-kw(t)ww Txi + εkw(t)k)- ε eχp (-r(t)Txi- kw(t)ww Txi + εkw⑴k) %r⑴
≤ N exp (-(ε1 - ε)kw(t)k) ,
(28)
where W is the hard margin classifier and ε is the distance between support vectors and W. Here we
use the relationship that ze-z ≤ 1 for any z and w(t)T r(t) ≥ 0. Since
(1 — cos(w(t), w))
r(t)T w
kw(t)kkwk ,
(29)
13
Under review as a conference paper at ICLR 2020
and r(t)TW ≤ 0, We have
d kr(t)k ≤ -CN kr(t)k(WTwkkwk exp (Tε1 - C)M(Uk) ≤ kWCNk exp(-(ε1 - ε)kw(t)k). (30)
Similar to Theorem 3 in SU et al. (2015), by exponential tail of l(∙) Assumption 2, We have
Ci exp (—w(t)TXi + ε∣∣w(t)k) ≤ ' (w(t)TXi — ε∣∣w(t)k) ≤ Nkw2t w0k ,	(31)
for some constant C1 and any xi . Then We see
log Mll 2C1U- + logt ≤ w(t)TXi — ε∣∣w(t)∣∣.	(32)
N kw — w0 k
On the other hand, by the definition of hard margin classifier and Lemma 1, there exists xi such that
w(t)T xi ≤ ε1 kw(t)k. Combining this and equation 32, We have
ε⅛ (log NkwC- wok+logt) ≤ kw(t)k.	(33)
Plugging this into equation 30, and we see that there exists a tι such that 11 logt ≥ log NkWC-w0k,
then
kr(t)k≤kr(to ∧ ti)k + (C1—C)NCw - w0 k Zt 焉 dt = O(loglog t),	(34)
CC1	t0 ∧t1 t log t
for t ≥ to ∧ tι. Combining these, we conclude that kr(t)k is in the order of o(∣∣w(t)k).	□
Next, We use a lemma to illustrate the explicit order of ρ(t)
Lemma 4. ρ(t) in Lemma 3 is on the scale of εlθg-∣，where {εi} is the Sorted distance of {xi}N=ι to
hard margin solution of SVM W.
Proof. From Lemma 3 and 3, we have
，、	，、w ∙，、
w(t) = p(t) kwk + h(t),
(35)
where ρ(t) → ∞ and kh(t)k = o(ρ(t)). Specifically, the h(t) can be chosen to be orthogonal with
W, otherwise we can use a decomposition in a direct sum. For ∣∣w(t)k, we have
kw(t)k = √ρ(t)2 + ∣∣h(t)k2
=Pρ(t)2 + kh(t)k2 — ρ(t) + ρ(t)
(Pρ(t)2 + kh(t)k2 + ρ(t)) (pρ(t)2 + kh(t)k2 — ρ(t))
√ρ(t)2 + ∣∣h(t)k2 + ρ(t)
+ ρ(t)
(36)
kh(t)k2
—ρ(t) + √ρ(t)2 + kh(t)k2
On the other hand, for t large enough,
dt kw(t)k = -VL(w(t))T 湍
+ ρ(t).
≤ N X exP
i=1
—w(t)T Xi + Ckw(t)k	Xi — C
w (t)	T w(t)
kw(t)k
N X exP (-P(t)
i=1
N
≤ C2 X
≤ N乙
i=1
wTX
kw k + h(t)T Xi + ερ(t) + ε
exp (—(Ci — C)ρ(t)) (Ci — C).
kw(t)k
kh(t)k2
ρ(t) + √ρ(t)2 + kh(t)k2
W T Xi — εkw k
kw k
(37)
14
Under review as a conference paper at ICLR 2020
for some constant C1 , C2. Since ρ(t) goes to infinity and kh(t)k = o(ρ(t)), the derivation of
伯)+√kh(t)∖也仕川2 will o(ρ0(t)). Similar to equation 37, we can get the lower bound of £∣∣w(t)k.
In summary, we have
CN3 exp (-(ει - ε)ρ(t))(ει - ε) ≤ ρ0(t) ≤ C2 exp (-(ει - ε)ρ(t))(εN - ε)	(38)
for constant some C3 . Hence we can conclude that
-logL + C5 ≤ ρ(t) ≤ -logL + C4,	(39)
ε1 - ε	ε1 - ε
for some constant C4, C5. It results in ρ(t) = O (log t).	□
We have proven that w(t) will converge to the direction of W. Now, we will show w(t) will have
the same support vector with W when t is large. It,s a key fact of proving Theorem 1.
Lemma 5. There exists to such that w(t) will have the same Support vectors with W for t > to.
Proof. Let S = {i : WTXi = εJ∣W∣∣}, for i ∈ S,j ∈ S, we have
w(t)T
kw(t)k
(xi - xj)
T
kwk + h⑴)
(xi - xj)
(40)
kw(t)k
1
≤-------
一kw(t)k
(xi - xj) + kh(t)kkxi - xjk
wT
Since Xi is the support vector of W, ^wj(Xi 一 Xj) > 0. Then, ∣∣h(t)∣ = o(ρ(t)) shows that
Ww(tt” (Xi 一 Xj) ≤ 0 for a large t. Then we get the conclusion.	□
C Proof of Theorem 1
In this section, we give a fully characterization to the proof of Theorem 1.
Restate of Theorem 1. For any ε-strongly linearly separable data (Definition 1), and loss function
'(∙) satisfies Assumption 1 and 2. Ifthe Assumption 3 holds, then the gradient flow iterates W(t),
竽= TL(W⑼
satisfies
w(t) = W ∙ O (log t) + h (t),
for a large t. Here W is the hard margin solution ofSVM:
W = arg min ∣∣w∣∣
w∈Rd
s.t. WTxi ≥ 1.
∣h(t)∣ is in the order of o(log t). Then
lim
t→∞
w(t) W
MtI-M
≤ O((∣ogt)-(1+ε*)),
(41)
(42)
(43)
(44)
where ε* = 罔 ENI-E), {εi} is the Sorted distance of data away from the hard margin solution of
SVM, and S = {i : WT Xi = εJ∣W ∣∣}; |S| is the number of elements in set S. Finally, iterates {Wt}
of adversarial training updated by GD with step size η will satisfy
kWk - W(kη)k ≤ O(η),
for k ∈ N+. Then we can conclude
lim R -国 ≤ lim ,w(tη),l -国 + O(η)
t→∞Il时-阿Il t→∞ii Mtm-M||+ (/)
(45)
(46)
15
Under review as a conference paper at ICLR 2020
Proof. Let w(t) = ρ(t)kW^j + h(t). Denoting the support vectors set by S, S = {i : WTXi
ει k Wk}. By the exponential tail of '0(u) (Assumption 2), there exists a to, for t > to, We have
1 d kh(t)k2
— (vL(w(t)) + P(t)高)T h(t) = -VL(w(t))τh(t)
C
N
≤ N
eχp(-w⑴Xi + εkw(t)k) (Xi - ε kW(t)k) h⑴
(/,∖wτXi	, /,∖T .	/,∖ ,
-ρ(t) -ψw^k——h(t) Xi + ερ(t) +
kh(t)k2
ρ(t) + √ρ(t)2 + kh(t)k2
w(t)	T
xi-ε MtlJ h(t)
「。「 W 小WTXi 〃小T ,小W W	w(t) V, m
≤ N AeXP (-ρ(t)^k< - h(t) Xi + ερ⑴J(Xi-εh⑴
≤ - N X exp (-kh(t)kkXik - (εi - ε)ρ(t))	,
i∈S
(47)
7Here we use a fact that XTh(t) ≤ 0 for i ∈ S, due to W is the hard margin classifier. In addition,
1 + -⅛≡ ≤ exP ("lbw
(48)
With this, we can choose a to such that
X exP (-P⑴ Wwxi — h(t)TXi + εP⑴)(Xi- εkW^) h⑴
i∈S
kh(t)k2
ρ(t) + √ρ(t)2 + kh(t)k2
+ X exp(-w(t)Xi + εkw(t)k) (Xi - ε 口：(；)口 ) h(t) ≤ 0,
(49)
for t > to. This concludes the equation equation 47. Let kh(t)kkXik be C(t), we can first derive
that kh(t)k will converge to zero, then C(t) can be close to zero. By Gronwall’s inequality, we have
llh(t)k2 ≤ kh(tO)k2exP Zj- - εNρ(u) X exP (-(εi-C)P(U)) du) ,	(50)
C(t) can be arbitrary small when t > to. Plugging this into equation 50, and combining Lemma 4,
we have
kh(t)k2 ≤ kh(to)k2 exp ( Zt — εNClt-U Xu-(⅛≡Idu
t0	N logu i∈S
=kh(to)k2 exp (一 X "JN " Z -τ1— du]
i∈S N	t0ulogu
=kh(to)k2 exp (一|S|C(N - ε) log log t)
=O(iog t)-ε*,
where ε* = lSlε(NI-E). Since We have
w(t)	ρ(t)	W	h(t)
∣W(t)∣ = j0(t)+ kh(t)k2 丽 + j0(t)+ kh(t)k2,
Il —II ρ(t)+ ρ(t)	11	11 P(t)+ ρ(t)
(51)
(52)
7Here we hide the constant C in the last inequality, which is decided by loss function l(∙). C will usually
smaller than 1 in fact.
16
Under review as a conference paper at ICLR 2020
(a) Adversarial training with different ε.	(b) Adversarial training with different num-
ber of support vectors.
Figure 5: Gap between the direction of iterates and direction of hard margin classifier in l4 space.
(a) Adversarial training with different ε.	(b) Adversarial training with different num-
ber of support vectors.
Figure 6: Gap between the direction of iterates and direction of hard margin classifier in l∞ space.
then we see
w(t)	W
---------	
kw(t)k-kW k
≤(1_	ρ(t)	ʌ +	kh(t)k
—	Mt) + kh(t)k2	+ o(t) + kh(t)k2
P(t)+ ρ(t) )	P(t)+ ρ(t)
=kh(t)k2	+	kh(t)k
=P(t)2 + kh(t)k2 + ρ(t) + 嘴F
=O (log) —
(53)
When t goes to infinity. Since h(t) ≤ O (log t)-ε*, for ε* = lSlε(NI-E). Combining Lemma 2, We
can get the conclusion.	口
D	Proof of Theorem 2
Proof. The xk,i will either satisfies |w*Txk,i∣ ≤ ε∣∣w*k nor |w*Txk,i∣ ≥ ε∣∣w*k∙ We know
the first inequality represents the distance between xk,i and w* is smaller than ε. Then for
w*τxk,i ≥ ε∣∣w*k, due to ε ≥ 2 max」∣∣xk,i — xk∕,j∣∣ for any i,j, there exists x1」j ∈
B2(xk0,j , ε) T B2(xk,i, ε) with k0, k = 1, 2 and k0 6= k. By triangle inequality, the distance be-
*T
tween x], j and w* is smaller than "的*厂 一 ε. Since xιoj locates in different category with xk,i,
by the monotone decreasing property of '(∙),we have
'(-w*TXk,i + ε∣∣w*k) ≤ '(xkoj)
≤ max	'(w*τ x).
kx-xk0,j k≤ε
(54)
17
Under review as a conference paper at ICLR 2020
(a) Non-ε-strongly linearly separa-
ble but linearly separable data.
(b) Non-linearly separable data.
(c) Frequency of confidence for
data point under adversarial train-
ing
Figure 7: Adversarial training in l4 space. Outliers are added into dataset of the first two subfig-
ure. The third subfigure is confidence distribution for adversarial training when data are linearly
separable but ε ≥ 2 max,/∣∣x, - Xj ∣∣. The y-axis in the third pictures is number of data.
(a) Non-ε-strongly linearly separa-
ble but linearly separable data.
(b) Non-linearly separable data.
(c) Frequency of confidence for
data point under adversarial train-
ing
Figure 8: Adversarial training in l∞ space. Outliers are added into dataset of the first two subfig-
ure. The third subfigure is confidence distribution for adversarial training when data are linearly
separable but ε ≥ 2 maxi,j ∣x, — Xj ∣∣. The y-axis in the third pictures is number of data.
Hence we have
1 Nk0
'(-w*Txk,i + ε∣∣w*k) ≤ —— T max	'(w*τX - ε∣∣w*k).	(55)
Nk0 j=1 kx-xk0,j k≤ε
Then, we have
N2' (-∣w*τxι,i| + εkw*k) + Ni' (-∣w*TX2,i| + εkw*k) ≤ (N + N2)'(0),	(56)
if w* can correctly predict a point far from margin larger than ε in each category, for those
w*τXk,i ≥ ε∣w*∣ due to w* is minimum, then L(w*) ≤ L(0). On the other hand, for
w*T Xk,i ≤ -ε∣w* ∣, we can immediately derive
'(w*TXk,i) ≤ '(-w*TxkOj- ε∣∣w*k),	(57)
by triangle inequality where w*TXk，，j ≥ 0. Then We get the conclusion.	□
E Extra Experiments
E.1 ADVERSARIAL TRAINING IN lp SPACE
Our conclusions are obtained for adversarial training in l2 space while it can be conducted in a more
general lp space (Goodfellow et al., 2014; Madry et al., 2018). It is meaningful to verify whether
these conclusions are still hold in lp space. Hence, we extend our experiments to lp space.
18
Under review as a conference paper at ICLR 2020
The adversarial training objective within linear classifier can be formulated as
1N
L(W) = Nn X
i=1
max '(wτ x),
kx-xi kp ≤ε
(58)
in lp space, where ∣∣ ∙ ∣∣p is Ip norm. It has an explicit formulation
L(w)
(59)
where 1 + 1 = 1. We conduct experiments for P = 4 andP = ∞.
We empirically verify that adversarial training in lp space helps accelerating the convergence to hard
margin solution of SVM on ε-strongly linearly separable data. Besides that, adversarial training
can benefit from more support vectors and an appropriate adversarial radius ε. For non-ε-strongly
linearly separable data, we try to validate that adversarial training in lp space is stable to outliers
while it will obtain a classifier with low confidence on each data. All the experiments we conducted
here are following the settings in Section 5, expect for the P are respectively chosen as 4 and ∞.
In addition, we should notice that the distance between support vectors locate in 1dT x = ±d and
hard margin classifier 1d in lp space is ^^^^-. Hence We should adjust the adversarial radius ε for
different P.
We first verify the conclusion for ε-strongly linearly separable data. The results for P = 4 and
P = ∞ can be respectively referred to Figure 5 and 6. Noticing that the distance between support
vectors and hard margin solution of SVM is = dP. Then for P = 4, P = ∞ and d = 2, we
have 2 4 ≈ 1.18, 20 = 1. From the results, we see that an appropriate adversarial radius (ε = ει∕2)
and more support vectors are helpful for adversarial training even in lp space.
Now, we pay our attention to non-ε-strongly linearly separable data. We respectively verify our
conclusions about stability and confidence of adversarial training. The experimental settings are
again same with Section 5. The results can be referred to Figure 7 and 8. We see that our conclusions
are still hold in lp space, i.e. classifier obtained by adversarial training is stable to outliers and the
confidence of it will consist keep in a low level for a large ε.
As a matter of fact, we can generalize our theoretical results to lp space by following the methods
in this paper. The inner product〈•，•〉of linear classifier is derived from ∣∣ ∙ ∣∣2 while l2 is a Hilbert
space, but lp space is not. The inner product we used does not math the lp space. Hence, we need
adding some extra bounded conditions to extend our conclusions to lp space.
E.2 Adversarial Training for Neural Network
Our conclusions are derived from linear classifier, but Du et al. (2019); Jacot et al. (2018); Lee et al.
(2019b); Arora et al. (2019) suggest that over-parameterized neural networks of sufficient width (or
infinite width) evolve as linear models with Neural Tangent Kernel (NTK). Hence our conclusions
can somehow represent the performance of adversarial training for neural network models. In this
subsection, we try to empirically verify our conclusions about adversarial training within neural
network model.
We use CIFAR10 (Krizhevsky et al., 2012) to conduct our experiments. CIFAR10 is a dataset with
10 categories. To simplify the experiments, we only keep the first two categories, which turns
the experiments into binary classification problem. The model we used for adversarial training is
ResNet20 (He et al., 2015), which is a CNN with 20 layers. The experiments are conducted in l2
and l∞ space.
Since we can not compute the exact distance between data and classifier, we use the loss on training
data 8 with perturbation to represent the robustness of trained model. The perturbations are founded
by running 10 steps projected gradient descent (PGD). We handle adversarial training by following
the settings in Madry etal. (2018). We use PGD to find the argmaXχRχ-χi∣∣≤ε '(x, θ) for each Xi.
8Here we only focus on training data because we do not discuss the generalization of model obtained by
adversarial training.
19
Under review as a conference paper at ICLR 2020
(a) Adversarial training with different ε for (b) Frequency of confidence for data point
outliers exist or not.	under adversarial training with an extremely
large ε.
Figure 9: Performance of adversarial training in l2 space
(a) Adversarial training with different ε for (b) Frequency of confidence for data point
outliers exist or not.	under adversarial training with an extremely
large ε.
Figure 10: Performance of adversarial training in l∞ space
The steps of PGD is 10 for each xi. All the models are trained by stochastic gradient descent with
0.1 learning rate and 0.9 momentum parameter for 100 epochs. The loss function is set to be cross
entropy.
We first validate our conclusions for ε-strongly linearly separable data. Since neural network is not a
linear model, itis hard to construct extra support vectors like in Section 5. Hence, we can only verify
our conclusion that iterates of adversarial training converge faster to a robust solution, and it will
benefit from an appropriate choice of adversarial radius ε. To compare the influence of ε, we choose
ε = 0.25, 1.0, 2.0 in l2 space and ε = 2/255, 8/255, 16/255 in l∞ space. The size of perturbations
are from Madry et al. (2018). Besides that, the perturbations on training set 9 to verify the robustness
of trained model are 1.0 and 8/255 respectively in l2 space and l∞ space. The experimental results
in l2 and l∞ space can be referred to Figure 9a 10 and 10a. The label of curves, for example, ”Adv:
0.25 varepsilon” means adversarial training with ε = 0.25. From the figures, we see that the models
returned by adversarial training with ε = 1 and ε = 8/255 are respectively the most robust in l2 and
l∞ space. Hence, the conclusion that adversarial training can benefit from an appropriate choice of
ε also holds for neural network.
Then, for non-ε-strongly linearly separable data, we respectively discuss the influence of outliers and
a large ε. First, We construct 50 outliers {xi}1=01 〜N(±0.078125 ∙ I32×32×3,0.045 ∙ I32×32×3) in
each category in both l2 and l∞ space. The influence of outliers for adversarial training in l2 and
l∞ space can be respectively referred to Figure 9a and 10a. ”Adv With outliers: 0.25 varepsilon”
9We should highlight that the perturbations are recalculated after each epoch of training.
10We only list the results of adversarial leaning, because the loss on data With perturbation of standard
training Will not converge (alWays larger than 2.0).
20
Under review as a conference paper at ICLR 2020
means adversarial training with outliers added into dataset and ε = 0.25. From the results, we see
that outliers will barely effect the performance of adversarial training. Hence, we can conclude that
adversarial training is still stable to outliers, even the model is neural network. On the other hand, we
respectively choose ε equal to 4 and 32/255 in l2 and l∞ space to verify the confidence of classifier
returned by adversarial training. The distributions of confidence via adversarial training in l2 and
l∞ space can be respectively referred to Figure 9b and 10b. The results reveal that the confidence of
neural network trained by adversarial training with large ε is at a fairly low level in each data.
To summary, although our conclusion about adversarial training are derived from linear classifier,
the empirically results suggest it can be somehow extended to the case of neural networks. Thus, a
theoretical exploration for neural network is crucial in the future work.
21