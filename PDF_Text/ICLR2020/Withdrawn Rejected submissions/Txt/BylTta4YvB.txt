Under review as a conference paper at ICLR 2020
How Well Do WGANs Estimate the Wasser-
stein Metric ?
Anonymous authors
Paper under double-blind review
Ab stract
Generative modelling is often cast as minimizing a similarity measure between
a data distribution and a model distribution. Recently, a popular choice for the
similarity measure has been the Wasserstein metric, which can be expressed in the
Kantorovich duality formulation as the optimum difference of the expected values
of a potential function under the real data distribution and the model hypothesis.
In practice, the potential is approximated with a neural network and is called
the discriminator. Duality constraints on the function class of the discriminator
are enforced approximately, and the expectations are estimated from samples.
This gives at least three sources of errors: the approximated discriminator and
constraints, the estimation of the expectation value, and the optimization required
to find the optimal potential. In this work, we study how well the methods, that
are used in generative adversarial networks to approximate the Wasserstein metric,
perform. We consider, in particular, the c-transform formulation, which eliminates
the need to enforce the constraints explicitly. We demonstrate that the c-transform
allows for a more accurate estimation of the true Wasserstein metric from samples,
but surprisingly, does not perform the best in the generative setting.
1	Introduction
Recently, optimal transport (OT) has become increasingly prevalent in computer vision and machine
learning, as it allows for robust comparison of structured data that can be cast as probability measures,
e.g., images, point clouds and empirical distributions (Rubner et al., 2000; Lai & Zhao, 2014), or
more general measures (Gangbo et al., 2019). Key properties of OT include its non-singular behavior,
when comparing measures with disjoint supports, and the fact that OT inspired objectives can be seen
as lifting similarity measures between samples to similarity measures between probability measures.
This is in stark contrast to the more traditional information theoretical divergences, which rely on only
comparing the difference in mass assignment. Additionally, when the cost function c is related to a
distance function dby c(x, y) = dp(x, y), p ≥ 1, the OT formulation defines the so called Wasserstein
metric, which is a distance on the space of probability measures, i.e. a symmetric and positive definite
function that satisfies the triangle inequality. Despite its success, scaling OT to big data applications
has not been without challenges, since it suffers from the curse of dimensionality (Dudley, 1969;
Weed & Bach, 2017). However, significant computational advancements have been made recently,
for which a summary is given by Peyre et al. (2θ19). Notably, the entropic regularization of OT
introduced by Cuturi (2013) preserves the geometrical structure endowed by the Wasserstein spaces
and provides an efficient way to approximate optimal transport between measures.
Generative modelling, where generators are trained for sampling from given data distributions, is
a popular application of OT. In this field, generative adversarial networks (GANs) by Goodfellow
et al. (2014) have attracted substantial interest, particularly due to their success in generating photo-
realistic images (Karras et al., 2017). The original GAN formulation minimizes the Jensen-Shannon
divergence between a model distribution and the data distribution, which suffers from unstable
training. Wasserstein GANs (WGANs) (Arjovsky et al., 2017) minimize the 1-Wasserstein distance
over the l2 -metric, instead, resulting in more robust training.
The main challenge in WGANs is estimating the Wasserstein metric, consisting of estimating
expected values of the discriminator from samples (drawn from a model distribution and a given
data distribution), and optimizing the discriminator to maximize an expression of these expected
1
Under review as a conference paper at ICLR 2020
values. The discriminators are functions from the sample space to the real line, that have different
interpretations in different GAN variations. The main technical issue is that the discriminators have
to satisfy specific conditions, such as being 1-Lipschitz in the 1-Wasserstein case. In the original
paper, this was enforced by clipping the weights of the discriminator to lie inside some small box,
which, however, proved to be inefficient. The Gradient penalty WGAN (WGAN-GP) (Gulrajani et al.,
2017) was more successful at this, by enforcing the constraint through a gradient norm penalization.
Another notable improvement was given by the consistency term WGAN (CT-WGAN) (Wei et al.,
2018), which penalizes diverging from 1-Lipschitzness directly. Other derivative work of the WGAN
include different OT inspired similarity measures between distributions, such as the sliced Wasserstein
distance (Deshpande et al., 2018), the Sinkhorn divergence (Genevay et al., 2018) and the Wasserstein
divergence (Wu et al., 2018). Another line of work studies how to incorporate more general ground
cost functions than the l2 -metric (Adler & Lunz, 2018; Mallasto et al., 2019; Dukler et al., 2019).
Recent works have studied the convergence of estimates of the Wasserstein distance between two
probability distributions, both in the case of continuous (Klein et al., 2017) and finite (Sommerfeld
& Munk, 2018; Sommerfeld, 2017) sample spaces. The decay rate of the approximation error of
estimating the true distance with minibatches of size N is of order O(N -1/d) for the Wasserstein
distances, where d is the dimension of the sample space (Weed & Bach, 2017). EntroPic regularized
optimal transport has more favorable sample complexity of order O(1/√N) for suitable choices of
regularization strength (see Genevay et al. 2019 and also Feydy et al. 2018; Mena & Weed 2019). For
this reason, entropic relaxation of the 1-Wasserstein distance is also considered in this work.
Contribution. In this work, we study the efficiency and stability of computing the Wasserstein metric
through its dual formulation under different schemes presented in the WGAN literature. We present a
detailed discussion on how the different approaches arise and differ from each other qualitatively, and
finally measure the differences in performance quantitatively. This is done by quantifying how much
the estimates differ from accurately computed ground truth values between subsets of commonly
used datasets. Finally, we measure how well the distance is approximated during the training of
a generative model. This results in a surprising observation; the method best approximating the
Wasserstein distance does not produce the best looking images in the generative setting.
2	Optimal Transport
In this section, we recall essential formulations of optimal transport to fix notation.
Probabilistic Notions. Let (X, dX) and (Y, dY) be Polish spaces, i.e., complete and separable metric
spaces, denote by P(X) the set of probability measures on X, and let f : X → Y be a measurable
map. Then, given a probability measure μ ∈ P(X), we write f#M for the push-forward of μ with
respect to f, given by f#R(A) = μ(f T(A)) for any measurable A ⊆ Y. Intuitively speaking, if ξ
is a random variable with law μ, then f (ξ) has law f#M. Then, given V ∈ P(Y), we define
ADM(μ,ν) = {γ ∈ P(X X Y)MnI)#Y = μ, (n2)#Y = V},	(I)
where ∏ denotes the projection onto the ith marginal. An element Y ∈ ADM(μ, V) is called an
admissible plan and defines ajoint probability between μ and V.
Optimal Transport Problem. Given a continuous and lower-bounded cost function c : X × Y → R,
the optimal transport problem between probability measures μ ∈ P(X) and V ∈ P(Y) is defined as
OTc(μ,ν) :=	min	EY[c],	(2)
γ∈ADM(μ,ν)
where Eμ[f] = JX f (χ)dμ(χ) is the expectation of a measurable function f with respect to μ.
Note that (2) defines a constrained linear program, and thus admits a dual formulation. From
the perspective of WGANs, the dual is more important than the primal formulation, as it can be
approximated using discriminator neural networks. Denote by L1(μ) = {f: X → R | Eμ [f] < ∞}
the set of measurable functions of μ that have finite expectations under μ, and by ADM(C) the set of
admissible pairs (夕,ψ) that satisfy
夕(x)+ ψ(y) ≤ c(x,y),	∀(x,y) ∈X×Y,夕 ∈ L1(μ),ψ ∈ L1 (v ).	(3)
Then, the following duality holds (Peyre et al., 2019, Sec. 4)
OTc(μ,ν) =	sup	{Eμ3 + EV [ψ]}.	(4)
(φ,ψ)∈ADM(c)
2
Under review as a conference paper at ICLR 2020
When the supremum is attained, the optimal 夕*, ψ* in (4) are called Kantorovich potentials, which, in
particular, satisfy 夕*(x) + ψ*(y) = c(x, y) for any (x, y) ∈ SuPP(Y*), where γ* solves (2). Given
夕，we can obtain an admissable ψ satisfying (4) through the C-transform of 夕，
ψc ： Y → R, y → inf {c(χ,y)- o(χ)},	⑸
x∈X
so that (夕,夕C) ∈ ADM(C) for any 夕 ∈ L1(μ). Moreover, the Kantorovich potentials satisfy ψ = 夕c,
and therefore (4) can be written as (Villani, 2008, Thm. 5.9)
OTc(μ,ν) =	max	{跖3+ EV[夕 CF	⑹
(θ,0c)∈ ADM(c)
In other words, the ADM(C) constraint can be enforced with the C-transform, and reduces the
optimization in (6) to be carried out over a single function.
Wasserstein Metric. Consider the case when X = Y and C(x, y) = dpX (x, y), p ≥ 1, where we
refer to dX as the ground metric. Then, the optimal transport problem (2) defines the p-Wasserstein
metric Wp(μ, V):=OTdp (μ, V) 1 on the space
PdX (X) = {μ ∈ P(X) / dχ(xo,x)dμ(x) < ∞} , for some xo ∈ X,	(7)
of probability measures with finite p-moments. It can be shown that (Pdp (X), Wp) forms a complete,
separable metric space (Villani, 2008, Sec. 6, Thm 6.16).
Entropy Relaxed Optimal Transport. We can relax (2) by imposing entropic penalization intro-
duced by Cuturi (2013). Recall the definition of the Kullback-Leibler (KL) divergence from V to μ as
KL
(μkν) = Ibg
X
PμdX,
(8)
where we assume that μ, V are absolutely continuous with respect to the Lebesgue measure X on X
with densities Pμ,Pν, respectively. Using the KL-divergence as penalization, the entropy relaxed
optimal transport is defined as
OTc(μ,ν):=	min	{Eγ [c] + EKL(γ∣∣μ 0 V)} ,	(9)
γ∈ADM(μ,ν)
where e > 0 defines the magnitude of the penalization, and μ 0 V denotes the independent joint
distribution of μ and v. We remark that when E → 0, any minimizing sequence (γe)e>o solving (9)
converges to a minimizer of (2), and in particular,OTc(μ, V) → OTc(μ, V).
Analogously to (4), the entropy relaxed optimal transport admits the following dual formulation
(Peyre et al., 2019; Feydy et al., 2018; Di Marino & Gerolin, 2019)
OTc(μ, V) =	SuP	∣Eμ3+ EV[ψ] 一 eEμ0ν exp ( C +("㊉ ψ) ) 一 1 ∖ , (10)
0∈L1(μ),ψ∈L1(ν) I	L ∖ E /JJ
where (夕㊉ψ)(x,y)=夕(x) + ψ(y). In contrastto (4), this is an unconstrained optimization problem,
where the entropic penalization can be seen as a smooth relaxation of the constraint.
As shown by Feydy et al. (2018); Di Marino & Gerolin (2019), a similar approach to (6) for computing
the Kantorovich potentials can be taken in the entropic case, by generalizing the C-transform. Let
Lexp(μ) := {g : X → R | Eμ[exp(g∕e)] < ∞}, and consider the (c, e)-transform of φ ∈ Lexp(μ),
dcQ(y) = -Elog ({ exp (一""'y)E- "(x)) dμ(x)) .	(11)
As E → 0,夕(c,e)(y) → 夕c(y), making the (c, e)-transform consistent with the c-transform. Analo-
gously to (6), one can show under mild assumptions on the cost C (Di Marino & Gerolin, 2019), that
OTc(μ, V)
max、{E“5 + Eν[dc") ]}.
0∈Le p(μ) I	J
(12)
Sinkhorn Divergence. Since the functional OTC fails to be positive-definite (e.g.OTc(μ, μ) = 0),
itis convenient to introduce the (p, E)-Sinkhorn divergence Sp with parameter E > 0, given by
Sp(μ, ν)=OTdX (μ, ν)1 - 2 (OTdX (μ,μ)1 + OTdX(V,ν)1),	(13)
3
Under review as a conference paper at ICLR 2020
where the terms OTdp (μ, μ)1 and OTdp (ν,ν)P are added to avoid bias, as in general
OTdp (μ, μ)1 = 0. The Sinkhorn divergence was introduced by Genevay et al. (2018), and has the
following properties: (i) it metrizes weak convergence in the space of probability measures; (ii) it
interpolates between maximum mean discrepancy (MMD), as → ∞, and the p-Wasserstein metric,
as → 0. For more about the Sinkhorn divergence, see Feydy et al. (2018).
3	Generative Adversarial Networks
Generative Adversarial Networks (GANs) are popular for learning to sample from distributions. The
idea behind GANs can be summarized as follows: given a source distribution μs ∈ P(Rns), we want
to push it forward by a parametrized generator gω0 : Rns → Rnt , so that a chosen similarity measure
P between the pushforward distribution and the target distribution μt ∈ P(Rnt) is minimized.
Usually the target distribution is only accessible in form of a dataset of samples, and one considers
an ‘empirical’ version of the distributions. Note that ns nt is chosen, which is justified by the
manifold hypothesis. This objective can be expressed as
min P((g3，)#〃s,〃t).	(14)
ω0
The similarity P is commonly estimated with a discriminator 夕ω : Rnt → R, parametrized by ω,
whose role will become apparent below.
The vanilla GAN (Goodfellow et al., 2014) minimizes an approximation to the Jensen-Shannon (JS)
divergence between the push-forward and target, given by
JS(Vkμ) ≈ max {Eχ〜μ [log(Pω(x))]+ Ey〜V [log(1 —夕ω(y))]} ,	(15)
ω
for probability measures μ and ν. The discriminator 夕ω is restricted to take values between 0 and 1,
assigning a probability to whether a point lies in μ or V. It can be shown (Goodfellow et al., 2014),
that at optimality the JS-divergence is recovered in (15), if optimized over all possible functions.
Substituting μ = μt and V = (gs，)#Ms in (15) yields the minimax objective for the vanilla GAN
min max {Eχ 〜μt [lθgWω (x))] + Ez 〜μs [lθg(1 一 ψω (gω√(z)))]} .	(16)
ω0	ω
As mentioned above, in practice one considers empirical versions of the distributions so that the
expectations are replaced by sample averages.
The Wasserstein GANs (WGANs) (Arjovsky et al., 2017) minimize an approximation to the 1-
Wasserstein metric over the l2 ground metric, instead. The reason why the p = 1 Wasserstein case is
considered is motivated by a special property of the c-transform of 1-Lipschitz functions, when c = d
for any metric d: if f is 1-Lipschitz, then fc = —f (Villani, 2008, Sec. 5). It can also be shown, that
a Kantorovich potential 夕* solving the dual problem (6) is 1-Lipschitz when C = d, and therefore the
WGAN minimax objective can be written as
min max {Eχ〜*=[夕幻(x)] 一 Ez〜μs [2ω (gω0 (z))]} .	(17)
ω0 ω
In the WGAN case, there is no restriction on the range of ψω as opposed to the GAN case above.
The assumptions above require enforcing ψω to be 1-Lipschitz. This poses a main implementational
difficulty in the WGAN formulation, and has been subjected to a considerable amount of research.
In this work, we will investigate the original approach by weight clipping (Arjovsky et al., 2017) and
the popular approach by gradient norm penalties for the discriminator (Gulrajani et al., 2017). We
furthermore consider a more direct approach that computes the c-transform over minibatches (Mal-
lasto et al., 2019), avoiding the need to ensure Lipschitzness. We also discuss an entropic relaxation
approach through (c, )-transforms over minibatches, introduced by Genevay et al. (2016). In the
original work, the discriminator 夕ω is expressed as a sum of kernel functions, however, in this work
we will consider multi-layer perceptrons (MLPs), as we do with the other methods we consider.
3.1	Estimating the 1 -Wasserstein Metric
In the experimental section, we will consider four ways to estimate the 1-Wasserstein distance between
two measures μ and V, these being the weight clipping (WC), gradient penalty (GP), c-transform and
4
Under review as a conference paper at ICLR 2020
(c, )-transform methods. To this end, we now discuss how these estimates are computed in practice
by sampling minibatches of size N from μ and V, yielding {χi}N=ι and {yi}N=ι, respectively, at
each training iteration. Then, with each method, the distance is estimated by maximizing a model
specific expression that relates to the dual formulation of the 1-Wasserstein distance in (6) over the
minibatches. In practice, this maximization is carried out via gradient ascent or one of its variants,
such as Adam (Kingma & Ba, 2014) or RMSprop (Tieleman & Hinton, 2012).
Weight clipping (WC). The vanilla WGAN enforces K-Lipschitzness of the discriminator at each
iteration by forcing the weights Wk of the neural network to lie inside some box -ξ ≤ Wk ≤ ξ,
considered coordinate-wise, for some small ξ > 0 (ξ = 0.01 in the original work). Here k stands for
the kth layer in the neural network. Then, the identity for the c-transform (with c = d) of 1-Lipschitz
maps is used, and so (17) can be written as
max
ω
(18)
i=1
i=1
.
Gradient penalty (GP). The weight clipping is omitted in WGAN-GP, by noticing that the 1-
LiPSchitz condition implies that ∣∣Vχ^ω(x)k ≤ 1 holds for X almost surely under μ and V. This
condition can be enforced through the penalization term Ex〜X [ max (0,1 -IlVx夕ω (X)Il)2 1, where
χ is some reference measure, proposed to be the uniform distribution between pairs of points of the
minibatches by Gulrajani et al. (2017). The authors remarked that in practice it suffices to enforce
IlVx夕ω (X)Il = 1, and thus the objective can be written as
max
ω
GX
i=1
1N	λM
夕 ω (Xi)- N X g ω (Ui)- M X (I -IlVz=Zi 夕 ω (Z)Il)
i=1	i=1
(19)
)
where λ is the magnitude of the penalization, which was chosen to be λ = 10 in the original paper,
and {zi}iM=1 are samples from χ.
c-transform. Enforcing 1-Lipschitzness has the benefit of reducing the computational cost of
the c-transform, but the enforcement introduces an additional cost, which in the gradient penalty
case is substantial. The ADM(c) constraint can be taken into account directly, as done in (q, p)-
WGANs (Mallasto et al., 2019), by directly computing the c-transform over the minibatches as
Ψω(yi) ≈ C(yi) = min {c(xj,yi) -中ω(Xj)},	QO)
j
where c = d2 in the 1-Wasserstein case. This amounts to the relatively cheap operation of computing
the row minima of the matrix Aij = c(Xj,yi) 一 夕ω(Xj). The original paper proposes to include
penalization terms to enforce the discriminator constraints, however, this is unnecessary as the
c-transform enforces the constraints. Therefore, the objective can be written as
(21)
(c, )-transform. As discussed in Section 2, entropic relaxation applied to W1 results in the (1, )-
Sinkhorn divergence S1, which satisfies S1 → W1 as → 0. Then, S1 can be viewed as a smooth
approximation to Wi. The benefits of this approach are that 夕ω is not required to satisfy the ADM(C)
constraint, and the resulting transport plan is smoother, providing robustness towards noisy samples.
The expression (13) for S1 consists of three terms, where each results from solving an entropy
relaxed optimal transport problem. As stated by Feydy et al. (2018, Sec. 3.1), the terms OT； (μ, μ)
and OT1 (V, V) are straight-forward to compute, and tend to converge within couple of iterations
of the symmetric Sinkhorn-Knopp algorithm. For efficiency, we approximate these terms with one
Sinkhorn-Knopp iteration. The discriminator is employed in approximating OT； (μ, ν), which is
done by computing the (c, )-transform defined in (11) over the minibatches
夕ω(yi) ≈ 2A)(yi) = -e log ( N X exp (—；(夕ω(Xj) - c(Xj,yi))” ,	(22)
5
Under review as a conference paper at ICLR 2020
and so we write the objective (12) for the discriminator as
f 1 N	1 N -----
max Nf中3(Xi) + N^ψ(ωc'e')(yj)
3 I N i=1	N j=1
(23)
4 Experiments
We now study how efficiently the four methods presented
in Section 3.1 estimate the 1-Wasserstein metric. The
experiments use the MNIST (LeCun et al., 1998), CIFAR-
10 (Krizhevsky et al., 2009), and CelebA (Liu et al., 2015)
datasets. On these datasets, we focus on two tasks: ap-
proximation and stability. By approximation we mean
how well the minibatch-wise distance between two mea-
sures can be computed, and by stability how well these
minibatch-wise distances relate to the 1-Wasserstein dis-
tance between the two full measures.
Implementation. In the approximation task, we model
the discriminator as either (i) a simple multilayer percep-
tron (MLP) with two hidden layers of width 128 and ReLU
activations, and a linear output, or (ii) a convolutional neu-
ral network architecture (CNN) used in DCGAN (Radford
Figure 1: Estimating the distance be-
tween two standard 2-dimensional Gaus-
sian distributions that have been shifted
by ±[1, 1].
et al., 2015). In the stability task we use the simpler MLPs
for computational efficiency. The discriminator is trained
by optimizing the objective function using stochastic or
batch gradient ascent. For the gradient penalty method,
we use the Adam optimizer with learning rate 10-4 and
beta values (0, 0.9). For weight clipping we use RMSprop
with learning rate 5 × 10-5 as specified in the original paper by Arjovsky et al. (2017). Finally, for
the c-transform and the (c, )-transform, we use RMSprop with learning rate 10-4.
The estimated distances dest obtained from the optimization are compared to ground truth values
dground computed by POT1. The (c, )-transform might improve on the POT estimates when dest >
dground, as both values result from maximizing the same unconstrained quantity. This discrepancy
cannot be viewed as error, which we quantify as
err
(dest , dground )
max(0, dground
- dest).
(24)
Note that this is a subjective error based on the POT estimate that can also err, and not absolute error.
In practice POT is rather accurate, and we found this to make only a small difference (see Figure 2
with the ground truth and estimated distances visualized). The weight clipping and the gradient
penalty methods might also return a higher value than POT, but in this case it is not guaranteed that
the discriminators are admissible, meaning that the constraints of the maximization objective would
not be satisfied. In the case of the c-transform, the discriminators are always admissible. However,
the POT package’s ot.emd (used to compute the 1-Wasserstein distance ground truth) does not utilize
the dual formulation for computing the optimal transport distance, and therefore we cannot argue in
the same way as in the (c, )-transform case. As the Sinkhorn divergence (13) consists of three terms
that are each maximized, we measure the error as the sum of the errors given in (24) of each term.
Approximation. We divide the datasets into two, forming the two measures μ and V, between which
the distance is approximated, and train the discriminators on 500 training minibatches μk ⊂ μ and
νk ⊂ ν, k = 1, ..., 500, of size 64. See Section 3.1 for how the discriminator objectives. Then,
without training the discriminators further, we sample another 100 evaluation minibatches μl ⊂ μ
and νl0 ⊂ ν, l = 1, ..., 100, and use the discriminators to approximate the minibatch-wise distance
between each μl and ν0. This approximation will then be compared to the ground-truth minibatch-
wise distance computed by POT. We run this experiment 20 times, initializing the networks again
each time, and report the average error in Table 1. Note that the discriminators are not updated for
the last 100 iterations. Results for a toy example between two Gaussians are presented in Fig. 1.
1Python Optimal Transport, https://pot.readthedocs.io/.
6
Under review as a conference paper at ICLR 2020
MLP	MNIST	CIFAR10	CelebA
WC	14.98 ± 0.32	27.26 ± 0.61	48.65 ± 1.29
GP	14.89 ± 0.38	27.14 ± 0.87	48.00 ± 2.88
c-transform	0.82 ± 0.16	1.53 ± 0.29	2.84 ± 0.49
(c, 0.1)-transform	0.43 ± 0.17	1.29 ± 0.48	2.52 ± 1.28
(c, 1)-transform	(1.12 ± 4.76) × 10-10	(0.26 ± 5.97) × 10-4	0.04 ± 0.26
ConvNet	MNIST	CIFAR10	CelebA
WC	20.73 ± 18.59	27.28 ± 0.63	48.72 ± 1.33
GP	14.78 ± 0.54	25.20 ± 24.32	96.19 ± 77.90
c-transform	0.79 ± 0.16	1.00 ± 0.26	2.11 ± 0.46
(c, 0.1)-transform	0.42 ± 0.17	0.60 ± 0.41	1.74 ± 1.13
(c, 1)-transform	(0.23 ± 1.90) × 10-8	(0.40 ± 3.60) × 10-13	0.02 ± 0.17
Table 1: Approximation. For each method, the discriminators are trained 20 times for 500 iterations
on minibatches of size 64 drawn without replacement, after which training is stopped and the error
between the ground truth and the estimate are computed.
MNIST	(c, 1)-transform	c-transform	GP	WC
N = 512, M = 64	17.20 ± 0.16	13.87 ± 0.23	4.25 ± 0.49	2.10 ± 0.26
N = 512, M = 512	16.95	12.64	4.21	2.03
N = 64, M = 64	17.45 ± 0.06	14.12 ± 0.13	1.54 ± 0.25	1.12 ± 0.13
N = 64, M = 512	16.76	11.4	1.49	1.08
Ground truth	14.22	12.65	12.65	12.65
CIFAR10	(c, 1)-transform	c-transform	GP	WC
N = 512, M = 64	29.98 ± 0.28	26.44 ± 0.25	11.04 ± 1.16	3.85 ± 0.67
N = 512, M = 512	29.41	24.77	11.10	4.00
N = 64, M = 64	29.67 ± 0.41	26.21 ± 0.40	3.25 ± 0.53	2.19 ± 0.23
N = 64, M = 512	29.18	24.16	3.59	2.34
Ground truth	26.10	24.78	24.78	24.78
CelebA	(c, 1)-transform	c-transform	GP	WC
N = 512, M = 64	50.55 ± 0.86	46.56 ± 0.89	28.07 ± 10.61	19.18 ± 73.86
N = 512, M = 512	48.42	43.06	28.17	20.93
N = 64, M = 64	50.80 ± 0.91	46.83 ± 0.86	10.24 ± 7.31	13.98 ± 39.54
N = 64, M = 512	47.60	41.80	10.10	15.20
Ground truth	43.74	43.07	43.07	43.07
Table 2: Stability. The discriminators are trained using two training batch sizes, N = 64 and
N = 512. Then, the distances between the measures are estimated, by evaluating the discriminators
on the full measures (of size M = 512), or by evaluating minibatch-wise with batch size M = 64.
Presented here are the distances approximated by each way of training the discriminators.
As Table 1 shows, the c-transform approximates the minibatch-wise 1-Wasserstein distance far better
than weight clipping or gradient penalty, and (c, )-transform does even better at approximating the
1-Sinkhorn divergence. The low errors in this case are due to the (c, )-transform outperforming the
POT library, which results in an error of 0 on many iterations, which also explains why the error
variance is so high in the = 1 case.
Stability. We train the discriminators for 500 iterations on small datasets of size 512, that form subsets
of the datasets mentioned above. We train with two minibatch sizes N = 64 and N = 512. We then
compare how the resulting discriminators estimate the minibatch-wise and total distances, that is,
the evaluation minibatches are of size M = 64 and M = 512. Letting ΨMethod be the objective
presented in 3.1 for a given method, We train the discriminator by maximizing ΨMethod(φ, μN,vn),
and finally compare ΨMethod(φ, μM, VM) for different M. The results are presented in Table 2. An
experiment on CIFAR10 Was carried out to illustrate hoW the distance estimate betWeen the full
measures behaves When trained minibatch-Wise, Which is included in Appendix A.
The ground truth values computed using POT are also included, but are not of the main interest
in this experiment. The focus is on observing hoW different batch-sizes on training and evaluation
affect the resulting distance. For the c-transform and (c, )-transform, the results varies depending on
Whether the distances are evaluated minibatch-Wise or on the full datasets. On the other hand, for the
gradient penalty and Weight clip methods, the training batch-size has more effect on the result, but
the minibatch-Wise and full evaluations are comparable.
7
Under review as a conference paper at ICLR 2020
c-transform
Iteration
Figure 2: Approximating the minibatch-wise distances while training a generator. Left: generated
faces after 5 × 104 generator iterations. Right: True and estimated log-distances between sampled
batches. The large values of GP and WC can be attributed to the fact that they are susceptible to
failure in enforcing the Lipschitz constraints on the discriminator.
WGAN training. Finally, we measure how the different methods fare when training a Wasserstein
GAN. For this, we use the architecture from DCGAN (Radford et al., 2015) to learn a generative
model on the CelebA dataset. During this training, we compute the POT ground truth distance between
presented minibatches, and compare these to the estimated distances given by the discriminators.
This is carried out for a total of 5 × 104 generator iterations, each of which is accompanied by 5
discriminator iterations. For (c, )- and c-transform the discriminators are evaluated on the fake
samples, which gave similar results to evaluating on the real samples. The results are presented in
Fig. 2. The results clearly show how the (c, )-transform and c-transform estimate the Wasserstein
distances at each iteration better than gradient penalty or weight clipping. However, the resulting
images are blurry and look like averages of clusters of faces. The best quality images are produced by
the gradient penalty method, whereas weight clipping does not yet converge to anything meaningful.
We include the same experiment ran with the simpler MLP architecture for the discriminator, while
the generator still is based on DCAN, in Appendix B.
5	Discussion
Based on the experiments, (c, )-transform and c-transform are more accurate at computing the
minibatch distance and estimating the batch distance than the gradient penalty and weight clipping
methods. However, despite the lower performance of the latter methods, in the generative setting they
produce more unique and compelling samples than the former. This raises the question, whether the
exact 1-Wasserstein distance between batches is the quantity that should be considered in generative
modelling, or not. On the other hand, an interesting direction is to study regularization strategies in
conjunction with the (c, )- and c-transforms to improve generative modelling with less training.
The results of Table 2 indicate that the entropic relaxation provides stability under different training
schemes, endorsing theoretical results implying more favorable sample complexity in the entropic
case. In contrast to what one could hypothetise, the blurriness in Fig. 2 seems not to be produced by
the entropic relaxation, but the c-transform scheme.
Finally, it is interesting to see how the gradient penalty method performs so well in the generative
setting, when based on our experiments, it is not able to approximate the 1-Wasserstein distance so
well. What is it, then, that makes it such a good objective in the generative case?
8
Under review as a conference paper at ICLR 2020
References
Jonas Adler and Sebastian Lunz. Banach Wasserstein GAN. In Advances in Neural Information
Processing Systems,pp. 6755-6764, 2018.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. arXiv preprint
arXiv:1701.07875, 2017.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in
neural information processing systems, pp. 2292-2300, 2013.
Ishan Deshpande, Ziyu Zhang, and Alexander Schwing. Generative modeling using the sliced
Wasserstein distance. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 3483-3491, 2018.
Simone Di Marino and Augusto Gerolin. An Optimal Transport approach for the Schrodinger bridge
problem and convergence of Sinkhorn algorithm. in preparation, 2019.
Richard Mansfield Dudley. The speed of mean Glivenko-Cantelli convergence. The Annals of
Mathematical Statistics, 40(1):40-50, 1969.
Yonatan Dukler, Wuchen Li, Alex Lin, and Guido Montufar. Wasserstein of Wasserstein loss for
learning generative models. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings
of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 1716-1725, 2019.
Jean Feydy, Thibault Sejourne, FrangOiS-XaVier Vialard, Shun-ichi Amari, Alain Trouve, and Gabriel
Peyre. Interpolating between optimal transport and MMD using Sinkhorn divergences. arXiv
preprint arXiv:1810.08278, 2018.
Wilfrid Gangbo, Wuchen Li, Stanley Osher, and Michael Puthawala. Unnormalized optimal transport.
arXiv preprint arXiv:1902.03367, 2019.
Aude Genevay, Marco Cuturi, Gabriel Peyre, and Francis Bach. Stochastic optimization for large-
scale optimal transport. In Advances in Neural Information Processing Systems, pp. 3440-3448,
2016.
Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning generative models with sinkhorn di-
vergences. In Amos Storkey and Fernando Perez-Cruz (eds.), Proceedings of the Twenty-First
International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of
Machine Learning Research, pp. 1608-1617, 2018.
Aude Genevay, LenaiC Chizat, Francis Bach, Marco Cuturi, and Gabriel Peyre. Sample complexity
of sinkhorn divergences. In Kamalika Chaudhuri and Masashi Sugiyama (eds.), Proceedings
of Machine Learning Research, volume 89 of Proceedings of Machine Learning Research, pp.
1574-1583, 2019.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of Wasserstein GANs. In Advances in Neural Information Processing Systems,
pp. 5767-5777, 2017.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for
improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thierry Klein, Jean-Claude Fort, and Philippe Berthet. Convergence ofan estimator of the wasserstein
distance between two continuous probability distributions. 2017.
9
Under review as a conference paper at ICLR 2020
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Rongjie Lai and Hongkai Zhao. Multi-scale non-rigid point cloud registration using robust sliced-
Wasserstein distance via Laplace-Beltrami eigenmap. arXiv preprint arXiv:1406.3758, 2014.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324,1998.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Anton Mallasto, Jes Frellsen, Wouter Boomsma, and Aasa Feragen. (q, p)-Wasserstein GANs:
Comparing ground metrics for Wasserstein GANs. arXiv preprint arXiv:1902.03642, 2019.
Gonzalo Mena and Jonathan Weed. Statistical bounds for entropic optimal transport: sample
complexity and the central limit theorem. arXiv preprint arXiv:1905.11882, 2019.
Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport. Foundations and Trends® in
Machine Learning, 11(5-6):355-607, 2019.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover’s distance as a metric for
image retrieval. International journal of computer vision, 40(2):99-121, 2000.
Martin Sommerfeld. Wasserstein distance on finite spaces: Statistical inference and algorithms. 2017.
Max Sommerfeld and Axel Munk. Inference for empirical wasserstein distances on finite spaces.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80(1):219-238, 2018.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31,
2012.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Jonathan Weed and Francis Bach. Sharp asymptotic and finite-sample rates of convergence of
empirical measures in Wasserstein distance. arXiv preprint arXiv:1707.00087, 2017.
Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, and Liqiang Wang. Improving the improved training of
wasserstein gans: A consistency term and its dual effect. In International Conference on Learning
Representation (ICLR), 2018.
Jiqing Wu, Zhiwu Huang, Janine Thoma, Dinesh Acharya, and Luc Van Gool. Wasserstein divergence
for GANs. In Computer Vision - ECCV 2018, pp. 673-688, Cham, 2018. Springer International
Publishing. ISBN 978-3-030-01228-1.
A	Appendix: Stability during Training
Related to the Stability experiment, we train discriminators modelled by the simple MLPs (see
Section 4) to approximate the distance between two measures μ and ν, which both are subsets of the
CIFAR10 dataset of size 512. We train for 15000 iterations with training minibatch size of 64, and
report the estimated minibatch-wise distances with the estimated distances between the full measures
in Fig. 3.
The experiments demonstrate, how (c, )-transform and c-transform converge rapidly compared to
gradient penalty and weight clipping method, which have not entirely converged after 15000 iterations.
Also visible is the bias resulting form minibatch-wise computing of the distances compared to the
distance between the full measures.
10
Under review as a conference paper at ICLR 2020
(c, l)-transform
c-transform
Gradient Penalty
Figure 3: Stability. While training the discriminators on minibatches, taken from two measures
consisting of 512 samples from CIFAR10, we also report the estimated distance between the full
measures.
Weight Clip
0	2500 5000 7500 10000 12500 15000
Iteration
B Appendix: WGAN Training with MLP
We repeat the WGAN training experiment with the simpler MLP architecture (see Section 4) for the
discriminators. The distance estimates at each iteration are given in Fig. 4, and generated samples in
Fig. 5.
The training process for (c, )- and c-transforms is more unstable with the MLP, as notable in
the sudden jumps in the true distance between minibatches. This seems to be caused when the
discriminator underestimates the distance. The fluctuation between estimated distances is much
higher in the gradient penalty and weight clipping cases, but the decrease in the true distance between
minibatches is still consistent. Notice how the fluctuation decreases considerably when we use a
ConvNet architecture for the gradient penalty method in Fig. 2.
11
Under review as a conference paper at ICLR 2020
(c, l)-transform
Iteration
Weight Clip
-4 ------ True
0	20000	40000
Iteration
Figure 4: Repeating the experiment presented in Fig. 2, but with the simpler MLP architecture for
the discriminator. Presented here are the estimated batchwise distances at each iteration against the
ground truth.
12
Under review as a conference paper at ICLR 2020
(c, 1)-transform
c-transform
Gradient Penalty
Figure 5: Repeating the experiment presented in Fig. 2, but with the simpler MLP architecture for
the discriminator. Presented here are generated samples after 5 × 104 iterations.
Weight Clip
13