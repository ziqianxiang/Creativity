Under review as a conference paper at ICLR 2020
Neural Networks for Principal Component
Analysis: A New Loss Function Provably
Yields Ordered Exact Eigenvectors
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose a new loss function for performing principal component
analysis (PCA) using linear autoencoders (LAEs). Optimizing the standard L2
loss results in a decoder matrix that spans the principal subspace of the sample
covariance of the data, but fails to identify the exact eigenvectors. This downside
originates from an invariance that cancels out in the global map. Here, we prove
that our loss function eliminates this issue, i.e. the decoder converges to the exact
ordered unnormalized eigenvectors of the sample covariance matrix. For this new
loss, we establish that all local minima are global optima and also show that com-
puting the new loss (and also its gradients) has the same order of complexity as
the classical loss. We report numerical results on both synthetic simulations, and a
real-data PCA experiment on MNIST (i.e., a 60, 000 × 784 matrix), demonstrating
our approach to be practically applicable and rectify previous LAEs’ downsides.
1	Introduction
Ranking among the most widely-used and valuable statistical tools, Principal Component Analysis
(PCA) represents a given set of data within a new orthogonal coordinate system in which the data
are uncorrelated and the variance of the data along each orthogonal axis is successively ordered
from the highest to lowest. The projection of data along each axis gives what are called principal
components. Theoretically, eigendecomposition of the covariance matrix provides exactly such a
transformation. For large data sets, however, classical decomposition techniques are infeasible and
other numerical methods, such as least squares approximation schemes, are practically employed.
An especially notable instance is the problem of dimensionality reduction, where only the largest
principal components—as the best representative of the data—are desired. Linear autoencoders
(LAEs) are one such scheme for dimensionality reduction that is applicable to large data sets.
An LAE with a single fully-connected and linear hidden layer, and Mean Squared Error (MSE) loss
function can discover the linear subspace spanned by the principal components. This subspace is
the same as the one spanned by the weights of the decoder. However, it failure to identify the exact
principal directions. This is due to the fact that, when the encoder is transformed by some matrix,
transforming the decoder by the inverse of that matrix will yield no change in the loss. In other
words, the loss possesses a symmetry under the action of a group of invertible matrices, so that
directions (and orderings/permutations thereto) will not be discriminated.
The early work of Bourlard & Kamp (1988) and Baldi & Hornik (1989) connected LAEs and PCA
and demonstrated the lack of identifiability of principal components. Several methods for neural
networks compute the exact eigenvectors (Rubner & Tavan, 1989; Xu, 1993; Kung & Diamantaras,
1990; Oja et al., 1992), but they depend on either particular network structures or special optimiza-
tion methods. It was recently observed (Plaut, 2018; Kunin et al., 2019) that regularization causes
the left singular vectors of the decoder to become the exact eigenvectors, but recovering them still
requires an extra decomposition step. As Plaut (2018) point out, no existent method recovers the
eigenvectors from an LAE in an optimization-independent way on a standard network — this work
fills that void.
1
Under review as a conference paper at ICLR 2020
Moreover, analyzing the loss surface for various architectures of linear/non-linear neural networks
is a highly active and prominent area of research (e.g. Baldi & Hornik (1989); Kunin et al. (2019);
Pretorius et al. (2018); Frye et al. (2019)). Most of these works extend the results of Baldi & Hornik
(1989) for shallow LAEs to more complex networks. However, most retain the original MSE loss,
and they prove the same critical point characterization for their specific architecture of interest. Most
notably Zhou & Liang (2018) extends the results of Baldi & Hornik (1989) to deep linear networks
and shallow RELU networks. In contrast in this work we are going after a loss with better loss
surface properties.
We propose a new loss function for performing PCA using LAEs. We show that with the proposed
loss function, the decoder converges to the exact ordered unnormalized eigenvectors of the sample
covariance matrix. The idea is simple: for identifying p principal directions we build up a total
loss function as a sum of p squared error losses, where the ith loss function identifies only the first i
principal directions. This approach breaks the symmetry since minimizing the first loss results in the
first principal direction, which forces the second loss to find the first and the second. This constraint
is propagated through the rest of the losses, resulting in all p principal components being identified.
For the new loss we prove that all local minima are global minima.
Consequently, the proposed loss function has both theoretical and practical implications. Theoret-
ically, it provides better understanding of the loss surface. Specifically, we show that any critical
point of our loss L is a critical point of the original MSE loss but not vice versa, and conclude that L
eliminates those undesirable global minima of the original loss (i.e., exactly those which suffer from
the invariance). Given that the set of critical points of L is a subset of critical points of MSE loss,
many of the previous work on loss surfaces of more complex networks likely extend. In light of the
removal of undesirable global minima through L, examining more complex networks is certainly a
very promising direction.
As for practical consequences, we show that the loss and its gradients can be compactly vectorized
so that their computational complexity is no different from the MSE loss. Therefore, the loss L can
be used to perform PCA/SVD on large datasets using any method of optimization such as Stochas-
tic Gradient Descent (SGD). Chief among the compellingly reasons to perform PCA/SVD using
this method is that, in recent years, there has been unprecedented gains in the performance of very
large SGD optimizations, with autoencoders in particular successfully handling larger numbers of
high-dimensional training data (e.g., images). The loss function we offer is attractive in terms of
parallelizability and distributability, and does not prescribe any single specific algorithm or imple-
mentation, so stands to continue to benefit from the arms race between SGD and its competitors.
More importantly, this single loss function (without an additional post hoc processing step) fits seam-
lessly into optimization pipelines (where SGD is but one instance). The result is that the loss allows
for PCA/SVD computation as single optimization layer, akin to an instance of a fully differentiable
building block in a NN pipeline Amos & Kolter (2017), potentially as part of a much larger network.
2	The Proposed Loss Function and Review of Final Results
Let X ∈ Rn×m and Y ∈ Rn×m be the input and output matrices, where m centered sample points,
each n-dimensional, are stacked column-wise. Let xj ∈ Rn and yj ∈ Rn be the jth sample input
and output (i.e. the jth column of X and Y , respectively). Define the loss function L(A, B) as
pm	p
L(A, B) :=XXkyj-AIi;pBxjk22=XkY -AIi;pBXk2F	(1)
i=1 j=1	i=1
whereh∙,)F and |卜|恨 are the FrobeniUs inner product and norm, Ii;P is a P X P matrix with all ele-
ments zero except the first i diagonal elements being one. (Or, equivalently, the matrix obtained by
100
setting the lastP-i diagonal elements ofaP ×P identity matrix to zero, e.g. I2;3 = 0 1 0 .) In
000
what follows, we shall denote the transpose of matrix M by M0. Moreover, the matrices A ∈ Rn×p,
and B ∈ Rp×n can be viewed as the weights of the decoder and encoder parts of an LAE.
The results are based on the following standard assumptions that hold generically:
2
Under review as a conference paper at ICLR 2020
Assumption 1. For an input X and an output Y , let Σxx := XX0, Σxy := XY 0, Σyx := Σ0xy
and Σyy = Y Y 0 be their sample covariance matrices. We assume
•	The input and output data are centered (zero mean).
•	Σxx, Σxy, Σyx and Σyy are positive definite (of full rank and invertible).
•	The covariance matrix Σ := ΣyxΣx-x1Σxy is of full rank with n distinct eigenvalues
λι >λ2 > …>λn.
•	The decoder matrix A has no zero columns.
Claim. The main result of this work proved in Theorem 2 is as follows:
If the above assumptions hold then all the local minima of L(A, B) are achieved iff A and B are
of the form
A = U1:pDp
B = D-1U0..p∑yx∑-X:,
where the ith column of U1:p is the unit eigenvector of Σ := ΣyxΣx-x1Σxy corresponding to the ith
largest eigenvalue and Dp is a diagonal matrix with nonzero diagonal elements. In other words,
A contains ordered unnormalized eigenvectors of Σ corresponding to the p largest eigenvalues.
Moreover, all the local minima are global minima with the value of the loss function at those
global minima being
p
L(A, B) = p Tr(Σyy) -	(p-i+1)λi,
i=1
where λi is the ith largest eigenvalue of Σ := ΣyxΣx-x1Σxy. In the case of autoencoder (Y = X):
Σ = Σxx. Finally, while L(A, B) in the given form contains O(p) matrix products, we will show
that it can be evaluated with constant (less than 7) matrix products independent of the value p.
3 Notation
In this paper, the underlying field is always R, and positive semidefinite matrices are symmetric by			
definition. The following constant matrices are used extensively throughout. Rp×p and Sp ∈ Rp×p are defined as		The matrices TP	∈
(TPXj = (P -i+1) δij, i.e. TP = diag(p,p - 1,…，1), -p	P - 1 …2 1 p — 1 P — 1 …2 1		N 3 2 1	(2)
(SP)ij =P-maχ(i, j) + 1, i.e. 4 SP=	.	.	... 2 1 2	2	2	21 1	1	1	11	,e.g. S4=	3321 2221. 1111	(3)
Another matrix that will appear in the formulation is SF := Tp-ISPT-1. Clearly, the diagonal
ʌ
matrix Tp is positive definite. As shown in Lemma 2, Sp and Sp are positive definite as well.
4 Main Theorems
The general strategy to prove the above claim is as follows. First the analytical gradients of the
loss is derived in a matrix form in Propositions 1 and 2. We compare the gradients with that of
the original Minimum Square Error (MSE) loss. Next, we analyze the loss surface by solving the
gradient equations which yields the general structure of critical points based on the rank of the
decoder matrix A. Next, we delineate several interesting properties of the critical points, notably,
any critical point of the loss is also a critical point for the MSE loss but not the other way around.
Finally, by performing second order analysis on the loss in Theorem 2 the exact equations for local
minima are derived which is shown to be as claimed.
3
Under review as a conference paper at ICLR 2020
∙-v
Let L(A, B) and L(A, B) be the original loss, and the proposed loss function, respectively, i.e.,
	m		pm
∙-v L(A, B)	:= X kyj - ABxjk22	L(A, B) :	= XXkyj -AIi;pBxjk22
	j=1		i=1 j=1
	= kY -ABXk2F		p kY -AIi;pBXk2F
			i=1
The first step is to calculate the gradients with respect to A and B and set them to zero to derive the
implicit expressions for the critical points. In order to do so, first, in Lemma 5, for a fixed A, we
derive the directional (Gateaux) derivative of the loss with respect to B along an arbitrary direction
W ∈ Rp×n, denoted as dBL(A, B)W, i.e.
dBL(A, B)W
L(A,B+W)-L(A,B)
lim ----------------
kWkF→0	kWkF
As shown in the proof of the lemma, dBL(A, B)W is derived by writing the norm in the loss as an
inner product, opening it up using linearity of inner product, dismiss second order terms in W (i.e.
O(kW k2)) and rearrange the result as the inner product between the gradient with respect to B,
and the direction W , which yields
dBL(A, B)W = -2Tr (W0 (TpA0∑yx -(Sp ◦ (AA)) B∑xx))
=-2TpAEyx - (Sp ◦ (A0A)) B∑xx, W〉F,	(4)
where, ◦ is the Hadamard product and the constant matrices Tp and Sp , were defined in the be-
ginning. Second, the same process is done in Lemma 6, to derive dAL(A, B)V ; the derivative of
L with respect to A in an arbitrary direction V ∈ Rn×p , for a fixed B, which is then set to zero
to derive the implicit expressions for the critical points. The results are formally stated in the two
following propositions.
Proposition 1. For any fixed matrix A ∈ Rn×p the function L(A, B) is convex in the coefficients
of B and attains its minimum for any B satisfying the equation
(Sp ◦ (A0A))BΣxx = TpA0Σyx,	(5)
where ◦ is the Hadamard (element-wise) product operator, and Sp and Tp are constant matrices
defined in the previous section. Further, if A has no zero column, then L(A, B) is strictly convex in
B and has a unique minimum when the critical B is
B = B(A) = (Sp ◦ (A0A))-1TpA0∑yx∑-1	(6)
and in the autoencoder case it becomes
B = JB(A) = (Sp ◦ (A0A))-1TdA0.	(60)
The proof is given in appendix A.2.
Remark 1. Note that as long as A has no zero column, Sp ◦ (A0A) is nonsingular (we will explain
the reason soon). In practice, A with zero columns can always be avoided by nudging the zero
columns of A during the gradient decent process.
Proposition 2. For any fixed matrix B ∈ Rp×n the function L(A, B) is a convex function in A.
Moreover, for a fixed B, the matrix A that satisfies
A (Sp ◦ (BΣxxB0)) =ΣyxB0Tp	(7)
is a critical point of L(A, B).
The proof is given in appendix A.3.
The pair (A, B) is a critical point of L if they make dB L(A, B)W and dAL(A, B)V zero for any
pair of directions (V , W). Therefore, the implicit equations for critical points are given below, next
∙-v
to the ones derived by Baldi & Hornik (1989) for L(A, B).
4
Under review as a conference paper at ICLR 2020
∙-v
For L(A, B):
A0ABΣxx = A0Σyx,
ABΣxxB0 = ΣyxB0.
For L(A, B):
(Sp ◦ (A0A))BΣχχ = TpA0Σyχ,
A (Sp ◦ (BΣxxB0)) = Σyx B0Tp.
Remark 2. Notice the similarity, and the difference only being the presence of Hadamard product
by Sp in the left and by diagonal Tp in the right. Therefore, practically, the added computational
cost of evaluating the gradients is negligible compare to that of MSE loss.
The next step is to determine the structure of (A, B) that satisfies the above equations, and find the
subset of those solutions that account for local minima. For the original loss, the first expression
(A0ABΣxx = A0Σyx) is used to solve for B and put it in the second to derive an expression
solely based on A. Obviously, in order to solve the first expression for B, two cases are considered
separately: the case where A is of full rank p, so A0A is invertible, and the case of A being of rank
r < p. Here we do the same but there is a twist; for us there is only one case. The reason is as
long as (not necessarily full rank) A has no zero column, Sp ◦ (A0A) is positive definite and hence,
invertible. This is discussed in detail in Lemma 2 and we briefly explain it here. As shown in the
lemma, Sp is positive definite and by Shur product theorem for any A (of any rank), Sp ◦ (A0A) is
positive semidefinite. However, as a result of Oppenheim inequality (Horn & Johnson (2012), Thm
7.8.16), that in our case translates to det(Sp) Qi(A0A)ii ≤ det(Sp ◦ (A0A)), as long as A has no
zero column, i(A0A)ii > 0 and therefore, det(Sp ◦ (A0A)) > 0. Here, we assume A of any rank
r ≤ p has no zero column (since this can be easily avoided in practice) and consider Sp ◦ (A0A) to
∙-v
be always invertible. Therefore, (A, B) define a critical point of losses L and L if
∙-v
For L(A, B) and full rank A:
B = B(A) = (A0A)-1A0∑yx∑-1
ABΣxxB0 = ΣyxB0.
For L(A, B) and no zero column A:
B = B(A) = (Sp ◦ (A0A))T"A0∑yx∑-1,
A (Sp ◦ (BΣxxB0)) = ΣyxB0Tp.
Before, we state the main theorem we need the following definitions. First, a rectangular permu-
tation matrix Πr ∈ Rr×p is a matrix that each column consists of at most one nonzero element
with the value 1. If the rank of Πr is r with r < p then clearly, Πr has p - r zero columns.
Also, by taking away those zero columns the resultant r × r submatrix of Πr is a standard square
permutation matrix.
Second, under the conditions provided in Assumption 1, the matrix Σ := ΣyxΣx-x1Σxy has an
eigenvalue decomposition Σ = UΛU0, where the ith column ofU, denoted as ui, is an eigenvector
of Σ corresponding to the ith largest eigenvalue of Σ, denoted as λ%. Also, Λ = diag(λι,…，λn)
is the diagonal vector of ordered eigenvalues of Σ, with λι > λ2 > •… > λn > 0. We use the
following notation to organize a subset of eigenvectors of Σ into a rectangular matrix. Let for any
r ≤ p, Ir = {iι,…，ir}(1 ≤ iι < …< ir < n) be any ordered r-index set. Define UIr ∈ Rn×p
as UIr = [uiι ,…，Uir]. That is the columns of UIr are the ordered orthonormal eigenvectors of Σ
associated with eigenvalues λ” < •…< λir. Clearly, when r = p, we have UIr = [u”, •…，Uip]
corresponding to anp-index set Ip = {iι, ∙∙∙ , ip}(1 ≤ iι < .…< ip < n). Similarly, we define
ΛIr ∈ Rp×p as ΛIr = diag(λiι,…，λ“).
Theorem 1.	Let A ∈ Rn×p and B ∈ Rp×n such that A is of rank r ≤ p. Under the conditions
provided in Assumption 1 and the above notation, The matrices A and B define a critical point of
L(A, B) if and only if for any given r-index set Ir, anda nonsingular diagonal matrix D ∈ Rr×r,
A and B are of the form
A=UIrCD,
B = B(A)= DTnC UIr ∑yχ∑-1,
(8)
(9)
where, C ∈ Rr×p is of of full rank r with nonzero and normalized columns such that ΠC :=
(Sp ◦ (C0C))-1 TpC0 is a rectangular permutation matrix of rank r and CΠC = Ir. For all
1 ≤ r ≤ p, such C always exists. In particular, if matrix A is of full rank p, i.e. r = p, the two
given conditions on ΠC are satisfied iff the invertible matrix C is any squared p × p permutation
matrix Π. In this case (A, B) define a critical point of L(A, B) iff they are of the form
A = UIpΠD,
(10)
5
Under review as a conference paper at ICLR 2020
B = B(A)= D-1∏0 UIp ∑yχ∑-:	(11)
The proof is given in appendix A.4.
Remark 3. The above theorem provides explicit equations for the critical points of the loss surface
in terms of the rank of the decoder matrix A and the eigenvectors ofΣ. This explicit structure allows
us to further analyze the loss surface and its local/global minima.
Here, we provide a proof sketch for the above theorem to make the claims more clear. Again as a
∙-v
reminder, the EVD ofΣ := ΣyxΣx-x1Σxy isΣ = UΛU0. For both L and L, the corresponding
B(A) is replaced by B on the RHS of critical point equations. For the loss L(A, B), as shown in
the proof of the theorem, results in the following identity
U0A (Sp ◦ (B∑xχB0)) A0U = Λ∆,
(12)
where ∆ := U0ATp(Sp ◦ (A0A))-1TpA0U is symmetric and positive semidefinite. The LHS of
eq. (12) is symmetric so the RHS is symmetric too, so Λ∆ = (Λ∆)0 = ∆0Λ0 = ∆Λ. Therefore,
∆ commutes with the diagonal matrix of eigenvalues Λ. Since eigenvalues are assumed to be
distinct, ∆ has to be diagonal as well. By Lemma 2 Tp(Sp ◦ (A0A))-1Tp is positive definite and U
is an orthogonal matrix. Therefore, r = rank(A) = rank(∆) = rank(U0∆U), which implies that
the diagonal matrix ∆, has r nonzero and positive diagonal entries. There exists an r-index set Ir
corresponding to the nonzero diagonal elements of ∆. Forming a diagonal matrix ∆Ir ∈ Rr×r by
filling its diagonal entries (in order) by the nonzero diagonal elements of ∆, we have
U∆U0 = UIr∆Ir UI0r
ATp(Sp ◦ (A0A))-1TpA0 = UIr∆IrUI0r
Defof∆
(13)
which indicates that the matrix A has the same column space as UIr . Therefore, there exists a full
rank matrix C ∈ Rr ×p such that A = UIr C. Since A has no zero column, C has no zero column.
Further, by normalizing the columns of C We can write A = UIr CD, where D ∈ Rp×p is diagonal
that contains the norms of columns of C.
〜	'—"
Baldi & Hornik (1989) did something similar for full rank A for the loss L to derive (AL = UIp C).
∙-v
But their C can be any invertible p × p matrix. However, in our case, the matrix C ∈ Rr×p
corresponding to rank r ≤ p matrix A, has to satisfy eq. (13) by replacing A by UIr CD and
ʌ ʌ
eq. (12) by replacing B(A) by B(UIrCD). In the case ofBaldi & Hornik (1989), for the original
∙-v
loss L, equations similar to eq. (13) and eq. (12) appear but they are are satisfied trivially by any
∙-v
invertible matrix C. Simplifying those equations by using A = UIrCD after some algebraic
manipulation results in the following two conditions for C :
CTp (Sp ◦ (C0C))-1 TpC0 =∆Ir and	(14)
C (Sp ◦ ((Sp ◦ (C0C))-1TpC0ΛIrCTp(Sp ◦ (C0C))T)) C0 =%,△».	(15)
As detailed in proof of Theorem 1, solving for C leads to its specific structure as laid out in the
theorem.
Remark 4. Note that when A is of rank r < p with no zero columns then the invariant matrix C is
not necessarily a rectangular permutation matrix but ΠC := (Sp ◦ (C0C))-1 TpC0 is a rectangular
permutation matrix with CΠC = Ir . It is only when r = p that the invariant matrix C becomes a
permutation matrix. Nevertheless, as we show in the following corollary, the global map is always
∀r ≤ p : G = AB = UIrUI0ΣyxΣx-x1. It is possible to find further structure (in terms of block
matrices) for the invariant matrix C when r < p. However, this is not necessary as we soon show
that all rank deficient matrix As are saddle points for the loss and ideally should be passed by during
the gradient decent process. Based on some numerical results our conjecture is that when r < p the
matrix C can only start with a r × k rectangular permutation matrix of rank r with r ≤ k ≤ p and
the rest ofp - k columns of C is arbitrary as long as none of the columns are identically zero.
Corollary 1. Let (A, B) be a critical point of L(A, B) under the conditions provided in Assump-
tion 1 and rankA = r ≤ p. Then the following are true
1. The matrix BΣxxB0 is a p × p diagonal matrix of rank r.
6
Under review as a conference paper at ICLR 2020
2.	For all 1 ≤ r ≤ p, for any critical pair (A, B), the global map G := AB becomes
G = UIrUI0rΣyxΣx-x1.	(16)
For the autoencoder case (Y = X) the global map is simply G = UIr UI0 .
p2
3.	(A, B) is also the critical point of the classical loss L(A, B) =	i=1 kY - ABX kF.
The proof is given in appendix A.5.
Remark 5. The above corollary implies that L(A, B) not only does not add any extra critical point
∙-v
compare to the original loss L(A, B), it provides the same global map G := AB. It only limits
the structure of the invariance matrix C as described in Theorem 1 so that the decoder matrix A can
recover the exact eigenvectors of Σ.
Lemma 1. The loss function L(A, B) can be written as
L(A, B) =pTr(Σyy) - 2 Tr (ATpBΣxy) + Tr (B0 (Sp ◦ (A0A)) BΣxx).	(17)
The above identity shows that the number of matrix operations required for computing the loss
L(A, B) is constant and thereby independent of the value ofp.
The proof is given in appendix A.6.
Theorem 2.	Let A* ∈ Rn×p and B* ∈ Rp×n such that A* is ofrank r ≤ P. Under the conditions
provided in Assumption 1, (A*, B*) define a local minima oftheproposed loss function iffthey are
of the form
A* = U1:pDp	(18)
B* = D-1 U0.p∑yx∑-1,	(19)
where the ith column of U1:p is a unit eigenvector of Σ := ΣyxΣx-x1Σxy corresponding the ith
largest eigenvalue and Dp is a diagonal matrix with nonzero diagonal elements. In other words,
A* contains ordered unnormalized eigenvectors of Σ corresponding to the p largest eigenvalues.
Moreover, all the local minima are global minima with the value of the loss function at those global
minima being
p
L(A*,B*) = p Tr(Σyy) -	(p-i+1)λi,	(20)
i=1
where λi is the ith largest eigenvalue of Σ.
The proof is given in appendix A.7.
Remark 6. Finally, the second and third assumptions we made in the beginning in Assumption 1 can
be relaxed by requiring only Σxx to be full rank. The output data can have a different dimension
than the input. That is Y ∈ Rn×m and X ∈ Rn0×m, where n 6= n0. The reason is that the given
loss function structurally is very similar to MSE loss and can be represented as a Frobenius norm
on the space of n × m matrices. In this case the covariance matrix Σ := ΣyxΣx-x1Σxy is still
n × n. Clearly, for under-constrained systems with n < n0 the full rank assumption of Σ holds.
For the overdetermined case, where n0 > n the second and third assumptions in Assumption 1 can
be relaxed: we only require Σxx to be full rank since this is the only matrix that is inverted in the
theorems. Note that ifp > min(n0, n) then ΛIp: the p × p diagonal matrix of eigenvalues of Σ for
a p-index-set Ip bounds to have some zeros and will be say rank r < p, which in turn, results in the
encoder A with rank r. However, the Theorem 1 is proved for encoder of any rank r ≤ p. Finally,
following theorem 2 then the first r columns of the encoder A converges to ordered eigenvectors
of Σ while the p - r remaining columns span the kernel space of Σ. Moreover, Σ need not to
have distinct eigenvectors. In this case ∆Ir becomes a block diagonal matrix, where the blocks
correspond to identical eigenvalues ΣIr . In this case, the corresponding eigenvectors in A* are not
unique but they span the respective eigenspace.
5	Experiments
5.1	Experimental Setup
LAEs with Two Loss functions We will verify the loss function L(A, B) defined in eq. (1) by
setting the input matrix X ∈ Rn×m equal to the output matrix Y ∈ Rn×m (Y = X), where the
7
Under review as a conference paper at ICLR 2020
linear autodecoder (LAE) becomes a solution to PCA. In order for comparison, we train another
2
LAE using the MSE loss L(A' B) defined as L(A, B) = Il Y - ABX∣l , where Y = X is also
applied in our experiments.
The weights of networks are initialized to random numbers with a small enough standard deviation
(10-7 in our case). We choose to use the Adam optimizer with a scheduled learning rate (starting
from 10-3 and ending with 10-6 in our case), which empirically benefits the optimization process.
The two training processes are stopped at the same iteration at which one of the models firstly finds
all of the principal directions. As a side note, we feed all data samples to the network at one time
with batch size equal to m, although mini-batch implementations are apparently amendable.
Evaluation Metrics We use the classical PCA approach to get the ground truth principal direction
matrix A* ∈ Rn×p, by conducting Eigen Value Decomposition (EVD) to XX0 ∈ Rn×n or Singular
Value Decomposition (SVD) to X ∈ Rn×m. As a reminder, A ∈ Rn×p stands for the decoder
weight matrix of an trained LAE given a loss function L. To measure the distance between A* and
A, we propose an absolute cosine similarity (ACS) matrix inspired by mutual coherence (Donoho
et al., 2005), which is defined as:
ACSij
IhA*, Aji∣
kA*∣H∣Ajk,
(21)
where Ai* ∈ Rn×1 denotes the ith ground truth principal direction, and Aj ∈ Rn×1 denotes the j th
column of the decoder A, i, j = 1, 2, . . . ,p. The elements of ACS ∈ Rp×p in eq. (21) take values
between [0,1], measuring pair-wise similarity across two sets of vectors. The absolute value absorbs
the sign ambiguity of principal directions.
The performances of LAEs are evaluated by defining the following metrics:
p
RatioT P = X I [ACSii > 1 - ]/p	(22)
i=1
p
RatioFP = I [ACSij > 1 - ]/p, and	(23)
i,j=1
i6=j
RatioT otal = RatioT P + RatioF P,	(24)
where I is the indicator function and is a manual tolerance threshold ( = 0.01 in our case). If
two vectors have absolute cosine similarity over 1 - , they are deemed equal. Considering some
columns of decoder may be correct principal directions but not in the right order, we introduce
RatioT P and RatioF P in eqs. (22) and (23) to check the ratio of correct in-place and out-of-place
principal directions respectively. Then RatioT otal in eq. (24) measures the total ratio of the correctly
obtained principal directions by the LAE regardless of the order.
Datasets As a proof-of-concept, both synthetic data and real data are used. For the synthetic
data, 2000 zero-centered data samples are generated from a 1000-dimension zero mean multivariate
normal distribution with the covariance matrix being diag(Np). For the real data, we choose to use
MNIST dataset (LeCun et al., 1998), which includes 60,000 grayscale handwritten digits images,
each of dimension 28 × 28 = 784.
5.2	Evaluation and Analysis
Synthetic Data Experiments In our experiment, p, the number of desired principal components
(PCs), is set to 100, i.e. the dimension is to be reduced from 1000 to 100. Figures 1 and 2 demon-
strate a few conclusions. First, during the training process, the loss ratio ofboth losses continuously
decreases to 1, i.e. they both converge to the optimal loss value. However, when both get close
enough, L require more iterations since the optimizer is forced to find the right directions: it fully
converges only after it has found all the principal directions in the right order.
Second, using the loss L results in finding more and more correct principal directions, with
RatioT P continuously rising; and ultimately affords all correct and ordered principal directions,
8
Under review as a conference paper at ICLR 2020
0-2仝 SSOq
suo-ɔə-p P3dδsjd p-ɔ-əp°22BH
Performance of finding the principal directions
for both L and L
Iterations on log scale (i)
∕~∙z
Figure 1: Convergence of losses to their corre- Figure 2: Performance of both losses L and L in
sponding optimal loss. Note that the correct shift finding the principal directions at the columns of
and scaling of the y-axis tick values is printed at their respective decoders.
the top left corner of the figure.
with RatioT P ending with 100%. Notice that occasionally and temporarily, some of the principal
directions is found but not at their correct position, which is indicated by the rise of RatioF P in
the figure. However, as optimization continues they are shifted to the right column, which results in
RatioF P going back to zero, and RatioT P reaching one. As for L, it fails to identify any principal
directions; both RatioT P and RatioF P for L stay at 0, which indicates that none of the columns of
the decoder A, aligns with any principal direction.
Third, as shown in the figure, while the optimizer finds almost all the principal directions rather
quickly, it requires much more iterations to find some final ones. This is because some eigenvalues
in the empirical covariance matrix of the finite 2000 samples become very close (the difference
becomes less than 1). Therefore, the loss has to get very close to the optimal loss, making the
gradient of the loss hard to distinguish between the two.
Real Data: MNIST Experiments We set the
number of principal components (PCs) as 100,
i.e., the dimension is to be reduced from 784 to
100. We also try to reconstruct with the top-10
columns found in this case. As in Fig. 3, the
reconstruction performance of L is consistently
better than L. That also reflects that L does not
identify PCs, while L is directly applicable to
performing PCA without bells and whistles.
6 Conclusion
In this paper, we have introduced a loss func-
tion for performing principal component anal-
ysis and linear regression using linear autoen-
coders. We have proved that the optimizing
with the given loss results in the decoder ma-
trix converges to the exact ordered unnormal-
ized eigenvectors of the sample covariance ma-
trix. We have also demonstrated the claims on
a synthetic data set of random samples drawn
(a) Original (b) Full decoder (c) 10 columns
(d) Original (e) Full decoder (f) 10 columns
Figure 3: Real data experimental comparison in
the reconstruction performance of MNIST im-
ages. First column: original image. Second col-
umn: reconstructed image using full columns of
the decoder. Third column: reconstructed image
using the first 10 columns of the decoder. Top
row: using L. Bottom row: using L .
from a multivariate normal distribution and on
MNIST data set. There are several possible generalizations of this approach we are currently work-
ing on. One is improving performance when the corresponding eigenvalues of two principal direc-
tions are very close and another is generalization of the loss for tensor decomposition.
9
Under review as a conference paper at ICLR 2020
References
Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks.
In International Conference on Machine Learning, pp. 136-145, 2017.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural networks, 2(1):53-58, 1989.
Herve Bourlard and Yves Kamp. Auto-association by multilayer perceptrons and singular value
decomposition. Biological cybernetics, 59(4-5):291-294, 1988.
David L Donoho, Michael Elad, and Vladimir N Temlyakov. Stable recovery of sparse overcomplete
representations in the presence of noise. IEEE Transactions on information theory, 52(1):6-18,
2005.
Charles G Frye, Neha S Wadia, Michael R DeWeese, and Kristofer E Bouchard. Numerically
recovering the critical points of a deep linear autoencoder. arXiv preprint arXiv:1901.10603,
2019.
R.A. Horn and C.R. Johnson. Matrix Analysis. Cambridge University Press, 2012. ISBN
9781139788885.
Sun-Yuan Kung and KI Diamantaras. A neural network learning algorithm for adaptive principal
component extraction (apex). In International Conference on Acoustics, Speech, and Signal Pro-
cessing, pp. 861-864. IEEE, 1990.
Daniel Kunin, Jonathan M Bloom, Aleksandrina Goeva, and Cotton Seed. Loss landscapes of regu-
larized linear autoencoders. arXiv preprint arXiv:1901.08168, 2019.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Erkki Oja, Hidemitsu Ogawa, and Jaroonsakdi Wangviwattana. Principal component analysis by
homogeneous neural networks, part 1: The weighted subspace criterion. IEICE Transactions on
Information and Systems, 75(3):366-375, 1992.
Elad Plaut. From principal subspaces to principal components with linear autoencoders. arXiv
preprint arXiv:1804.10253, 2018.
Arnu Pretorius, Steve Kroon, and Herman Kamper. Learning dynamics of linear denoising autoen-
coders. In International Conference on Machine Learning, pp. 4138-4147, 2018.
Jeanne Rubner and Paul Tavan. A self-organizing network for principal-component analysis. EPL
(Europhysics Letters), 10(7):693, 1989.
Lei Xu. Least mean square error reconstruction principle for self-organizing neural-nets. Neural
networks, 6(5):627-648, 1993.
E. Zeidler. Applied Functional Analysis: Main Principles and Their Applications. Applied Mathe-
matical Sciences. Springer New York, 1995. ISBN 9780387944227.
Y Zhou and Y Liang. Critical points of linear neural networks: Analytical forms and landscape
properties. In Proc. Sixth International Conference on Learning Representations (ICLR), 2018.
10
Under review as a conference paper at ICLR 2020
Appendix
A Proofs
A.1 Preliminaries
Before we present the proof for the main theorems, the following two lemmas introduce some nota-
tions and basic relations that are required for the proofs.
Lemma 2. The constant matrices Tp ∈ Rp×p and Sp ∈ Rp×p are defined as
(Tp)ij =(P - i + I) δij, i.e. Tp = diag (P, P - 1, ∙∙∙ , I),
	-p p - 1 …2 1		
	P — 1 P — 1 …2 1		N 3 2 1
(SP )ij = P - max(i, j ) + 1, i.e. SP =	.	.. .	.	..2 1	,e.g. S4 =	3321 2221
	2	2	2	21		1111
	1	1	1	11		
Clearly, the diagonal matrix Tp is positive definite. Another matrix that will appear in the formula-
tion is SP := T-1SpTp-
(Sp) ij = (TP ISPTp 1)ij = P - min(ij) + 1 'e TP 1
11
1 ɪ
P	P-1
11
1	1
P— 1	P— 1
e.g. S4
-
P
F-1-41-001-21-2
1-41-001-001-3
1-41-41-41-4
• ∙ 1-2 lɪ
♦ ♦ 1-21-2
1
The following properties of Hadamard product and matrices TP and SP are used throughout:
1.	For any arbitrary matrix A ∈ Rn×P,
P
Ii;P = TP, and	(25)
i=1
P
Ii;PA0AIi;P = SP ◦ (A0A),	(26)
i=1
where, ◦ is the Hadamard (element-wise) product.
2.	For any matrices M1, M2 ∈ RP×P and diagonal matrices D, E ∈ RP×P,
D (M1 ◦ M2 ) E = (D M1 E ) ◦ M2 = M1 ◦ (D M2 E ) .
Moreover, if Π1, Π2 ∈ RP×P are permutation matrices then
∏1 (M1 ◦ M2) ∏2 = (∏1M1∏2) ◦ (Π1 M2∏2).
3.	SP is invertible and its inverse is a symmetric tridiagonal matrix
	"1	-1 …0	0 -
I1 i = j = 1	-1	2	-1	0	0
(S--)ij = I -1 i二 j==11 , ie∙ S-I = I	...... .	.	..	.
	0	0	-1	2	-1
(0	otherwise	0	0	0	-1	2
11
Under review as a conference paper at ICLR 2020
4.	Sp is positive definite.
5.	For any matrix A ∈ Rn×p, Sp ◦ (A0A) is positive semidefinite. If (not necessarily full
rank) A has no zero column then Sp ◦ (A0A) is positive definite.
6.	For any diagonal matrix D ∈ Rp×p
Sp ◦ D = TpD , and	(27)
Sp ◦ D = T-1D.	(28)
7.	Let D , E ∈ Rp×p be positive semidefinite matrices, where E has no zero diagonal element,
and D isofrank r ≤ P. Also, letfor any r ≤ P, Jr = {iι,…，ir}(1 ≤ iι < …< ir < n)
be any ordered r-index set. Then D and E satisfy
E (Sp ◦ D) = (Sp ◦ E) D,
if and only if, the following two conditions are satisfied:
(a)	The matrix D is diagonal with P - r zero diagonal elements and r positive diagonal
elements indexed by the set Jr. That is for any i ∈ Jr : (D)ii > 0 and the rest of
elements of D are zero.
(b)	For any i, j ∈ Jr and i 6= j we have (E)i,j = 0.
Clearly, if D is positive definite then Jr = Np and hence, both D and E are diagonal.
Proof. . The proof of the properties are as follows.
1.	eq. (25) is trivial. For eq. (26) note that AIi;p selects the first i columns of A (zeros out
the rest), and similarly, Ii;pA0 selects the first i rows of A (zeros out the rest). Therefore,
Ii;pA0AIi;p is aP ×P matrix that its Leading Principal Submatrix of order i (LPSi) 1 is the
same as the LPSi of A0A (and the rest of the elements are zero). Hence, Pip=1 Ii;pA0AIi;p
(counting backwards) adds LPSp of A0A (i.e. A0A itself) with LPSp-1 that doubles
LPSp-1 part of the result and then adds LPSp-2 that triples the LPSp-2 part of result,
the process continues until by the last addition LPS1is added to the result for the Pthtimes.
This is exactly the same as evaluating Sp ◦ (A0A).
2.	This is a standard result (Horn & Johnson, 2012), and no proof is needed.
3.	Directly compute SpSp-1:
Eij= X(Sp)ik(S-i)kj ""1:(S-I)==0
k=1
((Spb,j-I(SpI)j-1,j+(Sp)i,j(Sp 1 )j,j +(Sp)i,j + 1(S- 1)j+1,j 2 ≤ j &
=I	j ≤ PT
(((Sp)i,p-1 (Sp- )p-1,p + (Sp)i,p(Sp- )p,p	j = P
I(Sp )i,1(S-1)1,1 + (Sp)i,2(S-1)2,1	j = 1
(-(Sp)i,j-1 + 2(Sp)i,j - (Sp)i,j+1 2 ≤ j ≤ P - 1
= -(Sp)i,p-1 + 2(Sp)i,p	j = P
I(Sp)i,1 - (Sp)i,2	j = 1
{max(i, j — 1) — 2max(i,j) + max(i,j + 1)	2 ≤ j ≤ P — 1
-(P - max(i, P - 1) + 1) + 2(P - max(i, P) + 1) j = P
— max(i, 1) + max(i, 2)	j = 1
1For a p × p matrix, the leading principal submatrix of order i is an i × i matrix derived by removing the
last p - i rows and columns of the original matrix (Horn & Johnson (2012), P17)
12
Under review as a conference paper at ICLR 2020
{max(i, j — 1) — 2max(i,j) + max(i,j + 1)
1 - p + max(i, p - 1)
max(i, 2) — max(i, 1)
2 ≤ j ≤ P - 1
j=p
j=1
1 i=j
0 i 6= j
1 i=p
0 i 6= p
1 i=1
0 i≥2
1<j<p
j = p = (Ip)ij.
j=1
4.	Firstly, note that Sp-1 is symmetric and nonsingular so all the eigenvalues are real and
nonzero. It is also a diagonally dominant matrix (Horn & Johnson (2012), Def 6.1.9) since
∀i ∈{1,…，p}: Ci = l(s-1)ii∣≥ X l(s-1)ijI =: Ri,
j=1,j 6=i
where the inequality is strict for the first and the last row and it is equal for the rows in
the middle. Moreover, by Gersgorin circle theorem (Horn & Johnson (2012), Thm 6.1.1)
for every eigenvalue li of Sp-1 there exists i such that li ∈ [Ci — Ri , Ci + Ri]. Since
∀i : Ci ≥ Ri we have all the eigenvalues are non-negative. They are also nonzero, hence,
Sp-1 is positive definite, which implies Sp is also positive definite.
5.	For any matrix A ∈ Rn×p , A0A is positive semidefinite. Also, Sp is positive definite so
by Schur product theorem (Horn & Johnson (2012), Thm 7.5.3(a)), Sp ◦ (A0A) is positive
semidefinite. Moreover, if all diagonal elements of A0A are positive (i.e. A has no zero
column) by the extension of Schur product theorem (Horn & Johnson (2012), Thm 7.5.3(b))
it is positive definite. This can also be easily deduced using the Oppenheim inequality
(Horn & Johnson (2012), Thm 7.8.16); that is for positive semidefinite matrices Sp and
A0A: det(Sp ) Qi(A0A)ii ≤ det(Sp ◦(A0A)). Since, Sp is positive definite, det(Sp ) > 0
(in fact it is 1 for any p) and if A0A has no zero diagonal then det(Sp ◦ (A0A)) > 0 and
therefore, Sp ◦ (A0A) is positive definite.
6.	Clearly, the matrix Tp is achieved by setting the off-diagonal elements ofSp to zero. Hence,
for any diagonal matrix D ∈ Rp×p : Sp ◦D = Tp ◦D. For the diagonal matrices Hadamard
product and matrix product are interchangeable so the latter may also be written as Tp D .
The same argument applies for the second identity.
7.	This property can easily be proved by induction onp and careful bookkeeping of indices.
□
Lemma 3 (Simultaneous diagonalization by congruence). Let M1, M2 ∈ Rp×p , where M1 is
positive definite and M2 is positive semidefinite. Also, let D, E ∈ Rr×r be positive definite diagonal
matrices with r ≤ p. Further, assume there is a C ∈ Rr×p of rank r ≤ p such that
CM1C0 =D and
CM2C0 =DE .
Then there exists a nonsingular C ∈ Rp×p that its first r rows are the matrix C and
__________________________________ ____
CJMiC0 =D and
C M2C 0 =D E,
where, D = D ㊉ Ir-P is a P X P diagonal matrix and E = E ㊉ E is another P X P diagonal matrix,
in which E ∈ Rp-r×p-r is a nonnegative diagonal matrix. Clearly, the rank of M2 is r plus the
number of nonzero diagonal elements of E.
Proof. The proof is rather straightforward since this lemma is the direct consequence of Theorem
7.6.4 in Horn & Johnson (2012). The theorem basically states that if M1, M2 ∈ Rp×p is symmetric
13
Under review as a conference paper at ICLR 2020
and M1 is positive definite then there exists an invertible S ∈ Rp×p such that SM1 S0 = Ip and
SM2S0 is a diagonal matrix with the same inertia as M2 . Here, we have M2 that is positive
semidefinite and C ∈ Rr×p of rank r ≤ p such that
0
(D-21 C) Mi (D-21 C) =Ir and
0
(D -21 C) M2 (D -21 C) =E.
Therefore, since S is of full rank P and D -21 C is of rank r ≤ p, there exists P 一 r rows in S that
are linearly independent of rows of D丁 C. Establish C ∈ Rp×p by adding those P 一 r rows to C.
Then C has P linearly independent rows so it is nonsingular, and fulfills the lemma,s proposition
that is
CMiC0 =D and
C M2C 0 =D E,
where, D = D ㊉ Ir-P is a P X P diagonal matrix and E = E ㊉ E is another P X P diagonal matrix,
in which E ∈ Rp-r ×p-r is a nonnegative diagonal matrix.	□
Lemma 4. Let A and B define a critical point of L. Further, let V ∈ Rn×p and W ∈ Rp×n are
such that kV k
F,kWkF = O(ε) for some ε > 0. Then
L(A+V,B+W) 一 L(A, B) =hV TpBΣxxB0, V iF
-2h∑yx W0Tp 一 A(Sp ◦ (BΣχχ W0 + WΣχχB0)), V〉F
+h(Sp ◦ (A0A)) WΣxx, WiF + O(ε3).	(29)
Further, for W = W := (Sp ◦ (A0A))-1 TpV'ΣyxΣ-1, the above equation becomes
L(A + V, B + W)- L(A, B)=Tr (V0VTpBΣxxB0) - Tr (V0ΣV" (Sp ◦ (A0A))-1 TD)
+2Tr V0A Sp ◦ BΣxyVTp (Sp ◦ (A0A))-i
+ (Sp ◦ (A0A))-i TpV 0ΣyxB0))) + O(ε3).	(30)
Finally, in case the critical A is of full rank P and so, (A, B) = (UIp ΠD, B (UIp ΠD)), for the
encoder direction V with ∣∣ V∣∣f = O(ε) and W = W we have,
L(A + V, B + W) 一 L(A, B)=Tr (V0VΠ0ΛιpΠ"D-2) - Tr (V0ΣVTpD-2)
+2Tr (V0 UIp ΠD (Sp ◦ (D-iΠ0UI0pΣVD-2)))
+2Tr (V0Uip∏D (Sp ◦ (D-2V0ΣUip∏D-1)))
+O(ε3).	(31)
Proof. As described in appendix B.1, the second order Taylor expansion for the loss L(A, B) is
then given by eq. (63), i.e.
L(A + V, B + W) 一 L(A, B) =dAL(A, B)V + dBL(A, B)W + 1 diAL(A, B)V2
+dABL(A, B)VW + 2dBL(A, B)W2 + Rv,w(A, B).
If ∣V∣F , ∣W∣F = O(ε) then ∣R(V, W)∣ = O(ε3). Moreover, when A and B define a
critical point of L we have dAL(A, B)V = dB L(A, B)W = 0. By setting the derivatives
d2AL(A, B)V 2, dAB L(A, B)VW, d2BL(A, B)W2 that are given by eq. (69), eq. (68), and
eq. (66) respectively, the above equation simplifies to
14
Under review as a conference paper at ICLR 2020
L(A + V, B + W) - L(A, B) ={V (Sp ◦ (BΣxxB,)), V〉尸
-2h∑yχ W0Tp - A(Sp。(BΣχχ W0 + WΣχχB0)), V〉尸
+ h(Sp。(A'A)) WΣχχ, W)尸 + O(ε3).
Now, based on the first item in Corollary 1, BΣxxB0 is a p×p diagonal matrix, so based on eq. (27):
Sp。(BΣχχB0) = TpBΣχχB0. The substitution then yields eq. (29). Finally, in the above equation
replace W with W = (SP。(A0A))-1 TPV0∑yχ∑*. We have
L(A + V, B + W) - L(A, B)=
=(VTpBΣχχB0, V〉F - 2h∑yχΣ-1 Σχy VlP (SP。(AX)) - 1 Tp, V〉F
+ 2(A(Sp。(βΣχχΣUΣχy VTp(Sp θ(A0A))-1+(Sp θ(A,A))-1TpV 0ΣyχΣ-±ΣχχB0)) ,V F
+ h(Sp。(A0A))(Sp O (A0A))-1 TpV0ΣyχΣ袅Σχχ, (Sp。(A0A))-1 TpV0ΣyχΣ-χ⅜ +O(ε3)
=Tr (V0VTpBΣχχB0) - Tr (V0ΣV" (SP。(A0A))-1 TD)
+ 2Tr(V0A (Sp。(BΣχyVTp (SP。(A0A))-1 + (SP o (A0A))T TDV0∑yχB0))) + O(ε3),
which is eq. (30). For the final equation, we have
TpBΣχχB0 =TpD-1∏0Up ∑yχ∑u∑χχ∑u∑χy UIpΠD-1
'---------------------------V------'
=TpDTn U0 ΣUιp ΠD-1 = TpD-1 Π0Λιp∏ DT
∣-p{,---------'	`“'
=∏0Λιp∏TpD-2, and
Tp (Sp O (A0A))-1 Tp =Tp (SP。(DΠ0UlpUIp∏D))	TP
=Tp (SP。D2)-1 Tp = TPITrD-lTP = TpD-.
Replace the above in eq. (30) and simplify:
(32)
(33)
L(A + V, B + W) - L(A, B) =Tr (V0VToBΣχχB0) - Tr (V0ΣV” (Sp。(A0A))-1 TP)
+2Tr(V0A (SP。(BΣχy VTo (SP。(A0A))-1
+ (Sp o (A0A))-1 TpV0∑yχB0))) + O(ε3) ==≡)>
JJJ	eq. (33)
L(A + V, B + W) - L(A, B) =Tr (V0VΠ0ΛlpΠ"D-2) - Tr (V0ΣVTpD-2)
+2 Tr (V0A (Sp o (BΣχyVD-2 + D-2V0∑yχB0)))
+O(ε3)
A=Ujp ΠD
・	>
B = B(UJp ΠD)
L(A + V, B + W) - L(A, B) =Tr (V0VΠ0Λ1p∏TpD-2) - Tr (V0ΣVTDD-2)
+2Tr(V0U1pΠD(SP。(DTn0%ΣVD-2)))
+2 Tr (V0UipΠD (SP o (D-2V0ΣUιpΠD-1)))
+O(ε3),
which finalizes the proof.
□
A.2 Proof of Proposition 1
For this proof we use the first and second order derivatives for L(A, B) wrt B derived in Lemma 5.
From eq. (66), we have that for a given A the second derivative wrt to B of the cost L(A, B) at B,
15
Under review as a conference paper at ICLR 2020
and in the direction W is the quadratic form
d2B2 L(A, B)W 2 = 2Tr(W0 (Sp ◦ A0A) WΣxx) .
The matrix Σxx is positive-definite and by Lemma 2, Sp ◦ A0A is positive-semidefinite. Hence,
d2B2 L(A, B)W2 is clearly non-negative for all W ∈ Rp×n. Therefore, L(A, B) is convex in
coefficients of B for a fixed matrix A. Also the critical points of L(A, B) for a fixed A is a matrix
B that satisfies ∀W ∈ Rp×n : dBL(A, B)W = 0 and hence, from eq. (64) we have
-2hTpA0Σyx - (Sp ◦ (A0A)) BΣxx, W iF = 0.
Setting W = TpA0Σyx - (Sp ◦ (A0A)) BΣxx we have
TpA0Σyx - (Sp ◦ (A0A)) BΣxx =0.
For a fixed A, the cost L(A, B) is convex in B, so any matrix B that satisfies the above
equation corresponds to a minimum of L(A, B). Further, if A has no zero column then
by Lemma 2, Sp ◦ A0A is positive definite. Hence, ∀W ∈ Rp×n : d2B2 L(A, B)W2 =
2 Tr (W0 (Sp ◦ A0A) WΣxx) is positive. Therefore, the cost L(A, B) becomes strictly convex
ʌ
and the unique global minimum is achieved at B = B(A) as defined in eq. (6).
A.3 Proof of Proposition 2
For this proof we use the first and second order derivatives for L(A, B) wrt A derived in Lemma 6.
For a fixed B, based on eq. (69) the second derivative wrt to A of L(A, B) at A, and in the direction
V is the quadratic form
d2A2L(A, B)V 2 =2hV (Sp ◦ (BΣxxB0)), V iF =2Tr(V (Sp ◦ (BΣxxB0)) V 0).
The matrix Σxx is positive-definite and by Lemma 2, Sp ◦ (BΣxxB0) is positive-semidefinite.
Hence, d2A2 L1 (A, B)V 2is non-negative for all V ∈ Rn×p. Therefore, L(A, B) is convex in
coefficients of A for a fixed matrix B. Based on eq. (67) the critical point of L(A, B) for a fixed
B is a matrix A that satisfies for all V ∈ Rn×p
dAL(A, B)V = h-2 (ΣyxB0Tp - A (Sp ◦ (BΣxxB0))), V iF =0 =⇒
∑yχB0Tp = A (Sp ◦ (B∑xχB0)),
which is eq. (7).
A.4 Proof of Theorem 1
Before we start, a reminder on notation and some useful identities that are used throughout the
proof. The matrix Σ := ΣyxΣx-x1Σxy has an eigenvalue decomposition Σ = UΛU0, where the ith
column of U, denoted as ui, is an eigenvector of Σ corresponding to the ith largest eigenvalue of
Σ, denoted as λ%. Also, Λ = diag(λι,…，λn) is the diagonal vector of ordered eigenvalues of Σ,
with λι > λ2 > …> λn > 0. We use the following notation to organize a subset of eigenvectors
of Σ into a rectangular matrix. Let for any r ≤ p, Ir = {iι,…,i『}(1 ≤ iι < .…< ir < n) be
any ordered r-index set. Define UIr ∈ Rn×p as UIr = [uiι, •…,u“]. That is the columns of UIr
are the ordered orthonormal eigenvectors of Σ associated with eigenvalues λ汨 < … < λir. The
following identities are then easy to verify:
UI0r UIr =Ir ,
ΣUIr =UIr ΛIr ,	(34)
UI0r ΣUIr =ΛIr .	(35)
The sufficient condition:
Let A ∈ Rn×pof rank r ≤ p and no zero column be given by eq. (8), B ∈ Rp×n given by eq. (9),
and the accompanying conditions are met. Notice that UI0 UIr = Ir implies that DC0CD =
DC0 U0 UI CD = A0A, so	r
Ir r	,
B=D-1ΠCUI0rΣyxΣx-x1
Πc = (Spo(c0c))-1 TpC0
D-1D=Ip
16
Under review as a conference paper at ICLR 2020
B = DT (Sp ◦ (C0C))TDTDnC'U,∑yx∑U ====⇒
B= Sp ◦ (DC0CD)-1 Tp DC0UI0 ΣyxΣx-x1 ==A=0 ==D==0 C=0=U=I0⇒r
p —z	p{z IS yx Xx DC0CD=A0A
B = (Sp ◦ (A0A))-1 TpA0∑yx∑-x: = B(A),
which is eq. (6). Therefore, based on Proposition 1, for the given A, the matrix B defines a critical
point of L(A, B). For the gradient wrt to A, first note that with B given by eq. (9) we have
BΣxxB0 =D-1ΠCUI0rΣyxΣx-x1ΣxxΣx-x1ΣxyUIrΠ0CD-1
=D-1ΠC UI0rΣyxΣx-x1ΣxyUIr Π0CD-1 =eq=.=(3⇒5)
|{}
BΣxxB0 =D-1ΠCΛIrΠ0CD-1.	(36)
The matrix ΠC is a rectangular permutation matrix so ΠC ΛIr Π0C is diagonal so as
D-1ΠCΛIrΠ0CD-1.
Therefore, BΣxxB0 is diagonal and by eq. (27) in Lemma 2-6 we have
Sp ◦ (BΣxxB0) =TpBΣxxB0 = BΣxxB0Tp
=D-1ΠC ΛIr Π0C D-1Tp =A=⇒×
A (Sp ◦ (BΣxxB0)) =AD-1ΠCΛIrΠ0CD-1Tp =A====U=Ir=C=D⇒
A (Sp ◦ (BΣxxB0)) =UIrCDD-1ΠCΛIrΠ0CD-1Tp =A====U=Ir=C=⇒D
=UIr C∏C} Λir ∏CD-1Tp cπc=⇒
A (Sp ◦ (BΣxxB0)) =UIrΛIrΠ0CD-1Tp=eq=.=(3⇒4)
1~{{}
=ΣUIr∏0CD-1Tp
=ΣyxΣx-x1ΣxyUIr∏0CD-1Tp
0
=Σyx D-1∏CUI0rΣyxΣx-x1 Tp
}
=ΣyxB0Tp,
z
which is eq. (7). Therefore, based on Proposition Proposition 2, for the given B, the matrix A define
a critical point of L(A, B). Hence, A and B together define a critical point of L(A, B).
The necessary condition:
Based on Proposition 1 and Proposition 2, for A (with no zero column) and B, to define a critical
ʌ
point of L(A, B), B has to be B(A) given by eq. (6), and A has to satisfy eq. (7). That is
a (sp ◦ (B∑xxB0)) =∑yxB0Tp B(A)OnR⇒
A (Sp ◦ (B ∑xxB 0)) =∑xy ∑-1 ∑yxATp (Sp ◦ (A,A))-1Tp	=A ⇒
Σ=ΣxyΣx-x1Σyx
A (sp ◦ (BΣxxB0)) A0 =ΣATp(Sp O(AOA))TTpA0 S=U=U0；
U0A (Sp ◦ (BΣxxB0)) A0U =U0UΛUOATO(Sp ◦ (A0A))TTOAOU =PT⇒
U0A (Sp ◦ (BΣχxB0)) A0U =Λ∆,	(37)
where, ∆ := U0ATp(Sp ◦ (A0A))-1TpA0U is symmetric and positive semidefinite. The LHS of
the above equation is symmetric so the RHS is symmetric too, so Λ∆ = (Λ∆)0 = ∆0Λ0 = ∆Λ.
Therefore, ∆ commutes with the diagonal matrix of eigenvalues Λ. Since, eigenvalues are assumed
to be distinct, ∆ has to be diagonal as well. By Lemma 2Tp(Sp ◦ (A0A))-1Tp is positive definite
and U is an orthogonal matrix. Therefore, r = rank(A) = rank(∆) = rank(U 0 ∆U), which
implies that the diagonal matrix ∆, has r nonzero and positive diagonal entries. There exists an
17
Under review as a conference paper at ICLR 2020
r-index set Ir corresponding to the nonzero diagonal elements of ∆. Forming a diagonal matrix
∆Ir ∈ Rr×r by filling its diagonal entries (in order) by the nonzero diagonal elements of ∆ we
have
U ∆U 0 = UIr∆IrUI0r =D=ef=o=f ⇒∆
UU0ATp(Sp ◦ (A0A))-1TpAUU0 = UIr∆I,UIr =U==I⇒
ATp(Sp ◦ (A0A))-1 TpA0 = Uir∆irUI,,
(38)
which indicates that the matrix A has the same column space as UIr . Therefore, there exists a full
rank matrix CJ ∈ Rr ×p such that A = UIr CJ. Since A has no zero column, CJ has no zero column.
∙-v
Further, by normalizing the columns of C we can write A = UIr C D, where D ∈ Rp×p is diagonal
∙-v
that contains the norms of columns of J. Therefore, A is exactly in the form given by eq. (8). The
matrix J has to satisfy eq. (38) that is
ATp(Sp ◦ (A0A))TTDAO = Uir∆irUIr
UIr CDTD(Sp ◦ (AOA))TTO DC 0UOr = Ud UIr
A=UIr C
-------S
×UIr ,UIr ×
=========⇒
A0A=DC0CD
JDTp(Sp ◦ (DJ0JD))-1Tp J0D = ∆Ir =L=em=m=a=2⇒-2
JTpDD-1(Sp ◦ (J0J))-1D-1DTpJ0 = ∆Ir =⇒
CTp(Sp ◦ (C0C))-1 TpC0 = ∆Ir.
(39)
入	_
Now that the structure of A has been identified, evaluate B(A) of eq. (6) by setting A = UIrCD,
that is
B =B(A) = (Sp ◦ (AOA))TTBA0∑yχ∑U
=(Sp ◦ (DC0CD))TTDDC'U]ΣyχΣ袅=em==⇒
B =D-1(Sp ◦ (C0C))-1TpC0UI0rΣyxΣx-x1,
which by defining ΠC := (Sp ◦ (C0C))-1TpC0 gives eq. (34) for B as claimed. While C has to
satisfy eq. (39), A and B in the given form have to satisfy eq. (37) that provides another condition
for C as follows. First, note that
Sp ◦ (B∑xxB0) = Sp ◦ (DT(Sp ◦ (C0C))-1TpC0U{rΣUIrCTp(Sp ◦ (C0C))TDT)
=Sp ◦ (D-1(Sp ◦ (C0C))TToC0ΛIrCTp(Sp ◦ (C0C))TDT) Lemma⇒
=DT(Sp ◦ ((Sp ◦ (C0C))TToC0ΛIrCTp(Sp ◦ (C0C))-1)) DT
Now, replace A and B in eq. (37) by their respective identities that we just derived. Performing the
same process for eq. (37) we have
U0A (Sp ◦ (BΣχχB0)) A0U = Λ∆ A==IriCD-
UIrC (Sp ◦ ((Sp ◦ (C0C))-1TdC0ΛIrCTp(Sp ◦ (C0C))-1)) C0UI, = UΛ∆U0 ==⇒
r	UI0r ×
C (Sp ◦ ((Sp ◦ (C0C))-1TpC0ΛIrCTD(Sp ◦ (C0C))-1)) C0 = UIrUΛ∆U0UIr =⇒
C (Sp ◦ ((Sp ◦ (C0C))-1TpC0ΛIrCTD(Sp ◦ (C0C))-1)) C0 = ΛIr∆Ir.	(40)
Now we have to find C such that it satisfies eq. (39) and eq. (40). To make the process easier to
follow, lets have them in one place. The matrix C ∈ Rr×p have to satisfy
CTp (Sp ◦ (C0C))-1TpC0 =∆Ir and	(41)
C (Sp ◦ ((Sp ◦ (COC))TTDC0ΛIrCTO(Sp ◦ (C0C))-1)) C0 =ΛIr∆Ir.	(42)
Since C is a rectangular matrix, solving above equations for C in this form seems intractable. We
use a trick to temporarily extend C into an invertible square matrix as follows.
18
Under review as a conference paper at ICLR 2020
•	Temporarily, let	M1	=	Tp (Sp ◦	(C0C))-1	Tp,	and	M2	=	Sp	◦
((Sp ◦ (C0C))TTpC0ΛιrCTp(Sp ◦ (C0C))-1). Then Mi is positive definite and
M2 is positive semidefinite, so they are simultaneously diagonalizable by congruence that
is based on Lemma 3 and eq. (41) and eq. (42), there exists a nonsingular C ∈ Rp×p such
that C consists of the first r rows of C and
CTp (Sp ◦ (C0C))-1 TpC0=∆Ir,	(43)
C (Sp ◦ ((Sp ◦ (C 0C ))-1 TpC0 Λlr CTp (Sp ◦ (C 0C ))-1)) C0 =Λ Ir ∆ Ir,	(44)
where, ∆Ir = ∆ιr ㊉ Ir-p is a P X P diagonal matrix and ΛIr = Λιr ㊉ Λ is another P X P
diagonal matrix, in which Λ ∈ Rr-p×r-p is a nonnegative diagonal matrix.
•	Substitute ∆I from eq. (43) in eq. (44), then left multiply by C0-1, and right multiply by
C 0Ir∙p:
r;p
C (SpO ((Sp ◦ (C0C))-1 TpC0ΛIrCTp (Sp ◦ (C0C))-1)) C0
ΛIr CTp (Sp ◦ (C0C))-1 TpC0
C Ir;p ×
=====⇒
×C0-1
C0IrpC (Sp ◦ ((Sp ◦ (C0C))-1 TpC0ΛIrCTp (Sp ◦ (C0C))-1))=
C0Ir.pΛIr CTp (Sp ◦ (C0C))-1 Tp.
•	Now We can revert back everything to C again. Since C consists of the first r rows of C
We have C叮r；pC = C0C, and C0Ir∙pΛIr C = C0ΛIr C, which turns the above equation
into
C0C (Sp ◦ (Ip (Sp ◦ (C0C))-1 TpC0ΛIrCTp (Sp ◦ (C0C))-1 Ip))=
IpC0ΛIrCTp (Sp ◦ (C0C))-1 Tp.
•	In	the	above	equation, replace	Ip	by	Tp-1Tp in LHS and	by
T-1	(Sp ◦	(C0C))	T-1 Tp (Sp	◦ (C0C))-1	Tp	in	the RHS. Use ∏c	:=
(Sp ◦ (C0C))-1 TpC0 to shrink it into :
C0C (SpQ (T71"∏CΛIr∏CTpT-1)) =T- (Sp ◦ (C0C)) T-1Tp∏cΛIr∏CTp.
•	By the second property of Lemma 2 we can collect diagonal matrices Tp-1’s around Sp to
arrive at
(	C0C) (Sp ◦ (TBnCΛIrΠ0CTp)) = (^^ρ ◦ (C0C)) (Tp∏cΛI,Π,cTp),
where, Sp := T-1SpT-1.
• Define P X P matrices Er := C0C and Dr := TpΠCΛIrΠ0CTp. Substitute in the above to
arrive at:
Er Spρ ◦ Dr) = S^pp ◦ Er) Dr.
Both Dr and Er in the above identity are positive semidefinite. Moreover, since by as-
sumption C has no zero columns, Er has no zero diagonal element. Then the 7th property
of Lemma 2 implies the following two conclusions:
1.	The matrix Dr is diagonal. The rank of Dr is r so it has exactly r positive di-
agonal elements and the rest is zero. This argument is true for Tp-1DrTp-1 =
ΠCΛIrΠ0C. Since ΛIr is a diagonal positive definite matrix, the P X r matrix
ΠC := (Sp ◦ (C0C))-1 TpC0 of rank r should have P - r zero rows. Let Jr be
an r-index set corresponding to nonzero diagonal elements of ΠCΛIrΠ0C. Then the
matrix ΠC[Jr, Nr] (r X r submatrix of ΠC consist of its Jr rows) is nonsingular.
2.	For every i, j ∈	Jr	and i	6=	j,	(Er)i,j	=	0.	Since	Er	:=	C0C	and so	(Er)i,j
is the inner product of ith and jth columns of C, we conclude that the columns of
C[Nr, Jr] (r X r submatrix of C consist of its Jr columns) are orthogonal or in other
words C[Nr, Jr]0C[Nr, Jr] is diagonal. The columns ofC are normalized. Therefore,
C [Nr , Jr]0C [Nr , Jr] = Ir and hence, C [Nr , Jr] is an orthogonal matrix.
19
Under review as a conference paper at ICLR 2020
• We use the two conclusions to solve the original eq. (41) and eq. (42). First use ΠC :=
(Sp ◦ (C0C))-1 TpC0 to shrink them into :
CTpΠC =∆Ir ,	(45)
C (Sp ◦ (∏c Λir∏C)) C 0 =Λir ∆ir.	(46)
Next, by the first conclusion, the matrix Tp-1DrTp-1 = ΠC ΛIr Π0C is diagonal and so
eq. (46) becomes
CTpΠCΛIrΠ0CC0=ΛIr∆Ir =eq=.=(4⇒5)
、—{z—}
∆IrΛIrΠ0CC0 =ΛIr∆Ir =⇒
Π0CC0 = CΠC =Ir ,
(47)
which is one of the two claimed conditions. What is left is to show that ΠC is a rectangular
permutation matrix. From the first conclusion we also have ΠC has exactly r nonzero
columns indexed by Jr so
C[Nr , Jr]ΠC [Jr , Nr] =Ir .
By the second conclusion C [Nr , Jr] is an orthogonal matrix therefore, ΠC [Jr , Nr] is
the orthogonal matrix C[Nr, Jr]0. Moreover, we had Tp-1DrTp-1 = ΠC ΛIr Π0C
is a p × p diagonal matrix with exactly r nonzero diagonal elements. Hence,
ΠC[Nr, Jr]ΛIr Π0C [Nr, Jr] is an r × r positive definite diagonal matrix with ΛIr having
distinct diagonal elements, and ΠC [Nr , Jr] being orthogonal. Therefore, ΠC [Jr , Nr] (as
well as C[Nr , Jr]) should be a square permutation matrix. Putting back the zero columns,
we conclude that C should be such that ΠC := (Sp ◦ (C0C))-1 TpC0 is a rectangular
permutation matrix and CΠC = Ir . Note that it is possible to further analyze these con-
ditions and determine the exact structure of C . However, this is not needed in general for
the critical point analysis of the next theorem except for the case where r = p and C is
a square invertible matrix. In this case, square matrix ΠC is of full rank p, Jr = Np and
therefore, C[Nr , Jr] = C [Np, Np] = C. Hence, C is any square permutation matrix Π,
C0C = Π0Π = Ip and ∏c := (Sp ◦ (C0C))-1 TpC0 = Tp-ITpn0 = Π0, which verifies
eq. (10) and eq. (11) for A and B when A is of full rank p.
A.5 Proof of Corollary 1
1.	We already show in the proof Theorem 1 that for critical (A, B) the matrix BΣxxB0 is
given by eq. (36) that is
BΣxxB0 =D-1ΠCΛIrΠ0CD-1.
The matrix ΠC is a p × r rectangular permutation matrix so ΠC ΛIr Π0C is diagonal as
well as D-1ΠC ΛIr Π0C D-1. Therefore, BΣxxB0 is diagonal. The diagonal matrix ΛIr
is of rank r therefore, BΣxxB0 is of rank r.
2.	Again by Theorem 1 critical (A, B) is of the form given by eq. (8) and eq. (9) with the
proceeding conditions on the invariance C . Therefore, the global map is
G = AB = UIrCDD-1ΠCUI0rΣyxΣx-x1
= UIrCΠCUI0r ΣyxΣx-x1 =C==Π=C===I⇒r
G = UIrUI0rΣyxΣx-x1.
∙-v
3.	Based on Baldi & Hornik (1989) (A, B) define a critical point of L(A, B)=
Pip=1 kY - ABXk2F iff they satisfy
A0ABΣxx =A0Σyx and	(48)
ABΣxxB0 =ΣyxB0 .	(49)
Again by assumption (A, B) define a critical point of L(A, B) so by Theorem 1 they are
of the form given by eq. (8) and eq. (9) with the proceeding conditions on the invariance
C. Hence,
ΠCUI0rΣyx Σx-x1Σ
A0ABΣxx =DC0 UI Ul CDD-1
^z_}	^{{Z^}
xx
.}
1-----------V
20
Under review as a conference paper at ICLR 2020
=DC0 C∏c} UIr Σyχ c=c=⇒
A0ABΣxx =DC0UI0rΣyx = A0Σyx.
Hence, eq. (48) is satisfied. For the second equation we use the first property of this corol-
lary that is BΣxxB0 is diagonal and satisfy eq. (7) of Proposition 2 that is
A (Sp ◦ (BΣxxB0)) =ΣyxB0Tp
BΣxxB0 is diagonal
ATpBΣxxB0 =ΣyxB0Tp
BΣxxB0 is diagonal
ABΣxxB0T =Σ xB0T =⇒
xx p yx p
ABΣxxB0 =ΣyxB0.
Hence, the second condition, eq. (49) is also satisfied. Therefore, any critical point of
∙-v
L(A, B) is a critical point of L(A, B).
A.6 Proof of Lemma 1
Proof. We have
pp
L(A, B) =	kY -AIi;pBXk2F =	hY - AIi;pBX, Y - AIi;pBXiF
i=1	i=1
p
=	(hY,YiF + hY, -AIi;pBXiF +h-AIi;pBX,YiF
i=1
+h-AIi;pBX, -AIi;pBXiF)
=phY,YiF -2hY,A Xp Ii;p BXiF+Xp hAIi;pBX, AIi;pBXiF =eq=.=(2⇒5)
i=1	i=1
p
=pTr(YY0) - 2 Tr (ATpBXY 0) +	Tr(X0B0Ii;pA0AIi;pBX)
i=1
=pTr(Σyy) -2Tr(ATpBΣxy)+Tr XX0B0 X=p1 (Ii;pA0AIi;p) B =eq=.=(2⇒6)
= p Tr(Σyy) - 2 Tr (ATpBΣxy) + Tr (B0 (Sp ◦ (A0A)) BΣxx) ,
Whichiseq. (17).	□
A.7 Proof of Theorem 2
Proof. The full rank matrices A* and B* given by eq. (18) and eq. (19) are clearly of the form given
by Theorem 1 with Ip = Np := {1,2, ∙∙∙ ,p}, and ∏p = Ip. Hence, they define a critical point of
L(A, B). We Want to shoW that these are the only local minima, that is any other critical (A, B)
is a saddle points. The proof is similar to the second partial derivative test. However, in this case
the Hessian is a forth order tensor. Therefore, the second order Taylor approximation of the loss,
derived in Lemma 4, is used directly. To prove the necessary condition, we show that at any other
critical point (A, B), where the first order derivatives are zero, there exists infinitesimal direction
along which the second derivative of loss is negative. Next, for the sufficient condition we show that
the any critical point of the form (A*, B*) is a local and global minima.
The necessary condition:
Recall that UIp is the matrix of eigenvectors indexed by the p-index set Ip and ∏ is a p × p
permutation matrix. Since all the index sets Ir, r ≤ p are assumed to be ordered, the only way to
have UNp = UIp ∏ is by having Ip = Np and ∏ = Ip . Let A (with no zero column) and B define
an arbitrary critical point of L(A, B). Then Based on the previous theorem, either A = UIrC
with r < p or A = UIp ∏D while in both cases B = B(A) given by eq. (6). If (A, B) is not
of the form of (A*, B*) then there are three possibilities either 1) A = UIrCD with r < p, or 2)
21
Under review as a conference paper at ICLR 2020
A = UIp ΠD with Ip 6= Np or 2) A = UNp ΠD but Π 6= Ip. The first two cases corresponds to not
having the “right” and/or “enough” eigenvectors, and the third corresponds to not having the “right”
ordering. We introduce the following notation and investigate each case separately. Let ε > 0 and
Ui;j ∈ Rn×p be a matrix of all zeros except the ith column, which contains uj; the eigenvector of
Σ corresponding to the jth largest eigenvalue. Therefore,
Uij ∑Uij = UijU ΛU 0Uij = λj Ei,	(50)
where, Ei ∈ Rp×p is matrix of zeros except the ith diagonal element that contains 1. In what follows,
for each case we define a encoder direction V ∈ Rn×p with kV kF = O(ε), and set the decoder
direction W ∈ Rp×n as W = W := (Sp ◦ (A0A))-1 TpV0ΣyχΣ-1. Then We use eq. (30) and
eq. (31) of Lemma 4, to show that the given direction (V, W) infinitesimally reduces the loss and
hence, in every case the corresponding critical (A, B) is a saddle point.
1. For the case A = UIr CD , With r < p, note that based on the first item in Corollary 1,
BΣχχB0 is ap ×p diagonal matrix of rank r so it has p-r zero diagonal elements. Pick an
i ∈ Np such that (BΣχχB0)^ is zero and a j ∈ Np \ Ir. Set V = εUi jD and W = W.
Clearly,
V 0A =εDUij UIr CD = 0,
V 0V TpBΣχχB0 =ε2D Ui0;jUi;j DTpBΣχχB0,
(51)
、-V—}
ε2DEiDTpBΣχχB0 = ε2D2TpEi (BΣχχB0) = 0and (52)
V 0ΣV =ε2DU0j U ΛU Pij D = ε2λj D2Ei.
(53)
Notice, kVkF , kWkF = O(ε), so based on eq. (30) of Lemma 4, We have
L(A+V,B+W)-L(A,B) =
Tr (V0VTpBΣχχB0) - Tr V0ΣVTp (Sp ◦ (A0A))-1 Tp
+ 2Tr (V0A (Sp ◦ (B∑xyVTp (Sp ◦ (A0A))-1 + (Sp ◦ (A0A))-1 "V0∑yχB0)))
+ O(ε3 * ) =e=q=. (=5⇒1)
eq. (52)
L(A+V,B+W)-L(A,B) =
- Tr (V 0ΣV Tp (Sp ◦ (A0A))-1 Tp + O(ε3) =A=0=A=e=q=D.=(5C=3=0)C=D⇒
L(A+V,B+W)-L(A,B) =
D-1 + O(ε3) =
-ε2λj ((Spo(C0C))T) + O(ε3).
Therefore, since (Sp ◦ (C0C)) is a positive definite matrix, as ε → 0, we have L(A +
ʌ
V, B + W) ≤ L(A, B). Hence, any (A, B) = (UIrCD, B(UIrCD)) with r < p isa
saddle point.
; ◦
- ε2λj Tr D2EiD-1	Tp-1SpTp-
			
2. Next, consider the case where A = UIp ΠD with Ip 6= Np. Then there exists at least one
j ∈ Ip \ Np and i ∈ Np \ Ip such that i < j (so λi > λj). Let σ be the permutation
corresponding to the permutation matrix Π. Also, let ε > 0 and U°(j)；i ∈ Rn×p be
a matrix of all zeros except the σ(j)th column, which contains ui; the eigenvector of Σ
corresponding to the ith largest eigenvalue. Set V = εUσj)iD and W = W. Then, since
i ∈/ Ip we have
V 0UIp =εDUσ (j)"UIp = 0,	(54)
V0V =ε2DUσ(j)"Uσj)iD = ε2D2Eσ(j), and	(55)
22
Under review as a conference paper at ICLR 2020
V 0∑V =ε2DUσ (j)"U ΛU "(Q = ε2λiD2Eσ(j).	(56)
Since kVkF , kW kF = O(ε), based on eq. (31) of Lemma 4, we have
L(A + V, B + W) - L(A, B)=Tr (V0VΠ0ΛιpΠ"D-2) - Tr (V0ΣVnD-2)
+2Tr V0UIpΠD Sp ◦ D-1Π0UI0pΣVD-2
+2Tr (V0UipΠD (Sp ◦ (D-2V5%∏DT)))
+O(ε3) ====eq=.=(5=4=) =⇒
eq. (55),eq. (56)
L(A + V, B + W) - L(A, B)=Tr (ε2D2Eσ(j)∏0ΛipΠTpD-2)
-Tr (ε2λiD2Eσ(j)TpD-2) + O(ε3)
=ε2 Tr(E σ(j)∏0Λip ∏ Tp) -ε2% Tr(Eσj)Tp)+ O(ε3)
=ε2λj Tr (Eσ(j)Tp) - ε2λi Tr (Eσ(j)Tp) + O(ε3)
= - ε2(p - σ(j) + 1) (λi - λj) + O(ε3).
Note that in the above, the diagonal matrix Π0ΛIp Π has the same diagonal elements as
ΛIp but they are permuted by σ. So Eσ(j)Π0ΛIp Π selects σ(j)th diagonal element of
Π0ΛIpΠ that is the jthdiagonal element of ΛIp, which is nothing but λj. Now, since i < j
so λi > λj and σ(j) ≤ p, as ε → 0, we have L(A + V, B + W) ≤ L(A, B). Hence, any
(A, B) = (UIpΠD, B(UIp∏D)) is a saddle point.
3. Finally consider the case where A = UNp ΠD with Π 6= Ip. Since Π 6= Ip, the per-
mutation σ of the set Np , corresponding to the permutation matrix Π, has at least a cycle
(iιi2 •…ik), where 1 < iι < i2 •…< ik < p and 2 ≤ k ≤ p. Hence, Π can be decom-
posed as Π = Π(i1i2…iQ∏, where Π is the permutation matrix corresponding to other cy-
cles of σ. The cycle (iιi2 •…ik) can be decomposed into transpositions as (i1i2 .…ik)=
(ikik-1)…(ikiι), which in matrix form is Π(i1i2∙∙∙ik)= 口&五)n&i2)…∏(ikik-1).
〜〜 ʌ
Therefore, Π can be decomposed as Π = n出五)n，where Π = 口色m)…n&ik-ι)Π.
Note that Π(iki1), the permutation matrix corresponding to transposition (iki1) is a sym-
metric involutory matrix, i.e. n2廉”)=Ip. Set V = £(0”同 -Uikik )∏D and W = W.
Again we replace V and W in eq. (31) of Lemma 4. There are some tedious steps to sim-
plify the equation, which is given in appendix A.7.1. The final result is as follows. With
the given V and W, the third and forth terms of the RHS of eq. (31) are canceled and the
first two terms are simplified to
Tr (V0V∏0ΛNpΠTpD-2) =ε2λi% (P - iι + 1)+ ε2%1 (P - im + 1), and (57)
Tr (V0ΣVTpD-2) =ε2λiι (p - iι + 1)+ ε2λi% (p Tm + 1),	(58)
in which, m = max{k - 1, 2}. This means that If the selected cycle is just a transposition
(iιi2) then im, = i2. But if for the selected cycle (i1i2 •…ik), k is greater than 2 then
im = ik-1. Using above equations, eq. (31) yields
L(A+V, B+W)-L(A, B) =Tr (V0VΠ0ΛIpΠTpD-2)-Tr (V0ΣVTpD-2) +O(ε3)
=ε2λik (p - i1 + 1) + ε2λi1 (p - im + 1)
-ε2λi1 (p - i1 + 1) - ε2 λik (p - im + 1) + O(ε3)
= - ε2i1λik - ε2imλi1 + ε2i1λi1 + ε2imλik
= - ε2 ((λi1 - λik)(im - i1)) + O(ε3).	(59)
By the above definition of im, we have im - i1 > 0 and since i1 < ik, λi1 - λik > 0.
Hence, the first term in the above equation is negative and as ε → 0, we have L(A +
V, B + W) - L(A, B) < 0. Therefore, any any (A, B) = (UIpΠD, B(UIp∏D)) with
Π 6= Ip is a saddle point.
23
Under review as a conference paper at ICLR 2020
The Sufficient condition:
From Lemma 1 we know that the loss L(A, B) can be written in the form of eq. (17). Use this
equation to evaluate loss at (A*, B*) = (UNpDp, D-IUNρ∑yχ∑-J as follows
L(A*, B*) = pTr(Σyy) - 2Tr (AtTpB*Σχy) + Tr(B*0 (Sp ◦(A*0A*)) B*Σχχ) =⇒
L(A*, B*)= P Tr(∑yy)-2Tr (UNp DpTBD-IUN p ∑yx∑-⅛)
+ Tr
◦ Dp UN0 p UNp
D-1UNp ∑yx∑-1∑xx∑-1∑xy UNpD-1
|-------------{------}
L(A*, B*) = P Tr(Σyy) - 2Tr ITp DpD-I UN P ΣU^
1	1~{{}|—{—}
DpDp-1 UN0pΣUNpDp-1D
+ Tr
P
-----------------{z
L(A*, B*) = PTr(∑yy) - 2Tr ("AnJ + Tr ("ANJ =⇒
p
L(A*, B*) = p Tr(Σyy)-Tr (KANp) = P Tr(Σyy) - E(P - i +1) λ
i=1
which is eq. (20), as claimed. Notice that the above value is independent of the diagonal matrix Dp .
From the necessary condition we know that any critical point not in the form of (A*, B*) is a saddle
point. Hence, due to the convexity of the loss at least one (A*, B*) is a global minimum but since
the value of the loss at (A*, B*) is independent of Dp all these critical points yield the same value
for the loss. Therefore, any critical point in the form of (A*, B*) is a local and global minima. □
A.7.1 Supplementary details of the proof of Theorem 2
To verify eq. (57), eq. (58), and eq. (59) in the proof of Theorem 2, we want to replace V and W in
eq. (31) of Lemma 4 with V =已①五血-Ui,k-i,k )∏⅛D and W = W and simplify. eq. (31) is
L(A + V, B + W) - L(A, B)=Tr (V0VΠ0ΛιpΠ"D-2) - Tr (V0ΣVTpD-2)
+2Tr (V0UIpΠD (Sp ◦ (D-1Π0UI0pΣVD-2)))
+2Tr (V0Uιp∏D (Sp ◦ (D-2V0∑Uιp∏DT)))
+O(ε3).
We investigate each term on the RHS separately. but before note that
Ein Tpn0 = (∏ TDn 0)i 严= (TD)KCi),σ-ιCi)Ei = (P - σ-1(i) + 1)Ei,	(60)
where, σ and its function inverse σ-1 are permutations corresponding to ∏ and ∏0 respectively.
∏Tp∏0 is a diagonal matrix where diagonal elements of Tp are ordered based on σ-1. More-
over, recall that We decomposed the permutation matrix ∏ in A with a cycle (iιi2 •…ik) as
∏ = ∏(iιik)∏(iki2)…∏(ikik-ι)∏ = ∏(iιik)∏, where i”,…i® are fixed points of ∏. There-
1----------------------{----------}
∙-v
fore, with σ being the permutation corresponding to ∏ we have
σ(iι) = iι =⇒ σ-1(i1) = iι, and	(61)
σ(ik-i) = im =⇒ σ-1(ik) = im,	(62)
where, m = max{k - 1, 2}. This means that If the selected cycle is just a transposition (i1i2) then
im = i2. But if for the selected cycle (i1i2 •…ik), k is greater than 2 then im = ik-ι.
For the first term we have
V0V =ε2D∏Pii；ii- Uik；ik)(Uiι；ii - Uik；ik)∏D UI = UC = =0
24
Under review as a conference paper at ICLR 2020
V V =ε2D∏ ,(U⅛i1 Uii；ii + Ukk Uik 淙)∏ D
UUfi1=Eiι
⇒
Uk；ik Uk ；ik =Ek
V V =2DDfl <Ei1 + Eik )∏ D π'(Eι+EkmiSdiagon⇒
V 0V =ε2Π0(Eiι + Eik )∏ D2 =⇒
Tr (V0VΠ,ΛnpΠTpD-2) =Tr (V0VD-2Π'口^八修∏cMk)∏TP)
=Tr (ε2∏0(Ei1 + Eik)∏D⅛-∏｝口^包,Π"k)∏TP
∖	IP
=ε2 Tr ((Ei】+ Eik)Π(i1ik)Λnp口^)ΠTPn0)
=ε2 Tr (λikEgTpΠ0 + λ^ Eik ΠTPn0) ==6⇒
Tr (V0 VΠ0ΛNpΠipD-2) =S% (p - σ-1 &)+ 1)右μ + ε2λi1 (p - σ-1 (ik) + 1)Eik ===⇒
eq. (62)
Tr (V0 VΠ0ANpπtPD-2) =ε2λik (p - i1 + I)EiI + ε2λi1 (p - im + I)Eik ,
which is eq. (57) as claimed.
For the second term we have
V0∑V =ε2DK(Ui1；i1 - Ukrik)UΛU0(Ui1；i1 - Uikɪk)∏D
=ε2Dn0(U!1；i1 UΛU0Ui1"1- U^iι UΛUP^k
J 1------------V-----------'
0
'-------V-
λi1 Ei1
∙-v
-Uik ；ikU λu 0Ui1；i1 + Uk ；ikU λu 0Uikik )∏D
1------V------' 1------V-------'
0	" EZk
=ε2 ∏0 (λi1 Ei1 + λik Eik )∏ D2 =⇒
Tr (V0ΣVTpD-2) =Tr (ε2∏" % +	Eik )∏D2"D-2)
=ε2 Tr (λi1 %∏Tp∏0 + 猛EiknTPn0) ==⇒
Tr (V0ΣVTpD-2) =ε2羽(p - σ-1(iι) + 1) + ε2%k (p - σ-1(ik) + 1)=
eq. (62
Tr (V0ΣVTpD-2) =ε2入钉(p - " + 1) + ε2%k (p - im + 1),
which is eq. (58) as claimed.
Finally, we have to show that the third and the forth terms of the eq. (31) are canceled. First, observe
that
Tr(V0UnpΠD (Sp ◦ (d-1Π0Ui0jPΣVD-2)))=
Tr CDn0(u"1 - Uk；ik)UNp∏ (sp ◦ (∏0UNpςVD-2)))=
ε Tr (∏0(Ei1 - Eik)∏ (SPo (∏0U0P∑VD-2)) D)=
ε2 Tr (∏0(Ei1 - Eik)∏ (Sp o (∏0(%1 % -猛Eik)∏)))=
ε2 Tr ((Ei1 - Eik) ((∏Sp∏0) o (∏∏0(λi1 % -猛Eik)∏∏')))=
ε2 Tr ((∏SpΠ0) o ((Ei1 - Eik)(羽岛一MEik)))=
ε2 Tr ((∏SpΠ0) o (λi1 Ei1 + 猛 Eik)), and
Tr (V0UNPΠD (SPO(D-2V5%∏D-1)))
25
Under review as a conference paper at ICLR 2020
Tr (εD∏0(U0ιiι - Uik"口灰∏ (Sp ◦ (DTV0∑0Np∏D-1)))=
ε Tr (∏0(Eiι - Eik)∏ (Sp。(DTV5%∏)))=
ε2 Tr ((EiI- Eik )∏ (Sp。(∏'(%ι %- %上 Eik )∏)) ∏ 0)=
ε2 Tr ((EiI- Eik) ((∏Sp∏ 0)。(∏∏ K %- 猛 Eik )∏∏ 0)))=
ε2 Tr ((EiI- Eik) ((∏Sp∏0) ◦ (∏(iιik)(λiιEii -XkEik)∏(Mk))))=
ε2 Tr ((EiI- Eik) ((∏Sp∏0)。(。江Eik- MEiI))))=
ε2 Tr ((∏Sp∏0) ◦ ((Eii - Eik)(λiιEik - λikEii)))=
-ε2 Tr ((∏Sp∏0)。(λi1 Eik + λik Ei1 ))=
-ε2 Tr ((∏Sp∏0) ◦ (λi1 Eik + λik Ei1 )).
∙-v
Now, note that in both cases the matrices that are multiplied elementwise with ∏Sp∏0 are diagonal
and hence, We only need to look at diagonal elements of ∏Sp∏0. Moreover,
00
∏SP∏ = ∏(i1ik)∏(iki2)…∏(ikik-i) ∏SP∏ ∏(ikik-i )…∏(iki2),
where, iι ∙∙∙ ik are fixed points of permutation corresponding to ∏
ʌ ʌ
so ∏Sp∏0 has the same values
at diagonal positions i1 and ik as the original matrix Sp . The only permutation that is only on the
left side is ∏(iiik) which exchanges the i1 and ik rows of Sp. Since Sp is such that the elements
at each row before the diagonal element are the same and ik > i1, we have the i1 and ik diagonal
∙-v
elements of ∏Sp ∏0 have the same value. Let that value be denoted as s. Then the sum of the above
two equations yields m(λii + λik) - m(λii + λik) = 0, as claimed.
26
Under review as a conference paper at ICLR 2020
B Derivatives of the Loss function
B.1	First and Second Order FRECHET Derivative
In order to derive and analyze the critical points of the cost function which is a real-valued function
of matrices We use the first and second order Frechet derivatives as described in chapter 4 of Zeidler
(1995). For a function f : Rn×m → R the first order FreChet derivative at the point A ∈ Rn×m is a
linear functional df (A) : Rn×m → R such that
|f (A + V)-f(A)- df (A)V| = 0
Vm0	kVkF	=0,
Where We used the shorthand df (A)V ≡ (df(A))(V). Similarly, the 2nd derivative is a bilinear
functional d2f(A) : Rn×m × Rn×m → R such that
Idf (A + V )K - df (A)K - d2f (A) V K I= 0
Vm0	西	=,
for all kKkF ≤ 1, Where again d2f(A)VK ≡ (d2f(A))(V, K). The generalized Taylor formula
then becomes:
f (A + V) = f (A) + df (A)V + 2 d2f (A)V2 + o(∣∣ Vk2),
Moreover, we derive functions Vf : Rn×m → Rn×m and H (A) : Rn×m → Rn×m such
that df (A)V = (Vf(A), V〉f and d2f (A)V2 = {H (A)V, V〉f, where again H (A)V ≡
H(A)(V). Then clearly, A ∈ Rn×mis a critical point of f iff Vf (A) = 0 and for such As
the sign of the bilinear form hH(A)V, V iover directions V determines the type of the critical
point.
Extending the generalized Taylor theorem of Zeidler (1995), the second order Taylor expansion for
the loss L(A, B) is then given by
L(A + V, B + W) — L(A, B) =dAL(A, B)V + dBL(A, B)W + 1 dAL(A, B)V2
+dABL(A, B)VW + 2dBL(A, B)W2 + Rv,w(A, B),
(63)
where, if kVkF , kWkF = O(ε) then kR(V, W)k = O(ε3). Clearly, as at critical points where
dAL(A, B)V+dBL(A, B)W = 0, as ε → 0 we have RV ,W (A, B) → 0 and the sign of the sum
of the second order partial Frechet derivatives determines the type of the critical point very much
similar to second partial derivative test for two variable functions. However, here for local minima
we have to show the sign is positive in all directions and for saddle points have to show the sign is
positive in some directions and negative at least in on direction. Finally, note that the smoothness of
the loss entails that Frechet derivative and directional derivative (Gateaux) both exist and (foregoing
some subtleties in definition) are the same.
B.2	FiRST AND SECOND ORDER DERivATivE OF THE LOSS WRT TO B
Lemma 5. Thefirst and second (partial Frechet) derivative ofthe loss L(A, B) wrt to B is derived
as follows.
dBL(A, B)W = -2 Tr (W0 ("A0∑yχ - (Sp ◦ (A0A)) B∑χχ))	(64)
=-2(TpA0Σyχ - (Sp ◦ (AA)) B∑χχ, WiF.	(65)
d2B2 L(A, B)W2 = 2((Sp ◦ (A,A)) W Σχχ, W〉f =2Tr(W0 (SP ◦ (A,A)) W Σχχ). (66)
Proof. Directly compute
p
L(A,B+W)=	kY -AIi;p(B+W)Xk2F
i=1
27
Under review as a conference paper at ICLR 2020
p
= hY - AIi;p(B + W)X, Y - AIi;p(B + W)XiF
i=1
pp
= 1hY - AIi;pBX, Y -AIi;pBXiF + 1hY - AIi;pBX, -AIi;pWXiF
pp
+Xh-AIi;pWX,Y - AIi;pBXiF +Xh-AIi;pWX, -AIi;pWXiF
i=1	i=1
p
=L(A,B)-X2hY -AIi;pBX,AIi;pWXi+O(kWk2F) =⇒
i=1
p
L(A,B+W)-L(A,B) =-2	hY -AIi;pBX,AIi;pWXiF+O(kWk2F) W=⇒→0
i=1
p
dBL(A, B)W = -2	Tr(X0W0Ii;pA0(Y - AIi;pBX))
i=1
-2Tr(W0 (TpA0Y X0 - (Sp ◦ (A0A)) BXX0)) ,
which can be written as the given form. For the second derivative wrt B we have
dBL(A, B)W = -2hTpA0Σyx - (Sp ◦ (A0A)) BΣxx, WiF =⇒
dB L(A, B + W)W = -2hTpA∑yχ -(SPQ (A0A))(B + W)∑xx, W〉f
=-2hTpA∑yχ - (Sp ◦ (A0A)) B∑xx, W〉f
+ 2h(Sp ◦ (A0A)) WΣχχ, W〉f =
dBL(A, B + W)W - dBL(A, B)W = 2((Sp ◦ (A0A)) WΣχχ, W〉f,
which by having W → 0 results in the second order partial derivative.
□
B.3	FIRST AND SECOND ORDER DERIVATIVE OF THE LOSS WRT TO A
Lemma 6. Thefirst and second (partial Frechet) derivative ofthe loss L(A, B) wrt to A is derived
as follows.
dAL(A, B)V = -2h∑yχB,Tp - A (Sp ◦ (BΣχχB0)), V〉尸,	(67)
dABL(A, B)VW = -2h∑yχW0Tp - A(SP ◦ (BΣχχW0)) - A(SP ◦ (WΣχχB0)), V〉尸,
(68)
d2A2L(A, B)V2= 2hV (Sp ◦ (BΣxxB0)), ViF.	(69)
Proof. Directly compute
p
L(A+V,B) =XhY - (A + V)Ii;pBX, Y - (A + V)Ii;pBXiF
i=1
pp
=	1hY - AIi;pBX, Y -AIi;pBXiF- 1hY - AIi;pBX, VIi;pBXiF
pp
+	h-VIi;pBX, Y -AIi;pBXiF+	h-VIi;pBX, -VIi;pBXiF
i=1	i=1
pp
= L(A, B) -X2hY -AIi;pBX, VIi;pBXiF+XhVIi;pBX, VIi;pBXiF
i=1	i=1
28
Under review as a conference paper at ICLR 2020
P
L(A + V, B)- L(A, B) = -E 2(Y - AI»,PBX, VIi pBX)尸 + O(IMIF) V⇒0
i=1
P
dAL(A, B)V = -E 2(Y - AIi；PBX, VIi pBX}f
i=1
PP
=-2Tr(V,(ΣyxB' X Ii；P - AXIitpBΣxxB'Ii；p)) =⇒
i=1	i=1
dAL(A, B)V = -2(ΣyxB'Tp - A (Sp ◦ (BΣxxB,)), V〉f =⇒
dAL(A + V, B)V = -2(ΣyxB,Tp - (A + V^)(Sp ◦ (BΣxxB,)), V〉尸
dAL(A + V, B)V - dAL(A, B)V = 2(V (Sp ◦ (BΣχχB0)), V〉尸 V⇒0
dA2L(A, B)(V, V) = 2(V (Sp 0 (BΣχχB0)), V〉f =⇒
dA2L(A, B)V2 = 2iy (Sp 0 (BΣχχB0)), V}f
dAL(A, B + W)V = - 2{Σyχ(B + W)0TP, V)尸
-	2h-A (Sp 0 ((B + W)Σχχ(B + W))), V〉f
-	2②yχB Tp - A (Sp O(BςXXBO)) , V〉F
=dAL(A, B)V - 2h∑yχW 'Tp, V}f
-	2{-A (Sp 0 (BΣχχW0)) - A (Sp 0 (WΣχχB0)), V)f + O(∣W∣∣F) =⇒
dAL(A, B + W)V - dAL(A, B)V = -2{ΣyχW'Tp, V}f
-2h-A (Sp 0 (BΣχχW0)) - A (Sp 0 (WΣχχB0)), V〉f
+ O(∣∣W IIF) w⇒0
d2AB L(A, B)VW = -2hΣyχW 0Tp - A (Sp 0 (BΣχχ W 0)) - A (Sp 0 (W ΣχχB0 )), V〉F .
□
29