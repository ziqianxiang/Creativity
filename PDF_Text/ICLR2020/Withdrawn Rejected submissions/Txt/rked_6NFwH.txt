Under review as a conference paper at ICLR 2020
Path Space for Recurrent Neural Networks
with ReLU Activations
Anonymous authors
Paper under double-blind review
Ab stract
It is well known that neural networks with rectified linear units (ReLU) activation
functions are positively scale-invariant (i.e., the neural network is invariant to
positive rescaling of weights). Optimization algorithms like stochastic gradient
descent that optimize the neural networks in the vector space of weights, which are
not positively scale-invariant. To solve this mismatch, anew parameter space, called
path space, has been proposed for feedforward and convolutional neural networks.
The path space is positively scale-invariant and optimization algorithms operating
in path space have been shown to be superior than that in the original weight space.
However, the theory of path space and the corresponding optimization algorithm
cannot be naturally extended to more complex neural networks, like Recurrent
Neural Networks (RNN) due to the recurrent structure and the parameter sharing
scheme over time. In this work, we aim to construct path space for RNN with
ReLU activations so that we can employ optimization algorithms in path space.
To achieve the goal, we propose leveraging the reduction graph of RNN which
removes the influence of time-steps, and prove that all the values of paths in the
reduction graph can serve as a sufficient representation of the RNN with ReLU
activations. We then prove that the path space for RNN is composed by the basis
paths in reduction graph, and design a Skeleton Method to identify the basis paths
efficiently. With the identified basis paths, we develop the optimization algorithm in
path space for RNN models. Our experiments on several benchmark datasets show
that we can obtain more effective RNN models in this way than using optimization
methods in the weight space.
1	Introduction
Over the past ten years, ReLU activations have become increasingly popular in various types of
neural networks such as Multilayer Perceptron(MLP) (Nair & Hinton, 2010; Glorot et al., 2011;
Neyshabur et al., 2015a) , Convolutional Neural Networks (CNN) ((Krizhevsky et al., 2012; He et al.,
2016)) and Recurrent Neural Networks (RNN) (Le et al., 2015; Neyshabur et al., 2016). Theoretical
studies on the MLP and CNN with ReLU activations, particularly on its positively scale-invariant
(PSI) property have also been conducted (Neyshabur et al., 2016; 2015a; Dinh et al., 2017; Meng
et al., 2018). Specifically, PSI property means if the incoming weights of a hidden node (or a feature
map for CNN) are multiplied by a positive scalar c, and the outgoing weights of this hidden node
(or the feature map) are divided by c, the output for arbitrary input will keep unchanged. However,
conventional algorithms like stochastic gradient descent optimize the neural networks in the vector
space of weights, which are not positively scale-invariant. This mismatch may lead to problems
during the optimization process (Neyshabur et al., 2015a; Meng et al., 2018), e.g., in an unbalanced
network (i.e., the norm of incoming weights to different units have relatively large differences), the
training process of vanilla stochastic gradient descent will be significantly slowed down.
To solve the above mismatch, some recent studies have been conducted on paths of ReLU neural
networks (Neyshabur et al., 2015a; Meng et al., 2018). The value of a path is defined as the
multiplication of weights along the path which starts from an input node, successively crosses a
hidden node at every layer, and finally ends at an output node. Obviously, the value of a path is
positively scale-invariant. Thus, a new parameter space, called path space, for MLP and CNN have
been proposed. Path space is a vector space constituted by the values of given paths called basis
paths, whose values are sufficient to calculate the output of the MLP and CNN with ReLU activations.
1
Under review as a conference paper at ICLR 2020
In a word, path space is a PSI parameter space and can sufficiently represent MLP and CNN with
ReLU activation. In the work (Meng et al., 2018), an algorithm is designed to identify the basis paths
for MLP and CNN. Efficient optimization algorithms in path space have also been proposed, which
iteratively update the values of basis paths according to the gradient of loss with respect to basis
paths. Optimizing MLP or CNN with ReLU activations in this way can achieve better performances
compared with traditional optimization methods in the weight space.
However, the studies of path space especially the identification of basis paths depend on the network
structure, so it cannot be naturally extended to more complex network structures, like recurrent
neural network (RNN). RNN is a powerful neural network model to process sequential data and
show excellent performance in various domains such as language modeling(Mikolov et al., 2010;
Jozefowicz et al., 2016), machine translation(Bahdanau et al., 2014) and speech recognition (Graves
et al., 2013; Miao et al., 2015). RNN with ReLU activations (abbrev. ReLU RNN) is also positively
scale-invariant (Neyshabur et al., 2016). Considering the superiority of optimizing MLP or CNN with
ReLU activations in their path space, the path space for RNN is worth studying.
In this work, we aim to construct path space for ReLU RNN so that we can employ the optimization
algorithm in path space. Due to the recurrent structure, the paths in RNN depend on the time-steps.
That means, if the sequence length of the input is not pre-given, the number of paths in RNN cannot
be determined. It makes the identification of basis paths from the undetermined number of paths
difficult. To handle this, we propose leveraging the reduction graph of RNN to remove the influence
of time-steps and theoretically develop path space for RNN in the following three steps. First, we
define paths on reduction graph, whose number is fixed and not influenced by the sequence length.
Second, we prove that the paths on reduction graph can serve as a sufficient representation of the
RNN with ReLU activations. Third, we define path space for RNN which is composed by the values
of basis paths in the reduction graph.
Next, we design Skeleton Method to identify basis paths in reduction graph of RNN efficiently.
Compared with MLP and CNN, the skeleton method for RNN needs extra steps to handle the recurrent
weights. Specifically, the skeleton method for RNN contains two steps: it first identifies the basis
paths in the reduction graph of RNN without recurrent weights; then it constructs the basis paths that
contain recurrent weight. We prove that paths selected by Skeleton Method are basis paths, and the
number of basis paths which is also the dimension of the path space is #(weights)-#(hidden nodes)
in the reduction graph of RNN.
Finally, with the identified basis paths, we employ optimization algorithm in path space for RNN
models, which operates update on basis paths according to the gradient of loss w.r.t basis paths. Our
experiments on several benchmark datasets show that we can obtain significantly more effective RNN
models in this way than using optimization methods in the weight space. It indicates that optimizing
ReLU RNN in path space can solve the optimization problem brought by positively scale-invariant
property and can indeed help optimization. To the best of our knowledge, our work is the first to
construct the new parameter space, path space, for the RNN model and the first to optimize RNN in
its path space.
The paper is organized as follows: in Section 2, we introduce the background about the path
representation of ReLU RNN and related works; in Section 3, we formally define the basis path for
ReLU RNN and propose using reduction graph as a tool to handle the difficulty when identifying the
basis paths in ReLU RNN; in Section 4, we present our the basis path identification algorithm and
the optimization algorithm in the path space; in Section 5, we show the experiment results to verify
the effectiveness of our proposed algorithm; finally, we conclude this paper in Section 6.
2	Background
In this section, we briefly review the path representation of recurrent neural networks with ReLU
activations and the related works in this direction.
2.1	Path Representation of ReLU RNN
As shown in Figure 1, the network structure of RNN can be regarded as a directed graph
G(N , E), where N = ∪tNt denotes the set of nodes and E denotes the set of edges, where
2
Under review as a conference paper at ICLR 2020
Nt ={NIs),n2")…，NH+S), NH2),…，NOh+ι, NOh+2 …NOh+κ} denotes the set of nodes
at the t-th time-step, the superscripts I , H, O means the input, hidden and output node, respectively.
Here, d, h, K denote the dimension of input, hidden nodes and output, respectively, and the super-
script t denotes the time-step. In the graph G(N, E), there must be an edge between an input node
and a hidden node that are at the same time step, a hidden node and an output node that are at the
same time-step, and two hidden nodes whose time-steps are adjacent. A Path on the G(N, E) is
defined in Definition 1.
Definition 1 (Paths in RNN). A path in the recurrent neural network is defined as a list of nodes
and the corresponding edges between adjacent nodes, which starts from an input node NiI,(t) to one
output node NiO,(t+s) with s ≥ 0 by successively crossing hidden nodes. Specifically, a path can be
expressed as Nat) - Nyt) → ∙ ∙ ∙ → NH[})→ N；[：+S), and there must be an edge between the
adjacent nodes, where i. is the index of node along the path.
For example, the red line in Figure 1 shows an example of the paths.
For simplicity, we omit the explicit expression of paths and use p
to denote the index of paths in the following context. The value of
a path is defined as the multiplication of the values of weights on
edges contained in the path. Specifically, for an edge ek ∈ E , we
use wk to denote the weight on edge ek, then the value of path p is
vp = Qek∈p wk.
Using Wi, Wo and Wr to denote three weight matrices, use x(t) to Figure 1: DAG of RNN
denote the input and use nH,(t), nO,(t) to denote the corresponding
hidden and output values at nodes N H,(t) and N O,(t), recurrent neural network computes the output
vector nO,(t) at t-th time-step by a recurrent formulation as:
nH,(t) = σ(Wix(t) +WrnH,(t-1)),	(1)
nO,(t) = WonH,(t) .	(2)
If the activation σ(∙) is ReLU, i.e., σ(z) = max(z, 0), We can get no,(t = Wo ∙ (Wix(t) +
WrnH,(tT)) ∙ diag(I(nH,(t) > 0),∙∙∙ ,I(nH,(t) > 0)) by combining Eq.(1) and Eq.(2). 1 ACCord-
ing to the definition of value of a path, the output of ReLU RNN can be calculated using the values of
paths and its activation status as folloWs,
dT
nko,(T)(x) =XX	X
Vp ∙ ap (w; x) ∙ x(t),	(3)
i=1 t=1 p∈PT,k,i,t
Where the set PT,k,i,t consists of the paths that connects the k-th output at time-step T and
the i-th input at time-step t With length T + 1 - t, vp is the value of path p, and ap (w; x) =
QN H,(t) ∈ I(niH,(t) (w; x) > 0) is the activation status of path p.
Ni	∈p
2.2	Related Works
In recent years, researchers have started to conduct theoretical studies on optimization and generaliza-
tion of ReLU neural netWork by leveraging paths instead of Weights. In the aspect of generalization,
the Works (Neyshabur et al., 2015b; 2017; Zheng et al., 2018) shoW that the generalization error of
feedforWard ReLU netWork is related to the path-norm. The Work (Weinan et al., 2019) also leverages
the path representation to analyze the generalization error of ReLU neural netWorks. Except for
the generalization analysis, optimization algorithms based on paths are also designed. Neyshabur,
et al. propose Path-SGD algorithm, Which optimizes the loss function regularized by path-norm
(Neyshabur et al., 2015a; 2016). Path-SGD algorithm does not directly optimize ReLU RNN using
the loss function composed by the path-represented output but optimizes the regularized loss defined
in the original Weight space. The Work (Meng et al., 2018) proposes G-SGD algorithm to optimize
the ReLU MLP and CNN directly by updating the values of paths according to the gradient of loss
W.r.t basis paths. HoWever, G-SGD is only designed for MLP and CNN. To the best of our knoWledge,
the path space constituted by basis paths for ReLU RNN has not been studied yet.In this paper, We
Will investigate the path space for RNN so that We can apply G -SGD to RNN.
1Here, diag (aɪ,…,ah) denotes a diagonal matrix whose diagonal elements are aɪ,…,ah.
3
Under review as a conference paper at ICLR 2020
3	Path Space for ReLU RNN
In this section, we investigate the path space, a vector space constituted by values of basis paths
for ReLU RNN. We first formally define the basis paths for ReLU RNN. Then we propose using
reduction graph as a tool to define the path space for ReLU RNN.
3.1	Definition of Basis Paths for RNN
As stated in the introduction, it has been shown in (Neyshabur et al., 2016) (cf. Theorem 1 and Figure
1) that the output of RNN is positively scale-invariant (PSI) i.e., if all the incoming weights(including
the shared weights) of a hidden node are multiplied by a positive scalar c and all the outgoing weight
(including the shared weights) of this hidden node are multiplied by 1/c, ReLU RNN will generate
the same output for arbitrary input. Because the values of paths are also positively scale-invariant,
which matches the PSI property of ReLU RNN, we are motivated to directly represent and optimize
the RNN model according to the path representation (Eq.(3)).
However, we cannot directly optimize the values of all the paths by regarding each of them to be
an independent parameter, because they are correlated with each other. We use Figure 1 to show
an example. For simplicity, we denote the weight of the edge that connects node i and node j as
wij .2 We denote the red path in Figure 1 as p1 : N2I,(1) → N3H,(1) → N3H,(2) → N6O,(2) whose value
is Vpi = w23 ∙ w33 ∙ w36. We can show that the value of path pi can be calculated by the values of
following four paths through multiplication and division.
p2 : N2I,(1) → N3H,(1)	→ N5O,(1)	vp2	二 W23 ∙ W35
p3 : N1I,(1) → N3H,(1)	→ N3H,(2) → N5O,(1)	vp3	二 W13 ∙ W33 ∙ W3,5
p4 : N1I,(1) → N3H,(1)	→ N6O,(1)	vp4	二 W13 ∙ W36
p5 : N1I,(1) → N3H,(1)	→ N5O,(1)	vp5	二 W13 ∙ W35.
VP2 ∙ VP3 ∙ vp4
vpi =	(VP5 )2
It is easy to verify that
(4)
Because Vpi can be calculated using Vp2,…，Vp5, We only need to know Vp2,…，Vp5 and the relation
in Eq.(4) when to represent the output of RNN.
For any given recurrent neural network, we need to first investigate and decouple the correlation
among their values so that we can identify a subset of paths whose values can be directly optimized.
Motivated by the example, we formally define basis paths as follows.
Definition 2 (Basis Paths). A set of paths Pb are called basis paths if they satisfy the following
properties: (1) for any path P ∈ P/Pb, there exists non-zero coefficient vector α = (αι, .…,az)
to make vp(w) = j∈P vj (w)αj for any w ∈ W, where z denotes the cardinal number of Pb, i.e.,
z = |Pb |; (2) among all the sets that satisfy property (1), Pb has the smallest cardinal number.
The first item in the definition ensures the sufficiency to use basis paths to calculate the values of
other paths, and the second item ensures that the values of basis paths are independent, i.e., the values
of basis paths cannot be calculated by each other. In the work (Meng et al., 2018), basis paths have
been defined for MLP, however, it is no longer suitable for RNN. Here, the definition 2 for basis paths
is more general and can be generalized to many kinds of network structures.
3.2	Defining Path Space for ReLU RNN through Reduction Graph
Although basis paths have been defined in the previous section, given an RNN, it is difficult to identify
the basis paths because: (1) the number of paths exponentially depends on the time-steps, which is
very large in general; (2) if the sequence length of input is not pre-given, the exact number of paths in
RNN cannot be determined. To handle this, we study the paths in reduction graph of ReLU RNN.
Here, we give the definition of the reduction graph for RNN and show an example in the Figure 2.
2The weights are shared for all time-steps, so we don’t use the subscript of time-step t in the weight matrices.
4
Under review as a conference paper at ICLR 2020
Definition 3 (Reduction Graph). Given a directed graph of RNN, we
use Gr (N r , Er) to denote its corresponding reduction graph, where
N r = {NI , N2 …，NHhI, Nd+2, ∙ ∙ ∙ , Nd+h+1 ,NO+h+2 …Nd+h+K }
denotes the set of nodes and Er denotes the set of edges. There must
be an edge between an input node and a hidden node, a hidden node
and an output node and two hidden nodes, respectively. We call the
edges that connect two hidden nodes as recurrent edges.
Figure 2: Reduction Graph
of RNN
Definition 4 (Path in Reduction Graph). A path in the reduction graph for RNN is a list of nodes
that satisfies the following conditions: (1) it starts from an input node and end at an output node of
the reduction graph; (2) it can at most cross one recurrent edge.
According to the definition, the paths in reduction graph are classified into two categories: the paths
does not contain recurrent edge denoted as NiI0 → NiH1 → NiO2 ; contain one recurrent edge denoted
as NiI0 → NiH1 → NiH2 → NiO3 . Hence, the paths in reduction graph is not related to the time-steps.
That means, no matter how long the sequence length of the input, the number of paths in reduction
graph is fixed. The next theorem shows that the values of paths in the reduction graph are sufficient
to represent RNN.
Theorem 1.	All the values of paths in directed graph of RNN can be calculated using the values of
paths in the reduction graph of RNN through multiplication and division operators.
We put the proof of Theorem 1 in Appendix and only deliver the techniques used in the proof here. We
index the trained weights as w1,… ,wi,…，Wm, and represent each path using an m-dimensional
vector Cp = (cp1,… ,Cpm) where Cp satisfy Vp = Qm=I Wcpi. We prove that the vectors of all paths
can be calculated through linear combinations of vectors of paths in reduction graph. Thus, we can
get the result in Theorem 1.
We use P to denote the set constituted by all paths of RNN, Pr to denote the set constituted by
paths in reduction graph and Pb to denote the set constituted by basis paths of RNN. According to
Definition 2 and Theorem 1, we have Pb ⊂ Pr ⊂ P . Thus, the basis paths in the reduction graph of
RNN is the basis paths of RNN model. Paths in reduction graph of RNN is totally determined by the
model and not influenced by the time-steps, and as well as the basis paths.
Next, we show that basis paths are sufficient to represent the output of ReLU RNN. According to
Eq.(3) and item (1) in the definition of basis paths, we only need to prove that the activation status of
paths can also be determined by the values of basis paths.
Theorem 2.	The activation status of the paths in ReLU RNN can be calculated using the values
of basis paths and the signs of the selected elements in W o if for each column in matrix W o with
dimension K × h, we randomly select one element whose sign is pre-given and fixed.
Using the basis paths, we can construct a new vector space, called path space, for ReLU RNN as :
V ：= {v = (vpι,…，vpz) ： v ∈ (R∕{0})z},	(5)
where Pi, i = 1, ∙ ∙ ∙ , z are the basis paths for RNN.
Till now, we have defined path space for ReLU RNN and proved that path space is sufficient to
represent ReLU RNN in Theorem 2. In next section, we will introduce how to optimize ReLU RNN
in its path space including that how to identify basis paths for given reduction graph.
4 Optimization Algorithm in Path Space for RNN
In this section, we design Skeleton Method to identify the basis paths in reduction graph of RNN
efficiently. Then, we introduce the update rule of optimization algorithm in path space for RNN.
We decompose the reduction graph of recurrent neural network into two parts: the feedforward part
and the recurrent edges part. The feedforward part contains all nodes in reduction graph and the
non-recurrent edges, and the recurrent edges part contains only the recurrent edges.
5
Under review as a conference paper at ICLR 2020
(a)
(b)
Figure 3: (a) An example of reduction graph. (b) An example of Skeleton Method
In Skeleton Method, we first define the skeleton edges in the feedforward part and then use the skeleton
edges to construct the basis paths. We prove that the paths that contains at most one non-skeleton
edges are basis path.
The Skeleton Method for RNN is described in the following box.
Skeleton Method
(1)	For each hidden node in the feedforward part, we randomly select one edge that pointing to this
hidden node and randomly select one edge that starts from this hidden node. Putting the selected
edges of all hidden nodes together, all the selected edges are called skeleton edges. All other edges
are called non-skeleton edges.
(2)	The paths that contain at most one non-skeleton edges in the feedforward part are selected into
set Pb1 .
(3)	The paths containing only skeleton edges and one recurrent edge are selected into set Pb2 .
(4)	The final selected path set is Pb1 ∪ Pb2 .
We illustrate the Skeleton Method in Figure 3(b). The red edges are the skeleton edges selected in
step (1). Left three basis paths are in Pb1 and the last one is in Pb2 . For the paths in Pb1 , we further
denote the set constituted by paths only contain skeleton edges as Ps and the set constituted by paths
contain one non-skeleton edges as Pns .
In the following theorem, we prove that the paths selected by Skeleton Method are basis paths.
Theorem 3. The set of paths Pb1 ∪ Pb2 selected by the Skeleton Method are basis paths. The number
of basis paths (or equivalently the dimension of the path space) for recurrent neural networks is
#(weights)-#(hidden nodes), where “#” means “the number of”.
After identifying the basis path using Skeleton Method, we aim to optimize the loss function L(v) by
following the negative direction of the gradients of loss w.r.t basis paths as below,
v
t+1
pi
t	∂L(v)
=vPi-ηt∙西T
=vt
(6)
where pi is the index of the basis paths. G-SGD algorithm has been designed in work (Meng et al.,
2018), which can efficiently implement the above update rule in path space for MLP. Here, we apply
G-SGD to RNNs. Due to space limitation, we only introduce the high-level idea as below and the
update rule in Alg. 2. For more details about its derivation, please refer to the Appendix B.1. or
Eq.(4) and Eq.(5) in (Meng et al., 2018).
First, we construct mask matrices to distinguish the skeleton edge and non-skeleton edge according
to step (1) in skeleton method. As shown in Alg.1, it generates mask matrices Mi, Mo which have
the same size with Wi and Wo, respectively, and using elements 1 and 0 in the mask matrices to
denote skeleton and non-skeleton edge, respectively. Second, G-SGD calculates the gradient w.r.t
basis path by using the fact that the value of basis path is multiplication of weights on this path. So it
first calculates the gradient w.r.t weights (Line 3-4 in Alg.2) and then using the relation between paths
and weights to obtain the gradients w.r.t basis paths Gps, Gpons and Gprns where footnotesizeps ∈ Ps,
pons ∈ Pns , prns ∈ Pb2 (Line 6 in Alg.2). This step is called Inverse-Chain-Rule. Third, after
updating the basis paths according to Eq.(6), it calculates the ratio Ri = vpt+i 1/vpti for pi ∈ Ps.
Fourth, it allocates the ratio to weights to obtain new weights (Line 9-10 in Alg.2) in order to
implement forward process in next iteration3. Detailed derivation can be referred to Appendix B.1.
3Operations SUCh as Shape(∙), zero_like, .sum(0) in Alg. 1 and 2 is similar to the function in the numpy
packages of python language. The exact definition are listed in Appendix B.2.
6
Under review as a conference paper at ICLR 2020
Algorithm 1: Mask-Matrix-Constructor
Input: weight matrices of RNN model Wi , Wo
Mi = zerosdike(Wi),
Mo = ZerosJJike(Wo);
n_out = ShaPe(Wi)[0] ; n_in = ShaPe(Wi)[1]；
for j = 1 to n_out do
I Mi[j][j % nin ] = 1;
end
n_out = shape(Wo)[0] ; n_in = shape(Wo)[1];
for j = 1 to n_in do
I Mo[j][j % n_out ] = 1;
end
Output: MaSk Matrix Mi , Mo
Algorithm 2: G-SGD for RNN
1
2
3
4
5
6
7
8
9
10
11
Input: RNN model FRNN(Wi, Wr, Wo), data X, Y, loss function lossfunc, learning rate η
Mi , Mo = Mask-Matrix-Constructor(W i , Wo ) ;
repeat
Sample a batch of data x, y, compute the loss function loss = loss_func(FRNN(x),y);
Using Back-Propagation to compute the gradient w.r.t the weight:
GWi,GWr,GWo =BP(loss,Wi,Wr,Wo) ;
### Using Inverse-Chain-Rule to obtain the gradient w.r.t basis paths:
(W — (W	(Wr ∙G wr ).sum(0) + (W o∙Gwo ).sum(0) .	C — Gwo .
Gps = GWi	(W i ∙M i).sum(0)	;	Gpns = -W钎;
### CakUlate the UPdate RatiO：	R = 1 - η ∙ (Wi∙MGp.sSum(O);
### Weight-Allocation and update weight matrices as:
Wi = Wi ∙ diag(R) + (Wi - η ∙ GWi) ∙ (1 - Mi);
Wo-η∙ GPns	,	W	W	Wr-η∙ GPns
Wo = Wo ∙ Mo +	J,Wi ∙ (1 - Mo);	Wr = J,Wi
diag(R)	diag(R)
until stopping criterion is met;
Output: Weights of RNN model Wi , Wr , Wo
Computational Cost Analysis. (1)The Mask-Matrix-Calculator(Algorithm 1) only run once at the
beginning. So the extra cost can be ignored. (2)As shown in the Algorithm 2, the forward and
backward phase in G-SGD for RNN is the same as traditional SGD algorithm (Line 3- 4). The extra
computational cost is in Inverse-Chain-Rule and Calculate the update ratio(Line 5-7). Please note
that we only need to calculate the ratio R for basis paths in Ps whose number equals to the number of
hidden nodes, which is small compared with the number of weights. Thus the computational cost of
step 5-10 is much less than the backward process. In realistic experiments, the extra time cost is less
than 10% compared to vanilla SGD. We list the running time of each algorithm in Appendix(Tab. 4).
Remark: The skeleton method and the update rule of G-SGD can be generalized to stack RNN
and recurrent convolutional neural networks (RCNN), which can be referred to Appendix. Besides,
according to Definition 2, the selection of basis paths is not unique. Every selection can serve as
a sufficient representation of the model. Skeleton Method only provides one selection of them. In
this work, we employ G-SGD based on this fixed selection. For other selections, the update rule of
G-SGD can also be derived similarly.
5	Experiment
In this section, we verify the effectiveness of the optimization algorithms in path space for ReLU RNN.
We train ReLU RNN on several tasks and show that we can obtain significantly more effective RNN
models in path space than using conventional optimization methods in the weight space. Specifically,
we take vanilla SGD and Path-SGD (Neyshabur et al., 2016) to be our baselines.
5.1	Sequential MNIST
In this section, we optimize the ReLU RNN for the Sequential MNIST (LeCun, 1998; Neyshabur et al.,
2016; Le et al., 2015; Arjovsky et al., 2015; Bai et al., 2019). In Sequential MNIST, each digit image
is reshaped into a sequence, turning the digit classification task into a sequence classification task
7
Under review as a conference paper at ICLR 2020
with long-term dependencies. To make the task even harder, we also use a fixed random permutation
on the pixels of the MNIST digits.
MNIST dataset is download from the official site (LeCun). The sequence length is set to 28 and 98.
The hidden nodes size of ReLU RNN are 100. We search the best learning rate in a reasonable range.
The size of minibatch is 64. More detailed experimental settings can refer to the Appendix A.1.
Every experiment is run 3 times with different random seeds. The averaged test accuracy is shown in
the Table 1. It can be observed that the classification error rates of G-SGD algorithm on the tasks are
significantly lower than that of Path-SGD and SGD. It indicates that optimizing RNN with ReLU
activations in the path space can consistently obtain more effective RNN models for all settings.
Table 1: Averaged testing classification error rate(%) of the sequential MNIST experiment.
		G-SGD	SGD	Path-SGD
Non-PermUtation	SMNIST-28	1.50 ± 0.03	1.59 ± 0.06	1.75± 0.03
	SMNIST-98	2.70 ± 0.09	3.07 ± 0.18	2.99± 0.08
Permutation	SMNIST-28	3.80± 0.09	4.19±。13	4.12 ± 0.24
	SMNIST-98 -	5.42± 0.05 -	5.70± 0.16 ~	5.47± 0.03 一
5.2	Language Modeling
In this section, we optimize the stacked ReLU RNN for the language modeling tasks (Merity et al.,
2018) with four datasets: word-level Penn Treebank(PTB) dataset (Marcus et al., 1993), Wikitext-2
dataset (Merity et al., 2016), character-level Penn Treebank(PTBc) dataset (Merity et al., 2016)
and Hutter Prize(enwik8) dataset (Hutter, 2012). The first two datasets are word level and the last
two datasets are character level. For all tasks, we train a stacked ReLU RNN with 4 layers. The
learning rate is turned in the range {10, 7.5, 5, 2.5, 1, 0.75, 0.5, 0.1}. Performance is evaluated using
the perplexity (PPL) metric for the word level datasets and bits-per-character (BPC) metric for the
character level datasets. The detailed experiment settings including the description of the datasets
and hyper-parameters are put into the Appendix A.2.
The results are shown in Figure 4. The perplexity of G-SGD algorithm on the PTB and Wikitext-2
datasets are lower than that of Path-SGD and SGD. The bits-per-character metric of G-SGD algorithm
on the PTB-c and enwik8 data sets are significantly lower than that of Path-SGD and SGD. It means
that G-SGD are more effective than Path-SGD and SGD, which shows the superiority of optimization
RNN with ReLU activations in the path space.
Wikitext-2
enwik8
Figure 4: Test metric for language tasks. The number means PPL in PTB and Wikitext-2 datasets. The number
means BPC for PTB-c and enwik8 datasets.
5.3	Recurrent Convolutional Neural Network
In this section, we optimize the Recurrent CNN(RCNN) for the image classification tasks(Liang &
Hu, 2015). The model structure is similar to the RNN. The main difference is that the full connection
in RNN is replaced by convolution layer. 4
We evaluate the algorithms on three benchmark image classification datasets, MNIST(LeCun &
Cortes, 2010), CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009). For CIFAR-10 and CI-
4Path-SGD algorithm is not implemented to CNN in its original papers (Neyshabur et al., 2015a; 2016), and
it is not clear how to extend it to CNN. Thus we do not use Path-SGD as a baseline for experiments on RCNN.
8
Under review as a conference paper at ICLR 2020
FAR100, we use (96,128,160) feature maps. Since MNIST is much easier, we use (32, 64, 96)
feature maps. In general, We follow the experiment setup in (Liang & Hu, 2015). Most of the
hyper-parameters is same as previous work such as the network structure, weight decay, scheduling
of learning rate, etc . The detailed experiment setting is put in Appendix A.3.
Table 2 shows the test classification error rate. We can see that optimizing RCNN in path space
outperforms the conventional optimization algorithms SGD that in weight space consistently.
Table 2: Classification error rate(%) of experiments on recurrent convolutional neural networks.
	MNIST			CIFAR-10			CIFAR-100		
	RCNN-32	RCNN-64	RCNN-96	RCNN-96	RCNN-128	RCNN-160	RCNN-96	RCNN-128	RCNN-160
G-SGD (our method)	0.36	0.31	0.31	7.38	6.61	6.52	31.52	29.64	28.78
SGD 一	0.38	0.37	0.39	7.47	7.53	—	7.54	33.34	31.51	29.67 一
6 Conclusion
In this paper, we construct a new parameter space, called path space, for the RNN with ReLU
activations and employ optimization algorithms in path space on benchmark experiments. To achieve
this, we propose reduction graph method to deal with the difficulty brought by the parameter-sharing
scheme, and propose a new Skeleton Method to identify the basis path for RNN in reduction graph
efficiently. We conduct several experiments to verify that we can obtain significantly more effective
RNN models in path space than using conventional optimization methods in the weight space. We
would like to highlight the following conclusions: (1) Basis paths are crucial in the ReLU RNN.
We have proved that the values of basis paths serve as a sufficient representation of ReLU RNN.
Compared with the weights, the values of basis paths are positively rescaling-invariant, which match
the PSI property of the output of ReLU RNN. Moreover, the number of basis paths is even less than
the number of weights of the recurrent neural network. (2) Reduction graph is a powerful tool.
Reduction graph and its paths are introduced to simplify the identification of basis paths in ReLU
RNN. It is a novel technique specially designed for RNN models. (3) Optimizing ReLU RNN in
path space is efficient. The algorithm G-SGD can achieve better performances than SGD with only
a little extra computational cost. In the future, we will investigate the power of path view on other
neural network structures such as LSTM and Transformer.
References
Martin Arjovsky, Amar Shah, and YoshUa Bengio. Unitary evolution recurrent neural networks. In
ICML, 2015.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. In
International Conference on Learning Representations (ICLR), 2019.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for
deep nets. arXiv preprint arXiv:1703.04933, 2017.
Github. Recurrent convolutional neural network for object recognition, 2018. URL https://github.
com/JimLee4530/RCNN.
Github. Recurrent convolutional neural network for object recognition, 2019. URL https://github.
com/TsukamotoShuchi/RCNN.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp.
315-323,2011.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent
neural networks. In 2013 IEEE international conference on acoustics, speech and signal processing,
pp. 6645-6649. IEEE, 2013.
9
Under review as a conference paper at ICLR 2020
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Marcus Hutter. The human knowledge compression contest, 2012. URL http://prize.hutter1.net/.
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the
limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. CIFAR,
2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. A simple way to initialize recurrent networks of
rectified linear units. arXiv preprint arXiv:1504.00941, 2015.
Yann LeCun. The mnist database of handwritten digits. URL http://yann.lecun.com/exdb/mnist/.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun.
com/exdb/mnist/.
Ming Liang and Xiaolin Hu. Recurrent convolutional neural network for object recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3367-3375,
2015.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of english: The penn treebank. Computational linguistics, 19(2):313-330, 1993.
Qi Meng, Shuxin Zheng, Huishuai Zhang, Wei Chen, Zhi-Ming Ma, and Tie-Yan Liu. G-sgd: Opti-
mizing relu neural networks in its positively scale-invariant space. In ICLR’2019 arXiv:1802.03713,
2018.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843, 2016.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and Optimizing LSTM
Language Models. arXiv preprint arXiv:1708.02182, 2017.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. An Analysis of Neural Language
Modeling at Multiple Scales. arXiv preprint arXiv:1803.08240, 2018.
Yajie Miao, Mohammad Gowayyed, and Florian Metze. Eesen: End-to-end speech recognition
using deep rnn models and wfst-based decoding. In 2015 IEEE Workshop on Automatic Speech
Recognition and Understanding (ASRU), pp. 167-174. IEEE, 2015.
TomaS Mikolov, Martin Karafiat, Lukas BUrgeL Jan Cernocky, and Sanjeev Khudanpur. Recurrent
neural network based language model. In Eleventh annual conference of the international speech
communication association, 2010.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807-814,
2010.
Behnam Neyshabur. Path-sgd: Path-normalized optimization in deep neural networks, 2019. URL
https://github.com/bneyshabur/path- sgd.
Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized opti-
mization in deep neural networks. In Advances in Neural Information Processing Systems, pp.
2422-2430, 2015a.
10
Under review as a conference paper at ICLR 2020
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015b.
Behnam Neyshabur, Yuhuai Wu, Ruslan R Salakhutdinov, and Nati Srebro. Path-normalized opti-
mization of recurrent neural networks with relu activations. In Advances in Neural Information
Processing Systems, pp. 3477-3485, 2016.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947-5956,
2017.
Pytorch. Word-level language modeling rnn, 2019. URL https://github.com/pytorch/examples/tree/
master/word」anguage_model.
salesforce. Lstm and qrnn language model toolkit, 2019. URL https://github.com/salesforce/
awd-lstm-lm.
E Weinan, Chao Ma, and Lei Wu. A priori estimates for two-layer neural networks. In arXiv preprint
arXiv:1810.06397, 2018, 2019.
Shuxin Zheng, Qi Meng, Huishuai Zhang, Wei Chen, Nenghai Yu, and Tie-Yan Liu. Capacity control
of relu neural networks by basis-path norm. arXiv preprint arXiv:1809.07122, 2018.
11
Under review as a conference paper at ICLR 2020
APPENDIX
A Experiment Settings
All the experiments are implemented by the Pytorch framework. And all the experiments run in the
Nvidia GPU.
A.1 Sequential MNIST Experiment
The MNIST dataset is downloaded from the official site and then normalized by subtracting the mean
value and then dividing the standard deviation of the training set. The procedure is standard and the
same as many code-base. The permutation dataset is constructed by generating a random permutation
of the index of each pixel(from 1 to 784) before the preprocessing procedure and then fix the random
permutation during the train and test phases.
In the experiments, we use a single-layer RNN with ReLU activation. The number of the hidden
node is set to 100. The size of minibatch is 64. We search the best learning rate in the range of {0.1,
0.05, 0.01, 0.05, 0.001, 0.005, 0.0001, 0.0005, 0.00001}. Each experiment runs for 400 epoch. We
do not use any regularization term.
The code of this experiment is based on pytorch RNN official examples(Pytorch (2019)). The
implementation of Path-SGD is adapted from the code released by the author(Neyshabur (2019)).
A.2 Language Modeling Experiment
The first two datasets are word level. The PTB dataset consists of 929K training words, 73K validation
words, and 82K test words. It has 10k words in its vocabulary. Wikitext-2 is roughly twice the size
of PTB dataset, with 2 million training words, 218k validation words, 245k test words and a vocab
size of 33k. The last two datasets are character level. The PTBc dataset consists of 5017k training
character, 393k validation character, and 442k test character. It has 50 alphabets in its vocabulary.
The enwik8 dataset has 100M characters from Wikipedia with an alphabet size of 27. We split 90M
characters for training set, 5M characters for validation set and 5M test set.
For all tasks, we train a stacked ReLU RNN with 4 layers. The embedding size is 400 and the hidden
size is 1550. The learning rate is turned in the range {10, 7.5, 5, 2.5, 1, 0.75, 0.5, 0.1}. The weight
decay is turned in the range { 1e-5, 1e-6, 1e-7 }. According to the (Merity et al. (2018; 2017)), we
use dropouts for the input, hidden and output layer. Also, we employ weight drops. We run 750
epochs for PTB, Wikitext-2 and PTB-c experiments and run 100 epochs for enwik8 experiment which
is enough for the convergence of all experiments. We identify the basis path for the RNN modules
of the model and optimize the RNN modules in its PSI-space using G-SGD algorithms. Similarly,
Path-SGD is also implemented in the RNN modules. Other parameters are optimized using traditional
SGD algorithms. Hyperparameters are list in the Table 3
Hyperparameter	Value
Dropouts	-04-
Layer numbers	4
Embedding size	400
Hidden size	1550
Batch size	40/128
Gradient clip	0.25
Dropoute*	0.1
DroPoUth*	0.2
Dropouti*	0.4
Wdrop*	0.1
Table 3: Hyperparameters for the language modeling experiments. The batch size is 40 for the
word level datasets and 128 for the character level datasets. The last four hyperparameters with * is
corresponding to the implementation of the Salesforce Language Model Toolkit and the details can
be find in salesforce (2019)
12
Under review as a conference paper at ICLR 2020
Performance is evaluated using the perplexity (PPL) metric for the word level datasets and bits-per-
character (BPC) metric for the character level datasets. The code of this experiment is adapted from
the Salesforce Language Model Toolkit(salesforce (2019)).
The implementation is based on the Salesforce Language Model Toolkitsalesforce (2019) and we
only change the optimizer.
A.3 Recurrent CNN Experiment
The CIFAR-10, CIFAR-100 and MNIST datasets are downloaded from the official site and then
normalized by subtracting the mean value and dividing the standard deviation of the training set.
The code of this experiment is based on this repository (Github, 2019) which is consistent with the
previous paperLiang & Hu (2015).
The RCNN network has 5 layers. There is one RCNN block in each layer and each RCNN block
unfolds 3 times. As many recent code-base suggested, we using BN hear instead of LRN(Github,
2019; 2018) and removing dropout layers except the last linear layer (Github, 2019). We show all the
testing error rate in the table 4.
The initial learning rate of both baseline and our method is searched in then range { 0.5, 0.2, 0.1,
0.07, 0.05, 0.01 } and then divided by 10 after 1/2, 3/4 and 7/8 epoch of all epochs. Momentum
parameter is set to 0.9. Weight decay is set to 0.0001. Dropout rate is set to 0.5. Batch size is 64.
Each experiment is run for 200 epoch and the test accuracy of the last epoch is reported.
Table 4: Classification error rate(%) of experiments on recurrent convolutional neural networks.
	MNIST			CIFAR-10			CIFAR-100		
	RCNN-32	RCNN-64	RCNN-96	RCNN-96	RCNN-128	RCNN-196	RCNN-96	RCNN-128	RCNN-196
G-SGD (our method)	0.36	0.31	0.31	7.38	6.61	6.52	32.02	29.64	28.78
SGD (Liang(2015))	0.42	0.32	0.31	7.37	7.24	7.09	34.18	32.59	31.75
SGD (our implementation)	0.38	0.37	0.39	7.47	7.53	7.54	33.34	31.51	29.67
B Algorithms
B.1 Derivation of the Algorithm
In this section, we review the process of Inverse-Chain-Rule and Weight-Allocation designed in the
work Meng et al. (2018) and apply it to RNN.
Denote the non-skeleton weight as wns and the skeleton weight as ws . We select the skeleton weight
in the first layer as special skeleton weights and denote as ws0 and the non-skeleton weight in this
layer is denoted as wns0. Denote all the basis path set as Pb. Denote the basis path that pass the
non-skeleton weight wns as pns and the basis path that compose by all-skeleton weight and pass the
skeleton weight ws0 as ps0. Note that there is a 1-1 mapping between non-skeleton weights and the
basis paths in set Pns. Similarly, there is also a 1-1 mapping between skeleton weights ws0 and the
basis paths in set Ps0. Here, for simplicity, we can view recurrent weight as a non-skeleton weight
wns . Thus Pb2 ∈ Pns .
Inverse-chain-rule calculates the gradient with respect to basis paths according to the following
equation:
∂L )
dvPm-H )
∂vp1	∂vp1
•^-- ∙ ∙ ∙ -π----
∂w1	∂wm
..	..	..
.	..
dvPm-H	dvPm-H
--	 • • •	--
∂w1----------------∂wm
(7)
By solving this equation, the gradients w.r.t basis paths can be derived as:
For vpns we have:
dL = dL ∕dvPns
∂Vpns	∂Wns / ∂Wns
13
Under review as a conference paper at ICLR 2020
For vps we have:
∂L _ ∂L	∑p∈Pt∕ps0 ∂LWns
=----=T-------------------------
∂vps0	∂ws0	ws0
We write the result into matrix manipulation forms and we can derive line 6 in Algorithm 2 in the
main paper. The detailed derivation is put at the end of this section for clarity.
Weight-Allocation projects the update on basis paths back to the update of weight. Notation Cs0 is
the production of weights in path ps0 except for ws0 . Specifically, vps = ws0
the production of weights in path Pns except for Wn§. Specifically, VpnS =wns∙
Vt — η adL	“
ps0 ∂	vt	V
_________ps0	J
• Cso. Notation Dns is
vpts0
Vt — η dL
pns ∂ vt
__________PnS
Vp ns
v
v
I
. We have:
v
•Dns
t+1 S0 厂 S0	,	R(Vpt S0) =	wt+1 ∙ Cs+1 -ws0 ∙ Cp0	(8)
t+1 nS P nS	,	R(Vpt nS) =	_ wn+1 ∙ Dn+1 Wns ∙ Dns	(9)
Solving this equation, the update rule for weight can be derived as:
t+1
wns
t+1
wns0
ws+1 = ws0 ∙ R(ViPS0 )
Wt - ηt ∙ ∂VtL~/vpS0
___________PnS_____
R(Vps0 )
t	∂L
=wns0 - ηt ∙ ∂W^
Wns0
(10)
(11)
(12)
We write the result into matrix manipulation forms and we can derive line 9-10 in Algorithm 2 in the
main paper. The detailed derivation is put at the end of this section for clarity.
We give the derivation details in the following:
According to the chain rule, we have:
∂L ∂Wns ∂L ∂ws0	=X ∂L p∈Pb Vp		dV _ dL	dVPnS •	-	 - T	 •-	 ∂Wns	∂VPnS ∂Wns ∂VP	∂L ∂VPS	∂L	∂VP •	—	 = —	 • —	 +	〉	-	 •-	 ∂Ws0	∂VPS ∂Ws0	∂VP ∂Ws0 p∈Pb∕PS	(13) (14)
	=X p∈Pb	∂L ∂Vp		
According to the skeleton method, each non-skeleton weight wns can only exists in one basis path pns . Besides, if Wns not in p, ∂dVp- = 0 Thus Eq. (13) is right Thus, We can derive the gradient of the basis path as:				
	∂L dVPnS	=dL 产 PnS ∂Wns ∂Wns		(15)
	∂L dVPS0	=(	∂L	X ∂L	∂vp ʌ /dVPS0 dws0	P∈p∕Psθ dVP	dws0 ] dws0	(16)
Let s[ws0] = Pp∈P∕pS0	∂L	∂vP 	 • ^π	. ∂vP	∂wS0		We can then calculate the update ratio for the basis paths	as:
R(VptS0),	Vt+1 pS0 —:	= VptS0	1-	∂L	p dvpS0	( dL	∣-	n〃dVPS0	t 、 ηt~PPS- =1 - ηt(∂Wt- - s[ws0])/(E • VPS)	(17)
R(VptnS) ,	Vt+1 VpnS —:	= Vt pnS	1-	∂L	+ ∂vp	∂L ∂ ∂Vp	t 、 ηt vɪ = 1 - ηt ∂WΓ/( dʃ • VPnS) VPnS	Wns	Wns	(18)
(19)
14
Under review as a conference paper at ICLR 2020
Above equation is right according to the gradient decent update rule of the path value vpt+1
Vt - γ1. ∙ dL
Vp	ηt ∂vp
Finally, we project the new basis path value into the weight. Notation Cs0 is the production of weights
in path ps0 except for ws0.
in path pns except for wns
Specifically, Vps = . Specifically, Vpns		:Wso ∙ Cso ∙ Notation Dns is the production of weights =Wns ∙ Dns .		
wt+1 ws0 wto	t+1 Cs0 •	:	- CSo	vt+1 _	pso -Vp so		(20)
	wst+1 =	R(vptso) • wsto •	Cto ∕-*t+1 Cso	(21)
wnt+s1 	:		 wt wns	Dn 1 		= Dt ns	vt+1 vpns vptns		(22)
	t+1 wns =	R(vptns) • wnts	Dns •	- Dt+1	(23)
Eq. (20) and ( 22 ) is the definition of the basis path. Note that we only update the skeleton
Ct
weight in the special layer Ws° and the non-skeleton weight Wns. Therefore, —tS+01 = 1. For the
Cs0
Dt
non-skeleton weight in the special layer, —tnsι = 1. For the non-skeleton weight not in the special
Dns
layer, D+ = 1/R(vpso).
B.2	Notations Explanation in the Algorithm
Table 5: Notations
notation	object
zeros」ike(w)	Return a all-zero matrix whose size is the same as W
% 一	Modulus operator
shape(w)	Return the size of matrix W
w.sum(0)	Return sum of the matrix W along axis 0. If w is a weight matrix of NN , this operation sums the weight that connect to the same input and returns the vector with the same size as input node size.
B.3	Computational Cost
Here, we provide the training time per epoch of our algorithm and the baselines in Table 6. We
implement all experiments on a single GPU. It shows that the real training time of G-SGD is also
comparable with that of plain SGD.
Table 6: Training Time of 1 epoch (seconds) in different experiments
	PTB	Wikitext-2	PTB-c	enwik8	s-MNIST-28	s-MNIST-98	C10-RCNN96	C10-RCNN128	C10-RCNN160
Path-SGD	53.9	90.2	79.8	1407	10.2	13.5	-	-	-
SGD	31.4	66.0	61.5	1054	8.5	10.9	56.0	74.0	125.5
G-SGD	33.9	-688	63.8	1103	92	115	60.9	798	1349
Extra time cost	2.5	28	-23-	-49-	07	0.6	4.9	58	9.4
time cost increase rate	8.0%	4.2%	3.7%	4.6%	8.2% 一	5.5% 一	8.7% 一	7.8%	7.5%
B.4	Stack RNN and Convolutional Layer
For stack RNN, there are multiple layers of hidden nodes. Specifically, there are more than one W h
and Wr. Note that the skeleton method is also held. Thus the Mask-Matrix-Constructor algorithms
can also be applied by generating mask matrix for all the weight matrix Wh according to the skeleton
method.
15
Under review as a conference paper at ICLR 2020
For convolutional layer, we view one feature map as a hidden node as in MLP structure. So we
select one of the edges in the convolutional kernel that connect two feature maps in two layers as the
skeleton weights. Thus the Mask-Matrix-Constructor algorithms can construct the mask matrix by
selecting one element in one convolutional weight matrix that connects one input feature maps and
one output feature maps as 1.
C Proof of Theorems
C.1 Proof of Theorem 1
To ease the presentation of the proof, we give the path a vector representation as follows. We index the
edges in the reduction graph of neural network as eι,…,e%, ∙∙∙ ,em,. We define the counting vector
for a path P either in the directed graph or in the reduction graph as Cp = (Cpe ,…，Cpem), where
cpe is equal to the times that path p passes the edge ei. Please note that, if path p is in the directed
graph, Cpe may be larger than 1 for some i because of weight sharing. Theorem 1 is proved if we
show the counting vector of any path in the directed graph is a linear combination of the counting
vectors of the paths in the reduction graph.
Consider an arbitrary path in the directed graph with counting vector Cp = (cpι, ∙∙∙ , cpm). For
each edge ei , we choose a path in reduction graph that contains this edge, which is denoted as
p(i) and its counting vector is Cp(i). We claim that the term Pea∈p cpi ∙ Cp(i)- Cp equals to the
linear combination of counting vectors of paths in reduction graph. Without loss of generality,
We suppose that P contains nodes and edges NI →→ NH e2 ∙∙∙ →→ N+ι, where eι is the edge
connecting an input node and a hidden node and ek is the edge connecting a hidden node and an
output node, and other edges are recurrent edges. For edge ej,j = 3,…，k - 1, We choose a path
NiI e→ij NjH-1 →ej NjH e→zj NzO that contains ej , where ij denotes the index of selected edge in the
input-to-hidden layer for ej and zj denote the index of selected edge in the hidden-to-output layer for
ej. For edge ej-1, we choose a path NiI ei→j-1 NjH-2 e→j-1 NjH-1 ez→j-1 NzO that contains ej-1.
Furthermore, for the edge e1 , we select an edge connecting with e1 at the hidden-to-output layer,
denoted as ez1. And for the edge ek, we select an edge connecting with ekat the input-to-hidden
layer, denoted as eik. We claim that the edge eij and edge ezj-1 compose a path in reduction graph
because they both connect node NHL、. This claim is right for all j = 2,…，k. Thus the linear
combination of the paths that contain e% and ezj-1 equals to Pea∈p cpi ∙ Cp(i)- Cp.
Figure 5: An example of reduction graph of RNN
Here, we use the example in Figure 5 to illustrate the above formulation. There are five weights
in Figure 5, denoted as (w1, w2, w3, w4, w5). Consider the path N1I,(1) → N3H,(1) → N3H,(2) →
N3H,(3) → N3H,(4) → N4O,(4) whose counting vector is Cp = (1, 0, 3, 1, 0). It can be calculated as
Cp = 1 ∙ (1, 0,0,1, 0) + 3 ∙ (1,0,1,1, 0) + 1 ∙ (1, 0, 0,1,0) - 4 ∙ (1,0,0,1, 0), where (1,0, 0,1,0)
and (1, 0, 1, 1, 0) are paths in the reduction graph.
C.2 Proof of Theorem 2
Recall the definition of the activation status ap(w; x) = QNH,t∈p I(NiH,t(w; x) > 0), We will first
prove that every indicator function of the node can be calculated by the values of paths. Then using
the definition of the basis paths we can know that the values of paths can be calculated using the
values of basis paths. Combine the conclusion above, we can prove the theorem.
16
Under review as a conference paper at ICLR 2020
Denote the weights connect the node with index i and the node with index j as Wiij , consider the
hidden node NjH,(t), we define the value of the node as the sum of all incoming value before the
activation and denote it using lower case letter ojH,(t). Formally, the value of the hidden node is
ojH,(t)=XWjiixi(t)+XWjrj0ojH0,(t-1)I(ojH0,(t-1) >0)
i	j0
where xi(t) is the input node of time-step t and ojH0,(t-1) is the pre-activated hidden output of time-step
t - 1. The corresponding weights Wjii and Wjrj 0 is in the Wi and Wr respectively the same
as the notations in preliminaries. For simplicity, we denote the activation status of nodes as
a(o∏,(t))，I(o∏,(t) > 0) where I is the indicator function. We use notation sign(∙) to denote
the sign function(sign(a) = 1 if a > 0 and sign(a) = -1 if a < 0)
We prove the theorem by using the induction method. Specifically, we will prove that the value of
hidden node can be expressed by the following formulation. And thus the activation of the hidden
node can be calculated by the value of path and the sign of skeleton weights in matrices Wo, which
we call it outgoing skeleton weight and denote it as ws .
oH,(t) = F(x,vp,sign(ws)) ∙ τr1^,	(24)
Wkj
where F(x, vp, sign(ws)) is a function that only related on the input x, values of paths vp and the
signs of outgoing skeleton weights ws. Wkoj is also an outgoing skeleton weight that connect to the
hidden node ojH and okO .
Firstly, we prove Equation 24 is satisfied in time-step 1. For an arbitrary hidden node with index j
OHa= X WjixiI) = W^ WkjX Wjix(I)
i	Wkj	i
1	(1)
=幅 JLxi ∙ vNI,α)→NH,α)→NO,(1)
=F(x,vp) ∙ Wo-.
kj
=F(X,vp ,sign(Ws)) ∙ W^
kj
Then, using induction method, the induction assumpution is that ot-1 can be represented by the
function of path value divided by the weights on the skeleton edges,
oH,(t-1) = F(x,Tp,sign(Wsy) ∙ wo-
Wk0j0
Then, we can get:
OH⑴=盛 Wkj ∙ (X Wiix(t) + X WjjOOH，"—。a(oH,(i))
kj	i	j0
=W1o^ (X xit) ∙ vNI,(t) →NH,(t)→NO,(t) + X WkjWjj0 oH,(tT)a(oH,(tT)))
=盛(X xit) ^ vPNI,(t)→NH,(t)→NO,(t) + X Wj WjjO F (x,vp,sign(Ws))a(oH,(j))
kj i	i → j → k	j0 k0j 0
=W1o- ( F(x,Vp,sign(ws)) + X F(x,Vp,sign(ws)')I (Sign(Wkj) ∙ F(x,Vp,sign(ws)))
Wkj	jO
=W^ F (x,vp,sign(Ws))
kj
17
Under review as a conference paper at ICLR 2020
The second term in third equality is from the induction assumption. The fourth equality is the
definition of activation function. It is obvious that for arbitrary functions F (x, vp, sign(ws)),
Wo
WOkj WrjoF(x,vp, sign(ws)) is also a function of input, value of paths and the sign of weight
Wk0j0
on skeleton edges. The reason is that the paths in function F both contain the weight WkO0j0. And an
arbitrary value of path in RNN multiplies the weight on the recurrent edge is also the value of a path.
So we prove that if the equation 24 is satisfied in time-step t - 1, it will also be satisfied in time-step
t. And the equation is proved through the induction method.
Next, recall the definition of the activation status of paths: ap(w; x) = QN H,t ∈p I(oiH,t(w; x) > 0).
By using Equation (24), we can know that the sign of ojH,(t) can be calculated using the value of
basis paths given the sign of weights on skeleton edges. Combine it with that the value of path can be
calculated by the value of basis path, we can easily prove the theorem.
C.3 Proof of Theorem 3
As we mentioned, the structure of ReLU RNN can be decomposed into a feedforward part and the
recurrent edges part. For the feedforward part, the number of basis paths has been proved to be
#(weights)-#(hidden nodes) in Meng et al. (2018). According to our Skeleton Method for RNN,
each recurrent weight can only be contained in one basis paths. Thus, the total number of recurrent
neural networks is #(weights of MLP)+#(recurrent weights)-#(hidden nodes). Thus, we finish
the proof.
D Testing Accuracy Curves for Vision Tasks
D.1 Sequential MNIST
(a)	(b)
(d)
(c)
Figure 6: Test Error Rate for Sequential MNIST experiments. x axis is the epoch index, y axis is the Error
Rate(%)
D.2 Recurrent CNN
E ADDITIONAL G-ADAM V.S. ADAM EXPERIMENT RESULTS ON SEQUENTIAL
MNIST experiments
18
Under review as a conference paper at ICLR 2020
(b)
(d)
(h)
(g)
(i)
Figure 7: Test Error Rate for Recurrent CNN experiments on 3 datasets. x axis is the epoch index, y axis is the
Error Rate(%)
Table 7: Averaged testing classification error rate(%) of the sequential MNIST experiment.
		G-ADAM	ADAM
Non-Permutation	SMNIST-28	1.50 ± 0.07	1.72 ± 0.07
	SMNIST-98	2.49 ± 0.08	3.10 ± 0.10
Permutation	SMNIST-28	3.73± 0.05	4.06± 0.1
	SMNIST-98一	5.42± 0.1 一	5.57± 0.09 一
19