Under review as a conference paper at ICLR 2020
The Effect of Residual Architecture on the
Per-Layer Gradient of Deep Networks
Anonymous authors
Paper under double-blind review
Ab stract
A critical part of the training process of neural networks takes place in the very first
gradient steps post-initialization. In this work, we study the connection between the
network’s architecture and initialization parameters, to the statistical properties of
the gradient in random fully connected ReLU networks, through the study of the the
per layer Jacobian in finite sized networks. We compare three types of architectures:
vanilla networks, ResNets and DenseNets. In our analysis, we show that while
the variance of Jacobian squared norm is exponential in depth for ResNets, and
polynomial for DenseNets, there exists an initialization strategy for both, such that
the norm is preserved through arbitrary depths, preventing exploding or decaying
gradients in deep networks.We also show that the statistics of the per layer Jacobian
norm is a function of the architecture and the layer’s size, but surprisingly, not the
layer’s depth.
1 Introduction
Understanding the effect of different architectures on the ability to train deep networks has long been
a major topic of research. A recent line of work has focused on the statistical properties of both the
activations and the gradients of different architectures at the point of initialization, in order to better
understand the effect of different architectural choices on the training dynamics.
A well-known hurdle in the ability to train deep networks, is the effect of depth on the magnitude of
the gradients. Specifically, the exploding and vanishing gradient phenomena have been observed in
various deep architectures. they are considered a main culprit in the failure of training deep networks
successfully. A flurry of architectural tricks and initialization techniques have been proposed, in order
to mitigate the problem of exploding and vanishing gradients, ranging from residual architectures,
different normalization techniques and orthogonal initializations.
In this work, we perform a rigorous analysis of the norm, of both the activations and the gradients of
each layer, in fully connected architectures at the point of initialization. Our setting is as follows:
we assume a fixed input y0 to an L layer neural network with weights given by W, and intermediate
output for any layer l denoted by yl. Denoting by wikj the weights ij of layer k, we are interested in
the quantities:
kylk2, kJkk2 = Xk%k2	⑴
representing the norm of the activations of layer l and Jacobian with respect to layer k. We consider
these quantities random variables at initialization, and, therefore, devote our analysis to deriving
expressions for the mean and variance of both kyl k2 and kJkk2, over the random initializations of
the weights. Note that the full gradient requires an additional loss term which is independent of the
architecture, and so our attention is focused on Jk. While most initialization techniques are designed
to maintain the squared length of intermediate activations through the layers, the multiplication
of i.i.d random weights along input-to-output paths in increasingly deep networks might lead to
unbounded higher order statistics. In the extreme case, while the mean of activation and gradients
remain constant in deep layers, it would be improbable to achieve mean values for any specific
realization of the weights, rendering training difficult. Specifically, given some loss function L, the
1
Under review as a conference paper at ICLR 2020
(a)
Figure 1: An illustration of Thm. 1. The activations of the network in (a) are completely different
from those of the network in (b), in which all skip connections bypassing layer k = 2 are removed.
However, the moments of the gradient norms at layer k = 2 are exactly the same in both (a) and (b).
(b)
norm of the gradient with respect to Wk is given by:
Ww k Lk = k ∂∂yL Jk k≤k ∂∂L kkJk k	(2)
Large fluctuations to the Jacobian norm with respect to small perturbations of the weights will,
therefore, cause wildly fluctuating gradients during training, hindering training dynamics.
Our analysis makes use of a duality theorem, which captures an intuitive concept. In residual
architectures, there are many alternative paths of varying lengths from input to output. The theorem
states that when considering the moments of the gradient at a certain layer, one can change the
architecture, so all connections which that skip that layer are removed. This is true, despite the
activations of the two networks being very different. See Fig. 1.
Our contributions are as follows: (i) We rigorously study the effect of different architectures, namely
vanilla, residual and densely connected architectures, on the mean and variance of kyl k2 and kJk k2
in finite depth and width networks. (ii) From our forward-backward norm propagation duality
theorem, we derive conditions that prevent exploding gradients in each of these architectures. (iii)
We prove that for densely connected networks, the commonly used initialization of He et al. (2015)
is enough to ensure the boundness of var(kJk k2) and var(kyl k2) for arbitrarily deep networks,
shedding new light on the apparent advantage of these architectures in practical applications. (iv)
We prove that replacing the standard conv-ReLU blocks with blocks of the concatenated ReLU
(CR) activation presented by Shang et al. (2016), while maintaining the same parameter budget,
significantly reduces the fluctuations of both the activations and gradients at initialization for any
ReLU based architecture. We evaluate the applicability of this simple alteration on a large number of
UCI datasets, and demonstrate the advantage for CR based networks over their standard counterparts.
2 Related Work
The effect of architecture and depth on the gradients has been a topic of considerable interest recently,
leading to several insightful research directions. A major line of work has been focused on the mean
field approach, where the propagation of signals in random networks are analyzed in the regime of
infinite width (Yang & Schoenholz (2017),Schoenholz et al. (2017),Xiao et al. (2018)). In that case,
the pre-activations in any layer are approximated by a Gaussian distribution with moments depending
on the specific layer. For ReLU networks (without residual connections), controlling the exponential
explosion or decay of both the activations and gradients with depth is a simple matter of correctly
scaling the initialization parameters. Indeed, it is shown in Schoenholz et al. (2017) that classical
feed forward networks exhibit exponential behavior on average, when not initialized properly. For
the gradients, a recursion relation is derived:
E ((∂y+ )2 卜W / √⅛e-z2 φ (√qZ ZdZ
(3)
where φ0 the derivative of the activation function and σw2 denotes the variance of the weights. By
initializing With σW = fa：-in, and considering that for the ReLU activation function φ0(z) = lz>0,
we have that
JyLL )2
∂yi+1
(4)
2
Under review as a conference paper at ICLR 2020
Mean field analysis of residual networks is presented in Yang & Schoenholz (2017), where it is shown
that ResNets exhibit sub-exponential, or even polynomial dynamics, depending on the activation
function. While mean field theory captures the “average” dynamics of random neural networks, it
does not take into account the scale of gradient fluctuations that are crucial to the stability of gradient
descent, which is the focus of this work. A more refined analysis presented in Pennington et al.
(2017), considers the full spectrum of the input output Jacobian of infinitely wide networks, given by:
J — dyL	⑸
JIO =而	⑸
Exploding or vanishing gradients are then prevented by analyzing conditions that give rise to dy-
namical isometry, a state in which the squared eigenvalues of JIO are all concentrated around 1. In
(Pennington et al., 2017) it is shown that fully connected ReLU networks are incapable of reaching
dynamical isometry, as apposed to sigmoidal networks, while in (Pennington et al., 2018) it is shown
that dynamical isometry is achieved in a universal manner for a variety of activation functions.
Dynamical isometry is a much stronger condition than the Jacobian norm, since it requires access to
the entire spectrum. However, in previous work, this is done using various forms of infinite width
approximations. Instead, this work focuses on finite depth corrections to the statistics of various
quantities concerning the squared Frobenius norm of the per-layer Jacobian (derivative of the output
with respect to the weights), and can be viewed as a complementary approach to analyzing the
dynamics at the start of training. Our results cannot be obtained using large width approximations,
since we seek to characterise the effect of both depth and width on the Jacobian.
A recent line of work, more related to the analysis presented in this paper, deals with the statistical
properties of the norm of activations and gradients in random finite width networks. Hanin & Rolnick
(2018) tackle two failure modes that are caused by exponential explosion or decay of the norm of
intermediate layers. It is shown that for random fully connected vanilla ReLU networks, the variance
of the squared norm of the activations exponentially increases, even when initializing with the 于@：_.
initialization. For ResNets, this failure mode can be overcome by correctly rescaling the residual
branches. However, it is not clear how such a rescaling affects the back propagation of gradients. In
our work, we prove an equivalence relation between the forward- and back-propagation of norms
in most random ReLU networks. In addition, we present an analysis of DenseNets, revealing a
surprising norm preservation quality that is independent of depth.
Hanin (2018) explores the conditions at initialization that give rise to the exploding or vanishing
gradient problem, by analyzing the individual entries of the input-output Jacobian. In the case of a
single output network, the layer k to output Jacobian is given by the row vector:
J(k→L)
i = 1...nk)
∂yL
∂yl
(6)
with nk denoting the width of layer k. In our work, we take a more direct approach to analyzing the
full expression for the Jacobian of random networks given by:
∂yL ∂yk
dyk dwk
hJ(k → L),
(7)
Moreover, the analysis presented by Hanin (2018) does not directly carry over to ResNets or
DenseNets.
3 Preliminaries And Notations
We make use of the following notations: N denotes a neural network architecture with intermediate
outputs yl of layer l for a fixed input y0. yil denotes the i’th component of the vector yl, and n1...nL
denote the width of the corresponding layers, with n the length of the input vector. ∣∣ ∙ ∣∣2 is the
squared Euclidean norm, and we assume ky0k2 = 1. We denote the weight matrix associated with
layer l by Wl ∈ Rnl-1 ×nl, with lower case letters wilj denoting the individual components of Wl.
Additional superscripts W lk are used, when several weight matrices are associated with layer l,
for instance in residual, CR and DenseNet architectures. Throughout the paper, we assume that
the weights are sampled i.i.d from some symmetric distribution with moments E((wilj)m) = clm,
3
Under review as a conference paper at ICLR 2020
such that ∀iji, ∀ι, cl = 0, c2 = 2α~. We further identify the following property of a distribution
nl-1
∆ = 2(cl4 - 3(cl2)2)/nl-1 (for Gaussian distributions, ∆ = 0). The derivative of the output unit
yL with respect to Wij is given by Jj = ∂WL =需L∂⅜, With IIJlk2 = Pij(Jij)2. We Will
be concerned in our analysis with the quantities E(kylk2), var(kylk2), E(kJ l k2) and var(kJ l k2),
Where the expectations are taken across the random sampling of the Weights, given the fixed input y0 .
Multi Pathway Architectures We compare a vanilla fully-connected netWork to ResNets He et al.
(2016) and DenseNets Huang et al. (2016) to a netWork that is built With CR Shang et al. (2016)
layers. For the standard vanilla fully-connected netWork, the output for of layer l is given by
yl = φ(Wl>yl-1),	(8)
Where φ() denotes the ReLU non-linearity.
For residual architectures, We study models With m layer residual branches of the form
yl = yl-1 + Rm(yl-1)
Where:
Rm(yl-1) = W lm>...φ(W l2>φ(W l1>yl-1).
We denote the k’th intermediate output of the l’th residual branch by
yl-1,k = Wlk> …φ(Wl2>φ(Wl1>ylT),
so that, for example
Rm(yl-1) = W lm>ylτ,mτ = W lm> φ(W ImTTylT,m-2).
(9)
(10)
(11)
(12)
Furthermore, since the last layer of any residual branch Wlm does not end With a non-linearity, We
assume c2m = *. DenseNets were recently introduced, demonstrating faster training, as well as
improved performance on several popular datasets. The main architectual features introduced by
DenseNets include the connection of each layer output to all subsequent layers, using concatenation
operations instead of summation, such that the weights of layer l multiply the concatenation of the
outputs y0...yl-1, and is of dimensions Wl ∈ Rln×n . For our analysis, we break Wl into l - 1 weight
matrices W l1...W ll-1 ∈ Rn×n , so that the output of layer l is given by yl = φ(Pll0-=11 Wll0>y4 * * * * * l0).
4 Forward-Backward Norm Propagation Duality
In this section, we introduce a link between the propagation of the norm of the activations, and the
norm of the Jacobian in different layers in random ReLU networks of finite width. This link will
then allow us to study the statistical properties of the gradient in general architectures incorporating
residual connections and concatenations with relative ease. Specifically, we would like to establish
a connection between the first and second moments of the squared norm of the output IyL I2, and
those of the Jacobian norm IJkI2. The family of architectures we consider is restricted, so that the
ReLU activations are only applied after multiplication with weights. Using a path-based notation,
any unit t of the output ytL can be decomposed to paths that go through weight matrix Wk, denoted
by ytL,k (neglecting an additional superscript for ResNets and DenseNets), and paths that skip Wk,
denoted by yL,k. Assuming the input ∀j, y0 = 1, we have:
∣γt∣	∣γt∣
yL =杷 + yL，k = X Y Wγ t,∣zγt + X Y Wγ t,∣ zγt	(⑶
k∈γt ∣=1	k∈γt l = 1
where the summation is over paths from input to unit t of the output, indexed by γt, with | γt | denoting
the length of the path indexed by γt. In non-residual networks, we have ∣γt | = L and PY古 sums over
QL201 n∣ paths. The term QlYtI WYtlZYt denotes the product of weights along path γt, multiplied by a
binary variable ZYt ∈ [0,1], indicating whether path Yt is active. Finally, Pk/丁 indicates summation
over all possible paths that do not include a weight from layer k. In all relevant architectures, the
squared norm of the Jacobian entry IJikj I2 is given by:
IJjk2 = k 号
ij
nL	|Yt |	2
k2 = X(X wk Y WY t,lZYt)
t=1	ijk∈Yt	ij l=1
(14)
4
Under review as a conference paper at ICLR 2020
That is, the derivative of output unit t with respect to weight wikj is given by the sum of all paths γ
that contain wikj divided by wikj . In order to link Jk with yL, we make the following definition:
Definition 1. Reduced network: the outputs of a reduced network N(k) denoted by y(1k) ...y(Lk) given
input y0 are obtained by removing all connections bypassing weight Wk from the network N.
Note that for vanilla networks, it holds that N(k) = N, and ∀0<l≤L , y(lk) = yl . The following
Lemma links between the moments of y(Lk) and those of yL :
Lemma 1. For any architecture described in Sec. 3, it holds that for m ∈ {2, 4}:
Eky(Lk)km =EkyL,kkm	(15)
Note that if network N contains connections bypassing layer k, then for a specific realization of the
weights, the equality y(Lk) = yL,k does not hold in general, due to different activation patterns in
N and N(k) induced by additional residual connections. In other words, paths that are open in N
(indicated by the variables zγt) are not necessarily so in N(k), and vice versa. Lem. 1, however, states
that the moments of both are equal in the family of considered ReLU networks.
The following theorem relates the moments of kJk k2 with those of ky(Lk) k:
Theorem 1.	For any architecture described in Sec. 3,the following hold:
EfkJkk)="E(kyLk)k1,	var(k⅛k2 ≤ var(k)k" ≤ Varok22	(⑹
c2	c4	(c2 )
where c2k and c4k are the second and fourth moments of the weights in layer k. Thm. 1 indicates that
we can compute the moments of the Jacobian norm of layer k, by computing the moments of the
output of the reduced network y(Lk). Thm. 1 also reveals a surprising property of the gradients in
general Relu networks. That is, when the weights are sampled from the same distribution in each
layer, the gradient’s magnitude for each layer are equal in expectation, and depend only on the output
statistics.
4.1	Vanilla networks
In order to compare our results for ResNets and DenseNets, we first derive the moments of y(lk) in
our setting for vanilla architectures. Note that for vanilla networks, the reduced network matches the
original, and so ∀0<l≤L, y(lk) = yl.
Lemma 2. For a vanilla ReLU neural network, as described in Sec, 3, it holds for α = 1:
CI exP [X *]-C ≤ var(kJkk2) ≤ C exP [X 5+H -C4	(17)
where C1, C2, C3, C4 are constants that do not depend on L.
By setting α = 1, the variance of the Jacobian squared norm kJk k2 will increase exponentially
with PL=ι n5, as noted by Hanin & Rolnick (2018). This can be mitigated by linearly increasing
the width of each layer with depth, so that nl = nl, resulting in constant asymptotic behavior
exp PL-II nil 〜 L1. This, however, would require significant resources, since the weight
matrices would grow quadratically with depth. For comparison, in DenseNets the variance is
maintained with depth, while the number of weights increases linearly.
4.2	Residual Networks
Residual networks have reintroduced the concept of bypass connections, allowing the training of deep
and narrow models with relative ease. Hanin & Rolnick (2018) showed shown that the fluctuations of
the norm of subsequent layers do not exponentially increase, provided that the weights are initialized,
5
Under review as a conference paper at ICLR 2020
such that the expected norm E(kyl k2) is bounded for any layer l. Such an initialization is not constant,
and is dependant on the depth of the network L. It is also worth mentioning that the same depth
dependent initialization for ResNets was found by Zhang et al. (2019) to be crucial for training
convolutional ResNets without applying batchnorm, as introduced in the original formulation of
ResNets. Recall that in our analysis, we consider residual branches with m layers with weights
Wkk0, 0 < k ≤ L, 0 <k0 ≤ m,, and so We will analyze the variance of ∣∣ Jkk0 k2 = Pi (。味)2
∂wij
for k0 < m. Unlike vanilla networks, the outputs of the reduced network y(lk) for l > k are not equal
to those of the full network yl , and are given by:
l	Rm(yl-1) + yl-1 l 6= k
y(k) = {	Rm(yl-1)	l=k
The following theorem states our results for ResNets.
Theorem 2.	For a ResNet as in Sec, 3 with m ≥ 2, it holds that for 0 < k ≤ L, 0 < k0 < m:
(18)
C1 ρm exp 1 LPm — C2 μ2 ≤ var(∣J kk0『)≤ C3ρm exp 2Lρf — C4 μ2	(19)
where ρ± = a2(1 + 5±δ1 ), C1,C2,C3, C4 are constants that do not depend on L, m, and μ
αm1+αmL-1.
The gradients of weights in residual branches are, therefore, sensitive to the depth of each residual
branch (large m). However, unlike their vanilla counterparts, by scaling the residual branches, such
that Lρ+m is kept constant, ensures that both E(∣J ∣2 ) and var(∣J ∣2) remain bounded as the overall
depth of the network grows (increasing L). Note that a similar initialization was proposed in Zhang
et al. (2019) for training ResNets without batchnorm, albeit for a more restricted class of networks,
and without an explicit dependency on width. Our result indicates that for narrow and deep networks
with residual connections, training without batchnorm requires scaling in a both depth and width
dependent manner.
4.3 Densely Connected Networks
DenseNets have shown improvement over ResNets in terms of accuracy and speed of training on
several notable image classification tasks, since their inception. As far as we can ascertain, there have
been no theoretical studies on the causes of DenseNets success, or the implications on the training
process of its architectural novelties, namely dense connections and feature reuse by concatenation
of previous layers. In the following, it is shown that the DenseNet architecture, coupled with the
standard, depth independent n2γ initialization, gives rise to well-behaving gradients in any depth.
By the definition of DenseNets given in Sec. 3, the reduced architecture outputs y(lk) are given by:
yl(k) = {
ll0=11 W ll0>yl0)	l ≤k
ll0-=1kWll0>yl0)	l>k
(20)
The following theorem states our results for DenseNets:
Theorem 3.	For a DenseNet as described in Sec, 3, it holds that for 0 < k ≤ L:
LgT)(CI exp -2∣∆lL - C2) ≤ var(∣J k∣2) ≤ L2(aT)(C3 exp 2l∆lL - C4)	(21)
where C1, C2, C3, C4 are constants that do not depend on L.
It is worth noting the difference between DenseNets, and both vanilla and ResNet architectures. In
vanilla architectures, the mean of the activation norm, as well as the gradient norm are exponential
in a. However, the variance of the same quantities is exponential in PL=I n1∙. That is, fixing the
mean by correctly setting α = 1 does not prevent the gradients and activations from exploding in
deep networks. In ResNets, scaling of the residual branches in order to avoid exploding gradients
6
Under review as a conference paper at ICLR 2020
(a)
Figure 2: The variance of the squared norm in log scale of the per layer Jacobian as a function of α
(held constant for all the layers) obtained from the simulated results of 200 independent runs, for (a)
ResNet with m = 2 and n = 40, and (b) DenseNet with n = 40. The x-axis represents the depth
of the network, and y-axis the variance as computed from 200 runs. The dashed lines represent
our theoretical predictions for both architectures, and full lines represent the empirical results. All
networks were initialized using Gaussian distributions.
(b)
requires scaling in a depth dependent fashion. However, in DenseNets, the variance of the squared
Jacobian norm is in fact polynomial in depth for Gaussian distributions. Constant mean and variance
is achieved (at least when ∆ = 0) by setting α = 1, for any depth. Moreover, the constant variance
result is achieved with a linear growth rate of the weight complexity, instead of quadratic with vanilla
networks.
Empirical support We employed Monte Carlo simulations in order to verify our theoretical
findings. As shown in Fig. 2, there is an excellent match between the simulations and our derivations.
5 The Curious Case Of Concatenated ReLU
Our results stated above demonstrate how different architectures and initialization schemes affect the
training process at the start of training, by analyzing the expected norm and variance of the gradients
for each layer, and hold for ReLU based architectures. Since a similar analysis can be performed on
linear random networks, it is interesting to see the effect of the ReLU non-linearity on the outcome.
As it turns out, the ReLU function has a detrimental effect on random networks, as shown in the
following lemma, effectively exacerbating the fluctuations of both the activations and the gradients:
Lemma 3. Given a vanilla linear neural network with Wij 〜N(0, -1-), then it holds that:
nl-1
9" N
(22)
Linear networks, of course, have limited capacity, considerably limiting their practical use. Fortu-
nately, we can use the ReLU function to construct computation blocks that behave as linear networks
at the initialization stage, while not compromising in terms of capacity. This can be achieved by the
use of CR units. In concatenated ReLUs blocks, the input signal x is split into two complementary
components x+ and x- such that the output of layer l is parameterized by two weight matrices
Wl1, Wl2, and is given by yl = W l1>φ(yl-1) - Wl2>φ(-yl-1). An algebraically equivalent way
to write the layer’s computation is yl = Wl2>yl-1+ (W l1> + W l2>)φ(yl-1), which can be viewed
as a variant of a residual connection. However, it is important to note that this equivalence holds for
the forward-pass only, since deriving by Wl1and Wl2 during the optimization process is not the same
as performing an optimization that is based on a derivation by Wl1- Wl2 and Wl2. The equivalence
at initialization between the CR based architectures and random linear networks is depicted in the
following Lemma:
7
Under review as a conference paper at ICLR 2020
Figure 3: Activations norm per layer, as obtained from simulated results of 5000 independent runs.
(a) The mean of the squared norm of the activations in each layer (y-axis) as a function of depth
(x-axis) for randomly initialized networks. The red plot is for a ReLU network, the blue for a CR
based network. (b) the y-axis is the standard deviation of the squared norm of the activation. In
addition to the blue and red plots, the black and green plots are the theoretical predictions according
to Lem. 2,3.
Lemma 4. The distribution of the output yl = Wl1>φ(yl-1) - Wl2>φ(yl-1) conditioned on yl-1,
where Wj Wj 〜N (0, nαγ) equals the distribution of y = W l>yl-1, where the elements of Wl
are distributed according to an i.i.d normal distribution Wij 〜N(0, nαγ).
Using CR, therefore, provides initial conditions identical to random linear networks in terms of
the output statistics. However, this equivalence also holds for back propagation. Indeed, from the
construction of Wl in the lemma, it is easy to see that (∂⅛ )2 = ( ∂⅛ )2 + (∂⅛ )2 . A surprising
property emerges. Lem. 4 shows that similar to vanilla ReLU based networks, CR based ReLU
networks have exponential behavior in PL=I n.Since CR employ twice as many parameters, then
for the same budget of parameters (having nι = √ for CR) We have PL=I 2n2 < PL=I 能,hence
more efficient than standard ReLU blocks. We can similarly define residual and densely connected
architecture comprised of concatenated ReLU blocks. In that case, the conditions of both architectures
in terms of output distributions at initialization will be identical to the linear counterparts of both
architectures, given by yl = Wlk1>φ(yl,k-1) + Wlk2>φ(-yl,k-1) and yl = P；-=。Wll0>yl0 for
ResNets and DenseNets, respectively.
Further evidence for the practical effectiveness of CR layers is provided in the appendix, in which we
present experiments on the UCI datasets.
6	Conclusions
The interplay between architecture and initialization determines much of the early dynamics of
training deep networks. In this work, we have demonstrated how careful initialization in general
residual architectures effects the gradients at the start of training, by a rigorous study of the variance
of the per layer gradient norm at the point of initialization. Through the use of the duality principle
between forward and backward propagation, we have derived the expression for the variance in three
architectures: standard networks, ResNets and DenseNets. We show that DenseNets benefit from a
simple variance preserving initialization, which is independent of depth, while a depth dependent
initialization exists for ResNet. We have also shown that CR blocks perform better then ReLU blocks,
given a fixed budget of weights, when it comes to norm preservation in deep layers. As future work,
we would like to extend our analysis to include normalization techniques, such as batchnorm.
References
DjOrk-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
8
Under review as a conference paper at ICLR 2020
Boris Hanin. Which neural net architectures give rise to exploding and vanishing gradients? In
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.),
Advances in Neural Information Processing Systems 31, pp. 582-591. 2018.
Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.),
Advances in Neural Information Processing Systems 31, pp. 571-581. 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely connected convolutional networks. In
arXiv:1608.06993, 2016.
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
neural networks. In Advances in Neural Information Processing Systems, pp. 972-981, 2017.
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information
Processing Systems 30, pp. 4785-4795. Curran Associates, Inc., 2017.
Jeffrey Pennington, Samuel S. Schoenholz, and Surya Ganguli. The emergence of spectral universality
in deep networks. In AISTATS, 2018.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. In ICLR, 2017.
Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and improving
convolutional neural networks via concatenated rectified linear units. In ICML, pp. 2217-2225,
2016.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington.
Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla con-
volutional neural networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 5393-5402, Stockholmsmassan, Stockholm Sweden, 10-15 JUl 2018. PMLR.
Greg Yang and Sam Schoenholz. Mean field residual networks: On the edge of chaos. 2017.
Hongyi Zhang, Yann N. Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without
normalization. In ICLR, 2019.
7	Proofs
We make use of the following propositions and definitions in order to prove lemma 1 and theorem 1.
For ResNet, we assume m = 2 for notation simplicity. The results hold for m > 2 as well with no
additional arguments.
A path γt from input to output unit t defines a product of weights along the path denoted by
|Yt|	|Yt|
Qγt =	wγt ,lzγt =	qγt ,l	(23)
l=1	l=1
where ∣γt∣ is the length of path Yt (for vanilla and DenseNets ∣γt∣ = L), zγt is a binary variable
zγt ∈ {0, 1} indicating if the path is "active" (i.e, all relevant Relu activations along the path are
open), and wγt,l are the weights along path γt, and qγt,l = wγt,lzγt .
9
Under review as a conference paper at ICLR 2020
We generalize the above expression the following way:
L
Qγt =	qγt,l
l=1
(24)
where:
qγt ,l
IWYt,lzγt,l
l ∈/ γt
else
(25)
where l ∈ Yt indicates if layer l is skipped, zγ± ,ι is the activation variable relevant for weight Wγ5ι.
That is, if Wγ5ι = Wj then Zγ5ι = Zj where Zj ∈ {0,1} is the activation variable unit j in layer l.
Wγt,ι denotes a generalized weight that can take the value of a weight Wγ%ι = Wγ%ι for vanilla layers,
or a product of weights WγQ = Wγt,iι Wγt,i2 for residual layers (wγt,iι ∈ Wl1 and Wγt,i2 ∈ Wl2).
proposition 1. Given a random vector W = [W1...Wn] such that each component is identically and
symmetrically distributed iid random variable with moments E(Wim) = cm, and random binary
variable Z such that Z|W ∈ {0, 1}, Z|W = 1 - Z| - W, then it holds that for n > l:
l	I0
e(∏ wmz) = ∖ (cm)l
i=1	IE(Qi=I WmZ)
m is odd, l is even
m is even
else
(26)
Proof. We have:
clm = Z Y Wimp(W)dW =ZY
Wimp(W)dW +ZY
Wimp(W)dW
w i=1	w|z=1 i=1	w|z=0 i=1
Z	Y Wimp(W)dW + Z	Y(-Wi)mp(W)dW
= Z Y WimZp(W)dW + Z Y(-Wi)mZp(W)dW
w i=1	w i=1
For even m or even l: it follows that:
clm = 2 Z Y WimZp(W)dW
w i=1
and so:
l
E(YWimZ)
i=1
cm
2
(27)
(28)
(29)
(30)
(31)
For odd l, since W is symmetrically distributed, we have that Cm = 0, and so E(Qi=I WmZ) = cm =
0	—	□
proposition 2. For any set of paths γt1, γt2, it holds that:
E(QYI Qγ2 ) = {E(QYι ) = Q= E ((qγi,ι)2)
γt1 6= γt2
γt1 = γt2
(32)
Proof. From proposition 2 we have:
L
E(QYtiQYt2) = E(YqYti,lqYt2,l)
l=1
(33)
10
Under review as a conference paper at ICLR 2020
Since the computations done by all considered architectures form a markov chain, such that the
output of layer yl depends only on weights Rl = {W1...Wl}, using conditional expectation,:
L-1
E(Qγt1Qγt2) =E E(Qγt1Qγt2RL-1) =E Yqγt1,lqγt2,lE(qγt1,Lqγt2,LRL-1)	(34)
l=1
Note that since both are paths leading to output unit t, the following holds:
(E(WYI ,Lwγ2,LzL)
EC “ IRL-1、_ JE(WYI ,LzL)
EEMq 寸NR	) =F(WY2 LzL)
L ∈ γt1,L ∈ γt2
L ∈ γt1,L ∈/ γt2
L ∈/ γt1 , L ∈ γt2
L ∈/ γt1 , L ∈/ γt2
(35)
and so it follows from proposition 1:
IE(WY1,LzL)
E(qYtL,lqYt2,LIIR - ) =	1
∣0
L ∈ Y1,L ∈ Y2,wγ1,L = Wγ2,L
L ∈Y1,L ∈γ2	= E ((qγ1,L)2)	(36)
else
Recursively going through l = l - 1...1 completes the proof.
□
proposition 3. For any set of paths γt1, γt2, γs1, γs2, from input to output units t and s, it holds that:
L
E(QYt1QYt2QYs1QYs2) =YE ξYs1t2,l	(37)
l=1
such that:
[E(q21,ιqY1,ι)	qγ1,ι	=	q)2,i,qγ1,ι =	q弋,
E ξ 12 l = E q21 q22	q 1 l	=	q 1 l, q 2 l =	q 2 l	(38)
Yst ,l	Yt1 ,l Yt2 ,l	Yt ,l	Ys ,l , Yt ,l Ys ,l
I 0	else
Proof. The proof is practically identical to proposition 3, and will be omitted.	□
For notation simplicity,
7.1	proof of lemma 1
The proof of lemma 1 stems from the following observation: Since the computations done by
all considered architectures form a markov chain, such that the output of layer yl depend only
weights W1...Wl, then for any particular realization of the weights, yL,k and yLk differ only in their
activation patterns in layers l ≥ k due to the removal of bypassing connections in yL (k). Using
Eq. 23:
nL	|Yt |
E kyL,kkm =XE ( X YWYt,lzYt)m	(39)
t=1	Yt∣k∈Yt l = 1
while:
nL	|Yt |
E	ky(Lk)km = XE ( X YWYt,lzY0t)m
t=1 γt∣k∈γt l=1
(40)
We must therefor prove the expectation is invariant to a switch between zYt and zY0 for layers l > k.
For m = 2, using proposition 2, the expectation factorizes:
nL	nL	L
E(kyL,kk2)= X X E(QYI) = X X YE ("B	⑷)
t=1 Y1∣k∈Yt	t=1 Y1∣k∈Yt l = 1
11
Under review as a conference paper at ICLR 2020
Similarly for m = 4, from proposition 3 the expectation factorizes:
nL	L
EkyL,kk4 =X X YEξγs1t2,l	(42)
t=1 k∈γt1,γt2,γs1,γs2 l=1
where ξγ12,l is defined is Eq. 38. Note that switching the sign of Wl for all architectures considered
will switch the state of both zjl , 0 < j ≤ nl and zj0l, 0 < j ≤ nl, and so from proposition 1, the
factorized expectations for m = 2 and m = 4 meet the required invariance.
7.2	proof of theorem 1
From the definition of the Jacobian for weights Wk, we have using lemma 1 and proposition 2:
E(kJjk2) = E(k∂Wk)k2) = X X 2E(IYYQYtQγ2)
nL	L
=XX E( (⅛ ∏ QYt)
t=1 ijk∈γt	ij	l=1
Since by definition wikj ∈ γt1,from proposition 2 the term E ((仅1尸 QL=1 Q；i) factorizes:
E( ⅛ ∏ QYt)=E((⅜ In=IE*。=E(jk In=IE%J
LL	L
=E(zk) ∏ E*ι) = 2 ∏ E(qYt,ι) = E ck (∏ QYt)
l6=k,l=1	l6=k,l=1	2 l=1
(43)
(44)
(45)
where the last transition stems from proposition 1. Plugging back into Eq. 43 and summing over ij
completes the proof for m = 2.
For m = 4, we have:
∂yL	∂yL
E(Jkk 4) = E(X k / k2k 尚 k2)
ijuv ij	uv
nL nL	L
=XX X X	E((w⅛τv∏QYtiQYt2QYsiQYs2) (46)
t=1 s=1 ijk∈Yti,Yt2 uvk∈Ysi,Ys2	(wij) (wuv) l=1
From proposition 3, for uvk ∈ γs1, γs2, ijk ∈ γt1, γt2 the expectation factorizes:
1L
E( (W 铲(Wuv )2 ∏QYI Qγ2 QYIQ 弋)
ξY12,k
(Wky(Wuv )2
L
∏ E ξYsit2,l
l6=k,l=1
where from proposition 1 and proposition 3, for uvk ∈ γs1, γs2, ijk ∈ γt1, γt2:
{E(ξ 12 ))
F {u,v} = {i,j}
Eξ2	{u,v} = {i,j}
c4
Note that since c4k ≥ (c2k )2, it holds that:
E(ξ i2 k )	ξ i2 k	E(ξ i2 k )
YSt Ik) ≤ E	、Yst ,k	) ≤	YSt Ik)
ʒk_ _	XWj)2 Wuv )2 J (	(Ck )2
(47)
(48)
(49)
(50)
E
12
Under review as a conference paper at ICLR 2020
Plugging into Eq. 46, it follows:
≤ var(kJkk2) = E(kJkk4) - E(kJkk2)2 ≤
Var(HyLk)k2)
-西-
(51)
Furthermore, it holds that:
ξL
E(kJkk4) = X X X E ((WkMfe )2) Y E (ξγ"(52)
st ijk∈γt1 ,γt2 uvk∈γs1 ,γs2	ij uv	l6=k,l=1
ξL
=XX X E ((W罩Wk )2) (l{u,v}={ij} + l{u,v}={ij}) Y E (ξγ12,l)(53)
st ijk∈γt1,γt2 uvk∈γs1,γs2	(wij) (wuv)	l6=k,l=1
Therefor for large width networks:
ξL
E(kJkk4) 〜X X X E ((WkF(Wk )2) i{u,v}={ij} γ E (ξγ12,ι)
st ijk∈γt1,γt2 uvk∈γs1 ,γs2	ij uv	l6=k,l=1
1L
=(ck)2 X X X	E (ξγ12,k) 1{u,v}={i,j} Y E (ξγ12,l)
2 st ijk∈γt1 ,γt2 uvk∈γs1 ,γs2	l6=k,l=1
〜(C1)2 E(HyLk)k4)
and so for large width networks:
k 2	var(Hy(Lk)H2)
Var(Jk k2)-(ck)2-
(54)
(55)
(56)
(57)
7.3	proof of lemma 2
Denoting by Zl the diagonal matrix holding in its diagonal the activation variables zjl for unit j in
layer l, and so we have:
yl = ZlWl>yl-1	(58)
Conditioning on Rl-1 = {W1...WL-1} and taking expectation:
E(HyL H2|RL-1) =yL-1>E(WLZLWL>)yL-1	(59)
nL nL-1
= X X yil1-1yil2-1E (WiL1,j WiL2,j zjL |RL-1)	(60)
j=1 i1 ,i2=1
From proposition 1, it follows that:
E(kyLk2∣RLT) =悬 E(kyLTk2)	(61)
Recursing through l = L - 1...1:
E(kyLk2) = nnL	(62)
Similarly:
E(kyL k4|RL-1) =E((yL-1>WLZLWL>yL-1)2)	(63)
= X	yiL1-1yiL2-1yiL3-1yiL4-1E (WiL1,j1WiL2,j1WiL3,j2WiL4,j2zjL1zjL2|RL-1)	(64)
j1 ,j2 ,i1 ,i2,i3,i4
13
Under review as a conference paper at ICLR 2020
From proposition 3:
E (WLl,jl wL2,j1 WL ,j2 wt,j2 zLl j IRLT) (65)
E (Wil,jl wi2,j1 wi3,j2 Wi4,j2Zj1 Zj2 ∣R ) (lj1=j2,i1=i2=i3=i4 + 3I j1 = j2 ,i1 =i2 ,i3 =i4,i 1 =i3 …(66)
+ ɪ jl =j2 ,i1 =i2 ,i3=i4 ) (67)
and so:
E(kyLk4) = * XE ((yL-1)4) +
i=1
F X E ((yLT)2%-1)2
nL-1 il =i2
(68)
n∑(n∑ - 1)
nL(cL - 3(CL)2)
nL-1
nL-1
X E ((yL-1)4)
i
XE (yiL-1)4
i
X E ((yL-1)2(y⅛-1)2)
i1 ,i2
nL(nL + 5)
"^2
n2L-1
nL(nL + 5)
E kyLTk4
(69)
(70)
一 2
nL-ι
E kyLTk4
(71)
+
2
+
n∑∆
+
finally:
nL (nL + 5 + ∆)
一 2
nL-ι
E kyLTk4 ≤
E (∣∣yLk4)
(72)
nL(nL + 5+ ∣∆∣)
一 2
nL-ι
yLTk4
(73)
Recursing through L - 1...1, and denoting C1
combining with Eq. 62:
nL(nL+5-∣∆∣)
,and C2
nL(nL+5 + ∣∆∣)
and
L-1
C1 ∏ (1 +
5 -∣∆∣
-Σ---) -
l=1
Using ∀1≥χ≥-1,
nι
2
nL ≤ var(kyLk2) ≤
e0-5x ≤ (1 + x) ≤ ex:
L
C1 exp X
l=1
5 -∣∆∣
2nι
-C2 ≤ var (∣∣yLk2) ≤
L-1
C ∏ (1 +
l=1
5+ ∣∆∣)
nι
C3 exp E
i=1
5 +1∆∣
nι
n∙
(74)
(75)
≤
E


—
L
n
2
L
—
For Gaussian distribution we have that ∆ = 0, and so:
var(kyLk2)=
nL(nL + 5)
n2
L-1	2
∏ (1 + 3- nL
nl	n2
L-1
(76)
7.4	PROOF OF THEOREM 2
In the following proof, for the sake of notation simplicity, we omit the notation k in ʤ, and assume
that yl stands for the reduced network y(k). We have for layer L:
E (kyLk2) = E (kyLTk2) + E (yL-1,m-1>w Lmw LmTyLT,m-1)
=E(kyLTk2) + αE(∣∣yLT,m-1k2) = E(∣∣yLTk2)(1 + αm
L
L
E (kyk k2) ∏ (1 + αm)= E (kyk-1k2) ∏ (1 + αm) αm
l = k+1
l = k + 1
∏ (1 + αm) 0m (77)
l=k
14
Under review as a conference paper at ICLR 2020
E(Il 内4) = E (IlyLTk4) + E (IIyLT 叫4) +4E((yjm V-1 y)
+ 2E(∣∣yLTmI 2kyLτ∣∣2) (78)
We now handle each term separately:
E (IIyLT,mk4) = E (E (∣∣yLT,mk4∣RLT))	(79)
Similarly to the vanilla case with a linear weight layer on top, we have:
a2m(1 + 5 -∣δ∣ )m-1∣∣yLTk4 ≤ E (IIyLTmk4∣RLT)	(80)
≤ α2m(1+ 5 + ∣δ∣ )m∣∣yLTk4	(81)
n
E ((yLTmTTyL「2)= X E WLTyLTn『中”卷 WLmj
jιj2i1i2
m
=nE(∣∣yLT,mτ∣∣2∣∣yLTk2) = -- E(IIyLTk4)
and:
E (∣∣yLTmk2kyLTk2) = -mE (IlyLT k4)
Plugging it all into Eq. 78, and denoting:
β+ = 1 + 2αm(1 + 2) + α2m(1 + 时m)m
l	n	n
β- = 1 + 2αm(1 + 2) + α2m(1 + 上均)m-1
l	n	n
we have after recursing through l = L - 1...1:
LL
E(kyfck4) ∏ β- ≤ E(kyLk4)≤ E(kyfck4) ∏ β+
l=k+1	l=k+1
(82)
(83)
(84)
(85)
(86)
(87)
Since for the reduced architecture 夕储，the transformation between yk-1 and yk is given by an m
layer fully connected network without any residual connections, we can use the results from the
vanilla case:
∏β--2m(1 + 5-^)m-1 ≤ E(kyLk4) ≤ ∏β+ -2m(1 +
l=k	l=k
5+ ∣∆∣ )m
n
Denoting ρ± = -2(1 + 5±δ1 ) it follows:
LL
C1 ∏ β-ρm ≤ E(kyLk4) ≤ ∏ β+ρm
l=k	l=k
From the definition of β±, it holds for ρ+ < 1:
and:
β- ≥ (1 + ρm) ≥ exp [1 ρm
(88)
(89)
(90)
(91)
and so:
C1 ρm exp	2 Lρm	-。2 μ2	≤ Var(IlyLk2) ≤	C⅛ρ7 exp	2LρJ	-	C⅛μ2	(92)
15
Under review as a conference paper at ICLR 2020
For Gaussian distributions, using a similar derivation and assigning ∆ = 0 we have:
L
E kyLk4 =ρYβl
l6=k
where:
βι = 1 + 2αm(1 + 2) + α2m(1 + 5)m-1(1 + 2)
n	nn
and:
52
P = a2m(1 + n)m (1 + -)
(93)
(94)
(95)
7.5	Proof of theorem 3
In the following proof, for the sake of notation simplicity, we omit the notation k in y(lk), and assume
that yl stands for the reduced network y(lk) . We have:
L-1	L-1	L-1
E(kyLk2) =〃l= e((X yl>wLl)ZL(X yl>wLl)) = CL X μι-ι	(96)
l=k	l=k	l=k
L-1
L-1
E kyLk4 =E	X(yl>WLl)ZL X(yl>WLl
l=k
L-1
l=k
L-1
L-1
L-1
E	X (yl1>W Ll1)ZL X (yl2>W Ll2) X(yl3>WLl3)ZL X (yl4>W Ll4)	(97)
l1=k
l2=k
l3=k
l4=k
2
Note that the above is a simple Relu block with concatenated y1...yL - 1 as input. Using the results
from the vanilla architecture, and denoting Cl,l0 = E kylk2 kyl0 k2 , it then follows:
(n + 5 — ∣∆∣)α'
nL2
2 L-1
X Cl1,l2 ≤ CL,L ≤
l1,l2=k
(n + 5+ ∣∆∣)α'
nL2
2 L-1
X Cl1,l2
l1,l2=k
(98)
From Eq. 98, it holds that:
n(L - 1)2
n + 5+ ∣∆∣
L-2
CL-1,L-1 ≤ X Cl1,l2
l1,l2=k
n(L - 1)2
n + 5 - ∣∆∣
CL-1,L-1
(99)
≤
It then follows:
E(kyLk4) = Cl,l ≤ (L(i + 5⅛δ1 ) X Clιl2
l1,l2=k
L-2
L-2
)CL-1,L-1 + X Cl1l2 + 2 X CL-1,l
i+5+n^ MCLT,L-1+
l1,l2=k
(L - 1)2n
aL-ι(n + 5 T△1)
l=k
L-2
CL-1,L-1 + 2 X CL-1,l
l=k
(100)
1 + 3
n
(101)
The following also holds:
l1-1	l1-1
∀lι>l2≥k, Clι,l2 = E((X yl>Wl1,lZl1 )2kyl2k2) = O- X C^
l=k	l=k
(102)
16
Under review as a conference paper at ICLR 2020
and so:
Cl,l ≤
(n +5+ ∣A)02 (C	+	(L- l)2n	C
nL2	(CLTL-i + α2(n + 5 _ ∣∆∣) LTLT
2α
L - 1
L-2 L-2
XX CM)
lι =k l2 = k
—(n + 5+ ∣δ∣)o2 (C	+	(L _ 1)2n C
=	nL2	(CLTLT + α2(n + 5 _ ∣∆∣) L-1，LT
,2n(L - 1) C )
+ α(n + 5 - ∣∆∣) L-1，LTJ
(n + 5 + ∣δ∣)o2 C	(I +	(L - 1)2n	+ 2n(L - 1))
nL2	L-1,L-II + α2(n + 5 - ∣∆∣) + α(n + 5 - ∣∆∣)7
_ C	(n + 5+ ∣δ∣) ( ( α + a(L - 1))2 + a2(5 - ∣δ∣)∖
= L-1,L-I (n + 5 -1∆∣) VLl + -Lo-) + -nL2-)
(103)
(104)
(105)
(106)
(107)
(108)
Cl,L ≤ CL-1,L-1 ( 1 +
：)((1+T )2+
a2(5 -∣∆∣))
nL2	)
cL-1,L-1 (1 +
：)(1 + ⅛∆⅛ )(1 +
a — 1)2
L )
Telescoping through l = L - 1...1:
Cl,l ≤
a2(5 - ∣δ∣) ) (I + a - 1 )2)
n(l + a — 1)2√ ∖ l J )
1+J)L-k+1ιY ((1+
≤ exp	.2∣∆∣(L - 1)- n + 5 - ∣∆∣	exp	X α2(5 - ∣δ∣) 白 n(l + α - 1)2)		exp	X 2(a - 1)- 工-1- .l=k	.	
	〜	exp	-2∣∆∣(L - 1)- n + 5 - ∣∆∣	exp	a2(5 -∣∆∣)- nL		L2(α-1)
L2(α-1)
〜Ci exp
2∣∆∣L
n
(109)
(110)
(111)
(112)
(113)
(114)
+
A similar relation holds for the mean:
a	a	(L - 1)
μL = L), μl = L (μL-1,L-1 + a	)μL-1,L-1 J	(115)
l=k
μL = μL-1,L-1(1 + a-1 )2 ≤ L2(aT)	(116)
Finally:
Var(IIyLk2) = Cl,l - μL	(117)
≤ L2(aT)(CI exp 2∣∆∣L - 1)	(118)
A lower bound can easily be derived using Eq. 98, 99, and follow the same derivation by flipping the
sign of ∣∆∣:
LgT)(CI exp	_2∆^	- C2)	≤ Var(IlyLl∣2)	≤ L2(a-1) (C3	exp	2^	-	C4)	(119)
17
Under review as a conference paper at ICLR 2020
For Gaussian distributions, using a similar derivation and assigning ∆ = 0 we have:
and so:
CL,L
5α2	、([ ι
n(l + α - 1)2 ) (1 +
L
μL = Y(1 +
l6=k
var(kyLk2)
八(1：： - 1)2 ) - a) Y (1 +
(120)
(121)
(122)
7.6	proof of lemma 3
The proof is identical to the Relu case, removing the activation variables Zl, and hence will be
omitted.
7.7	proof of lemma 4
Proof. We construct the matrix Wl as follows:
w
l
ij
yl-1 ≥ 0
yl-1 < 0
(123)
And so yl = Wl>yl-1. Note that the elements of Wl are i.i.d Gaussian random variables Wj 〜
N(0, nαι).	□
8 Empirical support for Concatenative ReLU
Our theoretical results are verified in Fig. 3. As can be seen, the simulations closely match the
predictions. The advantage of CR over conventional ReLU networks is also clearly visible.
CR has fallen out of flavor and was never shown to be effective outside CNNs. We, therefore, provide
empirical support for its effectiveness, to which our analysis above points. In our experiments, we
evaluate the performance of fully connected networks on the 40 UCI datasets with more than 1000
samples. The train/test splits were provided by Klambauer et al. (2017). In the experiment, we tested
three different depths: 8, 16, and 32. Each layer contained n = 256 hidden neurons. Dropout was not
employed. A learning rate of either 0.1, 0.01, or 0.001 was used for the first 50 epochs and then a
learning rate of one tenth for an additional 50 epochs. All runs were terminated after 100 epochs.
Batches were of size N = 50.
Following Klambauer et al. (2017), an averaging operator with a mask size of 5 was applied to the
validation error, and the epoch with the best smoothed validation error was selected. This was done
separately for each method, each dataset, and each split. Out of the nine options of depth and learning
rate for most methods, the mean validation error across the four splits was used to select the option
for which we report the results on the test splits. We compared the following activation functions
and architectures: vanilla neural networks (single pathway) with either SELU (Klambauer et al.,
2017) or ELU (Clevert et al., 2015) activation functions, ResNets, and CR based architecture. The
single pathway ReLU networks were not competitive and the experiments with this architecture
were terminated to allow for more experiments with the other architectures. The single pathway
experiments were done without batchnorm, as is usually done with ELU and SELU. For ResNets, we
report results both with batchnorm and without. Batchnorm was applied along the residual pathway,
twice, as is typically practiced. The first and last layers of the ResNet and CR architecture, i.e., the
layers that project from the input and to the output layer, are conventional projections that do not
employ multiple pathways. For the baseline architectures, we also evaluate using double width, i.e.,
18
Under review as a conference paper at ICLR 2020
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
0.99	0.98	0.97	0.96	0.95
Figure 4: A Dolan-More profile, based on the accuracy obtained for the UCI experiments. The x-axis
is the threshold (T). Note that since for accuracy scores, higher is better, whereas typical Dolan-More
plots show cost, the axis is ordered in reverse. The y-axis depicts, for a given architecture out of the
nine, the ratio of datasets in which the obtained accuracy is above the threshold τ times the maximal
accuracy obtained by any architecture .
19
Under review as a conference paper at ICLR 2020
Table 1: Error rates for each method per dataset. 512 denote the double width settings.
	CR	ELU	ELU 512	SELU	SELU 512	RES	RES 512	RES w/o bn	RES 512 w/o bn
semeion	0.102	0.1357	0.118	0.1543	0.1356	0.1192	0.1215	0.121	0.1308
oocytes..._2f	0.0941	0.0918	0.0949	0.0938	0.0892	0.0897	0.0921	0.0944	0.1084
wine-quality-white	0.4532	0.4633	0.4488	0.4597	0.4547	0.4521	0.4552	0.4573	0.4645
ringnorm	0.0208	0.0253	0.0251	0.026	0.0222	0.0261	0.0220	0.0205	0.0341
statlog-image	0.0448	0.0483	0.0472	0.042	0.0495	0.0467	0.0639	0.0489	0.0382
cardiotocography-10	0.2090	0.2207	0.212	0.2222	0.2073	0.2147	0.2134	0.2126	0.2141
abalone	0.3408	0.3439	0.3391	0.3476	0.3492	0.3452	0.3479	0.3464	0.344
ozone	0.0295	0.0300	0.0287	0.0339	0.0288	0.0287	0.0312	0.0288	0.0321
thyroid	0.0207	0.0179	0.0183	0.0211	0.0198	0.0192	0.0186	0.018	0.0247
oocyte..._4d	0.1855	0.1946	0.194	0.2052	0.2076	0.2021	0.2006	0.2047	0.2156
adult	0.1515	0.1539	0.1530	0.1559	0.1550	0.1515	0.1509	0.1507	0.1538
chess-krvkp	0.0215	0.0333	0.0257	0.0307	0.0255	0.0205	0.0262	0.0275	0.0178
waveform	0.1403	0.1419	0.1441	0.1468	0.1455	0.1355	0.1399	0.1377	0.1656
plant-texture	0.2614	0.3412	0.2991	0.3022	0.2777	0.2814	0.2704	0.2738	0.3296
statlog-landsat	0.1061	0.1025	0.1011	0.1011	0.0987	0.1053	0.1111	0.1069	0.0960
steel-plates	0.275	0.2933	0.2815	0.2832	0.2776	0.2961	0.2879	0.2881	0.2926
statlog-vehicle	0.2188	0.2115	0.2159	0.2213	0.2233	0.2198	0.2285	0.2164	0.2075
cardiotocography-3	0.0967	0.1057	0.1022	0.1023	0.1008	0.0988	0.1053	0.0994	0.1055
page-blocks	0.0324	0.0367	0.0345	0.0367	0.0355	0.0356	0.0343	0.0376	0.0367
wall-following	0.1131	0.1232	0.1272	0.1269	0.1235	0.1204	0.1236	0.1190	0.1192
miniboone	0.0793	0.0751	0.0750	0.0790	0.0790	0.0849	0.0889	0.0772	0.2481
statlog-shuttle	0.0016	0.002	0.0017	0.0015	0.0015	0.0025	0.0025	0.0022	0.0017
mushroom	0.0004	0.0004	0.0003	0.0010	0.0003	0.0010	0.0008	0.0007	0.0001
waveform-noise	0.1464	0.1501	0.1520	0.1626	0.1576	0.1437	0.1421	0.7090	0.1705
twonorm	0.0231	0.027	0.0252	0.0261	0.0258	0.0236	0.0243	0.0241	0.0341
connect-4	0.1442	0.1508	0.1493	0.1483	0.1442	0.1423	0.1431	0.1424	0.1628
letter	0.0488	0.0700	0.0650	0.0606	0.0583	0.0880	0.0778	0.0818	0.0485
titanic	0.2197	0.2148	0.2151	0.2170	0.2118	0.2149	0.2181	0.212	0.2149
statlog-heart	0.1844	0.1939	0.1808	0.2093	0.2113	0.1967	0.2016	0.1794	0.2276
plant-margin	0.2798	0.3330	0.2835	0.3258	0.2918	0.2772	0.2764	0.2951	0.3749
contrac	0.4676	0.4757	0.4716	0.4816	0.4852	0.4606	0.4622	0.4693	0.4889
yeast	0.4239	0.4205	0.4252	0.4333	0.4253	0.4141	0.4199	0.4431	0.4649
bank	0.1121	0.1114	0.1131	0.1180	0.1170	0.1084	0.1161	0.1099	0.1143
hill-valley	0.3667	0.3699	0.3774	0.3700	0.3554	0.3899	0.3657	0.3765	0.4062
optical	0.0299	0.0456	0.0408	0.0416	0.0417	0.0477	0.0475	0.0426	0.0366
plant-shape	0.4127	0.5439	0.4862	0.4300	0.4220	0.4593	0.4581	0.4509	0.4228
led-display	0.2906	0.2929	0.2827	0.2911	0.3060	0.2852	0.2856	0.2781	0.3093
magic	0.1293	0.1310	0.1331	0.1345	0.1344	0.1375	0.1348	0.1329	0.1332
pendigits	0.0388	0.0446	0.0434	0.0426	0.0432	0.0466	0.0483	0.0467	0.0439
chess-krvk	0.2452	0.2592	0.2426	0.2399	0.2284	0.2956	0.2848	0.2741	0.3289
each layer had 512 hidden neurons. This was done since CR employs more weights than the baselines
due to the dual pathway architecture.
In Fig. 4, we provide the Dolan-More profile of the methods. In this plot, the x-axis is a threshold
τ and the y-axis is the ratio, out of all datasets, in which the method has reached an accuracy that
is τ times the best accuracy for the dataset. It can be seen that the CR method dominates across all
threshold values, except for SELU double width at low values of τ .
9 UCI Results
In Tab. 1 we provide the full numbers of the UCI experiments in the paper.
20