Under review as a conference paper at ICLR 2020
A Generalized Framework of Sequence Gen-
eration with Application to Undirected Se-
quence Models
Anonymous authors
Paper under double-blind review
Ab stract
Undirected neural sequence models such as BERT (Devlin et al., 2019) have re-
ceived renewed interest due to their success on discriminative natural language
understanding tasks such as question-answering and natural language inference.
The problem of generating sequences directly from these models has received
relatively little attention, in part because generating from such models departs
significantly from the conventional approach of monotonic generation in directed
sequence models. We investigate this problem by first proposing a generalized
model of sequence generation that unifies decoding in directed and undirected
models. The proposed framework models the process of generation rather than
a resulting sequence, and under this framework, we derive various neural se-
quence models as special cases, such as autoregressive, semi-autoregressive, and
refinement-based non-autoregressive models. This unification enables us to adapt
decoding algorithms originally developed for directed sequence models to undi-
rected models. We demonstrate this by evaluating various decoding strategies
for a cross-lingual masked translation model (Lample and Conneau, 2019). Our
experiments show that generation from undirected sequence models, under our
framework, is competitive with the state of the art on WMT’14 English-German
translation. We also demonstrate that the proposed approach enables constant-time
translation with similar performance to linear-time translation from the same model
by rescoring hypotheses with an autoregressive model.
1	Introduction
Undirected neural sequence models such as BERT (Devlin et al., 2019) have recently brought
significant improvements to a variety of discriminative language modeling tasks such as question-
answering and natural language inference. Generation of sequences from such models has received
relatively little attention. Unlike directed sequence models, each word often depends on the full
left and right context around it in undirected sequence models. Thus, a decoding algorithm for an
undirected sequence model must specify both how to select positions and what symbols to place in
the selected positions. In this paper we formalize this process of selecting positions and replacing
symbols as a generalized framework of sequence generation, and unify decoding from both directed
and undirected sequence models under this framework. This framing enables us to study generation
on its own, independent from the specific parameterization of the sequence models.
Our proposed unified framework casts sequence generation as a process of determining the length
of the sequence, and then repeatedly alternating between selecting sequence positions followed
by generation of symbols for those positions. A variety of sequence models can be derived under
this framework by appropriately designing the length distribution, position selection distribution,
and symbol replacement distribution. Specifically, we derive popular neural decoding algorithms
such as monotonic autoregressive, non-autoregressive by iterative refinement and monotonic semi-
autoregressive decoding as special cases of the proposed model.
This separation of coordinate selection and symbol replacement allows us to build a diverse set of
decoding algorithms agnostic to the parameterization or training procedure of the underlying model.
We thus fix the symbol replacement distribution as a variant of BERT and focus on deriving novel
1
Under review as a conference paper at ICLR 2020
generation procedures for undirected neural sequence models under the proposed framework. We
design a coordinate selection distribution using a log-linear model and demonstrate that our model
generalizes various fixed-order generation strategies, while also being capable of adapting generation
order based on the content of intermediate sequences.
We empirically validate our proposal on machine translation using a translation-variant of BERT called
a masked translation model (Lample and Conneau, 2019). We design several generation strategies
based on features of intermediate sequence distributions and compare them against the state-of-the-art
monotonic autoregressive sequence model (Vaswani et al., 2017) on WMT’14 English-German.
Our experiments show that generation from undirected sequence models, under our framework, is
competitive with the state of the art, and that adaptive-order generation strategies generate sequences
in different ways, including left-to-right, right-to-left and mixtures of these. This suggests the
potential for designing and learning a more sophisticated coordinate selection mechanism.
Due to the flexibility in specifying a coordinate selection mechanism, we design constant-time variants
of the proposed generation strategies, closely following the experimental setup of Ghazvininejad et al.
(2019). Our experiments reveal that we can do constant-time translation with the budget as low as
20 iterations (equivalently, generating a sentence of length 20 in the conventional approach) while
achieving similar score to linear-time translation from the same masked translation model. This again
confirms the potential of the proposed framework and generation strategies.
2	A Generalized Framework of S equence Generation
We propose a generalized framework of probabilistic sequence generation to unify both directed
and undirected neural sequence models under a single framework. In this generalized framework,
we have a generation sequence G of pairs of an intermediate sequence Yt = (y1t , . . . , yLt ) and the
corresponding coordinate sequence Zt = (z1t , . . . , zLt ), where yit ∈ V , V is a vocabulary, L is a
length of a sequence, T is a number of generation steps, and zit ∈ {0, 1}. The coordinate sequence
indicates which of the current intermediate sequence are to be replaced. That is, each consecutive
pairs are related to each other by yt+1 = (1 - zt+1)yt + zt+1yt+1, where yt+1 ∈ V is a new
symbol for the position i. This sequence of pairs G describes a procedure in which a final sequence
Y T is created, starting from an empty sequence Y 1 = (hmaski , . . . , hmaski) and empty coordinate
sequence Z1 = (0, ..., 0). This procedure of sequence generation is probabilistically modelled as
p(G|X)
T L	t+1
p(L|X)	p(zit+1|Y ≤t, Zt, X) p(yit+1|Y ≤t, X)zi
X j{Z—}	t=1 i=1、----------{Z--------}、-------{-------}
(c) length prediction	(a) coordinate selection (b) symbol replacement
(1)
We condition the whole process on an input variable X to indicate that the proposed model is
applicable to both conditional and unconditional sequence generation. In the latter case, X = 0.
We first predict the length L of a target sequence Y according top(L|X) distribution to which we refer
as (c) length prediction. At each generation step t, we first select the next coordinates Zt+1 for which
the corresponding symbols will be replaced according to p(zit+1 |Y ≤t, Zt, X), to which we refer as
(a) coordinate selection. Once the coordinate sequence is determined, we replace the corresponding
symbols according to distribution p(yit+1 |Y ≤t, Zt+1, X), leading to the next intermediate sequence
Y t+1. From this sequence generation framework, we recover the sequence distribution p(Y |X) by
marginalizing out all the intermediate and coordinate sequences except for the final sequence Y T . In
the remainder of this section, we describe several special cases of the proposed framework, which are
monotonic autoregressive, non-autoregressive, semi-autoregressive neural sequence models.
2.1 Special Cases
Monotonic autoregressive neural sequence models We first consider one extreme case of the
generalized sequence generation model, where we replace one symbol at a time, monotoni-
cally moving from the left-most position to the right-most. In this case, we define the co-
ordinate selection distribution of the generalized sequence generation model in Eq. (1) (a) as
p(zt+1 = 1|Y≤t,Zt,X) = 1(zt = 1), where 1(∙) is an indicator function and z11 = 1. This
coordinate selection distribution is equivalent to saying that we replace one symbol at a time,
shifting from the left-most symbol to the right-most symbol, regardless of the content of inter-
mediate sequences. We then choose the symbol replacement distribution in Eq. (1) (b) to be
2
Under review as a conference paper at ICLR 2020
p(yt+1 |Y ≤t, X) = p(yt+1 |yt ,yt,…,yt, X), for zt+1 = 1. Intuitively, We limit the dependency
of yit++11 only to the symbols to its left in the previous intermediate sequence y<t (i+1) and the in-
put variable X. The length distribution (1) (c) is implicitly defined by considering hoW often
the special token heosi, Which indicates the end of a sequence, appears after L generation steps:
p(L∣X) 8 PyLL ] QL=-LIP(yl+1 = heosi∣y≤l,X). With these choices, the proposed generalized
model reduces to p(G|X) = QiL=1 p(yi|y<i, X) Which is a Widely-used monotonic autoregressive
neural sequence model.
Non-autoregressive neural sequence modeling by iterative refinement We next consider
the other extreme in Which We replace the symbols in all positions at every single gen-
eration step (Lee et al., 2018). We design the coordinate selection distribution to be
p(zt+1 = 1|Y≤t, Zt, X) = 1 ∀i ∈ {1,…，L}, implying that we replace the symbols in all the posi-
tions. We then choose the symbol replacement distribution to be as it Was in Eq. (1) (b). That is, the
distribution over the symbols in the position i in a new intermediate sequence yit+1 is conditioned on
the entire current sequence Yt and the input variable X . We do not need to assume any relationship
between the number of generation steps T and the length of a sequence L in this case. The length
prediction distributionp(L|X) is estimated from training data.
Semi-autoregressive neural sequence models Wang et al. (2018) recently proposed a compromise
between autoregressive and non-autoregresive sequence models by predicting a chunk of symbols
in parallel at a time. This approach can also be put under the proposed generalized model. We first
extend the coordinate selection distribution of the autoregressive sequence model into
p(zt+]	ι∣γ≤t,zt X) = ʃ1,	ifZki+j = 1,∀j ∈ {0,1,...,k}
k(i+1)+j	,	,	0,	otherwise,
where k is a group size. Similarly we modify the symbol replacement distribution:
p(yk+i+i)+j|Y ≤t, x )=p(ytk(i+i.)+j|y<t k(i+1) ,X),∀j ∈ {0,1,...,k},
for zit = 1. This naturally implies that T = dL/ke.
3 Decoding from Masked Language Models
In this section, we give an overview of masked language models like BERT, cast Gibbs sampling under
the proposed framework, and then use this connection to design a set of approximate, deterministic
decoding algorithms for undirected sequence models.
3.1	BERT as an undirected sequence model
BERT (Devlin et al., 2019) is a masked language model: It is trained to predict a word given
the word’s left and right context. Because the model gets the full context, there are no directed
dependencies among words, so the model is undirected. The word to be predicted is masked with a
special hmaski symbol and the model is trained to predictp(yi|y<i, hmaski , y>i, X). We refer to this
as the conditional BERT distribution. This objective was interpreted as a stochastic approximation to
the pseudo log-likelihood objective (Besag, 1977) by Wang and Cho (2019). This approach of full-
context generation with pseudo log-likelihood maximization for recurrent networks was introduced
earlier by Berglund et al. (2015). More recently, Sun et al. (2017) use it for image caption generation.
Recent work (Wang and Cho, 2019; Ghazvininejad et al., 2019) has demonstrated that undirected
neural sequence models like BERT can learn complex sequence distributions and generate well-
formed sequences. In such models, it is relatively straightforward to collect unbiased samples using,
for instance, Gibbs sampling. But due to high variance of Gibbs sampling, the generated sequence is
not guaranteed to be high-quality relative to a ground-truth sequence. Finding a good sequence in a
deterministic manner is also nontrivial.
A number of papers have explored using pretrained language models like BERT to initialize sequence
generation models. Ramachandran et al. (2017), Song et al. (2019), and Lample and Conneau (2019)
use a pretrained undirected language model to initialize a conventional monotonic autoregressive
sequence model, while Edunov et al. (2019) use a BERT-like model to initialize the lower layers of a
3
Under review as a conference paper at ICLR 2020
generator, without finetuning. Our work differs from these in that we attempt to directly generate
from the pretrained model, rather than using it as a starting point to learn another model.
3.2	Gibbs sampling in the generalized sequence generation model
Gibbs sampling: uniform coordinate selection To cast Gibbs sampling into our framework, we
first assume that the length prediction distribution P(L|X) is estimated from training data, as is
the case in the non-autoregressive neural sequence model. In Gibbs sampling, we often uniformly
select a new coordinate at random, which corresponds to p(zit+1 = 1|Y ≤t, Zt, X) = 1/L with the
constraint that PiL=1 zit = 1. By using the conditional BERT distribution as a symbol replacement
distribution, we end up with Gibbs sampling.
Adaptive Gibbs sampling: non-uniform coordinate selection Instead of selecting coordinates
uniformly at random, we can base selections on the intermediate sequences. We propose a log-linear
model with features φi based on the intermediate and coordinate sequences:
p(zt+1 = 1|Y ≤t,Z t,X) Y exp IT XX aiφi(Y t,Z t,X,i)},
(2)
again with the constraint that PiL=1 zit = 1. τ > 0 is a temperature parameter controlling the
sharpness of the coordinate selection distribution. A moderately high τ smooths the coordinate
selection distribution and ensures that all the coordinates are replaced in the infinite limit of T ,
making it a valid Gibbs sampler (Levine and Casella, 2006).
We investigate three features φi: (1) We compute how peaked the conditional distribution of
each position given the symbols in all the other positions is by measuring its negative entropy:
φnegent(Y t, Zt, X, i) = -H(yit+1|y<t i, hmaski ,y>t i,X). In other words, we prefer a position i if we
know the change in i has a high potential to alter the joint probability p(Y|X) = p(y1, y2, ..., yL|X).
(2) For each position i we measure how unlikely the current symbol (yit, not yit+1) is under the new
conditional distribution: φlogp (Yt, Zt, X, i) = - log p(yi = yit|y<t i, hmaski , y>t i, X). Intuitively,
we prefer to replace a symbol if it is highly incompatible with the input variable and all the other
symbols in the current sequence. (3) We encode a positional preference that does not consider the
content of intermediate sequences: φpos(i) = - log(|t - i| + ), where > 0 is a small constant
scalar to prevent log 0. This feature encodes our preference to generate from left to right if there is no
information about the input variable nor of any intermediate sequences.
Unlike the special cases of the proposed generalized model in §2, the coordinate at each generation
step is selected based on the intermediate sequences, previous coordinate sequences, and the input
variable. We mix the features using scalar coefficients αnegent, αlogp and αpos, which are selected or
estimated to maximize a target quality measure on validation set.
3.3	Optimistic decoding and beam search from a masked language model
Based on the adaptive Gibbs sampler with the coordinate selection distribution in Eq. (2), we
can now design an inference procedure to approximately find the most likely sequence from the
sequence distribution p(Y|X) by exploiting the corresponding model of sequence generation. In
doing so, a naive approach is to marginalize out the generation procedure G using a Monte Carlo
method: argmaxγP(Y|X) = argmaxγt 吉 PGm P(YTIYm,<T, Zm,≤T, X), where Gm is the m-
th sample from the sequence generation model. This approach suffers from a high variance and
non-deterministic behavior, and is less appropriate for practical use. We instead propose an optimistic
decoding approach following equation (1):
TL
argmax
L,Y1,...,YT
Z1,...,ZT
logP(LIX) + ΣΣlogP(zit+1IY≤t,Zt,X)+zit+1logP(yit+1IY≤t,X).
t=1 i=1
(3)
The proposed procedure is optimistic in that we consider a sequence generated by following the most
likely generation path to be highly likely under the sequence distribution obtained by marginalizing out
the generation path. This optimism in the criterion more readily admits a deterministic approximation
scheme such as greedy and beam search, although it is as intractable to solve this problem as the
original problem which required marginalization of the generation path.
4
Under review as a conference paper at ICLR 2020
	b	T	Baseline Autoregressive	Decoding from an undirected sequence model			
				Uniform	Left2Right	Least2Most	Easy-First
	1	L	25.33	21.01	24.27	23.08	23.73
	4	L	26.84	22.16	25.15	23.81	24.13
	4	L*	—	22.74	25.66	24.42	24.69
	1	2L	—	21.16	24.45	23.32	23.87
	4	2L	—	21.99	25.14	23.81	24.14
	1	L	29.83	26.01	28.34	28.85	29.00
	4	L	30.92	27.07	29.52	29.03	29.41
	4	L*	—	28.07	30.46	29.84	30.32
	1	2L	—	26.24	28.64	28.60	29.12
	4	2L	—	26.98	29.50	29.02	29.41
Table 1: Results (BLEU↑) on WMT'14 En什De translation using various decoding algorithms and
different settings of beam search width (b) and number of iterations (T) as a function of sentence
length (L). For each sentence we use 4 most likely sentence lengths. * denotes rescoring generated
hypotheses using autoregressive model instead of proposed model.
Length-conditioned beam search To solve this intractable optimization problem, we design a
heuristic algorithm, called length-conditioned beam search. Intuitively, given a length L, this algo-
rithm performs beam search over the coordinate and intermediate token sequences. At each step t of
this iterative algorithm, we start from the hypothesis set Ht-1 that contains K generation hypotheses:
HtT = {hk-1 = ((Y1 2,...,腔-1), (Zk,..., ZtT))}∖
Each generation hypothesis has a score:
t-1 L
s(hk-1) = logP(LX) + XXlogP(ZtlY<t0,zt0τ,x) + zi logp(yi ∖Yr≤t,X).
t0=1 i=1
For notational simplicity, we drop the time superscript t. Each of the K generation hypotheses is first
expanded with K0 candidate positions according to the coordinate selection distribution:
arg toP-K0u∈{1,…,L} s(hk) + logP(Zk,u = 1|Y<t,ztT,X)
{Z
{^^^^^^^^
=s(hk kone-hot(u))
so that We have K X K0 candidates { hk,k，}, where each candidate consists of a hypothesis hk with
the position sequence extended by the selected position uk,k0 and has a score s(hk kone-hot(uk,k0)).1
We then expand each candidate with the symbol replacement distribution:
{h k,k0,k00
oK00
k00=1
arg top-K00v∈v s(hk∣∣one-hot(uk,k0)) + logp(yzk 卜，=v|Y≤t, X).
X------------------------------------------------,---------------}
≡^^^{^^^^^^^^^^^^^^≡
= s(hk,k0 k(Y<-k,k0 ,V,Y>-1k0))
This results in K × K0 × K00 candidates {hk,k0,k00}, each consisting of hypothesis hk with inter-
mediate and coordinate sequence respectively extended by vk,k0,k00 and uk,k0. Each hypothesis has
a score s(hk,k/k(Y<-1小,vk,k',k'o,Y>-[k0 )),2 which we use to select K candidates to form a new
hypothesis set Ht = arg top-K, f^	1	s(h).
∈ k,k0,k00 k,k0,k00
After iterating for a predefined number T of steps, the algorithm terminates with the final set of K
generation hypotheses. We then choose one of them according to a prespecified criterion, such as
Eq. (3), and return the final symbol sequence YT.
1hk kone-hot(uk,k0) appends one-hot(uk,k0) at the end of the sequence of the coordinate sequences in hk
2hk,k0 k (Y^-1 k0, Vk,k0,k00, Y>-[ k0) denotes creating a new sequence from Yt-1 by replacing the Zk,k，-th
symbol with vk,k0,k00, and appending this sequence to the intermediate sequences in hk,k0.
5
Under review as a conference paper at ICLR 2020
4	Experimental Settings
Data and preprocessing We evaluate our framework on WMT’14 English-German translation. The
dataset consists of 4.5M parallel sentence pairs. We preprocess this dataset by tokenizing each
sentence using a script from Moses (Koehn et al., 2007) and then segmenting each word into subword
units using byte pair encoding (Sennrich et al., 2016) with a joint vocabulary of 60k tokens. We use
newstest-2013 and newstest-2014 as validation and test sets respectively.
Sequence models We base our models off those of Lample and Conneau (2019). Specifically, we
use a Transformer (Vaswani et al., 2017) with 1024 hidden units, 6 layers, 8 heads, and Gaussian
error linear units (Hendrycks and Gimpel, 2016). We use a pretrained model3 4 trained using a masked
language modeling objective (Lample and Conneau, 2019) on 5M monolingual sentences from WMT
NewsCrawl 2007-2008. To distinguish between English and German sentences, a special language
embedding is added as an additional input to the model.
We adapt the pretrained model to translation by finetuning it with a masked translation objective
(Lample and Conneau, 2019). We concatenate parallel English and German sentences, mask out a
subset of the tokens in either the English or German sentence, and predict the masked out tokens.
We uniformly mask out 0 - 100% tokens as in Ghazvininejad et al. (2019). Training this way more
closely matches the generation setting where the model starts with an input sequence of all masks.
Baseline model We compare against a standard Transformer encoder-decoder autoregressive neural
sequence model (Vaswani et al., 2017) trained for left-to-right generation and initialized with the
same pretrained model. We train a separate autoregressive model to translate an English sentence to a
German sentence and vice versa, with the same hyperparameters as our model.
Training details We train the models using Adam (Kingma and Ba, 2014) with an inverse square
root learning rate schedule, learning rate of 10-4, β1 = 0.9, β2 = 0.98, and dropout rate of 0.1
(Srivastava et al., 2014). Our models are trained on 8 GPUs with a batch size of 256 sentences.
Decoding strategies We design four generation strategies for the masked translation model based
on the log-linear coordinate selection distribution in §2:
1.	Uniform: τ → ∞, i.e., sample a position uniformly at random without replacement
2.	Left2Right: αnegent = 0,
αlogp = 0, αpos = 1
3.	Least2Most (Ghazvininejad et al., 2019): αnegent = 0, αlogp = 1, αpos = 0
4.	Easy-First: αnegent = 1, αlogp = 1, αpos = 0
We use beam search described in §3.3 with K0
fixed to 1, i.e., we consider only one possible po-
sition for replacing a symbol per hypothesis each
time of generation. We vary K = K00 between
1 (greedy) and 4. For each source sentence, we
consider four length candidates according to the
length distribution estimated from the training
pairs, based on early experiments showing that
using only four length candidates performs as
Gold
# of length candidates
1234
En→De	22.50	22.22	22.76	23.01	23.22
De→En	28.05	26.77	27.32	27.79	28.15
Table 2: Effect of the number of length candi-
dates considered during decoding on BLEU, mea-
sured on the validation set (newstest-2013) using
the easy-first strategy.
well as using the ground-truth length (see Table 2). Given the four candidate translations, we choose
the best one according to the pseudo log-probability of the final sequence (Wang and Cho, 2019).
Additionally, we experimented with choosing best translation according to log-probability of the final
sequence calculated by an autoregressive neural sequence model.
Decoding scenarios We consider two decoding scenarios: linear-time and constant-time decoding.
In the linear-time scenario, the number of decoding iterations T grows linearly w.r.t. the length
of a target sequence L. We test setting T to L and 2L. In the constant-time scenario, the number
of iterations is constant w.r.t. the length of a translation, i.e., T = O(1). At the t-th iteration of
generation, we replace ot-many symbols, where ot is either a constant dL/T e or linearly anneals
from L to 1 (L → 1) as done by Ghazvininejad et al. (2019).
3https://dl.fbaipublicfiles.com/XLM/mlm_ende_1024.pth
4We set αlogp = 0.9 for De→En based on the validation set performance.
6
Under review as a conference paper at ICLR 2020
Figure 1: Generation orders given by easy-first and least2most coordinate selection. We group
the orders into five clusters and visualize cluster centers with normalized positions (x-axis) over
normalized steps (y-axis). We use greedy search with L iterations on the development set.
5	Linear-Time Decoding: Result and Analysis
Main findings We present translation quality measured by BLEU (Papineni et al., 2002) in Table 1.
We identify a number of important trends. (1) The deterministic coordinate selection strategies
(left2right, least2most and easy-first) significantly outperform selecting coordinates uniformly
at random, by up to 3 BLEU in both directions. The success of these relatively simple hand-
crafted coordinate selection strategies suggest avenues for further improvement for generation from
undirected sequence models. (2) The proposed beam search algorithm for undirected sequence
models provides an improvement of about 1 BLEU over greedy search, confirming the utility of the
proposed framework as a way to move decoding techniques across different paradigms of sequence
modeling. (3) Rescoring generated translations with an autoregressive model adds about 1 BLEU
across all coordinate selection strategies. Rescoring adds minimal overhead as it is run in parallel.
(4) Different generation strategies result in translations of varying qualities depending on the setting.
On English-German translation, left2right is the best performing strategy, achieving up to 25.66
BLEU. Easy-first and left2right perform nearly the same in the other direction, achieving up to
30.46 BLEU. (5) We see little improvement in refining a sequence beyond the first pass, though we
suspect this may be due to the simplicity of the coordinate selection schemes tested. (6) Lastly, the
masked translation model lags behind the more conventional neural autoregressive model, although
the difference is within 1 BLEU point when greedy search is used with the autoregressive model and
approximately 2 BLEU with beam search. We hypothesize that a difference between train and test
settings causes a slight performance drop of masked translation model compared to conventional
autoregressive model. In the standard autoregressive case, the model is explicitly trained to generate
in left-to-right order, which matches the test time usage. By randomly selecting tokens to mask during
training, our undirected sequence model is trained to follow all possible generation orders and to use
context from both directions, which is not available when generating left-to-right at test time.
Adaptive generation order The least2most and easy-first generation strategies automatically adapt
the generation order based on the intermediate sequences generated. We investigate the resulting
generation orders on the development set by presenting each as a 10-dim vector (downsampling as
necessary), where each element corresponds to the selected position in the target sequence normalized
by sequence length. We cluster these sequences with k-means clustering and visualize the clusters
centers as curves with thickness proportional to the number of sequences in the cluster in Fig. 1.
In both strategies, we see two major trends. First, many sequences are generated largely monotonically
either left-to-right or right-to-left (see, e.g., green, blue and orange clusters in easy-first, En→De,
and blue, orange, and red clusters in least2most, De→En.) Second, another cluster of sequences
are generated from outside in, as seen in the red and purple clusters in easy-first, En→De, and
green, orange, and purple clusters in least2most, En→De. We explain these two behaviors by the
availability of contextual evidence, or lack thereof. At the beginning of generation, the only two
non-mask symbols are the beginning and end of sentence symbols, making it easier to predict a
symbol at the beginning or end of the sentence. As more symbols are filled near the boundaries,
more evidence is accumulated for the decoding strategy to accurately predict symbols near the center.
7
Under review as a conference paper at ICLR 2020
T	Ot	Uniform	Left2Right	Least2Most	Easy-First	Hard-First
10	L → 1	22.38	22.38	27.14	22.21	26.66
10	L → 1*	23.64	23.64	28.63	23.79	28.46
10	「l/t e	22.43	21.92	24.69	25.16	23.46
20	L → 1	26.01	26.01	28.54	22.24	28.32
20	L → 1*	27.28	27.28	30.13	24.55	29.82
20	「l/t e	24.69	25.94	27.01	27.49	25.56
Table 3: Constant-time machine translation on WMT’14 De→En with different settings of the budget
(T) and number of tokens predicted each iteration (ot). * denotes rescoring generated hypotheses
using autoregressive model instead of proposed model.
This process manifests either as monotonic or outside-in generation. We present sample sequences
generated using these strategies in Appendix D.
6	Constant-Time Decoding: Res ult and Analysis
The trends in constant-time decoding noticeably differ from those in linear-time decoding. First,
the left2right strategy significantly lags behind the least2most strategy, and the gap is wider (up
to 4.7 BLEU) with a tighter budget (T = 10). This gap suggests that a better, perhaps learned,
coordinate selection scheme could further improve constant-time translation. Second, the easy-first
strategy is surprisingly the worst in constant-time translation, unlike in linear-time translation. To
investigate this degradation, we test another strategy where we flip the signs of the coefficients in the
log-linear model. This new hard-first strategy works on par with least2most, which again confirms
that decoding strategies must be selected based on the target tasks and decoding setting.
With a fixed budget of T = 20, linearly annealing K , and least2most decoding, constant-time
translation can achieve translation quality comparable to linear-time translation with the same model
(30.13 vs. 30.46), and to beam-search translations using the strong neural autoregressive model
(30.13 vs 30.92). Even with a tighter budget of 10 iterations (less than half the average sentence
length), constant-time translation loses only 1.8 BLEU points (28.63 vs. 30.46), which confirms
the finding by Ghazvininejad et al. (2019) and offers new opportunities in advancing constant-time
machine translation systems. Compared to other constant-time machine translation approaches, our
model outperforms many recent approaches by Gu et al. (2018); Lee et al. (2018); Wang et al. (2019);
Ma et al. (2019), while being comparable to Ghazvininejad et al. (2019); Shu et al. (2019). We
present full results in Table 4 in Appendix A.
7	Conclusion
We present a generalized framework of neural sequence generation that unifies decoding in directed
and undirected neural sequence models. Under this framework, we separate position selection and
symbol replacement, allowing us to apply a diverse set of generation algorithms, inspired by those
for directed neural sequence models, to undirected models such as BERT and its translation variant.
We evaluate these generation strategies on WMT’14 En-De machine translation using a recently
proposed masked translation model. Our experiments reveal that undirected neural sequence models
achieve performance comparable to conventional, state-of-the-art autoregressive models, given an
appropriate choice of decoding strategy. We further show that constant-time translation in these
models performs similar to linear-time translation by using one of the proposed generation strategies.
Analysis of the generation order automatically determined by these adaptive decoding strategies
reveals that most sequences are generated either monotonically or outside-in.
We identify two promising extensions to our work. First, we could have a model learn the coordinate
selection distribution from data to maximize translation quality. Doing so would likely result in better
quality sequences as well as the discovery of more non-trivial generation orders. Second, we only
apply our framework to sequence generation, but we could also apply it to other structured data such
as grids (for e.g. images) and arbitrary graphs. Overall, we hope that our generalized framework
opens new avenues in developing and understanding generation algorithms for a variety of settings.
8
Under review as a conference paper at ICLR 2020
References
Mathias Berglund, TaPani Raiko, Mikko Honkala, Leo Karkkainen, Akos Vetek, and Juha T Karhunen.
Bidirectional recurrent neural networks as generative models. In Advances in Neural Information
Processing Systems, pages 856-864, 2015.
Julian Besag. Efficiency of Pseudolikelihood estimation for simPle gaussian fields. 1977.
Jacob Devlin, Ming-Wei Chang, and Kristina Toutanova Kenton Lee. Bert: Pre-training of deep
bidirectional transformers for language understanding. In NAACL, 2019.
Sergey Edunov, Alexei Baevski, and Michael Auli. Pre-trained language model representations for
language generation, 2019.
Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Constant-time machine
translation with conditional masked language models. arXiv preprint arXiv:1904.09324, 2019.
Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li, and Richard Socher. Non-autoregressive
neural machine translation. CoRR, abs/1711.02281, 2018.
Jiatao Gu, Qi Liu, and Kyunghyun Cho. Insertion-based decoding with automatically inferred
generation order. CoRR, abs/1902.01370, 2019.
Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian
error linear units. arXiv preprint arXiv:1606.08415,, 2016.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
1412.6980, 2014.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. Moses: Open source
toolkit for statistical machine translation. In ACL, 2007.
Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint
arXiv:1901.07291, 2019.
Jason Lee, Elman Mansimov, and Kyunghyun Cho. Deterministic non-autoregressive neural sequence
modeling by iterative refinement. arXiv preprint arXiv:1802.06901, 2018.
Richard A Levine and George Casella. Optimizing random scan gibbs samplers. Journal of
Multivariate Analysis, 97(10):2071-2100, 2006.
Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, and Eduard Hovy. Flowseq: Non-
autoregressive conditional sequence generation with generative flow. arXiv preprint 1909.02480,
2019.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting on association for
computational linguistics, pages 311-318. Association for Computational Linguistics, 2002.
Prajit Ramachandran, Peter Liu, and Quoc Le. Unsupervised pretraining for sequence to sequence
learning. Proceedings of the 2017 Conference on Empirical Methods in Natural Language
Processing, 2017. doi: 10.18653/v1/d17-1039. URL http://dx.doi.org/10.18653/v1/
d17-1039.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. In ACL, 2016.
Raphael Shu, Jason Lee, Hideki Nakayama, and Kyunghyun Cho. Latent-variable non-autoregressive
neural machine translation with deterministic inference using a delta posterior. arXiv preprint
1908.07181, 2019.
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence to sequence
pre-training for language generation, 2019.
9
Under review as a conference paper at ICLR 2020
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. 2014.
Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. Insertion transformer: Flexible
sequence generation via insertion operations. CoRR, abs/1902.03249, 2019.
Qing Sun, Stefan Lee, and Dhruv Batra. Bidirectional beam search: Forward-backward inference in
neural sequence models for fill-in-the-blank image captioning. 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕Ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.
Alex Wang and Kyunghyun Cho. Bert has a mouth, and it must speak: Bert as a markov random field
language model. arXiv preprint arXiv:1902.04094, 2019.
Chunqi Wang, Ji Zhang, and Haiqing Chen. Semi-autoregressive neural machine translation. arXiv
preprint arXiv:1808.08583, 2018.
Yiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Non-autoregressive
machine translation with auxiliary regularization. arXiv preprint 1902.10245, 2019.
Sean Welleck, Kiante Brantley, Hal Daum6, and KyUnghyUn Cho. Non-monotonic sequential text
generation. CoRR, abs/1902.02192, 2019.
10
Under review as a conference paper at ICLR 2020
Figure 2: Average difference in energy ↑ between sequences generated by selecting positions
uniformly at random versus by different algorithms, over the course of decoding.
A Comparison with other non-autoregressive neural machine
TRANSLATION APPROACHES
We present the comparison of results of our approach with other constant-time machine translation
approaches in Table 4.
B Non-monotonic neural sequence models
The proposed generalized framework subsumes recently proposed variants of non-monotonic gen-
eration (Welleck et al., 2019; Stern et al., 2019; Gu et al., 2019). Unlike the other special cases
described above, these non-monotonic generation approaches learn not only the symbol replacement
distribution but also the coordinate selection distribution, and implicitly the length distribution, from
data. Because the length of a sequence is often not decided in advance, the intermediate coordinate
sequence Zt and the coordinate selection distribution are reparameterized to work with relative
coordinates rather than absolute coordinates. We do not go into details of these recent algorithms, but
we emphasize that all these approaches are special cases of the proposed framework, which further
suggests other variants of non-monotonic generation.
C	Energy evolution over generation steps
While the results in Table 1 indicate that our decoding algorithms find better generations in terms of
BLEU relative to uniform decoding, we verify that the algorithms produce generations that are more
likely according to the model. We do so by computing the energy (negative logit) of the sequence
of intermediate sentences generated while using an algorithm, and comparing to the average energy
of intermediate sentences generated by picking positions uniformly at random. We plot this energy
difference over decoding in Figure 2. We additionally plot the evolution of energy of the sequence
by different position selection algorithms throughout generation process in Figure 3. Overall, we
find that left-to-right, least-to-most, and easy-first do find sentences that are lower energy than the
uniform baseline over the entire decoding process. Easy-first produces sentences with the lowest
energy, followed by least-to-most, and then left-to-right.
D Sample sequences and their generation orders
We present sample decoding processes using the easy-first decoding algorithm on De→En with
b = 1, T = L in Figures 4, 5, 6, and 7. We highlight examples decoding in right-to-left-to-right-
11
Under review as a conference paper at ICLR 2020
.0.5.0.5.0
LeieiO.L
(APBUB -e≡u 一 Ol) A6」① U ①①>le-①U
Energy throughout Generation Process
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
Relative iteration step
Figure 3:	Evolution of the energy of the sequence J over the course of decoding by different position
selection algorithms.
to-left order, outside-in, left-to-right, and right-to-left orders, which respectively correspond to the
orange, purple, red, and blue clusters from Figure 1, bottom left. These example demonstrate the
ability of the easy-first coordinate selection algorithm to adapt the generation order based on the
intermediate sequences generated. Even in the cases of largely monotonic generation order (left-to-
right and right-to-left), the algorithm has the capacity to make small changes to the generation order
as needed.
12
Under review as a conference paper at ICLR 2020
Models	WMT2014	
	EN-DE	DE-EN
AR Transformer-base (Vaswani et al., 2017)	27.30	—
AR (Gu et al.,2018)	23.4	—
NAR (+Distill +FT +NPD S=100)	21.61	—
AR (Lee etal., 2018)	24.57	28.47
Adaptive NAR Model	16.56	—
Adaptive NAR Model (+Distill)	21.54	25.43
AR (Wang et al., 2019)	27.3	31.29
NAT-REG (+Distill)	20.65	24.77
NAT-REG (+Distill +AR rescoring)	24.61	28.90
AR (Ghazvininejad et al., 2019)	27.74	31.09
CMLM with 4 iterations	22.25	—
CMLM with 4 iterations (+Distill)	25.94	29.90
CMLM with 10 iterations	24.61	—
CMLM with 10 iterations (+Distill)	27.03	30.53
AR (Shu etal., 2019)	26.1	—
Latent-Variable NAR	11.8	—
Latent-Variable NAR (+Distill)	22.2	—
Latent-Variable NAR (+Distill +AR Rescoring)	25.1	—
AR (Ma etal., 2019)	27.16	31.44
FlowSeq-base (+NPD n = 30)	21.15	26.04
FlowSeq-base (+Distill +NPD n= 30)	23.48	28.40
AR (ours)	26.84	30.92
Contant-time 10 iterations	21.98	27.14
Contant-time 10 iterations (+AR Rescoring)	24.53	28.63
Contant-time 20 iterations	23.92	28.54
Contant-time 20 iterations (+AR Rescoring)	25.69	30.13
Table 4: BLEU scores on WMT’14 En→De and De→En datasets showing performance of various
constant-time machine translation approaches. Each block shows the performance of autoregressive
model baseline with their proposed approach. AR denotes autoregressive model. Distill denotes
distillation. AR rescoring denotes rescoring of samples with autoregressive model. FT denotes
fertility. NPD denotes noisy parallel decoding followed by rescoring with autoregressive model.
13
Under review as a conference paper at ICLR 2020
Iteration
Right-to-Left-to-Right-to-Left
(source)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
(target)
Wurde es mir je gelingen , an der Universitat Oxford ein normales Leben Zu fuhren ?
_ _ ever _
_ I ever _
_ I ever _ _
Would I ever
Would I ever _ _
Would I ever _ _
Would I ever _ _
Would I ever _____
Would I ever _____
_______________ Oxford	?
_______________ Oxford	?
________________ Oxford	?
______________ of Oxford ?
_______________ of Oxford ?
_ _ _ normal _____ of Oxford ?
_ _ _ normal _ at _ _ of Oxford ?
_ _ _ normal _ at the _ of Oxford ?
normal _ at the University of Oxford ?
normal life at the University of Oxford ?
Would I ever _ _ _ live _ normal life at the University of Oxford ?
Would I ever _ _ _ live a normal life at the University of Oxford ?
Would I ever _ able _ live a normal life at the University of Oxford ?
Would I ever be able _ live a normal life at the University of Oxford ?
Would I ever be able to live a normal life at the University of Oxford ?
Would I ever be able to lead a normal life at Oxford ?
Figure 4:	Example sentences generated following an right-to-left-to-right-to-left generation order,
using the easy-first decoding algorithm on De→En.
Iteration
Outside-In
(source) DoCh ohne ZivilgesellsChaftliChe Organisationen konne eine Demokratie nicht funktionieren .
1		.
2		 cannot _ .
3		 democracy Cannot _ .
4	_ without 	 demoCraCy Cannot _ .
5	_ without 	 demoCraCy Cannot work .
6	But without 	 demoCraCy Cannot work .
7	But without 	 a demoCraCy Cannot work .
8	But without _ society _ _ a demoCraCy Cannot work .
9	But without _ soCiety _ , a demoCraCy Cannot work .
10	But without civil soCiety _ , a demoCraCy Cannot work .
11	But without Civil soCiety organisations , a demoCraCy Cannot work .
(target)	Yet without Civil soCiety organisations , a demoCraCy Cannot funCtion .
Figure 5:	Example sentenCes generated following an outside-in generation order, using the easy-first
deCoding algorithm on De→En.
Iteration	Left-to-Right
(sourCe)	Denken Sie , dass die Medien Zu viel vom PSG erwarten ?
1		?
2	Do 	 ?
3	Do you 	 ?
4	Do you think 	 ?
5	Do you think 	 PS _ ?
6	Do you think 	 PS @G ?
7	Do you think _ media 	 PS @G ?
8	Do you think the media 	 PS @G ?
9	Do you think the media expect _ _ _ PS @G ?
10	Do you think the media expeCt _ much _ PS @G ?
11	Do you think the media expeCt too muCh _ PS @G ?
12	Do you think the media expeCt too muCh of PS @G ?
(target)	Do you think the media expeCt too muCh of PS @G ?
Figure 6:	Example sentenCes generated following an left-to-right generation order, using the easy-first
deCoding algorithm on De→En.
14
Under review as a conference paper at ICLR 2020
Right-to-Left
Iteration
(source)	Ein weiterer Streitpunkt : die Befugnisse der Armee .
1		.
2		 army .
3		 of _ army .
4		 of the army .
5		 powers of the army .
6		 the powers of the army .
7		 : the powers of the army .
8	_ _ point : the powers of the army .
9	_ contentious point : the powers of the army .
10	Another contentious point : the powers of the army .
(target)	Another issue : the powers conferred on the army .
Figure 7:	Example sentences generated following an right-to-left generation order, using the easy-first
decoding algorithm on De→En.
15