Under review as a conference paper at ICLR 2020

ON  WEIGHT-SHARING  AND  BILEVEL  OPTIMIZATION
IN  ARCHITECTURE  SEARCH

Anonymous authors

Paper under double-blind review

ABSTRACT

Weight-sharing—the simultaneous optimization of multiple neural networks using
the same parameters—has emerged as a key component of state-of-the-art neural
architecture search.  However, its success is poorly understood and often found
to        be surprising.  We argue that, rather than just being an optimization trick, the
weight-sharing approach is induced by the relaxation of a structured hypothesis
space, and introduces new algorithmic and theoretical challenges as well as appli-
cations beyond neural architecture search. Algorithmically, we show how the ge-
ometry of ERM for weight-sharing requires greater care when designing gradient-
based minimization methods and apply tools from non-convex non-Euclidean op-
timization to give general-purpose algorithms that adapt to the underlying struc-
ture. We further analyze the learning-theoretic behavior of the bilevel optimization
solved by practical weight-sharing methods. Next, using kernel configuration and
NLP feature selection as case studies, we demonstrate how weight-sharing applies
to the architecture search generalization of NAS and effectively optimizes the re-
sulting bilevel objective.  Finally, we use our optimization analysis to develop a
simple exponentiated gradient method for NAS that aligns with the underlying
optimization geometry and matches state-of-the-art approaches on CIFAR-10.

1    INTRODUCTION

Weight-sharing  neural  architecture  search  (NAS)  methods  have  achieved  state-of-the-art  
perfor-
mance while requiring computation training of just a single shared-weights network (Pham et al.,
2018; Li and Talwalkar, 2019; Liu et al., 2019).  However, weight-sharing remains poorly under-
stood.  In this work, we present a novel perspective on weight-sharing NAS motivated by the key
observation that these methods subsume the architecture hyperparameters as another set of learned
parameters of the shared-weights network, in effect extending the hypothesis class.  An important
ramification of this insight is that weight-sharing is not NAS-specific and can be used to tune hy-
perparameters corresponding to parameterized feature maps of the input data.  We refer this larger
subset       of hyperparameter optimization problems as architecture search, and we study the 
following
two questions associated with weight-sharing applied to the architecture search problem:

1. How can we efficiently optimize the objective induced by applying weight sharing to architecture
search, namely minimizing empirical risk in the joint space of model and architecture parameters?
For large structured search spaces that preclude brute force search, a natural approach to 
architecture
search with weight-sharing is to use gradient-based methods to minimize the empirical risk over a
continuous relaxation of the discrete space (Liu et al., 2019).  Although this has allowed NAS re-
searchers to apply their preferred optimizers to determine architecture weights, it is far from 
clear
that  the success of established methods for unconstrained optimization in training neural networks
will naturally extend to these constrained and often non-Euclidean environments.  As we foresee
that architecture search spaces will continue to become more complex and multi-faceted,  we ar-
gue for and develop a more principled, geometry-aware formulation of the optimization problem.
Drawing upon the mirror descent meta-algorithm (Beck and Teboulle, 2003) and successive convex
approximation, we give non-asymptotic stationary-point convergence guarantees for the empirical
risk minimization (ERM) objective associated with weight-sharing via algorithms that simultane-
ously connect to the underlying problem structure and handle the alternating-block nature of the
architecture search. Our guarantees inform the design of gradient-based weight-sharing methods by
explicitly quantifying the impact of optimizing in the right geometry on convergence rates.

1


Under review as a conference paper at ICLR 2020

2.  What are the generalization benefits of solving a bilevel optimization for the architecture 
search
problem commonly considered in practice?  At its core, the goal of architecture search is to find a
configuration that achieves good generalization performance. Consequently, a bilevel objective that
optimizes the architecture weights using a separate validation loss is commonly used in practice in
lieu    of the ERM objective naturally induced by weight sharing (Pham et al., 2018; Liu et al., 
2019;
Cai et al., 2019). The learning aspects of this approach have generally been studied in settings 
with
much stronger control over the model complexity (Kearns et al., 1997).   We provide generaliza-
tion guarantees for this objective over structured hypothesis spaces associated with a finite set of
architectures; this leads to meaningful bounds for simple feature map selection problems as well as
insightful results for the NAS problem that depend on the size of the space of global optima.

To validate our theoretical results, we conduct empirical studies of weight-sharing in two 
settings:

(1) shallow feature map selection, i.e., tuning the hyperparameters of kernel classification and NLP
featurization pipelines, and (2) CNN neural architecture search. In (1) we demonstrate that weight-
sharing efficiently optimizes the bilevel objective and achieves low generalization error with 
respect
to the best architecture setting.  For (2), motivated by insights from our convergence analysis, we
develop a simple exponentiated gradient version of DARTS (Liu et al., 2019) called EDARTS that
better  exploits  the  geometry  of  the  optimization  problem.   We  evaluate  EDARTS  on  the  
design
of CNN architectures for CIFAR-10 and demonstrate that EDARTS finds better architectures than
DARTS in less than half the time. We also achieve very competitive results relative to 
state-of-the-art
architectures when using an extended evaluation routine.

Related Work:  Our work on optimization for weight-sharing benefits from the literature on first-
order stochastic optimization (Hazan and Kale, 2014; Beck, 2017) and in particular the mirror de-
scent framework (Beck and Teboulle, 2003).  Specifically,  we use successive convex approxima-
tion (Razaviyayn et al., 2013; Mairal, 2015) to show convergence of alternating minimization and
derive geometry-dependent rates comparable to existing work on non-convex stochastic mirror de-
scent (Dang and Lan, 2015; Zhang and He, 2018).  Our result generalizes to the constrained, non-
Euclidean, and multi-block setting an approach of Agarwal et al. (2019) for obtaining non-convex
convergence from strongly convex minimization, which may be of independent interest.  Previous
optimization results for NAS have generally only shown bounds on auxiliary quantities such as re-
gret     that are not well-connected to the learning objective (Noy et al., 2019; Nayman et al., 
2019;
Carlucci et al., 2019) or have only given monotonic improvement or asymptotic guarantees (Aki-
moto et al., 2019; Yao et al., 2019). However, due to the generality of mirror descent, the 
approaches
in the middle three papers can be seen as special cases of our analysis.  Finally, our analysis of 
the
properties of the bilevel optimization is related to work on model selection (Vuong, 1989; Kearns
et     al., 1997), but does not consider the configuration parameters as explicit controls on the 
model
complexity. Our learning results are broadly related to hyperparameter optimization, although most
work focuses on algorithmic and not statistical questions (Li et al., 2018; Kandasamy et al., 
2017).

2    THE  WEIGHT-SHARING  LEARNING  PROBLEM

In this section, we formalize the weight-sharing learning problem, relate it to traditional ERM, and
provide examples for the case of NAS and feature map selection that we use for the rest of the
paper.  Our main observation is that weight-sharing for architecture search extends the hypothesis
space to be further parameterized by a finite set of configurations C. Formally, we have a 
structured

hypothesis space H(C, W)  =  {h     :  X  ›→  Y  :  w ∈  W, c ∈  C} over input space Z  =  X × Y

(c)

with induced hypothesis subclasses Hc =  {h     :  X  ›→  Y  :  w ∈  W} parameterized by weights

from W.  The goal of learning is the usual one—find h⁽ᶜ⁾  ∈  H(C, W) with low population error


lD(h⁽ᶜ⁾) = E

(x,y)∼D

l(h⁽ᶜ⁾(x), y) for loss l : Y  × Y  ›→  R.  Hence, we can apply ERM as usual,

with optional regularization, to select a hypothesis from the extended hypothesis space; in fact 
this
is done by some NAS methods (e.g., Xie et al., 2019). The learning algorithm is then


min

L   (h⁽ᶜ⁾) =     min

  1      Σ

l(h⁽ᶜ⁾(x), y) + R

(w) + R

(c)         (1)

for block specific regularizers RW  and RC.  Note that in the absence of weight-sharing, we would

need to learn a separate hypothesis h⁽ᶜ⁾  for each hypothesis subclass H .  Although a brute force

wc                                                                                        c

approach to selecting a hypothesis from H(C, W) via ERM would in effect require this as well,
2


Under review as a conference paper at ICLR 2020

our subsequent examples demonstrate how the weight-sharing construct allows us to apply more
efficient gradient-based optimization approaches, which we study in Section 3.

Feature Map Selection: In this setting, the structure is induced by a set of feature 
transformations

=   φi : X      Rn for i = 1, . . . , k  , so the hypothesis space is   fw(φi( )) : w        , φi   
      for
some          Rd. Examples of feature map selection problems include tuning kernel hyperparameters
for kernel ridge classification and tuning NLP featurization pipelines for text classification. In 
these
cases fw is a linear mapping fw(·) = ⟨w, ·⟩ and W  ⊂ Rn.

Neural  Architecture  Search:  Weight-sharing  methods  almost  exclusively  use  micro  cell-based
search spaces for their tractability and additional structure (Pham et al., 2018; Liu et al., 2019).
These search spaces can be represented as directed acyclic graphs (DAGs) with a set of ordered
nodes N and edges E. Each node x⁽ⁱ⁾ in the DAG is a feature representation and each edge o⁽ⁱ,ʲ⁾ is
an operation on the feature of node j passed to node i and aggregated with other inputs to form 
x⁽ʲ⁾,
with the restriction that a given node j can only receive edges from prior nodes as input. Hence, 
the
feature at a given node i is x⁽ⁱ⁾ =     j<i o⁽ⁱ,ʲ⁾(x⁽ʲ⁾). Search spaces are then specified by the 
number

of nodes, the number of edges per node, and the set of operations O that can be applied at each 
edge.

In this case the structure C  ⊂ {0, 1}|E||O|  of the hypothesis space is the set of all valid 
architectures
for this DAG encoded by edge and operation decisions.  Treating both weights w ∈  W  and archi-
tecture decision c ∈ C as parameters, weight-sharing methods train a single shared-weights network

h⁽ᶜ⁾  : X      Y  encompassing all possible functions within the search space. Therefore, the 
shared-
weights network includes all possible edges between nodes and all possible operations per edges. In
addition to the weights w ∈ W corresponding to all the operations, the shared-weights network also

takes architecture weights c ∈  C  as input, where c⁽ⁱ,ʲ⁾ indicates the weight given to operation o 
on


edge (i, j) so that the feature of a given node i is x⁽ⁱ⁾ = Σ

j<i

Σo∈O

c(i,j)o(i,j)(x(j)).

Gradient-based  weight-sharing  methods  apply  continuous  relaxations  to  the  architecture  
search
space in order to compute gradients.  Some methods like DARTS (Liu et al., 2019) and its vari-
ants (Chen et al., 2019; Laube and Zell, 2019; Hundt et al., 2019; Liang et al., 2019; Noy et al.,
2019; Nayman et al., 2019) relax the search space by considering a mixture of operations per edge
and then discretize to a valid architecture in the search space.  With the mixture relaxation, we 
re-

place all c ∈ {0, 1}|E||O|  in the above expressions by continuous counterparts θ ∈ [0, 1]|E||O|, 
with


the constraint that Σ

o∈O

θ⁽ⁱ,ʲ⁾  = 1, i.e., the architecture weights for operations on each edge sum

to 1.  Other methods like SNAS (Xie et al., 2019), ASNG-NAS (Akimoto et al., 2019), and Prox-
ylessNAS (Cai et al., 2019) assume a parameterized distribution pθ from which architectures are
sampled. By substituting continuous parameters θ    Θ in for discrete parameters c      , we are 
able
to use gradient-based methods to optimize (1).  We address the question of how to effectively use
gradient optimization for weight-sharing in the next section.

3    GEOMETRY-AWARE  OPTIMIZATION  WITH  WEIGHT-SHARING

While continuous relaxation enables state-of-the-art results, architecture search remains expensive
and noisy, with state-of-the-art mixture methods requiring second-order computations (Liu et al.,
2019) and probabilistic methods suffering from high variance policy gradients (Xie et al., 2019).
Moreover, while the use of SGD to optimize network weights is a well-tested approach, architecture
weights typically lie in constrained, non-Euclidean geometries in which other algorithms may be
more appropriate.  Recognizing this, several efforts have attempted to derive a better optimization
schemes; however, the associated guarantees for most of them hold for auxiliary objectives, such
as regret of local configuration decisions, that are not connected to the optimization objective (1)
and ignore the two-block nature of the problem (Noy et al., 2019; Nayman et al., 2019; Carlucci
et al., 2019). While Akimoto et al. (2019) do consider an alternating descent method for the 
training
objective, their results are asymptotic and certainly do not indicate any finite-time convergence.

In this section we address these difficulties by showing that the mirror descent (MD) framework
(Beck and Teboulle, 2003) is the right tool for designing algorithms in the block optimization prob-
lems that occur in architecture search.  We describe how such geometry-aware gradient algorithms
lead to faster stationary-point convergence; as we will show in Section 5, this yields simple, 
princi-
pled, and effective algorithms for large-scale multi-geometry problems such as NAS.

3


Under review as a conference paper at ICLR 2020

Algorithm 1:  Two geometry-aware optimization algorithms for multi-block optimization for a

β-strongly-smooth function over X  =×b      X   given associated DGFs ω  : X   ›→ R.

i                                                 i         i

Input: initialization x⁽¹⁾ ∈ X, batch-size N ≥ 1 (for SBMD), tolerance ε > 0 (for ASCA)

for iteration t ∈ [T ] do

sample    i ∼ Unif[b]    and set    x⁽ᵗ⁺¹⁾ ← x⁽ᵗ⁾


if SBMD (Dang and Lan, 2015)

i                      i

then


x⁽ᵗ⁺¹⁾ ← arg min

⟨g¯, u⟩ + βDω (u||x   )    for

g¯ =   ¹  ΣN

gi(x(t), ζtj )


x(t+1)  ← xˆ

s.t.    Efˆ(xˆ)−minu∈X

fˆ(u) ≤ ε   for

fˆ(·) = f (·, x⁽ᵗ⁾)+2βDω (·||x⁽ᵗ⁾)


i                                                                              i

Output: x ∼ Unif{x⁽ᵗ⁾}T    .

−i                  i               i

Problem Geometry: We relax optimization problems of the form (1) to the problem of minimizing

a function f : X  ›→ R over a convex product space X  =×b      X   consisting of blocks i, each 
with

i

an associated norm ǁ · ǁ₍i₎. For example, in typical NAS we can set X₁ to be the set-product of |E|
simplices over the operations O and the associated norm to be ǁ · ǁ₁ while X₂  =  W  ⊂  Rd  is the
space of network weights and the associated norm is ǁ · ǁ₂.  To each block i we further associate
a distance-generating function (DGF) ωi  :  X   ›→  R that is 1-strongly-convex w.r.t.  ǁ · ǁ₍i₎; 
each

is associated to a Bregman divergence Dωi (x||y) = ωi(x) − ωi(y) − ⟨∇ωi(y), x − y⟩, a standard
measure of distance (Bregman, 1967).  For example, in the Euclidean case using ω₂(·)  =  ¹ ǁ · ǁ2

yields the usual squared Euclidean distance; over the probability simplex we often use the entropy

ω₁(·) = ⟨·, log(·)⟩, which is 1-strongly-convex w.r.t. ǁ · ǁ₁ and for which Dω1   is the 
KL-divergence.
Given an unbiased gradient estimate g(xt, ζ) = Eζ∇f (xt), the (single-block) stochastic MD step is


xt₊₁ = arg min

u∈X

η⟨g(xt, ζ), u⟩ + Dω(u||xt)                                     (2)

for some learning rate η > 0. In the Euclidean setting this reduces to SGD, while with the entropic
regularizer the iteration becomes equivalent to exponentiated gradient.  Key to the guarantees for
MD      is the fact that the dual norm of the problem geometry is used to measure the second moment
of  the  gradient;  if  every  coordinate  of  g is  bounded  a.s.   by  σ,  then  under  Euclidean 
 geometry

the dependence is on Eǁg(x)ǁ2      =  Eǁg(x)ǁ2   ≤  σ²d while using entropic regularization it is 
on


E          2                                 2

2,∗                                 2

g(x)  1,    = E  g(x)           σ².  Thus in such constrained l₁ geometries mirror descent can yield
dimension-free convergence guarantees.  While in this paper we focus on the benefit in this simple
case,  the  MD  meta-algorithm  can  be  used  for  many  other  geometries  of  interest  in  
architecture
search, such as for optimization over positive-definite matrices (Tsuda et al., 2005).

Algorithms and Guarantees:  We propose two methods for the above multi-block optimization
problems:  stochastic block mirror descent (SBMD) and alternating successive convex approxima-
tion (ASCA). At each step, both schemes pick a random coordinate i to update; SBMD then per-
forms a mirror descent update similar to (2) but with a batched gradient, while ASCA optimizes
a strongly-convex surrogate function using a user-specified solver.  Note that both methods require
that f is β-strongly-smooth w.r.t.  each block’s norm        ₍i₎ to achieve convergence guarantees, 
a
standard assumption in stochastic non-convex optimization (Dang and Lan, 2015; Agarwal et al.,
2019).  This condition holds for the architecture search under certain limitations, such as a 
restric-
tion to smooth activations.  In the supplement we also show that in the single-block case ASCA
converges under the more general relative-weak-convexity criterion (Zhang and He, 2018).

We first discuss SBMD, for which non-convex convergence guarantees were shown by Dang and
Lan (2015); this algorithm is the one we implement in Section 5 for NAS. A first issue is how to
measure stationarity in constrained, non-Euclidean geometries.  In the single-block setting we can
set    a smoothness-dependent constant λ  >  0 and measure how far the proximal gradient opera-
tor prox∇λ(x)  =  arg minu∈X  λ⟨∇f (x), u⟩ + Dω(u||x) is from a fixed point,  which yields the
projected gradient measure ǁGλ(x)ǁ2   =  ˣ−ᵖʳᵒˣ∇λ(x) .   Notably,  in the unconstrained Euclidean

case this yields the standard stationarity measure ǁ∇f (x)ǁ2.  For the multi-block case we replace

Dω(u||x)  =  Σb      Dω(ui||xi) and use the pseudo-norm ǁxǁ2   =  Σb      ǁxiǁ2   .  Then x ∈  X  
is

4


Under review as a conference paper at ICLR 2020

an ε-stationary point of f w.r.t.  the projected gradient if      λ(x)        ε.  In the 
non-Euclidean case
the norm of the projected gradient measures how far we are from satisfying a first-order optimality
condition, namely how far the negative gradient is from being in the normal cone of f to the set
(Dang   and Lan, 2015, Proposition 4.1). For this condition, Algorithm 1 has the following 
guarantee:


E  ǁg (x, ζ)ǁ2

≤  G² ∀ i ∈  [b] then SBMD needs O

βFb²  Σb

G²Σ oracle calls to reach an

ε-stationary-point x ∈ X  as measured by ǁG 1  (x)ǁ.

We next provide a guarantee for ASCA in the form of a reduction to strongly-convex optimization
algorithms.  This is accomplished by the construction of the solving of a surrogate function at each
iteration, which in the Euclidean case is effectively adding weight-decay. ASCA is useful in the 
case
when efficient strongly-convex solvers are available for some or all of the blocks; for example, 
this is
frequently the case for feature map selection problems such as our kernel approximation examples,
which employ l₂-regularized ERM in the inner optimization. Taking inspiration from Zhang and He
(2018), for ASCA we analyze a stronger notion of stationarity that upper-bounds ǁGλ(x)ǁ2, which


we term the projected stationarity measure: ∆λ

(x) =  Dω(x||x⁺)+Dω(x⁺||x)  for x+  = prox∇

(x).

Note that this is not the same measure used by Zhang and He (2018), although in the appendix we
show that in the single-block case our result holds for their notion also.

Theorem 3.2.  If f is β-strongly-smooth and F  = f (x⁽¹⁾) − mi.nu∈X fΣ(u) then after T iterations

		

tolerance and the expectation is over the randomness of the algorithm and associated oracles.

Proof Summary.  The proof generalizes a result of Agarwal et al. (2019) and is in Appendix A.

Thus if we have solvers that return approximate optima of strongly-convex functions on each block
then we can converge to a stationary point of the original function. The convergence rate will 
depend
on the solver used; for concreteness we give a specification for stochastic mirror descent.

Corollary 3.1.  Under the conditions of Theorem 3.1, if on i ASCA uses the Epoch-GD method of

Hazan and Kale (2014) with divergence D    then O . βFb2   Σb      G² + β²L²  Σ oracle calls 
suffice

to reach an ε-stationary-point x ∈ X  as measured by E.∆ 1   (x) ≥ EǁG 1   (x)ǁ, where Lωi  is the


Lipschitz constant of ωi w.r.t. ǁ · ǁi.

4β                                4β

This oracle complexity matches that of Theorem 3.1 apart from the extra β²L²   term due to the

surrogate  function,  which  we  show  below  is  in  practice  a  minor  term  that  does  not  
obviate  the
benefit of geometry-awareness. On the other hand, ASCA is much more general, allowing for many
different  algorithms  on  individual  blocks;  for  example,  many  popular  neural  optimizers  
such  as
Adam (Kingma and Ba, 2015) have variants with strongly-convex guarantees (Wang et al., 2019).

The Benefit of Geometry-Aware Optimization: We conclude this section with a formalization of
how convergence of gradient-based architecture-search algorithms can benefit from this optimization
strategy.  Recalling from our example, we have the architecture parameter space    ₁  consisting of
E  simplices over  O  variables equipped with the 1-norm and the shared weight space    ₂  =
equipped with the Euclidean norm.  We suppose that the stochastic gradients along each block i
have coordinates bounded a.s.  by σi > 0.  Then if we run SBMD using SGD (l₂) to optimize the
shared weights and exponentiated gradient (l₁) to update the architecture parameters, Theorem 3.1
implies that we reach an ε-stationary point in O    σ2+σ2d     stochastic gradient computations.  
The

main benefit here is that the first term in the numerator is not σ² E  O , which would be the case 
if
we used SGD; this improvement is critical as the noise σ² of the architecture gradient can be very
high, especially if a policy gradient is used to estimate probabilistic derivatives.

In the case of ASCA, we can get similar guarantees assuming the probabilities for each operation
are lower-bounded by some small δ  > 0 and that the space of shared weights is bounded by B;
then the guarantee will be as above except with an additional    (log ¹ + B²) term (independent of
σ₁, σ₂). While for both SBMD and ASCA the σ²d term from training the architecture remains, this
will be incurred even in single-architecture training using SGD. Furthermore, in the case of ASCA
it may be improved using adaptive algorithms (Agarwal et al., 2019; Wang et al., 2019).

5


Under review as a conference paper at ICLR 2020

4    GENERALIZATION  PROPERTIES  OF  THE  BILEVEL  FORMULATION

In Section 2, we described the weight-sharing hypothesis class H(  ,    ) as a set of functions non-
disjointly partitioned by a set of configurations    sharing weights in      and posed the ERM 
problem
associated with selecting a hypothesis from H(  ,    ). However, as mentioned in Section 1, the ob-
jective solved in practice is a bilevel problem where a separate validation set is used for 
architecture
parameter updates. Formally, the bilevel optimization problem considered is


min

w∈W,c∈C

(c)

w

w'∈W

LT (w′, c)                           (3)

where T, V  ⊂  Z is a pair of training/validation sets sampled i.i.d.   from D,  the upper 
objective

l  (h⁽ᶜ⁾) =    ¹  Σ           l(h⁽ᶜ⁾(x), y) is the empirical risk over V , and L   (w, c) is some 
objective

induced by T . We intentionally differentiate the two losses since training is often regularized.

This setup is closely related to the well-studied problems of model selection and cross-validation.
However, a key difference is that the choice of configuration c       does not necessarily provide 
any
control over the complexity of the hypothesis space; for example, in NAS as it is often unclear how
the hypothesis space changes due to the change in one decision.  By contrast, the theory of model
selection  is  often  directly  concerned  with  control  of  model  complexity.   Indeed,  in  
possibly  the
most common setting the hypothesis classes are nested according to some total order of increasing
complexity, forming a structure (Vapnik, 1982; Kearns et al., 1997).  This is for example the case
in     most norm-based regularization schemes.  Even in the non-nested case, there is often an 
explicit
tradeoff between parsimony and accuracy (Vuong, 1989).

With the configuration parameters in architecture search behaving more like regular model param-
eters rather than as controls on the model complexity, it becomes reasonable to wonder why most
NAS practitioners have used the bilevel formulation.  Does the training-validation split exploit the
partitioning of the hypothesis space H(  ,    ) induced by the configurations    ?  To see when this
might   be true, we first note that a key aspect of the optima of the bilevel weight-sharing 
problem


is the restriction on the model weights - that they must be in the set arg min

inner objective

w∈W

LT (h⁽ᶜ⁾) of the

T . As we will see, under certain assumptions this can reduce the complexity of the
hypothesis space without harming performance.


First,  for any sample T  ⊂  Z let H (T )  =  {h⁽ᶜ⁾

:  w  ∈  arg min

L   (h⁽ᶜ⁾)}  be the version


space

c                     w                                  w∈W      T     w

(Kearns et al., 1997, Equation 6) induced by some configuration c and the objective function.
Second, let N (F, ε) be the L∞-covering-number of a set of functions F  at scale ε > 0, i.e.  the
number  of  L∞  balls  required  to  construct  an  ε-cover  of  F  (Mohri  et  al.,  2012,  
Equation  3.60).
These two quantities let us define a complexity measure over the shared weight hypothesis space:

Definition 4.1.  The .vSersion entropyΣ of H(C, W) at scale ε > 0 induced by the objective LT  is

The version entropy is a data-dependent quantification of how much the hypothesis class is 
restricted
by the inner optimization. For finite   , a naive bound shows that Λ(H, ε, T ) is bounded by log  C 
 +
maxc    log N (Hc(T ), ε), so that the second term measures the worst-case complexity of the global
minimizers of    T .  In the feature selection problem,    T  is usually a strongly-convex loss due 
to
regularization and so all version spaces are singleton sets, making the version entropy log     . 
In the
other extreme case of nested model selection the version entropy reduces to the complexity of the
version space of the largest model and so may not be informative.  However, in practical problems
such as NAS an inductive bias is often imposed via constraints on the number of input edges.

To bound the excess risk in terms of the version entropy, we first discuss an important assumption
that describes cases when we expect the shared weights approach to perform well:


Assumption 4.1.  There exists a good c∗  ∈ C, i.e.  one satisfying (w∗, c∗) ∈ arg min

l  (h⁽ᶜ⁾)


for some    ∗

, such that w.h.p.  over the drawing of training set

m            W×C    D       w


w   ∈  W

T  ∼  D

T   at least one of the

minima of the optimization induced by c∗  and T  has low excess risk, i.e.  w.p.  1 − δ there 
exists

(c∗)                      (c∗)

w ∈ arg min  '        L   (w′, c∗) s.t. lD(h     ) − lD(h     ) ≤ ε∗   (mT , δ) for for excess risk 
ε∗   .

This assumption requires that w.h.p.  the inner optimization objective does not exclude all low-risk
classifiers for the optimal configuration. Note that it asks nothing of either the other 
configurations
in      C, which may be arbitrarily bad, nor of the hypotheses found by the procedure.  It does 
however

6


Under review as a conference paper at ICLR 2020

prevent the case where one knows the optimal configuration but minimizing the provided objective
T does not provide a set of good weights. Note that if the inner optimization is simply ERM over
the training set T , i.e.    T  = lT , then standard learning-theoretic guarantees will give 
εexc(mT , δ)
decreasing in mT  and increasing at most poly-logarithmically in  ¹ .  With this assumption, we can

show the following guarantee on solutions to the bilevel optimization.

Theorem 4.1.  Let hˆ be a hypothesis corresponding to the solution of the bilevel optimization (3).
Then under Assumption 4.1 if l is B-bounded we have w.p. 1 − 3δ that

l  (hˆ) ≤ min l  (h) + ε∗   (m  , δ) + inf 3ε +       .       .Λ(H, ε, T ) + log    Σ

3B      2                                   1

	 						

Proof Sketch.  We decompose the excess risk lD(hˆ) − lD(hᶜ∗  ) as

l  (hˆ) − lV (hˆ)   +   lV (hˆ) − lV (h⁽ᶜ∗ ))   +   lV (h⁽ᶜ∗ )) − lD(h⁽ᶜ∗ ))   +   lD(h⁽ᶜ∗ )) − 
lD(hᶜ∗  )

The first difference is bounded by the version entropy usng the constraint on hˆ     Hc, the second 
by
optimality of hˆ on V , the third by Hoeffding’s inequality, and the last by Assumption 4.1.

As shown in the applications below,  the significance of this theorem is that a bound on the ver-
sion entropy guarantees excess risk almost as good as that of the (unknown) optimal configuration
without assuming anything about the complexity or behavior of sub-optimal configurations.

Feature Map and Kernel Selection: In the feature map selection problem introduced in Section 2,

=   φi : X     Rn for i    [k]   is a set of feature maps and the inner problem    T is 
l₂-regularized
ERM for linear classification over the resulting feature vectors. The bilevel problem is then


min

c∈C

lV (h⁽ᶜ⁾)        s.t.        w = arg min

w'∈W

2

2

(x,y)∈T

l(⟨w′, φi(x)⟩, y)           (4)

Due to strong-convexity of    T , each map φi induces a unique minimizing weight w        and thus a
singleton version space, therefore upper bounding the version entropy by log  C  = log N . Further-
more, for Lipschitz losses and appropriate choice of regularization coefficient, standard results 
for
l₂-regularized ERM for linear classification (e.g. Sridharan et al. (2008)) show that Assumption 
4.1

is satisfied with ε∗   (m  , δ) = O .. ǁw∗ ǁ2+1  log ¹ Σ. Then applying Theorem 4.1 yields


exc       T

mT              δ


Corollary 4.1.  For feature map selection the bilevel optimization (4) with λ = .  1

log ¹  yields

l  (h) ≤ min         min         l  (h     ) + O ..        +1  log    + .      log    Σ.

(φi)                           ǁw∗ ǁ2                        1                  1              N

		

In the special case of kernel selection using random Fourier approximation, we can apply associated
generalization guarantees (Rudi and Rosasco, 2017, Theorem 1) to show that we can compete with
the   optimal RKHS from among those associated with one of the configurations :

Corollary 4.2.  In feature map selection suppose each map φ         is associated with a random
Fourier feature approximation of a continuous shift-invariant kernel that approximates an RKHS

Hφ and l is the square loss.  If the number of features d = O(√mT log mT /δ) and λ = 1/√mT

then w.p. 1 − δ we have l  (h) ≤ min       min         l  (h) + O . log2   1    + .    1     log N 
Σ.

In both cases we are able to get risk bounds almost identical to the excess risk achievable if we 
knew
the optimal configuration beforehand, up to an additional capacity term depending weakly on the
number of configurations.  This would not be possible with solving the regular ERM objective in-
stead of the bilevel optimization as we would then have to contend with the possibly high complexity
of the hypothesis space induced by the worst configuration.

Neural Architecture Search:  In the case of NAS we do not have a bound on the version entropy,
which now depends on all of    .  Whether the version space, and thus the complexity, of deep net-
works is small compared to the number of samples is unclear, although we gather some evidence.
The question amounts to how many (functional) global optima are induced by a training set of size
mT .  In an idealized spin-glass model, Choromanska et al. (2015, Theorem 11.1) suggest that the

7


Under review as a conference paper at ICLR 2020

Figure 1: Validation accuracy
on    CIFAR-10    (left)    and
IMDB (right) of feature map
selection with weight-sharing
compared   to   a   full   sweep
of    random    configurations.
Average over 16 seeds.

Figure 2:     Search   time   on
CIFAR-10  (left)  and  IMDB
(right) for feature map selec-
tion.      Weight-sharing  finds
a  configuration  with  almost
the   same   validation   accu-
racy much faster than random
search.

number of critical points is exponential only in the number of layers, which would yield a small
version entropy. It is conceivable that the quantity may be further bounded by the complexity of so-
lutions explored by the algorithm when optimizing    T (Nagarajan and Kolter, 2017; Bartlett et al.,
2017); indeed, we find that shared-weight optimization leads to models with smaller l₂-norm and
distance from initialization than from-scratch SGD on a single network (see Appendix D.4). On the
other hand, Nagarajan and Kolter (2019) argue, with evidence in restricted settings, that even the
most stringent implicit regularization cannot lead to a non-vacuous uniform convergence bound; if
true more generally this would imply that the NAS version entropy is quite large.

5    EMPIRICAL  RESULTS

Here we demonstrate how weight-sharing can be used as a tool to speed up general architecture
search problems by applying it to two feature map selection problems.  We then validate our opti-
mization analysis with a geometry-aware weight-sharing method to design CNN cells for CIFAR-10.

Feature Map Selection: Recall that here our configuration space has k feature maps φi : X     Rn
with  outputs  passed  to  a  linear  classifier  w      Rn,  which  will  be  the  shared  
weights.   We  will
approximate  the  bilevel  optimization  (3)  with  the  inner  minimization  over  l₂-regularized  
ERM


λǁwǁ2  + Σ

(x,y)∈T

l(⟨w, φi(x)⟩, y). Our weight-sharing procedure starts with a vector θ⁽¹⁾ ∈ ∆N

encoding a probability distribution pθ(1)   over [N ] and proceeds as follows:

1.  For each point (x, y) ∈ T pick i ∼ pθ(t)   and construct featurization fₓ = φi(x).


2.  Compute shared weights w⁽ᵗ⁾ = arg minw∈W        λǁwǁ2  + Σ

(x,y)

∈T l(⟨w, fₓ⟩, y).


3.  Update θ⁽ᵗ⁺¹⁾ according to (an estimate of) its validation loss Σ

(x,y)∈V

l(⟨w⁽ᵗ⁾, φi(x)⟩, y).

Observe the equivalence to probabilistic NAS: at each step the classifier (shared parameter) is up-
dated using random feature maps (architectures) on the training samples. The distribution over them
is then updated using estimated validation performance.  We consider two schemes for this update
of θ⁽ᵗ⁾: (1) exponentiated gradient using the score-function estimate and (2) successive 
elimination,
where we remove a fraction of the feature maps that perform poorly on validation and reassign their
probability among the remainder.  (1) may be viewed as a softer version of (2), with halving also
having only one hyperparameter (elimination rate) and not two (learning rate, stopping criterion).

The first problem we consider is kernel ridge regression over random Fourier features (Rahimi and
Recht, 2008) on CIFAR-10.  We consider three configuration decisions: data preprocessing, choice
of kernel, and bandwidth parameter.  This problem was considered by Li et al. (2018), except they
fixed the Gaussian kernel whereas we also consider Laplacian; however, they also select the regu-
larization parameter λ, which weight-sharing does not handle. We also study logistic regression for

8


Under review as a conference paper at ICLR 2020

Test Error               Params     Search Cost      Comparable            Search
Architecture                                     Source                    Best        Average      
     (M)        (GPU Days)     Search Space?          Method
Shake-Shake                       (Devries and Taylor, 2017)     N/A           2.56              
26.2                 -                          -                      manual
PyramidNet                            (Yamada et al., 2018)         2.31           N/A              
 26                   -                          -                      manual
NASNet-A∗                                   (Zoph et al., 2018)           N/A           2.65        
       3.3               2000                      N                         RL

AmoebaNet-B∗                             (Real et al., 2018)            N/A     2.55     0.05       
  2.8               3150                      N                    evolution

Random search WS†             (Li and Talwalkar, 2019)      2.71     2.85     0.08         3.8      
           0.7                       Y                     random

ProxylessNAS                            (Cai et al., 2019)             2.08           N/A           
    5.7                  4                         N                gradient-based

ENAS                                       (Pham et al., 2018)           2.89           N/A         
      4.6                 0.5                       Y                         RL

ASNG-NAS                           (Akimoto et al., 2019)         N/A     2.83     0.14         3.9 
                0.1                       Y                gradient-based

SNAS                                         (Xie et al., 2019)             N/A     2.85     0.02   
      2.8                 1.5                       Y                gradient-based

DARTS (first order)†                    (Liu et al., 2019)             N/A     3.00     0.14        
 3.3                 0.4                       Y                gradient-based

DARTS (second order)†               (Liu et al., 2019)             N/A     2.76     0.09         
3.3                  1                         Y                gradient-based

PDARTS                                   (Chen et al., 2019)           2.50           N/A           
    3.4                 0.3                       Y                gradient-based

XNAS#                                                 Ours                      2.54           2.70 
              3.8                 0.3                       Y                gradient-based

EDARTS†                                                  Ours                      2.54     2.69 ± 
0.10         3.9                 0.4                       Y                gradient-based

Table 1: CIFAR-10 Benchmark: Comparison with manual networks and SOTA NAS on final
(Stage 3) results. The results are grouped by those for manually designed networks, full evaluation
NAS, weight-sharing NAS, and methods that we evaluated. All test errors are for models trained with
auxiliary towers and cutout (parameter counts exclude auxiliary weights). Test errors we report are
averaged over 10 seeds. “-” indicates that the field does not apply while “N/A” indicates unknown.
Note that search cost is hardware-dependent and our results were procured using Tesla V100 GPUs.

∗ We show results for networks with comparable number of parameters.

† For fair comparison to other work,  we show the stage 1 cost for a single search trial instead of 
the four
conducted in stage 1. We exclude the cost for stage 2 and 3 evaluation for the same reason.

# We evaluate the reported architecture found by XNAS using the DARTS training scheme for fair 
comparison.

IMDB sentiment analysis of Bag-of-n-Gram (BonG) featurizations, a standard NLP baseline (Wang
and Manning, 2012). Here there are eight configuration decisions: tokenization method, whether to
remove stopwords, whether to lowercase, choice of n, whether to binarize features, type of feature
weighting, smoothing parameter, and post-processing. As some choices affect the feature dimension
we   hash the BonGs into a fixed number of bins (Weinberger et al., 2009).

To test the performance of weight-sharing for feature map selection, we randomly sample 64 con-
figurations each for CIFAR-10 and IMDB and examine whether the above schemes converge to the
optimal choice.  The main comparison method here is thus random search, which runs a full sweep
over these samples; by contrast successive halving will need to solve 6 = log₂ 64 regression prob-
lems, while for exponentiated gradient we perform early stopping after five iterations.  Note that
weight-sharing can do no better than random search in terms of accuracy because they are picking
a configuration from a space that random search sweeps over.  The goal is to see if it consistently
returns a good configuration much faster.  As our results in Figures 1 and 2 show, successive halv-
ing indeed does almost as well as random search in much less time.  While exponentiated gradient
usually does not recover a near-optimal solution, it does on average return a configuration in the 
top
10%.  We also note the strong benefit of over-parameterization for IMDB – the n-gram vocabulary
has size 4 million so the number of bins on the right is much larger than needed to learn in a 
single-
configuration setting. Overall, these experiments show that weight-sharing can also be used as a 
fast
way to obtain signal in regular learning algorithm configuration and not just NAS.

NAS on CIFAR-10: Recall from Section 3 that when the architecture space consists of  E  simplices
of dimension  O , the convergence rate of exponentiated gradient descent to a stationary point of 
the
objective function is independent of the dimension of the space, while SGD has linear dependence.
This result motivates our geometry-aware method called Exponentiated-DARTS (EDARTS).

EDARTS modifies first-order DARTS in two ways.  First, in lieu of the softmax operation used by
DARTS on the architecture weights, we use standard normalization so that the weight of operation

o on edge (i, j) is u⁽ⁱ,ʲ⁾  =  c⁽ⁱ,ʲ⁾/ Σ       c⁽ⁱ,ʲ⁾.  Second, in lieu of Adam, we use 
exponentiated

gradient to update the architecture weights: ct = ct  ₁ exp(   η   clV (hwt    1 (ct  ₁)). While 
EDARTS

resembles XNAS (Nayman et al., 2019), our justification for using exponentiated gradient comes
directly from aligning with the optimization geometry of ERM. Additionally, EDARTS only requires
two straightforward modifications of first-order DARTS, while XNAS relies on a wipeout subroutine
and granular gradient-clipping for each edge operation on the cell and data instance level. ¹

¹Our own XNAS implementation informed by correspondence with the authors did not produce competitive
results. We still compare to the architecture XNAS reported evaluated by the DARTS training routine 
in Table 1.

9


Under review as a conference paper at ICLR 2020

We evaluate EDARTS on the task of designing a CNN cell for CIFAR-10. We use the standard search
space as introduced in DARTS (Liu et al., 2019) for evaluation we use the same three stage process
used by DARTS and random search with weight-sharing (Li and Talwalkar, 2019),  with stage 3
results considered the ‘final’ results. We provide additional experimental details in Appendix D.

Table 1 shows the performance of EDARTS relative to both manually designed and NAS-discovered
architectures.  EDARTS finds an architecture that achieves competitive performance with manually
designed architectures which have nearly an order-of-magnitude more parameters. Additionally, not
only does EDARTS achieve significantly lower test error than first-order DARTS, it also outperforms
second order DARTS while requiring less compute time, showcasing the benefit of geometry-aware
optimization.  Finally, EDARTS achieve comparable performance to the reported architecture for
state-of-the-art method XNAS when evaluated using the stage 3 training routine of DARTS.

Following XNAS (Nayman et al., 2019),  we also perform an extended evaluation of the best ar-
chitecture  found  by  EDARTS  with  AutoAugment,  cosine  power  annealing  (Hundt  et  al.,  
2019),
cross-entropy with label smoothing (Szegedy et al., 2015), and trains for 1500 epochs.  We evalu-
ated the XNAS architecture using our implementation for a direct comparison and also to serve as a
reproducibility check. EDARTS achieved a test error of 2.18% in the extended evaluation compared
to 2.15% for XNAS in our reproduced evaluation; note the published test error for XNAS is 1.81%.

To meet a higher bar for reproducibility we report ‘broad reproducibility’ results by repeating the
entire pipeline from stage 1 to stage 3 for two additional sets of seeds.  Our results in Table 2 
(see
Appendix)  show  that  EDARTS  has  lower  variance  across  experiments  than  random  search  with
weight sharing (Li and Talwalkar, 2019).  However, we do observe non-negligible variance in the
performance of the architecture found by different random seed initializations of the shared-weights
network, necessitating running multiple searches before selecting an architecture.

REFERENCES

N. Agarwal, B. Bullins, X. Chen, E. Hazan, K. Singh, C. Zhang, and Y. Zhang.  The case for full-
matrix adaptive regularization.  In Proceedings of the 36th International Conference on Machine
Learning, 2019.

Y. Akimoto, S. Shirakawa, N. Noshinari, K. Uchida, S. Saito, and K. Nishida.  Adaptive stochastic
natural gradient method for one-shot neural architecture search.  In International Conference on
Machine Learning, 2019.

S. Arora, Y. Liang, and T. Ma.  A simple but tough-to-beat baseline for sentence embeddings.  In

Proceedings of the 5th International Conference on Learning Representations, 2017.

P. Bartlett, D. J. Foster, and M. Telgarsky. Spectrally-normalized margin bounds for neural 
networks.
In Advances in Neural Information Processing Systems, 2017.

A. Beck. First-Order Methods in Optimization. MOS-SIAM, 2017.

A. Beck and M. Teboulle.  Mirror descent and nonlinear projected subgradient methods for convex
optimization. Operations Research Letters, 31:167–175, 2003.

L. M. Bregman. The relaxation method of finding the common point of convex sets and its applica-
tion to the solution of problems in convex programming. USSR Computational Mathematics and
Mathematical Physics, 7:200–217, 1967.

H. Cai, L. Zhu, and S. Han.   ProxylessNAS: Direct neural architecture search on target task and
hardware. In International Conference on Learning Representations, 2019.

F. M. Carlucci, P. M. Esperanc¸a, M. Singh, V. Gabillon, A. Yang, H. Xu, Z. Chen, and J. Wang.
MANAS: Multi-Agent Neural Architecture Search. arXiv e-prints, 2019.

X. Chen, L. Xie, J. Wu, and Q. Tian.  Progressive Differentiable Architecture Search: Bridging the
Depth Gap between Search and Evaluation. arXiv e-prints, art. arXiv:1904.12760, Apr 2019.

A. Choromanska, M. Henaff, M. M. G. B. Arous, and Y. LeCun.   The loss surface of multilayer
networks.   In  Proceedings  of  the  18th  International  Conference  on  Artificial  Intelligence 
 and
Statistics, 2015.

10


Under review as a conference paper at ICLR 2020

C. D. Dang and G. Lan.   Stochastic block mirror descent methods for nonsmooth and stochastic
optimization. SIAM Journal on Optimization, 25:856–881, 2015.

T. Devries and G. W. Taylor. Improved regularization of convolutional neural networks with cutout.
2017.

S. Ghadimi and G. Lan.  Stochastic first- and zeroth-order methods for nonconvex stochastic pro-
gramming. SIAM Journal on Optimization, 23:2341–2368, 2013.

E. Hazan and S. Kale.  Beyond the regret minimization barrier:  Optimal algorithms for stochastic
strongly-convex optimization. Journal of Machine Learning Research, 15:2489–2512, 2014.

A. Hundt, V. Jain, and G. Hager.  sharpdarts:  Faster and more accurate differentiable architecture
search. 2019.

K. Kandasamy, G. Dasarathy, J. Schneider, and B. Po´czos. Multi-fidelity bayesian optimisation with
continuous approximations. In International Conference on Machine Learning, 2017.

M. Kearns,  Y. Mansour,  A. Y. Ng,  and D. Ron.   An experimental and theoretical comparison of
model selection methods. Machine Learning, 27:7–50, 1997.

D. P. Kingma and J. Ba.  Adam:  A method for stochastic optimization.  In Proceedings of the 3rd
International Conference on Learning Representations, 2015.

J. Lafferty, H. Liu, and L. Wasserman. Statistical machine learning. 2010.

K. A. Laube and A. Zell. Prune and Replace NAS. arXiv e-prints, art. arXiv:1906.07528, Jun 2019.

L.  Li  and  A.  Talwalkar.   Random  search  and  reproducibility  for  neural  architecture  
search.   In

Uncertainty in Artificial Intelligence, 2019.

L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar.  Hyperband: A novel bandit-
based approach to hyperparameter optimization. Journal of Machine Learning Research, 18(185):
1–52, 2018. URL http://jmlr.org/papers/v18/16-558.html.

H. Liang, S. Zhang, J. Sun, X. He, W. Huang, K. Zhuang, and Z. Li.   DARTS+:  Improved Dif-
ferentiable Architecture Search with Early Stopping.  arXiv e-prints, art. arXiv:1909.06035, Sep
2019.

H. Liu, K. Simonyan, and Y. Yang.  DARTS: Differentiable architecture search.  In International
Conference on Learning Representations, 2019.

E. Loper and S. Bird. Nltk: the natural language toolkit. arXiv preprint cs/0205028, 2002.

I. Loshchilov and F. Hutter.  Sgdr:  Stochastic gradient descent with warm restarts.  arXiv preprint
arXiv:1608.03983, 2016.

J. Mairal.  Incremental majorization-minimization optimization with application to large-scale ma-
chine learning. SIAM Journal on Optimization, 25:829–855, 2015.

M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. MIT Press, 2012.

V. Nagarajan and J. Z. Kolter. Generalization in deep networks: The role of distance from 
initializa-
tion. arXiv, 2017.

V. Nagarajan and J. Z. Kolter. Uniform convergence may be unable to explain generalization in deep
learning. In Advances in Neural Information Processing Systems, 2019. To Appear.

N. Nayman, A. Noy, T. Ridnik, I. Friedman, R. Jin, and L. Zelnik-Manor. Xnas: Neural architecture
search with expert advice. 2019.

A. Noy, N. Nayman, T. Ridnik, N. Zamir, S. Doveh, I. Friedman, R. Giryes, and L. Zelnik-Manor.
ASAP: Architecture Search, Anneal and Prune. arXiv e-prints, art. arXiv:1904.04123, Apr 2019.

11


Under review as a conference paper at ICLR 2020

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. Journal of machine
learning research, 12(Oct):2825–2830, 2011.

H. Pham, M. Guan, B. Zoph, Q. Le, and J. Dean. Efficient neural architecture search via parameters
sharing. In International Conference on Machine Learning, 2018.

A. Rahimi and B. Recht.  Random features for large-scale kernel machines.  In Advances in Neural
Information Processing Systems, 2008.

M.  Razaviyayn,  M.  Hong,  and  Z.-Q.  Luo.   A  unified  convergence  analysis  of  block  
successive
minimization methods for nonsmooth optimization.   SIAM Journal on Optimization, 23:1126–
1153, 2013.

E. Real, A. Aggarwal, Y. Huang, and Q. V. Le.  Regularized Evolution for Image Classifier Archi-
tecture Search. 2018.

A. Rudi and L. Rosasco.  Generalization properties of learning with random features.  In Advances
in Neural Information Processing Systems, 2017.

K. Sridharan, N. Srebro, and S. Shalev-Schwartz. Fast rates for regularized objectives. In Advances
in Neural Information Processing Systems, 2008.

C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Ra-
binovich.  Going deeper with convolutions.  In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 1–9, 2015.

C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the Inception Architecture
for Computer Vision. arXiv e-prints, art. arXiv:1512.00567, Dec 2015.

K. Tsuda, G. Ra¨tsch, and M. K. Warmuth. Matrix exponentiated gradient updates for on-line learn-
ing and bregman projection. Journal of Machine Learning Research, 6:995–1018, 2005.

V. Vapnik. Estimation of Dependences Based on Empirical Data. Springer-Verlag, 1982.

Q. H. Vuong. Likelihood ratio tests for model selection and non-nested hypotheses. Econometrica:
Journal of the Econometric Society, pages 307–333, 1989.

G. Wang, S. Lu, W. Tu, and L. Zhang.  Sadam:  A variant of adam for strongly convex functions.
arXiv, 2019.

S. Wang and C. D. Manning.  Baselines and bigrams: Simple, good sentiment and topic classifica-
tion. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,
2012.

K. Weinberger, A. Dasgupta, J. Langford, A. Smola, and J. Attenberg. Feature hashing for large scale
multitask learning.  In Proceedings of the 26th International Conference on Machine Learning,
2009.

S. Xie, H. Zheng, C. Liu, and L. Lin.  SNAS: stochastic neural architecture search.  In 
International
Conference on Learning Representations, 2019.

Y. Yamada, M. Iwamura, and K. Kise. Shakedrop regularization. 2018.

Q.  Yao,  J.  Xu,  W.-W.  Tu,  and  Z.  Zhu.   Differentiable  Neural  Architecture  Search  via  
Proximal
Iterations. arXiv e-prints, art. arXiv:1905.13577, May 2019.

S. Zhang and N. He. On the convergence rate of stochastic mirror descent for nonsmooth nonconvex
optimization. arXiv, 2018.

B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le.   Learning transferable architectures for scalable
image recognition. In Conference on Computer Vision and Pattern Recognition, 2018.

12


Under review as a conference paper at ICLR 2020

A    OPTIMIZATION

This section contains proofs and generalizations of the non-convex optimization results in Section 
3.

A.1    PRELIMINARIES

We first gather some necessary definitions and results from convex analysis.

Definition A.1.  Let     be a convex subset of a finite-dimensional real vector space and f :       
    R

be everywhere sub-differentiable.

1.  For α > 0, f is α-strongly-convex w.r.t. norm ǁ · ǁ if ∀ x, y ∈ X  we have

f (y) ≥ f (x) + ⟨∇f (x), y − x⟩ +    ǁy − xǁ

2.  For β > 0, f is β-strongly-smooth w.r.t. norm ǁ · ǁ if ∀ x, y ∈ X  we have

β              2

f (y) ≤ f (x) + ⟨∇f (x), y − x⟩ +    ǁy − xǁ

Definition A.2.  Let X  be a convex subset of a finite-dimensional real vector space.  The Bregman
divergence induced by a strictly convex distance-generating function (DGF) ω : X  ›→ R is

Dω(x||y) = ω(x) − ω(y) − ⟨∇ω(y), x − y⟩ ∀ x, y ∈ X

By definition, the Bregman divergence satisfies the following properties:
1.  Dω(x||y) ≥ 0 ∀ x, y ∈ X  and Dω(x||y) = 0  ⇐⇒   x = y.

2.  If ω is α-strongly-convex w.r.t.  norm ǁ · ǁ  then so is Dω(·||y) ∀  y  ∈  X .  Furthermore,

Dω(x||y) ≥     ǁx − yǁ   ∀ x, y ∈ X .

3.  If ω is β-strongly-smooth w.r.t.  norm ǁ · ǁ  then so is Dω(·||y) ∀  y  ∈  X .  Furthermore,

Dω(x||y) ≤     ǁx − yǁ2  ∀ x, y ∈ X .

Lemma A.1 (Three-Points Lemma).  (Beck, 2017, Lemma 9.11) For any DGF ω : X  ›→ R and all

x, y, z ∈ X  we have

⟨∇ω(y) − ∇ω(x), z − x⟩ = Dω(z||x) + Dω(x||y) − Dω(z||y)

Definition A.3.  Let ω  :  X   ›→  R be a 1-strongly-convex DGF. Then for constant λ > 0 and an
everywhere sub-differentiable function f : X  ›→ R the proximal operator is defined over x ∈ X  as

proxλ(x) = arg min λf (u) + Dω(u  x)

u∈X

Note that the prox operator is well-defined whenever f is β-strongly-smooth for some β < λ.  We
will also use the following notation for the proximal gradient operator:

prox∇λ(x) = arg min λ⟨∇f (x), u⟩ + Dω(u||x)

Note that the prox grad operator is always well-defined.

Theorem A.1.  (Beck, 2017, Theorem 9.12) For any λ > 0, 1-strongly-convex DGF ω :  X  ›→  R,
and x ∈  X  let f  :  X  ›→  R be an everywhere sub-differentiable function s.t.  λf (·) + Dω(·||x) 
is
convex over X . Then for x⁺ = proxλ(x) and all u ∈ X  we have

⟨∇ω(x) − ∇ω(x  ), u − x  ⟩ ≤ λ(f (u) − f (x  ))

Lemma A.2.  For any λ > 0, 1-strongly-convex DGF ω : X  ›→ R, and x ∈ X  let f : X  ›→ R be an
everywhere sub-differentiable function s.t. λf (·) + Dω(·||x) is convex over X . Then

λ(f (x) − f (x⁺)) ≥ Dω(x⁺||x) + Dω(x||x⁺)

Proof.  Applying Theorem A.1 followed by Lemma A.1 yields

λ(f (x) − f (x⁺)) ≥ ⟨∇ω(x) − ∇ω(x⁺), x − x⁺⟩ = Dω(x||x⁺) + Dω(x⁺||x) − Dω(x||x)

Corollary A.1.  For any λ  >  0,  1-strongly-convex DGF ω  :  X   ›→  R,  x  ∈  X ,  and everywhere
sub-differentiable function f : X  ›→ R we have for x⁺ = prox∇λ(x) that

λ⟨∇f (x), x − x⁺⟩ ≥ Dω(x⁺||x) + Dω(x||x⁺)

13


Under review as a conference paper at ICLR 2020

A.2    STATIONARITY MEASURES

Because we consider constrained non-convex optimization, we cannot measure convergence to a sta-
tionary point by the norm of the gradient. Instead, we analyze the convergence proximal-mapping-
based stationarity measures.  The most well-known measure is the norm of the projected gradient
(Ghadimi and Lan, 2013), which in the unconstrained Euclidean case reduces to the norm of the gra-
dient and in the general case measure the distance between the current point and a cone satisfying
first-order optimality conditions (Dang and Lan, 2015, Proposition 4.1).  Our convergence results
hold for a stronger measure that we call the projected stationarity and which is inspired by the 
Breg-
man stationarity measure of Zhang and He (2018) but using the prox grad operator instead of the
prox operator.

Definition A.4.  Let ω :            R be a 1-strongly-convex DGF and f  :            R be an 
everywhere
sub-differentiable function. Then for any λ > 0 we define the following two quantities:

1.  The projected gradient of f at x ∈ X  is Gλ(x) =  ˣ−ᵖʳᵒˣ∇λ(x) .


2.  The projected stationarity of f at x ∈ X  is ∆λ

The following properties follow:

(x) =  Dω(x|| prox∇λ(x))+Dω(prox∇λ(x)||x) .

1.  If X  = Rd for some d ≥ 1 and ω(·) =  ¹ ǁ · ǁ2  then we have Gλ(x) = ∇f (x).

2           2

2.  ǁGλ(x)ǁ2  ≤ ∆λ(x) ∀ x ∈ X .

We can also consider the Bregman stationarity measure of Zhang and He (2018) directly.  As this
measure depends on the prox operator,  which is not always defined,  we first state the notion of
non-convexity that Zhang and He (2018) consider.

Definition A.5.  An everywhere sub-differentiable function f : X  ›→ R is (γ, ω)-relatively-weakly-
convex ((γ, ω)-RWC) for γ > 0 and ω : X  ›→ R a 1-strongly-convex DGF if f (·) + γω(·) is convex
over X . Note that all γ-strongly-smooth functions are (γ, ω)-RWC.

Note that (γ, ω)-RWC is a generalization of γ-strong-smoothness w.r.t.  the norm w.r.t.  which ω is
strongly-convex. Furthermore, for such functions we can always define the prox operator for λ > γ,
allowing us to also define the Bregman gradient below. Similarly to before, bounding the Bregman
stationarity measure yields a stronger notion of convergence than the squared norm of the Bregman
gradient.  For the relationship between the Bregman stationarity measure and first-order optimality
conditions see Zhang and He (2018, Equation 2.11).

Definition A.6.  Let ω :            R be a 1-strongly-convex DGF and f  :            R be a (γ, 
ω)-RWC
everywhere sub-differentiable function for some γ > 0. Then for any λ > γ we define the following
two quantities:

1.  The Bregman gradient of f at x ∈ X  is Bλ(x) =  ˣ−ᵖʳᵒˣλ(x) .


2.  The Bregman stationarity of f at x ∈ X  is Γλ

(x) =  Dω(x|| proxλ(x))+Dω(proxλ(x)||x) .

Note that, similarly to Definition A.4, ǁBλ(x)ǁ2  ≤ Γλ(x) ∀ x ∈ X .

14


Under review as a conference paper at ICLR 2020

A.3    SUCCESSIVE STOCHASTIC STRONGLY CONVEX APPROXIMATION FOR CONSTRAINED
NON-CONVEX NON-EUCLIDEAN OPTIMIZATION

Here we prove our main optimization results.  We begin with a descent lemma guaranteeing im-
provement of a non-convex function due to approximately optimizing a strongly convex surrogate.

Lemma A.3.  Let ω :  X  ›→  R be a 1-strongly-convex DGF and f  :  X  ›→  R be everywhere sub-
differentiable.  For some x ∈  X  and ρ > 0 define fˆx(·)  =  f (·) + ρDω(·||x) and let xˆ  ∈  X  
be a
point s.t. Efˆx(xˆ) − minu∈X fˆx(u) ≤ ε. Then


1.  If f is β-strongly-smooth, ρ > β, and λ =   ¹

then λ ∆λ(x) ≤ E(f (x) − f (xˆ)) + ε.


2.  If f is (γ, ω)-RWC, ρ > γ, and λ =   ¹

we have that λ Γλ(x) ≤ E(f (x) − f (xˆ)) + ε.

Proof.  Generalizing an argument in Agarwal et al. (2019, Theorem A.2), for x⁺           we have by
strong-convexity of fˆx that

E(f (x) − f (xˆ)) ≥ E(f (x) − fˆx(xˆ)) = E .f (x) − min fˆ(u) + min fˆ(u) − fˆ(xˆ)Σ

u∈X                      u∈X

≥ f (x) − fˆ(x⁺) − ε

= f (x) − f (x⁺) − ρDω(x⁺||x) − ε

If f is β-strongly-smooth set x⁺ = prox∇λ(x), so that by Corollary A.1 we have

f (x) − f (x⁺) − ρD  (x⁺||x) ≥ ⟨∇f (x), x − x⁺            x − x⁺ǁ2  − ρD  (x⁺||x)

ω                                     ⟩ −  2 ǁ                              ω

Dω(x⁺||x) + Dω(x||x⁺)      ρ          +   2                         +


≥                        λ                 −  2 ǁx − x  ǁ   − ρDω(x

||x)

≥ . 1  − ρΣ (D  (x⁺||x) + D  (x||x⁺))

ω

λ

=  2 ∆λ(x)

In the other case of f being (γ, ω)-RWC set x⁺ = proxλ(x), so that by Lemma A.2 we have


f (x) − f (x

) − ρDω(x

||x) ≥

D  (x⁺   x) + D  (x  x⁺)                 +

λ                 − ρDω(x

||x)


≥ ρ(Dω(x
λ

||x) + Dω(x||x  ))

=  2 Γλ(x)

We now turn to formalizing our multi-block setting and assumptions.

Setting A.1.  For i = 1, . . . , b let Xi be a convex subset of a real vector space with an 
associated
DGF ωi : Xi ›→ R that is 1-strongly-convex w.r.t. some norm ǁ·ǁ₍i₎ over Xi. We have an everywhere

sub-differentiable function f : X  ›→ R over the product space X  =×b      X  .

i

Our main results will hold for the case when the following general assumption is satisfied. We will
later show how this assumption can follow from strong smoothness or relative weak convexity and
existing algorithmic results.

Assumption A.1.  In Setting A.1, for any given ε > 0 and each i ∈ [b] there exists a constant ρi > 0
and an algorithm Ai  :  X   ›→  X  that takes a point x ∈  X  and returns a point xˆ  ∈  X  
satisfying
xˆ−i = x−i and

EAi (f (xˆi, x−i) + ρiDωi (xˆi||xi)) ≤ ε + min f (u, x−i) + ρiDωi (u||xi)

where the subscript i selects block i, the subscript −i selects all blocks other than block i, and 
EAi

denotes expectation w.r.t. the randomness of algorithm Ai and any associated stochastic oracles.

15


Under review as a conference paper at ICLR 2020

Algorithm  2:  Generic  successive  convex  approximation  algorithm  for  reaching  a  stationary
point of the non-convex function in Setting A.1.

Input: Point x⁽¹⁾ ∈ X  in the product space of Setting A.1. Algorithms A₁, . . . , Ab : X  ›→ X.

for iteration t ∈ [T ] do

sample ξt ∼ Unif[b]

update x⁽ᵗ⁺¹⁾ ← Aξ (x⁽ᵗ⁾)

Output: x ∼ Unif{x⁽ᵗ⁾}T    .

Our main result relies on the following simple lemma guaranteeing non-convex convergence for a
generic measure satisfying guaranteed expected descent:

Lemma A.4.  In Setting A.1, for some ε > 0 and each i ∈  [b] let E⁽ⁱ⁾(x) be any measure s.t.  for
some λi and some algorithm Ai : X  ›→ X  we have λi E⁽ⁱ⁾ ≤ EA  (f (x) − f (Ai(x))) + ε ∀ x ∈ X .


Then the output x of Algorithm 2 satisfies

Σ

2      λi                       i

. F      Σ

where F  =  f (x⁽¹⁾)     arg minu       f (x) and the expectation is taken over the sampling at each
iteration,  the  sampling  of the  output,  and the  randomness of  the algorithms  and any  
associated
stochastic oracles.

Proof.  Define Ξt = {(ξs, Aξ )}t       and note that x⁽ᵗ⁺¹⁾ = Aξ (x⁽ᵗ⁾). We then have


F = f (x⁽¹⁾)     arg min f (x)     EΞ

u∈X

= EΞT

T

(f (x⁽¹⁾) − f (x⁽T ⁺¹⁾))

T

(f (x⁽ᵗ⁾) − f (x⁽ᵗ⁺¹⁾))

t=1


=        EΞ

t=1

T

t−1

Eξt EAξt

b

(f (x⁽ᵗ⁾) − f (x⁽ᵗ⁺¹⁾))


=  1        E

b

t=1

Ξt−1

EAi

i=1

(f (x⁽ᵗ⁾) − f (x⁽ᵗ⁺¹⁾))

T      b

≥  1 E     Σ Σ λi E⁽ⁱ⁾(x⁽ᵗ⁾) − ε


which implies that

Σ

b   ΞT

1         Σ Σ

		

2    λi

t=1  i=1

. F      Σ

In the single-block setting, Lemmas A.3 and A.4 directly imply the following guarantee:

Theorem A.2.  In Setting A.1 and under Assumption A.1, let b = 1 and ρ satisfy one of the 
following:

1.  f : X  ›→ R is β-strongly-smooth and ρ = 2β.

2.  f : X  ›→ R is (γ, ω)-RWC for some DGF ω : X  ›→ R and ρ = 2γ.

Then Algorithm 2 returns a point x ∈ X  satisfying one of the following (respectively, w.r.t. the 
above
settings) for F = f (x⁽¹⁾) − minu∈X f (u)


1.       ∆ 1   (x)     8β

4β

F  + ε
T

2.       Γ 1   (x)     8γ

4γ

F  + ε
T

Here the expectation is taken over the randomness of the algorithm and oracle.

16


Under review as a conference paper at ICLR 2020

We can apply a known result for the strongly convex case to recover the rate of Zhang and He (2018)
for non-convex stochastic mirror descent, up to an additional depending on ω:

Corollary A.2.  In Setting A.1 for b = 1 and (γ, ω)-RWC f , suppose we have access to f through a
stochastic gradient oracle                            such that         ²         ². Let            
         be an algorithm

that for any x ∈ X  runs the Epoch-GD method of Hazan an∗d Kale (2014) with total number of steps

N , initial epoch length T₁  =  4 and initial learning rate η₁  =  ¹  on fˆx(·)  =  f (·) + 
2γDω(·||x).
Then with NT calls to the stochastic gradient oracle Algorithm 2 returns a point x ∈ X  satisfying

. F     16(G² + 4γ²L² ) Σ

Γ 1   (x)     8γ       +

4γ                                 T               γN


expected ε-stationary-point, as measured by E.Γ

, can be reached in O

γF (G²+γ²L² )

oracle

Proof.  Apply Theorem 5 of Hazan and Kale (2014) together with the fact that fˆx  is γ-strongly-
convex w.r.t. ǁ · ǁ and its stochastic gradient is bounded by G² + 4γ²Lω.

For the multi-block case our results hold only the projected stationarity measure:

Theorem A.3.  In Setting A.1 and under Assumption A.1 assume f (·, x−i) is β-strongly-smooth

w.r.t.   ǁ · ǁ₍i₎  over  Xi for  each  i  ∈  [b]  and  each  x−i  ∈  X−i.   If  ρ  =  ρ₁  =  · · · 
ρb  =  2β and

F = f (x⁽¹⁾) − minu∈X f (u) then Algorithm 2 returns a point x ∈ X  satisfying


∆ 1   (x)     8βb

4β

F  + ε
T

Here the expectation is taken over the randomness of the algorithm and oracle and the projected

stationarity measure ∆λ is defined w.r.t. the Bregman divergence of the DGF ω(x) = Σb      ωi(xi).

Proof.  For each block i    [b] the conditions of Lemma A.4 are satisfied by assumption and applying
Lemma A.3 to the projected stationary measures


(i)

λ

Dω (xi|| prox∇⁽ⁱ⁾(x)) + Dω (prox∇⁽ⁱ⁾(x)||xi))

λ2


where prox∇⁽ⁱ⁾(x) = arg min

u∈Xi

λ⟨∇if (x), u⟩ + Dωi (u||xi). To apply Lemma A.4 in the multi-

block setting it suffices to show that the sum of the projected stationarity measures on each block 
is
equal to the projected stationarity measure induced by the sum of the DGFs.  For some λ > 0 and
any i ∈ [b] we have that

prox∇λ(x)i = .arg min λ⟨∇f (x), u⟩ + Dω(u||x)Σ


u∈X

=    arg min λ

u∈X

Σi=1

i

⟨∇if (x), ui⟩ + Dωi (ui||xi)

i

= arg min λ⟨∇if (x), u⟩ + Dω (u||xi) = prox∇⁽ⁱ⁾(x)


and so

i                                               λ

u∈Xi

∆  (x) =  Dω(x|| prox∇λ(x)) + Dω(prox∇λ(x)||x)

λ

b


=   1         D

λ2

i=1

b

ωi (xi|| prox∇

λ(x)i) + Dωi

(prox∇

λ(x)i||xi)

b


=   1  Σ D

(x || prox∇⁽ⁱ⁾(x)) + D

(prox∇⁽ⁱ⁾(x)||x ) = Σ ∆⁽ⁱ⁾(x)

Thus applying Lemma A.2 with λ =   ¹  yields the result.

17


Under review as a conference paper at ICLR 2020

In  the  following  corollary  we  recover  the  rate  of  Dang  and  Lan  (2015)  for  non-convex  
block-
stochastic mirror descent, up to an additional term depending on ωi:

Corollary A.3.  In Setting A.1 for β-strongly-smooth f ,  suppose we have access to f  through a

stochastic gradient oracle g(x) = E∇f (x) such that Eǁgiǁ2         ≤ G². For i ∈ [b] let Ai : X  ›→ 
X


be an algorithm that for any

(i),∗            i


total number of steps

x         runs the Epoch-GD method of Hazan and Kale (2014) with

N , initial epoch length T₁ = 4 and initial learning rate η₁ =  ¹  on surrogate

function fˆx(·)  =  f (·, x−i) + 2βDωi (·||xi).  Then with NT calls to the stochastic gradient 
oracle
Algorithm 2 returns a point x ∈ X  satisfying


. F      16

2              2    2   Σ

for F  =  f (x⁽¹⁾) − minu∈X f (x) and Lω  the Lips chitz constant of ωi w.r.t.  ǁ · ǁ₍i₎  over Xi.  
So

 


βFb²    b

O        ε4

i=1

G2  + β2    2

We can specialize this to the architecture search setting where we have a configuration search space
contained in the product of simplices induced by having n decisions with c choices each together
with a parameter space bounded in Euclidean norm.

Corollary A.4.  Under the assumptions of Corollary A.3, suppose b = 2 and we have the following
two geometries:

•  X₁ =×n    {p ∈ [δ, 1]ᶜ : ǁpǁ   = 1}, ǁ · ǁ      = ǁ · ǁ  , ω (·) = ⟨·, log(·)⟩.

1                           (1)                  1       1

•  X₂ = {w ∈ Rd  : ǁwǁ₂ ≤ B}, ǁ · ǁ₍₂₎ = ǁ · ǁ₂, ω₂(·) =  ¹ ǁ · ǁ2.

Suppose the stochastic gradient oracle of f has bounded l   -norm σ₁ over    ₁ and σ₂ over    ₂. 
Then
Algorithm 2 will return an expected ε-stationary point of f under the projected stationarity measure
in a number of stochastic oracle calls bounded by


. βFb² .  2           2           1        2

2     2ΣΣ

18


Under review as a conference paper at ICLR 2020

B    GENERALIZATION

This section contains proofs of the generalization results in Section 4.

B.1    SETTINGS AND MAIN ASSUMPTION

We first describe the setting for which we prove our general result.

Setting B.1.  Let C be a set of possible architecture/configurations of finite size such that each 
c ∈ C

is associated with a parameterized hypothesis class H ={h⁽ᶜ⁾  :  X  ›→  Y  :  w  ∈  W}  for input

space Z  =  X × Y  and fixed set of possible weights W.  We will measure the performance of a

hypothesis h⁽ᶜ⁾ on an input z = (x, y) ∈ Z using l  (w, c) = l(h⁽ᶜ⁾(x), y) for some B-bounded loss

w                                                                     z                         w

l : Y × Y  ›→ [−B, B].

We are given a training sample T  ∼  DmT   of size mT  and a validation sample V  ∼  DmV    of size

mV , where D is some distribution over the input space Z. Abusing notation, we will denote the the

(c)

population risk by lD(h   ) = lD(w, c) = Ez∼Dlz(w, c) and for any finite subset S ⊂  Z we will


denote the empirical risk by lS

(h⁽ᶜ⁾) = lS

(w, c) =    ¹ 

|S|

z∈S

lz(w, c).

Finally, we will consider solutions of optimization problems that depend on the training data and
architecture.  Specifically, for any configuration c       and finite subset S     Z let     c(S)   
        be
the set of global minima of some optimization problem induced by S and c and let the associated

version space (Kearns et al., 1997) be H (S) = {h⁽ᶜ⁾ : w ∈ W  (S)}.

We next give as examples two specific settings encompassed by Setting B.1.

Setting B.2.  For feature map selection, in Setting B.1 the configuration space C is a set of 
feature
maps fc : X ›→ Rd, the set of weights W  ⊂ Rd  consists of linear classifiers, for inputs x ∈ X the
hypotheses are h⁽ᶜ⁾(x) = ⟨f (x), w⟩ for w ∈ W, and so W  (S) is the singleton set of solutions to


the regularized ERM problem

arg min

w∈W

for some coefficient λ > 0.

λǁwǁ2

+

(x,y)∈S

l(⟨fc(x), w⟩, y)

Setting B.3.  For neural architecture search, in Setting B.1 the configuration space consists of all
possible choices of edges on a DAG of N nodes and a choice from one of K operations at each

edge,  for a total number of configurations bounded by KN2 .   The hypothesis h⁽ᶜ⁾  :  X  ›→  Y  is

determined by a choice of architecture c        and a set of network weights w          and the loss
l : Y     Y        0, 1   is the zero-one loss. In the simplest case     c(S) is the set of global 
minima of
the ERM problem


min

w∈W

(xΣ,y)∈S

l(h⁽ᶜ⁾(x), y)

We now state the main assumption we require.

Assumption  B.1.  In  Setting  B.1  there  exists  a  good  architecture  c∗   ∈   C,  i.e.   one  
satisfying

(w∗, c∗) ∈ arg minW×mC  lD(w, c) for some weights w∗ ∈ W, such that w.p. 1 − δ over the drawing

of training set T ∼ D   T   at least one of the minima of the optimization problem induced by c∗ 
and

T has low excess risk, i.e. ∃ w ∈ Wc∗ (T ) s.t.

lD(w, c∗) − lD(w∗, c∗) ≤ εc∗ (mT , δ)

for some error function εc∗ .

Clearly, we prefer error functions εc∗  that are decreasing in the number of training samples mT and
increasing at most poly-logarithmically in  ¹ . This assumption requires that if we knew the optimal
configuration a priori, then the provided optimization problem will find a good set of weights for 
it.
We will show how, under reasonable assumptions, Assumption B.1 can be formally shown to hold
in Settings B.2 and B.3.

19


Under review as a conference paper at ICLR 2020

B.2    MAIN RESULT

Our general result will be stated in terms of covering numbers of certain function classes.

Definition B.1.  Let H be a class of functions from X to Y .  For any ε  >  0 the associated L
covering number N (H, ε) of H is the minimal positive integer k such that H can be covered by k
balls of L∞-radius ε.

The following is then a standard result in statistical learning theory (see e.g.  Lafferty et al. 
(2010,
Theorem 7.82)):

Theorem B.1.  Let H be a class of functions from X to Y  and let l  :  Y  × Y  ›→  [0, B] be an

L-Lipschitz, B-bounded loss function. Then for any distribution D over X × Y  we have


Pr

S∼Dm

. sup |lD(h) − lS(h)| ≥ 3εΣ

≤ 2N (H, ε) exp

mε²

− 2B2

where we use the loss notation from Setting B.1.

Before stating our theorem, we define a final quantity, which measures the log covering number of
the version spaces induced by the optimization procedure over a given training set.

Definition  B.2.  In  S.eStting  B.1,  for Σany  sample  S  ⊂  X × Y  define  the  version  entropy 
 to  be

Theorem B.2.  In Setting B.1 let (wˆ, cˆ)                   be obtained as a solution to the 
following opti-
mization problem:


arg min

w∈W,c∈C

lV (w, c)        s.t.        w ∈ Wc(T )

Then under Assumption B.1 we have w.p. 1 − 3δ that

lD(wˆ, cˆ) ≤ lD(w∗, c∗)                                    

+ εc∗ (mT , δ) + B.         log     + inf 3ε + B.       .Λ(H, ε, T ) + log    Σ

   1          1                              2                                   1

2mV          δ     ε>0                         mV                                        δ


Proof.  We have that

lD(wˆ, cˆ) − lD(w∗, c∗)     ≤      lD(wˆ, cˆ) − lV (wˆ, cˆ)

1

+     l  (wˆ, cˆ)     l  (w, c∗)

`            ˛2¸              x


+     lV (w, c∗) − lD(w, c∗)

3

each term of which can be bounded as follows:

+     l  (w, c∗)     l  (w∗, c∗)

`              ˛4¸                x

1.  Since wˆ ∈ Wcˆ(T ) for some cˆ ∈ C the hypothesis space can be covered by the union of the
coverings of Hc(T ) over c ∈ C, so by Theorem B.1 we have that w.p. 1 − δ   

l  (wˆ, cˆ) − l  (wˆ, cˆ) ≤  inf 3ε + B.   2   .Λ(H, ε, T ) + log 1 Σ

2.  By optimality of the pair (wˆ, cˆ) and the fact that w ∈ Wc∗ (T ) we have


lV (wˆ, cˆ) =        min

c∈C,w∈Wc(T )

lV (wˆ, cˆ)          min

w∈Wc∗ (T )

lV (w, c∗) ≤ lV (w, c∗)

3.  Hoeffding’s inequality yields lV (w, c∗) − lD(w, c∗) ≤ B.   1      log ¹  w.p. 1 − δ

4.  Assumption B.1 states that lD(w, c∗) − lD(w∗, c∗) ≤ εc∗ (mT , δ) w.p. 1 − δ.

20


Under review as a conference paper at ICLR 2020

B.3    APPLICATIONS

For the feature map selection problem, Assumption B.1 holds by standard results for l₂-regularized
ERM for linear classification (e.g. Sridharan et al. (2008)):

Corollary B.1.  In Setting B.2,  suppose the loss function l is Lipschitz.   Then for 
regularization


mT          δ

∗

∗    ∗            . ǁw∗ǁ2  + 1       1 

We can then directly apply Theorem B.2 and the fact that the version entropy is bounded by log

because the minimizer over the training set is always unique to get the following:

Corollary B.2.  In Setting B.2 let (wˆ, cˆ)                  be obtained as a solution to the 
following opti-
mization problem:


Then

arg min

w∈W,c∈C

lV (w, c)        s.t.        w = arg min

w∈W

2

2

(x,y)∈T

l(⟨fc(x), w⟩, y)


∗    ∗            . ǁw∗ǁ2  + 1

1     .   1  

	

|C| + 1 

In the special case of kernel selection we can apply generalization results for learning with random
features to show that we can compete with the optimal RKHS from among those associated with
one  of the configurations (Rudi and Rosasco, 2017, Theorem 1):

Corollary  B.3.  In  Setting  B.2,  suppose  each  configuration  c         is  associated  with  a 
 random
Fourier feature approximation of a continuous shift-invariant kernel that approximates an RKHS
c. Suppose l is the squared loss so that (wˆ, cˆ)                 is obtained as a solution to the 
following

optimization problem:


arg min

w∈W,c∈C

lV (w, c)        s.t.        w = arg min

w∈W

2

2

(x,y)∈T

(⟨fc(x), w⟩ − y)²

If the number of random features d =     (√mT log √mT /δ) and λ = 1/√mT then w.p.  1     δ we

have


2  1

lD(h  ) − min min lD(h) ≤ O  √         +

.   1  

log

|C| + 1 

In the case of neural architecture search we are often solving (unregularized) ERM in the inner
optimization problem. In this case we can make an assumption weaker than Assumption B.1, namely
that the set of empirical risk minimizers contains a solution that, rather than having low excess 
risk,
simply has low generalization error; then applying Hoeffding’s inequality yields the following:

Corollary B.4.  In Setting B.1 let (wˆ, cˆ)                  be obtained as a solution to the 
following opti-
mization problem:

arg min     lV (w, c)        s.t.        w    arg min lT (w′, c)

w∈W,c∈C                                                                                w'∈W

Suppose  there  exists  c∗               satisfying  (w∗, c∗)         arg min         l  (w, c)  
for  some  weights
w∗                such  that  w.p.   1      δ  over  the  drawing  of  training  set  T          ᵐT 
   at  least  one  of
the  minima  of  the  optimization  problem  induced  by  c∗  and  T  has  low  generalization  
error,  i.e.

∃ w ∈ arg minw'∈W  lT (w′, c∗) s.t.

lD(w, c∗) − lT (w∗, c∗) ≤ εc∗ (mT , δ)

for some error function εc∗ . Then we have w.p. 1 − 4δ that

l  (wˆ, cˆ) ≤ l  (w∗, c∗) + ε ∗ (m  , δ) + B.   1    log 1 + B.   1    log 1

+ inf 3ε + B.  2   .Λ(H, ε, T ) + log 1 Σ

21


Under review as a conference paper at ICLR 2020

C    FEATURE  MAP  SELECTION  DETAILS

Solvers for Ridge regression and logistic regression were from scikit-learn (Pedregosa et al., 
2011).
For CIFAR-10 we use the kernel configuration setting from Li et al. (2018) but replacing the regu-
larization parameter by the option to use the Laplace kernel instead of Gaussian. The regularization
was fixed to λ =  ¹  The split is 40K/10K/10K. For IMDB we consider the following configuration

choices:

1.  Tokenizer: removing punctuation, splitting on punctuation, custom NLTK (Loper and Bird,
2002)

2.  removing stopwords or not

3.  lowercasing or not

4.  order: n = 1, 2, 3, higher order includes all lower-order n-grams

5.  binarizing features or not

6.  feature weights:  naive-Bayes (Wang and Manning, 2012) or smoothed inverse frequency
(Arora et al., 2017)

7.  α: smoothing parameter for this weighting

8.  preprocessing: None, l₂-normalization, averaging by number of tokens
The regularization was fixed to C = 1. The split is 25K/12.5K/12.5K

Figure 3:   Validation accuracy on CIFAR-10 (left) and IMDB (right) of feature map selection with
weight-sharing compared to a full sweep of random configurations. Average over 16 seeds.

D    NAS EXPERIMENT  DETAILS

We  provide  additional  details  on  CNN  NAS  benchmark  on  designing  CNN  cells  for  CIFAR-10
below.

D.1    SEARCH SPACE

We consider the same search space as DARTS (Liu et al., 2019),  which has become one of the
standard search spaces for CNN cell search (Nayman et al., 2019; Chen et al., 2019; Xie et al., 
2019;
Noy et al., 2019; Liang et al., 2019). Following DARTS and random search with weight-sharing (Li
and Talwalkar, 2019), our evaluation of EDARTS consists of three stages:

Stage 1:  In the search phase, we run EDARTS with four random seeds to reduce variance from
different initialization of the shared-weights network.

•  Stage 2: We evaluate the best architecture identified by each search run by training from 
scratch.
Stage 3: We perform a more thorough evaluation of the best architecture from stage 2 by training
with ten different random seed initializations.

22


Under review as a conference paper at ICLR 2020

For stages 2 and 3, we train each architecture for 600 epochs with the same hyperparameter settings
as DARTS.

For completeness, we describe the convolutional neural network search space considered. The set of
operations O considered at each node include: (1) 3     3 separable convolution, (2) 5     5 
separable

convolution, (3) 3    3 dilated convolution, (4) 5    5 dilated convolution, (5) max pooling, (6) 
average

pooling, (7) identity.

We use the same search space to design a “normal” cell and a “reduction” cell; the normal cells have
stride 1 operations that do not change the dimension of the input, while the reduction cells have
stride 2 operations that half the length and width dimensions of the input.  In the experiments, for
both cell types, we set N = 6 with 2 input nodes and 4 intermediate nodes, after which the output
of all intermediate nodes are concatenated to form the output of the cell.

D.2    STAGE 1: ARCHITECTURE SEARCH

We use EDARTS to train a smaller shared-weights network in the search phase with 8 layers and 24
initial channels instead of the 16 used by DARTS. Additionally, to more closely mirror the architec-
ture used for evaluation in stage 2 and 3, we use an auxiliary head with weight 0.4 and scheduled
path dropout of 0.2.  For the EDARTS architecture updates, we use a learning rate of 0.2 for the
normal cell and 0.6 for the reduction cell.  All other hyperparameters are the same as DARTS: 50
training epochs, batch size of 64, gradient clipping of 5 for network weights, SGD with momentum
set to 0.9 and learning rate annealed from 0.025 to 0.001 with cosine annealing (Loshchilov and
Hutter, 2016), and weight decay of 0.0003.

D.3    STAGE 2 AND 3: ARCHITECTURE EVALUATION

We use the same evaluation scheme as DARTS when retraining architectures from scratch.   The
larger evaluation network has 20 layers and 36 initial channels and is trained for 600 epochs using
SGD with momentum set to 0.9, a batch size of 96, and a learning rate of 0.025 annealed down to
0; the gradient clipping scheduled drop path rate and weight decay are identical to the search 
phase.
We also use an auxiliary head (Szegedy et al., 2015) with a weight of 0.4 and cutout (Devries and
Taylor, 2017).

D.4    DOES WEIGHT-SHARING PERFORM IMPLICIT REGULARIZATION


225%

200%

Percentage Difference in l2_from_init
Between Fixed Architecture and Mixture

        all weights

        sep_conv weights

45%

Percentage Difference in l2_norm
Between Fixed Architecture and Mixture

        all weights

        sep_conv weights


175%

150%

40%


125%

100%

75%

50%

35%

30%

0               10              20              30              40              50                  
                                  0               10              20              30              
40              50

Figure 4: Norm comparison between weight of the best EDARTS architecture trained from scratch
versus that of the corresponding shared-weights network.

We investigate whether weight-sharing implicitly regularizes the hypothesis space by examining the
l₂  norms and distance from initialization of the shared-weights network relative to that observed
when training the best EDARTS architecture from scratch. We use the same network depth and hy-
perparameters as those used for the shared-weights network to train the fixed architecture. Figure 4
shows    the percent difference in the norms between the fixed architecture and the shared-weights
network pruned to just the operations kept for the fixed architecture.  From the chart, we can see
that both the l₂  distance from initialization and the l₂  norm of the shared-weights is smaller 
than
that of a fixed network are higher than that of the shared-weights network by over 40%, suggesting
weight-sharing acts as a form of implicit regularization.

23


Under review as a conference paper at ICLR 2020


Set 1

1          2          3          4       Stage 3
2.79     2.91     2.99     2.88       2.79

Set 2

1          2          3          4       Stage 3
3.55     3.20     3.05     2.65       2.69

Set 3

1          2          3          4       Stage 3
2.74     2.96     2.63     2.61       2.71

Table 2:  EDARTS stage 2 and 3 test error for 3 sets of random seeds.  The bolded test error is the
best stage 3 performance across the 3 sets of random seeds.

D.5    DISCUSSION OF REPRODUCIBILITY IN NAS

Our ‘broad reproducibility’ results in Table 2 show that EDARTS has lower variance across experi-
ments than random search with weight sharing (Li and Talwalkar, 2019).  However, we do observe
non-negligible variance in the performance of the architecture found by different random seed ini-
tializations of the shared-weights network, necessitating running multiple searches before selecting
an architecture.

24

