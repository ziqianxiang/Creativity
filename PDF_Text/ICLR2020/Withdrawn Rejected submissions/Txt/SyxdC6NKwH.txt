Under review as a conference paper at ICLR 2020
Uncertainty-Aware Prediction for Graph Neu-
ral Networks
Anonymous authors
Paper under double-blind review
Ab stract
Thanks to graph neural networks (GNNs), semi-supervised node classification has
shown the state-of-the-art performance in graph data. However, GNNs have not
considered different types of uncertainties associated with the class probabilities
to minimize risk increasing misclassification under uncertainty in real life. In this
work, we propose a Subjective Bayesian deep learning framework reflecting vari-
ous types of uncertainties for classification predictions by leveraging the powerful
modeling and learning capabilities of GNNs. We considered multiple uncertainty
types in both deep learning (DL) and belief/evidence theory domains. We treat
the predictions of a Subjective Bayesian GNN (S-BGNN) as nodes’ multinomial
subjective opinions in a graph based on Dirichlet distributions where each belief
mass is a belief probability of each class. By collecting evidence from the given
labels of training nodes, the S-BGNN model is designed for accurately predict-
ing probabilities of each class and detecting out-of-distribution. We validated
the outperformance of the proposed S-BGNN, compared to the state-of-the-art
counterparts in terms of the accuracy of node classification prediction and out-of-
distribution detection based on six real network datasets.
1	Introduction
Inherent uncertainties introduced by different root causes have emerged as serious hurdles to find
effective solutions for real world problems. Critical safety concerns have been brought due to lack
of considering diverse causes of uncertainties, resulting in high risk due to misinterpretation of
uncertainties (e.g., misdetection or misclassification of an object by an autonomous vehicle). Graph
neural networks (GNNs) (KiPf & Welling, 2016; VelickoVic et al., 2018) have gained tremendous
attention in the data science community. Despite their superior performance in semi-supervised
node classification and/or regression, they didn’t allow to deal with various tyPes of uncertainties.
Predictive uncertainty estimation (Malinin & Gales, 2018) using Bayesian NNs (BNNs) has been
exPlored for classification Prediction or regression in the comPuter vision aPPlications, with well-
known uncertainties, aleatoric and ePistemic uncertainties. Aleatoric uncertainty only considers data
uncertainty derived from statistical randomness (e.g., inherent noises in observations) while ePistemic
uncertainty indicates model uncertainty due to limited knowledge or ignorance in collected data. On
the other hand, in the belief or evidence theory, Subjective Logic (SL) (Josang et al., 2018) considered
vacuity (or lack of evidence) as uncertainty in an subjective oPinion. Recently other uncertainties such
as dissonance, consonance, vagueness, and monosonance (Josang et al., 2018) are also introduced.
This work is the first that considers multidimensional uncertainty tyPes in both DL and belief theory
domains to Predict node classification and out-of-distribution (OOD) detection. To this end, we
incorPorate the multidimensional uncertainty, including vacuity, dissonance, aleatoric uncertainty, and
ePistemic uncertainty in selecting test nodes for Bayesian DL in GNNs. We Perform semi-suPervised
node classification and OOD detection based on GNNs. By leveraging the modeling and learning
caPability of GNNs and considering multidimensional uncertainties in SL, we ProPose a Bayesian
DL framework that allows simultaneous estimation of different uncertainty tyPes associated with
the Predicted class Probabilities of the test nodes generated by GNNs. We treat the Predictions of a
Subjective Bayesian GNN (S-BGNN) as nodes’ subjective oPinions in a graPh modeled as Dirichlet
distributions on the class Probabilities, and learn the S-BGNN model by collecting the evidence from
the given labels of the training nodes (see Figure 1). This work has the following key contributions:
•	A Subjective Bayesian framework to predictive uncertainty estimation for GNNs. Our Pro-
Posed framework directly Predicts subjective multinomial oPinions of the test nodes in a graPh,
1
Under review as a conference paper at ICLR 2020
Loss =KL(a∖∖a) + KL(p∖∖p] + ∖∖y - p∖∖2
Distribution
Aleatoric
Epistemic
(From Bayesian Framework)
Prior Distribution ∣
I Training Label ∖
Deterministic
GNN
vacuity Dissonance
(From Subjective Logic)
(a) Subjective Bayesian GNN ! (b) GrclPh-BaSeCl Kernel !⑹ TeaCher Network ∣ (d) Square Loss T (e) Multiple Uncertainties
1 Dirichlet Estimation 1	∣	1
Figure 1: Method Overview. our proposed framework is based on (a) Subjective Bayesian GNN designed
for estimating the different types of uncertainties including (e) vacuity, dissonance, aleatoric, and epistemic
uncertainties for the applications in graph data. The loss function includes (d) square error to reduce bias and (b)
(c) two KL components to reduce error in predicting uncertainty.
Categorical
Distribution
ʌ Knowledge
7 Distillation
with the opinions following Dirichlet distributions with each belief probability as a class probability.
Our proposed framework is a generative model, so it cal be highly applicable across all GNNs
and allows simultaneously estimating different types of associated uncertainties with the class
probabilities.
•	Efficient approximate inference algorithms: We propose a Graph-based Kernel Dirichlet distri-
bution Estimation (GKDE) method to reduce error in predicting Dirichlet distribution. We designed
an iterative knowledge distillation algorithm that treats a deterministic GNN as a teacher network
while considering our proposed Subjective Bayesian GNN model (a realization of our proposed
framework for a specific GNN) as a distilled network. This allows the expected class probabilities
based on the predicted Dirichlet distributions (i.e., outputs of our trained Bayesian model) to match
the predicted class probabilities of the deterministic GNN model, along with uncertainty estimated
in the predictions.
•	Comprehensive experiments for the validation of the performance of our proposed frame-
work. Based on six real graph datasets, we compared the performance of our propose framework
with that of other competitive DL algorithms. For a fair comparison, we tweaked the DL algorithms
to consider various uncertainty types in predicted decisions.
2	Related work
Epistemic Uncertainty in Bayesian Deep Learning (BDL): Machine/deep learning (M/DL) re-
search mainly considered aleatoric uncertainty (AU) and epistemic uncertainty (EU) using BNNs
for computer vision applications. AU consists of homoscedastic uncertainty (i.e., constant errors
for different inputs) and heteroscedastic uncertainty (i.e., different errors for different inputs) (Gal,
2016). A BDL framework was presented to estimate both AU and DU simultaneously in regression
settings (e.g., depth regression) and classification settings (e.g., semantic segmentation) (Kendall
& Gal, 2017). Later, a new type of uncertainty, called distributional uncertainty (DU), is defined
based on distributional mismatch between the test and training data distributions (Malinin & Gales,
2018). Dropout variational inference (Gal & Ghahramani, 2016) is used as one of key approximate
inference techniques in BNNs. Other methods (Eswaran et al., 2017; Zhang et al., 2018) measure
overall uncertainty in node classification but didn’t consider uncertainty decomposition and GNNs.
Uncertainty Quantification in Belief/Evidence Theory: In the belief/evidence theory domain, un-
certainty reasoning has been substantially explored, such as Fuzzy Logic (De Silva, 2018), Dempster-
Shafer Theory (DST)(SentZ et al., 2002), or Subjective LogiC(SL)(J0sang, 2016). Belief theory
focuses on reasoning of inherent uncertainty in information resulting from unreliable, incomplete,
deceptive, and/or conflicting evidence. SL considered uncertainty in subjective opinions in terms of
vacuity (i.e., lack of evidence) and vagueness (i.e., failing in discriminating a belief state) (J0sang,
2016). Recently, other uncertainty types have been studied, such as dissonance (due to conflicting
evidence) and consonance (due to evidence supporting composite states) (Josang et al., 2018).
In deep NNs, SL is considered to train a deterministic NN for supervised classification in computer
vision applications (Sensoy et al., 2018). However, they didn’t consider a generic way of estimating
multidimensional uncertainty using Bayesian DL for GNNs used for the applications in graph data.
2
Under review as a conference paper at ICLR 2020
3	Proposed Approach
Now we define the problem of uncertainty-aware semi-supervised node classification and then present
a Bayesian GNN framework to address the problem.
3.1	Problem Definition
Given an input graph G = (V, E, r, y%), where V = {1,…，N} is a ground set of nodes, E ⊆ V X V
is a ground set of edges, r = [ri, •…，rN]T ∈ RN×d is a node-level feature matrix, r ∈ Rd is
the feature vector of node i, yL = {yi | i ∈ L} are the labels of the training nodes L ⊂ V, and
yi ∈ {1, . . . , K} is the class label of node i. We aim to predict: (1) the class probabilities of
the testing nodes: pV\L = {pi ∈ [0, 1]K | i ∈ V \ L}; and (2) the associated multidimensional
uncertainty estimates introduced by different root causes: uV\L = {ui ∈ [0, 1]m | i ∈ V \ L},
where pi,k is the probability that the class label yi = k and m is the total number of uncertainty types.
3.2	Multidimensional Uncertainty Quantification
Multiple uncertainty types may be estimated, such as aleatoric uncertainty, epistemic uncertainty,
vacuity, dissonance, among others. The estimation of the first two types of uncertainty relies on the
design of an appropriate Bayesian DL model with parameters, θ. Following (Gal, 2016), node i’s
aleatoric uncertainty is: Aleatoric[pi] = EProb⑹。)[H(y∕r; θ)], where H(∙) is Shannon,s entropy
of Prob(pi|r; θ). The epistemic uncertainty of node i is estimated by:
Epistemic[pi] = H [Eprob(θ∣G) [(yi |r； θ)]] — Eprob(θ∣G) [H(y∕r; θ)]
(1)
where the first term indicates entropy (or total uncertainty).
Vacuity and dissonance can be estimated based on the subjective opinion for each testing node
i (Josang et al., 2018). Denote i,s subjective opinion as [bn,…,biκ, vi∖, where bik(≥ 0) is the
belief mass of the k-th category, vi (≥ 0) is the uncertainty mass (i.e., vacuity), and K is the total
number of categories, where PkK=1 bik + vi = 1. i’s dissonance is obtained by:
K
ω(bi) = X
k=1
PjK=1,j 6=k bij
(2)
where the relative mass balance between a pair of belief masses bij and bik is expressed by
Bal(bij, bik) = 1 - |bij - bik |/(bij + bik). To develop a Bayesian GNNs framework that pre-
dicts multiple types of uncertainty, we estimate vacuity and dissonance using a Bayesian model.
In SL, a multinomial opinion follows a Dirichlet distribution, Dir(Pi∣α∕ where α% ∈ [1, ∞]K
represents the distribution parameters. Given Si = PkK=1 αik, belief mass bi and uncertainty mass
vi can be obtained by bik = (αik - 1)/Si and vi = K/Si.
3.3	Proposed Bayesian Deep Learning Framework
Let p = [p1, . . . , pN]> ∈ RN×K denote the class probabilities of the node in V, where pi =
[pi1, . . . ,piK]> refers to the class probabilities of a specific node i. As shown in Figure 1, our
proposed Bayesian GNN framework can be described by the generative process:
•	Sample θ from a predefined prior distribution, i.e., N (0, I).
•	For each node i ∈ V: (1) Sample the class probabilities Pi from a Dirichlet distribution: Dir(Pi ∣αj,
where αi = fi(r; θ) is parameterized by a GNN network α = f(r; θ) : RN×d → [1, ∞]N×K
that takes the attribute matrix r as input and directly outputs all the node-level Dirichlet parameters
α = [αι, ∙∙∙ , αN], and θ refer to the hyper-parameters of the GNN network; and (2) Sample
yi 〜 Cat(y∕Pi), a categorical distribution on Pi.
In this design, the graph dependencies among the class labels in yL and yV\L are modeled via the
GNN network f (r; θ). Our proposed framework is different from the traditional Bayesian GNN
network (Zhang et al., 2018) in that the output of the former are the parameters of node-level Dirichlet
distributions (α), but the output of the latter are directly node-level class probabilities (p). The
conditional probability of P, Prob(P|r; θ), can be obtained by:
N
PrOb(PHθ) = ɪɪ=IDir(PiIai), αi = fi(r;θ)
where the Dirichlet probability function Dir(PiIai) is defined by:
Dir(PiM=QKESb YK=ιPakik-i, Si=XK=ι αik
(3)
(4)
3
Under review as a conference paper at ICLR 2020
Based on the proposed Bayesian GNN framework, the joint probability of y conditioned on the input
graph G and the node-level feature matrix r can be estimated by:
Prob(y|r;G) =
Prob(y∣p)Prob(p∣r; θ)Prob(θ∣G)dpdθ,
(5)
where Prob(θ∣G) is the posterior probability of the parameters θ conditioned on the input graph G,
which are estimated in Sections 3.4 and 3.6.
The aleatoric uncertainty and the epistemic uncertainty can be estimated using the equations de-
scribed in Section 3.2. The vacuity associated with the class probabilities (pi) of node i can be
estimated by: Vacuity(pi)
is estimated as: Disso.[pi]
EProb(θ∣G)[vi] = EProb(θ∣G) [K/ Pk=I αik . The dissonance of node i
EProb(θ∣G) [ω(b∕], where ω(bi) is defined in Eq. (2).
3.4	Bayesian Inference with Dropout
The marginalization in Eq. (5) is generally intractable. A dropout technique is used to obtain an
approximate solution and use samples from the posterior distribution of models (Gal & Ghahramani,
2016). Due to this reason, we adopt a dropout technique in (Gal & Ghahramani, 2015) for variational
inference in Bayesian CNNs where Bernoulli distributions are assumed over the network’s weights.
This dropout technique allows us to perform probabilistic inference over our Bayesian DL framework
using GNNs. For Bayesian inference, we identify a posterior distribution over the network’s weights,
given the input graph G and observed labels yL by Prob(θ | G), where θ = {W1, . . . , WL, b1, ..., bL},
L is the total number of layers and Wi refers to the GNN’s weight matrices of dimensions Pi × Pi-1,
and bi is a bias vector of dimensions Pi for layer i = 1,…，L.
Since the posterior distribution is intractable, we use a variational inference to learn q(θ, γ), a
distribution over matrices whose columns are randomly set to zero, approximating the intractable
posterior by minimizing the Kullback-Leibler (KL)-divergence between this approximated distribution
and the full posterior, which is given by:
min KL(q(θ, γ) ∣∣Prob(θ∣G))	(6)
γ
where γ = {M1, . . . , ML, m1, . . . , mL} are the variational parameters, where Mi ∈ RPi×Pi-1 and
mi ∈ RPi . We define Wi in q(θ, γ) by:
Wi = Midiag ([zj] P= J, Zij 〜BernOUni(di) for i = 1,...,L,j = 1,...,Pi-ι	(7)
where d = {d1, . . . , dL} is the dropout probabilities with zij of Bernoulli distributed random
variables. The binary variable zij = 0 corresponds to unit j in layer i - 1 being dropped out as
an input to layer i. We can obtain the approximate model of the Gaussian process from (Gal &
Ghahramani, 2015). The dropout probabilities, di’s, can be optimized or fixed (Kendall et al., 2015).
For simplicity, we fix di ’s in our experiments, as it is beyond the scope of our study. In (Gal &
Ghahramani, 2015), the minimization of the cross entropy (or square error) loss function is proven
to minimize the KL-divergence (see Eq. (6)). Therefore, training the GNN model with stochastic
gradient descent enables learning of an approximated distribution of weights, which provides good
explainability of data and prevents overfitting.
For the dropout inference, we performed training a GNN model with dropout before every weight
layer and dropout at test time to sample from the approximate posterior (i.e., stochastic forward
passes, a.k.a. Monte Carlo dropout; see Eq. (8)). At the test stage, we infer the joint probability
Eq. (5) by:
Prob(y|r; G) ≈ ɪ XM [
M	m=1
Prob(y|p)Prob(p|r; θ(m))dp,
θ(m) 〜q(θ),
(8)
which can infer the Dirichlet parameters a as: α ≈ M∙ PM=1 f (r, θ(m)), θ(m) 〜q(θ).
As our model is a generative model to predict Dirichlet distribution parameters, we use a loss function
to compute its Bayes risk with respect to the sum of squares loss ky - pk22 by:
L(Y) = Xn 八M-Pik2 ∙ PrOb(Pi|r； γ)dPi = XyT XK1 (yj - E[PijD2 + Var(Pij)	⑼
i∈L	i∈L	j=1
Eq. (9) aims to minimize the prediction error and variance, leading to maximizing classification
accuracy of each training node by removing excessive misleading evidence (Sensoy et al., 2018).
4
Under review as a conference paper at ICLR 2020
3.5	Graph-Based Kernel Dirichlet Distribution Estimation
To better learn the Dirichlet distribution from our Bayesian GNN framework, we proposed a Graph-
Based Kernel Dirichlet Distribution Estimation (GKDE). The key idea of GKDE is estimating prior
Dirichlet distribution parameters for each node based on training nodes (see Figure 1 (b)). And then,
we leave prior Dirichlet distribution in the training process to learn two trends: (i) nodes with high
vacuity (due to lack of evidence) will be shown far from training nodes; and (ii) nodes with high
dissonance (due to conflicting evidence) will be shown in the class boundary.
Based on SL, let each training node represent one evidence for its class label. Denote the contribution
of evidence estimation for target node j from node i by h(yi, dis(i, j)) = [h1 , . . . , hk, . . . , hK] ∈
[0, 1]K and hk (yi , dis(i, j )) is obtained by:
hk(yi,dis(i,jy) = 0Q k (G /.节 yi = k	(IO)
[σ√2π ∙ g(dis(i,j)) yi = k
where g(dis(i, j ))
1 e
σ√2π
dis(i,j)
~2^-
is the Gaussian kernel function to estimate the distribution
effect between nodes i and j, and dis(i,j) means the node distance (shortest path between nodes
i and j), and σ is the bandwidth parameter. The prior evidence estimation based GKDE: e^j =
Pi∈L h(yi, dis(i, j)), and the prior Dirichlet distribution α^j = ^j + 1. During training process, we
minimize the KL-divergence between model predictions of Dirichlet distribution and prior distribution:
min KL[Dir(α)kDir(α)].

3.6	A Teacher Network for Refined Inference
Our key contribution is that the proposed Bayesian GNN model is capable of estimating various
uncertainty types to predict existing GNNs. As one of the preferred features, the expected class
probabilities generated by our Bayesian GNNs model should be consistent with the predicted class
probabilities of the GNN model. In addition, our Bayesian GNN model is a generative model and
may not necessarily always outperform GNN models (i.e., discriminative models) for the task of node
classification prediction when uncertainty-based prediction is not fully benefited.
To refine the inference of our proposed model, we leverage the principles of Knowledge Distillation
in DL (Hinton et al., 2015). In particular, we consider our proposed model as a distilled model
and a deterministic GNN model as a teacher model, as shown in Figure 1 (c). The key idea is to
train our proposed model to imitate the outputs of the teacher network on the class probabilities
while minimizing the loss function of our proposed model. We observed that the modeling of data
uncertainty in our proposed model provides useful information to further improve the accuracy of the
deterministic GNN model. Therefore, we consider propagating the useful information back to the
teacher model to help train itself.
Let us denote Prob(y | r; β) as the joint probability of class labels via a deterministic GNN model,
where β refers to model parameters. The probability function Prob(y | r; γ, G) is estimated based on
Eq. (8) using the variational parameters γ. We measure the closeness between Prob(y | r; β) and
Prob(y | r; γ, G ) with KL-divergence to be minimized while minimizing their own loss functions
based on the labeled nodes. This leads to solving the following optimization problem:
minγ,β L(Y) + L(β) + λ ∙(KL[Prob(y | r; γ, G) ∣∣ Prob(y | r; β)] + KL[Dir(α)∣∣Dir(α)])	(11)
where L(β) is the loss function (i.e., cross entropy) of the deterministic GNN model and λ is a
trade-off parameter. Our inference algorithm using backpropagation is detailed in the Appendix.
4	Experiments
In this section, we describe our experimental settings and demonstrate the performance of our
proposed model based on semi-supervised node classification. For the performance comparison and
analysis of our model and other existing counterparts, we demonstrate and analyze the obtained
results in terms of the overall classification accuracy.
4.1	Datasets
We use six datasets, including three citation network datasets (Sen et al., 2008) (i.e., Cora, Citeseer,
Pubmed) and three new datasets (Shchur et al., 2018) (i.e., Coauthor Physics, Amazon Computer,
and Amazon Photo). We summarize the description and experimental setup of the used datasets in
Table 1. For all the used datasets, we deal with undirected graphs with 20 training nodes for each
5
Under review as a conference paper at ICLR 2020
Table 1: Description of datasets and their experimental setup for the node classification prediction.
	Cora	Citeseer	Pubmed	Co. Physics	Ama.Computer	Ama.Photo
#Nodes	2,708	3,327	19,717	34, 493	13, 381	7, 487
#Edges	5,429	4,732	44,338	282, 455	259, 159	126, 530
#Classes	7	6	3	5	10	8
#Features	1,433	3,703	500	8,415	767	745
#Training nodes	140	120	60	100	200	160
#Validation nodes	500	500	500	500	500	500
#Test nodes	1,000	1,000	1,000	1000	1,000	1000
category. We chose the same dataset splits as in (Yang et al., 2016) with an additional validation
node set of 500 labeled examples for the hyperparameter obtained from the citation datasets, and
followed the same dataset splits in (Shchur et al., 2018) for Coauthor Physics, Amazon Computer,
and Amazon Photo datasets, for fair comparison.
4.2	Comparing Schemes
We conduct the extensive comparative performance analysis based on our proposed models and a
number of other state-of-the-art counterparts. Our proposed Subjective GNN models are: (1) S-GNN
(Subjective GNN) with vacuity and dissonance estimation, which outputs subjective opinion (Dirichlet
distribution) instead of softmax probability; (2) S-BGNN (Subjective Bayesian GNN), Subjective
Graph Neural Network with Bayesian framework with multiple type uncertainty estimation; (3)
S-BGNN-T (S-BGNN with Teacher network), help improve the expected class probability estimation;
(4) S-BGNN-T-K (S-BGNN-T with GKDE), help improve the Dirichlet distribution estimation. Here
We use two popular GNN models: GCN (KiPf & Welling, 2016) and GAT (VelickoVic et al., 2018).
Our proposed models are compared against a
number of the state-of-the-art counterparts. For
eValuating three citation datasets (i.e., Cora,
Citeseer, Pubmed), we compared our mod-
els with: (1) GCN (Kipf & Welling, 2016);
(2) GAT (Velickovic et al., 2018); (3) non-
parametric Bayesian GCNN (BGCNN) (Pal
et al., 2019); (4) Bayesian GCN (Zhang et al.,
2019); (5) MC-dropout for Bayesian GNN
(GCN-Drop, GAT-Drop) (Ryu et al., 2019);
(6) skip-gram based graph embeddings (Deep-
Walk) (Perozzi et al., 2014); (7) iterative classi-
fication algorithm (ICA) (Lu & Getoor, 2003);
and (8) Planetoid (Yang et al., 2016). We se-
lected these for the comparison with our mod-
els based on (Velickovic et al., 2018) for fair
comparison with the latest comparable models.
Using Coauthor Physics, Amazon Computer,
and Amazon Photo, we compared the perfor-
mance of our models with that of GCN and
GAT(Shchur et al., 2018), we can not show S-
GAT (S-BGAT) due to memory limited, More
details of model setup refer to the Appendix A.
Table 2: Semi-supervised node classification accuracy.
	Cora	Citeseer	Pubmed
DeepWalk	67.2	43.2	65.3
ICA	75.1	69.1	73.9
Planetoid	75.7	64.7	77.2
GCN	81.5	70.3	79.0
Bayesian GCN	81.2 ± 0.8	72.2 ±0.6	76.6 ± 0.7
BGCNN	80.3 ± 0.6	72.6 ± 0.6	79.2 ± 0.5
GCN-Drop	81.3 ± 0.7	70.9 ± 0.5	79.0 ± 0.3
S-GCN	81.5 ± 1.0	71.2 ±0.6	79.0 ± 0.2
S-BGCN	81.2 ± 1.0	71.0 ± 0.6	79.0 ± 0.2
S-BGCN-T	82.2 ± 0.6	71.3 ±0.6	79.2 ± 0.4
S-BGCN-T-K	82.0 ± 0.6	71.0 ± 0.6	79.3 ± 0.3
GAT	83.0 ± 0.7	72.5 ± 0.7	79.0 ± 0.3
GAT-Drop	82.8 ± 0.8	72.6 ± 0.7	79.0 ± 0.3
S-GAT	83.0 ± 0.7	72.6 ± 0.6	79.0 ± 0.3
S-BGAT	82.9 ± 0.7	72.4 ± 0.7	78.9 ± 0.3
S-BGAT-T	83.7 ± 0.6	73.2 ± 0.5	79.1 ± 0.2
S-BGAT-T-K	83.8 ± 0.7	73.0 ± 0.7	79.1 ± 0.2
	Co.Physics	Ama.Computer	Ama.Photo
GAT*	92.5 ± 0.9	78.0 ± 19.0	85.7 ± 20.3
GCN*	92.8 ± 1.0	82.6 ± 2.4	91.2 ± 1.2
GCN	93.0 ± 0.8	79.7 ± 1.3	91.6 ± 1.2
GCN-Drop	93.0 ± 0.7	79.6 ± 1.2	91.3 ± 1.0
S-GCN	93.1 ± 0.8	78.9 ± 1.6	90.4 ± 1.4
S-BGCN	93.1 ± 0.7	78.3 ± 1.6	90.2 ± 1.6
S-BGCN-T	93.2 ± 0.8	84.1 ± 1.3	92.3 ± 1.2
S-BGCN-T-K	93.0 ± 0.8	84.0 ± 1.2	92.0 ± 1.3
GCN* and GAT* are implemented from (Shchur et al., 2018)
4.3	Experimental Results & Analysis
In Table 2, we summarized the mean percentage of classification accuracy with a standard deviation
of each model compared in this experiment. The results prove that our model achieves the best
accuracy result across all datasets except Citeseer. To be specific, our proposed S-BGCN-T is able
to improve over GCN by a margin of 0.7%, 1.2%, 0.2%, 0.2%, 4.5% and 0.7% on Cora, Citeseer,
Pumbed, Coauthor Physics, Amazon Computer, and Amazon Photo, respectively. In addition, our
proposed S-BGAT-T model improves 0.8% for both Cora and Citeseer datasets over GAT. Notice that
S-BGNN-T even outperforms S-BGNN particularly on the Cora and Citeseer dataset (i.e., 1% - 1.3%
increase). These results prove that the teacher network can prevent overfitting, leading to a further
improvement in classification prediction.
6
Under review as a conference paper at ICLR 2020
(a) PR curves on Cora
(b) PR curves on Citeseer
(c) PR curves on Pubmed
Figure 2: PR curves for Node Classification Prediction: S-BGCN-T-K is used to predict node
classification where test nodes are selected based on the low extent of uncertainty. The PR curves of
all compared models can be found in the Appendix.
5 Uncertainty Experiment and Analysis
In Section 4, we showed that our S-BGNN-T improves prediction performance. In this section, we
study the effectiveness of prediction based on different types of uncertainty. We studied the different
types of uncertainty-aware node classification and out-of-distribution in terms of the area under
the ROC (AUROC) and Precision-Recall (AUPR) curves in both experiments as in (Hendrycks &
Gimpel, 2016) for three citation network datasets. For the OOD detection, we randomly selected
1-3 categories as OOD categories and trained the models only based on training nodes of the other
categories. Due to the space constraint, we summarize the description of datasets and experimental
setup for the OOD detection in the Appendix.
To better evaluate our multiple uncertainties, we compare our model with two baseline models: (1)
GCN which uses GCN (Kipf & Welling, 2016) with the softmax probability entropy measuring
uncertainty; and (2) GCN-Drop, where one of the two uncertainty types (i.e., aleatoric, or epistemic
uncertainty) adapts Monte-Carlo Dropout (Gal & Ghahramani, 2016) into the GCN model (Ryu et al.,
2019). In the OOD, we also consider Distributional uncertainty (Malinin & Gales, 2018).
5.1	Quality of Uncertainty Metrics
In Figure 2, we used S-BGCN-T-K to pre-
dict node classification when test nodes
are selected based on the lowest uncer-
tainty for a given type. First of all, all
uncertainty types show decreasing pre-
cision as recall increases. This implies
that all uncertainty types are to some ex-
tent the indicators of prediction accuracy
because low uncertainty increases predic-
tion accuracy. In Figure 2, we can ob-
serve almost 100% performance of pre-
cision when recall is close to zero on
Cora and over 95% on Pubmed. Further,
Table 3: Node classification prediction in AUPR.
Data	Model	AUPR
		Va.	Dis.	AL	Ep.	En.
	S-BGCN-T-K	-904954926^^880931-
	S-BGCN-T	89.5	94.4	91.9	85.3	92.2
Cora	GCN-Drop.	-	-	92.7	90.0	93.6
	GCN	-	-	-	-	94.1
	S-BGCN-T-K	^98	856	822	752	83.5-
Citeseer	S-BGCN-T	80.0	85.6	82.4 76.6	83.6
	GCN-Drop.	-	-	82.3 77.8	83.7
	GCN	-	-	-	-	83.2
	S-BGCN-T-K	-856~~909^^889^^860^^893-
	S-BGCN-T	84.8	90.2	88.8	85.1	89.3
Pubmed	GCN-Drop.	-	-	88.6	85.6	89.0
	GCN	-	-	-	-	89.2
Va.: Vacuity, Dis.: Dissonance, Al.: Aleatoric, Ep.: Epistemic, En.: Entropy
the outperformance of Dissonance uncer-
tainty is obvious among all. This indicates that low uncertainty with few conflicting evidence is the
most critical factor to enhance classification prediction accuracy, compared to low extent of other
uncertainty types. In addition, although epistemic uncertainty was very low, epistemic uncertainty
performs the worst among all. This also indicates that epistemic uncertainty is not necessarily helpful
to enhance prediction accuracy in semi-supervised node classification. Lastly, we found that vacuity
is not as important as dissonance because accurate prediction is not necessarily dependent upon a
large amount of information, but is more affected by less conflicting (or more agreeing) evidence to
support a single class.
In Table 3, although all BGCN-T models with the five different uncertainty types do not necessarily
outperform all the existing models (i.e., GCN Entropy and variants of GCN-Drop.), the outperfor-
mance of Dissonance is fairly impressive. This result confirmed that low uncertainty caused by
dissonance is the key to maximize node classification prediction accuracy. When compare S-BGCN-T
and S-BGCN-T-K, we found GKDE can only help improve performance a little. To better understand
different uncertainty types, we used t-SNE (Maaten & Hinton, 2008) to represent the computed
7
Under review as a conference paper at ICLR 2020
Figure 3: Graph embed-
ding representations of the
Cora dataset for classes
and the extent of uncer-
tainty: (a) shows the rep-
resentation of seven differ-
ent classes; (b) shows our
model prediction; and (c)-
(f) present the extent of
uncertainty for respective
uncertainty types, includ-
ing vacuity, dissonance,
aleatoric, epistemic.
feature representations of a pre-trained S-BGCN-T-K model’s first hidden layer on the Cora dataset
in Figure 3.
5.2	Out-of-Distribution Detection
In this section, we discuss how different uncertainty types can prove the performance in the out-
of-distribution (ODD) detection. In Table 4, we considered 6 uncertainties with 3 models for our
performance comparison. Note that Distributional uncertainty is the the most recent model showing
the best performance in the OOD detection. Across the three citation network datasets, particularly S-
BGCN-T-K Vacuity showed significantly better performance, strikingly outperforming Distributional
uncertainty. Notice that S-BGCN-T-K outperforms S-BGCN-T (i.e., 4% - 7% increase), especially
the improvement of vacuity. These result prove that the GKDE can improve the Dirichlet distribution
estimation, leading to a better uncertainty estimation.
Table 4: AUROC and AUPR for the OOD detection.
Data	Model	AUROC						AUPR					
		Va.	Dis.	Al.	Ep.	D.En.	En.	Va.	Dis.	Al.	Ep.	D.En.	En.
	S-BGCN-T-K	87.6	75.5	85.5	70.8	85.1	84.8	78.4	49.0	75.3	44.5	73.8	73.1
	S-BGCN-T	84.5	81.2	83.5	71.8	84.1	83.5	74.4	53.4	75.8	46.8	70.8	71.7
Cora	GCN-Drop.	-	-	81.9	70.5	-	80.9	-	-	69.7	44.2	-	67.2
	GCN	-	-	-	-	-	80.7	-	-	-	-	-	66.9
	S-BGCN-T-K	84.8	55.2	78.4	55.1	79.1	74.0	86.8	54.1	80.8	55.8	81.3	74.0
	S-BGCN-T	78.6	59.6	73.9	56.1	75.1	69.3	79.8	57.4	76.4	57.8	78.3	69.3
Citeseer	GCN-Drop.	-	-	72.3	61.4	-	70.6	-	-	73.5	60.8	-	70.0
	GCN	-	-	-	-	-	70.8	-	-	-	-	-	70.2
	S-BGCN-T-K	74.6	67.9	71.8	59.2	69.7	72.2	69.6	52.9	63.6	44.0	64.8	56.5
	S-BGCN-T	71.8	68.6	70.0	60.1	68.0	70.8	65.7	53.9	61.8	46.0	62.9	55.1
Pubmed	GCN-Drop.	-	-	68.7	60.8	-	66.7	-	-	59.7	46.7	-	54.8
	GCN	-	-	-	-	-	68.3	-	-	-	-	-	55.3
Va.: Vacuity, Dis.: Dissonance, Al.: Aleatoric, Ep.: Epistemic, D.En.: Differential Entropy, En.: Entropy
6 Conclusion
In this work, we proposed a Subjective Bayesian GNNs framework for uncertainty-aware semi-
supervised node classification and out-of-distribution (OOD) detection for GNNs. Our proposed
framework provides an effective, efficient way of predicting node classification and detecting OOD
considering multiple uncertainty types. We leveraged the estimation of various types of uncertainty
from both DL and evidence/belief theory domains. In addition, We leveraged the Teacher network to
help refine the classification probability and GKDE to accuractly estimate Dirichlet distribution.
The key findings from this study include:
•	For the overall classification prediction, our proposed S-BGNN-T outperformed the competitive
baselines on most datasets. The key role to improve the accuracy is Teacher Network.
•	For the node classification prediction considering various uncertainty types, we found that disso-
nance (i.e., uncertainty derived from conflicting evidence) played a significant role to improve
classification prediction accuracy.
•	For the OOD detection, vacuity uncertainty played a key role when S-BGCN-T-K is used to detect
OOD. This means that less information and/or more randomness (or less predictability) enables
detecting OOD more effectively. More impressively, GKDE can indeed help to estimate Dirichlet
distribution accurately so that enhance the vacuity performance. Also vacuity outperformed the
most recent counterpart, Distributional uncertainty.
8
Under review as a conference paper at ICLR 2020
References
Clarence W De Silva. Intelligent control: fuzzy logic applications. CRC press, 2018.
DhivyaEsWaran, StePhan Gunnemann, and Christos Faloutsos. The power of certainty: A dirichlet-
multinomial model for belief propagation. In Proceedings of the 2017 SIAM International Confer-
ence on Data Mining, pp. 144-152. SIAM, 2017.
Yarin Gal. Uncertainty in deep learning. University of Cambridge, 2016.
Yarin Gal and Zoubin Ghahramani. Bayesian convolutional neural networks with bernoulli approxi-
mate variational inference. arXiv preprint arXiv:1506.02158, 2015.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In ICML, pp. 1050-1059, 2016.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In AISTATS, pp. 249-256, 2010.
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
AudunJ0sang. Subjective logic. Springer, 2016.
Audun Josang, Jin-Hee Cho, and Feng Chen. Uncertainty characteristics of subjective opinions. In
FUSION, pp. 1998-2005. IEEE, 2018.
Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer
vision? In NIPS, pp. 5574-5584, 2017.
Alex Kendall, Vijay Badrinarayanan, and Roberto Cipolla. Bayesian segnet: Model uncertainty
in deep convolutional encoder-decoder architectures for scene understanding. arXiv preprint
arXiv:1511.02680, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
arXiv preprint arXiv:1609.02907, 2016.
Qing Lu and Lise Getoor. Link-based classification. In ICML, pp. 496-503, 2003.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. JMLR, 9(Nov):2579-2605,
2008.
Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. arXiv preprint
arXiv:1802.10501, 2018.
Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based
recommendations on styles and substitutes. In SIGIR, pp. 43-52. ACM, 2015.
Soumyasundar Pal, Florence Regol, and Mark Coates. Bayesian graph convolutional neural networks
using non-parametric graph learning. arXiv preprint arXiv:1910.12132, 2019.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representa-
tions. In KDD, pp. 701-710. ACM, 2014.
Seongok Ryu, Yongchan Kwon, and Woo Youn Kim. Uncertainty quantification of molecular property
prediction with bayesian neural networks. arXiv preprint arXiv:1903.08375, 2019.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93, 2008.
9
Under review as a conference paper at ICLR 2020
Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classification
uncertainty. In NIPS,pp. 3183-3193, 2018.
Kari Sentz, Scott Ferson, et al. Combination of evidence in Dempster-Shafer theory, volume 4015.
Citeseer, 2002.
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and StePhan Gunnemann. Pitfalls
of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.
Petar Velickovic, Guillem CUCUrUlL Arantxa Casanova, Adriana Romero, Pietro Lid, and Yoshua
Bengio. GraPh Attention Networks. ICLR, 2018.
Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-suPervised learning with
graPh embeddings. arXiv preprint arXiv:1603.08861, 2016.
Yingxue Zhang, Soumyasundar Pal, Mark Coates, and Deniz Ustebay. Bayesian graph convolutional
neural networks for semi-suPervised classification. arXiv preprint arXiv:1811.11103, 2018.
Yingxue Zhang, Soumyasundar Pal, Mark Coates, and Deniz Ustebay. Bayesian graph convolutional
neural networks for semi-supervised classification. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 33, pp. 5829-5836, 2019.
10
Under review as a conference paper at ICLR 2020
A Appendix
A.1 Source code
For review purpose, the source code and datasets are accessible at https://www.dropbox.
com/sh/cs5gs2i1umdx4b6/AAC- r_EYRw9lryk95giqW8-Fa?dl=0
Description of Datasets
Cora, Citeseer, and Pubmed (Sen et al., 2008): These are citation network datasets, where a network
is a directed network where a node represents a document and an edge is a citation link, meaning
that there exists an edge when A document cites B document, or vice-versa with a direction. Each
node’s feature vector contains a bag-of-words representation of a document. For simplicity, we don’t
discriminate the direction of links and treat citation links as undirected edges and construct a binary,
symmetric adjacency matrix A. Each node is labeled with the class to which it belongs.
Coauthor Physics, Amazon Computers, and Amazon Photo (Shchur et al., 2018): Coauthor
Physics is the dataset for co-authorship graphs based on the Microsoft Academic Graph from the
KDD Cup 2016 Challenge1. In the graphs, a node is an author and an edge exists when two authors
co-author a paper. A node’s features represent the keywords of its papers and the node’s class label
indicates its most active field of study. Amazon Computers and Amazon Photo are the segments
of an Amazon co-purchase graph (McAuley et al., 2015), where a node is a good (i.e., product),
an edge exists when two goods are frequently bought together. A node’s features are bag-of-words
representation of product reviews and the node’s class label is the product category.
In semi-supervised node classification task,the training and test nodes are selected based on (Sen
et al., 2008) for the citation network datasets and are randomly selected for the Coauthor Physics,
Amazon Computers, and Amazon Photo.
A.2 Experimental Setup for Out-of-Distribution (OOD) Detection
For OOD detection, we summarize the experiment setup for the use of the three citation network
datasets (i.e., Cora, Citeseer, and Pubmed) in Table 5. In this setting, we still focus on the semi-
supervised node classification task, but only part of node categories are not using for training. Hence,
we suppose that our model only outputs partial categories (as we don’t know the OOD category).
For example, Cora dataset, we train the model with 80 nodes (20 nodes for each category) with the
predictions of 4 categories. Positive ratio is the ratio of out-of-distribution nodes among on all test
nodes.
Dataset	Cora	Citeseer	Pubmed
#Number of training categories	4	3	2
#Training nodes	80	60	40
#Test nodes	1000	1000	1000
#Positive ratio	38%	55%	40.4%
Table 5: Description of datasets and their experimental setup for the OOD detection.
A.3 Calculation of AUPR and AUROC
For the calculation of precision, recall, TPR, and FPR, we select a certain φ % of nodes out of test
nodes to label them as positive (correct) based on the extent of uncertainty, the lowest uncertainty for
classification prediction and the highest uncertainty for OOD detection. And the remaining test nodes
(i.e., 100 - φ %) are labeled as negative. Each test node’s prediction is checked with its ground truth
to derive AUPR and AUROC.
1KDD Cup 2016 Dataset: Online Available at https://kddcup2016.azurewebsites.net/
11
Under review as a conference paper at ICLR 2020
Time Complexity Analyze
BGCN has a similar time complexity with GCN while BGCN-T has the double complexity of GCN.
In the revised paper, we will add a table showing Big-O complexity of all schemes considered. For
a given network where |V| is the number of nodes, |E| is the number of edges, C is the number of
dimensions of the input feature vector for every node, and F is the number of features for the output
layer, the complexity of the compared schemes are: O(|E|C F) for GCN, O(|E|C F) for S-BCGN,
O(2|E|C F) for S-BCGB-T and S-BCGB-T-K, O(|V|CF + |E|F) forGAT,andO(2|V|CF+2|E|F)
for S-BGAT-T and S-BGAT-T-K.
A.4 Model Setups for semi-supervised node classification
Our models are initialized using Glorot initialization (Glorot & Bengio, 2010) and trained to minimize
loss using the Adam SGD optimizer (Kingma & Ba, 2014). As our proposed models (i.e., S-BGCN-
T-K, S-BGAT-T-K) need a discriminative model to refine inference, we use standard GCN and GAT
models as teacher networks for S-BGCN-T-K and S-BGAT-T-K, respectively. For the S-BGCN-T-K
model, we use the early stopping strategy (Shchur et al., 2018) on Coauthor Physics, Amazon
Computer and Amazon Photo datasets while non-early stopping strategy is used in citation datasets
(i.e., Cora, Citeseer and Pubmed). We set bandwidth σ = 1 for all datasets in GKDE, and set trade
off parameters λ = min(1, t/200) (where t is the index of a current training epoch); and other
hyperparameter configurations are summarized in Table 6. The S-BGAT-T-K model has two dropout
probabilities, which are a dropout on features and a dropout on attention coefficients, as showed in
Table 7. We changed the dropout on attention coefficients to 0.4 at the test stage and set trade off
parameters λ = min(1,t∕50), using the same early stopping strategy (VelickoVic et al., 2018). Note
that lack of memory (we used one Titan X GPU, 12 GB memory), we could not obtain the result for
GAT (also S-BGAT) on Coauthor Physics, Amazon Computer and Amazon Photo, which are Very
dense datasets.
For semi-superVised node classification, we use 50 random weight initialization for our models on
Citation network datasets. For Coauthor Physics, Amazon Computer and Amazon Photo datasets,
we report the result based on 10 random train/Validation/test splits. In both effect of uncertainty on
classification prediction accuracy and the OOD detection, we report the AUPR and AUROC results in
percent aVeraged oVer 50 times of randomly chosen 1000 test nodes in all of test sets (except training
or Validation set) for all models tested on the citation datasets. For S-BGCN-T-K model in these
tasks, we use the same hyperparameter configurations as in Table 6, except S-BGCN-T-K Epistemic
using 20,000 epochs to obtain the best result. For baseline models, GCN-Drop. models use the same
hyperparameters as in Table 6 to achieVe the best performance, also using 20,000 training epochs
for GCN-Drop. Epistemic. GCN Entropy uses the same hyperparameter configurations in (Kipf &
Welling, 2016).
	Cora	Citeseer	Pubmed	Co.Physics	Ama.Computer	Ama.Photo
Hidden units	16	16	16	64	64	64
Learning rate	0.01	0.01	0.01	0.01	0.01	0.01
Dropout	0.5	0.5	0.5	0.1	0.2	0.2
L2 reg.strength	0.0005	0.0005	0.0005	0.001	0.0001	0.0001
Monte-Carlo samples	500	500	500	100	100	100
Max epoch	200	200	200	100000	100000	100000
Table 6: Hyperparameter configurations of S-BGCN-T-K model.
12
Under review as a conference paper at ICLR 2020
	Cora	Citeseer	Pubmed
Hidden units	64	64	64
Learning rate	0.01	0.01	0.01
Dropout	0.6/0.6	0.6/0.6	0.6/0.6
L2 reg.strength	0.0005	0.0005	0.001
Monte-Carlo samples	100	100	100
Max epoch	100000	100000	100000
Table 7: Hyper-parameters of S-BGAT-T-K model.
A.5 Algorithm for Our Algorithm
1
2
3
4
5
6
7
8
9
10
11
12
13
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Algorithm 1: S-BGNN-T-K with jointly train for Teacher network
Input: G = (V, E, r) and 丫必
Output: pV\L , uV\L
' =0;
Set hyper-parameters;
Initialize the parameters γ, β ;
Calculate the prior DiriChlet distribution Dir(α);
repeat
Forward pass to compute α, Prob(pi |r; G), Prob(yi |r; β) for i ∈ V;
Compute joint probability Prob(y|r; G), Prob(y|r; β);
Backward pass via the chain-rule the calculate the sub-gradient gradient: g(') = ▽㊀L(Θ)
Update parameters using step size η via Θ('+1) = Θ(') — η ∙ g(')
' =' +1;
until convergence
Calculate pV\L, uV\L
return pV\L, uV\L
Algorithm 2: S-BGNN-T-K with pre-train for Teacher network
Input: G = (V, E, r) and 丫必
Output: pV\L , uV\L
' =0;
Set hyper-parameters;
Initialize the parameters γ, β ;
Calculate the prior Dirichlet distribution Dir(α);
Pre-train the Teacher Network to get Prob(y|r; β)
repeat
Forward pass to compute α, Prob(pi |r; G) for i ∈ V;
Compute joint probability Prob(y|r; G);
Backward pass via the chain-rule the calculate the sub-gradient gradient: g(') = ▽㊀L(Θ)
Update parameters using step size η via Θ('+1) = Θ(') — η ∙ g(')
' =' +1;
until convergence
Calculate pV\L, uV\L
return pV\L, uV\L
B	Additional Experiment Results
Further experiment have been run in addition to the uncertainty analysis in section 5. First, we
show the ablation experiment for each compents we proposed. Second, we show more uncertainty
visualization result in network node classification for Citeseer dataset. To better understand the
performance of uncertainty quality clearly for each uncertainty, we show the AUROC and AUPR
curves for all models and uncertainties.
13
Under review as a conference paper at ICLR 2020
B.1	Ablation Experiments
We conducted an additional experiment to ensure the benefit of the teacher network. We anticipate
that the graph kernel prior will improve the estimation accuracy of Dirichlet distribution. However,
due to the space constraint, we didn’t show the classification results without using the graph kernel
prior. In the revised version, we added a detailed ablation study in the revised paper in order to
clearly demonstrate the contribution of the key technical components, including a teacher Network,
Graph kernel Dirichlet Estimation (GKDE) and subjective Bayesian framework. The key findings
obtained from this experiment are: (1) The teacher Network can further improve node classification
accuracy (i.e., 0.2% - 1.5% increase, as shown in Table 2); and (2) GKDE (graph kernel prior) using
the uncertainty estimates can enhance OOD detection (i.e., 4% - 7% increase, as shown in Table 9).
Table 8: Ablation experiment for Node classification prediction in AUPR.
Data	Model	Va.	Dis.	AUPR Al.	Ep.	En.
	S-BGCN-T-K	90.4	95.4	92.6	88.0	93.4
Cora	S-BGCN-T	89.5	94.4	91.9	85.3	92.2
	S-BGCN.	89.4	94.3	91.6	85.3	92.0
	S-GCN	89.4	94.4	-	-	-
	S-BGCN-T-K	79.8	85.6	82.2	75.2	83.5
Citeseer	S-BGCN-T	80.0	85.6	82.4	76.6	83.6
	S-BGCN.	79.7	85.2	82.2	76.2	83.2
	S-GCN	79.8	85.3	-	-	-
	S-BGCN-T-K	85.6	90.9	88.9	86.0	89.3
PUbmed	S-BGCN-T	84.8	90.2	88.8	85.1	89.3
	S-BGCN.	84.4	90.1	88.5	85.1	89.0
	S-GCN	84.6	90.2	-	-	-
Va.: Vacuity, Dis.: Dissonance, Al.: Aleatoric, Ep.: Epistemic, En.: Entropy
Table 9: Ablation exepriemnt on AUROC and AUPR for the OOD detection.
Data	Model	AUROC						AUPR					
		Va.	Dis.	Al.	Ep.	D.En.	En.	Va.	Dis.	Al.	Ep.	D.En.	En.
	S-BGCN-T-K	87.6	75.5	85.5	70.8	^^851-	84.8	78.4	49.0	75.3	44.5	73.8	73.1
Cora	S-BGCN-T	84.5	81.2	83.5	71.8	84.1	83.5	74.4	53.4	75.8	46.8	70.8	71.7
	S-BGCN	84.1	81.4	83.3	71.9	84.0	83.3	74.1	53.8	75.4	47.0	70.3	71.6
	S-GCN	84.2	81.2	-	-	83.9	-	74.2	53.9	-	-	70.6	-
	S-BGCN-T-K	84.8	55.2	78.4	55.1	^^791-	74.0	86.8	54.1	80.8	55.8	81.3	74.0
Citeseer	S-BGCN-T	78.6	59.6	73.9	56.1	75.1	69.3	79.8	57.4	76.4	57.8	78.3	69.3
	S-BGCN	78.5	59.7	73.2	56.5	75.4	69.1	79.6	57.8	76.0	57.9	78.1	69.0
	S-GCN	78.6	60.0	-	-	75.7	-	79.9	57.8	-	-	78.4	-
	S-BGCN-T-K	74.6	67.9	71.8	59.2	^^69.7	72.2	69.6	52.9	63.6	44.0	64.8	56.5
Pubmed	S-BGCN-T	71.8	68.6	70.0	60.1	68.0	70.8	65.7	53.9	61.8	46.0	62.9	55.1
	S-BGCN	71.9	68.3	70.2	60.01	68.2	70.5	65.9	53.6	61.9	46.4	62.6	55.0
	S-GCN	71.7	68.8	-	-	68.8	-	65.8	53.9	-	-	63.0	-
Va.: Vacuity, Dis.: Dissonance, Al.: Aleatoric, Ep.: Epistemic, D.En.: Differential Entropy, En.: Entropy
B.2	Graph Embedding Representations of Different Uncertainty Types
To better understand different uncertainty types, we used t-SNE (t-Distributed Stochastic Neighbor
Embedding (Maaten & Hinton, 2008)) to represent the computed feature representations of a pre-
trained BGCN-T model’s first hidden layer on the Citeseer dataset.
Six Classes on Citeseer Dataset: In Figure 4 (a), a node’s color denotes a class on the Citeseer
dataset where 6 different classes are shown in different colors. Figure 4 (b) is our prediction result.
For Figures 4 (c)-(f), the extent of uncertainty is presented where a blue color refers to the lowest
uncertainty (i.e., minimum uncertainty) while a red color indicates the highest uncertainty (i.e.,
maximum uncertainty) based on the presented color bar. To examine the trends of the extent of
uncertainty depending on either training nodes or test nodes, we draw training nodes as bigger circles
than test nodes. Overall we notice that most training nodes (shown as bigger circles) have low
uncertainty (i.e., blue), which is reasonable because the training nodes are the ones that are already
observed. Now we discuss the extent of uncertainty under each uncertainty type.
Vacuity: In Figure 4 (c), although most training nodes show low uncertainty, we observe majority of
test nodes in the mid cluster show high uncertainty as appeared in red.
14
Under review as a conference paper at ICLR 2020
Figure 4: Graph embedding representations of the Citeseer dataset for classes and the extent of
uncertainty: (a) shows the representation of seven different classes, (b) shows our model prediction
and (c)-(f) present the extent of uncertainty for respective uncertainty types, including vacuity,
dissonance, and aleatoric uncertainty, respectively.
(a) PR curves on Cora
(b) PR curves on Citeseer
Figure 5: PR curves of node classification prediction for all models and uncertainties.
1.00
(c) PR curves on Pubmed
Dissonance: In Figure 4 (d), similar to vacuity, training nodes have low uncertainty. But unlike
vacuity, test nodes are much less uncertain. Recall that dissonance represents the degree of conflicting
evidence (i.e., discrepancy between each class probability). However, in this dataset, we observe a
fairly low level of dissonance and the obvious outperformance of Dissonance in node classification
prediction.
Aleatoric uncertainty: In Figure 4 (e), a lot of nodes show high uncertainty with larger than 0.5
except a small amount of training nodes with low uncertainty. High aleatoric uncertainty positively
affects, showing high performance in OOD detection.
Epistemic uncertainty: In Figure 4 (f), most nodes show very low epistemic uncertainty because
uncertainty derived from model parameters can disappear as they are trained well. Therefore, non-
distinctive low uncertainty for most nodes do not help much to select good test nodes to improve
performance in node classification.
15
Under review as a conference paper at ICLR 2020
Recall
(a) PR curves on Cora
(b) PR curves on Citeseer
(c) PR curves on Pubmed
Figure 6: PR cuves of OOD detection for all models and uncertainties.
(a) ROC curves on Cora	(b) ROC curves on Citeseer	(c) ROC curves on Pubmed
Figure 7: ROC curves of OOD detection for all models and uncertainties.
B.3	PR and ROC curves
AUPRC for the OOD Detection: Figure 6 shows the AUPRC for the OOD detection when S-BGCN-
T-K is used to detect OOD in which test nodes are considered based on their high uncertainty level,
given a different uncertainty type, such as vacuity, dissonance, aleatoric, epistemic, or entropy (or
total uncertainty). Also to check the performance of the proposed models with a baseline model, we
added S-BGCN-T-K with test nodes randomly selected (i.e., Random).
Obviously, in Random baseline, precision was not sensitive to increasing recall while in S-BGCN-T-K
(with test nodes being selected based on high uncertainty) precision decreases as recall increases. But
although most S-BGCN-T-K models with various uncertainty types used to select test nodes shows
sensitive precision to increasing recall (i.e., proving uncertainty being an indicator of improving
OOD detection), S-BGCN-T-K Epistemic even performed worse than the baseline (i.e., S-BGCN-T-K
Random). This is because epistemic uncertainty cannot distinguish the flat Dirichlet distribution
(α = (1, . . . , 1)) from sharp Dirichlet distribution (α = (10, . . . , 10)), which leads to no effective
selection of test nodes for improving the performance in OOD detection. In addition, unlike AUPR
in node classification prediction, which showed the best performance in S-BGCN-T-K Dissonance
(see Figure 5), S-BGCN-T-K Dissonance showed the second worst performance among the proposed
S-BGCN-T-K models with other uncertainty types. This means that less conflicting information does
not help OOD detection. On the other hand, overall we observe Vacuity performs the best among
all while S-BGCN-T-K Entropy also performs fairly well as the third best. From this finding, we
can claim that to improve OOD detection, more randomness with high aleatoric uncertainty and
less information with high vacuity can help boost the accuracy of the OOD detection. Although the
uncertainty level observed from aleatoric uncertainty and entropy was quite similar, the performance
in OOD detection is not necessarily similar, as shown in Figures 6 (b) and (c) on Citeseer and Pubmed.
The reason is that BCGN-T Aleatoric provides test nodes with more distinctive uncertainty levels
while BCGN-T Entropy doesn’t. This is because BCGN-T Entropy combines the aleatoric and
epistemic uncertainty where epistemic uncertainty is mostly highly low, ultimately leading to poor
distinctions of nodes based on different uncertainty levels.
AUROC for the OOD Detection: First, we investigated the performance of our proposed S-BGCN-
T-K models when test nodes are selected based on seven different criteria (i.e., 6 uncertainties and
random). Like AUPR in Figure 5, based on S-BGCN-T-K, we considered a baseline by selecting
16
Under review as a conference paper at ICLR 2020
test nodes randomly while five different uncertainty types are used to select test nodes based on the
order of high uncertainty. For AUROC in Figure 7, we observed much better performance in most
S-BGCN-T-K models with all uncertainty types except epistemic uncertainty. Although epistemic
uncertainty is known to be effective to improve OOD detection (Kendall & Gal, 2017) in computer
vision applications, our result showed fairly poor performance compared to the case other uncertainty
types are used. This is because our experiment is conducted with a very small of training nodes
(i.e., 3% on Cora, 2% on Citeseer, 0.2% on Pubmed) which is highly challenging to observe high
performance particularly with epistemic uncertainty. Recall that we used 200 epochs to train nodes
for all models except BCGN-T Epistemic which was trained with 20,000 epochs. In this experiment,
even S-BGCN-T-K Vacuity performed the best although S-BGCN-T-K Dissonance, S-BGCN-T-K
Aleatoric, or S-BGCN-T-K Entropy performs comparably. But on Citeseer and Pubmed datasets, we
also observed relatively low performance with S-BGCN-T-K Dissonance. This finding is also well
aligned with what we observed in Table 4 (in paper). S-BGCN-T-K Vacuity performs the best on all
three datesets. Obviously S-BGCN-T-K Vacuity outperform S-BGCN-T-K Distributional in OOD
detection.
B.4	Analyze for epistemic in OOD Detection
In OOD detection, epistemic uncertainty performed the worst because it cannot distinguish the
flat Dirichlet distribution (α = (1, . . . , 1)) from sharp Dirichlet distribution (α = (10, . . . , 10)),
resulting poor performance in OOD detection. Unlike AUPR in node classification prediction with
outperformance in S-BGCN-T-K Dissonance (see Figure 2), S-BGCN-T-K Dissonance showed the
second worst performance among the proposed S-BGCN-T-K models with other uncertainty types.
This implies that less conclusive belief mass does not help OOD detection.
Although epistemic uncertainty is known to be effective to improve OOD detection (Kendall & Gal,
2017) in computer vision applications, our result showed fairly poor performance compared to the
case other uncertainty types are used. This is because our experiment is conducted with a very small
of training nodes (i.e., 3% on Cora, 2% on Citeseer, 0.2% on Pubmed) which is highly challenging to
observe high performance particularly with epistemic uncertainty.
C Derivations for Uncertainty Measures and KL divergence
This appendix provides the derivations and shows how calculate the uncertainty measures discussed
in section 3 for BGCN. Additionally, it describes how to calculate the joint probability, Dirichlet
parameters and KL-divergence between Prob(y|r; β) and Prob(y|r; γ, G).
C.1 Uncertainty Measures
Vacuity uncertainty of Bayesian Graph neural networks for node i:
VacUity[pi]	= Eprob(θ∣G) [vi]
K
=EProb(θ∣G) [K/ X aik]
k=1
K
≈ Eq(θ) [K/ X aik]
k=1
MK
≈ M X [K/ X α(m)],	a(m) = f(r, θ(m)),	8(m)〜q(θ)
m=1	k=1
17
Under review as a conference paper at ICLR 2020
Dissonance uncertainty of Bayesian Graph neural networks for node i:
Disso.[pi]
=EProb(θ∣G) [ω(bi)]
≈	Eq(θ) ω(bi)
1M
≈ M X [ω(bi)],	θ(m 〜q(θ)
m=1
and
ω(b∙ ) = X ( bik Pj=1,j = k bijBal(bij ,bik) ^
k=1	Pj=1,j6=k bij
where the relative mass balance between a pair of belief masses bij and bik is expressed by
Bal(bij, bik) = 1 - |bij - bik |/(bij + bik).
Aleatoric uncertainty of Bayesian Graph neural networks for node i, followed (Malinin & Gales,
2018):
Aleatoric[pi]	= Eprob(θ∣G) [H(y∕r; θ)]
≈ Eq(θ)[H(y∕r; θ)]
1M
≈ M ∑H[(yi∣r; θ(m))], θ(m)~q(θ)
m=1
MK
≈ M XXPrοb(yi=j∣r; θ(m))lοg (PrOb(yi = j|r; θ(m))), θ(m) - q(θ)
m=1 j=1
Epistemic uncertainty of Bayesian Graph neural networks for node i, followed (Gal, 2016):
EPistemic[pi] = H[Eprob(e∣G)[(yi∣r; θ)]] - Eprob(θ∣g) [H(y∕r; θ)]
≈ H[Eq(θ)[(yiT; θ)]] - Eq(θ) [H(yi|r； θ)]
1M	1M
≈ H[M XPrοb(y∕r; θ(m))] - M XH[(y∕r; θ(m))],	8(m)〜q(θ)
C.2 Joint probability
At the test stage, we infer the joint Probability by:
p(y|r; G)	=
/ / Prοb(y∣p)Prοb(p∣r; θ)Prοb(θ∣G)dpdθ
Prob(y|p)Prob(p|r; θ)q(θ)dpdθ
Prοb(y∣p)Prοb(p∣r; θ(m) )dp,

θ(m) - q(θ)
Prοb(yi∣pjPrοb(pi∣r; θ(m))dpi,
Prοb(yi∣pjPrοb(pi∣r; θ(m))dpi,
MN
M XY/
m=1 i=1


θ(m) - q(θ)
θ(m) - q(θ)
Pr0b(yi|pi)Dir(Pi la(m))dpi,

α(m) = f (r, θ(m)), q θ(m) - q(θ)
18
Under review as a conference paper at ICLR 2020
where the posterior over class label p will be given by the mean of the Dirichlet:
Prob(yi = p∣θ(m)) = /Prob(y，= p∣Pi)Prob(p∕r; θ(m))dpi = -
The probabilistic form for a specific node i by using marginal probability,
α(m)
pK=i α(m)
Prob(yi|r;G)



Prob(y|r;G)
y\yi
XZZ Y Prob(yj∣Pj)Prob(pj∣r; θ)Prob(θ∣G)dpdθ
y\yi	j =1
X Z Z Y Prob(yj |Pj)Prob(Pj |r; θ)q(θ)dPdθ
y\yi	j=1
MN
XX	Y Prob(yj ∣Pj )Prob(Pj |r； θ(m))dP, θ(m) 〜q(θ)
m=1 y\yi	j=1
MN
X [X I Y Prob(yj ∣Pj )Prob(Pj |r； θ(m))dPj],	8(m) ~ q(θ)
m=1 y\yi	j=1
M
N


XXY
Prob(yj|rj; θ(m))] Prob(y∕r; θ(m)),	θ(m 〜q(θ)
m=1 y\yi j=1,j6=i
M
X J Prob(y∕Pi)Prob(Pi∣r; θ(m))dPi,	8(m)〜q(θ)
m=1
specifically for probability of label p,
1 M	α(m)
Prob(yi = p|r; G) ≈ M E PKlm), α(m) = f(r, θ(m)), θ(m) 〜q(θ)
m=1	k=1 αik
C.3 KL-divergence
KL-divergence between Prob(y|r; β) and Prob(y|r; γ, G):
KL[Prob(y∣r; G)∣∣Prob(y∣r; β))]


w	Γ1	Prob(y∣r; G)]
EProb(y|r;G) log
Prob(y∣r; β).l
E	h lcOQN=I PrOb(X|r; G) i
EP'0b(ylr;G)I g QN=I Prob(y∕r; β)]
E iG)hXiog ιrob(⅛i i
≈ XEi「©[kg Probgf i
≈ X X P"j∣r; G)(log 鬻―)
i=1 j =1
The KL divergence between two Dirichlet distributions Dir(α) and Dir(α) can be obtained in closed
form as follows:
KK
KL[Dir(α)kDir(α)] = lnΓ(S) - Inr(S) + X (lnΓ(α°) - lnΓ(α°)) + £(0。- α°)(ψ(αc) - ψ(S))
c=1	c=1
where S = PK=I αc and S = PK=I &c
19