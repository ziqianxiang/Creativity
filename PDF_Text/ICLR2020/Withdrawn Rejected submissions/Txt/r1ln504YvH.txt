Under review as a conference paper at ICLR 2020
Actor-Critic Approach for Temporal Predic-
tive Clustering
Anonymous authors
Paper under double-blind review
Ab stract
Due to the wider availability of modern electronic health records (EHR), patient
care data is often being stored in the form of time-series. Clustering such time-
series data is crucial for patient phenotyping, anticipating patients’ prognoses by
identifying “similar” patients, and designing treatment guidelines that are tailored
to homogeneous patient subgroups. In this paper, we develop a deep learning ap-
proach for clustering time-series data, where each cluster comprises patients who
share similar future outcomes of interest (e.g., adverse events, the onset of comor-
bidities, etc.). The clustering is carried out by using our novel loss functions that
encourage each cluster to have homogeneous future outcomes. We adopt actor-
critic models to allow “back-propagation” through the sampling process that is
required for assigning clusters to time-series inputs. Experiments on two real-
world datasets show that our model achieves superior clustering performance over
state-of-the-art benchmarks and identifies meaningful clusters that can be trans-
lated into actionable information for clinical decision-making.
1	Introduction
Chronic diseases - such as cystic fibrosis, dementia, and diabetes - are heterogeneous in nature,
with widely differing outcomes even in narrow patient subgroups. Disease progression manifests
through a broad spectrum of clinical factors, collected as a sequence of measurements over time in
electronic health records (EHR), which gives a rise to complex progression patterns among patients
(Samal et al., 2011). For example, cystic fibrosis evolves slowly, allowing for the development of
related comorbidities and bacterial infections, and creating distinct behaviors/responses to therapeu-
tic interventions, which in turn makes the survival and quality of life substantially different (Ramos
et al., 2017). Identifying patient subgroups with similar progression patterns can be advantageous
for understanding such heterogeneous underlying diseases. This allows clinicians to anticipate pa-
tients’ prognoses by comparing “similar” patients in order to design treatment guidelines that are
tailored to homogeneous patient subgroups (Zhang et al., 2019).
Temporal clustering has been recently used as a data-driven framework to partition patients with
time-series observations into a set of clusters (i.e., into subgroups of patients). Recent research has
typically focused on either finding fixed-length and low-dimensional representations (Zhang et al.,
2019; Rusanov et al., 2016) or on modifying the similarity measure (Giannoula et al., 2018; Lu-
ong and Chandola, 2017) both in an attempt to apply the conventional clustering algorithms (e.g.,
K-means (Lloyd, 1982)) to time-series observations. However, clusters identified from these ap-
proaches these approaches are purely unsupervised - they do not account for each patient’s observed
outcome (e.g., adverse events, the onset of comorbidities, etc.) - which leads to heterogeneous clus-
ters if the clinical presentation of the disease differs even for similar patients. Thus, a common
prognosis in each cluster remains unknown which can mystify the understanding of the underlying
disease progression (Boudier et al., 2019). For instance, patients who appear to have similar time-
series observations may develop different sets of comorbidities in the future which, in turn, require
different treatment guidelines to reduce such risks (Wami et al., 2013). To overcome this limitation,
we focus on predictive clustering (Blockeel et al., 2017) which combines prediction with clustering.
Therefore, the cluster assignments are optimized such that patients in a cluster share similar future
outcomes to provide a prognostic value.
1
Under review as a conference paper at ICLR 2020
In this paper, we propose an actor-critic approach for temporal predictive clustering, which we call
AC-TPC.1 2 Our model consists of three neural networks - an encoder, a Selector, and a predictor -
anda set of centroid candidates. More specifically, the encoder maps an input time-series into a latent
encoding; the selector utilizes the encoding and assigns a cluster to which the time-series belongs to
via a sampling process; and the predictor estimates the future outcome distribution conditioned on
either the encoding or the centroid of the selected cluster. The following three contributions render
our model to identify the predictive clusters. First, to encourage each cluster to have homogeneous
future outcomes, we define a clustering objective based on the Kullback-Leibler (KL) divergence
between the predictor’s output given the input time series, and the predictor’s output given estimated
cluster assignments. Second, we transform solving a combinatorial problem of identifying cluster
into iteratively solving two sub-problems: optimization of the cluster assignments and optimization
of the cluster centroids. Finally, we allow “back-propagation” through the sampling process of the
selector by adopting the training of actor-critic models (Konda and Tsitsiklis, 2000).
Throughout the experiments, we show significant performance improvements over the state-of-the-
art clustering methods on two real-world medical datasets. Then, to demonstrate the practical sig-
nificance of our model, we consider a more realistic scenario where the future outcomes of interest
are high-dimensional - such as, development of multiple comorbidities in the next year - and inter-
preting all possible combinations is intractable. Our experiments show that our model can identify
meaningful clusters that can be translated into actionable information for clinical decision-making.
2	Problem Formulation
Let X ∈ X and Y ∈ Y be random variables for an input feature and an output label (i.e., one or
a combination of future outcome(s) of interest) with a joint distribution pXY (and marginal distri-
butions are pX and pY , respectively) where X is the feature space and Y is the label space. Here,
We focus our description on C-class classification tasks, i.e., Y = {1, ∙∙∙ ,C}.2 We are given a
time-series dataset D = {(xtn, ytn)tT=n1}nN=1 comprising sequences of realizations (i.e., observations)
of the pair (X, Y ) for N patients. Here, (xtn, ytn)tT=n1 is a sequence ofTn observation pairs that cor-
respond to patient n and t ∈ Tn，{1,…，T n} denotes the time stamp at which the observations
are made.From this point forward, we omit the dependency on n when it is clear in the context and
denote xi：t = (xι ,…，Xt) for ease of notation.
Our aim is to identify a set of K predictive clusters, C = {C(1),…,C(K)}, for time-series data.
Each cluster consists of homogeneous data samples, that can be represented by its centroid, based
on a certain similarity measure. There are two main distinctions from the conventional notion of
clustering. First, we treat subsequences of each times-series as data samples and focus on parti-
tioning {{X1n:t}tT=n1}nN=1 into C. Hence, we define a cluster as C(k) = {X1n:t|t ∈ Tn, stn = k}
for k ∈ K，{1,…，K} where Sn ∈ K is the cluster assignment for a given Xne This is to
flexibly update the cluster assignment (in real-time) to which a patient belongs as new observations
are being accrued over time. Second, we define the similarity measure with respect to the label
distribution and associate it with clusters to provide a prognostic value. More specifically, we want
the distribution of output label for subsequences in each cluster to be homogeneous and, thus, can
be well-represented by the centroid of that cluster. Let S be a random variable for the cluster as-
signment - that depends on a given subsequence X1:t - and Y |S = k be a random variable for the
output given cluster k. Then, such property of predictive clustering can be achieved by minimizing
the following Kullback-Leibler (KL) divergence:
KL(YtXI：t = xi：t||Yt|St = k) for xi：t ∈ C(k)	(1)
where KL(Yt|Xi：t = xi：t^Yt|St = k) = Ry∈Yp(y|xi：t)(logP(y|xi：t) - logp(y∣st))dy. Here,
p(y|x1:t) and p(y|st) are the label distributions conditioned on a subsequence x1:t and a cluster
assignment st, respectively. Note that (1) achieves its minimum when the two distributions are
equivalent.
1Source code available at https://github.com/ICLR2020-ACTPC/ACTPC_submission.git
2In this paper, we focus our description on C-class classification task, i.e., Y = {1, •…,C}; in Appendix
A, we discuss simple modifications of our model for regression and M -dimensional binary classification tasks,
i.e., Y = R and Y = {0, 1}M , respectively.
2
Under review as a conference paper at ICLR 2020
Figure 1: The block diagram of AC-TPC. The red line implies the procedure of estimating p(y|St =
st) which includes a sampling process and the blue line implies that of estimating p(y|X1:t = x1:t).
Finally, we establish our goal as identifying a set of predictive clusters C that optimizes the following
objective:
minimize X X	KL{Yt[Xi：t = XLtIIYtISt = k).	(2)
k∈K XLt∈C(k)
Unfortunately, the optimization problem in (2) is highly non-trivial. We need to estimate the objec-
tive function in (2) while solving a non-convex combinatorial problem of finding the optimal cluster
assignments and cluster centroids.
3	Actor-Critic Approach for Temporal Predictive Clustering
To effectively estimate the objective function in (2), We introduce three networks - an encoder, a
selector, and a predictor - and an embedding dictionary as illustrated in Figure 1. These components
together provide the cluster assignment and the corresponding centroid based on a given sequence of
observations and enable us to estimate the probability density p(y|st). More specifically, we define
each component as follows:
•	The encoder, fθ : Qit=1 X → Z, is a RNN (parameterized by θ) that maps a (sub)sequence of
a time-series X1:t to a latent representation (i.e., encoding) zt ∈ Z where Z is the latent space.
•	The selector, hψ : Z → ∆K-1, is a fully-connected network (parameterized by ψ) that pro-
vides a probabilistic mapping to a categorical distribution from which the cluster assignment
st ∈ K is being sampled.
•	The predictor, gφ : Z → ∆C-1, is a fully-connected network (parameterized by φ) that esti-
mates the label distribution given the encoding of a time-series or the centroid of a cluster.
•	The embedding dictionary, E = {e(1),…，e(K)} where e(k) ∈ Z for k ∈ K, is a set of
cluster centroids lying in the latent space which represents the corresponding cluster.
Here, ∆D-1 =	{q ∈	[0, 1]k	:	qι	+-+ qD	= 1} is a	(D -	1)-simplex that denotes the probability
distribution for a D-dimensional categorical (class) variable.
At each time stamp t, the encoder maps a input (sub)sequence X1:t into a latent encoding zt ,
fθ (X1:t). Then, based on the encoding zt, the cluster assignment st is drawn from a categorical dis-
tribution that is defined by the selector output, i.e., St Z Cat(∏t) where ∏t = [∏t(1),…,∏t(K)]，
hψ(zt). Once the assignment st is chosen, we allocate the latent encoding zt to an embedding e(st)
in the embedding dictionary E. Since the allocated embedding e(st ) corresponds to the centroid of
the cluster to which X1:t belongs, we can, finally, estimate the density p(y|st) in (2) as the output of
the predictor given the embedding e(st), i.e.,加，gφ(e(st)).
3.1	Loss Functions
In this subsection, we define loss functions to achieve our objective in (2); the details of how we
train our model will be discussed in the following subsection.
3
Under review as a conference paper at ICLR 2020
Predictive Clustering Loss: Since finding the cluster assignment of a given sequence is a prob-
abilistic problem due to the sampling process, the objective function in (2) must be defined as an
expectation over the cluster assignment. Thus, we can estimate solving the objective problem in (2)
as minimizing the following loss function:
T
Ll(θ,ψ,φ, E)= Eχ,y〜PXY EEst〜Cat(∏t)⑶侬,％)]	⑶
t=1
where 'ι(yt,yt) = - PC=I yC logyC. Here, We slightly abuse the notation and denote y =
[y1 …yC] as the one-hot encoding of y, and yc and yc indicates the c-th component of y and y, re-
spectively. It is worth to highlight that minimizing `1 is equivalent to minimizing the KL divergence
in (2) since the former term of the KL divergence is independent of our optimization procedure.
One critical question that may arise is how to avoid trivial solutions in this unsupervised setting
of identifying the cluster assignments and the centroids (Yang et al., 2017). For example, all the
embeddings in E may collapse into a single point or the selector simply assigns equal probability
to all the clusters regardless of the input sequence. In both cases, our model will fail to correctly
estimate p(y|st) and, thus, end up finding a trivial solution. To address this issue, we introduce two
auxiliary loss functions that are tailored to address this concern. Itis worth to highlight that these loss
functions are not subject to the sampling process and their gradients can be simply back-propagated.
Sample-Wise Entropy of Cluster Assignment: To motivate sparse cluster assignment such that
the selector ultimately selects one dominant cluster for each sequence, we introduce sample-wise
entropy of cluster assignment which is given as
T
L2(θ,ψ) = Ex 〜PX -XXπt (k) log πt(k)	(4)
t=1 k∈K
where ∏t = [∏t(1)…∏t(K)] = hψ(fθ(xi：t)). The sample-wise entropy achieves its minimum
when πt becomes an one-hot vector.
Embedding Separation Loss: To prevent the embeddings in E from collapsing into a single point,
we define a loss function that encourages the embeddings to represent different label distributions,
i.e., gφ(e(k)) for k ∈ K, from each other:
L3(E) = - X 'ι(gφ(e(k)),gφ(e(k0)))	⑸
k6=k0
where `1 is reused to quantify the distance between label distributions conditioned on each cluster.
We minimize (5) when updating the embedding vectors e(1),…，e(K).
3.2	Optimization
The optimization problem in (2) is a non-convex combinatorial problem because it comprises not
only minimizing the KL divergence but also finding the optimal cluster assignments and centroids.
Hence, we propose an optimization procedure that iteratively solves two subproblems: i) optimizing
the three networks - the encoder, selector, and predictor - while fixing the embedding dictionary
and ii) optimizing the embedding dictionary while fixing the three networks. Pseudo-code of AC-
TPC can be found in Appendix F.
3.2.1	Optimizing the Three Networks — fθ, hψ, and gφ
Finding predictive clusters incorporates the sampling process which is non-differentiable. Thus,
to render “back-propagation”, we utilize the training of actor-critic models (Konda and Tsitsiklis,
2000). More specifically, we view the combination of the encoder (fθ) and the selector (hψ) as the
“actor” parameterized by ωA = [θ, ψ], and the predictor (gφ) as the “critic”. The critic takes as input
the the output of the actor (i.e., the cluster assignment) and estimates its value based on the sample-
wise predictive clustering loss (i.e., 'ι(yt, yt)) given the chosen cluster. This, in turn, renders the
actor to change the distribution of selecting a cluster to minimize such loss. Thus, it is important
for the critic to perform well on the updated output of the actor while it is important for the actor
4
Under review as a conference paper at ICLR 2020
to perform well on the updated loss estimation. As such, the parameters for the actor and the critic
need to be updated iteratively.
Given the embedding dictionary E fixed (thus, we will omit the dependency on E), we train the actor,
i.e., the encoder and the selector, by minimizing a combination of the predictive clustering loss L1
and the entropy of cluster assignments L2, which is given by
LA(θ, ψ, φ) = L1(θ, ψ, φ) + αL2(θ, ψ)	(6)
where α ≥ 0 is a coefficient chosen to balance between the two losses. To derive the gradient of this
loss with respect ωA = [θ, ψ], we utilize the ideas from actor-critic models (Konda and Tsitsiklis,
2000) as follows; please refer to Appendix B for the detailed derivation:
Ex,y~pχγ
▽sa LA(θ, ψ, φ) = Ex,y〜PXY
(7)
Note that since no sampling process is considered in L2 (θ,ψ), We can simply derive Vsa L2 (θ, Ψ).
Iteratively with training the actor, we train the critic, i.e., the predictor, by minimizing the predictive
clustering loss L1 as the folloWing:
LC(φ) =L1(θ,ψ,φ)	(8)
Whose gradient With respect to φ can be givens as VφLC (φ) = VφL1 (θ, ψ, φ). Note that since the
critic is independent of the sampling process, the gradient can be simply back-propagated.
3.2.2	Optimizing the Cluster Centroids
NoW, once the parameters for the three netWorks (θ, ψ, φ) are fixed (thus, We omit the dependency
on θ, ψ, and φ), We updated the embeddings in E by minimizing a combination of the predictive
clustering loss L1 and the embedding separation loss L3 , Which is given by
LE(E)=L1(E)+βL3(E)	(9)
Where β ≥ 0 is a coefficient chosen to balance betWeen the tWo losses.
3.2.3	Initializing AC-TPC via Pre-Training
Since We transform the non-trivial combinatorial optimization problem in (2) into iteratively solving
tWo sub-problems, initialization is crucial to achieve better optimization as a similar concern has
been addressed in (Yang et al., 2017).
Therefore, We initialize our model based on the folloWing procedure. First, We pre-train the encoder
and the predictor by minimizing the folloWing loss function based on the predicted label distribution
given the latent encodings of input sequences, i.e., yt，gφ(zt) = gφ(fθ (xi：t)), as the following:
T
LI(θ,φ)= Eχ,y〜PXY -X'ι(yt,yt) .	(10)
t=1
Minimizing (10) encourages the latent encoding to be enriched with information for accurately pre-
dicting the label distribution. Then, we perform K-means (other clustering method can be also
applied) based on the learned representations to initialize the embeddings E and the cluster assign-
ments {{stn}tT=n1}nN=1. Finally, we pre-train the selector hψ by minimizing the cross entropy treating
the initialized cluster assignments as the true clusters.
4	Related Work
Temporal clustering, also known as time-series clustering, is a process of unsupervised partitioning
of the time-series data into clusters in such a way that homogeneous time-series are grouped together
5
Under review as a conference paper at ICLR 2020
based on a certain similarity measure. Temporal clustering is challenging because i) the data is often
high-dimensional - it consists of sequences not only with high-dimensional features but also with
many time points - and ii) defining a proper similarity measure for time-series is not straightforward
since it is often highly sensitive to distortions (Ratanamahatana et al., 2005). To address these chal-
lenges, there have been various attempts to find a good representation with reduced dimensionality
or to define a proper similarity measure for times-series (Aghabozorgi et al., 2015).
Recently, Baytas et al. (2017) and Madiraju et al. (2018) proposed temporal clustering methods
that utilize low-dimensional representations learned by RNNs. These works are motivated by the
success of applying deep neural networks to find “clustering friendly” latent representations for
clustering static data (Xie et al., 2017; Yang et al., 2017). In particular, Baytas et al. (2017) utilized
a modified LSTM auto-encoder to find the latent representations that are effective to summarize the
input time-series and conducted K-means on top of the learned representations as an ad-hoc process.
Similarly, Madiraju et al. (2018) proposed a bidirectional-LSTM auto-encoder that jointly optimizes
the reconstruction loss for dimensionality reduction and the clustering objective. However, these
methods do not associate a target property with clusters and, thus, provide little prognostic value in
understanding the underlying disease progression.
Our work is most closely related to SOM-VAE (Fortuin et al., 2019). This method jointly opti-
mizes a static variational auto-encoder (VAE), that finds latent representations of input features, and
a self-organizing map (SOM), that allows to map the latent representations into a more interpretable
discrete representations, i.e., the embeddings. However, there are three key differences between our
work and SOM-VAE. First, SOM-VAE aims at minimizing the reconstruction loss that is specified
as the mean squared error between the original input and the reconstructed input based on the cor-
responding embedding. Thus, similar to the aforementioned methods, SOM-VAE neither associates
future outcomes of interest with clusters. In contrast, we focus on minimizing the KL divergence
between the outcome distribution given the original input sequence and that given the correspond-
ing embedding to build association between future outcomes of interest and clusters. Second, to
overcome non-differentiability caused by the sampling process (that is, mapping the latent represen-
tation to the embeddings), Fortuin et al. (2019) applies the gradient copying technique proposed by
(van den Oord et al., 2017), while we utilize the training of actor-critic model (Konda and Tsitsiklis,
2000). Finally, while we flexibly model time-series using LSTM, SOM-VAE handles time-series
by integrating a Markov model in the latent representations. This can be a strict assumption espe-
cially in clinical settings where a patient’s medical history is informative for predicting his/her future
clinical outcomes (Ranganath et al., 2016).
5	Experiments
In this section, we provide a set of experiments using two real-world time-series datasets. We itera-
tively update the three networks - the encoder, selector, and predictor - and the embedding dictio-
nary as described in Section 3.2. For the network architecture, we constructed the encoder utilizing a
single-layer LSTM (Hochreiter and Schmidhuber, 1997) with 50 nodes and constructed the selector
and predictor utilizing two-layer fully-connected network with 50 nodes in each layer, respectively.
The parameters (θ, ψ, φ) are initialized by Xavier initialization (Glorot and Bengio, 2010) and opti-
mized via Adam optimizer (Kingma and Ba, 2014) with learning rate of 0.001 and keep probability
0.7. We chose the balancing coefficients α, β ∈ {0.1, 1.0, 3.0} utilizing grid search that achieves the
minimum validation loss in (3); the effect of different loss functions are further investigated in the
experiments. Here, all the results are reported using 5 random 64/16/20 train/validation/test splits.
5.1	Real-World Datasets
We conducted experiments to investigate the performance of AC-TPC on two real-world medical
datasets; detailed statistics of each dataset can be found in Appendix C:
•	UK Cystic Fibrosis registry (UKCF)3: This dataset records annual follow-ups for 5,171 adult
patients (aged 18 years or older) enrolled in the UK CF registry over the period from 2008 and
2015, with a total of 25,012 hospital visits. Each patient is associated with 89 variables (i.e.,
11 static and 78 time-varying features), including information on demographics and genetic
3 https://www.cysticfibrosis.org.uk/the- work- we- do/uk- cf- registry
6
Under review as a conference paper at ICLR 2020
mutations, bacterial infections, lung function scores, therapeutic managements, and diagnosis
on comorbidities. We set the development of different comorbidities in the next year as the label
of interest at each time stamp.
•	Alzheimer’s Disease Neuroimaging Initiative (ADNI)4: This dataset consists of 1,346 patients
in the Alzheimer’s disease study with a total of 11,651 hospital visits, which tracks the disease
progression via follow-up observations at 6 months interval. Each patient is associated with 21
variables (i.e., 5 static and 16 time-varying features), including information on demographics,
biomarkers on brain functions, and cognitive test results. We set predictions on the three diag-
nostic groups - normal brain functioning, mild cognitive impairment, and AIzheimer's disease -
as the label of interest at each time stamp.
5.2	Benchmarks
We compare AC-TPC with clustering methods ranging from conventional approaches based on K-
means to the state-of-the-art approaches based on deep neural networks. All the benchmarks com-
pared in the experiments are tailored to incorporate time-series data as described below:
•	Dynamic time warping followed by K-means: Dynamic time warping (DTW) is utilized to
quantify pairwise distance between two variable-length sequences and, then, K-means is applied
(denoted as KM-DTW).
•	K-means with deep neural networks: To handle variable-length time-series data, we utilize
our encoder and predictor that are trained based on (10) for dimensionality reduction; this is
to provide fixed-length and low-dimensional representations for time-series. Then, we apply
K-means on the latent encodings z (denoted as KM-E2P (Z)) and on the predicted label distri-
butions y (denoted as KM-E2P (Y)), respectively.
•	Extensions of DCN (Yang et al., 2017): Since the DCN is designed for static data, we replace
their static auto-encoder with a sequence-to-sequence network to incorporate time-series data
(denoted as DCN-S2S).5 In addition, to associated with the label distribution, we compare a
DCN whose static auto-encoder is replaced with our encoder and predictor (denoted as DCN-
E2P) to focus dimensionality reduction while preserving information for predicting the label.
•	SOM-VAE (Fortuin et al., 2019): We compare with SOM-VAE - though, this method is oriented
towards visualizing input data via SOM - since it naturally clusters time-series data (denoted as
SOM-VAE). In addition, we compare with a variation of SOM-VAE by replacing the decoder
with our predictor in order to find embeddings that capture information for predicting the label
(denoted as SOM-VAE-P). For both cases, we set the dimension of SOM to K.
It is worth highlighting that the label information is provided for training DCN-E2P, KM-E2P, and
SOM-VAE-P while the label information is not provided for training KM-DTW, DCN-S2S, and
SOM-VAE. Please refer to Appendix D for the summary of major components of the tested bench-
marks and the implementation details.
5.3	Performance Metrics
To assess the clustering performance, we applied the following three standard metrics for evaluating
clustering performances when the ground-truth cluster label is available: purity score, normalized
mutual information (NMI) (Vinh et al., 2010), and adjusted Rand index (ARI) (Hubert and Arabie,
1985). More specifically, the purity score assesses how homogeneous each cluster is (ranges from 0
to 1 where 1 being a cluster consists of a single class), the NMI is an information theoretic measure
of how much information is shared between the clusters and the labels that is adjusted for the number
of clusters (ranges from 0 to 1 where 1 being a perfect clustering), and ARI is a corrected-for-chance
version of the Rand index which is a measure of the percentage of correct cluster assignments (ranges
from -1 to 1 where 1 being a perfect clustering and 0 being a random clustering). In a nutshell, all
the three performance metrics are commonly used but all have its pros and cons; for instance, the
4https://adni.loni.usc.edu
5This extension is a representative of recently proposed deep learning approaches for clustering of both
static data (Xie et al., 2017; Yang et al., 2017) and time-series data (Baytas et al., 2017; Madiraju et al., 2018)
since these methods are built upon the same concept - that is, applying deep networks for dimensionality
reduction to conduct conventional clustering methods, e.g., K -means.
7
Under review as a conference paper at ICLR 2020
Table 1: Performance Comparison on the UKCF and ADNI datasets.
Dataset	Method	Purity	NMI	ARI	AUROC	AUPRC
	KM-DTW	0.573±0.01*	0.010±0.01*	0.014±0.01*	N/A	N/A
	KM-E2P (Z)	0.719±0.01*	0.211±0.01*	0.107±0.01*	0.726±0.01*	0.425±0.02*
	KM-E2P (Y)	0.751±0.01*	0.325±0.01*	0.440±0.02*	0.807±0.00*	0.514±0.01*
UKCF	DCN-S2S	0.607±0.06*	0.059±0.08*	0.063±0.09*	N/A	N/A
	DCN-E2P	0.751±0.02*	0.275±0.02*	0.184±0.01*	0.772±0.03*	0.487±0.03*
	SOM-VAE	0.573±0.01*	0.006±0.00*	0.006±0.01*	N/A	N/A
	SOM-VAE-P	0.638±0.04*	0.201±0.05*	0.283±0.17t	0.754±0.05*	0.331±0.07*
	Proposed	0.807±0.01	0.463±0.01	0.602±0.01	0.843±0.01	0.605±0.01
	KM-DTW	0.566±0.02*	0.019±0.02*	0.006±0.02*	N/A	N/A
	KM-E2P (Z)	0.736±0.03t	0.249±0.02	0.230±0.03t	0.707±0.01*	0.509±0.01
	KM-E2P (Y)	0.776±0.05	0.264±0.07	0.317±0.11	0.756±0.04	0.503±0.04
ADNI	DCN-S2S	0.567±0.02*	0.005±0.00*	0.000±0.01* 0.215±0.06t	N/A	N/A
	DCN-E2P	0.749±0.06	0.261±0.05		0.721±0.03t	0.509±0.03
	SOM-VAE	0.566±0.02*	0.040±0.06*	0.011±0.02*	N/A	N/A
	SOM-VAE-P	0.586±0.06*	0.085±0.08*	0.038±0.06*	0.597±0.10t	0.376±0.05*
	Proposed	0.786±0.03	0.285±0.04	0.330±0.06	0.768±0.02	0.515±0.02
* indicates P-Value < 0.01, ↑ indicates P-Value < 0.05
(a) The aVeraged purity score.
(b) The aVeraged NMI.
(c) The aVeraged ARI.
Figure 2: The purity score, NMI, and ARI (mean and 95% confidence interVal) for the UKCF dataset
(C = 8) with Various K.
purity score easily conVerges to 1 when there are as many clusters as data samples. Thus, using them
together suffices to demonstrate the effectiVeness of the clustering methods.
To assess the prediction performance of the identified predictiVe clusters, we utilized both area un-
der receiVer operator characteristic curVe (AUROC) and area under precision-recall curVe (AUPRC)
based on the label predictions of each cluster and the ground-truth binary labels on the future out-
comes of interest. Note that the prediction performance is aVailable only for the benchmarks that
incorporate the label information during training.
5.4	Clustering Performance
We start with a simple scenario where the true class (i.e., the ground-truth cluster label) is aVailable
and the number of classes is tractable. In particular, we setC = 23 = 8 based on the binary labels for
the development of three common comorbidities of cystic fibrosis - diabetes, ABPA, and intestinal
obstruction - in the next year for the UKCF dataet and C = 3 based on the mutually exclusive
three diagnostic groups for the ADNI dataset. We compare AC-TPC against the aforementioned
benchmarks with respect to the clustering and prediction performance in Table 1.
As shown in Table 1, AC-TPC achieved performance gain over all the tested benchmarks in terms of
both clustering and prediction performance — where most of the improvements were statistically sig-
nificant with p-value < 0.01 or p-value < 0.05 — for both datasets. Importantly, clustering methods
—i.e., KM-DTW, DCN-S2S, and SOM-VAE - that do not associate with the future outcomes of in-
terest identified clusters that provide little prognostic value on the future outcomes (note that the true
class is derived from the future outcome of interest). This is clearly shown by the ARI value near
0 which indicates that the identified clusters have no difference with random assignments. There-
fore, similar sequences with respect to the latent representations tailored for reconstruction or with
respect to the shape-based measurement using DTW can have very different future outcomes.
In Figure 2, we further investigate the purity score, NMI, and ARI by varying the number of clusters
K from 4 to 16 on the UKCF dataset in the same setting with that stated above (i.e., C = 8).
8
Under review as a conference paper at ICLR 2020
Here, the three methods - i.e., KM-DTW, DCN-S2S, and SOM-VAE - are excluded for better
visualization. As we can see in Figure 2, our model rarely incur performance loss in both NMI and
ARI while the benchmarks (except for SOM-VAE-P) showed significant decrease in the performance
as K increased (higher than C). This is because the number of clusters identified by AC-TPC (i.e.,
the number of activated clusters where we define cluster k is activated if |C(k)| > 0) was the same
with C most of the times, while the DCN-based methods identified exactly K clusters (due to the
K-means). Since the NMI and ARI are adjusted for the number of clusters, a smaller number of
identified clusters yields, if everything being equal, a higher performance. In contrast, while our
model achieved the same purity score for K ≥ 8, the benchmark showed improved performance
as K increased since the purity score does not penalize having many clusters. This is an important
property of AC-TPC that we do not need to know a priori what the number of cluster is which is a
common practical challenge of applying the conventional clustering methods (e.g., K-means).
The performance gain of our model over SOM-VAE-P (and, our analysis is the same for SOM-
VAE) comes from two possible sources: i) SOM-VAE-P mainly focuses on visualizing the input
with SOM which makes both the encoder and embeddings less flexible - this is why it performed
better with higher K - and ii) the Markov property can be a too strict assumption for time-series data
especially in clinical settings where a patient’s medical history is informative for predicting his/her
future clinical outcomes (Ranganath et al., 2016).
5.5	Contributions of the Auxiliary Loss Functions
As described in Section 3.1, we introduced two auxiliary loss functions - the sample-wise entropy of
cluster assignment (4) and the embedding separation loss (5) -to avoid trivial solution that may arise
in identifying the predictive clusters. To analyze the contribution of each auxiliary loss function, we
report the average number of activated clusters, clustering performance, and prediction performance
on the UKCF dataset with 3 comorbidities as described in Section 5.4. Throughout the experiment,
we set K = 16 - which is larger than C - to find the contribution of these loss functions to the
number of activated clusters.
Table 2: Performance comparison with varying the balancing coefficients α, β for the UKCF dataset.
Coefficients		Activated No.	Clustering Performance		ARI	Prognostic Value	
α	β		Purity	NMI		AUROC	-_AUPRC
0.0	0.0	16	0.573±0.01	0.006±0.00	0.000±0.00	0.500±0.00	0.169±0.00
0.0	1.0	16	0.573±0.01	0.006±0.00	0.000±0.00	0.500±0.00	0.169±0.00
3.0	0.0	8.4	0.795±0.01	0.431±0.01	0.569±0.01	0.840±0.01	0.583±0.02
3.0	1.0	8	0.808±0.01	0.468±0.01	0.606±0.01	0.852±0.00	0.608±0.01
As we can see in Table 2, both auxiliary loss functions make important contributions in improving
the quality of predictive clustering. More specifically, the sample-wise entropy encourages the selec-
tor to choose one dominant cluster. Thus, as we can see results with α = 0, without the sample-wise
entropy, our selector assigns an equal probability to all 16 clusters which results in a trivial solution.
We observed that, by augmenting the embedding separation loss (5), AC-TPC identifies a smaller
number of clusters owing to the well-separated embeddings; in Appendix E, we further investigate
the usefulness of (5) in identifying the number of clusters in a data-driven fashion.
5.6	Targeting Multiple Future Outcomes - a Practical Scenario
In this experiment, we focus on a more practical scenario where the future outcome of interest
is high-dimensional and the number of classes based on all the possible combinations of future
outcomes becomes intractable. For example, suppose that we are interested in the development
of M comorbidities in the next year whose possible combinations grow exponentially C = 2M .
Interpreting such a large number of patient subgroups will be a daunting task which hinders the
understanding of underlying disease progression. Since different comorbidities may share common
driving factors (Ronan et al., 2017), we hope our model to identify much smaller underlying (latent)
clusters that govern the development of comorbidities. Here, to incorporate with M comorbidities
(i.e., M binary labels), we redefine the output space as Y = {0, 1}M and modify the predictor and
loss functions, accordingly, as described in Appendix A.
9
Under review as a conference paper at ICLR 2020
Table 3: The top-3 frequent comorbidities developed in the next year for the 12 identified clusters.
The values in parentheses indicate the corresponding frequency.
Clusters
0
1
2
3
4
5
6
7
8
9
10
11
Diabetes (0.85)
Liver Enzymes (0.09)
ABPA (0.77)
Asthma (0.89)
Osteoporosis (0.76)
Asthma (0.88)
Liver Disease (0.85)
ABPA (0.83)
Diabetes (0.94)
Asthma (0.89)
Osteopenia (0.82)
Osteopenia (0.77)
Top-3 Frequent Comorbidities
Liver Enzymes (0.21)
Arthropathy (0.08)
Osteopenia (0.21)
Liver Disease (0.87)
Diabetes (0.43)
Diabetes (0.81)
Asthma (0.03)
Diabetes (0.78)
Liver Disease (0.83)
Osteopenia (0.26)
Diabetes (0.81)
Liver Enzymes (0.18)
Arthropathy (0.14)
Depression (0.07)
Intestinal Obstruction (0.11)
Diabetes (0.29)
Arthropathy (0.20)
Osteopenia (0.28)
ABPA (0.09)
Osteopenia (0.25)
Liver Enzymes (0.43)
ABPA (0.19)
Arthropathy (0.23)
Arthropathy (0.12)
Figure 3: Clusters with high-risk of developing diabetes. We reported the cluster-specific frequen-
cies of developing comorbidities - liver disease, asthma, ABPA, and osteopenia that are co-occurred
with diabetes - in the next year.
Throughout this experiment, we aim at identifying subgroups of patients that are associated with the
next-year development of 22 different comorbidities in the UKCF dataset. In Table 3, we reported
12 identified clusters - on average, the number of activated clusters were 13.6 - and the top three
frequent comorbidities developed in the next year since the latest observation and the corresponding
frequency; please refer to Appendix E for a full list. Here, the frequency is calculated in a cluster-
specific fashion based on the true label. As we can see in Table 3, the identified clusters displayed
very different label distributions; that is, the combination of comorbidities as well as their frequency
were very different across the clusters. For example, patients in Cluster 1 experienced low-risk of
developing any comorbities in the next year while 85% of patients in Cluster 0 experienced diabetes
in the next year.
In Figure 3, we further investigated subgroups of patients - Cluster 0, 5, 7, 8, and 10 - who had
high risk of developing diabetes in the next year. Although all these clusters displayed high risk
of diabetes, the frequency of other co-occurred comorbidities was significantly different across the
clusters. In particular, around 89% of the patients in Cluster 5 experienced asthma in the next
year while it was less than 3% of the patients in the other cluster. Interestingly, “leukotriene” - a
medicine commonly used to manage asthma - and “FEV1% predicted” - a measure of lung function
- were the two most different input features between patients in Cluster 5 and those in the other
clusters. We observed similar findings in Cluster 7 with ABPA, Cluster 8 with liver disease, and
Cluster 10 with osteopenia. Therefore, by grouping patients who are likely to develop a similar set
of comorbidities, our method identified clusters that can be translated into actionable information
for clinical decision-making.
6 Conclusion
In this paper, we introduced AC-TPC, a novel deep learning approach for predictive clustering of
time-series data. We carefully defined novel loss functions to encourage each cluster to have ho-
mogeneous future outcomes (e.g., adverse events, the onset of comorbidities, etc.) and designed
optimization procedures to address challenges and to avoid trivial solution in identifying such clus-
ter assignments and the centroids. Throughout the experiments on two real-world datasets, we
showed that our model achieves superior clustering performance over state-of-the-art and identifies
meaningful clusters that can be translated into actionable information for clinical decision-making.
10
Under review as a conference paper at ICLR 2020
References
L. Samal, A. Wright, B. Wong, J. Linder, and D. Bates. Leveraging electronic health records to
support chronic disease management: the need for temporal data views. Informatics in Primary
Care,19(2):65-74, 2011.
K. J. Ramos, B. S. Quon, S. L. Heltshe, N. Mayer-Hamblett, E. D. Lease, M. L. Aitken, N. S. Weiss,
and C. H. Goss. Heterogeneity in survival in adult patients with cystic fibrosis with FEV1 < 30%
of predicted in the united states. Chest, 151(6):1320-1328, June2θ17.
Xi Zhang, Jingyuan Chou, Jian Liang, Cao Xiao, Yize Zhao, Harini Sarva, Claire Henchcliffe, and
Fei Wang. Data-driven subtyping of parkinson’s disease using longitudinal clinical records: A
cohort study. Scientific Reports, 9(797):1-12, January 2019.
A. Rusanov, P. V. Prado, and C. Weng. Unsupervised time-series clustering over lab data for au-
tomatic identification of uncontrolled diabetes. In Proceedings of the 4th IEEE International
Conference on Healthcare Informatics (ICHI), 2016.
A. Giannoula, A. Gutierrez-Sacristan, A. Bravo, F. Sanz, and L. I. Furlong. Identifying temporal
patterns in patient disease trajectories using dynamic ping: A population-based study. Scientific
Reports, 8(4216):1-14, March 2018.
D. T. A. Luong and V. Chandola. A k-means approach to clustering disease progressions. In Pro-
ceedings of the 5th IEEE International Conference on Healthcare Informatics (ICHI), 2017.
S. Lloyd. Least squares quantization in pcm. IEEE Transaction on Information Theory, 28(2):
129-137, March 1982.
A. Boudier, S. Chanoine, S. Accordini, J. M. Anto, X. Basaga na, J. Bousquet, P. Demoly, J. Garcia-
Aymerich, F. Gormand, J. Heinrich, C. Janson, N. Kunzli, R. Matran, C. Pison, C. Raherison,
J. Sunyer, R. Varraso, D. Jarvis, B. Leynaert, I. Pin, and V. Siroux. Data-driven adult asthma
phenotypes based on clinical characteristics are associated with asthma outcomes twenty years
later. Allegy, 74(5):953-963, May 2019.
W. M. Wami, F. Buntinx, S. Bartholomeeusen, G. Goderis, C. Mathieu, and M. Aerts. Influence
of chronic comorbidity and medication on the efficacy of treatment in patients with diabetes in
general practice. The British Journal ofGeneral Practice, 63(609):267-273, March 2013.
H. Blockeel, S. Dzeroski, J. Struyf, and B. Zenko. Predictive Clustering. Springer New York, 2017.
V. R. Konda and J. N. Tsitsiklis. Actor-critic algorithms. In Proceedings of the 13th Conference on
Neural Information Processing Systems (NIPS 2000), 2000.
B.	Yang, X. Fu, N. D. Sidiropoulos, and M. Hong. Towards k-means-friendly spaces: Simultaneous
deep learning and clustering. In Proceedings of the 34th International Conference on Machine
Learning (ICML 2017), 2017.
C.	A. Ratanamahatana, E. Keogh, A. J. Bagnall, and S. Lonardi. A novel bit level time series
representation with implications for similarity search and clustering. In Proceedings of the 9th
Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2005), 2005.
S.	Aghabozorgi, A. S. Shirkhorshidi, and T. Y. Wah. Time-series clustering - a decade review.
Information Systems, 53:16-38, May 2015.
I.	M. Baytas, C. Xiao, X. Zhang, F. Wang, A. K. Jain, and J. Zhou. Patient subtyping via time-aware
lstm networks. In Proceedings of the 23rd ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD 2017), 2017.
N. S. Madiraju, S. M. Sadat, D. Fisher, and H. Karimabadi. Deep temporal clustering: Fully unsu-
pervised learning of time-domain features. arXiv preprint arXiv:1802.01059, 2018.
J.	Xie, R. Girshick, and A. Farhadi. Unsupervised deep embedding for clustering analysis. In
Proceedings of the 33rd International Conference on Machine Learning (ICML 2016), 2017.
11
Under review as a conference paper at ICLR 2020
V. Fortuin, M. HUser, F. Locatello, H. Strathmann, and G. Ratsch. SOM-VAE: Interpretable discrete
representation learning on time series. In Proceedings of the 7th International Conference on
Learning Representations (ICLR 2019), 2019.
A. van den Oord, O. Vinyals, and K. Kavukcuoglu. Neural discrete representation learning. In Pro-
ceedings of the 31st Conference on Neural Information Processing Systems (NIPS 2017), 2017.
R.	Ranganath, A. Perotte, N. Elhadad, and D. Blei. Deep survival analysis. In Proceedings of the
1st Machine Learning for Healthcare Conference (MLHC 2016), 2016.
S.	Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735-1780,
1997.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.
In Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AIS-
TATS 2010), 2010.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
N. X. Vinh, J. Epps, and J. Bailey. Information theoretic measures for clusterings comparison: Vari-
ants, properties, normalization and correction for chance. Journal of Machine Learning Research,
11(1):2837-2854, October 2010.
L. Hubert and P. Arabie. Comparing partitions. Journal of Classification, 2(1):193-218, December
1985.
N. J. Ronan, J. Elborn, and B. J. Plant. Current and emerging comorbidities in cystic fibrosis. Presse
Med., 46(6):125-138, June 2017.
Peter J. Rousseeuw. Silhouettes: a graphical aid to the interpretation and validation of cluster anal-
ysis. Computational and Applied Mathematics, 20:53-65, 1987.
12
Under review as a conference paper at ICLR 2020
A	Variations for Regression and B inary Classification Tasks
As the task changes, estimating the label distribution and calculating the KL divergence in (2) must
be redefined accordingly:
•	For regression task, i.e., Y = R, we modify the predictor as gφ : Z → R and replace `1 by
'ι(yt, yt) = Ilyt - ytk2. Minimizing 'ι(yt, yt) is equivalent to minimizing the KL divergence
between p(yt|x1:t) and p(yt|st) when we assume these probability densities follow Gaussian
distribution with the same variance.
•	For the M -dimensional binary classification task, i.e., Y = {0, 1}M, we modify the predictor as
gφ : Z → [0,1]M and replace '1 by 'ι(yt, yt) = — PM=I ytm log ytm + (1 - ytn) log(1 - ytm)
which is required to minimize the KL divergence. Here, ytm and ytm indicate the m-th element
of yt and yt, respectively. The basic assumption here is that the distribution of each binary label
is independent given the input sequence.
B Detailed Derivation of (7)
To derive the gradient of the predictive clustering loss in (7) with respect ωA = [θ, ψ], we utilized the
ideas from actor-critic models (Konda and Tsitsiklis, 2000). The detailed derivation of the former
term in (7) is described below (for notational simplicity, We omit the expectation on Eχ,y〜PXY):
Rω I〉： Est〜Cat(∏t) ['l (yt, yt)] I = Rω ΣΣπt(st)'ι(yt,yt) I
t=1	t=1 st∈K
T
=	Rω ∏t(st)'ι(yt,yt)
t=1 st∈K
T
=XX -⅛tτ2 nt(st)'i(yt，yt)
t=1 st∈K
T
=	∏t(st)'ι(yt,yt)Rω log∏t(st)
t=1 st∈K
T
=EEst ~Cat(∏t) ['l(yt,yt)Vω log πt (St)]
t=1
(11)
C Details on the Datasets
C.1 UKCF DATASET
UK Cystic Fibrosis registry (UKCF)6 records annual follow-ups for 5,171 adult patients (aged 18
years or older) over the period from 2008 and 2015, with a total of 25,012 hospital visits. Each pa-
tient is associated with 89 variables (i.e., 11 static and 78 time-varying features), including informa-
tion on demographics and genetic mutations, bacterial infections, lung function scores, therapeutic
managements, and diagnosis on comorbidities. The detailed statistics are given in Table 4.
C.2 ADNI Dataset
Alzheimer’s Disease Neuroimaging Initiative (ADNI)7 study consists of 1,346 patients with a total of
11,651 hospital visits, which tracks the disease progression via follow-up observations at 6 months
interval. Each patient is associated with 21 variables (i.e., 5 static and 16 time-varying features),
including information on demographics, biomarkers on brain functions, and cognitive test results.
6 https://www.cysticfibrosis.org.uk/the- work- we- do/uk- cf- registry
7https://adni.loni.usc.edu
13
Under review as a conference paper at ICLR 2020
Table 4: Summary and description of the UKCF dataset.
STATIC COVARIATES
		Type	Freq.			Type	Freq.	
Demographic	Gender	Bin.	0.55					
Genetic	Class I Mutation	Bin.	0.05		Class VI Mutation	Bin.	0.86	
	Class II Mutation	Bin.	0.87		DF508 Mutation	Bin.	0.87	
	Class III Mutation	Bin.	0.89		G551D Mutation	Bin.	0.06	
	Class IV Mutation	Bin.	0.05		Homozygous	Bin.	0.58	
	Class V Mutation	Bin.	0.04		Heterozygous	Bin	0.42	
	TIME-VARYING COVARIATES									
			Mean				Mean	
		Type	(Freq.)	Min / Max		Type	(Freq.)	Min / Max
Demographic	Age	Cont.	30.4	18.0/86.0-	Height	Cont.	168.0	129.0 / 198.6
	Weight	Cont.	64.1	24.0 / 173.3	BMI	Cont.	22.6	10.9 / 30.0
	Smoking Status	Bin.	0.1					
Lung Func. Scores	FEV1	Cont.	2.3	0.2/6.3	Best FEV1	Cont.	2.5	0.3/ 8.0
	FEV1 % Pred.	Cont.	65.1	9.0/197.6	Best FEV1 % Pred.	Cont.	71.2	7.5 / 164.3
Hospitalization	IV ABX Days Hosp.	Cont.	12.3	0/431	Non-IV Hosp. Adm.	Cont.	1.2	0 / 203
	IV ABX Days Home	Cont.	11.9	0/441				
Lung Infections	B. Cepacia	Bin.	0.05		P. Aeruginosa	Bin.	0.59	
	H. Influenza	Bin.	0.05		K. Pneumoniae	Bin.	0.00	
	E. Coli	Bin.	0.01		ALCA	Bin.	0.03	
	Aspergillus	Bin.	0.14		NTM	Bin.	0.03	
	Gram-Negative	Bin.	0.01		Xanthomonas	Bin.	0.05	
	S. Aureus	Bin.	0.30					
Comorbidities	Liver Disease	Bin.	0.16		Depression	Bin.	0.07	
	Asthma	Bin.	0.15		Hemoptysis	Bin.	0.01	
	ABPA	Bin.	0.12		Pancreatitus	Bin.	0.01	
	Hypertension	Bin.	0.04		Hearing Loss	Bin.	0.03	
	Diabetes	Bin.	0.28		Gall bladder	Bin.	0.01	
	Arthropathy	Bin.	0.09		Colonic structure	Bin.	0.00	
	Bone fracture	Bin.	0.01		Intest. Obstruction	Bin.	0.08	
	Osteoporosis	Bin.	0.09		GI bleed - no var.	Bin.	0.00	
	Osteopenia	Bin.	0.21		GI bleed - var.	Bin.	0.00	
	Cancer	Bin.	0.00		Liver Enzymes	Bin.	0.16	
	Cirrhosis	Bin.	0.03		Kidney Stones	Bin.	0.02	
Treatments	Dornase Alpha	Bin.	0.56		Inhaled B. BAAC	Bin.	0.03	
	Anti-fungals	Bin.	0.07		Inhaled B. LAAC	Bin.	0.08	
	HyperSaline	Bin.	0.23		Inhaled B. SAAC	Bin.	0.05	
	HypertonicSaline	Bin.	0.01		Inhaled B. LABA	Bin.	0.11	
	Tobi Solution	Bin.	0.20		Inhaled B. Dilators	Bin.	0.57	
	Cortico Combo	Bin.	0.41		Cortico Inhaled	Bin.	0.15	
	Non-IV Ventilation	Bin.	0.05		Oral B. Theoph.	Bin.	0.04	
	Acetylcysteine	Bin.	0.02		Oral B. BA	Bin.	0.03	
	Aminoglycoside	Bin.	0.03		Oral Hypo. Agents	Bin.	0.01	
	iBuprofen	Bin.	0.00		Chronic Oral ABX	Bin.	0.526	
	Drug Dornase	Bin.	0.02		Cortico Oral	Bin.	0.14	
	HDI Buprofen	Bin.	0.00		Oxygen Therapy	Bin.	0.11	
	Tobramycin	Bin.	0.03		O2 Exacerbation	Bin.	0.03	
	Leukotriene	Bin.	0.07		O2 Nocturnal	Bin.	0.03	
	Colistin	Bin.	0.03		O2 Continuous	Bin.	0.03	
	Diabetes Insulin	Bin.	0.01		O2 Pro re nata	Bin.	0.01	
	Macrolida ABX	Bin.	0.02					
ABX: antibiotics								
Table 5: Summary and description of the ADNI dataset.
STATIC COVARIATES
		Type	Mean (Freq.)	Min/Max (Mode)			Type	Mean (Freq.)	Min/Max (Mode)
Demographic	Race	Cat.	0.93	White	Ethnicity	Cat.	0.97	No Hisp/Latino
	Education	Cat.	0.23	C16	Marital Status	Cat.	0.75	Married
Genetic	APOE4	Cont.	0.44	0/2				
	TIME-VARYING COVARIATES									
		Type	Mean	Min / Max		Type	Mean	Min / Max
Demographic	Age	Cont.	73.6	55/92				
Biomarker	Entorhinal	Cont.	3.6E+3	1.0E+3/6.7E+3	Mid Temp	Cont.	2.0E+4	8.9E+3 / 3.2E+4
	Fusiform	Cont.	1.8E+5	9.0E+4 / 2.9E+5	Ventricles	Cont.	4.1E+4	5.7E+3 / 1.6E+5
	Hippocampus	Cont.	6.9E+3	2.8E+3 / 1.1E+4	Whole Brain	Cont.	1.0E+6	6.5E+5 / 1.5E+6
	Intracranial	Cont.	1.5E+6	2.9E+2/2.1E+6				
Cognitive	ADAS-11	Cont.	8.58	0/70	ADAS-13	Cont.	13.61	0/85
	CRD Sum of Boxes	Cont.	1.21	0/17	Mini Mental State	Cont.	27.84	2/30
	RAVLT Forgetting	Cont.	4.19	-12/15	RAVLT Immediate	Cont.	38.25	0/75
	RAVLT Learning	Cont.	4.65	-5/14		RAVLT Percent	Cont.	51.70	-500/100
14
Under review as a conference paper at ICLR 2020
(a) DCN-S2S
(b) DCN-E2P, KM-E2P
(c) SOM-VAE
Figure 4: The block diagrams of the tested benchmarks.
SOM
Encoder (∕⅛)
Predictor (gφ)
(d) SOM-VAE-P
Table 6: Comparison table of benchmarks.
Methods	Handling Time-Series	Clustering Method	Similarity Measure	Label Provided	Label Associated
KM-DTW	DTW	K-means	DTW	N	N
KM-E2P (Z)	RNN	K-means	Euclidean in Z	Y	Y (indirect)
KM-E2P (Y)	RNN	K-means	Euclidean in Y	Y	Y (direct)
DCN-S2S	RNN	K-means	Euclidean in Z	N	N
DCN-E2P	RNN	K-means	Euclidean in Z	Y	Y (indirect)
SOM-VAE	Markov model	embedding mapping	reconstruction loss	N	N
SOM-VAE-P	Markov model	embedding mapping	prediction loss	Y	Y (direct)
Proposed	RNN	embedding mapping	KL divergence	Y	Y (direct)
The three diagnostic groups were normal brain functioning (0.55), mild cognitive impairment (0.43),
and Alzheimer’s disease (0.02). The detailed statistics are given in Table 5.
D Details on the Benchmarks
We compared AC-TPC in the experiments with clustering methods ranging from conventional ap-
proaches based on K -means to the state-of-the-art approaches based on deep neural networks. The
details of how we implemented the benchmarks are described as the following:
•	Dynamic time warping followed by K -means8: Dynamic time warping (DTW) is utilized to
quantify pairwise distance between two variable-length sequences and, then, K -means is applied
(denoted as KM-DTW).
•	K -means with deep neural networks: To handle variable-length time-series data, we utilized
an encoder-predictor network as depicted in Figure 4b and trained the network based on (10) for
dimensionality reduction; this is to provide fixed-length and low-dimensional representations for
time-series. Then, we applied K -means on the latent encodings z (denoted as KM-E2P (Z)) and
on the predicted label distributions y (denoted as KM-E2P (Y)), respectively. We implemented
the encoder and predictor of KM-E2P with the same network architectures with those of our
model: the encoder is a single-layer LSTM with 50 nodes and the decoder is a two-layered
fully-connected network with 50 nodes in each layer.
•	Extensions of DCN9 (Yang et al., 2017): Since the DCN is designed for static data, we utilized
a sequence-to-sequence model in Figure 4a for the encoder-decoder network as an extension to
8https://github.com/rtavenar/tslearn
9https://github.com/boyangumn/DCN
15
Under review as a conference paper at ICLR 2020
incorporate time-series data (denoted as DCN-S2S) and trained the network based on the recon-
StrUction loss (using the reconstructed input sequence Xi：t). For implementing DCN-S2S, We
used a single-layer LSTM with 50 nodes for both the encoder and the decoder. And, we aug-
mented a fully-connected layer With 50 nodes is used to reconstruct the original input sequence
from the latent representation of the decoder.
In addition, since predictive clustering is associated With the label distribution, We compared a
DCN Whose encoder-decoder structure is replaced With our encoder-predictor netWork in Figure
4b (denoted as DCN-E2P) to focus the dimensionality reduction - and, thus, finding latent en-
codings where clustering is performed - on the information for predicting the label distribution.
We implemented the encoder and predictor of DCN-E2P With the same netWork architectures
with those of our model as described in Section 5.
•	SOM-VAE10 (Fortuin et al., 2019): We compare with SOM-VAE 一 though, this method is ori-
ented towards visualization of input data via SOM - since it naturally clusters time-series data
assuming Markov property (denoted as SOM-VAE). We replace the original CNN architecture
of the encoder and the decoder with three-layered fully-connected network with 50 nodes in each
layer, respectively. The network architecture is depicted in Figure 4c where Xt and Xt indicate
the reconstructed inputs based on the encoding zt and the embedding et at time t, respectively.
In addition, we compare with a variation of SOM-VAE by replacing the decoder with the predic-
tor to encourage the latent encoding to capture information for predicting the label distribution
(denoted as SOM-VAE-P). For the implementation, we replaced the decoder of SOM-VAE with
our predictor which is a two-layered fully-connected layer with 50 nodes in each layer to predict
the label distribution as illustrated in Figure 4d. Here, yt and yt indicate the predicted labels
based on the encoding zt and the embedding et at time t, respectively.
For both cases, we used the default values for balancing coefficients of SOM-VAE and the di-
mension of SOM to be equal to K.
We compared and summarized major components of the benchmarks in Table 6.
E	Additional Experiments
E.1 Additional Results on Targeting Multiple Future Outcomes
Throughout the experiment in Section 5.6, we identified 12 subgroups of patients that are associated
with the next-year development of 22 different comorbidities in the UKCF dataset. In Table 7, we
reported 12 identified clusters and the full list of comorbidities developed in the next year since the
latest observation and the corresponding frequency. Here, the frequency is calculated in a cluster-
specific fashion based on the true label.
E.2 Trade-Off between Clustering and Prediction Performance
In predictive clustering, the trade-off between the clustering performance (for better interpretability)
一 which quantifies how the data samples are homogeneous within each cluster and heterogeneous
across clusters with respect to the future outcomes of interest - and the prediction performance is a
common issue. The most important parameter that governs this trade-off is the number of clusters.
More specifically, increasing the number of clusters will make the predictive clusters have higher
diversity to represent the output distribution and, thus, will increase the prediction performance
while decreasing the clustering performance. One extreme example is that there are as many clusters
as data samples which will make the identified clusters fully individualized; as a consequence, each
cluster will lose the interpretability as it no longer groups similar data samples.
To highlight this trade-off, we conducted experiments under the same experimental setup with that of
Section 5.6 where our aim is to identify underlying (unknown) clusters when the future outcome of
interest is high-dimensional. For the performance measures, we utilized the AUROC and AUPRC to
assess the prediction performance, and utilized the average Silhouette index (SI) (Rousseeuw, 1987)
一 a widely used measure of how similar a member is to its own cluster (homogeneity within a cluster)
compared to other clusters (heterogeneity across clusters ) when the ground-truth cluster labels are
10https://github.com/ratschlab/SOM-VAE
16
Under review as a conference paper at ICLR 2020
not available - to assess the identified clusters. Formally, the SI for a
given as follows:
subsequence x1n:t ∈ Ck can be
SI(n) =
b(n) - a(n)
max a(n), b(n)
(12)
where a(n) = ∣ckl-ι Pm=n l∣yn - ytmkιand b(n) = mink，=k 尚 Pm∈ck0 kyn - ytmkι. Here,
we used the L1-distance between the ground-truth labels of the future outcomes of interest since
our goal is to group input subsequences with similar future outcomes. To control the number of
identified clusters (i.e., the activated clusters) of our method, we set β = 0 (since the embedding
separation loss in (5) controls the activation of clusters) and reported the performance by increasing
the number of possible clusters K .
Figure 5: AUROC, AUPRC, and average SI (mean and 95% confidence interval) and the number of
activated clusters with various K .
As can be seen in Figure 5, the prediction performance increased with an increased number of identi-
fied clusters due to the higher diversity to represent the label distribution while making the identified
clusters less interpretable (i.e., the cohesion and separation among clusters become ambiguous as
seen in the low average SI). On the other hand, when we set β = 1.0 (which is selected based on the
validation loss in 3), our method consistently identified a similar number of clusters for K > 20,
i.e., 13.8 on average, in a data-driven fashion and provided slightly reduced prediction performance
with significantly better interpretability, i.e., the average SI 0.120 on average. This highlights the
usefulness of (5) which helps to identify clusters with different label distributions.
17
Under review as a conference paper at ICLR 2020
Table 7: The comorbidities developed in the next year for the 12 identified clusters. The values in
parentheses indicate the corresponding frequency.
Clusters	Comorbidities and Frequencies			
Cluster 0	Diabetes (0.85) Hypertens (0.08) ABPA (0.04) Asthma (0.02) Pancreatitis (0.01) GI bleed - no var. (0.00)	Liver Enzymes (0.21) Osteopenia (0.07) Liver Disease (0.04) Kidney Stones (0.01) Cancer (0.00) GI bleed — var. (0.00)	Arthropathy (0.14) Intest. Obstruction (0.07) Osteoporosis (0.03) Bone fracture (0.01) Gall bladder (0.00)	Depression (0.10) Cirrhosis (0.04) Hearing Loss (0.03) Hemoptysis (0.01) Colonic stricture (0.00)
Cluster 1	Liver Enzymes (0.09) Diabetes (0.06) Liver Disease (0.03) Kidney Stones (0.02) Cancer (0.01) GI bleed - no var. (0.00)	Arthropathy (0.08) Osteopenia (0.05) Hearing Loss (0.03) Hypertension (0.01) Hemoptysis (0.00) GI bleed — var. (0.00)	Depression (0.07) ABPA (0.04) Osteoporosis (0.02) Cirrhosis (0.01) Bone fracture (0.00)	Intest. Obstruction (0.06) Asthma (0.03) Pancreatitis (0.02) Gall bladder (0.01) Colonic stricture (0.00)
Cluster 2	ABPA (0.77) Liver Enzymes (0.07) Liver Disease (0.04) Osteoporosis (0.02) Kidney Stones (0.01) GI bleed - no var. (0.00)	Osteopenia (0.21) Diabetes (0.06) Arthropathy (0.04) Hypertension (0.01) Gall bladder (0.01) GI bleed — var. (0.00)	Intest. Obstruction (0.11) Depression (0.05) Asthma (0.03) Cancer (0.01) Hemoptysis (0.00)	Hearing Loss (0.10) Pancreatitis (0.05) Bone fracture (0.02) Cirrhosis (0.01) Colonic stricture (0.00)
Cluster 3	Asthma (0.89) Liver Enzymes (0.24) Arthropathy (0.05) Cirrhosis (0.02) Cancer (0.01) GI bleed - no var. (0.00)	Liver Disease (0.87) ABPA (0.15) Intest. Obstruction (0.05) Kidney Stones (0.02) Bone fracture (0.00) GI bleed — var. (0.00)	Diabetes (0.29) Osteoporosis (0.11) Depression (0.04) Pancreatitis (0.02) Hemoptysis (0.00)	Osteopenia (0.28) Hearing Loss (0.06) Hypertension (0.03) Gall bladder (0.02) Colonic stricture (0.00)
Cluster 4	Osteoporosis (0.76) Osteopenia (0.15) Hearing Loss (0.09) Kidney Stones (0.03) Gall bladder (0.01) GI bleed - no var. (0.00)	Diabetes (0.43) Depression (0.13) Liver Disease (0.08) Asthma (0.02) Pancreatitis (0.01) GI bleed — var. (0.00)	Arthropathy (0.20) Intest. Obstruction (0.11) Hypertension (0.07) Hemoptysis (0.02) Cancer (0.00)	Liver Enzymes (0.18) ABPA (0.11) Cirrhosis (0.07) Bone fracture (0.02) Colonic stricture (0.00)
Cluster 5	Asthma (0.88) Liver Enzymes (0.22) Hypertension (0.10) Bone fracture (0.01) Cancer (0.01) Colonic stricture (0.00)	Diabetes (0.81) Depression (0.15) Cirrhosis (0.10) Hemoptysis (0.01) Kidney Stones (0.01) GI bleed — no var. (0.00)	Osteopenia (0.28) Osteoporosis (0.14) Liver Disease (0.09) Pancreatitis (0.01) GIbleed-var. (0.01)	ABPA (0.24) Intest. Obstruction (0.12) Arthropathy (0.08) Hearing Loss (0.01) Gall bladder (0.00)
Cluster 6	Liver Disease (0.85) Arthropathy (0.07) Depression (0.03) Hemoptysis (0.02) Gall bladder (0.00) GI bleed - no var. (0.00)	Liver Enzymes (0.37) Diabetes (0.06) Asthma (0.03) Hypertension (0.01) Bone fracture (0.00) GI bleed — var. (0.00)	Osteopenia (0.27) Intest. Obstruction (0.06) Hearing Loss (0.03) Kidney Stones (0.01) Cancer (0.00)	ABPA (0.09) Osteoporosis (0.05) Cirrhosis (0.02) Pancreatitis (0.00) Colonic stricture (0.00)
Cluster 7	ABPA (0.83) Liver Enzymes (0.15) Hearing Loss (0.07) Asthma (0.01) Cancer (0.00) GI bleed - no var. (0.00)	Diabetes (0.78) Intest. Obstruction (0.12) Arthropathy (0.06) Bone fracture (0.01) Pancreatitis (0.00) GI bleed — var. (0.00)	Osteopenia (0.25) Liver Disease (0.09) Depression (0.04) Kidney Stones (0.01) Gall bladder (0.00)	Osteoporosis (0.24) Hypertension (0.07) Cirrhosis (0.02) Hemoptysis (0.01) Colonic stricture (0.00)
Cluster 8	Diabetes (0.94) Hearing Loss (0.11) Depression (0.08) Kidney Stones (0.05) Cancer (0.00) GI bleed - no var. (0.00)	Liver Disease (0.83) Osteoporosis (0.10) ABPA (0.07) Asthma (0.02) Pancreatitis (0.00) GI bleed — var. (0.00)	Liver Enzymes (0.43) Intest. Obstruction (0.09) Arthropathy (0.06) Hemoptysis (0.01) Gall bladder (0.00)	Osteopenia (0.30) Cirrhosis (0.08) Hypertension (0.05) Bone fracture (0.01) Colonic stricture (0.00)
Cluster 9	Asthma (0.89) Intest. Obstruction (0.11) Liver Enzymes (0.06) Pancreatitis (0.02) Gall bladder (0.01) GI bleed - no var. (0.00)	Osteopenia (0.26) Depression (0.08) Hemoptysis (0.03) Bone fracture (0.01) Hearing Loss (0.01) GI bleed — var. (0.00)	ABPA (0.19) Osteoporosis (0.08) Hypertension (0.02) Cancer (0.01) Kidney Stones (0.00)	Arthropathy (0.14) Diabetes (0.06) Liver Disease (0.02) Cirrhosis (0.01) Colonic stricture (0.00)
Cluster 10	Osteopenia (0.82) Liver Enzymes (0.18) Osteoporosis (0.10) Cirrhosis (0.05) Bone fracture (0.00) Colonic stricture (0.00)	Diabetes (0.81) Hypertension (0.16) Intest. Obstruction (0.09) Asthma (0.01) Hemoptysis (0.00) GI bleed — no var. (0.00)	Arthropathy (0.23) Hearing Loss (0.10) ABPA (0.09) Cancer (0.00) Pancreatitis (0.00)	Depression (0.19) Liver Disease (0.10) Kidney Stones (0.07) GI bleed - var. (0.00) Gall bladder (0.00)
Cluster 11	Osteopenia (0.77) Hypertension (0.06) Liver Disease (0.05) Asthma (0.02) Kidney Stones (0.00) GI bleed - no var. (0.00)	Liver Enzymes (0.18) Diabetes (0.06) Osteoporosis (0.04) Pancreatitis (0.02) Gall bladder (0.00) GI bleed — var. (0.00)	Arthropathy (0.12) Hearing Loss (0.06) Intest. Obstruction (0.04) Bone fracture (0.01) Colonic stricture (0.00)	Depression (0.09) ABPA (0.05) Cirrhosis (0.02) Cancer (0.01) Hemoptysis (0.00)
18
Under review as a conference paper at ICLR 2020
F Pseudo-Code of AC-TPC
As illustrated in Section 3.2, AC-TPC is trained in an iterative fashion. We provide the pseudo-code
for optimizing our model in Algorithm 1 and that for initializing the parameters in Algorithm 2.
Algorithm 1 Pseudo-code for Optimizing AC-TPC
Input: Dataset D = {(xtn, ytn)tT=n1}nN=1, number of clusters K, coefficients (α, β),
learning rate (ηA , ηC , ηE), mini-batch size nmb, and update step M
Output: AC-TPC parameters (θ, ψ, φ) and the embedding dictionary E
Initialize parameters (θ, ψ, φ) and the embedding dictionary E via Algorithm 2
repeat
Optimize the Encoder, Selector, and Predictor
for m = 1,…，M do
Sample a mini-batch of nmb data samples: {(xR, yn)T=nι}n=1 〜D
for n = 1, ∙.. ,nmb do
Calculate the assignment probability: ∏ = [∏n(1) ∙∙∙ ∏n(K)] — hψ(fθ (xf：t))
Draw the cluster assignment: Sn 〜Cat(∏n)
Calculate the label distributions: yn — gφ(e(sn)) and yn — gφ(fθ(x^t))
end for
Update the encoder fθ and selector hψ :
θ 一 θ — nA
nmb T n
ɪ XX 'ι(yn
nmb n=1 t=1
,yn)Vθ log ∏n(sn)—
K
αVθ X πn (k) log ∏n(k)
k=1
ψ 一 ψ — nA
Update the predictor gφ :
nmb T n	K
ΣΣ'ι(yn,yn)Vψ log∏n(sn) — αVψ Enn(k)logπn(k)
n=1 t=1	k=1
nmb T n
—-nc± XXVφ'ι(yn,yn)
nmb n=1 t=1
end for
Optimize the Cluster Centroids
for m = 1,…，M do
Sample a mini-batch of nmb data samples: {(xR, yn)T=nι}n=1 〜D
for n = 1, ∙.. ,nmb do
Calculate the assignment probability: πn = [πn(1) ∙∙∙ πn(K)] — hψ(fθ(xn.t)')
Draw the cluster assignment: Sn 〜Cat(πn)
Calculate the label distributions: yn — gφ(e(s7))
end for
for k = 1,…，K do
Update the embeddings e(k):
e(k) — e(k)—
1 nmb T n	K
nE n~. XXVe(k)'ι(yn,yn) — Y X Ve(k)'ι(gφ(e(k)),gφ(e(k0)))
nmb n=1 t=1	k0=1
k0 6=k
end for
Update the embedding dictionary: E — {e(1),... e(K)}
end for
until convergence
19
Under review as a conference paper at ICLR 2020
Algorithm 2 Pseudo-code for pre-training AC-TPC
Input: Dataset D = {(xtn, ytn)tT=n1}nN=1, number of clusters K, learning rate η, mini-batch size nmb
Output: AC-TPC parameters (θ, ψ, φ) and the embedding dictionary E
Initialize parameters (θ, ψ, φ) via Xavier Initializer
Predrain the Encoder and Predictor
repeat
Sample a mini-batch of nmb data samples: {(xn,yn)T=nι}n=1 〜D
for n = 1,…，nmb do
Calculate the label distributions: yn — gφ(fθ(xn：t))
end for
nmb T n	nmb T n
θ 一 θ-η± XXNe 'ι(yn,yn)	φ — φ - η ——XX Vφ'ι(yn,yn)
nmb n=1 t=1	nmb n=1 t=1
until convergence
Initialize the Cluster CentroidS
Calculate the embedding dictionary E and initial cluster assignments Cn
E, {{cn}T=nι}N=ι — K-means({{zn}W}N=ι,K)
Pre-train the Selector
repeat
Sample a mini-batch of nmb data samples: {(xn,yn)T=nι}n=1 〜D
for n = 1, ∙.. ,nmb do
Calculate the assignment probability: πn = [πn(1) ∙∙∙ πn(K)] — hψ (fe (xf：t))
end for
Update the selector hψ :
1 nmb T n K
ψ — ψ + η— ΣΣΣctn (k) log πtn(k)
nmb
n=1 t=1 k=1
until convergence
20