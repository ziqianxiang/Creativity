Under review as a conference paper at ICLR 2020
Divide-and-Conquer Adversarial Learning
for High-Resolution Image Enhancement
Anonymous authors
Paper under double-blind review
Ab stract
This paper introduces a divide-and-conquer inspired adversarial learning (DA-
CAL) approach for photo enhancement. The key idea is to decompose the photo
enhancement process into hierarchically multiple sub-problems, which can be bet-
ter conquered from bottom to up. On the top level, we propose a perception-based
division to learn additive and multiplicative components, required to translate a
low-quality image into its high-quality counterpart. On the intermediate level,
we use a frequency-based division with generative adversarial network (GAN) to
weakly supervise the photo enhancement process. On the lower level, we design
a dimension-based division that enables the GAN model to better approximates
the distribution distance on multiple independent one-dimensional data to train
the GAN model. While considering all three hierarchies, we develop a multiscale
training approach to optimize the image enhancement process, suitable for high-
resolution images, in a weakly-supervised manner. Both quantitative and qualita-
tive results clearly demonstrate that the proposed DACAL achieves the state-of-
the-art performance for high-resolution image enhancement.
1	Introduction
Despite many mobile camera technological advances we have today, our captured images often still
come with limited dynamic range, undesirable color rendition, and unsatisfactory texture sharpness.
Among many possible causes, low-light environments and under/over-exposed regions usually in-
troduce severe lack of texture details and low-dynamic range coverage, respectively. Another critical
issue is the amplification (during the enhancement process) of noise in the dark and/or texture-less
regions, where the enhancement may not even be necessary. Due to these issues, images acquired
under different conditions, or different parts of a single image, may require separate enhancement
operations. In this context, customarily used context/content agnostic enhancement methods often
lead to a poor performance in the overall visual assessment. Therefore, comprehensive methods that
improve the perceptual quality of images are in high demand.
In recent years, deep image enhancement methods have demonstrated their superiority for color
enrichment, texture sharpening as well as contrast adjustment. A large amount of works like (Dong
et al. (2014); Kim et al. (2016); Mao et al. (2016); Shi et al. (2016); Wang et al. (2018b); Timofte
et al. (2018); Nah et al. (2017); Zhang et al. (2017b; 2016); Yu et al. (2018); Wu et al. (2018);
Yang et al. (2018); Ruixing Wang & Jia (2019)) have made great success in one of such sub-tasks.
However, in the real-world setting it is non-trivial to decompose the low-quality factors and treat
them respectively from a low-quality images. Another family of approaches (e.g., Gharbi et al.
(2017); Ignatov et al. (2017); Liu & Jung (2018); Huang et al. (2018); Chen et al. (2018)) learns the
enhancement as some non-linear mapping between low and high-quality image pairs, in a supervised
manner. These methods require the desired high-quality images to be well aligned with the low-
quality ones. Such image pairs are collected either by manually enhancing the low-quality images
(using professional artists) or by using a calibrated rig of low and high-end cameras, with a very
small baseline. Unfortunately, the data collection for these setups is not convenient, mainly due to
the expensive manual retouching process and the physical limitation for small baseline. Moreover,
fully-supervised methods often lack flexibility towards new domain adaptation, thus requiring low-
high paired acquisitions for every low-end camera.
1
Under review as a conference paper at ICLR 2020
Figure 1: Conceptual illustration of the proposed adversarial learning approach of divide-and-
ConqUer (DACAL). It decomposes the high-quality image distribution learning problem into
perception-, frequency- and dimension-based sub-problems in a hierarchical structure. Following
the division, the sub-problems are conquered and combined in a deep adversarial training fashion.
To address the limitations of fully-supervised methods, the weakly-supervised deep photo enhance-
ment methods (e.g., Ignatov et al. (2018); Chen et al. (2018)) have emerged recently. These methods
merely require a set of target good quality images, whose contents and viewpoints share some simi-
larity with those of the low-quality image domain, without requiring image acquisitions for the same
scenes. On the collected unpaired low- and high-quality images, such kind of methods typically ap-
ply generative adversarial networks (GANs), which alternately optimizes its two components (i.e.,
generator and discriminator) using a min-max objective. The discriminator is optimized to approxi-
mate the desired-quality image distribution, and the generator is guided by the discriminator to learn
the enhancement map. While achieving some success, they still have drawbacks when enhancing
mixed visual perceptions or treating high-resolution images. Without the implementation ofa pixel-
wise loss function in a supervised setup, it appears exceptionally difficult to simultaneously enhance
multiple perceptual components (e.g., color, texture and illumination). Besides, due to the com-
putational complexity and unstable adversarial training, the existing image enhancement methods
treat high-resolution images using either the downscaled version or patch-wise processing strategies,
neither of which are optimal. It is needless to mention that the downscaling loses images details,
whereas, patch-wise enhancement does not maintain the spatial consistency over the whole image.
In this paper, inspired by the concept of the divide-and-conquer (DAC) paradigm, we aim at decom-
posing the original problem of high-quality image distribution learning into sub-problems, which
can be better addressed from bottom to up. For this purpose, we propose a DAC inspired adver-
sarial learning approach1 to better deal with the high-resolution photo enhancement problem. As
depicted in Figure 1, our basic learning process innovatively introduces the following three parts to
the context of adversarial learning for high-resolution image enhancement.
•	Divide: We hierarchically decompose the problem into three sub-problems: perception-
based (additive and multiplicative maps), frequency-based (low and high frequencies), and
dimension-based (multiple one dimensions) data distribution learning problems (from top
to bottom). The third sub-problem that learns for each dimension separately is the basics for
the original problem. In terms of implementation, these divisions are respectively carried
out by: (a) perception-wise branching of the generator; (b) frequency-wise division of the
discriminator’s inputs; and (c) dimension-wise division of the discriminator’s output.
•	Conquer: To conquer the base sub-problems, we exploit an adaptive sliced Wasserstein
GAN (AdaSWGAN) model. The basic idea of the AdaSWGAN is to first factorize high-
dimensional distributions into their multiple one-dimensional marginal distributions, fol-
lowed by the approximation of each one-dimensional distributions independently.
•	Combine: Our model combines the solutions from bottom to top by, (a) aggregating the
approximated one-dimensional distributions to obtain the frequency-based solutions; (b)
averaging frequency-based solutions to obtain the perception-based solutions; (c) summing
two perception-based solutions to learn the high-quality image distribution.
1There exist some networks like (Romaniuk & Hall (1993); Ghosh et al. (2017); Nowak et al. (2018)) apply-
ing the paradigm of DAC to general neural networks. While we found two concurrent works (Kim et al. (2019);
Lin et al. (2019)) introduce the DAC concept to adversarial networks, they merely study the decomposition of
generator. In contrast, our approach applies the DAC concept to the whole adversarial learning process.
2
Under review as a conference paper at ICLR 2020
Figure 2: Illustration of the proposed network design for our adversarial learning approach of divide-
and-conquer. Specially, We apply the perception-based division to the generator, the frequency-
based one to the discriminator input, and the dimension-based one to the discriminator output.
2	Proposed Method
2.1	Perception-based Division
Given an input low-quality image x ∈ X , image enhancement seeks a mapping function E : X →
Y , such that y = E(x), where y ∈ Y is the desired-quality image. We suggest to divide the
mapping function learning problem into two subproblems, which learn additive and multiplicative
perceptions respectively. On one hand, learning additive map is encouraged by the success of resid-
ual learning on image colorization and super-resolution et cetera. They apply the additive mapping
E(x) = x + rx (where rx is the residual component) to learn additive differences from low-
quality images to desired-quality ones. On the other hand, learning multiplicative map is inspired
by existing works like (Huang et al. (2009); Bioucas-Dias & Figueiredo (2010); Aubert & Aujol
(2008); Engel et al. (2016); Ruixing Wang & Jia (2019); Guo et al. (2017); Ghosh et al. (2017);
Fu et al. (2016)). Such works apply the multiplicative mapping E(x) = X * s-1 (where Sx is the
multiplicative component) to address the problem of multiplicative noise removal or illumination
improvement successfully. Accordingly, we introduce the perception-wise division to the gener-
ator so that the benefits of both additive and multiplicative map learning approaches can be fully
exploited. Formally, the final map of the generator is
E(x) = x + rx + β(x * sx-1),	(1)
where β plays the trade-off between such two perception-based components2 and the operator ‘*’
means element-wise multiplication.
To approximate the additive component rx and the multiplicative component sx , we design a two-
stream U-Net (Ronneberger et al. (2015)) for the generator (i.e., photo enhancer). As shown in
Figure 2, we follow the standard U-Net design to apply the skip connection across the encoder and
decoder 3 . As skip connections concatenate local feature maps, they are good for reliable local fea-
ture extraction. In addition, we investigate that global information (e.g., scene category, object class
or overall illumination) is useful for individual pixels to guide their local adjustment. Accordingly,
we exploit a global concatenation operation to embed high-level information into decoded feature
maps. While the encoder can transform input images into low-dimensional features, it does not
always achieve a global feature (with a vector form) when the input images are of different sizes.
To address this issue, we additionally apply an average pooling operation with a fully convolutional
layer to extract the global feature vector. As a result, this design enables our model to enhance full-
resolution images while it is trained on down-scaled images. On top of the decoder, we design two
different branches to learn the additive and multiplicative components separately. During the early
training, we optimize the additive and multiplicative mappings alternately. In the end of training,
we aggregate the resulting approximations from these two branches, followed by a group of regular
convolutional network layers.
2Our experiments were conducted with β = 1, which can be chosen differently for known sub-tasks priors.
3The encoder includes groups of convolution, SELU (Klambauer et al. (2017)) and batch normalization
(Ioffe & Szegedy (2015)), while the decoder consists of convolution, resizing operation based layers.
3
Under review as a conference paper at ICLR 2020
The idea of our designed two-stream U-Net is related to Chen et al. (2018); Ruixing Wang & Jia
(2019). For the additive component learning, Chen et al. (2018) introduces a global U-Net. Com-
pared to our model, the global U-Net from Chen et al. (2018) can only accept images of a fixed
low-resolution due to the hard design (a fixed filter for the fully-connected layer) for the global
feature extraction. For example, when feeding 8 × 8 × 128 feature map into the global feature ex-
traction, it has to use 8 × 8 filters to obtain the global feature 1 × 1 × 128. For different image
sizes, the resulting embedded feature size will be different, say 4 × 8 × 128. In such cases, the
global feature extraction will fail. For the multiplicative component learning, Ruixing Wang & Jia
(2019) designs a bilateral-filtering based network, while our enhancer adopts regular convolutions
and resizing operations.
2.2	Frequency-based Division
For the supervised photo enhancement problem that requires corresponding ground-truth high-
quality images for guidance, the common reconstruction loss can be used to train the proposed
deep enhancer. In contrast, the weakly-supervised photo enhancement task merely has a set of good
quality images for reference, and thus for this task we need to learn the map from the low-quality
image domain to high-quality image domain. One of the most promising approaches is cyclic GAN
models (Zhu et al. (2017)). Hence, we choose to adopt the cyclic GAN objective to train our devel-
oped enhancer:
min max LGAN(E, C) + LGAN(E,C) + γιLcyc(E,E) + γ2Gd(E,E),	(2)
E,E C,C	' "
where E : X → Y and E : Y → X are used for forward and backward image translation,
respectively. They share the same design using the proposed two-stream U-Net. Here, C and C
are the corresponding discriminators (or critics) which are used to guide the training of E and E,
respectively. The GAN loss LGAN(E, C) and LGAN(E) C) are first suggested to use the adaptive
Wasserstein GAN loss (Chen et al. (2018)) as,
LCAN(E,C) = Ey〜Py [C(y)] - Ey〜PE [C(E(x))]+ λEy〜Py [max(0, ∣∣VyC(y)∣∣2 - 1)],
LEAN(E,C) = Ex 〜Pχ[C(E(x))],
where y are random samples following the distribution Py, which is uniformly sampled along
straight lines between pairs of points sampled from Py and PE, and VyC(y) is the gradient w.r.t y.
Standard cyclic GAN models suggest to use the cyclic consistency loss and identity mapping loss.
We follow (Chen et al. (2018)) to define these two losses based on mean square error (MSE):
ʌ ʌ ʌ
Lcyc(E, E) = Ex〜Px [kE(E(x)) - xk2] + Ey〜Py [∣∣E(E(y)) - yk2],
ʌ ʌ
Lid(E, E) = Ex〜P, [∣∣E(x) - x∣∣2]+ Ey〜Py [∣∣E(y) - y∣∣2].
To further decompose the perception-based distribution learning, we follow (Ignatov et al. (2017;
2018)) to apply frequency-based division to the discriminator C. Specifically, we adopt the sep-
aration of enhancing on low frequencies (e.g., main structures, major contents and colors), and
optimizing on high frequencies (e.g., textures and small details). For this purpose, we replicate the
discriminators to specialize on various frequencies of input images. Without loss of generality, we
study two frequencies for discriminators: For low frequency data, we utilize the RGB image blurred
by a Gaussian kernels, whereas, the grayscale component of the discriminator input images is used
as the high frequency data. Both are equal in architecture and parameter values, and their GAN
losses are averaged in the end. In this case, individual discriminators are encouraged to focus on an
easier distribution approximation task, and thus become more reliable to supervise the training of
the proposed photo enhancer.
2.3	Dimension-based Division and Solution
While the frequency-based division for the discriminator’s input can reduce the complexity of train-
ing Wasserstein GAN to some extent, it still suffers from the unstable training issue when the raw
images lie in a high dimensional space, where we can only select samples very sparsely during train-
ing. To address this issue, we suggest to further decompose the challenging estimation of a high-
dimensional distribution into simpler estimation of multiple one-dimensional distributions, which is
4
Under review as a conference paper at ICLR 2020
Figure 3: Results of WGAN (Gulrajani et al. (2017)), AdaWGAN (Chen et al. (2018)),
SWGAN (Wu et al. (2019)) and the proposed AdaSWGAN (three rows indicate training for 1000,
2500 and 5000 iterations resp.) for the 25 Gaussians dataset. The orange points represent the target
distribution, green points are the generated samples, and the curves indicate the value surfaces of the
discriminators of compared models.
the underlying idea of sliced Wasserstein distance computation. To achieve this goal, We exploit an
adaptive version of sliced Wasserstein GAN (AdaSWGAN) loss:
mEin mCax
/ 1
θ∈Sn-1
(Ey-Py [C(y )]-Ey-PE [C(E (x))])
+ λEy~Py [max(0, kVyCCy)Il2 - 1)],
(5)
where θ is the orthogonal projection matrix for the decomposition from high-dimensional data to
independent one-dimensional ones under C, y^ denotes random samples following the distribution
PX which is sampled uniformly along straight lines between pairs of points sampled from Pχ
and PG, and VyC(y) is the gradient with respect to y^. As the parameter λ weights the gradient
penalty, it can highly influence the trade-off between the original sliced Wasserstein distance and
the Lipschitz constraint. To reach a good balance, we adopt an adaptive learning manner to update
the hyperparameter by observing the moving average of gradients:
______ _________ V C ㈤)
vyC(y) = η VyCW +(1-η) Vyfy2
(6)
where λ is the weight for gradient penalty, and η is the decay constant. When VyC(y) is larger than
a fixed upper bound τ, the relative influence of λ becomes too small and therefore the penalty is not
strong enough to satisfy the Lipschitz constraint. In this case, the weight λ should be increased with
a certain scale (in our experiments, we always set 2 times bigger). Otherwise, λ is decreased to half
of the current value.
During training, the parameterized projection matrices θ should remain orthogonal. For this require-
ment we first initialize the parameters with random orthogonal matrices through QR decomposition,
then update them on a Stifel manifold during training. Particularly, we follow (Wu et al. (2019)) to
optimize the orthogonal matrices θ on Stiefel manifolds.
Few variations of sliced Wasserstein GANs (Deshpande et al. (2018); Ishan Deshpande (2019);
Wu et al. (2019)) are proposed in the recent years. Differently from (Deshpande et al. (2018); Is-
han Deshpande (2019)), we use the parameterized projection matrix to optimize the sliced Wasser-
stein distance approximation. When compared to (Wu et al. (2019)), we adopt an adaptive weight
scheme to adjust the penalty weight. To show the advantage of the proposed AdaSWGAN, we fol-
low the sampling strategy of (Gulrajani et al. (2017)) to generate 25 Gaussian distributed data. On
the toy dataset, we compare it against the four existing Wasserstein GAN models4. The results in
Figure 3 show that the proposed AdaSWGAN has a better convergence to fit the toy data than the
other compared methods.
4For fair comparison, same generator and discriminator architectures (of Gulrajani et al. (2017)) were used
for all compared methods
5
Under review as a conference paper at ICLR 2020
3	Multiscale Network and Training
To improve the enhancement on high-resolution images, we introduce a multiscale enhancer that is
trained on images with various resolutions. This design allows for a wider range of learned receptive
fields, from coarse to fine. The multiscale enhancer consists of various enhancers at multiple scales.
Let’s take a two-scale case for example. The two enhancers are denoted as El , Eh, which enhance
images at resolution of 512 × 256 and 1024 × 512, respectively. The multiscaled design of enhancers
is expected to effectively aggregate the visual features over various scales.
The high-scale enhancer Eh has the same structure as the low-scale enhancer El, which is shown in
Figure 2. To transmit the low-scale information smoothly from the low-scale to the high-scale com-
ponents, the high-scale enhancer also inherits the learned parameters from the low-scale enhancer. In
addition, it repeats some more layers on the bottom and the top of the low-scale enhancer. Specially,
it adds one more down-sampling component including convolution, SELU, and batch normalization
at the bottom. One more up-sampling component is applied to the top. It contains an up-scale re-
sizing and a convolution operation. Following the lower-scale enhancer design, we apply the same
two-stream design to learn the additive and multiplicative components for the high-resolution image
enhancement as well.
It is known that synthesizing high-resolution images yields significant challenges to the GAN train-
ing. For example, differentiating high-resolution real and generated images requires the discrimina-
tor to have a deeper architecture or convolutions with a larger receptive field. But such two solutions
will often lead to overfitting and out of memory issues. Besides, due to the memory limit, we have
to use smaller minibatches that will further harm training stability. Therefore, we also follow the de-
sign of multiscale enhancer to exploit multiscale discriminators for more reliable training guidance
on different resolutions. Similarly, we insert some more layers into the bottom of the lower-scale
discriminator to construct higher-scale discriminators for more reliable training guidance on higher-
resolutions.
For more details on the design of the multiscale enhancer and discriminator, please refer to the
appendix. We train the proposed multiscale enhancer with three stages. At the first stage, we train
the lower-enhancer El using either reconstruction loss in the supervised case or using the proposed
frequency- and dimension-based GAN loss for the weakly-supervised case. At the second stage,
we tune the parameters of the newly added layers in the higher-scale enhancer Eh while fixing the
parameters of the lower-scale enhancer El. Finally, we train the multiscale enhancer as a whole.
To overcome the unstable GAN training issue on high-resolution images, there exist various cas-
caded or multiscaled network designs like (Denton et al. (2015); Dosovitskiy & Brox (2016); Gatys
et al. (2016); Johnson et al. (2016); Zhang et al. (2017a); Huang et al. (2017); Karras et al. (2017);
Wang et al. (2018a)) that synthesize high-resolution images. However, such works rarely study the
multiscaled network based on the U-Net design, which has shown its advantages over other net-
works for image enhancement (Chen et al. (2018); Huang et al. (2018)). In contrast, our multiscale
enhancer is built upon a two-stream U-Net based model, which is tailored for better high-resolution
image enhancement.
4	Experiment
4.1	Supervised Photo Enhancement
This task aims at learning the enhancement mapping between low- and high-quality image pairs in
a supervised fashion. We use the commonly-used MIT-Adobe FiveK (Bychkovsky et al. (2011))
dataset for this task. The dataset consists of 5,000 high-resolution (higher than 1K) images, each of
which is retouched by five experts. For the evaluation, we use 2,250 images and their retouched ver-
sions from photographer C for training, and the remaining 498 images are used for testing. Distinct
from most of existing works, we go for high-resolution image enhancement. To study this task, we
compare our proposed adersarial learning approach of divide-and-conquer (DACAL) against state-
of-the-art methods including White-Box (WB) (Hu et al. (2018)), Distort-and-Recover (DR) (Park
et al. (2018)), DSLR-Photo Enhancement (DPED) (Ignatov et al. (2017)), and the supervised ver-
sion of the Deep Photo Enhancement (DPE) method (Chen et al. (2018)). Except DPED, the rest
three methods are originally designed for down-scaled image enhancement. In contrast, our model
6
Under review as a conference paper at ICLR 2020
Table 1: PSNR and SSIM results for the MIT-Adobe FiveK (Bychkovsky et al. (2011)) test im-
ages. Here, WB and DR indicate the White-Box and Distort-and-Recover methods, respectively.
DACALl1 , DACALl2 , DACALl3 and DACALl represent the use of individual additive, individual
multiplicative, multiplicative cascaded by additive, and our suggested parallel fusion (two-stream
strategy), respectively. DACALh is our higher-scale version. PSNRd/SSIMd and PSNRf /SSIMf
indicate the results on downscaled images and full-resolution images, respectively.
	WB	DR	DPED	DPE	DACALlI	DACALl2	DACALl3	DACALl	DACALh
PSNRd	18.86	21.64	21.05	22.10	2273	22.99	23.01	23.52	24.15
PSNRf	19.09	21.52	20.86	21.65	22.43	22.69	23.02	23.56	24.07
SSIMd	0.928	0.936	0.922	0.947	-0.958-	0.942	0.949	0.959	0.962
SSIMf	0.920	0.922	0.916	0.894	0.948	0.942	0.940	0.954	0.956
(a) Input
(b) Additive component
(c) Multiplicative component
(d) Additive map
(e) Multiplicative map
(f) Fused Map
Figure 4: Visual results of additive, multiplicative components and their combination. It is best to
zoom in on the high-resolution pictures.
can be trained on multiscaled images. For details on our training details5, please refer to the ap-
pendix. As the dataset has paired original/retouched images, we use both the Peak Signal-to-Noise
Ratio (PSNR) and the multi-scale structural similarity index measure (SSIM)6 (Wang et al. (2003))
to evaluate the comparing methods.
Table 1 summarizes the PSNR and SSIM results on both downscaled images of size 512 × 256
and full-resolution ones. Compared to the state-of-the-art methods, our proposed DACAL achieves
the highest PSNR and SSIM. Note that our DACAL achieves the highest PSNR 7 (24.15 dB) on
the MIT-Adobe FiveK dataset. In particular, we discover the PSNR and SSIM values of some
competing methods like DPE decline by a certain margin when enhancing full-resolution images.
In contrast, our proposed DACAL has relatively stable PSNR and SSIM values in the high-res case.
For a quantitative perceptual quality comparison, we employ 20 Amazon Mechanical Turk (MTurk)
workers to compare 100 image pairs, which are from our DACAL and the competing methods
respectively. As shown in Table 3, the highest preference score shows the superiority of our DACAL.
In addition, we conduct the ablation study on the proposed two-stream U-Net. Specially, we eval-
uate the separative use of additive and multiplicative streams denoted by DACALl1 and DACALl2 ,
respectively. As shown in Table 1, they are outperformed by our two-stream version DACALl . We
also evaluate the performance by cascading these two streams: additive cascaded by multiplicative
5The code will be released once the paper is published.
6For short, we denote the mutli-scale SSIM as SSIM throughout the paper.
7Recently the CVPR19 paper (Ruixing Wang & Jia (2019)) has corrected their model’s PSNR from 30.80
dB to 23.04 dB, on its GitHub webpage.
7
Under review as a conference paper at ICLR 2020
Table 2: PSNR and SSIM results for the DPED (Ignatov et al. (2017)) test 100 × 100 image patches.
Here, l, f, d for DACAL represent the use of our proposed perception-based, frequency-based and
dimension-based division respectively. DACALh is our higher-scale version.
	WESPE	DPE	DACALl	DACALl+f	DACALl+f+d	DACALh
PSNR100	17.45	18.53	19.62	20:01	20.43	20.90
SSIM100	0.854	0.861	0.868	0.869	0.872	0.874
Table 3: Preference ratio (one vote is lost for DACAL vs. Input) from MTurk user study
DACAL vs.	Input	White-Box	Distort-and-Recover	DPED	WESPE	DPE
Adobe	395:4	389:11	303:97	365:35	N/A	370:30
DPED	384:16	N/A	N/A	N/A	384:16	344:56
Huawei-Flickr	366:34	N/A	N/A	N/A	341:59	360:40
stream, or multiplicative cascaded by additive stream. As we find the former case works better
than the later one in terms of PSNR and SSIM. Here, we only report the results of the former one
DACALl3. The improvement of our suggested two-stream version further justifies its effectiveness.
In Figure 4, we also study the visual results of the two components and their resulting maps. The
results reflect that the additive one targets for better color and texture, and the multiplicative one
targets on enhancing illumination. The result of final map shows the benefit of fusing additive and
multiplicative streams.
4.2 Weakly-Supervised Photo Enhancement
This task performs photo enhancement using one set of good-quality images instead of paired be-
fore/after images. For this task, we use the DPED dataset (Ignatov et al. (2017)) and our collected
Huawei-Flickr dataset. The DPED dataset is composed of images from smartphone cameras (i.e.,
iPhone 3GS, BlackBerry Passport and Sony Xperia Z) paired with images of the same scenes cap-
tured by a DSLR camera (i.e., Canon 70D). We use the iPhone-DSLR data (5,614 iPhone images of
size 2048 × 1536, 5,902 DSLR images of size 3648 × 2432) to evaluate the competing methods on
the full-resolution images in a weakly-supervised manner. As the dataset also contains 4,353 well-
aligned 100 × 100 iPhone-DSLR image patch pairs, we use its testing image patches to compute the
PSNR and SSIM values for quantitative evaluation. As for our Huawei-Flickr dataset, we collect
5,138 images of size 1920 × 1080 by Huawei Mate20 pro and use the downloaded 604 HDR Flickr
images with around 1K resolution from (Chen et al. (2018)) for the target.
Table 2 reports the PSNR and SSIM results of the evaluated methods on the DPED dataset. When
compared with the state-of-art weakly-supervised methods WESPE (Ignatov et al. (2018)) and
DPE (Chen et al. (2018)), our DACAL reaches the highest PSNR (20.90 dB) and SSIM. The table
also shows the improvement of the introduced frequency based GAN and sliced SWGAN loss. For
a quantitative perceptual quality comparison on our DACAL, WESPE and DPE, we ask 20 MTurk
works for user study on 120 image pairs for DPED and Huawei-Flickr. The results in Table 3 show
that our method achieves much higher preference score than all competing methods in terms of
the sampled human perception. The qualitative results from Figure 5 reflect that our DACAL has
the best performance in terms of color rendering, texture sharpening, and illumination adjustment
for the DPED iPhone images. In Figure 6, we present enhancement results for our Huawei-Flickr
dataset. We can find that WESPE tends to make the image over-exposed, while DPE has serious
texture issues due to its design for low-resolution image enhancement. By comparison, our DACAL
performs most promisingly for color, texture, and light condition improvements.
5 Conclusion
This paper addresses mix-perception enhancement of images with a divide-and-conquer inspired
adversarial learning approach, which divides and merges perception-based, frequency-based and
dimension-based sub-problems. For high-resolution image enhancement, we suggest to progres-
sively grow the proposed multiscaled model along various resolutions. Evaluations on both su-
8
Under review as a conference paper at ICLR 2020
(a) Input	(b) WESPE (Ignatov et al. (2018))
(c) DPE (Chen et al. (2018))	(d) Proposed DACAL
Figure 5: High-resolution image enhancement results for the DPED iPhone-DSLR data. It is best to
zoom in on the high-resolution pictures.
(a) Input
(b) WESPE (Ignatov et al. (2018))
(c) DPE (Chen et al. (2018))
(d) Proposed DACAL
Figure 6: High-resolution image enhancement results for our Huawei-Flickr data. It is best to zoom
in on the high-resolution pictures.
pervised and weakly-supervised image enhancement tasks demonstrate the clear superiority of our
proposed enhancer against the competing methods.
References
Gilles Aubert and Jean-Francois Aujol. A variational approach to removing multiplicative noise.
SIAM journal on applied mathematics, 68(4):925-946, 2008.
9
Under review as a conference paper at ICLR 2020
Jose M BioUcas-Dias and Mario AT Figueiredo. Multiplicative noise removal using variable splitting
and constrained optimization. IEEE Transactions on Image Processing, 19(7):1720-1730, 2010.
Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fredo Durand. Learning photographic global
tonal adjustment with a database of input/output image pairs. In Proceedings of the IEEE confer-
ence on computer vision and pattern recognition, pp. 97-104. IEEE, 2011.
Yu-Sheng Chen, Yu-Ching Wang, Man-Hsin Kao, and Yung-Yu Chuang. Deep photo enhancer:
Unpaired learning for image enhancement from photographs with gans. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 6306-6314, 2018.
Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using a
laplacian pyramid of adversarial networks. In Advances in neural information processing systems,
pp. 1486-1494, 2015.
Ishan Deshpande, Ziyu Zhang, and Alexander Schwing. Generative modeling using the sliced
Wasserstein distance. In Proceedings of the IEEE conference on computer vision and pattern
recognition, 2018.
Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional
network for image super-resolution. In European conference on computer vision, pp. 184-199.
Springer, 2014.
Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based
on deep networks. In Advances in Neural Information Processing Systems, pp. 658-666, 2016.
Jakob Engel, Vladyslav Usenko, and Daniel Cremers. A photometrically calibrated benchmark for
monocular visual odometry. arXiv preprint arXiv:1607.02555, 2016.
Xueyang Fu, Delu Zeng, Yue Huang, Xiao-Ping Zhang, and Xinghao Ding. A weighted variational
model for simultaneous reflectance and illumination estimation. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 2782-2790, 2016.
Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional
neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition, pp. 2414-2423, 2016.
Michael Gharbi, Jiawen Chen, Jonathan T Barron, Samuel W Hasinoff, and Fredo Durand. Deep
bilateral learning for real-time image enhancement. ACM Transactions on Graphics (TOG), 36
(4):118, 2017.
Dibya Ghosh, Avi Singh, Aravind Rajeswaran, Vikash Kumar, and Sergey Levine. Divide-and-
conquer reinforcement learning. arXiv preprint arXiv:1711.09874, 2017.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp.
5767-5777, 2017.
Xiaojie Guo, Yu Li, and Haibin Ling. Lime: Low-light image enhancement via illumination map
estimation. IEEE Trans. Image Processing, 26(2):982-993, 2017.
Yuanming Hu, Hao He, Chenxi Xu, Baoyuan Wang, and Stephen Lin. Exposure: A white-box photo
post-processing framework. ACM Transactions on Graphics (TOG), 37(2):26, 2018.
Jie Huang, Pengfei Zhu, Mingrui Geng, Jiewen Ran, Xingguang Zhou, Chen Xing, Pengfei Wan, and
Xiangyang Ji. Range scaling global u-net for perceptual image enhancement on mobile devices.
In European Conference on Computer Vision, pp. 230-242. Springer, 2018.
Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, and Serge Belongie. Stacked generative
adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5077-5086, 2017.
Yu-Mei Huang, Michael K Ng, and You-Wei Wen. A new total variation method for multiplicative
noise removal. SIAM Journal on imaging sciences, 2(1):20-40, 2009.
10
Under review as a conference paper at ICLR 2020
Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, and Luc Van Gool. Dslr-
quality photos on mobile devices with deep convolutional networks. In Proceedings of the IEEE
International Conference on Computer Vision,pp. 3277-3285, 2017.
Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, and Luc Van Gool. Wespe:
weakly supervised photo enhancer for digital cameras. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition Workshops, pp. 691-700, 2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Ruoyu Sun Ayis Pyrros Nasir Siddiqui Sanmi Koyejo Zhizhen Zhao David Forsyth Alexan-
der Schwing Ishan Deshpande, Yuan-Ting Hu. Max-sliced wasserstein distance and its use for
gans. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2019.
Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and
super-resolution. In European conference on computer vision, pp. 694-711. Springer, 2016.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-
proved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.
Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 1646-1654, 2016.
Soo Ye Kim, Jihyong Oh, and Munchurl Kim. Jsi-gan: Gan-based joint super-resolution and
inverse tone-mapping with pixel-wise task-specific filters for uhd hdr video. arXiv preprint
arXiv:1909.04391, 2019.
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
neural networks. In Advances in neural information processing systems, pp. 971-980, 2017.
Chieh Hubert Lin, Chia-Che Chang, Yu-Sheng Chen, Da-Cheng Juan, Wei Wei, and Hwann-
Tzong Chen. Coco-gan: Generation by parts via conditional coordinating. arXiv preprint
arXiv:1904.00284, 2019.
Jie Liu and Cheolkon Jung. Multiple connected residual network for image enhancement on smart-
phones. In European Conference on Computer Vision, pp. 182-196. Springer, 2018.
Xiaojiao Mao, Chunhua Shen, and Yu-Bin Yang. Image restoration using very deep convolutional
encoder-decoder networks with symmetric skip connections. In Advances in neural information
processing systems, pp. 2802-2810, 2016.
Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network
for dynamic scene deblurring. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 3883-3891, 2017.
Alex Nowak, David Folque, and Joan Bruna. Divide and conquer networks. In ICLR, 2018.
Jongchan Park, Joon-Young Lee, Donggeun Yoo, and In So Kweon. Distort-and-recover: Color
enhancement using deep reinforcement learning. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5928-5936, 2018.
Steve G Romaniuk and Lawrence O Hall. Divide and conquer neural networks. Neural Networks, 6
(8):1105-1116, 1993.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
cal image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, 2015.
Chi-Wing Fu Xiaoyong Shen Wei-Shi Zheng Ruixing Wang, Qing Zhang and Jiaya Jia. under-
exposed photo enhancement using deep illumination estimation. In Proceedings of the IEEE
conference on computer vision and pattern recognition, 2019.
11
Under review as a conference paper at ICLR 2020
Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel
Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient
sub-pixel convolutional neural network. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 1874-1883, 2016.
Radu Timofte, Shuhang Gu, Jiqing Wu, and Luc Van Gool. Ntire 2018 challenge on single image
super-resolution: methods and results. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition Workshops, pp. 852-863, 2018.
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-
resolution image synthesis and semantic manipulation with conditional gans. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8798-8807, 2018a.
Yifan Wang, Federico Perazzi, Brian McWilliams, Alexander Sorkine-Hornung, Olga Sorkine-
Hornung, and Christopher Schroers. A fully progressive approach to single-image super-
resolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
Workshops, pp. 864-873, 2018b.
Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality
assessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003,
volume 2, pp. 1398-1402. Ieee, 2003.
Jiqing Wu, Zhiwu Huang, Dinesh Acharya, Wen Li, Janine Thoma, Danda Pani Paudel, and Luc
Van Gool. Sliced wasserstein generative models. In Proceedings of the IEEE conference on
computer vision and pattern recognition, 2019.
Shangzhe Wu, Jiarui Xu, Yu-Wing Tai, and Chi-Keung Tang. Deep high dynamic range imaging
with large foreground motions. In Proceedings of the European Conference on Computer Vision
(ECCV), pp. 117-132, 2018.
Xin Yang, Ke Xu, Yibing Song, Qiang Zhang, Xiaopeng Wei, and Rynson WH Lau. Image cor-
rection via deep reciprocating hdr transformation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 1798-1807, 2018.
Runsheng Yu, Wenyu Liu, Yasen Zhang, Zhi Qu, Deli Zhao, and Bo Zhang. Deepexposure: Learning
to expose photos with asynchronously reinforced adversarial learning. In Advances in Neural
Information Processing Systems, pp. 2149-2159, 2018.
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dim-
itris N Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adver-
sarial networks. In Proceedings of the IEEE International Conference on Computer Vision, pp.
5907-5915, 2017a.
Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser:
Residual learning of deep cnn for image denoising. IEEE Transactions on Image Processing, 26
(7):3142-3155, 2017b.
Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European confer-
ence on computer vision, pp. 649-666. Springer, 2016.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE International Conference
on Computer Vision, pp. 2223-2232, 2017.
A Appendix
A.1 Network Architecture
In the paper, we use two-scale design for both enhancer and discriminator. Figure 7 illustrates the
network architecture of our proposed method. Specially, Figure 7 (a) shows the network design
of our multiscaled enhancer, consisting of 5 components: netG0, netG1, netG2, netG3 and netG4.
12
Under review as a conference paper at ICLR 2020
===== net
input]
conv]
se^Lu ]
bn ]
conv]
se^Lu ]
bn]
===== net
input]
conv]
se^Lu ]
bn]
conv]
se^Lu ]
bn]
conv]
se^Lu ]
bn]
conv]
se^Lu ]
bn]
===== net
input]
conv]
se^Lu ]
bn]
conv]
se^Lu ]
bn]
conv]
se^Lu ]
conv]
===== net
input]
conv]
g_concat]
∈O123456 Θ6789O12345678 ∈89O1234567 6889
m m 111111111 mll22222222 ml22
a a a a
conv][ 30]
se^Lu] [ 31]
bn][ 32]
conv][ 33]
resize][ 34]
concat][ 35]
se^Lu][
bn][
conv][
resize][
concat][
se^Lu][
bn][
conv][
resize][
concat][
6刀≡9θ
3 3 3 3 4
12刃45
4 4 4 4 4
0 ‘‘‘‘‘‘‘1
-3333333 -
G G
t t
e e
-<<<<<<<<<<<<<2
-3 333333333333 _
- G
- t
- e
-<<<<<<<<<<3 ,,,
-3333333333 -3 3 3
- G
- t
- e
3 3 3 3 3 3
3 3 3 3 3
3 3 3 3 3
[	se^Lu] [ 46]:(
[	bn][	47]:(
[	conv][	48]:(
[	se^Lu] [ 49]:(
[	bn][	50]:(
[conv_color_illu][ 51]
3 3 3 3 3
2 2 2 2 6 6 6
11115 5 5
5 5 5 5 2 2 2
4 4 4 4 2 2 2
2 2 2 2 111
0 0 0 0 5 5 5
3)6)6)6)3)3)3)
6666888444222
5555222666333
2 2 2 2 111
2222666888444
1111555222666
5555222111
3)6)6)6)2)2)2)4)4)4)8)8)8)
111333666222
2666888111
3 111
4222666999
6 3 3 3 111
8)8)8)8)8)8)8)8)8)8)
2222222222
2 2 2
3 3 3
4 4 4
6 6 6
8 8 6
2 2 5
112
2 2 2 2 4 4
3 3 3 3 6 6
4 4 4 8 8
6 6 6 2 2
8 8 8 6 6
2 2 2 5 5
1112 2
6 6 6 6
5 5 5 5
2 2 2 2
4 4 4 4 8 8
6 6 6 6 2 2
8)8)8)8)8)2)
2 2 2 2 2 9
8 8 8 6 6
2 2 2 5 5
1112 2
6 6 6 2 2
5 5 5 11
2 2 2 5 5
5 5 5 5
2 2 2 2
5 ,
6
5
< 2
6
5
2 <
3
-5
2
2
2)2)4)4)6)
9 9 6 6 9
6)6)2)2)8)
9 9 3 3 4
8)8)6)6)6)^—•
4 4 11 13
[ input][
[ conv][
[ IrelU][
[ in][
========== net_
[ input]!
[ conv][
[ IrelU][
[ in][
[ conv][
[ IrelU][
[ in][
[ conv][
[ IrelU][
[ in][
[ conv][
[ IrelU][
[ in][
[ conv][
[ IrelU][
[ in][
[ conv][
[ IrelU][
[ in][
[ conv][
========== net_
[ input]!
[ d_swd][
[ d_swd][
[ d_swd][
[ concat_swd][
[ concat_swd][
51
eθ12刃4
≡
eθl1121刃 e 刃456%≡9θ12刃456刀≡9θ12
m m Tx Tx Tx Tx Tx Tx Tx Tx Tx Tx f∖∣ f∖∣ f∖∣
a a
66668884442226666661
5555222666333111111
2 2 2 2 111
51
17)7)7)4)
Tx Tx Tx
3)3)3)3)
========== net name =		netG 4 ==========			
[	input][	51]	(3,	256,	512,	3)
[	conv][	52]	(3,	256,	512,	3)
[	resize][	53]	(3,	512,	1024,	3)
[	concat][	54]	(3,	512,	1024,	19)
[	selu][	55]:
[	bn][	56]:
[	conv][	57]:
[	se^Lu] [ 58]:
[	bn][	59]:
[conv_color_illu][ 60]
3 3 3 3 3
24
................O
4 4 4 4 4 1
2 2 2 2 2
rɔ rɔ rɔ rɔ rɔ ，
1 1 1 1 1 2
1
..............5
2 2 2 2 2
1 1 1 1 1
5 5 5 5 5 ，
3
9)9)6)6)6)。
111113
(a) Multi-scaled Enhancer
22226668884442222227
11115552226663333331
5555222111
(b) Multi-scaled Discriminator
3)6)6)6)2)2)2)4)4)4)8)8)8)8)8)8)8)8)8)D
111333666222222222
Figure 7: Illustration of the used multi-scaled enhancer (a) and multi-scaled discriminator (b) for
our proposed method
13
Under review as a conference paper at ICLR 2020
Among them, netG1 , netG2, netG3 form the lower-scale enhancer. Therefore, the higher-scale en-
hancer inherits netG1 , netG2, netG3 from lower-scale enhancer, and introduces two new compo-
nents that are netG0 and netG4. Note that the layer named ‘conv-color-illu’ realizes the two-stream
branches, which contains separate groups of convolutions operations, and finally aggregates them
using the Eqn.1 in the major paper. Figure 7 (b) shows the structure of our multiscaled discriminator
that contains netD0, netD1 and netS1. The component netS1 is designed for the computation of
sliced Wasserstein distance. Analogously, netD1 and netS1 are from lower-scale discriminator. The
higher-scale discriminators add one more component netD0 .
A.2 Training Details
For the supervised image enhancement task, we train the proposed two-scaled enhancer on one
single NVIDIA TITAN Xp GPU with 12GB GPU memory. We first train the lower-scaled enhancer
over down-scaled (512 × 256) images for 20 epochs, and then train higher-scaled enhancer through
tuning the new parameters of netG0 and netG4 while fixing the parameters of netG1 , netG2 , netG3
over higher-resolution (1024 × 512) images for 20 epochs. Finally, we train the whole multi-scale
enhancer for 20 epochs. For the weakly supervised image enhancement task, we train the two-
scaled enhancer on two NVIDIA TITAN Xp GPUs, each of which has 12GB GPU memory. Similar
to the supervised case, we use the same style to train the multi-scaled enhancer. For the training, we
empirically set the hyperparameters λ = 10, η = 0.99, τ = 0.05, γ1 = 10000, γ2 = 1000 from the
proposed GAN model. Note that it is still feasible for us to train our enhancer on higher-resolution
images if we have more computational resources.
14