Under review as a conference paper at ICLR 2020
Farkas layers: don’t shift the data, fix the ge-
OMETRY
Anonymous authors
Paper under double-blind review
Ab stract
Successfully training deep neural networks often requires either batch normaliza-
tion, appropriate weight initialization, both of which come with their own chal-
lenges. We propose an alternative, geometrically motivated method for training.
Using elementary results from linear programming, we introduce Farkas layers:
a method that ensures at least one neuron is active at a given layer. Focusing on
residual networks with ReLU activation, we empirically demonstrate a significant
improvement in training capacity in the absence of batch normalization or meth-
ods of initialization across a broad range of network sizes on benchmark datasets.
1	Introduction
The training process of deep neural networks has gone through significant shifts in recent years,
primarily due to revolutionary network architectures, such as Residual neural networks (He et al.,
2016) and normalization techniques, initially proposed as batch normalization (Ioffe & Szegedy,
2015). The former is the backbone for current “state-of-the-art” results for image-based tasks such
as classification. Normalization can be found in a variety of flavors and applications; (Ba et al.,
2016; Ulyanov et al., 2016; Wu & He, 2018; Vaswani et al., 2017; Zhu et al., 2017). For reasons that
are not fully understood, normalization gives rise to many desirable traits in network training, e.g. a
fast convergence rate, but also comes with a cost by increasing vulnerability to adversarial attacks
(Galloway et al., 2019). Apart from normalization, network weight initialization has been a driving
force of improved performance. Throughout the years, various initialization schemes have been
proposed, in particular Xavier initialization (Glorot & Bengio, 2010), FixUp (Zhang et al., 2019),
and a recent asymmetric initialization (Lu et al., 2019). These approaches are often connected to
probabilistic notions, or balancing learning rates while training.
Modern network architectures catered for image-classification tasks incorporate various one-
sided activation functions, the canonical example being the Rectified Linear Unit (ReLU) (Nair &
Hinton, 2010). Other one-sided activations include the Exponential Linear Unit (ELU) (Clevert
et al., 2015), Scaled ELU (SELU) (Klambauer et al., 2017), and LeakyReLU (Maas et al.). ELU
and SELU were constructed with the purpose of eliminating batch normalization (BN); these
activation functions minimize the internal covariate shift (ICS), which is what BN was intended
to accomplish. Recent developments have shown that BN has little impact on ICS, but affects the
smoothness of the loss landscape, allowing for easier (and faster) training regimes (Santurkar et al.,
2018).
A geometric interpretation of batch normalization can be readily seen in the context of the
dying ReLU problem. This phenomenon occurs when the gradient of the neuron is zero (when the
neuron outputs only negative values), in which case the neuron is “dead”. This effectively freezes
the neuron during training. If too many neurons are dead, the network learns slowly. In fact, it
is entirely possible for a network to be “born dead”, where it does not allow learning at all (Lu
et al., 2019). To motivate this, consider a simple binary classification problem: suppose two sets of
points in R2 are sufficiently separated and we wish to classify them using a simple 1-layer ReLU
network (with standard Cross Entropy loss). If the network is initialized such that one cloud of
points is not “observed”, as in Figure 1a, the network will not learn to classify those points. Batch
normalization will shift the points to behave like 1b; the network is no longer dead. There is a far
simpler geometric solution in R2: given one hyperplane, construct another to face the missing data,
1
Under review as a conference paper at ICLR 2020
as shown in Figure 1c by the green hyperplane, which is now a non-dead component of the network.
Our contributions
We present a geometrically motivated method for network training in the absence of initialization
and normalization. Our novel layer structure, called Farkas layers, ensures that at least one neuron
is active in every layer. Using off-the-shelf networks, Farkas layers can recover a significant part
of the explanatory power of DNNs when batch normalization is removed, sometimes up to 10%,
as demonstrated by empirical results on benchmark image classification tasks, such as CIFAR10
and CIFAR100. When used in conjunction with batch normalization on ImageNet-1k networks,
we empirically show an approximate 20% improvement on the first epoch over only using batch
normalization. Finally, we claim that input data normalization is a critical component for using
large learning rates in FixUp, which is not the case for Farkas layer-based networks. All in all,
this work provides a new research direction for architecture design beyond initialization methods,
normalization layers, or new activation functions.
2	Background
2.1	Notation
We define neural networks as iterative compositions of activation functions with linear functions.
More specifically, given an input x(k) ∈ Rn to the k-th layer, the output x(k+1) is typically written
x(k+1) =夕(W (k) x(k) + b(k))
where W(k) ∈ Rm×n is a weight matrix, the rows are written as wiT (i = 1, . . . , m), b(k) ∈ Rm is
the layer's bias, and φ is the activation function of the layer. For the remainder of our analysis, We
consider the ReLU activation function, written
φ(x) = max{x, 0},
Which acts component-Wise in the case of vector arguments.
2.2 Related work
Previous Work on the dying ReLU problem, or vanishing gradient problem, in the case of using
sigmoid-like activation functions, has heavily revolved around novel Weight initializations. To ad-
dress the vanishing gradient problem, (Glorot & Bengio, 2010) proposed an initialization that is still
often used With ReLU activations (Shang et al., 2017). (He et al., 2015) proposed a scaled version
of a normal distribution, Which improved training for pure convolutional neural netWorks. More
recently, (Lu et al., 2019) studied the dying ReLU problem from a probabilistic perspective and
addressed netWorks that are “born dead”, i.e. not capable of learning. Their analysis is based on
the reasonable assumption that all Weights and biases are initialized With a non-zero probability of
being dead:
P (WTX + bi < 0)≥ p > 0 (i = 1,..., m),
2
Under review as a conference paper at ICLR 2020
where m is the number of neurons in a given layer, and p is a fixed positive constant. They show
that, as the number of layers approaches infinity, the probability of having a network be born dead
approaches one. They also find that initializing weights with a symmetric distribution is more
likely to cause a network to be born dead and thus propose initializing weights using asymmetric
distributions to mitigate this problem. Their analysis is relevant in the case that the network is not
very wide, which is distinct from the case we study in this paper.
Normalization has been one of the driving forces of recent state-of-the-art results; where the
general idea is to manipulate the neuron activation with various statistics, such as subtracting the
mean and dividing by the variance. Popular normalization techniques include batch normalization
(Ioffe & Szegedy, 2015), Layer normalization Ba et al. (2016), Instance normalization (Ulyanov
et al., 2016) and Group normalization (Wu & He, 2018). However, there is a desire to eliminate
normalization from training. A recent work that does this is FixUp (Zhang et al., 2019), a novel
initialization technique that works by scaling the weights as a function of the network structure,
allowing the network to take large learning rate steps during training. Reportedly, FixUp provides
the same level of accuracy as that of a BN-enhanced network, and scales to networks that train on
ImageNet-1k. As stated in FixUp, the inherent structure of residual networks already prevents some
level of gradient vanishing, since the variance of the layer output grows with depth. In the case
of positively-homogeneous blocks (e.g. no bias in a Linear or Convolutional layer), this type of
variance increase can lead to gradient explosion (Hanin & Rolnick, 2018), which makes training
more difficult.
3 Farkas layers
3.1	Augmenting ReLU activated layers
We present a novel approach to understanding how weight matrices and bias vectors contribute to
neural network learning, called Farkas layers. As the name suggests, our method is influenced by
Farkas’ lemma. In particular, we use the following representation of Farkas’ lemma; the proof is an
exercise in linear programming and is left in the appendix.
Lemma 3.1 (Representation of Farkas’ lemma). Let W ∈ Rm×n , b ∈ Rm with x ∈ Rn. Then
the set {x | Wx + b ≤ 0} is empty if and only if there exists a λ ∈ Rm such that λi ≥ 0 (for
i = 1, . . . , m), with WTλ = 0 and λTb > 0.
We present the construction ofW and b, and prove using Lagrangian duality (Boyd & Vandenberghe,
2004) that such weights and bias vectors will satisfy Farkas’ lemma, resulting in at least one positive
component. In essence, this solves the dying ReLU problem while training.
Theorem 3.2. Let L be a layer of a neural network with ReLU activation function, with weights
W ∈ Rm×n (with rows wiT) and bias vector b ∈ Rm, and let λ ∈ Rm, with λi ≥ 0 (with at least
one λi > 0, without loss of generality λm > 0). Further, suppose W has the property that
-1 m-1
Wm = T ): λj Wj
λm
mj=1
-1	m-1
and that bm, > -- Ej=J λj bj. Then this layer has a non-zero gradient.
Proof. Let W ∈ Rm×n (with rows WiT) and bias b ∈ Rm, with the aforementioned assumptions,
and arbitrary input x. The output of this layer is 夕(Wx + b), and has a non-zero gradient if and only
if there exists at least one component such that
p* := max WTx + bi > 0.
i
(1)
The left-hand side of (1) (i.e. omitting the strict inequality constraint) can be written as the following
minimization problem:
p* := min S subject to WT x + bi ≤ S (i = 1,...,m),
x,s
3
Under review as a conference paper at ICLR 2020
or equivalently,
p* = min S subject to Wx + b ≤ s1,
x,s
where 1 is the vector of all ones. The Lagrangian for this problem is
L(x, s, λ) = s + λT (Wx + b - s1)
where λ ≥ 0. We compute the dual problem,
d(λ) =infL(x,s,λ)
x,s
= inf s(1 -λT1)+λTb+ (WTλ)Tx
x,s
λTb ifλT1 = 1,WTλ = 0, (λ ≥ 0)
-∞ otherwise.
The conditions on W are met by assumption, and there are many ways to ensure that λ is on the
simplex, and thus has at least one strictly positive component. Both the primal and dual problems
are linear optimization problems: the objective and constraints are linear. By weak duality, p* ≥
sup、d(λ). Thus to ensure thatp* > 0 (as in (1)), it suffices to show that λτb > 0. This is guaranteed
by the assumptions on the bias vectors, and so We are done.	□
It is highly unlikely that a given layer in a deep neural network has all dead neurons. However,
we believe that guaranteeing the explicit activity of one neuron is beneficial in that it improves
training. Figure 1 motivates the use of Farkas layers as a potential substitute for (BN) in the context
of learning: the arrows of the corresponding hyperplane indicate the “non-zero” part of the ReLU.
In the case of 1a, we see that all data points end up on the “zero” side, which would typically lead
to no learning. Heuristically, the effect of BN mimics Figure 1b, where the data points are centered
and scaled by a diagonal matrix (which geometrically acts like an ellipse) allowing for learning to
occur. Farkas layers will instead append another hyperplane, that will be guaranteed to see all the
points (in a “third” dimension).
3.2 Extension to general one-sided activations
The use of Farkas layers can be extended to other one-sided activation functions, such as a smooth-
ReLU (i.e. replacing the kink with a polynomial), ELU, and several others. In all these cases, one
can impose a scalar cutoff value c such that “beyond” this cutoff, we have very small (or zero)
gradients. This is shown in the following corollary, which has the same proof as Theorem 3.2.
Corollary 3.3. Let L be a layer of a neural network with an arbitrary one-sided activation function
ψ with cutoff c ∈ R, i.e. for all x < c, we have ψ(x) ' 0 (or exactly zero). Denote the weights by
W ∈ Rm×n (with rows wi) and bias vector b ∈ Rm, and let λ ∈ Rm, with λi ≥ 0, with at least
one positive component. To ensure that ψ(Wx + b) has at least one component greater than c, we
require that
-1 m-1
Wm =「jj λj Wj
λm
mj=1
and that bm > c —六 Pm-LI λjj
3.3 Algorithm and limitations
We denote a Farkas layer by F := F(∙; W, b) : Din → Dout, where the weights and biases
satisfy Theorem 3.2. The construction of W and b imposes certain restrictions on the existence
of Farkas layers, namely we require that dim(Dout) ≥ 2. Finally, some network architectures
claim to perform better without bias vectors present; we require bias vectors by construction.
We focus on replacing Convolutional and Linear layers in (deep) neural networks, with efficient
implementations in PyTorch. The general method of incorporating Farkas layers is provided in
Algorithm 1. To minimize computational cost, we consider λ to be the evenly spaced simplex
element i.e. λi = dim(Dout)-1 (for i = 1, . . . , dim(Dout)), but for smaller problems (such as the
4
Under review as a conference paper at ICLR 2020
motivating 1-layer example) λ can be learnable. Farkas layers can be made into Farkas blocks in
conjunction with residual networks as well, outlined in Algorithm 2, which we leave in the Ap-
pendix. In the case of multiple convolutional layers in a given residual block, the extension is natural.
In both Algorithm 1 and 2, we consider two choices for the aggregating function (AggFunc):
either the sum or the mean of the previous outputs and biases. Indeed, let dim(Dout) = m, and thus
λj = m-1. Then we have the following two aggregation methods:
m-1
wm = -	wj (sum),
j=1
wm
-1
m-1
m-1
X wj
j=1
(mean),
and similarly for the bias vectors. Theorem 3.2 is constructed using “sum”, but the result holds in
the case of using “mean” since ReLUs are 1-positive homogenous. Furthermore, by using “mean”,
We induce stability with respect to the '∞-induced matrix norm for a matrix A ∈ Rm×n, defined as
kAk∞ = max kajT k1,
1≤j≤m
where aj are the rows of A. Let A ∈ R(m-1)×n and aτ be the mean of the rows of A. Define
A := (A; aτ) ∈ Rm×n, then
)m— 1
kAk∞,(m - 1)-1 X kαjkι≤kAk∞.
Thus, in the context of network weights, the burden of stability lies on the trainable weights and not
the aggregated component. A similar analysis can be done for the bias vectors. Thus, for networks
that train on ImageNet-1k, or to potentially use larger learning rates, we believe that using “mean”
as the aggregation function is necessary.
Algorithm 1 FarkasLayer with ReLU activation; F(∙; W,b) with AggFUnc = "sum" or “mean”
Initialize W ∈ R(m-1)×n and b ∈ Rm
Forward pass: x
y = Wx	. e.g. Linear and/or Conv with no bias
Compute the Aggregated Components:
y := -AggFunc(y)
b := max{-AggFunc(b[0 : -1]), bm}
y J concatenate(y, y)
b J concatenate (b[0 : -1], b)
yJy+b
y J ReLU(y)
Optional: apply batch normalization to y
Return y
4	Experiments — Image clas sification
Image classification is a standard benchmark for measuring new advancements in deep learning.
Our goal is to show how training with Farkas layers, henceforth referred to as FLs, impacts learning
in these settings relative to other training techniques such as normalization and initialization (e.g.
FixUp).
We consider CIFAR10, CIFAR100, and ImageNet-1k datasets for the purposes of testing FL-
based residual networks on image classification tasks. For CIFAR10, we consider 18, 34, 50,
and 101 layer residual networks. The latter two of these use a BottleNeck block structure. On
CIFAR100, we only consider 34, 50, and 101 layers. All models are trained using the same SGD
schedule for 200 epochs. We make the distinction between a “large” learning rate, meaning 0.1,
and a “small” learning rate, which is 0.01. We use standard data augmentation (RandomCrop and
RandomHorizontalFlip), but do not normalize the inputs with respect to the mean and standard
5
Under review as a conference paper at ICLR 2020
deviation of the dataset in the case of CIFAR10/CIFAR100. The weights are left at the default
PyTorch initialization. Additionally, we use cutout to improve generalization (DeVries & Taylor,
2017), and use weight-decay. For ImageNet-1k, we use modified code from the DAWNBench
competition (Coleman et al., 2018; Shaw et al.). In this set of experiments, we augment the
data via RandomCrop, RandomHorizontalFlip, and normalize the input data. We only consider
the case of networks using BN and see how FLs can improve performance, and train with 30 epochs.
Henceforth, we call a residual network a FarkasNet if is it comprised of only FLs, and we
always use the “sum” aggregation function unless otherwise specified. We make the distinction of
networks that use or do not use BN in the tables. When omitting BN, we use the small learning
rate. For CIFAR10 and CIFAR100, the results presented are averaged across three runs; if one of
the runs failed1 to train, we omit it from the average but place an asterisk. Full training curves are
in the Appendix.
Interpreting dependence of batch normalization
Table 1: Percent misclassification comparison for CIFAR10 (lower is better)
	18 Layers	34 Layers	50 Layers	101 Layers
ResNet (with BN, large LR)	7.01	6.79	5.04	4.95
FarkasNet (with BN, large LR)	6.87	6.50	5.03	4.98
ResNet (no BN, small LR)	12.51	9.90	24.53	25.25
FarkasNet (no BN, small LR)	10.83	9.45	19.84	15.38
Table 2: Percent misclassification comparison for CIFAR100 (lower is better)
	34 Layers	50 Layers	101 Layers
ResNet (with BN)	30.95	23.43	22.90
FarkasNet (with BN)	30.34	23.88	23.22
ResNet (no BN)	40.10	55.05*	54.02
FarkasNet (no BN)	34.70	43.93	42.94
Tables 1 and 2 both show a strong disparity in the learning capacity of a DNN when batch nor-
malization is removed, which is unsurprising. On CIFAR10, we primarily observe similar, if not
better, test errors when batch normalization is used. Without normalization, our test error is always
better; the same can be said for the CIFAR100 dataset. Thus, while FarkasResNets also diminish
in quality, we note that adding a guaranteed undead neuron to all layers heavily impacts learning in
the case of a non-normalized network. This is not at all to say that training a deep neural network is
only possible using FLs, or that we achieve state-of-the-art performance; however, our implemen-
tation demonstrates the strong dependency of getting low test accuracy with normalization, and is
geometrically motivated.
Improvement at first epoch for ImageNet- 1 k
The final table shows similar behaviour on ImageNet-1k, where FLs2 neither dramatically improve
nor inhibit the learning capacity towards the end of training except in the case of 50-layers, where
it performs significantly better. Figure 2 shows that FarkasNets have a dramatic advantage over
standard ResNets at the start of training. Evidently, this advantage gradually diminishes; nonethe-
less, its presence alludes to a bigger problem in network initialization. We notice a roughly 20%
improvement on the first epoch, simply due to the use of FLs.
1Failed implies that the network did not make any progress in the first five epochs.
2For ImageNet-1k, we use “mean” aggregation function.
6
Under review as a conference paper at ICLR 2020
Table 3: Top1/Top5 percent misclassification comparison for ImageNet-1k (lower is better)
	50 Layers	101 Layers	152 Layers
ResNet (with BN)	25.64/7.68	23.23/6.49	22.26/5.95
FarkasNet (with BN)	23.70/6.65	23.74/6.67	22.17/5.86
Figure 2: Illustration of improved performance at starting epochs with Farkas layers
Impact of data normalization and best practice comparison
We briefly compare against other best practice learning regimes on CIFAR10. Table 4 highlights
the various differences between methods. At the core, we compare FarkasNets to FixUp, a recently
proposed initialization scheme that allows the use of larger learning rates in the absence of batch
normalization, and thus faster convergence. We consider a 34-layer ResNet with BasicBlock
structure using the official implementation of FixUp3 for the network architecture. We compare
using a 34-layer FarkasNet, where the last layer in a block is initialized to zero (as in FixUp, which
is motivated by several other works (Srivastava et al., 2015; Hardt & Ma, 2016; Goyal et al., 2017))
but use standard initialization otherwise, as well as “mean” AggregationFunction. We maintain the
setup of the previous experiments.
Data normalization is a standard “trick” for training deep neural networks, and acts like an
initial batch normalization step, where the incoming data is scaled to have zero mean and unit
variance. In the case of CIFAR10 and CIFAR100, we strove to study the problem of training without
any attempts at normalization, which is why we have omitted this from our training methodology.
In the absence of data normalization, we observed that FixUp requires a small learning rate for
training. Thus, while (Zhang et al., 2019) claims to present a theory for addressing training in the
absence of batch normalization, we find that it still heavily hinges on normalization principles,
namely on the first layer pass. This observation is not to diminish the benefits of FixUp, as faster
convergence is observed (see Figure 3), but the learning rate does need adjustment. On the other
hand, our 34-layer FarkasNet was able to use a medium learning rate of 0.05, allowing for improved
performance relative to Table 1.
3From one of the authors’ Github pages
7
Under review as a conference paper at ICLR 2020
Table 4: Best practice comparison on CIFAR10 (lower test error is better)
	Batch Normalization	LR capacity	Test Error
ResNet34	✓	Large	6.77
ResNet34	X	Small	9.18
FixUp34	X	Small	7.49
FarkasNet34	X	Medium	7.12
Figure 3: Training curves for best practice comparison on CIFAR10
Finally, we also address the notion that training a very deep neural network is challenging using
maximal learning rate without normalization, which has been experimentally observed in FixUp
and Layer-Sequential Unit-Variance (LSUV) orthogonal initialization (Mishkin & Matas, 2015).
Despite the shortcomings of previous work, by incorporating the “mean” aggegration function and
default initialization, we are able to successfully train a 101-layer FarkasNet with large learning
rate on CIFAR10. We present the averaged training curve in Figure 5, left in the Appendix, where
we repeated the experiment three times. We remark that using default initialization alone results in
complete failure to train.
5	Conclusion
To date, successful training of a deep neural networks requires either some form of normalization,
weight initialization, and/or choice of activation function, all of which come with their own chal-
lenges. In this work, we provide a fourth option, Farkas layers, which can be used alone or in
conjunction with the aforementioned techniques. The Farkas layer is based on the interaction of the
geometry of the weights with the data: by simply adding one linearly dependent row to the weight
matrix, we ensure that no neurons are dead. Using our method, we have shown that training with
larger learning rates is possible even in the absence of batch normalization, and with default initial-
ization. In this work, we only touched on image classification, but Farkas layers could be used in
many deep learning applications, which is left for future work.
References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
8
Under review as a conference paper at ICLR 2020
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Djork-Ame CleVerL Thomas Unterthiner, and SePP Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
Cody Coleman, Daniel Kang, DeePak Narayanan, Luigi Nardi, Tian Zhao, Jian Zhang, Peter
Bailis, Kunle Olukotun, Christopher Re, and Matei Zaharia. Analysis of dawnbench, a time-
to-accuracy machine learning Performance benchmark. CoRR, abs/1806.01427, 2018. URL
http://arxiv.org/abs/1806.01427.
Terrance DeVries and Graham W Taylor. ImproVed regularization of conVolutional neural networks
with cutout. arXiv preprint arXiv:1708.04552, 2017.
Angus Galloway, Anna GolubeVa, Thomas Tanay, Medhat Moussa, and Graham W. Taylor. Batch
normalization is a cause of adVersarial Vulnerability. CoRR, abs/1905.02161, 2019. URL http:
//arxiv.org/abs/1905.02161.
XaVier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
Statistics, pp. 249-256, 2010.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, LukaSz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Boris Hanin and DaVid Rolnick. How to start training: The effect of initialization and architecture.
In Advances in Neural Information Processing Systems, pp. 571-581, 2018.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231,
2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. DelVing deep into rectifiers: Surpassing
human-leVel performance on imagenet classification. 2015. URL http://arxiv.org/abs/
1502.01852.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer Vision -
ECCV 2016, pp. 630-645, Cham, 2016. Springer International Publishing.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal coVariate shift. arXiv preprint arXiv:1502.03167, 2015.
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
neural networks. In Advances in neural information processing systems, pp. 971-980, 2017.
Lu Lu, Yeonjong Shin, Yanhui Su, and George Em Karniadakis. Dying relu and initialization:
Theory and numerical examples. arXiv preprint arXiv:1903.06733, 2019.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improVe neural net-
work acoustic models.
Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422,
2015.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improVe restricted boltzmann machines. In
Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807-814,
2010.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch nor-
malization help optimization? In Advances in Neural Information Processing Systems, pp. 2483-
2493, 2018.
Wenling Shang, Justin Chiu, and Kihyuk Sohn. Exploring normalization in deep residual networks
with concatenated rectified linear units. In Thirty-First AAAI Conference on Artificial Intelligence,
2017.
9
Under review as a conference paper at ICLR 2020
Andrew Shaw, Yaroslav Bulatov, and Jeremy Howard. Imagenet in 18 minutes. URL https:
//github.com/diux-dev/imagenet18.
RUPesh KUmar Srivastava, KlaUs Greff, and Jurgen Schmidhuber. Highway networks. arXivpreprint
arXiv:1505.00387, 2015.
Dmitry Ulyanov, Andrea Vedaldi, and Victor LemPitsky. Instance normalization: The missing in-
gredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
YUxin WU and Kaiming He. GroUP normalization. In Proceedings of the European Conference on
Computer Vision (ECCV), pp. 3-19, 2018.
Hongyi Zhang, Yann N DaUphin, and TengyU Ma. FixUp initialization: ResidUal learning withoUt
normalization. arXiv preprint arXiv:1901.09321, 2019.
JUn-Yan ZhU, TaesUng Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
Using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference
on computer vision, pp. 2223-2232, 2017.
A Appendix
A. 1 Elementary linear programming
Proposition A.1 (Farkas’ Lemma). Let W ∈ Rm×n and b ∈ Rm.
{x | Wx + b = 0,x ≥ 0} = 0 ^⇒ @ λ s.t. λTW ≥ 0, λTb > 0.
Corollary A.2. Let W ∈ Rm×n and b ∈ Rm. Then
{x | Wx + b ≤ 0} = 0 O ∃ λ s.t λTW = 0, λTb > 0 (λ ≥ 0).
Proof. SUppose by contradiction there exists an x sUch that Wx + b ≤ 0. Let x+ := max{x, 0}
and x- := max{-x, 0}, and so x = x+ - x-. Then:
∃xs.tWx+b≤0
y⇒ ∃x+, X- s.t W(x+ — x-) + b ≤ 0
^⇒ ∃x+,x-,λ s.t W(x+ — x-) + λ + b = 0 (λ ≥ 0)
o X := (x+,x-,λ)T ≥ 0, W :=[W | — W 11] s.t Wx + b = 0,
which follows from Farkas, Lemma.	口
A.2 Algorithm for residual Farkas layer
Algorithm 2 Residual Farkas layer with ReLU activation
Initialize W ⑴，W ⑵,b⑴,b⑵
Forward pass: x
y = F(x; W(1), b(1))
y J W(2)y	. e.g. Linear and/or Conv with no bias
Compute aggregated components:
y := —AggFunc(x + y)	. residual component
b := max{—AggFunc(b(2) [0 : —1]), b(m2)}
y J concatenate(y, y)
b J concatenate(b(2) [0 : —1], b)
yJy+b
y J ReLU(y)
Optional: apply batch normalization to y
Return y
10
Under review as a conference paper at ICLR 2020
A.3 Training curves for CIFAR 1 0
(a) 18 layers
(b) 34 layers
(c) 50 layers
(d) 101 layers
Figure 4: Average training curves for CIFAR10; “Vanilla” refers to standard ResNet. Dotted lines
omit batch normalization, solid lines incorporate it.
Figure 5: Average training curve for 101-layer FarkasNet, with “mean” aggregation function, maxi-
mal learning rate, and without batch normalization.
11
Under review as a conference paper at ICLR 2020
A.4 Training curves for CIFAR 1 00
For 50-layers, one of the networks (standard ResNet without BN) failed to train, and we omit it from
the average.
(a) 34 layers
(b) 50 layers
(c) 101 layers
Figure 6: Average training curves for CIFAR100; “Vanilla” refers to standard ResNet
A.5 Training curves for ImageNet- 1 k
Here, dotted line means Farkas layer-based network.
(a) 50 layers	(b) 101 layers	(c) 152 layers
Figure 7: Training curves for ImageNet
12