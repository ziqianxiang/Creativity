Under review as a conference paper at ICLR 2020
WikiMatrix: Mining 135M Parallel Sentences
in 1620 Language Pairs from Wikipedia
Anonymous authors
Paper under double-blind review
Ab stract
We present an approach based on multilingual sentence embeddings to automat-
ically extract parallel sentences from the content of Wikipedia articles in 85 lan-
guages, including several dialects or low-resource languages. We do not limit the
extraction process to alignments with English, but systematically consider all pos-
sible language pairs. In total, we are able to extract 135M parallel sentences for
1620 different language pairs, out of which only 34M are aligned with English.
This corpus of parallel sentences is freely available.1
To get an indication on the quality of the extracted bitexts, we train neural MT
baseline systems on the mined data only for 1886 languages pairs, and evaluate
them on the TED corpus, achieving strong BLEU scores for many language pairs.
The WikiMatrix bitexts seem to be particularly interesting to train MT systems
between distant languages without the need to pivot through English.
1	Introduction
Most of the current approaches in Natural Language Processing (NLP) are data-driven. The size
of the resources used for training is often the primary concern, but the quality and a large variety
of topics may be equally important. Monolingual texts are usually available in huge amounts for
many topics and languages. However, multilingual resources, typically sentences in two languages
which are mutual translations, are more limited, in particular when the two languages do not involve
English. An important source of parallel texts are international organizations like the European
Parliament (Koehn, 2005) or the United Nations (Ziemski et al., 2016). These are professional
human translations, but they are in a more formal language and tend to be limited to political topics.
There are several projects relying on volunteers to provide translations for public texts, e.g. news
commentary (Tiedemann, 2012), OpensubTitles (Lison & Tiedemann, 2016) or the TED corpus (Qi
et al., 2018)
Wikipedia is probably the largest free multilingual resource on the Internet. The content of
Wikipedia is very diverse and covers many topics. Articles exist in more than 300 languages. Some
content on Wikipedia was human translated from an existing article into another language, not nec-
essarily from or into English. Eventually, the translated articles have been later independently edited
and are not parallel any more. Wikipedia strongly discourages the use of unedited machine transla-
tion,2 but the existence of such articles can not be totally excluded. Many articles have been written
independently, but may nevertheless contain sentences which are mutual translations. This makes
Wikipedia a very appropriate resource to mine for parallel texts for a large number of language pairs.
To the best of our knowledge, this is the first work to process the entire Wikipedia and systematically
mine for parallel sentences in all language pairs. We hope that this resource will be useful for several
research areas and enable the development of NLP applications for more languages.
In this work, we build on a recent approach to mine parallel texts based on a distance measure in a
joint multilingual sentence embedding space (Schwenk, 2018; Artetxe & Schwenk, 2018b). For this,
we use the freely available LASER toolkit3 which provides a language agnostic sentence encoder
which was trained on 93 languages (Artetxe & Schwenk, 2018a). We approach the computational
1Anonymized for review
2https://en.wikipedia.org/wiki/Wikipedia:Translation
3https://github.com/facebookresearch/LASER
1
Under review as a conference paper at ICLR 2020
challenge to mine in almost six hundred million sentences by using fast indexing and similarity
search algorithms.
The paper is organized as follows. In the next section, we first discuss related work. We then sum-
marize the underlying mining approach. Section 4 describes in detail how we applied this approach
to extract parallel sentences from Wikipedia in 1620 language pairs. To asses the quality of the
extracted bitexts, we train NMT systems for a subset of language pairs and evaluate them on the
TED corpus (Qi et al., 2018) for 45 languages. These results are presented in section 5. The paper
concludes with a discussion of future research directions.
2	Related work
There is a large body of research on mining parallel sentences in collections of monolingual texts,
usually named “comparable coprora”. Initial approaches to bitext mining have relied on heavily en-
gineered systems often based on metadata information, e.g. (Resnik, 1999; Resnik & Smith, 2003).
More recent methods explore the textual content of the comparable documents. For instance, it was
proposed to rely on cross-lingual document retrieval, e.g. (Utiyama & Isahara, 2003; Munteanu
& Marcu, 2005) or machine translation, e.g. (Abdul-Rauf & Schwenk, 2009; Bouamor & Sajjad,
2018), typically to obtain an initial alignment that is then further filtered. In the shared task for
bilingual document alignment (Buck & Koehn, 2016), many participants used techniques based on
n-gram or neural language models, neural translation models and bag-of-words lexical translation
probabilities for scoring candidate document pairs. The STACC method uses seed lexical transla-
tions induced from IBM alignments, which are combined with set expansion operations to score
translation candidates through the Jaccard similarity coefficient (Etchegoyhen & Azpeitia, 2016;
Azpeitia et al., 2017; 2018). Using multilingual noisy web-crawls such as ParaCrawl4 for filtering
good quality sentence pairs has been explored in the shared tasks for high resource (Koehn et al.,
2018) and low resource (Koehn et al., 2019) languages.
In this work, we rely on massively multilingual sentence embeddings and margin-based mining in
the joint embedding space, as described in (Schwenk, 2018; Artetxe & Schwenk, 2018b;a). This
approach has also proven to perform best in a low resource scenario (Chaudhary et al., 2019; Koehn
et al., 2019). Closest to this approach is the research described in EsPana-Bonet et al. (2017); Hassan
et al. (2018); Guo et al. (2018); Yang et al. (2019). However, in all these works, only bilingual
sentence representations have been trained. Such an approach does not scale to many languages,
in particular when considering all possible language pairs in Wikipedia. Finally, related ideas have
been also proposed in Bouamor & Sajjad (2018) or Gregoire & Langlais (2017). However, in those
works, mining is not solely based on multilingual sentence embeddings, but they are part of a larger
system. To the best of our knowledge, this work is the first one that applies the same mining approach
to all combinations of many different languages, written in more than twenty different scripts.
Wikipedia is arguably the largest comparable corpus. One of the first attempts to exploit this re-
source was performed by Adafre & de Rijke (2006). An MT system was used to translate Dutch
sentences into English and to compare them with the English texts. This method yielded several
hundreds of Dutch/English parallel sentences. Later, a similar technique was applied to the Per-
sian/English pair (Mohammadi & GhasemAghaee, 2010). Structural information in Wikipedia such
as the topic categories of documents was used in the alignment of multilingual corpora (Otero &
Lopez, 2010). In another work, the mining approach of Munteanu & Marcu (2005) was applied to
extract large corpora from Wikipedia in sixteen languages (Smith et al., 2010). Otero et al. (2011)
measured the comparability of Wikipedia corpora by the translation equivalents on three languages
Portuguese, Spanish, and English. Patry & Langlais (2011) came up with a set of features such as
Wikipedia entities to recognize parallel documents, and their approach was limited to a bilingual
setting. Tufis et al. (2013) proposed an approach to mine parallel sentences from Wikipedia textual
content, but they only considered high-resource languages, namely German, Spanish and Romanian
paired with English. Tsai & Roth (2016) grounded multilingual mentions to English wikipedia by
training cross-lingual embeddings on twelve languages. Gottschalk & Demidova (2017) searched
for parallel text passages in Wikipedia by comparing their named entities and time expressions. Fi-
nally, Aghaebrahimian (2018) propose an approach based on bilingual BiLSTM sentence encoders
to mine German, French and Persian parallel texts with English. Parallel data consisting of aligned
4http://www.paracrawl.eu/
2
Under review as a conference paper at ICLR 2020
Wikipedia titles have been extracted for twenty-three languages5. Since Wikipedia titles are rarely
entire sentences with a subject, verb and object, it seems that only modest improvements were ob-
served when adding this resource to the training material of NMT systems.
We are not aware of other attempts to systematically mine for parallel sentences in the textual content
of Wikipedia for a large number of languages.
3	Distance-based mining approach
The underling idea of the mining approach used in this work is to first learn a multilingual sentence
embedding, i.e. an embedding space in which semantically similar sentences are close independently
of the language they are written in. This means that the distance in that space can be used as an
indicator whether two sentences are mutual translations or not. Using a simple absolute threshold
on the cosine distance was shown to achieve competitive results (Schwenk, 2018). However, it has
been observed that an absolute threshold on the cosine distance is globally not consistent, e.g. (Guo
et al., 2018). The difficulty to select one global threshold is emphasized in our setting since we are
mining parallel sentences for many different language pairs.
3.1	Margin criterion
The alignment quality can be substantially improved by using a margin criterion instead of an ab-
solute threshold (Artetxe & Schwenk, 2018b). In that work, the margin between two candidate
sentences x and y is defined as the ratio between the cosine distance between the two sentence
embeddings, and the average cosine similarity of its nearest neighbors in both directions:
margin(x, y)
cos(x, y)
Xcos(x, z)
2k
z ∈NNk (x)
+ X cos(y,z)
+ 乙—2k―
z∈NNk (y)
(1)
where NNk(x) denotes thek unique nearest neighbors of x in the other language, and analogously
for NNk (y). We used k = 4 in all experiments.
We follow the “max” strategy as described in (Artetxe & Schwenk, 2018b): the margin is first cal-
culated in both directions for all sentences in language L1 and L2. We then create the union of these
forward and backward candidates. Candidates are sorted and pairs with source or target sentences
which were already used are omitted. We then apply a threshold on the margin score to decide
whether two sentences are mutual translations or not. Note that with this technique, we always get
the same aligned sentences, independently of the mining direction, e.g. searching translations of
French sentences in a German corpus, or in the opposite direction. The reader is referred to Artetxe
& Schwenk (2018b) for a detailed discussion with related work.
The complexity of a distance-based mining approach is O(N × M), where N and M are the num-
ber of sentences in each monolingual corpus. This makes a brute-force approach with exhaustive
distance calculations intractable for large corpora. Margin-based mining was shown to significantly
outperform the state-of-the-art on the shared-task of the workshop on Building and Using Compa-
rable Corpora (BUCC) (Artetxe & Schwenk, 2018b). The corpora in the BUCC corpus are rather
small: at most 567k sentences.
The languages with the largest Wikipedia are English and German with 134M and 51M sentences,
respectively, after pre-processing (see Section 4.1 for details). This would require 6.8×1015 distance
calculations.6 We show in Section 3.3 how to tackle this computational challenge.
3.2	Multilingual sentence embeddings
Distance-based bitext mining requires a joint sentence embedding for all the considered languages.
One may be tempted to train a bi-lingual embedding for each language pair, e.g. (EsPana-Bonet
5https://linguatools.org/tools/corpora/wikipedia-parallel-titles-corpora/
6Strictly speaking, Cebuano and Swedish are larger than German, yet mostly consist of template/machine
translated text https://en.wikipedia.org/wiki/List_of_Wikipedias
3
Under review as a conference paper at ICLR 2020
Table 1: Architecture of the system used to train massively multilingual sentence embeddings. See
Artetxe & Schwenk (2018a) for details.
et al., 2017; Hassan et al., 2018; Guo et al., 2018; Yang et al., 2019), but this is difficult to scale
to thousands of language pairs present in Wikipedia. Instead, we chose to use one single massively
multilingual sentence embedding for all languages, namely the one proposed by the open-source
LASER toolkit (Artetxe & Schwenk, 2018a). Training one joint multilingual embedding on many
languages at once also has the advantage that low-resource languages can benefit from the similarity
to other language in the same language family. For example, we were able to mine parallel data for
several Romance (minority) languages like Aragonese, Lombard, Mirandese or Sicilian although
data in those languages was not used to train the multilingual LASER embeddings.
The underlying idea of LASER is to train a sequence-to-sequence system on many language pairs at
once using a shared BPE vocabulary and a shared encoder for all languages. The sentence represen-
tation is obtained by max-pooling over all encoder output states. Figure 1 illustrates this approach.
The reader is referred to Artetxe & Schwenk (2018a) for a detailed description.
3.3 Fast similarity search
Fast large-scale similarity search is an area with a large body of research. Traditionally, the ap-
plication domain is image search, but the algorithms are generic and can be applied to any type
of vectors. In this work, we use the open-source FAISS library7 which implements highly ef-
ficient algorithms to perform similarity search on billions of vectors (Johnson et al., 2017). An
additional advantage is that FAISS has support to run on multiple GPUs. Our sentence represen-
tations are 1024-dimensional. This means that the embeddings of all English sentences require
153 ∙ 106 X 1024 X 4 = 513 GB of memory. Therefore, dimensionality reduction and data ComPres-
sion are needed for efficient search. In this work, we chose a rather aggressive compression based
on a 64-bit product-quantizer (JegoU et al., 2011), and portioning the search space in 32k cells. This
corresPonds to the index tyPe “OPQ64,IVF32768,PQ64” in FAISS terms.8 Another interesting
compression method is scalar quantization. A detailed comparison is left for future research. We
build and train one FAISS index for each language.
The compressed FAISS index for English requires only 9.2GB, i.e. more than fifty times smaller
than the original sentences embeddings. This makes it possible to load the whole index on a standard
GPU and to run the search in a very efficient way on multiple GPUs in parallel, without the need to
shard the index. The overall mining process for German/English requires less than 3.5 hours on 8
GPUs, including the nearest neighbor search in both direction and scoring all candidates
4	B itext mining in Wikipedia
For each Wikipedia article, it is possible to get the link to the corresponding article in other lan-
guages. This could be used to mine sentences limited to the respective articles. One one hand, this
local mining has several advantages: 1) mining is very fast since each article usually has a few
hundreds of sentences only; 2) it seems reasonable to assume that a translation ofa sentence is more
likely to be found in the same article than anywhere in the whole Wikipedia. On the other hand, we
7https://github.com/facebookresearch/faiss
8https://github.com/facebookresearch/faiss/wiki/Faiss-indexes
4
Under review as a conference paper at ICLR 2020
L1 (French)	Ceci est Une tres grande maison
L2 (German)	Das ist ein Sehr groβes Haus This is a very big house Ez egy nagyon nagy haz Ini rumah yang Sangat besar
Table 2: Illustration how sentences in the wrong language can hurt the alignment process with a
margin criterion. See text for a detailed discussion.
hypothesize that the margin criterion will be less efficient since one article has usually few sentences
which are similar. This may lead to many sentences in the overall mined corpus of the type “NAME
was born on DATE in CITY”, “BUILDING is a monument in CITY built on DATE”, etc. Although
those alignments may be correct, we hypothesize that they are of limited use to train an NMT sys-
tem, in particular when they are too frequent. In general, there is a risk that we will get sentences
which are close in structure and content.
The other option is to consider the whole Wikipedia for each language: for each sentence in the
source language, we mine in all target sentences. This global mining has several potential advan-
tages: 1) we can try to align two languages even though there are only few articles in common; 2)
many short sentences which only differ by the name entities are likely to be excluded by the margin
criterion. A drawback of this global mining is a potentially increased risk of misalignment and a
lower recall.
In this work, we chose the global mining option. This will allow us to scale the same approach to
other, potentially huge, corpora for which document-level alignments are not easily available, e.g.
Common Crawl. An in depth comparison of local and global mining (on Wikipedia) is left for future
research.
4.1	Corpus preparation
Extracting the textual content of Wikipedia articles in all languages is a rather challenging task, i.e.
removing all tables, pictures, citations, footnotes or formatting markup. There are several ways to
download Wikipedia content. In this study, we use the so-called CirrusSearch dumps since they
directly provide the textual content without any meta information.9 We downloaded this dump in
March 2019. A total of about 300 languages are available, but the size obviously varies a lot between
languages. We applied the following processing:
•	extract the textual content;
•	split the paragraphs into sentences;
•	remove duplicate sentences;
•	perform language identification and remove sentences which are not in the expected lan-
guage (usually, citations or references to texts in another language).
It should be pointed out that sentence segmentation is not a trivial task, with many exceptions and
specific rules for the various languages. For instance, itis rather difficult to make an exhaustive list of
common abbreviations for all languages. In German, points are used after numbers in enumerations,
but numbers may also appear at the end of sentences. Other languages do not use specific symbols
to mark the end of a sentence, namely Thai. We are not aware of a reliable and freely available
sentence segmenter for Thai and we had to exclude that language. We used the freely available
Python tool10 which is based on Moses scripts. Regular expressions were used for most of the Asian
languages, falling back to English for the remaining languages. This gives us 879 million sentences
in 300 languages. The margin criterion to mine for parallel data requires that the texts do not contain
duplicates. This removes about 25% of the sentences.11
9https://dumps.wikimedia.org/other/cirrussearch/
10https://pypi.org/project/sentence-splitter/
11The Cebuano and Waray Wikipedia were largely created by a bot and contain more than 65% of duplicates.
5
Under review as a conference paper at ICLR 2020
55453525
2 4. 2 3. 2 2. 2 1.
222 2
UELB
Margin threshold
3000
2800
2600
2400
2200
2000
1800
1600
1400
1200
1000
800
600
400
200
0
Xu- QZ-S-xq-ξ
11
UELB
10
1.06	1.055	1.05	1.045	1.04	1.035	1.03
Margin threshold
100
50
0
* U- Qz--XQ-5
Figure 1: BLEU scores (continuous lines) for several NMT systems trained on bitexts extracted
from Wikipedia for different margin thresholds. The size of the mined bitexts are depicted as dashed
lines.
LASER’s sentence embeddings are totally language agnostic. This has the side effect that the sen-
tences in other languages (e.g. citations or quotes) may be considered closer in the embedding space
than a potential translation in the target language. Table 2 illustrates this problem. The algorithm
would not select the German sentence although it is a perfect translation. The sentences in the other
languages are also valid translations which would yield a very small margin. To avoid this problem,
we perform language identification (LID) on all sentences and remove those which are not in the
expected language. LID is performed with fasttext12 (Joulin et al., 2016). Fasttext does not support
all the 300 languages present in Wikipedia and we disregarded the missing ones (which typically
have only few sentences anyway). After deduplication and LID, we dispose of 595M sentences in
182 languages. English accounts for 134M sentences, and German with 51M sentences is the second
largest language. The sizes for all languages are given in Tables 4 and 6.
4.2	Threshold optimization
Artetxe & Schwenk (2018b) optimized their mining approach for each language pair on a provided
corpus of gold alignments. This is not possible when mining Wikipedia, in particular when con-
sidering many language pairs. In this work, we use an evaluation protocol inspired by the WMT
shared task on parallel corpus filtering for low-resource conditions (Koehn et al., 2019): an NMT
system is trained on the extracted bitexts - for different thresholds - and the resulting BLEU scores
are compared. We choose newstest2014 of the WMT evaluations since it provides an N -way
parallel test sets for English, French, German and Czech. We favoured the translation between two
morphologically rich languages from different families and considered the following language pairs:
German/English, German/French, Czech/German and Czech/French. The size of mined bitexts is in
the range of 100k to more than 2M (see Table 3 and Figure 1). We did not try to optimize the archi-
tecture of the NMT system to the size of the bitexts and used the same architecture for all systems:
the encoder and decoder are 5-layer transformer models as implemented in fairseq (Ott et al.,
2019). The goal of this study is not to develop the best performing NMT system for the considered
languages pairs, but to compare different mining parameters.
The evolution of the BLEU score in function of the margin threshold is given in Figure 1. De-
creasing the threshold naturally leads to more mined data - we observe an exponential increase of
the data size. The performance of the NMT systems trained on the mined data seems to change as
expected, in a surprisingly smooth way. The BLEU score first improves with increasing amounts of
available training data, reaches a maximum and than decreases since the additional data gets more
and more noisy, i.e. contains wrong translations. It is also not surprising that a careful choice of
the margin threshold is more important in a low-resource setting. Every additional parallel sentence
is important. According to Figure 1, the optimal value of the margin threshold seems to be 1.05
when many sentences can be extracted, in our case German/English and German/French. When less
parallel data is available, i.e. Czech/German and Czech/French, a value in the range of 1.03-1.04
seems to be a better choice. Aiming at one threshold for all language pairs, we chose a value of 1.04.
It seems to be a good compromise for most language pairs. However, for the open release of this
corpus, we provide all mined sentence with a margin of 1.02 or better. This would enable end users
12https://fasttext.cc/docs/en/language-identification.html
6
Under review as a conference paper at ICLR 2020
Bitexts	de-en	de-fr	cs-de	cs-fr
	1.9M	1.9M	568k	627k
Europarl	21.5	23.6	14.9	21.5
	1.0M	370k	200k	220k
	21.2	21.1	12.6	19.2
Mined	1.0M	372k	201k	219k
Wikipedia	24.4	22.7	13.1	16.3
Europarl	3.0M	2.3M	768k	846k
+ Wikipedia	25.5	25.6	17.7	24.0
Table 3: Comparison of NMT systems trained on the Europarl corpus and on bitexts automatically
mined in Wikipedia by our approach at a threshold of 1.04. We give the number of sentences (first
line) and the BLEU score (second line of each bloc) on newstest2014.
to choose an optimal threshold for their particular applications. However, it should be emphasized
that we do not expect that many sentence pairs with a margin as low as 1.02 are good translations.
For comparison, we also trained NMT systems on the Europarl corpus V7 (Koehn, 2005), i.e. pro-
fessional human translations, first on all available data, and then on the same number of sentences
than the mined ones (see Table 3). With the exception of Czech/French, we were able to achieve
better BLEU scores with the automatically mined bitexts in Wikipedia than with Europarl of the
same size. Adding the mined text to the full Europarl corpus, also leads to further improvements of
1.1 to 3.1 BLEU. We argue that this is a good indicator of the quality of the automatically extracted
parallel sentences.
5	Result analysis
We run the alignment process for all possible combinations of languages in Wikipedia. This yielded
1620 language pairs for which we were able to mine at least ten thousand sentences. Remember
that mining L1 → L2 is identical to L2 → L1 , and is counted only once. We propose to analyze
and evaluate the extracted bitexts in two ways. First, we discuss the amount of extracted sentences
(Section 5.1). We then turn to a qualitative assessment by training NMT systems for all language
pairs with more than twenty-five thousand mined sentences (Section 5.2).
5.1	Quantitative analysis
Due to space limits, Table 4 summarizes the number of extracted parallel sentences only for lan-
guages which have a total of at least five hundred thousand parallel sentences (with all other lan-
guages at a margin threshold of 1.04). Additional results are given in Table 6 in the Appendix.
There are many reasons which can influence the number of mined sentences. Obviously, the larger
the monolingual texts, the more likely it is to mine many parallel sentences. Not surprisingly, we
observe that more sentences could be mined when English is one of the two languages. Let us point
out some languages for which it is usually not obvious to find parallel data with English, namely
Indonesian (1M), Hebrew (545k), Farsi (303k) or Marathi (124k sentences). The largest mined texts
not involving English are Russian/Ukrainian (2.5M), Catalan/Spanish (1.6M), between the Romance
languages French, Spanish, Italian and Portuguese (480k-923k), and German/French (626k).
It is striking to see that we were able to mine more sentences when Galician and Catalan are paired
with Spanish than with English. On one hand, this could be explained by the fact that LASER’s
multilingual sentence embeddings may be better since the involved languages are linguistically very
similar. On the other, it could be that the Wikipedia articles in both languages share a lot of content,
or are obtained by mutual translation.
Services from the European Commission provide human translations of (legal) texts in all the 24
official languages of the European Union. This N-way parallel corpus enables training of MT system
to directly translate between these languages, without the need to pivot through English. This is
7
ISOName Language size az ba be bg bn bs ca cs da de el en eo es et eu fa π ɪr gl he hi hr hu id is it ja kk ko It mk ml mr ne nl no oc pl pt ro ru sh sɪ sk Sl sq sr sv sw ta te tl tr tt uk vɪ zh total
Family
ar	Arabic	Arabic	6516 17 15 11 54 4 0 34 94 67 53
az	Azerbaijani Turkic		1873	7 3 14 8 9 15 17 11
ba	Bashkir	Turkic	536	2 14 4 9 17 16 13
be	Belarusian	Slavic	1690	16 4 7 16 14 9
bg	Bulgarian	Slavic	3327	38 34 76 79 53
bn	Bengali	Indo-Aryan	1412	21 41 47 33
bs	Bosnian	Slavic	1060	44 43 32
ca	Catalan	Romance	8332	100 86
cs	Czech	Slavic	7634	75
da	Danish	Germanic	2881
de	German	Germanic	50944
el	Greek	Hellenic	3211
en	English	Germanic	134431
eo	Esperanto	constructed	2371
es	Spanish	Romance	25202
et	Estonian	Uralic	2303
eu	Basque	Isolate	2259
fa	Farsi	Iranian	3954
fi	Finnish	Uralic	7428
fr	French	Romance	34494
gɪ	Galician	Romance	2221
he	Hebrew	Semitic	6962
hi	Hindi	Indo-Aryan	1353
hr	Croatian	Slavic	2229
hu	Hungarian	Uralic	7702
id	Indonesian	Malayo-	3899
		POIyneSian	
is	Icelandic	Germanic	488
it	Italian	Romance	21025
ja	Japanese	Japonic	31614
kk	Kazakh	Turkic	1684
ko	Korean	Koreanic	5400
It	Li⅛uanian	Baltic	2031
99	66	999	37 174		40	24	58	53 163		50 68		38	38	60	90	18∣	123	83	11	48	33	52	32	32	12	73	58	9	74	157	71	125	35	32	32	39	30	49	58	13	27	27	18
34	10	71	9	31	10	7	16	14 29		10	12	9	10	15	17	4	25	23	6	11	9	8	5	4	2	22	13	2	22		14	47	9	3	9	8	9	10	19	5	11	5	2
27	10	28	9	28	8	6	9	12	29	12	9	4	10	12	12	3	26	13	4	5	7	9	1	1	1	21	15	3	19	24	15	42	10	1	10	10	7	10	20	4	3	1	1
20	8	33	9	28	7	5	5	10	24	9	10	3	9	11	9	3	24	12	2	5	7	8	2	2	1	14	10	2	19	23	13∣	161	8	2	8	7	5	10	16	2	3	2	1
132	62∣	357	40	122	43	25	37	61	117	43	58	30	47	68	60	17	102	71	11	38	42	86	29	35	13	84	58	9	96	114	69∣	270	41	31	43	46	26	65	63	12	21	23	21
70	36	280	27	81	26	14	20	37	68	27	34	21	23	41	36	8	64	38	4	20	21	23	7	8	3	50	35	4	52	76	46	62	20	8	25	26	17	25	54	6	12	8	4
71	33	210	24	70	25	16	20	36	60	32	30	16∣	164	39	38	11	52	36	6	22	23	39	19	20	8	45	36	6	48	62	37	59	还	16	25	34	19∣	130	38	7	15	12	13
180	90	1205	8"	158^	54	77	44	83∣	490：	268	84	37	57	92	107	23	ʒɪ⅛	103	12	52	45	61	45	56	17	144	102	57	121	358	110	169	52	52	50	57	34	67	102	14	30	35	31
233	70	519	75	181	62	36	45	95	185	54	72	38	63	105	78	23	161	105	15	53	55	51	36	41	15	139	86	11	176	153	82	186	50	37∣	.	64	30	63	97	15	31	34	25
180	54	436	39	140	45	26	29	75	142	44	55	25	43	69	63	20	115	76	9	37	35	39	30	35	12	110：	303	8	89	123	70	109	37	32	39	40	23	43	168	11	20	21	23
	95I	1573	186	418	106	53	66	163	飞方	80	109	57	87	192	107	34	388：	217	23	82	78	64	51	58	21 .	472：	207	"I	285	294	129	368	68	50	94	106	51	81	216	20	58	57	32
		620	39	145	41	23	35	55	137	48	56	26	43	64	73	15	119	69	6	35	34	52	27	32	9	76	60	8	77	144	78	114	38	31	35	46	28	52	62	11	16	18	20
298 3377 243 119	303	375 2757 446 545 231 259 488	1019 85 2126 851	20 306 157 395 71	124	15 796 636	37 668 2461	631	1661	224 115 178 318 180 395 546 51	95 91	75∣
149 31 25	23	46 134 46 39 22 29 57	46 15 101 48	7 26 28 30 28	28	9 81 47	9 77 91	43	81	26 28 41 32 19 36 53 8	16 19	17
89 154	83	155∣ 905 610 153 71 94 167	198 42∣671 219	26 108 76 92 65	98 25 272 181	35 235 923	183	393	81 84 81 93 53 107 181 21	57 71	48
22	24	70	85	32	39	20	33「56	41 14	75	57	6	29	35	32 20	21	8	72	49	7	73	76	48	96	27	19	34	35	18	34	58	9 16 16 15 14	33	65	43	25	13	21	35	27 10	54	33	5	18	19	19 11	10	5	44	29	6	43	58	30	47	19	9	20	20	12	21	38	6 13 11 8 34	71	25	36	20	24	39	46 9	64	46	8	26	20	27 11	10	6	49	32	4	50	77	40	72	21	9	21	24	17	30	42	8 21 12 7 156 47 64 28 4 8 9 0 63 22∏31 87 9 43 47 40 29 30 10 126 86 WΓ∏9 131 69 139 39 27 50 46 25 46 126 12 23 24 21
154 136 60 85 164 161 3S 744 21∣ 24 89 71 83 62 83 25^31 166 124 255 558 206 410 72 74 83 86 48 92 186 19 56 65 42
41 21	33	50	56	14	120	50	9	28	27	35 29	39 11	66	52	17	65	227	56	84	30	36	29	33	20	39	54	8	15	17	22 28	41	65	63	17	121	82	7	43	35	42 26	25 9	86	64	8	84	133	67	131	35	21	36	38	23	45	67 10	21	25	13 21	33	31	7	56	35	3	18	16	24 6	11 12	40	27	4	44	63	35	56	17	6	18	21	14	22	40	7	13	18	3 58	47	14	80	48	8	27	31	52 24	24 10	65	48	7	71	85	51	85[66§	19	35	53	21 205	56	9	16	16	17 70 20	146 99 11	49	48 47 27 28 10 121 75	10 126 148	87	149	46 26 56 55 27 53 88 13 29 30 20 16	146 77 8	45	33 55 25 23 10 101 83	8 93 204	94	127	43 23 37 46 32 56 79 13 24 19 21
mk	Macedonian Slavic	1487
ml	Malayalam	Dravidian	272
mr	Marathi	Indo-Aryan	503
Nepali	Indo-Aryan	235
Dutch	Germanic	14091
Norwegian	Germanic	1364
Occitan	Romance	472
Polish	Slavic	16270
Portuguese	Romance	13354
Romanian	Romance	3504
Russian	Slavic	32537
Serbo-	Sou±	2069
31 18 2 9 12 12 6 5 3 27 22
■ 24 83 ^^3 ^78 24 240 150
14，47 48 21 23 8 123 81
6 5 6 2 2 1 18 11
23 26 10	10	4	56	41
28 16	16	6	57	39
21	22	9	53	46
3	1	41	32
1 46 35
ε 1 O Clt O U 1
nnnoppκrs
17 13
3 27
20 219
9 128
1 17
5 63
6 70
6 56
7 41
5 47
4 17
13 177
9 103
35 20 30
480 161 303
175 79 196
22 12 32
93 47 89
64 39 1 07
93 56 88
58 35 46
86 47 50
21 14 19
218 96 199
161 74 121
13 24
13
67
40
8
23
25
52
21
22
10
53
42
4
68
19
1
9
15
19
2
3
1
42
28
13
72
48
8
25
30
29
22
25
10
66
43
1381507263039212516451
9 13 28 4 7 5 5
48 83 15J 19 44 58 41
28 51 96 12 37 31 12
6 8 16 3 3 1 1
17 29 51 7 17 13 5
16 29 46 8 13 11 12
25 106 48 9 14 15 16
13 20 44 6 4 3 1
13 19 56 4 5 3 1
6 9 17 3 1 1
34 61 151 16 37 35 29
26 47 2雷 12 24 23 24
6 5 6 6 4 7 12 2 3 3 3
200 97	285	56	40	81	68	35 69	121	16 39 42 28
177	312	74	76	71	85	47 101	155	20 42 54 45
136	44	43	42	49	30 58	75	15 23 27 29
70 42 85 78
17 27 46
44 114 140 17 54 55 29
69 14 70 93 86 4435
42 8 19 10 19 885
11 8 15	7	10	697
8 2 80	8	8	803
56 12Q26 60 60 3770
33	37	31	31	鼓 91
33 8 39 38 31 S⅛348
77 16.98 106 90 酬47
75 14 104 74 80*325
55 11 62 68 57 爆)06
127 23Q65 107 134 嗖692
56 9 68 75 62@28
471
37 8	50	42	39 0817
147 26	187	206	174 §980
43 7	56	40	44¾11
29 5	30	25	23 §699
42 9	41	38	42 SS122
72 10匚 76 60 64 卷98
130 26 170 165 157 康41
43 12 51 58 46 奥69
54 8.73 66 62 §666
29 4	33	26	30⅛781
42 9	55	46	42辑629
7J 9	83	74	75-⅛357
79 11	73	146	83 R743
16 3	18	16	14T95
112 23	144	143	137 @84
84 12	92	75 267 ⅛382
10 4	14	6	9 吗 81
47 5	48	49	57 处39
36 6	57	33	35 洒91
43 8	57	57	45 661
16 2	26	15	17 1388
17 1	26	14	17 1648
7 1 11	6	6 584
90 18(04 84 88 6364
61 13	69	79	63 4771
7 3	9	8	7 706
^2 16 172 84 92 6035
140 23 156 213 16510932
72 13__82 96 74 4515
119 25EM 122 14811311
35 9 45 42 36 3337
Croatian Slavic
languages
si Sinhala		Indo-Aryan	320
sk	Slovak	Slavic	1904
si	Slovenian	Slavic	1836
Sq	Albanian	Albanian	740
sr	Serbian	Slavic	4226
sv	Swedish	Germanic	17906
SW	Swahili	Niger- Congo	235
ta	Tamil	Dravidian	1629
te	Telugu	Dravidian	1509
tl	Tagalog	Malayo- POIyneSian	312
tr	Turkish	Turkic	4067
tt	Tatar	Turkic	449
uk	Ukrainian	Slavic	12871
vi	Vietnamese Vietic		6727
Zh	Chinese	Chinese	12306
22 22
36
10 15 48 4 4 4 2
17 34 51 8 14 15 16
19 47 50 9 15 16 17
25 31 6 13 10 10
51 9 19 18 14
16 33 39 35
15 1 21
37 9 51
39 8 50
27	29
43 8匚71
72 17 82
12	12
14 16 1464
38 38 2726
48 42 2623
30 24 1515
56 45 3510
74 73 5107
10 11 652
6 2
1
29	5	30	19	27	1315
21	1	30	16	20	1315
12	1	16	17	W	1015
10 67 77 69 3698
11	7 10 603
73 72 7043
89 4660
4415
Table 4: WikiMatrix: number of extracted sentences for each language pair (in thousands), e.g. Es-Ja=219 corresponds to 219,260 sentences (rounded). The
column tisize,f gives the number of lines in the monolingual texts after deduplication and LID.
Under review as a conference paper at ICLR 2020
Src/Trgar bg bs cs da de el en eo es frfr-ca gl he hr hu id it ja ko mk nb nl pl ptpt-br ro ru sk sl sr sv tr uk vizh-cnzh-tw
ar
bg
bs
Cs
da
de
el
en I
eo
es
fr
fr-ca
gl
he
hr
hu
id
it
ja
ko
mk
nb
nl
Pl
Pt
pt-br
ro
ru
sk
sl
sr
sv
tr
Uk
vi
Zh-Cn
zh-tw
3.0
1.2
1.9
2.0
2.4
4.9
6.1
7.8
8.9
9.7
1.8 3.0
4.2 6.7
4.1
5.3 5.5 4.9
5.416.2 8.1
12.9
5.5 5.0 6.3
5.6 5.6 1.5 2.7 1.2
9.812.4 4.0 6.4 2.7
3.8 6.720.3 4.113.212.2 9.0 5.6 3.5 2.2 2.7 9.2 9.9 4.2
8.510.025.3 7.716.314.711.9 8.4 3.2 6.4 4.710.312.2 4.0
4.5
8.7
3.7
7.1
5.8 4.521.7
4.9
4.7
1.8
4.7
4.9
4.3
5.3
8.2
6.4
2.8
6.6
7.3
9.2
8.7
2.4 4.512.3
2.410.411.7
4.4
7.7
3.8
7.6
4.0
7.1
4.4
7.0
3.012.0 12.2
6.314.7 15.4
2.9 9.9 10.9
8.110.8 12.1
4.814.0 16.2
6.814.2 15.2
9.9 6.7 4.6 5.7 1.0 30.9 2.4 5.3 6.5 1.4
5.4 5.8
7.8 9.0
6.311.2
7.811.3
7.214.4
5.7
6.3
8.5
8.7
4.8 3.110.410.6
1.5
2.6
2.7
3.6
3.4
9.428.1 6.7
6.5
4.4
5.5
5.6
8.3 6.420.010.412.611.4 8.6 5.0
14.0 9.032.9 6.716.716.712.8 7.3
1	7.824.515.917.418.314.7 6.8
4.9 7.8 9.6
4.710.813.4
7.2 8.613.5
5.313.115.3
3.7
4.0
4.9
4.8
5.2 J
8.116.9
5.5 9.7 6.7	27.9
11.923.914.715.5 30.920.427.1	:
4.111.2
1.8 6.4
6.214.3
6.012.7
4.912.5
2.6
3.4
1.6
1.6
4.1
7.3
5.7
6.8
5.0
2.8
4.7
1.8
8.729.9
5.6 2.6
9.1 4.2
5.311.7 5.0
1.4 1.9 0.7
0.9 1.7
2.418.212.0
2.7 8.5
2.4 8.2
1.8 7.4
7.415.2
6.514.7
3.2 9.7
3.312.6
2.9
2.9
7.0
7.4
3.7
4.2
8.018.816.313.510.1
2.5
3.5
4.3
4.0
4.1
4.7
6.4
5.5
6.0 6.233.112.4
6.6 5.611.517.6
6.110.2 9.3 8.7
5.118.0 18.310.4
7.8 3.1
9.2 5.4
8.4 3.1
5.2
8.6
6.4
22.6 35.8 32.625.124.317.318.813.528.829.510.218.621.831.825.112.031.4 37.020.417.413.816.5
7.3 7.413.5 8.123.1	16.117.612.710.5
8.015.712.916.4 33.213.5	25.619.930.1
8.315.614.515.4 31.618.626.4	17.2
7.814.312.915.427.814.023.7	18.1
2.9 7.4
3.7 6.1
7.0 6.5
5.9
5.210.1
5.3
8.523.4
5.4 6.325.7
5.8
7.0
6.624.4
5.516.7
6.711.124.9
9.2 34.416.015.2
4.215.313.610.3
4.412.6 9.9 7.8
6.510.810.9 9.3
8.216.415.111.1
1.9
8.1
6.8
6.8
2.5
1.6
0.7
1.2
5.1
6.2
5.1
5.7
3.9
9.9
3.0
8.7
6.9
6.8
3.5
2.5
4.8 9.213.9
7.716.123.8
7.715.024.6
7.913.723.4
6.913.111.514.5 30.013.926.424.920.019.3
1.8 3.1 2.7 2.5 7.9 2.2 6.0 6.0 4.7 2.3
1.5
2.1
5.1
6.2
1.4
2.8
3.2
4.3
3.6
5.6 5.2
9.819.3
8.911.2
8.210.4
2.5
7.9
7.3
7.5
3.6
4.1
1.8
4.8 6.2 7.212.1
9.911.913.314.1
8.710.713.115.9
8.810.3
4.2 5.6
4.5 4.6
4.014.1
15.1
6.9 6.5
5.4 6.5
5.3 6.1
6.712.4 16.0 6.9 8.6
8.127.6 27.814.711.6
8.124.2 26.016.512.7
7.218.6 23.215.311.3
4.322.4 23.7 7.7 5.9
3.713.7 15.0 6.3 8.0
5.412.1 12.6 7.3 8.3
7.4
6.0
5.8
6.1
1.7
1.9
5.6
8.1
7.1
6.6
3.1
2.6
1.425.8
1.512.7
2.0 7.2
5.529.1
0.6 8.8
2.613.7
2.113.8
3.312.5
0.2 4.3
1.0 5.2
10.317.626.9 18.0 10.7
1.8 5.4 7.7 5.2
5.210.017.8 12.3
4.612.611.7 6.0
4.2
5.0
2.3
1.8
1.9
9.817.0 11.1
9.715.4
3.9 9.4
5.110.7
6.9 9.4
6.6 8.2 4.4 6.2 3.9 4.5 6.0 4.3 9.7 9.9 7.1 5.8 3.4 4.2 1.2 5.3 3.0 4.4 8.5
1.3 2.0 1.7 1.7 8.7 1.3 4.7 4.4 3.4 1.5 0.9 0.7 1.5 3.1 3.2 9.2
5.4 7.2 4.310.323.4 8.915.211.510.0
5.4 32.7 9.8 8.9 35.1 9.517.014.6
5.914.216.1 8.426.513.416.816.713.5
8.2 6.6 6.5 5.415.1 7.511.411.3 8.6
5.4
8.0
7.3
5.2
8.015.513.218.7 35.012.0 32.426.719.023.0
8.616.812.917.6 37.316.031.026.620.323.0
4.012.6
3.1 3.9
3.9 4.8
2.3 4.8
8.810.3
5.1
7.7
2.727.0
7.6 5.5
1.8 7.633.2 5.1
9.4 7.510.425.0
8.5 8.8 8.318.7
4.3 5.7 3.216.9
4.7 7.6 5.817.3
3.7 3.8 5.822.8
6.718.819.314.610.0
9.914.314.511.0
9.3 9.4 8.5 6.7
5.911.4 8.5 6.4
3.211.9 9.1 7.5
2.2 7.4 4.8 5.926.512.6 8.131.811.016.915.710.7
6.C
2.7
3.2
3.5
7.1
8.7
4.0
4.9
1.0
5.8
6.9
4.8
6.5
12.7	5.8	9.1	7.010.0	9.4	5.514.6	16.7	9.8	8.1	3.6	5.6	1.8	9.3	4.6	7.318.5	11.0
7.0	6.714.0	7.3	9.0	9.913.312.8	7.322.8	24.913.310.2	5.4	7.1	2.311.9	4.6	8.815.3	10.7
1.3	2.0	3.5	4.6	∣16.9	1.6 2.6	3.1	1.8 5.1	4.9	2.8	2.7	1.2	1.8	0.5	2.6	1.9	2.2 5.9
1.2 1.3	1.9	1.4 4.3	4.6	2.0	2.1	1.0	1.5	0.4	1.5	1.4	1.5 4.3
9.8
5.7
6.8
5.1
1.211.2
1.130.4
3.3 5.5
3.7 8.911.1
4.5 9.915.0
5.311.413.3
4.0 7.5 8.6
8.417.524.4
8.118.624.8
5.911.015.5
5.6 9.511.7
3.7 4.9 6.9
3.9 6.5 7.8
2.7 6.1 8.2
5.411.613.3
3.5
5.5
5.2
4.2
4.7
5.7
5.9
5.7
7.5
6.7
5.9
5.2 4.1
3.6
6.6
6.6
6.2
4.2
4.6
2.9
4.2
6.2
5.8
6.7 4.513.9 15.6 8.3 7.5 3.5 7.1 3.7 5.6 2.0 6.410.9 6.3 4.3
7.810.613.114.314.9
7.810.712.5	14.8
4.3 6.4 7.3 8.0 8.1
6.1 7.7 7.4 8.0 8.1
2.2
2.7
1.2
4.714.2
9.5 7.5 3.2 3.9 0.626.3 1.9 6.2 7.8
5.313.8 15.4 7.8 7.6 4.1 5.1 1.611.1 3.2 6.110.6 8.0
6.2	9.6 9.9 6.2 9.5 6.2 5.3 1.5 5.6 2.4 8.9 7.7 6.3
3.9 3.5 4.9 5.0
4.2 6.3 4.3 5.5
2.913.7 3.3 3.3
8.9
8.5
5.215.4 17.7
8.912.4 13.9
7.1 7.8 8.5
15.311.7
15.111.8
8.0
4.8
9.9
4.8 6.2 5.225.411.5
3.910.8 11.3
5.815.0 17.4
8.2
3.5
4.8
5.6
7.9
6.6
6.4
3.6
5.8
6.6
8.1
8.9
5.0
5.4
5.0
2.014.3
2.814.6
1.9 7.0
2.7 8.2
1.5 4.3
6.2 9.318.0 12.9
5.310.818.8 13.2
3.3 6.612.7
2.922.511.5
1.6 5.4 5.4
5.9 3.6
7.2 2.8 9.5
8.1 3.8 4.7
1.9 4.1 2.2 4.3 7.4
3.0 1.1 5.7 6.8
1.0
3.2 6.912.9
2.2 3.5 2.0 2.6 3.9 4.1 4.715.9 2.9 9.4 7.7 6.7 3.6 1.6 2.1 3.4 6.7 6.4 4.3 7.0 3.5 3.1 4.2 2.5 9.0 8.4 4.6 4.0 1.8 2.3 0.8 3.5
2.912.3 5.3 7.4 7.5 7.5 8.420.7 6.514.214.111.2 5.5 3.5 6.6 4.7 9.511.2 4.9 5.8 7.2 6.3 6.9 9.6
12.9 7.223.5 4.9 5.7 2.6 6.9 2.6
4.2 7.5 4.0 4.7 8.5 6.0 8.820.2 7.313.713.2 9.9 6.5 4.6 4.9 4.714.710.7 5.6 9.3 6.9 5.7 7.3 4.513.0 14.1 8.5 7.2 3.4 4.6 1.7 8.2 4.0 6.7
2.1 3.2 1.0 2.2 3.8 3.2 4.511.8 3.8 8.2 7.6
3.2 1.7 1.9 3.0 6.6 6.0
2.2 3.1 1.1 2.1 3.7 2.8 3.910.7 3.4 7.5 7.2 6.1 2.8 1.8 1.6 3.0 6.2 5.4
3.4
2.8
3.3 8.2
11.4
3.8 2.2 7.1 7.9 4.1 4.1 1.6 2.4 0.9 3.1 2.3 3.010.8
3.5 2.3 6.3 6.9 3.5 3.9 1.4 2.1 0.9 3.0 2.4 2.910.0
7.8
9.1
2.3
3.8
4.3
7.8
6.7
7.9
9.9
5.1
3.6
5.8
6.7
4.9
5.2
2.5
2.6
3.1
4.8
4.4
4.9
6.7
Table 5: BLEU scores on the TED test set as proposed in (Qi et al., 2018). NMT systems were
trained on bitexts mined in Wikipedia only (with at least twenty-five thousand parallel sentences).
No other resources were used.
usually not the case when translating between other major languages, for example in Asia. Let us
list some interesting language pairs for which we were able to mine more than hundred thousand
sentences: Korean/Japanese (222k), Russian/Japanese (196k), Indonesian/Vietnamese (146k), or
HebreW/Romance languages (120-150k sentences).
Overall, we were able to extract at least ten thousand parallel sentences for 85 different languages.13
For several loW-resource languages, We Were able to extract more parallel sentences With other
languages than English. These include, among others, Aragonse With Spanish, Lombard With Italian,
Breton With several Romance languages, Western Frisian With Dutch, Luxembourgish With German
or Egyptian Arabic and Wu Chinese With the respective major language.
Finally, Cebuano (ceb) falls clearly apart: it has a rather huge Wikipedia (17.9M filtered sentence),
but most of it Was generated by a bot, as for the Waray language14. This certainly explains that only
a very small number of parallel sentences could be extracted. Although the same bot Was also used
to generate articles in the SWedish Wikipedia, our alignments seem to be better for that language.
5.2	Qualitative evaluation
Aiming to perform a large-scale assessment of the quality of the extracted parallel sentences, We
trained NMT systems on the extracted parallel sentences. We identified a publicly available data
set Which provide test sets for many language pairs: translations of TED talks as proposed in the
context of a study on pretrained Word embeddings for NMT15 (Qi et al., 2018). We Would like to
emphasize that we did not use the training data provided by TED - we only trained on the mined
sentences from Wikipedia. The goal of this study is not to build state-of-the-art NMT system for for
the TED task, but to get an estimate of the quality of our extracted data, for many language pairs. In
1399 languages have more than 5,000 parallel sentences.
14https://en.wikipedia.org/wiki/Lsjbot
15https://github.com/neulab/word-embeddings-for-nmt
9
Under review as a conference paper at ICLR 2020
particular, there may be a mismatch in the topic and language style between Wikipedia texts and the
transcribed and translated TED talks.
For training NMT systems, we used a transformer model from fairseq (Ott et al., 2019) with the
parameter settings shown in Figure 2 in the appendix. For preprocessing, the text was tokenized
using the Moses tokenizer (without true casing) and a 5000 subword vocabulary was learnt using
SentencePiece (Kudo & Richardson, 2018). Decoding was done with beam size 5 and length nor-
malization 1.2.
We evaluate the trained translation systems on the TED dataset (Qi et al., 2018). The TED data con-
sists of parallel TED talk transcripts in multiple languages, and it provides development and test sets
for 50 languages. Since the development and test sets were already tokenized, we first detokenize
them using Moses. We trained NMT systems for all possible language pairs with more than twenty-
five thousand mined sentences. This gives us in total 1886 language pairs in 45 languages. We train
L1 → L2 and L2 → L1 with the same mined bitexts L1/L2. Scores on the test sets were computed
with SacreBLEU (Post, 2018). Table 5 summarizes all the results. Due to space constraints, we are
unable to report BLEU score for all language combinations in that table. Some additional results are
reported in Table 7 in the annex. 23 NMT systems achieve BLEU scores over 30, the best one being
37.3 for Brazilian Portuguese to English. Several results are worth mentioning, like Farsi/English:
16.7, Hebrew/English: 25.7, Indonesian/English: 24.9 or English/Hindi: 25.7 We also achieve inter-
esting results for translation between various non English language pairs for which it is usually not
easy to find parallel data, e.g. Norwegian 什 Danish ≈33, Norwegian 什 Swedish ≈25, Indonesian
什 Vietnamese ≈16 or Japanese / Korean ≈17.
Our results on the TED set give an indication on the quality of the mined parallel sentences. These
BLEU scores should be of course appreciated in context of the sizes of the mined corpora as given
in Table 4. Obviously, we can not exclude that the provided data contains some wrong alignments
even though the margin is large. Finally, we would like to point out that we run our approach on all
available languages in Wikipedia, independently of the quality of LASER’s sentence embeddings
for each one.
6	Conclusion
We have presented an approach to systematically mine for parallel sentences in the textual content
of Wikipedia, for all possible language pairs. We use a recently proposed mining approach based
on massively multilingual sentence embeddings (Artetxe & Schwenk, 2018a) and a margin criterion
(Artetxe & Schwenk, 2018b). The same approach is used for all language pairs without the need
of a language specific optimization. In total, we make available 135M parallel sentences in 85
languages, out of which only 34M sentences are aligned with English. We were able to mine more
than ten thousands sentences for 1620 different language pairs. This corpus of parallel sentences is
freely available.16 We also performed a large scale evaluation of the quality of the mined sentences
by training 1886 NMT systems and evaluating them on the 45 languages of the TED corpus (Qi
et al., 2018).
This work opens several directions for future research. The mined texts could be used to first re-
train LASER’s multilingual sentence embeddings with the hope to improve the performance on
low-resource languages, and then to rerun mining in Wikipedia. This process could be iteratively
repeated. We also plan to apply the same methodology to other large multilingual collections. The
monolingual texts made available by ParaCrawl or CommonCrawl17 are good candidates.
We expect that the WikiMatrix corpus has mostly well-formed sentences and it should not contain
social media language. The mined parallel sentences are not limited to specific topics like many of
the currently available resources (parliament proceedings, subtitles, software documentation, . . .),
but are expected to cover many topics of Wikipedia. The fraction of unedited machine translated
text is also expected to be low. We hope that this resource will be useful to support research in
multilinguality, in particular machine translation.
16Anonymized for review
17http://commoncrawl.org/
10
Under review as a conference paper at ICLR 2020
References
Sadaf Abdul-Rauf and Holger Schwenk. On the Use of Comparable Corpora to Improve SMT
performance. In EACL, pp. 16-23,2009. URL http://www.aclweb.org/anthology/
E09-1003.
Sisay Fissaha Adafre and Maarten de Rijke. Finding similar sentences across multiple languages in
Wikipedia. In Proceedings of the Workshop on NEW TEXT Wikis and blogs and other dynamic
text sources, 2006.
Ahmad Aghaebrahimian. Deep neural networks at the service of multilingual parallel sentence
extraction. In Coling, 2018.
Mikel Artetxe and Holger Schwenk. Massively multilingual sentence embeddings for zero-shot
cross-lingual transfer and beyond. In https://arxiv.org/abs/1812.10464, 2018a.
Mikel Artetxe and Holger Schwenk. Margin-based Parallel Corpus Mining with Multilingual Sen-
tence Embeddings. https://arxiv.org/abs/1811.01136, 2018b.
Andoni Azpeitia, Thierry Etchegoyhen, and Eva Martlnez Garcia. Weighted Set-Theoretic Align-
ment of Comparable Sentences. In BUCC, pp. 41-45, 2017. URL http://aclweb.org/
anthology/W17-2508.
Andoni Azpeitia, Thierry Etchegoyhen, and Eva Martlnez Garcia. Extracting Parallel Sentences
from Comparable Corpora with STACC Variants. In BUCC, may 2018.
Houda Bouamor and Hassan Sajjad. H2@BUCC18: Parallel Sentence Extraction from Comparable
Corpora Using Multilingual Sentence Embeddings. In BUCC, may 2018.
Christian Buck and Philipp Koehn. Findings of the wmt 2016 bilingual document alignment shared
task. In Proceedings of the First Conference on Machine Translation, pp. 554-563, Berlin, Ger-
many, August 2016. Association for Computational Linguistics. URL http://www.aclweb.
org/anthology/W/W16/W16-2347.
Vishrav Chaudhary, Yuqing Tang, Francisco Guzman, Holger Schwenk, and Philipp Koehn. LoW-
resource corpus filtering using multilingual sentence embeddings. In Proceedings of the Fourth
Conference on Machine Translation (WMT), 2019.
Cristina Espafia-Bonet, Adam Csaba Varga, Alberto Barron-Cedeno, and Josef van Genabith. An
Empirical Analysis of NMT-Derived Interlingual Embeddings and their Use in Parallel Sentence
Identification. IEEE Journal of Selected Topics in Signal Processing, pp. 1340-1348, 2017.
Thierry Etchegoyhen and Andoni Azpeitia. Set-Theoretic Alignment for Comparable Corpora. In
ACL, pp. 2009-2018, 2016. doi: 10.18653/v1/P16-1189. URL http://www.aclweb.org/
anthology/P16-1189.
Simon Gottschalk and Elena Demidova. Multiwiki: Interlingual text passage alignment in
Wikipedia. ACM Transactions on the Web (TWEB), 11(1):6, 2017.
Francis GregOire and Philippe Langlais. BUCC 2017 Shared Task: a First Attempt Toward a Deep
Learning Framework for Identifying Parallel Sentences in Comparable Corpora. In BUCC, pp.
46-50, 2017. URL http://aclweb.org/anthology/W17-2509.
Mandy Guo, Qinlan Shen, Yinfei Yang, Heming Ge, Daniel Cer, Gustavo Hernandez Abrego, Keith
Stevens, Noah Constant, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Effective Parallel
Corpus Mining using Bilingual Sentence Embeddings. arXiv:1807.11906, 2018.
Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federmann,
Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis, Mu Li, Shujie Liu, Tie-Yan Liu,
Renqian Luo, Arul Menezes, Tao Qin, Frank Seide, Xu Tan, Fei Tian, Lijun Wu, Shuangzhi
Wu, Yingce Xia, Dongdong Zhang, Zhirui Zhang, and Ming Zhou. Achieving Human Parity on
Automatic Chinese to English News Translation. arXiv:1803.05567, 2018.
11
Under review as a conference paper at ICLR 2020
Jeff Johnson, Matthijs Douze, and Herve Jegou. Billion-scale similarity search with GPUs. arXiv
preprint arXiv:1702.08734, 2017.
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient
text classification. https://arxiv.org/abs/1607.01759, 2016.
H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE Trans.
PAMI,33(1):117-128, 2011.
Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In MT summit, 2005.
Philipp Koehn, Huda Khayrallah, Kenneth Heafield, and Mikel L. Forcada. Findings of the wmt
2018 shared task on parallel corpus filtering. In Proceedings of the Third Conference on Machine
Translation: Shared Task Papers, pp. 726-739, Belgium, Brussels, October 2018. Association for
Computational Linguistics. URL https://www.aclweb.org/anthology/W18- 6453.
Philipp Koehn, Francisco Guzman, Vishrav Chaudhary, and Juan M. Pino. Findings of the Wmt 2019
shared task on parallel corpus filtering for low-resource conditions. In Proceedings of the Fourth
Conference on Machine Translation, Volume 2: Shared Task Papers, Florence, Italy, August 2019.
Association for Computational Linguistics.
Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing: System Demonstrations, pp. 66-71, Brussels,
Belgium, 2018. Association for Computational Linguistics. URL https://www.aclweb.
org/anthology/D18-2012.
P. Lison and J. Tiedemann. Opensubtitles2016: Extracting large parallel corpora from movie and tv
subtitles. In LREC, 2016.
Mehdi Zadeh Mohammadi and Nasser GhasemAghaee. Building bilingual parallel corpora based on
Wikipedia. In 2010 Second International Conference on Computer Engineering and Applications,
pp. 264-268, 2010.
Dragos Stefan Munteanu and Daniel Marcu. Improving Machine Translation Performance by
Exploiting Non-Parallel Corpora. Computational Linguistics, 31(4):477-504, 2005. URL
http://www.aclweb.org/anthology/J05-4003.
P Otero, I Lopez, S Cilenis, and Santiago de Compostela. Measuring comparability of multilingual
corpora extracted from Wikipedia. Iberian Cross-Language Natural Language Processings Tasks
(ICL), pp. 8, 2011.
Pablo Gamallo Otero and Isaac Gonzalez Lopez. Wikipedia as multilingual source of comparable
corpora. In Proceedings of the 3rd Workshop on Building and Using Comparable Corpora, LREC,
pp. 21-25, 2010.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the
2019 Conference of the North American Chapter of the Association for Computational Linguistics
(Demonstrations), pp. 48-53, Minneapolis, Minnesota, June 2019. Association for Computational
Linguistics. URL https://www.aclweb.org/anthology/N19-4009.
Alexandre Patry and Philippe Langlais. Identifying parallel documents from a large bilingual col-
lection of texts: Application to parallel article extraction in Wikipedia. In Proceedings of the 4th
Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web, pp.
87-95. Association for Computational Linguistics, 2011.
Matt Post. A call for clarity in reporting bleu scores. In Proceedings of the Third Conference
on Machine Translation: Research Papers, pp. 186-191, Belgium, Brussels, October 2018. As-
sociation for Computational Linguistics. URL https://www.aclweb.org/anthology/
W18-6319.
12
Under review as a conference paper at ICLR 2020
Ye Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. When
and why are pre-trained word embeddings useful for neural machine translation? In Pro-
ceedings of the 2018 Conference of the North American Chapter of the Association for Com-
Putational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 529-
535, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. URL
http://www.aclweb.org/anthology/N18-2084.
Philip Resnik. Mining the Web for Bilingual Text. In ACL, 1999. URL http://www.aclweb.
org/anthology/P99-1068.
Philip Resnik and Noah A. Smith. The Web as a Parallel Corpus. Computational Linguistics, 29(3):
349-380, 2003. URL http://www.aclweb.org/anthology/J03-3002.
Holger Schwenk. Filtering and mining parallel data in a joint multilingual space. In ACL, pp.
228-234, 2018.
Jason R. Smith, Chris Quirk, and Kristina Toutanova. Extracting parallel sentences from comparable
corpora using document level alignment. In NAACL, pp. 403-411, 2010.
J. Tiedemann. Parallel data, tools and interfaces in OPUS. In LREC, 2012.
Chen-Tse Tsai and Dan Roth. Cross-lingual wikification using multilingual embeddings. In Pro-
ceedings of the 2016 Conference of the North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, pp. 589-598, 2016.
Dan Tufis, Radu Ion, Stefan Daniel, Dumitrescu, and Dan Stefanescu. Wikipedia as an Smt training
corpus. In RANLP, pp. 702-709, 2013.
Masao Utiyama and Hitoshi Isahara. Reliable Measures for Aligning Japanese-English News
Articles and Sentences. In ACL, 2003. URL http://www.aclweb.org/anthology/
P03-1010.
Yinfei Yang, Gustavo Hernandez Abrego, Steve Yuan, Mandy Guo, Qinlan Shen, Daniel Cer, YUn-
Hsuan Sung, Brian Strope, and Ray Kurzweil. Improving multilingual sentence embedding using
bi-directional dual encoder with additive margin softmax. In https://arxiv.org/abs/
1902.08564, 2019.
MichaI Ziemski, Marcin Junczys-Dowmunt, and Bruno Pouliquen. The United Nations Parallel
Corpus v1.0. In LREC, may 2016.
13
Under review as a conference paper at ICLR 2020
A Appendix
Table 6 provides the amounts of mined parallel sentences for languages which have a rather small
Wikipedia. Aligning those languages obviously yields to a very small amount of parallel sentences.
Therefore, we only provide these results for alignment with high resource languages. It is also likely
that several of these alignments are of low quality since the LASER embeddings were not directly
trained on most these languages, but we still hope to achieve reasonable results since other languages
of the same family may be covered.
ISO	Name	Language Family	size	ca da de en	es fr	it nl	pl pt	sv	ru zh total
an	Aragonese	Romance	222	24 7 12 23	33 16	13 9	10 14	9	11 6 324
arz	Egyptian	Arabic	120	7 6 11 18	12 12	10 8	9 10	8	12 7 278
	Arabic								
as	Assamese	Indo-Aryan	124	8 6 11 7	11 12	10 9	9 8	8	9 3 216
azb	South Azer- Turkic		398	6 4 9 8	9 10	9 7	6 8	6	7 3 172
	baijani								
bar	Bavarian	Germanic	214	7 6 41 16	12 12	10 8	9 10	8	10 5 261
bpy	Bishnupriya Indo-Aryan		128	2 14 4	3 4	2 2	3 2	2	3 1 71
br	Breton	Celtic	413	20 16 22 23 22			19		16 6 200
ce	Chechen	Northeast	315	2 12 2	2 2	2 2	2 2	2	2 1 56
		Caucasian							
ceb	Cebuano	Malayo-	17919	14 9 22 29 27 24 24 15			17 20	55	21 9 594
		Polynesian							
ckb	Central Kur-	Iranian	127	2 2 6 8	5 5	4 4	4 4	3	6 4 113
	dish								
cv	Chuvash	Turkic	198	4 3 5 4	6 6	7 5	4 6	5	8 2 129
dv	Maldivian	Indo-Aryan	52	2 2 5 6	4 4	3 3	3 3	3	5 3 96
fo	Faroese	Germanic	114	13 12 14 32	21 18	15 11	11 17	12	13 6 335
fy	Western	Germanic	493	13 8 16 32	21 18	17 38	12 18	13	14 5 453
	Frisian								
gd	Gaelic	Celtic	66	1111	1 1	1 1	1 1	1	1 1	41
ga	Irish	Irish	216	2 3 4	3 3	3 2	2 3	2	3 1 70
gom	Goan	Indo-Aryan	69	9 7 10 8	13 13	13 9	9 11	9	10 4 240
	Konkami								
ht	Haitian Cre- Creole		60	2 13 4	3 4	3 2	3 2	2	3 1 72
	ole								
ilo	Iloko	Philippine	63	3 2 4 5	4 4	4 3	3 4	3	4 2 96
io	Ido	constructed	153	5 3 6 11	7 7	5 5	5 6	5	5 3 143
jv	Javanese	Malayo-	220	8 5 8 13	12 10	11 8	7 11	8	8 3 219
		Polynesian							
ka	Georgian	Kartvelian	480	11 7 15 12	16 17	16 12	11 14	12	13 5 288
ku	Kurdish	Iranian	165	5 4 8 5	8 7	8 7	6 7	6	6 3 222
la	Latin	Romance	558	12 9 17 32	20 18	17 12	13 18	13	14 6 478
lb	LuxembourgiGshermanic		372	12 7 26 22	19 18	15 11	11 16	12	11 4 305
lmo	Lombard	Romance	147	6 3 7 10	7 7	11 6	5 7	5	5 3 144
mg	Malagasy	Malayo-	263	6 5 9 13	9 12	8 7	7 7	8	7 4 199
		Polynesian							
mhr	Eastern	Uralic	61	3 2 4 3	4 4	5 3	3 4	3	4 2 96
	Mari								
min	MinangkabauMalayo-		255	4 2 6 7	5 5	5 4	4 4	5	5 2 121
		Polynesian							
mn	Mongolian	Mongolic	255	4 3 7 5	6 6	7 6	5 5	5	5 3 197
mwl	Mirandese	Romance	64	6 3 4 10	8 6	5 3	4 34	3	4 2 154
nds nl Low Ger-		Germanic	65	5 4 6 10	7 7	6 15	5 6	5	5 3 151
	man/Saxon								
ps	Pashto	Iranian	89	2 3 2	3 3	3 3	3 3	3	3 1 73
rm	Romansh	Italic	57	2 2 10 5	4 4	3 2	3 3	3	3 1	86
sah	Yakut	Turkic/Sib	134	4 3 7 5	6 6	6 5	5 5	5	6 3 134
scn	Sicilian	Romance	81	5 3 6 9	7 7	11 5	5 6	5	5 2 143
sd	Sindhi	Iranian	115	3 9	8 8	7 7	6 7	5	8 5 152
su	Sundanese	Malayo-	120	4 3 5 7	6 5	6 4	4 5	4	4 2 117
		Polynesian							
tk	Turkmen	Turkic	56	2 2 3 3	4 3	4 2	2 4	2	3 1 76
tg	Tajik	Iranian	248	5 4 11 15	9 9	8 8	7 8	6	10 6 192
ug	Uighur	Turkic	83	4 3 9 10	7 8	6 6	5 6	5	9 6 168
ur	Urdu	Indo-Aryan	150	2 2 3 5	3 3	3 3	3 3	3	3 2 123
wa	Walloon	Romance	56	3 2 4 5	5 4	4 3	3 4	3	3 2 93
wuu	Wu Chinese Chinese		75	8 6 11 17	12 11	10 8	9 11	9	10 43 283
yi	Yiddish	Germanic	131	3 2 4 3	4 4	5 3	3 4	3	4 1	92
Table 6: WikiMatrix (part 2): number of extracted sentences (in thousands) for languages with
a rather small Wikipedia. Alignments with other languages yield less than 5k sentences and are
omitted for clairty.
14
Under review as a conference paper at ICLR 2020
Table 2 gives the detailed configuration which was used to train NMT models on the mined data in
Section 5.
——arch transformer
——share-all-embeddings
——encoder-layers 5
——decoder-layers 5
——encoder-embed-dim 512
——decoder-embed-dim 512
——encoder-ffn-embed-dim 2048
——decoder-ffn-embed-dim 2048
——encoder-attention-heads 2
-	-decoder-attention-heads 2
--encoder-normalize-before
--decoder-normalize-before
-	-dropout 0.4
-	-attention-dropout 0.2
-	-relu-dropout 0.2
-	-weight-decay 0.0001
--label-smoothing 0.2
--criterion label_SmOOthed-Cross_entropy
--optimizer adam
--adam-betas '(0.9, 0.98)’
-	-clip-norm 0
-	-lr-scheduler inverSe.Sqrt
-	-warmup-update 4000
-	-warmup-init-lr 1e-7
-	-lr 1e-3 --min-lr 1e-9
-	-max-tokens 4000
-	-update-freq 4
-	-max-epoch 100
-	-save-interval 10
Figure 2: Model settings for NMT training with fairseq
Finally, Table 7 gives the BLEU scores on the TED corpus when translating into and from English
for some additional languages.
Lang	Xx → en	en → xx
et	15.9	14.3
eu	10.1	7.6
fa	16.7	8.8
fi	10.9	10.9
lt	13.7	10.0
hi	17.8	21.9
mr	2.6	3.5
Table 7: BLEU scores on the TED test set as proposed in (Qi et al., 2018). NMT systems were
trained on bitexts mined in Wikipedia only. No other resources were used.
15