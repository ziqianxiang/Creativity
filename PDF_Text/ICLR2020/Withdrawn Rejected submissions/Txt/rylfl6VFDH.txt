Under review as a conference paper at ICLR 2020
Adaptive Network Sparsification with Depen-
dent Variational Beta-Bernoulli Dropout
Anonymous authors
Paper under double-blind review
Ab stract
While variational dropout approaches have been shown to be effective for network
sparsification, they are still suboptimal in the sense that they set the dropout rate for
each neuron without consideration of the input data. With such input-independent
dropout, each neuron is evolved to be generic across inputs, which makes it difficult
to sparsify networks without accuracy loss. To overcome this limitation, we propose
adaptive variational dropout whose probabilities are drawn from sparsity-inducing
beta-Bernoulli prior. It allows each neuron to be evolved either to be generic or
specific for certain inputs, or dropped altogether. Such input-adaptive sparsity-
inducing dropout allows the resulting network to tolerate larger degree of sparsity
without losing its expressive power by removing redundancies among features.
We validate our dependent variational beta-Bernoulli dropout on multiple public
datasets, on which it obtains significantly more compact networks than baseline
methods, with consistent accuracy improvements over the base networks.
1	Introduction
One of the main obstacles in applying deep learning to large-scale problems and low-power com-
puting systems is the large number of network parameters, as it can lead to excessive memory and
computational overheads. To tackle this problem, researchers have explored network sparsification
methods to remove unnecessary connections in a network, which is implementable either by weight
pruning (Han et al., 2016) or sparsity-inducing regularizations (Wen et al., 2016).
Recently, variational Bayesian approaches have shown to be useful for network sparsification,
outperforming non-Bayesian counterparts. They take a completely different approach from the
conventional methods that either uses thresholding or sparsity-inducing norms on parameters, and
uses well-known dropout regularization instead. Specifically, these approaches use variational
dropout (Kingma et al., 2015) which adds in multiplicative stochastic noise to each neuron, as a
means of obtaining sparse neural networks. Removal of unnecessary neurons could be done by either
setting the dropout rate individually for each neuron with unbounded dropout rate (Molchanov et al.,
2017) or by pruning based on the signal-to-noise ratio (Neklyudov et al., 2017).
While these variational dropout approaches do yield compact networks, they are suboptimal in that
the dropout rate for each neuron is learned completely independently of the given input data and
labels. With input-independent dropout regularization, each neuron has no choice but to encode
generic information for all possible inputs, since it does not know what input and tasks it will be given
at evaluation time, as each neuron will be retained with fixed rate regardless of the input. Obtaining
high degree of sparsity in such as setting will be difficult as dropping any of the neurons will result in
information loss. For maximal utilization of the network capacity and thus to obtain a more compact
model, however, each neuron should be either irreplaceably generic and used by all tasks, or highly
specialized for a task such that there exists minimal redundancy among the learned representations.
This goal can be achieved by adaptively setting the dropout probability for each input, such that some
of the neurons are retained with high probability only for certain types of inputs and tasks.
To this end, we propose a novel input-dependent variational dropout regularization for network
sparsification. We first propose beta-Bernoulli dropout that learns to set dropout rate for each
individual neuron, by generating the dropout mask from beta-Bernoulli prior, and show how to train
it using variational inference. This dropout regularization is a proper way of obtaining a Bayesian
neural network and also sparsifies the network, since beta-Bernoulli distribution is a sparsity-inducing
1
Under review as a conference paper at ICLR 2020
prior. Then, we propose dependent beta-Bernoulli dropout, which is an input-dependent version of
our variational dropout regularization.
Such adaptive regularization has been utilized for general network regularization by a non-Bayesian
and non-sparsity-inducing model (Ba & Frey, 2013); yet, the increased memory and computational
overheads that come from learning additional weights for dropout mask generation made it less
appealing for generic network regularization. In our case of network sparsification, however, the over-
heads at training time is more than rewarded by the reduced memory and computational requirements
at evaluation time, thanks to the high degree of sparsification obtained in the final output model.
We validate our dependent beta-Bernoulli variational dropout regularizer on multiple public datasets
for network sparsification performance and prediction error, on which it obtains more compact
network with substantially reduced prediction errors, when compared with both the base network and
existing network sparsification methods. Further analysis of the learned dropout probability for each
unit reveals that our input-adaptive variational dropout approach generates a clearly distinguishable
dropout mask for each task, thus enables each task to utilize different sets of neurons for their
specialization.
Our contribution in this paper is threefold:
•	We propose beta-Bernoulli dropout, a novel dropout regularizer which learns to generate
Bernoulli dropout mask for each neuron with sparsity-inducing prior, that obtains high
degree of sparsity without accuracy loss.
•	We further propose dependent beta-Bernoulli dropout, which yields significantly more
compact network than input-independent beta-Bernoulli dropout, and further perform run-
time pruning for even less computational cost.
•	Our beta-Bernoulli dropout regularizations provide novel ways to implement a sparse
Bayesian Neural Network, and we provide a variational inference framework for learning it.
2	Related Work
Deep neural networks are prone to overfitting, due to its large number of parameters. Dropout (Srivas-
tava et al., 2014) is an effective regularization that helps prevent overfitting by reducing coadaptations
of the units in the networks. During dropout training, the hidden units in the networks are ran-
domly dropped with fixed probability p, which is equivalent to multiplying the Bernoulli noises
Z 〜Ber(1 - P) to the units. It was later found that multiplying Gaussian noises with the same mean
and variance, Z 〜 N(1, I-Ep), works just as well or even better (Srivastava et al., 2014).
Dropout generally treat the dropout rate p as a hyperparameter to be tuned, but there have been several
studies that aim to automatically determine proper dropout rate. Kingma et al. (2015) propose to
determine the variance of the Gaussian dropout by stochastic gradient variational Bayes. Generalized
dropout (Srinivas & Babu, 2016) places a beta prior on the dropout rate and learn the posterior of the
dropout rate through variational Bayes. They showed that by adjusting the hyperparameters of the
beta prior, we can obtain several regularization algorithms with different characteristics. Our beta-
Bernoulli dropout is similar to one of its special cases, but while they obtain the dropout estimates
via point-estimates and compute the gradients of the binary random variables with biased heuristics,
we approximate the posterior distribution of the dropout rate using continuous relaxation.
Ba & Frey (2013) proposed adaptive dropout (StandOut), where the dropout rates for each individual
neurons are determined as function of inputs. This idea is similar in spirit to our dependent beta-
Bernoulli dropout, but they use heuristics to model this function, while we use proper variational
Bayesian approach to obtain the dropout rates. One drawback of their model is the increased memory
and computational cost from additional parameters introduced for dropout mask generation, which is
not negligible when the network is large. Our model also requires additional parameters, but with our
model the increased cost at training time is rewarded at evaluation time, as it yields a significantly
sparse network than the baseline model as an effect of the sparsity-inducing prior.
Recently, there has been growing interest in structure learning or sparsification of deep neural
networks. Han et al. (2016) proposed a strategy to iteratively prune weak network weights for
efficient computations, and Wen et al. (2016) proposed a group sparsity learning algorithm to drop
neurons, filters or even residual blocks in deep neural networks. In Bayesian learning, various sparsity
2
Under review as a conference paper at ICLR 2020
inducing priors have been demonstrated to efficiently prune network weights with little drop in
accuracies (Molchanov et al., 2017; Louizos et al., 2017; Neklyudov et al., 2017; Louizos et al.,
2018; Dai et al., 2018). In the nonparametric Bayesian perspective, Feng et al. Feng & Darrell (2015)
proposed IBP based algorithm that learns proper number of channels in convolutional neural networks
using the asymptotic small-variance limit approximation of the IBP. While our dropout regularizer
is motivated by IBP as with this work, our work is differentiated from it by the input-adaptive
adjustments of dropout rates that allow each neuron to specialize into features specific for some
subsets of tasks.
3	Backgrounds
3.1	Bayesian Neural Networks and Stochastic Gradient Variational Bayes
Suppose that we are given a neural network f(x; W) parametrized by W, a training set D =
{(xn, yn)}nN=1, and a likelihood p(y|f (x; W)) chosen according to the problem of interest. In
Bayesian neural networks, the parameter W is treated as a random variable drawn from a pre-
specified prior distribution p(W), and the goal is to compute the posterior distribution p(W|D):
N
p(W∣D) (X P(W) Y p(yn∣f(Xn； W)).	(1)
n=1
When a novel input x* is given, the prediction y * is obtained as a distribution,
p(y*∣x*,D) = ∕p(y*∣f (x*; W))P(W∣D)dW.	⑵
Unfortunately, p(W|D) is in general computationally intractable due to computing p(D), and thus
we resort to approximate inference schemes. Specifically, we use variational Bayes (VB), where we
posit a variational distribution q(W； φ) of known parametric form and minimize the KL-divergence
between it and the true posterior p(W∣D). It turns out that minimizing DKL [q(W； φ) ∣∣p(W∣D)] is
equivalent to maximizing the evidence lower-bound (ELBO),
N
L(φ) =XEq[logP(yn|f(xn；W))] - DKL[q(W； φ)∣P(W)],	(3)
n=1
where the first term measures the expected log-likelihood of the dataset w.r.t. q(W； φ), and the
second term regularizes q(W； φ) so that it does not deviate too much from the prior. The pa-
rameter φ is learned by gradient descent, but these involves two challenges. First, the expected
likelihood is intractable in many cases, and so is its gradient. To resolve this, we assume that
q(W； φ) is reparametrizable, so that we can obtain i.i.d. samples from q(W； φ) by computing
differentiable transformation of i.i.d. noise (Kingma & Welling, 2014; Rezende et al., 2014) as
ε(S)〜r(ε), W(S) = T(ε(s); φ). Then We can obtain a low-variance unbiased estimator of the gra-
dient of the ELBO. The second challenge is that the number of training instances N may be too large,
which makes it impossible to compute the summation of all expected log-likelihood terms. Regarding
on this challenge, we employ the stochastic gradient descent technique where we approximate with
the summation over a uniformly sampled mini-batch B .
Combining the reparametrization and the mini-batch sampling, we obtain an unbiased estimator
of VφL(φ) to update φ. This procedure, often referred to as stochastic gradient variational Bayes
(SGVB) (Kingma & Welling, 2014), is guaranteed to converge to local optima under proper learning-
rate scheduling.
3.2	Latent feature models and Indian Buffet Processes
In latent feature model, data are assumed to be generated as combinations of latent features:
dn=f(Wzn)=fkXK=1zn,kwk,	(4)
where zn,k = 1 means that dn possesses the k-th feature wk, and f is an arbitrary function.
3
Under review as a conference paper at ICLR 2020
The Indian Buffet Process (IBP) (Griffiths & Ghahramani, 2005) is a generative process of binary
matrices with infinite number of columns. Given N data points, IBP generates a binary matrix
Z ∈ {0, 1}N×K whose n-th row encodes the feature indicator zn>. The IBP is suitable to use as a
prior process in latent feature models, since it generates possibly infinite number of columns and
adaptively adjust the number of features on given dataset. Hence, with an IBP prior we need not
specify the number of features in advance.
One interesting observation is that while it is a marginal of the beta-Bernoulli processes (Thibaux &
Jordan, 2007), the IBP may also be understood as a limit of the finite-dimensional beta-Bernoulli
process. More specifically, the IBP with parameter α > 0 can be obtained as
∏k 〜beta(α∕K, 1), z%k 〜Ber(∏k),	K →∞.	(5)
This beta-Bernoulli process naturally induces sparsity in the latent feature allocation matrix Z. As
K → ∞, the expected number of nonzero entries in Z converges to Nα (Griffiths & Ghahramani,
2005), where α is a hyperparameter to control the overall sparsity level of Z.
In this paper, we relate the latent feature models (4) to neural networks with dropout mask. Specif-
ically, the binary random variables zn,k correspond to the dropout indicator, and the features w
correspond to the inputs or units in neural networks. From this connection, we can think of a hierar-
chical Bayesian model with IBP or finite-dimensional beta-Bernoulli priors . We expect that due to
the property of the IBP, the resulting neural network would also be sparse.
3.3	Dependent Indian Buffet Processes
One important assumption in the IBP is that features are exchangeable - the distribution is invariant
to the permutation of feature assignments. This restricts flexibility when we want to model the
dependency of feature assignments to the input covariates x, such as times or spatial locations. To this
end, Williamson et al. (2010) proposed dependent Indian Buffet processes (dIBP), which triggered a
line of follow-up work (Zhou et al., 2011; Ren et al., 2011). These models can be summarized as
following generative process:
π 〜p(π) z∣π, X 〜 Ber(g(π, x)),	(6)
where g(∙, ∙) is an arbitrary function that maps π and X to a probability. In our latent feature
interpretation for neural network layers above, the input covariates X corresponds to the input or
activations in the previous layer. In other words, we build a data-dependent dropout model where the
dropout rates depend on the inputs.
4	Main contribution
4.1	Variational B eta-B ernoulli dropout
Inspired by the latent-feature model interpretation of layers in neural networks, we propose a Bayesian
neural network layer overlaid with binary random masks sampled from the finite-dimensional beta-
Bernoulli prior. Specifically, letWbe a parameter of a neural network layer, and let zn ∈ {0, 1}K be a
binary mask vector to be applied for the Xn. The dimension ofW needs not be equal to K. Instead, we
may enforce arbitrary group sparsity by sharing the binary masks among multiple elements of W. For
instance, letW ∈ RK×L×M be a parameter tensor in a convolutional neural network with K channels.
K
To enforce a channel-wise sparsity, we introduce zn ∈ {0, 1}K to produce masked parameter Wn for
the n-th observation is given as {zn,kWk,',m | (k,', m) = (1,1,1),..., (K, L, M)}. With a slight
abuse of notation, We denote this binary mask multiplication as Wn = Z 0 W. The generative
process of our Bayesian neural network is then described as
KK
W 〜N(0,λI), π 〜 Y beta(∏k; α∕K, 1), Zn∣π 〜 Y Ber(zn,k; ∏k), Wn = Zn Z W.	(7)
k=1	k=1
4
Under review as a conference paper at ICLR 2020
To approximate the posterior p(W, Z, π∣D), We introduce variational distributions as1
K	NK
q(W, Z, π∣X) = SW(W) Y q(∏k) Y Y q(zn,k∣∏k).	(8)
k=1	n=1 k=1
For W, We conduct computationally efficient point-estimate to get the single value Wc . For π,
folloWing Nalisnick & Smyth (2017), We use the KumarasWamy distribution (KumarasWamy, 1980),
q(πk; ak, bk) = akbkπkak-1 (1 - πkak )bk-1,	(9)
since it closely resembles the beta distribution and easily reparametrizable as
1	1
∏k(u; ak,bk) = (1 - Ubk) ak, U 〜unif([0, 1]).	(10)
We further assume that q(zn k∖∏k) = P(Zn k∖∏k) = Ber(∏k). Zk is reparametrized as in Maddison
et al. (2017); Jang et al. (2017); Gal et al. (2017),
where T is a temperature of continuous relaxation, U 〜unif([0,1]), and Sgm(X) = ɪ+三.The
KL-divergence betWeen the prior and the variational distribution is then obtained as folloWs (Nalisnick
& Smyth, 2017):
DKL[q(Z,π)kpZn)I=X(°ak~:K(—γ—ψ(bk)—bk)+logOK- bk-1 )，	(12)
where Y is Euler-Mascheroni constant and Ψ(∙) is the digamma function. Note that the infinite series
in the KL-divergence vanishes because of the choice p(∏) = beta(∏k; α∕K, 1).
We can apply the SGVB framework described in Section 3.1 to optimize the variational parameters
{ak, bk}K=ι. After the training, the prediction for a novel input x* is given as
p(y*∖x*,D, W) ≈ Eq(z*,∏)[p(y*∖f(x*;z* 乳 W))],	(13)
and we found that the following naive approximation works well in practice,
,	.	_	__ ,	，一，	_	.	r	.
p(y*∖x*, D, W) ≈ p(y*∖f(x*; Eq[z*] 0 W)),	(14)
where
EJz* k] = E0(πt)[πk],	E0(πt)[πk] = bkr(1 + a-i)r(bk).	(15)
q *,k	q(πk) k ,	q (πk) k	-1	.
Γ(1 + ak + bk )
4.2 Variational Dependent Beta-Bernoulli Dropout
We define a Bayesian neural network model with input-dependent beta-Bernoulli prior as follows:
KK
W 〜N(0,λI), π 〜ɪɪ beta(∏k; α∕K, 1), Zn∖π, Xn 〜ɪɪ Ber(Zn,k； ψk(xn,k)),	(16)
k=1	k=1
Here, xn is the input to the dropout layer 2. In principle, we may introduce another fully connected
layer as (夕 1(χ%1),...,夕K(χ%κ)) = Sgm(VXn + c), with additional parameters V ∈ Rk×k
and c ∈ RK, but this is undesirable for the network sparsification. Instead, we propose simple yet
effective way to generate input-dependent probability with minimal parameters. Motivated by the
batch normalization (Ioffe & Szegedy, 2015), we construct each πk(xn,k) independently as follows:
Ψk (xn,k) = ∏k ∙ clamp Yk
xn,k-μk + βk,e∖,
σk
(17)
1Here we wrote as if we only have one binary mask layer, but in practice every layer in a neural network is
equipped with its own binary mask. We abbreviate those multiple binary masks with Z for simplicity.
2For convolutional layers, we apply the global average pooling to tensors to get vectorized inputs.
5
Under review as a conference paper at ICLR 2020
where μk and σk are the estimates of k-th components of mean and standard deviation of inputs,
and γk and βk are scaling and shifting parameters , and > 0 is some small tolerance value, and
clamp(x, ) = min(1 - , max(, x)). The intuition behind this construction is as follows. The
inputs after the standardization would approximately be distributed as N(0, 1). If we pass them
through min(1 - , max(, x)), most of insignificant dimensions would have probability near zero.
However, some inputs may be important regardless of the significance of activation, for which we
expect the shifting βk to compensate. With β we control the overall sparsity, but we want them to be
small unless required to get sparse outcomes. We enforce this by placing a prior β 〜N(0, ρI).
The goal of variational inference is hence to learn the posterior distribution p(W, Z, ∏, β∣D), and
we approximate this with variational distribution of the form
K	NK
q(W, Z, ∏, β∣X) = δw(W) Y q(πk)q(βk)YY
q(zn,k |nk, Xn),	(18)
k=1	n=1 k=1
where q(∏k) are the same as in beta-Bernoulli dropout, q(βk) = N(βk ηk, κk), and q(zn,k ∣∏k)=
p(zn,kI∏k, Xn)3 The KL-divergence is computed as DκL[q(Z, π∣X)kp(Z, π)] + DκL[q(β)kp(β)],
where both terms can be computed analytically.
The prediction for the novel input x* is similarity done as in the beta-Bernoulli dropout, with the
naive approximation for the expectation:
p(y*∣x*, D, W) ≈ p(y*∣f(x*; Eq[z*]乳 W)),	(19)
where
Eq [z*,k] = Eq [∏k] ∙ clamp (Yk Xnk°- "k + η, e) .	(20)
Two stage pruning scheme Since πk ≥ πk(xn,k) for all xn,k, we expect the resulting network
to be sparser than the network pruned only with the beta-Bernoulli dropout (only with πk). To achieve
this, we propose a two-stage pruning scheme, where we first prune the network with beta-Bernoulli
dropout, and prune the network again with πk(Xn,k) while holding the variables π fixed. By fixing π
the resulting network is guaranteed to be sparser than the network before the second pruning.
5	Experiments
We now compare our beta-Bernoulli dropout (BB) and input-dependent beta-Bernoulli dropout (DBB)
to other structure learning/pruning algorithms on several neural networks using benchmark datasets.
Experiment Settings We follow a common experimental setting used by existing work to
evaluate pruning performance, and use LeNet 500-300, LeNet 5-Caffe 4, and VGG-like (Zagoruyko,
2015) on MNIST (LeCun et al., 1998), CIFAR-10, and CIFAR-100 datasets (Krizhevsky & Hinton,
2009). For baselines, we use the following recent Bayesian pruning methods: 1) SBP: Structured
Bayesian Pruning (Neklyudov et al., 2017), 2) VIB: Variational Information Bottleneck (Dai et al.,
2018), 3) L0: L0 regularization (Louizos et al., 2018), 4) GD: Generalized Dropout (Srinivas &
Babu, 2016), 5) CD: Concrete Dropout (Gal et al., 2017). We faithfully tune all hyperparameters of
baselines on a validation set to find a reasonable solution that is well balanced between accuracy and
sparsification, while fixing batch size (100) and the number of maximum epochs (200) to match our
experiment setting.
Implementation Details We pretrain all networks using the standard training procedure before
fine-tuning for network sparsification Neklyudov et al. (2017); Dai et al. (2018); Louizos et al. (2018).
While pruning, we set the learning rate for the weights W to be 0.1 times smaller than those for the
variational parameters as in Neklyudov et al. (2017). We used Adam (Kingma & Ba, 2015) for all
3In principle, We may introduce an inference network q(z∣π, x, y) and minimizes the KL-divergence between
q(z∣π, x, y) and p(z∣π, x), but this results in discrepancy between training and testing for sampling z, and also
make optimization cumbersome. Hence, we chose to simply set them equal. Please refer to Sohn et al. (2015)
for discussion about this.
4https://github.com/BVLC/caffe/blob/master/examples/mnist
6
Under review as a conference paper at ICLR 2020
Table 1: Results for LeNet-500-300 and LeNet5-Caffe on MNIST.
LeNet 500-300					LeNet5-Caffe			
	Error (%)	Neurons	xFLOPs	Memory (%)	Error (%)	Neurons/Filters	xFLOPs	Memory (%)
Original	1.63	784-500-300	1.0	100.0	0.71	20-50-800-500	1.0	100.0
CD	1.54±0.04	784-500-300	1.0	100.0	0.67±0.04	20-50-800-500	1.0	100.0
GD	1.45±0.02	518-187-135	4.41	22.68	0.67±0.02	12-37-496-300	2.21	41.29
SBP	1.58±0.06	164-91-30	30.43	3.30	0.70±0.02	5-15-134-44	11.41	6.47
L0	1.64±0.03	148-116-127	16.16	6.21	0.66±0.03	11-22-79-137	4.08	10.61
VIB	1.60±0.05	138-98-26	33.63	2.98	0.66±0.04	7-25-297-38	5.80	8.75
BB	1.59±0.07	137-90-37	33.81	2.97	0.66±0.03	7-17-132-56	7.30	7.85
	1.26±0.04	288-114-65	13.38	7.48	0∙59±0.04	9-24-174-58	4.55	9.5
DBB	1.59±0.03	93-35-25	47.07	2.22	0.68±0.01	7-17-64-38	7.27	7.10
	1.42±0.06	104-31-29	34.98	3.00	0∙59±0.02	9-23-61-32	4.55	8.28
Figure 1: Top: Classification errors and sparsificaton performance of various pruning methods for LeNet-500-
300 and LeNet5-Caffe on the MNIST dataset. Bottom: Error-Speedup tradeoff plots for LeNet 500-300 (left)
and LeNet5-Caffe (right).
methods. For DBB, as mentioned in Section 4.2, we first prune networks with BB, and then prune
again with DBB whiling holding the variational parameters for q(π) fixed.
We report all hyperparameters of BB and DBB for reproducing our results. Weset α∕K = 10-4 for
all layers of BB and DBB. In principle, we may fix K to be large number and tune α. However, in
the network sparsification tasks, K is given as the neurons/filters to be pruned. Hence, we choose to
set the ratio a/K to be 10-3 for all layers in all networks. For the inference, We prune neurons/filters
whose expected binary mask Eq [zk] are smaller than a threshold, and excluded them from the
computation, in order to show the performance of the pruned networks. We fixed the threshold to
10-3 for all settings 5. For the input-dependent dropout, since the number of pruned neurons/filters
differ according to the inputs, we report them as the running average over the test data. We fix the
temperature parameter of concrete distribution T = 10-1 and the prior variance of β, P = √5 for all
experiments. We ran all experiments three times to report mean and standard deviations for the error.
To control the tradeoff between classification error and pruned network size, we run each algorithm
with various tradeoff parameters. For VIB and L0, we controlled the tradeoff parameters originally
introduced in the papers. For variational inference based algorithms including SBP and BB, we
scaled the KL terms in ELBO with tradeoff parameter γ > 1. Note that when γ > 1, the modified
ELBO is a still lower bound on the marginal likelihood. For DBB we use fixed parameter settings but
retrain the model with different runs of BBs, that are trained with different tradeoff parameters γ. For
more detailed settings of tradeoff control, please refer to the appendix.
5.1	MNIST Experiments
We use LeNet 500-300 and LeNet 5-Caffe networks on MNIST for comparison. Following the
conventions, we apply dropout to the inputs to the fully connected layers and right after the convolution
for the convolutional layers. We report the trade-off between the error and speedup in terms of FLOPs
in Fig. 1, and show one representative result per each algorithm in Table 1 to compare the speedups and
memory savings with respect to a particular error level. For DBB, we included additional overhead
coming from input-dependent computation into FLOPs and memory computation. For LeNet 5-Caffe,
following Dai et al. (2018); Neklyudov et al. (2017), we used larger tradeoff parameters for the first
two convolutional layers - please refer to the appendix for the detail.
5We tried different threshold values ranging from 10-2 to 10-4 but the difference was insignificant.
7
Under review as a conference paper at ICLR 2020
CIFAR-10				CIFAR-100		
	Error (%)	xFLOPs	Memory (%)	Error (%)	xFLOPs	Memory (%)
Original	7.13	1.0	100.0	33.10	1.0	100.0
CD	6.94±0.08	1.0	100.0	30.31±0.13	1.0	100.0
GD	6.87±0.03	2.27	20.39	30.89±0.34	1.95	29.97
SBP	7.53±0.06	4.24	8.04	31.47±0.22	2.63	13.41
L0	7.05±0.29	3.46	10.54	32.09±0.39	2.77	15.64
VIB	7.01±0.14	3.31	10.26	30.09±0.07	2.32	16.38
BB	7.18±0.12	4.35	788	29.27±0.43	2.79	12.59
	6.45±0.19	3.00	11.13	28.97±0.24	2.46	15.49
DBB	7.04±0.05	4.60	8.86	29.08±0.27	2.91	13.30
	6.56±0.04	3.23	11.64	28.46±0.25	2.69	14.46
BB
VIB
GD
Lo
SBP
DBB
Speedup in FLOPs
1.0
0.8
0.6
0.4
0.2
0.0
apple
layer 3 layer 8 layer 15
Figure 2: Top: Classification errors and sparsification performances of various pruning methods on
CIFAR-10 and CIFAR-100 datasets. Bottom: Error-Speedup tradeoff plots (left, middle) and an
empirical analysis of learned filters from DBB (right).
With both neworks, BB and DBB either achieve significantly smaller error than the baseline methods,
or significant speedup and memory saving at similar error rates. DBB, with its input-adaptive pruning,
obtains larger speedup and memory saving compared to BB, which is better shown in the error-
speedup tradeoff plot. For LeNet 500-300, BB acheives the mean error 1.40% with 21.79 xFLOPs,
while DBB acheives similar the mean error with 38.75% xFLOPs. Table 2 in the appendix clearly
show this advantage.
On LeNet-500-300, DBB prunes large amount of neurons in the input layer, because the inputs to
this network are simply vectorized pixel values, so it can prune the inputs according to the digit
classes, which is shown in Fig. 1 Also, we observe that the dropout masks generated by DBB tend to
be generic at lower network layers to extract common features, but become class-specific at higher
layers to specialize features for class discriminativity (see Fig. 3 in the appendix). We observed
similar behavior of DBB in the experiments with VGG on both CIFAR10 and CIFAR100 datasets.
5.2	CIFAR- 1 0 and CIFAR- 1 00 Experiments
We further compare the pruning algorithms on VGG-like network on CIFAR-10 and CIFAR-100
datasets. Fig. 2 summarizes the performance of each algorithm on particular setting, where BB and
DBB achieve both impressive speedups and memory savings with significantly improved accracy.
When compared with the baseline sparsification methods, they either achieve better error at similar
sparsification rates, or achieve better speedup and memory saving at simlar error rates. DBB achieves
larger speedup than BB at similar error rates, and to all other baselines. For instance, in CIFAR-100,
BB achieves the error of 28.86% with 2.60 xFLOPs while DBB records the error 28.84% with
2.98 xFLOPs. As clearly depicted in the error-speedup tradeoff plot Fig. 2, DBB provides the best
tradeoff among the algorithms we tested. Please refer to Table 3 in the appendix for more detailed
experimental results.
Further analysis of the filters retained by DBB in Fig. 2 shows that DBB either retains most filters
(layer 3) or perform generic pruning (layer 8) at lower layers, while performing diversified pruning at
higher layers (layer 15). Further, at layer 15, instances from the same class (bus) retained similar
filters, while instances from different classes (bus vs. apple) retained different filters.
6	Conclusion
We have proposed novel beta-Bernoulli dropout for network regularization and sparsification, where
we learn dropout probabilities for each neuron either in an input-independent or input-dependent
manner. Our beta-Bernoulli dropout learns the distribution of sparse Bernoulli dropout mask for
8
Under review as a conference paper at ICLR 2020
each neuron in a variational inference framework, in contrast to existing work that learned the
distribution of Gaussian multiplicative noise or weights, and obtains significantly more compact
network compared to those competing approaches. Further, our dependent beta-Bernoulli dropout
that input-adaptively decides which neuron to drop further improves on the input-independent beta-
Bernoulli dropout, both in terms of size of the final network obtained and run-time computations.
Future work may include network structure learning (e.g. a tree structure) using a generalized version
of the method where dropout mask is applied to a block of weights rather than to each hidden unit.
References
J. Ba and B. Frey. Adaptive dropout for training deep neural networks. In Advances in Neural
Information Processing Systems 26, 2013.
Bin Dai, Chen Zhou, and David Wipf. Compressing neural networks using the variational information
bottleneck. arXiv:1802.10399, 2018.
J. Feng and T. Darrell. Learning the structure of deep convolutional networks. IEEE International
Conference on Computer Vision, 2015.
Y. Gal, J. Hron, and A. Kendall. Concrete dropout. Advances in Neural Information Processing
Systems, 2017.
T. L. Griffiths and Z. Ghahramani. Infinite latent feature models and the Indian buffet process. In
NIPS, 2005.
S. Han, H. Mao, and W. J. Dally. Deep compression: compressing deep neural networks with pruning,
trained quantization and Huffman coding. In Proceedings of the International Conference on
Learning Representations, 2016.
S. Ioffe and C. Szegedy. Batch normalization: accelerating deep network training by reducing internal
covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, 2015.
E. Jang, S. Gu, and B. Poole. Categorical reparametrization with Gumbel-softmax. In Proceedings of
the International Conference on Learning Representations, 2017.
D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In Proceedings of the
International Conference on Learning Representations, 2015.
D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In Proceedings of the International
Conference on Learning Representations, 2014.
D. P. Kingma, T. Salimans, and M. Welling. Variational dropout and the local reparametrization trick.
In Advances in Neural Information Processing Systems 28, 2015.
Alex Krizhevsky and Geoffrey E. Hinton. Learning multiple layers of features from tiny images.
Technical report, Computer Science Department, University of Toronto, 2009.
Ponnambalam Kumaraswamy. A generalized probability density function for double-bounded random
processes. Journal of Hydrology, 1980.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings ofthe IEEE, 86(11):2278-2324,1998.
C. Louizos, K. Ullrich, and M. Welling. Bayesian compression for deep learning. Advances in Neural
Information Processing Systems, 2017.
C.	Louizos, M. Welling, and D. P. Kingma. Learning sparse neural networks through L0 regularization.
International Conference on Learning Representations, 2018.
C.	J. Maddison, A. Mnih, and Y. W. Teh. The concrete distribution: a continuous relaxation of discrete
random variables. In Proceedings of the International Conference on Learning Representations,
2017.
9
Under review as a conference paper at ICLR 2020
D.	Molchanov, A. Ashukha, and D. Vetrov. Variational dropout sparsifies deep neural networks. In
Proceedings of the 34th International Conference on Machine Learning, 2017.
E.	Nalisnick and P. Smyth. Stick-breaking variational autoencoders. In Proceedings of the Interna-
tional Conference on Learning Representations, 2017.
K.	Neklyudov, D. Molchanov, A. Ashukha, and D. Vetrov. Structured Bayesian pruning via log-
normal multiplicative noise. Advances in Neural Information Processing Systems, 2017.
L.	Ren, Y. Wang, D. B. Dunson, and L. Carin. The kernel beta process. In Advances in Neural
Information Processing Systems 24, 2011.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference
in deep generative models. In Proceedings of the 31st International Conference on Machine
Learning, 2014.
K. Sohn, H. Lee, and X. Yan. Learning structured ouput representation using deep conditional
generative models. Advances in Neural Information Processing Systems 28, 2015.
S. Srinivas and R. V. Babu. Generalized dropout. arXiv preprint arXiv:1611.06791, 2016.
N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple
way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):
1929-1958, 2014.
R.	Thibaux and M. I. Jordan. Hierarchical beta processess and the Indian buffet processes. In
Proceedings of the 11th International Conference on Artificial Intelligence and Statistics, 2007.
W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning structured sparsity in deep neural networks.
In Advances in Neural Information Processing Systems 29, 2016.
S.	Williamson, P. Orbanz, and Z. Ghahramani. Dependent indian buffet processes. In Proceedings of
the 13th International Conference on Artificial Intelligence and Statistics, 2010.
S. Zagoruyko. 92.45 on CIFAR-10 in Torch. 2015.
M. Zhou, H. Yang, G. Sapiro, and D. B. Dunson. Dependent hierarchical beta process for image
interpolation and denoising. In Proceedings of the 14th International Conference on Artificial
Intelligence and Statistics, 2011.
10
Under review as a conference paper at ICLR 2020
0
1
2
3
S 4
e
P 5
6
7
8
9
0123456789
class
conv20
1.0
0.8
0.6
0.4
0.2
0.0
dense800
1.0
0.8
0.6
0.4
0.2
0123456789
class
0.0
0
1
2
3
S 4
e
P 5
6
7
8
9
dense500
0.8
0.6
0.4
0.2
0123456789
class
1.0
0.0
Figure 3: Correlation coefficients of class averages of 夕(x) for the four layers in LeNet5-Caffe, where
the darker shows the more correlation between the class average values of 夕(x). The correlation
between classes diminishes as layer goes up, implying that the input filters are being specialized to
the classes.
A	Appendix
B More details on the experiments
We first describe the tradeoff parameter settings we used in the experiments.
•	For the variational inference based methods (BB, SBP), we scaled the KL-term by γ ≥ 1.
We tested with γ ∈ {1.0, 2.0, 4.0, 6.0, 8.0}.
•	For GD, we tested with hyperparameter α ∈ {10-3, 10-4, 10-5, 10-6}.
•	For DBB, we didn’t do any tradeoff control. Instead, we started from different results of BB
produced with various tradeoff parameters.
•	For VIB, we tested with tradeoff parameter Y ∈ {10-4,10-5, 5 ∙ 10-6,10-6, 5 ∙ 10-7} for
LeNet-500-300, Y ∈ {10-6,5 ∙ 10-7,10-7, 5∙ 10-8,10-8} for LeNet5-Caffe and VGG-like.
•	For L0, we tested with λ ∈ {0.1/60000, 0.05/60000, 0.02/60000, 0.01/60000, 0.005/60000}
for LeNet-500-300 and LeNet5-Caffe, λ ∈ {10-7, 5 ∙ 10-8,2 ∙ 10-8,10-8,5 ∙ 10-9} for
VGG-like.
For LeNet5-Caffe, we used larger tradeoff parameters for the first two conv layers, because the
penalty for them are relatively underestimated due to the small number of filters (20, 50) compared to
those of fully-connected layers (800-500).
•	For BB, SBP, and GD, we multiplied the kl scaling factor Y by 20 and 8 for the first two
convolutional layers.
•	For VIB, following the paper, we multiplied the sizes of feature maps, 24 × 24 and 8 × 8 to
the tradeoff parameters of the first and second convolutional layers.
•	For L0 reg, we used the setting specified in the paper (L0-sep).
C Additional results
We present the errors, speedup in FLOPs, and memory savings for all trade-off settings of every
algorithm. The results of LeNet-500-300 and LeNet-5-Caffe are displayed in Table 2, and the results
for VGG-like on CIFAR-10 and CIFAR-100 are displayed in Table 3
11
Under review as a conference paper at ICLR 2020
Table 2: Comparison of pruning methods on LeNet-500-300 and LeNet5-Caffe with MNIST. Error
and Memory are in %.
LeNet 500-300	∣	LeNet5-Caffe
	Error	Speedup	Memory	Error	Speedup	Memory
CD	1.54±0.04	1.0	100.0	0.67±0.04	1.0	100.0
	1.48±0.01	1.00	^^99.62	0.70±0.04	1.93	45.14
	1.47±0.02	4.23	23.64	0.73±0.03	2.01	45.84
GD	1.53±0.04	4.44	22.55	0.68±0.02	1.96	45.56
	1.45±0.02	4.31	23.22	0.70±0.05	2.03	45.36
	1.93±0.09	22.99	-^434-	0.83±0.07	6.77	12.3
	2.06±0.13	27.68	3.61	0.84±0.05	8.06	11.83
SSL	2.30±0.09	39.13	2.55	0.97±0.06	12.74	10.97
	2.63±0.12	54.83	1.82	1.03±0.05	20.26	10.20
	3.52±0.12	75.69	1.32	1.44±0.08	23.17	10.09
	1.45±0.02	8.4	-^11.89	0.68±0.02	2.92	17.95
	1.51±0.04	18.39	5.43	0.71±0.01	4.78	14.75
SVD	1.50±0.04	14.73	6.78	0.72±0.01	5.17	14.04
	1.51±0.03	18.39	5.43	0.74±0.04	4.92	14.05
	2.75±0.14	21.44	4.66	1.47±0.11	3.62	16.38
	1.59±0.03	18.92	-^530-	0.70±0.02	11.41	6.47
	1.66±0.04	22.96	4.37	0.77±0.01	11.95	6.20
SBP	1.58±0.06	30.43	3.30	0.81±0.04	13.73	5.95
	1.64±0.04	38.68	2.60	0.88±0.06	15.30	5.68
	1.68±0.05	45.78	2.19	0.96±0.04	20.13	5.18
	1.64±0.03	16.16	-^6.21-	0.70±0.06	5.23	9.77
	1.46±0.02	12.82	7.82	0.66±0.03	4.08	10.61
L0	1.33±0.03	9.74	10.29	0.62±0.04	3.42	11.57
	1.28±0.03	8.27	12.10	0.55±0.05	3.15	12.64
	1.27±0.05	7.57	13.22	0.52±0.01	2.86	14.08
	1.60±0.05	33.63	-^2.98-	0.67±0.03	8.46	8.17
	1.71±0.06	21.54	4.65	0.69±0.05	7.11	8.44
VIB	1.66±0.05	20.10	4.99	0.66±0.04	5.80	8.75
	1.80±0.06	18.44	5.43	0.69±0.04	5.25	9.15
	1.68±0.05	17.10	5.86	0.73±0.04	4.67	9.68
	1.26±0.04	13.38	-^748-	0.59±0.04	4.55	9.50
	1.28±0.06	16.23	6.18	0.59±0.04	5.58	8.94
BB	1.40±0.09	21.79	4.60	0.66±0.03	7.30	7.85
	1.49±0.09	28.28	3.55	0.78±0.04	10.45	6.60
	1.59±0.07	33.81	2.97	0.86±0.05	13.77	5.87
	1.41±0.06	34.98	^^300-	0.59±0.02	4.55	8.28
	1.40±0.06	38.75	2.71	0.63±0.02	5.58	7.83
DBB	1.52±0.05	43.27	2.42	0.68±0.01	7.27	7.10
	1.59±0.03	47.07	2.22	0.78±0.06	10.32	6.32
	1.69±0.03	52.09	2.00	0.86±0.05	13.53	5.78
12
Under review as a conference paper at ICLR 2020
Table 3: Comparison of pruning methods on VGG-like with CIFAR10 and CIFAR100. Error and
Memory are in %.
VGG-CIFAR10				VGG-CIFAR100		
	Error	Speedup	Memory	Error	Speedup	Memory
CD	6.94±0.08	1.0	100.0	30.031±0.13	1.0	100.0
	6.91±0.15	2.25	20.58	31.17±0.42	2.03	29.01
	6.69±0.16	2.23	20.53	30.64±0.27	2.06	28.71
GD	6.84±0.08	2.28	20.40	30.84±0.04	2.02	29.08
	6.80±0.10	2.24	20.40	30.86±0.68	1.99	29.55
	7.12±0.16	1.41	20.30	30.45±0.19	1.25	36.14
OCT	7.32±0.07	1.43	19.04	30.64±0.06	1.25	34.44
SSL	8.27±0.11	1.67	14.78	32.75±0.16	1.28	28.24
	9.56±0.09	2.09	12.01	36.27±0.22	1.48	23.21
	7.78±0.01	1.49	21.71	31.22±0.06	1.25	36.80
C∖∕TΛ	7.38±0.05	1.40	22.47	31.31±0.08	1.25	36.90
SVD	7.85±0.08	1.41	21.18	31.26±0.06	1.25	36.77
	9.71±0.18	1.41	18.48	34.26±0.19	1.25	35.76
	7.65±0.04	3.69	^^976-	31.89±0.26	2.54	16.30
	7.37±0.22	3.90	8.71	31.87±0.11	2.49	14.31
SBP	7.53±0.06	4.24	8.04	31.52±0.09	2.56	13.74
	7.44±0.00	4.45	7.72	31.85±0.23	2.61	13.55
	7.55±0.16	4.78	7.27	31.47±0.22	2.63	13.41
	7.05±0.29	3.46	10.54	31.73±0.46	2.82	15.10
	7.43±0.15	3.42	10.65	31.59±0.66	2.79	15.36
L0	7.23±0.32	3.38	10.81	31.94±0.40	2.79	15.45
	7.25±0.05	3.33	10.89	31.87±0.20	2.78	15.58
	7.11±0.21	3.33	10.93	32.09±0.39	2.77	15.64
	7.01±0.14	3.31	10.26	30.17±0.11	2.37	15.81
	6.95±0.11	3.22	10.49	30.10±0.21	2.36	16.06
VIB	6.83±0.14	3.10	10.93	30.43±0.18	2.31	16.38
	6.92±0.14	3.11	10.87	30.09±0.13	2.32	16.40
	6.78±0.08	3.01	11.13	30.09±0.07	2.32	16.38
	6.45±0.19	3.00	11.13	28.97±0.24	2.46	15.49
	6.57±0.17	3.30	10.12	28.53±0.04	2.53	14.63
BB	6.88±0.14	4.00	8.45	28.86±0.25	2.60	13.80
	7.18±0.12	4.35	7.88	29.27±0.43	2.79	12.59
	7.40±0.11	5.29	6.83	29.36±0.23	2.84	12.19
	6.56±0.05	3.23	11.64	29.22±0.40	2.63	15.26
	6.51±0.20	3.52	10.70	29.07±0.58	2.68	14.89
DBB	7.02±0.24	4.24	9.47	28.46±0.25	2.73	14.26
	7.04±0.05	4.60	8.86	29.08±0.27	2.91	13.30
	7.52±0.04	5.57	7.91	28.84±0.34	2.96	13.29
13