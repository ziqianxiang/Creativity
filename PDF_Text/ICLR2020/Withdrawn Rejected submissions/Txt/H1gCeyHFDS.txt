Under review as a conference paper at ICLR 2020
Gram-Gauss-Newton Method: Learning Over-
parameterized Neural Networks for Regres-
sion Problems
Anonymous authors
Paper under double-blind review
Ab stract
First-order methods such as stochastic gradient descent (SGD) are currently the
standard algorithm for training deep neural networks. Second-order methods,
despite their better convergence rate, are rarely used in practice due to the pro-
hibitive computational cost in calculating the second-order information. In this
paper, we propose a novel Gram-Gauss-Newton (GGN) algorithm to train deep
neural networks for regression problems with square loss. Our method draws
inspiration from the connection between neural network optimization and kernel
regression of neural tangent kernel (NTK). Different from typical second-order
methods that have heavy computational cost in each iteration, GGN only has minor
overhead compared to first-order methods such as SGD. We also give theoretical
results to show that for sufficiently wide neural networks, the convergence rate of
GGN is quadratic. Furthermore, we provide convergence guarantee for mini-batch
GGN algorithm, which is, to our knowledge, the first convergence result for the
mini-batch version of a second-order method on overparameterized neural net-
works. Preliminary experiments on regression tasks demonstrate that for training
standard networks, our GGN algorithm converges much faster and achieves better
performance than SGD.
1	Introduction
First-order methods such as Stochastic Gradient Descent (SGD) are currently the standard choice for
training deep neural networks. The merit of first-order methods is obvious: they only calculate the
gradient and therefore are computationally efficient. In addition to better computational efficiency,
SGD has even more advantages among the first-order methods. At each iteration, SGD computes the
gradient only on a mini-batch instead of all training data. Such randomness introduced by sampling
the mini-batch can lead to better generalization (Hardt et al., 2015; Keskar et al., 2016; Masters &
Luschi, 2018; Mou et al., 2017; Zhu et al., 2018) and better convergence (Ge et al., 2015; Jin et al.,
2017a;b), which is crucial when the function class is highly overparameterized deep neural networks.
Recently there is a huge body of works trying to develop more efficient first-order methods beyond
SGD (Duchi et al., 2011; Kingma & Ba, 2014; Luo et al., 2019; Liu et al., 2019).
Second-order methods, despite their better convergence rate, are rarely used to train deep neural
networks. At each iteration, the algorithm has to compute second order information, for example,
the Hessian or its approximation, which is typically an m by m matrix where m is the number of
parameters of the neural network. Moreover, the algorithm needs to compute the inverse of this
matrix. The computational cost is prohibitive and usually it is not even possible to store such a matrix.
Recently, there is a series of works (Du et al., 2018b;a; Zou et al., 2018; Allen-Zhu et al., 2018a;
Oymak & Soltanolkotabi, 2018; Arora et al., 2019b;a; Cao & Gu, 2019; Zou & Gu, 2019) considering
the optimization of neural networks based on the idea of neural tangent kernel (NTK) (Jacot et al.,
2018). Roughly speaking, the idea of NTK is to linearly approximate the output of a network w.r.t.
the parameters at a local region, thus resulting in a kernel feature map, which is the gradient of the
output w.r.t. the parameters. Jacot et al. (2018) shows that when the width of the network tends
to infinity, the NTK tends to be unchanged during gradient flow since the network only needs o(1)
change (goes to zero as the width tends to infinity) of parameter to fit the data, so the change of kernel
1
Under review as a conference paper at ICLR 2020
is also o(1). As for finite-width networks, however, the NTK changes and previous works (Allen-Zhu
et al., 2018b; Arora et al., 2019a) show that for sufficiently wide neural networks the optimization
dynamic of GD/SGD is equivalent to that of using GD/SGD to solve an NTK kernel regression
problem where the kernel is slowly evolving (see Lemma 1 in Section 2 for a precise description). A
natural question then arises:
Can we gain acceleration by directly solving kernel regression w.r.t. the NTK at each step?
In this paper, we give a positive answer to this question and reveal the connection between NTK
regression and GaUss-NeWton method. We propose a novel optimization method - the Gram-Gauss-
Newton (GGN) method. Instead of doing gradient descent, GGN solves the kernel regression w.r.t.
the NTK at each step of the optimization. FolloWing this idea, We theoretically prove that for
overparameterized netWorks, GGN enjoys quadratic convergence compared to the linear rate of
gradient descent.
Besides theoretical fast convergence rate, GGN is also very efficient in practice. In fact, GGN is
implicitly a reformulation of the Gauss-NeWton method (see Section 3.1 for details) Which is a classic
second-order algorithm often used for solving nonlinear regression problems With square loss. In the
Gauss-NeWton method, one uses J>J as an approximation of the Hessian (see Section 2 for a formal
description) Where J is the Jacobian matrix. HoWever, the original Gauss-NeWton method faces
challenges When used for training deep neural netWorks. Most seriously, the size of the approximate
Hessian J>J is m by m, Where m is the number of parameters of the neural netWork. Moreover, for
overparameterized neural netWorks, J>J is not invertible, Which may make the algorithm intractable
for training commonly-used neural netWorks.
GGN bypasses the difficulty stated above as folloWs. Instead of using J>J as approximate Hessian
and applying NeWton-type method, each step of GGN only involves the Gram matrix JJ> Whose
size is n by n Where n is the number of data. Furthermore, as already mentioned, to get better
generalization performance, it is crucial to use mini-batch to introduce sampling noise When calculat-
ing derivatives. Therefore, like SGD, We also use mini-batch in GGN. In this case, the size of the
Gram matrix further reduces to b by b, Where b is the batch size. Though conventional Wisdom may
suggest that applying mini-batch scheme to second-order methods Will introduce biased estimation of
the accelerated gradient direction, We give the first convergence result for mini-batch second-order
method on overparameterized netWorks. Regarding computational complexity, We shoW that at each
iteration, the overhead of GGN is small compared to SGD: the extra computation of GGN is mainly
the matrix product JJ> and the inverse of this matrix Whose size is small for a mini-batch. Detailed
analyses can be found in Section 3.3. We next conduct experiments on tWo regression tasks to study
the effectiveness of the GGN algorithm. We demonstrate that in these tWo real applications, using a
practical neural netWork (e.g., ResNet-32) With standard Width, our proposed GGN algorithm can
converge faster and achieve better performance than several baseline algorithms.
1.1	Related Works
Despite the prevalence of first-order methods for training deep neural netWorks, there have been
continuing efforts in developing practical second-order methods (Becker et al., 1988; Pascanu &
Bengio, 2013). We summarize some of these Works beloW.
The main approach for these methods is to develop delicate approximations of the second-order
information matrix so that the update direction can be computed as efficiently as possible. For
example, Botev et al. (2017) proposed a recursive block-diagonal approximation of the Hessian. The
blocks are Kronecker factored and can be efficiently computed and inverted. Grosse and Martens
in a series of Works developed the K-FAC method (Martens & Grosse, 2015; Grosse & Martens,
2016). The key idea is a Kronecker-factored approximation of the Fisher information matrix, Which
is used as the second-order matrix in natural gradient methods. These Works received considerable
attention and have been further improved (Wu et al., 2017; George et al., 2018; Martens et al., 2018).
Bernacchia et al. (2018) derived an exact expression of the natural gradient update, but only Works
for linear netWorks. Different from all these Works, our GGN algorithm does not try to approximate
the second-order matrix Whose size is inevitably huge. Instead, We present an easy-to-compute
solution of the updating direction, reducing the computational cost significantly. One exceptional
concurrent Work Ren & Goldfarb (2019) also aims to use the exact Gauss-NeWton update. They
focus on reducing the complexity of inverting approximate Hessian by Sherman-Morrison-Woodbury
2
Under review as a conference paper at ICLR 2020
Formula and require subtle implementation tricks to use backpropagation. In contrast, GGN has
simpler update rule and better guarantee for neural networks.
In a concurrent and independent work, Zhang et al. (2019a) showed that natural gradient method and
K-FAC have a linear convergence rate for sufficiently wide networks in full-batch setting. In contrast,
our method enjoys a higher-order (quadratic) convergence rate guarantee for overparameterized
networks, and we focus on developing a practical and theoretically sound optimization method. We
also reveal the relation between our method and NTK kernel regression, so using results based on
NTK (Arora et al., 2019b), one can easily give generalization guarantee of our method. Another
independent work (Achiam et al., 2019) proposed a preconditioned Q-learning algorithm which has
similar form of our update rule. Unlike the methods considered in Zhang et al. (2019a); Achiam
et al. (2019) which contain the learning rate that needed to be tuned, our derivation of GGN does not
introduce a learning rate term (or understood as suggesting that the learning rate can be fixed to be 1
to get good performance which is verified in Figure 2 (c)).
2	Neural Tangent Kernel and the Classic Gauss-Newton Method
for Nonlinear Least S quares Regression
Nonlinear least squares regression problem is a general machine learning problem. Given data pairs
{xi , yi}in=1 and a class of nonlinear functions f, e.g. neural networks, parameterized by w, the
nonlinear least squares regression aims to solve the optimization problem
1n
WmRm L(w) = 2∑(f (W，Xi)-yi产	⑴
i=1
In the seminal work (Jacot et al., 2018), the authors consider the case when f is a neural network
with infinite width. They showed that optimization on this problem using gradient flow involves a
special kernel which is called neural tangent kernel (NTK). The follow-up works further extended the
relation between optimization and NTK which can be concluded in the following lemma:
Lemma 1 (Lemma 3.1 in Arora et al. (2019a), see also Dou & Liang (2019); Mei et al. (2019)).
Consider optimizing problem (1) by gradient descent with infinitesimally small learning rate: dWt =
-VL(wt). where Wt is the parameters at time t. Let f = (f (wt, Xi))n=ι ∈ Rn be the network
outputs on all Xi ’s at time t, and y = (yi)in=1 be the desired outputs. Then ft follows the following
evolution:
d = -GtYft -y),	(2)
where Gt is an n × n positive semidefinite matrix, i.e. the Gram matrix w.r.t. the NTK at time t,
whose (i,j)-th entry is hVwf(Wt, Xi), Vwf(Wt,Xj)i.
The key idea of Jacot et al. (2018) and its extensions (Du et al., 2018b;a; Zou et al., 2018; Allen-Zhu
et al., 2018a; Oymak & Soltanolkotabi, 2018; Lee et al., 2019; Yang, 2019; Arora et al., 2019b;a;
Cao & Gu, 2019; Zou & Gu, 2019) is that when the network is sufficiently wide, the Gram matrix at
initialization G0 is close to a fixed positive definite matrix defined by the infinite-width kernel and
Gt is close to G0 during training for all t. Under this situation, Gt remains invertible, and the above
dynamics is then identical to the dynamics of solving kernel regression with gradient flow w.r.t. the
current kernel at time t. In fact, Arora et al. (2019a) rigorously proves that a fully-trained sufficiently
wide ReLU neural network is equivalent to the kernel regression predictor.
As pointed out in Chizat & Bach (2018), the idea of NTK can be summarized as a linear approximation
using first order Taylor expansion. We give an example of this idea on the NTK at initialization:
f (w, Xi) — f(wo, Xi) ≈ Vwf (W0, Xi) ∙ (w — W0),	(3)
where Vwf(W0, X) can then be viewed as an explicit expression of feature map at X, W - W0 is the
parameter in reproducing kernel Hilbert space (RKHS) induced by NTK and f(w, Xi) - f(w0, Xi)
the target value.
The idea of linear approximation is also used in the classic Gauss-Newton method (Golub, 1965)
to obtain an acceleration algorithm for solving nonlinear least squares problem (1). Concretely, at
iteration t, Gauss-Newton method takes the following first-order approximation:
f (w, Xi) — f (wt, Xi) ≈ Vwf (wt, Xi) ∙ (w — wt),	(4)
3
Under review as a conference paper at ICLR 2020
where wt stands for the parameter at iteration t. We note that this is also the linear expansion for
deriving NTK at time t. According to Eq. (1) and (4), to update the parameter, one can instead solve
the following problem.
12
Wt+1 = argmin - Ilft + Jt(W - Wt) - y∣b ,	(5)
w2
where ft, y have the same meaning as in Lemma 1, and Jt = Bwf (wt, xι), .…，Nwf (wt, Xn))> ∈
Rn×m is the Jacobian matrix.
A necessary and sufficient condition for W to be the solution of Eq. (5) is
(J>Jt) ∙ (w - Wt) = -j>(ft- y).	⑹
Below we will denote Ht := Jt>Jt ∈ Rm×m . For under-parameterized model (i.e., the number of
parameters m is less than the number of data n), Ht is invertible, and the update rule is
wt+1 = wt - Ht-1Jt>(ft - y).	(7)
This can also be viewed as an approximate Newton’s method using Ht = Jt>Jt to approximate the
Hessian matrix. In fact, the exact Hessian matrix is
nn
Vw2 X(f(wt, Xi)- yi)2 = J> Jt + X(f (wt, Xi) - yi)Vwf (wt, Xi).	(8)
i=1	i=1
In the case when f is only mildly nonlinear w.r.t. w at data point Xi’s, V2wf(wt, Xi) ≈ 0, and Ht is
close to the real Hessian. In this situation, the behavior of the Gauss-Newton method is similar to
that of Newton’s method, and thus can achieve a superlinear convergence rate (Golub, 1965).
3	The Gram-Gauss-Newton Method
The classic second-order methods using approximate Hessian such as Gauss-Newton method de-
scribed in the previous section face obvious difficulties dealing with the intractable approximate
Hessian matrix when the regression model is an overparameterized neural network.
In Section 3.1, we develop a Gram-Gauss-Newton (GGN) method which is inspired by NTK kernel
regression and does not require the computation of the approximate Hessian. In Section 3.2, we show
that for sufficiently wide neural networks, GGN has quadratic convergence rate. In Section 3.3, we
show that the additional computational cost (per iteration) of GGN compared to SGD is small.
3.1	The Gram-Gauss-Newton Method for Overparameterized Neural Networks
We now describe our GGN method to learn overparameterized neural networks for regression
problems. As mentioned in the previous sections, for sufficiently wide networks, using gradient
descent for solving the regression problem (1) has similar dynamics as using gradient descent for
solving NTK kernel regression (Lemma 1) w.r.t. NTK at each step. However, one can also solve
the kernel regression problem w.r.t. the NTK at each step immediately using the explicit formula
of kernel regression. By explicitly solving instead of using gradient descent to solve NTK kernel
regression, one can expect the optimization to get accelerated. We propose our Gram-Gauss-Newton
(GGN) method to directly solve the NTK kernel regression with Gram matrix Gt at each time
step t. Note that the feature map of NTK at time t, based on the approximation in Eq. (4), can be
expressed as x 7→ Vwf(wt, X), and the linear parameters in RKHS are w - wt, also the target is
f(w, Xi) - f(wt, Xi). Therefore, the kernel (ridgeless) regression solution (Mohri et al., 2018; Liang
& Rakhlin, 2018) of Jt,S (w - wt) = (f (w, Xi) - f(wt, Xi)) w.r.t. (w - wt) gives the update
wt+1 = wt - Jt>,S Gt-,S1 (ft,S - yS),	(9)
where Jt,S is the matrix of features at iteration t computed on the training data set S which is equal
to the Jacobian , ft,S and yS are the vectorized outputs of neural network and the corresponding
targets on S respectively, and
Gt,S = Jt,SJt>,S
4
Under review as a conference paper at ICLR 2020
is the Gram matrix of the NTK on S.
One may wonder what is the relation between our derivation from NTK kernel regression and the
Gauss-Newton method. We point out that for overparameterized models, there are infinitely many
solutions of Eq. (5) but our update rule (9) essentially uses the minimum norm solution. In other
words, the GGN update rule re-derives the Gauss-Newton method with the minimum norm solution.
This somewhat surprising connection is due to the fact that in kernel learning, people usually choose
a kernel with powerful expressivity, i.e. the dimension of feature space is large or even infinite.
However, by the representer theorem (Mohri et al., 2018), the solution of kernel (ridgeless) regression
lies in the n-dimensional subspace of RKHS and minimizes the RKHS norm. We refer the readers to
Chapter 11 of Mohri et al. (2018) for details.
As mentioned in Section 1, the design of learning algorithms should consider not only optimization
but also generalization. It has been shown that using mini-batch instead of full batch to compute
derivatives is crucial for the learned model to have good generalization ability (Hardt et al., 2015;
Keskar et al., 2016; Masters & Luschi, 2018; Mou et al., 2017; Zhu et al., 2018). Therefore, we
propose a mini-batch version of GGN. The update rule is the following:
wt+1 = wt - Jt>,Bt Gt-,B* 1 t (ft,Bt - yBt),	(10)
where Bt is the mini-batch used at iteration t, Jt,Bt and Gt,Bt are the Jacobian and the Gram
matrix computed using the data of Bt respectively, and ft,Bt , yBt are the vectorized outputs and the
corresponding targets on Bt respectively. Gt,Bt = Jt,Bt Jt>,B is a very small matrix when using a
typical batch size.
One difference between Eq. (10) and Eq. (7) is that our update rule only requires to compute the
Gram matrix Gt,Bt and its inverse. Note that the size of Gt,Bt is equal to the size of the mini-batch
and is typically very small. So this also greatly reduces the computational cost.
Using the idea of kernel ridge regression (which can also be viewed as Levenberg-Marquardt
extension (Levenberg, 1944) of Gauss-Newton method), we introduce the following variant of GGN:
wt+1 = wt - Jt>,Bt(λGt,Bt + αI)-1(ft,Bt -yBt),	(11)
where λ > 0 is another hyper-parameter controlling the learning process. Our algorithm is formally
described in Algorithm 1.
Algorithm 1 (Mini-batch) Gram-GaUss-NeWton Method
1:	Input: Training dataset S. Hyper-parameters λ and α.
2:	Initialize the netWork parameter w0 . Set t = 0.
3:	for each iteration do
4:	Fetch a mini-batch Bt from the dataset.
5:	CalcUlate the Jacobian matrix Jt,Bt .
6:	CalcUlate the Gram matrix Gt,Bt = Jt,Bt Jt>,B .
7:	Update the parameter by wt+1 = wt - Jt>,Bt (λGt,Bt + αI)-1(ft,Bt - yBt).
8:	t = t + 1.
9:	end for
3.2 Convergence Analysis for Overparameterized Neural Networks
In this sUbsection, We shoW that for tWo-layer neUral netWorks, if the Width is sUfficiently large, then:
(1) FUll-batch GGN converges With qUadratic convergence rate. (2) Mini-batch GGN converges
linearly. (For clarity, here We only present a proof for tWo-layer neUral netWorks, bUt We believe
that it is not hard for the conclUsion to be extended to deep neUral netWorks Using the techniqUes
developed in DU et al. (2018a); ZoU & GU (2019)).
As We explained throUgh the lens of NTK, the resUlt is a conseqUence of the fact that for Wide enoUgh
neUral netWorks, if the Weights are initialized according to a sUitable probability distribUtion, then
With high probability the oUtpUt of the netWork is close to a linear fUnction W.r.t. the parameters (bUt
nonlinear W.r.t. the inpUt of the netWork) in a neighborhood containing the initialization point and
a global optimUm. AlthoUgh the neUral netWorks Used in practice are far from that Wide, this still
motivates Us to design the GGN algorithm.
5
Under review as a conference paper at ICLR 2020
Neural network structure. We use the following two-layer network
f(w, x) =
√M a>σ(W>x)
1
√M
M
arσ(wrx),
r=1
(12)
where X ∈ Rd is the input, M is the network width, W = (w>, ∙∙∙ , WM)>and σ(∙) isthe activation
function. Each entry of W is i.i.d. initialized with the standard Gaussian distribution Wr 〜N(0, Id)
and each entry of a is initialized from the uniform distribution on {±1}. Similar to Du et al. (2018b),
we only train the network on parameter W just for the clarity of the proof. We also assume the
activation function σ(∙) is '-Lipschitz and β-smooth, and ' and β are regarded as O(1) absolute
constants.
The key finding, as pointed out in Jacot et al. (2018); Du et al. (2018b;a), is that under such
initialization, the Gram matrix G has an asymptotic limit, which is, under mild conditions (e.g. input
data not being degenerate etc., see Lemma F.2 of Du et al. (2018a)), a positive definite matrix
K(Xi, Xj) = Ew〜N(0,I) [x>Xjσ0(wXi)σ0(wXj)] .	(13)
Assumption 1 (Least Eigenvalue of the Limit of Gram Matrix). We assume the matrix K defined in
(13) above is positive definite, and denote its least eigenvalue as
λ0 = λmin(K) > 0.
Now we are ready to state our theorem of full-batch GGN:
Theorem 1 (Quadratic Convergence of Full-batch GGN on Overparameterized Neural Networks).
Assume Assumption 1 holds. Assume the scale of the data is kXi k2 = O(1), |yi| = O(1) for
i ∈ {1, ∙∙∙ ,n}. Ifthe network width
M = Ω (max (n4, n2dlog^δ))),
then with probability 1 - δ over the random initialization, the full-batch version of GGN whose
update rule is given in Eq. (9) satisfies the following:
1)	The Gram matrix Gt,S at each iteration is invertible;
2)	The loss converges to zero in a way that
C
kft+1 - yk2 ≤ √M kft - yk2	(14)
for some C that is independent of M, which is a second-order convergence.
For the mini-batch version of GGN, by the analysis of its NTK limit, the algorithm is essentially
doing serial subspace correction (Xu, 2001) on subspaces induced by mini-batch. So mini-batch
GGN is similar to the Gauss-Siedel method (Golub & Van Loan, 1996) applied to solving systems of
linear equations, as shown in the proof of the following theorem. Similar to the full batch situation,
GGN takes the exact solution of the “kernel regression problem on the subspace” which is faster than
just doing a gradient step to optimize on the subspace. Moreover, we note that existing results of the
convergence of SGD on overparameterized networks usually use the idea that when the step size is
bounded by a quantity related to smoothness, the SGD can be reduced to GD. However, our analysis
takes a different way from the analysis of GD, thus does not rely on small step size.
In the following, we denote G0 ∈ Rn×n as the initial Gram matrix. Let n = bk, where b is the batch
size and k is the number of batches, and let
G0,ij :=G0((i-1)b+1:ib,(j-1)b+1:jb)
be the (i, j)-th b × b block of G0. We define the iteration matrix
where	D=	G0,11 0 . . .	A 0 G0,22	∙ ∙ ..	.. ..	L>(D - L)-1 ∈ Rn×n,			0 0 .. . .	•	0- ・ 0 . ..	(15) ,
				0 0 . . .	,L =-	-0 G0,21 . . .			
		0	0	...	G0,kk		G0,k1	G0,k2	.. 0	
6
Under review as a conference paper at ICLR 2020
represents the block-diagonal and block-lower-triangular parts of G0 . We will show that the conver-
gence of mini-batch GGN is highly related to the spectral radius of A. To simplify the proof, we
make the following mild assumption on A:
Assumption 2 (Assumption on the Iteration Matrix). Assume the matrix A defined in (15) above is
diagonalizable. So we choose an arbitary diagonalization of A as A = P-1QP and denote
μ ：= kPk2∣∣P-1∣∣2.
We note that Assumption 2 is only for the sake of simplicity. Even if it does not hold, an infinitesimally
small perturbation can make any matrix diagonalizable, and it will not affect the proof.
Now we are ready to state the theorem for mini-batch GGN.
Theorem 2 (Convergence of Mini-batch GGN on Overparameterized Neural Networks). Assume
Assumption 1 and 2 hold. Assume the scale of the data is kxik2 = O(1), |yi| = O(1) for i ∈
{1, ∙∙∙ ,n}. We use the mini-batch version of GGN whose update rule is given in Eq.(10), and the
batch Bt is chosen sequentially and cyclically with a fixed batch size b and k = n/b updates per
epoch. If the network width
M = max Ω
(μ2n18) α( n2d log(16n∕δ)))
then with probability 1 - δ over the random initialization, we have the following:
1)	The Gram matrix Gt,Bt at each iteration is invertible;
2)	The loss converges to zero in a way that after T epochs, we have
T
IIfTk- yk2 ≤ μ√n ( 1 - ω
(16)
Proof sketch for Theorem 1 and 2. Denote Jt = J(Wt) and
Jt,t+1 =Z 1 J((1 - s)Wt + sWt+1)ds.
0
For the full-batch version, we have
Ift+1 - yI2 = Ift - y + Jt,t+1(vec(Wt+1) - vec(Wt))I2
= ∣∣ft - y - Jt,t+1Jt>Gt-1(ft - y)∣∣2
= ∣∣(Jt - Jt,t+1)Jt>Gt-1(ft - y)∣∣2
≤ IJt - Jt,t+1I2 ∣∣Jt> ∣∣2 ∣∣Gt-1∣∣2 Ift - yI2 .	(17)
Then we control the first term in Eq. (17) in a way similar to the following:
ʌ ʌ
C^	C^ ,.  ........... 一
kJt - Jt,t+1k2 ≤ √M kWt - Wt+1k2 ≤ √M lljt l∣2 IIGt l∣2 kft - yk2 ,
and if ∣Jt> ∣2 ∣Gt-1 ∣2 can be upper bounded, we get our result Eq. (14).
For the mini-batch version, similarly we have
ft+1 - y = ft - y - Jt,t+1Jt>,BtGt-,B1t(ft,Bt - yBt)
=ft - y - Jt,t+1J>BtG-Bt (ft- y)，	(18)
where the subscript Bt denotes the sub-matrix/vector corresponding to the batch, and G-Bt ∈ Rb×n
is a zero-padded version of Gt-,B1 to make Eq. (18) hold. Therefore, after one epoch (from the
(tk + 1)-th to the ((t + 1)k)-th update), we have
f(t+1)k - y
tk
Y
t0=(t+1)k-1
I- Jt0,t0 + 1J>,BtoG-,Bto) ) (ftk - y)=: Atlftk- y).
7
Under review as a conference paper at ICLR 2020
We will see that the matrix At is close to the matrix A defined in (15), so it boils down to analyzing
the spectral properties of A.
For both theorems, we can compute that as M increases, the norm of the update kWt - Wt+1 kF
does not increase with M, so the update is small compared to the Gaussian initialization where
∣∣W0k2 = Θ(√M). From this We can derive that the matrices J, G etc. remain close to their
initialization, which makes bounding their norms possible. The full proof is in the appendix. □
In conclusion, the accelerated convergence is related to the local linearity and the stability of the
Jacobian and Gram matrix. We emphasize that our theorems serve more as a motivation than a
justification of our GGN algorithm, because we expect that GGN works in practice, even under
milder situations when M is not as large as the theorem demands or for deep networks with different
architectures, and that GGN would still perform much better than first-order methods.
3.3 Analysis of the Per-iteration Computational Complexity
We have proved that for sufficiently overparametrized deep neural networks, full-batch GGN has
quadratic convergence rate. In this subsection, we analyze the per-iteration computational cost of
GGN, and compare it to that of SGD.
For every mini-batch (i.e., iteration), there are two major steps of computation in GGN:
•	(A). Forward, and then backpropagate for computing the Jacobian matrix J.
•	(B). Use J to compute the update J>(λG + αI)-1 (f - y)
We show that the computational complexity of (A) is the same as that of SGD with the same batch
size; and the computational complexity of (B) is small compared to (A) for typical networks and batch
sizes. Thus, the per-iteration computation overhead of GGN is very small compared to SGD. Overall,
in terms of training time, GGN can be much faster than SGD.
For the computation in step (A), the forward part is just the same as that of SGD. For the backward
part, for every input data, GGN keeps track of the output’s derivative for the nodes in the middle of
the computational graph. This part is just the same as backpropagation in SGD. What is different is
that GGN also, for every input data, keeps track of the output’s derivative for the parameters; while in
SGD the derivatives for the parameters are averaged over a batch of data. However, it is not difficult
to see the computational costs of GGN and SGD are the same.
For the computation in step (B), observe that the size of the Jacobian is b × m where b is the
batch size and m is the number of parameters. The Gram matrix Gt,Bt = Jt,Bt Jt>,B in our Gram-
Gauss-Newton method is of size b × b and it only requires O(b2m + b3) for computing Gt,Bt and a
matrix inverse. Multiplying the two matrices to f - y requires even less computation. Overall, the
computational cost in step (B) is small compared to that of step (A). 4
4	Experiments
Given the theoretical findings above, in this section, we compare our proposed GGN algorithm with
several baseline algorithms in real applications. In particular, we mainly study two regression tasks,
AFAD-LITE (Niu et al., 2016) and RSNA Bone Age (rsn, 2017).
4.1 Experimental setting
AFAD-LITE task is to predict the age of human from the facial information. The training data
of the AFAD-LITE task contains 60k facial images and the corresponding age for each image. We
choose ResNet-32 (He et al., 2016) as the base model architecture. During training, all input images
are resized to 64 * 64. We study two variants of the ResNet-32 architecture: ResNet-32 with batch
normalization layer (referred to as ResNetBN), and ResNet-32 with Fixup initialization (Zhang et al.,
2019b) (referred to as ResNetFixup). In both settings, we use SGD as our baseline algorithm. In
particular, we follow Qian (1999) to use its momentum variant and set the hyper-parameters lr=0.003
and momentum=0.9 determined by selecting the best optimization performance using grid search.
Since batch normalization is computed over all samples within a mini-batch, it is not consistent with
8
Under review as a conference paper at ICLR 2020
0.0 l
5 4 3 2 I
♦ ♦ ♦ ♦ ♦
Ooooo
SSoq 演
Time
SSerJ 演
0.0 l
0
(a) Loss-time curve on AFAD-LITE
Time
(c) Loss-time curve on RSNA Bone Age
20	40	60	80	100
Epochs
(b) Loss-epoch curve on AFAD-LITE
0.30-
0.25-
S 0.20-
^0.15-
：s
10.10-
0.05-
0.00-...........................................
0：0	0.5 LO 1.5	2.0	2.5	3.0	3.5	4：0
Epochs
(d) Loss-epoch curve on RSNA Bone Age
Figure 1:	Training curves of GGN and SGD on two regression tasks.
our assumption in Section 2 that the regression function has the form of f(w, x), which only depends
on w and a single input datum x. For this reason, the GGN algorithm does not directly apply to
ResNetBN, and we test our proposed algorithm on ResNetFixup only. We set λ = 1 and α = 0.3 for
GGN. We follow the common practice to set the batch size to 128 for our proposed method and all
baseline algorithms. Mean square loss is used for training.
RSNA Bone Age task is a part of the 2017 Pediatric Bone Age Challenge organized by the
Radiological Society of North America (RSNA). It contains 12,611 labeled images. Each image in
this dataset is a radiograph of a left hand labeled with the corresponding bone age. During training, all
input images are resized to 64 * 64. We also choose ResNetBN and ResNetFiXuP for this experiment,
and use ResNetBN and ResNetFixup trained in the first task as warm-start initialization. We use
lr= 0.01 and momentum= 0.9 for SGD, and use λ = 1 and α = 0.1 for GGN. Batch size is set to
128 in these experiments, and mean square loss is used for training.
4.2 Experimental results
Convergence. The training loss curves of different optimization algorithms for AFAD-LITE and
RSNA Bone Age tasks are shown in Figure 1. On both tasks, our proposed method converges much
faster than the baselines. We can see from Figure 1a and Figure 1b that, on the AFAD-LITE task, the
loss using our GGN method quickly decreases to nearly zero in 30 epochs. On the contrary, for both
baselines using SGD, the loss decays much slower than our method in terms of wall clock time and
epochs. Similar advantage of GGN can also be observed on the RSNA bone age task.
Generalization performance and different hyper-parameters. We can see that our proposed
method trains much faster than other baselines. However, as a machine learning model, generalization
performance also needs to be evaluated. Due to space limitation, we only provide the test curve
for the RSNA Bone Age task in Figure 2a. From the figure, we can see that the test loss of our
proposed method also decreases faster than the baseline methods. Furthermore, the loss of our GGN
algorithm is lower than those of the baselines. These results show that the GGN algorithm can not
only accelerate the whole training process, but also learn better models.
9
Under review as a conference paper at ICLR 2020
Time
(a)
0.200、	.....
—λ = l,a = 0.01
0.175- ∖	— λ = i,a = 0.1
Epochs
(b)
Figure 2:	Test performance and ablation study on hyper-parameters on RSNA Bone Age dataset.
(a) Test curves of GGN and SGD. (b) Training curves of GGN with different hyper-parameter
configurations. The optimal α is shown for λ = 0.25 and λ = 4 by grid search. (c) Training loss at
10th epoch of models trained using GGN with different hyper-parameters.
We then study the effect of hyper-parameters used in the GGN algorithm. We try different λ and α
on the RSNA Bone Age task and report the training loss of all experiments at the 10th epoch. All
results are plotted in Figure 2c. In the figure, the x-axis is the value of λ and the y-axis is the value of
α. The gray value of each point corresponds to the loss, the lighter the color, the higher the loss. We
can see that the model converges faster when λ is close to 1. In GGN, α can be considered as the
inverse value of the learning rate in SGD. Empirically, we find that the convergence speed of training
loss is not that sensitive to α given a proper λ, such as λ = 1. Some training loss curves of different
hyper-parameter configurations are shown in Figure 2b.
5 Conclusion and Discussions
We propose a novel Gram-Gauss-Newton (GGN) method for solving regression problems with
square loss using overparameterized neural networks. Despite being a second-order method, the
computation overhead of the GGN algorithm at each iteration is small compared to SGD. We also
prove that if the neural network is sufficiently wide, GGN algorithm enjoys a quadratic convergence
rate. Experimental results on two regression tasks demonstrate that GGN compares favorably to
SGD on these data sets with standard network architectures. Our work illustrates that second-order
methods have the potential to compete with first-order methods for learning deep neural networks
with huge number of parameters.
In this paper, we mainly focus on the regression task, but our method can be easily generalized to
other tasks such as classification as well. Consider the k-category classification problem, the neural
network outputs a vector with k entries. Although this will increase the computational complexity
of getting the Jacobian whose size increases k times, i.e., J ∈ R(bk)×m, each row of J can be still
computed in parallel, which means the extra cost only comes from parallel computation overhead
when we calculate in a fully parallel setting. While most first-order methods for training neural
networks can hardly make use of the computational resource in parallel or distributed settings to
accelerate training, our GGN method can exploit this ability. For first-order methods, basically extra
computational resource can only be used to calculate more gradients at a time by increasing batch size,
which harms generalization a lot. But for GGN, more resource can be used to refine the gradients and
achieve accelerated convergence speed with the help of second-order information. It is an important
future work to study the application of GGN to classification problems.
References
Rsna pediatric bone age challenge. http://rsnachallenges.cloudapp.net/competitions/4, 2017.
Joshua Achiam, Ethan Knight, and Pieter Abbeel. Towards characterizing divergence in deep
q-learning. arXiv preprint arXiv:1903.08894, 2019.
10
Under review as a conference paper at ICLR 2020
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018b.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019b.
Sue Becker, Yann Le Cun, et al. Improving the convergence of back-propagation learning with
second order methods. 1988.
Alberto Bernacchia, Mate Lengyel, and Guillaume Hennequin. Exact natural gradient in deep linear
networks and its application to the nonlinear case. In Advances in Neural Information Processing
Systems,pp. 5941-5950, 2018.
Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical gauss-newton optimisation for deep
learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 557-565. JMLR. org, 2017.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep
neural networks. arXiv preprint arXiv:1905.13210, 2019.
Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming.
arXiv preprint arXiv:1812.07956, 2018.
Xialiang Dou and Tengyuan Liang. Training neural networks as learning data-adaptive kernels:
Provable representation and approximation benefits. arXiv preprint arXiv:1901.07114, 2019.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018b.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points - online stochastic
gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory,
pp. 797-842, 2015.
Thomas George, Cesar Laurent, Xavier Bouthillier, NiColas Ballas, and Pascal Vincent. Fast
approximate natural gradient descent in a kronecker factored eigenbasis. In Advances in Neural
Information Processing Systems, pp. 9550-9560, 2018.
Gene Golub. Numerical methods for solving linear least squares problems. Numerische Mathematik,
7(3):206-216, 1965.
Gene H. Golub and Charles F. Van Loan. Matrix computations (3rd ed.). Johns Hopkins University
Press, Baltimore, MD, USA, 1996. ISBN 0801854148.
Roger Grosse and James Martens. A kronecker-factored approximate fisher matrix for convolution
layers. In International Conference on Machine Learning, pp. 573-582, 2016.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
11
Under review as a conference paper at ICLR 2020
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape
saddle points efficiently. In Proceedings of the 34th International Conference on Machine Learning,
pp. 1724-1732, 2017a.
Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient descent escapes saddle
points faster than gradient descent. arXiv preprint arXiv:1711.10456, 2017b.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jeffrey
Pennington. Wide neural networks of any depth evolve as linear models under gradient descent.
arXiv preprint arXiv:1902.06720, 2019.
Kenneth Levenberg. A method for the solution of certain non-linear problems in least squares.
Quarterly of applied mathematics, 2(2):164-168, 1944.
Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel” ridgeless” regression can generalize.
arXiv preprint arXiv:1808.00387, 2018.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265,
2019.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. arXiv preprint arXiv:1902.09843, 2019.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
James Martens, Jimmy Ba, and Matt Johnson. Kronecker-factored curvature approximations for
recurrent neural networks. 2018.
Dominic Masters and Carlo Luschi. Revisiting small batch training for deep neural networks. arXiv
preprint arXiv:1804.07612, 2018.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural
networks: dimension-free bounds and kernel limit. arXiv preprint arXiv:1902.06015, 2019.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
2018.
Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. Generalization bounds of sgld for non-convex
learning: Two theoretical viewpoints. arXiv preprint arXiv:1707.05947, 2017.
Zhenxing Niu, Mo Zhou, Le Wang, Xinbo Gao, and Gang Hua. Ordinal regression with multiple
output cnn for age estimation. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 4920-4928, 2016.
Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent
takes the shortest path? arXiv preprint arXiv:1812.10004, 2018.
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv preprint
arXiv:1301.3584, 2013.
Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1):
145-151, 1999.
12
Under review as a conference paper at ICLR 2020
Yi Ren and Donald Goldfarb. Efficient subsampled gauss-newton and natural gradient methods for
training neural networks. arXiv preprint arXiv:1906.02353, 2019.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge
Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019. doi:
10.1017/9781108627771.
Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region
method for deep reinforcement learning using kronecker-factored approximation. In Advances in
neural information processing Systems, pp. 5279-5288, 2017.
Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on
Computer Vision (ECCV), pp. 3-19, 2018.
Jinchao Xu. The method of subspace corrections. Journal of Computational and Applied Mathematics,
128(1-2):335-362, 2001.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
Guodong Zhang, James Martens, and Roger Grosse. Fast convergence of natural gradient descent for
overparameterized neural networks. arXiv preprint arXiv:1905.10961, 2019a.
Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without
normalization. arXiv preprint arXiv:1901.09321, 2019b.
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic
gradient descent: Its behavior of escaping from minima and regularization effects. arXiv preprint
arXiv:1803.00195, 2018.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. arXiv preprint arXiv:1906.04688, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.
A Some Preparations for the Proof of Theorems
Notations. We use the following notations for the rest of the sections.
•	Let [n] = {1,…，n}.
•	Jw,χ ∈ RM×d denotes the gradient df(W,x), which is of the same size of W.
•	The bold JW or J(W) denotes the Jacobian with regard to all n data, with each gradient
for W vectorized, i.e.
vec(JW,x1)>
JW =	:	∈ Rn×(Md).	(19)
.
vec(JW,xn)>
•	wr denotes the r-th row of W, which is the incoming weights of the r-th neuron.
•	W0 stands for the parameters at initialization.
•	dW,x := σ0(Wx) ∈ RM×1 denotes the (entry-wise) derivative of the activation function.
•	We use h∙, ∙i to denote the inner PrOdUCt, ∣∣∙k2 to denote the Euclidean norm for vectors or
the spectral norm for matrices, and ∣∣∙k f to denote the Frobenius norm for matrices.
13
Under review as a conference paper at ICLR 2020
(20)
(21)
We can easily derive the formula for J as
JW,x = √M ((dW,x ◦ a)x>),
where ◦ is the point-wise product. So we can also easily solve G as
1M
Gij = hJXi , Jxji = M ESXI Xj σ (Wr Xi)σ (Wr Xj ).
r=1
Our analysis is based on the fact that G stays not too far from its infinite-width limit
K(Xi, Xj) = Ew〜N(0,Id) (x>Xjσ0(wXi)σ0(wXj)),
which is a positive definite matrix with least eigenvalue denoted λ0, and we assume λ0 > 0. λ0 is a
small data-dependent constant, and without loss of generality we assume λ0 ≤ 1, or else we can just
take λ0 = 1 if λmin(K) > 1.
The first lemma is about the estimation of relevant norms at initialization.
Lemma 2 (Bounds on Norms at Initialization). If M = Ω (dlog(16n∕δ)), then with probability at
least 1 一 δ∕2 the following holds
(a)	. kW0k2 = O(√M).
(b)	. f(W0,Xi) = O(1), for i ∈ [n].
(c)	. kJW0,xikF = O(1), for i ∈ [n].
Proof. (a). The lemma is a well-known result concerning the estimation of singular values of
Gaussian random matrices (see Corollary 5.35 of Vershynin (2010)). Notice that W0 ∈ RM×d is a
Gaussian random matrix, the Corollary states that with probability 1 一 2e-t2/2 one has
kW0k2 ≤ √M + √d +1.
By choosing M = max(d, P log(8∕δ)), We obtain k W0k2 ≤ 3√M with probability 1 一 δ∕4.
(b)	. First, ar, r ∈ [M] are Rademacher variables, thereby 1-sub-Gaussian, so with probability
1 一 2e-Mt2/2 We have 吉 PM=I a『≤ t. This means if We take M = Ω (log(16∕δ)),
1M	δ
Pr[√M X ar = O(1)] ≥ 1 - 8.
r=1
(22)
Next, the vector Vi = Ijl WoXi ∈ RM×1 is a standard Gaussian vector. Suppose the activation
kxi k2
σ(∙) is l-Lipschitz and l is O(1) by our assumption, with the vector a fixed, the function
φ : RM → R, Vi → √= a>σ(∣∣Xik2 Vi) = f (Wo, Xi)
has a Lipschitz parameter of l IlXik2 ∕√M = O(1∕√M). According to the classic result on the
concentration of a Lipschitz function over Gaussian variables (see Theorem 2.26 of WainWright
(2019)), We have
Pr[∣φ(vi) — Ewo(φ(vi))∣ ≥ t] ≤ 2 exp -
Mt2
2l2 kXik2
which means if M = Ω(log(16n∕δ)),
1	>	1 M
√Ma G(W0Xi) - √M (Z ar )Ew〜N(0,Id)[σ(WXi)] = O(I)
holds jointly for all i ∈ [n] with probability 1 一 δ∕8. Note that
IEw〜N(0,Id)[σ(WXi)]∣ ≤ |b(0)| + l ∙ Eξ〜N(0,kxik2))[lξl] = O(1).
(23)
(24)
14
Under review as a conference paper at ICLR 2020
Plugging in (22) and (24) into (23), We see that as long as M = Ω(log(16n∕δ)), then with probability
1 - δ∕4, for all i ∈ [n],
f(W0,xi)
1>
σ(W0xi) = O(1).
(c)	. Since σ is O(1)-Lipschitz, we have kdW,xi k∞ = O(1). According to (20) we can easily know
that k Jw,χi∣∣F ≤ √M IlDiag(d)∣∣2 IIaIl2 ||x||； = O(1).	□
The next lemma is about the least eigenvalue of the Gram matrix G at initialization. It shows that
when M is large, GW0 is close to K and has a lower bounded least eigenvalue. It is the same as
Lemma 3.1 in Du et al. (2018b), but here we restate it and its proof for the reader’s convenience.
Lemma 3 (Bound on the Least Eigenvalue of the Gram Matrix at Initialization). If the width
M = Ω (n loλ2n∕δ)), then with probability at least 1 一 δ∕2 over random initialization, we have
3
λmin(Gwo ) ≥ 4λ0.
Proof. Because σ is Lipschitz, σ0(wxi)σ0(wxj) is bounded by O(1). For every fixed (i, j) pair,
at initialization Gij is an average of independent random variables, and by Hoeffding’s inequality,
applying union bound for all n2 of (i,j) pairs, with probability 1 一 δ∕2 at initialization we have
IGij- Kij | ≤ O (r Iog(Mn/δ))
and then
∣Gwo - Kk2 ≤〔1Gw。一 KkF = O (n2logM2n∕δ)).
Thus if M = Ω ( n log(2n∕6 ) we can have ||Gw。一 K∣∣2 ≤ 1 λo and thus λmin(Gwo) ≥ 4 λ°. □
Next, in Lemma 4 and 5 we will bound the relevant norms and the least eigenvalue of G inside
some scope of W that covers the whole optimization trajectory starting from W0 . Specifically, we
consider the range
B(R)，{W ：||W - WoIf ≤ R},
where R is determined later to make sure that the optimization trajectory remains inside B(R). The
idea of the whole convergence theorem is that when the width M is large, R is very small compared
to its initialization scale: k W0k2 = O(√M). This way, neither the Jacobian nor the Gram matrix
changes much during optimization.
Lemma 4 (Bounds on Norms in the Optimization Scope). Suppose the events in Lemma 2 hold.
There exists a constant C > 0 such that if M ≥ CR2, we have the following:
(a)	For any W ∈ B(R), we have
∣∣W∣∣2 = O(√M).	(25)
(b)	Forany Wι, W ∈ B(R), if ∣Wι 一 W2∣∣F ≤ R0, then we have
∀i, and IJW1 一 JW2 I2
(26)
(27)
Also, for any W ∈ B(R), we have
IlJWIlF = O(1) and IIJW∣∣2 = O(√n).
15
Under review as a conference paper at ICLR 2020
Proof. (a). This is straightforward from Lemma 2(a), the definition of B(R), and M = Ω(R2).
(b). According to the O(1)-smoothness of the activation, we have
kdWι,Xi - dW2,Xil∣2 ≤ θ⑴∙ ∣∣W1xi - w2xik2 = O(RO),
so we can bound
kJW1 ,xi - JW2,xi kF
=√M UDiag(a)(dWi，Xi — dW2,Xi)x>IIF
≤√M kDiag(a)k2 kdWι,Xi - dW2,Xik2 kxik2
≤O(
And according to (19), we have
kJW1 -JW2k2 ≤ kJW1 -JW2kF
∖
- JW2,xi kF
n
X kJW1,xi
i=1
Also, taking W1 = W and W2 = Wo, combining with Lemma 2(c), we see there exists C such
that for M ≥ CR2 we have ∣∣Jw∣If = O(1), and naturally Jwk2 = O(√n)∙ Note: The constants
hidden in the O(∙) notation are irrelevant with C.	□
The next Lemma deals with the Gram matrix within B(R), ensuring a lower bound on the least
eigenvalue throughout the optimization process (Du et al., 2018a).
Lemma 5 (Least Eigenvalue in the Optimization Scope). For W ∈ B(R), suppose the events in
Lemma 2 and 3 hold, there exists a constant C0 such thatfor M ≥ C^R, we have
kGW - GWo k2 ≤ -40,
and thus combined with Lemma 3, we know that GW remains invertible when W ∈ B(R) and
SatifieS IIGW1II2 ≤ λ0.
Proof. Based on the results in Lemma 4(b), we have
kGW - GW0 k2 = IIJWJ>W - JW0 J>W0 II2	(28)
≤ II(JW - JW0 )J>W)II2 + IIJW0 (J>W - J>W0 )II2	(29)
nR
≤O( √M).	(30)
To make the above less than λ40-, choosing M greater than some Cn2R suffices, and the lemma is
thus proved. Again the constants hidden in the O(∙) notation are irrelevant with C0.	□
B Proof of Theorem 1
Proof idea. In this section, we use Wt,t ∈ {0,1, ∙…} to represent the parameter W after t
iterations. For convenience, Jt, Gt, ft is short for JWt, GWt , fWt respectively. We introduce
Jt,t+1
Z 1 J((1 -
0
s)Wt + sWt+1)ds.
(31)
16
Under review as a conference paper at ICLR 2020
For each iteration, if Gt is invertible, we have
ft+1 - y = ft - y + Jt,t+1 vec(Wt+1 - Wt)
= ft - y - Jt,t+1Jt>Gt-1 (ft - y)
= (Jt - Jt,t+1)Jt>Gt-1 (ft - y),
hence
kft+1 - yk2	≤	kJt	-	Jt,t+1k2	Jt>	2	Gt-1	2	kft	- yk2 .	(32)
Then we control the first term of the right hand side based on Lemma 4 in the following form
C^ ..
kJt - Jt,t+1 k2 ≤ √M kWt - Wt+1 k2
C^ ,, ɪ	,,
=√m IIJt Gt (ft-y)l∣2
C^	,, ɪ,,	,, T ,,
≤ √M IlJt ll2 IlG- ll2 kft -yk2 ,
and plugging into (32) along with norm bounds on J and G we obtain a second-order convergence.
Formal proof. Let Rt = ∣∣Wt - Wt+j∣F for t ∈ {0,1,…}. We take R = Θ (λn) in Lemma 4
and 5 (the constant is chosen to make the right hand side of (34) hold). We prove that there exists an
M = Ω (max (g, n d loλ(216n∕δ)) ) (with enough constant) that suffices. First We can easily verify
that all the requirements forM in Lemma 2-5 can be satisfied . Hence, with probability at least 1 - δ
all the events in Lemma 2-5 hold. Under this situation, we do induction on t to show the following:
• (a). Wt ∈ B(R).
•(b). Ift >0, then kft- yk2 ≤ λn√M kft-ι- yk2
As long as (b) is true for all t, then choosing M large enough to make sure the series {kft - yk2}t∞=0
converges to zero, we obtain the second-order convergence property.
For t = 0, (a) and (b) hold by definition. Suppose the proposition holds for t = 0, ∙∙∙ ,T. Then for
t = 0,…，T, Gt is invertible. Recall thatthe update rule is Vec(Wt+ι) = Vec(Wt) — J>G-1 (ft 一
y), we have
Rt= kWt - Wt+1kF
= kVec(Wt) - Vec(Wt+1)k2
≤IIj>I∣2∣Ig-1I∣2 kft-yk2
≤ O (√n kft-yk2).
(Lemma 4 and 5)
(33)
According to Lemma 2(b) and the assumption that the target label yi = O(1), we have kf0 - yk22
O(n). When T > 1, the decay ratio at the first step is bounded as
kf1-yk2
kf0-yk2
and taking M = Ω(n4) with enough constant can make sure r is a constant less than 1, in which case
the second-order convergence property (b) will ensure a faster ratio of decay at each subsequent step
in kft - yktT=0 . Combining (33), we have
T
X Rt ≤ O
t=0
O ⑺ ≤ r.
(34)
17
Under review as a conference paper at ICLR 2020
Therefore, WT+1 ∈ B(R), and (34) also holds when T = 0, so (a) is true.
Since B(R) is convex, this means we also have sWT + (1 - s)WT +1 ∈ B(R) for s ∈ [0, 1]. Hence
we can bound the difference of the Jacobian as
kJT,T +1 -
JTk2≤Z01
kJ(sWT + (1 -s)WT+1) - J(WT)k2 ds
(Lemma 4(b))
(Using (33))
(35)
So we use the bound of (32) and obtain
kfT +1 - yk2 ≤ kJT - JT,T+1k2 JT>2 GT-12 kfT - yk
n3/2	2
≤ λ0√M kfT -yk2,
(Using (35) and Lemma 4, 5)
which proves (b). This concludes our proof.
C Proof of Theorem 2
In this section, we give the proof of the convergence of our mini-batch GGN algorithm (Theorem 2).
Some additional notations. For the convenience of our proof, we will use slightly different
notations than that in Section 3.2. Let n = bk, where b is the batch size, k is the number of
batches, or equivalently, number of updates in each epoch. For epoch t ∈ {1,2,…} and batch
i ∈ [k], we will use Wti ∈ Rd×M to denote the parameters before the i-th update in epoch t.
Let Jti = J(Wti) ∈ Rn×Md, Gti = G(Wti) ∈ Rn×n, and fti = f(Wti) ∈ Rn×1. (All of
W0,W1,W11, G0, etc. can represent initial values in the proof.) Also, let Wt = Wt1, Jt =
Gt1, Gt = Jt1,ft = ft1, and let Wt(k+1) = W(t+1)1,Jt(k+1) = J(t+1)1, etc. In the i-th batch,
we use the data (xl, yl)li=b (i-1)b+1. To specify the matrix related to a batch, we use the following
indexing method: For i0, i00 ∈ [k], let
Jti,i0 = Jti((i0 - 1)b+1 : i0b, 1 :Md) ∈Rb×(Md),
which is the b rows of Jti that represent the Jacobian of the i-th batch. Similarly, we have
Gti,i0i00 = Jti,i0 Jt>i,i00 ∈ Rb×b,
fti,i0 = fti((i0 - 1)b+1 :ib) ∈ Rb×1,
yi0 = y((i0-1)b+1:ib) ∈ Rb×1.
Similar to (31) in the proof of Theorem 1, we make use of the notation
Jt(i,i+1) = Z J(sWti + (1 - s)Wt(i+1))ds,
0
(36)
and similarly define Jt(i,i+1),i0 ∈ Rb×Md, etc. In addition, we make use of the matrix
Gti = 0b×(i-1)b Gt-i,ii 0b×(k-i)b ∈ Rb×n
in the formula of the update.
Similar to the proof of Theorem 1, the idea of our proof is that due to the fact that the change of W is
small compared to the scale of its initialization, the matrix J and G remains stable. Informally, this
makes the algorithm close to solving kernel regression iteratively by batches, which is equivalent to
solving a system of linear equations
JJ>r = f(W0) - y,	(37)
18
Under review as a conference paper at ICLR 2020
for variables r ∈ Rn×1 (where vec(W) = vec(W0) + J>r), using the Gauss-Siedel method, which
means solving
Γ	r(old)(1 : (i - 1)b)-
JJ>G	r(new)((i - 1)b + 1 :ib) = (f(W0 - y))(i-1)b+1:ib
r(old)((ib + 1 : kb))
for the i-th batch in every epoch. Therefore, it is natural that the matrix A = L>(D - L)-1 in (15)
is introduced. We will show later that the real update follows
ft+1 - yt = At(ft - y)
for some At ≈ A.
In order to prove the theorem, we need some additional lemmas as follows.
Lemma 6 (Formula for the Update). If the Gram matrix at each step is invertible, then:
(a)	The update of W is
vec(Wt(i+1)) =vec(Wti)-Jt>i,iGt-i,1ii(fti,i-yi)	(38)
= Vec(Wti) — J>,iGti(ft- y)	(39)
(b)	The formula for f - y is
ft(i+1) — y =(In - Jt(i,i+1)J>,iGti)(fti - y)	(4O)
(c)	The update of f - y satisfies
ft(i+1) — y = Uti(Dt — Lt)-1 (ft — y),	(41)
where
-Gtι,ιι 0	…	0	-
0	Gt2,22 …	0
Dt =	.	.	.	.
.	..	.
-0	0	…Gtk,kk.
and
Jt(1,2),1Jt>1,1 — Gt1,11
0
Uti=Diag —	.
0	…0
0	… 0
0
Jt(1,2),2Jt1,1
Lt =—	.
Jt(1,2),kJt>1
,1
Jt(i,i+1),1Jt>i,i
Jt(i,i+1),2J>,i
.
.
.
Jt(i,i+1),iJt>i,i — Gti,ii ib×ib
\
Jt(2,3),1Jt>2,2
Jt(2,3),2Jt>2,2 — Gt2,22
Gt(i+1),(i+1)(i+1)
Jt(i+1,i+2),i+2Jt(i+1),i+1
00
0
Gt(i+2),(i+2)(i+2)
0
0
_ Jtci+1,i+2),kJ>i+I),i+1	JtCi+2,i+3),kJ>i+2),i+2	.-	Gtk,kk_ (k-i)b×(k-i)b,
Or, if we denote
Uti
-Uti (∈ Rib×ib)	0	一
0	Uti(∈ R(k-i)b×(k-i)b)],
then we have
Dti — Lti
Dti — Lti (∈ Rib×ib)
1^	^
Dti — Lti
0
Dti- Lti(∈ R(k-i)b×(k-i)b)J ,
ft(i+1) — y
Uti(Dti- Lti)-1
-(Dti- Lti)(Dti - Lti厂
0
I(k-i)b
(ft - y)
(42)
19
Under review as a conference paper at ICLR 2020
Specifically	we have			
	ft+1 - y = Ut(Dt- Lt)-1(ft - y) =		二：At(ft - y),	(43)	
where	jt(1,2),1j>1,1 - 0	Gt1,11	jt(2,3),1jt2,2 jt(2,3),2 jt2,2 - Gt2,22	…	jt(k,k+1),1 Jkk …	jt(k,k+1),2 Jkk	
Ut = -	. . . 0	. . . 0	..	.. .. •…	jt(k,k+1),k jtk,k - Gtk,kk_	.
Proof. (a) For (38), this is exactly the update formula (10) for the i-th batch where the Jacobian and
Gram matrix are Jtii and Gtiii respectively. Note that
Gti,ii(fti,i - yi) = [0b×(i-1)b GJii 0b×(k-i)b] (fti - y),
we then obtain (39) from (38).
(b). We have
(ft(i+i) - y) - (fti - y)
ft(i+1) - fti
jt(i,i+i)(VeC(Wt(i+1)) - VeC(Wti))
Jt(i,i+i)J>,iGti(fti - y) (By formula (39)),
we obtain (40).
(c). Based on (40) we know that
1
ft(i+1) - y = " (In - jt(i0,i0 + 1)j>0,i0 G ti0)(fti - y),
where the index goes in decreasing order from left to right. So in order to prove (41) we only need to
prove that
1
ɪɪ (In - jt(i0,i0 + 1)j>0,i0Gti0)	(Dt- Lt)= Uti,
_i°=i	_
(44)
which we will prove by induction on i. For i = 0 it is trivial that Dt - Lt = Ut0 by definition.
Suppose (44) holds for i, then
1
∏ (In- jt(i，
一i0=i+1
〜
,i0+1) j>0,i0 Gti0 )	(Dt- Lt)
(In- jt(i+1,i+2)j>i+1),i+1Gt(i+I))Uti
Uti- jt(i+1,i+2)j>i+1),i+1G-1+1),(i+1)(i+1)UtiGb +l：(i +l)b, 1： n))
Uti -
jt(i+1,i+2),1j>i+1),i+1
jt(i+1,i+2),2 jt(i+1),i+1
[0b×(ib)	1b×b	0b×(k-i-2)b
jt(i+1,i+2),k j>i+1),i+1-
=Ut(i+1),
which proves (41). Note that by the definition, we have Uti
Uti(Dt - Lt)-1
ɪ-〜
U ti
0
ɪ-〜
U ti
0
n	τ
Dti - Lti
A	τ
Dti - Lti
Dti - Lti, we can then obtain (42) by
-1
- 0_
Dti - Lti
(Dti - Lti)-1
ʌ
ij [-(Dti - Lti)T(D
ʌ
1 τ ʌ∕TΛ
ti - Lti)(Dti
~ ~
-Lti)-1	(Dti
0
-Lti)-I
~	, ~	~	.	-1
Uti(Dti- Lti)-1
ʌ ʌ
_-(Dti - Lti)(Dti
~ ~
0
-Lti)-1 L
□
20
Under review as a conference paper at ICLR 2020
By (43), we can see that the iteration matrix At is close to the matrix A defined in (15). The
convergence of the algorithm is much related to the eigenvalues of A. In the next two lemmas, we
bound the spectral radius of A and provide an auxiliary result on the convergence on perturbed
matrices based on the spectral radius.
Lemma 7. Suppose the least eigenvalue of the initial Gram matrix Go satisfies λmin(G0) ≥ 4 λo,
(which is true with probability 1 一 δ if M = Ω (n lo:fn/δ)), according to Lemma 3). Also assume
kJW0,xl kF = O(1) for any l ∈ [n] (which is true with probability 1 - δ, according to Lemma 2).
Then the spectral radius A, or equivalently, maximum norm of eigenvalues of A, denoted ρ(A),
satisfies
P(A) ≤ 1 - Ω (λ2) .	(45)
Proof. For any eigenvalue λ ∈ C of A, it is an eigenvalue of A>, so there exists a corresponding
unit vector v ∈ Cn×1 such that λv = A>v = (D - L>)-1Lv, which means
λ(D - L> )v - Lv = 0.	(46)
Since Go = D — L 一 L> is positive definite with eigenvalues at least 4λo, We have v*(D — L 一
L>)v ≥ 4λo, which means
3
d 一 2Re(l) ≥ J,	(47)
where d = v*Dv, l = v*Lv, and v*L>v = 7. Let Vi = v((i — 1)b +1 : ib) ∈ Rb×1 be the i-th
batch of the vector v, and Vi = [0ι×(i-i)b, v>, 0ι×(k-i)b]> ∈ Rn×1. Itis not hard to see that
k	kk
d = v*Dv = X vi G0,iivi = X V；GOVi ≥ X 4λ0 Ilvi k2 = 4λ0∙
i=1	i=1	i=1
Also, since by our assumption each entry of L (or of Go) is at most O(1), we have
|l| = v*Lv ≤ kLk2 ≤ kLkF = O(n).
Now we use take an inner product of v with (46) and get
0 = v*(λ(Dv — L>v) — LV) = λ(d — 7)+ l,
(48)
(49)
(50)
and therefore
l
d — 7
∣λ∣
(By solving (50))

I 1
d( d(d-2Re(l))	1
V IΦ H1
(Using (47), (48), (49))
This concludes the proof for this lemma.
Lemma 8. Denote ρ(A) = ρo ≤ 1. Let A be diagonalized as A = PTQP and μ = ∣∣P∣∣2 IlPTII2
(see Assumption 2). Suppose we have IAt — Ao I ≤ δ for t ≤ T, then
T
Y At	≤ μ(ρ + μδ)τ
i=1	I
21
Under review as a conference paper at ICLR 2020
Proof. We have
T
YAt
i=1
T
= Y (A + (At - A0))
2 i=1	2
T
= Y PT(Q + P(At- A0)P-1) P
i=1	2
Y (Q + P(At - Ao)P-1) ) P
i=1	2
≤ kPk2 YT (kQk2 + kPk2 kAt -A0k2 kPk2)	P-12
≤ μ(ρ + μδ)τ.
□
In addition to the bounds used in Theorem 1, we provide the next lemma with useful bounds of the
norms and eigenvalues of relevant matrices in the optimization scope
B(R)={W: kW-W0kF ≤R}.
Lemma 9 (Relevant Bounds for the Matrices in the Optimization Scope). We assume the events in
Lemma 2 and 3 hold, and let M = CμR R for some large enough constant C, then:
For any (t, i)-pair, we assume Wt0i0 ∈ B(R) for all i0 ∈ [k] when t0 ≤ t and i0 ∈ [i] when t0 = t in
the following propositions.
(a)	. minv∈Rn,∣M∣2=1 k(D - L)v∣∣2 ≥ 3λ°.
(b)	. Suppose up to t, Wt is in B(R) (which means for i0 ∈ [k + 1], and t0 ∈ [t - 1], Wti ∈ B(R)).
Then kAt-Ak2 ≤1 ρμA).
Proof. (a). Because for kvk2 = 1,
k(D - L)vk2 ≥v>(D-L)v
=gv>Dv + gv>(D — L — L>)v
≥	0 + 2λmin(GO)
3
≥	ξλo.	(By the Positive-definiteness of Go and Lemma 3)
(b). By Lemma 4 we know that within B(R), the size of the Jacobian kJxl kF w.r.t. data xl is O(1),
and the difference k Jwι,χ1 — Jw2,x1 ∣∣f within O(R) is bounded by O ^√=), this can be applied
to each entry of D, L, U, etc., including those Jt0 (i0,i0+1) terms by the convexity of B(R), and we
can see that each entry of these matrices has size at most O(1) and varies inside an O ^√=) range.
Therefore we get
kUtkF = O(n),
kDt - LtkF = O(n),
and
kUt - Uk2=O( √RM),
k(Dt- Lt)-(D - L)k2 = O (√M).
22
Under review as a conference paper at ICLR 2020
By our chosen M, O (√n) ≤ 1 λo can definitely hold, so along with (a) We have
(D - L)-12
Hence,
kAt - Ak2 = (Ut(Dt - Lt)-1 - U(D - L)-12
=	(Ut - U)(Dt - Lt)-1	+	U((Dt	-Lt)-1((Dt-Lt)	- (D - L))(D	-	L)-12
and along with Lemma 7, in order to have ∣∣ At - Ak2 ≤ 1*A), all we need is
n2 R
λ2√M
so choosing some M =。^^8铲 suffices.	口
With all the preparations, now we are ready to prove Theorem 2. The logic of the proof is just the
same as what we did in the formal proof of Theorem 1, where we then used induction on t and now
we use induction on the pair (t, i). The key, is still selecting some R so that in each step Wti remains
in B(R). Combined with the previous Lemmas, in B(R) we have At being close to A, and then
convergence is guaranteed.
Formal proof of Theorem 2. Let Rti = Wt(i+1) - Wti F. We take
R=Θ
in the range B(R) (where the constant is chosen to make the right hand side of (52) hold). We prove
that there exists an
M = max Ω
n n2d log(16n∕δ)
, ( -λ2—
with a large enough constant that suffices. First we can easily verify that all the requirements for M
in Lemma 2-9 (most importantly, Lemma 9) can be satisfied. Hence, with probability at least 1 - δ all
the events in Lemma 2-9 hold. Under this situation, we do induction on (t,i) (t ∈ {1,2,…},i ∈ [k],
by dictionary order) to show that:
• Wti ∈ B(R).
For (t, i) = (1, 1), it holds by definition.
Suppose the proposition holds up to (t, i). Then since Wti ∈ B(R), by Lemma 4 we know that
λmin(Gti) ≥ λ0. This naturally gives us λmin(Gti,ii) ≥ λ0, which means Gti,ii is invertible.
Similar to the argument in the proof of Lemma 9, we know that each entry of
Jti, Jt(i,i+1), Dti, Lti, Uti, etc., whose index of (t0, i0) only contains i0 with i0 ≤ i, is of scale
O ⑴ and varies at most O (-R=), which is a result of Lemma 4. Then we know
∣∣Jti,ikF = O( VZb) = O(√n),
-Lt(i-1) IF = O(n),
-Lt(i-1) |[ = O(n),
23
Under review as a conference paper at ICLR 2020
etc. We then also know
∕τ∖	f ʌ
(Dt(i-1) - Lt(i-1))-
(Dt- Lt)
(1:(i-l)b,l:(i-1)b) 11
Since we can also have Il((Dt- Lt)-(D - 1))(1：*1)仇1："1沏卜 ≤ O (署)，and given our
choice of M and R the right hand side is less than ɪ λ0, along with Lemma (9) (which says
min
v∈Rn,∣∣v∣∣2 = 1
3
II(D - L)v∣b ≥ -λo
8
and naturally, since this matrix is block-lower-triangular,
min
v∈R(i-1)b,kv∣∣2 = 1
-	ll 3、
Il(D -L)1：(i-1)b,1：(i-1)bv||2 ≥ 8λ0
holds) we know that Dt - Lt is invertible and has the bound ∣∣ (Dt - Lt)T ∣∣ ≤ λL.
Based on the update rule (38), we have
Rti = ∣∣Wti- wt(i+1)HF
=IIVeC(Wt)- VeC(Wt+ι)∣∣2
≤∣∣J>,iG-%(fti,i-yi)∣∣2
≤O
O(Vn), ∣∣G-Iiill2 = Og
llfti - y∣2
l Γ	IrT ∕r∖	τ	∖-1
∣	Ut(i-1) (Dt(i-1) - Lt(i-1))
l _一(Dt(i-1) - Lt(i-I))(Dt(i-1) - Lt(i-1))-1	1(k-i+1)b.
ft- y∣2
2
O
0
ʌ
~
~
(By formula (42))
i-1)(D t(i-1)
-
-Lt(i-1))
+	(Dt(i-1) - Lt(i-I))(Dt(i-1)
+ llI(k-i+1)b∣∣F) ∣∣ft - y∣∣2
-
-Lt(i-1))
1IIf
1IIf
≤O
kft-yk2
(51)
t-1
≤O
≤O
≤O
∏At	∣fι-y∣2
i=1	2
(t-1)
• μ ( 1 - Ω
(t-1)
k(f1-y)k2
(ByEq.(43))
(Lemma 7, 8, and 9(b).)
(Lemma 2)
ʌ
~
where Eq. (51) uses the following bounds proved above:
TT ∕τ∖	T
Ut(i-I)(Dt(i-1) - Lt(i-1)
)-1||f ≤llU t(i-i)lU(D
't(i-1) - Lt(i-1))
-1∣∣2=o(K

~
~
~
~
ʌ
ʌ
1Hf ≤ ll(Dtj)-L t(I))llF ll(Dt(I)-L t(iT))Tll2 = O (λθ),
-
(Dt(i-1) - Lt(i-I))(Dt(i-1) - Lt(i-1))
l l I(I+1)b l l f = θ(n)∙
24
Under review as a conference paper at ICLR 2020
Since this also hold for previous (t, i) pairs, we have
Rt0i0
(t0,i0)≤(t,i)
(52)
which is the reason Why We need to take R = Θ (nλ5). This means that Wti ∈ B(R) holds. And by
induction, we have proved that W remains in B(R) throughout the optimization process.
The last thing to do is to bound ft - y. By the same logic from above, We have
t-1
kft - yk2 ≤ Y At	kf1 - yk2
i=1	2
≤μ√n 1 — Ω
(t-1)
Which proves our theorem.
D Additional Experimental Results
In this section, We give test performance curve of AFAD-LITE dataset in Figure 3 under the same
setting With Section 4.
In addition, We provide more baseline results, e.g. Adam (Kingma & Ba, 2014) and K-FAC (Martens
& Grosse, 2015) on RSNA Bone Age dataset. Since We find that, as another alternation of BN,
Group Normalization (GN) (Wu & He, 2018) can largely improve the performance of Adam, We
also implement the GN layer for our GGN method. We use grid search to obtain approximately
best hyper-parameters for every experiment. All experiments are performed With batch size 128,
input size 64*64 and Weight decay 10-4. We set the number of groups to 8 for ResNetGN. Other
hyper-parameters are listed beloW.
•	SGD+ResNetBN: learning rate 0.01, momentum 0.9.
•	Adam+ResNetBN: learning rate 0.001.
•	SGD+ResNetGN: learning rate 0.002, momentum 0.9.
•	Adam+ResNetGN: learning rate 0.0005.
•	K-FAC+ResNet: learning rate 0.02, momentum 0.9, = 0.1, update frequency 100.
•	GGN+ResNetGN: λ = 1, α = 0.075.
The convergence results are summarized in Figure 4. Note to make comparison clearer, We use
logarithmic scale for training curves.
25
Under review as a conference paper at ICLR 2020
sso—ICT⊂-⊂-ra^π
Figure 3: Test performance on AFAD-LITE dataset.
KFAC+ResNet
GGN+ResNetGN
Io-2 7
IO-3：
IO-4 三
IO-5 ：
Time
(a) Training loss-time curve on RSNA
0.30-
0.25-
0.20-
0.15-
o.ιo-
0.05-
o.oo-
SSO-I⅛ΦH
Time
SSo-I 6ulu-eJJ. SSO-IlS①J_
(c) Testing loss-time curve on RSNA
IO-2 7
10~3 ：
IO-4 1
IoT :
0.25-
0.20-
0.15-
o.ιo-
10^5
O 10	20	30	40	50	60
Epochs
(b) Training loss-epoch curve on RSNA
0.30-
0.05-
o.oo-
0.0	2.5	5.0	7.5	10.0	12.5	15.0	17.5	20.0
Epochs
(d) Testing loss-epoch curve on RSNA
Figure 4: Training and testing curves of GGN and other baselines on RSNA Bone Age dataset.
26