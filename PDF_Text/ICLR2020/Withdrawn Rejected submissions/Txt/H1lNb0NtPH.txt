Under review as a conference paper at ICLR 2020

DIME:   AN   INFORMATION-THEORETIC   DIFFICULTY

MEASURE  FOR  AI DATASETS

Anonymous authors

Paper under double-blind review

ABSTRACT

Evaluating the relative difficulty of widely-used benchmark datasets across time
and across data modalities is important for accurately measuring progress in ma-
chine learning.  To help tackle this problem, we propose DIME, an information-
theoretic DIfficulty MEasure for datasets, based on conditional entropy estimation
of the sample-label distribution.   Theoretically,  we prove a model-agnostic and
modality-agnostic lower bound on the 0-1 error by extending Fano’s inequality to
the common supervised learning scenario where labels are discrete and features
are continuous. Empirically, we estimate this lower bound using a neural network
to compute DIME. DIME can be decomposed into components attributable to the
data distribution and the number of samples.  DIME can also compute per-class
difficulty  scores.   Through  extensive  experiments  on  both  vision  and  language
datasets, we show that DIME is well-aligned with empirically observed perfor-
mance of state-of-the-art machine learning models.  We hope that DIME can aid
future dataset design and model-training strategies.

1    INTRODUCTION

Empirical machine learning research relies heavily on comparing performance of algorithms on a
few standard benchmark datasets. Moreover, researchers frequently introduce new datasets that they
believe to be more challenging than existing benchmarks.  However,  we lack objective measures
of dataset difficulty that are independent of the choices made about algorithm- and model-design.
Moreover, it is also hard to compare algorithmic progress across data modalities, such as language
and vision.  So,  for instance,  it is difficult to compare the relative progress made on a 
sentiment
analysis benchmark such as the Stanford Sentiment Treebank (SST) (Socher et al., 2013) and an
image classification benchmark, like CIFAR-10 (Krizhevsky, 2009). With these challenges in mind,
we propose a model-agnostic and modality-agnostic measure for comparing how difficult it is to
perform supervised learning on a given dataset.

Intuitively,  assuming that dataset examples are sampled i.i.d.   from a static true distribution,  
we
argue that the difficulty of a dataset can be decoupled into two relatively independent sources:(a)
approximation complexity, the number of samples required to approximate the true distribution up
to certain accuracy, and (b) distributional complexity, the intrinsic difficulty involved in 
modeling
the statistical relationship between the labels and features.

We focus our analysis on the second source of the intrinsic difficulty in supervised learning, where
both  features  and  labels  are  available.    To  provide  a  model-agnostic  measure,  we  turn  
to  the
information-theoretic approach.   Indeed,  there already exist lower bounds on the lowest possible
errors  given  the  distribution  of  the  data.   If  both  the  samples  and  labels  are  
discrete,  Fano’s  in-
equality suggests the probability of 0-1 error is bounded by terms related to the conditional 
entropy
H(Y  X), where X is the random variable representing the features and Y  is the label.  When both
the features X and label Y  are continuous, results on differential entropy also suggest the 
expected
L₂ error is lower-bounded.  However,  in most of the supervised learning datasets where the fea-
tures are continuous and the labels are discrete, it is unknown how the lowest possible error can be
controlled regardless of models.

In  this  paper,  we  show  that  even  for  the  hybrid  case  where  labels  are  discrete  and  
features  are
continuous, with some additional assumptions, Fano’s inequality still holds.  Moreover, we show
that the lowest possible probability Pₑ of the 0-1 error for a given data distribution is lower 
bounded

1


Under review as a conference paper at ICLR 2020

Figure 1: We propose DIME, a model- and modality-agnostic difficulty metric for datasets

by terms related to a hybrid conditional entropy H(Y  X).  We further design an estimator for the
lower bound of    ₑ based on our generalized Fano’s inequality. The estimator uses neural networks
to approximate the KL divergence based on Donsker-Varadhan representation, which is then used to
estimate the hybrid conditional entropy H(Y |X) as well as the lower bound of Pₑ.

We emphasize that even though our lower bound is model-agnostic, the proposed estimator is based
on a neural network.   However,  we empirically show that,  for most image and natural language
datasets, a multilayer perceptron-based estimator produces a measure that effectively captures the
difficulty of the data and aligns well with the performance of state-of-the-art models.

Related Work:    Although conditional entropy and mutual information estimation have been exten-
sively studied, research has focused on purely discrete or continuous data.  Nair et al. (2006) were
among the first to study the theoretical properties for the case of mixture of discrete and 
continuous
variables.  Ross (2014), Gao et al. (2017) and Beknazaryan et al. (2019) proposed approaches for
estimating mutual information for the mixture case based on density ratio estimation (e.g.,binning,
kNN   or kernel methods), which is unsatisfactory for high dimensional data such as image and text.
We use neural network estimation (Belghazi et al., 2018) to avoid these issues.  More importantly,
we are the first to connect the hybrid conditional entropy with the lowest classification error and 
are
able to use it as a difficulty measure for datasets.

2    DESIGNING  A  DATASET  DIFFICULTY  MEASURE

For supervised learning across data modalities such as images and text, data samples can usually be
viewed as feature-label pairs (x, y) where x               Rdx , and y         .  We focus on 
classification
problems where the labels y  are discrete,  i.e.,           Z+.   We denote the joint distribution 
of the
feature-label pairs as         .   The marginal distributions of the features and labels are 
denoted as
and        respectively.  We make the following widely adopted assumption from learning theory

literature about how samples are generated:

Assumption 1.  The feature-label pairs (x, y) in the datasets, both training and testing, are 
sampled

i.i.d. from a static distribution (x, y) ∼ PXY .

Intuitively, there are many possible indicators for the potential difficulty of a dataset:  the 
number
of features, the number of classes, the number of samples, the distinguishability of samples across
classes, as well as the difference between the data distributions of the training set and the 
testing set.
However, none of these indicators alone can fully describe the relative difficulty of a dataset.

From Assumption 1,  if the samples (x, y) are sampled i.i.d.   from          ,  where      is 
discrete,  a
natural measure that characterizes the difficulty of the data distribution is the best probability 
of the
0-1 error that can be achieved by any estimator.

Definition 1 (Model-Agnostic Error).  Pₑ = inff Pₓ,y∼PXY  [f (x) /= y]

The measure 1 is straightforward, but unfortunately it is hard to compute since it involves 
evaluations
against all possible estimators. However, with mild assumptions, it can be lower bounded by terms
related to the conditional entropy, which is much easier to evaluate.

2


Under review as a conference paper at ICLR 2020

2.1    DISCRETE FEATURES

In the case where the features x ∈ X are discrete, according to Fano’s theorem, Pₑ is lower 
bounded:

Fano’s inequality.  If both X  and Y  are discrete random variables, then            H⁽Y |X⁾−¹ , 
where

log |Y|

|Y| is the cardinality of the label set Y, and H(Y |X) is the conditional entropy.

However, even though data can be represented using discrete integers, treating the features as dis-
crete random variables leads to the following difficulties:

1.  The cardinality of the feature space becomes extremely large if discrete features are used.  For
image  data,  since  each  pixel  is  represented  as  an  integer,  the  (raw)  feature  dimension 
 would
become the number of pixels in the image, which can be extremely large. Similarly, for language
data, when the sequence length is long, the feature dimension becomes large very quickly.

2.  Given the large feature space, finding a matching set of features between training and testing 
data
from the limited number of training and test samples would be unlikely. As a result, probability
mass estimation on each discrete value would be impractical, since we may only see at most one
sample for each discrete configuration.

2.2    CONTINUOUS FEATURES

As opposed to treating the features x as discrete random variables, if we view them as i.i.d. 
samples
from a continuous distribution with probability density pₓ, we can estimate the conditional entropy
H(Y |X) under some smoothness assumptions and also infer the model-agnostic error Pₑ.

However, classical Fano’s inequality only holds for discrete random variables. For the case with the
continuous features and discrete labels, it has not been shown how    ₑ can be controlled.  In this
paper, we prove a generalized version of Fano’s inequality that holds for the continuous-feature-
discrete-label scenario. Formally, for the continuous-X-discrete-Y  case:

Definition 2.  Hybrid Conditional Entropy


H(Y |X) := EX

− |Y|  P(Y  = y|X) log P(Y  = y|X) ,                            (1)

y=1

where P(Y  = y|X) := E[1(Y  = y)|X].

This definition of H(Y  X) is consistent with the classical definition, in the sense that both of 
them
give the intuition that how much information or uncertainty is left for Y  given X.

Next, to connect Pₑ and the hybrid H(Y |X), we introduce an assumption on the function f
Definition  3.  Smooth  Discretization  Property:  The  function  f   :  X  →  Y satisfies  the  
smooth
discretization property if for every y ∈ Y, almost every x ∈ X (x a.e. in X ),

f (x) = y  ⇐⇒  ∃δ > 0    s.t.    ∀x˜ ∈ Bδ(x),     f (x˜) = y,

where Bδ(x) := {x˜ ∈ Ω : ǁx˜ − xǁ₂ < δ} is a δ−neighborhood of x in X .

This assumption on the classifier function f is not unnatural considering f maps a continuous vari-
able to a discrete variable.   Without this assumption,  it would be extremely hard to quantify the
population error probability P (f (X)  =  Y ) since the behavior of f  may be erratic.  Furthermore,
in      the real data setting, this assumption is always satisfied for every classifier f , as we 
can always
construct a small enough neighborhood of each data point such that they are disjoint and assume f
is     a constant in each neighborhood.  In this sense the assumption on f  is pretty minimal.  Now 
we
are ready to extend Fano’s inequality:

Theorem 1.  Fano’s Inequality for Continuous Features: Let    ₑ be the minimum error probability,
i.e.,

Pₑ = inf P (f (X) /= Y )

where f is any estimator of Y based on the observation X  that satisfies the smooth discretization
property. Then we have

H(Pₑ) + Pₑ log(m − 1) ≥ H(Y |X),                                             (2)

where H(Pₑ) := −Pₑ log Pₑ − (1 − Pₑ) log(1 − Pₑ).

3


Under review as a conference paper at ICLR 2020

Proof.  See appendix B.

3    ESTIMATING  THE  LOWER  BOUND

It is natural to consider    ₑ defined in Theorem 1 as a measure of dataset difficulty.  
Unfortunately,
direct estimation of    ₑ is impractical since one has to evaluate the estimation error against all 
pos-
sible estimators.  However, theorem 1 provides an alternative towards estimating a lower bound on
Pₑ through estimating the hybrid conditional entropy H(Y |X) defined in Equation (1).

3.1    CONDITIONAL ENTROPY ESTIMATION

In real applications, direct calculation of hybrid conditional entropy H(Y |X) according to Defini-
tion 2 is impossible since P(Y  = y|X) is unknown. However, similar to the conditional entropy for
discrete random variables, the hybrid conditional entropy H(Y |X) can also be written as

|Y|

H(Y |X) = H(Y ) −      P (Y  = y)KL(X|Y  = y||X).                               (3)

y=1

Please refer to Appendix A for a detailed proof of Equation (3).  We also define the hybrid mutual
information I(X; Y ), which is compatible with H(Y |X):

|Y|

I(X; Y ) =        P (Y  = y)KL(X|Y  = y||X)                                       (4)

y=1

In  some  benchmark  datasets  with  balanced  classes  (e.g.,  CIFAR-10  and  MNIST),  H(Y |X)  =

log                                  (X Y   =  y  X).  This indicates that if a dataset has more 
classes and the|Y|      y=1
features for different classes are closer to each other on average, then H(Y |X) would be larger.

3.2    A VARIATIONAL KL DIVERGENCE ESTIMATOR USING NEURAL NETWORKS

We now discuss the practical design of an estimator for KL(X Y ).  Even though there are non-
parametric KL divergence estimators such as kNN based techniques (Noshad et al., 2017; Perez-
Cruz, 2008; Wang et al., 2009), they rely on some knowledge of the internal dimension of the data
manifold.  However, the internal dimension of the data manifold is usually much smaller than the
dimension of the raw features and is hard to estimate. As an alternative, we design a neural 
network-
based estimator for KL divergence estimation, inspired by Belghazi et al. (2018).  The estimator is
based on the following theorem:

Donsker-Varadhan representation.


KL(         ) =    sup

T :X→R

EP [T ] − log EQ[eT ]                                         (5)

If we parameterize the function T  using a neural network, we obtain:

KL(P||Q) ≥ sup EP [Tθ] − log EQ[eTθ ],                                         (6)

where θ represents the neural network parameters.

In our empirical evaluation, we optimize the empirical average instead of the expectation. However
this may cause issues such as overfitting.   To mitigate this issue,  we split the data into 
separate
training  and  validation  sets.   The  neural  network  models  are  trained  on  the  training  
set  and  the
estimation is made using the validation set. See Algorithm 1 for details.

4


Under review as a conference paper at ICLR 2020

Algorithm 1 DIME

Initialize       neural networks: Tθ1 , Tθ2 , . . . , Tθ    , one per class.

Initialize class counters k₁ = 0, . . . , kc = 0, one per class. Initialize the sample counter k = 
0.

for t in 0, . . . , T  do

Draw a batch of b training examples {(xi, yi)}, and b evaluation examples {(xᵉ, yᵉ)}.

i     i

for each class c ∈ {1, . . . , |Y|} do                              e              e

Find samples of the c-th class: Sc = {i|yi = c}, Sc  = {i|yi  = c}

Train θc = arg maxθ    ¹  Σ    Tθ (xi) − log[ ¹ Σ  exp Tθ (xj)]

								


Evaluate ˆKLθ

=     1   Σ

e  Tθ (xᵉ) − log[ ¹ Σ

exp Tθ (xᵉ)]

If KLθc  stops increasing, pause the training for Tθc .

Update counters: kc+ = |Sᵉ|.

end for

Update the sample counter: k+ = b

If all neural networks stop updating, break.

end for

Estimate the class probability pc =  ᵏc , c = {1, . . . , |Y|}.

k

^

3.3    DIME AS A DIFFICULTY MEASURE

Given that we are using a neural network model to estimate the KL divergence between P(X|y) and

P(X), according to Equation (6), KL(X|y||X) ≤ KL(X|y||X).  As a consequence, the estimated
mutual information I(X; Y ) ≤ I(X; Y ), which leads to a larger H(Y |X). In the end, DIME could
be larger as compared to true lower bound on Pₑ.

The caveat is that    ₑ is defined as a model-agnostic measure, but we use a neural network model
to  estimate  its  lower  bound  instead.   The  hope  is,  if  the  function  class  of  the  
neural  network  is
large enough, the gap between KL(X y  X) and KL(X y  X) is small enough so that DIME won’t

deviate too much from the true lower bound of    ₑ.  We now describe our experiments for DIME

using vision and language benchmark datasets.

4    EXPERIMENTS  AND  RESULTS

We try to answer the following questions through our experiments: Is the neural net estimator tight-
enough?  How does DIME align with the state-of-the-art benchmarks across modalities?  What are
the practical applications for DIME in evaluating machine learning progress?

4.1    DATA PREPARATION

We  evaluate  the  following  image  classification  datasets:    MNIST  (LeCun  et  al.,  1998),  
Ex-
tended  MNIST  (EMNIST)  (Cohen  et  al.,  2017),  Fashion  MNIST  (Xiao  et  al.,  2017a),  CIFAR-
10 (Krizhevsky, 2009), CIFAR-100 (Krizhevsky, 2009), Tiny ImageNet¹, and SVHN (Netzer et al.,
2011).   The  EMNIST  dataset  has  several  different  splits,  which  include  splits  by  
digits,  letters,
merge, class, and balanced.  For a controlled evaluation study, we also add the PyTorch Fakedata
dataset² which contains 10-classes of randomly-generated 32 × 32 grayscale images.

Among  natural  language  processing  datasets,  we  investigate  three  supervised  scenarios:   
senti-
ment  analysis,  text  classification,  and  language  modeling.   For  sentiment  analysis,  we  
analyze
IMDb (Maas et al., 2011), and Stanford Sentiment Treebank (SST) (Socher et al., 2013).  In the
case of SST, we utilize both fine-grained labels (SST-5) and binary labels (SST-2). For the text 
clas-
sification task, we use the TREC dataset (Li & Roth, 2002), AG News, DBPedia, as well as Yelp
Review dataset (Zhang et al., 2015).  For language modeling task, we analyze the character level
modeling for Penn Tree Bank (Marcus et al., 1994).

¹https://tiny-imagenet.herokuapp.com/

²https://github.com/pytorch/vision/blob/master/torchvision/datasets/

fakedata.py

5


Under review as a conference paper at ICLR 2020

For DIME estimation, we do not perform any preprocessing on the data except for simple rescaling.
For image data we rescale all the pixel values to floats in [0, 1]. By data processing inequality 
(Cover
& Thomas, 2012), the rescaling will not cause any change on H(Y  X) and    ₑ since the rescaling
function is invertible.

4.2    CHOICE OF NEURAL NETWORK ESTIMATOR

To estimate DIME for image datasets, we use a simple multi-layer perceptron (MLP) with resid-
ual layers with Rectified Linear Unit (ReLU) activation functions.  The network has three hidden
layers of 4096 neurons. For language datasets, we use an embedding layer of dimension 1500, fol-
lowed by three layers of 256 hidden neurons with residual connections and ReLU activations.  We
found that a higher dimension of the embedding helps achieve a tighter bound.   The embedding
layer is initialized with a concatenation of five pretrained embedding vectors:  GloVe-840B-300d,
GloVe-42B-300d,  GloVe-twitter-27B-200d,  GloVe-6B-300d,  FastText-en-300d,  and CharNGram-
100d.         The input sequences are truncated or padded to the length of 128.  The MLP is 
operating
on the token dimension instead of the embedding dimension to take the order of the sequence into
consideration.  For example, the first layer of MLP is 128-by-256 instead of 1500-by-256.  The last
fully-connected layer flattens everything into a vector and projects it to a scalar. For optimizing 
the
MLP, we use SGD with initial learning rate of 0.1 and anneal it to 0.01 and 0.001 if the objective
stops updating. We use the test set as evaluation set, as described in Algorithm 1.


0.8

0.6

0.4

0.2

0

CIFAR-10
MNIST
FASHION
SVHN

EMNIST-LETTERS

0          0.2        0.4        0.6        0.8         1

Label Corruption Rate

0.5

0.4

0.3

0.2

0.1

0

0.2      0.4      0.6      0.8       1

Fraction of Samples

Figure 2:  DIME increases with increase in the amount of label corruption (left).  DIME becomes
reasonably stable as the fraction of samples from the dataset used for estimation increase (right).

4.3    SANITY-CHECK EXPERIMENTS

Experiments  with  toy  datasets:    We  generate  three  two-dimensional  2-class  toy  datasets  
with
20,000 samples each with increasing level of difficulty (Figure 1).  Samples in Toy Data 1 and Toy
Data 2 are generated using random Gaussian variables by scaling differently in different dimensions
and then by rotating, while the two classes in Toy Data 3 are simply uniform and Gaussian random
variables.  Toy Data 1, which is relatively easy to be separated, gets a DIME of 0.001.  Toy Data 2,
which is a bit harder due to a small overlapping region around the axis origin, gets a DIME of 
0.013.
And Toy Data 3, which is the hardest due to overlapping supports of the two class, gets a DIME of

0.263. In sum, DIME accurately reflects the relative difficulty of the toy examples.

Label corruption test:    Next, we estimate DIME for the case of label corruption.  If we assign
random  labels  to  a  fraction  of  the  training  samples,  intuitively  the  dataset  should  
become  more
difficult. We experiment with five datasets with ten classes each—MNIST, Fashion MNIST, CIFAR-
10, EMNIST (digits), and SVHN—and perform label corruption on their training sets in steps of 0.1
from 0 to 1. We observe a consistent increase in DIME across all the datasets with increasing label
corruption (Figure 2-(left)).   For some datasets such as MNIST, DIME is almost tight given the
fraction of randomly corrupted labels.

6


Under review as a conference paper at ICLR 2020


Corpus

Image Classification
EMNIST (digits)
MNIST

EMNIST (letters)
EMNIST (bymerge)
Fashion-MNIST
EMNIST  (byclass)
EMNIST (balanced)
SVHN

CIFAR-10

CIFAR-100-Subclass
CIFAR-100

Tiny ImageNet
FakeData
Sentiment Analysis
IMDb

SST-2
SST-5

Text Classification

DBPedia
TREC

YelpReview (Polarity)
AG News

Language Modeling

Penn Treebank

#classes
10

10

26

47

10

62

47

10

10

10

100

200

10

2

2

5

14

6

2

4

42

H(Y )

2.303

2.301

3.258

3.554

2.303

3.679

3.850

2.223

2.303

2.303

4.605

5.298

2.303

0.693

0.693

1.573

2.639

1.638

0.693

1.386

2.966

I(X; Y )

2.255

2.192

2.872

3.098

1.912

3.126

3.212

1.436

0.915

0.503

1.239

0.692

-0.003

0.224

0.224

0.230

2.387

1.185

0.371

0.808

1.876

H(Y |X)

0.048

0.109

0.386

0.456

0.391

0.553

0.638

0.786

1.388

1.800

3.366

4.606

2.306

0.469

0.469

1.342

0.252

0.453

0.322

0.579

1.090

DIME
0.006

0.015

0.054

0.060

0.066

0.072

0.089

0.159

0.340

0.504

0.585

0.768

0.900

0.178

0.179

0.470

0.037

0.092

0.098

0.147

0.165

SOTA Error
0.002

0.002

0.056

0.190

0.033

0.240

0.095

0.010

0.010

N/A
0.087

0.268

0.900

0.038

0.032

0.356

0.013

0.019

0.044

0.076

1.083 (ppl)

Table 1:  We evaluate DIME on vision and language datasets and rank them by relative difficulty.
Comparisons  with  prediction  performance  of  state-of-the-art  neural  network  models  shows  
that
DIME       is roughly aligned with empirically observed performance. (ppl: perplexity)

4.4    EVALUATING BENCHMARK DATASETS IN VISION AND LANGUAGE

We evaluate DIME using popular image and language domain datasets for various classification
tasks. In addition to dataset statistics, we report the estimated label entropy Y, the estimated 
hybrid

mutual entropy (Eq. 4), the estimated hybrid conditional entropy H(Y  X) (Eq. 1), and DIME. We
also report the state-of-the-art error on the dataset from recent literature (Wan et al., 2013; 
Huang
et al., 2018; Cubuk et al., 2019; Yang et al., 2019; Patro et al., 2018; Cer et al., 2018; Melis et 
al.,
2019). From the results summarized in Table 1, we make the following empirical observations:

First, DIME align well with the state-of-the-art results on the benchmark datasets, indicating that 
the
relative difficulty of datasets is reflected in the performance of latest machine learning 
algorithms.
However, notably, DIME is lower than the reported state-of-the-art error for all the variants of EM-
NIST. This could indicate that since EMNIST is a relatively new, less-investigated dataset, we might
see model performance improving over time.

Second, a larger number of classes lead to a higher value of DIME in general, which is expected
given its dependence on H(Y ). But H(Y ) does not always dominate the value of the measure. For
example, MNIST has 10 classes, but its DIME is much smaller compared to IMDb which has only

two classes. This relative difference in difficulty of MNIST and IMDb is also reflected in their 
SOTA
errors, validating our measure.

Third,  we  can  also  evaluate  the  relative  difficulty  of  similar  datasets  introduced  over 
 time.   For
instance, Fashion-MNIST, which was designed to be more challenging than MNIST (Xiao et al.,
2017b), can be compared with MNIST on the basis of DIME. We find that Fashion MNIST is indeed
more difficult than MNIST, though not as difficult as SVHN, indicating that with more exploration,
we might see SOTA error approaching zero on Fashion MNIST.

Finally, for datasets such as MNIST, Fashion MNIST, EMNIST, FakeData, and DBPedia, DIME
provides a tight error bound.   But for certain datasets such as CIFAR-10,  CIFAR-100,  and Tiny
ImageNet, DIME seem slightly pessimistic. As we discussed in Section 3.3, DIME could be larger
than the true lower bound of    ₑ since we constrain the function space to be a neural network 
model.
While the relative order of difficulty suggested by DIME is reasonable, its large value also 
suggests

7


Under review as a conference paper at ICLR 2020

CIFAR-10                                                                                
Fashion-MNIST

Figure 3:  DIME can rank classes within datasets for difficulty.  A lower height of the bar 
indicates
higher relative difficulty.

than an MLP may not have enough capacity for accurate Pₑ estimation. While it is an open question
how to choose a model that is large enough to approximate    ₑ yet easy to optimize, Equation (6)
suggests that as the model size grows, the gap between DIME and true lower bound of    ₑ becomes
smaller.  We verify this by calculating DIME with increasing number of neurons for a three-layer
MLP. See Figure 5 in Appendix C for details.

4.5    ADDITIONAL USE CASES FOR DIME

Dataset difficulty per class:    Algorithm 1 can also estimate the relative difficulty of classes 
within
a dataset.  For this purpose, we simply rank the classes by their KL(X|y||X).  A smaller value of
KL(X y  X) indicates the class is harder to classify. As examples in Figure 3 show, ‘cat’ and ‘bird’
are the most difficult classes in CIFAR-10, while ‘automobile’ and ‘ship’ are the easiest. In 
addition,
‘shirt’ and ‘pullover’ are the most difficult classes in Fashion MNIST, while ‘sandal’ and ‘trouser’
are the easiest.

Controlled Study with CIFAR Subsets:    We obtain a 10-class subset from CIFAR-100 by choos-
ing subclasses from the ‘large carnivores’ and ‘large omnivores and herbivores’ superclasses. These
classes—bear, leopard, lion, tiger, wolf, camel, cattle, chimpanzee, elephant, and kangaroo—should
be intuitively harder to classify as compared to CIFAR-10, which includes disparate classes such as
bird, truck, and ship. Note that the image data for both of these datasets comes from the same, 
larger
‘80 million tiny images’ dataset (Torralba et al., 2008).  We find that the DIME for this 10-class
CIFAR-100 subset is 0.504, significantly larger than a DIME of 0.340 for CIFAR-10.

Effect of number of samples:    With increasing number of samples, the empirical distribution of
both training and testing tend towards the population distribution, making the distance between them
smaller.  It should also make the dataset easier, which is indeed reflected in a decreasing value of
DIME with increasing number of samples (Figure 2-(right)). But we also observe that our measure
becomes stable with increasing number of samples.  This suggests that our method can be used to
estimate the relative difficulty of new datasets in domains where data is hard to collect (e.g., 
medical
datasets, fine-grained image classification) and make determinations if additional data collection 
is
required to make the problem easier to solve.

Effect of model size:    We investigate the effect of model size on DIME optimization.  Equation

(6) suggests that as the estimator model becomes larger, the gap between DIME and the true lower
bound on    ₑ should become smaller. We experiment with different sizes of MLP and find that DIME
generally decreases with increasing number of neurons. However for easier datasets such as MNIST
and Fashion MNIST, DIME stabilizes fairly quickly. See Figure 5 from Appendix C for details.

5    CONCLUSION

We extend Fano’s inequality to the case of continuous features and discrete labels and prove a 
model-
and modality-agnostic lower bound on the 0-1 errors.  We further design DIME, an empirical dif-
ficulty measure for datasets.   We note that,  even though our lower bound is model-agnostic,  our
estimator is based on a neural network and the estimates are affected by choices about the neural
network design. Designing better estimators remains an avenue for future exploration. However, we
hope that this work can aid dataset-design in the future and help objectively compare the progress
in machine learning algorithms across modalities.  We will release code for DIME estimation and
DIME estimates for many common computer vision and natural language datasets to help further
research.

8


Under review as a conference paper at ICLR 2020

REFERENCES

Aleksandr Beknazaryan, Xin Dang, and Hailin Sang.  On mutual information estimation for mixed-
pair random variables. Statistics & Probability Letters, 2019.

Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and R Devon Hjelm.  Mutual information neural estimation.  International Conference
on Machine Learning, 2018.

Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Con-
stant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray
Kurzweil. Universal sentence encoder. CoRR, abs/1803.11175, 2018.

Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr van Schaik.   Emnist:  an extension of
mnist to handwritten letters. Retrieved from http://arxiv.org/abs/1702.05373, 2017.

Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.
Ekin Dogus Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. Autoaugment:

Learning augmentation policies from data. CoRR, abs/1805.09501, 2019.

Weihao Gao, Sreeram Kannan, Sewoong Oh, and Pramod Viswanath.  Estimating mutual informa-
tion for discrete-continuous mixtures. Advances in neural information processing systems, 2017.

Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, and
Zhifeng  Chen.   Gpipe:  Efficient  training  of  giant  neural  networks  using  pipeline  
parallelism.
CoRR, abs/1811.06965, 2018.

Alex Krizhevsky. Learning multiple layers of features from tiny images. tech report, 2009.

Y.  LeCun,  L.  Bottou,  Y.  Bengio,  and  P.  Haffner.   Gradient-based  learning  applied  to  
document
recognition. Proceedings of the IEEE, 1998.

Xin Li and Dan Roth.  Learning question classifiers.  International Conference on Computational
Linguistics (COLING), 2002.

Andrew L. Maas, Peter T. Pham Raymond E. Daly, Andrew Y. Ng Dan Huang, and Christopher
Potts.  Learning word vectors for sentiment analysis.  Association for Computational Linguistics
(ACL), 2011.

Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Fer-
guson, Karen Katz, and Britta Schasberger.  The penn treebank:  Annotating predicate argument
structure. Proceedings of the Workshop on Human Language Technology, 1994.

Gbor Melis, Tom Koisk, and Phil Blunsom. Mogrifier lstm. CoRR, abs/1909.01792, 2019.
Chandra Nair, Balaji Prabhakar, and Devavrat Shah.  On entropy for mixtures of discrete and con-

tinuous variables. arXiv preprint cs/0607075, 2006.

Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng.  Reading
digits in natural images with unsupervised feature learning.  NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011.

Morteza Noshad, Kevin R. Moon, Salimeh Yasaei Sekeh, and Alfred O. Hero III.  Direct estima-
tion of information divergence using nearest neighbor ratios.  IEEE International Symposium on
Information Theory, 2017.

Badri N. Patro, Vinod K. Kurmi, Sandeep Kumar, and Vinay P. Namboodiri.   Learning semantic
sentence embeddings using pair-wise discriminator. CoRR, abs/1806.00807, 2018.

Fernando Perez-Cruz.   Kullback-leibler divergence estimation of continuous distributions.   IEEE
International Symposium on Information Theory, 2008.

Brian C Ross. Mutual information between discrete and continuous data sets. PLoS ONE, 2014.

9


Under review as a conference paper at ICLR 2020

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, and
Christopher Potts.  Recursive deep models for semantic compositionality over a sentiment tree-
bank. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2013.

Antonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large data set for
nonparametric object and scene recognition. IEEE transactions on pattern analysis and machine
intelligence, 2008.

Li Wan,  Matthew Zeiler,  Sixin Zhang,  Yann Le Cun,  and Rob Fergus.   Regularization of neural
networks using dropconnect. International Conference on Machine Learning, 2013.

Qing Wang, Sanjeev R Kulkarni, and Sergio Verdu´.   Divergence estimation for multidimensional
densities  via  k-nearest-neighbor  distances.   IEEE  Transactions  on  Information  Theory,  
55(5):
2392–2405, 2009.

Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. CoRR, abs/1708.07747, 2017a.

Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017b.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V.
Le.      Xlnet:    Generalized  autoregressive  pretraining  for  language  understanding.      
CoRR,
abs/1906.08237, 2019.

Xiang Zhang, Junbo Zhao, and Yann LeCun.  Character-level convolutional networks for text clas-
sification. Neural Information Processing Systems, 2015.

10


Under review as a conference paper at ICLR 2020

A    HYBRID  CONDITIONAL  ENTROPY

Consider the usual classification setting, X, representing the feature, is a random vector in Rp; Y 
,
representing the label, is a random variable(or vector) in   1, . . . ,        .  Assume X  is a 
continuous
variable with density pX (x) for x in some compact domain Ω               Rp. Can we define a hybrid
conditional entropy H(Y  X) while X is continuous and Y  is discrete? We give a formal definition
for this mixed-pair case and demonstrate that H(Y  X) defined in our way can be a good indicator
for the hardness of classification on the data (X, Y ), which is consistent with the classical 
definition
in the sense that both of them give the intuition that how much information or uncertainty is left 
for
Y  given X.

Our definition is


H(Y |X) := EX

− |Y|  P(Y  = y|X) log P(Y  = y|X) ,                            (7)

y=1

where P(Y  = y X) := E[1(Y  = y) X].  By the definition of conditional expectation (or probabil-
ity), it is easy to check that

E[1(Y  = y) X] =  P (Y  = y)p(X|Y  = y) ,                                       (8)

pX (X)

where p(x Y  = y), x      Rp  is the conditional density function of X  given Y  = y.  Denote py :=

P (Y  = y), we have

H(Y |X) = ∫   − Σ p  p(x|Y  = y) log py p(x|Y  = y) dx

	


|Y|

= −      py

y=1

log py

|Y|

|Y|

−      py

y=1

p(x Y  = y) log p(x|Y  = y) dx

Ω                      pX (x)


This proves (3).

= H(Y ) −      pyKL(X|Y  = y||X).                                                         (9)

y=1

We summerize some basic properties of H(Y |X) under our definition (7):

Lemma 1.           (i)  0 ≤ H(Y |X) ≤ H(Y ) ≤ log |Y|.

(ii)  H(Y |X) = H(Y ) if and only if X and Y  are independent.

(iii)  H(Y |X)  =  0  if  and  only  if  Y  is  a  function  of  X.   That  is,  there  exist  |Y| 
subsets  of

Ω:  Ω₁, . . . , Ω|Y| such  that  P (X   ∈  Ωy ∩ Ωy' )  =  0  for  any  y  /=  y′  ∈  {1, . . . , 
|Y|},

P (X ∈ ∪|yY₌|₁Ωy) = 1 and P (Y  = y|X ∈ Ωy) = 1 for any y ∈ {1, . . . , |Y|}.

Proof.           (i)  This is trivial by (7), (9) and the fact that KL divergence is always 
nonnegative.

(ii)  By (9), it is easy to get

H(Y |X) = H(Y )  ⇐⇒  X|Y  = y =d= X,     ∀y ∈ {1, . . . , |Y|}.

⇐⇒  P (X ∈ A, Y  = y) = P (X ∈ A)P (Y  = y), for any
measurable set A in X , any y ∈ {1, . . . , |Y|}.

⇐⇒  X,Y  are independent.

(iii)  Sufficiency: If Y  is a function of X, then by assumptions, for any y₁ /= y₂, we have

P (X ∈ Ωy1 , Y  = y₂) ≤ P (X ∈ Ωy1 ) − P (X ∈ Ωy1 , Y  = y₁) = 0.

11


Under review as a conference paper at ICLR 2020

Therefore, p(x|Y  = y₂) = 0 for x ∈ Ωy1 (a.e.). Then for any y ∈ {1, . . . , |Y|}, we have


KL(X Y  = y) =

Ωy

p(x Y  = y) log p(x|Y  = y) dx                      (10)

pX (x)

Notice that for x ∈ Ωy(a.e.), pX (x) =     |yY'=| 1 py' p(x|Y  = y′) = pyp(x|Y  = y), and thus

KL(X|Y  = y) = − log py.                                           (11)

Then by (9), H(Y |X) = 0.

Necessity:  Assume H(Y |X)  =  0.  Notice that pX (x)  =      y|Y=|1 pyp(x|Y  =  y), ∀x  ∈ Ω.
Therefore, for any y ∈ {1, . . . , |Y|},


KL(X|Y  = y||X) = ∫

≤ ∫Ω

p(x Y  = y) log p(x|Y  = y) dx

pX (x)

p(x Y  = y) log   p(x|Y  = y)   dx

pyp(x|Y  = y)

(12)

= − log py.

Combing (9) and (12), we know if H(Y |X) = 0, then

pX (x) = pyp(x|Y  = y), ∀x ∈ {x ∈ Ω : p(x|Y  = y) > 0}(a.e.), ∀1 ≤ y ≤ |Y|.     (13)

Define Ωy  :=  {x  ∈ Ω  :  p(x|Y   =  y)  >  0}, for every y  ∈ {1, . . . , |Y|}.  Without loss
of generality, we assume pX (x)  >  0 for every x  ∈ Ω.  (Otherwise, we can replace Ω by
Ω ∩ {pX (x) > 0} and the following argument is still true.) Then we have

P (X ∈ ∪|yY₌|₁Ωy) = P (X ∈ Ω) = 1.


By (13), we have

P (X     Ωy) =

Ωy

pX (x)dx =

Ωy

pyp(x|Y  = y)dx

(14)

= pyP (X ∈ Ωy|Y  = y) = P (X ∈ Ωy, Y  = y).

Therefore, P (Y   =  y X      Ωy)  =  1.  Lastly, we show P (X      Ωy    Ωy' )  =  0 for any
y = y′. This is immediate if we notice that by replacing Ωy in (14) with Ωy    Ωy' , we can
have

P (X ∈ Ωy ∩ Ωy' ) = P (X ∈ Ωy ∩ Ωy' , Y  = y).

Changing the postion of y and y′, we have

P (X ∈ Ωy ∩ Ωy' ) = P (X ∈ Ωy ∩ Ωy' , Y  = y′).

Thus, P (X ∈ Ωy ∩ Ωy' ) = 0.

Before the end of the proof, we want to add a remark that the condition P (X ∈ Ωy ∩Ωy' ) =
0 is actually just a consequence of P (Y  = y|X ∈ Ωy) = 1, ∀y. We add it explicitly in the
conditions to make it clear that Ω₁, . . . , Ω|Y| is a partition of Ω according to the value of
Y

B    PROOF  OF  FANO’S  INEQUALITY  FOR  CONTINUOUS  FEATURES

We see the above essential properties for H(Y  X) have been preserved for the mixture case.  Fur-
thermore, we prove Fano’s inequality still holds for our case. Let us restate Theorem 1 here:

Theorem 2.  Let Pₑ be the minimum error probability, i.e.

Pₑ = inf P (f (X) /= Y )

12


Under review as a conference paper at ICLR 2020

Figure 4:  An example of f  with Smooth Discretization Property given the data and classifier.  Red

points represent {xi, f₀(xi)}10  , xi ∼ U [0, 1], f₀ is some given classifier in practice.

where f is any ”continuous” estimator of Y based on the observation X.  That is, f  is any function
satisfying that for any y = 1, . . . , |Y|, almost every x ∈ Ω, (x a.e. in Ω)

f (x) = y  ⇐⇒  ∃δ > 0    s.t.    ∀x˜ ∈ Bδ(x),     f (x˜) = y,

where Bδ(x) := {x˜ ∈ Ω : ǁx˜ − xǁ₂ < δ} is a δ−neighborhood of x in X . Then we have

H(Pₑ) + Pₑ log(|Y| − 1) ≥ H(Y |X).                                          (15)

Here, H(Pₑ) := −Pₑ log Pₑ − (1 − Pₑ) log(1 − Pₑ).

Proof.  We basically follow the idea in the classical proof in Cover & Thomas (2012),  although
many details need to be dealt with carefully under our definition.  Assume there exists a function f
achieves the minimum error probability, i.e.  P (f (X) = Y ) =    ₑ(If such f  doesn’t exist, we can
always find some f such that P (f (X) = Y )        ₑ + ϵ for some little ϵ > 0, then let ϵ      0 
we can
get the same result). Define an error random variable,


E =    1,    if    f (X) /= Y

0,    if    f (X) = Y.

(16)

Under our definition (7), we have (in what follows, we use the notation Y  = i for i = 1, . . . ,

instead of Y  = y to remind the reader that Y  is discrete.)


H(E, Y |X) := EX −

It is easy to check that

|Y|    1

i=1 j=0

P(Y  = i, E = j|X) log P(Y  = i, E = j|X) .         (17)

P(Y  = i, E = j X) =  P (Y  = i, E = j)p(X|Y  = i, E = j) ,                       (18)

pX (X)

13


Under review as a conference paper at ICLR 2020

where p(x Y  = i, E  = j) is the conditional density function of X given Y  = i, E  = j.  Similarly,
one may check that ³ ⁴


P(E = j Y  = i, X) =  P (Y  = i, E = j)p(X|Y  = i, E = j)

P (Y  = i)p(X|Y  = i)

Combining the above two equations and (8), we have

(19)

P(Y  = i, E = j|X) = P(Y  = i|X)P(E = j|Y  = i, X).                           (20)

Plug it in (17), and use the notation


we get

H(E|Y, X) := EX −

|Y|    1

i=1 j=0

P(Y  = i, E = j|X) log P(E = j|Y  = i, X) ,          (21)

H(E, Y |X) = H(Y |X) + H(E|Y, X)                                          (22)

For any x ∈ Ω, i = 1, . . . , |Y|, we know ∃δ > 0, such that


P (X ∈ B  (x), Y  = i, E = 0) = .P (X ∈ Bδ(x), Y  = i),    if    f (x) = i

(23)

0,                                      if    f (x) /= i.

Let δ  → 0, we have p(x|Y  = i, E  = 0)P (Y  = i, E  = 0) = p(x|Y  = i)P (Y  = i) or 0 for any
x ∈ Ω and 1 ≤ i ≤ m. Therefore, by (19), we know P(E = 0|Y  = i, X) is always 0 or 1 for any
1 ≤ i ≤ |Y|. So is P(E = 1|Y  = i, X). Then by (21), we have

H(E|Y, X) = 0.                                                           (24)


Similar to (22), we have

where

H(E, Y |X) = H(E|X) + H(Y |E, X),                                         (25)


and

H(E|X) := EX −

Σj=0

P(E = j|X) log P(E = j|X)                         (26)


H(Y |E, X) := EX −

|Y|    1

i=1 j=0

P(Y  = i, E = j|X) log P(Y  = i|E = j, X) .         (27)

Notice that H(E|X) ≤ H(E) = H(Pₑ), it suffices to show

H(Y |E, X) ≤ Pₑ log(|Y| − 1).                                               (28)

Similar to (23), we know for any x ∈ Ω, i = 1, . . . , |Y|, ∃δ > 0, such that


P (X ∈ B  (x), Y  = i, E = 0) = .P (X ∈ Bδ(x), E = 0),    if    f (x) = i

(29)

0,                                       if    f (x) /= i.

Then we get P(Y  = i|E = 0, X) is always 0 or 1 for any i = 1, . . . , |Y|. Therefore,


H(Y |E = 0, X) := EX

− |Y|  P(Y  = i, E = 0|X) log P(Y  = i|E = 0, X) = 0.         (30)

i=1

3   Strictly  speaking,  by  the  definition  of  conditional  expectation  or  probability,  we  
should  have  P(E  =

j|Y = i, X) = P (Y =i,E=j)p(X|Y =i,E=j) 1(Y = i) + P (Y =i,E=j)p(X|Y =i,E=j) 1(Y /= i).


4                                        P (Y =i)p(X|Y =i)

P (Y /=i)p(X|Y /=i)

For the consistency of the notations, we still use P(E = j|Y  = i, X) to denote the right side of 
(19). So

actually, one should understand P(E = j|Y = i, X) as P(E = j|Y = i, X)1(Y = i) in this context. 
Similar
for P(Y = i|E = j, X) below.

14


Under review as a conference paper at ICLR 2020


For the case E = 1, we have

P (X ∈ B  (x), Y  = i, E = 1) = .P (X ∈ Bδ(x), Y  = i),    if    f (x)     i

(31)


Thus, we have

P(Y  = i|E = 1, X) =  P (Y  = i, E = 1)p(X|Y  = i, E = 1)

. P (Y =i)p(X|Y =i) ,    if    f (X) /= i

(32)

Similar to (20), we have

P(Y  = i, E = 1|X) = P(E = 1|X)P(Y  = i|E = 1, X).                           (34)

Plug in the above two equations to H(Y |E = 1, X), we get


H(Y |E = 1, X) = EX

− |Y|  P(E = 1|X)P(Y  = i|E = 1, X) log P(Y  = i|E = 1, X)

i=1


= EX P(E = 1|X) −

i/=f (X)

P(Y  = i|E = 1, X) log P(Y  = i|E = 1, X)

≤ EX [P(E = 1|X) log(|Y| − 1)]

= Pₑ log(|Y| − 1).

Therefore,

H(Y |E, X) = H(Y |E = 0, X) + H(Y |E = 1, X) ≤ Pₑ log(|Y| − 1).

C    MODEL  COMPLEXITY  EXPERIMENT

Equation (6) suggests when the model class is larger the gap between DIME and the true lower
bound on    ₑ becomes smaller. This is verified in our experiment shown in the middle of Figure (5),
where we use neural networks with three hidden layers of varying number of neurons to calculate
DIME. In the experiment we try 32, 64, 128, 256, 512, 1024, and 2048 neurons for each hidden
layer.   As  the  number  of  hidden  neurons  grows,  DIME  roughly  decreases  as  expected.   
Still  on
CIFAR-10 the decrease is pretty monotonic.  For simple datasets such as MNIST, Fashion MNIST
and EMNIST (byletters) the numbers got stabilized pretty quickly.

15


Under review as a conference paper at ICLR 2020

(a) DIME decreases on CIFAR-10 as the model size grows.

(b) DIME decreases on SVHN as the model size grows.

(c) DIME decreases on MNIST, Fashion MNIST, and EMNIST(byletters) as the model
size grows.

Figure 5: DIME as the model size increases.

16

