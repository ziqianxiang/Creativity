Under review as a conference paper at ICLR 2020
Noise Regularization for Conditional
Density Estimation
Anonymous authors
Paper under double-blind review
Ab stract
Modelling statistical relationships beyond the conditional mean is crucial in many
settings. Conditional density estimation (CDE) aims to learn the full conditional
probability density from data. Though highly expressive, neural network based
CDE models can suffer from severe over-fitting when trained with the maximum
likelihood objective. Due to the inherent structure of such models, classical regu-
larization approaches in the parameter space are rendered ineffective. To address
this issue, we develop a model-agnostic noise regularization method for CDE
that adds random perturbations to the data during training. We demonstrate that
the proposed approach corresponds to a smoothness regularization and prove its
asymptotic consistency. In our experiments, noise regularization significantly and
consistently outperforms other regularization methods across seven data sets and
three CDE models. The effectiveness of noise regularization makes neural net-
work based CDE the preferable method over previous non- and semi-parametric
approaches, even when training data is scarce.
1	Introduction
While regression analysis aims to describe the conditional mean E[y|x] of a response y given inputs
x, many problems such as risk management and planning under uncertainty require gaining insight
about deviations from the mean and their associated likelihood. The stochastic dependency of y on
x can be captured by modeling the conditional probability density p(y|x). Inferring such a density
function from a set of empirical observations {(xn, yn)}nN=1 is typically referred to as conditional
density estimation (CDE) and is the focus of this paper.
In the recent machine learning literature, there has been a resurgence of interest in high-capacity
density models based on neural networks (Dinh et al., 2017; Ambrogioni et al., 2017; Kingma &
Dhariwal, 2018). Since this line of work mainly focuses on the modelling of images based on large
scale data sets, over-fitting and noisy observations are of minor concern in this context. In contrast,
we are interested in CDE in settings where data may be scarce and noisy. When combined with
maximum likelihood estimation, the flexibility of such high-capacity models results in over-fitting
and poor generalization. While regression typically assumes Gaussian conditional noise, CDE uses
expressive distribution families to model deviations from the conditional mean. Hence, the over-
fitting problem tends to be even more severe in CDE than in regression.
Classical regularization of the neural network weights such as weight decay (Pratt & Hanson, 1989)
has been shown to be effective for regression and classification. However, in the context of CDE, the
output of the neural network merely controls the parameters of a density model such as a Gaussian
Mixture or Normalizing Flow. This makes the standard regularization methods in the parameter
space less effective and harder to analyze.
Aiming to address this issue, we propose and analyze noise regularization, a method well-studied
in the context of regression and classification, for the purpose of conditional density estimation. In
that, the paper attempts to close a gap in previous research. By adding small random perturbations to
the data during training, the conditional density estimate is smoothed and tends to generalize better.
In fact, we show that adding noise during maximum likelihood estimation is equivalent to penalizing
the second derivatives of the conditional log-probability. Visually, the respective regularization term
punishes very curved or even spiky density estimators in favor of smoother variants, which proves
to be a favorable inductive bias in many applications. Moreover, under some regularity conditions,
we show that the proposed regularization scheme is asymptotically consistent, converging to the
unbiased maximum likelihood estimator. This does not only support the soundness of the proposed
1
Under review as a conference paper at ICLR 2020
method but also endows us with useful insight in how to set the regularization intensity relative to
the data dimensionality and training set size.
Overall, the proposed noise regularization scheme is easy to implement and agnostic to the
parameterization of the CDE model. We empirically demonstrate its effectiveness on three different
neural network based models. The experimental results show that noise regularization outperforms
other regularization methods significantly and consistently across various data sets. Finally, we
demonstrate that, when properly regularized, neural network based CDE is able to improve upon
state-of-the art non-parametric estimators, even when only 400 training observations are available.
2	Background
Density Estimation. Let X be a random variable with probability density function (PDF) p(x)
defined over the domain X ⊆ Rdx. Given a collection D = {x1, ..., xn} of observations sampled
from p(x), the goal is to find a good estimate f (x) of the true density function p. In parametric
estimation, the PDF f is assumed to belong to a parametric family F = {fθ(∙) ∣θ ∈ Θ} where the
density function is described by a finite dimensional parameter θ ∈ Θ. The standard method for
estimating θ is maximum likelihood estimation, wherein θ is chosen so that the likelihood of the
data D is maximized. This is equivalent to minimizing the Kullback-Leibler divergence between
the empirical data distribution PD (x) = 1 PZi δ(∣∣χ - x/|) (i.e., mixture of point masses in the
observations Xi) and the parametric distribution fθ:
n
X
log fθ(Xi) = arg minDKL(PD∣∣fθ)	(1)
θ∈Θ
i=1
From a geometric perspective, (1) can be viewed as an orthogonal projection ofPD(X) onto F w.r.t.
the reverse KL-divergence. Hence, (1) is also commonly referred to as an M-projection (Murphy,
2012; Nielsen, 2018). In contrast, non-parametric density estimators make implicit smoothness
assumptions through a kernel function. The most popular non-parametric method, kernel density
estimation (KDE), places a symmetric density function K(z), the so-called kernel, on each training
data point Xn (Rosenblatt, 1956; Parzen, 1962). The resulting density estimate reads as q(x)=
nhd Pn=ι K (X-Xi) . One popular choice of K(∙) is a Gaussian K(Z) = (2∏)-d exp (-2z2). Be-
yond the appropriate choice of K(∙), a central challenge is the selection of the bandwidth parameter
h which controls the smoothness of the estimated PDF (Li & Racine, 2007).
Conditional Density Estimation (CDE). Let (X, Y ) bea pair of random variables with respective
domains X ⊆ Rdx and Y ⊆ Rdy and realizations X and y. Let P(y|X) = P(X, y)/P(X) denote the
conditional probability density of y given X. Typically, Y is referred to as a dependent variable
(explained variable) and X as conditional (explanatory) variable. Given a dataset of observations
D = {(xn,yn)}N=ι drawn from the joint distribution (xn,yn)〜 p(x,y), the aim of conditional
density estimation (CDE) is to find an estimate f (y|X) of the true conditional density P(y|X).
In the context of CDE, the KL-divergence objective is expressed as expectation over P(X):
Ex~p(x) IDKL(P(y|x)||f：(y|x))] = E(x,y)~p(x,y)^ogp(y|x) - log /(y|x)]	⑵
Corresponding to (1), we refer to the minimization of (2) w.r.t. θ as conditional M-projection. Given
a dataset D drawn i.i.d. from P(X, y), the conditional MLE following from (2) can be stated as
n
θ* = arg min - X log fθ (y∕xi)	⑶
3	Related work
The first part of this section discusses relevant work in the field of CDE, focusing on high-capacity
models that make little prior assumptions. The second part relates our approach to previous regular-
ization and data augmentation methods.
Non-parametric CDE. A vast body of literature in statistics and econometrics studies nonpara-
metric kernel density estimators (KDE) (Rosenblatt, 1956; Parzen, 1962) and the associated band-
width selection problem, which concerns choosing the appropriate amount of smoothing (Silverman,
2
Under review as a conference paper at ICLR 2020
1982; Hall et al., 1992; Cao et al., 1994). To estimate conditional probabilities, previous work pro-
poses to estimate both the joint and marginal probability separately with KDE and then computing
the conditional probability as their ratio (Hyndman et al., 1996; Li & Racine, 2007). Other ap-
proaches combine non-parametric elements with parametric elements (Tresp, 2001; Sugiyama &
Takeuchi, 2010; Dutordoir et al., 2018). Despite their theoretical appeal, non-parametric density es-
timators suffer from poor generalization in regions where data is sparse (e.g., tail regions), causing
rapid performance deterioration as the data dimensionality increases (Scott & Wand, 1991).
CDE based on neural networks. Most work in machine learning focuses on flexible paramet-
ric function approximators for CDE. In our experiments, we use the work of Bishop (1994) and
Ambrogioni et al. (2017), who propose to use a neural network to control the parameters of a mix-
ture density model. A recent trend in machine learning are latent density models such as cGANs
(Mirza & Osindero, 2014) and cVAEs (Sohn et al., 2015). Although such methods have been shown
successful for estimating distributions of images, the probability density function (PDF) of such
models is intractable. More promising in this sense are normalizing flows (Rezende & Mohamed,
2015; Dinh et al., 2017; Trippe & Turner, 2018), since they provide the PDF in tractable form. We
employ a neural network controlling the parameters of a normalizing flow as our third CDE model
to showcase the empirical efficacy of our regularization approach.
Regularization. Since neural network based CDE models suffer from severe over-fitting when
trained with the MLE objective, they require proper regularization. Classical regularization of the
parameters such as weight decay (Pratt & Hanson, 1989; Krogh & Hertz, 1992; Nowlan & Hinton,
1992), l1/l2-penalties (Mackay, 1992; Ng, 2004) and Bayesian priors (Murray & Edwards, 1993;
Hinton & Van Camp, 1993) have been shown to work well in the regression and classification set-
ting. However, in the context of CDE, it is less clear what kind of inductive bias such a regular-
ization imposes on the density estimate. In contrast, our regularization approach is agnostic w.r.t.
parametrization and is shown to penalize strong variations of the log-density function.
Regularization methods such as dropout are closely related to ensemble methods (Srivastava et al.,
2014). Thus, they are orthogonal to our work and can be freely combined with noise regularization.
Adding noise during training. Adding noise during training is a common scheme that has been
proposed in various forms. This includes noise on the neural network weights or activations (Wan
et al., 2013; Srivastava et al., 2014; Gal & Uk, 2016) and additive noise on the gradients for scalable
MCMC posterior inference (Welling & Teh, 2011; Chen et al., 2014). While this line of work corre-
sponds to noise in the parameter space, other research suggests to augment the training data through
random and/or adversarial transformations of the data (Sietsma & Dow, 1991; BUrges & SchOlkopf,
1996; Goodfellow et al., 2015; Yuan et al., 2017). Our approach transforms the training observa-
tions by adding small random pertUrbations. While this form of regUlarization has been stUdied in
the context of regression and classification problems (Holmstrom & Koistinen, 1992a; Webb, 1994;
Bishop, 1995; Natarajan et al., 2013; Maaten et al., 2013), this paper focUses on the regUlarization
of CDE. In particUlar, we bUild on top of the resUlts of Webb (1994) showing that training with noise
corresponds to a penalty on strong variations of the log-density and extend previoUs consistency
resUlts for regression of Holmstrom & Koistinen (1992a) to the more general setting of CDE. To oUr
best knowledge, this is also the first paper to evalUate the empirical efficacy of noise regUlarization
for density estimation.
4	Noise Regularization
When considering expressive families of conditional densities, standard maximUm likelihood esti-
mation of the model parameters θ is ill sUited. As can be observed in FigUre 1, simply minimizing
the negative log-likelihood of the data leads to severe over-fitting and poor generalization beyond
the training data. Hence, it is necessary to impose additional indUctive bias, for instance, in the form
of regUlarization. Unlike in regression or classification, the form of indUctive bias imposed by pop-
ular regularization techniques such as weight decay (Krogh & Hertz, 1991; Kukacka et al., 2017)
is less clear in the CDE setting, where the neUral network weights often only indirectly control the
probability density through a unconditional density model, e.g., a Gaussian Mixture.
We propose to add noise perturbations to the data points during the optimization of the log-likelihood
objective. This can be understood as replacing the original data points (xi , yi) by random variables
Xi = Xi + ξχ and yi = yi + ξy where the perturbation vectors are sampled from noise distribu-
3
Under review as a conference paper at ICLR 2020
Figure 1: Conditional MDN density estimate (red) and true conditional density (green) for different
noise regularization intensities hz ∈ {0.0, 0.05, 0.2}. The MDN has been fitted with 3000 samples
drawn from a conditional Gaussian.
tions Kx(ξx) and Ky(ξy) respectively. Further, we choose the noise to be zero centered as well as
identically and independently distributed among the data dimensions, with standard deviation h:
Eξ〜K(ξ) [ξ] = 0 and Eξ〜κ(ξ) [ξξ>]=后I	(4)
This can be seen as data augmentation, where “synthetic” data is generated by randomly perturbing
the original data. Since the supply of noise vectors is technically unlimited, an arbitrary large aug-
mented data set can be generated by repetitively sampling data points from D, and adding a random
perturbation vector to the respective data point. This procedure is formalized in Algorithm 1.
For notational brevity, We set Z := XXY, Z := (x>,y>)> and denote fθ(Z) := fθ(y|x). The
presented noise regularization approach is agnostic to whether we are concerned with unconditional
or conditional MLE. Thus, the generic notation also alloWs us to generalize the results to both
settings (derived in the remainder of the paper).
Algorithm 1 (Conditional) MLE With Noise
Regularization - Generic Procedure
Require: D = {Z1, ..., Zn}, noise intensity h
Require: number of perturbed samples r,
1:	for j = 1 to r do
2:	Select i ∈ {1, ..., n} With equal prob.
3:	Draw perturbation ξ 〜K
4:	Set Zj = Zi + hξ
5:	return argminθ∈θ 一 Pj=I log fθ(Zj)
Algorithm 2 (Conditional) MLE with Noise
Regularization - Mini-Batch Gradient Descent
Require: D = {Z1, ..., Zn}, noise intensity h
Require: learning rate α, mini-batch size m
1:	Initialize θ
2:	while θ not converged do
3:	Sample minibatch {Z1, ..., Zm} ⊂ D
4:	for j = 1 to m do
5:	Draw perturbation ξ 〜K
6:	Set ZZj = Zj + hξ
7:	θ  θ + αVθ Pm=Ilog fθ(Zj)
8:	return optimized parameter θ
When considering highly flexible parametric families such as Mixture Density Networks (MDNs)
(Bishop, 1994), the maximum likelihood solution in line 5 of Algorithm 1 is no longer tractable. In
such case, one typically resorts to numerical optimization techniques such as mini-batch gradient
descent and variations thereof. In this context, the generic procedure in Algorithm 1 can be trans-
formed into a simple extensions of mini-batch gradient descent on the MLE objective (see Algorithm
2). Specifically, each mini-batch is perturbed with i.i.d. noise before computing the MLE objective
function (forward pass) and the respective gradients (backward pass).
4.1	Variable Noise as Smoothness Regularization
Intuitively, the previously presented variable noise can be interpreted as “smearing” the data points
during the maximum likelihood estimation. This alleviates the jaggedness of the density estimate
arising from an un-regularized maximum likelihood objective in flexible density classes. We will
now give this intuition a formal foundation, by mathematically analyzing the effect of the noise
perturbations.
Before discussing the particular effects of randomly perturbing the data during conditional maxi-
mum likelihood estimation, we first analyze noise regularization in a more general case. Let l(D)
be a loss function over a set of data points D = {Z1, ..., Zn}, which can be partitioned into a sum of
losses l(D) = Pin=1 l(Zi), corresponding to each data point Zi: The expected loss l(Zi +ξ), resulting
from adding random perturbations, can be approximated by a second order Taylor expansion around
4
Under review as a conference paper at ICLR 2020
zi . Using the assumption about ξ in (4), the expected loss an be written as
Eξ〜K(ξ) [l(zi + ξ)] = l(zi) + 2Eξ〜K© [ξ>H⑴ξ] + O(ξ3) ≈ l(zi) + h2tr(H⑴)(5)
where l(zi) is the loss without noise and H(i)
z)z the Hessian of l w.r.t z, evaluated at
zi. Assuming that the noise ξ is small in its magnitude, O(ξ3) is negligible. This effect has been
observed earlier by Webb (1994) and Bishop (1994). See Appendix A for derivations.
When concerned With maximum likelihood estimation of a conditional density fθ(y|x), the loss
function coincides With the negative conditional log-likelihood l(y%, Xi) = - log fθ(yi |xi). Let the
standard deviation of the additive data noise ξx, ξy be hx and hy respectively. Maximum likelihood
estimation (MLE) With data noise is equivalent to minimizing the loss
n	h2 n dy
I(D) ≈-X log fθ (yi|Xi)- hy XX
i=1
i=1 j=1
-C	ʌ .	.	.
∂2 log fθ (y|x)
∂y(j) ∂y(j)
-hX X X ∂2 log fθ (y|x)
X = Xi	2	∂-χ( ∂χCj)∂χCj)
y=yi	i=1 j=1
X=Xi
y=yi
(6)
In that, the first term corresponds to the standard MLE objective, While the other tWo terms constitute
a form of smoothness regularization. The second term in (6) penalizes large negative second deriva-
tives of the conditional log density estimate log fθ(y|x) w.r.t. y. As the MLE objective pushes the
density estimate toWards high densities and strong concavity in the data points yi , the regularization
term counteracts this tendency to over-fit and overall smoothes the fitted distribution. The third term
penalizes large negative second derivatives W.r.t. the conditional variable x, thereby regularizing the
sensitivity of the density estimate to changes in the conditional variable. The intensity of the noise
regularization can be controlled through the variance (h2X and hy2 ) of the random perturbations.
Figure 1 illustrates the effect of the introduced noise regularization scheme on MDN estimates. Plain
maximum likelihood estimation (left) leads to strong over-fitting, resulting in a spiky distribution
that generalizes poorly beyond the training data. In contrast, training With noise regularization
(center and right) results in smoother density estimates that are closer to the true conditional density.
4.2	Consistency of Noise Regularization
We noW establish asymptotic consistency results for the proposed noise regularization. In particular,
We shoW that, under some regularity conditions, concerning integrability and decay of the noise
regularization, the solution of Algorithm 1 converges to the asymptotic MLE solution.
Let fθ(Z) : Rdz X Θ → (0, ∞) a continuous function of Z and θ. Moreover, we assume that the
parameter space Θ is compact. In the classical MLE setting, the idealized loss, corresponding to a
(conditional) M-projection of the true data distribution onto the parametric family, reads as
l(θ) = -Ep(Z) [log fθ(z)i	⑺
As we typically just have a finite number of samples from p(Z), the respective empirical estimate
ln(θ) = - 1 Pn=I log fθ (Zi), Zi i2d P(Z) is used as training objective. Note that we now define
the loss as function of θ, and, for fixed θ, treat ln(θ) as a random variable. Under some regularity
conditions, one can invoke the uniform law of large numbers to show consistency of the empirical
ML objective in the sense that suPθ∈θ ∣ln(θ) - l(θ) | --→ 0 (see Appendix B for details).
In case of the presented noise regularization scheme, the maximum likelihood estimation is per-
formed using on the augmented data {Zj} rather than the original data {Zn}. For our analy-
sis, we view Algorithm 1 from a slightly different angle. In fact, the data augmentation proce-
dure of uniformly selecting a data point from {Z1, ..., Zn} and perturbing it with a noise vector
drawn from K can be viewed as drawing i.i.d. samples from a kernel density estimate ^(z) =
1 Pn=ι hdzK (Z-zi) . Hence, maximum likelihood estimation with variable noise can be under-
stood as
1.	forming a kernel density estimate q((h) of the training data
2.	followed by a (conditional) M-projection of q((h) onto the parametric family.
5
Under review as a conference paper at ICLR 2020
In that, step 2 aims to find the θ* that minimizes the following objective:
Inh) (θ) = -E^nh)(z) [log fθ (z)i	⑻
Since (8) is generally intractable, r samples are drawn from the kernel density estimate, forming the
following Monte Carlo approximation of (8) which corresponds to the loss in line 5 Algorithm 1:
1r
ιnhr(θ) = --∑iogfθ(Zj),	Zj 〜qnh)(z)	⑼
r j=1
We are concerned with the consistency of the training procedure in Algorithm 1, similar
to the classical MLE consistency result discussed above. Hence, we need to show that
supθ∈θ Ifnr(θ) — l(θ) J -,-→ 0 as n,r → ∞. We begin our argument by decomposing the problem
into easier sub-problems. In particular, the triangle inequality is used to obtain the following upper
bound:
sup Mhr (θ) - i(θ)I ≤ sup Mhr (θ) - Inh (θ)I+sup ∣ιnh)(θ)-仙)|	(⑼
θ∈Θ	θ∈Θ	θ∈Θ
Note that lnh) (θ) is based on samples from the kernel density estimate, which are obtained by adding
random noise vectors ξ 〜K(∙) to our original training data. Since We can sample an unlimited
amount of such random noise vectors, r can be chosen arbitrarily high. This allows us to make
suPθ∈θ Pnhr (θ) - lnh)(θ)∣ arbitrary small by the uniform law of large numbers. In order to make
suPθ∈θ ∣lnh)(θ) - l(θ)∣ small in the limit n → ∞, the sequence of bandwidth parameters hn needs
to be chosen appropriately. Such results can then be combined using a union bound argument. In
the following we outline the steps leading us to the desired results. In that, the proof methodology
is similar to Holmstrom & Koistinen (1992b). While they show consistency results for regression
with a quadratic loss function, our proof deals with generic and inherently unbounded log-likelihood
objectives and thus holds for a much more general class of learning problems. The full proofs can
be found in the Appendix.
Initially, we have to make asymptotic integrability assumptions that ensure that the expectations
in ln(h) (θ) and l(θ) are well-behaved in the limit (see Appendix C for details). Given respective
integrability, we are able to obtain the following proposition.
Proposition 1 Suppose the regularity conditions (28) and (29) are satisfied, and that
lim hn = 0,	lim n(hn)d = ∞	(11)
n→∞	n→∞
Then,
lim sup ∣ln(h) (θ) - l(θ)∣ = 0
n→∞ θ∈Θ
(12)
almost surely.
In (11) we find conditions on the asymptotic behavior of the smoothing sequence (hn). These con-
ditions also give us valuable guidance on how to properly choose the noise intensity in line 4 of
Algorithm 1 (see Section 4.3 for discussion). The result in (12) demonstrates that, under the dis-
cussed conditions, replacing the empirical data distribution with a kernel density estimate still results
in an asymptotically consistent maximum likelihood objective. However, as previously discussed,
lnh) (θ) is intractable and, thus, replaced by its sample estimate Inr. Since we can draw an arbitrary
amount of samples from qnh, we can approximate lnh)(θ) with arbitrary precision. Given a fixed
data set D of size n > no, this means that limr→∞ suPθ∈θ『(hT (θ) - lnh) (θ)∣ = 0 almost surely, by
(29) and the uniform law of large numbers. Since our original goal was to also show consistency for
n → ∞, this result is combined with Proposition 1, obtaining the following consistency theorem.
Theorem 1 Suppose the regularity conditions (28) and (29) are satisfied, hn fulfills (11) and Θ is
compact. Then,
lim lim sup Ifnhr(θ) — l(θ)∣ = 0
n→∞ r→∞ θ∈Θ	,
(13)
almost surely.
6
Under review as a conference paper at ICLR 2020
In that, lim used to denote the limit superior (“lim sup")of a sequence.
Training a (conditional) density model with noise regularization means minimizing lnr) (θ) w.r.t. θ.
As result of this optimization, one obtains a parameter vector θ", which we hope is close to the
minimizing parameter G of the ideal objective function l(θ). In the following, we establish asymp-
totic consistency results, similar to Theorem 1, in the parameter space. Therefore we first have to
formalize the concept of closeness and optimality in the parameter space. Since a minimizing pa-
rameter θ of l(θ) may not be unique, we define Θ* = {θ* | l(θ*) ≤ l(θ) ∀θ ∈ Θ} as the set of global
minimizers of l(θ), and d(θ, Θ*) = minj*∈θ* {∣∣θ - θ* ∣∣2} as the distance of an arbitrary parameter
θ to Θ*. Based on these definitions, it can be shown that Algorithm 1 is asymptotically consistent in
(h)
a sense that the mmιmizer of θn,r converges almost surely to the set of optimal parameters Θ*.
Theorem 2 Suppose the regularity conditions (28) and (29) are satisfied, hn fulfills (11) and Θ is
compact. For r > 0 and n > no, let θ(hhr ∈ Θ be a global minimizer ofthe empirical objective *?.
Then
lim HE d(θ((hr, Θ*)=0
n→∞ r→∞	,
(14)
almost surely.
Note that Theorem 2 considers global optimizers, but equivalently holds for compact neighborhoods
of a local minimum θ* (see discussion in Appendix C).
4.3	Choosing the noise intensity
After discussing the properties of noise regularization, we are interested in how to properly choose
the noise intensity h, for different training data sets. Ideally, we would like to choose h so that
∣l(h)(θ) - l(θ)∣ is minimized, which is practically not feasible since l(θ) is intractable. Inequality
(30) gives as an upper bound on this quantity, suggesting to minimize l1 distance between the ker-
nel density estimate qn(h) and the data distribution p(z). This is in turn a well-studied problem in
the kernel density estimation literature (see e.g., Devroye & Luc (1987)). Unfortunately, general
solutions of this problem require knowing p(z) which is not the case in practice. Under the assump-
tion that p(z ) and the kernel function K are Gaussian, the optimal bandwidth can be derived as
h = 1.06σn-4+d (Silverman, 1986). In that, σ denotes the estimated standard deviation of the data,
n the number of data points and d the dimensionality of Z . This formula is widely known as the
rule of thumb and often used as a heuristic for choosing h.
In addition, the conditions in (11) give us further intuition. The first condition tells us that hn needs
to decay towards zero as n becomes large. This reflects the general theme in machine learning that
the more data is available, the less inductive bias / regularization should be imposed. The second
condition suggests that the bandwidth decay must happen at a rate slower than n-d. For instance,
the rule of thumb fulfills these two criteria and thus constitutes a useful guideline for selecting h.
However, for highly non-Gaussian data distributions, the respective hn may decay too slowly and a
faster decay rate such as n- 1+d may be appropriate.
5	Experiments
This section provides a detailed experimental analysis of the proposed method, aiming to empiri-
cally validate the theoretical arguments outlined previously and investigating the practical efficacy
of our regularization approach. In all experiments we use Gaussian pertubations of the data, i.e.,
K(ξ) = N(0, I). Since one of the key features of our noise regularization scheme is that it is ag-
nostic to the choice of model, we evaluate its performance on three different neural network based
CDE models: Mixture Density Networks (MDN) (Bishop, 1994), Kernel Mixture Networks (KMN)
(Ambrogioni et al., 2017) and Normalizing Flows Networks (NFN) (Rezende & Mohamed, 2015;
Trippe & Turner, 2018). In our experiments, we consider both simulated as well as real-world data
sets. In particular, we simulate data from a 4-dimensional Gaussian Mixture (dx = 2, dy = 2) and a
Skew-Normal distribution whose parameters are functionally dependent on x (dx = 1, dy = 1). In
terms of real-world data, we use the following three data sources:
Euro Stoxx: Daily stock-market returns of the Euro Stoxx 50 index conditioned on various stock
return factors relevant in finance (dx = 14, dy = 1).
7
Under review as a conference paper at ICLR 2020
-"lUJoN M&s
Mixture Density Network
0 5 0 5 0 5 0
0 2 5 7 0 2 5
7 7 7T---
p。。=,60 一s
∂JrηxUe-Ssne
Kernel Mixture Network
Normalizing Flow
1.60
1.55
1.50
1.45
1.40
1.60
1.55
1.50
1.45
1.40
102 IO3 104
105
102 IO3 104	105
1.60-
1.55-
1.50-
1.45-
1.40-
numberoftrain observations
1.351.,........................，..........，
IO2 IO3 10* IO5
1.35
IO2 IO3 10* IO5
1.35
ιo2 ιo3 ιo4 ιb5
Figure 2: Comparison of different noise intensity schedules hn and their implications on the Perfor-
mance of various CDE models across different training set sizes.
NYC Taxi: Drop-off locations of Manhattan taxi trips conditioned on the pickup location, week-
day and time (dx = 6, dy = 2).
UCI datasets: Standard data sets from the UCI machine learning repository (Dua & Graff, 2017).
In particular, Boston Housing (dx = 13, dy = 1), Concrete (dx = 8, dy = 1), Energy (dx = 9, dy =
1).
The reported scores are test log-likelihoods, averaged over at least 5 random seeds alongside the
respective standard deviation. For further details regarding the data sets and simulated data, we refer
to Appendix E. The experiment data and code is available at TODO
5.1	Noise intensity schedules
We complement the discussion in 4.3 with an empirical investigation of different schedules of hn .
In particular, We compare a) the rule of thumb hn α n- 4+d b) a square root decay schedule hn 8
n- 1+d c) a constant bandwidth hn = const. ∈ (0, ∞) and d) no noise regularization, i.e. hn = 0.
Figure 2 plots the respective test log-likelihoods against an increasing training set size n for the tWo
simulated densities Gaussian Mixture and Skew Normal.
First, we observe that bandwidth rates which conform with the decay conditions seem to converge in
performance to the non-regularized maximum likelihood estimator (red) as n becomes large. This
reflects the theoretical result of Theorem 1. Second, a fixed bandwidth across n (green), violating
(11), imposes asymptotic bias and thus saturates in performance vastly before its counterparts. Third,
as hypothesized, the relatively slow decay of hn through the rule of thumb works better for data
distributions that have larger similarities to a Gaussian, i.e., in our case the Skew Normal distribution.
In contrast, the highly non-Gaussian data from the Gaussian Mixture requires faster decay rates like
the square root decay schedule. Most importantly, noise regularization substantially improves the
estimator’s performance when only little training data is available.
5.2	Regularization Comparison
We now investigate how the proposed noise regularization scheme compares to classical regular-
ization techniques. In particular, we consider an l1 and l2-penalty on the neural network weights as
regularization term, the weight decay technique of Loshchilov & Hutter (2019)1, as well a Bayesian
neural network (Neal, 2012) trained with variational inference using a Gaussian prior and posterior
(Blei et al., 2017).
First, we study the performance of the regularization techniques on our two simulation benchmarks.
Figure 3 depicts the respective test log-likelihood across different training set sizes. For each regu-
larization method, the regularization hyper-parameter has been optimized via grid search.
As one would expect, the importance of regularization, i.e., performance difference to un-regularized
model, decreases as the amount of training data becomes larger. The noise regularization scheme
1 Note that an l2 regularizer and weight decay are not equivalent since we use the adaptive learning rate
technique Adam. See Loshchilov & Hutter (2019) for details.
8
Under review as a conference paper at ICLR 2020
Mixture Density Network
p8q=E=i0一-jss
3」n 戈一ussne
Kernel Mixture Network
Normalizing Flow
102	IO3	104
number of train observations
-emjoN Mys
IO2	103	104
IO2	IO3	104
IO2	IO3	IO4 IO2	IO3	IO4 IO2	IO3	IO4
Figure 3: Comparison of various regularization methods for three neural network based CDE mod-
els. The models are trained with simulated data sets of different sizes.
NCIn
Euro Stoxx NYC Taxi Boston Concrete Energy
noise (ours)	3.94±0.03	5.25±0.04	-2.49±0.11	-2.92±0.08	-1.04±0.09
weight decay	3.78±0.06	5.07±0.04	-3.29±0.32	-3.33±0.14	-1.21±0.10
l1 reg.	3.19±0.19	5.00±0.05	-4.01±0.36	-3.87±0.29	-1.44±0.22
l2 reg.	3.16±0.21	4.99±0.04	-4.64±0.52	-3.84±0.26	-1.55±0.26
Bayes	3.26±0.43	5.08±0.03	-3.46±0.47	-3.19±0.21	-1.25±0.23
Nra
noise (ours)	3.92±0.01	5.39±0.02	-2.52±0.08	-3.09±0.06	-1.62±0.06
weight decay	3.85±0.03	5.31±0.02	-2.69±0.15	-3.15±0.06	-1.79±0.12
l1 reg.	3.76±0.04	5.39±0.02	-2.75±0.13	-3.25±0.07	-1.82±0.10
l2 reg.	3.71±0.05	5.37±0.02	-2.66±0.13	-3.18±0.07	-1.79±0.13
Bayes	3.33±0.02	4.47±0.02	-3.40±0.11	-4.08±0.05	-3.65±0.07
-2.48±0.11	-3.03±0.13	-1.21±0.08
	noise (ours)	3.90±0.01	5.20±0.03
N 工 N	weight decay	3.82±0.06	5.19±0.03
	l1 reg.	3.50±0.10	5.12±0.05
	l2 reg.	3.50±0.09	5.13±0.05
	Bayes	3.34±0.33	5.10±0.03
-3.12±0.39	-3.12±0.14	-1.22±0.16
-12.58±12.76	-3.91±0.52	-1.29±0.16
-14.22±9.60	-3.99±0.66	-1.34±0.19
-5.99±2.45	-3.55±0.46	-1.11±0.22
Table 1: Comparison of various regularization methods for three neural network based CDE models
across 5 data sets. We report the test log-likelihood and its respective standard deviation (higher
log-likelihood values are better).
yields similar performance across the different CDE models while the other regularizers vary greatly
in their performance depending on the different models. This reflects the fact that noise regulariza-
tion is agnostic to the parameterization of the CDE model while regularizers in the parameter space
are dependent on the internal structure of the model. Most importantly, noise regularization per-
forms well across all models and sample sizes. In the great majority of configurations it outperforms
the other methods. Especially when little training data is available, noise regularization ensures a
moderate test error while the other approaches mostly fail to do so.
Next, we consider real world data sets. Since now the amount of data we can use for hyper-parameter
selection, training and evaluation is limited, we use 5-fold cross-validation to select the parameters
for each regularization method. The test log-likelihoods, reported in Table 1, are averages over 3
different train/test splits and 5 seeds each for initializing the neural networks. The held out test set
amounts to 20% of the overall data sets. Consistent with the results of the simulation study, noise
regularization outperforms the other methods across the great majority of data sets and CDE models.
5.3	Conditional Density Estimator Benchmark Study
We benchmark neural network based density estimators against state-of-the art CDE approaches.
While neural networks are the obvious choice when a large amount of training data is available, we
pose the questions how such estimators compete against well-established non-parametric methods
in small data regimes. In particular, we compare to the three following CDE methods:
Conditional Kernel Density Estimation (CKDE). Non-parametric method that forms a KDE of
both p(x, y) andP(X) to compute its estimate as p(y∣x) := p(x, y)∕p(x) (Li & Racine, 2007).
9
Under review as a conference paper at ICLR 2020
	Euro Stoxx	NCY Taxi	Boston	Conrete	Energy
num. train obs.	2536	8000	405	824	6Γ5
MDN	4.00±0.03	5.41±0.02	-2.39±0.02	-2.89±0.03	-1.04±0.05
KMN	3.98±0.03	5.42±0.02	-2.44±0.02	-3.06±0.03	-1.59±0.09
NFN	4.00±0.03	5.12±0.03	-2.40±0.04	-2.93±0.02	-1.23±0.06
LSCDE	3.44±0.10	4.85±0.02	-2.78±0.00	-3.63±0.00	-2.16±0.02
CKDE R.O.T.	3.36±0.01	4.87±0.02	-3.12±0.03	-3.78±0.02	-2.90±0.01
CKDE CV-ML	3.87±0.01	5.27±0.06	-2.76±0.26	-3.35±0.13	-1.14±0.02
NKDE R.O.T	3.16±0.02	4.34±0.04	-3.52±0.05	-4.08±0.02	-3.35±0.03
NKDE CV-ML	3.41±0.02	4.93±0.08	-3.34±0.13	-3.93±0.05	-2.21±0.12
Table 2: Comparison of conditional density estimators across 5 data sets. Reported is the test log-
likelihood and its respective standard deviation (higher log-likelihood values are better).
-Neighborhood kernel density estimation (NKDE). Non-parametric method that considers
only a local subset of training points to form a density estimate.
Least-Squares Conditional Density Estimation (LSCDE). Semi-parametric estimator that com-
putes the conditional density as linear combination of fixed kernels (Sugiyama & Takeuchi, 2010).
For the kernel density estimation based methods CKDE and NKDE, we perform bandwidth selection
via the rule of thumb (R.O.T) (Silverman, 1982; Sheather & Jones, 1991) and via maximum likeli-
hood leave-one-out cross-validation (CV-ML) (Rudemo, 1982; Hall et al., 1992). In case of LSCDE,
MDN, KMN and NFN, the respective hyper-parameters are selected via 5-fold cross-validation grid
search on the training set. Note that, in contrast to Section 5.2 which focuses on regularization pa-
rameters, the grid search here extends to more hyper-parameters. The respective test log-likelihood
scores are listed in Table 2. For the majority of data sets, all three neural network based methods
outperform all of the non- and semi-parametric methods. Perhaps surprisingly, it can be seen that,
when properly regularized, neural network based CDE works well even when training data is scarce,
such as in case of the Boston Housing data set.
6	Conclusion
This paper addresses conditional density estimation with high-capacity models. In particular, we
propose to add small random perturbations to the data during training. We demonstrate that the re-
sulting noise regularization method corresponds to a smoothness regularization and prove its asymp-
totic consistency. The experimental results underline the effectiveness of the proposed method,
demonstrating that it consistently outperforms other regularization methods across various condi-
tional density models and data sets. This makes neural network based CDE the preferable method,
even when only little training data is available. While we assess the estimator performance in terms
of the test log-likelihood, an interesting question for future research is whether the noise regular-
ization also improves the respective uncertainty estimates for downstream tasks such as safe control
and decision making.
References
LUca Ambrogioni, UmUt GuclU, Marcel A. J. van Gerven, and Eric Maris. The Kernel Mixture
Network: A Nonparametric Method for Conditional Density Estimation of Continuous Random
Variables. 2017. URL http://arxiv.org/abs/1705.07111.
Ji Andel, Ivan Netuka, and Karel Zvara. On Threshold Autoregressive Processes. Kybernet-
ica, 20(2):89-106,1984. URL https://dml.cz/bitstream/handle/10338.dmlcz/
124493/Kybernetika_20- 1984-2_1.pdf.
Chris M. Bishop. Training with Noise is EqUivalent to Tikhonov RegUlarization. Neural Com-
putation, 7(1):108-116, 1995. ISSN 0899-7667. doi: 10.1162/neco.1995.7.1.108. URL
http://www.mitpressjournals.org/doi/10.1162/neco.1995.7.1.108.
Christopher M Bishop. MixtUre Density Networks. 1994.
10
Under review as a conference paper at ICLR 2020
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational Inference: A Review for Statis-
ticians. Journal of the American Statistical Association, 112(518):859-877, 2017. doi: 10.
1080/01621459.2017.1285773. URL https://doi.org/10.1080/01621459.2017.
1285773.
Chris J. C. BUrges and Bernhard ScholkoPf.Improving the accuracy and speed of support vector ma-
chines. In NIPS, pp. 375-381. MIT Press, 1996. URL https://dl.acm.org/citation.
cfm?id=2999034.
Ricardo Cao, Antonio Cuevas, and Wensceslao Gonzalez Manteiga. A comparative study
of several smoothing methods in density estimation. Computational Statistics & Data
Analysis, 17(2):153-176, 2 1994. ISSN 0167-9473. doi: 10.1016/0167-9473(92)
00066-Z. URL https://www.sciencedirect.com/science/article/pii/
016794739200066Z?via%3Dihub.
Tianqi Chen, Emily B Fox, and Carlos Guestrin. Stochastic Gradient Hamiltonian Monte Carlo. In
ICML, 2014. URL https://arxiv.org/pdf/1402.4102.pdf.
Luc Devroye. The equivalence of weak, strong and complete convergence in L1 for kernel
density estimates. Annals of Statistics, 11(3):896-904, 1983. URL https://pdfs.
semanticscholar.org/71d9/b1c7a54cb48ab12bc3c8dcad626dc93d867b.
pdf.
Luc. Devroye and Luc. A course in density estimation. Birkhauser, 1987. ISBN 0817633650. URL
https://dl.acm.org/citation.cfm?id=27672.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. In
Proceedings of the International Conference on Learning Representations, 5 2017. URL http:
//arxiv.org/abs/1605.08803.
Dheeru Dua and Casey Graff. {UCI} Machine Learning Repository, 2017. URL http://
archive.ics.uci.edu/ml.
Vincent Dutordoir, Hugh Salimbeni, Marc Peter Deisenroth, and James Hensman. Gaussian Pro-
cess Conditional Density Estimation. In NeurIPS, 2018. URL https://papers.nips.cc/
paper/7506- gaussian- process- conditional- density- estimation.pdf.
Yarin Gal and Zg201@cam Ac Uk. Dropout as a Bayesian Approximation: Representing Model
Uncertainty in Deep Learning Zoubin Ghahramani. In ICML, 2016. URL http://yarin.co.
Nicolas Gilardi, Samy Bengio, and Mikhail Kanevski. Conditional Gaussian Mixture Models for
Environmental Risk Mapping. In NNSP, 2002. URL http://www.idiap.ch.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adversarial
Examples. In ICLR, 2015. URL http://arxiv.org/abs/1412.6572.
Hsi Guang Sung. Gaussian Mixture Regression and Classification. PhD thesis, 2004. URL http:
//www.stat.rice.edu/~hgsung/thesis.pdf.
Peter Hall, JS Marron, and Byeong U Park. Smoothed cross-validation. Probability The-
ory, 92:1-20, 1992. URL https://link.springer.com/content/pdf/10.1007%
2FBF01205233.pdf.
Geoorey E Hinton and Drew Van Camp. Keeping Neural Networks Simple by Minimizing the
Description Length of the Weights. In COLT, 1993. URL http://www.cs.toronto.edu/
~fritz/absps/colt93.pdf.
L. Holmstrom and P. Koistinen. Using additive noise in back-propagation training. IEEE Transac-
tions on Neural Networks, 3(1):24-38, 1992a. ISSN 10459227. doi: 10.1109/72.105415. URL
http://ieeexplore.ieee.org/document/105415/.
Lasse Holmstrom and Petri Koistinen. Using Additive Noise in Back Propagation Training. IEEE
Transactions on Neural Networks, 3(1):24-38, 1992b. ISSN 19410093. doi: 10.1109/72.105415.
URL http://ieeexplore.ieee.org/document/105415/.
11
Under review as a conference paper at ICLR 2020
Rob J. Hyndman, David M. Bashtannyk, and Gary K. Grunwald. Estimating and Visualizing Con-
ditional Densities. Journal of Computational and Graphical Statistics, 5(4):315, 12 1996. ISSN
10618600. doi: 10.2307/1390887. URL https://www.jstor.org/stable/1390887?
origin=crossref.
Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative Flow with Invertible 1x1 Convolu-
tions. Technical report, 7 2018. URL http://arxiv.org/abs/1807.03039.
Anders Krogh and John A Hertz. A Simple Weight Decay Can Improve Gen-
eralization. In NIPS, 1991. URL https://papers.nips.cc/paper/
563- a- simple- weight- decay- can- improve- generalization.pdf.
Anders Krogh and John A Hertz. A Simple Weight Decay Can Improve Gener-
alization. Technical report, 1992. URL https://papers.nips.cc/paper/
563- a- simple- weight- decay- can- improve- generalization.pdf.
Jan KUkacka, Vladimir Golkov, and Daniel Cremers. Regularization for Deep Learning: A Taxon-
omy. Technical report, 10 2017. URL http://arxiv.org/abs/1710.10686.
Qi Li and Jeffrey S. Racine. Nonparametric econometrics : theory and practice. Princeton Univer-
sity Press, 2007.
Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In ICLR, 2019. URL
https://github.com/loshchil/AdamW-and-SGDW.
Laurens Maaten, Minmin Chen, Stephen Tyree, and Kilian Weinberger. Learning with marginalized
corrupted features. In International Conference on Machine Learning, pp. 410-418, 2013.
David J C Mackay. A Practical Bayesian Framework for Backprop Networks. Neural Computa-
tion, 1992. URL http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.
1.1.29.274&rep=rep1&type=pdf.
Mehdi Mirza and Simon Osindero. Conditional Generative Adversarial Nets. Technical report, 11
2014. URL http://arxiv.org/abs/1411.1784.
Kevin P. Murphy. Machine Learning: A Probabilistic Perspective. 2012.
Alan F Murray and Peter J Edwards. Synaptic Weight Noise During MLP Learning Enhances Fault-
Tolerance, Generalisation and Learning Trajectory. In NIPS, 1993. URL https://pdfs.
semanticscholar.org/b0fc/40f4a4e9db0a67bf644cd1d509044fd3c6c8.
pdf.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In Advances in neural information processing systems, pp. 1196-1204, 2013.
Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business
Media, 2012.
Andrew Y Ng. Feature selection, L 1 vs. L 2 regularization, and rotational invariance. In ICML,
2004. URL http://ai.stanford.edu/~ang/Papers/icml04-l1l2.pdf.
Frank Nielsen. What is an information projection? Notices of the AMS, 2018. doi: 10.1090/
noti1647. URL http://dx.doi.org/10.1090/noti1647.
Steven J. Nowlan and Geoffrey E. Hinton. Simplifying Neural Networks by Soft Weight Sharing.
Neural Computation, 1992. URL http://www.cs.toronto.edu/~hinton/absps/
sunspots.pdf.
Emanuel Parzen. On Estimation ofa Probability Density Function and Mode. The Annals of Mathe-
matical Statistics, 33(3):1065-1076, 9 1962. ISSN 0003-4851. doi: 10.1214/aoms/1177704472.
URL http://projecteuclid.org/euclid.aoms/1177704472.
Lorien Y. Pratt and Stephen J. Hanson. Comparing Biases for Minimal Network Construction with
Back-Propagation. In NIPS, 1989.
12
Under review as a conference paper at ICLR 2020
Danilo Jimenez Rezende and Shakir Mohamed. Variational Inference with Normalizing Flows. In
Proceedings of the 32nd International Conference on Machine Learning, 5 2015. URL http:
//arxiv.org/abs/1505.05770.
Murray Rosenblatt. Remarks on Some Nonparametric Estimates of a Density Function. The An-
nals OfMathematical Statistics,27(3):832-837,9 1956. ISSN 0003-4851. doi: 10.1214/aoms/
1177728190. URL http://projecteuclid.org/euclid.aoms/1177728190.
Jonas Rothfuss, Fabio Ferreira, Simon Walther, and Maxim Ulrich. Conditional Density Estimation
with Neural Networks: Best Practices and Benchmarks. Technical report, 2019. URL http:
//arxiv.org/abs/1903.00954.
Mats Rudemo. Empirical Choice of Histograms and Kernel Density Estimators, 1982. URL
https://www.jstor.org/stable/4615859.
David W. Scott and M. P. Wand. Feasibility of Multivariate Density Estimates. Biometrika, 78(1):
197, 3 1991. ISSN 00063444. doi: 10.2307/2336910. URL https://www.jstor.org/
stable/2336910?origin=crossref.
S. J. Sheather and M. C. Jones. A Reliable Data-Based Bandwidth Selection Method for Kernel
Density Estimation. Journal of the Royal Statistical Society, 53:683-690, 1991. doi: 10.2307/
2345597. URL https://www.jstor.org/stable/2345597.
Jocelyn Sietsma and Robert J.F. Dow. Creating artificial neural networks that generalize.
Neural Networks, 4(1):67-79, 1 1991. ISSN 0893-6080. doi: 10.1016/0893-6080(91)
90033-2. URL https://www.sciencedirect.com/science/article/pii/
0893608091900332.
B. Silverman. On the estimation of a probability density function by the maximum penal-
ized likelihood method. The Annals of Statistics, 10(3):795-810, 1982. URL https://
projecteuclid.org/download/pdf_1/euclid.aos/1176345872.
B. Silverman. Density estimation for statistics and data analysis. Monographs on Statistics and
Applied Probability, 1986.
Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning Structured Output Representation using
Deep Conditional Generative Models. In Advances in Neural Information Processing Systems,
pp. 3483-3491, 2015.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Ma-
chine Learning Research, 15:1929-1958, 2014. URL http://jmlr.org/papers/v15/
srivastava14a.html.
Masashi Sugiyama and Ichiro Takeuchi. Conditional density estimation via Least-Squares Density
Ratio Estimation. In Proceedings of the Thirteenth International Conference on Artificial Intel-
ligence and Statistics, volume 9, pp. 781-788, 2010. URL http://machinelearning.
wustl.edu/mlpapers/paper_files/AISTATS2010_SugiyamaTSKHO10.pdf.
Volker Tresp. Mixtures of Gaussian Processes. In NIPS, 2001. URL https://papers.nips.
cc/paper/1900- mixtures- of- gaussian- processes.pdf.
Brian L Trippe and Richard E Turner. Conditional Density Estimation with Bayesian Normalising
Flows. Technical report, 2 2018. URL http://arxiv.org/abs/1802.04908.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of Neural Net-
works using DropConnect. In ICML, pp. 1058-1066, 2 2013. URL http://proceedings.
mlr.press/v28/wan13.html.
A.R. Webb. Functional approximation by feed-forward networks: a least-squares approach to gen-
eralization. IEEE Transactions on Neural Networks, 5(3):363-371, 5 1994. ISSN 10459227. doi:
10.1109/72.286908. URL http://ieeexplore.ieee.org/document/286908/.
13
Under review as a conference paper at ICLR 2020
Max Welling and Yee Whye Teh. Bayesian Learning via Stochastic Gradient Langevin Dynam-
ics. In ICML, 2011. URL https://www.ics.uci.edu/ ~welling/Publications/
papers/stoclangevin_v6.pdf.
Halbert White. Learning in Artificial Neural Networks: A Statistical Perspective. Neural Com-
Putation, 1(4):425-464, 12 1989. ISSN 0899-7667. doi: 10.1162∕neco.1989.1.4.425. URL
http://www.mitpressjournals.org/doi/10.1162/neco.1989.1.4.425.
Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. Adversarial Examples: Attacks and Defenses for
Deep Learning. Technical report, 12 2017. URL http://arxiv.org/abs/1712.07107.
14
Under review as a conference paper at ICLR 2020
A Derivation Smoothness Regularization
Let l(D) be a loss function over a set of data points D = {z1, ..., zN}, which can be partitioned into
a sum of losses corresponding to each data point xn :
n
lD(D) = X l(zi)	(15)
i=1
Also, let each Zi be perturbed by a random noise vector ξ 〜K(ξ) with zero mean and i.i.d. elements,
i.e.
Eξ〜K(ξ) [ξ] = 0 and Eξ〜κ(ξ) [ξnξ>] = h2I	(16)
The resulting loss l(zi + ξ) can be approximated by a second order Taylor expansion around zi
l(zi + ξ) = l(zi) + ξτVz l(z)∣zi + 2 ξ>V2 l(z)∣zi ξ + O(ξ3)	(17)
Assuming that the noise ξ is small in its magnitude, O(ξ3) may be neglected. The expected loss
under K(ξ) follows directly from (17):
Eξ〜K(ξ) [Qi + ξ)] = l(zi) + Eξ〜K(ξ) [ξTVxl(Z)Izii + 1 Eξ〜κ(ξ) [ξTVxl(Z)Iziξ]	(18)
Using the assumption about ξ in (16) we can simplify (18) as follows:
Eξ〜K(ξ) [l(zi +	ξ)]	=	I(Zi) +	Eξ〜K(ξ)	[ξ]> Vzl(z)	Izi	+ 1 Eξ〜K©	[ξτ V2l(z)	∣ziξ]	(19)
=l(zi) + 2Eξ〜K(ξ) [ξ>H(i)ξ]	(20)
=I(Zi) + 2 Eξ 〜K(ξ) [xx ξj ξk ∂Z(j)∂Z(k) I 1	QI)
j k	zi
=I(Zi)+2 X Eξ [ξ2] ∂Z(j)∂Zj)	+ 2 XX Eξ [ξj ξk] ∂Z(j)∂Z(k)
j	zi	j k6=j	zi
(22)
=I(Zi)+η2 X ∂⅛⅛‰zz	(23)
2
=l(Zi) + η2 tr(H(i))	(24)
In that, l(Zi) is the loss without noise and H(i) = Vz2l(Z)Iz the Hessian of l at Zi. With Z(j) we
denote the elements of the column vector Z .
B	Vanilla conditional MLE objective is uniformly consistent
The objective function corresponding to a conditional M-projection.
l(θ) = -Ep(x,y)[log fθ (y|x)i	(25)
The sample equivalent:
1n
以⑻=—nɪɪ^iogfθ(yi∣χi),	(χi,y**dp(x,y)	(26)
Corollary 1 Let Θ be a compact set and and fθ : Rl X Rm X Θ → (0, ∞) continuous in θ for all
(x, y) ∈ Rl x Rm such that Ep(x,y)卜upθ∈θ log fθ(y|x)] < ∞, Then, as n → ∞, we have
SUP ∣fn(θ) - l(θ)∣ —→ 0	(27)
θ∈Θ
Proof. The corollary follows directly from the uniform law of large numbers.
15
Under review as a conference paper at ICLR 2020
C Consistency Proofs
Lemma 1 Suppose for some > 0 there exists a constant Bp() such that
/ ∖ log fθ(z)∖1+ep(z)dz ≤ Bpe) < ∞ ∀θ ∈ Θ
and there exists an no such thatfor all n > no there exists a constant B^e) such that
/ ∖logfθ(z)∖ι+eqnιn')(z)dz ≤ Bqe) < ∞ ∀θ ∈ θ
almost surely. Then, the inequality
Sup 愣(θ) - i(θ)∣ ≤ Ce U Iqnh)(Z)-p(z)∖dz) 1+'
where Ce is a constant holds with probability 1 for all n > no.
Proof of Lemma 1 Using Hoelder's inequality and the nonnegativity of P and qnh), We obtain
(28)
(29)
(30)
ln(h)(θ) - l(θ)
≤
=Z
| log fθ(z)ll^nh)(z) -p(z)∣dz
1
1 + e
1+^
| log fθ(z)∖1+eqnι')(z)dz
1
+ / ∖ log fθ(z)∖1+ep(z)dz
1+^
Employing the regularity conditions (28) and (29) and writing C() =Bp> + B^e), it follows that
∃n0 such that ∀n > n0
sup ln(h)(θ) - l(θ) ≤ Bp() +
中

with probability 1.

Lemma 1 states regularity conditions ensuring that the expectations in ln(h) (θ) and l(θ) are well-
behaved in the limit. In particular, (28) and (29) imply uniform and absolute integrability of the log-
likelihoods under the respective probability measures induced by P and qnh). Since we are interested
in the asymptotic behavior, it is sufficient for (29) to hold for n large enough with probability 1.
Inequality (30) shows that we can make ∖ln(h)(θ) - l(θ)∖ small by reducing the l1-distance between
the true density P and the kernel density estimate qnh). There exists already a vast body of liter-
ature, discussing how to properly choose the kernel K and the bandwidth sequence (hn) so that
R ∖qnhn)(z) 一 p(z)∖dz → 0. We employ the results in Devroye (1983) for our purposes, leading us
to Proposition 1.
16
Under review as a conference paper at ICLR 2020
Proof of Proposition 1. Let A denote the event that ∃n0∀n > n0 inequality (30) holds for some
constant C() . From our regularity assumptions it follows that P(Ac) = 0. Given that A holds, we
just have to show that R ∣q((h)(z) - p(z)∣dz -.→ 0. Then, the upper bound in (30) tends to zero and
we can conclude our proposition.
For any δ > 0 let Bn denote the event
∕l^nh)(z)-P(z)∣dz ≤ δ	(31)
wherein q(h)(z) is a kernel density estimate obtained based on n samples from p(z). Under the
conditions in (11) we can apply Theorem 1 of Devroye (1983), obtaining an upper bound on the
probability that (31) does not hold, i.e. ∃u, m0 such that P(Bnc) ≤ e-un for all n > m0.
Since we need both A and Bn for n → ∞ to hold, we consider the intersection of the events
(A ∩ Bn ). Using a union bound argument it follows that ∃k0 such that ∀n > k0 : P((A ∩ Bn )c) ≤
P(Ac) + P(Bnc) = 0 + e-un = e-un . Note that we can simply choose k0 = max{n0, m0} for this
to hold. Hence, P∞=ko+ι P((A ∩ Bn)C) < Pn=I e-un = eu—ɪ < ∞ and by the Borel-Cantelli
lemma we can conclude that
lim sup ln(h)(θ) - l(θ) =0	(32)
n→∞ θ∈Θ
holds with probability 1.
Proof of Theorem 1. The inequality in (10) implies that for any n > n0,
lim sup	Mhr(θ)	- l(θ)∣	≤ lim sup	Mhr(θ)	- lnh)(θ)I	+ sup	Ilnh)(θ)	- l(θ)∣	(33)
r→∞ θ∈Θ ,	r→∞ θ∈Θ ,	θ∈Θ
Let n > n0 be fixed but arbitrary and denote
Jn,r = sup∣⅞hr(θ) - lnh)(θ)∣ r ∈ N,n>no	(34)
θ∈Θ	,
It is important to note that Jn,r is a random variable that depends on the samples Z(n) = (Zι,...,Zn)
as well as on the randomness inherent in Algorithm 1. We define I(r) = (lɪ,…Ir) as the indices
sampled uniformly from {1,…，n} and Ξ(r) = (ξι,…Xir) as the sequence of perturbation vectors
sampled from K. Let P(Z(n)), P (I(r)) and P (Ξ(r)) be probability measures of the respective
random sequences.
If We fix Z(n) to be equal to an arbitrary sequence Z(n), then q(h) is fixed and We can treat Jn,r
as the regular difference between a sample estimate and expectation under q((h). By the regularity
condition (29), the compactness of Θ and the continuity of fθ in θ, we can invoke the uniform law
of large numbers to show that
lim Jnr = IimsUP Ifnhr(θ) - 1^)(。) ∣ = 0	(35)
r→∞	r→∞ θ∈Θ	,
with probability 1.
Now we want to show that (35) also holds with probability 1 for random training samples Z(n).
First, we write Jn,r as a deterministic function of random variables:
Jn,r = J (Z(n), I(r), Ξ(r))	(36)
This allows us to restate the result in (35) as follows:
PI(r),Ξ(r) ∀δ >0∃r0∀r > r0 : J(Z(n) = Z(n), I(r), Ξ(r)) < δ
=	1	∀δ	> 0∃r0∀r	>	r0	:	J(Z(n) = Z(n), I(r),	Ξ(r))	< δ	dP (Ξ(r))dP (I(r))	(37)
=1
17
Under review as a conference paper at ICLR 2020
In that 1(A) denotes an indicator function which returns 1 if A is true and 0 else. Next we consider
the probability that the convergence in (35) holds for random Z(n) :
PZ(n),I(r),Ξ(r) ∀δ > 0∃r0∀r > r0 : J (Z(n), I(r), Ξ(r)) < δ
1 ∀δ > 0∃r0∀r > r0 : J (Z(n), I(r), Ξ(r)) < δ dP (Ξ(r))dP (I(r))dP (Z(n))
dP (Z(n))	1 ∀δ > 0 ∃r0 ∀r > r0 :
J(Z(n), I(r), Ξ(r)) < δ dP (Ξ(r))dP (I(r))
_____ - /
=1
1
Note that we can dP(Z(n)) move outside of the inner integrals, since Z(n) is independent from I(r)
and Ξ(r). Hence, we can conclude that (35) also holds, which we denote as event A, with probability
1 for random training data.
From Proposition 1 we know, that
lim sup ln(h)(θ) - l(θ) = 0	(38)
n→∞ θ∈Θ
with probability 1. We denote the event that (38) holds as B. Since P (Ac) = P (Bc) = 0, we can
use a union bound argument to show that P(A ∩ B) = 1. From (35) and (33) it follows that for any
n > n0,
lim SUp 腮(θ) - l(θ)∣ ≤ SUp 卜nh)(θ) - l(θ)∣	(39)
r→∞ θ∈Θ	,	θ∈Θ
with probability 1. Finally, we combine this result with (38), obtaining that
lim lim SUp 卜密⑹-l(θ)∣ = 0	(40)
n→∞ r→∞ θ∈Θ	,
almost surely, which concludes the proof.
Proof of Theorem 2. The proof follows the argument used in Theorem 1 of White (1989). In the
following, we assume that (13) holds. From Theorem 1 we know that this is the case with probability
1. Respectively, we only consider realizations of our training data Z(n) and noise samples I(r), Ξ(r)
for which the convergence in (13) holds (see proof of Theorem 1 for details on this notation).
For SUch realization, let (θ(h)) be minimizers of f(h/. Also let (n・and for any i, (r*j j be increas-
ing sequences of positive integers. Define Vij := θ(h)ri,j and μi,j (θ) := /，F.(θ). Due to the
compactness of Θ and the Bolzano-Weierstrass property thereof, there exists a limit point θ0 ∈ Θ
and increasing subsequences (ik)k, (jk)k so that vik,jk → θ0 as k → ∞.
From the triangle inequality, it follows that for any > 0 there exists k0 so that ∀k > k0
lμik,jk (Vikjk ) - l(θ0)1 ≤ lμik,jk (Vikjk ) - I(Vikjk )| + Il(Vikjk ) - l(°O) | < 2	(41)
given the convergence established in Theorem 1 and the continuity of l in θ. Next, the result above
is extended to
l(θ0)-l(θ) = [l(θ0) - μik,jk (Vikjk )] + [μik ,jk (Vik jk)-μikjk (θ)] + [μikjk (θ)-l(θ)] ≤ 3≡ (42)
which again holds for k large enough. This due to (41), μik,jk (Vikjk) - μikjk (θ) ≤ 0 since Vikjk is
the minimizer of μik jk, and μik,jk (θ) - l(θ) < e by Theorem 1. Because E can be made arbitrarily
small, l(θ0) ≤ l(θ) as k → ∞. Because θ ∈ Θ is arbitrary, θ0 must be in Θ*. In turn, since (ni)i,
(ri,j)j and (ik)k, (jk)k were chosen arbitrarily, every limit point of a sequence (Vik,jk )k must be in
θE	，
In the final step, we proof the theorem by contradiction. Suppose that (14) does not hold. In this case,
theremustexistan E > 0 andsequences (nj, (rij j and (ik)k, (jk ) SUChthat ∣∣ (Vik jk )k-0∣2 > E
for all k and θ ∈ Θ*. However, by the previous argument the limit point of the any sequence
(Vikjk)k must be in Θ*. That is a contradiction to ||(Vik jk)k - 0∣2 > e ∀ k, θ ∈ Θ*. Since
18
Under review as a conference paper at ICLR 2020
the random sequences Z(n), I(r), Ξ(r) where chosen from a set with probability mass of 1, we can
conclude our proposition that
lim lim d(θnhr, Θ*)=0
n→∞ r→∞	,
almost surely.
Discussion of Theorem 2. Note that, similar to θ*, θ(h/ does not have to be unique. In case there
are multiple minimizers of 璃,We can chose one of them arbitrarily and the proof of the theorem
still holds. Theorem 2 considers global optimizers over a set of parameters Θ, which may not be
attainable in practical settings. HoWever, the application of the theorem to the context of local
optimization is straightforWard When Θ is chosen as a compact neighborhood of a local minimum
θ* of l (Holmstrom & Koistinen, 1992b). If we set Θ* = {θ*} and restrict minimization over 然
to the local region, then θnh converges to Θ* as n, r → ∞ in the sense of Theorem 2.
D Conditional density estimation models
D.1 Mixture Density Network
Mixture Density Networks (MDNs) combine conventional neural networks with a mixture density
model for the purpose of estimating conditional distributions p(y|x) (Bishop, 1994). In particular,
the parameters of the unconditional mixture distribution p(y) are outputted by the neural network,
which takes the conditional variable x as input.
For our purpose, we employ a Gaussian Mixture Model (GMM) with diagonal covariance matrices
as density model. The conditional density estimatep^(y∣χ) follows as weighted sum of K Gaussians
K
双y|X) = X wk (x; θ)N (y|〃k (x; θ),σk(χ; θ))	(43)
k=1
wherein wk (x; θ) denote the weight, μk (x; θ) the mean and σk (x; θ) the variance of the k-th Gaus-
sian component. All the GMM parameters are governed by the neural network with parameters θ
and input x.
The mixing weights wk (x; θ) must resemble a categorical distribution, i.e. it must hold that
PkK=1 wk(x; θ) = 1 and wk (x; θ) ≥ 0 ∀k. To satisfy the conditions, the softmax linearity is used
for the output neurons corresponding to wk (x; θ). Similarly, the standard deviations σk (x) must
be positive, which is ensured by a sofplus non-linearity. Since the component means μk (x; θ) are
not subject to such restrictions, we use a linear output layer without non-linearity for the respective
output neurons.
For the experiments in 5.2 and 5.1, we set K = 10 and use a neural network with two hidden layers
of size 32.
D.1.1 Kernel Mixture Network
While MDNs resemble a purely parametric conditional density model, a closely related approach,
the Kernel Mixture Network (KMN), combines both non-parametric and parametric elements (Am-
brogioni et al., 2017). Similar to MDNs, a mixture density model of p(y) is combined with a neural
network which takes the conditional variable x as an input. However, the neural network only
controls the weights of the mixture components while the component centers and scales are fixed
w.r.t. to x. For each of the kernel centers, M different scale/bandwidth parameters σm are chosen.
As for MDNs, we employ Gaussians as mixture components, wherein the scale parameter directly
coincides with the standard deviation.
Let K be the number of kernel centers μk and M the number of different kernel scales σm . The
KMN conditional density estimate reads as follows:
KM
p(y∣x)= XX
wkm(x; θ)N (y|〃k ,σm)	(44)
k=1 m=1
19
Under review as a conference paper at ICLR 2020
As previously, the weights wk,m correspond to a softmax function. The M scale parameters σm are
Iearnedjointly with the neural network parameters θ. The centers μk are initially chosen by k-means
clustering on the {yi }in=1 in the training data set. Overall, the KMN model is more restrictive than
MDN as the locations and scales of the mixture components are fixed during inference and cannot
be controlled by the neural network. However, due to the reduced flexibility of KMNs, they are less
prone to over-fit than MDNs.
For the experiments in 5.2 and 5.1, we set K = 50 and M = 2. The respective neural network has
two hidden layers of size 32.
D.2 Normalizing Flow Network
The Normalizing Flow Network (NFN) is similar to the MDN and KMN in that a neural network
takes the conditional variable x as its input and outputs parameters for the distribution over y. For
the NFN, the distribution is given by a Normalizing Flow (Rezende & Mohamed, 2015). It works by
transforming a simple base distribution and an accordingly distributed random variable Z0 through
a series of invertible, parametrized mappings f = /n ◦…。f into a successively more complex
distribution p(f (Zo)). The PDF of samples ZN 〜p(f (Zo)) can be evaluted using the change-of-
variable formula:
N	∂f
logp(zo) - Vlog I det K----
n=1	∂zn-1
logp(zN)
(45)
The Normalizing Flows from Rezende & Mohamed (2015) were introduced in the context of pos-
terior estimation in variational inference. They are optimized for fast sampling while the likelihood
evaluation for externally provided data is comparatively slow. To make them useful for CDE, we
invert the direction of the flows, defining a mapping from the transformed distribution p(ZN) to the
base distributionp(Zo) by setting f-1(zi) = fi(zi).
We experimented with three types of flows: planar flows, radial flows as parametrized by Trippe &
Turner (2018) and affine flows f-1(z) = exp(a)z + b. We have found that one affine flow combined
with multiple radial flows performs favourably in most settings.
For the experiments in 5.2 and 5.1, we used a standard Gaussian as the base distribution that is
transformed through one affine flow and ten radial flows. The respective neural network has two
hidden layers of size 32.
E S imulated densities and datasets
E.1 SkewNormal
The data generating process (x, y) ~ p(χ, y) resembles a bivariate joint-distribution, wherein X ∈
R follows a normal distribution and y ∈ R a conditional skew-normal distribution (AndeI et al.,
1984). The parameters (ξ, ω, α) of the skew normal distribution are functionally dependent on x.
Specifically, the functional dependencies are the following:
X ~N(∙ μ = 0,σ = 2J
ξ(x) = a * x + b a, b ∈ R
ω(x) = C * x2 + d	c,d ∈ R
α(X) = αlow + J + ~—χ * (αhigh - αlow )
y ~ SkewNormal(ξ(x), ω(x), α(x))
(46)
(47)
(48)
(49)
(50)
Accordingly, the conditional probability density p(y|x) corresponds to the skew normal density
function:
p(y|x) = ω(X)N (y-xt) φ (a(x) y-ξF)	(51)
In that, N(∙) denotes the density, and Φ(∙) the cumulative distribution function of the standard nor-
mal distribution. The shape parameter α(x) controls the skewness and kurtosis of the distribution.
20
Under review as a conference paper at ICLR 2020
(a) SkewNormal
Figure 4: Conditional density simulation models. Conditional probability densities corresponding
to the different simulation models. The coloured graphs represent the probability densities p(y|x),
conditioned on different values of x.
(b) GaussianMixture
We set αlow = -4 and αhigh = 0, giving p(y|x) a negative skewness that decreases as x increases.
This distribution will allow us to evaluate the performance of the density estimators in presence of
skewness, a phenomenon that we often observe in financial market variables. Figure 4a illustrates
the conditional skew normal distribution.
E.2 Gaussian Mixture
The joint distribution p(x, y) follows a Gaussian Mixture Model in R4 with 5 Gaussian components,
i.e. K = 5. We assume that x ∈ R2 and y ∈ R2 can be factorized, i.e.
P(X, y) =): Wk N(y lμy,k,，y,k)N(Xlμx,k,，x,k)
i=1
(52)
When X and y can be factorized as in (52), the conditional density p(y|X) can be derived in closed
form:
K
P(y|X) = EWk(X) N(y|〃y,k, £y,k)
i=1
wherein the mixture weights are a function of X:
Wk(X)
wk N(xlμx,k, »x,k)
Pj = 1 wk N(X lμx,j, £x,j )
(53)
(54)
For details and derivations we refer the interested reader to Guang Sung (2004) and Gilardi et al.
(2002). The weights wk are sampled from a uniform distribution U(0, 1) and then normalized to sum
to one. The component means are sampled from a spherical Gaussian with zero mean and standard
deviation of σ = 1.5. The covariance matrices Σy,k) and Σy,k) are sampled from a Gaussian with
mean 1 and standard deviation 0.5, and then projected onto the cone of positive definite matrices.
Since we can hardly visualize a 4-dimensional GMM, Figure 4b depicts a 2-dimensional equivalent,
generated with the procedure explained above.
E.3 Euro S toxx 50 data
The Euro Stoxx 50 data comprises 3169 trading days, dated from January 2003 until June 2015.
The goal is to predict the conditional probability density of 1-day log-returns, conditioned on 14
explanatory variables. These conditional variables comprise classical return factors from finance
as well as option implied moments. For details, we refer to Rothfuss et al. (2019). Overall, the
target variable is one-dimensional, i.e. y ∈ Y ⊆ R, whereas the conditional variable X constitutes a
14-dimensional vector, i.e. X ∈ X ⊆ R14.
21
Under review as a conference paper at ICLR 2020
E.4 NYC TAXI DATA
We follow the setup in Dutordoir et al. (2018). The dataset contains records of taxi trips in the
Manhattan area operated in January 2016. The objective is to predict spatial distributions of the drop-
off location, based on the pick-up location, the day of the week, and the time of day. In that, the two
temporal features are represented as sine and cosine with natural periods. Accordingly, the target
variable y is 2-dimensional (longitude and latitude of dropoff-location) whereas the conditional
variable is 6-dimensional. From the ca. 1 million trips, we randomly sample 10,000 trips to serve as
training data.
E.5 UCI
Boston Housing Concerns the value of houses in the suburban area of Boston. Conditional
variables are mostly socio-economic as well as geographical factors. For more details see
https://archive.ics.uci.edu/ml/machine-learning-databases/housing/
Concrete The task is to predict the compressive strength of concrete given variables describ-
ing the conrete composition. For more details see https://archive.ics.uci.edu/ml/machine-learning-
databases/concrete/compressive/
Energy Concerns the energy efficiency of homes. The task is to predict the cooling
load based on features describing the build of the respective house. For more details see
https://archive.ics.uci.edu/ml/datasets/energy+efficiency
22