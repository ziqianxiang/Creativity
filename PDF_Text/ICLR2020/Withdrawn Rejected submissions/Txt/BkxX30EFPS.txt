Under review as a conference paper at ICLR 2020
Perceptual Generative Autoencoders
Anonymous authors
Paper under double-blind review
Ab stract
Modern generative models are usually designed to match target distributions di-
rectly in the data space, where the intrinsic dimensionality of data can be much
lower than the ambient dimensionality. We argue that this discrepancy may con-
tribute to the difficulties in training generative models. We therefore propose to
map both the generated and target distributions to the latent space using the en-
coder of a standard autoencoder, and train the generator (or decoder) to match the
target distribution in the latent space. The resulting method, perceptual generative
autoencoder (PGA), is then incorporated with a maximum likelihood or variational
autoencoder (VAE) objective to train the generative model. With maximum like-
lihood, PGAs generalize the idea of reversible generative models to unrestricted
neural network architectures and arbitrary latent dimensionalities. When combined
with VAEs, PGAs can generate sharper samples than vanilla VAEs. Compared
to other autoencoder-based generative models using simple priors, PGAs achieve
state-of-the-art FID scores on CIFAR-10 and CelebA.
1	Introduction
Recent years have witnessed great interest in generative models, mainly due to the success of
generative adversarial networks (GANs) (Goodfellow et al., 2014; Radford et al., 2016; Karras et al.,
2018; Brock et al., 2019). Despite their prevalence, the adversarial nature of GANs can lead to a
number of challenges, such as unstable training dynamics and mode collapse. Since the advent of
GANs, substantial efforts have been devoted to addressing these challenges (Salimans et al., 2016;
Arjovsky et al., 2017; Gulrajani et al., 2017; Miyato et al., 2018), while non-adversarial approaches
that are free of these issues have also gained attention. Examples include variational autoencoders
(VAEs) (Kingma & Welling, 2014), reversible generative models (Dinh et al., 2014; 2017; Kingma &
Dhariwal, 2018), and Wasserstein autoencoders (WAEs) (Tolstikhin et al., 2018).
However, non-adversarial approaches often have significant limitations. For instance, VAEs tend
to generate blurry samples, while reversible generative models require restricted neural network
architectures or solving neural differential equations (Grathwohl et al., 2019). Furthermore, to use the
change of variable formula, the latent space of a reversible model must have the same dimensionality
as the data space, which is unreasonable considering that real-world, high-dimensional data (e.g.,
images) tends to lie on low-dimensional manifolds, and thus results in redundant latent dimensions
and variability. Intriguingly, recent research (Arjovsky et al., 2017; Dai & Wipf, 2019) suggests that
the discrepancy between the intrinsic and ambient dimensionalities of data also contributes to the
difficulties in training GANs and VAEs.
In this work, we present a novel framework for training autoencoder-based generative models, with
non-adversarial losses and unrestricted neural network architectures. Given a standard autoencoder
and a target data distribution, instead of matching the target distribution in the data space, we map
both the generated and target distributions to the latent space using the encoder, and train the generator
(or decoder) to minimize the divergence between the mapped distributions. We prove, under mild
assumptions, that by minimizing a form of latent reconstruction error, matching the target distribution
in the latent space implies matching it in the data space. We call this framework perceptual generative
autoencoder (PGA). We show that PGAs enable training generative autoencoders with maximum
likelihood, without restrictions on architectures or latent dimensionalities. In addition, when combined
with VAEs, PGAs can generate sharper samples than vanilla VAEs.1
1Code is available at https://bit.ly/2U0kRYL.
1
Under review as a conference paper at ICLR 2020
We summarize our main contributions as follows:
•	A training framework, PGA, for generative autoencoders is developed to match the target
distribution in the latent space, which, we prove, ensures the matching in the data space.
•	We combine the PGA framework with maximum likelihood, and remove the restrictions of
reversible generative models on neural network architectures and latent dimensionalities.
•	We combine the PGA framework with VAE, which solves the problem of blurry samples,
without introducing any auxiliary models or sophisticated model architectures.
2	Related Work
Autoencoder-based generative models are trained by minimizing an data reconstruction loss with
regularizations. As an early approach, denoising autoencoders (DAEs) (Vincent et al., 2008) are
trained to recover the original input from an intentionally corrupted input. Then a generative model
can be obtained by sampling from a Markov chain (Bengio et al., 2013). To sample from a decoder
directly, most recent approaches resort to mapping a simple prior distribution to a data distribution
using the decoder. For instance, adversarial autoencoders (AAEs) (Makhzani et al., 2016) and
Wasserstein autoencoders (WAEs) (Tolstikhin et al., 2018) attempt to match the aggregated posterior
and the prior, either by adversarial training or by minimizing their Wasserstein distance. However,
due to the use of deterministic encoders, there can be “holes” in the latent space that are not covered
by the aggregated posterior, which would result in poor sample quality (Rubenstein et al., 2018). By
using stochastic encoders and variational inference, variational autoencoders (VAEs) are likely to
suffer less from this problem, but are known to generate blurry samples (Rezende & Viola, 2018; Dai
& Wipf, 2019). Nevertheless, as we will show, the latter problem can be addressed by moving the
VAE reconstruction loss from the data space to the latent space.
In a different line of work, reversible generative models (Dinh et al., 2014; 2017; Kingma & Dhariwal,
2018) are developed to enable exact inference. Consequently, by the change of variables theorem,
the likelihood of each data sample can be exactly computed and optimized. However, to avoid
expensive Jacobian determinant computations, reversible models can only be composed of restricted
transformations, rather than general neural network architectures. While this restriction can be
relaxed by utilizing recently developed neural ordinary differential equations (Chen et al., 2018;
Grathwohl et al., 2019), they still rely on a shared dimensionality between latent and data spaces,
which remains an unnatural restriction. In this work, we use the proposed training framework to trade
exact inference for unrestricted neural network architectures and arbitrary latent dimensionalities,
generalizing maximum likelihood training to autoencoder-based models.
3	Methods
3.1	Perceptual Generative Model
Let fφ : RD → RH be the encoder parameterized by φ, and gθ : RH → RD be the decoder
parameterized by θ. Our goal is to obtain a decoder-based generative model, which maps a simple
prior distribution to a target data distribution, D. A summary of notations is provided in Appendix A.
Throughout this paper, we use N (0, I) as the prior distribution.
For z ∈ RH, the output of the decoder, gθ (z), lies in a manifold that is at most H -dimensional.
Therefore, if we train the autoencoder to minimize
Lr = 2Ex〜D [kx - xk2i ,	(1)
where x = gθ (fφ (x)), then X can be seen as a projection of the input data, x, onto the manifold
of gθ (z). Let DD denote the reconstructed data distribution, i.e., X 〜力.Given enough capacity of
the encoder, DD is the best approximation to D (in terms of '2-distance), that we can obtain from the
decoder, and thus can serve as a surrogate target distribution for training the decoder-based generative
model.
Due to the difficulty in directly matching the generated distribution with the data-space target
distribution, D, we reuse the encoder to map D to a latent-space target distribution, H. We then
2
Under review as a conference paper at ICLR 2020
Figure 1: Illustration of the training process of PGAs. The overall loss function consists of (a) the
basic PGA losses, and either (b) the LPGA-specific losses or (c) the VPGA-specific losses. Circles
indicate where the gradient is truncated, and dashed lines indicate where the gradient is ignored when
updating parameters.
transform the problem of matching DD in the data space into matching H in the latent space. In other
words, We aim to ensure that for Z 〜N (0, I), if fφ (gθ (Z))〜 H, then gθ (Z) 〜力.In the following,
we define h = fφ ◦ gθ for notational convenience.
To this end, we minimize the following latent reconstruction loss w.r.t. φ:
Lφr,N = 2EZ〜N(0,I) [Uh (Z)- zk2].
(2)
Let Z (x) be the set of all Z’s that are mapped to the same x by gθ, we have the following theorem:
Theorem 1. Assuming E [Z | x] ∈ Z (x) for all x generated by gθ, and sufficient capacity of fφ; for
ʌ ∕∖
z 〜N (0, I), if Eq. (2) is minimized and h (z)〜H ,then gθ (z)〜D
We defer the proof to Appendix B.1. Note that Theorem 1 requires that different x’s generated by gθ
(from N (0, I) and H) are mapped to different Z’s by fφ. In theory, minimizing Eq. (2) would suffice,
since N (0, I) is supported on the whole RH. However, there can be Z’s with low probabilities
in N (0, I), but with high probabilities in H that are not well covered by Eq. (2). Therefore, it is
sometimes helpful to minimize another latent reconstruction loss on H:
LrH =2EZ〜H [kh (Z)-Zk2].	⑶
In practice, we observe that Lφφr,H is often small without explicit minimization, which we attribute to
its consistency with the minimization of Lr .
By Theorem 1, the problem of training the generative model reduces to training h to map N (0, I) to
H, which we refer to as the perceptual generative model. In the subsequent subsections, we present a
maximum likelihood approach, a VAE-based approach, and a unified approach to train the perceptual
generative model. The basic loss function of PGAs is given by
Lpga = Lr + αLφφr,N + βLφφr,H ,	(4)
where α and β are hyperparameters to be tuned. Eq. (4) is also illustrated in Fig. 1a.
3
Under review as a conference paper at ICLR 2020
3.2	A Maximum Likelihood Approach
We first assume the invertibility of h. For X 〜力,let Z = fφ (X)〜H. We can train h directly with
maximum likelihood using the change of variables formula as

Ez〜H^ [logP (Z)] = Ez~H logP (Z) - log
det
(5)
Ideally, we would like to maximize Eq. (5) only w.r.t. the parameters of the generative model (i.e.,
θ). However, directly optimizing the first term in Eq. (5) requires computing Z = h-1 (Z), which is
usually unknown. Nevertheless, for Z 〜H, we have h-1 (Z) = fφ (x) and X 〜D, and thus we can
minimize the following loss function w.r.t. φ instead:
LM = -Ez〜H [logP (z)] = 2Ex〜D hkfφ (X)k2].	⑹
To avoid computing the Jacobian in the second term of Eq. (5), which is slow for unrestricted
architectures, we approximate the Jacobian determinant and derive a loss function to be minimized
w.r.t. θ:
Lnll = HEiN(JlOg kh(Z + δ)- h (Z)k2# ≈ JJlOg det (牛)],(7)
2	kδ k2	∂Z
where S () can be either N 0, 2I , or a uniform distribution on a small (H - 1)-sphere of radius
centered at the origin. The latter choice is expected to introduce slightly less variance. We show
below that the approximation gives an upper bound when → 0. Eqs. (6) and (7) are illustrated in
Fig. 1b.
Proposition 1. For → 0,
log det (M )≤ f Eδ*Jog "h (Z +I；- h (Z)k2 # .	(8)
∂Z	2	kδk2
The inequality is tight if h is a multiple of the identity function around Z.
We defer the proof to Appendix B.2. We note that the above discussion relies on the invertibility of
h, which, however, is not required by the resulting method. Indeed, when h is invertible at some
point Z, the latent reconstruction loss ensures that h is close to the identity function around Z, and
hence the tightness of the upper bound in Eq. (8). Otherwise, when h is not invertible at some Z,
the logarithm of the Jacobian determinant at Z becomes infinite, in which case Eq. (5) cannot be
optimized. Nevertheless, since kh (Z + δ) - h (Z)k22 is unlikely to be zero if the model is properly
initialized, the approximation in Eq. (7) remains finite, and thus can be optimized regardless.
To summarize, we train the autoencoder to obtain a generative model by minimizing the following
loss function:
Llpga = Lpga + γ Lnll + Lnll .	(9)
We refer to this approach as maximum likelihood PGA (LPGA).
3.3	A VAE-based Approach
The original VAE is trained by maximizing the evidence lower bound on logP (X) as
logP (X) ≥ logP (X) - KL(q (Z0 | X) || P (Z0 | X))
=Ez0〜q(z0 | x) [logP (x | Z0)] — KL(q (z0 | x) || P (z0)),
(10)
where P (X | Z0) is modeled with the decoder, and q (Z0 | X) is modeled with the encoder. Note that
Z0 denotes the stochastic version of Z, whereas Z remains deterministic for the basic PGA losses in
Eqs. (2) and (3). In our case, we would like to modify Eq. (10) in a way that helps maximize logP (Z).
Therefore, we replace P (x | z0) on the r.h.s. of Eq. (io) with P (Z | z0), and derive a lower bound on
log P (Z) as
log P (z) ≥ log P (z) ― KL(q (z0 | x) || P (z0 | Z))
=Ez0〜q(z0 | x) [logP (Z | Z0)] — KL(q (z0 | x) || P (z0)).
(11)
4
Under review as a conference paper at ICLR 2020
Similar to the original VAE, We make the assumption that q (z0 | x) and P (Z | z0) are Gaussian;
i.e., q (z0 | x) = N(z0 I μφ (x), diag (σφ (x))), and P (Z | z0) = N (Zl μθ,φ (z0) ,σ2l). Here,
μφ (∙) = fφ (∙), μθ,φ (∙) = h (∙), and σ > 0 is a tunable scalar. Note that if σ is fixed, the first term
on the r.h.s. of Eq. (11) has a trivial maximum, where z, Z, and μθ,φ (z0) are all close to zero. To
circumvent this, we set σ proportional to the '2-norm of z.
The VAE variant is trained by minimizing
Lvae = Lvr + Ltkl = -Ex〜D 甩，〜q3 | χ) [logP (Z I z0)] - KL(q (z0 | x) || P ⑺)],(12)
where Lvr and Lvφkl are, respectively, the reconstruction and KL divergence losses of VAE, as
illustrated in Fig. 1c. Accordingly, the overall loss function is given by
Lvpga = Lpga + ηLvae.
(13)
We refer to this approach as variational PGA (VPGA).
3.4	A High-level View of the PGA Framework
We summarize what each loss term achieves, and explain from a high-level how they work together.
Data reconstruction loss (Eq. (1)): For Theorem 1 to hold, we need to use the reconstructed data
ʌ
distribution (D), instead of the original data distribution (D), as the target distribution. Therefore,
minimizing the data reconstruction loss ensures that the target distribution is close to the data
distribution.
Latent reconstruction loss (Eqs. (2) and (3)): The encoder (fφ) is reused to map data-space distri-
butions to the latent space. As shown by Theorem 1, minimizing the latent reconstruction loss (w.r.t.
the parameters of the encoder) ensures that if the generated distribution and the target distribution
ʌ
can be mapped to the same distribution (H) in the latent space by the encoder, then the generated
distribution and the target distribution are the same.
Maximum likelihood loss (Eqs. (6) and (7)) or VAE loss (Eq. (12)): The decoder (gθ) and encoder
(fφ) together can be considered as a perceptual generative model (fφ ◦ gθ), which is trained to map
ʌ
N (0, I) to the latent-space target distribution (H) by minimizing either the maximum likelihood loss
or the VAE loss.
The first loss allows to use the reconstructed data distribution as the target distribution. The second
loss transforms the problem of matching the target distribution in the data space into matching it
in the latent space. The latter problem is then solved by the third loss. Therefore, the three losses
together ensure that the generated distribution is close to the data distribution.
3.5	A Unified Approach
While the loss functions of maximum likelihood and VAE seem completely different in their original
forms, they share remarkable similarities when considered in the PGA framework (see Figs. 1b and
1c). Intuitively, observe that
LCtkl=Lnll+1 Ex 〜D X [σΦ,i (X)Tog (σφ,i(X))],	(14)
i∈[H]
which means both Ltnll and Lvtkl tend to attract the latent representations of data samples to the origin.
In addition, Lθnll expands the volume occupied by each sample in the latent space, which can be also
achieved by Lvr with the second term of Eq. (14).
More concretely, we observe that both Lθnll and Lvr are minimizing the difference between h (z) and
h (z + δ0), where δ0 is some additive zero-mean noise. However, they differ in that the variance of δ0
is fixed for Lθnll, but is trainable for Lvr; and the distance between h (z) and h (z + δ0) are defined in
two different ways. In fact, Lvr is a squared '2-distance derived from the Gaussian assumption on Z,
whereas Lnll can be derived similarly by assuming that dH = ∣∣Z - h (z + δ)kH follows a reciprocal
distribution as
p (dH; a, b) = dH (log(b) - log(a)),
(15)
5
Under review as a conference paper at ICLR 2020
where a ≤ dH ≤ b, and a > 0. The exact values of a and b are irrelevant, as they only appear in an
additive constant when we take the logarithm of p dH ; a, b .
Since there is no obvious reason for assuming Gaussian z, We can instead assume Z to follow the
distribution defined in Eq. (15), and multiply H by a tunable scalar, γ0, similar to σ. Furthermore,
We can replace δ in Eq. (7) with δ0 〜N(0, diag (σφ (x))), as it is defined for VPGA with a subtle
difference that here σφ2 (x) is constrained to be greater than 2 . As a result, LPGA and VPGA are
unified into a single approach, which has a combined loss function as
Llvpga = Lpga + γ0Lvr + γLφnll + ηLvφkl.	(16)
When γ0 = γ and η = 0, Eq. (16) is equivalent to Eq. (9), considering that σφ2 (x) will be optimized
to approach 2. Similarly, when γ = 0, Eq. (16) is equivalent to Eq. (13). Interestingly, it also
becomes possible to have a mix of LPGA and VPGA by setting all three hyperparameters to positive
values. We refer to this approach as LVPGA.
4 Experiments
In this section, we evaluate the performance of LPGA and VPGA on three image datasets, MNIST
(LeCun et al., 1998), CIFAR-10 (Krizhevsky & Hinton, 2009), and CelebA (Liu et al., 2015). For
CelebA, we employ the discriminator and generator architecture of DCGAN (Radford et al., 2016)
for the encoder and decoder of PGA. We half the number of filters (i.e., 64 filters for the first
convolutional layer) for faster experiments, while more filters are observed to improve performance.
Due to smaller input sizes, we reduce the number of convolutional layers accordingly for MNIST
and CIFAR-10, and add a fully-connected layer of 1024 units for MNIST, as done in Chen et al.
(2016). SGD with a momentum of 0.9 is used to train all models. Other hyperparameters are tuned
heuristically, and could be improved by a more extensive grid search. For fair comparison, σ is tuned
for both VAE and VPGA. All experiments are performed on a single GPU.
Table 1: FID scores of autoencoder-based generative models. The first block shows the results
from Ghosh et al. (2019), where CV-VAE stands for constant-variance VAE, and RAE stands for
regularized autoencoder. The second block shows our results of LPGA, VPGA, LVPGA, and VAE.
Model	MNIST	CIFAR-10	CelebA
VAE	19.21	106.37	48.12
CV-VAE	33.79	94.75	48.87
WAE	20.42	117.44	53.67
RAE-L2	22.22	80.80	51.13
RAE-SN	19.67	84.25	44.74
VAE	15.55 ± 0.18	115.74 ± 0.63	43.60 ± 0.33
LPGA	12.06 ± 0.12	55.87 ± 0.25	14.53 ± 0.52
VPGA	11.67 ± 0.21	51.51 ± 1.16	24.73 ± 1.25
LVPGA	11.45 ± 0.25	52.94 ± 0.89	13.80 ± 0.20
As shown in Fig. 2, the visual quality of the PGA-generated samples is significantly improved over
that of VAEs. In particular, PGAs generate much sharper samples on CIFAR-10 and CelebA compared
to vanilla VAEs. The results of LVPGA much resemble that of either LPGA or VPGA, depending on
the hyperparameter settings. In addition, we use the FreChet Inception Distance (FID) (Heusel et al.,
2017) to evaluate the proposed methods, as well as VAE. For each model and each dataset, we take
5,000 generated samples to compute the FID score. The results (with standard errors of 3 or more
runs) are summarized in Table. 1. Compared to other autoencoder-based non-adversarial approaches
(Tolstikhin et al., 2018; Kolouri et al., 2019; Ghosh et al., 2019), where similar but larger architectures
are used, we obtain substantially better FID scores on CIFAR-10 and CelebA. Note that the results
from Ghosh et al. (2019) shown in Table. 1 are obtained using slightly different architectures and
evaluation protocols. Nevertheless, their results of VAE align well with ours, suggesting a good
comparability of the results. Interestingly, as a unified approach, LVPGA can indeed combine the
best performances of LPGA and VPGA on different datasets. For CelebA, we further show results on
140x140 crops in Fig. 5, and latent space interpolations in Fig. 6 (see Appendix C).
6
Under review as a conference paper at ICLR 2020
157& 38<7×> 夕
72qq5 7a:。夕
6f。夕。Cae 74
6632∕3∖c
<735rgq4mγys
8 6 2/13。0,1
<√JI6 夕夕自 054
70(∕^rl9iFof^u∙
♦ 77夕/夕G，eg
4q7∕3^0'c‰7
Ilr7夕，CΓ39g2
。7325qo6J7
/ / C-6O 5 O 0∕Λ
so OqozXS 717
q0 夕？
0 4 7 qG √ F □/
yΓ/彳夕夕夕，Dz
G3∕3 794q 70
q/q^e。5∕oθ
///OU，/2。/
。夕/夕7夕XG 7 Λz
名 5q 74c?〃7r5 3
z∙Iυ>∕ 9 4r4夕
5 Z 4 i O 3 73，/
3 夕4 6 35s√9o
9 3 4夕 3 乎 7 9X9 70
3 I C∖ 7 G 5，qG 3
∕2^∕zro∕5 3∕ζ
S / ? I ) 7 ə α∙
OΓ-P÷-∕∕<<2∕O
(a) MNIST by LPGA
(d) CIFAR-10 by LPGA
(b) MNIST by VPGA
(e) CIFAR-10 by VPGA
(c) MNIST by VAE
(f) CIFAR-10 by VAE
(g) CelebA by LPGA
(h) CelebA by VPGA
Figure 2: Random samples generated by LPGA, VPGA, and VAE.
(i) CelebA by VAE

The training process of PGAs is stable in general, given the non-adversarial losses. As shown in
Fig. 3a, the total losses change little after the initial rapid drops. This is due to the fact that the
encoder and decoder are optimized towards different objectives, as can be observed from Eqs. (4),
(9), and (12). In contrast, the corresponding FIDs, shown in Fig. 3b, tend to decrease monotonically
during training. However, when trained on CelebA, there is a significant performance gap between
LPGA and VPGA, and the FID of the latter starts to increase after a certain point of training. We
suspect this phenomenon is related to the limited expressiveness of the variational posterior, which is
not an issue for LPGA.
It is worth noting that stability issues can occur when batch normalization (Ioffe & Szegedy, 2015) is
introduced, since both the encoder and decoder are fed with multiple batches drawn from different
distributions. At convergence, different input distributions to the decoder (e.g., H and N (0, I))
are expected to result in similar distributions of the internal representations, which, intriguingly,
can be imposed to some degree by batch normalization. Therefore, it is observed that when batch
normalization does not cause stability issues, it can substantially accelerate convergence and lead to
7
Under review as a conference paper at ICLR 2020


——LPGA_MNIST
LPG∕<CIFAR-10
—LPGA_CelebA
——VPGA_MNIST
——VPG∕<CIFAR-1O
——VPGA_CelebA
0
0	1	2	3	4	5	6
Iterations	1e5
2	3	4	5	6
Iterations	1e5
(a) Total loss	(b) FID
Figure 3:	Training curves of LPGA and VPGA.
slightly better generative performance. Furthermore, we observe that LPGA tends to be more stable
than VPGA in the presence of batch normalization.
Finally, we conduct an ablation study. While the loss functions of LPGA and VPGA both consist
of multiple components, they are all theoretically motivated and indispensable. Specifically, the
data reconstruction loss minimizes the discrepancy between the input data and its reconstruction.
Since the reconstructed data distribution serves as the surrogate target distribution, removing the
data reconstruction loss will result in a random target. Moreover, removing the maximum likelihood
loss of LPGA or the VAE loss of VPGA will leave the perceptual generative model untrained. In
both cases, no valid generative model can be obtained. Nevertheless, it is interesting to see how the
latent reconstruction loss contributes to the generative performance. Therefore, we retrain the LPGAs
without the latent reconstruction loss and report the results in Fig. 4. Compared to Fig. 2a, 2d, 2g,
and the results in Table 1, the performance significantly degrades both visually and quantitatively,
confirming the importance of the latent reconstruction loss.
√7Λ77J∕σ6 σo^∙Γ?
N2、a Γ~ f /。3
夕y75>∙v∙¼Jry4
r7SΛS7y^7^4
57，79c34q
57,夕 6pf，么 S
9¼<D 夕夕1 45
G√-<a3"s3-5 40
午夕 ∕fo3tts6Qq
(a) MNIST, FID = 18.13	(b) CIFAR-10, FID = 114.19	(c) CelebA, FID = 48.02
Figure 4:	Random samples generated by LPGA with α = β = 0.
5 Conclusion
We proposed a framework, PGA, for training autoencoder-based generative models, with non-
adversarial losses and unrestricted neural network architectures. By matching target distributions in
the latent space, PGAs trained with maximum likelihood generalize the idea of reversible generative
models to unrestricted neural network architectures and arbitrary latent dimensionalities. In addition,
it improves the performance of VAE when combined together. Under the PGA framework, we further
show that maximum likelihood and VAE can be unified into a single approach.
In principle, the PGA framework can be combined with any method that can train the perceptual
generative model. While we have only considered non-adversarial approaches, an interesting future
work would be to combine it with an adversarial discriminator trained on latent representations.
Moreover, the compatibility issue with batch normalization deserves further investigation.
8
Under review as a conference paper at ICLR 2020
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International Conference on Machine Learning, 2017.
Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising auto-encoders
as generative models. In Advances in Neural Information Processing Systems, pp. 899-907, 2013.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis. In International Conference on Learning Representations, 2019.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary dif-
ferential equations. In Advances in Neural Information Processing Systems, pp. 6571-6583,
2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems, pp. 2172-2180, 2016.
Bin Dai and David Wipf. Diagnosing and enhancing vae models. In International Conference on
Learning Representations, 2019.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components
estimation. In International Conference on Learning Representations Workshop, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In
International Conference on Learning Representations, 2017.
Partha Ghosh, Mehdi S. M. Sajjadi, Antonio Vergari, Michael Black, and Bernhard Scholkopf. From
variational to deterministic autoencoders. arXiv preprint arXiv:1903.12436, 2019.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pp. 2672-2680, 2014.
Will Grathwohl, Ricky TQ Chen, Jesse Betterncourt, Ilya Sutskever, and David Duvenaud. Ffjord:
Free-form continuous dynamics for scalable reversible generative models. In International Confer-
ence on Learning Representations, 2019.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp.
5767-5777, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems, pp. 6626-6637, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448-456,
2015.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for
improved quality, stability, and variation. In International Conference on Learning Representations,
2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference
on Learning Representations, 2014.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
Advances in Neural Information Processing Systems, pp. 10236-10245, 2018.
Soheil Kolouri, Phillip E Pope, Charles E Martin, and Gustavo K Rohde. Sliced wasserstein auto-
encoders. In International Conference on Learning Representations, 2019.
9
Under review as a conference paper at ICLR 2020
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, University of Toronto, 2009.
Yann LeCun, Corinna Cortes, and Chris J. C. Burges. The mnist handwritten digit database, 1998.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
International Conference on Computer Vision, 2015.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, and Ian Goodfellow. Adversarial autoencoders.
In International Conference on Learning Representations Workshop, 2016.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In International Conference on Learning Representations, 2018.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In International Conference on Learning Represen-
tations, 2016.
Danilo Jimenez Rezende and Fabio Viola. Taming vaes. arXiv preprint arXiv:1810.00597, 2018.
Paul K Rubenstein, Bernhard Schoelkopf, and Ilya Tolstikhin. On the latent space of wasserstein
auto-encoders. arXiv preprint arXiv:1802.03761, 2018.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in neural information processing systems, pp.
2234-2242, 2016.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders.
In International Conference on Learning Representations, 2018.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In International Conference on Machine
Learning, pp. 1096-1103. ACM, 2008.
A Notations
Table 2: Notations and definitions
fφ∕gθ	encoder/decoder of an autoencoder
h	h = fφ ◦ gθ	
φ∕θ	parameters of the encoder/decoder
D/H	dimensionality of the data/latent space
-D-	distribution of data samples denoted by X
H	distribution of fφ (x) for X 〜D
-D-	distribution of X = gθ (fφ (x)) for X 〜D
H	distribution of Z = h (Z) for Z 〜H
Lr	standard reconstruction loss of the autoencoder
Lφ Llr,N	latent reconstruction loss of PGA for Z 〜N (0, I), minimized w.r.t. φ
Lφ Llr,H	latent reconstruction loss of PGA for Z 〜H, minimized w.r.t. φ
Lφ Lnll	part of the negative log-likelihood loss of LPGA, minimized w.r.t. φ
-L Lnll	part of the negative log-likelihood loss of LPGA, minimized w.r.t. θ
Lvr	VAE reconstruction loss of VPGA
Lvkl	VAE KL-divergence loss of VPGA
Lvae	Lvae = Lvr + Lvkl, VAE loss OfVPGA	一
10
Under review as a conference paper at ICLR 2020
B Proofs
B.1 Theorem 1
Proof sketch. We first show that any different x’s generated by gθ are mapped to different Z’s by
fφ. Let x1 = gθ (Z1), x2 = gθ (Z2), and x1 6= x2. Since fφ has sufficient capacity and Eq. (2) is
minimized, we have fφ (x1) = E [Z1 | x1] and fφ (x2) = E [Z2 | x2]. By assumption, fφ (x1) ∈
Z (xι) and fφ (x2) ∈ Z (x2). Therefore, since Z (xι) ∩ Z (x2) = 0, We have fφ (xι) = fφ (x2).
For z 〜N (0, I), denote the distributions of gθ (z) and h (z), respectively, by D and H. We then
consider the case where D and D are discrete distributions. If gθ (Z) D, then there exists an x
that is generated by gθ, such that PHe (fφ (x)) = PDe (x) = PD (x) = PH (fφ (x)), contradicting that
h (z)〜H. The result still holds when D and D approach continuous distributions, in which case
D = D almost everywhere.	□
B.2 Proposition 1
Proof. Let J (Z) = ∂h (z) /∂z, P = [δι δ? … 5h], and P = J (Z) P = [δι δ2 … $h]
where ∆ = {δι, δ2,..., 6h} is an orthogonal set of H-dimensional vectors. Since det (P)=
det (J (z)) det (P), we have
log ∣det(J (z))| = log ∣det (P) ∣ - log ∣det(P)∣ .
(17)
By the geometric interpretation of determinants, the volume of the parallelotope spanned by ∆ is
Vol (∆) = |det (P)| =	kδik2,
(18)
i∈[H]
where [H] = {1, 2,..., H}. While ∆ = {δι, $2,..., $h} is not necessarily an orthogonal set, an
upper bound on Vol (∆) can be derived in a similar fashion. Let ∆k = {δι,%...,δk }, and ak be
the included angle between δk and the plane spanned by ∆k-1. We have
Vol (∆2) = Wwδιww2 BwwJina2,and Vol
Vol (∆k-i) WwMww 2 sin ak.	(19)
Given fixed wwδk ww ,
orthogonal; and Vol
, ∀k ∈ [H], Vol
is maximized when a2 = π∕2, i.e., δι and δ are
is maximized when Vol (∆k-ι) is maximized and ak = π∕2. By
induction on k, we can conclude that Vol (∆) is maximized when ∆ = ∆H is an orthogonal set,
and therefore
Vol (A) = ∣det (P)∣≤ Y 阿w2.
i∈[H]
Combining Eq. (17) with Eqs. (18) and (20), we obtain
log |det(J (z))| ≤ X (log wwiiww2- log l∣δik2).
i∈[H]
(20)
(21)
We proceed by randomizing ∆. Let ∆k = {δ1, δ2, . . . , δk}. We inductively construct an orthogonal
set, ∆ = ∆H. In step 1, δ1 is sampled from S (), a uniform distribution on a (H- 1)-sphere of
radius , S (), centered at the origin of an H-dimensional space. In step k, δk is sampled from
S (; ∆k-1), a uniform distribution on an (H-k)-sphere, S (; ∆k-1), in the orthogonal complement
of the space spanned by ∆k-1. Step k is repeated until H mutually orthogonal vectors are obtained.
Obviously, when k = H - 1, for all j >
S (δj | g∆H-ι) = S (δj | 0∆k). When 1
P (δj | ∆k) = S (δj | e; ∆k), We get
and j ≤ H, p (δj | ∆k) = p (δj | ∆H-1) =
k < H, assuming for all j > k and j ≤ H,
k
≤
p (δj | ∆k-1) =
S(a∆k-ι∪{δj})
p(δk | ∆k-1)p(δj | ∆k) dδk,
(22)
11
Under review as a conference paper at ICLR 2020
where S (; ∆k-1 ∪ {δj}) is in the orthogonal complement of the space spanned by ∆k-1 ∪ {δj }.
Since p(δk | ∆k-1) is a constant on S(δk | ; ∆k-1), and S(; ∆k-1 ∪ {δj}) ⊂ S (; ∆k-1),
p (δk | ∆k-1) is also a constant on S (; ∆k-1 ∪ {δj}). In addition, δk ∈ S (; ∆k-1 ∪ {δj}) im-
plies that δj ∈ S (; ∆k), on which p (δj | ∆k) is also a constant. Then it follows from Eq. (22)
that, for all δj ∈ S (; ∆k-1), p (δj | ∆k-1) is a constant. Therefore, for all j > k - 1 and j ≤ H,
p (δj | ∆k-1) = S (δj | ; ∆k-1). By backward induction on k, we conclude that the marginal
probability density of δk, for all k ∈ [H], is p (δk) = S (δk | ).
Since Eq. (21) holds for any randomly (as defined above) sampled ∆, we have
log ∣det(J (z))| ≤ Eδ X(log|W&WW - log 1同。
i∈[H]	2	(23)
=HEδ〜S(e) ∣⅛ WWδwW2 - log llδk2i ∙
If h is a multiple of the identity function around z, then J (z) = CI, where C ∈ R is a constant. In
this case, ∆ becomes an orthogonal set as ∆, and therefore the inequalities in Eqs. (20), (21), and (23)
become tight. Furthermore, it is straightforward to extend the above result to the case δ 〜 N 0, 2I ,
considering that N 0, 2I is a mixture of S () with different ’s.
The Taylor expansion of h around z gives
h (z + δ) = h (z) + J (z) δ + O (δ2) .	(24)
ʌ
Therefore, for δ → 0 or e → 0, we have δ = J (z) δ = h (z + δ) — h (z). The result follows. □
C More Results on CelebA
In Fig. 5, we compare the generated samples and FID scores of LPGA and VAE on 140x140 crops.
In this experiment, we use the full DCGAN architecture (i.e., 128 filters for the first convolutional
layer) for both LPGA and VAE. Other hyperparameter settings remain the same as for 108x108 crops.
In Fig. 6, we show latent space interpolations of CelebA samples.
(a) LPGA, FID = 21.35
(b) VAE, FID = 54.25
Figure 5: Random CelebA (140x140 crops) samples generated by LPGA and VAE.
12
Under review as a conference paper at ICLR 2020
(a) Interpolations generated by LPGA.
(b) Interpolations generated by VPGA.
(c) Interpolations generated by VAE.
Figure 6: Latent space interpolations on CelebA.
13