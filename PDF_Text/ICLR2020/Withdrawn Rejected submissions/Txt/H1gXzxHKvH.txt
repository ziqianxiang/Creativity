Under review as a conference paper at ICLR 2020
Deep Nonlinear Stochastic Optimal Control
for Systems with Multiplicative Uncertain-
TIES
Anonymous authors
Paper under double-blind review
Ab stract
We present a deep recurrent neural network architecture to solve a class of stochas-
tic optimal control problems described by fully nonlinear Hamilton Jacobi Bellman
partial differential equations. Such PDEs arise when one considers stochastic dy-
namics characterized by uncertainties that are additive, state dependent and control
multiplicative. Stochastic models with the aforementioned characteristics have
been used in computational neuroscience, biology, finance and aerospace systems
and provide a more accurate representation of actuation than models with only
additive uncertainty. Previous literature has established the inadequacy of the linear
HJB theory and instead relies on a non-linear Feynman-Kac lemma resulting in a
second order Forward-Backward Stochastic Differential Equations representation.
However, the proposed solutions that use this representation suffer from compound-
ing errors and computational complexity resulting in lack of scalability. In this
paper, we propose a deep learning based algorithm that leverages the second order
Forward-Backward SDE representation and LSTM based recurrent neural networks
to not only solve such Stochastic Optimal Control problems but also overcome
the problems faced by traditional approaches and shows promising scalability.
The resulting control algorithm is tested on nonlinear systems from robotics and
biomechanics in simulation to demonstrate feasibility and out-performance against
previous methods.
1	Introduction
Stochastic Optimal Control (SOC) is the center of decision making under uncertainty with a long
history and extensive prior work both in terms of theory as well as algorithms (Stengel, 1994; Fleming
& Soner, 2006). One of the most celebrated formulations of stochastic optimal control is for linear
dynamics and additive noise, known as the Linear Quadratic Gaussian (LQG) case (Stengel, 1994).
For stochastic systems that are nonlinear in the state and affine in control, the stochastic optimal
control formulation results in the Hamilton-Jacobi-Bellman (HJB) equation, which is a backward
nonlinear Partial Differential Equation (PDE). Solving the HJB PDE for high dimensional systems is
generally a challenging task and suffers from the curse of dimensionality.
Different algorithms have been derived to address stochastic optimal control problems and solve the
HJB equation. These algorithms can be mostly classified into two groups: (i) algorithms that rely
on linearization and (ii) algorithms that rely on sampling. Linearization-based algorithms rely on
first order Taylor’s approximation of dynamics (iterative LQG or iLQG) or quadratic approximation
of dynamics (Stochastic Differential Dynamic Programming), and quadratic approximation of the
cost function (Todorov & Li, 2005b; Theodorou et al., 2010b). Application of the aforementioned
algorithms is not straightforward and requires very small time discretization or special linearization
schemes especially for the cases of control and/or state dependent noise. It is also worth mentioning
that the convergence properties of these algorithms have not been investigated and remain open
questions. Sampling-based methods include the Markov-Chain Monte Carlo (MCMC) approximation
of the HJB equation (Kushner & Dupuis, 1992; Huynh et al., 2016). MCMC-based algorithms rely
on backward propagation of the value function on a pre-specified grid. Recently researchers have
incorporated tensor-train decomposition techniques to scale these methods (Gorodetsky et al., 2015).
However, these techniques have been applied to special classes of systems and stochastic control
1
Under review as a conference paper at ICLR 2020
problem formulations and have demonstrated limited applicability so far. Other methods include
transforming the SOC problem into a deterministic one by working with the Fokker-Planck PDE
representation of the system dynamics (AnnUnziato & Borzi, 2010; AnnUnziato & Borzi, 2013).
This approach requires solving the problem on a grid as well and therefore doesn’t scale to high
dimensional systems dUe to the aforementioned cUrse of dimensionality.
Alternative sampling-based methodologies rely on the probabilistic representation of the backward
PDEs and generalization of the so-called linear Feynman-Kac lemma (Karatzas & Shreve, 1991) to
its nonlinear version (PardoUx & RascanU, 2014). Application of the linear Feynman-Kac lemma
reqUires the exponential transformation of the valUe fUnction and certain assUmptions related to
the control cost matrix and the variance of the noise. Controls are then compUted Using forward
sampling of stochastic differential eqUations (Kappen, 2005; Todorov, 2007; 2009; TheodoroU et al.,
2010a). The nonlinear version of the Feynman-Kac lemma overcomes the aforementioned limitations.
However it reqUires a more sophisticated nUmerical scheme than jUst forward sampling, which relies
on the theory of Forward-Backward Stochastic Differential EqUations (FBSDEs) and their connection
to backward PDEs. The FBSDE formUlation is very general and has been Utilized in many problem
formUlations sUch as L2 and L1 stochastic control (Exarchos & TheodoroU, 2018; 2016; Exarchos
et al., 2018), min-max and risk-sensitive control (Exarchos et al., 2019) and control of systems with
control mUltiplicative noise (Bakshi et al., 2017). The major limitation of the algorithms that rely on
FBSDEs, is the compoUnding of errors from Least SqUares approximation Used at every timestep of
the Backward Stochastic Differential EqUation (BSDE).
Recent efforts in the area of Deep Learning for solving nonlinear PDEs have demonstrated encoUrag-
ing resUlts in terms of scalability and nUmerical efficiency. A Deep Learning-based algorithm was
introdUced by Han et al. (2018) to approximate the solUtion of non-linear parabolic PDEs throUgh their
connection to first order FBSDEs. However, the algorithm was tested on a simple high-dimensional
linear system for which an analytical solUtion already exists. A more recent approach by Pereira et al.
(2019) extended this work with a more efficient deep learning architectUre and sUccessfUlly applied
the algorithm for control tasks on non-linear systems. However, similar to Han et al. (2016), the
approach is only applicable to stochastic systems wherein noise is either additive or state dependent.
In this paper, we develop a novel Deep NeUral Network (DNN) architectUre for HJB PDEs that
correspond to SOC problems in which noise is not only additive or state-dependent bUt control
mUltiplicative as well. SUch problem formUlations are important in biomechanics and compUtational
neUroscience, aUtonomoUs systems, and finance (Todorov & Li, 2005a; Mitrovic et al., 2010; Primbs,
2007; McLane, 1971; Phillis, 1985). Prior work on stochastic optimal control of sUch systems
considers linear dynamics and qUadratic cost fUnctions. Attempts to generalize these linear methods
to the case of stochastic nonlinear dynamics with control mUltiplicative noise are only primitive
and reqUire special treatment in terms of methods to forward propagate and linearize the Underlying
stochastic dynamics (Torre & TheodoroU., 2015). Aforementioned probabilistic methods relying on
the linear Feynman-Kac lemmma cannot be derived in case of control mUltiplicative noise.
Below we sUmmarize the contribUtions of oUr work:
•	We design a novel DNN architectUre tailored to solve second-order FBSDEs (2FBSDEs).
The neUral network architectUre consists of FUlly Connected (FC) feed-forward and Long-
Short Term Memory (LSTM) recUrrent layers. The resUlting Deep 2FBSDE network can be
Used to solve fUlly nonlinear PDEs for nonlinear systems with control mUltiplicative noise.
•	We demonstrate the applicability and correctness of the proposed algorithm in foUr examples
ranging from, traditionally Used linear and nonlinear systems in control theory, to robotics
and biomechanics. The proposed algorithm recovers analytical controls in the case of linear
dynamics while it is also able to sUccessfUlly control nonlinear dynamics with control-
mUltiplicative and additive soUrces of Uncertainty. OUr simUlations show the robUstness
of the Deep 2FBSDE algorithm and prove the importance of considering the natUre of the
stochastic distUrbances in the problem formUlation as well as in neUral network architectUre.
The rest of the paper is organized as follows: in Section 2 we first introdUce notation, some prelimi-
naries and discUss the problem formUlation. Next, in Section 3 we provide the 2FBSDE formUlation.
The Deep 2FBSDE algorithm is introdUced in Section 4. Then we demonstrate and discUss resUlts
from oUr simUlation experiments in Section 5. Finally we conclUde the paper in Section 6 with
discUssion and fUtUre directions.
2
Under review as a conference paper at ICLR 2020
2	Stochastic Optimal Control
In this section we provide notations and stochastic optimal control concepts essential for the develop-
ment of our proposed algorithm and then present the problem formulation. Note that the bold faced
notation will be abused for both vectors and matrices, while non-bold faced will be used for scalars.
2.1	Preliminaries
We first introduce stochastic dynamical systems which have a drift component (i.e. the non-stochastic
component of the dynamics) that is a nonlinear function of the state but affine with respect to the
controls. The stochastic component is comprised of nonlinear functions of the state and affine control
multiplicative matrix coefficients. Let x ∈ Rnx be the vector of state variables, and u ∈ Rnu
be the vector of control variables taking values in the set of all admissible controls U ([0, T]), for
a fixed finite time horizon T ∈ [0, ∞). Let [v(t)T w(t)T]T	be a Brownian motion in
t∈[0,T]
Rnw+1, where v(t) ∈ R, w(t) ∈ Rnw and the components of w(t) are mutually independent one
dimensional standard Brownian motions. We now assume that functions f : [0, T] × Rnx → Rnx,
G : [0, T] × Rnx → Rnx×nu, Σ : [0, T] × Rnx → Rnx×nw and σ ∈ R+ satisfy certain Lipschitz
and growth conditions (see supplementary materials (SM) A).
Given this assumption, it is known that for every initial condition ξ ∈ Rnx , there exists a unique
solution x(t)	to the Forward Stochastic Differential Equation (FSDE),
t∈[0,T]
dx(t)
x(0)
(f (t, x(t)) + G(t, x(t))u(t)) dt + σG(t, x(t))u(t)dv(t) + Σ(t, x(t))dw(t)
'---------------V---------------} S---------------V-----------} S---------V--------}
drift vector	control multiplicative noise only state-dependent noise
(f (t, x(t)) + G(t, x(t))u(t))dt + C(t, x(t), u(t))de(t)
(1)
where C(t, x(t), u(t)) = σG(t, x(t))u(t), Σ(t, x(t)) and (t)
v(t)
w(t)
2.2	Problem Statement and HJB PDE
For the controlled stochastic dynamical system above, we formulate the SOC problem as minimizing
the following expected cost quadratic in control
J t, x(t); u(t)
EQ
T
/ (q(s, x(s)) + 2U(S)TRu(s))ds + φ(x(T))
t
(2)
where q : Rnx → R+ is the running state-dependent cost function, R (control cost coefficients) is a
symmetric positive definite matrix of size nu × nu and φ : Rnx → R+ is the terminal state cost. q
and φ are differentiable with continuous derivatives up to the second order. The expectation is taken
with respect to the probability measure Q over the space of trajectories induced by the controlled
stochastic dynamics in equation 1. We can define the value function as
(V(t,x(t))	= u(t)∈iuUJ(t, x(t);u(t))
IV(T, X(T)) = φ(x(T)).
(3)
Under the condition that the value function is once differentialble in t and twice differentiable in
x with continuous derivatives, we can follow the standard stochastic optimal control derivation to
obtain the HJB PDE (written without all the functional dependencies for simplicity)
(Vt + q + VTf - 2VTGRTGTVx + 1 tr(Vχχ∑∑T) = 0
V(T,x(T))=φ(x(T)),	(4)
with the optimal control of the form,
u*(t, X) = -R-1G(t, X)TVx(t, x).	(5)
where, R，(R + σ2GTVxxG). The derivation for both equations 4 and 5 can be found in SM B.
3
Under review as a conference paper at ICLR 2020
3	A FBSDE Solution to the HJB PDE
The theory of BSDEs establishes a connection between the solution of a parabolic PDE and a set of
FBSDEs (Pardoux & Peng, 1990; El Karoui et al., 1997). This connection has been used to solve the
HJB PDE in the context of SOC problems. Exarchos & Theodorou (2018) solved the HJB PDE in
the absence of control multiplicative noise dv with a set of first order FBSDEs. Bakshi et al. (2017)
utilized the second order FBSDEs or 2FBSDEs to solve the fully nonlinear HJB PDE in the presence
of control multiplicative noise, but did not consider any control in the FSDE. Lack of control leads
to insufficient exploration and for highly nonlinear systems renders it impossible to find an optimal
solution to complete the task. In light of this, we introduce a new set of 2FBSDEs
{dx	=	f dt + Gu*dt + CdW	(FSDE)
dV	=	H(V )dt + VXTGu*dt + VTCdW	(BSDEI)
dVX	=	H(Vχ)dt + Vxx Gu*dt + VXxCdw	(BSDE 2)	()
x(0) =ξ, V(T) = φ(x(T)), Vx(T)=φx(x(T)),
where U is the optimal control given by equation 5 and the operator H is defined as
H(∙)，∂t(∙) + ∂χ(∙)τf + 2tr(∂χχ(∙)CCτ).	⑺
Note that the functional dependencies are dropped in this and the following section for simplicity.
We can obtain this particular set of 2FBSDEs by substituting for the optimal control (equation 5) in
the forward process (equation 1). The backward processes are obtained by applying Ito's lemma (Ito,
1944), which is essentially the second order Taylor series expansion, to the value function V and
its gradient Vx . Additionally, we substitute the expression for Vt from equation 4 into BSDE 1 and
obtain the final form of the 2FBSDEs (detailed derivation is included in SM C) as
dx	= f dt + G(u*dt + σu*dv) + Σdw
dV	= -(q - 2VTGR-T(R + 2σ2GTVxXG)RTGTVx)dt
<	+VxτG(u*dt + σu*dv) + VXτ∑dw	(8)
dVX	= (A + VxX f )dt + VxXG(u* dt + σu* dv) + VxχΣdw
.x(0) = ξ, V(T) = φ(x(T)), Vx(T) = φx(x(T)),
with A = ∂t(Vx) + 1 tr(∂xx(Vx)CCτ).
Using equation 8, we can forward sample the FSDE. The second-order BSDEs (2BSDEs), on the
other hand, need to satisfy a terminal condition and therefore have to be propagated backwards in time.
However, since the uncertainty that enters the dynamics evolves forward in time, only the conditional
expectations of the 2BSDEs can be back-propagated. For more details please refer to Shreve (2004,
Chapter 2). Bakshi et al. (2017) back-propagate approximate conditional expectations, computed
using regression, of the two processes. This method however, suffers from compounding errors
introduced by least squares estimation at every time step. In contrast a Deep Learning (DL) based
approach, first introduced by Han et al. (2018), mitigates this problem by using the terminal condition
as the prediction target for a forward propagated BSDE. This is enabled by randomly initializing
the value function and its gradient at the start time and treating them as trainable parameters of a
self-supervised deep learning problem. In addition, the approximation errors at each time step are
compensated for by backpropagation during training of the DNN. This allowed using FBSDEs to
solve the HJB PDE for high-dimensional linear systems. A more recent approach, the Deep FBSDE
controller (Pereira et al., 2019), futher extends the work successfully to nonlinear systems, with and
without control constraints, in simulation that correspond to first order FBSDEs. It also introduced
a LSTM-based network architecture, in contrast to using separate feed-forward neural networks
at every timestep as in Han et al. (2016), that resulted in superior performance, memory and time
complexity. Extending this work, we propose a new framework for solving SOC problems of systems
with control multiplicative noise, for which the value function solutions correspond to 2FBSDEs.
4 Deep 2FBSDE Controller
In this section, we introduce a new deep network architecture called the Deep 2FBSDE Controller
and present a training algorithm to solve SOC problems with control multiplicative noise.
4
Under review as a conference paper at ICLR 2020
Figure 1: Deep 2FBSDE neural network architecture. The blocks F C1,2 are fully connected
layers with linear activations while the LST M block represents recurrent layers of stacked LSTM
cells with standard nonlinear activations. These layers are parameterized by θ and shared temporally.
Additionally, V (x0, t0) and Vx(x, t0) are parameterized by ψ and ζ respectively. The self-supervised
targets VN,匕,改 are the terminal conditions from equation 8 and VXx,n = Φxx(xn).
Time discretization: In order to approximate numerical solutions of the Stochastic Differential
Equations (SDEs) we choose the explicit Euler-Maruyama time-discretization scheme (Kloeden &
Platen, 2013, Chapter 9) similar to (Pereira et al., 2019; Han et al., 2018). Here we overload t as both
the continuous-time variable and discrete time index and discretize the task horizon 0 < t < T as
t = {0,1, ∙∙∙ , N}, where T = N∆t. This is also used to discretize all variables as step functions
if their discrete time index t lies in the interval [t∆t, (t + 1)∆t). We use subscript t to denote the
discretized variables in our proposed algorithm Alg. 1.
Algorithm 1: Finite Time Horizon Deep 2FBSDE Controller
Given: ξ, f, G, Σ, σ: Initial state, drift, actuation dynamics, state dependent noise matrix and
control multiplicative noise std. deviation; φ, q, R: Cost function parameters; N: Task horizon,
K: Number of iterations, M : Batch size; ∆t: Time discretization; λ: regularization parameter;
Parameters: V0 = V (x0; ψ): Value function at t = 0 parameterized by trainable weight ψ;
Vx,0 = Vx (x; ζ): Gradient of value function at t = 0 parameterized by trainable weights ζ;
θ: Weights and biases of all fully-connected and LSTM layers;
Initialize all trainable paramters and states: θ0 , ψ0 , ζ0 , x0 = ξ
for k = 1 to K do
for i = 1 to M do
for t = 0 to N - 1 do
Compute G matrix: Gti = G xit, t ;
Network prediction: VXχ,t, At = fFCι (flstm(x; 0k-'», fFC2 (∕lstm(x; θk-1))
Compute optimal control: Ut = -(R + σ2GtTVXχ,tGt)-1 GtTVx,/
Sample Brownian noise: ∆wj 〜N(0, I∆t); ∆vti ~ N(0, ∆t)
Compute control: Kt = K(t, Xi) = Gt (U*∆t + σu,∆vi)
Forward propagate the SDEs:
xi+1,vt+l, Vx,t+1 = f2FBSDE (Xt, Vi, Vx,t, Vxx,t, Kt) - time discretized equation 8
end for
end for
Compute mini-batch loss: L = fLoss(VN ,VN*,Vx,N , Vx*,N , Vxx,N , Vx*x,N , θk-1)
Gradient update: θk, ψk, Zk J Adam.step(L, θk-1,ψk-1,Zk-1)
end for
return θK , ψK , ζK
Network architecture: Inspired by the LSTM-based recurrent neural network architecture intro-
duced by Pereira et al. (2019) for solving first order FBSDEs, we propose the network in fig.1
adapted to the uncertainties in 2FBSDEs given by equation 8. Instead of predicting the gradient of
the value function Vx at every time step, the output of the LSTM is used to predict the Hessian of
the value function Vxx and A = ∂t(Vx) + 1 tr(∂xx(VX)CCT) using two separate FC layers with
5
Under review as a conference paper at ICLR 2020
linear activations. Notice that ∂xx(Vx) is a rank 3 tensor and using neural networks to predict this
term explicitly would render this method unscalable. We, however, bypass this problem by instead
predicting the trace of the tensor product which is a vector allowing linear growth in output size with
state dimensionality. Of these, Vxx is used to compute the control term K = G(u* dt + σu* dv).
This in turn is used to propagate the stochastic dynamics in equation 8. Both Vxx and A are used to
propagate Vx which is then used to propagate V . This is repeated until the end of the time horizon
as shown in fig.1. Finally, the predicted values of V , Vx and Vxx are compared with their targets
computed using x at the end of the horizon, to compute a loss function for backpropagation.
Algorithm: Algorithm 1 details the training procedure of the Deep 2FBSDE Controller. Note that
superscripts indicate batch index for variables and iteration number for trainable parameters. The
value function V0 and its gradient Vx,0 (at time index t = 0), are randomly initialized and trainable.
Functions fLSTM , fFC1 and fFC2 denote the forward propagation equations of standard LSTM
layers (Hochreiter & Schmidhuber, 1997) and FC layers with tanh and linear activations respectively,
and f2F BSDE represents a discretized version of equation 8 using the explicit Euler-Maruyama time
discretization scheme. The loss function (L) is computed using the given terminal conditions as
targets, the propagated value function (V), its gradient (Vx) and Hessian (Vxx) at the final time index
as well as an L2 regularization term as follows
L = ci ||V*- VN||2 + C2IIK - %,N||2 + C3 IICx - %x,N||2 + C4 IIV*I∣2 + λ∣∣θ∣∣2.	(9)
A detailed justification of each loss term is included in SM D. The network is trained using any
variant of Stochastic Gradient Descent (SGD) such as Adam (Kingma & Ba, 2014) until convergence.
The algorithm returns a trained network capable of computing an optimal feedback control at every
timestep starting from the given initial state.
5 S imulation Results
In this section we demonstrate the capability of the Deep 2FBSDE Controller on 4 different systems
in simulation. We first compare the solution of the Deep 2FBSDE Controller to the analytical solution
for a scalar linear system to validate the correctness of our proposed algorithm. We then consider a
cart-pole swing-up task and a reaching task for a 12-state quadcopter. Finally, we tested on a 2-link
6-muscle (10-state) human arm model for a planar reaching task. The results were compared against
the Deep FBSDE Controller in Pereira et al. (2019), where in the effect of control multiplicative
noise was ignored by only considering additive noise models resulting in first order FBSDEs, and the
iLQG controller in Li & Todorov (2007). Hereon we use 2FBSDE, FBSDE and iLQG to denote the
Deep 2FBSDE, Deep FBSDE and iLQG controllers respectively. The system dynamics can be found
in SM E and the simulation parameters can be found in SM F.
All the comparison plots contain statistics gath-
ered over 128 test trials. For 2FBSDE and FB-
SDE, we used time discretization ∆t = 0.004
seconds for the linear system problem, ∆t =
0.02 seconds for the cart-pole and quadcopter
simulations and ∆t = 0.02 seconds for the hu-
man arm simulation. For the iLQG simulations,
∆t = 0.01 seconds for cartpole and quadcopter
and ∆t = 0.001 seconds for the human arm
were used to avoid numerical instability. These
values were hand-tuned until reasonable perfor-
mance was observed from iLQG. In all plots,
the solid lines represent mean trajectories, and
the shaded regions represent the 68% confidence
region (mean ± standard deviation).
Figure 2: The 2FBSDE solution is very similar to
the analytical solution for a scalar linear system.
Linear System: We consider a scalar linear time-invariant system
dx(t) = Ax(t)dt + Bu(t)dt + σBu(t)dv(t) + Cdw(t),	(10)
with quadratic running state cost and terminal state cost q(χ(t)) = 1 Qx(t)2, φ(x(T)) = 2QTx(t)2.
The dynamics and cost function parameters are set as A = 0.2, B = 1.0, C = 0.1, σ = 0.5, Q =
0, QT = 80, R = 2, x0 = 1.0. The task is to drive the state to 0. Note that the problem is different
6
Under review as a conference paper at ICLR 2020
Figure 3: Cartpole Swing-up Task: 2FBSDE is most aware of the presence of control multiplicative
noise and uses least control effort to perform the task. This is evident from the cart velocity plot, a
directly actuated state, which tries to stay as close to zero as possible.
——2FBSDE ——FBSDE ——iLQG ——target
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00	0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00	0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Time (s)	Time (s)	Time (s)
Figure 4: Quadcopter Reach and Hover task: 2FBSDE clearly out-performs both FBSDE and
iLQG, as indicated by the smaller variance in all states.
than the one for LQG due to the presence of control multiplicative noise (i.e. v(t) 6= 0). Let us
assume that the value function (equation 4) has the form V(t, x(t)) = 2P(t)x2(t) + S(t)x(t) + c(t),
where S(T) = 0, P(T) = QT,c(t) = RT 1 c2P(s)ds. The values of P and Q are obtained by
solving corresponding Riccatti equations, using ODE solvers, that can be derived in a similar manner
to the LQG case in Stengel (1994, Chapter 5) and are used to compute the optimal control (equation 5)
at every timestep. The solution obtained from 2FBSDE is compared against the analytical solution in
fig. 2. The resulting trajectories have matching mean and comparable variance, which verifies the
effectiveness of the controller on linear systems.
Cartpole: We applied the controllers to cartpole dynamics for a swing-up task with a time horizon
of T = 2.0 seconds. The networks were trained using a batch size of 256 each for 2000 iterations.
We imposed a strict restriction on the controllers with control cost coefficient R = 0.5 and did not
impose any cost or target for the cart position state. We would like to highlight in fig. 3 that both
2FBSDE and iLQG choose to pre-swing the pendulum and use the system’s momentum to achieve
the swing-up task. They thus respect the presence of control multiplicative noise entering the system
as compared to FBSDE which tries to drive the pendulum to the desired vertical position as soon as
possible by applying as much control as required. Additionally, we tested for a lower control cost
R = 0.1 (see SM G for plots) and observed qualitatively different behavior from the controllers.
Quadcopter: The controller was also tested on a 12-state quadcopter dynamics model for a task
of reaching and hovering at a target position with a time horizon of 2.0 seconds. The network was
trained with a batch size of 256 for 5000 iterations. Only linear and angular states are included in fig.
4 since they most directly reflect the task performance (velocity plots included in SM G). The figure
demonstrates superior performance of the 2FBSDE controller over the FBSDE and iLQG controller
in reaching the target state faster and maintaining smaller state variance. Moreover, these results also
convey the importance of taking into account multiplicative noise in the design of optimal controllers
as the state dimensionality and system complexity increases. We also tested the 2FBSDE controller
7
Under review as a conference paper at ICLR 2020
Figure 5: Comparison of 2FBSDE and iLQG for Human Arm system: Plots show terminal cost
versus additive (Σ) and control multiplicative noise standard deviations (σ). The FBSDE results
are omitted as failure (divergence) occurred in all test trials. When additive noise is varied (left),
2FBSDE outperforms iLQG at lower Σ and performance deteriorates much slower than iLQG as Σ
increases. When multiplicative noise is varied (right), iLQG exhibits very erratic behaviour while the
terminal cost gradually increases with σ for 2FBSDE.
for the same task (reach and hover) for a longer horizon of 3.0 seconds (see SM G for plots) to verify
its stabilizing capability in presence of control multiplicative noise.
2-link 6-muscle Human Arm: This is a 10-state bio-mechanical system wherein control multiplica-
tive noise models have been found to closely mimic empirical observations (Todorov, 2005). The
controllers were tasked with reaching a target position in 1.0 second. The networks were trained with
a batch size of 256 for 2000 iterations. Additive noise in the joint-angle acceleration channels and
multiplicative noise in muscle activations were considered. In fig. 5, we show the log of terminal
state cost (log φ(xN)) versus different values of additive noise standard deviations (while holding
multiplicative noise standard deviation fixed at 0.1) on the left and versus different values of mul-
tiplicative noise standard deviations (keeping additive noise standard deviation fixed at 0) on the
right. As seen in the fig. 5, the performance of iLQG is very erratic compared to 2FBSDE as σ is
varied. The difference in behaviors can be attributed to the fact that iLQG is only aware of the first
two moments of the uncertainty entering the system while the 2FBSDE, being a sampling based
controller is exposed to the true uncertainty entering the system.
Discussion: Although the performance of iLQG for the cartpole and human arm tasks seem com-
petitive to 2FBSDE, we would like to highlight the fact that iLQG becomes brittle as σ of the
control multiplicative noise is increased (or time horizon is increased) and requires very fine time
discretizations, proper regularization scheduling and fine state cost coefficient tuning in order to be
able to converge. Moreover, the control cost (R) had to be tuned to a high value in order to prevent
divergence at higher noise standard deviations. This is in contrast to the results in Li & Todorov
(2007), on account of two main reasons: firstly they do not consider the presence of additive noise in
the angular acceleration channels in addition to multiplicative noise in the muscle activation channels
and secondly the paper does not provide all details to fully reproduce the published results nor is code
with muscle actuation made publicly available. Some of the crucial elements to fully implement the
human arm model such as relationships between muscle lengths, muscle velocities, and the respective
joint angles had to be obtained by studying the work done by Teka et al. (2017).
6 Conclusions
In this paper, we proposed the Deep 2FBSDE Controller to solve the Stochastic Optimal Control
problems for systems with additive, state dependent and control multiplicative noise. The algorithm
relies on the 2FBSDE formulation with control in the forward process for sufficient exploration. The
effectiveness of the algorithm is demonstrated by comparing against analytical solution for a linear
system, and against the first order FBSDE controller and the iLQG controller on systems of cartpole,
quadcopter and 2-link human arm in simulation. Potential future directions of this work include
application to financial models with intrinsic control multiplicative noise and theoretical analysis of
error bounds on value function approximation as well as investigating newer architectures.
8
Under review as a conference paper at ICLR 2020
References
Marun Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy
Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving,
Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man6,
Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner,
Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda ViCgas, Oriol
Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-
scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.org/. Software
available from tensorflow.org.
Mario Annunziato and Alfio Borzi. Optimal control of probability density functions of stochastic processes.
Mathematical Modelling and Analysis,15(4):393-407, 2010.
Mario Annunziato and Alfio Borzi. A fokker-planck control framework for multidimensional stochastic
processes. Journal of Computational and Applied Mathematics, 237(1):487-507, 2013.
K. S. Bakshi, D. D. Fan, and E. A. Theodorou. Stochastic control of systems with control multiplicative noise
using second order fbsdes. In 2017 American Control Conference (ACC), pp. 424-431, May 2017. doi:
10.23919/ACC.2017.7962990.
R. Bellman and R. Kalaba. Selected Papers On mathematical trends in Control Theory. Dover Publications,
1964.
Nicole El Karoui, Shige Peng, and Marie Claire Quenez. Backward stochastic differential equations in finance.
Mathematical finance, 7(1):1-71, 1997.
Ht M ElKholy. Dynamic modeling and control of a quadrotor using linear and nonlinear approaches. Master of
Science in Robotics, The American University in Cairo, 2014.
I. Exarchos and E. A. Theodorou. Learning optimal control via forward and backward stochastic differential
equations. In American Control Conference (ACC), 2016, pp. 2155-2161. IEEE, 2016.
I. Exarchos and E. A. Theodorou. Stochastic optimal control via forward and backward stochastic differential
equations and importance sampling. Automatica, 87:159-165, 2018.
I. Exarchos, E. A. Theodorou, and P. Tsiotras. Stochastic L1-optimal control via forward and backward sampling.
Systems & Control Letters, 118:101-108, 2018.
Ioannis Exarchos, Evangelos Theodorou, and Panagiotis Tsiotras. Stochastic differential games: A sampling
approach via fbsdes. Dynamic Games and Applications, 9(2):486-505, Jun 2019. ISSN 2153-0793. doi:
10.1007/s13235-018-0268-4. URL https://doi.org/10.1007/s13235-018-0268-4.
W. H. Fleming and H. M. Soner. Controlled Markov processes and viscosity solutions. Applications of
mathematics. Springer, New York, 2nd edition, 2006.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks.
In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249-256,
2010.
Alex Gorodetsky, Sertac Karaman, and Youssef Marzouk. Efficient high-dimensional stochastic optimal motion
control using tensor-train decomposition. In Proceedings of Robotics: Science and Systems, Rome, Italy, July
2015. doi: 10.15607/RSS.2015.XI.015.
Jiequn Han, Arnulf Jentzen, and Weinan E. Solving high-dimensional partial differential equations using deep
learning. Proceedings of the National Academy of Sciences, 115(34):8505-8510, 2018. ISSN 0027-8424.
doi: 10.1073/pnas.1718942115. URL https://www.pnas.org/content/115/34/8505.
Jiequn Han et al. Deep Learning Approximation for Stochastic Control Problems. arXiv preprint
arXiv:1611.07422, 2016.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735-1780,1997.
doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735.
V. A. Huynh, S. Karaman, and E. Frazzoli. An incremental sampling-based algorithm for stochastic optimal
control. I. J. Robotic Res., 35(4):305-333, 2016. doi: 10.1177/0278364915616866. URL http://dx.
doi.org/10.1177/0278364915616866.
Kiyosi It6. 109. stochastic integral. Proceedings ofthe Imperial Academy, 20(8):519-524, 1944.
9
Under review as a conference paper at ICLR 2020
H.	J. Kappen. Linear theory for control of nonlinear stochastic systems. Phys Rev Lett, 95:200201, 2005. Journal
Article United States.
I.	Karatzas and S. E. Shreve. Brownian Motion and Stochastic Calculus (Graduate Texts in Mathematics).
Springer, 2nd edition, August 1991. ISBN 0387976558.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
Peter E Kloeden and Eckhard Platen. Numerical solution of stochastic differential equations, volume 23. Springer
Science & Business Media, 2013.
H. J. Kushner and P. G. Dupuis. Numerical Methods for Stochastic Control Problems in Continuous Time.
Springer-Verlag, London, UK, UK, 1992. ISBN 0-387-97834-8.
Weiwei Li and Emanuel Todorov. Iterative linear quadratic regulator design for nonlinear biological movement
systems. In ICINCO(1),pp. 222-229, 2004.
Weiwei Li and Emanuel Todorov. Iterative linearization methods for approximately optimal control and
estimation of non-linear stochastic system. International Journal of Control, 80(9):1439-1453, 2007.
P. McLane. Optimal stochastic control of linear systems with state- and control-dependent disturbances. IEEE
Transactions on Automatic Control, 16(6):793-798, December 1971. ISSN 0018-9286. doi: 10.1109/TAC.
1971.1099828.
Djordje Mitrovic, Stefan Klanke, Rieko Osu, Mitsuo Kawato, and Sethu Vijayakumar. A computational model
of limb impedance control based on principles of internal model uncertainty. PLOS ONE, 5(10):1-11,
10 2010. doi: 10.1371/journal.pone.0013601. URL https://doi.org/10.1371/journal.pone.
0013601.
Etienne Pardoux and Shige Peng. Adapted solution of a backward stochastic differential equation. Systems &
Control Letters, 14(1):55-61, 1990.
Etienne Pardoux and Aurel Rascanu. Stochastic Differential Equations, Backward SDEs, Partial Differential
Equations, volume 69. 07 2014. doi: 10.1007/978-3-319-05714-9.
Marcus A Pereira, Ziyi Wang, Ioannis Exarchos, and Evangelos A Theodorou. Learning deep stochastic optimal
control policies using forward-backward sdes. In Robotics: science and systems, 2019.
Y. Phillis. Controller design of systems with multiplicative noise. IEEE Transactions on Automatic Control, 30
(10):1017-1019, October 1985. ISSN 0018-9286. doi: 10.1109/TAC.1985.1103828.
J. A. Primbs. Portfolio optimization applications of stochastic receding horizon control. In 2007 American
Control Conference, pp. 1811-1816, July 2007. doi: 10.1109/ACC.2007.4282251.
Steven E Shreve. Stochastic calculus for finance II: Continuous-time models, volume 11. Springer Science &
Business Media, 2004.
R. F. Stengel. Optimal control and estimation. Dover books on advanced mathematics. Dover Publications, New
York, 1994.
Wondimu W Teka, Khaldoun C Hamade, William H Barnett, Taegyo Kim, Sergey N Markin, Ilya A Rybak, and
Yaroslav I Molkov. From the motor cortex to the movement and back again. PloS one, 12(6):e0179288, 2017.
E.A. Theodorou, J. Buchli, and S. Schaal. A generalized path integral approach to reinforcement learning.
Journal of Machine Learning Research, (11):3137-3181, 2010a.
E.A. Theodorou, Y. Tassa, and E. Todorov. Stochastic differential dynamic programming. In American Control
Conference, pp. 1125-1132, 2010b.
E. Todorov. Stochastic optimal control and estimation methods adapted to the noise characteristics of the
sensorimotor system. Neural Computation, 17(5):1084, 2005.
E. Todorov. Linearly-solvable markov decision problems. In B. Scholkopf, J. Platt, and T. Hoffman (eds.),
Advances in Neural Information Processing Systems 19, Vancouver, BC, 2007. Cambridge, MA: MIT Press.
E. Todorov. Efficient computation of optimal actions. Proceedings of the national academy of sciences, 106(28):
11478-11483, 2009.
10
Under review as a conference paper at ICLR 2020
E. Todorov and W. Li. A generalized iterative lqg method for locally-optimal feedback control of constrained
nonlinear stochastic systems. pp. 300-306, 2005a.
E. Todorov and Weiwei Li. A generalized iterative lqg method for locally-optimal feedback control of constrained
nonlinear stochastic systems. In Proceedings of the 2005, American Control Conference, 2005., pp. 300-306
vol. 1, June 2005b. doi: 10.1109/ACC.2005.1469949.
G. DeLa Torre and E.A. Theodorou. Stochastic variational integrators for system propagation and linearization.
Sept 2015.
11
Under review as a conference paper at ICLR 2020
Supplementary Materials
A	Assumptions
Assumption 1. There exists a constant k > 0 such that
|(f + Gu)(t, x, u) - (f + Gu)(t, x0,u0)| ≤ k(|x - x0| + |u - u0|)	(11)
|C(t, x, u) - C(t, x0, u0)| ≤ k(|x - x0| + |u - u0|)	(12)
|(f + Gu)(t, x, u) + C(t, x)| ≤ k(1+ |x|),	(13)
where C = [σGu Σ], ∀t ∈ [0,T], ∀x, x0 ∈ Rnx and ∀u, u0 ∈ Rnu.
B HJB PDE DERIVATION
Applying the dynamic programming principle (Bellman & Kalaba, 1964) to the value function we have
t+dt
V(t, x(t)) = ()inf	DEQ / (q(x(s)) + ^u(s)TRu(s))ds + V(t + dt, x(t + dt)) .	(14)
t
Then, We can approximate the running cost integral With a step function and apply Ito's lemma (Ito, 1944) to
obtain
V(t, x(t)) = infEq (q(x(t)) + Ju(t)TRu(t))dt + V(t + dt, x(t + dt))
=inf Eq (q(x(t)) + ^u(t)TRu(t))dt + V(t, x(t)) + Vt(t, x(t))dt
u∈U	2
+ VxT (t, x(t)) f (t, x(t))dt + G(t, x(t))(udt + σu(t)dv(t))
+ Σ(t, x(t))dw(t)) + 2tr(Vχχ(t, x(t))C(t, x(t), u(t))CT(t, x(t), u(t)))dt∣
=inf (q(x(t)) + 1 u(t)TRu(t))dt + V(t, x(t)) + Vt(t, x(t))dt
u∈U	2
+ VxT(t, x(t))f (t, x(t))dt + G(t, x(t))u(t)dt
+ 1tr(Vχx(t, x(t))C(t, x(t), u(t))CT(t, x(t), u(t)))] dt
We can cancel V(t, x(t)) on both sides and bring the terms not dependent on controls outside of the infimum to
get
Vt + VTf + q + °einfr ]{ 2 UTRu + VTGu + 2tr(VXχCCτ)0 = 0	(15)
Note that the functional dependencies have been dropped for brevity. NoW We can find the optimal control by
taking the gradient of the terms inside the infimum With respect to u and setting it to zero. Before that We first
substitute for C = σGu, Σ so that the term inside the trace operator becomes σ2VxxGuuTGT+VxxΣΣT.
By using the property of trace operators, We can Write, tr(Vxx CCT) = tr(σ2Vxx GuuTGT + Vxx ΣΣT) =
tr(σ2uTGTVxxGu) + tr(VxxΣΣT). Note that the second term is not a function of u and therefore is zero
When We take the gradient. Additionally, the first term is a scalar and taking its trace returns the same scalar.
NoW, taking the gradient W.r.t u, We get,
Ru* + (VTG)T + σ2GτVχχGu* = 0
⇒(R + σ2GτVXχG)u* = -(GTVx)
⇒R u* = -(GTVx)
⇒u* = -R TGTVx.
where, R，(R + σ2GTVxχG).
12
Under review as a conference paper at ICLR 2020
The optimal control above can be plugged back into equation 15 to obtain
Vt + VTf + q + 1 VxrGRTRRTGTVx - VxrGRTGTVx
+ 2σ2VxrGR-TGTVxxGRTGTVx + ∣ tr(VxxΣΣT) = 0
⇒ Vt + VxTf + q + 2VTGRT(R + b2GT%xG)RTGT匕
-VTgRTGT匕 + 1 tr(%xΣΣT) = 0
IT
⇒ ½ + VxTf + q + 2匕rGRTRRTGT匕
-VTGRTGTKi + 1 tr(Kx∑∑τ) = 0
⇒ ½ + VTf + q - 2VTGRTGT匕 + 2 tr(‰ΣΣτ) = 0
which gives the HJB PDE as mentioned in equation 4.
C Derivation of the 2FBSDEs
Firstly we can apply Ito,s differentiation rule to V:
dV = ½dt + VxTdx + 1 tr (VxxCCτ)dt.	(16)
Since V is the value function, we can substitute in the HJB PDE for Vt and forward dynamics for dx to get:
dV = -(q + VTf - 1VTGR-1 GτVx + 1 tr (%xΣΣτ))dt
+ VxTT (fdt + G(u*dt + σu*dv) + Σdw) + 1 tr (VxxCCT)dt
=-(q + VxTf - 2VTGR-1 GτVx + 1 tr (%xΣΣτ))dt
+ VxT (f dt + G(u*dt + σu*dv) + Σdw) + g tr (VxxΣΣτ + σ2u*TGT%xGu*)dt
-(q + VTf - 2 VfGR-1 GtVX -
2 σ2VirGR-τGτ VixGR TGTVi)dt
+ Vxτ (f dt + G(u*dt + σu*dv) + Σdw
(T	IT 八 T八 八 IT
q + VTf - 2 VTGR-TRR-1Gτ匕-
2 σ2VTGR-TGT VxxGR-1Gτ Vx) dt
+ Vxτ (f dt + G(u*dt + σu*dv) + Σdw)
=-(q + VxTf - 2VTGRT(R + 2σ2GτVixG)RTGTVx)dt
+ Vxτ (f dt + G(u*dt + σu*dv) + Σdw)
=-(q - 1VTGRT(R + 2σ2GτVixG)RTGT匕)dt + VTG(U*dt + σu*dv) + VxτΣdw.
(17)
For Vx, We can again apply Ito,s differentiation rule:
dVx = Vxtdt + Vxxdx + 2tr (VxxxCCτ).	(18)
We can again substitute in the forward dynamics for dx and get:
dVx = ∂tVxdt	(19)
+ ∂χVx (f dt + G(u*dt + σu*dv) + Σdw)
+ 1tr (∂χxVXCCτ)dt
=(A + Vxxf )dt + VixG(u*dt + σu*dv)	(20)
+ VCxΣdw.	(21)
13
Under review as a conference paper at ICLR 2020
We have A = ∂tVX + 2 tr(∂χχVXCCτ). Note that the transpose on Vxx is dropped since it is symmetric.
D	Loss Function
The loss function used in this work builds on the loss functions used in Han et al. (2018) and Pereira et al. (2019).
Because the Deep 2FBSDE Controller propagates 2 BSDEs, in addition to using the propagated value function
(Vt), the propagated gradient of the value function (Vx,t) can also be used in the loss function to enforce that the
network meets both the terminal constraints i.e. φ(xN) and φx (xN) respectively. Moreover, since the terminal
cost function is known, its Hessian can be computed and used to enforce that the output of the FC1 layer at the
terminal time index (Vxx,N) be equal to the target Hessian of the terminal cost function φxx (xN). Although
this is enforced only at the terminal time index (Vxx,N), because the weights of a recurrent neural network are
shared across time, in order to be able to predict Vxx,N accurately all of the prior predictions Vxx,t will have to
be adjusted accordingly, thereby representing some form of propagation of the Hessian of the value function.
Additionally, applying the optimal control, which uses the network prediction, to the forward process introduces
an additional gradient path through the system dynamics at every time step. Although this makes training
difficult (gradient vanishing problem) it allows the weights to now influence what the next state (i.e. at the next
time index) will be. As a result, the weights can control the state at the end time index and hence the target
(V* (XN)) for the neural network prediction itself. This can be added to the loss function which can accelerate
the minimization of the terminal cost to achieve the task objectives. The final form of our loss function is
L = ci ||V* - VN||2 + C2 ||V* - VX,N||2 + C31|Vxx - vXx,n||2 + c4 ||V*||2 + λ∣∣θ∣∣2,
where V * = φ(xN), Vx* = φx (xN) and Vx*x = φxx (xN).
E Non-linear System Dynamics and System Parameters
For all simulation experiments, we used the same quadratic state cost of the form q(x) = 1 (X — Xtarget)T Q(X —
xtarget) for both the running and terminal state costs.
E.1 Cartpole
The cartpole dynamics is given by
d
x
θ
X
1_夕」
X
θ
mp sin θ(lθ2 +g Cos θ)
mc +mp sin2 θ
-mplθ2 cos θ sin θ-(mc+mp)g sin θ
dt +
l(mc +mp sin2 θ)
(udt + σudv) + 02 × 2
m∙c + m∙p sin2 θ	02×2
l(mc +mp sin2 θ)
02×2
I2×2
dw.
0
0
1
The model and dynamics parameters are set as mp = 0.01 kg, mc = 1 kg, l = 0.5 m, σ = 0.125. The initial
pole and cart position are 0 rad and 0 m with no velocity, and the target state is a pole angle of π rad and zeros
for all other states. The control costs R are 0.5 and 0.1 for experiments in fig.3 and fig. 6 respectively. The state
cost matrix is the same for both experiments at Q = diag [0.0, 6.0, 0.3, 0.3].
E.2 Quadcopter
The quadcopter dynamics used can be found in ElKholy (2014). Its dynamics is given by
14
UnderreVieW as a ConferenCe PaPersICLR 2020
d
治♦仔2 0治
Il
+ Q
o OWe.6 Zy ♦
ΦGI φ
6— 苫
.. INN L
el
e2
3
.4.
de)+o∙≡××
0
0
0
0
0
0
-+
—H(Sin Gsin 容 +
I3(cos e sin 容 sin θ I CoS 容 sin G)
ILI COS
06X6
I6X6
de
Where
COSeCOS 住 Sine)
GCOSe
m
0
0
0
el
e2
3
.4.
a
The SrareS are PoSiHons》ve°c≡esa angles and angular races》and rhe COnrr-S are rhe four roδrδrques
The model and dynamics ParamererS are Ser as m = 0∙47 kg》I= Iyy = 4∙86 × 10——3 kg m2》T
∞∙∞ × IO——3kgm2" Z =225 m- d =05∙ The inifial Stafe COndifiOnS USed are all ZerOS- and The targef Sta一
1 mer eachrhe noeasdown direcrns and all Orher SraSS ZerO The ConrrCOSrmarriX is R = 21
The Stare COSrmarriX is Q = d203 2o" 5030303O31∙2531.25303 O∙253253 O∙25L
Il
4
E∙3 2—LINK 6—MUSCLE HUMAN ARM
The forward ribody dynamics Ofrhelink human arm as StaredLi 行 TodOroV (2004) is given by-
AUM①; CH) — B
Where 6 加京2 represents rhe jor angle VeCrOr ConSiSrg Of(ShoulderjOr angle) and。2 (elbow jor
angleΛ×t(e) m 定 2×2fheertiamafriXWhiChiSPoSifiVedefinifeandSymmric(α 4) m 毋 2 is The Vecsr
CenrriPal and COriis forcem 定2 × 2rhejor fricrn marrix∙ τ m 毋2rhejorrque applied On rhe
SySSm∙ Therque is generasd by aivarn Of 6 muscle groupFollowing are equations for componenOf
each Ofrhe above mrices and VeCr 巴
W (l + 22cose2+2cose2 J
——L+2cos623 )
飞、—42(241 +42) J ∙
C=L02SwIe2
bλ法泊)
L12 )
I=Il+』2 + M
2 = m2s2
=I2
Where 宇一 = b22 = O.05》宇2 = b21 =025a mSrhe mass (1.4kg》lkg)3 zSrhe Iengrh Oflink i (3。Cma
33cm)" Si is The distance befweenjof Cenfer and mass Oflink i (IICm- 16Cm)" Ii represenfs for The momenf of
eia (025m2045m2)
The aivarn dynamics for each muscle is given by-
• (1 + QdU— O; ∙ 1「；-1
B= * 5 <2 m -L
T ——f deact
15
UlIderreVieW as a COnferenCe PaPer at ICLR 2020
Where -deact = 66mseQ。m^6is a VeCrOr Of muscle acrivarions》片 m^6is a VeCrOr of5'sranraneous neural
ps. AS a resulf OffheSe dynamics- six new StaSVariables (4) WilI be inCOrPOraSdfhe dynamical SySfem∙
Wirh rhe muscle aivarn dynamics PrOPoSed above-rhejn二 OrqUe VeCr can be COmPUred as follow 巴
T H M(θγΓ(aJ(θ 】 v(θ--
Where5'T(O-f) m 毋 is rhe〔ensile force generasd by each muscle- WhoSe expression is given by-
λe Z)
T (£3) U 4(ez)ssFV(Z3) + Fps)
Fvf-
I5∙72 Ie
I5∙72+(1∙38+2∙O2≡-
O∙62l(l3∙12+4∙21zl2∙67z2)e
O∙62-l-e
ife ≤ O
OrherWiSe
S(Z) U l002 exp(13∙8l 18∙7Z)
B E	B F	E E	E F	S E	S F	Muscle
6 3 O	4 g	1 8 ∞	1 O ι	5	4 g	Maximal Force, Fmax (N)
O ι g		O ⅛ 5		QLVQ	O 1 8 5	Optimal Length, Lopt (ɪn)
-Θ1Rbfs —(仇—Θ∖)Rbee	~Θ1RbFS —(仇—%) REFE	⅛ ⅛	⅛ ⅛	-Θ1Rse	-ΘiRsf	Velocity, v (m∕sec)
The equar°n for M3is Obra5'ed from l⅛kasaL (2017》EqUarion 3∙The ParamererS USed in rhe equations
above are mos〔Iy Obtaed by WOrk in TekaaL (2017LWhiCh WeProVidehererhe following table
Where rhe IaSr column ConSiSrS Of moment: arms WhiCh are approXimared accordings(Li 行 IbdOroS 2。。4》
figure B).Z m 定6乙The Iengfh Of each muscle-WhiCh is CalCUlafed by appromafg The range Of each muscle
group in Li 行 TodOrOV (2。。4) USg rhe da Ofjor angle ranges and linear relationships Wirh corresponding
jangles asTekaaL (2017Each Ofrhered linear funcrns for muscle IengrhS and cosine funcrns
for moment: arms are available in rhe PrOvided MATLAB COde∙
The above SySremiS a derermisric model Ofrhelinkmuscle human arm∙ HOWeVeL〔he maPaPer We
ConSider experimenfs Wifh bofh addifive and Confrmumicafive SSChaSfiCifieA Stafe—space SDE model for
16
Under review as a conference paper at ICLR 2020
the human arm is given as follows,
θ1
θ2
θl
θ
aι
d
a2
a3
a4
a5
a6
*
θι
*
θ2
♦♦
θl
♦♦
θ2
-a1
tdeact
-a2
tdeact
-a3
tdeact
一a4
tdeact
-a5
tdeact
-a6
tdeact
dt +
04×2
.tdeact I6×6
02×2
(Udt + σudv) + ∑2×2
06×2
dw.
where, θ= [θι, θ2]T = M(θ)-1(τ — C(θ, θ) — Bθ) and U = [uι, u2, u3, u4, u5, U6]τ
In this system, the control cost coefficient matrix is R = 2I6×6 .	The state cost matrix is Q
diag[10.0, 10.0, 0.1, 0.1, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001].
F Simulation Parameters
The precise training/network parameters are shown in the following table
system	batch size	iterations	c1	c2	c3	c4	λ	dt (sec)	T (sec)	h
CartPole	256	2000	1	1	1	1	0.0005	0.02	2	[8, 8]
QuadCopter	256	5000	1	1	1	1	0.0005	0.02	2	[8, 8]
HumanHand	256	2000	1	0	1	1	0.0005	0.02	2	[8, 8]
where iterations is the number of epochs. ci to c4 represent of the weight of the components in the loss function
in SM D. λ is the weights for L2 regularization of neural network. dt is the time discretization. T is the time
horizon for the task. h is the output size of each LSTM layer (cell state dimension is the same).
G Additional Plots
G.1	Cartpole Swing-up with Lower Control Cost
The plots in fig. 6 show the comparison of the 2FBSDE, FBSDE and iLQG controllers when the restriction
on the controls is lowered as compared to that in the main paper. Here, the control cost is set to R = 0.1 as
compared to R = 0.5 in the main paper. Clearly, the behavior is very different. Now, all three controllers try to
reach the targets as soon as possible by applying the required control effort thus ignoring the presence of control
multiplicative noise in the system.
G.2	2-link 6-muscle Human Arm single control trajectory sample
Fig. 7	includes a single control trajectory as well as the mean and variance for the human arm simulation to
demonstrate the feasibility of control trajectories from 2FBSDE.
G.3 QuadCopter Velocity S tates
Fig. 8	shows the velocity trajectories of the quadcopter experiment in the main paper.
17
Under review as a conference paper at ICLR 2020
——2FBSDE ——FBSDE ——iLQG ——target
Figure 6
: Comparison of 2FBSDE, FBSDE and iLQG on Cartpole with control cost of 0.1.
Figure 7: Single control trajectory as well as mean and variance of 2FBSDE on Human Arm. The
single trajectory demonstrates the feasibility of the resulting controls.
——2FBSDE ——FBSDE ——iLQG ——target
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00	0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00	, 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Time (s)	Time (s)	Time (s)
Figure 8: Comparison of 2FBSDE, FBSDE and iLQG on the quadcopter velocities and angular rates.
Again the variance of 2FBSDE trajectories are smaller than FBSDE and iLQG.
18
Under review as a conference paper at ICLR 2020
Figure 9: Phase plot at control multiplicative standard deviation σ = 0.1 and additive noise standard
deviation Σ = 0.1I. The yellow circle denotes the starting point, and yellow star denotes the target.
(P£ @6Uq -JBP-FIOqS
----mean ---------- target
(S∕ps)若or=Oɑ:
time (s)	time (s)	time (s)
Figure 10: Quadcopter state trajectories of 2FBSDE for 3 seconds. The plots demonstrate the
controller’s capability to reach and hover.
G.4	Quadcopter longer time horizon
Fig. 10 shows quadcopter state trajectories of 2FBSDE for longer time horizon (3 seconds).
G.5 Phase Plot for human arm simulation
Fig. 9 is a phase plot of the human arm experiment experiment at σ = 0.1 and Σ = 0.1I. It clearly shows that
2FBSDE can reach the target location and maintain small variance in the trajectory whereas iLQG diverges from
the target.
19
Under review as a conference paper at ICLR 2020
H	Network Initialization S trategies
For linear dynamics and cartpole used the Xavier initialization strategy Glorot & Bengio (2010). This is
considered to be standard method for recurrent networks that use tanh activations. This strategy was crucial
to allow using large learning rates and allow convergence in 〜2000 — 4000 iterations. Without this strategy,
convergence is extremely slow and prohibits the use for large learning rates.
On the other hand, for high dimensional systems like the quadcopter and human arm, this strategy failed to
work. The reason is partially explained in the loss function section of this supplementary text. Essentially,
the FC layers having O(n2x) trainable parameters and random initialization values cause a snowballing effect
on the propagation of Vx,t causing the loss function to diverge and make training impossible as the gradient
step in never reached. A simple solution to this problem was to use zero initialization for all weights and
biases. This prevents Vx,t from diverging as network predictions are zero and the state trajectory propagation
is purely noise driven. This allows computing the loss function without diverging and taking gradient steps
to start making meaningful predictions at every time step. Although this allows for training the network for
high-dimensional systems, it slows down the process and convergence requires many more iterations. Therefore,
further investigation into initialization strategies or other recurrent network architectures would allow improving
convergence speed.
I	Machine information
We used Tensorflow Abadi et al. (2015) extensively for defining the computational graph in fig. 1 and model
training. Because our current implementation involves many consecutive CPU-GPU transfers, we decided to
implement the CPU version of Tensorflow as the data transfer overhead was very time consuming. The models
were trained on multiple desktops computers / laptops to evaluate diffent models and hyperparameters. An
Alienware laptop was used with the following specs:
Processor: Intel Core i9-8950HK CPU @ 2.90GHz×12, Memory: 32GiB.
The desktop computers used have the following specs:
Processor: Intel Xeon(R) CPU E5-1607v3 @ 3.10GHz×4 Memory: 32GiB
20