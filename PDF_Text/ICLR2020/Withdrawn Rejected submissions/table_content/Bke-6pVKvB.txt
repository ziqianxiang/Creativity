Table 1: pGAN architecture for the Synthetic experiment (Notation: SGD stands for StochasticGradient Descent)Generator	Architecture: DNN (2 X 20 X 2) Hidden layer act. functions: Leaky ReLU (negative slope = 0.1) Output layer act. functions: Linear Optimizer: Adam (learning rate = 10-4)Discriminator	Architecture: DNN (2 X 250 X 1) Hidden layer act. functions: Leaky ReLU (negative slope = 0.1) Output layer act. functions: Sigmoid Optimizer: SGD (learning rate = 10-3, momentum = 0.9)Classifier	Architecture: Logistic Regression Loss function LC: Cross-entropy Optimizer: SGD (learning rate = 10-3, momentum = 0.9)Figure 9: Synthetic experiment: Distribution of genuine (green and blue dots) and poisoning (reddots) data points for different values of α. The poisoning points are labelled as green.
Table 2: Characteristics of the datasets used in the experimentsName	# Training Examples	# Test Examples	# FeaturesMNIST (3 Vs 5)	6,131/5, 421	1, 010/892	784MNIST (all)	10,000	10, 000	784FMNIST (sneaker vs ankle boot)	6, 000/6, 000	1, 000/1, 000	784CIFAR (automobile vs truck)	5, 000/5, 000	1, 000/1, 000	3,072Table 3: pGAN architecture for MNIST and FMNISTGenerator	Architecture: DNN (100 X 784 X 1,024 X 784) Hidden layer act. functions: Leaky ReLU (negative slope = 0.1) Output layer act. functions: Tanh Optimizer: Adam (learning rate = 10-4) Dropout: P = 0.5Discriminator	Architecture: DNN (784 X 1,024 X 512 X 1) Hidden layer act. functions: Leaky ReLU (negative slope = 0.1) Output layer act. functions: Sigmoid Optimizer: SGD (learning rate = 10-3, momentum = 0.9) Dropout: p = 0.5Classifier	Architecture: DNN (784 X 1,024 X 512 X 1) Loss function LC: Cross-entropy Hidden layer act. functions: Leaky ReLU (negative slope = 0.1) Output layer act. functions: Sigmoid Optimizer: SGD (learning rate = 10-3, momentum = 0.9) Dropout: p = 0.5For MNIST we trained pGAN for 2, 000 epochs using a batch-size of 200, setting i, j = 4 andk = 1 in Alg. 1. For FMNIST we used similar settings but training for 3, 000 epochs. For CIFARwe trained pGAN using a batch-size of 25 for 300 epochs, with i, j = 4 and k = 1. Finally,the architecture of the Deep Neural Networks (DNNs) and Convolutional Neural Networks (CNNs)trained to test the attacks is described in Tables 5 and 6.
Table 3: pGAN architecture for MNIST and FMNISTGenerator	Architecture: DNN (100 X 784 X 1,024 X 784) Hidden layer act. functions: Leaky ReLU (negative slope = 0.1) Output layer act. functions: Tanh Optimizer: Adam (learning rate = 10-4) Dropout: P = 0.5Discriminator	Architecture: DNN (784 X 1,024 X 512 X 1) Hidden layer act. functions: Leaky ReLU (negative slope = 0.1) Output layer act. functions: Sigmoid Optimizer: SGD (learning rate = 10-3, momentum = 0.9) Dropout: p = 0.5Classifier	Architecture: DNN (784 X 1,024 X 512 X 1) Loss function LC: Cross-entropy Hidden layer act. functions: Leaky ReLU (negative slope = 0.1) Output layer act. functions: Sigmoid Optimizer: SGD (learning rate = 10-3, momentum = 0.9) Dropout: p = 0.5For MNIST we trained pGAN for 2, 000 epochs using a batch-size of 200, setting i, j = 4 andk = 1 in Alg. 1. For FMNIST we used similar settings but training for 3, 000 epochs. For CIFARwe trained pGAN using a batch-size of 25 for 300 epochs, with i, j = 4 and k = 1. Finally,the architecture of the Deep Neural Networks (DNNs) and Convolutional Neural Networks (CNNs)trained to test the attacks is described in Tables 5 and 6.
Table 4: pGAN architecture for CIFARGenerator	Architecture: DCNN: •	Layer 1: 2D transposed convolutional; input channels: 100; output channels: 1024; kernel size: (2×2); stride: 1; padding: 0; no bias terms; batch normalization •	Layer 2: 2D transposed convolutional; input channels: 1024; output channels: 256; kernel size: (4×4); stride: 2; padding: 1; no bias terms; batch normalization •	Layer 3: 2D transposed convolutional; input channels: 256; output channels: 128; kernel size: (4×4); stride: 2; padding: 1; no bias terms; batch normalization •	Layer 4: 2D transposed convolutional; input channels: 128; output channels: 64; kernel size: (4×4); stride: 2; padding: 1; no bias terms; batch normalization •	Layer 5: 2D transposed convolutional; input channels: 64; output channels: 3; kernel size: (4×4); stride: 2; padding: 1 Hidden layer act. functions: ReLU Output layer act. functions: Tanh Optimizer: Adam (learning rate = 10-3)Discriminator	Architecture: DCNN: •	Layer 1: 2D convolutional; input channels: 3; output channels: 64; kernel size: (4×4); stride: 2; padding: 1 •	Layer 2: 2D convolutional; input channels: 64; output channels: 128; kernel size: (4×4); stride: 2; padding: 1; no bias terms; batch normalization •	Layer 3: 2D convolutional; input channels: 128; output channels: 256; kernel size: (4×4); stride: 2; padding: 1; no bias terms; batch normalization •	Layer 4: 2D convolutional; input channels: 256; output channels: 512; kernel size: (4×4); stride: 2; padding: 1; no bias terms; batch normalization •	Layer 5: 2D convolutional; input channels: 512; output channels: 1; kernel size: (2×2); stride: 1; padding: 0 Hidden layer act. functions: Leaky ReLU (negative slope = 0.2) Output layer act. functions: Sigmoid Optimizer: SGD (learning rate = 1.5 ∙ 10-4, momentum = 0.5)Classifier	Architecture: DCNN: •	Layer 1: 2D convolutional; input channels: 3; output channels: 32; kernel size: (4×4); stride: 2; padding: 1 •	Layer 2: 2D convolutional; input channels: 32; output channels: 128; kernel size: (4×4); stride: 2; padding: 1; no bias terms; batch normalization •	Layer 3: 2D convolutional; input channels: 128; output channels: 256; kernel size: (4×4); stride: 2; padding: 1; no bias terms; batch normalization •	Layer 4: 2D convolutional; input channels: 256; output channels: 512; kernel size: (4×4); stride: 2; padding: 1; no bias terms; batch normalization •	Layer 5: 2D convolutional; input channels: 512; output channels: 1; kernel size: (2×2); stride: 1; padding: 0 Hidden layer act. functions: Leaky ReLU (negative slope = 0.2) Output layer act. functions: Sigmoid Loss function LC: Cross-entropy Optimizer: SGD (learning rate = 10-4, momentum = 0.5)16Under review as a conference paper at ICLR 2020Table 5:	Architecture of the classifiers to test the attacks on MNIST, FMNIST and CIFAR.
Table 5:	Architecture of the classifiers to test the attacks on MNIST, FMNIST and CIFAR.
Table 6:	Architecture of the classifiers to test the attacks on multi-class MNIST (i.e. using all the 10class labels).
