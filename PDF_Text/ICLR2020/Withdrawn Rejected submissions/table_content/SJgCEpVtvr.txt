Table 1: Summary of datasets	Cora	Citeseer	Pubmed	Cora-full	 # of Nodes	2708	3327	19717	18703# of Edges	5429	4732	44338	81124# of Features	1433	3703	500	8710# of Classes	7	6	3	67Coverage(0.5%)	14.78%	6.64%	21.58%	27.19%Coverage(1%)	24.78%	12.14%	34.60%	47.42%5.2	Experiment SettingsWe evaluate models on semi-supervised node classification tasks with varying label rates. Insteadof evaluating on a fixed data split as in (Kipf & Welling, 2017; Velickovic et al., 2018), we mainlyconsider random splits as (Li et al., 2018) does. In detail, for a given label rate, we randomly generate100 different splits on each dataset. In each split, there is a labeled set with prespecified size fortraining, and in this set each class contains the same number of labeled nodes. As in (Li et al., 2018),we donâ€™t use a validation set, and all the remaining nodes will be used for testing. For simplicity, wewill refer to a task in the form of dataset-l, where l is the number of labeled nodes per class. Forexample, Cora-1 denotes the classification task on dataset Cora with one seed per class.
Table 2: Summary of results in terms of mean classification accuracy (in percent) over 100 randomsplits in different tasks. Unsupervised approaches first learn a lower-dimensional embedding for eachnode in an unsupervised manner, and then the embeddings are used to train a supervised classifier fornode classification. Here we use logistic regression as the classifier for unsupervised embeddings.
Table 3: Summary of results in terms of mean classification accuracy(in percent) over 100 randomsplits in different tasks. GNN variants are excluded due to limited computation resources.
Table 4: Summary of results in terms of mean classification accuracy (in percent) over 50 randomsplits in different tasks(the results of GAT experiments are from Table 2).
Table 5: Total training time for various models in seconds(s), implemented on PyG.
