Table 1: Interactive human evaluation of batch RL techniques. KL-control models strongly outper-form other techniques. Ratings are Likert scale, votes and human reward are z-scores.
Table 2: Purely reward-maximizing methods like Batch Q (left) diverge away from realistic language(saying phrases like “where did you say to me?”) in order to trivially exploit the reward functionby asking a question every turn, and using the maximum number of tokens in every sentence. Incontrast, KL-control methods (right) output plausible language by staying close to the prior, but shiftto using polite, cheerful language to maximize implicit human reward.
Table 3: Interactive human evaluation of different reward functions (models trained with WOP).
