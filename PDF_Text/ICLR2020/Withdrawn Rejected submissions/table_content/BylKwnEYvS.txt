Table 1: Dataset details. References contain suggested rank r. See Appendix A for details.
Table 2: Typical length scales of local convexity for Resnet networks with various depth and width(indicated by k). We sample 25 random "lines" in parameter space of length 1 centered on currentparameters, and report average length of convex subset of such "lines". Increased width makes theloss surface increasingly locally convex, while convexity decreases with depth and during training.
Table 3: Hyper-parameters used for training.
Table 4: ContinUation of Table 2.
Table 5: Table 2 for architectures without skip-connectionsepoch	20-layers			32			44-layers			k=1	k=2	k=4	k=1	k=2	k=4	k=1	k=2	k=40	0.15	0.62	0.15	0.2	0.84	1.0	0.43	0.33	1.0100	0.71	0.79	0.93	0.68	0.7	0.83	0.57	0.61	0.6200	0.74	0.67	0.8	0.63	0.63	0.86	0.57	0.6	0.67300	0.69	0.63	0.82	0.5	0.63	0.84	0.47	0.56	0.67alnsΛJno alnsΛJnoFigure 9: The curvature equation 6 during training, as in Figure 6, for various datasets. Note that itis always positive, and thus that the function satisfies star-convexity. Note the small shaded regions,which represents the 95 % confidence computed over 5 repetitions.
Table 6: Links for used datasets.
