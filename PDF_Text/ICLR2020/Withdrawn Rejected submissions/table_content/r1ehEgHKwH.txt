Table 1: The number of sentences used in the experiment after pruning sentences longer than 40.
Table 2: The UAS results of running different encoder-decoder combinations with the two largesttreebanks, the best results are shown in bold. Here we denote Pt = Penn Treebank, CS = UD-Czech-CAC, LSTM-n = n-layer LSTM encoder, Non-N = non-neural encoder, FO=first order decoder, SOFigure 2: UAS of different encoder-decoder combinations relative dependency length5	Results and Analysis5.1	large training dataIt is widely believed that the more complex a model is, the more data it requires in parameter learn-ing. Considering the complexity brought by combining neural encoders and high-order decoders,we start our evaluations from large training datasets (more than 20000 sentences) from which we ex-pect all combinations will be effectively learned. Two treebanks meet the above require requirement:PTB and UD-Czech-CAC. In Table 2 we show the UAS results of running different encoder-decodercombinations on the two treebanks.
Table 3: The UAS results of running different encoder-decoder combinations with medium sizedtraining data. The best results are shown in bold. Here ru = UD-Russian, zh = UD-Chinese, he =UD-Heberew, sv = UD-Swedish, 1/10 denotes that only 1/10 of the training data in this treebank isused for training, the other denotations are the same as in Table 2.
Table 4: The UAS results of running different encoder-decoder combinations with small trainingdata. The best reults are shown in bold.
