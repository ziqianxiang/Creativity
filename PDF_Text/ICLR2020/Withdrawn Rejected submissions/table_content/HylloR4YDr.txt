Table 1: Quantitative results of our methods and all baselines on the three environments: Humanoid,Minitaur, and Ant respectively. In each environment, LR emerges as clearly the best performer,enabling the agent to navigate closest to the goal.
Table 2: Training details for Humanoid, Minitaur, Ant. Note that training algorithm refers to theRL algorithm used to collect data (for our methods), or train the policy (for baselines). Note thatthe hyperparameters for all experiments (VGCP, HER, PPO, SAC), have been taken from existingimplementations of these algorithms (Dhariwal et al. (2017); Hill et al. (2018)).
