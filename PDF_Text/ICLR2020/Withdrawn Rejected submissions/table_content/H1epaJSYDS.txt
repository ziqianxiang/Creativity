Table 1: Text classification results on AG-News. Domain knowledge is derived from WordNetand co-occurrence statistics. Our approach with different initializations and domain knowledgeachieves within 0.5% accuracy with 40× fewer parameters, outperforming the published baselines.
Table 2: Language modeling using LSTM (top) and AWD-LSTM (bottom) on PTB. We outperformvocab selection and compression baselines. 200/256 is embedding dimension. Incorporating domainknowledge further reduces params. Ppl: perplexity, # Emb: # (non-zero) embedding parameters.
Table 3: Language modeling on WikiText-103. We reach within 3 perplexity with Z 16× reductionand within 10 perplexity with 100× reduction, outperforming frequency and hashing baselines.
Table 4: Word association results after training language models	Largest word pairswith ANT on the word-level PTB dataset. Left: the non-anchor	trading, brokeragewords most induced by a given anchor word. Right: the largest	stock, junk(non-anchor, anchor) entries learnt in T after sparse regularization.	year, summeryork, angelesAnchor words	Non-anchor words	year, monthyear	august, night, week,, month, monday, summer, spring	government, administrationstock	bonds, certificates, debt, notes, securities, mortgages	two, nine4.3	DiscussionHere we list some general observations regarding the importance of various design decision in ANT:1) Sparsity is important: Baseline methods that only perform low-rank compression with densefactors (e.g. LR) tend to suffer in performance while using many parameters, while ANT with sparsityregularization retains performance with much better compression.
Table 5: Table of hyperparameters for text classification experiments on AG-News, DBPedia, Sogou-News, and Yelp-review datasets. All text classification experiments use the same base CNN modelwith the exception of different output dimensions (classes in the dataset): 4 for AG-News, 14 forDBPedia, 5 for Sogou-News, and 5 for Yelp-review.
Table 6: Table of hyperparameters for language modeling experiments using LSTM on PTB dataset.
Table 7: Table of hyperparameters for language modeling experiments using AWD-LSTM on PTBdataset.
Table 8: Table of hyperparameters for language modeling experiments using AWD-LSTM onWikiText-103 dataset.
Table 9: More text classification results on DBPedia (top), Sogou-News (middle), and Yelp-review(bottom). Domain knowledge is derived from WordNet and co-occurrence statistics. Our approachwith different initializations and domain knowledge achieves within 1% accuracy with 21× fewerparameters on DBPedia, within 1% accuracy with 10× fewer parameters on Sogou-News, andwithin 2% accuracy with 22× fewer parameters on Yelp-review. Acc: accuracy, # Emb: # (non-zero)embedding parameters.
Table 10: More language modeling results using AWD-LSTM on Penn Treebank. Using domainknowledge to infer sparse structures helps to reduce the embedding parameters by 100×. Ppl: per-plexity, # Emb: # (non-zero) embedding parameters.
