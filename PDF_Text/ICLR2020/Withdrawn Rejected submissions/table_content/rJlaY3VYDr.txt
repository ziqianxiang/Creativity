Table 1: Performance of four OOD detection methods. All methods in the table have no access toOOD data during training and validation. ODIN* and Mahalanobis* are modified versions that donâ€™tneed any OOD data for tuning (see Section 2.1). The base network used in the table is DenseNet.
Table 2: OOD detection with OOD data versus without OOD data. The values of ODINorig andMahaorig (abbreviation of Mahalanobis) are copied from the Mahalanobis paper (Lee et al., 2018b)which are tuned with OOD data. The values of ODIN*, Maha*, and DeConf-C* are copied fromTable 1 of our paper which do not have any access to OOD data. All methods in this table use thesame DenseNet for backbone. Note that the Mahalanobis paper (Lee et al., 2018b) only has threeOOD datasets for the table, so we only compare with those cases.
Table 3: Performance of four OOD detection methods using DomainNet. The in-distribution is thereal-A subset. Each value is averaged over three runs. The type of distribution shift presents a trendof difficulty to the OOD detection problem: Semantic shift > Non-semantic shift > Semantic +Non-semantic shift.
Table 4: OOD detection performance with DomainNet. In this table we use whole real set (345classes) for the in-distribution data. The remaining five domains are OOD data. The backbonenetworks is Resnet-34 trained from scratch. Each value is averaged with 3 runs. The values inparentheses are standard deviation. The type of distribution shift in this experiment is categorizedas non-semantic shift.
Table 5: Performance of six OOD detection methods on 8 benchmark datasets. This is a full versionof Table 1 which uses DenseNet for the backbone networks. The value in parentheses is the standarddeviation.
Table 6: Performance of six OOD detection methods on 8 benchmark datasets. The backbonenetworks for this table is Resnet-34. The value in parentheses is the standard deviation.
