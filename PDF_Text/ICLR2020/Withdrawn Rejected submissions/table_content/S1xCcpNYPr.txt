Table 1: Subject information4.2	Experimental DesignFor RQ1, we first collect neuron coverage and output distribution of the testing data from the trainedmodels, and then apply our approach on the whole testing data to obtain a subset. In order to investi-gate the effectiveness of our approach (DeepReduce), we compare it with both random approach andBOT (Li et al., 2019). BOT is the state-of-the-art approach in DL testing. Different from DeepRe-duce, BOT requires to specify the number of data instances to be selected. Besides, it does notconsider satisfying structural coverage in data selection. To enable comparison, we implement avariant of BOT, BOTv , by changing the termination criteria to the one used in DeepReduce. Thatis, BOTv iteratively selects testing data until the relative entropy value is smaller than α. We alsoimplement a random reduction approach, which randomly selects testing data from the whole testingdata until the relative entropy value is smaller than α. Both BOTv and random approach are repeated50 times to reduce the influence of their inherent randomness, and the average values are used forcomparison.
Table 2: The effectiveness of DeepReduce in input reductionFrom the table, we find that DeepReduce can reduce the whole testing data significantly, rangingfrom 261 (2.6%) to 827 (8.3%), with an average of 455 (4.5%). Also, we can find that DeepReducecan achieve an average of 98.1% of the original accuracy (the test accuracy achieved by the wholetesting data). In the worst case, the accuracy achieved by the selected data is still over 94.3%(0.8316/0.8815) of the original one. The results show that our approach can save more than 95.5%(1 - 4.5%) testing cost on average. Besides, the selected data of DeepReduce can achieve a similartest accuracy as the whole testing data achieves, which indicates that DeepReduce is effective inestimating the performance of DL models.
Table 3: Experiment results using different parameters on CIFAR-10The results show that only a small number of data instances is required in order to achieve thesame coverage as the whole testing data achieves, under different coverage criteria and terminationcriteria. The results also show that, at the first phase of our approach (HGS), more data is requiredwhen a higher β is used. This is reasonable as the neuron coverage is harder to achieve under ahigher activation threshold. The approximation of output distribution at the second phase selectsmore data instance than HGS. That is, testing adequacy can be achieved by selecting a smalleramount of data, while more data are needed in order to achieve the similar output distribution. Interms of test accuracy, in most cases, the results achieved when setting β to different values aresimilar when the same α is used in the termination criteria. When the termination criterion getsstricter, the accuracy becomes higher. In our experiments, we set α = 0.005 and β = 0.5 as defaultsettings.
Table 4: Effectiveness of our approach in regression scenarios5 ConclusionIn this paper, we propose DeepReduce, a software engineering approach for cost-effective testingof DL models. Our approach can accurately estimate the performance of a DL model using a smallsubset of testing data, by considering three different objectives: efficiency, completeness, and ef-fectiveness. We have evaluated DeepReduce on six DL models and two datasets. On average,DeepReduce can reduce the whole testing data to 4.6%. The results confirm that the proposed ap-proach can significantly reduce DL testing cost by reducing the amount of testing data required fortesting a DL model.
