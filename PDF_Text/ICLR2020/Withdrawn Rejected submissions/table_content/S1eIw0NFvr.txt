Table 1: Summary of class-level results for ImageNet. Only classes passing the significance test areincluded. The model accuracy difference column reports mean βtM - β0M as the percentage pointdifference between the pruned and baseline model accuracies; a negative value means the prunedmodel’s average accuracy is lower than the baseline model’s.
Table 2: ImageNet top-1 and top-5 accuracy at all levels of sparsity, averaged over all runs. Thefourth column is the number of classes significantly impacted by pruning.
Table 3: CIFAR-10 top-1 accuracy at all levels of sparsity, averaged over runs. Top-5 accuracyfor CIFAR-10 was 99.8% for all levels of sparsity. The fourth column is the number of classessignificantly impacted by pruning.
Table 4: Summary of class-level results for CIFAR-10. Only classes passing the significance test areincluded. The model accuracy difference column reports mean βtM - β0M as the percentage pointdifference between the pruned and baseline model accuracies; a negative value means the prunedmodel’s average accuracy is lower than the baseline model’s. The normalized difference is calculatedusing Equation 1.
Table 5: Sparse models are more sensitive to image corruptions that are meaningless to a human. Wemeasure the average Top-1 and Top-5 test set accuracy of models trained to varying levels of sparsityon the ImageNet-C test-set (the models were trained on uncorrupted ImageNet). For each corruptionwe consider and the relative measures see appendix (Table. 5)) we compute the average accuracy of50 trained models across all 5 levels of corruption severity.
