Table 1: FID scores obtained from different models. For our reported results, we executed 10independent trials and report the mean and standard deviation of the FID scores. Each trail iscomputing the FID between 10k generated images and 10k real images.
Table 2: Network structure for auto-encoder based on InfoGANEncoder	DecoderInput x 4 × 4 Conv64, ReLU 4 × 4 Conv128, BN, ReLU Flatten, FC 128 × M × M → 1024, BN, ReLU FC 1024 → nz	Input z FC nz → 1024, BN, ReLU FC 1024 → 128 × M × M,BN,ReLU 4 × 4 Deconv64, BN, ReLU 4 × 4 Deconv128, SigmoidA	Network ArchitecturesIn this section we provide Table 2 that summarizes the auto-encoder network structure. The networkstructure is adopted from InfoGAN(Chen et al., 2016a), and the difference between the networkswe used for each dataset is the size of the fully connected layers, which depends on the size of theimage. All convolution and deconvolution layers have stride = 2 and padding = 1 to ensure thespatial dimension decreases/increases by a factor of 2. M is simply the size of an input image dividedby 4. Specifically, for MNIST and Fashion MNIST, M = 7; for CIFAR-10, M = 8; for CelebA,M = 16. BN stands for batch normalization.
Table 3: Evaluation of sample quality by precision/recall.
Table 4: FID score comparisons of GANs and various AE based models	MNIST	Fashion	CIFAR-10	CelebAMM GAN	9.8 ± 0.9	29.6 ± 1.6	72.7 ± 3.6	65.6 ± 4.2NS GAN	6.8 ± 0.5	26.5 ± 1.6	58.5 ± 1.9	55.0 ± 3.3LSGAN	7.8 ± 0.6	30.7 ± 2.2	87.1 ± 47.5	53.9 ± 2.8WGAN	6.7 ± 0.4	21.5 ± 1.6	55.2 ± 2.3	41.3 ± 2.0WGAN GP	20.3 ± 5.0	24.5 ± 2.1	55.8 ± 0.9	30.3 ± 1.0DRAGAN	7.6 ± 0.4	27.7 ± 1.2	69.8 ± 2.0	42.3 ± 3.0BEGAN	13.1 ± 1.0	22.9 ± 0.9	71.4 ± 1.6	38.9 ± 0.9VAE	28.2 ± 0.3	57.5 ± 0.4	142.5 ± 0.6	71.0 ± 0.5WAE-GAN	12.4 ± 0.2	31.5 ± 0.4	93.1 ± 0.5	66.5 ± 0.7Two-Stage VAE	10.9 ± 0.7	26.1 ± 0.9	96.1 ± 0.9	65.2 ± 0.8RAE + GMM	10.8 ± 0.1	25.1 ± 0.2	91.6 ± 0.6	57.8 ± 0.4GLANN (with perceptual loss)	8.6 ± 0.1	13.0 ± 0.1	46.5 ± 0.2	46.3 ± 0.1VAE+flow prior	28.3 ± 0.2	51.8 ± 0.3	110.4 ± 0.5	54.3 ± 0.3VAE+flow posterior	26.7 ± 0.3	55.1 ± 0.3	143.6 ± 0.8	67.9 ± 0.3GLF (ours)	8.2 ± 0.1	21.3 ± 0.2	88.3 ± 0.4	53.2 ± 0.2GLF+perceptual loss (ours)	5.8 ± 0.1	10.3 ± 0.1	44.6 ± 0.3	41.8 ± 0.215Under review as a conference paper at ICLR 2020
Table 5: Number of training epochs for Two-stage VAE, GLANN, and GLF	MNIST/Fashion	CIFAR-10	CelebATwo-stage VAE First/Second	400/800	1000/2000	120/300GLANN First/Second	500/50	500/50	500/50GLF	100	200	40Table 6: Per-epoch training time in seconds	MNIST/Fashion	CIFAR-10	CelebA2-stage VAE 1st/2nd	5/2	6/2	60/28GLF	10	13	108GLANN with perceptual loss	14	16	292GLF with perceptual loss	16	19	343G Nearest Neighbors of Generated Samples in the Training SetQuantitative measurements of sample quality, such as FID score and Precision/Recall can be min-imized by letting the generative model memorize the training set. The smooth transition of noiseinterpolation shown in Figure 2 provides evidence that our model can generalize, i.e., it generatesamples that have not be seen during training. Here we provide additional evidence showing that ourgenerative model generalizes well.
Table 6: Per-epoch training time in seconds	MNIST/Fashion	CIFAR-10	CelebA2-stage VAE 1st/2nd	5/2	6/2	60/28GLF	10	13	108GLANN with perceptual loss	14	16	292GLF with perceptual loss	16	19	343G Nearest Neighbors of Generated Samples in the Training SetQuantitative measurements of sample quality, such as FID score and Precision/Recall can be min-imized by letting the generative model memorize the training set. The smooth transition of noiseinterpolation shown in Figure 2 provides evidence that our model can generalize, i.e., it generatesamples that have not be seen during training. Here we provide additional evidence showing that ourgenerative model generalizes well.
