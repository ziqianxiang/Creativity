Table 1: Description of the notations used in this workConsider the following topics (Z1-Z4), where (Z1-Z3) are respectively obtained from different(high-resource) source (S1 -S3) domains whereas Z4 from the (low-resource) target domain T inthe data-sparsity setting:Z1 (S1): profit, growth, stocks, apple, fall, consumer, buy, billion, shares → TradingZ2(S2): smartphone, ipad, apple, app, iphone, devices, phone, tablet → Product LineZ3 (S3 ): microsoft, mac, linux, ibm, ios, apple, xp, windows → Operating System/CompanyZ4 (T): apple, talk, computers, shares, disease, driver, electronics, profit, ios → ?Usually, the top words associated with topics learned on a large corpus are semantically coherent andrepresent meaningful semantics, e.g., Trading, Product Line, etc. However in sparse-data setting,topics (e.g., Z4) are incoherent (noisy) and therefore, it is difficult to infer meaningful semantics.
Table 2:	Data statistics: Short/long texts and/or small/large corpora in targetand source domains. Symbols- K: vocabulary size, L: average text length(#words), C: number of classes and k: thousand. For short-text, L<15. S3is also used in target domain. ‘-’: unlabeled data.
Table 3:	Domainoverlap in source-target corpora. I :Identical, R: Re-lated and D: Distantdomains.
Table 4	: Baselines (related works) vs this work. Here, NTM and AuR refer to neural network-basedTM and autoregressive assumPtion, resPectively. DocNADEe → DocNADE+Glove embeddings.
Table 5: State-of-the-art comparisons with TMs: Perplexity (PPL), topic coherence (COH) andprecision (IR) at retrieval fraction 0.02. Scores are reported on each of the target, given KBs fromone or several sources. Please read column-wise. Bold: best in column. Gain%: Bold vs DocNADE.
Table 6: State-of-the-art comparisons with TMs using word embeddings: PPL, COH and IR at re-trieval fraction 0.02. Scores are reported on each of the target, given KBs. Here, MVT: LVT+GVT(Table 5), DocNADEe: DocNADE+Glove and Gain%: Bold vs DocNADEe. For all the configura-tions, we apply a projection on word embeddings concatenated from several sources.
Table 7: PPL, COH, IR at retrieval fraction 0.02.
Table 8: Source S and target T topics before (-)and after (+) topic transfer(s) (GVT) from one ormore sources. DNE: DocNADEchip				source corpora			target corpus	20NS	R21578	AGnews	20NSshort				-GVT	+GVTkey	chips	chips	virus	chipsencrypted	semiconductor	chipmaker	intel	technologyencryption	miti	processors	gosh	intelclipper	makers	semiconductor	crash	encryptionkeys	semiconductors	intel	chips	clipperTable 9: Five nearest neighbors of the word chipin source and target semantic spaces before (-)and after (+) knowledge transfer (MST+GVT)In Table 7, we show PPL scores on two medical target corpora: Ohsumtitle and Ohsumed usingtwo sources: AGnews (news corpus) and PubMed (medical abstracts) to perform cross-domain andin-domain knowledge transfers. We see that using PubMed for LVT on both the target corporaimproves generalization. Overall, we report a gain of 17.3% (1268 vs 1534) on Ohsumtitle and8.55% (1497 vs 1637) on Ohsumtitle, compared to DocNADEe. Additionally, MST+GVT and
Table 9: Five nearest neighbors of the word chipin source and target semantic spaces before (-)and after (+) knowledge transfer (MST+GVT)In Table 7, we show PPL scores on two medical target corpora: Ohsumtitle and Ohsumed usingtwo sources: AGnews (news corpus) and PubMed (medical abstracts) to perform cross-domain andin-domain knowledge transfers. We see that using PubMed for LVT on both the target corporaimproves generalization. Overall, we report a gain of 17.3% (1268 vs 1534) on Ohsumtitle and8.55% (1497 vs 1637) on Ohsumtitle, compared to DocNADEe. Additionally, MST+GVT andMST+MVT boost generalization performance compared to DocNADE(e).
Table 6: 8.84% (.320 vs .294) on 20NSshort and 9.21% (.578 vs.540) on TMNtitle, and (c) Table7: 4.91% (.192 vs .183) on Ohsumed and 4.0% (.182 vs .175) on Ohsumedtitle.
Table 10: Label space of the corpora usedHyperparameter	Search Spaceretrieval fraction	[0.02]learning rate	[0.001]hidden units, H	[200]activation function (g)	sigmoiditerations	[100]λk	[1.0, 0.5, 0.1]γk	[0.1, 0.01, 0.001]Table 11: Hyperparameters in Generalization in DocNADE, DocNADEe, LVT, GVT and MVTconfigurations for 200 topicstarget, K0 << K due to additional word in the source. In order to perform GVT, we need the sametopic feature dimensions in the target and source, i.e., K0 of the target. Therefore, we remove thosek H ×Kcolumn vectors from W ∈ R	of the kth source for which there is no corresponding word inthe vocabulary of the target domain. As a result, we obtain Zk as a latent topic feature matrix tobe used in knowledge transfer to the target domain. Following the similar steps, we prepare a KBof Zs such that each latent topic feature matrix from a source domain gets the same topic featuredimension as the target.
Table 11: Hyperparameters in Generalization in DocNADE, DocNADEe, LVT, GVT and MVTconfigurations for 200 topicstarget, K0 << K due to additional word in the source. In order to perform GVT, we need the sametopic feature dimensions in the target and source, i.e., K0 of the target. Therefore, we remove thosek H ×Kcolumn vectors from W ∈ R	of the kth source for which there is no corresponding word inthe vocabulary of the target domain. As a result, we obtain Zk as a latent topic feature matrix tobe used in knowledge transfer to the target domain. Following the similar steps, we prepare a KBof Zs such that each latent topic feature matrix from a source domain gets the same topic featuredimension as the target.
Table 12: Hyperparameters search in the Information (text) Retrieval task, where λk and γk areweights for kth source. We use the same grid-search for all the source domains. Hyperparametersin IR task in DocNADE, DocNADEe, LVT, GVT and MVT configurations for 200 topicssetting	KBs from Source Corpus	Model/	Scores on Target Corpus (in sparse-data setting)													Transfer Type	20NSshort			TMNtitle			R21578title			20NSsmall					PPL	COH	IR	PPL	COH	IR	PPL	COH	IR	PPL	COH	IR		LVT	667	.661	.308	670	.730	.535	183	.716	.661	610	.440	.286parameterized	MST	GVT	651	.658	.285	701	.712	.523	190	.701	.656	602	.460	.273		MVT	667	.660	.309	667	.730	.535	183	.714	.661	608	.441	.293		+ Glove	662	.677	.296	672	.731	.540	183	.716	.662	634	.412	.207		LVT	640	.678	.308	663	.732	.547	182	.739	.673	594	.542	.277hyper-parameterized	MST	GVT	658	.705	.305	704	.746	.550	192	.727	.673	599	.585	.326		MVT	656	.740	.314	680	.752	.569	188	.745	.685	600	.637	.285Table 13: {λ, γ} as Parameter vs Hyperparameters: Perplexity (PPL), topic coherence (COH) andprecision (IR) at retrieval fraction 0.02, when λ and γ are (1) learned with backpropagation, and (2)treated as hyperparameters. The experimental results suggest that the second configuration performsbetter the former. Therefore, in the paper we have reported scores considering {λ, γ} as hyperpa-rameters. + Glove: MVT+Glove embeddings. Please read column-wise. Bold: best in column.
Table 13: {λ, γ} as Parameter vs Hyperparameters: Perplexity (PPL), topic coherence (COH) andprecision (IR) at retrieval fraction 0.02, when λ and γ are (1) learned with backpropagation, and (2)treated as hyperparameters. The experimental results suggest that the second configuration performsbetter the former. Therefore, in the paper we have reported scores considering {λ, γ} as hyperpa-rameters. + Glove: MVT+Glove embeddings. Please read column-wise. Bold: best in column.
