Table 1: Lipschitz bounds on building blocks of invertible neural networks. The second columnshows the operations of the forward mapping and the last two columns show bounds on the Lipschitzconstant of the forward and inverse mapping. M in the row for the forward mapping of an affineblock is defined as M = max(∣a∣, |b|) ∙ Cgo ∙ LiP(S) + Lip(t). Furthermore, M* for the inverse of anaffine block is M* = max(∣a* |, |b*|) ∙ c( 1)o ∙ Lip(s) + c(ɪ)o ∙ Lip(s) ∙ Ct + Cι ∙ Lip(t). Note thatthe bounds of the affine blocks hold only locally. Derivations of the bounds are given in Appendix A.
Table 2: Comparison of bits-per-dimension (BPD) and sample quality via FID scores for MLE- andADV-trained models. For reference, the FID of a untrained Flow is roughly 1500. Using the stableversion with ADV, though improves BPD significantly, might come at a trade-off on FID.
Table 3: Stability analysis on two architectures trained adversarially (ADV), and with maximumlikelihood (MLE). *means this number is meaningless as the network is visibly non-invertible. LDJdenotes the log-determinant of the Jacobian, BPD denotes bits-per-dimension and SV singular value.
Table 4: Hyperparameters for ADV models17Under review as a conference paper at ICLR 2020E Extended Results for Crafted, Non-Invertible InputsPGD Setup. To find non-invertible inputs for Glow and Residual Flows, we used PGD (Eq. 3) with= 0.1 and step size 0.01. For the Glow model in Section 4.2, we consistently found inputs withsevere reconstruction errors (as shown in Figure 3) in fewer than 10 PGD iterations. For the ResidualFlow model analyzed in this section, we ran 200 iterations of PGD. In each iteration, the pixel valuesof the perturbed image were clipped to the valid input range that the respective model was trained on([-0.5, 0.5] for Glow and [0, 1] for the Residual Flow).
