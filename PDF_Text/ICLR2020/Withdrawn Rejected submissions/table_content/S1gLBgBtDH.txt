Table 1: Episode score at the end of training attained by SLM Lab implementations on discrete-action control problems. Reported episode scores are the average over the last 100 checkpoints,and then averaged over 4 Sessions. A Random baseline with score averaged over 100 episodes isincluded. Results marked with ‘*’ were trained using the hybrid synchronous/asynchronous versionof SAC to parallelize and speed up training time. For SAC, Breakout, Pong and Seaquest weretrained for 2M frames instead of 10M frames.
Table 2: Episode score at the end of training attained by SLM Lab implementations on continuouscontrol problems. Reported episode scores are the average over the last 100 checkpoints, and thenaveraged over 4 Sessions. A Random baseline with score averaged over 100 episodes is included.
Table 3: Comparison ofRL software libraries. Algorithm acronyms are explained in SupplementarySection A.1. REINFORCE is excluded as are less well-known algorithms. “Benchmark” indicateswhether the library reports the performance of their implementations. “Config” indicates whetherhyperparameters are specified separately from the implementation and run scripts; “split” indicatesthat the configuration is divided across multiple files, “partial” indicates that some but not all hyper-parameters are included. “Parallel” denotes whether training for any algorithms can be parallelized.
