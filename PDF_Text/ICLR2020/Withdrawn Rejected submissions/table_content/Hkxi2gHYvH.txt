Table 1: Success Rate in GridWorld Environments. A successful run indicates that the agent reachesthe goal from a random stating potion in the maze within 100 steps.
Table 2: Environment dimensions and horizonsEnvironment State/Goal Dimensions Action Dimensions Maximum StepsGridWorld	(17, 17, 3)	(4,)	100HalfCheetah	(18,)	(6)	1000Pendulum	(3,)	(1,)	200Reacher	(11,)	(2,)	50AntMaze	(113,)	(2,)	600A.2 Network Parameters and Hyperparameters for LearningFor all experiments in this paper we use a standard PPO baseline (Hill et al., 2018) to train. Weuse two fully-connected layers with output size 64 for the actor critic. We provide hyperparametersbelow, and refer to Hill et al. (2018) for all other implementation details.
Table 3:	Hyper parameters for PPOParameter	Valuegamma	0.99entropy coefficient	0.01leaning rate	2.5 Ã—	10-4clip range	[-0.2, 0.2]max gradient norm	0.5batch size	12812Under review as a conference paper at ICLR 2020A.3 TRAINING CPCWe follow the approach proposed in Oord et al. (2018) to obtain predictive features from states. Thedetails are provided in the two tables below. All hyperparameters for training CPC are tuned throughperforming grid search.
Table 4:	Network details for training CPCEnvironment	Encoder Type	Autoregressive Model TypeGridWorld	2 FC layers with output size 64	GRU with output size 256All others	2 Cov layers with (3, 3) kernel, and stride 2	GRU with output size 256Table 5: CPC training detailsEnvironment	No. of Trajectories	Length of Trajectory	Context Length	Predict Length	EpochGridWorld	200	100	10	10	2HalfCheetah	500	1000	50	50	5Pendulum	200	300	10	10	10Reacher	200	500	20	20	10AntMaze	500	1000	30	30	5A.4 Hand-Shaped Reward SchemesFor continuous environments, we show that using predictive features provides reward signals asinformative as hand-shaped rewards, which encodes domain and task knowledge about the environ-ment. In particular, each hand-shaped reward scheme contains information about where the goal isand how to get there. We provide details below for each environment.
Table 5: CPC training detailsEnvironment	No. of Trajectories	Length of Trajectory	Context Length	Predict Length	EpochGridWorld	200	100	10	10	2HalfCheetah	500	1000	50	50	5Pendulum	200	300	10	10	10Reacher	200	500	20	20	10AntMaze	500	1000	30	30	5A.4 Hand-Shaped Reward SchemesFor continuous environments, we show that using predictive features provides reward signals asinformative as hand-shaped rewards, which encodes domain and task knowledge about the environ-ment. In particular, each hand-shaped reward scheme contains information about where the goal isand how to get there. We provide details below for each environment.
