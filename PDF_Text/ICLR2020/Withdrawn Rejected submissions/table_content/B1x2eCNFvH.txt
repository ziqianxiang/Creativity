Table 1: Top-1 accuracy (%) of ResNet-18 on ImageNet with varying p and q = 100	MethOd		P = 1	p = 3	p = 5	p = 10Supervised	17.35	28.61	36.01	47.89DCT (Qiao et al.,2018)	—	—	—	53.50MT (Tarvainen & Valpola, 2017)	16.91	40.81	48.34	56.70LLP (ours)	27.14	53.24	57.04	61.51	LLP + RJ(OUrs)		33.55	55.30	58.92	63.20Table 2: Top-5 accuracy (%) of ResNet-50 on ImageNet with p = {1, 10} and q = 100p	Supervised	Pseudolabels	VAT-EM	S4L1	MT	LLP (ours)	LLP + RJ (ours)1	48.43	51.56	46.96	53.37	40.54	61.89	72.2010	80.43	82.41	83.39	83.82	85.42	88.53	89.554	ResultsWe first evaluate the LLP method on visual object categorization in the large-scale ImageNetdataset (Deng et al., 2009), under a variety of training regimes. We also illustrate transfer learning toPlaces 205 (Zhou et al., 2014), a large-scale scene-recognition dataset.
Table 2: Top-5 accuracy (%) of ResNet-50 on ImageNet with p = {1, 10} and q = 100p	Supervised	Pseudolabels	VAT-EM	S4L1	MT	LLP (ours)	LLP + RJ (ours)1	48.43	51.56	46.96	53.37	40.54	61.89	72.2010	80.43	82.41	83.39	83.82	85.42	88.53	89.554	ResultsWe first evaluate the LLP method on visual object categorization in the large-scale ImageNetdataset (Deng et al., 2009), under a variety of training regimes. We also illustrate transfer learning toPlaces 205 (Zhou et al., 2014), a large-scale scene-recognition dataset.
Table 3: Top-1 accuracy (%) of ResNet-18 on ImageNet with p = 10 and varying p. “FT” means thefine-tuning process used in YFCC100M experiments, which is the same as the “rate-jump” phase.
Table 4: ResNet-50 transfer learning Top-1 accuracy (%) on Places205 using weights pretrained onImageNet with p = {1, 10}. Most numbers are from Zhai et al. (2019). *: Produced by us.
Table 5: Label propagation performance on ImageNet subsets. LS: Label Spreading (Zhou et al.,2004). LP: Label Propagation (Zhu & Ghahramani, 2002). LP_DMT: the method in DMT (Liu et al.,2018). LP_DLP: the method in DLP (Iscen et al., 2019). Standard deviations are across 10 subsets.
Table 6: Top1 accuracy (%) for ResNet-18 and ResNet-50 trained with p = 10, q = 100 and othersettings. “TopX” means training with K =X. “NoC” does not weight the loss by confidence. “NoDW”means the KNN weight is not weighted by densities.
