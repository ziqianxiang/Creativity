Table 1: The most important results obtained for the task of speech separation conducted on mixturesof 2 speakers using the standard setup with the Wall Street Journal corpus.
Table 2: Speech separation experiments on two speakers using the standard setup with the Wall StreetJournal corpus.
Table 3: Experiments on two speaker speech separation using the standard setup with the Wall StreetJournal corpus. We explore different numbers of input mixture transformations and different dropoutrates on the latter using the training losses defined in the spectral domain. The losses in questions areL2freq and CSimLoss. The number of parameters is expressed in millions. All tested models contain44 feature maps in the first downsampling layer of the U-Net instead of 40 in Table 2. The samenumber of k = 2 residual blocks is used inside the basic structure of the residual U-Net block. SDRscores are ShoWn in the last column.________________________________________________Param s	Transforms	Dropout	Loss function	TEST SDR13.97	0	0	L2 freq	9.8813.99	5	0	L2freq	10.1114.03	10	0	L2 freq	10.9114.09	15	0	L2freq	9.9213.97	0	0	CSimLoss	9.8713.99	5	0	CSimLoss	10.6414.03	10	0	CSimLoss	11.0514.09	15	0	CSimLoss	10.8213.99	5	0.1	L2 freq	10.5414.03	10	0.1	L2freq	10.7214.09	15	0.1	L2 freq	10.9113.99	5	0.1	CSimLoss	10.96
Table 4: Listing of some existing state-of-the-art methods for two speaker speech separationModel	SNR	Window Size	Hop Length	Temporal Model Used	Input	SDRDeep Clustering (Hershey et al., 2015)	Uniform [0, 5] dB	32 ms = 256 samples	8ms = 64 samples	2 BLSTMS	log ∣STFT(x)∣	6.5Deep attractor (ChenetaL,2016)		Uniform [0, 10] dB	32 ms = 256 samples	8ms = 64 samples	4 BLSTMs	log ∣STFT(x)∣	10.5Anchor Deep attractor (Luoetal,2017)		Uniform [0, 5] dB	32 ms = 256 samples	8ms = 64 samples	4 BLSTMs	log ∣STFT(x)∣	10.8TaSNet (Luo & Mesgarani, 2017)	Uniform [0, 5] dB	Time-domain segment size 5ms = 40 samples	None	4 BLSTMs	Raw time- domain signal	11.1ConvTasNet (Luo & Mesgarani, 2018)	Uniform [-5, 5] dB	Time-domain conv-filter = 2 ms = 16 samples	50% overlap 1 ms = 8 samples	Temporal Convolution Networks	Raw time- domain signal	15.6 (claimed) 12.1 (reproduced with [0,5] dB)Deep Complex U-Net (Ours, w/o extraction mechanism)	Uniform [0, 5] dB	32 ms = 256 samples	16ms = 128 samples	None (No temporal recurrent model used)	STFT(X)	9.70Deep Complex U-Net (Ours, w/ extraction mechanism)	Uniform [0, 5] dB	32 ms = 256 samples	16ms = 128 samples	None (No temporal recurrent model used)	STFT(X)	11.34As can be seen from Table 4, state-of-the-art results in speech separation depend largely on thefollowing:1.	The use of a model that takes into account short and long term temporal dependenciessuch as BLSTMs or Temporal Convolutional Networks (Bai et al., 2018). Almost all themethods since (Hershey et al, 2015) that have led to improvements in state-of-the-art speechseparation have used either BLSTMs or TCN;2.	The STFT window size and hop length or the time-domain input segment size when usingthe raw signal. Yu et al. (2017) demonstrated that the smaller the window size, hop lengthare, the better the quality of separation. This probably explains Luo & Mesgarani (2017)and Luo & Mesgarani (2018) selection of very short time-domain segment sizes of 5 and 2ms for the TasNet and ConvTasNet archtiectures respectively.
