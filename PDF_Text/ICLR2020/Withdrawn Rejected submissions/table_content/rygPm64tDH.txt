Table 1: Benchmark results on synthetic data with correlated features. Larger numbers are better forall metrics. For metric names (K = Keep, R = Remove), (P = Positive, N = Negative, A = Absolute),(M = Mean masking, R = Resample masking, and I = Impute masking) (see Appendix for details).
Table 2: Benchmark on Independent Linear 60 datasetAttribution Method	KPM	KPR	KPI	KNM	KNR	KNI	KAM	KAR	KAIExpected Gradients	4.096	4.179	4.264	4.014	3.835	4.153	0.941	0.946	0.938Integrated Gradients	4.055	4.112	4.176	3.949	3.753	4.070	0.941	0.945	0.938Gradients	0.044	0.107	0.029	0.155	-0.150	0.172	0.902	0.905	0.902Random	-0.152	0.102	-0.152	0.111	-0.126	0.060	0.470	0.482	0.438Attribution Method	RPM	RPR	RPI	RNM	RNR	RNI	RAM	RAR	RAIExpected Gradients	4.079	3.941	4.210	4.203	4.260	4.356	0.992	0.977	1.019Integrated Gradients	4.013	3.854	4.113	4.157	4.186	4.259	0.973	0.966	0.995Gradients	0.110	-0.125	0.133	0.057	0.080	0.041	0.947	0.936	0.985Random	0.012	-0.124	0.059	0.035	0.101	0.070	0.504	0.521	0.527C.2 Benchmark evaluation metricsTo compare the performance of expected gradients with other feature attribution methods, we used thebenchmark metrics proposed in Lundberg et al. (2019). These metrics were selected as they capture avariety of recent approaches to quantitatively evaluating feature importance estimates. For example,the Keep Positive Mask metric (KPM) is used to test how well an attribution method can find thefeatures that lead to the greatest increase in the model’s output. This metric progressively removesfeatures by masking with their mean value, in order from least positive impact on model output tomost positive impact on model output, as ranked by the attribution method being evaluated. As morefeatures are masked, the model’s output is increased, creating a curve. The KPM metric measures the
Table 3: Performance of the VGG16 architecture on the ImageNet 2012 validation dataset beforeand after fine-tuning.
