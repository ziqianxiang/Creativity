Table 1: Mapping a four layer network with 51 units per layer for different mismatch values.								Layer		μNMSE				σNMSE			cvp = 0	cvp = 0.2	cvp = 1	Cvp = 2	cvp = 0	cvp = 0.2	cvp = 1	cvp = 2								Rec. layer 1	1.0	1.0	0.4	0.6	0.1	0.1	1.8	1.3Rec. layer 2	0.9	-1.0	-48.9	-132.0	0.1	1.6	52.1	143.7Rec. layer 3	0.8	-6.8	-185.1	-683.6	0.2	2.9	73.2	295.9Rec. layer 4	0.2	-6.3	-133.7	-416.3	0.9	3.5	64.0	203.6Table 2: Mapping a four layer network with 500 units per layer for different mismatch values.								Layer	μNMSE					σNMSE				TsANN L	=10 μs = 20000	100μs 2000	1ms 200	10ms 20	10 μs 20000	100μs 2000	1ms 200	10ms 20Rec. layer 1		1.0	1.0	0.8	-0.4	0.0	0.0	0.1	0.4Rec. layer 2		0.9	1.0	0.7	-1.7	0.8	0.0	0.2	5.2Rec. layer 3		0.9	0.9	0.4	-1.6	0.5	0.1	1.0	2.2Rec. layer 4		0.9	0.9	0.1	-2.0	0.3	0.3	1.2	2.2Table 3: Performance when mapping resampled data for a four layer network with 128 units per layer.
Table 2: Mapping a four layer network with 500 units per layer for different mismatch values.								Layer	μNMSE					σNMSE				TsANN L	=10 μs = 20000	100μs 2000	1ms 200	10ms 20	10 μs 20000	100μs 2000	1ms 200	10ms 20Rec. layer 1		1.0	1.0	0.8	-0.4	0.0	0.0	0.1	0.4Rec. layer 2		0.9	1.0	0.7	-1.7	0.8	0.0	0.2	5.2Rec. layer 3		0.9	0.9	0.4	-1.6	0.5	0.1	1.0	2.2Rec. layer 4		0.9	0.9	0.1	-2.0	0.3	0.3	1.2	2.2Table 3: Performance when mapping resampled data for a four layer network with 128 units per layer.
Table 3: Performance when mapping resampled data for a four layer network with 128 units per layer.
Table 4: The performance of the lpRNN cell on three benchmarks. The reference literature columnreports the best results (to our knowledge) with neural networks with less than 100K parameters.
Table 5: Performance of the lpRNN cell on the Google commands classification task with differentfiltering coefficients, α. α = 0 is equivalent to a SimpleRNN and α = 1 is an MLP.
Table 6: Impact of temporal regularization on the Penn Treebank model.
