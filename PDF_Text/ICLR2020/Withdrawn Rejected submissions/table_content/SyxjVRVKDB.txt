Table 1: 2CONV network (conv = convolution; fc = fully connected)Layer	Type	Filters/ neurons	Window	Stride	Activation-1-	conv	32	5x5	1x1	relu-2-	maxpool	-	2x2	2x2	relu-3-	conv	64	5x5	1x1	relu-4-	maxpool	-	2x2	2x2	relu-5-	fc	-512-	-	-	relu5	fc	K	-	-	-where A is the set of neurons with activity v > 0, the active neurons. Substituting the expressionfor activity from Equation 10 into its recursive definition in Equation 9, where v0i (x) = xi, revealsthat the overall computation is a series of linear transformations of x equivalent to a single lineartransformationvli(x) = xwb lTi (x) +bbli(x).
Table 2: Average percentage of active neurons in a neural networkDataset	Network	Total# neurons	Average % active neuronsMNIST	2CONV	38144	17%MNIST gauss	2CONV	38144	25%MNIST outline	2CONV	38144	27%MNIST border	2CONV	38144	23%smallNORB	2CONV	442880	20%CIFAR-10	2CONV	28160	19%	â€”input consists of an even and odd MNIST digit concatenated in random order. The training labelidentifies the even digit, making it the object of interest. We call this dataset MNISTdbl. Figure 8shows heatmap visualisations for penultimate-layer Insens and other methods for a 2CONV networktrained on this dataset to achieve 99.5% test accuracy. All the methods seem to emphasise theobject of interest mostly by making the even digit brighter and more prominent in the visualisation.
