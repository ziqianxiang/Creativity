Table 1: Attack Accuracy (percentage of successful attacks on correct classified samples) and MeanL2 score (L2 difference between original sample and adversarial one) for the CIFAR-10 test datasetfor the different architectures.
Table 2: Overall ranking for both accuracy (Acc) and L2 attacks. The rankings are obtained byordering the average accuracy and L2 for all attacks.
Table 3: Mean DBI, the rank regarding this representation metric and the Hamming distance to therobustness rankings against adversarial samples for each neural network. AM value for each of thedifferent architectures and their respective ranking. The Hamming distance of the DBI’s and AM’srankings for both the accuracy and L2 robustness ranking against adversarial samples are also shown.
Table 4: Pearson correlation between DBI and both L2 score as well as accuracy of attackssimple networks are also easier to attack (low-rank accuracy) but need more perturbation to achievethe same accuracy (high rank L2). Therefore, LeNet and other simple networks might be easier toattack because the search space is less complicated (less obfuscation (Athalye et al., 2018)). However,this does not mean they are less robust. Alternatively, as DBI suggests, LeNet and other simplenetworks might have achieved relatively good representations but without high accuracy.
Table 5: Pearson correlation between Amalgam Metric and both L2 score as well as accuracy ofattacksThe Amalgam Metric showed that both CapsNet and AllConv have the best scores which is inaccordance with their top robustness score. Figure 6 shows a visualization of equation 6 which ispart of the main equation of Amalgam Metric. On analysing further the phenomena about absolutedifference between h’ and h in the Amalgam Metric or D in equation 6. It can be noted fromthe figure that for most labels of CapsNet and AllConv the difference is relatively low than theother architectures. This contributes to have CapsNet and AllConv to have best scores. Furtherinvestigations can be carried out to analyse the effect of a label in adversarial attack based on thisfigure. This can also provide insight on the labels which are robust to adversarial attacks. A furtherstudy can also be carried out to analyse the characteristics of the neural network’s representationwhich makes a label more robust than other labels.
