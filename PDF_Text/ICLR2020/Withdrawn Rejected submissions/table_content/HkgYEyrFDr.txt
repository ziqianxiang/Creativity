Table 1: Correlation coefficient of measured represen-				Table 2: Relative performance on the Movement		tation accuracy and objectives (reward across different				Distance (10 levels) objective when using CRL ini-		environment numbers in ViZDoom evaluate across 3				tialized with curious policies with different repre-		different seeds).	P values	are all in the 1e-6 - 1e-12		sentation learning values indicated compared with		range.				random policy after 1M frames of training.		with policies initialized with different visual representations from CRL. We report the the relativemagnitude of overall reward compared with initializing from random policy after 1 million frames oftraining in Table 2. Overall, we find that better visual representations obtained from CRL are able tolead to better relative performance after 1 million frames, with policy initialization with CRL with anautoencoding objective outperforming a policy from scratch by 242% in relative performance.
Table 3: Correspondence between both better representations in policies and representation learning models(RND = random network distillation) under CRL. Value evaluated across 3 different seeds with standard errorin parentheses. These result show stronger visual representation learning algorithms give stronger visualrepresentations in policies.
Table 4: Comparison of visual representations learned in policies from CRL and other objectives. We findthat CRL with an autoencoding objective consistently gives relatively good visual representations while otherobjective, such counts based may sporadically give better visual representation, but are not consistently acrossenvironments.
Table 5: Table of visual representations learned from an autoencoding model from data collected from differentpolicies trained in different number of ViZDoom environment. CRL incentives the policy to generate diversedata that allows the best model representations across different environment numbers.
Table 6: Comparison on linear finetuning classification accuracy on room scenes in Places using policies andrepresentation learning models (using colorization or specified CRL objective) trained from data from PointNavigation and different CRL objectives. We also compare with a colorization model trained on data fromPlaces room scenes and a randomly initialized model.
Table 7: List of configurations used for the Oblige game engine.
