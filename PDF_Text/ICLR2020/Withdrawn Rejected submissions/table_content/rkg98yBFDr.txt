Table 1: Clean test accuracies of SDIMs and the discriminative counterparts.
Table 2: Classification performances of SDIMs using the proposed decision function with rejection.
Table 3: Mean detection rates of SDIMs and Glows with different thresholds on OoD detection.
Table 4: Full logits of the adversarial examples generated with different attacks. The original imageis the first sample of class 0 of MNIST test set. The first row gives the 1st percentile thresholds, andthe second row shows the logits of the original image. The largest logits are marked in bold.
Table 5: Detection rates of our rejection policies. We perform untargeted adversarial evaluation onthe first 1000 images of test sets. CW-L2 is not involved here, but carefully investigated below.
Table 6: Adversarial examples generated with targeted CW with different confidences. The originalimage is the fist sample of class 0. The first row gives the 1st percentile thresholds. Below theimages are the logits corresponding to the given targets. “-” denotes failure of generation.
Table 7: Targeted adversarial evaluations results of our rejection policies on the first 1000 test sam-ples. We report the detection rates with different thresholds and success rates of generating adver-sarial examples.
Table 8: Local MI evaluation concat-and-convolve network architecture.
Table 9: Full logits of adversarial examples generated with different attacks. The original image isthe fist sample of class 0 of CIFAR10 test set. The first row gives the 1st percentile thresholds, andthe second row shows the logits of the original image. The largest logits are marked in bold.
