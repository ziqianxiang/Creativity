Table 1: (a): Test accuracies and parameter counts |Wconv | for our ‘E’, ‘S’ and ‘SL’ variants of VGGNet,for different widths n of the convolutional layer. (b), (c), (d): Comparing the accuracies and compressionfactors C of top-performing ‘SL’ variants of our approach, with ` ∈ 2, 3 layers per convolutional block, withVGGNet (Simonyan & Zisserman, 2015) and (for CIFAR-10) variants of LegoNet (Yang et al., 2019b), anotherstate-of-the-art compression method. Baseline models marked With a * were retrained for this study.
Table 2: Comparing the accuracies and compression factors C of our shared variants of ResNet-34 and ResNet-50 with the original models and (for ImageNet) with ShaResNet (Boulch, 2018), LegoNet (Yang et al., 2019b),FSNet (Yang et al., 2019a) and Shared Wide ResNet (SWRN) (Savarese & Maire, 2019). Baseline modelsmarked with a * were retrained for this study.
Table 3: The architectures for VGGNet and the VGGNet-like networks we trained as part of ourexperiments on the CIFAR-10/100 and Tiny ImageNet datasets. The notation is described in themain text. Note that the last max-pooling layer (marked with a *) is not used when training anetwork for Tiny ImageNet: this is in order to provide a longer feature vector to the first fully-connected layer (specifically of size n * 3 * 3). The fully-connected layer sizes differ across datasetsto account for the different numbers of classes, and are set as follows: (a) CIFAR-10: FC1 = FC-512, FC2 = FC-512, FC3 = FC-10; (b) CIFAR-100: FC1 = FC-1024, FC2 = FC-1024, FC3 =FC-100; (c) Tiny ImageNet: FC1 = FC-2048, FC2 = FC-200.
Table 4: The architectures for the ResNet-like networks we trained as part of our experiments on theCIFAR-10/100 datasets. The notation is described in the main text. The baselines Eb-ResNet(p) usep = 16 for training on CIFAR-10 (as in He et al. (2016)) and p = 32 for training on CIFAR-100.
Table 5: The architectures for the ResNet-like networks we trained as part of our experiments onthe Tiny ImageNet and ImageNet datasets. The notation is described in the main text. The finalfully-connected layer has its output size set to the number of classes in the dataset (i.e. numc = 200for Tiny ImageNet and numc = 1000 for ImageNet). One important difference in the architecturesfor the two datasets is that, in the case of Tiny ImageNet, to account for the smaller resolution ofthe images, in the first scale level we use a 3 × 3 convolution without striding and suppress the firstmaxpool-2 layer. This has the effect of allowing us to feed the convolutional architecture with aninput image of size 56 × 56.
Table 6: Test accuracies and parameter counts |Wconv | for our ‘E’, ‘S’ and ‘SL’ variants of VGGNet,for different widths n of the convolutional layer. The compression factors C for the ‘S’ and ‘SL’variants are computed relative to the corresponding E-VGGNet, which contains an equal number ofchannels n in its convolutional layers. Note that all the models are trained from a state of randominitialisation.
Table 7: Test accuracies and parameter counts |Wconv | for our ‘E’ and ‘SL’ variants of the ResNetarchitecture proposed for CIFAR-10 by He et al. (2016), for different widths n of the convolutionallayers and different number of blocks b per scale level. The compression factors C for the ‘SL’variants are computed relative to their corresponding ‘E’ variants, which contain an equal numberof blocks per scale level. Note that all the models are trained from a state of random initialisation.
Table 8: CIFAR-10: Comparing the accuracies and compression factors C of top-performing ‘SL’variant of the ResNet architecture (He et al., 2016), for b = 3 blocks per scale level, with the originalResNet, other baselines ResNet-18 and ResNet-34, and state-of-the-art compression methods. Thecompression factor of the proposed model with respect to the best performing ResNet-34 architec-ture is in triple digits. However, a more appropriate comparison is arguably with ResNet*, fromwhich the model has been directly compressed by virtue of sharing the convolutional layers. Thecompression factor is still a significant 4.0, with a final weight count of only 181K. Note that themodel marked with a * has been retrained for this study.
