Table 1: Summary of loss terms.
Table 2: Averaged VSR metric evaluations for the Vid4 data set with the following metrics, PSNR: pixel-wiseaccuracy. LPIPS (AlexNet): perceptual distance to the ground truth. T-diff: pixel-wise differences of warpedframes. tOF: pixel-wise distance of estimated motions. tLP: perceptual distance between consecutive frames.
Table 3: For the Obama&Trump dataset, the averaged tLP and tOF evaluations closely correspond to our userstudies. The table below summarizes user preferences as Bradley-Terry scores.
Table 4: Metrics evaluated for the VSR Vid4 scenes.
Table 5: Metrics evaluated for the VSR of ToS.
Table 6: Training parametersVSR Param	DsOnly	DSDtl DSDtPP^	TeCoGAN∙	TecoGAN	UVT Param	DsOnlyDst	DsDtPP	TeCOGANλa	1e-3	Ds: 1e-3, Dt: 3e-4	-1e-3-	1e-3	λa	0.5	Ds: 0.5 Dt: 0.3	05λp	0.0	0.0 I	0.5		λp	0.0 ∣0.0	100.0	λφ	0.02 for VGG and 1.0 for Discriminator				λφ	from 106 decays to 0.0		λω, λc		1.0,1.0	λω					0.0, a pre-trained F is used for UST tasks		learning- rate	5e-5	1.5e-5 for Dt, 5e-5 for others.		5e-5	5e-5	λc	10.0	—		For all UVT tasks, we use a learning rate of 10-4 to train the first 90k batches and the last 10kbatches are trained with the learning rate decay from 10-4 to 0. Images of the input domain arecropped into a size of 256 × 256 when training, while the original size is 288 × 288. While theAdditional training parameters are also listed in Table 6. For UVT, Lcontent and Lφ are only used toimprove the convergence of the training process. We fade out the Lcontent in the first 10k batches andthe Lφ is used for the first 80k and faded out in last 20k.
