Table 2: non-convex caseB Additional ExperimentsConvex Losses. In these experiments, we examine the convergence of PopSGD versus parallel timefor different node counts, and compared it with the sequential baseline. More precisely, for PopSGD,we execute the protocol by simulating the entire sequence of interactions sequentially, and track theevolution of train and test loss at an arbitrary fixed model xi with respect to the number of SGD stepsit performs. Notice that this is practically equivalent to tracking with respect to parallel time. In thiscase, the theory suggests that loss convergence and variance should both improve when increasingthe number of nodes. Figure 3(a) presents the results for the synthetic linear regression example withd = 32, for various values of n, for constant learning rate Î· = 0.001 across all models, and batchsize 1 for each local gradient. Figure 3(b) compares PopSGD convergence (with local batch size 1)against sequential mini-batch SGD with batch size equal to the number of nodes n.
