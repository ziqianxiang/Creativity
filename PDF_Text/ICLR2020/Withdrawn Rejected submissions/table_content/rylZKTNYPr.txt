Table 1: Overview over the different models used for comparisonNAME	DESCRIPTIONRNN	Vanilla ReLU based RNNL2RNN	Vanilla ReLU RNN with standard L2 regularization on all weightsiRNN	RNN with initialization Wo = I and ho = 0 (Le et al., 2015)npRNN	RNN with weights initialized to a normalized positive definite matrix with largest eigenvalue of 1 and biases initialized to zero (Talathi & Vartak, 2016)PLRNN	PLRNN as given in eq. 1 (KoPPe et al., 2019)iPLRNN	PLRNN with initialization Ao = I, Wo = 0 and ho = 0rPLRNN	PLRNN initialized as illustrated in Fig. S1, with additional regulariza- tion term (eq. 4) during trainingLSTM	Long Short-Term Memory (Hochreiter & Schmidhuber, 1997)4	Numerical experiments4.1	Machine learning benchmarksWe compared the performance of our rPLRNN to other models on the following three benchmarksrequiring long short-term maintenance of information (as in Talathi & Vartak (2016) and Hochreiter& Schmidhuber (1997)): 1) The addition problem of time length T consists of 100 000 training and10 000 test samples of 2 × T input series S = {s1, . . . , sT}, where entries s1,: ∈ [0, 1] are drawnfrom a uniform random distribution and s2,: ∈ {0, 1} contains zeros except for two indicator bitsplaced randomly at times t1 < 10 and t2 < T/2. Constraints on t1 and t2 are chosen such that everytrial requires a long memory of at least T/2 time steps. At the last time step T, the target output of thenetwork is the sum of the two inputs in s1,: indicated by the 1-entries in s2,:
