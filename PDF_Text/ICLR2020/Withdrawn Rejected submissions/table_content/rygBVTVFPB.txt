Table 1: Comparison of relative errors (× 10—2) of RL-WENO and WENO with standard deviationsof the errors among 10 trials in the parenthesis. Temporal discretization: RK4; flux function: ɪu2.
Table 2: Comparison of relative errors (× 10—2) of RL-WENO and WENO with standard deviationsof the errors among 10 trials in the parenthesis. Temporal discretization: RK4; flux function:看u4.
Table 3: Comparison of relative errors (×10-2) of RL-WENO, WENO, and SL-trained policy withstandard deviations of the errors among 10 trials in the parenthesis. Temporal discretization: RK4;flux function: 1 u2. RL-Weno consistently outperforms WENO and SL-trained policy in all test cases.
Table 4: Comparison of relative errors (×10-2) of RL-WENO, WENO, and SL-trained policy withstandard deviations of the errors among 10 trials in the parenthesis. Temporal discretization: RK4;flux function: 1 u4. RL-weno consistently outperforms WENO and SL-trained policy in all test cases.
Table 5: Average inference time (in seconds) for RL-WENO and WENO. Bold numbers are thesmallest ones.
Table 6: Relative error of RL-WENO and WENO (×10-2) on grid sizes tested in table 5. NoteRL-WENO is only trained on grid (∆x, ∆t) = (0.02, 0.004)From the table we can tell that as ∆x decreases, i.e., as the grid becomes denser, all methods, exceptfor the RL-WENO (GPU), requires significant more time to finish the computation. The reason thatthe time cost of the GPU-version of RL-WENO does not grow is that on GPU, we can compute15Under review as a conference paper at ICLR 2020all approximations in the next step (i.e., to compute (U0t+1, U1t+1, ..., UJt+1) given (U0t, U1t, ..., UJt ),which dominates the computation cost of the algorithm) together in parallel. Thus, the increaseof grids does not affect much of the computation time. Therefore, for coarse grid, well-optimizedWENO indeed has clear speed advantage over RL-WENO (even on GPU), but on a much denser grid,RL-WENO (GPU) can be faster than well-optimized WENO by leveraging the paralleling nature ofthe algorithm.
