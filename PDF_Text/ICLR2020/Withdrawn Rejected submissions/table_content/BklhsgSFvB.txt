Table 1: Experiment result for graph datasetsDataset	Labeled Data Ratio	Single-Task	Shared-Bottom	Cross-Stitch	MMoE	Our	All 80%	0.8063	0.8171	0.8204	0.8049	0.8333Tox21	Partially 10%	0.7138	0.7309	0.7128	0.7331	0.7410	All 10%	0.7719	0.7934	0.7823	0.7848	0.8033	All 80%	0.6458	0.6484	0.6676	0.6406	0.6701SIDER	Partially 10%	0.5682	0.5534	0.5504	0.5377	0.5741	All 10%	0.6277	0.6290	0.6285	0.6261	0.6363DBLP	Partially 1%	0.8069	0.8056	0.5148	0.7930	0.8241	All 10%	0.8232	0.8077	0.5150	0.8177	0.8367BlogCatalog	Partially 5%	0.5154	0.6521	0.5259	0.6720	0.6769	All 20%	0.6100	0.6667	0.5272	0.6850	0.6861Table 2: Experiment result for text datasetDataset	Labeled Data Ratio	Single-Task	Shared-Bottom	Cross-Stitch	MMoE	Our	All 80%	0.8172	0.8272	0.8543	0.8198	0.8484TMDb	Partially 10%	0.7324	0.7293	0.7234	0.6590	0.7404	All 10%	0.8033	0.8227	0.8452	0.7869	0.84804.4	ResultWe report the performance of our approach and baselines on graph classification, node classificationand text classification tasks in terms of AUC-ROC score in Table 1 and 2 respectively.3
Table 2: Experiment result for text datasetDataset	Labeled Data Ratio	Single-Task	Shared-Bottom	Cross-Stitch	MMoE	Our	All 80%	0.8172	0.8272	0.8543	0.8198	0.8484TMDb	Partially 10%	0.7324	0.7293	0.7234	0.6590	0.7404	All 10%	0.8033	0.8227	0.8452	0.7869	0.84804.4	ResultWe report the performance of our approach and baselines on graph classification, node classificationand text classification tasks in terms of AUC-ROC score in Table 1 and 2 respectively.3From the above result, first of all, we can see that the multi-task methods outperform the single-taskmethod in most cases which shows the effectiveness of knowledge transfer and multi-task learning.
Table 3: Graph datasets summaryDataset	Source	Graphs	Nodes Avg.	Edges Avg.	Graph Labels	Node LabelsTox21	Bio	8014	18	48	12	-SIDER	Bio	1427	33	105	27	-DBLP	Citation	1	14704	24778	-	18BlogCatalog	Social	1	10312	333983	-	39Table 4: Text dataset summaryDataset Movies Words Avg. GenresTMDb 8014	59	20B	Architecture DetailsB.1	Architecture Details for Graph ModelAs shown in Figure 4, in the Encoder Block, we use several layers of graph convolutional lay-ers (Kipf & Welling, 2016) followed by the layer normalization (Ba et al., 2016). In the ReadoutBlock, for graph-level task, we use set-to-set (Vinyals et al., 2015) as the global pooling operator10Under review as a conference paper at ICLR 2020to extract the graph-level representation which is later fed to a classifier; while for node-level task,we simply eliminate the global pooling layer and feed the node-level representation directly to theclassifier.
Table 4: Text dataset summaryDataset Movies Words Avg. GenresTMDb 8014	59	20B	Architecture DetailsB.1	Architecture Details for Graph ModelAs shown in Figure 4, in the Encoder Block, we use several layers of graph convolutional lay-ers (Kipf & Welling, 2016) followed by the layer normalization (Ba et al., 2016). In the ReadoutBlock, for graph-level task, we use set-to-set (Vinyals et al., 2015) as the global pooling operator10Under review as a conference paper at ICLR 2020to extract the graph-level representation which is later fed to a classifier; while for node-level task,we simply eliminate the global pooling layer and feed the node-level representation directly to theclassifier.
