Table 1: Hypothesis Prediction, broken down by triplet (pre-trained) and non-triplet (not seen inpre-training)	Method	Overall	Triplet Accuracy	New Template AccuracyColor Switch	Fixed Policy	89.6%	94.3%	88.7%	Finetuned Policy	89.3%	92.3%	86.3%Pushblock	Fixed Policy	88.1%	897%	86.5%	Finetuned Policy	85.1%	85.2%	85.4%Crafting	Fixed Policy	79.3%	91.4%	69.9%	Finetuned Policy	95.9%	96.7%	95.1%We conduct additional experiments in the Appendix. In Appendix G, we tease further analyse theproblem by experimenting with an oracle hypothesis predictor. In Appendix F we experiment withdifferent pretraining functions. In appendix Appendix H we look at training baselines for longer. AndIn Appendix I, we look at whether giving the baselines more past frames N improves performance.
Table 2:	Pretraining HyperparametersParameterValueAlgorithmTimesteps per batchClip paramEntropy coeffNumber of parallel processesOptimizer epochs per iterationOptimizer step sizeOptimizer batch sizeDiscount γGAE λlearning rate scheduleOptimizerPast Frame Window SizePPO (Schulman et al., 2017)20480.20.1
Table 3:	Finetuning HyperparametersParameterValueAlgorithmTimesteps per batchEntropy coeffNumber of parallel processesOptimizer epochs per iterationOptimizer step sizeOptimizer batch sizeDiscount γGAE λlearning rate scheduleOptimizerPast Frame Window SizePPO (Schulman et al., 2017)20480.184
Table 4:	Prediction HyperparametersParameterValueTimesteps per batchOptimizer step sizeOptimizer batch sizelearning rate scheduleOptimizerMemory Burn-inMemory SizeAlternate Training Window20481e-3128constantADAM Kingma & Ba (2014)1000002001000000017
Table 5:	Policy Network HyperparametersParameterSeq2Vec ModelWord Embedding SizeHidden SizeMLP Num Hidden LayersNumber of MLP ModulesTransfer LayerValueBag-of-Words3232216tanh18Under review as a conference paper at ICLR 2020Table 6:	MLP Baseline Policy Network HyperparametersParameterSeq2Vec Model
Table 6:	MLP Baseline Policy Network HyperparametersParameterSeq2Vec ModelWord Embedding SizeHidden SizeMLP Num Hidden LayersTransfer LayerValueBag-of-Words32322tanhTable 7:	Transformer Network HyperparametersParameter ValueWord Embedding Size 32Hidden Size	32Transfer Layer	ReLUTransformer N	3Table 8:	Baseline Prediction Network Hyperparameters
Table 7:	Transformer Network HyperparametersParameter ValueWord Embedding Size 32Hidden Size	32Transfer Layer	ReLUTransformer N	3Table 8:	Baseline Prediction Network HyperparametersParameter	ValueSeq2Vec Model	LSTMLSTM Num Layers	1Word Embedding Size	32Hidden Size	32MLP Num Hidden Layers	2Transfer Layer	tanh19Under review as a conference paper at ICLR 2020D Additional FiguresCoIorSwitch%t①」」oɔ -lωMSU4llιl
Table 8:	Baseline Prediction Network HyperparametersParameter	ValueSeq2Vec Model	LSTMLSTM Num Layers	1Word Embedding Size	32Hidden Size	32MLP Num Hidden Layers	2Transfer Layer	tanh19Under review as a conference paper at ICLR 2020D Additional FiguresCoIorSwitch%t①」」oɔ -lωMSU4llιlJhlnITmFTnr'H"'W∙T∙rιτ∣5 WE f'r'l'ΓTl%t①」」oɔ -lωMSU4Timesteps---- Fixed-Policy From Pretrain
Table 9: Oracle Evaluation of Learned PoliciesMethod	Oracle % can answer	Theoretic Upper BoundPretrained without Policy Finetuning	75.00	87.50Pretrained with Finetuning	98.90	99.45RL Baseline	3.00	51.50No Act Baseline	0	50.00Random Act Baseline	0.7	50.35Table 9 shows these results. What we see is that indeed, the actions taken by the baselines are notable to verify the hypothesis. The Baseline RL policy only allows the oracle predictor to predict thehypothesis 3% of the time, giving us a upper bound of 51.5% on hypothesis accuracy. Random actionis even worse, only leading to the right state sequence 0.7% of the time. No action (the agent that justtries to answer right away) as expected is never able to get the right sequence. For the pre-traininedmethods we see that we are able to get to the right states most of the time. The finetuned policygets the right states almost 100% of the time. With the fixed policy from pretraining, the oracle cananswer 75% of the time, meaning that by guessing you could theoretically get to about 88%.
