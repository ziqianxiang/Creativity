Table 1: Few-shot classification results on mini-ImageNet. The Gaussian DropGrad method im-proves the performance of gradient-based models on 1-shot and 5-shot classification tasks.
Table 2: Performance of applying DropGrad to different layers. We conduct experiments onthe 5-shot classification task using MAML on mini-ImageNet. It is more helpful in improving theperformance by dropping the gradients closer to the output layers (e.g., FC and Block4+FC).
Table 3: Cross-domain performance for few-shot classification. We use the mini-ImageNet andCUB datasets for the meta-training and meta-testing steps, respectively. The improvement of ap-plying the proposed DropGrad method is more significant in the cross-domain cases than the intra-domain ones._______________________________________________________________________________________Model	1-Shot	5-ShotMAML (Finn et al., 2017) MAML w/ DropGrad (p = 0.1)	31.52 ± 0.52% 33.20 ± 0.67%	45.56 ± 0.51% 51.05 ± 0.56%MetaSGD (Li et al., 2017) MetaSGD w/ DropGrad (p = 0.1)	34.52 ± 0.63% 36.77 ± 0.72%	49.22 ± 0.58% 55.13 ± 0.72%MetaSGD* MetaSGD* w/ Gaussian DropGrad (p	43.98 ± 0.77% 0.1)	45.33 ± 0.81%	57.95 ± 0.81% 59.94 ± 0.82%MAML++ (Antoniou et al., 2019) MAML++ w/ DropGrad (p = 0.2)	40.73 ± 0.49% 44.27 ± 0.50%	60.57 ± 0.49% 63.79 ± 0.48%optimized. The DropGrad regularization method mitigates the overfitting issue and facilitates thetraining procedure.
Table 4: Precision and success rate on the OTB2015 dataset. The DropGrad method can beapplied to visual tracking and improve the tracking performance.
Table 5: Viewpoint estimation results. The DropGrad method can be applied to few-shot viewpointestimation frameworks to mitigate the overfitting problem.
Table 6: 5-shot classification results of MAML under various hyper-parameter settings. Westudy the learning rate α and number of iterations ninnerin the inner-loop optimization of MAMLusing mini-ImageNet dataset.
