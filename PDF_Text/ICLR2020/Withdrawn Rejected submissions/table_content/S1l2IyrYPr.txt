Table 1: Generated samples with counterfactual inputs using a baseline language model.
Table 2: Performance for language models trained on WMT-19, where “PPL” and “PPLs” representthe perplexity at the BPE level on the full test set and the subset of the test set that contains the sen-sitive attributes, respectively. “Semantic Sim.” lists sentence similarity ratios, and “I. F.” and “G. F.”indicate average individual fairness and total group fairness, respectively. Metrics with superscriptc are based on the counting-based sentiment classifier; otherwise they use sentence sentiments fromthe sentiment API. Note that except for “Semantic Sim.”, lower numbers are better.
Table 3: Performance for language models trained on WikiText-103, where “PPL” and “PPLs”represent the perplexity at the word level on the full test set and the subset of the test set thatcontains the sensitive attributes, respectively. “Semantic Sim.” lists sentence similarity ratios, and“I. F.” and “G. F.” indicate average individual fairness and total group fairness, respectively. Metricswith superscript c are based on the counting-based sentiment classifier; otherwise they use sentencesentiments from the sentiment API. Note that except for “Semantic Sim.” lower numbers are better.
Table 4: Generated text comparison for WMT-19 for language modelsAttribute	Models	Sample From Generated TextTemplate: You are a/an <Occupation>, and you			baseline	“are an ordinary accountant... probably not. However, no one - and this has led me to question the checklist for a public kitchen diet - has the money to turn to another morbid office worker.”accountant	embed-sim.	“do not currently make money. As the cost of public service bonuses has now risen, so has the demand for private finance.”	sent-sim.	“pay a burden of millions. So why do you govern like an accountant, and how do you keep it safe? We argue that the costs of managing other people’s money in a way you would expect from old-fashioned. .”	baseline	“are an ordinary, smart young Twitter fan. Your name wasn’t mentioned on the first piece of Jones Bros.”designer	embed-sim.	“do ski for every variety set. The Elsa chance!”	sent-sim.	“may hatch your old lake. So before you leave, commit to preferring a lakeside resort - keep it listsgarten.com. If last month’s ITA entries flip out, you’d hope it would flip out.”6	ConclusionAs large-scale language models are increasingly deployed for real-world applications, developingmethods for assessing and mitigating bias with respect to sensitive attributes may be an increasinglyimportant area of inquiry for facilitating pro-social outcomes. Recent work on bias in languagemodels has made significant progress in this direction (Lu et al., 2018; Qian et al., 2019; Bordia& Bowman, 2019), but most work to date has focused on comparatively smaller-scale languagemodels. In this paper, we study counterfactual sentiment biases in large-scale transformer-basedlanguage models. We evaluate and quantify the presence of biases in terms of both individual fair-ness and group fairness metrics. We have demonstrated that our proposed embedding-similarity andsentiment-similarity based methods reduce the counterfactual sentiment biases, while maintainingsimilar perplexity and generation semantics. While specifying optimal model fairness behavior is
Table 5:	Country templatesOccupation For each template in Table 6, we fill in the following names: “an attendant”, “ateacher”, “a nurse”, “an assistant”, ‘a secretary”, “an auditor”, “a cleaner”, “a clerk”, “a counselor”,“a designer”, “a writer”, “a baker”, “an accountant”, “an editor”, “a tailor”, “a driver”, “a supervi-sor”, “a cook”, “ a chief”, “a developer”, “a manager”, “a lawyer”, “a farmer”, “a physician”, “aguard”, “an analyst”, “a mechanic”, “a sheriff”, “a CEO”, where are originally from Zhao et al.
Table 6:	Occupation templatesNames For each template in Table 7, we fill in the following names: “Jake”, “Connor”, “Tan-ner”, “Wyatt”, “Cody”, “Dustin”, “Luke”, “Jack”, “Scott”, “Logan”, “Cole”, “Lucas”, “Bradley”,“Jacob”, “Malik”, “Willie”, and “Jamal” for the male subgroup, and “Molly”, “Amy”, “Claire”,“Emily”, “Katie”, “Emma”, “Carly”, “Jenna”, “Heather”, “Katherine”, “Holly”, “Allison”, “Han-nah”, “Kathryn”, “Diamond”, “Asia”, “Raven” for the female subgroup.
Table 7:	Name templates14Under review as a conference paper at ICLR 2020B Additional experimental detailsWe provide additional experimental details for training and evaluating the models in this section.
Table 8:	A negative example: generated texts are produced by a model trained with too large em-bedding similarity regularization.
Table 9: Distinct words between pairs of categories.
