Table 1: Experiments on LeNet-5-Caffe architecture with 20-50-800-500 number of output filters andhidden layers and MLP with the architecture of 784-500-300 as the number of hidden units for eachlayer. The sparsity level is reported layer-wise. The sparsity and error are both reported as %.
Table 2: Experiments on Cifar-10 and Cifar-100 using VGG-16 network. The sparsity and error areboth reported as %.
Table 3: The accuracy drop, compared to the baseline, for different levels of sparsity using Cifar-100.
Table 4: Experiments on Cifar-100 for investigating the robustness of the proposed method to thehyperparameter selection.
