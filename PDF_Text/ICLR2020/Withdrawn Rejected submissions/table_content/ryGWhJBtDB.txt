Table 1: The optimal test accuracy and final training loss for a range of batch sizes under a constantstep budget. For each batch size, we train a 16-4 Wide-ResNet with ghost batch normalization for9765 updates, and we perform a grid search to identify the optimal learning rate which maximizesthe test set accuracy. We provide the average performance of the best 12 out of 15 training runs. Thefinal training loss falls as the batch size increases, but the optimal test accuracy drops significantly forbatch sizes greater than 4096. This strongly suggests that minibatch noise can enhance generalization.
Table 2: The optimal test accuracy and final training loss for a range of batch sizes under a constantstep budget. For each batch size, we train a 16-4 Wide-ResNet without batch normalization using“zeroInit” (defined in appendix C) for 156,250 updates. We perform a grid search to identify theoptimal learning rate which maximizes the test accuracy, and we provide the average performance ofthe best 12 out of 15 training runs. The final test accuracy falls for very large batches.
Table 3: ResNet-50, trained on ImageNet for 90 epochs. We follow the implementation of Goyalet al. (2017), however we introduce our modified learning rate schedule defined in appendix A. Wedo not use learning rate warmup. We perform a grid search to identify the optimal effective learningrate and report the performance of a single training run. The test accuracies achieved by SGD andMomentum are equal when the batch size is small, but Momentum outperforms SGD when the batchsize is large. For SGD with Momentum, the optimal effective learning rate is proportional to batchsize for all batch sizes considered, while this linear scaling rule breaks at large batch sizes for SGD.
Table 4: The optimal test set MSE and final training set MSE for a range of batch sizes under aconstant step budget. For each batch size, we train a fully connected autoencoder on MNIST for156,250 updates. We perform a grid search to identify the optimal learning rate which maximizes thetest set MSE, and we provide the average performance of the best 5 out of 7 training runs. The finaltest MSE falls for large batch sizes, although this effect is rather weak in this model.
Table 5: The optimal test set perplexity and final training set perplexity for a range of batch sizesunder a constant step budget. For each batch size, we train a word-level LSTM on PTB for 16560updates. We perform a grid search to identify the optimal learning rate which maximizes the test setperplexity, and we provide the average performance of the best 5 out of 7 training runs. The optimaltest perplexity increases considerably as the batch size rises, while the training perplexity falls.
