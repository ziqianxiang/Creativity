Table 1: Comparison of adding random errors to the sensitive or insensitive region of LSTM gates.
Table 2: LSTM perplexity and execution time (ms).
Table 3: GRU perplexity and execution time (ms).
Table 4: GNMT BLEU score and execution time (ms). (1024, 2048) indicates the hidden size is1024 and the input size is 2048; similarly for (1024, 1024).
Table 5: Sensitivity study of dimension reduction.
Table 6: Inference quality and parameter size comparison under different levels of quantization onthe little modulePrecision	Base	FP32	INT16	INT8	INT4	INT2	INT1Perplexity	80.64	81.28	81.18	81.25	81.28	82.14	94.75Diff.	n/a	-0.64	-0.54	-0.61	-0.64	-1.50	-14.11MSE	n/a	0.408	0.425	0.444	0.465	0.573	3.337Param. size (MB)	68.7	19.1	9.6	4.8	2.4	1.2	0.63We measured the execution time with multi-threading.
Table 7: Comparison of our proposed dual-module inference (using 50% insensitive ratio) withweight pruning using one LSTM layer with 1500 units in word language modeling task on WikiText-2 dataset.
