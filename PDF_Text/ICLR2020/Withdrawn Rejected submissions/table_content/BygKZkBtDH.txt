Table 1: BLEU scores of 36 separately trained vanilla models and our single tied-multi model usedwith n (1 ≤ n ≤ N) encoder and m (1 ≤ m ≤ M) decoder layers. One set of decoding times is alsoshown given the fact that a vanilla model with n encoder and m decoder layers and our tied-multimodel have identical shapes when used with n encoder and m decoder layers for decoding.
Table 2: Dynamic layer combination selection results in decoding speed (in seconds, batch size of 1)and BLEU, including the baseline and oracle for the WMT newstest2018 corpus using the tied-multimodel architecture.
Table 3: BLEU scores of a total of four NMT models: the tied-multi model with (left block) andwithout (center and right blocks) RS layers, each trained with (top block) and without (bottom block)sequence distillation.
Table 4: Model sizes for different encoder-decoder layer combinations. The relative size is calcu-lated regarding the tied-multi model as a standard. Similarly to “36 vanilla models,” “36 RS models”represents the total number of parameters of all RS models.
