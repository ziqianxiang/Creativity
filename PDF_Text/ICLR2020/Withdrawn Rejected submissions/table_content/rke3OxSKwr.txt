Table 1: Offline evaluation of the Pervasive Attention (PA), Masked Pervasive Attention (MPA),Transformer (T) and masked Transformer (MT) baselines. PA* corresponds to results reported byElbayad et al. (2018). We evaluate using greedy decoding (G) and beam-search (BS).
Table 2: IWSLT De-En: Impact of hidden state updates in transformer-based low-latency decoders.
Table 3: Training time of our models compared to the baseline: IWSLT’14 De-En models weretrained on a single GPU with a batch-size of 4k tokens (^2 gradient accumulation), and WMT'15De-En models were trained on 2 GPUs with a batch-size of 3.5k tokens (^64 gradient accumula-tion).
Table 4: Decoding speed in tokens/s on both GPU (RTX2080Ti) and CPU. For each model wemeasure the decoding speed with and without updating the decoder states.
Table 5: Numerical results of IWSLT De-EnB.3 IWSLT En-Dek	BLEU	AP	AL	DAL	k	BLEU	AP	AL	DAL	k	BLEU	AP	AL	DAL1	18.5	0.61	2.5	2.8	1	18.7	0.62	2.6	3.0	1	16.4	0.59	1.9	2.53	22.8	0.69	3.8	4.1	3	23.2	0.69	3.9	4.2	3	22.1	0.68	3.7	4.05	25.0	0.75	5.5	5.7	5	25.4	0.75	5.4	5.8	5	25.4	0.75	5.3	5.77	25.9	0.81	7.2	7.5	7	26.4	0.81	7.1	7.4	7	26.4	0.81	7.1	7.49	26.2	0.86	8.9	9.2	9	26.4	0.85	8.7	9.1	9	26.7	0.85	8.8	9.1	(a) MT, k		“8			(b) MT, k		“7			(c) MT, k		keval	k	BLEU	AP	AL	DAL	k	BLEU	AP	AL	DAL	k	BLEU	AP	AL	DAL1	21.2	0.60	2.3	2.6	1	21.4	0.60	2.3	2.5	1	16.6	0.57	1.6	2.13	24.3	0.68	3.8	4.0	3	24.7	0.68	3.7	3.9	3	22.9	0.67	3.5	3.75	25.8	0.75	5.4	5.7	5	26.2	0.75	5.3	5.6	5	25.6	0.75	5.2	5.57	26.2	0.81	7.2	7.5	7	26.3	0.81	7.0	7.3	7	26.3	0.81	7.0	7.39	26.4	0.86	8.9	9.2	9	26.0	0.85	8.6	9.0	9	26.2	0.85	8.6	9.0(d) MT, k “ 8 - Update(e) MT, k “ 7 - Update(f) MT, k “ keval - UpdateTable 6: Numerical results of IWSLT En-De16
Table 6: Numerical results of IWSLT En-De16Under review as a conference paper at ICLR 20208642222UELB6422228(a) MPA wait-k (AP)8(b) MPA wait-k (AL)(c) MPA wait-k (DAL)642222UELB6422228
Table 7: Numerical results of WMT De-En underlying figures 3 and 4 in the main paper.
