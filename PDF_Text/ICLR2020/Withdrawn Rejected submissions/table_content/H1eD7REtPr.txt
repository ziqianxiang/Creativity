Table 1: Hyper-parameters for LQR experimentsStep size T	Adam βι	Adam β2	Restart period r Stop criterion Y0.0001	0.01	0.9	0.999	100	kKi - K?k2 ≤ 10-4	14.2 Atari GamesWe apply the proposed AltQ algorithms to more challenging tasks of deep convolutional neuralnetwork playing a group of Atari 2600 games. The particular DQN we train to compare againstadopts the dueling network structure (Wang et al., 2016), double Q-learning setup (Van Hasselt et al.,2016), -greedy exploration and experience replay (Mnih et al., 2013). Adam is also adopted, withoutmomentum restart, as the optimizer for the inner-loop supervised learning process. AltQ-Adam andAltQ-AdamR are implemented using the identical setup of network construction, exploration andsampling strategies.
Table 2: Hyper-parameters for Atari games experiments of DQN, AltQ-Adam and AltQ-AdamRStep size 0.0001	Scale factor T 0.0001	Adam β1 Adam β2 0.9	0.999	Restart period r 104	Buffer size 105γ 0.99	Batch size B 32	Total training steps K 107	Target update frequency (DQN only) 104	5.1	Modification of AlgorithmsAlthough Adam has obtained great success as an optimizer in deep learning, it is well known thatAdam by nature is non-convergent even for simple convex loss functions (Reddi et al., 2018). Instead, aslightly modified version called AMSGrad (Reddi et al., 2018) is widely used to study the convergenceproperty of the Adam-type algorithms. Compared with the update rule of Adam, AMSGrad makesthe sequence Vt,i increasing along the time step t for each entry i ∈ [d]. Here, We apply the updaterule of AMSGrad to the AltQ algorithm and refer to such an algorithm as AltQ-AMSGrad. Algorithm3 describes AItQ-AMSGrad in detail, where ∏d V1/4 (θ0) = min Iw1/4 (θ0 - θ)∣∣. Correspondingly,We introduce AltQ-AMSGradR which applies the same update rule as Algorithm 3, but resets mt, Vtwith a period of r, i.e., mt = 0,Vt = 0, ∀t = kr,k = 1, 2, ∙∙∙.
Table 3: Best empirical return of 23 Atari games with DQN, AltQ-Adam and AltQ-AdamRB Proof of Theorem 1Different from the regret bound for AMSGrad obtained in Reddi et al. (2018), our analysis is onthe convergence rate. In fact, a slight modification of our proof also provides the convergence ratefor AMSGrad for conventional strongly convex optimization, which can be of independent interest.
