Table 1: gato performs well on test accuracy with both learning rates 4e-3 and 1e-4. We showthe accuracy of length of stay prediction averaged across seeds. sru performs best on this task. Theranges in performance for gato and sru overlap. *eurnn had an NAN error on all seeds shortlyfollowing the reported accuracies.
Table 2: gato outperforms gru and lstm. We calculate the perplexity averaged over three seedson Penn Treebank language modeling in the unregularized setting. gato achieves similar perplexityto gru. rhn gets infinite loss. Excluding the rhn, sru and eurnn have the highest test perplexity.
Table 3: Size of s can matter. For MIMIC, using 1/4 of the hidden state for s works slightly betterthan using 1/2 as in the main experiments. For Penn Treebank, 1/2 was better than 1/4, suggestinglong-term gradient propagation is important for this task.
Table 4: Tanh performs similarly to sigmoid for regularizing non-linearity on mimic andPenn TreeBank.
Table 5: sin for decoder non-linearity performs similarly to cos on mimic and Penn TreeBankModel	mimic Test Acc., lr 4e-3	mimic Test Acc., lr 1e-4	Penn Test pplcos (original)	0.693	0.646	112.85sin	0.692	0.648	112.8915Under review as a conference paper at ICLR 2020Numberof Training Examples Seen (Thousands)NumberofTraining Examples Seen (Thousands)Numberof Training Examples Seen (Thousands)NumberofTraining Examples Seen (Thousands)Figure 8: sin for decoder non-linearity performs similarly to cos on adding task.
