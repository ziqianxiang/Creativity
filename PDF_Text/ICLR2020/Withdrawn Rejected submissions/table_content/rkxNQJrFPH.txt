Table 1: Number of Class Labels in Our Meta-Split (both Standard Split and Proposed Split). Withineach meta-split, training, validation and testing class labels are disjoint.
Table 2: Zero-Shot Classification Accuracy On Various Datasets/Meta-Splits Combinations; ourmethod is compared with 2 versions of PDCNN (Lei Ba et al., 2015) and MEGAZSL (Zhu et al.,2017); number of unseen class labels in the test set are recorded next to the name of each dataset.
Table 3: Comparison of different Y and initialμ values. The pre-trained model is completelyignored in the case where Y = 0.
Table 4: Comparison of different embeddingmethods for N3 in zero-shot classification task.
Table 5: N3 Zero-Shot Classification Accuracy on CUB2011 with Different Base Model Architec-turesBased on the results shown in Table. 5, we were surprised to find out that SqueezeNet performsas well as ResNet as N3’s base model and GoogleNet can even out-perform ResNet-18 as a betterbase model. One explaination for the superior performance of GoogleNet is that we have noted inSection. 4.4, parameters or batch normalizations are hardly fine-tuned and GoogleNet in fact doesnot make use of Batch Normalization, thus avoiding the potential performance degradations causedby lack of fine-tuning on Batch Normalization layers.
Table 6: N3 Zero-Shot Classification Performance on CUB-2010, CUB-2011, Oxford FlowerAs shown in Table 6, N3 generated ResNet18 out-performs prior arts by a significant margin both interms of average precision and classification accuracy on both CUB and Oxford Flower dataset.
