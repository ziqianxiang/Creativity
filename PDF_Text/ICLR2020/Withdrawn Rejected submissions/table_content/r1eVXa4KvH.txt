Table 1: Performance of BERTLARGE (Devlin et al., 2018), a 24 layer Transformer with an embeddingsize of 1024, does not improve with the increasing number of heads after 8 heads.
Table 2: (A): 24 layer modified Transformer with a fixed head size of 128 and 512 embedding sizeshows an improvement in the accuracy with the increasing number of heads. (B) The fixed head sizemodel with 512 embedding size and 8 heads shows an improvement in accuracy with the increasinghead size. This shows that indeed head size is an important capacity controlling parameter in the selfattention architecture.
Table 3: (A): 24 layer modified Transformer with a fixed head size of 128 and embedding size of 768shows an improvement in the accuracy with the increasing number of heads.
