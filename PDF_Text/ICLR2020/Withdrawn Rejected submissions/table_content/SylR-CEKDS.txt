Table 1: Accuracy (%) on the compositionality task using different numbers of training examples from the heldout question type.
Table 2: Results of the second experiment.
Table 3: Evaluation results of experiment 3. Our grammar enhanced model is compared with a supervisedtrained baseline from experiment 2, a sequence generative RL baseline, and a text-based model. The models arecompared in terms of average energy value, average expected information gain (EIG) value, the ratio of EIGvalue greater than 0.9/0, number of unique questions generated, and number of unique novel questions generated(by “novel” we mean questions not presented in the human dataset). The EIG of text-based model is calculatedbased on the program form of the generated questions.
Table 4: Part 1 of the grammatical rules. Rules marked with b have a reference to the Battleship game board(e.g., during evaluation the function orient looks up the orientation of a ship on the game board) while all otherrules are domain-general (i.e., can be evaluated without access to a game board).
Table 5: Part 2 of the grammatical rules. See text for details.
Table 6: Results of the synthetic reasoning tasks.
Table 7: Log-likelihood (LL) on different splits of the sampled evaluations based on the uncertainty of the board.
