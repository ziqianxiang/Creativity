Table 1: Generalization on clean test data. LeNet’ models learned with varying amounts of trainingsamples per class are evaluted on MNIST test set. Jacobian regularizer substantially reduces the normof the Jacobian while retaining test accuracy. Errors indicate 95% confidence intervals over 5 distinctruns for full training and 15 for sub-sample training.
Table 2: Generalization on clean test data from an unseen domain. LeNet’ models learned withall MNIST training data are evaluated for accuracy on data from the novel input domain of USPS testset. Here, each regularizer, including Jacobian, increases accuracy over an unregularized model. Inaddition, the regularizers may be combined for the strongest generalization effects. Averages and95% ConfidenCe intervals are estimated over 5 distinct runs.________________________No regularization	L2	Dropout	Jacobian	All Combined80.4 ± 0.7	83.3 ± 0.8 81.9 ± 1.4	81.3 ± 0.9	85.7 ± 1.0(a) White noise	(b) PGD	(C) CWFigure 3: Robustness against random and adversarial input perturbations. This key result il-lustrates that JaCobian regularization signifiCantly inCreases the robustness of a learned model withLeNet’ arChiteCture trained on the MNIST dataset. (a) Considering robustness under white noise per-turbations, JaCobian minimization is the most effeCtive regularizer. (b,C) JaCobian regularization aloneoutperforms an adversarial training defense (base models all inClude L2 and dropout regularization).
Table 3: Generalization on clean test data. DDNet models learned with varying amounts of trainingsamples per class are evaluated on CIFAR-10 test set. Jacobian regularizer substantially reduces thenorm of the Jacobian. Errors indicate 95% confidence intervals over 5 distinct runs for full trainingand 15 for sub-sample training.
