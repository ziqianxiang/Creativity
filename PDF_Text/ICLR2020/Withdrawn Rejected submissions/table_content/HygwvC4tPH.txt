Table 1: RELIC achieves comparable results to best performing dedicated entity-linking systems de-spite using no external resources or task specific features. When given a standard CoNLL-Aida aliastable and tuned on the CoNLL-Aida training set, RELIC’s learned representations match the state-of-the-art DeepType system which relies on the large hand engineered Wikidata knowledge base.
Table 2: Performance on FIGMENT. We report P@1 (proportion of entities whose top ranked typesare correct), Micro F1 aggregated over all (entity, type) compatibility decisions, and overall accuracyof entity labeling decisions. RELIC outperforms prior work, even with only 5% of the training data.
Table 3: Mean Average Precision on TypeNet tasks. RELIC’s gains are particularly striking in thelow data setting from Murty et al. (2018).
Table 4: Mean average precision on exemplar-based category completion (Section 5.4). The Yamadasubset is filtered to only contain entities that are covered by Yamada et al. 2017, and categories arefiltered to those which contain at least 300 entities (131 categories). For the ”All Entities” setting,we use all Wikipedia entities covered by RELIC, and filter to categories which contain at least 1000entities (1083 categories). The embeddings learned by Yamada et al. 2017 are competitive withRELIC on the task of populating TypeNet categories, but they are much worse at capturing thecomplex, and compound, typing information present in Wikipedia categories.
Table 5: Answer exact match on TriviaQA. RELIC’s fast nearest neighbor search over entitiesachieves 80% of the performance of ORQA, which runs a BERT-based reading comprehesion modelover multiple retrieved evidence passages. Unlike ORQA and RELIC, the classifier baseline andSLQA have access to a single evidence document that is known to contain the answer. As a resultthey are solving a much easier task.
