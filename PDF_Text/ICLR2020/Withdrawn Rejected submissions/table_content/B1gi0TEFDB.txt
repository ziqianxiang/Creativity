Table 1: Experimental settings. All models are trained by SGD with a 0.9 momentum. “BS” is themini-batch size at each worker. “LR” is the initial learning rate which is decayed during training.
Table 2: Wall-clock time of end-to-end training with ImageNet on 16 Tesla V100 GPUs. The batchsize for each GPU is 128, and the input image resolution is 224×224. Scaling efficiency is definedby ιTτ6-, where Ti is the throughput of single GPU training, and Ti6 is the overall system throughputof distributed training on 16 GPUs with Weak-Scaling.
