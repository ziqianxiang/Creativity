Table 1: Validation set reconstruction quality, as measured by PSNR (higher is better) andLPIPS (Zhang et al., 2018) (lower is better), for various datasets. We compare between retrain-ing BicycleGAN (Zhu et al., 2017b) authorsâ€™ released code (Bicycle v0), our implementation of thetwo baselines (BicycleGAN v1 and BicycleGAN v2) described in Section 4, and our approach bothbefore finetuning (ours - stage 2), and after finetuning (ours - stage 3).
Table 2: Generalization of a pretrained style en-coder E. We report validation set reconstructionfor the edges2handbags and night2day datasetswhen pretraining with different datasets. Stages2, 3 show results before/after finetuning E respec-tively.
Table 3: Diversity score is the average LPIPSdistance (Zhang et al., 2018). User preferencescore is the percentage a method is preferredover Ours v4, on the edges2shoes dataset.
Table 4: Ablation study on the effect of different components and loss terms using the edges2handbagsdataset. We study direct and cyclic reconstructions on ground truth images (dir_recon, cyc_recon),discriminator loss on direct reconstructions and on generated images with a randomly sampled style(D_dir, D_rand_z), latent reconstruction (z_recon), L2 and KL regularization on the latent vector z(z_L2, z_KL), and finally the use of VAE vs. just an auto-encoder.
Table 5: Inception score comparison (higher is better) for different datasets.
Table 6: Training time (in seconds) per 1000 images for thebaselines, as well as different versions of our approach (definedin Table 4).
