Table 1: The characteristics of different chains generated by different models under different super-vision on the HotpotQA dev set: for different models and chain oracles, we report the average chainlength, fraction of chains containing the answer, F1 with respect to the annotated supporting facts,and F1 on the final QA task. Here we only pick the chain in the first beam.
Table 2: The blind test set performance achieved by our model on WikiHop and HotpotQA. OnHotpotQA, all published works except DecompRC use the annotated supporting facts as extra super-vision, which makes them not directly comparable to our model. However, our model still achievesstrong performance on this dataset despite not using this annotation.
Table 3: The downstream QA performance of the chains generated by different models on differentdatasets. The performance is evaluated by accuracy and F1 score respectively in WikiHop andHotpotQA dataset.
Table 4: The human evaluation on different evidence sets. For each row, 50 responses are bucketedbased on the Turkersâ€™ confidence ratings, and numbers denote the answer F1 within that bucket.
