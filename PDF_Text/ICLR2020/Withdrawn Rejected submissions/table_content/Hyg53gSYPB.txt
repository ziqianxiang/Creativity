Table 1: Classification accuracies of classifier A using various defense strategies on the MNIST(top) and F-MNIST (bottom) dataset, under FGSM ( = 0.3), PGD ( = 0.3) and CW (with L2norm) white-box attacks, gray-box attacks, and FGSM black-box attacks. Here the correspondingiterations of GD minimization are: 15 for our method, 60 for BiGAN, and 200 for Defense-GAN.
Table 2: Classification accuracy of classifier A using different defense strategy on test set of M-NIST (left) and F-MNIST (right), under FGSM ( = 0.3) attack, with different GD iterationsL = 10, 60, 200 and various numbers of restart points R = 1, 10.
Table 3: Classification accuracies of classifier A using various defense strategies on the MNIST dev-set, under FGSM ( = 0.3), CW (with l2 norm) gray-box attack and Half-Masked (H.M.) attack.
Table 4: Training parameters for AE-GAN and MagNetParameters MNIST, F-MNIST CelebAEpochs	60	100Learning Rate	0.0002	0.0002Optimization Method	Adam	AdamBatch Size	100	6412Under review as a conference paper at ICLR 2020Table 5: Neural architectures USed for dassifiers and SUbStitUte modelsA	B	CConv(64, 5 X 5,1)	Conv(64, 8 × 8,2)	ReLU	ReLU	FC(200)Conv(64, 5 × 5, 2)	Conv(128, 6 × 6, 2)	ReLUReLU	ReLU	Dropout(0.5)Dropout(0.25)	Conv(128, 5 × 5, 1)	FC(200)FC(128)	ReLU	ReLUReLU	Drpupout(0.5)	Dropout(0.5)Drpupout(0.5)	FC(10)+Softmax	FC(10)+SoftmaxFC(10)+Softmax		Table 6: Neural architectures used for AE-GAN and MagNet
Table 5: Neural architectures USed for dassifiers and SUbStitUte modelsA	B	CConv(64, 5 X 5,1)	Conv(64, 8 × 8,2)	ReLU	ReLU	FC(200)Conv(64, 5 × 5, 2)	Conv(128, 6 × 6, 2)	ReLUReLU	ReLU	Dropout(0.5)Dropout(0.25)	Conv(128, 5 × 5, 1)	FC(200)FC(128)	ReLU	ReLUReLU	Drpupout(0.5)	Dropout(0.5)Drpupout(0.5)	FC(10)+Softmax	FC(10)+SoftmaxFC(10)+Softmax		Table 6: Neural architectures used for AE-GAN and MagNetEncoder	Decoder	DiscriminatorConv(1, 64, 5 × 5, 2) LeakyReLU(0.2) Conv(64, 128, 5 × 5, 2) BatchNorm & LeakyReLU Conv(128, 256, 5 × 5, 2) BatchNorm & LeakyReLU FC(128)+tanh	FC(1024) & ReLU ConvT(256,128, 5 × 5, 2) BatchNorm & ReLU ConvT(128, 64, 5 × 5, 2) BatchNorm & ReLU ConvT(64, 1, 5 × 5, 2) Sigmoid	Conv(1, 64, 5 × 5, 2) LeakyReLU(0.2) Conv(64, 128, 5 × 5, 2) BatchNorm & LeakyReLU Conv(128, 256, 5 × 5, 2) BatchNorm & LeakyReLU FC(1)+SigmoidTable 7: Training parameters for classifiersParameters	MNIST, F-MNISTEpochs	10Learning Rate	0.001Optimization Method	AdamBatch Size	100
Table 6: Neural architectures used for AE-GAN and MagNetEncoder	Decoder	DiscriminatorConv(1, 64, 5 × 5, 2) LeakyReLU(0.2) Conv(64, 128, 5 × 5, 2) BatchNorm & LeakyReLU Conv(128, 256, 5 × 5, 2) BatchNorm & LeakyReLU FC(128)+tanh	FC(1024) & ReLU ConvT(256,128, 5 × 5, 2) BatchNorm & ReLU ConvT(128, 64, 5 × 5, 2) BatchNorm & ReLU ConvT(64, 1, 5 × 5, 2) Sigmoid	Conv(1, 64, 5 × 5, 2) LeakyReLU(0.2) Conv(64, 128, 5 × 5, 2) BatchNorm & LeakyReLU Conv(128, 256, 5 × 5, 2) BatchNorm & LeakyReLU FC(1)+SigmoidTable 7: Training parameters for classifiersParameters	MNIST, F-MNISTEpochs	10Learning Rate	0.001Optimization Method	AdamBatch Size	100C Detailed description of the training and inference stage.
Table 7: Training parameters for classifiersParameters	MNIST, F-MNISTEpochs	10Learning Rate	0.001Optimization Method	AdamBatch Size	100C Detailed description of the training and inference stage.
Table 8: Classification accuracy of A using MagNet and AE-GAN on MNIST with various latentspace dimensions. D: the dimension of latent space (the number of iterations is set to 15).
Table 9: Classification accuracies of classifier A using various defense strategies on CelebA dataset,under FGSM ( = 0.1), PGD ( = 0.1) and CW (with L2 norm) white-box, gray-box, and FGSMblack-box. Here the corresponding iterations of GD minimization are: 20 for our method, 200 forDefense-GAN. (Adv.Tr.: adversarial training with PGD ( = 0.1))Attack	Method	None	Our	Adv. Tr.	MagNet	Defense-Gan	FGSM	0.0416	0.9364	0.6119	0.0750	0.8879White	PGD	0.0404	0.9327	0.6119	0.0510	0.8942	CW	0.0404	0.9433	0.6119	0.0493	0.861	FGSM	0.0416	0.8872	-	0.8897	0.8559Gray	PGD	0.0404	0.9005	-	0.9065	0.8621	CW	0.0404	0.9233	-	0.9273	0.867Black	A/B	0.8325	0.8839	0.6121	0.8743	0.8517H Qualitative examples16Under review as a conference paper at ICLR 2020夕〃 △ , S夕心Vo(a)7 7 7 〃63 /7，6\9 夕夕 ∖s6)77∕r3尹夕“ 7^F>∙η
