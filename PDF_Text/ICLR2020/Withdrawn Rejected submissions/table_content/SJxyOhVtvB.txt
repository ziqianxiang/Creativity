Table 1: Validation accuracy on CIFAR10 with uniform noise. |Dp | denotes the number of trusted(probe) data used. 0.1k indicates 10 images per class. For reference, standard training of WRN-28-10∕ResNet29 (RN29) leads to 96.1%∕92.7% accuracy. * indicates results trained by us.
Table 2: Validation accuracy on CIFAR100 with uniform noise. Standard training of WRN-28-10/RN29 leads to 81.6%/71.3% accuracy. 0.1k indicates 1 images per class.
Table 4: Semantic noisy experiments whereTable 3: Asymmetric noise on CIFAR10. LC is a loss labels are generated by a neural network oncorrection approach (Patrini et al., 2017). 10 trusted a few data. Noise ratio is shown in paren-data per class are used as probe data.	theses. RoG uses DenseNet-100.
Table 3: Asymmetric noise on CIFAR10. LC is a loss labels are generated by a neural network oncorrection approach (Patrini et al., 2017). 10 trusted a few data. Noise ratio is shown in paren-data per class are used as probe data.	theses. RoG uses DenseNet-100.
Table 5: Large-scale WebVision experiments. The top-1/top-5 accuracy on the ImageNet validationset are compared. IEG uses ResNet-50. The full version does not use AA.
Table 6: Comparison with semi-supervised methods. MixMatch and EG use WRN-28-2. EG* andIEG use WRN-28-10. 10 labeled data per class are used. The same size is used for probe data inIEG. The results of IEG are reported under the 80% uniform noise ratio.
Table 7: Ablation study on CIFAR100. X/Xindicates the corresponding component is en-abled/disabled. So IEG-1 is equal to L2R; IEG-5is the full IEG. Abbreviations are defined in text.
Table A1: Open-set noise on CIFAR10. We follow the setting and created noisy datasets of RoG toconduct experiments. Each column indicates where the noisy out-of-distribution images are from.
Table A2: Accuracy (mean and std) of IEG on CIFAR100 with different uniform noise ratios.
