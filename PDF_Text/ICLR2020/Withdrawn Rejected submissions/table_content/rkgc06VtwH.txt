Table 1: One example from each dataset that we use in our experiments. Input utterances and thecorresponding logical forms are denoted by x and y respectively.
Table 2: Top-1 (greedy decoding) vs top-10 (25) oracle for the best performing sequence-to-sequence models on three semantic parsing datasets.
Table 3: Test accuracy for all models on OVERNIGHT dataset, which has eight domains: Basketball,Blocks, Calendar, Housing, Publications, Recipes, Restaurants, and Social. We use the generator-reranker (G-R) architecture with different options. Beam-n: Beam search is applied with size n, pQ:The critic is pre-trained over the Quora dataset, TH1: rerank if there is at least one score above 0.5,TH2: rerank if best score - second best score > 0.001. The candidate logical forms are processedwith templated expansions method (Section 3.2.3) in this experiment.
Table 4: Test accuracy for all models on GEO and ATIS datasets. The settings follow the same asTable 3 and we denote TH3 as the threshold rule of applying both TH1 and TH2, i.e. rerankingwhen there is at least one score above 0.5 and if best score - second best score > 0.001.
