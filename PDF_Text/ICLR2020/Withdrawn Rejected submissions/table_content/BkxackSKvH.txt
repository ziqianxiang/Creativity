Table 1:	Validation and	test accuracies (for	Table 2: Ablation results for the scoresMNLI, on matched/mismatched sets), and num-			(C)ontradiction, (E)ntailment, (N)eutral,ber of parameters in the encoder (#enc) and MLP			and (S)imilarity, on SNLI and MNLIand/or classifier (#mlp).			(matchedmismatched) Validation sets.
Table 3: Validation accuracies(for MNLI, on matched/mismatched sets), and number of parametersin the encoder (#enc) and MLP and/or classifier (#mlp).
Table 4: Accuracy results of models transferring to new datasets. All models are trained on MNLIand tested on target test sets. ∆ are absolute differences between our method and the baseline.
Table 5: Scores on supervised downstream tasks from SentEval toolkit attained by our model andthe baseline.
Table 6: Scores on unsupervised downstream tasks from SentEval toolkit attained by our model andthe baseline. Numbers reported are Pearson correlations ×100.
Table 7: Scores on the probing tasks attained by our model and the baseline.
