Table 1: Ablations on Something-Something V1 action classification.
Table 2: Comparisons with state-of-the-art results on Something-Something V1 dataset.
Table 3: Validation results on Kinetics-400 datasetMethod	Backbone	Top-1	Top-5ARTNet (Wang et al., 2018b)	-ResNet 18-	69.2	88.3I3D (Carreira & Zisserman, 2017)	BN-Inception	71.1	89.32-stream I3D (Carreira & Zisserman, 2017)	BN-Inception	74.2	91.32-stream R(2+1)D (Tran et al., 2018)	ResNet 50	73.9	90.9NL I3D (Wang et al., 2018c)	ResNet 50	76.5	92.6NL I3D (Wang & Gupta, 2018)	ResNet 101	77.7	93.3SlowFast (C. Feichtenhofer & He, 2018)	ResNet 50	77.0	92.6NL SloWFast(C Feichtenhofer & He, 2018)	ResNet 50	77.7	93.1HO I3D [ours]	ReSNet 50ä¸€	77.8	93.34.4 Experiments on other video datasetsIn this subsection we study the performance of higher-order neural networks on Charades dataset.
Table 4: Validation results on Something	Table 5: Validation results on the Charades datasetSomething V2 Dataset	_____________________________________	 model			mAPMethod	Top-1	I3D (Wang et al., 2018c)	31.8Multi-Scale TRN (Zhou, 2018)	48.8	NL I3D (Wang et al., 2018c)	33.52-Stream TRN (Zhou, 2018)	55.5	GCN (Wang & Gupta, 2018)	36.2HO I3D [ours]	62.6	NL I3D + GCN (Wang & Gupta, 2018)	37.5HO I3D [ours]				37.18Under review as a conference paper at ICLR 20205 ConclusionIn this paper, we have introduced higher-order networks to the task of action recognition. Higher-order networks are constructed by a general building block, termed as H-block, which aims to modelposition-varying contextual information. As demonstrated on the Something-Something (V1 andV2), Kinetics-400 and Charades datasets, the proposed higher-order networks are able to achievestate-of-the-art results, even using only RGB mobility inputs without fine-tuning with other imageor video datasets. The good performance may be ascribed to the fact that higher-order networks area natural for context modeling.
Table 6: Factorization of different context fields.
Table 7: Our backbone ResNet-50 I3D model.
