Table 1: The reward and success rate of learned 10 policies using different methodsMethod	Reward			Success Rate			Hopper	Walker2d	HalfCheetah	Hopper	Walker2d	HalfCheetahPPO	839 ± 753	1611 ± 467	913 ± 134	1.0	1.0	0.7PPO+WSR	1083 ± 768	1429 ± 692	603 ± 407	1.0	0.7	0.4PPO+TNB	1064 ± 644	1160 ± 484	592 ± 384	1.0	0.9	0.5PPO+Ours	1858 ± 744	1506 ± 541	1442 ± 588	1.0	1.0	0.9sequentially. Concretely, the 1st policy is trained by ordinary PPO without any social influence. The2nd policy should be different from 1st policy, and the 3rd should be different from the previoustwo policies, and so on. Fig.2 shows the qualitative results of our method. We visualize the motionof agents by drawing multiple frames representing the pose of agents at different time steps in thesame row. The horizontal interval between consecutive frames is proportional to the velocity ofagents. The settings of the frequency of highlighted frames and the correlation between interval andvelocity are fixed for each environment. The visualization starts from the beginning of each episodeand therefore the readers can get sense of the process of acceleration as well as the pattern of motionof agents clearly.
Table 2: The success rate of learned 10 novel policies by our method in different environments underdifferent thresholdsEnvironment	Threshold	10 hidden	64 hidden	256 hidden	512 hidden	0.5	0.7	0.8	0.7	-Hopper	0.6	1.0	0.7	1.0	-	0.7	0.9	0.4	0.7	-	0.8	0.5	0.5	0.3	-	0.9	0.3	0.1	0.4	-	1.0	0.2	0.8	1.0	-Walker2d	1.1 1.2	0.3 0.1	0.8 0.9	1.0 1.0	-	1.3	0.2	0.8	0.8	-	1.4	0.1	0.6	1.0	-	1.1	-	0.3	0.9	0.8HalfCheetah	1.2	-	0.8	0.8	0.7	1.3	-	1.0	1.0	1.0	1.4	-	0.4	0.9	1.0	1.5	-	0.2	0.2	0.7Training Timesteps We fix the training timesteps in our experiments. The timesteps are fixed tobe 1M in Hopper-v3, 1.6M for Walker2d-v3 and 3M for HalfCheetah.
Table 3: The final training performance of learned 10 novel policies by our method in differentenvironments under different thresholds, for Hopper and Walker, h1 = 10, h2 = 64, h3 = 256; forHalfCheetah, h1 = 64, h2 = 256, h3 = 512Environment	Threshold		Average Performance		Top 30% Performance		hid num	h1		h2	h3	h1	h2	h3	0	450 ± 135	914± 735	704 ± 598	607 ± 30	1665 ± 726	1202 ± 737	0.6	1858 ± 744	1040 ± 914	1626 ± 869	2719 ± 108	2263 ± 764	2628 ± 207Hopper	0.7	1180 ± 740	593 ± 159	785 ± 462	2188 ± 510	769 ± 17	1287 ± 590	0.8	397 ± 283	767 ± 743	950 ± 929	744 ± 152	1594 ± 875	2129 ± 896	0.9	154 ± 142	235 ± 279	604 ± 554	335 ± 122	645 ± 113	1347 ± 387	1.0	187 ± 229	298 ± 256	499 ± 754	490 ± 194	648 ± 141	1294 ± 979	0	1611 ± 467	1504 ± 502	1724 ± 584	2163 ± 48	2018 ± 54	2311 ± 45	1.0	725 ± 487	1174 ± 599	1571 ± 692	1346 ± 468	2042 ± 77	2270 ± 37Walker	1.1	725 ± 654	1506 ± 541	1453 ± 480	1561 ± 598	2079 ± 44	1903 ± 104	1.2	487 ± 375	1061 ± 346	1211 ± 657	880 ± 451	1552 ± 114	2114 ± 47	1.3	405 ± 647	1138 ± 591	995 ± 420	1124 ± 795	1984 ± 197	1523 ± 313	1.4	393 ± 518	831 ± 400	1333 ± 558	945 ± 652	1352 ± 273	2004 ± 106	0	1210 ± 391	1278 ± 373	1235 ± 317	1655 ± 296	1728 ± 179	1601 ± 241	1.1	434 ± 415	1055 ± 265	914 ± 427	967 ± 365	1275 ± 42	1330 ± 297HalfCheetah	1.2	1167 ± 491	988 ± 446	948 ± 476	1679 ± 241	1441 ± 254	1466 ± 265
