Table 1: Results on image generation on ImageNet 64 Ã— 64 in bits/dim.
Table 2: Results on language modeling on Wikitext-103 data-set. Local Transformer refers toTransformer (Vaswani et al., 2017) with relative position encoding (Shaw et al., 2018) together withlocal attention. Perplexity is reported on the test set.
Table 3: Results on language modeling on enwik-8 data-set. Local Transformer refers to Trans-former (Vaswani et al., 2017) with relative position encoding (Shaw et al., 2018) together with localattention. Bits per byte (bpc) is reported on the test set.
Table 4: Jensen-Shannon divergence between the attention distributions of a random local attentionhead and a random head that routes attention as in Section 3 averaged across all layers on theWikitext-103 data-set. We report means and standard deviations computed over 10 runs.
Table 5: Jensen-Shannon divergence between the attention distributions of a random local attentionhead and a random head that routes attention as in Section 3 per layer on the Wikitext-103 data-set. We report means and standard deviations computed over 10 runs and use the natural logarithm sothat divergences are upper-bounded by 0.6931.
Table 6:	Example unconditional character level text generation from the Routing Transformer modeltrained on enwik-8 with random sampling.
Table 7:	Example unConditional CharaCter level text generation from the LoCal Transformer modeltrained on enwik-8 with random samPling.
