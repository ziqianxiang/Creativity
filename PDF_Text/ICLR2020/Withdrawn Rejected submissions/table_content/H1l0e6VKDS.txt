Table 1:  Transfer learning performance with ImageNet source dataset, compared to MixDCNN(Wang et al., 2015), EMD (Cui et al., 2018), OPAM (Peng et al., 2017) and DATL (Ngiam et al.,2018) (∗ denotes the results fine-tuning baseline from (Ngiam et al., 2018)).
Table 2: Results on the test set for Birds using images from ANON.
Table 3: Top chosen classes from ImageNet source dataset that are related to the target datasets. ForBirds, the top source class is “bea eater” which is one of the bird species in ImageNet. The secondtop “aepyceros melampus” is an antelope that has narrow mouth, which is similar to some birds withsharp spout. The “valley” also matches the background in some images. For Cars, we interestinglyobserve the high-weight class “barrel, cask”, which indeed include wheels and car-looking bodytypes in many images. “Terrapin” is a reptile that crawls on the ground with four legs, whose shapelooks like vehicles in some way. For Food, the high-weight classes are the least relevant intuitivelybut still contain classes with visually-relevant patterns – e.g., “caldron, cauldron” may contain imageswith food inside, but most “seashore, coast” are less related to food.
Table 4: Results on the test set for DTD on split 1.
Table 5: AUC comparisons (based on the same evaluation protocol as in (Irvin et al., 2019)) on theCheXpert dataset. * denotes the results reported in (Irvin et al., 2019).
Table 6: Computational cost of training using Inception-V3 on Cloud TPU v2 when the source datasetis ImageNet. The last column of total time assumes availability of a pre-trained source model.
Table 7: Details for five fine-grained datasets: Birdsnap (Birds) (Berg et al., 2014), Oxford-IIIT Pets(Pets) (Parkhi et al., 2012), Stanford Cars (Cars) (Krause et al.), FGVC Aircraft (Aircraft) (Maji et al.,2013), and Food-101 (Food) (Bossard et al., 2014).
