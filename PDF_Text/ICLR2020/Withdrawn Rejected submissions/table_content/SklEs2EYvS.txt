Table 1: Ablation studies for messages and shared rewardsTable 1 shows how shared messages and rewards lead agents to cooperate with others. The modelwith only messages is made by removing the second term of equation 10. Meanwhile, the model thatonly shares rewards is by setting messages as zero. When agents aren’t allowed to share messagesand rewards, SMAL degenerates to a self-agent reinforcement learning method. The results demon-strate that the transmission of signals and rewards are both necessary for agents to achieve thoroughconsociation. These results further show that if rewards of other agents are taken into consideration,policy gradient methods can learn suboptimal policies for overall returns.
Table 2: Ablation study of graphMethod	Total ReWard(λ = 0)		Environment	Waterworld	Multi-walker	Multi-antGraph	812.45	70:68	126.11Mean	636.92	5934	115.70Quantitive improvements contributed to the graphare shown in table 2. In the second experiment, allthe weights in the graph are set as one. Agents di-rectly receive the messages from other agents andtake their mean vector as the input message. Withthe graph to model the proportion of messages andrewards, the agents choose their actions more in-telligently.
