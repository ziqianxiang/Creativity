Table 1: Scaling of common layersMethod	Maintains Scaling	NotesLinear layer	X	Will not be well-conditioned against other layers unless geometric initialization is used(Strided) convolution	X	As above, but only if all kernel sizes are the sameSkip connections	X	Operations in residual blocks will be scaled correctly against each other, but not against non-residual operationsAverage pooling	X	Max pooling	X	Dropout	X	ReLU/LeakyReLU	X	Any positively-homogenous function with degree 1Sigmoid	X	Tanhh	X	Maintains scaling if entirely within the linear regimecentered. Then the scaling factors for the weight and bias layers are:_ in 7 2 2 77 Γ 2^∣ 2 EDy2]	_ 2 E@y2]Yo = no koρ0E [xo]币7,	γ0b = Po -Ey『 ∙We can cancel terms to find the value of E x20 that makes these two quantities equal:E曷]=」∙PnonkoIn common computer vision architectures, the input planes are the 3 color channels, and the kernelsize is k = 3, giving E x2o ≈ 0∙2. Using the traditional variance-one normalization will result inthe effective learning rate for the bias terms being lower than that of the weight terms. This will result
Table 2: Comparison on 26 LIBSVM repository datasetsMethod		Average Normalized loss (±0.01)	Worst in #	Best in #Arithmetic mean	0.90	14	=	3	=Fan-in	084	3	5Fan-out	0.88	9	12Geometric mean	0.81	0	-	6	—and dimensionality; this follows from Equation 1. For instance, adding a simple scaling layer of theform xl+1 = 2xl doubles the (non-central) second moment during the forward pass and doubles thebackward (non-central) second moment during back-propagation, which maintains this product:EAx2+1]E[x2+1] = 2EAx2] ∙ 2E[x2].
