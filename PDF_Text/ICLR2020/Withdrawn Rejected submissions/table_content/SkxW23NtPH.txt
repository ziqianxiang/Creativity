Table 1: Run time comparison between GDP-one, human expert, Tensorflow METIS, and hierarchi-cal device placement (HDP) on six graphs (RNNLM, GNMT, Transformer-XL, Inception, Amoe-baNet, and WaveNet). Graph runtime speed up is compared with Human Placement (HP) and Hi-erarchical Device Placement (HDP). Search speed up is the policy network training time speed upcompared to HDP (reported values are averages of six runs).
Table 2: Run time comparison on GDP-batch vs. GDP-one.
Table 3: Hyperparameters for Policy Network. gs.layers: GraPhSAGE layers, gs-knn: Graph-SAGE maximum neighbors, trf_d_model: Dimension of the TransformerXL model, trf _n_head:Number of attention heads, trf Jayers: Number of TransformerXL layers, trf_d-heads: Dimen-sion of each attention head, trf-dinner: Dimension of inner hidden size in positionwise feed-forward.
Table 4: Hyperparameters for PPO.
Table 5: Run time comparison on GDP batch training vs. the best of related methods (human expert,METIS, HDP, and GDP no batch training).
