Table 1: Human judgements of whether AM images look like familiar objects in layers conv5, fc6,and fc8 in AlexNet.
Table A1: The units with the highest CCMAS and precision scores in AlexNet. Unit fc6.1199 wasdisplayed in Fig. 3.
Table A2: The correlations between the different measures. (All p’s < .001)C Further issues with the CCMAS measureThe CCMAS measure is based on comparing the mean activation of a category with the meanactivation for all other items, and this is problematic for a few reasons. First, in many units a largeproportion of images do not activate a unit at all. For instance, our butterfly ‘detector’ unit fc6.1199has a high proportion of images with an activation of 0.0 (see figure 3). Indeed, the inset on themiddle figure shows that the distribution can be better described by exponential-derived fits ratherthan a Gaussian. This means that the CCMAS selectivity is heavily influenced by the the proportionof images that have an activation value of zero (or close to zero). This can lead to very differentestimates of selectivity for CCMAS and precision or localist selectivity, which are driven by the mosthighly activated items. In A5 we generate example data to highlight ways in which CCMAS scoremay be non-intuitive. In subplot (a) we demonstrate that a unit can have a CCMAS score of of 1.0despite only a single item activating the unit. The point that we wish to emphasise is that a highCCMAS score does not necessarily imply selectivity for a given class, but might in fact relate toselectivity for a small subset of items from a given class, and this is especially true when a unit’s15Under review as a conference paper at ICLR 2020activation is sparse (many items do not activate the unit). However, the reverse can also be true. Insubplot (c) we demonstrate that a unit can have a very low CCMAS score of .06 despite all of themost active items being from the same class.
Table A3: Selectivity measures for VGG-16, trained on Places-365, top convolutional layer unitsidentified by Zhou et al. (2018a) as object detectors. Standard errors not shown for space, but werebelow ±5. The IoU is from Zhou et al. (2018a)’s network dissection method. no.ax>0 and no.ax>0x ∈ A refer to the proportion of activations that were greater than zero for busses and non-bussesrespectively. μA and μ-A are the class means for busses and non busses respectively.
Table A4: Selectivity measures for GoogLeNet, trained on Places-365, layer inception4e unitsidentified by Zhou et al. (2018a) as object detectors. Standard errors not shown for space, but werebelow ±2. The IoU is from Zhou et al. (2018a)’s network dissection method. A units is marked ascorrect if there was a single bus in the 4 example pictures on the website (http://netdissect.
Table A5: Selectivity measures for GoogLeNet, trained on ImageNet, layer inception4e units unitsidentified by Zhou et al. (2018a) as object detectors. Standard errors not shown for space, but werebelow ±2. A units is marked as correct if there was a single bus in the 4 example pictures on the web-site (http://netdissect.csail.mit.edu/dissect/googlenet_imagenet/), andfalse if not. This might suggest that the units were responding to ’bus like’ features in none busobjects.
