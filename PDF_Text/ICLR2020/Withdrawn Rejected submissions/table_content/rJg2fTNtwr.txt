Table 1: Samples of a MLE-trained STOA transformer LM when fed with different types of length-10 history prefix. To save space, we omitted the first 7 words of the random history.
Table 2:  An illustration for the next word collection process.  The choices are shuffled.  The firsthistory sample is from real data, and the second history sample is from the trained model.
Table 3: EB-C measurements with human as PD.
Table 4: A standard NMT transformer model fed with different types of length-3 history prefix. Wedid not do any cherry picking.  The “@@” is because BPE tokenization is used.  “DATA” meansthe first three output tokens are forced to be correct. “NORMAL” means no prefix is forced duringdecoding.  “UNREL” means the first three tokens are forced to be from another random unrelatedsentence (which is wrong but grammatical).  “RAND” means the first three tokens are completelyrandom words.
Table 5: PPL results for model trained on EMNLP-news data-set.
Table 6:  More samples of a STOA MLE-trained transformer LM (on the wiki-103 data-set) whenfed with different kinds of history. To save space, we omitted the first 7 words of the random history.
Table  7:  Samples  of  a  MLE-trained  LSTM  LM  (on  the  EMNLP-news  data-set)  when  fed  withdifferent kinds of history. To save space, we omitted the first 7 words of the random history.
