Table 1: Performance Comparison on Kaggle Credit Dataset. The table presents AUROC of the classifiertrained on synthetic data and tested on real data. The evaluation results for PATE-GAN and DP-GAN arerecorded in Yoon et al. (2019). We evaluate G-PATE under the same experimental setup. PATE-GAN, DP-GAN,and G-PATE all satisfy (1, 10-5)-differential privacy. The best results out of different models are bolded.
Table 2: Performance Comparison on Image Datasets. We compare G-PATE with DP-GAN and GAN onMNIST and Fashion-MNIST datasets. The table presents the 10-class classification accuracy of a model trainedon synthetic data and tested on real data. DP-GAN and G-PATE are both evaluated under two private settings:ε = 1, δ = 10-5 and ε = 10, δ = 10-5 .
Table 3: Analysis on the Number of Teachers and the Projection Dimensions. We performedcomprehensive studies on the number of teachers and the the projection dimensions on MNIST withε = 1 and δ = 10-5. The model has the best performance with 4000 teacher models and projectiondimension equals to 10.
Table 4: AUPRC on Kaggle Credit Dataset. The table presents AUPRC of classification modelstrained on synthetic data and tested on real data. PATE-GAN, DP-GAN, and G-PATE all satisfy(1, 10-5)-differential privacy. The best results among different DP generative models are bolded.
Table 5: Performance of Classification Models Trained on Real Data. The table presents AUROCand AUPRC of classification models trained and tested on real data. These results are the upper-bounds for evaluation results on Kaggle Credit dataset.
Table 6: Performance Comparison between GAN and nonprivate GPATE on Kaggle Credit Dataset.
