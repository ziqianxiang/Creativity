Table 1: Comparison between our work and related workCapability	Transferability	Out-scope target	Dynamic target	Large DatasetDumford & Scheirer (2018)	×	X	×	×Liu et al. (2017)	×	×	×	×Liao et al. (2018)	×	×	×	×Gu et al. (2017)	X	×	×	×Ours	X	X	X	Xwill adopt the pre-trained NN models directly, which is termed outsourced training attack. However,this situation rarely actually occurs. In practice, users typically fine-tune the FC layers of the pre-trained models to adapt to their working scenarios, which makes the attack more challenge; it istermed as transfer learning attack. Although the most related work, BadNet (Gu et al., 2017), hasimplemented a transfer learning attack, the triggers in their work are based on handcrafted patterns,which are statistically fixed. Therefore, their triggers can only support fixed target classes that areincluded in the pre-trained models. It cannot be applied to the scenario we demonstrate in this paper.
Table 2: The accuracy of clean and trojaned model and the attack success rate on ImageNet models.
Table 3: Transfer learning attack results. We use the same trojaned VGG16 model to test the transferattack success rate on two smaller dataset, Flower and Caltech-256. The trojaned model is made withonly ImageNet dataset. Smaller datasets are only used to train victim’s FC layers.
