Table 1: Test loss for standard vs uniform break-2point initialization, on sine and quadratic 号a potentially useful data-dependent initialization strategy, one that can scale to high dimensions, butwe leave this for future work.
Table 2: Comparison of the number of pieces induced in a network of up to depth 5, with 40 unitsevenly distributed across layers, trained to fit varying target functions.
Table 3: Comparison of testing loss (generalization ability) of various network shallow and deepnetworks with a standard vs ‘spiky’ initializationof measuring parameterization. Intriguingly, variability in the number of pieces appears to increasewith depth. From the functional approximation, we know that a unit induces breakpoints only whenthe ReLU function applied to the unit’s input has zero crossings. In layer one, this happens exactlyonce per unit as the input to each ReLU is just a line over the input space. In deeper layers, thefunction approximation is learned, allowing for a varying number of new breakpoints. Given ourprevious results on the flatness of the standard initializations, this will generally only happen onceper unit, implying that the number of pieces will strongly correlate with the number of units atinitialization.
Table 4: Top: Correlation of the BP distribution before and after training for depth 1 and 4 networksacross function classes. Bottom : Change in correlation over trainingover training - in the shallow case, they are more constrained by the initial condition, and continueto have a higher density near the origin even when not necessary or appropriate.
