Table 1: Comparison with previous worksabout layer growth.
Table 2: NetWork MorPhiSm tested on CIFAR10.
Table 3: Ablation study of c-AutoGrow.
Table 4: p-AutoGrow with different seed archi-tecture.
Table 5: p-AutoGrow with different growing interval K .
Table 6: The adaptability of AutoGrow to datasetsnet	dataset	found net	accu %	△*	net	dataset	found net	accu %	△*	CIFAR10	42-42-42	94.27	-0.03		CIFAR10	23-22-22	90.82	6.49	CIFAR100	54-53-53	74.72	-0.95		CIFAR100	28-28-27	66.34	31.53Basic3ResNet	SVHN	34-34-34	97.22	0.04	Plain3Net	SVHN	36-35-35	96.79	77.20	FashionMNIST	30-29-29	94.57	-0.06		FashionMNIST	17-17-17	94.49	0.56	MNIST	33-33-33	99.64	-0.03		MNIST	20-20-20	99.66	0.12	CIFAR10	22-22-22-22	95.49	-0.10		CIFAR10	17-17-17-17	94.20	5.72	CIFAR100	17-51-16-16	79.47	1.22		CIFAR100	16-15-15-15	73.91	29.34Basic4ResNet	SVHN	20-20-19-19	97.32	-0.08	Plain4Net	SVHN	12-12-12-11	97.08	0.32	FashionMNIST	27-27-27-26	94.62	-0.17		FashionMNIST	13-13-13-13	94.47	0.72	MNIST	11-10-10-10	99.66	0.01		MNIST	13-12-12-12	99.57	0.03* △ = (accuracy of AutoGrow) — (accuracy of training from scratch)1.	In Table 6, AutoGrow discovers layer depth across all scenarios without any tuning, achiev-ing the main goal of this work. Manual design needs m ∙ n ∙ k trials, where m and n arerespectively the numbers of datasets and sub-module categories, and k is the number oftrials per dataset per sub-module category;2.	For ResNets, a discovered depth (“ ” in Figure 3) falls at the location where accuracysaturates. This means AutoGrow discovers a near-optimal depth: a shallower depth willlose accuracy while a deeper one gains little. The final accuracy of AutoGrow is as good
Table 7: Scaling up to ImageNetnet	K	found net	Top-1	Top-5	∣∆ Top-1Basic4ResNet	2	12-12-11-11	76.28	92.79	0.43	5	9-3-6-4	74.75	91.97	0.72Bottleneck4ResNet	2	6-6-6-17	77.99	93.91	0.83	5	6-7-3-9	77.33	93.65	0.83Plain4Net	2	6-6-6-6	71.22	90.08	0.70	5	5-5-5-4	70.54	89.76	0.93^ ∆ = (Top-1 of AutoGrow) — (Top-1 oftraining fromscratch)helps training deeper nets. The comparison of AutoGrow and manual depth design (He et al., 2016) isin Figure 4, which shows that AutoGrow achieves better trade-off between accuracy and computation(measured by floating point operations).
Table 8: p-AutoGrow under initializers with K = 3.
Table 9: AutoGrow improves accuracy of plain nets.
Table 10: The efficiency of AutoGrownet	GPUS	growing	fine-tuningBasic4ResNet-12-12-11-11	4 GTX 1080 Ti	56.7 hours	157.9 hoursBasic4ResNet-9-3-6-4	4 GTX 1080	47.9 hours	65.8 hoursBottleneck4ResNet-6-6-6-17	4 TITAN V	45.3 hours	114.0 hoursBottleneck4ResNet-6-7-3-9	4 TITAN V	61.6 hours	78.6 hoursPlain4Net-6-6-6-6	4 GTX 1080 Ti	11.7 hours	29.7 hoursPlain4Net-5-5-5-4	4 GTX 1080 Ti	25.6 hours	25.3 hoursTop-1 Accuracy %105030 6090 120 150 180sjəa-JO Mqlunu əm554433221Figure 8: The convergence curves and growing process on ImageNet forBasic4ResNet-9-3-6-4 and (b) Plain4Net-6-6-6-6 in Table 10.
Table 11: The adaptability of AutoGrow to dataset sizesBasic3ResNet on CIFAR10			Plαin3Net on MNIST		dataset size	found net	accu %	dataset size	found net	accu %100%	42-42-42	94.27	100%	20-20-20	99.6675%	32-31-31	93.54	75%	12-12-12	99.5450%	17-17-17	91.34	50%	12-11-11	99.4625%	21-12-7	88.18	25%	10-9-9	99.33Basic4ResNet on CIFAR100			Plain4Net on SVHN		dataset size	found net	accu %	dataset size	found net	accu %100%	17-51-16-16	79.47	100%	12-12-12-11	97.0875%	17-17-16-16	77.26	75%	9-9-9-9	96.7150%	12-12-12-11	72.91	50%	8-8-8-8	96.3725%	6-6-6-6	62.53	25%	5-5-5-5	95.6813Under review as a conference paper at ICLR 2020the same. As expected, our experiments show that AutoGrow adapts to shallower networks when thesizes are smaller.
