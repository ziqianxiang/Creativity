Table 1: Summary of related work. (J:joint generation, C: conditional generation, S: style control.)	C	J	SJVAE (Vedantam et al., 2018), JMVAE (Suzuki et al., 2016)	O	O	FCVAE (Sohn et al., 2015)	O	X	OVCCA-private (Wang et al., 2016)	O	O	OVIB (Alemi et al., 2017)	O	X	XWyner VAE	O	O	Ohaving the local variables and controlling the common regularization parameter λ > 0. On the otherextreme, Z may capture no information, while U and V capture all the information of X and Y,respectively. It may happen in Wyner VAE if the regularization parameter λ is too large. To avoid thedegeneracy, we need to choose a proper λ by cross-validation.
Table 2: Wyner model vs. the IB principle (Tishby et al., 1999).
Table 3: Best negative log-likelihood (nll) values during 500 epochs of training for MoG dataset.
Table 4: Best nll values during 1000 epochs of training for MNIST left-right prediction task. Theconditional nll value for CVAE is taken from Sohn et al. (2015).
Table 5: Accompanying table for Fig. 5: Summary of numerical evaluations of MNISi-MNISTadd-1 experiments. For each row, we trained 10 different models and dropped two outliers for eachaverage and standard deviation.
Table 6: Numerical evaluation of Wyner VAE and JVAE for CelebA dataset.
Table 7: Network architecture for MoG experiments.
Table 8: Network architecture for MNIST-MNIST and MNIST-SVHN experiments.
Table 9: Network architecture for MNIST quadrant prediction experiments.
Table 10: Network architecture for CelebA experiments.
