Table 1:	Distance D achieved by transformation TModel	D(T (s1)∣∣s2)	D(T (s2)∣∣s1)Linear Regression		0.055	0.078Attention (w/o annealing T =	1)	0.138	0.207Attention (with annealing β	0.001)	0.028	0.048Attention (with annealing β	0.01)	0.025	0.037During training, we alternately optimize the representation model for language 1 and 2, so that theydo not learn a trivial solution of producing the same constant representation. The learning curve isshown in Figure 1.
Table 2:	Key-point prediction accuracyModelAccuracy Precision	Recallen zh en zh en zhFinetune	0.859	0.823	0.886Confident labels 0.864	0.826 0.8750.854 0.936 0.8470.838 0.959	0.872Figure 2: An example of predicted key-points, with (below) and without (above) confident labelsReorder In absolute order prediction, we use an N-way classifier to predict the absolute positionof a token. In relative order prediction, a binary classifier is used to predict the relative order of apair of tokens. During inference, beam search is used to find the best order assignment of tokens, sothat the total cost is minimized. We simply use the probability p of the classifier output as the costof a particular assignment.
Table 3: Reorder performanceModel		BLEU (2-gram)		BLEU (3-gram)		BLEU (4-gram)			en	Zh	en	zh	en	zhAbsolute order prediction (n	= 1)	0.309	0.386	0.166	0.227	0.101	0.152Absolute order prediction (n	= 10)	0.331	0.393	0.189	0.239	0.121	0.165Relative order prediction (n =	1)	0.535	0.566	0.384	0.410	0.290	0.308Relative order prediction (n =	10)	0.582	0.624	0.419	0.468	0.316	0.368The example in Figure 3 illustrates that absolute order prediction suffers from ambiguity exceptnear the beginning of the sentence, because the exact position depends on the specific translationand therefore does not have a unique correct answer. Relative order prediction is much more robustbecause it is invariant to the location of the pair of tokens in the sentence, the classifier only makespredictions based on the relative order relation between them.
Table 4: Performance of zero-shot cross-lingual transfer. T, E, R stands for Transform, Extract, andReorder, respectively. pGA stands for pseudo-groundtruth alignment. Gap stands for the transfergap from English to Chinese (the drop of accuracy in percentage)Model	XNLI test set en Acc. zh Acc.		GapCLCR			T+pGA	81.2	74.8	6.4T	81.2	68.0	13.2T+E	80.6	69.3	11.3T+E+R	80.2	71.7	8.5Non-CLCR models			Multilingual BERT	81.4	63.8	17.6Translate train	81.4	74.2	7.2Translate test	81.4	70.1	11.3XLM	85.0	76.5	8.57Under review as a conference paper at ICLR 2020Although the performance of our CLCR on Chinese XNLI is inferior to XLM, the transfer gap isroughly the same. XLM uses much more data and training time than our approach, and thus startswith better performance than BERT-base on English. Also XLM is not language-independent, whichmeans itis not feasible to use XLM as representations to learn models for cross-lingual transfer. One
Table 6: Edge probing statistics	BERT-base			F1	T Pre.	Rec.	T+E (δ =		0.2) Rec.	T+E (δ =		0.5) Rec.
