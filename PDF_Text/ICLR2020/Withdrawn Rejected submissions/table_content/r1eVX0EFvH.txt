Table 1: Model accuracy with respect to the oracle human labelers on the subset of examples wherethe human-obtained oracle label is different from the test label. Models which are more robust toperturbation adversarial examples (such as those trained with adversarial training) agree with humansless often on invariance-based adversarial examples. Values denoted with an asterisks * violate theperturbation threat model of the defense and should not be taken to be attacks. When the model iswrong, it classified the input as the original test label, and not the new oracle label.
