Table 1: Attack ablation study on SVHN. Comparison of our L∞ PGD-Conf attack with = 0.03on the test set for different number of iterations T and configurations of momentum, backtrackingand initialization. As backtracking needs an additional forward pass per iteration, we compareT = 200 with backtracking to T = 300 without. Attacks on adversarial training (AT) succeedwithin a few iterations, but are more difficult against CCAT and require initialization at zero.
Table 2: Test errors on MNIST, SVHN and Cifar10. Err for τ = 0, i.e., standard evaluation, andτ @99%TPR, i.e., detection evaluation, comparing normal training, AT and CCAT. Especially onCifar10, AT increases Err significantly, in both settings. CCAT, in contrast, is able to preserve theErr of the normally trained model better.
Table 3: Main results on MNIST, SVHN and Cifar10. Comparison of AT and CCAT on MNIST(top), SVHN (middle) and Cifar10 (bottom). Per threat model, i.e., for L∞, L2, L1 and L0 at-tacks, we report worst-case results across all tested attacks; the used values are reported in thecorresponding columns. During training, L∞ attacks with = 0.3 on MNIST and = 0.03 onSVHN/Cifar10 were used; adversarial examples from the remaining threat models were not en-countered during training. We report ROC AUC as well as confidence-thresholded Err and RErr forτ @99%TPR. CCAT is competitive with AT on the L∞ attacks seen during training, but generalizessignificantly better to previously unseen attacks.
Table 4: Results for noise and distal adversarial examples on MNIST, SVHN and Cifar10.
Table 5: Results on Corrupted MNIST and Cifar10. Performance on corrupted examples (spatialtransformations, brightness/contrast changes, noise etc.). We report ROC AUC and confidence-thresholded Err on corrupted examples for τ @99%TPR. Compared to normal training and AT,CCAT detects better corrupted examples (higher ROC AUC) and its test error Err on non-rejectedexamples is lower.
Table 6: Detailed attack ablation studies on MNIST, SVHN and Cifar10. Complementary toTab. 1, we compare our L∞ PGD-Conf, as introduced in Sec. 4.1, attack with T iterations and dif-ferent combinations of momentum, backtracking and initialization on all three datasets. We considerAT, AT trained with PGD-Conf, and CCAT; we report RErr for confidence threshold τ @99%TPR.
Table 7: Main results for 98%TPR on MNIST, SVHN and Cifar10. While reporting results for99%TPR in the main paper, cf. Tab. 3, reducing the TPR requirement for confidence-thresholding to98%TPR generally improves results for both AT and CCAT, but only slightly. Again, we report Errand RErr for τ = 0 and τ @98%TPR as well as ROC AUC.
Table 8: Comparison with (Pang et al., 2018) on MNIST, SVHN and Cifar10. We report “stan-dard” Err and RErr, i.e., τ = 0 (“Standard Setting”), as well as their confidence thresholdedvariants for τ @99%TPR (“Detection Setting”), cf. Sec. 3. On MNIST their method is competitiveregarding RErr after confidence-thresholding, even outperforming AT when it comes to the gener-alization to L2 attacks. However, on SVHN and Cifar10, the approach cannot be considered robustanymore regarding our rigorous evaluation; this might also be due to the fact that no adversarialexamples are used during training.
Table 9: Training ablation studies on MNIST, SVHN and Cifar10. We report results for differentρ and transitions, cf. Eq. (6). We report RErr and Err with confidence threshold τ = 0 (“Stan-dard Setting”) and τ @99%TPR as well as ROC AUC (“Detection Setting”). The models are testedagainst our L∞ PGD-Conf attack with T = 2000 iterations and zero as well as random initialization,as discussed in Sec. 4.1. On MNIST, the exponential transition, especially ρexp = 7 performs best;on Cifar10, the power transition with Ppow = 10 works best - performance stagnates for PPow > 10.
Table 10: Per-attack results on MNIST. Per-attack results considering PGD-CE, as in Madry et al.
Table 11: Per-attack results on SVHN. Per-attack results considering PGD-CE, as in Madry et al.
Table 12: Per-attack L∞ and L2 results on Cifar10. Per-attack results considering PGD-CE, asin Madry et al. (2018), our PGD-Conf and the remaining black-box attacks for all threat models,i.e., L∞, L2, L1 and L0, see text. The used values are reported in the left-most column. For theblack-box attacks, we take the per-example worst-case across all black-box attacks.
Table 13: Per-corruptions results on MNIST-C. Results on MNIST-C, broken down by individualcorruptions (first column); all includes all corruptions and mean are the averaged results over allcorruptions. We report ROC AUC, FPR and additionally the true negative rate (TNR) in addition tothe thresholded and unthresholded Err on the corrupted examples.
Table 14: Per-corruptions results on Cifar10-C. Results on Cifar10-C focusing on individual cor-ruptions (first column); all includes all corruptions and mean are the averaged results over allcorruptions. We report ROC AUC, FPR and additionally the true negative rate (TNR) in addition tothe thresholded and unthresholded Err on the corrupted examples.
