Table 1: Treating different examples as forms of span-extraction problems. For sentence pair clas-sification datasets, one sentence is present in each of the source text and auxiliary text. The possibleclassification labels are appended to the source text. For single sentence classification datasets,the source text only contains the possible classification labels. For question answering datasets, nochanges to the BERT formulation is required; the context is presented as source text and the questionas auxiliary text.
Table 2: Performance metrics on the GLUE tasks. We use Matthewâ€™s correlation for CoLA, anaverage of the Pearson and Spearman correlation for STS, and exact match accuracy for all others.
Table 5: Development set performance metrics on a single (joint) model obtained by multi-taskingon all included datasets. We include best single-task performances (without STILTs), labeled asindividual models, for the sake of easier comparison. We divide the remaining into two parts - inthe first, the scores indicate the performance on a single snapshot during training and not individualmaximum scores across the training trajectory. In the second, we include the best score for everydataset through the training; note that this involves inference on multiple model snapshots. For themodels trained with STILTs, the SpEx-BERT model is first fine-tuned on the intermediate task byitself after which the model is trained in multi-tasking fashion. Bold implies best in each column(i.e., task).
