Table 1: Error bounds after k iterations: Define α-moment E[kg(x)kα] ≤ Gα (Assump 1) andcoordinate-wise moments E[∣g(x)∣α] ≤ Ba (AssUmP 2), which satisfy G2 ≤ d∣∣B∣∣α. In the Stan-dard setting (α = 2), GClip recovers the optimal rates of SGD. For heavy-tailed noise (α ∈ (1, 2)),GCliP converges both for convex (Thm 1) and non-convex fUnctions (Thm 2), whereas the Proof forSGD fails, denoted as N/A. Under a more fine-grained noise model, CCliP has better convergencerates (Thm 3). We also show matching lower-boUnds for all α ∈ (1, 2] Proving the oPtimality ofcliPPing methods (Thm 4).
Table 2: BERT pretraining: Adam vs ACClip. Compared to Adam, the proposed ACClip algo-rithm achieves better evaluation loss and Masked LM accuracy for all model sizes.
Table 3: SQUAD v1.1 dev set: Adam vs ACClip. The mean and standard deviation of F1 andexact match score for 5 runs. The first row contains results reported from the original BERT paper,which are obtained by picking the best ones out of 10 repeated experiments.
