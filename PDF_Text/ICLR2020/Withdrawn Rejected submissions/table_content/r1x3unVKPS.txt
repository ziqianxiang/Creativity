Table 1: Average environment reward and standard de-viation on Lunar Lander, evaluated over 50 runs for thedefault and no-terminal environment.
Table 2: Episodic reward and standard deviation on the Mujoco tasks by different methods evaluated over 50runs. SAIL-b achieves overall the best performance, with significantly lower standard deviation, indicating therobustness of the learned policies.
Table 3: Environment information, number of expert trajectories and environment steps used for each taskB.1	Network ArchitectureThe default policy network from OpenAIâ€™s baselines are used for all tasks: two fully-connectedlayers of 100 units each, with tanh nonlinearities. The discriminator networks and the value functionnetworks use the same architecture.
Table 4: Hyperparameters used for each tasksC Additional Results on Lunar LanderIn the default environment, Lunar Lander contains several terminal states, including crashing, flyingout of view, and landing at the goal. In the no-terminal environment, all terminal states are disabled,such that the agent must solely rely on the expert demonstrations for training signals.
Table 5: Average environment reward and standard deviation on Lunar Lander, evaluated over 50 runs for thedefault, goal-terminal and no-terminal environment.
