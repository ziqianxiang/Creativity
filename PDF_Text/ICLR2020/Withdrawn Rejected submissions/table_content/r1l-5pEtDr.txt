Table 1: Comparisons of different designs of the second moment	SGDM	AdaGrad	RMSPropψt	I	diag(Pi=I g2 埒	(1 - β2)diag(Pit=1β2t-igi2)	Adam	AMSGrad	AdaShiftψt	(1-βt)diag(Pi=ι β2-ig2)	diag (max (Vt-ι,vt))	diag(β2vt-1 + (1 - β2)gt2-n)	NosAdam	...	AdaX (ours)ψt	diag(β2tvt-1 + (1 - β2t)gt2-n)	...	a+β¾t-ι diag(Pt=ι(1 + β2)t-ig2)framework setting, the optimization algorithm chooses a parameter set θt ∈ F and an unknowncost function ft (θ) evaluates its performance at θt in each iteration. Suppose that there exists a bestparameter ft(θ*) such that θ* = argmi□θ∈F (PT=I ft(θ)), then a metric used to show the algo-rithm's performance is the regret function RT = PT=I ft(θt) - ft(θ*) and we want to ensure thatRT = o(T) so that the algorithm will always converge to the optimal solution. (Zinkevich, 2003).
Table 2: Validation accuracy on CIFAR-10.
Table 3: Performance of AdaX on ImageNet, VOC2012 Segmentation and One Billion Wordschedule that the initial step size was scaled down by 0.1 and 0.01 at the 100-th and the 150-thepoch. The experimental results in Table 2 corresponded to our theoretical finding that Adam wasactually taking steps that were ”too large” and would potentially converge to local minimum at theend.
