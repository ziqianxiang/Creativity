Table 1: Statistics of ASAP datasetPrompt ID	# Essays	Avg Length	Score Range1	1783	350	2-122	1800	350	1-63	1726	150	0-34	1772	150	0-35	1805	150	0-46	1800	150	0-47	1569	250	0-308	723	650	0-604.2	Task 1: Training on the whole datasetWe firstly trained our model on the whole dataset and observed that RefNet tends to overfit when allthe training data is used. It is not supervising as the total training data is amplified by approximately300 times after pairing, which means that the model will see each essay around 300 times withineach epoch. As a result, the model can overfit before the first epoch is finished.
Table 2: Keep Rate for Different Essay PairsScore Difference Interval Keep Rate(0,0.1]	025(0.1, 0.2]	0.2(0.2, 0.4]	0.15(0.4,1]	0.1The first three blocks of Table 3 compares the of regression and RefNet with the same backbone.
Table 3: Performance of different backbones in regression and RefNet, performance of typical ex-isting AES models and human ratersMethods	Prompts								Average	1	2	3	4	5	6	7	8	Average (regression)	0.771	0.622	0.643	0.662	0.752	0.702	0.763	0.555	0.684-Average (RefNet)	0.822	0.694	0.664	0.789	0.800	0.802	0.802	0.727	0.762Improvement	6.6%	11.5%	3.3%	19.1%	6.3%	14.2%	5.1%	30.1%	11.5%RNN (regression)	0.622	0.530	0.618	0.747	0.711	0.696	0.700	0.288	0.614-RNN (RefNet)	0.791	0.672	0.665	0.782	0.782	0.783	0.790	0.627	0.737Improvement	27.2%	26.8%	7.6%	4.7%	10.0%	12.5%	12.9%	117.7%	20.0%LSTM (regression)	0.802	0.674	0.660	0.770	0.780	0.784	0.772	0.592	0.729-LSTM (RefNet)	0.822	0.695	0.676	0.799	0.800	0.811	0.807	0.711	0.765Improvement	2.5%	3.1%	2.4%	3.8%	4.5%	2.6%	4.5%	16.7%	4.9%10×CNN	0.804	0.656	0.637	0.762	0.752	0.765	0.750	0.680	0.726-10×LSTM	0.808	0.697	0.689	0.805	0.818	0.827	0.811	0.598	0.75610×(CNN+LSTM)	0.821	0.688	0.694	0.805	0.807	0.819	0.808	0.644	0.761SkipFlow (Bilinear)	0.830	0.678	0.677	0.778	0.795	0.807	0.790	0.670	0.753-SkipFlow (Tensor)	0.832	0.684	0.695	0.788	0.815	0.810	0.800	0.697	0.764Human Raters	0.721	0.814	0.769	0.851	0.753	0.776	0.721	0.624	0.7544.3	Task 2: Few-Shot Learning
Table 4: QWKs of RefNet with different backbones in mini-ASAP datasetBackbones	Data Size	Approach	Average QWK	Degeneration1	Improvement2Average	25%	regression RefNet	0.650 0.737	5.0% 3.3%	34.0%	10%	regression RefNet	0.606 0.703	114% 7.7%	32.5%RNN	25%	regression RefNet	0.525 0.703	14.5% 4.6%	68.3%	10%	regression RefNet	0.493 0.639	24.5% 13.3%	45.7%LSTM	25%	regression RefNet	0.623 0.738	14.5% 3.5%	75.9%	10%	regression RefNet	0.542 0.716	25.7% 6.4%	75.1%1 The percentage of QWK score decrease compared to full dataset scenario2 How much the degeneration of RefNet is less than that of regression methodFrom Table 4 we can see that with regression, the model will suffer from major performance degra-dation when the training data is reduced. On the contrary, RefNet is much more robust to scarcedata. Even after 90% of data is dropped, RefNet can still offer high quality predictions.
Table 5: Results of Ablation StudiesAblation	No transfer learning	No data adjustment	No fine tuningQWK Score	0746	0758	0754Degeneration	-2.48%	-0.92%	-1.44% —4.5	Analysis4.5.1	Regression vs Pairwise RankingRefNet has multiple advantages over regression. First, it leverages the mutual information betweenessays. Notice that in Table 5, the pairwise ranking approach achieved 0.754 on the fixed embed-dings learnt from regression task, which is higher than pure regression performance 0.729 in Table3. It shows that by taking mutual information into account, scoring by pairwise comparison canconsistently improve the performance.
