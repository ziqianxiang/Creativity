Table 1: Experimental results on SQuAD v1.1 dev F1 score in search of good model settings for theIB-BERTLARGE teacher (left) and the MobileBERT student (right). The number of layers is set to 24for all models. Especially, in the right table, the inter-block hidden size of all models is set to 512,which is the same as the chosen teacher model, and the total parameter numbers in these models areall roughly 24M.
Table 2: The detailed model settings of a few models. L, hinter, hintra, hFFN , hembedding ,#Head, #FFN, and #Params denote the number of layers, inter-block hidden size (feature mapsize), intra-block hidden size, FFN intermediate size, embedding table size, the number of heads inmulti-head attention, the number of FFN layers, and the number of parameters, respectively.
Table 3: The test results on the GLUE benchmark (except WNLI). The number below each taskdenotes the number of training examples. The metrics for these tasks can be found in the GLUEpaper (Wang et al., 2018). For tasks with multiple metrics, the metrics are arithmetically averaged tocompute the GLUE score. “OPT” denotes the operational optimizations introduced in Section 4.3.
Table 4: The results on the SQUAD dev datasets. f marks our runs with the official code.
Table 5: Ablation on the dev sets of GLUE benchmark. BERTBAse, BERTSMall*, and the bareMobileBERT (i.e., w/o PD, FMT, AT, FMT & OPT) use the standard BERT pre-training scheme.
Table 6: The effectiveness of operational optimizations on real-world inference latency.
