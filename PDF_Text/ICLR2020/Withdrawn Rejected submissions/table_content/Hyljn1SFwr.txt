Table 1: Mutual information betWeen input and output and entropy of input for SchWartZ-Ziv &TiShby (2017)- and MNIST-dataSetHoWever, thiS leadS to the queStion: why does the information plane not show constant MI?. Accord-ing to Saxe et al. (2018), uSing a binning method, one loSeS the invertibility of the TanH function anda bin almoSt by definition loSeS information about the value put in the bin (e.g. if one putS 2.53 and6H(X) iS Simply log2 (N) Where N iS the number of SampleS Since each Sample iS different and each have aprobability of p(xi) = N. log2 (x) > 0 for X > 1.
Table 2: Accuracy scores for big and prunned networks for Schwartz-Ziv & Tishby (2017)-datasetReLU-layer usually diversify the activations which in return will also lead to a diversification in theTanH activations. Hence I(X; T) and I(T; Y ) will increase.
