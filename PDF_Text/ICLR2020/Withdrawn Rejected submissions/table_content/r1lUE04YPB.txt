Table 1: Illustrations of input-output pairs for typical dialogue response training, next-sentence pre-training, or MASS pre-training.
Table 2: Perplexity and AMT-Rating evaluation for different training process on the three dialoguedata-sets. The rating scores are the average score of fluency, consistency, and engagingness.
Table 3: Samples of different models on the Dailydialogue test-set. The samples are consecutive(input of the next sample is the reference response for the previous one). More samples are given inAppendix C.
Table 4: The model’s PPL performance When Word-shuffle or Word-drop is applied to the contextinput. On the left We describe What training process is used and on Which test set is PPL evaluated.
Table 5: Example of trigger inputs for the knowledge term “pokemon”. Followed by referencedescription and model samples for “pokemon” and “deadpool”. Note that the pre-trained model’ssample is from news-style triggers, and the other samples are from dialogue-style triggers.
Table 6: Average BLEU-2/BLEU-3 scores for the model’s samples w.r.t. the reference description.
Table 7: AMT rating scores (calibrated mean and standard deviation) for multi-turn dialogue evalu-ation.
Table 8: Multi-turn and single-turn examPles of the model trained by mix-review on Dailydialoguedata. The single-turn examPles involve light cherry-Picking.
Table 9: Average of diversity metrics for models on the three dialogue data-sets.
Table 10:	Multi-turn dialogue samples with turkers from the model trained by mix-review. 0 repre-sents the turker, and 1 is the model.
Table 11:	Samples of different models on the Dailydialogue/Switchboard/Cornell-Movie test-set.
Table 12: The detailed rating scores from AMT.
Table 13: The model’s PPL performance when word-shuffle or word-drop is applied to the contextinput. On the left we describe what training process is used and on which test set is PPL evaluated.
Table 14: Average BLEU-2/BLEU-3 scores for the model’s samples w.r.t. the reference description.
