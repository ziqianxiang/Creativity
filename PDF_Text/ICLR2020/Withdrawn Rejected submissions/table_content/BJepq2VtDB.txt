Table 1: ImageNet classification — ResNet-50 v2, batch 1024, top-1 and top-5 accuracy(%).
Table 2: Large batch training with NovoGrad — ImageNet, ResNet-50 v2, 90 epochs, accuracy(%).
Table 3: Large batch training comparison — ImageNet, ResNet-50v 2, top-1 accuracy(%) .
Table 4: Speech recognition — Jasper-10x5, LibriSpeech, 400 epochs, WER (%).
Table 5: Large batch training with NovoGrad — Jasper-10x5, LibriSpeech, 400 epochs, WER (%).
Table 6: LM. Transformer-XL trained on WikiText-103 with batch size 256, sequence length 512.
Table 7: WMT’14 English-to-German translation, Transformer-big, batch 490K tokens, 150 epochs,no checkpoint averaging. Tokenized BLEU and detokenized SacreBLEU on WMT’14 (newstest14).
