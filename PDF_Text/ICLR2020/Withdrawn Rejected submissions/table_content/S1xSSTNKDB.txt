Table 1: Statistics of Face Attribute DatasetsRaCe AnnotationName	Source	#of faces	In-the- wild?	Age	Gender	White*		Asian*		Bla- ck	Ind- ian	Lat- ino	Balan- ced?						W I ME		E I SE					PPB (BUolamWini & Gebru, 2018)	Gov. Official Profiles	1K		X	X	**Skin color prediction							MORPH (Ricanek & Tesafaye, 2006)	Public Data	55K		X	X	merged				X		X	noPubFig (Kumar et al., 2011)	Celebrity	13K	X	Model generated predictions									noIMDB-WIKI (Rothe et al., 2016)	IMDB, WIKI	500K	X	X	X								noFotW (Escalera et al., 2016)	Flickr	25K	X	X	X								yesCACD (Chen et al., 2015)	celebrity	160K	X	X									noDiF (Merler et al., 2019)	Flickr	1M	X	X	X	**Skin color prediction							∣CelebA (LiU et al., 2015)	CelebFace LFW	200K	X	X	X								noLFW+ (Han et al., 2018)	LFW (Newspapers)	15K	X	X	X	merged		merged					no∣LFWA+ (LiU et al., 2015)	LFW (Newspapers)	13K	X		X	merged		merged		X	X		no∣UTKFace (Zhang et al., 2017)	MORPH,CACD Web	20K	X	X	X	merged		merged		X	X		yesFairFace (Ours)	-Flickr, Twitter Newspapers, Web	108K	X	X	X	X	X	X	X	X	X	X	yes*FairFace (Ours) also defines East (E) Asian, Southeast (SE) Asian, Middle Eastern (ME), and Western (W) White.
Table 2: Cross-Dataset Classification Accuracy on White Race.
Table 3: Cross-Dataset Classification Accuracy on non-White Races.
Table 4: Gender classification accuracy measured on external validation datasets across gender-racegroups.
Table 5: Gender classification accuracy on external validation datasets, across race and age groups.
Table 6: Classification accuracy of commercial services on FairFace dataset. (*Microsoft, *Face++,*IBM indicate accuracies only on the detected faces, ignoring mis-detections.)	White		Black		EastAsian		SE Asian		Latino		Indian		Mid-Eastern				F	M	F	M	F	M	F	M	F	M	F	M	F	M	Mean	STDAmazon	.923	.966	.901	7955^"	.925	.949	.918	.914	.921	.987	.951	.979	.906	.983	.941	.030Microsoft	.822	.777	.766	.717	.824	.775	.852	.794	.843	.848	.863	.790	.839	.772	.806	.042Face++	.888	.959	.805	.944	.876	.904	.884	.897	.865	.981	.770	.968	.822	.978	.896	.066IBM	.910	.966	.758	.927	.899	.910	.852	.919	.884	.972	.811	.957	.871	.959	.900	.061FairFace	.987	.991	.964	.974	.966	.979	.978	.961	.991	.989	.991	.987	.972	.991	.980	.011*Microsoft	.973	.998	.962	.967	.963	.976	.960	.957	.983	.993	.975	.991	.966	.993	.975	.014*Face++	.893	.968	.810	.956	.878	.911	.886	.899	.870	.983	.773	.975	.827	.983	.901	.067*IBM	.914	.981	.761	~.956~	.909	.920	.852	.926	.892	.977	.819	.975	.881	.979	.910	.066Table 6 shows the gender classification accuracies of the tested APIs. These APIs first detect a facefrom an input image and classify its gender. Not all 7,476 faces were detected by these APIs withthe exception of Amazon Rekognition which detected all of them. Table 8 in Appendix reports thedetection rate.1 We report two sets of accuracies: 1) treating mis-detections as mis-classificationsand 2) excluding mis-detections. For comparison, we included a model trained with our dataset toprovide an upper bound for classification accuracy. Following prior work (Merler et al., 2019), wealso show the classification accuracy as a function of skin color in Figure 6.
Table 7: Classification accuracy on external validation datasets.
Table 8: Face detection rate of commercial services on FairFace dataset.
