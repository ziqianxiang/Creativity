Table 1: Weight compression comparison of ResNet-20 and ResNet-32 on CIFAR-10. For FleXOR,we use warmup scheme, Stanh=10, and Nout=20. All of quantization schemes in the table follow theform of binary codes with q=1, and hence, the amount of computations becomes the same. FleXOR,however, saves on/off-chip memory requirements further becasue Nin/Nout is less than 1.
Table 2: Weight compression comparison of ResNet-18 on ImageNet using various compressionschemes. All of quantization schemes in the table follow the form of binary codes with q=1, andhence, the amount of computations becomes the same. FleXOR, however, saves on/off-chip memoryrequirements further becasue Nin/Nout is less than 1.
Table 3: An XOR gate modeling using F㊉(χi,χ2).
Table 4: Weight compression comparison of ResNet-20 and ResNet-32 on CIFAR-10 when Nout =10. Parameters and recipes not described in the table are the same as in Table 1. We also presentcompression ratio for fractional quantized ResNet-32 when one scaling factor (α) is assigned to eachoutput channel.
Table 5: Weight compression comparison of ResNet-20 and ResNet-32 on CIFAR-10 using learningrate warmup (for 100 epochs) and q = 2. As mentioned in Figure 4, multiple M㊉ can be combinedfor multi-bit quantization schemes. Then, the number of scaling factors should be doubled. FleXORwith q = 2 and two different M㊉ structures achieve full-precision accuracy when both Nin andNout are 10.
Table 6: Weight compression comparison of ResNet-18 on ImageNet when q = 2. Since q is 2, wealso list the other compression schemes which use 2-bit or ternary quantization scheme for modelcompression.
