Table 1: The performance of a range of 16-4 Wide-ResNets on CIFAR-10 without batch normalizationat batch size 64. We use a compute budget of 200 epochs and follow our standard learning rateschedule. We perform two independent grid searches to identify the learning rate which minimizesthe training loss and the learning rate which maximizes the test accuracy, and we provide the meanperformance of the best 12 out of 15 runs. Introducing a scalar multiplier initialized at zero to eachresidual branch is sufficient to achieve a test accuracy of 94%, and this configuration also achievesthe lowest training loss of 0.181. Meanwhile ZeroInit, which combines this scalar multiplier withbiases and dropout, improves the optimal test accuracy but also increases the training loss.
Table 2: The performance of a d-2 Wide-ResNet on CIFAR-10 at batch size 64, for a range of depthsd. We use a compute budget of 200 epochs and follow our standard learning rate schedule. Weperform a grid search to identify the optimal learning rate, and provide the mean performance of thebest 12 out of 15 runs. Both batch normalization and ZeroInit are able to train very deep networks upto (at least) one thousand layers. The optimal learning rate is only weakly dependent on depth.
Table 3: The performance of a d-2 Wide-ResNet on CIFAR-10 at batch size 64, for a range of depthsd. We train for 200 epochs. We perform a grid search to identify the optimal learning rate, andprovide the mean performance of the best 12 out of 15 runs. L2 regularization is not required totrain very deep networks with ZeroInit, and we can also train very deep networks without biases ordropout, using solely a scalar multiplier at the end of each residual branch initialized to zero.
Table 4: The performance of ResNet-50-V2 on ImageNet. We use a compute budget of 90 epochsand perform a grid search to identify the optimal learning rate which maximizes the top-1 validationaccuracy. We perform a single run at each learning rate and report both top-1 and top-5 accuracyscores. We use a drop probability of 0.2 for ZeroInit. Both Fixup and ZeroInit are competitive withbatch normalization at small batch sizes, while batch normalization is better at larger batch sizes.
Table 5: The performance of ResNet-50-V1 on ImageNet. We use a compute budget of 90 epochsand perform a grid search to identify the optimal learning rate which maximizes the top-1 validationaccuracy. We perform a single run at each learning rate and report both top-1 and top-5 accuracyscores. We use a drop probability of 0.2 for ZeroInit. Fixup performs well when the batch size issmall, but significantly underperforms batch normalization when the batch size is large. ZeroInitperforms poorly at all batch sizes, but its performance improves considerably if we add a scalar biasbefore the final ReLU in each residual block (after the skip connection and residual branch merge).
