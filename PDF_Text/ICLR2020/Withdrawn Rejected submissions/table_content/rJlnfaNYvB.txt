Table 1: Test accuracy results for ResNet models trained on CIFAR-10/100, each averaged from4 runs with different random seeds. The best result for each combination of dataset and model isbolded. Fixed loss scaling is tied with ours for the best result on ResNet-20 (C100).
Table 2: Image classification evaluation for different loss scaling methods. Numbers showed hereare top-1 test accuracy (%). None means no loss scaling applied.
Table 3: Test performance of SSD512 models trained by different loss scaling methods, measuredin mAP (%). FP32 denotes the baseline results trained in FP32, and we select the golden valuefrom (Fu et al., 2017) for the case using 32 as the batch size. None means no loss scaling is used,“Fixed (best)” stands for the best performance we can find after trying out several fixed loss scalevalues (including {8, 128, 1024, 2048}), and “Dynamic” shows the results of dynamic loss scaling.
Table 4: The effect of different fixed loss scales on the test accuracy, which is measured for ResNet-20 and ResNet-56 on CIFAR-10. Numbers on the first row give the fixed loss scales.
Table 5: Changes in mAP after changing the fixed loss scales. Batch size = 8.
