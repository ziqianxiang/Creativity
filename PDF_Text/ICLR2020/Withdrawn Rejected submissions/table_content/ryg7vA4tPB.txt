Table 1: Comparison of different sparse training techniques. Drop and Grow columns correspond tothe strategies used during the mask update. Selectable FLOPs is possible if the cost of training themodel is fixed at the beginning of training.
Table 2: Performance and cost of sparse training methods on training 80% and 90% sparse ResNet-50s. FLOPs needed for training and test are normalized with the FLOPs of a dense model (seeAppendix G for details on how FLOPs are calculated). Methods with a subscript indicate a rescaledtraining time, whereas ‘*’ indicates reported results. (ERK) corresponds to the sparse networks withErdos-Renyi-Kernel sparsity distribution. RigL5× (ERK) achieves 77.1% Top-1 Accuracy usingonly 20% of the parameters of a dense model and 42% of its FLOPs.
Table 3: Effect of lottery ticket initialization on the final performance. There are no special ticketsand dynamic connectivity provided by RigL is critical for good performance.
