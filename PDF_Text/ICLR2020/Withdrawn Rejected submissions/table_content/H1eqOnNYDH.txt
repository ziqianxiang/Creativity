Table 1: Average accuracy improvement over the baseline model of each combination of dataaugmentation level and presence of weight decay and dropout.
Table 2: Average fraction of the original accuracy of each corresponding combination of dataaugmentation level and presence of weight decay and dropout.
Table 3: Specification of the All-CNN architectures.
Table 4: Description and range of possible values of the parameters used for the heavier augmentation.
Table 5: Test accuracy of All-CNN and WRN, comparing the performance with and without explicitregularizers and the different augmentation schemes. Results within brackets show the performanceof the models without batch normalizationNetwork	WD	Dropout	Aug.	CIFAR-10	CIFAR-100	Acc. ImageNet	yes	yes	no	90.04 (88.35)	66.50 (60.54)	58.09	yes	yes	light	93.26 (91.97)	70.85 (65.57)	63.35	yes	yes	heavier	93.08 (92.44)	70.59 (68.62)	60.15	no	yes	no	77.99 (87.59)	52.39 (60.96)	—All-CNN	no	yes	light	77.20 (92.01)	69.71 (68.01)	—	no	yes	heavier	88.29 (92.18)	70.56 (68.40)	—	no	no	no	84.53 (71.98)	57.99 (39.03)	56.53	no	no	light	93.26 (90.10)	69.26 (63.00)	63.79	no	no	heavier	93.55 (91.48)	71.25 (71.46)	61.37	yes	yes	no	91.44 (89.30)	71.67 (67.42)	54.67	yes	yes	light	95.01 (93.48)	77.58 (74.23)	68.84	yes	yes	heavier	95.60 (94.38)	76.96 (74.79)	66.82	no	yes	no	91.47 (89.38)	71.31 (66.85)	—WRN	no	yes	light	94.76 (93.52)	77.42 (74.62)	—	no	yes	heavier	95.58 (94.52)	77.47 (73.96)	—	no	no	no	89.56 (85.45)	68.16 (59.90)	61.29
Table 6: Test accuracy of All-CNN and WRN when training with only 80 %, 50 %, 10 % and 1 % ofthe available examples. Results within brackets correspond to the models without batch normalizationPct. Data Expl. Reg. Aug. scheme Test CIFAR-10	Test CIFAR-10080 %	yes yes yes	no light heavier	All-CNN	WRN	All-CNN	WRN			89.41 (86.61) 92.20 (91.25) 92.83 (91.42)	90.27 94.07 94.57	63.93 (52.51) 67.63 (63.24) 68.01 (65.89)	70.41 75.66 75.51	no no no	no light heavier	-^83.04 (75.00) 92.25 (88.75) 92.80 (90.55)	88.98 93.97 94.84	55.78 (35.95) 69.05 (56.81) 69.40 (63.57)	66.10 75.07 75.38	yes	no	-^85.88 (82.33)	86.96	58.24 (44.94)	63.60	yes	light	90.30 (87.37)	92.65	61.03 (54.68)	70.8350 %	yes	heavier	90.09 (88.94)	92.86	63.25 (57.91)	70.33	no	no	78.61 (69.46)	85.56	48.62 (31.81)	60.64	no	light	90.21 (84.38)	91.87	62.83 (47.84)	69.97	no	heavier	90.76 (87.44)	92.77	64.41 (55.27)	70.72	yes	no	67.19(61.61)	70.73	33.77 (19.79)	34.11	yes	light	76.03 (69.18)	76.00	38.51 (22.79)	36.6510%	yes	heavier	78.69 (64.14)	78.10	38.34 (26.29)	38.93	no	no	60.97 (41.07)	60.39	26.05 (17.55)	23.65	no	light	78.29 (67.65)	79.19	37.84 (24.34)	39.24	no	heavier	79.87 (70.64)	80.29	39.85 (26.31)	41.44	yes	no	27.53 (29.90)	33.45	9.16 (3.60)	7.47	yes	light	37.18 (26.85)	34.13	9.64 (3.65)	7.50
Table 7: Test accuracy of the shallower and deeper versions of All-CNN on CIFAR-10 and CIFAR-100.
Table 8: Frobenius norm of the weight matrices learned by the networks All-CNN and WRNon CIFAR-10 and CIFAR-100, trained with and without exPlicit regularizers and the differentaugmentation schemes. Norms within brackets corresPond to the models without batch normalizationWD	Dropout	Aug.	Norm CIFAR-10		Norm CIFAR-100				All-CNN	WRN	All-CNN	WRNyes	yes	no	48.7 (64.9)	101.4 (122.6)	76.5 (97.9)	134.8 (126.5)yes	yes	light	52.7 (63.2)	106.1 (123.9)	77.6 (86.8)	140.8 (129.3)yes	yes	heavier	57.6 (62.8)	119.3(125.3)	78.1 (83.1)	164.2 (132.5)no	yes	no	52.4 (70.5)	153.3 (122.5)	79.7 (103.3)	185.1 (126.5)no	yes	light	57.0 (67.9)	160.6 (123.9)	83.6 (93.0)	199.0 (129.4)no	yes	heavier	62.8 (67.5)	175.1 (125.2)	84.0 (88.0)	225.4 (132.5)no	no	no	37.3 (63.7)	139.0(120.4)	47.6 (102.7)	157.9 (122.0)no	no	light	47.0 (69.5)	153.6 (123.2)	80.0 (108.9)	187.0 (127.2)no	no	heavier	62.0 (71.7)	170.4 (125.4)	91.7 (91.7)	217.6 (132.9)One of the relevant results Presented in this PaPer is the Poor Performance of the regularized models onthe shallower and deePer versions of All-CNN, comPared to the models without exPlicit regularization(see Table 7). One hyPothesis is that the amount of regularization is not ProPerly adjusted throughthe hyPerParameters. This could be reflected in the norm of the learned weights, shown in Table 9.
Table 9: Frobenius norm of the weight matrices learned by the shallower and deeper versions of theAll-CNN network on CIFAR-10 and CIFAR-100.
