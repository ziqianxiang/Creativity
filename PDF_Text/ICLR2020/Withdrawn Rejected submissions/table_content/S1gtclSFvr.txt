Table 1: Performance on IWSLT14 German-English (left-side) and IWSLT15 English-Vietnamese(right-side) test set. For BSO (Wiseman & Rush, 2016), Actor-Critic (Bahdanau et al., 2016),DenseNMT (Shen et al., 2018) and Graph2Seq (Xu et al., 2018), Stanford NMT Luong & Manning(2015), Hard monotonic (Raffel et al., 2017), DeconvDec (Lin et al., 2018a) and SACT (Lin et al.,2018b), we take the numbers from their papers. For the Seq2Seq with attention baseline (Bahdanauet al., 2014) and NPMT (Huang et al., 2018), we take the numbers from Huang et al. (2018). We usethe subscript ‘w’ and ‘b’ to denote the word level and BPE level for our model and Transformer.
Table 2: DE-EN translation examples, where "[•]" denotes the phrase boundary, “(•)” indicts a phraselooked up from the external dictionary, and "__” represents the frequency of the word replaced bythe UNK token in the neural network model. The subscript represents the corresponding phrasesalignments discovered by the phrase-attention mechanism (§2.2).
Table 3: Phrase length statistics. The ratio (%) of the length of the phrases learned by NP2MT in theIWSLT14 German-English when we set the threshold as 10.
Table 4: IWSLT15 English-Vietnamese Translation with an in-domain dictionary. We use DI todenote an in-domain dictionary.
Table 5: Results on cross-domain translation by training the model on IWSLT14 and testing onWMT14, both with German-English data. In the test data from WMT14, OOV rates are 12.8% forGerman and 6.7% for English.
Table 6: IWSLT14 German-English Translation with in-domain and out-of-domain dictionaries. Weuse DI to denote an in-domain dictionary and DO to denote an out-of-domain dataset.
