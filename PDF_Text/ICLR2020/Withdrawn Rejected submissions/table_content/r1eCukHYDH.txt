Table 1: FID (smaller is better) and Disentanglement (larger is better) scores are shown. We compareWGAN (Arjovsky et al., 2017), DMWGAN (Khayatkhoei et al., 2018), β-VAE (Higgins et al.,2016), InfoGAN (Chen et al., 2016) with our model. The mean and std. values are computed from10 (MNIST) and 5 (3D-Chair) replicated experiments.
Table C.1: MLA-GAN architecture used for MNIST datasetGenerator	DiscriminatorInput(8)	Input(1,28,28)FUll(1024), BN,LReLU(0.2)	Conv(c=64, k=4, s=2, p=1), BN, LReLU(0.2)Full(6272), BN, LReLU(0.2)	Conv(c=128, k=4, s=2, p=1), BN, LReLU(0.2)ReshaPeTo(128,7,7)	ReshapeTo(6272)ConvTrs(c=64, k=4, s=2,p=1), BN, LReLU(0.2)	Full(1024), BN, LReLU(0.2)ConvTrs(c=32, k=4, s=2,p=1), BN, LReLU(0.2)	Full(1)ConvTrs(c=1, k=3, s=1, p=1), Tanh	C.1.1 Notes on the Other Compared ModelsOverall, we match the architecture of other models with our model for fair comparison. Somedifferences to note are:•	DMWGAN: We used 10 generators. Each generator has the same architecture as oursexcept the number of features or the channels are divided by 4, to match the number oftrainable parameters. Note that 4 is the suggested number from the original paper.
Table C.2: MLA-GAN architecture used for 3D-Chair dataset.
Table C.3: MLA-GAN architecture used for UT-Zap50k dataset.
