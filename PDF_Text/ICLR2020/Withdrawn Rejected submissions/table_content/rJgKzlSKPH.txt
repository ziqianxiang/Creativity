Table 1: Survey of recent quantization methodsand their bit-size configuration in different layersConsidering that the optional pooling layer is of minor importance for quantization2, three maincomponents remain in each block: convolution and fully-connected layers (Linear), batch normal-ization (BN) and the non-linear activation function ReLU. Since each component differs in its com-putation task, different quantization strategies can be followed. Table 1 gives an overview of recentapproaches including the respective bit-sizes during test time. Components encoded with 32 bitsremain in high-precision floating-point arithmetic.
Table 2: Summary of the quantized performance on MNIST, CIFAR-10 and CIFAR-100.
