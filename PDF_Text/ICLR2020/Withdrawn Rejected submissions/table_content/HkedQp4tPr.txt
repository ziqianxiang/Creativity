Table 1: Results from models trained with teacher-forcing, Sequential Scheduled Sampling, andParallel Scheduled Sampling on dialog response generation. We report mean BLEU and maximumBLEU over 5 random restarts for each configuration except Sequential Scheduled Sampling, forwhich we report a single run. We provide results by varying different hyperparameters for bothvariants of Scheduled Sampling. We also provide training steps per second for the different trainingalgorithms. In the best setting, Parallel Scheduled Sampling achieves 1.6 BLEU score (11.5%)improvement over teacher-forcing.
Table 2: Performance on the summarization task using base and large Transformer when trainedwith teacher-forcing and Parallel Scheduled Sampling. We consider both beam search and greedydecoding. We adopt the widely-used ROUGE score as the evaluation metric (higher the better).
Table 3: Empirical results on CIFAR-10. We use Frechet Inception Distance (FID) (Heusel et al.,2017) and Inception Score (IS) (Salimans et al., 2016) metrics to evaluate the quality of the samplesfrom teacher-forcing and Parallel Scheduled Sampling. We provide upper-bound scores by computingthe metric on the ground-truth data.
