Table 1: Update rules considered in this work. SGD is due to Robbins and Monro (1951),MOMENTUM to Polyak (1964), NESTEROV to Nesterov (1983), RMSPROP to Tieleman and Hin-ton (2012), and NAdam to Dozat (2016). All operations are taken component-wise for vectors. Inparticular, for x ∈ Rd, x2 is a component-wise power function.
Table 2: Summary of workloads used in experiments.
Table 3: SGD	no	1 - Yinitial	[10-4,102]	[10-4,1]final	[10-5,101]	[10-4,1]Table 4: Momentum	no	1 - Yinitial	[10-4,102]	[10-4,1]final	[10-4,101]	[10-4,1]Table 5: Nesterov	no	1 - Y	1 - P	€initial	[10-4, 101]	[10-2,1]	[10-4,1]	[10-5,101]final	[10-5,1]	[10-2,1]	[10-3,1]	[10-1o,10-5]Table 6: RMSPROP	αo	1 - β1	1 - β2	€initial	[10-4,10-1]	[10-3, 5 × 10-1]	[10-4, 10-1]	[10-9,10-5]final	[10-5,10-1]	[10-3,1]一	[10-4,1]	[10-1o,10-5]Table 7: ADAM	αo/	1 - β1	1 - β2	€initial	[10-2,104]	[10-3,1]	[10-4,1]	[10-1o, 1010]final	[10-1, 101]	[10-3,1]	[10-4,1]	[10-6,10-2]
Table 4: Momentum	no	1 - Yinitial	[10-4,102]	[10-4,1]final	[10-4,101]	[10-4,1]Table 5: Nesterov	no	1 - Y	1 - P	€initial	[10-4, 101]	[10-2,1]	[10-4,1]	[10-5,101]final	[10-5,1]	[10-2,1]	[10-3,1]	[10-1o,10-5]Table 6: RMSPROP	αo	1 - β1	1 - β2	€initial	[10-4,10-1]	[10-3, 5 × 10-1]	[10-4, 10-1]	[10-9,10-5]final	[10-5,10-1]	[10-3,1]一	[10-4,1]	[10-1o,10-5]Table 7: ADAM	αo/	1 - β1	1 - β2	€initial	[10-2,104]	[10-3,1]	[10-4,1]	[10-1o, 1010]final	[10-1, 101]	[10-3,1]	[10-4,1]	[10-6,10-2]Table 8: NADAM16Under review as a conference paper at ICLR 2020D.2 RESNET-32 ON CIFAR- 1 0
Table 5: Nesterov	no	1 - Y	1 - P	€initial	[10-4, 101]	[10-2,1]	[10-4,1]	[10-5,101]final	[10-5,1]	[10-2,1]	[10-3,1]	[10-1o,10-5]Table 6: RMSPROP	αo	1 - β1	1 - β2	€initial	[10-4,10-1]	[10-3, 5 × 10-1]	[10-4, 10-1]	[10-9,10-5]final	[10-5,10-1]	[10-3,1]一	[10-4,1]	[10-1o,10-5]Table 7: ADAM	αo/	1 - β1	1 - β2	€initial	[10-2,104]	[10-3,1]	[10-4,1]	[10-1o, 1010]final	[10-1, 101]	[10-3,1]	[10-4,1]	[10-6,10-2]Table 8: NADAM16Under review as a conference paper at ICLR 2020D.2 RESNET-32 ON CIFAR- 1 0We used linear learning rate decay for all experiments. We tuned the number of decay steps within[0.5, 1.0] times the number of training steps and the learning rate decay factor f within the valuesshown in the tables below. λL2 denotes the L2 regularization coefficient.
Table 6: RMSPROP	αo	1 - β1	1 - β2	€initial	[10-4,10-1]	[10-3, 5 × 10-1]	[10-4, 10-1]	[10-9,10-5]final	[10-5,10-1]	[10-3,1]一	[10-4,1]	[10-1o,10-5]Table 7: ADAM	αo/	1 - β1	1 - β2	€initial	[10-2,104]	[10-3,1]	[10-4,1]	[10-1o, 1010]final	[10-1, 101]	[10-3,1]	[10-4,1]	[10-6,10-2]Table 8: NADAM16Under review as a conference paper at ICLR 2020D.2 RESNET-32 ON CIFAR- 1 0We used linear learning rate decay for all experiments. We tuned the number of decay steps within[0.5, 1.0] times the number of training steps and the learning rate decay factor f within the valuesshown in the tables below. λL2 denotes the L2 regularization coefficient.
Table 7: ADAM	αo/	1 - β1	1 - β2	€initial	[10-2,104]	[10-3,1]	[10-4,1]	[10-1o, 1010]final	[10-1, 101]	[10-3,1]	[10-4,1]	[10-6,10-2]Table 8: NADAM16Under review as a conference paper at ICLR 2020D.2 RESNET-32 ON CIFAR- 1 0We used linear learning rate decay for all experiments. We tuned the number of decay steps within[0.5, 1.0] times the number of training steps and the learning rate decay factor f within the valuesshown in the tables below. λL2 denotes the L2 regularization coefficient.
Table 8: NADAM16Under review as a conference paper at ICLR 2020D.2 RESNET-32 ON CIFAR- 1 0We used linear learning rate decay for all experiments. We tuned the number of decay steps within[0.5, 1.0] times the number of training steps and the learning rate decay factor f within the valuesshown in the tables below. λL2 denotes the L2 regularization coefficient.
Table 9: SGD	no	1 - Y		λL2			f	final	[10-4,102]	[10-3,1]	{10-5,10-4,10-3,10-2}	{10-4,10-3,10-2,10-1}Table 10: Momentum	no	1-γ		λL			f	initial	[10-4,102]	[10-4, 101]		10-4		{10-3,10-2,10-1}final	[10-4,101]	[10-4,1]	{10-5,10-4,10-3,10-2}	{10-4,10-3,10-2,10T }Table 11: Nesterov	no	1 - Y	1 - P	€	λL2	finitial	[10-4, 101]	[10-2,1]	[10-4,1]	[10-5,101]	10-4	{10-3,10-2,10-1}final	[10-4, 101]	[10-3,1]	[10-4, 1]	[10-5,101]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}Table 12: RMSProp	αo	1 - β1	1 — β2	€	λL2	finitial	[10-4, 10-1]	[10-3, 5 × 10-1]	[10-4,10-1]	[10-9,10-5]	10-4	{10-3,10-2 10-1}final	[10-3, 101]	[10-3,1]	[10-4,10-1]	[10-5,101]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}Table 13: ADAM	ɑo/	1 - β1	1 — β2	€	λL2	finitial	[10-2, 104]	[10-3,1]	[10-4,1]	[10-1o,101o]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}final	[10-2,1]	[10-3,1]	[10-4,1]	[1,104]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}Table 14: NADAM
Table 10: Momentum	no	1-γ		λL			f	initial	[10-4,102]	[10-4, 101]		10-4		{10-3,10-2,10-1}final	[10-4,101]	[10-4,1]	{10-5,10-4,10-3,10-2}	{10-4,10-3,10-2,10T }Table 11: Nesterov	no	1 - Y	1 - P	€	λL2	finitial	[10-4, 101]	[10-2,1]	[10-4,1]	[10-5,101]	10-4	{10-3,10-2,10-1}final	[10-4, 101]	[10-3,1]	[10-4, 1]	[10-5,101]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}Table 12: RMSProp	αo	1 - β1	1 — β2	€	λL2	finitial	[10-4, 10-1]	[10-3, 5 × 10-1]	[10-4,10-1]	[10-9,10-5]	10-4	{10-3,10-2 10-1}final	[10-3, 101]	[10-3,1]	[10-4,10-1]	[10-5,101]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}Table 13: ADAM	ɑo/	1 - β1	1 — β2	€	λL2	finitial	[10-2, 104]	[10-3,1]	[10-4,1]	[10-1o,101o]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}final	[10-2,1]	[10-3,1]	[10-4,1]	[1,104]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}Table 14: NADAM17Under review as a conference paper at ICLR 2020D.3 ResNet-50 on ImageNet
Table 11: Nesterov	no	1 - Y	1 - P	€	λL2	finitial	[10-4, 101]	[10-2,1]	[10-4,1]	[10-5,101]	10-4	{10-3,10-2,10-1}final	[10-4, 101]	[10-3,1]	[10-4, 1]	[10-5,101]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}Table 12: RMSProp	αo	1 - β1	1 — β2	€	λL2	finitial	[10-4, 10-1]	[10-3, 5 × 10-1]	[10-4,10-1]	[10-9,10-5]	10-4	{10-3,10-2 10-1}final	[10-3, 101]	[10-3,1]	[10-4,10-1]	[10-5,101]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}Table 13: ADAM	ɑo/	1 - β1	1 — β2	€	λL2	finitial	[10-2, 104]	[10-3,1]	[10-4,1]	[10-1o,101o]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}final	[10-2,1]	[10-3,1]	[10-4,1]	[1,104]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}Table 14: NADAM17Under review as a conference paper at ICLR 2020D.3 ResNet-50 on ImageNetWe used linear learning rate decay for all experiments. We tuned the number of decay steps within[0.5, 1.0] times the number of training steps and the learning rate decay factor f within the val-ues shown in the tables below. λwd denotes the weight decay coefficient and τ denotes the labelsmoothing coefficient.
Table 12: RMSProp	αo	1 - β1	1 — β2	€	λL2	finitial	[10-4, 10-1]	[10-3, 5 × 10-1]	[10-4,10-1]	[10-9,10-5]	10-4	{10-3,10-2 10-1}final	[10-3, 101]	[10-3,1]	[10-4,10-1]	[10-5,101]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}Table 13: ADAM	ɑo/	1 - β1	1 — β2	€	λL2	finitial	[10-2, 104]	[10-3,1]	[10-4,1]	[10-1o,101o]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}final	[10-2,1]	[10-3,1]	[10-4,1]	[1,104]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}Table 14: NADAM17Under review as a conference paper at ICLR 2020D.3 ResNet-50 on ImageNetWe used linear learning rate decay for all experiments. We tuned the number of decay steps within[0.5, 1.0] times the number of training steps and the learning rate decay factor f within the val-ues shown in the tables below. λwd denotes the weight decay coefficient and τ denotes the labelsmoothing coefficient.
Table 13: ADAM	ɑo/	1 - β1	1 — β2	€	λL2	finitial	[10-2, 104]	[10-3,1]	[10-4,1]	[10-1o,101o]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}final	[10-2,1]	[10-3,1]	[10-4,1]	[1,104]	{10-5,10-4 10-3,10-2}	{10-4,10-3 10-2,10-1}Table 14: NADAM17Under review as a conference paper at ICLR 2020D.3 ResNet-50 on ImageNetWe used linear learning rate decay for all experiments. We tuned the number of decay steps within[0.5, 1.0] times the number of training steps and the learning rate decay factor f within the val-ues shown in the tables below. λwd denotes the weight decay coefficient and τ denotes the labelsmoothing coefficient.
Table 14: NADAM17Under review as a conference paper at ICLR 2020D.3 ResNet-50 on ImageNetWe used linear learning rate decay for all experiments. We tuned the number of decay steps within[0.5, 1.0] times the number of training steps and the learning rate decay factor f within the val-ues shown in the tables below. λwd denotes the weight decay coefficient and τ denotes the labelsmoothing coefficient.
Table 15: SGD	no	1 - Y	λwd	T		f	initial	[10-3,1]	[10-3,1]	[10-5,10-2]	{0,10-2,10-1}	{10-4,10-3,10-2,10-1}final	[10-2,1]	[10-2,1]	[10-4,10-3]	10-2	{10-4,10-3,10-2,10-1}Table 16: Momentum	no	1 - Y	λwd	T		f	initial	[10-3,1]	[10-3,1]	[10-5,10-2]	{0,10-2,10-1}		10-3	final	[10-2,1]	[10-3,1]	[10-4,10-3]		0		{10-4,10-3,10-2,10-1}Table 17: Nesterov	n7√	1 - Y	1 - P	€	λwd	T	finitial	[10-2,104]	0.1	[10-4,1]	[10-1o,1010]	[10-5,10-2]	{0,10-2,10-1}	10-3final	[10-2,1]	0.1	[10-2,1]	[10-8,10-3]	[10-4,10-3]	0	{10-4,10-3 10-2,10-1}Table 18: RMSProp	ao/	1 - β1	€	λwd	T	finitial	[1,102]	[10-3,1]	[1,104]	[10-5,10-3]	{0,10-2,10-1}	10-3final	[1,102]	[10-2, 1]	[10-2,102]	10-4	10-1	{10-4,10-3 10-2,10-1}Table 19: ADAM	ao/	1 - β1	€		λwd		T	finitial	[10-1,103]	[10-3,1]	[10-2,1010]	[10-5,10-2]	{0,10-2,10-1}	10-3final	[1,102]	[10-3,1]	[103,107]	10-4	10-1	10-3
Table 16: Momentum	no	1 - Y	λwd	T		f	initial	[10-3,1]	[10-3,1]	[10-5,10-2]	{0,10-2,10-1}		10-3	final	[10-2,1]	[10-3,1]	[10-4,10-3]		0		{10-4,10-3,10-2,10-1}Table 17: Nesterov	n7√	1 - Y	1 - P	€	λwd	T	finitial	[10-2,104]	0.1	[10-4,1]	[10-1o,1010]	[10-5,10-2]	{0,10-2,10-1}	10-3final	[10-2,1]	0.1	[10-2,1]	[10-8,10-3]	[10-4,10-3]	0	{10-4,10-3 10-2,10-1}Table 18: RMSProp	ao/	1 - β1	€	λwd	T	finitial	[1,102]	[10-3,1]	[1,104]	[10-5,10-3]	{0,10-2,10-1}	10-3final	[1,102]	[10-2, 1]	[10-2,102]	10-4	10-1	{10-4,10-3 10-2,10-1}Table 19: ADAM	ao/	1 - β1	€		λwd		T	finitial	[10-1,103]	[10-3,1]	[10-2,1010]	[10-5,10-2]	{0,10-2,10-1}	10-3final	[1,102]	[10-3,1]	[103,107]	10-4	10-1	10-3Table 20: NADAM18Under review as a conference paper at ICLR 2020D.4 Transformer on LM1B
Table 17: Nesterov	n7√	1 - Y	1 - P	€	λwd	T	finitial	[10-2,104]	0.1	[10-4,1]	[10-1o,1010]	[10-5,10-2]	{0,10-2,10-1}	10-3final	[10-2,1]	0.1	[10-2,1]	[10-8,10-3]	[10-4,10-3]	0	{10-4,10-3 10-2,10-1}Table 18: RMSProp	ao/	1 - β1	€	λwd	T	finitial	[1,102]	[10-3,1]	[1,104]	[10-5,10-3]	{0,10-2,10-1}	10-3final	[1,102]	[10-2, 1]	[10-2,102]	10-4	10-1	{10-4,10-3 10-2,10-1}Table 19: ADAM	ao/	1 - β1	€		λwd		T	finitial	[10-1,103]	[10-3,1]	[10-2,1010]	[10-5,10-2]	{0,10-2,10-1}	10-3final	[1,102]	[10-3,1]	[103,107]	10-4	10-1	10-3Table 20: NADAM18Under review as a conference paper at ICLR 2020D.4 Transformer on LM1BWe used linear learning rate decay for all experiments. We tuned the number of decay stepswithin [0.5, 1.0] times the number of training steps and the learning rate decay factor within{10-4, 10-3, 10-2, 10-1, 1}.
Table 18: RMSProp	ao/	1 - β1	€	λwd	T	finitial	[1,102]	[10-3,1]	[1,104]	[10-5,10-3]	{0,10-2,10-1}	10-3final	[1,102]	[10-2, 1]	[10-2,102]	10-4	10-1	{10-4,10-3 10-2,10-1}Table 19: ADAM	ao/	1 - β1	€		λwd		T	finitial	[10-1,103]	[10-3,1]	[10-2,1010]	[10-5,10-2]	{0,10-2,10-1}	10-3final	[1,102]	[10-3,1]	[103,107]	10-4	10-1	10-3Table 20: NADAM18Under review as a conference paper at ICLR 2020D.4 Transformer on LM1BWe used linear learning rate decay for all experiments. We tuned the number of decay stepswithin [0.5, 1.0] times the number of training steps and the learning rate decay factor within{10-4, 10-3, 10-2, 10-1, 1}.
Table 19: ADAM	ao/	1 - β1	€		λwd		T	finitial	[10-1,103]	[10-3,1]	[10-2,1010]	[10-5,10-2]	{0,10-2,10-1}	10-3final	[1,102]	[10-3,1]	[103,107]	10-4	10-1	10-3Table 20: NADAM18Under review as a conference paper at ICLR 2020D.4 Transformer on LM1BWe used linear learning rate decay for all experiments. We tuned the number of decay stepswithin [0.5, 1.0] times the number of training steps and the learning rate decay factor within{10-4, 10-3, 10-2, 10-1, 1}.
Table 20: NADAM18Under review as a conference paper at ICLR 2020D.4 Transformer on LM1BWe used linear learning rate decay for all experiments. We tuned the number of decay stepswithin [0.5, 1.0] times the number of training steps and the learning rate decay factor within{10-4, 10-3, 10-2, 10-1, 1}.
Table 21: SGD	αo	1 - β1	1 — β2	€initial	[10-5, 10-2]	[10-3, 5 X 10-1]	[10-4,10-1]	[10-9,10-5]final	[10-4, 10-2]	[10-3,1]一	[10-5,10-1]	[10-7,10-2]Table 25: ADAM	αo	1 - β1	1 — β2	€final	[10-5,10-2]	[10-3,1]	[10-5,10-1]	[10-9,10-5]Table 26: NADAM19Under review as a conference paper at ICLR 2020D.5 VGG ON CIFAR- 1 0 USING CODE FROM WILSON ET AL. (2017)D.5.1 Grid Search over Learning RateWe tuned over the same grid of initial learning rate values for each optimizer as Wilson et al. (2017).
Table 25: ADAM	αo	1 - β1	1 — β2	€final	[10-5,10-2]	[10-3,1]	[10-5,10-1]	[10-9,10-5]Table 26: NADAM19Under review as a conference paper at ICLR 2020D.5 VGG ON CIFAR- 1 0 USING CODE FROM WILSON ET AL. (2017)D.5.1 Grid Search over Learning RateWe tuned over the same grid of initial learning rate values for each optimizer as Wilson et al. (2017).
Table 26: NADAM19Under review as a conference paper at ICLR 2020D.5 VGG ON CIFAR- 1 0 USING CODE FROM WILSON ET AL. (2017)D.5.1 Grid Search over Learning RateWe tuned over the same grid of initial learning rate values for each optimizer as Wilson et al. (2017).
Table 27: SGD	no	1 - Yinitial	[10-3,1]	[10-3,1]final	[10-1,101]	[10-1,1]Table 28: Momentum	€	α0 √initial	[10-1o, 1010]	[10-2, 104]final	[10-2, 102]	[10-1, 101]Table 29: RMSPROP	€	ao/initial	[10-1o, 1010]	[10-2,104]final	[106,1010]	[10-1,101]Table 30: ADAM20Under review as a conference paper at ICLR 2020D.6 VGG ON CIFAR- 1 0 USING OUR CODEWe used linear learning rate decay for all experiments. We tuned the number of decay stepswithin [0.5, 1.0] times the number of training steps and the learning rate decay factor within{10-4, 10-3, 10-2, 10-1}.
Table 28: Momentum	€	α0 √initial	[10-1o, 1010]	[10-2, 104]final	[10-2, 102]	[10-1, 101]Table 29: RMSPROP	€	ao/initial	[10-1o, 1010]	[10-2,104]final	[106,1010]	[10-1,101]Table 30: ADAM20Under review as a conference paper at ICLR 2020D.6 VGG ON CIFAR- 1 0 USING OUR CODEWe used linear learning rate decay for all experiments. We tuned the number of decay stepswithin [0.5, 1.0] times the number of training steps and the learning rate decay factor within{10-4, 10-3, 10-2, 10-1}.
Table 29: RMSPROP	€	ao/initial	[10-1o, 1010]	[10-2,104]final	[106,1010]	[10-1,101]Table 30: ADAM20Under review as a conference paper at ICLR 2020D.6 VGG ON CIFAR- 1 0 USING OUR CODEWe used linear learning rate decay for all experiments. We tuned the number of decay stepswithin [0.5, 1.0] times the number of training steps and the learning rate decay factor within{10-4, 10-3, 10-2, 10-1}.
Table 30: ADAM20Under review as a conference paper at ICLR 2020D.6 VGG ON CIFAR- 1 0 USING OUR CODEWe used linear learning rate decay for all experiments. We tuned the number of decay stepswithin [0.5, 1.0] times the number of training steps and the learning rate decay factor within{10-4, 10-3, 10-2, 10-1}.
Table 31: Learning rate search ranges.
Table 32: SGD	no	1 - Y	λL2initial	[10-3, 101]	[10-3,1]	[10-5,10-2]final	[10-2,1]	[10-1,1]	[10-3,10-1]Table 33: Momentum	α0∕√^	1-γ	1 - P		入L2initial	[10-2,104]	[10-3,1]	[10-3, 1]	[10-1o, 101o]	[10-5,10-2]final	[10-2,1]	[10-1,1]	[10-3,10-2]	[102,106]	[10-3,10-1]Table 34: RMSPROP	αo∕	1 - β1	1 - β2	€	λL2initial	[10-2,104]	[10-3,1]	[10-4, 10-1]	[10-1o,101o]	[10-5,10-2]final	[10-2, 101]	[10-1,1]	[10-4,10-1]	[106,1010]	[10-3,10-1]Table 35: ADAM21Under review as a conference paper at ICLR 2020D.7 CNN ON CIFAR- 1 00D.7.1 Tuning Constant Learning RateWe fixed all optimizer hyperparameters excluding the learning rate to match those specified inSchneider et al. (2019).
Table 33: Momentum	α0∕√^	1-γ	1 - P		入L2initial	[10-2,104]	[10-3,1]	[10-3, 1]	[10-1o, 101o]	[10-5,10-2]final	[10-2,1]	[10-1,1]	[10-3,10-2]	[102,106]	[10-3,10-1]Table 34: RMSPROP	αo∕	1 - β1	1 - β2	€	λL2initial	[10-2,104]	[10-3,1]	[10-4, 10-1]	[10-1o,101o]	[10-5,10-2]final	[10-2, 101]	[10-1,1]	[10-4,10-1]	[106,1010]	[10-3,10-1]Table 35: ADAM21Under review as a conference paper at ICLR 2020D.7 CNN ON CIFAR- 1 00D.7.1 Tuning Constant Learning RateWe fixed all optimizer hyperparameters excluding the learning rate to match those specified inSchneider et al. (2019).
Table 34: RMSPROP	αo∕	1 - β1	1 - β2	€	λL2initial	[10-2,104]	[10-3,1]	[10-4, 10-1]	[10-1o,101o]	[10-5,10-2]final	[10-2, 101]	[10-1,1]	[10-4,10-1]	[106,1010]	[10-3,10-1]Table 35: ADAM21Under review as a conference paper at ICLR 2020D.7 CNN ON CIFAR- 1 00D.7.1 Tuning Constant Learning RateWe fixed all optimizer hyperparameters excluding the learning rate to match those specified inSchneider et al. (2019).
Table 35: ADAM21Under review as a conference paper at ICLR 2020D.7 CNN ON CIFAR- 1 00D.7.1 Tuning Constant Learning RateWe fixed all optimizer hyperparameters excluding the learning rate to match those specified inSchneider et al. (2019).
Table 36: Learning rate search ranges.
Table 37: Learning rate search ranges.
Table 38: Momentum	a/e	1 - βι	1 — β2	einitial	[10-2, 10-2]	[10-3,1]	[10-4,10-1]	[10-10,10-10]final	[10-1,10-1]	[10-2,1]	[10-4,10-1]	[102,106]Table 39: ADAMD.7.4 TUNING LEARNING RATE SCHEDULE & {γ, β1, β2, }We used linear learning rate decay, and tuned the number of decay steps within [0.5, 1.0] times thenumber of training steps and the learning rate decay factor within {10-4, 10-3, 10-2, 10-1}. ForSGD, we reused the results from Appendix D.7.2, since there were no additional hyperparametersto tune.
Table 39: ADAMD.7.4 TUNING LEARNING RATE SCHEDULE & {γ, β1, β2, }We used linear learning rate decay, and tuned the number of decay steps within [0.5, 1.0] times thenumber of training steps and the learning rate decay factor within {10-4, 10-3, 10-2, 10-1}. ForSGD, we reused the results from Appendix D.7.2, since there were no additional hyperparametersto tune.
Table 40: Momentum22Under review as a conference paper at ICLR 2020	ao/	1 - β1	1 — β2	€initial	[10-1,101]	[10-3,1]	[10-4,10-1]	[102,106]final	[10-1,101]	[10-3,1]	[10-5,10-2]	[102,106]Table 41: ADAM23Under review as a conference paper at ICLR 2020D.8 LSTM ONWAR AND PEACED.8.1 Tuning constant Learning Rate	ηfinal	[10-2, 101]Table 42: SGD	η	1 - Yfinal	[10-4,1]	0.99Table 43: Momentum___________η 1 - Yfinal [10-4,1] 0.99	a/e	1 - β1	1 - β2	e
Table 41: ADAM23Under review as a conference paper at ICLR 2020D.8 LSTM ONWAR AND PEACED.8.1 Tuning constant Learning Rate	ηfinal	[10-2, 101]Table 42: SGD	η	1 - Yfinal	[10-4,1]	0.99Table 43: Momentum___________η 1 - Yfinal [10-4,1] 0.99	a/e	1 - β1	1 - β2	efinal	[10-5, 10-2]	0.9	0.999	10-8Table 44: ADAMD.8.2 TUNING LEARNING RATE SCHEDULE & {γ, β1, β2, }We used linear learning rate decay, and tuned the number of decay steps within [0.5, 1.0] times thenumber of training steps and the learning rate decay factor f within the values shown in the tablesbelow.
Table 42: SGD	η	1 - Yfinal	[10-4,1]	0.99Table 43: Momentum___________η 1 - Yfinal [10-4,1] 0.99	a/e	1 - β1	1 - β2	efinal	[10-5, 10-2]	0.9	0.999	10-8Table 44: ADAMD.8.2 TUNING LEARNING RATE SCHEDULE & {γ, β1, β2, }We used linear learning rate decay, and tuned the number of decay steps within [0.5, 1.0] times thenumber of training steps and the learning rate decay factor f within the values shown in the tablesbelow.
Table 43: Momentum___________η 1 - Yfinal [10-4,1] 0.99	a/e	1 - β1	1 - β2	efinal	[10-5, 10-2]	0.9	0.999	10-8Table 44: ADAMD.8.2 TUNING LEARNING RATE SCHEDULE & {γ, β1, β2, }We used linear learning rate decay, and tuned the number of decay steps within [0.5, 1.0] times thenumber of training steps and the learning rate decay factor f within the values shown in the tablesbelow.
Table 44: ADAMD.8.2 TUNING LEARNING RATE SCHEDULE & {γ, β1, β2, }We used linear learning rate decay, and tuned the number of decay steps within [0.5, 1.0] times thenumber of training steps and the learning rate decay factor f within the values shown in the tablesbelow.
Table 45: SGD	no	1 - Y		f	initial	[10-4,1]	[10-3,1]	{10-4,10-3,10-2,10-1}final	[10-1,101]	[10-2,1]	{10-4,10-3,10-2,10-1}Table 46: Momentum	ao/e	1 - β1	1 — β2	e		f	initial	[10-2,104]	[10-3,1]	[10-4,10-1 ]	[10-1o, 1010]	{10-4,10-3,10-2,10-1}final	[1, 102]	[10-2,1]	0.999	[1,104]		10-3	Table 47: ADAM24Under review as a conference paper at ICLR 2020E Additional plotsCNN on CIFAR-100 with Momentum0.540.520.500.480.460.440.42
Table 46: Momentum	ao/e	1 - β1	1 — β2	e		f	initial	[10-2,104]	[10-3,1]	[10-4,10-1 ]	[10-1o, 1010]	{10-4,10-3,10-2,10-1}final	[1, 102]	[10-2,1]	0.999	[1,104]		10-3	Table 47: ADAM24Under review as a conference paper at ICLR 2020E Additional plotsCNN on CIFAR-100 with Momentum0.540.520.500.480.460.440.42IO0 IO-3IO'2	IO_11 — 7IO0
Table 47: ADAM24Under review as a conference paper at ICLR 2020E Additional plotsCNN on CIFAR-100 with Momentum0.540.520.500.480.460.440.42IO0 IO-3IO'2	IO_11 — 7IO0Learning rateFigure 5: Example plot of final validation error projected onto the axes of the hyperparameter space.
