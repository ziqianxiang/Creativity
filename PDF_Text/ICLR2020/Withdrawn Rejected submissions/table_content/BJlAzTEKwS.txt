Table 1: Maximum average return after 1M (2M for Humanoid (rllab) and 600k forSparseHumanoid-v2) time steps 5 random seeds. Bold: best methods when the gap is less than100 units. See appendix for average return with standard deviation. Environment short names: HC:HalfCheetah-v2, Hu: Humanoid-v2, Standup: HumanoidStandup-v2Figure 4 displays the performance of all algorithms on three environments over time steps (seeAppendix Figure 7 for all environments). Results are averaged over 5 random seeds. Table 1 reportsthe best observed reward for each method.
Table 2: Maximum average return after 1M (2M for Humanoid (rllab) and 600k forSparseHumanoid-v2) time steps ± one standard deviation on 5 random seeds. Bold: bestmethods when the gap is less than 100 units.
Table 3: Performance after 1M (except for rllab which is 2M) timesteps on 5 seeds. Values takenfrom their corresponding papers. N/A means the values were not available in the original paper.
Table 4: ARAC parameters.						16Under review as a conference paper at ICLR 2020Impact of number of flows on the policy shapeWe used a single SAC agent with different radial flows numbers and randomly initialized weights,starting with actions centered at (0, 0). All flow parameters are `1 regularized with hyperparameter2. The agent is trained with the classical evidence lower bound (ELBO) objective augmented withthe AR loss (Eq. 1), where the coefficient of the repulsive policy π0 is given by βt =器.Fig. 8shows how both the NF and learned variance Gaussian policies manage to recover the target policy.
