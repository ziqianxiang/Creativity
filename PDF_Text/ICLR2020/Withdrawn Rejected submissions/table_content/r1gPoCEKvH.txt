Table 1: Supernet architecture. CB - choice Table 2: Results of building block search. SPSblock. GAP - global average pooling. The - single path supernet.
Table 3: Results of channel search. * Perfor-mances are reported in the form “x (y)”, where“x” means the accuracy retrained by us and “y”means accuracy reported by the original paper.
Table 4: Compared with state-of-the-art NAS methods (Wu et al., 2018a; Cai et al., 2018) using thesame search space. The latency is evaluated on a single NVIDIA Titan XP GPU, with batchsize =32. Accuracy numbers in the brackets are reported by the original papers; others are trained by us.
Table 5: Results of mixed-precision quantizationsearch. “kWkA” means k-bit quantization for allthe weights and activations. DNAS (Wu et al.,2018b).
Table 6: Search Cost. Gds - GPU daysthat our approach clearly uses less memory thanother two methods because of the single path supernet. And our approach is much more efficientoverall although we have an extra search step that costs less than 1 GPU day. Note Table 6 only com-pares a single run. In practice, our approach is more advantageous and more convenient to use whenmultiple searches are needed. As summarized in Table 7, it guarantees to find out the architecturesatisfying constraints within one search. Repeated search is easily supported.
Table 7: Overview and comparison of SOTA weight sharing approaches. Ours is the easiest to train,occupies the smallest memory, best satisfy the architecture (latency) constraint, and easily supportsthe large dataset. Note that those approaches belonging to the joint optimization category (Eq. (3))have “Supernet optimization” and “Architecture search” columns merged.
