Table 1: Accuracy and sparsity of ResNet20 and En2ResNet20 under different attacks and β, withλ = 1E - 6. (Unit: %, n/a: do not perform sparsification. Same for all the following tables.)I	ResNet20							En2ResNet20			β	AI	A2	A3	A4	Sparsity ∣ Ai		A2	A3	A4	Sparsityn/a	76.07	51.24	47.25	59.30	0	80.34	57.11	50.02	66.77	00.01	70.26	46.68	43.79	55.59	80.91	72.81	51.98	46.62	63.10	89.860.1	73.45	49.48	45.79	57.72	56.88	77.78	55.48	49.26	65.56	70.550.5	74.08	50.64	46.67	57.24	39.92	78.47	56.13	49.54	65.57	56.34Second, we verify the effectiveness of RGSM in channel pruning. We lists the accuracy and chan-nel sparsity of ResNet20, En2ResNet20, and En5ResNet20 in Table 2. Without any sparsification,En2ResNet20 improves the four type of accuracies by 4.27% (76.07% vs. 80.34%), 5.87% (51.24%vs. 57.11%), 2.77% (47.25% vs. 50.02%), and 7.47% (59.30% vs. 66.77%), resp. When we setβ = 1, λ1 = 5e - 2, and λ2 = 1e - 5, after channel pruning both natural and robust accuracies ofResNet20 and En2ResNet20 remain close to the unsparsified models, but En2ResNet20’s weightsare 33.48% (41.48% vs. 8%) sparser than that of ResNet20’s. When we increase the channel sparsitylevel by increasing λ1 to 1e - 1, both the accuracy and channel sparsity gaps between ResNet20 and1For NAttack, we use the default parameters in https://github.com/cmhcbb/attackbox.
Table 2: Accuracy and sparsity of different EnResNet20. (Ch. Sp.: Channel Sparsity)Net	β	λ1	λ2	Ai	A2	A3	A4	A5	Ch. Sp.
Table 3: PerformanCe of En2ReSNet20 and ReSNet38 Under RVSM.
Table 4: Contrasting ADMM versus RVSM for the AT ResNet20.
Table 5: Accuracies of ResNet20 and En2ResNet20 with and without skip connections.
Table 6: Sparsity and accuracies of ResNet20 and En2ResNet20 without skip connection underdifferent pruning algorithms.
Table 7: Accuracy and sparsity of different Ensembles of ResNet20's on the CIFAR100.
