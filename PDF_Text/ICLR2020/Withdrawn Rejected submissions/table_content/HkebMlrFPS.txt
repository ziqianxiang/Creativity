Table 1: Examples of the codebook embeddings which are predicted by models with different Kgiven an input phrase or an unseen input sentence. The embedding in each row is visualized by thethree words whose GloVe embeddings are closest to the codebook embedding (in terms of cosinedistances) and their corresponding cosine similarities.
Table 2: Performance of phrase similarity tasks. Every model is trained on a lowercased corpus. InSemEval 2013, AUC (%) is the area under the precision-recall curve of classifying similar phrasepairs. In Turney, we report the accuracy (%) of predicting the correct similar phrase pair among5 or 10 candidate pairs. In BiRD and WikiSRS, the correlation coefficient (%) between predictedsimilarity and ground truth similarity is presented. *The results are taken from Yu & Dredze (2015).
Table 3: Pearson correlation (%) in the develop-ment and test sets in the STS benchmark. Theperformances of all sentence pairs are indicatedas All. Low means the performances on the halfof sentence pairs with lower similarity (i.e., STSBLow). f indicates the methods which use train-ing distribution to approximate testing distribu-tion. The best score with and without f are high-lighted.
Table 4: Hypernym detection performancesin the development and test set of HypeNet.
Table 5: The ROUGE F1 scores of differentmethods on CNN/Daily Mail dataset. The re-sults with f are taken from Zheng & Lapata(2019). The results with * are taken from Ce-Iikyilmaz et al. (2018).
Table 6: Compare BERT Large with Ours in Table 2.
Table 7: Compare BERT Large with Ours in Ta-ble 3.
Table 8: Compare BERT Large with Ours inTable 4.
