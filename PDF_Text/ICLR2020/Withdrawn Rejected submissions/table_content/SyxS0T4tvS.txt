Table 1: Development set results for base models pretrained over BookCorpus and Wikipedia.
Table 2: Perplexity on held-out validation data and dev set accuracy on MNLI-m and SST-2 forvarious batch sizes (# sequences) as we vary the number of passes (epochs) through the Books +Wiki data. Reported results are medians over five random initializations (seeds). The learning rateis tuned for each batch size. All results are for BERTBASE with full-sentence inputs.
Table 3: Development set results for RoBERTa as we pretrain over more data (16GB → 160GB oftext) and pretrain for longer (100K → 300K → 500K steps). Each row accumulates improvementsfrom the rows above. RoBERTa matches the architecture and training objective of BERTLARGE .
Table 4: Results on GLUE. All results are based on a 24-layer architecture. BERTLARGE andXLNetLARGE results are from Devlin et al. (2019) and Yang et al. (2019), respectively. RoBERTaresults on the dev set are a median over five runs. RoBERTa results on the test set are ensembles ofsingle-task models. For RTE, STS and MRPC we finetune starting from the MNLI model.
Table 5: Results on SQuAD. f indicates results that depend on additional external training data.
Table 6: Results on the RACE test set. BERTLARGE and XLNetLARGE results from Yang et al. (2019).
Table 7: Comparison between the published BERTBASE results from Devlin et al. (2019) to ourreimplementation with either static or dynamic masking. We report F1 for SQuAD and accuracy forMNLI-m and SST-2. Reported results are medians over 5 random initializations (seeds). Referenceresults are from Yang et al. (2019). We find that our reimplementation with static masking performssimilar to the original BERT model, and dynamic masking is comparable or slightly better than staticmasking.
Table 8: Development set results on GLUE tasks for various configurations of RoBERTa. Allresults are a median over five runs.
Table 9: Hyperparameters for pretraining RoBERTaLARGE and RoBERTaBASE .
Table 10: Hyperparameters for finetuning RoBERTaLARGE on RACE, SQuAD and GLUE. We selectthe best hyperparameter values based on the median of 5 random seeds for each task.
Table 11: Results on SuperGLUE. All results are based on a 24-layer architecture. RoBERTa resultson the development set are a median over five runs. RoBERTa results on the test set are ensemblesof single-task models. Averages are obtained from the SuperGLUE leaderboard.
Table 12: Results on XNLI (Conneau et al., 2018) for RoBERTaLARGE in the TRANSLATE-TESTsetting. We report macro-averaged accuracy (∆) using the provided English translations of the XNLItest sets. RoBERTa achieves state of the art results on all 15 languages.
