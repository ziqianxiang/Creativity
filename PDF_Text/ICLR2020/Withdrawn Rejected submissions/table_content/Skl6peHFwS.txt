Table 1: Feature performance across various ml algorithmsFEATURE	Machine learning Algorithm									Name	Naive Bayes	Linear LRegression	SVM	KNN	DT	Bagging RF	Boosting Xgb	HAN	CNN1.BoW 2.TF-IDF	0.720	0.743	0.741	0.706	0.739	0.737	0.743		Word level	0.747	0.752	0.741	0.732	0.738	0.735	0.741		N-gram level	0.742	0.742	0.746	0.712	0.736	0.710	0.736		Character	0.739	0.751		0.725	0.738	0.724	0.743		level									3.Word Embeddings 4.PDC	0.741	0.741	0.740	0.735	0.736	0.736	0.736	0.600	0.672FeaturesThe word embeddings as features were based on the GloVe vector representations (Twitter.27B.100d dataset)with 1193514 word vectors. These were only used for the HAN and CNN models and not on the other modelsas shown in table 1.
