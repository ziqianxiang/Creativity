Table 1: Mean absolute error of prediction Kᵖʳᵉᵈ with respect to ground-truth Kᵍᵗ. The results areobtained on SQuAD Test Split1 and Test Split2.
Table 2: Comparison of BLEU-4 scores with existing models on SQuAD Test Split1 and Test Split2.
Table 3: Comparison among pre-training methods of the question generator in ASGen, i.e, withoutpre-training, pre-training on NS, pre-training on AS, pre-training on AS without conditioning ona given answer. Note that we use Small-Wiki for comparison of pre-training except those entriesincluding ”(Full-Wiki)”.
Table 4: Effects of pre-training on answer-containing sentence generation (AS) on other existingmethods. We use Small-Wiki data to pre-train existing models. Those models with * are reproduced.
Table 5: Comparison of EM/F1 scores of fine-tuned MRC model on SQuAD v1.1, SQuAD v2.0,and KorQuAD dev sets using their corresponding synthetic data for pre-training.
Table 6: Comparison of EM/F1 scores of the BERT fine-tuned on QUASAR-T dataset. The syntheticdata are generated from ASGen trained on SQuAD-v1.1.
Table 7: Examples from SQuAD-v1.1 dev set demonstrating generated questions. We compare ourmethod (AS) with NS.  Colored Text  indicates a given answer.
Table 8: Additional experiments for effectiveness of AS on SQuAD Test-Split3. We use Small-Wikidata to pre-train existing models. Those models with * are reproduced.
Table 9: Standard errors of EM/F1 scores in downstream MRC tasks with ASGen.
