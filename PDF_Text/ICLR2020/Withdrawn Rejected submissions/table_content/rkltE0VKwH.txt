Table 1: Multi-agent intrinsic rewards for agent i, with μ(θi) = n1 Pj f (θi)independent	minimum	covering	burrowing	leader-followerfi(θi)	min	fj(θi)	fi(θi)1	[fi(θi)	>μ(θi)]	fi(θi)1	[fi(θi)	<	μ(θi)]	See textj∈{1...n}In Table 1 we define the intrinsic rewards that we use in our experiments. Independent rewardsare analagous to single-agent approaches to exploration which define the intrinsic reward for anagent as the novelty of their new and own observation that occurs as a result of an action. Theremainder of intrinsic reward functions that we consider use the novelty functions of other agents,in addition to their own, to further inform their exploration.
Table 2: # of treasures found with standard deviation across 6 runs. Scoreswhere the best mean score falls within one standard deviation of the scoredistribution are bolded.
Table 3: Hyperparameter settings across all runs in gridworld.		Name	Description	ValueQ lr	learning rate for centralized critic	0.001Q optimizer	optimizer for centralized critic	Adam (Kingma & Ba, 2014)π lr	learning rate for decentralized policies	0.001π optimizer	optimizer for decentralized policies	AdamΠ lr	learning rate for policy selector	0.04Π optimizer	optimizer for policy selector	SGDτ	target function update rate	0.005bs	batch size	1024total steps	number of total environment steps	1e6steps per update	number of environment steps between updates	100niters	number of iterations per update	50max ep length	maximum length of an episode before resetting	500Ψ penalty	coefficient for weight decay on parameters of Q-function	0.001Θ penalty	coefficient on L2 penalty on pre-softmax output of policies	0.001θ penalty	coefficient for weight decay on parameters of policy selector	0.001|D|	maximum size of replay buffer	1e6α	action policy reward scale	100η	selector policy reward scale	5
Table 4: Hyperparameter settings across all runs in VizDoom (only wheredifferent from Table 3).
