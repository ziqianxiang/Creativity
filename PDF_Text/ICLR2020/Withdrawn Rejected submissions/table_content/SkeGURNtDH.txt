Table 1: Differences between classifica-tion and image retrieval: Retrieval archi-tectures incorporate a final pooling layerthat is regionalized (RMAC of Tolias &Jegou (2014)) or magnifies activations(GeM of Radenovic et al. (2018)). Thetriplet loss (Gordo et al., 2016) requires abatching strategy with pairs of matchingimages.
Table 3: Instance search results andbaselines, on Holidays (% mAP) andUKB (/4). We set p = 3 pooling attraining time for our MultiGrain mod-els, and P* set as given in section 3.5.
Table E.1: Margin loss and data-augmentation parametersparameter	valuemargin α	0.2initial β0	1.2β learning rate 0.1We consider the SGD training of an SVMfw (Pi) = w T Pi	(D.2)using the Hinge loss。hinge = max(1 - y- fw(Pi), 0)∙	(D.3)We consider the symmetry across the x-axisφ((pix,piy)) = φ((pix, -piy))	(D.4)as a label-preserving data-augmentation suited to our synthetic dataset. We train the SVM(equation D.2) using one pass through the data-augmented dataset D of size 4N, usingbatches of size 2.
Table E.2: full data-augmentation transforms and parameterstransformation	parameter rangehorizontal flip	random resized crop	scale ∈ [0.08, 1.0] ratio ∈ [3/4, 4/3]color jitter	brightness 0.3 contrast 0.3 saturation 0.3lighting transform	intensity 0.1Table D.1: Full results including Copydays + 10k distractors (CD10k, % mAP), and ablationstudy for the MultiGrain models. The Pytorch model simply extract the last activation layeras a descriptor (Babenko et al., 2014). Resnet-50 corresponds to features extracted from aclassification baseline with p = 1 or p = 3 GeM pooling, trained with cross-entropy with ourtraining schedule, data augmentation, and uniform batch sampling.
Table D.1: Full results including Copydays + 10k distractors (CD10k, % mAP), and ablationstudy for the MultiGrain models. The Pytorch model simply extract the last activation layeras a descriptor (Babenko et al., 2014). Resnet-50 corresponds to features extracted from aclassification baseline with p = 1 or p = 3 GeM pooling, trained with cross-entropy with ourtraining schedule, data augmentation, and uniform batch sampling.
Table E.1: Additional top-1/top-5 validation classification accuracies obtained by finetuningP* for higher evaluation scales from off-the-shelf networks: NASNet (ZoPh et al., 2018),SENet (Hu et al., 2018) and PNASNet (Liu et al., 2018). The first column indicatesthe training resolution s and the accuracy we measured at this resolution, with standardevaluation (resize of the largest scale to S ∙ 256/224 + center crop). The subsequent columnsshow the accuracy measured at higher resolutions s* = 350, 400, 450, 500 without croPPing,together with the p* found by finetuning for these resolutions (appendix G).
