Table 1: Index size divided by weight data size when weights are quantized to be Q bits and prunedto obtain sparsity S.
Table 2:  (MNIST LeNet-5 accuracy using different rank k.  At the 20Kᵗʰ  iteration,  we prune alllayers using magnitude-based pruning method (Han et al., 2015) except FC1 layer where pruning isperformed by using Algorithm 1.  Retraining is completed at the 60Kᵗʰ iteration.  The test accuracyof pre-trained model is 99.2%.
Table 3:  Memory footprint of FC1 layer is compared with various index formats assuming thatweights are quantized to be 2 bits (note that index size takes substantial memory footprint for theprevious schemes). Accuracy is higher than 99.0% for all formats.
Table 4: Index compression ratio (compared with binary index scheme) and accuracy of the proposedpruning-index compression method on various DNN models (involving convolutional layers andLSTM layers as well).
Table 5: Compression results on FC5 and FC6 layers of AlexNet with various index formats when Sis to be the same as 0.91 for both layers and non-zero weights are quantized to be 2 bits. Other thanthe propose scheme, index size is much larger than non-zero weight size.  Top-1 accuracy is higherthan 57.0% for all formats.
Table 6: Model accuracy of ResNet32 with CIFAR-10 using different ranks and pruning rates. Thebottom row presents the results of baseline pruning method without binary matrix factorization.
