Table 1: Compression rate on enwik8 for different methodsxt-n+1, ..., xt. Concretely, we have an embedding vector for each n-gram, and for a given timestep, we average the vectors corresponding to the n-grams of different lengths. For large n, thereare many rare n-grams, leading to an important increase of parameters and memory consumption. Apotential solution would be to only keep the most frequent ones, but this would require computing adictionary of n-grams and add it to the archive. Instead, we propose to hash the n-grams into a fixednumber of bins, allowing to constrain the memory requirement, without having to increase the sizeof the archive.
Table 2: Compression rateon the full enwik8 data formodels with different contextsizes.
