Table 1: Success rate (mean ± standard error) of the goal-conditioned policy when trained with differentexploration bonuses in addition to the extrinsic reward Re (t). We report results at ~5 × 105 timesteps forMultiRoomN3S4, MultiRoomN5S4 and at ~8.2 × 106 timesteps for MultiRoomN6S25. We also report theperformance of InfoBot for completeness. Note that for rooms of size 4 (MultiRoomN3S4, MultiRoomN5S4),incentivizing to visit corners and doorways (Heuristic Decision States) is equivalent to count-based exploration.
Table 2: Success rate (mean ± standard error) of the goal-conditioned policy when trained with differentexploration bonuses in addition to the extrinsic reward Re (t). We report results at 〜5 X 105 timesteps forMUltiRoomN3S4, MUltiRoomN5S4 and at 〜8.2 X 106 timesteps for MUltiRoomN6S25. We also reportresults from InfoBot (Goyal et al., 2019) for completeness. Note that for rooms of size 4 (MultiRoomN3S4,MUltiRoomN5S4), incentivizing to visit corners and doorways (HeUristic Decision States) is eqUivalent tocoUnt-based exploration.
