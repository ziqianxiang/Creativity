Table 1: Results on MathQA dataset testing setMODEL	Operation Accuracy(%)	Execution Accuracy(%)SEQ2PROG-orig	59.4	519SEQ2PROG-best	66.97	54.0TP2LSTM (ours)	68.84	54.61LSTM2TP (ours)	68.21	54.61TP-N2F (ours)	71.89	55.954.2	Generating program trees from natural-language descriptionsGenerating Lisp programs requires sensitivity to structural information because Lisp code can beregarded as tree-structured. Given a natural-language query, we need to generate code containingfunction calls with parameters. Each function call is a relational tuple, which has a function as therelation and parameters as arguments. We evaluate our model on the AlgoLisp dataset for this taskand achieve state-of-the-art performance. The AlgoLisp dataset (Polosukhin & Skidanov, 2018) is aprogram synthesis dataset. Each sample contains a problem description, a corresponding Lisp pro-gram tree, and 10 input-output testing pairs. We parse the program tree into a straight-line sequenceof tuples (same style as in MathQA). AlgoLisp provides an execution script to run the generatedprogram and has three evaluation metrics: the accuracy of passing all test cases (Acc), the accuracyof passing 50% of test cases (50p-Acc), and the accuracy of generating an exactly matching program(M-Acc). AlgoLisp has about 10% noisy data (details in the Appendix), so we report results both onthe full test set and the cleaned test set (in which all noisy testing samples are removed). TP-N2F is
Table 2: Results of AlgoLisp dataset	Full Testing Set			Cleaned Testing Set		MODEL (%)	Acc	50p-Acc	M-Acc	Acc	50p-Acc	M-AccSeq2Tree	61.0					LSTM2LSTM+atten	67.54	70.89	75.12	76.83	78.86	75.42TP2LSTM (ours)	72.28	77.62	79.92	77.67	80.51	76.75LSTM2TPR (ours)	75.31	79.26	83.05	84.44	86.13	83.43SAPSpre-VH-Att-256	83.80	87.45		92.98	94.15	TP-N2F (ours)	84.02	88.01	93.06	93.48	94.64	92.78compared with an LSTM seq2seq with attention model, the Seq2Tree model in Polosukhin & Ski-danov (2018), and a seq2seq model with a pre-trained tree decoder from the Tree2Tree autoencoder(SAPS) reported in Bednarek et al. (2019). As shown in Table 2, TP-N2F outperforms all existingmodels on both the full test set and the cleaned test set. Ablation experiments with TP2LSTM andLSTM2TP show that, for this task, the TP-N2F Decoder is more helpful than TP-N2F Encoder. Thismay be because lisp codes rely more heavily on structure representations.
