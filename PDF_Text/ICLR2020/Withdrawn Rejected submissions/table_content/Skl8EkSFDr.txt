Table 1: GAN compression comparison (network pruning)Generator(s)	Discriminator	Loss Terms	ResultsTechnique	Compressed	Init Scheme	Init Scheme	Fixed	L-Gc	L-Dc	L-Go	L-Do	Qualitative	FID Score(a) No Compression	Dense	Random	Dense,Random	No	-	-	Yes	Yes	Good	6.113(b) Self-Supervised (ours)	Dense,Sparse	From Dense	Dense,Pretrained	No	Yes	Yes	Yes	Yes	Good	6.929(c) Small & Dense Network	Dense	Random	Dense,Random	No	-	-	Yes	Yes	Mode collapse	72.821(d) One-shot Pruning & Fine-Tuning	Sparse	From Dense	Dense,Pretrained	No	Yes	Yes	-	-	Facial artifacts	24.404(e) Gradual Pruning & Fine-Tuning	Sparse	From Dense	Dense,Random	No	Yes	Yes	-	-	Facial artifacts	35.677(f) Gradual Pruning during Training	Sparse	Random	Dense,Random	No	Yes	Yes	-	-	No faces	84.941(g) One-shot Pruning & Distillation	Dense,Sparse	From Dense	-	-	Yes	-	Yes	-	Mode collapse	45.461(h) (d) & Distillation	Dense,Sparse	From Dense	Dense,Pretrained	No	Yes	Yes	Yes	-	Color artifacts	38.985(i) (g) & Fix Original Loss	Dense,Sparse	From Dense	Dense,Pretrained	Yes	Yes	Yes	-	-	Facial artifacts	15.182(j) Adversarial Learning	Dense,Sparse	Random	Dense,Random	No	Yes	Yes	Yes	Yes	Mode collapse	92.721(k) Knowledge Distillation	Dense,Sparse	From Dense	Dense,Random	No	Yes	-	Yes	Yes	Mode collapse	103.094(l) Distill Intermediate (LIT)	Dense,Sparse	From Dense	Dense,Pretrained	Yes	-	-	-	-	No faces	194.026(m) E-M Pruning	Dense,Sparse	From Dense	Sparse,Pretrained	No	Yes	Yes	Yes	-	Color artifacts	159.767(n) G & D Both Pruning	Dense,Sparse	From Dense	Sparse,Pretrained	No	Yes	Yes	Yes	-	Mode collapse	46.453mnFigure 1: Various approaches to compress StarGAN with network pruning. Each group shows oneinput face translated with different methods of compressing the network: a. Uncompressed, b.
Table 2: Tasks and networks overviewTask	Network	Dataset	Resolution	FID Scores when Pruned to								0% (dense)	25%	50%	75%	90%Image Synthesis	DCGAN	MNIST	64x64	50.391	50.128	50.634	50.805	51.356Domain Translation	Pix2Pix	Sat → Map	256x256	17.636	17.897	17.990	20.235	24.892Domain Translation	Pix2Pix	Sat - Map	256x256	30.826	30.628	30.720	34.051	38.936Style Transfer	CycleGAN	Monet → Photo	256x256	63.152	63.410	63.662	66.394	70.933Style Transfer	CycleGAN	Monet - Photo	256x256	31.987	32.102	32.346	33.913	41.409Image-Image Translation	CycleGAN	Zebra → Horse	256x256	60.930	61.005	61.102	65.898	68.450Image-Image Translation	CycleGAN	Zebra — Horse	256x256	52.862	52.631	52.688	58.356	63.274Image-Image Translation	StarGAN	CelebA	128x128	6.113	6.307	6.929	6.714	7.144Super Resolution	SRGAN	DIV2K	≥ 512x512	14.653	15.236	16.609	17.548	18.3765	Application to New Tasks and NetworksFor the experiments in this section, we choose to prune individual weights in the generator. The finalsparsity rate is 50% for all convolution and deconvolution layers in the generator (more aggressivesparsities are discussed in Section 6). Following AGP (Zhu & Gupta, 2018), we gradually increasethe sparsity from 5% at the beginning to our target of 50% halfway through the self-supervisedtraining process, and we set the loss adjustment parameter λ to 0.5 in all experiments. We usePyTorch (Paszke et al., 2017), implement the pruning and training schedules with Distiller (Zmoraet al., 2018), and train and generate results with a V100 GPU (NVIDIA, 2017) using FP32 to match
Table 3: PSNR (dB), SSIM and FID indicators for Validation DatasetsDatasetOriginal GeneratorPSNR SSIM FIDFilter-Compressed GPSNR SSIM FIDElement-Compressed GPSNR SSIM FIDSet5	30.063393	0.852733	30.761999	30.234316	0.859817	35.514204	30.484014	0.862475	36.824148Set14	26.643850	0.716294	55.457409	27.314664	0.744525	82.118059	27.417112	0.744101	70.125821DIV2K.Validation	28.205665	0.778364	14.653151	28.875953	0.800625	18.499896	28.974868	0.800767	16.608606Dense	Filter-pruned Fine-grainedSuper Resolution. We apply self-supervised compression to SRGAN (Ledig et al., 2017)5, whichuses a discriminator network trained to differentiate between upscaled and the original high-resolution images. We trained SRGAN on the DIV2K data set Agustsson & Timofte (2017), and usethe DIV2K validation images, as well as Set5 Bevilacqua et al. (2012) and Set14 Zeyde et al. (2010)to report deployment quality. In this task, quality is often evaluated by two metrics: Peak Signal-to-Noise Ratio (PSNR) (Huynh-Thu & Ghanbari, 2008) and Structural Similarity (SSIM) (Wang et al.,2004). We also show FID scores (Heusel et al., 2017) for our results in the results summarized inTable 3, and a representative output is shown in Figure 7. These results also include filter-pruned
