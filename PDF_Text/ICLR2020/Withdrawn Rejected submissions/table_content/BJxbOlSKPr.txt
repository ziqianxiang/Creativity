Table 1: Summary of differences between VQ and SX. DPQ-SX allows more flexibility in distancemetrics and whether to tie the Key and Value metrices. DPQ-VQ is more efficient during training andtherefore is more scalable to larger K, D values.
Table 2: Datasets and models used in our experiments. More details in Appendix C.
Table 3: Comparisons of DPQ variants vs. the full embedding baselines.
Table 4: Comparison of DPQ against recently proposed embedding compression techniques onthe PTB LM task (LSTMs with three model sizes are studied). Metrics are perplexity (PPL) andcompression ratio (CR).
Table 5: Nearest neighbours of ‘_evolve’ in the embedding space.
Table 6: Nearest neighbours of ‘_monopoly’ in the embedding space.
Table 7: Nearest neighbours of ‘_Toronto’ in the embedding space.
Table 8: Examples of KD codes.
Table 9: Performance comparison on PTB language modeling task. The proposed method providessignificantly better compression ratio over baselines while achieving similar or better/smaller PPL.
Table 10: Performance comparison on text classification task. The accuracy and compression ratios(in parenthesis) are shown below. The proposed method (DPQ) usually achieve better accuracies thanbaselines, at the same time providing better compression ratios.
Table 11: Performance comparisons against the reconstruction baselines.
Table 12: Ablation study of DPQ-SX on whether or not to tie K and V matrices. By default, DPQ-SXdoes not tie these two matrices.
Table 13: Effect of using DPQ on BERT. DPQ gives a compression ratio of 37× on the embeddingtable while the model’s performance on downstream tasks remains competitive.
