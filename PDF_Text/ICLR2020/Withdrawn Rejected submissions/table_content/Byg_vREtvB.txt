Table 1: Results of posterior distillation when the student architecture is fixed to match the teacherarchitecture and the base data sets are used with no sub-sampling or occlusion.
Table 2: Performance comparison between Uo and Us estimators for convolutional neural networkon MNIST. The NLL results correspond to the case of distilling the posterior predictive distributionwhile the MAE on entropy results correspond to the case of distilling the expectation of predictiveentropy.
Table 3: Performance comparison between Uo and Us estimators for fully-connected network onMNIST. The NLL results correspond to the case of distilling the posterior predictive distributionwhile the MAE on entropy results correspond to the case of distilling the expectation of predictiveentropy.
Table 4: Performance comparison between Uo and Us estimators for convolutional neural network onCIFAR10. The NLL results correspond to the case of distilling the posterior predictive distributionwhile the MAE on entropy results correspond to the case of distilling the expectation of predictiveentropy.
