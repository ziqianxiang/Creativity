Table 1: CER for models trained on smaller clean (WSJ si284 subset) and noisy (CHiME tr05_multi)data. We view the softmax layer, the topmost projection layer and the topmost BLSTM layer as theclassifier. Joint training means all layers are trained jointly. The next experiments are to freeze theclassifier trained from the joint training then reinitialize and retrain the feature extractor. The finalexperiments are to freeze the classifier which trained on full si284 and train the feature extractors onsi284 SUbSet/tr05_multi. The right part of the table is the sum of the absolute values of the changesof all parameters of each BLSTM layer from epoch 1 to the epoch which has the lowest valid loss.
Table 2: CER for training on WSJ si284Model(WSJ si284)	dev_93	eval_92Baseline 1	TΣ4^^	9.7+ top-down training	10.8	8.2+ fix bottom layer	13.1	10.5Baseline 2	TΣ6^^	10.4+ top-down training	10.6	8.5Baseline 3	TΣ6^^	10.2+ top-down training	11.5	8.9Dropout 0.2	ɪŋ^^	8.7Dropout 0.5	9.6	7.6Dropout 0.7	11.1	8.9Dropout 0.5	^96	7.6+ top-down training	8.2	6.3Previous work (Kim et al., 2017)CTC		11.5	9.0Attention (location)	12.0	8.2CTC-Attention	11.3	7.4In this training strategy, if cut Cj fails to improve validation performance, then retraining takes placeusing cut Cj-1. Since the top layers learn faster, it is possible the top layers with cut Cj already
Table 3: Character error rate (CER) on CHiME-4tr05 + si284		dt_05(CER)	et_05 (CER)baseline		36.0+top-down training	23.4	34.2dropout 0.2	^∑8	34.7dropout 0.5	19.9	30.8dropout 0.7	20.3	30.3dropout 0.5 + top-down training	T85	28.7dropout 0.5 + fix the bottom layer	20.6	31.4middle layers to the classifier, which are less prone to overfitting. Furthermore, with more layers, theclassifier has increased capacity, freeing the feature extractor from needing to learn more complexfeatures from the noisy data.
Table 4: The CNN architecture for the CNN-BLSTM model	in_channel	out_channel	kernel size	strideconv	~L	^64	-y×3	1conv	64	64	-	-y×3	1maxpool			-2×2	2conv	"64	128	-y×3	1conv	128	128	一	-y×3	1maxpool			2 × 2	2Table 5: Character error rate (CER) for WSJ of the CNN-BLSTM modelModeI(WSJ si284)	dev_93	eval_92CNN-BLSTM	09	8.2+ top-down training	9.7	7.3Dropout 0.2	^99	7.5Dropout 0.5	10.3	7.6Dropout 0.7	11.6	9.3Dropout 0.2	~9.9	7.5+ top-down training	8.4	6.3BLSTM	TΣ6^^	10.2+ dropout 0.5 + top-down training	8.2	6.3Previous work (Kim et al., 2017)
Table 5: Character error rate (CER) for WSJ of the CNN-BLSTM modelModeI(WSJ si284)	dev_93	eval_92CNN-BLSTM	09	8.2+ top-down training	9.7	7.3Dropout 0.2	^99	7.5Dropout 0.5	10.3	7.6Dropout 0.7	11.6	9.3Dropout 0.2	~9.9	7.5+ top-down training	8.4	6.3BLSTM	TΣ6^^	10.2+ dropout 0.5 + top-down training	8.2	6.3Previous work (Kim et al., 2017)CTC		11.5	9.0Attention (location)	12.0	8.2CTC-AttentiOn	11.3	7.4Table 5 demonstrates our training method is still successful in training a much deeper hybrid CNN-BLSTM model. Furthermore, in this set of experiments, applying our training method solely outper-forms applying dropout solely. Moreover, with dropout (probability 0.2) combined with top-downtraining, further gain is obtained. In these experiments, the top-down training method for the CNN-BLSTM stops at the second top-most BLSTM layer. Further accuracy gains may obtained if we
Table 6: Perplexity of the 2-layer LSTM model.
