Table 1: Performance on the copying task (left) and sequential MNIST resolution task right). Error (CE onthe last 10 time steps) on the copying task. Note that while all of the methods are able to learn to copyfor the length seen during training, the RIMs model generalizes to sequences longer than those seen duringtraining whereas the LSTM, RMC, and NTM degrade. Sequential MNIST resolution: Test Accuracy % on theSequential MNIST resolution generalization task (see text) after 100 epochs. Both the proposed and the Baselinemodel (LSTM) were trained on 14x14 resolution but evaluated at different resolutions; results averaged over 3different trials.
Table 2: A concise comparison of recurrent models with modular memory.
Table 3: HyperparametersParameter	ValueOptimizer	Adam(Kingma & Ba, 2014)learning rate	7 ∙ ILbatch size	64Inp keys	64Inp Values	Size of individual RIM * 4Inp Heads	4Inp Dropout	0.1Comm keys	32Comm Values	32Comm heads	4Comm Dropout	0.1C.3	Future Architectural ChangesWe have not conducted systematic optimizations of the proposed architecture. We believe that even principledhyperparameter tuning may significantly improve performance for many of the tasks we have considered in thepaper. We briefly mention a few architectural changes which we have studied:• On the output side, we concatenate the representations of the different RIMs, and use the concatenatedrepresentation for learning a policy (in RL experiments) or for predicting the input at the next timestep (for bouncing balls as well as all other experiments). We empirically found that adding another
Table 4: Wikitext-2 resultsApproach	Num. Parameters	Train PPL	Valid PPL	Test PPLLSTM (2-layer)	21.2M	39.78	109.25	102.53Relational Memory (Santoro et al., 2018)	11M	n/a	112.77	107.21RIMs (2-layer, kT = 6, kA = 6)	23.7M	41.27	103.60	98.66We investigate the task of word-based language modeling. We ran experiments on the wikitext-2 dataset (Merityet al., 2016). We ran each experiment for a fixed 100 epochs. These results are in Table 4. Our goal in thisexperiment is to demonstrate the breadth of the approach by showing that RIMs performs well even on datasetswhich are noisy and drawn from the real-world.
Table 5: Error (CE for last 10 time steps) on the copying task. Note that while all of the methods are able tolearn to copy on the length seen during training, the RIMs model generalizes to sequences longer than thoseseen during training whereas the LSTM fails catastrophically.
Table 6: Imitation Learning: Results on the half-cheetah imitation learning task. RIMs outperforms a baselineLSTM when we evaluate with perturbations not observed during training (left). An example of an input imagefed to the model (right).
Table 7: Scores obtained using PPO with the LSTM architecture and PPO with the RIMs architecture withkA = 5.
