Table 1: Experiments on sample complexity for global attention model. np denotes the number oftrainable parameters in each model. All results are averaged over 5 runsNumber of training samples	Test loss: baseline (np = 34186)	Test loss: attention (n = 34442)	Test loss: regularized attention (n = 34442)10000	2.1063	0.5484	0.014314000	0.4109	0.0382	0.010716000	0.1811	0.0211	0.010018000	0.1072	0.0163	0.012220000	0.0769	0.0101	0.009850000	0.0511		0.0060			0.0072	To test the sample complexity, we generate multiple datasets, each containing 10k, 14k, 16k, 18k,20k and 50k unique samples respectively using the scheme mentioned in the previous section. Acommon test set of 5000 samples is created to evaluate each of the models. A regression modelis then trained on each of these datasets. All models are trained with SGD optimizer with a fixedlearning rate of 10-3. Table 1 reports test errors for baseline and attention models at 400k iterations7Under review as a conference paper at ICLR 2020as the number of training samples vary. We observe that attention models need fewer training samplesthan baseline models to achieve a desired error. For instance, to attain the desired error 0.07, attentionmodels need 14000 samples, whereas baseline models need 20000 samples. We would like to pointout that improvements obtained by attention models is not because of increase in model parameters.
Table 2: Experiments on sample complexity for self attention model. np denotes the number ofparameters used in each model. All results are averaged over 5 runs.
