Table 1: Automatic evaluation results. AC = Style Accuracy; BLR = Average BLEU score w.r.thuman reference sentences; PL = Perplexity; HM = Harmonic Mean of (AC, BLR); GM = GeometricMean of (AC, BLR); RL-ST is our model; H is Human reference. Lower PL is better. A ‘-’ indicatesthat we could not obtain results for the model on the dataset.
Table 2: Human evaluation results: each 3-set of rows indicates the percentage of sentences preferredfor each model in the pair (and ‘None’), down a column. Cont. = Content Preservation ; Flu. =Fluency ; Sty. = Target Style Match ; All = Overall.
Table 3: Examples of generated sentences by RL-ST. Each cell has the source sentence first and thegenerated sentence second.
Table 4: Ablation results AC = Style Accuracy; BLR = Average BLEU score w.r.t human referencesentences; PL = Perplexity; HM = Harmonic Mean of (AC, BLR); GM = Geometric Mean of (AC,BLR). Lower PL is better.
Table 5: Dataset statisticsA.3 Example OutputsTable 6 below compares our model’s outputs with those of previous works.
Table 6: Examples of generated sentences to be compared down a column (RL-ST is our model,SRC is the input sentence). Attributes are colored.
