Table 1: FID Scores (Heusel et al., 2017) for randomly generated samples (smaller is better).
Table 2: Disentanglement Metric on DSprites and Teapot dataset with Lasso and Random Forestregressor (Eastwood & Williams, 2018). For disentanglement and completeness higher score isbetter, for informativeness, lower is better.
Table 3: Details of model architectures used in the paper. All convolutions and transposed-convolutions are with stride 2 and padding 1. Unless stated otherwise, the layers have Parametric-RELU (Î± = 0.2) activation function, except the output layers of the pre-image maps which hassigmoid activation function.
Table 4: Datasets and hyperparameters used for the experiments. The bandwidth of the Gaussiankernel for generation corresponds to the bandwidth that gave the best performance determined bycross-validation on the MNIST classification problem.
