Table 1: The statistical measures for examining the necessary conditionsLayer	Activation expectation	Covariance	Weight expectationf1	E(X)	Cov(An, An0)	E(An)f2	E(F1)	Cov(Wk,Wk0)	E(Wk)fY	E(F2)	Cov(Cl,Cl0)	E(Cl){Xm}M=1 〜P (X), {fln}N=i 〜P (Fl), {f2k }K=1 〜P H)An = ^Pm=I Amn, Where αmn 〜P(Amn)Wk = Pn=I Wnk, where βnk 〜P(Wnk)Cl = PK=I Ckl, where Ykl 〜P(Ckl)It is noteworthy that the i.i.d. assumption for the parameters of DNNs indeed satisfy the necessaryconditions for activations being i.i.d.. Nevertheless, since the i.i.d. prior is not an appropriate priorfor the parameters of DNNs, it cannot guarantee activations of a hidden layer being i.i.d..
Table 2: The correspondence between hidden layers and Gibbs distributionsArchitecture	neuron f1n	fully connected layer fl = {fln}N=i	conv. channel fk	conv. layer fi = {fik}kK=1Explanation	energy function	discrete Gibbs	energy function	MRF	EF1n =-f1n	{P(f1n)}nN=1	EFi=-PK=1 g k(fi-1)	ZF-exp[pK=ι rk(fi-1)] FiWe use Equation 16 and Equation 17 as examples to illustrate the correspondence.
Table 3: The architecture of the MLP for MNIST classificationLayer	Description	Dimension	Filter dimension (M × N)x	Input	784	—f1	FC(sigmoid)	128	128 × 784f2	FC(sigmoid)	64	64 × 128fY	Output(softmax)	10	10× 64FC denotes the fully connected layerDimension: the number of nodes (i.e., neurons) in every layer.
Table 4: The architecture of the NN = {x; f ; fY } for MNIST classificationLayer	Description	Dimension	Filter dimension (M × N)x	Input	784	—f	FC(sigmoid)	128	128 × 784fY	Output(softmax)	10	10 × 128FC denotes the fully connected layerDimension: the number of nodes (i.e., neurons) in every layer.
Table 5: The architectures of the CNN for CIFAR-10 classificationLayer	Description	Output dimension Filters dimension Correlation matrixxf1f2f3f4fYInputConv (3 × 3) + MaxpoolConv (3 × 3) + MaxpoolFC + SigmoidFC + SigmoidOutput(softmax)32 × 32 × 316 × 16 × 648 × 8 × 1281024256103 × 3 × 3 × 643 × 3 × 64 × 1288192 × 10241024 × 256256 × 10
Table 6: The statistical measures for examining the necessary conditionsLayer	Activation expectation	Covariance	Weight expectationf1	E(X)	Cov(An, An0)	E(An)f2	E(F1)	Cov(Wk,Wk0)	E(Wk)fY	E(F2)	Cov(CI,Cl)	E(Cl){Xm}M=1 〜P (X), {fln}N=1 〜P (Fl), {f2k}K=1 ~ P ®An = ^Pm = ι Amn, Where amn 〜P(Amn)Wk = Pn=I Wnk, where βnk 〜P(Wnk)Cl = PK=I Cki, where γki - P(Ckl)F Activations being i.i.d. cannot be valid for all the hiddenlayers of DNNsIn this section, we demonstrate that activations being i.i.d. cannot be valid for all the hidden layersof DNNs through showing that the hidden layers of two typical DNNs, i.e, the MLP and the CNN,cannot satisfy the necessary conditions for activations being i.i.d..
Table 7: The statistical measures for examining the necessary conditions of each layer in the CNN	f1	f2	f3	f4	fYDescription	Conv Maxpool	Conv Maxpool	FC Sigmoid	FC Sigmoid	Output SoftmaxActivation Dim.	16×16×64	8×8× 128	1024×1	256×1	10×1Filter Dim.	3×3×3×64	3×3×64× 128	8192×1024	1024×256	256×10Weight Dim.	M×N (27×64)	N XK (576×128)	K×L (8192×1024)	L×Q (1024×256)	Q×R (256×10)Weight PDF	amn~P (Amn)	bnk 〜P(Bnk )	Ckl 〜P (Ckl)	dlq 〜P(Dlq )	fqr 〜P (Fqr)Sum. PDF	An =PmM=1 Amn	Bk = Pn=I Bnk	Cl =PkK=1 Ckl	Dq =PlL=1 Dlq	Fr =PqQ=1 FqrCovariance	Cov(An,An0)	COv(Bk ,Bk0 )	COv(Cl,Cl0)	COv(Dq,Dq0)	COv(Fr,Fr0)Sum. Exp.	E(An)	E(Bk )	E(Cl)	E(Dq)	E(Fr)Activation Exp.	E(X)	E(Fl)	E(F2)	E(F3)	E(F4)Dim. is short for dimension. Sum. is short for Weights Summation. Exp. is short for Expectation.
Table 8: One iteration of the backpropagation training procedure for the MLPLayer	Gradients update Vθt(i)H(fγ)	Parameters and activations update		fY	vfγ H(fY )vθt (Y )fY	J	θt+1 (Y)=θt+1 (Y )-α[Vθt(Y)H(fY)], fY (f2,θt+1 (Y ))	↑f2	Vfγ H(fY )Vf2 fγ Vθt(2)f2	J	θt+1(2)=θt+1(2)-α[Vθt(2)H(fY)], f2(f1,θt+1(2))	↑f1	Vfγ H(fγ )Vf2 fγ Vfi f2Vθt(1)f1	J	θt+1(1)=θt+1(1)-α[Vθt(1)H(fY)], f1(x,θt+1(1))	↑x	—		—	The uparrow and downarrow indicate the order of gradients and parameters(activations) update, respectively.
Table 9: The architectures of the CNN for synthetic image classificationR.V. LayerDescriptionCNNxf1f2f3f4fYXF1 F2 FYInputConv (3 × 3) + ReLUMaxpoolConv (5 × 5) + ReLUMaxpoolOutput(softmax)32 × 32 × 130 × 30 × 2015 × 15 × 2011 × 11 × 605 × 5 × 601×1×10R.V. is the random variable of the hidden layer(s).
Table 10: The architectures of CNNs for classifying the synthetic datasetR.V.	Layer	Description	CNN1	CNN2	CNN3X	x	Input	32 X 32 X 1	32 X 32 X 1	32 X 32 X 1F1	f1	Conv (3 × 3) +ReLU	30 X 30 X 4	30 X 30 X 12	30 X 30 X 36	f2	Maxpool	15 X 15 X 4	15 X 15 X 12	15 X 15 X 36F F2	f3	Conv (5 × 5) + ReLU	11 X 11 X 20	11 X 11 X 20	11 X 11 X 20	f4	Maxpool	5 X 5 X 20	5 X 5 X 20	5 X 5 X 20F3	f5	Fully connected	1 X 1 X 20^^	1 X 1 X 20	1 X 1 X 20FY	fY	Output(softmax)	-^1 X 1 X 10^^	1X1X10	1 X 1 X 10R.V. is the random variable of the hidden layer(s).					Since the synthetic dataset obeys the Gaussian distribution P (X), we can measure the quality of theprior distribution by averaging KLD[P (X)||P (F1)] over all the testing images for every trainingepoch. In addition, we use the testing error to measure the generalization performance. Also notethat the only difference between the tree CNNs is they have different K numbers of convolutionalchannels in f1 , i.e., different CNNs have different prior distributions. Therefore, we can examinethe effect of the prior distribution by checking the generalization performance of the three CNNs.
Table 11: The architectures of CNN for CIFAR-10 classificationLayer	Description	Outputx	Input	32 × 32 × 1f1	-_Conv (3 × 3)~~	30 × 30 × 12f2	ReLU	30 × 30 × 12f3	-_Conv (5 X 5)~~	26 × 26 × 32f4	ReLU + Maxpool	13× 13× 32f5	-_Conv (5 × 5)~~	9 × 9 × 128f6	ReLU + Maxpool	4 × 4 × 128f7	Fully connected	1 × 1 × 512f8	ReLU	1 × 1 × 512f9	Fully connected	1 × 1 × 32f10	ReLU	1 × 1 × 32fY	OUtPUt(SOftmax)	1×1×1038Under review as a conference paper at ICLR 20200.70.650.6----Testing error of CNN without any regularization (33.95%)
