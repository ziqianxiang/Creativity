Table 1: Existing GNNs described in our abstraction. GraphSAGE-P and GraphSAGE-LSTM are the pooling and LSTM variants of GraphSAGE, respectively. σ and max indicate element-wise non-linear activation and max functions. For sequential AGGREGATE, vi denotes the i-th in-neighbor of node v .			GNN	AGGREGATE({hUk-1) |u ∈N(v)})		UPDATE(a(vk), h(vk-1))Set Aggregate			GCN (Kipf and Welling, 2016) GIN (Xu et al., 2019)	(k) av (k) av	= Pu∈N(V) hUk-1) Pu∈N (v) hu	hVk)=σ(W(k) ∙ J」 hVk) = σ(W ∙ ((1 + Ak))hVk-D + aVk)))Sequential Aggregate			GCN-LSTM (Hamilton et al., 2017) N-ary Tree-LSTM (Tai et al., 2015)	(k) av (k) av	= LSTM(hV1-1),…,hVk-1)) =Tree-LSTM-Agg (hVIT),…,h⅞-1))	hVk) = σ(W(k) ∙ (aVk),hVkτ))) h(vk) = Tree-LSTM-Update(a(vk), h(vk-1) )Computation reduction in DNNs. Several techniques have been proposed to reduce computationin DNNs, including pruning weights (Han et al., 2015) and quantization (Han et al., 2016). Thesetechniques reduce computation at the cost of modifying networks, resulting in decreased accuracy (asreported in these papers). By contrast, we propose a new GNN representation that accelerates GNNtraining by eliminating redundancy in GNN-graphs while maintaining the original network accuracy.
Table 2: Datasets used in the experiments.
