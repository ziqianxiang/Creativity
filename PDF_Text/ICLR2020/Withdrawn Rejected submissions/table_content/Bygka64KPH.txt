Table 1: Semi-Supervised Meta-Learning + Ablation StudyModel	Omniglot 1-shot	Mini-Imagenet			1-shot	5-shotPNall(Snell et al.,2017)	98.8	49.4	68.2PN (Renetal., 2018)	94.62 ± 0.09	43.61 ± 0.27	59.08 ± 0.22MetaGAN(Zhang et al., 2018)	97.58 ± 0.07	50.35 ± 0.23	64.43 ± 0.27EGNN-Semi (Kim et al., 2019)	N/A	N/A	62.52 ± N/APNV AT (Ours)	97.14 ± 0.16	49.18 ± 0.22	66.94 ± 0.20PRWN (Ours)	98.28 ± 0.15	50.89 ± 0.22	67.82 ± 0.19PN + Semi-supervised inference(Ren et al., 2018)	97.45 ± 0.05	49.98 ± 0.34	63.77 ± 0.20PN + Soft K-means(Ren et al., 2018)	97.25 ± 0.10	50.09 ± 0.45	64.59 ± 0.28PN + Soft K-means + cluster(Ren et al., 2018)	97.68 ± 0.07	49.03 ± 0.24	63.08 ± 0.18PN + Masked soft K-means(Ren et al., 2018)	97.52 ± 0.07	50.41 ± 0.24	64.39 ± 0.24TPN-Semi (Liu et al., 2018)	N/A	52.78 ± 0.27	66.42 ± 0.21EGNN-Semi(T) (Kim et al., 2019)	N/A	N/A	64.32 ± N/APRWN + Semi-supervised inference (Ours)	99.23 ± 0.08	56.65 ± 0.24	69.65 ± 0.20AnalysisAblation study. From Table 3, we can see that our PRW loss improves the baseline PN significantly,boosting the accuracy of PRWN up to 67.82% from 59.08% on 5-shot mini-imagenet for example.
Table 2: Experiments with distractor classesModel	Omniglot 1-shot	Mini-Imagenet			1-shot	5-shotPRWN (OUrs)	97.76 ± 0.11	50.96 ± 0.23	67.64 ± 0.18PN+ Semi-supervised inference (Ren et al., 2018)	95.08 ± 0.09	47.42 ± 0.33	62.62 ± 0.24PN+ Soft K-means (Ren et al., 2018)	95.01 ± 0.09	48.70 ± 0.32	63.55 ± 0.28PN+ Soft K-means + cluster (Ren et al., 2018)	97.17 ± 0.04	48.86 ± 0.32	61.27 ± 0.24PN+ Masked soft K-means (Ren et al., 2018)	97.30 ± 0.30	49.04 ± 0.31	62.96 ± 0.14TPN-Semi (Liu et al., 2018)	N/A	50.43 ± 0.84	64.95 ± 0.73PRWN+ Semi-supervised inference (Ours)	97.86 ± 0.22	53.61 ± 0.22	67.45 ± 0.21PRWN+ Semi-supervised inference + filter (Ours)	99.04 ± 0.18	54.51 ± 0.23	68.77 ± 0.20Discriminative Power. In order to study our approach and baselines in a more challenging setup,we evaluate their performance on a Higher-Way classification. Fig. 2b shows that our model stillperforms better than the baseline and close to PNall (the oracle). The accuracy of PRWN, PNall,and PN, on 800-ways in Omniglot, are 64.43%, 65.57% and 39.84%, respectively. In Fig. 2c, weshow the relative improvement over PN reaching ≈ 60% improvement on 800-ways classification.
Table 3: Semi-Supervised Meta-LearningModel	Tiered-Imagenet		1-shot	5-shotPN(Ren et al., 2018)	46.52± 0.52	66.15± 0.22PRWN(Ours)	54.87 ± 0.46	70.52 ± 0.43PN + Semi-supervised inference(Ren et al., 2018)	50.74 ± 0.75	69.37 ± 0.26PN + Soft K-means(Ren et al., 2018)	51.52 ± 0.36	70.25.59 ± 0.31PN + Soft K-means + cluster(Ren et al., 2018)	51.85 ± 0.24	69.42 ± 0.17PN + Masked soft K-means(Ren et al., 2018)	52.39 ± 0.44	69.88 ± 0.20TPN-Semi (Liu et al., 2018)	55.74	71.01PRWN + Semi-supervised inference (Ours)	59.17 ± 0.41	71.06 ± 0.39D TieredImagenetE Latent space visualizationFigure 3: tSNE visualization of the embeddings for 50 classes on Omniglot. The embeddings of un-labeled data got magnetized to the class prototypes forming more compacted clusters in our PRWNright in contrast to the embeddings of PN leftF miniImageNet higher way classification13Under review as a conference paper at ICLR 2020Figure 4: Left: Accuracy of our different models on 5-shot miniImageNet as we increase the number
