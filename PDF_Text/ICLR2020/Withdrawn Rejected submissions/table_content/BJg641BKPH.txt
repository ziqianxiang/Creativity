Table 1: Summary of hyperparameter settings and assumptions to achieve an expected -classificationerror by gradient descent for binary classifications. The “Separability” column denotes the types ofmodels where a separability assumption is made. m is the number of hidden units, n is the size of thetraining data, and T is the number of iterations of gradient descent. The notations Ω and Θ hide thelogarithmic terms in the big-Ω and -Θ notations. Smooth activations include sigmoid, tanh, swishactivations, and several smooth approximations of ReLU. As for Allen-Zhu et al. (2018a); Cao & Gu(2019a;b), we pick up results specialized to two-layer networks.
