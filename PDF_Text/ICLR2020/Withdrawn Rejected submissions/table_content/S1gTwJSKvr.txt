Table 1: Validation accuracy of a quantizedResNet-18 trained on ImageNet. ka and kware number of bits to quantize activationsand weights, respectively. T, Opt, GF, andFP refer to ternary, optimal, Greedy Fold-able, and full precision, respectively.
Table 2: Comparison with state-of-the-art quantization. Opt and GF are theproposed optimal and greedy foldable quantization algorithms, respectively. Tand FP refer to ternary and full precision network, respectively.
