Table 1: Percent misclassification comparison for CIFAR10 (lower is better)	18 Layers	34 Layers	50 Layers	101 LayersResNet (with BN, large LR)	7.01	6.79	5.04	4.95FarkasNet (with BN, large LR)	6.87	6.50	5.03	4.98ResNet (no BN, small LR)	12.51	9.90	24.53	25.25FarkasNet (no BN, small LR)	10.83	9.45	19.84	15.38Table 2: Percent misclassification comparison for CIFAR100 (lower is better)	34 Layers	50 Layers	101 LayersResNet (with BN)	30.95	23.43	22.90FarkasNet (with BN)	30.34	23.88	23.22ResNet (no BN)	40.10	55.05*	54.02FarkasNet (no BN)	34.70	43.93	42.94Tables 1 and 2 both show a strong disparity in the learning capacity of a DNN when batch nor-malization is removed, which is unsurprising. On CIFAR10, we primarily observe similar, if notbetter, test errors when batch normalization is used. Without normalization, our test error is alwaysbetter; the same can be said for the CIFAR100 dataset. Thus, while FarkasResNets also diminishin quality, we note that adding a guaranteed undead neuron to all layers heavily impacts learning inthe case of a non-normalized network. This is not at all to say that training a deep neural network isonly possible using FLs, or that we achieve state-of-the-art performance; however, our implemen-tation demonstrates the strong dependency of getting low test accuracy with normalization, and is
Table 2: Percent misclassification comparison for CIFAR100 (lower is better)	34 Layers	50 Layers	101 LayersResNet (with BN)	30.95	23.43	22.90FarkasNet (with BN)	30.34	23.88	23.22ResNet (no BN)	40.10	55.05*	54.02FarkasNet (no BN)	34.70	43.93	42.94Tables 1 and 2 both show a strong disparity in the learning capacity of a DNN when batch nor-malization is removed, which is unsurprising. On CIFAR10, we primarily observe similar, if notbetter, test errors when batch normalization is used. Without normalization, our test error is alwaysbetter; the same can be said for the CIFAR100 dataset. Thus, while FarkasResNets also diminishin quality, we note that adding a guaranteed undead neuron to all layers heavily impacts learning inthe case of a non-normalized network. This is not at all to say that training a deep neural network isonly possible using FLs, or that we achieve state-of-the-art performance; however, our implemen-tation demonstrates the strong dependency of getting low test accuracy with normalization, and isgeometrically motivated.
Table 3: Top1/Top5 percent misclassification comparison for ImageNet-1k (lower is better)	50 Layers	101 Layers	152 LayersResNet (with BN)	25.64/7.68	23.23/6.49	22.26/5.95FarkasNet (with BN)	23.70/6.65	23.74/6.67	22.17/5.86Figure 2: Illustration of improved performance at starting epochs with Farkas layersImpact of data normalization and best practice comparisonWe briefly compare against other best practice learning regimes on CIFAR10. Table 4 highlightsthe various differences between methods. At the core, we compare FarkasNets to FixUp, a recentlyproposed initialization scheme that allows the use of larger learning rates in the absence of batchnormalization, and thus faster convergence. We consider a 34-layer ResNet with BasicBlockstructure using the official implementation of FixUp3 for the network architecture. We compareusing a 34-layer FarkasNet, where the last layer in a block is initialized to zero (as in FixUp, whichis motivated by several other works (Srivastava et al., 2015; Hardt & Ma, 2016; Goyal et al., 2017))but use standard initialization otherwise, as well as “mean” AggregationFunction. We maintain thesetup of the previous experiments.
Table 4: Best practice comparison on CIFAR10 (lower test error is better)	Batch Normalization	LR capacity	Test ErrorResNet34	✓	Large	6.77ResNet34	X	Small	9.18FixUp34	X	Small	7.49FarkasNet34	X	Medium	7.12Figure 3: Training curves for best practice comparison on CIFAR10Finally, we also address the notion that training a very deep neural network is challenging usingmaximal learning rate without normalization, which has been experimentally observed in FixUpand Layer-Sequential Unit-Variance (LSUV) orthogonal initialization (Mishkin & Matas, 2015).
