Table 1: Accuracy and mean confidence (Acc%/C onf %) for Cifar10 dataset. Coarse, fine, and middleindicate the accuracy at each level of the label. End-to-end, denotes the model trained with exclusively dataannotated for the fine label. We compare two end-to-end models trained with different amounts of data withfine labels. We first set D3 = 20%, which we increase afterwards to D3 = 32%. On the other hand, “nested”denotes the same architecture, trained with coarse middle and fine labeled data. In this experiment we setD1 = D2 = D3 = 20%. We repeated the distortion generation 10 times (for all the levels of distortion),the last column “Bound on Std” shows the maximum standard deviation obtained across “Distortion 1” to“Distortion 4,” therefore the value on the last column can be interpreted as a bound on the variability of thereported results. (Similar results are reported for MNIST and fashion-MNIST datasets, see tables 5, and 6 inthe supplementary material.)The previous discussion is interesting as it shows that including additional coarse data tends to helpalso the discrimination of the fine task. However one may argue that the comparison is unfair,as one model sees more data than the other. That is a very interesting point and we address it infollowing experiments where we study how the proportion of fine and coarse granularity data affectperformance (for a fixed budget and different cost models). For now, let us observe what happensif we increase the amount of fine data from D3 = 20% to 32% (which assuming a linear costmodel equals the budget of training with D1 = D2 = D3 = 20%). As expected, (see Table 1)the performance on clean test data improves for the end-to-end model. However, (see columnsDistortion 1-4) it generalizes less to unseen (distorted) test data and also becomes significantly moreoverconfident.
Table 2: Comparison of the same network structure, trained on the same coarse, middle, and fine data, withand without skipped connections. 20% of fine middle and coarse samples of MNIST dataset where selected fortraining. As in the previous experiments, Distortion 1-4 correspond to test distorted samples with turbulence-like distortion (described in the supplementary material).
Table 3: Comparison of the fine accuracy for different combination techniques. The model is trained onMNIST dataset with D1 = D2 = D3 = 10%Convex model.098765432f-HLsciS 6 Ci 60.CisAJOFBISeU-J ①£UO uo<Figure 10: Accuracy on the classification of the fine label for MNIST data. These results complement theresults presented in Figure 4. Figure 4 illustrates the results on clean test data grouping five levels of budgetsand for three cost models (linear, concave, and convex). In this figure, we show the performance of eachindividual model. For each model we display the accuracy on the clean data (dots) as well as the accuracy ondifferent types of distortions. As before, the proportion of coarse and fine labels during training is illustratedcoloring each data point, blue indicates a higher proportion of coarse samples while red a higher proportion offine samples.
Table 4: Comparison of the visual label grouping (VG) and random label grouping (RG) in terms of accuracyfor our model trained on MNIST.
Table 5: Accuracy and mean confidence (Acc%/Conf %) for Fashion-MNIST dataset. Coarse, fine, andmiddle indicate the accuracy at each level of the label. End-to-end, denotes the model trained with exclusivelydata annotated for the fine label. We compare two end-to-end models trained with different amounts of datawith fine labels. We first set D3 = 20%, which we increase afterwards to D3 = 32%. On the other hand,“nested” denotes the same architecture, trained with coarse middle and fine labeled data. In this experiment weset D1 = D2 = D3 = 20%.
Table 6: Accuracy and mean confidence (Acc%/C onf %) for the MNIST dataset. Coarse, fine, and middleindicate the accuracy at each level of the label. End-to-end, denotes the model trained with exclusively dataannotated for the fine label. On the other hand, “nested” denotes the same architecture, trained with coarsemiddle and fine labeled data. In this experiment we set D1 = D2 = D3 = 20%.
Table 7: Classification accuracy and mean confidence (Acc%/C onf %) for an example of MTL and nestedlearning on the MNIST dataset. Coarse, fine, and middle indicate the accuracy at each level of the label. MTLdenotes the standard Multi-task Learning architecture described in Section F.1. On the other hand, “nested”denotes our nested model which enforces hierarchical feature embeddings. In this experiment we set D1 =D2 = D3 = 20%.
Table 8: Accuracy and mean confidence (Acc%/C onf %) for the MNIST dataset. Coarse, fine, and middleindicate the accuracy at each level of the label. In this table we compare the results of a traditional training toa cascaded training on the same architecture. In this experiment we set D1 = D2 = D3 = 20%. We noticesignificant improvement with our method.
