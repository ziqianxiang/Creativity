Table 1: Classification accuracies on the test set for different methods for binary quantization.
Table 2: Classification accuracies on the test set for ternary quantization. PQ* denotes performancewith fully-connected layers, first convolution layer and shortcut layers in floating point whereas PQrepresent results with all layers quantized. Also, PQ* optimize for the quantization levels as well(different for each layer), in contrast we fix it to Q = {-1, 0, 1}. GD-tanh denotes results withoutusing STE and actually calculating the gradient through the projection.
Table 3: Experiment setup. Here, b is the batchsize and K is the total number of iterations for allthe methods.
Table 4: The hyperparameter searchspace for all the experiments. Chosenparameters are given in Tables 5 and 6.
Table 5: Hyperparameter settings used for the binary quantization experiments. Here, the learningrate is multiplied by lr_scale after every 30k iterations and annealing hyperparameter (β) is multipliedby beta_scale after every beta_scale_interval iterations. We use Adam optimizer with zero weightdecay. For PQ, beta_scale denotes regularization rate.
Table 6: Hyperparameter settings used for the ternary quantization experiments. Here, the learningrate is multiplied by lr_scale after every 30k iterations and annealing hyperparameter (β) is multipliedby beta_scale after every beta_scale_interval iterations. We use Adam optimizer except for REF forwhich SGD with momentum 0.9 is used. For PQ, beta_scale denotes regularization rate.
