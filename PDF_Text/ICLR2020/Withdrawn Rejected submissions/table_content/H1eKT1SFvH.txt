Table 1: Comparing our method with state-of-the-art methods on ResNet-50 and MobileNet-v1 overImageNet validation dataset when weights and activations are compressed to low bitrates.
Table 2: Architecture parameters of considered deep learning hardware platforms.
Table 3: Inference rates of equal and unequal bit allocation schemes, as well as full precision, onResNet-50 and MobileNet-v1. TPU uses 8-bit precision while MIT Eyeriss uses 16-bit precision.
