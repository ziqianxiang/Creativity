Table 1: Dataset Specifications4.2	Models EvaluatedThe proposed architecture was evaluated against the fol-lowing architectures: 1) Standard Multi-layer Perceptron 2) MLP with Elastic Weight Consolidation3)GeppNet 4)Fixed Expansion layer architecture and 5) GeppNet+STM on MNIST - Handwrittendataset. The benchmarking performance of each model on MNIST Handwritten dataset was takenfrom Kemker et al. (2018). The experiment that was proposed there was replicated using the samedataset without any changes. The evaluation of the Ωbase, Ωnew and 0。〃 were based on the ɑideaithat were described in the paper.
Table 2: Incremental class learning results on MNIST Handwritten digits dataset. The benchmarkingresults were taken from Kemker et al. (2018)Dataset	Cbase	◎new	ΩallMNIST-Fashion-	0.943	0.941	0.963CIFAR10	0.9231	0.999	0.967Table 3: Incremental class learning results on MNIST-Fashion and CIFAR10 datasetsmodel showed similar performance on Fashion and CIFAR10 dataset. The αideal was set as 92%and 67.3% respectively. It has to be observed that EnsembleNet showed values greater than 0.92 forall Ω values on all the tested datasets.
Table 3: Incremental class learning results on MNIST-Fashion and CIFAR10 datasetsmodel showed similar performance on Fashion and CIFAR10 dataset. The αideal was set as 92%and 67.3% respectively. It has to be observed that EnsembleNet showed values greater than 0.92 forall Ω values on all the tested datasets.
Table 4: Multi modal incremental learning results on MNIST-Fashion and MNIST-Handwrittendigits datasetsThe experimental results show that the proposed architecture has better capacity to retain the baseknowledge and as well as learn new knowledge. Four variations of experiments were conducted onCNN and the CNN was allowed to rehearse the base knowledge in two experiments. Even after re-hearsal, the standard CNN failed to show results that were comparable to our proposed architecture.
