Table 1: Summary of correlation betWeen Q and π.
Table 2: branching and backup experiment where d = 2 and λ = 0.95In this section we explore the impact of using planning as the test policy and which backup schemecaptures the highest returns. We consider a PTN trained with a depth equal to 2 and we evaluateboth backup procedures found in Section 3.2.
Table 3: Returns from PTN(no πF /decision-time planning) and PPN using only the model-freepoliciesβ(found in PTN but not PPN) is to stabilize returns over different values of training depth. In PPN,Wellmer & Kwok (2019) showed that optimal depth is highly dependent on the environment andreturns can drastically differ. While the modifications certainly do not entirely fix this, we find thatit mitigates large differences in returns from different training depths.
