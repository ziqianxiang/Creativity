Table 2: Three challenges defined in the theory of Robinson Arithmetic. The eval set contains all problems for stage 1 and is randomly sampledfor stages 2 and 3.
Table 3: Curriculum learning compared with only exploration based learn-ing.
Table 4: Comparing Vampire, E, leanCoP, rlCoP and FLoP, with re-spect to success ratio for Stage 1, 2 and 3 problems. Our method(FLoP) is marked in grey. E1 - auto mode, E2 - auto-schedule mode.
Table 5: Comparing rlCoP and FLoP with respect to proofs found andproof lengths on Stage 3.
Table 6: Curriculum Learning on a single training proof. Numbers with# and ? are based on 1, 2 runs, respectively.
Table 7: Curriculum learning for Stage 3 on two harder problems withproofs of 113 and 108 steps. Results are based on 3 runs.
Table 8:	Curriculum Learning vs Supervised Learning on Stage 1 and 2, using training proofs	Experiment 6:	Cur-with some	1 - 3 steps added for distraction. FLoP is barely affected, while supervised learning’s		performance degrades. Numbers with ? are averaged from 2 runs.		riculum Learning vs			Supervised	Learning.	Stage	Proof Lengths	Supervised	Curriculum						When training proofs are		Succ.	Len.	Succ.	Len.	available, a natural baseline	1	5,9	0.98(0.04)	327(58)	1 (0.01)	363(5)	of curriculum learning is	1	7, 10	1(0)	359 (0)	0.98(0.01)	327(18)	supervised learning on	1	9, 11	0.52(0.08)	54(11)	0.98 (0.01)	340(18)	the proof steps.	While2	5,9,23	0.85 (0.04)	377(47)	0.76(0.02)?	291(16)?	such behavioral	cloning2	7, 10, 24	0.74 (0.04) 433(110)	0.71(0.01)?	311(61)?	sometimes leads	to great2	9, 11, 25	0.59(0.08)	193(49)	0.76 (0.01)?	267(109)?	performance, we	show inTable 8 that it greatly depends on the quality of the given proof. For the three problems in the trainingset of Stage 1 and 2, we take the shortest proofs (5, 9 and 23 steps) and construct variants with 1-3extra steps added. We observe that supervised learning degrades as superfluous steps are introduced,while FLoP’s exploration allows the system to recover and find the original proofs.
