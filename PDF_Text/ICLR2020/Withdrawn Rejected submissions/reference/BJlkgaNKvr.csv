title,year,conference
 Improved generalization bounds for robustlearning,2018, Technical report
 Spectrally-normalized margin bounds for neuralnetworks,2017, In NIPS
 PAC-learning in the presence of evasionadversaries,2018, In NIPS
 Understanding the difficUlty of training deep feedforward neUralnetworks,2010, In AISTATS
 Deep Sparse Rectifier NeUral Networks,2011, InAISTATS
 Explaining and Harnessing AdversarialExamples,2015, In ICLR
 Identity Mappings in Deep ResidUalNetworks,2016, In ECCV
 Adversarial Risk BoUnds for Binary Classification via FUnctionTransformation,2018, Technical report
 ImageNet Classification with DeepConvolUtional NeUral Networks,2012, In NIPS
 Adversarial Machine Learning at Scale,2017, InICLR
 Deeply-SUpervised Nets,2015, In AISTATS
 A Unified gradient regUlarization family foradversarial examples,2015, In ICDM
 VirtUal Adversarial Training: ARegUlarization Method for SUpervised and Semi-SUpervised Learning,2018, PAMI
 A PAC-Bayesian Approach toSpectrally-Normalized Margin BoUnds for NeUral Networks,2018, In ICLR
 The RoleOf Over-parametrization In Generalization Of NeUral Networks,2018, In ICLR
 AUtomatic differentiation in prose,2017, In ICLR Workshop
 The Singular Values of Convolutional Layers,2018, InICLR
 Certifying Some Distributional Robustnesswith Principled Adversarial Training,2018, In ICLR
 Robust Large MarginDeeP Neural Networks,2017, IEEE Transactions on Signal Processing
 Is Robustness the Cost of Accuracy? - A ComPrehensive Study on the Robustness of,2018, In ECCV
 Intriguing ProPerties of neural networks,2014, In ICLR
 Lipschitz-Margin Training : Scalable Certifica-tion of Perturbation Invariance for Deep Neural Networks,2018, In NIPS
 Robustness and generalization,2012, Machine Learning
 Rademacher Complexity for Adversarially Robust,2018, Technical report
 Wide Residual Networks,2016, In BMVC
 Understandingdeep learning requires rethinking generalization,2016, In ICLR
 Fixup Initialization: Residual Learning WithoutNormalization,2018, In ICLR
 As mentioned in thecaption of fig,2018, 1
	The high skewness results from the fact that the NN trained on CIFAR10 is of large enough capacityto overfit the training set,2016, As known
