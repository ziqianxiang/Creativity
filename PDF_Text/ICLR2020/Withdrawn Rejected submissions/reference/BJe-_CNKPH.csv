title,year,conference
 Multiple object recognition with visualattention,2014, In Proc
 Neural machine translation by jointlylearning to align and translate,2015, In Proc
 A large anno-tated corpus for learning natural language inference,2015, In Proc
 In Proc,2017, of UbiComp
 What does BERT lookat? an analysis of bertâ€™s attention,2019, In Proc
 Language modeling with gatedconvolutional networks,2017, In Proc
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proc
 Multi30k: Multilingual english-german image descriptions,2016, In Proc of the 5th Workshop on Vision and Language
 Interpreting recurrent and attention-based neuralmodels: a case study on natural language inference,2018, In Proc
 Teaching machines to read and comprehend,2015, In Proc
 Attention is not Explanation,2019, In Proc
 A structured self-attentive sentence embedding,2017, In Proc
 Effective approaches to attention-basedneural machine translation,2015, In Proc
 Learning Word vectors for sentiment analysis,2011, In Proc
 Encoding sentences with graph convolutional networks forsemantic role labeling,2017, In Proc
 Conditional image generation with pixelcnn decoders,2016, In Proc
 Glove: Global vectors for wordrepresentation,2014, In Proc
 In Proc,2016, of ICLR
 Is attention interpretable? In Proc,2019, of ACL
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In Proc
 Attention is all you need,2017, In Proc
 Analyzing the structure of attention in a transformer languagemodel,2019, In Proc
 Attention-based LSTM for aspect-levelsentiment classification,2016, In Proc
 Attention is not not explanation,2019, In Proc
 A broad-coverage challenge corpus for sen-tence understanding through inference,2018, In Proc
 Hierarchicalattention networks for document classification,2016, In Proc
