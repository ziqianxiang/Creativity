title,year,conference
 Obfuscated gradients give a false sense of se-curity: Circumventing defenses to adversarial examples,2018, In Proceedings of the 35th InternationalConference on Machine Learning
 A theory of learning from different domains,2010, Machine Learning
 Towards evaluating the robustness of neural networks,2016, arXivpreprint arXiv:1608
 Arotation and a translation suffice: Fooling cnns with simple transformations,2017, arXiv preprintarXiv:1712
 Explor-ing the landscape of spatial robustness,2019, In Proceedings of the 36th International Conference onMachine Learning
 Explaining and harnessing adversarialexamples,2015, In International Conference on Learning Representations (ICLR)
 Sparse dnns with improved adver-Sarial robustness,2018, In Advances in neural information processing Systems
 Adversarial logit pairing,2018, arXiv preprintarXiv:1803
 Adversarial machine learning at scale,2017, InInternational Conference on Learning Representations (ICLR)
 Delving into transferable adversarial ex-amples and black-box attacks,2017, In 5th International Conference on Learning Representations
 Deepfool: A simple andaccurate method to fool deep neural networks,2016, In IEEE Computer Vision and Pattern Recognition(CVPR)
 Practical black-box attacks against machine learning,2017, In Proceedings of the 2017ACM on Asia Conference on Computer and Communications Security
 Intriguing properties of neural networks,2013, In International Conference onLearning Representations (ICLR)
 On the uniform convergence of relative frequencies of eventsto their probabilities,1971, Theory of Probability and its Applications
 Wide residual networks,2016, In Edwin R
 mixup: Beyond empiri-cal risk minimization,2018, In 6th International Conference on Learning Representations
