title,year,conference
 Efficient full-matrixadaptive regularization,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov
 On the convergence of the proximal algorithm for nonsmooth functionsinvolving analytic features,2009, Mathematical Programming
 Proximal alternating linearized minimization for nonconvexand nonsmooth problems,2014, Mathematical Programming
 Optimization methods for large-scale machine learning,2018, SiamReview
 On the convergence of a class of adam-type algorithms fornon-convex optimization,2019, In International Conference on Learning Representations
 Convergence guarantees for rmsprop and adam in non-convexoptimization and their comparison to nesterov acceleration on autoencoders,2018, arXiv preprintarXiv:1807
 Generalized momentum-based methods: A hamiltonian perspec-tive,2019, arXiv preprint arXiv:1906
 Incorporating nesterov momentum into adam,2016, 2016
 A unified approach to adaptive regularization in online andstochastic optimization,2017, arXiv preprint arXiv:1706
 Convergence rates of inertial splitting schemes for nonconvex com-posite optimization,2017, In 2017 IEEE International Conference on Acoustics
 Adam: A method for stochastic optimization,2015, In International Conferenceon Learning Representations
 A multi-step inertial forward-backward splitting method for non-Convex optimization,2016, In Advances in Neural Information Processing Systems
 On the variance of the adaptive learningrate and beyond,2019, arXiv preprint arXiv:1908
 Une PrOPriete topologique des sous-ensembles analytiques reels,1963, Les equations auxderivees partielles
 Adaptive gradient methods with dynamic bound of learning rate,2019, InInternational Conference on Learning Representations
 Quasi-hyperbolic momentum and adam for deep learning,2019, In InternationalConference on Learning Representations
 Adaptive bound optimization for online convex optimization,2010, InCOLT
 On the difficulty of training recurrent neural networks,2013, InInternational conference on machine learning
 On the convergence of adam and beyond,2018, In InternationalConference on Learning Representations
 A stochastic approximation method,1985, In Herbert Robbins Selected Papers
 On the importance of initialization and momentumin deep learning,2013, In International conference on machine learning
 General inertial proximal gradient method for a class of nonconvex nonsmoothoptimization problems,2019, Computational OPtimizatiOn and Applications
 Adaptive methods for nonconvex optimiza-tion,2018, In Advances in Neural Information Processing Systems
 On the convergence of adaptive gradient methodsfor nonconvex optimization,2018, arXiv preprint arXiv:1808
 Adashift: Decorrelation and convergenceof adaptive learning rate methods,2019, In International Conference on Learning Representations
 A sufficient condition for convergences of adam andrmsprop,2019, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
