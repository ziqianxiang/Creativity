title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, arXiv preprint arXiv:1802
 Thermometer encoding: One hotway to resist adversarial examples,2018, 2018
 Towards evaluating the robustness of neural networks,2017, In 2017IEEE Symposium on Security and Privacy (SP)
 Certified adversarial robustness via randomizedsmoothing,2019, arXiv preprint arXiv:1902
 Provable robustness of relu networksvia maximization of linear regions,2018, arXiv preprint arXiv:1810
 Ai2: Safety and robustness certification of neural networks with abstract interpretation,2018, In2018 IEEE Symposium on Security and Privacy (SP)
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 On the effectiveness of interval bound propagationfor training verifiably robust models,2018, arXiv preprint arXiv:1810
 Countering adversarialimages using input transformations,2017, arXiv preprint arXiv:1711
 Decision boundary analysis of adversarial examples,2018, 2018
 Reluplex: An efficientsmt solver for verifying deep neural networks,2017, In International Conference on Computer AidedVerification
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2017, arXiv preprint arXiv:1711
 On certifying non-uniform bound against adversarialattacks,2019, arXiv preprint arXiv:1903
 Characterizing adversarial subspaces using localintrinsic dimensionality,2018, arXiv preprint arXiv:1801
 Universaladversarial perturbations,2017, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Certified defenses against adversarialexamples,2018, arXiv preprint arXiv:1801
 Provably robust deep learning via adversarially trained smoothed classifiers,2019, arXivpreprint arXiv:1906
 A convex relaxationbarrier to tight robust verification of neural networks,2019, arXiv preprint arXiv:1902
 Defense-gan: Protecting classifiers againstadversarial attacks using generative models,2018, arXiv preprint arXiv:1805
 An abstract domain forcertifying neural networks,2019, Proceedings of the ACM on Programming Languages
 Pixeldefend:Leveraging generative models to understand and defend against adversarial examples,2017, arXivpreprint arXiv:1710
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Evaluating robustness of neural networks with mixedinteger programming,2017, arXiv preprint arXiv:1711
 Mixtrain: Scalable training of formallyrobust neural networks,2018, arXiv preprint arXiv:1811
 Efficient formal safetyanalysis of neural networks,2018, In Advances in Neural Information Processing Systems
 Scaling provable adversarialdefenses,2018, In Advances in Neural Information Processing Systems
 Training for fasteradversarial robustness verification via inducing relu stability,2018, arXiv preprint arXiv:1809
 Mitigating adversarial effectsthrough randomization,2017, arXiv preprint arXiv:1711
 Efficient neural networkrobustness certification with general activation functions,2018, In Advances in Neural InformationProcessing Systems
 Towardsstable and efficient training of verifiably robust neural networks,2019, arXiv preprint arXiv:1906
