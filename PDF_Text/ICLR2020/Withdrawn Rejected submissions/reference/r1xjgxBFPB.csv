title,year,conference
 Fast randomized kernel ridge regression with statisticalguarantees,2015, In Advances in Neural Information Processing Systems
 Gradient based sample selectionfor online continual learning,2019, ArXiv
 Sharp analysis of low-rank kernel matrix approximations,2013, In Conference on LearningTheory
 Measuring and regularizing networks infunction space,2018, arXiv preprint arXiv:1805
 Pattern recognition and machine learning,2006, Springer
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofCOMPSTAT'2010
 Efficientlifelong learning with a-gem,2018, ArXiv
 On tiny episodic memories in continuallearning,2019, ArXiv
 Uncertainty-guidedcontinual learning with bayesian neural networks,2019, ArXiv
 A unifying bayesian view of continual learning,2019, ArXiv
 An empiri-cal investigation of catastrophic forgetting in gradient-based neural networks,2013, arXiv preprintarXiv:1312
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in neural information processing systems
 Approximateinference turns deep networks into gaussian processes,2019, NeurIPS
 Adam: A method for stochastic optimization,2015, ICLR
 Overcom-ing catastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Gradient episodic memory for continual learning,2017, InAdvances in Neural Information Processing Systems
 Gaussian processes in machine learning,2003, In Summer School on MachineLearning
 icarl:Incremental classifier and representation learning,2017, In Proceedings of the IEEE conference onComputer Vision and Pattern Recognition
 Progressive neural networks,2016, ArXiv
 Overcoming catastrophicforgetting with hard attention to the task,2018, In ICML
 Continual learning with deep generativereplay,2017, In NIPS
 Improving and under-standing variational continual learning,2019, arXiv preprint arXiv:1905
 Functional regularisation for continual learning using gaussian processes,2019, arXiv preprintarXiv:1901
