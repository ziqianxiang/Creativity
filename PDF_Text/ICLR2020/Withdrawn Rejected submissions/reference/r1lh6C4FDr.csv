title,year,conference
 Learning activation functions to improvedeep neural networks,2014, arXiv preprint:1412
 Learning long-term dependencies with gradient descent isdifficult,1994, IEEE transactions on neural networks
 Deep neural network using trainable activation functions,2016, InInternational Joint Conference on Neural Networks (IJCNN)
 Empirical evaluation of gated recurrent neuralnetworks on sequence modeling,2014, In NIPS 2014 Workshop on Deep Learning
 Approximation by superpositions of a sigmoidal function,1989, Mathematics of control
 Breaking the activation function bottleneck throughadaptive parameterization,2018, In Advances in Neural Information Processing Systems
 Maxout networks,2013, InInternational Conference on Machine Learning
 Deep learning,2016, MIT Press
 On the impact of the activation function on deep neuralnetworks training,2019, In International Conference on Machine Learning
 Delving deep into rectifiers: Surpassing human-level per-formance on imagenet classification,2015, In Proceedings of the IEEE International Conference onComputer Vision (ICCV)
 Approximation capabilities of multilayer feedforward networks,1991, Neural Networks
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, In International Conference on Machine Learning
 An empirical exploration of recurrent network archi-tectures,2015, In International Conference on Machine Learning
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Rectifier nonlinearities improve neural network acousticmodels,2013, In ICML Workshop on Deep Learning for Audio
 Learning combinations of activation functions,2018, In 2018 24th InternationalConference on Pattern Recognition (ICPR)
 Understanding the exploding gradient problem,2012, CoRR
 Adaptive blending units: Trainableactivation functions for deep neural networks,2018, arXiv preprint:1806
 Technical Report,2017,
