title,year,conference
 On the convergence ofa class of adam-typealgorithm for non-convex optimization,2019, Proceedings of 7th International Conference on LearningRepresentations(ICLR)
 Imagenet: A large-scalehierarchical image database,2009, in 2009 ieee conference on computer vision and pattern recognition
 Deep residual learning for image recog-nition,2016, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Nostalgic adam: Weighting more of the past gradientswhen designing the adaptive learning rate,2019, arXiv preprint arXiv: 1805
 Adam: A method for stochastic optimization,2015, Proceedingsof the 3rd International Conference on Learning Representations (ICLR)
 Microsoft COCO:common objects in context,2014, CoRR
 Decoupled Weight decay regularization,2019, Proceedings of 7thInternational Conference on Learning Representations (ICLR)
 Adaptive gradient methods With dynamicbound of learning rate,2019, Proceedings of 7th International Conference on Learning Representations
 Adaptive bound optimization for online convex opti-mization,2010, Proceedings ofthe 23rd Annual Conference On Learning Theory (COLT)
 Rmsprop: Divide the gradient by a running average of itsrecent magnitude,2012, COURSERA: Neural networksfor machine learning
 Adaptive meth-ods for nonconvex optimization,2018, Advances in Neural Information Processing Systems 31
 Adadelta: An adaptive learning rate method,2012, arXiv preprint arXiv:1212
2 Proof of Corollary 4,2020,2Similar to the proof of Theorem 4
