title,year,conference
 A closer look atmemorization in deep networks,2017, In ICML
 Combining labeled and unlabeled data with co-training,1998, In COLT
 Food-101-mining discriminative compo-nents with random forests,2014, In ECCV
 LOF: identifying density-based local outliers,2000, In ACM sigmod record
 Active Bias: Training moreaccurate neural networks by emphasizing high variance samples,2017, In NeurIPS
 Understanding and utilizingdeep neural networks trained with noisy labels,2019, In International Conference on Machine Learning
 Co-teaching: Robust training of deep neural networks with extremely noisy labels,2018, InNeurIPS
 Using pre-training can improve model robustnessand uncertainty,2019, In International Conference on Machine Learning
 Densely connectedconvolutional networks,2017, In CVPR
 MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In ICML
 ImageNet classification with deep convolu-tional neural networks,2012, In NeurIPS
 Cleannet: Transfer learning for scalableimage classifier training with label noise,2018, In CVPR
 Gradient descent with early stopping is prov-ably robust to label noise for overparameterized neural networks,2019, arXiv preprint arXiv:1903
" Decoupling “when to update"" from “how to update""",2017, InNeurIPS
 Generalization guaran-tees for neural networks via harnessing the low-rank structure of the jacobian,2019, arXiv preprintarXiv:1906
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In CVPR
 Training deep neural networks on noisy labels with bootstrapping,2015, In ICLR
 Learning with bad training data via iterative trimmed lossminimization,2019, In ICML
 Very deep convolutional networks for large-scale imagerecognition,2015, In ICLR
 SELFIE: Refurbishing unclean samples for robustdeep learning,2019, In ICML
 Learningfrom noisy large-scale datasets with minimal supervision,2017, In CVPR
 Learning from massive noisylabeled data for image classification,2015, In CVPR
 Understandingdeep learning requires rethinking generalization,2017, In ICLR
