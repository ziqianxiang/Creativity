title,year,conference
 A large anno-tated corpus for learning natural language inference,2015, arXiv preprint arXiv:1508
 Visualizing and measuring the geometry of BERT,2019, arXiv preprint arXiv:1906
 BERT: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 A structural probe for finding syntax in word representa-tions,2019, In Proceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies
 Long short-term memory,1997, Neural computation
 Tensor productgeneration networks for deep NLP modeling,2018, In Proceedings of the 2018 Conference of theNorth American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Open sesame: Getting inside BERTâ€™s linguisticknowledge,2019, arXiv preprint arXiv:1906
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Right for the wrong reasons: Diagnosing syntacticheuristics in natural language inference,2019, arXiv preprint arXiv:1902
 Physical symbol systems,1980, Cognitive science
 Sentence encoders on stilts: Supplementarytraining on intermediate labeled-data tasks,2018, arXiv preprint arXiv:1811
 Holographic reduced representations,1995, IEEE Transactions on Neural networks
 Learning to reason with third order tensor products,2018, InAdvances in Neural Information Processing Systems 31 
 Tensor product variable binding and the representation of symbolic structures inconnectionist systems,1990, Artificial Intelligence
 BERT rediscovers the classical NLP pipeline,2019, arXivpreprint arXiv:1905
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Can you tell me how to get past sesamestreet? sentence-level pretraining beyond language modeling,2019, In Proceedings of the 57th AnnualMeeting of the Association for Computational Linguistics
