title,year,conference
 Obfuscated gradients give a false sense of se-curity: Circumventing defenses to adversarial examples,2018, In Proceedings of the 35th InternationalConference on Machine Learning
 Security evaluation of pattern classifiers under attack,2014, IEEETransactions on Knowledge and Data Engineering
 Adversarial examples are not easily detected: Bypassing tendetection methods,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence andSecurity
 Targeted backdoor attacks on deeplearning systems using data poisoning,2017, arXiv preprint arXiv:1712
 Robust sparse regression under adver-sarial corruption,2013, In Sanjoy Dasgupta and David McAllester (eds
 Certified adversarial robustness via randomizedsmoothing,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Variance estimation in high-dimensional linear models,0006, Biometrika
 Decaf: A deep convolutional activation feature for generic visual recognition,2014, In Eric P
 Detecting adversarialsamples from artifacts,2017, arXiv preprint arXiv:1703
 Explaining and harnessing adversarialexamples,2015, In 3rd International Conference on Learning Representations
 Deep residual learning for image recog-nition,2016, In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In S
 Compressed sensing with adversarial sparse noise via l1 regres-sion,2018, arXiv preprint arXiv:1809
 Efficient algorithms for outlier-robust regres-sion,2018, arXiv preprint arXiv:1803
 Stronger data poisoning attacks break datasanitization defenses,2018, arXiv preprint arXiv:1811
 Human-level concept learningthrough probabilistic program induction,0036, Science
 Gradient-based learning applied to documentrecognition,0018, Proceedings of the IEEE
 Certifiedrobustness to adversarial examples with differential privacy,2018, arXiv preprint arXiv:1802
 A stratified approach to robust-ness for randomly smoothed classifiers,2019, arXiv preprint arXiv:1906
 Certified adversarial robustness withadditive gaussian noise,2018, arXiv preprint arXiv:1809
 On detecting adversarialperturbations,2017, In 5th International Conference on Learning Representations
 Towards poisoning of deep learning algorithms with back-gradientoptimization,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security
 Learning withnoisy labels,2013, In C
 Sok: Security and privacy in machinelearning,2018, In 2018 IEEE European Symposium on Security and Privacy (EuroS P)
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, pp
 Label sanitization against label flippingpoisoning attacks,2019, In Carlos Alzate
 Robust esti-mation via robust gradient estimation,2018, arXiv preprint arXiv:1802
 Provably robust deep learning via adversarially trained smoothed classifiers,2019, arXivpreprint arXiv:1906
 Learning with bad training data via iterative trimmed loss min-imization,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Connecting optimization and regulariza-tion paths,2018, In S
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Rethinkingthe inception architecture for computer vision,2016, In The IEEE Conference on Computer Vision andPattern Recognition (CVPR)
 Spectral signatures in backdoor attacks,2018, In S
 Generative poisoning attack method against neuralnetworks,2017, arXiv preprint arXiv:1703
 Representer point selectionfor explaining deep neural networks,2018, In S
 How transferable are features in deepneural networks? In Z,2014, Ghahramani
