title,year,conference
 Fast learning rates for plug-in classifiers,2007, TheAnnalsofStatistics
 Support vector machines with the ramp loss and the hard margin loss,2011, OperationsResearch
 Data cleaning: Overview and emergingchallenges,2016, In SIGMOD
 Rates of convergence for nearest neighbor procedures,1968, In Proceedings of theHawaii International Conference on Systems Sciences
 On the strong universal Consis-tency of nearest neighbor regression function estimates,1994, The Annals of Statistics
 Discriminatory analysis-nonparametric discrimination: consis-tency properties,1951, Technical report
 On the consistency of exact and approximate nearestneighbor with noisy data,2016, arXiv preprint arXiv:1607
 Training deep neural-networks using a noise adaptationlayer,2016, 2016
 Discovering informative patterns and data cleaning,1994, In AAAIWorkshop on Knowledge Discovery in Databases
 Using trusted data to traindeep networks on labels corrupted by severe noise,2018, In Advances in neural information processingsystems
 To trust or not to trust a classifier,2018, In Advances inNeural Information Processing Systems (NeurIPS)
 Non-asymptotic uniform rates of consistency for k-nn regression,2019, In Proceedings ofthe AAAI Conference on Artificial Intelligence
 Mentornet: Regularizing verydeep neural networks on corrupted labels,2017, arXiv preprint arXiv:1712
 Learning deep networks from noisy labels withdropout regularization,2016, In 2016 IEEE 16th International Conference on Data Mining (ICDM)
 Learning from noisy singly-labeleddata,2017, arXiv preprint arXiv:1712
 Learning fromnoisy labels with distillation,2017, In Proceedings of the IEEE International Conference on ComputerVision 
 Smooth discrimination analysis,1999, The Annals ofStatistics
 Learning withnoisy labels,2013, In Advances in Neural Information Processing Systems
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
 Fast rates for a kNN classifier robust to unknown asymmetric labelnoise,2019, arXiv preprint arXiv:1906
 Deep learning is robust to massivelabel noise,2017, arXiv preprint arXiv:1705
 Adaptive Hausdorff estimation of density levelsets,2009, The Annals of Statistics
 Consistent nonparametric regression,1977, The Annals of Statistics
 Trainingconvolutional networks with noisy labels,2014, arXiv preprint arXiv:1406
 Optimal aggregation of classifiers in statistical learning,2004, The Annalsof Statistics
 Learning with symmetric labelnoise: The importance of being unhinged,2015, In Advances in Neural Information Processing Systems
 Learningfrom noisy large-scale datasets with minimal supervision,2017, In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition
 Analyzing the robustness of nearest neighborsto adversarial examples,2018, In International Conference on Machine Learning
 Asymptotic properties of nearest neighbor rules using edited data,1972, IEEE Trans
 Learning from massive noisylabeled data for image classification,2015, In Proceedings of the IEEE conference on computer visionand pattern recognition
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In Advances in Neural Information Processing Systems
 The proof begins in the same way as the proof of Theorem 1,2020, As before
