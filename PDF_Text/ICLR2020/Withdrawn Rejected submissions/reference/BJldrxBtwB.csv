title,year,conference
 Wasserstein gan,2017, arXiv preprintarXiv:1701
 Approximability of discriminators implies diversity ingans,2018, arXiv preprint arXiv:1806
 Large scale gan training for high fidelity naturalimage synthesis,2018, arXiv preprint arXiv:1809
 Sgd learns over-parameterized networks that provably generalize on linearly separable data,2017, arXiv preprintarXiv:1710
 Convex optimization: Algorithms and complexity,2015, Foundations andTrendsR in Machine Learning
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 A style-based generator architecture for generativeadversarial networks,2018, arXiv preprint arXiv:1812
 Conditional generative adversarial nets,2014, arXiv preprintarXiv:1411
 Spectral normalizationfor generative adversarial networks,2018, arXiv preprint arXiv:1802
 Gradient descent gan optimization is locally stable,2017, InAdvances in Neural Information Processing Systems
 Unsupervised representation learning with deepconvolutional generative adversarial networks,2015, arXiv preprint arXiv:1511
 On the theory of the brownian motion,1930, Physicalreview
 Self-attention generativeadversarial networks,2018, arXiv preprint arXiv:1805
 Sothe variance of the OU process will increase as t grows,2020, And because the elements in Σ is of orderΘ(√m)
