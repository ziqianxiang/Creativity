title,year,conference
 Deep rewiring:Training very sparse deep networks,2017, CoRR
 Learning sparse neural networks through l0regularization,2018, In International Conference on Learning Representations
 Sparse networks from scratch: Faster training without losingperformance,2019, ArXiv
 Essentially No Bar-riers in Neural Network Energy Landscape,2018, In International Conference on Machine Learning
 The difficulty of training sparseneural networks,2019, ArXiv
 The lotteryticket hypothesis at scale,2019, ArXiv
 The state of sparsity in deep neural networks,2019, CoRR
 Dynamic network surgery for efficient DNNs,2016, CoRR
 EIE: Efficient Inference Engine on compressed deep neural network,2016, In Proceedings of the43rd International Symposium on Computer Architecture
 Long short-term memory,0899, Neural Comput
 AdaPtivesParse tiling for sParse matrix multiPlication,2019, In Proceedings of the 24th Symposium on Principlesand Practice of Parallel Programming
 Mobilenets: Efficient convolutional neural networks formobile vision aPPlications,2017, CoRR
 Efficient neuralaudio synthesis,2018, In International Conference on Machine Learning (ICML)
 Optimal Brain Damage,1990, In Advances in NeuralInformation Processing Systems
 SNIP: Single-shot Network Prun-ing based on Connection Sensitivity,2019, In International Conference on Learning Representations(ICLR)
 Pruning filters forefficient convnets,2016, In International Conference on Learning Representations
 Memory-efficient deep learning on a spinnaker 2 prototype,2018, In Front
 Bayesian comPression for deeP learning,2017, InAdvances in Neural Information Processing Systems
 Pointer sentinel mixturemodels,2016, ArXiv
 ExPloiting unstructured sParsity onnext-generation datacenter hardware,2019, 2019
 Scalable training of artificial neural networks with adaPtive sParse connec-tivity insPired by network science,2018, Nature Communications
 Variational DroPout SParsifies DeePNeural Networks,2017, In Proceedings of the 34th International Conference on Machine Learning
 Parameter efficient training of deeP convolutional neural networksby dynamic sParse reParameterization,2019, In Proceedings of the 36th International Conference onMachine Learning
 ExPloring sParsity in re-current neural networks,2017, In 5th International Conference on Learning Representations
 Training sparse neural networks,2017, In 2017 IEEEConference on Computer Vision and Pattern Recognition Workshops (CVPRW)
 Sparse Connection and Pruning in Large Dynamic Artificial Neural Networks,1997, InEUROSPEECH
 Re-thinking the inception architecture for computer vision,2016, In Proceedings of IEEE Conference onComputer Vision and Pattern Recognition
 Snrram： An efficientsparse neural network computation architecture based on resistive random-access memory,2018, InProceedings of the 55th Annual Design Automation Conference
 Wide residual networks,2016, In BMVC
1 seems to work best across all methods,1000, Aninteresting observation is that higher drop fractions (α) seem to work better with longer intervals∆T
