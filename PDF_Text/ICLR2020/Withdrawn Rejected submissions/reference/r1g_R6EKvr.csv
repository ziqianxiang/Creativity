title,year,conference
 Multi-stage pretraining for abstractivesummarization,2019, CoRR
 Deep residual learning for imagerecognition,2016, In Proceedings of CVPR
 Matrix capsules with em routing,2018, 2018
 Adam: A method for stochastic optimization,2015, ICLR
 Training millionsof personalized dialogue agents,2018, In EMNLP
 Computational optimal transport,2019, Foundations and Trends inMachine Learning
 A neural attention model for abstractivesentence summarization,2015, In Proceedings of EMNLP
 LXMERT: Learning cross-modality encoder representations fromtransformers,2019, ArXiv
 Attention is all you need,2017, In Proceedings of NeurIPS
 XLNet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Multimodal transformer with multi-view visualrepresentation for image captioning,2019, 05 2019a
 Selective encoding for abstractive sentencesummarization,2017, In Proceedings of the 55th Annual Meeting of the Association for ComputationalLinguistics
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In ICCV
