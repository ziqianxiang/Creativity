title,year,conference
 Sparse communication for distributed gradient descent,2017, arXivpreprint arXiv:1704
 Qsgd: Randomized quantization forcommunication-optimal stochastic gradient descent,2016, arXiv preprint arXiv:1610
 The convergence of sparsified gradient methods,2018, In Advances in Neural InformationProcessing Systems
 Variance reduction for faster non-convex optimization,2016, InInternational conference on machine learning
 Stochastic optimization with variance reduction for infinite datasetswith finite sum structure,2017, In Advances in Neural Information Processing Systems
 Towardsfederated learning at scale: System design,2019, arXiv preprint arXiv:1902
 Mxnet: A flexible and efficient machine learning library forheterogeneous distributed systems,2015, arXiv preprint arXiv:1512
 Project adam:Building an efficient and scalable deep learning training system,2014, In OSDI
 Large scale distributed deep networks,2012, In Advances inneural information processing systems
 On the ineffectiveness of variance reduced optimization for deeplearning,2018, arXiv preprint arXiv:1812
 Saga: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, In Advances in neural informationprocessing Systems
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR09
 Competing with the empirical riskminimizer in a single pass,2015, In Conference on learning theory
 Towards federated learning at scale: System design,2017, 2017
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 A linear speedup analysis of distributed deep learning withsparse and quantized communication,2018, In S
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in neural information processing systems
 Imagenet classification with deep convolu-tional neural networks,2012, In Advances in neural information processing systems
 Less than a single pass: Stochastically controlled stochastic gradientmethod,2016, arXiv preprint arXiv:1609
 Asynchronous parallel stochastic gradient fornonconvex optimization,2015, In Advances in Neural Information Processing Systems
 Asynchronous decentralized parallel stochasticgradient descent,2017, arXiv preprint arXiv:1710
 Deep gradient compression:Reducing the communication bandwidth for distributed training,2017, arXiv preprint arXiv:1712
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Communication-efficientlearning of deep networks from decentralized data,2016, arXiv preprint arXiv:1602
 Sparknet: Training deep networksin spark,2015, arXiv preprint arXiv:1511
 Hogwild: A lock-free approach toparallelizing stochastic gradient descent,2011, In Advances in neural information processing systems
 Sparse binary Com-pression: Towards distributed deep learning with minimal communication,2018, CoRR
 Minimizing finite sums with the stochasticaverage gradient,2017, Mathematical Programming
 1-bit stochastic gradient descent and itsapplication to data-parallel distributed training of speech dnns,2014, In Fifteenth Annual Conference ofthe International Speech Communication Association
 Horovod: fast and easy distributed deep learning intensorflow,2018, arXiv preprint arXiv:1802
 Accelerated proximal stochastic dual coordinate ascent forregularized loss minimization,2013, arXiv preprint arXiv:1309
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Scalable distributed dnn training using commodity gpu cloud computing,2015, In SixteenthAnnual Conference of the International Speech Communication Association
 Communication compression fordecentralized training,2018, In Advances in Neural Information Processing Systems
 D2: Decentralized training overdecentralized data,2018, CoRR
 Doublesqueeze: Parallel stochastic gradientdescent with double-pass error-compensated compression,2019, In International Conference on MachineLearning
 Distributed asynchronous deterministicand stochastic gradient optimization algorithms,1986, IEEE transactions on automatic control
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in neuralinformation processing systems
 Petuum: A new platform for distributed machine learningon big data,2015, IEEE Transactions on Big Data
 Parallel restarted sgd with faster convergence and lesscommunication: Demystifying why model averaging works for deep learning,2019, In Proceedings ofthe AAAI Conference on Artificial Intelligence
