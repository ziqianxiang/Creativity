title,year,conference
 Approximation by superpositions of a sigmoidal function,1989, Mathematics of Control
 Understanding deeplearning requires rethinking generalization,2016, International Conference on Learning Representations
 Deep linear networks with arbitrary loss: All local minima areglobal,2018, In International Conference on Machine Learning
 Deep learning without poor local minima,2016, In Advances in neural informationprocessing systems
 Generative adversarial nets,2014, In Z
 HoW to use t-sne effectively,2016, Distill
 On the importanceof single directions for generalization,2018, In International Conference on Learning Representations
 On the depth of deep neuralnetWorks: A theoretical vieW,2016, In AAAI
 Norm-based capacity control in neuralnetWorks,2015, In Proceedings of The 28th Conference on Learning Theory
 On the generalization error bounds of neural netWorksunder diversity-inducing mutual angular regularization,2015, arXiv preprint arXiv:1511
 Understanding Machine Learning: From Theory toAlgorithms,2014, Cambridge University Press
 Almost linear vc dimension bounds for pieceWisepolynomial netWorks,1998, In Advances in Neural Information Processing Systems
 Nearly-tight VC-dimension bounds forpieceWise linear neural netWorks,2017, In Proceedings of the 2017 Conference on Learning Theory
 A PAC-bayesian approach tospectrally-normalized margin bounds for neural netWorks,2018, In International Conference on LearningRepresentations
 Spectrally-normalized margin bounds forneural netWorks,2017, In Advances in Neural Information Processing Systems
 Rademacher and gaussian complexities: Risk bounds andstructural results,2003, J
 Fast rates for empirical risk minimization of strict saddleproblems,2017, In COLT
 Generalization error of invariantclassifiers,2017, In Artificial Intelligence and Statistics
 Exploring generaliza-tion in deep learning,2017, In Advances in Neural Information Processing Systems
 Size-independent sample complexity ofneural networks,2018, In Proceedings of the 31st Conference On Learning Theory
 Stronger generalization bounds fordeep nets via a compression approach,2018, In International Conference on Machine Learning
 Flat minima,1997, Neural Computation
 Identifying generalizationProPerties in neural networks,2018, arXiv preprint arXiv:1809
 Visualizing the loss landscaPeof neural nets,2018, In Advances in Neural Information Processing Systems
 EntroPy-sgd: Biasing gradient descent into wide valleys,2017, In InternationalConference on Learning Representations (ICLR)
 SharP minima can generalize fordeeP nets,2017, In International Conference on Machine Learning
 Readingdigits in natural images with unsuPervised feature learning,2011, NIPS WorkshoP on DeeP Learningand UnsuPervised Feature Learning 2011
 On Milmanâ€™s inequality and random subsPaces which escaPe through a mesh in Rn,1988, In Geometric Aspects of Functional Analysis
