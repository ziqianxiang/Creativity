title,year,conference
 Towards Characterizing Divergence in DeepQ-Learning,2019, arXiv:1903
 Generalization in reinforcement learning: Safely approxi-mating the value function,1995, In Advances in neural information processing systems
 GEP-PG: Decoupling Exploration andExploitation in Deep Reinforcement Learning Algorithms,2018, In International Conference in MachineLearning (ICML)
 Off-Policy Deep Reinforcement Learning withoutExploration,1812, arXiv:1812
 Parametric value function approximation: A unified view,2011, In 2011 IEEESymposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)
 Soft Actor-Critic: Off-PolicyMaximum Entropy Deep Reinforcement Learning with a Stochastic Actor,1801, arXiv:1801
 Soft actor-critic algorithms andapplications,2018, arXiv preprint arXiv:1812
 Reinforcement learning with unsupervised auxiliary tasks,2016, arXivpreprint arXiv:1611
 Qt-Opt: Scalable deepreinforcement learning for vision-based robotic manipulation,2018, arXiv preprint arXiv:1806
 Continuous control with deep reinforcement learn-ing,2015, arXiv:1509
 Playing Atari with Deep Reinforcement Learning,1312, arXiv:1312
 Learning by Playing -Solving Sparse Reward Tasks from Scratch,1802, arXiv:1802
 Q-learning forcontinuous actions with cross-entropy guided policies,2019, arXiv preprint arXiv:1903
 Reinforcement Learning: An Introduction,2018, MIT Press
 Analysis of temporal-diffference learning with functionapproximation,1997, In Advances in neural information processing systems
 Learning with Delayed Rewards,1989, PhD thesis
 Exploiting the sign of the advantage function to learn deterministicpolicies in continuous domains,2019, arXiv preprint arXiv:1906
