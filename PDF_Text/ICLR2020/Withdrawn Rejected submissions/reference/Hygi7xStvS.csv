title,year,conference
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Lossless data compression with neural networks,2019, 2019
 Lossless data compression with neural networks,2019, 2019
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, CoRR
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Asymmetric numeral systems,2009, CoRR
 Finding structure in time,1990, Cognitive science
 Lossless compression based on the sequence mem-oizer,2010, In 2010 Data Compression Conference
 Bridging nonlinearities and stochastic regularizers with gaussianerror linear units,2016, 2016
 Long short-term memory,1997, Neural computation
 A method for the construction of minimum-redundancy codes,1952, Proceedings ofthe Institute of Radio Engineers
 A method for the construction of minimum-redundancy codes,1952, Proceedings ofthe IRE
 CMIX version 17,2014, 2014
 lstm-compress: data comPression using LSTM,2015, 2015
 Languagemodels are unsupervised multitask learners,2019, 2019
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 Ppm: One step to practicality,2002, In Proceedings DCC 2002
 Adaptive attentionspan in transformers,2019, In Proceedings of the 57th Conference of the Association for ComputationalLinguistics
 Adaptive attentionspan in transformers,2019, arXiv preprint arXiv:1905
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Online learning with gated linear networks,2017, arXiv preprintarXiv:1712
 A stochasticmemoizer for sequence data,2009, In Proceedings of the 26th Annual International Conference onMachine Learning
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, CoRR
