title,year,conference
 Unitary evolution recurrent neural networks,2016, InInternational Conference on Machine Learning
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 An empirical evaluation of generic convolutional andrecurrent networks for sequence modeling,2018, arXiv preprint arXiv:1803
 Trellis networks for sequence modeling,2018, arXiv preprintarXiv:1810
 Learning long-term dependencies with gradientdescent is difficult,1994, IEEE transactions on neural networks
 Towards non-saturating recurrent units for modelling long-term dependencies,2019, arXivpreprint arXiv:1902
 Dilated recurrent neural networks,2017, In Advancesin Neural Information Processing Systems
 Learning phrase representations using rnn encoder-decoder forstatistical machine translation,2014, arXiv preprint arXiv:1406
 Empirical evaluation ofgated recurrent neural networks on sequence modeling,2014, arXiv preprint arXiv:1412
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXivpreprint arXiv:1901
 Neural turing machines,2014, arXiv preprint arXiv:1410
 Lstm:A search space odyssey,2016, IEEE transactions on neural networks and learning systems
 Noisy activation functions,2016, InInternational conference on machine learning
 Memory augmented neural networks withwormhole connections,2017, arXiv preprint arXiv:1701
 Recurrent orthogonal networks and long-memorytasks,2016, arXiv preprint arXiv:1602
 Long short-term memory,1997, Neural computation
 Optimizing agent behavior over long time scales by transportingvalue,2018, arXiv preprint arXiv:1810
 Categorical reparameterization with gumbel-softmax,2016, arXivpreprint arXiv:1611
 An empirical exploration of recurrentnetwork architectures,2015, In International Conference on Machine Learning
 Recurrent experi-ence replay in distributed reinforcement learning,2018, 2018
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 The narrativeqa reading comprehension challenge,2018, Transactions of theAssociation for Computational Linguistics
 A clockwork rnn,2014, arXiv preprintarXiv:1402
 Zoneout: Regularizing rnns byrandomly preserving hidden activations,2016, arXiv preprint arXiv:1606
 A simple way to initialize recurrent networks ofrectified linear units,2015, arXiv preprint arXiv:1504
 Towards binary-valuedgates for robust lstm training,2018, arXiv preprint arXiv:1806
 The concrete distribution: A continuous relaxationof discrete random variables,2016, arXiv preprint arXiv:1611
 On the state of the art of evaluation in neural languagemodels,2017, arXiv preprint arXiv:1707
 Asynchronous methods for deep reinforcementlearning,2016, In International conference on machine learning
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Fast parametric learning with activationmemorization,2018, arXiv preprint arXiv:1803
 Relational recurrent neuralnetworks,2018, In Advances in Neural Information Processing Systems
 Ordered neurons: Integrating treestructures into recurrent neural networks,2018, arXiv preprint arXiv:1810
 Learning longer-term dependenciesin rnns with auxiliary losses,2018, arXiv preprint arXiv:1803
 The unreasonable effectiveness of the forget gate,2018, arXivpreprint arXiv:1804
 Unsupervised predictivememory in a goal-directed agent,2018, arXiv preprint arXiv:1803
 Memory networks,2014, arXiv preprint arXiv:1410
 Learning to execute,2014, arXiv preprint arXiv:1410
