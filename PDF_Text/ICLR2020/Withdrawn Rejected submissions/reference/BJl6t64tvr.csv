title,year,conference
 Memory-efficient adaptive optimiza-tion for large-scale learning,2019, arXiv preprint arXiv:1901
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National Academyof Sciences
 Large scale gan training for high fidelity naturalimage synthesis,2018, arXiv preprint arXiv:1809
 The best of both worlds:Combining recent advances in neural machine translation,2018, In Proceedings of the 56th AnnualMeeting of the Association for Computational Linguistics
 Extreme tensoring forlow-memory preconditioning,2019, arXiv preprint arXiv:1902
 Parsing as language modeling,2016, In Proceedings of the 2016Conference on Empirical Methods in Natural Language Processing
 Span-based constituency parsing with a structure-label system andprovably optimal dynamic oracles,2016, 2016
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR09
 BERT: pre-training of deepbidirectional transformers for language understanding,2018, CoRR
 Characterizing implicit bias interms of optimization geometry,2018, arXiv preprint arXiv:1802
 Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor,2018, arXiv preprintarXiv:1801
 Introduction to online convex optimization,2016, Foundations and TrendsR in Optimization
 Deep residual learning for image recog-nition,2015, arXiv preprint arXiv:1512
 Neural networks for machine learninglecture 6a overview of mini-batch gradient descent,2012, Cited on
 In-datacenter performance analysisof a tensor processing unit,2017, In Computer Architecture (ISCA)
 Improving generalization performance by switching fromadam to sgd,2017, arXiv preprint arXiv:1712
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Glow: Generative flow with invertible 1x1 convolutions,2018, InAdvances in Neural Information Processing Systems
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 The penn treebank: annotating predicate argumentstructure,1994, In Proceedings of the workshop on Human Language Technology
 Adaptive bound optimization for online convex opti-mization,2010, arXiv preprint arXiv:1002
 The generalization error of random features regression: Preciseasymptotics and double descent curve,2019, arXiv preprint arXiv:1908
 Dynet: The dynamic neural network toolkit,2017, arXivpreprint arXiv:1701
 Languagemodels are unsupervised multitask learners,2019, 2019
 On the convergence of Adam and beyond,2018, InInternational Conference on Learning Representations
 Deepobs: A deep learning optimizer benchmarksuite,2019, arXiv preprint arXiv:1903
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
