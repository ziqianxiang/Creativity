title,year,conference
 Closing the generalization gap of adaptive gradient methods intraining deep neural networks,2018, arXiv preprint arXiv:1806
 Saga: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, In Advances in neural informationprocessing Systems
 Deep learning,2016, MIT press
 Neural networks for machine learninglecture 6a overview of mini-batch gradient descent,2012, Cited on
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in neural information processing systems
 Improving generalization performance by switching fromadam to sgd,2017, arXiv preprint arXiv:1712
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 On the variance of the adaptive learning rate and beyond,2019, arXiv preprint arXiv:1908
 On the convergence of adam and beyond,2019, arXivpreprint arXiv:1904
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Normalized direction-preserving adam,2017, arXivpreprint arXiv:1709
