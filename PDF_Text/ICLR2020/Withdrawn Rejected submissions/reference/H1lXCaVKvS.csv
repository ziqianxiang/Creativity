title,year,conference
   Identifying beneficial task relations for multi-task learningin deep neural networks,2017,  In Proceedings of the 15th Conference of the European Chapter of theAssociation for Computational Linguistics: Volume 2
 Multitask learning,1573, Machine Learning
   Semi-supervised  se-quence modeling with cross-view training,2018,  In Proceedings of the 2018 Conference on EmpiricalMethods in Natural Language Processing
  A unified architecture for natural language processing:  Deepneural networks with multitask learning,2008,   In Proceedings of the 25th International Conferenceon Machine Learning
  Long short-term memory,1997,  Neural computation
 Adaptive mixturesof local experts,0899, Neural Comput
  Scheduled multi-task learning: From syntax to trans-lation,2018,  Transactions of the Association for Computational Linguistics
  Simple and accurate dependency parsing using bidirec-tional lstm feature representations,2016, Transactions of the Association for Computational Linguistics
     Lstm-based  mixture-of-experts  forknowledge-aware dialogues,2016, In Proceedings of the 1st Workshop on Representation Learning forNLP
 Why Mheads are better than one: Training a diverse ensemble of deep networks,2015, CoRR
 Pseudo-task augmentation: From deep multitask learningto intratask sharingâ€”and back,2018, 03 2018
  Dynet:  The dynamic neural network toolkit,2017,  arXivpreprint arXiv:1701
 Universal dependencies 2,2018,2
   Strong baselines for neural semi-supervised learning underdomain shift,2018,  In Proceedings of the 56th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
  Re-thinking the inception architecture for computer vision,2016, In CVPR
