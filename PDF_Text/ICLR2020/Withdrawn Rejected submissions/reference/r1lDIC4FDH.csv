title,year,conference
 Learning long-term dependencies with gradi-ent descent is difficult,1994, IEEE transactions on neural networks
 Estimating or propagating gradientsthrough stochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 Skip rnn:Learning to skip state updates in recurrent neural networks,2017, arXiv preprint arXiv:1708
 Learning phrase representations using rnn encoder-decoderfor statistical machine translation,2014, arXiv preprint arXiv:1406
 Recurrent batch normal-ization,2016, CoRR
 From conditioning to conscious recollection: Memorysystems of the brain,2004, Number 35
 Hierarchical recurrent neural networks for long-term dependen-cies,1996, In Advances in neural information processing systems
 Hybrid computing using a neural network with dynamic external memory,2016, Nature
 An empirical exploration of recurren-t network architectures,2015, In International Conference on International Conference on MachineLearning
 Adam: A method for stochastic optimization,2014, CoRR
 A simple way to initialize recurrent networks ofrectified linear units,2015, CoRR
 A critical review of recurrent neural networksfor sequence learning,2015, arXiv preprint arXiv:1506
 Phased lstm: Accelerating recurrent networktraining for long or event-based sequences,2016, In Advances in neural information processing systems
 The unreasonable effectiveness of the forget gate,2018, CoRR
 Architectural complexity measures of recurrent neural networks,2016, CoRR
