title,year,conference
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Cross-lingual language model pretraining,2019, arXiv preprintarXiv:1901
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Pointer sentinel mixturemodels,2017, In ICLR
 Recurrentneural network based language model,2010, In INTERSPEECH
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Patient knowledge distillation for bert modelcompression,2019, arXiv preprint arXiv:1908
 Multilingual neural machine translation withknowledge distillation,2019, In International Conference on Learning Representations
 Distilling task-specific knowledge from bert into simple neural networks,2019, arXiv preprint arXiv:1903
 Knowledge distillation in generations:More tolerant teachers educate better students,2018, arXiv preprint arXiv:1805
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Kl-divergence regularized deep neuralnetwork adaptation for improved large vocabulary speech recognition,2013, In 2013 IEEE InternationalConference on Acoustics
 An efficient way to learn rules for grapheme-to-phonemeconversion in chinese,2002, In International Symposium on Chinese Spoken Language Processing
