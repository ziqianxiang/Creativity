title,year,conference
 A general and adaptive robust loss function,2017, arXiv preprint arXiv:1701
 Autoaugment:Learning augmentation policies from data,2018, arXiv preprint arXiv:1805
 Neural architecture search: A survey,2018, arXivpreprint arXiv:1808
 Faster training by selecting samplesusing embeddings,2019, In 2019 International Joint Conference on Neural Networks (IJCNN)
 Adapting arbitrary normal mutation distributions inevolution strategies: The covariance matrix adaptation,1996, In Proceedings of IEEE internationalconference on evolutionary computation
 Completely derandomized self-adaptation in evolutionstrategies,2001, Evolutionary computation
 Improving neural networks by preventing co-adaptation of feature detectors,2012, arXiv preprintarXiv:1207
 Evolved policy gradients,2018, In Advances in Neural Information Processing Systems
 On loss functions for deep neural networks inclassification,2017, arXiv preprint arXiv:1702
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009, 2009
 The surprising creativity of digital evolution: A collection of anecdotesfrom the evolutionary computation and artificial life research communities,2018, arXiv preprintarXiv:1803
 CMA-ES for hyperparameter optimization of deep neural net-works,2016, arXiv preprint arXiv:1604
 Regularizingneural networks by penalizing confident output distributions,2017, arXiv preprint arXiv:1701
 Regularized evolution for imageclassifier architecture search,2019, In Proceedings of the AAAI Conference on Artificial Intelligence
 Distilling free-form natural laWs from experimental data,0036, Science
 Designing neuralnetWorks through neuroevolution,2019, Nature Machine Intelligence
 Distributed computing in practice: the Condorexperience,2005, Concurrency and computation: practice and experience
