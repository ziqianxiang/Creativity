title,year,conference
 Label-embedding forimage classification,2015, IEEE transactions on pattern analysis and machine intelligence
 Large scale distributed neural network training through online distillation,2018, ArXiv
 Label refinery:Improving imagenet classification through label progression,2018, ArXiv
 Learn-ing efficient object detection models with knowledge distillation,2017, In NIPS
 Deep filter banks for texture recognition andsegmentation,2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Imagenet: A large-scalehierarchical image database,2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition
 Self-knowledge distillation in natural language processing,2019, arXivpreprint arXiv:1908
 Knowledge transfer via distillationof activation boundaries formed by hidden neurons,2018, ArXiv
 Learning multiple layers of features from tiny images,2009, 2009
 Self-referenced deep learning,2018, ArXiv
 Delta:Deep learning transfer using feature map with attention for convolutional networks,2019, ArXiv
 Sgdr: Stochastic gradient descent with warm restarts,2016, In ICLR
 Fine-grainedvisual classification of aircraft,2013, ArXiv
 Building a large annotatedcorpus of english: ThePenntreebank,1993, Computational Linguistics
 Regularizingneural networks by penalizing confident output distributions,2017, arXiv preprint arXiv:1701
 Fitnets: Hints for thin deep nets,2014, CoRR
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Learning hierarchicalsimilarity metrics,2012, In 2012 IEEE conference on computer vision and pattern recognition
 Dataset distillation,2019, ArXiv
 Snapshot distillation: Teacher-studentoptimization in one generation,2018, ArXiv
 Training deep neural netWorks ingenerations: A more tolerant teacher educates better students,2019, In AAAI
 Visual relationship detection With internaland external linguistic knoWledge distillation,2017, 2017 IEEE International Conference on ComputerVision (ICCV)
 Paying more attention to attention: Improving the perfor-mance of convolutional neural netWorks via attention transfer,2016, arXiv preprint arXiv:1612
