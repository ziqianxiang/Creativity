title,year,conference
 Shapenet: An information-rich 3dmodel repository,2015, arXiv preprint arXiv:1512
 On the convergence of a class of adam-typealgorithms for non-convex optimization,2019, In International Conference on Learning Representation(ICLR)
 Incorporating nesterov momentum into adam,2016, ICLR Workshop
 Accelerated gradient methods for nonconvex nonlinear andstochastic programming,2016, Mathematical Programming
 Logarithmic regret algorithms for online convexoptimization,2007, Machine Learning
 Long short-term memory,1997, Neural Computation
 Matrix analysis,2012, Cambridge university Press
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 ImProving generalization Performance by switching fromadam to sgd,2017, arXiv preprint arXiv:1712
 Adam: A method for stochastic oPtimization,2015, In InternationalConference on Learning Representation (ICLR)
 Diagonal rescaling for neural networks,2017, arXivpreprint arXiv:1705
 AdaPtive gradient methods with dynamicbound of learning rate,2019, arXiv preprint arXiv:1902
 AdaPtive bound oPtimization for online convex oPti-mization,2010, In Conference on Computational Learning Theory (COLT)
 SPectral normalization forgenerative adversarial networks,2018, In International Conference on Learning Representations
 A 3d facemodel for Pose and illumination invariant face recognition,2009, In 2009 Sixth IEEE InternationalConference on Advanced Video and Signal Based Surveillance
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representation (ICLR)
 A stochastic aPProximation method,1951, The annals of mathematicalstatistics
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems (NIPS)
 3d shapenets: A deep representation for volumetric shapes,2015, In Proceedings of the IEEEconference on computer vision and pattern recognition
 Block-normalized gradient method: An empirical study for training deep neural network,2017, arXiv preprintarXiv:1707
 Adaptive methodsfor nonconvex optimization,2018, In Advances in Neural Information Processing Systems
 Recurrent neural network regularization,2014, arXivpreprint arXiv:1409
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Blockwise adaptivity: Faster training and better generalization indeep learning,2019, arXiv preprint arXiv:1905
 On the convergence of adagrad with momentum for training deep neuralnetworks,2018, arXiv preprint arXiv:1808
