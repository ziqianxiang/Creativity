title,year,conference
 A causal framework for explaining the predictions ofblack-box sequence-to-sequence models,2017, arXiv preprint arXiv:1707
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Controlling attributeeffect in linear regression,2013, In 2013 IEEE 13th International Conference on Data Mining
 Intel-ligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission,2015, InProceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery andData Mining
 Report onthe 11th IWSLT evaluation campaign,2014, In Proceedings of IWSLT
 Retain: An interpretable predictive model for healthcare using reverse time attentionmechanism,2016, In Advances in Neural Information Processing Systems
 Latent alignment andvariational attention,2018, In Advances in Neural Information Processing Systems
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Attention is not explanation,2019, CoRR
 Categorical reparameterization with gumbel-softmax,2016, arXivpreprint arXiv:1611
 Auto-encoding variational bayes,2013, arXiv preprintarXiv:1312
 Rationalizing neural predictions,2016, arXiv preprintarXiv:1606
 Understanding random forests: From theory to practice,2014, arXiv preprintarXiv:1407
 Effective approaches to attention-based neural machine translation,2015, arXiv preprint arXiv:1508
 The concrete distribution: A continuousrelaxation of discrete random variables,2016, arXiv preprint arXiv:1611
 From softmax to sparsemax: A sparse model of attentionand multi-label classification,2016, In International Conference on Machine Learning
 A regularized framework for sparse and structured neural atten-tion,2017, In Advances in Neural Information Processing Systems
 Interpretable structure induction via sparse atten-tion,2018, In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and InterpretingNeural Networks for NLP
 Languagemodels are unsupervised multitask learners,2019, 2019
 Scalable and accurate deep learning with electronichealth records,2018, NPJ Digital Medicine
 Why should i trust you?: Explaining thepredictions of any classifier,2016, In Proceedings of the 22nd ACM SIGKDD international conferenceon knowledge discovery and data mining
 A neural attention model for abstractivesentence summarization,2015, arXiv preprint arXiv:1509
 Compositional generalization in a deep seq2seqmodel by separating syntax and semantics,2019, arXiv preprint arXiv:1904
 Grad-cam: Visual explanations from deep networks via gradient-based local-ization,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Surprisingly easy hard-attention for sequenceto sequence learning,2018, In Proceedings of the 2018 Conference on Empirical Methods in NaturalLanguage Processing
 Lstmvis: A toolfor visual analysis of hidden state dynamics in recurrent neural networks,2017, IEEE transactions onvisualization and computer graphics
 A shared attention mechanismfor interpretation of neural automatic post-editing systems,2018, arXiv preprint arXiv:1807
 Attention is All YoU Need,2017, In Proceedings ofNIPS
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
