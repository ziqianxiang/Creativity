title,year,conference
 Faster SGD training by minibatchpersistency,2018, arXiv preprint arXiv:1806
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In International Conference on Machine Learning
 Not all samples are created equal: Deep learning withimportance sampling,2018, arXiv preprint arXiv:1803
 Efficienttraining of convolutional neural nets on large distributed systems,2018, 2018 IEEE InternationalConference on Cluster Computing (CLUSTER)
 Microsoft COCO: Common objects in context,2014, In Europeanconference on computer vision
 SSD: Single shot multibox detector,2016, In European conference on computervision
 Human-level controlthrough deep reinforcement learning,2015, Nature
 Some methods of speeding up the convergence of iteration methods,1964, USSRComputational Mathematics and Mathematical Physics
 Learning representations byback-propagating errors,1986, Nature
 Measuring the effects of data parallelism on neural network training,2018, arXiv preprintarXiv:1811
 On the importance ofinitialization and momentum in deep learning,2013, In International Conference on Machine Learning
 Attention is all you need,2017, In Advances in Neural InformationProcessing Systems
 Image classification atsupercomputer scale,2018, arXiv preprint arXiv:1811
 Parallelized stochastic gradientdescent,2010, In Advances in neural information processing systems
