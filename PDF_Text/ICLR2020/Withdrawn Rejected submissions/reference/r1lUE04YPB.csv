title,year,conference
 Pseudo-recursal:Solving the catastrophic forgetting problem in deep neural networks,2018, CoRR
 Realor fake? learning to discriminate machine from human generated text,2019, CoRR
 Learning phrase representations using rnn encoder-decoder forstatistical machine translation,2014, In Proceedings of the 2014 Conference on Empirical Methods inNatural Language Processing (EMNLP)
 BERT: pre-training of deepbidirectional transformers for language understanding,2018, CoRR
 Why does unsupervised pre-training help deep learning? J,1532, Mach
 Detecting egregious responses in neural sequence-to-sequence mod-els,2019, In International Conference on Learning Representations
 Negative training for neural dialogue response generation,2019, CoRR
 The curious case of neural text degener-ation,2019, CoRR
 Overcoming catastrophic for-getting in neural networks,2016, Proceedings of the National Academy of Sciences of the United StatesOfAmerica
 Skip-thought vectors,2015, In C
 Importance of a searchstrategy in neural dialogue modelling,2018, arXiv preprint arXiv:1811
 An empirical evalua-tion of multi-task learning in deep neural networks for natural language processing,2019, CoRR
 A diversity-promotingobjective function for neural conversation models,2016, In NAACL HLT 2016
 Dailydialog: A man-ually labelled multi-turn dialogue dataset,2017, In Proceedings of the Eighth International JointConference on Natural Language Processing (Volume 1: Long Papers)
 Masktextspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes,2019, CoRR
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Effective approaches to attention-basedneural machine translation,2015, In Proceedings of the 2015 Conference on Empirical Methods inNatural Language Processing
 Pretraining meth-ods for dialog context representation learning,2019, CoRR
 Recurrentneural network based language model,2010, In INTERSPEECH 2010
 Deep contextualized word representations,2018, In Proc
 Languagemodels are unsupervised multitask learners,2019, 2019
 Representation stability as a regularizer forimproved text analytics transfer learning,2017, CoRR
 Do neural dialog systems use the conversation history effectively? an empirical study,2019, CoRR
 Neural machine translation of rare words withsubword units,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
 Mass: Masked sequence to sequencepre-training for language generation,2019, arXiv preprint arXiv:1905
 Sequence to sequence learning with neural net-works,2014, In Advances in Neural Information Processing Systems 27: Annual Conference on NeuralInformation Processing Systems 2014
 Multitask learning with low-levelauxiliary tasks for encoder-decoder based speech recognition,2017, CoRR
 Attention is all you need,2017, In I
 Transfertransfo: A transferlearning approach for neural network based conversational agents,2019, CoRR
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, CoRR
 Recent trends in deeplearning based natural language processing,2017, CoRR
 Flexibleend-to-end dialogue system for knowledge grounded conversation,2017, CoRR
