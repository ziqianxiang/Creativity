title,year,conference
 Sorting out Lipschitz function approximation,2019, InInternational Conference on Machine Learning (ICML)
 Naive feature selection: Sparsity innaive bayes,2019, 2019
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, In International Conference on MachineLearning (ICML)
 Approximation and estimation bounds for artificial neural networks,1994, Machine Learning
 A second order cone programming formula-tion for classifying missing data,2005, In Advances in Neural Information Processing Systems (NIPS)
 A kernel perspective for regular-izing deep neural networks,2019, In International Conference on Machine Learning (ICML)
 Robust Wasserstein profile inference and applica-tions to machine learning,2016, arXiv:1610
 Data-driven optimal transport costselection for distributionally robust optimization,2017, arXiv:1705
 Towards evaluating the robustness of neural networks,2017, In 2017IEEE Symposium on Security and Privacy (SP)
 Parsevalnetworks: Improving robustness to adversarial examples,2017, In International Conference on MachineLearning (ICML)
 Distributionally robust optimization under moment uncertainty withapplication to data-driven problems,2010, Operations Research
 On the nystr om method for approximating a gram matrix for improvedkernel-based learning,2005, JMLR
 Statistics of robust optimization: A generalizedempirical likelihood approach,2016, arXiv:1610
 Data-driven distributionally robust optimizationusing the Wasserstein metric: Performance guarantees and tractable reformulations,2018, MathematicalProgramming
 A minimax approach to supervised learning,2016, In Advances in NeuralInformation Processing Systems (NIPS)
 Generalizable adversarial training via spectral normaliza-tion,2019, In International Conference on Learning Representations (ICLR)
 Distributionally robust stochastic optimization with Wassersteindistance,2016, arXiv:1604
 Explaining and harnessing adversarialexamples,2015, arXiv:1412
 Regularisation of neural networksby enforcing Lipschitz continuity,2018, arXiv:1804
 A general formula on the conjugate of the difference of functions,1496, CanadianMathematical Bulletin
 From convex optimization to nonconvex optimization,1989, necessary andsufficient conditions for global optimality
 Limitations of the Lipschitz constant as adefense against adversarial examples,2018, arXiv:1807
 On the translocation of masses,1958, Management Science
 An approximate Shapley-Folkmantheorem,2019, 2019
 Eigenvalue Distribution ofCompact OPerators,1986, Birkhauser
 Introduction to Gaussian processes,1998, In C
 Extension of range of functions,1934, Bull
 Some properties of Gaussian reproducing kernel Hilbert spaces and their implica-tions for function approximation and learning theory,2010, Constructive Approximation
 Spectral normalization forgenerative adversarial networks,2018, In International Conference on Learning Representations (ICLR)
 Certified defenses against adversarialexamples,2018, In International Conference on Learning Representations (ICLR)
 Gaussian Processes for Machine Learning,2006, MIT Press
 Lipschitz regularity of deep neural networks: analysis andefficient estimation,2018, In Advances in Neural Information Processing Systems (NIPS)
 Distribu-tionally robust logistic regression,2015, In Advances in Neural Information Processing Systems (NIPS)
 Regularization viamass transportation,2017, arXiv:1710
 Understanding adversarial training: Increasinglocal stability of supervised models through robust optimization,2018, Neurocomputing
 Certifying some distributional robustness withprincipled adversarial training,2018, In International Conference on Learning Representations (ICLR)
 Revisiting ad-versarial risk,2019, In International Conference on Artificial Intelligence and Statistics (AISTATS)
 Intriguing properties of neural networks,2014, In International Conference on LearningRepresentations (ICLR)
 An introduction to matrix concentration inequalities,2015, Foundations and Trends inMachine Learning
 Lipschitz-margin training: Scalable certificationof perturbation invariance for deep neural networks,2018, In Advances in Neural Information ProcessingSystems (NIPS)
 Bounding duality gap for separable problems with linearconstraints,1573, Computational Optimization and Applications
 Analytic extensions of differentiable functions defined in closed sets,1934, Transactionsof the American Mathematical Society
 Using the NystrOm method to speed UP kernel machines,2000, InAdvances in Neural Information Processing Systems (NIPS)
 Generalization bounds for regularization networksand support vector machines via entropy numbers of compact operators,2001, IEEE Transactions onInformation Theory
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2018, In International Conference on Machine Learning (ICML)
 Scaling provable adversarialdefenses,2018, In Advances in Neural Information Processing Systems (NIPS)
 Robust regression and lasso,2009, In Advances inNeural Information Processing Systems (NIPS)
 Spectral norm regularization for improving the generalizabilityof deep learning,2017, arXiv:1705
 '1-regularized neural networks are improperlylearnable in polynomial time,2016, In International Conference on Machine Learning (ICML)
 Convexified convolutional neural networks,2017, InInternational Conference on Machine Learning (ICML)
 Data-driven risk-averse stochastic optimization with Wassersteinmetric,2018, Operations Research Letters
