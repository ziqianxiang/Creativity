title,year,conference
 Superpositionof many models into one,2019, feb 2019
 BinaryNet: Training Deep Neural Networks withWeights and Activations Constrained to +1 or -1,2016, arXiv
 BERT: Pre-training of DeepBidirectional Transformers for Language Understanding,2018, oct 2018
 Deep Residual Learning for ImageRecognition,1664, Arxiv
 LanguageModels are Unsupervised Multitask Learners,2019, 2019
 Attention Is All You Need,2017, (Nips)
