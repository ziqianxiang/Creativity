title,year,conference
   On exactcomputation with an infinitely wide neural net,2019,  In Advances in Neural Information Processing Systems(NeurIPS)
 Fine-grained analysis of optimization andgeneralization for overparameterized two-layer neural networks,2019, arXiv preprint arXiv:1901
   Sgd learns over-parameterizednetworks that provably generalize on linearly separable data,2017, arXiv preprint arXiv:1710
 A generalization theory of gradient descent for learning over-parameterized deeprelu networks,2019, arXiv preprint arXiv:1902
 On the global convergence of gradient descent for over-parameterized modelsusing optimal transport,2018, In Advances in Neural Information Processing Systems 31
 On lazy training in differentiable programming,2019, In Advancesin Neural Information Processing Systems (NeurIPS)
 Bert: Pre-training of deep bidirectionaltransformers for language understanding,2018, arXiv preprint arXiv:1810
 Gradient descent finds global minima ofdeep neural networks,2018, arXiv preprint arXiv:1811
   Gradient descent provably optimizes over-parameterized neural networks,2019, In International Conference on Learning Representations
 Limitations of lazy training oftwo-layers neural networks,2019, In Advances in Neural Information Processing Systems (NeurIPS)
  Identity matters in deep learning,2016,  In International Conference on LearningRepresentations
 Deep residual learning for image recognition,2016, InThe IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  Batch normalization:  Accelerating deep network training by reducinginternal covariate shift,2015, In International Conference on Machine Learning (ICML)
 Neural tangent kernel: Convergence and generalization inneural networks,2018, In Advances in neural information processing systems
 Adaptive estimation of a quadratic functional by model selection,2000, Annalsof Statistics
 Gradient-based learning applied to documentrecognition,1998, Proceedings of the IEEE
 Learning overparameterized neural networks via stochastic gradient descent onstructured data,2018, In Advances in Neural Information Processing Systems
  Skip connections eliminate singularities,2018,  In International Conference onLearning Representations (ICLR)
 Attention is all you need,2017, In Advances in neural information processing systems
  Residual networks behave like ensembles of relativelyshallow networks,2016, In Advances in Neural Information Processing Systems (NIPS)
 Introduction to the non-asymptotic analysis of random matrices,2012, Compressed Sensing
 On the local hessian in back-propagation,2018, In Advances in NeuralInformation Processing Systems (NeurIPS)
 Towards robust resnet: Asmall step but a giant leap,2019, In International Joint Conferences on Artificial Intelligence (IJCAI)
 An improved analysis of training over-parameterized deep neural networks,2019, InAdvances in Neural Information Processing Systems (NeurIPS)
    Stochastic  gradient  descent  optimizes  over-parameterized deep relu networks,2018, arXiv preprint arXiv:1811
