title,year,conference
 There are many con-sistent explanations of unlabeled data: Why you should average,2018, 2018
 Learning with pseudo-ensembles,2014, In Advancesin Neural Information Processing Systems 
 Mixmatch: A holistic approach to semi-supervised learning,2019, arXiv preprintarXiv:1905
 Unlabeled dataimproves adversarial robustness,2019, arXiv preprint arXiv:1905
 Semi-supervised se-quence modeling with cross-view training,2018, arXiv preprint arXiv:1809
 A unified architecture for natural language processing: Deepneural networks with multitask learning,2008, In Proceedings of the 25th international conference onMachine learning
 Autoaugment:Learning augmentation policies from data,2018, arXiv preprint arXiv:1805
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Understanding back-translation atscale,2018, arXiv preprint arXiv:1808
 Deep speech: Scaling up end-to-endspeech recognition,2014, arXiv preprint arXiv:1412
 Sequence to sequence mixture model fordiverse machine translation,2018, arXiv preprint arXiv:1810
 Data-efficientimage recognition with contrastive predictive coding,2019, arXiv preprint arXiv:1905
 Data augmentation instead of explicit regularization,2018, arXivpreprint arXiv:1806
 Semi-supervised learning by label gradient alignment,2019, arXivpreprint arXiv:1902
 Semi-supervised classification with graph convolutional net-works,2016, arXiv preprint arXiv:1609
 Stochastic beams and where to findthem: The gumbel-top-k trick for sampling sequences without replacement,2019, arXiv preprintarXiv:1903
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Temporal ensembling for semi-supervised learning,2016, arXiv preprintarXiv:1610
 Learning noise-invariant representations forrobust speech recognition,2018, In 2018 IEEE Spoken Language Technology Workshop (SLT)
 Smooth neighbors on teacher graphsfor semi-supervised learning,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Auxiliary deep gen-erative models,2016, arXiv preprint arXiv:1602
 Image-based rec-ommendations on styles and substitutes,2015, In Proceedings of the 38th International ACM SIGIRConference on Research and Development in Information Retrieval
 Adversarial training methods for semi-supervised text classification,2016, arXiv preprint arXiv:1605
 Virtual adversarial training: aregularization method for supervised and semi-supervised learning,2018, IEEE transactions on patternanalysis and machine intelligence
 Readingdigits in natural images with unsupervised feature learning,2011, 2011
 Realis-tic evaluation of deep semi-supervised learning algorithms,2018, In Advances in Neural InformationProcessing Systems
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Semi-supervised learning with ladder networks,2015, In Advances in neural information processing systems
 Revisiting lstm networks forsemi-supervised text classification via mixed objective function,2018, 2018
 Regularization with stochastic transfor-mations and perturbations for deep semi-supervised learning,2016, In Advances in Neural InformationProcessing Systems
 Invariant representation learn-ing for robust deep networks,2018, In Workshop on Integration of Deep Learning Theories
 Improving neural machine translation modelswith monolingual data,2015, arXiv preprint arXiv:1511
 Mixture models for diversemachine translation: Tricks of the trade,2019, arXiv preprint arXiv:1902
 Transformation invariancein pattern recognitionâ€”tangent distance and tangent propagation,1998, In Neural networks: tricks ofthe trade
 Mean teachers are better role models: Weight-averaged consis-tency targets improve semi-supervised deep learning results,2017, In Advances in neural informationprocessing systems
 Selfie: Self-supervised pretraining for imageembedding,2019, arXiv preprint arXiv:1906
 Interpolation con-sistency training for semi-supervised learning,2019, arXiv preprint arXiv:1903
 Switchout: an efficient data augmenta-tion algorithm for neural machine translation,2018, arXiv preprint arXiv:1808
 Deep learning via Semi-supervised embedding,2012, In Neural Networks: Tricks of the Trade
 Revisiting semi-supervised learningwith graph embeddings,2016, arXiv preprint arXiv:1603
 Semi-supervised qa withgenerative domain-adaptive nets,2017, arXiv preprint arXiv:1702
 Unsupervised embedding learning viainvariant and spreading instance feature,2019, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Qanet: Combining local convolution with global self-attention for reading compre-hension,2018, arXiv preprint arXiv:1804
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Adversariallyrobust generalization just requires more unlabeled data,2019, arXiv preprint arXiv:1906
 S4l: Self-supervised semi-supervised learning,2019, arXiv preprint arXiv:1905
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
 Character-level convolutional networks for text clas-sification,2015, In Advances in neural information processing systems
36 and 4,2019,23 with orwithout RandAugment
 Despitethat state-of-the-art data augmentation methods can generate diverse and valid augmented examplesas discussed in section 2,2020,2
