title,year,conference
 Byzantine stochastic gradient descent,2018, arXiv preprintarXiv:1803
 Machine learning with adversaries: Byzantine tolerantgradient descent,2017, In Advances in Neural Information Processing Systems
 Draco: Byzantine-resilient distributedtraining via redundant gradients,2018, In ICML
 Distributed statistical machine learning in adversarial settings: Byzantinegradient descent,2017, POMACS
 Asynchronous byzantinemachine learning,2018, arXiv preprint arXiv:1802
 Slow and stale gradients can win the race:Error-runtime trade-offs in distributed sgd,2018, In AISTATS
 Distributed robust learning,2014, arXiv preprint arXiv:1409
 Robust statistics,1248, In International Encyclopedia of Statistical Science
 Learning multiple layers of features from tiny images,2009, 2009
 The byzantine generals problem,1982, ACM Transactions onProgramming Languages and Systems (TOPLAS)
 Asynchronous decentralized parallel stochastic gradientdescent,2018, In ICML
 Regularizing and Optimizing LSTM Language Models,2017, arXivpreprint arXiv:1708
 The hidden vulnerability of distributed learning inbyzantium,2018, arXiv preprint arXiv:1802
 Gradient methods for minimizing functionals,1963, Zhurnal Vychislitelâ€™noi Matematiki iMatematicheskoi Fiziki
 Fault-tolerant multi-agent optimization: Optimal iterative distributedalgorithms,2016, In PODC
 Defending non-bayesian learning against adversarial attacks,2016, arXiv preprintarXiv:1606
 Zeno: Byzantine-suspicious stochastic gradient descent,2018, CoRR
 Byzantine-robust distributed learning: Towardsoptimal statistical rates,2018, arXiv preprint arXiv:1803
 Slow learners are fast,2009, In Advances in neural informationprocessing systems
