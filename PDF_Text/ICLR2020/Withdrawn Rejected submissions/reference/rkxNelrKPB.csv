title,year,conference
 signSGDwith majority vote is communication efficient and fault tolerant,2019, In International Conference onLearning Representations
 Large scale online learning,2003, In Advances in Neural InformationProcessing Systems
 Stochastic spectral descent for restricted boltz-mann machines,2015, In International Conference on Artificial Intelligence and Statistics (AISTATS)
 Error feedbackfixes SignSGD and other gradient compression schemes,2019, In Proceedings of the 36th InternationalConference on Machine Learning
 Distributed learning withcompressed gradients,2018, In arXiv preprint arXiv:1806
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations
 Imagenet classification with deep convolu-tional neural networks,2012, In Advances in Neural Information Processing Systems
 Deep gradient compression:Reducing the communication bandwidth for distributed training,2018, In International Conference onLearning Representations
 signSGD via zeroth-order oracle,2019, InInternational Conference on Learning Representations
 Distributed learningwith compressed gradient differences,2019, In arXiv preprint arXiv:1901
 SGD with arbitrary sampling: General analysis and improved rates,2019, In InternationalConference on Machine Learning
 On the convergence of Adam and beyond,2019, InInternational Conference on Learning Representations
 A direct adaptive method for faster backpropagation learning:The Rprop algorithm,1993, In IEEE International Conference on Neural Networks
 Deep learning in neural networks: An overview,2015, In Neural networks
 Fast convergence of stochastic gradient descent under a stronggrowth condition,2013, In arXiv preprint arXiv:1308
 Minimizing finite sums with the stochasticaverage gradient,2017, In Mathematical Programming
 1-bit stochastic gradient descent andapplication to data-parallel distributed training of speech DNNs,2014, In Fifteenth Annual Conferenceof the International Speech Communication Association
 On the absolute constants in the berry-esseen type inequalities for identicallydistributed summands,2011, In arXiv preprint arXiv:1111
 Scalable distributed DNN training using commodity GPU cloud computing,2015, InSixteenth Annual Conference of the International Speech Communication Association
 Atomo: Communication-efficient learning via atomic sparsification,2018, In Advances inNeural Information Processing Systems
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in NeuralInformation Processing Systems
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Adaptive methodsfor nonconvex optimization,2018, In Advances in Neural Information Processing Systems
