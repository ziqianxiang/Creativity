title,year,conference
 Mirror descent and nonlinear projected subgradient methods forconvex optimization,2003, Operations Research Letters
 Theory of convex optimization for machine learning,2014, arXiv preprintarXiv:1405
 On the convergence ofa class of adam-typealgorithms for non-convex optimization,2018, arXiv preprint arXiv:1808
 Logarithmic regret algorithms for online convexoptimization,2007, Machine Learning
 Neural networks for machine learninglecture 6a overview of mini-batch gradient descent,2012, lecture
 Nostalgic adam: Weighting more of the past gradientswhen designing the adaptive learning rate,2018, arXiv preprint arXiv:1805
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Exponentiated gradient versus gradient descent for linearpredictors,1997, information and computation
 On the variance of the adaptive learning rate and beyond,2019, arXiv preprint arXiv:1908
 Adaptive gradient methods with dynamicbound of learning rate,2019, arXiv preprint arXiv:1902
 Follow-the-regularized-leader and mirror descent: Equivalence theorems and implicitupdates,2010, CoRR
 Follow-the-regularized-leader and mirror descent: Equivalence theoremsand implicit updates,2010, CoRR
 Adaptive bound optimization for online convex opti-mization,2010, arXiv preprint arXiv:1002
 Problem complexity and methodefficiency in optimization,1983, J
 On the convergence of adam and beyond,2019, arXivpreprint arXiv:1904
 Less regret via online conditioning,2010, arXiv preprintarXiv:1002
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 On the convergence ofadaptive gradient methods for nonconvex optimization,2018, arXiv preprint arXiv:1808
