title,year,conference
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, CoRR
 Approximation by superpositions of a sigmoidal function,1989, MCSS
 Gradient Descent FindsGlobal Minima of Deep Neural Networks,2018, arXiv:1811
 Gradient Descent Provably OptimizesOver-parameterized Neural Networks,1810, arXiv:1810
 Multilayer feedforward networks are universal approxi-mators,0893, Neural Netw
 Learning multiple layers of features from tiny images,2009, 2009
 MNIST handwritten digit database,2010, 2010
 Measuring the intrinsic dimensionof objective landscapes,2018, CoRR
 On the number of linearregions of deep neural networks,2014, In NIPS
 Measurements of three-level hierarchical structure in the outliers in the spectrum ofdeepnet hessians,2019, In ICML
 Nonlinear random matrix theory for deep learning,2017, InI
 Deep informationpropagation,2016, CoRR
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, CoRR
 Understandingdeep learning requires rethinking generalization,2016, CoRR
