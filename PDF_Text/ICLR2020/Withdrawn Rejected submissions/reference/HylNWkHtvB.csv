title,year,conference
 On the convergence of a class of adam-typealgorithms for non-convex optimization,2019, ICLR
 Rmsprop and equilibrated adaptive learning ratesfor non-convex optimization,2015, corrL
 Adaptive subgradient methods for online learning and stochasticoptimization,2011, ICML
 Stochastic First- and Zeroth-order Methods for NonconvexStochastic Programming,2013, SIAM
 Deep residual learning for imagerecognition,2016, CVPR
 Identity mappings in deep residualnetworks,2016, ECCV
 Long short-term memory,1997, Neural computation
 Densely connectedconvolutional networks,2017, CVPR
 Adam: A method for stochastic oPtimization,2015, ICLR
 Learning multiPle layers of features from tiny images,2009, Technical rePort
 On the convergence of adam and beyond,2018, ICLR
 The MarginalValue of Adaptive Gradient Methods in Machine Learning,2017, NIPS
 Aggregated residualtransformations for deep neural networks,2017, CVPR
 Wide residual networks,2016, BMVC
 Adaptive methodsfor nonconvex optimization,2018, NIPS
 Adashift:Decorrelation and convergence of adaptive learning rate methods,2019, ICLR
