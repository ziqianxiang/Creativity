title,year,conference
 Distributed second-order optimization using kronecker-factored approximations,2020, International Conference On Learning Representations (ICLR2017)
 signSGD:Compressed optimisation for non-convex problems,2018, International conference on machine learning(ICML)
 Reconnaissance de la parole par reseaUx Connexionnistes,1988, In Proceedings ofNeuroNimes 88
 Algorithmic regularization in learning deep homogeneousmodels: Layers are automatically balanced,2018, Neural Information Processing Systems (NIPS)
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the Thirteenth International Conference on Artificial Intelligence andStatistics
 A kronecker-factored approximate fisher matrix for convolutionlayers,2016, International Conference on Machine Learning (ICML2016)
 Delving deep into rectifiers: Sur-passing human-level performance on imagenet classification,2015, In Proceedings of the 2015 IEEEInternational Conference on Computer Vision (ICCV)
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Bag of tricks forimage classification with convolutional neural networks,2018, Technical report
 Ueber eine neue auflosungsart der bei der methode der kleinsten quadrate vorkom-menden Iinearen gleichungen,1845, Astron
 Adam: A method for stochastic optimization,2015, InternationalConference for Learning Representations (ICLR2015)
 Imagenet classification with deep convolu-tional neural networks,2012, In Advances in neural information processing systems
 Some estimates of norms of random matrices,2005, Proc
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, International conference on machine learning (ICML)
 Non-asymptotic theory of random matrices: extreme singularvalues,2010, Proceedings of the International Congress of Mathematicians
 Very deep convolutional networks for large-scale imagerecognition,2015, international conference on learning representations (ICLR2015)
 On the importance of initializationand momentum in deep learning,2013, In International conference on machine learning (ICML)
 Topics in Random Matrix Theory,2012, American Mathematical Soc
 Lecture 6,2012,5â€”RmsProp: Divide the gradient by a running average of itsrecent magnitude
 Introduction to the non-asymptotic analysis of random matrices,2012, Compressedsensing
 Residual learning without normalization viabetter initialization,2019, In International Conference on Learning Representations
 Neural architecture search with reinforcement learning,2016, CoRR
