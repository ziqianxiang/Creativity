title,year,conference
 Confidence estimation in deep neuralnetworks via density modelling,2017, arXiv preprint arXiv:1707
 On pixel-wise explanations for non-linear classifier decisions by layer-wiserelevance propagation,2015, PloS one
 Curriculum learning,2009, InProceedings of the 26th Annual International Conference on Machine Learning
 Model compression,2006, In Proceedingsof the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
 Tip: Typifying theinterpretability of procedures,2017, arXiv preprint arXiv:1706
 Explanations based on the missing: Towards contrastive explanations with pertinentnegatives,2018, In Advances in Neural Information Processing Systems 31
 Improving simple modelswith confidence profiles,2018, Advances of Neural Inf
 Distilling a neural network into a soft decision tree,2017, arXivpreprint arXiv:1711
 Unifying distillation andprivileged information,2016, In International Conference on Learning Representations (ICLR 2016)
 Quantifying interpretability of arbitrarymachine learning models through functional decomposition,2019, CoRR
 Methods for interpreting andunderstanding deep neural networks,2017, Digital Signal Processing
" ""why should i trust you?‚Äù explaining thepredictions of any classifier",2016, In ACM SIGKDD Intl
 Fitnets: Hints for thin deep nets,2015, arXiv preprint arXiv:1412
 Please stop explaining black box models for high stakes decisions,2018, NIPS Workshopon Critiquing and Correcting Trends in Machine Learning
 Energy and policy considerations for deeplearning in nlp,2019, CoRR
 Auditing black-box models using transparentmodel distillation with side information,2017, CoRR
