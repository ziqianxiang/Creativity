title,year,conference
 Deep speech 2:End-to-end speech recognition in english and mandarin,2015, arXiv:1512
 Searching for exotic particles in high-energy physics withdeep learning,2014, Nature Commun
 Learning to explain: Aninformation-theoretic perspective on model interpretation,2018, arXiv:1802
 XGboost: A scalable tree boosting system,2016, In KDD
 State-of-the-art speech recognitionwith sequence-to-sequence models,2018, In ICASSP
 Notes from theai frontier,2018, McKinsey Global Institute
 Very deep convolutional networksfor natural language processing,2016, arXiv:1606
 Adanet: Adaptivestructural learning of artificial neural networks,2016, arXiv:1607
 Good semi-supervised learning that requires a bad GAN,2017, arxiv:1705
 Language modeling with gatedconvolutional networks,2016, arXiv:1612
 BERT: pre-training of deepbidirectional transformers for language understanding,2018, arxiv:1810
 Convolutionalsequence to sequence learning,2017, arXiv:1705
 Extremely randomized trees,1573, Machine Learning
 Deep Learning,2016, MIT Press
 Feature selection with decision tree criterion,2005, In HIS)
 Semi-supervised learning by entropy minimization,2004, In NIPS2004
 Deep residual learning for imagerecognition,2015, arXiv:1512
 The random subspace method for constructing decision forests,1998, IEEE Trans
 Compositional attention networks for machinereasoning,2018, arXiv:1803
 Deep neural network initialization with decisiontrees,2018, IEEE Transactions on Neural Networks and Learning Systems
 Global explanations of neuralnetworks: Mapping the landscape of predictions,2019, arXiv:1902
 Lightgbm: A highly efficientgradient boosting decision tree,2017, In NIPS
 Adam: A method for stochastic optimization,2014, In ICLR
 Deep neural decision forests,2015, In ICCV
 Recurrent convolutional neural networks for textclassification,2015, In AAAI
 Consistent individualized feature attribution fortree ensembles,2018, arXiv:1802
 EDDI: efficient dynamic discovery of high-value information withpartial VAE,2018, arXiv:1809
 From softmax to sparsemax: A sparse modelof attention and multi-label classification,2016, arXiv:1602
 Xgboost: Scalable GPU acceleratedlearning,2018, arXiv:1806
 Scalable training of artificial neural networks with adaptive sparse connectivity inspired bynetwork science,2018, Nature Communications
 Exploring sparsity inrecurrent neural networks,2017, arXiv:1704
 Online bagging and boosting,2005, In 2005 IEEE International Conference on Systems
 Catboost: unbiased boosting with categorical features,2018, In NIPS
 Unsupervised Representation Learning with DeepConvolutional Generative Adversarial Networks,2015, arXiv:1511
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv:1409
 Au-toint: Automatic feature interaction learning via self-attentive neural networks,2018, arxiv:1810
 Wavenet:A generative model for raw audio,2016, arXiv:1609
 Using a random forest to inspire a neural networkand improving on it,2017, In SDM
 Learning structured sparsity indeep neural networks,2016, arXiv:1608
 INVASE: Instance-wise variable selectionusing neural networks,2019, In ICLR
 Wide residual networks,2016, arXiv:1605
 mixup: Beyond empiricalrisk minimization,2017, arXiv:1710
