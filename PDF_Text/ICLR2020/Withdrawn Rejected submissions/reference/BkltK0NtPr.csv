title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Learning polynomials with neural networks,2014, InICML
 Neural networks and principal component analysis: Learning from exampleswithout local minima,1989, Neural networks
 Spectrally-normalized margin bounds forneural networks,2017, In Advances in Neural Information Processing Systems
 Convexneural networks,2006, In NIPS
 On the global convergence of gradient descent for over-parameterizedmodels using optimal transport,2018, arXiv preprint arXiv:1805
 Identifying and attackingthe saddle point problem in high-dimensional non-convex optimization,2014, In NIPS
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Topology and geometry of half-rectified network optimization,2016, ICLR
 Globally optimal training of generalized polynomial neuralnetworks with nonlinear spectral methods,2016, In NIPS
 Learning one-hidden-layer neural networks with landscape design,2018, ICLR
 The jamming transition as a paradigm to understand the loss landscape ofdeep neural networks,2018, arXiv preprint arXiv:1809
 Learning depth-three neural networks in polynomial time,2017, arXiv preprintarXiv:1709
 Identity matters in deep learning,2017, ICLR
 Beating the perils of non-convexity: Guaranteedtraining of neural networks using tensor methods,2015, arXiv preprint arXiv:1506
 A theory on the absence ofspurious solutions for nonconvex and nonsmooth optimization,2018, NIPS
 Approximation by entire functions,1955, Michigan Math
 Deep learning without poor local minima,2016, In NIPS
 Convergence analysis of two-layer neural networks with relu activation,2017, In NIPS
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 Understanding the loss surface of neural networks for binaryclassification,2018, 2018a
 Adding one neuron can eliminate all bad localminima,2018, NIPS
 On the computational efficiency of training neuralnetworks,2014, In Advances in Neural Information Processing Systems
 Easing non-convex optimization with neural networks,2018, 2018
 A mean field view of the landscape oftwo-layers neural networks,2018, arXiv preprint arXiv:1804
 The zero set of a real analytic function,2015, arXiv preprint arXiv:1512
 Exploring general-ization in deep learning,2017, In Advances in Neural Information Processing Systems
 The loss surface of deep and wide neural networks,2017, arXiv preprintarXiv:1704
 On the loss landscape of a classof deep neural networks with no bad local valleys,2018, arXiv preprint arXiv:1809
 Theoryof deep learning iii: the non-overfitting puzzle,2018, Technical report
 Searching for activation functions,2018, 2018
 Provable methods for training neural networks with sparse connectiv-ity,2014, arXiv preprint arXiv:1412
 Mean field analysis of neural networks,2018, arXivpreprint arXiv:1805
 Mean field analysis of neural networks: A centrallimit theorem,2018, arXiv preprint arXiv:1808
 Learning relus via gradient descent,2017, In NIPS
 No bad local minima: Data independent training error guarantees formultilayer neural networks,2016, arXiv preprint arXiv:1605
 Exponentially vanishing sub-optimal local minima in multilayer neuralnetworks,2017, arXiv preprint arXiv:1702
 Spurious valleys in two-layer neural networkoptimization landscapes,2018, arXiv preprint arXiv:1802
 Understanding deep learning requiresrethinking generalization,2017, ICLR
 Recovery guarantees for one-hidden-layerneural networks,2017, ICLR
 Stochastic gradient descent optimizesover-parameterized deep ReLU networks,2018, arXiv preprint arXiv:1811
