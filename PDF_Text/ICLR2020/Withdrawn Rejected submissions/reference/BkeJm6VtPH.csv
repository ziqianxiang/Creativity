title,year,conference
 Superposition ofmany models into one,2019, Feb 2019
 The State of Sparsity in Deep Neural Networks,2019, Feb 2019
 Network Trimming: A Data-DrivenNeuron Pruning Approach towards Efficient Deep Architectures,2016, Jul 2016
 Over-coming catastrophic forgetting in neural networks,2016, December 2016
 MNIST handwritten digit database,2010, 2010
 Optimal brain damage,1989, In NIPS
 Lifelong learning with dynamicallyexpandable networks,2017, CoRR
 ThiNet: A Filter Level Pruning Method for Deep NeuralNetwork Compression,2017, Jul 2017
 Towardsunderstanding the role of over-parametrization in generalization of neural networks,2018, CoRR
 Progressive Neural Networks,2016, June 2016
 Convolutional Neural Fabrics,2016, June 2016
 Outrageously LargeNeural Networks: The Sparsely-Gated Mixture-of-Experts Layer,2017, January 2017
 Dually optimal neuronal layers: Lobe component analysis,2009, IEEE TransactionsOnAutonomousMentalDevelopment
 Cortex-inspired goal-directed recurrent networks for develop-mental visual attention and recognition with complex backgrounds,2010, 2010
 Continual learning with intelligent synapses,2017, Proceedings ofInternational Conference on Machine Learning (ICML)
