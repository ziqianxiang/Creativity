title,year,conference
 h-detach: Modifying the lstm gradient towards better optimization,2018, arXiv preprintarXiv:1810
 Learning long-term dependencies with gradientdescent is difficult,1994, IEEE transactions on neural networks
 AntisymmetricRNN: A dynamical systemview on recurrent neural networks,2019, In International Conference on Learning Representations
 Dynamical isometry and a mean fieldtheory of rnns: Gating enables signal propagation in recurrent neural networks,2018, arXiv preprintarXiv:1806
 Learning phrase representations using rnn encoder-decoder forstatistical machine translation,2014, In Proceedings of the 2014 Conference on Empirical Methods inNatural Language Processing (EMNLP)
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Implicit renewal theory and tails of solutions of random equations,1991, The Annals ofApplied Probability
 Perpetuities with thin tails,1996, Advances in Applied Probability
 Almost surely asymptotic freeness for jacobian spectrum of deep network,2019, arXivpreprint arXiv:1908
 Long short-term memory,1997, Neural computation
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, arXiv preprint arXiv:1806
 Kronecker recurrent units,2017, arXiv preprintarXiv:1705
 On the difficulty of training recurrent neuralnetworks,2013, In International Conference on Machine Learning
 Resurrecting the sigmoid in deeplearning through dynamical isometry: theory and practice,2017, In Advances in neural informationprocessing systems
 The emergence of spectral universalityin deep networks,2018, arXiv preprint arXiv:1802
 Exponentialexpressivity in deep neural networks through transient chaos,2016, In Advances in neural informationprocessing systems
 Exact solutions to the nonlinear dynamicsof learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Deep informationpropagation,2016, arXiv preprint arXiv:1611
 Learning longer-term dependencies inrnns with auxiliary losses,2018, arXiv preprint arXiv:1803
 On orthogonality and learningrecurrent networks with long term dependencies,2017, In International Conference on Machine Learning
 Mean field residual networks: On the edge of chaos,2017, In Advancesin neural information processing systems
 Deep Mean Field Theory: Layerwise Variance and WidthVariation as Methods to Control Gradient Explosion,0000, ICLR Workshop
