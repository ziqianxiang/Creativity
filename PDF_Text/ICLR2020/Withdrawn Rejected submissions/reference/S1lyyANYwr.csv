title,year,conference
 Constrained policy optimization,2017, arXivpreprint arXiv:1705
 Safe model-basedreinforcement learning with stability guarantees,2017, In Advances in Neural Information ProcessingSystems
 Openai gym,2016, arXiv preprint arXiv:1606
 Risk-constrainedreinforcement learning with percentile risk criteria,2015, arXiv preprint arXiv:1512
 A lyapunov-based approach to safe reinforcement learning,2018, arXiv preprint arXiv:1805
 Lyapunov-based safe policy optimization for continuous control,2019, arXiv preprintarXiv:1901
 Safe exploration in continuous action spaces,2018, arXiv preprint arXiv:1801
 Benchmarking deepreinforcement learning for continuous control,2016, In International Conference on Machine Learning
 Leave no trace: Learning toreset for safe and autonomous reinforcement learning,2017, arXiv preprint arXiv:1711
 Nonlinear systems,1996, 1996
 Learning-based model pre-dictive control for safe exploration and reinforcement learning,2018, arXiv preprint arXiv:1803
 Actor-critic algorithms,2000, In Advances in neural informationprocessing systems
 Ai safety gridworlds,2017, arXiv preprint arXiv:1711
 Application of level set methods to control and reachability problems incontinuous and hybrid systems,2003, 2003
 Asynchronous methods for deep reinforcementlearning,2016, In International conference on machine learning
 Safe exploration in markov decision processes,2012, arXivpreprint arXiv:1205
 Derivatives oflogarithmic stationary distributions for policy gradient reinforcement learning,2010, Neural Computation
 A unified view of entropy-regularized markovdecision processes,2017, arXiv preprint arXiv:1705
 Trust regionpolicy optimization,2015, In International Conference on Machine Learning
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 Reinforcement learning: An introduction,2018, MIT press
 Policy evaluation with variance related risk criteriain markov decision processes,2013, arXiv preprint arXiv:1301
 Reward constrained policy optimization,2018, arXivpreprint arXiv:1805
 Safe exploration and optimization ofconstrained mdps using gaussian processes,2018, In AAAI Conference on Artificial Intelligence (AAAI)
 Scalable trust-regionmethod for deep reinforcement learning using kronecker-factored approximation,2017, In Advances inneural information processing systems
001 was used for all the experiments and the baselines,1000, Thetrajectory length for different environments
