title,year,conference
 Visualizing and measuring the geometry of BERT,2019, arXiv preprint arXiv:1906
 BERT: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andStatistics
 A structural probe for finding syntax in word representa-tions,2019, In Proceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies
 Generalization without systematicity: On the compositionalskills of sequence-to-sequence recurrent networks,2017, arXiv preprint arXiv:1711
 Open sesame: Getting inside BERTâ€™s linguisticknowledge,2019, arXiv preprint arXiv:1906
 Analysing mathematical rea-soning abilities of neural models,2019, In International Conference on Learning Representations
 Learning to reason with third order tensor products,2018, InAdvances in Neural Information Processing Systems (NeurIPS)
 Tensor product variable binding and the representation of symbolic structures inconnectionist systems,0004, Artif
 Learning distributed representations of symbolicstructure using binding and unbinding operations,2018, arXiv preprint arXiv:1810
 BERT rediscovers the classical NLP pipeline,2019, arXivpreprint arXiv:1905
 A perspective on objects and System-atic generalization in model-based rl,2019, arXiv preprint arXiv:1906
 Attention is all you need,2017, In Advances in neural informationprocessing Systems
 The correlation theory of brain function,1994, In E
