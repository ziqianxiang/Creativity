title,year,conference
 Exact solutions to the nonlinear dynamicsof learning in deep linear neural network,2013, In ICLR
 Beyond convexity: Stochastic quasi-convex oPtimiza-tion,2015, In NIPS
 Identity maPPings in deeP residualnetworks,2016, arXiv:11603
 Bag of tricks forimage classification with convolutional neural networks,2018, arXiv:1812
 Qualitatively characterizing neural networkoPtimization Problems,2015, In ICLR
 ImProving generalization Performance by switching fromadam to SGD,2017, arXiv:1712
 Adam: A method for stochastic oPtimization,2015, In ICLR
 OPenseq2seq: extensible toolkit for distributed and mixed Precision training of sequence-to-sequence models,2018, arXiv:1805
 Sgdr: Stochastic gradient descent with warm restarts,2016, ICLR
 AdaPtive gradient methods with dynamicbound of learning rate,2019, In ICLR
 Pointer sentinel mixturemodels,2016, arXiv:1609
 MixedPrecision training,2017, ICLR
 Minimization methods for nonsmooth convex and quasiconvex functions,1984, Matekon
 Librispeech: an asr corpusbased on public domain audio books,2015, In ICASSP
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 On the convergence of adam and beyond,2018, InICLR
 Neural machine translation of rare words withsubword units,2015, arXiv:1508
 Layer-specificadaptive learning rates for deep networks,2015, In ICML
 On the importance of initializa-tion and momentum in deep learning,2013, In ICML
 Attention is all you need,2017, arXiv: 1706
 The marginalvalue of adaptive gradient methods in machine learning,2017, In NIPS
 100-epoch imagenet training withalexnet in 24 minutes,2018, arXiv:1709
 Block-normalized gradientmethod: An empirical study for training deep neural network,2018, arXiv:1707
