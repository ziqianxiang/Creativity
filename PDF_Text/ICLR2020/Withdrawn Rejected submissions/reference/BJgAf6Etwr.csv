title,year,conference
 A robust self-learning method for fully unsupervisedcross-lingual mappings of word embeddings,2018, arXiv preprint arXiv:1805
 Multilingual extractivereading comprehension by runtime machine translation,2018, arXiv preprint arXiv:1809
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Enriching word vectors withsubword information,2017, Transactions ofthe Association for Computational Linguistics
 Xnli: Evaluating cross-lingual sentence representations,2018, arXivpreprint arXiv:1809
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Improving vector space word representations using multilingualcorrelation,2014, In Proceedings of the 14th Conference of the European Chapter of the Association forComputational Linguistics
 Bilbowa: Fast bilingual distributedrepresentations without word alignments,2014, In ICML
 Googleâ€™s multilingual neuralmachine translation system: Enabling zero-shot translation,2017, Transactions of the Association forComputational Linguistics
 Inducing crosslingual distributed representa-tions of words,2012, Proceedings of COLING 2012
 Cross-lingual language model pretraining,2019, arXiv preprintarXiv:1901
 Bilingual word representations withmonolingual quality in mind,2015, In Proceedings of the 1st Workshop on Vector Space Modeling forNatural Language Processing
 Learned in translation:Contextualized word vectors,2017, In Advances in Neural Information Processing Systems
 Exploiting similarities among languages for machinetranslation,2013, arXiv preprint arXiv:1309
 Distributed representationsof words and phrases and their compositionality,2013, In Advances in neural information processingsystems
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Edinburgh neural machine translation systemsfor wmt 16,2016, arXiv preprint arXiv:1606
 Attention is all you need,2017, In Advances in Neural InformationProcessing Systems
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, arXiv preprintarXiv:1804
 A broad-coverage challenge corpus forsentence understanding through inference,2017, arXiv preprint arXiv:1704
 Qanet: Combining local convolution with global self-attention for readingcomprehension,2018, arXiv preprint arXiv:1804
 Bilingual word embeddings forphrase-based machine translation,2013, In Proceedings of the 2013 Conference on Empirical Methodsin Natural Language Processing
