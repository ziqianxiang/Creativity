title,year,conference
 A convergence theory for deep learning via over-parameterization,2018, arXiv preprint arXiv:1811
 Implicit regularization in deep matrixfactorization,2019, arXiv preprint arXiv:1905
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 Gradient descent finds globalminima of deep neural networks,2018, arXiv preprint arXiv:1811
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations
 Escaping from saddle pointsâ€”online stochasticgradient for tensor decomposition,2015, In Proceedings of The 28th Conference on Learning Theory
 Exponentiated gradient meets gradient descent,2019, arXivpreprint arXiv:1902
 Characterizing implicit bias interms of optimization geometry,2018, arXiv preprint arXiv:1802
 Implicit bias of gradient descenton linear convolutional networks,2018, arXiv preprint arXiv:1806
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, arXiv preprint arXiv:1806
 Gradient descent aligns the layers of deep linear networks,2018, arXivpreprint arXiv:1810
 Wide neural networks of any depth evolve as linear models under gradientdescent,2019, arXiv preprint arXiv:1902
 Mean-field theory of two-layers neuralnetworks: dimension-free bounds and kernel limit,2019, arXiv preprint arXiv:1902
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 Guaranteed minimum-rank solutions of linearmatrix equations via nuclear norm minimization,2010, SIAM review
 Implicit regularization via hadamard product over-parametrization in high-dimensional linear regression,2019, 03 2019
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2018, arXiv preprint arXiv:1811
