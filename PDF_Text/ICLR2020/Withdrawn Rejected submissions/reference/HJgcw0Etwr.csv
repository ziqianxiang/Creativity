title,year,conference
 High-dimensional dynamics of generalization error in neuralnetworks,2017, arXiv preprint arXiv:1710
  A convergence theory for deep learning via over-parameterization,2019,  In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
   A convergence analysis of gradientdescent for deep linear neural networks,2019, In ICLR
 Thecommittee machine: Computational to statistical gaps in learning a two-layers neural network,2018, InAdvances in Neural Information Processing Systems
  On exponential convergence of sgd in non-convexover-parametrized learning,2018, arXiv preprint arXiv:1811
     On  the  global  convergence  of  gradient  descent  for  over-parameterized models using optimal transport,2018,   In Advances in neural information processingsystems
  Bert:  Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
  Gradient descentlearns one-hidden-layer cnn: Donâ€™t be afraid of spurious local minima,2018, ICML
  Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
   Online learning in radial basis function networks,1997,   NeuralComputation
 Learning one-hidden-layer neural networks with landscapedesign,2017, arXiv preprint arXiv:1711
  Optimal brain surgeon and general networkpruning,1993, In IEEE international conference on neural networks
  Multilayer feedforward networks are uni-versal approximators,1989, Neural networks
   Network trimming:  A data-drivenneuron pruning approach towards efficient deep architectures,2016,  arXiv preprint arXiv:1607
  Neural tangent kernel: Convergence and gen-eralization in neural networks,2018,  In Advances in neural information processing systems
  Deep learning without poor local minima,2016,  In Advances in neural informationprocessing systems
 An analytic theory of generalization dynamics and transferlearning in deep linear networks,2019, ICLR
  Deep linear networks with arbitrary loss: All local minima areglobal,2018, In International Conference on Machine Learning
  The multilinear structure of relu networks,2017,  arXiv preprintarXiv:1712
   Optimal brain damage,1990,   In Advances in neuralinformation processing systems
 Measuring the intrinsic dimensionof objective landscapes,2018, ICLR
  Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018,  In Advances in Neural Information Processing Systems
  Mass:  an accelerated stochastic method for over-parametrizedlearning,2018, arXiv preprint arXiv:1810
   On the computational efficiency of trainingneural networks,2014, In Advances in neural information processing systems
  Statistical mechanical analysis of the dynamics of learning in per-ceptrons,1998, Statistics and Computing
 A mean field view of the landscape of two-layer neural networks,2018, Proceedings of the National Academy of Sciences
  Dynamics of on-line gradient descent learning for multilayer neuralnetworks,1996, In Advances in neural information processing systems
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
  Masteringthe game of go with deep neural networks and tree search,2016, nature
 Luck matters: Understanding trainingdynamics of deep relu networks,2019, arXiv preprint arXiv:1905
   Global  optimality  conditions  for  deep  neural  net-works,2018,    In  International  Conference  on  Learning  Representations
  Small nonlinearities in activation functions create badlocal minima in neural networks,2019, In International Conference on Learning Representations
  Understandingdeep learning requires rethinking generalization,2017, ICLR
