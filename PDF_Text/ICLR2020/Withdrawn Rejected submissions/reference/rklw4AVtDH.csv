title,year,conference
 Faster rates for convex-concavegames,2018, COLT
 Riemannian adaptive optimization methods,2019, ICLR
 Extrapolation methods: theory and practice,2013, Elsevier
 Closing the generalization gap of adaptive gradient methods intraining deep neural networks,2018, arXiv:1806
 On the convergence of a class of adam-typealgorithms for non-convex optimization,2019, ICLR
 Universalstagewise learning for non-convex problems with convergence on averaged solutions,2019, ICLR
 Online optimization with gradual variations,2012, COLT
 Training gans withoptimism,2018, ICLR
 Incorporating nesterov momentum into adam,2016, ICLR (Workshop Track)
 Direct nonlinearacceleration,2019, arXiv:1905
 Generative adversarial nets,2014, NIPS
 Speech recognition with deep recurrentneural networks,2013, ICASSP
 Shampoo: Preconditioned stochastic tensor optimiza-tion,2018, ICML
 Introduction to online convex optimization,2016, Foundations and Trends in Optimization
 Deep residual learning for imagerecognition,2016, CVPR
 Improving generalization performance by switching fromadam to sgd,2017, arXiv:1712
 Adam: A method for stochastic optimization,2015, ICLR
 An empiricalevaluation of deep architectures on problems with many factors of variation,2007, ICML
 End-to-end training of deepvisuomotor policies,2017, NIPS
 Several tunable GMM kernels,2018, arXiv:1805
 On the convergence of stochastic gradient descent with adaptivestepsizes,2019, AISTAT
 On the variance of the adaptive learning rate and beyond,2019, arXiv:1908
 Adaptive gradient methods with dynamicbound of learning rate,2019, ICLR
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, ICML
 Adaptive bound optimization for online convexoptimization,2010, COLT
 Playing atari with deep reinforcement learning,2013, NIPS (DeepLearning Workshop)
 Accelerating optimization via adaptive prediction,2016, AISTATS
 Some methods of speeding up the convergence of iteration methods,1964, Mathematics andMathematical Physics
 Online learning with predictable sequence,2013, COLT
 On the convergence of adam and beyond,2018, ICLR
 Escaping saddle pointswith adaptive gradient methods,2019, ICML
 Fast convergence ofregularized learning in games,2015, NIPS
 Rmsprop: Divide the gradient by a running average of its recentmagnitude,2012, COURSERA: Neural Networks for Machine Learning
 On accelerated proximal gradient methods for convex-concave optimization,2008, 2008
 The marginalvalue of adaptive gradient methods in machine learning,2017, NIPS
 Adaptive methodsfor nonconvex optimization,2018, NeurIPS
 Adadelta: An adaptive learning rate method,2012, arXiv:1212
 On the convergence of adaptivegradient methods for nonconvex optimization,2018, arXiv:1808
 Adashift:Decorrelation and convergence of adaptive learning rate methods,2019, ICLR
 On the convergence of adagrad with momentum for training deep neuralnetworks,2018, arXiv:1808
