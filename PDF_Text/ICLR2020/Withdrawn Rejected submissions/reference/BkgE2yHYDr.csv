title,year,conference
 Better mixing via deep rep-resentations,2013, In Sanjoy Dasgupta and David McAllester (eds
 Neuronal Noise,2012, Springer
 Dataset augmentation in feature space,2017, arXiv:1702
 Generative adversarial nets,2014, In Z
 Explaining and harnessing adversarialexamples,2015, In International Conference on Learning Representations
 Explaining and harnessing adversarialexamples,2014, arXiv:1412
 Semi-supervised learning by entropy minimization,2005, InL
 Adam: A method for stochastic optimization,2014, arXiv:1412
 Auto-encoding variational bayes,2013, arXiv:1312
 Smooth neighbors on teacher graphs forsemi-supervised learning,2018, In The IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Virtual adversarial training: A regularization methodfor supervised and semi-supervised learning,0162, IEEE Transactions on Pattern Analysis and MachineIntelligence
 Readingdigits in natural images with unsupervised feature learning,2011, In NIPS Workshop on Deep Learningand Unsupervised Feature Learning 2011
 Semi-supervised learning with ladder networks,2015, In C
 Weight normalization: A simple reparameterization to acceleratetraining of deep neural networks,2016, In D
 Mean teachers are better role models: Weight-averaged con-sistency targets improve semi-supervised deep learning results,2017, In I
 The benefits of noise in neural systems: bridging theoryand experiment,2011, Nature Reviews Neuroscience
 Bayesian inference with probabilisticpopulation codes,2006, Nature Reviews Neuroscience
 Semi-supervised learning literature survey,2005, Technical Report 1530
 Learning from labeled and unlabeled data with label propa-gation,2002, Technical report
