title,year,conference
 An analysis of single layer networks in unsupervisedfeature learning,2011, AISTATS
 Towards principled methods for training generative adversarialnetworks,2017, In NIPS 2016 Workshop on Adversarial Training
 A formal proof in coq of lasalleâ€™s invariance principle,2017, InInternational Conference on Interactive Theorem Proving
 Training gans withoptimism,2017, arXiv preprint arXiv:1711
 Understanding gans: the lqg setting,2017, arXivpreprint arXiv:1710
 Im-proved training of wasserstein gans,2017, arXiv preprint arXiv:1704
 Nonlinear dynamical systems and control: aLyapunov-based approach,2011, Princeton university press
 Minmax optimization: Stable limit points ofgradient descent ascent are locally optimal,2019, arXiv preprint arXiv:1902
 Learning multiple layers of features from tiny images,2009, 2009
 A tutorial on energy-basedlearning,2006, Predicting structured data
 Towards understanding the dy-namics of generative adversarial networks,2017, arXiv preprint arXiv:1706
 On the convergence properties of gan training,2018, arXiv preprint arXiv:1801
 Unrolled generative adversarialnetworks,2016, arXiv preprint arXiv:1611
 Spectral normalizationfor generative adversarial networks,2018, arXiv preprint arXiv:1802
 Deep boltzmann machines,2009, In Artificial intelligenceand statistics
 On the convergence and ro-bustness of training gans with regularized optimal transport,2018, In Advances in Neural InformationProcessing Systems
 The unusual effectiveness of averaging in gan training,2018, arXiv preprintarXiv:1806
 LsUn:ConstrUction of a large-scale image dataset Using deep learning with hUmans in the loop,2015, arXivpreprint arXiv:1506
 Suppose the true distribution is pdata and thegenerated distribution is pg ,2020, Further
 A more detailed proof is given as folloWs,2020, Pick an arbitrary k
