title,year,conference
 Adaptive importance sampling to accelerate training ofa neural probabilistic language model,1045, Transactions on Neural Networks
 Learning k-way d-dimensional discrete codesfor compact embedding representations,2018, In Proceedings of the 35th International Conference onMachine Learning
 Short and deep: Sketching andneural networks,2017, In ICLR Workshop
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Classes for fast maximum entropy training,2001, In IEEE International Conference onAcoustics
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Hierarchical probabilistic neural network language model,2005, InRobert G
 Neural machine translation of rare words withsubword units,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics
 Compressing word embeddings via deep compositional codelearning,2018, In ICLR
 Matching theblanks: Distributional similarity for relation learning,2019, arXiv preprint arXiv:1906
 Bert4rec: Sequen-tial recommendation with bidirectional encoder representations from transformer,2019, arXiv preprintarXiv:1904
 Hash embeddings for efficient word representations,2017, InAdvances in Neural Information Processing Systems
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Featurehashing for large scale multitask learning,2009, In ICML
 Dynamic intention-aware recommendation withself-attention,2018, arXiv preprint arXiv:1808
 Ernie: Enhancedlanguage representation with informative entities,2019, arXiv preprint arXiv:1905
