title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, CoRR
 Training with noise is equivalent to tikhonov regularization,1995, Neural computation
 Keeping the bad guys out: Protecting and vaccinating deep learning withjpeg compression,2017, arXiv preprint arXiv:1705
 Adversarial feature learning,2016, arXiv preprintarXiv:1605
 A study of the effect of jpgcompression on adversarial images,2016, arXiv preprint arXiv:1608
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Towards deep neural network architectures robust to adversarialexamples,2014, arXiv preprint arXiv:1412
 Adversarial machine learning at scale,2016, arXivpreprint arXiv:1611
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Magnet: a two-pronged defense against adversarial examples,2017, CoRR
 Distillationas a defense to adversarial perturbations against deep neural networks,2015, CoRR
 Practical black-box attacks against machine learning,2017, In Proceedings of the 2017 ACMon Asia Conference on Computer and Communications Security
 Technical report on the cleverhans v2,2018,1
 Nag: Network for ad-versary generation,2018, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Pixeldefend:Leveraging generative models to understand and defend against adversarial examples,2017, arXivpreprint arXiv:1710
 Intriguing properties of neural networks,2013, CoRR
