title,year,conference
 Katyusha X: Practical momentum method for stochastic sum-of-nonconvexoptimization,2018, In ICML
 Communication complexity of distributed convex learning andoptimization,2015, In NIPS
 A simple practical accelerated method for finite sums,2016, In NIPS
 SAGA: A fast incremental gradientmethod with support for non-strongly convex composite objectives,2014, In NIPS
 Spider: Near-optimal non-convexoptimization via stochastic path-integrated differential estimator,2018, In NIPS
 Breaking the span assumption yieldsfast finite-sum minimization,2018, In Advances in Neural Information Processing Systems
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In NIPS
 An optimal randomized incremental gradient method,2017, Mathematicalprogramming
 Minimizing finite sums with the stochasticaverage gradient,2017, Mathematical Programming
 Tight complexity bounds for optimizing composite objec-tives,2016, In NIPS
 Linear convergence with condition number inde-pendent access of full gradients,2013, In NIPS
