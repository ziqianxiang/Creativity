title,year,conference
 Obfuscated gradients give a false sense of security:Circumventing defenses to adversarial examples,2018, In ICML
 Measuringneural net robustness with constraints,2016, In NeurIPS
 Decision-based adversarial attacks: Reliable attacksagainst black-box machine learning models,2018, In ICLR
 Adversarial patch,2017, In NeurIPS2017 Workshop on Machine Learning and Computer Security
 Towards evaluating the robustness of neural networks,2017, In IEEESymposium on Security and Privacy
 Adversarial examples are not easily detected: Bypassing tendetection methods,2017, In ACM Workshop on Artificial Intel ligence and Security
 Ead: Elastic-net attacks to deep neuralnetworks via adversarial examples,2018, In AAAI
 Scaling up the randomized gradient-free adversarial attackreveals overestimation of robustness using established attacks,2019, preprint
 A rotation and a translationsuffice: Fooling CNNs with simple transformations,2017, In NeurIPS 2017 Workshop on MachineLearning and Computer Security
 Towards deep neural network architectures robust to adversarialexamples,2015, In ICLR Workshop
 Deep residual learning for image recognition,2016, InCVPR
 Formal guarantees on the robustness of a classifier againstadversarial manipulation,2017, In NeurIPS
 Learning with a strong adversary,2016, InICLR
 Reluplex: An efficient smtsolver for verifying deep neural networks,2017, In CAV
 Adversarial examples in the physical world,2017, InICLR Workshop
 Towards deep learning modelsresistant to adversarial attacks,2018, In ICLR
 Sparsefool: a few pixels make a bigdifference,2019, In CVPR
 Deepfool: a simple and accurate methodto fool deep neural networks,2016, In CVPR
 Logit pairing methodscan fool gradient-based attacks,2018, In NeurIPS 2018 Workshop on Security in MachineLearning
 Simple black-box adversarial perturbations fordeep networks,2016, In CVPR 2017 Workshops
 Foolbox: A python toolbox to benchmark therobustness of machine learning models,2017, In ICML Reliable Machine Learning in the WildWorkshop
 Evaluating robustness of neural networks with mixedinteger programming,2019, In ICLR
 Robustness may be atodds with accuracy,2019, In ICLR
 Improving the robustness of deep neuralnetworks via stability training,2016, In CVPR
 Distributionally adversarial attack,2019, In AAAI
