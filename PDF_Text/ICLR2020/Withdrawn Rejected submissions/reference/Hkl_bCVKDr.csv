title,year,conference
 Obfuscated gradients give a false sense of security:Circumventing defenses to adversarial examples,2018, In Jennifer Dy and Andreas Krause (eds
 Decision-based adversarial attacks: Reliable attacksagainst black-box machine learning models,2018, In 6th International Conference on Learning Representations
 Adversarial examples are not easily detected: Bypassing ten detectionmethods,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security
 Towards evaluating the robustness of neural networks,2017, In 2017 IEEESymposium on Security and Privacy
 On evaluating adversarial robustness,2019, CoRR
 Imagenet: A large-scale hierarchical imagedatabase,2009, In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR2009)
 The LogBarrier adversarial attack: makingeffective use of decision boundary information,2019, CoRR
 Making machine learning robust against adver-Sarial inputs,2018, Communications of the ACM
 Formal Guarantees on the Robustness of a Classifier against Adver-sarial Manipulation,2017, In Advances in Neural Information Processing Systems 30: Annual Conference on NeuralInformation Processing Systems 2017
 Adversarial logit pairing,2018, CoRR
 Learning multiple layers of features from tiny images,2009, Technical report
 Adversarial examples in the physical world,2016, CoRR
 Second-order adversarial attack and certifiablerobustness,2018, CoRR
 A unified gradient regularization family for adversarialexamples,2015, In 2015 IEEE International Conference on Data Mining
 Towards deeplearning models resistant to adversarial attacks,2017, CoRR
 Virtual adversarial training: a regularizationmethod for supervised and semi-supervised learning,2018, IEEE transactions on pattern analysis and machineintelligence
 Gradient descent GAN optimization is locally stable,2017, In Advances inNeural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems2017
 Sensitivityand generalization in neural networks: an empirical study,2018, In 6th International Conference on LearningRepresentations
 Unifying adversarial training algorithms with datagradient regularization,2017, Neural Computation
 Perspectives on research in artificial intelligence and artificial general intelligence relevant toDoD,2017, Technical report
 Certified defenses against adversarial examples,2018, In6th International Conference on Learning Representations
 Semidefinite relaxations for certifying robustness to ad-versarial examples,2018, In Advances in Neural Information Processing Systems 31: Annual Conference on NeuralInformation Processing Systems 2018
 Contractive auto-encoders:Explicit invariance during feature extraction,2011, In Proceedings of the 28th International Conference on MachineLearning
 Stabilizing training of generativeadversarial networks through regularization,2017, In Advances in Neural Information Processing Systems 30:Annual Conference on Neural Information Processing Systems 2017
 Adversarially robust trainingthrough structured gradient regularization,2018, CoRR
 L1-norm double backpropagation adversarial defense,2019, arXivpreprint arXiv:1903
 Adversarialvulnerability of neural networks increases with input dimension,2018, CoRR
 Intriguing properties of neural networks,2013, CoRR
 Ensembleadversarial training: Attacks and defenses,2018, In International Conference on Learning Representations
 Robustness maybe at odds with accuracy,2018, CoRR
 Evaluating the robustness of neural networks: An extreme value theory approach,2018, In 6th InternationalConference on Learning Representations
 Provable defenses against adversarial examples via the convex outer ad-versarial polytope,2018, In Proceedings of the 35th International Conference on Machine Learning
 Training for faster adversarialrobustness verification via inducing relu stability,2018, CoRR
 Feature denoising for improv-ing adversarial robustness,2018, CoRR
 You only propagate once:Accelerating adversarial training via maximal principle,2019, CoRR
