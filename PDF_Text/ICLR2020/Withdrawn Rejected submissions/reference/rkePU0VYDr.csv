title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, In ICML
 Evasion attacks against machine learning at test time,2013, In Joint Europeanconference on machine learning and knowledge discovery in databases
 Thermometer encoding: One hot wayto resist adversarial examples,2018, In International Conference on Learning Representations
 Towards evaluating the robustness of neural networks,2017, In 2017 IEEESymposium on Security and Privacy (SP)
 Adversarial examples are not easily detected: Bypassing tendetection methods,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence andSecurity
 Certified adversarial robustness via randomizedsmoothing,2019, arXiv preprint arXiv:1902
 Keeping the bad guys out: Protecting and vaccinating deep learning withJPEG compression,2017, CoRR
 A study of the effect of jpgcompression on adversarial images,2016, arXiv preprint arXiv:1608
 Detecting adversarialsamples from artifacts,2017, arXiv preprint arXiv:1703
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Ppd: Permuta-tion phase defense against adversarial examples in deep learning,2018, arXiv preprint arXiv:1812
 Towards robust neural networksvia random self-ensemble,2018, In Vittorio Ferrari
 Feature distillation: Dnn-oriented jpeg compression against adversarial examples,2019, In The IEEE Conference on ComputerVision and Pattern Recognition (CVPR)
 Distillation as adefense to adversarial perturbations against deep neural networks,2015, arXiv preprint arXiv:1511
 Practical black-box attacks against machine learning,2017, In Proceedings of the 2017 ACM onAsia conference on computer and communications security
 Deflectingadversarial attacks with pixel deflection,2018, In The IEEE Conference on Computer Vision and PatternRecognition (CVPR)
 Foolbox: A python toolbox to benchmark therobustness of machine learning models,2017, arXiv preprint arXiv:1707
 The odds are odd: A statistical test for detectingadversarial examples,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Provably robust deep learning via adversarially trained smoothed classifiers,2019, CoRR
 Intriguing properties of neural networks,2014, CoRR
 Exploring the space of adversarial images,2015, CoRR
 Mitigating adversarial effectsthrough randomization,2017, arXiv preprint arXiv:1711
 Feature squeezing: Detecting adversarial examples in deepneural networks,2017, arXiv preprint arXiv:1704
 Hessian-based analysisof large batch training and robustness to adversaries,2018, In S
 Defending against whitebox adversarial attacks via randomizeddiscretization,2019, AISTATS
