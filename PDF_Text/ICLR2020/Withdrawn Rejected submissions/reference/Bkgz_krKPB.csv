title,year,conference
 Entity commonsense representationfor neural abstractive summarization,2018, In NAACL
 Neural machine translation by jointlylearning to align and translate,2015, In ICLR
 Scheduled sampling for sequenceprediction with recurrent neural networks,2015, In NIPS
 Enriching word vectorswith subword information,2017, TACL
 A large anno-tated corpus for learning natural language inference,2015, In EMNLP
 Model compression,2006, In KDD
 Faithful to the original: Fact aware neural ab-stractive summarization,2018, In AAAI
 Improving sequence-to-sequence learning viaoptimal transport,2019, In ICLR
 Fast abstractive summarization with reinforce-selected sentencerewriting,2018, In ACL
 Learning phrase representations using rnn encoder-decoderfor statistical machine translation,2014, In EMNLP
 Semi-supervised se-quence modeling with cross-view training,2018, In EMNLP
 Xnli: Evaluating cross-lingual sentence representations,2018, InEMNLP
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In NAACL
 Classicalstructured prediction losses for sequence to sequence learning,2018, In NAACL
 Semantic compositional networks for visual captioning,2017, In CVPR
 Convolutionalsequence to sequence learning,2017, In ICML
 Constant-time machinetranslation with conditional masked language models,2019, arXiv preprint arXiv:1904
 Distilling the knowledge in a neural network,2015, InNIPS Deep Learning and Representation Learning Workshop
 Sequence-level knowledge distillation,2016, In EMNLP
 Adam: A method for stochastic optimization,2015, In ICLR
 Cross-lingual language model pretraining,2019, arXiv preprintarXiv:1901
 Unsupervisedmachine translation using monolingual corpora only,2018, In ICLR
 Global encoding for abstractive summarization,2018, InACL
 Agreement on target-bidirectionalneural machine translation,2016, In NAACL
 Stanford neural machine translation systems forspoken language domain,2015, In IWSLT
 Effective approaches to attention-basedneural machine translation,2015, In EMNLP
 Learned in translation:Contextualized word vectors,2017, In NIPS
 Distributed representa-tions of words and phrases and their compositionality,2013, In NIPS
 Abstractive text summarizationusing sequence-to-sequence rnns and beyond,2016, In CoNLL
 Bleu: a method for automaticevaluation of machine translation,2002, In ACL
 Automatic differentiation inpytorch,2017, In NIPS Autodiff Workshop
 Glove: Global vectors for wordrepresentation,2014, In EMNLP
 Deep contextualized word representations,2018, In NAACL
 Improving language under-standing by generative pre-training,2018, 2018
 A neural attention model for abstractivesentence summarization,2015, In EMNLP
 Neural machine translation of rare words withsubword units,2016, In ACL
 Twin networks: Matching the future for sequence generation,2018, In ICLR
 Mass: Masked sequence tosequence pre-training for language generation,2019, In ICML
 Patient knowledge distillation for bert modelcompression,2019, arXiv preprint arXiv:1908
 Re-thinking the inception architecture for computer vision,2016, In CVPR
 Multilingual neural machine trans-lation with knowledge distillation,2019, In ICLR
 Attention is all you need,2017, In NIPS
 Show and tell: A neural imagecaption generator,2015, In CVPR
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2019, In ICLR
 Pretraining-based natural language generation for textsummarization,2019, arXiv preprint arXiv:1902
 Regularizingneural machine translation by target-bidirectional agreement,2019, In AAAI
