title,year,conference
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, In ICML
 A closer look at memorization in deep networks,2017, In ICML
 Curriculum learning,2009, InProceedings of the 26th annual international conference on machine learning
 Understanding and utilizingdeep neural networks trained with noisy labels,2019, In ICML
 Imagenet: A large-scalehierarchical image database,2009, CVPR
 Co-teaching: Robust training of deep neural networks with extremely noisylabels,2018, In NeurIPS
 Deep residual learning for image recog-nition,2016, In CVPR
 Using trusted data to traindeep networks on labels corrupted by severe noise,2018, In NeurIPS
 Understanding generalization of deep neural networks trainedwith noisy labels,2019, In NeurIPS
 Convolutional networks withdense connectivity,2019, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Densely connectedconvolutional networks,2017, In CVPR
 Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2017, In ICML
 The mnist database of handwritten digits,2005, 2005
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In NeurIPS
 Learning fromnoisy labels with distillation,2017, ICCV
 Dimensionality-driven learning with noisy labels,2018, ICML
" Decoupling ""when to update"" from ""how to update""",2017, InNIPS
 Towardsunderstanding the role of over-parametrization in generalization of neural networks,2019, In ICLR
 Makingneural networks robust to label noise: a loss correction approach,2016, ArXiv
 Training deep neural networks on noisy labels with bootstrapping,2014, CoRR
 Learning with bad training data via iterative trimmed loss mini-mization,2019, In ICML
 Trainingconvolutional networks with noisy labels,2015, In ICLR Workshop
 Learning from massive noisylabeled data for image classification,2015, In CVPR
 Understandingdeep learning requires rethinking generalization,2017, In ICLR
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In NeurIPS
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2019, In ICLR
