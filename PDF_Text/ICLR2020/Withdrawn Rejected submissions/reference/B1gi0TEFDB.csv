title,year,conference
 Acoustical and environmental robustness in automatic speech recognition,1990, InProc
 Sparse communication for distributed gradient descent,2017, InProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing
 QSGD:Communication-efficient SGD via gradient quantization and encoding,2017, In Advances in NeuralInformation Processing Systems
 The convergence of sparsified gradient methods,2018, In Advances in Neural InformationProcessing Systems
 ImageNet: A large-scalehierarchical image database,2009, In Computer Vision and Pattern Recognition
 Communication quantizationfor data-parallel training of deep neural networks,2016, In 2016 2nd Workshop on Machine Learningin HPC Environments (MLHPC)
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Tradingredundancy for communication: Speeding up distributed SGD for non-convex optimization,2019, InInternational Conference on Machine Learning
 A linear speedup analysis of distributed deep learning with sparseand quantized communication,2018, In Advances in Neural Information Processing Systems
 Error feedbackfixes signsgd and other gradient compression schemes,2019, In International Conference on MachineLearning
 ImageNet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Deep gradient compression:Reducing the communication bandwidth for distributed training,2018, In International Conference onLearning Representations
 Building a large annotatedcorpus of English: ThePennTreebank,1993, Computational linguistics
 Mixed precisiontraining,2018, International Conference on Learning Representations
 1-bit stochastic gradient descent and itsapplication to data-parallel distributed training of speech DNNs,2014, In Fifteenth Annual Conferenceof the International Speech Communication Association
 Horovod: fast and easy distributed deep learning in Ten-sorFlow,2018, arXiv preprint arXiv:1802
 Efficient top-k query processing on massivelyparallel hardware,2018, In Proceedings of the 2018 International Conference on Management of Data
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Sparsified SGD with memory,2018, InAdvances in Neural Information Processing Systems
 Doublesqueeze: Parallel stochasticgradient descent with double-pass error-compensated compression,2019, In International Conferenceon Machine Learning
 Terngrad:Ternary gradients to reduce communication in distributed deep learning,2017, In Advances in neuralinformation processing systems
 Error compensated quantized SGDand its applications to large-scale distributed optimization,2018, International Conference on MachineLearning
 Communication-efficient distributed blockwisemomentum SGD with error-feedback,2019, In Advances in neural information processing systems
