title,year,conference
 Deepspeech 2: End-to-end speech recognition in English and Mandarin,2016, In Proceedings of the 33rdInternational Conference on Machine Learning
 Stability and generalization of learning algorithms that convergeto global optima,2018, In Proceedings of the 35th International Conference on Machine Learning
 Automated inference with adaptive batches,2017, InProceedings of the 20th International Conference on Artificial Intelligence and Statistics
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In IEEE Conference on Computer Vision and Pattern Recognition
 Adabatch: Adaptive batch sizes for training deepneural networks,2017, arXiv:1712
 Deep residual learning for image recognition,2016, In Proceedingsof the IEEE conference on computer vision and pattern recognition
 Identity mappings in deep residual networks,2016, In Europeanconference on computer vision
 Threefactors influencing minima in SGD,2018, In Proceedings of the 27th International Conference onArtificial Neural Networks
 Linear convergence of gradient and proximal-gradient meth-ods under the Polyak-IojasieWicz condition,2016, In Joint European Conference on Machine Learningand Knowledge Discovery in Databases
 Adam: A method for stochastic oPtimization,2015, In Proceedings of the 3rdInternational Conference on Learning Representations
 Numerical Solution of Stochastic Differential Equations,1992, SPringer
 Learning multiPle layers of features from tiny images,2009, Technical rePort
 One Weird trick for Parallelizing convolutional neural netWorks,2014, arXiv:1404
 Nonconvex finite-sum oPtimization via SCSG methods,2017, InAdvances in Neural Information Processing Systems 30
 The PoWer of interPolation: Understanding the effectiveness ofSGD in modern over-Parametrized learning,2018, In Proceedings of the 35th International Conferenceon Machine Learning
 An emPirical model of large-batchtraining,2018, arXiv:1812
 LibrisPeech: An ASR corPus based on Publicdomain audio books,2015, In IEEE International Conference on Acoustics
 Stochastic variance reduction for nonconvexoPtimization,2016, In Proceedings of the 33rd International Conference on Machine Learning
 YOLOv3: An incremental imProvement,2018, arXiv:1804
 No more Pesky learning rates,2013, In Proceedings of the 30thInternational Conference on Machine Learning
 Attention is all you need,2017, In Advances in Neural Information Processing Systems 31
 Gradient diver-sity: A key ingredient for scalable distributed learning,2018, In Proceedings of the 21st InternationalConference on Artificial Intelligence and Statistics
 Stagewise training accelerates convergence of testing errorover sgd,2019, In Advances in Neural Information Processing Systems 32
 Bag of freebies for training object detectionneural networks,2019, arXiv:1902
