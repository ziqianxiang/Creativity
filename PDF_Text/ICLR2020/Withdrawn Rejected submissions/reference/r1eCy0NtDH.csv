title,year,conference
 On exact computation with aninfinitely wide neural net,2019, arXiv preprint arXiv:1904
 On the inductive bias of neural tangent kernels,2019, arXiv PreprintarXiv:1905
 A note on lazy training in supervised differentiable programming,2018, arXivpreprint arXiv:1812
 Gradient descent finds global minima of deep neuralnetworks,2018, arXiv preprint arXiv:1811
 Gradient descent learns one-hidden-layer CNN:Donâ€™t be afraid of spurious local minima,2018, ICML
 Gradient descent provably optimizes over-parameterizedneural networks,2019, ICLR
 Linearized two-layers neural networks inhigh dimension,2019, arXiv preprint arXiv:1904
 On the impact of the activation function on deep neuralnetworks training,2019, ICML
 Delving deep into rectifiers: Surpassing human-level perfor-mance on imagenet classification,2015, ICCV
 On the diffusion approximation of nonconvex stochasticgradient descent,2018, arXiv preprint arXiv:1705
 Dynamics of deep neural networks and neural tangent hierarchy,2019, arXivpreprint arXiv:1909
 Neural tangent kernel: Convergence and generalization in neuralnetworks,2018, 32nd Conference on Neural Information Processing Systems
 Universal statistics of Fisher information in deep neuralnetworks: Mean field approach,2018, arXiv preprint arXiv:1806
 Implicit regularization in over-parameterized neuralnetworks,2019, arXiv preprint arXiv:1903
 Deep neuralnetworks as Gaussian processes,2018, 6th International Conference on Learning Representations
 Wide neural networksof any depth evolve as linear models under gradient descent,2019, arXiv preprint arXiv:1902
 Implicit regularization of stochastic gradient descent innatural language processing: Observations and implications,2018, arXiv preprint arXiv:1811
 Optimization landscape and expressivity of deep CNNs,2018, ICML
 Deep information propagation,2017, 5thInternational Conference on Learning Representations
 Mean field residual networks: On the edge of chaos,2017, Advances in NeuralInformation Processing Systems
 Understanding deep learning requiresrethinking generalization,2017, arXiv preprint arXiv:1611
 Stochastic gradient descent optimizes over-parameterized deepReLU networks,2018, arXiv preprint arXiv:1811
