title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, arXiv preprint arXiv:1802
 Signature Verifi-cation using a” siamese” time delay neural network,1994, In Advances in neural information processingSystems
 Adversarial examPles are not easily detected: ByPassing tendetection methods,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence andSecurity
 Magnet and” efficient defenses against adversarial attacks” arenot robust to adversarial examples,2017, arXiv preprint arXiv:1711
 Towards evaluating the robustness of neural networks,2017, In 2017IEEE Symposium on Security and Privacy (SP)
 On evaluating adversarial robustness,2019, arXivpreprint arXiv:1902
 Certified adversarial robustness via randomizedsmoothing,2019, arXiv preprint arXiv:1902
 A cluster separation measure,1979, IEEE transactions on patternanalysis and machine intelligence
 Stochastic activation pruning for robust adversarial de-fense,2018, arXiv preprint arXiv:1803
 Largemargin deep networks for classification,2018, In Advances in neural information processing systems
 Evaluating and understanding the robustnessof adversarial logit pairing,2018, arXiv preprint arXiv:1807
 Detecting adversarialsamples from artifacts,2017, arXiv preprint arXiv:1703
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Gradient-based learning appliedto document recognition,1998, Proceedings of the IEEE
 Large-margin softmax loss for convolu-tional neural networks,2016, In ICML
 Magnet: a two-pronged defense against adversarial examples,2017, InProceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security
 Towards robust detection of adversarial exam-ples,2018, In Advances in Neural Information Processing Systems
 Distillation as adefense to adversarial perturbations against deep neural networks,2016, In 2016 IEEE Symposium onSecurity and Privacy (SP)
 Practical black-box attacks against machine learning,2017, In Proceedings of the 2017 ACMon Asia conference on computer and communications security
 Technical report on the cleverhans v2,2018,1
 Defense-gan: Protecting classifiers againstadversarial attacks using generative models,2018, arXiv preprint arXiv:1805
 Pixeldefend:Leveraging generative models to understand and defend against adversarial examples,2017, arXivpreprint arXiv:1710
 Ensemble meth-ods as a defense to adversarial perturbations against deep neural networks,2017, arXiv preprintarXiv:1709
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Evaluating robustness of neural networks with mixedinteger programming,2018, 2018
 Ensemble adversarial training: Attacks and defenses,2017, arXiv preprint arXiv:1705
 Lipschitz-margin training: Scalable certifi-cation of perturbation invariance for deep neural networks,2018, In Advances in Neural InformationProcessing Systems
 A discriminative feature learning approachfor deep face recognition,2016, In European conference on computer vision
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2017, arXiv preprint arXiv:1711
 Scaling provable adversarialdefenses,2018, In Advances in Neural Information Processing Systems
 Deep defense: Training dnns with improved adver-sarial robustness,2018, In Advances in Neural Information Processing Systems
 Recurjac: An efficient recursive algorithm forbounding jacobian matrix of neural networks and its applications,2019, In Proceedings of the AAAIConference on Artificial Intelligence
 Exploiting the inherent limitation of l0 ad-versarial examples,2020, In 22nd International Symposium on Research in Attacks
