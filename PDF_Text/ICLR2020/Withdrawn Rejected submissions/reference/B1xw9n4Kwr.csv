title,year,conference
 A convergence theory for deep learning viaover-parameterization,2018, arXiv preprint arXiv:1811
 A convergence analysis of gradientdescent for deep linear neural networks,2018, arXiv preprint arXiv:1810
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, arXiv preprint arXiv:1802
 Fine-grained analysisof optimization and generalization for overparameterized two-layer neural networks,2019, arXivpreprint arXiv:1901
 On exponential convergence of sgd in non-convexover-parametrized learning,2018, arXiv preprint arXiv:1811
 Neural ordinarydifferential equations,2018, In Advances in neural information processing systems
 Identifying and attacking the saddle point problem in high-dimensional non-convex optimization,2014, In Advances in neural information processing systems
 Width provably matters in optimization for deep linear neuralnetworks,2019, arXiv preprint arXiv:1901
 Gradient descent findsglobal minima of deep neural networks,2018, arXiv preprint arXiv:1811
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Augmented neural odes,2019, arXiv preprintarXiv:1904
 Escaping from saddle points—onlinestochastic gradient for tensor decomposition,2015, In Conference on Learning Theory
 Anode: Unconditionally accurate memory-efficient gradients for neural odes,2019, arXiv preprint arXiv:1902
 Qualitatively characterizing neuralnetwork optimization problems,2014, arXiv preprint arXiv:1412
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, In Advances in neural information processing systems
 Deep learning without poor local minima,2016, In Advances in neural informationprocessing systems
 On Large-Batch Training for Deep Learning: Generalization Gap and SharpMinima,2016, arXiv preprint arXiv:1609
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Beyond finite layer neural net-works: Bridging deep architectures and numerical differential equations,2017, arXiv preprintarXiv:1710
 Introductory lectures on convex programming,1998, 1998
 On the momentum term in gradient descent learning algorithms,1999, Neural networks
 No bad local minima: Data independent training errorguarantees for multilayer neural networks,2016, arXiv preprint arXiv:1605
 A differential equation for modeling nes-terov’s accelerated gradient method: Theory and insights,2014, In Advances in Neural InformationProcessing Systems
 On the importance ofinitialization and momentum in deep learning,2013, In International conference on machine learning
 Asymptotic analysis via stochastic differential equations of gradient descentalgorithms in statistical and computational paradigms,2017, arXiv preprint arXiv:1711
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Learning transferablearchitectures for scalable image recognition,2018, In Proceedings of the IEEE conference on computervision and pattern recognition
 The underscore separated stringsrepresent hidden dimensions,2000," ""2000"" represents hidden dimension of width 2000 and ""2000_1""represents hidden layers of width 2000 and 1"
