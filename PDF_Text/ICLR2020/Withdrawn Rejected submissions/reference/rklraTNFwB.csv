title,year,conference
 Learning to understand goal specifications by modelling reward,2018, arXivpreprint arXiv:1806
 Actrce: Augmenting experiencevia teacherâ€™s advice for multi-goal reinforcement learning,2019, arXiv preprint arXiv:1902
 Shapenet: An information-rich 3dmodel repository,2015, arXiv preprint arXiv:1512
 Babyai: First steps towards grounded language learningwith a human in the loop,2018, arXiv preprint arXiv:1810
 A unified architecture for natural language processing: Deepneural networks with multitask learning,2008, In Proceedings of the 25th international conference onMachine learning
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXivpreprint arXiv:1901
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Grounded lan-guage learning in a simulated 3d world,2017, arXiv preprint arXiv:1706
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Language as an abstraction for hier-archical deep reinforcement learning,2019, arXiv preprint arXiv:1906
 Viz-doom: A doom-based ai research platform for visual reinforcement learning,2016, In 2016 IEEEConference on CompUtational Intelligence and Games (CIG)
 Adam: A method for stochastic oPtimization,2014, arXiv preprintarXiv:1412
 Inferlite: Simple universal sentence representations from naturallanguage inference data,2018, In Proceedings of the 2018 Conference on Empirical Methods in NatUralLangUage Processing
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Vilbert: Pretraining task-agnostic visiolin-guistic representations for vision-and-language tasks,2019, arXiv preprint arXiv:1908
 Learning to parse naturallanguage commands to a robot control system,2013, In Experimental Robotics
 Mapping instructions and visual observations toactions with reinforcement learning,2017, arXiv preprint arXiv:1704
 Asynchronous methods for deep reinforcementlearning,2016, In International conference on machine learning
 Combating adversarial misspellings withrobust word recognition,2019, arXiv preprint arXiv:1905
 Languagemodels are unsupervised multitask learners,2019, OpenAI Blog
 Approaching the symbol grounding problem with probabilistic graphi-cal models,2011, AI magazine
 Attention is all you need,2017, In Advances in neural informationprocessing Systems
 Learning language games through interac-tion,2016, arXiv preprint arXiv:1606
 Reinforced cross-modal matching and self-supervised imita-tion learning for vision-language navigation,2019, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 No training required: Exploring random encoders for sentenceclassification,2019, arXiv preprint arXiv:1901
 Understanding natural language,1972, Cognitive psychology
 Building generalizable agents with arealistic and rich 3d environment,2018, arXiv preprint arXiv:1801
 Interactive grounded language acquisition and general-ization in a 2d world,2018, arXiv preprint arXiv:1802
