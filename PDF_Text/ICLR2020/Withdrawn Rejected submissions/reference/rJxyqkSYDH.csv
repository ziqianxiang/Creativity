title,year,conference
 Identifying and attacking the saddle point problem in high-dimensional non-convex op-timization,2014, In Advances in neural information processing systems
 Densely connectedconvolutional networks,2017, CVPR
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Adam: A method for stochastic optimization,2015, ICLR
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Gradient-based hyperparameter optimization throughreversible learning,2015, ICML 2015
 Practical bayesian optimization of machine learning algo-rithms,2012, NIPS
 The marginal value of adaptive gradientmethods in machine learning,2017, NIPS 2017
 A walk with sgd,2018, arXiv preprintarXiv:1802
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
