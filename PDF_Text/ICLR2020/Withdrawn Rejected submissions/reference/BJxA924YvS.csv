title,year,conference
 Large scale distributed neural network training through online distillation,2018, arXiv preprintarXiv:1804
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Shufflenet v2: Practical guidelinesfor efficient cnn architecture design,2018, In Proceedings of the European Conference on ComputerVision (ECCV)
 Regularizingneural networks by penalizing confident output distributions,2017, arXiv preprint arXiv:1701
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Distilling object detectors with fine-grainedfeature imitation,2019, In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Aggregated residual trans-formations for deep neural networks,2017, In Proceedings of the IEEE conference on computer visionandpattern recognition
 Deep mutual learning,2018, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
