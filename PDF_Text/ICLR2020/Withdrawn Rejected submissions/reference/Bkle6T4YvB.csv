title,year,conference
 Massively multilingual sentence embeddings for zero-shotcross-lingual transfer and beyond,2019, Transactions of the Association for Computational Linguistics
 Learning principled bilingual mappings of wordembeddings while preserving monolingual invariance,1250, In Proceedings of the 2016 Conference onEmpirical Methods in Natural Language Processing
 Enriching word vectors withsubword information,2017, Transactions of the Association for Computational Linguistics
 XNLI: Evaluating cross-lingual sentence representations,2018, InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing
 Loss in trans-lation: Learning bilingual word mapping with a retrieval criterion,2018, In Proceedings of the 2018Conference on Empirical Methods in Natural Language Processing
 Cross-lingual language model pretraining,2019, NeurIPS
 Unsupervisedmachine translation using monolingual corpora only,2018, In International Conference on LearningRepresentations
 FUlly character-level neUral machine translationwithoUt explicit segmentation,2017, Transactions of the Association for Computational Linguistics
 Roberta: A robUstly optimized BERT pretrainingapproach,2019, CoRR
 Universal dependencies 2,2019,4
 Languagemodels are unsupervised multitask learners,2019, 2019
 Attention is all you need,2017, In I
 The united nations parallel cor-pus v1,2016,0
