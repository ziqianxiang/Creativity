title,year,conference
 BERT: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Compressing deep convolutional net-works using vector quantization,2014, arXiv preprint arXiv:1412
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Like what you like: Knowledge distill via neuron selectivitytransfer,2017, arXiv preprint arXiv:1707
 Sequence-level knoWledge distillation,2016, arXiv preprintarXiv:1606
 Race: Large-scale readingcomprehension dataset from examinations,2017, In Proceedings of the 2017 Conference on EmpiricalMethods in Natural Language Processing
 Cross-lingual language model pretraining,2019, arXiv preprintarXiv:1901
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Fixed point quantization of deep convolu-tional netWorks,2016, In International Conference on Machine Learning
 ToWards accurate binary convolutional neural netWork,2017, InAdvances in Neural Information Processing Systems
 Thinet: A filter level pruning method for deep neuralnetWork compression,2017, In Proceedings of the IEEE international conference on computer vision
 Glove: Global vectors for Wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Languagemodels are unsupervised multitask learners,2019, 2019
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Neural machine translation of rare words withsubword units,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
 Q-bert: Hessian based ultra low precision quantization of bert,2019, arXiv preprintarXiv:1909
 Structured transforms for small-footprint deeplearning,2015, In Advances in Neural Information Processing Systems
 Patient knowledge distillation for bert modelcompression,2019, arXiv preprint arXiv:1908
 Distilling task-specific knowledge from bert into simple neural networks,2019, arXiv preprint arXiv:1903
 High performance ultra-low-precision convolutions on mobiledevices,2017, arXiv preprint arXiv:1712
 Binarized neural networks on the imagenet classificationtask,2016, arXiv preprint arXiv:1604
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Large batch optimization for deep learning: Trainingbert in 76 minutes,2019, arXiv preprint arXiv:1904
 On-device neural language modelbased word prediction,2018, In Proceedings of the 27th International Conference on ComputationalLinguistics: System Demonstrations
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2016, arXiv preprint arXiv:1612
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In Proceedings of the IEEE international conference on computervision
