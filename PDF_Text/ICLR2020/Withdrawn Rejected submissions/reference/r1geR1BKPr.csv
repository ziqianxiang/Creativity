title,year,conference
 A framework for learning predictive structures from multipletasks and unlabeled data,2005, J
 Understand-ing the origins of bias in word embeddings,2019, In ICML
 Characterizations of an empirical influence function fordetecting influential cases in regression,1980, Technometrics
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Deep residual learning for image recognition,2016, In 2016 IEEEConference on Computer Vision and Pattern Recognition (CVPR)
 On the accuracy of influencefunctions for measuring group effects,2019, In NeurIPS
 Distributed representationsof words and phrases and their compositionality,2013, In Advances in neural information processingsystems
 Visual explanation by interpretation: Improvingvisual feedback capabilities of deep neural networks,2019, In International Conference on LearningRepresentations
 Glove: Global vectors for wordrepresentation,2014, In Proceedings of the 2014 conference on empirical methods in natural languageprocessing (EMNLP)
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Repairing without retraining: Avoiding disparateimpact with counterfactual distributions,2019, In Proceedings of the 36th International Conference onMachine Learning
 The pretrained and finetuned models are trained to converge,1000, When validatingthe influence function score
 The finetune task is a bi-nary sentiment classification of twitter1 and the ELMo model is pretrained on a one-billion-worddataset Chelba et al,1000, (2013)
