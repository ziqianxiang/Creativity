title,year,conference
 Online embedding com-pression for text classification using low rank matrix factorization,2019, In AAAI
 Groupreduce: Block-wise low-rank approximation for neural language model shrinking,2018, In Advances in Neural InformationProcessing Systems
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Product quantization for nearest neighborsearch,2010, IEEE transactions on pattern analysis and machine intelligence
 Tensorized embed-ding layers for efficient model compression,2019, arXiv preprint arXiv:1901
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Sentencepiece: A simple and language independent subwordtokenizer and detokenizer for neural text processing,2018, CoRR
 Recurrentneural network based language model,2010, In Eleventh annual conference of the international speechcommunication association
 Probabilistic matrix factorization,2008, In Advances in neuralinformation processing systems
 Tensorizing neuralnetworks,2015, In Advances in neural information processing systems
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th annual meeting on association forcomputational linguistics
 Regular-izing neural networks by penalizing confident output distributions,2017, CoRR
 A call for clarity in reporting BLEU scores,2018, In Proceedings of the Third Conferenceon Machine Translation: Research Papers
 Using the output embedding to improve language models,2016, arXiv preprintarXiv:1608
 Fitnets: Hints for thin deep nets,2015, In ICLR
 Deep learning in neural networks: An overview,2015, Neural networks
 Compressing word embeddings via deep compositional codelearning,2017, arXiv preprint arXiv:1711
 Re-thinking the inception architecture for computer vision,2015, CoRR
 Multilingual neural machine trans-lation with knowledge distillation,2019, CoRR
 Attention is all you need,2017, In I
 Low-rank matrix factoriza-tion of lstm as effective model compression,2018, 2018
