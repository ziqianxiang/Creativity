title,year,conference
 Reconciling modern machine learningand the bias-variance trade-off,2018, CoRR
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofCOMPSTATâ€™2010
 On the ineffectiveness of variance reduced optimization for deeplearning,2018, CoRR
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Optimal mini-batch and step sizes forSAGA,2019, In International Conference on Machine Learning (ICML)
 Surprises in high-dimensional ridgeless least squares interpolation,2019, CoRR
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In NeurIPS
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Neural Information Processing Systems (NeurIPS)
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Don'tjump through hoops and remove thoseloops: Svrg and katyusha are better without the outer loop,2019, arXiv preprint arXiv:1901
 Learning multiple layers of features from tiny images,2009, Technical report
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Gradient-based learning applied todocument recognition,0018, Proceedings of the IEEE
 Wide neural networks of any depth evolve as linear models under gradientdescent,2019, arXiv preprint arXiv:1902
 A linearly-convergent stochastic l-bfgs al-gorithm,2016, In Artificial Intelligence and Statistics
 A stochastic gradient method with anexponential convergence rate for finite training sets,2012, In Neural Information Processing Systems(NeurIPS)
 No more pesky learning rates,2013, In ICML (3)
 Fast convergence of stochastic gradient descent under a stronggrowth condition,2013, arXiv preprint arXiv:1308
 Asymptotic and finite-sample properties of estimators basedon stochastic gradients,2017, Ann
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Fast and faster convergence of SGD for over-parameterized models and an accelerated perceptron,2019, In AISTATS
 Variance reduction for stochasticgradient optimization,2013, In Advances in Neural Information Processing Systems
 Understanding short-horizon bias instochastic meta-optimization,2018, In ICLR (Poster)
 Understandingdeep learning requires rethinking generalization,2017, In ICLR
 Which algorithmic choices matter at which batch sizes?insights from a noisy quadratic model,2019, arXiv preprint arXiv:1907
