title,year,conference
 The effects of adding noise during backpropagation training on a generalizationperformance,1996, Neural computation
 Stronger generalization bounds for deepnets via a compression approach,2018, Proceedings of the 34th International Conference on MachineLearning
 Entropy-sgd: Biasing gradient descentinto wide valleys,2016, In ICLR 2017
 An investigation into neural net optimizationvia hessian eigenvalue density,2019, arXiv preprint arXiv:1901
 Simplifying neural nets by discovering flat minima,1995, InA
 Flat minima,1997, Neural Computation
 Three factors influencing minima in sgd,2017, arXiv preprint arXiv:1711
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, In ICLR2018
 Geometric integration theory,2008, Springer Science and BusinessMedia
 Simplified pac-bayesian margin bounds,2003, In Learning theory and Kernel machines
 Some pac-bayesian theorems,1998, In Proceedings of the eleventh annual conferenceon Computational learning theory
 On the importance ofsingle directions for generalization,2018, In ICLR 2018
 Exploring general-ization in deep learning,2017, In Advances in Neural Information Processing Systems
 A scale invariant flatness measure for deep netWork minima,2019, arXiv preprintarXiv:1902
 Understanding machine learning: From theory toalgorithms,2014, Cambridge university press
 Intriguing properties of neural netWorks,2013, arXiv preprint arXiv:1312
 Normalized flat minima: Exploring scaleinvariant definition of flat minima for neural netWorks using pac-bayesian analysis,2019, arXiv preprintarXiv:1901
 Identifying generalizationproperties in neural netWorks,2018, arXiv preprint arXiv:1809
 Robustness and generalization,2012, Machine learning
 Understandingdeep learning requires rethinking generalization,2016, In ICLR 2017
 Theory of deep learning iib: Optimization properties of sgd,2018, arXiv preprint arXiv:1801
