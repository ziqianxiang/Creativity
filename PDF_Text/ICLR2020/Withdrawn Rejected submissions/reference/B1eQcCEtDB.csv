title,year,conference
 Language model rep-resentations for beam-search decoding,1995, In 1995 International Conference on Acoustics
 An actor-critic algorithm for sequence prediction,2016, arXiv preprintarXiv:1607
 Fundamentals of Statistical Exponential Families: With Applications in StatisticalDecision Theory,1986, Institute of Mathematical Statistics
 Towards improved language model evaluation measures,1999, InSixth European Conference on Speech Communication and Technology
 Information theory: coding theorems for discrete memoryless sys-tems,2011, Cambridge University Press
 Transformer-xl: Language modeling with longer-term dependency,2018, 2018
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Maskgan: better text generation via filling inthe_,2018, arXivpreprint arXiv:180L07736
 Prediction in the worst case,1991, Annals of Statistics
 Frage: frequency-agnosticword representation,2018, In Advances in Neural Information Processing Systems
 Improving neural language models with acontinuous cache,2016, arXiv preprint arXiv:1612
 Document contextlanguage models,2015, arXiv preprint arXiv:1511
 Exploring thelimits of language modeling,2016, arXiv preprint arXiv:1602
 Calibrated forecasting and merging,1999, Games and EconomicBehavior
 Sparse attentive backtracking: Temporal credit assignment through remind-ing,2018, In Advances in Neural Information Processing Systems
 A simple way to initialize recurrent networksof rectified linear units,2015, arXiv preprint arXiv:1504
 Adversarial ranking forlanguage generation,2017, In Advances in Neural Information Processing Systems
 Agreement on target-bidirectionalneural machine translation,2016, In Proceedings of the 2016 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Building a large annotatedcorpus of english: The penn treebank,1993, 1993
 Information theoretic co-training,2018, arXiv preprint arXiv:1802
 On the state of the art of evaluation in neural languagemodels,2017, arXiv preprint arXiv:1707
 Regularizing and Optimizing LSTMLanguage Models,2017, arXiv preprint arXiv:1708
 An Analysis of Neural Language Mod-eling at Multiple Scales,2018, arXiv preprint arXiv:1803
 Fisher gan,2017, In I
 Integral probability metrics and their generating classes of functions,1997, Advances inApplied Probability
 Probabilistic outputs for support vector machines and comparisons to regularizedlikelihood methods,1999, In Advances in Large Margin Classifiers
 Languagemodels are unsupervised multitask learners,2019, 2019
 Improvements in beam search,1994, In ThirdInternational Conference on Spoken Language Processing
 Cross entropy of neural language models at infinityanew bound of the entropy rate,2018, Entropy
 Learning longer-term dependenciesin rnns with auxiliary losses,2018, arXiv preprint arXiv:1803
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Competitive on-line statistics,2001, International Statistical Review
 Topic compositional neural language model,2017, arXiv preprint arXiv:1712
 Neural text generation: A practical guide,2017, arXiv preprint arXiv:1711
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Regularizing neuralmachine translation by target-bidirectional agreement,2019, In Proceedings of the AAAI Conference onArtificial Intelligence
