Figure 1: Results on four-room domain. Six tasks were used for training and 24 for testing.
Figure 2: Visualization of our framework in four rooms domain. A novel task is seen in the top left,where the agent (red) has to navigate to a goal (green). On the top right, we show the solution foundby the agent. The three rows below show how the options were learned and exploited in the newtask. The highlighted area in the top row shows a sample trajectory and the color corresponds tothe probability that the option would take the demonstrated action. Notice that this trajectory wasobtained on a previous tasks, so it does not correspond to the new task on top. The arrows show theaction that is most likely at each state. After training (first row) each option specializes in a specificskill (a navigation pattern). In this case, the demonstrated trajectory can be generated by usingoption 3 and 2. The middle rows shows a heat-map indicating where an option is likely to terminate(close to walls), and the last row shows where each options is likely to be used by the policy learnedin the new task. The agent learns to use each option in specific situations; for example, option 1 islikely to be called to make the agent move up, if it is located in one of the bottom rooms.
Figure 3: Comparison on Atari domains for primitives (blue), options before training (orange) andlearned options for different values of λ1 and λ2 . Shaded regions indicate standard error.
