Figure 1: Left: The VIC framework (Gregor et al., 2016) in a navigation context: an agent learns high-levelmacro-actions (or options) to reach different states in an environment reliably without any extrinsic reward.
Figure 2: Illustration of VIC for 2 timesteps. L: Given a start state S0 , VIC samples option ω and followspolicy π(at | Ω = ω, St) and infers Ω from the terminating state (S2), optimizing a lower bound on I(S2, Ω |So). R: DS-VIC considers a particular parameterization of π and imposes a bottleneck on I (At, Ω∣St).
Figure 3: Decision state heatmaps plottingI(Ω, Zt∣St,So) at visited states on simple envi-ronments 一 4-Room (top) and maze (bottom). Firstcolumn depicts environment layout, second and thirdshow results for DS-VIC for β = 1e-3 and β = 1respectively, and the fourth column shows DIAYN.
Figure 4: We visualize identified decision states for 3 options by DS-VIC on the Mountain Car environment. (a)shows the environment layout, (b) the trajectories corresponding to the 3 options in position-velocity space — iftrajectories corresponding to two different options reach the same position and velocity, then they are said tointersect, (c) the final-state distributions in the position-velocity space, and (d) the identified decision states. In(b), (c), (d), x-axis (position) shows the x-coordinate of the car, while y-axis shows the velocity of the car.
Figure 5: Effect of β during DS-VIC pre-trainingon success with exploration bonus for the goal-conditioned policy. Shaded areas represent stan-dard error of the mean over 10 random seeds.
Figure 6: Transfer results on MultiRoomN6S25 after unsupervised pre-training to identify decision states onMultiRoomN2S6. Shaded regions represent standard errors of the mean over 10 random seeds.
