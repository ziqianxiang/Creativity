Figure 1: Given a video and a sentence, our aim is to retrieve the most relevant segment (the red boundingbox in this example). Existing methods consider video frames as independent inputs and ignore the contextualinformation derived from other frames in the video. They compute a similarity score between the segment andthe entire sentence to determine their relevance to each other. In contrast, our proposed approach aggregatescontextual information from all the frames using graph propagation and leverages fine-grained frame-by-wordinteractions for more accurate retrieval. (Only some interactions are shown to prevent overcrowding the figure.)In this paper, we take another step towards addressing the limitations of current weakly-supervisedvideo moment retrieval methods by exploiting the fine-grained temporal and visual relevance of eachvideo frame to each word (Figure 1b). Our approach is built on two core insights: 1) The temporaloccurrence of frames or segments in a video provides vital visual information required to reasonabout the presence of an event; 2) The semantics of the query are integral to reasoning about therelationships between entities in the video. With this in mind, we propose our Weakly-SupervisedMoment Alignment Network (wMAN). An illustrative overview of our model is shown in Figure 2.
Figure 2: An overview of our combined wMAN model which is trained end-to-end. We use theoutputs of the GRU as word representations where its inputs are word embeddings. The visualrepresentations are the outputs of the LSTM unit where its inputs are the extracted features from apretrained CNN. The visual representations are concatenated with positional encodings to integratecontextual information about their relative positions in the sequence. Our model consists of a two-stage multimodal interaction mechanism - Frame-By-Word Interactions and the WCVG.
Figure 3: Visualization of the final relevance weights of each word in the query with respect to eachframe. Here, we display the top three weights assigned to the frames for each phrase. The colors ofthe three numbers (1,2,3) indicate the correspondence to the words in the query sentence. We alsoshow the ground truth (GT) temporal annotation as well as our predicted weakly localized temporalsegments in seconds. The highly correlated frames to each query word generally fall into the GTtemporal segment in both examples.
