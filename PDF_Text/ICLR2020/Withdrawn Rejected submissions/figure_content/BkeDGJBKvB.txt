Figure 1: Two hierarchical posterior poli-cies (left and right) with common priors(middle). For each task i, the policy con-ditions on the current state sti and the lastselected option zti-1. It samples, in order,whether to terminate the last option (bit),which option to execute next (zti) and whatprimitive action (ait) to execute in the envi-ronment.
Figure 2: Performance during testing of the learned options and exploration priors. Each line is the medianover 5 random seeds (2 for MLSH) and shaded areas indicate standard deviations. More details in text.
Figure 3: Options learned with MSOL on the taxi domain, before (top) and after pickup (bottom). The lightgray area indicates walls. The left plots show the intra-option policies: arrows and colors indicated directionof most likely action, the size indicates its probability. A square indicates the pickup/dropoff action. The rightplots show the termination policies: intensity and size of the circles indicate termination probability.
Figure 4: Hierarchical learning of two concurrent tasks (a and b) using two options (z1 and z2) to reach tworelevant targets (A and B). a) Local minimum when simply sharing options across tasks. b) Escaping the localminimum by using prior (Wi) and posterior (z(j)) policies. C) Learned options after training. Details are givenin the text in Appendix A.
Figure 5: Performance during training phase. Note that msol and msol(frozen) share the sametraining as they only differ during testing. Further, note that the highest achievable performance forTaxi and Directional Taxi is higher during training as they can be initialized closer to the final goal(i.e. with the passenger on board).
Figure 6: Tradeoff between exploration and misspecification: For smaller grid sizes, exploration is easier andlearning from scratch performs competitively. For larger grid sizes, soft options can accelerate training throughfaster exploration, even if they are misspecified because they were trained on a different set of tasks.
