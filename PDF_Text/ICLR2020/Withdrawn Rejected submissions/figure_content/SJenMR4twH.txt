Figure 1: Left. The interpretable neural network model divides input images into several pieces.
Figure 2: (a) The architecture of GFNN for grayscale images. The difference between a regularCNN and our model is the substitution of the kernels in the first layer by predefined ones. (b) Ourinterpretable components for color images. Predefined kernels are applied to each color componentsand resulting layers are concatenated. 1 × 1 × 96 × D filters, not predefined, are convoluted to reducethe dimension.
Figure 3: (a) Accuracy comparison when the number of predefined kernel filters decreases withMNIST dataset. Accuracy drops as 32 edge filters in Table 1 decrease to 4 filters. (b) Accuracycomparison between different conditions with MNIST. (c) Accuracy comparison when the numberof predefined kernel filters decreases with ImageNet dataset. (d) Accuracy comparison betweendifferent conditions with ImageNet.
Figure 4: The average filter scores of each filter for the entire MNIST data set. Filter numbersrepresent kernel types as described in Table 1.
Figure 5: Typical MNIST images of 0, 5 and 8. Rows show the most significant filters for the givennumbers, and their filter score images.
Figure 6: (a) The interpretation of an entire ’green mamba’ class. The color filter scores and edgefilter scores are shown. (b) An example of the ’steel arch bridge’ class. The original image and itscolor filter scores are on the left side. The red and blue channels of the original image are shown inthe center. The magnitude of each color channel is shown with colors. The sky background is notconsidered for classification as indicated in the color filter score.
