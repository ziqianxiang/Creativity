Figure 1: Geometric intuition of the working of the proposed retrospection loss. The figures showpolytopes in the weight parameter space. (Left) For all θi inside the shown colored polytope, theretrospective loss is negative and is positive outside. Our objective is to push parameters of thecurrent θτ further inside this polygon close to θ*; (Right) In a future time step T0 > T, by designof the retrospective loss, the polytope region shrinks and our objective at this time step is to pushparameters to a near-optimal region around θ*.
Figure 2: Classification performance using retrospection on F-MNIST and SVHN datasetsconsistency of the direction of the gradient update in the training trajectory, and that one can usethese previous directions to get a more robust estimate of the gradient step to be taken currently. Incontrast, retrospection leverages the same idea from the opposite perspective, viz., consistency ofthe direction of the gradient update is only local, and hence the parameter state, θTp farther awayfrom the current state θT , provides a cue of what the next parameter must be far from. This raisesinteresting discussions, and the possibility of analyzing retrospection as a thrust obtained from anundesirable parameter state, as opposed to momentum. We leave these as interesting directions offuture work, and focus this work on proposing the method, and showing its effectiveness in trainingneural networks.
Figure 3: Inception Scores using retrospection on CIFAR-10 (Krizhevsky, 2009)(row 1) and F-MNIST (Xiao et al., 2017) (row 2) datasets using DCGAN (Radford et al., 2015) (col 1), ACGAN(Odena et al., 2016)(col 2), LSGAN (Mao et al., 2016)(col 3).
Figure 4: Images generated over training epochs when ACGAN (Odena et al., 2016) trained onFMNIST dataset: (a) without retrospection (row 1) (b) with retrospection (row 2)5Under review as a conference paper at ICLR 2020For all experiments, the retrospection loss is initialized without any warm-up period (zero epochs).
Figure 5: Classification performance using retrospection on LeNet(Lecun et al., 2001) across differ-ent batch sizes on FMNIST (Xiao et al., 2017)Choice of scaling mar-gin, κ We conduct exper-iments using different ini-tial values of the loss scal-ing margin, κ. For thisanalysis, the value of κremains unchanged duringthe training. Results arepresented in Figure 6 (Row1) with best performanceachieved with κ = 4. Allconfigurations produce bet-ter performance than withκ = 1.
Figure 6: Ablation studies of classification performance on the F-MNIST dataset: (Row 1) Varying loss scaling parameter, κ (left), andretrospective update frequency (right); (Row 2) Using retrospectionon LeNet (Lecun et al., 2001) and SGD vs Adam optimizers; (Row3) Using SGD, SGD +momentum, SGD + retrospection for LeNet,ResNet-20 architecturesconducted preliminary ex-periments on image classi-fication to evaluate the impact of the retrospective loss on optimization. We contrast performancefrom three different configurations on image classification: (a) trained without retrospective loss(SGD); (b) trained without retrospective loss (SGD + momentum); and (c) with retrospective loss(SGD). Results in Figure 6 (Row 3) highlight that introducing retrospection improves performance(blue vs green); moreover, using the retrospective loss improves convergence even when SGD isoptimized without momentum.
Figure 7: Training DCGAN on FMNIST dataset using retrospection loss with different warm-upperiods.
