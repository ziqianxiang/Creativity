Figure 1: Overview of our Interactive Attention Learning Framework.
Figure 2: Neural Attention Process (NAP). Embeded outputs l and the global latent variable z are generatedfrom the embedding network (A-1) and neural process (A-2), respectively, and the final attention output Î± isproduced by the linear transformation (A-3).
Figure 3: Attention Annota-tion Interface (Healthcare).
Figure 4:	ViSualization of attention for a Selected patient on theCardio VaScular DiSeaSe (CVD) prediction taSk. Contribution indi-cateS the extent to which each individual feature affectS the onSetof CVD in 1 year. Age - Age, Smoking - Whether the patient cur-rently SmokeS, SysBP - SyStolic blood preSSure, HDL - High-denSitylipoproteinS choleSterol, LDL - Low-denSity lipoprotein choleSterol.
Figure 5:	Top 5 variableS that are of-ten Selected the moSt by NP-Random,NP-Selective, IF-Selection, and Atten-tion CroSS Entropy (ACE) on the CVDtaSk. VariableS in red StandS for impor-tant key factor determined by phySicianS.
Figure 6: (Top) Change of accuracy with 40 annotations for each iterations(s) on the all tasks between NAP-Selective (Red) vs NAP-Random (Green). (Bottom) Mean Response Time (mean-RT) of annotators to evaluateone data sample (Being prioritized by uncertainty vs Randomly ordered).
Figure 7: Top 10 variables (Green)ranked by the total number of beingchecked by physicians and top 10variables (Blue) ranked by attentionuncertainty in the CVD task.
Figure 8: Attention map for 14 pairs of body joints, generated from RNNs trained for squat pose correctiontask (Fitness).
Figure 9: Interactive Attention learning Annotation Interface for Squat Pose Correction Task.
Figure 10: Interactive Attention learning Interface for EHR datsets, on which physicians interactively guidethe attention network to re-learn how to properly attend to features of a given input. Attentions from theattention network are visualized as above and physicians evaluate them on the web-based attention annotationinterface.
