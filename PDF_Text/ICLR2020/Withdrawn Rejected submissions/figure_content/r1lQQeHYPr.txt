Figure 1: Examples of embodied multi-modal tasks, following instructions andanswering questions.
Figure 3: Overview of our proposed architecture, described in detail in Section 4.
Figure 4: Architecture of the Dual-Attention unit with example intermediate representations and operations.
Figure 5: Outputs for relations ‘not’, ‘left of’, andmodified. For example, for the above question, we ‘right of’ learned by the relational modules.
Figure 6: Spatial Attention and Answer Prediction Visualizations. An example EQA episode with thequestion “Which is the smallest blue object?”. The sentence embedding of the question is shown on the top(xsent). As expected, the embedding attends to object type words (’torch’, ’pillar’, ’skullkey’, etc.) as thequestion is asking about an object type (’Which object’). The rows show increasing time steps and columns showthe input frame, the input frame overlaid with the spatial attention map, the predicted answer distribution, andthe action at each time step. As the agent is turning, the spatial attention attends to small and blue objects. Timesteps 1, 2: The model is attending to the yellow skullkey but the probability of the answer is not sufficientlyhigh, likely because the skullkey is not blue. Time step 3: The model cannot see the skullkey anymore so itattends to the armor which is next smallest object. Consequently, the answer prediction also predicts armor, butthe policy decides not to answer due to low probability. Time step 4: As the agent turns more, it observes andattends to the blue skullkey. The answer prediction for ‘skullkey’ has high probability because it is small andblue, so the policy decides to answer the question.
Figure 7: Visualizations of convolutional output channels. We visualize the convolutional channels corre-sponding to 7 words (one in each row) for the same frame (shown in the rightmost column). The first columnshows the auxiliary task labels for reference. The second column and third column show the output of the corre-sponding channel for the proposed Dual-Attention model trained without and with auxiliary tasks, respectively.
Figure 8: Training accuracy of all models trained with auxiliary tasks for Easy (left) and Hard (right).
Figure 9: Training accuracy of all models trained without auxiliary tasks for Easy (left) and Hard (right).
Figure 10: Training accuracy of proposed Dual-Attention model with all ablation models trained without (left)and with (right) auxiliary tasks for the Easy environment.
Figure 11: Objects of various colors and sizes used in the ViZDoom environment.
Figure 12: Architecture of the policy module.
Figure 13: Gated-Attention unit fGAFigure 14: Spatial-Attention unit fSAD.1 Hyperparameters and network detailsThe input image is rescaled to size 3 × 168 × 300. The convolutional network for processing theimage consisted of 3 convolutional layers: conv1 containing 32 8x8 filters with stride 4, conv2containing 64 4x4 filters with stride 2, and conv3 containing V 3 × 3 filters with stride 2. We useReLU activations for conv1 and conv2 and sigmoid for conv3, as its output is used as auxiliary taskpredictions directly. We use word embeddings and GRU of size 32 followed by a linear layer of sizeV to get the sentence-level representation. The policy module uses hidden dimension 128 for thelinear and GRU layers (see Figure 12).
Figure 14: Spatial-Attention unit fSAD.1 Hyperparameters and network detailsThe input image is rescaled to size 3 × 168 × 300. The convolutional network for processing theimage consisted of 3 convolutional layers: conv1 containing 32 8x8 filters with stride 4, conv2containing 64 4x4 filters with stride 2, and conv3 containing V 3 × 3 filters with stride 2. We useReLU activations for conv1 and conv2 and sigmoid for conv3, as its output is used as auxiliary taskpredictions directly. We use word embeddings and GRU of size 32 followed by a linear layer of sizeV to get the sentence-level representation. The policy module uses hidden dimension 128 for thelinear and GRU layers (see Figure 12).
Figure 15: Example first-person views of the House3D environment with sample objects of various colors.
