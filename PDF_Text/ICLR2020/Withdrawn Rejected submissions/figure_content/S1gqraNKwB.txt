Figure 1: The COIRL framework (left): a context vector parametrizes the environment. For eachcontext, the expert uses the true mapping from contexts to rewards, W *, and provides demonstrations.
Figure 2: Experimental results in the autonomous driving simulation with a linear mapping (a & b)and a nonlinear mapping (c & d)Nonlinear: For the nonlinear task, we consider two reward coefficient vectors r1 and r2, and definethe mapping by f *(c) = ri if ∣∣c∣∣∞ ≥ 0.55, and τ? otherwise - an illustration is provided in theappendix. In order to learn the nonlinear mapping, we represent fW (c) using a DNN, a multi-layeredperceptron, which maps from context to reward vector. DNNs have proven to be capable of extractingmeaningful features from complex high-dimensional data, e.g., images - in these scenarios, the linearassumption no longer holds, yet DNNs often overcome such issues. In this setting, the superiorityof the descent methods rises; as the linear assumption in the ellipsoid algorithm is not met, it failsto generalize and keeps requiring new demonstrations. We believe these results to be crucial whenconsidering real-life applications, in which the problem is not necessarily linear. Such cases highlightthe strength of the descent methods, which, as Fig. 2 shows, are capable of scaling to nonlinear highdimensional mappings.
Figure 3: Experimental results in the dynamic treatment regime with a linear mappingLooking at Fig. 3a, we can see that all the algorithms manage to minimize the loss to roughly thesame error. The small bias is explained by the fact that we use truncated trajectories (as we discussedin the practical MDA paragraph) where in the ellipsoid framework experiments we used featureexpectations. We can also see that minimizing the loss leads to policies that attain -optimal valuew.r.t. the true reward Fig. 3b. Finally, in Fig. 3 we can see that all the algorithms reach around 70%accuracy with the expert policy. We emphasize here that 100% accuracy should not be expected fortwo reasons: (i) different policies may have the same feature expectations (hence the same value)but make different decisions (ii) there exists reward for which there is more than one optimal policy.
Figure 4: Experimental results in the dynamic treatment regime with a non-linear mappingof the context vector. We use a DNN to learn the mapping and follow Section 3.1 (PSGD). As seen inFig. 4, the PSGD algorithm minimizes the loss and achieves a value that is close to that of the expert.
Figure 5: Comparison between COIRL and AL on a large MDPWe compare the performance of PSGD with the projection algorithm of (Abbeel & Ng, 2004) inFig. 5. We measure performance by three metrics: run-time, value, and accuracy. Inspecting theresults, we can see that AL in the large MDP requires significantly more time to run as the numberof contexts grows, while the run time of PSGD (COIRL) is not affected by the number of contexts.
Figure 6: Offline framework, autonomous driving simulation, linear mappingA.2 Dynamic treatment regimeSimilar to Section 4.1, we compare the various methods in the ellipsoid framework . We observe thatES outperforms the ellipsoid method. Additionally, we compare the accuracy, i.e., how often doesthe policy derived from W match the expert's policy, which is derived from W*. As IRL is not asupervised learning problem, we observe that while there is a correlation between the success in thetask and the ability to act similarly to the experts policy - this correlation is not strict, in the sensethat the agent is capable of finding near-optimal policies with a relatively high miss rate (accuracy ofapproximately 80%). For more intuition, see the proof of Lemma 1.
Figure 7: Ellipsoid framework, dynamic treatment regime, linear mapping14Under review as a conference paper at ICLR 2020B	Experimental detailsIn this section, we describe the technical details of our experiments, including the hyper-parametersused. To solve MDPs, we use value iteration. Our implementation is based on a stopping conditionwith a tolerance threshold, τ, such that the algorithm stops if |Vt -Vt-1 | < τ. In the driving simulationwe used τ = 10-4 and in the sepsis treatment we use τ = 10-3.
Figure 8: Driving simulatorThe environment is modeled as a tabular MDP that consists of 1531 states. The speed is selectedonce, at the initial state, and is kept constant afterward. The other 1530 states are generated by 17X-axis positions for the agent’s car, 3 available speed values, 3 lanes and 10 Y-axis positions in whichcar B may reside. During the simulation, the agent controls the steering direction of the car, movingleft or right, i.e., two actions.
Figure 9: Visualization of nonlinear decision boundariesHyper-parameter selection and adjustments:Ellipsoid Framework: For the linear model the algorithms maintained a 3 × 3 matrix to estimateW* . Ellipsoid: By definition, the ellipsoid algorithm is hyper-parameter free and does not requiretuning.
