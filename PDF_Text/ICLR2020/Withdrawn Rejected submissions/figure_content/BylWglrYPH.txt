Figure 1: The architecture applied to the rule learning task of Marcus et al. (1999), consisting ofconvolution followed by max-pooling and softmax.
Figure 2: The encoder-decoder architecture applied to the composition task. For each input word,two embeddings are learned, one of which is concatenated with the current hidden state and theother forms the filter in a convolution applied to that input. In the decoder, actions are predicted bytaking a convolution of the hidden state, and recurrence between time steps is also a convolution.
Figure 3:	A simple CFG, producing palindromic strings.
Figure 4:	The modified LSTM architecture has a stack of memory cells which a convolutional gatecontrols the flow of information through, while a standard multiplicative output gate and additiveinput gate read and write to the cells in the bottom of the stack. As in a standard LSTM, the hiddenunits are concatenated with the input embeddings to form the vector that feeds into the units thatdrive the various gates.
