Figure 1: Training error in (b) is shown by thin curves, while test error in (b) by bold curves.
Figure 2: Gradient Descent explanation. From left to right: 1) learning rate is small enough toconverge around a minimum, 2) moderate so that it bounces among minima, 3) too large to converge.
Figure 3: SGD explanation (taken from Kleinberg et al. (2018)). The first plot: an initially largelearning rate helps escape spurious local minima. From the second to the fourth plots: after morerounds of learning rate decay, the probability of reaching the minimum becomes larger.
Figure 4: Training of WideResNet on CIFAR10 with Gradient Descent. X-axis indicates the numberof epochs (in 103). Arrows indicate the epoch with learning rate decay.
Figure 5: The largest ten eigenvalues λ (bluecurve) and converge intervals (0, 2) (bar) forWideResNet trained with Gradient Descent.
Figure 6: Expected behavior (but not observed)induced by the SGD explanation: best perfor-mances before and after decay are comparable.
Figure 7: Training of WideResNet on CIFAR10 with SGD. X-axis indicates the number of epochs.
Figure 8: The PS10 dataset. (a) Simple patterns: 10 patterns per category, complexity log2 10. (b)Complex patterns: 100 patterns per category, complexity log2 100. (c) Data composition: half of thedata only contain simple patterns while another half only contain complex patterns.
Figure 9: Experiments with lrDecay and without lrDecay (constant learning rates) w.r.t accuracies indifferent patterns. From left to right: Train with lrDecay; Train with a constant learning rate equal tothe learning rate in Stage 1, 2, and 3 of lrDecay, respectively. The X-axis shows the epoch number.
Figure 10: Comparison between lrDecay and a constant small learning rate on the PS10 dataset with10% noise. Accuracies on simple patterns, complex patterns, and noise data are plotted respectively.
Figure 11: Transferability of additional patterns learned in each stage w.r.t different target datasets.
Figure 12: Decision Procedure of AutoDecay. The counter t is reset to 0 at the action of “Decay”.
Figure 14: Results of AutoDecay.
Figure 15: Training of WideResNet on CIFAR10 by Gradient Descent with a mildly larger learningrate. X-axis indicates number of epochs. Arrows and texts show the moment of learning rate decay.
