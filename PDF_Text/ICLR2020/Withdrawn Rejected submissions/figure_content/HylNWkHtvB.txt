Figure 1: Plots of Adam, AMSGrad, and Delayed Adam trained on the synthetic example in Equation11, with a stationary point at w? ≈ 0.5. Left: The expected iterate sampled uniformly from{w1, . . . , wt}, for each iteration t. As predicted by our theoretical results, Adam moves towardsW = 1 with ∣∣Vf (w)k = 1, while Delayed Adam converges to w?. Right: The expected normsquared of the gradient, for w randomly sampled from {w1, . . . , wt}. Delayed Adam convergessignificantly faster than AMSGrad, while Adam fails to converge.
Figure 2: Validation error of a Wide ResNet 28-4 trained on the CIFAR-10 dataset with Adam (left)and AvaGrad (right), for different values of the learning rate α and parameter , where larger yieldsless adaptability. Best performance is achieved with small adaptability ( > 0.001).
Figure 3: Validation bits-per-character (lower is better) of a 3-layer LSTM with 300 hidden units,trained on the Penn Treebank dataset with Adam (left) and AvaGrad (right), for different valuesof the learning rate α and parameter , where larger yields less adaptability. Best performance isachieved with high adaptability ( < 0.0001).
