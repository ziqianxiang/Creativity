Figure 1: Training procedure. Squares are networks and rounded rectangles are loss terms.
Figure 2: Images generated by fixing the style in each group of two rows and varying thecontent. Two different styles are shown. Leftmost column taken from training set, courtesyof respective artists. Top group: Sayori. Bottom group: Swordsouls.
Figure 3: Images generated from a single content code and an assortment of styles. Includingboth style of artists from the training set and style codes randomly samples from the styledistributionEvaluation. Other than visual results, there is no well-established quantitative measurefor the quality of style transfer methods and accessing experts for anime styles is challengingfor a proper user study. The evaluation in (Chou et al., 2018) is audio specific. In (Choiet al., 2018), the classification accuracy on the generated samples is given as a qualitymeasure. However, this seemingly reasonable measure, as we shall argue in appendix C.2, isnot adequate. Nevertheless, we do report that the samples generated by our generator canbe classified by style by our classifier with 86.65% top-1 accuracy. Detailed testing procedure,results and analysis can be found in appendix C.2. A comprehensive ablation study isgiven in Appendix C, where major differences from previous approaches are evaluated. Theeffectiveness of stage 1, the disentangling encoder, is evaluated quantitatively in section B.4.
Figure 4: Style transfer results. In the top two rows, in each column are two samples fromthe training dataset by the same artist. In each subsequent group of three rows, the leftmostimage is from the training dataset. The images to the right are style transfer results generatedby three different methods, from the content of the left image in the group and from thestyle of the top artist in the column. In each group, first row is our method, second rowis StarGAN and third row is neural style. For neural style, the style image is the topmostimage in the column. Training samples courtesy of respective artists. Style samples, fromleft: Sayori, Ideolo, Peko, Yabuki Kentarou, Coffee-kizoku, Mishima Kurone, Ragho no Erika.
Figure 5: Generated samples when disentangling W from D + R(a) Stage 1 encoder for W vs. D + R(b) VAE encoderFigure 6:	Comparison of output distribution between stage 1 encoder and vanilla VAE.
Figure 6:	Comparison of output distribution between stage 1 encoder and vanilla VAE.
Figure 7:	Generated samples when disentangling D from W + RO / ə 3 √∙ ʃ 6 787OIq 3 9sc 7 P ?。/f⅛3q5∙679 9O / ⅛ 3 √∙ S 6 Z δ 9.
Figure 8:	Variation within the same digit written by the same writerWe can observe that the variation in the same row is much more dramatic in figure 7 thanin figure 5, despite that in both cases we can (roughly) consider each row to have the samecontent and each column to have the same style. This is in fact the correct behaviour: infigure 5 the variation in each row is due to W only while in figure 7 the variation in each rowis due to the combined effect of W and R. This reveals the fact that, even within multipleimages of the same digit written by the same person, the variation can be significant.
Figure 9: Distribution of each digit from stage 1 encoder for D vs. W + RTable 4: Mean Euclidean distance of a sample to the center of its classEncoder	By writer	By digit	Whole datasetEW	1.2487	0.2788	1.2505ED	0.7929	1.2558	1.2597EV	1.2185	0.4672	1.2475(a) First 2 dimensions			Encoder	By writer	By digit	Whole datasetEW	2.6757	2.0670	2.6957ED	2.4020	2.6699	2.7409EV	2.6377	1.7629	2.7363(b) First 8 dimensionsThe encoder aims to encode the unlabelled feature in the dataset while avoiding encodingthe labelled feature. Intuitively, this means that each unlabelled class should be distributeddifferently in the encoder’s code space while each labelled class should be distributed similarly.
Figure 10: Comparison of stage 1 image reconstruction with correct style and zero style,using different methods. Column 1: images from the dataset. Column 2: VAE reconstruction.
Figure 11: Change of output distribution of different encodersbut the classifier as well. The classifier explicitly tries to classify samples generated using anartist a as “not by a”.
Figure 12: Images generated from fixed style and different contents, when stage 2 classifieris not adversarial.
Figure 13: Images generated from fixed style and different contents, when explicit conditionon content is removed.
Figure 14: Images generated from fixed content and different styles, when explicit conditionon content is removed.
