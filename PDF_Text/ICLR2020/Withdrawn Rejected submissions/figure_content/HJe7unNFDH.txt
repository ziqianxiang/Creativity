Figure 1: Comparison with several existing workflows. We use nested squares to denote models withshared weights, and use the size of the square to denote the size of each model.  Workflow in themiddle refers the concurrent work from Cai et al. (2019), where submodels are sequentially inducedthrough progressive distillation and channel sorting.  We simultaneously train all child models in asingle-stage model with proposed modifications, and deploy them without retraining or finetuning.
Figure 2: On the left, we show typical accuracy curves during the training process for both small andbig child models. On the right, we plot the modified learning rate schedules with constant ending.
Figure 3: Main results of BigNASModels on ImageNet.
Figure 4: Focusing on the start of training. Ablation study on different initialization methods. Weshow the validation accuracy of a small (left) and big (right) child model.
Figure 5: Focusing on the end of training. Ablation study on different initialization methods. Weshow the validation accuracy of a small (left) and big (right) child model.
Figure 6:  The validation accuracy curves during the training process for both small and big childmodels before (left) and after (right) our modifications.
Figure 7: The validation accuracy of a small (left) and big (right) child model using different regu-larization rules.
Figure 8: Benchmark results of coarse-to-fine architecture selection.  The red dot in coarse-grainedarchitecture selection is picked and mutated for fine-grained architecture selection.
Figure 9:  Architecture visualization of the single-stage model and child models BigNASModel-S,BigNASModel-M, BigNASModel@660M, BigNASModel-L. All child models are directly slicedfrom the single-stage model without retraining or finetuning.
