Figure 1: Affine Autoregressive Transforms. Computational diagrams for forward and inverseaffine autoregressive transforms (PaPamakariOS et al., 2017). Each y is an affine transform of xt,with the affine parameters potentially non-linear functions of x<t . The inverse transform is capableof converting a correlated input, x1:T, into a less correlated variable, y1:T.
Figure 2: Motivating Example. Plots are shown for a sample of x1:T (left), u1:T (center), and w1:T(right). Here, wi：T 〜 N(wi：T; 0, I), and U and X are initialized at 0. Moving from X → U → W viaaffine transforms results in successively less temporal correlation and therefore simpler dynamics.
Figure 3: Graphical Models. Diagrams for (a) a single-transform affine autoregressive flow-basedmodel, with random variables, yi：T 〜N(yi：T; 0, I), and (b) a sequential latent variable modelwith a flow-based conditional likelihood. We have depicted simplified dynamics for y1:T and z1:Tfor clarity, however, in general, these can be non-Markov. The flow removes low-level temporalcorrelations in xi：T, whereas the latent variables, zi：T, capture any remaining structure in yi：T.
Figure 4: Flow Visualization. Visualization of the flow component for (a), (c) standalone flow-based models and (b), (d) sequential latent variable models with flow-based conditional likelihoodsfor Moving MNIST and BAIR Robot Pushing. From top to bottom, each figure shows 1) the originalframes, Xt, 2) the predicted shift, μθ(x<t), for the frame, 3) the predicted scale, σθ(x<t), for theframe, and 4) the noise, yt , obtained from the inverse transform.
Figure 5: Implementation Visualization of the autoregressive flow.
Figure 6: Model Architecture Diagrams. Diagrams are shown for the (a) approximate Posterior,(b) conditional prior, and (c) conditional likelihood of the sequential latent variable model. Convdenotes a convolutional layer, LSTM denotes a long short-term memory layer, fc denotes a fully-connected layer, and t_conv denotes a transposed convolutional layer. For Conv and t_convlayers, the numbers in parentheses respectively denote the number of filters, filter size, stride, andpadding of the layer. For fC and LSTM layers, the number in parentheses denotes the number ofunits.
Figure 7: Temporal Correlation During Training. corry during training for SLVM w/ 1-AF onthe KTH Actions. Temporal correlation decreases substantially during training.
Figure 8: Flow Visualization on KTH Action. Visualization of the flow component for (a) stan-dalone flow-based models and (b) sequential latent variable models with flow-based conditionallikelihoods for KTH Actions. From top to bottom, each figure shows 1) the original frames, xt , 2)the predicted shift, μθ(x<t), for the frame, 3) the predicted scale, σθ(x<t), for the frame, and 4)the noise, yt , obtained from the inverse transform.
Figure 9: SLVM w/ 2-AF Visualization on Moving MNIST. Visualization of the flow componentfor sequential latent variable models with 2-layer flow-based conditional likelihoods for MovingMNIST. From top to bottom on the left side, each figure shows 1) the original frames, xt , 2) thelower-level predicted shift, μ1 (x<t), for the frame, 3) the predicted scale, σ1 (x<t), for the frame.
Figure 10: Generated Moving MNIST Samples. Samples frame sequences generated from a 2-AFmodel.
Figure 11: Generated BAIR Robot Pushing Samples. Samples frame sequences generated fromSLVM w/ 1-AF.
