Figure 1: Contrastive multi-views framework. The model is trained to distinguish between differentviews of context sentences and negative examples. The two views are obtained using a standardLSTM and a Tree LSTM networks on top of a dependency tree structure. A discriminative objectiveis used to contrast between samples.
Figure 2: Evolution of performances on the downstream SICK-E (top) and SICK-R (bottom) tasks.
Figure 3: (left) End-to-end framework where the algorithm learns to map directly the inputs to thetarget. The hidden sates of the neural network are only used for this task. (right) In contrast toend-to-end approaches the algorithm proceeds in two phases. First a representation is learned witha self-supervised proxy task. The same representation might then be used for different tasks.
Figure 4: The batching procedure to optimize the graph computation. For each batch, the compu-tation is decomposed in steps which insure that every node dependent have already been computed.
