Figure 1: For He initialization, the norm of hidden activation hi roughly equals the norm of input x;and the norm of weight gradient ∂ Wi := WdW(Xv' roughly equals the product of norm of input Xand the norm of output error δ(x, y) when width is sufficiently large. Glorot initialization does nothave this property.
Figure 2: Tightness of lower bounds on network width derived in lemma 2 and lemma 5 shown in theleft sub-figure and right sub-figure respectively. See text for more details.
Figure 3: Effect of non-uniformity of width in deep ReLU network on the gradient norm equalityproperty. For each of the three networks, width of each layer is selected independently fromU (1000 - v, 1000 + v), where v is the width variation shown in the plot. Gradient norm equalityholds more accurately when width variation is smaller.
