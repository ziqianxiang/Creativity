Figure 1: We report CIFAR-10 test (b) and ImageNet val (a) top-1 accuracy for winning tickets foundwith label-agnostic tasks: RotNet or Exemplar. The x-axis corresponds to different pruning ratios.
Figure 2: ImageNet top-1 validation accuracy of AlexNet winning tickets generated by pruningpartly or entirely (all) a network with 2 generation tasks: labels classification or RotNet. We alsoshow for reference results when the network layers are randomly pruned.
Figure 3:	Top-1 accuracy on ImageNet validation set of winning tickets generated with differenttraining data. The x-axis represents different levels of winning ticket sparsity.
Figure 4:	CIFAR-10 top-1 test accuracy of win-ning tickets generated by labels classification ona randomly permuted labels. Deep models arehighly sparse on CIFAR-10 with only 15% of non-zero weights (see 4.5 for details). For this reasonwe adjust the random baseline to start with thecorrect mask for 85% sparsity.
Figure 5:	(a) ImageNet top-1 validation accuracy of winning tickets generated with different accessto both data and labels. We compare using the full dataset, all images but only 10% of labels (semi-supervised), 10% of labels only and all images but no labels (self-supervised). (b) ImageNet top-1validation accuracy of winning tickets generated from other datasets.
Figure 6: Magnitude of the weights of a trainednetwork on two different datasets: CIFAR-10(green) and ImageNet (red). We perform thresh-olding at machine precision value (bottom of y-axis). On CIFAR-10, a trained VGG-19 is 84.5%sparse while a trained ResNet-18 is 80.3% sparse.
Figure 7: ImageNet top 1 validation accuracy of winning tickets generated with a subset of 10% ofImageNet dataset. We show the influence of different values for the late resetting parameter.
Figure 8: ImageNet top 1 validation accuracy of winning tickets generated by pruning partly orentirely (all) a network with 2 generation tasks: labels classification or RotNet. AlexNet is used intop row and ResNet-50 in the bottom row. We also show for reference results when the networklayers are randomly pruned.
Figure 9: Magnitude of the weights of a trained network on two different datasets: CIFAR-10(green) and ImageNet (red) with different datasets. We perform thresholding at machine precisionvalue (bottom of y-axis).
Figure 10: Top-1 accuracy on CIFAR-10 test set of winning tickets generated with different trainingdata. The x-axis represents different levels of winning ticket sparsity.
Figure 11: We report CIFAR-10 test (b) and ImageNet val (a) top-1 accuracy for winning ticketsfound with RotNet label-agnostic task. The x-axis corresponds to different pruning ratios. We com-pare the performance with supervised winning tickets and random subnetworks (randomly drawnweights from the initialization distribution and randomly permuted masks). On CIFAR-10, deepmodels are highly sparse with only ã€œ15% of non-zero weights. Thus, We adjust the random base-line to start with the correct mask at the natural level of network sparsity (4.5 for details). The dashedlines correspondong to applying the pruning masks found with labels or RotNet, but with randomlyre-initialized weights.
