Figure 1: (a) Adversarial examples are a summation of clean examples and adversarial perturbations.
Figure 2: Performance of LeNet with adversarial perturbation bias (our method) versus classicalnetwork LeNet under three different computation power budgets for three different scenarios. Theadversarial examples are generated using FGSM ( = 0.3). 1. Negligible costs scenario (A,B): training neural network only on clean examples. LeNet with adversarial perturbation biasdemonstrates much increased test accuracy on adversarial examples, and slightly increased testaccuracy on clean examples compared to classical LeNet. 2. Moderate extra costs scenario (C, D):training neural network only on adversarial examples. LeNet with adversarial perturbation biasdemonstrates largely increased test accuracy on clean examples and slightly increased test accuracyon adversarial examples compared to classical LeNet. 3. High extra costs scenario (E, F): training theneural network on both clean and adversarial examples. LeNet with adversarial perturbation biasdemonstrates slightly increased test accuracy on both clean and adversarial examples compared toclassical LeNet.
Figure 3: Test accuracy on adversarial examples after training only on clean examples from MNIST(Blue) or FashionMNIST (Orange) datasets. (a): Influence of different adversarial perturbationbudgets () on LeNet with adversarial perturbation bias. (b): Influence of different structures ofadversarial perturbation bias on LeNet under adversarial attack: FGSM = 0.3.
