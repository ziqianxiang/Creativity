Figure 1: Left: A demonstration of Shuffle Conv, GAP+FC and 1x1Conv on a VGG-16 architecture,where last 2 conv layers are modified accordingly.   Right:  The detail of the shuffle conv.   Eachfeature map from the input tensor will be randomly and independently shuffled before being fed intoan ordinary convolution.
Figure  2:  Classification  results  for  VGG-16  and  ResNet-50  on  CIFAR100.   K  is  the  number  ofmodified last layers, which refer to convolutional layers for VGG-16 and bottlenecks for ResNet-50.
Figure 3:  Left:  The test accuracy of ResNet-50 on Small-ImageNet increases monotonically withthe increase of the number of 1x1 convolutional layers.  Right:  The relation between the receptivefield size and the test accuracy difference to the baseline for different image size on VGG-16 overCIFAR100 shows that the test accuracy saturates with the increase of the receptive field size for agiven image size.  The minimal required receptive field tends to be larger for larger image size andthis minimum is normally larger than the actual image size. The exact relation is however unclear.
Figure 4: Pooling methods give better test accuracy than convolution with stride 2 as down-samplingmethod for ResNet-50 on Small-ImageNet.
Figure 5: Classification results for VGG-16 and ResNet-50 on Small-ImageNet. K is the number ofmodified last layers and 0 indicates the baseline performance. We observe that test accuracy can bepreserved even the last several layers are modified by GAP+FC or 1x1Conv, suggesting that spatialinformation at last layers is not necessary for a good test accuracy.  All models are trained with thesame setup.
Figure 6:  The orange curve is the test accuracy of the vanilla VGG-16 i.e.  the baseline.  The bluecurve shows the test accuracy of the VGG-16 with a single convolutional layer modified by shuffleconv at different depth.  The x-axis is the layer index with 13 being the last convolutional layer inVGG-16. Random shuffle is applied both at training and test time. The result implies random shufflehas      a larger impact at first layers than the last layers.
Figure 7: VGG-16 test accuracy with mismatched training and test schemes. The performance of astandard VGG-16 drops to the random guess level if evaluated with shuffling while shuffled modelsat last layer are invariant to this.
Figure  8:  Classification  results  for  ResNet-152  modified  by  1x1Conv  on  ImageNet.   The  mostcompressed model without affecting the test performance has the last 19 layers being modified by1x1Conv.
Figure  9:   Classification  results  for  ResNet-50  modified  by  1x1Conv  on  ImageNet.    The  mostcompressed model without affecting the test performance has the last 5 layers being modified by1x1Conv.
Figure 10:  In contrast to random spatial shuffle, VGG-16 doesnâ€™t seem to be robust to the channelshuffle. The test accuracy drops from 74.10% to 71.49% with only the last layer being shuffled.
Figure 11:  An example of ResNet-50 with the last 3 bottlenecks being modified by shuffle conv,GAP+FC and 1x1Conv.
