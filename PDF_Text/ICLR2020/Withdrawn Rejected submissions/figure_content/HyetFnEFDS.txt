Figure 1: From natural view to topological view of networks with residual connections. We mapboth addition and unary layer to nodes, and data flow to edges. Red node denotes input X and greenone means output y. Red arrows give an example of this mapping for a node with in-degree of 3.
Figure 2: The impact of topology changes on the training process in CIFAR-100. Each subgraphrepresents the loss curves of networks composed of a single node type when the topology changes.
Figure 3: The effect of sparsity constraint on distributions of α. Histogram on the left indicates thatsparsity drives most of the weights near zero. Adjacency matrices on the right shows the differencebetween uniform and adaptive one, whose rows correspond to the input edges for a particular nodeand columns represent the output ones. Colors indicate the weights of edges.
Figure 4: Impact of node (left) and edge (right) removal for complete type of TopoNet.
Figure 5: We run several times and analyze the optimized topologies, which achieve top-1 accuracyof 78.60 ± 0.06%. The matrices optimized from multiple runs have high similarity, where weightson the diagonal are larger, and connections on the off-diagonal are relatively sparse. This indicatesnodes with adjacent topological orderings have more information interactions. Besides, some essen-tial long-range connections are kept as inputs to intermediate layers. Many nodes contribute to thefinal output, leading to better representations for the subsequent layers.
Figure 6: Stand-alone top-1 accuracies on the validation set during optimization. We select topolo-gies under different iterations and freeze their α. This reveals the process of topology changes. Thenstand-alone accuracies are obtained by training the parameters of network w from scratch. It is clearthat the expression ability of topology itself increases with the training process.
