Figure 1: (a) Splitting one neuron into two off-springs. (b) Steepest descent on the overall ar-chitecture space consists of both standard gradient descent on the parameters (with fixed networkstructures), and updates of the network structures (via splitting). (c) The local optima in the lowdimensional space is turned into a saddle point in a higher dimensional of the augmented networks,and hence can be escaped by our splitting strategy, yielding monotonic decrease of the loss.
Figure 2: Illustration of theauto-differentiation trick withauxiliary activation.
Figure 3: Comparisons between our energy-aware splitting and standard splitting in Wu et al. (2019) onCIFAR-10. Results are shown for two variants of MobileNet, one with k = 3 (4 MobileNet blocks, 9 layers intotal), another with k = 6 (7 MobileNet blocks, 15 layers in total).
Figure 4: (a) Results on CIFAR-100 using MobileNet(Howard et al., 2017); (b) we show our energy-awaresplitting approach can learn more accurate and energy-efficient (with small flops) networks than pruning meth-ods (Liu et al., 2017).
Figure 5: Comparison of testing accuracy (a) and splitting time(b) using exact eigen-decomposition (denoted as splitting (ex-act)) and our fast gradient-based eigen-approximation (denotedas splitting (approx)).
Figure 6: Comparison on the test accuracy for various ab-lation study settings.
