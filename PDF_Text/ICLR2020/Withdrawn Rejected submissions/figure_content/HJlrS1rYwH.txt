Figure 1: Predict policies, rewards, ab-stract states, and the value of the abstractstates (Wellmer & Kwok, 2019).
Figure 2: PTN Expansion, a, r, v outputs fromf core are omitted for simplicitym≡θθ ©When performing decision-time planning, the action associated with the max backed-up Q-value istaken. This policy is referred to as πθ0,F, where θ0 denotes which parameters were used in the tree.
Figure 3: PTN BackupAlgorithm 1 π_Q_expand(s, i)initialize a,r,s,v0 ,Qfor j in b do^[j]〜∏(s)r[j] = fθ(s, a[j])^0[j] = ftr (s,^[j])v0[j] = fVa (^0[j])if i +1== d thenQ[j] = J (r[j] + γV0[j])∏(a = ^[j]∣s)elseQJtmP = ∏_Q_expand(S0 [j] ,i + 1)Q[j] = y(r^[j]+7((l-^)v50[j] + ^maxaiQ-tmP))n(a=^[j]Nreturn QFigure 4: pseudo-code for π-Q expansion and backup algo-rithm. Where the Q returned is a b dimensional vector.
Figure 4: pseudo-code for π-Q expansion and backup algo-rithm. Where the Q returned is a b dimensional vector.
Figure 5: Results on 1 million step MuJoCo benchmark. Dark lines represent the mean return andthe shaded region is a standard deviation above and below the mean.
Figure 6: Numerical evaluation of bound in Theorem 1 for various branching values	train depth = 5		train depth = 10		PTN	PPN	PTN	PPNHoPPer-V2	1944.0 ± 258.5	2191.6 ± 183.3-	1672.7 ± 457.5	1752.3 ± 625.4-Walker2d-v2	2936.9 ± 893.2	2808.4 ± 647.3	3054.8 ± 434.0	2565.9 ± 327.9SWimmer-V2	89.4 ± 35.3	73.3 ± 34.5	80.3 ± 35.7	57.5 ± 30.3HalfCheetah-v2	3439.8 ± 698.6	3410.5 ± 840.9	3638.9 ± 545.2	3632.8 ± 819.0InVertedPendUlum-v2	971.7 ± 56.5	979.6 ± 25.7	1000.0 ± 0.0	1000.0 ± 0.0InVertedDoUbIePendUlum-v2	4332.6 ± 192.9	3344.5 ± 234.4	4360.2 ± 145.1	3186.8 ± 87.7Table 3: Returns from PTN(no πF /decision-time planning) and PPN using only the model-freepoliciesβ(found in PTN but not PPN) is to stabilize returns over different values of training depth. In PPN,Wellmer & Kwok (2019) showed that optimal depth is highly dependent on the environment andreturns can drastically differ. While the modifications certainly do not entirely fix this, we find thatit mitigates large differences in returns from different training depths.
