Figure 1: Test accuracy per image size, modelstrained on specific sizes (ResNet50, ImageNet).
Figure 2: Training (dotted) and test accuracy vs optimization step (ResNet44, CIFAR10). We com-pare vanilla training with two computationally equivalent stochastic regimes: increased duplicates(D+) and increased batch (B+). B+ regime achieves better test accuracy at a reduced numberof iterations, while D+ improves accuracy further at a similar computational cost.
Figure 3: Training (dotted) and test accuracyon ImageNet using the Baseline, B+ and D+regimes (224 × 224 evaluation size). All regimesrequired similar computational resources per step.
Figure 4: Left: Test accuracy on validation set per image size, all models trained using the samecomputational and memory resources (regime D+). Right: Test accuracy per billion flop (at evalu-ation).
Figure 5: Impact of gradient smoothing on CIFAR10, ResNet-44. The training regime includes twoimage sizes: 32 × 32 and 16 × 16 (average size is S = 24). Using a B+ regime creates two batchsizes: 256 and 2, 048 respectively. Gradient smoothing helps to reduce gap between gradient normsat difference batch sizes and improves final accuracy.
Figure 6: Test accuracy vs step for 3 size sampling regimes: (1) From small to large (2) Sample eachEpoch (3) Sample each step. All methods reached a similar accuracy, but sampling each epoch wasless noisy and did not require hyper-parameter tuning.
