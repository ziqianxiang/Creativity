Figure 1: synchrony in a binary state s ∈ {0, 1}. We can confirm that the optimal policy for maxi-mizing synchrony is to report the information obtained from the environment without any perturba-tion. The characteristics can be extended to m-bit tapes S = {0, 1}m with the naive Bayes modelp(s) = Qi p(si), a practical model representing word distribution (McCallum, Nigam, and others,1998). In this case, each bit can be represented as events, words or symbols. Namely, the optimalpolicy for the uninformed agent (blue) is to report z = 1/2, which maximizes entropy H [s] = log 2.
Figure 2: The generative integration networks. The exploration scenario in GIN is achieved by com-munication among n non-cooperative agents made of the controller and two additional modules, adifferentiable generator Gi to send adversarial reporting, and a shared validator V to receive thedifferentiable reports and distribute synchrony to the other agents. At convergence, synchrony be-came zero and all the generators drew samples from p(s). The exploration mechanism works withoutany other intrinsic reward such as curiosity, which has negative relation to synchrony.
Figure 3: An illustration of the predator prey (PP) tasks in three difficulties. Each agent continuouslyreceives a reward signal -0.05 in each timestep until the arrival on the prey. After the predatorsreaches their goals, they receive a competitive reward 1/m that m is the number of predators whoreached the prey. Every episode ends in fixed steps. Although communication is used to tell theposition of prey, predators could fool other predators to corner the prey.
Figure 4: An illustration of the traffic junction (TJ) tasks in three difficulties. Each car had twoactions, ”accelerate” to proceed 1 step and ”brake” to stop. At the initial state, each car is given thestarting point and destination point, and is instructed to run the path as fast as possible by avoidingcollisions. To incentivize running faster, they receives a negative reward -0.05 in each time step.
Figure 5: Comparison in three methods for learning curves in the three harder tasks (PP-hard, TJ-medium, TJ-hard) .
Figure 6: Details of the intrinsiC reward and truthful reporting. Note that synChrony is relativelylower than the external reward.
Figure 7: A learning Curve in a zero-sum predator-prey task (n=3).
