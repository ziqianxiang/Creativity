Figure 1: Critical initialization improves trainability ofrecurrent networks. Test accuracy for peephole LSTMtrained to classify sequences OfMNIST digits after 8000iterations. As the sequence length increases, the networkis no longer trainable with standard initialization, but stilltrainable using critical initialization.
Figure 2: Training accuracy on the padded MNIST classification task described in 5.1 at differentsequence lengths T and hyperparameter values Θ0 + αΘ1 for networks with untied weights, withdifferent values of Θ0, Θ1 chosen for each architecture. The dark and light green curves are respec-tively 3ξ, 6ξ where ξ is the theoretical signal propagation time scale in eqn. (8). As can be seen, thistime scale predicts the transition between the regions of high and low accuracy across the differentarchitectures and directions in hyperparameter space.
Figure 3: Training accuracy for unrolled, concatenated MNIST digits (top) and unrolled MNISTdigits with replicated pixels (bottom) for different sequence lengths. Left: For shorter sequences thestandard and critical initialization perform equivalently. Middle: As the sequence length is increased,training with a critical initialization is faster by orders of magnitude. Right: For very long sequencelengths, training with a standard initialization fails completely (and is unstable from initialization inthe lower right panel).
Figure 4: Squared singular values of the State-to-stateJacobian in eqn. (15) for two choices of hyperparam-eter settings Θ. The red lines denote the empiricalmean and standard deviations, while the dotted linesdenote the theoretical prediction based on the calcu-lation described in Section 4.3. Note the dramaticdifference in the spectrum caused by choosing an ini-tialization that approximately satisfies the dynamicalisometry conditions.
Figure 5: Top: Dynamics of the correlations (6) for the GRU with 3 different values of μf as afunction of time. The dashed line is the prediction from the mean field calculation, while the redcurves are from a simulation of the network with i.i.d. Gaussian inputs. Left: Network with untiedweights. Right: Network with tied weights. Bottom: The predicted fixed point of (6) as a function ofdifferent μf. Left: Network with untied weights. Right: Network with tied weights.
Figure 6: Training accuracy on the padded MNIST classification task described in 5.1 at differentsequence lengths T and hyperparameter values Θ for networks with tied weights. The green curvesare multiples of the forward propagation time scale ξ calculated under the untied assumption. Wegenerally observe improved performance when the predicted value of ξ is high, yet the behavior ofthe network with tied weights is not part of the scope of the current analysis and deviations from theprediction are indeed observed.
Figure 7: Sampling from the LSTM cell state distribution using Algorithm 1, showing good agreementwith the cell state distribution obtained by simulating a network with untied weights. The two panelscorrespond to two different choices of Θ19Under review as a conference paper at ICLR 2020E.4 Critical initializationsPeephole LSTM:〃i,〃r,μo,ρ2,ρf ,ρ2,ρ2,ν2,νf, ν2, ν2 = 0μf = 5σi2 , σf2 , σr2 , σo2 = 10-5LSTM (Unrolled CIFAR-10 task):μi,μr ,μo,ρ2,ρf ,ρ2,ρ2,νf, ν2222ν2,ν2,σ0,μf = 1σi2, σf2, σr2 = 10-50The value of μf was found by a grid search, since for this task information necessaryto solve it did not require signal propagation across the entire sequence. In otherwords, classification of an image can be achieved with access only to the last fewrows of pixels. The utility of the analytical results in this case, as mentioned
