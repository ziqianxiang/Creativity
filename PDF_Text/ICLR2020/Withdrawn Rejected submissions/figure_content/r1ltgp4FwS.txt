Figure 1: When learning a mapping between Trump and Obama, the CycleGAN model gives good spatialfeatures, but collapses to essentially static outputs of Obama. It manages to transfer facial expressions back toTrump using tiny differences encoded in its Obama outputs, instead of learning a meaningful mapping. Beingable to establish the correct temporal cycle-consistency between domains, ours and RecycleGAN can generatecorrect blinking motions. Our model outperforms the latter in terms of coherent detail that is generated.
Figure 2: a) G. b) The UVT cycle link using recurrent G.
Figure 3: Conditional VSR Ds,t.
Figure 4: UnconditionalUVT Ds,t.
Figure 5: a) Result without PP loss. The VSR network is trained with a recurrent frame-length of 10. Wheninference on long sequences, frame 15 and latter frames of the foliage scene show the drifting artifacts. b)Result trained with PP loss. These artifacts are removed successfully for the latter. c) The ground-truth image.
Figure 6: In VSR of the foliage scene, adversarial models (ENet, DsOnly, DsDt, DsDtPH TeCoGANâˆ™andTecoGAN) yield better perceptual quality than methods using L2 loss (FRVSR and DUF). In temporal profileson the right, DsDt, DsDtPP and TecoGAN show significantly less temporal discontinuities compared to ENetand DsOnly. The temporal information of our discriminators successfully suppresses these artifacts.
Figure 7: Visual summary of VSR models. LPIPS (x-axis) measures spatial detail and temporal coherence ismeasured by tLP (y-axis) and tOF (bubble size with smaller as better). The middle graph zooms in the red-dashed-box region on the left, containing models in our ablation study. The right graph shows network sizes.
Figure 9: Additional VSR comparisons. The TecoGANmodel generates sharp details in both scenes.
Figure 10: Comparisons for VSR of captured images.
Figure 11: Results of UVT tasks on different datasets.
Figure 12: Detail views of the VSR results of ToS scenes (first three columns) and Vid4 scenes(two right-most columns) with comparisons.
Figure 13: Tables and visualization of perceptualmetrics computed with PieAPP (Prashnani et al.,2018) (instead of LPIPS used in Fig. 7 previously)on ENet, FRVSR, DUF and TecoGAN for theLPIPS-based evaluation: our network architec-VSR of Vid4. Bubble size indicates the tOF score.
Figure 14: Bar graphs of temporal metrics for Vid4.
Figure 15: Spatial metrics for Vid4.
Figure 16: Metrics for ToS.
Figure 20: Near image boundaries, flow estimation is less accu-rate and warping often fails to align well. First two columns showoriginal and warped frames and the third one shows differencesafter warping (ideally all black). The top row shows things moveinto the view with problems near lower boundaries, while the sec-boundary of 16 pixels. Thus, for an ond row has objects moving out of the view.
Figure 17: A sample setup of user study. Figure 18: Tables and bar graphs of Bradley-Terryscores and standard errors for Vid4 VSR.
Figure 19: Tables and graphs of Bradley-Terry scores and standard errors for Obama&Trump UVT.
Figure 21: 1st & 2nd row: Frame 15 & 40 of the Foliagescene. While DsDt leads to strong recurrent artifacts early on, PP-Augment shows similar artifacts later in time (2nd row, middle).
