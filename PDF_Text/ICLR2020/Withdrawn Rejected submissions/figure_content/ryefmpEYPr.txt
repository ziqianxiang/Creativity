Figure 1: Overview of weight distribution and model accuracies for MNIST dataset (LeNet-5)2018): (Li et al., 2016) proposed to eliminate neurons that have low l2-norms of their weights,whereas (Yu et al., 2018) proposed a neuron importance score propagation (NISP) technique whereneuron importance scores (using Roffo & et. al. (2015) - See Equation 5) are propagated fromthe output layer to the input layer in a back-propagation fashion. Drop-out (Srivastava & et. al.,2014) technique instead deactivates neuron activations at random. As an edge sparsification tech-nique, DropConnect (Wan et al., 2013) selects edges to be sparsified randomly. (Ashouri et al.,2018) showed that the network performance can be maintained by eliminating insignificant weightswithout modifying the network architecture.
Figure 2: Overview of iSparse sparsification, considering the ni’s contribution to overall outputrather than only between ni and nj neuronsnetwork to scales that were not practical until recently, (Lawrence et al., 1997; Yang et al., 2015;Hinton et al., 2012; Liang & Hu, 2015; Karpathy & et. at., 2014) have shown impressive success inseveral data analysis and machine learning applications.
Figure 3: A sample network architecture and its sparsification using Retraining-free (Ashouri et al.,2018) and iSparse; here node labels indicate input to the node; edge labels [0,1] indicate the edgeweights; and edge labels between parentheses indicate edge contributionMore specifically, let Wl+ be the absolute positive of the weight matrix,Wl , for edges in lth layer.
Figure 4: Top-1 and top-5 accuracy for SPar-FigUre 5: Model classification time vs sparsi-sified VGG-16 for ImageNet (sparsify-with) fication factor for MNIST dataset (train-with)∙2al1 1(SPUo.S u∙E0.9O 5	10	15	20	25	30	35	40	45	50SParSifiCation Factor^^≡PFEC ^^≡NISP DropCOnnect ^^Retrain∙Free ^^≡iSparse8°mm2°o寅)Λ□e-!n<PowCIFAR10CIFAR20CIFAR⑩0SPar市CatiOn FaCtorandDataSetSSVHNGTSRB(a) VGG network (VGG-16)IeOOooo
Figure 6: Top-1 classification accuracy results for sparsified pre-trained models (sparsify-with)VGG-16: VGG (Simonyan & Zisserman, 2015),s, a 16 layer network with 13 convolution and 3dense layers, with interleaved 5 max-pooling layers. VGG leverages ReLU as the hidden activationto overcome the problem of vanishing gradient, as opposed to tanh. Given the ability of VGGnetwork to learn the complex pattern in the real-world dataset, we use the network on benchmarkdatasets, such as CIFAR10/20/100 (Krizhevsky, 2009), SVHN (Netzer & et. al., 2011), GTSRB(Stallkamp & et. al., 2012), and ImageNet (Deng et. al., 2009). Table 1 reports the number oftrainable parameters (or weights) for each model/data set pair considered in the experiments.
Figure 7: Mask matrices for the LeNet network ConvN layer for MNIST data (SParsification factor= 50%): dark regions indicate the edges that have been marked for sparsification; in (e) iSparse,the arrows Point to those edges that are subject to different Pruning decision from retrain-free in(d)(green arrows Point to edges that are kePt in iSParse instead of being Pruned and red arrows Point toedges that are sParsified in iSParse instead of being kePt)5.4	Accuracy Results5.4.1	SPARSIFICATION OF PRE-TRAINED MODELS (sparsify-with)In Figure 4, we first Present toP-1 and toP-5 classification results for ImageNet dataset for VGG-16 network. As we see in the Figure 4, iSParse Provides the highest robustness to the degree ofsParsification in the network. In Particular, with iSParse , the network can be sParsified by 50% with≤ 6% droP in accuracy for toP-1 and ≤ 2% droP in accuracy for toP-5 classification, resPectively. Incontrast, the comPetitors, see larger droPs in accuracy. The closest comPetitor, Retrain-free, suffersa loss in accuracy of 〜16% and 〜6% for top-1 and top-5 classification, respectively. The othercomPetitors suffer significant accuracy droPs after a mere 10-20% sParsification.
Figure 8: Robustness to the layer order while sparsifying the network with iSparse (train-with)hidden activation functions and network optimizers. Table 3 presents classification performancesfor networks that rely on different activation functions (tanh and ReLU) and for optimizers (Adamand RMSProp). As we see in these two tables, iSparse remains the alternative which provides thebest classification accuracy under different activation/optimization configurations.
