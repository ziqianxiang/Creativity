Figure 1: VAE and MIM models with 2D inputs, a 2D latent space, and 5, 20 and 500 hidden units.
Figure 2: Test performance for MIM (blue) and VAE (red) for the 2D GMM data (cf. Fig. 1), all asfunctions of the number of hidden units (on x-axis). Each plot shows the mean and standard deviationof 10 experiments.
Figure 3: MIM (blue) and VAE (red) for 20D Fashion-MNIST, with latent dimension between 2 and20. Plots depict mean and standard deviation of 10 experiments.
Figure 4: MIM and VAE learning with the PixelHVAE (VP) architecture, applied to Fashion-MNIST,MNIST, and Omniglot (left to right). The top three rows (from top to bottom) are test data samples,VAE reconstruction, and A-MIM reconstruction. Bottom: random samples from VAE and A-MIM.
Figure 5: MIM and VAE z embedding for Fashion MNIST (top) and MNIST (botom).
Figure 6: MIM learning estimates two factorizations of a joint distribution: (a) encoding; (b) decodingfactorizations. (c) The estimated joint distribution.
Figure 7: Effects of entropy as a mutual information regularizer in 2D x and 2D z synthetic problem.
Figure 8: Effects of entropy as a mutual information regularizer in 2D x and 2D z synthetic problem.
Figure 9: We explore the influence of consistency regularizer Rθ . CE and MIM indicate the loss,LCE or LMIM (the regularized objective), respectively. Top row shows anchor P (x) (dashed), priorqθ(x) (red), and reconstruction distribution Xi 〜P(x) → Zi 〜qθ(z|xi) → Xi 〜pθ(x|zi) (green).
Figure 10: MIM prior expressiveness. In this experiment we explore the effect of learning a prior,where the priors q(x) and p(z) are normal Gaussian distributions. Top row shows anchor P(x)(dashed), prior qθ(x) (red), and decoding distribution Zi 〜 pθ(z) → Xi 〜 pθ(x|zi) (green).
Figure 11: Effects of MIM consistency regularizer and optimization on encoding-decoding con-sistency. (i) and (ii) differ in initialization order. Odd rows: anchor P (x) (dashed), priorqθ (x) (red). Even rows: anchor P (z) (dotted), prior pθ (z) (blue). (a-b,e-f) ReconstructionXi 〜 P(x) → Zi 〜 qθ(z|xi) → Xi 〜 pθ(x|zi) (Xi green, Zi yellow). (c-d,g-h) Prior decod-ing Zi 〜Pθ(Z) → Xi 〜Pθ(x|zi) (green), and prior encoding Xi 〜qθ(x) → Zi 〜qe(Z∣Xi)(yellow). See text for details.
Figure 12: MIM and VAE learning with PixelHVAE for Fashion MNIST. The top three rows (fromtop to bottom) are test data samples, VAE reconstruction, A-MIM reconstruction. Bottom: randomsamples from VAE and MIM. (c-d) We initialized all pseudo-inputs with training samples, and usedthe same random seed for both models. As a result the samples order is similar.
Figure 13: MIM and VAE learning with convHVAE for Fashion MNIST. The top three rows (fromtop to bottom) are test data samples, VAE reConstruCtion, MIM reConstruCtion. Bottom: randomsamples from VAE and MIM.
Figure 14:	MIM and VAE learning with PixelHVAE for MNIST. ToP three rows are test data samPles,followed by VAE and A-MIM reconstructions. Bottom: random samPles from VAE and MIM. (c-d)We initialized all Pseudo-inPuts with training samPles, and used the same random seed for bothmodels. As a result the samPles order is similar.
Figure 15:	MIM and VAE learning with convHVAE for MNIST. ToP three rows are test data samPles,followed by VAE and MIM reconstructions. Bottom: random samPles from VAE and MIM.
Figure 16:	MIM and VAE learning with PixelHVAE for Omniglot. Top three rows are test datasamples, followed by VAE and A-MIM reConstruCtions. Bottom: random samples from VAE andMIM.
Figure 17:	MIM and VAE learning with ConvHVAE for Omniglot. Top three rows are test datasamples, followed by VAE and MIM reConstruCtions. Bottom: random samples from VAE and MIM.
Figure 18: MIM and VAE z embedding for Fashion MNIST with convHVAE architecture.
Figure 19: MIM and VAE z embedding for MNIST with convHVAE architecture.
Figure 20: A-MIM and VAE z embedding for Fashion MNIST with PixelHVAE architecture.
Figure 21: A-MIM and VAE z embedding for MNIST with PixelHVAE architecture.
