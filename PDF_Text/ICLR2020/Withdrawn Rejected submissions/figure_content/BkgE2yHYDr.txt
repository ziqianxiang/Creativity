Figure 1: Feature-based augmentation for semi-supervised learning. The figure depicts the flow oflabeled data and unlabeled data. We employ the output of the last convolution layer as the features,i.e. the input of the fully connected layer. Random noise is not applied to the layer but to the weightscorresponding to label logit, similar to the synaptic variability of biological neurons. Our methodadds noise to the weights that contribute the most to classification, so the feature-based augmentedimage has implicitly the regularization.
Figure 2: The effect of feature-based augmentation on the input images. We reconstruct the noise-added image using autoencoder(Appendix). It was verified that higher-level augmentation occurred,such as a change in direction of the animalâ€™s head, creating a new wing shape of the airplane,etc. All of input images were preprocessed by ZCA. Although the restored images are somewhatblurred because the decoder of the autoencoder is fairly shallow compared with the encoder, theaugmentation can be clearly observed. (a) Reconstructed original image without any augmentation.
Figure 3: Test error comparison on 4000-label CIFAR-10 for varying dropout probability p, feature-based augmentation scaling factor k. Feature-based augmentation has the regularization effect astraditional regularization, which is useful for semi-supervised learning. (a) feature-based augmenta-tion scaling factor k = 0.15. (b) dropout probability p = 0.84	Related workZhu (2005) say that unlabeled data can be easily obtained compared to labeled data, but there arenot many ways to use it. Therefore, semi-supervised learning can solve this problem by designingbetter classifiers using large amounts of unlabeled data, together with labeled data. Semi-supervisedlearning is receiving great attention because it can achieve high accuracy with little effort.
