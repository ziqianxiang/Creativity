Figure 1: Illustration for a backdoored/Trojan CNN classifier. (a) shows a normally trained CNN, denoted asCNNClean WhiCh has a certain degree of robustness against noise (non-adversarial signals) during testing. (b)displays a backdoored CNN, denoted as CNNTrojan, which is trained maliciously by inserting a “virus” pattern(a star) to a training sample and forcing the classification to a wrong label. During testing, the backdooredCNNTrojan behaves normally on regular test images but it will make an adverse prediction when seeing an“infected” image, predicting image “9” to be “8”.
Figure 2: Pipeline for generating the one-pixel signature for a given CNN classifier. Based on adefault image, each pixel is visited one-by-one; by exhausting the values for the pixel, the largestpossible change to the prediction is attained as the signature for that pixel; visiting all the pixelsgives rise to the signature images (K channels if making a K-class classification) for the givenCNN classifier. See the mathematical definition in Eq. 1.
Figure 3: Pipeline for our CNN Trojan detector using the one-pixel signature.
Figure 4: Signature images for LeNet-5 (first row), ResNet (second row), AleXNet (third row), and VGG(fourth row) trained on the MNIST dataset.
Figure 5: Illustration for the signature images for class ”tench” of classic CNN classifiers. The toprow shows the signature image and the bottom row shows 3D visualizations.
Figure 6: Training and testing data generation for evaluating our Trojan detector as seen in Fig. 3. Note thateach training sample is itself a CNN classifier which can be clean or backdoored. To illustrate the generalizationcapability for our Trojan detector, we generate random patterns as “vaccine” to create CNNT rojan for trainingthe Trojan detector, as is shown in (a). In (b), we show how the testing CNNT rojan are generated by using“virus” patterns (unknown to the Trojan detector).
Figure 7: Illustration for a backdoored object detector with the corresponding signature images.
Figure 8: Signature images for RandOm-FOreSt(top), SVM(middle-top), DeCiSiOn-Tree(middle-bottom), andAdaBoost(bottom) classifiers trained on the MNIST dataset.
Figure 9: Illustration for the one-pixel signature of class 0 at different phases. The model used hereis ResNet-50 (He et al., 2016) trained on cifar-10 dataset.
