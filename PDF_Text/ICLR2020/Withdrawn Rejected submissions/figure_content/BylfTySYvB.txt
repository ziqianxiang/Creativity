Figure 1: gato learns to copy long sequences with an order of magnitude fewer parametersthan the gru and lstm. We plot average probability of copied tokens on a held-out set. Errorbands correspond to random seeds. The gru and lstm output probabilities near chance for thecopied tokens. gru-diag with same number of parameters as gato beats other baselines whenexcluding eurnn. eurnn NANed on one seed (not shown) and was unstable for the other two.
Figure 2: gato is more robust across different seeds and sequence lengths. We show the meansquared errors of the adding problem for sequence lengths T = 100, 200, 400, 750 over number oftraining examples. LSTM failed to learn the task. For length T = 400, the GRU numerically failed onone seed (NAN error). We denote this by infinite error. gru-diag could learn the task with moresamples than gato. sru, eurnn, and rhn could not learn this task (NAN and/or worse than chance).
Figure 3: Both sizes of s work well for the add task. 50% is more stable, suggesting long-termgradient propagation is important for this task. This is consistent with the findings in Appendix Awhere 0% s failed on this task.
Figure 4: Both sizes of s work well for the copy task.
Figure 5: Tanh performs similarly to sigmoid for regularizing non-linearity on copy.
Figure 6: Tanh performs similarly to sigmoid for regularizing non-linearity on add.
Figure 7: sin for decoder non-linearity performs similarly to sos on copy task.
Figure 8: sin for decoder non-linearity performs similarly to cos on adding task.
