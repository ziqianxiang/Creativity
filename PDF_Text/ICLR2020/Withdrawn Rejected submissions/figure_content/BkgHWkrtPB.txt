Figure 1: (Left) Plot of the marginal distribution of the parameter w at the end of training, marginal-ized over all possible training tasks D, as the batch size B of SGD changes. Notice that as the batchsize gets smaller SGD shifts farther away from areas with high Fisher Information (dotted line),supporting Proposition 3.6. (Center) Effect of the batch size on the Information in the Weights.
Figure 2: (Left) Trace of the Fisher Information of the weights of a ResNet-18 after 30 epochs oftraining on CIFAR-10, for different values of the batch size. Smaller batch size have smaller FisherInformation, supporting Proposition 3.6. (Right) Fisher Information at the end of the training on asubset of CIFAR-10 with only the first k classes: After training on fewer classes, the network hasless information in the weights, suggesting that the Information in the Weight has a semantic role.
Figure 3: Plot of the log-determinant of the Fisher Information Matrix during training of a 3-layersfully connected network on a simple 2D binary classification task. As the network learns an increas-ingly complex classification boundary the Fisher increases. Moreover, learning of a new featurecorrespond to small bumps in the Fisher plot, supporting the idea that feature learning may coincidewith crossing of narrow bottlenecks in the loss landscape.
