Figure 1: AnisotroPicBoWlIterationsFigure 2: Ridge RegressionFigure 3: Smooth-BPDNFigures 1 to 3: Results from experiments onstrongly convex functions with Lipschitz gradi-ents. Geodesic and Geodesic-N are our methodsand the baselines are Gradient Descent, HeavyBall, Nesterov and Fletcher-Reeves. All meth-ods start from the same point and are terminatedwithin a tolerance of the global optimum. Thehorizontal axes show the iterations and the ver-tical axes show the distance to the optimal value.
Figure 2: Ridge RegressionFigure 3: Smooth-BPDNFigures 1 to 3: Results from experiments onstrongly convex functions with Lipschitz gradi-ents. Geodesic and Geodesic-N are our methodsand the baselines are Gradient Descent, HeavyBall, Nesterov and Fletcher-Reeves. All meth-ods start from the same point and are terminatedwithin a tolerance of the global optimum. Thehorizontal axes show the iterations and the ver-tical axes show the distance to the optimal value.
Figure 3: Smooth-BPDNFigures 1 to 3: Results from experiments onstrongly convex functions with Lipschitz gradi-ents. Geodesic and Geodesic-N are our methodsand the baselines are Gradient Descent, HeavyBall, Nesterov and Fletcher-Reeves. All meth-ods start from the same point and are terminatedwithin a tolerance of the global optimum. Thehorizontal axes show the iterations and the ver-tical axes show the distance to the optimal value.
Figure 4:	MNIST dataset.
Figure 5:	FACES dataset.
Figure 6: CURVES datasetFigures 4 to 6: Results from autoencoder ex-periments on three datasets. The horizontal axisshows computation time and the vertical axisshows log-scale training error. Our methods areSGeO and SGeO-N and the baselines are SGD-HB and SGD-N, variants of SGD that use theHeavy Ball momentum and Nesterov,s momen-tum respectively, along with K-FAC. All meth-ods use the same initialization. SGeO-N is able tooutperform other methods on the MNIST datasetand performs similarly to K-FAC on the other twowhile outperforming other baselines. Figures arebest viewed in color.
Figure 7: MNIST dataset.
Figure 8: FACES dataset.
Figure 9: CURVES datasetFigures 7 to 9: Results from autoencoder ex-periments on three datasets. The horizontal axisshows iterations and the vertical axis shows log-scale training error. K-FAC is a second-ordermethod and takes much fewer iterations to con-verge. However, the per-iteration cost is muchhigher than the other methods. SGeO and SGeO-N are our methods and the baselines are Nesterov(SGD-N), Heavy Ball (SGD-HB) and KFAC-M(M indicating a form of momentum). Figures arebest viewed in color.
Figure 11: FACES dataset.
Figure 10: MNIST dataset.
Figure 12:	CURVES datasetB Generalization ExperimentsWe include generalization experiments on the test set here. However, as mentioned before, ourfocus is optimization and not generalization, we are aware that the choice of optimizer can have asignificant effect on the performance of a trained model in practise. Results are shown in Figures10 to 12. SGeO-N shows a significant better predictive performance than SGeO on the CURVESdata set and both perform similarly on the two other datasets.. Note that the algorithms are tunedfor best performance on the training set. Overfitting can be dealt with in various ways such as usingappropriate regularization during training and using a small validation set to tune the parameters.
Figure 13:	Adaptive Coefficient and cosine (dot product) values for the strongly convex YC = 1-g∙dand non-convex YNC = 1 + g ∙ d cases where g ∙ d is equal to cos (∏ - φ).
Figure 14:	Anisotropic BowlFigure 15:	Ridge RegressionF - - - 1 O2 5 15 06IU①一。①Oo ①≥ldep<Figure 16: Smooth-BPDNFigures 14 to 16 show the value of the adap-tive coefficient per iteration during training for thestrongly convex functions with LiPschitz gradi-ents from our experiments. Note that here YC =1 - gt ∙ dt. Figures are best viewed in color.
Figure 15:	Ridge RegressionF - - - 1 O2 5 15 06IU①一。①Oo ①≥ldep<Figure 16: Smooth-BPDNFigures 14 to 16 show the value of the adap-tive coefficient per iteration during training for thestrongly convex functions with LiPschitz gradi-ents from our experiments. Note that here YC =1 - gt ∙ dt. Figures are best viewed in color.
Figure 16: Smooth-BPDNFigures 14 to 16 show the value of the adap-tive coefficient per iteration during training for thestrongly convex functions with LiPschitz gradi-ents from our experiments. Note that here YC =1 - gt ∙ dt. Figures are best viewed in color.
Figure 17:	MNIST dataset.
Figure 18:	FACES dataset.
Figure 19: CURVES datasetFigures 17 to 19 show the behaviour of the adap-tive coefficient per iteration for the autoencoderexperiments. Note that here YNC = 1 + gt ∙ dtand the experiments are taken from optimizationon the training set. We can see the effect ofNesterov,s lookahead gradient on the autoencoderbenchmarks. Figures are best viewed in color.
Figure 20: Results from the scaled Goldstein-Price and the Levy function experiments. The figuresshow the contour plots of the functions and the paths taken by the methods in different colors.
