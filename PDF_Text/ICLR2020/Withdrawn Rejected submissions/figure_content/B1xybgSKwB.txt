Figure 1: Example of a Triggers envi-ronment.
Figure 3: On the left, the distribution of attention weights around triggers for correct positive rewardpredictions in a 8x8 Triggers maze with 3 triggers and one reward. The x-axis denotes the signednumber of steps between the state-action couple receiving attention and the closest actual moment theagent activated a switch. On the right, the distribution of attention weights around keys for correctreward predictions for door traversals in DMLab.
Figure 4: On the left, in-domain transfer results on a 8x8 Triggers with 3 triggers and 1 reward. Onthe right, results in DMLab.
Figure 5: Bigger environments we considerare bigger mazes where the structure of theoriginal task is conserved (number of triggers,number of prizes). Environments drawn are12x12 grids with 1 trigger and 1 prize for thetop figure versus 2 prizes for the bottom one.
Figure 6: The controls of the out-of-domaindistribution are inverted (up becomes down,right becomes left). Environments are 8x8grids with 1 trigger and 1 prize (top figure)or 2 prizes (bottom figure). The effect of theshaping is exclusively beneficial, while trans-ferring weights from the source task can bedetrimental to the learning process.
Figure 7:	The architecture used for Secret. α∙^t is the vector containing the attention weights ofthe model for its prediction at step t.
Figure 8:	Attention heatmap in DMLab. Attention concentrates around the key which is necessary tounlock reward.
Figure 9: Effect of various parameters on Secret.
Figure 10: On the left, the distribution of attention weights around triggers for correct positive rewardpredictions in a Triggers maze from an out-of-domain distribution (bigger mazes). The x-axis denotesthe signed number of steps between the state-action couple receiving attention and the closest actualmoment the agent activated a switch. On the right, same figure for another out-of-domain distribution(inverted dynamics).
