Figure 1: Adversarial examples generated by Defective CNNs. First row: the adversarial examplesand the predicted labels. Second row: the corresponding original images and the ground truth labels.
Figure 2: An illustration of three channels (neuron-wise) from standard convolutional layer todefective convolutional layer. Black neurons are the defective neurons which are randomly selected.
Figure 3: An example image that is randomly shuffled after being divided into 1 × 1, 2 × 2, 4 × 4and 8 × 8 patches respectively.
Figure 4: Relationship between success defense rates against adversarial examples generated by PGDand test accuracy with respect to different keep probabilities. Each red star represents a specific keepprobability with its value written near the star.
Figure 5: Defense performance against images with additive Gaussian noise, Defective versusStandard CNN. p-Bottom means applying defective convolutional layers with keep probability p tothe bottom layers of ResNet-18.
Figure 6: CIFAR-10 dataset. First row: the adversarial examples generated by defective CNNs andthe predicted labels. Second row: original images. Second row: the adversarial examples generatedby the standard CNN and the predicted labels.
Figure 7: Tiny-ImageNet dataset. First row: the adversarial examples generated by defective CNNsand the predicted labels. Second row: original images. Second row: the adversarial examplesgenerated by the standard CNN and the predicted labels.
Figure 8: The architecture of ResNet-183X3 Coa 643X3 c"nu- 643X3 ConF 643X3 COnF 64C.2 RESNET-50Similar to ResNet-18, ResNet-50 (He et al., 2016) contains 5 blocks and each block contains several1 × 1 and 3 × 3 convolutional layers (i.e. Bottlenecks). In our experiment, we apply defectiveconvolutional layers to the 3 × 3 convolutional layers in the first three bottom blocks. The defectivelayers in the 1st block are marked by the red arrows in Figure 9.
Figure 9: The architecture of ResNet-50Dense-Block 0Figure 10: The architecture of DenseNet-121C.5 VGG-19VGG-19 (Simonyan & Zisserman, 2014) is a typical neural network architecture with sixteen 3 × 3convolutional layers and three fully-connected layers. We slightly modified the architecture byreplacing the final 3 fully connected layers with 1 fully connected layer as is suggested by recentarchitectures. Figure 12 shows the whole structure of VGG-19. We apply defective convolutionallayers on the first four 3 × 3 convolutional layers.
Figure 10: The architecture of DenseNet-121C.5 VGG-19VGG-19 (Simonyan & Zisserman, 2014) is a typical neural network architecture with sixteen 3 × 3convolutional layers and three fully-connected layers. We slightly modified the architecture byreplacing the final 3 fully connected layers with 1 fully connected layer as is suggested by recentarchitectures. Figure 12 shows the whole structure of VGG-19. We apply defective convolutionallayers on the first four 3 × 3 convolutional layers.
Figure 11: The architecture of SENet-18Figure 12: The architecture of VGG-19curves of standard and defective ResNet-18 on CIFAR-10 and MNIST. Different network structuresshare similar tendency regarding the train and test curves.
Figure 12: The architecture of VGG-19curves of standard and defective ResNet-18 on CIFAR-10 and MNIST. Different network structuresshare similar tendency regarding the train and test curves.
Figure 13: The architecture of WideResNet-3220TeSt CUrVeResNet-18 train0.5-Bottom train0.3-Bottom trainResNet-18 test0.5-Bottom test0.3-Bottom test10	20^ResNet-18 train0.5-Bottom train0.3-Bottom trainResNet-18 test0.5-Bottom test0.3-Bottom testOJ-----.---------.------0	50	100	150200	250Number of epochs
Figure 14: Train and test curve of standard and defective ResNet-18 on CIFAR-10 and MNISTcontrols the '∞-norm between the adversarial example and the original one by the parameter.
Figure 15: Randomly sampled images from Tiny-ImageNet dataset. The network structure usedto generate these images is defective ResNet-18 with keep probability 0.1 on the 1st, 2nd blocks.
