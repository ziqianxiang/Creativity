Figure 1: (a) The inference speed of each method. (b) The approximation error of the originalGraphSAGE-GCN (i.e., with ReLU activation and normalization) and GraphSAGE-GCN with ReLUactivation. (c) The approximation error of the gradient with ReLU and sigmoid activations. (d)The approximation error of Algorithm 2 and its theoretical bound. (e) The approximation error ofGraphSAGE-GCN, GAT, and GCN with the Barabasi-Albert model. (f) The approximation error ofGraPhSAGE-GCN, GAT, GCN, and GraPhSAGE-Pool with the Erdos-Renyi model.
Figure 2: Real data.
