Figure 1: (Left) Networks trained independently on MNIST achieve similar performance, butlearn different functions. For two networks trained independently on the MNIST odd-even classifi-cation task, we show the averaged final fractional test error, fgrac (blue dots). We also plot f1r,a2c (5),the fraction of Gaussian i.i.d. inputs and MNIST test images the networks classify differently aftertraining (green diamonds and orange crosses, resp.). (Right) Training independent networks on ateacher task with i.i.d. inputs does not reproduce this behaviour. We plot the results of the sameexperiment, but for Gaussian i.i.d. inputs with teacher labels y* (Eq. 4, M = 4). For both plots,g(x) = erf (x∕√2) , η = 0.2, P = 76N, N = 784.
Figure 2: (Left) Extended periods with stationary test error during training (“plateaus”)appear in the vanilla teacher-student setup, not on MNIST. We plot the generalisation errorgmse (3) of a network trained on Gaussian i.i.d. inputs with teacher labels (Eq. 4, M = 4, blue)and when learning to discriminate odd from even digits in MNIST (orange). We trained eitherthe first layer only (dashed) or both layers (solid). Notice the log scale on the x-axes. (Right)Both structured inputs and latent labels are required to remove the plateau for synthetic data.
Figure 3: A latent task on structured inputs makes independent networks behave like networkstrained on MNIST. (Left) For two networks trained independently on a binary classification taskwith structured inputs (6) and latent labels 需(Eq. 7, M = 1), We plot the final fractional test error,fgrac (blue dots). We also plot f1r,a2c (5), the fraction of Gaussian i.i.d. inputs and structured inputsthe networks classify differently after training (green diamonds and orange crosses, resp.). (Right)In the same experiment, structured inputs with teacher labels y* (4) (M = 4) fail to reproduce thebehaviour observed on MNIST (cf. Fig. 1). In both plots, f(x) = Sgn(x), g(x) = erf (x/√2), D =10, η = 0.2.
Figure 4: (Left) Same plot as the right plot of Fig. 1 With Gaussian i.i.d. inputs Xi and labels y* (4)provided by a teacher network with M = 4 hidden units that was pre-trained on the MNIST task,reaching 〜5% on the task. Inset: Typical generalisation dynamics of networks where We train thefirst or both layers (dashed and solid, resp.). g(x) = erf (x/vz2) , η = 0.2, N = 784, M = K =4, P = 76N . (Right) Four different setups for synthetic data sets in supervised learning problems.
Figure 5:	The input-generating function must be non-linear. We repeat the plots of Fig. 3,where we plot the fractional test errors of networks trained on labels generated by a teacher withM = 1 hidden units acting on the inputs (Left) and on the coefficients (Right), only that we takethe inputs to be X = cF, i.e. we choose a linear data-generating function f(x) = x. Notably,even networks trained within the vanilla teacher-student setup will disagree on Gaussian inputs.
Figure 7: The plateau in the vanilla teacher-student setup can have larger generalisationerror than the asymptotic error in a latenttask on structured inputs. Generalisation dy-namics of a sigmoidal network where we trainonly the first layer on (i) structured inputs X =max(0, CF) with latent labels yi (7)((blue, D =10) and (ii) the vanilla teacher-student setup(Sec. 2, orange). In both cases, M = 5, K =6, η = 0.2, P = 76N, Vm = V = 1.
Figure 6:	The qualitative behaviour of inde-pendent students trained on the hidden man-ifold model does not depend on our choice ofdata-generating non-linearity f (x). Same plotas Fig. 3, with X = max(0, CF). M = 1, η =0.2, D = 10, Vm = 1.
Figure 8: Measuring early stopping errors does not affect the phenomenology of latent andteacher tasks. (Left) Performance of independent sigmoidal students on the MNIST task as evaluatedby the early-stopping generalisation error. (Center and Right) We reproduce Fig. 3 of the maintext, but this time We plot the early-stopping generalisation error ^g^ac for two networks trainedindependently on a binary classification task with structured inputs (6) and latent labels 禧(Eq. 7,M = 1, Center) and teacher labels 滤(4) (M = 4) (Left). In both plots, f(x) = Sgn(X),g(x)=erf (x∕√2) ,D = 10, η = 0.2.
Figure 9: Performance of independent networks trained on a latent task with inputs in manylatent directions D = N/2. (Top Left) For two networks trained independently on a binary classifi-cation task with structured inputs (6) and latent labels y↑ (Eq. 7, M = 1), We plot the final fractionaltest error, fgrac (blue dots). We also plot f1r,a2c (5), the fraction of Gaussian i.i.d. inputs and structuredinputs the networks classify differently after training (green diamonds and orange crosses, resp.).
Figure 10: Behaviour of independent students with ReLU activation functions. (Left) Asymp-totic generalisation error of independent students with ReLU activation function g(x) = max(0, x)on the MNIST task. (Center and Right) We reproduce Fig. 3 of the main text for two networks withReLU activation trained independently on a binary classification task with structured inputs (6) andlatent labels y↑ (Eq. 7, M = 1) (Center) and teacher labels y↑ (4) (M = 4 Right). In both plots,f(x) = sgn(x), g(x) = max(0, x), D = 10, η = 0.1.
