Figure 1: Toy IXOR dataset, with x1, x2 being two coordinates and x3 being the size of the pointIn this dataset, (x1, x2, x3) clearly have different levels of feature. x3 can be directly used by theGLM layer as it has a linear decision boundary. (x1, x2) is more complex as they form an XORpattern and cannot be linearly separated, thus requiring further decomposition to be made sufficientfor the GLM layer. To make correct decisions, the DNN should use one layer to decompose theXOR into lower level features, and directly transport x3â€™s value to into the GLM layer.
Figure 2: Weight heatmap of Baseline and proposed model with the initial architecture of 3-16-8-2.
Figure 3: Illustration of the model with three hidden layers. Yellow denotes hidden layer that typi-cally has ReLU activations and green denotes the kth level feature separated out by the gates. Thickarrows denote vector form of input and output. The dimension between the input of the hiddenlayers and the output can be different.
Figure 4: MNIST training performance curve and number of inputs passed to the following hiddenlayer (blue denotes the number of features passed to the firs hidden layer. Orange curve denotes thesecond).
Figure 5: MNIST model trained with 4 layers of convolutional neural network structure. We usethe gradients to show different level of features our model extract from each layer. We see that eachclass is emphasized on features on different levels. 5, 0, 1 and 8 are more sensitive to l0 feature.
