Figure 1: Comparing fixed α with adaptively trained α. Black dashed lines are the performance of thepartially trained policies (distinct from the behavior policies which have injected noise). We reportthe mean over the last 10 evaluation points (during training) averaged over 5 different random seeds.
Figure 2: Comparing different number of Q-functions for target Q-value ensemble. We use a weightedmixture to compute the target value for all of these variants. As expected, we find that using anensemble (k > 1) is better than using a single Q-function.
Figure 3: Comparing taking the minimum v.s. a weighted mixture in Q-value ensemble. We find thatsimply taking the minimum is usually slightly better, except in Hopper-v2.
Figure 4: Comparing policy regularization (pr) v.s. value penalty (vp) with MMD. The use of valuepenalty is usually slightly better.
Figure 5: Comparing different divergences under both policy regularization (top row) and valuepenalty (bottom row). All variants yield similar performance, which is significantly better than thepartially trained policy.
Figure 6:	Comparing value penalty with KL divergence (kl_vp) to vanilla SAC, behavior cloning(bc), BCQ and BEAR. Bottom row shows sampled training curves with 1 out of the 5 datasets. SeeAppendix for training curves on all datasets.
Figure 7:	Correlation between learned Q-values and performance. x-axis is the average of learnedQψ(s, a) over the last 500 training batches. y-axis is the average performance over the last 10evaluation points. Each plot corresponds to a (environment, algorithm, dataset) tuple. Different pointsin each plot correspond to different hyperparameters and different random seeds.
Figure 8: Comparing policy regularization (pr) v.s. value penalty (vp) with all four divergences. Theuse of value penalty is usually slightly better.
Figure 9: Visualization of performance under different hyperparameters. The performance is averagedover all five datasets.
Figure 10: Visualization of performance under different hyperparameters.
Figure 11: Training curves on all five datasets when comparing kl_vp to other baselines.
Figure 12:	Training curves when comparing different divergences with policy regularization. Alldivergences perform similarly.
Figure 13:	Training curves when comparing different divergences with value penalty. All divergencesperform similarly.
