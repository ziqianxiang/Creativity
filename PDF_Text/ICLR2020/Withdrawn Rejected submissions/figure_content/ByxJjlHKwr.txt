Figure 1: The latent reward prediction model (φθ, fψz, Rζz). Blue and green circles represent theinput and output variables. The squares indicate components of the prediction model and whitecircles represent the latent variables. The model takes in an initial state st and a sequence of actionsat<t+H-i) to return a sequence of H-step predicted rewards rt：(t+H-1).
Figure 2: Training curves for the 10-step reward prediction loss. All experiments are ran in theoffline setting using 20000 steps from the true environment under a random policy and trained for300 epochs. The results of our reward prediction model is shown in the left, the state predictionmodel in the middle, and DeepMDP in the right.
Figure 3: Training curves for the iterative scheme in the 5-pendulum environment. We initialize theeach model with 2500 steps, and then perform 100 training iterations of batch-size 256 after eachepisode is collected with the current policy with = 0.7-greedy exploration. Each method is runwith 5 different seeds. The solid lines and shaded areas are the average training returns and their onestandard deviation regions under the exploration policy. The corresponding dashed lines mark theaverage final evaluation performance after ≈ 23800 environment samples.
Figure 4: The average return training curves for SAC, Deepmdp and our reward prediction model inthe 1-cheetah and 5-cheetah environments (85-dimensional). The reward model is trained iterativelyonline with zero-mean Gaussian exploration noise. Solid lines indicate training return (with 5 seedsand shaded one standard deviation regions), and the dashed lines are the final evaluation returnswithout exploration noise.
Figure 5: Depicted is the open loop prediction result of the Image-Pendulum environment successfullystabilized by the learned latent-reward CEM policy. The model is first trained on 10000 samples froma random policy, then another 10000 sample under the new control policy.
