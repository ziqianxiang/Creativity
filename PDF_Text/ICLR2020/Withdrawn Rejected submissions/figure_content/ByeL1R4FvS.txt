Figure 1: Training objective for UDA, where M is a model that predicts a distribution of y given x.
Figure 2: Augmented examples using back-translation and RandAugment.
Figure 3: Three schedules of TSA. We set Î· = a * (1 - K) + K. a is set to 1 - exp(- T * 5),T and exp((T - 1) * 5) for the log, linear and exp schedules.
Figure 4: Comparison with two semi-supervised learning methods on CIFAR-10 and SVHN withvaried number of labeled examples.
Figure 5: Accuracy on IMDb and Yelp-2 with different number of labeled examples. In the large-data regime, with the full training set of IMDb, UDA also provides robust gains.
Figure 6: Error rate of UDA on CIFAR-10 with different numbers of possible transformations inRandAugment. UDA achieves lower error rate when we increase the number of possible transfor-mations, which demonstrates the importance of a rich set of augmentation transformations.
