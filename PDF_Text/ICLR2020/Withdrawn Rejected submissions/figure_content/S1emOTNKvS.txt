Figure 1: A subgraph of 38 organizations from Transaction dataset: (a) The original subgraph sam-pled from the Transaction dataset, where nodes and edges represent organizations and their trans-actions, respectively; (b) The sparsified subgraph by NeuralSparse; (c) Testing AUC on identifyingpromising organizations.
Figure 2: The overview of NeuralSparse2018; Voudigari et al., 2016), node distance distribution (Leskovec & Faloutsos, 2006), or clusteringcoefficient (Maiya & Berger-Wolf, 2010). Importance based edge sampling has also been studied ina scenario where we could predefine edge importance (Zhao, 2015; Chen et al., 2018).
Figure 3: Sparsified subgraphs and performance vs hyper-parameters(3) In comparison With the tWo NeuralSparse variants SS-GraphSAGE and RD-GraphSAGE, Neu-ralSparse outperforms because of the automatically learned graph sparsification With both structuraland non-structural information as input.
Figure S1:	Impact from hyper-parameter k on validation and testing on the Transaction datasetIn this section, we demonstrate how the hyper-parameter k impacts the performance ofNeuralSparse-GAT and NeuralSparse-GraphSAGE in both validation and testing on the Transac-tion dataset. In terms of validation, as shown in Figure S1, the validation performance increaseswhen k ranges from 2 to 10 with more available graph data. After k exceeds 10, the increase invalidation performance slows down and turns to be saturated. In terms of testing performance, itshares a similar trend when k ranges from 2 to 10. Meanwhile, the testing performance drops moreafter k exceeds 10.
Figure S2:	Node classification performance when adding noise to graph structure.
Figure S3:	Distributions of Z in graphs with different dIn Figure S3, the distributions of node representation Z are demonstrated at different d. When d is0, A is an identity matrix so that We simply use input node features in model learning. As shown inFigure S3(a), it is difficult to find a good boundary that well separates the positive and negative nodesby using node features only. However, when We adjusts d to 10 or 20 with richer connections, thesituation doesnâ€™t get better. Because of the noise introduced by irrelevant edges, it becomes harder tofind the classification boundary. While a deep learning may still be able to find a complex boundarythat well separates the training data, the boundary could overfit the introduced noise, resulting inlow generalization power.
Figure S4: Distributions of Z in sparsified subgraphs by NeuralSparse(a) k=1	(b) k=5	(C) k=10	(d) k= 15Figure S5: Distributions of Z in sparsified subgraphs by random downsamplingIn Figure S5, we demonstrate how random downsampling could impact the prediction accuracy ofthe GCN. In general, we could not see any significant improvement. Indeed, it is crucial to performa task-driven sparsification as NeuralSparse does.
Figure S5: Distributions of Z in sparsified subgraphs by random downsamplingIn Figure S5, we demonstrate how random downsampling could impact the prediction accuracy ofthe GCN. In general, we could not see any significant improvement. Indeed, it is crucial to performa task-driven sparsification as NeuralSparse does.
