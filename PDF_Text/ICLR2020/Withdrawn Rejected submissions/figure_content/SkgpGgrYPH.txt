Figure 1: Discrimination accuracy as a function of theratio between the prefix length and the total length ofAccuracy1 random negative	82.93 random negatives	84.2worst negative out of 3	84.6Table 7: Effect of different strategies to minenegatives using TransfBig generator and BiL-STMSmall energy function on Book Corpus.
Figure 2: Effect of applying various perturbations (word replacement and swap of adjacent words) to ground-truth sequences at different positions in terms of energy function and generator negative log-likelihood (averagedover the whole test set of Wikitext). The energy is only affected by corruptions at either end of the sequence.
Figure 3: Distributions of score (negative energy) differences between pairs of ground truth completionsand generated ones for different scoring models. We show results for two generations (left to right and rightto left) from Wikitext dataset. In both cases we generate 40 tokens. Examples on the right of the red line(margin = 0.1) have zero rankiing loss.
Figure 4: Real and fake (negatives generated from TransfBig language model) completions as scored by thelearned energy function. The energy function is able to separate them well. These scores are calcuated based onthe single example reported in the main text of Â§F. f (x) is the negative energy.
