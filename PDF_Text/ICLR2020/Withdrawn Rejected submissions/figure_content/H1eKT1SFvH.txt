Figure 1: An example of uniform quantizer with dead zone.
Figure 2: Demonstration of the additivity of mean squared output error on AleXNet and VGG-16.
Figure 3:	Example of finding optimal bit allocation under Pareto condition (best viewed in color).
Figure 4:	Pareto-optimal bit allocation of weights and activations across layers on ResNet-50 Whencompressed into 2 bits on average.
Figure 5: Effectiveness of each component in the proposed compression framework. Results areshown over the ImageNet validation dataset when weights and activations are compressed to differ-ent sizes. The size of original ResNet-50 and MobileNet-v1 are 116 MB and 35 MB, respectively.
Figure 6: Tradeoff between size and accuracy on ResNet-50 and MobileNet-v1 over ImageNetvalidation dataset comparing with state-of-thhe-art methods from the literature.
Figure 1: Examples of a neural network F and a modified neural network F.
Figure 2: Pareto-optimal bit allocation of weights and activations across layers on ResNet-50 andMobileNet-v1 when weights and activations are compressed to 2 bit and 4 bits on average.
Figure 3: The relationship between the variances and the bitrates of weights across layers on ResNet-50 and MobileNet-VI when compressed to 2 bits on average.
