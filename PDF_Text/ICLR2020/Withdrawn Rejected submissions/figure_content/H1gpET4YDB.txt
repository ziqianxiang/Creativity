Figure 1: Memory Profiling for BERT2.2	A Regression Analysis on Activation MemoryFor BERT, or more specifically, Transformer, the activation memory corresponds to intermediateresults of different layers It grows linearly in all the model hyper-parameters, except the sequencelength N, due to the attention layers. To quantify more clearly the O(N) and O(N 2) componentsin the activation memory, we conduct a regression analysis as follows. Assume that the activationmemory (in each GPU) is a polynomial a2bN2 + a1bN + a0, where b is the batch size in eachGPU. If we fix the total number of tokens in a GPU, i.e., b × N, to be constant (in our case, 4096),we should have a linear function w.r.t. N, i.e., 4096a2N + 4096a1 + a0. We enumerate N from{128, 256, 512, 1024} in our experiments, and plot the corresponding profiled activation memory inFigure 1b. Using ordinary least squares (OLS), with b × N = 4096, the estimated linear functionfor activation memory is 0.00715 × N + 4.83, where the first term is responsible for the O(N 2)component. When N = 512, we can see that for BERT-Base, the O(N 2) component accounts for3.66 GB and O(N) accounts for 4.83 GB. When the sequence length N increases to 1024, however,the O(N2) component increases to 7.32 GB, while O(N) is unchanged.
Figure 2: Architecture of Blockwise Multi-head Attention.
Figure 3: Regression analysis on activationmemory for BERT and BlockB ert.
Figure 4: Ablation over blockwise attention heads assignment.
Figure 5: The sparse masking matrices we use in Sparse Transformer (fixed mode) encoder. Whitecolor indicates attention values to be masked. (a) N = 512, ` = 128, c = 32, density 44.20%; (b)N = 1024, ` = 128,c= 32, density 34.97%.
Figure 6: Paragraph length (after tokenization) distribution. The distribution of SQuAD 2.0 is verysimilar to SQuAD 1.1, so we only plot SQuAD 1.1 here.
