Figure 1: Delayed Feedback1.	A way to handle delayed feedback is presented and implemented by maintaining a queueto store some recent data of samples. This concept introduces the same delayed feedbackof actual brain in artificial neural networks.
Figure 2: Growing Phase3	Continual Learning Framework which Handles DelayedFeedback3.1	Basic ModelThis work introduces 2 terms seeding and growing instead of training and testing. Seeding is thephase where the initial learning occurs. This is the phase where the model takes advantage and learnsfrom labeled data. Growing is the phase where the model learns information from new unlabeleddata and updates the model. The learnt neural network of a classification problem can be seen as afunction which produces confidence values(f (X) = Y ). Initial function f is improved to f0 in theconventional training of a neural network, same thing happens here with seeding. But the function fis fixed in conventional neural networks in testing phase where as in growing phase of the continuallearning framework the model parameters are updated with the framework rules. Hence mostly anew function(f 00) will be seen after every new inputs.
Figure 3: Update rule with CBIR Features4.	If there are no samples with bhattacharrya coefficient â‰¥ 0.5, then the oldest element in thequeue is removed (dequeue operation in the queue) without any update to the model.
Figure 4: Accuracy vs Seeding-Learning SplitTest corresponds to the test accuracy of a T-DT model whereas learning corresponds to the growingaccuracy of a CL-DT. Figure 4 illustrates that when DT depth is small, the model performs betterthan T-DT. Depth 4 and 5 of CL-DT produce better results than a T-DT in most of the differentevaluating portion of the data. With increase in depth, the difference in accuracy is not significant.
