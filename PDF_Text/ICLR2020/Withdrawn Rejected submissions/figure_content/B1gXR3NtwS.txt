Figure 1: BNNs with uncertainty on the weights (left) vs. DBSN with uncertainty on the networkstructure (right) (we only depict three operations between tensors N 1 and N2 for simplicity). wand α represent network weights and network structure, respectively. In DBSN, w is also learnable.
Figure 2: Each column includes 5 samples α(i,j) from an adaptive concrete distribution with someβ(i,j) at τ = 1. Samples in every row share the same (i,j) . The base class probabilities aresoftmax(θ(i,j)) = [0.05, 0.05, 0.5, 0.4] in each sample.
Figure 3: Visualization of the segmentation and uncertainty results of DBSN on CamVid. Fromleft to right: original image, ground-truth segmentation, the estimated segmentation, and pixel-wisepredictive uncertainty. The black color in ground-truth labels represents the background (void) class.
Figure 4: Accuracy (solid) and entropy (dashed) vary w.r.t. the adversarial perturbation size onCIFAR-10 (left) and CIFAR-100 (right).
Figure 5: Empirical CDF for the entropy of the predictive distributions on SVHN dataset of modelstrained on CIFAR-10 (left) and CIFAR-100 (right). The curves that are closer to the bottom rightcorner are better.
Figure 6: Test loss (left), test error rate (middle), and test ECE (right) of DBSN vary w.r.t. thenumber of MC samples used in estimating Eq. (8). (CIFAR-10)O 20	40 CO 80 IOONumber of MC samples0	20	40	60	80	100Numberof MC samplesFigure 7: Test loss (left), test error rate (middle), and test ECE (right) of DBSN vary w.r.t. thenumber of MC samples used in estimating Eq. (8). (CIFAR-100)0	20	40 CO 80	100Number of MC samplesNaturally, we can integrate out r, and get:TK QK=1 P1/eexp(Y)βK QK=ι a(1+T/e)β—exp(γ — Kγ)Γ(K)τTK-1 QK=1 p1/eK-1 QK=1 a(1+T/e)exp(—K γ )Γ(K)((K - 1)!)TKT X	QK=ι(Piα-τ)1/e
Figure 7: Test loss (left), test error rate (middle), and test ECE (right) of DBSN vary w.r.t. thenumber of MC samples used in estimating Eq. (8). (CIFAR-100)0	20	40 CO 80	100Number of MC samplesNaturally, we can integrate out r, and get:TK QK=1 P1/eexp(Y)βK QK=ι a(1+T/e)β—exp(γ — Kγ)Γ(K)τTK-1 QK=1 p1/eK-1 QK=1 a(1+T/e)exp(—K γ )Γ(K)((K - 1)!)TKT X	QK=ι(Piα-τ)1/eβK-1Q3 a	(pκ=ι(Piα-)1/e)kThen, the log density is:log((K — 1)!) + (K — 1)log T —XXlog a + XXlogpi-βτlogαi - K * LKElOgpi-βτlogαiβ i=1	i=1	β	β= log((K — 1)!) + (K — 1) log T — XXlog ai + XX θi - [og，- K * LKEθi - [og，,β	β	i=1 β
Figure 8: Reliability diagrams for DBSN, NEK-FAC, Dropout and Fixed α on CIFAR-10 (top row)and CIFAR-100 (bottom row). The bars aligning more closely to the diagonal are preferred. SmallerECE is better.
Figure 9: Accuracy (solid) vs entropy (dashed) as a function of the adversarial perturbation size onCIFAR-10. Attack with BIM.
Figure 10: Structure of the cell learned on CIFAR-10. The pen width ofan edge implies the samplingprobability of its corresponding operation.
Figure 11: Structure of the cell learned on CIFAR-100. The pen width of an edge implies thesampling probability of its corresponding operation.
Figure 12: Structure of the cell learned on CamVid (in the downsampling path). The pen width ofan edge implies the sampling probability of its corresponding operation.
Figure 13: Structure of the cell learned on CamVid (in the upsampling path). The pen width of anedge implies the sampling probability of its corresponding operation.
