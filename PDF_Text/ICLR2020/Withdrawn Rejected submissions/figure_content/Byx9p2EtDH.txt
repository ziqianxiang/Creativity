Figure 1: Ant Example. A policy of a target agent(right) is learned by utilizing the policies of othersource agents with different leg designs (left).
Figure 2: Overview of MULTIPOLAR. We formulate a target policy πtarget with the sum of 1)the adaptive aggregation Fagg of deterministic actions from source policies L and 2) the auxiliarynetwork Faux for predicting residuals around Fagg .
Figure 3: Average Learning Curves of MLP, RPL, and MULTIPOLAR (K = 4) over all theexperiments for each environment. The shaded area represents 1 standard error.
Figure 4: Histogram of source pol-icy performances in Hopper.
Figure 5:	Histogram of final episodic rewards obtained by source policies per environment.
Figure 6:	Aggregation parameters θagg during the training of MULTIPOLAR (K = 4) in the Hopperthat has 3-dimensional actions. Here, the first two source policies are low-performing and the lasttwo are high-performing in their original environment instance.
Figure 7: Average learning curves of MULTIPOLAR with K = 4 in red, RPL in green and MLPin blue over 3 random seeds and 3 random source policy sets for all the 100 target environmentinstances of Hopper.
Figure 8: Average learning curves of MULTIPOLAR with K = 4 in red, RPL in green and MLPin blue over 3 random seeds and 3 random source policy sets for all the 100 target environmentinstances of Ant.
Figure 9: Average learning curves of MULTIPOLAR with K = 4 in red, RPL in green and MLPin blue over 3 random seeds and 3 random source policy sets for all the 100 target environmentinstances of InvertedPendulumSwingup.
