Figure 1: Illustration of class-wise self-knowledge distillation (CS-KD). We match or distill theoutput distribution of DNNs between (a) different samples with the same label and (b) augmentedsamples of the same source.
Figure 2: Visualization of features on the penultimate layer using t-SNE, from 10,000 number ofrandomly chosen training samples of CIFAR-100. Note that 20 superclasses in CIFAR-100 aredrawn by 20 different colors. (a) Cross-entropy, (b) Virtual-softmax, (c) AdaCos, (d) Maximum-entropy, (e) Mixup and (f) CS-KD (ours).
Figure 3: Reliability diagrams (DeGroot & Fienberg, 1983; Niculescu-Mizil & Caruana, 2005)which show accuracy as a function of confidence, for ResNet-18 trianed on CIFAR-100 using (a)Cross-entropy, (b) Virtual-softmax, (c) AdaCos, and (d) Maximum-entropy. All methods are com-pared with our proposed method, CS-KD.
