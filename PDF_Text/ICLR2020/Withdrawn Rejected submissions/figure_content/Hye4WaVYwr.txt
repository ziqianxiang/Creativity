Figure 1: Left: The dynam-ics of two randomly gener-ated MDPs (from the RAND,and SEMI-RAND methodsoutlined in Section 4.3 anddetailed in Appendix D.1).
Figure 2: A visualization of the dynamics, the reward function in the MDP defined in Definition 4.1,and the approximation of its optimal Q-function for the effective horizon H = 4. We can alsoconstruct slightly more involved construction with Lipschitz dynamics and very similar properties.
Figure 3: (Left): The performance of DQN, SLBO, and MBPO on the bottom dynamics in Figure 1.
Figure 4: Comparison of BOOTS-MBSAC vs MBSAC and BOOTS-SAC vs SAC on Ant and Hu-manoid. Particularly on the Humanoid environment, BOOTS improves the performance signifi-cantly. The test policies for MBSAC and SAC are the deterministic policy that takes the mean of theoutput of the policy network, because the deterministic policy performs better than the stochasticpolicy in the test time.
Figure 5: BOOTS-MBSAC or BOOTS-MBPO outperforms previous state-of-the-artalgorithms on Humanoid. The results are av-eraged over 5 random seeds and shadow areaindicates a single standard deviation from themean.
Figure 6: BOOTS with oracle dynamics on top of SAC (top) and MBSAC (bottom) on HalfCheetah,Walker, Ant and Humanoid. The solid lines are average over 5 runs, and the shadow areas indicatethe standard deviation.
Figure 7: The relative gains of BOOTS over SAC (top) and MBSAC (bottom) on HalfCheetah,Walker, Ant and Humanoid. The solid lines are average over 5 runs, and the shadow areas indicatethe standard deviation.
Figure 8: Different BOOTS planning horizon k on top of SAC (left) and MBSAC (right) on Hu-manoid. The solid lines are average over 5 runs, and the shadow areas indicate the standard devia-tion.
Figure 9: The histogram of number of pieces in optimal policy Ï€? in random method (left) andsemi-random method(right).
