Figure 1: A visualization of REMBO embeddings for two test functions. (Top left) The Braninfunction, d=2, extended to D=100. (Top right) A REMBO embedding of the D=100 Branin func-tion. (Bottom left) A center slice of the d=6 Hartmann6 function, similarly extended to D=100.
Figure 2: The probability that a randomly se-lected point in the REMBO embedding satis-fies the ambient box bounds after being pro-jected up. For de > 2, nearly all points in theembedding map outside the box bounds.
Figure 3: Probability the embedding contains an optimum (Popt) when restricted to the constraintsof (1), under a uniform prior for the location of the optima and D = 100, for three embeddingstrategies. Setting de > d rapidly increases Popt, and high probabilities can achieved with reasonablevalues of de. Hypersphere sampling produces the best embedding, particularly for d small.
Figure 5: The simulatedhexapod robot Daisy.
Figure 4: Optimization performance on three HDBO minimization problems. For each row, theleft plot shows the best value by each iteration, averaged over repeated runs. The right plot showsthe distribution of the best value at the final iteration. For all three tasks, ALEBO achieved thebest average performance, and had the lowest variance in final performance of the linear embeddingmethods.
Figure 6: Optimization performance on the D = 72 hexapod locomotion task (higher is better).
Figure 7: Three possible HeSBO embeddings of the d = 2 Branin function. (Left) The first em-bedding fully captures the function, and thus captures all three optima. (Middle) The second isrestricted to the subspace x1 = -x2. This subspace does not contain an optimum, but comes fairlyclose. (Right) The third embedding is restricted to the subspace x1 = x2 and does not come closeto any optima.
Figure 8: Test-set model predictions for three GP kernels on the same train/test data generated byevaluating the Hartmann6 D=100 function on a fixed linear embedding. A typical ARD kernel failsto learn and predicts the mean. The Mahalanobis kernel predicts well, and posterior sampling isimportant for getting reasonable predictive variance.
Figure 9: Average test-set log likelihood as a function of training set size, for training sets randomlysampled from a fixed linear embedding. Log marginal probabilities were averaged over a fixed testset of 1000 random points. For each training set size, 20 random training sets were drawn of that sizeand the figure shows the average result over those draws (with error bars for two standard errors).
Figure 10: (Left) An embedding from a N (0, 1) projection matrix on the same Branin D = 100problem from Fig. 1 subject to constraints of (1). (Right) The embedding from the same projectionmatrix after normalizing the columns to produce unit circle samples. Sampling from the unit circleincreases the probability that an optimum will fall within the embedding, and polytope bounds avoidnonlinear distortions.
Figure 11: Popt for hypersphere sampling, as estimated in Fig. 3 but here for a wider range of valuesof d and D. Contour color indicates Popt. Doubling D decreases Popt for d and de fixed, howevereven at D = 200, high values of Popt with reasonable values of de can be had for many values of d.
Figure 12: Log regret for the benchmark experiments of Fig. 4, plus Hartmann6 D=100. Each traceis the mean over repeated runs, with errors bars showing two standard errors of the mean. On thefirst three problems ALEBO performs significantly better than the other methods, and on Hartmann6D=100 it is tied with REMBO-γkΨ as the best methods.
Figure 13: ALEBO performance on the Branin problem, (Left) as a function of embedding dimen-sion de and (Right) as a function of ambient dimension D. Performance shown is the average of 50repeated runs. Optimization performance is poor with de = 2, but shows little sensitivity to de forvalues greater than 2. Optimization performance shows little sensitivity with D, all the way up toD = 1000.
Figure 14: Final best value for the Branin problem optimizations Fig. 13, as mean with error barsshowing two standard errors. With the exception of de = 2, optimization performance was goodacross a wide range of values of de and D.
