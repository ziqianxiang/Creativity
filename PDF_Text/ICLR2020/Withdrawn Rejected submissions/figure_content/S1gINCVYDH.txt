Figure 1: Reward function Rd(SO, âˆ™).
Figure 2: Training curves for Neural-PSRL and both model-free SAC (Haarnoja et al., 2018)and model-based (Janner et al., 2019) baselines on continuous control benchmarks Hopper-v3 andHalfCheetah-v3 Each environments is evaluated with the canonical 1000-step variant of the task.
Figure 3: Cumulative training curves of runs from figure 2. To maximize cumulative reward (orequivalently minimize regret), learning must quickly learn a good policy and constantly improve it.
