Figure 1: (Left) The figure shows two probability distributions, both with zero means and unitvariances. Although they have the same mean and variance, it is obvious that they are not similar;(Right) An illustration of how å¤•P reduce the skewness. The original data X (blue) is mapped to thenew data Y (orange), and the skewness is reduced.
Figure 2: Comparison of performance among (1) BNSR; (2) BN; (3) BN with noisy mean andvariance; (3) BN with noisy skewness on CIFAR-100. We show (a) the training loss; (b) the testingerror v.s. numbers of training epochs. The model is VGG-19.
Figure 3: Comparison of performance among (1) Batch Normalization with Skewness Reduction(BNSR); (2) Batch Normalization (BN); (3) Layer Normalization (LN); (4) Instance Normalization(IN); on CIFAR-100. We show (a) the training loss; (b) the testing error v.s. numbers of trainingepochs. The model is ResNet-50.
Figure 4: Comparison of performance between BNSR and BN on Tiny ImageNet dataset. We show(a) the training loss; (b) the testing error v.s. numbers of training epochs. The model is ResNet-50.
Figure 5: Comparison of performance of BNSR under different percentage of usage on CIFAR-100.
Figure 6: The histograms of the features in BN layers. (a)-(c) show the histograms of two featuresin the same layer (earlier part) at epoch = 1, 5, 15; (d)-(f) show the histograms of two features in thesame layer (later part) at epoch = 1, 5, 15.
Figure 7: The histograms of the features in BNSR layers. (a)-(c) show the histograms of twofeatures in the same layer (earlier part) at epoch = 1, 5, 15; (d)-(f) show the histograms of twofeatures in the same layer (later part) at epoch = 1, 5, 15.
