Figure 1: Architecture. The set of model parametersΘ = {θm }mM=1 and Θ0 = {θm0 }mM=1 are sampled fromthe recognition networks fη and fη0 . Given an input x ∈D, we use E to sample the latent codes z1 , ..., zM viaΘ. These codes are passed to E0 to learn their perturbedversions z1,…,ZM using Θ0. The output X 〜pφ (x01Z)is generated via posterior sampling of a z0 (in red).
Figure 2: Inversion. Process for computing the likelihoodp(D∣θ). As the decoder pφ gets accurate,the error ∣∣x 一 X∣∣2 becomes small (see Algorithm 2), and We get closer to sampling the optimal Z.
Figure 3: Invariance. Swiss Roll manifold learned with our encoder E (left), and after perturbing itselements with our encoder E0 (middle) vs. that of PGD adversarial examples (right) learned using E .
Figure 4: Inputs (left) - Adversarial exam-ples (right, inside red boxes). MNIST: (a)-(b),CelebA: (c)-(d), SVHN: (e)-(f). See AppendixA for more samples with higher resolution.
Figure 5: Marginal distributions of clean (blue) and perturbed (red) latent codes over few minibatches.
