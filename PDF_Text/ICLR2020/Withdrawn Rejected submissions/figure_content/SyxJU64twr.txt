Figure 1: Impact of single intrinsic reward value extraction for K = 2: minimum selection (min),maximum selection (max), average (avg), and 1-step surprise with ensemble (1-step).
Figure 2: Mean performance for considered sparse reward environments as a function of K. K = 0means PPO without intrinsic reward, and K = 1 means the single-model surprise method. (K = 4yielded similar performance to that of K = 3, so we omitted the curve of K = 4 for simplicity)3.3 Performance ComparisonWith the above verification, we compared the proposed method with existing intrinsic rewardgeneration methods by using PPO as the background algorithm. We considered the existing intrinsicreward generation methods: curiosity (Pathak et al., 2017), hashing (Tang et al., 2017), informationgain approximation (de Abril & Kanai, 2018), and single-model surprise (Achiam & Sastry, 2017).
Figure 3: Performance comparison.
Figure 4: Reproduced mean performance of the module method over 10 random seeds with âˆ† = 40when B = 0.
