Figure 1: Balanced training and selective drop strategy. The evaluation of each candidate onlyinfluence architecture search strategy, and paths with low performance will be gradually droppedto reduce conflicts among paths. In addition, paths still remaining are all trained with comparablefrequency to insure a fair comparison among candidate operators.
Figure 2: Weight sharing methods. In alternatively training methods, candidate paths with betterperformance get more training opportunities or higher fusion weight. In one-shot methods, sub-networks are uniformly randomly trained to convergence before architecture selection.
Figure 3: The searched architectures with different complexity limitations: BetaNet-A is searchedwith flops limitation. BetaNet-B is searched with latency limitaion.
Figure 4: BetaNet-A is compared with MobileNetV2 (Sandler et al., 2018) ProxyLessNAS Cai et al.
Figure 5: The variation of α and training frequency of 2 choice blocks from each method are shownin (a) and (b). From left to right in each line: α in block 1, paths training frequency in block 1, α inblock 2, paths training frequency in block 2.
Figure 6: Comparison between architectures searched with different random seeds and randomlysampled ones. Architectures searched with our method and with random policy are shown with solidlines and dotted lines respectively.
Figure 7: 4 Networks are trained in 3 groups with different training policies. 00, 01, 10, 11 representarchitectures conv1-conv1, conv1-conv3, conv3-conv1, conv3-conv3 respectively.
