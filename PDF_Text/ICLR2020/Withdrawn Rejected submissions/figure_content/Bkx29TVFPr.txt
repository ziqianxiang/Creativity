Figure 1: (a) shows a dataset generated by uniformly sampling x ∈ (-1, 1), conditioned which y issampled from 0.5N(√1 - x2, 0.12)+0.5N(-√1 - x2, 0.12). (b) shows the conditional distributionwhen x = 0.
Figure 2: (a)(b) shows learning curves on single-circle and double-circle datasets respectively. In thesingle-circle problem, MDN uses 3 mixture components for learning; on the double-circle datasetand the high dimensional variant, it uses 4 components to learn. All results are averaged over 30random seeds and the shaded area indicates standard error.
Figure 4: (a)(b) shows the empirical density of fθ(x, y) for (x, y)s in the training set S when thealgorithm is trained by not adding noise to target and adding noise (standard deviation σ = 0.1)respectively.
Figure 3: (a)(b) shows the predictions of both our algorithm and MDN on the single circle dataset.
Figure 5: Figure (a)(b)(c) show performances of Implicit (red) and l2 regression L2 (black)objective as we increase the Gaussian noise variance. We show the testing error measured by RMSEon entire testing set (solid line), on high frequency region (i.e. x ∈ [-2.5, 0.0), dashed line) and onlow frequency region (x ∈ [0.0, 2.5], dotted line). The results are averaged over 30 random seeds.
Figure 6: (a) Approximated functions and true function. (b) The distance matrix showed in heat mapcomputed by hidden layer representation learned by L2 (left) and Implicit (right) method.
Figure 7: Figure (a) shows what the training data looks like. (b) shows the predictions of our implicitlearning approach.
Figure 8: We show the learning curves of mixture density network with different number ofcomponents. In (a) and (b), the learning curves of component 2 are out of figure. For each number ofcomponent, we choose the learning curve by optimizing other parameter (i.e. stepsize). The resultsare averaged over 30 random seeds.
Figure 9: Best parameter setting for reproducing experiments.
Figure 10: In (a) we repeat the figure shown in previousB.1.3	Experiments on real world datasetsThe bike sharing dataset (Fanaee-T & Gama, 2013) (https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset) and song year dataset Bertin-Mahieux et al. (2011) (https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd) information are presented in figure 11. Note that the two datasets havevery different target distributions as shown in figure 12. On the two dataset, we use 64 × 64 tanhhidden units and sweep over learning rate from {0.01, 0.001, 0.0001}. For the l2 regression, wefound that using tanh unit works better than relu.
Figure 11:	Data preprocessing information.
Figure 12:	Bike sharing targets show a clear Poisson distribution while song year dataset’s targetdistribution is not intuitive.
