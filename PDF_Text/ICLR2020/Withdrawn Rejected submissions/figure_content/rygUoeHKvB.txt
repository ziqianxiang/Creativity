Figure 1: Illustration for the proposed method. A goal-conditioned policy firstly reaches the explo-ration boundary, then perform random actions to discover new states.
Figure 2: Histograms for normalized state visitation counts, where the x-axis represents the indexof state. Top row: directly maximizing the entropy of empirical state distribution over visited states;Bottom row: firstly maximizing the counting measure of induced state distribution support, thenmaximizing the entropy of state distribution with full support.
Figure 3: Illustration of four environments considered in this paper.
Figure 4: Comparison of goal-selection. Figure 5: Comparison of training techniques.
Figure 6: Average episode returns over 5 seeds on the Empty Room, Four Rooms and FetchReachenvironments. Shadows indicate the standard deviation.
Figure 7: Average episode returns over 3 seeds on SuperMarioBros. Shadows indicate the standarddeviation.
Figure 8: Trajectory visualization on SuperMarioBros-1-3. Trajectories are plotted in green cycleswith the same number samples (18M). The agent starts from the most left part and needs to fetch theflag on the most right part. Top row: vanilla; middle row: bonus; bottom row: novelty-pursuit.
Figure 9: Visualization for the exploration boundary given by visitation counts and the estimated viaprediction errors of RND.
Figure 10: Trajectory visualization. For each figure, toP row: vanilla (ACER); middle row: bonus;bottom role: novelty-Pursuit (ours). The vanilla method gets stuck into the local oPtimum evenwith Policy entroPy regularization on SuPerMarioBros-1-1. Only our method can get the flag onSuPerMarioBros-1-2.
