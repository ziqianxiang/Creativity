Figure 1: InfoCNF With CNN gates that learn the tolerances of the ODE solvers using reinforcement learning.
Figure 2: (a) Log plot that shows that the density under the model does integrate to one given sufficiently lowtolerance. (b) Distribution of error tolerances learned by InfoCNF at each layer over CIFAR10 train and test sets.
Figure 4: (a) Test NLLs, (b) NFEs ofCNF with/WithOUt learned tolerancesin small-batch training on CIFAR10.
Figure 3: Evolution of test errors, test NLLs, and NFEs from the ODE solvers during the training of InfoCNFwith learned tolerances vs. InfoCNF with fixed tolerances vs. CCNF (baseline) on CIFAR10 using small batchsize (top) and large batch size (bottom). Each experiment is averaged over 3 runs.
Figure 5: (a) Test errors, (b) test NLLs, and (c) NFES of InfoCNF and CCNF trained on CIFAR10 using largebatch size with and without large learning rate and manually-tuned error tolerance.
Figure 6: Trajectories learned by the LatentODE (a)without and (b) with the conditioning via partitioningthe latent code.
Figure 7:	MNIST images generated by InfoCNF.
Figure 8:	Distribution from which we sample the 1-D synthetic data for computing the value of error tolerancesused to evaluate the trained InfoCNF (blue curve) and the distribution learned by InfoCNF (orange curve)D	Evaluating the Trained InfoCNF Using the Learned ErrorTolerancesWe explore if the error tolerances computed from batches of input data can be used to evaluate thetrained InfoCNF. First, we repeat the experiment on 1-D synthetic data described in Section 4.2 andAppendix C above using the learned error tolerances instead of the fixed error tolerances. We observethat the numerical error in this case is 0.00014, which is small enough. We further use the learnederror tolerances to evaluate our trained InfoCNF on CIFAR10 with small-batches. Distribution ofthese error tolerances at different layers can be found in Figure 2b. The test error and NLL we obtainare 20.85 ± 1.48 and 3.566 ± 0.003, which are close enough to the results obtained when settingthe error tolerances to 10-5 (test error and NLL in this case are 20.99 ± 0.67 and 3.568 ± 0.003,respectively, as shown in Table 1). Furthermore, when using the learned error tolerances to evaluatethe trained model, we observe that the trained InfoCNF achieves similar classification errors, negativelog-likelihoods (NLLs), and number of function evaluations (NFEs) with various small values for thebatch size (e.g. 1, 500, 900, 1000, 2000). However, when we use large batch sizes for evaluation (e.g.
Figure 9: Ground truth spirals: One spiral is clockwise and another is counter-clockwise.
Figure 10: Evolution of test classification errors, test negative log-likelihoods, and the number of functionevaluations in the ODE solvers during the training of InfoCNF with learned tolerances (blue) vs. InfoCNF withmanually-tuned error tolerance (yellow) vs. InfoCNF with tolerances=10-5 (green) vs. CCNF (red, baseline) onCIFAR10 using small batch size. Each experiment is averaged over 3 runs.
Figure 11: Evolution of test marginal negative log-likelihoods during the training of InfoCNF with learnedtolerances (blue) vs. InfoCNF with fixed tolerances (green) vs. CCNF (red, baseline) on CIFAR10 using smallbatches of size 900 (left) and large batches of size 8,000 (right). Each experiment is averaged over 3 runs.
Figure 12: Test marginal NLLS of InfoCNF and CCNF trained on CIFAR10 using large batch size with andwithout large learning rate and manually-tuned error tolerance.
Figure 13: (a) Test error and (b) NLLs vs. NFEs during training of CNF and InfoCNF (with learned tolerances)on CIFAR10.
Figure 14: Evolution of test classification errors, test negative log-likelihoods, and the number of functionevaluations in the ODE solvers during the training of the large size InfOCNF with 2x more flows per scaleblock and learned tolerances (brown) vs. InfoCNF with learned tolerances (blue) vs. CCNF (red, baseline) onCIFAR10 using large batchs of size 8,000.
