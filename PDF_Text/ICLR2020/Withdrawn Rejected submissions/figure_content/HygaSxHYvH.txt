Figure 1: Visualization of an MTM model during training. The architecture is equivalent to a stan-dard Transformer encoder with bidirectional self-attention. Additionally, the corruption decisionsfor each position are illustrated in an oval above the model input. The corrupted positions M areindicated before the softmax layer by a filled diamond. Non-corrupted positions (blank diamond)are not predicted in training.
Figure 2: Example of the MTM corruption process on the target input sentence “Thanks forreading this" with resulting loss P4=2 logpθ(e∕fJ, e4).
Figure 3: Average value of the log-probability over newsdev2016 for each decoding iteration i.e.
Figure 4: Comparison of different decoding strategies for the MTM for different amounts of decod-ing iterations T (on newsdev2016).
