Figure 1: The Four Demands example. (A) An agent populates a 6x6 environment separated into four rooms.
Figure 2: (A) The architecture of the 3-layer fully connected network computing the Q-function Q(a∣~, ~). (B)The average reward per time step received by the network trained with maximum allowed motivation value θ(blue circles - motivation is provided as an input to the network; yellow - no motivation; orange - randomwalk). For small θs, the motivated network displays two-room binge behavior, while for larger θs, migrationdominates. Under the same conditions, the non-motivated network mostly displays two-room binge behavior.
Figure 3: Addiction model: (A) addiction motivation schedule. In the first three rooms, motivations are allowedto grow up to the value of 1. In the fourth (”smoking” room), the motivation may grow to the value of 10. (B)Strategy learned by the network: spend 5 steps in the room #2, then visit the room #4 (”smoking”)smokingDoes motivation contribute to learning optimal strategies? To address this question, we performeda similar set of simulations, except the motivation input to the network was suppressed (μ = 0).
Figure 4:	The transport network example. An agent (black dot) navigates in a network of roads connectingthe cities - each associated With its own binary motivation. The perceived reward is equal to the value of themotivation vector μ at the position of the agent, less the distance traveled. When the agent visits a city withnon-zero motivation (red circle), the motivation toward this city is reset to zero. The task continues until μ = ~.
Figure 5:	Training a neural networks to find the shortest route to visit a subset of target cities using the motiva-tion framework. (A) One-layer neural network computing the Q-values. (B) Test performance on 50 sets of 3random targets (blue). The precomputed shortest path solutions (red). The policy converges on iteration 3 ∙ 103.
Figure 6: (A, B) A behavioral task for the recording. Trials were separated into the blocks during which onlynegative rewards (air puffs) or only positive rewards (water drops) were delivered. (C) Responses of neuronsrecorded in 3 mice clustered into the negative motivation, positive motivation, and neurons of mixed sensitivity.
Figure 7: (A) The architecture of the RNN computing the Q-function in the Pavlovian conditioning task. (B)Inputs and outputs of the RNN for each trial type. Inputs: motivation, cue, reward. Outputs: sample V-function. Bottom row: TD error. Trial types (left to right): large negative reward, small negative reward, nonegative reward, no positive reward, small positive reward, large positive reward. (C) Sample responses ofneurons in the RNN clustered into the negative motivation (blue), positive motivation (red) and neurons ofmixed sensitivity (other). (D) Recurrent connectivity matrix. (E) t-SNE representation. (F) Push-pull circuit.
