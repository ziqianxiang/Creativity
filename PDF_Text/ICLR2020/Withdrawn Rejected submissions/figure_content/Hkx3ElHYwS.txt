Figure 1: Architecture of the proposed GQ-Net. Input x0 follows the top and bottom paths toproduce the full precision and quantized outputs XL and Xl, resp. These are combined through lossfunctions Lf and Lq to form the overall loss L, which is optimized by backpropagation. For moredetails please refer to Section 3.
Figure 2: (a) Schedule for learning rate, ωf and ωq . x-axis shows the number of train-ing steps, y-axis shows the value of the learning rate or loss weight. (b) Distribution oflayer2.0.downsample.0 convolution layer outputs in full precision (blue) and 4-bit quan-tized (orange) ResNet-18, showing the necessity of multi-domain BN. Note that outputs are notquantized until non-linear layers, thus the quantized distribution has more than 16 bins.
