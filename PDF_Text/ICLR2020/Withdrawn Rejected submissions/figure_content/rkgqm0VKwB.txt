Figure 1: Joint named entity recognition (NER) and relation extraction (RE) model architecture.
Figure 2: Visualization of the attention weights from select layers and heads of BERT after itwas fine-tuned within our model on the CoNLL04 corpus. Darker squares indicate larger atten-tion weights. Attention weights are shown for the input sentence: "Ruby fatally shot Oswald twodays after Kennedy was assassinated.‚Äù. The CLS and SEP tokens have been removed. Four majorpatterns are displayed: paying attention to the next word (first image from the left) and previousword (second from the left), paying attention to the word itself (third from the left) and the end ofthe sentence (fourth from the left).
