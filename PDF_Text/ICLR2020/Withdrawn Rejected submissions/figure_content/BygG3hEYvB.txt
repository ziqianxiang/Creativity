Figure 1: Coherency of our metrics. We show saliency maps of intermediate-layer features yieldedby different baselines. (bottom left) We also compare layerwise magnitudes of different results. Ourmetrics measure how input information is gradually discarded through layers. (top left) Our metricsprovide consistent measures of information discarding that enabled faithful comparisons over layersand networks. (right) We visualize pixel-level SID and RU using Hi(σi) and Hi(σ), respectively.
Figure 2: Overview of the algorithm. Given a trained DNN, we compute the maximal entropy ofthe input H(Xc) and the maximal entropy of image reconstruction H(Xc), when We constraint theintermediate-layer feature f within the small range of the concept of a specific object.
Figure 3:	Layerwise strict information discarding, reconstruction uncertainty, and concentration.
Figure 4:	Diagnosis of architectural revision (a) and network compression (b). Values were normal-ized by the pixel number per image and averaged over images.
Figure 5:	Effects of knowledge distillation and learning epochs. (a) We compared layerwise in-formation discarding between DNNs learned with and without distillations. (b) Each curve showsinformation discarding of the output of a specific block during the learning process.
