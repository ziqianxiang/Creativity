Figure 1: a) Comparing the standard monolingual approach, a naive multilingual approach thataggregates examples in various languages in a way that each example is solely in one language, andcross-lingual data augmentation (XLDA). Prediction is always in a single language. b) Two examplesof XLDA inputs using the XNLI dataset.
Figure 2: a) A comprehensive pairwise evaluation demonstrates that all languages have a cross-lingualaugmentor that improves over monolingual training. Rows correspond to the evaluated language.
Figure 3: Greedily adding cross-lingual augmentors to BERTML based on the pairwise evaluation.
Figure 4: Both cross-lingual data augmentation (XLDA) and disjoint, multilingual training (DMT,see Section 3) improve over monolingual training, but XLDA provides greater improvements as moreaugmentors are used.
Figure 5: a) XLDA is on average even more effective for a given language when used on randomlyinitialized models compared to large, pretrained ones. b) Greedily adding cross-lingual augmentorsto an LSTM baseline.
