Figure 1:  EB-M (with metric dJS) comparison for MLE and non-MLE training on EMNLP-Newsdata.  For each training method, we show corpus-BLEU (Yu et al., 2016) measurement using thetest-set as reference set in the legend.
Figure 2: EB-C measurement for the LSTM-32 and LSTM-512 model with different metrics. Alsoaverage value of EB-C along all history length is shown in the legend.
Figure 3: CGD measurement for corrupted PM (with dT V ) for the LSTM-512 synthetic experiment.
Figure 4:  (a):  EB-C measurements (with dJS) for comparing non-MLE methods in the LSTM-512 synthetic experiment.   (b):  EB-C measurements for comparing RankGAN and MLE for theMʳᵃⁿᵈᵒᵐ synthetic experiment, the metric used is dJS.
Figure 5: EB-M measurements (with metric dJS) using different number of samples on wikitext-103data.
Figure 6: The HIT interface for our evaluation.
Figure 7: A histogram of the number of HITs done by a unique turker.
Figure 8: EB-C measurements with different number of samples for the LSTM-512 synthetic exper-iment.
Figure 9:  EB-M measurements (with metric dT V ) using different number of samples on wiki103data.
Figure 10: EB-M (with metric dT V ) comparison for MLE and non-MLE training on EMNLP-Newsdata. For each training method, we show corpus-BLEU measurement using the test-set as referenceset in the legend.
Figure 11: EB-C measures for the transformer LM.
