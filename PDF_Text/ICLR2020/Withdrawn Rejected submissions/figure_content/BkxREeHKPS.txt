Figure 1: Approaches to variational Bayes on Bayesianneural networks, ordered by i) whether they factorize thevariational distribution q, and ii) whether they tie the vari-ational parameters.
Figure 2: Explained variance per singular value from SVD of matrices of posterior means and posterior stan-dard deviations for different layers of three types of models trained using standard GMFVI: MLP (left), CNN(center), LSTM (right). Posterior standard deviations clearly display strong low-rank structure, with most ofthe variance contained in the top few singular values, while this is not the case for posterior means.
Figure 3: Unlike posterior means, the pos-terior standard deviations of both denseand convolutional layers in the ResNetmodel trained using standard GMFVI dis-play strong low-rank structure and can beapproximated without loss in predictive met-rics. Top: Explained variance per singularvalue of the matrices of converged posteriormeans and standard deviations. Bottom: Im-pact of post training low-rank approxima-tion of the posterior standard deviation ma-trices on model’s performance. We reportmean and SEM of each metric across 100models samples.
Figure 4: Left: impact of the k-tied Normal posterior on test ELBO, test predictive performance and number ofmodel parameters. Test performance is reported as a mean and SEM across 100 weights samples after trainingeach model for ≈300 epochs. Right top: mean gradient SNR in the Dense 2 layer of the MNIST MLP modelat increasing training steps for different ranks of tying k. We observe a similar increase in the SNR from tyingfor the CNN and the LSTM models as for the MLP model shown here. We report mean and SEM across 3training runs with different random seeds. Right bottom: Negative ELBO on the MNIST validation data setat increasing training steps for different ranks of tying k. See also Figure 5, which shows negative ELBOconvergence plots for the all three models types.
Figure 5: Convergence of negative ELBO (lower is better) reported for validation dataset when training Withtied variational posterior standard deviations for MLP (left), CNN (center), and LSTM (right) with differentlow-rank factorizations of the posterior standard deviation matrix. Full-rank is the standard parametrization ofthe GMFVI.
Figure 6: Illustration of the difference between the k-tied Normal (green), the MN distribution (red), theGaussian mean field (blue) and the full Gaussian covariance (black) for a matrix of posterior standard devia-tions for a layer of size m × n. k-tied Normal with k = 1 is equivalent to MN with diagonal row and columncovariance matrices (half-red, half-green circle). Our experiments show that the k = 1 fails to capture the per-formance of the mean field. On the other hand, while the full/non-diagonal MN increases the expressivenessof the posterior, it also increases the number of parameters. In contrast, k-tied Normal with k ≥ 2 not onlydecreases the number of parameters, but also matches the predictive performance of the mean field.
Figure 7:	Heat maps of the partially flattened posterior standard deviation tensors for the selected convolutionallayers of the ResNet-18 GMFVI BNN trained on CIFAR-10. The partially flattened posterior standard deviationtensors of the convolutional layers display similar low-rank patterns that we observe for the dense layers.
Figure 8:	Explained variance per singular value from SVD of partially flattened tensors of posterior meansand posterior standard deviations for different convolutional layers of the ResNet-18 GMFVI BNN trained onCIFAR-10. Posterior standard deviations clearly display strong low-rank structure, with most of the variancecontained in the top few singular values, while this is not the case for posterior means.
Figure 9: Heat map of the posterior standard deviation matrix for the weights in the first dense layer of a LeNetCNN trained using GMFVI on the CIFAR-100 dataset (left), as well as its rank-1 approximation (middle)and rank-2 approximation (right). The rank-2 approximation looks visually similar to the full-rank matrix,confirming our numerical results from Figure 2.
