Figure 1: Overview of our training pipeline. Stage 1: pretraining the style encoder E using a tripletloss. Stages 2, 3: training the generator G, and finetuning both G, E together using GAN andreconstruction losses.
Figure 2: Qualitative comparison with baselines. Ours better matches the ground truth (GT) style.
Figure 3: Style transfer for different datasets. For each dataset, we show output for applying differentstyles to the same input image.
Figure 4: Style interpolation. Left column is the input to the generator G, second and last columnsare input style images to the style encoder, and the middle images are linear interpolation in theembedding space.
Figure 5: Style sampling for different datasets using our approach v3. We sample either from N(μ, σ),where μ, σ are computed from the train set (middle), or using the mapper network M (right).
Figure 6: t-SNE plots for the latent style space learned by the style encoder E (a) after stylepretraining, (b) after finetuning, and (c) using the BicycleGAN v1 baseline.
Figure 7: t-SNE plot for the pre-trained latent space learned for facial expressions on a subset of theKDEF dataset.
Figure 8: Emotion translation results. First row shows the input image, as well as the ground truthimages from which we encode the latent emotion vector for reconstruction. Our staged trainingapproach is able to achieve multi-modal synthesis, while the baselines collapse to a single mode.
Figure 9: Sixteen randomly sampled styles using both the mapper network M (left), as well as adhocsampling from the empirically computed N(μ, σ) distribution of a L2-regularized latent space (right).
Figure 10: Convergence comparison between the proposed staged training (ours - v3) and theBicycleGAN baselines measured by the reconstruction error (LPIPS) of the validation set of theedges2handbags dataset. Dotted line shows the transition between stages 2 and 3 of our training (i.e,switching from a fixed E to finetuning both G and E together).
