Figure 1: Diagram of the contest design process (Algorithm 2). Step 1) Simulate contests and agentlearning to determine the utility of possible designs. Each data point represents an (contest design,utility) pair. Step 2) Fit a deep network to predict utility given contest design. Step 3) Optimize theoutput (utility) over the input (contest design) of the deep network to find the optimal design.
Figure 2: Bidding strategy CDFs. The top left plot shows the CDFs for each of five different contestdesigns assuming the bidder plays the Nash distribution. The first prize is listed in the legend; thesecond prize equals one minus the first; no prize is given to the third bidder. The remaining plotscompare the Nash CDF with the CDF learned by fictitious self-play for different first prize amounts.
Figure 3: Principal’s utility as a function of thefirst rank prize, as given by the analytical NashCDFs, the simulation data, and the trained neuralnetwork. Markers denote maxima.
Figure 4: Heatmap for the principal’s utility forvarious four bidder contest designs. The starmarks the optimal design.
Figure 5: Heatmaps for the principal’s utility in four bidder contests with realized bids drawn froman interval [bid-d,bid+d] either (L) uniformly or (R) according to Beta( 1, 1), d = 0.06. Plotted ontop of the heatmap are arrows to the optimal designs for different values of d annotated on the map.
Figure 6: First three prizes of the optimized tenbidder contest plotted on top a simplex for refer-ence. Each point denotes the optimal design fora different noise level. The square marks zeronoise with the trajectory ending at the star withrealized bids drawn uniformly from bid ±0.06.
Figure 7: Utility for the ten bidder contest vs the top four prizes with realized bids drawn uniformlyfrom bid ±.06. Note plots are shown on different scales and for different slices of the design space.
Figure 8: This figure compares the bidder CDFs. The top left plot shows the CDFs for each of fivedifferent auction designs assuming the bidder plays the Nash distribution. The first prize is listedin the legend; the second prize equals one minus the first; no prize is given to the third bidder. Theremaining plots compare the Nash CDF with the CDF learned by fictitious self-play for different firstprize amounts. Figures (b-d) also show the effect of training iterations and discretization granularityon the final CDF. The arrow in Figure (c) highlights a common trend where the CDF convergesto Nash from above as a finer discretization is introduced. In other words, coarser discretizationslead to under bidding and, in turn, underestimates of the auctioneer utility. Figure (d) shows thatincreasing training iterations reduces error, but in a less structured manner than bid granularity.
Figure 9: This figure compares the bidder CDFs. The top left plot shows the CDFs for each offive different auction designs assuming the bidder plays the Nash distribution. The first prize islisted in the legend; the second prize equals one minus the first; no prize is given to the third bid-der. The remaining plots compare the Nash CDF with the CDFs learned by fictitious self-play andREINFORCE respectively using their optimal hyperparameter configurations (iterations, bid levels,learning rate, batch size). Both REINFORCE and fictitious self-play agree closely with the Nashequilibrium when the first prize ≥ 0.8. For smaller prizes, there is a larger discrepancy. We see thatFP is consistently closer to the Nash equilibrium than REINFORCE.
Figure 10: The principal’s utility function: max(log(a(q - b)), 0).
