Figure 1: Comparisons between Single model, Mixture of Experts (MoE)(Jacobs et al., 1991), andAttention over Parameters (AoP).
Figure 3: Results for the Persona-Chat dataset.
Figure 4: Positional Embedding of the dialoguehistory and the memory content.
Figure 5: Attention over Parameters visualization, vector Î± for different reference (Ref.) and AoPgenerated answers. ToP rows (Usr) are the last utterances from each dialogue contexts.
Figure 6: Mixture of Experts (MoE) (Shazeer et al., 2017) model consist of r feed-forward neuralnetwork (experts) which are embedded between two LSTM layers, a trainable gating network toselect experts.
Figure 7: Attention over Representation (AoR) consist of a transformer encoder which encode thesource input and compute the attention over the skills. Then r transformer decoder layers computesr specialized representation and the output response is generated based on the weighted sum therepresentation. In the figure, we omitted the output layer.
Figure 8: Attention over Parameters (AoP) consist of a transformer encoder which encode the sourceinput and compute the attention over the skills. Then, r specialized transformer decoder layersand a dummy transformer decoder layer parameterized by the weighted sum of the r specializedtransformer decoder layers parameters. In the figure, we omitted the output layer.
