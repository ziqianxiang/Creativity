Figure 1:	Our low-precision DNN compression pipeline. We utilize partial L2 regularization andMSQE regularization to transform a pre-trained high-precision model into a sparse low-precisionmodel with fixed-point weights and activations. The low-precision weights are further compressedin size with lossless entropy source coding.
Figure 2:	Low-precision convolutional layer using fixed-point (FXP) convolution and bias addition.
Figure 3: Weight histogram snapshots of the MNIST LeNet-5 second convolutional layer capturedat different training batch iteration numbers while a pre-trained model is quantized to have 4-bitweights and activations with the proposed regularization method.
Figure 4: ResNet-18 model training convergence curves for binary weights and 2-bit activations.
Figure 5: Weight histogram snapshots of the MNIST LeNet-5 at different training batch iterationnumbers when trained from scratch with the partial L2 regularizer for 90% sparsity (r = 90).
Figure 6: Ablation study of ResNet-18 quantization on ImageNet classification. We use “W: Weightprecision” and “A: Activation precision” to denote weight and activation precisions, respectively.
Figure 7: Comparison of our low-precision MobileNet and ShuffleNet compression results to theones of the state-of-the-art network compression methods on ImageNet classification. We use “W:Weight precision” and “A: Activation precision” to denote weight and activation precisions used inthe compressed models, respectively.
