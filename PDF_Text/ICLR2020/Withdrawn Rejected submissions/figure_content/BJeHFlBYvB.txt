Figure 1: BERT + SGM. An overview of the model.
Figure 2: An example of a subtree of the tree hierarchy over classes in Y.Taxi Riders dataset.
Figure 3: Performance of BERT and BERT+SGM on Reuters-21578 validation set during training.
Figure 4: Hamming accuracy, set accuracy, miF1, and maF1 metrics of the mixed model on RCV1-v2 validation set.
Figure 5:	Visualization of feature importance for multi-label BERT and BERT+SGM models trainedon AAPD and applied to BERT paper (Devlin et al., 2018) abstract (cs.LG - machine learning;cs.CL - computation & linguistics; cs.NE - neural and evolutionary computing).
Figure 6:	Projection of label embeddings obtained from the fully connected classification layer ofmulti-label BERT fine-tuned on AAPD dataset.
