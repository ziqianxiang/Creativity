Figure 1: Comparison of MPQ against baselines during training. Baselines are MPPI and S OFTQLEARNING. The number following H inthe legend corresponds to horizon for MPC optimization. SOFTQ is equivalent to MPQ with H = 1 (a) In the PENDULUMSWINGUP tasksMPQ with H = 8 is able to outperform MPPI with H = 32 due to the Q function adding global information and correcting for modelbias. (b) Due to the sparse nature of BALLINCUPSPARSE task, both SOFTQLEARNING and MPPI with long horizon of H = 48 are unableto succeed consistently, whereas the learner is able to outperform them after only a few episodes of training. (c) In the FetchPushBlocktask, MPQ with H = 10 is able to out-perform MPPI using H = 64 in merely few minutes of real system interaction. (d) Similarlyin FRANKADRAWEROPEN, MPQ with H = 10, considerably outperforms MPPI with H=10 and H=64 within a few episodes of trainingdemonstrating scalability to high-dimensional problems. MPQ beats SoftQLearning baseline in all tasks.
