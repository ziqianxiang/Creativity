Figure 1: Diffusion curves for model weights (calculated as Euclidean distance between weights vector atinitial time t0 and the vector at epoch t) for replicas of the model at different rate of Dropout, learning rate andL2 regularization. See discussion in the main text for details.
Figure 2: Weight diffusion curves with andwithout Batch Normalization(BN)comparing training with and without BN, it can be immediately shown to improve weight diffusion(see Fig.2).
Figure 3: Test error curves for LeNet-like architecture for replica exchange training over Dropout p and learn-ing rate Î³ . In the upper/lower row results for EMNIST / CIFAR-10 datasets. The dark lines are runningaverages.
Figure 4: Left: Example of exchanges between replicas differing in learning rates, for the ResNet architecture.
Figure 5: The training dynamics of the algorithm. The model replicas at different values of hyperparameterscorrespond to different fictitious temperatures Ti . At every temperature the algorithm performs standard SGDoptimization (as opposed to Markov Chain Monte Carlo dynamics typically used in physical simulations),which is depicted with green trajectories. Periodically, swaps between neighbouring temperatures are proposed.
