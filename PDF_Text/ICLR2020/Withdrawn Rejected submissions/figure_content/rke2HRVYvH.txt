Figure 1: (a) Illustration of the stochastic prototype embedding. The model learns a mapping frominput space, x, to embedding space, z, in which same-class instances are near and different-classinstances are far. Embeddings are represented as Gaussian random variables. Prototypes, noted as+ symbols in the embedding, are formed via a confidence-weighted average of the embeddings ofinstances known to belong to a class (support instances). Prototype uncertainty is depicted withthe dotted ovals. Given the prototypes, a prediction of class y is made for a query instance bymarginalizing a softmax prediction over the embedding space. (b) Depiction of intersection sampler.
Figure 2: (a) Samples from the four classes in our synthetic data set. In each plot, class centroidsare circled, along with samples spanning ±2 standard deviations in both orientation and color. Asample,s transparency is set according to its class-conditional likelihood. Both dimensions can becoded as directional variables. The class centroids on each dimension are 90° apart with standarddeviation of 30°. (b) A set of examples, with the four class centroids located in the corners and otherexamples obtained by linear interpolation in the generative space. (c) The 2D stochastic prototypeembedding for the examples in (b). The shape is plotted at the mean of p(z|x), and the outlines ofthe ovals represent equiprobability contours at 0.4 standard deviations.
Figure 3:	Syntheticdata set: uncertaintyon the two embed-ding dimensions as itbecomes more diffi-cult to discern the hue(left) and orientation(right).
Figure 4:	Test classification accuracy as afunction of number of training samples perquery instance for a naive-sampling andintersection-sampling 2D SPE on a I-Shot,20-class Omniglot task. Performance isa mean over 5 replications of running themodel, showing ±1 standard error of themean.
Figure 5:	Two-dimensional em-bedding learned by the SPE onthe Omniglot test set. Each squarethumbnail image in the figure is arandomly-sampled instance fromone of 125 randomly-sampled testclasses and the location of the im-age represents the location of theclass prototype. The images havea gray bounding box for visualiza-tion purposes only.
Figure 6:	(a) Comparison of few-shot accuracy on Omniglot test classes for the PN (Snell et al., 2017)and our SPE.(b) Comparison of test accuracy on seen classes for 2 and 3-digit MNIST for HIB (Ohet al., 2019) and our SPE. (c) Same as (b) except for unseen classes. In (a)-(c), error bars reflect ±1standard error of the mean, corrected to remove cross-condition variance (Masson and Loftus, 2003).
Figure 7:	Two-dimensional embedding learned by the SPE on the 2-digit MNIST test set. A class isspecified by a two-digit number. In both figures, the location of the class corresponds to the mean ofthe prototype in the test set using 140 support instances. The digits surrounded by a black border areclasses that were not seen during training. In the left and right figures, the prototypes are coloredaccording to the first and second digit of the class, respectively.
Figure 8: Examples of occluded 2-digit se-quences. occlusion is based on random rectan-gles that black out portions of each digit.
Figure 9: Two-dimensional embedding learned by the PN on the Omniglot test Set Each squarethumbnail image in the figure is a randomly-sampled instance from one of 125 randomly-sampledtest classes and the location of the image represents the location of the class prototype. The imageshave a gray bounding box for visualization purposes only.
Figure 10: Two-dimensional embedding learned by the PN on the 2-digit MNIST test set. A class isspecified by a two-digit number. In both figures, the location of the class corresponds to the mean ofthe prototype in the test set using 140 support instances. The digits surrounded by a black border areclasses that were not seen during training. In the left and right figures, the prototypes are coloredaccording to the first and second digit of the class, respectively.
Figure 11: Chain graph representing SPEThe model proposed by Fort (2017) shares with SPE the notion that prototypes are a confidence-weighted average of support instances (Equation 5), and the embedding procedure produces a scalingmatrix for computing a Mahalanobis distance, similar to the Gaussian covariance matrix of SPE.
