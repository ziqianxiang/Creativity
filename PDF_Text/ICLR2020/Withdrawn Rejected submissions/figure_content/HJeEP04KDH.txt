Figure 1: Exploration with different training processes and rewards achieved by corresponding mod-els. Lower variance in inferred action distribution implies higher exploration. Training with higherquantization levels, like layer norm regularization, induces lower action distribution variance andthus higher exploration.   Reward plot indicates that training with quantization achieves a similarlevel of rewards despite more exploration.  Note that quantization during training is turned on after5,000,000 steps (quant delay = 5,000,000) and the differences manifest shortly after this point.
Figure 2: Quantization aware training (QAT) of PPO, A2C, and DDPG algorithms on OpenAI gym,Atari, and PyBullet. FP is achieved by fp32 and 8* is achieved by 8-bit post-training quantization.
Figure 3:  Weight distribution and cor-responding   8-bit   quantized   error   formodels   trained   on   the   Breakout,Beamrider and  Pong environmentswith DQN.
Figure  4:  Weight  distributions  for  the  policies  trained  using  DQN,  PPO  and  A2C.  DQN  policyweights are more spread out and more difficult to cover effectively by 8-bit quantization (yellowlines).  This explains the higher quantization error for DQN in Table 3.  A negative error indicatesthat the quantized model outperformed the full precision baseline.
Figure 5: Mixed precision v/s fp32 training rewards.
Figure 6: Table lists the inference speed in milliseconds (ms) on Ras-Pi3b+ and success rate (%) forthree policies. The figure shows the memory consumption for Policy IIIâ€™s fp-32 and int8 policies.
Figure 7: Post training quantization sweet spot for DQN MsPacman, DQN SeaQuest, DQN Break-out.  We see that post-training quantization sweet spot depends on the specific task at hand.  Notethat 16-bit in this plot is 16-bit affine quantization, not fp16.
