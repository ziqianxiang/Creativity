Figure 1: Geometric illustration of the translation terms considered in MDEthe score for all the positive samples become less than a fixed limit. Sun et al. (2018) extends thelimit-based loss so that the score of the negative samples become greater than a fixed limit. We trainour model with the same loss function which is:loss = β1 X [f(τ) - γ1]+ + β2 X [γ2 - f (τ0)]+	(6)τ∈T+	τ0∈T-where [.]+ = max(., 0), γ1,γ2 ∈ R+. T+, T- are the set of positive and negative samples andβ1 , β2 > 0 are constants denoting the importance of the positive and negative samples. This ver-sion of limit-based loss minimizes the aggregated error such that the score for the positive samplesbecome less than γ1 and the score for negative samples become greater than γ2 . To find the op-timal limits for the limit-based loss, we suggest updating the limits during the training. (See theexplanation in Appendix D).
