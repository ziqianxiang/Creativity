Figure 1: Neural architectures for dynamics learning in an implicit 3D feature space (A): Inputsto our model are one or more images of the scene captured from various camera viewpoints. (B):Each RGB image is unprojected into a set of 3D feature maps through ray shooting and 3D convo-lutions, oriented to match a common coordinate system, and integrated through plain averaging intoa single 3D feature map M, following Tung et al. (2018) (C): An object detector detects 3D objectboxes and 3D object voxel occupancies using the scene 3D feature M as input. (D): The detectedboxes are used to crop the scene 3D feature maps around the object of interest and feed it to a 3Dencoder-decoder network to predicts the objectâ€™s future 3D relative rotation and translation for theobject. The action of the agent is represented as a segmentation and 3D flow in the object crop.
Figure 2: Forward unrolling of our dynamics model and baselines In the top row, We showrandomly sampled input image views for our model and the ours-depth baseline. The second rowshows the ground-truth motion of the object from the front view. Rows 3-6 show the object motionfrom an overhead camera. The ground truth object poses are colored in red while the predictedobject poses are colored in green. Their intersected regions stay in black. Our model shows a clearperformance margin over the baselines.
Figure 3: Various end-effector geometries for generalizability evaluationA.3 Implementation detailsWe train our model and baselines for single step prediction. Both after unprojection and after aggre-gation, we have a 3D encoder-decoder tower with 4 3D convolutional layers and 4 deconvolutionallayers. The channel size is set to 8, 16, 32, 64. We apply relu activation and batch normalizationafter each layer. The size of the object-centric latent memory is set to 16. To convert the object mem-ory to the final motion prediction, we use 4 convolutional layers without batch normalization. Thechannel size is set to 32, 32, 64, respectively. We then flatten it as a vector and pass it through twomore fully-connected layers. For the XYZ-orn baseline, we use four fully-connected layers with thesize of 32 for each. We also experiment with deeper and wider network, but the performance seemssimilar. For the appearance-based baseline, we use seven convolutional layers with channel size 16,32, 32, 64, 64, 128 and filter size 3,5,3,5,3,5,3. Each layers has batch normalization and relu acti-vation. We again flatten the output from the convolutional layers and pass it to two fully connectedlayers. For the SLAM baseline, we remove the 2D encoder-decoder tower from our model and wedouble the channel size in the following 3D convolutional layers. The learning is set to 1e - 3 forall the experiments with Adam optimizer.
