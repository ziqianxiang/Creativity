Figure 1: Illustration of the learned latent geometry by AAE before and after introducing x perturbations. Withhigh-capacity encoder/decoder networks, a standard AAE has no preference over x-z couplings and thus canlearn a random mapping between them (Left). Trained with local perturbations C (x), DAAE learns to mapsimilar x to close z to best achieve the denoising objective (Right).
Figure 2: Generation-reconstruction trade-off of various text autoencoders on Yelp. The “real data” line marksthe PPL of a language model trained and evaluated on real data. We strive to approach the lower right cornerwith both high BLEU and low PPL. The grey box identifies hyperparameters we use for respective models insubsequent experiments. Points of severe collapse (Reverse PPL > 200) are removed from the right panel.
Figure 3: Recall rate of 10 nearest neighbors in thesentence space retrieved by k nearest neighbors in thelatent space on Yelp. ARAE is not plotted here as wefind its recall significantly below other models (< 1%).
