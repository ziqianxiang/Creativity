Figure 1: N-layer Neural Networkweight matrix Wi and bias bi , and σ is a non-linear activation function, such as sigmod or ReLU.
Figure 2: MNIST. Batch size 100Sso-J6u⊂raJJ.
Figure 3: CIFAR-10. Batch size 1006Under review as a conference paper at ICLR 20205	ConclusionWe proposed a novel optimization scheme in order to overcome the difficulties of small stepsize andvanishing gradient in training neural networks. The computation of new scheme is in the spirit oferror back propagation, with an implicit updates on the parameters sets and semi-implicit updateson the hidden neurons. The experiments on both MNIST and CIFAR-10 show that the proposedsemi-implicit back propagation has better performance per epoch compared to SGD and ProxBP.
