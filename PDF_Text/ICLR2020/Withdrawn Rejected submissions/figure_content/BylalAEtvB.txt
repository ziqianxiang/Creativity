Figure 1: Experimental resultsour study with the LRMaxQInit algorithm. Similarly, LRMaxQInit(x) consists in the latter algorithm,benefiting from prior knowledge Dmax = x.
Figure 2: 2-states MDPalways s0. In the first MDP M1 ∈ M, the reward is 0 everywhere except for Rsa0 = 1. In the secondMDP M2 ∈ M, the reward is 0 everywhere except for Rsa11 = 1. With a discount factor γ = 0.9,the value functions and Q-functions of both MDPs are summarized in Table 3 Using the weighted	VM 3)	QMI (∙, ao)	QMI (∙, aC	VM 2(∙)	QM 2 (∙,aO)	QM2 (∙, aQs0	10	10	8.1	4.74	4.26	4.74s1	9	8.1	9	5.26	4.74	5.26Figure 3: Value functions and Q-functions of MDPs M1 and M2transfer technique from M1 to M2 proposed by Song et al. (2016) (Definition 4.1), the Q-functiondescribed below is used as an initialization for the exploration of M2 .
Figure 3: Value functions and Q-functions of MDPs M1 and M2transfer technique from M1 to M2 proposed by Song et al. (2016) (Definition 4.1), the Q-functiondescribed below is used as an initialization for the exploration of M2 .
Figure 5: The corridor grid-world environment.
Figure 6: Results of the corridor lifelong RL experiment with 95% confidence interval.
Figure 7: The maze grid-world environment. The walls correspond to the black cells and either thegreen ones or the orange ones.
Figure 8: Averaged discounted return over tasks for the maze grid-world lifelong RL experiment.
Figure 9: 4 times 4 heat-map grid-world.
Figure 11: Proportion of times where DmaX ≤ DMM, i.e. use of the prior, Vs computation of theLipschitz bound. Each curve is displayed with 95% confidence intervals.
