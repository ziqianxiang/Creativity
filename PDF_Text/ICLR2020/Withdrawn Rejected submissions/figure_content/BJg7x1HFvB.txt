Figure 1: Pre-trained DistillationSection 6.2, PD outperforms the pre-training+fine-tuning (PF) baseline, especially in the presenceof a large transfer set for distillation.
Figure 3: Pre-trained Distillation (PD) and concurrent work on model compression.
Figure 4: Baselines for building compact models, used for analysis (Section 6).
Figure 5: Pre-training outperforms truncation.
Figure 6: Depth outweighs width when models arepre-trained (PD and PF), as emphasized by the sharpdrops in the plot. For instance, the 6L/512H (35.4mparameters) model outperforms the 2L/768H model(39.2m parameters). Randomly initialized modelstake poor advantage of extra parameters.
Figure 7: Comparison against analysis baselines. Pre-trained Distillation out-performs all baselines: pre-training+fine-tuning, distillation, and basiC training over five different student sizes. Pre-training is performedon a large unlabeled LM set (BookCorpus & English Wikipedia). Distillation uses the task-speCifiC unlabeledtransfer sets listed in Table 4. TeaChers are pre-trained BERTLARGE, fine-tuned on labeled data.
Figure 8: Robustness to transfer set size. Weverify that distillation requires a large transfer set:8m instances are needed to match the performanceof the teacher using TransformerBASE. PD achievesthe same performance with TransformerMINI , on a 5mtransfer set (10x smaller, 13x faster, 1.5x less data).
Figure 9: Robustness to domain shift in transferset. By keeping |DT | fixed (1.7m) and varying thecorrelation between DL and DT (denoted by S), weshow that distillation requires an in-domain transferset. PD and PD-F are more robust to transfer set do-main.
Figure 10: Pre-training complements distillation.
