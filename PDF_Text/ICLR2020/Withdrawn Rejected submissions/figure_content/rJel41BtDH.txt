Figure 1: Pseudo-labeling in the “two moons” data (4 labels/class) for 1000 samples. From left toright: no mixup, mixup, and mixup with a minimum number of labeled samples per mini-batch. Weuse an NN classifier with one hidden layer with 50 hidden units as in (Miyato et al., 2018).
Figure 2: Example of certainty of incorrect predictions rt during training when using 500 (left) and4000 (right) labeled images in CIFAR-10. Moving from cross-entropy (C) to mixup (M) reduces rt,whereas adding a minimum number of samples per mini-batch (*) also helps in 500 labels, where M*(with slightly lower rt than M) is the only configuration that converges, as shown in Table 1 (left).
Figure 3:  Cross-entropy loss for labeled samples.  First (second) row show 500 (250) labels inCIFAR-10. From left to right: 13-CNN, WR-28 and PR-18. The heavy lines represent the medianlosses and the shaded areas are the interquartile ranges.
