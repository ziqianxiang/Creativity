Figure 1: Patch Gaussian augmenta-tion overcomes the accuracy/robustness trade-off observed in other augmentation strate-gies. Larger σ of Patch Gaussian (→)improves mean corruption error (mCE) andmaintains clean accuracy, whereas larger σof Gaussian (→) and patch size of Cutout(→) hurt accuracy or robustness. More robustand accurate models are down and to the right.
Figure 2: The apparent robustness-accuracy trade-off between Cutout and Gaussian augmen-tations. Each dot represents a model trained with different augmentations and hyper-parameters.
Figure 3: Patch Gaussian is the addition ofGaussian noise to pixels in a square patch. Itallows us to interpolate between Gaussian andCutout, approaching Gaussian with increas-ing patch size and Cutout with increasing σ.
Figure 4: (left) Fourier analysis of various models, using method from Yin et al. (2019). Heatmapsdepict model sensitivity to various sinusoidal gratings. Cutout encourages the use of high frequenciesin earlier layers, but its test error remains too sensitive to them. Gaussian learns low-pass filteringof features, which increases robustness at later layers, but makes lower layers too invariant to high-frequency information (thus hurting accuracy). Patch Gaussian allows high frequencies to beused in lower layers, and its test error remains relatively robust to them. This can also be seen by thepresence of high-frequency kernels in the first layer filters of the models (or lack thereof, in the caseof Gaussian). (right) Indeed, Patch Gaussian models match the performance of Cutout andBaseline when presented with only the high frequency information of images, whereas Gaussianfails to effectively utilize this information (see Appendix Fig. 12 for experiment details). This patternof reduced sensitivity of predictions to high frequencies in the input occurs across all augmentationmagnitudes, but here we use larger patch sizes and σ of noise to highlight the differences in modelsindicated by *. See text for details.
Figure 5: Accuracy/robustness trade-off observed for Cutout and Gaussian on Resnet-50 models.
Figure 6: Patch Gaussian hyper-parameter sweep for Wide-Resnet on CIFAR-10 (left) andRN50 on Imagenet (right). Patch Gaussian approaches Gaussian with increasing patch size andCutout with increasing σ. Each dot is a model trained with different hyper parameters. Colorsindicate different σ.
Figure 7: Training with Patch Gaussian improves clean data accuracy and Gaussian robustnesssimultaneously. Each dot represents a model trained with various σ (left) or patch sizes (right), whilekeeping the other fixed at the value indicated by the diamond. The y-axis is the average absoluteaccuracy when tested on data corrupted by Gaussian noise at various σ . The diamond indicates theaugmentation hyper-parameters selected by the method in Section 3.2.
Figure 8: TensorFlow implementation of Patch Gaussian15Under review as a conference paper at ICLR 2020Patch Size WFigure 9: Images modified with Patch Gaussian, with centered patch, at various W & σ.
Figure 9: Images modified with Patch Gaussian, with centered patch, at various W & σ.
Figure 10: Fourier analysis for Cutout and Gaussian models selected by the method in Section 3.2.
Figure 11: Complete filters for Resnet-50 models trained on ImageNet. * Indicates augmentationswith larger patch sizes and σ . See Figure 4 for details. We again note the presence of filters ofhigh fourier frequency in models trained with Cutout* and Patch Gaussian. We also note thatGaussian* exhibits high variance filters. We posit these have not been trained and have littleimportance, given the low sensitivity of this model to high frequencies. Future work will investigatethe importance of filters on sensitivity.
Figure 12:	Examples of high pass filters at various radii, in fourier space centered at the zero-frequencycomponent, used in the high-pass experiment of Figure 4.
Figure 13:	Frequency-based analysis (Yin et al., 2019) for models with different hyper-parameters ofPatch Gaussian.
