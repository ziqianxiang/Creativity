Figure 1: Latently invertible autoencoder (LIA). LIA consists of five functional modules: an encoderto unfold the manifold y = f (x), an invertible network φ to reshape feature embeddings to matchthe prior distribution z = φ(x) and φ-1 to map latent variables to feature vectors y = ψ-1(z), adecoder to produce output X = g(y), a feature extractor e to perform reconstruction measure, and adiscriminator c to distinguish real/fake distributions .
Figure 2: Detaching the functional modules of LIA for two-stage training.
Figure 3: Reconstructed faces by generative models on FFHQ database.
Figure 4: Manipulating reconstructed faces. The algorithm in (Shen et al., 2019) is applied tomanipulate the image reconstructed from the latent code w given by LIA. Each roW shoWs theoriginal image, the reconstruction, glass, pose, gender, smile, and age.
Figure 5: The exemplar real images of objects and scenes from LSUN database and their reconstructedimages by LIA. Three categories are tested, i.e. cat, bedroom, and car. For each group, the first row isthe original images, the second row shows the reconstructed images.
Figure 6: Interpolation on objects in the LSUN database.
Figure 7: Illustration of gradients and losses for training encoders on FFHQ dataset. For VAE, thedecoder is exactly the generator of StyleGAN. The two-stage training of VAE is the same as that ofLIA. The first and last layers in (a) and (b) refer to the neural architecture of the encoder. The y-axisindicates the average value of the gradients. The x-axis is re-scaled by 10,000. The gradients forLIA case are much more stable than the ones for VAE. The gradient of middle layer is also shown inAppendix A.8.
Figure 8: Generative results by StyleGAN and LIA. For each group of objects, the first row showsthe generative results obtained by StyleGAN and the second row by LIA.
Figure 9: Interpolation results by three generative models. The first row shows the result of LIA(ours), the second row that of the MSE-based optimization, and the last row that of Glow. The firstand last faces in the Glow result are the real face images of FFHQ.
Figure 10: Style mixing for real faces. The faces in the first row are the source faces where the stylecomes from.
Figure 11: Interpolation on FFHQ faces and objects in the LSUN database.
Figure 12: Interpolation with the latent code z and the feature w. For each group, the first row showsthe result with z and the second row shows the result with w.
Figure 13: Generated faces along the optimization path with the latent code z and the feature w.
Figure 14: Real images from FFHQ dataset and their reconstructed faces by various generativemodels.
Figure 17: Manipulating reconstructed faces. The algorithm in (Shen et al., 2019) is applied tomanipulate faces reconstructed from the latent code w given by LIA. Each row shows the originalimage, the reconstruction, glass, pose, gender, smile, and age.
Figure 18: illustration of gradients when training encoders on FFHQ dataset. For VAE, the decoder isexactly the generator of StyleGAN. The two-stage training of VAE is the same as that of LiA. The redand blue lines represent the gradients for VAE and LiA respectively. This figure shows the gradientof middle layer in the neural architecture of the encoder, which is complementary to Figure 7.
Figure 19: The architectural detail of LIA for each block of the whole network. Here the encoder andthe invertible network are shown. The synthesis network in the decoder is the same as StyleGAN.
