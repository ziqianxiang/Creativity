Figure 1: Graph-structured data: two moleculeswith atoms as nodes and bonds as edges. Eachmolecule has a different number of nodes andmolecular structure. In graph classification (andregression), each input datum is an individualgraph with features defined on the graph nodes(e.g., indicating the chemical element).
Figure 2: Computational flow of a Graph Neural Network consisting of three blocks of GCN graphconvolutional and HaarPooling layers, followed by an MLP. In this example, the output feature ofthe last pooling layer has dimension 4, which is the number of input units of the MLP.
Figure 3: A coarse-grained chain ofgraphs, where the input has 8 nodes,the second level has 3 nodes, and thetop level has single node.
Figure 4: Computational strategy of HaarPooling. We use the chain in Figure 3 and then thereare two HaarPooling layers in the network from G0 → G1 and G1 → G2 respectively. The inputof each layer is pooled by the compressive Haar transform for each layer: in the first layer inputX1in = (xi,j) ∈ R8×d1 is transformed by the compressive Haar basis matrix Φ(80×)3 with size 8 × 3formed by the first three column vectors, and the output is a feature array with size 3 × d1; in thesecond layer X2in = (yi,j ) ∈ R3×d2 is transformed by the first column vector Φ(31×)1 and the outputis a feature vector with size 1 × d2. In the plots of Haar basis matrix, the colors indicate the value ofthe entries of the Haar basis matrix.
Figure 5: Comparison for fast com-putation and direct matrix productfor HaarPooling for input feature ar-ray with up to 5000 nodes. Thecost of HaarPooling has near lin-ear computational complexity. Thecost of direct matrix product growsat O(N2.1).
