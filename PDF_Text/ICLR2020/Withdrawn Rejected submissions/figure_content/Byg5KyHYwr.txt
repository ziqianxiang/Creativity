Figure 1: Map of Apple-Gold domain,where the reward for getting an apple, get-ting the gold and taking a step in the rockis 1, 10, -0.05 respectively. The time limitfor one episode is 45 steps.
Figure 2: An example showing the updates of u, given ∆t =4. At each step t, we check the state embedding et+ι to findsimilar state embedding e= satisfying et+ι ≈ e= (i.e. ∣∣et+ι —eUo k < th) and to determine the reward according to Equation 1.
Figure 3: Architecture of the trajectory-conditioned policy (see Appendix B).
Figure 4: Learning curves on Apple-Gold domain averaged over 5 runs, where the curves in dark colors areaverage over 5 curves in light colors. The x-axis and y-axis correspond to the number of steps and statisticsabout the performance, respectively. The average reward and average imitation success ratio are the mean valuesover 40 recent episodes.
Figure 5: Visualization of trajectories stored in the buffer for PPO+SIL and DTSIL (ours) over time. The agent(gray), apple (red) and gold (yellow) are shown as squares for simplicity. The rocky region is in light blue.
Figure 6: Learning Curves of the average episode reward, the best episode reward, and the number of differentrooms found on Montezuma’s Revenge and Pitfall, averaged over 3 runs. On Montezuma’s Revenge, DT-SIL+EXP disCovers around 40 rooms on average while PPO+EXP never finds a path to pass through all 24rooms at the first level and then proCeed to the next level.
Figure 7: The reward for getting the key, opening the door, and collecting the treasure (yellow block) is 1, 2, and6 reSPeCtively. The learning Curve of the ePiSode reward iS averaged over 3 indePendent runS.
Figure 8: Architecture of the trajectory-conditioned policy (Repeating Figure 3).
Figure 9: Learning curves on Apple-Gold domain with random initial location of the agent in the lower leftcorner, where the curves in dark colors are average over 5 curves in light colors. The x-axis and y-axis correspondto the number of steps and statistics about the performance, respectively. The average reward and averageimitation success ratio are the mean values over 40 recent episodes.
Figure 10: Learning curves on Apple-Gold domain with sticky action of the agent, where the curves in darkcolors are average over 5 curves in light colors. The x-axis and y-axis correspond to the number of steps andstatistics about the performance, respectively. The average reward and average imitation success ratio are themean values over 40 recent episodes.
Figure 11: Learning curves on Apple-Gold domain with stochastic location of the gold in the upper middle partof the maze, where the curves in dark colors are average over 5 curves in light colors. The x-axis and y-axiscorrespond to the number of steps and statistics about the performance, respectively. The average reward andaverage imitation success ratio are the mean values over 40 recent episodes.
Figure 12: Map of Toy Montezuma’s Revenge, wherewe show the agent (gray), key(blue), door(green), andtreasure (yellow) as squares. The rewards are 100, 300,and 10000, respectively. An optimal path with the high-est total reward of 11,600 is shown as a red line.
Figure 13: Learning curves on Toy Montezuma’sRevenge averaged over 5 runs.
Figure 14: Experiment on Deep Sea. The learning curves shows the average episode reward, best episode reward,the number of found state representations, and the average success ratio of imitating the demonstrations in order.
Figure 15: Visualization of the trajectories stored in the buffer for PPO+SIL, SVPG diverse Gangwani et al.
Figure 16: Map of Atari Montezuma’s Revenge at the first level with 24 rooms. On Montezuma’s Revenge, thereare multiple levels and each level consists of 24 rooms. At the first level, it is challenging to bring two keys toopen the two doors in room 17 behind the treasure in room 15, where the agent can pass to the next level.
Figure 17: Learning curves of the number of rooms and the number of different state representations found onAtari Montezuma’s Revenge, averaged over 5 runs. The curves in dark colors are the average of the 5 curves inlight colors. During training, the state representation used is (levelt , roomt , xt , yt , kt).
Figure 18: Learning curves of the average reward, the best episode reward, the number of rooms, the numberof different state representations found on Atari Montezuma’s Revenge, averaged over 5 runs. The curves indark colors are the average of the 5 curves in light colors. During training, the state representation used is(levelt , roomt , xt , yt , kt).
Figure 19:	Learning curves averaged over 5 runs. The curves in dark colors are the average of the 5 curves inlight colors.
Figure 20:	Learning curves averaged over at least 3 independent runs. The curves in dark colors are the averageof the curves in light colors.
Figure 21:	Learning curves averaged over 5 independent runs. The curves in dark colors are the average of thecurves in light colors. The Apple-Gold domain is with random initial state of the agent.
Figure 22:	Learning curves averaged over 3 independent runs. The curves in dark colors are the average of thecurves in light colors. The Montezuma’s Revenge is with random initial delay of the agent.
Figure 23:	Learning curves of the average episode rewards over the recent 40 episodes. On the Apple-Golddomain, DTSIL reaches episode reward of 8.5 as shown in Section 4.1 while the random exploration withepsilon-greedy policy gets stuck at the episode reward of 2.
Figure 24:	Learning curves of the average episode rewards over the recent 40 episodes. On the Deap Seaenvironment with N = 30, DTSIL reaches episode reward of 1 as shown in Appendix C.3 while the randomexploration with epsilon-greedy policy gets stuck at the negative episode reward.
Figure 25:	Learning curves of the average episode rewards over the recent 40 episodes. On the Montezuma’sRevenge, DTSIL reaches episode reward over 20,000 as shown in Section 4.2 while the random exploration withepsilon-greedy policy could not reaches score over 100.
