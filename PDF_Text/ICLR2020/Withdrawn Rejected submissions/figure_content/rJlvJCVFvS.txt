Figure 1: A comparison of recovered relationships on the MIT67 dataset, with training only on theminicoco dataset. The reference object is surrounded by a blue box and regions with which it learnsrelationships are shown with orange boxes with the relationship weights visible in red text (zoom-inon the PDF). Left: Relation Networks (Hu et al., 2018) often learn weights between a referenceobject and its immediate surrounding context, while Focused Attention Networks better emphasizerelationships between distinct and spatially separated objects. Right: Relation Networks can sufferfrom a poor selection of regions to pair, or low between object relationship weights in comparison toFocused Attention Networks.
Figure 2: Left: Visualization of the word importance factor, which models the contribution of agiven word to its sentence level context (see Section 4.5). The sentence is in a clockwise directionstarting from 12 o’clock. "ait": weights learned using Hierarchical Attention Networks (Yang et al.,2016), "unsup": weights from the unsupervised case of our Focused Attention Network module, and"sup": weights from the supervised case. Right: semantic word-to-word relationship labels used inthe supervision of our network (see Section 3.2).
Figure 3: Top: The Focused Attention Network backbone. Bottom left: We add a detection branchto the backbone, similar to that of HU et al. (2018). Bottom middle: We add a scene recognitionbranch to the backbone. Bottom right: we insert the Focused Attention Module into a HierarchicalAttention Network (Yang et al., 2016). Here “FC" stands for fully connected layer and “Conv" standsfor convolutional layer.
Figure 4: The visualization of supervision targets for vision tasks. The blue box indicates a fixedreference object a and the orange boxes indicate the objects b that have ground truth relationship witha, for which we assign T[a, b] = 1. Left: different category supervision. Note that the sheep in theblue box is not related to the other sheep in the image. Right: different instance supervision. Thesheep in the blue box now has a relationship to other sheep (in yellow boxes).
Figure 5: The input/output dimension details related to Figure 3 of the main article. The dimensionsshown are for the case of a batch size of 1. Left: We add a detection branch to the backbone. Middle:We add a scene recognition branch to the backbone. Right: We insert the Focused Attention Moduleinto a Hierarchical Attention Network.
Figure 6: Additional visualization of the word importance factor in a sentence. See Section 3.2 andthe caption of Figure 2 of the main article for an explanation.
Figure 7: The visualization of relationships recovered on additional images of the MIT67 dataset.
