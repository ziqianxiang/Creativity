Figure 1: Entire procedure of our method: Conventional convolutional neural network is used asthe classifier. The perturbation generator network receives a one-hot encoded label as input, whichis processed with fully connected and up-convolutional layer, concatenates with the gradient imageand the original image, and finally generates an adversarial perturbation.
Figure 2: Trade-off relationship between benign accuracy and the adversarial robustness. Left:Perturbation power-accuracy graph for FGM adversarial training with various epsilon against CWattack. The bigger the epsilon, the more robust the network to adversarial attacks, but less benignaccuracy. Right: The network with FGM adversarial training tend to overfit easily because of itsfixed attack algorithm. As the training progresses, the benign accuracy rises, whereas the adversarialrobustness declines.
Figure 3: Evaluation of the attack performance of the proposed generator network. Accuracy vsperturbation norm was plotted for various attack methods. Left: a classifier was adversarially trainedwith Fast Gradient Method. Right: a classifier was adversarially trained with Projected GradientDescent.
Figure 4: The comparison of the robustness of the defense methods with various benign accuracy.
Figure 5: Robustness-curve: A plot showing the relationship between benign accuracy and ρcw bychanging the hyper-parameters of each adversarial training algorithm. Left: For FGM and PGDadversarial training, each data point was acquired through changing , whereas for our algorithm,each data point was acquired through changing cL . The outer curves are considered more robustadversarial algorithms. Right: Robustness-curves for our algorithm under different capacities of theclassifier. It shows that the classifier is still underfitting in terms of adversarial robustness.
Figure 6: Adversarial Robustness with Gaussian Normal Distribution. Left: The red curve indicatesour target Gaussian normal probability function with mean 0 and variance σ2 . The blue dottedcurve indicates the classifiers class probability of the label in accordance with the L2 norm of theperturbation. As the training progresses, the classifiers class probability converges to the targetfunction, ensuring the robustness of the network. Right: Conceptual illustration of the adversarialtraining. By minimizing the loss function on the region with large adversarial loss, the networkbecomes increasingly robust against adversarial attacks.
Figure 7: Robustness-curve. Left: Varying hyper-parameter, Right: Varying capacity0.3	0.4	0.5	0.6	0.7	0.8	0.9	1.0Pcw13Under review as a conference paper at ICLR 2020C SOME ADVERSARIAL EXAMPLES GENERATED BY THEPROPOSED GENERATOR NETWORKTable 4: Adversarial examples for various perturbation powers from the CIFAR-100 dataset. Theseimages are generated from the generator which is fully trained to maximize the classification loss.
Figure 8: In order to measure how much contribution each input make in terms of the robustnessof the classifier, we removed each input one by one, and plotted an accuracy vs average distortioncurve. It can be observed that the gradient is the most important factor in classifier robustness, andthe other two factors have relatively smaller impact on classifier robustness.
