Figure 1: The illustration of one layer of R-Transformer. There are three different networks that arearranged hierarchically. In particular, the lower-level is localRNNs that process positions in a localwindow sequentially (This figure shows an example of local window of size 3); The middle-levelis multi-head attention networks which capture the global long-term dependencies; The upper-levelis Position-wise feedforward networks that conduct non-linear feature transformation. These threenetworks are connected by a residual and layer normalization operation. The circles with dash lineare the paddings of the input sequencehowever, have been shown to be limited (Dehghani et al., 2018; Al-Rfou et al., 2018). In addition,it requires considerable amount of efforts to design more effective position embeddings or differentways to incorporate them in the learning process (Dai et al., 2019). Second, while multi-head atten-tion mechanism is able to learn the global dependencies, we argue that it ignores the local structuresthat are inherently important in sequences such as natural languages. Even with the help of positionembeddings, the signals at local positions can still be very weak as the number of other positions issignificantly more.
Figure 2: An illustration of the original and local RNN. In contrast to orignal RNN which maintainsa hidden state at each position summarizing all the information seen so far, LocalRNN only operateson positions within a local window. At each position, LocalRNN will produce a hidden state thatrepresents the information in the local window ending at that position.
