Figure 1: Our framework combines supervised and self-supervised losses to learn represen-tations on small datasets. Self-supervised tasks, such as jigsaw puzzle or rotation prediction, canbe derived from the images alone and act as a data-dependent regularizer for the shared featurebackbone.
Figure 2: Benefits of SSL for few-shot learning tasks. We show the accuracy over the ProtoNetbaseline of using different SSL tasks. ThejigsaW task results in an improvement of the 5-way 5-shotclassification accuracy for a ProtoNet across datasets. Combining SSL tasks can be beneficial forsome datasets. Here SSL was performed on images within the base classes only. See Table 2 in theAppendix for a tabular version and results for 20-way 5-shot classification.
Figure 3: Benefits of SSL for harder few-shot learning tasks. We show the accuracy of usingjigsaw puzzle task over ProtoNet baseline on harder versions of the datasets. We see that SSL iseffective even on smaller datasets and the relative benefits are higher.
Figure 4: Effect of size and domain of SSL on 5-way 5-shot accuracy. (a) More unlabeleddata from the same domain for SSL improves the performance of the meta-learner. (b) Replacinga fraction (x-axis) of the images with those from other domains makes SSL less effective. (c)A comparison of approaches to sample images from a pool for SSL. With random sampling, theextra unlabeled data often hurts the performance, while those sampled using the “domain weighs”improves performance on most datasets. When the pool contains images from related domains, e.g.,birds and dogs, the performance using selected images matches that of using 100% within-datasetimages. (d) Accuracy of the meta-learner with SSL on different domains as function of distancebetween the supervised domain Ds and the self-supervised domain Dss. See text for details.
Figure 5: Saliency maps for various images and models. For each image We visualize the magni-tude of the gradient with respect to the correct class for models trained with various loss functions.
