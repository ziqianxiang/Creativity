Figure 1: MHAL architecture, taking a sequence of three words. Illustrated for one head h only. Weobtain character representations as presented in the dashed rectangle (here, for the word sat).
Figure 2: Attention evidence scores, normalized acrossed heads, assigned by MHAL-zero for thewords in a sentence from the CoNLL-2003 dataset.
Figure 3: Semi-supervised experiments for SST, CoNLL-2003, and FCE, comparing the sequencelabeling performance of the multi-task model MHAL-joint with the single-task model MHAL-tok.
Figure 4: Attention evidence scores, normalized acrossed heads, assigned by MHAL-zero for thewords in three sentences from the SST (leftmost), CoNLL-2003 (middle), and FCE (rightmost)datasets.
Figure 5: Attention evidence scores, normalized acrossed heads, assigned by MHAL-joint for thewords in three sentences from the SST (leftmost), CoNLL-2003 (middle), and FCE (rightmost)datasets.
