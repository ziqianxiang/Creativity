Figure 1: Real image: 3; Invariance-based attack: 5. [Left]: Training a classifier without constraintsmay learn a decision boundary unrobust to perturbation-based adversarial examples. [Right]: Enforc-ing robustness to norm-bounded perturbations introduces erroneous invariance (dashed regions inepsilon spheres). Note, that we display real data here, the misclassified 5 is an image found by ourinvariance-based attack which resides within a typically reported -region around the displayed 3 (inthe `0 norm). This excessive invariance of the perturbation-robust model in task-relevant directionsmay be exploited, as shown by the attack proposed in this paper.
Figure 2: Robustness experiment on spheres with radii R1 = 1 and R2 = 1.3 and max-marginclassifier that does not see n = 10 dimensions of the d = 500 dimensional input. [Left]: Attackingpoints from the outer sphere with perturbation-based attacks; accuracy drops when increasingthe upper bound on '2-norm perturbations. [Right]: Attacking points from the inner sphere withinvariance-based attacks; accuracy drops when increasing the upper bound on '2-norm perturbations.
Figure 3: Invariance-based ad-versarial example (top-left) islabeled differently by a hu-man than original (bottom-left). However, both becomeidentical after binarization.
Figure 4: Process for generating `0 invariant adversarial examples. From left to right: (a) the originalimage of an 8; (b) the nearest training image (labeled as 3), before alignment; (c) the nearest trainingimage (still labeled as 3), after alignment; (d) the δ perturbation between the original and alignedtraining example; (e) spectral clustering of the perturbation δ; and (f-h) possible invariance adversarialexamples, selected by applying subsets of clusters of δ to the original image. (f) is a failed attemptat an invariance adversarial example. (g) is successful, but introduces a larger perturbation thannecessary (adding pixels to the bottom of the 3). (h) is successful and minimally perturbed.
Figure 5: Our invariance-based adversarial examples. Humans (acting as the oracle) switch theirclassification of the image from the original test label to a different label.
Figure 6: Visualization that large `2 norms can also fail to measure semantic changes in images.
Figure 7: Histogram of MNIST pixel values (note the log scale on the y-axis) With two modes around0 and 1. Hence, binarizing inputs to a MNIST model does not impact its performance importantly.
Figure 8: Invariance-based adversarial examples for a toy '∞-robust model on MNIST. By thresh-olding inputs, the model is robust to perturbations δ such that ∣∣δ∣∣∞ . 0.5. Adversarial examples(top-right of each set of 4 images) are labeled differently by a human. However, they become identicalafter binarization; the model thus labels both images confidently in the source image,s class.
