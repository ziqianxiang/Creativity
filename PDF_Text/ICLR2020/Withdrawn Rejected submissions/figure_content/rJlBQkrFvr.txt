Figure 1: We show how our model integrates with reinforcement learning algorithms to provideintrinsic rewards in (a). The optimization procedure for our perception model as well as our methodfor computing intrinsic reward are visualized in (b).
Figure 2: Extrinsic reward per episode achieved in training over 10 million time steps and 3 seedsfor the following Atari games: Beam Rider, Breakout, Gravitar, River Raid, Private Eye, SpaceInvaders, Montezumaâ€™s Revenge, and Pitfall.
Figure 3: Extrinsic reward per episode achieved with sticky actions in training over 10 million timesteps and 2 seeds for the following Atari games: Gravitar, River Raid, Private Eye, and Pitfall.
Figure 4: Relationship between reconstruction er-ror and intrinsic reward.
Figure 5: The first row shows the game scene images fed into our perception model as states. Thesecond row shows the reconstructed images produced by our perception model. The remaining rowslist intrinsic reward and embedded image reconstruction error respectively for each image pair.
