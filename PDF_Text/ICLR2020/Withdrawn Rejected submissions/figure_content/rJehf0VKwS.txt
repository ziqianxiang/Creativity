Figure  1:    Student-Teacher  learning  system.     (a)  Given  previous  tokens  (yellow),   the  student  modelasks   feedback   for   producing   a   token,   e.g.,   “view”,   and   the   teacher   model   gives   rewards,   e.g.,log q(“view” “Amazing”, x).  (b) The difference in learning strategy matters!  We ignore y<t  and x for sim-plicity. We flip the sign of gradients and demonstrate them in the 2nd row. When q(yt) dominates p(yt), thegradients less than 0 (magenta) and p(yt)   .  In contrast, when p(yt) dominates q(yt), the gradients greaterthan    0 (cyan) and p(yt)   . More importantly, KA pulls the probability of “view” and “scene” more gently butpush the probability of “bird” and “boat” much harder compared to KD. In other words, KD attempts to rank“view”, “scene” higher to increase recall, while KA aims at reducing the chance of “bird” and “boat” beingpicked to maximize precision.
Figure 2:  Matching two 1-D distributions.  (a) p(x) and q(x).  (b) The derivatives of DKL  w.r.t p(x) in bothorders. DKL(q(x)  p(x)) pulls more on x where q(x) > p(x) (blue region) while DKL(p(x)  q(x)) pushesmore on x where p(x) > q(x) (red region).
Figure 3:  p(x) evolves over epochs.  p0, p1, p2  and p3  are shown from top to bottom.  Purple lines denotep(x) and blue lines denote ground-truth q(x). p0 and p1 in KD converge to q0 and q1 in     30 epochs, whilep2 and p3 stay far away from the ground-truth q2 and q3 with respect to those in KA.
Figure 4:  Ablation study on WMT’17 De-En task.  (a) Accuracy on validation set with variable k.  Larger kmeans a larger part of distribution is observed. (b) Number of novel tokens emerge in the top-16. High numberindicates strong exploration in search space.  We noticed that     80% probability mass are put on the top-16tokens.
Figure 5: Entropy over epochs.
