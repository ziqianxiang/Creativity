Figure 1: The architecture and training diagram describing the proposed method. Each batch consistsof randomly sampled instances, i.e. pairs of images and their corresponding texts. Images areembedded via ResNet and texts are embedded via a CNN/LSTM stack. Image and text featuresare projected via a fully connected layer into the same dimensional space. In this space, distancesbetween text and image features from different instances are computed. The negative distances arefed into softmax to train on both the image and the text retrieval tasks. The image retrieval taskconsists of retrieving the image corresponding to the given text of the same instance and the textretrieval task is vice versa. In addition to that, image and text embeddings are trained on auxiliaryimage and text classification tasks on the class labels corresponding to instances.
Figure 2: Harmonic mean Top-1 accuracy on seen and unseen, H, against the value of α on thevalidation set. The curves represent the mean and 95% confidence intervals over 10 optimizationruns. Results are stable over different runs. H exhibits a distinct inverted U-shape w.r.t. α.
Figure 4: The plot of the harmonic mean Top-1 accuracy on seen and unseen, H, against κ, therelative weight of the retrieval and the classification loss terms. K = 0 corresponds to the case ofclassification loss having weight 0. The curves represent the mean over 10 optimization runs.
Figure 3: Harmonic mean Top-1 accuracy on seen and unseen, H, against λ, the relative weight ofimage and text retrieval loss terms. λ = 0 corresponds to the case of image retrieval loss havingweight 1 and text retrieval loss having weight 0. Mean over 10 optimization runs.
