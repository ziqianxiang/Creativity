Figure 1: Concept diagram of Recurrent Layer Attention network (RLA network). Through concur-rently propagating with CNN, RNN stores summarized feature and infers a scaling coefficient offeature volume considering a feature hierarchy. Recurrently applying this operation for every layer ofCNN, RLA network applies overall weight balance of layers.
Figure 2: The structure of Recurrent Layer Attention network (* = 1), RLA-Vector (* = Cl), andIA+RLA (including a dotted line). 0 denotes scale operations belonging to intra-山yer attention.
Figure 3: Training and validation error curve on the CIFAR-100 dataset of the ResNet-56 base-bone(left) and ResNet-110 backbone(right).
Figure 4: Grad-CAM visualization on the intermediate layers of RLA network(left) and normalizedlayer attention curve(right). Layer attention is layer-wisely normalized by substracting sample meanand dividing by sample standard deviation. The layer that achieve the highest layer attention valuemarked with red border, and a representative of layers that captures class-agnostic local patternsmarked with yellow border.
Figure 5: Mean layer attention curve over layers of RLA-18 (left), RLA-50 (middle), and RLA-101 (right). Data points signify mean layer attention value calculated on ImageNet 1K validationdataset. Vertical range on each point denotes an ±1 standard deviation. We mark the first layer ofeach bottleneck block as red for visibility.
Figure 6: Grad-CAM visualization on the intermediate layers of RLA network. Target class of Junco,achidna, killer whales, leonberg, tiger cat, sleeping bags(top to bottom) are depicted.
