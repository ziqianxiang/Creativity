Figure 1: 2D-contour plot of surface y = (w1 ∙ w2 - 1)2 of linear model With two layers. The lossfunctions has many global minima are located on hyperbola w2 = 1/w1. Solutions near (-1, 1) and(1, 1) are good ”flat” minima, and solutions near axes are ”sharp” minima.
Figure 2: Deep Linear Network with two layers: training with SGD, Adam, and NovoGrad4Under review as a conference paper at ICLR 20205 Experiments with large DNNsWe train four deep models: ResNet-50 (He et al., 2016) — for ImageNet classification, Transformer-big (Vaswani et al., 2017) — for WMT 2014 translation, Jasper (Li et al., 2019) — for LibriSpeechspeech recognition, and Transformer-XL (Dai et al., 2019) — for WikiText-103 word-level languagemodeling, with NovoGrad, SGD with momentum, and Adam/AdamW.4 Each model was trained ona single DGX-1 machine with 8 NVIDIA V100 GPUs with gradient accumulation used for largebatch training. In all the experiments, NovoGrad performed on par or better than other algorithms.
Figure 3: ASR, large batch training. Jasper-10x5 trained with NovoGrad on LibriSpeech.
Figure 4: LM. Transformer-XL trained with Adam and NovoGrad on WikiText-103.
Figure 5: 2D-contour plot of surface y = (w1 ∙ w2 - 1)2 of linear model With tWo layers. The lossfunctions has many global minima located on hyperbola w2 = 1/w1. Solutions near (-1, -1) and(1, 1) are good ”flat” minima, and solutions near axes are ”sharp” minima.
Figure 6: DLN training - baseline: learning rate 0.2, weight decay 0.1.
Figure 7: DLN training - increased learning rate 0.2 → 1.0: SGD and Adam diverges, AdamW andNovoGrad converge.
Figure 8:diverge.
Figure 9: DLN training - "bad” initialization: AdamW diverges.
