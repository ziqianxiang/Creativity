Figure 1: ExpandNets. We study 3 strategies tolinearly expand a compact network. The resultingexpanded network can be compressed back to thecompact one algebraically, and consistently out-performs training the compact one from scratchand with knowledge distillation.
Figure 2: Training behavior of networks with 7 X 7 kernels on CIFAR-10. Left: Training and testcurves over 150 epochs. Middle: Minimum pairwise gradient cosine similarity at the end of eachtraining epoch. Right: Kernel density estimation of pairwise gradient cosine similarity at the end oftraining (over 5 independent runs).
Figure 3: Loss landscapes of networks with 9 Ã— 9 kernels on CIFAR-10.
Figure 4: Loss landscape plots on CIFAR-10 (We report top-1 error (%)).
Figure 5: Training behavior of networks on CIFAR-10. Left: Training and test curves over 150epochs. Middle: Minimum pairwise gradient cosine similarity at the end of each training epoch.
Figure 6: Training behavior of networks on CIFAR-100. Left: Training and test curves over 150epochs. Middle: Minimum pairwise gradient cosine similarity at the end of each training epoch.
Figure 7: Product L2 Vs Normal L2 (best viewed in color). Left: Training curves of the overall lossfunction. Middle Left: Training curves of the cross-entropy loss. Middle Right: Curves of trainingerrors. Right: Curves of test errors. (Note that the y-axis is in log scale.)D Discussion of (Arora et al., 2018)In this section, we discuss in more detail the work of Arora et al. (2018) to further evidence thedifferences with our work. In (Arora et al., 2018), the authors only worked with linear modelsor linear layers (fully-connected layers). By contrast, we focus on practical, nonlinear, compactconvolutional networks, and we propose 2 ways to expand convolutional layers, which has not beenstudied before. Our convolutional linear expansion strategies yield better solutions with higheraccuracy, more zero-centered gradient cosine similarity during training and minima that generalizebetter.
