Figure 1: Starting from full-precision weights (a), we create a PDF of the sorted absolute values (b)and uniformly sample from the corresponding CDF (c). The sampling process produces quantizedinteger network weights based on the number of hits per weight (d). Note that since weights 7, 8, and9 were not hit, sparsity is introduced which can be exploited by hardware accelerators.
Figure 2: Results of quantizing both weights and activations on CIFAR-10 using different samplingamounts. The quantized models reach close to full-precision accuracy at around half the sample sizewhile using only around half the weights and one-third of the activations of the full-precision models.
Figure 3: Results of quantizing both weights and activations on SVHN using different samplingamounts. The quantized VGG-7* model reaches close to full-precision accuracy using around 0.5samples per weight/activation, requiring around 8 bits and using 22% of the weights of the originalmodel, with 22% nonzero activations. Model A, B, C, and D are less redundant models that requiremore sampling to achieve close to full-precision accuracy.
Figure 4: Results of quantizing both weights and activations on ImageNet using different samplingamounts. All quantized models reach close to full-precision accuracy at K = 3.
Figure 5: Quantized weights on CIFAR-10.
Figure 6: Quantized weights on SVHN.
Figure 7: Quantized weights on ImageNet.
Figure 8: Quantized activations on CIFAR-10.
Figure 9: Quantized activations on SVHN.
Figure 10: Quantized activations on ImageNet.
Figure 11: Different sampling seeds on CIFAR-10 with K = 1.0.
