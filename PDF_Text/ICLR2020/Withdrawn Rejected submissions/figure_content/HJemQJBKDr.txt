Figure 1: Demonstration of generative models in continual learning. At task i the training set consistsof samples of category i and samples generated by the model at the previous task, and the task is togenerate samples from all previously seen categories (figure reproduced from Lesort et al. (2018)).
Figure 2:  Synthetic experiments of estimating KL-divergence by CDRE (error bars from 5 runs).
Figure 3: Evaluating GANs in continual learning on Fashion-MNIST, features for FID and KID areextracted from the classifier, features for  f -divergences are generated by the CVAE. The dimensionof generated features is 64.  The sample size is 6000 for each class.  The shaded area are plotted bystandard deviation of 10 runs.  The y-axis in the right side of Fig. 3b is the y-axis of the purple line( f -GAN-rvKL), which is in a much larger scale than others.
Figure 4:  Fashion-MNIST samples generated by GANs in continual learning.  In each figure, eachrow displays images generated by the model at each task,the order is from the top to bottom (task 1to 10). The displayed samples are randomly chosen from generated samples of each class.
Figure  5:  Toy  experiments  with  differ-ent types of noise injected into MNIST(error bars from 5 runs).
Figure 6: Evaluating GANs in continual learning on MNIST, features for FID and KID are extractedfrom the classifier, features for  f -divergences are generated by CVAE. The shaded area are plottedby standard deviation of 10 runs.
Figure 7: MNIST samples generated by evaluated GANs in continual learning. In each figure, eachrow displays figures generated at each task,the order is from the top to bottom.
