Figure 1: DNNs consist of interconnectedblocks which combine a convolution or fully-connected layer (Linear), batch normalization(BN), ReLU, and pooling (optional).
Figure 2: Clipping and saturation using bit-shifting and comparison.
Figure 3: Weight distribution of Layer-1, Layer-4 and Layer-7 (from top to bottom) of VGG11 afterseveral epochs. Since weight decay is used for pre-training, the weight distribution is unimodal atthe beginning with a peak at zero. Then, our approach continuously rearranges the weights into aternary-valued distribution, clearly visible at epoch 80. The variance of each mode is continuouslydecreased by the exponentially increasing regularization parameter. After 100 epochs, the weightsare that close to the fixed-point centers that post quantization does not produce a remarkable error.
