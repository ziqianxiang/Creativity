Figure 1: We extracted the output from each layer of the encoders and concatenated all the layers toform a three-dimensional tensor U. We then performed Squeeze fsq(U) and Excitation fₑₓ(fsq(U))to obtain the weight of each output layer. Finally, we fed the weighted average of all layers into theclassifier. In this work we employed n = 12 attention layers.
Figure 2: Diagram of a one-dimensional Gaussianblur kernel, which was convoluted through the in-put dimension. This approach enabled the centralword to acquire information concerning neighbor-ing words with weights proportional to the Gaus-sian distribution.
Figure 3: We compared BERT and SesameBERT for each case. Left: Results of heuristics-entailedcases.  Right:  Results of heuristics labeled as Nonentailment.  In contrast to the results in Left:,BERT performed poorly in all three cases in Right; this indicated that the model had adopted shallowheuristics rather than learning the latent information that it intended to capture.
Figure 4:  Evaluation of the weights calculated by Squeeze and Excitation for all layers, with theRTE dataset as an example.
