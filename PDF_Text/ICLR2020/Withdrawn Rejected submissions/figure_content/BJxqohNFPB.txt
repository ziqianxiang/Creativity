Figure 1: in this paper we propose a method for generating photo-realistic images from semanticlabels of a simulator scene. This figure provides images related to the Synthia dataset Ros et al.
Figure 2: example of the images for training the model. Left is the semantic map. Middle is theedge map extracted from the real image. Right is the real image being used by the discriminator foradversarial training. Please note that the real image is not used by the generator neither at trainingnor at test time, but only its edge map. The main issue in the CG2real model compared to the imageto image models is that the simulators image is available to the generator at test time. We thus usethe simulators image to extract the edge map allowing the generator to generate the necessary finedetails in the output image.
Figure 3: comparison of 768x384 pix images generated by pix2pixHD (Left) and our model (Right).
Figure 4: Comparison of generated test images, when training with a single discriminator and amulti-scale one. The left image is generated when the generator was trained with a single discrimi-nator, while the right image while using a multi-scale one. This figure demonstrates that when usingour model (with our skeleton), training with a single discriminator might be enough.
Figure 5: By using edge maps, the model learns to separate objects of the same semantics. The mostdominant example is buildings. Unlike cars, pedestrians or bicycle riders, that are separable usingthe instance map, buildings are not. The semantic label provides the pixels in which the buildingexists. Considering the fact that a scene of adjacent buildings is somewhat common, the abilityto separate them is of high value. Left - the label map. Middle - generated image by Wang et al.
Figure 6: previous work test images Wang et al. (2018b) (Top) compared to our model test images(Bottom). The images generated by our model contain low level details, allowing the desired photo-realism3.3	Video generationUsing pre trained CG2real networks, we generate two consecutive images, and then estimate twoflow maps. The first flow map is between xi, xi+1, where xi and xi+1 are two consecutive real im-ages. The second flow map is between G(si, ei), G(si+1, ei+1) , where G(si, ei) and G(si+1, ei+1)are two consecutive generated (fake) images. Note that the generation of G(si, ei), G(si+1, ei+1) isdone independently, meaning we apply our CG2real method twice, without any modifications. Toconclude we enforce temporal coherency by using the following loss:LflowL1 (Freal , Ffake)(8)Where FreaI = F(xi,xi+Î¹), Ffake = F(G(si,ei),G(si+1,ei+1)) and F(*) is the optical flowoperator. This formulation eliminates the need of using a sequential generator as in Wang et al.
Figure 7: block diagram of the video generation model. Two identical CG2real models generateFake image (t) and Fake image (t+1). The two consecutive fake images are fed to the flow-fakeestimator, while two consecutive real images are fed to the flow-real estimator. Both real and fakeflow maps are trained using L1 (Freal , Ff ake) loss. This enables the pre-trained CG2real models tolearn the required coherency for generating photo-realistic videos.
Figure 8: Comparison of video generation. Up - images generated by vid2vid Wang et al. (2018a).
Figure 9: Additional Test images from Cityscapes Dataset. Left - pix2pixHD. Right - Ours. Theseimages further demonstrate the photo-realism achieved by our model.
Figure 10: Additional test images on Synthia dataset. Left - pix2pixHD. Right - Ours. These imagesdemonstrate improved image quality, better and finer details in the generated objects, buildings andvegetation.
Figure 11: Additional test images on Synthia dataset. Left - pix2pixHD. Right - Ours. These imagesdemonstrate improved image quality, better and finer details in the generated objects, buildings andvegetation.
Figure 12: Test video on CityScapes. Left - vid2vid. Right - Ours video gen model. These imagesdemonstrate better temporal coherency in the generated images. Moreover, in the top left corner ofthe left video, we notice the error propagates. Better yet, the buildings in the right video are morereasonable, w.r.t. windows, shades, general texture, etc. As for image quality, the road signs in theright video are better emphasized.
Figure 13: Test video on CityScapes. Left - vid2vid. Right - Ours video gen model. This figureprovides more images from the same video presented in fig 4. Pay attention to the error propagationon the top right images of vid2vid. Again, our model demonstrates finer road signs and higher levelof details in the generated buildings.
