Figure 1: (a) The minimum loss achieved over a set of hyperparameters in a noisy least squares regressionproblem (simulated dynamics). (b) The minimum loss achieved in real dataset MNIST (a logistic regressionmodel). Our theoretical prediction (a) matched with the training dynamics for real datasets, demonstratingtradeoffs between computational cost and convergence speed. The curves in red are SVRG and curves in blueare SGD. Different markers refer to different per-iteration computational cost, i.e., the number of backpropaga-tion used per iteration on average.
Figure 2: The minimum loss achieved by following SGD (blue) and SVRG (red) over a set of hyperparametersin a noisy least-square dynamics simulation for cases with and without label noise. The plot suggests that in thepresence of label noise, there is a tradeoff between computational cost and convergence speed. In the absenceof label noise, SGD strictly dominates SVRG in convergence speed for all computational cost.
Figure 3: The minimum loss achieved by following SGD (blue) and SVRG (red) over a set of hyperparametersfor training on MNIST and CIFAR-10 with underparameterized models. All the results in these plot suggestedthere is a tradeoff between computational cost and convergence speed when comparing SGD and SVRG.
Figure 4: The minimum loss achieved by following SGD (blue) and SVRG (red) over a set of hyperparametersfOr training On MNIST and CIFAR-10 with OverParameterized mOdels. In this setting we Observed strict dOmi-nance Of SGD Over SVRG in cOnvergence sPeed fOr all cOmPutatiOnal cOst, matching Our PreviOus theOreticalPredictiOn.
Figure 5: Evaluate the sensitivity of N in Figure 5a with Î± = 0.5, T = 256 and b = 64 for SVRGunder the noisy least square model. And compare large batch SGD to SVRG in Figure 5b withT = 256 and fixing computational budget as 2048, varying b0, N for SVRG.
