Figure 1: Sequence processing model for a many-to-one mapping. The target value y can be either anestimate for yE (entanglement classification) or ySRV (SRV regression).
Figure 2: Negative and positive samples in the data set as a function of the leading Schmidt rank n.
Figure 3:	Workflow. We split the entire data by their leading Schmidt rank n. All samples with n ≥ 9constitute the extrapolation set, which we use to explore the out-of-distribution capabilities of ourmodel. For the remaining samples (i.e. n < 9) we make a random test split at a ratio of 1/4. The testset is used to estimate the conventional generalization error of our model. We use the training set toperform cluster cross validation.
Figure 4:	True negative rate (TNR), true positive rate (TPR), rediscovery ratio of the LSTM networkusing cluster cross validation for different folds 0-8. True negative rates are high for all validationfolds. All metrics are good for the extrapolation set 9-12, demonstrating that the models performwell on data beyond the training set distribution, covering only Schmidt rank numbers 0-8. Error barsrepresent 95 % binomial proportion confidence intervals.
Figure 5:	True negative rate (scale starts at 0.6), true positive rate, rediscovery ratio, and hit ratefor the extrapolation set 9-12 for varying sigmoid threshold τ and SRV radius r. For too restrictiveparameter choices (τ → 1 and r → 0.5) the TNR approaches 1, while TPR and rediscovery ratioapproach 0, such that no interesting new setups would be identified. For too loose choices (small τ ,large r), too few negative samples would be rejected, such that the advantage over random searchbecomes negligible. For a large variety of τ and r the models perform satisfyingly well, allowing adecent compromise between TNR and TPR. This is reflected in large values for the hit rate, which is0.736 on average over all depicted thresholds.
