Figure 1: Examples of reasoning paths to answer two questions of the HotpotQA dataset. In thispicture, we do not display the full paragraphs, but only the supporting facts.
Figure 2: Overview of LQR-net with K parallel heads and T sequential reading modules. In thisarchitecture, a latent representation of the question is sequentially updated to perform multi-hopreasoning. K independent reading heads collect pieces of information before feeding them to theanswering module. Sections 3 present the different building blocks of this end-to-end trainablemodel.
Figure 3: Distribution of the probabilities for each word to be part of the predicted span, before thefirst reformulation module and in the answering module. We display the reading-based attentioncomputed in Equation 7 and the reading-based attention computed from ps and pe from Equation10. In these examples, we show only the supporting facts.
