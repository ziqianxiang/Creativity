Figure 1: Generative Data AugmentationFigure 2: Parallel Image Generation3.3	Adaptive Generator WeightingFurthermore, to determine which generators are the most effective in generating authentic images,we introduce adaptive generator weighting at each augmentation epoch. At the initial stage, all thegenerators are treated equally. Before the batch of sample images generated by one generator Gi4Under review as a conference paper at ICLR 2020Figure 3: Adaptive Generator Weightingare sent to the data group corresponding to other K - 1 generators, we collect the inception scores{wi}iK=1 computed in section 3.1. Since higher inception scores imply better performance of thegenerator, we define the generator weight pi of a generator Gi asP =	eχp(Wi)"PK=I exP (Wj)’and use this weight to determine how many images should be sampled from generator Gi to be sentto other data groups for subsequent training in the very next augmentation epoch. When the totalnumber of samples to be collected from generators are fixed, this method enables generators withbetter realistic image generation power to contribute more to the future training data groups. Morerealistic training sets thus augmented, in turn, exert more positive influence on the images to begenerated. Algorithm 1 illustrates the whole 3-step process, which we denote as PAGDA(Parallel
Figure 2: Parallel Image Generation3.3	Adaptive Generator WeightingFurthermore, to determine which generators are the most effective in generating authentic images,we introduce adaptive generator weighting at each augmentation epoch. At the initial stage, all thegenerators are treated equally. Before the batch of sample images generated by one generator Gi4Under review as a conference paper at ICLR 2020Figure 3: Adaptive Generator Weightingare sent to the data group corresponding to other K - 1 generators, we collect the inception scores{wi}iK=1 computed in section 3.1. Since higher inception scores imply better performance of thegenerator, we define the generator weight pi of a generator Gi asP =	eχp(Wi)"PK=I exP (Wj)’and use this weight to determine how many images should be sampled from generator Gi to be sentto other data groups for subsequent training in the very next augmentation epoch. When the totalnumber of samples to be collected from generators are fixed, this method enables generators withbetter realistic image generation power to contribute more to the future training data groups. Morerealistic training sets thus augmented, in turn, exert more positive influence on the images to begenerated. Algorithm 1 illustrates the whole 3-step process, which we denote as PAGDA(ParallelAdaptive Generative Data Augmentation).
Figure 3: Adaptive Generator Weightingare sent to the data group corresponding to other K - 1 generators, we collect the inception scores{wi}iK=1 computed in section 3.1. Since higher inception scores imply better performance of thegenerator, we define the generator weight pi of a generator Gi asP =	eχp(Wi)"PK=I exP (Wj)’and use this weight to determine how many images should be sampled from generator Gi to be sentto other data groups for subsequent training in the very next augmentation epoch. When the totalnumber of samples to be collected from generators are fixed, this method enables generators withbetter realistic image generation power to contribute more to the future training data groups. Morerealistic training sets thus augmented, in turn, exert more positive influence on the images to begenerated. Algorithm 1 illustrates the whole 3-step process, which we denote as PAGDA(ParallelAdaptive Generative Data Augmentation).
Figure 4: Accuracy with different number of folders4.3	Evaluation on New Image GenerationAlong with the task of classification, we considered the task of new image generation from giventraining images. We have applied similar settings as in image classification, and the goal is to6Under review as a conference paper at ICLR 2020produce images with higher resolution and more authentic semantics with generative adversarialnetworks. In our setting, we first train the state-of-the-art WGAN to produce images from an unaug-mented dataset, and repeat the procedure on an image data set that has been augmented.
Figure 5: Augmented GAN at Figure 6: Unaugmented GAN Figure 7: Unaugmented GANEpoch 271,Reduced-CIFAR at Epoch 361,Reduced-CIFAR at Epoch 342, RedUced-CIFAR4.4	Evaluation on InpaintingFor the task of inpainting, we aUgment the redUced Places dataset constrUcted in the experiment. Aswe mentioned 4.1, we apply 3 sUbset named with Ocean, Orchard and Piers, each sUbset contains5000 images. WithoUt loss of generality, we train a WGAN-GP model as generative model forinpainting from the aUgmented dataset. We then select testing images that are not selected in thetraining set, and add to them gray masks covering the center part of these images. We then appliedoUr trained WGAN-GP to generate patches that cover the masked portion of the inpainting image.
Figure 10: Inpainting results on reduced datasetsThe inception score(IS) and Frechet Inception Distance (FID) we used in all experi-ments are calculated by Python scripts available respectively at https://github.com/openai/improved-gan/tree/master/inception_score and https://github.
