Figure 1: GAIL and TRAIL succeed at lifting (a), but when distractor objects are added, GAIL failswhile TRAIL succeeds (b). Due to robustness to initial conditions, TRAIL can stack from pixels whilestandard GAIL fails (c). We witness this difference again in insertion with distractors (d). A videoshowing agents performing these tasks can be seen at https://youtu.be/Rz5G15rDKcg .
Figure 2: Illustration of several task-irrelevant changes between the expert demonstrations and thedistribution of agent observations, for the lift (red cube) task. The naively-trained discriminatornetwork will use these differences rather than task performance to distinguish agent and expert.
Figure 3: Results for lift alone, lift distracted, and lift distracted seeded. Only TRAIL excels.
Figure 4: Lift red block, where expert has a different body appearance, and with distractor blocks.
Figure 5: Demonstrating the memorization problem on the lift distracted task (here higher accuracyis worse). Accuracy of different discriminator heads is presented (A-D). In A, the overall accuracyfor all timesteps. Then main (m) and extra (e) heads accuracy for the first steps are presented in Band C, respectively. Accuracy of the head predicting randomly assigned demonstration class is shownin D. Average discriminator predictions for training and holdout demonstration are shown in E and F.
Figure 6: Results for lift, box, and stack on Jaco environment.
Figure 7: When the expert differs in body or prop appearance, TRAIL outperforms GAIL.
Figure 8: Results comparing TRAIL, TRAIL-0 and GAIL for diverse manipulation tasks.
Figure 9: Two work spaces, Jaco (left) which uses the Jaco arm and is 20 × 20 cm, and Sawyer(right) which uses the Sawyer arm and more closely resembles a real robot cage and is 35 × 35 cm.
Figure 10: Illustration of the pixels inputs to the agent.
Figure 11: TRAIL performance, varying the number of first frames in each episode Used to form I.
Figure 13: Results for stack in Jaco work space. Fixed step termination policy can be very effectivebut the final performance is very sensitive to the hyperparameter. TRAIL-0 does not need tuning noraccess to the environment reward.
Figure 14: Results for lift alone and lift distracted in Sawyer work space. TRAIL and TRAIL-0 areby default with data augmentation. Additional results for these methods without data augmentationare presented to show its importance.
Figure 15: Network architecture for the policy and critic.
Figure 16: With fixed rewards, the agent is able to learn lift alone somewhat, but performs worse thanTRAIL. When distractor blocks are added, the fixed reward agent fails to learn completely.
