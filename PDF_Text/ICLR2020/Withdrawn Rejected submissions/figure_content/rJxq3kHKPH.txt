Figure 1: Different stages in the presence of label noise. (a) We plot 'total (total loss), 'clean (loss on DClean),and 'corrupt (loss on Dcorrupt), We see that during the gap stage, there is a clear “gap” between the 'cleanand 'corrupt where training on clean labels has completed but training on noisy labels has barely started; (b)Hypothesized qualitative division of the three stages: fast learning stage, gap stage, and the memorization stage.
Figure 2: Training trajecto-ries on the gambler’s loss us-ing different optimizers, withr=0.5.
Figure 3: Critical behavior of the gambler’s loss. Data showing that the learning almost do not happenat all for o < ocrit , while above ocrit the behavior is qualitatively similar. The optimal o can be tunedfor using this phenomenon; however, more often one does not need to tune for o.
Figure 4: Testing accuracythrough out training; raw referstraining on the default loss func-tion, i.e. the nll loss, raw is plot-ted in red. We see that, wheno > ocrit, the gambler’s loss pro-vides robustness on its againstnoisy labels.
Figure 5: Early stopping on MNIST (1st row) and CIFAR10 (2nd row). r refers to corruption rate. Thehorizontal line is the predicted early stopping point. We see that this point corresponds to where the testingaccuracy (blue solid line) is at maximum.
Figure 6: Training accuracy and train-ing loss On MNIST with corruption rate0.8 with Adam. o = 9.7. Training withgambler’s loss prevents memorizationof noisy labels. At convergence, nll lossreaches 19% testing accuracy, while thegambler’s loss stays around 76%.
Figure 7: Early stopping on MNIST (1st and 2nd row) and CIFAR10 (3rd and 4th row). r refersto corruption rate. The horizontal line is the predicted early stopping point. We see that this pointalmost always corresponds to where the testing accuracy (vertical blue solid line) is at maximum.
Figure 8: critical behavior of the gambler’s loss. We see that the learning almost do not happen at allfor o < ocrit , while above ocrit the behavior is qualitatively similar. The optimal o can be tuned forusing this phenomenon; however, more often one does not need to tune for o.
Figure 9: Gambler’s loss is used alone without early stopping method. Initially, the performancegets better with the o decreasing. While the performance gets worse with the o decreasing when o isless than 8.5, which indicates there is a critical point for o locates in between 8.0 and 8.5. The mostsuitable o is just larger than the critical point(e) o = 8.15(f) o = 8.1(g) o = 8.05(h) o = 8.0Figure 10: Under higher resolution on hyperparameter o, it’s for sure that the performance will getbetter when o gets closer to critical point from right side on number axis. While the performance willdramatically get terrible when o is just lower than the critical points. What’s more, the critical pointis in the range of [8.2, 8.25]16Under review as a conference paper at ICLR 2020Figure 11: put all curve of Figure 8 in one Figure17Under review as a conference paper at ICLR 202050 ~∣	,	,0	5	10	15epoch
Figure 10: Under higher resolution on hyperparameter o, it’s for sure that the performance will getbetter when o gets closer to critical point from right side on number axis. While the performance willdramatically get terrible when o is just lower than the critical points. What’s more, the critical pointis in the range of [8.2, 8.25]16Under review as a conference paper at ICLR 2020Figure 11: put all curve of Figure 8 in one Figure17Under review as a conference paper at ICLR 202050 ~∣	,	,0	5	10	15epoch(a) Training on IMDB dataset (r = 0.1) with LSTM withpretrained GloVe embedding.
Figure 11: put all curve of Figure 8 in one Figure17Under review as a conference paper at ICLR 202050 ~∣	,	,0	5	10	15epoch(a) Training on IMDB dataset (r = 0.1) with LSTM withpretrained GloVe embedding.
Figure 12: Three stage phenomenon across different datasets and architecturesJ	More evidence on the three s tage phenomenonThe three stage learning curve observed in section 1 is actually quite universal when significant levelof label noise is present. Of course, the exact details of the curve really depends on the architecuresand the tasks. See Figure 12. The left pictures shows training of an LSTM with hidden size 300with pretrained GloVe embedding (Pennington et al., 2014) on the IMDB dataset (Maas et al., 2011).
