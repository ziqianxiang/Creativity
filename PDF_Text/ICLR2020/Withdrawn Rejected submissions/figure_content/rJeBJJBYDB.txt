Figure 1: Left: Data on a double torus. Middle two: Data autoencoded to a flat latent space. Right:Data autoencoded to a 4-chart latent space.
Figure 2: Left: Illustration of a manifold. Right: Possible parameterizations of a circle. The mani-fold approach (bottom) preserves all desired properties.
Figure 3: Architecture diagram of CAE and transition functions. The red path illustrates the computation oftransition function φ12.
Figure 4: Left: Latent space. Middle: Model with Lipschitz Regularization. Right: UnregularizedModelIn our first experiment illustrated in Figure 4, we visualize the process of applying a four-chart CAEto a data set sampled from the unit sphere (see Appendix A.2 for the network architecture). Weapply the proposed loss function with and without the Lipschitz regularization discussed in section4.2. We use four copies of (0, 1)2 as the chart latent space in this experiment. We color code (0, 1)2using the distance of each point to the origin. After training, we uniformly sample points on thelatent space and use the learned decoders to generate points back to the unit sphere. As we can seefrom the middle panel of Figure 4, the four charts, when glued together, successfully cover the unitsphere. Moreover, all charts occupy the data manifold in a balanced and regularized way; that is,even thought they are not uniform, no single chart dominates the rest. From the right panel of Figure4, we can see that, when no regularization is employed, the charts are less localized. This behaviorshows the necessity of using Lipschitz regularization to control the regularity of the decoder.
Figure 5: Left: Points sampled from high confidence regions. Right: Segmentation of manifoldfrom chart selectionNext, we test our CAE on a genus-3 surface with ten 2-dimensional charts (detailed as CAE 2 inA.2). The left of Figure 5 shows the result of randomly sampling zα in the chart latent space U,and decoding the latent representations. The right of this figure shows which chart is active in eachregion of the manifold. Since this model uses a network to predict the chart segmentation, theresulting parameterization has charts of varying sizes. This allows the network to place more chartsin areas of high curvature, and allow charts to grow over more linear regions. Nevertheless, thisexample demonstrates the effectiveness of our method to handle objects with complex topology.
Figure 6: Left: Reconstruction of data from MNIST data set. Middle: Reconstruction from randomsample on the multi-chart latent space. Right: Digit morphing5.3	Model EvaluationIn this experiment, we apply four traditional models (2 auto-encoders and 2 variational auto-encoders) as well as three CAEs on several data sets. Details of the exact architecture of thesenetworks can be found in A.2. For each model and data set, we are primarily interested in threemeasures of success, including reconstruction error, unfaithfulness, and coverage (See A.10 for de-tailed definitions). The reconstruction error measures the fidelity of the model. The unfaithfulnessmeasures how far synthesized data decoded from samples drawn on the latent space are to samplesfrom the original training data. Coverage indicates how much of the training data is covered by theencoder. Models which produce unrealistic data when sampling from the latent space will have highunfaithfulness sores and models which experience mode collapse will have low coverage scores.
Figure 7: Summary of benchmark test on Sphere, Genus-3, MNIST and SVHN data setsautoencoders. We also apply our method to real-life data sets, including MNIST and SVHN, todemonstrate the effectiveness of the proposed model.
Figure 8: Top row: Embedding of Charts and Maximum of Patch Prediction. Second row: charttransitions. Training data is marked with ‘+’ and samples taken from the charts are denoted by thesolid colored line. The black dash line shows the ground truth. Bottom: Partition of Unity functionsA.8 Automatic Chart RemovalIn this experiment, we train a model with four 2-dimensional charts for data sampled on a sphere inorder to visualize the effect of the regularization scheme when using a neural network as the chartprediction module. Figure 9 shows the patch prediction function on the training data at the end of thepre-training and at the completion of training, respectively. From this figure we see that even thoughall charts cover the sphere at the beginning, the regularization is able to automatically remove somecharts after training.
Figure 9: Left: Initial Predictions . Right: Final regionsA.9 Measuring GeodesicsIn this experiment, we demonstrate a simple example of recovering geometric information from atrained model by computing the length of some geodesic curves on a sphere. Let p and q be pointson the manifold with latent representations zp and zq. Then the line γz (t) = αzp + (1 - α)zqin the latent space will correspond to a path on γ ⊂ M. To measure the length of this path, wecan sample points along γz (t), decode them and then measure the euclidean distance between thedecoded points. Figure 10 shows an example of such a test using different numbers of samplingpoints for five difference curves on the sphere. From this experiment we observe convergence ofthese measurements as more points are sampled along the geodesic path, validating our geometricintuition. We remark that this is a very preliminary result to show a potential of understandinggeometric structure of data manifold using multi-chart latent space. We will explore in the directionin our future work.
Figure 10: Left: Geodesic approximation error v.s. number of points sampled in the latent space.
Figure 11: Five overparametized VAE with 2 dimensional latent space for data sampled doubletorus. Blue:training data, Red:samples from latent space.
