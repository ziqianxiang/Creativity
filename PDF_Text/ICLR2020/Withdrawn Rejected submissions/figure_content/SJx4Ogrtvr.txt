Figure 1: Activated and non-activated regions of ReLU (left panel). Activated region of ReLU atinitialization (right panel).
Figure 2: Geometric behavior of ReLu during forward pass, trained hyperplanes (left panel) andtheir geometry (right panel).
Figure 3: Activated region and non-activated region of htanh activation function(left panel). Acti-vated region of Hard Tanh at initialization (right panel)of a data point Xi is small enough, it can activate all hyper-planes in the same layer, see the deep-red region of Figure 3 (right panel). As a consequence, in backward gradients, few data instancesaffect all hyper-planes. In other words, the backward gradients from a part of the training datapoints have a larger impact on the model than the others. We considered this imbalance ultimatelyaffects model generalization problem since the model training focuses on a subset of the trainingdata points close to the origin. iii) Hyperplane equality: The initial activated regions should cover asimilar-sized subset of the training data points overall, and this property is shared in both ReLU andhtanh activations.
Figure 4: Training of full-precision ResNet architecture (left panel) Binary VGG-7 architecture(middle panel), and Binary ResNet (right panel)6Under review as a conference paper at ICLR 2020with ReLU activation and BNN from 4% to 1.5%. The bias initialization strategy is effective toclose the gap on binary ResNet architecture by almost 1%, even while the full-precision model evenunder-fits on CIFAR10 data.
