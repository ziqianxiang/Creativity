Figure 1: Left: Open labyrinth - A 21 × 21 empty labyrinth environment. Right: 4-room labyrinth -A 21 × 21 4-room labyrinth environment.
Figure 2: (a): A learned abstract representation of the open labyrinth environment from Figure 1aafter 500 environment steps. Colors denote which side of the maze the agent was in, grid coordinatesand transitions are shown. (b): Two views of the same learned abstract 3-dimensional representationof our multi-step maze after 300 steps. Orange and blue points denote states with and without keysrespectively. Our agent is able to disentangle states where the agent has a key (z = 1) and whenit doesn’t (z = 0) as seen in the distance between orange and blue states. Meaningful informationabout the agent position is also maintained in the relative positions of states in abstract state space.
Figure 3: An example of the state counts of our agent in the open labyrinth with d = 5 step plan-ning. Titles of each subplot denotes the number of steps taken. The brightness of the points areproportional to the state visitation count.
Figure 4: Labyrinth results for both open labyrinth and 4-room labyrinth over 3 trials.
Figure 5: Illustrations of the Acrobot and multi-step goal maze environments. b) Left: The passage-way to the west portion of the environment are blocked before the key (black) is collected. b) Right:The passageway is traversable after collecting the key, and the reward (red) is then available. Theenvironment terminates after collecting the reward.
Figure 6: Illustration of a simplistic 3 × 3 grid-like environment, with A = {up, down, left, right}and distance 1 between neighboring states. All actions which lead out of the grid are no-ops.
Figure 7: Ablation results for unweighted versus weighted nearest neighbor heuristics (over 3 trials).
