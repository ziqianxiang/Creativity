Figure 1: Latent Action Neural Archi-tecture Search: Starting from the entiremodel space, at each search stage welearn an action (or a set of linear con-straints) to separate good from bad mod-els for providing distinctive rewards forbetter searching.
Figure 2: Illustration of motivation: (a) visualizes the MCTS search trees using sequential andglobal action space. The node value (i.e. accuracy) is higher if the color is darker. (b) For a givennode, the reward distributions for its children. d is the average distance over all nodes. globalbetter separates the search space by network quality, and provides distinctive reward in recognizing apromising path. (c) As a result, global finds the best network much faster than sequential.
Figure 3: An overview ofLaNAS: LaNAS is an iterative algorithm in which each iteration comprisesa search phase and learning phase. The search phase uses MCTS to samples networks, while thelearning phase learns a linear model between network hyper-parameters and their accuracies.
Figure 4: Evaluations of search dynamics:(a) KL-divergence of Pj and Pj dips and bounces back.
Figure 5: Evaluations of sample-efficiency on NASBench, ConVNet-60K and LSTM-10K. Eachsearch algorithm is repeated 100 times on each datasets with different random seeds. The top rowshows the time-course of the search algorithms (current best accuracy with interquartile range), whilethe bottom row illustrates the number of samples to reach the global optimum. NASBench-420Klimits the edges ≤ 9 and LSTM-10K limits the edges = 9, rendering a huge number of architecturesuntrained in the search space. We compared to SMAC and TPE on ConvNet-60k as ConvNet-60kcontains the accuracy of all the architectures in the search space, .
Figure 6: (a-b) Comparing LaNAS to several mainstream BO methods: though BO and TPEworks well when samples ∈ [0, 200] on ConvNet and when samples ∈ [0, 500] on NASBench-Node6 (a subset of NASBench containing 62010 6-nodes networks), LaNAS quickly dominatesthe performance afterwards. This highlights the different focuses of LaNAS and BO methods. (c-d)illustrations of the limitation of BO methods: optimizing the acquisition function with TreeParzen Estimator (TPE) and Regularized Evolution (RE) using different budgets, that represent themax number of queries to a surrogate model in optimizing the acquisition. We denote the budgets asthe percentages of ConvNet dataset.
Figure 7: Ablation study: (a) the effect of different tree heights and #Select in MCTS. Numberin each entry is #SamPleS to reach global optimal. (b) the choice of predictor for splitting searchspace. (c) the effect of C in UCB toward the performance on nasbench. (d) the effect of #SamPIeS forinitialization toward the search performance.
Figure 8: Latent actions transfer: learned la-tent actions can generalize within the same taskor across different tasks, to further boost searchefficiency.
Figure 9: previous Fig.5 in log scale.
