Figure 1: First column: attentional regions indicated by filter patterns. Second to fourth column:filter visualization by maximum activated images. Last two columns: 2D (top row) and 3D (bottomrow, height indicating activation strength) feature map visualization. It,s clear that max poolingproduces sparser feature maps and encourages part-level features to be learned.
Figure 2: Left and middle: discrete entropy (plotted in log scale) and thresholed lo norm for variedk in k-max pooling. Feature maps become more densly activated when k-max pooling transformsfrom max to average pooling (from k=1 to k=196). Right: model accuracy against input resolutions.
Figure 3: Model accuracy on training and testing set for k-max (left), mixed (middle) and lp norm(right) pooling with varied k, Î± and p. Shaded area indicates the standard deviation obtained frommultiple runs. Our observation is: max pooling is consistently better than average pooling; modelaccuracy increases approximately monotonically when generalized pooling changes from averageto max pooling; and max pooling generalizes better than average pooling.
