Figure 1: Intrinsically Motivated Perception and Action Cycle. Left-to-right: (a) the observationswith the corresponding action sequences are embedded to the latent space by DNNs, where thefinal states are constrained to be linear in the action sequences (non-linear in the initial states). (b)Gaussian channel capacity between the embedded actions and the embedded final states is computedefficiently by the ‘Water-filling’ algorithm. (c) an intrinsic reward is derived, triggering the agent toimprove its policy for better reward. (d) when the agent operates under the new policy, additionalobservations from the environment are perceived by the sensors, closing the loop.
Figure 2: An illustration of data flow around a single training sample.
Figure 3: Reconstruction through the latent dynamics in comparison with the ground truth. Startingfrom the same upright position, a different action sequence is taken in each row. In all 3 cases,reconstruction matches the actual observation after the actions are taken.
Figure 4: Left: Optimal trajectory super-imposed on our empowerment landscape. The pendulumswings up from bottom (π, 0) to balance at the top (0, 0).Right: Empowerment values calculatedwith knowledge of the ground truth model (Salge et al., 2013a). This figure shows that the empow-erment landscapes are qualitatively similar, which verifies the validity of our approach.
Figure 5: Left sub-figure: Double tunnel environment. The goal is marked in red. The agent inblue. Right sub-figure: Empowerment landscape for the tunnel environment. The values of em-powerment reduce at the corner and inside the tunnel where the control of the agent is less effectivecompared to more open locations.
Figure 6: Trajectories of trained policy. As β increase, the agent develops a stronger preference overa safer route, sacrificing hitting time.
