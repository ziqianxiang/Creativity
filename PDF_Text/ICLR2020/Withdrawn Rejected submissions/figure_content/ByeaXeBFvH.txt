Figure 1: Existing distillation methods compared to Hydra. Knowledge distillation, (Hinton et al.,2015), trains a distillation network to imitate the prediction of a larger network. Applying knowledgedistillation to ensemble models Hinton et al. (2015) train a network to imitate the average ensembleprediction. Hydra instead learns to distill the individual predictions of each ensemble member intoseparate light-weight head models while amortizing the computation through a shared heavy-weightbody network. This retains the diversity of ensemble member predictions which is otherwise lost inknowledge distillation.
Figure 2: Model uncertainty, total uncertainty and expected data uncertainty applied on in-domainand out-of-distribution data for both (a) ensemble and (b). The original dataset is visualized (c),where each color corresponds to a single class.
Figure 3: Model evaluation metrics plotted against intensity of distributional shift for CIFAR-10.
