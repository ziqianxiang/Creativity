Figure 1: (a) Given a pair of sensory views, a deep representation is learnt by bringing views of the samescene together in embedding space, while pushing views of different scenes apart. Here we show an exampleof learning from the luminance channel (L) of an image and the ab-color channel. The strawberry,s L and abchannels embed to nearby points whereas the ab channel of a different image (a photo of blueberries) embeds toa far away point. (b) Example of a 4-view dataset (NYU RGBD (Nathan Silberman & Fergus, 2012)) and itslearned representation. Dotted lines represent the contrastive objective. The encodings for each view may beconcatenated to form the full representation of a scene.
Figure 2: Predictive Learning vs Contrastive Learn-ing. Cross-view prediction (Top) learns latent repre-sentations that predict one view from another, withloss measured in the output space. Common pre-diction losses, such as the L1 and L2 norms, areunstructured, in the sense that they penalize eachoutput dimension independently, perhaps leadingto representations that do not capture all the sharedinformation between the views. In contrastive learn-ing (Bottom), representations are learnt by contrast-ing congruent and incongruent views, with loss mea-sured in representation space. The red dotted out-lines show where the loss function is applied.
Figure 3: Graphical models and infor-mation diagrams (inf) associated withthe core view and full graph paradigms,for the case of 4 views, which gives atotal of 6 learning objectives. The num-bers within the regions show how much“weight” the total loss places on each par-tition of information (i.e. how many ofthe 6 objectives that partition contributesto). A region with no number corre-sponds to 0 weight. For example, in thefull graph case, the mutual informationbetween all 4 views is considered in all6 objectives, and hence is marked withthe number 6.
Figure 4: We show the Intersection over Union (IoU) (left) and Pixel Accuracy (right) for the NYU-Depth-V2dataset, as CMC is trained with increasingly more views from 1 to 4. As more views are added, both thesemetrics steadily increase. The views are (in order of inclusion): L, ab, depth and surface normals.
Figure 5: How does mutual information between views relate to representation quality? (Left) Classificationaccuracy against estimated MI between channels of different color spaces; (Right) Classification accuracy vsestimated MI between patches at different distances (distance in pixels is denoted next to each data point). MIestimated using MINE (Belghazi et al., 2018).
Figure 6: We plot the number of negative examples m in NCE-based contrastive loss against the accuracy for100 randomly chosen classes of Imagenet 100. It is seen that the accuracy steadily increases With m.
