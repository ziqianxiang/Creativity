Figure 1: True Bounds in Expectation. As shown in Figure 1a, our proposed interval bounds [LM, UM], aspredicted by Theorem 1, get closer to being a true super set to the true interval bounds [Ltrue, Utrue] estimated byMonte-Carlo Sampling as the input dimension n increases regardless of the number of hidden nodes. Figure 1bshows that a similar behaviour is present even under varying network depth.
Figure 2: Tighter than IBP with Varying Input Size and Hidden Nodes. We show a bound tightnesscomparison between our proposed interval bounds and those of IBP by comparing the difference and ratio oftheir interval lengths with varying k, n, and for a two-layer network. The proposed bounds are significantlytighter than IBP, as predicted by Theorem 2.
Figure 3: Tighter than IBP in Deeper Networks. We show a bound tightness comparison between ourproposed interval bounds and those of IBP by varying the number of layers for several choices of . Theproposed bounds are significantly tighter than IBP.
Figure 4: Qualitative Results. We plot visualizations of the output polytope of a 20-100-100-100-100-2network through Monte-Carlo evaluations of the network with a uniform random input with varying . We alsoplot our proposed bounds [LM, UM] in red. Each row is for a given with different randomized weights for thenetwork. As for the IBP bounds [LIBP, UIBP], they were omitted as they were significantly larger. For example,for the first figure with = 0.05, IBP bounds are [-43.7, 32.9] for the x-axis and [-47.8, 37.0] for the y-axis.
Figure 5:	Better Test Accuracy and Robustness on MNIST. We compare the PGD robustness and testaccuracy of three models (small, medium, and large) robustly trained on the MNIST dataset using our boundsand those robustly trained With IBP. We have trained both methods using four different train, but We eliminated allmodels With test accuracy loWer than 97.5%. Our results demonstrate an impressive trade-off betWeen accuracyand robustness and, in some cases (medium and large models), We excel in both.
Figure 6:	Better Test Accuracy and Robustness on CIFAR10. We compare the PGD robustness and testaccuracy of three models (small, medium, and large) robustly trained on the CIFAR10 dataset using our boundsand those robustly trained with IBP. We eliminated all models with test accuracy lower than 40.0%. PGDrobustness is averaged over multiple test (refer to appendix).
Figure 7: Each row represents 5 different randomly initialized networks for a given with n = 2.
Figure 8: Each row represents 5 different randomly initialized networks for a given with n = 10.
Figure 9: Each row represents 5 different randomly initialized networks for a given with n = 20.
Figure 10: Compares PGD (test = 0.1) and test accuracy of our models against IBP on MNIST.
Figure 11: Compares PGD (test = 0.2) and test accuracy of our models against IBP on MNIST.
Figure 12: Compares PGD (test = 0.3) and test accuracy of our models against IBP on MNIST.
Figure 13: Compares PGD (test = 0.4) and test accuracy of our models against IBP on MNIST.
Figure 14: Compares PGD (test = 2/255) and test accuracy of our models against IBP on CIFAR10.
Figure 15: Compares PGD (test = 8/255) and test accuracy of our models against IBP on CIFAR10.
Figure 16: Compares PGD (test= 16/255) and test accuracy of our models against IBP on CIFAR10.
Figure 17: Compares PGD (test = 0.1) and test accuracy of our models against IBP on CIFAR10.
Figure 18: Histogram of weights of medium CNN trained on MNIST without `2 regularization.
Figure 19: Histogram of weights of medium CNN trained on MNIST with `2 regularization.
Figure 20: Histogram of weights of medium CNN trained on CIFAR10 without `2 regularization.
Figure 21: Histogram of weights of medium CNN trained on CIFAR10 with `2 regularization.
Figure 22: Histogram of weights of medium CNN trained on CIFAR100 without `2 regularization.
Figure 23: Histogram of weights of medium CNN trained on CIFAR100 with `2 regularization.
