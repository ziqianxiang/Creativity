Figure 1: Pipeline for the CNNs used for the study. The bottleneck features (the flattened out-put of the final Convolutional layer) are later extracted and used as a model’s visual representation(dim=1568). The final predicted output, ”Label Vector” is either one-hot or word embedding ac-cording to the model’s target labels.
Figure 2: Cosine similarity matrix visualizing relationships between between 30 basic levelcategories. Lighter yellow colors denote higher similarity, and darker purple colors denote lowersimilarity. Categories from the same superordinate class are located near to each other in xtickswith the order of ’mammal’, ’bird’, ’insect’, ’fruit’, ’vegetable’, ’vehicle’, ’container’, ’kitchenappliance’, ’musical instrument’, and ’tool’.
Figure 3: Comparison of Triplet Prediction Accuracy. IMAGENET: when using categorical rep-resentations averaged over the IMAGENET training dataset (〜1000 images per category); THINGS:when using categorical representation averaged over the THINGS dataset (〜10 images per cate-gory); Single Exemplar: when using categorical representation extracted from the single image usedfor behavioral data collection. Baseline accuracies are indicated by the dashed lines.
