Figure 1: We train VGGNet on CIFAR-10 with various ‘Norm+ReLU’ blocks (details about trainingsetings are provided in Sec.F of Appendix). The inhibited channel ratio is defined as the averagepercentage of values less than 1e-2 in the first six feature maps of VGGNet. (a) & (b) show that in-hibited channels emerge in many ‘Norm+ReLU-like’ blocks such as ‘BN+ReLU’, ‘LN+ReLU’ and‘BN+ELU’. (c) plots cumulative ablation curves that is a technique demonstrating channel equal-ization (Morcos et al., 2018). Equipped with CE, both ‘BN+ReLU’ and ‘BN+ELU’ presents a moregentle drop curve of top-1 accuracy versus cumulative ablation of channels, implying that CE helpschannel equalization.
Figure 2: Illustrations of SE block (HU et al., 2018) (a), CE block (b) and CE residual block inResNet (c). Θ denotes broadcast element-wise multiplication,㊉ denotes broadcast elementwiseaddition and 0 denotes matrix multiplication. (b) shows CE has two lightweight branches, BN andAII. (c) shows CE can be easily stacked into many advanced networks such as ResNet with merelysmall extra computation.
Figure 3: (a) & (b) show channel drop ratios versus top-1 accuracy for MobileNet V2 and ResNet50on ImageNet dataset respectively. We randomly ablate channels with an increasing fraction in thefirst normalization layers. Note that CE-ResNet50 and CE-MobileNet V2 are more robust to cumu-lative ablation of channels than those networks trained with only BN, suggesting that CE also helpschannels equalization on ImageNet. For (c) and (d), we train VGGNet in CIFAR-10 under differentweight decays. It is observed that the networks trained with the proposed BD and CE were consis-tently and substantially more robust to the increasing strength of weight decay than those trainedwith single BN.
Figure 4: We do principal component analy-sis (PCA) on the input of AII sub-network, thevariance of each channel. This figure show thebox chart of principal components. CENet haslower means and variances than AIINet, indi-cating the input of AII sub-network in CENetis more equal and informative.
Figure 5: Training and validation error curves on ImageNet with ResNet50 as backbone for BN, SEand CE.
Figure 6: (a) compares the cumulative ablation curves of ’BN+ReLU’, ’BN+ELU’, ’BN+LReLU’and ’BN+CE+ReLU’ with VGGNet on CIFAR-10 dataset. We see that the Both LReLU and CEcan improve the channel equalization in ’BN+ReLU’ block. (b) compares the cumulative ablationcurves of ’BN+ReLU’, ’SN+ReLU’ and ’BN+CE+ReLU’ with ResNet-50 on ImageNet dataset.
Figure 7: Grad-cam visualization results from the final convolutional layer for plain ResNet50,SE-ResNet50, and CE-ResNet50.
