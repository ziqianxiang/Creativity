Figure 1: An illustration of how the proposed constraint drew inspiration from BNNs and bipartite graphs. (a)neuron correlations in BNNs correspond to connections between dendrites, which are represented by blue lines,and axons, which are represented by red lines. (b) and (c) analogy of figure (a) represented as connectionsbetween layers in DNNs; although nodes i and j cannot form direct links, they can be correlated by a given nodek as a first-order correlation, or by two nodes k and m as a second-order correlation which is also equivalent to a4-cycle in bipartite graphs. (d) an example of a learned neuronal assembly in neurons outgoing weight space,with the dimensionality reduced to 2D with T-SNE (Maaten & Hinton (2008)). Each point represents one neuronand the neurons are colored according to their highest activated class in the test data.
Figure 2:	Neuronal assembly patterns found in neurons’ weight space of the dense layer of different models on both MNIST (top) and CIFAR-10 (bottom) datasets, along with clustering validation via Silhouette score on 10 clusters K-means clustering. The dimensionality of neurons’weight space was reduced to 2D with T-SNE for visualization.
Figure 3:	Neuron co-activation patterns found in the representation of the last dense layer of LeNet-5+BEAN-2 model. The dimensionalityof neurons’ weight space was reduced to 2D with T-SNE for visualization. Each point represents one neuron within the last dense layer of themodel and is colored based on its activation scale. The 10 subplots show the average activation heat-maps when each digit’s samples were fedinto the model. The warmer color indicates a higher neuron activation.
Figure 4: The strong association between neuronal assemblies and neurons’ class selectivity index with BEAN regularization. Each pointrepresents one neuron and the color represents the class where the neuron achieved its highest class-conditional mean activity in the test data.
Figure 5: The ablation study at the neuron population level of the last dense layer of LeNet-5 models. Each time, one distinct group of neuronswere ablated based on their most selective class and the model performance changes for each individual class were recorded.
Figure 6: Analysis and visualization of the last dense layer of LeNet-5+BEAN-2 model on MNIST 10-shot learning from scratch task, BEANregularization helped dense layer form efficient parameter usage via sparse and structured connectivity learning and weak parameter sharingamong neurons. (a) The heat-map of the learned second-order neuron correlation matrix, neuron indices are re-ordered for best visualization ofneuronal assembly patterns, BEAN is able to form plausible assembly structures even with such extreme limited sample size. (b) Visualizationof the sparse and structured connectivity learned in dense layer, neurons are grouped and colored by neuronal assembly. (c) Visualization ofthe scales of neruons’ out-going weights, the weights of neuron are colored to be consistent with neuron group in (b).
