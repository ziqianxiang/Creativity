Figure 1: A large |emin | can givebad approximation. Orange and greentriangles shoW loss samples used infitting and testing respectively. Redcircle shoWs the minimum loss valueof the quadratic, and purple triangleshoWs the true loss there.
Figure 2: SGD training loss for Cifar-10at LR of 0.1. Loss stagnates after â‰ˆ 50epochs.
Figure 3: SGD training loss for Cifar-10 on Resnet-18. Plots are split across two to permit highery-axis fidelity. In the orange plot, a fixed LR of 0.1 is used, while in the blue plot the LR is reducedfrom 0.1 to 0.01 at epoch 50. Clearly, at epoch 50, it locally makes sense to reduce the LR as itincreases the rate of descent significantly.
Figure 4: Generalization accuracies and learning rates. AutoLR plots are show in blue, and baselinein orange. Learning rates are plotted with dashed lines with y-scale on the right, while generalizationaccuracies are plotted with solid lines with y-scale on the left. The generalization metrics plottedare (a) test top-5 accuracy (b) test accuracy (c) test EM score, and (d) validation perplexity.
Figure 5: Quadratic approximation of loss as a function of . Shown are two examples from Cifar-10on Resnet-18 runs where the minimum is (a) within and (b) outside the range of loss samples. Theorange triangles show loss samples used in fitting the quadratic, the blue line shows our quadraticapproximation, and the green triangles show more loss samples which were not used for fitting. Thered circle shows the minimum loss value as per the quadratic, while the purple triangle show the truevalue at that .
Figure 6: Quadratic approximation curve samples from (a) ImageNet on Resnet-50, (b) IWSLT onTransformer and (c) SQuAD fine-tuning on BERT. The legend is same as figure 5. It can be seenthat the quadratic approximation works pretty well.
Figure 7: Standard deviation of loss as a function of superbatch size for (a) Cifar10 on Resnet-18,and (b) MNIST.
Figure 8: LR Range test for selecting the maximum learning rate. A good choice is the learning rateis a bit before the minima in a region where the loss is still decreasing.
Figure 9: ImageNet on Resnet-50 trained with Momentum. Shown are the training loss, top-1/top-5 test accuracy and learning rate as a function of epochs, for the baseline scheme (orange) vs theAutoLR scheme (blue). The plot is split into 3 parts to permit higher fidelity in the y-axis range.
Figure 10:	Cifar-10 on Resnet-18 trained with SGD. Shown are the training loss, test accuracy andlearning rate as a function of epochs, for the baseline scheme (orange) vs the AutoLR scheme (blue).
Figure 11:	Cifar-10 on Resnet-18 trained with Momentum. Shown are the training loss, test accuracyand learning rate as a function of epochs, for the baseline scheme (orange) vs the AutoLR scheme(blue). The plot is split into 3 parts to permit higher fidelity in the y-axis range.
Figure 12:	Cifar-10 on Resnet-18 trained with Adam. Shown are the training loss, test accuracy andlearning rate as a function of epochs, for the baseline scheme (orange) vs the AutoLR scheme (blue).
Figure 13:	IWSLT on Transformer network trained with Adam. Shown are the training perplexity,validation perplexity and learning rate as a function of epochs, for the baseline scheme (orange) vsthe AutoLR scheme (blue). The plot is split into 3 parts to permit higher fidelity in the y-axis range.
Figure 14: SQuAD fine-tuning on BERT trained with Adam. Shown are the training loss, test EMscore, and learning rate as a function of epochs, for the baseline scheme (orange) vs the AutoLRscheme (blue). The plot is split into 2 parts to permit higher fidelity in the y-axis range. It is clearthat with AutoLR the network starts to overfit after the 2nd epoch, where the testing loss continuesto go down, but generalization suffers. We saw similar behavior with different seeds, and thus needto train with AutoLR for only 2 epochs.
Figure 15: MNIST on Resnet-18 trained with SGD. Shown are the training loss, test accuracy andlearning rate as a function of epochs, for the baseline scheme (orange) vs the AutoLR scheme (blue).
Figure 16: MNIST on Resnet-18 trained with Momentum. Shown are the training loss, test accuracyand learning rate as a function of epochs, for the baseline scheme (orange) vs the AutoLR scheme(blue). The plot is split into 2 parts to permit higher fidelity in the y-axis range.
Figure 17: MNIST on Resnet-18 trained with Adam. Shown are the training loss, test accuracy andlearning rate as a function of epochs, for the baseline scheme (orange) vs the AutoLR scheme (blue).
Figure 18: Fashion MNIST trained with SGD. Shown are the training loss, test accuracy and learningrate as a function of epochs, for the baseline scheme (orange) vs the AutoLR scheme (blue). Theplot is split into 2 parts to permit higher fidelity in the y-axis range.
Figure 19: Fashion MNIST trained with Momentum. Shown are the training loss, test accuracy andlearning rate as a function of epochs, for the baseline scheme (orange) vs the AutoLR scheme (blue).
Figure 20: Fashion MNIST trained with Adam. Shown are the training loss, test accuracy andlearning rate as a function of epochs, for the baseline scheme (orange) vs the AutoLR scheme (blue).
