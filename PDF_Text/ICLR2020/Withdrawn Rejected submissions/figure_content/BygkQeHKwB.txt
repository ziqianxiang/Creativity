Figure 1: Adversarial attacks on a binary classifier in two dimensions. The two class regions areshown in red and blue. Contours indicate class probabilities. The objective is to find a point in thered (adversarial) region that is at the minimal distance to input x. Gray (black) paths correspondto low (high) distortion target e for PGD2 (Kurakin et al., 2016) (a, in green) or parameter λ forC&W (Carlini & Wagner, 2017) (b). The simulation is only meant to illustrate basic properties ofthe methods. In particular, it does not include Adam optimizer (Kingma & Ba, 2015) for C&W.
Figure 2: Refinement stage ofBP. Case OUT when ∖V∖>1 (a); case IN (b). See text for details.
Figure 3: OPerating characteristics on MNIST, CIFAR10 and ImageNet. The number of iterationsis 5 x 20 for C&W and 100 for I-FGSM, PGD2, DDN and our BP.
Figure 4: >∕E(D2) as a function of 0 for = 3 * 2992 and △ = 1/255.
Figure 5:	Success probability RUC and average distortion D for different values of parameters a and7min of BP with 20 iterations.
Figure 6:	(a) Average distortion VS. number of iterations for I-FGSM, PGD2, C&W, DDN and ourmethod BP on ImageNet. I-FGSM is not improving with iterations because it is constrained by e.
Figure 7: Operating characteristics on MNIST, CIFAR10 and ImageNet without quantization. Thenumber of iterations is 5 x 20 for C&W and 100 for I-FGSM, PGD2, DDN and our BP.
Figure 8: Operating characteristics of attacks against robust models: adversarial training with PGDas the reference attack (Madry et al., 2017) for MNIST and CIFAR10, and ensemble adversarialtraining (Tramer et al., 2017a) for ImageNet. The number of iterations is 5 x 20 for C&W and 100for I-FGSM, PGD2, DDN and our BP.
Figure 9: Original (left), adversarial (top row) and scaled perturbation (below) images against In-ceptionV3 on ImageNet. The five images are the worst 5 images for BP requiring the strongestdistortions, yet these are smaller than the distortions necessary with all other methods (The red colormeans that the forged image is not adversarial). Perturbations are inverted (low is white; high iscolored, per channel) and scaled in the same way for a fair comparison.
