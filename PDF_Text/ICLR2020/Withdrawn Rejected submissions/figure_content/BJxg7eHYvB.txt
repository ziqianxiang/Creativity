Figure 1: The upper graph shows GPU operations in a standard neural network in a time sequence.
Figure 2: If we do not store X1 in the memory in the forward propagation, we need to execute thelayer0 forward function again to get X1 for the layer1 backward function.
Figure 3: The inputs are a sequence of GPU operations and a memory limit from a user. Thealgorithm gets the inputs and interacts with an environment simulator. After a few optimizationsteps, the algorithm gives a best-found strategy which minimizes the training time and subjects tothe memory limit.
Figure 4: Solid and dashed lines represent time relation between nodes and agent transition (Section4.6) respectively. Each node represents exactly one GPU operation and will be executed in a timesequence. Each node represents at most one action, and not all of the actions will be executed. Ineach node, we can choose one action from several candidate actions. For example, we can chooseto do nothing, remove X0, or offload X0 in the 3th node.
Figure 5: Different overheads for different memory reductions. The y-axis represents performanceoverhead. The lower x-axis shows memory reduction, and the upper x-axis shows correspondingmemory usage.
