Figure 1: The molecular graph of the amino acid Tryptophan (a). To construct a SMILES string, allcycles are broken, forming a spanning tree (b); a depth-first traversal is selected (c); and this traversalis flattened (d). The beginning and end of intermediate branches in the traversal are denoted by (and ) respective. The ends of broken cycles are indicated with matching digits. The full grammar islisted in Appendix D. A small set of SMILES strings can cover all paths through a molecule (e).
Figure 2: In each layer of the encoder after the initial BiGRU and linear transformation, hiddenstates corresponding to each atom are harmonized across encodings of different SMILES strings for acommon molecule, followed by layer norm and a GRU on each SMILES encoding independently.
Figure 3:	To pass information between multiple SMILES representations of a molecule, the encoderharmonizes the representation of each atom. Homologous messages corresponding to the sameatom are pooled (b), and the original messages are replaced with this pooled message, reversing theinformation flow of (b).
Figure 4:	The approximating posterior is an autoregressive set of Gaussian distributions. The mean (μ)and log-variance (log σ2) of the first subset of latent variables z1 is a linear transformation of themax-pooled final hidden state of GRUs fed the encoder outputs. Succeeding subsets zi are producedvia Bahdanau-style attention with the pooled atom outputs of the GRUs as keys (k), and the query (q)computed by a neural network on z<i .
Figure 6: Dense decodings of true logP along a local 2D sheet in latent space, with the y axis alignedwith the regressor (a), and predicted and true penalized logP across steps of optimization (b).
Figure 5: Semi-supervised mean absolute error (MAE) ± the standard deviation across ten replicatesfor the log octanol-water partition coefficient (a), molecular weight (b), and the quantitative estimatedrug-likeness (c; Bickerton et al., 2012) on the ZINC310k dataset. Plots are log-log; the All SMILESMAE is a fraction of that of the SSVAE (Kang & Cho, 2018) and graph convolutions (Kearnes et al.,2016). Semi-supervised VAE (SSVAE) and graph convolution results are those reported by Kang &Cho (2018).
Figure 7: Molecules produced by gradient-based optimization in the All SMILES VAE.
Figure 8: Multiple SMILES strings of a single molecule may be more dissimilar than SMILES stringsof radically dissimilar molecules. The top SMILES string for molecule (a) is 30% similar to thebottom SMILES string by string edit distance, but 60% similar to the SMILES string for molecule (b).
Figure 9: Histogram of molecular diameters in the ZINC250k dataset. The diameter is defined asthe maximum eccentricity over all atoms in the molecular graph. The mean is 11.1; the maximumis 24. Typical implementations of graph convolution use only three to seven rounds of messagepassing (Duvenaud et al., 2015; Gilmer et al., 2017; Jin et al., 2018; Kearnes et al., 2016; Liu et al.,2018; Samanta et al., 2018; You et al., 2018), and so cannot propagate information across mostmolecules in this dataset.
Figure 10: Multiple SMILES strings representing a single, common molecule are preprocessed by aBiGRU and a linear transformation, followed by multiple encoder blocks as in Figures 2 and 3. Theapproximating posterior depicted in Figure 4 then produces a sample from the latent state z, which isdecoded into SMILES strings by LSTMs. Note that all SMILES strings, in both the input and theoutput, are distinct. The encoder blocks also receive a linear embedding of the original SMILESstrings as input.
Figure 11: The prior distribution over Z = [zι, z2,…]isa hierarchy of autoregressive Gaussians. Theconditional prior distribution ofhierarchical layer i given layers 1 through i -1, p(z∕zι, z, •一Zi-ι),is a Gaussian with mean μ and log-variance log σ2 determined by a neural network with input[Z1, Z2,…，Zi-l]∙In all experiments, we use encoder stacks of depth three, with 512 hidden units in each GRU. Theapproximating posterior uses four layers of hierarchy, with 128 hidden units in the one-hidden-layerneural network that computes the attentional query vector. In practice, separate GRUs were used toproduce the final hidden state for z1 and the atom representations for z>1. The single-layer LSTMdecoder has 2048 hidden units. Training was performed using ADAM, with a decaying learning16Under review as a conference paper at ICLR 2020rate and KL annealing. In all multiple SMILES strings architectures, we use 5 SMILES strings forencoding and decoding which are selected with RDkit (Landrum et al., 2006).
Figure 12: Dense decodings of true logP (a) and predicted logP (b) along a local 2D sheet in latentspace, with the y axis aligned with the trained logP regressor. We also display a coarse sampling ofthe molecules corresponding to the logP heatmap (c).
Figure 13: Predicted (red line) and true (blue x’s) quantitative estimate of drug-likeness (QED) overthe optimization trajectory resulting in the molecule with the maximum observed true QED (0.948).
Figure 14:	Molecules with the top three true penalized LogP values produced by gradient-basedoptimization subject to the hierarchical radius constraint in the All SMILES VAE, but with the KLterm unscaled by the number of SMILES strings in the decoder. Molecules are shown as SMILESstrings, wrapped across multiple lines, as they are too large to be properly rendered into an image.
Figure 15:	Context-free grammar of SMILES stringsProductions generally begin with a unique, defining symbol or set of symbols. Exceptions includebond and charge (both can begin with -), and aromatic_organic and aromatic_symbols(both include c, n, o, s, and p), but these pairs of productions never occur in the same context,and so cannot be confused. The particular production for chiral can only be resolved by parsingcharacters up to the next production, but the end of chiral and the identity of the subsequentproduction can be inferred from its first symbol of the production after chiral. Alternatively, thestrings of chiral can be encoded as monolithic tokens.
