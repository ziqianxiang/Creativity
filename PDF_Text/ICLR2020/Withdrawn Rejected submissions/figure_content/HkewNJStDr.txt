Figure 1:	Comparison of all the hard thresholding algorithms for solving sparse logistic regression problems. Ineach plot, the vertical axis shows the objective value minus the minimum, and the horizontal axis is the numberof effective passes over data (left) or running time (seconds, right).
Figure 2:	Comparison of all the hard thresholding algorithms for solving sparse linear regression problems.
Figure 3: Comparison of ASG-HT, ASVRG-HT, A2SBCD-HT, S2BCD-HTP and ASBCD-HTP for solvingsparse logistic regression problems. The four asynchronous parallel algorithms (i.e., ASG-HT, ASVRG-HT,A2SBCD-HT and ASBCD-HTP) run on 20 threads.
Figure 4: Speedup evaluation on rcv1-test and real-sim. Left: Evaluation of objective function gap in terms ofCPU time for ASBCD-HTP with different threads. Right: Speedup ratio with respect to the number of threads.
Figure 5: Recognition rates of all the algorithms on the Extended Yale B With different levels of Gaussian noise:犷=0.3 (left) and 犷=0.6 (right).
Figure 6:	Mini-batch size in the convergence rate of SBCD-HTP algorithm for sparse logistic regression onrcv1-train data.
Figure 7:	Comparison of all the algorithms for solving sparse logistic regression problems. In each plot, thevertical axis shows the objective value minus the minimum, and the horizontal axis is the number of effectivepasses over data or running time (seconds).
Figure 8:	Comparison of all the algorithms for solving sparse linear regression problems.
Figure 9:	Comparison of ASG-HT, ASVRH-HT, A2SBCD-HT, S2BCD-HTP and ASBCD-HTP for solvingsparse linear regression problems. The four asynchronous parallel algorithms (i.e., ASG-HT, ASVRH-HT,A2SBCD-HT and ASBCD-HTP) run on 20 threads.
Figure 10:	The upper bound of Theorem 1 vs the convergence performance.
Figure 11:	Comparison of FG-HT, SG-HT, SVRH-HT, ASBCDHT and SBCD-HTP for solving sparse SVMproblems.
