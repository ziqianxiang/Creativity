Figure 1: Illustration of the proposed intrinsic reward learning framework. The intrinsic reward rη is used toupdate the agent’s parameter θi throughout its lifetime which consists of many episodes. The goal is to find theoptimal intrinsic reward parameters η* across many lifetimes that maximises the lifetime return (Ghfe) givenany randomly initialised agents and possibly non-stationary tasks drawn from some distribution p(T).
Figure 2: Illustration of domains. (a) The agent needs to find the goal location which gives a positive reward,but the goal is not visible to the agent. (b) Each object (A, B, and C) gives rewards. (c) The agent is requiredto first collect the key and visit one of the boxes (A, B, and C) to receive the corresponding reward. All objectsare placed to random locations after every episode.
Figure 3: Evaluation of different reward functions averaged over 30 seeds. The learning curves show agentstrained with our intrinsic reward (blue), with the extrinsic reward with the episodic return objective (orange)and the lifetime return objective (brown), and with a count-based exploration reward (green). The dashed linecorresponds to a hand-designed near-optimal exploration strategy.
Figure 4: Visualisation of the first 3000 steps of an agent trained with different reward functions in EmptyRooms. (a) The blue and yellow squares represent the agent and the hidden goal, respectively. (b) The learnedreward encourages the agent to visit many locations if the goal is not found (top). However, when the goalis found early, the intrinsic reward makes the agent exploit it without further exploration (bottom). (c) Anagent trained only with extrinsic rewards explores poorly. (d-e) Both the count-based and ICM rewards tend toencourage exploration (top) but hinders exploitation when the goal is found (bottom).
Figure 5: Visualisation of the learned intrinsic reward in Random ABC, where the extrinsic rewards for A, B,and C are 0.2, -0.5, and 0.1 respectively. Each figure shows the sum of intrinsic rewards for a trajectory towardseach object (A, B, and C). In the first episode, the intrinsic reward encourages the agent to explore A. In thesecond episode, the intrinsic reward encourages exploring C if A is visited (top) or vice versa (bottom). Inepisode 3, after both A and C are explored, the intrinsic reward encourages to revisit A (both top and bottom).
Figure 6:	Visualisation of the agent’s intrinsic and extrinsic rewards (left) and the entropy of its policy (right)on Non-stationary ABC. The task changes at 500th episode (dashed vertical line). The intrinsic reward givesa negative reward even before the task changes (green rectangle) and makes the policy less peaky (entropyincreases). As a result, the agent quickly adapts to the change.
Figure 7:	Evaluation of different intrinsic reward architectures and objectives. For ‘LSTM’ the reward networkhas an LSTM taking the lifetime history as input. For ‘FF’ a feed-forward reward network takes only the currenttime-step. ‘Lifetime’ and ‘Episode’ means the lifetime and episodic return as objective respectively.
Figure 8: Comparison to policy transfer methods.
Figure 9: Generalisation to new agent-environment interfaces in Random ABC. (a) ‘Permuted’ agents havedifferent action semantics. ‘Extended’ agents have additional actions. (b) ‘AC-Intrinsic’ is the original actor-critic agent trained with the intrinsic reward. ‘Q-Intrinsic’ is a Q-learning agent with the intrinsic rewardlearned from actor-critic agents. ‘Q-Extrinsic’ is the Q-learning agent with the extrinsic reward. (c) shows theperformances of the policy transfer baselines with permuted actions during evaluation.
Figure 10: Evaluation of different rewards in the Fixed ABC domain. The x-axis shows the number of episodeswithin a single lifetime; the y-axis measures the episode return.
