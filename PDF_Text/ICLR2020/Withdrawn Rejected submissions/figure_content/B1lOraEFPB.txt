Figure 1: Architecture of the parserTraι ned NetworkModel torPredictingFigure 2: Network model of learning transition action3Under review as a conference paper at ICLR 2020Stack state	Stack POS state	Bu∏er state	Buffer POS state	Transition action[-root-]	[-root-]	熙Sd 二］	[NOUN, DETs ACCj VERB, SUBJC3 OBJC. PUNCT]		Shift[-root-]	[-root-]	［聪益朗3A 诵::］	[NOUN, XTRB r SUBJCj ADPr OBJCs PUNCT]	吐IiftFigure 3: Sample instances of transition configurationthe buffer and S is the corresponding part of speech tags in the buffer.
Figure 2: Network model of learning transition action3Under review as a conference paper at ICLR 2020Stack state	Stack POS state	Bu∏er state	Buffer POS state	Transition action[-root-]	[-root-]	熙Sd 二］	[NOUN, DETs ACCj VERB, SUBJC3 OBJC. PUNCT]		Shift[-root-]	[-root-]	［聪益朗3A 诵::］	[NOUN, XTRB r SUBJCj ADPr OBJCs PUNCT]	吐IiftFigure 3: Sample instances of transition configurationthe buffer and S is the corresponding part of speech tags in the buffer.
Figure 3: Sample instances of transition configurationthe buffer and S is the corresponding part of speech tags in the buffer.
Figure 4: Network model for learning relationship typeDependent word	Dependent word s POS tag	Headword	Headword's POStag	Class (relationship type)	^⅛i∩	PROPN		VERB	asjj!⅛i	NOUN		VERB	okɪ	VERB	ROOT	ROOT	root~h	SUBJC	"∏∆	VERB	Figure 5: Sample inputs for the dependency relation predictor network modelDependent word	Dependent WOTd飞 POS tag	Headword	Headword1 s PQStag	Class (relationship type)	^∩?	NOUN		VERB	shι^∩A	VERB	ROOT	ROOT	root^h	SUBJC	^∩A	VERB	Figure 6: Sample inputs for the dependency relation predictor network model5Under review as a conference paper at ICLR 2020using time distributed dense layer. This layer outputs the predicted dependency label for a givenhead-dependent pairs of a sentence.
Figure 5: Sample inputs for the dependency relation predictor network modelDependent word	Dependent WOTd飞 POS tag	Headword	Headword1 s PQStag	Class (relationship type)	^∩?	NOUN		VERB	shι^∩A	VERB	ROOT	ROOT	root^h	SUBJC	^∩A	VERB	Figure 6: Sample inputs for the dependency relation predictor network model5Under review as a conference paper at ICLR 2020using time distributed dense layer. This layer outputs the predicted dependency label for a givenhead-dependent pairs of a sentence.
Figure 6: Sample inputs for the dependency relation predictor network model5Under review as a conference paper at ICLR 2020using time distributed dense layer. This layer outputs the predicted dependency label for a givenhead-dependent pairs of a sentence.
Figure 7: Accuracy of predicting transition action using different splitting ratioFigure 8: The accuracy of transition action prediction with and without adding of attention layerof the stack and buffer to have 26 dimensions. For the LSTM layer we used 256 output dimensionsand 128 dimensions for attention layer. For the output layer we used four output dimensions that is0, 1, 2 and 3 (for Shit, Left-arc, Right-arc and Reduce respectively). We used Adam optimizationtechnique with learning rate 0.01 and batch-size 256. In addition, the training iterates for 50 epochs.
Figure 8: The accuracy of transition action prediction with and without adding of attention layerof the stack and buffer to have 26 dimensions. For the LSTM layer we used 256 output dimensionsand 128 dimensions for attention layer. For the output layer we used four output dimensions that is0, 1, 2 and 3 (for Shit, Left-arc, Right-arc and Reduce respectively). We used Adam optimizationtechnique with learning rate 0.01 and batch-size 256. In addition, the training iterates for 50 epochs.
Figure 9: Attachment scores of the parserscore is the accuracy of the system to attach the correct headword to the correct dependent word andgiving the correct relationship type. The parser is evaluated on 4,660 head dependent pairs, whichis 30 of the total dataset. As depicted in figure 10 the parser scores 91.54 accuracy for unlabeledattachment and 81.4 accuracy for labeled attachment score.
