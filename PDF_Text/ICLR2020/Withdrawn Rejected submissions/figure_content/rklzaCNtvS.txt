Figure 1: Document sentences are first projected into a semantic space typically by an encoder ina sequence-to-sequence model. g1, g2, g3 are highlights of a document representing closely relatedsentence-semantics {h(11), h(21), h(31)}, {h(12), h(22), h(32)}, {h(13), h(23), h(33)} respectively. These high-lights are then used by the decoder to form concise summaries.
Figure 2: Hierarchical NSE: From a given article, all the M sentences consisting of N words eachare processed by the NSE using read (R), compose (C) and write (W) operations. Each sentencememory is updated N times by each word in the sentence ({Ms(ik)}kN=1). After the last encoder step,all the updated sentence memories MsN1 , MsN2 , ..., MsNM are concatenated to form the cumulativesentence memory Ms . The decoder then uses the cumulative sentence memory Ms and documentmemory Md in a similar fashion to produce the write vectors ht that are passed through a softmaxlayer to obtain the vocabulary distribution.
Figure 3: Self-Critic training reduces exposure bias and by learning a policy whose samples scorebetter than the greedy samples that are used during test time in a supervised learning setting.
