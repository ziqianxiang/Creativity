Figure 1: Loss distributions at the training accuracy of 50% using DenseNet (L=40, k=12): (a) and (b)show those on CIFAR-100 with two types of synthetic noises of 40%, where “symmetric noise” flipsa true label into other labels with equal probability, and “pair noise” flips a true label into a specificfalse label; (c) shows those on FOOD-101N (Lee et al., 2018) with real-world noise of 18.4%.
Figure 2: Key idea of Prestopping: (a) and (b) show how many true-labeled and false-labeled samplesare memorized when training DenseNet (L=40, k=12)1on CIFAR-100 with two types of syntheticnoises of 40%. “Default” is a standard training method, and “Prestopping” is our proposed one; (c)contrasts the convergence of test error between the two methods.
Figure 3: Early stop point estimated by ideal and heuristic methods when training DenseNet (L=40,k=12) on CIFAR-100 with two types of synthetic noises of 40%: (a) and (b) show the stop pointderived by the ground-truth labels and the clean validation set, respectively.
Figure 4: Label precision and label recall of the maximal safe set during the remaining epochs whentraining DenseNet (L=40, k=12) on CIFAR data sets with two types of synthetic noises of 40%.
Figure 5:	Best test errors using two CNNs on two data sets with varying pair noise rates.
Figure 6:	Best test errors using two CNNs on two data sets with varying symmetric noise rates.
Figure 7:	Best test errors using two CNNs on two data sets with real-world noises.
Figure 8:	Convergence curves of DenseNet (L=40, k=12) on CIFAR-10 with a noise rate of 40%.
Figure 9: Average accuracy of selecting true-labeled samples by the small-loss trick in Co-teachingand the maximal safe set in Prestopping using DenseNet on two data sets with synthetic noises.
Figure 10: Anatomy of Co-teaching+ on CIFAR-100 with 40% symmetric noise: (a) and (b) showthe change in disagreement ratio for all true-labeled samples, when using Co-teaching+ to train twonetworks with different complexity, where “simple network” is a network with seven layers used byYu et al. (2019), and “complex network” is a DenseNet (L=40, k=12) used for our evaluation; (c)shows the accuracy of selecting true-labeled samples on the DenseNet.
Figure 11:	Best test errors using VGG-19 on two simulated noisy data sets with varying noise rates.
Figure 12:	Difference in test error between two heuristics on two simulated noisy data sets.
Figure 13:	Grid search on CIFAR-10 and CIFAR-100 with two types of noises of 40%.
Figure 14:	Best test errors using VGG-19 on Tiny-ImageNet with varying noise rates.
Figure 15:	Best test errors on Clothing70k (τ ≈ 38.5%) along with the detailed result.
Figure 16:	Refurbishing ratio and accuracy of Prestopping+ when training DenseNet (L=40, k=12)on CIFAR-10 and CIFAR-100 with pair noise of 40%.
Figure 17: Refurbishing of false-labeled samples originally contained in CIFAR-100. The subcaptionrepresents “original label” → “refurbished label” recognized by Prestopping+.
