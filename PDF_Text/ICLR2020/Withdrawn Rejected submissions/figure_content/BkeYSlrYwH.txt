Figure 1: An overview of collaborative inter-agent knowledge distillation.
Figure 2: Performance evaluation of inter-agent knowledge distillation. SAC-CIKD represents the imple-mentation of our method upon SAC; Vanilla-SAC stands for the original SAC; Ensemble-SAC is an analogousvariant of Osband et al. [37]’s method on vanilla-SAC (effectively SAC-CIKD without inter-agent knowledgedistillation). See Section 5.2 for details. Notice that in most domains, SAC-CIKD is able to reach the convergentperformance of the baselines in less than half the training time.
Figure 3: (a) Comparison between knowledge distillation and extra policy updates. Vanilla-SAC (extra)and Ensemble-SAC (extra) stand for Vanilla-SAC and Ensemble-SAC variants that use extra policy updates,respectively (see Section 5.3 and Section 3 for details). (b) Comparison between knowledge distillation andcopying parameters. SAC-CIKD (hardcopy) stands for the variant of our method which directly copy the neuralnetworks parameters of the best agent to the others. (a)(b) Both figures show that knowledge distillation is moreeffective.
Figure 4: Comparison between the selecting the best-performing teacher vs. a random teacher. SAC-CIKD(random teacher) refers to the variant of our SAC-CIKD where a randomly chosen teacher is used for knowledgedistillation. This figure demonstrates that it can be more effective to select the best-performing agent as theteacher.
Figure 5: (a) The distribution of selected teacher agent indices. This figure shows that each agent has a chanceto become the teacher. (b) The proportion of time that each agent is selected to be the teacher over theentire training process. This figure shows that there is no strictly dominant agent within the ensemble. (c) Theproportion of time that the students surpass the teachers after knowledge distillation. This figure showsthat as more experience is collected, inter-agent knowledge distillation enables students to become better thantheir teachers (see Section 4.4 for the detail of inter-knowledge distillation). (d) The importance of students’inherent knowledge. SAC-CIKD (reset) denotes the variant of our method where we randomly re-initializestudents’ parameters before knowledge distillation. This figure shows that students’ inherent knowledge iscrucial for the performance of our method.
Figure 6: Performance comparison under different ensemble sizes. Three different ensemble configurationswith 2, 3, and 5 agents lead to similar performance. This result shows that CIKD does not require a largeensemble size.
Figure 7: Comparison of different distillation schemes. SAC-CIKD (policy only) and SAC-CIKD (critic)denote the variants of SAC-CIKD that distill the policy only and distill the critic only, respectively. This figureshows that distilling the critic and the policy simultaneously is more effective.
Figure 8: Full performance evaluation of inter-agent knowledge distillation. SAC-CIKD represents theimplementation of our method upon SAC; Vanilla-SAC stands for the original SAC; Ensemble-SAC is ananalogous variant of Osband et al. [37]’s method on vanilla-SAC (effectively SAC-CIKD without inter-agentknowledge distillation). See Section 5.2 for details.
Figure 9: (a) The distribution of selected teacher agent indices. (b) The proportion of time that each agentis selected to be the teacher over the entire training process. (c) The distribution of the best-performingagent among three agents Vanilla-SAC trained with different random seeds. (d) The proportion of timethat each agent is the best-performing agent over the entire training process.
