Figure 1: Architecture overviews of Group-Transformer and its sub-modules when the number ofgroups is two. The gray arrows show the information flow across the entire groups, and the blue andred arrows indicate the information flow for each group.
Figure 2: Performance comparison of three reduction methods from model parameters such as thenumber of layers (“L”), the hidden dimension (“D”), and the number of groups (“G”). The FLOPsindicates the number of calculations to generate 512 length of a character sequence.
Figure 3: Similarity matrices of multi-head attentions. The black box indicates a high similarity ofattention patterns and the white box does the opposite. The red boxes represent groups of the mul-tiple heads. The similarity is measured based on the euclidean distance between attention weightsover a test sequence.
