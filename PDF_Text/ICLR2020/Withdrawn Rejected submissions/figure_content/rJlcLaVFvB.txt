Figure 1: Inference update scheme for the 2L-SPC network. x is the input image, and λi tunesthe sparseness level of Yi . The encoding and decoding dictionaries (Di and DiT , respectively) arereciprocal. The sparse maps (Yi) are updated through a bi-directional dynamical process (plain andempty arrows). This recursive process alone describes the Hi-La network. If we add the top-downinfluence, called inter-layer feedback connection (blue arrow), it then becomes a 2L-SPC network.
Figure 2:	Evolution of all layers prediction error, evaluated on the testing set, for both 2L-SPC andHi-La networks and trained on STL-10, CFD and MNIST databases. We vary the first layer sparsityin the top 3 graphs (a) and the second layer sparsity in the bottom 3 graphs (b).
Figure 3:	Heatmaps of the global loss error when varying layers’ sparsity for 2L-SPC (i) and Hi-La(ii) on CFD database. (iii) shows the heatmap of the relative difference between the Hi-La and the2L-SPC global losses when varying the layers’ sparsityinference process to converge towards a stable state on the testing set. Fig. 4 shows the evolution ofthis quantity, for STL-10, CFD and MNIST databases (see Appendix E for AT&T database), whenvarying both layers’ sparsity. For all the simulations, the 2L-SPC needs less iteration than the Hi-Lamodel to converge towards a stable state. We also observe that the data dispersion is, in general,more pronounced for the Hi-La model. In addition to converging to lower prediction error, the 2L-SPC is also decreasing the number of iterations in the inference process to converge towards a stablestate.
Figure 4:	Evolution of the number of iterations needed to reach stability criterium for both 2L-SPCand Hi-La netWorks on the testing set of STL-10, CFD and MNIST databases. We vary the first layersparsity in the top 3 graphs (a) and the second layer sparsity in the bottom 3 graphs (b). Shadedareas correspond to mean absolute deviation on 7 runs. Sometimes the dispersion is so small that itlooks like there is no shade.
Figure 5: Evolution of the global prediction error during the training evaluated on the STL-10, CFDand MNIST testing sets. Shaded areas correspond to mean absolute deviation on 7 runs. All graphshave a logarithmic scale in both x and y-axis.
Figure 6: Hi-La and 2L-SPC RFs obtained on the CFD database (with λ1 = 0.3, λ2 = 1.8) andtheir associated second layer activation probability histogram. The first and second layer RFs havea size of 9 × 9 px and 33 × 33 px respectively. For the first layer RFs, we randomly selected 12out of 64 atoms. For the second layer RFs, we sub-sampled 32 out of 128 atoms ranked by theiractivation probability in descending order. For readability, we removed the most activated filter (RFframed black) in 2L-SPC and Hi-La second layer activation histogram. The activation probabilityof the RFs framed in red are shown as a red bar in the corresponding histogram.
Figure 7: Evolution of the global prediction error evaluated on the testing set for both 2L-SPCand Hi-La networks. We vary the first layer sparsity in the top 3 graphs (a) and the second layersparsity in the bottom 3 graphs (b). Experiments have been conducted on STL-10, CFD and MNIST-databases. Shaded areas correspond to mean absolute deviation on 7 runs. Sometimes the dispersionis so small that it looks like there is no shade.
Figure 8: Evolution of all layers prediction error, evaluated on the testing set, for both 2L-SPC andHi-La networks and trained on AT&T. We vary the first layer sparsity in (a) and the second layersparsity in (b).
Figure 9: Evolution of the number of iterations needed to reach stability criterium for both 2L-SPCand Hi-La networks on the AT&T testing set. We vary the first layer sparsity in (a) and the secondlayer sparsity in (b). Shaded areas correspond to mean absolute deviation on 7 runs. Sometimes thedispersion is so small that it looks like there is no shade.
Figure 10: Evolution of the global prediction error during the training for the ATT testing set. Shadedareas correspond to mean absolute deviation on 7 runs. The graph have a logarithmic scale in bothx and y-axis.
Figure 11: Generation of the second-layer effective dictionary. The result of this back-projectionis called effective dictionary and could be assimilate to the notion of preferred stimulus inneuroscience. In a general case, the effective dictionary at layer i is computed as follow:Dieff,T = D0T..DiT-1DiT (Sulam et al., 2018).
Figure 12: 2L-SPC (a & c) and Hi-La (b & d) effective dictionaries obtained on the STL-10database, with sparsity parameter: (λ1=0.5,λ2=1). All other parameters are those described in Ta-ble 1 for the STL-10 database. Atoms are sorted by activation probabilities in a descending order.
Figure 13: 2L-SPC (a & c) and Hi-La (b & d) effective dictionaries obtained on the CFD database,with sparsity parameter: (λ1=0.3,λ2=1.8). All other parameters are those described in Table 1 for theCFD database. Atoms are sorted by activation probabilities in a descending order. The visualizationshown here is the projection of the dictionaries into the input space. First layer effective dictionarieshave a size of 9 × 9 px (a & b) and second layer RFs have a size of 33 × 33 (c & d) px respectively.
Figure 14: 2L-SPC (a & c) and Hi-La (b & d) effective dictionaries obtained on the MNISTdatabase, with sparsity parameter: (λ1=0.2,λ2=0.3). Atoms are sorted by activation probabilitiesin a descending order. All other parameters are those described in Table 1 for the MNIST database.
Figure 15: 2L-SPC (a & c) and Hi-La (b & d) effective dictionaries obtained on the AT&T database,with sparsity parameter: (λ1=0.5,λ2=1). All other parameters are those described in Table 2 for theAT&T database.Atoms are sorted by activation probabilities in a descending order. The visualizationshown here is the projection of the dictionaries into the input space. First layer effective dictionarieshave a size of 9 × 9 px (a & b) and second layer RFs have a size of 26 × 26 (c & d) px respectively.
