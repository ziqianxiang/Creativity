Figure 1: (a) Illustration of the multivariate, geo-tagged time series imputation task: the inputdata has three dimensions (i.e. time, location, measurement) with some missing values (indicated bythe orange dot); the output is of same shape as the input while the missing values have been imputed(indicated by the red dot). (b) Self-attention mechanism: the Attention Map is first computed usingevery pair of Query vector and Key vector and then guides the updating of Value vectors via weightedsum to take into account contextual information. (c) Traditional Self-Attention mechanism updatesValue vector along the temporal dimension only vs. Cross-Dimensional Self-Attention mechanismupdates Value vector according to data across all dimensions.
Figure 2: Three choices of implementing our Cross-Dimensional Self-Attention mechanismRNN-based data imputation methods. Li et al. (2018) proposed DCGRU for seq-to-seq byadopting graph convolution (Chung & Graham (1997); Shi (20o9); Shuman et al. (2012)) to modelspatial-temporal relationship. Luo et al. (2018a) built GRUI by incorporating RNN into a GenerativeAdversarial Network (GAN). Nevertheless, the spatiotemporal and measurements correlation aremixed and indistinguishable. so that the mediate back propagation from loss of available observationcan contribute to the missing value updating. Nevertheless, these RNN-based models fundamentallysuffer from the constraint of sequential processing, which leads to long training time and prohibitsthe direct modeling of the relationship between two distant data values.
Figure 3: The framework employing CDSA for data imputation and forecasting.
Figure 4: Visualization of the cross-dimensional self-attention on KDD-2018. (a) Part of Time-Measurement attention map. (b) Two time series of PM2.5 and PM10. The value at purple dot ismissing and our model predicts its value based on other values. The arrow in (b) represents attentionwhose score is highlighted with bounding box in (a) of the same color.
Figure 5: Different correlation between different measurements. The time slots When both of thechosen measurements are available are selected and the value of the first 300 selected time slots areplotted. Upper: PM2.5 & PM10 are significantly positively correlated and Lower: NO2 & O3 arenegatively correlated.
Figure 6: The effective attention map calculation in Joint and Shared.
Figure 7: Reshape Attention Map: The original attention map in (a) is reshaped into (b-d) respec-tively. The variable (t,l,m) denotes the attention map units for Time, Location and Measurementwhere the subscript labels the attention between two units, i.e., t12 = SOftmax(qιk>/√d) whereqι and k2 are d-dim vectors. Besides, the empty entry indicates 0.
Figure 8: The framework of using Crossing-Dimensional Self-Attention (CDSA) for data forecasting.
Figure 9: Model ArchitectureXN15Under review as a conference paper at ICLR 2020C.2 Discussion for Input FormThe decoder in NLP task originally sets the shifted output as input. Take the German-to-Englishtranslation scenario as an example where the embedded word vectors of German are set as theEncoder input, the model will first send a [GO] vector into the decoder and generate the first wordvector of the translated English sequence, then the predicted vector will be sent into the decoder topredict the next word vector and the decoder will complete the sentence translation by repeating thisoperation until the end.
Figure 10: Casual Mask Design for Attention Map on LocationBesides, the decoder generates predictions given previous ground truth observations during trainingwhile the ground truth observations are replaced by predictions generated by the model itself duringtesting. As, the discrepancy between the input distributions of training and testing can cause degradedperformance, We adopt the integrated sampling Bengio et al. (2015) as in Li et al. (2018) to mitigatethis impact while this method is very time-consuming for the Transformer framework. During testing,Table 10: Comparisons of Prediction Performance on dataset METR-LATime	Metric	Shifted Output Shifted Output Encoder Input Encoder Input (Mean Final) (Mean Step) (Complemented)	(Mean)15 min	MAE RMSE MAPE	3.05	3.09	3.15	3.01	= 5.46	5.50	5.31	5.08 8.31%	8.47%	8.18%	7.82%30 min	MAE RMSE MAPE	349	354	347	3.14 6.62	6.66	5.96	5.38 9.56%	9.88%	9.19%	8.30%60 min	MAE RMSE MAPE	4.21	4.26	4TT7	3:40 8.14	8.19	7.67	6.27 11.44%	11.79%	11.94%	9.76%Mean	MAE RMSE MAPE	35	356	354	3.16 6.56	6.61	6.14	5.48 9.56%	9.84%	9.53%	8.50%In summary, by setting shifted output as the Decoder input, multiple Attention Map are calculatedfor forecasting value of different time stamps which requires huge memory usage. Still, integrated16Under review as a conference paper at ICLR 2020sampling makes this framework suffer from an exhausted training time, since we need to send thepredicted output back to decoder (Run) and repeat this Run for T times. During testing, we can usethe output corresponding to its own Run (Step) as the predicted result, as well as the output of thelast run (Final). As shown in the first 2 columns in Table 10, the performance of outputs in the last
Figure 11: Loss condition of NYC dataset: The horizontal axis represents time line while the verticalaxis represents sensors. Each unit indicates whether the data is missed in a 5-min window. The whitearea indicates available observation. The blue area indicates the Burst Loss. The red area indicatesthe time slots when the data of all the sensors are missed. The green area denotes as Abnormal for acertain location the data is continuously missed for a very long period.
Figure 12: Attention map example of the last CDSA layer67tTYQL J FF，GTSk-NTIAL 丰台'LaoxiangfenScenic Arec老象峰景区S2198BAD慰留ENHuyu NaturalScenic Area虎峪自然风景区ShuhuashanScenic Area书画山风景区Finggu S305平谷区
Figure 13: KDD-2015 Visualization of Location Correlation (arrow with darker color indicates ahigher weight)20Under review as a conference paper at ICLR 2020We provide the attention map examples extracted from the last CDSA layer. As shown in Fig. 4(b),correlation exists between different measurement, i.e., PM2.5 and PM10 are highly correlated. Asshown in Fig. 12(a), the estimation of PM2.5 and PM10 is also highly relied on each other, i.e., forthe estimation of PM2.5, the color in second unit, representing the weight of PM10, is darker than therest in the first row.
Figure 14: Visualization of prediction of missing point A, our model not only attends to availablepoints (e.g. C, D) but also attends to missing points (e.g. B).
Figure 15: RMSE comparison for Downstream Forecasting on NYC-TrafficAs described in the main paper, we use the 23-day data of NYC-Traffic for further forecasting.
