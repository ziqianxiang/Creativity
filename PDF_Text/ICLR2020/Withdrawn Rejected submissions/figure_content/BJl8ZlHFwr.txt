Figure 1: An overview of MCMAE-D. Images and attributes are embedded in the shared represen-tation space by MCMAE inference models learned given the training set. We propose a model todistinguish different domains (seen or unseen) in space and to learn jointly with MCMAE. Afterlearning, we can perform class prediction with a reduced bias toward the seen classes by combiningrelation-based classifier and domain discriminator as a mixture model, as shown in the equation onthe right side.
Figure 2: (a) Two requirements exist for achieving good performance in ZSL: modality invariantand class separability. (b) Failure of class separation between domains. The unseen domain overlapswith the seen domain. Therefore, all examples of the unseen classes might be predicted as one ofthe seen ones. (c) The network architecture of our proposed model, MCMAE-D.
Figure 3: 2-D representation of PSE, MCMAE, and MCMAE-D. These were obtained by learningby setting the z dimension of the model to 2. Circle plots show embedding of the test images bypφx (z|x). Triangles represent embedding of the class-attributes: 4 denotes a seen class and 5is an unseen one. The contour lines in MCMAE-D represent the domain prediction probabilityp(d = 1|z) obtained by the domain discriminator.
Figure 4: Transition of GZSL performance (the harmonic mean) when each parameter is changed:(a) the dimension of the shared representation and (b) the coefficient of domain discriminator α inMCMAE-D.
Figure 5: Learning curves of MCMAE-D for each dataset.
