Figure 1: Integration of the paragraph vector framework as a TEB module to an existing deeplearning based image captioning model. There are three interconnected components divided intothree dashed rectangular boxes. In the green box on the top left, the image encoder extracts visualfeatures through a CNN model. In the yellow box on the bottom, a RNN based language modeldecoder is used to generate paragraphs. Existing deep learning based models only contain thesetwo components. The red box on the top right box is the TEB module: In the training stage, foran image, paragraph pair, the varied-length paragraph is mapped to a fixed-length vector which iscalled TEB through the paragraph vector framework. The visual features from the image encoderare converted to the predicted TEB (called TEB’) through several fully connected layers. The TEB’is supervised by the TEB through an L1 loss, which acts as global deep supervision to regularize thevisual feature extraction for the image encoder. The visual features and TEB’ are concatenated andfeed into the RNN as input. The generated paragraph is supervised by the ground truth paragraphthrough a word-level loss. In the inference stage, the TEB is not available and the TEB’ acts as theTEB to provide the features of the whole paragraph to alleviate the long-term dependency problemfor the language model.
Figure 2: Qualitative result comparison of paragraph outputs of our model (Diversity with TEB) andthe baseline Diversity model Melas-Kyriazi et al. (2018)6 ConclusionIn this paper, we propose the Text Embedding Bank (TEB) module for visual paragraph caption-ing, a task which requires capturing fine-grained entities in the image to generate a detailed andcoherent paragraph, like a story. Our TEB module provides global and parallel deep supervision anddistributed memory for find-grained image understanding and long-term language reasoning. Inte-grating the TEB module to existing state-of-the-art methods achieves new state-of-the-art results bya large margin.
