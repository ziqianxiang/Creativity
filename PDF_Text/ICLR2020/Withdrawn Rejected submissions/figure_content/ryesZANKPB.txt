Figure 1: Framework overviewSpecifically, the purpose of this work is to encode learning strategies into an adaptive high-dimensionalloss function, or a meta-loss, which generalizes across multiple training contexts or tasks. Inspired byinverse reinforcement learning (Ng et al., 2000), our work combines the learning to learn paradigmof meta-learning with the generality of learning loss landscapes. We construct a unified, fullydifferentiable framework that can learn model-agnostic loss functions to provide a strong learningsignal for a large range of model classes, such as classifiers, regressors or control policies.
Figure 2: Meta-learning for regression (top) and binary classification (bottom) tasks. (a) meta-train task, (b)meta-test tasks, (c) performance of the meta-network on the meta-train task as a function of (outer) meta-trainiterations in blue, as compared to SGD using the task-loss directly in orange, (d) average performance ofmeta-loss on meta-test tasks as a function of the number of gradient update stepsIn this set of experiments, we evaluate how well our meta-learning framework can learn loss functionsMφ for regression and classification tasks. In particular, we perform experiments on sine functionregression and binary classification of digits (see details in Appendix A.4). At meta-train time, werandomly draw one task for meta-training (see Fig. 2 (a)), and at meta-test time we randomly draw 10test tasks for regression, and 4 test tasks for classification (Fig. 2(b)). We compare the performanceof using SGD with the task-loss L directly (in orange) to SGD using the learned meta-network M(in blue), both using a learning rate α = 0.001. In Fig. 2 (c) we show the average performance ofthe meta-network Mφ as it is being learned, as a function of (outer) meta-train iterations in blue. Inboth regression and classification tasks, the meta-loss eventually leads to a better performance onthe meta-train task as compared to the task loss. In Fig. 2 (d) we evaluate SGD using Mφ vs SGDusing L on previously unseen (and out-of-distribution) meta-test tasks as a function of the number ofgradient steps. Even on these novel test tasks, our learned Mφ leads to improved performance ascompared to the task-loss.
Figure 3: Results of ML3 for MBRL. We can see that, for MBRL the meta-loss generalizes well (a)and speeds up learning when compared to the task-specific loss during meta test. The results areshown for the PointmassGoal (a and b) and the ReacherGoal (c) environments.
Figure 4: (a+b) Policy learned with ML3 loss compared to PPO performance during meta-test time. (c+d)Using the same ML3 loss, we can optimize policies of different architectures, showing that our learned lossmaintains generality. Each curve is an average over ten different tasks.
Figure 5: Left: Comparison of learned meta-loss (top) and mean-squared loss (bottom) landscapes for fitting thefrequency of a sine function. The red lines indicate the target values of the frequency, comparison of optimizationperformance during meta test time, with and without shaped loss compared to regular gradient descent. Right:Improved exploration behavior in the MountainCar environment when using ML3 with intermediate goals duringmeta-train time and average distance to the goal at the final timestep.
Figure 6: ReacherGoal with expert demonstrations available during meta-train time. (a) showsthe targets in end-effector space. The four blue dots show the training targets for which expertdemonstrations are available, the orange dots show the meta-test targets. In (b) we show the reachingperformance of a policy trained with ML3 at meta-test time, compared to the performance of trainingsimply on the behavioral cloning objective and testing on test targets.
