Figure 1: Calibration of the predictive uncertainty under domain shift. Here, a LeNet model is trainedon MNIST data and calibration of the predictive uncertainty is evaluated on images perturbed withincreasing y-zoom. Epsilon denotes the relative perturbation strength. Top: For in-domain samplesthe model has a high accuracy and low entropy, for higher domain shifts wrong predictions are oftenmade with high confidence (left). While increasing domain shift results in a decreased accuracyand increased entropy, it is not clear whether this increased entropy reflects a well calibrated modelconfidence (right). Bottom: Only reliability diagrams and the expected calibration error (ECE) revealthat the decline in accuracy does not match the confidence of the model. Left: Confidence matchesaccuracy for most bins. Middle: Model makes overconfident predictions (red bars illustrate calibrationgap). Right: ECE curve quantifies how miss-calibration changes with increasing perturbation strength.
Figure 2: Technical robustness of sequence models for classifying sequential MNIST data, quantifiedby computing the micro-averaged expected calibration error (lower is better). FALCON results inconsistently well calibrated and robust predictions across 9 different perturbation strategies withsubstantially lower micro-averaged ECEs compared to existing methods, both for LSTM and GRUmodels. For fair comparison, we only show micro-averaged ECE for models with competitiveaccuracy, omitting EDL0.60.50.10.0net with L2 regularisation as baseline, (ii) MC-Dropout corresponding to the modelling approachpresented by Gal & Ghahramani (2016), (iii) Deep Ensembles referring to an approach based on anensemble of neural nets trained using adversarial examples (Lakshminarayanan et al., 2017), (iv)EDL referring to Evidential Deep Learning (Sensoy et al., 2018), (v) MNF referring to a Bayesianneural network trained using multiplicative normalising flows (Louizos & Welling, 2017) and (vi)FALCON, which is our method based on Fast AdversariaL CalibratiON. Additional comparisons totemperature scaling (Guo et al., 2017) and stochasctic variational inference (SVI) based on Flipout(Wen et al., 2018) are shown in the Appendix A.53.1 Predictive uncertainty for sequence modelingWe trained LSTM models with one hidden layer of 130 hidden units using the RMSPROP optimizer.
Figure 4: Calibration of the predictive uncertainty under domain shift generated by increasing they-zoom of each image in the test set in 10 steps (MNIST data). Left With increasing domain shift theconfidence of predictions with FALCON decreases such that they match accuracy (c.f. overconfidentpredictions of same samples with L2 in Fig. 1). Middle: expected calibration error at 10 increasinglylarge levels of y-zoom. Only EDL and FALCON maintain a low ECE across all levels of y-zoom.
Figure 3: Expected cali-bration error for 20 News-groups data.
Figure 5: Technical robustness of image classification models, quantified by computing the micro-averaged expected calibration error (lower is better). FALCON results in consistently well calibratedand robust predictions across 9 different perturbation strategies.
Figure S1:	Micro-averaged ECE for FALCON with only one loss term loss term, based on the LeNetmodel trained on MNIST.
Figure S2:	Robustness of hyperparameters. Even when varying both hyperparameters, micro-avergedECE for perturbation yzoom was robust.
Figure S3:	Calibration under domain shift of the LeNet model for MNIST data, quantified bycomputing the micro- averaged expected calibration error (lower is better). FALCON results inconsistently well calibrated and robust predictions across 9 different perturbation strategies. L2-TSstands for temperature scaling, SVI stands for Stochastic Variational Inference.
Figure S4:	Distribution of confidence levels for no peturbation . EDL makes the most underconfidentpredictions, which can be seen both in the boxplot as well as in the empirical CDF. While FALCONalso makes underconfident predictions, this is notably less severe than for EDL.
Figure S5: SoftmaX probabilities of a test sample WIth increasingly strong perturbation (y-zoom; samesample series as Fig. 1 and Fig. 4). Top: Predictions of L2-Dropout model start with a very highconfidence, corresponding to a good calibration (Fig. 4 Middle), hoWever, for strong perturbations(epsilon greater than 40) false predictions are made With a very high confidence, reflecting the typicaloverconfident behaviour of the L2-Dropout model When moving aWay from in-domain samples.
Figure S6: Test accuracy for the L2-Dropout model trained on the 20 Newsgroups data. Accuracydeclines gradually with increasing fraction of swapped words until it reaches random levels.
Figure S7: Entropy for OOD predictions based on epsilon 90 on the y-zoom perturbation; withentropy approaching random levels, well calibrated predictions correspond to an entropy close tomaximal entropy. The good calibration of FALCON and, for the MNIST data also EDL, is reflectedby entropy levels that are substantially higher than for the other baseline methods.
Figure S8: Distribution of confidence scores for OOD predictions based on epsilon 90 on the y-zoomperturbation (across all predictions); predictions by baseline models are often severely overconfident.
