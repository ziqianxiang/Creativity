Figure 1: Combiner Architecture. The Sparse Attention Gate truncates entries in the self-attentionmatrix (a). The new sparser attention matrix is passed on via the Hierarchical Attention Block (b).
Figure 2: Distribution of the learned SAG thresholds vs. Top-2 pooling on Search sessions andWikiText. X-axes represent the network layers and Y-axes indicate the induced thresholds.
Figure 3: Distribution of attention distances of Combiner and Top-2 pooling trained on Searchsessions and WikiText. X-axes mark the network layers and Y-axes mark the connection distances.
Figure 4: Distribution of cross-query connection rates of Combiner vs. Top-2 pooling on searchsessions. X-axes represent the network layers and Y-axes indicate the cross-query connection rates.
Figure 5: Distribution of the max widths of connected trees of Combiner and Top-2 pooling onsearch sessions and WikiText.
Figure 6: The latent structures learned by our Combiner model on the search session dataset.
Figure 7: The latent structures learned by our Combiner model on the wikitext dataset.
