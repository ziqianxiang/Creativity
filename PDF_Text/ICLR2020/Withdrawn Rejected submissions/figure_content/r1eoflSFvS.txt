Figure 1: The whole big circle is only one iteration, and the white bottom rectangle on the left isthe optimization process of the inner neural network. The inner process optimizes W by mini-batchSGD to minimize the loss function in training set. The blue part on the right is the gradient descentof cross validation loss on validation set. Weight is fixed and regularization coefficient is updatedfor several epoches.
Figure 2: This Figure shows the performance of DenseNet-40 on CIFAR-10 with different learn-ing rate of hyperparameters. We only list the data at the end of each iteration. Left: With theLR=0.00001, the test accuracy rate of DenseNet-40 is steady increasing. When the learning rateincreases, there is a downward trend in test accuracy rate. Right: The larger the learning rate ofhyperparameters is, the sparser the model obtained in the same iteration process is.
Figure 3: The performance of VGG-16 on CIFAR-100 with different starting pruning epoch. ”st”means starting pruning epoch and the fraction (such as 1/4, 2/4) means the ratio of the initial pruningepoch to the total epoch in an iteration. ”iter” meaans one iteration. ”pr” is the pruning rate and ”testacc” means the accuracy rate in test set.
