Figure 1: Overview of the discriminative variational autoencoder. The prior network and classificationnetwork can be implemented With a simple neural network, e.g., one fully connected layer. The ㊉and 0 represent element-wise addition and multiplication, respectively.
Figure 2: Visualization of the latent space (top) and generated samples (bottom). In this visualization,we randomly sampled 1000 images from MNIST test set. We used the t-SNE (Maaten & Hinton(2008)) to reduce the dimension of latent vectors for plotting.
Figure 3: Learning curve for both Permuted (top) and Split MNIST (bottom). Numbers written nextto the ’VCL - ’ mean the size of random coreset data Nguyen et al. (2018) per each task.
Figure 4: From left to right, real, r2s translation, s2r translation, and sample images generated byDiVA are displayed.
Figure 5: Three general incremental learning settings. This is based on Hsu et al. (2018); van de Ven& Tolias (2018)FeatureExtractorFeatureExtractorFigure 6: The process for incremental learning with the DiVA. At first, the model learns real data.
Figure 6: The process for incremental learning with the DiVA. At first, the model learns real data.
