Figure 1: The HoMM architecture allows for transformations at different levels of abstraction. (a) Forbasic meta-learning a dataset consisting of (input embedding, output embedding) tuples is processedby the meta-network M to produce a function embedding Zfunc, which is processed by the hypernetwork H to parameterize a function Fzfunc, which attempts to compute the transformation onprobe inputs (used to encourage the system to generalize). However, our approach goes beyondbasic meta-learning. The function embedding Zfunc can then be seen as a single input or outputat the next level of abstraction, when the same networks M and H are used to transform functionembeddings based on examples of a meta-mapping (b). To evaluate meta-mapping performance,a probe embedding of a held-out function is transformed by the architecture to yield a predictedembedding for the transformed task. The performance of this predicted embedding is evaluated bymoving back down a level of abstraction and evaluating on the actual target task (c). Because thefunction embedding is predicted by a transformation rather than from examples, new tasks can beperformed zero-shot. (M and H are learnable deep networks, and Fz is a deep network parameterizedby H conditioned on function embedding Z . Input and output encoders/decoders are omitted forsimplicity. See the text and Appendix F.2 for details.)but it can also transform these task functions to adapt to new tasks. This is related to the conceptsof homoiconicity, defined above, and higher-order functions. Under this perspective, basic tasksand meta-mappings from task to task are really the same type of problem. The functions at one levelof abstraction (the basic tasks) become inputs and outputs for higher-level functions at the next levelof abstraction (meta-mapping between tasks).
Figure 2: The HoMM system succeeds at basic meta-learning, which is a necessary prerequisite formeta-mappings. (a) The polynomials domain, Section 4. The system successfully generalizes to heldout polynomials. The solid line indicates optimal performance; the dashed line indicates untrainedmodel performance. (b) The card games domain, Section 5. The system successfully generalizesto held out games, both when trained on a random sample of half the tasks, or when a targetedsubset is held out. The gray dashed line indicates chance performance (otuputting the mean acrossall polynomials, or playing randomly), while the solid lines are optimal performance. The orangedashed lines shows performance on held-out tasks of playing the strategy from the most correlatedtrained task. The system generally exceeds this difficult baseline, thus showing deeper generalizationthan just memorizing strategies and picking the closest. Error-bars are bootstrap 95%-CIs, numericalvalues for plots can be found in Appendix G.
Figure 3: The HoMM architecture performs tasks zero-shot by meta-mappings. (a) The systemgeneralizes to apply learned meta-mappings to new polynomials, and even to apply unseen meta-mappings. The plots show the loss produced when evaluating the mapped embedding on the targettask. For example, if the initial polynomial is p(x) = x + 1, and the meta-task is “square,” the losswould be evaluated by transforming the embedding of p(x) and evaluating how well the mappedembedding regresses on to p(x)2 = x2 + 2x + 1. The results show that the system succeeds atapplying meta-mappings it is trained on to held-out polynomials, as well as applying held-out meta-mappings to either trained or held-out polynomials. The solid line indicates optimal performance; thedashed line is untrained model performance. (b) The system generalizes to meta-mapping new tasksin the cards domain. The system is trained to do the meta-mappings shown here on a subset of itsbasic tasks, and is able to generalize these mappings to perform novel tasks zero-shot. For example,for the “losers” mapping, the sytem is trained to map games to their losers variants. When given aheld-out game, it is able to apply the mapping to guess how to play the losing variation. This plotshows the reward produced by taking the mapped embedding and playing the targeted game. The graydashed line indicates random performance, while the colored dashed lines indicate performance if thesystem did not alter its behavior in response to the meta-mapping. The system generally exceeds thesebaselines, although the switch-suits baseline is more difficult with the targeted holdout. Error-barsare bootstrap 95%-CIs.
Figure 4: The HoMM system can perform meta-mappings from language cues rather than meta-mapping examples. Compare to Fig. 3, which shows the same results when using examples insteadof language. (a) In the polynomials domain, language cues still lead to good performance, even onheld-out tasks or held-out meta-mappings, although examples perform slightly better (Fig. 3a). (b)Similarly, in the cards domain, language cues perform well. Error-bars are bootstrap 95%-CIs.
Figure 5: Comparison of a variety of methods for performing one of the 10% held-out tasks in themore difficult hold-out set in the cards domain. There are a number of ways the system could adapt toa new task: from seeing example of the new task, from hearing the new task described from languagealone, or from leveraging its knowledge about prior tasks via meta-mappings (in this case, fromthe non-losers variations of the same games). The meta-mappings offer a happy medium betweenthe other two alternatives - they only require cached knowledge of prior tasks, rather than needingto collect experience on the task before a policy can be derived, but they outperform a system thatsimply tries to construct the task embedding from a description alone. Language alone is not nearlyas rich a cue as knowledge of how a new task relates to prior tasks.
Figure 6: Once the meta-learning system has been trained on a distribution of prior tasks, itsperformance on new tasks can be tuned by caching its guessed embeddings for the tasks and thenoptimizing those, thus avoiding any possibility of interfering with performance on prior tasks. Startingfrom random embeddings in the trained model results in slower convergence, while in an untrainedmodel the embeddings cannot be optimized well. Error-bars are bootstrap 95%-CIs.
Figure 7:	The system is able to infer polynomials from only seeing a few data points (i.e. evaluationsof the polynomial), despite the fact that during training it always saw 50. A minimum of 15 randompoints is needed to correctly infer polynomials without prior knowledge of the polynomial distribution,but the system is performing well below this value, and quite well above it, although it continues torefine its estimates slightly when given more data.
Figure 8:	Learning curves for basic regression in the polynomials domain.
Figure 9:	Learning curves for meta-mappings in the polynomials domain. Although the resultsseem to be leveling off at the end, we found that generalization performance was slightly increasingor stable in this region, which may have interesting implications about the structure of these tasks(Lampinen and Ganguli, 2019).
Figure 10: Continual learning in the polynomials domain: a more direct comparison. Once themeta-learning system has been trained on a distribution of prior tasks, its performance on new taskscan be tuned by caching its guessed embeddings for the tasks and then optimizing those, thus avoidingany possibility of interfering with performance on prior tasks. Starting with the guessed embeddingsubstantially speeds-up the process compared to a randomly-initialized embedding. Furthermore, thisability to learn is due to training, not simply the expressiveness of the architecture, as is shown byattempting the same with an untrained network.
Figure 11: Integrating new tasks into the system by training all parameters results in some initialinterference with prior tasks (even with replay), suggesting that an approach like the continuallearning-approach may be useful.
Figure 12:	t-SNE embedding of the function embeddings the system learned for the basic card gametasks. (Note that the pairs of nearby embeddings differ in the “suits rule“ attribute, discussed inAppendix F.1.2.)18Under review as a conference paper at ICLR 2020C uoωuφluQTaskBasicis_high_cardis_pokerft is_matchft is_pairsis_blackjackIsJosersis_switched_suitstogglejosersft toggle_switch_suitsFigure 13:	t-SNE embedding of the function embeddings the system learned for the meta tasks (basictasks are included in the background).
Figure 13:	t-SNE embedding of the function embeddings the system learned for the meta tasks (basictasks are included in the background).
Figure 14:	Having a separate embedding space for tasks results in worse performance on meta-mappings. (Results are from only 1 run.)(A-Uo uro-l) XSElΦPE1 UO SSOl■ HoMMFigure 15: Having a separate embedding space for tasks results in noisier, slower learning ofmeta-mappings. (Results are from only 1 run.)HoMM (unshared Z)20Under review as a conference paper at ICLR 2020XSElΦPE1 UO SSol-∙- Trained mapping, trained polynomial-∙- Trained mapping, held-out polynomialHeld-out mapping, trained polynomial-∙- Held-out mapping, held-out polynomialFigure 16: Conditioning the task network on the task embedding, rather than parameterizing it via ahyper network causes it to fail at the meta-mapping tasks. Results are from only 2 runs.
Figure 15: Having a separate embedding space for tasks results in noisier, slower learning ofmeta-mappings. (Results are from only 1 run.)HoMM (unshared Z)20Under review as a conference paper at ICLR 2020XSElΦPE1 UO SSol-∙- Trained mapping, trained polynomial-∙- Trained mapping, held-out polynomialHeld-out mapping, trained polynomial-∙- Held-out mapping, held-out polynomialFigure 16: Conditioning the task network on the task embedding, rather than parameterizing it via ahyper network causes it to fail at the meta-mapping tasks. Results are from only 2 runs.
Figure 16: Conditioning the task network on the task embedding, rather than parameterizing it via ahyper network causes it to fail at the meta-mapping tasks. Results are from only 2 runs.
