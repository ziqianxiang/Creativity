Figure 1: Discriminator with Gradients Reversal Layer (GRL). The red layer is the GRL whichreverses the gradients during backprop. The yellow layer is a dynamics-invariant layer that is sharedwith the classification task.
Figure 2: The ADAIL architecture. “Environment” is sampled from a population of environmentswith varying dynamics, “Demonstrations” are collected from one environment within the environ-ment distribution, “Posterior” is the dynamics predictor, Q(c∣τ); Latent code "c" represents theground truth or learned dynamics parameters; The policy input is extended to include the latentdynamics embedding c.
Figure 3: VAE-based unsupervised dynamics latent variable learning.
Figure 4: Vary x-component of gravity in HalfCheetah environment. The red arrows in the pictureshow the gravity directions.
Figure 5: (a): ADAIL on CartPole-V0. Blue: PPO Expert; green: GAIL with Dynamics Ran-domization; red: ADAIL with latent parameters from the dynamics posterior; light blue: ADAILwith uniformly random latent parameters. (b): GAIL with Dynamics Randomization without (left,2024.89 ± 669.39) or with (right, 2453.63 ± 430.51) GRL on HopperHow does the overall algorithm work in comparison with baseline methods?We demonstrate the overall performance of ADAIL by applying it to three Mujoco control tasks:HalfCheetah, Ant and Hopper. For each of the Mujoco environments, we vary 2 continuous dynam-ics parameters and we compare the performance of ADAIL with a few baseline methods, including1) the PPO expert which was used to collect demonstrations; 2) the UP-true algorithm of Yu et al.
Figure 6: Comparing ADAIL with baselines on Mujoco tasks. Each plot is a heatmap that demon-strates the performance of an algorithm in environments with different dynamics. Each cell of theplot shows 10 episodes averaged cumulative rewards on a particular 2D range of dynamics. Notethat to aid visualization, we render plots for Ant in log scale.
Figure 7: Generalization of our policy to held out parameters on the HalfCheetah environment. Thered rectangles in plots show the blackout regions not seen during policy training.
Figure 8: VAE-ADAIL performance on HalfCheetah5 ConclusionIn this work we proposed the ADaptive Adversarial Imitation Learning (ADAIL) algorithm for learn-ing adaptive control policies from a limited number of expert demonstrations. We demonstrated theeffectiveness of ADAIL on two challenging MuJoCo test suites and compared against recent SoTA.
Figure 9: Comparing ADAIL with a few baselines on HalfCheetah. Each plot is a heatmap thatdemonstrates the performance of an algorithm in environments with different dynamics. Each cellof the plot shows 10 episodes averaged cumulative rewards on a particular 2D range of dynamics.
