Figure 1: CIFAR10 trained on Preact-ResNet18 (left) and VGG16 (right). Main plots show trainingand validation loss of projected paths. Projected paths are obtained by projecting each model stateduring optimization to a corresponding state found by GD with small training loss. Each pair ofconsecutive projected states are then connected by line interpolation. The inset shows training andvalidation loss of training paths.
Figure 2: (a) The training loss v.s. the sum of negative eigenvalues. Red is the best fit line, y = wx+bwith W = —40.07 and b = 1.22 * 10-5. The data is nearly a straight line and the y-intercept isalmost zero, consistent with our prediction in Appendix A.3. (b) The trace evolution v.s. the squaredlearning rate. The result shows a linear relation. Red is best fit line. (c)-(d) Change of the Hessiantrace and determinant during training using isotropic (blue) and SGD noise (red).
Figure 3: Hessian Trace estimation of projected paths found in Section 2 with network architectureof Preact-ResNet18 (left) and VGG16 (right). Red bars represent the errors in trace estimation. Thedecreasing trace during SGD optimization confirms our theory prediction.
Figure 4: (a) Experiments performed as in the main text with DenseNet on CIFAR100. (b) Ex-periments performed as described in Section 1.2 using 4-layer ConvNet on MNIST and (c) Preact-ResNet18 on CIFAR10.
Figure 5: The cosine similarity between H and C and training curve of 2-layer networks trained onCIFAR10 with label-smoothing cross-entropy loss (left) and mean-squared loss (right).
Figure 6: |Ol/Or - 1| plots along training. The smaller this quantity is, the closer a state is to aequilibrium. This shows that a state in the top-1 eigenspace of the Hessian decays much faster toequilibrium then in the raw parameter space and supports the assumption of timescale separation.
Figure 7: Toy model L(θ1,θ2) = 1 遇 + 1)θ2 + α * θ2 trained with gradient descent (left) and noisygradient descent (right).
