Figure 1:	Influence of several SpectroBank layer properties on the network accuracy. (Dataset:Google Speech Commands)(or kernel stride) is shown, exhibiting two different behaviors. First, for large windows (beyond5ms), a large stride lead to a drop in accuracy. Indeed, the filter width (spread of the modulatedwindow or Wavelet) may be much smaller than the filter total length N . Nevertheless, the overlapis measured on the total length. Narrow windows may not overlap at all and information is lostduring the convolution process. Secondly, short kernels (less than 4ms) with large overlap (or smallstride), can render the network short-sighted in time. In that case, long temporal patterns requirethe combination of a large amount of successive output values. The convolutional layers followingthe SpectroBank layer, deeper inside the network, may not be able capture these long patterns. Thisresults as well in a drop of the accuracy observed on Fig. 1b.
Figure 2:	Bandwidth and frequency of the learned Gammatone filters (B of Eq. (4) and f parameters)using the Google Speech Commands datasetWe also note a difference in the low-frequency region below 100Hz, where the distribution drops inour case. We point out that both our dataset and classification task are different, which could explainthe discrepancies. It still shows the high focus of the filters on the range 100Hz - 1kHz, where thedistribution curve is the highest.
Figure 3: Cumulative frequency energy distribution for learned filters on AudioMNIST dataset,SpectroBank-XS network trained with Gammatone (order 4) and SincNet first layer, with 32 filters.
Figure 4: Examples of filter banks in time domain. From left to right: Wavelet filters, Gaussian filters(cosine modulation), Gammatone filters (envelope, cosine and sine modulations) and Gammachirpfilters, for fixed bandwidth and different frequencies.
Figure 5: Dataset energy distribution per class and corresponding labels.
