Figure 1: This figure illustrates our method. Leftmost figure shows the result of training on task1. Examples corresponding to memorable-past, shown with big markers, are chosen using a GPformulation of the neural network. These points usually are the ones that support the decisionboundary. Middle figure shows the result after task 2 where new network functions are regularisedat memorable-past examples to give the same prediction as the previous ones. The resulting greendecision boundary classifies both task 1 and 2 well. The rightmost figure shows the result along withmemorable-past of each task where the performance over the past tasks is maintained.
Figure 2: A pseudo-code for our FROMP algorithm. The additional computations on top of Adamare in lines 4 and 7, where we add the contribution from the functional-regularisation term. Afterevery task, we update the memorable past in Eq. 9-11 which involves a matrix inversion of size M,and a forward pass through the network to compute Λ(x, y).
Figure 3: Permuted MNIST: added kernels across classes (with subtracted diagonal for visualisationpurposes), and performance as a function of memory size (average accuracy after 10 tasks). Mem-orable past examples lead to a more uniform kernel structure that prevents weighting previouslyoverfit examples too highly, e.g. task 1 in the random selection exhibits strong correlation and lowvariance. As we reduce the number of examples in memory, FROMP gracefully reduces accuracy.
Figure 4: Split MNIST: randomly selected samples and memorable past in comparison. 10 samplesper class and method. In line with the toy example in Fig. 1, memorable past examples appear tobe closer to the decision boundary of a classifier: many samples of the memorable past are possiblyhard to distinguish from other classes.
Figure 5: Split CIFAR: performance for each task, and performance variation as function of memorysize, after training on the final task. We run all methods 5 times and report the mean and standarderror. For baselines, We train from scratch on each task andjointly on all tasks achieving 73.6% ± 0.4and 78.1% ± 0.3, respectively. The left figure reports results for 200 memory examples per task.
Figure 6: Three runs of VCL-MP on toy 2D data. These are the middle performing 3 runs out of 5runs with different random seeds. VCL’s inconsistent behaviour is clear.
Figure 7: FROMP (middle performing of 5 runs) and batch Adam on a dataset 10x smaller (400points per task).
Figure 8: FROMP (middle performing of5 runs), left, and batch Adam, right, on a dataset 10x larger(40,000 points per task).
Figure 9: FROMP (middle performing of 5 runs), left, and batch Adam, right, on a dataset with anew, easy, 6th task.
Figure 10: FROMP (middle performing of 5 runs), left, and batch Adam, right, on a dataset withincreased standard deviations of each class’ points, making classification tougher.
Figure 11: FROMP (middle performing of 5 runs), left, and batch Adam, right, on a dataset with 2tasks having overlapping data, which is not separable.
