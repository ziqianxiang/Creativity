Figure 1: Objective mismatch in MBRL arises when a dynamics model is trained to maximize thelikelihood but then used for the policy to maximize a reward signal that is not used during training.
Figure 2: Sketch of the state-action space forsystem identification and MBRL. (Left) In sys-tem identification, the elicitation trajectories aredesigned off-line to cover the entire state-actionspace, without considering a specific task. (Right)In MBRL instead, the data collected during thelearning is often concentrated in trajectories to-wards the goal, with other parts of the state-actionspace being completely unexplored (grey area).
Figure 3: The distribution of dynamiCs models from our PETS exPeriments Plotting in the LL-RewardsPaCe on three datasets, with Correlation CoeffiCients ρ. (Notes: we are using log likelihood (LL) hererather than NLL for easier interpretability and the reward at each point is the mean of 10 trials withthe CEM optimizer, to disentangle the stochasticity of MPC.) The numbers of models evaluated areMcp = 1000 and Mhc = 2400. For the datasets shown in (C,d,f) many of the LL’s are extremely highand outside of the range of the figure, but the trend lines and Correlation CoeffiCients are CalCulated onall Points. There is a trend of high reward to ‘good’ LL that breaks down as the datasets Contain moreof the state-sPaCe than only exPert trajeCtories. Trend lines are regressed from total least squares, andinCluded when the sum of squares solution imPlemented Converges.
Figure 4: The reward versus epoch when re-evaluating the controller leveraging a dynamics model ateach training epoch for different types of dynamics models. Even for the simple cartpole environment,networks of width 500 and depth 3 cannot learn the entire grid dataset: simple models (D, DE) failto achieve full performance, while more advanced models reach higher performance but eventuallyover-fit to available data (P, PE). The over-fitting of the P model is further evaluated in Fig. 5a.
Figure 5: The effect of the dataset choice onmodel (P) training and accuracy in differ-ent regions of the state-space. (Top) whentraining on the complete dataset, the modelbegin over-fitting to the on-policy data evenbefore the performance drops in the con-troller. (Bottom) A model trained only onpolicy data does not accurately model theentire state-space.
Figure 6: Planned trajectories along the expert trajectory for the initial model and the adversariallygenerated model trained to lower the reward. It can be seen how the planned trajectories arequalitatively similar except for the peak at t = 25. There, the adversarially generated model learnedthat applying a small nudge to the dynamics model at the right place/moment yield to significantlyinfluencing the control outcome with minimal change in terms of NLL.
Figure 7: Convergence of the CMA-ES population’s best member overiterations to the minimum rewardachieved with comparable NLL.
Figure 8: Mean reward of PETS trials, with and without model re-weighting, on a log-grid ofdynamics model training sets with number of points S ∈ [10, 10000] and sampling expert-distancebounds ∈ [.28, 15.66]. The re-weighting shows an ability to learn moderate performance atsubstantially lower number of datapoints, but suffers from increased variance in larger set sizes. Theperformance of PETS declines when the dynamics model is trained on points too near to the expertdataset because the model lacks robustness when running online with the stochastic MPC.
Figure 9: Cartpole (Mujoco simulations) learning effi-ciency is suppressed when additional data not relevantto the task is added to the dynamics model training set.
Figure 10: Learning curve for the standard Cartpoletask used in this paper (Xgoal = 0). The median rewardfrom 10 trials is plotted with the mean NLL of thedynamics models at each iteration. The reward reachesmaximum (180) well before the NLL is at it’s minimum.
Figure 11: MPC control with different reward functions with the same dynamics models loadedfrom trials shown in Fig. 10. The cartpole solves tasks further from 0 proportional to the statespace coverage (Goal further from zero causes reduced performance). The distribution of x dataencountered is shown in Fig. 12.
Figure 12: Distribution of x position encountered during the trials shown in Fig. 10. The distributionconverges to a high concentration around 0, making it difficult for MPC to control outside of the areaclose to 0.
