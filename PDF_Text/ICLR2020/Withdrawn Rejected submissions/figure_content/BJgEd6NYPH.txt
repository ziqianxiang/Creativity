Figure 1: Top: Iterates and implicit trust regions of GD and Adagrad on three quadratic objectives Withdifferent condition number κ. Bottom: Average log suboptimality over iterations as well as 90% confidenceintervals of 30 runs with random initializationIt is well known that GD struggles to progress towards the minimizer of quadratics along low-curvature directions (see e.g., Goh (2017)). While this effect is negligible for well-conditionedobjectives (Fig. 1, left), it leads to drastically slow-down when the problem is ill-conditioned (Fig. 1,center). Particularly, once the method has reached the bottom of the valley, it struggles to makeprogress along the horizontal axis. Here is precisely where the advantage of adaptive stepsize methodscomes into play. As illustrated by the dashed lines, Adagrad,s search space is damped along thedirection of high curvature (vertical axis) and elongated along the low curvature direction (horizontalaxis). This allows the method to move further horizontally early on to enter the valley with a smallerdistance to the optimizer W along the low curvature direction which accelerates convergence.
Figure 2: Diagonal mass of neural network Hessian 6h relative to 6e[∣w∣] and 匹㈣口印.]of correspondingdimensionality for random inputs as well as at random initialization, middle and end of training with RMSPropon CIFAR-10. Mean and 95% CI over 10 independent runs.
Figure 3:	Log loss over backpropagations. Mean and 95% CI of 10 runs. Green dotted line indicates 99% acc.
Figure 4:	Log loss over backpropagations. Same setting as Figure 3. See Figure 10 for epoch results.
Figure 5: Iterates (left) and log suboptimality (right) of GD, Adagrad and two full-featured first-order TRalgorithms of which one (1st TR) is spherically constraint and the other (1st TRada) uses Aada as ellispoid.
Figure 6: Share of diagonal mass of the Hessian δH relative to δW of the corresponding Wignermatrix at random initialization, after 50% iterations and at the end of training with RMSprop onMNIST. Average and 95% confidence interval over 10 runs. See Figure 2 for CIFAR-10 results.
Figure 7: Validity of Proposition 2 in the small n regime for Gaussian data.
Figure 8: Level sets of the non-convex, coercive objective function f(w) = 0.5w02 + 0.25w14 - 0.5w12.
Figure 9: Both, the GGN method and saddle-free Newton method make a positive definite quadratic modelaround the current iterate and thereby overcome the abstractedness of pure Newton towards the saddle (compareFigure 8). However, (i) none of these methods can escape the saddle once they are in the gradient manifoldof attraction and (ii) as reported in Mizutani & Dreyfus (2008) the GN matrix can be significantly less wellconditioned than the absolute Hessian (here KGN = 4904 870554 and k∣h∣ = 1.03 so We had to add a dampingfactor of λ = 0.1 to make the GN step fit the plot.
Figure 10: Experiment comparing TR and gradient methods in terms of epochs. Average log loss aswell as 95% confidence interval shown.
Figure 11: Experiment comparing TR and gradient methods in terms of wall-clock time. Average logloss as well as 95% confidence interval shown. The advantage of extremely low-iteration costs offirst-order methods is particularly notable in the ResNet18 architecture due to the large network size.
Figure 12: Original and reconstructed MNIST digits (left), Fashion-MNIST items (middle), and CIFAR-10classes (right) for different optimization methods after convergence.
