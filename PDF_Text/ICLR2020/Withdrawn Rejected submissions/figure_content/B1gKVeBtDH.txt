Figure 1: Illustration of the attention matrix Com-ponents for QA task.
Figure 2: Average variance of passage representa-tions when paired with different questions at dif-ferent layers.
Figure 3: Decomposing Transformers up to layer k, which enables encoding each segment indepen-dently from layer 1 to layer k. Auxiliary supervision of upper layer information from the originalmodel further helps the decomposed model to compensate for information loss in the lower layers.
Figure 4: F1 drop versus speedup of Decomposed BERT model (without auxiliary supervision)when separating at different layers.
Figure 5: Representation distance of BERT vs decomp-BERT and distance of BERT vs decomp-BERT w/o auxiliary loss/supervisionpaper, we present a set of basic calculations to illustrate that the storage cost of caching can besubstantially smaller compared to the inference cost.
