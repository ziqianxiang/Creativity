Figure 1:	Performance (Inception score: the bigger the better, and FID: the lower the better) ofdifferent baselines (blue bars) and corresponding RGANs (orange bar). Our methods consistentlyperform better than baselines on different datasets and criteria.2different baseline models and their robust version with RGAN on two datasets (CIFAR-10 and STL-10).2 It is noted that the robust strategy can consistently improve the baselines on the two datasetsin terms of both the criteria. In addition, we also show the convergence curves in Figure 2. Clearly,when our robust strategy is applied on the baseline GANs, an obvious increase of the inceptionscores can be observed (though the convergence speed is similar to that of baseline models). Allthese experiments indicate that the robust training is indeed necessary and useful.
Figure 2:	Inception score versus training step. Each subfigure shows the comparison between a dif-ferent baseline model (blue curve) and its corresponding robust version (by applying the RGANstrategies, red curve). Robust GANs consistently achieve much better performance though they con-verge in a similar speed to baseline models.
Figure 3:	Inception score of baselines and RGANs on both the original input noise and the worstinput noise on CIFAR-10. Performance of baselines are almost consistently degraded in the worst in-put noise (compared from the original input noise), while their robust versions (trained with RGAN)perform similar and stable for both worst and original input noise.
Figure 4: Visualization and T-SNE embedding on CIFAR-10. (a): Red points are the input noisepoints sampled from the original Gaussian distribution, and blue points are sampled from the worstdistribution. The worst distribution covers a wider range of area, especially low density area oforiginal distribution which might cause poor generation. (b): The worst real distribution (red) andworst generation distribution (blue). It can be noted that the worst data distributions are more similarto each other which is more difficult to be classified. (c): Red points are the images sampled fromreal distributionand blue points are generated by WGAN-GP. (d): Red points are the images sampledfrom real distributionand blue points are generated by RGAN. The data distribution generated byour method is apparently closer to the real distribution.
Figure 5: Face images generated by WGAN-GP, DCGAN and corresponding RGANs. In (a),WGAN-GP generates two obviously strange faces highlighted with red circles. In (c), several re-peated low quality faces are generated by DCGAN highlighted by red circles. Our method achievesbetter results.
