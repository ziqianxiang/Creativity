Figure 1: Diagrams of the variational hyper RNN. Operators are indicated by arrows in differentcolors, and dashed lines and boxes represent the hypernetwork components. (a) Prior distribution inEq. 3. (b) Recurrent model in Eq. 1. (c) Generative model in Eq. 2. (d) Inference model in Eq. 5. (e)The overall computational path. The hypernetwork components are left out.
Figure 2: Qualitative study of VRNN and VHRNN under the NOISELESS setting. (a) and (b)show the values of concatenated data at each time step. (c) shows the KL divergence between thevariational posterior and the prior of the latent variable at each time step for VHRNN. (d) shows theKL divergence for VRNN. (e) shows L2 distance between the predicted mean values by VHRNN andVRNN and the target. (f) shows the predicted log-variance of the output distribution for VRNN andVHRNN.
Figure 3: Qualitative study of VRNN and VHRNN under the SWITCH setting. The layout ofsubfigures is the same as Fig. 2. Vertical red lines indicate time steps when regime shift happen.
Figure 4: Qualitative study of VRNN and VHRNN under the RAND setting. (a) shows the L2 normand standard deviation of the additive noise at each time step. (b) shows the log-variance of the outputdistribution for VRNN and VHRNN.
Figure 5: VRNN and VHRNN parameter-performance comparison.
Figure 6: Implementation of the recurrence model in VHRNN using LSTM cell.
Figure 7: Qualitative study of VRNN and VHRNN under the ADD setting. (a) and (b) show the valuesof concatenated data at each time step. (c) shows the KL divergence between the variational posteriorand the prior of the latent variable at each time step for VHRNN. (d) shows the KL divergence forVRNN. (e) shows L2 distance between the predicted mean values by VHRNN and VRNN and thetarget. (f) shows the predicted log-variance of the output distribution for VRNN and VHRNN.
Figure 8:	Qualitative study of VRNN and VHRNN under the ZERO-SHOT setting. The layout ofsubfigures is the same as Fig. 7.
Figure 9:	Qualitative study of VRNN and VHRNN under the LONG setting. The layout of subfiguresis the same as Fig. 7. Fig. 9a, 9b, 9d, 9e use scientific notations for the value of Y axis.
Figure 10: VRNN and VHRNN Hidden Units vs Performance comparison.
Figure 11:	VRNN and VHRNN arameter-performance comparison using GRU implementation onJSB Chorale dataset.
Figure 12:	Parameter vs Performance comparison among VHRNN, VRNN and HyperLSTM. Wereport FIVO for VHRNN and VRNN, and exact log likelihood for HyperLSTM. We use a Gaussianmixture distribution of 5 components in the HyperLSTM model to estimate log likelihood in Stockdataset. The indicator above each point shows the number of hidden units in the model.
Figure 13:	Hidden units vs Performance comparison among VHRNN, VRNN and HyperLSTM. Wereport FIVO for VHRNN and VRNN, and exact log likelihood for HyperLSTM. We use a Gaussianmixture distribution of 5 components in the HyperLSTM model to estimate log likelihood in Stockdataset. The indicator above each point shows the number of hidden units in the model.
