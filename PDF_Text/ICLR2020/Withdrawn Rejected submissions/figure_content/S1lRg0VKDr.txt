Figure 1: Visualizing the presence of a temporal discrepancy in five of CIFAR100 labels (trainingis detailed in Section 5). The vertical black line is the best overall epoch ts. The dotted and solidcurves represent the averaged validation curve and the label specific validation curves respectively.
Figure 2: Smoothed decomposed validation curves (left) and the equivalent interval plot (right) forCIFAR100 with = 2%. For both plots, the straight black line is the best overall epoch ts . On theinterval plot, the blue lines indicate the interval τi , the blue dots are the centers of the interval, andthe red dots are the task-optimal epoch ti . The intervals have been sorted by their centers4	Quantifying Performance Lost Due To Temporal DiscrepancyIn this section, we present two simple techniques that consider the per-task validation metrics forselecting the best model for testing. Two aspects common to these techniques is their hand-crafted4Under review as a conference paper at ICLR 2020& engineered nature and their inefficiency in terms of deployment, training time and/or inferencetime. The aim with these techniques to assess how much potential gain in performance could beattained ifwe account for per-task validation metrics in selecting a model and we’d like to stress thatthese techniques serve as a proof-of-concept of this gain (if any). They are intended as a baselineand also a stimulus for increasing research into the effect of the subtleties of the validation curveson model performance and selection.
Figure 3: Interval plots for Tiny ImageNet and PadChest with = 0.02. The gray/green lines anddots indicate the interval τi and its center respectively. The black horizontal line is the best overallepoch ts and the red dots are the task-optimal epoch ti . The intervals have been sorted by theircenters5.1	B rute ForceBy brute forcing the best model selection, we wanted to assess how much performance is lost whenthe summarized validation curve is used. This involves evaluating each task with its own set ofoptimal weights determined from its specific optimal epoch. This would require N models in theorybut the number of validation-optimal models is actually lower as many tasks have inter-dependentlearning profiles. These correlated tasks may reach their optimal validation performance at the sameepoch, thus requiring only a single common set of weights for all of them. N also decreases with thetotal number of training epochs as that increases the probability of tasks having the same optimalepoch. On analyzing the task validation curves for the three datasets, we do find that the numberof models required is much lower than N. CIFAR100 and Tiny ImageNet both required less than 60models for brute forcing the evaluation, despite the latter having twice as many labels. Also sinceTiny ImageNet was trained for one-third of the epochs, the number of models reduced drastically.
Figure 4: Reducing the number of models needed to represent all per-task optimal validation points.
