Figure 1: Performance change of each post-processing method when the dimension of learntword vectors increases. Our method is comparable to the method that removes top PCs on skip-gram, and better than that on CBOW, and slightly worse than that on GloVe. However, generallySkipgram and CBOW themselves provide better performance, thus boosting performance by post-processing on those two learning algorithms is more important. In addition, our model doesn’trequire manually picking the dimensions to remove instead the optimal β? is found by optimisation.
Figure 2: Averaged score on 17 tasks vs. Percentage of training data. To simulate the situa-tion where only limited data is available for shrinkage estimation, two algorithms are trained withsubsampled data with different portions, and three postprocessing methods including ours and twocomparison partners are applied afterwards. The observation here is that our method is able torecover the similarity between words better than others when only small amount of data is available.
