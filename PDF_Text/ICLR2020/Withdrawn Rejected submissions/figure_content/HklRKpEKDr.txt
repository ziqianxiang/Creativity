Figure 1: Examples of value factorization for 3 agents: (a) sum of independent utilities (as in VDN,Sunehag et al., 2018) corresponds to an unconnected CG. QMIX uses a monotonic mixture of util-ities instead of a sum (Rashid et al., 2018); (b) sum of pairwise payoffs (Castellini et al., 2019),which correspond to pairwise edges; (c) no factorization (as in QTRAN, Son et al., 2019) corre-sponds to one hyper-edge connecting all agents. Factorization allows parameter sharing betweenfactors, shown next to the CG, which can dramatically improve the algorithm’s sample complexity.
Figure 2: Influence of punishment p for attempts to catch prey alone on greedy test episode return(mean and shaded standard error, [number of seeds]) in a coordination task where 8 agents hunt8 prey (dotted line denotes best possible return). Note that fully connected DCG (DCG, solid) areable to represent the value of joint actions and coordinate maximization, which leads to a betterperformance for larger p, where DCG without edges (VDN, dashed) has to fail eventually (p < -1).
Figure 3: Greedy test episode return for the coordination task of Figure 2 with punishment p = -2:(a) comparison to baseline algorithms; (b) comparison between DCG topologies. Note that QMIX,IQL, VDN and CG (dashed) do not solve the task (return 0) due to relative overgeneralization and thatQTRAN learns very slowly due to the large action space. The reliability of DCG depends on the CG-topology: all seeds with fully connected DCG solved the task, but the high standard error for CYCLE,LINE and STAR topologies is caused by some seeds succeeding while others fail completely.
Figure 4: Greedy test episode return (mean and shaded standard error, [number of seeds]) in anon-decentralizable task where 8 agents hunt 8 prey: (a) comparison to baseline algorithms; (b)comparison between DCG topologies. The prey turns randomly into punishing ghosts, which areindistinguishable from normal prey. The prey status is only visible at an indicator that is placed ran-domly at each episode in one of the grid’s corners. QTRAN, QMIX, IQL and VDN learn decentralizedpolicies, which are at best suboptimal in this task (around lower dotted line). Fully connected DCGand CG can learn a near-optimal policy (upper dotted line denotes best possible return), but a lack ofparameter sharing slows down CG and yields sub-optimal performance in comparison to DCG.
Figure 5: Low-rank payoff approximation for the tasks in (a) Figure 3 and (b) Figure 4. The inlayin (a) magnifies the area of the gray box. Note that in (a) all approximation ranks learn much fasterthan full DCG, and rank 2-4 also have better performance.
Figure 6: Cumulative reward for test episodes on SMAC maps (mean and shaded standard error,[number of seeds]) for QMIX, VDN and fully connected DCG with rank K = 1 payoff approximation(DCG (rank 1)) and additional state-dependent bias function (DCG-V (rank 1)).
