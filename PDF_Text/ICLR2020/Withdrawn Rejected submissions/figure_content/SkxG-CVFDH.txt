Figure 1: The procedure for training with GraphMix . The Fully-Connected Network (FCN) andthe Graph Neural Network (GNN) share linear transformation matrix (W) applied on the nodefeatures. The FCN is trained using Manifold Mixup by interpolating the hidden states HFCN and thecorresponding labels Y . This leads to better features which are transferred to the GNN via parametersharing. The predicted targets generated by the GNN for unlabeled data are used to augment the inputdata for the FCN. The FCN and the GNN losses are minimized jointly by alternate minimization.
Figure 2: 2D representation of the hidden states of Citeseer dataset using (a) GCN, (b) GCN(predicted-targets), (c) GraphMix, and (d) Soft-Rank of Class-specific hidden states (lower Soft-Rank reflectsmore concentrated class-specific hidden states)5	Related Work5.1	Semi-supervised Learning over Graph DataThere exists a long line of work for Semi-supervised learning over Graph Data. Earlier work includedusing Graph Laplacian Regularizer for enforcing local smoothness over the predicted targets forthe nodes (Zhu & Ghahramani, 2002; Zhu et al., 2003; Belkin et al., 2006). Another line of worklearns node embedding in an unsupervised way (Perozzi et al., 2014) which can then be used as aninput to any classifier, or learns the node embedding and target prediction jointly (Yang et al., 2016).
Figure 3: T-SNE of hidden states for Cora (left), Pubmed (middle), and Citeseer (right). Top row isGCN baseline, bottom row is GraphMix.
