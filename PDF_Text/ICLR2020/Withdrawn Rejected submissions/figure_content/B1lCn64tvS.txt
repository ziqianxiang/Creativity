Figure 1: State-action space representation of GQSAT2.3 Graph Neural NetworksWe use Graph Neural Networks (Gori et al., 2005, GNN) to approximate our Q-function due to theirinput size, structure, and permutation invariance. We use the formalism of Battaglia et al. (2018)which unifies most existing GNN approaches. Under this formalism, GNN is a set of functions thattake a labeled graph as input and output a graph with modified labels but the same topology.
Figure 3: Dataset size effect on generalization.
Figure 2: GQSAT number of maximum firstdecisions vs performance. GQSAT shows im-provement starting from 10 iterations confirm-ing our hypothesis of VSIDS initialization prob-lem. Best viewed in colour.
Figure 4: Average number of variable assignments change per(un)SAT-100-430.
Figure 5: Encode-Process-Decode architecture. Encoder and Decoder are independent graph net-works, i.e. MLPs taking whole vertex/edge data array as a batch. k is the index of a message passingiteration. When concatenating for the first time, encoder output is concatenated with zeros.
