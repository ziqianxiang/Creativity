Figure 1: On the left, an illustration of a set of nested predictions and their associated confidence given aninput image of a face. The top block illustrates a desired behavior. Depending on the quality of the input data,one may be able to provide up to a certain level of prediction. This nested learning is the problem addressedin this paper. The bottom block illustrates how standard DNN-based models behave when they are trainedin a end-to-end fashion to perform specific tasks such as face recognition. Clearly the traditional network isover-confident in its predictions (potentially wrong for the last 3 cases), and provides an all or nothing responseinstead of responding only what it can for the given input quality. On the right, we see a real example withresults from the proposed framework; while a sharp image gets all the nested levels with high confidence in ourproposed system, a low-quality one is getting the first two levels with confidence, while the finer one is correctbut low confidence as expected. (Additional examples are presented in Figure 5 in the supplementary material.)Recently, Alsallakh et al. (2017) showed that convolutional neural networks (CNNs) naturally tendto learn hierarchical high-level features that discriminate groups of classes in the early layers, whilethe deeper layers develop more specialized feature detectors. We explicitly enforce this behaviourby creating a sequence of nested information bottlenecks. Looking at the problem of nested learningfrom an information theory perspective, we design a network topology with two important prop-erties. First, a sequence of low dimensional (nested) feature embeddings are enforced for eachlevel in the taxonomy of the labels. This encourages generalization by forcing information bottle-necks [Tishby et al. (1999), Shwartz-Ziv & Tishby (2017)]. Second, skipped connections allow finerembeddings to access information of the input that may be useful for finer classification but not in-formative on coarser categories Ronneberger et al. (2015). Additionally, we show how the explicit
Figure 2: On the left we illustrate the taxonomy of an example of strictly nested labels. First handwrittencharacters are classified as “number” or “letter,” and then these categories are refined into specific numbersand letters. X , Y1 and Y2 denote random variables representing the input, the coarse label, and the fine labelrespectively. Yik represents the random variable associated to each value k in the coarser node, i.e., Yi giventhat yi-1 = k, k ∈ Yi-1. The diagram in the center, illustrate the entropy of a fine and coarse level, and howhaving information about a coarser level may reduce the entropy of the fine level. The right diagram illustratesthe reduction of uncertainty on the labels given the input, and how the uncertainty on the fine labels can bereduced even further if input information and coarse information are combined.
Figure 3: Illustrative scheme of the proposed framework. From left to right, the input data X 〜X, a first setof layers that extract from X a feature representation f1, which leads to Y1 (estimation of the coarse label Y1).
Figure 4: Accuracy on the MNIST data when different ratios of coarse, middle, and fine samples are selectedfor training. Plots report the accuracy on the prediction of the finest category (ten classes: 0 - 9). If We definen1 , n2 and n3 the number of samples for Which We knoW only the coarse, middle, and fine label respectively,the budget associated to a training set is Bn1,n2,n3 = n1g(|Y1 |) + n2g(|Y2|) + n3g(|Y3|). g represents a costfunction associated to labeling a coarse, middle, and fine sample. In this experiment, We tested three models forg, a linear model where the cost is linear, a concave model (We chose g(u) Z log(u)), and a convex model (Wechose g(u) z eu). The colors represent the proportion of coarse samples in the training set. Blue representsmore proportion of coarse samples, and red a larger proportion of fine labels. (Additional results are presentedin Figure 10 in the supplementary material.)5	ConclusionIn this Work We introduced the concept of nested learning, Which improves classification accuracyand robustness. Moreover, it alloWs to leverage information from datasets annotated With differentlevels of granularity. Additionally, experiments suggest that nested models have a very desired be-haviour, e.g., they gradually break as the quality of the test data deteriorates. We shoWed that imple-menting nested learning using a hierarchy of information bottlenecks provides a natural frameWorkto also enforce calibrated outputs, Where each level comes With its confidence value.
Figure 5: These results complement the example illustrated in Figure 1. The output of a standard(end-to-end) DNN and our proposed nested learning version are compared. On the left we showclean images from the test set of CIFAR-10 dataset, on the right, the same examples but blured. Nextto each image our prediction (for the fine, middle, and coarse level) and the prediction of a standard(end-to-end) DNN are displayed. Both DNN share the same architecture and their performance iscompared on Table 1 (rows corresponding to “(end-to-end) D3 = 32%” and “(nested) our D1,2,3 =20%”). As shown in Table 1 the performance of both networks is similar on clean data (i.e., datathat match the train distribution), but our approach can provide more accurate middle and coarsepredictions when the input data is corrupted, moreover, we are significantly less overconfident onthe prediction of out-of-distribution test samples.
Figure 6: Architecture of our CNN for both Fashion-MNIST and MNIST. This model is an adaptation of theU-Net network (Ronneberger et al. (2015)) designed to fit our nested learning framework. The blue boxes repre-sent the feature extracted by convolutional layers. We perform a global average pooling rather than a flatteningto decrease the number of parameters. After the global average pooling, the feature vector is normalized withrespect to the L2 norm (instance normalization layer). The normalized features are followed by fully connectedlayers to compute the final output. The model used to test CIFAR10 set is very similar but has convolutionallayers with more kernels handle this (sightly more complex) task.
Figure 7:	One dimensional toy example that illustrates the problem of prediction overconfidence for inputsamples that are far from the training distribution. In this example, two classes (“1” and “0”) are consideredand we assume that the input X 〜X is one-dimensional. (a) illustrates the empirical distribution of eachclass (on green P(X|Y = 1) and yellow P(X|Y = 0)). In addition, we illustrate (blue distribution) theuniform distribution from which we sample synthetic training instances associates the “rejection” class. Figure(b) shows the confidence output associated to the class “1” and “0” for different values ofx, for a model trainedonly on the original data (standard approach). Figure (c) illustrates the output of the same DNN trained withthe samples associated to the classes “1” and “0”, plus the synthetic samples from the “rejection” class.
Figure 8:	Nested groups of labels obtained by minimizing the non-diagonal components on the confusionmatrix of an auxiliary simple classifier.
Figure 9:	Example of different perturbations applied during testing time. From left to right: the originalsample, and the distorted version with parameters (S, T) = (1, 0.8), (S, T) = (1, 1.0), (S, T) = (1, 1.3), and(S, T ) = (1, 1.5) respectively.
Figure 10: Accuracy on the classification of the fine label for MNIST data. These results complement theresults presented in Figure 4. Figure 4 illustrates the results on clean test data grouping five levels of budgetsand for three cost models (linear, concave, and convex). In this figure, we show the performance of eachindividual model. For each model we display the accuracy on the clean data (dots) as well as the accuracy ondifferent types of distortions. As before, the proportion of coarse and fine labels during training is illustratedcoloring each data point, blue indicates a higher proportion of coarse samples while red a higher proportion offine samples.
Figure 11: Architecture of a test Multi-Task-Learning CNN. This architecture corresponds to a sequence ofthree blocks of two (convolutional + batch normalization) layers followed by a Maxpooling Layer. In order toreduce the dimension of the penultimate feature vector, we perform a global average pooling. This network andour Nested CNN have approximately the same number of parameters and are trained using the same trainingprotocol and data.
Figure 12: Left: we sketch the sections of the model where the mutual information is compared.
Figure 13: These plots compare the accuracy of the classifiers for the cascaded training and thetraditional training. On the top, we represent the evolution of the coarse classifiers during training.
