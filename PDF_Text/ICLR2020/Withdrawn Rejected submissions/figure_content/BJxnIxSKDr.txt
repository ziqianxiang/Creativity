Figure 1: Mint architecture. (a) A Mint Layer applies a task-specific matrix multiplication and vectoraddition to an input vector x. The matrix and vector make up an affine transformation to the inputvector, given by a task network. (b) The modified neural network model with a distinct Mint layeradded after each fully-connected layer.
Figure 2: Visualization of the 50 tasks from Meta-World used in the MT50 evaluation. Mint is able to learnabout 30 of these tasks.
Figure 3: Learning curves on MT10 (left) and MT50 (right). We observe that independent training performswell on both benchmarks. Mint, unlike prior multi-task learning approaches, is able to perform at a similar levelto independent training.
Figure 4: On the left, We show the percent increase in 'ι distance between Mint layers learned for one ofthe duplicate tasks and each of the other tasks in MT10 as compared to the distance between the Mint layerslearned for the two duplicate tasks. We can see that for most of the tasks, the percent increase in `1 distance isapproximately 10%, except that the distance between ‘reach’ and ‘press button top’ is smaller, which could beexplained by the fact that ‘press button top’ is inherently just a reaching task. On the right, we compare Mint toa list of other methods with the same number of layers and find that Mint achieves a significantly higher successrate than any of the other approaches.
Figure 5: Learning curves on goal-conditioned pushing. Mint is able to outperform all other methods in termsof both distance to the goal and the required number of environment steps.
