Figure 1: Discrete Transformer architec-ture. Syntactic transformer stream com-putes the attention distribution which isused to produce next hidden states whilealso constructing the semantic architecturethrough latent hard attention.
Figure 2: Receptive fields for synthetic data. (ToP-L) Ground truth bracketing; (ToP-R) Continuousreceptive field of sparsity regularized discrete model r(i, L) for rows i; (Bot-L) Attention weightedrecePtive field of soft transformer; (Bot-R) Attention weighted sParsity regularized recePtive field ofsoft transformer.
Figure 3: t-SNE plot of (a) semantic embeddings and (b) syntactic controller embeddings of Dis-crete Transformer trained on WMT. Datapoints are colored by POS tag assigned by a unigramtagger trained on the WMT train corpus. Best viewed in color.
