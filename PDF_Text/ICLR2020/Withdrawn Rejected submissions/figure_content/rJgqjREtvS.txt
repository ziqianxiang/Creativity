Figure 1: PSNRs of recent state-of-the-arts for scale factor ×4 on Set5 (Bevilacqua et al., 2012)and Urban100 (Huang et al., 2015). Red names represent our proposed models.
Figure 2: The architecture of the pre-upsampling model CRNet-A. The proposed CISTA block withK recursions is surrounded by the dashed box and its unfolded version is shown in the bottom. S isshared across every recursion.
Figure 3: SR results of “img016” and “img059” from Urban100 with scale factor ×4. Red indicatesthe best performance.
Figure 4: (a) and (b): PSNR of recent state-of-the-arts versus the number of parameters for scalefactor ×4 on Set5. The number of layers are marked in the parentheses; (c) and (d): Training lossof EDSR without residual scaling and CRNet-B.
Figure 5: Performance curve for residual/non-residual networks with different recursions. The testsare conducted on Set5 for scale factor ×3.
Figure 6:	The architecture of the post-upsampling model CRNet-B.
Figure 7:	Simplified network structures of (a) DRRN (Tai et al., 2017a), (b) SCN (Wang et al.,2015), (c) DRCN (Kim et al., 2016b), (d) our model CRNet.
Figure 8:	PSNR of proposed models versus different number of filters on Set5 with scale factor ×3.
Figure 9: PSNR of proposed models versus different number of recursions on Set5 with scale factor×3.
Figure 10: SR results of “img004” from Urban100 with scale factor ×4.
Figure 11: SR results of “img026” from Urban100 with scale factor ×4.
Figure 12: SR results of “img028” from Urban100 with scale factor ×4.
Figure 13: SR results of “img062” from Urban100 with scale factor ×4.
Figure 14: SR results of “ppt3” from Set14 with scale factor ×2.
Figure 15: SR results of “8023” from B100 with scale factor ×4.
Figure 16: SR results of “86000” from B100 with scale factor ×4.
