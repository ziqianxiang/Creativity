Figure 1: Samples drawn from a variational autoencoder trained on MNIST without constraints(left) and with a checkerboard constraint on the output domain (right). For a pixel intensity do-main [-1, 1], the checkerboard constraint forces the image tiles to have average positive or negativebrightness.
Figure 2: Diagram illustrating an iteration of the double description method. Adding a constraint tothe k-constraint set Ak at iteration k + 1 introduces a hyperplane H. The intersection points of Hwith the boundary of the current polyhedron Rk (marked by ◦) are added as rays r6 and r7 to thepolyhedral cone. The ray r2 is cut-off by the hyperplane H and is removed from Rk . The result isthe next iterate Rk+1.
Figure 3: Mean-squared validation loss averaged over all pixels for 10 runs; shaded area denotesstandard deviation. The objective function (6) is computed on a held-out validation set for theproposed constraint parameterization method and unconstrained optimization with subsequent testtime projection. The average optimum over the validation set is obtained as a solution to a convexoptimization problem. For the box delay curve, the box constraints are activated after 25 epochs(after 〜30s), which results in better generalization. The best average validation error during trainingis within 9% of the optimum for the constraint parameterization method with box constraint delayand within 1% of the optimum for the test time projection method.
Figure 4: Learning to solve the orthogonal projection onto a constraint set as defined in (6). Fromleft to right: MNIST sample from a test set, optimal projection by solving a quadratic program,constraint parameterization model inference, and test time projection model inference.
Figure 5: Samples from a constrained variational autoencoder trained with the test time projectionmethod and our constraint parameterization method. The images represent authentic digits whilesatisfying the imposed checkerboard constraint. Inference is significantly faster using our method.
