Figure 1: Extended lottery ticket experiments using ResNet20 on CIFAR10. From the baseline ResNet20 training, weight-magnitudebased pruning is applied at different epoch (columns, labeled as s) to obtain the sparsified structure. Each structure is then rewoundto the weights from another epoch (rows, labeled as v), where v=0 indicates rewind to the initial weight. Lottery tickets emerge muchearlier before the full training ends, achieving matching accuracy compared to the conventional winning ticket, i.e., (s=200,v=200).
Figure 2: (a) Change of the standard deviation of weights (Wstd) in each layer during the training of ResNet20 on CIFAR10. In thebeginning of the training, the weights of different layers change in different rate, dominated by the gradient terms. When the trainingevolves, weight magnitude is primarily determined by the interplay between learning rate and weight decay, resulting in parallelmovement of Wstd . (b) Change of per-layer pruning rate over the epochs. Due to regular shift of Wstd at the later epochs of training,the per-layer pruning rate converges to a saturating point, emerging stable structure for pruning.
Figure 3: The hamming distance of the sparse structure of the lottery tickets at different configuration for ResNet20 on CIFAR10.
Figure 4: The pseudo-pruned-then-retrained (PPR)models for CIFAR10 ResNet20, which exhibit increas-ing capability of memorizing the complex patterns overthe epochs. Validation accuracy for the baseline trainingand validation accuracy after pruning without retrainingare also included for comparison.
Figure 5: Left: Memorization capacity (i.e., training accu-racy) of the models pruned at 3 different epochs. The modelspruned at epoch 120 and 200 show almost identical memoriza-tion capacity, whereas the model pruned at epoch 20 exhibitslower memorization capacity. Right: Memorization capacity ofmodels pruned at different epochs (training data size = 15000).
Figure 6: The results of PPR accuracy check (retraining learning rate=0.01) for (a) ResNet56 on CIFAR10 (sampled at every epoch)and (b) ResNet18 on ImageNet (sampled every 5 epochs).
Figure 7: Structured pruning with variable group size. We consider a group of weights along the channel dimension with a varyinggroup-size (gs). In case of gs=1, it is the same as the element-wise pruning.
Figure 8: Revisiting the extended lottery ticket experiments of Fig. 1, but with the group size (gs) of 8. As before, (s, v) stands for(epoch drawing the sparse structure, epoch for rewinding). In contrast to Fig. 1, the winning tickets found when gs=1 (green color inFig. 1) disappear as the group size becomes 8.
Figure 9: Impact of structured sparsity on the saturation of pruning accuracy. The x-axis corresponds to the epoch when the modelis 80% pruned. The dotted lines correspond to the moving average of the accuracy to show trends. The larger the group size, the laterthe accuracy of the pruned models converges at.
Figure 10: Mode connectivity: Lottery tickets of ImageNet-ResNet50 and CIFAR10-ResNet20 drawn from different epochs arelinearly interpolated, then the accuracy is measured for each interpolation coefficient. It can be observed that only the early winningtickets are connected to the winning tickets (drawn at the end of training, i.e., epoch 90 for ImageNet and epoch 200 for CIFAR10).
