Figure 1: Confidentiality scheme: Left During training, optimize a Policy Ensemble by estimatinggradients using both the policies in the ensemble and the fictitious observer policy. Right Whencollecting a dataset for cloning, the context variable is marginalized out. Thus cloning the PolicyEnsemble can result in a useless policy•	We introduce a novel method APE, as well as the mathematical justification of the notion ofadversarial experts.
Figure 2: Visualization of APE. We set β = 0.6. Arrows indicate action probabilities, and thecolour scale represents the hitting time. Yellow indicates expected reward of 0, while purple indicatesexpected reward of -100, which is the maximum episode length. The top left corner is the goal state,and the adjacent states that are purple are an example of how APE is adversarial to cloning, as thosestates will cause the cloned policy to suffer larger losses.
Figure 3: Visualization of the cloned APE. Thepolicy obtained from cloning the APE trained has av-erage expected reward of -45.18, while the optimalpolicy has an average expected reward of -9, whichis over a 5× increase.
