Figure 1: The DPQ embedding framework. During training, differentiable product quantization isused to approximate the raw embedding table (i.e. the Query Matrix). At inference, only the codebookC ∈ {1, ..., K}n×D and the Value matrix V ∈ RK×d are needed to construct the embedding table.
Figure 2: Illustration of two types of approximation to enable differentiability in DPQ.
Figure 3: Heat-maps of task performance and compression ratio for various K and D values. Darkeris better. Key observations are: 1) increasing K or D typically improves the task performance at theexpense of lower CRs; 2) the combination of a small K and a large D is better than the other wayround.
Figure 4: Extra training cost incurred by DPQ, measured on a medium sized LSTM for LM trainedon Tesla-V100 GPUs. For most K and D values, the extra training time is within 10%, and the extramemory usage is zero. For very large K and D values, DPQ-VQ has better computational efficiencythan DPQ-SX in both memory and speed (as expected).
Figure 5: Code heat-maps. Left: DPQ-SX. Right: DPQ-VQ. x-axis: K codes per group. y-axis: Dgroups. K = D = 32.
Figure 6: Percentage of code bits in codebook which changed from the previous checkpoint. Trans-former on WMT’19 En-De. D = 128 for all runs. Checkpoints are saved every 600 iterations.
Figure 7:	Heat-maps of task performance and compression ratio. Darker is better.
Figure 8:	Task performance vs compression ratio trade-off curves. Each subplot comes from onetask/dataset and contains four configurations: {DPX-SX, DPX-VQ} × {subspace-sharing, NO-subspace-sharing}.
