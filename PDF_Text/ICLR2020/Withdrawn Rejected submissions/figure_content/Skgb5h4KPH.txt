Figure 1: Projection method. (a, b) are for MNIST, (c, d) for CIFAR10. (a, c) Amplitude |Vk | vs.
Figure 2: F-Principle in real datasets. elow and ehigh indicated by color against training epoch.
Figure 3: Poisson's equation. (a) Uref (x). Inset: ∣Uref (k)| as a function of frequency. Frequenciespeaks are marked with black dots. (b,c) ∆F (k) computed on the inputs of training data at differentepochs for the selected frequencies for DNN (b) and Jacobi (c). (d) kh - uref k∞ at different runningtime. Green stars indicate kh - uref k∞ using DNN alone. The dashed lines indicate kh - uref k∞ forthe Jacobi method with different colors indicating initialization by different timing of DNN training.
Figure 4: Fourier analysis for different generalization ability. The plot is the amplitude of the Fouriercoefficient against frequency k . The red dots are for the training dataset, the green line is for thewhole dataset, and the blue dashed line is for an output of well-trained DNN on the input of the wholedataset. For (c), d = 10. The training data is 200 randomly selected points.
Figure 5: 1d input. (a) f (x). Inset : |f (k)|. (b) ∆F (k) of three important frequencies (indicated byblack dots in the inset of (a)) against different training epochs.
Figure 6: F-Principle in fitting a natural image. The training data are all pixels whose horizontalindices are odd. We initialize DNN parameters by a Gaussian distribution with mean 0 and standarddeviation 0.08 (small initial) or 1 (large initial). (a) True image. (b-g) correspond to the case of thesmall initial parameters. (f-h) correspond to the case of the large initial parameters. (b) DNN outputsof all pixels at different training epochs. (c, g) DNN outputs (blue) and the true gray-scale (red) of testpixels at the red dashed position in (a). (d) |h(k)| (green) at certain training epoch and |f (k)| (red) atthe red dashed position in (a), as a function of frequency index. Selected peaks are marked by blackdots. (e, h) ∆F (k) computed by the training data at different epochs for the selected frequencies in(d). (f) DNN outputs of training pixels (left) and all pixels (right) after training. We use a tanh-DNNwith widths 2-400-200-100-1. We train the DNN with the full batch and learning rate 0.0002. TheDNN is trained by Adam optimizer (Kingma & Ba, 2014) with the MSE loss function.
