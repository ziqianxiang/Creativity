Figure 1: Model overview: The m-dimensional user and item sampling probabilities are learnedand then sampled by repeating m Bernoulli trials. The AND operation denotes the self-masking. Themodel is optimized using MSE between an affine transformation of the Hamming distance and theobserved rating.
Figure 2: 2a shows convergence using the validation NDCG@10 on ML-1M, where self-maskingsignificantly speeds UP convergence. We observe the same trend on the other datasets (see AppendixA.1). 2b Shows the test NDCG@10 when varying whether hash codes are sampled stochastically ordeterministically while training and for evaluation. For example, Det.Eval + Stoc.Train correspondsto deterministic sampling of hash codes for evaluation, while sampling stochastically when trainingthe model.
Figure 3: Convergence plot for Yelp, Amazon, and ML-10M.
