Figure 1: Visualization of episodes in the illustrative example of Section 3.4. Model-free value prediction seethe start state on the left and must predict the corresponding color-coded reward on the right. Hindsight valueprediction can leverage the observed structure in the intermediate state to obtain a better value prediction. Inmore detail, this plot shows the second half s2 of initial state s on the left. In the middle, superimposed isthe observed reward-relevant quantity Pi s02(i) that has been color-coded on the s2 vectors. On the right is thecolor-coded reward for each trajectory. The dimension of states is D = 4 in this example.
Figure 2: Learning the value of the initial state in the example of Section 3.4. The dimension of the data isD = 32 for this experiment, with the dimension of the useful data in the next state D2 = 4. The resultsare averaged over 4 different instances, each repeated twice. Note that v+ (dotted line) is using privilegedinformation (the next state).
Figure 3: Network architecture for HiMo. Double blue arrows denote losses on different outputs of the network.
Figure 4: Portal Choice task. Left: an observation in the starting room of the Portal Choice task. Two portals(cyan squares) are available to the agent (orange), each of them leading to a different room deterministicallybased on their position. Right: The two possible goal rooms are identified by a green and red pixel. The rewardupon reaching the goal (blue square) is a function of the room and the initial context.
Figure 5: Results in the Portal Choice task. (a) shows the median performance as a function of environmentsteps out of 4 seeds. (b) shows the value error averaged across states on the same x-axis scale for different valuefunction estimate. (c) is an analysis that shows the cross-entropy loss of a classifier that takes as input φ (solidline) or φ (dotted line) and predicts the identity of the goal room (red or green) as a binary classification task.
Figure 6: Difference in human normalized score per game in Atari, HiMo versus the improved R2D2 after200k learning steps, alongside learning curves for a selection of HiMo worst and top performing games. Notethat the high variance of the curves in Atari between seeds can usually be explained by the variable timestep atwhich different seeds jump from one performance plateau to the next.
Figure 7: (a) The bowling game in Atari, where a delayed reward can be predicted by the intermediate event ofthe ball hitting the pins. (b-c) Learning curves for HiMo in the bowling game using two different RL methods:a value-based method (R2D2) in (b) and a policy-gradient method (IMPALA) in (c).
