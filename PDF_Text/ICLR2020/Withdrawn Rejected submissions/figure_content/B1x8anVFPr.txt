Figure 1: (a) Post-LN Transformer layer; (b) Pre-LN Transformer layer.
Figure 2:	Performances of the models optimized by Adam and SGD on the IWSLT14 De-En task.
Figure 3:	Norm of expected gradients for Pre-LN/Post-LN TransformerWe further study the gradient statistics for the Post-LN Transformer after the warm-up stage withAdam. It can be seen from the figure that the scale of the gradients are very small, and the model canbe trained with large learning rates. We believe the gradient scale is one of the reasons that the Post-LN Transformer needs a careful learning rate scheduling in the beginning. Since the gradients arelarge for some layers, using a large learning rate without warm-up may make the training unstable(see Appendix I). As the gradients are well-behaved for the Pre-LN Transformer, we will show thatthe learning rate warm-up stage can be removed for this model architecture in the next section.
Figure 4: Performances of the models on the IWSLT14 De-En task and WMT14 En-De taskfollowing Vaswani et al. (2017) and then use the inverse square root learning rate sCheduler. Forall experiments above, we use the Adam optimizer and set the hyper-parameter Î² to be (0.9, 0.98).
Figure 5: Performances of the models on unsupervised pre-training (BERT) and downstream tasksUnsupervised Pre-training (BERT) We record validation loss of the model checkpoints and plotthem in Figure 5(a). Similar to the machine translation tasks, the learning rate warm-up stage canbe removed for the Pre-LN model. The Pre-LN model can be trained faster. For example, thePost-LN model achieves 1.69 validation loss at 500k updates while the Pre-LN model achievessimilar validation loss at 700k updates, which suggests there is a 40% speed-up rate. Note thatTwarmup (10k) is far less than the acceleration (200k) which suggests the Pre-LN Transformer iseasier to optimize using larger learning rates. We also evaluate different model checkpoints on thedownstream task MRPC and RTE (more details can be found in the appendix). The experimentsresults are plotted in Figure 5(b) and 5(c). We can see that the Pre-LN model also converges fasteron the downstream tasks.
Figure 6:	Norm of expected gradients of W2 in the last FFN sub-layer in different size of theTransformer architectureEmpirical verification of Theorem 1 In Theorem 1, the theory suggests that for any sizes of thePost-LN Transformer, the scale of the gradient norm in the last FFN sub-layer remains the same.
Figure 7:	Performances of the models on the IWSLT14 De-En task.
