Figure 1: The heatmaps of standard deviations of ensemble values in the Deep-sea envi-ronment. High values are marked in blue and low in white. At the beginning of training(left picture) the standard deviation is high for all states. Gradually it is decreased in thestates that have been explored. Finally (the right) the reward state is found. Note that theupper-right part of the board is unreachable.
Figure 2: Comparison of number of episodes needed to solve the deep-sea environment withgiven grid size N . Orange dots marks trials which were unable to solve problem in 30000episodes. Large problem instances (N > 20) are solved only when exploration bonus is used(right-most plot, κ = 50)we work with with the biggest map with 24 rooms, see Figure 3.3 In order to concentrate onthe evaluation of exploration we chose to work with sparse rewards. The agent gets reward1 only if it reaches the treasure room, otherwise the episode is terminated after 300 steps.
Figure 3: The biggest Montezuma’s Revenge map, con-sisting of 24 room. The goal is to reach the roommarked with G. The agent needs to avoid traps(marked in red) and pass through doors (marked inblue). Doors are open using keys (marked in yellow).
Figure 4: Example (10, 10)Sokoban board with 4boxes. Boxes (yellow) areto be pushed by agent(green) to designed spots(red). The optimal solutionin this level has 37 steps.
Figure 5: Learning curve (left axis) and the size of ex-plored graph (right axis) of Sokoban states. The shapeof the latter plot may show a gradual switch from ex-ploration to exploitation.
