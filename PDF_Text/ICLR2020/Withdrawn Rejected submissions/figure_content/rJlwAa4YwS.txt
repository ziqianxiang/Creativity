Figure 1: Two examples of two dimensional lattices. In the left, a cubic lattice with basis vectorsb1 = (0, ∆), b2 = (∆, 0). In the right, a hexagonal lattice with basis vectors b1 = (0, 2),b2 = (√3,1). Also shown for each lattice is the cell Po, containing the origin. The parameter of thecubic lattice ∆ has been set so that the areas of both lattice cells are identical.
Figure 2: Pre/post dithered quantization in the context of lattice representation learning. The systemin the left is used during inference time, the one in the right during training time.
Figure 3: Comparison of the performance (negative log likelihood in nats) of a rectangular latticeVAE, a hypothetical high dimensional lattice (in green), approximated using a Gaussian VAE wherethe variance of the approximate posterior is not data dependent, and and a Gaussian VAE (in red)where the variance of the Gaussian of the approximate posterior is allowed to be data dependent.
Figure 4: Comparison of the performance of a hypothetical high dimensional lattice, approximatedusing Gaussian dithering, and several quantizers trained using straight-through estimation. Thex-axis is the average word negative log likelihood in bits. The y-axis is the representation cost of thequantized representation vector, averaged per word. Lower is better for both. All numbers plotted arefor the test data.
