Figure 1: ROC curves for networks ensembles trained on MNIST,evaluated on notMNIST (left) and fashionMNIST (right).
Figure 2: LogitS and probabilities from 10 randomly initialized ReLU-networks (left) and Fourier-networks (right). x-axis: Input x, y-axis: Estimated logits/probabilites for input x.
Figure 3: A 2-dim classification problem. Left image: Input, color indicates label.
Figure 4: Predictions of 4 randomly initialized networks.
Figure 5: Influence of initialization and activation functionx-axis: log(σ), y-axis: - log(Area above ROC curve), higher is betterLeft to right: notMNIST, fashion MNIST, flipped MNIST, masked notMNISTmasked version of notMNIST, see figures 20 and 21 in appendix L for masked fashionMNIST andimages in which each pixel is independently sampled from MNIST images, so each pixel has thesame value distribution as in MNIST. Even with the “looking at the corners shortcut” disabled, theFourier networks still perform well.
Figure 6: Entropy histograms for SVHN as in-distribution set (top row) and CIFAR10 as out-of-distribution set (bottom row). Comparison between using sin(x) (right-hand side) or ReLU activa-tions (left-hand side) on the last layer of a Shallow VGG network.
Figure 7: Vector fields for differential equations (equation 3), (equation 4)di + πbiand using the dot notation for the time derivative we get the simple differential equationC = U ∙ (1 — c2)U = c(4)(See figure 7 for a plot of the corresponding vector fields.)Lemma 1: For (LebesgUe-)almost all (c0 ,u0) ∈ [-1,1] × R the solution curve of (equation 4)starting at t = 0 at (c0, U0) will go either to (1, +∞) or to (-1, -∞) for t → ∞.
Figure 8: Fourier approximation for set S of four small clusters in 1DF Gauss NetworksWe can define “Gauss networks” for which we take as activation functions Gauss functionsg(x) := e-x2/2Figure 9: Example function g(0.4 ∙ X — 1)Using this function instead of ReLU (x) or sin(x) gives in the 2-dim toy example the figure 10(compare with figure 4).
Figure 9: Example function g(0.4 ∙ X — 1)Using this function instead of ReLU (x) or sin(x) gives in the 2-dim toy example the figure 10(compare with figure 4).
Figure 10: Upper row: Input, Average output for ensemble of 50 ReLU / Gauss / Fourier networks.
Figure 11: Input - the points on the left have label “red”, in the middle “green”, and on the right“blue”.
Figure 12: From left to right: Output of ReLU network, and Fourier networks with σ =0.15, 0.55, 1, 2. Areas with confidence < 0.75 are grayed out.
Figure 13: Embedding MNIST / fashionMNIST into a random backgroundMetric		Area Under ROC			Classifier accuracy	Model	Original	With background	Original	With backgroundReLU nets	91.8%	91.1%	98.2%	91.3%Fourier nets	99.7%	95.8%	97.5%	91.2%Nearest Neighbor	98.9%	77.0%	96.9%	76.2%Table 4: Comparison between an ensemble of one-layer ReLU networks, an ensemble of one-layerFourier networks and just using the distance to the nearest neighbor in feature space (which in thiscase coincides with the pixel space) as a score for OOD detection. The in-distribution dataset isMNIST while the out-of-distribution data is coming from FashionMNIST.
Figure 14: 4 digitsH	ReLU networks with infinitesimal initializationFor given training data and infinitesimal initialization, there are only finitely many possibilities towhich networks can converge (independent of network size), see (Maennel et al., 2018). The argu-ments given in that paper show also that with increasing number of weights we should with highprobability end up at the same possibility.
Figure 15: 10 random samples of ROC curves for networks ensembles trained on MNIST,evaluated on notMNIST (left) and fashionMNIST (right).
Figure 16: Histogram of the predictive entropy from figure 3 in (Lakshminarayanan et al., 2017)(first three columns) compared to the corresponding Fourier networks (last column). Top row:Known classes (MNIST), bottom row: Unknown classes (notMNIST).
Figure 17: FashionMNIST (top) and notMNIST (bottom) data sets.
Figure 18: Constructed data sets.
Figure 19: Average pixel value ≤ thresholds 0, 0.0001, 0.001, 0.01, 0.1 on MNISTdeviation σ. We consider the area above the ROC curve as “error” and plot - log(error) against theinitialization σ used for the networks. The resulting graphs are in figure 20.
Figure 20: Influence of initializationx-axis: log(σ), y-axis: - log(error), higher is betterTest sets: Circles and lines, pseudo mnist, IID noise.
Figure 21: Influence of initialization, axes as in figure 5Left to right: masked notMnist / fashion MNIST, pixels sampled independently from MNISTM Training on fashion MNISTWe use the same two hidden layer network architecture, initializations, and learning rates as in figure1, but train on fashion MNIST, the result is in figure 22 below. Since this is a more challenging dataset, the same architecture gives not as good results as for MNIST (note the x-axis extends nowto 0.2), but the difference between ReLU and sin(x) networks is comparable, resulting in graphs(figure 22) that look very similar to figure 1.
Figure 22: 10 random samples of ROC curves for networks ensembles trained on fashion MNIST,evaluated on notMNIST (left) and MNIST (right).
Figure 23: Influence of initialization, networks trained on fashion MNISTx-axis: log(σ), y-axis: - log(error), higher is betterTest sets: Circles and lines, MNIST, notMnist.
Figure 24: Input for the 2-dim example.
Figure 25: ReLU networks, extrapolating from a line to 2d.
Figure 26: Fourier networks, extrapolating from a line to 2d.
Figure 27: Different Fourier networks with weights initialized by samples from same distribution,20000 hidden sin-neuronsSo in this case, we can no longer rely on different sampling from the same distribution, but needto change the distribution itself. For higher dimensional input, this effect may be more theoreticalbecause the number of neurons required to get into that regime is no longer realistic.
Figure 28: Ensemble average, 100 hidden ReLU-neuronsLeft to right: σ1,σ2 = (3, 1), (0.01, 0.01), (5, 5) (0.01, 1)In particular, the networks initialized by σ1 , σ2 = (5, 5) could in theory contain the same initializa-tions as the networks initialized by σ1, σ2 = (0.01, 1), but the emphasis shifts to different networks:The latter give blue cones extending downwards most of the time, whereas such a result is not amongthe 50 samples I obtained for σ1, σ2 = (5, 5).
Figure 29: Restricting MNIST input to half of the image•	Only allow access to “line detectors” with certain directions.
Figure 30: Convolution with stripes in some directions, centered at some locationsThis seems to give a significant increase of the area under the ROC curve in particular for the difficultcase “trained on 0-4, evaluate 5-9 as outliers”:Area under ROC curve“Outliers”	sin	sin with half input	sin with stripes	combinedMNIST 5-9	96.5%	97.3%	97.5%	97.6%fashionMNIST	99.3%	99.5%	99.6%	99.7%notMNIST	99.9%	99.9%	99.9%	99.9%Same test with restriction to half images on fashionMNIST:The classifier is trained on fashionMNIST 0-4.
Figure 31: Activation function input for images of “0”, left: largest , right: smallest weight.
Figure 32: notMnist activationHowever, when we plot the input to the sin activation for images from notMnist, we no longer seethe peaks at maxima / minima of sin, suggesting that the output for these images will be “random”:31Under review as a conference paper at ICLR 2020Q More experimental results for SVHN vs CIFAR10Figure 33 shows which are the regions of optimal initialization for the final fully-connected layerfor both ReLU and Fourier networks. Note that if the initialization is too large, gradient descent onthe Simple VGG Fourier network will sometimes fail to converge.
Figure 33: Influence of initialization on AUROC for SVHN, axes like in figure 5.
Figure 34: Entropy histograms for SVHN as in-distribution set (top row) and CIFAR10 as out-of-distribution set (bottom row). Comparison between using sin(x) (right-hand side) or ReLUactivations (left-hand side) on the last layer of a Simple CNN network.
