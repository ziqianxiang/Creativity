Figure 1: Illustration of TeamRegwith two agents. Each agent’s policyis equipped with additional heads thatare trained to predict other agents’actions and every agent is regular-ized to produce actions that its team-mates correctly predict. Note that themethod is depicted for agent 1 only toavoid cluttering.
Figure 2: Illustration of CoachRegwith two agents. A central model, thecoach, takes all agents’ observationsas input and outputs the current mode(policy mask). Agents are regular-ized to predict the same mask fromtheir local observations only and op-timize the corresponding sub-policy.
Figure 3: Multi-agent tasks used in this work. (a) SPREAD: Agents must spread out and covera set of landmarks. (b) BOUNCE: Two agents are linked together by a spring and must positionthemselves so that the falling black ball bounces towards a target. (c) COMPROMISE: Two linkedagents must compete or cooperate to reach their own assigned landmark. (d) CHASE: Two agentschase a (non-learning) prey (turquoise) that moves w.r.t repulsion forces from predators and walls.
Figure 4: Learning curves (mean return over agents) for all algorithms on all four environments.
Figure 5: Learning curves (mean return over agents) for the ablated algorithms on all environments.
Figure 6: Average performance difference(∆perf) between the two agents in COM-PROMISE for each 150 runs of the hyper-parameter searches (left). All occurrences ofabnormally high performance difference areassociated with high values of λ2 (right).
Figure 7: Comparison between enabling and disabling the regularizing weights λ1 and λ2 forMADDPG+TeamReg on the SPREAD environment. Values are averaged over the 3 agents andover the 3 seeds used in the hyper-parameter exploration.
Figure 8: Visualization of two different BOUNCE evaluation episodes. Note that here, the agents’colors represent their chosen policy mask. Agents have learned to synchronously identify twodistinct situations and act accordingly. The coach’s masks (not used at evaluation time) are displayedwith the timestep at the bottom of each frame.
Figure 9:	(a) Entropy ofthe policy mask distributionsfor each task, averaged overagents and training seeds.
Figure 10: Hyper-parameter tuning results for all algorithms. There is one distribution per(algorithm, environment) pair, each one formed of 50 points (hyper-parameter configurationsamples). Each point represents the best model performance averaged over 100 evaluation episodesand averaged over the 3 training seeds for one sampled hyper-parameters configuration (total of 300performance values per sampled configuration).
Figure 11: Summarized performance distributions of the sampled hyper-parameters configurationsfor each (algorithm, environment) pair. The box-plots divide in quartiles the 49 lower-performingconfigurations for each distribution while the score of the best-performing configuration is high-lighted above the box-plots by a single dot.
Figure 12: Hyper-parameter tuning results for ablated algorithms compared to their full approachcounterparts and MADDPG. There is one distribution per (algorithm, environment) pair, eachone formed of 50 points (hyper-parameter configuration sample). Each point represents the bestmodel performance averaged over 100 evaluation episodes and averaged over the 3 training seedsfor one sampled hyper-parameters configuration (total of 300 performance values per sampledconfiguration).
Figure 13: Summarized performance distributions of the sampled hyper-parameters configurationsfor each (ablated algorithm, environment) pair. The box-plots divide in quartiles the 49 lower-performing configurations for each distribution while the score of the best-performing configurationis highlighted above the box-plots by a single dot.
Figure 14: Learning curves for TeamReg and the three baselines on COMPROMISE. We see thatwhile both agents remain equally performant as they improve at the task for the baseline algorithms,TeamReg tends to make one agent much stronger than the other one. This domination is optimal aslong as the other agent remains docile, as the dominant agent can gather much more reward than ifit had to compromise. However, when the dominated agent finally picks up the task, the dominantagent that has learned a policy that does not compromise see its return dramatically go down and themean over agents overall then remains lower than for the baselines.
Figure 15:	Agent’s policy mask distributions. For each (seed, environment) we collected the masksof each agents on 100 episodes.
Figure 16:	Visualization sequences on two different environments. An agent’s color represent itscurrent policy mask. For informative purposes the policy mask that the coach would have producedif these situations would have happened during training is displayed next to the frame’s timestep.
Figure 17: Entropy of the policy mask distributions for each task and method, averaged overagents and training seeds. Hmax,k is the entropy of a k-CUD. (b) Hamming Proximity betweenthe policy mask sequence of each agent averaged across agent pairs and seeds. randk stands foragents independently sampling their masks from k-CUD. Error bars are SE across seeds.
Figure 18: Learning curves (mean return over agents) for all algorithms on the SPREADenvironment for varying number of agents. Solid lines are the mean and envelopes are the StandardError (SE) across the 10 training seeds.
