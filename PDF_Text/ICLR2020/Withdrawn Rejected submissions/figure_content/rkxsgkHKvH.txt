Figure 1: ReLU, Tanh, and Swish approximations derived for initializing polynomial activations.
Figure 2: Learned polynomial activations across two different runs by Network-1 (1st and 4th row),Network-2 (2nd and 5th row), and Network-3 (3rd and 6th row) on MNIST dataset. Best viewed incolor.
Figure 3: Learned polynomial activations on CIFAR10 data. Each row represents activations for agiven network. All the activations in a network are split into 6 columns for better visibility. ForR164, we only plot the first activation of each residual block for better visibility. Best viewed incolor.
