Figure 1: (a) Generalization error (yel low, the difference between training and test classifi-cation accuracy) in CIFAR10 and (b) generalization error in CIFAR10 with random labels.
Figure 2: The plot in a) shows testing vs training cross-entropy loss for networks trainedon the same data sets but with different initializations. The graph in b) shows the testingvs training loss for the same networks, normalized by dividing each weight by the Frobeniusnorm of its layer. Notice that all points have zero classification error at training. The redpoint in b) refers to a network trained on the same CIFAR-10 data set but with randomizedlabels. It shows zero classification error at training and test error at chance level. The blackline is a square-loss regression of slope 1 with positive intercept. The blue line is the diagonalat which training and test loss are equal. The networks are 3-layer convolutional networks.
Figure 3: Classification error in CIFAR-10 as a function of number of parameters for fixedtraining set. The DNN is the same as in Figure 1. The classification error at test does notincrease here when increasing the number of parameters beyond the size of the training set .
Figure 4: a) Test loss for the normalized networks vs the product of the unnormalizedFrobenius norms of the layers, b) test classification error as a function of the normalized testloss and c) test classification error vs the product of the unnormalized Frobenius norms ofthe layers. The network architecture is the same as in the top of Figure 2.
Figure 5:	Random Pretraining vs Generalization Performance on CIFAR-10: a 5-layerConvNet (described in section L.2) is pretrained on training data with partial ly “corrupted”labels for 30 epochs. It is then trained on normal data for 80 epochs. Among the networksnapshots saved from all the epochs we pick a network that is closest to an arbitrarily (butlow enough) chosen reference training loss (0.006 here). The number on the x axis indicatesthe percentage of labels that are swapped randomly. Loss is cross-entropy loss; error isclassification error. As pretraining data gets increasingly “corrupted”, the generalizationperformance of the resultant model becomes increasingly worse, even though they have similartraining losses and the same zero classification error in training. Batch normalization (BN)is used. After training, the means and standard deviations of BN are “absorbed” into thenetwork’s weights and biases. No data augmentation is performed.
Figure 6:	Standard Deviation in weight initialization vs generalization performance onCIFAR-10: the network is initialized with weights of different standard deviations. The othersettings are the same as in Figure 5. As the norm of the initial weights becomes larger, thegeneralization performance of the resulting model is worse, even though all models have thesame classification error in training.
Figure 7: Same as Figure 5, but on CIFAR-100.
Figure 8: Same as Figure 6, but on CIFAR-100.
Figure 9: Test loss/error vs training loss with all networks normalized layerwise by the L1norm (divided by 100 to avoid numerical issues because the L1 norms are here very large).
Figure 10: Left: test loss vs training loss with all networks normalized layerwise by theFrobenius norm. Right: test loss vs training loss with all unnormalized networks. The modelwas a 3 layer neural network described in section L.1.1 and was trained with 50K exampleson CIFAR10. In this experiments the networks converged (and had zero train error) but notto the same loss. All networks were trained for 300 epochs. The losses range approximatelyfrom 1.5 × 10-4 to 2.5 × 10-3 . The numbers in the figure indicate the amount of corruptionof random labels used during pretraining. The slope and intercept of the line of best fit are0.836 and 0.377 respectively. The ordinary and adjusted R2 values are both 0.9998 while theroot mean square (RMSE) is 4.7651 × 10-5 .
Figure 11: (Part 1) Random pretraining experiment with batch normalization on CIFAR-10 using a 5 layer neural network as described in section L.2. The red numbers in thefigures indicate the percentages of “corrupted labels” used in pretraining. The green starsin the figures indicate the precise locations of the points. Top: Training and test lossesof unnormalized networks: there is no apparent relationship. Bottom: under layerwisenormalization of the weights (using the Frobenius norm), there is a surprisingly good linearrelationship between training and testing losses, implying tight generalization. See next pagefor the product of L2 norms and classification errors.
Figure 11:	(Part 2) Top the product of L2 norms from all layers of the network. We observea positive correlation between the norm of the weights and the testing loss. Bottom: underlayerwise normalization of the weights (using the Frobenius norm), the classification errordoes not change.
Figure 12:	(Part 1) Same as Figure 11 but using different standard deviations for initializationof the weights instead of “random pretraining”. The red numbers in the figures indicate thestandard deviations used in initializing weights. The “RL” point (initialized with standarddeviation 0.05) refers to training and testing on completely random labels.
Figure 12: (Part 2) Same as Figure 11 but using different standard deviations for initializationof the weights instead of “random pretraining”. The red numbers in the figures indicate thestandard deviations used in initializing weights. The “RL” point (initialized with standarddeviation 0.05) refers to training and testing on completely random labels.
Figure 13: Plot of test error Vs the product of the Frobenius norms of the layers ∣∣ W∣∣product =“L=1 Il Wi Il - The model WaS a 3 layer neural network described in section L.1.1 and trainedwith 50K examples on CIFAR10. The models were obtained by pretraining on random labelsand then fine tuning on natural labels. SGD without batch normalization was run on allnetworks in this plot until each reached approximately 0.0044 ± 0.0001 cross-entropy losson the training data. Similar results can be found in Neyshabur et al. (2017); Liang et al.
Figure 14:	Test classification error Vs training Cross-entropy loss with all networks normalizedlayerwise. The model was a 3 layer neural network and was trained with 50K examples onCIFAR10. The empirical dependence of classification error on normalized cross-entropy lossshown here is different from the predicted Bartlett et al. (2003) upper bound provided by ψ-1(pers. comm. Y. Yao).
Figure 15: The figure shows the cross-entropy loss on the test set vs the training loss fornetworks normalized layerwise in terms of the Frobenius norm. The model was a 3 layerneural network described in section L.1.2 and was trained with 50K examples on MNIST.
Figure 16: The figure shows the cross-entropy loss on the test set vs the training loss fornetworks normalized layerwise in terms of the Frobenius norm. The model was a 3 layerneural network described in section L.1.1 and was trained with 50K examples on CIFAR10.
Figure 17:	(Part 1) Same as Figure 11 but on CIFAR-100.
Figure 17: (Part 2) Same as Figure 11 but on CIFAR-10027Under review as a conference paper at ICLR 20204 2 08 6)teN dezilamronU( ssoL tseT20.060.080.10.12	0.14	0.16	0.18	0.2	0.22Train Loss (Unormalized Net)Test Loss (Normalized Net)4.60524.60514.6054.60494.60484.60474.60464.6045
Figure 18:	(Part 1) Same as Figure 12 but on CIFAR-100.
Figure 18: (Part 2) Same as Figure 12 but on CIFAR-100.
Figure 19: (Part 1) Top: Unnormalized cross-entropy loss in CIFAR10 for normal ly labeleddata. Middle: Cross-entropy loss for the normalized network for normal ly labeled data.
Figure 19: (Part 2) Top: Unnormalized cross-entropy loss in CIFAR10 for randomly labeleddata. Midd le: Cross-entropy loss for the normalized network for randomly labeled data.
Figure 20: Histogram of the values of f(x) for the most likely class of the layerwise normalizedneural network over the 50K images of the MNIST training set. The average value f (x) ofthe most likely class according to the normalized neural network is 0.026683 with standarddeviation 0.007144.
