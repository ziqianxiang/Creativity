Figure 2:	The methods are divided into five groups: Supervised with 100% labels, semi-supervisedwith 10% labels, self-supervised without labels, generative models, and training from-scratch. Theplots ShoW results on ∙natural, ∙specialized, and ∙structured datasets, respectively. Thebar height indicates the mean top-1 accuracy across all methods and 3 test repeats within each group.
Figure 3:	Average top-1 accuracy across the tasks in each group. The x-axis indexes the methods.
Figure 4:	Absolute difference in top-1 accuracy between method pairs for each dataset. Thebar colour indicates the task group: ∙natural, ∙specialized, and ∙structured. Left:Sup-100% versus From-Scratch — supervised pre-training yields a large improvement on the• natural datasets and some others. Right: SUP-ROTATION-100% versus Sup-100% — theadditional self-supervised loss on top of supervised loss yields improvements, especially on the• structured tasks. Note that the y-scales differ; the left-hand side's differences are much larger.
Figure 5: Kendall rank correlation coefficient be-tween fine-tuning and linear evaluation on eachdataset.
Figure 6: Performance of two methods under var-ious evaluation time constraints.
Figure 7: Ranking of the methods using the average scores across datasets (validation split) usingthree different metrics: top-1 accuracy (left), mean-per-class accuracy (center), Cohen’s quadratickappa (right). The methods on the x-axis are sorted according to the highest scores according toeach metric. Although there are some minor changes in the ranking between top-1 and Cohen’squadratic kappa, the overall performance of groups of methods remains unchanged.
Figure 8:	Training from scratch with only 1000 examples. Best hyperparameter values are markedin red and those within 2% in green to show sensitivity.
Figure 9:	Training from scratch on the full datasets. Best hyperparameter values are marked in redand those within 2% in green to show sensitivity.
Figure 10:	ImageNet supervised representation fine-tuned on only 1000 examples. Best hyperpa-rameter values are marked in red and those within 2% in green to show sensitivity.
Figure 11:	ImageNet supervised representation fine-tuned on the full datasets. Best hyperparametervalues are marked in red and those within 2% in green to show sensitivity.
Figure 12: Heavy parameter sweep on CIFAR-100 dataset. Top 2 rows show the results of fromscratch algorithm and bottom 2 rows show the results of supervised 100% algorithm. Each plotshows the violin plot of the results with respect to a given parameter. Here we show the resultsevaluated on full datasets and 1k datasets.
Figure 13: Heavy parameter sweep on DMLab dataset. Top 2 rows show the results of from scratchalgorithm and bottom 2 rows show the results of supervised 100% algorithm. Each box shows theviolin plot of the results with respect to a given parameter. We show the results evaluated on fulldatasets and 1k datasets.
Figure 15: Absolute difference in top-1 accuracy between pairs of methods for each dataset. Thebar colour denotes the task group as usual. Top: Lightweight hyperparameter sweep. Bottom:HeavyWeight hyperparameter sweep. Left: Sup-100% versus FROM-SCRATCH - supervised pre-training yields a substantial improvement on the ∙ natural datasets and some others. Right: SUP-Rotation-100% versus Sup- 100% - the additional self-supervised loss yields better representa-tions for the ∙ structured tasks.
Figure 16: Kendall rank correlation coefficient between finetuning and linear evaluation on eachdataset.
Figure 17: Top-1 accuracy on ImageNet public validation set when scaling up the architectures. Theaccuracy goes up with either wider or deeper architecture.
Figure 18: VTAB performance on the scaled up architectures. Here the architecture is representedby different color and width is represented by different circle size. The performance increases whenswitching from the standard ResNet50 architecture to ResNet152 2x wider architecture.
Figure 19:	Top-1 accuracy for each task with respect to the number of fine-tuning steps on the1000-example datasets. More steps is usually better, but performance is stable after 1000 steps.
Figure 20:	Top-1 accuracy for each task with respect to the number of fine-tuning steps on the fulldatasets. More steps is usually better, but performance is stable after 1000 steps.
Figure 21: Absolute differences in Kendall’s rank correlation score between the “gold” ranking ineach dataset, and the ranking obtained with either VTAB or Visual Decathlon. Bar colors indicate thecategory as usual, and yellow indicates datasets only present in Visual Decathlon. Positive valuesindicate that VTAB ranking is closer than Visual Decathlon ranking, when compared against the“gold” ranking of a particular dataset. The average ranking correlation for VTAB is 0.76, and 0.70for Visual Decathlon.
