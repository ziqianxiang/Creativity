Figure 1: Approach overview. First, the graph is fed into the graph neural network, which capturesglobal and local graph structure and generates a prior probability that indicates how likely each ver-tex is in the tour sequence. Then, with the help of the graph neural network, a developed MCTSoutputs an improved probability by scouting simulations. Lastly, we visit the best vertex among un-visited vertices according to the improved probability. The above process will loop until all verticesare visited.
Figure 2: Neural network architecture. The architecture on the left (A) is used to compute the priorprobability map that indicates how likely each vertex is in the tour sequence. Firstly, the “tagged”graph is fed into the GNN to generate new feature expressions for each vertex. Then all new nodefeature is concentrated into a long vector that denotes the context of the “tagged” graph. Lastly, thevector is fed into a multilayer perceptron to output the prior probability. The picture on the right (B)depicts the mechanism of computing a new feature of the vertex in one update-layer.
