Figure 1: The four-room domain (Sutton et al., 1999). The agent starts at the red cross and transitionsto an adjacent state at each time step. The goal is to explore the four rooms when no extrinsic rewardis provided. In a) each state is annotated by its SD (Eq.3) to the starting state and b) shows for eachstate the highest possible SFC reward (Eq.4) for a one-step transition from it. Here the successorfeatures are learned using a random walk. c) and d) show a comparison between visitation counts ofeach state from a random agent and an agent that uses the SFC rewards for control via Q-learning.
Figure 2: VizDoom environments we evaluated on. 2a and 2b show the top-down views of My-WayHome and FlytrapEscape with the same downscaling ratio, with red dots marking the startinglocations, green dots indicating the goal locations; 2c and 2dto 2f show exemplary first-person viewscaptured from the marked poses (blue dots with arrows) from those two maps respectively.
Figure 3: Extrinsic rewards per episode obtained in MyWayHome (left) and FlytrapEscape (right).
Figure 4: Extrinsic rewards per episode obtained in AppleDistractions (left) and Cartpole (right).
Figure 5: Ablation study results for AppleDistractions. Each plot shows the mean over 5 non-tunedrandom seeds.
Figure 6: Ablation study results for FlytrapEscape (left) and Cartpole (right).
Figure 7: Results on ”DoomMyWayHome” for different times of scheduling per episode in SIDwith SFC as intrinsic reward.
Figure 8: Flow diagram of the algorithm implementation (Sec.3.4).
Figure 9: Model architecture for the SID (M, SFC) agent. Components with color yellow arerandomly intialized and not trained during learning.
Figure 10: Top-down view and exemplary first-person view observations captured in the AppleDis-tractions environment.
Figure 11: Exemplary observations captured in the Cartpole environment.
Figure 12: Projection of the SFs. For the purpose of visualization we discretized the map into85 × 330 grids and position the trained agent SID(M,SFC) at each grid, then computed the successorfeatures ψ for that location for each of the 4 orientations (0°, 90°, 180°, 270°), which resulted in a4 × 512 matrix. We then calculated the l2-difference of this matrix with a 4 × 512 vector containingthe successor features of the starting position with the 4 different orientations. Shown in log-scale.
Figure 13: Learning curVes for each run for each agent trained on the flytrap enVironment.
Figure 14: Learning curVes for each run for each agent trained on the MyWayHome enVironment.
Figure 15: Learning curVes for each run for each agent trained on the AppleDistraction enVironment.
Figure 16: Learning curves for each run for each agent trained on the Cartpole environment.
