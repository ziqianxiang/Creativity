Figure 1: The Slow Thinking to Learn (STL)model. To model the interactions between theshared SP f and per-task FLs {(g(t), M(t))}t,we feed the output of FLs into the SP whilesimultaneously letting the FLs learn from thefeedback given by SP.
Figure 2: The prediction processes of (a) anFL, and (b) the SP at runtime.
Figure 3: The relative positions between theinvariant representations θ and the approx-imate hypotheses θ(t),s of FLs for differenttasks T(t) ’s on the loss surface defined by FLsafter seeing the (a) first, (b) second, and (c)third task. Since ∣∣θ-6叫| ≤ R for any t in Eq.
Figure 4:	Average all-task performance after seeing different number of sequential taskson the permuted MNIST dataset. For memory-augmented networks, we store at most (a)1000 raw examples, (b) 1000 embedded examples, and (c) 10 embedded examples in theirexternal memory.
Figure 5:	Average all-task performance after seeing different number of sequential (a)CIFAR-100 Normal tasks and (b) CIFAR-100 Hard tasks. The size of external memoryis set to 100 embedded examples. Colors follow Figure 4. (c) Performance of SP withdifferent memory sizes. Dashed lines show the average performance of FLs.
Figure 6: Performance for (a) the latest taskand (b) all previous tasks after seeing dif-ferent number of sequential CIFAR-100 Nor-mal tasks. Colors follow Figure 4. Solid anddashed lines denote models with large andsmall capacities, respectively.
Figure 7: Performance for the (a) 2-nd, (b) 4-th, (c) 6-th, (d) 8-th, and (e) 10-th sequentialCIFAR-100 Normal task that has only a few shots (in number of batches) of training data.
Figure 8: (a) Trade-off between the average all-task performance at the 10-th sequentialtask on the permuted MNIST dataset and the size (in number of embedded examples) ofexternal memory. (b) Space consumption in order to achieve at least 0.9 average all-taskaccuracy. A pair [a, b] denotes a embedded examples, each with b features, in memory. (c)Performance gain of different adaptive models after runtime adaptation.
Figure 9: Performance for the (a) 2-nd, (b) 3-rd, (c) 4-th, and (d) 5-th sequential CIFAR-100Hard task that has only a few shots (in number of batches) of training data.
Figure 10: t-SNE visualization of the embeddings of examples saved in different FLs forthe 5 CIFAR-100 Normal tasks. Different colors represent different tasks.
