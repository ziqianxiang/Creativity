Figure 1: NFSIPAverage policy learning deep network with parameters θΠ is trained by minimizing the loss of thepolicy:L(θπ) = E(s,a)〜...[-log(Π(s, a∣θπ)iwhere MRL and MSL refer to the stored Reinforcement Learning and Supervised Learning expe-riences respectively.
Figure 2: Credit assignment5Under review as a conference paper at ICLR 2020Algorithm 1: Neural Fictitious Self Imitation and Play, NFSIP1: 2: 3: 4:	Initialize policy network(θΠ), action-value network(θQ) and target action-value network(θQ ) networks best reward achieve so far = -∞ while Not Converged do policy = b(Q) with probability η p y	Π	with probability 1 - η5: 6: 7: 8: 9: 10:	for every time step do Simulate agents for 1 step Store experiences in MRL Store experiences in MSL if agent took best response action (b (Q)) for all agents do Sample from MRL , train action-value network: L(θQ) = E(s,a,r,s0) [(r + ma，Xa，Q(S, a0∣θQ0) - Q(s, a∣θQ))]11: 12:	Sample from Msl, train policy network: L(θπ) = E(s,a)[ - log(Π(s, a∣θπ)] if Episode reward > best reward achieve so far then13:	Reset prioritized experience buffer14:	best reward achieve so far = Episode reward15:	if Episode reward >= best reward achieve so far then16:	Compute cumulative reward, R17:	Store experiences in prioritized experience buffer prioritized on R18:	for some iteration do19:	for all agents do20:	Sample from prioritized replay buffer21:	Train action-value network: L(θQ) = E(§,a,R)[([R(s,a) - V(s∣Θq,Θπ)] + )2]22:	Train policy network: L(θπ) = E(s,a,R) h - log(Π(s, a∣θπ)) ∙ [R(s, a) - V(s∣Θq,Θπ)] + )]23:	[R(s, a) — V(s)]+ = max(0, R(s, a) — V(S))24:	V(S) = Pa π(s,a) ∙ Q(s,a)25:	Update target action-value network periodically
Figure 3: Box pushing v1, v2 and Fire Fighting v1Figure 4: Fire Fighting v2, Search and Rescue v1 and v26	Experimental SectionIn this section, we evaluate the performance of our NFSIP approach in comparison to leading ap-proaches for cooperative MARL. We perform the comparison on three different problem settingsfrom literature: (a) Box Pushing Seuken & Zilberstein (2012); (b) Fire Fighting Oliehoek et al.
Figure 4: Fire Fighting v2, Search and Rescue v1 and v26	Experimental SectionIn this section, we evaluate the performance of our NFSIP approach in comparison to leading ap-proaches for cooperative MARL. We perform the comparison on three different problem settingsfrom literature: (a) Box Pushing Seuken & Zilberstein (2012); (b) Fire Fighting Oliehoek et al.
