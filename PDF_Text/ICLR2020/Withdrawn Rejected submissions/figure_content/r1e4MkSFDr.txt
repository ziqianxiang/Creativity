Figure 1: CCNN Structure for predicting future events. Given uniform-interval time sequence {ti},CCNN layer performs both interpolating non-uniformly sampled signal sequence {x(ti)} to {X(ti)} andconvolution({y(t0i)}). Now that {y(t0i)} is uniformly resampled, normal convolution layer can be applied.
Figure 2: CCNN structure.
Figure 3: Two-hot encoding. Each cross on the 1-D axis denotes a value of ∆t. The stem plot above shows itstwo-hot vector. Assuming d = 5, the left plot shows when ∆t = πk-1 + 0.4δ, the two-hot encoding of ∆t is [0,0.6, 0.4, 0, 0]. The right plot shows when ∆t = πk-1, the encoding is [0, 1, 0, 0, 0]4.4	Two-Hot EncodingThe kernel and bias functions can be complicated functions of the input times, so model complexityand convergence can be serious concerns. Therefore, we introduce a two-hot encoding scheme for theinput times, which is an extension to the one-hot scheme, but which does not lose information.
Figure 5: Example predicted time intervals, which is the expectation over Eq. (10) (upper) and RMSE (lower)on the predicting time interval to next event (section 5.2). Standard deviation is calculated among 5 train-test-validation splits. The ReTweet dataset has only one split so no standard deviation is reported. N-SM-MPP didnot report RMSE on retweet dataset.
Figure 6: Illustration of interpolation kernels. The red crosses denote the input data samples. The black lineshows the interpolation kernel for x(ti); the gray lines show the kernels for the other two points. The blue lineshows the interpolated result.
Figure 7: Examples for CCNN and ICNN in restoring nonuniformly down-sampled speech. CCNN generatesbetter approximation at especially at crests and troughs.
