Figure 1: Biased estimators on a toy problem6.2	FouST vs. State-of-the-Art Gradient EstimatorsTraining Stochastic MLPs. We train MLPs with one and two stochastic layers on OMNIGLOT,following the non-linear architecture of Tucker et al. (2017). Each stochastic Boolean layer ispreceded by two deterministic layers of 200 tanh units.
Figure 2: Training ELBO for the one (left) and two (right) stochastic layer nonlinear models onOMNIGLOT8Under review as a conference paper at ICLR 2020Figure 3: Training ELBO on CIFAR-10 (left) and mini-ImageNet (right)7	ConclusionIn this work we introduce the framework of harmonic analysis for Boolean functions. We then use theharmonic analysis to derive the source of bias in the Straight-Through estimator for Boolean randomvariables. Based on the analysis we propose FouST, which is a novel gradient estimate algorithm.
Figure 3: Training ELBO on CIFAR-10 (left) and mini-ImageNet (right)7	ConclusionIn this work we introduce the framework of harmonic analysis for Boolean functions. We then use theharmonic analysis to derive the source of bias in the Straight-Through estimator for Boolean randomvariables. Based on the analysis we propose FouST, which is a novel gradient estimate algorithm.
Figure 4: Training ELBO for the one (left) and two (right) stochastic layer nonlinear models onMNIST13Under review as a conference paper at ICLR 2020Table 2: Test set performance with increasing stochastic depth on MNIST.
Figure 5: Ablations for the one stochastic layer nonlinear model on OMNIGLOT (left) and MNIST(right)E Architectures Used in the experimentsE.1	Architectures for MNIST and OmniglotThe encoder and decoder networks in this case are MLPâ€™s with one or more stochastic layers. Each stochasticlayer is preceded by 2 deterministic layers with a tanh activation function.
