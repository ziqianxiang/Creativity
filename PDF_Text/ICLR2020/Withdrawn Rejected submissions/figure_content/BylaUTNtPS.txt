Figure 1: Illustration of Recurrent Independent Mechanisms (RIMs). A single step under the proposedmodel occurs in four stages (left figure shows two steps). In the first stage, individual RIMs produce a querywhich is used to read from the current input. In the second stage, an attention based competition mechanism isused to select which RIMs to activate (right figure) based on encoded visual input (blue RIMs are active, basedon attention score, white RIMs remain inactive). In the third stage, individual activated RIMs follow their owndefault transition dynamics while non-activated RIMs remain unchanged. In the fourth stage, the RIMs sparselycommunicate information between themselves, also using attention.
Figure 2: Copying Task RIM Activation Pattern fora model with K = 6 RIMs and KA = 3 active RIMsper step (the activated RIMs are in black, non-activatedin white). We can see that the RIM activation pattern isdistinct during the dormant part of the sequence.
Figure 4: Handling Novel Out-of-Distribution Variations. Here, we study the performance of our proposedmodel compared to an LSTM baseline. The first 15 frames of ground truth are fed in and then the system isrolled out for the next 10 time steps. During the rollout phase, RIMs perform better than the LSTMs in accuratelypredicting the dynamics of the balls as reflected by the lower Cross Entropy (CE) [see blue for RIMs, purple forLSTM]. Notice the substantially better out-of-distribution generalization of RIMs when testing on a differentnumber of objects than during training.
Figure 5: Robustness to Novel Distractors:. Left: per-formance of the proposed method compared to an LSTMbaseline in solving the object picking task in the presenceof distractors. Right: performance of proposed methodand the baseline when novel distractors are added.
Figure 6: RIMs-PPO relative score improvement over LSTM-PPO baseline (Schulman et al., 2017) across allAtari games averaged over 3 trials per game. In both cases, PPO was used with the exact same settings, andthe only change is the choice of recurrent architecture. More detailed experiments with learning curves are inAppendix C.
Figure 7: An example of the minigrid task.
Figure 8: Different RIMs attending to Different Balls. For understanding what each RIM is actually doing,we associate each with a separate encoder, which are spatially masked. Only 4 encoders can be active at anyparticular instant and there are four different balls. We did this to check if there would be the expected geometricactivation of RIMs. 1.) Early in training, RIM activations correlated more strongly with the locations of thefour different balls. Later in training, this correlation decreased and the active strips did not correlate as stronglywith the location of balls. As the model got better at predicting the location, it needed to attend less to the actualobjects. The top row shows every 5th frame when the truth is fed in and the bottom shows the results duringrollout. The gray region shows the active block. In the top row, the orange corresponds to the prediction and inthe bottom, green corresponds to the prediction.
Figure 9: Example of the other LSTM baselines. For the 2 other experiments that we consider, here we showexample outputs of our LSTM baselines. In each row, the top panel represents the ground truth and the bottomrepresents the prediction. All shown examples use an LSTM with 250 hidden units, as shown in Fig. 4. Framesare plotted every 3rd time step. The red line marks 10 rollout frames. This is marked because after this we donot find BCE to be a reliable measure of dissimilarity.
Figure 11: Comparison between RIMs and LSTM baseline. For the 4 ball task and the 6-8 ball extrapolationtask, here we show an example output of from our LSTM baseline and from RIMs. All shown examples use anLSTM with 250 hidden units, as shown in Fig. 4. Frames are plotted every 3rd time step. The red line marks 10rollout frames. This is marked because after this we do not find BCE to be a reliable measure of dissimilarity.
Figure 12: Comparison of RIMs to LSTM baseline. For 4 different experiments in the text, we compare RIMsto two different LSTM baselines. In all cases we find that during rollout, RIMs perform better than the LSTMsat accurately capturing the trajectories of the balls through time. Due to the number of hard collisions, accuratemodeling is very difficult.
Figure 13: RIMs on dataset with an occlusion. We show two trajectories (top and bottom) of three balls. Forthe left frames, at each step the true frame is used as input. On the right, outlined in black, the previous output isused as input.
Figure 14: RIMs transferred on new data. We train the RIMs model on the 6-8 ball dataset (as shown in thetop row). Then, we apply the model to the 4 ball dataset, as shown in the bottom.
Figure 15: Ablation loss For the normal, a one-head model, and without input attention, we show the loss duringtraining and the loss for the 4th and 5th frame of rollout. We find that the one-head and without input attentionmodels perform worse than the normal RIMs model during the rollout phase.
Figure 16: One head and no attention Using one head and no attention models, we show the rollout predictionsin blue. On top we show results on the 4 ball dataset and on the bottom we show results on the curtains dataset.
Figure 17: A comparison showing relative improvement of RIMs with kA = 5 over a kA = 4 baseline. UsingkA = 5 performs slightly worse than kA = 4 but still outperforms PPO, and has similar results across themajority of games.
Figure 18: RIMs-PPO relative score improvement over LSTM-PPO baseline (Schulman et al., 2017) across allAtari games averaged over 3 trials per game. In both cases PPO was used with the exact same settings with theonly change being the choice of the recurrent architecture (RIMs with kA = 5).
Figure 19: 4 RIMs, (top k = 2). Each sub-figure shows the effect of masking a particular RIM and studying theeffect of masking on the other RIMs. For example, the top figure shows the effect of masking the first RIM, thesecond figure shows the effect of masking the second RIM etc.
Figure 21: 400dim, 5 RIMs, (top k = 2). Each sub-figure shows the effect of masking a particular RIM andstudying the effect of masking on the other RIMs. For example, the top figure shows the effect of masking thefirst RIM, the second figure shows the effect of masking the second RIM etc.
Figure 22: 400dim, 5 blocks, (top k = 3). Each sub-figure shows the effect of masking a particular RIM andstudying the effect of masking on the other RIMs. For examples, the top figure shows the effect of masking thefirst RIM, the second figure shows the effect of masking the second RIM etc.
Figure 23: 400dim, 5 blocks, (top k = 4). Each sub-figure shows the effect of masking a particular RIM andstudying the effect of masking on the other RIMs. For example, the top figure shows the effect of masking thefirst RIM, the second figure shows the effect of masking the second RIM etc.
Figure 24:	Comparing RIMs-PPO with LSTM-PPO: Learning curves for kA = 4, kA = 5 RIMs-PPOmodels and the LSTM-PPO baseline across all Atari games.
Figure 25:	Baseline agent with no input attention mechanism: Here we compare the RIMs to the baseline,where their is no input attention (i.e., top down attention) as well as all the RIMs communicate with each otherat all the time steps. Learning curves for RIMs-PPO models, Baseline Agent, the LSTM-PPO baseline across 30Atari games.
