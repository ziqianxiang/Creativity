Figure 1: Greedy State Representation Learning.
Figure 2: MDP setting.
Figure 3: Translating the state representation: (1) the new observation is provided by the environment(and optionally further preprocessed); (2) a neural network model translates the observation to itsold pendant; (3) using this observation we sample an action at from policy ∏; (4.1) through theforward model f we obtain a prediction for the succeeding observation ^t+ι and (4.2) by samplingthe environment we obtain the succeeding observation o++1(5) we train μψ with 卜++i； 0t+ι].
Figure 4: Ambiguous Dynamics.
Figure 5: DQN baselines on OpenAI gym’s MountainCar-v0.
Figure 6: GSRL on the MountainCar-v0 environment.
Figure 7: Translated policy emerging over iterations. Encoding: left (blue circles), noop (greenrectangles), right (orange crosses). The rightmost policy is almost identical to the original policy.
Figure 8: Creating translation targets through multi-step predictions.
Figure 9: Alternative Transcoding Scheme.
