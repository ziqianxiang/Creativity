Figure 1: General pipeline of learning initial neural surrogate model using weighted Reptile algo-rithm from meta train set of functions fi ,i = 1, ∙ ∙ ∙ , N.
Figure 2: MSES w.r.t. optimization iterations on synthetic functions With a = 3∏, π, 2∏.
Figure 3: MSEs w.r.t. iterations for f5, f6 in CEC 2017 for hyper-parameters tuning of DE.
Figure 4: MSEs w.r.t. optimization iterations for hyper-parameters tuning of neural networks ondata sets of S4 , S5 , S6 .
Figure 5: MSEs w.r.t. iterations forhyper-parameters tuning of SVMs.
Figure 6: Ablation study.
Figure 7: MSEs (in logarithm) w.r.t. different optimization iterations on 15 remaining test functionsin synthetic function set.
Figure 8: MSEs (in logarithm) w.r.t. different optimization iterations for tuning number of hiddenunits and learning rate on EMNIST data sets of S7, S8, S9, S10, S11.
Figure 9: MSEs (in logarithm) w.r.t. different optimization iterations for tuning hyper-parameters ofDEonf7 - f10 of CEC 2017.
Figure 10: An Illustration of neural surrogate model for hyper-parameter tuning of DE.
Figure 11: Training curves of rewards on meta train set respectively for our four different experi-ments in sections 4.1-4.4.
