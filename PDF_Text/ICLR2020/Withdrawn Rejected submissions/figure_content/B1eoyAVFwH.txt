Figure 1: Feature partitioning can be used to control how much network capacity is used by tasks,and how much sharing is done across tasks. In this work we identify effective partitioning strategiesto maximize performance while reducing average computation per task.
Figure 2: Multi-task distillation: At a given step, a teacher network pretrained on a single task ischosen from the available tasks. Features at a target depth are produced then passed through the nextresidual block of both the teacher and student (where a mask is applied). Features are then comparedusing a standard MSE loss. This process can be applied to multiple layers at once.
Figure 3: Correlation between distillation ac-curacy and final validation accuracy.
Figure 4: (left) Distribution of performance of random partitioning strategies; (middle) Distribu-tion of performance when fixing channel use but varying sharing across tasks; (right) Optimizationtrajectories of ES with different degrees of regularizationof the time) correlates higher with final accuracy. This allows us to sample and compare manymore parameterizations during our search process and have more confidence that the top performingparameterizations will do well when training a full model.
Figure 5: Distillation performance per task as a function of parameters allocated to that task. Colorindicates the average parameters used across all tasks, notice as more parameters are used overall(lighter color) individual task performance drops.
Figure 6: Measuring correlation of feature activations across three blocks of task-specific ResNetmodels given a shared input image. Even though no restriction is made when finetuning individualmodels, the features produced are highly correlated through the first two-thirds of the model, andgreater task differentiation is not seen until the end. Differentiation in early blocks would normallyoccur due to different batch normalization statistics across datasets, but that is controlled for here.
