Figure 1: We train an invertible generative model with CelebA images (including those at left). Whenused as a prior for compressed sensing, it can yield higher quality image reconstructions than Lassoand a trained DCGAN, even on out-of-distribution images. Note that the DCGAN reflects biases ofthe training set by removing the man’s glasses and beard, whereas our invertible prior does not.
Figure 2: Recovered PSNR values as a function of γ for denoising by the Glow and DCGAN priors.
Figure 3: Denoising results using the Glow prior, the DCGAN prior, and BM3D at noise levelσ = 0.1. Note that the Glow prior gives a sharper image than BM3D.
Figure 4: The left panel shows recovered PSNRs averaged over 12 test set images under the Glow,and DCGAN prior with γ = 0; and the Lasso with respect to the DCT and a Wavelet Transform.
Figure 6:	Inpainting: Recoveries underWe have demonstrated that pretrained generative invertible DCGAN and Glow, both with γ = 0.
Figure 7:	Landscapes of (a) the loss surface in x-space, (b) just the image likelihood in x-space, and(c) the loss surface in z-space, as functions of two random directions in either x or z, as appropriate.
Figure 8:	An invertible net was trained on the data points in x-space (left), resulting in the given plotsof latent z-likelihood versus x (middle), and x-likelihood versus latent representation z (right).
Figure 9: Samples from training set of CelebA downsampled to 64 × 64 × 3.
Figure 10: Image Denoising — Recovered PSNR values as a function of γ under Glow prior, andDCGAN prior on (within-distribution) test set CelebA images. For reference, we show the averagePSNRs of the original noisy images, and under the Glow prior in the noiseless case (σ = 0) in bothpanels. The average PSNR after applying BM3D, and the average PSNR under the Glow prior atnoise levels σ = 0.01, 0.05, 0.10, 0.20 are reported.
Figure 11: Image Denoising — Recovered PSNR values as a function of γ under Glow prior withkz k and kz k2 penalization on (within-distribution) test set CelebA images. Comparison is providedwith BM3D denoising at noise level σ = 0.1we only show the case of γ = 0 as this consistently gave the best performance for DCGAN. UnderGlow prior, the best performance over is achieved with γ = 1, overfitting of the image occurs withγ = 0 and underfitting occurs at γ = 5. Note that the Glow prior with γ = 1 also gives a sharperimage than BM3D.
Figure 13: Image Denoising — Visual comparisons under the Glow prior, the DCGAN prior, andBM3D at noise level σ = 0.2 on CelebA (within-distribution) test set images. Under DCGAN prior,we only show the case of γ = 0 as this consistently gives the best performance. Under Glow prior,the best performance is achieved with γ = 2.5, overfitting of the image occurs with γ = 0 andunderfitting occurs with γ = 5. Note that the Glow prior with γ = 2.5 also gives a sharper imagethan BM3D.
Figure 14: Image Denoising — Visual comparisons under the Glow prior, and BM3D at noise levelσ = 0.1 on (within-distribution) test set Flowers images. Under Glow prior, the best performance isobtained with γ = 1. Note that the Glow prior with γ = 1 also gives a sharper image than BM3D.
Figure 15: Compressed sensing — Reconstruction error vs. number of measurements under Glowprior, DCGAN prior, LASSO-DCT and LASSO-WVT on CelebA (within-distribution) test setimages. Noise η is scaled such that Ekηk2 = 0.01 and the penalization parameter γ = 0 for Glow,and DCGAN; andγ = 0.01 for LASSO-DCT, and LASSO-WVT.
Figure 16: Compressed sensing — Zoomed-in version of the left panel of Figure 4 in the main paperin the low measurement regime for CelebA. PSNR vs. number of measurements under Glow prior,DCGAN prior, LASSO-DCT and LASSO-WVT on the CelebA (within distribution) test set images.
Figure 17: Compressed sensing under Glow prior. Performance comparison between LBFGS andAdam solver for the inverse problem. For Adam solver, 2000 gradient steps were taken with learningrate chosen to be 0.01. The rest of the parameters were fixed to be the same as with LBFGS.
Figure 18: Residual error vs. number of iterations. Left panel compares DCGAN and Glow priors.
Figure 19: The left panel shows the average PSNR over 12 test set images and norm of the optimizerZ as a function of the norm of the initialization for the LBFGS solver to equation 2 for Compressedsensing under Glow prior with γ = 0. The initialization z0 was chosen randomly and rescaled to thedesired norm. The right panel shows the norm of the estimated latent representation as a function ofiteration number for multiple initializations. The Adam solver behaves similarly.
Figure 20: Compressed sensing — Norm of the latent codes with iterations. Left panel showshow the norm of the latent codes evolves over iterations of the LBFGS solver under different sizeinitializations. Right panel shows the same experiment for the Adam solver (although over muchlarger number of iterations as Adam requires comparatively more iterations to converge). Each pointis averaged over 12 test set images under random rescaled initializations z0 . We set the penalizationparameter γ = 0 in both experiments.
Figure 21: Compressed sensing visual comparisons — Recoveries on (within-distribution) test setimages with a number m = 200 (≈ 1.5%) of measurements under the Glow prior, the DCGAN prior,LASSO-WVT, and LASSO-DCT at a noise level PEknk2 = 0.1. In each case, We choose values ofthe penalization parameter γ to yield the best performance among the tested values. We use γ = 0for both DCGAN, and Glow prior and γ = 0.01 for LASSO-WVT, and LASSO-DCT, respectively.
Figure 22: Compressed sensing visual comparisons — Recoveries on the (within-distribution) test setimages with a number m = 300 (≈ 2%) of measurements under the Glow prior, the DCGAN prior,LASSO-WVT, and LASSO-DCT at a noise level PEknk2 = 0.1. In each case, we choose values ofthe penalization parameter γ to yield the best performance among the tested values. We use γ = 0for both DCGAN, and Glow prior and γ = 0.01 for LASSO-WVT, and LASSO-DCT, respectively.
Figure 23: Compressed sensing visual comparisons — Recoveries on (within-distribution) test setimages with a number m = 400 (≈ 3%) of measurements under the Glow prior, the DCGAN prior,LASSO-WVT, and LASSO-DCT at a noise level PEknk2 = 0.1. In each case, we choose values ofthe penalization parameter γ to yield the best performance among the tested values. We use γ = 0for both DCGAN, and Glow prior and γ = 0.01 for LASSO-WVT, and LASSO-DCT, respectively.
Figure 24: Compressed sensing visual comparisons — Recoveries on (within-distribution) test setimages with a number m = 500 (≈ 4%) of measurements under the Glow prior, the DCGAN prior,LASSO-WVT, and LASSO-DCT at a noise level PEknk2 = 0.1. In each case, we choose values ofthe penalization parameter γ to yield the best performance among the tested values. We use γ = 0for both DCGAN, and Glow prior and γ = 0.01 for LASSO-WVT, and LASSO-DCT, respectively.
Figure 26: Compressed sensing visual comparisons — Recoveries on (Within-distribution) test setimages with a number m = 1000 (≈ 8%) of measurements under the Glow prior, the DCGAN prior,LASSO-WVT, and LASSO-DCT at a noise level p∕E∣∣ηk2 = 0.1. In each case, we choose values ofthe penalization parameter γ to yield the best performance among the tested values. We use γ = 0for both DCGAN, and Glow prior and γ = 0.01 for LASSO-WVT, and LASSO-DCT, respectively.
Figure 27: Compressed sensing visual comparisons — Recoveries on (within-distribution) test setimages with a number m = 2500 (≈ 20%) of measurements under the Glow prior, the DCGAN prior,LASSO-WVT, and LASSO-DCT at a noise level PEknk2 = 0.1. In each case, We choose values ofthe penalization parameter γ to yield the best performance among the tested values. We use γ = 0for both DCGAN, and Glow prior and γ = 0.01 for LASSO-WVT, and LASSO-DCT, respectively.
Figure 28: Compressed sensing visual comparisons — Recoveries on (within-distribution) test setimages with a number m = 5000 (≈ 41%) of measurements under the Glow prior, the DCGAN prior,LASSO-WVT, and LASSO-DCT at a noise level PEknk2 = 0.1. In each case, we choose values ofthe penalization parameter γ to yield the best performance among the tested values. We use γ = 0for both DCGAN, and Glow prior and γ = 0.01 for LASSO-WVT, and LASSO-DCT, respectively.
Figure 29: Compressed sensing visual comparisons — Recoveries on (within-distribution) test setimages with a number m = 7500 (≈ 61%) of measurements under the Glow prior, the DCGAN prior,LASSO-WVT, and LASSO-DCT at a noise level PEknk2 = 0.1. In each case, We choose values ofthe penalization parameter γ to yield the best performance among the tested values. We use γ = 0for both DCGAN, and Glow prior and γ = 0.01 for LASSO-WVT, and LASSO-DCT, respectively.
Figure 30: Compressed sensing visual comparisons — Recoveries on (within-distribution) test setimages with a number m = 10,000 (≈ 81%) of measurements under the Glow prior, the DCGANprior, LASSO-WVT, and LASSO-DCT at a noise level PEknk2 = 0.1. In each case, we choosevalues of the penalization parameter γ to yield the best performance among the tested values. Weuseγ = 0 for both DCGAN, and Glow prior and γ = 0.01 for LASSO-WVT, and LASSO-DCT,respectively.
Figure 31: PSNR vs. number of measurements m in compressed sensing under Glow prior, LASSO-DCT and LASSO-WVT on Birds dataset (left panel) and Flowers dataset (right panel). Noise η isscaled such that VZEknk2 = 0.1 and the penalization parameter Y = 0 for Glow, and Y = 0.01 forLASSO-DCT, and LASSO-WVT.
Figure 33: Compressed sensing — Visual comparisons on (within-distribution) test set images fromBirds and Flowers dataset with a number m = 300 (≈ 2%) of measurements under the Glow prior,LASSO-WVT, and LASSO-DCT at a noise level PEknk2 = 0.1. In each case, we choose values ofthe penalization parameter γ to yield the best performance among the tested values. We use γ = 0for Glow prior and γ = 0.01 for LASSO-WVT, and LASSO-DCT, respectively.
Figure 34: Compressed sensing — Visual comparisons on (within-distribution) test set images fromBirds and Flowers dataset with a number m = 400 (≈ 3%) of measurements under the Glow prior,LASSO-WVT, and LASSO-DCT at a noise level PEknk2 = 0.1. In each case, we choose values ofthe penalization parameter γ to yield the best performance among the tested values. We use γ = 0for Glow prior and γ = 0.01 for LASSO-WVT, and LASSO-DCT, respectively.
Figure 35: Compressed sensing — Visual comparisons on (within-distribution) test set images fromBirds and Flowers dataset with a number m = 500 (≈ 4%) of measurements under the Glow prior,LASSO-WVT, and LASSO-DCT at a noise level PEknk2 = 0.1. In each case, we choose values ofthe penalization parameter γ to yield the best performance among the tested values. We use γ = 0for Glow prior and γ = 0.01 for LASSO-WVT, and LASSO-DCT, respectively.
Figure 36: Compressed sensing — Visual comparisons on the test set images from Birds and Flowersdataset with a number m = 750 (≈ 6%) of measurements under the Glow prior, LASSO-WVT, andLASSO-DCT at a noise level VzEknk2 = 0.1. In each case, we choose values of the penalizationparameter γ to yield the best performance among the tested values. We use γ = 0 for Glow prior andγ = 0.01 for LASSO-WVT, and LASSO-DCT, respectively.
Figure 37: Compressed sensing — Visual comparisons on the test set images from Birds and Flowersdataset with a number m = 1,000 (≈ 8%) of measurements under the Glow prior, LASSO-WVT,and LASSO-DCT at a noise level VZEknk2 = 0.1. In each case, we choose values of the penalizationparameter γ to yield the best performance among the tested values. We use γ = 0 for Glow prior andγ = 0.01 for LASSO-WVT, and LASSO-DCT, respectively.
Figure 38: Compressed sensing — Visual comparisons on the test set images from Birds and Flowersdataset with a number m = 2, 500 (≈ 20%) of measurements under the Glow prior, LASSO-WVT,and LASSO-DCT at a noise level ,Eknk2 = 0.1. In each case, we choose values of the penalizationparameter γ to yield the best performance among the tested values. We use γ = 0 for Glow prior andγ = 0.01 for LASSO-WVT, and LASSO-DCT, respectively.
Figure 40: Visual comparisons of compressed sensing of the test set images from Birds and FloWersdataset with a number m = 7, 500 (≈ 61%) of measurements under the Glow prior, LASSO-WVT,and LASSO-DCT at a noise level ,Eknk2 = 0.1. In each case, we choose values of the penalizationparameter γ to yield the best performance among the tested values. We use γ = 0 for Glow prior andγ = 0.01 for LASSO-WVT, and LASSO-DCT, respectively.
Figure 41: Visual comparisons of compressed sensing of the test set images from Birds and Flowersdataset with a number m = 10,000 (≈ 81%) of measurements under the Glow prior, LASSO-WVT,and LASSO-DCT at a noise level ,Eknk2 = 0.1. In each case, we choose values of the penalizationparameter γ to yield the best performance among the tested values. We use γ = 0 for Glow prior andγ = 0.01 for LASSO-WVT, and LASSO-DCT, respectively.
Figure 43: Compressed sensing (m = 2500 ≈ 20% of n) visual comparisons on out-of-distributionimages. We compare the recoveries under Glow (trained on CelebA) prior, DCGAN (trained onCelebA) prior, LASSO-WVT, and LASSO-DCT at a noise level PEknk2 = 0.1. In each case,we choose values of the penalization parameter γ to yield the best performance. We use γ = 0for both DCGAN, and Glow prior and and optimize γ for each recovery using LASSO-WVT, andLASSO-DCT.
Figure 44: Compressed sensing (m = 5000 ≈ 41% of n) visual comparisons on out-of-distributionimages. We compare the recoveries under Glow prior (trained on CelebA), DCGAN prior (trained onCelebA), LASSO-WVT, and LASSO-DCT at a noise level PEknk2 = 0.1. In each case, We choosevalues of the penalization parameter γ to yield the best performance. We use γ = 0 for both DCGAN,and Glow prior and and optimize γ for each recovery using LASSO-WVT, and LASSO-DCT.
Figure 46: Compressed sensing (m = 10, 000, ≈ 81% ofn) visual comparisons on out-of-distributionimages. We compare the recoveries under Glow prior (trained on CelebA), DCGAN prior (trained onCelebA), LASSO-WVT, and LASSO-DCT at a noise level PEknk2 = 0.1. In each case, We choosevalues of the penalization parameter γ to yield the best performance. We use γ = 0 for both DCGAN,and Glow prior and and optimize γ for each recovery using LASSO-WVT, and LASSO-DCT.
Figure 47: Inpainiting: PSNR (averaged over 12 test images of CelebA) vs. penalization parameter γunder Glow prior and DCGAN prior (left panel) and using different initializations under Glow prior(right panel).
Figure 48: Image inpainiting results on CelebA test set. Masked images are recovered under DCGANprior and Glow prior. Recoveries under DCGAN prior are skewed and blurred whereas Glow priorleads to sharper and coherent inpainted images. For both Glow and DCGAN, we set γ = 0.
Figure 49: Image inpainiting results on out-of-distribution images. Masked images are recoveredunder DCGAN prior and Glow prior. Recoveries under DCGAN prior are skewed and blurredwhereas Glow prior leads to sharper and coherent inpainted images. For both Glow and DCGAN, weset γ = 0.
Figure 50: Histograms of the norm of the latent representation, z, over 3000 test images underadditive Gaussian noise with σ = 0.1 (left), σ = 0.05 (middle), and σ = 0.01 (right).
Figure 51: The magnitude of the change in image space as a function of the size of a perturbation inlatent space. Solid lines are the mean behavior and shaded region depicts 95% confidence interval.
Figure 52: Loss landscapes for kAG(Z) - yk22 + γkZk2 with γ = 0 around the latent representationof a fixed image and with respect to either random latent directions or latent directions that interpolatebetween images.
Figure 53: A point cloud of the synthetically generated data x ∈ R2For comparison, we plot the x-likelihood versus x (left), latent z-likelihood versus x (middle), andx-likelihood versus z (right) in Figure 54. These plots illustrate that generally high-likelihood xpoints are also given higher latent z-likelihood, however, some low x-likelihood might be assigned ahigher Gaussian z-likelihood; these are, for example, the points living on the darker contour spearingthrough the Gaussian bowl in the right plot. Figure 55 shows some of the points in the x-likelihood(left) that map to this contour in the z-space (right).
Figure 54: x-likelihood versus x (left), z-likelihood versus x (middle), and x-likelihood versus z(right).
Figure 55: Some points (red-crosses) in x-space mapped to z-space. The (unwanted, as it correspondsto low-likelihood points) bridge connecting the models of the learned bimodal distribution is mappedto the contour in the z-space.
Figure 56: We show gradient descent path from the initialization to the final estimate along with truesolution. In the first row, we initialized from z = 0 (good initialization) and in the second row weused low likelihood data points as intializations (bad initialization).
Figure 57: Landscapes of the loss surfaces in the x-space (first row), the loss surfaces of in thez-space (second row) for various values of γ, and loss surface of x-likelihood - log p(x) (third row).
Figure 58: We report PSNR against number of measurements m when optimizing in the latent spaceequation 4 with γ = 0 and the image space equation 3 with γ set to 10, 50 and 100.
Figure 60: Denoising comparision at σ = 0.05 when optimizing over latent space (left) versus imagespace (right).
Figure 59: We plot PSNR against gradient iterations for compressive sensing at m = 5000 on asingle image under the presence and absence of noise with different values of Y with noise levelPEknk2 = 0.1 (left) and PEknk2 = 1 (right).
Figure 61: Denoising comparision at σ = 0.10 when optimizing over latent space (left) versus imagespace (right).
