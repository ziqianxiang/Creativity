Figure 1: Illustration of our scalable neural networks. (a) Inference phase of our method. The modelcan scale according to the target size by changing the rank of a weight matrix in each layer. (b)Learning phase of our method. We explicitly minimizes losses for both of full- and low-rank net-work, which is designed not only to keep the performance of full-rank network but also to improvemultiple low-rank networks (to be used at the inference phase). These schemes can also be appliedto CNNs.
Figure 2: Training results for VGG-15 on CIFAR-10. (Left) Full-rank validation accuracy by chang-ing ɑι with au = 1.0. (Center) Full-rank validation accuracy by changing a” with aι = 0.01.
Figure 3: Inference results for VGG-15 on CIFAR-10. (a) # of parameters vs. validation accuracy.
Figure 4: Results of rank selection for VGG-15 on CIFAR-10. (Left) Layer-wise importance withcomplexity-based criterion (normalized to sum 1). (Right) Remaining rank ratio per layer by differ-ent selection methods. “uni”, “c1”, and “c1c2” indicate selection results by a uniform method, byEq. (1), and by Eq. (1 & 2), respectively. We do not reduce parameters for the last fully connectedlayer for the uniform method because it significantly decreases accuracy.
Figure 5: Scalability for ResNet-34 on CIFAR-100 dataset When using (a) US-Nets and (b) ourmethod. We show the results for each of 5 trials with different random seeds in this figure.
Figure 6: Comparisons with slimmable networks (YU et al., 2019) and US-Nets (YU & Huang,2019) for VGG-15 and ReSNet-34 on CIFAR-10/100 datasets. (Left) # of parameters vs. validationaccuracy. (Right) # OfMACS vs. validation accuracy.
Figure 7: The effect of a hyper-parameter for balancing the losses (λ). Validation accuracies areevaluated with λ ∈ {0.1, 0.2, 0.3, 0.4, 0.5} for VGG-15 and ResNet-34 on CIFAR-10 /100 datasets.
Figure 8: Comparisons with slimmable networks (YU et al., 2019) and US-Nets (YU & Huang,2019) for VGG-15 and ResNet-34 on CIFAR-10/100 datasets. (Left) # of parameters vs. validationaccuracy. (Right) # OfMACS vs. validation accuracy.
Figure 9: Visualizations of weight coefficients of VGG-15 trained on the CIFAR-10 dataset. (Upper)Weight tensors in the 1st convolutional layer. (Lower) Weight matrices in the 14th fully connectedlayer. “baseline” indicates normal learning.
Figure 10: # of parameters vs. the sum of squared error for the 1st, 8th, and 14th layers of VGG-15trained on the CIFAR-10 dataset.
