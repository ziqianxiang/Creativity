Figure 1: A basic model for recognizing unseen visual predicates.  The visual data and knowledgegraphâ€™s nodes are mapped into a common space by a visual and knowledge module respectively,where the sub-spaces from visual and knowledge module are named with visual feature and semanticembedding space correspondingly.  Note that the visual predicate feature contains features of thesubject, object and the union of them. The basic model contains two stages. First, the visual featureand semantic embedding space are aligned by taking the seen predicates in the training set as anchors,the so-called compatibility learning. Second, the samples in the test set are mapped into visual featurespace and matched with the nearest predicate neighbor from the semantic embedding space.
Figure 2: A) The pipeline of our basic model.  The visual features (from the visual module) andcorresponding semantic embeddings (from the knowledge module) are constrained to be close. B)Simplified PinSage. We sample the graph nodes in two aspects during training. Take 2-layer GCN asan example, for on-demand sampling, to get the final embedding of the target node u, the neighborsof u are needed.  Backtracking in this way, we only need first and second-order neighbors of u.
Figure 3: The images with paired located entities are on the top, where the blue and yellow boxesrepresent subject and object respectively. The results of the generalized and traditional setting are atthe first two rows (correct: blue, wrong: yellow), while the ground truth triplets are at the last row.
Figure 4: Distribution of predicates, objects and relationship triplets.
Figure 5: Comparing the initial embeddings and the final embeddings. The unseen predicates aremarked in blue, while the seen predicates are marked in green.
