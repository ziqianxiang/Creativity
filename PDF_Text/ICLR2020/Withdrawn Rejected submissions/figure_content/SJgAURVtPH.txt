Figure 1: Embodied language grounding with implicit 3D visual feature representations. Wemap RGB images to 3D feature maps of the scene they depict, and 3D object boxes of the objectspresent (column 1) building upon the method of Tung et al. (50). We map an utterance and its parsetree to object-centric 3D feature maps and cross-object relative 3D offsets using stochastic generativenetworks (column 2). We map a referential expression to the 3D box of the object referent (column3). Last, given a placement instruction, we localize the referents in the scene and infer the 3D desiredlocation for the object to be manipulated (column 4). We use predicted location to supply rewardsfor trajectory optimization of placement policies.
Figure 2: Language to scene generation for utterances longer than training utterances for ourmodel (rows 1,2) and the 2D baseline model of (11) (row 3). Both our model and the baseline arestochastic, and We show three generated scenes per utterance. The baseline changes the shape of theobjects arbitrarily (the brown sphere is mapped to a cylinder and the red cylinder to a cube).
Figure 4: Mapping natural language to object-centric appearance tensors and cross-object 3Dspatial offsets using conditional variational autoencoders.
Figure 5: 3D referential object detection with metric learning between language generated andimage generated appearance object 3D feature tensors, and cross object location classifiers.
Figure 6: Natural language conditioned neural and blender scene renderings generated by theproposed model. We visualize each scene from two nearby views, a unique ability of our model,due to its 3-dimensional generation space.
Figure 7: (Additional) Natural language conditioned neural and blender scene renderings gen-erated by the proposed model.
Figure 8:	Neural and blender scene renderings generated by the proposed model, conditionedon natural language and the visual scene. Our model uses a 3D object detector to localize objectsin the scene, and the learnt 2D-to-3D unprojection neural module to compute a 3D feature tensor foreach, by cropping accordingly the scene tensor. Then, it compares the natural language conditionedgenerated object tensors to those obtained from the image, and grounds objects references in theparse tree of the utterance to objects presents in the environment of the agent, if the feature distanceis below a threshold. If such binding occurs, as is the case for the “green cube” in top left, then,our model used the image-generated tensors of the binded objects, instead of the natural languagegenerated ones, to complete the imagination. In this way, our model grounds natural language toboth perception and imagination.
Figure 9:	(Additional) Neural and blender scene renderings generated by the proposed model,conditioned on natural language and the visual scene.
Figure 10: Affordability prediction comparison of our model with the baseline work of (11). Inthe top 2 rows, We show the Neural and Blender renderings of our model. Since We reason about thescene in 3D, our model allows checks for expression affordability by computing the 3D intersection-over-union (IoU) scores. In contrast, the bottom row shows the baseline model which operates in 2Dlatent space and hence cannot differentiate between 2D occlusions and overlapping objects in 3D.
Figure 11: Consistent scene generation . We render the generated 3D feature canvas from vari-ous viewpoints in the first row using the neural GRNN decoder, and compare against the differentviewpoint projected Blender rendered scenes. Indeed, our model correctly predicts occlusions andvisibilities of objects from various viewpoints, and can generalize across different number of objects.
Figure 12: Language-guided placement policy learning. Displayed are the final configurationsof the learned policy using different language expressions. Top: Goals generated with our method.
