Figure 1: This figure depicts a flaw of the Wasserstein distance. In these figures a blue (desired out-put distribution and green, generated fake distribution can be seen), the assignments/desired trans-ports are illustrated by red arrows. Wasserstein distance calculates minimal transportation betweenall the samples. In many cases this results shifting all the samples to cover the whole distributioncorrectly, this is depicted on the left subfigure. Unfortunately this means that those samples whichare at the intersection of the two distributions, meaning that for these generated samples an identicalpair could be found in the real samples, will also be altered and shifted towards an other part ofthe real distribution. Instead of this we propose the calculation of the assignment using a greedymethod. This will ensure that that identical (or similar) sample pairs will be selected first, and afterthis the transportation between the disjunct regions will be calculated, which is depicted on the rightsubfigure.
Figure 2: In this figure we would like to demonstrate different approaches for minimal transportationin one-dimension using mini-batches. Real samples are noted by blue and fake samples are plottedby red stars. The position of the samples were shifted vertically for better display, but only theirhorizontal position matters. On both figures the same constellation can be seen: four similar samplepairs and one which is very different. The left subfigure depicts sample assignment by sorting,meanwhile the right figure depicts assignment by a greedy approach. The summed distances are thesame in both cases for p = 1. We would like to argue that the lower assignment can be better fornetwork training.
Figure 3: Histograms of the one-dimensional distributions. The blue curve plots the original dis-tribution of the Gaussian Mixture Model, the green curve depicts the generated distribution usingsorting for sample assignment, meanwhile the red curve display the result of the greedy approach.
Figure 4: This Figure depicts the real (blue) and the generated (red) samples for two-dimensionalGaussian Mixture Models. The upper rows were generated using sorting, meanwhile the lowersamples were produced using greedy assignment. The subfigures from left to right display results at2, 3, 4 and five hundred thousand iterations.
Figure 5: This Figure displays randomly selected samples generated with the same network archi-tecture and training parameters on the MNIST dataset using batches of 16. The samples on the leftwere generated using sorting assignment with Max-sliced Wasserstein distance, the samples in themiddle were generated using greedy sample assignment and the samples on the right were generatedusing the hybrid approach. All samples were generated after 20 epochs of training.
Figure 6: This Figure depicts random samples generated with the same network architecture andtraining parameters on the CelebA dataset using batches of 16. All samples were generated usingMax-sliced Wasserstein distance, the samples on the left were generated using sorting for sampleassignment, in the middle the greedy approach was used, meanwhile the samples on the right weregenerated using the hybrid approach. All samples were generated after 20 epochs of training.
