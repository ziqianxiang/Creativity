Figure 1: This figure depicts an overview of our training pipeline on video data. Given an imageframe x, we produce Tcj (x) and Ttemp(x), which are appearance and pose perturbed variants of xrespectively. The model learns to combine the appearance information from Ttemp(x), and combineit with the pose from the Tcj (x) in order to reconstruct foreground object from the foregrounddecoder. Foreground masks are predicted as part of the pipeline to separate the foreground renderingfrom the background rendering. Specifically, the background is rendered from a UNet that learnsto extract clean backgrounds from Ttemp (x). This allows the learned pose representation to focuson the more dynamic foreground object. The pose encoder and MaskNet are each depicted twice asthey are applied twice during the forward pass.
Figure 2: Landmark analysis experiments. 2a plots the BBC validation dataset keypoint accuracyversus number of learned keypoints. By factorizing out the background rendering, we are ableto achieve better landmark-to-annotation mappings with fewer landmarks than the baseline. 2bplots the percentage of the per-landmark normalized activation maps contained within the providedforeground segmentation masks on Human3.6M, sorted in ascending order. We compare our modelagainst our baseline at 8, 12, and 16 learned landmarks. We see that the least-contained landmarksin the proposed approach are significantly more contained than those of the baseline.
Figure 3: Qualitative results of our landmark prediction pipeline. From top to bottom, we show ourregressed annotated keypoint predictions, our predicted foreground mask, and the underlying land-mark activation heatmaps. Datasets are BBC Pose, Human3.6M, and CelebA/MAFL respectively.
Figure 5: We base our main evaluation to LPIPS score which closely correlates with human percep-tion. We also provide SSIM and PSNR metrics for completeness. Our method is competitive withstate-of-the-art methods on KTH, and shows a large improvement over our controlled baseline.
Figure 6: From left to right input image, color jittered image, thin-plate-spline warped image, re-constructed background, predicted foreground, mask, and reconstructed output.
Figure 7: Additional qualitative results for keypoint predictions for BBC Pose, Human3.6M, andCelebA/MAFL respectively.
Figure 8: The LSTM predictsperturbations to the Gaussianmeans and Cholesky parame-ters of the covariances.
Figure 9: Qualitative results on KTH action test dataset comparing our method to prior work. SAVPand SAVP-deterministic methods produce blurry foreground images. On the other hand, the base-line method produces sharp foreground but the background does not match the initial frames. Ourmethod maintains sharpness and high fidelity to the background. We show the foreground image re-constructions and masks that are used to produce output images. In the last row, first column showsthe reconstructed background image.
Figure 10: Qualitative results on the BAIR dataset comparing our method to prior work. Methodsare conditioned on 2 initial frames to predict the next 28. SVGLP, SAVP and SAVP-deterministicmethods produce blurry outputs in the previously occluded regions. Our method maintains sharpnessand high fidelity to the background. We show the foreground image reconstructions and masks thatare used to produce output images. In the last row, first column shows the reconstructed backgroundimage.
Figure 11: We base our main evaluation to LPIPS score (Zhang et al., 2018a) which closely cor-relates with human perception. We also provide SSIM and PSNR metrics for completeness. Ourimplementation achieves better LPIPS score than the competing methods. Importantly, the factor-ized method significantly outperforms our baseline by a large margin on all metrics.
