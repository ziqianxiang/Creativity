Figure 2: Samples from BigGAN-deep (a) and LOGAN (b) with similarly low FID. Samples fromthe two panels were drawn from truncation levels corresponding to points A and B in figure 3 brespectively. (FID/IS: (a) 5.04/126.8, (b) 5.09/217.0)Figure 1: Samples from BigGAN-deep (a) and LOGAN (b) with similarly high IS. Samples fromthe two panels were drawn from truncation levels corresponding to points C and D in figure 3 brespectively. (FID/IS: (a) 27.97/259.4, (b) 8.19/259.9)(b)a variable after one update step, e.g., θfD = θD - α df (ZdθDSG). p(x) and p(z) denote the datadistribution and source distribution respectively. Ep(x) [f (x)] indicates taking the expectation offunction f(x) over the distribution p(x).
Figure 1: Samples from BigGAN-deep (a) and LOGAN (b) with similarly high IS. Samples fromthe two panels were drawn from truncation levels corresponding to points C and D in figure 3 brespectively. (FID/IS: (a) 27.97/259.4, (b) 8.19/259.9)(b)a variable after one update step, e.g., θfD = θD - α df (ZdθDSG). p(x) and p(z) denote the datadistribution and source distribution respectively. Ep(x) [f (x)] indicates taking the expectation offunction f(x) over the distribution p(x).
Figure 3: (a) Schematic of LOGAN. We first compute a forward pass through G and D with asampled latent z . Then, gradients from the generator loss (dashed red arrow) are used to compute animproved latent, z0 . After this optimised latent code is used in a second forward pass, we computegradients of the discriminator back through the latent optimisation into the model parameters θD,θG. These gradients are used to update the model. (b) Truncation curves illustrate the FID/IS trade-off for each model by altering the range of the noise source p(z). GD: gradient descent. NGD:natural gradient descent. Points A, B, C, D correspond to samples shown in Figure 1 and 2.
Figure 4: (a) Scaling of gradients in natural gradient descent. We use β = 5 in BigGAN-Deepexperiments. (b) The update speed of the discriminator relative to the generator shown as the dif-ference k∆θD k - k∆θG k after each update step. Lines are smoothed with moving average usingwindow size 20 (in total, there are 3007, 1659 and 1768 data points for each curve). For all curvesoscillation strongly after training collapsed.
Figure 5: (a) The change from ∆z across training, in D’s output space and z’s Euclidean space.
Figure 6: Samples from BigGAN-deep (a) and LOGAN (b) with the similarly high inception scores.
Figure 7: Samples from BigGAN-deep (a) and LOGAN (b) with the similarly low FID. Samplesfrom the two panels were draw from truncations correspond to points A, B in figure 3 b respectively.
Figure 8: Truncation curves with additional baselines. In addition to the truncation curves reportedin Figure 3 b, here we also include the Spectral-Normalised GAN (Miyato et al., 2018), Self-Attention GAN (Zhang et al., 2019), original BigGAN and BigGAN-deep as presented in Brocket al. (2018).
Figure 9: (a) Samples from SN-GAN. (b) Samples from LOGAN.
