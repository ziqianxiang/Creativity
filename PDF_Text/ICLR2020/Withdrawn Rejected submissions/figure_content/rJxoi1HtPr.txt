Figure 1: Sketch of the proposed model: Modules in the STM are actively trained while those inLTM remain frozen. To consolidate an STM module into LTM a new copy is created. A frozen LTMmodule can be reinstated back into STM for further training by overwriting the weights of the STMmodule that was just consolidated into LTM. When a maximum size limit is reached, the LTM copyof the reinstated module is removed to maintain the model size constant.
Figure 2: Mean cross-entropy for the 15 batches after a switch, averaged over all such occurrences.
Figure 3: Weight values for GLTMN computed over the last seven sequences observed by the model.
