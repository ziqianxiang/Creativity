Figure 1: Traffic network.
Figure 2: Simplified representation of the traffic example as a DBN unrolled over time. We use xfor factors that belong to the agent’s local observation and y to denote external state variables. Thearrows illustrate the dependencies between the factors. For simplicity, some of the external statevariables as well as the actions taken at intersection B are omitted from the Bayesian network.
Figure 3: InfluenceNet architecture (left). Automatic d-patch selection (right)4	Influence-aware MemoryThe upshot of IBA is that one can replace the dependence on the full history of actions and obser-vations by a dependence on the d-separating set and create local models without any loss in value.
Figure 4: Traffic control task (left): The observable region corresponds to the non-shaded box cen-tered at the intersection. The hand-specified d-patches are placed at the edges of the left and bottomroad segments. Model performance comparison (right): Mean episodic reward as a function oftraining time . Shaded areas indicate the standard deviation of the mean.
Figure 5: Two examples of the regions chosen by the d-patch selection mechanism (1st and 2ndimages). True state (3rd image) and state predicted by the memory decoder (4th image).
Figure 6: Model performance comparison (right): Mean episodic reward as a function of trainingtime. Shaded areas indicate the standard deviation of the mean.
Figure 7: Traffic controlFigure 8: Breakout12Under review as a conference paper at ICLR 2020Figure 9: PongFigure 10: Space InvadersC Decoding the agent’ s internal memoryThe memory decoder consists of a two-layer fully connected FNN which takes as input the RNN’sinternal memory ht-1 and the output xt of the FNN (see Figure 3) and outputs a prediction for eachof the pixels in the screen. The network is trained on a dataset containing screenshots and their cor-responding network activations (ht-1 and xt), using the pixel-wise cross entropy loss. The datasetis collected after training the agent’s policy. The images are first transformed to grey-scale and thennormalized to simplify the task. The results are shown in Figure 11. The goal of this experimentwas to confirm that although the agent can only see the region delimited by the red boxes, the infor-mation regarding the number and location of cars outside the agent’s observation is somehow storedin the internal memory. It is important to point out that the agent is only trained to maximize the13Under review as a conference paper at ICLR 2020reward and has no explicit knowledge of the environment dynamics. We believe d-patch selectionmechanism plays an important role since it ensures that the RNN focuses on those variables that can
Figure 8: Breakout12Under review as a conference paper at ICLR 2020Figure 9: PongFigure 10: Space InvadersC Decoding the agent’ s internal memoryThe memory decoder consists of a two-layer fully connected FNN which takes as input the RNN’sinternal memory ht-1 and the output xt of the FNN (see Figure 3) and outputs a prediction for eachof the pixels in the screen. The network is trained on a dataset containing screenshots and their cor-responding network activations (ht-1 and xt), using the pixel-wise cross entropy loss. The datasetis collected after training the agent’s policy. The images are first transformed to grey-scale and thennormalized to simplify the task. The results are shown in Figure 11. The goal of this experimentwas to confirm that although the agent can only see the region delimited by the red boxes, the infor-mation regarding the number and location of cars outside the agent’s observation is somehow storedin the internal memory. It is important to point out that the agent is only trained to maximize the13Under review as a conference paper at ICLR 2020reward and has no explicit knowledge of the environment dynamics. We believe d-patch selectionmechanism plays an important role since it ensures that the RNN focuses on those variables that caninfluence future transitions and rewards, namely the d-separating set, while ignoring the rest.
Figure 9: PongFigure 10: Space InvadersC Decoding the agent’ s internal memoryThe memory decoder consists of a two-layer fully connected FNN which takes as input the RNN’sinternal memory ht-1 and the output xt of the FNN (see Figure 3) and outputs a prediction for eachof the pixels in the screen. The network is trained on a dataset containing screenshots and their cor-responding network activations (ht-1 and xt), using the pixel-wise cross entropy loss. The datasetis collected after training the agent’s policy. The images are first transformed to grey-scale and thennormalized to simplify the task. The results are shown in Figure 11. The goal of this experimentwas to confirm that although the agent can only see the region delimited by the red boxes, the infor-mation regarding the number and location of cars outside the agent’s observation is somehow storedin the internal memory. It is important to point out that the agent is only trained to maximize the13Under review as a conference paper at ICLR 2020reward and has no explicit knowledge of the environment dynamics. We believe d-patch selectionmechanism plays an important role since it ensures that the RNN focuses on those variables that caninfluence future transitions and rewards, namely the d-separating set, while ignoring the rest.
Figure 10: Space InvadersC Decoding the agent’ s internal memoryThe memory decoder consists of a two-layer fully connected FNN which takes as input the RNN’sinternal memory ht-1 and the output xt of the FNN (see Figure 3) and outputs a prediction for eachof the pixels in the screen. The network is trained on a dataset containing screenshots and their cor-responding network activations (ht-1 and xt), using the pixel-wise cross entropy loss. The datasetis collected after training the agent’s policy. The images are first transformed to grey-scale and thennormalized to simplify the task. The results are shown in Figure 11. The goal of this experimentwas to confirm that although the agent can only see the region delimited by the red boxes, the infor-mation regarding the number and location of cars outside the agent’s observation is somehow storedin the internal memory. It is important to point out that the agent is only trained to maximize the13Under review as a conference paper at ICLR 2020reward and has no explicit knowledge of the environment dynamics. We believe d-patch selectionmechanism plays an important role since it ensures that the RNN focuses on those variables that caninfluence future transitions and rewards, namely the d-separating set, while ignoring the rest.
Figure 11: True state (left) and state predicted by the memory decoder (right)D HyperparametersThe hyperparameters used in the two experiments are provided in tables 3 and 4. We used the samehyperparameter configuration reported by Schulman et al. (2017) as a starting point for all our dif-ferent models, and hand-tuned those parameters that were added in the InfluenceNet architecture.
