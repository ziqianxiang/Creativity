Figure 1: Illustrating the effectiveness of AdvSPADE generated unrestricted adversarial example com-pared to norm bounded attacks. The first column shows a real image from Cityscapes dataset and its pre-diction result of DRN-105 segmentation network. The 2nd column shows the result of applying PGD to realimages. The 3rd column shows the result of applying PGD to a synthesized image generated by SPADE, whilethe 4th column shows an unrestricted adversarial image created by AdvSPADE which completely fools DRN-105. Note that, while the image in the 4th column is clean, there are conspicuous noises in the 2nd and 3rdcolumn images. Also, the segmentation results are still good for the first three columns. Notice that the objectscritical for a driving system like street lamps, cars, buildings are all well preserved in our generated image(fourth column); the only differences are in their colors and textures, which does not affect the semantics for ahuman driver. However, the prediction of the segmentation model is totally wrong in the fourth column, whichdemonstrates the effectiveness of our unrestricted adversarial attacks for segmentation models.
Figure 2: Illustration of proposed AdvSPADE Architecture for generating unrestricted adversarial ex-amples. Image encoder E takes real images X as input to compute mean and variance vectors (μ(x), σ(x)) andapply reparameterization trick to generate random noise z. SPADE generator G considers z and semantic labelss and generates synthetic images x0 . Next, x0 s are fed into a fixed pre-trained target segmentation network Sand encouraged to mislead S’s predictions by maximizing adversarial loss between predictions and semanticlabels. Meanwhile, x0, as SPADE discriminator D’s input, also aims to fool D. D is trained to reliably dis-tinguish between generated (x0) and real images (x). Random sampled brings randomness into the modelso that G can generate various adversarial examples. Notice that, due to the adversarial loss item, predictionresults at the top left corner of the figure are completely mis-segmented.
Figure 3: Visual Results on ADE20K. As we can see, the semantic meaning of the generated adversarialexamples are well aligned to the original image, but are different in the style and mis-predicted by the seg-mentation model. For example, the color and strip on bed in the first two columns are changed, but humanstill perceive them as bed while the segmentation model predict the wrong label. The results demonstrate theeffectiveness of our method for generating realistic adversarial examples that mislead the target model.
Figure 4: Comparison of norm-bounded samples and AdvSPADE generated unrestricted adversarialexamples at the same mIoU level on ADE20K. We Apply different attack methods to descend mIoU scoreto the same level (around 0.01) and show the visual comparison. First and second columns are adversarialexamples generated by FGSM and PGD with = 32 on real images, and third and fourth columns are the sameon vanilla SPADE synthesis images. The last column is examples generated by AdvSPADE. We can clearly seethe noise pattern in norm-bounded adversarial images rather than in our examples showing that our examplescan attack target networks successfully, yet keep undetectable to human.
Figure 5: Generator,discriminator and encoder architecturesFigure 6: SPADE ResBlock ArchitectureOtherwise, even though generated unrestricted adversarial example can mislead the target networkwhile training, it will still fail in the testing phase due to a large value shifting. We provide a simplequantitative computation to prove our statement.
Figure 6: SPADE ResBlock ArchitectureOtherwise, even though generated unrestricted adversarial example can mislead the target networkwhile training, it will still fail in the testing phase due to a large value shifting. We provide a simplequantitative computation to prove our statement.
Figure 7: User Interfaces for AMT Workers.
Figure 8: Variety of Our Unrestricted Adversarial Examples On Cityscapes: Our model is able togenerate various unrestricted adversarial examples which can mislead the state-of-the-art semanticsegmentation networks. First column is the real images from Cityscapes dataset. Second and thirdcolumns are our stylized unrestricted adversarial examples, the fourth column is the groudtruth labeland the final column shows the prediction of our examples on target segmentation network.
Figure 9: Variety of Our Unrestricted Adversarial Examples On ADE20K: Our model is able to gen-erate various unrestricted adversarial examples which can mislead the state-of-the-art semantic seg-mentation networks. First column is the real images from ADE20K dataset. Second and thirdcolumns are our stylized unrestricted adversarial examples, the fourth column is the groudtruth labeland the final column shows the prediction of our examples on target segmentation network.
Figure 10: Comparison of norm-bounded samples and unrestricted adversarial examples on same mIoUlevel: First and second columns are adversarial examples generated by FGSM and PGD with = 32on real images. Third and fourth columns are adversarial examples generated by FGSM and PGDwith = 32 on vanilla SPADE synthesis images. Last column is unrestricted adversarial examplesgenerated by AdvSPADE.
Figure 11: Relationship between l∞ norm bound size and mIoU decline.
Figure 12: Visual Comparison on Cityscapes: We show the visual comparison between our unrestrictedadversarial examples and norm-bounded adversarial examples with multiple settings on target segmentationnetwork (DRN-105) on Cityscapes. From left to right, the first and second row show the adversarial exam-ples generated by adding different bound size ( = 0, 8, 32) perturbation on real images with different attackmethods (FGSM, PGD) and their corresponding semantic predictions from DRN-105. The third and fourth rowfollows the same setting except the norm perturbation is added on standard SPADE generated images. The lastrow shows our unrestricted adversarial example and its prediction.
Figure 13: Visual Comparison between our unrestricted adversarial examples and norm-bounded ad-versarial examples on ADE20KFigure 14: Visual Results on Cityscapes: First row is the real images in Cityscapes validation set. Sec-ond row is the corresponding groundtruth labels. Third row is our unrestricted adversarial examplesand last row is the corresponding segmentation results.
Figure 14: Visual Results on Cityscapes: First row is the real images in Cityscapes validation set. Sec-ond row is the corresponding groundtruth labels. Third row is our unrestricted adversarial examplesand last row is the corresponding segmentation results.
Figure 15: Visual Results on ADE20K: First row is the real images in Cityscapes validation set. Sec-ond row is the corresponding groundtruth labels. Third row is our unrestricted adversarial examplesand last row is the corresponding segmentation results.
