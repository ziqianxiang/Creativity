Figure 1: Eigenvalue histograms of the value-based linear layer during training. 50 agent runs data-efficient Rainbow (van Hasselt et al., 2019), of 100,000 steps on the Atari game Road Runner.
Figure 2: Our architectural approach consists in replacing hidden layers in deep RL agents withtensor regression (top). Optionally we substitute the topmost convolutional layer with scattering(middle), and combine both methods (bottom).
Figure 3: Prioritized tensorized DQN on Atari Pong. Original learning curve versus several learningcurves for five different Tucker ranks factorizations and therefore parameter compression rates (3different random seeds each, with a 30 episodes moving average for legibility). Best viewed incolour.
Figure 4: Focus on a typical single run of the tensorized DQN learning (score vs. number of thou-sand episodes). The overall shape of the typical learning curve is preserved, but drawdowns in theplateauing phase do appear.
