Figure 1: RNN based filter's architecture: Bayesian Filter Net (BFN).
Figure 2: Our approach,s illustrative diagramAs shown in the diagram 2, we model the conditional prior probability p(xk|Yk-1) posteriorprobability p(xk |Yk) resp. of the state at step k by finite dimensional prior statistics sk|k-1 (posteriorstatistics Sk∣k resp.). In finite dimensional filter case (Benes (1981; 1985); Daum (1987)), there existfinite dimensional statistics that are sufficient, that is to say, the evolution of conditional probabilitycan be fully captured by the evolution of a finite dimensional vector. We denote the evolutionfunction in updating step by 夕 and the evolution function in prediction step by φ. And further, aftermodelling the the probability distribution as finite dimensional statistics, we use two neural networksto approximate the evolution of them. And to get the final estimate of the state, we use another neuralnetwork to approximate the map from the statistics to the optimal estimation.
Figure 3: (a) The decrease of TPMSE of RNN based filter with different hidden variable dimensionsin training. (b) Convergent TPMSE of RNN based filter with different dimensions of hidden variables.
Figure 4: (a) The evolution of PMSEk. (b) Xk tracks Xk well.
