Figure 1: Learning curves for regression on MNIST and CIFAR10 (a-b); and for classificationon MNIST and CIFAR10 (c-d). Curves are averaged over 400 runs. A power law is plotted toestimate the asymptotic behavior at large n: the exponent is fitted on the last decade on the averageof the two curves, since it does not seem to depend significantly on the specific kernel or on thetask. In each setting We use both a Gaussian kernel K(x) 8 exp(-∣∣χ∣∣2∕(2σ2)) and a Laplace oneK(x) H exp(-∣∣x∣∣∕σ), with σ = 1000.
Figure 2: Results for the Teacher-Student kernel regression problem, where the Student is always aLaplace kernel. Data points are sampled uniformly at random on a d-dimensional hypersphere. (a-b)Mean-square error versus the size of the training dataset, for Gaussian and Laplace Teachers and formultiple spatial dimensions. Dotted lines are the fitted power laws — we fit starting from n = 700.
Figure 3: Average distance from one point to its nearest neighbor as a function of the dataset size n.
Figure 4: Learning curves for kernel regression on the MNIST dataset. Regression is performed withseveral Laplace kernels of varying variance σ ranging from σ = 10 to σ = 10000.
Figure 5: In these plots we show the results for the Teacher-Student kernel regression. The Student isalways a Laplace kernel, the Teacher is either Gaussian or Laplace. The four plots on the left depictthe mean-square error against the size of the dataset for different spatial dimensions of the data, thoseon the right show the fitted asymptotic exponent against the spatial dimension for different datasetsizes. For every case we show both the the results for σT = σS = d and σT = σS = 10.
Figure 6: Left: The test error of a Laplace Teacher (αT = d + 1) with a Gaussian Student (αS = ∞)decays as a power law with the predicted exponent β = d min(1, ∞) = 1 in d = 6 dimensions.
Figure 7: Mean-squared error for Matern Teacher kernels and Laplace students. The variance of thekernels is equal to 2 for all the curves.
