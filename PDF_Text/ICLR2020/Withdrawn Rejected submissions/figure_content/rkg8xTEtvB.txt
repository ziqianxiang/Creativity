Figure 1: An illustration of a hierarchical structure (a) and extracting the hierarchical features thatconstitute a leaf-level image (b). In (b), the common features that only contain the information of itsbeing the root category are first extracted. By tracing from the root to leaf, the unique features thatcontain additional information of its being the finer-grained category are further extracted.
Figure 2: An illustration of the framework of our method. Given images belonging to a hierarchy,the common feature maps of their being the root category and the unique features in different non-root levels that can further distinguish them as finer-grained categories are extracted by the upperresidual and bottom 1*1 convolutional branches, respectively. The unique features are transformedas the parameters of Adaptive Instance Normalization (AdaIN) and thus be aggregated into the com-mon feature maps to obtain comprehensive representations of images. To ensure disentanglementof the unique features, different levels of them are randomly combined and then reconstructed in theimage space where adversarial loss and hierarchical classification loss are elaborately designed.
Figure 3: Semantic translation results of the source images controlled by hierarchically disentangledfeatures of the targets on CelebA. Different columns denote results of using F1, {Rl}lL=2 or theircombinations disentangled from the target images to replace the corresponding levels of the sources.
Figure 4: 2D tSNE of disentangled Fl on test set of CelebA for different levels. For easy understand,M and F mean male and female, S and N mean Smile and Neural, Bl, G and Br mean Black, Golden4.1 Disentangled ResultsFirstly, we replace one or several levels of disentangled features for an image with those of anotherimage, and then observe the visual changes of generated image to validate the semantic consistencewith pre-defined hierarchical structure. Fig.3 (and Fig.12, Fig.13 in the Appendix) shows suchsemantic translation results. It is observed that different level of features perform their own duties,i.e. they carry just enough information to control the variations within that level (e.g. gender,smile and hair color for CelebA we specially predefined on CelebA), but would not involve morebelongs to other level. For instance, in Fig.3 change features of an image in arbitrary one, twoor all levels to those of another image, the semantics would be changed correspondingly. Apartfrom {Rl }lL=2, the common feature F1 also encodes information that is not discriminative amongits offspring categories but is necessary to construct the object (e.g. the identity, pose and eventhe background information of a face image). To give a more intuitive feeling about the ability ofdisentangled features, we investigate the discriminabilites of them via the popular tSNE tool (Maaten& Hinton, 2008). As shown in Fig.4 (and Fig.14, Fig.15, Fig.16, Fig.17 in the Appendix), withonly the common feature F1 , samples are mixed together. When progressively be combined withfeatures of deeper levels Rl, samples are better separated and almost consistent with the hierarchicalstructure.
Figure 7: Semantic translation results between seen and unseen objects. Here We replace the sourceimages with all levels of Rl of the targets, i.e. the right most case in Fig.3.
Figure 8:	Typical samples of hierarchical data on CelebA. Images within a purple rectangular boxare some instances of a leaf-level category. Categories within a green rectangular box belong toone common super-category. The super-categories within a red rectangular box share one commonancestor.
Figure 9:	Typical samples of hierarchical data on Fashion-MNIST (a) and ShapeNet (b). Imageswithin a purple rectangular box are some instances of a leaf-level category. Categories within agreen rectangular box belong to one common super-category. The super-categories within a redrectangular box share one common ancestor. On ShapeNet, categories within one purple rectangu-lar box can be further divided into four child categories based on pose variations. Therefore, onehierarchy named Shape-C (Category) and another one named ShapeNet-P (Pose) are defined.
Figure 10: Typical samples of hierarchical data on CADCars. Images within a purple rectangularbox are some instances of a leaf-level category. Categories within a green rectangular box belong toone common super-category. The super-categories within a red rectangular box share one commonancestor.
Figure 11:	Typical samples of unseen leaf-level data on CelebA (bald and gray hair), ShapeNet-C(kinds of tables and sofas) and ShapeNet-P (more poses). Large differences with seen hierarchicaldata can be found here.
Figure 12:	Semantic translation results of the source images controlled by hierarchically disentan-gled features of the targets on CADCars(a) and Fashion-MNIST (b).
Figure 13:	Semantic translation results of the source images controlled by hierarchically disentan-gled features of the targets on ShapeNet-C (a) and ShapeNet-P (b).
Figure 14:	2D tSNE of disentangled Fl on test set of Fashion-MNIST for different levels. T andB mean Top and Bottom, Ts, P, C, Tr and D mean T-shirt, Polo shirt, Coat, Trousers and DressesrespeCtively.
Figure 15:	2D tSNE of disentangled Fl on test set of CADCars for different levels. M SU, Sp andSe mean Minibus, SUV Sports and Sedan, P1 ã€œP4 and P6 mean Pose 1, 2, 3, 4 and 6 respectively.
Figure 16:	2D tSNE of disentangled Fl on test set of ShapeNet-C for different levels. S and T meanSofa and Table, Lo, C, Lc, W, B and Te mean Loveseat, Club chair, L-couch, Work table, Billiardsand Tennis-table respectively.
Figure 17: 2D tSNE of disentangled Fl on test set of ShapeNet-P for different levels. L, C, W andB mean Loveseat, Club chair, Work table and Billiards respectively.
Figure 19:	Semantic translation results between images from different datasets (i.e. image Pairs fromCelebA and RAF, and from CADCars and ComPCars.) using all levels of features of the targets.
Figure 20:	Semantic translation results of baselines and HDN-full on CelebA. Different columnsdenote results of using F1, {Rl}lL=2 or their combinations disentangled from the target images toreplace the corresponding levels of the sources. Ground truths of R2 , R3 , R4 are gender, smile andhair color, respectively.
Figure 21: 2D tSNE of disentangled Fl by HDN without reconstruction feature loss on test set ofCelebA for different levels. For easy understand, M and F mean male and female, S and N meanSmile and Neural, Bl, G and Br mean Black, Golden and Brown hair respectively.
Figure 22: Semantic translation results of compared cGANs and HDN on CelebA. R2 , R3 , R4 aregender, smile and hair color, respectively. StarGAN only needs attribute conditions to generateimages, and the target in its row has no use for it. ELEGANT-2 is trained with only gender andsmile attributes. The ELEGANT models can only change one attribute from the target each time.
Figure 23:	Typical samples of hierarchical data on ImageNet. Images within a purple rectangularbox are some instances of a leaf-level category. Categories within a green rectangular box belong toone common super-category. The super-categories within a red rectangular box share one commonancestor.
Figure 24:	2D tSNE of disentangled Fl on test set of ImageNet for different levels. H, D and B meanHouse cat, Dog and Big cat, E, P, Si, Ta, Cor, G, Hu, Sa, Cou, Le, Li and Ti mean Egyptian, Persian,Siamese, Tabby cat, Corgi, German shepherd, Husky, Samoyed, Cougar, Leopard, Lion, and Tigerrespectively.
Figure 25: Semantic translation results of the source images controlled by hierarchically disentan-gled features of the targets on ImageNet.
