Figure 1: Overview of the proposed UNITER model (best viewed in color), consisting of an Image Embedder,a Text Embedder and a multi-layer self-attention Transformer, learned through three pre-training tasks.
Figure 2: Different data splits from downstream tasks based on COCO images. Our UNITER pre-training avoids seeing any downstream evaluation images.
Figure 3: Attention heatmaps of UNITER(f) Reversed BlockIn the end, we have two randomly sampled negatives and two hard negative samples per positivesample. N is set to 4000 for COCO and 2500 for Flickr30K.
Figure 4: Text-to-image attention visualization example.
Figure 5: Text-to-image attention visualization example.
Figure 6: Text-to-image attention visualization example.
Figure 7: Example showing difference between conditional masking and joint random masking.
Figure 8: Comparison of MLM and MRC-kl validation accuracy using joint masking and our pro-posed conditional masking.
