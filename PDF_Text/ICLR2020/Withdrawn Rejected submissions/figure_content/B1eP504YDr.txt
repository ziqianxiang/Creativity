Figure 1: Model Dependency Graph.
Figure 2: Results on dependency factor modeling. (a): The top and bottom image respectively illus-trate st and st+k in the case study, and here we set k to be 7. (b): Dashed lines show the true dis-tribution of Pπ (at∣st, st+k); solid lines show the dependency model prediction Pφ (at∣st, st+k, k);x-axis represents the number of training iterations of dependency model. Four different colors rep-resent four different actions. Blue line shows the KL divergence between predicted distribution andtrue distribution. (c): The blue line shows the mean KL divergence between Pπ(at∣st, st+k) andPφ (at |st, st+k, k) over the dataset of random (St, st+k) pairs, averaged in 10 runs.
Figure 3: (a): Mean squared error of two value functions Vw1 and Iw2 averaged in 10 runs on PixelGrid World. Green and red lines show the MSE of Vw1 and Iw2 respectively, when rψ is fixed; blueand orange lines show the MSE of Vw1 and Iw2 respectively, when rψ is trained by our method.
Figure 4: Overall performance curve. Figure (a) and (b) respectively show the training curve onPixel Grid World environment in per-step punishment setting and no punishment setting, averagedin 10 random seeds. In per-step punishment setting agent gets negative rewards before reachinggoals, while in no punishment setting agent gets no reward before reaching goals.
Figure 5: Observation in Pixel Grid World.
Figure 6: Network architecture for dependency model and reward decomposition model.
Figure 7: Case study on reward decomposition model. We fix a state-action pair (st , at) and visu-alize reward decomposition for every future reward. The x-axis represents the interval from currentstate st to future states st+k , green line represents the dependency factor of future states, blue andorange lines are future rewards that assigned to Monte-Carlo estimator and importance samplingestimator respectively, and red lines are the estimation by importance sampling estimator on eachfuture time. Black dashed lines represent periodical resets.
