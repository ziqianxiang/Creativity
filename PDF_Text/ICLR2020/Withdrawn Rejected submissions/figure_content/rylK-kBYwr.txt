Figure 1: Examples of single-domain (Left) and multi-domain dialogues (Right). U : user utterance, BS:dialogue belief state, S: system response. The subscript indicates dialogue step. Best viewed in color.
Figure 2: Our architecture consists of 3 major components: (i) Encoders encode sequences of dialogue history,previous user utterances, domain and slot tokens, and dialogue belief states of previous turn, into continuousrepresentations; (ii) Multi-level Neural Belief Tracker consists of 2 modules, one for learning slot-level signalsand one for domain-level signals; the outputs are combined to obtain domain-slot joint features, which are useddecode multi-domain belief states; and (iii) Response Generator incorporates information from dialogue context,dialogue states, and knowledge base, to decode system responses. For simplicity, Feed Forward, ResidualConnection, and Layer Normalization layers are not presented. Best viewed in color.
Figure 3: Example dialogue with the input system response St-1 and current user utterance Ut, and the outputbelief state BSt and system response St . Compared with TSCP (Row 3), our dialogue state and response (LastRow) are more correct and closer to the ground truth (Row 2). Visualization of attention to the user utterancesequence at slot-level (lower right) and domain-level (upper right) is also included. More red denotes higherattention score between domain or slot representation and token representation. Best viewed in color.
Figure 4: Joint Accuracy metric by dialogue turn in the test data.
Figure 5: BLEU4 metric by dialogue turn in the test data.
