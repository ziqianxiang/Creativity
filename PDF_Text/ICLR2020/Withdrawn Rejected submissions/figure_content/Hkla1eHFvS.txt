Figure 1: State Marginal Matching: (Left) Our goal is to learn a policy whose distribution over states (bluehistogram) matches some target density (black line). Our algorithm iteratively increases the reward on statesvisited too infrequently (green arrow) and decreases the reward on states visited too frequently (red arrow).
Figure 2: Exploration in State Space (SMM) vs. Action Space (SAC) for Navigation: (a): A point-massagent is spawned at the center of m long hallways that extend radially outward, and the target state distributionplaces uniform probability mass ml at the end of each hallway. We can vary the length of the hallway and thenumber of hallways to control the task difficulty. (b) A heatmap showing states visited by SAC and SMMduring training illustrates that SMM explores a wider range of states. (c) SMM reaches more goals than theMaxEnt baseline. SM4 is an extension of SMM that incorporates mixture modelling with n > 1 skills (seeAppendix B), and further improves exploration of SMM.
Figure 3: Exploration for Manipulation. (a) Task: The robot agent controls a single gripper arm to move ablock object to a goal location on the table surface. The goal is not observed by the robot, thus requiring the robotto explore by moving the block to different locations on the table. (b) Test-Time Exploration: At test-time,we sample goal locations uniformly on the table, and plot the percentage of goals found within N episodes.
Figure 4: Real-World Exploration: (a) D’Claw is a 9-DoF robotic hand (Ahn et al., 2019) that is trainedto turn a valve object. (b) Sim2Real: We trained each algorithm in simulation, and then measured how farthe trained policy rotated the knob on the hardware robot. We also measured the maximum angle that theagent turned the knob in the clockwise and counter-clockwise directions within one episode. (c) Training onHardware: We trained SAC and SMM on the real robot for 1e5 environment steps (about 9 hours in real time),and measured the maximum angle turned throughout training. We see that SMM moves the knob more and visitsa wider range of states than SAC. All results are averaged over 4-5 seeds.
Figure 5: Ablation Analysis of State Marginal Matching with Mixtures of Mixtures (SM4). (a):On the Navigation task, we compare SM4 (with three mixture components) against ablation baselinesthat lack conditional state entropy, latent conditional action entropy, or both (i.e., SAC) in the SM4objective (Equation 8). We see that both terms contribute heavily to the exploration ability of SM4,but the state entropy term is especially critical. (b): We compare SMM/SM4 with different numbersof mixtures, and with vs. without historical averaging. We found that increasing the number of latentmixture components n ∈ {1, 2, 4} accelerates exploration, as does historical averaging. Error barsshow std. dev. across 4 random seeds.
Figure 6: GAIL Ablation Study: We studied the effect of restricting the GAIL discriminator inputto fewer state dimensions. (a) Manipulation: We trained the GAIL discriminator on the entire statevector s; on the object and gripper positions {sobject, srobot} only; or on the object position sobject only.
Figure 7: The log state marginal log ρπ (s) over block XY-coordinates, averaged over 1e3 epochs.
Figure 8: SM4 with Eight Mixture Components. The log state marginal log ρπz (s) over blockXY-coordinates for each latent skill z ∈ {0, . . . , 7}, averaged over 1000 epochs.
Figure 9: Goals sampled uniformly on the table surface, colored by the number of episodes until thepolicy finds the goal. Red (100 episodes) indicates failure. The block alWays starts at the center.
