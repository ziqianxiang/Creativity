Figure 1: Vector quantized latent representations for different question turns.
Figure 2: Vector quantized model.
Figure 3: Average position of different codewords in FlowQA (K=8, 16).
Figure 4: Codewords distribution on start/end punctuation of FlowQA (K=16).
Figure 5: The frequency of POS tags in each codeword of FlowQA (K=16). Note that for all heatmaps in this work, the codewords are ordered by their frequecy in the latent representation, from themost frequent to the rarest. For each codeword in hidden layer, we can find the corresponding wordtoken in input layer. Then we use that word to generate POS tags.
Figure 6: FlowQA with vector quantized layer.
Figure 7: SDNet with vector quantized layer.
Figure 8: Average position of different codewords in FlowQA.
Figure 9: Influence of important words on answerability.
Figure 10: Codebooks distribution on start/end punctuation in FlowQA (K=32, 64, 128, 512).
Figure 11:	Example 1 of FlowQA.
Figure 12:	Example 2 of FlowQA.
