Figure 1: Fetch robot arm environments and a navigation task based on the Gazebo simulator:FetchPush, FetchPickAndPlace, FetchSlide, SocialBot-PlayGround.
Figure 2: MISC Algorithm: We update the discriminator to better estimate the mutual information(MI), and update the agent to control states of interest to have higher MI with the context states.
Figure 3: Manipulation skills: Without any reward, MISC learns skills for reaching, pushing,sliding, and picking up an object. The video of the learned skills is shown at https://youtu.
Figure 4: Experimental results in the navigation taskand compare the performance with ICM (Pathak et al., 2017) and Empowerment (Mohamed &Rezende, 2015). During training, we only use one of the intrinsic rewards such as MISC, ICM, orEmpowerment to train the agent. Then, we use the averaged task reward as the evaluation metric.
Figure 5: Mean success rate with standard deviation: The percentage values after colon (:) rep-resent the best mean success rate during training. The shaded area describes the standard deviation.
Figure 6: Performance comparison: We compare the MISC variants, including MISC-f, MISC-r,and MISC-p, with DIAYN, VIME, and PER, respectively.
Figure 7: Transferred MISCas “MISC-t”, where “-t” stands fortransfer. The MISC reward function trained in its corresPonding environments is denoted as “MISC-r”. We comPare the Performance of DDPG baseline, MISC-r, and MISC-t. The results are shownin Figure 7. PerhaPs surPrisingly, the transferred MISC still imProves the Performance significantly.
