Figure 1: Performance of S2VG with H = 1 and other baselines in benchmark tasks. The x-axisis the training step (epoch or step). Each experiment is tested on five trailed using five differentrandom seeds and initialized parameters. For a simple task, i.e., InvertedPendulum, we limit thetraining steps at 40 epochs. For the other three complex tasks, the total training steps are 200Kor 300K. The solid line is the mean of the average return. The shaded region represents the stan-dard deviation. On Invertedpendulum,HalfCheetah, Hopper, Swimmer, S2VG outperforms the otherbaselines significantly. In the task Walker2d, SLBO is slightly better than S2VG. They both surpassother algorithms. On Reacher, S2VG and SAC perform best.
Figure 2: S2VG with value expansion. We do the ablation study on (a) Pendulum and (b) HalfChee-tah problem where the x-axis is the training step and the y-axis is the reward. (c) reflects the bias ofthe value function in the training procedure.
