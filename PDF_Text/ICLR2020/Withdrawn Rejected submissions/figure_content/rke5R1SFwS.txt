Figure 1:	Using a readout layer to measure catastrophic forgetting on representations. A) A networkis first pre-trained on Task A. B) Then it is finetuned on Task B. C) We then feed in Task A trainingdata to the network, and record the representation at the last layer prior to the classification layer. Were-train a readout layer to recover Task A output. Test accuracy on Task A is recorded as “Readout”,and original classification layer accuracy is recorded as “Original”. Chance is 20%.
Figure 2:	Overview of our purposed method during one training step: 1) the teacher network isupdated using SGD on multi-task data; 2) the student network is updated using the meta-learner onTask B data only; 3) the multi-task data are fed into both networks and we record the representationsh and h; 4) the metaasmodule is then updated to minimize the difference between h and h. Atmeta-test time, the teacher network is no longer present, and we update the student network solelywith the trained meta-learner for the entire training sequence.
Figure 3: Our meta-learner module is a stacked LSTM network. For a given output neuron hj, themeta-network takes in its activation, its pre-activations, current weights and gradients, and output theweight updates assiciated with hj. Weights in the same layer will be shared a single meta-network,while different layers will have different nmeta-networks.
Figure 4: Left: Exp 1: MNIST → FashionMNIST, “Freeze” has 98.37% on Task A and 75.64% onTask B; Right: ExP 1: CIFAR 5A 7→ 5B, “Freeze” has 92.31% on Task A and 74.81% on Task B.
Figure 5: Left: ExP 2: CIFAR 5A → 100 with unseen classes; Right: ExP 3: CIFAR 5A → 100with unseen initialization checkpoints. “Freeze” has 92.25% on Task A and 71.68% on Task B. Errorbar denotes standard error of the mean of 5 runs.
Figure 6: Training curve on Experiment 2Figure 7: Visualization of the meta-learner outputslearner on Task B2. Unlike the previous experiment, the class definition changes from meta-trainingto meta-testing. This is a more practical setting since in reality We do not know a priori which newtask the model needs to adapt to.
Figure 7: Visualization of the meta-learner outputslearner on Task B2. Unlike the previous experiment, the class definition changes from meta-trainingto meta-testing. This is a more practical setting since in reality We do not know a priori which newtask the model needs to adapt to.
