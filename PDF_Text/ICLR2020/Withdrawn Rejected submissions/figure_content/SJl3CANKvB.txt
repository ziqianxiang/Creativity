Figure 1: Recall vs Imbalance RatioTable 3: ReCall@k on In-ShoPRecall@K	1	10	20	30	40	50FashionNet(Liu et al. (2016))	53.7	73.0	76.0	77.0	79.0	80.0HDC(Oh Song et al. (2017))	62.1	84.9	89.0	91.2	92.3	93.1HDL(Ge (2018))	80.9	94.3	95.8	97.2	97.4	97.8ABIER(Opitz et al. (2018))	83.1	95.1	96.9	97.5	97.8	98.0ABE(Yu et al. (2018))	87.3	96.7	97.9	98.2	98.5	98.7MS(Wang et al. (2019))	89.7	97.9	98.5	98.8	99.1	99.2DRO-TopKM (Ours)	91.0	98.1	98.7	99.0	99.1	99.2DRO-TopKB (Ours)	90.7	97.7	98.4	98.8	99.0	99.1DRO-TopK-PNM (Ours)	91.3	98.0	98.7	98.9	99.1	99.2DRO-TopK-PNB(Ours)	91.1	98.1	98.6	98.8	99.0	99.2DRO-KLM (Ours)	90.8	98.0	98.6	99.0	99.1	99.2Table 4: Recover of MS loss and LS loss on In-ShopRecall@K(%)	1	10		20	30	40	50MS	79.8	94.9	96.8	97.6	97.9	98.3LS	82.6	94.1	95.6	96.4	96.9	97.4DRO-KL-G-γ = 1	84.8	95.9	97.3	97.9	98.2	98.5DRO-KL-G-γ = 0.1	85.1	96.1	97.5	98.0	98.3	98.5
Figure 2: The effects of K on recall@k on In-ShopFigure 3: Average running time of every iteration4.2.4	Runtime ComparisonNext, we compare the running time of our proposed three variants of our DRO framework withdifferent pair mining methods, MS and LS losses on In-shop. Our experiments conducted on eightGTX1080Ti GPU. The embedding dimension d = 1024, and results are compared under differentbatchsize B = {80,160, 320,480,640}. The same as previous experiments, we set K = 2 * B bothfor DRO-TopKM and DRO-TopK-PNM. γ = 0.1 for DRO-KLM. SH is implemented according tothe paper Schroff et al. (2015), Wu et al. (2017). DWS and MS are implemented based on the codeprovided by the author. LS loss is implemented following the code provided by Wang et al. (2019).
Figure 3: Average running time of every iteration4.2.4	Runtime ComparisonNext, we compare the running time of our proposed three variants of our DRO framework withdifferent pair mining methods, MS and LS losses on In-shop. Our experiments conducted on eightGTX1080Ti GPU. The embedding dimension d = 1024, and results are compared under differentbatchsize B = {80,160, 320,480,640}. The same as previous experiments, we set K = 2 * B bothfor DRO-TopKM and DRO-TopK-PNM. γ = 0.1 for DRO-KLM. SH is implemented according tothe paper Schroff et al. (2015), Wu et al. (2017). DWS and MS are implemented based on the codeprovided by the author. LS loss is implemented following the code provided by Wang et al. (2019).
Figure 4: Mean and Std of ReCall@k over five runs Figure 5: ReCall@1 Vs Imbalance Ratio on embed-ComParing the best baseline performer (MS)	ding space dimension 512To investigate the effect of randomness of the stochastic algorithms and evaluate the robustness ofour DRO framework, We report the average mean and standard variance of recall@k on Cub-200-2011 in Figure 4. We do not plot recall@4 and recall@16 for better visualization. The experimentalsetting is the same as the experiments of the SOTA quantitative results we reported in Table 1 (sec-tion 4.1) but with five runs. The gray bars are the recall@k of best performer among SOTA baselines,i.e., MS. It is clear to see that all our DRO variants outperform MS in terms of the average recall@kover all different values of k. Specifically, the average recall@1 OfDRO-TOPKB is 67.9%, whichhas a significant improvement over the baselines, i.e., 65.7% of MS. In addition, the small standarddeviation error bars imply that our DRO framework is robust enough to have a better performancethan SOTA methods.
