Figure 1: Three examples of natural perturbations from nearby video frames and resulting classifierconfidences from a ReSNet-152 model fine-tuned on ImageNet-Vid. While the images appear almostidentical to the human eye, the classifier confidence changes substantially.
Figure 2: Temporally adjacent frames may not be visually similar. We show three randomly sampledframe pairs where the nearby frame was marked as “dissimilar” to the anchor frame during humanreview and then discarded from our dataset.
Figure 3: Model accuracy on original vs. perturbed images. Each data point corresponds to onemodel in our testbed (shown with 95% Clopper-Pearson confidence intervals). Each perturbed framewas taken from a ten frame neighborhood of the original frame (approximately 0.3 seconds). Allframes were reviewed by humans to confirm visual similarity to the original frames.
Figure 4: Naturally perturbed examples for detection. Red boxes indicate false positives; green boxesindicate true positives; white boxes are ground truth. Classification errors are common failures, suchas the fox on the left, which is classified correctly in the anchor frame, and misclassified as a sheep ina nearby frame. However, detection models also have localization errors, where the object of interestis not correctly localized in addition to being misclassified, such as the airplane (middle) and themotorcycle (right). All visualizations show predictions with confidence greater than 0.5.
Figure 5: We plot the fraction of times each offset caused an error, across all evaluated models, forframes with and without review. Frames further away more frequently cause classifiers to misfire.
Figure 6: Model accuracy on original vs. perturbed images for a static set of perturbed frames acrossall models. The grey points and grey linear fit correspond to the perturbed accuracies of modelsevaluated on per model perturbations studied in Figure 3D Per class analysisWe study the effect of our perturbations on the 30 classes in ImageNet-Vid-Robust andYTBB-Robust to determine whether the performance drop was concentrated in a few “hard” classes.
Figure 7: Per-class accuracy statistics for our best performing classification model (fine-tunedResNet152) on ImageNet-Vid-Robust and YTBB-Robust. For Youtube-BB, note that ‘zebra’is the least common label, present in only 24 anchor frames sampled by Gu et al. (2019), of which 4are included in our dataset.
Figure 8: Conditional robustness metric from Gu et al. (2019) on perturbed frames as a function ofperturbation distance on ImageNet-Vid-Robust and YTBB-Robust. Model accuracies fromfive different model types and the best performing model are shown. The model architecture isResNet-50 unless otherwise mentioned.
Figure 9: For the two example videos above the score from Gu et al. (2019) metric (Accuracy @ K)is identical, but the PM-k metric behaves substantially differently when the errors are spread acrossmany independent videos, as shown in the right example∙112∙3sə-duJeXə oəpF '∞ DISTANCE VS PM-K ACCURACY'∞ adversarial examples are well studied in the robustness community, yet the connection between'∞ and other forms of more “natural” robustness is unclear. Here, we plot the cumulative distributionof the '∞ distance between pairs of nearby frames in our datasets. In Figure 10, we show the CDFof '∞ distance for all pairs, all reviewed pairs, and mistakes made by 3 indicative models. Note thefbrobust model is trained specifically to be robust to '∞ adversaries.
Figure 10: CDF showing the '∞ distance between pairs of frames from different distributions.
Figure 11: Model classification accuracy on perturbed frames as a function of perturbation distance(shown with 95% Clopper-Pearson confidence intervals). Model accuracies from five differentmodel types and the best performing model are shown. The model architecture is ResNet-50 unlessotherwise mentioned.
