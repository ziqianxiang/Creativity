Figure 1: MaxEnt RL solves the Meta-POMDP. On a5-armed bandit problem, we show that solving a MaxEntRL problem with a reward of r(i) = 2 logp(i) mini-mizes regret on the meta-POMDP defined by p(i). Thethick line is the average across 10 randomly-generatedbandit problems (thin lines). Lower is better.
Figure 2: MaxEnt RL = Robust-Reward Control: The policy obtained by running MaxEnt RL on rewardfunction r is the optimal robust policy for a collection of rewards, Rr . (Left) We plot the original rewardfunction, r as a red dot, and the collection of reward functions, Rr , as a blue line. (Center) For each policy,parameterized solely by its probability of choosing action 1, we plot the expected reward for each rewardfunction in Rr . The robust-reward control problem is to choose the policy whose worst-case reward (dark blueline) is largest. (Right) For each policy, we plot the MaxEnt RL objective (i.e., the sum of expected reward andentropy).
Figure 3: MaxEnt RL solves a robust-reward con-trol problem. On a 5-armed bandit problem, MaXEntRL converges to the optimal minimaX policy. Fictitiousplay, a prior method for solving adversarial problems,fails to solve this task, but an oracle variant achieves re-ward similar to MaXEnt RL. The thick line is the averageover 10 random seeds (thin lines). Higher is better.
Figure 4: Expanded Robust Set: MaxEnt RL is robust to the reward functions along the blue line,which implies that it is also robust to the reward functions in the shaded region.
Figure 5: Global Affine Transformations: In a simple, 2-armed bandit, we draw one reward function(turquoise dot) an its robust set (turquoise, thick line). The policy that is minimax for the rewardfunctions in the robust set. The policy is also minimax for other robust sets, obtained by (Left) scalingand (Right) shifting the original robust set. Importantly, the policy is not simultaneously robustagainst the union of these robust sets.
Figure 7:	Approximately solving an arbitrary robust-reward control problem. In this experi-ment, we aim to solve the robust-reward control problem for an arbitrary set of reward functions.
Figure 8:	MaxEnt RL solves robust-reward control problems for robotic control tasks. In the top row, weplot the average cumulative reward, with each column showing a different environment. In the bottom row,we plot the average worst-case reward. While both MaxEnt RL and standard RL succeed at maximizing thecumulative reward, only MaxEnt RL succeeds at maximizing worst-case reward. The thick line is the averageover five random seeds (thin lines).
