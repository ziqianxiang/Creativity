Figure 1: The idealised world state completely described by compositions of the following inde-pendent transformations: changes in position x, y and colour. Such a state may be projected into ahigh-dimensional observation, which may contain a lot of irrelevant detail, like the particular instan-tiation of the Qbert, or the grassy background. Disentangled representations recover the meaningfulinformation about the independently transformable aspects of the world and disregard the irrelevantdetails.
Figure 2: Schematic illustration of the three steps of our method. First We use an existing method forunsupervised disentangled feature discovery from observations obtained using an exploration policy.
Figure 4:	Average reward achieved by the different methods across the same set of tasks. The agentreceives a reward of 1 if the goal location is reached, otherwise it receives 0. The rewards are averagedacross 3 seeds and 3 tasks per task category. Agent tasks include moving the agent to a specifiedposition. Easy tasks specify a vertical or a horizontal position (e.g. "get the agent to the top"). Hardtasks specify a conjunction of a vertical and a horizontal position (e.g. "get the agent to the bottomright"). Object tasks are similar to agent tasks but specify the goal position of one of the objects(e.g. "get the square to the centre left"). Disjunction tasks are set by specifying a goal in terms of adisjunction of the corresponding easy or hard agent and object tasks (e.g. "get the agent to the leftOR get the circle to the middle"). Finally, conjunction tasks are specified as a particular vertical andhorizontal position for both objects (e.g. "get the square to the top left AND the circle to the bottomright"). GPI with disentangled features and the off-diagonal trick is able to solve all the tasks at least50% of the time almost immediately. DQN takes orders of magnitude more steps to achieve similarperformance. DIAYN has discovered some agent control policies that allow it to solve some agentbased tasks, but it never discovered how to control objects. We used 9 bin feature discretisation totrain the feature control policies used by GPI.
Figure 5:	Average reward achieved by GPI with disentangled features and the off-diagonal trick on thesame tasks as in Fig. 4, but using different numbers of feature discretisation bins. The performance ofour approach decreases with smaller numbers of bins, since GPI has fewer policies to work with, butstill significantly outperforms all baselines shown in Fig. 4 in terms of data efficiency. Aligning thedownstream tasks with the bin discretisation used to train the feature control policies (bin=7, aligned)slightly improves the performance of our approach, especially on harder tasks.
