Figure 1: Clustered stroke predictions on MNIST sequences. Our multi-modal Conditional Nor-malizing Flow based prior (right) enables our regularized CF-VAE to capture the two modes of theconditional distribution, while predictions with uni-modal Gaussian prior (left) have limited diversity.
Figure 2: CF-VAE. The decoder is regu-lariZed by removing conditioning (greyarrow) to prevent posterior collapse.
Figure 3: Random samples clustered using k-means. The number of clusters is set manually to thenumber of expected digits. The corresponding priors of our CF-VAE + pR on the right. Note, our64D CF-VAE latent distribution is (approximately) projected to 2D using tSNE and KDE.
Figure 4: Randomly sampled predictions of our CF-VAE + pR model on the Stanford Drone. Weobserve that our prediction are highly multi-modal and is reflected by the Conditional Flow Priors.
Figure 5: Comparison of our CF-VAE + pR (Red) and the “Shoutgun” baseline (Yellow) of (Pa-jouheshgar & Lampert, 2018), Groundtruth (Blue). Initial conditioning trajectory in white. OurCF-VAE not only learns to capture the correct modes but also generates more fine-grained predictions.
Figure 6: Comparison between conditional affine flows of (Atanov et al., 2019; Lu & Huang, 2019)and our conditional non-linear (Cond NL) flows. We see that the conditional affine flows cannot fullycapture multi-modal distributions (“tails” between modes), while our conditional non-linear flowsdoes not have distinctive “tails”.
Figure 7: Comparison between conditional affine flows of (Atanov et al., 2019; Lu & Huang, 2019)and our conditional non-linear (Cond NL) flows. We see that the conditional affine flows cannot fullycapture “ring”-like conditional distributions (note the discontinuity at the top), while our conditionalnon-linear flows does not have such discontinuities.
Figure 8: Comparison between our conditional non-linear (Cond NL) flows and a Mixture ofGaussians (MoG) model. We see that even with 64 mixture components, the learnt density is notsmooth in comparison to our conditional non-linear flows.
Figure 9: Random samples using the CDV Prior of (Klushyn et al., 2019) clustered using k-means.
Figure 10: Analysis of all four terms of our CF-VAE objective (7) at training time, with (C ={0.05, 0.10, 0.20}) and without our posterior regularization (pR) scheme. We observe better datalog-likelihoods and stable training with our posterior regularization (pR) scheme. Without pR, weobserve that the Jacobian term dominates at the cost of data log-likelihood.
Figure 11: Predictions on the HighD dataset. Left: 128 random samples from the HighD test set(in yellow). Middle: CVAE predictions (5 samples per test set example). Right: Our CV-VAE +{pR,cR} predictions (5 samples per test set example). While the predictions by the CVAE are linearcontinuations, our CF-VAE sample predictions are much more diverse and cover events like lanechanges e.g. top most sample track from the test set.
