Figure 1: Task-agnostic robust encodings confer robustness to adversarial perturbations. Clean inputsxa and xb (blue) can be perturbed to many different sentences, denoted by the sets B(xa) and B(xb).
Figure 2: Visualization of three different encodings. Vocabulary words (large font, blue) share anedge if they share a common perturbation (small font, red). We show three different encodings: thebold line corresponds to the encoding with maximal stability (connected component encoding), thedotted lines correspond to maximizing fidelity, and the thin solid lines balance stability and fidelity(agglomerative clusters).
Figure 3: (a) Histogram of ∣Bα(x)∣ for SST-2 and RTE. SST-2 has the highest percentage of inputs Xwhere ∣Bα(x)∣ = 1, while RTE has the least. On both datasets, ∣Bα(x)∣ < 9 for most x, and is often1. CONNCOMP encodings are more stable than the AGGCLUST encodings, at the cost of fidelity. (b)Standard and robust accuracies on SST-2 with agglomerative cluster encodings using different valuesof γ. While the gap between standard and robust accuracy increases monotonically, robust accuracyincreases before decreasing.
Figure 4: Robust accuracy averaged across all tasks based on different adversarial budgets b. b = 0corresponds to clean performance, and robust performance is reached at b = 4A.5 DatasetsWe use six out of the nine tasks from GLUE: SST, MRPC, QQP, MNLI, QNLI, and RTE, all of whichare classification tasks measured by accuracy. The Stanford Sentiment Treebank (SST-2) (Socheret al., 2013) contains movie reviews that are classified as positive and negative. The MicrosoftResearch Paraphrase Corpus (MRPC) (Dolan & Brockett, 2005) and the Quora Question Pairsdataset6 contain pairs of input which are classified as semantically equivalent or not; QQP containsquestion pairs from Quora, while MRPC contains pairs from online news sources. MNLI, andRTE are entailment tasks, where the goal is to predict whether or not a premise sentence entails ahypothesis (Williams et al., 2018). MNLI gathers premise sentences from ten different sources, whileRTE gathers premises from entailment challenges. QNLI gives pairs of sentences and questionsextracted from the Stanford Question Answering Dataset (Rajpurkar et al., 2016), and the task is topredict whether or not the answer to the question is in the sentence.
Figure 5: Histograms showing sizes of Bα for MRPC, QQP, MNLI, and QNLI.
