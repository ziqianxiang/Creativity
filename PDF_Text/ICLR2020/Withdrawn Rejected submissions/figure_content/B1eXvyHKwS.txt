Figure 1: Three types of datasets.
Figure 2: Gap between the direction of iterates and direction of hard margin classifier.
Figure 3: Adversarial and standard training on non-ε-strongly linearly separable but linearly sepa-rable data and non-linearly separable data.
Figure 4: The confidence distributions of adversarial training and standard training when data arelinearly separable but ε ≥ 1 max。∣∣xi — Xj ∣∣. The y-axis is number of data.
Figure 5: Gap between the direction of iterates and direction of hard margin classifier in l4 space.
Figure 6: Gap between the direction of iterates and direction of hard margin classifier in l∞ space.
Figure 7: Adversarial training in l4 space. Outliers are added into dataset of the first two subfig-ure. The third subfigure is confidence distribution for adversarial training when data are linearlyseparable but ε ≥ 2 max,/∣∣x, - Xj ∣∣. The y-axis in the third pictures is number of data.
Figure 8: Adversarial training in l∞ space. Outliers are added into dataset of the first two subfig-ure. The third subfigure is confidence distribution for adversarial training when data are linearlyseparable but ε ≥ 2 maxi,j ∣x, — Xj ∣∣. The y-axis in the third pictures is number of data.
Figure 9: Performance of adversarial training in l2 space(a) Adversarial training with different ε for (b) Frequency of confidence for data pointoutliers exist or not.	under adversarial training with an extremelylarge ε.
Figure 10: Performance of adversarial training in l∞ spaceThe steps of PGD is 10 for each xi. All the models are trained by stochastic gradient descent with0.1 learning rate and 0.9 momentum parameter for 100 epochs. The loss function is set to be crossentropy.
