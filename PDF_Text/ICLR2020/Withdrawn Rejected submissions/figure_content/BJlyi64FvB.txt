Figure 1: Activation atlases for the penultimate fully-connected layer of narrow and wide convo-lutional networks trained on MNIST. Top: 2D histograms of the output of UMAP applied to theactivation vectors, with color corresponding to class label. While both networks cluster the inputsaccording to class as expected, there are additional sub-clusters in wide network activations. Mid-dle: Activation atlases, indicating the representations learned by the networks. The sub-clustersobserved in the top plot for wide networks can be seen to correspond to digits with distinct features.
Figure 2: Activation atlases for the penultimate fully-connected layer of convolutional networks ofdifferent width trained on translated MNIST. Top: 2D histograms of the output of UMAP appliedto the activation vectors, with color corresponding to class label. Compared to MNIST (Figure 1),the additional structure due to the translations is clearly visible in the wide network but not in thenarrow one. Middle: Activation atlases. For the wide network, the location of the activations inan annulus for a given class is indicative of the degree of translation of the image. Bottom: Thefeatures that randomly selected individual neurons respond to. As in the case of MNIST, these aremore interpretable for the narrow network.
Figure 3: Activation atlases for the penultimate fully-connected layer of convolutional networks ofdifferent width trained on CIFAR-10. Top: 2D histograms of the output of UMAP applied to theactivation vectors, with color corresponding to class label. Middle: Activation atlases. For the widenetwork, there is a direction of large variance in the activation space corresponding to automobilecolor in the bottom left, and to left/right orientation of horses on the right. Bottom: The featuresthat randomly selected individual neurons respond to. As before, these are more interpretable forthe narrow network.
Figure 4: Wide networks learn features that are useful on novel tasks. We train networks withdifferent numbers of features at the penultimate layer on one task, then fine-tune the last layer on anovel task. Performance on the original task is essentially independent of width, but performance ofthe fine-tuned classifier improves dramatically with increased width. The results are the mean andstandard deviation over 5 initializations. Left: Translated MNIST (described in Section 4.2). Theoriginal task is shift classification, and the new task is digit classification. The networks are fullyconnected and the total number of parameters is held approximately constant (by making the narrownetworks deeper). Right: 3 coarse class CIFAR-100, convolutional networks. The original task isimage classification by 3 coarse classes (superclasses) of CIFAR-100. The new task is classificationby the fine classes of the same images. A linear classifier trained directly on the new task achievestest accuracy 37.5%.
Figure 5: Activation atlas for the penultimate fully-connected layer of an untrained convolutionalnetwork. The atlas images suggest that the hidden states that are activated by images of a similarcolor are adjacent to each other, and this correlates with the class label to an extent.
Figure 6: Activation atlas for MNIST digits. The class clusters are not as well separated as those ina trained network, and the atlas images are less interpretable.
