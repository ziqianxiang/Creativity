Figure 1: Goal conditioned supervised learning: We can learn how to reach goals by simply sam-pling trajectories, relabeling them to be optimal in hindsight and treating them as expert data, andthen performing supervised learning via behavior cloning.
Figure 2:	Evaluation Tasks: For each of the following tasks, we evaluate our algorithm using low-dimensional sensory data and pixel observations: (Left) 2D Navigation, (Center) robotic pushing,and (Right) Lunar Lander.
Figure 3:	State-based tasks: GCSL is competitive with state-of-the-art off-policy value functionRL algorithms for goal-reaching from low-dimensional sensory observations. Shaded regions denotethe standard deviation across 3 random seeds (lower is better).
Figure 4:	Image-based tasks: On three tasks with image observations, GSCL achieves similarperformance to a state-of-the-art baseline, TD3, while being substantially simpler. Shaded regionsdenote the standard deviation across 3 random seeds (lower is better).
Figure 5: Performance across variationsof GCSL (Section 5.3) for the pushingdomain. Plots for other domains in Ap-pendix A.4We investigate how the quality of the data in the datasetused to train the policy affects the learned policy. Weconsider two variations of GCSL: one which collects datausing a fixed policy (”Fixed Data Collection” in Figure5) and another which limits the size of the dataset to besmall, forcing all the data to be on-policy (”On-Policy” in Figure 5). When collecting data using afixed policy, the learning progress of the algorithm demonstratedly decreases, which indicates thatthe iterative loop of collecting data and training the policy is crucial for converging to a performantsolution. By forcing the data to be all on-policy, the algorithm cannot utilize the full set of expe-riences seen thus far and must discard data. Although this on-policy process remains effective onsimple domains, the technique leads to slower learning progress on tasks requiring more challengingcontrol.
Figure 6: Initializing from Demon-strations: GCSL is more amenable toinitializing using expert demonstrationsthan value-function RL methods.
Figure 7: Performance across variations of GCSL (Section 5.3) for all three experimental domains.
Figure 8: Examples of trajectories generated by GCSL for the Lunar Lander and 2D Room environ-ments. Stars indicate the goal state.
