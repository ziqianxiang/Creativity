Figure 1: It is convenient to think about different hashing algorithms in terms of representationalcontraction (large dimension of input is mapped into a smaller dimensional latent space), or expansion(large dimensional input is mapped into an even larger dimensional latent space). The projections canbe random or data driven.
Figure 2: (Panel A) Distribution of the hidden units (red circles) for a given distribution of the data(in one dimension). (Panel B) Arrangement of hidden units for the case of homogeneous distributionof the training data P = 1∕(2π). For hash length k = 2 only two hidden units are activated (filledcircles). If two data points are close to each other (x and y1) they elicit similar hash codes, if the twodata points are far away from each other (x and y3) - the hash codes are different.
Figure 3: Examples of queries and top 15 retrievals using BioHash (k = 16) on VGG16 fc7 featuresof CIFAR-10. Retrievals have a green (red) border if the image is in the same (different) semanticclass as the query image. We show some success (top 4) and failure (bottom 2) cases. However, itcan be seen that even the failure cases are reasonable.
Figure 4: Effect of varying sparsity (activity): optimal activity % for MNIST and CIFAR-10 are 5%and 0.25%. Since the improvement in performance is small from 0.5 % to 0.25 %, we use 0.5% forCIFAR-10 experiments. The change of activity is accomplished by changing m at fixed k.
Figure 5: tSNE embedding of MNIST as the activity is varied for a fixed m = 160 (the changeof activity is accomplished by changing k at fixed m). When the sparsity of activations decreases(activity increases), some clusters merge together, though highly dissimilar clusters (e.g. orange andblue in the lower left) stay separated.
