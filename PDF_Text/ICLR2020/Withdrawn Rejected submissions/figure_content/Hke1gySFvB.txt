Figure 1: Illustration of the referential game used as a testbed. The speaker agent takes the concept”cat” as input and forms a message to describe it. The listener agent takes the message and severalcandidate concepts as input, and decides which concept is the target seen by the speaker.
Figure 2: Integration of the auxiliary prediction task of the speaker into the neural network architec-ture.
Figure 3: Learning curve for the net with and without prediction task7	ConclusionUsing an auxiliary predictive task on a communication learning task has proven auspicious. Sample-efficiency is highly desirable when acquiring language, so the fact that our auxiliary task doublesthe learning speed is of high significance. Our experiments do only feature a small partition of thepotential of this elegant mechanism, yet. Higher sample-efficiency at no computational cost nowallows acquiring more complicated language tasks, that were previously impossible to learn in areasonable time. We plan to apply our algorithm to much more challenging tasks in the future. We5Under review as a conference paper at ICLR 2020did, for example, only test disentangled input due to computational limitations. The mechanismwould be even more useful when applied on entangled input because developing an expressive rep-resentation is then of higher importance. For future research, we propose the use of an auxiliaryprediction task for the listener to align with the word usage of the speaker, as well. We hope thatthis simple but powerful mechanism brings the field of language emergence a big step forward.
