Figure 1: Resolution scaling. (a) is a baseline network example. (b) are conventional scaling oninput resolution. (c) is our proposed resizable networks. It resizes the feature map resolution bya non-interger scaling ratio at the stage-level. A single network is trained with different scalingconfigurations with shared convolution. The imaginary line represents the original feature mapresolution, and the full line represents the actual feature map resolution after resize operation.
Figure 2: Fair Sampling. Left: Random Sampling. In each update step, We sample a sequence ofscaling factors at each stage. Right: Fair Sampling. In each update step, we sample a sequence ofscaling factors at each stage multiple times. The same scaling factor at the stage does not sampletwice in the same update step.
Figure 3: (a)-(b): ResiZable-NAS ResNet-50 and ShufleNetV2 compare with the individuallytrained networks and naive resizable approach. (c): Resizable-NAS ResNet-50 compare with state-of-the-art model compression methods. (d) Resizable-Adapt ResNet-50 compare with the Resizable-NAS ResNet-50. The horizontal line mean the FLOPs range of sub-network used in resizable-adaptnetwork for data-dependent inference. Models with larger target network achieve better accuracy.
Figure 4: Architecture of ResiZableNet. We highlight the input and output tensor shape. Convdenotes convolution layer. SF denotes ShuffleNetV2 module. SE denotes SqUeeZe-and-Excitationmodule.
Figure 5: Is network naturally resizable? We compare the result of sub-networks in naive Shuf-fleNetV2, RS-ShuffleNetV2 and RS-NAS-ShuffleNetVZD.2 SCALE-AWARE BN Analysis.
