Figure 1: Overview of our framework. Left block is a standard GAN with an attention embeddeddiscriminator. Mx is the attention map provided by the discriminator. TheL1 loss between generatedy0 and corresponding ground truth yi is computed. Right side is the framework for unsupervisedtranslation using cycle consistency. Ground truth yi is not available and the L1 loss between X andχ0 is calculated instead.
Figure 2: Different combination of attention and concatenation for apple2orange and sum-mer2winter. First column is the real input. From second column to the right: PHA and alphachannel, PHA and RHP, TAM and alpha channel, TAM and RHPMethodCycleGANStarGANAGGANUNIT(A)PPle6 (O)rangeA→O	O→A8.48 ± 0.5313.32 ± 0.5210.61 ± 0.7917.41 ± 1.135.94 ± 0.6511.19 ± 0.514.57 ± 0.307.26 ± 0.57Ours(PHA+RHP) 6.31 ± 0.60	2.99 ± 0.38(H)orse6(Z)ebra
Figure 3: Image-to-Image translation results generated by different approaches on apple2orangeand horse2zebra. Every two rows from top: apple→orange, orange→apple, zebra→horse,horse→zebra.
Figure 4: Image-to-Image translation results generated by different approaches on day2nightand summer2winter. Every two rows from top: night→day, day→night, winter→summer, sum-mer→winter.
Figure 5: Left: PatchGAN discriminator using TAM, the attention map is denoted as Ax ; Right:Patch discriminator using post hoc attention, the attention map Ax is computed from 4-th convlayer.
Figure 6: Different unsupervised translation methods for mapping labels-photos trained onCityscape images.
Figure 7: The statistic frequency for all 18 classes presented in the Cityscape dataset.
Figure 9: The average per-class attention map intensity in epochs 20, 40, 60.
Figure 10: Inputs, outputs and corresponding attention maps at training epoch 10. Left: attentionmap generated by the post hoc attention; Right: attention map generated by RAM attention mecha-nism.
Figure 11: Inputs, outputs and corresponding attention maps at training epoch 50. Left: attentionmap generated by the post hoc attention; Right: attention map generated by RAM attention mecha-nism.
Figure 12: Inputs, outputs and corresponding attention maps at training epoch 100. Left: atten-tion map generated by the post hoc attention; Right: attention map generated by RAM attentionmechanism.
Figure 13: Additional translation results on day2night dataset with default setting (PHA + RHP).
Figure 14: Additional translation results on apple2orange dataset with default setting (PHA + RHP).
Figure 15: Additional translation results on horse2zebra dataset using RAM + RHP. From left toright: real horse images, fake zebra images, real zebra images, fake horse images.
