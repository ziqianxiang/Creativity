Figure 1: A flowchart of the Transformer autoencoder. We first transcribe the .wav data files intoMIDI using the Onsets and Frames framework, then encode them into performance representationsto use as input. The output of the performance encoder is aggregated across time and (optionally)combined with a melody embedding to produce a representation of the entire performance, which isthen used by the Transformer decoder at inference time.
Figure 2: For the internal dataset, the relative distance from performance A (α = 1) to the interpo-lated sample increases as α is slowly increased to 1.0.
Figure 3: For the internal dataset, relative distance from performance A (α = 1) as α is slowlyincreased to 1.0 while the conditioned melody remains fixed. As in (b), we note that the relativedistance to the fixed conditioning melody with respect to a random performance remains fixed whilethe interpolation is conducted between performances A and B, which shows that we can control forelements of style and melody separately.
Figure 4: Results of our listening studies, showing the number of times each source won in a pairwisecomparison. Black error bars indicate estimated standard deviation of means.
Figure 5: The distance to the original performance increases as the value of α increases in (a), asexpected. In (b), we see that there is a very slight increase in the relative distance to the originalmelody during the interpolation procedure.
Figure 6: Interpolation of a starting performance (a) from the internal dataset to a final performance(h), with the coefficient α controlling the level of interpolation between the latent encodings betweenthe two performances.
Figure 7: Interpolation of a starting performance (b) from the internal dataset to a final performance(j), with the coefficient α controlling the level of interpolation between the latent encodings betweenthe two performances. The original conditioning melody (a) is kept fixed throughout the interpola-tion.
