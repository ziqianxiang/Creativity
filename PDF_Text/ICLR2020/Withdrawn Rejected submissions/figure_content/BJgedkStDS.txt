Figure 1: (a) Baseline Deep Neural Network (DNN). (b) DNN with Relevant features based Auxil-iary Cells (RACs) added at validation layers (selected hidden layers) whose output is monitored todetect early classificationThe training and construction of the linear classifiers is instrumental towards the accurate and ef-ficient error detection with our approach. We find that at a given hidden layer, the error detectioncapability (detecting natural errors) is higher if we use class-specific binary classifiers trained onthe corresponding relevant feature maps from the layer. In fact, using a fully connected classifiertrained on all feature maps (conventionally used in early exit techniques of Panda et al. (2016)) doesnot improve error detection capability. Training these binary classifiers on relevant features can beconsidered as encoding prior knowledge on the learned hidden feature maps, thereby, yielding betterdetection capability. Besides improved error detection, a key advantage of using class wise binary2Under review as a conference paper at ICLR 2020linear classifiers trained on only relevant features is that they incur less overhead in terms of totalnumber of parameters, as compared to a fully connected classifier trained on all feature maps.
Figure 2: (a) Normalised #OPS as the validation layers are shifted toWards the final classifier (b)True negative rate as the validation layers are shifted toWards the final classifier (c) False negativerate as the validation layers are shifted toWards the final classifier(c)ənφ>4e6ΘNφmlATNR ⅛FNRConfidence thresholdφleαφ>4e6φNφsreu-Confidence threshold(a)	(b)	(c)Figure 3: (a) TNR and FNR as the no. of relevant features k is increased at RACs (b) TNR andFNR as the confidence threshold δth is increased at RACs (c) Normalized #OPS as the confidencethreshold δth is increased at RACs.
Figure 3: (a) TNR and FNR as the no. of relevant features k is increased at RACs (b) TNR andFNR as the confidence threshold δth is increased at RACs (c) Normalized #OPS as the confidencethreshold δth is increased at RACs.
Figure 4: (a) Test error comparison between baseline DNN and DNN with RACs (b) NormalizedOPS benefits with respect to baseline5 ConclusionDeep neural networks are crucial for many classification tasks and require robust and energy efficientimplementations for critical applications. In this work, we device a novel post-hoc technique for en-ergy efficient detection of natural errors. In essence, our main idea is to append class-specific binarylinear classifiers at few selected hidden layers referred as Relevant features based Auxiliary Cells(RACs) which enables energy efficient detection of natural errors. With explainable techniques suchas Layerwise Relevance Propagation (LRP), we determine relevant hidden features corresponding toa particular class which are fed to the RACs. The consensus of RACs (and final classifier if there isno early termination) is used to detect natural errors and the confidence of RACs is utilized to decideon early classification. We also evaluate robustness of DNN with RACs towards adversarial inputsand out-of-distribution samples. Beyond the immediate application to increase robustness towardsnatural errors and reduce energy requirement, the success of our framework suggests further studyof energy efficient error detection mechanisms using hidden representations.
