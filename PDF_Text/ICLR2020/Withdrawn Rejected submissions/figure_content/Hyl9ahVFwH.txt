Figure 1: Example of field data from a fluid simulation of hot smoke with normalized distances fordifferent metrics. Our method (LNSM, in green) approximates the ground truth distances (GT, gray)determined by the data generation method best, i.e., version (a) is closer to the ground truth datathan (b). An L2 metric (red) erroneously yields a reversed ordering.
Figure 2:	Overview of the proposed distance computation for a simplified base network that con-tains three layers with four feature maps each in this example. The output shape for every operationis illustrated below the transitions in orange and white; bold operations are learned by the CNN.
Figure 3:	General data generation method from a PDE solver for a time dependent problem. Withincreasing changes of the initial conditions for a parameter pi in ∆i increments, the outputs decreasein similarity. Controlled Gaussian noise is injected in a simulation field of the solver. The difficultyof the learning task can be controlled by scaling ∆i as well as the noise strength s.
Figure 4: Performance on our test data for theproposed approach (LNSM) and a smaller model(AlexNetfrozen) using different loss functions.
Figure 5: Samples from our data sets. For each subset the reference is on the left, and three varia-tions in equal parameter steps follow. From left to right and top to bottom: Smo (density, velocity,and pressure), Adv (density), Liq (flags, velocity, and levelset), Bur (velocity), LiqN (velocity),AdvD (density), Sha and Vid.
Figure 6: Proposed base network architecture consisting of five layers with up to 192 feature mapsthat are decreasing in spatial size. It is similar to the feature extractor from AlexNet as identicalspatial dimensions for the feature maps are used, but it reduces the number of feature maps for eachlayer by 50% to have fewer weights.
Figure 7: Analysis of the distributions of learned feature map aggregation weights across the basenetwork layers. Displayed is our method for fully training the base network (left) in comparison tousing pre-trained weights (right).
Figure 8: Performance on our test data for theproposed approach (LNSM) and a smaller model(AlexNetfrozen) using different normalizations.
Figure 9: Adjusted distance computation for a LPIPS-based latent space difference. To providesufficiently large inputs for LPIPS, small feature maps are spatially enlarged with nearest neighborinterpolation. In addition, LPIPS creates scalar instead of spatial differences leading to a simplifiedaggregation.
Figure 10: Outputs from FlowNet2 on data examples. The flow streamlines are sparse visualizationof the resulting flow field and indicate the direction of the flow by their orientation and its magnitudeby their color (darker being larger). The two visualizations on the right show the dense flow fieldand are colorcoded to show the flow direction (blue/yellow: vertical, green/red: horizontal) and theflow magnitude (brighter being larger).
Figure 11:	Non-Siamese network architecture with the same feature extractor used in Fig. 6. It usesboth stacked inputs and directly predicts the final distance value from the last set of feature mapswith several fully connected layers.
Figure 12:	Network architecture with skip connections for better information transport betweenfeature maps. Transposed convolutions are used to upscale the feature maps in the second half ofthe network to match the spatial size of earlier layers for the skip connections.
Figure 13: Impact of increasing data difficulty for a reduced training data set. Evaluations ontraining data for L2 and LPIPS, and the test performance of models trained with the different reduceddata sets (LNSMreduced) are shown.
Figure 14: Various smoke simulation examples using one component of the velocity (top rows), thedensity (middle rows), and the pressure field (bottom rows).
Figure 15: Several liquid simulation examples using the binary indicator flags (top rows), the ex-trapolated levelset values (middle rows), and one component of the velocity field (bottom rows) forthe training data and only the velocity field for the test data.
Figure 16: Various examples from the Advection-Diffusion equation using the density field.
Figure 17: Different simulation examples from the Burger’s equation using the velocity field.
Figure 18: Examples from the shapes data set using a field with only binary shape values (first row),shape values with additional noise (second row), smoothed shape values (third row), and smoothedvalues with additional noise (fourth row).
Figure 19: Multiple examples from the video data set.
Figure 20: Examples from the TID2013 data set proposed by Ponomarenko et al. (2015). Displayedare a change of contrast, three types of noise, denoising, jpg2000 compression, and two color quan-tizations (from left to right and top to bottom).
Figure 21: Augmented samples from the training sets in groups with Smoke, Advection-DiffusionEquation, Burger’s Equation, and Liquid data. The upper row in each group shows the same ref-erence simulation, and the lower row contains variations with different ground truth distances (in-creasing from left to right).
Figure 22:	Distribution evaluation of ground truth distances against normalized predicted distancesfor L2 , LPIPS and LNSM on all test data (color coded).
Figure 23:	Mean and standard deviation of normalized distances over multiple data samples forL2 and LNSM. The samples differ by the quantity displayed in brackets. Each data sample uses 200parameter variation steps instead of 10 like the others in our data sets. For the shape data the positionof the shape varies and for the liquid data the gravity in x-direction is adjusted.
