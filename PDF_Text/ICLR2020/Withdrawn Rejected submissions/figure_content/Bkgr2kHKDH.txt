Figure 1: An overview of our end-to-end design methodology. We first train an accuracy predictorfor the full precision NN, then incrementally train an accuracy predictor for the quantized NN(predictor-transfer). Finally, evolutionary search is performed to find the specialized nN architecturethat fits hardware constraints.
Figure 2: Predictor-transfer technique. Westart from a pre-trained full-precision predic-tor and add another input head (green squareat bottom right) denoting quantization policy.
Figure 3: Comparison with mixed-precision models searched by HAQ (Wang et al., 2019) underlatency/energy constraints. When the constraint is strict, our model can outperform fixed precisionmodel by more than 10% accuracy, and 5% compared with HAQ. Such performance boost maybenefit from the dynamic architecture search space rather than fixed one as MobileNetV2.
Figure 5: Comparison with quantized model underBitOps constraint. Our model achieves 0.5% accu-racy boost (from 74.6% to 75.1%) at half BitOpscompared to single path one-shot. Also note thataccuracy of our model is the same level as floatversion ResNet-34 model (75.0%).
Figure 4: Comparison with sequentially designedmixed-precision models searched by AMC andHAQ (Cai et al., 2019b; He et al., 2018; Wang et al.,2019) under latency constraints. Our end-to-enddesigned model while achieving better accuracythan sequentially designed models.
Figure 6: Illustration of the performance w/ or w/o predictor-transfer technique. Pairwise accuracy isa metric that measures the relative relationship between each two architectures. Left graph shows thatthe quantization-aware predictor could attain a faster and higher convergence compared w/o transfer.
