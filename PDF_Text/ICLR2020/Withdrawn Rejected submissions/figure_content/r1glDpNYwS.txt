Figure 1: This graph illustrates the importance of the imperceptibility of adversarial samples in bothimage space and label space. Triangles are three attackers. Circles represent human observers.
Figure 2: Pipeline of our method. There are two steps. In the first step, we first compute thedistance Di,j between every two class i, j in a n-classes dataset. Then we choose the target label txfor an input image X by two strategies according to the value of p^ι. The second step is to attackthe input into this target label. Solid lines are the real boundaries between the current label and theindicated label and dashed lines with notes F are the approximate two-dimensional boundaries. Redindicates the target label while blue indicates other labels. Our method moves the input towards theboundary Ftx until it is classified as tx .
Figure 3: A line chart for average performance gain of 10 observers. The horizontal axis representsfour attack methods. The vertical axis represents the mean value of 10 human observers’ perfor-mance gain. The graph in the left is for total results, and the right one is for animal images.
Figure 4: Mean value of perceptibility, perceptual similarity and PieAPP for adversarial samplesgenerated by different attack methods on different models.
Figure 5: Two visual results of adversarial samples generated from AlexNet in the image space.
Figure 6:	Interface presentation of our subjective experiment.
Figure 7:	Three examples for animal images. The first column shows the clean image. The secondcolumn shows the ground truth label and other columns show the label after attacked.
Figure 8: The line chart in the right part shows the top 10 elements in p^, When an image X whoseground truth class is class 2 is given into the classifier VGG_19bn. The horizontal axis representsthe class and the vertical axis represents the probability that x belongs to this class. The predictedclass lx is class 3 and it means the classifier fail to give a correct classification.
Figure 9: Some other examples for illustrating what LabelFool does. The first column shows theclean image. The second column shows the ground truth label and other columns show the labelafter attacked.
Figure 10: A is the person who is using a face system to enter the gate. Green lines represent whatLabelFool aims to do, that is to miscalssify A and B who looks like A. Red lines represent whatother untargeted attacks do. They misclassify A and C who looks totally different from A and thiserror is easy to be detected by the guard.
