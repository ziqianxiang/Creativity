Figure 1: The "simplicity bias" is not so simple. (a) Following Valle-Perez et al. (2018), Wesample 104 boolean functions {±1}7 → {±1} as follows: for each combination of nonlinearity,weight variance σw2 , and bias variance σb2 (as used in Eq. (MLP)), we randomly initialize a networkof 2 hidden layers, 40 neurons each. Then we threshold the function output to a boolean output,and obtain a boolean function sample. We repeat this for 104 random seeds to obtain all samples.
Figure 2: The depth maximizing degree k fractional variance increases with k for both reluand erf. For relu (a) and erf (b), we plot for each degree k the depth such that there exists somecombination of other hyperparameters (such as σb2 or σw2 ) that maximizes the degree k fractionalvariance. For both relu, σb2 = 0 maximizes fractional variance in general, and same holds for erfin the odd degrees (see Appendix D), so we take a closer look at this slice by plotting heatmaps offractional variance of various degrees versus depth for relu (c) and erf (d) NTK, with bright colorsrepresenting high variance. Clearly, we see the brightest region of each column, corresponding to afixed degree, moves up as we increase the degree, barring for the even/odd degree alternating patternfor erf NTK. The pattern for CKs are similar and their plots are omitted.
Figure 3: (a) We train relu networks of different depths against a ground truth polynomial on ㈤128of different degrees k . We either train only the last layer (marked “ck”) or all layers (marked “ntk”),and plot the degree k fractional variance of the corresponding kernel against the best validation lossover the course of training. We see that the best validation loss is in general inversely correlated withfraction variance, as expected. However, their precise relationship seems to change depending on thedegree, or whether training all layers or just the last. See Appendix E for experimental details. (b)Same experimental setting as (a), with slightly different hyperparameters, and plotting depth againstbest validation loss (solid curves), as well as the corresponding kernel’s (1- fractional variance)(dashed curves). We see that the loss-minimizing depth increases with the degree, as predicted byFig. 2. Note that we do not expect the dashed and solid curves to match up, just that they are positivelycorrelated as shown by (a). In higher degrees, the losses are high across all depths, and the variance islarge, so we omit them. See Appendix E for experimental details. (c) Similar experimental setting as(a), but with more hyperparameters, and now comparing training last layer vs training all layers. Thecolor of each dot indicates the degree of the ground truth polynomial. Below the identity line, trainingall layers is better than training last layer. We see that the only nontrivial case where this is not trueis when learning degree 0 polynomials, i.e. constant functions. See Appendix E for experimentaldetails. We also replicate (b) for MNIST and CIFAR10, and moreover both (b) and (c) over the inputdistributions of standard Gaussian and the uniform measure over the sphere. See Figs. 6 to 8.
Figure 4: Across nonlinearities and hyperparameters, NTK tends to have higher fraction ofvariance attributed to higher degrees than CK. In (a), we give several examples of the fractionalvariance curves for relu CK and NTK across several representative hyperparameters. In (b), we do thesame for erf CK and NTK. In both cases, we clearly see that, while for degree 0 or 1, the fractionalvariance is typically higher for CK, the reverse is true for larger degrees. In (c), for each degree k, weplot the fraction of hyperparameters where the degree k fractional variance of NTK is greater thanthat of CK. Consistent with previous observations, this fraction increases with the degree.
Figure 5: Spectral theory of CK and NTK over boolean cube predicts max learning rate forSGD over real datasets MNIST and CIFAR10 as well as over boolean cube ㈤128, the sphere√128S128-1, and the standard Gaussian N(0, I128). In all three plots, for different depth, nonlin-earity, σw2 , σb2 of the MLP, we obtain its maximal nondiverging learning rate (“max learning rate”)via binary search. We center and normalize each image of MNIST and CIFAR10 to the √dSd-1sphere, where d = 282 = 784 for MNIST and d = 3 × 322 = 3072 for CIFAR10. See Appendix E.2for more details. (a) We empirically find max learning rate for training only the last layer of anMLP. Theoretically, We predict 1 /Φ(0) where Φ corresponds to the CK of the MLP. We see that ourtheoretical prediction is highly accurate. Note that the Gaussian and Sphere points in the scatter plotcoincide with and hide behind the BoolCube points. (b) and (c) We empirically find max learningrate for training all layers. Theoretically, we predict 1∕Φ(0) where Φ corresponds to the NTK of theMLP. The points are identical between (b) and (c), but the color coding is different. Note that theGaussian points in the scatter plots coincide with and hide behind the Sphere points. In (b) we seethat our theoretical prediction when training all layers is not as accurate as when we train only the lastlayer, but it is still highly correlated with the empirical max learning rate. It in general underpredicts,so that half of the theoretical learning rate should always have SGD converge. This is expected,since the NTK limit of training dynamics is only exact in the large width limit, and larger learningrate just means the training dynamics diverges from the NTK regime, but not necessarily that thetraining diverges. In (c), we see that deeper networks tend to accept higher learning rate than ourtheoretical prediction. If we were to preprocess MNIST and CIFAR10 differently, then our theory is
Figure 6: Optimal depths exist over the standard Gaussian N (0, Id) and the uniform distribu-tion over the sphere √dSd-1 as well. Here We use the exact same experimental setup as Fig. 3(b)(see Appendix E for details) except that the input distribution is changed from uniform over theboolean cube ≤Pd to standard Gaussian N(0, Id) (solid lines) and uniform over the sphere √dSd-1(dashed lines), Where d = 128. We also compare against the results over the boolean cube fromFig. 3(b), Which are draWn With dotted lines. Colors indicate the degrees of the ground truth poly-nomial functions. The best validation loss for degree 0 to 2 are all very close no matter Whichdistribution the input is sampled from, such that the curves all sit on top of each other. For degree 3,there is less precise agreement betWeen the validation loss over the different distributions, but theoverall trend is unmistakably the same. We see that for netWorks deeper or shalloWer than the optimaldepth, the loss monotonically increases as the depth moves aWay from the optimum.
Figure 7: Optimal depths exist over realistic distributions of MNIST and CIFAR10. Here, wetrained relu networks with σw2 = 2, σb2 = 0 for all depths from 0 to 10. We used SGD with learningrate 10 and batch size 256, and trained until convergence. We record the best test error throughout thetraining procedure for each depth. For each configuration, we repeat the randomly initialization andtraining for 10 random seeds to estimate the variance of the best test error. The rows demonstrate thebest test error over the course of training on CIFAR10 and MNIST, and the columns demonstratethe same for training only the last layer or training all layers. As one can see, the best depth whentraining only the last layer is 1, for both CIFAR10 and MNIST. The best depth when training alllayers is around 5 for both CIFAR10 and MNIST. Performance monotically decreases for networksshallower or deeper than the optimal depth. Note that we have reached the SOTA accuracy for MLPsreported in Lee et al. (2018) on CIFAR10, and within 1 point of their accuracy on MNIST.
Figure 8: Just like over the boolean cube: Over the sphere √dSd-1 and the standard GaussianN(0, Id), training only the last layer is better for learning low degree polynomials, but trainingall layers is better for learning high degree polynomials. Here we use the exact same experimentalsetup as Fig. 3(c) (see Appendix E for details) except that the input distribution is changed fromuniform over the boolean cube ㈤d to standard Gaussian N(0, Id) (left) and uniform over the sphere√dSd-1 (right), where d = 128.
Figure 9: We perform binary search for the empirical max learning rate of MLPs of different depth,activations, σw2 , and σb2 on MNIST and CIFAR10 preprocessed in different ways. See Appendix E.2for experimental details. The first row compares the theoretical and empirical max learning rateswhen training only the last layer. The second row compares the same when training all layers (underNTK parametrization (Eq. (MLP))). The three columns correspond to the different preprocessingprocedures: no preprocessing, PCA projection to the first 128 components (PCA128), and ZCAprojection to the first 128 components (ZCA128). In general, the theoretical prediction is less accurate(compared to preprocessing by centering and projecting to the sphere, as in Fig. 5), though still wellcorrelated with the empirical max learning rate. The most blatant caveat is the relu networks trainedon PCA128- and ZCA128-processed CIFAR10.
Figure 10:	2D contour plots of how fractional variance of each degree varies with σb2 anddepth, fixing σw2 = 2, for relu CK and NTK. For each degree k, and for each selected fractionalvariance value, we plot the level curve of (depth, σb2) achieving this value. The color indicates thefractional variance, as given in the color bars.
Figure 11:	2D contour plots of how fractional variance of each degree varies with σw2 anddepth, for different slices of σb2, for erf NTK. These plots essentially show slices of the NTK 3Dcontour plots in Fig. 13. For σb = 0, μk for all even degrees k are 0, so We omit the plots. Note therapid change in the shape of the contours for odd degrees, going from σb2 = 0 to σb2 = 0.1. This isreflected in Fig. 13 as Well.
Figure 12:	2D contour plots of how fractional variance of each degree varies with σw2 anddepth, for different slices of σb2 , for erf CK. These plots essentially show slices of the CK 3Dcontour plots in Fig. 13. For σb = 0, μk for all even degrees k are 0, so We omit the plots. Note therapid change in the shape of the contours for odd degrees, going from σb2 = 0 to σb2 = 0.1. This isreflected in Fig. 13 as Well.
Figure 13: 3D contour plots of how fractional variance of each degree varies with σw2 , σb2 andlog2(depth), for erf CK and NTK. For each value of fractional variance, as given in the legend onthe right, we plot the level surface in the (σw2 , σb2 , log2 (depth))-space achieving this value in thecorresponding color. The closer to blue the color, the higher the value. Note that the contour for thehighest values in higher degree plots “floats in mid-air”, implying that there is an optimal depth forlearning features of that degree that is not particularly small nor particularly big.
Figure 15: Examples of Gegenbauer Polynomials for d = 128 (or α = 63).
Figure 16: Visualization of a zonal harmonic, which depends only on the “height” of the input alonga fixed axis. Color indicates function value.
Figure 17: In high dimension d, Φd as defined in Eq. (20) approximates Φ very well. We showfor 3 different erf kernels that the function Φ defining K as an integral operator on the sphere iswell-approximated by Φd when d is moderately large (d ≥ 32 seems to work well). This suggeststhat the eigenvalues of K as an integral operator on the standard Gaussian should approximate thoseof K as an integral operator on the sphere.
Figure 18: In high dimension d, the eigenvalues are very close for the kernel over the booleancube, the sphere, and standard Gaussian. We plot the eigenvalues μk of the erf CK, with σW =2, σb2 = 0.001, depth 2, over the boolean cube, the sphere, as well as kernel on the sphere induced byΦd (Eq. (20)). We do so for each degree k ≤ 5 and for dimensions d = 16, 32, 64,128. We see thatby dimension d = 128, the eigenvalues shown are already very close to each other.
Figure 19: The same experiments as Fig. 1 but over {0, 1}7.
