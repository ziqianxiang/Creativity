Figure 1:  Improved gradient estimation.  (a) The gradients before and after a parameter updateare highly correlated.  The cosine between two consecutive gradients (blue line) and the cosinebetween two random vectors (green line) are plotted. (b) A network is trained with parameter updatesaccording to gES using SGD (blue line) and Adam (yellow line) as optimizers.  At any step wecompute our gradient estimate with gₒur and the true gradient     f with backpropagation. The plotshows that the ratio of the cosine between gₒur and     f  and the cosine between gES and     f  isalways strictly larger than 1.
Figure 2: Performance of ES (red lines) and our algorithm (blue lines) on MNIST classification. Theevolution of the training log-likelihood is plotted for the best three learning rates found for ES whenusing the Adam optimizer (top) and SGD (bottom). Our algorithm uses the same learning rates andhyper-parameters as ES.
Figure 3: Using several past descent directions improves robustness to function evaluation noise. Theplots show the performance on MNIST digit classification task with (blue lines) and without (redlines) function evaluation noise. The noise is created artificially by randomly permuting the fitnessvalues of the evaluated search directions (see Equation 2) in 20% of parameter updates. The x-axisrepresents the number of proper (i.e. non-permuted) parameter updates. (a) Standard ES does notsuffer from this.  (b) Function evaluation noise heavily impairs learning for our iterative gradientestimation scheme when using one past update direction as surrogate gradient.  (c) Using 4 pastupdate directions as surrogate gradients makes our iterative gradient estimation scheme robust tofunction evaluation noise.
Figure 4: Performance of our algorithm (red line) and ES (blue line) on three different Roboschooltasks: Ant (left), Cheetah (center) and Pendulum (right). The plot shows the mean average rewardover 9 repetitions as a function of time-steps (in thousands).
Figure 5: Performance when optimizing a quadratic function. Performances of our approach (green),standard ES (blue) and SNES (red). (a) On small dimensional functions (N  = 32 and 16 sampleddirections per time step) SNES outperforms our approach and standard ES. Our approach and ESdo     not improve further because they do not adapt their stepsize. (b) For high dimensional functions(N  = 10000 and 16 sampled directions per time step), our approach outperforms SNES and standardES.
Figure 6:  Optimizing a quadratic function with biased surrogate gradients.  Performances of ourapproach (green line), SGD using the surrugate gradient for the parameter update (purple line), ES(blue line), guided-ES (yellow line) are plotted. Our approach (green line) improves sharply, makinguse of the biased gradient, until it becomes useless (crossing with purple line).  For guided-ES,we observed a binary behaviour when optimizing for the parameter that controls the bias-variancetrade-off analysed in (14). Either it follows the surrogate gradient very closely or it behaves similarlyto ES. This observation is consistent with the analysis conducted in (14). The optimal value that wefound for this parameter is very close to 1 and results in a performance very close to ES (yellow andblue lines).
