Figure 1: Example of distributions of the BERT scores when fooling sentences are foundFigure 2: Example of distributions of the BERT scores when fooling sentences are not foundAfter running the fooling algorithm on the same initial BERT model n times, we collect all thesentences that obtain a BERT score greater than a fixed threshold of 0.98 and we merge those asnegative samples to the original CoLA dataset, obtaining an augmented dataset. Finally, we finetunethe original pre-trained BERT model using this new augmented dataset. We call the obtained modelImproved BERT in the following sections and we check the performance of the new model calcu-lating the score through the official GLUE benchmark leaderboard, obtaining a result similar to theoriginal score.
Figure 2: Example of distributions of the BERT scores when fooling sentences are not foundAfter running the fooling algorithm on the same initial BERT model n times, we collect all thesentences that obtain a BERT score greater than a fixed threshold of 0.98 and we merge those asnegative samples to the original CoLA dataset, obtaining an augmented dataset. Finally, we finetunethe original pre-trained BERT model using this new augmented dataset. We call the obtained modelImproved BERT in the following sections and we check the performance of the new model calcu-lating the score through the official GLUE benchmark leaderboard, obtaining a result similar to theoriginal score.
Figure 3: Example of distributions of the improved BERT scores when fooling sentences are notfound6.5	Robustness ExperimentsNow we investigate the robustness of the models, to understand if the fooling datasets are universalor not, performing the two following experiments.
