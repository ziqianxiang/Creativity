Figure 1: Interpretation (L) of benign (x) and adversarial (x0) image from restricted ImageNet (Engstromet al., 2019) with respect to the true label t=‘monkey’ and the target label t0=‘fish’. Here the adversarialexample is generated by 10-step PGD attack with perturbation size 0.02 (Madry et al., 2018), and we considerthree types of interpretation maps, CAM, GradCAM++ and IG. Given an interpretation method, the first columnis L(x, t) versus L(x0, t), the second column is L(x, t0) versus L(x0, t0), and all maps under each category arenormalized w.r.t. their largest value. At the bottom of each column, we quantify the resulting interpretabilitydiscrepancy by Kendall’s Tau order rank correlation (Selvaraju et al., 2017) between every pair of L(x, i) andL(x0, i) for i = t or t0.
Figure 2: Seeing the effect of discrepancy measure on hiding adversarial examples from network interpreta-tion. The same benign image in Figure 1 is considered. (a) ISA using CAM-based `1 1/2/all-class discrepancymeasure versus perturbation size , (b) ISA using CAM-based squared `2 1/2/all-class discrepancy measureversus , (c) CAM interpretation of example in Figure 1 and its adversarial counterparts from PGD attack anddifferent specifications of ISA. All interpretation maps are normalized w.r.t. their largest value. At the bottomof each interpretation map L(x0, ∙), We quantify the interpretability discrepancy by Kendall's TaU order rankcorrelation between every pair of L(x0, i) and L(x, i) for i ∈ {t, t0}, where x0 is obtained from PGD attack oreach specification of ISA.
Figure 3: Computation time per epoch and ad-versarial test accuracy for a Small MNIST modeltrained with different methods.
Figure A1: Clean test accuracy and adversarial test accuracy for a Small MNIST model trained withInt using different values of regularization parameter γ .
