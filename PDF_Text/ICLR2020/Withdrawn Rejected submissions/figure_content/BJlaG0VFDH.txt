Figure 1: Distribution of weight noise after low-rank approximation via SVD (Left) or quantization(Right) when R is the rank and Q is the number of quantization bits.
Figure 2: Gradient descent and weight regularization when NR period is given as a multiple ofbatches. Depending on the loss surface and/or strength of regularization, regularization would leadto step 2 (escaping from a local minimum) or step 5 (returning to a local minimum).
Figure 3:	Model accuracy of ResNet-32 on CIFAR-10 using various NR period and amount ofweight decay or noise for regularization (original model accuracy without regularization is 92.6%).
Figure 4:	Perplexity of LSTM model on PTB dataset using various NR period and amounts of weightdecay or noise. (Left): Weight decay. (Right): Uniform weight noise insertion.
Figure 5: Model accuracy of an LSTM model on PTB compressed by quantization, low-rank ap-proximation or pruning. Original perplexity without model compression is 114.6. For more details,refer to Figure 14.
Figure 6: Relationship between model accuracy and error (defined as the difference betweenew/pNR and eopt) using PTB LSTM model when weights are regularized by weight decay, SVD,or pruning.
Figure 7: Test accuracy comparison on ResNet-32 using CIFAR-10 trained by typical trainingmethod and the proposed training method with various compression ratios. For the proposed scheme,test accuracy is measured only at Step 3 that allows to extract a decomposed structure, and pNR is200.
Figure 8: Training loss and test accuracy of ResNet-32 using CIFAR-10. For the proposed scheme,training loss and test accuracy are only monitored right before or after weight regularization forcompression (pre-regularization or post-regularization). Compression ratio is 2.8 with Rc=0.5.
Figure 9: Model accuracy of ResNet-32 on CIFAR-10 using various NR period and amount ofweight regularization (original model accuracy without regularization is 92.6%). (Left): Weightdecay. (Right): Uniform weight noise insertion.
Figure 10: Model accuracy (%) of ResNet-32 for various weight decay factors and batch size whenpNR=1. Large batch size demands larger weight decay factors that is also reported by Loshchilov& Hutter (2017).
Figure 11: Perplexity of LSTM model on PTB dataset using various NR period and amounts ofweight decay (original perplexity without regularization is 114.60).
Figure 12:	Perplexity of LSTM model on PTB dataset using various NR period and amounts ofuniform noise (original perplexity without regularization is 114.60).
Figure 13:	Training and test accuracy of ResNet-32 on CIFAR-10 for variuos NR period and theamount of weight regularization. (Left): Weight decay. (Right): Uniform weight noise.
Figure 14: Relationship between test perplexity and pNR using PTB LSTM model.
Figure 15: Weight distribution of LSTM layer 1 of the medium PTB model after retraining with(Left) a magnitude-based pruning and (Right) pNR-based pruning with 90% pruning rate. Ourcompression scheme incurs a sharp drop in the count of near-zero weights.
Figure 16: Difference of training loss function and average Frobenius norm of weight values by Step2 and Step 3 of Figure 2. Rc = 0.5 and pNR = 200 are used.
Figure 17: Skewed matrix and a tiling technique are illustrated on the left side, while the right sidepresents distributions of weights after SVD with different tiling schemes (only positive weights areincluded).
Figure 18: Test accuracy of ResNet-32 model using CIFAR-10 with various target compression ratioand decomposition methods. Except the first small convolution layer, all layers are compressed bythe same compression ratio. Convolution layers can be grouped according to 3 different S values(=16, 32, or 64). For tiled SVD, three groups (of different S) are tiled in (k1 × k1), (k2 × k2), or(k3 × k3) tile size. (k1, k2, k3) configuration is described in legends.
Figure 19: Comparison of two compression schemes on training loss and (top-1) test accuracy ofResNet-34 model using ImageNet. pNR=500.
Figure 20: An example of lowering technique using im2col.
