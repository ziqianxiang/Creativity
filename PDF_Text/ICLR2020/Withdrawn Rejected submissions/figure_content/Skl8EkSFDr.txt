Figure 1: Various approaches to compress StarGAN with network pruning. Each group shows oneinput face translated with different methods of compressing the network: a. Uncompressed, b.
Figure 3: Image synthesis on MNIST dataset with DCGAN. Columns 1-3: Handwritten numbersgenerated by the original generator, pruned generator of 50%, 75% fine-grained sparsity.
Figure 4: Representative results for domain translation: pix2pix.
Figure 5: Representative results for style transfer: CycleGAN.
Figure 6: Representative image-to-image translation results: CycleGAN.
Figure 8: Representative results for pruning rate and granularity study of style transfer.
Figure 9: Loss curves of image-to-image translation pruning. (a). Loss curve of StarGAN baseline.
Figure 10: Example 1 of various approaches to compress StarGAN.
Figure 11: Example 2 of various approaches to compress StarGAN.
Figure 12: Example 3 of various approaches to compress StarGAN.
Figure 13: Image synthesis on MNIST dataset with DCGAN pruned to 50% with fine-grained spar-sity. Column 1: Handwritten numbers generated by the original generator, 2: Handwritten numbersgenerated by the generator pruned with our method, 3: Handwritten numbers generated by thepruned generator with traditional knowledge distillation adapted for GANs (Aguinaldo et al., 2019).
Figure 14: Image synthesis on MNIST dataset with DCGAN of 75% fine-grained sparsity. Column1: Handwritten numbers generated by the original generator, 2: Handwritten numbers generated bythe generator pruned with our method, Column 3: Handwritten numbers generated by the prunedgenerator with traditional knowledge distillation adapted for GANs.
Figure 15: Image synthesis: from label maps to fake satellite images. Row 1: Original label maps,Row 2: Satellite images generated by the original generator, Row 3: Satellite images generated bythe pruned generator, Row 4: Residual difference between generated images in Row 2 and 3, Row5: Residuals amplified by 10x.
Figure 16: Image synthesis: Two different random seeds, unpruned. Row 1: Original label maps,Rows 2-3: Generated fake satellite images by original generator trained with random seeds 15 and63, Row 4: Residual difference between generated images in Row 2 and 3. Row 5: Residualsamplified by 10x for higher contrast.
Figure 17: Image synthesis: from satellite images to fake label maps. Row 1: Original satelliteimages, Row 2: Label maps generated by the original generator, Row 3: Label maps generated bythe pruned generator, Row 4: Residual difference between generated images in Row 2 and 3, Row5: Residuals amplified by 10x.
Figure 18: Image synthesis: Two different random seeds, unpruned. Row 1: Original satelliteimages, Rows 2-3: Generated fake label maps by original generator trained with random seeds 15and 63, Row 4: Residual difference between generated images in Row 2 and 3. Row 5: Residualsamplified by 10x for higher contrast.
Figure 19: Style transfer: from Monet to real photo style. Row 1: Original artwork images fromMonet, Row 2: photographic style applied by the original generator, Row 3: photographic styleapplied by the compressed generator, Row 4: Residual difference between style transferred imagesin Row 2 and 3, amplified by 10x.
Figure 21: Image-to-image translation experiment: from real zebra images to fake horse images.
Figure 23: Image-to-image translation example 1: facial attribute translation. Columns: 1. Originalfacial images, 2-4. Translated images to (black, blond, brown) hair colors, 5. Translated images toother gender, 6. Translated images to other age. Rows: Images translated by 1. original generatorand 2. compressed generator, 3. Residual difference between Rows 1 and 2, 4. Residuals amplifiedby 10x.
Figure 24: Image-to-image translation example 2: facial attribute translation. Columns: 1. Originalfacial images, 2-4. Translated images to (black, blond, brown) hair colors, 5. Translated images toother gender, 6. Translated images to other age. Rows: Images translated by 1. original generatorand 2. compressed generator, 3. Residual difference between Rows 1 and 2, 4. Residuals amplifiedby 10x.
Figure 25: Image-to-image translation example 3: facial attribute translation. Columns: 1. Originalfacial images, 2-4. Translated images to (black, blond, brown) hair colors, 5. Translated images toother gender, 6. Translated images to other age. Rows: Images translated by 1. original generatorand 2. compressed generator, 3. Residual difference between Rows 1 and 2, 4. Residuals amplifiedby 10x.
Figure 26: Image-to-image translation example 4: facial attribute translation. Columns: 1. Originalfacial images, 2-4. Translated images to (black, blond, brown) hair colors, 5. Translated images toother gender, 6. Translated images to other age. Rows: Images translated by 1. original generatorand 2. compressed generator, 3. Residual difference between Rows 1 and 2, 4. Residuals amplifiedby 10x.
Figure 27: Super resolution experiment. Column 1: Original high resolution images, Columns2-4: Corresponding generated real high resolution images by original, filter-compressed, element-compressed generators. Each second row provides a detailed view of boxed regions.
Figure 29: Domain translation: fine-grained pruning to different sparsity levels. RoW 1: Output ofthe baseline generator. Rows 2-5: Synthesized satellite images by generators pruned to sparsities of25%, 50%, 75%, 90%.
Figure 37:Image-to-image translation: fine-grained pruning to different sparsities. Row 1: Baselinegenerator output. Rows 2-5: Generated real photo style images by generators pruned to sparsities of25%, 50%, 75%, 90%.
Figure 38: Image-to-image translation example 1: filter pruning to different sparsities. Row 1:Baseline generator output. Rows 2-5: Facial attribute translated images by generators pruned tosparsities of 25%, 50%, 75%, 90%.
Figure 39: Image-to-image translation example 1: fine-grained pruning to different sparsities. Row1: Baseline generator output. Rows 2-5: Facial attribute translated images by generators pruned tosparsities of 25%, 50%, 75%, 90%.
Figure 40: Image-to-image translation example 2: filter pruning to different sparsities. Row 1:Baseline generator output. Rows 2-5: Facial attribute translated images by generators pruned tosparsities of 25%, 50%, 75%, 90%.
Figure 41: Image-to-image translation example 2: fine-grained pruning to different sparsities. Row1: Baseline generator output. Rows 2-5: Facial attribute translated images by generators pruned tosparsities of 25%, 50%, 75%, 90%.
Figure 42: Super resolution: fine-grained pruning to different sparsities. Columns 1-4: Correspond-ing generated real high resolution images by generators pruned to sparsities of 25%, 50%, 75%,90%.
Figure 43: Loss curves of image-to-image translation experiments of filter pruning to different spar-sities. (a)-(d): Corresponding loss curve of the generator pruned to sparsities of 25%, 50%, 75%,90%.
Figure 44: Loss curves of image-to-image translation experiments of fine-grained pruning to differ-ent sparsities. (a)-(d): Corresponding loss curve of the generator pruned to sparsities of 25%, 50%,75%, 90%.
