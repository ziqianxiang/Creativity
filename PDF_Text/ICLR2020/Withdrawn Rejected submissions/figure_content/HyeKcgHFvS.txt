Figure 1: Two possibilities for simplifying GMMs, illustrated for a dataset with two clusters andtwo Gaussian components. Green dots represent data samples and arrows represent normalized localprincipal directions along which variances (represented by ellipses) are computed. Left: diagonalcovariance matrix, where it is assumed that clusters vary only in the coordinate directions (notthe case here). Right: non-diagonal covariance matrix where only a limited number (here one) ofprincipal directions is considered for variance computation.
Figure 2:	Exemplary results for learned centroids, using normal experimental conditions, trained onvariants of the two datasets (from left to right): MNIST, MNIST patches, SVHN, SVHN patches.
Figure 3:	Comparing the max-component approximation to the negative log-likelihood during SGDtraining, shown for MNIST (left), MNIST patches (center) and SVHN patches (right).
Figure 4:	Effects of annealing on SGD convergence to centroids on MNIST. Left: single-componentsolution (no annealing). Right: regular solution (with annealing).
Figure 5: Comparing different initialization ranges for centroids: [-0.5, 0.5] (left), [-0.7, 0.7](center) and [-1, 1] (right). The orange line represents the same value in all diagrams, the seemingdifference is due to scaling as initial loss values in some experiments are much higher (i.e., worse).
Figure 6: Comparing different values of S when training GMMs with the local principal directionssimplification. Shown are centroids for S = 25 (left), S = 50 (middle) and S = 100 (right).
Figure 7: Visualization of prototypes training on different datasets (from left to right): FashionM-NIST (15000 iterations), NotMNIST (132275 iterations) and MADBase (15000 iterations).
Figure 8: Visualization of prototypes training on different datasets (from left to right): Devanagari(4500 iterations), EMNIST (86250 iterations, rotated and mirrored MNIST classes) and Fruits (1475iterations).
