Figure 1: Perplexity (exponential of conditional entropy, given the past) of the t-th generated word,for two popular language models, averaged over more than 500 generation runs with different con-texts. At t = 1, this is the model’s upper bound for the language’s perplexity. As t → ∞, this is theexponential of the entropy rate of the model’s own generations. For a perfectly calibrated model,this curve would be flat (gray dotted lines). Left: LSTM trained on Penn Treebank. Right: GPT-2Transformer.
Figure 2: Effect of calibrating an LSTM genera-tive model with 1-step lookahead. Blue: perplex-ity curve (i.e. exponential of conditional entropyH) from the setting of Figure 1. Green: the sameperplexity measurements after applying local cal-ibration.
Figure 3: Left: Plot of the upper bound on Iτ derived from calibrated models. Right: The measure-ments of the upper bound on mutual information, the cross entropy of the limited memory modelPr as well as the optimal calibration coefficient a* for various time lengths T. Details of the modelused here can be found in the supplementary material.
