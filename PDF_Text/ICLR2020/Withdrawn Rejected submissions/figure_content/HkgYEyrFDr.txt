Figure 1: Overview of CRL (curious representation learning). We use the loss of a representation learningalgorithm as an intrinsic reward for an agent. Correspondingly, we train the representation learning algorithm onthe experience generated by the agent. This forces the agent and model compete in minimax game to learn goodvisual representationsIn addition, we investigate optimizing visual representation through CRL as a curiosity bonus. Wefind that on Atari, CRL is able to obtain better overall performance compared to other approachessuch as Forward Dynamics and RND, when used as a sole intrinsic bonus.
Figure 2: An visualization of objective optimization with corresponding perceptual representation learning on						the maximum distance from origin objective with 10 environments.						Environment	Counts	Movement	Shooter	CRL	Training	ClassificationNumber	Based	Distance	Opt.	Model	Reward	Performance1	0.794	0.719	0.573	Autoencoding	47.63	0.79910	0.911	0.733	0.549	RND	15.61	0.721100	0.970	0.922	0.647	Colorization	29.69	0.7661000	0.689	0.826	0.635	Random	19.61	0.468Table 1: Correlation coefficient of measured represen-				Table 2: Relative performance on the Movement		tation accuracy and objectives (reward across different				Distance (10 levels) objective when using CRL ini-		environment numbers in ViZDoom evaluate across 3				tialized with curious policies with different repre-		different seeds).	P values	are all in the 1e-6 - 1e-12		sentation learning values indicated compared with		range.				random policy after 1M frames of training.		with policies initialized with different visual representations from CRL. We report the the relativemagnitude of overall reward compared with initializing from random policy after 1 million frames oftraining in Table 2. Overall, we find that better visual representations obtained from CRL are able tolead to better relative performance after 1 million frames, with policy initialization with CRL with anautoencoding objective outperforming a policy from scratch by 242% in relative performance.
Figure 3: Plots of policy visual representa-tions When trained under different objectiveswith different number of environmentsFigure 4: Plots of policy visual representationswith representation model visual representation un-der CRL from 10, 100, 1000 environments. Thereis a good correlation between policy representationand model representationEnvironment Number	Counts Based	Movement Distance	Shooter Optimization	Autoencode	CRL Colorization	RND~L	0.749 (0.028)	0.717 (0.017)	0.772 (0.019)	0.748 (0.016)	0.676 (0.022)	0.672 (0.013)10	0.783 (0.046)	0.780 (0.010)	0.679 (0.025)	0.794 (0.011)	0.733 (0.010)	0.746 (0.010)100	0.818 (0.019)	0.778 (0.013)	0.723 (0.010)	0.811 (0.012)	0.762 (0.007)	0.810 (0.012)1000	0.623 (0.079)	0.832 (0.002)	0.763 (0.006)	0.819 (0.021)	0.747 (0.005)	0.805 (0.012)Table 4: Comparison of visual representations learned in policies from CRL and other objectives. We findthat CRL with an autoencoding objective consistently gives relatively good visual representations while otherobjective, such counts based may sporadically give better visual representation, but are not consistently acrossenvironments.
Figure 4: Plots of policy visual representationswith representation model visual representation un-der CRL from 10, 100, 1000 environments. Thereis a good correlation between policy representationand model representationEnvironment Number	Counts Based	Movement Distance	Shooter Optimization	Autoencode	CRL Colorization	RND~L	0.749 (0.028)	0.717 (0.017)	0.772 (0.019)	0.748 (0.016)	0.676 (0.022)	0.672 (0.013)10	0.783 (0.046)	0.780 (0.010)	0.679 (0.025)	0.794 (0.011)	0.733 (0.010)	0.746 (0.010)100	0.818 (0.019)	0.778 (0.013)	0.723 (0.010)	0.811 (0.012)	0.762 (0.007)	0.810 (0.012)1000	0.623 (0.079)	0.832 (0.002)	0.763 (0.006)	0.819 (0.021)	0.747 (0.005)	0.805 (0.012)Table 4: Comparison of visual representations learned in policies from CRL and other objectives. We findthat CRL with an autoencoding objective consistently gives relatively good visual representations while otherobjective, such counts based may sporadically give better visual representation, but are not consistently acrossenvironments.
Figure 5:	Comparison of random policy with CRL (curious representation learning) on a test level. CRL (withautoencoding objective) is able to induce diverse paths that explore different sets of rooms.
Figure 6:	Illustration of nearest neighbors on room scene in Places of a CRL model trained on Habitat comparedto a random network. CRL training in simulation transfers to real scene as seen in the beds in the last row.
Figure 7: Comparison of CRL (curious representation learning with autoencoding objective) vs RND vsDynamics on Atari using only intrinsic reward across 3 different seeds. CRL performs favorably and gets thehighest score in 6 of the 8 evaluated environments.
Figure 8: Illustration of nearest neighbors in Doom of a CRL(AE) policy trained on Habitat compared to arandom network. Nearest neighbor in CRL space is able to cluster more visually similar imagesWe further show nearest neighbor images on VizDoom in Figure 8. The leftmost column is the queryimage while the other 4 columns are the 4 nearest neighbors in embedding space. Training throughCRL allows clustering of various doom objects.
