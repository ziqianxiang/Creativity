Figure 1: An example of metro line expansion.
Figure 2: Network architecturedecoding, the attention mechanism is employed. The attention mechanism takes the encoder outputXt as reference and the hidden state ht as query to generate the probability distribution whichindicates the probability of each candidate station. In the attention mechanism, the probability iscalculated asqit = vaT tanh(Wa [Xit ; ht]), at = softmax(qt)	(4)ct = X ait Xit	(5)iP(zt+1 |G, Zt) = SoftmaX(VT tanh(Wc[X；; ct])㊉ H ∙ Mt)	(6)where at is the relevant parameter, and ct is the context parameter. Wa, Wc, vaT and vcT are train-ing parameters. [...;... ] represents the element-wise concatenation operator and ㊉ represents theelement-wise sum operator. H is a huge constant and Mt reflects the feasibility rules.
Figure 3: The current city operational status.
Figure 4: The next expanded metro line with higher priority. The blue lines are our expanding lines.
Figure 5: Comparisons with the expanded lines in predefined corridors. The area between twoyellow lines is the predefined corridors. The blue (violet) lines are the expanded lines with our(baseline) method, considering only OD trips.
Figure 6: Multiple expanded metro lines.
