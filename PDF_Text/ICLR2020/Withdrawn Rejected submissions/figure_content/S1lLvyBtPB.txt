Figure 1: Example of subset scanning score distributions across layers of an autoencoder foradversarial BIM noise = 0.01. In the top of the graph we can see subset score distributions pernodes in a layer. The distributions of subset scanning scores are shown in blue for clean images(C) (expected distribution), and in orange for noised samples At . Higher AUCs are expected whendistributions are separated from each other and lower AUCs when they overlap. The purple struc-ture corresponds to convolutional layers at the Encoder, while the red structure corresponds to theconvolution layers for the Decoder. The computed AUC for the subset score distributions can befound in Table 1. The highest mutual information exchange with the adversarial input happens onthe first layers (convolutional and maxpooling). This is why the greatest divergence in both C andAt subset scores distributions is seen. In the latent space, due to properties described in Section 4,the autoencoder abstracts basic representations of the images, losing subset scanning power due tothe autoencoder mapping the new sample to the expected distribution. This can be seen as an almostperfect overlap of distribution in convJ2dJ.
Figure 2: The connection between the number of nodes in a subset, α value that maximizesthe non-parametric scan statistic, and the resulting subset score. These results are for Fashion-MNIST examples with activations coming from the first layer of the autoencoder. Under the pres-ence of BIM adversarial noise, we observe a larger number of nodes that have smaller p-values. Thiscombination results in a higher subset score than the clean images. Critically, the LTSS property al-lows α to be efficiently chosen to maximize the score for each individual image. The subset size isall nodes with p-values less than the α threshold. We enforce a αmax = 0.5 constraint on the search.
Figure 3: (a) ROC curves for each of the noised cases as compared to the scores from test setscontaining all natural images for layer Conv2d_1. (b) Distribution of subset scores for test sets ofimages over Conv2d_1. Test sets containing all natural images had lower than scores than test setscontaining noised images. Higher proportion of noised images resulted in higher scores.
Figure 4: Reconstruction error baseline visualization. (a) Baseline mean reconstruction errorsamples for clean images and BIM noised samples processed by the autoencoder. We can observethat in clean samples reconstruction error only appears on the contours of the number, while noisesamples have lower reconstruction error values distributed throughout the image. (b) Mean recon-struction error distribution for clean (blue distribution) and noise samples (orange samples).
Figure 5: Anomalous nodes visualization. Overlap of anomalous nodes (white) and reconstructionerror (darker blue) per sample. (a) Noised samples with BIM. We can observe that nodes outside thecontour will make the sample be classified as noised. (b) Whereas clean we expect the anomalousnodes will be along the contour of the figure.
Figure 6: (a) Distribution of subset scores for test sets of images over reconstruction error. Test setscontaining all natural images had lower than scores than test sets containing noised images. Higherproportion of noised images resulted in higher scores. (b) ROC curves for each of the noised casesas compared to the scores from test sets containing all natural images.
Figure 7: Successful noised samples from MNIST and Fashion-MNIST generated with Adver-sarial Robustness Toolbox (Nicolae et al., 2018). (a) BIM noised samples (b) DeepFool samples(c) FGSM. Parameters used for each attack are detailed at Section 5.2.
Figure 8:	Autoencoder architecture diagram. Same architecture was implemented for bothdatasets and for different proportions of noised samples.
Figure 9:	Activation distribution values across all layers. Most values are accumulated around 0 due to ReLu activations. The large skew and sometimes bi-modaldistribution of activations motivated the use of non-parametric scan statistics to quantify what it means for an activation to be larger-than-expected.
