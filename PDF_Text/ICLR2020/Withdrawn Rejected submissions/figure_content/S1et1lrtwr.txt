Figure 1: Unsupervised meta-reinforCement learning: Given an environment, unsupervised meta-RL produces an environment-specific learning algorithm that quickly acquire new policies thatmaximize any task reward function.
Figure 3: Unsupervised meta-learning accelerates learning: After unsupervised meta-learning, our approach(UML-DIAYN and UML-RANDOM) quickly learns a new task significantly faster than learning from scratch,especially on complex tasks. Learning the task distribution with DIAYN helps more for complex tasks. Resultsare averaged across 20 evaluation tasks, and 3 random seeds for testing. UML-DIAYN and random alsosignificantly outperform learning with DIAYN initialization or an initialization with a policy pretrained withVIME.
Figure 4: Comparison with handcrafted tasks: Unsupervised meta-learning (UML-DIAYN) is competitivewith meta-training on handcrafted reward functions (i.e., an oracle). A misspecified, handcrafted meta-trainingtask distribution often performs worse, illustrating the benefits of learning the task distribution.
Figure 5: Analysis of effect of additional meta-training on meta-test time learning of new tasks. For largeriterations of meta-trained policies, We have improved test time performance, shoWing that additional meta-training is beneficial.
Figure 6: Learned meta-training task distribution and evaluation tasks: We plot the center of mass forvarious skills discovered by point mass and ant using DIAYN, and a blue histogram of goal velocities for cheetah.
