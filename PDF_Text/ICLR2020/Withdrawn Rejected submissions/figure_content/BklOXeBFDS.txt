Figure 1: Overview of our model. The green box represents the components that forming the state.
Figure 2: Performance of DAG-Distill under different budgets. Vertical lines represent the standarderror of experiments5 ConclusionThis paper studies active learning for graph neural networks, where two different settings are con-sidered, i.e., active learning on single graphs and transfer active learning on multiple graphs. We for-malize the problem as a sequential decision making process, and propose a policy gradient methodon single graphs. For transfer reinforcement learning, a joint training approach and a distillation-based approach are proposed. Experimental results prove the effectiveness of our approaches. In thefuture, we plan to use our approach in larger datasets, where a large number of graphs are available.
Figure 3: Case study carried out on Cora dataset11Under review as a conference paper at ICLR 2020A.2 Ablation StudyWe conduct contrast experiments to study the importance of different features to the model. Wetake DAG-Single on Cora dataset as an example. To study the importance of structure feature to themodel (denoted by -structure in Table 5), we substitute the Struc2vec features with random vectorsand report the performance of model trained with the absence of Struc2vec features. Similarly, otherfeatures are replace by zero or random vectors in order to demonstrate the their importance. From thetable, we see that heuristic features and structure features both have significant benefits to the model.
