Figure 1: The importance of retaining the original minimum: A number of works have shown theexistence of an empirical correlation between generalization error and flatness in the loss landscapearound the deep neural network minimum μ0. Recently popular PAC-Bayes analyses can be seenas a formal way of quantifying this flatness. However, moving forward from simple empirical cor-relations, existing optimization based non-vacuous bounds compute implicitly a different minimumμo and then evaluate the flatness around that minimum. By contrast We aim to estimate the flatnessand a related optimal posterior distribution around the original minimum μ0.
Figure 2: Feasible solutions vs Non-Vacuous solutions: We merge the 10 classes in Mnist and Cifarto create simpler 2 class problems. For different values of β We compute the optimal complexityKL(Q∣∣P) + ln 2(N-1)terms V -' ', ]N------δ- using 7, 8. We compute the accuracy of the stochastic classifier withMC for 5 samples. We plot this loWer bound With the solid black line. All points above it arefeasible. We see that for the Mnist problem the two regions intersect suggesting that we might beable to prove generalization using Gaussians with diagonal covariance. By contrast in the Cifar casethe two regions do not intersect suggesting that the prior and posterior means have a prohibitivedistance between them, and we cannot prove generalization with diagonal covariances.
Figure 3: Accuracy Vs Complexity for different bounds: We plot ʌ/ KL(QIP2+n δ and train-ing accuracy (of the randomized classifier) for different architectures and datasets. Points to the rightof the dashed line correspond to non-vacuous pairs. All Mnist bounds are non-vacuous. All Cifarbounds are vacuous. We are able to progressively get tighter bounds by using the diagonal Hessianand then the full layerwise Hessian. The improvement is larger over the more difficult Cifar dataset.
Figure 4: Accuracy Vs Complexity for different bounds: We plot V -寺 δ- and tram-ing accuracy (of the randomized classifier) for different architectures and datasets. Points to the rightof the dashed line correspond to non-vacuous pairs. All Mnist bounds are non-vacuous. All Cifarbounds are vacuous. We are able to progressively get tighter bounds by using the diagonal Hessianand then the full layerwise Hessian. The improvement is larger over the more difficult Cifar dataset.
