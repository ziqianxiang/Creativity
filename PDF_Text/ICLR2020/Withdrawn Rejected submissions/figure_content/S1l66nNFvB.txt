Figure 1: Train loss reduction and test loss re-duction achieved by GWM on various model-dataset pair. The shape of each point presentsthe dataset used, and the color of each pointpresents the GNN model used. The horizon-tal axis denotes the ratio of reduced trainingloss, and the vertical axis denotes that of thetest loss. That the x-coordinate of a point ispositive implies that the attachment of GWMimproved the train performance for the cor-responding model-dataset pair. That the y-coordinate of a point is positive implies thatGWM improves the test-performance.
Figure 2: The overview of the proposed Graph Warp Module (GWM). A GWM consists of asupernode, a transmitter unit, and a warp gate unit. A GWM can be added to the original GNN asan auxiliary module. At each layer, the supernode and the main network communicate through thetransmitter and the warp gate.
Figure 3: Details of the GWM computations.
Figure 4: Training losses of various GNN models on a molecule graph dataset (Tox21). The horizontalaxis denotes the number of GNN layers (the left panel) or the dimension of the node feature vectors(the right panel). Color denotes the GNN model. Thinner dashed lines are the losses of the vanillaGNNs, while thicker solid lines show the losses of the GNNs attached with the proposed Graph WarpModule (“GWM”). Scores are partially unavailable due to memory shortages.
Figure 5: Train (horizontal) and test (vertical) loss reduction ratios on various pairs of GNN modelsand datasets. L = 3, D = 32. Each plot presents the rational train/test loss reductions induced by theGWM attachment for a specific pair of (dataset(symbol), GNN(color)).
Figure 6: Train (horizontal) and test (vertical) loss reduction ratios on various pairs of GNN modelsand datasets. L = 4, D = 100. Each plot presents the rational train/test loss reductions induced bythe GWM attachment for a specific pair of (dataset(symbol), GNN(color)).
