Figure 1: Implicit competitive regularization in the nashless game: Under full information, eachplayer moves towards infinity as quickly as possible (first panel). Under limited information, butwithout accounting each other’s actions, the players oscillate and eventually diverge (second panel).
Figure 2: Overtraining Discriminators: We begin by training a GAN on MNIST using Adam for21 (11) epochs, saving the resulting generator and discriminator as a ”checkpoint”. We then over-train the discriminator while keeping the checkpoint generator fixed, achieving low discriminatorloss (second panel). As indicated by the increasing gradient norm of the generator during training(third panel), the resulting discriminator is brittle and incurs large losses as we start training the gen-erator again (fourth panel). Values computed from minibatches and averaged over small x-windows.
Figure 3:	Implicit competitive regularization: First panel: While simple gradient ascent wouldlead towards the overtrained discriminator D* (first arrow), CGD tries to also reduce the other Play-ers gradient (second arrow) and for large mixed Hessian DD2 G f, projects the update on the orthog-onal comPlement of the leading singular vector. We illustrate how this could Prevent overtraining,with the thick black curve dePicting the ”manifold of robust Play”. Second and third Panel: WhenattemPting to overtrain the discriminator using CGD, it actually becomes more robust (comPare toFigure 2).
Figure 4:	We plot the inception score against the number of iterations (first panel) and gradient orHessian-vector product computation (second panel). The third panel shows final samples of WGANtrained with ACGD and without regularization.
Figure 5:	First panel: We compare the variance over seven runs of the same model with ACGD andAdam. Second panel: We plot the difference between inception scores between ACGD and Adam(positive values correspond to a larger score for ACGD) over all iterations and models. Third panel:The only cases where we observe nonconvergence of ACGD are OGAN without regularization orwith weight decay of weight 0.0001. The inception score is however still higher than for the samemodel trained with Adam.
Figure 6: We plot the Tensorflow inception scores against the number of generator iterations(firstpanel) and gradient or Hessian-vector product computation (second panel).
