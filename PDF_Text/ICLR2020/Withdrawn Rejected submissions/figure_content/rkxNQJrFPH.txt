Figure 1: Overview of N3: class descriptions for a new classification task are fed to the N3 meta-model and it generates parameters for task-specific model that classifies the designated objects.
Figure 2: N3 Model Architecture. N3 utilizes a two-level attention mechanism to map task meta-data to adapted neural network parameters: the Semantic Encoding Module applies a first levelof attention mechanism over contextualized word embeddings of class description to produce de-scription embeddings for each class; the Parameter Adaptation Module applies a second level ofattention mechanism over all of the encoded class description embeddings to generate a parameterdelta, which is then combined with the corresponding pre-trained model parameters to produce atask-specific neural network with its parameters adapted toward the new task.
Figure 3: Composition of Fine-Tuned ResNet18 Parameters Produced by N3, Measured by L2 Norm(the image enters through the leftmost layer and the prediction exits from the rightmost layer)we replaced its image feature embedding module, which is trained with the help of per-sample partsannotations, the performance dropped significantly (from 11.3% to 3.3%), confirming our conjecturethat such methods cannot be easily adapted to work under strict metadata-efficiency constraint.
Figure 4: Textual Descriptions for ”Wind-flowers”samples are mis-classified as windflowers. This alsolead Us to conclude that the quality of metadata plays a critical role dictating the performance of thefine-tuned model; and a single poorly constructed metadata entry can have disproportionate effecton fine-tuned model accuracy; nonetheless, such problem is not unique to our approach but rathera consequence of an efficiency-robustness trade-off inherent in our problem setup. Moreover, ournatural language based metadata makes such problem exceedingly easy to diagnose. On the otherhand, We also found that the metadata procured from online sources vary in quality and tend to fo-cus on facets that are most appealing to human readers; such aspects of visual descriptions tend toconstitute only necessary, but not sufficient conditions for classification decisions. To conclude, webelieve that N3's performance can be increased further with higher quality task descriptions.
Figure 5: Upper Left: (a) Per-Layer Average L2 Norm of Parameter Adaptations Grouped By LayerTypes. Lower Left: (b) Penultimate Layer Activation Vector Magnitude Distribution Before andAfter N3 Fine-Tuning. Right: (c) Normalized Confusion Matrix for Flowers-Species.
Figure 6: Zero-Shot Classification Performance on CUB-2011/NAB with SCE-split.
