Figure 1: Generalization across scale shifts between training and testing conditions is difficult.
Figure 2: Overview. Dynamic receptive field scale (top) is optimized according to the output (bottom)at test time. We optimize receptive field scales and filter parameters to minimize the output entropy(middle). Optimizing during inference makes iterative updates shown from left to right: receptivefield scale adapts, entropy is reduced, and accuracy is improved. This gives a modest refinement fortraining and testing at the same scale, and generalization improves for testing at different scales.
Figure 3: Iterative dynamic inference by our entropy minimization. We optimize output entropywith respect to task and scale parameters. (a) Input and ground truth. (b) Output entropy. (c) Outputprediction. Our optimization reduces entropy and improves prediction accuracy.
Figure 4: Analysis of dynamic receptive field sizes across scale shift. (a) plots the distributionof dynamic receptive fields, confirming that optimization shifts the distribution further. (b) is theprediction at 1× scale while (c) and (d) are the prediction baseline and our iterative optimization at3× scale. (c) and (d) are visually similar, in spite of the 3× shift, showing that the predictor has failedto adapt. Optimization adapts further by updating the output and scale parameters, and the receptivefields are accordingly larger. For visualization darker indicates smaller, and brighter indicates larger.
Figure 5: Qualitative results from the PASCAL VOC validation set. Our model is trained on 1×scale and tested on 3× scale. (a) and (e) are the input image and ground truth. (b) indicates thereference in-distribution prediction on 1× scale. (c) is the out-of-distribution prediction for thedynamic prediction baseline. (d) is the out-of-distribution prediction for our iterative optimizationmethod. Our method corrects noisy, over-segmented fragments and false negatives in true segments.
