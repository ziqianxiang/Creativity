Figure 1: The three steps in a federated learning round. (a) The server communicates the latest modelparameters wt and the training hyper-parameters ht to each client. (b) client i trains the modelparameters wti and the representation matching parameters θti to minimize the classification lossand the matching loss. (c) Each client sends back the updated model parameters to the server. Theserver aggregates the received parameters, evaluates the loss of the aggregate model, and updatesthe parameters of the hyper-parameter distribution, ψ, based on the history of the aggregate losses.
Figure 2: Illustration of representation matching using a model with one convolutional layer andtwo fully connected layers. Layers whose activations are used in the matching loss are shown in red.
Figure 3: Evolution of the mean of the hyper-parameter distribution P(H∣ψ) for the iid and non-iidcases. Results taken from the FA+RM+AH algorithm when C = 1.0. The evolution of the meansare shown separately for the learning rate (first row) and for the number of SGD steps per round(second row), with one column each for the MNIST, CIFAR10, and keyword spotting (KWS) tasks.
