Figure 1: Toy example. By compar-ing the simulations under σ = 0 andσ = 2.8, We see adding noise to the sys-tem can be an effective Way to controlxt. Average over multiple runs is used tocancel out the volatility during the earlystage. It is noteWorthy that here We em-ploy the multiplicative noise, Where thedeviation term scales proportional to xt .
Figure 2: Our model architecture (more details can be found in appendix). The initial value of SDEis the output of a convolutional layer, and the value at time T is passed to a linear classifier afteraverage pooling.
Figure 3: Left: we compare the propagation time between Neural ODE, ODE with adjoint, andSDE. We can see that the running time increases proportionally with network depth and there isno significant overhead in neural SDE. Right: We compute the error of SDE solver caused bydiscretization in EUler schemes, measured by the relative error in ht, i.e. ε = khTh-hTk andhτ is the ground-truth (computed with a very fine grid), hT is computed with coarse grid ∆t ∈[1.0 X 10-4,1.0 X 10-1] (note that network depth t = T/∆T).
Figure 4: Comparing the robustness against '2-norm constrained adversarial perturbations, on CIFAR-10 (left), sTL-10 (middle) and Tiny-ImageNet (right) data. We evaluate testing accuracy with threemodels, namely Neural oDE, Neural sDE with multiplicative noise and dropout noise.
Figure 5: Comparing the perturbationsof hidden states, εt , on both oDE andsDE (we choose dropout-style noise).
Figure 6: Discretization error under different step size in SDE solver.
Figure 7: Running time comparison (forward propagation) between Neural SDE, Neural ODE andNeural ODE - adjoint. The curves are largely overlapped, meaning all methods have running timeproportional to network depth.
