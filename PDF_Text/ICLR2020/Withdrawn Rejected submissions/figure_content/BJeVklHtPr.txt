Figure 1: A 16-4 Wide-ResNet, trained on CIFAR-10 for 200 epochs at a range of batch sizes. Wereport the performance both with and without ghost batch normalization, and we also provide theperformance without ghost batch normalization using our modified initialization scheme “ZeroInit”.
Figure 2: A 16-4 Wide-ResNet, trained on CIFAR-10 for 200 epochs at a range of batch sizes.
Figure 3: The performance of a 16-4 Wide-ResNet in the small batch limit, trained on CIFAR-10 for200 epochs. We perform a grid search over the learning rate and provide the mean performance of thebest 12 out of 15 runs. In contrast to the rest of this paper, we do not use ghost batch normalization,and evaluate the batch statistics over the whole batch. Batch normalization outperforms ZeroInit onthe training loss. However it underperforms ZeroInit on the test set when the batch size is very small.
Figure 4: A 16-4 Wide-ResNet, trained on CIFAR-10 for 200 epochs at a range of batch sizes in thesmall batch limit. In order to study batch normalization as the batch size B → 1, we use the originalformulism of batch normalization here, where the batch statistics are evaluated over the trainingminibatch. a) The learning rate which maximizes the test accuracy is proportional to the batch sizewhen the batch size is small, and all three methods exhibit similar learning rates at the same batchsize in this limit. This demonstrates that the stability of batch normalization at large learning rates isnot the reason why batch normalization performs better on the test set when the batch size is small. b)We observe similar trends when we evaluate the optimal learning rates which minimize the trainingloss.
