Figure 1: Schematic of the Video Instance Embedding (VIE) Framework. a. Frames fromindividual videos (v1 , v2, v3) are b. sampled into sequences of varying lengths and temporaldensities, and input into c. deep neural network pathways that are either static (single image)or dynamic (multi-image). d. Outputs of frame samples from either pathway are vectors in theD-dimensional unit sphere SD âŠ‚ RD+1. The running mean value of embedding vectors arecalculated over online samples for each video, e. stored in a memory bank, and f. at each timestep compared via unsupervised loss functions in the video embedding space. The loss functionsrequire the computation of distribution properties of embedding vectors. For example, the LocalAggregation (LA) loss function involves the identification of Close Neighbors Ci (light brown points)and Background Neighbors Bi (dark brown points), which are used to determine how to move targetpoint (green) relative to other points (red/blue).
Figure 2: Video retrieval results for VIE-Single and VIE-Slowfast models from Kinetics validationset. GT=ground truth action label, Pred=model prediction. For each query video, top three nearesttraining neighbors are shown. Red font indicates a kNN-classifier prediction error.
