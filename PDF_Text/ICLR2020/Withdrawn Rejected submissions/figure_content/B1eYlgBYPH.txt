Figure 1: The generic form of the proximal operators for Algorithm 1 - but also the activation functionin the proposed reweighted-RNN. Note that per unit per layer gι leads to a different activation function.
Figure 2: The proposed (a) reweighted-RNN vs. (b) 'ι-'ι-RNN and (c) Stacked RNN with d layers.
Figure 3:	Average mean square error between the original and reconstructed frames vs. trainingepoches on the training and the validation sets for the default setting (a CS rate is 0.2, d = 3, h = 210).
Figure 4:	Reweighted-RNN: Average mean square error between the original and reConstruCted framesvs. training epoChes on the training and the validation sets with different CS rates (d = 3, h = 210).
Figure 5:	Reweighted-RNN: Average mean square error between the original and reconstructedframes vs. training epoches on the training and the validation sets with different network widths h (aCS rates is 0.2, d = 3).
Figure 6:	Reweighted-RNN: Average mean square error between the original and reconstructedframes vs. training epoches on the training and the validation sets with different network depths d (aCS rate is 0.2, h = 210).
Figure 8: Reweighted-RNN on the (a) adding task with average mean square error and the (b) copytask with average cross entropy vs. training epoches on the validation set.
