Figure 1: Behavioral Embedding Maps(BEMs) map trajectories to points in thebehavior embedding space E . Two trajec-tories may map to the same point in E .
Figure 2: Behavioral embedding func-tions corresponding to two policies π1(green) and π2 (blue) whose BEMs maptrajectories to points in the real line.
Figure 3: BGPG vs. TRPO: We compare BGPG and TRPO (KL divergence) on several continuous controltasks. As a baseline we also include results without a trust region (β = 0 in Algorithm 2). Plots show themean ± std across 5 random seeds. BGPG consistently outperforms other methods.
Figure 4: The clock-time comparison (in sec) of BGPG (alternating optimization) with particle approximation.
Figure 5: Efficient Exploration. On the left we show a visualization of the simulated environment, with thedeceptive barrier between the (quadruped) agent and the goal. On the right, we show two plots with the mediancurve across five seeds, with the IQR shaded for the quadruped and point environment respectively.
Figure 6: Escaping Local Maxima.
Figure 7: Imitation Learning.
Figure 8: Choice of BEM8 Conclusion and Future WorkIn this paper we proposed a new paradigm for on-policy learning in RL, where policies are em-bedded into expressive latent behavioral spaces and the optimization is conducted by utilizing therepelling/attraction signals in the corresponding probabilistic distribution spaces. The use of Wasser-stein distances (WDs) guarantees flexibility in choosing cost funtions between embedded policytrajectories, enables stochastic gradient steps through corresponding regularized objectives (as op-posed to KL divergence methods) and provides an elegant method, via their dual formulations, toquantify behaviorial difference of policies through the behavioral test functions. Furthermore, thedual formulations give rise to efficient algorithms optimizing RL objectives regularized with WDs.
Figure 9: Additional Experiment on TRPO. We compare No Trust Region with two alternative trust regionconstructions: KL-divergence and Wassertein distance (ours).
Figure 10: A sensitivity analysis investigating a) the impact of the embedding and b) the number of previouspolicies θt-i , i ∈ 1, 2, 5(b) Previous Policies15Under review as a conference paper at ICLR 2020For embeddings, we compare the reward-to-go (RTG), concatenation of states (SV) and final state(SF). In both the RTG and SF case the agent learns to navigate past the wall (> -800). For thenumber of previous policies, we use the SF embedding, and using 2 appears to work best, but both 1and 5 do learn the correct behavior.
Figure 11: Initial state of policies πa , πb and Behavioral Test functions λa , λb in the Multigoal environment.
Figure 12: Evolution of the policies and Behavioral Test Functions throughout optimization.
