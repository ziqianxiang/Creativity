Figure 1: Impact of Batch Augmentation (BA, withfour augmentations per sample) on ResNet-50 andImageNet. Depicted - training (dashed) and Vali-dation (solid) errors.
Figure 2: Comparison of gradient L2 norm(ResNet44 + cutout, Cifar10, B = 64) betWeen thebaseline (M = 1) and batch augmentation WithM ∈ {2, 4, 8, 16, 32}4Under review as a conference paper at ICLR 2020Table 1: ResNet-44 Gradient correlation on Cifar10. We measure the Pearson correlation coefficientρ between random images and augmented versions thereof ρ (x, T (x)), as well as for random imagesof the same class ρ (x, y) and different classes ρ (z, w). Augmentation types: RC=Random Crop,F=flip, CO=Cutout.
Figure 3: Impact of batch augmentation (ResNet44, Cifar10). We used the original (red) training(b) Final Validation errorFigure 4: A comparison between (1) baseline with B=640 and 10x more epochs. (2) our batchaugmentation (BA) method with M=10.
Figure 4: A comparison between (1) baseline with B=640 and 10x more epochs. (2) our batchaugmentation (BA) method with M=10.
Figure 5: Training (dashed) and validation error over time (in hours) of ResNet50 with B = 256and M = 4 (Red) vs M = 10 (Blue). Difference in runtime is negligible, while higher batchaugmentation reaches lower error. Runtime for Baseline (M = 1): 1.43 ± 0.13 steps/second, M = 4:1.47 ± 0.13 steps/second, M = 10: 1.46 ± 0.14 steps/second.
