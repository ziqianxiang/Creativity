Figure 1: A high-level depict for the proposed reasoning paradigm of inferring novelty from multi-step forwarddynamics prediction. A {L+H}-step transition graph is shown. The prediction of ot depends on a sequence ofobservations with length H followed by a sequence of actions with length L. Generally, the longer L, the moredifficult to predict ot .
Figure 2: Dual-LSTM architecture for the proposed sequence-level intrinsic model. Overall, the forwardmodel employs an observation sequence and an action sequence as input to predict the forward dynamics. Theprediction target for forward model is computed from a target function f *(∙). An inverse dynamics model isemployed to let the latent features ht encode more transition information.
Figure 3: The 3D navigation task domains adopted for empirical evaluation: (1) an example of partialobservation frame from ViZDoom task; (2) the spawn/goal location settings for ViZDoom tasks; (3/4) anexample of partial observation frame from the apple-distractions/goal-exploration task in DeepMind Lab.
Figure 4: Learning curves measured in terms of the navigation success ratio in ViZDoom. The figuresare ordered as: 1) dense; 2) sparse; 3) very sparse. We run each method for 6 times.
Figure 5: Learning curves for the procedu-rally generated goal searching task in Deep-Mind Lab. We run each method for 5 times.
Figure 6: Learning curves for ‘Stairway to Melon’ task in DeepMind Lab. Left: cumulative episodereward; Right: navigation success ratio. We run each method for 5 times.
Figure 7: Results of ablation study in the very sparse task of ViZDoom.
Figure 8: average steps of episode: (1) Vizdoom dense (2) Vizdoom sparse (3): Vizdoom very sparseWe observe that the episodic lengths decrease at different ratios for the compared algorithms. Notably,our algorithm decreases at a much faster ratio compared to all the baselines. Meanwhile, the episodiclength for both ‘SIM’ and ‘RND’ would converge to a better standard than the other methods.
Figure 9: Comparing different scaling factor setting on the following two ViZDoom scenarios: dense(left) and very sparse (right). Results are presented in terms of task rewards.
Figure 10: Comparing different scaling factor setting on the following two ViZDoom scenarios:dense (left) and very sparse (right). We present the episodic cumulative intrinsic rewards.
Figure 11: Experiment result on performing multi-step forward prediction evaluated on verySparseViZDoom task.
Figure 12: Average episodic length in the 'Stairway to melon' task from DeePMind Lab.
Figure 13: Learning curves for two hard exploration tasks in Atari 2600 domain. Left: ms-pacman (9actions); Right: solaris (18 actions).
Figure 14: Learning curves for a dense-rewarded Atari 2600 task Seaquest which has a large actionspace that consists of 18 actions.
