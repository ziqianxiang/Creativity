Figure 1:  Consider two tasks, MLEFT and MDOWN, in which an agent must navigate to the left andbottom regions of an xy-plane respectively.  From left to right we plot the reward for entering aregion of the state space for the individual tasks, the negation of MLEFT, and the union (disjunction)and intersection (conjunction) of tasks.  For reference,  we also plot the average reward function,which has been used in previous work to approximate the conjunction operator (Haarnoja et al.,2018; Hunt et al., 2019; Van Niekerk et al., 2019).  Note that by averaging reward, terminal statesthat are not in the intersection are erroneously given rewards.
Figure 2: An example of zero-shot Boolean algebraic composition using the learned extended valuefunctions.   Arrows represent the optimal action in a given state.   (a–b) The learned optimal goaloriented value functions for the base tasks.  (c) Zero-shot disjunctive composition.  (d) Zero-shotconjunctive composition.  (e) Combining operators to model exclusive-or composition.  (f) Compo-sition that produces logical nor. Note that the resulting optimal value function can attain a goal notexplicitly represented by the base tasks.
Figure 3: Results in comparison to the disjunctive composition of Van Niekerk et al. (2019). (a) Thenumber of samples required to learn the extended value function is greater than learning a standardvalue function.  However, both scale linearly and differ only by a constant factor.  (b) The extendedvalue functions allow us to solve exponentially more tasks than the disjunctive approach withoutfurther learning.   (c) In the modified task with 40 goals,  we need to learn only 7 base tasks,  asopposed to 40 for the disjunctive case.
Figure 4: By composing extended value functions from the base tasks (collecting blue objects, andcollecting squares), we can act optimally in new tasks with no further learning. To generate the valuefunctions, we place the agent at every location and compute the maximum output of the network overall goals and actions. We then interpolate between the points to smooth the graph. Any error in thevisualisation is due to the use of non-linear function approximation.
Figure 5: A Q-learning algorithm for learning extended value functions. Note that the greedy actionselection step is equivalent to generalised policy improvement (Barreto et al., 2017) over the set ofextended value functions.
Figure 6: An example of Boolean algebraic composition using the learned extended value functionswith dense rewards. Arrows represent the optimal action in a given state. (a–b) The learned optimalgoal oriented value functions for the base tasks with dense rewards. (c) Disjunctive composition. (d)Conjunctive composition. (e) Combining operators to model exclusive-or composition. (f) Compo-sition  that produces logical nor. We note that the resulting value functions are very similar to thoseproduced in the sparse reward setting.
Figure 7: Box plots indicating returns for each of the 16 compositional tasks, and for each of the fourvariations of the domain. Results are collected over 100000 episodes with random start positions.
Figure 8:  Results for the video game environment with relaxed assumptions.  We generate valuefunctions to solve the disjunction of blue and purple tasks, and the conjunction and exclusive-or ofblue and square tasks.
Figure 9:  The layout of the Four Rooms domain.  The circles indicate goals the agent must reach.
