Figure 1: DAP framework illustrated with a synthetic sound mixture spectrogram. With randomnoises as inputs, we use two sound prediction networks: Si and S? and two mask modulationnetworks: Mi and M? to perform source separation. The four networks share a same U-Netstructure (Ulyanov et al., 2018).
Figure 2: Synthetic tests: two single-tone sources (top two rows) and two cosine-tone sources(bottom two rows). Given single input mixture, DAP achieves perfect separation on both inputs.
Figure 3: Audio comparison on Universal-150 benchmark. Qualitatively, DAP significantlyoutperforms all the other blind separation methods. For full results, please check out our projectpage: https://iclr-dap.github.io/Deep-Audio-Prior/(ii) Curved input sounds Our next test is to validate if our DAP framework can handle temporallycoherent spectrogram changes. Two shifted cosine curves are combined together as input toour separation pipeline (see bottom two rows in Figure 2). We set the input noise according toequation (3). Again, our DAP framework achieves the desired separation of sources and masks.
Figure 4: Comparison with other deep network models. DNP does not require training dataset andDAP achieve higher denoising quality. As for supervised models, for example SEGAN, that aretrained on large dataset, although they usually work well on seen noises, it is hard to generalize tounseen noise. Here the speech is mixed with a noise that was not in the training set. While SEGANdid not remove the novel noise, DAP can better remove noises at the price of lower speech qualityin some segments. Please listen to the sounds to appreciate the difference.
Figure 5: Example of interactive DAP. Given a sound mixture, our DAP model can predict separatedsounds and the corresponding mask activation maps. Users can simply draw boxes to interact withour DAP model to tell where to deactivate or activate the predicted masks for refining predictedsounds. As shown, a user deactivate a region in the predicted mask for a separated sound, and thenwe obtain better results with the refined mask. All spectrograms are zoomed in for visualization.
Figure 6: Audio texture synthesis via latent space augmentation. From a 3-second input audio, wecan interpolate (in green) and extrapolate (in orange) the latent input noise to synthesize a seamless12-second audio.
Figure 7: DAP can also be applied to automatically remove audio watermark. Top row shows threeinput mixture audios A, B, and C, all mixed with watermark. Given these 3 mixtures, DAP canseperate each individual sound source out. Note that DAP is only trained on these 3 input mixtures.
Figure 8: Ablation study on noise design. From left to right, spectrograms are from (a) input mixture(b) without temporal noise (w/o TN) (c) without dynamic noise (w/o DN) (d) without abrupt changein dynamic noise (w/o AC) (e) our full DAP model (f) ground truth.
Figure 9: Ablation study on mask design. From left to right, spectrograms and masks are frommodels without 1D mask (w/o 1D), without nonzero mask loss (w/o NZ), and with both 1D maskand the loss (DAP).
Figure 10: Our audio generator and mask generator networks share the same U-Net structureas in (Ulyanov et al., 2018). There are mainly three modules: downsampling module (see Di),upsampling module (see Ui), and skip connection module (see Si). Batch normalization (BN) (Ioffe& Szegedy, 2015) is used accelerating and stabilizing training and LeakyReLU (He et al., 2015)is used as the nonlinearity function. nd[i], n。[i], n§[i] denote the number of filters at depth i forthe downsampling, upsampling, and skip-connections respectively. The values kd[i], ku[i], ks[i]correspond to the respective kernel sizes for the convolutional layers in different modules.
Figure 11: Dynamic results of our DAP from different training iterations. We show generated audiosources, masks, and the separated sounds with corresponding mask modulations at 500th, 1000th,..., 5000th iterations, respectively. After adding dynamic noise into inputs at 2000th iteration, themodel will quickly capture the violin sounds with large variations (see results at 2500th).
