Figure 1: (Left) Density estimation with a RealNVP model trained on Fashion MNIST. The modelassigns similar / higher likelihoods to several OoD datasets. (Right) Model mis-specification canresult in OoD samples having higher log-likelihoods.
Figure 2: Example demonstrating how BatchNorm miti-gates high-likelihood in OoD distributions. p(x), pf (x)and q(x) denote the original, model and OoD distribu-tions. Distribution of z2 are different for q(x) under thetwo modes of BatchNorm, leading to different likeli-hood results.
Figure 3: Average BPD (- logpθ(xj∣x-j)) for test samples with varying ratios of test samples inthe batch. The BPD of in-distribution samples do not increase as the ratio increase, yet that of OoDsamples increase significantly. This justifies the use of ∆b,r1,r2(x0) for OoD detection.
Figure 4: ROC curves for using the p-values for likelihood-based permutation tests. We assignpositive labels to samples in q(x) and negative labels to samples in p(x). While such tests can detectOoD samples from other datasets, they could be confused by samples from another generative model.
Figure 5: Samples and their BPD evaluated under pθ(x). (Top) pθ(x) is a RealNVP trained onCIFAR. (Bottom) pθ (x) is a PixelCNN++ trained on Fashion MNIST.
Figure 6: Histograms of BPD of test datasets on a RealNVP dataset trained on CIFAR-10.
