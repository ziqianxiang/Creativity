Figure 1: Linear regression on an over-parameterized(d = 120) and under-parameterized (d = 80) model withN = 100 samples generated randomly from a Gaussian,trained using SGD with minibatch size 1. Plots are averagedover 3 independent runs. Gradient cosine similarities werecalculated over all pairs of gradients.
Figure 2: The effect of network depth with CNN-β-2 on CIFAR-10. Left plot: convergence curves of SGD,Middle plot: minimum of pairwise gradient cosine similarities at the end of training, Right plot: kernel densityestimate of the pairwise gradient cosine similarities at the end of training (over all independent runs).
Figure 3: The effect of width with CNN-16-' on CIFAR-10. Left plot: convergence curves of SGD (for cleanerfigures, we plot results for width factors 2, 4 and 6 here), Middle plot: minimum of pairwise gradient cosinesimilarities at the end of training, Right plot: kernel density estimate of the pairwise gradient cosine similaritiesat the end of training (over all independent runs).
Figure 4: The effect of adding skip connections and batch normalization to CNN-β-2 on CIFAR-10. Plots showthe optimal training loss (left plot), minimum pairwise gradient cosine similarities (middle plot), and test setaccuracies (right plot) at the end of training.
Figure 5: Effect of varying depth on MLP-β-100.
Figure 6: Effect of varying width on MLP-300-'.
Figure 7:	The effect of network depth with CNN-β-2 on CIFAR-10. Left plot: final training loss values at theend of training, Middle plot: final test set accuracy values at the end of training. Right plot: curves showing theminimum of pairwise gradient cosine similarities during training.
Figure 8:	The effect of width With CNN-16-' on CIFAR-10. Left plot: final training loss values at the endof training, Middle plot: final test set accuracy values at the end of training. Right plot: curves showing theminimum of pairwise gradient cosine similarities during training.
Figure 9:	The effect of network depth with CNN-β-2 on CIFAR-100. Left plot: training loss values at the endof training. Middle plot: minimum of pairwise gradient cosine similarities at the end of training, Right plot:kernel density estimate of the pairwise gradient cosine similarities at the end of training.
Figure 10: The effect of width With CNN-16-' on CIFAR-100. Left plot: training loss values at the end oftraining. Middle plot: minimum of pairwise gradient cosine similarities at the end of training, Right plot: kerneldensity estimate of the pairwise gradient cosine similarities at the end of training.
Figure 11: The effect of depth with WRN-β-2 (no batch normalization) on CIFAR-10. Left plot: training lossvalues at the end of training. Middle plot: minimum of pairwise gradient cosine similarities at the end of training,Right plot: kernel density estimate of the pairwise gradient cosine similarities at the end of training.
Figure 12: The effect of depth with WRN-β-2 (no batch normalization) on CIFAR-100. Left plot: trainingloss values at the end of training. Middle plot: minimum of pairwise gradient cosine similarities at the end oftraining, Right plot: kernel density estimate of the pairwise gradient cosine similarities at the end of training.
Figure 13: The effect of width With WRN-16-' (no batch normalization) on CIFAR-10. Left plot: trainingloss values at the end of training. Middle plot: minimum of pairwise gradient cosine similarities at the end oftraining, Right plot: kernel density estimate of the pairwise gradient cosine similarities at the end of training.
Figure 14: The effect of width with WRN-16-' (no batch normalization) on CIFAR-100. Left plot: trainingloss values at the end of training. Middle plot: minimum of pairwise gradient cosine similarities at the end oftraining, Right plot: kernel density estimate of the pairwise gradient cosine similarities at the end of training.
Figure 15: The effect of adding skip connections and batch normalization to CNN-β-2 on CIFAR-100. Plotsshow the training loss, minimum pairwise gradient cosine similarities, and test accuracies at the end of training.
