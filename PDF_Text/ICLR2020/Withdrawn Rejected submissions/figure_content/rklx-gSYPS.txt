Figure 1: General model setup3.3	Choice of μWe found that the choice of the distribution μ that parametrized Lμ is critical for the performance ofthe machine learning algorithm. The reason is the following. Recall that We want to learn Vf ? toprecondition DPGD algorithm whose goal is to minimize nonsmooth and non-strongly convex (orill-conditioned) objectives f , see section 2. Let us imagine that we chose an uniform distribution forμ. Then, the situation in dimension one is represented by the figure 2.
Figure 2: The curve of Vf is typical of objectives f that we want to minimizeThe image distribution V of μ by Vf over-represents some areas of the feature space while otherareas are under-represented. When the distribution of the features is as degenerated as ν, it is obviousthat machine learning algorithms cannot generalize well (see Bengio (2012); Mesnil et al. (2011)).
Figure 3: Model with log-rescalingThe two first blocks correspond to the map G and the last block to the map H of section 3.2.
Figure 4: Learning Vf *. Left: Learning curves. Right: predictions.
Figure 5: Minimizing a one dimensional power function. Left: Objective function values. Right:iterates values.
Figure 6: Logistic regression with r0.5Figure 7: Logistic regression with r = 0.1Figure 8: Logistic regression with r = 0.05Our careful preconditioning allows LDPGD to outperform GD especially when the objective isill-conditioned, as predicted by the theory, see section 2.
Figure 7: Logistic regression with r = 0.1Figure 8: Logistic regression with r = 0.05Our careful preconditioning allows LDPGD to outperform GD especially when the objective isill-conditioned, as predicted by the theory, see section 2.
Figure 8: Logistic regression with r = 0.05Our careful preconditioning allows LDPGD to outperform GD especially when the objective isill-conditioned, as predicted by the theory, see section 2.
Figure 9: Regressed model on the imprecise datasetAs mentioned above, the lack of precision due to a naive standardization of the features is especiallyproblematic around 0. We therefore propose to preserve the scale around small values more thanaround large values. This can be achieved by applying the following transformation to the featuresbefore normalizing them (to have zero mean and unit variance):log(1 +x) ifx > 0log-rescaling : x 7→- log(1 - x) else.
Figure 10: Log-rescaling.
