Figure 1: Depiction of the architectures of different ligand-target binding-affinity prediction models.
Figure 2: Scatterplot of pairwise MCC performance per protein on the low-coverage split. Panel Acompares the DeepPCM model with unsupervised-learned descriptors to the benchmark model withhandcrafted descriptors. Panels B and C compare the performance of the full models versus the No-Interaction-Terms models when using unsupervised-learned or handcrafted descriptors respectively.
Figure 3: Architecture and Hyperparameters for the DeepPCM model.
Figure 4: Architecture and Hyperparameters for the No-Interaction-Terms Baseline model.
Figure 5: Architecture and Hyperparameters for the benchmark model from Lenselink et al. (2017)13Under review as a conference paper at ICLR 2020D CDDD DiagramFigure 6: General architecture of the translation model, using the example of translating betweenIUPAC and SMILES representations of 1,3-benzodioxole. The final CDDD model translates fromnon-canonical to canonical SMILES representations of compounds. Figure and text taken fromWinter et al. (2019) with permission from the authors.
Figure 6: General architecture of the translation model, using the example of translating betweenIUPAC and SMILES representations of 1,3-benzodioxole. The final CDDD model translates fromnon-canonical to canonical SMILES representations of compounds. Figure and text taken fromWinter et al. (2019) with permission from the authors.
Figure 7: Workflow to learn and apply deep protein representations. a. UniRep model was trainedon 24 million UniRef50 primary amino acid sequences. The model was trained to perform nextamino acid prediction (minimizing cross-entropy loss), and in so doing, was forced to learn how tointernally represent proteins. b . During application, the trained model is used to generate a singlefixed-length vector representation of the input sequence by globally averaging intermediate mLSTMnumerical summaries (the hidden states). A top model (e.g. a sparse linear regression or randomforest) trained on top of the representation, which acts as a featurization of the input sequence,enables supervised learning on diverse protein informatics tasks. Figure and text taken from Alleyet al. (2019) with permission from the authors.
