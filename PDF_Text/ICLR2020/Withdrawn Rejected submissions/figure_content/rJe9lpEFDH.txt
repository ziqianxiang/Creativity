Figure 1: L∞ increases as the Hes-sian becomes less axis-aligned.
Figure 2: We consider quadratic objectives varying across two axes: λm∕λ as well as a rotationvalue θ. The left plot depicts the ratio of the two relevant smoothness constants. L∞ is sensitiveto θ and grows relative to L2 = λmax as the problem becomes less axis-aligned. The right plotdepicts the relative performance of gradient descent and sign gradient descent on these problems.
Figure 3: Experiments on CIFAR10 dataset. When using a CNN (top), signSGD outperforms SGD(top left), which matches the behaviour predicted by the improvement ratio (top right), above 1 atthe beginning. When using a ResNet20 (bottom), the situation is reversed but the improvement ratiostill correctly predicts the relative performance of both algorithms.
Figure 4: For H ∈ Rd×d, We plot a contour line of f (x) = 1 xτHx, which forms an ellipsewith principal axes given by the eigenvectors and axis lengths given by the inverse eigenvalues.
